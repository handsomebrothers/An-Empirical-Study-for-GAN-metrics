{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:1 [D loss: 0.150988, acc.: 79.69%] [G loss: 0.590881]\n",
      "epoch:0 step:2 [D loss: 3.114324, acc.: 18.75%] [G loss: 0.439059]\n",
      "epoch:0 step:3 [D loss: 0.435287, acc.: 37.50%] [G loss: 0.525373]\n",
      "epoch:0 step:4 [D loss: 0.248321, acc.: 63.28%] [G loss: 0.735926]\n",
      "epoch:0 step:5 [D loss: 0.211151, acc.: 70.31%] [G loss: 0.787312]\n",
      "epoch:0 step:6 [D loss: 0.187532, acc.: 78.12%] [G loss: 0.773824]\n",
      "epoch:0 step:7 [D loss: 0.104695, acc.: 84.38%] [G loss: 0.910553]\n",
      "epoch:0 step:8 [D loss: 0.116839, acc.: 87.50%] [G loss: 0.774721]\n",
      "epoch:0 step:9 [D loss: 0.126653, acc.: 87.50%] [G loss: 0.831373]\n",
      "epoch:0 step:10 [D loss: 0.108630, acc.: 91.41%] [G loss: 0.941410]\n",
      "epoch:0 step:11 [D loss: 0.120206, acc.: 86.72%] [G loss: 0.926349]\n",
      "epoch:0 step:12 [D loss: 0.123305, acc.: 88.28%] [G loss: 0.852949]\n",
      "epoch:0 step:13 [D loss: 0.119570, acc.: 84.38%] [G loss: 0.854669]\n",
      "epoch:0 step:14 [D loss: 0.101526, acc.: 89.84%] [G loss: 1.000775]\n",
      "epoch:0 step:15 [D loss: 0.159257, acc.: 80.47%] [G loss: 0.952771]\n",
      "epoch:0 step:16 [D loss: 0.222929, acc.: 70.31%] [G loss: 0.941955]\n",
      "epoch:0 step:17 [D loss: 0.414942, acc.: 45.31%] [G loss: 1.063627]\n",
      "epoch:0 step:18 [D loss: 0.969338, acc.: 37.50%] [G loss: 0.856537]\n",
      "epoch:0 step:19 [D loss: 0.876854, acc.: 37.50%] [G loss: 1.116632]\n",
      "epoch:0 step:20 [D loss: 0.434797, acc.: 44.53%] [G loss: 0.930391]\n",
      "epoch:0 step:21 [D loss: 0.149763, acc.: 80.47%] [G loss: 1.060605]\n",
      "epoch:0 step:22 [D loss: 0.121529, acc.: 86.72%] [G loss: 1.041069]\n",
      "epoch:0 step:23 [D loss: 0.167284, acc.: 79.69%] [G loss: 0.867262]\n",
      "epoch:0 step:24 [D loss: 0.143607, acc.: 82.81%] [G loss: 1.079348]\n",
      "epoch:0 step:25 [D loss: 0.106098, acc.: 87.50%] [G loss: 1.160514]\n",
      "epoch:0 step:26 [D loss: 0.098252, acc.: 92.19%] [G loss: 1.093453]\n",
      "epoch:0 step:27 [D loss: 0.087716, acc.: 90.62%] [G loss: 0.995545]\n",
      "epoch:0 step:28 [D loss: 0.091825, acc.: 87.50%] [G loss: 0.970629]\n",
      "epoch:0 step:29 [D loss: 0.101321, acc.: 90.62%] [G loss: 1.073825]\n",
      "epoch:0 step:30 [D loss: 0.114854, acc.: 82.81%] [G loss: 1.079636]\n",
      "epoch:0 step:31 [D loss: 0.080732, acc.: 92.19%] [G loss: 1.035041]\n",
      "epoch:0 step:32 [D loss: 0.117356, acc.: 88.28%] [G loss: 0.945124]\n",
      "epoch:0 step:33 [D loss: 0.105197, acc.: 89.84%] [G loss: 1.109734]\n",
      "epoch:0 step:34 [D loss: 0.084702, acc.: 89.84%] [G loss: 1.023541]\n",
      "epoch:0 step:35 [D loss: 0.114515, acc.: 85.94%] [G loss: 1.068847]\n",
      "epoch:0 step:36 [D loss: 0.104787, acc.: 89.84%] [G loss: 1.066369]\n",
      "epoch:0 step:37 [D loss: 0.108644, acc.: 86.72%] [G loss: 1.008075]\n",
      "epoch:0 step:38 [D loss: 0.097684, acc.: 89.84%] [G loss: 1.092678]\n",
      "epoch:0 step:39 [D loss: 0.098920, acc.: 89.06%] [G loss: 1.058789]\n",
      "epoch:0 step:40 [D loss: 0.075903, acc.: 92.19%] [G loss: 1.246995]\n",
      "epoch:0 step:41 [D loss: 0.092303, acc.: 89.84%] [G loss: 1.133354]\n",
      "epoch:0 step:42 [D loss: 0.082191, acc.: 91.41%] [G loss: 0.975716]\n",
      "epoch:0 step:43 [D loss: 0.080258, acc.: 91.41%] [G loss: 1.134903]\n",
      "epoch:0 step:44 [D loss: 0.084054, acc.: 92.97%] [G loss: 0.935547]\n",
      "epoch:0 step:45 [D loss: 0.077194, acc.: 92.97%] [G loss: 1.035289]\n",
      "epoch:0 step:46 [D loss: 0.082997, acc.: 88.28%] [G loss: 0.950399]\n",
      "epoch:0 step:47 [D loss: 0.066388, acc.: 92.19%] [G loss: 0.997533]\n",
      "epoch:0 step:48 [D loss: 0.080689, acc.: 90.62%] [G loss: 1.032530]\n",
      "epoch:0 step:49 [D loss: 0.067397, acc.: 92.19%] [G loss: 0.929829]\n",
      "epoch:0 step:50 [D loss: 0.075499, acc.: 91.41%] [G loss: 1.043525]\n",
      "epoch:0 step:51 [D loss: 0.079605, acc.: 92.19%] [G loss: 1.171100]\n",
      "epoch:0 step:52 [D loss: 0.098161, acc.: 90.62%] [G loss: 1.119005]\n",
      "epoch:0 step:53 [D loss: 0.114859, acc.: 92.19%] [G loss: 0.971123]\n",
      "epoch:0 step:54 [D loss: 0.126451, acc.: 86.72%] [G loss: 0.862285]\n",
      "epoch:0 step:55 [D loss: 0.152344, acc.: 84.38%] [G loss: 1.143194]\n",
      "epoch:0 step:56 [D loss: 0.170429, acc.: 79.69%] [G loss: 0.854177]\n",
      "epoch:0 step:57 [D loss: 0.134832, acc.: 81.25%] [G loss: 0.939200]\n",
      "epoch:0 step:58 [D loss: 0.156321, acc.: 75.78%] [G loss: 1.003459]\n",
      "epoch:0 step:59 [D loss: 0.124698, acc.: 85.94%] [G loss: 0.935144]\n",
      "epoch:0 step:60 [D loss: 0.132295, acc.: 86.72%] [G loss: 0.981095]\n",
      "epoch:0 step:61 [D loss: 0.085149, acc.: 93.75%] [G loss: 1.077750]\n",
      "epoch:0 step:62 [D loss: 0.068973, acc.: 94.53%] [G loss: 1.067167]\n",
      "epoch:0 step:63 [D loss: 0.080057, acc.: 92.19%] [G loss: 1.023054]\n",
      "epoch:0 step:64 [D loss: 0.080338, acc.: 92.97%] [G loss: 0.941608]\n",
      "epoch:0 step:65 [D loss: 0.087784, acc.: 86.72%] [G loss: 1.009579]\n",
      "epoch:0 step:66 [D loss: 0.065999, acc.: 93.75%] [G loss: 0.926253]\n",
      "epoch:0 step:67 [D loss: 0.073589, acc.: 94.53%] [G loss: 1.113249]\n",
      "epoch:0 step:68 [D loss: 0.062861, acc.: 96.09%] [G loss: 1.006338]\n",
      "epoch:0 step:69 [D loss: 0.073042, acc.: 92.19%] [G loss: 0.962368]\n",
      "epoch:0 step:70 [D loss: 0.066850, acc.: 90.62%] [G loss: 0.964904]\n",
      "epoch:0 step:71 [D loss: 0.080630, acc.: 90.62%] [G loss: 1.012017]\n",
      "epoch:0 step:72 [D loss: 0.066959, acc.: 92.97%] [G loss: 0.858819]\n",
      "epoch:0 step:73 [D loss: 0.078682, acc.: 90.62%] [G loss: 1.001314]\n",
      "epoch:0 step:74 [D loss: 0.084076, acc.: 89.84%] [G loss: 1.154677]\n",
      "epoch:0 step:75 [D loss: 0.074427, acc.: 93.75%] [G loss: 1.102192]\n",
      "epoch:0 step:76 [D loss: 0.069119, acc.: 94.53%] [G loss: 0.838220]\n",
      "epoch:0 step:77 [D loss: 0.065655, acc.: 93.75%] [G loss: 1.056030]\n",
      "epoch:0 step:78 [D loss: 0.071708, acc.: 92.19%] [G loss: 1.161297]\n",
      "epoch:0 step:79 [D loss: 0.066418, acc.: 91.41%] [G loss: 0.923463]\n",
      "epoch:0 step:80 [D loss: 0.078111, acc.: 89.84%] [G loss: 0.942550]\n",
      "epoch:0 step:81 [D loss: 0.097113, acc.: 86.72%] [G loss: 0.913106]\n",
      "epoch:0 step:82 [D loss: 0.084900, acc.: 87.50%] [G loss: 0.870118]\n",
      "epoch:0 step:83 [D loss: 0.056832, acc.: 92.97%] [G loss: 0.996381]\n",
      "epoch:0 step:84 [D loss: 0.069633, acc.: 92.97%] [G loss: 1.024913]\n",
      "epoch:0 step:85 [D loss: 0.082378, acc.: 90.62%] [G loss: 1.095243]\n",
      "epoch:0 step:86 [D loss: 0.093659, acc.: 92.19%] [G loss: 1.179729]\n",
      "epoch:0 step:87 [D loss: 0.118610, acc.: 85.16%] [G loss: 0.937576]\n",
      "epoch:0 step:88 [D loss: 0.079646, acc.: 92.19%] [G loss: 1.045532]\n",
      "epoch:0 step:89 [D loss: 0.084210, acc.: 91.41%] [G loss: 1.057177]\n",
      "epoch:0 step:90 [D loss: 0.051694, acc.: 96.88%] [G loss: 1.030477]\n",
      "epoch:0 step:91 [D loss: 0.058480, acc.: 93.75%] [G loss: 1.033511]\n",
      "epoch:0 step:92 [D loss: 0.070815, acc.: 92.19%] [G loss: 0.938452]\n",
      "epoch:0 step:93 [D loss: 0.062387, acc.: 91.41%] [G loss: 0.898300]\n",
      "epoch:0 step:94 [D loss: 0.051785, acc.: 93.75%] [G loss: 1.155010]\n",
      "epoch:0 step:95 [D loss: 0.056777, acc.: 94.53%] [G loss: 1.047126]\n",
      "epoch:0 step:96 [D loss: 0.047853, acc.: 96.88%] [G loss: 1.148854]\n",
      "epoch:0 step:97 [D loss: 0.055232, acc.: 97.66%] [G loss: 0.760552]\n",
      "epoch:0 step:98 [D loss: 0.051826, acc.: 97.66%] [G loss: 0.953660]\n",
      "epoch:0 step:99 [D loss: 0.087689, acc.: 92.19%] [G loss: 0.877768]\n",
      "epoch:0 step:100 [D loss: 0.101418, acc.: 92.19%] [G loss: 1.264500]\n",
      "epoch:0 step:101 [D loss: 0.155731, acc.: 81.25%] [G loss: 1.037686]\n",
      "epoch:0 step:102 [D loss: 0.194061, acc.: 64.84%] [G loss: 1.041488]\n",
      "epoch:0 step:103 [D loss: 0.218253, acc.: 60.16%] [G loss: 0.833002]\n",
      "epoch:0 step:104 [D loss: 0.138221, acc.: 75.78%] [G loss: 1.069086]\n",
      "epoch:0 step:105 [D loss: 0.078838, acc.: 92.97%] [G loss: 1.093954]\n",
      "epoch:0 step:106 [D loss: 0.051036, acc.: 96.88%] [G loss: 0.953724]\n",
      "epoch:0 step:107 [D loss: 0.053150, acc.: 93.75%] [G loss: 1.042353]\n",
      "epoch:0 step:108 [D loss: 0.060535, acc.: 94.53%] [G loss: 0.995313]\n",
      "epoch:0 step:109 [D loss: 0.056847, acc.: 96.09%] [G loss: 1.230815]\n",
      "epoch:0 step:110 [D loss: 0.074501, acc.: 90.62%] [G loss: 1.052766]\n",
      "epoch:0 step:111 [D loss: 0.114363, acc.: 82.81%] [G loss: 1.054640]\n",
      "epoch:0 step:112 [D loss: 0.212417, acc.: 75.00%] [G loss: 0.741935]\n",
      "epoch:0 step:113 [D loss: 0.201822, acc.: 75.00%] [G loss: 0.848385]\n",
      "epoch:0 step:114 [D loss: 0.077780, acc.: 91.41%] [G loss: 0.952860]\n",
      "epoch:0 step:115 [D loss: 0.058712, acc.: 96.09%] [G loss: 0.861790]\n",
      "epoch:0 step:116 [D loss: 0.065202, acc.: 91.41%] [G loss: 0.960020]\n",
      "epoch:0 step:117 [D loss: 0.045944, acc.: 96.09%] [G loss: 1.087413]\n",
      "epoch:0 step:118 [D loss: 0.034691, acc.: 97.66%] [G loss: 1.059259]\n",
      "epoch:0 step:119 [D loss: 0.039616, acc.: 97.66%] [G loss: 1.002271]\n",
      "epoch:0 step:120 [D loss: 0.040267, acc.: 98.44%] [G loss: 1.055301]\n",
      "epoch:0 step:121 [D loss: 0.031195, acc.: 100.00%] [G loss: 0.994090]\n",
      "epoch:0 step:122 [D loss: 0.045012, acc.: 97.66%] [G loss: 0.937907]\n",
      "epoch:0 step:123 [D loss: 0.062495, acc.: 96.09%] [G loss: 0.982116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:124 [D loss: 0.077606, acc.: 89.84%] [G loss: 1.010274]\n",
      "epoch:0 step:125 [D loss: 0.088967, acc.: 86.72%] [G loss: 0.948462]\n",
      "epoch:0 step:126 [D loss: 0.099102, acc.: 83.59%] [G loss: 0.960249]\n",
      "epoch:0 step:127 [D loss: 0.167791, acc.: 75.00%] [G loss: 0.929278]\n",
      "epoch:0 step:128 [D loss: 0.181054, acc.: 77.34%] [G loss: 1.034270]\n",
      "epoch:0 step:129 [D loss: 0.117866, acc.: 88.28%] [G loss: 1.025499]\n",
      "epoch:0 step:130 [D loss: 0.069222, acc.: 96.88%] [G loss: 1.170825]\n",
      "epoch:0 step:131 [D loss: 0.036045, acc.: 96.88%] [G loss: 1.031993]\n",
      "epoch:0 step:132 [D loss: 0.026651, acc.: 99.22%] [G loss: 1.009367]\n",
      "epoch:0 step:133 [D loss: 0.039974, acc.: 98.44%] [G loss: 0.904996]\n",
      "epoch:0 step:134 [D loss: 0.037087, acc.: 98.44%] [G loss: 0.942154]\n",
      "epoch:0 step:135 [D loss: 0.035425, acc.: 99.22%] [G loss: 0.918650]\n",
      "epoch:0 step:136 [D loss: 0.030655, acc.: 96.09%] [G loss: 1.025735]\n",
      "epoch:0 step:137 [D loss: 0.025510, acc.: 100.00%] [G loss: 0.963492]\n",
      "epoch:0 step:138 [D loss: 0.038987, acc.: 96.88%] [G loss: 1.010492]\n",
      "epoch:0 step:139 [D loss: 0.029035, acc.: 100.00%] [G loss: 0.984400]\n",
      "epoch:0 step:140 [D loss: 0.027602, acc.: 99.22%] [G loss: 0.953161]\n",
      "epoch:0 step:141 [D loss: 0.056095, acc.: 93.75%] [G loss: 1.183045]\n",
      "epoch:0 step:142 [D loss: 0.045950, acc.: 95.31%] [G loss: 0.894137]\n",
      "epoch:0 step:143 [D loss: 0.054858, acc.: 96.09%] [G loss: 0.971548]\n",
      "epoch:0 step:144 [D loss: 0.075662, acc.: 97.66%] [G loss: 0.859112]\n",
      "epoch:0 step:145 [D loss: 0.055643, acc.: 97.66%] [G loss: 1.072026]\n",
      "epoch:0 step:146 [D loss: 0.092968, acc.: 93.75%] [G loss: 0.778325]\n",
      "epoch:0 step:147 [D loss: 0.079044, acc.: 96.88%] [G loss: 1.106819]\n",
      "epoch:0 step:148 [D loss: 0.071790, acc.: 96.09%] [G loss: 0.973996]\n",
      "epoch:0 step:149 [D loss: 0.070721, acc.: 93.75%] [G loss: 0.869925]\n",
      "epoch:0 step:150 [D loss: 0.100586, acc.: 83.59%] [G loss: 0.927893]\n",
      "epoch:0 step:151 [D loss: 0.150066, acc.: 75.78%] [G loss: 1.070894]\n",
      "epoch:0 step:152 [D loss: 0.191472, acc.: 77.34%] [G loss: 1.503310]\n",
      "epoch:0 step:153 [D loss: 0.187987, acc.: 73.44%] [G loss: 0.864238]\n",
      "epoch:0 step:154 [D loss: 0.143418, acc.: 88.28%] [G loss: 0.947392]\n",
      "epoch:0 step:155 [D loss: 0.078766, acc.: 94.53%] [G loss: 0.950434]\n",
      "epoch:0 step:156 [D loss: 0.046612, acc.: 96.09%] [G loss: 1.090127]\n",
      "epoch:0 step:157 [D loss: 0.039284, acc.: 96.88%] [G loss: 1.052889]\n",
      "epoch:0 step:158 [D loss: 0.026506, acc.: 99.22%] [G loss: 0.921778]\n",
      "epoch:0 step:159 [D loss: 0.027989, acc.: 100.00%] [G loss: 0.957263]\n",
      "epoch:0 step:160 [D loss: 0.044794, acc.: 97.66%] [G loss: 1.041935]\n",
      "epoch:0 step:161 [D loss: 0.034475, acc.: 99.22%] [G loss: 0.985074]\n",
      "epoch:0 step:162 [D loss: 0.040967, acc.: 97.66%] [G loss: 0.963006]\n",
      "epoch:0 step:163 [D loss: 0.023724, acc.: 100.00%] [G loss: 0.978307]\n",
      "epoch:0 step:164 [D loss: 0.029807, acc.: 98.44%] [G loss: 0.996317]\n",
      "epoch:0 step:165 [D loss: 0.024124, acc.: 100.00%] [G loss: 0.903315]\n",
      "epoch:0 step:166 [D loss: 0.025994, acc.: 99.22%] [G loss: 0.939156]\n",
      "epoch:0 step:167 [D loss: 0.030252, acc.: 100.00%] [G loss: 0.936836]\n",
      "epoch:0 step:168 [D loss: 0.027196, acc.: 98.44%] [G loss: 1.070158]\n",
      "epoch:0 step:169 [D loss: 0.026281, acc.: 99.22%] [G loss: 1.020494]\n",
      "epoch:0 step:170 [D loss: 0.029210, acc.: 97.66%] [G loss: 0.890419]\n",
      "epoch:0 step:171 [D loss: 0.037540, acc.: 95.31%] [G loss: 1.037611]\n",
      "epoch:0 step:172 [D loss: 0.023170, acc.: 100.00%] [G loss: 0.897179]\n",
      "epoch:0 step:173 [D loss: 0.024379, acc.: 99.22%] [G loss: 0.988082]\n",
      "epoch:0 step:174 [D loss: 0.035553, acc.: 99.22%] [G loss: 0.957510]\n",
      "epoch:0 step:175 [D loss: 0.029095, acc.: 98.44%] [G loss: 0.948963]\n",
      "epoch:0 step:176 [D loss: 0.038209, acc.: 96.88%] [G loss: 1.005572]\n",
      "epoch:0 step:177 [D loss: 0.028649, acc.: 99.22%] [G loss: 1.003381]\n",
      "epoch:0 step:178 [D loss: 0.022709, acc.: 100.00%] [G loss: 1.028142]\n",
      "epoch:0 step:179 [D loss: 0.042226, acc.: 98.44%] [G loss: 0.969539]\n",
      "epoch:0 step:180 [D loss: 0.027032, acc.: 100.00%] [G loss: 0.946448]\n",
      "epoch:0 step:181 [D loss: 0.025877, acc.: 100.00%] [G loss: 0.958062]\n",
      "epoch:0 step:182 [D loss: 0.034797, acc.: 100.00%] [G loss: 0.794131]\n",
      "epoch:0 step:183 [D loss: 0.028963, acc.: 99.22%] [G loss: 1.034219]\n",
      "epoch:0 step:184 [D loss: 0.058500, acc.: 95.31%] [G loss: 0.852996]\n",
      "epoch:0 step:185 [D loss: 0.028967, acc.: 99.22%] [G loss: 1.084252]\n",
      "epoch:0 step:186 [D loss: 0.041017, acc.: 98.44%] [G loss: 0.897705]\n",
      "epoch:0 step:187 [D loss: 0.040671, acc.: 100.00%] [G loss: 1.068650]\n",
      "epoch:0 step:188 [D loss: 0.089633, acc.: 91.41%] [G loss: 0.882903]\n",
      "epoch:0 step:189 [D loss: 0.079511, acc.: 92.19%] [G loss: 1.078803]\n",
      "epoch:0 step:190 [D loss: 0.125508, acc.: 81.25%] [G loss: 0.901844]\n",
      "epoch:0 step:191 [D loss: 0.134370, acc.: 79.69%] [G loss: 0.910013]\n",
      "epoch:0 step:192 [D loss: 0.160556, acc.: 72.66%] [G loss: 0.891836]\n",
      "epoch:0 step:193 [D loss: 0.110284, acc.: 84.38%] [G loss: 0.753323]\n",
      "epoch:0 step:194 [D loss: 0.053615, acc.: 96.88%] [G loss: 1.046979]\n",
      "epoch:0 step:195 [D loss: 0.057349, acc.: 96.09%] [G loss: 0.957844]\n",
      "epoch:0 step:196 [D loss: 0.027689, acc.: 100.00%] [G loss: 1.009354]\n",
      "epoch:0 step:197 [D loss: 0.071838, acc.: 93.75%] [G loss: 0.791157]\n",
      "epoch:0 step:198 [D loss: 0.047385, acc.: 98.44%] [G loss: 1.080748]\n",
      "epoch:0 step:199 [D loss: 0.088621, acc.: 92.97%] [G loss: 0.866321]\n",
      "epoch:0 step:200 [D loss: 0.063301, acc.: 97.66%] [G loss: 1.109219]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "WARNING:tensorflow:From /home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:185: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:208: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute score in space: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ot/lp/__init__.py:211: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute score in space: 1\n",
      "IS socre: 1.237855\n",
      "FID: 275.825256\n",
      "0 = 30.561240275192354\n",
      "1 = 0.6210724928954968\n",
      "2 = 1.0\n",
      "3 = 1.0\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 1.0\n",
      "7 = 16.808647215414116\n",
      "8 = 0.26737623185725135\n",
      "9 = 0.9991999864578247\n",
      "10 = 0.9983999729156494\n",
      "11 = 1.0\n",
      "12 = 1.0\n",
      "13 = 0.9983999729156494\n",
      "14 = 1.2378555536270142\n",
      "15 = 4.6223859786987305\n",
      "16 = 0.9252229928970337\n",
      "17 = 1.237855076789856\n",
      "18 = 275.82525634765625\n",
      "epoch:0 step:201 [D loss: 0.084527, acc.: 94.53%] [G loss: 0.816879]\n",
      "epoch:0 step:202 [D loss: 0.046304, acc.: 98.44%] [G loss: 0.940835]\n",
      "epoch:0 step:203 [D loss: 0.091177, acc.: 89.84%] [G loss: 0.985664]\n",
      "epoch:0 step:204 [D loss: 0.038789, acc.: 96.88%] [G loss: 1.028707]\n",
      "epoch:0 step:205 [D loss: 0.063445, acc.: 92.19%] [G loss: 0.876093]\n",
      "epoch:0 step:206 [D loss: 0.026743, acc.: 100.00%] [G loss: 1.066087]\n",
      "epoch:0 step:207 [D loss: 0.068833, acc.: 92.97%] [G loss: 0.850119]\n",
      "epoch:0 step:208 [D loss: 0.039297, acc.: 97.66%] [G loss: 0.936284]\n",
      "epoch:0 step:209 [D loss: 0.050810, acc.: 96.88%] [G loss: 0.894319]\n",
      "epoch:0 step:210 [D loss: 0.035179, acc.: 96.88%] [G loss: 1.036655]\n",
      "epoch:0 step:211 [D loss: 0.037246, acc.: 99.22%] [G loss: 1.010460]\n",
      "epoch:0 step:212 [D loss: 0.031883, acc.: 97.66%] [G loss: 0.990669]\n",
      "epoch:0 step:213 [D loss: 0.032997, acc.: 98.44%] [G loss: 0.847535]\n",
      "epoch:0 step:214 [D loss: 0.050472, acc.: 96.09%] [G loss: 0.887110]\n",
      "epoch:0 step:215 [D loss: 0.025025, acc.: 99.22%] [G loss: 0.927275]\n",
      "epoch:0 step:216 [D loss: 0.063511, acc.: 92.97%] [G loss: 0.871137]\n",
      "epoch:0 step:217 [D loss: 0.040447, acc.: 98.44%] [G loss: 0.999911]\n",
      "epoch:0 step:218 [D loss: 0.092854, acc.: 92.97%] [G loss: 0.832709]\n",
      "epoch:0 step:219 [D loss: 0.081075, acc.: 92.19%] [G loss: 0.968206]\n",
      "epoch:0 step:220 [D loss: 0.197320, acc.: 75.00%] [G loss: 0.623571]\n",
      "epoch:0 step:221 [D loss: 0.209960, acc.: 64.84%] [G loss: 1.166160]\n",
      "epoch:0 step:222 [D loss: 0.373991, acc.: 30.47%] [G loss: 0.544512]\n",
      "epoch:0 step:223 [D loss: 0.169418, acc.: 75.00%] [G loss: 0.999884]\n",
      "epoch:0 step:224 [D loss: 0.142321, acc.: 85.16%] [G loss: 0.829771]\n",
      "epoch:0 step:225 [D loss: 0.052794, acc.: 97.66%] [G loss: 0.813217]\n",
      "epoch:0 step:226 [D loss: 0.047428, acc.: 95.31%] [G loss: 0.888275]\n",
      "epoch:0 step:227 [D loss: 0.049838, acc.: 92.19%] [G loss: 0.865586]\n",
      "epoch:0 step:228 [D loss: 0.030945, acc.: 97.66%] [G loss: 0.884598]\n",
      "epoch:0 step:229 [D loss: 0.058714, acc.: 93.75%] [G loss: 0.876058]\n",
      "epoch:0 step:230 [D loss: 0.051771, acc.: 92.97%] [G loss: 0.860482]\n",
      "epoch:0 step:231 [D loss: 0.052649, acc.: 96.09%] [G loss: 0.813139]\n",
      "epoch:0 step:232 [D loss: 0.042756, acc.: 96.88%] [G loss: 0.934007]\n",
      "epoch:0 step:233 [D loss: 0.055192, acc.: 92.97%] [G loss: 0.959950]\n",
      "epoch:0 step:234 [D loss: 0.045787, acc.: 95.31%] [G loss: 0.968282]\n",
      "epoch:0 step:235 [D loss: 0.037605, acc.: 96.09%] [G loss: 0.904366]\n",
      "epoch:0 step:236 [D loss: 0.045952, acc.: 95.31%] [G loss: 0.888816]\n",
      "epoch:0 step:237 [D loss: 0.051757, acc.: 93.75%] [G loss: 0.876938]\n",
      "epoch:0 step:238 [D loss: 0.042610, acc.: 96.09%] [G loss: 0.946790]\n",
      "epoch:0 step:239 [D loss: 0.057342, acc.: 94.53%] [G loss: 0.929330]\n",
      "epoch:0 step:240 [D loss: 0.042979, acc.: 96.88%] [G loss: 1.120241]\n",
      "epoch:0 step:241 [D loss: 0.081937, acc.: 90.62%] [G loss: 0.843132]\n",
      "epoch:0 step:242 [D loss: 0.036975, acc.: 96.88%] [G loss: 0.922637]\n",
      "epoch:0 step:243 [D loss: 0.099387, acc.: 89.06%] [G loss: 0.696645]\n",
      "epoch:0 step:244 [D loss: 0.042478, acc.: 96.09%] [G loss: 0.997655]\n",
      "epoch:0 step:245 [D loss: 0.086808, acc.: 89.84%] [G loss: 0.728057]\n",
      "epoch:0 step:246 [D loss: 0.056143, acc.: 94.53%] [G loss: 0.912198]\n",
      "epoch:0 step:247 [D loss: 0.076560, acc.: 91.41%] [G loss: 0.917051]\n",
      "epoch:0 step:248 [D loss: 0.064425, acc.: 93.75%] [G loss: 0.851510]\n",
      "epoch:0 step:249 [D loss: 0.060866, acc.: 90.62%] [G loss: 0.982101]\n",
      "epoch:0 step:250 [D loss: 0.101497, acc.: 89.06%] [G loss: 0.858004]\n",
      "epoch:0 step:251 [D loss: 0.049737, acc.: 97.66%] [G loss: 1.084303]\n",
      "epoch:0 step:252 [D loss: 0.144720, acc.: 79.69%] [G loss: 0.731290]\n",
      "epoch:0 step:253 [D loss: 0.089945, acc.: 96.88%] [G loss: 1.027300]\n",
      "epoch:0 step:254 [D loss: 0.182041, acc.: 77.34%] [G loss: 0.787312]\n",
      "epoch:0 step:255 [D loss: 0.060227, acc.: 96.09%] [G loss: 0.989261]\n",
      "epoch:0 step:256 [D loss: 0.100150, acc.: 89.84%] [G loss: 0.714013]\n",
      "epoch:0 step:257 [D loss: 0.042883, acc.: 96.88%] [G loss: 0.964612]\n",
      "epoch:0 step:258 [D loss: 0.056320, acc.: 96.09%] [G loss: 0.819040]\n",
      "epoch:0 step:259 [D loss: 0.032240, acc.: 97.66%] [G loss: 0.953989]\n",
      "epoch:0 step:260 [D loss: 0.066924, acc.: 90.62%] [G loss: 0.942315]\n",
      "epoch:0 step:261 [D loss: 0.079737, acc.: 90.62%] [G loss: 0.825517]\n",
      "epoch:0 step:262 [D loss: 0.062031, acc.: 91.41%] [G loss: 0.927737]\n",
      "epoch:0 step:263 [D loss: 0.087694, acc.: 91.41%] [G loss: 0.885804]\n",
      "epoch:0 step:264 [D loss: 0.053658, acc.: 94.53%] [G loss: 0.799668]\n",
      "epoch:0 step:265 [D loss: 0.081063, acc.: 89.06%] [G loss: 0.810403]\n",
      "epoch:0 step:266 [D loss: 0.066305, acc.: 92.97%] [G loss: 0.847320]\n",
      "epoch:0 step:267 [D loss: 0.051204, acc.: 92.97%] [G loss: 1.014683]\n",
      "epoch:0 step:268 [D loss: 0.072536, acc.: 90.62%] [G loss: 0.800103]\n",
      "epoch:0 step:269 [D loss: 0.048815, acc.: 92.19%] [G loss: 0.886084]\n",
      "epoch:0 step:270 [D loss: 0.075587, acc.: 94.53%] [G loss: 0.730491]\n",
      "epoch:0 step:271 [D loss: 0.073068, acc.: 90.62%] [G loss: 0.899972]\n",
      "epoch:0 step:272 [D loss: 0.120099, acc.: 89.84%] [G loss: 0.728658]\n",
      "epoch:0 step:273 [D loss: 0.092447, acc.: 89.06%] [G loss: 0.848765]\n",
      "epoch:0 step:274 [D loss: 0.246184, acc.: 61.72%] [G loss: 0.562849]\n",
      "epoch:0 step:275 [D loss: 0.117759, acc.: 89.06%] [G loss: 1.110510]\n",
      "epoch:0 step:276 [D loss: 0.271535, acc.: 48.44%] [G loss: 0.648998]\n",
      "epoch:0 step:277 [D loss: 0.082807, acc.: 92.97%] [G loss: 1.139280]\n",
      "epoch:0 step:278 [D loss: 0.112068, acc.: 90.62%] [G loss: 0.739682]\n",
      "epoch:0 step:279 [D loss: 0.079046, acc.: 89.06%] [G loss: 0.854210]\n",
      "epoch:0 step:280 [D loss: 0.067248, acc.: 93.75%] [G loss: 0.806118]\n",
      "epoch:0 step:281 [D loss: 0.082150, acc.: 89.06%] [G loss: 0.951524]\n",
      "epoch:0 step:282 [D loss: 0.058678, acc.: 90.62%] [G loss: 0.953027]\n",
      "epoch:0 step:283 [D loss: 0.098562, acc.: 83.59%] [G loss: 0.751175]\n",
      "epoch:0 step:284 [D loss: 0.047967, acc.: 94.53%] [G loss: 0.785233]\n",
      "epoch:0 step:285 [D loss: 0.064669, acc.: 91.41%] [G loss: 0.865603]\n",
      "epoch:0 step:286 [D loss: 0.059444, acc.: 91.41%] [G loss: 0.845502]\n",
      "epoch:0 step:287 [D loss: 0.061867, acc.: 92.97%] [G loss: 0.876439]\n",
      "epoch:0 step:288 [D loss: 0.077289, acc.: 86.72%] [G loss: 1.050772]\n",
      "epoch:0 step:289 [D loss: 0.072288, acc.: 89.84%] [G loss: 0.886010]\n",
      "epoch:0 step:290 [D loss: 0.064500, acc.: 93.75%] [G loss: 0.733409]\n",
      "epoch:0 step:291 [D loss: 0.097075, acc.: 86.72%] [G loss: 0.854537]\n",
      "epoch:0 step:292 [D loss: 0.133409, acc.: 81.25%] [G loss: 0.887684]\n",
      "epoch:0 step:293 [D loss: 0.083596, acc.: 89.06%] [G loss: 0.832954]\n",
      "epoch:0 step:294 [D loss: 0.093895, acc.: 87.50%] [G loss: 0.823739]\n",
      "epoch:0 step:295 [D loss: 0.091709, acc.: 85.16%] [G loss: 0.920230]\n",
      "epoch:0 step:296 [D loss: 0.098085, acc.: 86.72%] [G loss: 0.822348]\n",
      "epoch:0 step:297 [D loss: 0.088152, acc.: 88.28%] [G loss: 0.748419]\n",
      "epoch:0 step:298 [D loss: 0.100514, acc.: 85.94%] [G loss: 0.853126]\n",
      "epoch:0 step:299 [D loss: 0.061544, acc.: 92.19%] [G loss: 1.025087]\n",
      "epoch:0 step:300 [D loss: 0.132829, acc.: 78.12%] [G loss: 0.924981]\n",
      "epoch:0 step:301 [D loss: 0.123003, acc.: 83.59%] [G loss: 0.827006]\n",
      "epoch:0 step:302 [D loss: 0.061135, acc.: 93.75%] [G loss: 1.040077]\n",
      "epoch:0 step:303 [D loss: 0.234936, acc.: 69.53%] [G loss: 0.676424]\n",
      "epoch:0 step:304 [D loss: 0.068521, acc.: 94.53%] [G loss: 1.047245]\n",
      "epoch:0 step:305 [D loss: 0.222636, acc.: 55.47%] [G loss: 0.580424]\n",
      "epoch:0 step:306 [D loss: 0.085302, acc.: 88.28%] [G loss: 0.858465]\n",
      "epoch:0 step:307 [D loss: 0.118276, acc.: 87.50%] [G loss: 0.714245]\n",
      "epoch:0 step:308 [D loss: 0.100587, acc.: 81.25%] [G loss: 0.772407]\n",
      "epoch:0 step:309 [D loss: 0.090819, acc.: 89.84%] [G loss: 0.815215]\n",
      "epoch:0 step:310 [D loss: 0.064972, acc.: 90.62%] [G loss: 0.900766]\n",
      "epoch:0 step:311 [D loss: 0.058381, acc.: 92.19%] [G loss: 0.901275]\n",
      "epoch:0 step:312 [D loss: 0.158407, acc.: 76.56%] [G loss: 0.690827]\n",
      "epoch:0 step:313 [D loss: 0.062371, acc.: 92.19%] [G loss: 1.046070]\n",
      "epoch:0 step:314 [D loss: 0.173230, acc.: 80.47%] [G loss: 0.662244]\n",
      "epoch:0 step:315 [D loss: 0.068985, acc.: 95.31%] [G loss: 1.348020]\n",
      "epoch:0 step:316 [D loss: 0.241480, acc.: 67.97%] [G loss: 0.679123]\n",
      "epoch:0 step:317 [D loss: 0.132435, acc.: 82.81%] [G loss: 0.859006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:318 [D loss: 0.154389, acc.: 78.12%] [G loss: 0.695889]\n",
      "epoch:0 step:319 [D loss: 0.064999, acc.: 91.41%] [G loss: 0.725940]\n",
      "epoch:0 step:320 [D loss: 0.057689, acc.: 92.97%] [G loss: 0.931328]\n",
      "epoch:0 step:321 [D loss: 0.066772, acc.: 88.28%] [G loss: 0.800298]\n",
      "epoch:0 step:322 [D loss: 0.071248, acc.: 90.62%] [G loss: 0.768811]\n",
      "epoch:0 step:323 [D loss: 0.067763, acc.: 90.62%] [G loss: 0.891265]\n",
      "epoch:0 step:324 [D loss: 0.101213, acc.: 83.59%] [G loss: 0.805159]\n",
      "epoch:0 step:325 [D loss: 0.071669, acc.: 85.16%] [G loss: 0.839464]\n",
      "epoch:0 step:326 [D loss: 0.137002, acc.: 82.03%] [G loss: 0.758742]\n",
      "epoch:0 step:327 [D loss: 0.071036, acc.: 90.62%] [G loss: 0.989705]\n",
      "epoch:0 step:328 [D loss: 0.218689, acc.: 64.84%] [G loss: 0.625324]\n",
      "epoch:0 step:329 [D loss: 0.058937, acc.: 89.84%] [G loss: 1.027147]\n",
      "epoch:0 step:330 [D loss: 0.111896, acc.: 86.72%] [G loss: 0.824517]\n",
      "epoch:0 step:331 [D loss: 0.077135, acc.: 85.16%] [G loss: 0.804181]\n",
      "epoch:0 step:332 [D loss: 0.077414, acc.: 90.62%] [G loss: 0.823929]\n",
      "epoch:0 step:333 [D loss: 0.089941, acc.: 86.72%] [G loss: 0.871664]\n",
      "epoch:0 step:334 [D loss: 0.080575, acc.: 86.72%] [G loss: 0.997876]\n",
      "epoch:0 step:335 [D loss: 0.095512, acc.: 85.16%] [G loss: 0.793576]\n",
      "epoch:0 step:336 [D loss: 0.059408, acc.: 98.44%] [G loss: 0.879621]\n",
      "epoch:0 step:337 [D loss: 0.078480, acc.: 89.06%] [G loss: 0.808550]\n",
      "epoch:0 step:338 [D loss: 0.067142, acc.: 88.28%] [G loss: 1.065990]\n",
      "epoch:0 step:339 [D loss: 0.131118, acc.: 79.69%] [G loss: 0.813717]\n",
      "epoch:0 step:340 [D loss: 0.033628, acc.: 100.00%] [G loss: 0.856429]\n",
      "epoch:0 step:341 [D loss: 0.150132, acc.: 75.78%] [G loss: 0.672315]\n",
      "epoch:0 step:342 [D loss: 0.050033, acc.: 91.41%] [G loss: 0.993527]\n",
      "epoch:0 step:343 [D loss: 0.099547, acc.: 89.84%] [G loss: 0.848616]\n",
      "epoch:0 step:344 [D loss: 0.057657, acc.: 95.31%] [G loss: 1.027322]\n",
      "epoch:0 step:345 [D loss: 0.182299, acc.: 72.66%] [G loss: 0.674628]\n",
      "epoch:0 step:346 [D loss: 0.061423, acc.: 91.41%] [G loss: 1.003202]\n",
      "epoch:0 step:347 [D loss: 0.124716, acc.: 82.03%] [G loss: 0.739497]\n",
      "epoch:0 step:348 [D loss: 0.077550, acc.: 85.94%] [G loss: 0.880523]\n",
      "epoch:0 step:349 [D loss: 0.121212, acc.: 83.59%] [G loss: 0.888664]\n",
      "epoch:0 step:350 [D loss: 0.057758, acc.: 92.97%] [G loss: 0.819975]\n",
      "epoch:0 step:351 [D loss: 0.100127, acc.: 83.59%] [G loss: 0.848083]\n",
      "epoch:0 step:352 [D loss: 0.075186, acc.: 90.62%] [G loss: 0.952356]\n",
      "epoch:0 step:353 [D loss: 0.072058, acc.: 88.28%] [G loss: 0.810906]\n",
      "epoch:0 step:354 [D loss: 0.099648, acc.: 86.72%] [G loss: 0.799080]\n",
      "epoch:0 step:355 [D loss: 0.078811, acc.: 84.38%] [G loss: 0.837375]\n",
      "epoch:0 step:356 [D loss: 0.102077, acc.: 86.72%] [G loss: 0.831604]\n",
      "epoch:0 step:357 [D loss: 0.063561, acc.: 89.06%] [G loss: 0.846229]\n",
      "epoch:0 step:358 [D loss: 0.077352, acc.: 91.41%] [G loss: 0.934377]\n",
      "epoch:0 step:359 [D loss: 0.061842, acc.: 92.19%] [G loss: 0.851207]\n",
      "epoch:0 step:360 [D loss: 0.090374, acc.: 82.81%] [G loss: 0.856738]\n",
      "epoch:0 step:361 [D loss: 0.081601, acc.: 86.72%] [G loss: 0.985089]\n",
      "epoch:0 step:362 [D loss: 0.139293, acc.: 79.69%] [G loss: 0.842288]\n",
      "epoch:0 step:363 [D loss: 0.040648, acc.: 95.31%] [G loss: 0.985067]\n",
      "epoch:0 step:364 [D loss: 0.121780, acc.: 77.34%] [G loss: 0.700717]\n",
      "epoch:0 step:365 [D loss: 0.076225, acc.: 89.84%] [G loss: 0.898894]\n",
      "epoch:0 step:366 [D loss: 0.070364, acc.: 89.84%] [G loss: 0.800061]\n",
      "epoch:0 step:367 [D loss: 0.100280, acc.: 81.25%] [G loss: 0.817033]\n",
      "epoch:0 step:368 [D loss: 0.083207, acc.: 92.97%] [G loss: 0.910975]\n",
      "epoch:0 step:369 [D loss: 0.069540, acc.: 87.50%] [G loss: 0.874741]\n",
      "epoch:0 step:370 [D loss: 0.126588, acc.: 78.91%] [G loss: 0.857201]\n",
      "epoch:0 step:371 [D loss: 0.086958, acc.: 89.06%] [G loss: 0.861943]\n",
      "epoch:0 step:372 [D loss: 0.083296, acc.: 86.72%] [G loss: 0.944947]\n",
      "epoch:0 step:373 [D loss: 0.078237, acc.: 89.84%] [G loss: 0.896789]\n",
      "epoch:0 step:374 [D loss: 0.109646, acc.: 83.59%] [G loss: 0.932653]\n",
      "epoch:0 step:375 [D loss: 0.078954, acc.: 85.94%] [G loss: 0.896540]\n",
      "epoch:0 step:376 [D loss: 0.083186, acc.: 87.50%] [G loss: 0.997965]\n",
      "epoch:0 step:377 [D loss: 0.064924, acc.: 91.41%] [G loss: 0.917545]\n",
      "epoch:0 step:378 [D loss: 0.106897, acc.: 83.59%] [G loss: 0.934823]\n",
      "epoch:0 step:379 [D loss: 0.096016, acc.: 83.59%] [G loss: 0.918380]\n",
      "epoch:0 step:380 [D loss: 0.089845, acc.: 84.38%] [G loss: 0.862557]\n",
      "epoch:0 step:381 [D loss: 0.062535, acc.: 90.62%] [G loss: 0.856279]\n",
      "epoch:0 step:382 [D loss: 0.126081, acc.: 80.47%] [G loss: 0.947313]\n",
      "epoch:0 step:383 [D loss: 0.071115, acc.: 89.06%] [G loss: 0.792417]\n",
      "epoch:0 step:384 [D loss: 0.120895, acc.: 85.16%] [G loss: 0.874341]\n",
      "epoch:0 step:385 [D loss: 0.061928, acc.: 89.84%] [G loss: 0.960247]\n",
      "epoch:0 step:386 [D loss: 0.080086, acc.: 87.50%] [G loss: 0.819473]\n",
      "epoch:0 step:387 [D loss: 0.057983, acc.: 94.53%] [G loss: 1.123650]\n",
      "epoch:0 step:388 [D loss: 0.084451, acc.: 87.50%] [G loss: 0.943211]\n",
      "epoch:0 step:389 [D loss: 0.054331, acc.: 95.31%] [G loss: 0.956253]\n",
      "epoch:0 step:390 [D loss: 0.084842, acc.: 84.38%] [G loss: 1.013950]\n",
      "epoch:0 step:391 [D loss: 0.153264, acc.: 76.56%] [G loss: 0.909983]\n",
      "epoch:0 step:392 [D loss: 0.071167, acc.: 89.84%] [G loss: 1.019569]\n",
      "epoch:0 step:393 [D loss: 0.201723, acc.: 67.97%] [G loss: 1.016364]\n",
      "epoch:0 step:394 [D loss: 0.071220, acc.: 92.97%] [G loss: 0.964806]\n",
      "epoch:0 step:395 [D loss: 0.141338, acc.: 79.69%] [G loss: 0.775615]\n",
      "epoch:0 step:396 [D loss: 0.071932, acc.: 89.06%] [G loss: 1.073605]\n",
      "epoch:0 step:397 [D loss: 0.114567, acc.: 81.25%] [G loss: 0.668795]\n",
      "epoch:0 step:398 [D loss: 0.094332, acc.: 82.81%] [G loss: 1.224084]\n",
      "epoch:0 step:399 [D loss: 0.183224, acc.: 69.53%] [G loss: 0.718144]\n",
      "epoch:0 step:400 [D loss: 0.141475, acc.: 73.44%] [G loss: 1.051171]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 1.339740\n",
      "FID: 278.975677\n",
      "0 = 24.412776180935012\n",
      "1 = 0.4638426442931438\n",
      "2 = 1.0\n",
      "3 = 1.0\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 1.0\n",
      "7 = 16.89488394215109\n",
      "8 = 0.26061071160025917\n",
      "9 = 0.9994000196456909\n",
      "10 = 0.9987999796867371\n",
      "11 = 1.0\n",
      "12 = 1.0\n",
      "13 = 0.9987999796867371\n",
      "14 = 1.3397390842437744\n",
      "15 = 4.991701126098633\n",
      "16 = 0.8640685081481934\n",
      "17 = 1.3397396802902222\n",
      "18 = 278.9756774902344\n",
      "epoch:0 step:401 [D loss: 0.211191, acc.: 65.62%] [G loss: 0.572027]\n",
      "epoch:0 step:402 [D loss: 0.080819, acc.: 84.38%] [G loss: 0.897133]\n",
      "epoch:0 step:403 [D loss: 0.191188, acc.: 72.66%] [G loss: 1.000347]\n",
      "epoch:0 step:404 [D loss: 0.062727, acc.: 89.06%] [G loss: 0.882849]\n",
      "epoch:0 step:405 [D loss: 0.074827, acc.: 89.84%] [G loss: 0.997406]\n",
      "epoch:0 step:406 [D loss: 0.115911, acc.: 82.03%] [G loss: 0.870586]\n",
      "epoch:0 step:407 [D loss: 0.064890, acc.: 89.06%] [G loss: 1.053138]\n",
      "epoch:0 step:408 [D loss: 0.115465, acc.: 81.25%] [G loss: 0.890594]\n",
      "epoch:0 step:409 [D loss: 0.054752, acc.: 94.53%] [G loss: 0.950327]\n",
      "epoch:0 step:410 [D loss: 0.094090, acc.: 84.38%] [G loss: 0.857864]\n",
      "epoch:0 step:411 [D loss: 0.108079, acc.: 81.25%] [G loss: 0.945579]\n",
      "epoch:0 step:412 [D loss: 0.113737, acc.: 82.81%] [G loss: 0.955616]\n",
      "epoch:0 step:413 [D loss: 0.067328, acc.: 90.62%] [G loss: 0.931982]\n",
      "epoch:0 step:414 [D loss: 0.069125, acc.: 89.84%] [G loss: 1.001797]\n",
      "epoch:0 step:415 [D loss: 0.139206, acc.: 77.34%] [G loss: 0.849911]\n",
      "epoch:0 step:416 [D loss: 0.066770, acc.: 91.41%] [G loss: 0.997340]\n",
      "epoch:0 step:417 [D loss: 0.111137, acc.: 83.59%] [G loss: 0.839981]\n",
      "epoch:0 step:418 [D loss: 0.058894, acc.: 92.19%] [G loss: 0.905761]\n",
      "epoch:0 step:419 [D loss: 0.096596, acc.: 86.72%] [G loss: 1.002459]\n",
      "epoch:0 step:420 [D loss: 0.107068, acc.: 78.12%] [G loss: 1.020704]\n",
      "epoch:0 step:421 [D loss: 0.140327, acc.: 78.91%] [G loss: 0.889952]\n",
      "epoch:0 step:422 [D loss: 0.068028, acc.: 90.62%] [G loss: 1.260852]\n",
      "epoch:0 step:423 [D loss: 0.169219, acc.: 77.34%] [G loss: 0.653698]\n",
      "epoch:0 step:424 [D loss: 0.075451, acc.: 89.84%] [G loss: 1.044072]\n",
      "epoch:0 step:425 [D loss: 0.194927, acc.: 70.31%] [G loss: 0.757348]\n",
      "epoch:0 step:426 [D loss: 0.039866, acc.: 98.44%] [G loss: 1.067010]\n",
      "epoch:0 step:427 [D loss: 0.145738, acc.: 81.25%] [G loss: 0.830394]\n",
      "epoch:0 step:428 [D loss: 0.051044, acc.: 93.75%] [G loss: 1.078493]\n",
      "epoch:0 step:429 [D loss: 0.127072, acc.: 78.91%] [G loss: 0.849968]\n",
      "epoch:0 step:430 [D loss: 0.091525, acc.: 89.84%] [G loss: 0.847898]\n",
      "epoch:0 step:431 [D loss: 0.060509, acc.: 92.97%] [G loss: 0.996566]\n",
      "epoch:0 step:432 [D loss: 0.101131, acc.: 86.72%] [G loss: 0.787567]\n",
      "epoch:0 step:433 [D loss: 0.069017, acc.: 89.84%] [G loss: 1.012621]\n",
      "epoch:0 step:434 [D loss: 0.171648, acc.: 76.56%] [G loss: 0.783307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:435 [D loss: 0.065694, acc.: 89.06%] [G loss: 1.083408]\n",
      "epoch:0 step:436 [D loss: 0.126517, acc.: 85.94%] [G loss: 0.793138]\n",
      "epoch:0 step:437 [D loss: 0.061252, acc.: 91.41%] [G loss: 1.009714]\n",
      "epoch:0 step:438 [D loss: 0.108010, acc.: 82.81%] [G loss: 0.813004]\n",
      "epoch:0 step:439 [D loss: 0.067589, acc.: 92.19%] [G loss: 0.966122]\n",
      "epoch:0 step:440 [D loss: 0.122785, acc.: 80.47%] [G loss: 0.905665]\n",
      "epoch:0 step:441 [D loss: 0.096180, acc.: 81.25%] [G loss: 0.877259]\n",
      "epoch:0 step:442 [D loss: 0.114685, acc.: 84.38%] [G loss: 0.804679]\n",
      "epoch:0 step:443 [D loss: 0.064891, acc.: 90.62%] [G loss: 0.925650]\n",
      "epoch:0 step:444 [D loss: 0.112968, acc.: 82.03%] [G loss: 0.903855]\n",
      "epoch:0 step:445 [D loss: 0.104493, acc.: 82.81%] [G loss: 1.051892]\n",
      "epoch:0 step:446 [D loss: 0.114359, acc.: 82.03%] [G loss: 0.772443]\n",
      "epoch:0 step:447 [D loss: 0.045334, acc.: 100.00%] [G loss: 1.059622]\n",
      "epoch:0 step:448 [D loss: 0.212022, acc.: 67.19%] [G loss: 0.651834]\n",
      "epoch:0 step:449 [D loss: 0.071565, acc.: 90.62%] [G loss: 0.932893]\n",
      "epoch:0 step:450 [D loss: 0.113095, acc.: 84.38%] [G loss: 0.821464]\n",
      "epoch:0 step:451 [D loss: 0.085340, acc.: 88.28%] [G loss: 0.977257]\n",
      "epoch:0 step:452 [D loss: 0.071020, acc.: 92.97%] [G loss: 0.982864]\n",
      "epoch:0 step:453 [D loss: 0.083911, acc.: 88.28%] [G loss: 0.893679]\n",
      "epoch:0 step:454 [D loss: 0.074496, acc.: 92.97%] [G loss: 1.055803]\n",
      "epoch:0 step:455 [D loss: 0.110953, acc.: 82.81%] [G loss: 0.921397]\n",
      "epoch:0 step:456 [D loss: 0.078915, acc.: 91.41%] [G loss: 0.964642]\n",
      "epoch:0 step:457 [D loss: 0.096223, acc.: 82.81%] [G loss: 0.994200]\n",
      "epoch:0 step:458 [D loss: 0.066024, acc.: 94.53%] [G loss: 0.944705]\n",
      "epoch:0 step:459 [D loss: 0.120365, acc.: 82.03%] [G loss: 0.967608]\n",
      "epoch:0 step:460 [D loss: 0.064357, acc.: 92.19%] [G loss: 1.125295]\n",
      "epoch:0 step:461 [D loss: 0.094756, acc.: 86.72%] [G loss: 0.909244]\n",
      "epoch:0 step:462 [D loss: 0.106683, acc.: 81.25%] [G loss: 0.964982]\n",
      "epoch:0 step:463 [D loss: 0.101097, acc.: 92.19%] [G loss: 0.823661]\n",
      "epoch:0 step:464 [D loss: 0.082089, acc.: 87.50%] [G loss: 1.190544]\n",
      "epoch:0 step:465 [D loss: 0.149356, acc.: 82.03%] [G loss: 0.747002]\n",
      "epoch:0 step:466 [D loss: 0.050381, acc.: 96.09%] [G loss: 1.054202]\n",
      "epoch:0 step:467 [D loss: 0.142902, acc.: 81.25%] [G loss: 0.937269]\n",
      "epoch:0 step:468 [D loss: 0.096319, acc.: 82.03%] [G loss: 0.879858]\n",
      "epoch:0 step:469 [D loss: 0.077722, acc.: 88.28%] [G loss: 0.922431]\n",
      "epoch:0 step:470 [D loss: 0.084901, acc.: 86.72%] [G loss: 0.975326]\n",
      "epoch:0 step:471 [D loss: 0.055928, acc.: 94.53%] [G loss: 0.870833]\n",
      "epoch:0 step:472 [D loss: 0.072736, acc.: 87.50%] [G loss: 0.978034]\n",
      "epoch:0 step:473 [D loss: 0.100789, acc.: 89.84%] [G loss: 0.890854]\n",
      "epoch:0 step:474 [D loss: 0.058262, acc.: 94.53%] [G loss: 0.998052]\n",
      "epoch:0 step:475 [D loss: 0.081991, acc.: 89.84%] [G loss: 1.028334]\n",
      "epoch:0 step:476 [D loss: 0.104067, acc.: 87.50%] [G loss: 0.973844]\n",
      "epoch:0 step:477 [D loss: 0.081549, acc.: 89.06%] [G loss: 0.993697]\n",
      "epoch:0 step:478 [D loss: 0.095219, acc.: 88.28%] [G loss: 0.875700]\n",
      "epoch:0 step:479 [D loss: 0.082800, acc.: 85.16%] [G loss: 1.014184]\n",
      "epoch:0 step:480 [D loss: 0.075324, acc.: 88.28%] [G loss: 0.968893]\n",
      "epoch:0 step:481 [D loss: 0.091816, acc.: 85.94%] [G loss: 0.876870]\n",
      "epoch:0 step:482 [D loss: 0.068420, acc.: 92.19%] [G loss: 1.045399]\n",
      "epoch:0 step:483 [D loss: 0.124127, acc.: 82.81%] [G loss: 0.947786]\n",
      "epoch:0 step:484 [D loss: 0.058914, acc.: 92.97%] [G loss: 0.992011]\n",
      "epoch:0 step:485 [D loss: 0.088034, acc.: 85.94%] [G loss: 0.909713]\n",
      "epoch:0 step:486 [D loss: 0.092843, acc.: 86.72%] [G loss: 0.940660]\n",
      "epoch:0 step:487 [D loss: 0.065916, acc.: 90.62%] [G loss: 0.986020]\n",
      "epoch:0 step:488 [D loss: 0.105631, acc.: 84.38%] [G loss: 0.932467]\n",
      "epoch:0 step:489 [D loss: 0.074866, acc.: 89.84%] [G loss: 0.973938]\n",
      "epoch:0 step:490 [D loss: 0.089966, acc.: 87.50%] [G loss: 0.891214]\n",
      "epoch:0 step:491 [D loss: 0.067279, acc.: 92.97%] [G loss: 0.946910]\n",
      "epoch:0 step:492 [D loss: 0.112253, acc.: 84.38%] [G loss: 1.016751]\n",
      "epoch:0 step:493 [D loss: 0.079115, acc.: 95.31%] [G loss: 0.944597]\n",
      "epoch:0 step:494 [D loss: 0.069946, acc.: 89.06%] [G loss: 0.917922]\n",
      "epoch:0 step:495 [D loss: 0.113301, acc.: 85.16%] [G loss: 1.060233]\n",
      "epoch:0 step:496 [D loss: 0.088744, acc.: 85.16%] [G loss: 0.988011]\n",
      "epoch:0 step:497 [D loss: 0.102953, acc.: 85.94%] [G loss: 1.025317]\n",
      "epoch:0 step:498 [D loss: 0.079228, acc.: 92.97%] [G loss: 1.014913]\n",
      "epoch:0 step:499 [D loss: 0.156085, acc.: 76.56%] [G loss: 0.950041]\n",
      "epoch:0 step:500 [D loss: 0.105029, acc.: 87.50%] [G loss: 1.006062]\n",
      "epoch:0 step:501 [D loss: 0.109005, acc.: 82.81%] [G loss: 1.033356]\n",
      "epoch:0 step:502 [D loss: 0.083558, acc.: 89.06%] [G loss: 1.087881]\n",
      "epoch:0 step:503 [D loss: 0.135740, acc.: 78.12%] [G loss: 0.963297]\n",
      "epoch:0 step:504 [D loss: 0.084049, acc.: 89.84%] [G loss: 0.966440]\n",
      "epoch:0 step:505 [D loss: 0.089762, acc.: 87.50%] [G loss: 0.896614]\n",
      "epoch:0 step:506 [D loss: 0.067176, acc.: 92.97%] [G loss: 0.971563]\n",
      "epoch:0 step:507 [D loss: 0.120496, acc.: 78.91%] [G loss: 0.934737]\n",
      "epoch:0 step:508 [D loss: 0.101188, acc.: 85.16%] [G loss: 0.923537]\n",
      "epoch:0 step:509 [D loss: 0.091966, acc.: 87.50%] [G loss: 0.984674]\n",
      "epoch:0 step:510 [D loss: 0.097172, acc.: 84.38%] [G loss: 0.949440]\n",
      "epoch:0 step:511 [D loss: 0.085969, acc.: 85.16%] [G loss: 1.044766]\n",
      "epoch:0 step:512 [D loss: 0.083933, acc.: 89.06%] [G loss: 0.847668]\n",
      "epoch:0 step:513 [D loss: 0.059155, acc.: 92.97%] [G loss: 0.996755]\n",
      "epoch:0 step:514 [D loss: 0.110310, acc.: 83.59%] [G loss: 0.972798]\n",
      "epoch:0 step:515 [D loss: 0.077538, acc.: 90.62%] [G loss: 0.946559]\n",
      "epoch:0 step:516 [D loss: 0.105444, acc.: 81.25%] [G loss: 0.960408]\n",
      "epoch:0 step:517 [D loss: 0.116876, acc.: 85.16%] [G loss: 1.061959]\n",
      "epoch:0 step:518 [D loss: 0.069792, acc.: 92.97%] [G loss: 0.822360]\n",
      "epoch:0 step:519 [D loss: 0.087813, acc.: 88.28%] [G loss: 0.984406]\n",
      "epoch:0 step:520 [D loss: 0.088736, acc.: 84.38%] [G loss: 1.039388]\n",
      "epoch:0 step:521 [D loss: 0.104342, acc.: 83.59%] [G loss: 0.982416]\n",
      "epoch:0 step:522 [D loss: 0.110375, acc.: 81.25%] [G loss: 0.988236]\n",
      "epoch:0 step:523 [D loss: 0.068745, acc.: 95.31%] [G loss: 0.983164]\n",
      "epoch:0 step:524 [D loss: 0.075910, acc.: 88.28%] [G loss: 0.903597]\n",
      "epoch:0 step:525 [D loss: 0.064531, acc.: 89.84%] [G loss: 0.984198]\n",
      "epoch:0 step:526 [D loss: 0.117254, acc.: 81.25%] [G loss: 0.947389]\n",
      "epoch:0 step:527 [D loss: 0.093238, acc.: 87.50%] [G loss: 0.898614]\n",
      "epoch:0 step:528 [D loss: 0.073034, acc.: 90.62%] [G loss: 1.036841]\n",
      "epoch:0 step:529 [D loss: 0.073977, acc.: 91.41%] [G loss: 1.042641]\n",
      "epoch:0 step:530 [D loss: 0.092473, acc.: 88.28%] [G loss: 1.076060]\n",
      "epoch:0 step:531 [D loss: 0.146212, acc.: 79.69%] [G loss: 0.963095]\n",
      "epoch:0 step:532 [D loss: 0.091689, acc.: 85.94%] [G loss: 1.140270]\n",
      "epoch:0 step:533 [D loss: 0.140235, acc.: 77.34%] [G loss: 1.091973]\n",
      "epoch:0 step:534 [D loss: 0.087888, acc.: 90.62%] [G loss: 1.127821]\n",
      "epoch:0 step:535 [D loss: 0.129480, acc.: 78.91%] [G loss: 1.092196]\n",
      "epoch:0 step:536 [D loss: 0.080933, acc.: 96.09%] [G loss: 1.094244]\n",
      "epoch:0 step:537 [D loss: 0.064906, acc.: 95.31%] [G loss: 0.964018]\n",
      "epoch:0 step:538 [D loss: 0.110246, acc.: 86.72%] [G loss: 1.142916]\n",
      "epoch:0 step:539 [D loss: 0.080460, acc.: 92.19%] [G loss: 0.844614]\n",
      "epoch:0 step:540 [D loss: 0.094149, acc.: 88.28%] [G loss: 1.146403]\n",
      "epoch:0 step:541 [D loss: 0.092871, acc.: 92.97%] [G loss: 1.123364]\n",
      "epoch:0 step:542 [D loss: 0.101064, acc.: 88.28%] [G loss: 1.025737]\n",
      "epoch:0 step:543 [D loss: 0.074061, acc.: 90.62%] [G loss: 1.075402]\n",
      "epoch:0 step:544 [D loss: 0.091431, acc.: 94.53%] [G loss: 1.196504]\n",
      "epoch:0 step:545 [D loss: 0.103753, acc.: 88.28%] [G loss: 1.083500]\n",
      "epoch:0 step:546 [D loss: 0.061293, acc.: 95.31%] [G loss: 1.040433]\n",
      "epoch:0 step:547 [D loss: 0.063243, acc.: 91.41%] [G loss: 1.164610]\n",
      "epoch:0 step:548 [D loss: 0.098888, acc.: 92.97%] [G loss: 1.157655]\n",
      "epoch:0 step:549 [D loss: 0.146858, acc.: 78.12%] [G loss: 1.076749]\n",
      "epoch:0 step:550 [D loss: 0.065635, acc.: 92.19%] [G loss: 0.932635]\n",
      "epoch:0 step:551 [D loss: 0.082095, acc.: 89.84%] [G loss: 1.229367]\n",
      "epoch:0 step:552 [D loss: 0.095306, acc.: 92.19%] [G loss: 1.067630]\n",
      "epoch:0 step:553 [D loss: 0.062077, acc.: 95.31%] [G loss: 0.937893]\n",
      "epoch:0 step:554 [D loss: 0.103882, acc.: 93.75%] [G loss: 1.099731]\n",
      "epoch:0 step:555 [D loss: 0.100604, acc.: 89.84%] [G loss: 1.032327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:556 [D loss: 0.071277, acc.: 92.19%] [G loss: 1.012024]\n",
      "epoch:0 step:557 [D loss: 0.054138, acc.: 98.44%] [G loss: 1.040647]\n",
      "epoch:0 step:558 [D loss: 0.082639, acc.: 90.62%] [G loss: 0.882849]\n",
      "epoch:0 step:559 [D loss: 0.097526, acc.: 94.53%] [G loss: 1.129369]\n",
      "epoch:0 step:560 [D loss: 0.095314, acc.: 94.53%] [G loss: 0.767883]\n",
      "epoch:0 step:561 [D loss: 0.054074, acc.: 97.66%] [G loss: 0.924574]\n",
      "epoch:0 step:562 [D loss: 0.121486, acc.: 85.94%] [G loss: 1.027028]\n",
      "epoch:0 step:563 [D loss: 0.057878, acc.: 100.00%] [G loss: 0.993628]\n",
      "epoch:0 step:564 [D loss: 0.076477, acc.: 91.41%] [G loss: 0.991070]\n",
      "epoch:0 step:565 [D loss: 0.069562, acc.: 92.19%] [G loss: 1.091120]\n",
      "epoch:0 step:566 [D loss: 0.076686, acc.: 92.97%] [G loss: 1.102199]\n",
      "epoch:0 step:567 [D loss: 0.062814, acc.: 96.88%] [G loss: 0.992956]\n",
      "epoch:0 step:568 [D loss: 0.086344, acc.: 92.97%] [G loss: 1.051157]\n",
      "epoch:0 step:569 [D loss: 0.066313, acc.: 95.31%] [G loss: 0.989800]\n",
      "epoch:0 step:570 [D loss: 0.072282, acc.: 89.06%] [G loss: 1.107428]\n",
      "epoch:0 step:571 [D loss: 0.090725, acc.: 91.41%] [G loss: 0.963510]\n",
      "epoch:0 step:572 [D loss: 0.049632, acc.: 99.22%] [G loss: 1.012884]\n",
      "epoch:0 step:573 [D loss: 0.082857, acc.: 93.75%] [G loss: 1.021478]\n",
      "epoch:0 step:574 [D loss: 0.088808, acc.: 92.19%] [G loss: 1.154191]\n",
      "epoch:0 step:575 [D loss: 0.064083, acc.: 97.66%] [G loss: 1.084802]\n",
      "epoch:0 step:576 [D loss: 0.094088, acc.: 93.75%] [G loss: 1.158094]\n",
      "epoch:0 step:577 [D loss: 0.088607, acc.: 90.62%] [G loss: 0.944087]\n",
      "epoch:0 step:578 [D loss: 0.048920, acc.: 96.88%] [G loss: 1.171841]\n",
      "epoch:0 step:579 [D loss: 0.087933, acc.: 92.19%] [G loss: 1.019079]\n",
      "epoch:0 step:580 [D loss: 0.065653, acc.: 96.88%] [G loss: 1.131069]\n",
      "epoch:0 step:581 [D loss: 0.055907, acc.: 97.66%] [G loss: 0.963084]\n",
      "epoch:0 step:582 [D loss: 0.050932, acc.: 99.22%] [G loss: 1.145523]\n",
      "epoch:0 step:583 [D loss: 0.109818, acc.: 85.94%] [G loss: 1.158967]\n",
      "epoch:0 step:584 [D loss: 0.083241, acc.: 89.06%] [G loss: 1.042930]\n",
      "epoch:0 step:585 [D loss: 0.110209, acc.: 88.28%] [G loss: 1.163694]\n",
      "epoch:0 step:586 [D loss: 0.082967, acc.: 93.75%] [G loss: 0.982422]\n",
      "epoch:0 step:587 [D loss: 0.076903, acc.: 94.53%] [G loss: 1.028023]\n",
      "epoch:0 step:588 [D loss: 0.063692, acc.: 97.66%] [G loss: 1.109015]\n",
      "epoch:0 step:589 [D loss: 0.090049, acc.: 93.75%] [G loss: 1.077798]\n",
      "epoch:0 step:590 [D loss: 0.083479, acc.: 94.53%] [G loss: 1.028664]\n",
      "epoch:0 step:591 [D loss: 0.097670, acc.: 90.62%] [G loss: 1.010643]\n",
      "epoch:0 step:592 [D loss: 0.070183, acc.: 96.09%] [G loss: 1.002758]\n",
      "epoch:0 step:593 [D loss: 0.062939, acc.: 96.88%] [G loss: 1.103667]\n",
      "epoch:0 step:594 [D loss: 0.081393, acc.: 93.75%] [G loss: 1.001133]\n",
      "epoch:0 step:595 [D loss: 0.100876, acc.: 86.72%] [G loss: 1.008105]\n",
      "epoch:0 step:596 [D loss: 0.060687, acc.: 96.88%] [G loss: 1.021975]\n",
      "epoch:0 step:597 [D loss: 0.085136, acc.: 94.53%] [G loss: 1.060474]\n",
      "epoch:0 step:598 [D loss: 0.075105, acc.: 95.31%] [G loss: 0.957880]\n",
      "epoch:0 step:599 [D loss: 0.065206, acc.: 96.88%] [G loss: 1.014767]\n",
      "epoch:0 step:600 [D loss: 0.121548, acc.: 81.25%] [G loss: 1.133010]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 1.353443\n",
      "FID: 273.444702\n",
      "0 = 20.59339847793583\n",
      "1 = 0.34799195272634176\n",
      "2 = 0.9998000264167786\n",
      "3 = 0.9995999932289124\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9995999932289124\n",
      "7 = 16.8357341937542\n",
      "8 = 0.27531139776245195\n",
      "9 = 0.9991000294685364\n",
      "10 = 0.998199999332428\n",
      "11 = 1.0\n",
      "12 = 1.0\n",
      "13 = 0.998199999332428\n",
      "14 = 1.353442907333374\n",
      "15 = 4.914165019989014\n",
      "16 = 0.8680800795555115\n",
      "17 = 1.3534427881240845\n",
      "18 = 273.4447021484375\n",
      "epoch:0 step:601 [D loss: 0.055886, acc.: 98.44%] [G loss: 0.986158]\n",
      "epoch:0 step:602 [D loss: 0.087833, acc.: 95.31%] [G loss: 1.017754]\n",
      "epoch:0 step:603 [D loss: 0.048490, acc.: 97.66%] [G loss: 1.036446]\n",
      "epoch:0 step:604 [D loss: 0.113783, acc.: 92.19%] [G loss: 1.015834]\n",
      "epoch:0 step:605 [D loss: 0.060754, acc.: 97.66%] [G loss: 1.013199]\n",
      "epoch:0 step:606 [D loss: 0.067134, acc.: 95.31%] [G loss: 1.017026]\n",
      "epoch:0 step:607 [D loss: 0.077281, acc.: 96.88%] [G loss: 1.081005]\n",
      "epoch:0 step:608 [D loss: 0.082601, acc.: 92.19%] [G loss: 1.077519]\n",
      "epoch:0 step:609 [D loss: 0.070893, acc.: 99.22%] [G loss: 1.063476]\n",
      "epoch:0 step:610 [D loss: 0.100489, acc.: 92.19%] [G loss: 1.062744]\n",
      "epoch:0 step:611 [D loss: 0.047532, acc.: 98.44%] [G loss: 1.090735]\n",
      "epoch:0 step:612 [D loss: 0.081330, acc.: 96.09%] [G loss: 1.105309]\n",
      "epoch:0 step:613 [D loss: 0.060189, acc.: 96.88%] [G loss: 1.093652]\n",
      "epoch:0 step:614 [D loss: 0.081269, acc.: 95.31%] [G loss: 1.162348]\n",
      "epoch:0 step:615 [D loss: 0.079436, acc.: 93.75%] [G loss: 1.061785]\n",
      "epoch:0 step:616 [D loss: 0.072560, acc.: 95.31%] [G loss: 1.168010]\n",
      "epoch:0 step:617 [D loss: 0.103883, acc.: 85.94%] [G loss: 1.176404]\n",
      "epoch:0 step:618 [D loss: 0.064263, acc.: 97.66%] [G loss: 0.979650]\n",
      "epoch:0 step:619 [D loss: 0.089351, acc.: 94.53%] [G loss: 1.370768]\n",
      "epoch:0 step:620 [D loss: 0.112968, acc.: 86.72%] [G loss: 1.059607]\n",
      "epoch:0 step:621 [D loss: 0.075169, acc.: 96.09%] [G loss: 1.087971]\n",
      "epoch:0 step:622 [D loss: 0.139803, acc.: 80.47%] [G loss: 1.194026]\n",
      "epoch:0 step:623 [D loss: 0.065237, acc.: 97.66%] [G loss: 0.904224]\n",
      "epoch:0 step:624 [D loss: 0.127765, acc.: 79.69%] [G loss: 1.130890]\n",
      "epoch:0 step:625 [D loss: 0.122699, acc.: 85.94%] [G loss: 1.032660]\n",
      "epoch:0 step:626 [D loss: 0.053233, acc.: 99.22%] [G loss: 1.091914]\n",
      "epoch:0 step:627 [D loss: 0.105078, acc.: 91.41%] [G loss: 1.025229]\n",
      "epoch:0 step:628 [D loss: 0.133661, acc.: 86.72%] [G loss: 1.265502]\n",
      "epoch:0 step:629 [D loss: 0.108214, acc.: 88.28%] [G loss: 1.011291]\n",
      "epoch:0 step:630 [D loss: 0.089452, acc.: 92.97%] [G loss: 1.076203]\n",
      "epoch:0 step:631 [D loss: 0.092793, acc.: 92.19%] [G loss: 1.055352]\n",
      "epoch:0 step:632 [D loss: 0.122722, acc.: 80.47%] [G loss: 1.147880]\n",
      "epoch:0 step:633 [D loss: 0.078042, acc.: 95.31%] [G loss: 0.899146]\n",
      "epoch:0 step:634 [D loss: 0.121886, acc.: 91.41%] [G loss: 1.041061]\n",
      "epoch:0 step:635 [D loss: 0.085899, acc.: 94.53%] [G loss: 0.941728]\n",
      "epoch:0 step:636 [D loss: 0.130003, acc.: 85.16%] [G loss: 1.146702]\n",
      "epoch:0 step:637 [D loss: 0.106219, acc.: 85.94%] [G loss: 1.012656]\n",
      "epoch:0 step:638 [D loss: 0.063033, acc.: 93.75%] [G loss: 1.032736]\n",
      "epoch:0 step:639 [D loss: 0.103316, acc.: 89.06%] [G loss: 1.118401]\n",
      "epoch:0 step:640 [D loss: 0.095866, acc.: 92.19%] [G loss: 1.149729]\n",
      "epoch:0 step:641 [D loss: 0.101001, acc.: 92.97%] [G loss: 1.103048]\n",
      "epoch:0 step:642 [D loss: 0.084294, acc.: 96.88%] [G loss: 0.932154]\n",
      "epoch:0 step:643 [D loss: 0.113771, acc.: 88.28%] [G loss: 1.075025]\n",
      "epoch:0 step:644 [D loss: 0.092407, acc.: 94.53%] [G loss: 1.143955]\n",
      "epoch:0 step:645 [D loss: 0.105950, acc.: 85.94%] [G loss: 0.982435]\n",
      "epoch:0 step:646 [D loss: 0.059715, acc.: 100.00%] [G loss: 1.134618]\n",
      "epoch:0 step:647 [D loss: 0.100309, acc.: 96.09%] [G loss: 1.193867]\n",
      "epoch:0 step:648 [D loss: 0.074704, acc.: 96.88%] [G loss: 0.971373]\n",
      "epoch:0 step:649 [D loss: 0.057449, acc.: 99.22%] [G loss: 1.234598]\n",
      "epoch:0 step:650 [D loss: 0.159599, acc.: 82.03%] [G loss: 1.074419]\n",
      "epoch:0 step:651 [D loss: 0.079964, acc.: 96.09%] [G loss: 1.096877]\n",
      "epoch:0 step:652 [D loss: 0.106678, acc.: 92.19%] [G loss: 1.032619]\n",
      "epoch:0 step:653 [D loss: 0.087300, acc.: 92.97%] [G loss: 0.971387]\n",
      "epoch:0 step:654 [D loss: 0.069244, acc.: 97.66%] [G loss: 0.981351]\n",
      "epoch:0 step:655 [D loss: 0.097029, acc.: 88.28%] [G loss: 1.228320]\n",
      "epoch:0 step:656 [D loss: 0.080739, acc.: 95.31%] [G loss: 0.988153]\n",
      "epoch:0 step:657 [D loss: 0.094083, acc.: 90.62%] [G loss: 1.107649]\n",
      "epoch:0 step:658 [D loss: 0.109300, acc.: 90.62%] [G loss: 0.974559]\n",
      "epoch:0 step:659 [D loss: 0.064110, acc.: 98.44%] [G loss: 1.155992]\n",
      "epoch:0 step:660 [D loss: 0.136782, acc.: 80.47%] [G loss: 1.026614]\n",
      "epoch:0 step:661 [D loss: 0.059612, acc.: 98.44%] [G loss: 1.182562]\n",
      "epoch:0 step:662 [D loss: 0.185741, acc.: 75.00%] [G loss: 1.000224]\n",
      "epoch:0 step:663 [D loss: 0.064503, acc.: 97.66%] [G loss: 1.056541]\n",
      "epoch:0 step:664 [D loss: 0.076336, acc.: 96.09%] [G loss: 1.036027]\n",
      "epoch:0 step:665 [D loss: 0.092863, acc.: 93.75%] [G loss: 1.116019]\n",
      "epoch:0 step:666 [D loss: 0.062847, acc.: 94.53%] [G loss: 1.000513]\n",
      "epoch:0 step:667 [D loss: 0.088835, acc.: 95.31%] [G loss: 1.373325]\n",
      "epoch:0 step:668 [D loss: 0.093775, acc.: 92.97%] [G loss: 1.135115]\n",
      "epoch:0 step:669 [D loss: 0.055586, acc.: 98.44%] [G loss: 1.142208]\n",
      "epoch:0 step:670 [D loss: 0.154978, acc.: 82.03%] [G loss: 1.244111]\n",
      "epoch:0 step:671 [D loss: 0.125962, acc.: 82.03%] [G loss: 1.062514]\n",
      "epoch:0 step:672 [D loss: 0.093019, acc.: 85.94%] [G loss: 1.073802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:673 [D loss: 0.088035, acc.: 96.09%] [G loss: 1.124397]\n",
      "epoch:0 step:674 [D loss: 0.105630, acc.: 84.38%] [G loss: 1.136855]\n",
      "epoch:0 step:675 [D loss: 0.055865, acc.: 99.22%] [G loss: 1.004599]\n",
      "epoch:0 step:676 [D loss: 0.158209, acc.: 80.47%] [G loss: 1.471850]\n",
      "epoch:0 step:677 [D loss: 0.102958, acc.: 89.06%] [G loss: 0.827341]\n",
      "epoch:0 step:678 [D loss: 0.095168, acc.: 96.88%] [G loss: 1.323584]\n",
      "epoch:0 step:679 [D loss: 0.211885, acc.: 65.62%] [G loss: 1.006583]\n",
      "epoch:0 step:680 [D loss: 0.067567, acc.: 98.44%] [G loss: 1.137778]\n",
      "epoch:0 step:681 [D loss: 0.134494, acc.: 86.72%] [G loss: 1.122148]\n",
      "epoch:0 step:682 [D loss: 0.120712, acc.: 91.41%] [G loss: 0.981454]\n",
      "epoch:0 step:683 [D loss: 0.098450, acc.: 92.19%] [G loss: 1.352630]\n",
      "epoch:0 step:684 [D loss: 0.136334, acc.: 85.16%] [G loss: 1.158019]\n",
      "epoch:0 step:685 [D loss: 0.136484, acc.: 82.03%] [G loss: 1.326513]\n",
      "epoch:0 step:686 [D loss: 0.102220, acc.: 89.84%] [G loss: 1.177317]\n",
      "epoch:0 step:687 [D loss: 0.088559, acc.: 92.97%] [G loss: 1.199230]\n",
      "epoch:0 step:688 [D loss: 0.160883, acc.: 65.62%] [G loss: 1.115015]\n",
      "epoch:0 step:689 [D loss: 0.114278, acc.: 88.28%] [G loss: 1.218377]\n",
      "epoch:0 step:690 [D loss: 0.121724, acc.: 87.50%] [G loss: 1.028602]\n",
      "epoch:0 step:691 [D loss: 0.109256, acc.: 87.50%] [G loss: 1.248411]\n",
      "epoch:0 step:692 [D loss: 0.167429, acc.: 75.00%] [G loss: 1.019322]\n",
      "epoch:0 step:693 [D loss: 0.090455, acc.: 94.53%] [G loss: 1.062007]\n",
      "epoch:0 step:694 [D loss: 0.096502, acc.: 90.62%] [G loss: 1.107995]\n",
      "epoch:0 step:695 [D loss: 0.099633, acc.: 91.41%] [G loss: 1.064153]\n",
      "epoch:0 step:696 [D loss: 0.084640, acc.: 94.53%] [G loss: 1.116201]\n",
      "epoch:0 step:697 [D loss: 0.068950, acc.: 95.31%] [G loss: 1.129331]\n",
      "epoch:0 step:698 [D loss: 0.128218, acc.: 83.59%] [G loss: 1.166935]\n",
      "epoch:0 step:699 [D loss: 0.094834, acc.: 90.62%] [G loss: 0.977950]\n",
      "epoch:0 step:700 [D loss: 0.095727, acc.: 94.53%] [G loss: 1.329522]\n",
      "epoch:0 step:701 [D loss: 0.116147, acc.: 89.06%] [G loss: 1.026292]\n",
      "epoch:0 step:702 [D loss: 0.101723, acc.: 90.62%] [G loss: 1.460641]\n",
      "epoch:0 step:703 [D loss: 0.169343, acc.: 78.12%] [G loss: 1.053652]\n",
      "epoch:0 step:704 [D loss: 0.099641, acc.: 94.53%] [G loss: 1.406356]\n",
      "epoch:0 step:705 [D loss: 0.129484, acc.: 89.84%] [G loss: 0.801297]\n",
      "epoch:0 step:706 [D loss: 0.063601, acc.: 97.66%] [G loss: 1.193175]\n",
      "epoch:0 step:707 [D loss: 0.171947, acc.: 72.66%] [G loss: 1.084262]\n",
      "epoch:0 step:708 [D loss: 0.047945, acc.: 99.22%] [G loss: 1.145704]\n",
      "epoch:0 step:709 [D loss: 0.092980, acc.: 93.75%] [G loss: 1.190626]\n",
      "epoch:0 step:710 [D loss: 0.162692, acc.: 74.22%] [G loss: 1.227684]\n",
      "epoch:0 step:711 [D loss: 0.096867, acc.: 89.84%] [G loss: 0.968117]\n",
      "epoch:0 step:712 [D loss: 0.085969, acc.: 93.75%] [G loss: 1.445397]\n",
      "epoch:0 step:713 [D loss: 0.146958, acc.: 85.16%] [G loss: 1.087367]\n",
      "epoch:0 step:714 [D loss: 0.079462, acc.: 94.53%] [G loss: 1.112669]\n",
      "epoch:0 step:715 [D loss: 0.131536, acc.: 92.19%] [G loss: 1.255222]\n",
      "epoch:0 step:716 [D loss: 0.134568, acc.: 82.03%] [G loss: 1.136932]\n",
      "epoch:0 step:717 [D loss: 0.130408, acc.: 85.94%] [G loss: 0.944316]\n",
      "epoch:0 step:718 [D loss: 0.068612, acc.: 96.09%] [G loss: 1.229958]\n",
      "epoch:0 step:719 [D loss: 0.215015, acc.: 64.06%] [G loss: 1.434290]\n",
      "epoch:0 step:720 [D loss: 0.109694, acc.: 88.28%] [G loss: 0.944219]\n",
      "epoch:0 step:721 [D loss: 0.173026, acc.: 74.22%] [G loss: 1.373717]\n",
      "epoch:0 step:722 [D loss: 0.197641, acc.: 75.78%] [G loss: 1.226791]\n",
      "epoch:0 step:723 [D loss: 0.104043, acc.: 90.62%] [G loss: 1.141765]\n",
      "epoch:0 step:724 [D loss: 0.097505, acc.: 91.41%] [G loss: 1.041591]\n",
      "epoch:0 step:725 [D loss: 0.117024, acc.: 90.62%] [G loss: 1.278446]\n",
      "epoch:0 step:726 [D loss: 0.091405, acc.: 92.97%] [G loss: 0.996637]\n",
      "epoch:0 step:727 [D loss: 0.118324, acc.: 92.19%] [G loss: 1.365667]\n",
      "epoch:0 step:728 [D loss: 0.155519, acc.: 76.56%] [G loss: 1.019870]\n",
      "epoch:0 step:729 [D loss: 0.094867, acc.: 92.97%] [G loss: 1.432965]\n",
      "epoch:0 step:730 [D loss: 0.095365, acc.: 92.19%] [G loss: 1.049067]\n",
      "epoch:0 step:731 [D loss: 0.064031, acc.: 96.88%] [G loss: 1.314374]\n",
      "epoch:0 step:732 [D loss: 0.161467, acc.: 82.03%] [G loss: 1.381385]\n",
      "epoch:0 step:733 [D loss: 0.089538, acc.: 94.53%] [G loss: 1.230835]\n",
      "epoch:0 step:734 [D loss: 0.107413, acc.: 87.50%] [G loss: 1.147107]\n",
      "epoch:0 step:735 [D loss: 0.142921, acc.: 87.50%] [G loss: 1.153980]\n",
      "epoch:0 step:736 [D loss: 0.140048, acc.: 82.03%] [G loss: 1.217805]\n",
      "epoch:0 step:737 [D loss: 0.095392, acc.: 92.97%] [G loss: 1.166091]\n",
      "epoch:0 step:738 [D loss: 0.164101, acc.: 77.34%] [G loss: 1.407834]\n",
      "epoch:0 step:739 [D loss: 0.118764, acc.: 89.06%] [G loss: 0.806198]\n",
      "epoch:0 step:740 [D loss: 0.105652, acc.: 88.28%] [G loss: 1.288951]\n",
      "epoch:0 step:741 [D loss: 0.103706, acc.: 89.06%] [G loss: 1.118577]\n",
      "epoch:0 step:742 [D loss: 0.061612, acc.: 97.66%] [G loss: 0.980304]\n",
      "epoch:0 step:743 [D loss: 0.075758, acc.: 96.88%] [G loss: 1.086113]\n",
      "epoch:0 step:744 [D loss: 0.116257, acc.: 91.41%] [G loss: 1.135165]\n",
      "epoch:0 step:745 [D loss: 0.070283, acc.: 97.66%] [G loss: 1.067474]\n",
      "epoch:0 step:746 [D loss: 0.079642, acc.: 96.88%] [G loss: 1.043733]\n",
      "epoch:0 step:747 [D loss: 0.074003, acc.: 93.75%] [G loss: 1.030761]\n",
      "epoch:0 step:748 [D loss: 0.104068, acc.: 92.97%] [G loss: 1.061502]\n",
      "epoch:0 step:749 [D loss: 0.077200, acc.: 94.53%] [G loss: 1.089064]\n",
      "epoch:0 step:750 [D loss: 0.121453, acc.: 88.28%] [G loss: 1.383016]\n",
      "epoch:0 step:751 [D loss: 0.095099, acc.: 92.19%] [G loss: 0.997419]\n",
      "epoch:0 step:752 [D loss: 0.090365, acc.: 92.19%] [G loss: 1.358028]\n",
      "epoch:0 step:753 [D loss: 0.157903, acc.: 82.81%] [G loss: 0.876812]\n",
      "epoch:0 step:754 [D loss: 0.070954, acc.: 98.44%] [G loss: 1.122896]\n",
      "epoch:0 step:755 [D loss: 0.118289, acc.: 89.84%] [G loss: 1.335989]\n",
      "epoch:0 step:756 [D loss: 0.105131, acc.: 88.28%] [G loss: 1.081075]\n",
      "epoch:0 step:757 [D loss: 0.071581, acc.: 95.31%] [G loss: 1.142779]\n",
      "epoch:0 step:758 [D loss: 0.107307, acc.: 91.41%] [G loss: 1.000706]\n",
      "epoch:0 step:759 [D loss: 0.072880, acc.: 95.31%] [G loss: 1.046364]\n",
      "epoch:0 step:760 [D loss: 0.109756, acc.: 88.28%] [G loss: 1.234555]\n",
      "epoch:0 step:761 [D loss: 0.117529, acc.: 88.28%] [G loss: 1.122706]\n",
      "epoch:0 step:762 [D loss: 0.093281, acc.: 92.19%] [G loss: 1.092339]\n",
      "epoch:0 step:763 [D loss: 0.080844, acc.: 92.19%] [G loss: 1.221480]\n",
      "epoch:0 step:764 [D loss: 0.116963, acc.: 88.28%] [G loss: 1.006684]\n",
      "epoch:0 step:765 [D loss: 0.104885, acc.: 92.19%] [G loss: 1.125108]\n",
      "epoch:0 step:766 [D loss: 0.144146, acc.: 83.59%] [G loss: 1.110255]\n",
      "epoch:0 step:767 [D loss: 0.090543, acc.: 92.97%] [G loss: 1.049026]\n",
      "epoch:0 step:768 [D loss: 0.074977, acc.: 92.19%] [G loss: 1.292787]\n",
      "epoch:0 step:769 [D loss: 0.151727, acc.: 84.38%] [G loss: 1.530698]\n",
      "epoch:0 step:770 [D loss: 0.135709, acc.: 83.59%] [G loss: 0.814159]\n",
      "epoch:0 step:771 [D loss: 0.133005, acc.: 78.12%] [G loss: 1.510314]\n",
      "epoch:0 step:772 [D loss: 0.139713, acc.: 83.59%] [G loss: 0.820824]\n",
      "epoch:0 step:773 [D loss: 0.086951, acc.: 96.09%] [G loss: 1.151340]\n",
      "epoch:0 step:774 [D loss: 0.095333, acc.: 89.84%] [G loss: 0.860254]\n",
      "epoch:0 step:775 [D loss: 0.079888, acc.: 96.09%] [G loss: 1.203350]\n",
      "epoch:0 step:776 [D loss: 0.136490, acc.: 88.28%] [G loss: 1.037920]\n",
      "epoch:0 step:777 [D loss: 0.069123, acc.: 94.53%] [G loss: 1.122118]\n",
      "epoch:0 step:778 [D loss: 0.100210, acc.: 94.53%] [G loss: 1.100742]\n",
      "epoch:0 step:779 [D loss: 0.117817, acc.: 91.41%] [G loss: 0.974534]\n",
      "epoch:0 step:780 [D loss: 0.090589, acc.: 93.75%] [G loss: 1.203187]\n",
      "epoch:0 step:781 [D loss: 0.109888, acc.: 91.41%] [G loss: 1.050717]\n",
      "epoch:0 step:782 [D loss: 0.061619, acc.: 96.09%] [G loss: 0.963318]\n",
      "epoch:0 step:783 [D loss: 0.101046, acc.: 89.06%] [G loss: 1.349421]\n",
      "epoch:0 step:784 [D loss: 0.132235, acc.: 86.72%] [G loss: 0.898547]\n",
      "epoch:0 step:785 [D loss: 0.064855, acc.: 95.31%] [G loss: 1.124651]\n",
      "epoch:0 step:786 [D loss: 0.130596, acc.: 82.03%] [G loss: 1.090770]\n",
      "epoch:0 step:787 [D loss: 0.102856, acc.: 89.84%] [G loss: 1.001181]\n",
      "epoch:0 step:788 [D loss: 0.129741, acc.: 91.41%] [G loss: 1.197005]\n",
      "epoch:0 step:789 [D loss: 0.046378, acc.: 96.88%] [G loss: 1.008577]\n",
      "epoch:0 step:790 [D loss: 0.139720, acc.: 85.16%] [G loss: 1.149147]\n",
      "epoch:0 step:791 [D loss: 0.099042, acc.: 91.41%] [G loss: 0.994352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:792 [D loss: 0.115339, acc.: 92.97%] [G loss: 1.021451]\n",
      "epoch:0 step:793 [D loss: 0.122339, acc.: 84.38%] [G loss: 1.221938]\n",
      "epoch:0 step:794 [D loss: 0.090971, acc.: 90.62%] [G loss: 0.905319]\n",
      "epoch:0 step:795 [D loss: 0.077513, acc.: 95.31%] [G loss: 1.120854]\n",
      "epoch:0 step:796 [D loss: 0.090087, acc.: 92.19%] [G loss: 0.905048]\n",
      "epoch:0 step:797 [D loss: 0.092794, acc.: 92.97%] [G loss: 1.048988]\n",
      "epoch:0 step:798 [D loss: 0.089217, acc.: 93.75%] [G loss: 1.082417]\n",
      "epoch:0 step:799 [D loss: 0.080363, acc.: 90.62%] [G loss: 1.150130]\n",
      "epoch:0 step:800 [D loss: 0.083692, acc.: 91.41%] [G loss: 1.202466]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 2.409691\n",
      "FID: 211.812912\n",
      "0 = 17.814068099594138\n",
      "1 = 0.2409881922413288\n",
      "2 = 0.9993000030517578\n",
      "3 = 0.9986000061035156\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9986000061035156\n",
      "7 = 15.268195880079276\n",
      "8 = 0.24467519600091453\n",
      "9 = 0.9973499774932861\n",
      "10 = 0.994700014591217\n",
      "11 = 1.0\n",
      "12 = 1.0\n",
      "13 = 0.994700014591217\n",
      "14 = 2.4096925258636475\n",
      "15 = 5.403438091278076\n",
      "16 = 0.7044846415519714\n",
      "17 = 2.4096908569335938\n",
      "18 = 211.8129119873047\n",
      "epoch:0 step:801 [D loss: 0.096401, acc.: 93.75%] [G loss: 1.283820]\n",
      "epoch:0 step:802 [D loss: 0.109595, acc.: 87.50%] [G loss: 1.090336]\n",
      "epoch:0 step:803 [D loss: 0.142018, acc.: 85.16%] [G loss: 0.804650]\n",
      "epoch:0 step:804 [D loss: 0.101796, acc.: 94.53%] [G loss: 1.155242]\n",
      "epoch:0 step:805 [D loss: 0.109237, acc.: 89.06%] [G loss: 1.042295]\n",
      "epoch:0 step:806 [D loss: 0.093372, acc.: 96.88%] [G loss: 1.005906]\n",
      "epoch:0 step:807 [D loss: 0.087205, acc.: 94.53%] [G loss: 1.037658]\n",
      "epoch:0 step:808 [D loss: 0.108754, acc.: 90.62%] [G loss: 1.132895]\n",
      "epoch:0 step:809 [D loss: 0.096786, acc.: 89.84%] [G loss: 1.076377]\n",
      "epoch:0 step:810 [D loss: 0.105424, acc.: 89.06%] [G loss: 1.087520]\n",
      "epoch:0 step:811 [D loss: 0.099458, acc.: 91.41%] [G loss: 1.098952]\n",
      "epoch:0 step:812 [D loss: 0.134205, acc.: 88.28%] [G loss: 1.018597]\n",
      "epoch:0 step:813 [D loss: 0.115478, acc.: 89.06%] [G loss: 1.126088]\n",
      "epoch:0 step:814 [D loss: 0.134426, acc.: 89.84%] [G loss: 1.021575]\n",
      "epoch:0 step:815 [D loss: 0.081838, acc.: 90.62%] [G loss: 1.072533]\n",
      "epoch:0 step:816 [D loss: 0.128402, acc.: 85.94%] [G loss: 1.163252]\n",
      "epoch:0 step:817 [D loss: 0.120803, acc.: 89.06%] [G loss: 1.148300]\n",
      "epoch:0 step:818 [D loss: 0.118777, acc.: 87.50%] [G loss: 1.141485]\n",
      "epoch:0 step:819 [D loss: 0.083554, acc.: 95.31%] [G loss: 1.015708]\n",
      "epoch:0 step:820 [D loss: 0.123242, acc.: 88.28%] [G loss: 1.263487]\n",
      "epoch:0 step:821 [D loss: 0.082069, acc.: 95.31%] [G loss: 1.000679]\n",
      "epoch:0 step:822 [D loss: 0.124431, acc.: 86.72%] [G loss: 1.155609]\n",
      "epoch:0 step:823 [D loss: 0.119618, acc.: 88.28%] [G loss: 1.124137]\n",
      "epoch:0 step:824 [D loss: 0.101299, acc.: 95.31%] [G loss: 1.021981]\n",
      "epoch:0 step:825 [D loss: 0.086742, acc.: 94.53%] [G loss: 1.045667]\n",
      "epoch:0 step:826 [D loss: 0.105076, acc.: 94.53%] [G loss: 1.171388]\n",
      "epoch:0 step:827 [D loss: 0.107361, acc.: 89.06%] [G loss: 1.151055]\n",
      "epoch:0 step:828 [D loss: 0.074183, acc.: 94.53%] [G loss: 1.086908]\n",
      "epoch:0 step:829 [D loss: 0.133991, acc.: 88.28%] [G loss: 1.230327]\n",
      "epoch:0 step:830 [D loss: 0.091876, acc.: 91.41%] [G loss: 1.178763]\n",
      "epoch:0 step:831 [D loss: 0.178148, acc.: 68.75%] [G loss: 1.231010]\n",
      "epoch:0 step:832 [D loss: 0.099375, acc.: 90.62%] [G loss: 0.844426]\n",
      "epoch:0 step:833 [D loss: 0.187150, acc.: 71.88%] [G loss: 1.413134]\n",
      "epoch:0 step:834 [D loss: 0.204160, acc.: 70.31%] [G loss: 0.864258]\n",
      "epoch:0 step:835 [D loss: 0.074249, acc.: 95.31%] [G loss: 1.253379]\n",
      "epoch:0 step:836 [D loss: 0.215295, acc.: 71.09%] [G loss: 0.961155]\n",
      "epoch:0 step:837 [D loss: 0.109746, acc.: 90.62%] [G loss: 1.007900]\n",
      "epoch:0 step:838 [D loss: 0.118050, acc.: 91.41%] [G loss: 1.341102]\n",
      "epoch:0 step:839 [D loss: 0.176098, acc.: 75.00%] [G loss: 1.065471]\n",
      "epoch:0 step:840 [D loss: 0.092902, acc.: 94.53%] [G loss: 1.160546]\n",
      "epoch:0 step:841 [D loss: 0.131430, acc.: 87.50%] [G loss: 1.282014]\n",
      "epoch:0 step:842 [D loss: 0.166985, acc.: 79.69%] [G loss: 1.137484]\n",
      "epoch:0 step:843 [D loss: 0.088005, acc.: 91.41%] [G loss: 1.468806]\n",
      "epoch:0 step:844 [D loss: 0.196379, acc.: 67.97%] [G loss: 1.506266]\n",
      "epoch:0 step:845 [D loss: 0.154627, acc.: 79.69%] [G loss: 1.160579]\n",
      "epoch:0 step:846 [D loss: 0.121725, acc.: 89.06%] [G loss: 1.266184]\n",
      "epoch:0 step:847 [D loss: 0.136291, acc.: 81.25%] [G loss: 1.355241]\n",
      "epoch:0 step:848 [D loss: 0.154027, acc.: 82.81%] [G loss: 1.271504]\n",
      "epoch:0 step:849 [D loss: 0.148248, acc.: 78.91%] [G loss: 1.229397]\n",
      "epoch:0 step:850 [D loss: 0.077742, acc.: 93.75%] [G loss: 1.034241]\n",
      "epoch:0 step:851 [D loss: 0.115199, acc.: 88.28%] [G loss: 1.137338]\n",
      "epoch:0 step:852 [D loss: 0.112283, acc.: 89.06%] [G loss: 1.215084]\n",
      "epoch:0 step:853 [D loss: 0.129044, acc.: 86.72%] [G loss: 1.254634]\n",
      "epoch:0 step:854 [D loss: 0.062185, acc.: 92.97%] [G loss: 1.016465]\n",
      "epoch:0 step:855 [D loss: 0.123129, acc.: 89.84%] [G loss: 1.134902]\n",
      "epoch:0 step:856 [D loss: 0.155684, acc.: 85.16%] [G loss: 1.108901]\n",
      "epoch:0 step:857 [D loss: 0.077841, acc.: 94.53%] [G loss: 1.133356]\n",
      "epoch:0 step:858 [D loss: 0.275200, acc.: 51.56%] [G loss: 1.260139]\n",
      "epoch:0 step:859 [D loss: 0.118319, acc.: 84.38%] [G loss: 0.811023]\n",
      "epoch:0 step:860 [D loss: 0.093565, acc.: 92.19%] [G loss: 1.341128]\n",
      "epoch:0 step:861 [D loss: 0.178273, acc.: 72.66%] [G loss: 0.847864]\n",
      "epoch:0 step:862 [D loss: 0.045896, acc.: 100.00%] [G loss: 1.190353]\n",
      "epoch:0 step:863 [D loss: 0.124319, acc.: 88.28%] [G loss: 0.942373]\n",
      "epoch:0 step:864 [D loss: 0.108317, acc.: 90.62%] [G loss: 1.051991]\n",
      "epoch:0 step:865 [D loss: 0.090022, acc.: 90.62%] [G loss: 1.041283]\n",
      "epoch:0 step:866 [D loss: 0.118680, acc.: 84.38%] [G loss: 1.045624]\n",
      "epoch:0 step:867 [D loss: 0.119189, acc.: 85.94%] [G loss: 0.946176]\n",
      "epoch:0 step:868 [D loss: 0.089234, acc.: 90.62%] [G loss: 1.133560]\n",
      "epoch:0 step:869 [D loss: 0.088415, acc.: 94.53%] [G loss: 1.111188]\n",
      "epoch:0 step:870 [D loss: 0.111012, acc.: 90.62%] [G loss: 1.217552]\n",
      "epoch:0 step:871 [D loss: 0.093057, acc.: 89.84%] [G loss: 0.982098]\n",
      "epoch:0 step:872 [D loss: 0.129519, acc.: 86.72%] [G loss: 1.180553]\n",
      "epoch:0 step:873 [D loss: 0.133169, acc.: 86.72%] [G loss: 1.092668]\n",
      "epoch:0 step:874 [D loss: 0.133251, acc.: 87.50%] [G loss: 0.985666]\n",
      "epoch:0 step:875 [D loss: 0.109958, acc.: 90.62%] [G loss: 1.058669]\n",
      "epoch:0 step:876 [D loss: 0.159920, acc.: 85.16%] [G loss: 0.996384]\n",
      "epoch:0 step:877 [D loss: 0.073587, acc.: 96.88%] [G loss: 1.057292]\n",
      "epoch:0 step:878 [D loss: 0.133358, acc.: 89.84%] [G loss: 1.038576]\n",
      "epoch:0 step:879 [D loss: 0.121055, acc.: 87.50%] [G loss: 1.058023]\n",
      "epoch:0 step:880 [D loss: 0.100342, acc.: 89.84%] [G loss: 1.083634]\n",
      "epoch:0 step:881 [D loss: 0.099874, acc.: 92.97%] [G loss: 0.982336]\n",
      "epoch:0 step:882 [D loss: 0.093881, acc.: 93.75%] [G loss: 1.177321]\n",
      "epoch:0 step:883 [D loss: 0.123397, acc.: 89.84%] [G loss: 1.066413]\n",
      "epoch:0 step:884 [D loss: 0.152559, acc.: 82.03%] [G loss: 1.042957]\n",
      "epoch:0 step:885 [D loss: 0.084856, acc.: 90.62%] [G loss: 1.059128]\n",
      "epoch:0 step:886 [D loss: 0.151104, acc.: 80.47%] [G loss: 1.088003]\n",
      "epoch:0 step:887 [D loss: 0.070112, acc.: 94.53%] [G loss: 0.966860]\n",
      "epoch:0 step:888 [D loss: 0.101232, acc.: 93.75%] [G loss: 1.018798]\n",
      "epoch:0 step:889 [D loss: 0.134474, acc.: 85.94%] [G loss: 1.076682]\n",
      "epoch:0 step:890 [D loss: 0.098220, acc.: 93.75%] [G loss: 0.987883]\n",
      "epoch:0 step:891 [D loss: 0.151690, acc.: 82.81%] [G loss: 1.064224]\n",
      "epoch:0 step:892 [D loss: 0.089479, acc.: 93.75%] [G loss: 0.974652]\n",
      "epoch:0 step:893 [D loss: 0.083217, acc.: 93.75%] [G loss: 1.201432]\n",
      "epoch:0 step:894 [D loss: 0.130423, acc.: 89.06%] [G loss: 0.980375]\n",
      "epoch:0 step:895 [D loss: 0.086275, acc.: 93.75%] [G loss: 1.070221]\n",
      "epoch:0 step:896 [D loss: 0.128623, acc.: 89.06%] [G loss: 0.946742]\n",
      "epoch:0 step:897 [D loss: 0.075660, acc.: 96.88%] [G loss: 1.283365]\n",
      "epoch:0 step:898 [D loss: 0.136710, acc.: 82.81%] [G loss: 0.909314]\n",
      "epoch:0 step:899 [D loss: 0.080050, acc.: 97.66%] [G loss: 1.127461]\n",
      "epoch:0 step:900 [D loss: 0.170260, acc.: 75.00%] [G loss: 1.006085]\n",
      "epoch:0 step:901 [D loss: 0.067924, acc.: 93.75%] [G loss: 1.068088]\n",
      "epoch:0 step:902 [D loss: 0.231048, acc.: 56.25%] [G loss: 1.245425]\n",
      "epoch:0 step:903 [D loss: 0.126558, acc.: 82.81%] [G loss: 0.910612]\n",
      "epoch:0 step:904 [D loss: 0.127767, acc.: 89.84%] [G loss: 0.859657]\n",
      "epoch:0 step:905 [D loss: 0.116915, acc.: 85.16%] [G loss: 1.054863]\n",
      "epoch:0 step:906 [D loss: 0.138836, acc.: 85.94%] [G loss: 1.027432]\n",
      "epoch:0 step:907 [D loss: 0.081772, acc.: 92.97%] [G loss: 1.023391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:908 [D loss: 0.125503, acc.: 91.41%] [G loss: 1.119227]\n",
      "epoch:0 step:909 [D loss: 0.136254, acc.: 84.38%] [G loss: 1.207936]\n",
      "epoch:0 step:910 [D loss: 0.148876, acc.: 85.16%] [G loss: 0.986808]\n",
      "epoch:0 step:911 [D loss: 0.076895, acc.: 96.88%] [G loss: 1.038185]\n",
      "epoch:0 step:912 [D loss: 0.078643, acc.: 96.88%] [G loss: 1.118022]\n",
      "epoch:0 step:913 [D loss: 0.176067, acc.: 76.56%] [G loss: 1.000354]\n",
      "epoch:0 step:914 [D loss: 0.079662, acc.: 94.53%] [G loss: 1.101835]\n",
      "epoch:0 step:915 [D loss: 0.219584, acc.: 65.62%] [G loss: 0.979762]\n",
      "epoch:0 step:916 [D loss: 0.103450, acc.: 90.62%] [G loss: 1.297833]\n",
      "epoch:0 step:917 [D loss: 0.077395, acc.: 93.75%] [G loss: 0.997803]\n",
      "epoch:0 step:918 [D loss: 0.094486, acc.: 94.53%] [G loss: 1.189355]\n",
      "epoch:0 step:919 [D loss: 0.188967, acc.: 75.78%] [G loss: 1.141924]\n",
      "epoch:0 step:920 [D loss: 0.299706, acc.: 49.22%] [G loss: 1.007747]\n",
      "epoch:0 step:921 [D loss: 0.061898, acc.: 97.66%] [G loss: 1.013937]\n",
      "epoch:0 step:922 [D loss: 0.143813, acc.: 88.28%] [G loss: 1.034258]\n",
      "epoch:0 step:923 [D loss: 0.080138, acc.: 92.19%] [G loss: 1.003249]\n",
      "epoch:0 step:924 [D loss: 0.068361, acc.: 96.09%] [G loss: 1.141917]\n",
      "epoch:0 step:925 [D loss: 0.076676, acc.: 94.53%] [G loss: 0.939846]\n",
      "epoch:0 step:926 [D loss: 0.088088, acc.: 94.53%] [G loss: 1.119967]\n",
      "epoch:0 step:927 [D loss: 0.166218, acc.: 75.00%] [G loss: 1.097197]\n",
      "epoch:0 step:928 [D loss: 0.160466, acc.: 77.34%] [G loss: 1.182722]\n",
      "epoch:0 step:929 [D loss: 0.032588, acc.: 97.66%] [G loss: 1.073329]\n",
      "epoch:0 step:930 [D loss: 0.154841, acc.: 85.16%] [G loss: 0.863252]\n",
      "epoch:0 step:931 [D loss: 0.150024, acc.: 81.25%] [G loss: 1.047667]\n",
      "epoch:0 step:932 [D loss: 0.124623, acc.: 85.94%] [G loss: 1.002121]\n",
      "epoch:0 step:933 [D loss: 0.117698, acc.: 87.50%] [G loss: 1.020549]\n",
      "epoch:0 step:934 [D loss: 0.089639, acc.: 90.62%] [G loss: 1.042706]\n",
      "epoch:0 step:935 [D loss: 0.073537, acc.: 93.75%] [G loss: 1.082954]\n",
      "epoch:0 step:936 [D loss: 0.077769, acc.: 94.53%] [G loss: 1.014650]\n",
      "epoch:0 step:937 [D loss: 0.343642, acc.: 35.16%] [G loss: 0.867247]\n",
      "epoch:1 step:938 [D loss: 0.079339, acc.: 92.19%] [G loss: 1.076238]\n",
      "epoch:1 step:939 [D loss: 0.105742, acc.: 91.41%] [G loss: 0.977524]\n",
      "epoch:1 step:940 [D loss: 0.129314, acc.: 85.16%] [G loss: 0.936078]\n",
      "epoch:1 step:941 [D loss: 0.074066, acc.: 96.09%] [G loss: 1.037625]\n",
      "epoch:1 step:942 [D loss: 0.123186, acc.: 85.94%] [G loss: 1.043964]\n",
      "epoch:1 step:943 [D loss: 0.096889, acc.: 88.28%] [G loss: 0.930598]\n",
      "epoch:1 step:944 [D loss: 0.071485, acc.: 96.88%] [G loss: 1.072524]\n",
      "epoch:1 step:945 [D loss: 0.184195, acc.: 75.00%] [G loss: 0.824961]\n",
      "epoch:1 step:946 [D loss: 0.081937, acc.: 95.31%] [G loss: 1.032615]\n",
      "epoch:1 step:947 [D loss: 0.121756, acc.: 86.72%] [G loss: 1.078514]\n",
      "epoch:1 step:948 [D loss: 0.076158, acc.: 94.53%] [G loss: 1.029842]\n",
      "epoch:1 step:949 [D loss: 0.107777, acc.: 90.62%] [G loss: 1.048048]\n",
      "epoch:1 step:950 [D loss: 0.046597, acc.: 97.66%] [G loss: 1.045209]\n",
      "epoch:1 step:951 [D loss: 0.112589, acc.: 92.97%] [G loss: 0.986714]\n",
      "epoch:1 step:952 [D loss: 0.115792, acc.: 89.06%] [G loss: 1.043505]\n",
      "epoch:1 step:953 [D loss: 0.083521, acc.: 93.75%] [G loss: 0.976238]\n",
      "epoch:1 step:954 [D loss: 0.154950, acc.: 82.03%] [G loss: 1.029237]\n",
      "epoch:1 step:955 [D loss: 0.137103, acc.: 85.94%] [G loss: 1.026742]\n",
      "epoch:1 step:956 [D loss: 0.103673, acc.: 92.97%] [G loss: 1.015960]\n",
      "epoch:1 step:957 [D loss: 0.093188, acc.: 92.19%] [G loss: 1.119344]\n",
      "epoch:1 step:958 [D loss: 0.152619, acc.: 82.81%] [G loss: 0.952018]\n",
      "epoch:1 step:959 [D loss: 0.080904, acc.: 90.62%] [G loss: 1.045628]\n",
      "epoch:1 step:960 [D loss: 0.185095, acc.: 75.00%] [G loss: 0.898360]\n",
      "epoch:1 step:961 [D loss: 0.110393, acc.: 91.41%] [G loss: 1.205327]\n",
      "epoch:1 step:962 [D loss: 0.147260, acc.: 85.94%] [G loss: 0.992978]\n",
      "epoch:1 step:963 [D loss: 0.123633, acc.: 91.41%] [G loss: 1.006193]\n",
      "epoch:1 step:964 [D loss: 0.133056, acc.: 84.38%] [G loss: 1.107004]\n",
      "epoch:1 step:965 [D loss: 0.086929, acc.: 94.53%] [G loss: 1.051760]\n",
      "epoch:1 step:966 [D loss: 0.083344, acc.: 96.88%] [G loss: 1.110195]\n",
      "epoch:1 step:967 [D loss: 0.138329, acc.: 86.72%] [G loss: 1.152394]\n",
      "epoch:1 step:968 [D loss: 0.103689, acc.: 92.97%] [G loss: 1.016396]\n",
      "epoch:1 step:969 [D loss: 0.131628, acc.: 89.84%] [G loss: 1.275846]\n",
      "epoch:1 step:970 [D loss: 0.152222, acc.: 82.03%] [G loss: 0.939183]\n",
      "epoch:1 step:971 [D loss: 0.086218, acc.: 92.97%] [G loss: 1.120818]\n",
      "epoch:1 step:972 [D loss: 0.148310, acc.: 83.59%] [G loss: 0.949978]\n",
      "epoch:1 step:973 [D loss: 0.077289, acc.: 93.75%] [G loss: 1.158876]\n",
      "epoch:1 step:974 [D loss: 0.187498, acc.: 78.12%] [G loss: 1.051402]\n",
      "epoch:1 step:975 [D loss: 0.157425, acc.: 83.59%] [G loss: 1.032269]\n",
      "epoch:1 step:976 [D loss: 0.073773, acc.: 96.88%] [G loss: 1.081643]\n",
      "epoch:1 step:977 [D loss: 0.135011, acc.: 84.38%] [G loss: 0.939170]\n",
      "epoch:1 step:978 [D loss: 0.203496, acc.: 70.31%] [G loss: 1.187381]\n",
      "epoch:1 step:979 [D loss: 0.163952, acc.: 75.00%] [G loss: 1.038983]\n",
      "epoch:1 step:980 [D loss: 0.151488, acc.: 81.25%] [G loss: 1.012482]\n",
      "epoch:1 step:981 [D loss: 0.140367, acc.: 81.25%] [G loss: 1.052244]\n",
      "epoch:1 step:982 [D loss: 0.107284, acc.: 89.84%] [G loss: 1.070823]\n",
      "epoch:1 step:983 [D loss: 0.153692, acc.: 82.81%] [G loss: 1.023447]\n",
      "epoch:1 step:984 [D loss: 0.073788, acc.: 92.97%] [G loss: 1.171634]\n",
      "epoch:1 step:985 [D loss: 0.213992, acc.: 71.09%] [G loss: 0.876456]\n",
      "epoch:1 step:986 [D loss: 0.055617, acc.: 96.88%] [G loss: 1.178429]\n",
      "epoch:1 step:987 [D loss: 0.111539, acc.: 89.84%] [G loss: 1.003296]\n",
      "epoch:1 step:988 [D loss: 0.069255, acc.: 91.41%] [G loss: 1.102925]\n",
      "epoch:1 step:989 [D loss: 0.118011, acc.: 85.16%] [G loss: 1.131536]\n",
      "epoch:1 step:990 [D loss: 0.161726, acc.: 87.50%] [G loss: 1.056083]\n",
      "epoch:1 step:991 [D loss: 0.109509, acc.: 89.84%] [G loss: 1.055135]\n",
      "epoch:1 step:992 [D loss: 0.129515, acc.: 82.03%] [G loss: 1.026379]\n",
      "epoch:1 step:993 [D loss: 0.134653, acc.: 86.72%] [G loss: 0.997076]\n",
      "epoch:1 step:994 [D loss: 0.111790, acc.: 88.28%] [G loss: 1.029653]\n",
      "epoch:1 step:995 [D loss: 0.079894, acc.: 92.97%] [G loss: 1.204670]\n",
      "epoch:1 step:996 [D loss: 0.142478, acc.: 81.25%] [G loss: 1.028598]\n",
      "epoch:1 step:997 [D loss: 0.066446, acc.: 95.31%] [G loss: 1.033651]\n",
      "epoch:1 step:998 [D loss: 0.115310, acc.: 89.84%] [G loss: 1.094656]\n",
      "epoch:1 step:999 [D loss: 0.124730, acc.: 85.16%] [G loss: 1.158507]\n",
      "epoch:1 step:1000 [D loss: 0.173761, acc.: 75.00%] [G loss: 1.033942]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 3.152479\n",
      "FID: 145.941910\n",
      "0 = 16.403909711599248\n",
      "1 = 0.19712839924573164\n",
      "2 = 0.9987499713897705\n",
      "3 = 0.9975000023841858\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9975000023841858\n",
      "7 = 13.446993141317368\n",
      "8 = 0.22650856165251698\n",
      "9 = 0.9922500252723694\n",
      "10 = 0.9847000241279602\n",
      "11 = 0.9998000264167786\n",
      "12 = 0.9997969269752502\n",
      "13 = 0.9847000241279602\n",
      "14 = 3.15248441696167\n",
      "15 = 7.296423435211182\n",
      "16 = 0.5751596093177795\n",
      "17 = 3.1524791717529297\n",
      "18 = 145.94190979003906\n",
      "epoch:1 step:1001 [D loss: 0.072465, acc.: 94.53%] [G loss: 1.105393]\n",
      "epoch:1 step:1002 [D loss: 0.153954, acc.: 85.94%] [G loss: 1.040151]\n",
      "epoch:1 step:1003 [D loss: 0.129078, acc.: 87.50%] [G loss: 1.051391]\n",
      "epoch:1 step:1004 [D loss: 0.131870, acc.: 88.28%] [G loss: 1.047547]\n",
      "epoch:1 step:1005 [D loss: 0.096651, acc.: 91.41%] [G loss: 1.066806]\n",
      "epoch:1 step:1006 [D loss: 0.109352, acc.: 85.94%] [G loss: 1.066789]\n",
      "epoch:1 step:1007 [D loss: 0.120990, acc.: 84.38%] [G loss: 1.138203]\n",
      "epoch:1 step:1008 [D loss: 0.195845, acc.: 70.31%] [G loss: 1.145797]\n",
      "epoch:1 step:1009 [D loss: 0.149643, acc.: 84.38%] [G loss: 0.997790]\n",
      "epoch:1 step:1010 [D loss: 0.103669, acc.: 89.84%] [G loss: 1.128019]\n",
      "epoch:1 step:1011 [D loss: 0.109577, acc.: 89.06%] [G loss: 1.033712]\n",
      "epoch:1 step:1012 [D loss: 0.199162, acc.: 69.53%] [G loss: 1.146709]\n",
      "epoch:1 step:1013 [D loss: 0.100375, acc.: 92.97%] [G loss: 1.053260]\n",
      "epoch:1 step:1014 [D loss: 0.090107, acc.: 91.41%] [G loss: 1.035548]\n",
      "epoch:1 step:1015 [D loss: 0.124362, acc.: 89.84%] [G loss: 1.098373]\n",
      "epoch:1 step:1016 [D loss: 0.131390, acc.: 83.59%] [G loss: 0.996827]\n",
      "epoch:1 step:1017 [D loss: 0.075324, acc.: 92.97%] [G loss: 1.107811]\n",
      "epoch:1 step:1018 [D loss: 0.168521, acc.: 76.56%] [G loss: 1.096588]\n",
      "epoch:1 step:1019 [D loss: 0.076233, acc.: 92.19%] [G loss: 1.060043]\n",
      "epoch:1 step:1020 [D loss: 0.133981, acc.: 85.16%] [G loss: 1.162370]\n",
      "epoch:1 step:1021 [D loss: 0.127354, acc.: 84.38%] [G loss: 1.087517]\n",
      "epoch:1 step:1022 [D loss: 0.112867, acc.: 89.06%] [G loss: 1.179462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1023 [D loss: 0.143348, acc.: 78.91%] [G loss: 1.171425]\n",
      "epoch:1 step:1024 [D loss: 0.091880, acc.: 88.28%] [G loss: 1.161030]\n",
      "epoch:1 step:1025 [D loss: 0.136979, acc.: 82.81%] [G loss: 1.184432]\n",
      "epoch:1 step:1026 [D loss: 0.186424, acc.: 75.00%] [G loss: 0.996913]\n",
      "epoch:1 step:1027 [D loss: 0.098391, acc.: 90.62%] [G loss: 1.116908]\n",
      "epoch:1 step:1028 [D loss: 0.125805, acc.: 87.50%] [G loss: 1.067249]\n",
      "epoch:1 step:1029 [D loss: 0.140226, acc.: 85.94%] [G loss: 1.045568]\n",
      "epoch:1 step:1030 [D loss: 0.085807, acc.: 89.06%] [G loss: 1.041771]\n",
      "epoch:1 step:1031 [D loss: 0.100214, acc.: 92.19%] [G loss: 1.125508]\n",
      "epoch:1 step:1032 [D loss: 0.149328, acc.: 86.72%] [G loss: 1.016661]\n",
      "epoch:1 step:1033 [D loss: 0.059253, acc.: 94.53%] [G loss: 1.020011]\n",
      "epoch:1 step:1034 [D loss: 0.135638, acc.: 85.16%] [G loss: 1.064178]\n",
      "epoch:1 step:1035 [D loss: 0.120189, acc.: 85.94%] [G loss: 0.958278]\n",
      "epoch:1 step:1036 [D loss: 0.083623, acc.: 91.41%] [G loss: 1.075789]\n",
      "epoch:1 step:1037 [D loss: 0.117445, acc.: 89.06%] [G loss: 1.072526]\n",
      "epoch:1 step:1038 [D loss: 0.141066, acc.: 84.38%] [G loss: 1.020103]\n",
      "epoch:1 step:1039 [D loss: 0.108790, acc.: 89.06%] [G loss: 1.060700]\n",
      "epoch:1 step:1040 [D loss: 0.103988, acc.: 88.28%] [G loss: 1.160944]\n",
      "epoch:1 step:1041 [D loss: 0.133122, acc.: 85.94%] [G loss: 0.995431]\n",
      "epoch:1 step:1042 [D loss: 0.110096, acc.: 90.62%] [G loss: 1.120535]\n",
      "epoch:1 step:1043 [D loss: 0.190331, acc.: 72.66%] [G loss: 1.031874]\n",
      "epoch:1 step:1044 [D loss: 0.141278, acc.: 81.25%] [G loss: 1.082250]\n",
      "epoch:1 step:1045 [D loss: 0.113218, acc.: 88.28%] [G loss: 1.205116]\n",
      "epoch:1 step:1046 [D loss: 0.131096, acc.: 88.28%] [G loss: 0.940363]\n",
      "epoch:1 step:1047 [D loss: 0.099472, acc.: 87.50%] [G loss: 1.210567]\n",
      "epoch:1 step:1048 [D loss: 0.151566, acc.: 81.25%] [G loss: 1.058192]\n",
      "epoch:1 step:1049 [D loss: 0.133794, acc.: 82.81%] [G loss: 1.106425]\n",
      "epoch:1 step:1050 [D loss: 0.107088, acc.: 87.50%] [G loss: 1.115196]\n",
      "epoch:1 step:1051 [D loss: 0.138732, acc.: 85.94%] [G loss: 1.073649]\n",
      "epoch:1 step:1052 [D loss: 0.113428, acc.: 83.59%] [G loss: 1.117872]\n",
      "epoch:1 step:1053 [D loss: 0.168110, acc.: 75.00%] [G loss: 1.069926]\n",
      "epoch:1 step:1054 [D loss: 0.105460, acc.: 88.28%] [G loss: 0.858682]\n",
      "epoch:1 step:1055 [D loss: 0.084825, acc.: 92.97%] [G loss: 1.127213]\n",
      "epoch:1 step:1056 [D loss: 0.156210, acc.: 82.03%] [G loss: 1.077639]\n",
      "epoch:1 step:1057 [D loss: 0.169796, acc.: 81.25%] [G loss: 0.963674]\n",
      "epoch:1 step:1058 [D loss: 0.128209, acc.: 87.50%] [G loss: 0.974958]\n",
      "epoch:1 step:1059 [D loss: 0.088480, acc.: 92.19%] [G loss: 1.017834]\n",
      "epoch:1 step:1060 [D loss: 0.178728, acc.: 73.44%] [G loss: 1.012294]\n",
      "epoch:1 step:1061 [D loss: 0.165798, acc.: 80.47%] [G loss: 0.941220]\n",
      "epoch:1 step:1062 [D loss: 0.093471, acc.: 89.06%] [G loss: 1.127210]\n",
      "epoch:1 step:1063 [D loss: 0.197761, acc.: 74.22%] [G loss: 0.876884]\n",
      "epoch:1 step:1064 [D loss: 0.090855, acc.: 92.19%] [G loss: 0.976877]\n",
      "epoch:1 step:1065 [D loss: 0.125223, acc.: 88.28%] [G loss: 0.891646]\n",
      "epoch:1 step:1066 [D loss: 0.145391, acc.: 82.03%] [G loss: 0.936193]\n",
      "epoch:1 step:1067 [D loss: 0.106682, acc.: 89.84%] [G loss: 1.091187]\n",
      "epoch:1 step:1068 [D loss: 0.153337, acc.: 81.25%] [G loss: 1.012900]\n",
      "epoch:1 step:1069 [D loss: 0.118892, acc.: 86.72%] [G loss: 1.020282]\n",
      "epoch:1 step:1070 [D loss: 0.061532, acc.: 96.09%] [G loss: 1.109886]\n",
      "epoch:1 step:1071 [D loss: 0.168116, acc.: 77.34%] [G loss: 0.962471]\n",
      "epoch:1 step:1072 [D loss: 0.079261, acc.: 93.75%] [G loss: 1.058088]\n",
      "epoch:1 step:1073 [D loss: 0.193322, acc.: 76.56%] [G loss: 1.117146]\n",
      "epoch:1 step:1074 [D loss: 0.228174, acc.: 60.16%] [G loss: 0.974207]\n",
      "epoch:1 step:1075 [D loss: 0.117729, acc.: 87.50%] [G loss: 0.942278]\n",
      "epoch:1 step:1076 [D loss: 0.167058, acc.: 80.47%] [G loss: 0.971936]\n",
      "epoch:1 step:1077 [D loss: 0.132758, acc.: 84.38%] [G loss: 1.095234]\n",
      "epoch:1 step:1078 [D loss: 0.167387, acc.: 81.25%] [G loss: 1.059251]\n",
      "epoch:1 step:1079 [D loss: 0.162290, acc.: 80.47%] [G loss: 1.032756]\n",
      "epoch:1 step:1080 [D loss: 0.174522, acc.: 75.00%] [G loss: 0.909013]\n",
      "epoch:1 step:1081 [D loss: 0.129491, acc.: 82.81%] [G loss: 0.979434]\n",
      "epoch:1 step:1082 [D loss: 0.160251, acc.: 79.69%] [G loss: 1.054826]\n",
      "epoch:1 step:1083 [D loss: 0.160064, acc.: 77.34%] [G loss: 1.125467]\n",
      "epoch:1 step:1084 [D loss: 0.188691, acc.: 70.31%] [G loss: 0.979297]\n",
      "epoch:1 step:1085 [D loss: 0.125295, acc.: 84.38%] [G loss: 1.081635]\n",
      "epoch:1 step:1086 [D loss: 0.131362, acc.: 83.59%] [G loss: 1.019055]\n",
      "epoch:1 step:1087 [D loss: 0.158901, acc.: 77.34%] [G loss: 0.986888]\n",
      "epoch:1 step:1088 [D loss: 0.119416, acc.: 89.06%] [G loss: 1.159027]\n",
      "epoch:1 step:1089 [D loss: 0.142535, acc.: 83.59%] [G loss: 1.131300]\n",
      "epoch:1 step:1090 [D loss: 0.191219, acc.: 75.00%] [G loss: 0.905187]\n",
      "epoch:1 step:1091 [D loss: 0.142682, acc.: 84.38%] [G loss: 0.973701]\n",
      "epoch:1 step:1092 [D loss: 0.099048, acc.: 90.62%] [G loss: 1.037404]\n",
      "epoch:1 step:1093 [D loss: 0.124138, acc.: 89.06%] [G loss: 0.993621]\n",
      "epoch:1 step:1094 [D loss: 0.097334, acc.: 90.62%] [G loss: 1.013863]\n",
      "epoch:1 step:1095 [D loss: 0.224132, acc.: 65.62%] [G loss: 0.991179]\n",
      "epoch:1 step:1096 [D loss: 0.085132, acc.: 91.41%] [G loss: 0.974610]\n",
      "epoch:1 step:1097 [D loss: 0.090262, acc.: 89.84%] [G loss: 1.083507]\n",
      "epoch:1 step:1098 [D loss: 0.124608, acc.: 84.38%] [G loss: 1.032290]\n",
      "epoch:1 step:1099 [D loss: 0.110340, acc.: 89.84%] [G loss: 1.058673]\n",
      "epoch:1 step:1100 [D loss: 0.126236, acc.: 91.41%] [G loss: 1.004404]\n",
      "epoch:1 step:1101 [D loss: 0.076714, acc.: 94.53%] [G loss: 1.023926]\n",
      "epoch:1 step:1102 [D loss: 0.136274, acc.: 86.72%] [G loss: 0.968078]\n",
      "epoch:1 step:1103 [D loss: 0.121511, acc.: 87.50%] [G loss: 0.957142]\n",
      "epoch:1 step:1104 [D loss: 0.153870, acc.: 78.12%] [G loss: 0.959589]\n",
      "epoch:1 step:1105 [D loss: 0.107798, acc.: 88.28%] [G loss: 1.014017]\n",
      "epoch:1 step:1106 [D loss: 0.092079, acc.: 92.19%] [G loss: 0.964705]\n",
      "epoch:1 step:1107 [D loss: 0.090898, acc.: 88.28%] [G loss: 1.100004]\n",
      "epoch:1 step:1108 [D loss: 0.119288, acc.: 85.94%] [G loss: 0.929786]\n",
      "epoch:1 step:1109 [D loss: 0.122601, acc.: 92.97%] [G loss: 0.993589]\n",
      "epoch:1 step:1110 [D loss: 0.099928, acc.: 89.06%] [G loss: 0.986223]\n",
      "epoch:1 step:1111 [D loss: 0.159374, acc.: 82.03%] [G loss: 0.927106]\n",
      "epoch:1 step:1112 [D loss: 0.099905, acc.: 92.19%] [G loss: 0.982029]\n",
      "epoch:1 step:1113 [D loss: 0.090116, acc.: 94.53%] [G loss: 1.003167]\n",
      "epoch:1 step:1114 [D loss: 0.155405, acc.: 78.91%] [G loss: 0.890794]\n",
      "epoch:1 step:1115 [D loss: 0.095883, acc.: 89.84%] [G loss: 1.048147]\n",
      "epoch:1 step:1116 [D loss: 0.153360, acc.: 82.03%] [G loss: 0.852283]\n",
      "epoch:1 step:1117 [D loss: 0.103661, acc.: 89.06%] [G loss: 1.016204]\n",
      "epoch:1 step:1118 [D loss: 0.155079, acc.: 80.47%] [G loss: 0.850136]\n",
      "epoch:1 step:1119 [D loss: 0.129297, acc.: 85.94%] [G loss: 0.896367]\n",
      "epoch:1 step:1120 [D loss: 0.070614, acc.: 95.31%] [G loss: 0.965975]\n",
      "epoch:1 step:1121 [D loss: 0.141157, acc.: 84.38%] [G loss: 0.891994]\n",
      "epoch:1 step:1122 [D loss: 0.097794, acc.: 92.19%] [G loss: 1.075079]\n",
      "epoch:1 step:1123 [D loss: 0.123948, acc.: 84.38%] [G loss: 0.900016]\n",
      "epoch:1 step:1124 [D loss: 0.104825, acc.: 88.28%] [G loss: 1.044449]\n",
      "epoch:1 step:1125 [D loss: 0.129478, acc.: 85.16%] [G loss: 0.961231]\n",
      "epoch:1 step:1126 [D loss: 0.111106, acc.: 87.50%] [G loss: 1.006214]\n",
      "epoch:1 step:1127 [D loss: 0.101406, acc.: 89.84%] [G loss: 1.008685]\n",
      "epoch:1 step:1128 [D loss: 0.135351, acc.: 83.59%] [G loss: 0.907967]\n",
      "epoch:1 step:1129 [D loss: 0.128970, acc.: 78.12%] [G loss: 1.098559]\n",
      "epoch:1 step:1130 [D loss: 0.153291, acc.: 83.59%] [G loss: 0.906332]\n",
      "epoch:1 step:1131 [D loss: 0.118917, acc.: 87.50%] [G loss: 0.971646]\n",
      "epoch:1 step:1132 [D loss: 0.139642, acc.: 85.94%] [G loss: 0.905789]\n",
      "epoch:1 step:1133 [D loss: 0.138863, acc.: 87.50%] [G loss: 0.922152]\n",
      "epoch:1 step:1134 [D loss: 0.135471, acc.: 82.81%] [G loss: 1.020535]\n",
      "epoch:1 step:1135 [D loss: 0.130500, acc.: 87.50%] [G loss: 0.898273]\n",
      "epoch:1 step:1136 [D loss: 0.156172, acc.: 87.50%] [G loss: 0.900474]\n",
      "epoch:1 step:1137 [D loss: 0.169047, acc.: 73.44%] [G loss: 0.921425]\n",
      "epoch:1 step:1138 [D loss: 0.154763, acc.: 78.91%] [G loss: 0.851914]\n",
      "epoch:1 step:1139 [D loss: 0.149320, acc.: 83.59%] [G loss: 0.934671]\n",
      "epoch:1 step:1140 [D loss: 0.124902, acc.: 85.16%] [G loss: 1.036425]\n",
      "epoch:1 step:1141 [D loss: 0.097423, acc.: 89.84%] [G loss: 0.931343]\n",
      "epoch:1 step:1142 [D loss: 0.092732, acc.: 90.62%] [G loss: 1.001333]\n",
      "epoch:1 step:1143 [D loss: 0.156159, acc.: 81.25%] [G loss: 0.830377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1144 [D loss: 0.070587, acc.: 96.09%] [G loss: 1.066970]\n",
      "epoch:1 step:1145 [D loss: 0.128749, acc.: 85.94%] [G loss: 0.867991]\n",
      "epoch:1 step:1146 [D loss: 0.135261, acc.: 84.38%] [G loss: 0.981219]\n",
      "epoch:1 step:1147 [D loss: 0.160285, acc.: 79.69%] [G loss: 0.843265]\n",
      "epoch:1 step:1148 [D loss: 0.133551, acc.: 81.25%] [G loss: 0.936252]\n",
      "epoch:1 step:1149 [D loss: 0.128552, acc.: 82.81%] [G loss: 0.976007]\n",
      "epoch:1 step:1150 [D loss: 0.187966, acc.: 67.97%] [G loss: 0.983865]\n",
      "epoch:1 step:1151 [D loss: 0.239128, acc.: 60.94%] [G loss: 0.846479]\n",
      "epoch:1 step:1152 [D loss: 0.130913, acc.: 84.38%] [G loss: 0.963387]\n",
      "epoch:1 step:1153 [D loss: 0.171775, acc.: 73.44%] [G loss: 0.889588]\n",
      "epoch:1 step:1154 [D loss: 0.130511, acc.: 85.16%] [G loss: 0.986672]\n",
      "epoch:1 step:1155 [D loss: 0.155848, acc.: 82.81%] [G loss: 0.824779]\n",
      "epoch:1 step:1156 [D loss: 0.139425, acc.: 86.72%] [G loss: 0.868546]\n",
      "epoch:1 step:1157 [D loss: 0.189610, acc.: 71.88%] [G loss: 0.764390]\n",
      "epoch:1 step:1158 [D loss: 0.097963, acc.: 92.19%] [G loss: 1.007619]\n",
      "epoch:1 step:1159 [D loss: 0.091856, acc.: 90.62%] [G loss: 1.042619]\n",
      "epoch:1 step:1160 [D loss: 0.119966, acc.: 89.84%] [G loss: 0.918705]\n",
      "epoch:1 step:1161 [D loss: 0.109477, acc.: 91.41%] [G loss: 0.909114]\n",
      "epoch:1 step:1162 [D loss: 0.168438, acc.: 79.69%] [G loss: 0.910520]\n",
      "epoch:1 step:1163 [D loss: 0.134356, acc.: 82.81%] [G loss: 0.890163]\n",
      "epoch:1 step:1164 [D loss: 0.173788, acc.: 78.12%] [G loss: 0.788522]\n",
      "epoch:1 step:1165 [D loss: 0.120862, acc.: 87.50%] [G loss: 0.910637]\n",
      "epoch:1 step:1166 [D loss: 0.181246, acc.: 72.66%] [G loss: 0.815482]\n",
      "epoch:1 step:1167 [D loss: 0.113736, acc.: 84.38%] [G loss: 1.028104]\n",
      "epoch:1 step:1168 [D loss: 0.141536, acc.: 78.91%] [G loss: 0.900528]\n",
      "epoch:1 step:1169 [D loss: 0.144222, acc.: 80.47%] [G loss: 1.070150]\n",
      "epoch:1 step:1170 [D loss: 0.219802, acc.: 64.84%] [G loss: 0.823467]\n",
      "epoch:1 step:1171 [D loss: 0.109914, acc.: 89.06%] [G loss: 1.017055]\n",
      "epoch:1 step:1172 [D loss: 0.207746, acc.: 68.75%] [G loss: 0.724339]\n",
      "epoch:1 step:1173 [D loss: 0.112406, acc.: 89.84%] [G loss: 0.937877]\n",
      "epoch:1 step:1174 [D loss: 0.114137, acc.: 89.84%] [G loss: 0.930902]\n",
      "epoch:1 step:1175 [D loss: 0.142692, acc.: 81.25%] [G loss: 0.854454]\n",
      "epoch:1 step:1176 [D loss: 0.117099, acc.: 86.72%] [G loss: 0.904421]\n",
      "epoch:1 step:1177 [D loss: 0.127236, acc.: 85.94%] [G loss: 0.936448]\n",
      "epoch:1 step:1178 [D loss: 0.169836, acc.: 77.34%] [G loss: 0.852126]\n",
      "epoch:1 step:1179 [D loss: 0.087809, acc.: 91.41%] [G loss: 0.936606]\n",
      "epoch:1 step:1180 [D loss: 0.116302, acc.: 86.72%] [G loss: 0.915896]\n",
      "epoch:1 step:1181 [D loss: 0.164142, acc.: 82.81%] [G loss: 0.866099]\n",
      "epoch:1 step:1182 [D loss: 0.110377, acc.: 91.41%] [G loss: 0.975763]\n",
      "epoch:1 step:1183 [D loss: 0.227221, acc.: 69.53%] [G loss: 0.752327]\n",
      "epoch:1 step:1184 [D loss: 0.145009, acc.: 83.59%] [G loss: 1.030098]\n",
      "epoch:1 step:1185 [D loss: 0.178052, acc.: 77.34%] [G loss: 0.855284]\n",
      "epoch:1 step:1186 [D loss: 0.147525, acc.: 83.59%] [G loss: 0.949409]\n",
      "epoch:1 step:1187 [D loss: 0.124128, acc.: 85.94%] [G loss: 0.949692]\n",
      "epoch:1 step:1188 [D loss: 0.152866, acc.: 83.59%] [G loss: 0.836312]\n",
      "epoch:1 step:1189 [D loss: 0.136255, acc.: 85.16%] [G loss: 0.979638]\n",
      "epoch:1 step:1190 [D loss: 0.140408, acc.: 83.59%] [G loss: 0.851523]\n",
      "epoch:1 step:1191 [D loss: 0.135156, acc.: 86.72%] [G loss: 0.845735]\n",
      "epoch:1 step:1192 [D loss: 0.134128, acc.: 85.16%] [G loss: 0.872618]\n",
      "epoch:1 step:1193 [D loss: 0.135841, acc.: 86.72%] [G loss: 0.835628]\n",
      "epoch:1 step:1194 [D loss: 0.133918, acc.: 83.59%] [G loss: 0.870409]\n",
      "epoch:1 step:1195 [D loss: 0.153950, acc.: 76.56%] [G loss: 0.841586]\n",
      "epoch:1 step:1196 [D loss: 0.123955, acc.: 86.72%] [G loss: 0.963559]\n",
      "epoch:1 step:1197 [D loss: 0.153189, acc.: 82.03%] [G loss: 0.891506]\n",
      "epoch:1 step:1198 [D loss: 0.151008, acc.: 82.81%] [G loss: 0.859328]\n",
      "epoch:1 step:1199 [D loss: 0.137625, acc.: 84.38%] [G loss: 0.830831]\n",
      "epoch:1 step:1200 [D loss: 0.239451, acc.: 58.59%] [G loss: 0.860922]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 3.201148\n",
      "FID: 139.088593\n",
      "0 = 15.95582917413715\n",
      "1 = 0.17068555379209874\n",
      "2 = 0.9983999729156494\n",
      "3 = 0.9968000054359436\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9968000054359436\n",
      "7 = 13.29361783219572\n",
      "8 = 0.21662689968889653\n",
      "9 = 0.9891499876976013\n",
      "10 = 0.979200005531311\n",
      "11 = 0.9991000294685364\n",
      "12 = 0.9990817308425903\n",
      "13 = 0.979200005531311\n",
      "14 = 3.2011537551879883\n",
      "15 = 7.873729705810547\n",
      "16 = 0.47308042645454407\n",
      "17 = 3.2011475563049316\n",
      "18 = 139.08859252929688\n",
      "epoch:1 step:1201 [D loss: 0.127780, acc.: 84.38%] [G loss: 0.937856]\n",
      "epoch:1 step:1202 [D loss: 0.199176, acc.: 73.44%] [G loss: 0.698053]\n",
      "epoch:1 step:1203 [D loss: 0.098804, acc.: 93.75%] [G loss: 1.007064]\n",
      "epoch:1 step:1204 [D loss: 0.211081, acc.: 70.31%] [G loss: 0.710393]\n",
      "epoch:1 step:1205 [D loss: 0.144453, acc.: 81.25%] [G loss: 0.955920]\n",
      "epoch:1 step:1206 [D loss: 0.175902, acc.: 79.69%] [G loss: 0.813222]\n",
      "epoch:1 step:1207 [D loss: 0.121266, acc.: 85.94%] [G loss: 0.859308]\n",
      "epoch:1 step:1208 [D loss: 0.114153, acc.: 89.06%] [G loss: 0.990352]\n",
      "epoch:1 step:1209 [D loss: 0.172490, acc.: 76.56%] [G loss: 0.769999]\n",
      "epoch:1 step:1210 [D loss: 0.120462, acc.: 85.16%] [G loss: 0.962905]\n",
      "epoch:1 step:1211 [D loss: 0.128419, acc.: 83.59%] [G loss: 0.887963]\n",
      "epoch:1 step:1212 [D loss: 0.185796, acc.: 75.00%] [G loss: 0.994071]\n",
      "epoch:1 step:1213 [D loss: 0.172921, acc.: 74.22%] [G loss: 0.807906]\n",
      "epoch:1 step:1214 [D loss: 0.207076, acc.: 63.28%] [G loss: 0.804583]\n",
      "epoch:1 step:1215 [D loss: 0.178889, acc.: 74.22%] [G loss: 0.961390]\n",
      "epoch:1 step:1216 [D loss: 0.180897, acc.: 74.22%] [G loss: 0.750749]\n",
      "epoch:1 step:1217 [D loss: 0.173841, acc.: 72.66%] [G loss: 0.744919]\n",
      "epoch:1 step:1218 [D loss: 0.114109, acc.: 87.50%] [G loss: 0.888668]\n",
      "epoch:1 step:1219 [D loss: 0.168889, acc.: 75.00%] [G loss: 0.811461]\n",
      "epoch:1 step:1220 [D loss: 0.173536, acc.: 76.56%] [G loss: 0.754650]\n",
      "epoch:1 step:1221 [D loss: 0.075635, acc.: 93.75%] [G loss: 0.973145]\n",
      "epoch:1 step:1222 [D loss: 0.143862, acc.: 83.59%] [G loss: 0.836377]\n",
      "epoch:1 step:1223 [D loss: 0.153315, acc.: 77.34%] [G loss: 0.859918]\n",
      "epoch:1 step:1224 [D loss: 0.172400, acc.: 75.78%] [G loss: 0.851858]\n",
      "epoch:1 step:1225 [D loss: 0.167601, acc.: 78.12%] [G loss: 0.882107]\n",
      "epoch:1 step:1226 [D loss: 0.189562, acc.: 72.66%] [G loss: 0.813324]\n",
      "epoch:1 step:1227 [D loss: 0.141883, acc.: 78.91%] [G loss: 0.872362]\n",
      "epoch:1 step:1228 [D loss: 0.164684, acc.: 77.34%] [G loss: 0.843794]\n",
      "epoch:1 step:1229 [D loss: 0.200212, acc.: 70.31%] [G loss: 0.901941]\n",
      "epoch:1 step:1230 [D loss: 0.177604, acc.: 75.00%] [G loss: 0.862484]\n",
      "epoch:1 step:1231 [D loss: 0.128021, acc.: 85.94%] [G loss: 0.902786]\n",
      "epoch:1 step:1232 [D loss: 0.186529, acc.: 67.97%] [G loss: 0.887644]\n",
      "epoch:1 step:1233 [D loss: 0.128007, acc.: 87.50%] [G loss: 0.891818]\n",
      "epoch:1 step:1234 [D loss: 0.225865, acc.: 64.06%] [G loss: 0.769604]\n",
      "epoch:1 step:1235 [D loss: 0.138078, acc.: 83.59%] [G loss: 0.933630]\n",
      "epoch:1 step:1236 [D loss: 0.201895, acc.: 74.22%] [G loss: 0.766778]\n",
      "epoch:1 step:1237 [D loss: 0.114147, acc.: 89.06%] [G loss: 0.955944]\n",
      "epoch:1 step:1238 [D loss: 0.254978, acc.: 62.50%] [G loss: 0.763006]\n",
      "epoch:1 step:1239 [D loss: 0.125327, acc.: 83.59%] [G loss: 0.784292]\n",
      "epoch:1 step:1240 [D loss: 0.127516, acc.: 85.16%] [G loss: 0.765615]\n",
      "epoch:1 step:1241 [D loss: 0.137565, acc.: 81.25%] [G loss: 0.853370]\n",
      "epoch:1 step:1242 [D loss: 0.138952, acc.: 85.16%] [G loss: 0.905173]\n",
      "epoch:1 step:1243 [D loss: 0.202818, acc.: 69.53%] [G loss: 0.738586]\n",
      "epoch:1 step:1244 [D loss: 0.101425, acc.: 92.97%] [G loss: 0.934446]\n",
      "epoch:1 step:1245 [D loss: 0.164401, acc.: 78.12%] [G loss: 0.841601]\n",
      "epoch:1 step:1246 [D loss: 0.158065, acc.: 82.03%] [G loss: 0.811838]\n",
      "epoch:1 step:1247 [D loss: 0.136860, acc.: 85.16%] [G loss: 0.892547]\n",
      "epoch:1 step:1248 [D loss: 0.120742, acc.: 89.06%] [G loss: 0.861744]\n",
      "epoch:1 step:1249 [D loss: 0.136492, acc.: 86.72%] [G loss: 0.791726]\n",
      "epoch:1 step:1250 [D loss: 0.124754, acc.: 89.06%] [G loss: 0.920765]\n",
      "epoch:1 step:1251 [D loss: 0.170074, acc.: 78.91%] [G loss: 0.888595]\n",
      "epoch:1 step:1252 [D loss: 0.153333, acc.: 79.69%] [G loss: 0.823188]\n",
      "epoch:1 step:1253 [D loss: 0.226929, acc.: 63.28%] [G loss: 0.696197]\n",
      "epoch:1 step:1254 [D loss: 0.136897, acc.: 82.81%] [G loss: 0.915618]\n",
      "epoch:1 step:1255 [D loss: 0.130145, acc.: 87.50%] [G loss: 0.825097]\n",
      "epoch:1 step:1256 [D loss: 0.129506, acc.: 85.16%] [G loss: 0.818205]\n",
      "epoch:1 step:1257 [D loss: 0.164154, acc.: 79.69%] [G loss: 0.869467]\n",
      "epoch:1 step:1258 [D loss: 0.136482, acc.: 82.81%] [G loss: 0.853574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1259 [D loss: 0.126231, acc.: 89.06%] [G loss: 0.888495]\n",
      "epoch:1 step:1260 [D loss: 0.144291, acc.: 82.03%] [G loss: 0.883315]\n",
      "epoch:1 step:1261 [D loss: 0.138686, acc.: 82.81%] [G loss: 0.885474]\n",
      "epoch:1 step:1262 [D loss: 0.154508, acc.: 80.47%] [G loss: 0.816234]\n",
      "epoch:1 step:1263 [D loss: 0.145020, acc.: 82.81%] [G loss: 0.860609]\n",
      "epoch:1 step:1264 [D loss: 0.139212, acc.: 82.81%] [G loss: 0.797638]\n",
      "epoch:1 step:1265 [D loss: 0.183919, acc.: 73.44%] [G loss: 0.923110]\n",
      "epoch:1 step:1266 [D loss: 0.155819, acc.: 78.91%] [G loss: 0.881861]\n",
      "epoch:1 step:1267 [D loss: 0.232417, acc.: 63.28%] [G loss: 0.719766]\n",
      "epoch:1 step:1268 [D loss: 0.147174, acc.: 82.03%] [G loss: 0.887962]\n",
      "epoch:1 step:1269 [D loss: 0.134259, acc.: 85.16%] [G loss: 0.885186]\n",
      "epoch:1 step:1270 [D loss: 0.170412, acc.: 78.91%] [G loss: 0.854220]\n",
      "epoch:1 step:1271 [D loss: 0.173763, acc.: 78.91%] [G loss: 0.857404]\n",
      "epoch:1 step:1272 [D loss: 0.149850, acc.: 83.59%] [G loss: 0.876506]\n",
      "epoch:1 step:1273 [D loss: 0.146098, acc.: 84.38%] [G loss: 0.867599]\n",
      "epoch:1 step:1274 [D loss: 0.168345, acc.: 78.91%] [G loss: 0.797047]\n",
      "epoch:1 step:1275 [D loss: 0.179433, acc.: 74.22%] [G loss: 0.781887]\n",
      "epoch:1 step:1276 [D loss: 0.171074, acc.: 75.78%] [G loss: 0.852258]\n",
      "epoch:1 step:1277 [D loss: 0.162266, acc.: 81.25%] [G loss: 0.823106]\n",
      "epoch:1 step:1278 [D loss: 0.104454, acc.: 92.19%] [G loss: 0.882659]\n",
      "epoch:1 step:1279 [D loss: 0.124457, acc.: 86.72%] [G loss: 0.751301]\n",
      "epoch:1 step:1280 [D loss: 0.089573, acc.: 93.75%] [G loss: 0.977092]\n",
      "epoch:1 step:1281 [D loss: 0.167645, acc.: 80.47%] [G loss: 0.723133]\n",
      "epoch:1 step:1282 [D loss: 0.169822, acc.: 76.56%] [G loss: 0.725446]\n",
      "epoch:1 step:1283 [D loss: 0.152410, acc.: 78.12%] [G loss: 1.020850]\n",
      "epoch:1 step:1284 [D loss: 0.200721, acc.: 71.09%] [G loss: 0.795307]\n",
      "epoch:1 step:1285 [D loss: 0.179973, acc.: 75.78%] [G loss: 0.788187]\n",
      "epoch:1 step:1286 [D loss: 0.187198, acc.: 75.78%] [G loss: 0.692200]\n",
      "epoch:1 step:1287 [D loss: 0.106118, acc.: 91.41%] [G loss: 0.828240]\n",
      "epoch:1 step:1288 [D loss: 0.158161, acc.: 80.47%] [G loss: 0.809254]\n",
      "epoch:1 step:1289 [D loss: 0.161620, acc.: 78.91%] [G loss: 0.752469]\n",
      "epoch:1 step:1290 [D loss: 0.097766, acc.: 91.41%] [G loss: 0.928793]\n",
      "epoch:1 step:1291 [D loss: 0.127486, acc.: 83.59%] [G loss: 0.780024]\n",
      "epoch:1 step:1292 [D loss: 0.098315, acc.: 91.41%] [G loss: 0.891543]\n",
      "epoch:1 step:1293 [D loss: 0.138943, acc.: 83.59%] [G loss: 0.751042]\n",
      "epoch:1 step:1294 [D loss: 0.109776, acc.: 91.41%] [G loss: 0.847093]\n",
      "epoch:1 step:1295 [D loss: 0.095338, acc.: 93.75%] [G loss: 0.916537]\n",
      "epoch:1 step:1296 [D loss: 0.148159, acc.: 82.03%] [G loss: 0.766060]\n",
      "epoch:1 step:1297 [D loss: 0.122371, acc.: 87.50%] [G loss: 0.798583]\n",
      "epoch:1 step:1298 [D loss: 0.094603, acc.: 94.53%] [G loss: 0.923513]\n",
      "epoch:1 step:1299 [D loss: 0.190630, acc.: 73.44%] [G loss: 0.730683]\n",
      "epoch:1 step:1300 [D loss: 0.151668, acc.: 79.69%] [G loss: 0.877690]\n",
      "epoch:1 step:1301 [D loss: 0.134231, acc.: 85.94%] [G loss: 0.768413]\n",
      "epoch:1 step:1302 [D loss: 0.170903, acc.: 78.12%] [G loss: 0.814255]\n",
      "epoch:1 step:1303 [D loss: 0.162100, acc.: 82.03%] [G loss: 0.742011]\n",
      "epoch:1 step:1304 [D loss: 0.171271, acc.: 78.91%] [G loss: 0.817051]\n",
      "epoch:1 step:1305 [D loss: 0.125867, acc.: 85.16%] [G loss: 0.793921]\n",
      "epoch:1 step:1306 [D loss: 0.160458, acc.: 81.25%] [G loss: 0.722750]\n",
      "epoch:1 step:1307 [D loss: 0.145806, acc.: 80.47%] [G loss: 0.925012]\n",
      "epoch:1 step:1308 [D loss: 0.139035, acc.: 81.25%] [G loss: 0.831402]\n",
      "epoch:1 step:1309 [D loss: 0.128284, acc.: 87.50%] [G loss: 0.853501]\n",
      "epoch:1 step:1310 [D loss: 0.178031, acc.: 78.12%] [G loss: 0.718391]\n",
      "epoch:1 step:1311 [D loss: 0.129281, acc.: 88.28%] [G loss: 0.858912]\n",
      "epoch:1 step:1312 [D loss: 0.176271, acc.: 77.34%] [G loss: 0.741653]\n",
      "epoch:1 step:1313 [D loss: 0.215493, acc.: 64.06%] [G loss: 0.723127]\n",
      "epoch:1 step:1314 [D loss: 0.113893, acc.: 89.06%] [G loss: 0.840770]\n",
      "epoch:1 step:1315 [D loss: 0.165022, acc.: 78.12%] [G loss: 0.690590]\n",
      "epoch:1 step:1316 [D loss: 0.097458, acc.: 92.19%] [G loss: 0.786314]\n",
      "epoch:1 step:1317 [D loss: 0.168323, acc.: 76.56%] [G loss: 0.755775]\n",
      "epoch:1 step:1318 [D loss: 0.125706, acc.: 84.38%] [G loss: 0.868836]\n",
      "epoch:1 step:1319 [D loss: 0.122276, acc.: 85.16%] [G loss: 0.778366]\n",
      "epoch:1 step:1320 [D loss: 0.159388, acc.: 75.78%] [G loss: 0.743229]\n",
      "epoch:1 step:1321 [D loss: 0.139924, acc.: 84.38%] [G loss: 0.832088]\n",
      "epoch:1 step:1322 [D loss: 0.132965, acc.: 80.47%] [G loss: 0.818104]\n",
      "epoch:1 step:1323 [D loss: 0.150835, acc.: 81.25%] [G loss: 0.765295]\n",
      "epoch:1 step:1324 [D loss: 0.159025, acc.: 79.69%] [G loss: 0.763089]\n",
      "epoch:1 step:1325 [D loss: 0.149268, acc.: 86.72%] [G loss: 0.767177]\n",
      "epoch:1 step:1326 [D loss: 0.193524, acc.: 74.22%] [G loss: 0.729156]\n",
      "epoch:1 step:1327 [D loss: 0.190419, acc.: 73.44%] [G loss: 0.679262]\n",
      "epoch:1 step:1328 [D loss: 0.121824, acc.: 85.94%] [G loss: 0.835842]\n",
      "epoch:1 step:1329 [D loss: 0.141232, acc.: 84.38%] [G loss: 0.755965]\n",
      "epoch:1 step:1330 [D loss: 0.175251, acc.: 74.22%] [G loss: 0.783908]\n",
      "epoch:1 step:1331 [D loss: 0.122533, acc.: 88.28%] [G loss: 0.782368]\n",
      "epoch:1 step:1332 [D loss: 0.136100, acc.: 83.59%] [G loss: 0.833572]\n",
      "epoch:1 step:1333 [D loss: 0.137121, acc.: 82.03%] [G loss: 0.774314]\n",
      "epoch:1 step:1334 [D loss: 0.114701, acc.: 89.06%] [G loss: 0.753252]\n",
      "epoch:1 step:1335 [D loss: 0.116453, acc.: 87.50%] [G loss: 0.871693]\n",
      "epoch:1 step:1336 [D loss: 0.098400, acc.: 91.41%] [G loss: 0.835034]\n",
      "epoch:1 step:1337 [D loss: 0.214029, acc.: 69.53%] [G loss: 0.649417]\n",
      "epoch:1 step:1338 [D loss: 0.105880, acc.: 94.53%] [G loss: 0.867772]\n",
      "epoch:1 step:1339 [D loss: 0.114183, acc.: 90.62%] [G loss: 0.814606]\n",
      "epoch:1 step:1340 [D loss: 0.195002, acc.: 71.88%] [G loss: 0.682605]\n",
      "epoch:1 step:1341 [D loss: 0.167234, acc.: 75.78%] [G loss: 0.791205]\n",
      "epoch:1 step:1342 [D loss: 0.149918, acc.: 84.38%] [G loss: 0.732125]\n",
      "epoch:1 step:1343 [D loss: 0.161413, acc.: 78.12%] [G loss: 0.821915]\n",
      "epoch:1 step:1344 [D loss: 0.133099, acc.: 82.03%] [G loss: 0.762253]\n",
      "epoch:1 step:1345 [D loss: 0.171850, acc.: 77.34%] [G loss: 0.689698]\n",
      "epoch:1 step:1346 [D loss: 0.117010, acc.: 85.94%] [G loss: 0.889064]\n",
      "epoch:1 step:1347 [D loss: 0.186457, acc.: 74.22%] [G loss: 0.676352]\n",
      "epoch:1 step:1348 [D loss: 0.174572, acc.: 75.00%] [G loss: 0.734170]\n",
      "epoch:1 step:1349 [D loss: 0.142638, acc.: 78.91%] [G loss: 0.796230]\n",
      "epoch:1 step:1350 [D loss: 0.180983, acc.: 73.44%] [G loss: 0.712252]\n",
      "epoch:1 step:1351 [D loss: 0.165041, acc.: 78.12%] [G loss: 0.774587]\n",
      "epoch:1 step:1352 [D loss: 0.160039, acc.: 78.91%] [G loss: 0.805363]\n",
      "epoch:1 step:1353 [D loss: 0.148304, acc.: 78.91%] [G loss: 0.803811]\n",
      "epoch:1 step:1354 [D loss: 0.173294, acc.: 78.91%] [G loss: 0.785992]\n",
      "epoch:1 step:1355 [D loss: 0.189291, acc.: 67.19%] [G loss: 0.702306]\n",
      "epoch:1 step:1356 [D loss: 0.111572, acc.: 89.06%] [G loss: 0.872147]\n",
      "epoch:1 step:1357 [D loss: 0.185704, acc.: 76.56%] [G loss: 0.739261]\n",
      "epoch:1 step:1358 [D loss: 0.201202, acc.: 70.31%] [G loss: 0.727863]\n",
      "epoch:1 step:1359 [D loss: 0.161445, acc.: 78.91%] [G loss: 0.829383]\n",
      "epoch:1 step:1360 [D loss: 0.173918, acc.: 72.66%] [G loss: 0.745428]\n",
      "epoch:1 step:1361 [D loss: 0.138046, acc.: 85.16%] [G loss: 0.793989]\n",
      "epoch:1 step:1362 [D loss: 0.132267, acc.: 83.59%] [G loss: 0.760191]\n",
      "epoch:1 step:1363 [D loss: 0.108373, acc.: 88.28%] [G loss: 0.835824]\n",
      "epoch:1 step:1364 [D loss: 0.137657, acc.: 82.03%] [G loss: 0.718250]\n",
      "epoch:1 step:1365 [D loss: 0.133033, acc.: 87.50%] [G loss: 0.800720]\n",
      "epoch:1 step:1366 [D loss: 0.160500, acc.: 77.34%] [G loss: 0.719464]\n",
      "epoch:1 step:1367 [D loss: 0.124320, acc.: 87.50%] [G loss: 0.839938]\n",
      "epoch:1 step:1368 [D loss: 0.156818, acc.: 82.81%] [G loss: 0.784254]\n",
      "epoch:1 step:1369 [D loss: 0.152338, acc.: 82.81%] [G loss: 0.762286]\n",
      "epoch:1 step:1370 [D loss: 0.160630, acc.: 76.56%] [G loss: 0.706695]\n",
      "epoch:1 step:1371 [D loss: 0.124949, acc.: 89.06%] [G loss: 0.789969]\n",
      "epoch:1 step:1372 [D loss: 0.115223, acc.: 89.06%] [G loss: 0.754180]\n",
      "epoch:1 step:1373 [D loss: 0.083512, acc.: 91.41%] [G loss: 0.872071]\n",
      "epoch:1 step:1374 [D loss: 0.153578, acc.: 82.03%] [G loss: 0.749763]\n",
      "epoch:1 step:1375 [D loss: 0.121093, acc.: 88.28%] [G loss: 0.854124]\n",
      "epoch:1 step:1376 [D loss: 0.122197, acc.: 86.72%] [G loss: 0.822509]\n",
      "epoch:1 step:1377 [D loss: 0.112197, acc.: 91.41%] [G loss: 0.833062]\n",
      "epoch:1 step:1378 [D loss: 0.162472, acc.: 80.47%] [G loss: 0.729893]\n",
      "epoch:1 step:1379 [D loss: 0.125705, acc.: 85.94%] [G loss: 0.822143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1380 [D loss: 0.119444, acc.: 88.28%] [G loss: 0.809392]\n",
      "epoch:1 step:1381 [D loss: 0.143076, acc.: 83.59%] [G loss: 0.798505]\n",
      "epoch:1 step:1382 [D loss: 0.148392, acc.: 85.94%] [G loss: 0.729644]\n",
      "epoch:1 step:1383 [D loss: 0.114270, acc.: 88.28%] [G loss: 0.839618]\n",
      "epoch:1 step:1384 [D loss: 0.122263, acc.: 86.72%] [G loss: 0.804498]\n",
      "epoch:1 step:1385 [D loss: 0.191275, acc.: 71.88%] [G loss: 0.731823]\n",
      "epoch:1 step:1386 [D loss: 0.156861, acc.: 82.81%] [G loss: 0.787336]\n",
      "epoch:1 step:1387 [D loss: 0.161245, acc.: 80.47%] [G loss: 0.831526]\n",
      "epoch:1 step:1388 [D loss: 0.121254, acc.: 85.94%] [G loss: 0.833188]\n",
      "epoch:1 step:1389 [D loss: 0.185255, acc.: 71.88%] [G loss: 0.806101]\n",
      "epoch:1 step:1390 [D loss: 0.147275, acc.: 86.72%] [G loss: 0.783917]\n",
      "epoch:1 step:1391 [D loss: 0.207665, acc.: 74.22%] [G loss: 0.682196]\n",
      "epoch:1 step:1392 [D loss: 0.190738, acc.: 70.31%] [G loss: 0.767651]\n",
      "epoch:1 step:1393 [D loss: 0.217781, acc.: 65.62%] [G loss: 0.707153]\n",
      "epoch:1 step:1394 [D loss: 0.140039, acc.: 84.38%] [G loss: 0.885068]\n",
      "epoch:1 step:1395 [D loss: 0.192928, acc.: 73.44%] [G loss: 0.735490]\n",
      "epoch:1 step:1396 [D loss: 0.122410, acc.: 86.72%] [G loss: 0.833419]\n",
      "epoch:1 step:1397 [D loss: 0.143440, acc.: 85.16%] [G loss: 0.810114]\n",
      "epoch:1 step:1398 [D loss: 0.144717, acc.: 82.03%] [G loss: 0.791751]\n",
      "epoch:1 step:1399 [D loss: 0.192051, acc.: 71.88%] [G loss: 0.729765]\n",
      "epoch:1 step:1400 [D loss: 0.183080, acc.: 78.12%] [G loss: 0.806842]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 3.724453\n",
      "FID: 114.623634\n",
      "0 = 15.445788289880745\n",
      "1 = 0.13667122150987554\n",
      "2 = 0.9972500205039978\n",
      "3 = 0.9944999814033508\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9944999814033508\n",
      "7 = 12.464586687040276\n",
      "8 = 0.20010448349214885\n",
      "9 = 0.9819999933242798\n",
      "10 = 0.9678000211715698\n",
      "11 = 0.9962000250816345\n",
      "12 = 0.9960889220237732\n",
      "13 = 0.9678000211715698\n",
      "14 = 3.7244620323181152\n",
      "15 = 7.788257122039795\n",
      "16 = 0.46163466572761536\n",
      "17 = 3.7244527339935303\n",
      "18 = 114.6236343383789\n",
      "epoch:1 step:1401 [D loss: 0.151381, acc.: 80.47%] [G loss: 0.788573]\n",
      "epoch:1 step:1402 [D loss: 0.227909, acc.: 57.81%] [G loss: 0.749537]\n",
      "epoch:1 step:1403 [D loss: 0.181172, acc.: 74.22%] [G loss: 0.892646]\n",
      "epoch:1 step:1404 [D loss: 0.230998, acc.: 61.72%] [G loss: 0.768983]\n",
      "epoch:1 step:1405 [D loss: 0.215988, acc.: 67.19%] [G loss: 0.818411]\n",
      "epoch:1 step:1406 [D loss: 0.209718, acc.: 67.97%] [G loss: 0.792382]\n",
      "epoch:1 step:1407 [D loss: 0.167425, acc.: 76.56%] [G loss: 0.849134]\n",
      "epoch:1 step:1408 [D loss: 0.134894, acc.: 82.03%] [G loss: 0.879637]\n",
      "epoch:1 step:1409 [D loss: 0.096218, acc.: 89.06%] [G loss: 0.926986]\n",
      "epoch:1 step:1410 [D loss: 0.216839, acc.: 69.53%] [G loss: 0.762282]\n",
      "epoch:1 step:1411 [D loss: 0.127854, acc.: 86.72%] [G loss: 0.885164]\n",
      "epoch:1 step:1412 [D loss: 0.131703, acc.: 82.03%] [G loss: 0.838827]\n",
      "epoch:1 step:1413 [D loss: 0.184904, acc.: 74.22%] [G loss: 0.730823]\n",
      "epoch:1 step:1414 [D loss: 0.186487, acc.: 74.22%] [G loss: 0.749038]\n",
      "epoch:1 step:1415 [D loss: 0.162446, acc.: 78.12%] [G loss: 0.755903]\n",
      "epoch:1 step:1416 [D loss: 0.229860, acc.: 63.28%] [G loss: 0.690491]\n",
      "epoch:1 step:1417 [D loss: 0.184024, acc.: 68.75%] [G loss: 0.761293]\n",
      "epoch:1 step:1418 [D loss: 0.220626, acc.: 61.72%] [G loss: 0.729505]\n",
      "epoch:1 step:1419 [D loss: 0.161962, acc.: 79.69%] [G loss: 0.766481]\n",
      "epoch:1 step:1420 [D loss: 0.127096, acc.: 87.50%] [G loss: 0.785888]\n",
      "epoch:1 step:1421 [D loss: 0.117860, acc.: 92.19%] [G loss: 0.796295]\n",
      "epoch:1 step:1422 [D loss: 0.150850, acc.: 81.25%] [G loss: 0.797359]\n",
      "epoch:1 step:1423 [D loss: 0.216192, acc.: 62.50%] [G loss: 0.759453]\n",
      "epoch:1 step:1424 [D loss: 0.137162, acc.: 84.38%] [G loss: 0.831071]\n",
      "epoch:1 step:1425 [D loss: 0.169347, acc.: 76.56%] [G loss: 0.806156]\n",
      "epoch:1 step:1426 [D loss: 0.191992, acc.: 76.56%] [G loss: 0.759907]\n",
      "epoch:1 step:1427 [D loss: 0.131679, acc.: 85.16%] [G loss: 0.823168]\n",
      "epoch:1 step:1428 [D loss: 0.170432, acc.: 78.12%] [G loss: 0.777298]\n",
      "epoch:1 step:1429 [D loss: 0.172032, acc.: 78.12%] [G loss: 0.804944]\n",
      "epoch:1 step:1430 [D loss: 0.185707, acc.: 75.00%] [G loss: 0.740287]\n",
      "epoch:1 step:1431 [D loss: 0.177817, acc.: 74.22%] [G loss: 0.780898]\n",
      "epoch:1 step:1432 [D loss: 0.173415, acc.: 74.22%] [G loss: 0.793322]\n",
      "epoch:1 step:1433 [D loss: 0.205466, acc.: 71.88%] [G loss: 0.747338]\n",
      "epoch:1 step:1434 [D loss: 0.132055, acc.: 83.59%] [G loss: 0.840538]\n",
      "epoch:1 step:1435 [D loss: 0.129407, acc.: 85.16%] [G loss: 0.808793]\n",
      "epoch:1 step:1436 [D loss: 0.115783, acc.: 91.41%] [G loss: 0.793678]\n",
      "epoch:1 step:1437 [D loss: 0.159781, acc.: 78.91%] [G loss: 0.788509]\n",
      "epoch:1 step:1438 [D loss: 0.189186, acc.: 70.31%] [G loss: 0.743551]\n",
      "epoch:1 step:1439 [D loss: 0.169006, acc.: 76.56%] [G loss: 0.749520]\n",
      "epoch:1 step:1440 [D loss: 0.087081, acc.: 93.75%] [G loss: 0.890113]\n",
      "epoch:1 step:1441 [D loss: 0.154421, acc.: 82.03%] [G loss: 0.732164]\n",
      "epoch:1 step:1442 [D loss: 0.141987, acc.: 86.72%] [G loss: 0.817954]\n",
      "epoch:1 step:1443 [D loss: 0.131645, acc.: 89.84%] [G loss: 0.779146]\n",
      "epoch:1 step:1444 [D loss: 0.130707, acc.: 85.16%] [G loss: 0.789919]\n",
      "epoch:1 step:1445 [D loss: 0.129301, acc.: 85.16%] [G loss: 0.859300]\n",
      "epoch:1 step:1446 [D loss: 0.164616, acc.: 80.47%] [G loss: 0.701432]\n",
      "epoch:1 step:1447 [D loss: 0.163408, acc.: 79.69%] [G loss: 0.691945]\n",
      "epoch:1 step:1448 [D loss: 0.147369, acc.: 82.03%] [G loss: 0.767203]\n",
      "epoch:1 step:1449 [D loss: 0.143270, acc.: 82.03%] [G loss: 0.766100]\n",
      "epoch:1 step:1450 [D loss: 0.157697, acc.: 79.69%] [G loss: 0.752593]\n",
      "epoch:1 step:1451 [D loss: 0.178032, acc.: 74.22%] [G loss: 0.736359]\n",
      "epoch:1 step:1452 [D loss: 0.132125, acc.: 88.28%] [G loss: 0.844986]\n",
      "epoch:1 step:1453 [D loss: 0.166917, acc.: 76.56%] [G loss: 0.726112]\n",
      "epoch:1 step:1454 [D loss: 0.160299, acc.: 82.81%] [G loss: 0.737932]\n",
      "epoch:1 step:1455 [D loss: 0.157711, acc.: 83.59%] [G loss: 0.742388]\n",
      "epoch:1 step:1456 [D loss: 0.144310, acc.: 82.81%] [G loss: 0.797155]\n",
      "epoch:1 step:1457 [D loss: 0.115851, acc.: 90.62%] [G loss: 0.774724]\n",
      "epoch:1 step:1458 [D loss: 0.160715, acc.: 82.03%] [G loss: 0.707879]\n",
      "epoch:1 step:1459 [D loss: 0.133559, acc.: 81.25%] [G loss: 0.776754]\n",
      "epoch:1 step:1460 [D loss: 0.146425, acc.: 85.94%] [G loss: 0.798241]\n",
      "epoch:1 step:1461 [D loss: 0.174154, acc.: 78.12%] [G loss: 0.722327]\n",
      "epoch:1 step:1462 [D loss: 0.169288, acc.: 75.00%] [G loss: 0.773955]\n",
      "epoch:1 step:1463 [D loss: 0.124322, acc.: 85.16%] [G loss: 0.815987]\n",
      "epoch:1 step:1464 [D loss: 0.173309, acc.: 74.22%] [G loss: 0.781869]\n",
      "epoch:1 step:1465 [D loss: 0.171239, acc.: 79.69%] [G loss: 0.742629]\n",
      "epoch:1 step:1466 [D loss: 0.139238, acc.: 79.69%] [G loss: 0.814339]\n",
      "epoch:1 step:1467 [D loss: 0.146703, acc.: 81.25%] [G loss: 0.769790]\n",
      "epoch:1 step:1468 [D loss: 0.224280, acc.: 64.06%] [G loss: 0.646382]\n",
      "epoch:1 step:1469 [D loss: 0.119464, acc.: 86.72%] [G loss: 0.830408]\n",
      "epoch:1 step:1470 [D loss: 0.162541, acc.: 82.81%] [G loss: 0.807509]\n",
      "epoch:1 step:1471 [D loss: 0.125667, acc.: 92.97%] [G loss: 0.786579]\n",
      "epoch:1 step:1472 [D loss: 0.185206, acc.: 71.09%] [G loss: 0.681688]\n",
      "epoch:1 step:1473 [D loss: 0.128046, acc.: 84.38%] [G loss: 0.868478]\n",
      "epoch:1 step:1474 [D loss: 0.138804, acc.: 85.16%] [G loss: 0.808916]\n",
      "epoch:1 step:1475 [D loss: 0.160505, acc.: 75.00%] [G loss: 0.744615]\n",
      "epoch:1 step:1476 [D loss: 0.129427, acc.: 82.81%] [G loss: 0.790165]\n",
      "epoch:1 step:1477 [D loss: 0.126843, acc.: 87.50%] [G loss: 0.723591]\n",
      "epoch:1 step:1478 [D loss: 0.122267, acc.: 84.38%] [G loss: 0.824621]\n",
      "epoch:1 step:1479 [D loss: 0.183765, acc.: 75.00%] [G loss: 0.644008]\n",
      "epoch:1 step:1480 [D loss: 0.156736, acc.: 81.25%] [G loss: 0.739725]\n",
      "epoch:1 step:1481 [D loss: 0.111106, acc.: 89.06%] [G loss: 0.865481]\n",
      "epoch:1 step:1482 [D loss: 0.155924, acc.: 78.12%] [G loss: 0.746472]\n",
      "epoch:1 step:1483 [D loss: 0.134884, acc.: 85.94%] [G loss: 0.783096]\n",
      "epoch:1 step:1484 [D loss: 0.118890, acc.: 86.72%] [G loss: 0.853211]\n",
      "epoch:1 step:1485 [D loss: 0.166129, acc.: 77.34%] [G loss: 0.732031]\n",
      "epoch:1 step:1486 [D loss: 0.149461, acc.: 78.91%] [G loss: 0.737709]\n",
      "epoch:1 step:1487 [D loss: 0.163812, acc.: 82.03%] [G loss: 0.729954]\n",
      "epoch:1 step:1488 [D loss: 0.128061, acc.: 89.06%] [G loss: 0.776112]\n",
      "epoch:1 step:1489 [D loss: 0.144313, acc.: 82.81%] [G loss: 0.802474]\n",
      "epoch:1 step:1490 [D loss: 0.150294, acc.: 82.81%] [G loss: 0.818073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1491 [D loss: 0.118814, acc.: 89.06%] [G loss: 0.834488]\n",
      "epoch:1 step:1492 [D loss: 0.126273, acc.: 85.94%] [G loss: 0.791197]\n",
      "epoch:1 step:1493 [D loss: 0.123857, acc.: 87.50%] [G loss: 0.745630]\n",
      "epoch:1 step:1494 [D loss: 0.156764, acc.: 83.59%] [G loss: 0.875349]\n",
      "epoch:1 step:1495 [D loss: 0.094025, acc.: 89.84%] [G loss: 0.868738]\n",
      "epoch:1 step:1496 [D loss: 0.164603, acc.: 76.56%] [G loss: 0.682525]\n",
      "epoch:1 step:1497 [D loss: 0.118008, acc.: 89.06%] [G loss: 0.768924]\n",
      "epoch:1 step:1498 [D loss: 0.129733, acc.: 85.16%] [G loss: 0.748694]\n",
      "epoch:1 step:1499 [D loss: 0.185321, acc.: 69.53%] [G loss: 0.767076]\n",
      "epoch:1 step:1500 [D loss: 0.175531, acc.: 75.00%] [G loss: 0.735523]\n",
      "epoch:1 step:1501 [D loss: 0.104397, acc.: 89.84%] [G loss: 0.901453]\n",
      "epoch:1 step:1502 [D loss: 0.171214, acc.: 75.78%] [G loss: 0.735774]\n",
      "epoch:1 step:1503 [D loss: 0.161861, acc.: 75.78%] [G loss: 0.801799]\n",
      "epoch:1 step:1504 [D loss: 0.098913, acc.: 91.41%] [G loss: 0.838336]\n",
      "epoch:1 step:1505 [D loss: 0.162472, acc.: 77.34%] [G loss: 0.776520]\n",
      "epoch:1 step:1506 [D loss: 0.176270, acc.: 75.78%] [G loss: 0.655789]\n",
      "epoch:1 step:1507 [D loss: 0.094373, acc.: 92.19%] [G loss: 0.799388]\n",
      "epoch:1 step:1508 [D loss: 0.183199, acc.: 74.22%] [G loss: 0.687437]\n",
      "epoch:1 step:1509 [D loss: 0.131441, acc.: 85.16%] [G loss: 0.731403]\n",
      "epoch:1 step:1510 [D loss: 0.138957, acc.: 83.59%] [G loss: 0.771258]\n",
      "epoch:1 step:1511 [D loss: 0.146387, acc.: 81.25%] [G loss: 0.760480]\n",
      "epoch:1 step:1512 [D loss: 0.104370, acc.: 89.06%] [G loss: 0.864150]\n",
      "epoch:1 step:1513 [D loss: 0.172020, acc.: 77.34%] [G loss: 0.658233]\n",
      "epoch:1 step:1514 [D loss: 0.150914, acc.: 82.03%] [G loss: 0.727879]\n",
      "epoch:1 step:1515 [D loss: 0.135909, acc.: 89.06%] [G loss: 0.752940]\n",
      "epoch:1 step:1516 [D loss: 0.174515, acc.: 74.22%] [G loss: 0.721206]\n",
      "epoch:1 step:1517 [D loss: 0.184343, acc.: 77.34%] [G loss: 0.754016]\n",
      "epoch:1 step:1518 [D loss: 0.131975, acc.: 84.38%] [G loss: 0.749002]\n",
      "epoch:1 step:1519 [D loss: 0.096282, acc.: 91.41%] [G loss: 0.808041]\n",
      "epoch:1 step:1520 [D loss: 0.164600, acc.: 77.34%] [G loss: 0.657089]\n",
      "epoch:1 step:1521 [D loss: 0.148963, acc.: 79.69%] [G loss: 0.741606]\n",
      "epoch:1 step:1522 [D loss: 0.150879, acc.: 80.47%] [G loss: 0.719916]\n",
      "epoch:1 step:1523 [D loss: 0.175501, acc.: 78.91%] [G loss: 0.676844]\n",
      "epoch:1 step:1524 [D loss: 0.165301, acc.: 78.91%] [G loss: 0.752972]\n",
      "epoch:1 step:1525 [D loss: 0.134207, acc.: 86.72%] [G loss: 0.766047]\n",
      "epoch:1 step:1526 [D loss: 0.138733, acc.: 82.81%] [G loss: 0.833255]\n",
      "epoch:1 step:1527 [D loss: 0.166639, acc.: 78.12%] [G loss: 0.709649]\n",
      "epoch:1 step:1528 [D loss: 0.202752, acc.: 70.31%] [G loss: 0.665394]\n",
      "epoch:1 step:1529 [D loss: 0.113647, acc.: 85.16%] [G loss: 0.791366]\n",
      "epoch:1 step:1530 [D loss: 0.185331, acc.: 76.56%] [G loss: 0.670031]\n",
      "epoch:1 step:1531 [D loss: 0.163495, acc.: 81.25%] [G loss: 0.665017]\n",
      "epoch:1 step:1532 [D loss: 0.133923, acc.: 85.16%] [G loss: 0.710643]\n",
      "epoch:1 step:1533 [D loss: 0.185966, acc.: 73.44%] [G loss: 0.679812]\n",
      "epoch:1 step:1534 [D loss: 0.144961, acc.: 82.81%] [G loss: 0.729055]\n",
      "epoch:1 step:1535 [D loss: 0.143011, acc.: 85.94%] [G loss: 0.685487]\n",
      "epoch:1 step:1536 [D loss: 0.149495, acc.: 84.38%] [G loss: 0.751658]\n",
      "epoch:1 step:1537 [D loss: 0.196556, acc.: 71.88%] [G loss: 0.692520]\n",
      "epoch:1 step:1538 [D loss: 0.136443, acc.: 88.28%] [G loss: 0.711257]\n",
      "epoch:1 step:1539 [D loss: 0.124180, acc.: 85.16%] [G loss: 0.809296]\n",
      "epoch:1 step:1540 [D loss: 0.175231, acc.: 76.56%] [G loss: 0.698128]\n",
      "epoch:1 step:1541 [D loss: 0.120209, acc.: 87.50%] [G loss: 0.742592]\n",
      "epoch:1 step:1542 [D loss: 0.138732, acc.: 84.38%] [G loss: 0.731916]\n",
      "epoch:1 step:1543 [D loss: 0.146952, acc.: 82.03%] [G loss: 0.645539]\n",
      "epoch:1 step:1544 [D loss: 0.168837, acc.: 74.22%] [G loss: 0.668760]\n",
      "epoch:1 step:1545 [D loss: 0.108374, acc.: 90.62%] [G loss: 0.800383]\n",
      "epoch:1 step:1546 [D loss: 0.141843, acc.: 77.34%] [G loss: 0.775158]\n",
      "epoch:1 step:1547 [D loss: 0.188244, acc.: 73.44%] [G loss: 0.644908]\n",
      "epoch:1 step:1548 [D loss: 0.112998, acc.: 92.19%] [G loss: 0.793637]\n",
      "epoch:1 step:1549 [D loss: 0.132560, acc.: 85.16%] [G loss: 0.756280]\n",
      "epoch:1 step:1550 [D loss: 0.133593, acc.: 83.59%] [G loss: 0.751098]\n",
      "epoch:1 step:1551 [D loss: 0.144657, acc.: 81.25%] [G loss: 0.732395]\n",
      "epoch:1 step:1552 [D loss: 0.129909, acc.: 85.16%] [G loss: 0.738656]\n",
      "epoch:1 step:1553 [D loss: 0.131676, acc.: 85.16%] [G loss: 0.782988]\n",
      "epoch:1 step:1554 [D loss: 0.181551, acc.: 78.91%] [G loss: 0.706988]\n",
      "epoch:1 step:1555 [D loss: 0.154023, acc.: 78.91%] [G loss: 0.706681]\n",
      "epoch:1 step:1556 [D loss: 0.117961, acc.: 85.94%] [G loss: 0.770767]\n",
      "epoch:1 step:1557 [D loss: 0.172199, acc.: 75.78%] [G loss: 0.722664]\n",
      "epoch:1 step:1558 [D loss: 0.184013, acc.: 74.22%] [G loss: 0.665877]\n",
      "epoch:1 step:1559 [D loss: 0.239839, acc.: 57.81%] [G loss: 0.626204]\n",
      "epoch:1 step:1560 [D loss: 0.122751, acc.: 83.59%] [G loss: 0.837500]\n",
      "epoch:1 step:1561 [D loss: 0.173176, acc.: 74.22%] [G loss: 0.745643]\n",
      "epoch:1 step:1562 [D loss: 0.167522, acc.: 75.00%] [G loss: 0.721682]\n",
      "epoch:1 step:1563 [D loss: 0.142899, acc.: 82.03%] [G loss: 0.800482]\n",
      "epoch:1 step:1564 [D loss: 0.154140, acc.: 78.91%] [G loss: 0.730925]\n",
      "epoch:1 step:1565 [D loss: 0.176319, acc.: 78.91%] [G loss: 0.685287]\n",
      "epoch:1 step:1566 [D loss: 0.122474, acc.: 84.38%] [G loss: 0.732179]\n",
      "epoch:1 step:1567 [D loss: 0.109569, acc.: 91.41%] [G loss: 0.819451]\n",
      "epoch:1 step:1568 [D loss: 0.172313, acc.: 75.00%] [G loss: 0.697861]\n",
      "epoch:1 step:1569 [D loss: 0.136033, acc.: 84.38%] [G loss: 0.779170]\n",
      "epoch:1 step:1570 [D loss: 0.146725, acc.: 80.47%] [G loss: 0.753915]\n",
      "epoch:1 step:1571 [D loss: 0.130194, acc.: 85.94%] [G loss: 0.780585]\n",
      "epoch:1 step:1572 [D loss: 0.147419, acc.: 85.94%] [G loss: 0.765621]\n",
      "epoch:1 step:1573 [D loss: 0.154738, acc.: 77.34%] [G loss: 0.727821]\n",
      "epoch:1 step:1574 [D loss: 0.138972, acc.: 82.81%] [G loss: 0.687312]\n",
      "epoch:1 step:1575 [D loss: 0.135230, acc.: 85.94%] [G loss: 0.714500]\n",
      "epoch:1 step:1576 [D loss: 0.132565, acc.: 85.94%] [G loss: 0.715264]\n",
      "epoch:1 step:1577 [D loss: 0.129752, acc.: 86.72%] [G loss: 0.725270]\n",
      "epoch:1 step:1578 [D loss: 0.107431, acc.: 86.72%] [G loss: 0.836634]\n",
      "epoch:1 step:1579 [D loss: 0.125345, acc.: 86.72%] [G loss: 0.810397]\n",
      "epoch:1 step:1580 [D loss: 0.145293, acc.: 83.59%] [G loss: 0.767180]\n",
      "epoch:1 step:1581 [D loss: 0.152374, acc.: 80.47%] [G loss: 0.728729]\n",
      "epoch:1 step:1582 [D loss: 0.157614, acc.: 78.12%] [G loss: 0.735695]\n",
      "epoch:1 step:1583 [D loss: 0.123931, acc.: 87.50%] [G loss: 0.767126]\n",
      "epoch:1 step:1584 [D loss: 0.132572, acc.: 82.81%] [G loss: 0.769807]\n",
      "epoch:1 step:1585 [D loss: 0.125229, acc.: 85.16%] [G loss: 0.778723]\n",
      "epoch:1 step:1586 [D loss: 0.134700, acc.: 84.38%] [G loss: 0.771723]\n",
      "epoch:1 step:1587 [D loss: 0.108994, acc.: 89.06%] [G loss: 0.815154]\n",
      "epoch:1 step:1588 [D loss: 0.170871, acc.: 79.69%] [G loss: 0.680283]\n",
      "epoch:1 step:1589 [D loss: 0.205017, acc.: 67.97%] [G loss: 0.630682]\n",
      "epoch:1 step:1590 [D loss: 0.214314, acc.: 66.41%] [G loss: 0.712215]\n",
      "epoch:1 step:1591 [D loss: 0.180856, acc.: 68.75%] [G loss: 0.714707]\n",
      "epoch:1 step:1592 [D loss: 0.199485, acc.: 73.44%] [G loss: 0.670106]\n",
      "epoch:1 step:1593 [D loss: 0.137114, acc.: 85.16%] [G loss: 0.715701]\n",
      "epoch:1 step:1594 [D loss: 0.166401, acc.: 76.56%] [G loss: 0.676018]\n",
      "epoch:1 step:1595 [D loss: 0.136701, acc.: 82.81%] [G loss: 0.724232]\n",
      "epoch:1 step:1596 [D loss: 0.172786, acc.: 77.34%] [G loss: 0.664320]\n",
      "epoch:1 step:1597 [D loss: 0.132666, acc.: 84.38%] [G loss: 0.745267]\n",
      "epoch:1 step:1598 [D loss: 0.142598, acc.: 81.25%] [G loss: 0.707400]\n",
      "epoch:1 step:1599 [D loss: 0.219902, acc.: 65.62%] [G loss: 0.617771]\n",
      "epoch:1 step:1600 [D loss: 0.133387, acc.: 84.38%] [G loss: 0.770511]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.054758\n",
      "FID: 102.839500\n",
      "0 = 14.916195792770354\n",
      "1 = 0.12562784827500242\n",
      "2 = 0.9957500100135803\n",
      "3 = 0.9915000200271606\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9915000200271606\n",
      "7 = 12.11811520831583\n",
      "8 = 0.1988456073861885\n",
      "9 = 0.9703999757766724\n",
      "10 = 0.9509000182151794\n",
      "11 = 0.9898999929428101\n",
      "12 = 0.9894900918006897\n",
      "13 = 0.9509000182151794\n",
      "14 = 4.054774284362793\n",
      "15 = 7.461410045623779\n",
      "16 = 0.4169396162033081\n",
      "17 = 4.054758071899414\n",
      "18 = 102.8395004272461\n",
      "epoch:1 step:1601 [D loss: 0.169194, acc.: 76.56%] [G loss: 0.698943]\n",
      "epoch:1 step:1602 [D loss: 0.145668, acc.: 82.81%] [G loss: 0.693500]\n",
      "epoch:1 step:1603 [D loss: 0.131212, acc.: 85.16%] [G loss: 0.758402]\n",
      "epoch:1 step:1604 [D loss: 0.182372, acc.: 75.78%] [G loss: 0.691316]\n",
      "epoch:1 step:1605 [D loss: 0.162349, acc.: 79.69%] [G loss: 0.671560]\n",
      "epoch:1 step:1606 [D loss: 0.143683, acc.: 82.81%] [G loss: 0.698945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1607 [D loss: 0.160911, acc.: 76.56%] [G loss: 0.722283]\n",
      "epoch:1 step:1608 [D loss: 0.187335, acc.: 70.31%] [G loss: 0.679394]\n",
      "epoch:1 step:1609 [D loss: 0.198602, acc.: 69.53%] [G loss: 0.679461]\n",
      "epoch:1 step:1610 [D loss: 0.179238, acc.: 72.66%] [G loss: 0.700005]\n",
      "epoch:1 step:1611 [D loss: 0.171925, acc.: 77.34%] [G loss: 0.731160]\n",
      "epoch:1 step:1612 [D loss: 0.152037, acc.: 81.25%] [G loss: 0.753091]\n",
      "epoch:1 step:1613 [D loss: 0.189375, acc.: 76.56%] [G loss: 0.685207]\n",
      "epoch:1 step:1614 [D loss: 0.127656, acc.: 88.28%] [G loss: 0.720133]\n",
      "epoch:1 step:1615 [D loss: 0.155513, acc.: 81.25%] [G loss: 0.727642]\n",
      "epoch:1 step:1616 [D loss: 0.156471, acc.: 78.91%] [G loss: 0.747467]\n",
      "epoch:1 step:1617 [D loss: 0.133296, acc.: 85.94%] [G loss: 0.741617]\n",
      "epoch:1 step:1618 [D loss: 0.169809, acc.: 75.78%] [G loss: 0.678182]\n",
      "epoch:1 step:1619 [D loss: 0.167273, acc.: 78.91%] [G loss: 0.720082]\n",
      "epoch:1 step:1620 [D loss: 0.182112, acc.: 73.44%] [G loss: 0.662915]\n",
      "epoch:1 step:1621 [D loss: 0.158835, acc.: 76.56%] [G loss: 0.708379]\n",
      "epoch:1 step:1622 [D loss: 0.188464, acc.: 79.69%] [G loss: 0.690586]\n",
      "epoch:1 step:1623 [D loss: 0.198478, acc.: 75.00%] [G loss: 0.601524]\n",
      "epoch:1 step:1624 [D loss: 0.148468, acc.: 82.03%] [G loss: 0.775123]\n",
      "epoch:1 step:1625 [D loss: 0.217782, acc.: 68.75%] [G loss: 0.640896]\n",
      "epoch:1 step:1626 [D loss: 0.158529, acc.: 76.56%] [G loss: 0.720911]\n",
      "epoch:1 step:1627 [D loss: 0.170139, acc.: 77.34%] [G loss: 0.676363]\n",
      "epoch:1 step:1628 [D loss: 0.193946, acc.: 71.09%] [G loss: 0.691584]\n",
      "epoch:1 step:1629 [D loss: 0.167891, acc.: 78.12%] [G loss: 0.726592]\n",
      "epoch:1 step:1630 [D loss: 0.165566, acc.: 82.03%] [G loss: 0.717716]\n",
      "epoch:1 step:1631 [D loss: 0.164887, acc.: 76.56%] [G loss: 0.681076]\n",
      "epoch:1 step:1632 [D loss: 0.124282, acc.: 85.16%] [G loss: 0.761063]\n",
      "epoch:1 step:1633 [D loss: 0.172979, acc.: 73.44%] [G loss: 0.623711]\n",
      "epoch:1 step:1634 [D loss: 0.135814, acc.: 82.81%] [G loss: 0.841624]\n",
      "epoch:1 step:1635 [D loss: 0.206408, acc.: 67.97%] [G loss: 0.600410]\n",
      "epoch:1 step:1636 [D loss: 0.152215, acc.: 76.56%] [G loss: 0.684345]\n",
      "epoch:1 step:1637 [D loss: 0.158479, acc.: 78.12%] [G loss: 0.696092]\n",
      "epoch:1 step:1638 [D loss: 0.167959, acc.: 78.91%] [G loss: 0.737945]\n",
      "epoch:1 step:1639 [D loss: 0.184211, acc.: 71.88%] [G loss: 0.689489]\n",
      "epoch:1 step:1640 [D loss: 0.171426, acc.: 74.22%] [G loss: 0.697610]\n",
      "epoch:1 step:1641 [D loss: 0.172835, acc.: 78.91%] [G loss: 0.750880]\n",
      "epoch:1 step:1642 [D loss: 0.178080, acc.: 71.09%] [G loss: 0.645891]\n",
      "epoch:1 step:1643 [D loss: 0.159420, acc.: 80.47%] [G loss: 0.644550]\n",
      "epoch:1 step:1644 [D loss: 0.102470, acc.: 88.28%] [G loss: 0.797115]\n",
      "epoch:1 step:1645 [D loss: 0.130002, acc.: 85.16%] [G loss: 0.766234]\n",
      "epoch:1 step:1646 [D loss: 0.161702, acc.: 75.78%] [G loss: 0.676948]\n",
      "epoch:1 step:1647 [D loss: 0.228745, acc.: 67.97%] [G loss: 0.562262]\n",
      "epoch:1 step:1648 [D loss: 0.195251, acc.: 71.09%] [G loss: 0.621499]\n",
      "epoch:1 step:1649 [D loss: 0.176806, acc.: 72.66%] [G loss: 0.655403]\n",
      "epoch:1 step:1650 [D loss: 0.218501, acc.: 65.62%] [G loss: 0.612469]\n",
      "epoch:1 step:1651 [D loss: 0.152465, acc.: 78.91%] [G loss: 0.713248]\n",
      "epoch:1 step:1652 [D loss: 0.187588, acc.: 74.22%] [G loss: 0.647792]\n",
      "epoch:1 step:1653 [D loss: 0.200976, acc.: 71.88%] [G loss: 0.603029]\n",
      "epoch:1 step:1654 [D loss: 0.226347, acc.: 61.72%] [G loss: 0.623567]\n",
      "epoch:1 step:1655 [D loss: 0.206402, acc.: 65.62%] [G loss: 0.611247]\n",
      "epoch:1 step:1656 [D loss: 0.182636, acc.: 72.66%] [G loss: 0.713540]\n",
      "epoch:1 step:1657 [D loss: 0.223884, acc.: 66.41%] [G loss: 0.588962]\n",
      "epoch:1 step:1658 [D loss: 0.169631, acc.: 79.69%] [G loss: 0.705286]\n",
      "epoch:1 step:1659 [D loss: 0.176256, acc.: 72.66%] [G loss: 0.620043]\n",
      "epoch:1 step:1660 [D loss: 0.180717, acc.: 71.88%] [G loss: 0.690117]\n",
      "epoch:1 step:1661 [D loss: 0.165289, acc.: 74.22%] [G loss: 0.725555]\n",
      "epoch:1 step:1662 [D loss: 0.153970, acc.: 82.03%] [G loss: 0.714434]\n",
      "epoch:1 step:1663 [D loss: 0.191510, acc.: 74.22%] [G loss: 0.631618]\n",
      "epoch:1 step:1664 [D loss: 0.192733, acc.: 71.88%] [G loss: 0.599013]\n",
      "epoch:1 step:1665 [D loss: 0.136048, acc.: 85.16%] [G loss: 0.666405]\n",
      "epoch:1 step:1666 [D loss: 0.193189, acc.: 69.53%] [G loss: 0.692890]\n",
      "epoch:1 step:1667 [D loss: 0.146829, acc.: 82.81%] [G loss: 0.710205]\n",
      "epoch:1 step:1668 [D loss: 0.157922, acc.: 80.47%] [G loss: 0.652445]\n",
      "epoch:1 step:1669 [D loss: 0.130304, acc.: 82.81%] [G loss: 0.715563]\n",
      "epoch:1 step:1670 [D loss: 0.135226, acc.: 85.16%] [G loss: 0.720123]\n",
      "epoch:1 step:1671 [D loss: 0.154574, acc.: 78.91%] [G loss: 0.703982]\n",
      "epoch:1 step:1672 [D loss: 0.186783, acc.: 71.88%] [G loss: 0.606277]\n",
      "epoch:1 step:1673 [D loss: 0.154596, acc.: 80.47%] [G loss: 0.663594]\n",
      "epoch:1 step:1674 [D loss: 0.155303, acc.: 79.69%] [G loss: 0.682717]\n",
      "epoch:1 step:1675 [D loss: 0.180900, acc.: 75.78%] [G loss: 0.647212]\n",
      "epoch:1 step:1676 [D loss: 0.167254, acc.: 79.69%] [G loss: 0.630886]\n",
      "epoch:1 step:1677 [D loss: 0.186493, acc.: 75.78%] [G loss: 0.643451]\n",
      "epoch:1 step:1678 [D loss: 0.127926, acc.: 86.72%] [G loss: 0.756892]\n",
      "epoch:1 step:1679 [D loss: 0.182454, acc.: 71.88%] [G loss: 0.610353]\n",
      "epoch:1 step:1680 [D loss: 0.183670, acc.: 67.19%] [G loss: 0.678067]\n",
      "epoch:1 step:1681 [D loss: 0.143042, acc.: 80.47%] [G loss: 0.728916]\n",
      "epoch:1 step:1682 [D loss: 0.143292, acc.: 78.91%] [G loss: 0.693628]\n",
      "epoch:1 step:1683 [D loss: 0.106917, acc.: 87.50%] [G loss: 0.802549]\n",
      "epoch:1 step:1684 [D loss: 0.126425, acc.: 84.38%] [G loss: 0.702504]\n",
      "epoch:1 step:1685 [D loss: 0.178980, acc.: 78.91%] [G loss: 0.603285]\n",
      "epoch:1 step:1686 [D loss: 0.135784, acc.: 82.81%] [G loss: 0.684789]\n",
      "epoch:1 step:1687 [D loss: 0.139110, acc.: 84.38%] [G loss: 0.728615]\n",
      "epoch:1 step:1688 [D loss: 0.159121, acc.: 81.25%] [G loss: 0.682315]\n",
      "epoch:1 step:1689 [D loss: 0.131861, acc.: 83.59%] [G loss: 0.690720]\n",
      "epoch:1 step:1690 [D loss: 0.137326, acc.: 82.03%] [G loss: 0.715871]\n",
      "epoch:1 step:1691 [D loss: 0.121538, acc.: 86.72%] [G loss: 0.736445]\n",
      "epoch:1 step:1692 [D loss: 0.152352, acc.: 79.69%] [G loss: 0.688154]\n",
      "epoch:1 step:1693 [D loss: 0.132725, acc.: 82.81%] [G loss: 0.704638]\n",
      "epoch:1 step:1694 [D loss: 0.129401, acc.: 82.81%] [G loss: 0.688520]\n",
      "epoch:1 step:1695 [D loss: 0.148133, acc.: 79.69%] [G loss: 0.714844]\n",
      "epoch:1 step:1696 [D loss: 0.149887, acc.: 80.47%] [G loss: 0.704328]\n",
      "epoch:1 step:1697 [D loss: 0.171661, acc.: 76.56%] [G loss: 0.663388]\n",
      "epoch:1 step:1698 [D loss: 0.185157, acc.: 68.75%] [G loss: 0.635835]\n",
      "epoch:1 step:1699 [D loss: 0.136647, acc.: 85.16%] [G loss: 0.694210]\n",
      "epoch:1 step:1700 [D loss: 0.124209, acc.: 85.16%] [G loss: 0.718192]\n",
      "epoch:1 step:1701 [D loss: 0.140783, acc.: 82.81%] [G loss: 0.657962]\n",
      "epoch:1 step:1702 [D loss: 0.247535, acc.: 62.50%] [G loss: 0.576648]\n",
      "epoch:1 step:1703 [D loss: 0.183797, acc.: 73.44%] [G loss: 0.625219]\n",
      "epoch:1 step:1704 [D loss: 0.172290, acc.: 75.78%] [G loss: 0.749173]\n",
      "epoch:1 step:1705 [D loss: 0.172592, acc.: 74.22%] [G loss: 0.695849]\n",
      "epoch:1 step:1706 [D loss: 0.165356, acc.: 82.03%] [G loss: 0.708181]\n",
      "epoch:1 step:1707 [D loss: 0.155023, acc.: 79.69%] [G loss: 0.698719]\n",
      "epoch:1 step:1708 [D loss: 0.182272, acc.: 71.88%] [G loss: 0.733870]\n",
      "epoch:1 step:1709 [D loss: 0.137151, acc.: 83.59%] [G loss: 0.755940]\n",
      "epoch:1 step:1710 [D loss: 0.153328, acc.: 76.56%] [G loss: 0.684064]\n",
      "epoch:1 step:1711 [D loss: 0.158273, acc.: 80.47%] [G loss: 0.638993]\n",
      "epoch:1 step:1712 [D loss: 0.138924, acc.: 79.69%] [G loss: 0.716596]\n",
      "epoch:1 step:1713 [D loss: 0.170001, acc.: 78.91%] [G loss: 0.668128]\n",
      "epoch:1 step:1714 [D loss: 0.158700, acc.: 78.12%] [G loss: 0.657344]\n",
      "epoch:1 step:1715 [D loss: 0.160292, acc.: 80.47%] [G loss: 0.673064]\n",
      "epoch:1 step:1716 [D loss: 0.151438, acc.: 77.34%] [G loss: 0.683069]\n",
      "epoch:1 step:1717 [D loss: 0.135948, acc.: 83.59%] [G loss: 0.662837]\n",
      "epoch:1 step:1718 [D loss: 0.159793, acc.: 79.69%] [G loss: 0.852468]\n",
      "epoch:1 step:1719 [D loss: 0.144795, acc.: 78.12%] [G loss: 0.773396]\n",
      "epoch:1 step:1720 [D loss: 0.161330, acc.: 80.47%] [G loss: 0.700523]\n",
      "epoch:1 step:1721 [D loss: 0.162623, acc.: 76.56%] [G loss: 0.713425]\n",
      "epoch:1 step:1722 [D loss: 0.162867, acc.: 77.34%] [G loss: 0.703844]\n",
      "epoch:1 step:1723 [D loss: 0.139301, acc.: 83.59%] [G loss: 0.772284]\n",
      "epoch:1 step:1724 [D loss: 0.222304, acc.: 68.75%] [G loss: 0.640619]\n",
      "epoch:1 step:1725 [D loss: 0.163588, acc.: 82.81%] [G loss: 0.656829]\n",
      "epoch:1 step:1726 [D loss: 0.125991, acc.: 85.16%] [G loss: 0.750484]\n",
      "epoch:1 step:1727 [D loss: 0.139633, acc.: 85.94%] [G loss: 0.760027]\n",
      "epoch:1 step:1728 [D loss: 0.200522, acc.: 64.84%] [G loss: 0.643283]\n",
      "epoch:1 step:1729 [D loss: 0.112655, acc.: 87.50%] [G loss: 0.788648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1730 [D loss: 0.156509, acc.: 77.34%] [G loss: 0.675503]\n",
      "epoch:1 step:1731 [D loss: 0.149372, acc.: 80.47%] [G loss: 0.703625]\n",
      "epoch:1 step:1732 [D loss: 0.159434, acc.: 81.25%] [G loss: 0.775051]\n",
      "epoch:1 step:1733 [D loss: 0.118641, acc.: 86.72%] [G loss: 0.781905]\n",
      "epoch:1 step:1734 [D loss: 0.200409, acc.: 68.75%] [G loss: 0.664830]\n",
      "epoch:1 step:1735 [D loss: 0.141668, acc.: 83.59%] [G loss: 0.733559]\n",
      "epoch:1 step:1736 [D loss: 0.193931, acc.: 74.22%] [G loss: 0.655877]\n",
      "epoch:1 step:1737 [D loss: 0.180564, acc.: 77.34%] [G loss: 0.620552]\n",
      "epoch:1 step:1738 [D loss: 0.159144, acc.: 78.12%] [G loss: 0.735165]\n",
      "epoch:1 step:1739 [D loss: 0.160836, acc.: 76.56%] [G loss: 0.694059]\n",
      "epoch:1 step:1740 [D loss: 0.112695, acc.: 86.72%] [G loss: 0.760205]\n",
      "epoch:1 step:1741 [D loss: 0.127228, acc.: 89.06%] [G loss: 0.725488]\n",
      "epoch:1 step:1742 [D loss: 0.118957, acc.: 87.50%] [G loss: 0.746221]\n",
      "epoch:1 step:1743 [D loss: 0.107854, acc.: 83.59%] [G loss: 0.779828]\n",
      "epoch:1 step:1744 [D loss: 0.138827, acc.: 81.25%] [G loss: 0.734080]\n",
      "epoch:1 step:1745 [D loss: 0.203512, acc.: 74.22%] [G loss: 0.578642]\n",
      "epoch:1 step:1746 [D loss: 0.170482, acc.: 75.00%] [G loss: 0.731014]\n",
      "epoch:1 step:1747 [D loss: 0.156409, acc.: 78.12%] [G loss: 0.711274]\n",
      "epoch:1 step:1748 [D loss: 0.177620, acc.: 76.56%] [G loss: 0.699122]\n",
      "epoch:1 step:1749 [D loss: 0.192080, acc.: 70.31%] [G loss: 0.660999]\n",
      "epoch:1 step:1750 [D loss: 0.170341, acc.: 79.69%] [G loss: 0.675337]\n",
      "epoch:1 step:1751 [D loss: 0.144324, acc.: 82.03%] [G loss: 0.698368]\n",
      "epoch:1 step:1752 [D loss: 0.181419, acc.: 73.44%] [G loss: 0.643905]\n",
      "epoch:1 step:1753 [D loss: 0.136639, acc.: 83.59%] [G loss: 0.738351]\n",
      "epoch:1 step:1754 [D loss: 0.170509, acc.: 77.34%] [G loss: 0.647604]\n",
      "epoch:1 step:1755 [D loss: 0.168910, acc.: 78.12%] [G loss: 0.685830]\n",
      "epoch:1 step:1756 [D loss: 0.141783, acc.: 85.94%] [G loss: 0.695531]\n",
      "epoch:1 step:1757 [D loss: 0.158597, acc.: 76.56%] [G loss: 0.729433]\n",
      "epoch:1 step:1758 [D loss: 0.129066, acc.: 86.72%] [G loss: 0.711321]\n",
      "epoch:1 step:1759 [D loss: 0.124028, acc.: 88.28%] [G loss: 0.698829]\n",
      "epoch:1 step:1760 [D loss: 0.131656, acc.: 83.59%] [G loss: 0.691987]\n",
      "epoch:1 step:1761 [D loss: 0.205522, acc.: 65.62%] [G loss: 0.639893]\n",
      "epoch:1 step:1762 [D loss: 0.117349, acc.: 91.41%] [G loss: 0.726817]\n",
      "epoch:1 step:1763 [D loss: 0.138056, acc.: 83.59%] [G loss: 0.717156]\n",
      "epoch:1 step:1764 [D loss: 0.180431, acc.: 75.78%] [G loss: 0.670433]\n",
      "epoch:1 step:1765 [D loss: 0.157370, acc.: 79.69%] [G loss: 0.637338]\n",
      "epoch:1 step:1766 [D loss: 0.122572, acc.: 82.03%] [G loss: 0.734035]\n",
      "epoch:1 step:1767 [D loss: 0.130596, acc.: 81.25%] [G loss: 0.723185]\n",
      "epoch:1 step:1768 [D loss: 0.174238, acc.: 74.22%] [G loss: 0.617419]\n",
      "epoch:1 step:1769 [D loss: 0.136696, acc.: 82.81%] [G loss: 0.722399]\n",
      "epoch:1 step:1770 [D loss: 0.110274, acc.: 88.28%] [G loss: 0.760375]\n",
      "epoch:1 step:1771 [D loss: 0.130736, acc.: 83.59%] [G loss: 0.740746]\n",
      "epoch:1 step:1772 [D loss: 0.189552, acc.: 70.31%] [G loss: 0.694952]\n",
      "epoch:1 step:1773 [D loss: 0.132231, acc.: 82.03%] [G loss: 0.743931]\n",
      "epoch:1 step:1774 [D loss: 0.161460, acc.: 78.91%] [G loss: 0.676502]\n",
      "epoch:1 step:1775 [D loss: 0.162761, acc.: 80.47%] [G loss: 0.697873]\n",
      "epoch:1 step:1776 [D loss: 0.158574, acc.: 78.12%] [G loss: 0.702285]\n",
      "epoch:1 step:1777 [D loss: 0.159834, acc.: 82.03%] [G loss: 0.649395]\n",
      "epoch:1 step:1778 [D loss: 0.133731, acc.: 82.81%] [G loss: 0.708496]\n",
      "epoch:1 step:1779 [D loss: 0.129942, acc.: 85.94%] [G loss: 0.723379]\n",
      "epoch:1 step:1780 [D loss: 0.133773, acc.: 85.16%] [G loss: 0.729160]\n",
      "epoch:1 step:1781 [D loss: 0.160677, acc.: 78.91%] [G loss: 0.690428]\n",
      "epoch:1 step:1782 [D loss: 0.162003, acc.: 78.91%] [G loss: 0.675101]\n",
      "epoch:1 step:1783 [D loss: 0.176943, acc.: 74.22%] [G loss: 0.662973]\n",
      "epoch:1 step:1784 [D loss: 0.120992, acc.: 88.28%] [G loss: 0.774642]\n",
      "epoch:1 step:1785 [D loss: 0.148322, acc.: 85.16%] [G loss: 0.666607]\n",
      "epoch:1 step:1786 [D loss: 0.195698, acc.: 72.66%] [G loss: 0.610153]\n",
      "epoch:1 step:1787 [D loss: 0.134098, acc.: 85.16%] [G loss: 0.771178]\n",
      "epoch:1 step:1788 [D loss: 0.152793, acc.: 77.34%] [G loss: 0.731042]\n",
      "epoch:1 step:1789 [D loss: 0.146643, acc.: 77.34%] [G loss: 0.700528]\n",
      "epoch:1 step:1790 [D loss: 0.129878, acc.: 85.16%] [G loss: 0.754519]\n",
      "epoch:1 step:1791 [D loss: 0.102181, acc.: 86.72%] [G loss: 0.728125]\n",
      "epoch:1 step:1792 [D loss: 0.175392, acc.: 75.00%] [G loss: 0.641515]\n",
      "epoch:1 step:1793 [D loss: 0.176652, acc.: 76.56%] [G loss: 0.699121]\n",
      "epoch:1 step:1794 [D loss: 0.126877, acc.: 85.16%] [G loss: 0.817895]\n",
      "epoch:1 step:1795 [D loss: 0.247929, acc.: 64.06%] [G loss: 0.608463]\n",
      "epoch:1 step:1796 [D loss: 0.170044, acc.: 78.12%] [G loss: 0.668210]\n",
      "epoch:1 step:1797 [D loss: 0.123092, acc.: 82.81%] [G loss: 0.722139]\n",
      "epoch:1 step:1798 [D loss: 0.177338, acc.: 75.00%] [G loss: 0.612841]\n",
      "epoch:1 step:1799 [D loss: 0.126669, acc.: 86.72%] [G loss: 0.697392]\n",
      "epoch:1 step:1800 [D loss: 0.155336, acc.: 81.25%] [G loss: 0.694269]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.017178\n",
      "FID: 104.089996\n",
      "0 = 15.01212683734894\n",
      "1 = 0.13113101834139945\n",
      "2 = 0.9974499940872192\n",
      "3 = 0.9948999881744385\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9948999881744385\n",
      "7 = 12.102652975905023\n",
      "8 = 0.20585441800542809\n",
      "9 = 0.961899995803833\n",
      "10 = 0.9405999779701233\n",
      "11 = 0.9832000136375427\n",
      "12 = 0.9824524521827698\n",
      "13 = 0.9405999779701233\n",
      "14 = 4.017199516296387\n",
      "15 = 6.440546035766602\n",
      "16 = 0.48827242851257324\n",
      "17 = 4.017177581787109\n",
      "18 = 104.08999633789062\n",
      "epoch:1 step:1801 [D loss: 0.119135, acc.: 88.28%] [G loss: 0.734871]\n",
      "epoch:1 step:1802 [D loss: 0.160513, acc.: 77.34%] [G loss: 0.630212]\n",
      "epoch:1 step:1803 [D loss: 0.153044, acc.: 79.69%] [G loss: 0.758020]\n",
      "epoch:1 step:1804 [D loss: 0.188191, acc.: 69.53%] [G loss: 0.645072]\n",
      "epoch:1 step:1805 [D loss: 0.157561, acc.: 81.25%] [G loss: 0.690652]\n",
      "epoch:1 step:1806 [D loss: 0.128830, acc.: 85.94%] [G loss: 0.695387]\n",
      "epoch:1 step:1807 [D loss: 0.159784, acc.: 78.12%] [G loss: 0.663378]\n",
      "epoch:1 step:1808 [D loss: 0.163904, acc.: 78.91%] [G loss: 0.694789]\n",
      "epoch:1 step:1809 [D loss: 0.125411, acc.: 84.38%] [G loss: 0.736287]\n",
      "epoch:1 step:1810 [D loss: 0.189913, acc.: 73.44%] [G loss: 0.653451]\n",
      "epoch:1 step:1811 [D loss: 0.162135, acc.: 76.56%] [G loss: 0.694524]\n",
      "epoch:1 step:1812 [D loss: 0.126328, acc.: 83.59%] [G loss: 0.724490]\n",
      "epoch:1 step:1813 [D loss: 0.162948, acc.: 77.34%] [G loss: 0.655852]\n",
      "epoch:1 step:1814 [D loss: 0.137147, acc.: 81.25%] [G loss: 0.688485]\n",
      "epoch:1 step:1815 [D loss: 0.183149, acc.: 75.00%] [G loss: 0.722261]\n",
      "epoch:1 step:1816 [D loss: 0.163554, acc.: 81.25%] [G loss: 0.690449]\n",
      "epoch:1 step:1817 [D loss: 0.202900, acc.: 67.97%] [G loss: 0.551895]\n",
      "epoch:1 step:1818 [D loss: 0.193283, acc.: 72.66%] [G loss: 0.667019]\n",
      "epoch:1 step:1819 [D loss: 0.170824, acc.: 76.56%] [G loss: 0.707791]\n",
      "epoch:1 step:1820 [D loss: 0.163419, acc.: 78.12%] [G loss: 0.657971]\n",
      "epoch:1 step:1821 [D loss: 0.135357, acc.: 82.03%] [G loss: 0.683688]\n",
      "epoch:1 step:1822 [D loss: 0.137385, acc.: 79.69%] [G loss: 0.734999]\n",
      "epoch:1 step:1823 [D loss: 0.126803, acc.: 83.59%] [G loss: 0.678899]\n",
      "epoch:1 step:1824 [D loss: 0.097940, acc.: 89.84%] [G loss: 0.789461]\n",
      "epoch:1 step:1825 [D loss: 0.155096, acc.: 82.81%] [G loss: 0.745055]\n",
      "epoch:1 step:1826 [D loss: 0.191599, acc.: 69.53%] [G loss: 0.647141]\n",
      "epoch:1 step:1827 [D loss: 0.118126, acc.: 85.94%] [G loss: 0.701424]\n",
      "epoch:1 step:1828 [D loss: 0.174560, acc.: 78.12%] [G loss: 0.647366]\n",
      "epoch:1 step:1829 [D loss: 0.214814, acc.: 63.28%] [G loss: 0.579788]\n",
      "epoch:1 step:1830 [D loss: 0.186291, acc.: 71.88%] [G loss: 0.593468]\n",
      "epoch:1 step:1831 [D loss: 0.152617, acc.: 79.69%] [G loss: 0.740361]\n",
      "epoch:1 step:1832 [D loss: 0.145841, acc.: 80.47%] [G loss: 0.665894]\n",
      "epoch:1 step:1833 [D loss: 0.193293, acc.: 75.00%] [G loss: 0.628112]\n",
      "epoch:1 step:1834 [D loss: 0.123362, acc.: 85.16%] [G loss: 0.730600]\n",
      "epoch:1 step:1835 [D loss: 0.175205, acc.: 74.22%] [G loss: 0.670039]\n",
      "epoch:1 step:1836 [D loss: 0.205172, acc.: 71.88%] [G loss: 0.662501]\n",
      "epoch:1 step:1837 [D loss: 0.164911, acc.: 77.34%] [G loss: 0.625825]\n",
      "epoch:1 step:1838 [D loss: 0.148611, acc.: 79.69%] [G loss: 0.660649]\n",
      "epoch:1 step:1839 [D loss: 0.175249, acc.: 70.31%] [G loss: 0.574629]\n",
      "epoch:1 step:1840 [D loss: 0.188666, acc.: 73.44%] [G loss: 0.671378]\n",
      "epoch:1 step:1841 [D loss: 0.157114, acc.: 78.12%] [G loss: 0.663866]\n",
      "epoch:1 step:1842 [D loss: 0.169271, acc.: 71.88%] [G loss: 0.647267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1843 [D loss: 0.144768, acc.: 82.81%] [G loss: 0.750759]\n",
      "epoch:1 step:1844 [D loss: 0.146724, acc.: 77.34%] [G loss: 0.707569]\n",
      "epoch:1 step:1845 [D loss: 0.141852, acc.: 80.47%] [G loss: 0.636190]\n",
      "epoch:1 step:1846 [D loss: 0.133638, acc.: 85.94%] [G loss: 0.744613]\n",
      "epoch:1 step:1847 [D loss: 0.155436, acc.: 78.91%] [G loss: 0.684616]\n",
      "epoch:1 step:1848 [D loss: 0.143494, acc.: 82.81%] [G loss: 0.684662]\n",
      "epoch:1 step:1849 [D loss: 0.148934, acc.: 77.34%] [G loss: 0.662351]\n",
      "epoch:1 step:1850 [D loss: 0.180230, acc.: 75.00%] [G loss: 0.646164]\n",
      "epoch:1 step:1851 [D loss: 0.143296, acc.: 81.25%] [G loss: 0.709448]\n",
      "epoch:1 step:1852 [D loss: 0.192914, acc.: 71.88%] [G loss: 0.628584]\n",
      "epoch:1 step:1853 [D loss: 0.150756, acc.: 79.69%] [G loss: 0.669083]\n",
      "epoch:1 step:1854 [D loss: 0.162228, acc.: 75.78%] [G loss: 0.668249]\n",
      "epoch:1 step:1855 [D loss: 0.132909, acc.: 85.16%] [G loss: 0.745461]\n",
      "epoch:1 step:1856 [D loss: 0.190976, acc.: 69.53%] [G loss: 0.657132]\n",
      "epoch:1 step:1857 [D loss: 0.216040, acc.: 68.75%] [G loss: 0.629802]\n",
      "epoch:1 step:1858 [D loss: 0.079619, acc.: 93.75%] [G loss: 0.885529]\n",
      "epoch:1 step:1859 [D loss: 0.241936, acc.: 62.50%] [G loss: 0.572398]\n",
      "epoch:1 step:1860 [D loss: 0.135602, acc.: 82.03%] [G loss: 0.678082]\n",
      "epoch:1 step:1861 [D loss: 0.145929, acc.: 78.91%] [G loss: 0.718988]\n",
      "epoch:1 step:1862 [D loss: 0.120190, acc.: 85.94%] [G loss: 0.723836]\n",
      "epoch:1 step:1863 [D loss: 0.143797, acc.: 82.81%] [G loss: 0.646854]\n",
      "epoch:1 step:1864 [D loss: 0.133496, acc.: 81.25%] [G loss: 0.745423]\n",
      "epoch:1 step:1865 [D loss: 0.176540, acc.: 77.34%] [G loss: 0.677228]\n",
      "epoch:1 step:1866 [D loss: 0.066944, acc.: 94.53%] [G loss: 0.945693]\n",
      "epoch:1 step:1867 [D loss: 0.139123, acc.: 79.69%] [G loss: 0.688873]\n",
      "epoch:1 step:1868 [D loss: 0.174329, acc.: 75.78%] [G loss: 0.565396]\n",
      "epoch:1 step:1869 [D loss: 0.173451, acc.: 72.66%] [G loss: 0.625795]\n",
      "epoch:1 step:1870 [D loss: 0.159797, acc.: 78.12%] [G loss: 0.648964]\n",
      "epoch:1 step:1871 [D loss: 0.176277, acc.: 74.22%] [G loss: 0.616346]\n",
      "epoch:1 step:1872 [D loss: 0.127322, acc.: 82.81%] [G loss: 0.730243]\n",
      "epoch:1 step:1873 [D loss: 0.088394, acc.: 87.50%] [G loss: 0.860674]\n",
      "epoch:1 step:1874 [D loss: 0.222384, acc.: 59.38%] [G loss: 0.602831]\n",
      "epoch:2 step:1875 [D loss: 0.170499, acc.: 76.56%] [G loss: 0.731334]\n",
      "epoch:2 step:1876 [D loss: 0.161494, acc.: 83.59%] [G loss: 0.744453]\n",
      "epoch:2 step:1877 [D loss: 0.183558, acc.: 74.22%] [G loss: 0.678063]\n",
      "epoch:2 step:1878 [D loss: 0.133407, acc.: 81.25%] [G loss: 0.707538]\n",
      "epoch:2 step:1879 [D loss: 0.160761, acc.: 80.47%] [G loss: 0.651375]\n",
      "epoch:2 step:1880 [D loss: 0.137025, acc.: 87.50%] [G loss: 0.665166]\n",
      "epoch:2 step:1881 [D loss: 0.193139, acc.: 71.09%] [G loss: 0.648591]\n",
      "epoch:2 step:1882 [D loss: 0.150258, acc.: 84.38%] [G loss: 0.668654]\n",
      "epoch:2 step:1883 [D loss: 0.161378, acc.: 83.59%] [G loss: 0.661598]\n",
      "epoch:2 step:1884 [D loss: 0.211770, acc.: 67.19%] [G loss: 0.641665]\n",
      "epoch:2 step:1885 [D loss: 0.138338, acc.: 83.59%] [G loss: 0.729222]\n",
      "epoch:2 step:1886 [D loss: 0.163521, acc.: 79.69%] [G loss: 0.658762]\n",
      "epoch:2 step:1887 [D loss: 0.187531, acc.: 75.00%] [G loss: 0.631598]\n",
      "epoch:2 step:1888 [D loss: 0.137587, acc.: 79.69%] [G loss: 0.703470]\n",
      "epoch:2 step:1889 [D loss: 0.173338, acc.: 76.56%] [G loss: 0.651030]\n",
      "epoch:2 step:1890 [D loss: 0.157856, acc.: 81.25%] [G loss: 0.691449]\n",
      "epoch:2 step:1891 [D loss: 0.204155, acc.: 69.53%] [G loss: 0.598416]\n",
      "epoch:2 step:1892 [D loss: 0.180368, acc.: 75.00%] [G loss: 0.638659]\n",
      "epoch:2 step:1893 [D loss: 0.170689, acc.: 75.00%] [G loss: 0.714983]\n",
      "epoch:2 step:1894 [D loss: 0.209649, acc.: 68.75%] [G loss: 0.611701]\n",
      "epoch:2 step:1895 [D loss: 0.133532, acc.: 85.94%] [G loss: 0.745436]\n",
      "epoch:2 step:1896 [D loss: 0.144095, acc.: 79.69%] [G loss: 0.727095]\n",
      "epoch:2 step:1897 [D loss: 0.200955, acc.: 71.09%] [G loss: 0.596927]\n",
      "epoch:2 step:1898 [D loss: 0.182760, acc.: 71.88%] [G loss: 0.652347]\n",
      "epoch:2 step:1899 [D loss: 0.137830, acc.: 85.16%] [G loss: 0.732776]\n",
      "epoch:2 step:1900 [D loss: 0.207835, acc.: 69.53%] [G loss: 0.595536]\n",
      "epoch:2 step:1901 [D loss: 0.139969, acc.: 82.03%] [G loss: 0.694434]\n",
      "epoch:2 step:1902 [D loss: 0.166052, acc.: 78.12%] [G loss: 0.675461]\n",
      "epoch:2 step:1903 [D loss: 0.139495, acc.: 84.38%] [G loss: 0.677391]\n",
      "epoch:2 step:1904 [D loss: 0.155676, acc.: 79.69%] [G loss: 0.659187]\n",
      "epoch:2 step:1905 [D loss: 0.158580, acc.: 78.91%] [G loss: 0.671728]\n",
      "epoch:2 step:1906 [D loss: 0.120238, acc.: 89.06%] [G loss: 0.801375]\n",
      "epoch:2 step:1907 [D loss: 0.180353, acc.: 74.22%] [G loss: 0.657170]\n",
      "epoch:2 step:1908 [D loss: 0.160525, acc.: 81.25%] [G loss: 0.697633]\n",
      "epoch:2 step:1909 [D loss: 0.110473, acc.: 89.06%] [G loss: 0.749736]\n",
      "epoch:2 step:1910 [D loss: 0.139173, acc.: 80.47%] [G loss: 0.721163]\n",
      "epoch:2 step:1911 [D loss: 0.192233, acc.: 73.44%] [G loss: 0.596764]\n",
      "epoch:2 step:1912 [D loss: 0.195072, acc.: 70.31%] [G loss: 0.598634]\n",
      "epoch:2 step:1913 [D loss: 0.146538, acc.: 78.91%] [G loss: 0.751530]\n",
      "epoch:2 step:1914 [D loss: 0.127946, acc.: 81.25%] [G loss: 0.751717]\n",
      "epoch:2 step:1915 [D loss: 0.191188, acc.: 76.56%] [G loss: 0.609996]\n",
      "epoch:2 step:1916 [D loss: 0.150016, acc.: 81.25%] [G loss: 0.662417]\n",
      "epoch:2 step:1917 [D loss: 0.123302, acc.: 82.03%] [G loss: 0.707083]\n",
      "epoch:2 step:1918 [D loss: 0.191613, acc.: 71.09%] [G loss: 0.631165]\n",
      "epoch:2 step:1919 [D loss: 0.163492, acc.: 75.78%] [G loss: 0.662224]\n",
      "epoch:2 step:1920 [D loss: 0.153285, acc.: 80.47%] [G loss: 0.656902]\n",
      "epoch:2 step:1921 [D loss: 0.173529, acc.: 76.56%] [G loss: 0.623492]\n",
      "epoch:2 step:1922 [D loss: 0.208742, acc.: 72.66%] [G loss: 0.555119]\n",
      "epoch:2 step:1923 [D loss: 0.178876, acc.: 74.22%] [G loss: 0.634219]\n",
      "epoch:2 step:1924 [D loss: 0.146056, acc.: 78.91%] [G loss: 0.718186]\n",
      "epoch:2 step:1925 [D loss: 0.203295, acc.: 70.31%] [G loss: 0.629908]\n",
      "epoch:2 step:1926 [D loss: 0.180959, acc.: 71.88%] [G loss: 0.647884]\n",
      "epoch:2 step:1927 [D loss: 0.141382, acc.: 85.16%] [G loss: 0.749781]\n",
      "epoch:2 step:1928 [D loss: 0.189776, acc.: 72.66%] [G loss: 0.654231]\n",
      "epoch:2 step:1929 [D loss: 0.187382, acc.: 72.66%] [G loss: 0.644902]\n",
      "epoch:2 step:1930 [D loss: 0.184622, acc.: 76.56%] [G loss: 0.652568]\n",
      "epoch:2 step:1931 [D loss: 0.158242, acc.: 79.69%] [G loss: 0.660264]\n",
      "epoch:2 step:1932 [D loss: 0.164232, acc.: 74.22%] [G loss: 0.702572]\n",
      "epoch:2 step:1933 [D loss: 0.148846, acc.: 79.69%] [G loss: 0.658919]\n",
      "epoch:2 step:1934 [D loss: 0.146543, acc.: 78.91%] [G loss: 0.624998]\n",
      "epoch:2 step:1935 [D loss: 0.179063, acc.: 74.22%] [G loss: 0.627766]\n",
      "epoch:2 step:1936 [D loss: 0.186274, acc.: 71.88%] [G loss: 0.616573]\n",
      "epoch:2 step:1937 [D loss: 0.179058, acc.: 74.22%] [G loss: 0.605181]\n",
      "epoch:2 step:1938 [D loss: 0.174517, acc.: 77.34%] [G loss: 0.606373]\n",
      "epoch:2 step:1939 [D loss: 0.185392, acc.: 73.44%] [G loss: 0.629220]\n",
      "epoch:2 step:1940 [D loss: 0.178463, acc.: 77.34%] [G loss: 0.607563]\n",
      "epoch:2 step:1941 [D loss: 0.195994, acc.: 68.75%] [G loss: 0.609631]\n",
      "epoch:2 step:1942 [D loss: 0.178455, acc.: 77.34%] [G loss: 0.606735]\n",
      "epoch:2 step:1943 [D loss: 0.198336, acc.: 68.75%] [G loss: 0.592973]\n",
      "epoch:2 step:1944 [D loss: 0.154362, acc.: 78.12%] [G loss: 0.664508]\n",
      "epoch:2 step:1945 [D loss: 0.188151, acc.: 71.09%] [G loss: 0.631131]\n",
      "epoch:2 step:1946 [D loss: 0.153147, acc.: 81.25%] [G loss: 0.638551]\n",
      "epoch:2 step:1947 [D loss: 0.152349, acc.: 81.25%] [G loss: 0.637444]\n",
      "epoch:2 step:1948 [D loss: 0.140017, acc.: 80.47%] [G loss: 0.679645]\n",
      "epoch:2 step:1949 [D loss: 0.151004, acc.: 78.12%] [G loss: 0.765905]\n",
      "epoch:2 step:1950 [D loss: 0.183649, acc.: 73.44%] [G loss: 0.622849]\n",
      "epoch:2 step:1951 [D loss: 0.110562, acc.: 86.72%] [G loss: 0.757541]\n",
      "epoch:2 step:1952 [D loss: 0.256451, acc.: 59.38%] [G loss: 0.536044]\n",
      "epoch:2 step:1953 [D loss: 0.163736, acc.: 79.69%] [G loss: 0.649772]\n",
      "epoch:2 step:1954 [D loss: 0.180856, acc.: 71.09%] [G loss: 0.613912]\n",
      "epoch:2 step:1955 [D loss: 0.202657, acc.: 70.31%] [G loss: 0.576151]\n",
      "epoch:2 step:1956 [D loss: 0.174746, acc.: 74.22%] [G loss: 0.621658]\n",
      "epoch:2 step:1957 [D loss: 0.168713, acc.: 76.56%] [G loss: 0.630671]\n",
      "epoch:2 step:1958 [D loss: 0.175498, acc.: 75.00%] [G loss: 0.618577]\n",
      "epoch:2 step:1959 [D loss: 0.168888, acc.: 78.91%] [G loss: 0.632471]\n",
      "epoch:2 step:1960 [D loss: 0.172274, acc.: 77.34%] [G loss: 0.608352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1961 [D loss: 0.148447, acc.: 78.91%] [G loss: 0.637601]\n",
      "epoch:2 step:1962 [D loss: 0.146737, acc.: 81.25%] [G loss: 0.654141]\n",
      "epoch:2 step:1963 [D loss: 0.170523, acc.: 78.91%] [G loss: 0.619872]\n",
      "epoch:2 step:1964 [D loss: 0.161576, acc.: 76.56%] [G loss: 0.654751]\n",
      "epoch:2 step:1965 [D loss: 0.179364, acc.: 76.56%] [G loss: 0.573400]\n",
      "epoch:2 step:1966 [D loss: 0.173909, acc.: 74.22%] [G loss: 0.623407]\n",
      "epoch:2 step:1967 [D loss: 0.165682, acc.: 81.25%] [G loss: 0.637863]\n",
      "epoch:2 step:1968 [D loss: 0.145020, acc.: 80.47%] [G loss: 0.678800]\n",
      "epoch:2 step:1969 [D loss: 0.153303, acc.: 78.91%] [G loss: 0.646511]\n",
      "epoch:2 step:1970 [D loss: 0.141469, acc.: 85.16%] [G loss: 0.652760]\n",
      "epoch:2 step:1971 [D loss: 0.173923, acc.: 75.78%] [G loss: 0.662703]\n",
      "epoch:2 step:1972 [D loss: 0.142836, acc.: 79.69%] [G loss: 0.692085]\n",
      "epoch:2 step:1973 [D loss: 0.159945, acc.: 74.22%] [G loss: 0.673496]\n",
      "epoch:2 step:1974 [D loss: 0.131753, acc.: 79.69%] [G loss: 0.691648]\n",
      "epoch:2 step:1975 [D loss: 0.138313, acc.: 80.47%] [G loss: 0.660324]\n",
      "epoch:2 step:1976 [D loss: 0.182991, acc.: 75.00%] [G loss: 0.618374]\n",
      "epoch:2 step:1977 [D loss: 0.137615, acc.: 82.81%] [G loss: 0.663914]\n",
      "epoch:2 step:1978 [D loss: 0.157375, acc.: 78.91%] [G loss: 0.668663]\n",
      "epoch:2 step:1979 [D loss: 0.165047, acc.: 75.78%] [G loss: 0.634818]\n",
      "epoch:2 step:1980 [D loss: 0.233681, acc.: 60.94%] [G loss: 0.528719]\n",
      "epoch:2 step:1981 [D loss: 0.180488, acc.: 78.12%] [G loss: 0.577726]\n",
      "epoch:2 step:1982 [D loss: 0.174761, acc.: 77.34%] [G loss: 0.692741]\n",
      "epoch:2 step:1983 [D loss: 0.204877, acc.: 71.88%] [G loss: 0.584539]\n",
      "epoch:2 step:1984 [D loss: 0.169915, acc.: 77.34%] [G loss: 0.608369]\n",
      "epoch:2 step:1985 [D loss: 0.202607, acc.: 66.41%] [G loss: 0.594589]\n",
      "epoch:2 step:1986 [D loss: 0.171958, acc.: 75.00%] [G loss: 0.622979]\n",
      "epoch:2 step:1987 [D loss: 0.212889, acc.: 70.31%] [G loss: 0.606261]\n",
      "epoch:2 step:1988 [D loss: 0.184293, acc.: 74.22%] [G loss: 0.618120]\n",
      "epoch:2 step:1989 [D loss: 0.187184, acc.: 72.66%] [G loss: 0.667038]\n",
      "epoch:2 step:1990 [D loss: 0.208348, acc.: 67.19%] [G loss: 0.609081]\n",
      "epoch:2 step:1991 [D loss: 0.149397, acc.: 85.16%] [G loss: 0.677805]\n",
      "epoch:2 step:1992 [D loss: 0.148515, acc.: 82.03%] [G loss: 0.713160]\n",
      "epoch:2 step:1993 [D loss: 0.128593, acc.: 82.81%] [G loss: 0.756917]\n",
      "epoch:2 step:1994 [D loss: 0.222996, acc.: 69.53%] [G loss: 0.562269]\n",
      "epoch:2 step:1995 [D loss: 0.160625, acc.: 78.91%] [G loss: 0.637732]\n",
      "epoch:2 step:1996 [D loss: 0.133486, acc.: 85.94%] [G loss: 0.703031]\n",
      "epoch:2 step:1997 [D loss: 0.198498, acc.: 67.97%] [G loss: 0.597215]\n",
      "epoch:2 step:1998 [D loss: 0.200961, acc.: 71.09%] [G loss: 0.569270]\n",
      "epoch:2 step:1999 [D loss: 0.129826, acc.: 84.38%] [G loss: 0.670576]\n",
      "epoch:2 step:2000 [D loss: 0.143302, acc.: 78.12%] [G loss: 0.690704]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.339808\n",
      "FID: 89.148323\n",
      "0 = 14.397038281154575\n",
      "1 = 0.11129331881202405\n",
      "2 = 0.9958000183105469\n",
      "3 = 0.991599977016449\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.991599977016449\n",
      "7 = 11.512144168818018\n",
      "8 = 0.193864874471455\n",
      "9 = 0.9501500129699707\n",
      "10 = 0.9254999756813049\n",
      "11 = 0.9747999906539917\n",
      "12 = 0.973493218421936\n",
      "13 = 0.9254999756813049\n",
      "14 = 4.339829444885254\n",
      "15 = 7.4029669761657715\n",
      "16 = 0.4013656675815582\n",
      "17 = 4.339808464050293\n",
      "18 = 89.14832305908203\n",
      "epoch:2 step:2001 [D loss: 0.167147, acc.: 75.00%] [G loss: 0.587337]\n",
      "epoch:2 step:2002 [D loss: 0.181115, acc.: 72.66%] [G loss: 0.599342]\n",
      "epoch:2 step:2003 [D loss: 0.200310, acc.: 66.41%] [G loss: 0.587619]\n",
      "epoch:2 step:2004 [D loss: 0.114086, acc.: 88.28%] [G loss: 0.708786]\n",
      "epoch:2 step:2005 [D loss: 0.150147, acc.: 79.69%] [G loss: 0.689323]\n",
      "epoch:2 step:2006 [D loss: 0.167324, acc.: 76.56%] [G loss: 0.625717]\n",
      "epoch:2 step:2007 [D loss: 0.152523, acc.: 78.12%] [G loss: 0.621144]\n",
      "epoch:2 step:2008 [D loss: 0.121949, acc.: 82.81%] [G loss: 0.754224]\n",
      "epoch:2 step:2009 [D loss: 0.143474, acc.: 82.03%] [G loss: 0.689725]\n",
      "epoch:2 step:2010 [D loss: 0.169990, acc.: 74.22%] [G loss: 0.568017]\n",
      "epoch:2 step:2011 [D loss: 0.182976, acc.: 75.78%] [G loss: 0.558357]\n",
      "epoch:2 step:2012 [D loss: 0.173050, acc.: 77.34%] [G loss: 0.670218]\n",
      "epoch:2 step:2013 [D loss: 0.174748, acc.: 77.34%] [G loss: 0.567775]\n",
      "epoch:2 step:2014 [D loss: 0.184866, acc.: 75.00%] [G loss: 0.537713]\n",
      "epoch:2 step:2015 [D loss: 0.149631, acc.: 77.34%] [G loss: 0.681628]\n",
      "epoch:2 step:2016 [D loss: 0.157085, acc.: 71.09%] [G loss: 0.660761]\n",
      "epoch:2 step:2017 [D loss: 0.173381, acc.: 76.56%] [G loss: 0.617617]\n",
      "epoch:2 step:2018 [D loss: 0.147275, acc.: 82.03%] [G loss: 0.641584]\n",
      "epoch:2 step:2019 [D loss: 0.164357, acc.: 77.34%] [G loss: 0.657287]\n",
      "epoch:2 step:2020 [D loss: 0.156623, acc.: 79.69%] [G loss: 0.664638]\n",
      "epoch:2 step:2021 [D loss: 0.187423, acc.: 76.56%] [G loss: 0.634753]\n",
      "epoch:2 step:2022 [D loss: 0.158181, acc.: 81.25%] [G loss: 0.627252]\n",
      "epoch:2 step:2023 [D loss: 0.125384, acc.: 87.50%] [G loss: 0.717707]\n",
      "epoch:2 step:2024 [D loss: 0.225480, acc.: 68.75%] [G loss: 0.559276]\n",
      "epoch:2 step:2025 [D loss: 0.144500, acc.: 81.25%] [G loss: 0.676961]\n",
      "epoch:2 step:2026 [D loss: 0.138376, acc.: 84.38%] [G loss: 0.713586]\n",
      "epoch:2 step:2027 [D loss: 0.183068, acc.: 74.22%] [G loss: 0.645014]\n",
      "epoch:2 step:2028 [D loss: 0.175918, acc.: 71.09%] [G loss: 0.583724]\n",
      "epoch:2 step:2029 [D loss: 0.146592, acc.: 84.38%] [G loss: 0.642883]\n",
      "epoch:2 step:2030 [D loss: 0.175051, acc.: 73.44%] [G loss: 0.651100]\n",
      "epoch:2 step:2031 [D loss: 0.170036, acc.: 77.34%] [G loss: 0.605542]\n",
      "epoch:2 step:2032 [D loss: 0.172384, acc.: 76.56%] [G loss: 0.567081]\n",
      "epoch:2 step:2033 [D loss: 0.168587, acc.: 76.56%] [G loss: 0.628977]\n",
      "epoch:2 step:2034 [D loss: 0.181994, acc.: 75.00%] [G loss: 0.609228]\n",
      "epoch:2 step:2035 [D loss: 0.170134, acc.: 74.22%] [G loss: 0.615766]\n",
      "epoch:2 step:2036 [D loss: 0.145730, acc.: 76.56%] [G loss: 0.638708]\n",
      "epoch:2 step:2037 [D loss: 0.160126, acc.: 75.78%] [G loss: 0.664244]\n",
      "epoch:2 step:2038 [D loss: 0.149963, acc.: 80.47%] [G loss: 0.692144]\n",
      "epoch:2 step:2039 [D loss: 0.147793, acc.: 84.38%] [G loss: 0.653050]\n",
      "epoch:2 step:2040 [D loss: 0.164790, acc.: 79.69%] [G loss: 0.574617]\n",
      "epoch:2 step:2041 [D loss: 0.143218, acc.: 85.16%] [G loss: 0.600372]\n",
      "epoch:2 step:2042 [D loss: 0.174170, acc.: 78.91%] [G loss: 0.597659]\n",
      "epoch:2 step:2043 [D loss: 0.151846, acc.: 76.56%] [G loss: 0.638844]\n",
      "epoch:2 step:2044 [D loss: 0.146661, acc.: 80.47%] [G loss: 0.594262]\n",
      "epoch:2 step:2045 [D loss: 0.123175, acc.: 88.28%] [G loss: 0.682811]\n",
      "epoch:2 step:2046 [D loss: 0.140852, acc.: 84.38%] [G loss: 0.665019]\n",
      "epoch:2 step:2047 [D loss: 0.132913, acc.: 82.03%] [G loss: 0.658897]\n",
      "epoch:2 step:2048 [D loss: 0.209643, acc.: 67.97%] [G loss: 0.586221]\n",
      "epoch:2 step:2049 [D loss: 0.167363, acc.: 71.88%] [G loss: 0.604660]\n",
      "epoch:2 step:2050 [D loss: 0.115890, acc.: 89.06%] [G loss: 0.665132]\n",
      "epoch:2 step:2051 [D loss: 0.171698, acc.: 74.22%] [G loss: 0.635668]\n",
      "epoch:2 step:2052 [D loss: 0.117215, acc.: 89.84%] [G loss: 0.677888]\n",
      "epoch:2 step:2053 [D loss: 0.150115, acc.: 76.56%] [G loss: 0.651465]\n",
      "epoch:2 step:2054 [D loss: 0.166658, acc.: 76.56%] [G loss: 0.617613]\n",
      "epoch:2 step:2055 [D loss: 0.167037, acc.: 79.69%] [G loss: 0.647584]\n",
      "epoch:2 step:2056 [D loss: 0.153014, acc.: 81.25%] [G loss: 0.630666]\n",
      "epoch:2 step:2057 [D loss: 0.157032, acc.: 78.12%] [G loss: 0.637472]\n",
      "epoch:2 step:2058 [D loss: 0.154978, acc.: 79.69%] [G loss: 0.615786]\n",
      "epoch:2 step:2059 [D loss: 0.187412, acc.: 71.09%] [G loss: 0.559406]\n",
      "epoch:2 step:2060 [D loss: 0.116651, acc.: 87.50%] [G loss: 0.699858]\n",
      "epoch:2 step:2061 [D loss: 0.159169, acc.: 77.34%] [G loss: 0.640574]\n",
      "epoch:2 step:2062 [D loss: 0.143112, acc.: 85.16%] [G loss: 0.678228]\n",
      "epoch:2 step:2063 [D loss: 0.160118, acc.: 77.34%] [G loss: 0.689982]\n",
      "epoch:2 step:2064 [D loss: 0.117713, acc.: 89.84%] [G loss: 0.678470]\n",
      "epoch:2 step:2065 [D loss: 0.150311, acc.: 82.03%] [G loss: 0.661298]\n",
      "epoch:2 step:2066 [D loss: 0.168645, acc.: 75.78%] [G loss: 0.624440]\n",
      "epoch:2 step:2067 [D loss: 0.182812, acc.: 74.22%] [G loss: 0.645117]\n",
      "epoch:2 step:2068 [D loss: 0.121264, acc.: 91.41%] [G loss: 0.676087]\n",
      "epoch:2 step:2069 [D loss: 0.176982, acc.: 73.44%] [G loss: 0.672472]\n",
      "epoch:2 step:2070 [D loss: 0.182747, acc.: 73.44%] [G loss: 0.652083]\n",
      "epoch:2 step:2071 [D loss: 0.157229, acc.: 76.56%] [G loss: 0.662831]\n",
      "epoch:2 step:2072 [D loss: 0.132091, acc.: 85.94%] [G loss: 0.692070]\n",
      "epoch:2 step:2073 [D loss: 0.202090, acc.: 71.88%] [G loss: 0.631014]\n",
      "epoch:2 step:2074 [D loss: 0.205800, acc.: 65.62%] [G loss: 0.592227]\n",
      "epoch:2 step:2075 [D loss: 0.157279, acc.: 75.00%] [G loss: 0.634900]\n",
      "epoch:2 step:2076 [D loss: 0.190668, acc.: 71.88%] [G loss: 0.617428]\n",
      "epoch:2 step:2077 [D loss: 0.160833, acc.: 78.91%] [G loss: 0.599320]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2078 [D loss: 0.183213, acc.: 73.44%] [G loss: 0.643682]\n",
      "epoch:2 step:2079 [D loss: 0.118096, acc.: 85.94%] [G loss: 0.681289]\n",
      "epoch:2 step:2080 [D loss: 0.149276, acc.: 78.12%] [G loss: 0.667619]\n",
      "epoch:2 step:2081 [D loss: 0.125400, acc.: 85.16%] [G loss: 0.654362]\n",
      "epoch:2 step:2082 [D loss: 0.120790, acc.: 86.72%] [G loss: 0.653706]\n",
      "epoch:2 step:2083 [D loss: 0.127748, acc.: 86.72%] [G loss: 0.674796]\n",
      "epoch:2 step:2084 [D loss: 0.182702, acc.: 71.09%] [G loss: 0.617751]\n",
      "epoch:2 step:2085 [D loss: 0.162394, acc.: 75.78%] [G loss: 0.586938]\n",
      "epoch:2 step:2086 [D loss: 0.144153, acc.: 83.59%] [G loss: 0.642027]\n",
      "epoch:2 step:2087 [D loss: 0.154784, acc.: 78.12%] [G loss: 0.669114]\n",
      "epoch:2 step:2088 [D loss: 0.219002, acc.: 66.41%] [G loss: 0.530271]\n",
      "epoch:2 step:2089 [D loss: 0.182663, acc.: 73.44%] [G loss: 0.600766]\n",
      "epoch:2 step:2090 [D loss: 0.164438, acc.: 80.47%] [G loss: 0.614415]\n",
      "epoch:2 step:2091 [D loss: 0.148456, acc.: 82.03%] [G loss: 0.679703]\n",
      "epoch:2 step:2092 [D loss: 0.167521, acc.: 82.03%] [G loss: 0.643976]\n",
      "epoch:2 step:2093 [D loss: 0.163131, acc.: 77.34%] [G loss: 0.628348]\n",
      "epoch:2 step:2094 [D loss: 0.140322, acc.: 77.34%] [G loss: 0.683783]\n",
      "epoch:2 step:2095 [D loss: 0.135632, acc.: 83.59%] [G loss: 0.725216]\n",
      "epoch:2 step:2096 [D loss: 0.170099, acc.: 75.78%] [G loss: 0.677823]\n",
      "epoch:2 step:2097 [D loss: 0.139268, acc.: 82.81%] [G loss: 0.695795]\n",
      "epoch:2 step:2098 [D loss: 0.168174, acc.: 76.56%] [G loss: 0.624046]\n",
      "epoch:2 step:2099 [D loss: 0.191217, acc.: 76.56%] [G loss: 0.616734]\n",
      "epoch:2 step:2100 [D loss: 0.145429, acc.: 82.03%] [G loss: 0.614755]\n",
      "epoch:2 step:2101 [D loss: 0.190222, acc.: 75.00%] [G loss: 0.571980]\n",
      "epoch:2 step:2102 [D loss: 0.192305, acc.: 73.44%] [G loss: 0.602611]\n",
      "epoch:2 step:2103 [D loss: 0.151264, acc.: 81.25%] [G loss: 0.620262]\n",
      "epoch:2 step:2104 [D loss: 0.137172, acc.: 84.38%] [G loss: 0.689595]\n",
      "epoch:2 step:2105 [D loss: 0.162339, acc.: 83.59%] [G loss: 0.713516]\n",
      "epoch:2 step:2106 [D loss: 0.121323, acc.: 88.28%] [G loss: 0.738166]\n",
      "epoch:2 step:2107 [D loss: 0.206404, acc.: 74.22%] [G loss: 0.612140]\n",
      "epoch:2 step:2108 [D loss: 0.139362, acc.: 85.16%] [G loss: 0.644043]\n",
      "epoch:2 step:2109 [D loss: 0.167997, acc.: 76.56%] [G loss: 0.584518]\n",
      "epoch:2 step:2110 [D loss: 0.141044, acc.: 82.03%] [G loss: 0.639491]\n",
      "epoch:2 step:2111 [D loss: 0.178764, acc.: 75.78%] [G loss: 0.590985]\n",
      "epoch:2 step:2112 [D loss: 0.140283, acc.: 85.16%] [G loss: 0.607861]\n",
      "epoch:2 step:2113 [D loss: 0.176211, acc.: 77.34%] [G loss: 0.607833]\n",
      "epoch:2 step:2114 [D loss: 0.163016, acc.: 74.22%] [G loss: 0.602348]\n",
      "epoch:2 step:2115 [D loss: 0.187696, acc.: 67.97%] [G loss: 0.615262]\n",
      "epoch:2 step:2116 [D loss: 0.147197, acc.: 80.47%] [G loss: 0.661203]\n",
      "epoch:2 step:2117 [D loss: 0.136040, acc.: 84.38%] [G loss: 0.659889]\n",
      "epoch:2 step:2118 [D loss: 0.134824, acc.: 85.16%] [G loss: 0.641854]\n",
      "epoch:2 step:2119 [D loss: 0.130706, acc.: 85.16%] [G loss: 0.678315]\n",
      "epoch:2 step:2120 [D loss: 0.175134, acc.: 74.22%] [G loss: 0.662180]\n",
      "epoch:2 step:2121 [D loss: 0.159872, acc.: 80.47%] [G loss: 0.656415]\n",
      "epoch:2 step:2122 [D loss: 0.146989, acc.: 80.47%] [G loss: 0.643233]\n",
      "epoch:2 step:2123 [D loss: 0.152513, acc.: 81.25%] [G loss: 0.644940]\n",
      "epoch:2 step:2124 [D loss: 0.200471, acc.: 69.53%] [G loss: 0.590587]\n",
      "epoch:2 step:2125 [D loss: 0.143657, acc.: 82.81%] [G loss: 0.651124]\n",
      "epoch:2 step:2126 [D loss: 0.145333, acc.: 85.94%] [G loss: 0.658345]\n",
      "epoch:2 step:2127 [D loss: 0.147856, acc.: 79.69%] [G loss: 0.684048]\n",
      "epoch:2 step:2128 [D loss: 0.126253, acc.: 82.81%] [G loss: 0.687876]\n",
      "epoch:2 step:2129 [D loss: 0.120163, acc.: 86.72%] [G loss: 0.694539]\n",
      "epoch:2 step:2130 [D loss: 0.135184, acc.: 82.81%] [G loss: 0.717422]\n",
      "epoch:2 step:2131 [D loss: 0.147780, acc.: 82.81%] [G loss: 0.646982]\n",
      "epoch:2 step:2132 [D loss: 0.128083, acc.: 85.16%] [G loss: 0.663966]\n",
      "epoch:2 step:2133 [D loss: 0.124077, acc.: 86.72%] [G loss: 0.689720]\n",
      "epoch:2 step:2134 [D loss: 0.138796, acc.: 82.81%] [G loss: 0.680273]\n",
      "epoch:2 step:2135 [D loss: 0.152194, acc.: 75.78%] [G loss: 0.679414]\n",
      "epoch:2 step:2136 [D loss: 0.144106, acc.: 82.03%] [G loss: 0.656178]\n",
      "epoch:2 step:2137 [D loss: 0.207744, acc.: 66.41%] [G loss: 0.547631]\n",
      "epoch:2 step:2138 [D loss: 0.143814, acc.: 79.69%] [G loss: 0.709472]\n",
      "epoch:2 step:2139 [D loss: 0.186867, acc.: 74.22%] [G loss: 0.647663]\n",
      "epoch:2 step:2140 [D loss: 0.148125, acc.: 82.81%] [G loss: 0.606579]\n",
      "epoch:2 step:2141 [D loss: 0.172842, acc.: 73.44%] [G loss: 0.620909]\n",
      "epoch:2 step:2142 [D loss: 0.153822, acc.: 78.91%] [G loss: 0.640990]\n",
      "epoch:2 step:2143 [D loss: 0.159482, acc.: 80.47%] [G loss: 0.644379]\n",
      "epoch:2 step:2144 [D loss: 0.127991, acc.: 84.38%] [G loss: 0.703590]\n",
      "epoch:2 step:2145 [D loss: 0.144188, acc.: 82.81%] [G loss: 0.699127]\n",
      "epoch:2 step:2146 [D loss: 0.178931, acc.: 74.22%] [G loss: 0.641524]\n",
      "epoch:2 step:2147 [D loss: 0.114241, acc.: 88.28%] [G loss: 0.769251]\n",
      "epoch:2 step:2148 [D loss: 0.207860, acc.: 70.31%] [G loss: 0.584345]\n",
      "epoch:2 step:2149 [D loss: 0.201355, acc.: 69.53%] [G loss: 0.578292]\n",
      "epoch:2 step:2150 [D loss: 0.169042, acc.: 76.56%] [G loss: 0.658405]\n",
      "epoch:2 step:2151 [D loss: 0.182901, acc.: 71.88%] [G loss: 0.593226]\n",
      "epoch:2 step:2152 [D loss: 0.160512, acc.: 73.44%] [G loss: 0.679784]\n",
      "epoch:2 step:2153 [D loss: 0.139506, acc.: 78.91%] [G loss: 0.727031]\n",
      "epoch:2 step:2154 [D loss: 0.161462, acc.: 75.78%] [G loss: 0.705368]\n",
      "epoch:2 step:2155 [D loss: 0.164671, acc.: 79.69%] [G loss: 0.676367]\n",
      "epoch:2 step:2156 [D loss: 0.144057, acc.: 77.34%] [G loss: 0.653777]\n",
      "epoch:2 step:2157 [D loss: 0.127389, acc.: 82.81%] [G loss: 0.737765]\n",
      "epoch:2 step:2158 [D loss: 0.164093, acc.: 75.78%] [G loss: 0.669433]\n",
      "epoch:2 step:2159 [D loss: 0.120474, acc.: 85.94%] [G loss: 0.766459]\n",
      "epoch:2 step:2160 [D loss: 0.151263, acc.: 81.25%] [G loss: 0.655487]\n",
      "epoch:2 step:2161 [D loss: 0.185745, acc.: 72.66%] [G loss: 0.601204]\n",
      "epoch:2 step:2162 [D loss: 0.163403, acc.: 77.34%] [G loss: 0.671482]\n",
      "epoch:2 step:2163 [D loss: 0.120342, acc.: 87.50%] [G loss: 0.717883]\n",
      "epoch:2 step:2164 [D loss: 0.156495, acc.: 79.69%] [G loss: 0.705666]\n",
      "epoch:2 step:2165 [D loss: 0.187373, acc.: 70.31%] [G loss: 0.661594]\n",
      "epoch:2 step:2166 [D loss: 0.153912, acc.: 78.91%] [G loss: 0.656185]\n",
      "epoch:2 step:2167 [D loss: 0.145216, acc.: 82.03%] [G loss: 0.697616]\n",
      "epoch:2 step:2168 [D loss: 0.140044, acc.: 82.03%] [G loss: 0.691873]\n",
      "epoch:2 step:2169 [D loss: 0.165393, acc.: 80.47%] [G loss: 0.646274]\n",
      "epoch:2 step:2170 [D loss: 0.131319, acc.: 84.38%] [G loss: 0.714879]\n",
      "epoch:2 step:2171 [D loss: 0.167466, acc.: 76.56%] [G loss: 0.672218]\n",
      "epoch:2 step:2172 [D loss: 0.168490, acc.: 74.22%] [G loss: 0.620918]\n",
      "epoch:2 step:2173 [D loss: 0.134847, acc.: 83.59%] [G loss: 0.751369]\n",
      "epoch:2 step:2174 [D loss: 0.157447, acc.: 84.38%] [G loss: 0.704300]\n",
      "epoch:2 step:2175 [D loss: 0.171143, acc.: 74.22%] [G loss: 0.649484]\n",
      "epoch:2 step:2176 [D loss: 0.123101, acc.: 84.38%] [G loss: 0.724631]\n",
      "epoch:2 step:2177 [D loss: 0.139979, acc.: 80.47%] [G loss: 0.686720]\n",
      "epoch:2 step:2178 [D loss: 0.123810, acc.: 83.59%] [G loss: 0.745221]\n",
      "epoch:2 step:2179 [D loss: 0.115180, acc.: 88.28%] [G loss: 0.738213]\n",
      "epoch:2 step:2180 [D loss: 0.161422, acc.: 77.34%] [G loss: 0.686260]\n",
      "epoch:2 step:2181 [D loss: 0.116826, acc.: 84.38%] [G loss: 0.714655]\n",
      "epoch:2 step:2182 [D loss: 0.149000, acc.: 82.03%] [G loss: 0.700336]\n",
      "epoch:2 step:2183 [D loss: 0.128305, acc.: 83.59%] [G loss: 0.713803]\n",
      "epoch:2 step:2184 [D loss: 0.166887, acc.: 78.12%] [G loss: 0.734895]\n",
      "epoch:2 step:2185 [D loss: 0.125747, acc.: 82.81%] [G loss: 0.721518]\n",
      "epoch:2 step:2186 [D loss: 0.123239, acc.: 85.94%] [G loss: 0.713851]\n",
      "epoch:2 step:2187 [D loss: 0.130307, acc.: 84.38%] [G loss: 0.748361]\n",
      "epoch:2 step:2188 [D loss: 0.106568, acc.: 89.06%] [G loss: 0.793673]\n",
      "epoch:2 step:2189 [D loss: 0.151343, acc.: 78.91%] [G loss: 0.724174]\n",
      "epoch:2 step:2190 [D loss: 0.234896, acc.: 64.06%] [G loss: 0.578869]\n",
      "epoch:2 step:2191 [D loss: 0.156418, acc.: 78.91%] [G loss: 0.621339]\n",
      "epoch:2 step:2192 [D loss: 0.171051, acc.: 75.00%] [G loss: 0.702152]\n",
      "epoch:2 step:2193 [D loss: 0.166101, acc.: 75.00%] [G loss: 0.665093]\n",
      "epoch:2 step:2194 [D loss: 0.154726, acc.: 82.03%] [G loss: 0.733068]\n",
      "epoch:2 step:2195 [D loss: 0.112051, acc.: 86.72%] [G loss: 0.776525]\n",
      "epoch:2 step:2196 [D loss: 0.188035, acc.: 69.53%] [G loss: 0.636593]\n",
      "epoch:2 step:2197 [D loss: 0.157031, acc.: 78.12%] [G loss: 0.684828]\n",
      "epoch:2 step:2198 [D loss: 0.171424, acc.: 76.56%] [G loss: 0.632131]\n",
      "epoch:2 step:2199 [D loss: 0.117414, acc.: 86.72%] [G loss: 0.712749]\n",
      "epoch:2 step:2200 [D loss: 0.183935, acc.: 71.09%] [G loss: 0.647978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.499365\n",
      "FID: 83.166122\n",
      "0 = 14.214161709880905\n",
      "1 = 0.10798193844130145\n",
      "2 = 0.9943000078201294\n",
      "3 = 0.9886000156402588\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9886000156402588\n",
      "7 = 11.201973614943011\n",
      "8 = 0.18549161389255012\n",
      "9 = 0.9389500021934509\n",
      "10 = 0.9140999913215637\n",
      "11 = 0.9638000130653381\n",
      "12 = 0.9619067907333374\n",
      "13 = 0.9140999913215637\n",
      "14 = 4.499384880065918\n",
      "15 = 7.195716857910156\n",
      "16 = 0.41936439275741577\n",
      "17 = 4.499364852905273\n",
      "18 = 83.16612243652344\n",
      "epoch:2 step:2201 [D loss: 0.155556, acc.: 78.91%] [G loss: 0.674899]\n",
      "epoch:2 step:2202 [D loss: 0.146203, acc.: 82.81%] [G loss: 0.739001]\n",
      "epoch:2 step:2203 [D loss: 0.154779, acc.: 79.69%] [G loss: 0.729167]\n",
      "epoch:2 step:2204 [D loss: 0.162812, acc.: 75.00%] [G loss: 0.655089]\n",
      "epoch:2 step:2205 [D loss: 0.141693, acc.: 78.91%] [G loss: 0.679728]\n",
      "epoch:2 step:2206 [D loss: 0.166334, acc.: 78.91%] [G loss: 0.671347]\n",
      "epoch:2 step:2207 [D loss: 0.130073, acc.: 83.59%] [G loss: 0.701117]\n",
      "epoch:2 step:2208 [D loss: 0.184981, acc.: 71.09%] [G loss: 0.693427]\n",
      "epoch:2 step:2209 [D loss: 0.140080, acc.: 83.59%] [G loss: 0.740317]\n",
      "epoch:2 step:2210 [D loss: 0.215659, acc.: 70.31%] [G loss: 0.658217]\n",
      "epoch:2 step:2211 [D loss: 0.153935, acc.: 80.47%] [G loss: 0.694821]\n",
      "epoch:2 step:2212 [D loss: 0.183940, acc.: 73.44%] [G loss: 0.669324]\n",
      "epoch:2 step:2213 [D loss: 0.127683, acc.: 89.84%] [G loss: 0.681789]\n",
      "epoch:2 step:2214 [D loss: 0.184325, acc.: 71.88%] [G loss: 0.626862]\n",
      "epoch:2 step:2215 [D loss: 0.188658, acc.: 72.66%] [G loss: 0.632632]\n",
      "epoch:2 step:2216 [D loss: 0.145179, acc.: 76.56%] [G loss: 0.681711]\n",
      "epoch:2 step:2217 [D loss: 0.128424, acc.: 82.03%] [G loss: 0.769557]\n",
      "epoch:2 step:2218 [D loss: 0.145772, acc.: 78.91%] [G loss: 0.697804]\n",
      "epoch:2 step:2219 [D loss: 0.150914, acc.: 83.59%] [G loss: 0.685475]\n",
      "epoch:2 step:2220 [D loss: 0.148586, acc.: 75.00%] [G loss: 0.734874]\n",
      "epoch:2 step:2221 [D loss: 0.125264, acc.: 86.72%] [G loss: 0.755076]\n",
      "epoch:2 step:2222 [D loss: 0.213077, acc.: 70.31%] [G loss: 0.597407]\n",
      "epoch:2 step:2223 [D loss: 0.224765, acc.: 61.72%] [G loss: 0.556383]\n",
      "epoch:2 step:2224 [D loss: 0.135083, acc.: 83.59%] [G loss: 0.707704]\n",
      "epoch:2 step:2225 [D loss: 0.196806, acc.: 74.22%] [G loss: 0.619697]\n",
      "epoch:2 step:2226 [D loss: 0.215636, acc.: 65.62%] [G loss: 0.563274]\n",
      "epoch:2 step:2227 [D loss: 0.181400, acc.: 75.00%] [G loss: 0.613635]\n",
      "epoch:2 step:2228 [D loss: 0.133502, acc.: 85.16%] [G loss: 0.748891]\n",
      "epoch:2 step:2229 [D loss: 0.127250, acc.: 85.94%] [G loss: 0.686155]\n",
      "epoch:2 step:2230 [D loss: 0.162070, acc.: 76.56%] [G loss: 0.647552]\n",
      "epoch:2 step:2231 [D loss: 0.138480, acc.: 77.34%] [G loss: 0.656730]\n",
      "epoch:2 step:2232 [D loss: 0.129144, acc.: 83.59%] [G loss: 0.672495]\n",
      "epoch:2 step:2233 [D loss: 0.149571, acc.: 82.03%] [G loss: 0.671215]\n",
      "epoch:2 step:2234 [D loss: 0.152693, acc.: 78.12%] [G loss: 0.636612]\n",
      "epoch:2 step:2235 [D loss: 0.145462, acc.: 83.59%] [G loss: 0.679826]\n",
      "epoch:2 step:2236 [D loss: 0.157869, acc.: 80.47%] [G loss: 0.649977]\n",
      "epoch:2 step:2237 [D loss: 0.140881, acc.: 82.03%] [G loss: 0.649560]\n",
      "epoch:2 step:2238 [D loss: 0.158362, acc.: 80.47%] [G loss: 0.687764]\n",
      "epoch:2 step:2239 [D loss: 0.171097, acc.: 75.00%] [G loss: 0.649589]\n",
      "epoch:2 step:2240 [D loss: 0.135417, acc.: 82.03%] [G loss: 0.688510]\n",
      "epoch:2 step:2241 [D loss: 0.185792, acc.: 70.31%] [G loss: 0.647225]\n",
      "epoch:2 step:2242 [D loss: 0.170196, acc.: 73.44%] [G loss: 0.606989]\n",
      "epoch:2 step:2243 [D loss: 0.176876, acc.: 71.09%] [G loss: 0.604401]\n",
      "epoch:2 step:2244 [D loss: 0.159841, acc.: 76.56%] [G loss: 0.620320]\n",
      "epoch:2 step:2245 [D loss: 0.123283, acc.: 85.94%] [G loss: 0.723327]\n",
      "epoch:2 step:2246 [D loss: 0.164510, acc.: 75.78%] [G loss: 0.656190]\n",
      "epoch:2 step:2247 [D loss: 0.177245, acc.: 75.00%] [G loss: 0.613542]\n",
      "epoch:2 step:2248 [D loss: 0.155559, acc.: 77.34%] [G loss: 0.672512]\n",
      "epoch:2 step:2249 [D loss: 0.189133, acc.: 69.53%] [G loss: 0.666078]\n",
      "epoch:2 step:2250 [D loss: 0.224133, acc.: 67.19%] [G loss: 0.574128]\n",
      "epoch:2 step:2251 [D loss: 0.183375, acc.: 69.53%] [G loss: 0.667469]\n",
      "epoch:2 step:2252 [D loss: 0.143529, acc.: 78.12%] [G loss: 0.688493]\n",
      "epoch:2 step:2253 [D loss: 0.200955, acc.: 67.97%] [G loss: 0.640005]\n",
      "epoch:2 step:2254 [D loss: 0.185940, acc.: 73.44%] [G loss: 0.644549]\n",
      "epoch:2 step:2255 [D loss: 0.105518, acc.: 87.50%] [G loss: 0.726372]\n",
      "epoch:2 step:2256 [D loss: 0.140011, acc.: 84.38%] [G loss: 0.652213]\n",
      "epoch:2 step:2257 [D loss: 0.197731, acc.: 67.97%] [G loss: 0.559010]\n",
      "epoch:2 step:2258 [D loss: 0.136825, acc.: 82.81%] [G loss: 0.676016]\n",
      "epoch:2 step:2259 [D loss: 0.148516, acc.: 80.47%] [G loss: 0.700205]\n",
      "epoch:2 step:2260 [D loss: 0.212627, acc.: 66.41%] [G loss: 0.573990]\n",
      "epoch:2 step:2261 [D loss: 0.173566, acc.: 76.56%] [G loss: 0.621250]\n",
      "epoch:2 step:2262 [D loss: 0.170208, acc.: 75.00%] [G loss: 0.611719]\n",
      "epoch:2 step:2263 [D loss: 0.164727, acc.: 76.56%] [G loss: 0.625115]\n",
      "epoch:2 step:2264 [D loss: 0.183573, acc.: 75.00%] [G loss: 0.593647]\n",
      "epoch:2 step:2265 [D loss: 0.139897, acc.: 83.59%] [G loss: 0.672630]\n",
      "epoch:2 step:2266 [D loss: 0.142499, acc.: 79.69%] [G loss: 0.689371]\n",
      "epoch:2 step:2267 [D loss: 0.177879, acc.: 69.53%] [G loss: 0.614471]\n",
      "epoch:2 step:2268 [D loss: 0.133281, acc.: 82.03%] [G loss: 0.666604]\n",
      "epoch:2 step:2269 [D loss: 0.142761, acc.: 82.03%] [G loss: 0.669092]\n",
      "epoch:2 step:2270 [D loss: 0.164447, acc.: 78.91%] [G loss: 0.591322]\n",
      "epoch:2 step:2271 [D loss: 0.137829, acc.: 83.59%] [G loss: 0.692541]\n",
      "epoch:2 step:2272 [D loss: 0.125784, acc.: 84.38%] [G loss: 0.754271]\n",
      "epoch:2 step:2273 [D loss: 0.126061, acc.: 85.16%] [G loss: 0.710670]\n",
      "epoch:2 step:2274 [D loss: 0.210898, acc.: 70.31%] [G loss: 0.595040]\n",
      "epoch:2 step:2275 [D loss: 0.182696, acc.: 75.00%] [G loss: 0.573445]\n",
      "epoch:2 step:2276 [D loss: 0.103785, acc.: 88.28%] [G loss: 0.739648]\n",
      "epoch:2 step:2277 [D loss: 0.154264, acc.: 77.34%] [G loss: 0.691732]\n",
      "epoch:2 step:2278 [D loss: 0.238790, acc.: 64.06%] [G loss: 0.559406]\n",
      "epoch:2 step:2279 [D loss: 0.155130, acc.: 75.78%] [G loss: 0.646553]\n",
      "epoch:2 step:2280 [D loss: 0.148618, acc.: 81.25%] [G loss: 0.651583]\n",
      "epoch:2 step:2281 [D loss: 0.183978, acc.: 69.53%] [G loss: 0.635361]\n",
      "epoch:2 step:2282 [D loss: 0.184845, acc.: 75.78%] [G loss: 0.586715]\n",
      "epoch:2 step:2283 [D loss: 0.148303, acc.: 78.91%] [G loss: 0.674867]\n",
      "epoch:2 step:2284 [D loss: 0.164165, acc.: 76.56%] [G loss: 0.646387]\n",
      "epoch:2 step:2285 [D loss: 0.163760, acc.: 78.12%] [G loss: 0.623224]\n",
      "epoch:2 step:2286 [D loss: 0.182555, acc.: 77.34%] [G loss: 0.641957]\n",
      "epoch:2 step:2287 [D loss: 0.123902, acc.: 85.94%] [G loss: 0.735448]\n",
      "epoch:2 step:2288 [D loss: 0.185426, acc.: 72.66%] [G loss: 0.619746]\n",
      "epoch:2 step:2289 [D loss: 0.168340, acc.: 77.34%] [G loss: 0.647336]\n",
      "epoch:2 step:2290 [D loss: 0.176757, acc.: 75.78%] [G loss: 0.654159]\n",
      "epoch:2 step:2291 [D loss: 0.204967, acc.: 67.97%] [G loss: 0.593922]\n",
      "epoch:2 step:2292 [D loss: 0.177616, acc.: 73.44%] [G loss: 0.598228]\n",
      "epoch:2 step:2293 [D loss: 0.158294, acc.: 77.34%] [G loss: 0.685781]\n",
      "epoch:2 step:2294 [D loss: 0.144904, acc.: 79.69%] [G loss: 0.723967]\n",
      "epoch:2 step:2295 [D loss: 0.215157, acc.: 62.50%] [G loss: 0.618154]\n",
      "epoch:2 step:2296 [D loss: 0.175668, acc.: 74.22%] [G loss: 0.657361]\n",
      "epoch:2 step:2297 [D loss: 0.169871, acc.: 75.78%] [G loss: 0.603307]\n",
      "epoch:2 step:2298 [D loss: 0.139484, acc.: 84.38%] [G loss: 0.621098]\n",
      "epoch:2 step:2299 [D loss: 0.163539, acc.: 77.34%] [G loss: 0.625289]\n",
      "epoch:2 step:2300 [D loss: 0.118766, acc.: 87.50%] [G loss: 0.731120]\n",
      "epoch:2 step:2301 [D loss: 0.141042, acc.: 78.91%] [G loss: 0.683241]\n",
      "epoch:2 step:2302 [D loss: 0.141892, acc.: 78.91%] [G loss: 0.701409]\n",
      "epoch:2 step:2303 [D loss: 0.150518, acc.: 82.03%] [G loss: 0.677860]\n",
      "epoch:2 step:2304 [D loss: 0.153859, acc.: 81.25%] [G loss: 0.668924]\n",
      "epoch:2 step:2305 [D loss: 0.154661, acc.: 78.12%] [G loss: 0.660443]\n",
      "epoch:2 step:2306 [D loss: 0.190262, acc.: 71.88%] [G loss: 0.593185]\n",
      "epoch:2 step:2307 [D loss: 0.172215, acc.: 77.34%] [G loss: 0.645123]\n",
      "epoch:2 step:2308 [D loss: 0.144244, acc.: 82.81%] [G loss: 0.665870]\n",
      "epoch:2 step:2309 [D loss: 0.184569, acc.: 75.78%] [G loss: 0.613041]\n",
      "epoch:2 step:2310 [D loss: 0.160374, acc.: 78.12%] [G loss: 0.678323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2311 [D loss: 0.195325, acc.: 74.22%] [G loss: 0.608606]\n",
      "epoch:2 step:2312 [D loss: 0.172676, acc.: 76.56%] [G loss: 0.621617]\n",
      "epoch:2 step:2313 [D loss: 0.171856, acc.: 75.78%] [G loss: 0.634777]\n",
      "epoch:2 step:2314 [D loss: 0.160184, acc.: 76.56%] [G loss: 0.651570]\n",
      "epoch:2 step:2315 [D loss: 0.173300, acc.: 72.66%] [G loss: 0.620811]\n",
      "epoch:2 step:2316 [D loss: 0.155175, acc.: 79.69%] [G loss: 0.631091]\n",
      "epoch:2 step:2317 [D loss: 0.152320, acc.: 79.69%] [G loss: 0.635590]\n",
      "epoch:2 step:2318 [D loss: 0.166035, acc.: 75.00%] [G loss: 0.664931]\n",
      "epoch:2 step:2319 [D loss: 0.150049, acc.: 79.69%] [G loss: 0.657197]\n",
      "epoch:2 step:2320 [D loss: 0.149510, acc.: 82.03%] [G loss: 0.693743]\n",
      "epoch:2 step:2321 [D loss: 0.155444, acc.: 77.34%] [G loss: 0.662351]\n",
      "epoch:2 step:2322 [D loss: 0.195359, acc.: 71.88%] [G loss: 0.648423]\n",
      "epoch:2 step:2323 [D loss: 0.187634, acc.: 71.88%] [G loss: 0.606905]\n",
      "epoch:2 step:2324 [D loss: 0.120061, acc.: 85.16%] [G loss: 0.701929]\n",
      "epoch:2 step:2325 [D loss: 0.156555, acc.: 78.91%] [G loss: 0.706039]\n",
      "epoch:2 step:2326 [D loss: 0.170744, acc.: 75.00%] [G loss: 0.657078]\n",
      "epoch:2 step:2327 [D loss: 0.179587, acc.: 76.56%] [G loss: 0.597198]\n",
      "epoch:2 step:2328 [D loss: 0.182966, acc.: 72.66%] [G loss: 0.648509]\n",
      "epoch:2 step:2329 [D loss: 0.190230, acc.: 72.66%] [G loss: 0.645333]\n",
      "epoch:2 step:2330 [D loss: 0.235834, acc.: 60.94%] [G loss: 0.557860]\n",
      "epoch:2 step:2331 [D loss: 0.133470, acc.: 83.59%] [G loss: 0.719691]\n",
      "epoch:2 step:2332 [D loss: 0.167056, acc.: 77.34%] [G loss: 0.623435]\n",
      "epoch:2 step:2333 [D loss: 0.169217, acc.: 72.66%] [G loss: 0.601509]\n",
      "epoch:2 step:2334 [D loss: 0.158695, acc.: 79.69%] [G loss: 0.654711]\n",
      "epoch:2 step:2335 [D loss: 0.154390, acc.: 78.91%] [G loss: 0.691691]\n",
      "epoch:2 step:2336 [D loss: 0.189094, acc.: 75.00%] [G loss: 0.625077]\n",
      "epoch:2 step:2337 [D loss: 0.198659, acc.: 71.09%] [G loss: 0.574532]\n",
      "epoch:2 step:2338 [D loss: 0.144830, acc.: 81.25%] [G loss: 0.655887]\n",
      "epoch:2 step:2339 [D loss: 0.203388, acc.: 69.53%] [G loss: 0.546013]\n",
      "epoch:2 step:2340 [D loss: 0.172505, acc.: 76.56%] [G loss: 0.646934]\n",
      "epoch:2 step:2341 [D loss: 0.194725, acc.: 73.44%] [G loss: 0.602960]\n",
      "epoch:2 step:2342 [D loss: 0.191120, acc.: 71.88%] [G loss: 0.604750]\n",
      "epoch:2 step:2343 [D loss: 0.175974, acc.: 75.78%] [G loss: 0.622554]\n",
      "epoch:2 step:2344 [D loss: 0.160207, acc.: 78.12%] [G loss: 0.660644]\n",
      "epoch:2 step:2345 [D loss: 0.174594, acc.: 75.00%] [G loss: 0.685807]\n",
      "epoch:2 step:2346 [D loss: 0.158481, acc.: 75.78%] [G loss: 0.754870]\n",
      "epoch:2 step:2347 [D loss: 0.198406, acc.: 71.88%] [G loss: 0.626012]\n",
      "epoch:2 step:2348 [D loss: 0.126275, acc.: 83.59%] [G loss: 0.752400]\n",
      "epoch:2 step:2349 [D loss: 0.123152, acc.: 85.94%] [G loss: 0.739796]\n",
      "epoch:2 step:2350 [D loss: 0.190345, acc.: 74.22%] [G loss: 0.585489]\n",
      "epoch:2 step:2351 [D loss: 0.215802, acc.: 64.84%] [G loss: 0.553888]\n",
      "epoch:2 step:2352 [D loss: 0.159833, acc.: 80.47%] [G loss: 0.604276]\n",
      "epoch:2 step:2353 [D loss: 0.186934, acc.: 74.22%] [G loss: 0.558150]\n",
      "epoch:2 step:2354 [D loss: 0.195041, acc.: 75.78%] [G loss: 0.584953]\n",
      "epoch:2 step:2355 [D loss: 0.170526, acc.: 75.00%] [G loss: 0.639410]\n",
      "epoch:2 step:2356 [D loss: 0.223199, acc.: 66.41%] [G loss: 0.528738]\n",
      "epoch:2 step:2357 [D loss: 0.159730, acc.: 80.47%] [G loss: 0.667645]\n",
      "epoch:2 step:2358 [D loss: 0.172721, acc.: 76.56%] [G loss: 0.675451]\n",
      "epoch:2 step:2359 [D loss: 0.148146, acc.: 85.16%] [G loss: 0.684554]\n",
      "epoch:2 step:2360 [D loss: 0.193399, acc.: 76.56%] [G loss: 0.616281]\n",
      "epoch:2 step:2361 [D loss: 0.158331, acc.: 74.22%] [G loss: 0.676249]\n",
      "epoch:2 step:2362 [D loss: 0.156760, acc.: 80.47%] [G loss: 0.698462]\n",
      "epoch:2 step:2363 [D loss: 0.185808, acc.: 75.00%] [G loss: 0.603497]\n",
      "epoch:2 step:2364 [D loss: 0.189986, acc.: 67.19%] [G loss: 0.634321]\n",
      "epoch:2 step:2365 [D loss: 0.165331, acc.: 77.34%] [G loss: 0.683615]\n",
      "epoch:2 step:2366 [D loss: 0.162619, acc.: 77.34%] [G loss: 0.672959]\n",
      "epoch:2 step:2367 [D loss: 0.177598, acc.: 71.09%] [G loss: 0.624315]\n",
      "epoch:2 step:2368 [D loss: 0.163569, acc.: 78.12%] [G loss: 0.606497]\n",
      "epoch:2 step:2369 [D loss: 0.157592, acc.: 76.56%] [G loss: 0.677201]\n",
      "epoch:2 step:2370 [D loss: 0.182611, acc.: 73.44%] [G loss: 0.618782]\n",
      "epoch:2 step:2371 [D loss: 0.115343, acc.: 86.72%] [G loss: 0.725610]\n",
      "epoch:2 step:2372 [D loss: 0.107947, acc.: 87.50%] [G loss: 0.792359]\n",
      "epoch:2 step:2373 [D loss: 0.133208, acc.: 82.03%] [G loss: 0.758140]\n",
      "epoch:2 step:2374 [D loss: 0.241787, acc.: 60.16%] [G loss: 0.561166]\n",
      "epoch:2 step:2375 [D loss: 0.181332, acc.: 72.66%] [G loss: 0.602762]\n",
      "epoch:2 step:2376 [D loss: 0.175035, acc.: 74.22%] [G loss: 0.581860]\n",
      "epoch:2 step:2377 [D loss: 0.139001, acc.: 83.59%] [G loss: 0.632850]\n",
      "epoch:2 step:2378 [D loss: 0.138553, acc.: 82.03%] [G loss: 0.681535]\n",
      "epoch:2 step:2379 [D loss: 0.161057, acc.: 77.34%] [G loss: 0.646829]\n",
      "epoch:2 step:2380 [D loss: 0.155970, acc.: 75.00%] [G loss: 0.693519]\n",
      "epoch:2 step:2381 [D loss: 0.174544, acc.: 77.34%] [G loss: 0.620998]\n",
      "epoch:2 step:2382 [D loss: 0.119085, acc.: 85.16%] [G loss: 0.707743]\n",
      "epoch:2 step:2383 [D loss: 0.147332, acc.: 83.59%] [G loss: 0.661442]\n",
      "epoch:2 step:2384 [D loss: 0.161184, acc.: 80.47%] [G loss: 0.592514]\n",
      "epoch:2 step:2385 [D loss: 0.187082, acc.: 71.88%] [G loss: 0.649761]\n",
      "epoch:2 step:2386 [D loss: 0.144089, acc.: 82.81%] [G loss: 0.693103]\n",
      "epoch:2 step:2387 [D loss: 0.170637, acc.: 77.34%] [G loss: 0.626458]\n",
      "epoch:2 step:2388 [D loss: 0.113781, acc.: 86.72%] [G loss: 0.756472]\n",
      "epoch:2 step:2389 [D loss: 0.113475, acc.: 86.72%] [G loss: 0.690889]\n",
      "epoch:2 step:2390 [D loss: 0.161997, acc.: 78.12%] [G loss: 0.661554]\n",
      "epoch:2 step:2391 [D loss: 0.186030, acc.: 71.88%] [G loss: 0.661383]\n",
      "epoch:2 step:2392 [D loss: 0.127142, acc.: 86.72%] [G loss: 0.689258]\n",
      "epoch:2 step:2393 [D loss: 0.157138, acc.: 78.91%] [G loss: 0.599736]\n",
      "epoch:2 step:2394 [D loss: 0.136623, acc.: 84.38%] [G loss: 0.663812]\n",
      "epoch:2 step:2395 [D loss: 0.159266, acc.: 77.34%] [G loss: 0.706953]\n",
      "epoch:2 step:2396 [D loss: 0.169493, acc.: 72.66%] [G loss: 0.663009]\n",
      "epoch:2 step:2397 [D loss: 0.126894, acc.: 82.03%] [G loss: 0.722253]\n",
      "epoch:2 step:2398 [D loss: 0.152883, acc.: 81.25%] [G loss: 0.660804]\n",
      "epoch:2 step:2399 [D loss: 0.160058, acc.: 74.22%] [G loss: 0.624194]\n",
      "epoch:2 step:2400 [D loss: 0.140150, acc.: 82.81%] [G loss: 0.672226]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.547549\n",
      "FID: 80.164902\n",
      "0 = 14.150717476749358\n",
      "1 = 0.10846550044030867\n",
      "2 = 0.992900013923645\n",
      "3 = 0.98580002784729\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.98580002784729\n",
      "7 = 11.18343968864678\n",
      "8 = 0.18126417959587446\n",
      "9 = 0.934149980545044\n",
      "10 = 0.9085999727249146\n",
      "11 = 0.9596999883651733\n",
      "12 = 0.9575297832489014\n",
      "13 = 0.9085999727249146\n",
      "14 = 4.547573089599609\n",
      "15 = 7.881330490112305\n",
      "16 = 0.38160115480422974\n",
      "17 = 4.547549247741699\n",
      "18 = 80.16490173339844\n",
      "epoch:2 step:2401 [D loss: 0.163481, acc.: 78.91%] [G loss: 0.706259]\n",
      "epoch:2 step:2402 [D loss: 0.222394, acc.: 64.84%] [G loss: 0.556683]\n",
      "epoch:2 step:2403 [D loss: 0.134837, acc.: 82.03%] [G loss: 0.676025]\n",
      "epoch:2 step:2404 [D loss: 0.165267, acc.: 76.56%] [G loss: 0.690044]\n",
      "epoch:2 step:2405 [D loss: 0.211621, acc.: 67.19%] [G loss: 0.567035]\n",
      "epoch:2 step:2406 [D loss: 0.167037, acc.: 74.22%] [G loss: 0.622515]\n",
      "epoch:2 step:2407 [D loss: 0.173129, acc.: 77.34%] [G loss: 0.633674]\n",
      "epoch:2 step:2408 [D loss: 0.137999, acc.: 83.59%] [G loss: 0.687760]\n",
      "epoch:2 step:2409 [D loss: 0.167502, acc.: 75.78%] [G loss: 0.599482]\n",
      "epoch:2 step:2410 [D loss: 0.149932, acc.: 77.34%] [G loss: 0.642789]\n",
      "epoch:2 step:2411 [D loss: 0.137024, acc.: 83.59%] [G loss: 0.687823]\n",
      "epoch:2 step:2412 [D loss: 0.182031, acc.: 75.00%] [G loss: 0.597461]\n",
      "epoch:2 step:2413 [D loss: 0.172830, acc.: 76.56%] [G loss: 0.645781]\n",
      "epoch:2 step:2414 [D loss: 0.172892, acc.: 78.12%] [G loss: 0.675726]\n",
      "epoch:2 step:2415 [D loss: 0.136596, acc.: 82.03%] [G loss: 0.681855]\n",
      "epoch:2 step:2416 [D loss: 0.216174, acc.: 67.97%] [G loss: 0.553178]\n",
      "epoch:2 step:2417 [D loss: 0.216236, acc.: 64.84%] [G loss: 0.576175]\n",
      "epoch:2 step:2418 [D loss: 0.177992, acc.: 71.09%] [G loss: 0.632447]\n",
      "epoch:2 step:2419 [D loss: 0.181934, acc.: 70.31%] [G loss: 0.673975]\n",
      "epoch:2 step:2420 [D loss: 0.145118, acc.: 82.81%] [G loss: 0.612803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2421 [D loss: 0.155363, acc.: 78.91%] [G loss: 0.643594]\n",
      "epoch:2 step:2422 [D loss: 0.158481, acc.: 78.12%] [G loss: 0.655542]\n",
      "epoch:2 step:2423 [D loss: 0.184784, acc.: 72.66%] [G loss: 0.639731]\n",
      "epoch:2 step:2424 [D loss: 0.160426, acc.: 78.12%] [G loss: 0.652833]\n",
      "epoch:2 step:2425 [D loss: 0.185318, acc.: 73.44%] [G loss: 0.622254]\n",
      "epoch:2 step:2426 [D loss: 0.145202, acc.: 80.47%] [G loss: 0.698934]\n",
      "epoch:2 step:2427 [D loss: 0.172320, acc.: 78.91%] [G loss: 0.600804]\n",
      "epoch:2 step:2428 [D loss: 0.160685, acc.: 80.47%] [G loss: 0.624591]\n",
      "epoch:2 step:2429 [D loss: 0.150096, acc.: 82.03%] [G loss: 0.606230]\n",
      "epoch:2 step:2430 [D loss: 0.135026, acc.: 82.81%] [G loss: 0.645344]\n",
      "epoch:2 step:2431 [D loss: 0.165161, acc.: 79.69%] [G loss: 0.602354]\n",
      "epoch:2 step:2432 [D loss: 0.138958, acc.: 78.91%] [G loss: 0.708196]\n",
      "epoch:2 step:2433 [D loss: 0.199498, acc.: 74.22%] [G loss: 0.603282]\n",
      "epoch:2 step:2434 [D loss: 0.158466, acc.: 72.66%] [G loss: 0.619878]\n",
      "epoch:2 step:2435 [D loss: 0.122511, acc.: 85.16%] [G loss: 0.651848]\n",
      "epoch:2 step:2436 [D loss: 0.185894, acc.: 71.88%] [G loss: 0.614887]\n",
      "epoch:2 step:2437 [D loss: 0.183710, acc.: 73.44%] [G loss: 0.604094]\n",
      "epoch:2 step:2438 [D loss: 0.139526, acc.: 82.03%] [G loss: 0.646208]\n",
      "epoch:2 step:2439 [D loss: 0.155162, acc.: 80.47%] [G loss: 0.638877]\n",
      "epoch:2 step:2440 [D loss: 0.176937, acc.: 75.78%] [G loss: 0.625479]\n",
      "epoch:2 step:2441 [D loss: 0.151392, acc.: 81.25%] [G loss: 0.702181]\n",
      "epoch:2 step:2442 [D loss: 0.160577, acc.: 79.69%] [G loss: 0.660485]\n",
      "epoch:2 step:2443 [D loss: 0.199455, acc.: 67.97%] [G loss: 0.608565]\n",
      "epoch:2 step:2444 [D loss: 0.153912, acc.: 75.78%] [G loss: 0.631453]\n",
      "epoch:2 step:2445 [D loss: 0.138171, acc.: 82.81%] [G loss: 0.671283]\n",
      "epoch:2 step:2446 [D loss: 0.134300, acc.: 82.81%] [G loss: 0.637697]\n",
      "epoch:2 step:2447 [D loss: 0.173497, acc.: 75.78%] [G loss: 0.573538]\n",
      "epoch:2 step:2448 [D loss: 0.168119, acc.: 79.69%] [G loss: 0.656646]\n",
      "epoch:2 step:2449 [D loss: 0.139466, acc.: 84.38%] [G loss: 0.721146]\n",
      "epoch:2 step:2450 [D loss: 0.186705, acc.: 74.22%] [G loss: 0.621349]\n",
      "epoch:2 step:2451 [D loss: 0.161396, acc.: 81.25%] [G loss: 0.630502]\n",
      "epoch:2 step:2452 [D loss: 0.162356, acc.: 73.44%] [G loss: 0.647259]\n",
      "epoch:2 step:2453 [D loss: 0.174452, acc.: 75.00%] [G loss: 0.584142]\n",
      "epoch:2 step:2454 [D loss: 0.162008, acc.: 75.00%] [G loss: 0.622406]\n",
      "epoch:2 step:2455 [D loss: 0.178474, acc.: 69.53%] [G loss: 0.636092]\n",
      "epoch:2 step:2456 [D loss: 0.128161, acc.: 82.81%] [G loss: 0.694349]\n",
      "epoch:2 step:2457 [D loss: 0.190164, acc.: 74.22%] [G loss: 0.632415]\n",
      "epoch:2 step:2458 [D loss: 0.168516, acc.: 77.34%] [G loss: 0.611899]\n",
      "epoch:2 step:2459 [D loss: 0.170253, acc.: 76.56%] [G loss: 0.643374]\n",
      "epoch:2 step:2460 [D loss: 0.163600, acc.: 78.91%] [G loss: 0.611858]\n",
      "epoch:2 step:2461 [D loss: 0.187423, acc.: 71.88%] [G loss: 0.573535]\n",
      "epoch:2 step:2462 [D loss: 0.190754, acc.: 67.19%] [G loss: 0.604894]\n",
      "epoch:2 step:2463 [D loss: 0.162301, acc.: 78.12%] [G loss: 0.709563]\n",
      "epoch:2 step:2464 [D loss: 0.204010, acc.: 71.88%] [G loss: 0.616426]\n",
      "epoch:2 step:2465 [D loss: 0.156870, acc.: 80.47%] [G loss: 0.644899]\n",
      "epoch:2 step:2466 [D loss: 0.134419, acc.: 80.47%] [G loss: 0.639410]\n",
      "epoch:2 step:2467 [D loss: 0.183442, acc.: 70.31%] [G loss: 0.626684]\n",
      "epoch:2 step:2468 [D loss: 0.183232, acc.: 71.09%] [G loss: 0.579944]\n",
      "epoch:2 step:2469 [D loss: 0.162964, acc.: 76.56%] [G loss: 0.617227]\n",
      "epoch:2 step:2470 [D loss: 0.172626, acc.: 72.66%] [G loss: 0.661507]\n",
      "epoch:2 step:2471 [D loss: 0.154469, acc.: 75.00%] [G loss: 0.665370]\n",
      "epoch:2 step:2472 [D loss: 0.147875, acc.: 78.12%] [G loss: 0.648347]\n",
      "epoch:2 step:2473 [D loss: 0.173091, acc.: 71.88%] [G loss: 0.684901]\n",
      "epoch:2 step:2474 [D loss: 0.192162, acc.: 67.97%] [G loss: 0.604833]\n",
      "epoch:2 step:2475 [D loss: 0.171900, acc.: 75.00%] [G loss: 0.618043]\n",
      "epoch:2 step:2476 [D loss: 0.119716, acc.: 85.94%] [G loss: 0.658959]\n",
      "epoch:2 step:2477 [D loss: 0.152400, acc.: 79.69%] [G loss: 0.646351]\n",
      "epoch:2 step:2478 [D loss: 0.140522, acc.: 78.91%] [G loss: 0.659820]\n",
      "epoch:2 step:2479 [D loss: 0.134145, acc.: 81.25%] [G loss: 0.698228]\n",
      "epoch:2 step:2480 [D loss: 0.197503, acc.: 67.97%] [G loss: 0.630689]\n",
      "epoch:2 step:2481 [D loss: 0.169335, acc.: 78.12%] [G loss: 0.626208]\n",
      "epoch:2 step:2482 [D loss: 0.180399, acc.: 75.00%] [G loss: 0.667104]\n",
      "epoch:2 step:2483 [D loss: 0.145551, acc.: 83.59%] [G loss: 0.664065]\n",
      "epoch:2 step:2484 [D loss: 0.166550, acc.: 77.34%] [G loss: 0.629634]\n",
      "epoch:2 step:2485 [D loss: 0.130018, acc.: 85.16%] [G loss: 0.620257]\n",
      "epoch:2 step:2486 [D loss: 0.121798, acc.: 85.16%] [G loss: 0.730130]\n",
      "epoch:2 step:2487 [D loss: 0.121337, acc.: 84.38%] [G loss: 0.734032]\n",
      "epoch:2 step:2488 [D loss: 0.158662, acc.: 81.25%] [G loss: 0.627859]\n",
      "epoch:2 step:2489 [D loss: 0.205474, acc.: 71.09%] [G loss: 0.557159]\n",
      "epoch:2 step:2490 [D loss: 0.129443, acc.: 82.81%] [G loss: 0.705464]\n",
      "epoch:2 step:2491 [D loss: 0.165785, acc.: 78.91%] [G loss: 0.623423]\n",
      "epoch:2 step:2492 [D loss: 0.187129, acc.: 71.88%] [G loss: 0.562760]\n",
      "epoch:2 step:2493 [D loss: 0.133501, acc.: 83.59%] [G loss: 0.675581]\n",
      "epoch:2 step:2494 [D loss: 0.137689, acc.: 81.25%] [G loss: 0.661657]\n",
      "epoch:2 step:2495 [D loss: 0.180893, acc.: 75.00%] [G loss: 0.558642]\n",
      "epoch:2 step:2496 [D loss: 0.220373, acc.: 66.41%] [G loss: 0.608368]\n",
      "epoch:2 step:2497 [D loss: 0.122151, acc.: 86.72%] [G loss: 0.706261]\n",
      "epoch:2 step:2498 [D loss: 0.186681, acc.: 71.09%] [G loss: 0.653356]\n",
      "epoch:2 step:2499 [D loss: 0.186637, acc.: 75.00%] [G loss: 0.641851]\n",
      "epoch:2 step:2500 [D loss: 0.135420, acc.: 85.16%] [G loss: 0.651430]\n",
      "epoch:2 step:2501 [D loss: 0.158810, acc.: 79.69%] [G loss: 0.646182]\n",
      "epoch:2 step:2502 [D loss: 0.165858, acc.: 74.22%] [G loss: 0.663155]\n",
      "epoch:2 step:2503 [D loss: 0.149315, acc.: 78.91%] [G loss: 0.650497]\n",
      "epoch:2 step:2504 [D loss: 0.161393, acc.: 77.34%] [G loss: 0.610680]\n",
      "epoch:2 step:2505 [D loss: 0.144090, acc.: 82.81%] [G loss: 0.639769]\n",
      "epoch:2 step:2506 [D loss: 0.151019, acc.: 78.91%] [G loss: 0.657490]\n",
      "epoch:2 step:2507 [D loss: 0.123487, acc.: 86.72%] [G loss: 0.697442]\n",
      "epoch:2 step:2508 [D loss: 0.114826, acc.: 87.50%] [G loss: 0.664631]\n",
      "epoch:2 step:2509 [D loss: 0.166214, acc.: 76.56%] [G loss: 0.641108]\n",
      "epoch:2 step:2510 [D loss: 0.165596, acc.: 75.00%] [G loss: 0.598136]\n",
      "epoch:2 step:2511 [D loss: 0.155668, acc.: 79.69%] [G loss: 0.624682]\n",
      "epoch:2 step:2512 [D loss: 0.136337, acc.: 82.03%] [G loss: 0.696438]\n",
      "epoch:2 step:2513 [D loss: 0.155380, acc.: 79.69%] [G loss: 0.682068]\n",
      "epoch:2 step:2514 [D loss: 0.146973, acc.: 78.12%] [G loss: 0.652293]\n",
      "epoch:2 step:2515 [D loss: 0.112022, acc.: 86.72%] [G loss: 0.743946]\n",
      "epoch:2 step:2516 [D loss: 0.131059, acc.: 83.59%] [G loss: 0.716843]\n",
      "epoch:2 step:2517 [D loss: 0.206356, acc.: 71.09%] [G loss: 0.570797]\n",
      "epoch:2 step:2518 [D loss: 0.177146, acc.: 74.22%] [G loss: 0.597216]\n",
      "epoch:2 step:2519 [D loss: 0.162571, acc.: 74.22%] [G loss: 0.644700]\n",
      "epoch:2 step:2520 [D loss: 0.190198, acc.: 71.09%] [G loss: 0.600890]\n",
      "epoch:2 step:2521 [D loss: 0.152588, acc.: 78.12%] [G loss: 0.680791]\n",
      "epoch:2 step:2522 [D loss: 0.131585, acc.: 80.47%] [G loss: 0.743562]\n",
      "epoch:2 step:2523 [D loss: 0.132862, acc.: 84.38%] [G loss: 0.737691]\n",
      "epoch:2 step:2524 [D loss: 0.176532, acc.: 74.22%] [G loss: 0.634452]\n",
      "epoch:2 step:2525 [D loss: 0.134328, acc.: 84.38%] [G loss: 0.632293]\n",
      "epoch:2 step:2526 [D loss: 0.216489, acc.: 64.06%] [G loss: 0.550279]\n",
      "epoch:2 step:2527 [D loss: 0.174057, acc.: 69.53%] [G loss: 0.627502]\n",
      "epoch:2 step:2528 [D loss: 0.171739, acc.: 75.00%] [G loss: 0.662077]\n",
      "epoch:2 step:2529 [D loss: 0.182555, acc.: 72.66%] [G loss: 0.619904]\n",
      "epoch:2 step:2530 [D loss: 0.141223, acc.: 79.69%] [G loss: 0.610439]\n",
      "epoch:2 step:2531 [D loss: 0.132505, acc.: 81.25%] [G loss: 0.634832]\n",
      "epoch:2 step:2532 [D loss: 0.178810, acc.: 74.22%] [G loss: 0.628819]\n",
      "epoch:2 step:2533 [D loss: 0.190608, acc.: 71.88%] [G loss: 0.661874]\n",
      "epoch:2 step:2534 [D loss: 0.126477, acc.: 83.59%] [G loss: 0.696443]\n",
      "epoch:2 step:2535 [D loss: 0.172904, acc.: 74.22%] [G loss: 0.645206]\n",
      "epoch:2 step:2536 [D loss: 0.155982, acc.: 76.56%] [G loss: 0.657541]\n",
      "epoch:2 step:2537 [D loss: 0.151366, acc.: 80.47%] [G loss: 0.662046]\n",
      "epoch:2 step:2538 [D loss: 0.187310, acc.: 71.88%] [G loss: 0.617995]\n",
      "epoch:2 step:2539 [D loss: 0.149353, acc.: 78.91%] [G loss: 0.687151]\n",
      "epoch:2 step:2540 [D loss: 0.126498, acc.: 86.72%] [G loss: 0.665056]\n",
      "epoch:2 step:2541 [D loss: 0.200119, acc.: 64.84%] [G loss: 0.590104]\n",
      "epoch:2 step:2542 [D loss: 0.180663, acc.: 74.22%] [G loss: 0.657758]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2543 [D loss: 0.171235, acc.: 78.12%] [G loss: 0.643092]\n",
      "epoch:2 step:2544 [D loss: 0.149505, acc.: 82.81%] [G loss: 0.653679]\n",
      "epoch:2 step:2545 [D loss: 0.193643, acc.: 74.22%] [G loss: 0.608229]\n",
      "epoch:2 step:2546 [D loss: 0.155837, acc.: 76.56%] [G loss: 0.640546]\n",
      "epoch:2 step:2547 [D loss: 0.154313, acc.: 77.34%] [G loss: 0.724078]\n",
      "epoch:2 step:2548 [D loss: 0.181049, acc.: 76.56%] [G loss: 0.637117]\n",
      "epoch:2 step:2549 [D loss: 0.207474, acc.: 69.53%] [G loss: 0.585685]\n",
      "epoch:2 step:2550 [D loss: 0.182165, acc.: 70.31%] [G loss: 0.635193]\n",
      "epoch:2 step:2551 [D loss: 0.150050, acc.: 79.69%] [G loss: 0.707183]\n",
      "epoch:2 step:2552 [D loss: 0.186095, acc.: 71.88%] [G loss: 0.646392]\n",
      "epoch:2 step:2553 [D loss: 0.176106, acc.: 74.22%] [G loss: 0.620562]\n",
      "epoch:2 step:2554 [D loss: 0.134607, acc.: 84.38%] [G loss: 0.655773]\n",
      "epoch:2 step:2555 [D loss: 0.158890, acc.: 78.91%] [G loss: 0.653710]\n",
      "epoch:2 step:2556 [D loss: 0.164363, acc.: 78.91%] [G loss: 0.623711]\n",
      "epoch:2 step:2557 [D loss: 0.164991, acc.: 76.56%] [G loss: 0.615899]\n",
      "epoch:2 step:2558 [D loss: 0.162497, acc.: 77.34%] [G loss: 0.651382]\n",
      "epoch:2 step:2559 [D loss: 0.174323, acc.: 78.12%] [G loss: 0.636480]\n",
      "epoch:2 step:2560 [D loss: 0.182180, acc.: 73.44%] [G loss: 0.598967]\n",
      "epoch:2 step:2561 [D loss: 0.174009, acc.: 76.56%] [G loss: 0.626113]\n",
      "epoch:2 step:2562 [D loss: 0.197991, acc.: 68.75%] [G loss: 0.585338]\n",
      "epoch:2 step:2563 [D loss: 0.209409, acc.: 71.09%] [G loss: 0.581738]\n",
      "epoch:2 step:2564 [D loss: 0.158073, acc.: 79.69%] [G loss: 0.695581]\n",
      "epoch:2 step:2565 [D loss: 0.183124, acc.: 73.44%] [G loss: 0.666750]\n",
      "epoch:2 step:2566 [D loss: 0.156477, acc.: 80.47%] [G loss: 0.624943]\n",
      "epoch:2 step:2567 [D loss: 0.173851, acc.: 67.97%] [G loss: 0.622387]\n",
      "epoch:2 step:2568 [D loss: 0.160132, acc.: 75.78%] [G loss: 0.670792]\n",
      "epoch:2 step:2569 [D loss: 0.154109, acc.: 77.34%] [G loss: 0.669028]\n",
      "epoch:2 step:2570 [D loss: 0.228195, acc.: 63.28%] [G loss: 0.570923]\n",
      "epoch:2 step:2571 [D loss: 0.137744, acc.: 81.25%] [G loss: 0.681362]\n",
      "epoch:2 step:2572 [D loss: 0.177090, acc.: 77.34%] [G loss: 0.668939]\n",
      "epoch:2 step:2573 [D loss: 0.155462, acc.: 77.34%] [G loss: 0.670491]\n",
      "epoch:2 step:2574 [D loss: 0.205156, acc.: 69.53%] [G loss: 0.612418]\n",
      "epoch:2 step:2575 [D loss: 0.189491, acc.: 72.66%] [G loss: 0.619566]\n",
      "epoch:2 step:2576 [D loss: 0.171191, acc.: 73.44%] [G loss: 0.647900]\n",
      "epoch:2 step:2577 [D loss: 0.143330, acc.: 85.16%] [G loss: 0.606680]\n",
      "epoch:2 step:2578 [D loss: 0.175739, acc.: 78.12%] [G loss: 0.597175]\n",
      "epoch:2 step:2579 [D loss: 0.162499, acc.: 77.34%] [G loss: 0.594515]\n",
      "epoch:2 step:2580 [D loss: 0.148513, acc.: 79.69%] [G loss: 0.652878]\n",
      "epoch:2 step:2581 [D loss: 0.143678, acc.: 80.47%] [G loss: 0.723195]\n",
      "epoch:2 step:2582 [D loss: 0.136263, acc.: 79.69%] [G loss: 0.689644]\n",
      "epoch:2 step:2583 [D loss: 0.127300, acc.: 82.03%] [G loss: 0.696268]\n",
      "epoch:2 step:2584 [D loss: 0.238839, acc.: 65.62%] [G loss: 0.579607]\n",
      "epoch:2 step:2585 [D loss: 0.171172, acc.: 75.00%] [G loss: 0.664777]\n",
      "epoch:2 step:2586 [D loss: 0.149791, acc.: 80.47%] [G loss: 0.656727]\n",
      "epoch:2 step:2587 [D loss: 0.192730, acc.: 67.97%] [G loss: 0.647673]\n",
      "epoch:2 step:2588 [D loss: 0.135299, acc.: 88.28%] [G loss: 0.668184]\n",
      "epoch:2 step:2589 [D loss: 0.200658, acc.: 71.09%] [G loss: 0.595742]\n",
      "epoch:2 step:2590 [D loss: 0.216357, acc.: 68.75%] [G loss: 0.589203]\n",
      "epoch:2 step:2591 [D loss: 0.181565, acc.: 73.44%] [G loss: 0.628070]\n",
      "epoch:2 step:2592 [D loss: 0.196715, acc.: 69.53%] [G loss: 0.602618]\n",
      "epoch:2 step:2593 [D loss: 0.162211, acc.: 74.22%] [G loss: 0.620658]\n",
      "epoch:2 step:2594 [D loss: 0.212712, acc.: 68.75%] [G loss: 0.571260]\n",
      "epoch:2 step:2595 [D loss: 0.196393, acc.: 68.75%] [G loss: 0.617864]\n",
      "epoch:2 step:2596 [D loss: 0.165247, acc.: 77.34%] [G loss: 0.707162]\n",
      "epoch:2 step:2597 [D loss: 0.163205, acc.: 78.12%] [G loss: 0.635373]\n",
      "epoch:2 step:2598 [D loss: 0.177987, acc.: 70.31%] [G loss: 0.610492]\n",
      "epoch:2 step:2599 [D loss: 0.179347, acc.: 73.44%] [G loss: 0.646842]\n",
      "epoch:2 step:2600 [D loss: 0.159143, acc.: 78.12%] [G loss: 0.648540]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.063443\n",
      "FID: 63.753914\n",
      "0 = 13.54250686936376\n",
      "1 = 0.08918921842035663\n",
      "2 = 0.9881500005722046\n",
      "3 = 0.9763000011444092\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9763000011444092\n",
      "7 = 10.255841186404222\n",
      "8 = 0.1617210056313482\n",
      "9 = 0.9049999713897705\n",
      "10 = 0.8779000043869019\n",
      "11 = 0.9320999979972839\n",
      "12 = 0.9282089471817017\n",
      "13 = 0.8779000043869019\n",
      "14 = 5.063479900360107\n",
      "15 = 7.949697971343994\n",
      "16 = 0.3438687324523926\n",
      "17 = 5.063442707061768\n",
      "18 = 63.75391387939453\n",
      "epoch:2 step:2601 [D loss: 0.203567, acc.: 70.31%] [G loss: 0.621063]\n",
      "epoch:2 step:2602 [D loss: 0.154976, acc.: 78.12%] [G loss: 0.644017]\n",
      "epoch:2 step:2603 [D loss: 0.169026, acc.: 75.00%] [G loss: 0.628408]\n",
      "epoch:2 step:2604 [D loss: 0.174604, acc.: 74.22%] [G loss: 0.620736]\n",
      "epoch:2 step:2605 [D loss: 0.157747, acc.: 78.12%] [G loss: 0.651657]\n",
      "epoch:2 step:2606 [D loss: 0.139047, acc.: 82.03%] [G loss: 0.691522]\n",
      "epoch:2 step:2607 [D loss: 0.136008, acc.: 83.59%] [G loss: 0.702546]\n",
      "epoch:2 step:2608 [D loss: 0.187347, acc.: 70.31%] [G loss: 0.618244]\n",
      "epoch:2 step:2609 [D loss: 0.198120, acc.: 70.31%] [G loss: 0.588923]\n",
      "epoch:2 step:2610 [D loss: 0.151063, acc.: 80.47%] [G loss: 0.683082]\n",
      "epoch:2 step:2611 [D loss: 0.154093, acc.: 78.91%] [G loss: 0.647289]\n",
      "epoch:2 step:2612 [D loss: 0.204492, acc.: 65.62%] [G loss: 0.572538]\n",
      "epoch:2 step:2613 [D loss: 0.205990, acc.: 66.41%] [G loss: 0.585215]\n",
      "epoch:2 step:2614 [D loss: 0.225752, acc.: 60.94%] [G loss: 0.546396]\n",
      "epoch:2 step:2615 [D loss: 0.201898, acc.: 68.75%] [G loss: 0.593576]\n",
      "epoch:2 step:2616 [D loss: 0.212232, acc.: 65.62%] [G loss: 0.610112]\n",
      "epoch:2 step:2617 [D loss: 0.164696, acc.: 77.34%] [G loss: 0.676837]\n",
      "epoch:2 step:2618 [D loss: 0.179275, acc.: 73.44%] [G loss: 0.651070]\n",
      "epoch:2 step:2619 [D loss: 0.155099, acc.: 82.81%] [G loss: 0.654831]\n",
      "epoch:2 step:2620 [D loss: 0.130478, acc.: 84.38%] [G loss: 0.712542]\n",
      "epoch:2 step:2621 [D loss: 0.124849, acc.: 86.72%] [G loss: 0.695647]\n",
      "epoch:2 step:2622 [D loss: 0.200419, acc.: 71.09%] [G loss: 0.586649]\n",
      "epoch:2 step:2623 [D loss: 0.175182, acc.: 75.78%] [G loss: 0.583065]\n",
      "epoch:2 step:2624 [D loss: 0.173213, acc.: 77.34%] [G loss: 0.585970]\n",
      "epoch:2 step:2625 [D loss: 0.175140, acc.: 74.22%] [G loss: 0.607868]\n",
      "epoch:2 step:2626 [D loss: 0.146684, acc.: 76.56%] [G loss: 0.660886]\n",
      "epoch:2 step:2627 [D loss: 0.153193, acc.: 78.91%] [G loss: 0.591624]\n",
      "epoch:2 step:2628 [D loss: 0.147746, acc.: 78.12%] [G loss: 0.660560]\n",
      "epoch:2 step:2629 [D loss: 0.146349, acc.: 82.03%] [G loss: 0.661260]\n",
      "epoch:2 step:2630 [D loss: 0.164387, acc.: 80.47%] [G loss: 0.572552]\n",
      "epoch:2 step:2631 [D loss: 0.178171, acc.: 75.00%] [G loss: 0.718637]\n",
      "epoch:2 step:2632 [D loss: 0.212652, acc.: 64.06%] [G loss: 0.631731]\n",
      "epoch:2 step:2633 [D loss: 0.150028, acc.: 82.81%] [G loss: 0.666069]\n",
      "epoch:2 step:2634 [D loss: 0.147663, acc.: 80.47%] [G loss: 0.581523]\n",
      "epoch:2 step:2635 [D loss: 0.158468, acc.: 80.47%] [G loss: 0.642107]\n",
      "epoch:2 step:2636 [D loss: 0.171988, acc.: 72.66%] [G loss: 0.638687]\n",
      "epoch:2 step:2637 [D loss: 0.151567, acc.: 79.69%] [G loss: 0.637760]\n",
      "epoch:2 step:2638 [D loss: 0.158420, acc.: 80.47%] [G loss: 0.641787]\n",
      "epoch:2 step:2639 [D loss: 0.219451, acc.: 65.62%] [G loss: 0.574514]\n",
      "epoch:2 step:2640 [D loss: 0.235383, acc.: 59.38%] [G loss: 0.585878]\n",
      "epoch:2 step:2641 [D loss: 0.135222, acc.: 85.16%] [G loss: 0.684658]\n",
      "epoch:2 step:2642 [D loss: 0.187836, acc.: 75.00%] [G loss: 0.632370]\n",
      "epoch:2 step:2643 [D loss: 0.139540, acc.: 78.12%] [G loss: 0.676429]\n",
      "epoch:2 step:2644 [D loss: 0.153241, acc.: 77.34%] [G loss: 0.685610]\n",
      "epoch:2 step:2645 [D loss: 0.179464, acc.: 77.34%] [G loss: 0.641673]\n",
      "epoch:2 step:2646 [D loss: 0.174691, acc.: 73.44%] [G loss: 0.616048]\n",
      "epoch:2 step:2647 [D loss: 0.139797, acc.: 85.94%] [G loss: 0.644083]\n",
      "epoch:2 step:2648 [D loss: 0.179109, acc.: 73.44%] [G loss: 0.609290]\n",
      "epoch:2 step:2649 [D loss: 0.132474, acc.: 82.81%] [G loss: 0.715999]\n",
      "epoch:2 step:2650 [D loss: 0.165068, acc.: 79.69%] [G loss: 0.610021]\n",
      "epoch:2 step:2651 [D loss: 0.159954, acc.: 82.81%] [G loss: 0.625928]\n",
      "epoch:2 step:2652 [D loss: 0.156323, acc.: 78.91%] [G loss: 0.594305]\n",
      "epoch:2 step:2653 [D loss: 0.156757, acc.: 78.12%] [G loss: 0.633911]\n",
      "epoch:2 step:2654 [D loss: 0.152606, acc.: 82.03%] [G loss: 0.651941]\n",
      "epoch:2 step:2655 [D loss: 0.125553, acc.: 85.94%] [G loss: 0.730868]\n",
      "epoch:2 step:2656 [D loss: 0.125728, acc.: 83.59%] [G loss: 0.789707]\n",
      "epoch:2 step:2657 [D loss: 0.156744, acc.: 81.25%] [G loss: 0.697793]\n",
      "epoch:2 step:2658 [D loss: 0.163750, acc.: 78.12%] [G loss: 0.583142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2659 [D loss: 0.132466, acc.: 85.94%] [G loss: 0.666797]\n",
      "epoch:2 step:2660 [D loss: 0.144828, acc.: 80.47%] [G loss: 0.697053]\n",
      "epoch:2 step:2661 [D loss: 0.191503, acc.: 72.66%] [G loss: 0.644684]\n",
      "epoch:2 step:2662 [D loss: 0.189971, acc.: 69.53%] [G loss: 0.556040]\n",
      "epoch:2 step:2663 [D loss: 0.168032, acc.: 75.78%] [G loss: 0.589293]\n",
      "epoch:2 step:2664 [D loss: 0.131218, acc.: 84.38%] [G loss: 0.656551]\n",
      "epoch:2 step:2665 [D loss: 0.169460, acc.: 78.91%] [G loss: 0.628754]\n",
      "epoch:2 step:2666 [D loss: 0.116906, acc.: 83.59%] [G loss: 0.699251]\n",
      "epoch:2 step:2667 [D loss: 0.216047, acc.: 66.41%] [G loss: 0.579090]\n",
      "epoch:2 step:2668 [D loss: 0.194606, acc.: 72.66%] [G loss: 0.546747]\n",
      "epoch:2 step:2669 [D loss: 0.149589, acc.: 78.12%] [G loss: 0.700136]\n",
      "epoch:2 step:2670 [D loss: 0.143252, acc.: 80.47%] [G loss: 0.638612]\n",
      "epoch:2 step:2671 [D loss: 0.185825, acc.: 74.22%] [G loss: 0.603813]\n",
      "epoch:2 step:2672 [D loss: 0.171147, acc.: 77.34%] [G loss: 0.684120]\n",
      "epoch:2 step:2673 [D loss: 0.177213, acc.: 73.44%] [G loss: 0.647668]\n",
      "epoch:2 step:2674 [D loss: 0.155008, acc.: 75.78%] [G loss: 0.640693]\n",
      "epoch:2 step:2675 [D loss: 0.119042, acc.: 85.94%] [G loss: 0.668553]\n",
      "epoch:2 step:2676 [D loss: 0.173656, acc.: 75.78%] [G loss: 0.670697]\n",
      "epoch:2 step:2677 [D loss: 0.143822, acc.: 82.03%] [G loss: 0.671334]\n",
      "epoch:2 step:2678 [D loss: 0.119901, acc.: 85.94%] [G loss: 0.694245]\n",
      "epoch:2 step:2679 [D loss: 0.127576, acc.: 85.94%] [G loss: 0.699660]\n",
      "epoch:2 step:2680 [D loss: 0.138508, acc.: 80.47%] [G loss: 0.681536]\n",
      "epoch:2 step:2681 [D loss: 0.146851, acc.: 76.56%] [G loss: 0.700174]\n",
      "epoch:2 step:2682 [D loss: 0.214668, acc.: 70.31%] [G loss: 0.555011]\n",
      "epoch:2 step:2683 [D loss: 0.166994, acc.: 78.12%] [G loss: 0.606247]\n",
      "epoch:2 step:2684 [D loss: 0.180318, acc.: 71.88%] [G loss: 0.594941]\n",
      "epoch:2 step:2685 [D loss: 0.162487, acc.: 77.34%] [G loss: 0.631145]\n",
      "epoch:2 step:2686 [D loss: 0.198664, acc.: 71.09%] [G loss: 0.569543]\n",
      "epoch:2 step:2687 [D loss: 0.155651, acc.: 81.25%] [G loss: 0.617451]\n",
      "epoch:2 step:2688 [D loss: 0.151798, acc.: 82.03%] [G loss: 0.737556]\n",
      "epoch:2 step:2689 [D loss: 0.142604, acc.: 78.91%] [G loss: 0.685950]\n",
      "epoch:2 step:2690 [D loss: 0.202562, acc.: 71.09%] [G loss: 0.613686]\n",
      "epoch:2 step:2691 [D loss: 0.180065, acc.: 69.53%] [G loss: 0.675343]\n",
      "epoch:2 step:2692 [D loss: 0.186100, acc.: 70.31%] [G loss: 0.583791]\n",
      "epoch:2 step:2693 [D loss: 0.144714, acc.: 84.38%] [G loss: 0.637188]\n",
      "epoch:2 step:2694 [D loss: 0.199060, acc.: 72.66%] [G loss: 0.606867]\n",
      "epoch:2 step:2695 [D loss: 0.147265, acc.: 78.91%] [G loss: 0.679532]\n",
      "epoch:2 step:2696 [D loss: 0.140637, acc.: 82.03%] [G loss: 0.689954]\n",
      "epoch:2 step:2697 [D loss: 0.143181, acc.: 77.34%] [G loss: 0.696608]\n",
      "epoch:2 step:2698 [D loss: 0.224257, acc.: 64.84%] [G loss: 0.536766]\n",
      "epoch:2 step:2699 [D loss: 0.149531, acc.: 82.03%] [G loss: 0.669259]\n",
      "epoch:2 step:2700 [D loss: 0.185616, acc.: 71.88%] [G loss: 0.593656]\n",
      "epoch:2 step:2701 [D loss: 0.174964, acc.: 72.66%] [G loss: 0.581358]\n",
      "epoch:2 step:2702 [D loss: 0.174890, acc.: 75.78%] [G loss: 0.602449]\n",
      "epoch:2 step:2703 [D loss: 0.147171, acc.: 82.03%] [G loss: 0.638937]\n",
      "epoch:2 step:2704 [D loss: 0.165783, acc.: 78.91%] [G loss: 0.670201]\n",
      "epoch:2 step:2705 [D loss: 0.146337, acc.: 82.03%] [G loss: 0.644929]\n",
      "epoch:2 step:2706 [D loss: 0.163741, acc.: 77.34%] [G loss: 0.599769]\n",
      "epoch:2 step:2707 [D loss: 0.167153, acc.: 76.56%] [G loss: 0.672554]\n",
      "epoch:2 step:2708 [D loss: 0.130422, acc.: 83.59%] [G loss: 0.732725]\n",
      "epoch:2 step:2709 [D loss: 0.146267, acc.: 78.91%] [G loss: 0.669673]\n",
      "epoch:2 step:2710 [D loss: 0.171610, acc.: 75.00%] [G loss: 0.679137]\n",
      "epoch:2 step:2711 [D loss: 0.160562, acc.: 78.91%] [G loss: 0.651806]\n",
      "epoch:2 step:2712 [D loss: 0.159957, acc.: 75.78%] [G loss: 0.660203]\n",
      "epoch:2 step:2713 [D loss: 0.167256, acc.: 76.56%] [G loss: 0.617524]\n",
      "epoch:2 step:2714 [D loss: 0.165439, acc.: 78.12%] [G loss: 0.612329]\n",
      "epoch:2 step:2715 [D loss: 0.170348, acc.: 78.12%] [G loss: 0.639289]\n",
      "epoch:2 step:2716 [D loss: 0.150931, acc.: 78.91%] [G loss: 0.630441]\n",
      "epoch:2 step:2717 [D loss: 0.164387, acc.: 77.34%] [G loss: 0.683775]\n",
      "epoch:2 step:2718 [D loss: 0.177523, acc.: 71.88%] [G loss: 0.643444]\n",
      "epoch:2 step:2719 [D loss: 0.183168, acc.: 73.44%] [G loss: 0.605790]\n",
      "epoch:2 step:2720 [D loss: 0.119015, acc.: 89.84%] [G loss: 0.693353]\n",
      "epoch:2 step:2721 [D loss: 0.140465, acc.: 81.25%] [G loss: 0.660720]\n",
      "epoch:2 step:2722 [D loss: 0.141251, acc.: 82.81%] [G loss: 0.616710]\n",
      "epoch:2 step:2723 [D loss: 0.153902, acc.: 80.47%] [G loss: 0.661297]\n",
      "epoch:2 step:2724 [D loss: 0.155561, acc.: 78.12%] [G loss: 0.680979]\n",
      "epoch:2 step:2725 [D loss: 0.178771, acc.: 70.31%] [G loss: 0.641896]\n",
      "epoch:2 step:2726 [D loss: 0.133395, acc.: 86.72%] [G loss: 0.705656]\n",
      "epoch:2 step:2727 [D loss: 0.136004, acc.: 80.47%] [G loss: 0.715782]\n",
      "epoch:2 step:2728 [D loss: 0.127080, acc.: 81.25%] [G loss: 0.723090]\n",
      "epoch:2 step:2729 [D loss: 0.139591, acc.: 85.94%] [G loss: 0.648863]\n",
      "epoch:2 step:2730 [D loss: 0.157952, acc.: 82.03%] [G loss: 0.702461]\n",
      "epoch:2 step:2731 [D loss: 0.138385, acc.: 83.59%] [G loss: 0.708601]\n",
      "epoch:2 step:2732 [D loss: 0.246633, acc.: 63.28%] [G loss: 0.550709]\n",
      "epoch:2 step:2733 [D loss: 0.187521, acc.: 71.09%] [G loss: 0.538081]\n",
      "epoch:2 step:2734 [D loss: 0.144523, acc.: 84.38%] [G loss: 0.697054]\n",
      "epoch:2 step:2735 [D loss: 0.192719, acc.: 67.97%] [G loss: 0.653951]\n",
      "epoch:2 step:2736 [D loss: 0.195942, acc.: 71.88%] [G loss: 0.573110]\n",
      "epoch:2 step:2737 [D loss: 0.180865, acc.: 69.53%] [G loss: 0.623104]\n",
      "epoch:2 step:2738 [D loss: 0.169646, acc.: 82.03%] [G loss: 0.644764]\n",
      "epoch:2 step:2739 [D loss: 0.169116, acc.: 78.91%] [G loss: 0.690959]\n",
      "epoch:2 step:2740 [D loss: 0.174017, acc.: 75.78%] [G loss: 0.588151]\n",
      "epoch:2 step:2741 [D loss: 0.191511, acc.: 69.53%] [G loss: 0.581497]\n",
      "epoch:2 step:2742 [D loss: 0.167061, acc.: 76.56%] [G loss: 0.622902]\n",
      "epoch:2 step:2743 [D loss: 0.151691, acc.: 78.12%] [G loss: 0.707899]\n",
      "epoch:2 step:2744 [D loss: 0.165244, acc.: 76.56%] [G loss: 0.667903]\n",
      "epoch:2 step:2745 [D loss: 0.179001, acc.: 74.22%] [G loss: 0.612496]\n",
      "epoch:2 step:2746 [D loss: 0.178418, acc.: 75.00%] [G loss: 0.614660]\n",
      "epoch:2 step:2747 [D loss: 0.212490, acc.: 66.41%] [G loss: 0.591954]\n",
      "epoch:2 step:2748 [D loss: 0.140922, acc.: 84.38%] [G loss: 0.667349]\n",
      "epoch:2 step:2749 [D loss: 0.158994, acc.: 81.25%] [G loss: 0.648645]\n",
      "epoch:2 step:2750 [D loss: 0.201073, acc.: 64.84%] [G loss: 0.594737]\n",
      "epoch:2 step:2751 [D loss: 0.175311, acc.: 75.00%] [G loss: 0.615005]\n",
      "epoch:2 step:2752 [D loss: 0.202031, acc.: 67.97%] [G loss: 0.615470]\n",
      "epoch:2 step:2753 [D loss: 0.202025, acc.: 70.31%] [G loss: 0.629744]\n",
      "epoch:2 step:2754 [D loss: 0.251195, acc.: 58.59%] [G loss: 0.560897]\n",
      "epoch:2 step:2755 [D loss: 0.175152, acc.: 72.66%] [G loss: 0.614567]\n",
      "epoch:2 step:2756 [D loss: 0.200703, acc.: 73.44%] [G loss: 0.614931]\n",
      "epoch:2 step:2757 [D loss: 0.180697, acc.: 77.34%] [G loss: 0.624037]\n",
      "epoch:2 step:2758 [D loss: 0.124284, acc.: 90.62%] [G loss: 0.731296]\n",
      "epoch:2 step:2759 [D loss: 0.150021, acc.: 76.56%] [G loss: 0.694030]\n",
      "epoch:2 step:2760 [D loss: 0.146355, acc.: 80.47%] [G loss: 0.699813]\n",
      "epoch:2 step:2761 [D loss: 0.137674, acc.: 86.72%] [G loss: 0.707476]\n",
      "epoch:2 step:2762 [D loss: 0.144838, acc.: 78.91%] [G loss: 0.711400]\n",
      "epoch:2 step:2763 [D loss: 0.135409, acc.: 85.16%] [G loss: 0.691181]\n",
      "epoch:2 step:2764 [D loss: 0.110767, acc.: 87.50%] [G loss: 0.696174]\n",
      "epoch:2 step:2765 [D loss: 0.218974, acc.: 71.88%] [G loss: 0.594802]\n",
      "epoch:2 step:2766 [D loss: 0.216929, acc.: 65.62%] [G loss: 0.567191]\n",
      "epoch:2 step:2767 [D loss: 0.176255, acc.: 71.09%] [G loss: 0.633463]\n",
      "epoch:2 step:2768 [D loss: 0.175895, acc.: 76.56%] [G loss: 0.671027]\n",
      "epoch:2 step:2769 [D loss: 0.173117, acc.: 76.56%] [G loss: 0.645727]\n",
      "epoch:2 step:2770 [D loss: 0.166376, acc.: 75.78%] [G loss: 0.635361]\n",
      "epoch:2 step:2771 [D loss: 0.157666, acc.: 78.91%] [G loss: 0.625341]\n",
      "epoch:2 step:2772 [D loss: 0.152331, acc.: 81.25%] [G loss: 0.646544]\n",
      "epoch:2 step:2773 [D loss: 0.142436, acc.: 82.03%] [G loss: 0.672700]\n",
      "epoch:2 step:2774 [D loss: 0.156186, acc.: 79.69%] [G loss: 0.619034]\n",
      "epoch:2 step:2775 [D loss: 0.148866, acc.: 82.81%] [G loss: 0.607601]\n",
      "epoch:2 step:2776 [D loss: 0.215004, acc.: 64.06%] [G loss: 0.616232]\n",
      "epoch:2 step:2777 [D loss: 0.161712, acc.: 80.47%] [G loss: 0.683295]\n",
      "epoch:2 step:2778 [D loss: 0.167794, acc.: 72.66%] [G loss: 0.698508]\n",
      "epoch:2 step:2779 [D loss: 0.151996, acc.: 81.25%] [G loss: 0.650030]\n",
      "epoch:2 step:2780 [D loss: 0.153297, acc.: 82.03%] [G loss: 0.739829]\n",
      "epoch:2 step:2781 [D loss: 0.173357, acc.: 75.78%] [G loss: 0.611862]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2782 [D loss: 0.151411, acc.: 75.00%] [G loss: 0.635975]\n",
      "epoch:2 step:2783 [D loss: 0.139570, acc.: 80.47%] [G loss: 0.667332]\n",
      "epoch:2 step:2784 [D loss: 0.173161, acc.: 71.88%] [G loss: 0.644566]\n",
      "epoch:2 step:2785 [D loss: 0.144872, acc.: 80.47%] [G loss: 0.736761]\n",
      "epoch:2 step:2786 [D loss: 0.125037, acc.: 81.25%] [G loss: 0.729259]\n",
      "epoch:2 step:2787 [D loss: 0.184435, acc.: 76.56%] [G loss: 0.644598]\n",
      "epoch:2 step:2788 [D loss: 0.105415, acc.: 85.94%] [G loss: 0.714447]\n",
      "epoch:2 step:2789 [D loss: 0.176886, acc.: 75.78%] [G loss: 0.607925]\n",
      "epoch:2 step:2790 [D loss: 0.181645, acc.: 73.44%] [G loss: 0.648343]\n",
      "epoch:2 step:2791 [D loss: 0.193091, acc.: 67.97%] [G loss: 0.654232]\n",
      "epoch:2 step:2792 [D loss: 0.141632, acc.: 83.59%] [G loss: 0.656280]\n",
      "epoch:2 step:2793 [D loss: 0.155619, acc.: 77.34%] [G loss: 0.709944]\n",
      "epoch:2 step:2794 [D loss: 0.195612, acc.: 72.66%] [G loss: 0.698375]\n",
      "epoch:2 step:2795 [D loss: 0.103872, acc.: 87.50%] [G loss: 0.825449]\n",
      "epoch:2 step:2796 [D loss: 0.266758, acc.: 59.38%] [G loss: 0.591494]\n",
      "epoch:2 step:2797 [D loss: 0.118567, acc.: 86.72%] [G loss: 0.686557]\n",
      "epoch:2 step:2798 [D loss: 0.136792, acc.: 82.81%] [G loss: 0.738366]\n",
      "epoch:2 step:2799 [D loss: 0.130340, acc.: 82.81%] [G loss: 0.757164]\n",
      "epoch:2 step:2800 [D loss: 0.127096, acc.: 84.38%] [G loss: 0.708193]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.103530\n",
      "FID: 62.762413\n",
      "0 = 13.739715190649031\n",
      "1 = 0.09868476934442073\n",
      "2 = 0.9907000064849854\n",
      "3 = 0.9815000295639038\n",
      "4 = 0.9998999834060669\n",
      "5 = 0.9998981356620789\n",
      "6 = 0.9815000295639038\n",
      "7 = 10.399863776075861\n",
      "8 = 0.16538793043591335\n",
      "9 = 0.9148499965667725\n",
      "10 = 0.891700029373169\n",
      "11 = 0.9380000233650208\n",
      "12 = 0.9349900484085083\n",
      "13 = 0.891700029373169\n",
      "14 = 5.103561878204346\n",
      "15 = 8.14067268371582\n",
      "16 = 0.32388079166412354\n",
      "17 = 5.103529930114746\n",
      "18 = 62.762413024902344\n",
      "epoch:2 step:2801 [D loss: 0.154729, acc.: 74.22%] [G loss: 0.697614]\n",
      "epoch:2 step:2802 [D loss: 0.266572, acc.: 67.97%] [G loss: 0.568303]\n",
      "epoch:2 step:2803 [D loss: 0.124244, acc.: 82.81%] [G loss: 0.872784]\n",
      "epoch:2 step:2804 [D loss: 0.122795, acc.: 84.38%] [G loss: 0.761558]\n",
      "epoch:2 step:2805 [D loss: 0.140052, acc.: 82.81%] [G loss: 0.629588]\n",
      "epoch:2 step:2806 [D loss: 0.168341, acc.: 76.56%] [G loss: 0.630319]\n",
      "epoch:2 step:2807 [D loss: 0.131149, acc.: 81.25%] [G loss: 0.652374]\n",
      "epoch:2 step:2808 [D loss: 0.196047, acc.: 72.66%] [G loss: 0.623386]\n",
      "epoch:2 step:2809 [D loss: 0.135559, acc.: 82.03%] [G loss: 0.751668]\n",
      "epoch:2 step:2810 [D loss: 0.106460, acc.: 90.62%] [G loss: 0.798939]\n",
      "epoch:2 step:2811 [D loss: 0.212644, acc.: 66.41%] [G loss: 0.596622]\n",
      "epoch:3 step:2812 [D loss: 0.180444, acc.: 76.56%] [G loss: 0.629631]\n",
      "epoch:3 step:2813 [D loss: 0.172584, acc.: 77.34%] [G loss: 0.628579]\n",
      "epoch:3 step:2814 [D loss: 0.167621, acc.: 77.34%] [G loss: 0.608087]\n",
      "epoch:3 step:2815 [D loss: 0.155976, acc.: 73.44%] [G loss: 0.669233]\n",
      "epoch:3 step:2816 [D loss: 0.161466, acc.: 75.00%] [G loss: 0.624295]\n",
      "epoch:3 step:2817 [D loss: 0.170886, acc.: 75.00%] [G loss: 0.601443]\n",
      "epoch:3 step:2818 [D loss: 0.179361, acc.: 72.66%] [G loss: 0.613602]\n",
      "epoch:3 step:2819 [D loss: 0.195174, acc.: 71.09%] [G loss: 0.585766]\n",
      "epoch:3 step:2820 [D loss: 0.153214, acc.: 82.03%] [G loss: 0.624272]\n",
      "epoch:3 step:2821 [D loss: 0.220321, acc.: 67.19%] [G loss: 0.599494]\n",
      "epoch:3 step:2822 [D loss: 0.137222, acc.: 80.47%] [G loss: 0.683544]\n",
      "epoch:3 step:2823 [D loss: 0.168660, acc.: 72.66%] [G loss: 0.615093]\n",
      "epoch:3 step:2824 [D loss: 0.152070, acc.: 79.69%] [G loss: 0.639639]\n",
      "epoch:3 step:2825 [D loss: 0.207678, acc.: 71.88%] [G loss: 0.637577]\n",
      "epoch:3 step:2826 [D loss: 0.142667, acc.: 81.25%] [G loss: 0.678650]\n",
      "epoch:3 step:2827 [D loss: 0.137312, acc.: 81.25%] [G loss: 0.659546]\n",
      "epoch:3 step:2828 [D loss: 0.219026, acc.: 65.62%] [G loss: 0.574567]\n",
      "epoch:3 step:2829 [D loss: 0.218527, acc.: 64.84%] [G loss: 0.588808]\n",
      "epoch:3 step:2830 [D loss: 0.144593, acc.: 81.25%] [G loss: 0.652186]\n",
      "epoch:3 step:2831 [D loss: 0.237110, acc.: 60.94%] [G loss: 0.572578]\n",
      "epoch:3 step:2832 [D loss: 0.149788, acc.: 77.34%] [G loss: 0.697022]\n",
      "epoch:3 step:2833 [D loss: 0.147705, acc.: 75.78%] [G loss: 0.690645]\n",
      "epoch:3 step:2834 [D loss: 0.159232, acc.: 79.69%] [G loss: 0.664819]\n",
      "epoch:3 step:2835 [D loss: 0.141510, acc.: 78.12%] [G loss: 0.677980]\n",
      "epoch:3 step:2836 [D loss: 0.157672, acc.: 78.91%] [G loss: 0.611523]\n",
      "epoch:3 step:2837 [D loss: 0.172615, acc.: 71.88%] [G loss: 0.620082]\n",
      "epoch:3 step:2838 [D loss: 0.136793, acc.: 84.38%] [G loss: 0.678028]\n",
      "epoch:3 step:2839 [D loss: 0.155938, acc.: 75.78%] [G loss: 0.644747]\n",
      "epoch:3 step:2840 [D loss: 0.173776, acc.: 72.66%] [G loss: 0.625499]\n",
      "epoch:3 step:2841 [D loss: 0.166571, acc.: 78.91%] [G loss: 0.619034]\n",
      "epoch:3 step:2842 [D loss: 0.141904, acc.: 83.59%] [G loss: 0.672844]\n",
      "epoch:3 step:2843 [D loss: 0.184102, acc.: 73.44%] [G loss: 0.595437]\n",
      "epoch:3 step:2844 [D loss: 0.120599, acc.: 83.59%] [G loss: 0.707004]\n",
      "epoch:3 step:2845 [D loss: 0.175150, acc.: 76.56%] [G loss: 0.639408]\n",
      "epoch:3 step:2846 [D loss: 0.133368, acc.: 87.50%] [G loss: 0.651432]\n",
      "epoch:3 step:2847 [D loss: 0.103609, acc.: 88.28%] [G loss: 0.720640]\n",
      "epoch:3 step:2848 [D loss: 0.167142, acc.: 78.91%] [G loss: 0.651296]\n",
      "epoch:3 step:2849 [D loss: 0.197727, acc.: 71.88%] [G loss: 0.583747]\n",
      "epoch:3 step:2850 [D loss: 0.168549, acc.: 74.22%] [G loss: 0.649948]\n",
      "epoch:3 step:2851 [D loss: 0.108271, acc.: 85.94%] [G loss: 0.660497]\n",
      "epoch:3 step:2852 [D loss: 0.176006, acc.: 77.34%] [G loss: 0.594701]\n",
      "epoch:3 step:2853 [D loss: 0.178831, acc.: 71.88%] [G loss: 0.566614]\n",
      "epoch:3 step:2854 [D loss: 0.161366, acc.: 77.34%] [G loss: 0.610597]\n",
      "epoch:3 step:2855 [D loss: 0.179215, acc.: 75.78%] [G loss: 0.651045]\n",
      "epoch:3 step:2856 [D loss: 0.163561, acc.: 80.47%] [G loss: 0.645958]\n",
      "epoch:3 step:2857 [D loss: 0.155371, acc.: 80.47%] [G loss: 0.625449]\n",
      "epoch:3 step:2858 [D loss: 0.151446, acc.: 75.00%] [G loss: 0.637290]\n",
      "epoch:3 step:2859 [D loss: 0.200414, acc.: 71.09%] [G loss: 0.547704]\n",
      "epoch:3 step:2860 [D loss: 0.194166, acc.: 69.53%] [G loss: 0.626458]\n",
      "epoch:3 step:2861 [D loss: 0.154984, acc.: 77.34%] [G loss: 0.652063]\n",
      "epoch:3 step:2862 [D loss: 0.206206, acc.: 68.75%] [G loss: 0.604718]\n",
      "epoch:3 step:2863 [D loss: 0.164469, acc.: 77.34%] [G loss: 0.635136]\n",
      "epoch:3 step:2864 [D loss: 0.132003, acc.: 82.81%] [G loss: 0.682563]\n",
      "epoch:3 step:2865 [D loss: 0.152610, acc.: 81.25%] [G loss: 0.684493]\n",
      "epoch:3 step:2866 [D loss: 0.173427, acc.: 73.44%] [G loss: 0.652752]\n",
      "epoch:3 step:2867 [D loss: 0.175996, acc.: 69.53%] [G loss: 0.607416]\n",
      "epoch:3 step:2868 [D loss: 0.160553, acc.: 81.25%] [G loss: 0.679937]\n",
      "epoch:3 step:2869 [D loss: 0.154628, acc.: 78.91%] [G loss: 0.663264]\n",
      "epoch:3 step:2870 [D loss: 0.179280, acc.: 72.66%] [G loss: 0.627769]\n",
      "epoch:3 step:2871 [D loss: 0.183917, acc.: 71.09%] [G loss: 0.638251]\n",
      "epoch:3 step:2872 [D loss: 0.154322, acc.: 80.47%] [G loss: 0.634526]\n",
      "epoch:3 step:2873 [D loss: 0.195665, acc.: 67.19%] [G loss: 0.610150]\n",
      "epoch:3 step:2874 [D loss: 0.154793, acc.: 80.47%] [G loss: 0.627697]\n",
      "epoch:3 step:2875 [D loss: 0.160569, acc.: 79.69%] [G loss: 0.628577]\n",
      "epoch:3 step:2876 [D loss: 0.156668, acc.: 78.12%] [G loss: 0.600685]\n",
      "epoch:3 step:2877 [D loss: 0.151949, acc.: 82.03%] [G loss: 0.616967]\n",
      "epoch:3 step:2878 [D loss: 0.175139, acc.: 76.56%] [G loss: 0.627893]\n",
      "epoch:3 step:2879 [D loss: 0.176232, acc.: 73.44%] [G loss: 0.599606]\n",
      "epoch:3 step:2880 [D loss: 0.175042, acc.: 75.00%] [G loss: 0.601662]\n",
      "epoch:3 step:2881 [D loss: 0.143948, acc.: 81.25%] [G loss: 0.657371]\n",
      "epoch:3 step:2882 [D loss: 0.156523, acc.: 82.03%] [G loss: 0.612180]\n",
      "epoch:3 step:2883 [D loss: 0.129365, acc.: 84.38%] [G loss: 0.714027]\n",
      "epoch:3 step:2884 [D loss: 0.140862, acc.: 79.69%] [G loss: 0.659880]\n",
      "epoch:3 step:2885 [D loss: 0.159722, acc.: 78.91%] [G loss: 0.615916]\n",
      "epoch:3 step:2886 [D loss: 0.135760, acc.: 82.03%] [G loss: 0.728748]\n",
      "epoch:3 step:2887 [D loss: 0.142210, acc.: 80.47%] [G loss: 0.711721]\n",
      "epoch:3 step:2888 [D loss: 0.113168, acc.: 85.16%] [G loss: 0.738802]\n",
      "epoch:3 step:2889 [D loss: 0.248409, acc.: 57.81%] [G loss: 0.562101]\n",
      "epoch:3 step:2890 [D loss: 0.167514, acc.: 77.34%] [G loss: 0.599736]\n",
      "epoch:3 step:2891 [D loss: 0.171387, acc.: 71.88%] [G loss: 0.613369]\n",
      "epoch:3 step:2892 [D loss: 0.189107, acc.: 72.66%] [G loss: 0.594563]\n",
      "epoch:3 step:2893 [D loss: 0.164632, acc.: 75.78%] [G loss: 0.614737]\n",
      "epoch:3 step:2894 [D loss: 0.139370, acc.: 83.59%] [G loss: 0.649033]\n",
      "epoch:3 step:2895 [D loss: 0.160825, acc.: 79.69%] [G loss: 0.631938]\n",
      "epoch:3 step:2896 [D loss: 0.160004, acc.: 77.34%] [G loss: 0.641541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2897 [D loss: 0.138250, acc.: 82.03%] [G loss: 0.699307]\n",
      "epoch:3 step:2898 [D loss: 0.145628, acc.: 78.91%] [G loss: 0.618285]\n",
      "epoch:3 step:2899 [D loss: 0.154433, acc.: 80.47%] [G loss: 0.627171]\n",
      "epoch:3 step:2900 [D loss: 0.167102, acc.: 76.56%] [G loss: 0.605902]\n",
      "epoch:3 step:2901 [D loss: 0.133955, acc.: 83.59%] [G loss: 0.662794]\n",
      "epoch:3 step:2902 [D loss: 0.159555, acc.: 80.47%] [G loss: 0.622535]\n",
      "epoch:3 step:2903 [D loss: 0.157147, acc.: 77.34%] [G loss: 0.625778]\n",
      "epoch:3 step:2904 [D loss: 0.164838, acc.: 73.44%] [G loss: 0.614909]\n",
      "epoch:3 step:2905 [D loss: 0.136483, acc.: 78.12%] [G loss: 0.695191]\n",
      "epoch:3 step:2906 [D loss: 0.162767, acc.: 74.22%] [G loss: 0.677686]\n",
      "epoch:3 step:2907 [D loss: 0.119006, acc.: 84.38%] [G loss: 0.672800]\n",
      "epoch:3 step:2908 [D loss: 0.151427, acc.: 78.91%] [G loss: 0.658651]\n",
      "epoch:3 step:2909 [D loss: 0.154983, acc.: 74.22%] [G loss: 0.619354]\n",
      "epoch:3 step:2910 [D loss: 0.159722, acc.: 80.47%] [G loss: 0.639392]\n",
      "epoch:3 step:2911 [D loss: 0.137254, acc.: 80.47%] [G loss: 0.700661]\n",
      "epoch:3 step:2912 [D loss: 0.150155, acc.: 80.47%] [G loss: 0.690780]\n",
      "epoch:3 step:2913 [D loss: 0.202702, acc.: 67.19%] [G loss: 0.582648]\n",
      "epoch:3 step:2914 [D loss: 0.134576, acc.: 83.59%] [G loss: 0.663815]\n",
      "epoch:3 step:2915 [D loss: 0.168470, acc.: 73.44%] [G loss: 0.665470]\n",
      "epoch:3 step:2916 [D loss: 0.188735, acc.: 68.75%] [G loss: 0.647417]\n",
      "epoch:3 step:2917 [D loss: 0.168421, acc.: 75.78%] [G loss: 0.607685]\n",
      "epoch:3 step:2918 [D loss: 0.194681, acc.: 67.97%] [G loss: 0.613184]\n",
      "epoch:3 step:2919 [D loss: 0.194824, acc.: 70.31%] [G loss: 0.660548]\n",
      "epoch:3 step:2920 [D loss: 0.212719, acc.: 70.31%] [G loss: 0.588608]\n",
      "epoch:3 step:2921 [D loss: 0.169932, acc.: 73.44%] [G loss: 0.629392]\n",
      "epoch:3 step:2922 [D loss: 0.146305, acc.: 78.12%] [G loss: 0.673740]\n",
      "epoch:3 step:2923 [D loss: 0.175069, acc.: 74.22%] [G loss: 0.648325]\n",
      "epoch:3 step:2924 [D loss: 0.171171, acc.: 74.22%] [G loss: 0.570734]\n",
      "epoch:3 step:2925 [D loss: 0.191658, acc.: 69.53%] [G loss: 0.542646]\n",
      "epoch:3 step:2926 [D loss: 0.158567, acc.: 78.91%] [G loss: 0.655451]\n",
      "epoch:3 step:2927 [D loss: 0.164793, acc.: 75.00%] [G loss: 0.681375]\n",
      "epoch:3 step:2928 [D loss: 0.147925, acc.: 82.81%] [G loss: 0.659837]\n",
      "epoch:3 step:2929 [D loss: 0.169843, acc.: 73.44%] [G loss: 0.675389]\n",
      "epoch:3 step:2930 [D loss: 0.137246, acc.: 78.91%] [G loss: 0.703434]\n",
      "epoch:3 step:2931 [D loss: 0.239509, acc.: 64.06%] [G loss: 0.565467]\n",
      "epoch:3 step:2932 [D loss: 0.173313, acc.: 71.88%] [G loss: 0.591104]\n",
      "epoch:3 step:2933 [D loss: 0.130302, acc.: 83.59%] [G loss: 0.763350]\n",
      "epoch:3 step:2934 [D loss: 0.209807, acc.: 70.31%] [G loss: 0.609229]\n",
      "epoch:3 step:2935 [D loss: 0.200759, acc.: 72.66%] [G loss: 0.591124]\n",
      "epoch:3 step:2936 [D loss: 0.157711, acc.: 75.78%] [G loss: 0.597369]\n",
      "epoch:3 step:2937 [D loss: 0.147802, acc.: 80.47%] [G loss: 0.643333]\n",
      "epoch:3 step:2938 [D loss: 0.150751, acc.: 77.34%] [G loss: 0.614468]\n",
      "epoch:3 step:2939 [D loss: 0.145225, acc.: 78.91%] [G loss: 0.634247]\n",
      "epoch:3 step:2940 [D loss: 0.210206, acc.: 67.19%] [G loss: 0.573326]\n",
      "epoch:3 step:2941 [D loss: 0.155180, acc.: 80.47%] [G loss: 0.629570]\n",
      "epoch:3 step:2942 [D loss: 0.171168, acc.: 73.44%] [G loss: 0.659941]\n",
      "epoch:3 step:2943 [D loss: 0.187121, acc.: 71.88%] [G loss: 0.618997]\n",
      "epoch:3 step:2944 [D loss: 0.188403, acc.: 71.09%] [G loss: 0.584277]\n",
      "epoch:3 step:2945 [D loss: 0.140805, acc.: 85.16%] [G loss: 0.638754]\n",
      "epoch:3 step:2946 [D loss: 0.137097, acc.: 84.38%] [G loss: 0.677207]\n",
      "epoch:3 step:2947 [D loss: 0.222428, acc.: 67.19%] [G loss: 0.556594]\n",
      "epoch:3 step:2948 [D loss: 0.166150, acc.: 77.34%] [G loss: 0.656817]\n",
      "epoch:3 step:2949 [D loss: 0.160621, acc.: 76.56%] [G loss: 0.667683]\n",
      "epoch:3 step:2950 [D loss: 0.177344, acc.: 75.78%] [G loss: 0.591951]\n",
      "epoch:3 step:2951 [D loss: 0.175590, acc.: 74.22%] [G loss: 0.559014]\n",
      "epoch:3 step:2952 [D loss: 0.164569, acc.: 75.00%] [G loss: 0.606190]\n",
      "epoch:3 step:2953 [D loss: 0.175397, acc.: 76.56%] [G loss: 0.622688]\n",
      "epoch:3 step:2954 [D loss: 0.174011, acc.: 71.88%] [G loss: 0.629949]\n",
      "epoch:3 step:2955 [D loss: 0.154755, acc.: 76.56%] [G loss: 0.678449]\n",
      "epoch:3 step:2956 [D loss: 0.182578, acc.: 70.31%] [G loss: 0.647231]\n",
      "epoch:3 step:2957 [D loss: 0.178379, acc.: 75.00%] [G loss: 0.643950]\n",
      "epoch:3 step:2958 [D loss: 0.206896, acc.: 70.31%] [G loss: 0.576129]\n",
      "epoch:3 step:2959 [D loss: 0.219504, acc.: 69.53%] [G loss: 0.582550]\n",
      "epoch:3 step:2960 [D loss: 0.132176, acc.: 83.59%] [G loss: 0.698051]\n",
      "epoch:3 step:2961 [D loss: 0.223920, acc.: 69.53%] [G loss: 0.569770]\n",
      "epoch:3 step:2962 [D loss: 0.149532, acc.: 80.47%] [G loss: 0.591341]\n",
      "epoch:3 step:2963 [D loss: 0.112943, acc.: 88.28%] [G loss: 0.739933]\n",
      "epoch:3 step:2964 [D loss: 0.199845, acc.: 74.22%] [G loss: 0.653218]\n",
      "epoch:3 step:2965 [D loss: 0.142623, acc.: 80.47%] [G loss: 0.658060]\n",
      "epoch:3 step:2966 [D loss: 0.132986, acc.: 85.16%] [G loss: 0.660774]\n",
      "epoch:3 step:2967 [D loss: 0.154406, acc.: 78.12%] [G loss: 0.629847]\n",
      "epoch:3 step:2968 [D loss: 0.159902, acc.: 80.47%] [G loss: 0.601172]\n",
      "epoch:3 step:2969 [D loss: 0.165342, acc.: 78.12%] [G loss: 0.608035]\n",
      "epoch:3 step:2970 [D loss: 0.155025, acc.: 79.69%] [G loss: 0.614699]\n",
      "epoch:3 step:2971 [D loss: 0.169067, acc.: 74.22%] [G loss: 0.632879]\n",
      "epoch:3 step:2972 [D loss: 0.166050, acc.: 74.22%] [G loss: 0.650394]\n",
      "epoch:3 step:2973 [D loss: 0.146159, acc.: 80.47%] [G loss: 0.771899]\n",
      "epoch:3 step:2974 [D loss: 0.176084, acc.: 77.34%] [G loss: 0.696993]\n",
      "epoch:3 step:2975 [D loss: 0.176308, acc.: 78.91%] [G loss: 0.586005]\n",
      "epoch:3 step:2976 [D loss: 0.158461, acc.: 76.56%] [G loss: 0.645907]\n",
      "epoch:3 step:2977 [D loss: 0.133686, acc.: 80.47%] [G loss: 0.655486]\n",
      "epoch:3 step:2978 [D loss: 0.178275, acc.: 73.44%] [G loss: 0.579030]\n",
      "epoch:3 step:2979 [D loss: 0.166263, acc.: 77.34%] [G loss: 0.595982]\n",
      "epoch:3 step:2980 [D loss: 0.173362, acc.: 72.66%] [G loss: 0.647157]\n",
      "epoch:3 step:2981 [D loss: 0.167573, acc.: 82.03%] [G loss: 0.644158]\n",
      "epoch:3 step:2982 [D loss: 0.154828, acc.: 80.47%] [G loss: 0.630260]\n",
      "epoch:3 step:2983 [D loss: 0.220298, acc.: 66.41%] [G loss: 0.585274]\n",
      "epoch:3 step:2984 [D loss: 0.140056, acc.: 84.38%] [G loss: 0.656910]\n",
      "epoch:3 step:2985 [D loss: 0.171675, acc.: 74.22%] [G loss: 0.597651]\n",
      "epoch:3 step:2986 [D loss: 0.145941, acc.: 85.16%] [G loss: 0.613882]\n",
      "epoch:3 step:2987 [D loss: 0.171732, acc.: 75.78%] [G loss: 0.641215]\n",
      "epoch:3 step:2988 [D loss: 0.172595, acc.: 75.78%] [G loss: 0.587736]\n",
      "epoch:3 step:2989 [D loss: 0.174841, acc.: 74.22%] [G loss: 0.558410]\n",
      "epoch:3 step:2990 [D loss: 0.199664, acc.: 73.44%] [G loss: 0.631307]\n",
      "epoch:3 step:2991 [D loss: 0.151226, acc.: 77.34%] [G loss: 0.664944]\n",
      "epoch:3 step:2992 [D loss: 0.181031, acc.: 72.66%] [G loss: 0.598983]\n",
      "epoch:3 step:2993 [D loss: 0.199597, acc.: 70.31%] [G loss: 0.581794]\n",
      "epoch:3 step:2994 [D loss: 0.183690, acc.: 77.34%] [G loss: 0.588740]\n",
      "epoch:3 step:2995 [D loss: 0.168951, acc.: 75.00%] [G loss: 0.587227]\n",
      "epoch:3 step:2996 [D loss: 0.220176, acc.: 67.97%] [G loss: 0.621115]\n",
      "epoch:3 step:2997 [D loss: 0.177848, acc.: 77.34%] [G loss: 0.626395]\n",
      "epoch:3 step:2998 [D loss: 0.173863, acc.: 72.66%] [G loss: 0.616310]\n",
      "epoch:3 step:2999 [D loss: 0.194137, acc.: 72.66%] [G loss: 0.653657]\n",
      "epoch:3 step:3000 [D loss: 0.147917, acc.: 80.47%] [G loss: 0.643305]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.533000\n",
      "FID: 51.931789\n",
      "0 = 13.334164308404942\n",
      "1 = 0.08628981072311694\n",
      "2 = 0.9854999780654907\n",
      "3 = 0.9714000225067139\n",
      "4 = 0.9995999932289124\n",
      "5 = 0.9995883703231812\n",
      "6 = 0.9714000225067139\n",
      "7 = 9.869382768082586\n",
      "8 = 0.14884371137728641\n",
      "9 = 0.8897500038146973\n",
      "10 = 0.866100013256073\n",
      "11 = 0.9133999943733215\n",
      "12 = 0.9091004729270935\n",
      "13 = 0.866100013256073\n",
      "14 = 5.533040523529053\n",
      "15 = 8.929036140441895\n",
      "16 = 0.25875124335289\n",
      "17 = 5.532999515533447\n",
      "18 = 51.93178939819336\n",
      "epoch:3 step:3001 [D loss: 0.148013, acc.: 79.69%] [G loss: 0.635974]\n",
      "epoch:3 step:3002 [D loss: 0.151330, acc.: 75.78%] [G loss: 0.654252]\n",
      "epoch:3 step:3003 [D loss: 0.172265, acc.: 72.66%] [G loss: 0.633795]\n",
      "epoch:3 step:3004 [D loss: 0.169877, acc.: 75.78%] [G loss: 0.603539]\n",
      "epoch:3 step:3005 [D loss: 0.161280, acc.: 78.91%] [G loss: 0.643372]\n",
      "epoch:3 step:3006 [D loss: 0.189054, acc.: 69.53%] [G loss: 0.630081]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3007 [D loss: 0.168390, acc.: 74.22%] [G loss: 0.596204]\n",
      "epoch:3 step:3008 [D loss: 0.165511, acc.: 81.25%] [G loss: 0.577594]\n",
      "epoch:3 step:3009 [D loss: 0.140413, acc.: 81.25%] [G loss: 0.648291]\n",
      "epoch:3 step:3010 [D loss: 0.163993, acc.: 75.00%] [G loss: 0.636052]\n",
      "epoch:3 step:3011 [D loss: 0.203227, acc.: 73.44%] [G loss: 0.587766]\n",
      "epoch:3 step:3012 [D loss: 0.182385, acc.: 71.09%] [G loss: 0.584589]\n",
      "epoch:3 step:3013 [D loss: 0.201989, acc.: 68.75%] [G loss: 0.617134]\n",
      "epoch:3 step:3014 [D loss: 0.201720, acc.: 67.19%] [G loss: 0.576174]\n",
      "epoch:3 step:3015 [D loss: 0.148190, acc.: 81.25%] [G loss: 0.639313]\n",
      "epoch:3 step:3016 [D loss: 0.156550, acc.: 79.69%] [G loss: 0.664653]\n",
      "epoch:3 step:3017 [D loss: 0.149696, acc.: 80.47%] [G loss: 0.597385]\n",
      "epoch:3 step:3018 [D loss: 0.129463, acc.: 82.03%] [G loss: 0.614672]\n",
      "epoch:3 step:3019 [D loss: 0.153682, acc.: 78.91%] [G loss: 0.661979]\n",
      "epoch:3 step:3020 [D loss: 0.138412, acc.: 79.69%] [G loss: 0.643490]\n",
      "epoch:3 step:3021 [D loss: 0.202156, acc.: 68.75%] [G loss: 0.571192]\n",
      "epoch:3 step:3022 [D loss: 0.165310, acc.: 74.22%] [G loss: 0.558927]\n",
      "epoch:3 step:3023 [D loss: 0.183483, acc.: 72.66%] [G loss: 0.623185]\n",
      "epoch:3 step:3024 [D loss: 0.164382, acc.: 77.34%] [G loss: 0.616866]\n",
      "epoch:3 step:3025 [D loss: 0.219232, acc.: 67.19%] [G loss: 0.527479]\n",
      "epoch:3 step:3026 [D loss: 0.165753, acc.: 76.56%] [G loss: 0.606798]\n",
      "epoch:3 step:3027 [D loss: 0.183707, acc.: 68.75%] [G loss: 0.631906]\n",
      "epoch:3 step:3028 [D loss: 0.164855, acc.: 77.34%] [G loss: 0.608617]\n",
      "epoch:3 step:3029 [D loss: 0.178448, acc.: 76.56%] [G loss: 0.614374]\n",
      "epoch:3 step:3030 [D loss: 0.186239, acc.: 71.09%] [G loss: 0.566830]\n",
      "epoch:3 step:3031 [D loss: 0.180022, acc.: 75.00%] [G loss: 0.598665]\n",
      "epoch:3 step:3032 [D loss: 0.115382, acc.: 85.94%] [G loss: 0.705848]\n",
      "epoch:3 step:3033 [D loss: 0.146508, acc.: 78.91%] [G loss: 0.635708]\n",
      "epoch:3 step:3034 [D loss: 0.122948, acc.: 83.59%] [G loss: 0.711483]\n",
      "epoch:3 step:3035 [D loss: 0.183297, acc.: 71.88%] [G loss: 0.609146]\n",
      "epoch:3 step:3036 [D loss: 0.202090, acc.: 66.41%] [G loss: 0.583040]\n",
      "epoch:3 step:3037 [D loss: 0.217241, acc.: 62.50%] [G loss: 0.545666]\n",
      "epoch:3 step:3038 [D loss: 0.216854, acc.: 69.53%] [G loss: 0.560216]\n",
      "epoch:3 step:3039 [D loss: 0.207778, acc.: 70.31%] [G loss: 0.581147]\n",
      "epoch:3 step:3040 [D loss: 0.167364, acc.: 79.69%] [G loss: 0.626213]\n",
      "epoch:3 step:3041 [D loss: 0.140117, acc.: 81.25%] [G loss: 0.666034]\n",
      "epoch:3 step:3042 [D loss: 0.164809, acc.: 78.12%] [G loss: 0.709058]\n",
      "epoch:3 step:3043 [D loss: 0.127892, acc.: 85.16%] [G loss: 0.724317]\n",
      "epoch:3 step:3044 [D loss: 0.188952, acc.: 71.09%] [G loss: 0.620452]\n",
      "epoch:3 step:3045 [D loss: 0.173619, acc.: 74.22%] [G loss: 0.578056]\n",
      "epoch:3 step:3046 [D loss: 0.162673, acc.: 79.69%] [G loss: 0.596400]\n",
      "epoch:3 step:3047 [D loss: 0.158020, acc.: 78.12%] [G loss: 0.679160]\n",
      "epoch:3 step:3048 [D loss: 0.200772, acc.: 71.88%] [G loss: 0.597630]\n",
      "epoch:3 step:3049 [D loss: 0.192491, acc.: 71.09%] [G loss: 0.561255]\n",
      "epoch:3 step:3050 [D loss: 0.174239, acc.: 72.66%] [G loss: 0.593119]\n",
      "epoch:3 step:3051 [D loss: 0.173009, acc.: 74.22%] [G loss: 0.621486]\n",
      "epoch:3 step:3052 [D loss: 0.207774, acc.: 67.19%] [G loss: 0.573323]\n",
      "epoch:3 step:3053 [D loss: 0.172905, acc.: 77.34%] [G loss: 0.618567]\n",
      "epoch:3 step:3054 [D loss: 0.167312, acc.: 76.56%] [G loss: 0.646996]\n",
      "epoch:3 step:3055 [D loss: 0.171584, acc.: 72.66%] [G loss: 0.646251]\n",
      "epoch:3 step:3056 [D loss: 0.166906, acc.: 78.91%] [G loss: 0.659466]\n",
      "epoch:3 step:3057 [D loss: 0.179119, acc.: 73.44%] [G loss: 0.614687]\n",
      "epoch:3 step:3058 [D loss: 0.199732, acc.: 69.53%] [G loss: 0.555296]\n",
      "epoch:3 step:3059 [D loss: 0.170820, acc.: 75.00%] [G loss: 0.577506]\n",
      "epoch:3 step:3060 [D loss: 0.176850, acc.: 73.44%] [G loss: 0.602018]\n",
      "epoch:3 step:3061 [D loss: 0.204942, acc.: 63.28%] [G loss: 0.627460]\n",
      "epoch:3 step:3062 [D loss: 0.212061, acc.: 66.41%] [G loss: 0.667114]\n",
      "epoch:3 step:3063 [D loss: 0.197941, acc.: 67.97%] [G loss: 0.624201]\n",
      "epoch:3 step:3064 [D loss: 0.159499, acc.: 78.91%] [G loss: 0.638788]\n",
      "epoch:3 step:3065 [D loss: 0.166761, acc.: 75.00%] [G loss: 0.656075]\n",
      "epoch:3 step:3066 [D loss: 0.193954, acc.: 65.62%] [G loss: 0.590947]\n",
      "epoch:3 step:3067 [D loss: 0.155664, acc.: 75.78%] [G loss: 0.663100]\n",
      "epoch:3 step:3068 [D loss: 0.178124, acc.: 75.00%] [G loss: 0.623418]\n",
      "epoch:3 step:3069 [D loss: 0.154601, acc.: 79.69%] [G loss: 0.620326]\n",
      "epoch:3 step:3070 [D loss: 0.169245, acc.: 79.69%] [G loss: 0.660376]\n",
      "epoch:3 step:3071 [D loss: 0.176554, acc.: 74.22%] [G loss: 0.650139]\n",
      "epoch:3 step:3072 [D loss: 0.164615, acc.: 80.47%] [G loss: 0.690351]\n",
      "epoch:3 step:3073 [D loss: 0.172016, acc.: 76.56%] [G loss: 0.635862]\n",
      "epoch:3 step:3074 [D loss: 0.208008, acc.: 71.09%] [G loss: 0.526164]\n",
      "epoch:3 step:3075 [D loss: 0.164954, acc.: 76.56%] [G loss: 0.640165]\n",
      "epoch:3 step:3076 [D loss: 0.171124, acc.: 74.22%] [G loss: 0.616956]\n",
      "epoch:3 step:3077 [D loss: 0.181235, acc.: 71.88%] [G loss: 0.645223]\n",
      "epoch:3 step:3078 [D loss: 0.196322, acc.: 69.53%] [G loss: 0.559801]\n",
      "epoch:3 step:3079 [D loss: 0.186664, acc.: 77.34%] [G loss: 0.556327]\n",
      "epoch:3 step:3080 [D loss: 0.183006, acc.: 74.22%] [G loss: 0.619648]\n",
      "epoch:3 step:3081 [D loss: 0.173144, acc.: 77.34%] [G loss: 0.662035]\n",
      "epoch:3 step:3082 [D loss: 0.160633, acc.: 74.22%] [G loss: 0.662261]\n",
      "epoch:3 step:3083 [D loss: 0.207600, acc.: 66.41%] [G loss: 0.542906]\n",
      "epoch:3 step:3084 [D loss: 0.154301, acc.: 85.16%] [G loss: 0.577645]\n",
      "epoch:3 step:3085 [D loss: 0.183836, acc.: 69.53%] [G loss: 0.594209]\n",
      "epoch:3 step:3086 [D loss: 0.217860, acc.: 67.97%] [G loss: 0.561050]\n",
      "epoch:3 step:3087 [D loss: 0.152641, acc.: 77.34%] [G loss: 0.615047]\n",
      "epoch:3 step:3088 [D loss: 0.198943, acc.: 66.41%] [G loss: 0.575788]\n",
      "epoch:3 step:3089 [D loss: 0.173729, acc.: 78.12%] [G loss: 0.684761]\n",
      "epoch:3 step:3090 [D loss: 0.155843, acc.: 78.91%] [G loss: 0.641009]\n",
      "epoch:3 step:3091 [D loss: 0.198743, acc.: 70.31%] [G loss: 0.693314]\n",
      "epoch:3 step:3092 [D loss: 0.228055, acc.: 69.53%] [G loss: 0.552704]\n",
      "epoch:3 step:3093 [D loss: 0.170812, acc.: 74.22%] [G loss: 0.605889]\n",
      "epoch:3 step:3094 [D loss: 0.156969, acc.: 78.12%] [G loss: 0.617046]\n",
      "epoch:3 step:3095 [D loss: 0.186601, acc.: 73.44%] [G loss: 0.581183]\n",
      "epoch:3 step:3096 [D loss: 0.130385, acc.: 79.69%] [G loss: 0.628275]\n",
      "epoch:3 step:3097 [D loss: 0.154945, acc.: 79.69%] [G loss: 0.642653]\n",
      "epoch:3 step:3098 [D loss: 0.190784, acc.: 74.22%] [G loss: 0.581349]\n",
      "epoch:3 step:3099 [D loss: 0.162514, acc.: 78.12%] [G loss: 0.635024]\n",
      "epoch:3 step:3100 [D loss: 0.158075, acc.: 77.34%] [G loss: 0.627965]\n",
      "epoch:3 step:3101 [D loss: 0.150753, acc.: 75.00%] [G loss: 0.621653]\n",
      "epoch:3 step:3102 [D loss: 0.142607, acc.: 83.59%] [G loss: 0.658045]\n",
      "epoch:3 step:3103 [D loss: 0.214366, acc.: 64.84%] [G loss: 0.526633]\n",
      "epoch:3 step:3104 [D loss: 0.154082, acc.: 76.56%] [G loss: 0.659970]\n",
      "epoch:3 step:3105 [D loss: 0.199764, acc.: 72.66%] [G loss: 0.614802]\n",
      "epoch:3 step:3106 [D loss: 0.153351, acc.: 82.81%] [G loss: 0.608201]\n",
      "epoch:3 step:3107 [D loss: 0.143371, acc.: 83.59%] [G loss: 0.592145]\n",
      "epoch:3 step:3108 [D loss: 0.174747, acc.: 74.22%] [G loss: 0.629723]\n",
      "epoch:3 step:3109 [D loss: 0.177795, acc.: 75.00%] [G loss: 0.577179]\n",
      "epoch:3 step:3110 [D loss: 0.159776, acc.: 79.69%] [G loss: 0.617203]\n",
      "epoch:3 step:3111 [D loss: 0.165005, acc.: 78.91%] [G loss: 0.617305]\n",
      "epoch:3 step:3112 [D loss: 0.175740, acc.: 73.44%] [G loss: 0.621019]\n",
      "epoch:3 step:3113 [D loss: 0.141982, acc.: 82.03%] [G loss: 0.639228]\n",
      "epoch:3 step:3114 [D loss: 0.144796, acc.: 84.38%] [G loss: 0.631722]\n",
      "epoch:3 step:3115 [D loss: 0.138514, acc.: 81.25%] [G loss: 0.637820]\n",
      "epoch:3 step:3116 [D loss: 0.143606, acc.: 83.59%] [G loss: 0.642209]\n",
      "epoch:3 step:3117 [D loss: 0.186358, acc.: 75.78%] [G loss: 0.652070]\n",
      "epoch:3 step:3118 [D loss: 0.120613, acc.: 87.50%] [G loss: 0.725936]\n",
      "epoch:3 step:3119 [D loss: 0.161635, acc.: 74.22%] [G loss: 0.629746]\n",
      "epoch:3 step:3120 [D loss: 0.136563, acc.: 85.94%] [G loss: 0.712024]\n",
      "epoch:3 step:3121 [D loss: 0.114421, acc.: 87.50%] [G loss: 0.709636]\n",
      "epoch:3 step:3122 [D loss: 0.144280, acc.: 79.69%] [G loss: 0.667312]\n",
      "epoch:3 step:3123 [D loss: 0.110425, acc.: 87.50%] [G loss: 0.731578]\n",
      "epoch:3 step:3124 [D loss: 0.124605, acc.: 82.81%] [G loss: 0.719853]\n",
      "epoch:3 step:3125 [D loss: 0.108410, acc.: 85.94%] [G loss: 0.742799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3126 [D loss: 0.134296, acc.: 86.72%] [G loss: 0.735152]\n",
      "epoch:3 step:3127 [D loss: 0.209153, acc.: 69.53%] [G loss: 0.580000]\n",
      "epoch:3 step:3128 [D loss: 0.177954, acc.: 70.31%] [G loss: 0.592721]\n",
      "epoch:3 step:3129 [D loss: 0.125043, acc.: 85.94%] [G loss: 0.636197]\n",
      "epoch:3 step:3130 [D loss: 0.146174, acc.: 82.03%] [G loss: 0.588088]\n",
      "epoch:3 step:3131 [D loss: 0.173400, acc.: 70.31%] [G loss: 0.586188]\n",
      "epoch:3 step:3132 [D loss: 0.153052, acc.: 76.56%] [G loss: 0.644106]\n",
      "epoch:3 step:3133 [D loss: 0.141956, acc.: 82.81%] [G loss: 0.659024]\n",
      "epoch:3 step:3134 [D loss: 0.168231, acc.: 77.34%] [G loss: 0.585950]\n",
      "epoch:3 step:3135 [D loss: 0.173902, acc.: 73.44%] [G loss: 0.617863]\n",
      "epoch:3 step:3136 [D loss: 0.144706, acc.: 80.47%] [G loss: 0.621756]\n",
      "epoch:3 step:3137 [D loss: 0.169983, acc.: 80.47%] [G loss: 0.595451]\n",
      "epoch:3 step:3138 [D loss: 0.156107, acc.: 77.34%] [G loss: 0.630141]\n",
      "epoch:3 step:3139 [D loss: 0.143402, acc.: 85.94%] [G loss: 0.677070]\n",
      "epoch:3 step:3140 [D loss: 0.150608, acc.: 81.25%] [G loss: 0.643301]\n",
      "epoch:3 step:3141 [D loss: 0.150083, acc.: 80.47%] [G loss: 0.614310]\n",
      "epoch:3 step:3142 [D loss: 0.122181, acc.: 86.72%] [G loss: 0.649262]\n",
      "epoch:3 step:3143 [D loss: 0.151509, acc.: 82.03%] [G loss: 0.615708]\n",
      "epoch:3 step:3144 [D loss: 0.129772, acc.: 82.03%] [G loss: 0.683324]\n",
      "epoch:3 step:3145 [D loss: 0.180685, acc.: 73.44%] [G loss: 0.622807]\n",
      "epoch:3 step:3146 [D loss: 0.126726, acc.: 82.03%] [G loss: 0.678282]\n",
      "epoch:3 step:3147 [D loss: 0.154923, acc.: 81.25%] [G loss: 0.613651]\n",
      "epoch:3 step:3148 [D loss: 0.136687, acc.: 83.59%] [G loss: 0.664792]\n",
      "epoch:3 step:3149 [D loss: 0.172157, acc.: 78.12%] [G loss: 0.629347]\n",
      "epoch:3 step:3150 [D loss: 0.157773, acc.: 82.03%] [G loss: 0.686219]\n",
      "epoch:3 step:3151 [D loss: 0.157030, acc.: 78.91%] [G loss: 0.634127]\n",
      "epoch:3 step:3152 [D loss: 0.196627, acc.: 73.44%] [G loss: 0.575525]\n",
      "epoch:3 step:3153 [D loss: 0.136799, acc.: 79.69%] [G loss: 0.638730]\n",
      "epoch:3 step:3154 [D loss: 0.140518, acc.: 77.34%] [G loss: 0.730755]\n",
      "epoch:3 step:3155 [D loss: 0.122540, acc.: 85.94%] [G loss: 0.703315]\n",
      "epoch:3 step:3156 [D loss: 0.173568, acc.: 73.44%] [G loss: 0.655548]\n",
      "epoch:3 step:3157 [D loss: 0.147815, acc.: 81.25%] [G loss: 0.680454]\n",
      "epoch:3 step:3158 [D loss: 0.120621, acc.: 84.38%] [G loss: 0.778797]\n",
      "epoch:3 step:3159 [D loss: 0.242016, acc.: 64.84%] [G loss: 0.576327]\n",
      "epoch:3 step:3160 [D loss: 0.213956, acc.: 68.75%] [G loss: 0.528974]\n",
      "epoch:3 step:3161 [D loss: 0.150220, acc.: 83.59%] [G loss: 0.630623]\n",
      "epoch:3 step:3162 [D loss: 0.135274, acc.: 84.38%] [G loss: 0.713744]\n",
      "epoch:3 step:3163 [D loss: 0.170940, acc.: 76.56%] [G loss: 0.591029]\n",
      "epoch:3 step:3164 [D loss: 0.141824, acc.: 80.47%] [G loss: 0.622989]\n",
      "epoch:3 step:3165 [D loss: 0.154879, acc.: 76.56%] [G loss: 0.685733]\n",
      "epoch:3 step:3166 [D loss: 0.192594, acc.: 70.31%] [G loss: 0.635530]\n",
      "epoch:3 step:3167 [D loss: 0.179318, acc.: 75.00%] [G loss: 0.622961]\n",
      "epoch:3 step:3168 [D loss: 0.163330, acc.: 74.22%] [G loss: 0.595214]\n",
      "epoch:3 step:3169 [D loss: 0.151766, acc.: 80.47%] [G loss: 0.725730]\n",
      "epoch:3 step:3170 [D loss: 0.188538, acc.: 74.22%] [G loss: 0.664335]\n",
      "epoch:3 step:3171 [D loss: 0.139907, acc.: 77.34%] [G loss: 0.702546]\n",
      "epoch:3 step:3172 [D loss: 0.163361, acc.: 78.12%] [G loss: 0.641417]\n",
      "epoch:3 step:3173 [D loss: 0.188316, acc.: 73.44%] [G loss: 0.597677]\n",
      "epoch:3 step:3174 [D loss: 0.141205, acc.: 81.25%] [G loss: 0.653711]\n",
      "epoch:3 step:3175 [D loss: 0.146799, acc.: 77.34%] [G loss: 0.668886]\n",
      "epoch:3 step:3176 [D loss: 0.145423, acc.: 80.47%] [G loss: 0.620066]\n",
      "epoch:3 step:3177 [D loss: 0.160150, acc.: 78.91%] [G loss: 0.611739]\n",
      "epoch:3 step:3178 [D loss: 0.158698, acc.: 74.22%] [G loss: 0.640892]\n",
      "epoch:3 step:3179 [D loss: 0.167843, acc.: 77.34%] [G loss: 0.557751]\n",
      "epoch:3 step:3180 [D loss: 0.181289, acc.: 73.44%] [G loss: 0.572767]\n",
      "epoch:3 step:3181 [D loss: 0.166966, acc.: 76.56%] [G loss: 0.702289]\n",
      "epoch:3 step:3182 [D loss: 0.150381, acc.: 79.69%] [G loss: 0.696741]\n",
      "epoch:3 step:3183 [D loss: 0.180642, acc.: 70.31%] [G loss: 0.614918]\n",
      "epoch:3 step:3184 [D loss: 0.215592, acc.: 67.19%] [G loss: 0.628431]\n",
      "epoch:3 step:3185 [D loss: 0.135781, acc.: 84.38%] [G loss: 0.639732]\n",
      "epoch:3 step:3186 [D loss: 0.230151, acc.: 63.28%] [G loss: 0.592823]\n",
      "epoch:3 step:3187 [D loss: 0.229524, acc.: 60.94%] [G loss: 0.545898]\n",
      "epoch:3 step:3188 [D loss: 0.169815, acc.: 75.78%] [G loss: 0.581714]\n",
      "epoch:3 step:3189 [D loss: 0.145758, acc.: 80.47%] [G loss: 0.656720]\n",
      "epoch:3 step:3190 [D loss: 0.161962, acc.: 80.47%] [G loss: 0.691173]\n",
      "epoch:3 step:3191 [D loss: 0.195576, acc.: 71.09%] [G loss: 0.592027]\n",
      "epoch:3 step:3192 [D loss: 0.164784, acc.: 77.34%] [G loss: 0.561799]\n",
      "epoch:3 step:3193 [D loss: 0.159538, acc.: 78.91%] [G loss: 0.646835]\n",
      "epoch:3 step:3194 [D loss: 0.209809, acc.: 66.41%] [G loss: 0.620731]\n",
      "epoch:3 step:3195 [D loss: 0.182001, acc.: 70.31%] [G loss: 0.603687]\n",
      "epoch:3 step:3196 [D loss: 0.181749, acc.: 76.56%] [G loss: 0.607617]\n",
      "epoch:3 step:3197 [D loss: 0.189148, acc.: 76.56%] [G loss: 0.608709]\n",
      "epoch:3 step:3198 [D loss: 0.197436, acc.: 74.22%] [G loss: 0.600031]\n",
      "epoch:3 step:3199 [D loss: 0.177493, acc.: 72.66%] [G loss: 0.666332]\n",
      "epoch:3 step:3200 [D loss: 0.190711, acc.: 72.66%] [G loss: 0.598569]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.588795\n",
      "FID: 51.915409\n",
      "0 = 13.332756963682186\n",
      "1 = 0.08757198642312111\n",
      "2 = 0.9858499765396118\n",
      "3 = 0.9718000292778015\n",
      "4 = 0.9998999834060669\n",
      "5 = 0.9998971223831177\n",
      "6 = 0.9718000292778015\n",
      "7 = 9.785912238574\n",
      "8 = 0.15535095966731477\n",
      "9 = 0.8790000081062317\n",
      "10 = 0.8544999957084656\n",
      "11 = 0.9035000205039978\n",
      "12 = 0.8985278606414795\n",
      "13 = 0.8544999957084656\n",
      "14 = 5.588834285736084\n",
      "15 = 8.752375602722168\n",
      "16 = 0.2847013771533966\n",
      "17 = 5.588794708251953\n",
      "18 = 51.915409088134766\n",
      "epoch:3 step:3201 [D loss: 0.194835, acc.: 71.88%] [G loss: 0.609011]\n",
      "epoch:3 step:3202 [D loss: 0.201858, acc.: 67.19%] [G loss: 0.610162]\n",
      "epoch:3 step:3203 [D loss: 0.151736, acc.: 81.25%] [G loss: 0.670776]\n",
      "epoch:3 step:3204 [D loss: 0.184832, acc.: 72.66%] [G loss: 0.609101]\n",
      "epoch:3 step:3205 [D loss: 0.173570, acc.: 76.56%] [G loss: 0.605601]\n",
      "epoch:3 step:3206 [D loss: 0.168626, acc.: 75.78%] [G loss: 0.601986]\n",
      "epoch:3 step:3207 [D loss: 0.172490, acc.: 74.22%] [G loss: 0.637207]\n",
      "epoch:3 step:3208 [D loss: 0.134943, acc.: 82.81%] [G loss: 0.681839]\n",
      "epoch:3 step:3209 [D loss: 0.119400, acc.: 85.94%] [G loss: 0.738810]\n",
      "epoch:3 step:3210 [D loss: 0.122872, acc.: 86.72%] [G loss: 0.721474]\n",
      "epoch:3 step:3211 [D loss: 0.195092, acc.: 68.75%] [G loss: 0.588693]\n",
      "epoch:3 step:3212 [D loss: 0.195099, acc.: 74.22%] [G loss: 0.625857]\n",
      "epoch:3 step:3213 [D loss: 0.124593, acc.: 85.16%] [G loss: 0.671735]\n",
      "epoch:3 step:3214 [D loss: 0.193525, acc.: 75.00%] [G loss: 0.585792]\n",
      "epoch:3 step:3215 [D loss: 0.227173, acc.: 64.06%] [G loss: 0.538000]\n",
      "epoch:3 step:3216 [D loss: 0.172701, acc.: 72.66%] [G loss: 0.621566]\n",
      "epoch:3 step:3217 [D loss: 0.160382, acc.: 78.91%] [G loss: 0.665488]\n",
      "epoch:3 step:3218 [D loss: 0.179776, acc.: 76.56%] [G loss: 0.644811]\n",
      "epoch:3 step:3219 [D loss: 0.181497, acc.: 67.97%] [G loss: 0.628847]\n",
      "epoch:3 step:3220 [D loss: 0.166597, acc.: 77.34%] [G loss: 0.637270]\n",
      "epoch:3 step:3221 [D loss: 0.206069, acc.: 67.19%] [G loss: 0.601231]\n",
      "epoch:3 step:3222 [D loss: 0.183202, acc.: 73.44%] [G loss: 0.669275]\n",
      "epoch:3 step:3223 [D loss: 0.178580, acc.: 71.88%] [G loss: 0.661914]\n",
      "epoch:3 step:3224 [D loss: 0.199297, acc.: 71.09%] [G loss: 0.613148]\n",
      "epoch:3 step:3225 [D loss: 0.182833, acc.: 73.44%] [G loss: 0.611966]\n",
      "epoch:3 step:3226 [D loss: 0.171693, acc.: 75.00%] [G loss: 0.649799]\n",
      "epoch:3 step:3227 [D loss: 0.170901, acc.: 76.56%] [G loss: 0.661478]\n",
      "epoch:3 step:3228 [D loss: 0.197081, acc.: 68.75%] [G loss: 0.578748]\n",
      "epoch:3 step:3229 [D loss: 0.214402, acc.: 65.62%] [G loss: 0.549228]\n",
      "epoch:3 step:3230 [D loss: 0.179155, acc.: 75.78%] [G loss: 0.615327]\n",
      "epoch:3 step:3231 [D loss: 0.201996, acc.: 65.62%] [G loss: 0.613181]\n",
      "epoch:3 step:3232 [D loss: 0.193489, acc.: 72.66%] [G loss: 0.597619]\n",
      "epoch:3 step:3233 [D loss: 0.166817, acc.: 74.22%] [G loss: 0.622563]\n",
      "epoch:3 step:3234 [D loss: 0.192585, acc.: 75.00%] [G loss: 0.613431]\n",
      "epoch:3 step:3235 [D loss: 0.187214, acc.: 71.88%] [G loss: 0.552779]\n",
      "epoch:3 step:3236 [D loss: 0.177439, acc.: 75.00%] [G loss: 0.618589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3237 [D loss: 0.159074, acc.: 79.69%] [G loss: 0.679629]\n",
      "epoch:3 step:3238 [D loss: 0.127069, acc.: 85.94%] [G loss: 0.773680]\n",
      "epoch:3 step:3239 [D loss: 0.145948, acc.: 80.47%] [G loss: 0.637081]\n",
      "epoch:3 step:3240 [D loss: 0.142117, acc.: 82.81%] [G loss: 0.690861]\n",
      "epoch:3 step:3241 [D loss: 0.145018, acc.: 76.56%] [G loss: 0.707560]\n",
      "epoch:3 step:3242 [D loss: 0.170209, acc.: 77.34%] [G loss: 0.642890]\n",
      "epoch:3 step:3243 [D loss: 0.216767, acc.: 64.84%] [G loss: 0.572384]\n",
      "epoch:3 step:3244 [D loss: 0.172265, acc.: 74.22%] [G loss: 0.604734]\n",
      "epoch:3 step:3245 [D loss: 0.180165, acc.: 74.22%] [G loss: 0.641504]\n",
      "epoch:3 step:3246 [D loss: 0.178855, acc.: 81.25%] [G loss: 0.566027]\n",
      "epoch:3 step:3247 [D loss: 0.157067, acc.: 78.91%] [G loss: 0.610729]\n",
      "epoch:3 step:3248 [D loss: 0.293701, acc.: 57.03%] [G loss: 0.638777]\n",
      "epoch:3 step:3249 [D loss: 0.199183, acc.: 67.97%] [G loss: 0.651144]\n",
      "epoch:3 step:3250 [D loss: 0.182717, acc.: 72.66%] [G loss: 0.604850]\n",
      "epoch:3 step:3251 [D loss: 0.153511, acc.: 77.34%] [G loss: 0.634534]\n",
      "epoch:3 step:3252 [D loss: 0.202359, acc.: 68.75%] [G loss: 0.614676]\n",
      "epoch:3 step:3253 [D loss: 0.200915, acc.: 64.84%] [G loss: 0.562146]\n",
      "epoch:3 step:3254 [D loss: 0.168306, acc.: 78.91%] [G loss: 0.668195]\n",
      "epoch:3 step:3255 [D loss: 0.180209, acc.: 71.88%] [G loss: 0.666535]\n",
      "epoch:3 step:3256 [D loss: 0.164520, acc.: 75.78%] [G loss: 0.604976]\n",
      "epoch:3 step:3257 [D loss: 0.202710, acc.: 71.09%] [G loss: 0.646511]\n",
      "epoch:3 step:3258 [D loss: 0.178296, acc.: 74.22%] [G loss: 0.674089]\n",
      "epoch:3 step:3259 [D loss: 0.204848, acc.: 71.09%] [G loss: 0.578605]\n",
      "epoch:3 step:3260 [D loss: 0.171379, acc.: 74.22%] [G loss: 0.657275]\n",
      "epoch:3 step:3261 [D loss: 0.145452, acc.: 79.69%] [G loss: 0.661297]\n",
      "epoch:3 step:3262 [D loss: 0.136323, acc.: 80.47%] [G loss: 0.676023]\n",
      "epoch:3 step:3263 [D loss: 0.172133, acc.: 76.56%] [G loss: 0.630761]\n",
      "epoch:3 step:3264 [D loss: 0.172677, acc.: 72.66%] [G loss: 0.653075]\n",
      "epoch:3 step:3265 [D loss: 0.173785, acc.: 78.12%] [G loss: 0.596334]\n",
      "epoch:3 step:3266 [D loss: 0.179607, acc.: 77.34%] [G loss: 0.565788]\n",
      "epoch:3 step:3267 [D loss: 0.187929, acc.: 66.41%] [G loss: 0.586145]\n",
      "epoch:3 step:3268 [D loss: 0.166850, acc.: 78.91%] [G loss: 0.606349]\n",
      "epoch:3 step:3269 [D loss: 0.205890, acc.: 64.06%] [G loss: 0.589932]\n",
      "epoch:3 step:3270 [D loss: 0.170552, acc.: 75.78%] [G loss: 0.610578]\n",
      "epoch:3 step:3271 [D loss: 0.180105, acc.: 71.88%] [G loss: 0.599243]\n",
      "epoch:3 step:3272 [D loss: 0.184248, acc.: 70.31%] [G loss: 0.583200]\n",
      "epoch:3 step:3273 [D loss: 0.158414, acc.: 80.47%] [G loss: 0.635782]\n",
      "epoch:3 step:3274 [D loss: 0.169318, acc.: 78.12%] [G loss: 0.604803]\n",
      "epoch:3 step:3275 [D loss: 0.157937, acc.: 79.69%] [G loss: 0.577254]\n",
      "epoch:3 step:3276 [D loss: 0.204096, acc.: 67.19%] [G loss: 0.582326]\n",
      "epoch:3 step:3277 [D loss: 0.168853, acc.: 76.56%] [G loss: 0.629588]\n",
      "epoch:3 step:3278 [D loss: 0.176247, acc.: 75.78%] [G loss: 0.616751]\n",
      "epoch:3 step:3279 [D loss: 0.197501, acc.: 70.31%] [G loss: 0.584287]\n",
      "epoch:3 step:3280 [D loss: 0.183068, acc.: 74.22%] [G loss: 0.646356]\n",
      "epoch:3 step:3281 [D loss: 0.178781, acc.: 71.88%] [G loss: 0.635519]\n",
      "epoch:3 step:3282 [D loss: 0.145710, acc.: 82.03%] [G loss: 0.660134]\n",
      "epoch:3 step:3283 [D loss: 0.124212, acc.: 85.16%] [G loss: 0.725297]\n",
      "epoch:3 step:3284 [D loss: 0.208381, acc.: 68.75%] [G loss: 0.596405]\n",
      "epoch:3 step:3285 [D loss: 0.145625, acc.: 78.12%] [G loss: 0.700580]\n",
      "epoch:3 step:3286 [D loss: 0.157772, acc.: 74.22%] [G loss: 0.680974]\n",
      "epoch:3 step:3287 [D loss: 0.221154, acc.: 67.97%] [G loss: 0.572244]\n",
      "epoch:3 step:3288 [D loss: 0.223311, acc.: 63.28%] [G loss: 0.520072]\n",
      "epoch:3 step:3289 [D loss: 0.197041, acc.: 70.31%] [G loss: 0.571663]\n",
      "epoch:3 step:3290 [D loss: 0.178015, acc.: 78.12%] [G loss: 0.578199]\n",
      "epoch:3 step:3291 [D loss: 0.174325, acc.: 71.88%] [G loss: 0.554443]\n",
      "epoch:3 step:3292 [D loss: 0.190788, acc.: 68.75%] [G loss: 0.637571]\n",
      "epoch:3 step:3293 [D loss: 0.264684, acc.: 57.03%] [G loss: 0.517371]\n",
      "epoch:3 step:3294 [D loss: 0.201992, acc.: 65.62%] [G loss: 0.595393]\n",
      "epoch:3 step:3295 [D loss: 0.158106, acc.: 77.34%] [G loss: 0.678392]\n",
      "epoch:3 step:3296 [D loss: 0.168842, acc.: 75.00%] [G loss: 0.669433]\n",
      "epoch:3 step:3297 [D loss: 0.209927, acc.: 67.97%] [G loss: 0.632144]\n",
      "epoch:3 step:3298 [D loss: 0.198274, acc.: 72.66%] [G loss: 0.656891]\n",
      "epoch:3 step:3299 [D loss: 0.159051, acc.: 78.12%] [G loss: 0.636723]\n",
      "epoch:3 step:3300 [D loss: 0.174171, acc.: 76.56%] [G loss: 0.605227]\n",
      "epoch:3 step:3301 [D loss: 0.203659, acc.: 67.19%] [G loss: 0.622093]\n",
      "epoch:3 step:3302 [D loss: 0.184308, acc.: 75.00%] [G loss: 0.632599]\n",
      "epoch:3 step:3303 [D loss: 0.194806, acc.: 72.66%] [G loss: 0.624333]\n",
      "epoch:3 step:3304 [D loss: 0.198752, acc.: 73.44%] [G loss: 0.559627]\n",
      "epoch:3 step:3305 [D loss: 0.180975, acc.: 71.88%] [G loss: 0.598191]\n",
      "epoch:3 step:3306 [D loss: 0.152515, acc.: 82.03%] [G loss: 0.676848]\n",
      "epoch:3 step:3307 [D loss: 0.179273, acc.: 70.31%] [G loss: 0.613259]\n",
      "epoch:3 step:3308 [D loss: 0.127876, acc.: 83.59%] [G loss: 0.679616]\n",
      "epoch:3 step:3309 [D loss: 0.136185, acc.: 80.47%] [G loss: 0.666014]\n",
      "epoch:3 step:3310 [D loss: 0.160271, acc.: 75.78%] [G loss: 0.703886]\n",
      "epoch:3 step:3311 [D loss: 0.216569, acc.: 63.28%] [G loss: 0.564818]\n",
      "epoch:3 step:3312 [D loss: 0.217760, acc.: 64.06%] [G loss: 0.555952]\n",
      "epoch:3 step:3313 [D loss: 0.177474, acc.: 72.66%] [G loss: 0.621609]\n",
      "epoch:3 step:3314 [D loss: 0.137382, acc.: 82.81%] [G loss: 0.660821]\n",
      "epoch:3 step:3315 [D loss: 0.148501, acc.: 80.47%] [G loss: 0.639861]\n",
      "epoch:3 step:3316 [D loss: 0.196831, acc.: 69.53%] [G loss: 0.590816]\n",
      "epoch:3 step:3317 [D loss: 0.194368, acc.: 71.09%] [G loss: 0.606133]\n",
      "epoch:3 step:3318 [D loss: 0.171775, acc.: 71.88%] [G loss: 0.576616]\n",
      "epoch:3 step:3319 [D loss: 0.155219, acc.: 78.12%] [G loss: 0.679709]\n",
      "epoch:3 step:3320 [D loss: 0.192917, acc.: 72.66%] [G loss: 0.580696]\n",
      "epoch:3 step:3321 [D loss: 0.190643, acc.: 70.31%] [G loss: 0.616061]\n",
      "epoch:3 step:3322 [D loss: 0.203188, acc.: 73.44%] [G loss: 0.551545]\n",
      "epoch:3 step:3323 [D loss: 0.206166, acc.: 69.53%] [G loss: 0.621228]\n",
      "epoch:3 step:3324 [D loss: 0.145128, acc.: 83.59%] [G loss: 0.667720]\n",
      "epoch:3 step:3325 [D loss: 0.167362, acc.: 76.56%] [G loss: 0.640528]\n",
      "epoch:3 step:3326 [D loss: 0.146222, acc.: 81.25%] [G loss: 0.655229]\n",
      "epoch:3 step:3327 [D loss: 0.166837, acc.: 73.44%] [G loss: 0.625278]\n",
      "epoch:3 step:3328 [D loss: 0.202023, acc.: 67.19%] [G loss: 0.539595]\n",
      "epoch:3 step:3329 [D loss: 0.158477, acc.: 78.12%] [G loss: 0.600164]\n",
      "epoch:3 step:3330 [D loss: 0.159607, acc.: 77.34%] [G loss: 0.652398]\n",
      "epoch:3 step:3331 [D loss: 0.147877, acc.: 78.91%] [G loss: 0.637498]\n",
      "epoch:3 step:3332 [D loss: 0.156337, acc.: 78.12%] [G loss: 0.614613]\n",
      "epoch:3 step:3333 [D loss: 0.163580, acc.: 73.44%] [G loss: 0.608925]\n",
      "epoch:3 step:3334 [D loss: 0.145803, acc.: 79.69%] [G loss: 0.605684]\n",
      "epoch:3 step:3335 [D loss: 0.185987, acc.: 72.66%] [G loss: 0.572973]\n",
      "epoch:3 step:3336 [D loss: 0.185655, acc.: 75.78%] [G loss: 0.534455]\n",
      "epoch:3 step:3337 [D loss: 0.198555, acc.: 68.75%] [G loss: 0.539686]\n",
      "epoch:3 step:3338 [D loss: 0.172961, acc.: 72.66%] [G loss: 0.608511]\n",
      "epoch:3 step:3339 [D loss: 0.225042, acc.: 64.06%] [G loss: 0.504965]\n",
      "epoch:3 step:3340 [D loss: 0.164806, acc.: 77.34%] [G loss: 0.588711]\n",
      "epoch:3 step:3341 [D loss: 0.168196, acc.: 75.78%] [G loss: 0.567643]\n",
      "epoch:3 step:3342 [D loss: 0.203399, acc.: 68.75%] [G loss: 0.641198]\n",
      "epoch:3 step:3343 [D loss: 0.175175, acc.: 75.78%] [G loss: 0.641914]\n",
      "epoch:3 step:3344 [D loss: 0.177238, acc.: 73.44%] [G loss: 0.613348]\n",
      "epoch:3 step:3345 [D loss: 0.142235, acc.: 84.38%] [G loss: 0.668723]\n",
      "epoch:3 step:3346 [D loss: 0.189793, acc.: 76.56%] [G loss: 0.597241]\n",
      "epoch:3 step:3347 [D loss: 0.156887, acc.: 79.69%] [G loss: 0.616510]\n",
      "epoch:3 step:3348 [D loss: 0.136410, acc.: 84.38%] [G loss: 0.646089]\n",
      "epoch:3 step:3349 [D loss: 0.204082, acc.: 67.19%] [G loss: 0.584405]\n",
      "epoch:3 step:3350 [D loss: 0.167371, acc.: 75.78%] [G loss: 0.579032]\n",
      "epoch:3 step:3351 [D loss: 0.189980, acc.: 70.31%] [G loss: 0.615804]\n",
      "epoch:3 step:3352 [D loss: 0.154990, acc.: 74.22%] [G loss: 0.627012]\n",
      "epoch:3 step:3353 [D loss: 0.199375, acc.: 68.75%] [G loss: 0.592622]\n",
      "epoch:3 step:3354 [D loss: 0.183537, acc.: 73.44%] [G loss: 0.572175]\n",
      "epoch:3 step:3355 [D loss: 0.175509, acc.: 74.22%] [G loss: 0.564977]\n",
      "epoch:3 step:3356 [D loss: 0.177488, acc.: 72.66%] [G loss: 0.592756]\n",
      "epoch:3 step:3357 [D loss: 0.149778, acc.: 78.91%] [G loss: 0.655032]\n",
      "epoch:3 step:3358 [D loss: 0.156174, acc.: 79.69%] [G loss: 0.621704]\n",
      "epoch:3 step:3359 [D loss: 0.157752, acc.: 71.88%] [G loss: 0.622368]\n",
      "epoch:3 step:3360 [D loss: 0.159266, acc.: 76.56%] [G loss: 0.627516]\n",
      "epoch:3 step:3361 [D loss: 0.151598, acc.: 80.47%] [G loss: 0.643441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3362 [D loss: 0.144487, acc.: 82.03%] [G loss: 0.656753]\n",
      "epoch:3 step:3363 [D loss: 0.126155, acc.: 84.38%] [G loss: 0.641681]\n",
      "epoch:3 step:3364 [D loss: 0.167865, acc.: 73.44%] [G loss: 0.639205]\n",
      "epoch:3 step:3365 [D loss: 0.156106, acc.: 78.12%] [G loss: 0.630675]\n",
      "epoch:3 step:3366 [D loss: 0.153757, acc.: 77.34%] [G loss: 0.653025]\n",
      "epoch:3 step:3367 [D loss: 0.155738, acc.: 77.34%] [G loss: 0.614613]\n",
      "epoch:3 step:3368 [D loss: 0.164082, acc.: 76.56%] [G loss: 0.612294]\n",
      "epoch:3 step:3369 [D loss: 0.138579, acc.: 82.03%] [G loss: 0.645968]\n",
      "epoch:3 step:3370 [D loss: 0.224719, acc.: 67.19%] [G loss: 0.581436]\n",
      "epoch:3 step:3371 [D loss: 0.174676, acc.: 74.22%] [G loss: 0.585491]\n",
      "epoch:3 step:3372 [D loss: 0.151503, acc.: 79.69%] [G loss: 0.620402]\n",
      "epoch:3 step:3373 [D loss: 0.212625, acc.: 65.62%] [G loss: 0.616541]\n",
      "epoch:3 step:3374 [D loss: 0.210129, acc.: 70.31%] [G loss: 0.545199]\n",
      "epoch:3 step:3375 [D loss: 0.159497, acc.: 76.56%] [G loss: 0.654955]\n",
      "epoch:3 step:3376 [D loss: 0.235401, acc.: 66.41%] [G loss: 0.591195]\n",
      "epoch:3 step:3377 [D loss: 0.236150, acc.: 63.28%] [G loss: 0.553882]\n",
      "epoch:3 step:3378 [D loss: 0.147711, acc.: 75.78%] [G loss: 0.638923]\n",
      "epoch:3 step:3379 [D loss: 0.148851, acc.: 80.47%] [G loss: 0.691533]\n",
      "epoch:3 step:3380 [D loss: 0.214830, acc.: 66.41%] [G loss: 0.559858]\n",
      "epoch:3 step:3381 [D loss: 0.157725, acc.: 79.69%] [G loss: 0.602008]\n",
      "epoch:3 step:3382 [D loss: 0.159764, acc.: 74.22%] [G loss: 0.634924]\n",
      "epoch:3 step:3383 [D loss: 0.198423, acc.: 65.62%] [G loss: 0.570545]\n",
      "epoch:3 step:3384 [D loss: 0.179095, acc.: 75.00%] [G loss: 0.577263]\n",
      "epoch:3 step:3385 [D loss: 0.138384, acc.: 81.25%] [G loss: 0.612621]\n",
      "epoch:3 step:3386 [D loss: 0.122586, acc.: 84.38%] [G loss: 0.651396]\n",
      "epoch:3 step:3387 [D loss: 0.199331, acc.: 69.53%] [G loss: 0.595294]\n",
      "epoch:3 step:3388 [D loss: 0.165178, acc.: 78.12%] [G loss: 0.621547]\n",
      "epoch:3 step:3389 [D loss: 0.205700, acc.: 64.84%] [G loss: 0.586426]\n",
      "epoch:3 step:3390 [D loss: 0.159092, acc.: 76.56%] [G loss: 0.693804]\n",
      "epoch:3 step:3391 [D loss: 0.205600, acc.: 69.53%] [G loss: 0.604911]\n",
      "epoch:3 step:3392 [D loss: 0.168309, acc.: 75.00%] [G loss: 0.641704]\n",
      "epoch:3 step:3393 [D loss: 0.148947, acc.: 79.69%] [G loss: 0.637008]\n",
      "epoch:3 step:3394 [D loss: 0.189411, acc.: 71.88%] [G loss: 0.594745]\n",
      "epoch:3 step:3395 [D loss: 0.202966, acc.: 67.97%] [G loss: 0.620594]\n",
      "epoch:3 step:3396 [D loss: 0.164777, acc.: 78.91%] [G loss: 0.610813]\n",
      "epoch:3 step:3397 [D loss: 0.185121, acc.: 70.31%] [G loss: 0.591403]\n",
      "epoch:3 step:3398 [D loss: 0.174608, acc.: 74.22%] [G loss: 0.613528]\n",
      "epoch:3 step:3399 [D loss: 0.175279, acc.: 75.00%] [G loss: 0.605892]\n",
      "epoch:3 step:3400 [D loss: 0.171258, acc.: 72.66%] [G loss: 0.668119]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.644750\n",
      "FID: 50.918530\n",
      "0 = 12.978386095285423\n",
      "1 = 0.07637739576712131\n",
      "2 = 0.9796500205993652\n",
      "3 = 0.9603999853134155\n",
      "4 = 0.9988999962806702\n",
      "5 = 0.9988559484481812\n",
      "6 = 0.9603999853134155\n",
      "7 = 9.716387619316613\n",
      "8 = 0.15222593789465616\n",
      "9 = 0.8676000237464905\n",
      "10 = 0.84579998254776\n",
      "11 = 0.8894000053405762\n",
      "12 = 0.8843579888343811\n",
      "13 = 0.84579998254776\n",
      "14 = 5.644792079925537\n",
      "15 = 8.769582748413086\n",
      "16 = 0.27123793959617615\n",
      "17 = 5.644750118255615\n",
      "18 = 50.91852951049805\n",
      "epoch:3 step:3401 [D loss: 0.194443, acc.: 71.09%] [G loss: 0.573907]\n",
      "epoch:3 step:3402 [D loss: 0.161833, acc.: 75.78%] [G loss: 0.621295]\n",
      "epoch:3 step:3403 [D loss: 0.156823, acc.: 75.00%] [G loss: 0.632919]\n",
      "epoch:3 step:3404 [D loss: 0.200461, acc.: 67.97%] [G loss: 0.562266]\n",
      "epoch:3 step:3405 [D loss: 0.176817, acc.: 71.09%] [G loss: 0.564993]\n",
      "epoch:3 step:3406 [D loss: 0.183515, acc.: 68.75%] [G loss: 0.560326]\n",
      "epoch:3 step:3407 [D loss: 0.176612, acc.: 75.78%] [G loss: 0.608223]\n",
      "epoch:3 step:3408 [D loss: 0.178972, acc.: 75.78%] [G loss: 0.562016]\n",
      "epoch:3 step:3409 [D loss: 0.183488, acc.: 75.78%] [G loss: 0.604798]\n",
      "epoch:3 step:3410 [D loss: 0.170985, acc.: 71.88%] [G loss: 0.669078]\n",
      "epoch:3 step:3411 [D loss: 0.221613, acc.: 68.75%] [G loss: 0.552356]\n",
      "epoch:3 step:3412 [D loss: 0.183996, acc.: 70.31%] [G loss: 0.549492]\n",
      "epoch:3 step:3413 [D loss: 0.151707, acc.: 83.59%] [G loss: 0.650771]\n",
      "epoch:3 step:3414 [D loss: 0.169387, acc.: 73.44%] [G loss: 0.608783]\n",
      "epoch:3 step:3415 [D loss: 0.180997, acc.: 75.78%] [G loss: 0.583573]\n",
      "epoch:3 step:3416 [D loss: 0.168843, acc.: 73.44%] [G loss: 0.582394]\n",
      "epoch:3 step:3417 [D loss: 0.176432, acc.: 78.12%] [G loss: 0.578936]\n",
      "epoch:3 step:3418 [D loss: 0.170734, acc.: 73.44%] [G loss: 0.596298]\n",
      "epoch:3 step:3419 [D loss: 0.157809, acc.: 81.25%] [G loss: 0.613594]\n",
      "epoch:3 step:3420 [D loss: 0.168578, acc.: 77.34%] [G loss: 0.624834]\n",
      "epoch:3 step:3421 [D loss: 0.181142, acc.: 71.88%] [G loss: 0.598756]\n",
      "epoch:3 step:3422 [D loss: 0.139360, acc.: 86.72%] [G loss: 0.586755]\n",
      "epoch:3 step:3423 [D loss: 0.170579, acc.: 78.91%] [G loss: 0.624549]\n",
      "epoch:3 step:3424 [D loss: 0.142691, acc.: 82.81%] [G loss: 0.650259]\n",
      "epoch:3 step:3425 [D loss: 0.184119, acc.: 73.44%] [G loss: 0.582216]\n",
      "epoch:3 step:3426 [D loss: 0.204602, acc.: 68.75%] [G loss: 0.579401]\n",
      "epoch:3 step:3427 [D loss: 0.170039, acc.: 75.78%] [G loss: 0.680704]\n",
      "epoch:3 step:3428 [D loss: 0.194339, acc.: 72.66%] [G loss: 0.573347]\n",
      "epoch:3 step:3429 [D loss: 0.198734, acc.: 68.75%] [G loss: 0.606705]\n",
      "epoch:3 step:3430 [D loss: 0.151891, acc.: 78.91%] [G loss: 0.589278]\n",
      "epoch:3 step:3431 [D loss: 0.174304, acc.: 79.69%] [G loss: 0.573391]\n",
      "epoch:3 step:3432 [D loss: 0.181995, acc.: 75.78%] [G loss: 0.605055]\n",
      "epoch:3 step:3433 [D loss: 0.221611, acc.: 64.06%] [G loss: 0.541729]\n",
      "epoch:3 step:3434 [D loss: 0.189519, acc.: 71.88%] [G loss: 0.602342]\n",
      "epoch:3 step:3435 [D loss: 0.152967, acc.: 79.69%] [G loss: 0.698538]\n",
      "epoch:3 step:3436 [D loss: 0.206662, acc.: 70.31%] [G loss: 0.556949]\n",
      "epoch:3 step:3437 [D loss: 0.195014, acc.: 67.97%] [G loss: 0.566197]\n",
      "epoch:3 step:3438 [D loss: 0.195024, acc.: 68.75%] [G loss: 0.613234]\n",
      "epoch:3 step:3439 [D loss: 0.185301, acc.: 70.31%] [G loss: 0.604045]\n",
      "epoch:3 step:3440 [D loss: 0.170113, acc.: 75.78%] [G loss: 0.580975]\n",
      "epoch:3 step:3441 [D loss: 0.165335, acc.: 78.12%] [G loss: 0.587591]\n",
      "epoch:3 step:3442 [D loss: 0.156787, acc.: 80.47%] [G loss: 0.646562]\n",
      "epoch:3 step:3443 [D loss: 0.172077, acc.: 71.09%] [G loss: 0.629684]\n",
      "epoch:3 step:3444 [D loss: 0.191083, acc.: 71.09%] [G loss: 0.616036]\n",
      "epoch:3 step:3445 [D loss: 0.154863, acc.: 80.47%] [G loss: 0.694130]\n",
      "epoch:3 step:3446 [D loss: 0.167079, acc.: 77.34%] [G loss: 0.609161]\n",
      "epoch:3 step:3447 [D loss: 0.210940, acc.: 68.75%] [G loss: 0.547479]\n",
      "epoch:3 step:3448 [D loss: 0.156298, acc.: 81.25%] [G loss: 0.663581]\n",
      "epoch:3 step:3449 [D loss: 0.182493, acc.: 69.53%] [G loss: 0.570100]\n",
      "epoch:3 step:3450 [D loss: 0.142800, acc.: 83.59%] [G loss: 0.656342]\n",
      "epoch:3 step:3451 [D loss: 0.148125, acc.: 79.69%] [G loss: 0.611076]\n",
      "epoch:3 step:3452 [D loss: 0.152102, acc.: 77.34%] [G loss: 0.635827]\n",
      "epoch:3 step:3453 [D loss: 0.118581, acc.: 85.94%] [G loss: 0.700883]\n",
      "epoch:3 step:3454 [D loss: 0.210550, acc.: 69.53%] [G loss: 0.607587]\n",
      "epoch:3 step:3455 [D loss: 0.195403, acc.: 71.09%] [G loss: 0.595500]\n",
      "epoch:3 step:3456 [D loss: 0.200403, acc.: 67.97%] [G loss: 0.571253]\n",
      "epoch:3 step:3457 [D loss: 0.220285, acc.: 60.94%] [G loss: 0.582348]\n",
      "epoch:3 step:3458 [D loss: 0.167283, acc.: 75.78%] [G loss: 0.666889]\n",
      "epoch:3 step:3459 [D loss: 0.151375, acc.: 80.47%] [G loss: 0.733501]\n",
      "epoch:3 step:3460 [D loss: 0.168483, acc.: 75.78%] [G loss: 0.692336]\n",
      "epoch:3 step:3461 [D loss: 0.180547, acc.: 70.31%] [G loss: 0.623365]\n",
      "epoch:3 step:3462 [D loss: 0.166415, acc.: 75.00%] [G loss: 0.599926]\n",
      "epoch:3 step:3463 [D loss: 0.210562, acc.: 66.41%] [G loss: 0.583068]\n",
      "epoch:3 step:3464 [D loss: 0.194576, acc.: 68.75%] [G loss: 0.635339]\n",
      "epoch:3 step:3465 [D loss: 0.191464, acc.: 74.22%] [G loss: 0.631489]\n",
      "epoch:3 step:3466 [D loss: 0.226255, acc.: 64.06%] [G loss: 0.562411]\n",
      "epoch:3 step:3467 [D loss: 0.177603, acc.: 75.78%] [G loss: 0.555162]\n",
      "epoch:3 step:3468 [D loss: 0.193903, acc.: 70.31%] [G loss: 0.591685]\n",
      "epoch:3 step:3469 [D loss: 0.207249, acc.: 67.97%] [G loss: 0.596190]\n",
      "epoch:3 step:3470 [D loss: 0.173975, acc.: 78.91%] [G loss: 0.602080]\n",
      "epoch:3 step:3471 [D loss: 0.134184, acc.: 87.50%] [G loss: 0.608145]\n",
      "epoch:3 step:3472 [D loss: 0.166950, acc.: 77.34%] [G loss: 0.618870]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3473 [D loss: 0.172572, acc.: 76.56%] [G loss: 0.642709]\n",
      "epoch:3 step:3474 [D loss: 0.135845, acc.: 82.03%] [G loss: 0.623087]\n",
      "epoch:3 step:3475 [D loss: 0.169999, acc.: 72.66%] [G loss: 0.656265]\n",
      "epoch:3 step:3476 [D loss: 0.180040, acc.: 71.09%] [G loss: 0.627211]\n",
      "epoch:3 step:3477 [D loss: 0.163119, acc.: 78.12%] [G loss: 0.650229]\n",
      "epoch:3 step:3478 [D loss: 0.165487, acc.: 75.00%] [G loss: 0.583098]\n",
      "epoch:3 step:3479 [D loss: 0.194540, acc.: 71.09%] [G loss: 0.633086]\n",
      "epoch:3 step:3480 [D loss: 0.185903, acc.: 72.66%] [G loss: 0.605765]\n",
      "epoch:3 step:3481 [D loss: 0.164626, acc.: 72.66%] [G loss: 0.624487]\n",
      "epoch:3 step:3482 [D loss: 0.200298, acc.: 69.53%] [G loss: 0.569145]\n",
      "epoch:3 step:3483 [D loss: 0.188477, acc.: 75.00%] [G loss: 0.582724]\n",
      "epoch:3 step:3484 [D loss: 0.218433, acc.: 67.97%] [G loss: 0.624246]\n",
      "epoch:3 step:3485 [D loss: 0.190095, acc.: 72.66%] [G loss: 0.602798]\n",
      "epoch:3 step:3486 [D loss: 0.164283, acc.: 78.91%] [G loss: 0.616661]\n",
      "epoch:3 step:3487 [D loss: 0.165394, acc.: 75.78%] [G loss: 0.591913]\n",
      "epoch:3 step:3488 [D loss: 0.164899, acc.: 75.78%] [G loss: 0.579230]\n",
      "epoch:3 step:3489 [D loss: 0.169357, acc.: 72.66%] [G loss: 0.590905]\n",
      "epoch:3 step:3490 [D loss: 0.161532, acc.: 78.91%] [G loss: 0.610125]\n",
      "epoch:3 step:3491 [D loss: 0.168950, acc.: 75.00%] [G loss: 0.633448]\n",
      "epoch:3 step:3492 [D loss: 0.172446, acc.: 78.12%] [G loss: 0.623585]\n",
      "epoch:3 step:3493 [D loss: 0.189762, acc.: 75.00%] [G loss: 0.595892]\n",
      "epoch:3 step:3494 [D loss: 0.172853, acc.: 75.78%] [G loss: 0.564128]\n",
      "epoch:3 step:3495 [D loss: 0.183244, acc.: 73.44%] [G loss: 0.595862]\n",
      "epoch:3 step:3496 [D loss: 0.189116, acc.: 70.31%] [G loss: 0.590786]\n",
      "epoch:3 step:3497 [D loss: 0.164045, acc.: 78.12%] [G loss: 0.565734]\n",
      "epoch:3 step:3498 [D loss: 0.193290, acc.: 71.09%] [G loss: 0.599148]\n",
      "epoch:3 step:3499 [D loss: 0.181134, acc.: 71.88%] [G loss: 0.601780]\n",
      "epoch:3 step:3500 [D loss: 0.155993, acc.: 79.69%] [G loss: 0.620650]\n",
      "epoch:3 step:3501 [D loss: 0.146600, acc.: 78.91%] [G loss: 0.719919]\n",
      "epoch:3 step:3502 [D loss: 0.196474, acc.: 69.53%] [G loss: 0.671052]\n",
      "epoch:3 step:3503 [D loss: 0.190564, acc.: 71.88%] [G loss: 0.622607]\n",
      "epoch:3 step:3504 [D loss: 0.179020, acc.: 75.00%] [G loss: 0.618043]\n",
      "epoch:3 step:3505 [D loss: 0.181948, acc.: 71.09%] [G loss: 0.624056]\n",
      "epoch:3 step:3506 [D loss: 0.162013, acc.: 74.22%] [G loss: 0.699308]\n",
      "epoch:3 step:3507 [D loss: 0.191194, acc.: 71.88%] [G loss: 0.580429]\n",
      "epoch:3 step:3508 [D loss: 0.162224, acc.: 79.69%] [G loss: 0.615772]\n",
      "epoch:3 step:3509 [D loss: 0.177691, acc.: 72.66%] [G loss: 0.625243]\n",
      "epoch:3 step:3510 [D loss: 0.197511, acc.: 66.41%] [G loss: 0.595856]\n",
      "epoch:3 step:3511 [D loss: 0.174287, acc.: 75.00%] [G loss: 0.622941]\n",
      "epoch:3 step:3512 [D loss: 0.155430, acc.: 77.34%] [G loss: 0.624452]\n",
      "epoch:3 step:3513 [D loss: 0.191456, acc.: 69.53%] [G loss: 0.627806]\n",
      "epoch:3 step:3514 [D loss: 0.188082, acc.: 72.66%] [G loss: 0.617449]\n",
      "epoch:3 step:3515 [D loss: 0.188977, acc.: 71.88%] [G loss: 0.597469]\n",
      "epoch:3 step:3516 [D loss: 0.162212, acc.: 75.78%] [G loss: 0.617803]\n",
      "epoch:3 step:3517 [D loss: 0.149115, acc.: 80.47%] [G loss: 0.634611]\n",
      "epoch:3 step:3518 [D loss: 0.121940, acc.: 82.03%] [G loss: 0.720785]\n",
      "epoch:3 step:3519 [D loss: 0.175720, acc.: 72.66%] [G loss: 0.643819]\n",
      "epoch:3 step:3520 [D loss: 0.139603, acc.: 82.03%] [G loss: 0.710721]\n",
      "epoch:3 step:3521 [D loss: 0.209957, acc.: 64.84%] [G loss: 0.604866]\n",
      "epoch:3 step:3522 [D loss: 0.215522, acc.: 57.81%] [G loss: 0.566031]\n",
      "epoch:3 step:3523 [D loss: 0.179965, acc.: 74.22%] [G loss: 0.633385]\n",
      "epoch:3 step:3524 [D loss: 0.160348, acc.: 80.47%] [G loss: 0.638481]\n",
      "epoch:3 step:3525 [D loss: 0.151023, acc.: 80.47%] [G loss: 0.622096]\n",
      "epoch:3 step:3526 [D loss: 0.183115, acc.: 74.22%] [G loss: 0.639210]\n",
      "epoch:3 step:3527 [D loss: 0.255555, acc.: 55.47%] [G loss: 0.519060]\n",
      "epoch:3 step:3528 [D loss: 0.194489, acc.: 71.09%] [G loss: 0.587577]\n",
      "epoch:3 step:3529 [D loss: 0.221636, acc.: 64.06%] [G loss: 0.618851]\n",
      "epoch:3 step:3530 [D loss: 0.202135, acc.: 67.97%] [G loss: 0.688564]\n",
      "epoch:3 step:3531 [D loss: 0.218128, acc.: 65.62%] [G loss: 0.602534]\n",
      "epoch:3 step:3532 [D loss: 0.200697, acc.: 71.88%] [G loss: 0.609435]\n",
      "epoch:3 step:3533 [D loss: 0.175684, acc.: 71.88%] [G loss: 0.604579]\n",
      "epoch:3 step:3534 [D loss: 0.179752, acc.: 74.22%] [G loss: 0.616499]\n",
      "epoch:3 step:3535 [D loss: 0.203359, acc.: 68.75%] [G loss: 0.551135]\n",
      "epoch:3 step:3536 [D loss: 0.138407, acc.: 82.03%] [G loss: 0.661426]\n",
      "epoch:3 step:3537 [D loss: 0.170251, acc.: 74.22%] [G loss: 0.612037]\n",
      "epoch:3 step:3538 [D loss: 0.204958, acc.: 66.41%] [G loss: 0.549710]\n",
      "epoch:3 step:3539 [D loss: 0.152201, acc.: 76.56%] [G loss: 0.621052]\n",
      "epoch:3 step:3540 [D loss: 0.192917, acc.: 72.66%] [G loss: 0.577832]\n",
      "epoch:3 step:3541 [D loss: 0.172243, acc.: 75.00%] [G loss: 0.646066]\n",
      "epoch:3 step:3542 [D loss: 0.158829, acc.: 75.78%] [G loss: 0.676119]\n",
      "epoch:3 step:3543 [D loss: 0.168743, acc.: 75.00%] [G loss: 0.623583]\n",
      "epoch:3 step:3544 [D loss: 0.154783, acc.: 77.34%] [G loss: 0.578759]\n",
      "epoch:3 step:3545 [D loss: 0.181270, acc.: 75.00%] [G loss: 0.601436]\n",
      "epoch:3 step:3546 [D loss: 0.184250, acc.: 76.56%] [G loss: 0.583756]\n",
      "epoch:3 step:3547 [D loss: 0.128186, acc.: 82.03%] [G loss: 0.676447]\n",
      "epoch:3 step:3548 [D loss: 0.159555, acc.: 79.69%] [G loss: 0.658082]\n",
      "epoch:3 step:3549 [D loss: 0.198343, acc.: 67.19%] [G loss: 0.621201]\n",
      "epoch:3 step:3550 [D loss: 0.232723, acc.: 63.28%] [G loss: 0.525514]\n",
      "epoch:3 step:3551 [D loss: 0.204102, acc.: 63.28%] [G loss: 0.558426]\n",
      "epoch:3 step:3552 [D loss: 0.157209, acc.: 78.91%] [G loss: 0.694074]\n",
      "epoch:3 step:3553 [D loss: 0.180953, acc.: 72.66%] [G loss: 0.597286]\n",
      "epoch:3 step:3554 [D loss: 0.146864, acc.: 78.12%] [G loss: 0.589512]\n",
      "epoch:3 step:3555 [D loss: 0.165993, acc.: 78.12%] [G loss: 0.601418]\n",
      "epoch:3 step:3556 [D loss: 0.162549, acc.: 72.66%] [G loss: 0.599214]\n",
      "epoch:3 step:3557 [D loss: 0.139414, acc.: 82.03%] [G loss: 0.663059]\n",
      "epoch:3 step:3558 [D loss: 0.140577, acc.: 79.69%] [G loss: 0.678649]\n",
      "epoch:3 step:3559 [D loss: 0.175179, acc.: 71.09%] [G loss: 0.578061]\n",
      "epoch:3 step:3560 [D loss: 0.155264, acc.: 78.12%] [G loss: 0.608023]\n",
      "epoch:3 step:3561 [D loss: 0.159925, acc.: 80.47%] [G loss: 0.625380]\n",
      "epoch:3 step:3562 [D loss: 0.195437, acc.: 72.66%] [G loss: 0.579573]\n",
      "epoch:3 step:3563 [D loss: 0.143568, acc.: 81.25%] [G loss: 0.643421]\n",
      "epoch:3 step:3564 [D loss: 0.148556, acc.: 79.69%] [G loss: 0.619755]\n",
      "epoch:3 step:3565 [D loss: 0.140442, acc.: 82.81%] [G loss: 0.671005]\n",
      "epoch:3 step:3566 [D loss: 0.145617, acc.: 79.69%] [G loss: 0.661090]\n",
      "epoch:3 step:3567 [D loss: 0.172066, acc.: 76.56%] [G loss: 0.590173]\n",
      "epoch:3 step:3568 [D loss: 0.182282, acc.: 73.44%] [G loss: 0.686000]\n",
      "epoch:3 step:3569 [D loss: 0.171378, acc.: 73.44%] [G loss: 0.601627]\n",
      "epoch:3 step:3570 [D loss: 0.193481, acc.: 75.78%] [G loss: 0.564945]\n",
      "epoch:3 step:3571 [D loss: 0.165011, acc.: 75.78%] [G loss: 0.611873]\n",
      "epoch:3 step:3572 [D loss: 0.203857, acc.: 69.53%] [G loss: 0.593927]\n",
      "epoch:3 step:3573 [D loss: 0.227613, acc.: 64.06%] [G loss: 0.537355]\n",
      "epoch:3 step:3574 [D loss: 0.174446, acc.: 74.22%] [G loss: 0.635553]\n",
      "epoch:3 step:3575 [D loss: 0.139726, acc.: 78.91%] [G loss: 0.619083]\n",
      "epoch:3 step:3576 [D loss: 0.237107, acc.: 61.72%] [G loss: 0.526214]\n",
      "epoch:3 step:3577 [D loss: 0.178317, acc.: 75.00%] [G loss: 0.614424]\n",
      "epoch:3 step:3578 [D loss: 0.166032, acc.: 77.34%] [G loss: 0.670383]\n",
      "epoch:3 step:3579 [D loss: 0.239299, acc.: 62.50%] [G loss: 0.568056]\n",
      "epoch:3 step:3580 [D loss: 0.184295, acc.: 74.22%] [G loss: 0.646896]\n",
      "epoch:3 step:3581 [D loss: 0.204885, acc.: 69.53%] [G loss: 0.622623]\n",
      "epoch:3 step:3582 [D loss: 0.195677, acc.: 74.22%] [G loss: 0.600133]\n",
      "epoch:3 step:3583 [D loss: 0.200036, acc.: 67.97%] [G loss: 0.548834]\n",
      "epoch:3 step:3584 [D loss: 0.187095, acc.: 74.22%] [G loss: 0.566816]\n",
      "epoch:3 step:3585 [D loss: 0.200545, acc.: 71.09%] [G loss: 0.590130]\n",
      "epoch:3 step:3586 [D loss: 0.159102, acc.: 79.69%] [G loss: 0.620726]\n",
      "epoch:3 step:3587 [D loss: 0.194134, acc.: 73.44%] [G loss: 0.550750]\n",
      "epoch:3 step:3588 [D loss: 0.192270, acc.: 69.53%] [G loss: 0.594876]\n",
      "epoch:3 step:3589 [D loss: 0.182886, acc.: 74.22%] [G loss: 0.599148]\n",
      "epoch:3 step:3590 [D loss: 0.176492, acc.: 72.66%] [G loss: 0.594156]\n",
      "epoch:3 step:3591 [D loss: 0.181753, acc.: 73.44%] [G loss: 0.654574]\n",
      "epoch:3 step:3592 [D loss: 0.153029, acc.: 78.12%] [G loss: 0.665146]\n",
      "epoch:3 step:3593 [D loss: 0.170288, acc.: 79.69%] [G loss: 0.564207]\n",
      "epoch:3 step:3594 [D loss: 0.205658, acc.: 67.19%] [G loss: 0.623314]\n",
      "epoch:3 step:3595 [D loss: 0.211932, acc.: 67.97%] [G loss: 0.572417]\n",
      "epoch:3 step:3596 [D loss: 0.177305, acc.: 75.00%] [G loss: 0.595511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3597 [D loss: 0.156180, acc.: 79.69%] [G loss: 0.621241]\n",
      "epoch:3 step:3598 [D loss: 0.236392, acc.: 64.84%] [G loss: 0.561633]\n",
      "epoch:3 step:3599 [D loss: 0.171627, acc.: 82.03%] [G loss: 0.582169]\n",
      "epoch:3 step:3600 [D loss: 0.143475, acc.: 81.25%] [G loss: 0.592984]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.684062\n",
      "FID: 50.887917\n",
      "0 = 13.113142719936342\n",
      "1 = 0.0812669296233273\n",
      "2 = 0.9817500114440918\n",
      "3 = 0.9646000266075134\n",
      "4 = 0.9988999962806702\n",
      "5 = 0.9988609552383423\n",
      "6 = 0.9646000266075134\n",
      "7 = 9.684793295550346\n",
      "8 = 0.15676679975658242\n",
      "9 = 0.8598999977111816\n",
      "10 = 0.8406999707221985\n",
      "11 = 0.8791000247001648\n",
      "12 = 0.8742720484733582\n",
      "13 = 0.8406999707221985\n",
      "14 = 5.684100151062012\n",
      "15 = 8.784037590026855\n",
      "16 = 0.2648255527019501\n",
      "17 = 5.684062480926514\n",
      "18 = 50.887916564941406\n",
      "epoch:3 step:3601 [D loss: 0.196994, acc.: 66.41%] [G loss: 0.598843]\n",
      "epoch:3 step:3602 [D loss: 0.179973, acc.: 71.88%] [G loss: 0.603754]\n",
      "epoch:3 step:3603 [D loss: 0.149050, acc.: 77.34%] [G loss: 0.658248]\n",
      "epoch:3 step:3604 [D loss: 0.224082, acc.: 67.19%] [G loss: 0.609220]\n",
      "epoch:3 step:3605 [D loss: 0.187643, acc.: 75.00%] [G loss: 0.606929]\n",
      "epoch:3 step:3606 [D loss: 0.172577, acc.: 74.22%] [G loss: 0.636049]\n",
      "epoch:3 step:3607 [D loss: 0.138496, acc.: 82.03%] [G loss: 0.678336]\n",
      "epoch:3 step:3608 [D loss: 0.204357, acc.: 68.75%] [G loss: 0.580283]\n",
      "epoch:3 step:3609 [D loss: 0.134274, acc.: 88.28%] [G loss: 0.626903]\n",
      "epoch:3 step:3610 [D loss: 0.173040, acc.: 76.56%] [G loss: 0.550200]\n",
      "epoch:3 step:3611 [D loss: 0.171547, acc.: 70.31%] [G loss: 0.602497]\n",
      "epoch:3 step:3612 [D loss: 0.162574, acc.: 76.56%] [G loss: 0.624075]\n",
      "epoch:3 step:3613 [D loss: 0.160007, acc.: 78.12%] [G loss: 0.626294]\n",
      "epoch:3 step:3614 [D loss: 0.135794, acc.: 82.81%] [G loss: 0.642760]\n",
      "epoch:3 step:3615 [D loss: 0.163526, acc.: 75.78%] [G loss: 0.634481]\n",
      "epoch:3 step:3616 [D loss: 0.174902, acc.: 74.22%] [G loss: 0.578593]\n",
      "epoch:3 step:3617 [D loss: 0.132253, acc.: 80.47%] [G loss: 0.643029]\n",
      "epoch:3 step:3618 [D loss: 0.146503, acc.: 78.12%] [G loss: 0.704926]\n",
      "epoch:3 step:3619 [D loss: 0.224732, acc.: 64.06%] [G loss: 0.571554]\n",
      "epoch:3 step:3620 [D loss: 0.182082, acc.: 71.09%] [G loss: 0.583278]\n",
      "epoch:3 step:3621 [D loss: 0.137835, acc.: 80.47%] [G loss: 0.645900]\n",
      "epoch:3 step:3622 [D loss: 0.175195, acc.: 76.56%] [G loss: 0.589821]\n",
      "epoch:3 step:3623 [D loss: 0.200363, acc.: 69.53%] [G loss: 0.531138]\n",
      "epoch:3 step:3624 [D loss: 0.185203, acc.: 73.44%] [G loss: 0.606151]\n",
      "epoch:3 step:3625 [D loss: 0.197033, acc.: 71.09%] [G loss: 0.610745]\n",
      "epoch:3 step:3626 [D loss: 0.172364, acc.: 72.66%] [G loss: 0.663877]\n",
      "epoch:3 step:3627 [D loss: 0.211950, acc.: 64.84%] [G loss: 0.662078]\n",
      "epoch:3 step:3628 [D loss: 0.200176, acc.: 70.31%] [G loss: 0.568581]\n",
      "epoch:3 step:3629 [D loss: 0.209847, acc.: 64.84%] [G loss: 0.553586]\n",
      "epoch:3 step:3630 [D loss: 0.158972, acc.: 80.47%] [G loss: 0.637486]\n",
      "epoch:3 step:3631 [D loss: 0.207294, acc.: 67.19%] [G loss: 0.552289]\n",
      "epoch:3 step:3632 [D loss: 0.159571, acc.: 79.69%] [G loss: 0.601575]\n",
      "epoch:3 step:3633 [D loss: 0.147333, acc.: 82.03%] [G loss: 0.646192]\n",
      "epoch:3 step:3634 [D loss: 0.155144, acc.: 79.69%] [G loss: 0.625804]\n",
      "epoch:3 step:3635 [D loss: 0.231418, acc.: 59.38%] [G loss: 0.535101]\n",
      "epoch:3 step:3636 [D loss: 0.187730, acc.: 72.66%] [G loss: 0.581873]\n",
      "epoch:3 step:3637 [D loss: 0.193454, acc.: 70.31%] [G loss: 0.612143]\n",
      "epoch:3 step:3638 [D loss: 0.231484, acc.: 62.50%] [G loss: 0.617106]\n",
      "epoch:3 step:3639 [D loss: 0.199054, acc.: 69.53%] [G loss: 0.539360]\n",
      "epoch:3 step:3640 [D loss: 0.180512, acc.: 73.44%] [G loss: 0.645162]\n",
      "epoch:3 step:3641 [D loss: 0.187939, acc.: 73.44%] [G loss: 0.580169]\n",
      "epoch:3 step:3642 [D loss: 0.171932, acc.: 79.69%] [G loss: 0.611561]\n",
      "epoch:3 step:3643 [D loss: 0.180767, acc.: 71.88%] [G loss: 0.626903]\n",
      "epoch:3 step:3644 [D loss: 0.186237, acc.: 73.44%] [G loss: 0.638142]\n",
      "epoch:3 step:3645 [D loss: 0.170660, acc.: 78.12%] [G loss: 0.605243]\n",
      "epoch:3 step:3646 [D loss: 0.162480, acc.: 76.56%] [G loss: 0.602080]\n",
      "epoch:3 step:3647 [D loss: 0.179451, acc.: 75.00%] [G loss: 0.589931]\n",
      "epoch:3 step:3648 [D loss: 0.167905, acc.: 77.34%] [G loss: 0.598330]\n",
      "epoch:3 step:3649 [D loss: 0.182811, acc.: 74.22%] [G loss: 0.625750]\n",
      "epoch:3 step:3650 [D loss: 0.190151, acc.: 74.22%] [G loss: 0.614239]\n",
      "epoch:3 step:3651 [D loss: 0.200319, acc.: 72.66%] [G loss: 0.552796]\n",
      "epoch:3 step:3652 [D loss: 0.180411, acc.: 75.00%] [G loss: 0.572663]\n",
      "epoch:3 step:3653 [D loss: 0.171827, acc.: 75.78%] [G loss: 0.641793]\n",
      "epoch:3 step:3654 [D loss: 0.191376, acc.: 75.78%] [G loss: 0.624420]\n",
      "epoch:3 step:3655 [D loss: 0.194970, acc.: 70.31%] [G loss: 0.594131]\n",
      "epoch:3 step:3656 [D loss: 0.190186, acc.: 71.88%] [G loss: 0.547927]\n",
      "epoch:3 step:3657 [D loss: 0.193068, acc.: 70.31%] [G loss: 0.586008]\n",
      "epoch:3 step:3658 [D loss: 0.190056, acc.: 75.00%] [G loss: 0.552124]\n",
      "epoch:3 step:3659 [D loss: 0.197669, acc.: 75.00%] [G loss: 0.551131]\n",
      "epoch:3 step:3660 [D loss: 0.192946, acc.: 66.41%] [G loss: 0.572209]\n",
      "epoch:3 step:3661 [D loss: 0.189088, acc.: 67.97%] [G loss: 0.601762]\n",
      "epoch:3 step:3662 [D loss: 0.190191, acc.: 71.88%] [G loss: 0.562345]\n",
      "epoch:3 step:3663 [D loss: 0.151209, acc.: 84.38%] [G loss: 0.619372]\n",
      "epoch:3 step:3664 [D loss: 0.159336, acc.: 77.34%] [G loss: 0.663888]\n",
      "epoch:3 step:3665 [D loss: 0.136584, acc.: 80.47%] [G loss: 0.631912]\n",
      "epoch:3 step:3666 [D loss: 0.207358, acc.: 66.41%] [G loss: 0.577961]\n",
      "epoch:3 step:3667 [D loss: 0.205593, acc.: 67.19%] [G loss: 0.626785]\n",
      "epoch:3 step:3668 [D loss: 0.151888, acc.: 78.12%] [G loss: 0.632341]\n",
      "epoch:3 step:3669 [D loss: 0.264063, acc.: 54.69%] [G loss: 0.562670]\n",
      "epoch:3 step:3670 [D loss: 0.158472, acc.: 81.25%] [G loss: 0.641191]\n",
      "epoch:3 step:3671 [D loss: 0.181086, acc.: 71.09%] [G loss: 0.656115]\n",
      "epoch:3 step:3672 [D loss: 0.214525, acc.: 63.28%] [G loss: 0.556850]\n",
      "epoch:3 step:3673 [D loss: 0.214620, acc.: 62.50%] [G loss: 0.508350]\n",
      "epoch:3 step:3674 [D loss: 0.147946, acc.: 80.47%] [G loss: 0.615445]\n",
      "epoch:3 step:3675 [D loss: 0.183295, acc.: 71.09%] [G loss: 0.591511]\n",
      "epoch:3 step:3676 [D loss: 0.199530, acc.: 71.09%] [G loss: 0.552734]\n",
      "epoch:3 step:3677 [D loss: 0.200944, acc.: 64.06%] [G loss: 0.580659]\n",
      "epoch:3 step:3678 [D loss: 0.192853, acc.: 68.75%] [G loss: 0.568682]\n",
      "epoch:3 step:3679 [D loss: 0.173054, acc.: 78.91%] [G loss: 0.605750]\n",
      "epoch:3 step:3680 [D loss: 0.174094, acc.: 72.66%] [G loss: 0.627302]\n",
      "epoch:3 step:3681 [D loss: 0.167733, acc.: 75.00%] [G loss: 0.614182]\n",
      "epoch:3 step:3682 [D loss: 0.187497, acc.: 71.88%] [G loss: 0.593339]\n",
      "epoch:3 step:3683 [D loss: 0.185624, acc.: 70.31%] [G loss: 0.644677]\n",
      "epoch:3 step:3684 [D loss: 0.194242, acc.: 74.22%] [G loss: 0.579731]\n",
      "epoch:3 step:3685 [D loss: 0.135863, acc.: 81.25%] [G loss: 0.593313]\n",
      "epoch:3 step:3686 [D loss: 0.151090, acc.: 78.12%] [G loss: 0.603886]\n",
      "epoch:3 step:3687 [D loss: 0.197370, acc.: 66.41%] [G loss: 0.611539]\n",
      "epoch:3 step:3688 [D loss: 0.154453, acc.: 78.12%] [G loss: 0.618291]\n",
      "epoch:3 step:3689 [D loss: 0.214852, acc.: 67.19%] [G loss: 0.533661]\n",
      "epoch:3 step:3690 [D loss: 0.161521, acc.: 74.22%] [G loss: 0.650805]\n",
      "epoch:3 step:3691 [D loss: 0.210037, acc.: 68.75%] [G loss: 0.587457]\n",
      "epoch:3 step:3692 [D loss: 0.176358, acc.: 76.56%] [G loss: 0.571141]\n",
      "epoch:3 step:3693 [D loss: 0.164423, acc.: 75.00%] [G loss: 0.642531]\n",
      "epoch:3 step:3694 [D loss: 0.185610, acc.: 71.88%] [G loss: 0.591561]\n",
      "epoch:3 step:3695 [D loss: 0.140545, acc.: 82.03%] [G loss: 0.647870]\n",
      "epoch:3 step:3696 [D loss: 0.169133, acc.: 79.69%] [G loss: 0.699269]\n",
      "epoch:3 step:3697 [D loss: 0.158860, acc.: 75.78%] [G loss: 0.700586]\n",
      "epoch:3 step:3698 [D loss: 0.158132, acc.: 81.25%] [G loss: 0.671458]\n",
      "epoch:3 step:3699 [D loss: 0.188399, acc.: 75.00%] [G loss: 0.596715]\n",
      "epoch:3 step:3700 [D loss: 0.140180, acc.: 81.25%] [G loss: 0.660732]\n",
      "epoch:3 step:3701 [D loss: 0.126591, acc.: 82.81%] [G loss: 0.705640]\n",
      "epoch:3 step:3702 [D loss: 0.241006, acc.: 64.84%] [G loss: 0.550986]\n",
      "epoch:3 step:3703 [D loss: 0.238651, acc.: 59.38%] [G loss: 0.558843]\n",
      "epoch:3 step:3704 [D loss: 0.156828, acc.: 78.91%] [G loss: 0.594175]\n",
      "epoch:3 step:3705 [D loss: 0.156695, acc.: 77.34%] [G loss: 0.650173]\n",
      "epoch:3 step:3706 [D loss: 0.193590, acc.: 73.44%] [G loss: 0.654729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3707 [D loss: 0.218321, acc.: 63.28%] [G loss: 0.586085]\n",
      "epoch:3 step:3708 [D loss: 0.144767, acc.: 78.91%] [G loss: 0.654590]\n",
      "epoch:3 step:3709 [D loss: 0.169574, acc.: 77.34%] [G loss: 0.596439]\n",
      "epoch:3 step:3710 [D loss: 0.172429, acc.: 76.56%] [G loss: 0.596938]\n",
      "epoch:3 step:3711 [D loss: 0.163791, acc.: 77.34%] [G loss: 0.628558]\n",
      "epoch:3 step:3712 [D loss: 0.174784, acc.: 76.56%] [G loss: 0.598320]\n",
      "epoch:3 step:3713 [D loss: 0.210268, acc.: 67.19%] [G loss: 0.553886]\n",
      "epoch:3 step:3714 [D loss: 0.180261, acc.: 74.22%] [G loss: 0.565041]\n",
      "epoch:3 step:3715 [D loss: 0.163606, acc.: 75.78%] [G loss: 0.676682]\n",
      "epoch:3 step:3716 [D loss: 0.152423, acc.: 82.81%] [G loss: 0.660562]\n",
      "epoch:3 step:3717 [D loss: 0.154484, acc.: 78.12%] [G loss: 0.657635]\n",
      "epoch:3 step:3718 [D loss: 0.199926, acc.: 67.97%] [G loss: 0.582272]\n",
      "epoch:3 step:3719 [D loss: 0.173954, acc.: 79.69%] [G loss: 0.570414]\n",
      "epoch:3 step:3720 [D loss: 0.165350, acc.: 75.78%] [G loss: 0.639306]\n",
      "epoch:3 step:3721 [D loss: 0.127052, acc.: 85.16%] [G loss: 0.661458]\n",
      "epoch:3 step:3722 [D loss: 0.181630, acc.: 78.12%] [G loss: 0.615941]\n",
      "epoch:3 step:3723 [D loss: 0.130310, acc.: 85.94%] [G loss: 0.716936]\n",
      "epoch:3 step:3724 [D loss: 0.227301, acc.: 69.53%] [G loss: 0.627042]\n",
      "epoch:3 step:3725 [D loss: 0.146810, acc.: 82.81%] [G loss: 0.645199]\n",
      "epoch:3 step:3726 [D loss: 0.209316, acc.: 68.75%] [G loss: 0.581096]\n",
      "epoch:3 step:3727 [D loss: 0.175075, acc.: 75.00%] [G loss: 0.606706]\n",
      "epoch:3 step:3728 [D loss: 0.191508, acc.: 68.75%] [G loss: 0.614773]\n",
      "epoch:3 step:3729 [D loss: 0.132945, acc.: 81.25%] [G loss: 0.718128]\n",
      "epoch:3 step:3730 [D loss: 0.213675, acc.: 71.88%] [G loss: 0.658867]\n",
      "epoch:3 step:3731 [D loss: 0.247685, acc.: 66.41%] [G loss: 0.616647]\n",
      "epoch:3 step:3732 [D loss: 0.156313, acc.: 78.12%] [G loss: 0.642738]\n",
      "epoch:3 step:3733 [D loss: 0.221334, acc.: 67.97%] [G loss: 0.534326]\n",
      "epoch:3 step:3734 [D loss: 0.145699, acc.: 82.81%] [G loss: 0.627589]\n",
      "epoch:3 step:3735 [D loss: 0.141913, acc.: 80.47%] [G loss: 0.673789]\n",
      "epoch:3 step:3736 [D loss: 0.147997, acc.: 82.03%] [G loss: 0.667794]\n",
      "epoch:3 step:3737 [D loss: 0.156488, acc.: 76.56%] [G loss: 0.666707]\n",
      "epoch:3 step:3738 [D loss: 0.190749, acc.: 69.53%] [G loss: 0.660258]\n",
      "epoch:3 step:3739 [D loss: 0.254643, acc.: 60.94%] [G loss: 0.636622]\n",
      "epoch:3 step:3740 [D loss: 0.126455, acc.: 84.38%] [G loss: 0.904700]\n",
      "epoch:3 step:3741 [D loss: 0.180253, acc.: 75.78%] [G loss: 0.702700]\n",
      "epoch:3 step:3742 [D loss: 0.172875, acc.: 77.34%] [G loss: 0.585954]\n",
      "epoch:3 step:3743 [D loss: 0.209908, acc.: 64.84%] [G loss: 0.562359]\n",
      "epoch:3 step:3744 [D loss: 0.198027, acc.: 72.66%] [G loss: 0.569308]\n",
      "epoch:3 step:3745 [D loss: 0.205973, acc.: 67.19%] [G loss: 0.514506]\n",
      "epoch:3 step:3746 [D loss: 0.184577, acc.: 71.88%] [G loss: 0.589035]\n",
      "epoch:3 step:3747 [D loss: 0.134420, acc.: 80.47%] [G loss: 0.701303]\n",
      "epoch:3 step:3748 [D loss: 0.135426, acc.: 82.03%] [G loss: 0.763395]\n",
      "epoch:4 step:3749 [D loss: 0.200102, acc.: 69.53%] [G loss: 0.596828]\n",
      "epoch:4 step:3750 [D loss: 0.165047, acc.: 77.34%] [G loss: 0.613442]\n",
      "epoch:4 step:3751 [D loss: 0.185783, acc.: 73.44%] [G loss: 0.608337]\n",
      "epoch:4 step:3752 [D loss: 0.177165, acc.: 74.22%] [G loss: 0.609025]\n",
      "epoch:4 step:3753 [D loss: 0.181227, acc.: 75.78%] [G loss: 0.597290]\n",
      "epoch:4 step:3754 [D loss: 0.177751, acc.: 69.53%] [G loss: 0.561515]\n",
      "epoch:4 step:3755 [D loss: 0.164517, acc.: 75.78%] [G loss: 0.622788]\n",
      "epoch:4 step:3756 [D loss: 0.227232, acc.: 65.62%] [G loss: 0.584356]\n",
      "epoch:4 step:3757 [D loss: 0.174541, acc.: 75.78%] [G loss: 0.574283]\n",
      "epoch:4 step:3758 [D loss: 0.201769, acc.: 72.66%] [G loss: 0.602913]\n",
      "epoch:4 step:3759 [D loss: 0.207923, acc.: 64.84%] [G loss: 0.563914]\n",
      "epoch:4 step:3760 [D loss: 0.162314, acc.: 75.78%] [G loss: 0.562226]\n",
      "epoch:4 step:3761 [D loss: 0.165390, acc.: 79.69%] [G loss: 0.578620]\n",
      "epoch:4 step:3762 [D loss: 0.169200, acc.: 71.09%] [G loss: 0.597655]\n",
      "epoch:4 step:3763 [D loss: 0.158450, acc.: 79.69%] [G loss: 0.635123]\n",
      "epoch:4 step:3764 [D loss: 0.148999, acc.: 81.25%] [G loss: 0.652673]\n",
      "epoch:4 step:3765 [D loss: 0.258680, acc.: 60.16%] [G loss: 0.548564]\n",
      "epoch:4 step:3766 [D loss: 0.204065, acc.: 71.88%] [G loss: 0.549347]\n",
      "epoch:4 step:3767 [D loss: 0.166576, acc.: 77.34%] [G loss: 0.589184]\n",
      "epoch:4 step:3768 [D loss: 0.206606, acc.: 68.75%] [G loss: 0.566534]\n",
      "epoch:4 step:3769 [D loss: 0.183003, acc.: 75.78%] [G loss: 0.606210]\n",
      "epoch:4 step:3770 [D loss: 0.186594, acc.: 73.44%] [G loss: 0.621710]\n",
      "epoch:4 step:3771 [D loss: 0.150897, acc.: 80.47%] [G loss: 0.622850]\n",
      "epoch:4 step:3772 [D loss: 0.172371, acc.: 72.66%] [G loss: 0.602212]\n",
      "epoch:4 step:3773 [D loss: 0.159435, acc.: 78.12%] [G loss: 0.669517]\n",
      "epoch:4 step:3774 [D loss: 0.192724, acc.: 73.44%] [G loss: 0.584178]\n",
      "epoch:4 step:3775 [D loss: 0.168704, acc.: 75.78%] [G loss: 0.571546]\n",
      "epoch:4 step:3776 [D loss: 0.168263, acc.: 77.34%] [G loss: 0.578605]\n",
      "epoch:4 step:3777 [D loss: 0.135963, acc.: 82.81%] [G loss: 0.658136]\n",
      "epoch:4 step:3778 [D loss: 0.202494, acc.: 72.66%] [G loss: 0.544937]\n",
      "epoch:4 step:3779 [D loss: 0.166777, acc.: 76.56%] [G loss: 0.593946]\n",
      "epoch:4 step:3780 [D loss: 0.173297, acc.: 75.00%] [G loss: 0.592987]\n",
      "epoch:4 step:3781 [D loss: 0.161868, acc.: 76.56%] [G loss: 0.591895]\n",
      "epoch:4 step:3782 [D loss: 0.189654, acc.: 73.44%] [G loss: 0.597308]\n",
      "epoch:4 step:3783 [D loss: 0.166562, acc.: 81.25%] [G loss: 0.606635]\n",
      "epoch:4 step:3784 [D loss: 0.137832, acc.: 78.12%] [G loss: 0.662102]\n",
      "epoch:4 step:3785 [D loss: 0.185390, acc.: 79.69%] [G loss: 0.609465]\n",
      "epoch:4 step:3786 [D loss: 0.198030, acc.: 68.75%] [G loss: 0.562765]\n",
      "epoch:4 step:3787 [D loss: 0.142697, acc.: 82.81%] [G loss: 0.646504]\n",
      "epoch:4 step:3788 [D loss: 0.120684, acc.: 85.94%] [G loss: 0.711584]\n",
      "epoch:4 step:3789 [D loss: 0.187796, acc.: 72.66%] [G loss: 0.604746]\n",
      "epoch:4 step:3790 [D loss: 0.194237, acc.: 70.31%] [G loss: 0.571110]\n",
      "epoch:4 step:3791 [D loss: 0.147327, acc.: 78.91%] [G loss: 0.632137]\n",
      "epoch:4 step:3792 [D loss: 0.176732, acc.: 75.00%] [G loss: 0.574505]\n",
      "epoch:4 step:3793 [D loss: 0.187088, acc.: 74.22%] [G loss: 0.534393]\n",
      "epoch:4 step:3794 [D loss: 0.159673, acc.: 75.78%] [G loss: 0.583877]\n",
      "epoch:4 step:3795 [D loss: 0.151158, acc.: 78.12%] [G loss: 0.623237]\n",
      "epoch:4 step:3796 [D loss: 0.170430, acc.: 76.56%] [G loss: 0.622268]\n",
      "epoch:4 step:3797 [D loss: 0.198159, acc.: 67.97%] [G loss: 0.548116]\n",
      "epoch:4 step:3798 [D loss: 0.175337, acc.: 72.66%] [G loss: 0.618747]\n",
      "epoch:4 step:3799 [D loss: 0.206077, acc.: 66.41%] [G loss: 0.614328]\n",
      "epoch:4 step:3800 [D loss: 0.180529, acc.: 72.66%] [G loss: 0.624820]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.888853\n",
      "FID: 42.631756\n",
      "0 = 13.102439505815523\n",
      "1 = 0.07610611644732215\n",
      "2 = 0.9788500070571899\n",
      "3 = 0.9609000086784363\n",
      "4 = 0.9968000054359436\n",
      "5 = 0.9966808557510376\n",
      "6 = 0.9609000086784363\n",
      "7 = 9.282582567763296\n",
      "8 = 0.14316469533241785\n",
      "9 = 0.8503999710083008\n",
      "10 = 0.8331000208854675\n",
      "11 = 0.8676999807357788\n",
      "12 = 0.8629583716392517\n",
      "13 = 0.8331000208854675\n",
      "14 = 5.8888959884643555\n",
      "15 = 8.7243013381958\n",
      "16 = 0.2661885619163513\n",
      "17 = 5.888853073120117\n",
      "18 = 42.63175582885742\n",
      "epoch:4 step:3801 [D loss: 0.156516, acc.: 77.34%] [G loss: 0.665643]\n",
      "epoch:4 step:3802 [D loss: 0.160217, acc.: 76.56%] [G loss: 0.633371]\n",
      "epoch:4 step:3803 [D loss: 0.165251, acc.: 78.12%] [G loss: 0.616794]\n",
      "epoch:4 step:3804 [D loss: 0.147264, acc.: 80.47%] [G loss: 0.613045]\n",
      "epoch:4 step:3805 [D loss: 0.160523, acc.: 76.56%] [G loss: 0.625934]\n",
      "epoch:4 step:3806 [D loss: 0.172307, acc.: 76.56%] [G loss: 0.585583]\n",
      "epoch:4 step:3807 [D loss: 0.169458, acc.: 72.66%] [G loss: 0.594227]\n",
      "epoch:4 step:3808 [D loss: 0.197553, acc.: 64.84%] [G loss: 0.576312]\n",
      "epoch:4 step:3809 [D loss: 0.194942, acc.: 70.31%] [G loss: 0.586418]\n",
      "epoch:4 step:3810 [D loss: 0.160334, acc.: 75.78%] [G loss: 0.635019]\n",
      "epoch:4 step:3811 [D loss: 0.188267, acc.: 68.75%] [G loss: 0.549102]\n",
      "epoch:4 step:3812 [D loss: 0.192227, acc.: 66.41%] [G loss: 0.609600]\n",
      "epoch:4 step:3813 [D loss: 0.181968, acc.: 75.78%] [G loss: 0.635666]\n",
      "epoch:4 step:3814 [D loss: 0.182561, acc.: 71.09%] [G loss: 0.606002]\n",
      "epoch:4 step:3815 [D loss: 0.192780, acc.: 71.88%] [G loss: 0.637138]\n",
      "epoch:4 step:3816 [D loss: 0.160689, acc.: 76.56%] [G loss: 0.606767]\n",
      "epoch:4 step:3817 [D loss: 0.155812, acc.: 78.91%] [G loss: 0.584753]\n",
      "epoch:4 step:3818 [D loss: 0.155248, acc.: 78.12%] [G loss: 0.625251]\n",
      "epoch:4 step:3819 [D loss: 0.164765, acc.: 78.91%] [G loss: 0.605754]\n",
      "epoch:4 step:3820 [D loss: 0.182577, acc.: 75.78%] [G loss: 0.544981]\n",
      "epoch:4 step:3821 [D loss: 0.173040, acc.: 74.22%] [G loss: 0.612582]\n",
      "epoch:4 step:3822 [D loss: 0.147333, acc.: 81.25%] [G loss: 0.626001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3823 [D loss: 0.190326, acc.: 74.22%] [G loss: 0.579076]\n",
      "epoch:4 step:3824 [D loss: 0.157915, acc.: 78.12%] [G loss: 0.656550]\n",
      "epoch:4 step:3825 [D loss: 0.115059, acc.: 87.50%] [G loss: 0.670129]\n",
      "epoch:4 step:3826 [D loss: 0.256923, acc.: 59.38%] [G loss: 0.602218]\n",
      "epoch:4 step:3827 [D loss: 0.172915, acc.: 75.78%] [G loss: 0.595874]\n",
      "epoch:4 step:3828 [D loss: 0.166351, acc.: 82.03%] [G loss: 0.646756]\n",
      "epoch:4 step:3829 [D loss: 0.181458, acc.: 74.22%] [G loss: 0.574556]\n",
      "epoch:4 step:3830 [D loss: 0.177565, acc.: 77.34%] [G loss: 0.588379]\n",
      "epoch:4 step:3831 [D loss: 0.176117, acc.: 68.75%] [G loss: 0.648749]\n",
      "epoch:4 step:3832 [D loss: 0.201809, acc.: 70.31%] [G loss: 0.627419]\n",
      "epoch:4 step:3833 [D loss: 0.220595, acc.: 65.62%] [G loss: 0.551277]\n",
      "epoch:4 step:3834 [D loss: 0.176667, acc.: 74.22%] [G loss: 0.577951]\n",
      "epoch:4 step:3835 [D loss: 0.185360, acc.: 75.00%] [G loss: 0.566998]\n",
      "epoch:4 step:3836 [D loss: 0.153277, acc.: 79.69%] [G loss: 0.591685]\n",
      "epoch:4 step:3837 [D loss: 0.182274, acc.: 73.44%] [G loss: 0.581724]\n",
      "epoch:4 step:3838 [D loss: 0.182409, acc.: 70.31%] [G loss: 0.577060]\n",
      "epoch:4 step:3839 [D loss: 0.191581, acc.: 69.53%] [G loss: 0.609416]\n",
      "epoch:4 step:3840 [D loss: 0.176270, acc.: 75.78%] [G loss: 0.562706]\n",
      "epoch:4 step:3841 [D loss: 0.198882, acc.: 72.66%] [G loss: 0.577209]\n",
      "epoch:4 step:3842 [D loss: 0.206471, acc.: 69.53%] [G loss: 0.588683]\n",
      "epoch:4 step:3843 [D loss: 0.170469, acc.: 75.00%] [G loss: 0.626026]\n",
      "epoch:4 step:3844 [D loss: 0.128187, acc.: 84.38%] [G loss: 0.658325]\n",
      "epoch:4 step:3845 [D loss: 0.159004, acc.: 79.69%] [G loss: 0.622657]\n",
      "epoch:4 step:3846 [D loss: 0.247103, acc.: 60.94%] [G loss: 0.535331]\n",
      "epoch:4 step:3847 [D loss: 0.182520, acc.: 75.00%] [G loss: 0.575551]\n",
      "epoch:4 step:3848 [D loss: 0.181783, acc.: 71.09%] [G loss: 0.650768]\n",
      "epoch:4 step:3849 [D loss: 0.194483, acc.: 69.53%] [G loss: 0.601814]\n",
      "epoch:4 step:3850 [D loss: 0.225664, acc.: 64.06%] [G loss: 0.537854]\n",
      "epoch:4 step:3851 [D loss: 0.172205, acc.: 78.91%] [G loss: 0.564885]\n",
      "epoch:4 step:3852 [D loss: 0.185041, acc.: 75.00%] [G loss: 0.524748]\n",
      "epoch:4 step:3853 [D loss: 0.188486, acc.: 71.09%] [G loss: 0.536660]\n",
      "epoch:4 step:3854 [D loss: 0.197988, acc.: 66.41%] [G loss: 0.591039]\n",
      "epoch:4 step:3855 [D loss: 0.209780, acc.: 70.31%] [G loss: 0.582423]\n",
      "epoch:4 step:3856 [D loss: 0.251942, acc.: 58.59%] [G loss: 0.555791]\n",
      "epoch:4 step:3857 [D loss: 0.221055, acc.: 64.84%] [G loss: 0.541598]\n",
      "epoch:4 step:3858 [D loss: 0.201934, acc.: 72.66%] [G loss: 0.564024]\n",
      "epoch:4 step:3859 [D loss: 0.171475, acc.: 74.22%] [G loss: 0.565584]\n",
      "epoch:4 step:3860 [D loss: 0.184466, acc.: 73.44%] [G loss: 0.651607]\n",
      "epoch:4 step:3861 [D loss: 0.239149, acc.: 60.16%] [G loss: 0.562259]\n",
      "epoch:4 step:3862 [D loss: 0.213137, acc.: 64.84%] [G loss: 0.542342]\n",
      "epoch:4 step:3863 [D loss: 0.223813, acc.: 64.84%] [G loss: 0.530022]\n",
      "epoch:4 step:3864 [D loss: 0.171664, acc.: 77.34%] [G loss: 0.575045]\n",
      "epoch:4 step:3865 [D loss: 0.167464, acc.: 77.34%] [G loss: 0.629107]\n",
      "epoch:4 step:3866 [D loss: 0.191652, acc.: 69.53%] [G loss: 0.619626]\n",
      "epoch:4 step:3867 [D loss: 0.148070, acc.: 81.25%] [G loss: 0.700720]\n",
      "epoch:4 step:3868 [D loss: 0.237064, acc.: 68.75%] [G loss: 0.578084]\n",
      "epoch:4 step:3869 [D loss: 0.183285, acc.: 74.22%] [G loss: 0.560245]\n",
      "epoch:4 step:3870 [D loss: 0.149442, acc.: 82.03%] [G loss: 0.634075]\n",
      "epoch:4 step:3871 [D loss: 0.207203, acc.: 70.31%] [G loss: 0.602761]\n",
      "epoch:4 step:3872 [D loss: 0.170671, acc.: 74.22%] [G loss: 0.533976]\n",
      "epoch:4 step:3873 [D loss: 0.172752, acc.: 72.66%] [G loss: 0.543911]\n",
      "epoch:4 step:3874 [D loss: 0.171978, acc.: 75.78%] [G loss: 0.591263]\n",
      "epoch:4 step:3875 [D loss: 0.190361, acc.: 68.75%] [G loss: 0.529036]\n",
      "epoch:4 step:3876 [D loss: 0.192709, acc.: 69.53%] [G loss: 0.573399]\n",
      "epoch:4 step:3877 [D loss: 0.203089, acc.: 71.88%] [G loss: 0.556107]\n",
      "epoch:4 step:3878 [D loss: 0.174745, acc.: 73.44%] [G loss: 0.591506]\n",
      "epoch:4 step:3879 [D loss: 0.178832, acc.: 71.88%] [G loss: 0.606055]\n",
      "epoch:4 step:3880 [D loss: 0.201525, acc.: 69.53%] [G loss: 0.556645]\n",
      "epoch:4 step:3881 [D loss: 0.206793, acc.: 64.84%] [G loss: 0.579001]\n",
      "epoch:4 step:3882 [D loss: 0.180860, acc.: 72.66%] [G loss: 0.582105]\n",
      "epoch:4 step:3883 [D loss: 0.166316, acc.: 81.25%] [G loss: 0.633036]\n",
      "epoch:4 step:3884 [D loss: 0.193754, acc.: 69.53%] [G loss: 0.570518]\n",
      "epoch:4 step:3885 [D loss: 0.206386, acc.: 68.75%] [G loss: 0.525693]\n",
      "epoch:4 step:3886 [D loss: 0.200117, acc.: 65.62%] [G loss: 0.559446]\n",
      "epoch:4 step:3887 [D loss: 0.208802, acc.: 66.41%] [G loss: 0.547079]\n",
      "epoch:4 step:3888 [D loss: 0.209755, acc.: 62.50%] [G loss: 0.541088]\n",
      "epoch:4 step:3889 [D loss: 0.156757, acc.: 81.25%] [G loss: 0.573814]\n",
      "epoch:4 step:3890 [D loss: 0.214871, acc.: 70.31%] [G loss: 0.583651]\n",
      "epoch:4 step:3891 [D loss: 0.171253, acc.: 73.44%] [G loss: 0.610545]\n",
      "epoch:4 step:3892 [D loss: 0.164791, acc.: 78.12%] [G loss: 0.589823]\n",
      "epoch:4 step:3893 [D loss: 0.167543, acc.: 77.34%] [G loss: 0.595439]\n",
      "epoch:4 step:3894 [D loss: 0.193210, acc.: 70.31%] [G loss: 0.635613]\n",
      "epoch:4 step:3895 [D loss: 0.224145, acc.: 62.50%] [G loss: 0.542986]\n",
      "epoch:4 step:3896 [D loss: 0.193179, acc.: 73.44%] [G loss: 0.544140]\n",
      "epoch:4 step:3897 [D loss: 0.161070, acc.: 75.78%] [G loss: 0.582026]\n",
      "epoch:4 step:3898 [D loss: 0.219237, acc.: 67.97%] [G loss: 0.546945]\n",
      "epoch:4 step:3899 [D loss: 0.192452, acc.: 73.44%] [G loss: 0.576762]\n",
      "epoch:4 step:3900 [D loss: 0.156085, acc.: 77.34%] [G loss: 0.605198]\n",
      "epoch:4 step:3901 [D loss: 0.211793, acc.: 67.19%] [G loss: 0.579413]\n",
      "epoch:4 step:3902 [D loss: 0.166352, acc.: 75.00%] [G loss: 0.583064]\n",
      "epoch:4 step:3903 [D loss: 0.141321, acc.: 78.91%] [G loss: 0.648989]\n",
      "epoch:4 step:3904 [D loss: 0.188102, acc.: 73.44%] [G loss: 0.607878]\n",
      "epoch:4 step:3905 [D loss: 0.219018, acc.: 64.84%] [G loss: 0.562943]\n",
      "epoch:4 step:3906 [D loss: 0.219507, acc.: 67.19%] [G loss: 0.553263]\n",
      "epoch:4 step:3907 [D loss: 0.226930, acc.: 62.50%] [G loss: 0.562402]\n",
      "epoch:4 step:3908 [D loss: 0.200989, acc.: 68.75%] [G loss: 0.576491]\n",
      "epoch:4 step:3909 [D loss: 0.165484, acc.: 77.34%] [G loss: 0.604266]\n",
      "epoch:4 step:3910 [D loss: 0.164194, acc.: 73.44%] [G loss: 0.638083]\n",
      "epoch:4 step:3911 [D loss: 0.174819, acc.: 74.22%] [G loss: 0.580122]\n",
      "epoch:4 step:3912 [D loss: 0.189178, acc.: 71.88%] [G loss: 0.536431]\n",
      "epoch:4 step:3913 [D loss: 0.199193, acc.: 68.75%] [G loss: 0.556864]\n",
      "epoch:4 step:3914 [D loss: 0.173916, acc.: 75.00%] [G loss: 0.614178]\n",
      "epoch:4 step:3915 [D loss: 0.216634, acc.: 64.84%] [G loss: 0.527458]\n",
      "epoch:4 step:3916 [D loss: 0.204173, acc.: 68.75%] [G loss: 0.582745]\n",
      "epoch:4 step:3917 [D loss: 0.190423, acc.: 72.66%] [G loss: 0.569512]\n",
      "epoch:4 step:3918 [D loss: 0.194421, acc.: 71.88%] [G loss: 0.547926]\n",
      "epoch:4 step:3919 [D loss: 0.174964, acc.: 71.88%] [G loss: 0.565972]\n",
      "epoch:4 step:3920 [D loss: 0.181101, acc.: 72.66%] [G loss: 0.586824]\n",
      "epoch:4 step:3921 [D loss: 0.199358, acc.: 71.88%] [G loss: 0.560557]\n",
      "epoch:4 step:3922 [D loss: 0.206037, acc.: 66.41%] [G loss: 0.507057]\n",
      "epoch:4 step:3923 [D loss: 0.195712, acc.: 70.31%] [G loss: 0.548754]\n",
      "epoch:4 step:3924 [D loss: 0.201972, acc.: 68.75%] [G loss: 0.597772]\n",
      "epoch:4 step:3925 [D loss: 0.191828, acc.: 71.88%] [G loss: 0.590470]\n",
      "epoch:4 step:3926 [D loss: 0.195047, acc.: 68.75%] [G loss: 0.586059]\n",
      "epoch:4 step:3927 [D loss: 0.200944, acc.: 70.31%] [G loss: 0.554130]\n",
      "epoch:4 step:3928 [D loss: 0.176421, acc.: 72.66%] [G loss: 0.590929]\n",
      "epoch:4 step:3929 [D loss: 0.191755, acc.: 67.97%] [G loss: 0.573281]\n",
      "epoch:4 step:3930 [D loss: 0.211413, acc.: 70.31%] [G loss: 0.536594]\n",
      "epoch:4 step:3931 [D loss: 0.187578, acc.: 74.22%] [G loss: 0.579089]\n",
      "epoch:4 step:3932 [D loss: 0.184252, acc.: 78.12%] [G loss: 0.605429]\n",
      "epoch:4 step:3933 [D loss: 0.188291, acc.: 73.44%] [G loss: 0.561051]\n",
      "epoch:4 step:3934 [D loss: 0.158822, acc.: 77.34%] [G loss: 0.605301]\n",
      "epoch:4 step:3935 [D loss: 0.188752, acc.: 70.31%] [G loss: 0.566210]\n",
      "epoch:4 step:3936 [D loss: 0.187777, acc.: 73.44%] [G loss: 0.590273]\n",
      "epoch:4 step:3937 [D loss: 0.185202, acc.: 73.44%] [G loss: 0.555045]\n",
      "epoch:4 step:3938 [D loss: 0.161811, acc.: 74.22%] [G loss: 0.579757]\n",
      "epoch:4 step:3939 [D loss: 0.176126, acc.: 78.12%] [G loss: 0.599589]\n",
      "epoch:4 step:3940 [D loss: 0.182380, acc.: 71.88%] [G loss: 0.574391]\n",
      "epoch:4 step:3941 [D loss: 0.171110, acc.: 71.88%] [G loss: 0.610426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3942 [D loss: 0.167097, acc.: 79.69%] [G loss: 0.608603]\n",
      "epoch:4 step:3943 [D loss: 0.172784, acc.: 73.44%] [G loss: 0.586488]\n",
      "epoch:4 step:3944 [D loss: 0.186103, acc.: 71.09%] [G loss: 0.544635]\n",
      "epoch:4 step:3945 [D loss: 0.166326, acc.: 78.12%] [G loss: 0.548575]\n",
      "epoch:4 step:3946 [D loss: 0.161915, acc.: 79.69%] [G loss: 0.603252]\n",
      "epoch:4 step:3947 [D loss: 0.161625, acc.: 78.91%] [G loss: 0.665329]\n",
      "epoch:4 step:3948 [D loss: 0.230211, acc.: 64.84%] [G loss: 0.570903]\n",
      "epoch:4 step:3949 [D loss: 0.195862, acc.: 73.44%] [G loss: 0.557950]\n",
      "epoch:4 step:3950 [D loss: 0.162121, acc.: 75.78%] [G loss: 0.651712]\n",
      "epoch:4 step:3951 [D loss: 0.208173, acc.: 68.75%] [G loss: 0.499503]\n",
      "epoch:4 step:3952 [D loss: 0.199441, acc.: 71.88%] [G loss: 0.601057]\n",
      "epoch:4 step:3953 [D loss: 0.173769, acc.: 74.22%] [G loss: 0.589974]\n",
      "epoch:4 step:3954 [D loss: 0.172697, acc.: 75.00%] [G loss: 0.552629]\n",
      "epoch:4 step:3955 [D loss: 0.149394, acc.: 82.03%] [G loss: 0.615867]\n",
      "epoch:4 step:3956 [D loss: 0.130792, acc.: 82.81%] [G loss: 0.705414]\n",
      "epoch:4 step:3957 [D loss: 0.180676, acc.: 73.44%] [G loss: 0.595014]\n",
      "epoch:4 step:3958 [D loss: 0.212229, acc.: 64.06%] [G loss: 0.549592]\n",
      "epoch:4 step:3959 [D loss: 0.203375, acc.: 67.97%] [G loss: 0.531564]\n",
      "epoch:4 step:3960 [D loss: 0.168042, acc.: 75.00%] [G loss: 0.551723]\n",
      "epoch:4 step:3961 [D loss: 0.180634, acc.: 72.66%] [G loss: 0.517145]\n",
      "epoch:4 step:3962 [D loss: 0.232906, acc.: 60.16%] [G loss: 0.503429]\n",
      "epoch:4 step:3963 [D loss: 0.205822, acc.: 71.09%] [G loss: 0.583302]\n",
      "epoch:4 step:3964 [D loss: 0.200144, acc.: 70.31%] [G loss: 0.594767]\n",
      "epoch:4 step:3965 [D loss: 0.150338, acc.: 78.12%] [G loss: 0.609386]\n",
      "epoch:4 step:3966 [D loss: 0.189787, acc.: 72.66%] [G loss: 0.597699]\n",
      "epoch:4 step:3967 [D loss: 0.160135, acc.: 78.91%] [G loss: 0.647743]\n",
      "epoch:4 step:3968 [D loss: 0.195035, acc.: 70.31%] [G loss: 0.539926]\n",
      "epoch:4 step:3969 [D loss: 0.160036, acc.: 74.22%] [G loss: 0.675126]\n",
      "epoch:4 step:3970 [D loss: 0.165065, acc.: 76.56%] [G loss: 0.693934]\n",
      "epoch:4 step:3971 [D loss: 0.140502, acc.: 83.59%] [G loss: 0.657702]\n",
      "epoch:4 step:3972 [D loss: 0.221496, acc.: 68.75%] [G loss: 0.516481]\n",
      "epoch:4 step:3973 [D loss: 0.236336, acc.: 60.94%] [G loss: 0.509309]\n",
      "epoch:4 step:3974 [D loss: 0.202011, acc.: 68.75%] [G loss: 0.529344]\n",
      "epoch:4 step:3975 [D loss: 0.204012, acc.: 67.19%] [G loss: 0.515810]\n",
      "epoch:4 step:3976 [D loss: 0.232607, acc.: 62.50%] [G loss: 0.507284]\n",
      "epoch:4 step:3977 [D loss: 0.207114, acc.: 72.66%] [G loss: 0.543310]\n",
      "epoch:4 step:3978 [D loss: 0.135500, acc.: 82.81%] [G loss: 0.656214]\n",
      "epoch:4 step:3979 [D loss: 0.150858, acc.: 82.03%] [G loss: 0.651070]\n",
      "epoch:4 step:3980 [D loss: 0.146978, acc.: 82.03%] [G loss: 0.707159]\n",
      "epoch:4 step:3981 [D loss: 0.228356, acc.: 67.97%] [G loss: 0.555416]\n",
      "epoch:4 step:3982 [D loss: 0.191766, acc.: 67.19%] [G loss: 0.568309]\n",
      "epoch:4 step:3983 [D loss: 0.191098, acc.: 76.56%] [G loss: 0.567307]\n",
      "epoch:4 step:3984 [D loss: 0.185512, acc.: 71.88%] [G loss: 0.532609]\n",
      "epoch:4 step:3985 [D loss: 0.198152, acc.: 70.31%] [G loss: 0.555494]\n",
      "epoch:4 step:3986 [D loss: 0.192389, acc.: 71.09%] [G loss: 0.583665]\n",
      "epoch:4 step:3987 [D loss: 0.177281, acc.: 75.78%] [G loss: 0.554452]\n",
      "epoch:4 step:3988 [D loss: 0.167093, acc.: 76.56%] [G loss: 0.590902]\n",
      "epoch:4 step:3989 [D loss: 0.197398, acc.: 67.19%] [G loss: 0.566481]\n",
      "epoch:4 step:3990 [D loss: 0.195348, acc.: 66.41%] [G loss: 0.558510]\n",
      "epoch:4 step:3991 [D loss: 0.176187, acc.: 75.00%] [G loss: 0.586460]\n",
      "epoch:4 step:3992 [D loss: 0.160733, acc.: 79.69%] [G loss: 0.549698]\n",
      "epoch:4 step:3993 [D loss: 0.213362, acc.: 64.06%] [G loss: 0.495757]\n",
      "epoch:4 step:3994 [D loss: 0.196622, acc.: 72.66%] [G loss: 0.573138]\n",
      "epoch:4 step:3995 [D loss: 0.184149, acc.: 71.09%] [G loss: 0.583188]\n",
      "epoch:4 step:3996 [D loss: 0.224894, acc.: 67.19%] [G loss: 0.567348]\n",
      "epoch:4 step:3997 [D loss: 0.233582, acc.: 58.59%] [G loss: 0.545988]\n",
      "epoch:4 step:3998 [D loss: 0.211270, acc.: 69.53%] [G loss: 0.504609]\n",
      "epoch:4 step:3999 [D loss: 0.200586, acc.: 73.44%] [G loss: 0.560772]\n",
      "epoch:4 step:4000 [D loss: 0.223028, acc.: 69.53%] [G loss: 0.582680]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.849896\n",
      "FID: 44.159145\n",
      "0 = 12.862877000522648\n",
      "1 = 0.07703400829398592\n",
      "2 = 0.9686499834060669\n",
      "3 = 0.9422000050544739\n",
      "4 = 0.9951000213623047\n",
      "5 = 0.9948263168334961\n",
      "6 = 0.9422000050544739\n",
      "7 = 9.364907941997004\n",
      "8 = 0.14348034653008426\n",
      "9 = 0.8470500111579895\n",
      "10 = 0.8327999711036682\n",
      "11 = 0.861299991607666\n",
      "12 = 0.857231080532074\n",
      "13 = 0.8327999711036682\n",
      "14 = 5.8499369621276855\n",
      "15 = 8.833694458007812\n",
      "16 = 0.2581261992454529\n",
      "17 = 5.84989595413208\n",
      "18 = 44.15914535522461\n",
      "epoch:4 step:4001 [D loss: 0.191459, acc.: 71.09%] [G loss: 0.546979]\n",
      "epoch:4 step:4002 [D loss: 0.155410, acc.: 78.12%] [G loss: 0.597753]\n",
      "epoch:4 step:4003 [D loss: 0.174063, acc.: 76.56%] [G loss: 0.605300]\n",
      "epoch:4 step:4004 [D loss: 0.186076, acc.: 75.00%] [G loss: 0.582974]\n",
      "epoch:4 step:4005 [D loss: 0.192818, acc.: 67.97%] [G loss: 0.555988]\n",
      "epoch:4 step:4006 [D loss: 0.173204, acc.: 75.00%] [G loss: 0.535919]\n",
      "epoch:4 step:4007 [D loss: 0.170029, acc.: 78.12%] [G loss: 0.579305]\n",
      "epoch:4 step:4008 [D loss: 0.206482, acc.: 65.62%] [G loss: 0.559089]\n",
      "epoch:4 step:4009 [D loss: 0.185359, acc.: 75.78%] [G loss: 0.550835]\n",
      "epoch:4 step:4010 [D loss: 0.183658, acc.: 75.00%] [G loss: 0.600103]\n",
      "epoch:4 step:4011 [D loss: 0.232686, acc.: 67.19%] [G loss: 0.563664]\n",
      "epoch:4 step:4012 [D loss: 0.141138, acc.: 82.03%] [G loss: 0.643918]\n",
      "epoch:4 step:4013 [D loss: 0.232772, acc.: 64.84%] [G loss: 0.545638]\n",
      "epoch:4 step:4014 [D loss: 0.168762, acc.: 78.12%] [G loss: 0.567265]\n",
      "epoch:4 step:4015 [D loss: 0.206573, acc.: 67.97%] [G loss: 0.557969]\n",
      "epoch:4 step:4016 [D loss: 0.204769, acc.: 65.62%] [G loss: 0.517528]\n",
      "epoch:4 step:4017 [D loss: 0.224182, acc.: 63.28%] [G loss: 0.551367]\n",
      "epoch:4 step:4018 [D loss: 0.197837, acc.: 68.75%] [G loss: 0.585298]\n",
      "epoch:4 step:4019 [D loss: 0.169491, acc.: 74.22%] [G loss: 0.589594]\n",
      "epoch:4 step:4020 [D loss: 0.219898, acc.: 64.84%] [G loss: 0.517203]\n",
      "epoch:4 step:4021 [D loss: 0.183211, acc.: 73.44%] [G loss: 0.571339]\n",
      "epoch:4 step:4022 [D loss: 0.220296, acc.: 60.16%] [G loss: 0.525478]\n",
      "epoch:4 step:4023 [D loss: 0.193027, acc.: 68.75%] [G loss: 0.591036]\n",
      "epoch:4 step:4024 [D loss: 0.189892, acc.: 74.22%] [G loss: 0.599504]\n",
      "epoch:4 step:4025 [D loss: 0.238274, acc.: 61.72%] [G loss: 0.565301]\n",
      "epoch:4 step:4026 [D loss: 0.221012, acc.: 67.19%] [G loss: 0.561372]\n",
      "epoch:4 step:4027 [D loss: 0.181461, acc.: 73.44%] [G loss: 0.600968]\n",
      "epoch:4 step:4028 [D loss: 0.144839, acc.: 78.12%] [G loss: 0.630151]\n",
      "epoch:4 step:4029 [D loss: 0.217307, acc.: 71.09%] [G loss: 0.535765]\n",
      "epoch:4 step:4030 [D loss: 0.198719, acc.: 67.19%] [G loss: 0.543717]\n",
      "epoch:4 step:4031 [D loss: 0.166077, acc.: 74.22%] [G loss: 0.556212]\n",
      "epoch:4 step:4032 [D loss: 0.190550, acc.: 68.75%] [G loss: 0.557960]\n",
      "epoch:4 step:4033 [D loss: 0.179601, acc.: 71.88%] [G loss: 0.583184]\n",
      "epoch:4 step:4034 [D loss: 0.168568, acc.: 76.56%] [G loss: 0.590011]\n",
      "epoch:4 step:4035 [D loss: 0.216277, acc.: 68.75%] [G loss: 0.543316]\n",
      "epoch:4 step:4036 [D loss: 0.212769, acc.: 64.84%] [G loss: 0.534175]\n",
      "epoch:4 step:4037 [D loss: 0.170870, acc.: 73.44%] [G loss: 0.565582]\n",
      "epoch:4 step:4038 [D loss: 0.158821, acc.: 79.69%] [G loss: 0.624527]\n",
      "epoch:4 step:4039 [D loss: 0.205804, acc.: 67.19%] [G loss: 0.572002]\n",
      "epoch:4 step:4040 [D loss: 0.184348, acc.: 76.56%] [G loss: 0.569955]\n",
      "epoch:4 step:4041 [D loss: 0.180343, acc.: 75.78%] [G loss: 0.589672]\n",
      "epoch:4 step:4042 [D loss: 0.185390, acc.: 76.56%] [G loss: 0.561344]\n",
      "epoch:4 step:4043 [D loss: 0.191834, acc.: 71.88%] [G loss: 0.581520]\n",
      "epoch:4 step:4044 [D loss: 0.170278, acc.: 74.22%] [G loss: 0.592946]\n",
      "epoch:4 step:4045 [D loss: 0.197509, acc.: 71.09%] [G loss: 0.576979]\n",
      "epoch:4 step:4046 [D loss: 0.148752, acc.: 82.03%] [G loss: 0.632123]\n",
      "epoch:4 step:4047 [D loss: 0.166738, acc.: 77.34%] [G loss: 0.592476]\n",
      "epoch:4 step:4048 [D loss: 0.184795, acc.: 75.78%] [G loss: 0.556873]\n",
      "epoch:4 step:4049 [D loss: 0.251432, acc.: 60.94%] [G loss: 0.485952]\n",
      "epoch:4 step:4050 [D loss: 0.131105, acc.: 85.94%] [G loss: 0.585408]\n",
      "epoch:4 step:4051 [D loss: 0.152221, acc.: 82.03%] [G loss: 0.571046]\n",
      "epoch:4 step:4052 [D loss: 0.156622, acc.: 78.12%] [G loss: 0.675651]\n",
      "epoch:4 step:4053 [D loss: 0.193270, acc.: 74.22%] [G loss: 0.646170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4054 [D loss: 0.175511, acc.: 78.91%] [G loss: 0.592767]\n",
      "epoch:4 step:4055 [D loss: 0.177237, acc.: 74.22%] [G loss: 0.587366]\n",
      "epoch:4 step:4056 [D loss: 0.200226, acc.: 66.41%] [G loss: 0.569893]\n",
      "epoch:4 step:4057 [D loss: 0.152717, acc.: 77.34%] [G loss: 0.598925]\n",
      "epoch:4 step:4058 [D loss: 0.171604, acc.: 76.56%] [G loss: 0.598304]\n",
      "epoch:4 step:4059 [D loss: 0.160272, acc.: 76.56%] [G loss: 0.613239]\n",
      "epoch:4 step:4060 [D loss: 0.178286, acc.: 71.88%] [G loss: 0.655033]\n",
      "epoch:4 step:4061 [D loss: 0.148436, acc.: 82.03%] [G loss: 0.674805]\n",
      "epoch:4 step:4062 [D loss: 0.137622, acc.: 82.81%] [G loss: 0.670921]\n",
      "epoch:4 step:4063 [D loss: 0.145367, acc.: 82.03%] [G loss: 0.651722]\n",
      "epoch:4 step:4064 [D loss: 0.239008, acc.: 62.50%] [G loss: 0.528587]\n",
      "epoch:4 step:4065 [D loss: 0.186583, acc.: 74.22%] [G loss: 0.540619]\n",
      "epoch:4 step:4066 [D loss: 0.202124, acc.: 71.09%] [G loss: 0.527707]\n",
      "epoch:4 step:4067 [D loss: 0.190668, acc.: 71.88%] [G loss: 0.576058]\n",
      "epoch:4 step:4068 [D loss: 0.174682, acc.: 73.44%] [G loss: 0.551613]\n",
      "epoch:4 step:4069 [D loss: 0.152969, acc.: 77.34%] [G loss: 0.563589]\n",
      "epoch:4 step:4070 [D loss: 0.205061, acc.: 67.97%] [G loss: 0.562296]\n",
      "epoch:4 step:4071 [D loss: 0.213872, acc.: 63.28%] [G loss: 0.544171]\n",
      "epoch:4 step:4072 [D loss: 0.191000, acc.: 70.31%] [G loss: 0.584999]\n",
      "epoch:4 step:4073 [D loss: 0.160198, acc.: 75.78%] [G loss: 0.597270]\n",
      "epoch:4 step:4074 [D loss: 0.179170, acc.: 74.22%] [G loss: 0.605132]\n",
      "epoch:4 step:4075 [D loss: 0.186804, acc.: 74.22%] [G loss: 0.595933]\n",
      "epoch:4 step:4076 [D loss: 0.201271, acc.: 71.09%] [G loss: 0.556602]\n",
      "epoch:4 step:4077 [D loss: 0.154310, acc.: 75.00%] [G loss: 0.583800]\n",
      "epoch:4 step:4078 [D loss: 0.208682, acc.: 66.41%] [G loss: 0.575999]\n",
      "epoch:4 step:4079 [D loss: 0.157965, acc.: 78.12%] [G loss: 0.576657]\n",
      "epoch:4 step:4080 [D loss: 0.170199, acc.: 78.12%] [G loss: 0.582282]\n",
      "epoch:4 step:4081 [D loss: 0.168259, acc.: 76.56%] [G loss: 0.610662]\n",
      "epoch:4 step:4082 [D loss: 0.199552, acc.: 68.75%] [G loss: 0.565208]\n",
      "epoch:4 step:4083 [D loss: 0.162189, acc.: 75.00%] [G loss: 0.614802]\n",
      "epoch:4 step:4084 [D loss: 0.186121, acc.: 72.66%] [G loss: 0.604926]\n",
      "epoch:4 step:4085 [D loss: 0.155460, acc.: 80.47%] [G loss: 0.631501]\n",
      "epoch:4 step:4086 [D loss: 0.173622, acc.: 75.78%] [G loss: 0.618700]\n",
      "epoch:4 step:4087 [D loss: 0.173444, acc.: 78.12%] [G loss: 0.566601]\n",
      "epoch:4 step:4088 [D loss: 0.193460, acc.: 73.44%] [G loss: 0.584653]\n",
      "epoch:4 step:4089 [D loss: 0.223608, acc.: 61.72%] [G loss: 0.557456]\n",
      "epoch:4 step:4090 [D loss: 0.176981, acc.: 76.56%] [G loss: 0.575280]\n",
      "epoch:4 step:4091 [D loss: 0.145941, acc.: 80.47%] [G loss: 0.605940]\n",
      "epoch:4 step:4092 [D loss: 0.174633, acc.: 73.44%] [G loss: 0.675349]\n",
      "epoch:4 step:4093 [D loss: 0.166097, acc.: 78.91%] [G loss: 0.654344]\n",
      "epoch:4 step:4094 [D loss: 0.151239, acc.: 79.69%] [G loss: 0.647264]\n",
      "epoch:4 step:4095 [D loss: 0.160023, acc.: 75.78%] [G loss: 0.666626]\n",
      "epoch:4 step:4096 [D loss: 0.239450, acc.: 66.41%] [G loss: 0.553967]\n",
      "epoch:4 step:4097 [D loss: 0.256451, acc.: 53.91%] [G loss: 0.469180]\n",
      "epoch:4 step:4098 [D loss: 0.193124, acc.: 72.66%] [G loss: 0.495993]\n",
      "epoch:4 step:4099 [D loss: 0.190817, acc.: 67.19%] [G loss: 0.568979]\n",
      "epoch:4 step:4100 [D loss: 0.221430, acc.: 60.16%] [G loss: 0.584287]\n",
      "epoch:4 step:4101 [D loss: 0.196600, acc.: 68.75%] [G loss: 0.587785]\n",
      "epoch:4 step:4102 [D loss: 0.137747, acc.: 79.69%] [G loss: 0.630502]\n",
      "epoch:4 step:4103 [D loss: 0.181695, acc.: 77.34%] [G loss: 0.628188]\n",
      "epoch:4 step:4104 [D loss: 0.235328, acc.: 64.06%] [G loss: 0.560896]\n",
      "epoch:4 step:4105 [D loss: 0.163623, acc.: 75.00%] [G loss: 0.625450]\n",
      "epoch:4 step:4106 [D loss: 0.140474, acc.: 81.25%] [G loss: 0.638269]\n",
      "epoch:4 step:4107 [D loss: 0.163137, acc.: 79.69%] [G loss: 0.574463]\n",
      "epoch:4 step:4108 [D loss: 0.167553, acc.: 78.91%] [G loss: 0.604957]\n",
      "epoch:4 step:4109 [D loss: 0.173990, acc.: 74.22%] [G loss: 0.592297]\n",
      "epoch:4 step:4110 [D loss: 0.215876, acc.: 67.19%] [G loss: 0.499802]\n",
      "epoch:4 step:4111 [D loss: 0.170056, acc.: 74.22%] [G loss: 0.601389]\n",
      "epoch:4 step:4112 [D loss: 0.155170, acc.: 77.34%] [G loss: 0.640850]\n",
      "epoch:4 step:4113 [D loss: 0.165543, acc.: 77.34%] [G loss: 0.574719]\n",
      "epoch:4 step:4114 [D loss: 0.178477, acc.: 77.34%] [G loss: 0.592698]\n",
      "epoch:4 step:4115 [D loss: 0.178103, acc.: 78.12%] [G loss: 0.561935]\n",
      "epoch:4 step:4116 [D loss: 0.150849, acc.: 81.25%] [G loss: 0.597097]\n",
      "epoch:4 step:4117 [D loss: 0.210985, acc.: 69.53%] [G loss: 0.549908]\n",
      "epoch:4 step:4118 [D loss: 0.189613, acc.: 71.88%] [G loss: 0.584810]\n",
      "epoch:4 step:4119 [D loss: 0.153599, acc.: 81.25%] [G loss: 0.649006]\n",
      "epoch:4 step:4120 [D loss: 0.209111, acc.: 69.53%] [G loss: 0.547017]\n",
      "epoch:4 step:4121 [D loss: 0.235869, acc.: 62.50%] [G loss: 0.505041]\n",
      "epoch:4 step:4122 [D loss: 0.153340, acc.: 76.56%] [G loss: 0.583742]\n",
      "epoch:4 step:4123 [D loss: 0.195676, acc.: 67.97%] [G loss: 0.599021]\n",
      "epoch:4 step:4124 [D loss: 0.248881, acc.: 64.06%] [G loss: 0.471942]\n",
      "epoch:4 step:4125 [D loss: 0.205967, acc.: 75.00%] [G loss: 0.518398]\n",
      "epoch:4 step:4126 [D loss: 0.178427, acc.: 74.22%] [G loss: 0.590825]\n",
      "epoch:4 step:4127 [D loss: 0.188849, acc.: 70.31%] [G loss: 0.581717]\n",
      "epoch:4 step:4128 [D loss: 0.223064, acc.: 60.16%] [G loss: 0.558614]\n",
      "epoch:4 step:4129 [D loss: 0.163127, acc.: 79.69%] [G loss: 0.592223]\n",
      "epoch:4 step:4130 [D loss: 0.181400, acc.: 77.34%] [G loss: 0.579409]\n",
      "epoch:4 step:4131 [D loss: 0.185996, acc.: 71.09%] [G loss: 0.555697]\n",
      "epoch:4 step:4132 [D loss: 0.172252, acc.: 72.66%] [G loss: 0.552073]\n",
      "epoch:4 step:4133 [D loss: 0.164181, acc.: 79.69%] [G loss: 0.621226]\n",
      "epoch:4 step:4134 [D loss: 0.187993, acc.: 71.09%] [G loss: 0.576364]\n",
      "epoch:4 step:4135 [D loss: 0.223135, acc.: 66.41%] [G loss: 0.552486]\n",
      "epoch:4 step:4136 [D loss: 0.197019, acc.: 73.44%] [G loss: 0.556079]\n",
      "epoch:4 step:4137 [D loss: 0.187288, acc.: 71.09%] [G loss: 0.588821]\n",
      "epoch:4 step:4138 [D loss: 0.193773, acc.: 71.09%] [G loss: 0.561212]\n",
      "epoch:4 step:4139 [D loss: 0.184626, acc.: 71.09%] [G loss: 0.560268]\n",
      "epoch:4 step:4140 [D loss: 0.181086, acc.: 75.00%] [G loss: 0.549310]\n",
      "epoch:4 step:4141 [D loss: 0.209094, acc.: 68.75%] [G loss: 0.607121]\n",
      "epoch:4 step:4142 [D loss: 0.183192, acc.: 70.31%] [G loss: 0.539374]\n",
      "epoch:4 step:4143 [D loss: 0.202208, acc.: 67.19%] [G loss: 0.516001]\n",
      "epoch:4 step:4144 [D loss: 0.221397, acc.: 67.19%] [G loss: 0.565073]\n",
      "epoch:4 step:4145 [D loss: 0.155689, acc.: 78.12%] [G loss: 0.601506]\n",
      "epoch:4 step:4146 [D loss: 0.151365, acc.: 81.25%] [G loss: 0.669572]\n",
      "epoch:4 step:4147 [D loss: 0.158656, acc.: 81.25%] [G loss: 0.644755]\n",
      "epoch:4 step:4148 [D loss: 0.239065, acc.: 61.72%] [G loss: 0.541984]\n",
      "epoch:4 step:4149 [D loss: 0.230120, acc.: 64.06%] [G loss: 0.518345]\n",
      "epoch:4 step:4150 [D loss: 0.149216, acc.: 83.59%] [G loss: 0.588514]\n",
      "epoch:4 step:4151 [D loss: 0.201745, acc.: 70.31%] [G loss: 0.570683]\n",
      "epoch:4 step:4152 [D loss: 0.214405, acc.: 67.19%] [G loss: 0.570469]\n",
      "epoch:4 step:4153 [D loss: 0.178289, acc.: 75.00%] [G loss: 0.555124]\n",
      "epoch:4 step:4154 [D loss: 0.183293, acc.: 73.44%] [G loss: 0.613539]\n",
      "epoch:4 step:4155 [D loss: 0.225111, acc.: 62.50%] [G loss: 0.594280]\n",
      "epoch:4 step:4156 [D loss: 0.209715, acc.: 65.62%] [G loss: 0.524674]\n",
      "epoch:4 step:4157 [D loss: 0.185111, acc.: 71.09%] [G loss: 0.605924]\n",
      "epoch:4 step:4158 [D loss: 0.218584, acc.: 67.97%] [G loss: 0.545820]\n",
      "epoch:4 step:4159 [D loss: 0.195571, acc.: 71.09%] [G loss: 0.507130]\n",
      "epoch:4 step:4160 [D loss: 0.191752, acc.: 70.31%] [G loss: 0.522539]\n",
      "epoch:4 step:4161 [D loss: 0.189374, acc.: 71.09%] [G loss: 0.530754]\n",
      "epoch:4 step:4162 [D loss: 0.200247, acc.: 70.31%] [G loss: 0.508903]\n",
      "epoch:4 step:4163 [D loss: 0.236534, acc.: 67.97%] [G loss: 0.518760]\n",
      "epoch:4 step:4164 [D loss: 0.194497, acc.: 68.75%] [G loss: 0.619317]\n",
      "epoch:4 step:4165 [D loss: 0.206217, acc.: 69.53%] [G loss: 0.546902]\n",
      "epoch:4 step:4166 [D loss: 0.255019, acc.: 60.94%] [G loss: 0.496443]\n",
      "epoch:4 step:4167 [D loss: 0.211944, acc.: 61.72%] [G loss: 0.517520]\n",
      "epoch:4 step:4168 [D loss: 0.210295, acc.: 67.19%] [G loss: 0.612858]\n",
      "epoch:4 step:4169 [D loss: 0.212639, acc.: 64.84%] [G loss: 0.492699]\n",
      "epoch:4 step:4170 [D loss: 0.197087, acc.: 67.97%] [G loss: 0.529082]\n",
      "epoch:4 step:4171 [D loss: 0.196167, acc.: 68.75%] [G loss: 0.544647]\n",
      "epoch:4 step:4172 [D loss: 0.170176, acc.: 75.00%] [G loss: 0.579684]\n",
      "epoch:4 step:4173 [D loss: 0.202429, acc.: 67.19%] [G loss: 0.586173]\n",
      "epoch:4 step:4174 [D loss: 0.139220, acc.: 82.03%] [G loss: 0.618985]\n",
      "epoch:4 step:4175 [D loss: 0.152191, acc.: 82.03%] [G loss: 0.657896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4176 [D loss: 0.173927, acc.: 78.12%] [G loss: 0.598760]\n",
      "epoch:4 step:4177 [D loss: 0.154991, acc.: 82.03%] [G loss: 0.662410]\n",
      "epoch:4 step:4178 [D loss: 0.165906, acc.: 80.47%] [G loss: 0.634133]\n",
      "epoch:4 step:4179 [D loss: 0.200037, acc.: 68.75%] [G loss: 0.599626]\n",
      "epoch:4 step:4180 [D loss: 0.204283, acc.: 71.88%] [G loss: 0.539483]\n",
      "epoch:4 step:4181 [D loss: 0.196648, acc.: 71.88%] [G loss: 0.507362]\n",
      "epoch:4 step:4182 [D loss: 0.182573, acc.: 75.00%] [G loss: 0.558060]\n",
      "epoch:4 step:4183 [D loss: 0.189993, acc.: 71.88%] [G loss: 0.522296]\n",
      "epoch:4 step:4184 [D loss: 0.156052, acc.: 81.25%] [G loss: 0.596475]\n",
      "epoch:4 step:4185 [D loss: 0.291173, acc.: 53.91%] [G loss: 0.508214]\n",
      "epoch:4 step:4186 [D loss: 0.202722, acc.: 64.06%] [G loss: 0.536573]\n",
      "epoch:4 step:4187 [D loss: 0.190370, acc.: 70.31%] [G loss: 0.541493]\n",
      "epoch:4 step:4188 [D loss: 0.192676, acc.: 66.41%] [G loss: 0.607954]\n",
      "epoch:4 step:4189 [D loss: 0.229507, acc.: 63.28%] [G loss: 0.496136]\n",
      "epoch:4 step:4190 [D loss: 0.206507, acc.: 65.62%] [G loss: 0.613102]\n",
      "epoch:4 step:4191 [D loss: 0.215814, acc.: 67.19%] [G loss: 0.573596]\n",
      "epoch:4 step:4192 [D loss: 0.201528, acc.: 75.00%] [G loss: 0.562287]\n",
      "epoch:4 step:4193 [D loss: 0.146930, acc.: 80.47%] [G loss: 0.592990]\n",
      "epoch:4 step:4194 [D loss: 0.196165, acc.: 73.44%] [G loss: 0.572089]\n",
      "epoch:4 step:4195 [D loss: 0.212269, acc.: 67.19%] [G loss: 0.549570]\n",
      "epoch:4 step:4196 [D loss: 0.201961, acc.: 70.31%] [G loss: 0.628290]\n",
      "epoch:4 step:4197 [D loss: 0.196197, acc.: 68.75%] [G loss: 0.629805]\n",
      "epoch:4 step:4198 [D loss: 0.178917, acc.: 76.56%] [G loss: 0.631646]\n",
      "epoch:4 step:4199 [D loss: 0.148563, acc.: 80.47%] [G loss: 0.618927]\n",
      "epoch:4 step:4200 [D loss: 0.190556, acc.: 72.66%] [G loss: 0.611546]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.462911\n",
      "FID: 29.299128\n",
      "0 = 12.818963908028566\n",
      "1 = 0.0697883567214956\n",
      "2 = 0.9692000150680542\n",
      "3 = 0.9455000162124634\n",
      "4 = 0.992900013923645\n",
      "5 = 0.9925467371940613\n",
      "6 = 0.9455000162124634\n",
      "7 = 8.437885681486115\n",
      "8 = 0.11730645817688336\n",
      "9 = 0.8185999989509583\n",
      "10 = 0.8030999898910522\n",
      "11 = 0.8341000080108643\n",
      "12 = 0.8287925720214844\n",
      "13 = 0.8030999898910522\n",
      "14 = 6.462967872619629\n",
      "15 = 9.267950057983398\n",
      "16 = 0.18771561980247498\n",
      "17 = 6.462911128997803\n",
      "18 = 29.29912757873535\n",
      "epoch:4 step:4201 [D loss: 0.162496, acc.: 75.00%] [G loss: 0.603314]\n",
      "epoch:4 step:4202 [D loss: 0.197370, acc.: 75.00%] [G loss: 0.552703]\n",
      "epoch:4 step:4203 [D loss: 0.225776, acc.: 65.62%] [G loss: 0.551962]\n",
      "epoch:4 step:4204 [D loss: 0.225622, acc.: 61.72%] [G loss: 0.572846]\n",
      "epoch:4 step:4205 [D loss: 0.219072, acc.: 67.97%] [G loss: 0.555534]\n",
      "epoch:4 step:4206 [D loss: 0.215092, acc.: 69.53%] [G loss: 0.536029]\n",
      "epoch:4 step:4207 [D loss: 0.194786, acc.: 67.97%] [G loss: 0.471404]\n",
      "epoch:4 step:4208 [D loss: 0.199892, acc.: 67.97%] [G loss: 0.585685]\n",
      "epoch:4 step:4209 [D loss: 0.212146, acc.: 64.84%] [G loss: 0.529943]\n",
      "epoch:4 step:4210 [D loss: 0.213841, acc.: 65.62%] [G loss: 0.513116]\n",
      "epoch:4 step:4211 [D loss: 0.203737, acc.: 69.53%] [G loss: 0.563824]\n",
      "epoch:4 step:4212 [D loss: 0.177406, acc.: 75.00%] [G loss: 0.594835]\n",
      "epoch:4 step:4213 [D loss: 0.230355, acc.: 65.62%] [G loss: 0.542464]\n",
      "epoch:4 step:4214 [D loss: 0.199609, acc.: 72.66%] [G loss: 0.562771]\n",
      "epoch:4 step:4215 [D loss: 0.197007, acc.: 72.66%] [G loss: 0.570417]\n",
      "epoch:4 step:4216 [D loss: 0.217223, acc.: 67.97%] [G loss: 0.553662]\n",
      "epoch:4 step:4217 [D loss: 0.193319, acc.: 68.75%] [G loss: 0.583447]\n",
      "epoch:4 step:4218 [D loss: 0.184547, acc.: 68.75%] [G loss: 0.556159]\n",
      "epoch:4 step:4219 [D loss: 0.176937, acc.: 74.22%] [G loss: 0.626319]\n",
      "epoch:4 step:4220 [D loss: 0.168581, acc.: 79.69%] [G loss: 0.637881]\n",
      "epoch:4 step:4221 [D loss: 0.251937, acc.: 57.81%] [G loss: 0.531094]\n",
      "epoch:4 step:4222 [D loss: 0.164390, acc.: 77.34%] [G loss: 0.558400]\n",
      "epoch:4 step:4223 [D loss: 0.162388, acc.: 76.56%] [G loss: 0.582703]\n",
      "epoch:4 step:4224 [D loss: 0.213777, acc.: 63.28%] [G loss: 0.550145]\n",
      "epoch:4 step:4225 [D loss: 0.231371, acc.: 64.84%] [G loss: 0.555071]\n",
      "epoch:4 step:4226 [D loss: 0.225488, acc.: 66.41%] [G loss: 0.506537]\n",
      "epoch:4 step:4227 [D loss: 0.227612, acc.: 67.19%] [G loss: 0.530334]\n",
      "epoch:4 step:4228 [D loss: 0.217567, acc.: 62.50%] [G loss: 0.563096]\n",
      "epoch:4 step:4229 [D loss: 0.209042, acc.: 68.75%] [G loss: 0.535480]\n",
      "epoch:4 step:4230 [D loss: 0.274588, acc.: 51.56%] [G loss: 0.461614]\n",
      "epoch:4 step:4231 [D loss: 0.216513, acc.: 67.97%] [G loss: 0.537033]\n",
      "epoch:4 step:4232 [D loss: 0.165116, acc.: 78.12%] [G loss: 0.596305]\n",
      "epoch:4 step:4233 [D loss: 0.171178, acc.: 76.56%] [G loss: 0.604128]\n",
      "epoch:4 step:4234 [D loss: 0.232464, acc.: 63.28%] [G loss: 0.538766]\n",
      "epoch:4 step:4235 [D loss: 0.193957, acc.: 71.88%] [G loss: 0.515588]\n",
      "epoch:4 step:4236 [D loss: 0.205835, acc.: 64.84%] [G loss: 0.572380]\n",
      "epoch:4 step:4237 [D loss: 0.215939, acc.: 70.31%] [G loss: 0.566905]\n",
      "epoch:4 step:4238 [D loss: 0.212820, acc.: 65.62%] [G loss: 0.575970]\n",
      "epoch:4 step:4239 [D loss: 0.187295, acc.: 73.44%] [G loss: 0.578136]\n",
      "epoch:4 step:4240 [D loss: 0.205999, acc.: 71.09%] [G loss: 0.536855]\n",
      "epoch:4 step:4241 [D loss: 0.183215, acc.: 73.44%] [G loss: 0.541531]\n",
      "epoch:4 step:4242 [D loss: 0.176693, acc.: 74.22%] [G loss: 0.511537]\n",
      "epoch:4 step:4243 [D loss: 0.195591, acc.: 69.53%] [G loss: 0.628776]\n",
      "epoch:4 step:4244 [D loss: 0.208653, acc.: 67.19%] [G loss: 0.576524]\n",
      "epoch:4 step:4245 [D loss: 0.175656, acc.: 74.22%] [G loss: 0.628151]\n",
      "epoch:4 step:4246 [D loss: 0.162053, acc.: 77.34%] [G loss: 0.642954]\n",
      "epoch:4 step:4247 [D loss: 0.141843, acc.: 82.03%] [G loss: 0.631895]\n",
      "epoch:4 step:4248 [D loss: 0.240976, acc.: 58.59%] [G loss: 0.525642]\n",
      "epoch:4 step:4249 [D loss: 0.279620, acc.: 51.56%] [G loss: 0.457326]\n",
      "epoch:4 step:4250 [D loss: 0.191746, acc.: 69.53%] [G loss: 0.536212]\n",
      "epoch:4 step:4251 [D loss: 0.169611, acc.: 78.12%] [G loss: 0.554491]\n",
      "epoch:4 step:4252 [D loss: 0.160976, acc.: 79.69%] [G loss: 0.604018]\n",
      "epoch:4 step:4253 [D loss: 0.175895, acc.: 70.31%] [G loss: 0.531842]\n",
      "epoch:4 step:4254 [D loss: 0.176067, acc.: 75.00%] [G loss: 0.535756]\n",
      "epoch:4 step:4255 [D loss: 0.195108, acc.: 71.88%] [G loss: 0.570889]\n",
      "epoch:4 step:4256 [D loss: 0.162173, acc.: 78.12%] [G loss: 0.634128]\n",
      "epoch:4 step:4257 [D loss: 0.189334, acc.: 70.31%] [G loss: 0.567710]\n",
      "epoch:4 step:4258 [D loss: 0.182484, acc.: 72.66%] [G loss: 0.507089]\n",
      "epoch:4 step:4259 [D loss: 0.172720, acc.: 75.00%] [G loss: 0.578278]\n",
      "epoch:4 step:4260 [D loss: 0.210873, acc.: 67.19%] [G loss: 0.562105]\n",
      "epoch:4 step:4261 [D loss: 0.161885, acc.: 80.47%] [G loss: 0.591181]\n",
      "epoch:4 step:4262 [D loss: 0.181046, acc.: 71.09%] [G loss: 0.553575]\n",
      "epoch:4 step:4263 [D loss: 0.174885, acc.: 74.22%] [G loss: 0.608769]\n",
      "epoch:4 step:4264 [D loss: 0.171568, acc.: 68.75%] [G loss: 0.619174]\n",
      "epoch:4 step:4265 [D loss: 0.185910, acc.: 71.88%] [G loss: 0.567870]\n",
      "epoch:4 step:4266 [D loss: 0.172205, acc.: 75.00%] [G loss: 0.553756]\n",
      "epoch:4 step:4267 [D loss: 0.146953, acc.: 82.81%] [G loss: 0.623191]\n",
      "epoch:4 step:4268 [D loss: 0.180786, acc.: 74.22%] [G loss: 0.541255]\n",
      "epoch:4 step:4269 [D loss: 0.143887, acc.: 81.25%] [G loss: 0.614153]\n",
      "epoch:4 step:4270 [D loss: 0.177366, acc.: 71.09%] [G loss: 0.623076]\n",
      "epoch:4 step:4271 [D loss: 0.166033, acc.: 74.22%] [G loss: 0.612639]\n",
      "epoch:4 step:4272 [D loss: 0.216906, acc.: 64.84%] [G loss: 0.527926]\n",
      "epoch:4 step:4273 [D loss: 0.189624, acc.: 63.28%] [G loss: 0.542750]\n",
      "epoch:4 step:4274 [D loss: 0.157633, acc.: 83.59%] [G loss: 0.606691]\n",
      "epoch:4 step:4275 [D loss: 0.209000, acc.: 66.41%] [G loss: 0.593179]\n",
      "epoch:4 step:4276 [D loss: 0.242367, acc.: 57.81%] [G loss: 0.519591]\n",
      "epoch:4 step:4277 [D loss: 0.203109, acc.: 63.28%] [G loss: 0.552306]\n",
      "epoch:4 step:4278 [D loss: 0.181146, acc.: 73.44%] [G loss: 0.549776]\n",
      "epoch:4 step:4279 [D loss: 0.254874, acc.: 60.16%] [G loss: 0.514546]\n",
      "epoch:4 step:4280 [D loss: 0.197811, acc.: 71.88%] [G loss: 0.557719]\n",
      "epoch:4 step:4281 [D loss: 0.212648, acc.: 70.31%] [G loss: 0.559139]\n",
      "epoch:4 step:4282 [D loss: 0.154313, acc.: 79.69%] [G loss: 0.588618]\n",
      "epoch:4 step:4283 [D loss: 0.232872, acc.: 60.94%] [G loss: 0.509192]\n",
      "epoch:4 step:4284 [D loss: 0.166371, acc.: 78.12%] [G loss: 0.538261]\n",
      "epoch:4 step:4285 [D loss: 0.169064, acc.: 75.00%] [G loss: 0.597519]\n",
      "epoch:4 step:4286 [D loss: 0.206494, acc.: 69.53%] [G loss: 0.563429]\n",
      "epoch:4 step:4287 [D loss: 0.210515, acc.: 69.53%] [G loss: 0.546859]\n",
      "epoch:4 step:4288 [D loss: 0.208048, acc.: 71.88%] [G loss: 0.589927]\n",
      "epoch:4 step:4289 [D loss: 0.172640, acc.: 74.22%] [G loss: 0.559850]\n",
      "epoch:4 step:4290 [D loss: 0.211944, acc.: 64.06%] [G loss: 0.480089]\n",
      "epoch:4 step:4291 [D loss: 0.206822, acc.: 63.28%] [G loss: 0.533189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4292 [D loss: 0.199112, acc.: 68.75%] [G loss: 0.581325]\n",
      "epoch:4 step:4293 [D loss: 0.181074, acc.: 73.44%] [G loss: 0.608252]\n",
      "epoch:4 step:4294 [D loss: 0.190334, acc.: 71.88%] [G loss: 0.509813]\n",
      "epoch:4 step:4295 [D loss: 0.179593, acc.: 73.44%] [G loss: 0.506700]\n",
      "epoch:4 step:4296 [D loss: 0.175813, acc.: 73.44%] [G loss: 0.580218]\n",
      "epoch:4 step:4297 [D loss: 0.178951, acc.: 73.44%] [G loss: 0.613383]\n",
      "epoch:4 step:4298 [D loss: 0.175598, acc.: 73.44%] [G loss: 0.593063]\n",
      "epoch:4 step:4299 [D loss: 0.179700, acc.: 71.88%] [G loss: 0.621352]\n",
      "epoch:4 step:4300 [D loss: 0.186921, acc.: 70.31%] [G loss: 0.559121]\n",
      "epoch:4 step:4301 [D loss: 0.230467, acc.: 63.28%] [G loss: 0.489629]\n",
      "epoch:4 step:4302 [D loss: 0.174799, acc.: 75.00%] [G loss: 0.590030]\n",
      "epoch:4 step:4303 [D loss: 0.183612, acc.: 72.66%] [G loss: 0.574884]\n",
      "epoch:4 step:4304 [D loss: 0.168790, acc.: 77.34%] [G loss: 0.567450]\n",
      "epoch:4 step:4305 [D loss: 0.172148, acc.: 75.00%] [G loss: 0.549946]\n",
      "epoch:4 step:4306 [D loss: 0.141473, acc.: 83.59%] [G loss: 0.600594]\n",
      "epoch:4 step:4307 [D loss: 0.213713, acc.: 65.62%] [G loss: 0.553655]\n",
      "epoch:4 step:4308 [D loss: 0.198642, acc.: 74.22%] [G loss: 0.524294]\n",
      "epoch:4 step:4309 [D loss: 0.185690, acc.: 75.00%] [G loss: 0.610369]\n",
      "epoch:4 step:4310 [D loss: 0.234163, acc.: 67.19%] [G loss: 0.540605]\n",
      "epoch:4 step:4311 [D loss: 0.198053, acc.: 70.31%] [G loss: 0.555096]\n",
      "epoch:4 step:4312 [D loss: 0.158918, acc.: 77.34%] [G loss: 0.570844]\n",
      "epoch:4 step:4313 [D loss: 0.207580, acc.: 64.84%] [G loss: 0.538692]\n",
      "epoch:4 step:4314 [D loss: 0.234008, acc.: 63.28%] [G loss: 0.538292]\n",
      "epoch:4 step:4315 [D loss: 0.164548, acc.: 75.78%] [G loss: 0.582284]\n",
      "epoch:4 step:4316 [D loss: 0.185182, acc.: 71.09%] [G loss: 0.625099]\n",
      "epoch:4 step:4317 [D loss: 0.200941, acc.: 66.41%] [G loss: 0.562774]\n",
      "epoch:4 step:4318 [D loss: 0.182959, acc.: 65.62%] [G loss: 0.513595]\n",
      "epoch:4 step:4319 [D loss: 0.202523, acc.: 70.31%] [G loss: 0.534826]\n",
      "epoch:4 step:4320 [D loss: 0.189326, acc.: 74.22%] [G loss: 0.562696]\n",
      "epoch:4 step:4321 [D loss: 0.148863, acc.: 78.91%] [G loss: 0.590385]\n",
      "epoch:4 step:4322 [D loss: 0.168032, acc.: 73.44%] [G loss: 0.588613]\n",
      "epoch:4 step:4323 [D loss: 0.139271, acc.: 79.69%] [G loss: 0.640519]\n",
      "epoch:4 step:4324 [D loss: 0.225980, acc.: 60.94%] [G loss: 0.552952]\n",
      "epoch:4 step:4325 [D loss: 0.170300, acc.: 74.22%] [G loss: 0.579875]\n",
      "epoch:4 step:4326 [D loss: 0.197317, acc.: 69.53%] [G loss: 0.507899]\n",
      "epoch:4 step:4327 [D loss: 0.174483, acc.: 75.78%] [G loss: 0.569728]\n",
      "epoch:4 step:4328 [D loss: 0.207677, acc.: 67.97%] [G loss: 0.549128]\n",
      "epoch:4 step:4329 [D loss: 0.176958, acc.: 75.00%] [G loss: 0.595833]\n",
      "epoch:4 step:4330 [D loss: 0.148591, acc.: 81.25%] [G loss: 0.601646]\n",
      "epoch:4 step:4331 [D loss: 0.211513, acc.: 63.28%] [G loss: 0.557797]\n",
      "epoch:4 step:4332 [D loss: 0.242054, acc.: 60.16%] [G loss: 0.544576]\n",
      "epoch:4 step:4333 [D loss: 0.219100, acc.: 62.50%] [G loss: 0.557507]\n",
      "epoch:4 step:4334 [D loss: 0.220255, acc.: 60.94%] [G loss: 0.529258]\n",
      "epoch:4 step:4335 [D loss: 0.200181, acc.: 71.88%] [G loss: 0.563166]\n",
      "epoch:4 step:4336 [D loss: 0.197037, acc.: 74.22%] [G loss: 0.569507]\n",
      "epoch:4 step:4337 [D loss: 0.188481, acc.: 75.00%] [G loss: 0.553771]\n",
      "epoch:4 step:4338 [D loss: 0.206870, acc.: 67.97%] [G loss: 0.558729]\n",
      "epoch:4 step:4339 [D loss: 0.200973, acc.: 71.88%] [G loss: 0.588951]\n",
      "epoch:4 step:4340 [D loss: 0.197216, acc.: 71.88%] [G loss: 0.582753]\n",
      "epoch:4 step:4341 [D loss: 0.195244, acc.: 69.53%] [G loss: 0.550914]\n",
      "epoch:4 step:4342 [D loss: 0.204558, acc.: 70.31%] [G loss: 0.539209]\n",
      "epoch:4 step:4343 [D loss: 0.213337, acc.: 69.53%] [G loss: 0.548725]\n",
      "epoch:4 step:4344 [D loss: 0.209933, acc.: 69.53%] [G loss: 0.512204]\n",
      "epoch:4 step:4345 [D loss: 0.207637, acc.: 65.62%] [G loss: 0.517869]\n",
      "epoch:4 step:4346 [D loss: 0.167137, acc.: 74.22%] [G loss: 0.587162]\n",
      "epoch:4 step:4347 [D loss: 0.214141, acc.: 61.72%] [G loss: 0.513912]\n",
      "epoch:4 step:4348 [D loss: 0.247805, acc.: 60.16%] [G loss: 0.473123]\n",
      "epoch:4 step:4349 [D loss: 0.185913, acc.: 71.88%] [G loss: 0.579596]\n",
      "epoch:4 step:4350 [D loss: 0.178503, acc.: 74.22%] [G loss: 0.570682]\n",
      "epoch:4 step:4351 [D loss: 0.179713, acc.: 72.66%] [G loss: 0.619565]\n",
      "epoch:4 step:4352 [D loss: 0.182971, acc.: 75.78%] [G loss: 0.558650]\n",
      "epoch:4 step:4353 [D loss: 0.158687, acc.: 78.12%] [G loss: 0.573877]\n",
      "epoch:4 step:4354 [D loss: 0.195280, acc.: 74.22%] [G loss: 0.535989]\n",
      "epoch:4 step:4355 [D loss: 0.169744, acc.: 75.00%] [G loss: 0.588955]\n",
      "epoch:4 step:4356 [D loss: 0.163434, acc.: 79.69%] [G loss: 0.532395]\n",
      "epoch:4 step:4357 [D loss: 0.167527, acc.: 76.56%] [G loss: 0.592776]\n",
      "epoch:4 step:4358 [D loss: 0.200252, acc.: 70.31%] [G loss: 0.564445]\n",
      "epoch:4 step:4359 [D loss: 0.150088, acc.: 84.38%] [G loss: 0.635133]\n",
      "epoch:4 step:4360 [D loss: 0.209286, acc.: 71.88%] [G loss: 0.521998]\n",
      "epoch:4 step:4361 [D loss: 0.165726, acc.: 78.12%] [G loss: 0.610679]\n",
      "epoch:4 step:4362 [D loss: 0.194457, acc.: 71.09%] [G loss: 0.532109]\n",
      "epoch:4 step:4363 [D loss: 0.227644, acc.: 64.06%] [G loss: 0.472074]\n",
      "epoch:4 step:4364 [D loss: 0.173565, acc.: 73.44%] [G loss: 0.542201]\n",
      "epoch:4 step:4365 [D loss: 0.184411, acc.: 75.00%] [G loss: 0.553554]\n",
      "epoch:4 step:4366 [D loss: 0.185950, acc.: 75.00%] [G loss: 0.566469]\n",
      "epoch:4 step:4367 [D loss: 0.207954, acc.: 71.09%] [G loss: 0.567700]\n",
      "epoch:4 step:4368 [D loss: 0.180172, acc.: 75.78%] [G loss: 0.563256]\n",
      "epoch:4 step:4369 [D loss: 0.209671, acc.: 64.84%] [G loss: 0.571887]\n",
      "epoch:4 step:4370 [D loss: 0.272189, acc.: 53.91%] [G loss: 0.515931]\n",
      "epoch:4 step:4371 [D loss: 0.197537, acc.: 75.00%] [G loss: 0.572985]\n",
      "epoch:4 step:4372 [D loss: 0.169672, acc.: 78.12%] [G loss: 0.612105]\n",
      "epoch:4 step:4373 [D loss: 0.223842, acc.: 61.72%] [G loss: 0.550668]\n",
      "epoch:4 step:4374 [D loss: 0.221866, acc.: 64.84%] [G loss: 0.549537]\n",
      "epoch:4 step:4375 [D loss: 0.180580, acc.: 71.88%] [G loss: 0.553481]\n",
      "epoch:4 step:4376 [D loss: 0.209946, acc.: 70.31%] [G loss: 0.535428]\n",
      "epoch:4 step:4377 [D loss: 0.178104, acc.: 75.00%] [G loss: 0.557386]\n",
      "epoch:4 step:4378 [D loss: 0.193881, acc.: 68.75%] [G loss: 0.545811]\n",
      "epoch:4 step:4379 [D loss: 0.180091, acc.: 74.22%] [G loss: 0.590902]\n",
      "epoch:4 step:4380 [D loss: 0.172243, acc.: 71.88%] [G loss: 0.570585]\n",
      "epoch:4 step:4381 [D loss: 0.148571, acc.: 78.91%] [G loss: 0.646128]\n",
      "epoch:4 step:4382 [D loss: 0.170735, acc.: 75.00%] [G loss: 0.567203]\n",
      "epoch:4 step:4383 [D loss: 0.161339, acc.: 78.12%] [G loss: 0.573862]\n",
      "epoch:4 step:4384 [D loss: 0.183997, acc.: 72.66%] [G loss: 0.592269]\n",
      "epoch:4 step:4385 [D loss: 0.185751, acc.: 71.88%] [G loss: 0.545067]\n",
      "epoch:4 step:4386 [D loss: 0.166564, acc.: 72.66%] [G loss: 0.613006]\n",
      "epoch:4 step:4387 [D loss: 0.159439, acc.: 78.91%] [G loss: 0.578533]\n",
      "epoch:4 step:4388 [D loss: 0.185329, acc.: 75.78%] [G loss: 0.596733]\n",
      "epoch:4 step:4389 [D loss: 0.133864, acc.: 82.81%] [G loss: 0.647021]\n",
      "epoch:4 step:4390 [D loss: 0.161358, acc.: 76.56%] [G loss: 0.650181]\n",
      "epoch:4 step:4391 [D loss: 0.167537, acc.: 75.00%] [G loss: 0.606660]\n",
      "epoch:4 step:4392 [D loss: 0.213379, acc.: 63.28%] [G loss: 0.553098]\n",
      "epoch:4 step:4393 [D loss: 0.189691, acc.: 71.88%] [G loss: 0.573604]\n",
      "epoch:4 step:4394 [D loss: 0.180691, acc.: 76.56%] [G loss: 0.563672]\n",
      "epoch:4 step:4395 [D loss: 0.193003, acc.: 70.31%] [G loss: 0.603784]\n",
      "epoch:4 step:4396 [D loss: 0.169144, acc.: 79.69%] [G loss: 0.671733]\n",
      "epoch:4 step:4397 [D loss: 0.170817, acc.: 72.66%] [G loss: 0.678715]\n",
      "epoch:4 step:4398 [D loss: 0.197044, acc.: 71.09%] [G loss: 0.600540]\n",
      "epoch:4 step:4399 [D loss: 0.186809, acc.: 72.66%] [G loss: 0.548692]\n",
      "epoch:4 step:4400 [D loss: 0.185659, acc.: 73.44%] [G loss: 0.514015]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.125913\n",
      "FID: 36.038925\n",
      "0 = 12.794075510215725\n",
      "1 = 0.07387422605777177\n",
      "2 = 0.9700999855995178\n",
      "3 = 0.9462000131607056\n",
      "4 = 0.9940000176429749\n",
      "5 = 0.99369877576828\n",
      "6 = 0.9462000131607056\n",
      "7 = 8.91219914246206\n",
      "8 = 0.12941710077341098\n",
      "9 = 0.8282999992370605\n",
      "10 = 0.8148999810218811\n",
      "11 = 0.84170001745224\n",
      "12 = 0.8373407125473022\n",
      "13 = 0.8148999810218811\n",
      "14 = 6.125961780548096\n",
      "15 = 9.228361129760742\n",
      "16 = 0.20692268013954163\n",
      "17 = 6.125912666320801\n",
      "18 = 36.03892517089844\n",
      "epoch:4 step:4401 [D loss: 0.194235, acc.: 68.75%] [G loss: 0.569818]\n",
      "epoch:4 step:4402 [D loss: 0.169616, acc.: 75.00%] [G loss: 0.635229]\n",
      "epoch:4 step:4403 [D loss: 0.246770, acc.: 60.94%] [G loss: 0.536000]\n",
      "epoch:4 step:4404 [D loss: 0.188460, acc.: 75.00%] [G loss: 0.572998]\n",
      "epoch:4 step:4405 [D loss: 0.167514, acc.: 74.22%] [G loss: 0.571750]\n",
      "epoch:4 step:4406 [D loss: 0.220007, acc.: 64.06%] [G loss: 0.520496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4407 [D loss: 0.183383, acc.: 69.53%] [G loss: 0.519985]\n",
      "epoch:4 step:4408 [D loss: 0.169888, acc.: 79.69%] [G loss: 0.599926]\n",
      "epoch:4 step:4409 [D loss: 0.186964, acc.: 72.66%] [G loss: 0.583113]\n",
      "epoch:4 step:4410 [D loss: 0.199704, acc.: 70.31%] [G loss: 0.570136]\n",
      "epoch:4 step:4411 [D loss: 0.177232, acc.: 76.56%] [G loss: 0.566344]\n",
      "epoch:4 step:4412 [D loss: 0.180899, acc.: 75.00%] [G loss: 0.618514]\n",
      "epoch:4 step:4413 [D loss: 0.194118, acc.: 72.66%] [G loss: 0.575694]\n",
      "epoch:4 step:4414 [D loss: 0.203154, acc.: 69.53%] [G loss: 0.578797]\n",
      "epoch:4 step:4415 [D loss: 0.195102, acc.: 69.53%] [G loss: 0.583294]\n",
      "epoch:4 step:4416 [D loss: 0.193533, acc.: 71.09%] [G loss: 0.523705]\n",
      "epoch:4 step:4417 [D loss: 0.209014, acc.: 66.41%] [G loss: 0.540843]\n",
      "epoch:4 step:4418 [D loss: 0.186164, acc.: 71.09%] [G loss: 0.588796]\n",
      "epoch:4 step:4419 [D loss: 0.222473, acc.: 67.19%] [G loss: 0.567019]\n",
      "epoch:4 step:4420 [D loss: 0.219947, acc.: 64.06%] [G loss: 0.540233]\n",
      "epoch:4 step:4421 [D loss: 0.200656, acc.: 71.88%] [G loss: 0.577225]\n",
      "epoch:4 step:4422 [D loss: 0.179916, acc.: 70.31%] [G loss: 0.583634]\n",
      "epoch:4 step:4423 [D loss: 0.207005, acc.: 67.97%] [G loss: 0.543699]\n",
      "epoch:4 step:4424 [D loss: 0.204650, acc.: 71.88%] [G loss: 0.598609]\n",
      "epoch:4 step:4425 [D loss: 0.183693, acc.: 71.88%] [G loss: 0.565727]\n",
      "epoch:4 step:4426 [D loss: 0.207368, acc.: 64.84%] [G loss: 0.545658]\n",
      "epoch:4 step:4427 [D loss: 0.200940, acc.: 70.31%] [G loss: 0.531199]\n",
      "epoch:4 step:4428 [D loss: 0.206221, acc.: 67.97%] [G loss: 0.544379]\n",
      "epoch:4 step:4429 [D loss: 0.170356, acc.: 74.22%] [G loss: 0.560249]\n",
      "epoch:4 step:4430 [D loss: 0.198989, acc.: 67.19%] [G loss: 0.529218]\n",
      "epoch:4 step:4431 [D loss: 0.197203, acc.: 68.75%] [G loss: 0.551285]\n",
      "epoch:4 step:4432 [D loss: 0.190746, acc.: 70.31%] [G loss: 0.539255]\n",
      "epoch:4 step:4433 [D loss: 0.176275, acc.: 75.00%] [G loss: 0.580939]\n",
      "epoch:4 step:4434 [D loss: 0.204779, acc.: 71.09%] [G loss: 0.504237]\n",
      "epoch:4 step:4435 [D loss: 0.206957, acc.: 64.84%] [G loss: 0.560623]\n",
      "epoch:4 step:4436 [D loss: 0.186633, acc.: 68.75%] [G loss: 0.529779]\n",
      "epoch:4 step:4437 [D loss: 0.201486, acc.: 70.31%] [G loss: 0.564280]\n",
      "epoch:4 step:4438 [D loss: 0.163533, acc.: 77.34%] [G loss: 0.589092]\n",
      "epoch:4 step:4439 [D loss: 0.175140, acc.: 75.00%] [G loss: 0.661361]\n",
      "epoch:4 step:4440 [D loss: 0.193976, acc.: 71.09%] [G loss: 0.576834]\n",
      "epoch:4 step:4441 [D loss: 0.189568, acc.: 74.22%] [G loss: 0.587100]\n",
      "epoch:4 step:4442 [D loss: 0.168903, acc.: 80.47%] [G loss: 0.585379]\n",
      "epoch:4 step:4443 [D loss: 0.189542, acc.: 65.62%] [G loss: 0.551277]\n",
      "epoch:4 step:4444 [D loss: 0.204204, acc.: 67.97%] [G loss: 0.526673]\n",
      "epoch:4 step:4445 [D loss: 0.162691, acc.: 78.91%] [G loss: 0.568540]\n",
      "epoch:4 step:4446 [D loss: 0.211244, acc.: 71.88%] [G loss: 0.512458]\n",
      "epoch:4 step:4447 [D loss: 0.194794, acc.: 69.53%] [G loss: 0.570682]\n",
      "epoch:4 step:4448 [D loss: 0.181478, acc.: 72.66%] [G loss: 0.615701]\n",
      "epoch:4 step:4449 [D loss: 0.193106, acc.: 70.31%] [G loss: 0.611628]\n",
      "epoch:4 step:4450 [D loss: 0.250004, acc.: 63.28%] [G loss: 0.552557]\n",
      "epoch:4 step:4451 [D loss: 0.211651, acc.: 67.97%] [G loss: 0.571150]\n",
      "epoch:4 step:4452 [D loss: 0.213361, acc.: 73.44%] [G loss: 0.515542]\n",
      "epoch:4 step:4453 [D loss: 0.171584, acc.: 76.56%] [G loss: 0.580198]\n",
      "epoch:4 step:4454 [D loss: 0.168624, acc.: 75.78%] [G loss: 0.578913]\n",
      "epoch:4 step:4455 [D loss: 0.147930, acc.: 82.81%] [G loss: 0.618668]\n",
      "epoch:4 step:4456 [D loss: 0.206123, acc.: 69.53%] [G loss: 0.555713]\n",
      "epoch:4 step:4457 [D loss: 0.201440, acc.: 71.09%] [G loss: 0.590713]\n",
      "epoch:4 step:4458 [D loss: 0.214928, acc.: 69.53%] [G loss: 0.557298]\n",
      "epoch:4 step:4459 [D loss: 0.214517, acc.: 64.06%] [G loss: 0.537731]\n",
      "epoch:4 step:4460 [D loss: 0.191062, acc.: 73.44%] [G loss: 0.554389]\n",
      "epoch:4 step:4461 [D loss: 0.215640, acc.: 66.41%] [G loss: 0.522349]\n",
      "epoch:4 step:4462 [D loss: 0.196726, acc.: 71.09%] [G loss: 0.523490]\n",
      "epoch:4 step:4463 [D loss: 0.223673, acc.: 65.62%] [G loss: 0.563228]\n",
      "epoch:4 step:4464 [D loss: 0.233921, acc.: 62.50%] [G loss: 0.556527]\n",
      "epoch:4 step:4465 [D loss: 0.225087, acc.: 60.94%] [G loss: 0.522761]\n",
      "epoch:4 step:4466 [D loss: 0.224042, acc.: 62.50%] [G loss: 0.556449]\n",
      "epoch:4 step:4467 [D loss: 0.179161, acc.: 72.66%] [G loss: 0.588799]\n",
      "epoch:4 step:4468 [D loss: 0.228365, acc.: 60.94%] [G loss: 0.498929]\n",
      "epoch:4 step:4469 [D loss: 0.225159, acc.: 64.06%] [G loss: 0.568983]\n",
      "epoch:4 step:4470 [D loss: 0.206378, acc.: 68.75%] [G loss: 0.536991]\n",
      "epoch:4 step:4471 [D loss: 0.192216, acc.: 72.66%] [G loss: 0.548574]\n",
      "epoch:4 step:4472 [D loss: 0.237324, acc.: 64.84%] [G loss: 0.537635]\n",
      "epoch:4 step:4473 [D loss: 0.175453, acc.: 75.78%] [G loss: 0.537186]\n",
      "epoch:4 step:4474 [D loss: 0.205920, acc.: 65.62%] [G loss: 0.604487]\n",
      "epoch:4 step:4475 [D loss: 0.222062, acc.: 65.62%] [G loss: 0.531339]\n",
      "epoch:4 step:4476 [D loss: 0.184423, acc.: 71.09%] [G loss: 0.530567]\n",
      "epoch:4 step:4477 [D loss: 0.213333, acc.: 64.06%] [G loss: 0.515947]\n",
      "epoch:4 step:4478 [D loss: 0.170374, acc.: 78.12%] [G loss: 0.600919]\n",
      "epoch:4 step:4479 [D loss: 0.185039, acc.: 67.97%] [G loss: 0.600474]\n",
      "epoch:4 step:4480 [D loss: 0.174810, acc.: 75.78%] [G loss: 0.585855]\n",
      "epoch:4 step:4481 [D loss: 0.184616, acc.: 72.66%] [G loss: 0.570909]\n",
      "epoch:4 step:4482 [D loss: 0.200303, acc.: 71.88%] [G loss: 0.548124]\n",
      "epoch:4 step:4483 [D loss: 0.215128, acc.: 69.53%] [G loss: 0.522468]\n",
      "epoch:4 step:4484 [D loss: 0.171423, acc.: 74.22%] [G loss: 0.555491]\n",
      "epoch:4 step:4485 [D loss: 0.181188, acc.: 69.53%] [G loss: 0.606036]\n",
      "epoch:4 step:4486 [D loss: 0.228707, acc.: 65.62%] [G loss: 0.546226]\n",
      "epoch:4 step:4487 [D loss: 0.240411, acc.: 57.03%] [G loss: 0.518168]\n",
      "epoch:4 step:4488 [D loss: 0.180932, acc.: 72.66%] [G loss: 0.559037]\n",
      "epoch:4 step:4489 [D loss: 0.232330, acc.: 64.06%] [G loss: 0.500134]\n",
      "epoch:4 step:4490 [D loss: 0.205192, acc.: 69.53%] [G loss: 0.565646]\n",
      "epoch:4 step:4491 [D loss: 0.163799, acc.: 75.78%] [G loss: 0.565715]\n",
      "epoch:4 step:4492 [D loss: 0.201395, acc.: 66.41%] [G loss: 0.542097]\n",
      "epoch:4 step:4493 [D loss: 0.218224, acc.: 64.06%] [G loss: 0.535296]\n",
      "epoch:4 step:4494 [D loss: 0.177410, acc.: 76.56%] [G loss: 0.661618]\n",
      "epoch:4 step:4495 [D loss: 0.163914, acc.: 75.78%] [G loss: 0.626044]\n",
      "epoch:4 step:4496 [D loss: 0.190125, acc.: 70.31%] [G loss: 0.556299]\n",
      "epoch:4 step:4497 [D loss: 0.182775, acc.: 70.31%] [G loss: 0.536541]\n",
      "epoch:4 step:4498 [D loss: 0.193786, acc.: 70.31%] [G loss: 0.548420]\n",
      "epoch:4 step:4499 [D loss: 0.182649, acc.: 76.56%] [G loss: 0.595712]\n",
      "epoch:4 step:4500 [D loss: 0.193174, acc.: 71.09%] [G loss: 0.525415]\n",
      "epoch:4 step:4501 [D loss: 0.157904, acc.: 75.78%] [G loss: 0.598133]\n",
      "epoch:4 step:4502 [D loss: 0.142501, acc.: 82.03%] [G loss: 0.615328]\n",
      "epoch:4 step:4503 [D loss: 0.194228, acc.: 65.62%] [G loss: 0.512173]\n",
      "epoch:4 step:4504 [D loss: 0.161745, acc.: 77.34%] [G loss: 0.552302]\n",
      "epoch:4 step:4505 [D loss: 0.183535, acc.: 73.44%] [G loss: 0.574847]\n",
      "epoch:4 step:4506 [D loss: 0.193946, acc.: 71.09%] [G loss: 0.549129]\n",
      "epoch:4 step:4507 [D loss: 0.186879, acc.: 71.88%] [G loss: 0.556598]\n",
      "epoch:4 step:4508 [D loss: 0.203486, acc.: 70.31%] [G loss: 0.547213]\n",
      "epoch:4 step:4509 [D loss: 0.211411, acc.: 67.19%] [G loss: 0.573389]\n",
      "epoch:4 step:4510 [D loss: 0.187952, acc.: 71.88%] [G loss: 0.492570]\n",
      "epoch:4 step:4511 [D loss: 0.190661, acc.: 67.97%] [G loss: 0.594933]\n",
      "epoch:4 step:4512 [D loss: 0.200971, acc.: 69.53%] [G loss: 0.559783]\n",
      "epoch:4 step:4513 [D loss: 0.245515, acc.: 62.50%] [G loss: 0.512347]\n",
      "epoch:4 step:4514 [D loss: 0.227502, acc.: 60.94%] [G loss: 0.523015]\n",
      "epoch:4 step:4515 [D loss: 0.199076, acc.: 66.41%] [G loss: 0.596886]\n",
      "epoch:4 step:4516 [D loss: 0.279156, acc.: 54.69%] [G loss: 0.557444]\n",
      "epoch:4 step:4517 [D loss: 0.159251, acc.: 75.00%] [G loss: 0.647527]\n",
      "epoch:4 step:4518 [D loss: 0.197720, acc.: 72.66%] [G loss: 0.552703]\n",
      "epoch:4 step:4519 [D loss: 0.198192, acc.: 67.97%] [G loss: 0.563910]\n",
      "epoch:4 step:4520 [D loss: 0.223939, acc.: 62.50%] [G loss: 0.514046]\n",
      "epoch:4 step:4521 [D loss: 0.194033, acc.: 73.44%] [G loss: 0.522281]\n",
      "epoch:4 step:4522 [D loss: 0.248490, acc.: 57.81%] [G loss: 0.489118]\n",
      "epoch:4 step:4523 [D loss: 0.194288, acc.: 68.75%] [G loss: 0.588313]\n",
      "epoch:4 step:4524 [D loss: 0.200045, acc.: 68.75%] [G loss: 0.564468]\n",
      "epoch:4 step:4525 [D loss: 0.192235, acc.: 68.75%] [G loss: 0.594588]\n",
      "epoch:4 step:4526 [D loss: 0.198559, acc.: 67.97%] [G loss: 0.542125]\n",
      "epoch:4 step:4527 [D loss: 0.208062, acc.: 66.41%] [G loss: 0.519164]\n",
      "epoch:4 step:4528 [D loss: 0.195921, acc.: 67.97%] [G loss: 0.485273]\n",
      "epoch:4 step:4529 [D loss: 0.167429, acc.: 71.88%] [G loss: 0.566592]\n",
      "epoch:4 step:4530 [D loss: 0.189378, acc.: 70.31%] [G loss: 0.582359]\n",
      "epoch:4 step:4531 [D loss: 0.227110, acc.: 65.62%] [G loss: 0.538439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4532 [D loss: 0.231665, acc.: 60.94%] [G loss: 0.513702]\n",
      "epoch:4 step:4533 [D loss: 0.181315, acc.: 74.22%] [G loss: 0.567748]\n",
      "epoch:4 step:4534 [D loss: 0.155007, acc.: 82.03%] [G loss: 0.596696]\n",
      "epoch:4 step:4535 [D loss: 0.220383, acc.: 64.84%] [G loss: 0.480877]\n",
      "epoch:4 step:4536 [D loss: 0.218452, acc.: 64.84%] [G loss: 0.479104]\n",
      "epoch:4 step:4537 [D loss: 0.171419, acc.: 71.88%] [G loss: 0.530796]\n",
      "epoch:4 step:4538 [D loss: 0.200938, acc.: 69.53%] [G loss: 0.603849]\n",
      "epoch:4 step:4539 [D loss: 0.193443, acc.: 68.75%] [G loss: 0.529667]\n",
      "epoch:4 step:4540 [D loss: 0.151037, acc.: 78.91%] [G loss: 0.604102]\n",
      "epoch:4 step:4541 [D loss: 0.202727, acc.: 67.19%] [G loss: 0.541843]\n",
      "epoch:4 step:4542 [D loss: 0.206785, acc.: 64.06%] [G loss: 0.513915]\n",
      "epoch:4 step:4543 [D loss: 0.210562, acc.: 65.62%] [G loss: 0.527865]\n",
      "epoch:4 step:4544 [D loss: 0.200233, acc.: 69.53%] [G loss: 0.590777]\n",
      "epoch:4 step:4545 [D loss: 0.208156, acc.: 68.75%] [G loss: 0.549067]\n",
      "epoch:4 step:4546 [D loss: 0.174381, acc.: 75.00%] [G loss: 0.536453]\n",
      "epoch:4 step:4547 [D loss: 0.180104, acc.: 74.22%] [G loss: 0.530635]\n",
      "epoch:4 step:4548 [D loss: 0.179623, acc.: 71.88%] [G loss: 0.575445]\n",
      "epoch:4 step:4549 [D loss: 0.188869, acc.: 70.31%] [G loss: 0.585454]\n",
      "epoch:4 step:4550 [D loss: 0.166045, acc.: 80.47%] [G loss: 0.619409]\n",
      "epoch:4 step:4551 [D loss: 0.168658, acc.: 75.78%] [G loss: 0.598605]\n",
      "epoch:4 step:4552 [D loss: 0.198339, acc.: 69.53%] [G loss: 0.529599]\n",
      "epoch:4 step:4553 [D loss: 0.181441, acc.: 71.09%] [G loss: 0.553603]\n",
      "epoch:4 step:4554 [D loss: 0.177048, acc.: 71.88%] [G loss: 0.575126]\n",
      "epoch:4 step:4555 [D loss: 0.169911, acc.: 78.12%] [G loss: 0.573752]\n",
      "epoch:4 step:4556 [D loss: 0.229963, acc.: 59.38%] [G loss: 0.521129]\n",
      "epoch:4 step:4557 [D loss: 0.194015, acc.: 69.53%] [G loss: 0.553893]\n",
      "epoch:4 step:4558 [D loss: 0.190330, acc.: 69.53%] [G loss: 0.516993]\n",
      "epoch:4 step:4559 [D loss: 0.204363, acc.: 70.31%] [G loss: 0.556953]\n",
      "epoch:4 step:4560 [D loss: 0.227272, acc.: 64.84%] [G loss: 0.472013]\n",
      "epoch:4 step:4561 [D loss: 0.204159, acc.: 67.19%] [G loss: 0.512050]\n",
      "epoch:4 step:4562 [D loss: 0.204438, acc.: 71.09%] [G loss: 0.535462]\n",
      "epoch:4 step:4563 [D loss: 0.180331, acc.: 71.88%] [G loss: 0.599482]\n",
      "epoch:4 step:4564 [D loss: 0.224749, acc.: 61.72%] [G loss: 0.569690]\n",
      "epoch:4 step:4565 [D loss: 0.202756, acc.: 66.41%] [G loss: 0.559146]\n",
      "epoch:4 step:4566 [D loss: 0.196120, acc.: 71.09%] [G loss: 0.508744]\n",
      "epoch:4 step:4567 [D loss: 0.200789, acc.: 67.97%] [G loss: 0.568528]\n",
      "epoch:4 step:4568 [D loss: 0.210759, acc.: 70.31%] [G loss: 0.552593]\n",
      "epoch:4 step:4569 [D loss: 0.167419, acc.: 71.09%] [G loss: 0.578756]\n",
      "epoch:4 step:4570 [D loss: 0.155739, acc.: 82.03%] [G loss: 0.613171]\n",
      "epoch:4 step:4571 [D loss: 0.184700, acc.: 75.00%] [G loss: 0.581085]\n",
      "epoch:4 step:4572 [D loss: 0.242155, acc.: 60.16%] [G loss: 0.509509]\n",
      "epoch:4 step:4573 [D loss: 0.176780, acc.: 72.66%] [G loss: 0.531838]\n",
      "epoch:4 step:4574 [D loss: 0.219421, acc.: 68.75%] [G loss: 0.547527]\n",
      "epoch:4 step:4575 [D loss: 0.229152, acc.: 60.94%] [G loss: 0.511848]\n",
      "epoch:4 step:4576 [D loss: 0.235822, acc.: 61.72%] [G loss: 0.512542]\n",
      "epoch:4 step:4577 [D loss: 0.181039, acc.: 75.00%] [G loss: 0.624848]\n",
      "epoch:4 step:4578 [D loss: 0.193418, acc.: 68.75%] [G loss: 0.564797]\n",
      "epoch:4 step:4579 [D loss: 0.188689, acc.: 75.00%] [G loss: 0.550593]\n",
      "epoch:4 step:4580 [D loss: 0.179751, acc.: 75.78%] [G loss: 0.582133]\n",
      "epoch:4 step:4581 [D loss: 0.170656, acc.: 77.34%] [G loss: 0.566600]\n",
      "epoch:4 step:4582 [D loss: 0.180170, acc.: 71.88%] [G loss: 0.531547]\n",
      "epoch:4 step:4583 [D loss: 0.183887, acc.: 70.31%] [G loss: 0.528640]\n",
      "epoch:4 step:4584 [D loss: 0.193440, acc.: 67.97%] [G loss: 0.541847]\n",
      "epoch:4 step:4585 [D loss: 0.184949, acc.: 75.78%] [G loss: 0.534638]\n",
      "epoch:4 step:4586 [D loss: 0.190737, acc.: 70.31%] [G loss: 0.544472]\n",
      "epoch:4 step:4587 [D loss: 0.194693, acc.: 71.88%] [G loss: 0.546613]\n",
      "epoch:4 step:4588 [D loss: 0.195204, acc.: 72.66%] [G loss: 0.512948]\n",
      "epoch:4 step:4589 [D loss: 0.186027, acc.: 73.44%] [G loss: 0.527072]\n",
      "epoch:4 step:4590 [D loss: 0.180564, acc.: 71.88%] [G loss: 0.564045]\n",
      "epoch:4 step:4591 [D loss: 0.193124, acc.: 71.09%] [G loss: 0.555985]\n",
      "epoch:4 step:4592 [D loss: 0.160219, acc.: 77.34%] [G loss: 0.592480]\n",
      "epoch:4 step:4593 [D loss: 0.195549, acc.: 71.09%] [G loss: 0.566651]\n",
      "epoch:4 step:4594 [D loss: 0.199804, acc.: 64.84%] [G loss: 0.512174]\n",
      "epoch:4 step:4595 [D loss: 0.196073, acc.: 70.31%] [G loss: 0.507799]\n",
      "epoch:4 step:4596 [D loss: 0.182204, acc.: 73.44%] [G loss: 0.518081]\n",
      "epoch:4 step:4597 [D loss: 0.197373, acc.: 69.53%] [G loss: 0.489697]\n",
      "epoch:4 step:4598 [D loss: 0.218570, acc.: 62.50%] [G loss: 0.452873]\n",
      "epoch:4 step:4599 [D loss: 0.193331, acc.: 75.00%] [G loss: 0.543711]\n",
      "epoch:4 step:4600 [D loss: 0.169501, acc.: 74.22%] [G loss: 0.579724]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.435879\n",
      "FID: 33.077763\n",
      "0 = 12.670121651601807\n",
      "1 = 0.06991890842388766\n",
      "2 = 0.9606999754905701\n",
      "3 = 0.9355999827384949\n",
      "4 = 0.98580002784729\n",
      "5 = 0.9850494861602783\n",
      "6 = 0.9355999827384949\n",
      "7 = 8.591227079260396\n",
      "8 = 0.12681031631236547\n",
      "9 = 0.8093500137329102\n",
      "10 = 0.798799991607666\n",
      "11 = 0.8198999762535095\n",
      "12 = 0.8160179853439331\n",
      "13 = 0.798799991607666\n",
      "14 = 6.435927867889404\n",
      "15 = 9.231463432312012\n",
      "16 = 0.1935572475194931\n",
      "17 = 6.435879230499268\n",
      "18 = 33.077762603759766\n",
      "epoch:4 step:4601 [D loss: 0.200148, acc.: 71.09%] [G loss: 0.591213]\n",
      "epoch:4 step:4602 [D loss: 0.167877, acc.: 81.25%] [G loss: 0.593017]\n",
      "epoch:4 step:4603 [D loss: 0.214728, acc.: 63.28%] [G loss: 0.535686]\n",
      "epoch:4 step:4604 [D loss: 0.181942, acc.: 76.56%] [G loss: 0.555338]\n",
      "epoch:4 step:4605 [D loss: 0.191782, acc.: 73.44%] [G loss: 0.526659]\n",
      "epoch:4 step:4606 [D loss: 0.226983, acc.: 64.84%] [G loss: 0.558198]\n",
      "epoch:4 step:4607 [D loss: 0.236043, acc.: 63.28%] [G loss: 0.609574]\n",
      "epoch:4 step:4608 [D loss: 0.159323, acc.: 78.12%] [G loss: 0.656045]\n",
      "epoch:4 step:4609 [D loss: 0.217185, acc.: 68.75%] [G loss: 0.559084]\n",
      "epoch:4 step:4610 [D loss: 0.188035, acc.: 74.22%] [G loss: 0.491574]\n",
      "epoch:4 step:4611 [D loss: 0.195441, acc.: 67.97%] [G loss: 0.491006]\n",
      "epoch:4 step:4612 [D loss: 0.190144, acc.: 69.53%] [G loss: 0.557596]\n",
      "epoch:4 step:4613 [D loss: 0.210845, acc.: 66.41%] [G loss: 0.511473]\n",
      "epoch:4 step:4614 [D loss: 0.185378, acc.: 74.22%] [G loss: 0.539311]\n",
      "epoch:4 step:4615 [D loss: 0.212569, acc.: 63.28%] [G loss: 0.487258]\n",
      "epoch:4 step:4616 [D loss: 0.211115, acc.: 66.41%] [G loss: 0.528137]\n",
      "epoch:4 step:4617 [D loss: 0.193883, acc.: 72.66%] [G loss: 0.552873]\n",
      "epoch:4 step:4618 [D loss: 0.169015, acc.: 77.34%] [G loss: 0.598136]\n",
      "epoch:4 step:4619 [D loss: 0.176011, acc.: 75.00%] [G loss: 0.534253]\n",
      "epoch:4 step:4620 [D loss: 0.186252, acc.: 72.66%] [G loss: 0.496266]\n",
      "epoch:4 step:4621 [D loss: 0.226796, acc.: 67.97%] [G loss: 0.486483]\n",
      "epoch:4 step:4622 [D loss: 0.195775, acc.: 72.66%] [G loss: 0.543799]\n",
      "epoch:4 step:4623 [D loss: 0.143438, acc.: 82.81%] [G loss: 0.603479]\n",
      "epoch:4 step:4624 [D loss: 0.187836, acc.: 69.53%] [G loss: 0.580252]\n",
      "epoch:4 step:4625 [D loss: 0.184653, acc.: 75.00%] [G loss: 0.509077]\n",
      "epoch:4 step:4626 [D loss: 0.191760, acc.: 71.88%] [G loss: 0.573117]\n",
      "epoch:4 step:4627 [D loss: 0.199433, acc.: 71.09%] [G loss: 0.568333]\n",
      "epoch:4 step:4628 [D loss: 0.236433, acc.: 59.38%] [G loss: 0.497341]\n",
      "epoch:4 step:4629 [D loss: 0.192790, acc.: 70.31%] [G loss: 0.568466]\n",
      "epoch:4 step:4630 [D loss: 0.220769, acc.: 60.16%] [G loss: 0.546283]\n",
      "epoch:4 step:4631 [D loss: 0.226417, acc.: 63.28%] [G loss: 0.498631]\n",
      "epoch:4 step:4632 [D loss: 0.143147, acc.: 84.38%] [G loss: 0.636134]\n",
      "epoch:4 step:4633 [D loss: 0.188037, acc.: 81.25%] [G loss: 0.602337]\n",
      "epoch:4 step:4634 [D loss: 0.195827, acc.: 71.88%] [G loss: 0.622246]\n",
      "epoch:4 step:4635 [D loss: 0.191388, acc.: 70.31%] [G loss: 0.595779]\n",
      "epoch:4 step:4636 [D loss: 0.196803, acc.: 65.62%] [G loss: 0.545054]\n",
      "epoch:4 step:4637 [D loss: 0.189198, acc.: 75.00%] [G loss: 0.547514]\n",
      "epoch:4 step:4638 [D loss: 0.175248, acc.: 75.00%] [G loss: 0.576118]\n",
      "epoch:4 step:4639 [D loss: 0.217345, acc.: 65.62%] [G loss: 0.557467]\n",
      "epoch:4 step:4640 [D loss: 0.250931, acc.: 58.59%] [G loss: 0.492541]\n",
      "epoch:4 step:4641 [D loss: 0.170359, acc.: 79.69%] [G loss: 0.609084]\n",
      "epoch:4 step:4642 [D loss: 0.215012, acc.: 66.41%] [G loss: 0.592112]\n",
      "epoch:4 step:4643 [D loss: 0.216441, acc.: 67.19%] [G loss: 0.562427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4644 [D loss: 0.183827, acc.: 72.66%] [G loss: 0.564160]\n",
      "epoch:4 step:4645 [D loss: 0.184362, acc.: 71.09%] [G loss: 0.584406]\n",
      "epoch:4 step:4646 [D loss: 0.188644, acc.: 73.44%] [G loss: 0.580376]\n",
      "epoch:4 step:4647 [D loss: 0.162388, acc.: 78.12%] [G loss: 0.672467]\n",
      "epoch:4 step:4648 [D loss: 0.157234, acc.: 78.91%] [G loss: 0.659937]\n",
      "epoch:4 step:4649 [D loss: 0.211494, acc.: 66.41%] [G loss: 0.577298]\n",
      "epoch:4 step:4650 [D loss: 0.242885, acc.: 59.38%] [G loss: 0.528966]\n",
      "epoch:4 step:4651 [D loss: 0.185958, acc.: 68.75%] [G loss: 0.550451]\n",
      "epoch:4 step:4652 [D loss: 0.194961, acc.: 72.66%] [G loss: 0.557712]\n",
      "epoch:4 step:4653 [D loss: 0.196466, acc.: 68.75%] [G loss: 0.601921]\n",
      "epoch:4 step:4654 [D loss: 0.178193, acc.: 74.22%] [G loss: 0.579654]\n",
      "epoch:4 step:4655 [D loss: 0.218399, acc.: 66.41%] [G loss: 0.523320]\n",
      "epoch:4 step:4656 [D loss: 0.170672, acc.: 78.12%] [G loss: 0.590519]\n",
      "epoch:4 step:4657 [D loss: 0.151945, acc.: 81.25%] [G loss: 0.564043]\n",
      "epoch:4 step:4658 [D loss: 0.156305, acc.: 80.47%] [G loss: 0.634286]\n",
      "epoch:4 step:4659 [D loss: 0.184277, acc.: 70.31%] [G loss: 0.554335]\n",
      "epoch:4 step:4660 [D loss: 0.171268, acc.: 75.78%] [G loss: 0.624977]\n",
      "epoch:4 step:4661 [D loss: 0.212809, acc.: 64.84%] [G loss: 0.541957]\n",
      "epoch:4 step:4662 [D loss: 0.183983, acc.: 72.66%] [G loss: 0.591126]\n",
      "epoch:4 step:4663 [D loss: 0.268949, acc.: 55.47%] [G loss: 0.550541]\n",
      "epoch:4 step:4664 [D loss: 0.204279, acc.: 70.31%] [G loss: 0.559842]\n",
      "epoch:4 step:4665 [D loss: 0.186824, acc.: 71.88%] [G loss: 0.596558]\n",
      "epoch:4 step:4666 [D loss: 0.161802, acc.: 76.56%] [G loss: 0.668229]\n",
      "epoch:4 step:4667 [D loss: 0.181776, acc.: 72.66%] [G loss: 0.658366]\n",
      "epoch:4 step:4668 [D loss: 0.314387, acc.: 47.66%] [G loss: 0.489725]\n",
      "epoch:4 step:4669 [D loss: 0.142504, acc.: 84.38%] [G loss: 0.658319]\n",
      "epoch:4 step:4670 [D loss: 0.257944, acc.: 62.50%] [G loss: 0.524636]\n",
      "epoch:4 step:4671 [D loss: 0.183693, acc.: 72.66%] [G loss: 0.607952]\n",
      "epoch:4 step:4672 [D loss: 0.144927, acc.: 82.81%] [G loss: 0.687625]\n",
      "epoch:4 step:4673 [D loss: 0.151073, acc.: 79.69%] [G loss: 0.717608]\n",
      "epoch:4 step:4674 [D loss: 0.191583, acc.: 72.66%] [G loss: 0.692970]\n",
      "epoch:4 step:4675 [D loss: 0.135774, acc.: 81.25%] [G loss: 0.657666]\n",
      "epoch:4 step:4676 [D loss: 0.279272, acc.: 64.06%] [G loss: 0.559479]\n",
      "epoch:4 step:4677 [D loss: 0.163280, acc.: 75.78%] [G loss: 0.727676]\n",
      "epoch:4 step:4678 [D loss: 0.189811, acc.: 72.66%] [G loss: 0.635985]\n",
      "epoch:4 step:4679 [D loss: 0.202261, acc.: 67.19%] [G loss: 0.493512]\n",
      "epoch:4 step:4680 [D loss: 0.223506, acc.: 64.84%] [G loss: 0.524694]\n",
      "epoch:4 step:4681 [D loss: 0.177136, acc.: 75.78%] [G loss: 0.611241]\n",
      "epoch:4 step:4682 [D loss: 0.195038, acc.: 74.22%] [G loss: 0.583088]\n",
      "epoch:4 step:4683 [D loss: 0.191848, acc.: 73.44%] [G loss: 0.641715]\n",
      "epoch:4 step:4684 [D loss: 0.152108, acc.: 78.12%] [G loss: 0.697166]\n",
      "epoch:4 step:4685 [D loss: 0.107916, acc.: 88.28%] [G loss: 0.719163]\n",
      "epoch:5 step:4686 [D loss: 0.281908, acc.: 58.59%] [G loss: 0.566275]\n",
      "epoch:5 step:4687 [D loss: 0.244978, acc.: 63.28%] [G loss: 0.579779]\n",
      "epoch:5 step:4688 [D loss: 0.217112, acc.: 61.72%] [G loss: 0.558888]\n",
      "epoch:5 step:4689 [D loss: 0.186443, acc.: 70.31%] [G loss: 0.563642]\n",
      "epoch:5 step:4690 [D loss: 0.194606, acc.: 65.62%] [G loss: 0.529672]\n",
      "epoch:5 step:4691 [D loss: 0.192946, acc.: 69.53%] [G loss: 0.547825]\n",
      "epoch:5 step:4692 [D loss: 0.187949, acc.: 67.97%] [G loss: 0.555862]\n",
      "epoch:5 step:4693 [D loss: 0.198136, acc.: 68.75%] [G loss: 0.583324]\n",
      "epoch:5 step:4694 [D loss: 0.181392, acc.: 78.12%] [G loss: 0.596846]\n",
      "epoch:5 step:4695 [D loss: 0.205157, acc.: 67.97%] [G loss: 0.597011]\n",
      "epoch:5 step:4696 [D loss: 0.181792, acc.: 76.56%] [G loss: 0.584526]\n",
      "epoch:5 step:4697 [D loss: 0.148478, acc.: 83.59%] [G loss: 0.580364]\n",
      "epoch:5 step:4698 [D loss: 0.170791, acc.: 73.44%] [G loss: 0.532189]\n",
      "epoch:5 step:4699 [D loss: 0.180743, acc.: 73.44%] [G loss: 0.562473]\n",
      "epoch:5 step:4700 [D loss: 0.166197, acc.: 77.34%] [G loss: 0.587956]\n",
      "epoch:5 step:4701 [D loss: 0.181518, acc.: 70.31%] [G loss: 0.557485]\n",
      "epoch:5 step:4702 [D loss: 0.220481, acc.: 65.62%] [G loss: 0.531073]\n",
      "epoch:5 step:4703 [D loss: 0.210811, acc.: 67.19%] [G loss: 0.547282]\n",
      "epoch:5 step:4704 [D loss: 0.222080, acc.: 65.62%] [G loss: 0.542998]\n",
      "epoch:5 step:4705 [D loss: 0.238591, acc.: 60.94%] [G loss: 0.520600]\n",
      "epoch:5 step:4706 [D loss: 0.194796, acc.: 72.66%] [G loss: 0.581193]\n",
      "epoch:5 step:4707 [D loss: 0.193443, acc.: 71.09%] [G loss: 0.642690]\n",
      "epoch:5 step:4708 [D loss: 0.176179, acc.: 73.44%] [G loss: 0.604569]\n",
      "epoch:5 step:4709 [D loss: 0.170861, acc.: 76.56%] [G loss: 0.548064]\n",
      "epoch:5 step:4710 [D loss: 0.166968, acc.: 75.00%] [G loss: 0.558150]\n",
      "epoch:5 step:4711 [D loss: 0.202143, acc.: 68.75%] [G loss: 0.521925]\n",
      "epoch:5 step:4712 [D loss: 0.190221, acc.: 70.31%] [G loss: 0.548657]\n",
      "epoch:5 step:4713 [D loss: 0.167265, acc.: 76.56%] [G loss: 0.592474]\n",
      "epoch:5 step:4714 [D loss: 0.179251, acc.: 77.34%] [G loss: 0.574178]\n",
      "epoch:5 step:4715 [D loss: 0.234306, acc.: 60.94%] [G loss: 0.526738]\n",
      "epoch:5 step:4716 [D loss: 0.179794, acc.: 72.66%] [G loss: 0.546109]\n",
      "epoch:5 step:4717 [D loss: 0.240404, acc.: 63.28%] [G loss: 0.521680]\n",
      "epoch:5 step:4718 [D loss: 0.171762, acc.: 75.00%] [G loss: 0.549798]\n",
      "epoch:5 step:4719 [D loss: 0.205353, acc.: 68.75%] [G loss: 0.533569]\n",
      "epoch:5 step:4720 [D loss: 0.174929, acc.: 77.34%] [G loss: 0.563743]\n",
      "epoch:5 step:4721 [D loss: 0.164049, acc.: 78.12%] [G loss: 0.600526]\n",
      "epoch:5 step:4722 [D loss: 0.187089, acc.: 75.00%] [G loss: 0.598224]\n",
      "epoch:5 step:4723 [D loss: 0.210532, acc.: 71.09%] [G loss: 0.498592]\n",
      "epoch:5 step:4724 [D loss: 0.190355, acc.: 68.75%] [G loss: 0.551372]\n",
      "epoch:5 step:4725 [D loss: 0.158065, acc.: 75.78%] [G loss: 0.642305]\n",
      "epoch:5 step:4726 [D loss: 0.206483, acc.: 70.31%] [G loss: 0.585775]\n",
      "epoch:5 step:4727 [D loss: 0.167132, acc.: 75.00%] [G loss: 0.559956]\n",
      "epoch:5 step:4728 [D loss: 0.179646, acc.: 75.78%] [G loss: 0.551490]\n",
      "epoch:5 step:4729 [D loss: 0.193983, acc.: 69.53%] [G loss: 0.561035]\n",
      "epoch:5 step:4730 [D loss: 0.212388, acc.: 67.97%] [G loss: 0.503988]\n",
      "epoch:5 step:4731 [D loss: 0.192899, acc.: 67.19%] [G loss: 0.489425]\n",
      "epoch:5 step:4732 [D loss: 0.167044, acc.: 75.78%] [G loss: 0.622384]\n",
      "epoch:5 step:4733 [D loss: 0.174900, acc.: 75.78%] [G loss: 0.552049]\n",
      "epoch:5 step:4734 [D loss: 0.186952, acc.: 72.66%] [G loss: 0.547248]\n",
      "epoch:5 step:4735 [D loss: 0.196412, acc.: 71.88%] [G loss: 0.585516]\n",
      "epoch:5 step:4736 [D loss: 0.214384, acc.: 66.41%] [G loss: 0.534602]\n",
      "epoch:5 step:4737 [D loss: 0.185414, acc.: 75.00%] [G loss: 0.565748]\n",
      "epoch:5 step:4738 [D loss: 0.190675, acc.: 72.66%] [G loss: 0.585271]\n",
      "epoch:5 step:4739 [D loss: 0.209159, acc.: 71.09%] [G loss: 0.552126]\n",
      "epoch:5 step:4740 [D loss: 0.233208, acc.: 60.94%] [G loss: 0.550946]\n",
      "epoch:5 step:4741 [D loss: 0.169222, acc.: 75.78%] [G loss: 0.556111]\n",
      "epoch:5 step:4742 [D loss: 0.190804, acc.: 71.88%] [G loss: 0.553849]\n",
      "epoch:5 step:4743 [D loss: 0.211164, acc.: 66.41%] [G loss: 0.513470]\n",
      "epoch:5 step:4744 [D loss: 0.174791, acc.: 75.78%] [G loss: 0.550444]\n",
      "epoch:5 step:4745 [D loss: 0.196675, acc.: 72.66%] [G loss: 0.561240]\n",
      "epoch:5 step:4746 [D loss: 0.272371, acc.: 62.50%] [G loss: 0.522881]\n",
      "epoch:5 step:4747 [D loss: 0.188802, acc.: 70.31%] [G loss: 0.560471]\n",
      "epoch:5 step:4748 [D loss: 0.182528, acc.: 75.00%] [G loss: 0.534297]\n",
      "epoch:5 step:4749 [D loss: 0.206581, acc.: 70.31%] [G loss: 0.519637]\n",
      "epoch:5 step:4750 [D loss: 0.215321, acc.: 67.19%] [G loss: 0.548850]\n",
      "epoch:5 step:4751 [D loss: 0.201010, acc.: 67.19%] [G loss: 0.516739]\n",
      "epoch:5 step:4752 [D loss: 0.223468, acc.: 67.19%] [G loss: 0.533303]\n",
      "epoch:5 step:4753 [D loss: 0.159803, acc.: 78.91%] [G loss: 0.598901]\n",
      "epoch:5 step:4754 [D loss: 0.184458, acc.: 74.22%] [G loss: 0.567377]\n",
      "epoch:5 step:4755 [D loss: 0.173246, acc.: 75.00%] [G loss: 0.573470]\n",
      "epoch:5 step:4756 [D loss: 0.202712, acc.: 66.41%] [G loss: 0.528891]\n",
      "epoch:5 step:4757 [D loss: 0.186223, acc.: 71.88%] [G loss: 0.536027]\n",
      "epoch:5 step:4758 [D loss: 0.211320, acc.: 63.28%] [G loss: 0.489953]\n",
      "epoch:5 step:4759 [D loss: 0.155319, acc.: 77.34%] [G loss: 0.577859]\n",
      "epoch:5 step:4760 [D loss: 0.189772, acc.: 73.44%] [G loss: 0.569573]\n",
      "epoch:5 step:4761 [D loss: 0.184397, acc.: 73.44%] [G loss: 0.632026]\n",
      "epoch:5 step:4762 [D loss: 0.165550, acc.: 78.91%] [G loss: 0.606186]\n",
      "epoch:5 step:4763 [D loss: 0.231649, acc.: 64.84%] [G loss: 0.545155]\n",
      "epoch:5 step:4764 [D loss: 0.182351, acc.: 73.44%] [G loss: 0.528433]\n",
      "epoch:5 step:4765 [D loss: 0.210711, acc.: 67.97%] [G loss: 0.526317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4766 [D loss: 0.212202, acc.: 66.41%] [G loss: 0.516478]\n",
      "epoch:5 step:4767 [D loss: 0.207950, acc.: 71.09%] [G loss: 0.526695]\n",
      "epoch:5 step:4768 [D loss: 0.182903, acc.: 71.09%] [G loss: 0.568810]\n",
      "epoch:5 step:4769 [D loss: 0.197771, acc.: 74.22%] [G loss: 0.527712]\n",
      "epoch:5 step:4770 [D loss: 0.229075, acc.: 64.84%] [G loss: 0.508999]\n",
      "epoch:5 step:4771 [D loss: 0.188576, acc.: 72.66%] [G loss: 0.537196]\n",
      "epoch:5 step:4772 [D loss: 0.206939, acc.: 69.53%] [G loss: 0.503665]\n",
      "epoch:5 step:4773 [D loss: 0.197439, acc.: 71.09%] [G loss: 0.529065]\n",
      "epoch:5 step:4774 [D loss: 0.200882, acc.: 69.53%] [G loss: 0.531730]\n",
      "epoch:5 step:4775 [D loss: 0.157423, acc.: 76.56%] [G loss: 0.618120]\n",
      "epoch:5 step:4776 [D loss: 0.179988, acc.: 73.44%] [G loss: 0.540323]\n",
      "epoch:5 step:4777 [D loss: 0.205293, acc.: 67.97%] [G loss: 0.553214]\n",
      "epoch:5 step:4778 [D loss: 0.170887, acc.: 74.22%] [G loss: 0.566541]\n",
      "epoch:5 step:4779 [D loss: 0.183148, acc.: 73.44%] [G loss: 0.547362]\n",
      "epoch:5 step:4780 [D loss: 0.188675, acc.: 71.09%] [G loss: 0.528627]\n",
      "epoch:5 step:4781 [D loss: 0.168701, acc.: 72.66%] [G loss: 0.542398]\n",
      "epoch:5 step:4782 [D loss: 0.175418, acc.: 77.34%] [G loss: 0.583980]\n",
      "epoch:5 step:4783 [D loss: 0.182299, acc.: 67.97%] [G loss: 0.552533]\n",
      "epoch:5 step:4784 [D loss: 0.225033, acc.: 67.97%] [G loss: 0.504228]\n",
      "epoch:5 step:4785 [D loss: 0.177670, acc.: 75.00%] [G loss: 0.613135]\n",
      "epoch:5 step:4786 [D loss: 0.179850, acc.: 72.66%] [G loss: 0.584358]\n",
      "epoch:5 step:4787 [D loss: 0.216602, acc.: 64.06%] [G loss: 0.513856]\n",
      "epoch:5 step:4788 [D loss: 0.179602, acc.: 75.00%] [G loss: 0.533492]\n",
      "epoch:5 step:4789 [D loss: 0.187766, acc.: 77.34%] [G loss: 0.520859]\n",
      "epoch:5 step:4790 [D loss: 0.180306, acc.: 74.22%] [G loss: 0.514431]\n",
      "epoch:5 step:4791 [D loss: 0.182889, acc.: 72.66%] [G loss: 0.551051]\n",
      "epoch:5 step:4792 [D loss: 0.223790, acc.: 62.50%] [G loss: 0.595312]\n",
      "epoch:5 step:4793 [D loss: 0.242384, acc.: 63.28%] [G loss: 0.583473]\n",
      "epoch:5 step:4794 [D loss: 0.214047, acc.: 64.84%] [G loss: 0.512673]\n",
      "epoch:5 step:4795 [D loss: 0.218208, acc.: 63.28%] [G loss: 0.517382]\n",
      "epoch:5 step:4796 [D loss: 0.172529, acc.: 74.22%] [G loss: 0.594340]\n",
      "epoch:5 step:4797 [D loss: 0.173498, acc.: 75.78%] [G loss: 0.528279]\n",
      "epoch:5 step:4798 [D loss: 0.232753, acc.: 62.50%] [G loss: 0.532960]\n",
      "epoch:5 step:4799 [D loss: 0.219347, acc.: 65.62%] [G loss: 0.549990]\n",
      "epoch:5 step:4800 [D loss: 0.184400, acc.: 71.88%] [G loss: 0.606058]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.506724\n",
      "FID: 26.249922\n",
      "0 = 12.526030621147173\n",
      "1 = 0.0629816569312228\n",
      "2 = 0.9575499892234802\n",
      "3 = 0.9315000176429749\n",
      "4 = 0.9836000204086304\n",
      "5 = 0.9826986193656921\n",
      "6 = 0.9315000176429749\n",
      "7 = 8.240002188965665\n",
      "8 = 0.11004867170503681\n",
      "9 = 0.8029000163078308\n",
      "10 = 0.7903000116348267\n",
      "11 = 0.815500020980835\n",
      "12 = 0.8107303977012634\n",
      "13 = 0.7903000116348267\n",
      "14 = 6.506779670715332\n",
      "15 = 9.247654914855957\n",
      "16 = 0.18960040807724\n",
      "17 = 6.506723880767822\n",
      "18 = 26.249921798706055\n",
      "epoch:5 step:4801 [D loss: 0.201958, acc.: 66.41%] [G loss: 0.576418]\n",
      "epoch:5 step:4802 [D loss: 0.203445, acc.: 70.31%] [G loss: 0.593204]\n",
      "epoch:5 step:4803 [D loss: 0.200508, acc.: 67.97%] [G loss: 0.571582]\n",
      "epoch:5 step:4804 [D loss: 0.148540, acc.: 82.03%] [G loss: 0.633254]\n",
      "epoch:5 step:4805 [D loss: 0.245282, acc.: 62.50%] [G loss: 0.516670]\n",
      "epoch:5 step:4806 [D loss: 0.217452, acc.: 71.09%] [G loss: 0.503215]\n",
      "epoch:5 step:4807 [D loss: 0.152073, acc.: 83.59%] [G loss: 0.540759]\n",
      "epoch:5 step:4808 [D loss: 0.194834, acc.: 70.31%] [G loss: 0.599923]\n",
      "epoch:5 step:4809 [D loss: 0.220258, acc.: 64.06%] [G loss: 0.541904]\n",
      "epoch:5 step:4810 [D loss: 0.192033, acc.: 70.31%] [G loss: 0.540546]\n",
      "epoch:5 step:4811 [D loss: 0.176897, acc.: 77.34%] [G loss: 0.526305]\n",
      "epoch:5 step:4812 [D loss: 0.198600, acc.: 64.06%] [G loss: 0.526365]\n",
      "epoch:5 step:4813 [D loss: 0.177021, acc.: 74.22%] [G loss: 0.518058]\n",
      "epoch:5 step:4814 [D loss: 0.227422, acc.: 64.06%] [G loss: 0.511366]\n",
      "epoch:5 step:4815 [D loss: 0.188185, acc.: 74.22%] [G loss: 0.585156]\n",
      "epoch:5 step:4816 [D loss: 0.218279, acc.: 66.41%] [G loss: 0.542447]\n",
      "epoch:5 step:4817 [D loss: 0.195054, acc.: 68.75%] [G loss: 0.569188]\n",
      "epoch:5 step:4818 [D loss: 0.250475, acc.: 58.59%] [G loss: 0.482279]\n",
      "epoch:5 step:4819 [D loss: 0.188031, acc.: 72.66%] [G loss: 0.579040]\n",
      "epoch:5 step:4820 [D loss: 0.205565, acc.: 72.66%] [G loss: 0.533136]\n",
      "epoch:5 step:4821 [D loss: 0.208630, acc.: 64.84%] [G loss: 0.572006]\n",
      "epoch:5 step:4822 [D loss: 0.219369, acc.: 66.41%] [G loss: 0.478611]\n",
      "epoch:5 step:4823 [D loss: 0.231928, acc.: 64.06%] [G loss: 0.487139]\n",
      "epoch:5 step:4824 [D loss: 0.207039, acc.: 66.41%] [G loss: 0.546735]\n",
      "epoch:5 step:4825 [D loss: 0.200217, acc.: 71.09%] [G loss: 0.526346]\n",
      "epoch:5 step:4826 [D loss: 0.176953, acc.: 79.69%] [G loss: 0.549341]\n",
      "epoch:5 step:4827 [D loss: 0.218482, acc.: 70.31%] [G loss: 0.483413]\n",
      "epoch:5 step:4828 [D loss: 0.235536, acc.: 60.16%] [G loss: 0.494088]\n",
      "epoch:5 step:4829 [D loss: 0.167328, acc.: 78.12%] [G loss: 0.573650]\n",
      "epoch:5 step:4830 [D loss: 0.213230, acc.: 64.84%] [G loss: 0.496622]\n",
      "epoch:5 step:4831 [D loss: 0.187248, acc.: 67.97%] [G loss: 0.583356]\n",
      "epoch:5 step:4832 [D loss: 0.223395, acc.: 69.53%] [G loss: 0.536331]\n",
      "epoch:5 step:4833 [D loss: 0.236351, acc.: 67.19%] [G loss: 0.482631]\n",
      "epoch:5 step:4834 [D loss: 0.184154, acc.: 71.88%] [G loss: 0.548206]\n",
      "epoch:5 step:4835 [D loss: 0.218869, acc.: 64.06%] [G loss: 0.518075]\n",
      "epoch:5 step:4836 [D loss: 0.170982, acc.: 74.22%] [G loss: 0.533978]\n",
      "epoch:5 step:4837 [D loss: 0.158174, acc.: 75.78%] [G loss: 0.573493]\n",
      "epoch:5 step:4838 [D loss: 0.237362, acc.: 60.94%] [G loss: 0.540971]\n",
      "epoch:5 step:4839 [D loss: 0.195532, acc.: 67.19%] [G loss: 0.566705]\n",
      "epoch:5 step:4840 [D loss: 0.190783, acc.: 66.41%] [G loss: 0.532925]\n",
      "epoch:5 step:4841 [D loss: 0.180237, acc.: 74.22%] [G loss: 0.642420]\n",
      "epoch:5 step:4842 [D loss: 0.239376, acc.: 59.38%] [G loss: 0.540581]\n",
      "epoch:5 step:4843 [D loss: 0.200203, acc.: 70.31%] [G loss: 0.522306]\n",
      "epoch:5 step:4844 [D loss: 0.205510, acc.: 71.88%] [G loss: 0.524184]\n",
      "epoch:5 step:4845 [D loss: 0.226969, acc.: 63.28%] [G loss: 0.535510]\n",
      "epoch:5 step:4846 [D loss: 0.180532, acc.: 73.44%] [G loss: 0.600807]\n",
      "epoch:5 step:4847 [D loss: 0.159882, acc.: 78.12%] [G loss: 0.629672]\n",
      "epoch:5 step:4848 [D loss: 0.203605, acc.: 72.66%] [G loss: 0.532566]\n",
      "epoch:5 step:4849 [D loss: 0.196432, acc.: 67.97%] [G loss: 0.524449]\n",
      "epoch:5 step:4850 [D loss: 0.167579, acc.: 75.78%] [G loss: 0.554585]\n",
      "epoch:5 step:4851 [D loss: 0.201100, acc.: 66.41%] [G loss: 0.544044]\n",
      "epoch:5 step:4852 [D loss: 0.185126, acc.: 71.09%] [G loss: 0.552259]\n",
      "epoch:5 step:4853 [D loss: 0.195462, acc.: 67.19%] [G loss: 0.545854]\n",
      "epoch:5 step:4854 [D loss: 0.219219, acc.: 64.06%] [G loss: 0.548393]\n",
      "epoch:5 step:4855 [D loss: 0.217502, acc.: 65.62%] [G loss: 0.539482]\n",
      "epoch:5 step:4856 [D loss: 0.187643, acc.: 72.66%] [G loss: 0.513614]\n",
      "epoch:5 step:4857 [D loss: 0.175348, acc.: 71.09%] [G loss: 0.601225]\n",
      "epoch:5 step:4858 [D loss: 0.201975, acc.: 71.88%] [G loss: 0.529930]\n",
      "epoch:5 step:4859 [D loss: 0.231022, acc.: 64.84%] [G loss: 0.535909]\n",
      "epoch:5 step:4860 [D loss: 0.212020, acc.: 67.19%] [G loss: 0.526106]\n",
      "epoch:5 step:4861 [D loss: 0.192007, acc.: 67.97%] [G loss: 0.563410]\n",
      "epoch:5 step:4862 [D loss: 0.200986, acc.: 70.31%] [G loss: 0.532614]\n",
      "epoch:5 step:4863 [D loss: 0.204169, acc.: 71.88%] [G loss: 0.558575]\n",
      "epoch:5 step:4864 [D loss: 0.177475, acc.: 71.88%] [G loss: 0.576774]\n",
      "epoch:5 step:4865 [D loss: 0.213266, acc.: 67.97%] [G loss: 0.509192]\n",
      "epoch:5 step:4866 [D loss: 0.185203, acc.: 71.88%] [G loss: 0.602995]\n",
      "epoch:5 step:4867 [D loss: 0.241306, acc.: 67.19%] [G loss: 0.531870]\n",
      "epoch:5 step:4868 [D loss: 0.235603, acc.: 57.03%] [G loss: 0.548013]\n",
      "epoch:5 step:4869 [D loss: 0.211349, acc.: 66.41%] [G loss: 0.569738]\n",
      "epoch:5 step:4870 [D loss: 0.200689, acc.: 67.97%] [G loss: 0.520158]\n",
      "epoch:5 step:4871 [D loss: 0.215694, acc.: 69.53%] [G loss: 0.543798]\n",
      "epoch:5 step:4872 [D loss: 0.189971, acc.: 71.09%] [G loss: 0.592594]\n",
      "epoch:5 step:4873 [D loss: 0.228627, acc.: 63.28%] [G loss: 0.520249]\n",
      "epoch:5 step:4874 [D loss: 0.218705, acc.: 67.19%] [G loss: 0.519174]\n",
      "epoch:5 step:4875 [D loss: 0.198151, acc.: 69.53%] [G loss: 0.546863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4876 [D loss: 0.153348, acc.: 82.03%] [G loss: 0.577057]\n",
      "epoch:5 step:4877 [D loss: 0.195132, acc.: 64.84%] [G loss: 0.525712]\n",
      "epoch:5 step:4878 [D loss: 0.175350, acc.: 73.44%] [G loss: 0.584077]\n",
      "epoch:5 step:4879 [D loss: 0.202677, acc.: 67.97%] [G loss: 0.559475]\n",
      "epoch:5 step:4880 [D loss: 0.192315, acc.: 66.41%] [G loss: 0.556587]\n",
      "epoch:5 step:4881 [D loss: 0.209965, acc.: 65.62%] [G loss: 0.514372]\n",
      "epoch:5 step:4882 [D loss: 0.177581, acc.: 69.53%] [G loss: 0.586791]\n",
      "epoch:5 step:4883 [D loss: 0.166506, acc.: 77.34%] [G loss: 0.574225]\n",
      "epoch:5 step:4884 [D loss: 0.188773, acc.: 75.00%] [G loss: 0.522474]\n",
      "epoch:5 step:4885 [D loss: 0.249039, acc.: 61.72%] [G loss: 0.529562]\n",
      "epoch:5 step:4886 [D loss: 0.210572, acc.: 67.97%] [G loss: 0.527972]\n",
      "epoch:5 step:4887 [D loss: 0.211120, acc.: 66.41%] [G loss: 0.538330]\n",
      "epoch:5 step:4888 [D loss: 0.225990, acc.: 64.06%] [G loss: 0.505747]\n",
      "epoch:5 step:4889 [D loss: 0.200268, acc.: 67.97%] [G loss: 0.527846]\n",
      "epoch:5 step:4890 [D loss: 0.162449, acc.: 79.69%] [G loss: 0.548631]\n",
      "epoch:5 step:4891 [D loss: 0.186867, acc.: 69.53%] [G loss: 0.585347]\n",
      "epoch:5 step:4892 [D loss: 0.155205, acc.: 77.34%] [G loss: 0.586425]\n",
      "epoch:5 step:4893 [D loss: 0.164032, acc.: 72.66%] [G loss: 0.594779]\n",
      "epoch:5 step:4894 [D loss: 0.155242, acc.: 78.12%] [G loss: 0.570906]\n",
      "epoch:5 step:4895 [D loss: 0.218240, acc.: 64.84%] [G loss: 0.502648]\n",
      "epoch:5 step:4896 [D loss: 0.234850, acc.: 58.59%] [G loss: 0.495796]\n",
      "epoch:5 step:4897 [D loss: 0.197570, acc.: 74.22%] [G loss: 0.520404]\n",
      "epoch:5 step:4898 [D loss: 0.191784, acc.: 73.44%] [G loss: 0.519845]\n",
      "epoch:5 step:4899 [D loss: 0.257643, acc.: 55.47%] [G loss: 0.509786]\n",
      "epoch:5 step:4900 [D loss: 0.214237, acc.: 67.97%] [G loss: 0.561204]\n",
      "epoch:5 step:4901 [D loss: 0.217256, acc.: 64.84%] [G loss: 0.522908]\n",
      "epoch:5 step:4902 [D loss: 0.166430, acc.: 81.25%] [G loss: 0.578458]\n",
      "epoch:5 step:4903 [D loss: 0.187924, acc.: 69.53%] [G loss: 0.591943]\n",
      "epoch:5 step:4904 [D loss: 0.177264, acc.: 73.44%] [G loss: 0.621680]\n",
      "epoch:5 step:4905 [D loss: 0.261213, acc.: 57.03%] [G loss: 0.465154]\n",
      "epoch:5 step:4906 [D loss: 0.175289, acc.: 75.78%] [G loss: 0.627935]\n",
      "epoch:5 step:4907 [D loss: 0.162810, acc.: 71.09%] [G loss: 0.612469]\n",
      "epoch:5 step:4908 [D loss: 0.160489, acc.: 75.78%] [G loss: 0.572448]\n",
      "epoch:5 step:4909 [D loss: 0.217522, acc.: 66.41%] [G loss: 0.555147]\n",
      "epoch:5 step:4910 [D loss: 0.243303, acc.: 61.72%] [G loss: 0.511767]\n",
      "epoch:5 step:4911 [D loss: 0.223127, acc.: 64.06%] [G loss: 0.488546]\n",
      "epoch:5 step:4912 [D loss: 0.211907, acc.: 73.44%] [G loss: 0.476749]\n",
      "epoch:5 step:4913 [D loss: 0.213728, acc.: 68.75%] [G loss: 0.527565]\n",
      "epoch:5 step:4914 [D loss: 0.171149, acc.: 76.56%] [G loss: 0.542726]\n",
      "epoch:5 step:4915 [D loss: 0.179103, acc.: 72.66%] [G loss: 0.544229]\n",
      "epoch:5 step:4916 [D loss: 0.168447, acc.: 77.34%] [G loss: 0.600265]\n",
      "epoch:5 step:4917 [D loss: 0.150254, acc.: 81.25%] [G loss: 0.638463]\n",
      "epoch:5 step:4918 [D loss: 0.223961, acc.: 63.28%] [G loss: 0.595978]\n",
      "epoch:5 step:4919 [D loss: 0.171698, acc.: 71.88%] [G loss: 0.552928]\n",
      "epoch:5 step:4920 [D loss: 0.185625, acc.: 74.22%] [G loss: 0.589241]\n",
      "epoch:5 step:4921 [D loss: 0.173522, acc.: 75.00%] [G loss: 0.571430]\n",
      "epoch:5 step:4922 [D loss: 0.221733, acc.: 63.28%] [G loss: 0.517784]\n",
      "epoch:5 step:4923 [D loss: 0.218587, acc.: 64.84%] [G loss: 0.525941]\n",
      "epoch:5 step:4924 [D loss: 0.198198, acc.: 67.19%] [G loss: 0.510093]\n",
      "epoch:5 step:4925 [D loss: 0.186446, acc.: 74.22%] [G loss: 0.531510]\n",
      "epoch:5 step:4926 [D loss: 0.176624, acc.: 75.00%] [G loss: 0.562778]\n",
      "epoch:5 step:4927 [D loss: 0.183447, acc.: 75.00%] [G loss: 0.547828]\n",
      "epoch:5 step:4928 [D loss: 0.209691, acc.: 65.62%] [G loss: 0.554092]\n",
      "epoch:5 step:4929 [D loss: 0.199108, acc.: 71.88%] [G loss: 0.548782]\n",
      "epoch:5 step:4930 [D loss: 0.212402, acc.: 67.97%] [G loss: 0.518648]\n",
      "epoch:5 step:4931 [D loss: 0.225655, acc.: 60.16%] [G loss: 0.521663]\n",
      "epoch:5 step:4932 [D loss: 0.200425, acc.: 65.62%] [G loss: 0.574762]\n",
      "epoch:5 step:4933 [D loss: 0.210878, acc.: 64.84%] [G loss: 0.529912]\n",
      "epoch:5 step:4934 [D loss: 0.246304, acc.: 60.94%] [G loss: 0.491139]\n",
      "epoch:5 step:4935 [D loss: 0.218655, acc.: 63.28%] [G loss: 0.512709]\n",
      "epoch:5 step:4936 [D loss: 0.202235, acc.: 67.19%] [G loss: 0.527618]\n",
      "epoch:5 step:4937 [D loss: 0.257711, acc.: 56.25%] [G loss: 0.496627]\n",
      "epoch:5 step:4938 [D loss: 0.199978, acc.: 67.97%] [G loss: 0.537209]\n",
      "epoch:5 step:4939 [D loss: 0.178764, acc.: 77.34%] [G loss: 0.537772]\n",
      "epoch:5 step:4940 [D loss: 0.177681, acc.: 76.56%] [G loss: 0.545661]\n",
      "epoch:5 step:4941 [D loss: 0.178218, acc.: 74.22%] [G loss: 0.559354]\n",
      "epoch:5 step:4942 [D loss: 0.204937, acc.: 67.97%] [G loss: 0.535071]\n",
      "epoch:5 step:4943 [D loss: 0.189133, acc.: 71.88%] [G loss: 0.557834]\n",
      "epoch:5 step:4944 [D loss: 0.208898, acc.: 72.66%] [G loss: 0.536388]\n",
      "epoch:5 step:4945 [D loss: 0.215801, acc.: 65.62%] [G loss: 0.551104]\n",
      "epoch:5 step:4946 [D loss: 0.179953, acc.: 75.78%] [G loss: 0.562704]\n",
      "epoch:5 step:4947 [D loss: 0.184396, acc.: 73.44%] [G loss: 0.593314]\n",
      "epoch:5 step:4948 [D loss: 0.257388, acc.: 53.12%] [G loss: 0.536767]\n",
      "epoch:5 step:4949 [D loss: 0.159184, acc.: 78.91%] [G loss: 0.532576]\n",
      "epoch:5 step:4950 [D loss: 0.257780, acc.: 59.38%] [G loss: 0.492621]\n",
      "epoch:5 step:4951 [D loss: 0.187132, acc.: 69.53%] [G loss: 0.511305]\n",
      "epoch:5 step:4952 [D loss: 0.195116, acc.: 70.31%] [G loss: 0.525236]\n",
      "epoch:5 step:4953 [D loss: 0.209598, acc.: 65.62%] [G loss: 0.508076]\n",
      "epoch:5 step:4954 [D loss: 0.206231, acc.: 65.62%] [G loss: 0.505657]\n",
      "epoch:5 step:4955 [D loss: 0.182700, acc.: 78.91%] [G loss: 0.547035]\n",
      "epoch:5 step:4956 [D loss: 0.183632, acc.: 70.31%] [G loss: 0.555112]\n",
      "epoch:5 step:4957 [D loss: 0.187755, acc.: 75.00%] [G loss: 0.527933]\n",
      "epoch:5 step:4958 [D loss: 0.200571, acc.: 64.84%] [G loss: 0.498921]\n",
      "epoch:5 step:4959 [D loss: 0.189805, acc.: 69.53%] [G loss: 0.552938]\n",
      "epoch:5 step:4960 [D loss: 0.224779, acc.: 63.28%] [G loss: 0.499151]\n",
      "epoch:5 step:4961 [D loss: 0.211248, acc.: 69.53%] [G loss: 0.598368]\n",
      "epoch:5 step:4962 [D loss: 0.229318, acc.: 63.28%] [G loss: 0.532213]\n",
      "epoch:5 step:4963 [D loss: 0.219030, acc.: 63.28%] [G loss: 0.530634]\n",
      "epoch:5 step:4964 [D loss: 0.173530, acc.: 76.56%] [G loss: 0.546754]\n",
      "epoch:5 step:4965 [D loss: 0.207804, acc.: 69.53%] [G loss: 0.590535]\n",
      "epoch:5 step:4966 [D loss: 0.243102, acc.: 63.28%] [G loss: 0.515371]\n",
      "epoch:5 step:4967 [D loss: 0.211538, acc.: 67.19%] [G loss: 0.529848]\n",
      "epoch:5 step:4968 [D loss: 0.176960, acc.: 70.31%] [G loss: 0.541820]\n",
      "epoch:5 step:4969 [D loss: 0.211409, acc.: 69.53%] [G loss: 0.506316]\n",
      "epoch:5 step:4970 [D loss: 0.187688, acc.: 71.09%] [G loss: 0.530811]\n",
      "epoch:5 step:4971 [D loss: 0.155334, acc.: 81.25%] [G loss: 0.600579]\n",
      "epoch:5 step:4972 [D loss: 0.234999, acc.: 60.94%] [G loss: 0.500937]\n",
      "epoch:5 step:4973 [D loss: 0.210713, acc.: 69.53%] [G loss: 0.530298]\n",
      "epoch:5 step:4974 [D loss: 0.179465, acc.: 74.22%] [G loss: 0.573118]\n",
      "epoch:5 step:4975 [D loss: 0.199925, acc.: 69.53%] [G loss: 0.555122]\n",
      "epoch:5 step:4976 [D loss: 0.259826, acc.: 55.47%] [G loss: 0.510740]\n",
      "epoch:5 step:4977 [D loss: 0.212590, acc.: 64.84%] [G loss: 0.516544]\n",
      "epoch:5 step:4978 [D loss: 0.176250, acc.: 71.09%] [G loss: 0.573179]\n",
      "epoch:5 step:4979 [D loss: 0.231711, acc.: 63.28%] [G loss: 0.509490]\n",
      "epoch:5 step:4980 [D loss: 0.206535, acc.: 72.66%] [G loss: 0.541648]\n",
      "epoch:5 step:4981 [D loss: 0.186550, acc.: 69.53%] [G loss: 0.500315]\n",
      "epoch:5 step:4982 [D loss: 0.184002, acc.: 73.44%] [G loss: 0.555057]\n",
      "epoch:5 step:4983 [D loss: 0.171452, acc.: 76.56%] [G loss: 0.581221]\n",
      "epoch:5 step:4984 [D loss: 0.167451, acc.: 75.78%] [G loss: 0.588923]\n",
      "epoch:5 step:4985 [D loss: 0.193183, acc.: 71.09%] [G loss: 0.552908]\n",
      "epoch:5 step:4986 [D loss: 0.247979, acc.: 57.03%] [G loss: 0.501233]\n",
      "epoch:5 step:4987 [D loss: 0.178714, acc.: 71.88%] [G loss: 0.509770]\n",
      "epoch:5 step:4988 [D loss: 0.181538, acc.: 72.66%] [G loss: 0.535497]\n",
      "epoch:5 step:4989 [D loss: 0.173977, acc.: 76.56%] [G loss: 0.540328]\n",
      "epoch:5 step:4990 [D loss: 0.195887, acc.: 67.97%] [G loss: 0.561874]\n",
      "epoch:5 step:4991 [D loss: 0.200903, acc.: 68.75%] [G loss: 0.557591]\n",
      "epoch:5 step:4992 [D loss: 0.185566, acc.: 70.31%] [G loss: 0.543548]\n",
      "epoch:5 step:4993 [D loss: 0.217807, acc.: 64.84%] [G loss: 0.515660]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4994 [D loss: 0.157331, acc.: 75.78%] [G loss: 0.519317]\n",
      "epoch:5 step:4995 [D loss: 0.184341, acc.: 74.22%] [G loss: 0.517987]\n",
      "epoch:5 step:4996 [D loss: 0.184928, acc.: 69.53%] [G loss: 0.523469]\n",
      "epoch:5 step:4997 [D loss: 0.142707, acc.: 82.03%] [G loss: 0.656423]\n",
      "epoch:5 step:4998 [D loss: 0.178236, acc.: 74.22%] [G loss: 0.620778]\n",
      "epoch:5 step:4999 [D loss: 0.154889, acc.: 76.56%] [G loss: 0.685066]\n",
      "epoch:5 step:5000 [D loss: 0.154796, acc.: 77.34%] [G loss: 0.631401]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.497965\n",
      "FID: 29.233416\n",
      "0 = 12.548575629615753\n",
      "1 = 0.06225538066605167\n",
      "2 = 0.9585999846458435\n",
      "3 = 0.9361000061035156\n",
      "4 = 0.9811000227928162\n",
      "5 = 0.9802094101905823\n",
      "6 = 0.9361000061035156\n",
      "7 = 8.35840725098251\n",
      "8 = 0.11640772466557538\n",
      "9 = 0.8076000213623047\n",
      "10 = 0.7998999953269958\n",
      "11 = 0.8152999877929688\n",
      "12 = 0.8124111294746399\n",
      "13 = 0.7998999953269958\n",
      "14 = 6.498020648956299\n",
      "15 = 9.395477294921875\n",
      "16 = 0.1689136028289795\n",
      "17 = 6.497965335845947\n",
      "18 = 29.233415603637695\n",
      "epoch:5 step:5001 [D loss: 0.240566, acc.: 63.28%] [G loss: 0.509573]\n",
      "epoch:5 step:5002 [D loss: 0.196543, acc.: 71.09%] [G loss: 0.511402]\n",
      "epoch:5 step:5003 [D loss: 0.189416, acc.: 71.09%] [G loss: 0.507034]\n",
      "epoch:5 step:5004 [D loss: 0.174952, acc.: 77.34%] [G loss: 0.546209]\n",
      "epoch:5 step:5005 [D loss: 0.209775, acc.: 70.31%] [G loss: 0.528614]\n",
      "epoch:5 step:5006 [D loss: 0.163630, acc.: 78.91%] [G loss: 0.606883]\n",
      "epoch:5 step:5007 [D loss: 0.187568, acc.: 71.88%] [G loss: 0.613042]\n",
      "epoch:5 step:5008 [D loss: 0.224703, acc.: 63.28%] [G loss: 0.527664]\n",
      "epoch:5 step:5009 [D loss: 0.203492, acc.: 68.75%] [G loss: 0.484139]\n",
      "epoch:5 step:5010 [D loss: 0.198310, acc.: 69.53%] [G loss: 0.583631]\n",
      "epoch:5 step:5011 [D loss: 0.181531, acc.: 72.66%] [G loss: 0.553291]\n",
      "epoch:5 step:5012 [D loss: 0.198634, acc.: 71.09%] [G loss: 0.568885]\n",
      "epoch:5 step:5013 [D loss: 0.190367, acc.: 71.09%] [G loss: 0.599679]\n",
      "epoch:5 step:5014 [D loss: 0.162448, acc.: 75.00%] [G loss: 0.584543]\n",
      "epoch:5 step:5015 [D loss: 0.208935, acc.: 67.97%] [G loss: 0.527250]\n",
      "epoch:5 step:5016 [D loss: 0.180505, acc.: 75.00%] [G loss: 0.526486]\n",
      "epoch:5 step:5017 [D loss: 0.174858, acc.: 73.44%] [G loss: 0.561581]\n",
      "epoch:5 step:5018 [D loss: 0.180386, acc.: 73.44%] [G loss: 0.583053]\n",
      "epoch:5 step:5019 [D loss: 0.230968, acc.: 62.50%] [G loss: 0.542017]\n",
      "epoch:5 step:5020 [D loss: 0.173061, acc.: 72.66%] [G loss: 0.604624]\n",
      "epoch:5 step:5021 [D loss: 0.185560, acc.: 78.12%] [G loss: 0.589412]\n",
      "epoch:5 step:5022 [D loss: 0.170587, acc.: 76.56%] [G loss: 0.570987]\n",
      "epoch:5 step:5023 [D loss: 0.177334, acc.: 75.78%] [G loss: 0.585176]\n",
      "epoch:5 step:5024 [D loss: 0.189292, acc.: 69.53%] [G loss: 0.544875]\n",
      "epoch:5 step:5025 [D loss: 0.175618, acc.: 75.00%] [G loss: 0.551260]\n",
      "epoch:5 step:5026 [D loss: 0.220578, acc.: 62.50%] [G loss: 0.543472]\n",
      "epoch:5 step:5027 [D loss: 0.180866, acc.: 72.66%] [G loss: 0.556260]\n",
      "epoch:5 step:5028 [D loss: 0.137164, acc.: 82.03%] [G loss: 0.626607]\n",
      "epoch:5 step:5029 [D loss: 0.159709, acc.: 81.25%] [G loss: 0.568227]\n",
      "epoch:5 step:5030 [D loss: 0.193297, acc.: 68.75%] [G loss: 0.615709]\n",
      "epoch:5 step:5031 [D loss: 0.183764, acc.: 71.09%] [G loss: 0.595523]\n",
      "epoch:5 step:5032 [D loss: 0.179675, acc.: 76.56%] [G loss: 0.657304]\n",
      "epoch:5 step:5033 [D loss: 0.269325, acc.: 57.81%] [G loss: 0.501570]\n",
      "epoch:5 step:5034 [D loss: 0.254766, acc.: 54.69%] [G loss: 0.466460]\n",
      "epoch:5 step:5035 [D loss: 0.169029, acc.: 78.91%] [G loss: 0.532153]\n",
      "epoch:5 step:5036 [D loss: 0.183171, acc.: 71.88%] [G loss: 0.547371]\n",
      "epoch:5 step:5037 [D loss: 0.239666, acc.: 64.84%] [G loss: 0.565015]\n",
      "epoch:5 step:5038 [D loss: 0.202237, acc.: 65.62%] [G loss: 0.547653]\n",
      "epoch:5 step:5039 [D loss: 0.164482, acc.: 72.66%] [G loss: 0.588380]\n",
      "epoch:5 step:5040 [D loss: 0.227091, acc.: 64.84%] [G loss: 0.541471]\n",
      "epoch:5 step:5041 [D loss: 0.219185, acc.: 67.97%] [G loss: 0.538887]\n",
      "epoch:5 step:5042 [D loss: 0.148045, acc.: 81.25%] [G loss: 0.575386]\n",
      "epoch:5 step:5043 [D loss: 0.155291, acc.: 77.34%] [G loss: 0.614615]\n",
      "epoch:5 step:5044 [D loss: 0.180184, acc.: 74.22%] [G loss: 0.566752]\n",
      "epoch:5 step:5045 [D loss: 0.161482, acc.: 78.12%] [G loss: 0.597641]\n",
      "epoch:5 step:5046 [D loss: 0.193269, acc.: 71.09%] [G loss: 0.554417]\n",
      "epoch:5 step:5047 [D loss: 0.221684, acc.: 62.50%] [G loss: 0.550636]\n",
      "epoch:5 step:5048 [D loss: 0.195068, acc.: 72.66%] [G loss: 0.494155]\n",
      "epoch:5 step:5049 [D loss: 0.173312, acc.: 75.00%] [G loss: 0.593183]\n",
      "epoch:5 step:5050 [D loss: 0.182440, acc.: 75.00%] [G loss: 0.511395]\n",
      "epoch:5 step:5051 [D loss: 0.207093, acc.: 69.53%] [G loss: 0.535518]\n",
      "epoch:5 step:5052 [D loss: 0.206980, acc.: 65.62%] [G loss: 0.590039]\n",
      "epoch:5 step:5053 [D loss: 0.195247, acc.: 71.88%] [G loss: 0.521623]\n",
      "epoch:5 step:5054 [D loss: 0.199793, acc.: 69.53%] [G loss: 0.514544]\n",
      "epoch:5 step:5055 [D loss: 0.210223, acc.: 71.09%] [G loss: 0.495435]\n",
      "epoch:5 step:5056 [D loss: 0.163616, acc.: 77.34%] [G loss: 0.555515]\n",
      "epoch:5 step:5057 [D loss: 0.197774, acc.: 70.31%] [G loss: 0.558626]\n",
      "epoch:5 step:5058 [D loss: 0.228297, acc.: 66.41%] [G loss: 0.533625]\n",
      "epoch:5 step:5059 [D loss: 0.183742, acc.: 76.56%] [G loss: 0.526942]\n",
      "epoch:5 step:5060 [D loss: 0.180722, acc.: 71.09%] [G loss: 0.534440]\n",
      "epoch:5 step:5061 [D loss: 0.269887, acc.: 50.00%] [G loss: 0.504714]\n",
      "epoch:5 step:5062 [D loss: 0.217815, acc.: 68.75%] [G loss: 0.469366]\n",
      "epoch:5 step:5063 [D loss: 0.214972, acc.: 67.19%] [G loss: 0.504208]\n",
      "epoch:5 step:5064 [D loss: 0.202453, acc.: 70.31%] [G loss: 0.580924]\n",
      "epoch:5 step:5065 [D loss: 0.206800, acc.: 69.53%] [G loss: 0.573505]\n",
      "epoch:5 step:5066 [D loss: 0.168036, acc.: 75.00%] [G loss: 0.550287]\n",
      "epoch:5 step:5067 [D loss: 0.205870, acc.: 67.19%] [G loss: 0.525330]\n",
      "epoch:5 step:5068 [D loss: 0.187428, acc.: 71.88%] [G loss: 0.561615]\n",
      "epoch:5 step:5069 [D loss: 0.186731, acc.: 75.00%] [G loss: 0.525605]\n",
      "epoch:5 step:5070 [D loss: 0.189785, acc.: 75.78%] [G loss: 0.516373]\n",
      "epoch:5 step:5071 [D loss: 0.226630, acc.: 71.09%] [G loss: 0.539002]\n",
      "epoch:5 step:5072 [D loss: 0.240108, acc.: 64.84%] [G loss: 0.519748]\n",
      "epoch:5 step:5073 [D loss: 0.184519, acc.: 73.44%] [G loss: 0.544401]\n",
      "epoch:5 step:5074 [D loss: 0.216723, acc.: 70.31%] [G loss: 0.547264]\n",
      "epoch:5 step:5075 [D loss: 0.211944, acc.: 66.41%] [G loss: 0.502221]\n",
      "epoch:5 step:5076 [D loss: 0.170424, acc.: 77.34%] [G loss: 0.599123]\n",
      "epoch:5 step:5077 [D loss: 0.164575, acc.: 72.66%] [G loss: 0.555978]\n",
      "epoch:5 step:5078 [D loss: 0.227885, acc.: 60.16%] [G loss: 0.519879]\n",
      "epoch:5 step:5079 [D loss: 0.204681, acc.: 66.41%] [G loss: 0.522851]\n",
      "epoch:5 step:5080 [D loss: 0.219818, acc.: 62.50%] [G loss: 0.512950]\n",
      "epoch:5 step:5081 [D loss: 0.224636, acc.: 62.50%] [G loss: 0.527626]\n",
      "epoch:5 step:5082 [D loss: 0.197317, acc.: 67.97%] [G loss: 0.561857]\n",
      "epoch:5 step:5083 [D loss: 0.155112, acc.: 79.69%] [G loss: 0.592442]\n",
      "epoch:5 step:5084 [D loss: 0.188250, acc.: 71.88%] [G loss: 0.581182]\n",
      "epoch:5 step:5085 [D loss: 0.208072, acc.: 67.97%] [G loss: 0.526645]\n",
      "epoch:5 step:5086 [D loss: 0.216267, acc.: 63.28%] [G loss: 0.514527]\n",
      "epoch:5 step:5087 [D loss: 0.167183, acc.: 76.56%] [G loss: 0.605173]\n",
      "epoch:5 step:5088 [D loss: 0.210330, acc.: 67.19%] [G loss: 0.602724]\n",
      "epoch:5 step:5089 [D loss: 0.279760, acc.: 53.12%] [G loss: 0.499617]\n",
      "epoch:5 step:5090 [D loss: 0.192169, acc.: 71.88%] [G loss: 0.568772]\n",
      "epoch:5 step:5091 [D loss: 0.186509, acc.: 68.75%] [G loss: 0.572227]\n",
      "epoch:5 step:5092 [D loss: 0.211358, acc.: 66.41%] [G loss: 0.566893]\n",
      "epoch:5 step:5093 [D loss: 0.220321, acc.: 64.06%] [G loss: 0.509512]\n",
      "epoch:5 step:5094 [D loss: 0.176909, acc.: 75.78%] [G loss: 0.576053]\n",
      "epoch:5 step:5095 [D loss: 0.220283, acc.: 64.84%] [G loss: 0.512870]\n",
      "epoch:5 step:5096 [D loss: 0.194895, acc.: 65.62%] [G loss: 0.525752]\n",
      "epoch:5 step:5097 [D loss: 0.213340, acc.: 65.62%] [G loss: 0.532538]\n",
      "epoch:5 step:5098 [D loss: 0.201677, acc.: 70.31%] [G loss: 0.505117]\n",
      "epoch:5 step:5099 [D loss: 0.233883, acc.: 61.72%] [G loss: 0.486287]\n",
      "epoch:5 step:5100 [D loss: 0.199799, acc.: 69.53%] [G loss: 0.544121]\n",
      "epoch:5 step:5101 [D loss: 0.220824, acc.: 61.72%] [G loss: 0.587187]\n",
      "epoch:5 step:5102 [D loss: 0.230672, acc.: 64.84%] [G loss: 0.514573]\n",
      "epoch:5 step:5103 [D loss: 0.218793, acc.: 70.31%] [G loss: 0.505469]\n",
      "epoch:5 step:5104 [D loss: 0.189278, acc.: 73.44%] [G loss: 0.514916]\n",
      "epoch:5 step:5105 [D loss: 0.196110, acc.: 68.75%] [G loss: 0.516542]\n",
      "epoch:5 step:5106 [D loss: 0.244020, acc.: 63.28%] [G loss: 0.480755]\n",
      "epoch:5 step:5107 [D loss: 0.190152, acc.: 70.31%] [G loss: 0.540819]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5108 [D loss: 0.197025, acc.: 71.09%] [G loss: 0.541618]\n",
      "epoch:5 step:5109 [D loss: 0.206333, acc.: 64.84%] [G loss: 0.513234]\n",
      "epoch:5 step:5110 [D loss: 0.198404, acc.: 71.09%] [G loss: 0.501259]\n",
      "epoch:5 step:5111 [D loss: 0.217691, acc.: 68.75%] [G loss: 0.546988]\n",
      "epoch:5 step:5112 [D loss: 0.177613, acc.: 75.00%] [G loss: 0.636197]\n",
      "epoch:5 step:5113 [D loss: 0.173334, acc.: 73.44%] [G loss: 0.613944]\n",
      "epoch:5 step:5114 [D loss: 0.170302, acc.: 76.56%] [G loss: 0.568340]\n",
      "epoch:5 step:5115 [D loss: 0.187974, acc.: 67.97%] [G loss: 0.605718]\n",
      "epoch:5 step:5116 [D loss: 0.195589, acc.: 72.66%] [G loss: 0.550540]\n",
      "epoch:5 step:5117 [D loss: 0.242122, acc.: 59.38%] [G loss: 0.535077]\n",
      "epoch:5 step:5118 [D loss: 0.229805, acc.: 64.06%] [G loss: 0.496136]\n",
      "epoch:5 step:5119 [D loss: 0.182605, acc.: 78.12%] [G loss: 0.534523]\n",
      "epoch:5 step:5120 [D loss: 0.200643, acc.: 68.75%] [G loss: 0.558438]\n",
      "epoch:5 step:5121 [D loss: 0.160605, acc.: 78.91%] [G loss: 0.572820]\n",
      "epoch:5 step:5122 [D loss: 0.296037, acc.: 56.25%] [G loss: 0.476057]\n",
      "epoch:5 step:5123 [D loss: 0.209333, acc.: 69.53%] [G loss: 0.517931]\n",
      "epoch:5 step:5124 [D loss: 0.195815, acc.: 66.41%] [G loss: 0.488742]\n",
      "epoch:5 step:5125 [D loss: 0.175325, acc.: 78.12%] [G loss: 0.546120]\n",
      "epoch:5 step:5126 [D loss: 0.249832, acc.: 61.72%] [G loss: 0.509805]\n",
      "epoch:5 step:5127 [D loss: 0.199743, acc.: 70.31%] [G loss: 0.527105]\n",
      "epoch:5 step:5128 [D loss: 0.211541, acc.: 67.19%] [G loss: 0.499408]\n",
      "epoch:5 step:5129 [D loss: 0.218763, acc.: 64.84%] [G loss: 0.503032]\n",
      "epoch:5 step:5130 [D loss: 0.204479, acc.: 68.75%] [G loss: 0.532905]\n",
      "epoch:5 step:5131 [D loss: 0.186902, acc.: 74.22%] [G loss: 0.565138]\n",
      "epoch:5 step:5132 [D loss: 0.198054, acc.: 71.88%] [G loss: 0.554055]\n",
      "epoch:5 step:5133 [D loss: 0.234698, acc.: 62.50%] [G loss: 0.519142]\n",
      "epoch:5 step:5134 [D loss: 0.201591, acc.: 69.53%] [G loss: 0.545365]\n",
      "epoch:5 step:5135 [D loss: 0.179517, acc.: 75.78%] [G loss: 0.557711]\n",
      "epoch:5 step:5136 [D loss: 0.168472, acc.: 71.88%] [G loss: 0.581730]\n",
      "epoch:5 step:5137 [D loss: 0.194845, acc.: 70.31%] [G loss: 0.575232]\n",
      "epoch:5 step:5138 [D loss: 0.216130, acc.: 65.62%] [G loss: 0.556214]\n",
      "epoch:5 step:5139 [D loss: 0.191023, acc.: 68.75%] [G loss: 0.559044]\n",
      "epoch:5 step:5140 [D loss: 0.213884, acc.: 64.84%] [G loss: 0.527862]\n",
      "epoch:5 step:5141 [D loss: 0.237478, acc.: 63.28%] [G loss: 0.528371]\n",
      "epoch:5 step:5142 [D loss: 0.201185, acc.: 67.19%] [G loss: 0.539554]\n",
      "epoch:5 step:5143 [D loss: 0.259005, acc.: 62.50%] [G loss: 0.463912]\n",
      "epoch:5 step:5144 [D loss: 0.197807, acc.: 68.75%] [G loss: 0.556697]\n",
      "epoch:5 step:5145 [D loss: 0.210781, acc.: 70.31%] [G loss: 0.544813]\n",
      "epoch:5 step:5146 [D loss: 0.185174, acc.: 78.12%] [G loss: 0.544992]\n",
      "epoch:5 step:5147 [D loss: 0.216174, acc.: 68.75%] [G loss: 0.511470]\n",
      "epoch:5 step:5148 [D loss: 0.215776, acc.: 67.97%] [G loss: 0.535015]\n",
      "epoch:5 step:5149 [D loss: 0.196716, acc.: 71.88%] [G loss: 0.526287]\n",
      "epoch:5 step:5150 [D loss: 0.234326, acc.: 60.94%] [G loss: 0.520782]\n",
      "epoch:5 step:5151 [D loss: 0.209995, acc.: 67.97%] [G loss: 0.536972]\n",
      "epoch:5 step:5152 [D loss: 0.203266, acc.: 67.19%] [G loss: 0.554235]\n",
      "epoch:5 step:5153 [D loss: 0.218119, acc.: 61.72%] [G loss: 0.508728]\n",
      "epoch:5 step:5154 [D loss: 0.207145, acc.: 65.62%] [G loss: 0.562635]\n",
      "epoch:5 step:5155 [D loss: 0.170078, acc.: 76.56%] [G loss: 0.591247]\n",
      "epoch:5 step:5156 [D loss: 0.186524, acc.: 76.56%] [G loss: 0.550473]\n",
      "epoch:5 step:5157 [D loss: 0.180237, acc.: 69.53%] [G loss: 0.620830]\n",
      "epoch:5 step:5158 [D loss: 0.256994, acc.: 55.47%] [G loss: 0.473980]\n",
      "epoch:5 step:5159 [D loss: 0.167596, acc.: 78.12%] [G loss: 0.559427]\n",
      "epoch:5 step:5160 [D loss: 0.185831, acc.: 74.22%] [G loss: 0.578035]\n",
      "epoch:5 step:5161 [D loss: 0.241891, acc.: 61.72%] [G loss: 0.546565]\n",
      "epoch:5 step:5162 [D loss: 0.220922, acc.: 62.50%] [G loss: 0.545960]\n",
      "epoch:5 step:5163 [D loss: 0.221950, acc.: 66.41%] [G loss: 0.491048]\n",
      "epoch:5 step:5164 [D loss: 0.195325, acc.: 68.75%] [G loss: 0.552996]\n",
      "epoch:5 step:5165 [D loss: 0.225374, acc.: 71.09%] [G loss: 0.512120]\n",
      "epoch:5 step:5166 [D loss: 0.170575, acc.: 78.12%] [G loss: 0.584828]\n",
      "epoch:5 step:5167 [D loss: 0.275867, acc.: 56.25%] [G loss: 0.472658]\n",
      "epoch:5 step:5168 [D loss: 0.198608, acc.: 67.19%] [G loss: 0.527861]\n",
      "epoch:5 step:5169 [D loss: 0.167394, acc.: 75.78%] [G loss: 0.541130]\n",
      "epoch:5 step:5170 [D loss: 0.177351, acc.: 72.66%] [G loss: 0.574605]\n",
      "epoch:5 step:5171 [D loss: 0.229216, acc.: 64.06%] [G loss: 0.499516]\n",
      "epoch:5 step:5172 [D loss: 0.203837, acc.: 68.75%] [G loss: 0.524353]\n",
      "epoch:5 step:5173 [D loss: 0.186468, acc.: 68.75%] [G loss: 0.555335]\n",
      "epoch:5 step:5174 [D loss: 0.235541, acc.: 67.19%] [G loss: 0.486529]\n",
      "epoch:5 step:5175 [D loss: 0.200354, acc.: 70.31%] [G loss: 0.533161]\n",
      "epoch:5 step:5176 [D loss: 0.189881, acc.: 71.88%] [G loss: 0.592796]\n",
      "epoch:5 step:5177 [D loss: 0.221289, acc.: 62.50%] [G loss: 0.568089]\n",
      "epoch:5 step:5178 [D loss: 0.215895, acc.: 67.97%] [G loss: 0.516508]\n",
      "epoch:5 step:5179 [D loss: 0.198900, acc.: 71.09%] [G loss: 0.499694]\n",
      "epoch:5 step:5180 [D loss: 0.172536, acc.: 76.56%] [G loss: 0.627351]\n",
      "epoch:5 step:5181 [D loss: 0.229713, acc.: 61.72%] [G loss: 0.590507]\n",
      "epoch:5 step:5182 [D loss: 0.177643, acc.: 76.56%] [G loss: 0.617768]\n",
      "epoch:5 step:5183 [D loss: 0.159967, acc.: 78.12%] [G loss: 0.611981]\n",
      "epoch:5 step:5184 [D loss: 0.153460, acc.: 77.34%] [G loss: 0.620694]\n",
      "epoch:5 step:5185 [D loss: 0.286257, acc.: 53.12%] [G loss: 0.465769]\n",
      "epoch:5 step:5186 [D loss: 0.256322, acc.: 60.94%] [G loss: 0.494285]\n",
      "epoch:5 step:5187 [D loss: 0.212731, acc.: 66.41%] [G loss: 0.526981]\n",
      "epoch:5 step:5188 [D loss: 0.165552, acc.: 75.00%] [G loss: 0.527193]\n",
      "epoch:5 step:5189 [D loss: 0.197069, acc.: 71.88%] [G loss: 0.598617]\n",
      "epoch:5 step:5190 [D loss: 0.212438, acc.: 72.66%] [G loss: 0.529594]\n",
      "epoch:5 step:5191 [D loss: 0.218521, acc.: 67.97%] [G loss: 0.524611]\n",
      "epoch:5 step:5192 [D loss: 0.208323, acc.: 64.84%] [G loss: 0.533001]\n",
      "epoch:5 step:5193 [D loss: 0.171080, acc.: 76.56%] [G loss: 0.591045]\n",
      "epoch:5 step:5194 [D loss: 0.223842, acc.: 60.94%] [G loss: 0.561489]\n",
      "epoch:5 step:5195 [D loss: 0.197455, acc.: 70.31%] [G loss: 0.504866]\n",
      "epoch:5 step:5196 [D loss: 0.191896, acc.: 73.44%] [G loss: 0.530689]\n",
      "epoch:5 step:5197 [D loss: 0.196489, acc.: 71.09%] [G loss: 0.519208]\n",
      "epoch:5 step:5198 [D loss: 0.211929, acc.: 67.19%] [G loss: 0.544075]\n",
      "epoch:5 step:5199 [D loss: 0.203567, acc.: 67.19%] [G loss: 0.582861]\n",
      "epoch:5 step:5200 [D loss: 0.180921, acc.: 76.56%] [G loss: 0.604500]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.578976\n",
      "FID: 28.681108\n",
      "0 = 12.407117784881555\n",
      "1 = 0.06706627844185577\n",
      "2 = 0.9518499970436096\n",
      "3 = 0.9225000143051147\n",
      "4 = 0.9811999797821045\n",
      "5 = 0.9800276160240173\n",
      "6 = 0.9225000143051147\n",
      "7 = 8.283724732655307\n",
      "8 = 0.1182775031722129\n",
      "9 = 0.7985000014305115\n",
      "10 = 0.7919999957084656\n",
      "11 = 0.8050000071525574\n",
      "12 = 0.802431583404541\n",
      "13 = 0.7919999957084656\n",
      "14 = 6.579031944274902\n",
      "15 = 9.198225975036621\n",
      "16 = 0.196965754032135\n",
      "17 = 6.578975677490234\n",
      "18 = 28.681108474731445\n",
      "epoch:5 step:5201 [D loss: 0.172563, acc.: 71.88%] [G loss: 0.571089]\n",
      "epoch:5 step:5202 [D loss: 0.165249, acc.: 73.44%] [G loss: 0.559387]\n",
      "epoch:5 step:5203 [D loss: 0.232073, acc.: 61.72%] [G loss: 0.500047]\n",
      "epoch:5 step:5204 [D loss: 0.172495, acc.: 77.34%] [G loss: 0.532549]\n",
      "epoch:5 step:5205 [D loss: 0.188326, acc.: 66.41%] [G loss: 0.514030]\n",
      "epoch:5 step:5206 [D loss: 0.182103, acc.: 71.09%] [G loss: 0.508832]\n",
      "epoch:5 step:5207 [D loss: 0.185199, acc.: 68.75%] [G loss: 0.552016]\n",
      "epoch:5 step:5208 [D loss: 0.189613, acc.: 69.53%] [G loss: 0.533446]\n",
      "epoch:5 step:5209 [D loss: 0.196730, acc.: 69.53%] [G loss: 0.535289]\n",
      "epoch:5 step:5210 [D loss: 0.216648, acc.: 67.19%] [G loss: 0.531004]\n",
      "epoch:5 step:5211 [D loss: 0.176606, acc.: 75.78%] [G loss: 0.513761]\n",
      "epoch:5 step:5212 [D loss: 0.256535, acc.: 54.69%] [G loss: 0.542306]\n",
      "epoch:5 step:5213 [D loss: 0.264688, acc.: 50.78%] [G loss: 0.517976]\n",
      "epoch:5 step:5214 [D loss: 0.214743, acc.: 64.06%] [G loss: 0.525714]\n",
      "epoch:5 step:5215 [D loss: 0.193943, acc.: 74.22%] [G loss: 0.501117]\n",
      "epoch:5 step:5216 [D loss: 0.219625, acc.: 62.50%] [G loss: 0.510815]\n",
      "epoch:5 step:5217 [D loss: 0.202335, acc.: 67.97%] [G loss: 0.496255]\n",
      "epoch:5 step:5218 [D loss: 0.219355, acc.: 67.97%] [G loss: 0.522672]\n",
      "epoch:5 step:5219 [D loss: 0.164732, acc.: 78.12%] [G loss: 0.558784]\n",
      "epoch:5 step:5220 [D loss: 0.228053, acc.: 66.41%] [G loss: 0.477983]\n",
      "epoch:5 step:5221 [D loss: 0.188720, acc.: 73.44%] [G loss: 0.556960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5222 [D loss: 0.207926, acc.: 68.75%] [G loss: 0.535792]\n",
      "epoch:5 step:5223 [D loss: 0.243014, acc.: 60.94%] [G loss: 0.521587]\n",
      "epoch:5 step:5224 [D loss: 0.196243, acc.: 67.97%] [G loss: 0.480593]\n",
      "epoch:5 step:5225 [D loss: 0.219869, acc.: 67.19%] [G loss: 0.526840]\n",
      "epoch:5 step:5226 [D loss: 0.199420, acc.: 71.09%] [G loss: 0.540022]\n",
      "epoch:5 step:5227 [D loss: 0.261699, acc.: 53.12%] [G loss: 0.494422]\n",
      "epoch:5 step:5228 [D loss: 0.265745, acc.: 49.22%] [G loss: 0.493502]\n",
      "epoch:5 step:5229 [D loss: 0.204218, acc.: 63.28%] [G loss: 0.526321]\n",
      "epoch:5 step:5230 [D loss: 0.193971, acc.: 70.31%] [G loss: 0.557672]\n",
      "epoch:5 step:5231 [D loss: 0.201672, acc.: 66.41%] [G loss: 0.541150]\n",
      "epoch:5 step:5232 [D loss: 0.177526, acc.: 75.00%] [G loss: 0.556013]\n",
      "epoch:5 step:5233 [D loss: 0.194949, acc.: 71.88%] [G loss: 0.534010]\n",
      "epoch:5 step:5234 [D loss: 0.208598, acc.: 67.19%] [G loss: 0.603208]\n",
      "epoch:5 step:5235 [D loss: 0.202945, acc.: 66.41%] [G loss: 0.541996]\n",
      "epoch:5 step:5236 [D loss: 0.185527, acc.: 71.88%] [G loss: 0.572558]\n",
      "epoch:5 step:5237 [D loss: 0.191560, acc.: 67.97%] [G loss: 0.570224]\n",
      "epoch:5 step:5238 [D loss: 0.230472, acc.: 59.38%] [G loss: 0.481222]\n",
      "epoch:5 step:5239 [D loss: 0.170455, acc.: 78.12%] [G loss: 0.556964]\n",
      "epoch:5 step:5240 [D loss: 0.145967, acc.: 81.25%] [G loss: 0.614925]\n",
      "epoch:5 step:5241 [D loss: 0.182966, acc.: 74.22%] [G loss: 0.513177]\n",
      "epoch:5 step:5242 [D loss: 0.189611, acc.: 70.31%] [G loss: 0.528011]\n",
      "epoch:5 step:5243 [D loss: 0.185220, acc.: 74.22%] [G loss: 0.570911]\n",
      "epoch:5 step:5244 [D loss: 0.238296, acc.: 64.84%] [G loss: 0.603840]\n",
      "epoch:5 step:5245 [D loss: 0.214852, acc.: 63.28%] [G loss: 0.495310]\n",
      "epoch:5 step:5246 [D loss: 0.175190, acc.: 71.88%] [G loss: 0.505891]\n",
      "epoch:5 step:5247 [D loss: 0.228190, acc.: 64.06%] [G loss: 0.527522]\n",
      "epoch:5 step:5248 [D loss: 0.189273, acc.: 70.31%] [G loss: 0.551130]\n",
      "epoch:5 step:5249 [D loss: 0.198756, acc.: 69.53%] [G loss: 0.571679]\n",
      "epoch:5 step:5250 [D loss: 0.251119, acc.: 60.94%] [G loss: 0.537353]\n",
      "epoch:5 step:5251 [D loss: 0.223339, acc.: 61.72%] [G loss: 0.566379]\n",
      "epoch:5 step:5252 [D loss: 0.195167, acc.: 74.22%] [G loss: 0.540615]\n",
      "epoch:5 step:5253 [D loss: 0.162401, acc.: 78.12%] [G loss: 0.525869]\n",
      "epoch:5 step:5254 [D loss: 0.234843, acc.: 60.16%] [G loss: 0.476507]\n",
      "epoch:5 step:5255 [D loss: 0.191748, acc.: 68.75%] [G loss: 0.455260]\n",
      "epoch:5 step:5256 [D loss: 0.212462, acc.: 62.50%] [G loss: 0.503826]\n",
      "epoch:5 step:5257 [D loss: 0.213556, acc.: 65.62%] [G loss: 0.538901]\n",
      "epoch:5 step:5258 [D loss: 0.181012, acc.: 72.66%] [G loss: 0.530042]\n",
      "epoch:5 step:5259 [D loss: 0.184019, acc.: 69.53%] [G loss: 0.593945]\n",
      "epoch:5 step:5260 [D loss: 0.180628, acc.: 74.22%] [G loss: 0.583788]\n",
      "epoch:5 step:5261 [D loss: 0.257960, acc.: 57.81%] [G loss: 0.543277]\n",
      "epoch:5 step:5262 [D loss: 0.211821, acc.: 65.62%] [G loss: 0.484620]\n",
      "epoch:5 step:5263 [D loss: 0.187533, acc.: 71.09%] [G loss: 0.513932]\n",
      "epoch:5 step:5264 [D loss: 0.199881, acc.: 67.97%] [G loss: 0.503157]\n",
      "epoch:5 step:5265 [D loss: 0.215209, acc.: 64.84%] [G loss: 0.538251]\n",
      "epoch:5 step:5266 [D loss: 0.219617, acc.: 61.72%] [G loss: 0.535453]\n",
      "epoch:5 step:5267 [D loss: 0.162076, acc.: 78.12%] [G loss: 0.589780]\n",
      "epoch:5 step:5268 [D loss: 0.204900, acc.: 66.41%] [G loss: 0.591571]\n",
      "epoch:5 step:5269 [D loss: 0.227755, acc.: 63.28%] [G loss: 0.527992]\n",
      "epoch:5 step:5270 [D loss: 0.232274, acc.: 60.94%] [G loss: 0.509398]\n",
      "epoch:5 step:5271 [D loss: 0.202010, acc.: 67.19%] [G loss: 0.479280]\n",
      "epoch:5 step:5272 [D loss: 0.191249, acc.: 71.09%] [G loss: 0.552495]\n",
      "epoch:5 step:5273 [D loss: 0.214742, acc.: 65.62%] [G loss: 0.528071]\n",
      "epoch:5 step:5274 [D loss: 0.170194, acc.: 73.44%] [G loss: 0.619770]\n",
      "epoch:5 step:5275 [D loss: 0.263238, acc.: 63.28%] [G loss: 0.537767]\n",
      "epoch:5 step:5276 [D loss: 0.206641, acc.: 70.31%] [G loss: 0.541193]\n",
      "epoch:5 step:5277 [D loss: 0.198047, acc.: 72.66%] [G loss: 0.522973]\n",
      "epoch:5 step:5278 [D loss: 0.220486, acc.: 67.97%] [G loss: 0.509691]\n",
      "epoch:5 step:5279 [D loss: 0.213489, acc.: 65.62%] [G loss: 0.528341]\n",
      "epoch:5 step:5280 [D loss: 0.200916, acc.: 70.31%] [G loss: 0.480739]\n",
      "epoch:5 step:5281 [D loss: 0.234080, acc.: 64.06%] [G loss: 0.530179]\n",
      "epoch:5 step:5282 [D loss: 0.217843, acc.: 64.84%] [G loss: 0.557162]\n",
      "epoch:5 step:5283 [D loss: 0.198712, acc.: 74.22%] [G loss: 0.542056]\n",
      "epoch:5 step:5284 [D loss: 0.232324, acc.: 58.59%] [G loss: 0.502572]\n",
      "epoch:5 step:5285 [D loss: 0.246170, acc.: 54.69%] [G loss: 0.488479]\n",
      "epoch:5 step:5286 [D loss: 0.203244, acc.: 69.53%] [G loss: 0.557524]\n",
      "epoch:5 step:5287 [D loss: 0.187470, acc.: 71.09%] [G loss: 0.511717]\n",
      "epoch:5 step:5288 [D loss: 0.190442, acc.: 73.44%] [G loss: 0.506880]\n",
      "epoch:5 step:5289 [D loss: 0.210352, acc.: 69.53%] [G loss: 0.498633]\n",
      "epoch:5 step:5290 [D loss: 0.166004, acc.: 79.69%] [G loss: 0.531630]\n",
      "epoch:5 step:5291 [D loss: 0.230877, acc.: 58.59%] [G loss: 0.519137]\n",
      "epoch:5 step:5292 [D loss: 0.176138, acc.: 72.66%] [G loss: 0.557216]\n",
      "epoch:5 step:5293 [D loss: 0.196250, acc.: 75.00%] [G loss: 0.486306]\n",
      "epoch:5 step:5294 [D loss: 0.172001, acc.: 73.44%] [G loss: 0.554175]\n",
      "epoch:5 step:5295 [D loss: 0.205107, acc.: 65.62%] [G loss: 0.454193]\n",
      "epoch:5 step:5296 [D loss: 0.184475, acc.: 74.22%] [G loss: 0.521948]\n",
      "epoch:5 step:5297 [D loss: 0.203563, acc.: 64.84%] [G loss: 0.494453]\n",
      "epoch:5 step:5298 [D loss: 0.179975, acc.: 72.66%] [G loss: 0.557702]\n",
      "epoch:5 step:5299 [D loss: 0.210800, acc.: 66.41%] [G loss: 0.567597]\n",
      "epoch:5 step:5300 [D loss: 0.257905, acc.: 60.16%] [G loss: 0.476987]\n",
      "epoch:5 step:5301 [D loss: 0.200712, acc.: 71.09%] [G loss: 0.516195]\n",
      "epoch:5 step:5302 [D loss: 0.183418, acc.: 70.31%] [G loss: 0.524586]\n",
      "epoch:5 step:5303 [D loss: 0.189381, acc.: 72.66%] [G loss: 0.539737]\n",
      "epoch:5 step:5304 [D loss: 0.228835, acc.: 57.81%] [G loss: 0.532432]\n",
      "epoch:5 step:5305 [D loss: 0.175300, acc.: 73.44%] [G loss: 0.563564]\n",
      "epoch:5 step:5306 [D loss: 0.201633, acc.: 66.41%] [G loss: 0.548759]\n",
      "epoch:5 step:5307 [D loss: 0.237704, acc.: 56.25%] [G loss: 0.545375]\n",
      "epoch:5 step:5308 [D loss: 0.181845, acc.: 74.22%] [G loss: 0.569149]\n",
      "epoch:5 step:5309 [D loss: 0.182870, acc.: 70.31%] [G loss: 0.620266]\n",
      "epoch:5 step:5310 [D loss: 0.223560, acc.: 65.62%] [G loss: 0.503788]\n",
      "epoch:5 step:5311 [D loss: 0.213129, acc.: 64.06%] [G loss: 0.496441]\n",
      "epoch:5 step:5312 [D loss: 0.194470, acc.: 71.88%] [G loss: 0.509656]\n",
      "epoch:5 step:5313 [D loss: 0.233195, acc.: 63.28%] [G loss: 0.485826]\n",
      "epoch:5 step:5314 [D loss: 0.183637, acc.: 74.22%] [G loss: 0.514112]\n",
      "epoch:5 step:5315 [D loss: 0.199163, acc.: 72.66%] [G loss: 0.526855]\n",
      "epoch:5 step:5316 [D loss: 0.156295, acc.: 80.47%] [G loss: 0.572988]\n",
      "epoch:5 step:5317 [D loss: 0.186208, acc.: 74.22%] [G loss: 0.587713]\n",
      "epoch:5 step:5318 [D loss: 0.206897, acc.: 64.84%] [G loss: 0.547859]\n",
      "epoch:5 step:5319 [D loss: 0.201681, acc.: 70.31%] [G loss: 0.552013]\n",
      "epoch:5 step:5320 [D loss: 0.189142, acc.: 71.09%] [G loss: 0.532642]\n",
      "epoch:5 step:5321 [D loss: 0.240984, acc.: 57.81%] [G loss: 0.558418]\n",
      "epoch:5 step:5322 [D loss: 0.198764, acc.: 66.41%] [G loss: 0.559096]\n",
      "epoch:5 step:5323 [D loss: 0.179827, acc.: 77.34%] [G loss: 0.569680]\n",
      "epoch:5 step:5324 [D loss: 0.181717, acc.: 78.91%] [G loss: 0.552263]\n",
      "epoch:5 step:5325 [D loss: 0.179668, acc.: 76.56%] [G loss: 0.546269]\n",
      "epoch:5 step:5326 [D loss: 0.177639, acc.: 71.88%] [G loss: 0.536425]\n",
      "epoch:5 step:5327 [D loss: 0.186180, acc.: 72.66%] [G loss: 0.541560]\n",
      "epoch:5 step:5328 [D loss: 0.205248, acc.: 70.31%] [G loss: 0.519048]\n",
      "epoch:5 step:5329 [D loss: 0.187699, acc.: 71.09%] [G loss: 0.527004]\n",
      "epoch:5 step:5330 [D loss: 0.224116, acc.: 63.28%] [G loss: 0.507921]\n",
      "epoch:5 step:5331 [D loss: 0.218551, acc.: 67.97%] [G loss: 0.501387]\n",
      "epoch:5 step:5332 [D loss: 0.183366, acc.: 69.53%] [G loss: 0.588852]\n",
      "epoch:5 step:5333 [D loss: 0.179707, acc.: 76.56%] [G loss: 0.622607]\n",
      "epoch:5 step:5334 [D loss: 0.207783, acc.: 68.75%] [G loss: 0.644833]\n",
      "epoch:5 step:5335 [D loss: 0.168678, acc.: 73.44%] [G loss: 0.646132]\n",
      "epoch:5 step:5336 [D loss: 0.212048, acc.: 71.09%] [G loss: 0.543130]\n",
      "epoch:5 step:5337 [D loss: 0.256771, acc.: 56.25%] [G loss: 0.495694]\n",
      "epoch:5 step:5338 [D loss: 0.199324, acc.: 70.31%] [G loss: 0.572128]\n",
      "epoch:5 step:5339 [D loss: 0.220459, acc.: 67.97%] [G loss: 0.528062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5340 [D loss: 0.238515, acc.: 58.59%] [G loss: 0.514325]\n",
      "epoch:5 step:5341 [D loss: 0.172404, acc.: 78.91%] [G loss: 0.531684]\n",
      "epoch:5 step:5342 [D loss: 0.192505, acc.: 67.97%] [G loss: 0.518806]\n",
      "epoch:5 step:5343 [D loss: 0.241215, acc.: 61.72%] [G loss: 0.535802]\n",
      "epoch:5 step:5344 [D loss: 0.209237, acc.: 67.19%] [G loss: 0.535820]\n",
      "epoch:5 step:5345 [D loss: 0.186273, acc.: 71.09%] [G loss: 0.540631]\n",
      "epoch:5 step:5346 [D loss: 0.160778, acc.: 80.47%] [G loss: 0.569604]\n",
      "epoch:5 step:5347 [D loss: 0.237141, acc.: 62.50%] [G loss: 0.490456]\n",
      "epoch:5 step:5348 [D loss: 0.197126, acc.: 64.84%] [G loss: 0.550597]\n",
      "epoch:5 step:5349 [D loss: 0.215976, acc.: 66.41%] [G loss: 0.547524]\n",
      "epoch:5 step:5350 [D loss: 0.226279, acc.: 60.16%] [G loss: 0.539879]\n",
      "epoch:5 step:5351 [D loss: 0.213077, acc.: 64.84%] [G loss: 0.555880]\n",
      "epoch:5 step:5352 [D loss: 0.198670, acc.: 67.97%] [G loss: 0.515966]\n",
      "epoch:5 step:5353 [D loss: 0.219154, acc.: 69.53%] [G loss: 0.539797]\n",
      "epoch:5 step:5354 [D loss: 0.185337, acc.: 75.00%] [G loss: 0.515226]\n",
      "epoch:5 step:5355 [D loss: 0.199880, acc.: 71.09%] [G loss: 0.512764]\n",
      "epoch:5 step:5356 [D loss: 0.208359, acc.: 65.62%] [G loss: 0.482070]\n",
      "epoch:5 step:5357 [D loss: 0.221241, acc.: 64.06%] [G loss: 0.580396]\n",
      "epoch:5 step:5358 [D loss: 0.224167, acc.: 68.75%] [G loss: 0.538772]\n",
      "epoch:5 step:5359 [D loss: 0.175954, acc.: 71.09%] [G loss: 0.570983]\n",
      "epoch:5 step:5360 [D loss: 0.211231, acc.: 70.31%] [G loss: 0.527939]\n",
      "epoch:5 step:5361 [D loss: 0.173609, acc.: 73.44%] [G loss: 0.549636]\n",
      "epoch:5 step:5362 [D loss: 0.192911, acc.: 68.75%] [G loss: 0.536319]\n",
      "epoch:5 step:5363 [D loss: 0.204611, acc.: 67.97%] [G loss: 0.461607]\n",
      "epoch:5 step:5364 [D loss: 0.210029, acc.: 68.75%] [G loss: 0.539305]\n",
      "epoch:5 step:5365 [D loss: 0.215371, acc.: 64.84%] [G loss: 0.528155]\n",
      "epoch:5 step:5366 [D loss: 0.180688, acc.: 71.09%] [G loss: 0.541869]\n",
      "epoch:5 step:5367 [D loss: 0.199974, acc.: 70.31%] [G loss: 0.505218]\n",
      "epoch:5 step:5368 [D loss: 0.216810, acc.: 62.50%] [G loss: 0.475023]\n",
      "epoch:5 step:5369 [D loss: 0.202976, acc.: 67.97%] [G loss: 0.504029]\n",
      "epoch:5 step:5370 [D loss: 0.190750, acc.: 69.53%] [G loss: 0.517027]\n",
      "epoch:5 step:5371 [D loss: 0.192703, acc.: 68.75%] [G loss: 0.522796]\n",
      "epoch:5 step:5372 [D loss: 0.204234, acc.: 70.31%] [G loss: 0.508814]\n",
      "epoch:5 step:5373 [D loss: 0.207197, acc.: 72.66%] [G loss: 0.525279]\n",
      "epoch:5 step:5374 [D loss: 0.193044, acc.: 73.44%] [G loss: 0.615712]\n",
      "epoch:5 step:5375 [D loss: 0.165152, acc.: 77.34%] [G loss: 0.573783]\n",
      "epoch:5 step:5376 [D loss: 0.160710, acc.: 78.91%] [G loss: 0.588936]\n",
      "epoch:5 step:5377 [D loss: 0.210670, acc.: 68.75%] [G loss: 0.558000]\n",
      "epoch:5 step:5378 [D loss: 0.190051, acc.: 73.44%] [G loss: 0.587997]\n",
      "epoch:5 step:5379 [D loss: 0.185769, acc.: 71.09%] [G loss: 0.570990]\n",
      "epoch:5 step:5380 [D loss: 0.182294, acc.: 74.22%] [G loss: 0.572614]\n",
      "epoch:5 step:5381 [D loss: 0.206292, acc.: 64.06%] [G loss: 0.546336]\n",
      "epoch:5 step:5382 [D loss: 0.200619, acc.: 68.75%] [G loss: 0.557886]\n",
      "epoch:5 step:5383 [D loss: 0.189095, acc.: 73.44%] [G loss: 0.531222]\n",
      "epoch:5 step:5384 [D loss: 0.193403, acc.: 70.31%] [G loss: 0.544874]\n",
      "epoch:5 step:5385 [D loss: 0.228386, acc.: 63.28%] [G loss: 0.558121]\n",
      "epoch:5 step:5386 [D loss: 0.167714, acc.: 75.78%] [G loss: 0.615019]\n",
      "epoch:5 step:5387 [D loss: 0.249038, acc.: 59.38%] [G loss: 0.475505]\n",
      "epoch:5 step:5388 [D loss: 0.235900, acc.: 58.59%] [G loss: 0.502910]\n",
      "epoch:5 step:5389 [D loss: 0.212846, acc.: 64.06%] [G loss: 0.530058]\n",
      "epoch:5 step:5390 [D loss: 0.188246, acc.: 78.12%] [G loss: 0.548383]\n",
      "epoch:5 step:5391 [D loss: 0.220941, acc.: 67.19%] [G loss: 0.454202]\n",
      "epoch:5 step:5392 [D loss: 0.172418, acc.: 76.56%] [G loss: 0.536052]\n",
      "epoch:5 step:5393 [D loss: 0.191119, acc.: 70.31%] [G loss: 0.602831]\n",
      "epoch:5 step:5394 [D loss: 0.200383, acc.: 73.44%] [G loss: 0.562471]\n",
      "epoch:5 step:5395 [D loss: 0.225588, acc.: 67.19%] [G loss: 0.567772]\n",
      "epoch:5 step:5396 [D loss: 0.207944, acc.: 67.97%] [G loss: 0.513186]\n",
      "epoch:5 step:5397 [D loss: 0.221598, acc.: 64.84%] [G loss: 0.525995]\n",
      "epoch:5 step:5398 [D loss: 0.235575, acc.: 62.50%] [G loss: 0.501516]\n",
      "epoch:5 step:5399 [D loss: 0.193759, acc.: 68.75%] [G loss: 0.553255]\n",
      "epoch:5 step:5400 [D loss: 0.221081, acc.: 64.06%] [G loss: 0.539497]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.648387\n",
      "FID: 27.351063\n",
      "0 = 12.45168332982062\n",
      "1 = 0.06346153668312479\n",
      "2 = 0.9519000053405762\n",
      "3 = 0.9264000058174133\n",
      "4 = 0.977400004863739\n",
      "5 = 0.9761854410171509\n",
      "6 = 0.9264000058174133\n",
      "7 = 8.195639530235546\n",
      "8 = 0.11481515956699291\n",
      "9 = 0.7824000120162964\n",
      "10 = 0.7746999859809875\n",
      "11 = 0.7900999784469604\n",
      "12 = 0.7868169546127319\n",
      "13 = 0.7746999859809875\n",
      "14 = 6.648441791534424\n",
      "15 = 9.30945873260498\n",
      "16 = 0.1756509691476822\n",
      "17 = 6.648387432098389\n",
      "18 = 27.351062774658203\n",
      "epoch:5 step:5401 [D loss: 0.289666, acc.: 47.66%] [G loss: 0.463265]\n",
      "epoch:5 step:5402 [D loss: 0.202347, acc.: 72.66%] [G loss: 0.533156]\n",
      "epoch:5 step:5403 [D loss: 0.232501, acc.: 64.06%] [G loss: 0.514263]\n",
      "epoch:5 step:5404 [D loss: 0.188734, acc.: 67.97%] [G loss: 0.576926]\n",
      "epoch:5 step:5405 [D loss: 0.229448, acc.: 66.41%] [G loss: 0.554954]\n",
      "epoch:5 step:5406 [D loss: 0.190545, acc.: 67.97%] [G loss: 0.569513]\n",
      "epoch:5 step:5407 [D loss: 0.197899, acc.: 74.22%] [G loss: 0.585602]\n",
      "epoch:5 step:5408 [D loss: 0.215626, acc.: 64.06%] [G loss: 0.548810]\n",
      "epoch:5 step:5409 [D loss: 0.207375, acc.: 65.62%] [G loss: 0.537038]\n",
      "epoch:5 step:5410 [D loss: 0.179821, acc.: 76.56%] [G loss: 0.582298]\n",
      "epoch:5 step:5411 [D loss: 0.187026, acc.: 74.22%] [G loss: 0.543967]\n",
      "epoch:5 step:5412 [D loss: 0.226435, acc.: 64.84%] [G loss: 0.499370]\n",
      "epoch:5 step:5413 [D loss: 0.185262, acc.: 74.22%] [G loss: 0.513619]\n",
      "epoch:5 step:5414 [D loss: 0.215214, acc.: 70.31%] [G loss: 0.498844]\n",
      "epoch:5 step:5415 [D loss: 0.160135, acc.: 81.25%] [G loss: 0.585105]\n",
      "epoch:5 step:5416 [D loss: 0.189279, acc.: 71.09%] [G loss: 0.582252]\n",
      "epoch:5 step:5417 [D loss: 0.196117, acc.: 68.75%] [G loss: 0.522108]\n",
      "epoch:5 step:5418 [D loss: 0.187315, acc.: 71.09%] [G loss: 0.502210]\n",
      "epoch:5 step:5419 [D loss: 0.217712, acc.: 67.19%] [G loss: 0.533149]\n",
      "epoch:5 step:5420 [D loss: 0.211975, acc.: 65.62%] [G loss: 0.488150]\n",
      "epoch:5 step:5421 [D loss: 0.191292, acc.: 72.66%] [G loss: 0.516327]\n",
      "epoch:5 step:5422 [D loss: 0.170105, acc.: 75.00%] [G loss: 0.574363]\n",
      "epoch:5 step:5423 [D loss: 0.233184, acc.: 59.38%] [G loss: 0.486189]\n",
      "epoch:5 step:5424 [D loss: 0.248955, acc.: 60.16%] [G loss: 0.473913]\n",
      "epoch:5 step:5425 [D loss: 0.215780, acc.: 66.41%] [G loss: 0.525733]\n",
      "epoch:5 step:5426 [D loss: 0.248391, acc.: 56.25%] [G loss: 0.511457]\n",
      "epoch:5 step:5427 [D loss: 0.229521, acc.: 67.97%] [G loss: 0.508985]\n",
      "epoch:5 step:5428 [D loss: 0.171602, acc.: 79.69%] [G loss: 0.567299]\n",
      "epoch:5 step:5429 [D loss: 0.231125, acc.: 63.28%] [G loss: 0.514059]\n",
      "epoch:5 step:5430 [D loss: 0.210226, acc.: 66.41%] [G loss: 0.529414]\n",
      "epoch:5 step:5431 [D loss: 0.176625, acc.: 72.66%] [G loss: 0.571249]\n",
      "epoch:5 step:5432 [D loss: 0.199948, acc.: 66.41%] [G loss: 0.509638]\n",
      "epoch:5 step:5433 [D loss: 0.202847, acc.: 67.97%] [G loss: 0.500476]\n",
      "epoch:5 step:5434 [D loss: 0.217268, acc.: 67.97%] [G loss: 0.485883]\n",
      "epoch:5 step:5435 [D loss: 0.225890, acc.: 63.28%] [G loss: 0.472177]\n",
      "epoch:5 step:5436 [D loss: 0.182664, acc.: 68.75%] [G loss: 0.522132]\n",
      "epoch:5 step:5437 [D loss: 0.216305, acc.: 62.50%] [G loss: 0.521349]\n",
      "epoch:5 step:5438 [D loss: 0.185381, acc.: 69.53%] [G loss: 0.496711]\n",
      "epoch:5 step:5439 [D loss: 0.175646, acc.: 75.78%] [G loss: 0.528333]\n",
      "epoch:5 step:5440 [D loss: 0.191350, acc.: 71.09%] [G loss: 0.508633]\n",
      "epoch:5 step:5441 [D loss: 0.202882, acc.: 71.09%] [G loss: 0.529640]\n",
      "epoch:5 step:5442 [D loss: 0.201866, acc.: 69.53%] [G loss: 0.541878]\n",
      "epoch:5 step:5443 [D loss: 0.225084, acc.: 65.62%] [G loss: 0.543617]\n",
      "epoch:5 step:5444 [D loss: 0.213967, acc.: 64.06%] [G loss: 0.481033]\n",
      "epoch:5 step:5445 [D loss: 0.197963, acc.: 70.31%] [G loss: 0.474931]\n",
      "epoch:5 step:5446 [D loss: 0.228980, acc.: 64.06%] [G loss: 0.509906]\n",
      "epoch:5 step:5447 [D loss: 0.190962, acc.: 72.66%] [G loss: 0.545796]\n",
      "epoch:5 step:5448 [D loss: 0.213620, acc.: 67.19%] [G loss: 0.497878]\n",
      "epoch:5 step:5449 [D loss: 0.202472, acc.: 66.41%] [G loss: 0.523707]\n",
      "epoch:5 step:5450 [D loss: 0.267515, acc.: 56.25%] [G loss: 0.554467]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5451 [D loss: 0.236197, acc.: 65.62%] [G loss: 0.584493]\n",
      "epoch:5 step:5452 [D loss: 0.203687, acc.: 69.53%] [G loss: 0.588000]\n",
      "epoch:5 step:5453 [D loss: 0.224542, acc.: 64.06%] [G loss: 0.527993]\n",
      "epoch:5 step:5454 [D loss: 0.182294, acc.: 67.97%] [G loss: 0.566714]\n",
      "epoch:5 step:5455 [D loss: 0.197240, acc.: 73.44%] [G loss: 0.547477]\n",
      "epoch:5 step:5456 [D loss: 0.176960, acc.: 75.00%] [G loss: 0.561008]\n",
      "epoch:5 step:5457 [D loss: 0.232716, acc.: 63.28%] [G loss: 0.538487]\n",
      "epoch:5 step:5458 [D loss: 0.206371, acc.: 68.75%] [G loss: 0.532425]\n",
      "epoch:5 step:5459 [D loss: 0.205370, acc.: 71.09%] [G loss: 0.536226]\n",
      "epoch:5 step:5460 [D loss: 0.187592, acc.: 77.34%] [G loss: 0.545394]\n",
      "epoch:5 step:5461 [D loss: 0.189235, acc.: 74.22%] [G loss: 0.515400]\n",
      "epoch:5 step:5462 [D loss: 0.194461, acc.: 71.09%] [G loss: 0.521374]\n",
      "epoch:5 step:5463 [D loss: 0.194468, acc.: 73.44%] [G loss: 0.510720]\n",
      "epoch:5 step:5464 [D loss: 0.235182, acc.: 67.97%] [G loss: 0.511130]\n",
      "epoch:5 step:5465 [D loss: 0.217625, acc.: 68.75%] [G loss: 0.548914]\n",
      "epoch:5 step:5466 [D loss: 0.174139, acc.: 76.56%] [G loss: 0.638520]\n",
      "epoch:5 step:5467 [D loss: 0.195626, acc.: 67.97%] [G loss: 0.643327]\n",
      "epoch:5 step:5468 [D loss: 0.194627, acc.: 69.53%] [G loss: 0.580299]\n",
      "epoch:5 step:5469 [D loss: 0.243206, acc.: 58.59%] [G loss: 0.438290]\n",
      "epoch:5 step:5470 [D loss: 0.202038, acc.: 73.44%] [G loss: 0.472766]\n",
      "epoch:5 step:5471 [D loss: 0.172692, acc.: 78.91%] [G loss: 0.567535]\n",
      "epoch:5 step:5472 [D loss: 0.262073, acc.: 59.38%] [G loss: 0.479928]\n",
      "epoch:5 step:5473 [D loss: 0.232425, acc.: 60.16%] [G loss: 0.510694]\n",
      "epoch:5 step:5474 [D loss: 0.202905, acc.: 70.31%] [G loss: 0.573195]\n",
      "epoch:5 step:5475 [D loss: 0.195260, acc.: 69.53%] [G loss: 0.538385]\n",
      "epoch:5 step:5476 [D loss: 0.227201, acc.: 64.06%] [G loss: 0.456830]\n",
      "epoch:5 step:5477 [D loss: 0.168520, acc.: 76.56%] [G loss: 0.554673]\n",
      "epoch:5 step:5478 [D loss: 0.204547, acc.: 66.41%] [G loss: 0.558616]\n",
      "epoch:5 step:5479 [D loss: 0.267112, acc.: 58.59%] [G loss: 0.497265]\n",
      "epoch:5 step:5480 [D loss: 0.219754, acc.: 66.41%] [G loss: 0.510948]\n",
      "epoch:5 step:5481 [D loss: 0.195203, acc.: 70.31%] [G loss: 0.563223]\n",
      "epoch:5 step:5482 [D loss: 0.211352, acc.: 64.84%] [G loss: 0.583370]\n",
      "epoch:5 step:5483 [D loss: 0.191710, acc.: 70.31%] [G loss: 0.523633]\n",
      "epoch:5 step:5484 [D loss: 0.185486, acc.: 71.88%] [G loss: 0.594059]\n",
      "epoch:5 step:5485 [D loss: 0.204949, acc.: 67.19%] [G loss: 0.536691]\n",
      "epoch:5 step:5486 [D loss: 0.185696, acc.: 72.66%] [G loss: 0.605698]\n",
      "epoch:5 step:5487 [D loss: 0.184306, acc.: 67.97%] [G loss: 0.558521]\n",
      "epoch:5 step:5488 [D loss: 0.194342, acc.: 66.41%] [G loss: 0.555735]\n",
      "epoch:5 step:5489 [D loss: 0.204025, acc.: 64.06%] [G loss: 0.516660]\n",
      "epoch:5 step:5490 [D loss: 0.173407, acc.: 68.75%] [G loss: 0.545539]\n",
      "epoch:5 step:5491 [D loss: 0.184170, acc.: 70.31%] [G loss: 0.497360]\n",
      "epoch:5 step:5492 [D loss: 0.203814, acc.: 67.97%] [G loss: 0.582424]\n",
      "epoch:5 step:5493 [D loss: 0.228474, acc.: 63.28%] [G loss: 0.531187]\n",
      "epoch:5 step:5494 [D loss: 0.218385, acc.: 69.53%] [G loss: 0.515577]\n",
      "epoch:5 step:5495 [D loss: 0.203807, acc.: 69.53%] [G loss: 0.512100]\n",
      "epoch:5 step:5496 [D loss: 0.216852, acc.: 64.84%] [G loss: 0.483738]\n",
      "epoch:5 step:5497 [D loss: 0.205914, acc.: 71.09%] [G loss: 0.521669]\n",
      "epoch:5 step:5498 [D loss: 0.205382, acc.: 69.53%] [G loss: 0.481548]\n",
      "epoch:5 step:5499 [D loss: 0.196470, acc.: 68.75%] [G loss: 0.543728]\n",
      "epoch:5 step:5500 [D loss: 0.194820, acc.: 71.09%] [G loss: 0.593869]\n",
      "epoch:5 step:5501 [D loss: 0.198357, acc.: 67.97%] [G loss: 0.547772]\n",
      "epoch:5 step:5502 [D loss: 0.220548, acc.: 69.53%] [G loss: 0.538803]\n",
      "epoch:5 step:5503 [D loss: 0.245443, acc.: 60.16%] [G loss: 0.477873]\n",
      "epoch:5 step:5504 [D loss: 0.189300, acc.: 73.44%] [G loss: 0.574712]\n",
      "epoch:5 step:5505 [D loss: 0.243749, acc.: 61.72%] [G loss: 0.521441]\n",
      "epoch:5 step:5506 [D loss: 0.185299, acc.: 71.88%] [G loss: 0.549561]\n",
      "epoch:5 step:5507 [D loss: 0.175255, acc.: 75.00%] [G loss: 0.604110]\n",
      "epoch:5 step:5508 [D loss: 0.199074, acc.: 68.75%] [G loss: 0.512973]\n",
      "epoch:5 step:5509 [D loss: 0.247595, acc.: 59.38%] [G loss: 0.470784]\n",
      "epoch:5 step:5510 [D loss: 0.175437, acc.: 77.34%] [G loss: 0.519884]\n",
      "epoch:5 step:5511 [D loss: 0.227392, acc.: 61.72%] [G loss: 0.555772]\n",
      "epoch:5 step:5512 [D loss: 0.263043, acc.: 54.69%] [G loss: 0.465361]\n",
      "epoch:5 step:5513 [D loss: 0.234862, acc.: 57.81%] [G loss: 0.509573]\n",
      "epoch:5 step:5514 [D loss: 0.200546, acc.: 65.62%] [G loss: 0.520609]\n",
      "epoch:5 step:5515 [D loss: 0.222232, acc.: 63.28%] [G loss: 0.513959]\n",
      "epoch:5 step:5516 [D loss: 0.197707, acc.: 65.62%] [G loss: 0.501973]\n",
      "epoch:5 step:5517 [D loss: 0.199335, acc.: 75.78%] [G loss: 0.510426]\n",
      "epoch:5 step:5518 [D loss: 0.187711, acc.: 67.97%] [G loss: 0.589349]\n",
      "epoch:5 step:5519 [D loss: 0.243462, acc.: 62.50%] [G loss: 0.500170]\n",
      "epoch:5 step:5520 [D loss: 0.162788, acc.: 82.03%] [G loss: 0.553641]\n",
      "epoch:5 step:5521 [D loss: 0.214874, acc.: 65.62%] [G loss: 0.516132]\n",
      "epoch:5 step:5522 [D loss: 0.174178, acc.: 75.00%] [G loss: 0.524518]\n",
      "epoch:5 step:5523 [D loss: 0.209006, acc.: 67.19%] [G loss: 0.510783]\n",
      "epoch:5 step:5524 [D loss: 0.194967, acc.: 67.97%] [G loss: 0.593732]\n",
      "epoch:5 step:5525 [D loss: 0.194830, acc.: 71.88%] [G loss: 0.542006]\n",
      "epoch:5 step:5526 [D loss: 0.167449, acc.: 81.25%] [G loss: 0.556467]\n",
      "epoch:5 step:5527 [D loss: 0.161311, acc.: 75.78%] [G loss: 0.533739]\n",
      "epoch:5 step:5528 [D loss: 0.204347, acc.: 68.75%] [G loss: 0.561966]\n",
      "epoch:5 step:5529 [D loss: 0.208116, acc.: 69.53%] [G loss: 0.564723]\n",
      "epoch:5 step:5530 [D loss: 0.198354, acc.: 68.75%] [G loss: 0.535788]\n",
      "epoch:5 step:5531 [D loss: 0.203481, acc.: 70.31%] [G loss: 0.528349]\n",
      "epoch:5 step:5532 [D loss: 0.208070, acc.: 70.31%] [G loss: 0.467991]\n",
      "epoch:5 step:5533 [D loss: 0.194302, acc.: 67.97%] [G loss: 0.509135]\n",
      "epoch:5 step:5534 [D loss: 0.207052, acc.: 67.19%] [G loss: 0.487231]\n",
      "epoch:5 step:5535 [D loss: 0.241505, acc.: 56.25%] [G loss: 0.526826]\n",
      "epoch:5 step:5536 [D loss: 0.212137, acc.: 70.31%] [G loss: 0.508305]\n",
      "epoch:5 step:5537 [D loss: 0.203789, acc.: 69.53%] [G loss: 0.524122]\n",
      "epoch:5 step:5538 [D loss: 0.173894, acc.: 78.91%] [G loss: 0.541942]\n",
      "epoch:5 step:5539 [D loss: 0.202574, acc.: 68.75%] [G loss: 0.556924]\n",
      "epoch:5 step:5540 [D loss: 0.215726, acc.: 67.19%] [G loss: 0.519822]\n",
      "epoch:5 step:5541 [D loss: 0.219075, acc.: 61.72%] [G loss: 0.549071]\n",
      "epoch:5 step:5542 [D loss: 0.182003, acc.: 75.00%] [G loss: 0.570271]\n",
      "epoch:5 step:5543 [D loss: 0.240413, acc.: 61.72%] [G loss: 0.537701]\n",
      "epoch:5 step:5544 [D loss: 0.239423, acc.: 67.19%] [G loss: 0.486156]\n",
      "epoch:5 step:5545 [D loss: 0.178107, acc.: 70.31%] [G loss: 0.553315]\n",
      "epoch:5 step:5546 [D loss: 0.266447, acc.: 53.12%] [G loss: 0.491293]\n",
      "epoch:5 step:5547 [D loss: 0.236827, acc.: 57.81%] [G loss: 0.492067]\n",
      "epoch:5 step:5548 [D loss: 0.179074, acc.: 77.34%] [G loss: 0.497609]\n",
      "epoch:5 step:5549 [D loss: 0.203602, acc.: 69.53%] [G loss: 0.518281]\n",
      "epoch:5 step:5550 [D loss: 0.207324, acc.: 75.00%] [G loss: 0.539017]\n",
      "epoch:5 step:5551 [D loss: 0.226305, acc.: 66.41%] [G loss: 0.468582]\n",
      "epoch:5 step:5552 [D loss: 0.245690, acc.: 62.50%] [G loss: 0.489647]\n",
      "epoch:5 step:5553 [D loss: 0.192874, acc.: 75.00%] [G loss: 0.539629]\n",
      "epoch:5 step:5554 [D loss: 0.204451, acc.: 69.53%] [G loss: 0.529571]\n",
      "epoch:5 step:5555 [D loss: 0.199736, acc.: 70.31%] [G loss: 0.542550]\n",
      "epoch:5 step:5556 [D loss: 0.216284, acc.: 64.06%] [G loss: 0.511630]\n",
      "epoch:5 step:5557 [D loss: 0.202398, acc.: 64.84%] [G loss: 0.554821]\n",
      "epoch:5 step:5558 [D loss: 0.220001, acc.: 63.28%] [G loss: 0.515325]\n",
      "epoch:5 step:5559 [D loss: 0.200208, acc.: 70.31%] [G loss: 0.570503]\n",
      "epoch:5 step:5560 [D loss: 0.168936, acc.: 77.34%] [G loss: 0.588966]\n",
      "epoch:5 step:5561 [D loss: 0.226000, acc.: 68.75%] [G loss: 0.520697]\n",
      "epoch:5 step:5562 [D loss: 0.220446, acc.: 64.84%] [G loss: 0.569936]\n",
      "epoch:5 step:5563 [D loss: 0.242085, acc.: 57.81%] [G loss: 0.506859]\n",
      "epoch:5 step:5564 [D loss: 0.239580, acc.: 58.59%] [G loss: 0.499785]\n",
      "epoch:5 step:5565 [D loss: 0.213806, acc.: 71.09%] [G loss: 0.527038]\n",
      "epoch:5 step:5566 [D loss: 0.191163, acc.: 70.31%] [G loss: 0.508335]\n",
      "epoch:5 step:5567 [D loss: 0.203573, acc.: 69.53%] [G loss: 0.533182]\n",
      "epoch:5 step:5568 [D loss: 0.193198, acc.: 75.00%] [G loss: 0.535209]\n",
      "epoch:5 step:5569 [D loss: 0.165478, acc.: 78.12%] [G loss: 0.585327]\n",
      "epoch:5 step:5570 [D loss: 0.181509, acc.: 77.34%] [G loss: 0.597670]\n",
      "epoch:5 step:5571 [D loss: 0.216446, acc.: 63.28%] [G loss: 0.561017]\n",
      "epoch:5 step:5572 [D loss: 0.218698, acc.: 61.72%] [G loss: 0.562459]\n",
      "epoch:5 step:5573 [D loss: 0.195523, acc.: 74.22%] [G loss: 0.502237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5574 [D loss: 0.190457, acc.: 77.34%] [G loss: 0.575375]\n",
      "epoch:5 step:5575 [D loss: 0.184490, acc.: 72.66%] [G loss: 0.574755]\n",
      "epoch:5 step:5576 [D loss: 0.240611, acc.: 61.72%] [G loss: 0.493592]\n",
      "epoch:5 step:5577 [D loss: 0.242614, acc.: 59.38%] [G loss: 0.509443]\n",
      "epoch:5 step:5578 [D loss: 0.195279, acc.: 65.62%] [G loss: 0.546308]\n",
      "epoch:5 step:5579 [D loss: 0.169819, acc.: 75.00%] [G loss: 0.585813]\n",
      "epoch:5 step:5580 [D loss: 0.216865, acc.: 64.06%] [G loss: 0.561391]\n",
      "epoch:5 step:5581 [D loss: 0.196589, acc.: 68.75%] [G loss: 0.534059]\n",
      "epoch:5 step:5582 [D loss: 0.185425, acc.: 75.00%] [G loss: 0.544383]\n",
      "epoch:5 step:5583 [D loss: 0.182839, acc.: 75.00%] [G loss: 0.541437]\n",
      "epoch:5 step:5584 [D loss: 0.175148, acc.: 73.44%] [G loss: 0.586152]\n",
      "epoch:5 step:5585 [D loss: 0.199700, acc.: 71.09%] [G loss: 0.554799]\n",
      "epoch:5 step:5586 [D loss: 0.213260, acc.: 67.19%] [G loss: 0.562731]\n",
      "epoch:5 step:5587 [D loss: 0.233484, acc.: 65.62%] [G loss: 0.513562]\n",
      "epoch:5 step:5588 [D loss: 0.215781, acc.: 64.06%] [G loss: 0.517391]\n",
      "epoch:5 step:5589 [D loss: 0.199041, acc.: 75.00%] [G loss: 0.527426]\n",
      "epoch:5 step:5590 [D loss: 0.185911, acc.: 78.12%] [G loss: 0.598401]\n",
      "epoch:5 step:5591 [D loss: 0.195469, acc.: 75.00%] [G loss: 0.513120]\n",
      "epoch:5 step:5592 [D loss: 0.204504, acc.: 66.41%] [G loss: 0.545563]\n",
      "epoch:5 step:5593 [D loss: 0.181795, acc.: 78.91%] [G loss: 0.508704]\n",
      "epoch:5 step:5594 [D loss: 0.162751, acc.: 82.03%] [G loss: 0.555299]\n",
      "epoch:5 step:5595 [D loss: 0.207643, acc.: 65.62%] [G loss: 0.520849]\n",
      "epoch:5 step:5596 [D loss: 0.175076, acc.: 73.44%] [G loss: 0.572916]\n",
      "epoch:5 step:5597 [D loss: 0.161945, acc.: 82.03%] [G loss: 0.609982]\n",
      "epoch:5 step:5598 [D loss: 0.208339, acc.: 67.19%] [G loss: 0.544769]\n",
      "epoch:5 step:5599 [D loss: 0.177144, acc.: 72.66%] [G loss: 0.532249]\n",
      "epoch:5 step:5600 [D loss: 0.254518, acc.: 57.03%] [G loss: 0.464160]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.759928\n",
      "FID: 24.715210\n",
      "0 = 12.40317343072887\n",
      "1 = 0.06545518975952234\n",
      "2 = 0.9507499933242798\n",
      "3 = 0.9258999824523926\n",
      "4 = 0.975600004196167\n",
      "5 = 0.9743238687515259\n",
      "6 = 0.9258999824523926\n",
      "7 = 7.938062506628054\n",
      "8 = 0.10952856999273441\n",
      "9 = 0.7838000059127808\n",
      "10 = 0.775600016117096\n",
      "11 = 0.7919999957084656\n",
      "12 = 0.7885318994522095\n",
      "13 = 0.775600016117096\n",
      "14 = 6.759981632232666\n",
      "15 = 9.374786376953125\n",
      "16 = 0.16719429194927216\n",
      "17 = 6.759927749633789\n",
      "18 = 24.7152099609375\n",
      "epoch:5 step:5601 [D loss: 0.192197, acc.: 71.09%] [G loss: 0.557744]\n",
      "epoch:5 step:5602 [D loss: 0.227085, acc.: 64.06%] [G loss: 0.558503]\n",
      "epoch:5 step:5603 [D loss: 0.174928, acc.: 75.00%] [G loss: 0.636547]\n",
      "epoch:5 step:5604 [D loss: 0.200094, acc.: 69.53%] [G loss: 0.573372]\n",
      "epoch:5 step:5605 [D loss: 0.292747, acc.: 50.78%] [G loss: 0.533195]\n",
      "epoch:5 step:5606 [D loss: 0.172819, acc.: 71.09%] [G loss: 0.596292]\n",
      "epoch:5 step:5607 [D loss: 0.238136, acc.: 59.38%] [G loss: 0.541365]\n",
      "epoch:5 step:5608 [D loss: 0.175797, acc.: 73.44%] [G loss: 0.569099]\n",
      "epoch:5 step:5609 [D loss: 0.154538, acc.: 79.69%] [G loss: 0.590374]\n",
      "epoch:5 step:5610 [D loss: 0.135365, acc.: 84.38%] [G loss: 0.639915]\n",
      "epoch:5 step:5611 [D loss: 0.171193, acc.: 78.12%] [G loss: 0.646451]\n",
      "epoch:5 step:5612 [D loss: 0.147904, acc.: 77.34%] [G loss: 0.667172]\n",
      "epoch:5 step:5613 [D loss: 0.380700, acc.: 50.78%] [G loss: 0.589663]\n",
      "epoch:5 step:5614 [D loss: 0.119965, acc.: 86.72%] [G loss: 0.745379]\n",
      "epoch:5 step:5615 [D loss: 0.218320, acc.: 64.84%] [G loss: 0.574239]\n",
      "epoch:5 step:5616 [D loss: 0.219647, acc.: 60.94%] [G loss: 0.518152]\n",
      "epoch:5 step:5617 [D loss: 0.218003, acc.: 64.06%] [G loss: 0.530369]\n",
      "epoch:5 step:5618 [D loss: 0.189449, acc.: 70.31%] [G loss: 0.550834]\n",
      "epoch:5 step:5619 [D loss: 0.234827, acc.: 58.59%] [G loss: 0.555328]\n",
      "epoch:5 step:5620 [D loss: 0.196120, acc.: 70.31%] [G loss: 0.521358]\n",
      "epoch:5 step:5621 [D loss: 0.130995, acc.: 83.59%] [G loss: 0.664521]\n",
      "epoch:5 step:5622 [D loss: 0.150374, acc.: 83.59%] [G loss: 0.692404]\n",
      "epoch:6 step:5623 [D loss: 0.265290, acc.: 60.16%] [G loss: 0.562283]\n",
      "epoch:6 step:5624 [D loss: 0.235308, acc.: 65.62%] [G loss: 0.500938]\n",
      "epoch:6 step:5625 [D loss: 0.206614, acc.: 69.53%] [G loss: 0.549644]\n",
      "epoch:6 step:5626 [D loss: 0.191483, acc.: 67.97%] [G loss: 0.582183]\n",
      "epoch:6 step:5627 [D loss: 0.223875, acc.: 60.94%] [G loss: 0.533767]\n",
      "epoch:6 step:5628 [D loss: 0.199721, acc.: 67.19%] [G loss: 0.571865]\n",
      "epoch:6 step:5629 [D loss: 0.202492, acc.: 74.22%] [G loss: 0.531125]\n",
      "epoch:6 step:5630 [D loss: 0.213105, acc.: 66.41%] [G loss: 0.537004]\n",
      "epoch:6 step:5631 [D loss: 0.174809, acc.: 75.00%] [G loss: 0.550189]\n",
      "epoch:6 step:5632 [D loss: 0.193898, acc.: 67.19%] [G loss: 0.541005]\n",
      "epoch:6 step:5633 [D loss: 0.184283, acc.: 75.00%] [G loss: 0.548899]\n",
      "epoch:6 step:5634 [D loss: 0.184755, acc.: 75.00%] [G loss: 0.573811]\n",
      "epoch:6 step:5635 [D loss: 0.181885, acc.: 69.53%] [G loss: 0.556980]\n",
      "epoch:6 step:5636 [D loss: 0.206330, acc.: 67.19%] [G loss: 0.517023]\n",
      "epoch:6 step:5637 [D loss: 0.156355, acc.: 78.12%] [G loss: 0.598635]\n",
      "epoch:6 step:5638 [D loss: 0.203247, acc.: 67.19%] [G loss: 0.521527]\n",
      "epoch:6 step:5639 [D loss: 0.241091, acc.: 59.38%] [G loss: 0.531848]\n",
      "epoch:6 step:5640 [D loss: 0.197845, acc.: 67.19%] [G loss: 0.539563]\n",
      "epoch:6 step:5641 [D loss: 0.188784, acc.: 74.22%] [G loss: 0.524119]\n",
      "epoch:6 step:5642 [D loss: 0.233017, acc.: 64.06%] [G loss: 0.519402]\n",
      "epoch:6 step:5643 [D loss: 0.210301, acc.: 68.75%] [G loss: 0.530038]\n",
      "epoch:6 step:5644 [D loss: 0.176400, acc.: 74.22%] [G loss: 0.595438]\n",
      "epoch:6 step:5645 [D loss: 0.198127, acc.: 64.06%] [G loss: 0.575304]\n",
      "epoch:6 step:5646 [D loss: 0.202170, acc.: 71.88%] [G loss: 0.561372]\n",
      "epoch:6 step:5647 [D loss: 0.173338, acc.: 73.44%] [G loss: 0.557179]\n",
      "epoch:6 step:5648 [D loss: 0.216010, acc.: 66.41%] [G loss: 0.547248]\n",
      "epoch:6 step:5649 [D loss: 0.196446, acc.: 69.53%] [G loss: 0.530657]\n",
      "epoch:6 step:5650 [D loss: 0.190996, acc.: 70.31%] [G loss: 0.515228]\n",
      "epoch:6 step:5651 [D loss: 0.187149, acc.: 74.22%] [G loss: 0.540829]\n",
      "epoch:6 step:5652 [D loss: 0.196112, acc.: 73.44%] [G loss: 0.549921]\n",
      "epoch:6 step:5653 [D loss: 0.174867, acc.: 75.78%] [G loss: 0.551512]\n",
      "epoch:6 step:5654 [D loss: 0.213255, acc.: 71.09%] [G loss: 0.519629]\n",
      "epoch:6 step:5655 [D loss: 0.164206, acc.: 75.78%] [G loss: 0.566776]\n",
      "epoch:6 step:5656 [D loss: 0.203282, acc.: 68.75%] [G loss: 0.476689]\n",
      "epoch:6 step:5657 [D loss: 0.204532, acc.: 70.31%] [G loss: 0.558400]\n",
      "epoch:6 step:5658 [D loss: 0.187081, acc.: 74.22%] [G loss: 0.541913]\n",
      "epoch:6 step:5659 [D loss: 0.238918, acc.: 65.62%] [G loss: 0.527568]\n",
      "epoch:6 step:5660 [D loss: 0.242675, acc.: 60.94%] [G loss: 0.512993]\n",
      "epoch:6 step:5661 [D loss: 0.204983, acc.: 67.19%] [G loss: 0.514585]\n",
      "epoch:6 step:5662 [D loss: 0.177641, acc.: 75.00%] [G loss: 0.577392]\n",
      "epoch:6 step:5663 [D loss: 0.220980, acc.: 66.41%] [G loss: 0.529336]\n",
      "epoch:6 step:5664 [D loss: 0.158902, acc.: 78.91%] [G loss: 0.548079]\n",
      "epoch:6 step:5665 [D loss: 0.207220, acc.: 71.09%] [G loss: 0.507218]\n",
      "epoch:6 step:5666 [D loss: 0.199741, acc.: 69.53%] [G loss: 0.543583]\n",
      "epoch:6 step:5667 [D loss: 0.204564, acc.: 66.41%] [G loss: 0.545609]\n",
      "epoch:6 step:5668 [D loss: 0.214910, acc.: 68.75%] [G loss: 0.528241]\n",
      "epoch:6 step:5669 [D loss: 0.174545, acc.: 76.56%] [G loss: 0.567802]\n",
      "epoch:6 step:5670 [D loss: 0.191565, acc.: 73.44%] [G loss: 0.576746]\n",
      "epoch:6 step:5671 [D loss: 0.186807, acc.: 72.66%] [G loss: 0.572962]\n",
      "epoch:6 step:5672 [D loss: 0.192169, acc.: 66.41%] [G loss: 0.524413]\n",
      "epoch:6 step:5673 [D loss: 0.225498, acc.: 62.50%] [G loss: 0.464287]\n",
      "epoch:6 step:5674 [D loss: 0.202530, acc.: 71.88%] [G loss: 0.553043]\n",
      "epoch:6 step:5675 [D loss: 0.212375, acc.: 71.09%] [G loss: 0.563254]\n",
      "epoch:6 step:5676 [D loss: 0.211898, acc.: 66.41%] [G loss: 0.565317]\n",
      "epoch:6 step:5677 [D loss: 0.218518, acc.: 67.97%] [G loss: 0.561540]\n",
      "epoch:6 step:5678 [D loss: 0.190598, acc.: 71.09%] [G loss: 0.559655]\n",
      "epoch:6 step:5679 [D loss: 0.204096, acc.: 69.53%] [G loss: 0.525898]\n",
      "epoch:6 step:5680 [D loss: 0.196486, acc.: 70.31%] [G loss: 0.550570]\n",
      "epoch:6 step:5681 [D loss: 0.209951, acc.: 64.06%] [G loss: 0.500560]\n",
      "epoch:6 step:5682 [D loss: 0.225223, acc.: 64.06%] [G loss: 0.504006]\n",
      "epoch:6 step:5683 [D loss: 0.231096, acc.: 65.62%] [G loss: 0.527081]\n",
      "epoch:6 step:5684 [D loss: 0.217605, acc.: 65.62%] [G loss: 0.512383]\n",
      "epoch:6 step:5685 [D loss: 0.196875, acc.: 70.31%] [G loss: 0.498599]\n",
      "epoch:6 step:5686 [D loss: 0.200754, acc.: 70.31%] [G loss: 0.545291]\n",
      "epoch:6 step:5687 [D loss: 0.236786, acc.: 59.38%] [G loss: 0.513475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5688 [D loss: 0.216701, acc.: 63.28%] [G loss: 0.515297]\n",
      "epoch:6 step:5689 [D loss: 0.232460, acc.: 61.72%] [G loss: 0.513443]\n",
      "epoch:6 step:5690 [D loss: 0.184995, acc.: 75.00%] [G loss: 0.556437]\n",
      "epoch:6 step:5691 [D loss: 0.193434, acc.: 74.22%] [G loss: 0.604407]\n",
      "epoch:6 step:5692 [D loss: 0.204027, acc.: 67.19%] [G loss: 0.546877]\n",
      "epoch:6 step:5693 [D loss: 0.222566, acc.: 60.94%] [G loss: 0.509783]\n",
      "epoch:6 step:5694 [D loss: 0.213741, acc.: 63.28%] [G loss: 0.519662]\n",
      "epoch:6 step:5695 [D loss: 0.210643, acc.: 67.97%] [G loss: 0.520252]\n",
      "epoch:6 step:5696 [D loss: 0.177898, acc.: 77.34%] [G loss: 0.507564]\n",
      "epoch:6 step:5697 [D loss: 0.218064, acc.: 68.75%] [G loss: 0.520061]\n",
      "epoch:6 step:5698 [D loss: 0.200706, acc.: 67.97%] [G loss: 0.550621]\n",
      "epoch:6 step:5699 [D loss: 0.174651, acc.: 72.66%] [G loss: 0.610296]\n",
      "epoch:6 step:5700 [D loss: 0.255627, acc.: 57.81%] [G loss: 0.502219]\n",
      "epoch:6 step:5701 [D loss: 0.228434, acc.: 64.84%] [G loss: 0.507385]\n",
      "epoch:6 step:5702 [D loss: 0.215738, acc.: 66.41%] [G loss: 0.507038]\n",
      "epoch:6 step:5703 [D loss: 0.223508, acc.: 62.50%] [G loss: 0.482111]\n",
      "epoch:6 step:5704 [D loss: 0.207213, acc.: 70.31%] [G loss: 0.481305]\n",
      "epoch:6 step:5705 [D loss: 0.189852, acc.: 70.31%] [G loss: 0.511168]\n",
      "epoch:6 step:5706 [D loss: 0.217199, acc.: 65.62%] [G loss: 0.541387]\n",
      "epoch:6 step:5707 [D loss: 0.215065, acc.: 66.41%] [G loss: 0.543555]\n",
      "epoch:6 step:5708 [D loss: 0.186705, acc.: 71.88%] [G loss: 0.528664]\n",
      "epoch:6 step:5709 [D loss: 0.190327, acc.: 75.00%] [G loss: 0.551196]\n",
      "epoch:6 step:5710 [D loss: 0.187604, acc.: 75.00%] [G loss: 0.529060]\n",
      "epoch:6 step:5711 [D loss: 0.201869, acc.: 67.19%] [G loss: 0.505734]\n",
      "epoch:6 step:5712 [D loss: 0.174886, acc.: 77.34%] [G loss: 0.553598]\n",
      "epoch:6 step:5713 [D loss: 0.232187, acc.: 63.28%] [G loss: 0.538901]\n",
      "epoch:6 step:5714 [D loss: 0.194656, acc.: 70.31%] [G loss: 0.498294]\n",
      "epoch:6 step:5715 [D loss: 0.188709, acc.: 72.66%] [G loss: 0.512535]\n",
      "epoch:6 step:5716 [D loss: 0.199144, acc.: 71.88%] [G loss: 0.578021]\n",
      "epoch:6 step:5717 [D loss: 0.203029, acc.: 67.97%] [G loss: 0.529331]\n",
      "epoch:6 step:5718 [D loss: 0.190925, acc.: 71.09%] [G loss: 0.479584]\n",
      "epoch:6 step:5719 [D loss: 0.203364, acc.: 71.09%] [G loss: 0.507935]\n",
      "epoch:6 step:5720 [D loss: 0.220236, acc.: 62.50%] [G loss: 0.537617]\n",
      "epoch:6 step:5721 [D loss: 0.205458, acc.: 70.31%] [G loss: 0.462570]\n",
      "epoch:6 step:5722 [D loss: 0.150313, acc.: 78.91%] [G loss: 0.543039]\n",
      "epoch:6 step:5723 [D loss: 0.199252, acc.: 66.41%] [G loss: 0.518249]\n",
      "epoch:6 step:5724 [D loss: 0.241562, acc.: 59.38%] [G loss: 0.499545]\n",
      "epoch:6 step:5725 [D loss: 0.174681, acc.: 75.00%] [G loss: 0.545622]\n",
      "epoch:6 step:5726 [D loss: 0.205695, acc.: 65.62%] [G loss: 0.539983]\n",
      "epoch:6 step:5727 [D loss: 0.225206, acc.: 64.84%] [G loss: 0.520290]\n",
      "epoch:6 step:5728 [D loss: 0.206806, acc.: 66.41%] [G loss: 0.486122]\n",
      "epoch:6 step:5729 [D loss: 0.199296, acc.: 69.53%] [G loss: 0.607649]\n",
      "epoch:6 step:5730 [D loss: 0.271824, acc.: 56.25%] [G loss: 0.553494]\n",
      "epoch:6 step:5731 [D loss: 0.242547, acc.: 60.16%] [G loss: 0.510946]\n",
      "epoch:6 step:5732 [D loss: 0.227759, acc.: 63.28%] [G loss: 0.529186]\n",
      "epoch:6 step:5733 [D loss: 0.189150, acc.: 73.44%] [G loss: 0.528508]\n",
      "epoch:6 step:5734 [D loss: 0.205287, acc.: 68.75%] [G loss: 0.525095]\n",
      "epoch:6 step:5735 [D loss: 0.212317, acc.: 67.19%] [G loss: 0.531620]\n",
      "epoch:6 step:5736 [D loss: 0.236890, acc.: 59.38%] [G loss: 0.550240]\n",
      "epoch:6 step:5737 [D loss: 0.220875, acc.: 65.62%] [G loss: 0.571572]\n",
      "epoch:6 step:5738 [D loss: 0.206897, acc.: 63.28%] [G loss: 0.607460]\n",
      "epoch:6 step:5739 [D loss: 0.213844, acc.: 66.41%] [G loss: 0.518990]\n",
      "epoch:6 step:5740 [D loss: 0.201475, acc.: 68.75%] [G loss: 0.526997]\n",
      "epoch:6 step:5741 [D loss: 0.152224, acc.: 80.47%] [G loss: 0.578483]\n",
      "epoch:6 step:5742 [D loss: 0.231510, acc.: 64.84%] [G loss: 0.525672]\n",
      "epoch:6 step:5743 [D loss: 0.204202, acc.: 73.44%] [G loss: 0.554328]\n",
      "epoch:6 step:5744 [D loss: 0.172563, acc.: 75.78%] [G loss: 0.573129]\n",
      "epoch:6 step:5745 [D loss: 0.191667, acc.: 72.66%] [G loss: 0.587944]\n",
      "epoch:6 step:5746 [D loss: 0.201829, acc.: 67.97%] [G loss: 0.522292]\n",
      "epoch:6 step:5747 [D loss: 0.204579, acc.: 64.06%] [G loss: 0.501270]\n",
      "epoch:6 step:5748 [D loss: 0.169451, acc.: 76.56%] [G loss: 0.547510]\n",
      "epoch:6 step:5749 [D loss: 0.222646, acc.: 62.50%] [G loss: 0.522548]\n",
      "epoch:6 step:5750 [D loss: 0.232206, acc.: 66.41%] [G loss: 0.497755]\n",
      "epoch:6 step:5751 [D loss: 0.203319, acc.: 70.31%] [G loss: 0.516748]\n",
      "epoch:6 step:5752 [D loss: 0.208848, acc.: 67.97%] [G loss: 0.482988]\n",
      "epoch:6 step:5753 [D loss: 0.205671, acc.: 69.53%] [G loss: 0.516885]\n",
      "epoch:6 step:5754 [D loss: 0.200731, acc.: 67.97%] [G loss: 0.528906]\n",
      "epoch:6 step:5755 [D loss: 0.228253, acc.: 60.94%] [G loss: 0.516459]\n",
      "epoch:6 step:5756 [D loss: 0.177177, acc.: 77.34%] [G loss: 0.511033]\n",
      "epoch:6 step:5757 [D loss: 0.187818, acc.: 72.66%] [G loss: 0.550823]\n",
      "epoch:6 step:5758 [D loss: 0.224254, acc.: 58.59%] [G loss: 0.507705]\n",
      "epoch:6 step:5759 [D loss: 0.239189, acc.: 58.59%] [G loss: 0.450929]\n",
      "epoch:6 step:5760 [D loss: 0.256346, acc.: 60.16%] [G loss: 0.476227]\n",
      "epoch:6 step:5761 [D loss: 0.207612, acc.: 64.06%] [G loss: 0.520259]\n",
      "epoch:6 step:5762 [D loss: 0.211607, acc.: 64.84%] [G loss: 0.523334]\n",
      "epoch:6 step:5763 [D loss: 0.235550, acc.: 63.28%] [G loss: 0.475530]\n",
      "epoch:6 step:5764 [D loss: 0.224360, acc.: 67.97%] [G loss: 0.486129]\n",
      "epoch:6 step:5765 [D loss: 0.225270, acc.: 60.94%] [G loss: 0.509100]\n",
      "epoch:6 step:5766 [D loss: 0.171276, acc.: 76.56%] [G loss: 0.570654]\n",
      "epoch:6 step:5767 [D loss: 0.216339, acc.: 67.19%] [G loss: 0.512612]\n",
      "epoch:6 step:5768 [D loss: 0.212643, acc.: 65.62%] [G loss: 0.515511]\n",
      "epoch:6 step:5769 [D loss: 0.241657, acc.: 63.28%] [G loss: 0.493686]\n",
      "epoch:6 step:5770 [D loss: 0.219697, acc.: 66.41%] [G loss: 0.484055]\n",
      "epoch:6 step:5771 [D loss: 0.185684, acc.: 74.22%] [G loss: 0.571748]\n",
      "epoch:6 step:5772 [D loss: 0.244663, acc.: 65.62%] [G loss: 0.513043]\n",
      "epoch:6 step:5773 [D loss: 0.181424, acc.: 77.34%] [G loss: 0.528465]\n",
      "epoch:6 step:5774 [D loss: 0.195313, acc.: 75.00%] [G loss: 0.527770]\n",
      "epoch:6 step:5775 [D loss: 0.224971, acc.: 64.06%] [G loss: 0.549299]\n",
      "epoch:6 step:5776 [D loss: 0.189743, acc.: 71.09%] [G loss: 0.568249]\n",
      "epoch:6 step:5777 [D loss: 0.185749, acc.: 75.00%] [G loss: 0.521034]\n",
      "epoch:6 step:5778 [D loss: 0.187380, acc.: 72.66%] [G loss: 0.568049]\n",
      "epoch:6 step:5779 [D loss: 0.245281, acc.: 57.81%] [G loss: 0.510230]\n",
      "epoch:6 step:5780 [D loss: 0.228389, acc.: 62.50%] [G loss: 0.512990]\n",
      "epoch:6 step:5781 [D loss: 0.214873, acc.: 67.97%] [G loss: 0.482860]\n",
      "epoch:6 step:5782 [D loss: 0.234408, acc.: 60.16%] [G loss: 0.477817]\n",
      "epoch:6 step:5783 [D loss: 0.183581, acc.: 74.22%] [G loss: 0.577793]\n",
      "epoch:6 step:5784 [D loss: 0.173041, acc.: 75.78%] [G loss: 0.565292]\n",
      "epoch:6 step:5785 [D loss: 0.237283, acc.: 65.62%] [G loss: 0.534192]\n",
      "epoch:6 step:5786 [D loss: 0.191604, acc.: 66.41%] [G loss: 0.573446]\n",
      "epoch:6 step:5787 [D loss: 0.184518, acc.: 71.09%] [G loss: 0.513910]\n",
      "epoch:6 step:5788 [D loss: 0.194053, acc.: 73.44%] [G loss: 0.518328]\n",
      "epoch:6 step:5789 [D loss: 0.229467, acc.: 60.16%] [G loss: 0.455651]\n",
      "epoch:6 step:5790 [D loss: 0.193029, acc.: 71.09%] [G loss: 0.524150]\n",
      "epoch:6 step:5791 [D loss: 0.206088, acc.: 67.19%] [G loss: 0.539619]\n",
      "epoch:6 step:5792 [D loss: 0.225554, acc.: 65.62%] [G loss: 0.534690]\n",
      "epoch:6 step:5793 [D loss: 0.180011, acc.: 71.09%] [G loss: 0.555633]\n",
      "epoch:6 step:5794 [D loss: 0.199190, acc.: 67.19%] [G loss: 0.503380]\n",
      "epoch:6 step:5795 [D loss: 0.221499, acc.: 64.84%] [G loss: 0.478618]\n",
      "epoch:6 step:5796 [D loss: 0.216494, acc.: 67.97%] [G loss: 0.500959]\n",
      "epoch:6 step:5797 [D loss: 0.202856, acc.: 75.00%] [G loss: 0.484495]\n",
      "epoch:6 step:5798 [D loss: 0.188856, acc.: 70.31%] [G loss: 0.566610]\n",
      "epoch:6 step:5799 [D loss: 0.230275, acc.: 67.19%] [G loss: 0.489760]\n",
      "epoch:6 step:5800 [D loss: 0.189462, acc.: 74.22%] [G loss: 0.502036]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.091103\n",
      "FID: 17.755087\n",
      "0 = 12.354521187329278\n",
      "1 = 0.05971222530155669\n",
      "2 = 0.9489499926567078\n",
      "3 = 0.9248999953269958\n",
      "4 = 0.9729999899864197\n",
      "5 = 0.9716356992721558\n",
      "6 = 0.9248999953269958\n",
      "7 = 7.400517283511165\n",
      "8 = 0.09142080337869096\n",
      "9 = 0.7722499966621399\n",
      "10 = 0.7699000239372253\n",
      "11 = 0.7746000289916992\n",
      "12 = 0.7735356092453003\n",
      "13 = 0.7699000239372253\n",
      "14 = 7.091165065765381\n",
      "15 = 9.497990608215332\n",
      "16 = 0.13255426287651062\n",
      "17 = 7.0911030769348145\n",
      "18 = 17.75508689880371\n",
      "epoch:6 step:5801 [D loss: 0.243118, acc.: 61.72%] [G loss: 0.465524]\n",
      "epoch:6 step:5802 [D loss: 0.235808, acc.: 64.06%] [G loss: 0.511540]\n",
      "epoch:6 step:5803 [D loss: 0.216015, acc.: 67.19%] [G loss: 0.482278]\n",
      "epoch:6 step:5804 [D loss: 0.236205, acc.: 62.50%] [G loss: 0.533141]\n",
      "epoch:6 step:5805 [D loss: 0.218987, acc.: 68.75%] [G loss: 0.490206]\n",
      "epoch:6 step:5806 [D loss: 0.238909, acc.: 60.94%] [G loss: 0.485425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5807 [D loss: 0.212327, acc.: 67.97%] [G loss: 0.523056]\n",
      "epoch:6 step:5808 [D loss: 0.208177, acc.: 71.88%] [G loss: 0.508298]\n",
      "epoch:6 step:5809 [D loss: 0.193237, acc.: 68.75%] [G loss: 0.497887]\n",
      "epoch:6 step:5810 [D loss: 0.209094, acc.: 68.75%] [G loss: 0.490017]\n",
      "epoch:6 step:5811 [D loss: 0.233985, acc.: 60.94%] [G loss: 0.470724]\n",
      "epoch:6 step:5812 [D loss: 0.196143, acc.: 68.75%] [G loss: 0.514207]\n",
      "epoch:6 step:5813 [D loss: 0.175673, acc.: 78.12%] [G loss: 0.549861]\n",
      "epoch:6 step:5814 [D loss: 0.203424, acc.: 65.62%] [G loss: 0.519694]\n",
      "epoch:6 step:5815 [D loss: 0.181712, acc.: 72.66%] [G loss: 0.568510]\n",
      "epoch:6 step:5816 [D loss: 0.216639, acc.: 65.62%] [G loss: 0.578083]\n",
      "epoch:6 step:5817 [D loss: 0.199280, acc.: 64.84%] [G loss: 0.546456]\n",
      "epoch:6 step:5818 [D loss: 0.225204, acc.: 66.41%] [G loss: 0.478621]\n",
      "epoch:6 step:5819 [D loss: 0.193497, acc.: 71.88%] [G loss: 0.499876]\n",
      "epoch:6 step:5820 [D loss: 0.176538, acc.: 71.09%] [G loss: 0.601629]\n",
      "epoch:6 step:5821 [D loss: 0.196501, acc.: 71.88%] [G loss: 0.566583]\n",
      "epoch:6 step:5822 [D loss: 0.231479, acc.: 65.62%] [G loss: 0.597585]\n",
      "epoch:6 step:5823 [D loss: 0.213663, acc.: 65.62%] [G loss: 0.511712]\n",
      "epoch:6 step:5824 [D loss: 0.218040, acc.: 71.09%] [G loss: 0.557128]\n",
      "epoch:6 step:5825 [D loss: 0.272313, acc.: 53.12%] [G loss: 0.494839]\n",
      "epoch:6 step:5826 [D loss: 0.197458, acc.: 69.53%] [G loss: 0.557983]\n",
      "epoch:6 step:5827 [D loss: 0.185662, acc.: 71.88%] [G loss: 0.549486]\n",
      "epoch:6 step:5828 [D loss: 0.183753, acc.: 68.75%] [G loss: 0.538244]\n",
      "epoch:6 step:5829 [D loss: 0.171869, acc.: 71.09%] [G loss: 0.570182]\n",
      "epoch:6 step:5830 [D loss: 0.175332, acc.: 70.31%] [G loss: 0.561528]\n",
      "epoch:6 step:5831 [D loss: 0.168089, acc.: 78.91%] [G loss: 0.598291]\n",
      "epoch:6 step:5832 [D loss: 0.255583, acc.: 55.47%] [G loss: 0.480381]\n",
      "epoch:6 step:5833 [D loss: 0.223451, acc.: 60.94%] [G loss: 0.501345]\n",
      "epoch:6 step:5834 [D loss: 0.237028, acc.: 62.50%] [G loss: 0.496726]\n",
      "epoch:6 step:5835 [D loss: 0.209136, acc.: 67.19%] [G loss: 0.551288]\n",
      "epoch:6 step:5836 [D loss: 0.250008, acc.: 60.94%] [G loss: 0.465675]\n",
      "epoch:6 step:5837 [D loss: 0.263188, acc.: 55.47%] [G loss: 0.454465]\n",
      "epoch:6 step:5838 [D loss: 0.208843, acc.: 67.19%] [G loss: 0.528248]\n",
      "epoch:6 step:5839 [D loss: 0.202823, acc.: 71.09%] [G loss: 0.530706]\n",
      "epoch:6 step:5840 [D loss: 0.183848, acc.: 72.66%] [G loss: 0.557263]\n",
      "epoch:6 step:5841 [D loss: 0.187481, acc.: 71.88%] [G loss: 0.550772]\n",
      "epoch:6 step:5842 [D loss: 0.268867, acc.: 57.03%] [G loss: 0.535135]\n",
      "epoch:6 step:5843 [D loss: 0.180016, acc.: 71.09%] [G loss: 0.574036]\n",
      "epoch:6 step:5844 [D loss: 0.189676, acc.: 71.88%] [G loss: 0.590467]\n",
      "epoch:6 step:5845 [D loss: 0.197569, acc.: 70.31%] [G loss: 0.620064]\n",
      "epoch:6 step:5846 [D loss: 0.242016, acc.: 60.94%] [G loss: 0.489363]\n",
      "epoch:6 step:5847 [D loss: 0.254537, acc.: 61.72%] [G loss: 0.494499]\n",
      "epoch:6 step:5848 [D loss: 0.237177, acc.: 59.38%] [G loss: 0.469996]\n",
      "epoch:6 step:5849 [D loss: 0.221939, acc.: 66.41%] [G loss: 0.437458]\n",
      "epoch:6 step:5850 [D loss: 0.220362, acc.: 60.16%] [G loss: 0.514956]\n",
      "epoch:6 step:5851 [D loss: 0.217746, acc.: 67.97%] [G loss: 0.508278]\n",
      "epoch:6 step:5852 [D loss: 0.172107, acc.: 77.34%] [G loss: 0.553577]\n",
      "epoch:6 step:5853 [D loss: 0.177581, acc.: 72.66%] [G loss: 0.593977]\n",
      "epoch:6 step:5854 [D loss: 0.148012, acc.: 81.25%] [G loss: 0.620272]\n",
      "epoch:6 step:5855 [D loss: 0.257397, acc.: 58.59%] [G loss: 0.487988]\n",
      "epoch:6 step:5856 [D loss: 0.192953, acc.: 67.97%] [G loss: 0.481668]\n",
      "epoch:6 step:5857 [D loss: 0.211413, acc.: 69.53%] [G loss: 0.509018]\n",
      "epoch:6 step:5858 [D loss: 0.174655, acc.: 80.47%] [G loss: 0.516372]\n",
      "epoch:6 step:5859 [D loss: 0.231099, acc.: 61.72%] [G loss: 0.523900]\n",
      "epoch:6 step:5860 [D loss: 0.205337, acc.: 69.53%] [G loss: 0.510544]\n",
      "epoch:6 step:5861 [D loss: 0.223901, acc.: 67.97%] [G loss: 0.478253]\n",
      "epoch:6 step:5862 [D loss: 0.190859, acc.: 71.88%] [G loss: 0.600006]\n",
      "epoch:6 step:5863 [D loss: 0.218936, acc.: 64.06%] [G loss: 0.542582]\n",
      "epoch:6 step:5864 [D loss: 0.187166, acc.: 67.19%] [G loss: 0.584847]\n",
      "epoch:6 step:5865 [D loss: 0.198951, acc.: 71.09%] [G loss: 0.529578]\n",
      "epoch:6 step:5866 [D loss: 0.215001, acc.: 64.06%] [G loss: 0.517528]\n",
      "epoch:6 step:5867 [D loss: 0.189232, acc.: 68.75%] [G loss: 0.509391]\n",
      "epoch:6 step:5868 [D loss: 0.200352, acc.: 64.06%] [G loss: 0.555212]\n",
      "epoch:6 step:5869 [D loss: 0.202356, acc.: 68.75%] [G loss: 0.527613]\n",
      "epoch:6 step:5870 [D loss: 0.218365, acc.: 67.97%] [G loss: 0.537956]\n",
      "epoch:6 step:5871 [D loss: 0.227801, acc.: 63.28%] [G loss: 0.511068]\n",
      "epoch:6 step:5872 [D loss: 0.230180, acc.: 64.06%] [G loss: 0.443016]\n",
      "epoch:6 step:5873 [D loss: 0.238901, acc.: 64.06%] [G loss: 0.492582]\n",
      "epoch:6 step:5874 [D loss: 0.245611, acc.: 59.38%] [G loss: 0.531725]\n",
      "epoch:6 step:5875 [D loss: 0.182495, acc.: 73.44%] [G loss: 0.543408]\n",
      "epoch:6 step:5876 [D loss: 0.197921, acc.: 66.41%] [G loss: 0.484869]\n",
      "epoch:6 step:5877 [D loss: 0.175604, acc.: 75.78%] [G loss: 0.506360]\n",
      "epoch:6 step:5878 [D loss: 0.197027, acc.: 68.75%] [G loss: 0.499797]\n",
      "epoch:6 step:5879 [D loss: 0.229361, acc.: 64.84%] [G loss: 0.467487]\n",
      "epoch:6 step:5880 [D loss: 0.201805, acc.: 71.09%] [G loss: 0.540345]\n",
      "epoch:6 step:5881 [D loss: 0.206818, acc.: 67.97%] [G loss: 0.563220]\n",
      "epoch:6 step:5882 [D loss: 0.216591, acc.: 64.06%] [G loss: 0.549852]\n",
      "epoch:6 step:5883 [D loss: 0.197852, acc.: 71.88%] [G loss: 0.515829]\n",
      "epoch:6 step:5884 [D loss: 0.199670, acc.: 67.97%] [G loss: 0.535543]\n",
      "epoch:6 step:5885 [D loss: 0.219316, acc.: 65.62%] [G loss: 0.546958]\n",
      "epoch:6 step:5886 [D loss: 0.161787, acc.: 76.56%] [G loss: 0.571859]\n",
      "epoch:6 step:5887 [D loss: 0.283088, acc.: 58.59%] [G loss: 0.503406]\n",
      "epoch:6 step:5888 [D loss: 0.226124, acc.: 64.06%] [G loss: 0.490109]\n",
      "epoch:6 step:5889 [D loss: 0.233883, acc.: 59.38%] [G loss: 0.500159]\n",
      "epoch:6 step:5890 [D loss: 0.233802, acc.: 60.16%] [G loss: 0.522768]\n",
      "epoch:6 step:5891 [D loss: 0.196143, acc.: 72.66%] [G loss: 0.504638]\n",
      "epoch:6 step:5892 [D loss: 0.199580, acc.: 71.09%] [G loss: 0.508148]\n",
      "epoch:6 step:5893 [D loss: 0.206539, acc.: 67.19%] [G loss: 0.502638]\n",
      "epoch:6 step:5894 [D loss: 0.210833, acc.: 70.31%] [G loss: 0.524461]\n",
      "epoch:6 step:5895 [D loss: 0.223843, acc.: 64.06%] [G loss: 0.500589]\n",
      "epoch:6 step:5896 [D loss: 0.174884, acc.: 75.78%] [G loss: 0.505585]\n",
      "epoch:6 step:5897 [D loss: 0.217770, acc.: 67.97%] [G loss: 0.473257]\n",
      "epoch:6 step:5898 [D loss: 0.198057, acc.: 73.44%] [G loss: 0.523626]\n",
      "epoch:6 step:5899 [D loss: 0.243561, acc.: 57.03%] [G loss: 0.470727]\n",
      "epoch:6 step:5900 [D loss: 0.236064, acc.: 62.50%] [G loss: 0.493548]\n",
      "epoch:6 step:5901 [D loss: 0.208958, acc.: 68.75%] [G loss: 0.503603]\n",
      "epoch:6 step:5902 [D loss: 0.207024, acc.: 69.53%] [G loss: 0.528249]\n",
      "epoch:6 step:5903 [D loss: 0.243507, acc.: 63.28%] [G loss: 0.506860]\n",
      "epoch:6 step:5904 [D loss: 0.202715, acc.: 65.62%] [G loss: 0.531682]\n",
      "epoch:6 step:5905 [D loss: 0.182295, acc.: 70.31%] [G loss: 0.547472]\n",
      "epoch:6 step:5906 [D loss: 0.190392, acc.: 71.88%] [G loss: 0.526077]\n",
      "epoch:6 step:5907 [D loss: 0.189669, acc.: 69.53%] [G loss: 0.520070]\n",
      "epoch:6 step:5908 [D loss: 0.176792, acc.: 71.88%] [G loss: 0.538720]\n",
      "epoch:6 step:5909 [D loss: 0.244817, acc.: 61.72%] [G loss: 0.513131]\n",
      "epoch:6 step:5910 [D loss: 0.207191, acc.: 68.75%] [G loss: 0.524048]\n",
      "epoch:6 step:5911 [D loss: 0.160748, acc.: 82.03%] [G loss: 0.494280]\n",
      "epoch:6 step:5912 [D loss: 0.219239, acc.: 62.50%] [G loss: 0.451722]\n",
      "epoch:6 step:5913 [D loss: 0.252108, acc.: 57.81%] [G loss: 0.545025]\n",
      "epoch:6 step:5914 [D loss: 0.197665, acc.: 68.75%] [G loss: 0.529563]\n",
      "epoch:6 step:5915 [D loss: 0.199689, acc.: 71.88%] [G loss: 0.502651]\n",
      "epoch:6 step:5916 [D loss: 0.225726, acc.: 60.94%] [G loss: 0.494299]\n",
      "epoch:6 step:5917 [D loss: 0.210242, acc.: 64.06%] [G loss: 0.480552]\n",
      "epoch:6 step:5918 [D loss: 0.203418, acc.: 67.19%] [G loss: 0.529597]\n",
      "epoch:6 step:5919 [D loss: 0.201096, acc.: 69.53%] [G loss: 0.521841]\n",
      "epoch:6 step:5920 [D loss: 0.173000, acc.: 78.12%] [G loss: 0.556194]\n",
      "epoch:6 step:5921 [D loss: 0.179171, acc.: 72.66%] [G loss: 0.539947]\n",
      "epoch:6 step:5922 [D loss: 0.213020, acc.: 71.88%] [G loss: 0.529650]\n",
      "epoch:6 step:5923 [D loss: 0.238424, acc.: 63.28%] [G loss: 0.538303]\n",
      "epoch:6 step:5924 [D loss: 0.190986, acc.: 67.97%] [G loss: 0.512437]\n",
      "epoch:6 step:5925 [D loss: 0.205272, acc.: 67.19%] [G loss: 0.517290]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5926 [D loss: 0.172155, acc.: 71.88%] [G loss: 0.527237]\n",
      "epoch:6 step:5927 [D loss: 0.204795, acc.: 65.62%] [G loss: 0.526714]\n",
      "epoch:6 step:5928 [D loss: 0.198203, acc.: 70.31%] [G loss: 0.531125]\n",
      "epoch:6 step:5929 [D loss: 0.216548, acc.: 71.88%] [G loss: 0.548047]\n",
      "epoch:6 step:5930 [D loss: 0.213598, acc.: 73.44%] [G loss: 0.518749]\n",
      "epoch:6 step:5931 [D loss: 0.196888, acc.: 70.31%] [G loss: 0.522414]\n",
      "epoch:6 step:5932 [D loss: 0.214800, acc.: 65.62%] [G loss: 0.560286]\n",
      "epoch:6 step:5933 [D loss: 0.174587, acc.: 74.22%] [G loss: 0.563488]\n",
      "epoch:6 step:5934 [D loss: 0.167474, acc.: 79.69%] [G loss: 0.594800]\n",
      "epoch:6 step:5935 [D loss: 0.160076, acc.: 78.91%] [G loss: 0.561498]\n",
      "epoch:6 step:5936 [D loss: 0.178746, acc.: 75.00%] [G loss: 0.605608]\n",
      "epoch:6 step:5937 [D loss: 0.182876, acc.: 75.00%] [G loss: 0.575019]\n",
      "epoch:6 step:5938 [D loss: 0.279022, acc.: 54.69%] [G loss: 0.501104]\n",
      "epoch:6 step:5939 [D loss: 0.198808, acc.: 67.97%] [G loss: 0.504291]\n",
      "epoch:6 step:5940 [D loss: 0.181740, acc.: 70.31%] [G loss: 0.502080]\n",
      "epoch:6 step:5941 [D loss: 0.202459, acc.: 67.19%] [G loss: 0.486854]\n",
      "epoch:6 step:5942 [D loss: 0.206403, acc.: 67.19%] [G loss: 0.509716]\n",
      "epoch:6 step:5943 [D loss: 0.150577, acc.: 81.25%] [G loss: 0.568331]\n",
      "epoch:6 step:5944 [D loss: 0.211883, acc.: 67.97%] [G loss: 0.499730]\n",
      "epoch:6 step:5945 [D loss: 0.217670, acc.: 66.41%] [G loss: 0.500491]\n",
      "epoch:6 step:5946 [D loss: 0.231343, acc.: 58.59%] [G loss: 0.485952]\n",
      "epoch:6 step:5947 [D loss: 0.208523, acc.: 66.41%] [G loss: 0.475235]\n",
      "epoch:6 step:5948 [D loss: 0.222497, acc.: 65.62%] [G loss: 0.545936]\n",
      "epoch:6 step:5949 [D loss: 0.220083, acc.: 64.84%] [G loss: 0.516433]\n",
      "epoch:6 step:5950 [D loss: 0.177302, acc.: 75.78%] [G loss: 0.541137]\n",
      "epoch:6 step:5951 [D loss: 0.190182, acc.: 68.75%] [G loss: 0.527575]\n",
      "epoch:6 step:5952 [D loss: 0.203087, acc.: 67.97%] [G loss: 0.504780]\n",
      "epoch:6 step:5953 [D loss: 0.201598, acc.: 63.28%] [G loss: 0.491220]\n",
      "epoch:6 step:5954 [D loss: 0.197748, acc.: 69.53%] [G loss: 0.488043]\n",
      "epoch:6 step:5955 [D loss: 0.199375, acc.: 67.97%] [G loss: 0.553231]\n",
      "epoch:6 step:5956 [D loss: 0.212577, acc.: 67.19%] [G loss: 0.549909]\n",
      "epoch:6 step:5957 [D loss: 0.183311, acc.: 69.53%] [G loss: 0.550726]\n",
      "epoch:6 step:5958 [D loss: 0.230837, acc.: 64.06%] [G loss: 0.505942]\n",
      "epoch:6 step:5959 [D loss: 0.182868, acc.: 71.88%] [G loss: 0.569360]\n",
      "epoch:6 step:5960 [D loss: 0.208791, acc.: 66.41%] [G loss: 0.492851]\n",
      "epoch:6 step:5961 [D loss: 0.195781, acc.: 69.53%] [G loss: 0.586063]\n",
      "epoch:6 step:5962 [D loss: 0.208114, acc.: 73.44%] [G loss: 0.521278]\n",
      "epoch:6 step:5963 [D loss: 0.229149, acc.: 64.84%] [G loss: 0.551906]\n",
      "epoch:6 step:5964 [D loss: 0.226657, acc.: 64.06%] [G loss: 0.550545]\n",
      "epoch:6 step:5965 [D loss: 0.174941, acc.: 74.22%] [G loss: 0.560187]\n",
      "epoch:6 step:5966 [D loss: 0.182742, acc.: 75.00%] [G loss: 0.552889]\n",
      "epoch:6 step:5967 [D loss: 0.184064, acc.: 74.22%] [G loss: 0.537832]\n",
      "epoch:6 step:5968 [D loss: 0.197568, acc.: 67.97%] [G loss: 0.543643]\n",
      "epoch:6 step:5969 [D loss: 0.151404, acc.: 79.69%] [G loss: 0.608982]\n",
      "epoch:6 step:5970 [D loss: 0.256810, acc.: 58.59%] [G loss: 0.555224]\n",
      "epoch:6 step:5971 [D loss: 0.256422, acc.: 53.12%] [G loss: 0.460869]\n",
      "epoch:6 step:5972 [D loss: 0.197705, acc.: 67.19%] [G loss: 0.486017]\n",
      "epoch:6 step:5973 [D loss: 0.217189, acc.: 64.84%] [G loss: 0.515162]\n",
      "epoch:6 step:5974 [D loss: 0.254267, acc.: 58.59%] [G loss: 0.518638]\n",
      "epoch:6 step:5975 [D loss: 0.197572, acc.: 67.19%] [G loss: 0.529215]\n",
      "epoch:6 step:5976 [D loss: 0.173265, acc.: 75.78%] [G loss: 0.554729]\n",
      "epoch:6 step:5977 [D loss: 0.215913, acc.: 64.84%] [G loss: 0.551037]\n",
      "epoch:6 step:5978 [D loss: 0.223636, acc.: 60.94%] [G loss: 0.536517]\n",
      "epoch:6 step:5979 [D loss: 0.201963, acc.: 70.31%] [G loss: 0.525739]\n",
      "epoch:6 step:5980 [D loss: 0.169358, acc.: 75.00%] [G loss: 0.574872]\n",
      "epoch:6 step:5981 [D loss: 0.166469, acc.: 81.25%] [G loss: 0.555853]\n",
      "epoch:6 step:5982 [D loss: 0.206294, acc.: 69.53%] [G loss: 0.500071]\n",
      "epoch:6 step:5983 [D loss: 0.220348, acc.: 68.75%] [G loss: 0.552805]\n",
      "epoch:6 step:5984 [D loss: 0.234453, acc.: 60.16%] [G loss: 0.487042]\n",
      "epoch:6 step:5985 [D loss: 0.218033, acc.: 69.53%] [G loss: 0.514970]\n",
      "epoch:6 step:5986 [D loss: 0.189726, acc.: 68.75%] [G loss: 0.524928]\n",
      "epoch:6 step:5987 [D loss: 0.212625, acc.: 71.88%] [G loss: 0.538122]\n",
      "epoch:6 step:5988 [D loss: 0.202998, acc.: 72.66%] [G loss: 0.549063]\n",
      "epoch:6 step:5989 [D loss: 0.202008, acc.: 67.97%] [G loss: 0.552020]\n",
      "epoch:6 step:5990 [D loss: 0.212639, acc.: 65.62%] [G loss: 0.484859]\n",
      "epoch:6 step:5991 [D loss: 0.222817, acc.: 65.62%] [G loss: 0.501897]\n",
      "epoch:6 step:5992 [D loss: 0.198279, acc.: 70.31%] [G loss: 0.556781]\n",
      "epoch:6 step:5993 [D loss: 0.175353, acc.: 77.34%] [G loss: 0.617520]\n",
      "epoch:6 step:5994 [D loss: 0.217093, acc.: 68.75%] [G loss: 0.500808]\n",
      "epoch:6 step:5995 [D loss: 0.273747, acc.: 55.47%] [G loss: 0.439168]\n",
      "epoch:6 step:5996 [D loss: 0.172346, acc.: 75.78%] [G loss: 0.561371]\n",
      "epoch:6 step:5997 [D loss: 0.233806, acc.: 61.72%] [G loss: 0.516726]\n",
      "epoch:6 step:5998 [D loss: 0.266668, acc.: 57.81%] [G loss: 0.477320]\n",
      "epoch:6 step:5999 [D loss: 0.250708, acc.: 58.59%] [G loss: 0.455034]\n",
      "epoch:6 step:6000 [D loss: 0.190776, acc.: 69.53%] [G loss: 0.480958]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.972085\n",
      "FID: 23.129129\n",
      "0 = 12.285586864185301\n",
      "1 = 0.056427653893293905\n",
      "2 = 0.9401999711990356\n",
      "3 = 0.9157999753952026\n",
      "4 = 0.9646000266075134\n",
      "5 = 0.9627838730812073\n",
      "6 = 0.9157999753952026\n",
      "7 = 7.71284650519494\n",
      "8 = 0.10666179516714881\n",
      "9 = 0.7687000036239624\n",
      "10 = 0.7623999714851379\n",
      "11 = 0.7749999761581421\n",
      "12 = 0.7721288204193115\n",
      "13 = 0.7623999714851379\n",
      "14 = 6.972140789031982\n",
      "15 = 9.377038955688477\n",
      "16 = 0.16602838039398193\n",
      "17 = 6.9720845222473145\n",
      "18 = 23.12912940979004\n",
      "epoch:6 step:6001 [D loss: 0.214389, acc.: 64.06%] [G loss: 0.484922]\n",
      "epoch:6 step:6002 [D loss: 0.234072, acc.: 58.59%] [G loss: 0.457973]\n",
      "epoch:6 step:6003 [D loss: 0.174534, acc.: 74.22%] [G loss: 0.558487]\n",
      "epoch:6 step:6004 [D loss: 0.216305, acc.: 64.06%] [G loss: 0.525837]\n",
      "epoch:6 step:6005 [D loss: 0.206600, acc.: 62.50%] [G loss: 0.512851]\n",
      "epoch:6 step:6006 [D loss: 0.205409, acc.: 63.28%] [G loss: 0.507253]\n",
      "epoch:6 step:6007 [D loss: 0.197630, acc.: 71.09%] [G loss: 0.512975]\n",
      "epoch:6 step:6008 [D loss: 0.216254, acc.: 65.62%] [G loss: 0.488954]\n",
      "epoch:6 step:6009 [D loss: 0.212330, acc.: 68.75%] [G loss: 0.527108]\n",
      "epoch:6 step:6010 [D loss: 0.229738, acc.: 67.19%] [G loss: 0.526007]\n",
      "epoch:6 step:6011 [D loss: 0.180242, acc.: 71.88%] [G loss: 0.543332]\n",
      "epoch:6 step:6012 [D loss: 0.221864, acc.: 66.41%] [G loss: 0.501958]\n",
      "epoch:6 step:6013 [D loss: 0.189012, acc.: 71.09%] [G loss: 0.541593]\n",
      "epoch:6 step:6014 [D loss: 0.188089, acc.: 75.00%] [G loss: 0.534347]\n",
      "epoch:6 step:6015 [D loss: 0.238415, acc.: 60.94%] [G loss: 0.456421]\n",
      "epoch:6 step:6016 [D loss: 0.221941, acc.: 64.84%] [G loss: 0.514017]\n",
      "epoch:6 step:6017 [D loss: 0.196228, acc.: 69.53%] [G loss: 0.536647]\n",
      "epoch:6 step:6018 [D loss: 0.221455, acc.: 60.94%] [G loss: 0.536253]\n",
      "epoch:6 step:6019 [D loss: 0.220309, acc.: 64.84%] [G loss: 0.492893]\n",
      "epoch:6 step:6020 [D loss: 0.172735, acc.: 75.00%] [G loss: 0.579179]\n",
      "epoch:6 step:6021 [D loss: 0.198375, acc.: 73.44%] [G loss: 0.545640]\n",
      "epoch:6 step:6022 [D loss: 0.220425, acc.: 66.41%] [G loss: 0.506737]\n",
      "epoch:6 step:6023 [D loss: 0.216263, acc.: 64.06%] [G loss: 0.474297]\n",
      "epoch:6 step:6024 [D loss: 0.202058, acc.: 74.22%] [G loss: 0.551569]\n",
      "epoch:6 step:6025 [D loss: 0.216793, acc.: 62.50%] [G loss: 0.555871]\n",
      "epoch:6 step:6026 [D loss: 0.257908, acc.: 58.59%] [G loss: 0.479832]\n",
      "epoch:6 step:6027 [D loss: 0.174431, acc.: 73.44%] [G loss: 0.533404]\n",
      "epoch:6 step:6028 [D loss: 0.193240, acc.: 72.66%] [G loss: 0.595238]\n",
      "epoch:6 step:6029 [D loss: 0.255979, acc.: 65.62%] [G loss: 0.537466]\n",
      "epoch:6 step:6030 [D loss: 0.224672, acc.: 62.50%] [G loss: 0.523985]\n",
      "epoch:6 step:6031 [D loss: 0.223827, acc.: 63.28%] [G loss: 0.479206]\n",
      "epoch:6 step:6032 [D loss: 0.251442, acc.: 61.72%] [G loss: 0.475184]\n",
      "epoch:6 step:6033 [D loss: 0.196026, acc.: 67.97%] [G loss: 0.554321]\n",
      "epoch:6 step:6034 [D loss: 0.238475, acc.: 60.94%] [G loss: 0.461802]\n",
      "epoch:6 step:6035 [D loss: 0.225248, acc.: 67.97%] [G loss: 0.460543]\n",
      "epoch:6 step:6036 [D loss: 0.200182, acc.: 67.97%] [G loss: 0.521408]\n",
      "epoch:6 step:6037 [D loss: 0.201141, acc.: 69.53%] [G loss: 0.504764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6038 [D loss: 0.213628, acc.: 65.62%] [G loss: 0.531137]\n",
      "epoch:6 step:6039 [D loss: 0.236383, acc.: 60.94%] [G loss: 0.545959]\n",
      "epoch:6 step:6040 [D loss: 0.255272, acc.: 52.34%] [G loss: 0.496472]\n",
      "epoch:6 step:6041 [D loss: 0.212228, acc.: 66.41%] [G loss: 0.539577]\n",
      "epoch:6 step:6042 [D loss: 0.226763, acc.: 58.59%] [G loss: 0.525188]\n",
      "epoch:6 step:6043 [D loss: 0.255346, acc.: 60.16%] [G loss: 0.513981]\n",
      "epoch:6 step:6044 [D loss: 0.222173, acc.: 59.38%] [G loss: 0.502551]\n",
      "epoch:6 step:6045 [D loss: 0.222106, acc.: 62.50%] [G loss: 0.486203]\n",
      "epoch:6 step:6046 [D loss: 0.197195, acc.: 67.97%] [G loss: 0.519316]\n",
      "epoch:6 step:6047 [D loss: 0.199640, acc.: 70.31%] [G loss: 0.508765]\n",
      "epoch:6 step:6048 [D loss: 0.175477, acc.: 76.56%] [G loss: 0.556716]\n",
      "epoch:6 step:6049 [D loss: 0.183860, acc.: 70.31%] [G loss: 0.566479]\n",
      "epoch:6 step:6050 [D loss: 0.182622, acc.: 71.88%] [G loss: 0.565872]\n",
      "epoch:6 step:6051 [D loss: 0.177152, acc.: 72.66%] [G loss: 0.565841]\n",
      "epoch:6 step:6052 [D loss: 0.205255, acc.: 69.53%] [G loss: 0.573910]\n",
      "epoch:6 step:6053 [D loss: 0.222609, acc.: 65.62%] [G loss: 0.552434]\n",
      "epoch:6 step:6054 [D loss: 0.206795, acc.: 64.84%] [G loss: 0.518351]\n",
      "epoch:6 step:6055 [D loss: 0.244688, acc.: 61.72%] [G loss: 0.455238]\n",
      "epoch:6 step:6056 [D loss: 0.205817, acc.: 67.97%] [G loss: 0.469829]\n",
      "epoch:6 step:6057 [D loss: 0.221627, acc.: 68.75%] [G loss: 0.525003]\n",
      "epoch:6 step:6058 [D loss: 0.185740, acc.: 75.00%] [G loss: 0.542098]\n",
      "epoch:6 step:6059 [D loss: 0.277660, acc.: 55.47%] [G loss: 0.489334]\n",
      "epoch:6 step:6060 [D loss: 0.229223, acc.: 61.72%] [G loss: 0.448519]\n",
      "epoch:6 step:6061 [D loss: 0.194734, acc.: 67.97%] [G loss: 0.543013]\n",
      "epoch:6 step:6062 [D loss: 0.207614, acc.: 66.41%] [G loss: 0.518880]\n",
      "epoch:6 step:6063 [D loss: 0.229790, acc.: 67.19%] [G loss: 0.468863]\n",
      "epoch:6 step:6064 [D loss: 0.208088, acc.: 67.97%] [G loss: 0.534034]\n",
      "epoch:6 step:6065 [D loss: 0.232483, acc.: 66.41%] [G loss: 0.540254]\n",
      "epoch:6 step:6066 [D loss: 0.185299, acc.: 73.44%] [G loss: 0.513590]\n",
      "epoch:6 step:6067 [D loss: 0.193957, acc.: 68.75%] [G loss: 0.566802]\n",
      "epoch:6 step:6068 [D loss: 0.196214, acc.: 67.97%] [G loss: 0.510035]\n",
      "epoch:6 step:6069 [D loss: 0.180150, acc.: 75.78%] [G loss: 0.523318]\n",
      "epoch:6 step:6070 [D loss: 0.264219, acc.: 59.38%] [G loss: 0.462034]\n",
      "epoch:6 step:6071 [D loss: 0.222450, acc.: 65.62%] [G loss: 0.482530]\n",
      "epoch:6 step:6072 [D loss: 0.179470, acc.: 75.00%] [G loss: 0.508947]\n",
      "epoch:6 step:6073 [D loss: 0.176594, acc.: 75.78%] [G loss: 0.567355]\n",
      "epoch:6 step:6074 [D loss: 0.183678, acc.: 75.78%] [G loss: 0.530898]\n",
      "epoch:6 step:6075 [D loss: 0.183380, acc.: 71.09%] [G loss: 0.517073]\n",
      "epoch:6 step:6076 [D loss: 0.212226, acc.: 64.06%] [G loss: 0.549359]\n",
      "epoch:6 step:6077 [D loss: 0.210241, acc.: 71.88%] [G loss: 0.477731]\n",
      "epoch:6 step:6078 [D loss: 0.220634, acc.: 65.62%] [G loss: 0.535075]\n",
      "epoch:6 step:6079 [D loss: 0.212968, acc.: 62.50%] [G loss: 0.603407]\n",
      "epoch:6 step:6080 [D loss: 0.270445, acc.: 57.81%] [G loss: 0.508183]\n",
      "epoch:6 step:6081 [D loss: 0.211217, acc.: 65.62%] [G loss: 0.503006]\n",
      "epoch:6 step:6082 [D loss: 0.217162, acc.: 66.41%] [G loss: 0.481866]\n",
      "epoch:6 step:6083 [D loss: 0.217090, acc.: 67.97%] [G loss: 0.486775]\n",
      "epoch:6 step:6084 [D loss: 0.215467, acc.: 64.06%] [G loss: 0.495090]\n",
      "epoch:6 step:6085 [D loss: 0.235558, acc.: 62.50%] [G loss: 0.491940]\n",
      "epoch:6 step:6086 [D loss: 0.199568, acc.: 65.62%] [G loss: 0.536371]\n",
      "epoch:6 step:6087 [D loss: 0.247002, acc.: 55.47%] [G loss: 0.536216]\n",
      "epoch:6 step:6088 [D loss: 0.210352, acc.: 64.06%] [G loss: 0.500129]\n",
      "epoch:6 step:6089 [D loss: 0.193115, acc.: 72.66%] [G loss: 0.529067]\n",
      "epoch:6 step:6090 [D loss: 0.227363, acc.: 67.19%] [G loss: 0.509355]\n",
      "epoch:6 step:6091 [D loss: 0.202940, acc.: 70.31%] [G loss: 0.573432]\n",
      "epoch:6 step:6092 [D loss: 0.194078, acc.: 71.88%] [G loss: 0.621711]\n",
      "epoch:6 step:6093 [D loss: 0.182569, acc.: 75.00%] [G loss: 0.605366]\n",
      "epoch:6 step:6094 [D loss: 0.178849, acc.: 74.22%] [G loss: 0.569615]\n",
      "epoch:6 step:6095 [D loss: 0.286155, acc.: 54.69%] [G loss: 0.454559]\n",
      "epoch:6 step:6096 [D loss: 0.183291, acc.: 71.09%] [G loss: 0.562627]\n",
      "epoch:6 step:6097 [D loss: 0.183719, acc.: 74.22%] [G loss: 0.562286]\n",
      "epoch:6 step:6098 [D loss: 0.238462, acc.: 61.72%] [G loss: 0.524922]\n",
      "epoch:6 step:6099 [D loss: 0.220554, acc.: 64.84%] [G loss: 0.478965]\n",
      "epoch:6 step:6100 [D loss: 0.246103, acc.: 59.38%] [G loss: 0.404967]\n",
      "epoch:6 step:6101 [D loss: 0.194558, acc.: 75.78%] [G loss: 0.455445]\n",
      "epoch:6 step:6102 [D loss: 0.242730, acc.: 63.28%] [G loss: 0.469725]\n",
      "epoch:6 step:6103 [D loss: 0.207329, acc.: 67.19%] [G loss: 0.532696]\n",
      "epoch:6 step:6104 [D loss: 0.268097, acc.: 51.56%] [G loss: 0.493199]\n",
      "epoch:6 step:6105 [D loss: 0.244085, acc.: 60.16%] [G loss: 0.437637]\n",
      "epoch:6 step:6106 [D loss: 0.174882, acc.: 73.44%] [G loss: 0.562600]\n",
      "epoch:6 step:6107 [D loss: 0.201397, acc.: 67.97%] [G loss: 0.548276]\n",
      "epoch:6 step:6108 [D loss: 0.231458, acc.: 59.38%] [G loss: 0.469193]\n",
      "epoch:6 step:6109 [D loss: 0.211588, acc.: 67.97%] [G loss: 0.456407]\n",
      "epoch:6 step:6110 [D loss: 0.195458, acc.: 67.97%] [G loss: 0.575542]\n",
      "epoch:6 step:6111 [D loss: 0.229817, acc.: 64.06%] [G loss: 0.497409]\n",
      "epoch:6 step:6112 [D loss: 0.226288, acc.: 64.84%] [G loss: 0.478175]\n",
      "epoch:6 step:6113 [D loss: 0.187902, acc.: 71.88%] [G loss: 0.529689]\n",
      "epoch:6 step:6114 [D loss: 0.222676, acc.: 60.94%] [G loss: 0.477416]\n",
      "epoch:6 step:6115 [D loss: 0.204792, acc.: 68.75%] [G loss: 0.489867]\n",
      "epoch:6 step:6116 [D loss: 0.221476, acc.: 66.41%] [G loss: 0.476226]\n",
      "epoch:6 step:6117 [D loss: 0.196813, acc.: 71.09%] [G loss: 0.505344]\n",
      "epoch:6 step:6118 [D loss: 0.217135, acc.: 67.97%] [G loss: 0.503508]\n",
      "epoch:6 step:6119 [D loss: 0.197681, acc.: 68.75%] [G loss: 0.502551]\n",
      "epoch:6 step:6120 [D loss: 0.174219, acc.: 75.00%] [G loss: 0.523391]\n",
      "epoch:6 step:6121 [D loss: 0.179845, acc.: 73.44%] [G loss: 0.532604]\n",
      "epoch:6 step:6122 [D loss: 0.254284, acc.: 56.25%] [G loss: 0.482435]\n",
      "epoch:6 step:6123 [D loss: 0.274395, acc.: 53.12%] [G loss: 0.407616]\n",
      "epoch:6 step:6124 [D loss: 0.212173, acc.: 67.97%] [G loss: 0.449524]\n",
      "epoch:6 step:6125 [D loss: 0.217451, acc.: 64.06%] [G loss: 0.468072]\n",
      "epoch:6 step:6126 [D loss: 0.170494, acc.: 74.22%] [G loss: 0.568532]\n",
      "epoch:6 step:6127 [D loss: 0.210813, acc.: 65.62%] [G loss: 0.487807]\n",
      "epoch:6 step:6128 [D loss: 0.210204, acc.: 65.62%] [G loss: 0.500550]\n",
      "epoch:6 step:6129 [D loss: 0.181849, acc.: 68.75%] [G loss: 0.521252]\n",
      "epoch:6 step:6130 [D loss: 0.179640, acc.: 77.34%] [G loss: 0.563616]\n",
      "epoch:6 step:6131 [D loss: 0.221376, acc.: 64.84%] [G loss: 0.523411]\n",
      "epoch:6 step:6132 [D loss: 0.214335, acc.: 64.84%] [G loss: 0.528416]\n",
      "epoch:6 step:6133 [D loss: 0.221058, acc.: 63.28%] [G loss: 0.512564]\n",
      "epoch:6 step:6134 [D loss: 0.215361, acc.: 64.06%] [G loss: 0.515611]\n",
      "epoch:6 step:6135 [D loss: 0.196333, acc.: 71.09%] [G loss: 0.571369]\n",
      "epoch:6 step:6136 [D loss: 0.189525, acc.: 69.53%] [G loss: 0.509026]\n",
      "epoch:6 step:6137 [D loss: 0.193992, acc.: 70.31%] [G loss: 0.524165]\n",
      "epoch:6 step:6138 [D loss: 0.204377, acc.: 66.41%] [G loss: 0.513710]\n",
      "epoch:6 step:6139 [D loss: 0.231102, acc.: 62.50%] [G loss: 0.520425]\n",
      "epoch:6 step:6140 [D loss: 0.202220, acc.: 67.97%] [G loss: 0.500730]\n",
      "epoch:6 step:6141 [D loss: 0.164328, acc.: 75.78%] [G loss: 0.529336]\n",
      "epoch:6 step:6142 [D loss: 0.197389, acc.: 71.09%] [G loss: 0.561454]\n",
      "epoch:6 step:6143 [D loss: 0.202539, acc.: 69.53%] [G loss: 0.481205]\n",
      "epoch:6 step:6144 [D loss: 0.174781, acc.: 71.88%] [G loss: 0.520849]\n",
      "epoch:6 step:6145 [D loss: 0.173231, acc.: 78.91%] [G loss: 0.531201]\n",
      "epoch:6 step:6146 [D loss: 0.185653, acc.: 72.66%] [G loss: 0.508988]\n",
      "epoch:6 step:6147 [D loss: 0.205286, acc.: 67.97%] [G loss: 0.541495]\n",
      "epoch:6 step:6148 [D loss: 0.202720, acc.: 69.53%] [G loss: 0.540199]\n",
      "epoch:6 step:6149 [D loss: 0.243259, acc.: 62.50%] [G loss: 0.485541]\n",
      "epoch:6 step:6150 [D loss: 0.275015, acc.: 52.34%] [G loss: 0.498445]\n",
      "epoch:6 step:6151 [D loss: 0.221388, acc.: 66.41%] [G loss: 0.512880]\n",
      "epoch:6 step:6152 [D loss: 0.211626, acc.: 66.41%] [G loss: 0.538077]\n",
      "epoch:6 step:6153 [D loss: 0.260276, acc.: 56.25%] [G loss: 0.485441]\n",
      "epoch:6 step:6154 [D loss: 0.213722, acc.: 65.62%] [G loss: 0.503444]\n",
      "epoch:6 step:6155 [D loss: 0.231401, acc.: 60.94%] [G loss: 0.530353]\n",
      "epoch:6 step:6156 [D loss: 0.188851, acc.: 71.09%] [G loss: 0.570977]\n",
      "epoch:6 step:6157 [D loss: 0.247095, acc.: 58.59%] [G loss: 0.484959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6158 [D loss: 0.186984, acc.: 74.22%] [G loss: 0.523790]\n",
      "epoch:6 step:6159 [D loss: 0.221842, acc.: 64.84%] [G loss: 0.530576]\n",
      "epoch:6 step:6160 [D loss: 0.228731, acc.: 64.06%] [G loss: 0.482474]\n",
      "epoch:6 step:6161 [D loss: 0.208264, acc.: 63.28%] [G loss: 0.527073]\n",
      "epoch:6 step:6162 [D loss: 0.234748, acc.: 64.84%] [G loss: 0.500020]\n",
      "epoch:6 step:6163 [D loss: 0.200527, acc.: 67.97%] [G loss: 0.492894]\n",
      "epoch:6 step:6164 [D loss: 0.273635, acc.: 54.69%] [G loss: 0.473577]\n",
      "epoch:6 step:6165 [D loss: 0.215970, acc.: 68.75%] [G loss: 0.497901]\n",
      "epoch:6 step:6166 [D loss: 0.200430, acc.: 64.84%] [G loss: 0.474615]\n",
      "epoch:6 step:6167 [D loss: 0.161441, acc.: 78.12%] [G loss: 0.541583]\n",
      "epoch:6 step:6168 [D loss: 0.215792, acc.: 66.41%] [G loss: 0.472014]\n",
      "epoch:6 step:6169 [D loss: 0.175009, acc.: 71.09%] [G loss: 0.522286]\n",
      "epoch:6 step:6170 [D loss: 0.178479, acc.: 75.00%] [G loss: 0.547511]\n",
      "epoch:6 step:6171 [D loss: 0.210397, acc.: 67.19%] [G loss: 0.587954]\n",
      "epoch:6 step:6172 [D loss: 0.204407, acc.: 67.97%] [G loss: 0.539652]\n",
      "epoch:6 step:6173 [D loss: 0.205062, acc.: 71.88%] [G loss: 0.534458]\n",
      "epoch:6 step:6174 [D loss: 0.202068, acc.: 70.31%] [G loss: 0.494982]\n",
      "epoch:6 step:6175 [D loss: 0.227267, acc.: 61.72%] [G loss: 0.450623]\n",
      "epoch:6 step:6176 [D loss: 0.166833, acc.: 76.56%] [G loss: 0.533898]\n",
      "epoch:6 step:6177 [D loss: 0.166721, acc.: 78.12%] [G loss: 0.542610]\n",
      "epoch:6 step:6178 [D loss: 0.208825, acc.: 67.97%] [G loss: 0.522286]\n",
      "epoch:6 step:6179 [D loss: 0.177710, acc.: 74.22%] [G loss: 0.517260]\n",
      "epoch:6 step:6180 [D loss: 0.176355, acc.: 73.44%] [G loss: 0.523565]\n",
      "epoch:6 step:6181 [D loss: 0.235939, acc.: 60.94%] [G loss: 0.522587]\n",
      "epoch:6 step:6182 [D loss: 0.244833, acc.: 56.25%] [G loss: 0.505763]\n",
      "epoch:6 step:6183 [D loss: 0.195997, acc.: 67.97%] [G loss: 0.524310]\n",
      "epoch:6 step:6184 [D loss: 0.224136, acc.: 61.72%] [G loss: 0.481748]\n",
      "epoch:6 step:6185 [D loss: 0.192455, acc.: 69.53%] [G loss: 0.520459]\n",
      "epoch:6 step:6186 [D loss: 0.211113, acc.: 64.84%] [G loss: 0.538945]\n",
      "epoch:6 step:6187 [D loss: 0.274320, acc.: 57.03%] [G loss: 0.522656]\n",
      "epoch:6 step:6188 [D loss: 0.255397, acc.: 60.16%] [G loss: 0.491973]\n",
      "epoch:6 step:6189 [D loss: 0.186562, acc.: 71.09%] [G loss: 0.539272]\n",
      "epoch:6 step:6190 [D loss: 0.173899, acc.: 74.22%] [G loss: 0.552088]\n",
      "epoch:6 step:6191 [D loss: 0.263937, acc.: 53.91%] [G loss: 0.500819]\n",
      "epoch:6 step:6192 [D loss: 0.201156, acc.: 68.75%] [G loss: 0.491182]\n",
      "epoch:6 step:6193 [D loss: 0.221789, acc.: 64.84%] [G loss: 0.463569]\n",
      "epoch:6 step:6194 [D loss: 0.220671, acc.: 62.50%] [G loss: 0.523552]\n",
      "epoch:6 step:6195 [D loss: 0.201994, acc.: 67.19%] [G loss: 0.518080]\n",
      "epoch:6 step:6196 [D loss: 0.164558, acc.: 75.78%] [G loss: 0.552599]\n",
      "epoch:6 step:6197 [D loss: 0.194425, acc.: 68.75%] [G loss: 0.557871]\n",
      "epoch:6 step:6198 [D loss: 0.241961, acc.: 60.16%] [G loss: 0.497917]\n",
      "epoch:6 step:6199 [D loss: 0.225472, acc.: 65.62%] [G loss: 0.475986]\n",
      "epoch:6 step:6200 [D loss: 0.193412, acc.: 75.00%] [G loss: 0.511819]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.010545\n",
      "FID: 21.735344\n",
      "0 = 12.190580756044378\n",
      "1 = 0.05638538598182455\n",
      "2 = 0.9376000165939331\n",
      "3 = 0.9075999855995178\n",
      "4 = 0.9675999879837036\n",
      "5 = 0.9655318856239319\n",
      "6 = 0.9075999855995178\n",
      "7 = 7.636462782078984\n",
      "8 = 0.10267673514483425\n",
      "9 = 0.7656499743461609\n",
      "10 = 0.7635999917984009\n",
      "11 = 0.7677000164985657\n",
      "12 = 0.7667436599731445\n",
      "13 = 0.7635999917984009\n",
      "14 = 7.010613918304443\n",
      "15 = 9.48947525024414\n",
      "16 = 0.13769516348838806\n",
      "17 = 7.010544776916504\n",
      "18 = 21.73534393310547\n",
      "epoch:6 step:6201 [D loss: 0.187079, acc.: 73.44%] [G loss: 0.493272]\n",
      "epoch:6 step:6202 [D loss: 0.230223, acc.: 59.38%] [G loss: 0.483112]\n",
      "epoch:6 step:6203 [D loss: 0.188748, acc.: 69.53%] [G loss: 0.581070]\n",
      "epoch:6 step:6204 [D loss: 0.188078, acc.: 68.75%] [G loss: 0.605460]\n",
      "epoch:6 step:6205 [D loss: 0.212105, acc.: 67.19%] [G loss: 0.561697]\n",
      "epoch:6 step:6206 [D loss: 0.226434, acc.: 64.84%] [G loss: 0.517478]\n",
      "epoch:6 step:6207 [D loss: 0.214221, acc.: 64.06%] [G loss: 0.528885]\n",
      "epoch:6 step:6208 [D loss: 0.224629, acc.: 65.62%] [G loss: 0.501648]\n",
      "epoch:6 step:6209 [D loss: 0.202036, acc.: 71.09%] [G loss: 0.491864]\n",
      "epoch:6 step:6210 [D loss: 0.220581, acc.: 67.19%] [G loss: 0.491254]\n",
      "epoch:6 step:6211 [D loss: 0.162868, acc.: 80.47%] [G loss: 0.579649]\n",
      "epoch:6 step:6212 [D loss: 0.265487, acc.: 53.12%] [G loss: 0.544695]\n",
      "epoch:6 step:6213 [D loss: 0.223713, acc.: 64.06%] [G loss: 0.465874]\n",
      "epoch:6 step:6214 [D loss: 0.202137, acc.: 69.53%] [G loss: 0.484367]\n",
      "epoch:6 step:6215 [D loss: 0.241969, acc.: 57.03%] [G loss: 0.484061]\n",
      "epoch:6 step:6216 [D loss: 0.238819, acc.: 63.28%] [G loss: 0.496247]\n",
      "epoch:6 step:6217 [D loss: 0.197302, acc.: 74.22%] [G loss: 0.514474]\n",
      "epoch:6 step:6218 [D loss: 0.221742, acc.: 61.72%] [G loss: 0.507222]\n",
      "epoch:6 step:6219 [D loss: 0.213665, acc.: 67.19%] [G loss: 0.500316]\n",
      "epoch:6 step:6220 [D loss: 0.208816, acc.: 66.41%] [G loss: 0.520928]\n",
      "epoch:6 step:6221 [D loss: 0.233252, acc.: 64.84%] [G loss: 0.511238]\n",
      "epoch:6 step:6222 [D loss: 0.250226, acc.: 60.94%] [G loss: 0.456650]\n",
      "epoch:6 step:6223 [D loss: 0.209513, acc.: 69.53%] [G loss: 0.494941]\n",
      "epoch:6 step:6224 [D loss: 0.212452, acc.: 65.62%] [G loss: 0.544652]\n",
      "epoch:6 step:6225 [D loss: 0.193722, acc.: 73.44%] [G loss: 0.453896]\n",
      "epoch:6 step:6226 [D loss: 0.236352, acc.: 64.84%] [G loss: 0.451842]\n",
      "epoch:6 step:6227 [D loss: 0.180522, acc.: 71.88%] [G loss: 0.475398]\n",
      "epoch:6 step:6228 [D loss: 0.225285, acc.: 64.06%] [G loss: 0.495396]\n",
      "epoch:6 step:6229 [D loss: 0.191924, acc.: 72.66%] [G loss: 0.514436]\n",
      "epoch:6 step:6230 [D loss: 0.206660, acc.: 67.97%] [G loss: 0.515802]\n",
      "epoch:6 step:6231 [D loss: 0.186604, acc.: 74.22%] [G loss: 0.495523]\n",
      "epoch:6 step:6232 [D loss: 0.185289, acc.: 74.22%] [G loss: 0.519216]\n",
      "epoch:6 step:6233 [D loss: 0.220689, acc.: 63.28%] [G loss: 0.521240]\n",
      "epoch:6 step:6234 [D loss: 0.220179, acc.: 61.72%] [G loss: 0.502469]\n",
      "epoch:6 step:6235 [D loss: 0.190478, acc.: 71.09%] [G loss: 0.495871]\n",
      "epoch:6 step:6236 [D loss: 0.207509, acc.: 67.97%] [G loss: 0.522759]\n",
      "epoch:6 step:6237 [D loss: 0.251376, acc.: 56.25%] [G loss: 0.455053]\n",
      "epoch:6 step:6238 [D loss: 0.218336, acc.: 68.75%] [G loss: 0.493569]\n",
      "epoch:6 step:6239 [D loss: 0.207929, acc.: 62.50%] [G loss: 0.500680]\n",
      "epoch:6 step:6240 [D loss: 0.228049, acc.: 63.28%] [G loss: 0.498486]\n",
      "epoch:6 step:6241 [D loss: 0.212870, acc.: 63.28%] [G loss: 0.511021]\n",
      "epoch:6 step:6242 [D loss: 0.183678, acc.: 72.66%] [G loss: 0.501953]\n",
      "epoch:6 step:6243 [D loss: 0.223864, acc.: 65.62%] [G loss: 0.455515]\n",
      "epoch:6 step:6244 [D loss: 0.250001, acc.: 60.16%] [G loss: 0.490385]\n",
      "epoch:6 step:6245 [D loss: 0.202938, acc.: 73.44%] [G loss: 0.498020]\n",
      "epoch:6 step:6246 [D loss: 0.172777, acc.: 79.69%] [G loss: 0.572799]\n",
      "epoch:6 step:6247 [D loss: 0.238865, acc.: 60.94%] [G loss: 0.484625]\n",
      "epoch:6 step:6248 [D loss: 0.207213, acc.: 66.41%] [G loss: 0.529932]\n",
      "epoch:6 step:6249 [D loss: 0.208156, acc.: 67.19%] [G loss: 0.486020]\n",
      "epoch:6 step:6250 [D loss: 0.216229, acc.: 67.19%] [G loss: 0.509520]\n",
      "epoch:6 step:6251 [D loss: 0.215477, acc.: 65.62%] [G loss: 0.464144]\n",
      "epoch:6 step:6252 [D loss: 0.223328, acc.: 66.41%] [G loss: 0.455204]\n",
      "epoch:6 step:6253 [D loss: 0.217776, acc.: 69.53%] [G loss: 0.525647]\n",
      "epoch:6 step:6254 [D loss: 0.198097, acc.: 71.09%] [G loss: 0.565271]\n",
      "epoch:6 step:6255 [D loss: 0.235963, acc.: 60.16%] [G loss: 0.499936]\n",
      "epoch:6 step:6256 [D loss: 0.205128, acc.: 69.53%] [G loss: 0.529839]\n",
      "epoch:6 step:6257 [D loss: 0.187492, acc.: 66.41%] [G loss: 0.541344]\n",
      "epoch:6 step:6258 [D loss: 0.231503, acc.: 61.72%] [G loss: 0.454104]\n",
      "epoch:6 step:6259 [D loss: 0.208286, acc.: 64.84%] [G loss: 0.465561]\n",
      "epoch:6 step:6260 [D loss: 0.200264, acc.: 67.97%] [G loss: 0.538797]\n",
      "epoch:6 step:6261 [D loss: 0.205617, acc.: 70.31%] [G loss: 0.501554]\n",
      "epoch:6 step:6262 [D loss: 0.210944, acc.: 69.53%] [G loss: 0.485512]\n",
      "epoch:6 step:6263 [D loss: 0.181329, acc.: 68.75%] [G loss: 0.511460]\n",
      "epoch:6 step:6264 [D loss: 0.184850, acc.: 72.66%] [G loss: 0.615970]\n",
      "epoch:6 step:6265 [D loss: 0.207538, acc.: 65.62%] [G loss: 0.577462]\n",
      "epoch:6 step:6266 [D loss: 0.210977, acc.: 64.84%] [G loss: 0.470740]\n",
      "epoch:6 step:6267 [D loss: 0.234504, acc.: 67.19%] [G loss: 0.511121]\n",
      "epoch:6 step:6268 [D loss: 0.208839, acc.: 63.28%] [G loss: 0.503316]\n",
      "epoch:6 step:6269 [D loss: 0.168426, acc.: 75.00%] [G loss: 0.617261]\n",
      "epoch:6 step:6270 [D loss: 0.164577, acc.: 81.25%] [G loss: 0.613634]\n",
      "epoch:6 step:6271 [D loss: 0.161690, acc.: 79.69%] [G loss: 0.580171]\n",
      "epoch:6 step:6272 [D loss: 0.221799, acc.: 68.75%] [G loss: 0.538423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6273 [D loss: 0.212978, acc.: 63.28%] [G loss: 0.507952]\n",
      "epoch:6 step:6274 [D loss: 0.228622, acc.: 60.16%] [G loss: 0.520423]\n",
      "epoch:6 step:6275 [D loss: 0.189726, acc.: 73.44%] [G loss: 0.570827]\n",
      "epoch:6 step:6276 [D loss: 0.210663, acc.: 71.88%] [G loss: 0.520584]\n",
      "epoch:6 step:6277 [D loss: 0.257491, acc.: 62.50%] [G loss: 0.486517]\n",
      "epoch:6 step:6278 [D loss: 0.203046, acc.: 70.31%] [G loss: 0.493120]\n",
      "epoch:6 step:6279 [D loss: 0.196019, acc.: 65.62%] [G loss: 0.573933]\n",
      "epoch:6 step:6280 [D loss: 0.234185, acc.: 65.62%] [G loss: 0.468579]\n",
      "epoch:6 step:6281 [D loss: 0.178303, acc.: 82.81%] [G loss: 0.520910]\n",
      "epoch:6 step:6282 [D loss: 0.177980, acc.: 74.22%] [G loss: 0.523579]\n",
      "epoch:6 step:6283 [D loss: 0.191613, acc.: 71.88%] [G loss: 0.526937]\n",
      "epoch:6 step:6284 [D loss: 0.224242, acc.: 60.94%] [G loss: 0.471951]\n",
      "epoch:6 step:6285 [D loss: 0.218432, acc.: 65.62%] [G loss: 0.511962]\n",
      "epoch:6 step:6286 [D loss: 0.215537, acc.: 67.97%] [G loss: 0.537306]\n",
      "epoch:6 step:6287 [D loss: 0.231310, acc.: 64.84%] [G loss: 0.503429]\n",
      "epoch:6 step:6288 [D loss: 0.221776, acc.: 64.84%] [G loss: 0.521928]\n",
      "epoch:6 step:6289 [D loss: 0.207677, acc.: 64.84%] [G loss: 0.517824]\n",
      "epoch:6 step:6290 [D loss: 0.207703, acc.: 67.97%] [G loss: 0.496158]\n",
      "epoch:6 step:6291 [D loss: 0.216960, acc.: 62.50%] [G loss: 0.506886]\n",
      "epoch:6 step:6292 [D loss: 0.222209, acc.: 61.72%] [G loss: 0.500437]\n",
      "epoch:6 step:6293 [D loss: 0.232596, acc.: 63.28%] [G loss: 0.496901]\n",
      "epoch:6 step:6294 [D loss: 0.231193, acc.: 60.94%] [G loss: 0.477267]\n",
      "epoch:6 step:6295 [D loss: 0.190380, acc.: 67.19%] [G loss: 0.535267]\n",
      "epoch:6 step:6296 [D loss: 0.220227, acc.: 68.75%] [G loss: 0.603577]\n",
      "epoch:6 step:6297 [D loss: 0.206942, acc.: 67.19%] [G loss: 0.550552]\n",
      "epoch:6 step:6298 [D loss: 0.184405, acc.: 71.88%] [G loss: 0.543155]\n",
      "epoch:6 step:6299 [D loss: 0.193990, acc.: 68.75%] [G loss: 0.547229]\n",
      "epoch:6 step:6300 [D loss: 0.203852, acc.: 67.19%] [G loss: 0.512520]\n",
      "epoch:6 step:6301 [D loss: 0.204820, acc.: 68.75%] [G loss: 0.491292]\n",
      "epoch:6 step:6302 [D loss: 0.201343, acc.: 69.53%] [G loss: 0.501761]\n",
      "epoch:6 step:6303 [D loss: 0.194715, acc.: 71.88%] [G loss: 0.533616]\n",
      "epoch:6 step:6304 [D loss: 0.211352, acc.: 66.41%] [G loss: 0.491325]\n",
      "epoch:6 step:6305 [D loss: 0.235302, acc.: 60.94%] [G loss: 0.490490]\n",
      "epoch:6 step:6306 [D loss: 0.202954, acc.: 70.31%] [G loss: 0.496650]\n",
      "epoch:6 step:6307 [D loss: 0.209261, acc.: 68.75%] [G loss: 0.454857]\n",
      "epoch:6 step:6308 [D loss: 0.220522, acc.: 67.19%] [G loss: 0.510980]\n",
      "epoch:6 step:6309 [D loss: 0.216527, acc.: 67.97%] [G loss: 0.476166]\n",
      "epoch:6 step:6310 [D loss: 0.220620, acc.: 61.72%] [G loss: 0.563377]\n",
      "epoch:6 step:6311 [D loss: 0.221289, acc.: 62.50%] [G loss: 0.544527]\n",
      "epoch:6 step:6312 [D loss: 0.177425, acc.: 72.66%] [G loss: 0.536352]\n",
      "epoch:6 step:6313 [D loss: 0.190717, acc.: 78.12%] [G loss: 0.541836]\n",
      "epoch:6 step:6314 [D loss: 0.183730, acc.: 71.09%] [G loss: 0.538480]\n",
      "epoch:6 step:6315 [D loss: 0.205902, acc.: 66.41%] [G loss: 0.536446]\n",
      "epoch:6 step:6316 [D loss: 0.183133, acc.: 75.78%] [G loss: 0.553317]\n",
      "epoch:6 step:6317 [D loss: 0.235354, acc.: 58.59%] [G loss: 0.510601]\n",
      "epoch:6 step:6318 [D loss: 0.196956, acc.: 70.31%] [G loss: 0.503255]\n",
      "epoch:6 step:6319 [D loss: 0.192967, acc.: 74.22%] [G loss: 0.532478]\n",
      "epoch:6 step:6320 [D loss: 0.201152, acc.: 66.41%] [G loss: 0.496709]\n",
      "epoch:6 step:6321 [D loss: 0.184441, acc.: 75.78%] [G loss: 0.561985]\n",
      "epoch:6 step:6322 [D loss: 0.170537, acc.: 78.12%] [G loss: 0.544824]\n",
      "epoch:6 step:6323 [D loss: 0.200871, acc.: 72.66%] [G loss: 0.564058]\n",
      "epoch:6 step:6324 [D loss: 0.276697, acc.: 57.03%] [G loss: 0.435944]\n",
      "epoch:6 step:6325 [D loss: 0.235603, acc.: 56.25%] [G loss: 0.476155]\n",
      "epoch:6 step:6326 [D loss: 0.222936, acc.: 64.06%] [G loss: 0.501001]\n",
      "epoch:6 step:6327 [D loss: 0.223957, acc.: 67.97%] [G loss: 0.487411]\n",
      "epoch:6 step:6328 [D loss: 0.219036, acc.: 63.28%] [G loss: 0.516136]\n",
      "epoch:6 step:6329 [D loss: 0.183657, acc.: 74.22%] [G loss: 0.505705]\n",
      "epoch:6 step:6330 [D loss: 0.201927, acc.: 71.88%] [G loss: 0.529759]\n",
      "epoch:6 step:6331 [D loss: 0.180990, acc.: 72.66%] [G loss: 0.592644]\n",
      "epoch:6 step:6332 [D loss: 0.245871, acc.: 60.16%] [G loss: 0.544201]\n",
      "epoch:6 step:6333 [D loss: 0.207913, acc.: 68.75%] [G loss: 0.501240]\n",
      "epoch:6 step:6334 [D loss: 0.191590, acc.: 70.31%] [G loss: 0.525924]\n",
      "epoch:6 step:6335 [D loss: 0.236419, acc.: 64.84%] [G loss: 0.491371]\n",
      "epoch:6 step:6336 [D loss: 0.207499, acc.: 69.53%] [G loss: 0.504439]\n",
      "epoch:6 step:6337 [D loss: 0.234181, acc.: 63.28%] [G loss: 0.529138]\n",
      "epoch:6 step:6338 [D loss: 0.250613, acc.: 63.28%] [G loss: 0.511854]\n",
      "epoch:6 step:6339 [D loss: 0.223823, acc.: 64.06%] [G loss: 0.479296]\n",
      "epoch:6 step:6340 [D loss: 0.239084, acc.: 62.50%] [G loss: 0.497662]\n",
      "epoch:6 step:6341 [D loss: 0.195092, acc.: 73.44%] [G loss: 0.545635]\n",
      "epoch:6 step:6342 [D loss: 0.224494, acc.: 66.41%] [G loss: 0.496917]\n",
      "epoch:6 step:6343 [D loss: 0.216715, acc.: 64.84%] [G loss: 0.543113]\n",
      "epoch:6 step:6344 [D loss: 0.230251, acc.: 55.47%] [G loss: 0.525734]\n",
      "epoch:6 step:6345 [D loss: 0.201787, acc.: 69.53%] [G loss: 0.536901]\n",
      "epoch:6 step:6346 [D loss: 0.217789, acc.: 64.84%] [G loss: 0.490119]\n",
      "epoch:6 step:6347 [D loss: 0.194082, acc.: 72.66%] [G loss: 0.547856]\n",
      "epoch:6 step:6348 [D loss: 0.206533, acc.: 66.41%] [G loss: 0.537686]\n",
      "epoch:6 step:6349 [D loss: 0.224463, acc.: 58.59%] [G loss: 0.512996]\n",
      "epoch:6 step:6350 [D loss: 0.219933, acc.: 67.19%] [G loss: 0.475107]\n",
      "epoch:6 step:6351 [D loss: 0.212554, acc.: 69.53%] [G loss: 0.488480]\n",
      "epoch:6 step:6352 [D loss: 0.164656, acc.: 77.34%] [G loss: 0.555869]\n",
      "epoch:6 step:6353 [D loss: 0.201478, acc.: 67.97%] [G loss: 0.520717]\n",
      "epoch:6 step:6354 [D loss: 0.203516, acc.: 67.19%] [G loss: 0.545545]\n",
      "epoch:6 step:6355 [D loss: 0.187737, acc.: 69.53%] [G loss: 0.539638]\n",
      "epoch:6 step:6356 [D loss: 0.214439, acc.: 67.19%] [G loss: 0.533544]\n",
      "epoch:6 step:6357 [D loss: 0.217484, acc.: 68.75%] [G loss: 0.485738]\n",
      "epoch:6 step:6358 [D loss: 0.201802, acc.: 70.31%] [G loss: 0.533596]\n",
      "epoch:6 step:6359 [D loss: 0.179552, acc.: 75.78%] [G loss: 0.556291]\n",
      "epoch:6 step:6360 [D loss: 0.241595, acc.: 56.25%] [G loss: 0.493494]\n",
      "epoch:6 step:6361 [D loss: 0.246007, acc.: 60.94%] [G loss: 0.462367]\n",
      "epoch:6 step:6362 [D loss: 0.222294, acc.: 68.75%] [G loss: 0.492022]\n",
      "epoch:6 step:6363 [D loss: 0.224992, acc.: 64.84%] [G loss: 0.480741]\n",
      "epoch:6 step:6364 [D loss: 0.228931, acc.: 64.06%] [G loss: 0.513275]\n",
      "epoch:6 step:6365 [D loss: 0.212731, acc.: 71.88%] [G loss: 0.501610]\n",
      "epoch:6 step:6366 [D loss: 0.221274, acc.: 67.97%] [G loss: 0.475381]\n",
      "epoch:6 step:6367 [D loss: 0.229231, acc.: 60.94%] [G loss: 0.527617]\n",
      "epoch:6 step:6368 [D loss: 0.195990, acc.: 67.97%] [G loss: 0.544943]\n",
      "epoch:6 step:6369 [D loss: 0.197101, acc.: 66.41%] [G loss: 0.515326]\n",
      "epoch:6 step:6370 [D loss: 0.198259, acc.: 65.62%] [G loss: 0.546336]\n",
      "epoch:6 step:6371 [D loss: 0.199576, acc.: 69.53%] [G loss: 0.496517]\n",
      "epoch:6 step:6372 [D loss: 0.217581, acc.: 67.19%] [G loss: 0.495548]\n",
      "epoch:6 step:6373 [D loss: 0.222112, acc.: 63.28%] [G loss: 0.492618]\n",
      "epoch:6 step:6374 [D loss: 0.203589, acc.: 68.75%] [G loss: 0.512636]\n",
      "epoch:6 step:6375 [D loss: 0.192088, acc.: 71.09%] [G loss: 0.492768]\n",
      "epoch:6 step:6376 [D loss: 0.191968, acc.: 71.88%] [G loss: 0.546122]\n",
      "epoch:6 step:6377 [D loss: 0.209000, acc.: 62.50%] [G loss: 0.523258]\n",
      "epoch:6 step:6378 [D loss: 0.199814, acc.: 72.66%] [G loss: 0.547121]\n",
      "epoch:6 step:6379 [D loss: 0.234926, acc.: 62.50%] [G loss: 0.520897]\n",
      "epoch:6 step:6380 [D loss: 0.228460, acc.: 68.75%] [G loss: 0.468672]\n",
      "epoch:6 step:6381 [D loss: 0.218117, acc.: 67.19%] [G loss: 0.485693]\n",
      "epoch:6 step:6382 [D loss: 0.250470, acc.: 58.59%] [G loss: 0.442768]\n",
      "epoch:6 step:6383 [D loss: 0.181498, acc.: 75.00%] [G loss: 0.525147]\n",
      "epoch:6 step:6384 [D loss: 0.231514, acc.: 59.38%] [G loss: 0.528033]\n",
      "epoch:6 step:6385 [D loss: 0.244052, acc.: 57.81%] [G loss: 0.500152]\n",
      "epoch:6 step:6386 [D loss: 0.202158, acc.: 71.88%] [G loss: 0.497405]\n",
      "epoch:6 step:6387 [D loss: 0.268445, acc.: 54.69%] [G loss: 0.437581]\n",
      "epoch:6 step:6388 [D loss: 0.214830, acc.: 67.97%] [G loss: 0.530788]\n",
      "epoch:6 step:6389 [D loss: 0.183934, acc.: 71.88%] [G loss: 0.546215]\n",
      "epoch:6 step:6390 [D loss: 0.257917, acc.: 54.69%] [G loss: 0.506169]\n",
      "epoch:6 step:6391 [D loss: 0.189845, acc.: 68.75%] [G loss: 0.568389]\n",
      "epoch:6 step:6392 [D loss: 0.233824, acc.: 61.72%] [G loss: 0.538482]\n",
      "epoch:6 step:6393 [D loss: 0.206859, acc.: 64.84%] [G loss: 0.539733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6394 [D loss: 0.191713, acc.: 70.31%] [G loss: 0.505526]\n",
      "epoch:6 step:6395 [D loss: 0.204670, acc.: 64.06%] [G loss: 0.451338]\n",
      "epoch:6 step:6396 [D loss: 0.215177, acc.: 67.19%] [G loss: 0.518828]\n",
      "epoch:6 step:6397 [D loss: 0.212685, acc.: 67.19%] [G loss: 0.569551]\n",
      "epoch:6 step:6398 [D loss: 0.244749, acc.: 57.03%] [G loss: 0.529360]\n",
      "epoch:6 step:6399 [D loss: 0.211380, acc.: 66.41%] [G loss: 0.545316]\n",
      "epoch:6 step:6400 [D loss: 0.215128, acc.: 67.97%] [G loss: 0.529182]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.020840\n",
      "FID: 21.133934\n",
      "0 = 12.170328209352506\n",
      "1 = 0.05413429564543031\n",
      "2 = 0.934249997138977\n",
      "3 = 0.9077000021934509\n",
      "4 = 0.9607999920845032\n",
      "5 = 0.9586017727851868\n",
      "6 = 0.9077000021934509\n",
      "7 = 7.619857830011844\n",
      "8 = 0.10112492123519413\n",
      "9 = 0.762499988079071\n",
      "10 = 0.7544999718666077\n",
      "11 = 0.7705000042915344\n",
      "12 = 0.7667682766914368\n",
      "13 = 0.7544999718666077\n",
      "14 = 7.020904064178467\n",
      "15 = 9.44926643371582\n",
      "16 = 0.14622174203395844\n",
      "17 = 7.020840167999268\n",
      "18 = 21.133934020996094\n",
      "epoch:6 step:6401 [D loss: 0.219653, acc.: 60.94%] [G loss: 0.489617]\n",
      "epoch:6 step:6402 [D loss: 0.224159, acc.: 65.62%] [G loss: 0.492000]\n",
      "epoch:6 step:6403 [D loss: 0.188155, acc.: 68.75%] [G loss: 0.522847]\n",
      "epoch:6 step:6404 [D loss: 0.186688, acc.: 73.44%] [G loss: 0.535455]\n",
      "epoch:6 step:6405 [D loss: 0.236606, acc.: 60.16%] [G loss: 0.515070]\n",
      "epoch:6 step:6406 [D loss: 0.227502, acc.: 64.06%] [G loss: 0.440717]\n",
      "epoch:6 step:6407 [D loss: 0.203547, acc.: 69.53%] [G loss: 0.511830]\n",
      "epoch:6 step:6408 [D loss: 0.161653, acc.: 78.12%] [G loss: 0.562810]\n",
      "epoch:6 step:6409 [D loss: 0.239825, acc.: 63.28%] [G loss: 0.507428]\n",
      "epoch:6 step:6410 [D loss: 0.244926, acc.: 57.81%] [G loss: 0.459086]\n",
      "epoch:6 step:6411 [D loss: 0.189983, acc.: 74.22%] [G loss: 0.489661]\n",
      "epoch:6 step:6412 [D loss: 0.223231, acc.: 63.28%] [G loss: 0.508463]\n",
      "epoch:6 step:6413 [D loss: 0.237167, acc.: 64.06%] [G loss: 0.458454]\n",
      "epoch:6 step:6414 [D loss: 0.182086, acc.: 70.31%] [G loss: 0.488510]\n",
      "epoch:6 step:6415 [D loss: 0.221439, acc.: 66.41%] [G loss: 0.528969]\n",
      "epoch:6 step:6416 [D loss: 0.265055, acc.: 55.47%] [G loss: 0.467627]\n",
      "epoch:6 step:6417 [D loss: 0.251747, acc.: 60.94%] [G loss: 0.461514]\n",
      "epoch:6 step:6418 [D loss: 0.200128, acc.: 69.53%] [G loss: 0.542950]\n",
      "epoch:6 step:6419 [D loss: 0.244147, acc.: 58.59%] [G loss: 0.529079]\n",
      "epoch:6 step:6420 [D loss: 0.205440, acc.: 64.84%] [G loss: 0.504239]\n",
      "epoch:6 step:6421 [D loss: 0.201638, acc.: 71.09%] [G loss: 0.505906]\n",
      "epoch:6 step:6422 [D loss: 0.198571, acc.: 70.31%] [G loss: 0.543950]\n",
      "epoch:6 step:6423 [D loss: 0.171655, acc.: 77.34%] [G loss: 0.542430]\n",
      "epoch:6 step:6424 [D loss: 0.185519, acc.: 68.75%] [G loss: 0.549988]\n",
      "epoch:6 step:6425 [D loss: 0.212133, acc.: 67.97%] [G loss: 0.599454]\n",
      "epoch:6 step:6426 [D loss: 0.241397, acc.: 53.91%] [G loss: 0.450122]\n",
      "epoch:6 step:6427 [D loss: 0.209071, acc.: 69.53%] [G loss: 0.519589]\n",
      "epoch:6 step:6428 [D loss: 0.232378, acc.: 59.38%] [G loss: 0.484367]\n",
      "epoch:6 step:6429 [D loss: 0.199716, acc.: 67.19%] [G loss: 0.507215]\n",
      "epoch:6 step:6430 [D loss: 0.231452, acc.: 64.06%] [G loss: 0.483497]\n",
      "epoch:6 step:6431 [D loss: 0.216714, acc.: 70.31%] [G loss: 0.480833]\n",
      "epoch:6 step:6432 [D loss: 0.214586, acc.: 67.97%] [G loss: 0.506807]\n",
      "epoch:6 step:6433 [D loss: 0.224926, acc.: 64.84%] [G loss: 0.500800]\n",
      "epoch:6 step:6434 [D loss: 0.266448, acc.: 58.59%] [G loss: 0.500431]\n",
      "epoch:6 step:6435 [D loss: 0.190534, acc.: 74.22%] [G loss: 0.449006]\n",
      "epoch:6 step:6436 [D loss: 0.223770, acc.: 67.97%] [G loss: 0.505198]\n",
      "epoch:6 step:6437 [D loss: 0.182979, acc.: 75.78%] [G loss: 0.577884]\n",
      "epoch:6 step:6438 [D loss: 0.220853, acc.: 69.53%] [G loss: 0.536171]\n",
      "epoch:6 step:6439 [D loss: 0.209945, acc.: 69.53%] [G loss: 0.529869]\n",
      "epoch:6 step:6440 [D loss: 0.257860, acc.: 53.12%] [G loss: 0.483724]\n",
      "epoch:6 step:6441 [D loss: 0.222427, acc.: 65.62%] [G loss: 0.472452]\n",
      "epoch:6 step:6442 [D loss: 0.233814, acc.: 61.72%] [G loss: 0.506485]\n",
      "epoch:6 step:6443 [D loss: 0.197521, acc.: 68.75%] [G loss: 0.548533]\n",
      "epoch:6 step:6444 [D loss: 0.189537, acc.: 71.09%] [G loss: 0.513295]\n",
      "epoch:6 step:6445 [D loss: 0.193998, acc.: 67.97%] [G loss: 0.558500]\n",
      "epoch:6 step:6446 [D loss: 0.269307, acc.: 54.69%] [G loss: 0.469612]\n",
      "epoch:6 step:6447 [D loss: 0.186469, acc.: 69.53%] [G loss: 0.487806]\n",
      "epoch:6 step:6448 [D loss: 0.216862, acc.: 64.84%] [G loss: 0.452184]\n",
      "epoch:6 step:6449 [D loss: 0.264932, acc.: 52.34%] [G loss: 0.488397]\n",
      "epoch:6 step:6450 [D loss: 0.241960, acc.: 57.03%] [G loss: 0.495376]\n",
      "epoch:6 step:6451 [D loss: 0.202920, acc.: 70.31%] [G loss: 0.513886]\n",
      "epoch:6 step:6452 [D loss: 0.208850, acc.: 69.53%] [G loss: 0.513776]\n",
      "epoch:6 step:6453 [D loss: 0.231658, acc.: 64.06%] [G loss: 0.485779]\n",
      "epoch:6 step:6454 [D loss: 0.188458, acc.: 74.22%] [G loss: 0.532307]\n",
      "epoch:6 step:6455 [D loss: 0.203710, acc.: 68.75%] [G loss: 0.500361]\n",
      "epoch:6 step:6456 [D loss: 0.221280, acc.: 60.16%] [G loss: 0.474027]\n",
      "epoch:6 step:6457 [D loss: 0.209034, acc.: 66.41%] [G loss: 0.466763]\n",
      "epoch:6 step:6458 [D loss: 0.213979, acc.: 67.19%] [G loss: 0.487703]\n",
      "epoch:6 step:6459 [D loss: 0.203312, acc.: 70.31%] [G loss: 0.491323]\n",
      "epoch:6 step:6460 [D loss: 0.190101, acc.: 71.88%] [G loss: 0.537441]\n",
      "epoch:6 step:6461 [D loss: 0.217775, acc.: 62.50%] [G loss: 0.510780]\n",
      "epoch:6 step:6462 [D loss: 0.218520, acc.: 65.62%] [G loss: 0.512804]\n",
      "epoch:6 step:6463 [D loss: 0.215440, acc.: 67.97%] [G loss: 0.506175]\n",
      "epoch:6 step:6464 [D loss: 0.182381, acc.: 75.00%] [G loss: 0.546139]\n",
      "epoch:6 step:6465 [D loss: 0.202259, acc.: 68.75%] [G loss: 0.481129]\n",
      "epoch:6 step:6466 [D loss: 0.187770, acc.: 75.00%] [G loss: 0.557098]\n",
      "epoch:6 step:6467 [D loss: 0.203096, acc.: 69.53%] [G loss: 0.520860]\n",
      "epoch:6 step:6468 [D loss: 0.234012, acc.: 60.94%] [G loss: 0.484675]\n",
      "epoch:6 step:6469 [D loss: 0.242712, acc.: 60.16%] [G loss: 0.450184]\n",
      "epoch:6 step:6470 [D loss: 0.210502, acc.: 66.41%] [G loss: 0.513165]\n",
      "epoch:6 step:6471 [D loss: 0.199705, acc.: 66.41%] [G loss: 0.485208]\n",
      "epoch:6 step:6472 [D loss: 0.225720, acc.: 60.94%] [G loss: 0.482230]\n",
      "epoch:6 step:6473 [D loss: 0.246703, acc.: 61.72%] [G loss: 0.507807]\n",
      "epoch:6 step:6474 [D loss: 0.187531, acc.: 73.44%] [G loss: 0.531566]\n",
      "epoch:6 step:6475 [D loss: 0.203634, acc.: 67.19%] [G loss: 0.559157]\n",
      "epoch:6 step:6476 [D loss: 0.207821, acc.: 71.88%] [G loss: 0.539186]\n",
      "epoch:6 step:6477 [D loss: 0.206454, acc.: 66.41%] [G loss: 0.464356]\n",
      "epoch:6 step:6478 [D loss: 0.214845, acc.: 66.41%] [G loss: 0.505500]\n",
      "epoch:6 step:6479 [D loss: 0.219536, acc.: 66.41%] [G loss: 0.519485]\n",
      "epoch:6 step:6480 [D loss: 0.265666, acc.: 50.78%] [G loss: 0.482478]\n",
      "epoch:6 step:6481 [D loss: 0.217024, acc.: 64.84%] [G loss: 0.470211]\n",
      "epoch:6 step:6482 [D loss: 0.193141, acc.: 68.75%] [G loss: 0.513225]\n",
      "epoch:6 step:6483 [D loss: 0.237076, acc.: 64.06%] [G loss: 0.498365]\n",
      "epoch:6 step:6484 [D loss: 0.226009, acc.: 66.41%] [G loss: 0.486781]\n",
      "epoch:6 step:6485 [D loss: 0.209593, acc.: 67.97%] [G loss: 0.424900]\n",
      "epoch:6 step:6486 [D loss: 0.247383, acc.: 56.25%] [G loss: 0.484925]\n",
      "epoch:6 step:6487 [D loss: 0.233950, acc.: 58.59%] [G loss: 0.467880]\n",
      "epoch:6 step:6488 [D loss: 0.231672, acc.: 66.41%] [G loss: 0.439135]\n",
      "epoch:6 step:6489 [D loss: 0.262744, acc.: 53.12%] [G loss: 0.436465]\n",
      "epoch:6 step:6490 [D loss: 0.199489, acc.: 72.66%] [G loss: 0.508700]\n",
      "epoch:6 step:6491 [D loss: 0.210362, acc.: 69.53%] [G loss: 0.443673]\n",
      "epoch:6 step:6492 [D loss: 0.199797, acc.: 67.97%] [G loss: 0.465620]\n",
      "epoch:6 step:6493 [D loss: 0.223230, acc.: 64.84%] [G loss: 0.485198]\n",
      "epoch:6 step:6494 [D loss: 0.210647, acc.: 67.97%] [G loss: 0.525751]\n",
      "epoch:6 step:6495 [D loss: 0.214652, acc.: 66.41%] [G loss: 0.477508]\n",
      "epoch:6 step:6496 [D loss: 0.195953, acc.: 67.97%] [G loss: 0.453178]\n",
      "epoch:6 step:6497 [D loss: 0.199979, acc.: 67.97%] [G loss: 0.541219]\n",
      "epoch:6 step:6498 [D loss: 0.241902, acc.: 57.03%] [G loss: 0.502429]\n",
      "epoch:6 step:6499 [D loss: 0.235351, acc.: 57.03%] [G loss: 0.480142]\n",
      "epoch:6 step:6500 [D loss: 0.234667, acc.: 59.38%] [G loss: 0.477617]\n",
      "epoch:6 step:6501 [D loss: 0.204538, acc.: 70.31%] [G loss: 0.464912]\n",
      "epoch:6 step:6502 [D loss: 0.240586, acc.: 60.94%] [G loss: 0.476437]\n",
      "epoch:6 step:6503 [D loss: 0.213344, acc.: 64.84%] [G loss: 0.497534]\n",
      "epoch:6 step:6504 [D loss: 0.186777, acc.: 71.09%] [G loss: 0.492659]\n",
      "epoch:6 step:6505 [D loss: 0.218649, acc.: 68.75%] [G loss: 0.538427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6506 [D loss: 0.154598, acc.: 80.47%] [G loss: 0.556952]\n",
      "epoch:6 step:6507 [D loss: 0.214267, acc.: 67.19%] [G loss: 0.537114]\n",
      "epoch:6 step:6508 [D loss: 0.202629, acc.: 67.97%] [G loss: 0.525840]\n",
      "epoch:6 step:6509 [D loss: 0.242017, acc.: 60.16%] [G loss: 0.507725]\n",
      "epoch:6 step:6510 [D loss: 0.241613, acc.: 66.41%] [G loss: 0.495383]\n",
      "epoch:6 step:6511 [D loss: 0.201924, acc.: 67.97%] [G loss: 0.512607]\n",
      "epoch:6 step:6512 [D loss: 0.181996, acc.: 76.56%] [G loss: 0.544212]\n",
      "epoch:6 step:6513 [D loss: 0.266096, acc.: 55.47%] [G loss: 0.468463]\n",
      "epoch:6 step:6514 [D loss: 0.246156, acc.: 56.25%] [G loss: 0.485565]\n",
      "epoch:6 step:6515 [D loss: 0.221957, acc.: 62.50%] [G loss: 0.492949]\n",
      "epoch:6 step:6516 [D loss: 0.168559, acc.: 78.91%] [G loss: 0.537128]\n",
      "epoch:6 step:6517 [D loss: 0.227607, acc.: 58.59%] [G loss: 0.555607]\n",
      "epoch:6 step:6518 [D loss: 0.205486, acc.: 64.84%] [G loss: 0.540712]\n",
      "epoch:6 step:6519 [D loss: 0.191650, acc.: 73.44%] [G loss: 0.536680]\n",
      "epoch:6 step:6520 [D loss: 0.199141, acc.: 69.53%] [G loss: 0.534193]\n",
      "epoch:6 step:6521 [D loss: 0.172910, acc.: 76.56%] [G loss: 0.560009]\n",
      "epoch:6 step:6522 [D loss: 0.180444, acc.: 73.44%] [G loss: 0.542670]\n",
      "epoch:6 step:6523 [D loss: 0.217594, acc.: 64.06%] [G loss: 0.512750]\n",
      "epoch:6 step:6524 [D loss: 0.259088, acc.: 51.56%] [G loss: 0.473534]\n",
      "epoch:6 step:6525 [D loss: 0.193649, acc.: 70.31%] [G loss: 0.495171]\n",
      "epoch:6 step:6526 [D loss: 0.220519, acc.: 67.19%] [G loss: 0.513164]\n",
      "epoch:6 step:6527 [D loss: 0.193552, acc.: 71.88%] [G loss: 0.500702]\n",
      "epoch:6 step:6528 [D loss: 0.175626, acc.: 75.00%] [G loss: 0.548475]\n",
      "epoch:6 step:6529 [D loss: 0.223697, acc.: 60.16%] [G loss: 0.526993]\n",
      "epoch:6 step:6530 [D loss: 0.225091, acc.: 63.28%] [G loss: 0.485910]\n",
      "epoch:6 step:6531 [D loss: 0.200008, acc.: 65.62%] [G loss: 0.506653]\n",
      "epoch:6 step:6532 [D loss: 0.200581, acc.: 65.62%] [G loss: 0.497755]\n",
      "epoch:6 step:6533 [D loss: 0.185395, acc.: 71.09%] [G loss: 0.532336]\n",
      "epoch:6 step:6534 [D loss: 0.168846, acc.: 73.44%] [G loss: 0.564830]\n",
      "epoch:6 step:6535 [D loss: 0.210212, acc.: 67.97%] [G loss: 0.532927]\n",
      "epoch:6 step:6536 [D loss: 0.178389, acc.: 78.12%] [G loss: 0.534248]\n",
      "epoch:6 step:6537 [D loss: 0.243747, acc.: 57.81%] [G loss: 0.464503]\n",
      "epoch:6 step:6538 [D loss: 0.207216, acc.: 63.28%] [G loss: 0.480252]\n",
      "epoch:6 step:6539 [D loss: 0.216543, acc.: 67.19%] [G loss: 0.481137]\n",
      "epoch:6 step:6540 [D loss: 0.216194, acc.: 67.97%] [G loss: 0.601393]\n",
      "epoch:6 step:6541 [D loss: 0.210976, acc.: 67.97%] [G loss: 0.579962]\n",
      "epoch:6 step:6542 [D loss: 0.310658, acc.: 50.78%] [G loss: 0.497325]\n",
      "epoch:6 step:6543 [D loss: 0.182013, acc.: 72.66%] [G loss: 0.528457]\n",
      "epoch:6 step:6544 [D loss: 0.217148, acc.: 64.84%] [G loss: 0.451269]\n",
      "epoch:6 step:6545 [D loss: 0.186803, acc.: 69.53%] [G loss: 0.472348]\n",
      "epoch:6 step:6546 [D loss: 0.176005, acc.: 72.66%] [G loss: 0.538312]\n",
      "epoch:6 step:6547 [D loss: 0.165599, acc.: 78.12%] [G loss: 0.596734]\n",
      "epoch:6 step:6548 [D loss: 0.181888, acc.: 70.31%] [G loss: 0.606229]\n",
      "epoch:6 step:6549 [D loss: 0.142705, acc.: 78.91%] [G loss: 0.693563]\n",
      "epoch:6 step:6550 [D loss: 0.375925, acc.: 57.03%] [G loss: 0.512516]\n",
      "epoch:6 step:6551 [D loss: 0.156637, acc.: 80.47%] [G loss: 0.677125]\n",
      "epoch:6 step:6552 [D loss: 0.180411, acc.: 71.88%] [G loss: 0.607592]\n",
      "epoch:6 step:6553 [D loss: 0.261633, acc.: 58.59%] [G loss: 0.527481]\n",
      "epoch:6 step:6554 [D loss: 0.209112, acc.: 67.19%] [G loss: 0.518904]\n",
      "epoch:6 step:6555 [D loss: 0.206136, acc.: 65.62%] [G loss: 0.535677]\n",
      "epoch:6 step:6556 [D loss: 0.223639, acc.: 59.38%] [G loss: 0.484332]\n",
      "epoch:6 step:6557 [D loss: 0.147059, acc.: 75.78%] [G loss: 0.576198]\n",
      "epoch:6 step:6558 [D loss: 0.174965, acc.: 75.00%] [G loss: 0.581882]\n",
      "epoch:6 step:6559 [D loss: 0.170312, acc.: 73.44%] [G loss: 0.590212]\n",
      "epoch:7 step:6560 [D loss: 0.248467, acc.: 63.28%] [G loss: 0.564961]\n",
      "epoch:7 step:6561 [D loss: 0.261755, acc.: 57.81%] [G loss: 0.495822]\n",
      "epoch:7 step:6562 [D loss: 0.206392, acc.: 68.75%] [G loss: 0.569927]\n",
      "epoch:7 step:6563 [D loss: 0.218461, acc.: 64.06%] [G loss: 0.491958]\n",
      "epoch:7 step:6564 [D loss: 0.203084, acc.: 69.53%] [G loss: 0.533936]\n",
      "epoch:7 step:6565 [D loss: 0.203686, acc.: 67.19%] [G loss: 0.510897]\n",
      "epoch:7 step:6566 [D loss: 0.222410, acc.: 64.84%] [G loss: 0.502084]\n",
      "epoch:7 step:6567 [D loss: 0.213069, acc.: 66.41%] [G loss: 0.570988]\n",
      "epoch:7 step:6568 [D loss: 0.208632, acc.: 66.41%] [G loss: 0.529844]\n",
      "epoch:7 step:6569 [D loss: 0.219786, acc.: 62.50%] [G loss: 0.521093]\n",
      "epoch:7 step:6570 [D loss: 0.229716, acc.: 61.72%] [G loss: 0.494336]\n",
      "epoch:7 step:6571 [D loss: 0.180679, acc.: 75.78%] [G loss: 0.524089]\n",
      "epoch:7 step:6572 [D loss: 0.188431, acc.: 68.75%] [G loss: 0.546614]\n",
      "epoch:7 step:6573 [D loss: 0.186966, acc.: 75.78%] [G loss: 0.521502]\n",
      "epoch:7 step:6574 [D loss: 0.168319, acc.: 79.69%] [G loss: 0.532953]\n",
      "epoch:7 step:6575 [D loss: 0.200066, acc.: 65.62%] [G loss: 0.553729]\n",
      "epoch:7 step:6576 [D loss: 0.234208, acc.: 57.03%] [G loss: 0.531804]\n",
      "epoch:7 step:6577 [D loss: 0.226133, acc.: 62.50%] [G loss: 0.468196]\n",
      "epoch:7 step:6578 [D loss: 0.210750, acc.: 64.84%] [G loss: 0.472196]\n",
      "epoch:7 step:6579 [D loss: 0.217797, acc.: 68.75%] [G loss: 0.546157]\n",
      "epoch:7 step:6580 [D loss: 0.226685, acc.: 62.50%] [G loss: 0.520842]\n",
      "epoch:7 step:6581 [D loss: 0.204764, acc.: 69.53%] [G loss: 0.559002]\n",
      "epoch:7 step:6582 [D loss: 0.213763, acc.: 67.19%] [G loss: 0.495799]\n",
      "epoch:7 step:6583 [D loss: 0.206521, acc.: 67.19%] [G loss: 0.481791]\n",
      "epoch:7 step:6584 [D loss: 0.177863, acc.: 70.31%] [G loss: 0.498170]\n",
      "epoch:7 step:6585 [D loss: 0.225105, acc.: 62.50%] [G loss: 0.525910]\n",
      "epoch:7 step:6586 [D loss: 0.221098, acc.: 65.62%] [G loss: 0.458905]\n",
      "epoch:7 step:6587 [D loss: 0.177094, acc.: 71.09%] [G loss: 0.548307]\n",
      "epoch:7 step:6588 [D loss: 0.199052, acc.: 71.09%] [G loss: 0.510186]\n",
      "epoch:7 step:6589 [D loss: 0.191621, acc.: 72.66%] [G loss: 0.488602]\n",
      "epoch:7 step:6590 [D loss: 0.208274, acc.: 64.06%] [G loss: 0.470468]\n",
      "epoch:7 step:6591 [D loss: 0.244297, acc.: 64.06%] [G loss: 0.498559]\n",
      "epoch:7 step:6592 [D loss: 0.202096, acc.: 68.75%] [G loss: 0.466027]\n",
      "epoch:7 step:6593 [D loss: 0.199237, acc.: 68.75%] [G loss: 0.567968]\n",
      "epoch:7 step:6594 [D loss: 0.234816, acc.: 53.12%] [G loss: 0.513051]\n",
      "epoch:7 step:6595 [D loss: 0.176729, acc.: 74.22%] [G loss: 0.562556]\n",
      "epoch:7 step:6596 [D loss: 0.235044, acc.: 64.06%] [G loss: 0.498220]\n",
      "epoch:7 step:6597 [D loss: 0.220043, acc.: 63.28%] [G loss: 0.489374]\n",
      "epoch:7 step:6598 [D loss: 0.233806, acc.: 56.25%] [G loss: 0.509287]\n",
      "epoch:7 step:6599 [D loss: 0.143325, acc.: 80.47%] [G loss: 0.635286]\n",
      "epoch:7 step:6600 [D loss: 0.229995, acc.: 63.28%] [G loss: 0.534424]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.218185\n",
      "FID: 17.995426\n",
      "0 = 12.214109809398607\n",
      "1 = 0.05559870580635226\n",
      "2 = 0.9341999888420105\n",
      "3 = 0.9057999849319458\n",
      "4 = 0.9625999927520752\n",
      "5 = 0.9603477716445923\n",
      "6 = 0.9057999849319458\n",
      "7 = 7.346909900182503\n",
      "8 = 0.09322592163121401\n",
      "9 = 0.7458999752998352\n",
      "10 = 0.7379000186920166\n",
      "11 = 0.7538999915122986\n",
      "12 = 0.749898374080658\n",
      "13 = 0.7379000186920166\n",
      "14 = 7.21824836730957\n",
      "15 = 9.448565483093262\n",
      "16 = 0.14100299775600433\n",
      "17 = 7.2181854248046875\n",
      "18 = 17.995426177978516\n",
      "epoch:7 step:6601 [D loss: 0.174063, acc.: 74.22%] [G loss: 0.528444]\n",
      "epoch:7 step:6602 [D loss: 0.228071, acc.: 64.84%] [G loss: 0.491232]\n",
      "epoch:7 step:6603 [D loss: 0.223976, acc.: 65.62%] [G loss: 0.487370]\n",
      "epoch:7 step:6604 [D loss: 0.192575, acc.: 71.88%] [G loss: 0.491346]\n",
      "epoch:7 step:6605 [D loss: 0.210712, acc.: 66.41%] [G loss: 0.466529]\n",
      "epoch:7 step:6606 [D loss: 0.230285, acc.: 61.72%] [G loss: 0.467854]\n",
      "epoch:7 step:6607 [D loss: 0.209690, acc.: 66.41%] [G loss: 0.468237]\n",
      "epoch:7 step:6608 [D loss: 0.212286, acc.: 65.62%] [G loss: 0.531000]\n",
      "epoch:7 step:6609 [D loss: 0.208244, acc.: 65.62%] [G loss: 0.528557]\n",
      "epoch:7 step:6610 [D loss: 0.229798, acc.: 59.38%] [G loss: 0.511717]\n",
      "epoch:7 step:6611 [D loss: 0.208534, acc.: 67.97%] [G loss: 0.501140]\n",
      "epoch:7 step:6612 [D loss: 0.181421, acc.: 75.00%] [G loss: 0.551549]\n",
      "epoch:7 step:6613 [D loss: 0.195939, acc.: 71.88%] [G loss: 0.514447]\n",
      "epoch:7 step:6614 [D loss: 0.203651, acc.: 67.97%] [G loss: 0.507947]\n",
      "epoch:7 step:6615 [D loss: 0.204827, acc.: 67.19%] [G loss: 0.475759]\n",
      "epoch:7 step:6616 [D loss: 0.211342, acc.: 63.28%] [G loss: 0.518875]\n",
      "epoch:7 step:6617 [D loss: 0.212420, acc.: 70.31%] [G loss: 0.510857]\n",
      "epoch:7 step:6618 [D loss: 0.214386, acc.: 65.62%] [G loss: 0.517824]\n",
      "epoch:7 step:6619 [D loss: 0.235247, acc.: 63.28%] [G loss: 0.489357]\n",
      "epoch:7 step:6620 [D loss: 0.231631, acc.: 65.62%] [G loss: 0.452984]\n",
      "epoch:7 step:6621 [D loss: 0.207810, acc.: 63.28%] [G loss: 0.464642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6622 [D loss: 0.210130, acc.: 67.19%] [G loss: 0.520478]\n",
      "epoch:7 step:6623 [D loss: 0.204284, acc.: 67.19%] [G loss: 0.532779]\n",
      "epoch:7 step:6624 [D loss: 0.217586, acc.: 61.72%] [G loss: 0.473461]\n",
      "epoch:7 step:6625 [D loss: 0.230716, acc.: 66.41%] [G loss: 0.497929]\n",
      "epoch:7 step:6626 [D loss: 0.212485, acc.: 68.75%] [G loss: 0.481631]\n",
      "epoch:7 step:6627 [D loss: 0.226471, acc.: 63.28%] [G loss: 0.486132]\n",
      "epoch:7 step:6628 [D loss: 0.225420, acc.: 62.50%] [G loss: 0.509689]\n",
      "epoch:7 step:6629 [D loss: 0.195439, acc.: 72.66%] [G loss: 0.628542]\n",
      "epoch:7 step:6630 [D loss: 0.236469, acc.: 64.06%] [G loss: 0.528078]\n",
      "epoch:7 step:6631 [D loss: 0.214815, acc.: 67.19%] [G loss: 0.501175]\n",
      "epoch:7 step:6632 [D loss: 0.192723, acc.: 71.09%] [G loss: 0.500427]\n",
      "epoch:7 step:6633 [D loss: 0.192727, acc.: 69.53%] [G loss: 0.501987]\n",
      "epoch:7 step:6634 [D loss: 0.214330, acc.: 62.50%] [G loss: 0.564640]\n",
      "epoch:7 step:6635 [D loss: 0.200031, acc.: 66.41%] [G loss: 0.561865]\n",
      "epoch:7 step:6636 [D loss: 0.196872, acc.: 72.66%] [G loss: 0.541268]\n",
      "epoch:7 step:6637 [D loss: 0.263369, acc.: 57.03%] [G loss: 0.496758]\n",
      "epoch:7 step:6638 [D loss: 0.196812, acc.: 71.88%] [G loss: 0.479989]\n",
      "epoch:7 step:6639 [D loss: 0.197740, acc.: 70.31%] [G loss: 0.528555]\n",
      "epoch:7 step:6640 [D loss: 0.211264, acc.: 64.06%] [G loss: 0.499652]\n",
      "epoch:7 step:6641 [D loss: 0.218863, acc.: 63.28%] [G loss: 0.507760]\n",
      "epoch:7 step:6642 [D loss: 0.193760, acc.: 76.56%] [G loss: 0.504035]\n",
      "epoch:7 step:6643 [D loss: 0.229620, acc.: 60.94%] [G loss: 0.509956]\n",
      "epoch:7 step:6644 [D loss: 0.229799, acc.: 63.28%] [G loss: 0.522761]\n",
      "epoch:7 step:6645 [D loss: 0.188001, acc.: 68.75%] [G loss: 0.529160]\n",
      "epoch:7 step:6646 [D loss: 0.203080, acc.: 67.19%] [G loss: 0.522174]\n",
      "epoch:7 step:6647 [D loss: 0.192717, acc.: 71.09%] [G loss: 0.481459]\n",
      "epoch:7 step:6648 [D loss: 0.188570, acc.: 74.22%] [G loss: 0.501792]\n",
      "epoch:7 step:6649 [D loss: 0.227836, acc.: 63.28%] [G loss: 0.502313]\n",
      "epoch:7 step:6650 [D loss: 0.248334, acc.: 58.59%] [G loss: 0.506547]\n",
      "epoch:7 step:6651 [D loss: 0.202304, acc.: 67.97%] [G loss: 0.519656]\n",
      "epoch:7 step:6652 [D loss: 0.229471, acc.: 64.84%] [G loss: 0.537427]\n",
      "epoch:7 step:6653 [D loss: 0.208555, acc.: 67.19%] [G loss: 0.517258]\n",
      "epoch:7 step:6654 [D loss: 0.200783, acc.: 70.31%] [G loss: 0.544312]\n",
      "epoch:7 step:6655 [D loss: 0.187465, acc.: 69.53%] [G loss: 0.515941]\n",
      "epoch:7 step:6656 [D loss: 0.212990, acc.: 64.84%] [G loss: 0.522071]\n",
      "epoch:7 step:6657 [D loss: 0.237109, acc.: 59.38%] [G loss: 0.500610]\n",
      "epoch:7 step:6658 [D loss: 0.231849, acc.: 61.72%] [G loss: 0.454161]\n",
      "epoch:7 step:6659 [D loss: 0.200165, acc.: 67.97%] [G loss: 0.550832]\n",
      "epoch:7 step:6660 [D loss: 0.208377, acc.: 67.97%] [G loss: 0.530670]\n",
      "epoch:7 step:6661 [D loss: 0.250248, acc.: 59.38%] [G loss: 0.465434]\n",
      "epoch:7 step:6662 [D loss: 0.203456, acc.: 67.19%] [G loss: 0.469807]\n",
      "epoch:7 step:6663 [D loss: 0.217528, acc.: 67.97%] [G loss: 0.493426]\n",
      "epoch:7 step:6664 [D loss: 0.222090, acc.: 65.62%] [G loss: 0.440575]\n",
      "epoch:7 step:6665 [D loss: 0.203750, acc.: 68.75%] [G loss: 0.551698]\n",
      "epoch:7 step:6666 [D loss: 0.206950, acc.: 69.53%] [G loss: 0.604346]\n",
      "epoch:7 step:6667 [D loss: 0.272556, acc.: 54.69%] [G loss: 0.496985]\n",
      "epoch:7 step:6668 [D loss: 0.240184, acc.: 60.94%] [G loss: 0.480867]\n",
      "epoch:7 step:6669 [D loss: 0.232483, acc.: 53.91%] [G loss: 0.447448]\n",
      "epoch:7 step:6670 [D loss: 0.200304, acc.: 70.31%] [G loss: 0.536954]\n",
      "epoch:7 step:6671 [D loss: 0.199071, acc.: 71.09%] [G loss: 0.507663]\n",
      "epoch:7 step:6672 [D loss: 0.233783, acc.: 60.16%] [G loss: 0.527040]\n",
      "epoch:7 step:6673 [D loss: 0.225317, acc.: 62.50%] [G loss: 0.533667]\n",
      "epoch:7 step:6674 [D loss: 0.228204, acc.: 60.94%] [G loss: 0.497539]\n",
      "epoch:7 step:6675 [D loss: 0.193811, acc.: 74.22%] [G loss: 0.503680]\n",
      "epoch:7 step:6676 [D loss: 0.190865, acc.: 69.53%] [G loss: 0.504930]\n",
      "epoch:7 step:6677 [D loss: 0.194918, acc.: 71.09%] [G loss: 0.513510]\n",
      "epoch:7 step:6678 [D loss: 0.142660, acc.: 80.47%] [G loss: 0.552991]\n",
      "epoch:7 step:6679 [D loss: 0.247081, acc.: 60.94%] [G loss: 0.533007]\n",
      "epoch:7 step:6680 [D loss: 0.204937, acc.: 72.66%] [G loss: 0.530846]\n",
      "epoch:7 step:6681 [D loss: 0.199108, acc.: 69.53%] [G loss: 0.586455]\n",
      "epoch:7 step:6682 [D loss: 0.177913, acc.: 71.88%] [G loss: 0.612476]\n",
      "epoch:7 step:6683 [D loss: 0.217946, acc.: 64.84%] [G loss: 0.479060]\n",
      "epoch:7 step:6684 [D loss: 0.206961, acc.: 66.41%] [G loss: 0.493230]\n",
      "epoch:7 step:6685 [D loss: 0.181781, acc.: 73.44%] [G loss: 0.494041]\n",
      "epoch:7 step:6686 [D loss: 0.206478, acc.: 66.41%] [G loss: 0.468910]\n",
      "epoch:7 step:6687 [D loss: 0.228437, acc.: 65.62%] [G loss: 0.526164]\n",
      "epoch:7 step:6688 [D loss: 0.223637, acc.: 63.28%] [G loss: 0.499866]\n",
      "epoch:7 step:6689 [D loss: 0.204241, acc.: 65.62%] [G loss: 0.497788]\n",
      "epoch:7 step:6690 [D loss: 0.200875, acc.: 67.19%] [G loss: 0.528186]\n",
      "epoch:7 step:6691 [D loss: 0.226017, acc.: 62.50%] [G loss: 0.539765]\n",
      "epoch:7 step:6692 [D loss: 0.220996, acc.: 66.41%] [G loss: 0.506171]\n",
      "epoch:7 step:6693 [D loss: 0.196605, acc.: 71.88%] [G loss: 0.539457]\n",
      "epoch:7 step:6694 [D loss: 0.191298, acc.: 72.66%] [G loss: 0.501452]\n",
      "epoch:7 step:6695 [D loss: 0.221336, acc.: 64.84%] [G loss: 0.534325]\n",
      "epoch:7 step:6696 [D loss: 0.254895, acc.: 60.16%] [G loss: 0.499858]\n",
      "epoch:7 step:6697 [D loss: 0.220603, acc.: 69.53%] [G loss: 0.480995]\n",
      "epoch:7 step:6698 [D loss: 0.219491, acc.: 67.19%] [G loss: 0.493257]\n",
      "epoch:7 step:6699 [D loss: 0.196745, acc.: 67.97%] [G loss: 0.460639]\n",
      "epoch:7 step:6700 [D loss: 0.206345, acc.: 70.31%] [G loss: 0.481518]\n",
      "epoch:7 step:6701 [D loss: 0.218586, acc.: 67.19%] [G loss: 0.481035]\n",
      "epoch:7 step:6702 [D loss: 0.241205, acc.: 55.47%] [G loss: 0.481594]\n",
      "epoch:7 step:6703 [D loss: 0.192331, acc.: 77.34%] [G loss: 0.529236]\n",
      "epoch:7 step:6704 [D loss: 0.223916, acc.: 62.50%] [G loss: 0.506381]\n",
      "epoch:7 step:6705 [D loss: 0.222669, acc.: 64.06%] [G loss: 0.508788]\n",
      "epoch:7 step:6706 [D loss: 0.232391, acc.: 63.28%] [G loss: 0.477642]\n",
      "epoch:7 step:6707 [D loss: 0.237430, acc.: 60.16%] [G loss: 0.484684]\n",
      "epoch:7 step:6708 [D loss: 0.181810, acc.: 73.44%] [G loss: 0.503831]\n",
      "epoch:7 step:6709 [D loss: 0.218903, acc.: 68.75%] [G loss: 0.475440]\n",
      "epoch:7 step:6710 [D loss: 0.172571, acc.: 76.56%] [G loss: 0.512956]\n",
      "epoch:7 step:6711 [D loss: 0.206482, acc.: 67.97%] [G loss: 0.547189]\n",
      "epoch:7 step:6712 [D loss: 0.230602, acc.: 65.62%] [G loss: 0.523342]\n",
      "epoch:7 step:6713 [D loss: 0.196824, acc.: 69.53%] [G loss: 0.478838]\n",
      "epoch:7 step:6714 [D loss: 0.160466, acc.: 75.78%] [G loss: 0.546867]\n",
      "epoch:7 step:6715 [D loss: 0.208211, acc.: 63.28%] [G loss: 0.547363]\n",
      "epoch:7 step:6716 [D loss: 0.228918, acc.: 60.16%] [G loss: 0.483527]\n",
      "epoch:7 step:6717 [D loss: 0.244984, acc.: 57.81%] [G loss: 0.471105]\n",
      "epoch:7 step:6718 [D loss: 0.212161, acc.: 67.97%] [G loss: 0.542858]\n",
      "epoch:7 step:6719 [D loss: 0.265921, acc.: 52.34%] [G loss: 0.463563]\n",
      "epoch:7 step:6720 [D loss: 0.190336, acc.: 70.31%] [G loss: 0.509289]\n",
      "epoch:7 step:6721 [D loss: 0.210187, acc.: 66.41%] [G loss: 0.517074]\n",
      "epoch:7 step:6722 [D loss: 0.222698, acc.: 65.62%] [G loss: 0.519536]\n",
      "epoch:7 step:6723 [D loss: 0.223814, acc.: 68.75%] [G loss: 0.477186]\n",
      "epoch:7 step:6724 [D loss: 0.213977, acc.: 67.97%] [G loss: 0.483109]\n",
      "epoch:7 step:6725 [D loss: 0.196516, acc.: 68.75%] [G loss: 0.491101]\n",
      "epoch:7 step:6726 [D loss: 0.202456, acc.: 71.09%] [G loss: 0.492988]\n",
      "epoch:7 step:6727 [D loss: 0.198615, acc.: 69.53%] [G loss: 0.528422]\n",
      "epoch:7 step:6728 [D loss: 0.243978, acc.: 57.81%] [G loss: 0.538258]\n",
      "epoch:7 step:6729 [D loss: 0.223081, acc.: 61.72%] [G loss: 0.542771]\n",
      "epoch:7 step:6730 [D loss: 0.193073, acc.: 65.62%] [G loss: 0.470606]\n",
      "epoch:7 step:6731 [D loss: 0.205682, acc.: 71.09%] [G loss: 0.488998]\n",
      "epoch:7 step:6732 [D loss: 0.205988, acc.: 67.97%] [G loss: 0.460026]\n",
      "epoch:7 step:6733 [D loss: 0.255127, acc.: 60.94%] [G loss: 0.431631]\n",
      "epoch:7 step:6734 [D loss: 0.217316, acc.: 64.06%] [G loss: 0.451256]\n",
      "epoch:7 step:6735 [D loss: 0.219081, acc.: 61.72%] [G loss: 0.472775]\n",
      "epoch:7 step:6736 [D loss: 0.220107, acc.: 64.06%] [G loss: 0.497264]\n",
      "epoch:7 step:6737 [D loss: 0.202144, acc.: 72.66%] [G loss: 0.520793]\n",
      "epoch:7 step:6738 [D loss: 0.226726, acc.: 63.28%] [G loss: 0.477718]\n",
      "epoch:7 step:6739 [D loss: 0.234583, acc.: 61.72%] [G loss: 0.527186]\n",
      "epoch:7 step:6740 [D loss: 0.252867, acc.: 53.12%] [G loss: 0.484226]\n",
      "epoch:7 step:6741 [D loss: 0.226283, acc.: 62.50%] [G loss: 0.543392]\n",
      "epoch:7 step:6742 [D loss: 0.229276, acc.: 64.84%] [G loss: 0.491779]\n",
      "epoch:7 step:6743 [D loss: 0.213916, acc.: 67.97%] [G loss: 0.493062]\n",
      "epoch:7 step:6744 [D loss: 0.204437, acc.: 74.22%] [G loss: 0.505652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6745 [D loss: 0.240670, acc.: 61.72%] [G loss: 0.486579]\n",
      "epoch:7 step:6746 [D loss: 0.257450, acc.: 53.12%] [G loss: 0.496073]\n",
      "epoch:7 step:6747 [D loss: 0.231510, acc.: 62.50%] [G loss: 0.493081]\n",
      "epoch:7 step:6748 [D loss: 0.229477, acc.: 61.72%] [G loss: 0.505980]\n",
      "epoch:7 step:6749 [D loss: 0.197029, acc.: 74.22%] [G loss: 0.456599]\n",
      "epoch:7 step:6750 [D loss: 0.186492, acc.: 75.00%] [G loss: 0.514170]\n",
      "epoch:7 step:6751 [D loss: 0.187106, acc.: 68.75%] [G loss: 0.509430]\n",
      "epoch:7 step:6752 [D loss: 0.215650, acc.: 64.84%] [G loss: 0.472323]\n",
      "epoch:7 step:6753 [D loss: 0.183197, acc.: 71.09%] [G loss: 0.529905]\n",
      "epoch:7 step:6754 [D loss: 0.209489, acc.: 70.31%] [G loss: 0.518654]\n",
      "epoch:7 step:6755 [D loss: 0.224699, acc.: 62.50%] [G loss: 0.515755]\n",
      "epoch:7 step:6756 [D loss: 0.183939, acc.: 70.31%] [G loss: 0.522579]\n",
      "epoch:7 step:6757 [D loss: 0.181778, acc.: 74.22%] [G loss: 0.511375]\n",
      "epoch:7 step:6758 [D loss: 0.194614, acc.: 70.31%] [G loss: 0.567539]\n",
      "epoch:7 step:6759 [D loss: 0.232734, acc.: 63.28%] [G loss: 0.472140]\n",
      "epoch:7 step:6760 [D loss: 0.203570, acc.: 69.53%] [G loss: 0.511403]\n",
      "epoch:7 step:6761 [D loss: 0.234535, acc.: 60.94%] [G loss: 0.505156]\n",
      "epoch:7 step:6762 [D loss: 0.267021, acc.: 51.56%] [G loss: 0.464672]\n",
      "epoch:7 step:6763 [D loss: 0.206460, acc.: 62.50%] [G loss: 0.520364]\n",
      "epoch:7 step:6764 [D loss: 0.185881, acc.: 73.44%] [G loss: 0.578638]\n",
      "epoch:7 step:6765 [D loss: 0.225182, acc.: 65.62%] [G loss: 0.513816]\n",
      "epoch:7 step:6766 [D loss: 0.218318, acc.: 61.72%] [G loss: 0.501162]\n",
      "epoch:7 step:6767 [D loss: 0.179192, acc.: 73.44%] [G loss: 0.547831]\n",
      "epoch:7 step:6768 [D loss: 0.180659, acc.: 71.88%] [G loss: 0.538767]\n",
      "epoch:7 step:6769 [D loss: 0.238644, acc.: 62.50%] [G loss: 0.516526]\n",
      "epoch:7 step:6770 [D loss: 0.239490, acc.: 53.91%] [G loss: 0.456409]\n",
      "epoch:7 step:6771 [D loss: 0.227117, acc.: 62.50%] [G loss: 0.459865]\n",
      "epoch:7 step:6772 [D loss: 0.217364, acc.: 63.28%] [G loss: 0.481457]\n",
      "epoch:7 step:6773 [D loss: 0.245717, acc.: 56.25%] [G loss: 0.457547]\n",
      "epoch:7 step:6774 [D loss: 0.228802, acc.: 58.59%] [G loss: 0.457136]\n",
      "epoch:7 step:6775 [D loss: 0.213717, acc.: 65.62%] [G loss: 0.481681]\n",
      "epoch:7 step:6776 [D loss: 0.203451, acc.: 65.62%] [G loss: 0.513929]\n",
      "epoch:7 step:6777 [D loss: 0.207283, acc.: 64.84%] [G loss: 0.472699]\n",
      "epoch:7 step:6778 [D loss: 0.181734, acc.: 71.09%] [G loss: 0.543648]\n",
      "epoch:7 step:6779 [D loss: 0.255762, acc.: 57.03%] [G loss: 0.468044]\n",
      "epoch:7 step:6780 [D loss: 0.184910, acc.: 77.34%] [G loss: 0.555372]\n",
      "epoch:7 step:6781 [D loss: 0.178877, acc.: 75.00%] [G loss: 0.560430]\n",
      "epoch:7 step:6782 [D loss: 0.203256, acc.: 67.97%] [G loss: 0.539521]\n",
      "epoch:7 step:6783 [D loss: 0.220692, acc.: 64.84%] [G loss: 0.507489]\n",
      "epoch:7 step:6784 [D loss: 0.233006, acc.: 59.38%] [G loss: 0.442971]\n",
      "epoch:7 step:6785 [D loss: 0.200479, acc.: 68.75%] [G loss: 0.485389]\n",
      "epoch:7 step:6786 [D loss: 0.227212, acc.: 58.59%] [G loss: 0.444227]\n",
      "epoch:7 step:6787 [D loss: 0.236369, acc.: 58.59%] [G loss: 0.451294]\n",
      "epoch:7 step:6788 [D loss: 0.198435, acc.: 70.31%] [G loss: 0.520841]\n",
      "epoch:7 step:6789 [D loss: 0.193058, acc.: 67.97%] [G loss: 0.558261]\n",
      "epoch:7 step:6790 [D loss: 0.182401, acc.: 72.66%] [G loss: 0.659218]\n",
      "epoch:7 step:6791 [D loss: 0.187351, acc.: 71.09%] [G loss: 0.595179]\n",
      "epoch:7 step:6792 [D loss: 0.299209, acc.: 57.03%] [G loss: 0.480915]\n",
      "epoch:7 step:6793 [D loss: 0.209150, acc.: 67.19%] [G loss: 0.525792]\n",
      "epoch:7 step:6794 [D loss: 0.231136, acc.: 64.06%] [G loss: 0.471880]\n",
      "epoch:7 step:6795 [D loss: 0.211663, acc.: 65.62%] [G loss: 0.498966]\n",
      "epoch:7 step:6796 [D loss: 0.219704, acc.: 66.41%] [G loss: 0.477968]\n",
      "epoch:7 step:6797 [D loss: 0.203520, acc.: 71.88%] [G loss: 0.445464]\n",
      "epoch:7 step:6798 [D loss: 0.200021, acc.: 72.66%] [G loss: 0.472678]\n",
      "epoch:7 step:6799 [D loss: 0.213886, acc.: 67.97%] [G loss: 0.497029]\n",
      "epoch:7 step:6800 [D loss: 0.217645, acc.: 64.84%] [G loss: 0.496183]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.062308\n",
      "FID: 20.430750\n",
      "0 = 12.252994880056415\n",
      "1 = 0.06156120957314668\n",
      "2 = 0.9327999949455261\n",
      "3 = 0.9111999869346619\n",
      "4 = 0.9544000029563904\n",
      "5 = 0.9523411393165588\n",
      "6 = 0.9111999869346619\n",
      "7 = 7.490482520788897\n",
      "8 = 0.09924909473073598\n",
      "9 = 0.7548499703407288\n",
      "10 = 0.7537999749183655\n",
      "11 = 0.7559000253677368\n",
      "12 = 0.7553862929344177\n",
      "13 = 0.7537999749183655\n",
      "14 = 7.0623698234558105\n",
      "15 = 9.377462387084961\n",
      "16 = 0.16530336439609528\n",
      "17 = 7.062308311462402\n",
      "18 = 20.430749893188477\n",
      "epoch:7 step:6801 [D loss: 0.213410, acc.: 69.53%] [G loss: 0.532001]\n",
      "epoch:7 step:6802 [D loss: 0.196890, acc.: 67.19%] [G loss: 0.472351]\n",
      "epoch:7 step:6803 [D loss: 0.223655, acc.: 66.41%] [G loss: 0.498313]\n",
      "epoch:7 step:6804 [D loss: 0.206119, acc.: 64.84%] [G loss: 0.489702]\n",
      "epoch:7 step:6805 [D loss: 0.216287, acc.: 63.28%] [G loss: 0.585236]\n",
      "epoch:7 step:6806 [D loss: 0.205355, acc.: 64.84%] [G loss: 0.575593]\n",
      "epoch:7 step:6807 [D loss: 0.237781, acc.: 62.50%] [G loss: 0.522090]\n",
      "epoch:7 step:6808 [D loss: 0.253503, acc.: 59.38%] [G loss: 0.487272]\n",
      "epoch:7 step:6809 [D loss: 0.269642, acc.: 53.12%] [G loss: 0.439284]\n",
      "epoch:7 step:6810 [D loss: 0.207541, acc.: 68.75%] [G loss: 0.491112]\n",
      "epoch:7 step:6811 [D loss: 0.239688, acc.: 66.41%] [G loss: 0.465850]\n",
      "epoch:7 step:6812 [D loss: 0.197445, acc.: 68.75%] [G loss: 0.514691]\n",
      "epoch:7 step:6813 [D loss: 0.225789, acc.: 61.72%] [G loss: 0.491358]\n",
      "epoch:7 step:6814 [D loss: 0.188434, acc.: 74.22%] [G loss: 0.498062]\n",
      "epoch:7 step:6815 [D loss: 0.218154, acc.: 66.41%] [G loss: 0.474829]\n",
      "epoch:7 step:6816 [D loss: 0.232609, acc.: 57.03%] [G loss: 0.485569]\n",
      "epoch:7 step:6817 [D loss: 0.193394, acc.: 74.22%] [G loss: 0.487537]\n",
      "epoch:7 step:6818 [D loss: 0.191318, acc.: 74.22%] [G loss: 0.497143]\n",
      "epoch:7 step:6819 [D loss: 0.224291, acc.: 64.84%] [G loss: 0.492012]\n",
      "epoch:7 step:6820 [D loss: 0.200825, acc.: 73.44%] [G loss: 0.518210]\n",
      "epoch:7 step:6821 [D loss: 0.206520, acc.: 67.19%] [G loss: 0.507806]\n",
      "epoch:7 step:6822 [D loss: 0.229985, acc.: 58.59%] [G loss: 0.471919]\n",
      "epoch:7 step:6823 [D loss: 0.185418, acc.: 71.88%] [G loss: 0.561552]\n",
      "epoch:7 step:6824 [D loss: 0.273152, acc.: 58.59%] [G loss: 0.460333]\n",
      "epoch:7 step:6825 [D loss: 0.212809, acc.: 64.06%] [G loss: 0.460384]\n",
      "epoch:7 step:6826 [D loss: 0.217678, acc.: 67.97%] [G loss: 0.512639]\n",
      "epoch:7 step:6827 [D loss: 0.218183, acc.: 64.06%] [G loss: 0.492097]\n",
      "epoch:7 step:6828 [D loss: 0.211727, acc.: 67.97%] [G loss: 0.478421]\n",
      "epoch:7 step:6829 [D loss: 0.219617, acc.: 65.62%] [G loss: 0.474988]\n",
      "epoch:7 step:6830 [D loss: 0.204675, acc.: 71.09%] [G loss: 0.504848]\n",
      "epoch:7 step:6831 [D loss: 0.224895, acc.: 60.94%] [G loss: 0.462700]\n",
      "epoch:7 step:6832 [D loss: 0.199898, acc.: 68.75%] [G loss: 0.477725]\n",
      "epoch:7 step:6833 [D loss: 0.186326, acc.: 72.66%] [G loss: 0.493305]\n",
      "epoch:7 step:6834 [D loss: 0.216990, acc.: 69.53%] [G loss: 0.500297]\n",
      "epoch:7 step:6835 [D loss: 0.216052, acc.: 65.62%] [G loss: 0.506816]\n",
      "epoch:7 step:6836 [D loss: 0.241084, acc.: 57.03%] [G loss: 0.499203]\n",
      "epoch:7 step:6837 [D loss: 0.233761, acc.: 60.16%] [G loss: 0.522689]\n",
      "epoch:7 step:6838 [D loss: 0.224550, acc.: 58.59%] [G loss: 0.539260]\n",
      "epoch:7 step:6839 [D loss: 0.211123, acc.: 68.75%] [G loss: 0.525993]\n",
      "epoch:7 step:6840 [D loss: 0.265487, acc.: 52.34%] [G loss: 0.465024]\n",
      "epoch:7 step:6841 [D loss: 0.198822, acc.: 65.62%] [G loss: 0.496580]\n",
      "epoch:7 step:6842 [D loss: 0.194604, acc.: 69.53%] [G loss: 0.504706]\n",
      "epoch:7 step:6843 [D loss: 0.241607, acc.: 57.03%] [G loss: 0.423623]\n",
      "epoch:7 step:6844 [D loss: 0.227957, acc.: 65.62%] [G loss: 0.479956]\n",
      "epoch:7 step:6845 [D loss: 0.177165, acc.: 75.00%] [G loss: 0.520166]\n",
      "epoch:7 step:6846 [D loss: 0.241817, acc.: 59.38%] [G loss: 0.468370]\n",
      "epoch:7 step:6847 [D loss: 0.225086, acc.: 63.28%] [G loss: 0.499671]\n",
      "epoch:7 step:6848 [D loss: 0.208359, acc.: 69.53%] [G loss: 0.496222]\n",
      "epoch:7 step:6849 [D loss: 0.219652, acc.: 65.62%] [G loss: 0.492117]\n",
      "epoch:7 step:6850 [D loss: 0.222042, acc.: 67.19%] [G loss: 0.497686]\n",
      "epoch:7 step:6851 [D loss: 0.227080, acc.: 61.72%] [G loss: 0.517898]\n",
      "epoch:7 step:6852 [D loss: 0.230682, acc.: 61.72%] [G loss: 0.502070]\n",
      "epoch:7 step:6853 [D loss: 0.239991, acc.: 58.59%] [G loss: 0.476761]\n",
      "epoch:7 step:6854 [D loss: 0.237909, acc.: 58.59%] [G loss: 0.445952]\n",
      "epoch:7 step:6855 [D loss: 0.190791, acc.: 73.44%] [G loss: 0.511655]\n",
      "epoch:7 step:6856 [D loss: 0.232015, acc.: 60.94%] [G loss: 0.467856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6857 [D loss: 0.194332, acc.: 68.75%] [G loss: 0.489183]\n",
      "epoch:7 step:6858 [D loss: 0.203674, acc.: 66.41%] [G loss: 0.555537]\n",
      "epoch:7 step:6859 [D loss: 0.190833, acc.: 69.53%] [G loss: 0.535939]\n",
      "epoch:7 step:6860 [D loss: 0.247003, acc.: 59.38%] [G loss: 0.519708]\n",
      "epoch:7 step:6861 [D loss: 0.208773, acc.: 70.31%] [G loss: 0.518683]\n",
      "epoch:7 step:6862 [D loss: 0.193227, acc.: 72.66%] [G loss: 0.526193]\n",
      "epoch:7 step:6863 [D loss: 0.196867, acc.: 69.53%] [G loss: 0.504301]\n",
      "epoch:7 step:6864 [D loss: 0.226864, acc.: 66.41%] [G loss: 0.561489]\n",
      "epoch:7 step:6865 [D loss: 0.218706, acc.: 67.97%] [G loss: 0.582166]\n",
      "epoch:7 step:6866 [D loss: 0.219709, acc.: 65.62%] [G loss: 0.539509]\n",
      "epoch:7 step:6867 [D loss: 0.229239, acc.: 60.16%] [G loss: 0.454071]\n",
      "epoch:7 step:6868 [D loss: 0.195979, acc.: 66.41%] [G loss: 0.494923]\n",
      "epoch:7 step:6869 [D loss: 0.211503, acc.: 69.53%] [G loss: 0.447165]\n",
      "epoch:7 step:6870 [D loss: 0.217692, acc.: 64.06%] [G loss: 0.530507]\n",
      "epoch:7 step:6871 [D loss: 0.149634, acc.: 78.12%] [G loss: 0.595868]\n",
      "epoch:7 step:6872 [D loss: 0.197800, acc.: 70.31%] [G loss: 0.542327]\n",
      "epoch:7 step:6873 [D loss: 0.184358, acc.: 73.44%] [G loss: 0.573474]\n",
      "epoch:7 step:6874 [D loss: 0.191569, acc.: 73.44%] [G loss: 0.572165]\n",
      "epoch:7 step:6875 [D loss: 0.251581, acc.: 61.72%] [G loss: 0.492063]\n",
      "epoch:7 step:6876 [D loss: 0.222254, acc.: 64.06%] [G loss: 0.482891]\n",
      "epoch:7 step:6877 [D loss: 0.219823, acc.: 64.84%] [G loss: 0.491698]\n",
      "epoch:7 step:6878 [D loss: 0.191581, acc.: 73.44%] [G loss: 0.531233]\n",
      "epoch:7 step:6879 [D loss: 0.210968, acc.: 66.41%] [G loss: 0.501745]\n",
      "epoch:7 step:6880 [D loss: 0.184139, acc.: 68.75%] [G loss: 0.535024]\n",
      "epoch:7 step:6881 [D loss: 0.230708, acc.: 64.06%] [G loss: 0.484239]\n",
      "epoch:7 step:6882 [D loss: 0.279076, acc.: 50.78%] [G loss: 0.455308]\n",
      "epoch:7 step:6883 [D loss: 0.226704, acc.: 57.81%] [G loss: 0.438417]\n",
      "epoch:7 step:6884 [D loss: 0.194633, acc.: 69.53%] [G loss: 0.501802]\n",
      "epoch:7 step:6885 [D loss: 0.195657, acc.: 70.31%] [G loss: 0.488173]\n",
      "epoch:7 step:6886 [D loss: 0.213958, acc.: 67.97%] [G loss: 0.510377]\n",
      "epoch:7 step:6887 [D loss: 0.187839, acc.: 72.66%] [G loss: 0.532702]\n",
      "epoch:7 step:6888 [D loss: 0.226682, acc.: 59.38%] [G loss: 0.479084]\n",
      "epoch:7 step:6889 [D loss: 0.212065, acc.: 66.41%] [G loss: 0.516533]\n",
      "epoch:7 step:6890 [D loss: 0.212149, acc.: 61.72%] [G loss: 0.494572]\n",
      "epoch:7 step:6891 [D loss: 0.214967, acc.: 63.28%] [G loss: 0.514187]\n",
      "epoch:7 step:6892 [D loss: 0.184595, acc.: 71.09%] [G loss: 0.511144]\n",
      "epoch:7 step:6893 [D loss: 0.196885, acc.: 71.09%] [G loss: 0.495913]\n",
      "epoch:7 step:6894 [D loss: 0.182451, acc.: 68.75%] [G loss: 0.550436]\n",
      "epoch:7 step:6895 [D loss: 0.196600, acc.: 67.19%] [G loss: 0.480312]\n",
      "epoch:7 step:6896 [D loss: 0.207009, acc.: 69.53%] [G loss: 0.492835]\n",
      "epoch:7 step:6897 [D loss: 0.209338, acc.: 64.84%] [G loss: 0.485328]\n",
      "epoch:7 step:6898 [D loss: 0.185720, acc.: 72.66%] [G loss: 0.487889]\n",
      "epoch:7 step:6899 [D loss: 0.212330, acc.: 64.84%] [G loss: 0.526417]\n",
      "epoch:7 step:6900 [D loss: 0.243330, acc.: 64.06%] [G loss: 0.507703]\n",
      "epoch:7 step:6901 [D loss: 0.246842, acc.: 56.25%] [G loss: 0.433120]\n",
      "epoch:7 step:6902 [D loss: 0.209112, acc.: 71.09%] [G loss: 0.494923]\n",
      "epoch:7 step:6903 [D loss: 0.174540, acc.: 74.22%] [G loss: 0.529266]\n",
      "epoch:7 step:6904 [D loss: 0.191810, acc.: 72.66%] [G loss: 0.533861]\n",
      "epoch:7 step:6905 [D loss: 0.180193, acc.: 68.75%] [G loss: 0.564263]\n",
      "epoch:7 step:6906 [D loss: 0.203319, acc.: 66.41%] [G loss: 0.548943]\n",
      "epoch:7 step:6907 [D loss: 0.281256, acc.: 58.59%] [G loss: 0.504372]\n",
      "epoch:7 step:6908 [D loss: 0.260777, acc.: 54.69%] [G loss: 0.465214]\n",
      "epoch:7 step:6909 [D loss: 0.211687, acc.: 70.31%] [G loss: 0.517071]\n",
      "epoch:7 step:6910 [D loss: 0.215328, acc.: 65.62%] [G loss: 0.547671]\n",
      "epoch:7 step:6911 [D loss: 0.209241, acc.: 65.62%] [G loss: 0.524589]\n",
      "epoch:7 step:6912 [D loss: 0.241209, acc.: 60.16%] [G loss: 0.528483]\n",
      "epoch:7 step:6913 [D loss: 0.164577, acc.: 76.56%] [G loss: 0.597042]\n",
      "epoch:7 step:6914 [D loss: 0.243168, acc.: 59.38%] [G loss: 0.575804]\n",
      "epoch:7 step:6915 [D loss: 0.240813, acc.: 56.25%] [G loss: 0.499676]\n",
      "epoch:7 step:6916 [D loss: 0.220449, acc.: 62.50%] [G loss: 0.471751]\n",
      "epoch:7 step:6917 [D loss: 0.193067, acc.: 71.88%] [G loss: 0.475609]\n",
      "epoch:7 step:6918 [D loss: 0.166676, acc.: 78.12%] [G loss: 0.492340]\n",
      "epoch:7 step:6919 [D loss: 0.201728, acc.: 66.41%] [G loss: 0.525692]\n",
      "epoch:7 step:6920 [D loss: 0.209989, acc.: 67.97%] [G loss: 0.491460]\n",
      "epoch:7 step:6921 [D loss: 0.222093, acc.: 66.41%] [G loss: 0.440179]\n",
      "epoch:7 step:6922 [D loss: 0.228389, acc.: 64.84%] [G loss: 0.459950]\n",
      "epoch:7 step:6923 [D loss: 0.193004, acc.: 70.31%] [G loss: 0.511703]\n",
      "epoch:7 step:6924 [D loss: 0.185924, acc.: 70.31%] [G loss: 0.556057]\n",
      "epoch:7 step:6925 [D loss: 0.211157, acc.: 67.19%] [G loss: 0.530574]\n",
      "epoch:7 step:6926 [D loss: 0.197837, acc.: 69.53%] [G loss: 0.509306]\n",
      "epoch:7 step:6927 [D loss: 0.237632, acc.: 59.38%] [G loss: 0.486612]\n",
      "epoch:7 step:6928 [D loss: 0.226721, acc.: 61.72%] [G loss: 0.519032]\n",
      "epoch:7 step:6929 [D loss: 0.235510, acc.: 64.06%] [G loss: 0.449497]\n",
      "epoch:7 step:6930 [D loss: 0.180228, acc.: 70.31%] [G loss: 0.568099]\n",
      "epoch:7 step:6931 [D loss: 0.224439, acc.: 64.06%] [G loss: 0.494044]\n",
      "epoch:7 step:6932 [D loss: 0.244740, acc.: 53.12%] [G loss: 0.485351]\n",
      "epoch:7 step:6933 [D loss: 0.196936, acc.: 69.53%] [G loss: 0.504166]\n",
      "epoch:7 step:6934 [D loss: 0.209002, acc.: 69.53%] [G loss: 0.516567]\n",
      "epoch:7 step:6935 [D loss: 0.261762, acc.: 53.91%] [G loss: 0.409698]\n",
      "epoch:7 step:6936 [D loss: 0.246822, acc.: 57.81%] [G loss: 0.426105]\n",
      "epoch:7 step:6937 [D loss: 0.211871, acc.: 62.50%] [G loss: 0.439831]\n",
      "epoch:7 step:6938 [D loss: 0.224677, acc.: 64.84%] [G loss: 0.493001]\n",
      "epoch:7 step:6939 [D loss: 0.225595, acc.: 63.28%] [G loss: 0.526294]\n",
      "epoch:7 step:6940 [D loss: 0.187004, acc.: 71.88%] [G loss: 0.514195]\n",
      "epoch:7 step:6941 [D loss: 0.224508, acc.: 64.84%] [G loss: 0.490666]\n",
      "epoch:7 step:6942 [D loss: 0.219808, acc.: 60.94%] [G loss: 0.483630]\n",
      "epoch:7 step:6943 [D loss: 0.208235, acc.: 66.41%] [G loss: 0.463270]\n",
      "epoch:7 step:6944 [D loss: 0.202138, acc.: 70.31%] [G loss: 0.509824]\n",
      "epoch:7 step:6945 [D loss: 0.219158, acc.: 64.06%] [G loss: 0.519553]\n",
      "epoch:7 step:6946 [D loss: 0.249975, acc.: 59.38%] [G loss: 0.500868]\n",
      "epoch:7 step:6947 [D loss: 0.239759, acc.: 60.16%] [G loss: 0.504882]\n",
      "epoch:7 step:6948 [D loss: 0.207662, acc.: 71.88%] [G loss: 0.501875]\n",
      "epoch:7 step:6949 [D loss: 0.219917, acc.: 64.06%] [G loss: 0.504449]\n",
      "epoch:7 step:6950 [D loss: 0.207251, acc.: 67.19%] [G loss: 0.454795]\n",
      "epoch:7 step:6951 [D loss: 0.208676, acc.: 66.41%] [G loss: 0.496491]\n",
      "epoch:7 step:6952 [D loss: 0.199708, acc.: 71.88%] [G loss: 0.507400]\n",
      "epoch:7 step:6953 [D loss: 0.231029, acc.: 66.41%] [G loss: 0.463632]\n",
      "epoch:7 step:6954 [D loss: 0.193664, acc.: 71.09%] [G loss: 0.506561]\n",
      "epoch:7 step:6955 [D loss: 0.268916, acc.: 50.78%] [G loss: 0.494230]\n",
      "epoch:7 step:6956 [D loss: 0.202203, acc.: 73.44%] [G loss: 0.483523]\n",
      "epoch:7 step:6957 [D loss: 0.205445, acc.: 69.53%] [G loss: 0.536256]\n",
      "epoch:7 step:6958 [D loss: 0.212530, acc.: 68.75%] [G loss: 0.549144]\n",
      "epoch:7 step:6959 [D loss: 0.239762, acc.: 62.50%] [G loss: 0.529621]\n",
      "epoch:7 step:6960 [D loss: 0.241410, acc.: 64.06%] [G loss: 0.498377]\n",
      "epoch:7 step:6961 [D loss: 0.175860, acc.: 75.00%] [G loss: 0.564075]\n",
      "epoch:7 step:6962 [D loss: 0.216231, acc.: 66.41%] [G loss: 0.492301]\n",
      "epoch:7 step:6963 [D loss: 0.241839, acc.: 58.59%] [G loss: 0.467016]\n",
      "epoch:7 step:6964 [D loss: 0.201677, acc.: 68.75%] [G loss: 0.511089]\n",
      "epoch:7 step:6965 [D loss: 0.170119, acc.: 76.56%] [G loss: 0.529834]\n",
      "epoch:7 step:6966 [D loss: 0.262445, acc.: 57.03%] [G loss: 0.482016]\n",
      "epoch:7 step:6967 [D loss: 0.233001, acc.: 65.62%] [G loss: 0.496946]\n",
      "epoch:7 step:6968 [D loss: 0.201931, acc.: 71.88%] [G loss: 0.452487]\n",
      "epoch:7 step:6969 [D loss: 0.233419, acc.: 61.72%] [G loss: 0.484644]\n",
      "epoch:7 step:6970 [D loss: 0.228236, acc.: 65.62%] [G loss: 0.482799]\n",
      "epoch:7 step:6971 [D loss: 0.241823, acc.: 60.16%] [G loss: 0.452517]\n",
      "epoch:7 step:6972 [D loss: 0.246308, acc.: 58.59%] [G loss: 0.479632]\n",
      "epoch:7 step:6973 [D loss: 0.231519, acc.: 67.97%] [G loss: 0.473842]\n",
      "epoch:7 step:6974 [D loss: 0.220656, acc.: 61.72%] [G loss: 0.497077]\n",
      "epoch:7 step:6975 [D loss: 0.187155, acc.: 70.31%] [G loss: 0.544867]\n",
      "epoch:7 step:6976 [D loss: 0.249126, acc.: 53.91%] [G loss: 0.526421]\n",
      "epoch:7 step:6977 [D loss: 0.257378, acc.: 55.47%] [G loss: 0.438295]\n",
      "epoch:7 step:6978 [D loss: 0.230042, acc.: 67.19%] [G loss: 0.460226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6979 [D loss: 0.223158, acc.: 61.72%] [G loss: 0.522394]\n",
      "epoch:7 step:6980 [D loss: 0.249860, acc.: 53.12%] [G loss: 0.473865]\n",
      "epoch:7 step:6981 [D loss: 0.250119, acc.: 56.25%] [G loss: 0.482571]\n",
      "epoch:7 step:6982 [D loss: 0.201218, acc.: 71.88%] [G loss: 0.466715]\n",
      "epoch:7 step:6983 [D loss: 0.229410, acc.: 61.72%] [G loss: 0.453356]\n",
      "epoch:7 step:6984 [D loss: 0.234713, acc.: 63.28%] [G loss: 0.439704]\n",
      "epoch:7 step:6985 [D loss: 0.205888, acc.: 70.31%] [G loss: 0.456721]\n",
      "epoch:7 step:6986 [D loss: 0.202569, acc.: 69.53%] [G loss: 0.503862]\n",
      "epoch:7 step:6987 [D loss: 0.155934, acc.: 80.47%] [G loss: 0.566812]\n",
      "epoch:7 step:6988 [D loss: 0.194679, acc.: 71.09%] [G loss: 0.573217]\n",
      "epoch:7 step:6989 [D loss: 0.185381, acc.: 73.44%] [G loss: 0.605786]\n",
      "epoch:7 step:6990 [D loss: 0.239567, acc.: 57.03%] [G loss: 0.543139]\n",
      "epoch:7 step:6991 [D loss: 0.237373, acc.: 60.94%] [G loss: 0.474771]\n",
      "epoch:7 step:6992 [D loss: 0.248974, acc.: 56.25%] [G loss: 0.459516]\n",
      "epoch:7 step:6993 [D loss: 0.198163, acc.: 64.84%] [G loss: 0.495334]\n",
      "epoch:7 step:6994 [D loss: 0.220633, acc.: 67.97%] [G loss: 0.534348]\n",
      "epoch:7 step:6995 [D loss: 0.182327, acc.: 71.09%] [G loss: 0.580510]\n",
      "epoch:7 step:6996 [D loss: 0.276055, acc.: 54.69%] [G loss: 0.507984]\n",
      "epoch:7 step:6997 [D loss: 0.232004, acc.: 61.72%] [G loss: 0.431102]\n",
      "epoch:7 step:6998 [D loss: 0.201116, acc.: 66.41%] [G loss: 0.510065]\n",
      "epoch:7 step:6999 [D loss: 0.204005, acc.: 67.19%] [G loss: 0.480819]\n",
      "epoch:7 step:7000 [D loss: 0.235208, acc.: 60.94%] [G loss: 0.463960]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.196036\n",
      "FID: 19.635063\n",
      "0 = 12.071640169978064\n",
      "1 = 0.0539942810421058\n",
      "2 = 0.9262999892234802\n",
      "3 = 0.8942000269889832\n",
      "4 = 0.9584000110626221\n",
      "5 = 0.9555460810661316\n",
      "6 = 0.8942000269889832\n",
      "7 = 7.386855727982512\n",
      "8 = 0.10036585164452383\n",
      "9 = 0.7462999820709229\n",
      "10 = 0.7386999726295471\n",
      "11 = 0.7538999915122986\n",
      "12 = 0.7501015663146973\n",
      "13 = 0.7386999726295471\n",
      "14 = 7.196097373962402\n",
      "15 = 9.445588111877441\n",
      "16 = 0.14080676436424255\n",
      "17 = 7.196036338806152\n",
      "18 = 19.63506317138672\n",
      "epoch:7 step:7001 [D loss: 0.210338, acc.: 65.62%] [G loss: 0.452173]\n",
      "epoch:7 step:7002 [D loss: 0.213380, acc.: 69.53%] [G loss: 0.498119]\n",
      "epoch:7 step:7003 [D loss: 0.208849, acc.: 65.62%] [G loss: 0.507201]\n",
      "epoch:7 step:7004 [D loss: 0.200159, acc.: 63.28%] [G loss: 0.499603]\n",
      "epoch:7 step:7005 [D loss: 0.192952, acc.: 67.97%] [G loss: 0.507282]\n",
      "epoch:7 step:7006 [D loss: 0.209273, acc.: 68.75%] [G loss: 0.522099]\n",
      "epoch:7 step:7007 [D loss: 0.294813, acc.: 52.34%] [G loss: 0.493264]\n",
      "epoch:7 step:7008 [D loss: 0.213991, acc.: 67.97%] [G loss: 0.461124]\n",
      "epoch:7 step:7009 [D loss: 0.179683, acc.: 71.88%] [G loss: 0.490588]\n",
      "epoch:7 step:7010 [D loss: 0.167669, acc.: 74.22%] [G loss: 0.512773]\n",
      "epoch:7 step:7011 [D loss: 0.215314, acc.: 69.53%] [G loss: 0.519433]\n",
      "epoch:7 step:7012 [D loss: 0.213806, acc.: 62.50%] [G loss: 0.497376]\n",
      "epoch:7 step:7013 [D loss: 0.229305, acc.: 65.62%] [G loss: 0.505209]\n",
      "epoch:7 step:7014 [D loss: 0.217600, acc.: 65.62%] [G loss: 0.488916]\n",
      "epoch:7 step:7015 [D loss: 0.223455, acc.: 63.28%] [G loss: 0.499397]\n",
      "epoch:7 step:7016 [D loss: 0.226789, acc.: 64.06%] [G loss: 0.528343]\n",
      "epoch:7 step:7017 [D loss: 0.245976, acc.: 55.47%] [G loss: 0.484728]\n",
      "epoch:7 step:7018 [D loss: 0.237834, acc.: 67.97%] [G loss: 0.497611]\n",
      "epoch:7 step:7019 [D loss: 0.226029, acc.: 63.28%] [G loss: 0.487193]\n",
      "epoch:7 step:7020 [D loss: 0.216694, acc.: 61.72%] [G loss: 0.502721]\n",
      "epoch:7 step:7021 [D loss: 0.201023, acc.: 63.28%] [G loss: 0.488395]\n",
      "epoch:7 step:7022 [D loss: 0.221161, acc.: 62.50%] [G loss: 0.433570]\n",
      "epoch:7 step:7023 [D loss: 0.219342, acc.: 64.06%] [G loss: 0.433784]\n",
      "epoch:7 step:7024 [D loss: 0.228189, acc.: 60.16%] [G loss: 0.467692]\n",
      "epoch:7 step:7025 [D loss: 0.235199, acc.: 59.38%] [G loss: 0.480825]\n",
      "epoch:7 step:7026 [D loss: 0.211855, acc.: 63.28%] [G loss: 0.506197]\n",
      "epoch:7 step:7027 [D loss: 0.242680, acc.: 58.59%] [G loss: 0.472432]\n",
      "epoch:7 step:7028 [D loss: 0.222604, acc.: 64.84%] [G loss: 0.485485]\n",
      "epoch:7 step:7029 [D loss: 0.215700, acc.: 67.97%] [G loss: 0.532481]\n",
      "epoch:7 step:7030 [D loss: 0.175977, acc.: 74.22%] [G loss: 0.558554]\n",
      "epoch:7 step:7031 [D loss: 0.180429, acc.: 70.31%] [G loss: 0.576727]\n",
      "epoch:7 step:7032 [D loss: 0.289254, acc.: 56.25%] [G loss: 0.489850]\n",
      "epoch:7 step:7033 [D loss: 0.209673, acc.: 68.75%] [G loss: 0.496964]\n",
      "epoch:7 step:7034 [D loss: 0.173627, acc.: 75.78%] [G loss: 0.522228]\n",
      "epoch:7 step:7035 [D loss: 0.218828, acc.: 64.84%] [G loss: 0.520893]\n",
      "epoch:7 step:7036 [D loss: 0.229817, acc.: 63.28%] [G loss: 0.474809]\n",
      "epoch:7 step:7037 [D loss: 0.258285, acc.: 53.12%] [G loss: 0.425428]\n",
      "epoch:7 step:7038 [D loss: 0.223682, acc.: 62.50%] [G loss: 0.491668]\n",
      "epoch:7 step:7039 [D loss: 0.217029, acc.: 62.50%] [G loss: 0.489949]\n",
      "epoch:7 step:7040 [D loss: 0.204477, acc.: 70.31%] [G loss: 0.507521]\n",
      "epoch:7 step:7041 [D loss: 0.269471, acc.: 54.69%] [G loss: 0.464743]\n",
      "epoch:7 step:7042 [D loss: 0.244941, acc.: 57.81%] [G loss: 0.440742]\n",
      "epoch:7 step:7043 [D loss: 0.209808, acc.: 67.97%] [G loss: 0.498082]\n",
      "epoch:7 step:7044 [D loss: 0.219418, acc.: 58.59%] [G loss: 0.482749]\n",
      "epoch:7 step:7045 [D loss: 0.235978, acc.: 59.38%] [G loss: 0.492695]\n",
      "epoch:7 step:7046 [D loss: 0.225522, acc.: 60.16%] [G loss: 0.465876]\n",
      "epoch:7 step:7047 [D loss: 0.189249, acc.: 72.66%] [G loss: 0.524149]\n",
      "epoch:7 step:7048 [D loss: 0.229899, acc.: 69.53%] [G loss: 0.477921]\n",
      "epoch:7 step:7049 [D loss: 0.233883, acc.: 60.16%] [G loss: 0.512347]\n",
      "epoch:7 step:7050 [D loss: 0.228440, acc.: 61.72%] [G loss: 0.489400]\n",
      "epoch:7 step:7051 [D loss: 0.220265, acc.: 63.28%] [G loss: 0.507534]\n",
      "epoch:7 step:7052 [D loss: 0.216094, acc.: 63.28%] [G loss: 0.532536]\n",
      "epoch:7 step:7053 [D loss: 0.220155, acc.: 64.06%] [G loss: 0.472522]\n",
      "epoch:7 step:7054 [D loss: 0.181830, acc.: 71.88%] [G loss: 0.531179]\n",
      "epoch:7 step:7055 [D loss: 0.233111, acc.: 64.06%] [G loss: 0.514276]\n",
      "epoch:7 step:7056 [D loss: 0.197467, acc.: 76.56%] [G loss: 0.523835]\n",
      "epoch:7 step:7057 [D loss: 0.182729, acc.: 75.00%] [G loss: 0.537813]\n",
      "epoch:7 step:7058 [D loss: 0.185593, acc.: 70.31%] [G loss: 0.508952]\n",
      "epoch:7 step:7059 [D loss: 0.281335, acc.: 51.56%] [G loss: 0.438048]\n",
      "epoch:7 step:7060 [D loss: 0.259146, acc.: 59.38%] [G loss: 0.458871]\n",
      "epoch:7 step:7061 [D loss: 0.243697, acc.: 57.81%] [G loss: 0.449179]\n",
      "epoch:7 step:7062 [D loss: 0.192730, acc.: 68.75%] [G loss: 0.475680]\n",
      "epoch:7 step:7063 [D loss: 0.188176, acc.: 74.22%] [G loss: 0.528559]\n",
      "epoch:7 step:7064 [D loss: 0.217887, acc.: 67.19%] [G loss: 0.504580]\n",
      "epoch:7 step:7065 [D loss: 0.252033, acc.: 53.91%] [G loss: 0.472760]\n",
      "epoch:7 step:7066 [D loss: 0.232937, acc.: 63.28%] [G loss: 0.548279]\n",
      "epoch:7 step:7067 [D loss: 0.182833, acc.: 68.75%] [G loss: 0.569224]\n",
      "epoch:7 step:7068 [D loss: 0.241490, acc.: 63.28%] [G loss: 0.472287]\n",
      "epoch:7 step:7069 [D loss: 0.230294, acc.: 61.72%] [G loss: 0.467036]\n",
      "epoch:7 step:7070 [D loss: 0.231584, acc.: 59.38%] [G loss: 0.435259]\n",
      "epoch:7 step:7071 [D loss: 0.218630, acc.: 66.41%] [G loss: 0.442415]\n",
      "epoch:7 step:7072 [D loss: 0.189398, acc.: 73.44%] [G loss: 0.508386]\n",
      "epoch:7 step:7073 [D loss: 0.232050, acc.: 58.59%] [G loss: 0.522064]\n",
      "epoch:7 step:7074 [D loss: 0.190601, acc.: 76.56%] [G loss: 0.563707]\n",
      "epoch:7 step:7075 [D loss: 0.188857, acc.: 75.78%] [G loss: 0.559838]\n",
      "epoch:7 step:7076 [D loss: 0.220778, acc.: 62.50%] [G loss: 0.496651]\n",
      "epoch:7 step:7077 [D loss: 0.230840, acc.: 60.94%] [G loss: 0.472955]\n",
      "epoch:7 step:7078 [D loss: 0.197105, acc.: 69.53%] [G loss: 0.502471]\n",
      "epoch:7 step:7079 [D loss: 0.213169, acc.: 67.19%] [G loss: 0.506776]\n",
      "epoch:7 step:7080 [D loss: 0.191624, acc.: 71.09%] [G loss: 0.546655]\n",
      "epoch:7 step:7081 [D loss: 0.209062, acc.: 67.19%] [G loss: 0.480171]\n",
      "epoch:7 step:7082 [D loss: 0.186606, acc.: 73.44%] [G loss: 0.504196]\n",
      "epoch:7 step:7083 [D loss: 0.223134, acc.: 60.94%] [G loss: 0.442277]\n",
      "epoch:7 step:7084 [D loss: 0.213556, acc.: 62.50%] [G loss: 0.457483]\n",
      "epoch:7 step:7085 [D loss: 0.210130, acc.: 71.09%] [G loss: 0.523816]\n",
      "epoch:7 step:7086 [D loss: 0.233644, acc.: 67.19%] [G loss: 0.454187]\n",
      "epoch:7 step:7087 [D loss: 0.281902, acc.: 44.53%] [G loss: 0.453962]\n",
      "epoch:7 step:7088 [D loss: 0.215045, acc.: 67.19%] [G loss: 0.464810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7089 [D loss: 0.199605, acc.: 69.53%] [G loss: 0.482318]\n",
      "epoch:7 step:7090 [D loss: 0.248033, acc.: 57.03%] [G loss: 0.456754]\n",
      "epoch:7 step:7091 [D loss: 0.216778, acc.: 64.06%] [G loss: 0.484542]\n",
      "epoch:7 step:7092 [D loss: 0.223701, acc.: 60.94%] [G loss: 0.530375]\n",
      "epoch:7 step:7093 [D loss: 0.183271, acc.: 68.75%] [G loss: 0.584191]\n",
      "epoch:7 step:7094 [D loss: 0.251223, acc.: 53.91%] [G loss: 0.433859]\n",
      "epoch:7 step:7095 [D loss: 0.185705, acc.: 75.00%] [G loss: 0.483121]\n",
      "epoch:7 step:7096 [D loss: 0.212729, acc.: 67.19%] [G loss: 0.480179]\n",
      "epoch:7 step:7097 [D loss: 0.239075, acc.: 62.50%] [G loss: 0.524487]\n",
      "epoch:7 step:7098 [D loss: 0.200768, acc.: 68.75%] [G loss: 0.545725]\n",
      "epoch:7 step:7099 [D loss: 0.226739, acc.: 66.41%] [G loss: 0.535486]\n",
      "epoch:7 step:7100 [D loss: 0.197941, acc.: 67.97%] [G loss: 0.491029]\n",
      "epoch:7 step:7101 [D loss: 0.286609, acc.: 48.44%] [G loss: 0.430506]\n",
      "epoch:7 step:7102 [D loss: 0.249906, acc.: 51.56%] [G loss: 0.431478]\n",
      "epoch:7 step:7103 [D loss: 0.211143, acc.: 67.97%] [G loss: 0.495575]\n",
      "epoch:7 step:7104 [D loss: 0.191020, acc.: 70.31%] [G loss: 0.513110]\n",
      "epoch:7 step:7105 [D loss: 0.215591, acc.: 66.41%] [G loss: 0.506900]\n",
      "epoch:7 step:7106 [D loss: 0.208159, acc.: 65.62%] [G loss: 0.516202]\n",
      "epoch:7 step:7107 [D loss: 0.210576, acc.: 68.75%] [G loss: 0.484805]\n",
      "epoch:7 step:7108 [D loss: 0.189883, acc.: 72.66%] [G loss: 0.507149]\n",
      "epoch:7 step:7109 [D loss: 0.193422, acc.: 67.97%] [G loss: 0.548324]\n",
      "epoch:7 step:7110 [D loss: 0.203796, acc.: 67.97%] [G loss: 0.508575]\n",
      "epoch:7 step:7111 [D loss: 0.229581, acc.: 61.72%] [G loss: 0.466401]\n",
      "epoch:7 step:7112 [D loss: 0.228287, acc.: 60.94%] [G loss: 0.496358]\n",
      "epoch:7 step:7113 [D loss: 0.178963, acc.: 73.44%] [G loss: 0.527357]\n",
      "epoch:7 step:7114 [D loss: 0.178667, acc.: 79.69%] [G loss: 0.543901]\n",
      "epoch:7 step:7115 [D loss: 0.194369, acc.: 69.53%] [G loss: 0.470067]\n",
      "epoch:7 step:7116 [D loss: 0.193135, acc.: 73.44%] [G loss: 0.502118]\n",
      "epoch:7 step:7117 [D loss: 0.206790, acc.: 67.19%] [G loss: 0.480478]\n",
      "epoch:7 step:7118 [D loss: 0.235321, acc.: 62.50%] [G loss: 0.471772]\n",
      "epoch:7 step:7119 [D loss: 0.227038, acc.: 64.84%] [G loss: 0.448557]\n",
      "epoch:7 step:7120 [D loss: 0.194803, acc.: 69.53%] [G loss: 0.468771]\n",
      "epoch:7 step:7121 [D loss: 0.221061, acc.: 59.38%] [G loss: 0.507001]\n",
      "epoch:7 step:7122 [D loss: 0.216243, acc.: 65.62%] [G loss: 0.525157]\n",
      "epoch:7 step:7123 [D loss: 0.178704, acc.: 75.78%] [G loss: 0.526822]\n",
      "epoch:7 step:7124 [D loss: 0.258859, acc.: 60.94%] [G loss: 0.484136]\n",
      "epoch:7 step:7125 [D loss: 0.270073, acc.: 56.25%] [G loss: 0.455464]\n",
      "epoch:7 step:7126 [D loss: 0.212690, acc.: 67.19%] [G loss: 0.535106]\n",
      "epoch:7 step:7127 [D loss: 0.177212, acc.: 76.56%] [G loss: 0.468742]\n",
      "epoch:7 step:7128 [D loss: 0.255847, acc.: 60.16%] [G loss: 0.470187]\n",
      "epoch:7 step:7129 [D loss: 0.193820, acc.: 70.31%] [G loss: 0.453550]\n",
      "epoch:7 step:7130 [D loss: 0.229313, acc.: 60.94%] [G loss: 0.466506]\n",
      "epoch:7 step:7131 [D loss: 0.228793, acc.: 66.41%] [G loss: 0.485183]\n",
      "epoch:7 step:7132 [D loss: 0.207544, acc.: 69.53%] [G loss: 0.481231]\n",
      "epoch:7 step:7133 [D loss: 0.165254, acc.: 76.56%] [G loss: 0.509741]\n",
      "epoch:7 step:7134 [D loss: 0.181456, acc.: 70.31%] [G loss: 0.524632]\n",
      "epoch:7 step:7135 [D loss: 0.223987, acc.: 62.50%] [G loss: 0.538588]\n",
      "epoch:7 step:7136 [D loss: 0.222844, acc.: 63.28%] [G loss: 0.496128]\n",
      "epoch:7 step:7137 [D loss: 0.218936, acc.: 64.84%] [G loss: 0.454728]\n",
      "epoch:7 step:7138 [D loss: 0.226789, acc.: 66.41%] [G loss: 0.500673]\n",
      "epoch:7 step:7139 [D loss: 0.238752, acc.: 53.91%] [G loss: 0.457555]\n",
      "epoch:7 step:7140 [D loss: 0.216378, acc.: 64.06%] [G loss: 0.501270]\n",
      "epoch:7 step:7141 [D loss: 0.165462, acc.: 76.56%] [G loss: 0.565757]\n",
      "epoch:7 step:7142 [D loss: 0.186701, acc.: 70.31%] [G loss: 0.527077]\n",
      "epoch:7 step:7143 [D loss: 0.256085, acc.: 59.38%] [G loss: 0.477920]\n",
      "epoch:7 step:7144 [D loss: 0.228831, acc.: 61.72%] [G loss: 0.466659]\n",
      "epoch:7 step:7145 [D loss: 0.228038, acc.: 61.72%] [G loss: 0.484495]\n",
      "epoch:7 step:7146 [D loss: 0.254580, acc.: 57.81%] [G loss: 0.459764]\n",
      "epoch:7 step:7147 [D loss: 0.241178, acc.: 60.16%] [G loss: 0.466502]\n",
      "epoch:7 step:7148 [D loss: 0.179278, acc.: 75.00%] [G loss: 0.573487]\n",
      "epoch:7 step:7149 [D loss: 0.251094, acc.: 61.72%] [G loss: 0.529648]\n",
      "epoch:7 step:7150 [D loss: 0.238400, acc.: 64.84%] [G loss: 0.488458]\n",
      "epoch:7 step:7151 [D loss: 0.215385, acc.: 69.53%] [G loss: 0.492527]\n",
      "epoch:7 step:7152 [D loss: 0.228830, acc.: 60.16%] [G loss: 0.516864]\n",
      "epoch:7 step:7153 [D loss: 0.261921, acc.: 53.91%] [G loss: 0.475702]\n",
      "epoch:7 step:7154 [D loss: 0.202810, acc.: 67.97%] [G loss: 0.509759]\n",
      "epoch:7 step:7155 [D loss: 0.257308, acc.: 57.03%] [G loss: 0.495861]\n",
      "epoch:7 step:7156 [D loss: 0.217614, acc.: 63.28%] [G loss: 0.496221]\n",
      "epoch:7 step:7157 [D loss: 0.201267, acc.: 72.66%] [G loss: 0.500127]\n",
      "epoch:7 step:7158 [D loss: 0.236105, acc.: 61.72%] [G loss: 0.484470]\n",
      "epoch:7 step:7159 [D loss: 0.250494, acc.: 57.81%] [G loss: 0.443034]\n",
      "epoch:7 step:7160 [D loss: 0.235669, acc.: 55.47%] [G loss: 0.449545]\n",
      "epoch:7 step:7161 [D loss: 0.236690, acc.: 57.03%] [G loss: 0.490062]\n",
      "epoch:7 step:7162 [D loss: 0.228634, acc.: 66.41%] [G loss: 0.509525]\n",
      "epoch:7 step:7163 [D loss: 0.233891, acc.: 59.38%] [G loss: 0.504967]\n",
      "epoch:7 step:7164 [D loss: 0.202739, acc.: 69.53%] [G loss: 0.453575]\n",
      "epoch:7 step:7165 [D loss: 0.224344, acc.: 63.28%] [G loss: 0.457672]\n",
      "epoch:7 step:7166 [D loss: 0.188584, acc.: 64.84%] [G loss: 0.500388]\n",
      "epoch:7 step:7167 [D loss: 0.194211, acc.: 68.75%] [G loss: 0.516991]\n",
      "epoch:7 step:7168 [D loss: 0.192200, acc.: 67.97%] [G loss: 0.496086]\n",
      "epoch:7 step:7169 [D loss: 0.226583, acc.: 64.84%] [G loss: 0.466020]\n",
      "epoch:7 step:7170 [D loss: 0.198379, acc.: 67.97%] [G loss: 0.482732]\n",
      "epoch:7 step:7171 [D loss: 0.213485, acc.: 65.62%] [G loss: 0.459992]\n",
      "epoch:7 step:7172 [D loss: 0.197320, acc.: 71.88%] [G loss: 0.454156]\n",
      "epoch:7 step:7173 [D loss: 0.220540, acc.: 60.94%] [G loss: 0.468696]\n",
      "epoch:7 step:7174 [D loss: 0.243326, acc.: 65.62%] [G loss: 0.428520]\n",
      "epoch:7 step:7175 [D loss: 0.236223, acc.: 62.50%] [G loss: 0.454068]\n",
      "epoch:7 step:7176 [D loss: 0.197987, acc.: 66.41%] [G loss: 0.473352]\n",
      "epoch:7 step:7177 [D loss: 0.197762, acc.: 69.53%] [G loss: 0.487060]\n",
      "epoch:7 step:7178 [D loss: 0.270143, acc.: 49.22%] [G loss: 0.482135]\n",
      "epoch:7 step:7179 [D loss: 0.180088, acc.: 72.66%] [G loss: 0.512268]\n",
      "epoch:7 step:7180 [D loss: 0.215078, acc.: 71.09%] [G loss: 0.489438]\n",
      "epoch:7 step:7181 [D loss: 0.240913, acc.: 55.47%] [G loss: 0.449733]\n",
      "epoch:7 step:7182 [D loss: 0.205023, acc.: 67.97%] [G loss: 0.471685]\n",
      "epoch:7 step:7183 [D loss: 0.192201, acc.: 75.00%] [G loss: 0.530038]\n",
      "epoch:7 step:7184 [D loss: 0.260113, acc.: 58.59%] [G loss: 0.451787]\n",
      "epoch:7 step:7185 [D loss: 0.221148, acc.: 64.84%] [G loss: 0.495931]\n",
      "epoch:7 step:7186 [D loss: 0.229504, acc.: 61.72%] [G loss: 0.466459]\n",
      "epoch:7 step:7187 [D loss: 0.212645, acc.: 64.06%] [G loss: 0.482302]\n",
      "epoch:7 step:7188 [D loss: 0.188706, acc.: 73.44%] [G loss: 0.487978]\n",
      "epoch:7 step:7189 [D loss: 0.199524, acc.: 70.31%] [G loss: 0.487848]\n",
      "epoch:7 step:7190 [D loss: 0.216047, acc.: 64.84%] [G loss: 0.487936]\n",
      "epoch:7 step:7191 [D loss: 0.185601, acc.: 69.53%] [G loss: 0.536195]\n",
      "epoch:7 step:7192 [D loss: 0.191272, acc.: 64.84%] [G loss: 0.535048]\n",
      "epoch:7 step:7193 [D loss: 0.211435, acc.: 68.75%] [G loss: 0.541594]\n",
      "epoch:7 step:7194 [D loss: 0.197805, acc.: 69.53%] [G loss: 0.517735]\n",
      "epoch:7 step:7195 [D loss: 0.216189, acc.: 68.75%] [G loss: 0.458807]\n",
      "epoch:7 step:7196 [D loss: 0.215989, acc.: 63.28%] [G loss: 0.485428]\n",
      "epoch:7 step:7197 [D loss: 0.205939, acc.: 71.09%] [G loss: 0.487518]\n",
      "epoch:7 step:7198 [D loss: 0.217713, acc.: 60.94%] [G loss: 0.464915]\n",
      "epoch:7 step:7199 [D loss: 0.217869, acc.: 61.72%] [G loss: 0.468683]\n",
      "epoch:7 step:7200 [D loss: 0.184376, acc.: 71.88%] [G loss: 0.515440]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.138300\n",
      "FID: 20.048262\n",
      "0 = 12.202788566923122\n",
      "1 = 0.056694268441860284\n",
      "2 = 0.9248999953269958\n",
      "3 = 0.9020000100135803\n",
      "4 = 0.9477999806404114\n",
      "5 = 0.9452944993972778\n",
      "6 = 0.9020000100135803\n",
      "7 = 7.491502614671017\n",
      "8 = 0.0987801537167793\n",
      "9 = 0.7459999918937683\n",
      "10 = 0.7383999824523926\n",
      "11 = 0.753600001335144\n",
      "12 = 0.7497969269752502\n",
      "13 = 0.7383999824523926\n",
      "14 = 7.138363361358643\n",
      "15 = 9.353025436401367\n",
      "16 = 0.16026505827903748\n",
      "17 = 7.13830041885376\n",
      "18 = 20.048261642456055\n",
      "epoch:7 step:7201 [D loss: 0.182746, acc.: 75.00%] [G loss: 0.550838]\n",
      "epoch:7 step:7202 [D loss: 0.208807, acc.: 68.75%] [G loss: 0.490570]\n",
      "epoch:7 step:7203 [D loss: 0.230902, acc.: 64.06%] [G loss: 0.462466]\n",
      "epoch:7 step:7204 [D loss: 0.201614, acc.: 68.75%] [G loss: 0.475048]\n",
      "epoch:7 step:7205 [D loss: 0.238288, acc.: 64.84%] [G loss: 0.445424]\n",
      "epoch:7 step:7206 [D loss: 0.192998, acc.: 71.88%] [G loss: 0.529686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7207 [D loss: 0.156061, acc.: 80.47%] [G loss: 0.641172]\n",
      "epoch:7 step:7208 [D loss: 0.201558, acc.: 68.75%] [G loss: 0.522864]\n",
      "epoch:7 step:7209 [D loss: 0.212334, acc.: 69.53%] [G loss: 0.504043]\n",
      "epoch:7 step:7210 [D loss: 0.225102, acc.: 60.94%] [G loss: 0.469595]\n",
      "epoch:7 step:7211 [D loss: 0.254702, acc.: 57.03%] [G loss: 0.500078]\n",
      "epoch:7 step:7212 [D loss: 0.221548, acc.: 65.62%] [G loss: 0.501527]\n",
      "epoch:7 step:7213 [D loss: 0.211347, acc.: 66.41%] [G loss: 0.509611]\n",
      "epoch:7 step:7214 [D loss: 0.248638, acc.: 56.25%] [G loss: 0.482284]\n",
      "epoch:7 step:7215 [D loss: 0.194593, acc.: 71.88%] [G loss: 0.472539]\n",
      "epoch:7 step:7216 [D loss: 0.197719, acc.: 70.31%] [G loss: 0.472793]\n",
      "epoch:7 step:7217 [D loss: 0.234228, acc.: 58.59%] [G loss: 0.500748]\n",
      "epoch:7 step:7218 [D loss: 0.197730, acc.: 67.97%] [G loss: 0.467742]\n",
      "epoch:7 step:7219 [D loss: 0.183315, acc.: 73.44%] [G loss: 0.521132]\n",
      "epoch:7 step:7220 [D loss: 0.205595, acc.: 72.66%] [G loss: 0.499434]\n",
      "epoch:7 step:7221 [D loss: 0.214043, acc.: 64.84%] [G loss: 0.517492]\n",
      "epoch:7 step:7222 [D loss: 0.221330, acc.: 64.06%] [G loss: 0.494165]\n",
      "epoch:7 step:7223 [D loss: 0.213897, acc.: 65.62%] [G loss: 0.514697]\n",
      "epoch:7 step:7224 [D loss: 0.211265, acc.: 64.06%] [G loss: 0.478407]\n",
      "epoch:7 step:7225 [D loss: 0.240263, acc.: 60.16%] [G loss: 0.510062]\n",
      "epoch:7 step:7226 [D loss: 0.223594, acc.: 66.41%] [G loss: 0.516376]\n",
      "epoch:7 step:7227 [D loss: 0.233272, acc.: 67.97%] [G loss: 0.498410]\n",
      "epoch:7 step:7228 [D loss: 0.204518, acc.: 68.75%] [G loss: 0.536370]\n",
      "epoch:7 step:7229 [D loss: 0.274529, acc.: 48.44%] [G loss: 0.434809]\n",
      "epoch:7 step:7230 [D loss: 0.223985, acc.: 63.28%] [G loss: 0.505935]\n",
      "epoch:7 step:7231 [D loss: 0.228551, acc.: 63.28%] [G loss: 0.478650]\n",
      "epoch:7 step:7232 [D loss: 0.208169, acc.: 68.75%] [G loss: 0.510808]\n",
      "epoch:7 step:7233 [D loss: 0.221233, acc.: 63.28%] [G loss: 0.551552]\n",
      "epoch:7 step:7234 [D loss: 0.215717, acc.: 68.75%] [G loss: 0.503466]\n",
      "epoch:7 step:7235 [D loss: 0.220515, acc.: 60.94%] [G loss: 0.486714]\n",
      "epoch:7 step:7236 [D loss: 0.173376, acc.: 78.12%] [G loss: 0.534715]\n",
      "epoch:7 step:7237 [D loss: 0.205649, acc.: 66.41%] [G loss: 0.493781]\n",
      "epoch:7 step:7238 [D loss: 0.197319, acc.: 69.53%] [G loss: 0.461693]\n",
      "epoch:7 step:7239 [D loss: 0.206642, acc.: 64.06%] [G loss: 0.464195]\n",
      "epoch:7 step:7240 [D loss: 0.208662, acc.: 66.41%] [G loss: 0.472188]\n",
      "epoch:7 step:7241 [D loss: 0.251978, acc.: 57.81%] [G loss: 0.467474]\n",
      "epoch:7 step:7242 [D loss: 0.231292, acc.: 62.50%] [G loss: 0.476750]\n",
      "epoch:7 step:7243 [D loss: 0.215768, acc.: 66.41%] [G loss: 0.468314]\n",
      "epoch:7 step:7244 [D loss: 0.221857, acc.: 63.28%] [G loss: 0.445307]\n",
      "epoch:7 step:7245 [D loss: 0.217144, acc.: 67.97%] [G loss: 0.495977]\n",
      "epoch:7 step:7246 [D loss: 0.228157, acc.: 63.28%] [G loss: 0.477884]\n",
      "epoch:7 step:7247 [D loss: 0.208380, acc.: 67.19%] [G loss: 0.536325]\n",
      "epoch:7 step:7248 [D loss: 0.221885, acc.: 60.16%] [G loss: 0.535330]\n",
      "epoch:7 step:7249 [D loss: 0.186012, acc.: 67.19%] [G loss: 0.536843]\n",
      "epoch:7 step:7250 [D loss: 0.208642, acc.: 68.75%] [G loss: 0.556531]\n",
      "epoch:7 step:7251 [D loss: 0.191566, acc.: 70.31%] [G loss: 0.578323]\n",
      "epoch:7 step:7252 [D loss: 0.228509, acc.: 59.38%] [G loss: 0.535895]\n",
      "epoch:7 step:7253 [D loss: 0.173632, acc.: 80.47%] [G loss: 0.560910]\n",
      "epoch:7 step:7254 [D loss: 0.219648, acc.: 70.31%] [G loss: 0.511691]\n",
      "epoch:7 step:7255 [D loss: 0.213155, acc.: 63.28%] [G loss: 0.501843]\n",
      "epoch:7 step:7256 [D loss: 0.202885, acc.: 66.41%] [G loss: 0.533064]\n",
      "epoch:7 step:7257 [D loss: 0.183373, acc.: 73.44%] [G loss: 0.536720]\n",
      "epoch:7 step:7258 [D loss: 0.214110, acc.: 69.53%] [G loss: 0.496110]\n",
      "epoch:7 step:7259 [D loss: 0.201394, acc.: 67.19%] [G loss: 0.518429]\n",
      "epoch:7 step:7260 [D loss: 0.213635, acc.: 63.28%] [G loss: 0.553289]\n",
      "epoch:7 step:7261 [D loss: 0.262256, acc.: 51.56%] [G loss: 0.526578]\n",
      "epoch:7 step:7262 [D loss: 0.251685, acc.: 53.12%] [G loss: 0.467181]\n",
      "epoch:7 step:7263 [D loss: 0.242558, acc.: 58.59%] [G loss: 0.467349]\n",
      "epoch:7 step:7264 [D loss: 0.200952, acc.: 68.75%] [G loss: 0.502098]\n",
      "epoch:7 step:7265 [D loss: 0.185906, acc.: 74.22%] [G loss: 0.523906]\n",
      "epoch:7 step:7266 [D loss: 0.193789, acc.: 71.88%] [G loss: 0.533434]\n",
      "epoch:7 step:7267 [D loss: 0.193039, acc.: 71.88%] [G loss: 0.499831]\n",
      "epoch:7 step:7268 [D loss: 0.198415, acc.: 67.19%] [G loss: 0.539309]\n",
      "epoch:7 step:7269 [D loss: 0.285567, acc.: 55.47%] [G loss: 0.472242]\n",
      "epoch:7 step:7270 [D loss: 0.224043, acc.: 62.50%] [G loss: 0.504221]\n",
      "epoch:7 step:7271 [D loss: 0.241180, acc.: 63.28%] [G loss: 0.510767]\n",
      "epoch:7 step:7272 [D loss: 0.279910, acc.: 53.91%] [G loss: 0.479142]\n",
      "epoch:7 step:7273 [D loss: 0.238511, acc.: 59.38%] [G loss: 0.482452]\n",
      "epoch:7 step:7274 [D loss: 0.212979, acc.: 66.41%] [G loss: 0.483598]\n",
      "epoch:7 step:7275 [D loss: 0.262111, acc.: 57.03%] [G loss: 0.459891]\n",
      "epoch:7 step:7276 [D loss: 0.227698, acc.: 64.06%] [G loss: 0.478416]\n",
      "epoch:7 step:7277 [D loss: 0.233854, acc.: 58.59%] [G loss: 0.455519]\n",
      "epoch:7 step:7278 [D loss: 0.196897, acc.: 66.41%] [G loss: 0.543972]\n",
      "epoch:7 step:7279 [D loss: 0.240830, acc.: 54.69%] [G loss: 0.463518]\n",
      "epoch:7 step:7280 [D loss: 0.225588, acc.: 61.72%] [G loss: 0.495634]\n",
      "epoch:7 step:7281 [D loss: 0.229941, acc.: 56.25%] [G loss: 0.472471]\n",
      "epoch:7 step:7282 [D loss: 0.223610, acc.: 60.94%] [G loss: 0.488693]\n",
      "epoch:7 step:7283 [D loss: 0.227209, acc.: 66.41%] [G loss: 0.498714]\n",
      "epoch:7 step:7284 [D loss: 0.187854, acc.: 69.53%] [G loss: 0.512733]\n",
      "epoch:7 step:7285 [D loss: 0.205195, acc.: 64.84%] [G loss: 0.512815]\n",
      "epoch:7 step:7286 [D loss: 0.247570, acc.: 54.69%] [G loss: 0.488905]\n",
      "epoch:7 step:7287 [D loss: 0.227007, acc.: 61.72%] [G loss: 0.462340]\n",
      "epoch:7 step:7288 [D loss: 0.244544, acc.: 61.72%] [G loss: 0.491968]\n",
      "epoch:7 step:7289 [D loss: 0.172605, acc.: 71.09%] [G loss: 0.538750]\n",
      "epoch:7 step:7290 [D loss: 0.197528, acc.: 69.53%] [G loss: 0.494865]\n",
      "epoch:7 step:7291 [D loss: 0.211097, acc.: 60.16%] [G loss: 0.490097]\n",
      "epoch:7 step:7292 [D loss: 0.202908, acc.: 67.19%] [G loss: 0.517637]\n",
      "epoch:7 step:7293 [D loss: 0.231821, acc.: 64.06%] [G loss: 0.482544]\n",
      "epoch:7 step:7294 [D loss: 0.218646, acc.: 65.62%] [G loss: 0.485804]\n",
      "epoch:7 step:7295 [D loss: 0.244296, acc.: 57.81%] [G loss: 0.478913]\n",
      "epoch:7 step:7296 [D loss: 0.192425, acc.: 73.44%] [G loss: 0.469936]\n",
      "epoch:7 step:7297 [D loss: 0.252457, acc.: 53.12%] [G loss: 0.454694]\n",
      "epoch:7 step:7298 [D loss: 0.230799, acc.: 60.94%] [G loss: 0.427694]\n",
      "epoch:7 step:7299 [D loss: 0.216546, acc.: 68.75%] [G loss: 0.465506]\n",
      "epoch:7 step:7300 [D loss: 0.240256, acc.: 60.16%] [G loss: 0.456653]\n",
      "epoch:7 step:7301 [D loss: 0.227966, acc.: 60.16%] [G loss: 0.485095]\n",
      "epoch:7 step:7302 [D loss: 0.204070, acc.: 66.41%] [G loss: 0.498124]\n",
      "epoch:7 step:7303 [D loss: 0.235358, acc.: 61.72%] [G loss: 0.512253]\n",
      "epoch:7 step:7304 [D loss: 0.235262, acc.: 58.59%] [G loss: 0.476449]\n",
      "epoch:7 step:7305 [D loss: 0.200808, acc.: 67.97%] [G loss: 0.516351]\n",
      "epoch:7 step:7306 [D loss: 0.195190, acc.: 67.97%] [G loss: 0.505928]\n",
      "epoch:7 step:7307 [D loss: 0.228909, acc.: 60.16%] [G loss: 0.498537]\n",
      "epoch:7 step:7308 [D loss: 0.192612, acc.: 69.53%] [G loss: 0.507600]\n",
      "epoch:7 step:7309 [D loss: 0.205953, acc.: 65.62%] [G loss: 0.500681]\n",
      "epoch:7 step:7310 [D loss: 0.187924, acc.: 67.97%] [G loss: 0.481338]\n",
      "epoch:7 step:7311 [D loss: 0.228672, acc.: 66.41%] [G loss: 0.451135]\n",
      "epoch:7 step:7312 [D loss: 0.202484, acc.: 68.75%] [G loss: 0.460711]\n",
      "epoch:7 step:7313 [D loss: 0.214666, acc.: 64.84%] [G loss: 0.509783]\n",
      "epoch:7 step:7314 [D loss: 0.248016, acc.: 60.16%] [G loss: 0.498786]\n",
      "epoch:7 step:7315 [D loss: 0.212056, acc.: 66.41%] [G loss: 0.510553]\n",
      "epoch:7 step:7316 [D loss: 0.229611, acc.: 64.06%] [G loss: 0.480261]\n",
      "epoch:7 step:7317 [D loss: 0.222787, acc.: 63.28%] [G loss: 0.486011]\n",
      "epoch:7 step:7318 [D loss: 0.229839, acc.: 61.72%] [G loss: 0.501467]\n",
      "epoch:7 step:7319 [D loss: 0.224365, acc.: 61.72%] [G loss: 0.451576]\n",
      "epoch:7 step:7320 [D loss: 0.225525, acc.: 64.84%] [G loss: 0.468551]\n",
      "epoch:7 step:7321 [D loss: 0.210091, acc.: 68.75%] [G loss: 0.472594]\n",
      "epoch:7 step:7322 [D loss: 0.209249, acc.: 67.97%] [G loss: 0.471250]\n",
      "epoch:7 step:7323 [D loss: 0.216690, acc.: 62.50%] [G loss: 0.447760]\n",
      "epoch:7 step:7324 [D loss: 0.281930, acc.: 53.91%] [G loss: 0.485347]\n",
      "epoch:7 step:7325 [D loss: 0.221389, acc.: 66.41%] [G loss: 0.530704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7326 [D loss: 0.215143, acc.: 63.28%] [G loss: 0.534071]\n",
      "epoch:7 step:7327 [D loss: 0.236247, acc.: 60.94%] [G loss: 0.471192]\n",
      "epoch:7 step:7328 [D loss: 0.204399, acc.: 66.41%] [G loss: 0.517575]\n",
      "epoch:7 step:7329 [D loss: 0.208955, acc.: 69.53%] [G loss: 0.535389]\n",
      "epoch:7 step:7330 [D loss: 0.221414, acc.: 67.19%] [G loss: 0.460739]\n",
      "epoch:7 step:7331 [D loss: 0.227337, acc.: 59.38%] [G loss: 0.479580]\n",
      "epoch:7 step:7332 [D loss: 0.222708, acc.: 63.28%] [G loss: 0.454201]\n",
      "epoch:7 step:7333 [D loss: 0.213466, acc.: 67.97%] [G loss: 0.497104]\n",
      "epoch:7 step:7334 [D loss: 0.232882, acc.: 62.50%] [G loss: 0.458005]\n",
      "epoch:7 step:7335 [D loss: 0.214268, acc.: 63.28%] [G loss: 0.498995]\n",
      "epoch:7 step:7336 [D loss: 0.210896, acc.: 64.06%] [G loss: 0.496457]\n",
      "epoch:7 step:7337 [D loss: 0.237737, acc.: 50.78%] [G loss: 0.455719]\n",
      "epoch:7 step:7338 [D loss: 0.226485, acc.: 60.94%] [G loss: 0.477989]\n",
      "epoch:7 step:7339 [D loss: 0.183040, acc.: 75.00%] [G loss: 0.497671]\n",
      "epoch:7 step:7340 [D loss: 0.180915, acc.: 76.56%] [G loss: 0.522889]\n",
      "epoch:7 step:7341 [D loss: 0.195472, acc.: 71.88%] [G loss: 0.576575]\n",
      "epoch:7 step:7342 [D loss: 0.238038, acc.: 60.94%] [G loss: 0.526814]\n",
      "epoch:7 step:7343 [D loss: 0.261569, acc.: 55.47%] [G loss: 0.462553]\n",
      "epoch:7 step:7344 [D loss: 0.229772, acc.: 60.94%] [G loss: 0.438262]\n",
      "epoch:7 step:7345 [D loss: 0.193076, acc.: 74.22%] [G loss: 0.510616]\n",
      "epoch:7 step:7346 [D loss: 0.243518, acc.: 64.06%] [G loss: 0.459822]\n",
      "epoch:7 step:7347 [D loss: 0.221453, acc.: 63.28%] [G loss: 0.438444]\n",
      "epoch:7 step:7348 [D loss: 0.184012, acc.: 73.44%] [G loss: 0.469634]\n",
      "epoch:7 step:7349 [D loss: 0.199344, acc.: 70.31%] [G loss: 0.515917]\n",
      "epoch:7 step:7350 [D loss: 0.251501, acc.: 54.69%] [G loss: 0.454717]\n",
      "epoch:7 step:7351 [D loss: 0.187451, acc.: 78.12%] [G loss: 0.533258]\n",
      "epoch:7 step:7352 [D loss: 0.225893, acc.: 63.28%] [G loss: 0.496579]\n",
      "epoch:7 step:7353 [D loss: 0.259415, acc.: 53.12%] [G loss: 0.465293]\n",
      "epoch:7 step:7354 [D loss: 0.250082, acc.: 60.94%] [G loss: 0.465841]\n",
      "epoch:7 step:7355 [D loss: 0.182001, acc.: 75.78%] [G loss: 0.516394]\n",
      "epoch:7 step:7356 [D loss: 0.209830, acc.: 63.28%] [G loss: 0.474368]\n",
      "epoch:7 step:7357 [D loss: 0.220494, acc.: 62.50%] [G loss: 0.483303]\n",
      "epoch:7 step:7358 [D loss: 0.201629, acc.: 67.97%] [G loss: 0.535221]\n",
      "epoch:7 step:7359 [D loss: 0.235391, acc.: 60.16%] [G loss: 0.454993]\n",
      "epoch:7 step:7360 [D loss: 0.204977, acc.: 69.53%] [G loss: 0.530494]\n",
      "epoch:7 step:7361 [D loss: 0.204574, acc.: 67.19%] [G loss: 0.540235]\n",
      "epoch:7 step:7362 [D loss: 0.200530, acc.: 72.66%] [G loss: 0.550101]\n",
      "epoch:7 step:7363 [D loss: 0.222745, acc.: 57.81%] [G loss: 0.466137]\n",
      "epoch:7 step:7364 [D loss: 0.217370, acc.: 64.06%] [G loss: 0.493333]\n",
      "epoch:7 step:7365 [D loss: 0.222599, acc.: 64.84%] [G loss: 0.443875]\n",
      "epoch:7 step:7366 [D loss: 0.173657, acc.: 76.56%] [G loss: 0.524891]\n",
      "epoch:7 step:7367 [D loss: 0.225282, acc.: 63.28%] [G loss: 0.478932]\n",
      "epoch:7 step:7368 [D loss: 0.226752, acc.: 64.06%] [G loss: 0.473616]\n",
      "epoch:7 step:7369 [D loss: 0.217651, acc.: 62.50%] [G loss: 0.455430]\n",
      "epoch:7 step:7370 [D loss: 0.225595, acc.: 66.41%] [G loss: 0.463720]\n",
      "epoch:7 step:7371 [D loss: 0.257675, acc.: 57.03%] [G loss: 0.468177]\n",
      "epoch:7 step:7372 [D loss: 0.215373, acc.: 64.06%] [G loss: 0.495203]\n",
      "epoch:7 step:7373 [D loss: 0.210337, acc.: 67.97%] [G loss: 0.450481]\n",
      "epoch:7 step:7374 [D loss: 0.195993, acc.: 69.53%] [G loss: 0.546142]\n",
      "epoch:7 step:7375 [D loss: 0.237695, acc.: 62.50%] [G loss: 0.535121]\n",
      "epoch:7 step:7376 [D loss: 0.220343, acc.: 63.28%] [G loss: 0.484318]\n",
      "epoch:7 step:7377 [D loss: 0.255812, acc.: 54.69%] [G loss: 0.440593]\n",
      "epoch:7 step:7378 [D loss: 0.203020, acc.: 67.97%] [G loss: 0.475907]\n",
      "epoch:7 step:7379 [D loss: 0.260584, acc.: 59.38%] [G loss: 0.467666]\n",
      "epoch:7 step:7380 [D loss: 0.224631, acc.: 58.59%] [G loss: 0.439228]\n",
      "epoch:7 step:7381 [D loss: 0.192823, acc.: 67.97%] [G loss: 0.526619]\n",
      "epoch:7 step:7382 [D loss: 0.198598, acc.: 69.53%] [G loss: 0.549314]\n",
      "epoch:7 step:7383 [D loss: 0.243290, acc.: 56.25%] [G loss: 0.473667]\n",
      "epoch:7 step:7384 [D loss: 0.212986, acc.: 65.62%] [G loss: 0.469337]\n",
      "epoch:7 step:7385 [D loss: 0.228382, acc.: 63.28%] [G loss: 0.498070]\n",
      "epoch:7 step:7386 [D loss: 0.241787, acc.: 55.47%] [G loss: 0.479276]\n",
      "epoch:7 step:7387 [D loss: 0.251967, acc.: 58.59%] [G loss: 0.482914]\n",
      "epoch:7 step:7388 [D loss: 0.235302, acc.: 59.38%] [G loss: 0.484857]\n",
      "epoch:7 step:7389 [D loss: 0.247091, acc.: 58.59%] [G loss: 0.481469]\n",
      "epoch:7 step:7390 [D loss: 0.213783, acc.: 67.97%] [G loss: 0.498905]\n",
      "epoch:7 step:7391 [D loss: 0.196937, acc.: 69.53%] [G loss: 0.478867]\n",
      "epoch:7 step:7392 [D loss: 0.222789, acc.: 67.97%] [G loss: 0.486750]\n",
      "epoch:7 step:7393 [D loss: 0.203095, acc.: 64.84%] [G loss: 0.472761]\n",
      "epoch:7 step:7394 [D loss: 0.211952, acc.: 67.19%] [G loss: 0.480668]\n",
      "epoch:7 step:7395 [D loss: 0.251464, acc.: 59.38%] [G loss: 0.457300]\n",
      "epoch:7 step:7396 [D loss: 0.202886, acc.: 68.75%] [G loss: 0.485701]\n",
      "epoch:7 step:7397 [D loss: 0.207952, acc.: 67.97%] [G loss: 0.470779]\n",
      "epoch:7 step:7398 [D loss: 0.210348, acc.: 63.28%] [G loss: 0.459467]\n",
      "epoch:7 step:7399 [D loss: 0.215506, acc.: 64.84%] [G loss: 0.477239]\n",
      "epoch:7 step:7400 [D loss: 0.192629, acc.: 71.09%] [G loss: 0.516696]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.224747\n",
      "FID: 17.629412\n",
      "0 = 12.151982283282276\n",
      "1 = 0.058343990805588185\n",
      "2 = 0.927049994468689\n",
      "3 = 0.9064000248908997\n",
      "4 = 0.947700023651123\n",
      "5 = 0.9454469680786133\n",
      "6 = 0.9064000248908997\n",
      "7 = 7.237993097412583\n",
      "8 = 0.09502999384362515\n",
      "9 = 0.7547000050544739\n",
      "10 = 0.7494999766349792\n",
      "11 = 0.7598999738693237\n",
      "12 = 0.7573767304420471\n",
      "13 = 0.7494999766349792\n",
      "14 = 7.224816799163818\n",
      "15 = 9.487767219543457\n",
      "16 = 0.13868896663188934\n",
      "17 = 7.224747180938721\n",
      "18 = 17.629411697387695\n",
      "epoch:7 step:7401 [D loss: 0.182971, acc.: 71.88%] [G loss: 0.491685]\n",
      "epoch:7 step:7402 [D loss: 0.252644, acc.: 59.38%] [G loss: 0.469752]\n",
      "epoch:7 step:7403 [D loss: 0.191506, acc.: 72.66%] [G loss: 0.545008]\n",
      "epoch:7 step:7404 [D loss: 0.249209, acc.: 54.69%] [G loss: 0.468950]\n",
      "epoch:7 step:7405 [D loss: 0.228082, acc.: 58.59%] [G loss: 0.486580]\n",
      "epoch:7 step:7406 [D loss: 0.210654, acc.: 67.19%] [G loss: 0.432159]\n",
      "epoch:7 step:7407 [D loss: 0.224699, acc.: 57.81%] [G loss: 0.451535]\n",
      "epoch:7 step:7408 [D loss: 0.211506, acc.: 65.62%] [G loss: 0.436071]\n",
      "epoch:7 step:7409 [D loss: 0.250089, acc.: 51.56%] [G loss: 0.428456]\n",
      "epoch:7 step:7410 [D loss: 0.254591, acc.: 57.81%] [G loss: 0.455335]\n",
      "epoch:7 step:7411 [D loss: 0.208264, acc.: 67.19%] [G loss: 0.463651]\n",
      "epoch:7 step:7412 [D loss: 0.205215, acc.: 64.84%] [G loss: 0.487055]\n",
      "epoch:7 step:7413 [D loss: 0.199383, acc.: 73.44%] [G loss: 0.504104]\n",
      "epoch:7 step:7414 [D loss: 0.225294, acc.: 59.38%] [G loss: 0.478095]\n",
      "epoch:7 step:7415 [D loss: 0.222814, acc.: 55.47%] [G loss: 0.487884]\n",
      "epoch:7 step:7416 [D loss: 0.234617, acc.: 62.50%] [G loss: 0.455097]\n",
      "epoch:7 step:7417 [D loss: 0.277736, acc.: 51.56%] [G loss: 0.467538]\n",
      "epoch:7 step:7418 [D loss: 0.249262, acc.: 56.25%] [G loss: 0.488136]\n",
      "epoch:7 step:7419 [D loss: 0.213868, acc.: 67.19%] [G loss: 0.512298]\n",
      "epoch:7 step:7420 [D loss: 0.243995, acc.: 57.81%] [G loss: 0.509224]\n",
      "epoch:7 step:7421 [D loss: 0.210499, acc.: 64.06%] [G loss: 0.484015]\n",
      "epoch:7 step:7422 [D loss: 0.209473, acc.: 68.75%] [G loss: 0.470077]\n",
      "epoch:7 step:7423 [D loss: 0.209478, acc.: 60.16%] [G loss: 0.474511]\n",
      "epoch:7 step:7424 [D loss: 0.234565, acc.: 63.28%] [G loss: 0.489037]\n",
      "epoch:7 step:7425 [D loss: 0.215238, acc.: 64.06%] [G loss: 0.461236]\n",
      "epoch:7 step:7426 [D loss: 0.263143, acc.: 54.69%] [G loss: 0.447266]\n",
      "epoch:7 step:7427 [D loss: 0.216557, acc.: 66.41%] [G loss: 0.486337]\n",
      "epoch:7 step:7428 [D loss: 0.210585, acc.: 68.75%] [G loss: 0.485425]\n",
      "epoch:7 step:7429 [D loss: 0.210025, acc.: 67.97%] [G loss: 0.457797]\n",
      "epoch:7 step:7430 [D loss: 0.235074, acc.: 62.50%] [G loss: 0.484933]\n",
      "epoch:7 step:7431 [D loss: 0.206900, acc.: 66.41%] [G loss: 0.524887]\n",
      "epoch:7 step:7432 [D loss: 0.238497, acc.: 60.94%] [G loss: 0.454832]\n",
      "epoch:7 step:7433 [D loss: 0.222715, acc.: 62.50%] [G loss: 0.503116]\n",
      "epoch:7 step:7434 [D loss: 0.171609, acc.: 75.78%] [G loss: 0.546614]\n",
      "epoch:7 step:7435 [D loss: 0.229986, acc.: 61.72%] [G loss: 0.534248]\n",
      "epoch:7 step:7436 [D loss: 0.220718, acc.: 64.84%] [G loss: 0.469439]\n",
      "epoch:7 step:7437 [D loss: 0.229026, acc.: 64.06%] [G loss: 0.448869]\n",
      "epoch:7 step:7438 [D loss: 0.225910, acc.: 61.72%] [G loss: 0.490348]\n",
      "epoch:7 step:7439 [D loss: 0.232028, acc.: 62.50%] [G loss: 0.487530]\n",
      "epoch:7 step:7440 [D loss: 0.230462, acc.: 63.28%] [G loss: 0.442600]\n",
      "epoch:7 step:7441 [D loss: 0.211796, acc.: 64.84%] [G loss: 0.464160]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7442 [D loss: 0.231088, acc.: 64.06%] [G loss: 0.517531]\n",
      "epoch:7 step:7443 [D loss: 0.182201, acc.: 72.66%] [G loss: 0.538720]\n",
      "epoch:7 step:7444 [D loss: 0.216648, acc.: 64.84%] [G loss: 0.556444]\n",
      "epoch:7 step:7445 [D loss: 0.193110, acc.: 74.22%] [G loss: 0.508787]\n",
      "epoch:7 step:7446 [D loss: 0.200236, acc.: 71.09%] [G loss: 0.475394]\n",
      "epoch:7 step:7447 [D loss: 0.207141, acc.: 65.62%] [G loss: 0.502954]\n",
      "epoch:7 step:7448 [D loss: 0.210158, acc.: 67.19%] [G loss: 0.488331]\n",
      "epoch:7 step:7449 [D loss: 0.200450, acc.: 72.66%] [G loss: 0.554907]\n",
      "epoch:7 step:7450 [D loss: 0.260276, acc.: 54.69%] [G loss: 0.447874]\n",
      "epoch:7 step:7451 [D loss: 0.261655, acc.: 50.78%] [G loss: 0.445774]\n",
      "epoch:7 step:7452 [D loss: 0.209565, acc.: 68.75%] [G loss: 0.488550]\n",
      "epoch:7 step:7453 [D loss: 0.179829, acc.: 74.22%] [G loss: 0.524384]\n",
      "epoch:7 step:7454 [D loss: 0.217030, acc.: 65.62%] [G loss: 0.519113]\n",
      "epoch:7 step:7455 [D loss: 0.189255, acc.: 72.66%] [G loss: 0.512977]\n",
      "epoch:7 step:7456 [D loss: 0.209387, acc.: 66.41%] [G loss: 0.478353]\n",
      "epoch:7 step:7457 [D loss: 0.231512, acc.: 63.28%] [G loss: 0.522948]\n",
      "epoch:7 step:7458 [D loss: 0.174344, acc.: 76.56%] [G loss: 0.504375]\n",
      "epoch:7 step:7459 [D loss: 0.180156, acc.: 75.78%] [G loss: 0.523294]\n",
      "epoch:7 step:7460 [D loss: 0.234416, acc.: 64.06%] [G loss: 0.478637]\n",
      "epoch:7 step:7461 [D loss: 0.226012, acc.: 58.59%] [G loss: 0.450214]\n",
      "epoch:7 step:7462 [D loss: 0.206749, acc.: 68.75%] [G loss: 0.472077]\n",
      "epoch:7 step:7463 [D loss: 0.227046, acc.: 65.62%] [G loss: 0.480248]\n",
      "epoch:7 step:7464 [D loss: 0.225994, acc.: 72.66%] [G loss: 0.496684]\n",
      "epoch:7 step:7465 [D loss: 0.174911, acc.: 75.78%] [G loss: 0.546585]\n",
      "epoch:7 step:7466 [D loss: 0.203134, acc.: 69.53%] [G loss: 0.519597]\n",
      "epoch:7 step:7467 [D loss: 0.228024, acc.: 63.28%] [G loss: 0.447035]\n",
      "epoch:7 step:7468 [D loss: 0.184815, acc.: 75.78%] [G loss: 0.501600]\n",
      "epoch:7 step:7469 [D loss: 0.216448, acc.: 64.06%] [G loss: 0.502005]\n",
      "epoch:7 step:7470 [D loss: 0.195334, acc.: 71.09%] [G loss: 0.504506]\n",
      "epoch:7 step:7471 [D loss: 0.175193, acc.: 77.34%] [G loss: 0.540350]\n",
      "epoch:7 step:7472 [D loss: 0.236723, acc.: 64.06%] [G loss: 0.513059]\n",
      "epoch:7 step:7473 [D loss: 0.200264, acc.: 72.66%] [G loss: 0.500462]\n",
      "epoch:7 step:7474 [D loss: 0.239179, acc.: 59.38%] [G loss: 0.457968]\n",
      "epoch:7 step:7475 [D loss: 0.231134, acc.: 59.38%] [G loss: 0.468005]\n",
      "epoch:7 step:7476 [D loss: 0.222133, acc.: 65.62%] [G loss: 0.469911]\n",
      "epoch:7 step:7477 [D loss: 0.175582, acc.: 75.00%] [G loss: 0.557562]\n",
      "epoch:7 step:7478 [D loss: 0.209749, acc.: 67.97%] [G loss: 0.549268]\n",
      "epoch:7 step:7479 [D loss: 0.302932, acc.: 50.00%] [G loss: 0.488655]\n",
      "epoch:7 step:7480 [D loss: 0.201370, acc.: 66.41%] [G loss: 0.480231]\n",
      "epoch:7 step:7481 [D loss: 0.246822, acc.: 60.16%] [G loss: 0.466084]\n",
      "epoch:7 step:7482 [D loss: 0.196538, acc.: 70.31%] [G loss: 0.516392]\n",
      "epoch:7 step:7483 [D loss: 0.171472, acc.: 79.69%] [G loss: 0.567349]\n",
      "epoch:7 step:7484 [D loss: 0.179790, acc.: 75.78%] [G loss: 0.549136]\n",
      "epoch:7 step:7485 [D loss: 0.189590, acc.: 72.66%] [G loss: 0.600617]\n",
      "epoch:7 step:7486 [D loss: 0.160293, acc.: 77.34%] [G loss: 0.602562]\n",
      "epoch:7 step:7487 [D loss: 0.379089, acc.: 56.25%] [G loss: 0.523842]\n",
      "epoch:7 step:7488 [D loss: 0.167267, acc.: 72.66%] [G loss: 0.666909]\n",
      "epoch:7 step:7489 [D loss: 0.219215, acc.: 66.41%] [G loss: 0.542450]\n",
      "epoch:7 step:7490 [D loss: 0.268023, acc.: 49.22%] [G loss: 0.455343]\n",
      "epoch:7 step:7491 [D loss: 0.210942, acc.: 63.28%] [G loss: 0.479524]\n",
      "epoch:7 step:7492 [D loss: 0.197734, acc.: 68.75%] [G loss: 0.566218]\n",
      "epoch:7 step:7493 [D loss: 0.215519, acc.: 65.62%] [G loss: 0.532761]\n",
      "epoch:7 step:7494 [D loss: 0.215019, acc.: 68.75%] [G loss: 0.502652]\n",
      "epoch:7 step:7495 [D loss: 0.163239, acc.: 80.47%] [G loss: 0.568385]\n",
      "epoch:7 step:7496 [D loss: 0.170446, acc.: 78.12%] [G loss: 0.650277]\n",
      "epoch:8 step:7497 [D loss: 0.265172, acc.: 58.59%] [G loss: 0.550408]\n",
      "epoch:8 step:7498 [D loss: 0.253175, acc.: 57.03%] [G loss: 0.519028]\n",
      "epoch:8 step:7499 [D loss: 0.229582, acc.: 62.50%] [G loss: 0.491480]\n",
      "epoch:8 step:7500 [D loss: 0.231126, acc.: 60.94%] [G loss: 0.509145]\n",
      "epoch:8 step:7501 [D loss: 0.199602, acc.: 68.75%] [G loss: 0.536290]\n",
      "epoch:8 step:7502 [D loss: 0.227480, acc.: 60.94%] [G loss: 0.518584]\n",
      "epoch:8 step:7503 [D loss: 0.236682, acc.: 62.50%] [G loss: 0.485586]\n",
      "epoch:8 step:7504 [D loss: 0.208164, acc.: 64.84%] [G loss: 0.468128]\n",
      "epoch:8 step:7505 [D loss: 0.183701, acc.: 78.12%] [G loss: 0.564080]\n",
      "epoch:8 step:7506 [D loss: 0.220526, acc.: 64.06%] [G loss: 0.498989]\n",
      "epoch:8 step:7507 [D loss: 0.209716, acc.: 66.41%] [G loss: 0.511914]\n",
      "epoch:8 step:7508 [D loss: 0.201664, acc.: 69.53%] [G loss: 0.514707]\n",
      "epoch:8 step:7509 [D loss: 0.208382, acc.: 60.94%] [G loss: 0.445677]\n",
      "epoch:8 step:7510 [D loss: 0.233966, acc.: 63.28%] [G loss: 0.489341]\n",
      "epoch:8 step:7511 [D loss: 0.197493, acc.: 65.62%] [G loss: 0.553828]\n",
      "epoch:8 step:7512 [D loss: 0.202366, acc.: 69.53%] [G loss: 0.555082]\n",
      "epoch:8 step:7513 [D loss: 0.232420, acc.: 64.06%] [G loss: 0.491387]\n",
      "epoch:8 step:7514 [D loss: 0.248510, acc.: 53.91%] [G loss: 0.459778]\n",
      "epoch:8 step:7515 [D loss: 0.230035, acc.: 65.62%] [G loss: 0.462661]\n",
      "epoch:8 step:7516 [D loss: 0.241140, acc.: 57.03%] [G loss: 0.501325]\n",
      "epoch:8 step:7517 [D loss: 0.237072, acc.: 60.94%] [G loss: 0.504580]\n",
      "epoch:8 step:7518 [D loss: 0.211268, acc.: 66.41%] [G loss: 0.544087]\n",
      "epoch:8 step:7519 [D loss: 0.216170, acc.: 60.16%] [G loss: 0.525702]\n",
      "epoch:8 step:7520 [D loss: 0.227147, acc.: 63.28%] [G loss: 0.472704]\n",
      "epoch:8 step:7521 [D loss: 0.206680, acc.: 70.31%] [G loss: 0.515592]\n",
      "epoch:8 step:7522 [D loss: 0.228870, acc.: 61.72%] [G loss: 0.511753]\n",
      "epoch:8 step:7523 [D loss: 0.228331, acc.: 64.84%] [G loss: 0.450859]\n",
      "epoch:8 step:7524 [D loss: 0.198381, acc.: 66.41%] [G loss: 0.489538]\n",
      "epoch:8 step:7525 [D loss: 0.191894, acc.: 73.44%] [G loss: 0.518656]\n",
      "epoch:8 step:7526 [D loss: 0.217714, acc.: 65.62%] [G loss: 0.507909]\n",
      "epoch:8 step:7527 [D loss: 0.197230, acc.: 71.88%] [G loss: 0.499499]\n",
      "epoch:8 step:7528 [D loss: 0.224771, acc.: 64.84%] [G loss: 0.490150]\n",
      "epoch:8 step:7529 [D loss: 0.197813, acc.: 72.66%] [G loss: 0.522882]\n",
      "epoch:8 step:7530 [D loss: 0.249106, acc.: 58.59%] [G loss: 0.435500]\n",
      "epoch:8 step:7531 [D loss: 0.184592, acc.: 71.09%] [G loss: 0.528874]\n",
      "epoch:8 step:7532 [D loss: 0.175779, acc.: 69.53%] [G loss: 0.496634]\n",
      "epoch:8 step:7533 [D loss: 0.242664, acc.: 63.28%] [G loss: 0.467064]\n",
      "epoch:8 step:7534 [D loss: 0.225048, acc.: 64.84%] [G loss: 0.464112]\n",
      "epoch:8 step:7535 [D loss: 0.205128, acc.: 68.75%] [G loss: 0.454701]\n",
      "epoch:8 step:7536 [D loss: 0.167692, acc.: 78.91%] [G loss: 0.536992]\n",
      "epoch:8 step:7537 [D loss: 0.223948, acc.: 64.06%] [G loss: 0.487317]\n",
      "epoch:8 step:7538 [D loss: 0.198199, acc.: 67.97%] [G loss: 0.508593]\n",
      "epoch:8 step:7539 [D loss: 0.224423, acc.: 64.06%] [G loss: 0.426406]\n",
      "epoch:8 step:7540 [D loss: 0.256632, acc.: 53.91%] [G loss: 0.458745]\n",
      "epoch:8 step:7541 [D loss: 0.215379, acc.: 66.41%] [G loss: 0.467651]\n",
      "epoch:8 step:7542 [D loss: 0.217341, acc.: 64.84%] [G loss: 0.449590]\n",
      "epoch:8 step:7543 [D loss: 0.218525, acc.: 70.31%] [G loss: 0.483261]\n",
      "epoch:8 step:7544 [D loss: 0.212582, acc.: 69.53%] [G loss: 0.477091]\n",
      "epoch:8 step:7545 [D loss: 0.209236, acc.: 67.97%] [G loss: 0.497171]\n",
      "epoch:8 step:7546 [D loss: 0.197388, acc.: 69.53%] [G loss: 0.503457]\n",
      "epoch:8 step:7547 [D loss: 0.247127, acc.: 55.47%] [G loss: 0.458434]\n",
      "epoch:8 step:7548 [D loss: 0.184019, acc.: 76.56%] [G loss: 0.515536]\n",
      "epoch:8 step:7549 [D loss: 0.202704, acc.: 69.53%] [G loss: 0.506556]\n",
      "epoch:8 step:7550 [D loss: 0.224527, acc.: 64.84%] [G loss: 0.498955]\n",
      "epoch:8 step:7551 [D loss: 0.229577, acc.: 64.84%] [G loss: 0.464147]\n",
      "epoch:8 step:7552 [D loss: 0.207478, acc.: 68.75%] [G loss: 0.523498]\n",
      "epoch:8 step:7553 [D loss: 0.204088, acc.: 65.62%] [G loss: 0.473142]\n",
      "epoch:8 step:7554 [D loss: 0.218529, acc.: 69.53%] [G loss: 0.489762]\n",
      "epoch:8 step:7555 [D loss: 0.229868, acc.: 65.62%] [G loss: 0.479736]\n",
      "epoch:8 step:7556 [D loss: 0.235009, acc.: 62.50%] [G loss: 0.483687]\n",
      "epoch:8 step:7557 [D loss: 0.252387, acc.: 62.50%] [G loss: 0.444980]\n",
      "epoch:8 step:7558 [D loss: 0.253242, acc.: 53.91%] [G loss: 0.458426]\n",
      "epoch:8 step:7559 [D loss: 0.191005, acc.: 67.97%] [G loss: 0.483900]\n",
      "epoch:8 step:7560 [D loss: 0.233127, acc.: 67.19%] [G loss: 0.497535]\n",
      "epoch:8 step:7561 [D loss: 0.227145, acc.: 62.50%] [G loss: 0.474718]\n",
      "epoch:8 step:7562 [D loss: 0.223323, acc.: 64.84%] [G loss: 0.469872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7563 [D loss: 0.220262, acc.: 60.94%] [G loss: 0.517327]\n",
      "epoch:8 step:7564 [D loss: 0.209741, acc.: 66.41%] [G loss: 0.509025]\n",
      "epoch:8 step:7565 [D loss: 0.198492, acc.: 69.53%] [G loss: 0.465195]\n",
      "epoch:8 step:7566 [D loss: 0.203216, acc.: 67.97%] [G loss: 0.535913]\n",
      "epoch:8 step:7567 [D loss: 0.233854, acc.: 57.03%] [G loss: 0.510932]\n",
      "epoch:8 step:7568 [D loss: 0.228645, acc.: 59.38%] [G loss: 0.454700]\n",
      "epoch:8 step:7569 [D loss: 0.223281, acc.: 59.38%] [G loss: 0.442379]\n",
      "epoch:8 step:7570 [D loss: 0.226632, acc.: 61.72%] [G loss: 0.472267]\n",
      "epoch:8 step:7571 [D loss: 0.231456, acc.: 62.50%] [G loss: 0.462483]\n",
      "epoch:8 step:7572 [D loss: 0.197158, acc.: 69.53%] [G loss: 0.512584]\n",
      "epoch:8 step:7573 [D loss: 0.185126, acc.: 69.53%] [G loss: 0.522253]\n",
      "epoch:8 step:7574 [D loss: 0.258042, acc.: 50.78%] [G loss: 0.435961]\n",
      "epoch:8 step:7575 [D loss: 0.224608, acc.: 60.16%] [G loss: 0.452664]\n",
      "epoch:8 step:7576 [D loss: 0.223696, acc.: 66.41%] [G loss: 0.466997]\n",
      "epoch:8 step:7577 [D loss: 0.225404, acc.: 61.72%] [G loss: 0.440411]\n",
      "epoch:8 step:7578 [D loss: 0.217175, acc.: 63.28%] [G loss: 0.467536]\n",
      "epoch:8 step:7579 [D loss: 0.187045, acc.: 75.78%] [G loss: 0.503683]\n",
      "epoch:8 step:7580 [D loss: 0.215211, acc.: 70.31%] [G loss: 0.520369]\n",
      "epoch:8 step:7581 [D loss: 0.216046, acc.: 68.75%] [G loss: 0.510869]\n",
      "epoch:8 step:7582 [D loss: 0.223664, acc.: 60.16%] [G loss: 0.462154]\n",
      "epoch:8 step:7583 [D loss: 0.204726, acc.: 71.88%] [G loss: 0.466031]\n",
      "epoch:8 step:7584 [D loss: 0.203348, acc.: 70.31%] [G loss: 0.527798]\n",
      "epoch:8 step:7585 [D loss: 0.224430, acc.: 66.41%] [G loss: 0.525895]\n",
      "epoch:8 step:7586 [D loss: 0.198645, acc.: 67.97%] [G loss: 0.548490]\n",
      "epoch:8 step:7587 [D loss: 0.224848, acc.: 63.28%] [G loss: 0.498643]\n",
      "epoch:8 step:7588 [D loss: 0.224232, acc.: 66.41%] [G loss: 0.498453]\n",
      "epoch:8 step:7589 [D loss: 0.227918, acc.: 67.97%] [G loss: 0.472624]\n",
      "epoch:8 step:7590 [D loss: 0.216350, acc.: 61.72%] [G loss: 0.515662]\n",
      "epoch:8 step:7591 [D loss: 0.211972, acc.: 65.62%] [G loss: 0.500821]\n",
      "epoch:8 step:7592 [D loss: 0.179848, acc.: 73.44%] [G loss: 0.517162]\n",
      "epoch:8 step:7593 [D loss: 0.184217, acc.: 75.00%] [G loss: 0.544323]\n",
      "epoch:8 step:7594 [D loss: 0.217127, acc.: 57.81%] [G loss: 0.474124]\n",
      "epoch:8 step:7595 [D loss: 0.198331, acc.: 66.41%] [G loss: 0.494063]\n",
      "epoch:8 step:7596 [D loss: 0.180733, acc.: 71.88%] [G loss: 0.477313]\n",
      "epoch:8 step:7597 [D loss: 0.229045, acc.: 63.28%] [G loss: 0.454147]\n",
      "epoch:8 step:7598 [D loss: 0.227245, acc.: 59.38%] [G loss: 0.482035]\n",
      "epoch:8 step:7599 [D loss: 0.234207, acc.: 61.72%] [G loss: 0.448016]\n",
      "epoch:8 step:7600 [D loss: 0.235351, acc.: 57.81%] [G loss: 0.439208]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.410051\n",
      "FID: 14.284843\n",
      "0 = 12.067627091145486\n",
      "1 = 0.05409133759424308\n",
      "2 = 0.9207000136375427\n",
      "3 = 0.8924999833106995\n",
      "4 = 0.9488999843597412\n",
      "5 = 0.9458457231521606\n",
      "6 = 0.8924999833106995\n",
      "7 = 6.934292976331714\n",
      "8 = 0.08142300307085922\n",
      "9 = 0.7376000285148621\n",
      "10 = 0.7391999959945679\n",
      "11 = 0.7360000014305115\n",
      "12 = 0.7368420958518982\n",
      "13 = 0.7391999959945679\n",
      "14 = 7.410122394561768\n",
      "15 = 9.472955703735352\n",
      "16 = 0.13427698612213135\n",
      "17 = 7.410051345825195\n",
      "18 = 14.284843444824219\n",
      "epoch:8 step:7601 [D loss: 0.221804, acc.: 63.28%] [G loss: 0.469837]\n",
      "epoch:8 step:7602 [D loss: 0.228292, acc.: 65.62%] [G loss: 0.513263]\n",
      "epoch:8 step:7603 [D loss: 0.170297, acc.: 78.12%] [G loss: 0.528756]\n",
      "epoch:8 step:7604 [D loss: 0.334040, acc.: 48.44%] [G loss: 0.466710]\n",
      "epoch:8 step:7605 [D loss: 0.265494, acc.: 46.88%] [G loss: 0.466305]\n",
      "epoch:8 step:7606 [D loss: 0.236480, acc.: 64.84%] [G loss: 0.480046]\n",
      "epoch:8 step:7607 [D loss: 0.205453, acc.: 64.06%] [G loss: 0.517043]\n",
      "epoch:8 step:7608 [D loss: 0.201996, acc.: 69.53%] [G loss: 0.498248]\n",
      "epoch:8 step:7609 [D loss: 0.248126, acc.: 57.81%] [G loss: 0.466569]\n",
      "epoch:8 step:7610 [D loss: 0.240639, acc.: 61.72%] [G loss: 0.480271]\n",
      "epoch:8 step:7611 [D loss: 0.219992, acc.: 63.28%] [G loss: 0.539808]\n",
      "epoch:8 step:7612 [D loss: 0.212797, acc.: 67.19%] [G loss: 0.536037]\n",
      "epoch:8 step:7613 [D loss: 0.219496, acc.: 64.06%] [G loss: 0.493335]\n",
      "epoch:8 step:7614 [D loss: 0.220286, acc.: 66.41%] [G loss: 0.497730]\n",
      "epoch:8 step:7615 [D loss: 0.149378, acc.: 81.25%] [G loss: 0.550433]\n",
      "epoch:8 step:7616 [D loss: 0.276091, acc.: 56.25%] [G loss: 0.499393]\n",
      "epoch:8 step:7617 [D loss: 0.229845, acc.: 60.94%] [G loss: 0.496799]\n",
      "epoch:8 step:7618 [D loss: 0.183163, acc.: 69.53%] [G loss: 0.515392]\n",
      "epoch:8 step:7619 [D loss: 0.191816, acc.: 74.22%] [G loss: 0.525002]\n",
      "epoch:8 step:7620 [D loss: 0.229291, acc.: 61.72%] [G loss: 0.469762]\n",
      "epoch:8 step:7621 [D loss: 0.209224, acc.: 66.41%] [G loss: 0.467895]\n",
      "epoch:8 step:7622 [D loss: 0.188301, acc.: 69.53%] [G loss: 0.443116]\n",
      "epoch:8 step:7623 [D loss: 0.258733, acc.: 53.12%] [G loss: 0.421665]\n",
      "epoch:8 step:7624 [D loss: 0.219203, acc.: 61.72%] [G loss: 0.425299]\n",
      "epoch:8 step:7625 [D loss: 0.224399, acc.: 64.06%] [G loss: 0.451648]\n",
      "epoch:8 step:7626 [D loss: 0.213913, acc.: 67.19%] [G loss: 0.472470]\n",
      "epoch:8 step:7627 [D loss: 0.207612, acc.: 64.06%] [G loss: 0.512812]\n",
      "epoch:8 step:7628 [D loss: 0.232727, acc.: 63.28%] [G loss: 0.484309]\n",
      "epoch:8 step:7629 [D loss: 0.237120, acc.: 60.16%] [G loss: 0.477164]\n",
      "epoch:8 step:7630 [D loss: 0.211967, acc.: 67.19%] [G loss: 0.505479]\n",
      "epoch:8 step:7631 [D loss: 0.211306, acc.: 64.84%] [G loss: 0.488707]\n",
      "epoch:8 step:7632 [D loss: 0.203225, acc.: 67.19%] [G loss: 0.549463]\n",
      "epoch:8 step:7633 [D loss: 0.228367, acc.: 64.06%] [G loss: 0.472288]\n",
      "epoch:8 step:7634 [D loss: 0.201924, acc.: 67.97%] [G loss: 0.495473]\n",
      "epoch:8 step:7635 [D loss: 0.197918, acc.: 68.75%] [G loss: 0.468826]\n",
      "epoch:8 step:7636 [D loss: 0.222159, acc.: 67.19%] [G loss: 0.460219]\n",
      "epoch:8 step:7637 [D loss: 0.226615, acc.: 64.06%] [G loss: 0.463803]\n",
      "epoch:8 step:7638 [D loss: 0.235265, acc.: 60.16%] [G loss: 0.453912]\n",
      "epoch:8 step:7639 [D loss: 0.236543, acc.: 60.16%] [G loss: 0.512511]\n",
      "epoch:8 step:7640 [D loss: 0.204171, acc.: 66.41%] [G loss: 0.515822]\n",
      "epoch:8 step:7641 [D loss: 0.215459, acc.: 64.84%] [G loss: 0.507574]\n",
      "epoch:8 step:7642 [D loss: 0.209224, acc.: 66.41%] [G loss: 0.455697]\n",
      "epoch:8 step:7643 [D loss: 0.260085, acc.: 54.69%] [G loss: 0.413177]\n",
      "epoch:8 step:7644 [D loss: 0.242810, acc.: 59.38%] [G loss: 0.431943]\n",
      "epoch:8 step:7645 [D loss: 0.187020, acc.: 71.09%] [G loss: 0.457410]\n",
      "epoch:8 step:7646 [D loss: 0.233146, acc.: 63.28%] [G loss: 0.435370]\n",
      "epoch:8 step:7647 [D loss: 0.187812, acc.: 75.78%] [G loss: 0.500040]\n",
      "epoch:8 step:7648 [D loss: 0.189974, acc.: 68.75%] [G loss: 0.505658]\n",
      "epoch:8 step:7649 [D loss: 0.255100, acc.: 57.03%] [G loss: 0.479428]\n",
      "epoch:8 step:7650 [D loss: 0.208839, acc.: 66.41%] [G loss: 0.516940]\n",
      "epoch:8 step:7651 [D loss: 0.203096, acc.: 68.75%] [G loss: 0.496844]\n",
      "epoch:8 step:7652 [D loss: 0.202229, acc.: 71.09%] [G loss: 0.491804]\n",
      "epoch:8 step:7653 [D loss: 0.233537, acc.: 62.50%] [G loss: 0.445239]\n",
      "epoch:8 step:7654 [D loss: 0.212269, acc.: 60.94%] [G loss: 0.473839]\n",
      "epoch:8 step:7655 [D loss: 0.212092, acc.: 69.53%] [G loss: 0.444664]\n",
      "epoch:8 step:7656 [D loss: 0.255396, acc.: 56.25%] [G loss: 0.409891]\n",
      "epoch:8 step:7657 [D loss: 0.230733, acc.: 64.84%] [G loss: 0.451886]\n",
      "epoch:8 step:7658 [D loss: 0.208781, acc.: 64.84%] [G loss: 0.508488]\n",
      "epoch:8 step:7659 [D loss: 0.240980, acc.: 58.59%] [G loss: 0.454270]\n",
      "epoch:8 step:7660 [D loss: 0.220065, acc.: 64.84%] [G loss: 0.465337]\n",
      "epoch:8 step:7661 [D loss: 0.196081, acc.: 68.75%] [G loss: 0.463657]\n",
      "epoch:8 step:7662 [D loss: 0.185427, acc.: 77.34%] [G loss: 0.510888]\n",
      "epoch:8 step:7663 [D loss: 0.234809, acc.: 60.94%] [G loss: 0.435883]\n",
      "epoch:8 step:7664 [D loss: 0.221912, acc.: 67.97%] [G loss: 0.534861]\n",
      "epoch:8 step:7665 [D loss: 0.224989, acc.: 62.50%] [G loss: 0.479522]\n",
      "epoch:8 step:7666 [D loss: 0.238312, acc.: 62.50%] [G loss: 0.469768]\n",
      "epoch:8 step:7667 [D loss: 0.201092, acc.: 70.31%] [G loss: 0.473703]\n",
      "epoch:8 step:7668 [D loss: 0.227410, acc.: 67.19%] [G loss: 0.498059]\n",
      "epoch:8 step:7669 [D loss: 0.212326, acc.: 60.94%] [G loss: 0.505850]\n",
      "epoch:8 step:7670 [D loss: 0.246437, acc.: 57.81%] [G loss: 0.454117]\n",
      "epoch:8 step:7671 [D loss: 0.215071, acc.: 66.41%] [G loss: 0.461281]\n",
      "epoch:8 step:7672 [D loss: 0.195062, acc.: 67.97%] [G loss: 0.474488]\n",
      "epoch:8 step:7673 [D loss: 0.245750, acc.: 57.81%] [G loss: 0.451742]\n",
      "epoch:8 step:7674 [D loss: 0.202616, acc.: 73.44%] [G loss: 0.492152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7675 [D loss: 0.195997, acc.: 69.53%] [G loss: 0.501682]\n",
      "epoch:8 step:7676 [D loss: 0.230001, acc.: 58.59%] [G loss: 0.471929]\n",
      "epoch:8 step:7677 [D loss: 0.253185, acc.: 55.47%] [G loss: 0.447790]\n",
      "epoch:8 step:7678 [D loss: 0.220812, acc.: 60.16%] [G loss: 0.457199]\n",
      "epoch:8 step:7679 [D loss: 0.257803, acc.: 55.47%] [G loss: 0.430315]\n",
      "epoch:8 step:7680 [D loss: 0.221410, acc.: 63.28%] [G loss: 0.442527]\n",
      "epoch:8 step:7681 [D loss: 0.220824, acc.: 59.38%] [G loss: 0.458229]\n",
      "epoch:8 step:7682 [D loss: 0.248473, acc.: 60.16%] [G loss: 0.502689]\n",
      "epoch:8 step:7683 [D loss: 0.214239, acc.: 64.06%] [G loss: 0.510843]\n",
      "epoch:8 step:7684 [D loss: 0.236708, acc.: 60.16%] [G loss: 0.479477]\n",
      "epoch:8 step:7685 [D loss: 0.208252, acc.: 67.19%] [G loss: 0.485986]\n",
      "epoch:8 step:7686 [D loss: 0.219513, acc.: 64.06%] [G loss: 0.489460]\n",
      "epoch:8 step:7687 [D loss: 0.219326, acc.: 65.62%] [G loss: 0.499023]\n",
      "epoch:8 step:7688 [D loss: 0.187484, acc.: 71.88%] [G loss: 0.506932]\n",
      "epoch:8 step:7689 [D loss: 0.213334, acc.: 63.28%] [G loss: 0.521103]\n",
      "epoch:8 step:7690 [D loss: 0.219159, acc.: 68.75%] [G loss: 0.452604]\n",
      "epoch:8 step:7691 [D loss: 0.233623, acc.: 64.84%] [G loss: 0.459995]\n",
      "epoch:8 step:7692 [D loss: 0.228177, acc.: 60.16%] [G loss: 0.500032]\n",
      "epoch:8 step:7693 [D loss: 0.220263, acc.: 60.16%] [G loss: 0.508388]\n",
      "epoch:8 step:7694 [D loss: 0.173972, acc.: 78.12%] [G loss: 0.531689]\n",
      "epoch:8 step:7695 [D loss: 0.236718, acc.: 57.03%] [G loss: 0.485630]\n",
      "epoch:8 step:7696 [D loss: 0.247688, acc.: 58.59%] [G loss: 0.473132]\n",
      "epoch:8 step:7697 [D loss: 0.214843, acc.: 67.19%] [G loss: 0.525315]\n",
      "epoch:8 step:7698 [D loss: 0.244456, acc.: 58.59%] [G loss: 0.480099]\n",
      "epoch:8 step:7699 [D loss: 0.250718, acc.: 57.03%] [G loss: 0.426590]\n",
      "epoch:8 step:7700 [D loss: 0.206733, acc.: 65.62%] [G loss: 0.484887]\n",
      "epoch:8 step:7701 [D loss: 0.192841, acc.: 71.09%] [G loss: 0.523435]\n",
      "epoch:8 step:7702 [D loss: 0.199899, acc.: 67.97%] [G loss: 0.518635]\n",
      "epoch:8 step:7703 [D loss: 0.220514, acc.: 68.75%] [G loss: 0.510891]\n",
      "epoch:8 step:7704 [D loss: 0.186211, acc.: 70.31%] [G loss: 0.540580]\n",
      "epoch:8 step:7705 [D loss: 0.191834, acc.: 74.22%] [G loss: 0.520964]\n",
      "epoch:8 step:7706 [D loss: 0.268788, acc.: 54.69%] [G loss: 0.435104]\n",
      "epoch:8 step:7707 [D loss: 0.246885, acc.: 56.25%] [G loss: 0.418869]\n",
      "epoch:8 step:7708 [D loss: 0.215811, acc.: 66.41%] [G loss: 0.469853]\n",
      "epoch:8 step:7709 [D loss: 0.205449, acc.: 65.62%] [G loss: 0.454115]\n",
      "epoch:8 step:7710 [D loss: 0.232789, acc.: 64.06%] [G loss: 0.455494]\n",
      "epoch:8 step:7711 [D loss: 0.237610, acc.: 57.03%] [G loss: 0.483350]\n",
      "epoch:8 step:7712 [D loss: 0.219465, acc.: 66.41%] [G loss: 0.444935]\n",
      "epoch:8 step:7713 [D loss: 0.211235, acc.: 64.06%] [G loss: 0.491315]\n",
      "epoch:8 step:7714 [D loss: 0.199820, acc.: 68.75%] [G loss: 0.526019]\n",
      "epoch:8 step:7715 [D loss: 0.208011, acc.: 70.31%] [G loss: 0.507948]\n",
      "epoch:8 step:7716 [D loss: 0.286934, acc.: 50.00%] [G loss: 0.481037]\n",
      "epoch:8 step:7717 [D loss: 0.171587, acc.: 76.56%] [G loss: 0.513342]\n",
      "epoch:8 step:7718 [D loss: 0.199806, acc.: 71.88%] [G loss: 0.535439]\n",
      "epoch:8 step:7719 [D loss: 0.182373, acc.: 70.31%] [G loss: 0.550908]\n",
      "epoch:8 step:7720 [D loss: 0.260023, acc.: 55.47%] [G loss: 0.475136]\n",
      "epoch:8 step:7721 [D loss: 0.246951, acc.: 57.03%] [G loss: 0.437859]\n",
      "epoch:8 step:7722 [D loss: 0.225803, acc.: 67.97%] [G loss: 0.418161]\n",
      "epoch:8 step:7723 [D loss: 0.201788, acc.: 68.75%] [G loss: 0.442998]\n",
      "epoch:8 step:7724 [D loss: 0.277532, acc.: 57.81%] [G loss: 0.428222]\n",
      "epoch:8 step:7725 [D loss: 0.212039, acc.: 69.53%] [G loss: 0.472892]\n",
      "epoch:8 step:7726 [D loss: 0.187166, acc.: 73.44%] [G loss: 0.532446]\n",
      "epoch:8 step:7727 [D loss: 0.160126, acc.: 79.69%] [G loss: 0.566230]\n",
      "epoch:8 step:7728 [D loss: 0.186541, acc.: 71.09%] [G loss: 0.543667]\n",
      "epoch:8 step:7729 [D loss: 0.251932, acc.: 60.94%] [G loss: 0.444416]\n",
      "epoch:8 step:7730 [D loss: 0.233562, acc.: 65.62%] [G loss: 0.476450]\n",
      "epoch:8 step:7731 [D loss: 0.202967, acc.: 67.19%] [G loss: 0.482541]\n",
      "epoch:8 step:7732 [D loss: 0.205553, acc.: 67.97%] [G loss: 0.443856]\n",
      "epoch:8 step:7733 [D loss: 0.231147, acc.: 60.94%] [G loss: 0.439201]\n",
      "epoch:8 step:7734 [D loss: 0.229920, acc.: 65.62%] [G loss: 0.427342]\n",
      "epoch:8 step:7735 [D loss: 0.185758, acc.: 73.44%] [G loss: 0.487722]\n",
      "epoch:8 step:7736 [D loss: 0.216043, acc.: 63.28%] [G loss: 0.462446]\n",
      "epoch:8 step:7737 [D loss: 0.209468, acc.: 72.66%] [G loss: 0.499874]\n",
      "epoch:8 step:7738 [D loss: 0.217647, acc.: 67.19%] [G loss: 0.519737]\n",
      "epoch:8 step:7739 [D loss: 0.199476, acc.: 72.66%] [G loss: 0.501028]\n",
      "epoch:8 step:7740 [D loss: 0.214635, acc.: 64.84%] [G loss: 0.497255]\n",
      "epoch:8 step:7741 [D loss: 0.223254, acc.: 57.81%] [G loss: 0.459823]\n",
      "epoch:8 step:7742 [D loss: 0.223495, acc.: 62.50%] [G loss: 0.488384]\n",
      "epoch:8 step:7743 [D loss: 0.180306, acc.: 75.00%] [G loss: 0.529634]\n",
      "epoch:8 step:7744 [D loss: 0.220209, acc.: 67.97%] [G loss: 0.536841]\n",
      "epoch:8 step:7745 [D loss: 0.269790, acc.: 52.34%] [G loss: 0.470905]\n",
      "epoch:8 step:7746 [D loss: 0.245228, acc.: 60.16%] [G loss: 0.498788]\n",
      "epoch:8 step:7747 [D loss: 0.237856, acc.: 57.81%] [G loss: 0.488068]\n",
      "epoch:8 step:7748 [D loss: 0.220656, acc.: 67.19%] [G loss: 0.504597]\n",
      "epoch:8 step:7749 [D loss: 0.212055, acc.: 68.75%] [G loss: 0.499216]\n",
      "epoch:8 step:7750 [D loss: 0.214652, acc.: 65.62%] [G loss: 0.524652]\n",
      "epoch:8 step:7751 [D loss: 0.212021, acc.: 67.19%] [G loss: 0.499463]\n",
      "epoch:8 step:7752 [D loss: 0.232743, acc.: 64.84%] [G loss: 0.471940]\n",
      "epoch:8 step:7753 [D loss: 0.202389, acc.: 68.75%] [G loss: 0.511092]\n",
      "epoch:8 step:7754 [D loss: 0.203456, acc.: 67.97%] [G loss: 0.418791]\n",
      "epoch:8 step:7755 [D loss: 0.194294, acc.: 73.44%] [G loss: 0.486809]\n",
      "epoch:8 step:7756 [D loss: 0.202553, acc.: 67.97%] [G loss: 0.510472]\n",
      "epoch:8 step:7757 [D loss: 0.214871, acc.: 67.97%] [G loss: 0.482858]\n",
      "epoch:8 step:7758 [D loss: 0.233015, acc.: 58.59%] [G loss: 0.457040]\n",
      "epoch:8 step:7759 [D loss: 0.262934, acc.: 52.34%] [G loss: 0.481560]\n",
      "epoch:8 step:7760 [D loss: 0.208416, acc.: 71.88%] [G loss: 0.462682]\n",
      "epoch:8 step:7761 [D loss: 0.245224, acc.: 57.03%] [G loss: 0.440059]\n",
      "epoch:8 step:7762 [D loss: 0.237150, acc.: 54.69%] [G loss: 0.416271]\n",
      "epoch:8 step:7763 [D loss: 0.224642, acc.: 63.28%] [G loss: 0.476642]\n",
      "epoch:8 step:7764 [D loss: 0.230316, acc.: 64.06%] [G loss: 0.464239]\n",
      "epoch:8 step:7765 [D loss: 0.222270, acc.: 66.41%] [G loss: 0.486153]\n",
      "epoch:8 step:7766 [D loss: 0.222985, acc.: 67.19%] [G loss: 0.476002]\n",
      "epoch:8 step:7767 [D loss: 0.186525, acc.: 73.44%] [G loss: 0.475636]\n",
      "epoch:8 step:7768 [D loss: 0.213534, acc.: 65.62%] [G loss: 0.514684]\n",
      "epoch:8 step:7769 [D loss: 0.207893, acc.: 65.62%] [G loss: 0.440415]\n",
      "epoch:8 step:7770 [D loss: 0.217623, acc.: 59.38%] [G loss: 0.420543]\n",
      "epoch:8 step:7771 [D loss: 0.235169, acc.: 61.72%] [G loss: 0.443902]\n",
      "epoch:8 step:7772 [D loss: 0.213719, acc.: 60.94%] [G loss: 0.502296]\n",
      "epoch:8 step:7773 [D loss: 0.252679, acc.: 53.12%] [G loss: 0.453702]\n",
      "epoch:8 step:7774 [D loss: 0.235634, acc.: 58.59%] [G loss: 0.444437]\n",
      "epoch:8 step:7775 [D loss: 0.226983, acc.: 63.28%] [G loss: 0.465663]\n",
      "epoch:8 step:7776 [D loss: 0.198015, acc.: 66.41%] [G loss: 0.471218]\n",
      "epoch:8 step:7777 [D loss: 0.266350, acc.: 54.69%] [G loss: 0.487896]\n",
      "epoch:8 step:7778 [D loss: 0.209555, acc.: 69.53%] [G loss: 0.495203]\n",
      "epoch:8 step:7779 [D loss: 0.240961, acc.: 63.28%] [G loss: 0.441952]\n",
      "epoch:8 step:7780 [D loss: 0.242731, acc.: 61.72%] [G loss: 0.450778]\n",
      "epoch:8 step:7781 [D loss: 0.204038, acc.: 69.53%] [G loss: 0.449400]\n",
      "epoch:8 step:7782 [D loss: 0.203945, acc.: 72.66%] [G loss: 0.450467]\n",
      "epoch:8 step:7783 [D loss: 0.225439, acc.: 64.84%] [G loss: 0.462122]\n",
      "epoch:8 step:7784 [D loss: 0.234756, acc.: 58.59%] [G loss: 0.418036]\n",
      "epoch:8 step:7785 [D loss: 0.213319, acc.: 62.50%] [G loss: 0.539531]\n",
      "epoch:8 step:7786 [D loss: 0.217258, acc.: 62.50%] [G loss: 0.501153]\n",
      "epoch:8 step:7787 [D loss: 0.229070, acc.: 64.84%] [G loss: 0.474036]\n",
      "epoch:8 step:7788 [D loss: 0.196841, acc.: 71.09%] [G loss: 0.521064]\n",
      "epoch:8 step:7789 [D loss: 0.234936, acc.: 60.16%] [G loss: 0.429935]\n",
      "epoch:8 step:7790 [D loss: 0.240105, acc.: 60.94%] [G loss: 0.436936]\n",
      "epoch:8 step:7791 [D loss: 0.237070, acc.: 60.16%] [G loss: 0.434753]\n",
      "epoch:8 step:7792 [D loss: 0.216192, acc.: 66.41%] [G loss: 0.442338]\n",
      "epoch:8 step:7793 [D loss: 0.200695, acc.: 67.97%] [G loss: 0.493285]\n",
      "epoch:8 step:7794 [D loss: 0.192664, acc.: 68.75%] [G loss: 0.496963]\n",
      "epoch:8 step:7795 [D loss: 0.193928, acc.: 67.19%] [G loss: 0.467560]\n",
      "epoch:8 step:7796 [D loss: 0.195775, acc.: 75.00%] [G loss: 0.496468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7797 [D loss: 0.242500, acc.: 60.16%] [G loss: 0.438562]\n",
      "epoch:8 step:7798 [D loss: 0.221241, acc.: 64.06%] [G loss: 0.492933]\n",
      "epoch:8 step:7799 [D loss: 0.205889, acc.: 68.75%] [G loss: 0.494970]\n",
      "epoch:8 step:7800 [D loss: 0.192380, acc.: 71.09%] [G loss: 0.488063]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.308998\n",
      "FID: 14.985348\n",
      "0 = 12.064682660222076\n",
      "1 = 0.05086803356715549\n",
      "2 = 0.9211500287055969\n",
      "3 = 0.8948000073432922\n",
      "4 = 0.9474999904632568\n",
      "5 = 0.9445793032646179\n",
      "6 = 0.8948000073432922\n",
      "7 = 7.02750667237639\n",
      "8 = 0.08455446724755916\n",
      "9 = 0.7489500045776367\n",
      "10 = 0.7444000244140625\n",
      "11 = 0.7534999847412109\n",
      "12 = 0.7512362599372864\n",
      "13 = 0.7444000244140625\n",
      "14 = 7.309061527252197\n",
      "15 = 9.507369995117188\n",
      "16 = 0.12938396632671356\n",
      "17 = 7.308997631072998\n",
      "18 = 14.985347747802734\n",
      "epoch:8 step:7801 [D loss: 0.201340, acc.: 70.31%] [G loss: 0.476794]\n",
      "epoch:8 step:7802 [D loss: 0.228828, acc.: 67.19%] [G loss: 0.464355]\n",
      "epoch:8 step:7803 [D loss: 0.227291, acc.: 64.06%] [G loss: 0.504476]\n",
      "epoch:8 step:7804 [D loss: 0.257074, acc.: 58.59%] [G loss: 0.485457]\n",
      "epoch:8 step:7805 [D loss: 0.200988, acc.: 67.97%] [G loss: 0.497128]\n",
      "epoch:8 step:7806 [D loss: 0.213067, acc.: 65.62%] [G loss: 0.494331]\n",
      "epoch:8 step:7807 [D loss: 0.190857, acc.: 65.62%] [G loss: 0.469593]\n",
      "epoch:8 step:7808 [D loss: 0.180842, acc.: 74.22%] [G loss: 0.525603]\n",
      "epoch:8 step:7809 [D loss: 0.183060, acc.: 70.31%] [G loss: 0.519718]\n",
      "epoch:8 step:7810 [D loss: 0.197127, acc.: 65.62%] [G loss: 0.567665]\n",
      "epoch:8 step:7811 [D loss: 0.186071, acc.: 72.66%] [G loss: 0.527610]\n",
      "epoch:8 step:7812 [D loss: 0.277655, acc.: 51.56%] [G loss: 0.496855]\n",
      "epoch:8 step:7813 [D loss: 0.229958, acc.: 58.59%] [G loss: 0.438273]\n",
      "epoch:8 step:7814 [D loss: 0.212018, acc.: 67.97%] [G loss: 0.485344]\n",
      "epoch:8 step:7815 [D loss: 0.194609, acc.: 67.97%] [G loss: 0.512022]\n",
      "epoch:8 step:7816 [D loss: 0.209391, acc.: 64.84%] [G loss: 0.489623]\n",
      "epoch:8 step:7817 [D loss: 0.188423, acc.: 72.66%] [G loss: 0.510515]\n",
      "epoch:8 step:7818 [D loss: 0.208709, acc.: 67.97%] [G loss: 0.513023]\n",
      "epoch:8 step:7819 [D loss: 0.239072, acc.: 60.94%] [G loss: 0.446990]\n",
      "epoch:8 step:7820 [D loss: 0.210770, acc.: 69.53%] [G loss: 0.465585]\n",
      "epoch:8 step:7821 [D loss: 0.207035, acc.: 67.19%] [G loss: 0.477352]\n",
      "epoch:8 step:7822 [D loss: 0.209638, acc.: 63.28%] [G loss: 0.499211]\n",
      "epoch:8 step:7823 [D loss: 0.223858, acc.: 65.62%] [G loss: 0.463774]\n",
      "epoch:8 step:7824 [D loss: 0.203594, acc.: 68.75%] [G loss: 0.498088]\n",
      "epoch:8 step:7825 [D loss: 0.200275, acc.: 67.97%] [G loss: 0.484533]\n",
      "epoch:8 step:7826 [D loss: 0.214510, acc.: 66.41%] [G loss: 0.448898]\n",
      "epoch:8 step:7827 [D loss: 0.204764, acc.: 68.75%] [G loss: 0.499784]\n",
      "epoch:8 step:7828 [D loss: 0.185477, acc.: 71.09%] [G loss: 0.484246]\n",
      "epoch:8 step:7829 [D loss: 0.188551, acc.: 70.31%] [G loss: 0.528672]\n",
      "epoch:8 step:7830 [D loss: 0.227441, acc.: 68.75%] [G loss: 0.532739]\n",
      "epoch:8 step:7831 [D loss: 0.205300, acc.: 65.62%] [G loss: 0.533166]\n",
      "epoch:8 step:7832 [D loss: 0.190928, acc.: 67.97%] [G loss: 0.522682]\n",
      "epoch:8 step:7833 [D loss: 0.183604, acc.: 71.09%] [G loss: 0.518850]\n",
      "epoch:8 step:7834 [D loss: 0.264343, acc.: 54.69%] [G loss: 0.468731]\n",
      "epoch:8 step:7835 [D loss: 0.207077, acc.: 66.41%] [G loss: 0.513177]\n",
      "epoch:8 step:7836 [D loss: 0.227140, acc.: 58.59%] [G loss: 0.496068]\n",
      "epoch:8 step:7837 [D loss: 0.269752, acc.: 57.81%] [G loss: 0.457330]\n",
      "epoch:8 step:7838 [D loss: 0.205847, acc.: 64.84%] [G loss: 0.503830]\n",
      "epoch:8 step:7839 [D loss: 0.195450, acc.: 72.66%] [G loss: 0.548224]\n",
      "epoch:8 step:7840 [D loss: 0.191972, acc.: 73.44%] [G loss: 0.521436]\n",
      "epoch:8 step:7841 [D loss: 0.205579, acc.: 67.19%] [G loss: 0.544426]\n",
      "epoch:8 step:7842 [D loss: 0.206480, acc.: 63.28%] [G loss: 0.544538]\n",
      "epoch:8 step:7843 [D loss: 0.179708, acc.: 71.09%] [G loss: 0.552609]\n",
      "epoch:8 step:7844 [D loss: 0.298568, acc.: 53.91%] [G loss: 0.458892]\n",
      "epoch:8 step:7845 [D loss: 0.237252, acc.: 56.25%] [G loss: 0.415815]\n",
      "epoch:8 step:7846 [D loss: 0.198677, acc.: 71.88%] [G loss: 0.484020]\n",
      "epoch:8 step:7847 [D loss: 0.204542, acc.: 66.41%] [G loss: 0.505732]\n",
      "epoch:8 step:7848 [D loss: 0.225920, acc.: 63.28%] [G loss: 0.492983]\n",
      "epoch:8 step:7849 [D loss: 0.205053, acc.: 66.41%] [G loss: 0.540034]\n",
      "epoch:8 step:7850 [D loss: 0.176164, acc.: 75.78%] [G loss: 0.544730]\n",
      "epoch:8 step:7851 [D loss: 0.236648, acc.: 64.84%] [G loss: 0.485363]\n",
      "epoch:8 step:7852 [D loss: 0.234316, acc.: 68.75%] [G loss: 0.464675]\n",
      "epoch:8 step:7853 [D loss: 0.225981, acc.: 66.41%] [G loss: 0.444346]\n",
      "epoch:8 step:7854 [D loss: 0.171819, acc.: 78.12%] [G loss: 0.542314]\n",
      "epoch:8 step:7855 [D loss: 0.193195, acc.: 70.31%] [G loss: 0.543383]\n",
      "epoch:8 step:7856 [D loss: 0.211102, acc.: 63.28%] [G loss: 0.509140]\n",
      "epoch:8 step:7857 [D loss: 0.185984, acc.: 74.22%] [G loss: 0.496108]\n",
      "epoch:8 step:7858 [D loss: 0.247175, acc.: 57.03%] [G loss: 0.413626]\n",
      "epoch:8 step:7859 [D loss: 0.208979, acc.: 64.84%] [G loss: 0.464826]\n",
      "epoch:8 step:7860 [D loss: 0.188458, acc.: 69.53%] [G loss: 0.461353]\n",
      "epoch:8 step:7861 [D loss: 0.191787, acc.: 71.09%] [G loss: 0.479047]\n",
      "epoch:8 step:7862 [D loss: 0.210332, acc.: 71.09%] [G loss: 0.493618]\n",
      "epoch:8 step:7863 [D loss: 0.212563, acc.: 68.75%] [G loss: 0.503918]\n",
      "epoch:8 step:7864 [D loss: 0.218682, acc.: 64.06%] [G loss: 0.446566]\n",
      "epoch:8 step:7865 [D loss: 0.213109, acc.: 66.41%] [G loss: 0.453072]\n",
      "epoch:8 step:7866 [D loss: 0.192261, acc.: 67.97%] [G loss: 0.520156]\n",
      "epoch:8 step:7867 [D loss: 0.191575, acc.: 74.22%] [G loss: 0.566003]\n",
      "epoch:8 step:7868 [D loss: 0.225730, acc.: 64.84%] [G loss: 0.544603]\n",
      "epoch:8 step:7869 [D loss: 0.271321, acc.: 52.34%] [G loss: 0.464528]\n",
      "epoch:8 step:7870 [D loss: 0.188215, acc.: 72.66%] [G loss: 0.456191]\n",
      "epoch:8 step:7871 [D loss: 0.227605, acc.: 62.50%] [G loss: 0.440180]\n",
      "epoch:8 step:7872 [D loss: 0.267742, acc.: 47.66%] [G loss: 0.422267]\n",
      "epoch:8 step:7873 [D loss: 0.243446, acc.: 62.50%] [G loss: 0.427531]\n",
      "epoch:8 step:7874 [D loss: 0.186510, acc.: 71.88%] [G loss: 0.451309]\n",
      "epoch:8 step:7875 [D loss: 0.227166, acc.: 64.06%] [G loss: 0.420507]\n",
      "epoch:8 step:7876 [D loss: 0.229893, acc.: 63.28%] [G loss: 0.471360]\n",
      "epoch:8 step:7877 [D loss: 0.178804, acc.: 73.44%] [G loss: 0.502863]\n",
      "epoch:8 step:7878 [D loss: 0.229500, acc.: 60.94%] [G loss: 0.511604]\n",
      "epoch:8 step:7879 [D loss: 0.206549, acc.: 64.84%] [G loss: 0.457484]\n",
      "epoch:8 step:7880 [D loss: 0.239016, acc.: 62.50%] [G loss: 0.465915]\n",
      "epoch:8 step:7881 [D loss: 0.208959, acc.: 66.41%] [G loss: 0.536062]\n",
      "epoch:8 step:7882 [D loss: 0.255162, acc.: 56.25%] [G loss: 0.517700]\n",
      "epoch:8 step:7883 [D loss: 0.244072, acc.: 59.38%] [G loss: 0.503620]\n",
      "epoch:8 step:7884 [D loss: 0.223510, acc.: 61.72%] [G loss: 0.513464]\n",
      "epoch:8 step:7885 [D loss: 0.214493, acc.: 61.72%] [G loss: 0.509043]\n",
      "epoch:8 step:7886 [D loss: 0.244157, acc.: 57.81%] [G loss: 0.453480]\n",
      "epoch:8 step:7887 [D loss: 0.207600, acc.: 71.09%] [G loss: 0.471329]\n",
      "epoch:8 step:7888 [D loss: 0.216860, acc.: 66.41%] [G loss: 0.490733]\n",
      "epoch:8 step:7889 [D loss: 0.239607, acc.: 63.28%] [G loss: 0.487627]\n",
      "epoch:8 step:7890 [D loss: 0.239615, acc.: 60.16%] [G loss: 0.428942]\n",
      "epoch:8 step:7891 [D loss: 0.202839, acc.: 69.53%] [G loss: 0.492920]\n",
      "epoch:8 step:7892 [D loss: 0.267458, acc.: 54.69%] [G loss: 0.458003]\n",
      "epoch:8 step:7893 [D loss: 0.209051, acc.: 69.53%] [G loss: 0.455968]\n",
      "epoch:8 step:7894 [D loss: 0.198102, acc.: 68.75%] [G loss: 0.535158]\n",
      "epoch:8 step:7895 [D loss: 0.213726, acc.: 67.97%] [G loss: 0.506093]\n",
      "epoch:8 step:7896 [D loss: 0.250128, acc.: 63.28%] [G loss: 0.475180]\n",
      "epoch:8 step:7897 [D loss: 0.203130, acc.: 64.84%] [G loss: 0.470222]\n",
      "epoch:8 step:7898 [D loss: 0.211465, acc.: 64.84%] [G loss: 0.452974]\n",
      "epoch:8 step:7899 [D loss: 0.218139, acc.: 63.28%] [G loss: 0.499342]\n",
      "epoch:8 step:7900 [D loss: 0.261838, acc.: 52.34%] [G loss: 0.442750]\n",
      "epoch:8 step:7901 [D loss: 0.196176, acc.: 71.88%] [G loss: 0.516241]\n",
      "epoch:8 step:7902 [D loss: 0.193072, acc.: 71.09%] [G loss: 0.488826]\n",
      "epoch:8 step:7903 [D loss: 0.238377, acc.: 59.38%] [G loss: 0.465162]\n",
      "epoch:8 step:7904 [D loss: 0.259565, acc.: 57.03%] [G loss: 0.473797]\n",
      "epoch:8 step:7905 [D loss: 0.211256, acc.: 65.62%] [G loss: 0.499072]\n",
      "epoch:8 step:7906 [D loss: 0.257223, acc.: 57.81%] [G loss: 0.412409]\n",
      "epoch:8 step:7907 [D loss: 0.211688, acc.: 65.62%] [G loss: 0.485469]\n",
      "epoch:8 step:7908 [D loss: 0.231551, acc.: 59.38%] [G loss: 0.483830]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7909 [D loss: 0.229223, acc.: 64.84%] [G loss: 0.480365]\n",
      "epoch:8 step:7910 [D loss: 0.233372, acc.: 61.72%] [G loss: 0.448466]\n",
      "epoch:8 step:7911 [D loss: 0.267641, acc.: 54.69%] [G loss: 0.479445]\n",
      "epoch:8 step:7912 [D loss: 0.202175, acc.: 69.53%] [G loss: 0.487700]\n",
      "epoch:8 step:7913 [D loss: 0.238939, acc.: 60.16%] [G loss: 0.504655]\n",
      "epoch:8 step:7914 [D loss: 0.230195, acc.: 62.50%] [G loss: 0.447317]\n",
      "epoch:8 step:7915 [D loss: 0.232939, acc.: 64.84%] [G loss: 0.466381]\n",
      "epoch:8 step:7916 [D loss: 0.221857, acc.: 64.06%] [G loss: 0.475727]\n",
      "epoch:8 step:7917 [D loss: 0.262155, acc.: 53.91%] [G loss: 0.432943]\n",
      "epoch:8 step:7918 [D loss: 0.213978, acc.: 63.28%] [G loss: 0.513421]\n",
      "epoch:8 step:7919 [D loss: 0.219518, acc.: 61.72%] [G loss: 0.445962]\n",
      "epoch:8 step:7920 [D loss: 0.215493, acc.: 59.38%] [G loss: 0.469580]\n",
      "epoch:8 step:7921 [D loss: 0.197001, acc.: 70.31%] [G loss: 0.508596]\n",
      "epoch:8 step:7922 [D loss: 0.220867, acc.: 65.62%] [G loss: 0.477642]\n",
      "epoch:8 step:7923 [D loss: 0.213022, acc.: 65.62%] [G loss: 0.483993]\n",
      "epoch:8 step:7924 [D loss: 0.188446, acc.: 71.88%] [G loss: 0.531767]\n",
      "epoch:8 step:7925 [D loss: 0.192554, acc.: 66.41%] [G loss: 0.548255]\n",
      "epoch:8 step:7926 [D loss: 0.196748, acc.: 72.66%] [G loss: 0.561447]\n",
      "epoch:8 step:7927 [D loss: 0.258996, acc.: 55.47%] [G loss: 0.454549]\n",
      "epoch:8 step:7928 [D loss: 0.234334, acc.: 58.59%] [G loss: 0.460451]\n",
      "epoch:8 step:7929 [D loss: 0.250708, acc.: 53.91%] [G loss: 0.462544]\n",
      "epoch:8 step:7930 [D loss: 0.202421, acc.: 68.75%] [G loss: 0.477393]\n",
      "epoch:8 step:7931 [D loss: 0.225273, acc.: 62.50%] [G loss: 0.454686]\n",
      "epoch:8 step:7932 [D loss: 0.182577, acc.: 70.31%] [G loss: 0.516319]\n",
      "epoch:8 step:7933 [D loss: 0.266329, acc.: 48.44%] [G loss: 0.448635]\n",
      "epoch:8 step:7934 [D loss: 0.254510, acc.: 57.03%] [G loss: 0.463053]\n",
      "epoch:8 step:7935 [D loss: 0.202071, acc.: 70.31%] [G loss: 0.503191]\n",
      "epoch:8 step:7936 [D loss: 0.219498, acc.: 63.28%] [G loss: 0.499866]\n",
      "epoch:8 step:7937 [D loss: 0.268644, acc.: 58.59%] [G loss: 0.454158]\n",
      "epoch:8 step:7938 [D loss: 0.224087, acc.: 60.94%] [G loss: 0.489930]\n",
      "epoch:8 step:7939 [D loss: 0.232661, acc.: 60.94%] [G loss: 0.435371]\n",
      "epoch:8 step:7940 [D loss: 0.208391, acc.: 71.88%] [G loss: 0.478548]\n",
      "epoch:8 step:7941 [D loss: 0.202862, acc.: 64.06%] [G loss: 0.505665]\n",
      "epoch:8 step:7942 [D loss: 0.230615, acc.: 64.84%] [G loss: 0.479810]\n",
      "epoch:8 step:7943 [D loss: 0.213007, acc.: 67.19%] [G loss: 0.490168]\n",
      "epoch:8 step:7944 [D loss: 0.243688, acc.: 60.16%] [G loss: 0.449791]\n",
      "epoch:8 step:7945 [D loss: 0.206575, acc.: 72.66%] [G loss: 0.477691]\n",
      "epoch:8 step:7946 [D loss: 0.185214, acc.: 71.09%] [G loss: 0.493371]\n",
      "epoch:8 step:7947 [D loss: 0.186785, acc.: 71.09%] [G loss: 0.530079]\n",
      "epoch:8 step:7948 [D loss: 0.190581, acc.: 75.78%] [G loss: 0.503758]\n",
      "epoch:8 step:7949 [D loss: 0.204029, acc.: 67.97%] [G loss: 0.580770]\n",
      "epoch:8 step:7950 [D loss: 0.218220, acc.: 65.62%] [G loss: 0.505079]\n",
      "epoch:8 step:7951 [D loss: 0.234867, acc.: 58.59%] [G loss: 0.439486]\n",
      "epoch:8 step:7952 [D loss: 0.239691, acc.: 65.62%] [G loss: 0.461557]\n",
      "epoch:8 step:7953 [D loss: 0.200548, acc.: 67.19%] [G loss: 0.500576]\n",
      "epoch:8 step:7954 [D loss: 0.245432, acc.: 54.69%] [G loss: 0.495322]\n",
      "epoch:8 step:7955 [D loss: 0.251695, acc.: 61.72%] [G loss: 0.462232]\n",
      "epoch:8 step:7956 [D loss: 0.246578, acc.: 60.16%] [G loss: 0.437568]\n",
      "epoch:8 step:7957 [D loss: 0.213078, acc.: 66.41%] [G loss: 0.490974]\n",
      "epoch:8 step:7958 [D loss: 0.219470, acc.: 70.31%] [G loss: 0.491483]\n",
      "epoch:8 step:7959 [D loss: 0.233922, acc.: 60.94%] [G loss: 0.426620]\n",
      "epoch:8 step:7960 [D loss: 0.216006, acc.: 64.06%] [G loss: 0.492312]\n",
      "epoch:8 step:7961 [D loss: 0.244384, acc.: 61.72%] [G loss: 0.474454]\n",
      "epoch:8 step:7962 [D loss: 0.230928, acc.: 62.50%] [G loss: 0.464315]\n",
      "epoch:8 step:7963 [D loss: 0.244671, acc.: 57.03%] [G loss: 0.446971]\n",
      "epoch:8 step:7964 [D loss: 0.205389, acc.: 71.88%] [G loss: 0.504648]\n",
      "epoch:8 step:7965 [D loss: 0.206847, acc.: 71.09%] [G loss: 0.501296]\n",
      "epoch:8 step:7966 [D loss: 0.228370, acc.: 62.50%] [G loss: 0.476911]\n",
      "epoch:8 step:7967 [D loss: 0.166622, acc.: 76.56%] [G loss: 0.549085]\n",
      "epoch:8 step:7968 [D loss: 0.186772, acc.: 70.31%] [G loss: 0.529612]\n",
      "epoch:8 step:7969 [D loss: 0.262843, acc.: 56.25%] [G loss: 0.479636]\n",
      "epoch:8 step:7970 [D loss: 0.210828, acc.: 66.41%] [G loss: 0.499681]\n",
      "epoch:8 step:7971 [D loss: 0.199339, acc.: 75.00%] [G loss: 0.504368]\n",
      "epoch:8 step:7972 [D loss: 0.228089, acc.: 63.28%] [G loss: 0.501780]\n",
      "epoch:8 step:7973 [D loss: 0.250208, acc.: 61.72%] [G loss: 0.449404]\n",
      "epoch:8 step:7974 [D loss: 0.243998, acc.: 57.03%] [G loss: 0.434896]\n",
      "epoch:8 step:7975 [D loss: 0.233230, acc.: 66.41%] [G loss: 0.402841]\n",
      "epoch:8 step:7976 [D loss: 0.245100, acc.: 54.69%] [G loss: 0.444383]\n",
      "epoch:8 step:7977 [D loss: 0.189756, acc.: 71.09%] [G loss: 0.498096]\n",
      "epoch:8 step:7978 [D loss: 0.266713, acc.: 55.47%] [G loss: 0.445614]\n",
      "epoch:8 step:7979 [D loss: 0.249486, acc.: 55.47%] [G loss: 0.475448]\n",
      "epoch:8 step:7980 [D loss: 0.199399, acc.: 62.50%] [G loss: 0.511183]\n",
      "epoch:8 step:7981 [D loss: 0.230428, acc.: 62.50%] [G loss: 0.486061]\n",
      "epoch:8 step:7982 [D loss: 0.260102, acc.: 53.12%] [G loss: 0.440710]\n",
      "epoch:8 step:7983 [D loss: 0.194186, acc.: 71.09%] [G loss: 0.498865]\n",
      "epoch:8 step:7984 [D loss: 0.211221, acc.: 64.84%] [G loss: 0.530193]\n",
      "epoch:8 step:7985 [D loss: 0.217557, acc.: 62.50%] [G loss: 0.492495]\n",
      "epoch:8 step:7986 [D loss: 0.235651, acc.: 64.84%] [G loss: 0.445057]\n",
      "epoch:8 step:7987 [D loss: 0.216296, acc.: 66.41%] [G loss: 0.458109]\n",
      "epoch:8 step:7988 [D loss: 0.230378, acc.: 67.97%] [G loss: 0.485907]\n",
      "epoch:8 step:7989 [D loss: 0.240618, acc.: 58.59%] [G loss: 0.482268]\n",
      "epoch:8 step:7990 [D loss: 0.215586, acc.: 68.75%] [G loss: 0.476417]\n",
      "epoch:8 step:7991 [D loss: 0.194846, acc.: 69.53%] [G loss: 0.497557]\n",
      "epoch:8 step:7992 [D loss: 0.229072, acc.: 65.62%] [G loss: 0.502695]\n",
      "epoch:8 step:7993 [D loss: 0.236026, acc.: 59.38%] [G loss: 0.501902]\n",
      "epoch:8 step:7994 [D loss: 0.180826, acc.: 71.09%] [G loss: 0.539577]\n",
      "epoch:8 step:7995 [D loss: 0.194542, acc.: 71.88%] [G loss: 0.494267]\n",
      "epoch:8 step:7996 [D loss: 0.240600, acc.: 58.59%] [G loss: 0.462149]\n",
      "epoch:8 step:7997 [D loss: 0.263859, acc.: 53.12%] [G loss: 0.425325]\n",
      "epoch:8 step:7998 [D loss: 0.235562, acc.: 56.25%] [G loss: 0.463193]\n",
      "epoch:8 step:7999 [D loss: 0.188877, acc.: 69.53%] [G loss: 0.452194]\n",
      "epoch:8 step:8000 [D loss: 0.206857, acc.: 71.88%] [G loss: 0.447685]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.226239\n",
      "FID: 19.669958\n",
      "0 = 12.187724488902038\n",
      "1 = 0.05680035398131198\n",
      "2 = 0.9272000193595886\n",
      "3 = 0.9071999788284302\n",
      "4 = 0.9472000002861023\n",
      "5 = 0.9449999928474426\n",
      "6 = 0.9071999788284302\n",
      "7 = 7.370669296348135\n",
      "8 = 0.10398900458091936\n",
      "9 = 0.7353500127792358\n",
      "10 = 0.732200026512146\n",
      "11 = 0.7384999990463257\n",
      "12 = 0.7368420958518982\n",
      "13 = 0.732200026512146\n",
      "14 = 7.226304531097412\n",
      "15 = 9.326833724975586\n",
      "16 = 0.16251033544540405\n",
      "17 = 7.226239204406738\n",
      "18 = 19.669958114624023\n",
      "epoch:8 step:8001 [D loss: 0.198634, acc.: 68.75%] [G loss: 0.511615]\n",
      "epoch:8 step:8002 [D loss: 0.256665, acc.: 57.03%] [G loss: 0.455794]\n",
      "epoch:8 step:8003 [D loss: 0.211857, acc.: 67.19%] [G loss: 0.483991]\n",
      "epoch:8 step:8004 [D loss: 0.192538, acc.: 71.09%] [G loss: 0.463032]\n",
      "epoch:8 step:8005 [D loss: 0.235190, acc.: 65.62%] [G loss: 0.497318]\n",
      "epoch:8 step:8006 [D loss: 0.222600, acc.: 64.06%] [G loss: 0.462287]\n",
      "epoch:8 step:8007 [D loss: 0.242290, acc.: 57.81%] [G loss: 0.451805]\n",
      "epoch:8 step:8008 [D loss: 0.220547, acc.: 62.50%] [G loss: 0.458681]\n",
      "epoch:8 step:8009 [D loss: 0.192880, acc.: 75.00%] [G loss: 0.516899]\n",
      "epoch:8 step:8010 [D loss: 0.220921, acc.: 64.84%] [G loss: 0.438809]\n",
      "epoch:8 step:8011 [D loss: 0.198778, acc.: 72.66%] [G loss: 0.543131]\n",
      "epoch:8 step:8012 [D loss: 0.198894, acc.: 67.19%] [G loss: 0.557958]\n",
      "epoch:8 step:8013 [D loss: 0.234377, acc.: 64.84%] [G loss: 0.487886]\n",
      "epoch:8 step:8014 [D loss: 0.227985, acc.: 67.19%] [G loss: 0.469044]\n",
      "epoch:8 step:8015 [D loss: 0.190401, acc.: 68.75%] [G loss: 0.491838]\n",
      "epoch:8 step:8016 [D loss: 0.200111, acc.: 74.22%] [G loss: 0.502209]\n",
      "epoch:8 step:8017 [D loss: 0.191760, acc.: 70.31%] [G loss: 0.466384]\n",
      "epoch:8 step:8018 [D loss: 0.212540, acc.: 70.31%] [G loss: 0.466449]\n",
      "epoch:8 step:8019 [D loss: 0.213061, acc.: 64.84%] [G loss: 0.470833]\n",
      "epoch:8 step:8020 [D loss: 0.221738, acc.: 64.84%] [G loss: 0.497376]\n",
      "epoch:8 step:8021 [D loss: 0.234580, acc.: 64.06%] [G loss: 0.437508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8022 [D loss: 0.210582, acc.: 65.62%] [G loss: 0.510269]\n",
      "epoch:8 step:8023 [D loss: 0.234726, acc.: 63.28%] [G loss: 0.494282]\n",
      "epoch:8 step:8024 [D loss: 0.270543, acc.: 57.81%] [G loss: 0.455945]\n",
      "epoch:8 step:8025 [D loss: 0.244670, acc.: 57.81%] [G loss: 0.475085]\n",
      "epoch:8 step:8026 [D loss: 0.203466, acc.: 72.66%] [G loss: 0.465274]\n",
      "epoch:8 step:8027 [D loss: 0.241146, acc.: 59.38%] [G loss: 0.445505]\n",
      "epoch:8 step:8028 [D loss: 0.244844, acc.: 59.38%] [G loss: 0.425904]\n",
      "epoch:8 step:8029 [D loss: 0.258506, acc.: 55.47%] [G loss: 0.478991]\n",
      "epoch:8 step:8030 [D loss: 0.190283, acc.: 71.88%] [G loss: 0.506336]\n",
      "epoch:8 step:8031 [D loss: 0.285813, acc.: 51.56%] [G loss: 0.433475]\n",
      "epoch:8 step:8032 [D loss: 0.195268, acc.: 75.78%] [G loss: 0.484138]\n",
      "epoch:8 step:8033 [D loss: 0.217489, acc.: 64.84%] [G loss: 0.470303]\n",
      "epoch:8 step:8034 [D loss: 0.235416, acc.: 59.38%] [G loss: 0.487562]\n",
      "epoch:8 step:8035 [D loss: 0.201094, acc.: 72.66%] [G loss: 0.483443]\n",
      "epoch:8 step:8036 [D loss: 0.228367, acc.: 66.41%] [G loss: 0.524167]\n",
      "epoch:8 step:8037 [D loss: 0.209579, acc.: 71.09%] [G loss: 0.487392]\n",
      "epoch:8 step:8038 [D loss: 0.288361, acc.: 46.88%] [G loss: 0.401820]\n",
      "epoch:8 step:8039 [D loss: 0.225768, acc.: 64.84%] [G loss: 0.461059]\n",
      "epoch:8 step:8040 [D loss: 0.212646, acc.: 68.75%] [G loss: 0.483257]\n",
      "epoch:8 step:8041 [D loss: 0.228479, acc.: 66.41%] [G loss: 0.471446]\n",
      "epoch:8 step:8042 [D loss: 0.234057, acc.: 62.50%] [G loss: 0.460315]\n",
      "epoch:8 step:8043 [D loss: 0.178664, acc.: 72.66%] [G loss: 0.507584]\n",
      "epoch:8 step:8044 [D loss: 0.225863, acc.: 66.41%] [G loss: 0.505081]\n",
      "epoch:8 step:8045 [D loss: 0.202214, acc.: 68.75%] [G loss: 0.498299]\n",
      "epoch:8 step:8046 [D loss: 0.188948, acc.: 72.66%] [G loss: 0.524711]\n",
      "epoch:8 step:8047 [D loss: 0.234870, acc.: 61.72%] [G loss: 0.489304]\n",
      "epoch:8 step:8048 [D loss: 0.208449, acc.: 66.41%] [G loss: 0.493501]\n",
      "epoch:8 step:8049 [D loss: 0.231871, acc.: 59.38%] [G loss: 0.473750]\n",
      "epoch:8 step:8050 [D loss: 0.195100, acc.: 71.09%] [G loss: 0.489626]\n",
      "epoch:8 step:8051 [D loss: 0.195089, acc.: 71.88%] [G loss: 0.485732]\n",
      "epoch:8 step:8052 [D loss: 0.232798, acc.: 62.50%] [G loss: 0.455663]\n",
      "epoch:8 step:8053 [D loss: 0.188907, acc.: 68.75%] [G loss: 0.477071]\n",
      "epoch:8 step:8054 [D loss: 0.218515, acc.: 64.84%] [G loss: 0.492273]\n",
      "epoch:8 step:8055 [D loss: 0.242097, acc.: 60.16%] [G loss: 0.497497]\n",
      "epoch:8 step:8056 [D loss: 0.251403, acc.: 55.47%] [G loss: 0.446044]\n",
      "epoch:8 step:8057 [D loss: 0.229839, acc.: 62.50%] [G loss: 0.456115]\n",
      "epoch:8 step:8058 [D loss: 0.218594, acc.: 66.41%] [G loss: 0.461301]\n",
      "epoch:8 step:8059 [D loss: 0.205618, acc.: 67.19%] [G loss: 0.522437]\n",
      "epoch:8 step:8060 [D loss: 0.199542, acc.: 69.53%] [G loss: 0.559643]\n",
      "epoch:8 step:8061 [D loss: 0.253835, acc.: 60.16%] [G loss: 0.485051]\n",
      "epoch:8 step:8062 [D loss: 0.247475, acc.: 54.69%] [G loss: 0.461736]\n",
      "epoch:8 step:8063 [D loss: 0.182255, acc.: 73.44%] [G loss: 0.461442]\n",
      "epoch:8 step:8064 [D loss: 0.192018, acc.: 71.88%] [G loss: 0.476080]\n",
      "epoch:8 step:8065 [D loss: 0.242705, acc.: 64.06%] [G loss: 0.471547]\n",
      "epoch:8 step:8066 [D loss: 0.225630, acc.: 65.62%] [G loss: 0.413550]\n",
      "epoch:8 step:8067 [D loss: 0.203229, acc.: 75.00%] [G loss: 0.434619]\n",
      "epoch:8 step:8068 [D loss: 0.232558, acc.: 64.06%] [G loss: 0.451964]\n",
      "epoch:8 step:8069 [D loss: 0.213398, acc.: 68.75%] [G loss: 0.449412]\n",
      "epoch:8 step:8070 [D loss: 0.174706, acc.: 73.44%] [G loss: 0.552559]\n",
      "epoch:8 step:8071 [D loss: 0.175037, acc.: 73.44%] [G loss: 0.498374]\n",
      "epoch:8 step:8072 [D loss: 0.250243, acc.: 54.69%] [G loss: 0.463089]\n",
      "epoch:8 step:8073 [D loss: 0.231278, acc.: 59.38%] [G loss: 0.448792]\n",
      "epoch:8 step:8074 [D loss: 0.235991, acc.: 57.03%] [G loss: 0.482784]\n",
      "epoch:8 step:8075 [D loss: 0.240947, acc.: 63.28%] [G loss: 0.456562]\n",
      "epoch:8 step:8076 [D loss: 0.235704, acc.: 61.72%] [G loss: 0.481428]\n",
      "epoch:8 step:8077 [D loss: 0.205069, acc.: 67.19%] [G loss: 0.512060]\n",
      "epoch:8 step:8078 [D loss: 0.213785, acc.: 69.53%] [G loss: 0.473582]\n",
      "epoch:8 step:8079 [D loss: 0.226997, acc.: 62.50%] [G loss: 0.495194]\n",
      "epoch:8 step:8080 [D loss: 0.232494, acc.: 62.50%] [G loss: 0.476347]\n",
      "epoch:8 step:8081 [D loss: 0.229115, acc.: 59.38%] [G loss: 0.480176]\n",
      "epoch:8 step:8082 [D loss: 0.205049, acc.: 67.19%] [G loss: 0.518308]\n",
      "epoch:8 step:8083 [D loss: 0.245561, acc.: 57.81%] [G loss: 0.450810]\n",
      "epoch:8 step:8084 [D loss: 0.211385, acc.: 66.41%] [G loss: 0.453930]\n",
      "epoch:8 step:8085 [D loss: 0.196595, acc.: 68.75%] [G loss: 0.512570]\n",
      "epoch:8 step:8086 [D loss: 0.260074, acc.: 55.47%] [G loss: 0.491976]\n",
      "epoch:8 step:8087 [D loss: 0.222837, acc.: 59.38%] [G loss: 0.451626]\n",
      "epoch:8 step:8088 [D loss: 0.192628, acc.: 67.97%] [G loss: 0.485298]\n",
      "epoch:8 step:8089 [D loss: 0.228613, acc.: 59.38%] [G loss: 0.434603]\n",
      "epoch:8 step:8090 [D loss: 0.248998, acc.: 54.69%] [G loss: 0.434160]\n",
      "epoch:8 step:8091 [D loss: 0.225722, acc.: 65.62%] [G loss: 0.493618]\n",
      "epoch:8 step:8092 [D loss: 0.258249, acc.: 57.03%] [G loss: 0.448955]\n",
      "epoch:8 step:8093 [D loss: 0.223504, acc.: 60.16%] [G loss: 0.462636]\n",
      "epoch:8 step:8094 [D loss: 0.197000, acc.: 70.31%] [G loss: 0.498654]\n",
      "epoch:8 step:8095 [D loss: 0.234568, acc.: 60.16%] [G loss: 0.476007]\n",
      "epoch:8 step:8096 [D loss: 0.268012, acc.: 51.56%] [G loss: 0.411820]\n",
      "epoch:8 step:8097 [D loss: 0.235833, acc.: 61.72%] [G loss: 0.441782]\n",
      "epoch:8 step:8098 [D loss: 0.236998, acc.: 60.94%] [G loss: 0.481688]\n",
      "epoch:8 step:8099 [D loss: 0.231496, acc.: 64.06%] [G loss: 0.433662]\n",
      "epoch:8 step:8100 [D loss: 0.238849, acc.: 62.50%] [G loss: 0.469813]\n",
      "epoch:8 step:8101 [D loss: 0.232941, acc.: 63.28%] [G loss: 0.431602]\n",
      "epoch:8 step:8102 [D loss: 0.250555, acc.: 50.78%] [G loss: 0.469106]\n",
      "epoch:8 step:8103 [D loss: 0.195943, acc.: 68.75%] [G loss: 0.482464]\n",
      "epoch:8 step:8104 [D loss: 0.204581, acc.: 67.19%] [G loss: 0.477209]\n",
      "epoch:8 step:8105 [D loss: 0.219905, acc.: 62.50%] [G loss: 0.459589]\n",
      "epoch:8 step:8106 [D loss: 0.201964, acc.: 71.09%] [G loss: 0.463809]\n",
      "epoch:8 step:8107 [D loss: 0.206754, acc.: 66.41%] [G loss: 0.483331]\n",
      "epoch:8 step:8108 [D loss: 0.226092, acc.: 67.97%] [G loss: 0.467950]\n",
      "epoch:8 step:8109 [D loss: 0.186265, acc.: 79.69%] [G loss: 0.491430]\n",
      "epoch:8 step:8110 [D loss: 0.220439, acc.: 60.16%] [G loss: 0.459631]\n",
      "epoch:8 step:8111 [D loss: 0.238226, acc.: 59.38%] [G loss: 0.486931]\n",
      "epoch:8 step:8112 [D loss: 0.224960, acc.: 64.06%] [G loss: 0.494550]\n",
      "epoch:8 step:8113 [D loss: 0.226664, acc.: 69.53%] [G loss: 0.456199]\n",
      "epoch:8 step:8114 [D loss: 0.181095, acc.: 74.22%] [G loss: 0.506486]\n",
      "epoch:8 step:8115 [D loss: 0.250728, acc.: 58.59%] [G loss: 0.462991]\n",
      "epoch:8 step:8116 [D loss: 0.213540, acc.: 64.06%] [G loss: 0.451573]\n",
      "epoch:8 step:8117 [D loss: 0.214721, acc.: 63.28%] [G loss: 0.458528]\n",
      "epoch:8 step:8118 [D loss: 0.227298, acc.: 64.06%] [G loss: 0.487419]\n",
      "epoch:8 step:8119 [D loss: 0.198112, acc.: 69.53%] [G loss: 0.526434]\n",
      "epoch:8 step:8120 [D loss: 0.216701, acc.: 65.62%] [G loss: 0.496236]\n",
      "epoch:8 step:8121 [D loss: 0.234375, acc.: 60.94%] [G loss: 0.441443]\n",
      "epoch:8 step:8122 [D loss: 0.250414, acc.: 56.25%] [G loss: 0.426093]\n",
      "epoch:8 step:8123 [D loss: 0.229306, acc.: 63.28%] [G loss: 0.471375]\n",
      "epoch:8 step:8124 [D loss: 0.245557, acc.: 52.34%] [G loss: 0.488486]\n",
      "epoch:8 step:8125 [D loss: 0.222523, acc.: 62.50%] [G loss: 0.443878]\n",
      "epoch:8 step:8126 [D loss: 0.202607, acc.: 68.75%] [G loss: 0.477510]\n",
      "epoch:8 step:8127 [D loss: 0.199105, acc.: 72.66%] [G loss: 0.446323]\n",
      "epoch:8 step:8128 [D loss: 0.233320, acc.: 62.50%] [G loss: 0.504235]\n",
      "epoch:8 step:8129 [D loss: 0.204843, acc.: 67.97%] [G loss: 0.499404]\n",
      "epoch:8 step:8130 [D loss: 0.195973, acc.: 71.09%] [G loss: 0.539656]\n",
      "epoch:8 step:8131 [D loss: 0.191915, acc.: 75.78%] [G loss: 0.472469]\n",
      "epoch:8 step:8132 [D loss: 0.221280, acc.: 60.16%] [G loss: 0.485584]\n",
      "epoch:8 step:8133 [D loss: 0.221229, acc.: 67.19%] [G loss: 0.479828]\n",
      "epoch:8 step:8134 [D loss: 0.208261, acc.: 66.41%] [G loss: 0.475325]\n",
      "epoch:8 step:8135 [D loss: 0.204101, acc.: 64.06%] [G loss: 0.499027]\n",
      "epoch:8 step:8136 [D loss: 0.200741, acc.: 70.31%] [G loss: 0.497321]\n",
      "epoch:8 step:8137 [D loss: 0.210469, acc.: 62.50%] [G loss: 0.484604]\n",
      "epoch:8 step:8138 [D loss: 0.155325, acc.: 78.12%] [G loss: 0.581178]\n",
      "epoch:8 step:8139 [D loss: 0.208153, acc.: 69.53%] [G loss: 0.532342]\n",
      "epoch:8 step:8140 [D loss: 0.241475, acc.: 58.59%] [G loss: 0.487361]\n",
      "epoch:8 step:8141 [D loss: 0.228095, acc.: 60.16%] [G loss: 0.476258]\n",
      "epoch:8 step:8142 [D loss: 0.212307, acc.: 63.28%] [G loss: 0.481414]\n",
      "epoch:8 step:8143 [D loss: 0.209775, acc.: 70.31%] [G loss: 0.502881]\n",
      "epoch:8 step:8144 [D loss: 0.193613, acc.: 70.31%] [G loss: 0.557929]\n",
      "epoch:8 step:8145 [D loss: 0.190621, acc.: 69.53%] [G loss: 0.547935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8146 [D loss: 0.216869, acc.: 71.09%] [G loss: 0.533987]\n",
      "epoch:8 step:8147 [D loss: 0.206661, acc.: 70.31%] [G loss: 0.505482]\n",
      "epoch:8 step:8148 [D loss: 0.240581, acc.: 58.59%] [G loss: 0.466448]\n",
      "epoch:8 step:8149 [D loss: 0.237298, acc.: 62.50%] [G loss: 0.460938]\n",
      "epoch:8 step:8150 [D loss: 0.215289, acc.: 70.31%] [G loss: 0.508630]\n",
      "epoch:8 step:8151 [D loss: 0.258947, acc.: 57.81%] [G loss: 0.480470]\n",
      "epoch:8 step:8152 [D loss: 0.210914, acc.: 67.97%] [G loss: 0.489817]\n",
      "epoch:8 step:8153 [D loss: 0.210524, acc.: 64.06%] [G loss: 0.503169]\n",
      "epoch:8 step:8154 [D loss: 0.216742, acc.: 64.06%] [G loss: 0.497043]\n",
      "epoch:8 step:8155 [D loss: 0.194728, acc.: 72.66%] [G loss: 0.529669]\n",
      "epoch:8 step:8156 [D loss: 0.205918, acc.: 64.84%] [G loss: 0.482930]\n",
      "epoch:8 step:8157 [D loss: 0.195234, acc.: 68.75%] [G loss: 0.471982]\n",
      "epoch:8 step:8158 [D loss: 0.210629, acc.: 68.75%] [G loss: 0.491872]\n",
      "epoch:8 step:8159 [D loss: 0.229404, acc.: 64.06%] [G loss: 0.443820]\n",
      "epoch:8 step:8160 [D loss: 0.205975, acc.: 72.66%] [G loss: 0.493471]\n",
      "epoch:8 step:8161 [D loss: 0.242369, acc.: 62.50%] [G loss: 0.450632]\n",
      "epoch:8 step:8162 [D loss: 0.214405, acc.: 70.31%] [G loss: 0.519779]\n",
      "epoch:8 step:8163 [D loss: 0.220956, acc.: 60.16%] [G loss: 0.531291]\n",
      "epoch:8 step:8164 [D loss: 0.225241, acc.: 66.41%] [G loss: 0.487788]\n",
      "epoch:8 step:8165 [D loss: 0.193770, acc.: 71.09%] [G loss: 0.531214]\n",
      "epoch:8 step:8166 [D loss: 0.248041, acc.: 55.47%] [G loss: 0.439120]\n",
      "epoch:8 step:8167 [D loss: 0.279357, acc.: 53.91%] [G loss: 0.412688]\n",
      "epoch:8 step:8168 [D loss: 0.241065, acc.: 64.06%] [G loss: 0.484018]\n",
      "epoch:8 step:8169 [D loss: 0.213358, acc.: 63.28%] [G loss: 0.478843]\n",
      "epoch:8 step:8170 [D loss: 0.219855, acc.: 67.97%] [G loss: 0.449934]\n",
      "epoch:8 step:8171 [D loss: 0.245899, acc.: 63.28%] [G loss: 0.415697]\n",
      "epoch:8 step:8172 [D loss: 0.206100, acc.: 67.19%] [G loss: 0.520435]\n",
      "epoch:8 step:8173 [D loss: 0.206607, acc.: 66.41%] [G loss: 0.481487]\n",
      "epoch:8 step:8174 [D loss: 0.207234, acc.: 67.19%] [G loss: 0.508750]\n",
      "epoch:8 step:8175 [D loss: 0.226141, acc.: 63.28%] [G loss: 0.477703]\n",
      "epoch:8 step:8176 [D loss: 0.218730, acc.: 67.19%] [G loss: 0.524159]\n",
      "epoch:8 step:8177 [D loss: 0.214262, acc.: 66.41%] [G loss: 0.463027]\n",
      "epoch:8 step:8178 [D loss: 0.220331, acc.: 60.16%] [G loss: 0.491308]\n",
      "epoch:8 step:8179 [D loss: 0.232410, acc.: 62.50%] [G loss: 0.475066]\n",
      "epoch:8 step:8180 [D loss: 0.233833, acc.: 61.72%] [G loss: 0.442567]\n",
      "epoch:8 step:8181 [D loss: 0.205034, acc.: 73.44%] [G loss: 0.444382]\n",
      "epoch:8 step:8182 [D loss: 0.218454, acc.: 68.75%] [G loss: 0.459066]\n",
      "epoch:8 step:8183 [D loss: 0.227377, acc.: 60.16%] [G loss: 0.472236]\n",
      "epoch:8 step:8184 [D loss: 0.203534, acc.: 64.84%] [G loss: 0.513155]\n",
      "epoch:8 step:8185 [D loss: 0.231623, acc.: 61.72%] [G loss: 0.497518]\n",
      "epoch:8 step:8186 [D loss: 0.175286, acc.: 75.00%] [G loss: 0.544048]\n",
      "epoch:8 step:8187 [D loss: 0.197930, acc.: 66.41%] [G loss: 0.489576]\n",
      "epoch:8 step:8188 [D loss: 0.157164, acc.: 78.91%] [G loss: 0.603915]\n",
      "epoch:8 step:8189 [D loss: 0.217347, acc.: 63.28%] [G loss: 0.503780]\n",
      "epoch:8 step:8190 [D loss: 0.198913, acc.: 67.19%] [G loss: 0.502262]\n",
      "epoch:8 step:8191 [D loss: 0.205583, acc.: 64.84%] [G loss: 0.502678]\n",
      "epoch:8 step:8192 [D loss: 0.247570, acc.: 57.81%] [G loss: 0.477873]\n",
      "epoch:8 step:8193 [D loss: 0.202968, acc.: 66.41%] [G loss: 0.501849]\n",
      "epoch:8 step:8194 [D loss: 0.207413, acc.: 69.53%] [G loss: 0.537200]\n",
      "epoch:8 step:8195 [D loss: 0.198621, acc.: 71.09%] [G loss: 0.526360]\n",
      "epoch:8 step:8196 [D loss: 0.210750, acc.: 69.53%] [G loss: 0.546386]\n",
      "epoch:8 step:8197 [D loss: 0.219725, acc.: 65.62%] [G loss: 0.480707]\n",
      "epoch:8 step:8198 [D loss: 0.260333, acc.: 51.56%] [G loss: 0.453292]\n",
      "epoch:8 step:8199 [D loss: 0.252863, acc.: 57.81%] [G loss: 0.452798]\n",
      "epoch:8 step:8200 [D loss: 0.228371, acc.: 61.72%] [G loss: 0.465346]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.233567\n",
      "FID: 19.346846\n",
      "0 = 12.123621140789925\n",
      "1 = 0.06540508661816591\n",
      "2 = 0.9229999780654907\n",
      "3 = 0.8956999778747559\n",
      "4 = 0.9502999782562256\n",
      "5 = 0.9474296569824219\n",
      "6 = 0.8956999778747559\n",
      "7 = 7.328411271101212\n",
      "8 = 0.10129394237033769\n",
      "9 = 0.7430999875068665\n",
      "10 = 0.7348999977111816\n",
      "11 = 0.7512999773025513\n",
      "12 = 0.7471533417701721\n",
      "13 = 0.7348999977111816\n",
      "14 = 7.233628749847412\n",
      "15 = 9.414978981018066\n",
      "16 = 0.14852868020534515\n",
      "17 = 7.233567237854004\n",
      "18 = 19.346845626831055\n",
      "epoch:8 step:8201 [D loss: 0.207706, acc.: 71.88%] [G loss: 0.458022]\n",
      "epoch:8 step:8202 [D loss: 0.181035, acc.: 74.22%] [G loss: 0.516469]\n",
      "epoch:8 step:8203 [D loss: 0.181470, acc.: 71.09%] [G loss: 0.516306]\n",
      "epoch:8 step:8204 [D loss: 0.191538, acc.: 69.53%] [G loss: 0.539876]\n",
      "epoch:8 step:8205 [D loss: 0.178918, acc.: 71.88%] [G loss: 0.539627]\n",
      "epoch:8 step:8206 [D loss: 0.234417, acc.: 62.50%] [G loss: 0.454789]\n",
      "epoch:8 step:8207 [D loss: 0.234264, acc.: 61.72%] [G loss: 0.458204]\n",
      "epoch:8 step:8208 [D loss: 0.204895, acc.: 61.72%] [G loss: 0.511624]\n",
      "epoch:8 step:8209 [D loss: 0.246775, acc.: 55.47%] [G loss: 0.467397]\n",
      "epoch:8 step:8210 [D loss: 0.219742, acc.: 65.62%] [G loss: 0.456636]\n",
      "epoch:8 step:8211 [D loss: 0.219721, acc.: 64.06%] [G loss: 0.469023]\n",
      "epoch:8 step:8212 [D loss: 0.269795, acc.: 54.69%] [G loss: 0.433766]\n",
      "epoch:8 step:8213 [D loss: 0.230149, acc.: 63.28%] [G loss: 0.413473]\n",
      "epoch:8 step:8214 [D loss: 0.252104, acc.: 56.25%] [G loss: 0.451970]\n",
      "epoch:8 step:8215 [D loss: 0.194785, acc.: 69.53%] [G loss: 0.489791]\n",
      "epoch:8 step:8216 [D loss: 0.271027, acc.: 53.12%] [G loss: 0.541997]\n",
      "epoch:8 step:8217 [D loss: 0.234388, acc.: 64.06%] [G loss: 0.530175]\n",
      "epoch:8 step:8218 [D loss: 0.210647, acc.: 67.19%] [G loss: 0.513343]\n",
      "epoch:8 step:8219 [D loss: 0.230863, acc.: 57.03%] [G loss: 0.486825]\n",
      "epoch:8 step:8220 [D loss: 0.199544, acc.: 71.09%] [G loss: 0.463496]\n",
      "epoch:8 step:8221 [D loss: 0.193217, acc.: 73.44%] [G loss: 0.504089]\n",
      "epoch:8 step:8222 [D loss: 0.208434, acc.: 67.19%] [G loss: 0.490591]\n",
      "epoch:8 step:8223 [D loss: 0.208110, acc.: 60.94%] [G loss: 0.498394]\n",
      "epoch:8 step:8224 [D loss: 0.212152, acc.: 67.19%] [G loss: 0.443541]\n",
      "epoch:8 step:8225 [D loss: 0.228822, acc.: 64.06%] [G loss: 0.467458]\n",
      "epoch:8 step:8226 [D loss: 0.195987, acc.: 71.09%] [G loss: 0.540645]\n",
      "epoch:8 step:8227 [D loss: 0.203318, acc.: 67.97%] [G loss: 0.520787]\n",
      "epoch:8 step:8228 [D loss: 0.233522, acc.: 61.72%] [G loss: 0.450842]\n",
      "epoch:8 step:8229 [D loss: 0.207259, acc.: 66.41%] [G loss: 0.502261]\n",
      "epoch:8 step:8230 [D loss: 0.246413, acc.: 57.03%] [G loss: 0.452347]\n",
      "epoch:8 step:8231 [D loss: 0.231117, acc.: 60.94%] [G loss: 0.452734]\n",
      "epoch:8 step:8232 [D loss: 0.227206, acc.: 64.84%] [G loss: 0.459662]\n",
      "epoch:8 step:8233 [D loss: 0.202873, acc.: 70.31%] [G loss: 0.479454]\n",
      "epoch:8 step:8234 [D loss: 0.221418, acc.: 61.72%] [G loss: 0.455847]\n",
      "epoch:8 step:8235 [D loss: 0.269043, acc.: 50.00%] [G loss: 0.439813]\n",
      "epoch:8 step:8236 [D loss: 0.221309, acc.: 66.41%] [G loss: 0.420644]\n",
      "epoch:8 step:8237 [D loss: 0.270130, acc.: 51.56%] [G loss: 0.435092]\n",
      "epoch:8 step:8238 [D loss: 0.211865, acc.: 68.75%] [G loss: 0.443984]\n",
      "epoch:8 step:8239 [D loss: 0.206975, acc.: 67.97%] [G loss: 0.490253]\n",
      "epoch:8 step:8240 [D loss: 0.221106, acc.: 61.72%] [G loss: 0.467868]\n",
      "epoch:8 step:8241 [D loss: 0.241491, acc.: 61.72%] [G loss: 0.465381]\n",
      "epoch:8 step:8242 [D loss: 0.202633, acc.: 65.62%] [G loss: 0.504029]\n",
      "epoch:8 step:8243 [D loss: 0.175005, acc.: 76.56%] [G loss: 0.502259]\n",
      "epoch:8 step:8244 [D loss: 0.224057, acc.: 64.06%] [G loss: 0.433964]\n",
      "epoch:8 step:8245 [D loss: 0.228105, acc.: 60.16%] [G loss: 0.472496]\n",
      "epoch:8 step:8246 [D loss: 0.216322, acc.: 61.72%] [G loss: 0.459916]\n",
      "epoch:8 step:8247 [D loss: 0.238776, acc.: 60.94%] [G loss: 0.449541]\n",
      "epoch:8 step:8248 [D loss: 0.242454, acc.: 59.38%] [G loss: 0.465137]\n",
      "epoch:8 step:8249 [D loss: 0.229235, acc.: 61.72%] [G loss: 0.446232]\n",
      "epoch:8 step:8250 [D loss: 0.206685, acc.: 68.75%] [G loss: 0.489016]\n",
      "epoch:8 step:8251 [D loss: 0.203750, acc.: 68.75%] [G loss: 0.516546]\n",
      "epoch:8 step:8252 [D loss: 0.216351, acc.: 62.50%] [G loss: 0.458484]\n",
      "epoch:8 step:8253 [D loss: 0.226690, acc.: 63.28%] [G loss: 0.475134]\n",
      "epoch:8 step:8254 [D loss: 0.211303, acc.: 68.75%] [G loss: 0.459524]\n",
      "epoch:8 step:8255 [D loss: 0.200366, acc.: 67.97%] [G loss: 0.488967]\n",
      "epoch:8 step:8256 [D loss: 0.246384, acc.: 56.25%] [G loss: 0.462651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8257 [D loss: 0.210230, acc.: 67.19%] [G loss: 0.481101]\n",
      "epoch:8 step:8258 [D loss: 0.217980, acc.: 60.16%] [G loss: 0.458314]\n",
      "epoch:8 step:8259 [D loss: 0.211664, acc.: 63.28%] [G loss: 0.496438]\n",
      "epoch:8 step:8260 [D loss: 0.206106, acc.: 67.97%] [G loss: 0.453695]\n",
      "epoch:8 step:8261 [D loss: 0.278323, acc.: 53.91%] [G loss: 0.426679]\n",
      "epoch:8 step:8262 [D loss: 0.241490, acc.: 55.47%] [G loss: 0.499627]\n",
      "epoch:8 step:8263 [D loss: 0.204638, acc.: 68.75%] [G loss: 0.535282]\n",
      "epoch:8 step:8264 [D loss: 0.280582, acc.: 54.69%] [G loss: 0.446516]\n",
      "epoch:8 step:8265 [D loss: 0.217663, acc.: 61.72%] [G loss: 0.515822]\n",
      "epoch:8 step:8266 [D loss: 0.202901, acc.: 64.84%] [G loss: 0.536038]\n",
      "epoch:8 step:8267 [D loss: 0.237304, acc.: 61.72%] [G loss: 0.475486]\n",
      "epoch:8 step:8268 [D loss: 0.224976, acc.: 64.84%] [G loss: 0.473038]\n",
      "epoch:8 step:8269 [D loss: 0.222686, acc.: 65.62%] [G loss: 0.452357]\n",
      "epoch:8 step:8270 [D loss: 0.256470, acc.: 58.59%] [G loss: 0.470541]\n",
      "epoch:8 step:8271 [D loss: 0.212514, acc.: 62.50%] [G loss: 0.495884]\n",
      "epoch:8 step:8272 [D loss: 0.226923, acc.: 60.16%] [G loss: 0.510222]\n",
      "epoch:8 step:8273 [D loss: 0.236480, acc.: 62.50%] [G loss: 0.480945]\n",
      "epoch:8 step:8274 [D loss: 0.234021, acc.: 64.06%] [G loss: 0.435780]\n",
      "epoch:8 step:8275 [D loss: 0.229771, acc.: 60.16%] [G loss: 0.474181]\n",
      "epoch:8 step:8276 [D loss: 0.192141, acc.: 74.22%] [G loss: 0.472693]\n",
      "epoch:8 step:8277 [D loss: 0.181305, acc.: 68.75%] [G loss: 0.476646]\n",
      "epoch:8 step:8278 [D loss: 0.211858, acc.: 71.88%] [G loss: 0.568422]\n",
      "epoch:8 step:8279 [D loss: 0.239323, acc.: 62.50%] [G loss: 0.501964]\n",
      "epoch:8 step:8280 [D loss: 0.275528, acc.: 53.91%] [G loss: 0.420296]\n",
      "epoch:8 step:8281 [D loss: 0.229230, acc.: 62.50%] [G loss: 0.449732]\n",
      "epoch:8 step:8282 [D loss: 0.184052, acc.: 73.44%] [G loss: 0.499865]\n",
      "epoch:8 step:8283 [D loss: 0.240941, acc.: 57.81%] [G loss: 0.487968]\n",
      "epoch:8 step:8284 [D loss: 0.247876, acc.: 57.81%] [G loss: 0.427708]\n",
      "epoch:8 step:8285 [D loss: 0.235385, acc.: 58.59%] [G loss: 0.454085]\n",
      "epoch:8 step:8286 [D loss: 0.208821, acc.: 67.97%] [G loss: 0.448771]\n",
      "epoch:8 step:8287 [D loss: 0.252042, acc.: 54.69%] [G loss: 0.432741]\n",
      "epoch:8 step:8288 [D loss: 0.211499, acc.: 69.53%] [G loss: 0.448933]\n",
      "epoch:8 step:8289 [D loss: 0.202996, acc.: 68.75%] [G loss: 0.498882]\n",
      "epoch:8 step:8290 [D loss: 0.246105, acc.: 58.59%] [G loss: 0.481587]\n",
      "epoch:8 step:8291 [D loss: 0.246655, acc.: 60.94%] [G loss: 0.468857]\n",
      "epoch:8 step:8292 [D loss: 0.190417, acc.: 71.88%] [G loss: 0.492118]\n",
      "epoch:8 step:8293 [D loss: 0.234596, acc.: 59.38%] [G loss: 0.441690]\n",
      "epoch:8 step:8294 [D loss: 0.214704, acc.: 65.62%] [G loss: 0.444812]\n",
      "epoch:8 step:8295 [D loss: 0.176519, acc.: 75.78%] [G loss: 0.487075]\n",
      "epoch:8 step:8296 [D loss: 0.223920, acc.: 64.06%] [G loss: 0.462165]\n",
      "epoch:8 step:8297 [D loss: 0.186302, acc.: 71.09%] [G loss: 0.538979]\n",
      "epoch:8 step:8298 [D loss: 0.206903, acc.: 68.75%] [G loss: 0.554031]\n",
      "epoch:8 step:8299 [D loss: 0.190168, acc.: 71.09%] [G loss: 0.525945]\n",
      "epoch:8 step:8300 [D loss: 0.252256, acc.: 54.69%] [G loss: 0.450216]\n",
      "epoch:8 step:8301 [D loss: 0.200151, acc.: 73.44%] [G loss: 0.484965]\n",
      "epoch:8 step:8302 [D loss: 0.226111, acc.: 58.59%] [G loss: 0.485077]\n",
      "epoch:8 step:8303 [D loss: 0.209391, acc.: 67.19%] [G loss: 0.500019]\n",
      "epoch:8 step:8304 [D loss: 0.219696, acc.: 67.19%] [G loss: 0.465826]\n",
      "epoch:8 step:8305 [D loss: 0.244709, acc.: 58.59%] [G loss: 0.432248]\n",
      "epoch:8 step:8306 [D loss: 0.224280, acc.: 57.81%] [G loss: 0.456689]\n",
      "epoch:8 step:8307 [D loss: 0.244807, acc.: 57.81%] [G loss: 0.456701]\n",
      "epoch:8 step:8308 [D loss: 0.236721, acc.: 57.03%] [G loss: 0.472037]\n",
      "epoch:8 step:8309 [D loss: 0.233642, acc.: 56.25%] [G loss: 0.447498]\n",
      "epoch:8 step:8310 [D loss: 0.199662, acc.: 71.09%] [G loss: 0.519518]\n",
      "epoch:8 step:8311 [D loss: 0.192184, acc.: 76.56%] [G loss: 0.477838]\n",
      "epoch:8 step:8312 [D loss: 0.236027, acc.: 57.03%] [G loss: 0.506599]\n",
      "epoch:8 step:8313 [D loss: 0.239346, acc.: 62.50%] [G loss: 0.469741]\n",
      "epoch:8 step:8314 [D loss: 0.258630, acc.: 50.78%] [G loss: 0.435269]\n",
      "epoch:8 step:8315 [D loss: 0.213058, acc.: 67.19%] [G loss: 0.479086]\n",
      "epoch:8 step:8316 [D loss: 0.256987, acc.: 54.69%] [G loss: 0.450559]\n",
      "epoch:8 step:8317 [D loss: 0.203346, acc.: 67.19%] [G loss: 0.460000]\n",
      "epoch:8 step:8318 [D loss: 0.212973, acc.: 64.84%] [G loss: 0.492920]\n",
      "epoch:8 step:8319 [D loss: 0.206165, acc.: 68.75%] [G loss: 0.491152]\n",
      "epoch:8 step:8320 [D loss: 0.248341, acc.: 61.72%] [G loss: 0.437082]\n",
      "epoch:8 step:8321 [D loss: 0.217353, acc.: 64.06%] [G loss: 0.464542]\n",
      "epoch:8 step:8322 [D loss: 0.204722, acc.: 68.75%] [G loss: 0.502541]\n",
      "epoch:8 step:8323 [D loss: 0.255662, acc.: 54.69%] [G loss: 0.430010]\n",
      "epoch:8 step:8324 [D loss: 0.248653, acc.: 59.38%] [G loss: 0.474667]\n",
      "epoch:8 step:8325 [D loss: 0.221358, acc.: 62.50%] [G loss: 0.460366]\n",
      "epoch:8 step:8326 [D loss: 0.241366, acc.: 60.16%] [G loss: 0.462776]\n",
      "epoch:8 step:8327 [D loss: 0.245471, acc.: 53.91%] [G loss: 0.416085]\n",
      "epoch:8 step:8328 [D loss: 0.215338, acc.: 63.28%] [G loss: 0.448014]\n",
      "epoch:8 step:8329 [D loss: 0.197546, acc.: 73.44%] [G loss: 0.502126]\n",
      "epoch:8 step:8330 [D loss: 0.209248, acc.: 70.31%] [G loss: 0.451822]\n",
      "epoch:8 step:8331 [D loss: 0.233655, acc.: 62.50%] [G loss: 0.456817]\n",
      "epoch:8 step:8332 [D loss: 0.239162, acc.: 58.59%] [G loss: 0.461740]\n",
      "epoch:8 step:8333 [D loss: 0.177965, acc.: 76.56%] [G loss: 0.493127]\n",
      "epoch:8 step:8334 [D loss: 0.200082, acc.: 71.88%] [G loss: 0.474662]\n",
      "epoch:8 step:8335 [D loss: 0.208840, acc.: 61.72%] [G loss: 0.511045]\n",
      "epoch:8 step:8336 [D loss: 0.225073, acc.: 67.97%] [G loss: 0.486037]\n",
      "epoch:8 step:8337 [D loss: 0.211517, acc.: 67.19%] [G loss: 0.449850]\n",
      "epoch:8 step:8338 [D loss: 0.194971, acc.: 70.31%] [G loss: 0.511781]\n",
      "epoch:8 step:8339 [D loss: 0.209105, acc.: 70.31%] [G loss: 0.515498]\n",
      "epoch:8 step:8340 [D loss: 0.213337, acc.: 67.19%] [G loss: 0.512178]\n",
      "epoch:8 step:8341 [D loss: 0.242435, acc.: 61.72%] [G loss: 0.475629]\n",
      "epoch:8 step:8342 [D loss: 0.237880, acc.: 63.28%] [G loss: 0.448502]\n",
      "epoch:8 step:8343 [D loss: 0.267282, acc.: 54.69%] [G loss: 0.397747]\n",
      "epoch:8 step:8344 [D loss: 0.226080, acc.: 63.28%] [G loss: 0.456335]\n",
      "epoch:8 step:8345 [D loss: 0.226524, acc.: 61.72%] [G loss: 0.435822]\n",
      "epoch:8 step:8346 [D loss: 0.250052, acc.: 55.47%] [G loss: 0.475408]\n",
      "epoch:8 step:8347 [D loss: 0.217636, acc.: 65.62%] [G loss: 0.463944]\n",
      "epoch:8 step:8348 [D loss: 0.189105, acc.: 72.66%] [G loss: 0.483942]\n",
      "epoch:8 step:8349 [D loss: 0.167116, acc.: 76.56%] [G loss: 0.552071]\n",
      "epoch:8 step:8350 [D loss: 0.183306, acc.: 75.78%] [G loss: 0.475393]\n",
      "epoch:8 step:8351 [D loss: 0.235467, acc.: 61.72%] [G loss: 0.465752]\n",
      "epoch:8 step:8352 [D loss: 0.231020, acc.: 62.50%] [G loss: 0.472503]\n",
      "epoch:8 step:8353 [D loss: 0.248653, acc.: 56.25%] [G loss: 0.455073]\n",
      "epoch:8 step:8354 [D loss: 0.278926, acc.: 50.00%] [G loss: 0.418199]\n",
      "epoch:8 step:8355 [D loss: 0.247014, acc.: 58.59%] [G loss: 0.464984]\n",
      "epoch:8 step:8356 [D loss: 0.204343, acc.: 67.19%] [G loss: 0.498532]\n",
      "epoch:8 step:8357 [D loss: 0.231050, acc.: 60.16%] [G loss: 0.491230]\n",
      "epoch:8 step:8358 [D loss: 0.222560, acc.: 66.41%] [G loss: 0.435775]\n",
      "epoch:8 step:8359 [D loss: 0.215617, acc.: 64.06%] [G loss: 0.447926]\n",
      "epoch:8 step:8360 [D loss: 0.228262, acc.: 63.28%] [G loss: 0.424772]\n",
      "epoch:8 step:8361 [D loss: 0.231091, acc.: 64.06%] [G loss: 0.465301]\n",
      "epoch:8 step:8362 [D loss: 0.216667, acc.: 61.72%] [G loss: 0.452275]\n",
      "epoch:8 step:8363 [D loss: 0.250563, acc.: 53.12%] [G loss: 0.440498]\n",
      "epoch:8 step:8364 [D loss: 0.238240, acc.: 60.94%] [G loss: 0.434891]\n",
      "epoch:8 step:8365 [D loss: 0.209810, acc.: 71.09%] [G loss: 0.437177]\n",
      "epoch:8 step:8366 [D loss: 0.203913, acc.: 72.66%] [G loss: 0.486823]\n",
      "epoch:8 step:8367 [D loss: 0.209207, acc.: 67.19%] [G loss: 0.500237]\n",
      "epoch:8 step:8368 [D loss: 0.195494, acc.: 70.31%] [G loss: 0.465542]\n",
      "epoch:8 step:8369 [D loss: 0.243761, acc.: 60.94%] [G loss: 0.474629]\n",
      "epoch:8 step:8370 [D loss: 0.226005, acc.: 59.38%] [G loss: 0.500883]\n",
      "epoch:8 step:8371 [D loss: 0.194886, acc.: 74.22%] [G loss: 0.523555]\n",
      "epoch:8 step:8372 [D loss: 0.209397, acc.: 68.75%] [G loss: 0.507656]\n",
      "epoch:8 step:8373 [D loss: 0.229678, acc.: 57.03%] [G loss: 0.483487]\n",
      "epoch:8 step:8374 [D loss: 0.230685, acc.: 65.62%] [G loss: 0.451806]\n",
      "epoch:8 step:8375 [D loss: 0.235322, acc.: 59.38%] [G loss: 0.459519]\n",
      "epoch:8 step:8376 [D loss: 0.236060, acc.: 60.94%] [G loss: 0.502929]\n",
      "epoch:8 step:8377 [D loss: 0.221219, acc.: 62.50%] [G loss: 0.470307]\n",
      "epoch:8 step:8378 [D loss: 0.201922, acc.: 71.09%] [G loss: 0.463871]\n",
      "epoch:8 step:8379 [D loss: 0.214241, acc.: 68.75%] [G loss: 0.496764]\n",
      "epoch:8 step:8380 [D loss: 0.189009, acc.: 70.31%] [G loss: 0.544584]\n",
      "epoch:8 step:8381 [D loss: 0.229586, acc.: 65.62%] [G loss: 0.532618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8382 [D loss: 0.209350, acc.: 68.75%] [G loss: 0.515981]\n",
      "epoch:8 step:8383 [D loss: 0.211453, acc.: 62.50%] [G loss: 0.488568]\n",
      "epoch:8 step:8384 [D loss: 0.189419, acc.: 75.78%] [G loss: 0.505754]\n",
      "epoch:8 step:8385 [D loss: 0.186033, acc.: 74.22%] [G loss: 0.463860]\n",
      "epoch:8 step:8386 [D loss: 0.192626, acc.: 66.41%] [G loss: 0.474918]\n",
      "epoch:8 step:8387 [D loss: 0.270630, acc.: 51.56%] [G loss: 0.478360]\n",
      "epoch:8 step:8388 [D loss: 0.270080, acc.: 52.34%] [G loss: 0.432258]\n",
      "epoch:8 step:8389 [D loss: 0.229649, acc.: 62.50%] [G loss: 0.478821]\n",
      "epoch:8 step:8390 [D loss: 0.188118, acc.: 73.44%] [G loss: 0.513519]\n",
      "epoch:8 step:8391 [D loss: 0.210989, acc.: 65.62%] [G loss: 0.551645]\n",
      "epoch:8 step:8392 [D loss: 0.197870, acc.: 67.97%] [G loss: 0.531679]\n",
      "epoch:8 step:8393 [D loss: 0.198493, acc.: 69.53%] [G loss: 0.511987]\n",
      "epoch:8 step:8394 [D loss: 0.214411, acc.: 64.06%] [G loss: 0.499380]\n",
      "epoch:8 step:8395 [D loss: 0.190395, acc.: 73.44%] [G loss: 0.496100]\n",
      "epoch:8 step:8396 [D loss: 0.195979, acc.: 70.31%] [G loss: 0.483622]\n",
      "epoch:8 step:8397 [D loss: 0.222710, acc.: 65.62%] [G loss: 0.462862]\n",
      "epoch:8 step:8398 [D loss: 0.256648, acc.: 56.25%] [G loss: 0.446154]\n",
      "epoch:8 step:8399 [D loss: 0.196390, acc.: 71.09%] [G loss: 0.484473]\n",
      "epoch:8 step:8400 [D loss: 0.241863, acc.: 59.38%] [G loss: 0.440446]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.475948\n",
      "FID: 13.205962\n",
      "0 = 12.073360681319217\n",
      "1 = 0.05319241005526816\n",
      "2 = 0.9200000166893005\n",
      "3 = 0.8967999815940857\n",
      "4 = 0.9431999921798706\n",
      "5 = 0.9404362440109253\n",
      "6 = 0.8967999815940857\n",
      "7 = 6.845540343105811\n",
      "8 = 0.0788676252348241\n",
      "9 = 0.7343999743461609\n",
      "10 = 0.7275999784469604\n",
      "11 = 0.7411999702453613\n",
      "12 = 0.7376317977905273\n",
      "13 = 0.7275999784469604\n",
      "14 = 7.476016998291016\n",
      "15 = 9.521571159362793\n",
      "16 = 0.12046360969543457\n",
      "17 = 7.475948333740234\n",
      "18 = 13.205962181091309\n",
      "epoch:8 step:8401 [D loss: 0.211017, acc.: 69.53%] [G loss: 0.488052]\n",
      "epoch:8 step:8402 [D loss: 0.197971, acc.: 71.09%] [G loss: 0.509718]\n",
      "epoch:8 step:8403 [D loss: 0.215737, acc.: 61.72%] [G loss: 0.507120]\n",
      "epoch:8 step:8404 [D loss: 0.204450, acc.: 71.88%] [G loss: 0.472907]\n",
      "epoch:8 step:8405 [D loss: 0.183701, acc.: 72.66%] [G loss: 0.474945]\n",
      "epoch:8 step:8406 [D loss: 0.217888, acc.: 62.50%] [G loss: 0.463286]\n",
      "epoch:8 step:8407 [D loss: 0.205764, acc.: 65.62%] [G loss: 0.470267]\n",
      "epoch:8 step:8408 [D loss: 0.207542, acc.: 69.53%] [G loss: 0.480160]\n",
      "epoch:8 step:8409 [D loss: 0.243701, acc.: 59.38%] [G loss: 0.442494]\n",
      "epoch:8 step:8410 [D loss: 0.219291, acc.: 64.84%] [G loss: 0.448388]\n",
      "epoch:8 step:8411 [D loss: 0.240208, acc.: 60.94%] [G loss: 0.467244]\n",
      "epoch:8 step:8412 [D loss: 0.239804, acc.: 60.94%] [G loss: 0.434421]\n",
      "epoch:8 step:8413 [D loss: 0.252166, acc.: 60.94%] [G loss: 0.474294]\n",
      "epoch:8 step:8414 [D loss: 0.170982, acc.: 78.12%] [G loss: 0.548701]\n",
      "epoch:8 step:8415 [D loss: 0.215799, acc.: 66.41%] [G loss: 0.564815]\n",
      "epoch:8 step:8416 [D loss: 0.270339, acc.: 49.22%] [G loss: 0.562911]\n",
      "epoch:8 step:8417 [D loss: 0.179664, acc.: 72.66%] [G loss: 0.507119]\n",
      "epoch:8 step:8418 [D loss: 0.243794, acc.: 59.38%] [G loss: 0.471132]\n",
      "epoch:8 step:8419 [D loss: 0.208894, acc.: 67.19%] [G loss: 0.474833]\n",
      "epoch:8 step:8420 [D loss: 0.172118, acc.: 76.56%] [G loss: 0.510499]\n",
      "epoch:8 step:8421 [D loss: 0.158042, acc.: 82.03%] [G loss: 0.537573]\n",
      "epoch:8 step:8422 [D loss: 0.177288, acc.: 74.22%] [G loss: 0.570479]\n",
      "epoch:8 step:8423 [D loss: 0.150678, acc.: 78.12%] [G loss: 0.578146]\n",
      "epoch:8 step:8424 [D loss: 0.321337, acc.: 53.91%] [G loss: 0.550658]\n",
      "epoch:8 step:8425 [D loss: 0.181409, acc.: 74.22%] [G loss: 0.648704]\n",
      "epoch:8 step:8426 [D loss: 0.208620, acc.: 65.62%] [G loss: 0.599004]\n",
      "epoch:8 step:8427 [D loss: 0.251227, acc.: 57.03%] [G loss: 0.436438]\n",
      "epoch:8 step:8428 [D loss: 0.235829, acc.: 64.06%] [G loss: 0.429404]\n",
      "epoch:8 step:8429 [D loss: 0.206473, acc.: 67.19%] [G loss: 0.452228]\n",
      "epoch:8 step:8430 [D loss: 0.230720, acc.: 63.28%] [G loss: 0.441969]\n",
      "epoch:8 step:8431 [D loss: 0.195465, acc.: 71.09%] [G loss: 0.532809]\n",
      "epoch:8 step:8432 [D loss: 0.196816, acc.: 70.31%] [G loss: 0.562784]\n",
      "epoch:8 step:8433 [D loss: 0.157625, acc.: 79.69%] [G loss: 0.593469]\n",
      "epoch:9 step:8434 [D loss: 0.254929, acc.: 60.16%] [G loss: 0.533841]\n",
      "epoch:9 step:8435 [D loss: 0.239625, acc.: 60.16%] [G loss: 0.488547]\n",
      "epoch:9 step:8436 [D loss: 0.238824, acc.: 58.59%] [G loss: 0.471253]\n",
      "epoch:9 step:8437 [D loss: 0.224597, acc.: 64.84%] [G loss: 0.493086]\n",
      "epoch:9 step:8438 [D loss: 0.209002, acc.: 67.19%] [G loss: 0.511321]\n",
      "epoch:9 step:8439 [D loss: 0.220398, acc.: 59.38%] [G loss: 0.530352]\n",
      "epoch:9 step:8440 [D loss: 0.206861, acc.: 71.09%] [G loss: 0.549306]\n",
      "epoch:9 step:8441 [D loss: 0.217817, acc.: 69.53%] [G loss: 0.449127]\n",
      "epoch:9 step:8442 [D loss: 0.207737, acc.: 67.97%] [G loss: 0.441227]\n",
      "epoch:9 step:8443 [D loss: 0.226562, acc.: 62.50%] [G loss: 0.497499]\n",
      "epoch:9 step:8444 [D loss: 0.216262, acc.: 70.31%] [G loss: 0.495768]\n",
      "epoch:9 step:8445 [D loss: 0.206133, acc.: 67.97%] [G loss: 0.517151]\n",
      "epoch:9 step:8446 [D loss: 0.222803, acc.: 64.84%] [G loss: 0.496268]\n",
      "epoch:9 step:8447 [D loss: 0.197384, acc.: 69.53%] [G loss: 0.514842]\n",
      "epoch:9 step:8448 [D loss: 0.189323, acc.: 69.53%] [G loss: 0.538800]\n",
      "epoch:9 step:8449 [D loss: 0.205655, acc.: 68.75%] [G loss: 0.549134]\n",
      "epoch:9 step:8450 [D loss: 0.260063, acc.: 57.81%] [G loss: 0.465488]\n",
      "epoch:9 step:8451 [D loss: 0.246843, acc.: 64.84%] [G loss: 0.493291]\n",
      "epoch:9 step:8452 [D loss: 0.251260, acc.: 57.81%] [G loss: 0.469963]\n",
      "epoch:9 step:8453 [D loss: 0.270344, acc.: 50.78%] [G loss: 0.499824]\n",
      "epoch:9 step:8454 [D loss: 0.185610, acc.: 73.44%] [G loss: 0.537034]\n",
      "epoch:9 step:8455 [D loss: 0.172969, acc.: 71.88%] [G loss: 0.539377]\n",
      "epoch:9 step:8456 [D loss: 0.225062, acc.: 60.94%] [G loss: 0.498479]\n",
      "epoch:9 step:8457 [D loss: 0.210657, acc.: 67.19%] [G loss: 0.427898]\n",
      "epoch:9 step:8458 [D loss: 0.202606, acc.: 70.31%] [G loss: 0.463158]\n",
      "epoch:9 step:8459 [D loss: 0.210475, acc.: 64.06%] [G loss: 0.492933]\n",
      "epoch:9 step:8460 [D loss: 0.216124, acc.: 64.06%] [G loss: 0.469867]\n",
      "epoch:9 step:8461 [D loss: 0.205192, acc.: 66.41%] [G loss: 0.467102]\n",
      "epoch:9 step:8462 [D loss: 0.202322, acc.: 67.19%] [G loss: 0.498117]\n",
      "epoch:9 step:8463 [D loss: 0.217214, acc.: 67.19%] [G loss: 0.515174]\n",
      "epoch:9 step:8464 [D loss: 0.215624, acc.: 65.62%] [G loss: 0.460800]\n",
      "epoch:9 step:8465 [D loss: 0.220692, acc.: 69.53%] [G loss: 0.461685]\n",
      "epoch:9 step:8466 [D loss: 0.228582, acc.: 58.59%] [G loss: 0.432904]\n",
      "epoch:9 step:8467 [D loss: 0.217316, acc.: 71.09%] [G loss: 0.458510]\n",
      "epoch:9 step:8468 [D loss: 0.218553, acc.: 64.84%] [G loss: 0.447698]\n",
      "epoch:9 step:8469 [D loss: 0.194576, acc.: 70.31%] [G loss: 0.540842]\n",
      "epoch:9 step:8470 [D loss: 0.212038, acc.: 67.97%] [G loss: 0.479172]\n",
      "epoch:9 step:8471 [D loss: 0.263114, acc.: 53.12%] [G loss: 0.447450]\n",
      "epoch:9 step:8472 [D loss: 0.200900, acc.: 67.19%] [G loss: 0.486901]\n",
      "epoch:9 step:8473 [D loss: 0.168093, acc.: 76.56%] [G loss: 0.521618]\n",
      "epoch:9 step:8474 [D loss: 0.211542, acc.: 70.31%] [G loss: 0.491791]\n",
      "epoch:9 step:8475 [D loss: 0.197989, acc.: 72.66%] [G loss: 0.458354]\n",
      "epoch:9 step:8476 [D loss: 0.205491, acc.: 65.62%] [G loss: 0.475605]\n",
      "epoch:9 step:8477 [D loss: 0.262417, acc.: 48.44%] [G loss: 0.426718]\n",
      "epoch:9 step:8478 [D loss: 0.218125, acc.: 63.28%] [G loss: 0.461195]\n",
      "epoch:9 step:8479 [D loss: 0.199452, acc.: 68.75%] [G loss: 0.472850]\n",
      "epoch:9 step:8480 [D loss: 0.204747, acc.: 67.19%] [G loss: 0.470218]\n",
      "epoch:9 step:8481 [D loss: 0.207628, acc.: 70.31%] [G loss: 0.461506]\n",
      "epoch:9 step:8482 [D loss: 0.218724, acc.: 66.41%] [G loss: 0.499786]\n",
      "epoch:9 step:8483 [D loss: 0.216990, acc.: 69.53%] [G loss: 0.468555]\n",
      "epoch:9 step:8484 [D loss: 0.237649, acc.: 59.38%] [G loss: 0.474002]\n",
      "epoch:9 step:8485 [D loss: 0.192597, acc.: 71.88%] [G loss: 0.517396]\n",
      "epoch:9 step:8486 [D loss: 0.191568, acc.: 70.31%] [G loss: 0.512589]\n",
      "epoch:9 step:8487 [D loss: 0.210595, acc.: 64.84%] [G loss: 0.521724]\n",
      "epoch:9 step:8488 [D loss: 0.211409, acc.: 69.53%] [G loss: 0.513572]\n",
      "epoch:9 step:8489 [D loss: 0.219974, acc.: 69.53%] [G loss: 0.479858]\n",
      "epoch:9 step:8490 [D loss: 0.189256, acc.: 72.66%] [G loss: 0.522960]\n",
      "epoch:9 step:8491 [D loss: 0.235467, acc.: 62.50%] [G loss: 0.503207]\n",
      "epoch:9 step:8492 [D loss: 0.217091, acc.: 68.75%] [G loss: 0.485551]\n",
      "epoch:9 step:8493 [D loss: 0.224395, acc.: 61.72%] [G loss: 0.441305]\n",
      "epoch:9 step:8494 [D loss: 0.236248, acc.: 63.28%] [G loss: 0.424535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8495 [D loss: 0.216523, acc.: 63.28%] [G loss: 0.492370]\n",
      "epoch:9 step:8496 [D loss: 0.232698, acc.: 57.81%] [G loss: 0.430083]\n",
      "epoch:9 step:8497 [D loss: 0.200976, acc.: 72.66%] [G loss: 0.489848]\n",
      "epoch:9 step:8498 [D loss: 0.221374, acc.: 62.50%] [G loss: 0.450826]\n",
      "epoch:9 step:8499 [D loss: 0.207387, acc.: 68.75%] [G loss: 0.419487]\n",
      "epoch:9 step:8500 [D loss: 0.206071, acc.: 67.19%] [G loss: 0.466995]\n",
      "epoch:9 step:8501 [D loss: 0.199938, acc.: 63.28%] [G loss: 0.517516]\n",
      "epoch:9 step:8502 [D loss: 0.216910, acc.: 65.62%] [G loss: 0.463189]\n",
      "epoch:9 step:8503 [D loss: 0.191921, acc.: 69.53%] [G loss: 0.491103]\n",
      "epoch:9 step:8504 [D loss: 0.224132, acc.: 67.19%] [G loss: 0.491405]\n",
      "epoch:9 step:8505 [D loss: 0.237301, acc.: 56.25%] [G loss: 0.423268]\n",
      "epoch:9 step:8506 [D loss: 0.210845, acc.: 68.75%] [G loss: 0.443955]\n",
      "epoch:9 step:8507 [D loss: 0.189723, acc.: 69.53%] [G loss: 0.460105]\n",
      "epoch:9 step:8508 [D loss: 0.203963, acc.: 67.19%] [G loss: 0.517604]\n",
      "epoch:9 step:8509 [D loss: 0.200322, acc.: 69.53%] [G loss: 0.497485]\n",
      "epoch:9 step:8510 [D loss: 0.179576, acc.: 71.09%] [G loss: 0.525885]\n",
      "epoch:9 step:8511 [D loss: 0.279216, acc.: 50.78%] [G loss: 0.450840]\n",
      "epoch:9 step:8512 [D loss: 0.233366, acc.: 59.38%] [G loss: 0.416156]\n",
      "epoch:9 step:8513 [D loss: 0.210143, acc.: 66.41%] [G loss: 0.435606]\n",
      "epoch:9 step:8514 [D loss: 0.265274, acc.: 54.69%] [G loss: 0.445411]\n",
      "epoch:9 step:8515 [D loss: 0.221135, acc.: 64.84%] [G loss: 0.431149]\n",
      "epoch:9 step:8516 [D loss: 0.203017, acc.: 67.97%] [G loss: 0.495724]\n",
      "epoch:9 step:8517 [D loss: 0.225478, acc.: 63.28%] [G loss: 0.482150]\n",
      "epoch:9 step:8518 [D loss: 0.201451, acc.: 70.31%] [G loss: 0.512251]\n",
      "epoch:9 step:8519 [D loss: 0.219627, acc.: 64.84%] [G loss: 0.471903]\n",
      "epoch:9 step:8520 [D loss: 0.220471, acc.: 66.41%] [G loss: 0.447656]\n",
      "epoch:9 step:8521 [D loss: 0.221149, acc.: 63.28%] [G loss: 0.460933]\n",
      "epoch:9 step:8522 [D loss: 0.240415, acc.: 62.50%] [G loss: 0.496308]\n",
      "epoch:9 step:8523 [D loss: 0.208748, acc.: 65.62%] [G loss: 0.481853]\n",
      "epoch:9 step:8524 [D loss: 0.229048, acc.: 58.59%] [G loss: 0.505588]\n",
      "epoch:9 step:8525 [D loss: 0.216055, acc.: 66.41%] [G loss: 0.448441]\n",
      "epoch:9 step:8526 [D loss: 0.233142, acc.: 63.28%] [G loss: 0.463021]\n",
      "epoch:9 step:8527 [D loss: 0.225607, acc.: 63.28%] [G loss: 0.494652]\n",
      "epoch:9 step:8528 [D loss: 0.229935, acc.: 61.72%] [G loss: 0.455362]\n",
      "epoch:9 step:8529 [D loss: 0.195415, acc.: 66.41%] [G loss: 0.500280]\n",
      "epoch:9 step:8530 [D loss: 0.188124, acc.: 75.00%] [G loss: 0.498238]\n",
      "epoch:9 step:8531 [D loss: 0.242308, acc.: 59.38%] [G loss: 0.456096]\n",
      "epoch:9 step:8532 [D loss: 0.206851, acc.: 68.75%] [G loss: 0.494258]\n",
      "epoch:9 step:8533 [D loss: 0.182970, acc.: 72.66%] [G loss: 0.489429]\n",
      "epoch:9 step:8534 [D loss: 0.206992, acc.: 64.84%] [G loss: 0.508066]\n",
      "epoch:9 step:8535 [D loss: 0.278153, acc.: 46.09%] [G loss: 0.407773]\n",
      "epoch:9 step:8536 [D loss: 0.213253, acc.: 61.72%] [G loss: 0.468347]\n",
      "epoch:9 step:8537 [D loss: 0.192940, acc.: 70.31%] [G loss: 0.469187]\n",
      "epoch:9 step:8538 [D loss: 0.238679, acc.: 58.59%] [G loss: 0.406521]\n",
      "epoch:9 step:8539 [D loss: 0.218708, acc.: 64.84%] [G loss: 0.484213]\n",
      "epoch:9 step:8540 [D loss: 0.201891, acc.: 67.97%] [G loss: 0.567510]\n",
      "epoch:9 step:8541 [D loss: 0.312367, acc.: 42.97%] [G loss: 0.468335]\n",
      "epoch:9 step:8542 [D loss: 0.248623, acc.: 57.03%] [G loss: 0.472786]\n",
      "epoch:9 step:8543 [D loss: 0.246802, acc.: 53.91%] [G loss: 0.465234]\n",
      "epoch:9 step:8544 [D loss: 0.197674, acc.: 69.53%] [G loss: 0.469471]\n",
      "epoch:9 step:8545 [D loss: 0.232494, acc.: 60.16%] [G loss: 0.478124]\n",
      "epoch:9 step:8546 [D loss: 0.226043, acc.: 63.28%] [G loss: 0.497377]\n",
      "epoch:9 step:8547 [D loss: 0.231391, acc.: 63.28%] [G loss: 0.491317]\n",
      "epoch:9 step:8548 [D loss: 0.231873, acc.: 64.84%] [G loss: 0.511606]\n",
      "epoch:9 step:8549 [D loss: 0.195557, acc.: 70.31%] [G loss: 0.523957]\n",
      "epoch:9 step:8550 [D loss: 0.193719, acc.: 70.31%] [G loss: 0.530711]\n",
      "epoch:9 step:8551 [D loss: 0.233773, acc.: 64.84%] [G loss: 0.447530]\n",
      "epoch:9 step:8552 [D loss: 0.198012, acc.: 67.19%] [G loss: 0.562815]\n",
      "epoch:9 step:8553 [D loss: 0.240878, acc.: 60.94%] [G loss: 0.524741]\n",
      "epoch:9 step:8554 [D loss: 0.227920, acc.: 66.41%] [G loss: 0.493653]\n",
      "epoch:9 step:8555 [D loss: 0.173065, acc.: 76.56%] [G loss: 0.490058]\n",
      "epoch:9 step:8556 [D loss: 0.198444, acc.: 66.41%] [G loss: 0.500767]\n",
      "epoch:9 step:8557 [D loss: 0.236824, acc.: 60.16%] [G loss: 0.484650]\n",
      "epoch:9 step:8558 [D loss: 0.213964, acc.: 63.28%] [G loss: 0.441725]\n",
      "epoch:9 step:8559 [D loss: 0.175416, acc.: 76.56%] [G loss: 0.470552]\n",
      "epoch:9 step:8560 [D loss: 0.221859, acc.: 60.94%] [G loss: 0.468163]\n",
      "epoch:9 step:8561 [D loss: 0.231751, acc.: 58.59%] [G loss: 0.448688]\n",
      "epoch:9 step:8562 [D loss: 0.226570, acc.: 61.72%] [G loss: 0.440687]\n",
      "epoch:9 step:8563 [D loss: 0.210211, acc.: 65.62%] [G loss: 0.487219]\n",
      "epoch:9 step:8564 [D loss: 0.208797, acc.: 63.28%] [G loss: 0.519095]\n",
      "epoch:9 step:8565 [D loss: 0.251667, acc.: 57.81%] [G loss: 0.445560]\n",
      "epoch:9 step:8566 [D loss: 0.227110, acc.: 61.72%] [G loss: 0.470674]\n",
      "epoch:9 step:8567 [D loss: 0.204259, acc.: 67.97%] [G loss: 0.470456]\n",
      "epoch:9 step:8568 [D loss: 0.209657, acc.: 66.41%] [G loss: 0.488724]\n",
      "epoch:9 step:8569 [D loss: 0.236747, acc.: 64.06%] [G loss: 0.498106]\n",
      "epoch:9 step:8570 [D loss: 0.240312, acc.: 59.38%] [G loss: 0.473142]\n",
      "epoch:9 step:8571 [D loss: 0.246216, acc.: 60.94%] [G loss: 0.452447]\n",
      "epoch:9 step:8572 [D loss: 0.222712, acc.: 67.19%] [G loss: 0.421428]\n",
      "epoch:9 step:8573 [D loss: 0.223189, acc.: 61.72%] [G loss: 0.496106]\n",
      "epoch:9 step:8574 [D loss: 0.220630, acc.: 64.06%] [G loss: 0.481356]\n",
      "epoch:9 step:8575 [D loss: 0.227038, acc.: 60.16%] [G loss: 0.468405]\n",
      "epoch:9 step:8576 [D loss: 0.219869, acc.: 61.72%] [G loss: 0.464078]\n",
      "epoch:9 step:8577 [D loss: 0.219759, acc.: 70.31%] [G loss: 0.477166]\n",
      "epoch:9 step:8578 [D loss: 0.220238, acc.: 65.62%] [G loss: 0.514004]\n",
      "epoch:9 step:8579 [D loss: 0.235906, acc.: 58.59%] [G loss: 0.479445]\n",
      "epoch:9 step:8580 [D loss: 0.240622, acc.: 60.94%] [G loss: 0.457399]\n",
      "epoch:9 step:8581 [D loss: 0.242179, acc.: 62.50%] [G loss: 0.433515]\n",
      "epoch:9 step:8582 [D loss: 0.213202, acc.: 68.75%] [G loss: 0.433589]\n",
      "epoch:9 step:8583 [D loss: 0.220772, acc.: 64.06%] [G loss: 0.447040]\n",
      "epoch:9 step:8584 [D loss: 0.205188, acc.: 70.31%] [G loss: 0.485781]\n",
      "epoch:9 step:8585 [D loss: 0.205471, acc.: 69.53%] [G loss: 0.469337]\n",
      "epoch:9 step:8586 [D loss: 0.247914, acc.: 52.34%] [G loss: 0.456318]\n",
      "epoch:9 step:8587 [D loss: 0.207579, acc.: 64.06%] [G loss: 0.490936]\n",
      "epoch:9 step:8588 [D loss: 0.188066, acc.: 69.53%] [G loss: 0.473744]\n",
      "epoch:9 step:8589 [D loss: 0.223441, acc.: 69.53%] [G loss: 0.486207]\n",
      "epoch:9 step:8590 [D loss: 0.244762, acc.: 60.94%] [G loss: 0.445756]\n",
      "epoch:9 step:8591 [D loss: 0.211485, acc.: 63.28%] [G loss: 0.474370]\n",
      "epoch:9 step:8592 [D loss: 0.232229, acc.: 65.62%] [G loss: 0.473673]\n",
      "epoch:9 step:8593 [D loss: 0.254390, acc.: 57.03%] [G loss: 0.475185]\n",
      "epoch:9 step:8594 [D loss: 0.220083, acc.: 62.50%] [G loss: 0.488197]\n",
      "epoch:9 step:8595 [D loss: 0.197667, acc.: 69.53%] [G loss: 0.503526]\n",
      "epoch:9 step:8596 [D loss: 0.217454, acc.: 63.28%] [G loss: 0.489649]\n",
      "epoch:9 step:8597 [D loss: 0.227360, acc.: 67.97%] [G loss: 0.446277]\n",
      "epoch:9 step:8598 [D loss: 0.215525, acc.: 64.84%] [G loss: 0.474720]\n",
      "epoch:9 step:8599 [D loss: 0.222050, acc.: 66.41%] [G loss: 0.469159]\n",
      "epoch:9 step:8600 [D loss: 0.235792, acc.: 53.91%] [G loss: 0.452102]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.503511\n",
      "FID: 12.074081\n",
      "0 = 12.064009889388075\n",
      "1 = 0.04828926291529859\n",
      "2 = 0.9133999943733215\n",
      "3 = 0.89410001039505\n",
      "4 = 0.932699978351593\n",
      "5 = 0.9299979209899902\n",
      "6 = 0.89410001039505\n",
      "7 = 6.642729119503495\n",
      "8 = 0.07260957264488513\n",
      "9 = 0.7299000024795532\n",
      "10 = 0.7301999926567078\n",
      "11 = 0.7296000123023987\n",
      "12 = 0.7297621369361877\n",
      "13 = 0.7301999926567078\n",
      "14 = 7.503577709197998\n",
      "15 = 9.448556900024414\n",
      "16 = 0.14200763404369354\n",
      "17 = 7.503511428833008\n",
      "18 = 12.074081420898438\n",
      "epoch:9 step:8601 [D loss: 0.206108, acc.: 67.97%] [G loss: 0.486404]\n",
      "epoch:9 step:8602 [D loss: 0.243468, acc.: 60.94%] [G loss: 0.487984]\n",
      "epoch:9 step:8603 [D loss: 0.250813, acc.: 60.16%] [G loss: 0.424981]\n",
      "epoch:9 step:8604 [D loss: 0.210078, acc.: 65.62%] [G loss: 0.479239]\n",
      "epoch:9 step:8605 [D loss: 0.213454, acc.: 68.75%] [G loss: 0.485453]\n",
      "epoch:9 step:8606 [D loss: 0.210125, acc.: 62.50%] [G loss: 0.469384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8607 [D loss: 0.227413, acc.: 64.84%] [G loss: 0.439652]\n",
      "epoch:9 step:8608 [D loss: 0.230136, acc.: 65.62%] [G loss: 0.484811]\n",
      "epoch:9 step:8609 [D loss: 0.221914, acc.: 57.81%] [G loss: 0.483311]\n",
      "epoch:9 step:8610 [D loss: 0.254119, acc.: 57.81%] [G loss: 0.411874]\n",
      "epoch:9 step:8611 [D loss: 0.225484, acc.: 64.06%] [G loss: 0.487491]\n",
      "epoch:9 step:8612 [D loss: 0.232189, acc.: 61.72%] [G loss: 0.457927]\n",
      "epoch:9 step:8613 [D loss: 0.246712, acc.: 53.91%] [G loss: 0.457710]\n",
      "epoch:9 step:8614 [D loss: 0.258136, acc.: 56.25%] [G loss: 0.434755]\n",
      "epoch:9 step:8615 [D loss: 0.254682, acc.: 58.59%] [G loss: 0.440442]\n",
      "epoch:9 step:8616 [D loss: 0.227406, acc.: 60.16%] [G loss: 0.432841]\n",
      "epoch:9 step:8617 [D loss: 0.215755, acc.: 69.53%] [G loss: 0.435046]\n",
      "epoch:9 step:8618 [D loss: 0.231697, acc.: 63.28%] [G loss: 0.500818]\n",
      "epoch:9 step:8619 [D loss: 0.250509, acc.: 59.38%] [G loss: 0.485450]\n",
      "epoch:9 step:8620 [D loss: 0.230874, acc.: 60.94%] [G loss: 0.509876]\n",
      "epoch:9 step:8621 [D loss: 0.255934, acc.: 59.38%] [G loss: 0.447565]\n",
      "epoch:9 step:8622 [D loss: 0.239091, acc.: 57.81%] [G loss: 0.420956]\n",
      "epoch:9 step:8623 [D loss: 0.183884, acc.: 71.09%] [G loss: 0.473274]\n",
      "epoch:9 step:8624 [D loss: 0.198272, acc.: 68.75%] [G loss: 0.455633]\n",
      "epoch:9 step:8625 [D loss: 0.201831, acc.: 65.62%] [G loss: 0.469904]\n",
      "epoch:9 step:8626 [D loss: 0.216528, acc.: 64.84%] [G loss: 0.495899]\n",
      "epoch:9 step:8627 [D loss: 0.184404, acc.: 69.53%] [G loss: 0.508337]\n",
      "epoch:9 step:8628 [D loss: 0.215729, acc.: 64.06%] [G loss: 0.505456]\n",
      "epoch:9 step:8629 [D loss: 0.233128, acc.: 58.59%] [G loss: 0.450142]\n",
      "epoch:9 step:8630 [D loss: 0.201041, acc.: 70.31%] [G loss: 0.491126]\n",
      "epoch:9 step:8631 [D loss: 0.196356, acc.: 67.97%] [G loss: 0.505975]\n",
      "epoch:9 step:8632 [D loss: 0.215327, acc.: 68.75%] [G loss: 0.496613]\n",
      "epoch:9 step:8633 [D loss: 0.243593, acc.: 57.03%] [G loss: 0.464165]\n",
      "epoch:9 step:8634 [D loss: 0.253498, acc.: 63.28%] [G loss: 0.440641]\n",
      "epoch:9 step:8635 [D loss: 0.228634, acc.: 60.16%] [G loss: 0.456418]\n",
      "epoch:9 step:8636 [D loss: 0.252867, acc.: 53.91%] [G loss: 0.467871]\n",
      "epoch:9 step:8637 [D loss: 0.211388, acc.: 62.50%] [G loss: 0.463566]\n",
      "epoch:9 step:8638 [D loss: 0.211794, acc.: 67.19%] [G loss: 0.458871]\n",
      "epoch:9 step:8639 [D loss: 0.190408, acc.: 75.00%] [G loss: 0.506736]\n",
      "epoch:9 step:8640 [D loss: 0.211780, acc.: 66.41%] [G loss: 0.484632]\n",
      "epoch:9 step:8641 [D loss: 0.209820, acc.: 70.31%] [G loss: 0.522919]\n",
      "epoch:9 step:8642 [D loss: 0.170163, acc.: 75.00%] [G loss: 0.521283]\n",
      "epoch:9 step:8643 [D loss: 0.246005, acc.: 56.25%] [G loss: 0.444272]\n",
      "epoch:9 step:8644 [D loss: 0.214047, acc.: 66.41%] [G loss: 0.454093]\n",
      "epoch:9 step:8645 [D loss: 0.208898, acc.: 68.75%] [G loss: 0.442521]\n",
      "epoch:9 step:8646 [D loss: 0.201439, acc.: 66.41%] [G loss: 0.488373]\n",
      "epoch:9 step:8647 [D loss: 0.250179, acc.: 58.59%] [G loss: 0.422679]\n",
      "epoch:9 step:8648 [D loss: 0.236465, acc.: 60.16%] [G loss: 0.441812]\n",
      "epoch:9 step:8649 [D loss: 0.218186, acc.: 67.19%] [G loss: 0.462023]\n",
      "epoch:9 step:8650 [D loss: 0.220490, acc.: 62.50%] [G loss: 0.492240]\n",
      "epoch:9 step:8651 [D loss: 0.188923, acc.: 71.09%] [G loss: 0.465159]\n",
      "epoch:9 step:8652 [D loss: 0.205920, acc.: 68.75%] [G loss: 0.501477]\n",
      "epoch:9 step:8653 [D loss: 0.279022, acc.: 48.44%] [G loss: 0.527465]\n",
      "epoch:9 step:8654 [D loss: 0.200883, acc.: 67.19%] [G loss: 0.545175]\n",
      "epoch:9 step:8655 [D loss: 0.174430, acc.: 71.88%] [G loss: 0.604484]\n",
      "epoch:9 step:8656 [D loss: 0.210336, acc.: 68.75%] [G loss: 0.508715]\n",
      "epoch:9 step:8657 [D loss: 0.258100, acc.: 61.72%] [G loss: 0.455522]\n",
      "epoch:9 step:8658 [D loss: 0.255719, acc.: 53.12%] [G loss: 0.447772]\n",
      "epoch:9 step:8659 [D loss: 0.233386, acc.: 66.41%] [G loss: 0.450765]\n",
      "epoch:9 step:8660 [D loss: 0.221417, acc.: 63.28%] [G loss: 0.442602]\n",
      "epoch:9 step:8661 [D loss: 0.240431, acc.: 63.28%] [G loss: 0.435254]\n",
      "epoch:9 step:8662 [D loss: 0.208950, acc.: 67.97%] [G loss: 0.476126]\n",
      "epoch:9 step:8663 [D loss: 0.213397, acc.: 62.50%] [G loss: 0.488765]\n",
      "epoch:9 step:8664 [D loss: 0.195415, acc.: 69.53%] [G loss: 0.607832]\n",
      "epoch:9 step:8665 [D loss: 0.169852, acc.: 72.66%] [G loss: 0.617177]\n",
      "epoch:9 step:8666 [D loss: 0.275097, acc.: 57.81%] [G loss: 0.454801]\n",
      "epoch:9 step:8667 [D loss: 0.243794, acc.: 57.03%] [G loss: 0.465874]\n",
      "epoch:9 step:8668 [D loss: 0.220067, acc.: 66.41%] [G loss: 0.434114]\n",
      "epoch:9 step:8669 [D loss: 0.198587, acc.: 67.97%] [G loss: 0.496991]\n",
      "epoch:9 step:8670 [D loss: 0.222932, acc.: 66.41%] [G loss: 0.441782]\n",
      "epoch:9 step:8671 [D loss: 0.216472, acc.: 65.62%] [G loss: 0.472727]\n",
      "epoch:9 step:8672 [D loss: 0.217362, acc.: 64.06%] [G loss: 0.438890]\n",
      "epoch:9 step:8673 [D loss: 0.206519, acc.: 66.41%] [G loss: 0.444923]\n",
      "epoch:9 step:8674 [D loss: 0.232106, acc.: 64.84%] [G loss: 0.498653]\n",
      "epoch:9 step:8675 [D loss: 0.207867, acc.: 64.84%] [G loss: 0.492585]\n",
      "epoch:9 step:8676 [D loss: 0.211461, acc.: 70.31%] [G loss: 0.473722]\n",
      "epoch:9 step:8677 [D loss: 0.223047, acc.: 64.06%] [G loss: 0.484908]\n",
      "epoch:9 step:8678 [D loss: 0.203039, acc.: 68.75%] [G loss: 0.493264]\n",
      "epoch:9 step:8679 [D loss: 0.219806, acc.: 64.84%] [G loss: 0.451713]\n",
      "epoch:9 step:8680 [D loss: 0.234078, acc.: 62.50%] [G loss: 0.434446]\n",
      "epoch:9 step:8681 [D loss: 0.185322, acc.: 72.66%] [G loss: 0.522226]\n",
      "epoch:9 step:8682 [D loss: 0.276923, acc.: 51.56%] [G loss: 0.460000]\n",
      "epoch:9 step:8683 [D loss: 0.285286, acc.: 53.12%] [G loss: 0.462352]\n",
      "epoch:9 step:8684 [D loss: 0.268887, acc.: 52.34%] [G loss: 0.487366]\n",
      "epoch:9 step:8685 [D loss: 0.226557, acc.: 64.06%] [G loss: 0.470343]\n",
      "epoch:9 step:8686 [D loss: 0.219199, acc.: 61.72%] [G loss: 0.465350]\n",
      "epoch:9 step:8687 [D loss: 0.218778, acc.: 67.97%] [G loss: 0.486197]\n",
      "epoch:9 step:8688 [D loss: 0.190717, acc.: 70.31%] [G loss: 0.486011]\n",
      "epoch:9 step:8689 [D loss: 0.214940, acc.: 67.19%] [G loss: 0.453777]\n",
      "epoch:9 step:8690 [D loss: 0.242523, acc.: 58.59%] [G loss: 0.460244]\n",
      "epoch:9 step:8691 [D loss: 0.192785, acc.: 70.31%] [G loss: 0.489098]\n",
      "epoch:9 step:8692 [D loss: 0.236462, acc.: 63.28%] [G loss: 0.461482]\n",
      "epoch:9 step:8693 [D loss: 0.221481, acc.: 64.06%] [G loss: 0.460147]\n",
      "epoch:9 step:8694 [D loss: 0.219766, acc.: 72.66%] [G loss: 0.524721]\n",
      "epoch:9 step:8695 [D loss: 0.212580, acc.: 64.84%] [G loss: 0.509454]\n",
      "epoch:9 step:8696 [D loss: 0.250133, acc.: 57.81%] [G loss: 0.464668]\n",
      "epoch:9 step:8697 [D loss: 0.233549, acc.: 63.28%] [G loss: 0.446920]\n",
      "epoch:9 step:8698 [D loss: 0.244974, acc.: 60.16%] [G loss: 0.435168]\n",
      "epoch:9 step:8699 [D loss: 0.226279, acc.: 60.16%] [G loss: 0.520975]\n",
      "epoch:9 step:8700 [D loss: 0.223233, acc.: 64.06%] [G loss: 0.444237]\n",
      "epoch:9 step:8701 [D loss: 0.223299, acc.: 59.38%] [G loss: 0.483319]\n",
      "epoch:9 step:8702 [D loss: 0.206729, acc.: 70.31%] [G loss: 0.459825]\n",
      "epoch:9 step:8703 [D loss: 0.204204, acc.: 71.88%] [G loss: 0.491213]\n",
      "epoch:9 step:8704 [D loss: 0.180960, acc.: 74.22%] [G loss: 0.488733]\n",
      "epoch:9 step:8705 [D loss: 0.245146, acc.: 57.81%] [G loss: 0.467056]\n",
      "epoch:9 step:8706 [D loss: 0.233342, acc.: 60.16%] [G loss: 0.497352]\n",
      "epoch:9 step:8707 [D loss: 0.210013, acc.: 68.75%] [G loss: 0.497943]\n",
      "epoch:9 step:8708 [D loss: 0.226131, acc.: 64.06%] [G loss: 0.473852]\n",
      "epoch:9 step:8709 [D loss: 0.223509, acc.: 64.84%] [G loss: 0.497298]\n",
      "epoch:9 step:8710 [D loss: 0.265972, acc.: 51.56%] [G loss: 0.440504]\n",
      "epoch:9 step:8711 [D loss: 0.243940, acc.: 60.94%] [G loss: 0.484853]\n",
      "epoch:9 step:8712 [D loss: 0.202964, acc.: 73.44%] [G loss: 0.456984]\n",
      "epoch:9 step:8713 [D loss: 0.215675, acc.: 64.06%] [G loss: 0.483175]\n",
      "epoch:9 step:8714 [D loss: 0.246027, acc.: 56.25%] [G loss: 0.448740]\n",
      "epoch:9 step:8715 [D loss: 0.222701, acc.: 65.62%] [G loss: 0.433860]\n",
      "epoch:9 step:8716 [D loss: 0.201242, acc.: 71.09%] [G loss: 0.416348]\n",
      "epoch:9 step:8717 [D loss: 0.213885, acc.: 71.88%] [G loss: 0.445980]\n",
      "epoch:9 step:8718 [D loss: 0.202297, acc.: 67.97%] [G loss: 0.461945]\n",
      "epoch:9 step:8719 [D loss: 0.193493, acc.: 72.66%] [G loss: 0.541394]\n",
      "epoch:9 step:8720 [D loss: 0.201330, acc.: 67.97%] [G loss: 0.511838]\n",
      "epoch:9 step:8721 [D loss: 0.227612, acc.: 61.72%] [G loss: 0.430374]\n",
      "epoch:9 step:8722 [D loss: 0.205942, acc.: 67.19%] [G loss: 0.492507]\n",
      "epoch:9 step:8723 [D loss: 0.219442, acc.: 64.84%] [G loss: 0.465410]\n",
      "epoch:9 step:8724 [D loss: 0.243847, acc.: 59.38%] [G loss: 0.470367]\n",
      "epoch:9 step:8725 [D loss: 0.259905, acc.: 60.94%] [G loss: 0.511391]\n",
      "epoch:9 step:8726 [D loss: 0.233265, acc.: 60.94%] [G loss: 0.514878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8727 [D loss: 0.246349, acc.: 58.59%] [G loss: 0.460167]\n",
      "epoch:9 step:8728 [D loss: 0.205543, acc.: 69.53%] [G loss: 0.444236]\n",
      "epoch:9 step:8729 [D loss: 0.208571, acc.: 61.72%] [G loss: 0.459737]\n",
      "epoch:9 step:8730 [D loss: 0.197029, acc.: 67.97%] [G loss: 0.538288]\n",
      "epoch:9 step:8731 [D loss: 0.202651, acc.: 71.88%] [G loss: 0.469346]\n",
      "epoch:9 step:8732 [D loss: 0.187519, acc.: 74.22%] [G loss: 0.556993]\n",
      "epoch:9 step:8733 [D loss: 0.234320, acc.: 60.16%] [G loss: 0.469159]\n",
      "epoch:9 step:8734 [D loss: 0.240121, acc.: 62.50%] [G loss: 0.485255]\n",
      "epoch:9 step:8735 [D loss: 0.216339, acc.: 65.62%] [G loss: 0.448527]\n",
      "epoch:9 step:8736 [D loss: 0.230882, acc.: 63.28%] [G loss: 0.442496]\n",
      "epoch:9 step:8737 [D loss: 0.208724, acc.: 65.62%] [G loss: 0.471244]\n",
      "epoch:9 step:8738 [D loss: 0.239333, acc.: 59.38%] [G loss: 0.486334]\n",
      "epoch:9 step:8739 [D loss: 0.215106, acc.: 69.53%] [G loss: 0.458469]\n",
      "epoch:9 step:8740 [D loss: 0.206493, acc.: 66.41%] [G loss: 0.501573]\n",
      "epoch:9 step:8741 [D loss: 0.229665, acc.: 60.94%] [G loss: 0.466411]\n",
      "epoch:9 step:8742 [D loss: 0.183883, acc.: 70.31%] [G loss: 0.483521]\n",
      "epoch:9 step:8743 [D loss: 0.215709, acc.: 66.41%] [G loss: 0.458762]\n",
      "epoch:9 step:8744 [D loss: 0.207905, acc.: 67.97%] [G loss: 0.459146]\n",
      "epoch:9 step:8745 [D loss: 0.184468, acc.: 71.88%] [G loss: 0.519582]\n",
      "epoch:9 step:8746 [D loss: 0.200924, acc.: 60.16%] [G loss: 0.518979]\n",
      "epoch:9 step:8747 [D loss: 0.169572, acc.: 79.69%] [G loss: 0.566597]\n",
      "epoch:9 step:8748 [D loss: 0.173684, acc.: 78.91%] [G loss: 0.557099]\n",
      "epoch:9 step:8749 [D loss: 0.271567, acc.: 57.03%] [G loss: 0.459023]\n",
      "epoch:9 step:8750 [D loss: 0.240578, acc.: 61.72%] [G loss: 0.431131]\n",
      "epoch:9 step:8751 [D loss: 0.209800, acc.: 67.97%] [G loss: 0.489952]\n",
      "epoch:9 step:8752 [D loss: 0.205577, acc.: 64.84%] [G loss: 0.475049]\n",
      "epoch:9 step:8753 [D loss: 0.221327, acc.: 64.06%] [G loss: 0.478539]\n",
      "epoch:9 step:8754 [D loss: 0.183566, acc.: 72.66%] [G loss: 0.509463]\n",
      "epoch:9 step:8755 [D loss: 0.220325, acc.: 66.41%] [G loss: 0.512206]\n",
      "epoch:9 step:8756 [D loss: 0.248219, acc.: 58.59%] [G loss: 0.468148]\n",
      "epoch:9 step:8757 [D loss: 0.258321, acc.: 55.47%] [G loss: 0.429646]\n",
      "epoch:9 step:8758 [D loss: 0.208261, acc.: 69.53%] [G loss: 0.491631]\n",
      "epoch:9 step:8759 [D loss: 0.194999, acc.: 70.31%] [G loss: 0.491693]\n",
      "epoch:9 step:8760 [D loss: 0.257783, acc.: 54.69%] [G loss: 0.457813]\n",
      "epoch:9 step:8761 [D loss: 0.202100, acc.: 71.09%] [G loss: 0.503223]\n",
      "epoch:9 step:8762 [D loss: 0.222615, acc.: 60.94%] [G loss: 0.484741]\n",
      "epoch:9 step:8763 [D loss: 0.230872, acc.: 61.72%] [G loss: 0.467952]\n",
      "epoch:9 step:8764 [D loss: 0.189753, acc.: 68.75%] [G loss: 0.509062]\n",
      "epoch:9 step:8765 [D loss: 0.194991, acc.: 70.31%] [G loss: 0.497824]\n",
      "epoch:9 step:8766 [D loss: 0.213601, acc.: 65.62%] [G loss: 0.498181]\n",
      "epoch:9 step:8767 [D loss: 0.232676, acc.: 60.16%] [G loss: 0.458727]\n",
      "epoch:9 step:8768 [D loss: 0.197586, acc.: 69.53%] [G loss: 0.499702]\n",
      "epoch:9 step:8769 [D loss: 0.232922, acc.: 59.38%] [G loss: 0.464377]\n",
      "epoch:9 step:8770 [D loss: 0.216583, acc.: 69.53%] [G loss: 0.454870]\n",
      "epoch:9 step:8771 [D loss: 0.216647, acc.: 61.72%] [G loss: 0.475389]\n",
      "epoch:9 step:8772 [D loss: 0.219451, acc.: 67.19%] [G loss: 0.462336]\n",
      "epoch:9 step:8773 [D loss: 0.222362, acc.: 64.06%] [G loss: 0.460250]\n",
      "epoch:9 step:8774 [D loss: 0.257714, acc.: 58.59%] [G loss: 0.503827]\n",
      "epoch:9 step:8775 [D loss: 0.221667, acc.: 61.72%] [G loss: 0.499062]\n",
      "epoch:9 step:8776 [D loss: 0.188519, acc.: 72.66%] [G loss: 0.511536]\n",
      "epoch:9 step:8777 [D loss: 0.183826, acc.: 74.22%] [G loss: 0.554262]\n",
      "epoch:9 step:8778 [D loss: 0.200552, acc.: 69.53%] [G loss: 0.504827]\n",
      "epoch:9 step:8779 [D loss: 0.178938, acc.: 70.31%] [G loss: 0.504903]\n",
      "epoch:9 step:8780 [D loss: 0.183310, acc.: 71.09%] [G loss: 0.547122]\n",
      "epoch:9 step:8781 [D loss: 0.295940, acc.: 48.44%] [G loss: 0.460057]\n",
      "epoch:9 step:8782 [D loss: 0.269624, acc.: 53.91%] [G loss: 0.437593]\n",
      "epoch:9 step:8783 [D loss: 0.208089, acc.: 71.09%] [G loss: 0.468721]\n",
      "epoch:9 step:8784 [D loss: 0.211108, acc.: 72.66%] [G loss: 0.462364]\n",
      "epoch:9 step:8785 [D loss: 0.229132, acc.: 64.84%] [G loss: 0.479720]\n",
      "epoch:9 step:8786 [D loss: 0.202713, acc.: 67.19%] [G loss: 0.499475]\n",
      "epoch:9 step:8787 [D loss: 0.196424, acc.: 69.53%] [G loss: 0.533641]\n",
      "epoch:9 step:8788 [D loss: 0.215565, acc.: 66.41%] [G loss: 0.496295]\n",
      "epoch:9 step:8789 [D loss: 0.246466, acc.: 60.16%] [G loss: 0.481780]\n",
      "epoch:9 step:8790 [D loss: 0.196138, acc.: 71.09%] [G loss: 0.450629]\n",
      "epoch:9 step:8791 [D loss: 0.176734, acc.: 75.78%] [G loss: 0.486708]\n",
      "epoch:9 step:8792 [D loss: 0.195648, acc.: 71.88%] [G loss: 0.508614]\n",
      "epoch:9 step:8793 [D loss: 0.189107, acc.: 68.75%] [G loss: 0.550540]\n",
      "epoch:9 step:8794 [D loss: 0.192111, acc.: 68.75%] [G loss: 0.526074]\n",
      "epoch:9 step:8795 [D loss: 0.223340, acc.: 64.84%] [G loss: 0.453421]\n",
      "epoch:9 step:8796 [D loss: 0.201482, acc.: 64.06%] [G loss: 0.473458]\n",
      "epoch:9 step:8797 [D loss: 0.209675, acc.: 72.66%] [G loss: 0.460291]\n",
      "epoch:9 step:8798 [D loss: 0.217887, acc.: 65.62%] [G loss: 0.452168]\n",
      "epoch:9 step:8799 [D loss: 0.218645, acc.: 63.28%] [G loss: 0.525820]\n",
      "epoch:9 step:8800 [D loss: 0.216585, acc.: 61.72%] [G loss: 0.499287]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.441865\n",
      "FID: 16.937761\n",
      "0 = 12.132580425834695\n",
      "1 = 0.05328533436622602\n",
      "2 = 0.918150007724762\n",
      "3 = 0.8981999754905701\n",
      "4 = 0.9380999803543091\n",
      "5 = 0.9355275630950928\n",
      "6 = 0.8981999754905701\n",
      "7 = 7.1240145309567415\n",
      "8 = 0.09147881130171781\n",
      "9 = 0.7320500016212463\n",
      "10 = 0.7303000092506409\n",
      "11 = 0.7337999939918518\n",
      "12 = 0.732865035533905\n",
      "13 = 0.7303000092506409\n",
      "14 = 7.441929340362549\n",
      "15 = 9.372638702392578\n",
      "16 = 0.15263056755065918\n",
      "17 = 7.44186544418335\n",
      "18 = 16.937761306762695\n",
      "epoch:9 step:8801 [D loss: 0.226142, acc.: 60.94%] [G loss: 0.486669]\n",
      "epoch:9 step:8802 [D loss: 0.253807, acc.: 63.28%] [G loss: 0.486985]\n",
      "epoch:9 step:8803 [D loss: 0.197144, acc.: 68.75%] [G loss: 0.496153]\n",
      "epoch:9 step:8804 [D loss: 0.197406, acc.: 71.09%] [G loss: 0.500377]\n",
      "epoch:9 step:8805 [D loss: 0.211879, acc.: 64.84%] [G loss: 0.493545]\n",
      "epoch:9 step:8806 [D loss: 0.225692, acc.: 64.06%] [G loss: 0.484602]\n",
      "epoch:9 step:8807 [D loss: 0.188408, acc.: 68.75%] [G loss: 0.500646]\n",
      "epoch:9 step:8808 [D loss: 0.211288, acc.: 67.97%] [G loss: 0.485494]\n",
      "epoch:9 step:8809 [D loss: 0.296986, acc.: 46.09%] [G loss: 0.412611]\n",
      "epoch:9 step:8810 [D loss: 0.245353, acc.: 56.25%] [G loss: 0.436463]\n",
      "epoch:9 step:8811 [D loss: 0.232724, acc.: 60.94%] [G loss: 0.447228]\n",
      "epoch:9 step:8812 [D loss: 0.226789, acc.: 62.50%] [G loss: 0.421058]\n",
      "epoch:9 step:8813 [D loss: 0.240752, acc.: 64.06%] [G loss: 0.465597]\n",
      "epoch:9 step:8814 [D loss: 0.197731, acc.: 70.31%] [G loss: 0.473508]\n",
      "epoch:9 step:8815 [D loss: 0.226540, acc.: 61.72%] [G loss: 0.472579]\n",
      "epoch:9 step:8816 [D loss: 0.221282, acc.: 58.59%] [G loss: 0.468620]\n",
      "epoch:9 step:8817 [D loss: 0.231911, acc.: 54.69%] [G loss: 0.448561]\n",
      "epoch:9 step:8818 [D loss: 0.213903, acc.: 64.06%] [G loss: 0.457701]\n",
      "epoch:9 step:8819 [D loss: 0.236403, acc.: 61.72%] [G loss: 0.479768]\n",
      "epoch:9 step:8820 [D loss: 0.223329, acc.: 60.94%] [G loss: 0.487575]\n",
      "epoch:9 step:8821 [D loss: 0.205304, acc.: 64.84%] [G loss: 0.493291]\n",
      "epoch:9 step:8822 [D loss: 0.214130, acc.: 64.84%] [G loss: 0.447609]\n",
      "epoch:9 step:8823 [D loss: 0.230751, acc.: 58.59%] [G loss: 0.472728]\n",
      "epoch:9 step:8824 [D loss: 0.200767, acc.: 72.66%] [G loss: 0.484708]\n",
      "epoch:9 step:8825 [D loss: 0.213670, acc.: 65.62%] [G loss: 0.493004]\n",
      "epoch:9 step:8826 [D loss: 0.243069, acc.: 60.94%] [G loss: 0.441294]\n",
      "epoch:9 step:8827 [D loss: 0.250947, acc.: 61.72%] [G loss: 0.410897]\n",
      "epoch:9 step:8828 [D loss: 0.194497, acc.: 67.97%] [G loss: 0.500816]\n",
      "epoch:9 step:8829 [D loss: 0.269338, acc.: 50.78%] [G loss: 0.458715]\n",
      "epoch:9 step:8830 [D loss: 0.229749, acc.: 61.72%] [G loss: 0.446952]\n",
      "epoch:9 step:8831 [D loss: 0.173210, acc.: 75.00%] [G loss: 0.537139]\n",
      "epoch:9 step:8832 [D loss: 0.205441, acc.: 64.84%] [G loss: 0.527938]\n",
      "epoch:9 step:8833 [D loss: 0.256637, acc.: 54.69%] [G loss: 0.452570]\n",
      "epoch:9 step:8834 [D loss: 0.221337, acc.: 68.75%] [G loss: 0.466889]\n",
      "epoch:9 step:8835 [D loss: 0.238227, acc.: 60.94%] [G loss: 0.426806]\n",
      "epoch:9 step:8836 [D loss: 0.249591, acc.: 61.72%] [G loss: 0.467139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8837 [D loss: 0.241042, acc.: 60.94%] [G loss: 0.492188]\n",
      "epoch:9 step:8838 [D loss: 0.180345, acc.: 75.00%] [G loss: 0.523081]\n",
      "epoch:9 step:8839 [D loss: 0.179189, acc.: 75.00%] [G loss: 0.534336]\n",
      "epoch:9 step:8840 [D loss: 0.223898, acc.: 64.06%] [G loss: 0.498696]\n",
      "epoch:9 step:8841 [D loss: 0.239052, acc.: 57.81%] [G loss: 0.474768]\n",
      "epoch:9 step:8842 [D loss: 0.231448, acc.: 58.59%] [G loss: 0.487288]\n",
      "epoch:9 step:8843 [D loss: 0.217658, acc.: 65.62%] [G loss: 0.463274]\n",
      "epoch:9 step:8844 [D loss: 0.218388, acc.: 60.16%] [G loss: 0.436140]\n",
      "epoch:9 step:8845 [D loss: 0.226747, acc.: 60.16%] [G loss: 0.468244]\n",
      "epoch:9 step:8846 [D loss: 0.212689, acc.: 70.31%] [G loss: 0.462113]\n",
      "epoch:9 step:8847 [D loss: 0.225306, acc.: 65.62%] [G loss: 0.480614]\n",
      "epoch:9 step:8848 [D loss: 0.248418, acc.: 61.72%] [G loss: 0.481959]\n",
      "epoch:9 step:8849 [D loss: 0.201135, acc.: 65.62%] [G loss: 0.509784]\n",
      "epoch:9 step:8850 [D loss: 0.235723, acc.: 63.28%] [G loss: 0.488698]\n",
      "epoch:9 step:8851 [D loss: 0.266214, acc.: 50.78%] [G loss: 0.440253]\n",
      "epoch:9 step:8852 [D loss: 0.237252, acc.: 64.84%] [G loss: 0.499719]\n",
      "epoch:9 step:8853 [D loss: 0.223884, acc.: 64.84%] [G loss: 0.482116]\n",
      "epoch:9 step:8854 [D loss: 0.248822, acc.: 60.16%] [G loss: 0.433149]\n",
      "epoch:9 step:8855 [D loss: 0.248221, acc.: 57.03%] [G loss: 0.435101]\n",
      "epoch:9 step:8856 [D loss: 0.209449, acc.: 65.62%] [G loss: 0.457346]\n",
      "epoch:9 step:8857 [D loss: 0.229884, acc.: 56.25%] [G loss: 0.422636]\n",
      "epoch:9 step:8858 [D loss: 0.223254, acc.: 61.72%] [G loss: 0.424693]\n",
      "epoch:9 step:8859 [D loss: 0.207850, acc.: 66.41%] [G loss: 0.476855]\n",
      "epoch:9 step:8860 [D loss: 0.200146, acc.: 70.31%] [G loss: 0.471619]\n",
      "epoch:9 step:8861 [D loss: 0.209506, acc.: 64.84%] [G loss: 0.559861]\n",
      "epoch:9 step:8862 [D loss: 0.207418, acc.: 68.75%] [G loss: 0.537358]\n",
      "epoch:9 step:8863 [D loss: 0.200836, acc.: 68.75%] [G loss: 0.498031]\n",
      "epoch:9 step:8864 [D loss: 0.216509, acc.: 67.97%] [G loss: 0.511230]\n",
      "epoch:9 step:8865 [D loss: 0.219139, acc.: 60.16%] [G loss: 0.492693]\n",
      "epoch:9 step:8866 [D loss: 0.238271, acc.: 62.50%] [G loss: 0.451393]\n",
      "epoch:9 step:8867 [D loss: 0.206636, acc.: 67.97%] [G loss: 0.493115]\n",
      "epoch:9 step:8868 [D loss: 0.244995, acc.: 57.03%] [G loss: 0.476176]\n",
      "epoch:9 step:8869 [D loss: 0.196435, acc.: 69.53%] [G loss: 0.485526]\n",
      "epoch:9 step:8870 [D loss: 0.272349, acc.: 56.25%] [G loss: 0.485589]\n",
      "epoch:9 step:8871 [D loss: 0.243032, acc.: 58.59%] [G loss: 0.391875]\n",
      "epoch:9 step:8872 [D loss: 0.213639, acc.: 64.06%] [G loss: 0.428436]\n",
      "epoch:9 step:8873 [D loss: 0.204478, acc.: 67.19%] [G loss: 0.495058]\n",
      "epoch:9 step:8874 [D loss: 0.249864, acc.: 57.03%] [G loss: 0.491679]\n",
      "epoch:9 step:8875 [D loss: 0.218119, acc.: 67.19%] [G loss: 0.543843]\n",
      "epoch:9 step:8876 [D loss: 0.208645, acc.: 67.97%] [G loss: 0.471001]\n",
      "epoch:9 step:8877 [D loss: 0.240562, acc.: 54.69%] [G loss: 0.421212]\n",
      "epoch:9 step:8878 [D loss: 0.234741, acc.: 61.72%] [G loss: 0.433488]\n",
      "epoch:9 step:8879 [D loss: 0.213849, acc.: 62.50%] [G loss: 0.511064]\n",
      "epoch:9 step:8880 [D loss: 0.197457, acc.: 72.66%] [G loss: 0.484935]\n",
      "epoch:9 step:8881 [D loss: 0.241702, acc.: 54.69%] [G loss: 0.439380]\n",
      "epoch:9 step:8882 [D loss: 0.206742, acc.: 66.41%] [G loss: 0.465049]\n",
      "epoch:9 step:8883 [D loss: 0.183138, acc.: 71.88%] [G loss: 0.439763]\n",
      "epoch:9 step:8884 [D loss: 0.186708, acc.: 71.88%] [G loss: 0.442879]\n",
      "epoch:9 step:8885 [D loss: 0.208742, acc.: 63.28%] [G loss: 0.481603]\n",
      "epoch:9 step:8886 [D loss: 0.224985, acc.: 64.06%] [G loss: 0.507616]\n",
      "epoch:9 step:8887 [D loss: 0.219958, acc.: 60.94%] [G loss: 0.509104]\n",
      "epoch:9 step:8888 [D loss: 0.261490, acc.: 53.12%] [G loss: 0.435708]\n",
      "epoch:9 step:8889 [D loss: 0.242598, acc.: 66.41%] [G loss: 0.468467]\n",
      "epoch:9 step:8890 [D loss: 0.214760, acc.: 64.84%] [G loss: 0.490352]\n",
      "epoch:9 step:8891 [D loss: 0.271352, acc.: 53.12%] [G loss: 0.446383]\n",
      "epoch:9 step:8892 [D loss: 0.232075, acc.: 59.38%] [G loss: 0.442514]\n",
      "epoch:9 step:8893 [D loss: 0.232245, acc.: 58.59%] [G loss: 0.457303]\n",
      "epoch:9 step:8894 [D loss: 0.222019, acc.: 63.28%] [G loss: 0.446935]\n",
      "epoch:9 step:8895 [D loss: 0.229647, acc.: 60.16%] [G loss: 0.430198]\n",
      "epoch:9 step:8896 [D loss: 0.222353, acc.: 68.75%] [G loss: 0.460952]\n",
      "epoch:9 step:8897 [D loss: 0.224531, acc.: 65.62%] [G loss: 0.457776]\n",
      "epoch:9 step:8898 [D loss: 0.243714, acc.: 56.25%] [G loss: 0.449846]\n",
      "epoch:9 step:8899 [D loss: 0.226490, acc.: 61.72%] [G loss: 0.504749]\n",
      "epoch:9 step:8900 [D loss: 0.215233, acc.: 66.41%] [G loss: 0.521761]\n",
      "epoch:9 step:8901 [D loss: 0.244483, acc.: 64.06%] [G loss: 0.476477]\n",
      "epoch:9 step:8902 [D loss: 0.208339, acc.: 66.41%] [G loss: 0.496605]\n",
      "epoch:9 step:8903 [D loss: 0.198571, acc.: 67.19%] [G loss: 0.505073]\n",
      "epoch:9 step:8904 [D loss: 0.184633, acc.: 69.53%] [G loss: 0.549372]\n",
      "epoch:9 step:8905 [D loss: 0.202384, acc.: 71.09%] [G loss: 0.558235]\n",
      "epoch:9 step:8906 [D loss: 0.289153, acc.: 52.34%] [G loss: 0.452205]\n",
      "epoch:9 step:8907 [D loss: 0.180388, acc.: 72.66%] [G loss: 0.502954]\n",
      "epoch:9 step:8908 [D loss: 0.191222, acc.: 71.09%] [G loss: 0.519801]\n",
      "epoch:9 step:8909 [D loss: 0.240032, acc.: 60.16%] [G loss: 0.504473]\n",
      "epoch:9 step:8910 [D loss: 0.262222, acc.: 53.12%] [G loss: 0.430011]\n",
      "epoch:9 step:8911 [D loss: 0.235413, acc.: 64.84%] [G loss: 0.427210]\n",
      "epoch:9 step:8912 [D loss: 0.229005, acc.: 60.94%] [G loss: 0.442323]\n",
      "epoch:9 step:8913 [D loss: 0.218035, acc.: 68.75%] [G loss: 0.483267]\n",
      "epoch:9 step:8914 [D loss: 0.206790, acc.: 70.31%] [G loss: 0.480188]\n",
      "epoch:9 step:8915 [D loss: 0.229724, acc.: 59.38%] [G loss: 0.448349]\n",
      "epoch:9 step:8916 [D loss: 0.254285, acc.: 55.47%] [G loss: 0.437850]\n",
      "epoch:9 step:8917 [D loss: 0.198406, acc.: 67.97%] [G loss: 0.513590]\n",
      "epoch:9 step:8918 [D loss: 0.238694, acc.: 58.59%] [G loss: 0.446459]\n",
      "epoch:9 step:8919 [D loss: 0.219097, acc.: 67.97%] [G loss: 0.456005]\n",
      "epoch:9 step:8920 [D loss: 0.228490, acc.: 61.72%] [G loss: 0.436883]\n",
      "epoch:9 step:8921 [D loss: 0.185116, acc.: 74.22%] [G loss: 0.504539]\n",
      "epoch:9 step:8922 [D loss: 0.247614, acc.: 60.94%] [G loss: 0.506626]\n",
      "epoch:9 step:8923 [D loss: 0.243329, acc.: 64.06%] [G loss: 0.484855]\n",
      "epoch:9 step:8924 [D loss: 0.226770, acc.: 64.06%] [G loss: 0.485904]\n",
      "epoch:9 step:8925 [D loss: 0.228560, acc.: 67.97%] [G loss: 0.489441]\n",
      "epoch:9 step:8926 [D loss: 0.219029, acc.: 65.62%] [G loss: 0.444808]\n",
      "epoch:9 step:8927 [D loss: 0.246121, acc.: 60.94%] [G loss: 0.457404]\n",
      "epoch:9 step:8928 [D loss: 0.178335, acc.: 74.22%] [G loss: 0.535913]\n",
      "epoch:9 step:8929 [D loss: 0.221994, acc.: 64.06%] [G loss: 0.513908]\n",
      "epoch:9 step:8930 [D loss: 0.211029, acc.: 67.97%] [G loss: 0.493306]\n",
      "epoch:9 step:8931 [D loss: 0.216873, acc.: 67.97%] [G loss: 0.544872]\n",
      "epoch:9 step:8932 [D loss: 0.185760, acc.: 72.66%] [G loss: 0.550684]\n",
      "epoch:9 step:8933 [D loss: 0.272564, acc.: 52.34%] [G loss: 0.529975]\n",
      "epoch:9 step:8934 [D loss: 0.258457, acc.: 53.91%] [G loss: 0.410352]\n",
      "epoch:9 step:8935 [D loss: 0.228435, acc.: 63.28%] [G loss: 0.416045]\n",
      "epoch:9 step:8936 [D loss: 0.209437, acc.: 66.41%] [G loss: 0.441216]\n",
      "epoch:9 step:8937 [D loss: 0.206765, acc.: 71.09%] [G loss: 0.502205]\n",
      "epoch:9 step:8938 [D loss: 0.202939, acc.: 65.62%] [G loss: 0.464863]\n",
      "epoch:9 step:8939 [D loss: 0.226437, acc.: 64.06%] [G loss: 0.486403]\n",
      "epoch:9 step:8940 [D loss: 0.215024, acc.: 60.16%] [G loss: 0.519114]\n",
      "epoch:9 step:8941 [D loss: 0.185412, acc.: 75.78%] [G loss: 0.532250]\n",
      "epoch:9 step:8942 [D loss: 0.259382, acc.: 54.69%] [G loss: 0.483477]\n",
      "epoch:9 step:8943 [D loss: 0.211112, acc.: 66.41%] [G loss: 0.432655]\n",
      "epoch:9 step:8944 [D loss: 0.265613, acc.: 54.69%] [G loss: 0.432558]\n",
      "epoch:9 step:8945 [D loss: 0.226222, acc.: 59.38%] [G loss: 0.455278]\n",
      "epoch:9 step:8946 [D loss: 0.225475, acc.: 57.81%] [G loss: 0.479993]\n",
      "epoch:9 step:8947 [D loss: 0.196858, acc.: 70.31%] [G loss: 0.482461]\n",
      "epoch:9 step:8948 [D loss: 0.185650, acc.: 74.22%] [G loss: 0.520918]\n",
      "epoch:9 step:8949 [D loss: 0.212613, acc.: 62.50%] [G loss: 0.514158]\n",
      "epoch:9 step:8950 [D loss: 0.231655, acc.: 67.19%] [G loss: 0.488158]\n",
      "epoch:9 step:8951 [D loss: 0.238800, acc.: 57.03%] [G loss: 0.452156]\n",
      "epoch:9 step:8952 [D loss: 0.205710, acc.: 65.62%] [G loss: 0.472199]\n",
      "epoch:9 step:8953 [D loss: 0.224037, acc.: 63.28%] [G loss: 0.455941]\n",
      "epoch:9 step:8954 [D loss: 0.202365, acc.: 71.09%] [G loss: 0.462780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8955 [D loss: 0.198209, acc.: 68.75%] [G loss: 0.493003]\n",
      "epoch:9 step:8956 [D loss: 0.193378, acc.: 71.09%] [G loss: 0.518607]\n",
      "epoch:9 step:8957 [D loss: 0.246337, acc.: 59.38%] [G loss: 0.456454]\n",
      "epoch:9 step:8958 [D loss: 0.213804, acc.: 64.06%] [G loss: 0.451320]\n",
      "epoch:9 step:8959 [D loss: 0.196694, acc.: 71.88%] [G loss: 0.474131]\n",
      "epoch:9 step:8960 [D loss: 0.224206, acc.: 67.19%] [G loss: 0.453886]\n",
      "epoch:9 step:8961 [D loss: 0.268021, acc.: 53.12%] [G loss: 0.405011]\n",
      "epoch:9 step:8962 [D loss: 0.251098, acc.: 58.59%] [G loss: 0.455229]\n",
      "epoch:9 step:8963 [D loss: 0.217597, acc.: 65.62%] [G loss: 0.506714]\n",
      "epoch:9 step:8964 [D loss: 0.245848, acc.: 60.94%] [G loss: 0.421597]\n",
      "epoch:9 step:8965 [D loss: 0.210590, acc.: 64.84%] [G loss: 0.450297]\n",
      "epoch:9 step:8966 [D loss: 0.232253, acc.: 63.28%] [G loss: 0.507198]\n",
      "epoch:9 step:8967 [D loss: 0.183105, acc.: 68.75%] [G loss: 0.478937]\n",
      "epoch:9 step:8968 [D loss: 0.236156, acc.: 59.38%] [G loss: 0.453325]\n",
      "epoch:9 step:8969 [D loss: 0.199217, acc.: 71.09%] [G loss: 0.478761]\n",
      "epoch:9 step:8970 [D loss: 0.218509, acc.: 63.28%] [G loss: 0.466198]\n",
      "epoch:9 step:8971 [D loss: 0.237701, acc.: 64.06%] [G loss: 0.465102]\n",
      "epoch:9 step:8972 [D loss: 0.197127, acc.: 66.41%] [G loss: 0.486488]\n",
      "epoch:9 step:8973 [D loss: 0.247930, acc.: 60.94%] [G loss: 0.434108]\n",
      "epoch:9 step:8974 [D loss: 0.220911, acc.: 59.38%] [G loss: 0.464426]\n",
      "epoch:9 step:8975 [D loss: 0.252596, acc.: 59.38%] [G loss: 0.428180]\n",
      "epoch:9 step:8976 [D loss: 0.257682, acc.: 60.16%] [G loss: 0.405104]\n",
      "epoch:9 step:8977 [D loss: 0.253717, acc.: 55.47%] [G loss: 0.422730]\n",
      "epoch:9 step:8978 [D loss: 0.236089, acc.: 57.81%] [G loss: 0.445006]\n",
      "epoch:9 step:8979 [D loss: 0.218014, acc.: 67.19%] [G loss: 0.487205]\n",
      "epoch:9 step:8980 [D loss: 0.201252, acc.: 67.19%] [G loss: 0.502431]\n",
      "epoch:9 step:8981 [D loss: 0.197159, acc.: 67.19%] [G loss: 0.510047]\n",
      "epoch:9 step:8982 [D loss: 0.189067, acc.: 69.53%] [G loss: 0.523876]\n",
      "epoch:9 step:8983 [D loss: 0.195182, acc.: 68.75%] [G loss: 0.559089]\n",
      "epoch:9 step:8984 [D loss: 0.214009, acc.: 69.53%] [G loss: 0.561583]\n",
      "epoch:9 step:8985 [D loss: 0.213861, acc.: 64.06%] [G loss: 0.509396]\n",
      "epoch:9 step:8986 [D loss: 0.244024, acc.: 55.47%] [G loss: 0.420302]\n",
      "epoch:9 step:8987 [D loss: 0.185623, acc.: 71.88%] [G loss: 0.480194]\n",
      "epoch:9 step:8988 [D loss: 0.208407, acc.: 66.41%] [G loss: 0.501842]\n",
      "epoch:9 step:8989 [D loss: 0.212927, acc.: 66.41%] [G loss: 0.522012]\n",
      "epoch:9 step:8990 [D loss: 0.211828, acc.: 67.19%] [G loss: 0.498249]\n",
      "epoch:9 step:8991 [D loss: 0.213042, acc.: 67.97%] [G loss: 0.499622]\n",
      "epoch:9 step:8992 [D loss: 0.240053, acc.: 60.94%] [G loss: 0.465165]\n",
      "epoch:9 step:8993 [D loss: 0.245367, acc.: 58.59%] [G loss: 0.435468]\n",
      "epoch:9 step:8994 [D loss: 0.198802, acc.: 67.97%] [G loss: 0.499595]\n",
      "epoch:9 step:8995 [D loss: 0.234503, acc.: 60.94%] [G loss: 0.502869]\n",
      "epoch:9 step:8996 [D loss: 0.208463, acc.: 70.31%] [G loss: 0.484698]\n",
      "epoch:9 step:8997 [D loss: 0.186497, acc.: 71.09%] [G loss: 0.505369]\n",
      "epoch:9 step:8998 [D loss: 0.240526, acc.: 62.50%] [G loss: 0.489908]\n",
      "epoch:9 step:8999 [D loss: 0.249753, acc.: 53.91%] [G loss: 0.453276]\n",
      "epoch:9 step:9000 [D loss: 0.186070, acc.: 67.19%] [G loss: 0.510327]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.480484\n",
      "FID: 13.135968\n",
      "0 = 11.887787614393238\n",
      "1 = 0.04902912729991738\n",
      "2 = 0.9043999910354614\n",
      "3 = 0.8776000142097473\n",
      "4 = 0.9312000274658203\n",
      "5 = 0.9273034930229187\n",
      "6 = 0.8776000142097473\n",
      "7 = 6.783752229231597\n",
      "8 = 0.07877839560446552\n",
      "9 = 0.7200000286102295\n",
      "10 = 0.7214000225067139\n",
      "11 = 0.7185999751091003\n",
      "12 = 0.7193857431411743\n",
      "13 = 0.7214000225067139\n",
      "14 = 7.480549335479736\n",
      "15 = 9.591310501098633\n",
      "16 = 0.10357799381017685\n",
      "17 = 7.480484485626221\n",
      "18 = 13.135968208312988\n",
      "epoch:9 step:9001 [D loss: 0.195658, acc.: 74.22%] [G loss: 0.419437]\n",
      "epoch:9 step:9002 [D loss: 0.296521, acc.: 46.88%] [G loss: 0.386361]\n",
      "epoch:9 step:9003 [D loss: 0.207575, acc.: 67.97%] [G loss: 0.405195]\n",
      "epoch:9 step:9004 [D loss: 0.222549, acc.: 63.28%] [G loss: 0.409519]\n",
      "epoch:9 step:9005 [D loss: 0.206700, acc.: 71.09%] [G loss: 0.449616]\n",
      "epoch:9 step:9006 [D loss: 0.204818, acc.: 65.62%] [G loss: 0.466520]\n",
      "epoch:9 step:9007 [D loss: 0.174849, acc.: 73.44%] [G loss: 0.525407]\n",
      "epoch:9 step:9008 [D loss: 0.186994, acc.: 71.88%] [G loss: 0.511002]\n",
      "epoch:9 step:9009 [D loss: 0.250136, acc.: 57.81%] [G loss: 0.493474]\n",
      "epoch:9 step:9010 [D loss: 0.243687, acc.: 58.59%] [G loss: 0.435239]\n",
      "epoch:9 step:9011 [D loss: 0.213824, acc.: 67.97%] [G loss: 0.458041]\n",
      "epoch:9 step:9012 [D loss: 0.214427, acc.: 69.53%] [G loss: 0.471640]\n",
      "epoch:9 step:9013 [D loss: 0.234857, acc.: 58.59%] [G loss: 0.451682]\n",
      "epoch:9 step:9014 [D loss: 0.223439, acc.: 60.94%] [G loss: 0.475507]\n",
      "epoch:9 step:9015 [D loss: 0.205708, acc.: 69.53%] [G loss: 0.533541]\n",
      "epoch:9 step:9016 [D loss: 0.218635, acc.: 64.84%] [G loss: 0.512068]\n",
      "epoch:9 step:9017 [D loss: 0.230762, acc.: 59.38%] [G loss: 0.447758]\n",
      "epoch:9 step:9018 [D loss: 0.258336, acc.: 62.50%] [G loss: 0.466270]\n",
      "epoch:9 step:9019 [D loss: 0.239671, acc.: 64.06%] [G loss: 0.438377]\n",
      "epoch:9 step:9020 [D loss: 0.218233, acc.: 62.50%] [G loss: 0.444256]\n",
      "epoch:9 step:9021 [D loss: 0.217984, acc.: 59.38%] [G loss: 0.455821]\n",
      "epoch:9 step:9022 [D loss: 0.205773, acc.: 71.09%] [G loss: 0.475654]\n",
      "epoch:9 step:9023 [D loss: 0.234676, acc.: 62.50%] [G loss: 0.490623]\n",
      "epoch:9 step:9024 [D loss: 0.231700, acc.: 61.72%] [G loss: 0.437844]\n",
      "epoch:9 step:9025 [D loss: 0.184442, acc.: 69.53%] [G loss: 0.473265]\n",
      "epoch:9 step:9026 [D loss: 0.236737, acc.: 62.50%] [G loss: 0.457413]\n",
      "epoch:9 step:9027 [D loss: 0.226848, acc.: 65.62%] [G loss: 0.482174]\n",
      "epoch:9 step:9028 [D loss: 0.237328, acc.: 67.19%] [G loss: 0.480020]\n",
      "epoch:9 step:9029 [D loss: 0.224481, acc.: 62.50%] [G loss: 0.487456]\n",
      "epoch:9 step:9030 [D loss: 0.231159, acc.: 60.16%] [G loss: 0.458955]\n",
      "epoch:9 step:9031 [D loss: 0.195542, acc.: 71.88%] [G loss: 0.491559]\n",
      "epoch:9 step:9032 [D loss: 0.248238, acc.: 57.81%] [G loss: 0.432112]\n",
      "epoch:9 step:9033 [D loss: 0.231169, acc.: 59.38%] [G loss: 0.442261]\n",
      "epoch:9 step:9034 [D loss: 0.240127, acc.: 57.81%] [G loss: 0.450550]\n",
      "epoch:9 step:9035 [D loss: 0.239447, acc.: 60.16%] [G loss: 0.460299]\n",
      "epoch:9 step:9036 [D loss: 0.208772, acc.: 68.75%] [G loss: 0.489744]\n",
      "epoch:9 step:9037 [D loss: 0.213161, acc.: 67.19%] [G loss: 0.434717]\n",
      "epoch:9 step:9038 [D loss: 0.227343, acc.: 61.72%] [G loss: 0.434356]\n",
      "epoch:9 step:9039 [D loss: 0.199928, acc.: 69.53%] [G loss: 0.515364]\n",
      "epoch:9 step:9040 [D loss: 0.200712, acc.: 67.19%] [G loss: 0.477712]\n",
      "epoch:9 step:9041 [D loss: 0.224130, acc.: 65.62%] [G loss: 0.445494]\n",
      "epoch:9 step:9042 [D loss: 0.198761, acc.: 69.53%] [G loss: 0.473016]\n",
      "epoch:9 step:9043 [D loss: 0.214390, acc.: 62.50%] [G loss: 0.490241]\n",
      "epoch:9 step:9044 [D loss: 0.210822, acc.: 66.41%] [G loss: 0.456520]\n",
      "epoch:9 step:9045 [D loss: 0.212422, acc.: 67.97%] [G loss: 0.469246]\n",
      "epoch:9 step:9046 [D loss: 0.191154, acc.: 67.19%] [G loss: 0.473361]\n",
      "epoch:9 step:9047 [D loss: 0.265181, acc.: 50.78%] [G loss: 0.427627]\n",
      "epoch:9 step:9048 [D loss: 0.250316, acc.: 56.25%] [G loss: 0.471907]\n",
      "epoch:9 step:9049 [D loss: 0.242591, acc.: 60.16%] [G loss: 0.456050]\n",
      "epoch:9 step:9050 [D loss: 0.209244, acc.: 66.41%] [G loss: 0.468152]\n",
      "epoch:9 step:9051 [D loss: 0.217119, acc.: 67.19%] [G loss: 0.456354]\n",
      "epoch:9 step:9052 [D loss: 0.216278, acc.: 67.97%] [G loss: 0.448759]\n",
      "epoch:9 step:9053 [D loss: 0.238317, acc.: 56.25%] [G loss: 0.471535]\n",
      "epoch:9 step:9054 [D loss: 0.225482, acc.: 64.06%] [G loss: 0.512441]\n",
      "epoch:9 step:9055 [D loss: 0.234619, acc.: 59.38%] [G loss: 0.486526]\n",
      "epoch:9 step:9056 [D loss: 0.212498, acc.: 71.09%] [G loss: 0.454517]\n",
      "epoch:9 step:9057 [D loss: 0.204808, acc.: 65.62%] [G loss: 0.464921]\n",
      "epoch:9 step:9058 [D loss: 0.250384, acc.: 60.16%] [G loss: 0.456559]\n",
      "epoch:9 step:9059 [D loss: 0.234222, acc.: 64.06%] [G loss: 0.459377]\n",
      "epoch:9 step:9060 [D loss: 0.224680, acc.: 67.19%] [G loss: 0.480513]\n",
      "epoch:9 step:9061 [D loss: 0.241091, acc.: 58.59%] [G loss: 0.483100]\n",
      "epoch:9 step:9062 [D loss: 0.193162, acc.: 74.22%] [G loss: 0.486641]\n",
      "epoch:9 step:9063 [D loss: 0.239410, acc.: 61.72%] [G loss: 0.472666]\n",
      "epoch:9 step:9064 [D loss: 0.204158, acc.: 70.31%] [G loss: 0.487167]\n",
      "epoch:9 step:9065 [D loss: 0.234856, acc.: 62.50%] [G loss: 0.466686]\n",
      "epoch:9 step:9066 [D loss: 0.201407, acc.: 64.84%] [G loss: 0.487340]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9067 [D loss: 0.187774, acc.: 72.66%] [G loss: 0.513985]\n",
      "epoch:9 step:9068 [D loss: 0.205775, acc.: 67.97%] [G loss: 0.495924]\n",
      "epoch:9 step:9069 [D loss: 0.248488, acc.: 54.69%] [G loss: 0.463035]\n",
      "epoch:9 step:9070 [D loss: 0.219669, acc.: 63.28%] [G loss: 0.428597]\n",
      "epoch:9 step:9071 [D loss: 0.223717, acc.: 65.62%] [G loss: 0.452185]\n",
      "epoch:9 step:9072 [D loss: 0.221903, acc.: 63.28%] [G loss: 0.450069]\n",
      "epoch:9 step:9073 [D loss: 0.216330, acc.: 62.50%] [G loss: 0.478681]\n",
      "epoch:9 step:9074 [D loss: 0.183031, acc.: 71.88%] [G loss: 0.505434]\n",
      "epoch:9 step:9075 [D loss: 0.182351, acc.: 75.78%] [G loss: 0.538814]\n",
      "epoch:9 step:9076 [D loss: 0.199986, acc.: 66.41%] [G loss: 0.531849]\n",
      "epoch:9 step:9077 [D loss: 0.244477, acc.: 59.38%] [G loss: 0.484380]\n",
      "epoch:9 step:9078 [D loss: 0.245939, acc.: 59.38%] [G loss: 0.454765]\n",
      "epoch:9 step:9079 [D loss: 0.232165, acc.: 64.06%] [G loss: 0.488824]\n",
      "epoch:9 step:9080 [D loss: 0.200242, acc.: 68.75%] [G loss: 0.515165]\n",
      "epoch:9 step:9081 [D loss: 0.195817, acc.: 71.88%] [G loss: 0.565493]\n",
      "epoch:9 step:9082 [D loss: 0.195344, acc.: 71.09%] [G loss: 0.555970]\n",
      "epoch:9 step:9083 [D loss: 0.211072, acc.: 67.19%] [G loss: 0.475478]\n",
      "epoch:9 step:9084 [D loss: 0.211753, acc.: 65.62%] [G loss: 0.513322]\n",
      "epoch:9 step:9085 [D loss: 0.257395, acc.: 58.59%] [G loss: 0.448364]\n",
      "epoch:9 step:9086 [D loss: 0.224623, acc.: 62.50%] [G loss: 0.531336]\n",
      "epoch:9 step:9087 [D loss: 0.206795, acc.: 68.75%] [G loss: 0.467326]\n",
      "epoch:9 step:9088 [D loss: 0.265390, acc.: 56.25%] [G loss: 0.468283]\n",
      "epoch:9 step:9089 [D loss: 0.219254, acc.: 63.28%] [G loss: 0.458894]\n",
      "epoch:9 step:9090 [D loss: 0.216372, acc.: 67.19%] [G loss: 0.468285]\n",
      "epoch:9 step:9091 [D loss: 0.248476, acc.: 57.03%] [G loss: 0.432504]\n",
      "epoch:9 step:9092 [D loss: 0.218828, acc.: 67.97%] [G loss: 0.462952]\n",
      "epoch:9 step:9093 [D loss: 0.199969, acc.: 70.31%] [G loss: 0.471429]\n",
      "epoch:9 step:9094 [D loss: 0.192477, acc.: 67.97%] [G loss: 0.452144]\n",
      "epoch:9 step:9095 [D loss: 0.226926, acc.: 64.06%] [G loss: 0.478295]\n",
      "epoch:9 step:9096 [D loss: 0.212745, acc.: 60.16%] [G loss: 0.506287]\n",
      "epoch:9 step:9097 [D loss: 0.223664, acc.: 67.97%] [G loss: 0.510531]\n",
      "epoch:9 step:9098 [D loss: 0.233600, acc.: 61.72%] [G loss: 0.460933]\n",
      "epoch:9 step:9099 [D loss: 0.209455, acc.: 67.19%] [G loss: 0.466064]\n",
      "epoch:9 step:9100 [D loss: 0.223224, acc.: 67.97%] [G loss: 0.478438]\n",
      "epoch:9 step:9101 [D loss: 0.244422, acc.: 60.94%] [G loss: 0.457519]\n",
      "epoch:9 step:9102 [D loss: 0.226824, acc.: 61.72%] [G loss: 0.459290]\n",
      "epoch:9 step:9103 [D loss: 0.252369, acc.: 55.47%] [G loss: 0.445194]\n",
      "epoch:9 step:9104 [D loss: 0.243243, acc.: 60.94%] [G loss: 0.456547]\n",
      "epoch:9 step:9105 [D loss: 0.218984, acc.: 67.97%] [G loss: 0.423165]\n",
      "epoch:9 step:9106 [D loss: 0.244195, acc.: 60.94%] [G loss: 0.462079]\n",
      "epoch:9 step:9107 [D loss: 0.199749, acc.: 72.66%] [G loss: 0.494167]\n",
      "epoch:9 step:9108 [D loss: 0.220918, acc.: 69.53%] [G loss: 0.451967]\n",
      "epoch:9 step:9109 [D loss: 0.215949, acc.: 66.41%] [G loss: 0.489970]\n",
      "epoch:9 step:9110 [D loss: 0.196186, acc.: 70.31%] [G loss: 0.504370]\n",
      "epoch:9 step:9111 [D loss: 0.235379, acc.: 60.94%] [G loss: 0.446271]\n",
      "epoch:9 step:9112 [D loss: 0.204476, acc.: 63.28%] [G loss: 0.463608]\n",
      "epoch:9 step:9113 [D loss: 0.240400, acc.: 63.28%] [G loss: 0.396032]\n",
      "epoch:9 step:9114 [D loss: 0.211573, acc.: 64.84%] [G loss: 0.462532]\n",
      "epoch:9 step:9115 [D loss: 0.223646, acc.: 57.81%] [G loss: 0.430576]\n",
      "epoch:9 step:9116 [D loss: 0.214448, acc.: 65.62%] [G loss: 0.461323]\n",
      "epoch:9 step:9117 [D loss: 0.231765, acc.: 63.28%] [G loss: 0.436089]\n",
      "epoch:9 step:9118 [D loss: 0.206239, acc.: 64.06%] [G loss: 0.467621]\n",
      "epoch:9 step:9119 [D loss: 0.231054, acc.: 62.50%] [G loss: 0.415717]\n",
      "epoch:9 step:9120 [D loss: 0.200163, acc.: 65.62%] [G loss: 0.490106]\n",
      "epoch:9 step:9121 [D loss: 0.237994, acc.: 58.59%] [G loss: 0.453371]\n",
      "epoch:9 step:9122 [D loss: 0.225898, acc.: 67.19%] [G loss: 0.474096]\n",
      "epoch:9 step:9123 [D loss: 0.227158, acc.: 64.84%] [G loss: 0.514326]\n",
      "epoch:9 step:9124 [D loss: 0.190612, acc.: 73.44%] [G loss: 0.581857]\n",
      "epoch:9 step:9125 [D loss: 0.186164, acc.: 76.56%] [G loss: 0.485697]\n",
      "epoch:9 step:9126 [D loss: 0.231636, acc.: 61.72%] [G loss: 0.490846]\n",
      "epoch:9 step:9127 [D loss: 0.186540, acc.: 72.66%] [G loss: 0.533431]\n",
      "epoch:9 step:9128 [D loss: 0.219786, acc.: 68.75%] [G loss: 0.496073]\n",
      "epoch:9 step:9129 [D loss: 0.214661, acc.: 63.28%] [G loss: 0.526563]\n",
      "epoch:9 step:9130 [D loss: 0.211891, acc.: 63.28%] [G loss: 0.474321]\n",
      "epoch:9 step:9131 [D loss: 0.204344, acc.: 67.97%] [G loss: 0.516410]\n",
      "epoch:9 step:9132 [D loss: 0.209878, acc.: 68.75%] [G loss: 0.530407]\n",
      "epoch:9 step:9133 [D loss: 0.208359, acc.: 67.97%] [G loss: 0.509125]\n",
      "epoch:9 step:9134 [D loss: 0.231134, acc.: 59.38%] [G loss: 0.483470]\n",
      "epoch:9 step:9135 [D loss: 0.255835, acc.: 57.03%] [G loss: 0.450833]\n",
      "epoch:9 step:9136 [D loss: 0.264456, acc.: 53.91%] [G loss: 0.447366]\n",
      "epoch:9 step:9137 [D loss: 0.215662, acc.: 65.62%] [G loss: 0.503275]\n",
      "epoch:9 step:9138 [D loss: 0.230095, acc.: 57.81%] [G loss: 0.430933]\n",
      "epoch:9 step:9139 [D loss: 0.225590, acc.: 63.28%] [G loss: 0.466639]\n",
      "epoch:9 step:9140 [D loss: 0.205041, acc.: 68.75%] [G loss: 0.535976]\n",
      "epoch:9 step:9141 [D loss: 0.189862, acc.: 76.56%] [G loss: 0.542891]\n",
      "epoch:9 step:9142 [D loss: 0.211783, acc.: 67.97%] [G loss: 0.491971]\n",
      "epoch:9 step:9143 [D loss: 0.245768, acc.: 60.16%] [G loss: 0.450943]\n",
      "epoch:9 step:9144 [D loss: 0.221371, acc.: 62.50%] [G loss: 0.470404]\n",
      "epoch:9 step:9145 [D loss: 0.238696, acc.: 55.47%] [G loss: 0.444165]\n",
      "epoch:9 step:9146 [D loss: 0.222654, acc.: 67.19%] [G loss: 0.483727]\n",
      "epoch:9 step:9147 [D loss: 0.228773, acc.: 62.50%] [G loss: 0.456898]\n",
      "epoch:9 step:9148 [D loss: 0.205963, acc.: 64.84%] [G loss: 0.493022]\n",
      "epoch:9 step:9149 [D loss: 0.253485, acc.: 58.59%] [G loss: 0.466052]\n",
      "epoch:9 step:9150 [D loss: 0.214861, acc.: 62.50%] [G loss: 0.491278]\n",
      "epoch:9 step:9151 [D loss: 0.233097, acc.: 60.94%] [G loss: 0.463045]\n",
      "epoch:9 step:9152 [D loss: 0.220798, acc.: 66.41%] [G loss: 0.487244]\n",
      "epoch:9 step:9153 [D loss: 0.226095, acc.: 64.84%] [G loss: 0.504893]\n",
      "epoch:9 step:9154 [D loss: 0.235696, acc.: 60.16%] [G loss: 0.474394]\n",
      "epoch:9 step:9155 [D loss: 0.211615, acc.: 67.19%] [G loss: 0.454018]\n",
      "epoch:9 step:9156 [D loss: 0.210703, acc.: 68.75%] [G loss: 0.447849]\n",
      "epoch:9 step:9157 [D loss: 0.205915, acc.: 65.62%] [G loss: 0.467806]\n",
      "epoch:9 step:9158 [D loss: 0.200565, acc.: 71.88%] [G loss: 0.466394]\n",
      "epoch:9 step:9159 [D loss: 0.216921, acc.: 67.97%] [G loss: 0.489583]\n",
      "epoch:9 step:9160 [D loss: 0.235161, acc.: 63.28%] [G loss: 0.473776]\n",
      "epoch:9 step:9161 [D loss: 0.229202, acc.: 62.50%] [G loss: 0.484971]\n",
      "epoch:9 step:9162 [D loss: 0.224303, acc.: 67.19%] [G loss: 0.479486]\n",
      "epoch:9 step:9163 [D loss: 0.176310, acc.: 75.00%] [G loss: 0.508653]\n",
      "epoch:9 step:9164 [D loss: 0.197351, acc.: 67.97%] [G loss: 0.504205]\n",
      "epoch:9 step:9165 [D loss: 0.216225, acc.: 58.59%] [G loss: 0.484211]\n",
      "epoch:9 step:9166 [D loss: 0.215832, acc.: 67.97%] [G loss: 0.478927]\n",
      "epoch:9 step:9167 [D loss: 0.235910, acc.: 60.16%] [G loss: 0.485251]\n",
      "epoch:9 step:9168 [D loss: 0.233516, acc.: 60.94%] [G loss: 0.452754]\n",
      "epoch:9 step:9169 [D loss: 0.208693, acc.: 67.19%] [G loss: 0.505072]\n",
      "epoch:9 step:9170 [D loss: 0.187449, acc.: 69.53%] [G loss: 0.484614]\n",
      "epoch:9 step:9171 [D loss: 0.253592, acc.: 57.03%] [G loss: 0.427415]\n",
      "epoch:9 step:9172 [D loss: 0.262479, acc.: 53.12%] [G loss: 0.426685]\n",
      "epoch:9 step:9173 [D loss: 0.220333, acc.: 64.06%] [G loss: 0.457105]\n",
      "epoch:9 step:9174 [D loss: 0.249894, acc.: 58.59%] [G loss: 0.425347]\n",
      "epoch:9 step:9175 [D loss: 0.223925, acc.: 62.50%] [G loss: 0.472577]\n",
      "epoch:9 step:9176 [D loss: 0.199904, acc.: 67.97%] [G loss: 0.515026]\n",
      "epoch:9 step:9177 [D loss: 0.227842, acc.: 59.38%] [G loss: 0.481766]\n",
      "epoch:9 step:9178 [D loss: 0.217823, acc.: 57.81%] [G loss: 0.470998]\n",
      "epoch:9 step:9179 [D loss: 0.210293, acc.: 72.66%] [G loss: 0.458847]\n",
      "epoch:9 step:9180 [D loss: 0.182747, acc.: 71.88%] [G loss: 0.544169]\n",
      "epoch:9 step:9181 [D loss: 0.236156, acc.: 63.28%] [G loss: 0.441733]\n",
      "epoch:9 step:9182 [D loss: 0.232686, acc.: 57.03%] [G loss: 0.463538]\n",
      "epoch:9 step:9183 [D loss: 0.215407, acc.: 66.41%] [G loss: 0.447355]\n",
      "epoch:9 step:9184 [D loss: 0.227980, acc.: 60.16%] [G loss: 0.433658]\n",
      "epoch:9 step:9185 [D loss: 0.225503, acc.: 60.16%] [G loss: 0.472647]\n",
      "epoch:9 step:9186 [D loss: 0.208995, acc.: 66.41%] [G loss: 0.507492]\n",
      "epoch:9 step:9187 [D loss: 0.200384, acc.: 67.19%] [G loss: 0.537671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9188 [D loss: 0.218953, acc.: 67.97%] [G loss: 0.452052]\n",
      "epoch:9 step:9189 [D loss: 0.207832, acc.: 64.84%] [G loss: 0.480908]\n",
      "epoch:9 step:9190 [D loss: 0.223314, acc.: 67.97%] [G loss: 0.435457]\n",
      "epoch:9 step:9191 [D loss: 0.255548, acc.: 60.16%] [G loss: 0.481107]\n",
      "epoch:9 step:9192 [D loss: 0.228289, acc.: 63.28%] [G loss: 0.456923]\n",
      "epoch:9 step:9193 [D loss: 0.244577, acc.: 58.59%] [G loss: 0.468068]\n",
      "epoch:9 step:9194 [D loss: 0.231341, acc.: 63.28%] [G loss: 0.452699]\n",
      "epoch:9 step:9195 [D loss: 0.224427, acc.: 61.72%] [G loss: 0.449835]\n",
      "epoch:9 step:9196 [D loss: 0.232094, acc.: 57.03%] [G loss: 0.505002]\n",
      "epoch:9 step:9197 [D loss: 0.221612, acc.: 60.94%] [G loss: 0.475780]\n",
      "epoch:9 step:9198 [D loss: 0.256947, acc.: 59.38%] [G loss: 0.425084]\n",
      "epoch:9 step:9199 [D loss: 0.212656, acc.: 70.31%] [G loss: 0.471011]\n",
      "epoch:9 step:9200 [D loss: 0.239353, acc.: 60.16%] [G loss: 0.452551]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.526451\n",
      "FID: 15.564986\n",
      "0 = 12.017000098037732\n",
      "1 = 0.05562121431991462\n",
      "2 = 0.9090999960899353\n",
      "3 = 0.8823999762535095\n",
      "4 = 0.9358000159263611\n",
      "5 = 0.9321783185005188\n",
      "6 = 0.8823999762535095\n",
      "7 = 6.963297539287814\n",
      "8 = 0.08676580578609835\n",
      "9 = 0.7305499911308289\n",
      "10 = 0.7271999716758728\n",
      "11 = 0.7339000105857849\n",
      "12 = 0.7321050763130188\n",
      "13 = 0.7271999716758728\n",
      "14 = 7.5265116691589355\n",
      "15 = 9.414833068847656\n",
      "16 = 0.144761323928833\n",
      "17 = 7.5264506340026855\n",
      "18 = 15.564986228942871\n",
      "epoch:9 step:9201 [D loss: 0.259998, acc.: 55.47%] [G loss: 0.450012]\n",
      "epoch:9 step:9202 [D loss: 0.196025, acc.: 73.44%] [G loss: 0.517186]\n",
      "epoch:9 step:9203 [D loss: 0.229839, acc.: 57.81%] [G loss: 0.486087]\n",
      "epoch:9 step:9204 [D loss: 0.230786, acc.: 64.06%] [G loss: 0.491702]\n",
      "epoch:9 step:9205 [D loss: 0.232990, acc.: 55.47%] [G loss: 0.463760]\n",
      "epoch:9 step:9206 [D loss: 0.201183, acc.: 71.09%] [G loss: 0.490067]\n",
      "epoch:9 step:9207 [D loss: 0.236006, acc.: 61.72%] [G loss: 0.447278]\n",
      "epoch:9 step:9208 [D loss: 0.214510, acc.: 62.50%] [G loss: 0.506202]\n",
      "epoch:9 step:9209 [D loss: 0.222681, acc.: 66.41%] [G loss: 0.518318]\n",
      "epoch:9 step:9210 [D loss: 0.220174, acc.: 62.50%] [G loss: 0.460099]\n",
      "epoch:9 step:9211 [D loss: 0.226128, acc.: 67.19%] [G loss: 0.472879]\n",
      "epoch:9 step:9212 [D loss: 0.230158, acc.: 67.19%] [G loss: 0.465124]\n",
      "epoch:9 step:9213 [D loss: 0.198347, acc.: 66.41%] [G loss: 0.449114]\n",
      "epoch:9 step:9214 [D loss: 0.199120, acc.: 71.09%] [G loss: 0.450600]\n",
      "epoch:9 step:9215 [D loss: 0.183673, acc.: 73.44%] [G loss: 0.548288]\n",
      "epoch:9 step:9216 [D loss: 0.228483, acc.: 60.16%] [G loss: 0.479000]\n",
      "epoch:9 step:9217 [D loss: 0.268308, acc.: 55.47%] [G loss: 0.427405]\n",
      "epoch:9 step:9218 [D loss: 0.221956, acc.: 64.06%] [G loss: 0.444562]\n",
      "epoch:9 step:9219 [D loss: 0.174318, acc.: 78.91%] [G loss: 0.479560]\n",
      "epoch:9 step:9220 [D loss: 0.258710, acc.: 55.47%] [G loss: 0.448944]\n",
      "epoch:9 step:9221 [D loss: 0.246168, acc.: 58.59%] [G loss: 0.473693]\n",
      "epoch:9 step:9222 [D loss: 0.229507, acc.: 63.28%] [G loss: 0.444041]\n",
      "epoch:9 step:9223 [D loss: 0.214465, acc.: 68.75%] [G loss: 0.430832]\n",
      "epoch:9 step:9224 [D loss: 0.238523, acc.: 55.47%] [G loss: 0.466672]\n",
      "epoch:9 step:9225 [D loss: 0.220608, acc.: 65.62%] [G loss: 0.512553]\n",
      "epoch:9 step:9226 [D loss: 0.204332, acc.: 65.62%] [G loss: 0.521619]\n",
      "epoch:9 step:9227 [D loss: 0.243844, acc.: 57.03%] [G loss: 0.461022]\n",
      "epoch:9 step:9228 [D loss: 0.240520, acc.: 56.25%] [G loss: 0.463668]\n",
      "epoch:9 step:9229 [D loss: 0.200343, acc.: 64.84%] [G loss: 0.497698]\n",
      "epoch:9 step:9230 [D loss: 0.226552, acc.: 62.50%] [G loss: 0.453487]\n",
      "epoch:9 step:9231 [D loss: 0.208934, acc.: 68.75%] [G loss: 0.494497]\n",
      "epoch:9 step:9232 [D loss: 0.238567, acc.: 64.84%] [G loss: 0.433098]\n",
      "epoch:9 step:9233 [D loss: 0.212338, acc.: 64.84%] [G loss: 0.473187]\n",
      "epoch:9 step:9234 [D loss: 0.188701, acc.: 69.53%] [G loss: 0.486603]\n",
      "epoch:9 step:9235 [D loss: 0.213035, acc.: 66.41%] [G loss: 0.512329]\n",
      "epoch:9 step:9236 [D loss: 0.213860, acc.: 68.75%] [G loss: 0.549879]\n",
      "epoch:9 step:9237 [D loss: 0.225015, acc.: 60.94%] [G loss: 0.458767]\n",
      "epoch:9 step:9238 [D loss: 0.209172, acc.: 64.06%] [G loss: 0.463951]\n",
      "epoch:9 step:9239 [D loss: 0.243740, acc.: 60.94%] [G loss: 0.459878]\n",
      "epoch:9 step:9240 [D loss: 0.205520, acc.: 69.53%] [G loss: 0.484733]\n",
      "epoch:9 step:9241 [D loss: 0.252663, acc.: 58.59%] [G loss: 0.453114]\n",
      "epoch:9 step:9242 [D loss: 0.254583, acc.: 56.25%] [G loss: 0.443329]\n",
      "epoch:9 step:9243 [D loss: 0.243863, acc.: 57.81%] [G loss: 0.407671]\n",
      "epoch:9 step:9244 [D loss: 0.210027, acc.: 65.62%] [G loss: 0.504657]\n",
      "epoch:9 step:9245 [D loss: 0.244377, acc.: 58.59%] [G loss: 0.443611]\n",
      "epoch:9 step:9246 [D loss: 0.251724, acc.: 57.03%] [G loss: 0.408227]\n",
      "epoch:9 step:9247 [D loss: 0.214566, acc.: 67.19%] [G loss: 0.494320]\n",
      "epoch:9 step:9248 [D loss: 0.196441, acc.: 72.66%] [G loss: 0.473557]\n",
      "epoch:9 step:9249 [D loss: 0.256274, acc.: 57.03%] [G loss: 0.457594]\n",
      "epoch:9 step:9250 [D loss: 0.247337, acc.: 57.03%] [G loss: 0.512661]\n",
      "epoch:9 step:9251 [D loss: 0.254879, acc.: 56.25%] [G loss: 0.444774]\n",
      "epoch:9 step:9252 [D loss: 0.221776, acc.: 65.62%] [G loss: 0.479863]\n",
      "epoch:9 step:9253 [D loss: 0.253393, acc.: 52.34%] [G loss: 0.477947]\n",
      "epoch:9 step:9254 [D loss: 0.188614, acc.: 69.53%] [G loss: 0.501293]\n",
      "epoch:9 step:9255 [D loss: 0.225119, acc.: 57.81%] [G loss: 0.448150]\n",
      "epoch:9 step:9256 [D loss: 0.181583, acc.: 75.78%] [G loss: 0.468292]\n",
      "epoch:9 step:9257 [D loss: 0.254857, acc.: 55.47%] [G loss: 0.412253]\n",
      "epoch:9 step:9258 [D loss: 0.226218, acc.: 60.16%] [G loss: 0.443722]\n",
      "epoch:9 step:9259 [D loss: 0.221681, acc.: 61.72%] [G loss: 0.456381]\n",
      "epoch:9 step:9260 [D loss: 0.237947, acc.: 60.94%] [G loss: 0.417998]\n",
      "epoch:9 step:9261 [D loss: 0.253457, acc.: 58.59%] [G loss: 0.420757]\n",
      "epoch:9 step:9262 [D loss: 0.228410, acc.: 60.94%] [G loss: 0.416608]\n",
      "epoch:9 step:9263 [D loss: 0.242093, acc.: 59.38%] [G loss: 0.459736]\n",
      "epoch:9 step:9264 [D loss: 0.219022, acc.: 67.19%] [G loss: 0.443237]\n",
      "epoch:9 step:9265 [D loss: 0.212074, acc.: 65.62%] [G loss: 0.481690]\n",
      "epoch:9 step:9266 [D loss: 0.208725, acc.: 72.66%] [G loss: 0.475398]\n",
      "epoch:9 step:9267 [D loss: 0.238326, acc.: 57.81%] [G loss: 0.408845]\n",
      "epoch:9 step:9268 [D loss: 0.206707, acc.: 67.97%] [G loss: 0.487605]\n",
      "epoch:9 step:9269 [D loss: 0.203776, acc.: 72.66%] [G loss: 0.480552]\n",
      "epoch:9 step:9270 [D loss: 0.189169, acc.: 71.88%] [G loss: 0.490506]\n",
      "epoch:9 step:9271 [D loss: 0.203885, acc.: 64.84%] [G loss: 0.480504]\n",
      "epoch:9 step:9272 [D loss: 0.188109, acc.: 73.44%] [G loss: 0.466077]\n",
      "epoch:9 step:9273 [D loss: 0.243087, acc.: 60.16%] [G loss: 0.428591]\n",
      "epoch:9 step:9274 [D loss: 0.217522, acc.: 60.94%] [G loss: 0.446700]\n",
      "epoch:9 step:9275 [D loss: 0.204041, acc.: 63.28%] [G loss: 0.434455]\n",
      "epoch:9 step:9276 [D loss: 0.219793, acc.: 65.62%] [G loss: 0.515210]\n",
      "epoch:9 step:9277 [D loss: 0.216409, acc.: 69.53%] [G loss: 0.488848]\n",
      "epoch:9 step:9278 [D loss: 0.213129, acc.: 64.84%] [G loss: 0.493597]\n",
      "epoch:9 step:9279 [D loss: 0.238547, acc.: 55.47%] [G loss: 0.472185]\n",
      "epoch:9 step:9280 [D loss: 0.231560, acc.: 61.72%] [G loss: 0.435523]\n",
      "epoch:9 step:9281 [D loss: 0.207720, acc.: 64.06%] [G loss: 0.455236]\n",
      "epoch:9 step:9282 [D loss: 0.211322, acc.: 66.41%] [G loss: 0.429543]\n",
      "epoch:9 step:9283 [D loss: 0.219533, acc.: 61.72%] [G loss: 0.442456]\n",
      "epoch:9 step:9284 [D loss: 0.239312, acc.: 57.03%] [G loss: 0.504916]\n",
      "epoch:9 step:9285 [D loss: 0.215926, acc.: 63.28%] [G loss: 0.480416]\n",
      "epoch:9 step:9286 [D loss: 0.213356, acc.: 66.41%] [G loss: 0.473292]\n",
      "epoch:9 step:9287 [D loss: 0.199199, acc.: 69.53%] [G loss: 0.474263]\n",
      "epoch:9 step:9288 [D loss: 0.222178, acc.: 61.72%] [G loss: 0.496345]\n",
      "epoch:9 step:9289 [D loss: 0.275552, acc.: 53.91%] [G loss: 0.441317]\n",
      "epoch:9 step:9290 [D loss: 0.222020, acc.: 65.62%] [G loss: 0.463973]\n",
      "epoch:9 step:9291 [D loss: 0.242841, acc.: 60.94%] [G loss: 0.449506]\n",
      "epoch:9 step:9292 [D loss: 0.261261, acc.: 61.72%] [G loss: 0.416273]\n",
      "epoch:9 step:9293 [D loss: 0.180868, acc.: 76.56%] [G loss: 0.463845]\n",
      "epoch:9 step:9294 [D loss: 0.214410, acc.: 66.41%] [G loss: 0.438209]\n",
      "epoch:9 step:9295 [D loss: 0.219884, acc.: 67.97%] [G loss: 0.453489]\n",
      "epoch:9 step:9296 [D loss: 0.205959, acc.: 72.66%] [G loss: 0.437364]\n",
      "epoch:9 step:9297 [D loss: 0.247654, acc.: 57.81%] [G loss: 0.460828]\n",
      "epoch:9 step:9298 [D loss: 0.246022, acc.: 60.16%] [G loss: 0.415213]\n",
      "epoch:9 step:9299 [D loss: 0.201701, acc.: 68.75%] [G loss: 0.463943]\n",
      "epoch:9 step:9300 [D loss: 0.266649, acc.: 49.22%] [G loss: 0.428885]\n",
      "epoch:9 step:9301 [D loss: 0.228550, acc.: 60.16%] [G loss: 0.438139]\n",
      "epoch:9 step:9302 [D loss: 0.242163, acc.: 57.81%] [G loss: 0.423480]\n",
      "epoch:9 step:9303 [D loss: 0.214023, acc.: 65.62%] [G loss: 0.481966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9304 [D loss: 0.208903, acc.: 67.19%] [G loss: 0.468537]\n",
      "epoch:9 step:9305 [D loss: 0.229172, acc.: 55.47%] [G loss: 0.462404]\n",
      "epoch:9 step:9306 [D loss: 0.228828, acc.: 63.28%] [G loss: 0.461235]\n",
      "epoch:9 step:9307 [D loss: 0.223461, acc.: 63.28%] [G loss: 0.441487]\n",
      "epoch:9 step:9308 [D loss: 0.182055, acc.: 75.78%] [G loss: 0.496442]\n",
      "epoch:9 step:9309 [D loss: 0.239229, acc.: 62.50%] [G loss: 0.447995]\n",
      "epoch:9 step:9310 [D loss: 0.228504, acc.: 62.50%] [G loss: 0.461291]\n",
      "epoch:9 step:9311 [D loss: 0.221945, acc.: 59.38%] [G loss: 0.436964]\n",
      "epoch:9 step:9312 [D loss: 0.222666, acc.: 64.06%] [G loss: 0.466336]\n",
      "epoch:9 step:9313 [D loss: 0.231858, acc.: 64.06%] [G loss: 0.432557]\n",
      "epoch:9 step:9314 [D loss: 0.243821, acc.: 58.59%] [G loss: 0.405199]\n",
      "epoch:9 step:9315 [D loss: 0.214362, acc.: 63.28%] [G loss: 0.487687]\n",
      "epoch:9 step:9316 [D loss: 0.236383, acc.: 63.28%] [G loss: 0.466609]\n",
      "epoch:9 step:9317 [D loss: 0.211497, acc.: 63.28%] [G loss: 0.489237]\n",
      "epoch:9 step:9318 [D loss: 0.216995, acc.: 66.41%] [G loss: 0.474602]\n",
      "epoch:9 step:9319 [D loss: 0.205034, acc.: 69.53%] [G loss: 0.491505]\n",
      "epoch:9 step:9320 [D loss: 0.217660, acc.: 64.06%] [G loss: 0.507851]\n",
      "epoch:9 step:9321 [D loss: 0.210871, acc.: 66.41%] [G loss: 0.509631]\n",
      "epoch:9 step:9322 [D loss: 0.224930, acc.: 61.72%] [G loss: 0.455885]\n",
      "epoch:9 step:9323 [D loss: 0.191731, acc.: 67.97%] [G loss: 0.491671]\n",
      "epoch:9 step:9324 [D loss: 0.264896, acc.: 54.69%] [G loss: 0.450244]\n",
      "epoch:9 step:9325 [D loss: 0.247136, acc.: 55.47%] [G loss: 0.432421]\n",
      "epoch:9 step:9326 [D loss: 0.205850, acc.: 71.88%] [G loss: 0.443360]\n",
      "epoch:9 step:9327 [D loss: 0.197272, acc.: 67.19%] [G loss: 0.510024]\n",
      "epoch:9 step:9328 [D loss: 0.204144, acc.: 70.31%] [G loss: 0.503139]\n",
      "epoch:9 step:9329 [D loss: 0.200811, acc.: 71.88%] [G loss: 0.488865]\n",
      "epoch:9 step:9330 [D loss: 0.179957, acc.: 72.66%] [G loss: 0.507819]\n",
      "epoch:9 step:9331 [D loss: 0.215896, acc.: 63.28%] [G loss: 0.454584]\n",
      "epoch:9 step:9332 [D loss: 0.189349, acc.: 72.66%] [G loss: 0.511992]\n",
      "epoch:9 step:9333 [D loss: 0.177487, acc.: 73.44%] [G loss: 0.557024]\n",
      "epoch:9 step:9334 [D loss: 0.219008, acc.: 66.41%] [G loss: 0.453771]\n",
      "epoch:9 step:9335 [D loss: 0.246377, acc.: 56.25%] [G loss: 0.446893]\n",
      "epoch:9 step:9336 [D loss: 0.206155, acc.: 67.97%] [G loss: 0.446003]\n",
      "epoch:9 step:9337 [D loss: 0.215602, acc.: 67.97%] [G loss: 0.443430]\n",
      "epoch:9 step:9338 [D loss: 0.225260, acc.: 67.19%] [G loss: 0.455124]\n",
      "epoch:9 step:9339 [D loss: 0.184577, acc.: 71.09%] [G loss: 0.526056]\n",
      "epoch:9 step:9340 [D loss: 0.246152, acc.: 59.38%] [G loss: 0.463627]\n",
      "epoch:9 step:9341 [D loss: 0.191272, acc.: 73.44%] [G loss: 0.501627]\n",
      "epoch:9 step:9342 [D loss: 0.213172, acc.: 67.19%] [G loss: 0.478969]\n",
      "epoch:9 step:9343 [D loss: 0.231807, acc.: 62.50%] [G loss: 0.468649]\n",
      "epoch:9 step:9344 [D loss: 0.192700, acc.: 72.66%] [G loss: 0.491302]\n",
      "epoch:9 step:9345 [D loss: 0.167904, acc.: 75.00%] [G loss: 0.505639]\n",
      "epoch:9 step:9346 [D loss: 0.215631, acc.: 68.75%] [G loss: 0.525532]\n",
      "epoch:9 step:9347 [D loss: 0.233689, acc.: 61.72%] [G loss: 0.515919]\n",
      "epoch:9 step:9348 [D loss: 0.269452, acc.: 58.59%] [G loss: 0.469066]\n",
      "epoch:9 step:9349 [D loss: 0.235164, acc.: 59.38%] [G loss: 0.488237]\n",
      "epoch:9 step:9350 [D loss: 0.218294, acc.: 64.06%] [G loss: 0.488553]\n",
      "epoch:9 step:9351 [D loss: 0.207203, acc.: 72.66%] [G loss: 0.509470]\n",
      "epoch:9 step:9352 [D loss: 0.209964, acc.: 68.75%] [G loss: 0.467906]\n",
      "epoch:9 step:9353 [D loss: 0.297057, acc.: 46.88%] [G loss: 0.430428]\n",
      "epoch:9 step:9354 [D loss: 0.219578, acc.: 61.72%] [G loss: 0.467022]\n",
      "epoch:9 step:9355 [D loss: 0.215336, acc.: 63.28%] [G loss: 0.533025]\n",
      "epoch:9 step:9356 [D loss: 0.205102, acc.: 68.75%] [G loss: 0.494882]\n",
      "epoch:9 step:9357 [D loss: 0.177992, acc.: 78.91%] [G loss: 0.518311]\n",
      "epoch:9 step:9358 [D loss: 0.173535, acc.: 73.44%] [G loss: 0.515286]\n",
      "epoch:9 step:9359 [D loss: 0.188195, acc.: 71.88%] [G loss: 0.577244]\n",
      "epoch:9 step:9360 [D loss: 0.180235, acc.: 71.88%] [G loss: 0.529885]\n",
      "epoch:9 step:9361 [D loss: 0.283610, acc.: 53.91%] [G loss: 0.550865]\n",
      "epoch:9 step:9362 [D loss: 0.166092, acc.: 79.69%] [G loss: 0.578333]\n",
      "epoch:9 step:9363 [D loss: 0.211567, acc.: 70.31%] [G loss: 0.545739]\n",
      "epoch:9 step:9364 [D loss: 0.255044, acc.: 61.72%] [G loss: 0.479460]\n",
      "epoch:9 step:9365 [D loss: 0.291225, acc.: 46.88%] [G loss: 0.413461]\n",
      "epoch:9 step:9366 [D loss: 0.219073, acc.: 67.97%] [G loss: 0.492952]\n",
      "epoch:9 step:9367 [D loss: 0.239921, acc.: 61.72%] [G loss: 0.485379]\n",
      "epoch:9 step:9368 [D loss: 0.201378, acc.: 71.88%] [G loss: 0.509238]\n",
      "epoch:9 step:9369 [D loss: 0.174633, acc.: 75.00%] [G loss: 0.514834]\n",
      "epoch:9 step:9370 [D loss: 0.174350, acc.: 71.88%] [G loss: 0.554221]\n",
      "epoch:10 step:9371 [D loss: 0.229600, acc.: 60.94%] [G loss: 0.522188]\n",
      "epoch:10 step:9372 [D loss: 0.285948, acc.: 57.81%] [G loss: 0.525349]\n",
      "epoch:10 step:9373 [D loss: 0.230442, acc.: 58.59%] [G loss: 0.547958]\n",
      "epoch:10 step:9374 [D loss: 0.231605, acc.: 63.28%] [G loss: 0.487075]\n",
      "epoch:10 step:9375 [D loss: 0.216301, acc.: 67.19%] [G loss: 0.493440]\n",
      "epoch:10 step:9376 [D loss: 0.212231, acc.: 65.62%] [G loss: 0.522763]\n",
      "epoch:10 step:9377 [D loss: 0.196546, acc.: 68.75%] [G loss: 0.496460]\n",
      "epoch:10 step:9378 [D loss: 0.209389, acc.: 64.06%] [G loss: 0.461293]\n",
      "epoch:10 step:9379 [D loss: 0.223473, acc.: 64.06%] [G loss: 0.457060]\n",
      "epoch:10 step:9380 [D loss: 0.200849, acc.: 75.00%] [G loss: 0.507550]\n",
      "epoch:10 step:9381 [D loss: 0.203624, acc.: 68.75%] [G loss: 0.487881]\n",
      "epoch:10 step:9382 [D loss: 0.229814, acc.: 61.72%] [G loss: 0.483501]\n",
      "epoch:10 step:9383 [D loss: 0.220224, acc.: 63.28%] [G loss: 0.480799]\n",
      "epoch:10 step:9384 [D loss: 0.210327, acc.: 66.41%] [G loss: 0.466867]\n",
      "epoch:10 step:9385 [D loss: 0.189928, acc.: 72.66%] [G loss: 0.516369]\n",
      "epoch:10 step:9386 [D loss: 0.189854, acc.: 71.88%] [G loss: 0.557075]\n",
      "epoch:10 step:9387 [D loss: 0.266575, acc.: 54.69%] [G loss: 0.481671]\n",
      "epoch:10 step:9388 [D loss: 0.227865, acc.: 62.50%] [G loss: 0.488984]\n",
      "epoch:10 step:9389 [D loss: 0.241323, acc.: 65.62%] [G loss: 0.465193]\n",
      "epoch:10 step:9390 [D loss: 0.235605, acc.: 55.47%] [G loss: 0.451660]\n",
      "epoch:10 step:9391 [D loss: 0.226036, acc.: 64.84%] [G loss: 0.560987]\n",
      "epoch:10 step:9392 [D loss: 0.221149, acc.: 63.28%] [G loss: 0.511665]\n",
      "epoch:10 step:9393 [D loss: 0.209052, acc.: 67.97%] [G loss: 0.448932]\n",
      "epoch:10 step:9394 [D loss: 0.243228, acc.: 57.03%] [G loss: 0.418916]\n",
      "epoch:10 step:9395 [D loss: 0.194532, acc.: 70.31%] [G loss: 0.482804]\n",
      "epoch:10 step:9396 [D loss: 0.233480, acc.: 62.50%] [G loss: 0.524082]\n",
      "epoch:10 step:9397 [D loss: 0.231515, acc.: 65.62%] [G loss: 0.474054]\n",
      "epoch:10 step:9398 [D loss: 0.201359, acc.: 69.53%] [G loss: 0.464948]\n",
      "epoch:10 step:9399 [D loss: 0.208778, acc.: 69.53%] [G loss: 0.509860]\n",
      "epoch:10 step:9400 [D loss: 0.240511, acc.: 60.94%] [G loss: 0.472666]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.715139\n",
      "FID: 10.527513\n",
      "0 = 11.99852776653767\n",
      "1 = 0.053281978479142976\n",
      "2 = 0.9158999919891357\n",
      "3 = 0.8889999985694885\n",
      "4 = 0.942799985408783\n",
      "5 = 0.9395476579666138\n",
      "6 = 0.8889999985694885\n",
      "7 = 6.383378343862291\n",
      "8 = 0.06619228224007248\n",
      "9 = 0.7121999859809875\n",
      "10 = 0.7042999863624573\n",
      "11 = 0.7200999855995178\n",
      "12 = 0.7156065702438354\n",
      "13 = 0.7042999863624573\n",
      "14 = 7.715212821960449\n",
      "15 = 9.505192756652832\n",
      "16 = 0.12362464517354965\n",
      "17 = 7.715138912200928\n",
      "18 = 10.527512550354004\n",
      "epoch:10 step:9401 [D loss: 0.235772, acc.: 58.59%] [G loss: 0.494889]\n",
      "epoch:10 step:9402 [D loss: 0.211503, acc.: 67.19%] [G loss: 0.523880]\n",
      "epoch:10 step:9403 [D loss: 0.224206, acc.: 62.50%] [G loss: 0.488991]\n",
      "epoch:10 step:9404 [D loss: 0.233184, acc.: 62.50%] [G loss: 0.454088]\n",
      "epoch:10 step:9405 [D loss: 0.227162, acc.: 64.06%] [G loss: 0.499963]\n",
      "epoch:10 step:9406 [D loss: 0.206845, acc.: 68.75%] [G loss: 0.486625]\n",
      "epoch:10 step:9407 [D loss: 0.232329, acc.: 60.16%] [G loss: 0.465000]\n",
      "epoch:10 step:9408 [D loss: 0.245029, acc.: 66.41%] [G loss: 0.452535]\n",
      "epoch:10 step:9409 [D loss: 0.189850, acc.: 74.22%] [G loss: 0.452301]\n",
      "epoch:10 step:9410 [D loss: 0.180793, acc.: 71.09%] [G loss: 0.507304]\n",
      "epoch:10 step:9411 [D loss: 0.232605, acc.: 59.38%] [G loss: 0.439737]\n",
      "epoch:10 step:9412 [D loss: 0.202802, acc.: 65.62%] [G loss: 0.467267]\n",
      "epoch:10 step:9413 [D loss: 0.204636, acc.: 68.75%] [G loss: 0.456958]\n",
      "epoch:10 step:9414 [D loss: 0.246972, acc.: 54.69%] [G loss: 0.454930]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9415 [D loss: 0.224725, acc.: 64.84%] [G loss: 0.473790]\n",
      "epoch:10 step:9416 [D loss: 0.209428, acc.: 65.62%] [G loss: 0.494505]\n",
      "epoch:10 step:9417 [D loss: 0.221286, acc.: 65.62%] [G loss: 0.451580]\n",
      "epoch:10 step:9418 [D loss: 0.217796, acc.: 66.41%] [G loss: 0.476353]\n",
      "epoch:10 step:9419 [D loss: 0.211024, acc.: 64.84%] [G loss: 0.495228]\n",
      "epoch:10 step:9420 [D loss: 0.201095, acc.: 71.09%] [G loss: 0.515083]\n",
      "epoch:10 step:9421 [D loss: 0.233730, acc.: 60.16%] [G loss: 0.465570]\n",
      "epoch:10 step:9422 [D loss: 0.223731, acc.: 63.28%] [G loss: 0.481507]\n",
      "epoch:10 step:9423 [D loss: 0.208311, acc.: 65.62%] [G loss: 0.484263]\n",
      "epoch:10 step:9424 [D loss: 0.205332, acc.: 68.75%] [G loss: 0.483290]\n",
      "epoch:10 step:9425 [D loss: 0.214044, acc.: 64.84%] [G loss: 0.447068]\n",
      "epoch:10 step:9426 [D loss: 0.231502, acc.: 62.50%] [G loss: 0.464827]\n",
      "epoch:10 step:9427 [D loss: 0.200093, acc.: 70.31%] [G loss: 0.505532]\n",
      "epoch:10 step:9428 [D loss: 0.210161, acc.: 69.53%] [G loss: 0.485968]\n",
      "epoch:10 step:9429 [D loss: 0.244109, acc.: 53.91%] [G loss: 0.484242]\n",
      "epoch:10 step:9430 [D loss: 0.238984, acc.: 57.03%] [G loss: 0.464679]\n",
      "epoch:10 step:9431 [D loss: 0.279012, acc.: 49.22%] [G loss: 0.461905]\n",
      "epoch:10 step:9432 [D loss: 0.211329, acc.: 71.09%] [G loss: 0.471319]\n",
      "epoch:10 step:9433 [D loss: 0.222498, acc.: 62.50%] [G loss: 0.450557]\n",
      "epoch:10 step:9434 [D loss: 0.221084, acc.: 64.84%] [G loss: 0.494191]\n",
      "epoch:10 step:9435 [D loss: 0.229141, acc.: 61.72%] [G loss: 0.510812]\n",
      "epoch:10 step:9436 [D loss: 0.213192, acc.: 64.84%] [G loss: 0.446862]\n",
      "epoch:10 step:9437 [D loss: 0.228886, acc.: 63.28%] [G loss: 0.471609]\n",
      "epoch:10 step:9438 [D loss: 0.252649, acc.: 56.25%] [G loss: 0.419520]\n",
      "epoch:10 step:9439 [D loss: 0.189084, acc.: 71.88%] [G loss: 0.513114]\n",
      "epoch:10 step:9440 [D loss: 0.189162, acc.: 76.56%] [G loss: 0.519071]\n",
      "epoch:10 step:9441 [D loss: 0.247339, acc.: 56.25%] [G loss: 0.490280]\n",
      "epoch:10 step:9442 [D loss: 0.232603, acc.: 60.16%] [G loss: 0.426891]\n",
      "epoch:10 step:9443 [D loss: 0.225389, acc.: 67.97%] [G loss: 0.451891]\n",
      "epoch:10 step:9444 [D loss: 0.184730, acc.: 75.78%] [G loss: 0.445295]\n",
      "epoch:10 step:9445 [D loss: 0.199072, acc.: 68.75%] [G loss: 0.487981]\n",
      "epoch:10 step:9446 [D loss: 0.230598, acc.: 64.84%] [G loss: 0.485740]\n",
      "epoch:10 step:9447 [D loss: 0.169998, acc.: 77.34%] [G loss: 0.526490]\n",
      "epoch:10 step:9448 [D loss: 0.269005, acc.: 56.25%] [G loss: 0.469908]\n",
      "epoch:10 step:9449 [D loss: 0.229285, acc.: 59.38%] [G loss: 0.446690]\n",
      "epoch:10 step:9450 [D loss: 0.204324, acc.: 69.53%] [G loss: 0.497189]\n",
      "epoch:10 step:9451 [D loss: 0.204752, acc.: 70.31%] [G loss: 0.465600]\n",
      "epoch:10 step:9452 [D loss: 0.223371, acc.: 61.72%] [G loss: 0.481181]\n",
      "epoch:10 step:9453 [D loss: 0.204829, acc.: 64.84%] [G loss: 0.510826]\n",
      "epoch:10 step:9454 [D loss: 0.220090, acc.: 64.06%] [G loss: 0.466096]\n",
      "epoch:10 step:9455 [D loss: 0.248411, acc.: 51.56%] [G loss: 0.458059]\n",
      "epoch:10 step:9456 [D loss: 0.219452, acc.: 63.28%] [G loss: 0.466843]\n",
      "epoch:10 step:9457 [D loss: 0.214392, acc.: 65.62%] [G loss: 0.485625]\n",
      "epoch:10 step:9458 [D loss: 0.225870, acc.: 63.28%] [G loss: 0.462061]\n",
      "epoch:10 step:9459 [D loss: 0.207932, acc.: 67.19%] [G loss: 0.486661]\n",
      "epoch:10 step:9460 [D loss: 0.197755, acc.: 67.97%] [G loss: 0.492535]\n",
      "epoch:10 step:9461 [D loss: 0.226485, acc.: 64.84%] [G loss: 0.470023]\n",
      "epoch:10 step:9462 [D loss: 0.213573, acc.: 67.97%] [G loss: 0.443837]\n",
      "epoch:10 step:9463 [D loss: 0.221070, acc.: 68.75%] [G loss: 0.455584]\n",
      "epoch:10 step:9464 [D loss: 0.222179, acc.: 60.16%] [G loss: 0.460298]\n",
      "epoch:10 step:9465 [D loss: 0.196719, acc.: 68.75%] [G loss: 0.489882]\n",
      "epoch:10 step:9466 [D loss: 0.196420, acc.: 64.06%] [G loss: 0.487095]\n",
      "epoch:10 step:9467 [D loss: 0.187524, acc.: 71.88%] [G loss: 0.542997]\n",
      "epoch:10 step:9468 [D loss: 0.215897, acc.: 64.06%] [G loss: 0.478736]\n",
      "epoch:10 step:9469 [D loss: 0.221088, acc.: 62.50%] [G loss: 0.452661]\n",
      "epoch:10 step:9470 [D loss: 0.175334, acc.: 75.78%] [G loss: 0.515711]\n",
      "epoch:10 step:9471 [D loss: 0.213309, acc.: 64.06%] [G loss: 0.476194]\n",
      "epoch:10 step:9472 [D loss: 0.282379, acc.: 53.91%] [G loss: 0.377701]\n",
      "epoch:10 step:9473 [D loss: 0.215237, acc.: 66.41%] [G loss: 0.467160]\n",
      "epoch:10 step:9474 [D loss: 0.229451, acc.: 64.84%] [G loss: 0.447552]\n",
      "epoch:10 step:9475 [D loss: 0.248889, acc.: 53.91%] [G loss: 0.467731]\n",
      "epoch:10 step:9476 [D loss: 0.185349, acc.: 73.44%] [G loss: 0.463485]\n",
      "epoch:10 step:9477 [D loss: 0.183322, acc.: 75.78%] [G loss: 0.541544]\n",
      "epoch:10 step:9478 [D loss: 0.276031, acc.: 55.47%] [G loss: 0.488530]\n",
      "epoch:10 step:9479 [D loss: 0.255826, acc.: 58.59%] [G loss: 0.437846]\n",
      "epoch:10 step:9480 [D loss: 0.241434, acc.: 57.03%] [G loss: 0.458702]\n",
      "epoch:10 step:9481 [D loss: 0.230015, acc.: 62.50%] [G loss: 0.458578]\n",
      "epoch:10 step:9482 [D loss: 0.194372, acc.: 72.66%] [G loss: 0.491124]\n",
      "epoch:10 step:9483 [D loss: 0.233833, acc.: 65.62%] [G loss: 0.501661]\n",
      "epoch:10 step:9484 [D loss: 0.226941, acc.: 60.94%] [G loss: 0.463028]\n",
      "epoch:10 step:9485 [D loss: 0.200040, acc.: 67.19%] [G loss: 0.535181]\n",
      "epoch:10 step:9486 [D loss: 0.206930, acc.: 66.41%] [G loss: 0.517829]\n",
      "epoch:10 step:9487 [D loss: 0.204954, acc.: 71.09%] [G loss: 0.500043]\n",
      "epoch:10 step:9488 [D loss: 0.225541, acc.: 64.84%] [G loss: 0.469965]\n",
      "epoch:10 step:9489 [D loss: 0.174627, acc.: 73.44%] [G loss: 0.569529]\n",
      "epoch:10 step:9490 [D loss: 0.232417, acc.: 67.19%] [G loss: 0.490834]\n",
      "epoch:10 step:9491 [D loss: 0.262181, acc.: 57.81%] [G loss: 0.435886]\n",
      "epoch:10 step:9492 [D loss: 0.216652, acc.: 62.50%] [G loss: 0.486934]\n",
      "epoch:10 step:9493 [D loss: 0.192349, acc.: 71.88%] [G loss: 0.524840]\n",
      "epoch:10 step:9494 [D loss: 0.242980, acc.: 55.47%] [G loss: 0.503277]\n",
      "epoch:10 step:9495 [D loss: 0.224748, acc.: 64.06%] [G loss: 0.429424]\n",
      "epoch:10 step:9496 [D loss: 0.202631, acc.: 61.72%] [G loss: 0.444057]\n",
      "epoch:10 step:9497 [D loss: 0.223061, acc.: 60.94%] [G loss: 0.477148]\n",
      "epoch:10 step:9498 [D loss: 0.230591, acc.: 61.72%] [G loss: 0.453326]\n",
      "epoch:10 step:9499 [D loss: 0.217600, acc.: 60.16%] [G loss: 0.458749]\n",
      "epoch:10 step:9500 [D loss: 0.224826, acc.: 57.03%] [G loss: 0.473222]\n",
      "epoch:10 step:9501 [D loss: 0.222332, acc.: 64.84%] [G loss: 0.455070]\n",
      "epoch:10 step:9502 [D loss: 0.226825, acc.: 64.84%] [G loss: 0.541400]\n",
      "epoch:10 step:9503 [D loss: 0.229446, acc.: 60.16%] [G loss: 0.492099]\n",
      "epoch:10 step:9504 [D loss: 0.208395, acc.: 69.53%] [G loss: 0.463080]\n",
      "epoch:10 step:9505 [D loss: 0.233004, acc.: 61.72%] [G loss: 0.466061]\n",
      "epoch:10 step:9506 [D loss: 0.219824, acc.: 69.53%] [G loss: 0.468480]\n",
      "epoch:10 step:9507 [D loss: 0.248511, acc.: 53.91%] [G loss: 0.457310]\n",
      "epoch:10 step:9508 [D loss: 0.253345, acc.: 59.38%] [G loss: 0.405025]\n",
      "epoch:10 step:9509 [D loss: 0.245957, acc.: 60.16%] [G loss: 0.440137]\n",
      "epoch:10 step:9510 [D loss: 0.240791, acc.: 58.59%] [G loss: 0.419620]\n",
      "epoch:10 step:9511 [D loss: 0.236851, acc.: 64.06%] [G loss: 0.448677]\n",
      "epoch:10 step:9512 [D loss: 0.241547, acc.: 57.81%] [G loss: 0.429567]\n",
      "epoch:10 step:9513 [D loss: 0.247743, acc.: 51.56%] [G loss: 0.471461]\n",
      "epoch:10 step:9514 [D loss: 0.198515, acc.: 67.97%] [G loss: 0.503527]\n",
      "epoch:10 step:9515 [D loss: 0.226147, acc.: 58.59%] [G loss: 0.479852]\n",
      "epoch:10 step:9516 [D loss: 0.216236, acc.: 67.19%] [G loss: 0.470864]\n",
      "epoch:10 step:9517 [D loss: 0.239989, acc.: 60.94%] [G loss: 0.399962]\n",
      "epoch:10 step:9518 [D loss: 0.258909, acc.: 57.81%] [G loss: 0.436432]\n",
      "epoch:10 step:9519 [D loss: 0.205948, acc.: 66.41%] [G loss: 0.452450]\n",
      "epoch:10 step:9520 [D loss: 0.251535, acc.: 58.59%] [G loss: 0.456983]\n",
      "epoch:10 step:9521 [D loss: 0.235251, acc.: 63.28%] [G loss: 0.485479]\n",
      "epoch:10 step:9522 [D loss: 0.215978, acc.: 67.19%] [G loss: 0.459361]\n",
      "epoch:10 step:9523 [D loss: 0.264078, acc.: 58.59%] [G loss: 0.438861]\n",
      "epoch:10 step:9524 [D loss: 0.202442, acc.: 71.09%] [G loss: 0.474043]\n",
      "epoch:10 step:9525 [D loss: 0.191276, acc.: 70.31%] [G loss: 0.414893]\n",
      "epoch:10 step:9526 [D loss: 0.206143, acc.: 67.97%] [G loss: 0.502890]\n",
      "epoch:10 step:9527 [D loss: 0.249363, acc.: 60.16%] [G loss: 0.445872]\n",
      "epoch:10 step:9528 [D loss: 0.231463, acc.: 62.50%] [G loss: 0.450778]\n",
      "epoch:10 step:9529 [D loss: 0.235442, acc.: 64.06%] [G loss: 0.470946]\n",
      "epoch:10 step:9530 [D loss: 0.263033, acc.: 49.22%] [G loss: 0.442352]\n",
      "epoch:10 step:9531 [D loss: 0.220511, acc.: 64.06%] [G loss: 0.465712]\n",
      "epoch:10 step:9532 [D loss: 0.229626, acc.: 63.28%] [G loss: 0.463416]\n",
      "epoch:10 step:9533 [D loss: 0.228827, acc.: 55.47%] [G loss: 0.462155]\n",
      "epoch:10 step:9534 [D loss: 0.231784, acc.: 57.03%] [G loss: 0.465035]\n",
      "epoch:10 step:9535 [D loss: 0.196712, acc.: 68.75%] [G loss: 0.489037]\n",
      "epoch:10 step:9536 [D loss: 0.229096, acc.: 64.06%] [G loss: 0.460077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9537 [D loss: 0.211562, acc.: 66.41%] [G loss: 0.451283]\n",
      "epoch:10 step:9538 [D loss: 0.241041, acc.: 66.41%] [G loss: 0.433033]\n",
      "epoch:10 step:9539 [D loss: 0.228717, acc.: 63.28%] [G loss: 0.462229]\n",
      "epoch:10 step:9540 [D loss: 0.225672, acc.: 59.38%] [G loss: 0.484734]\n",
      "epoch:10 step:9541 [D loss: 0.228271, acc.: 58.59%] [G loss: 0.423907]\n",
      "epoch:10 step:9542 [D loss: 0.222989, acc.: 63.28%] [G loss: 0.430451]\n",
      "epoch:10 step:9543 [D loss: 0.211251, acc.: 65.62%] [G loss: 0.436228]\n",
      "epoch:10 step:9544 [D loss: 0.252546, acc.: 52.34%] [G loss: 0.449196]\n",
      "epoch:10 step:9545 [D loss: 0.211173, acc.: 67.97%] [G loss: 0.444585]\n",
      "epoch:10 step:9546 [D loss: 0.216109, acc.: 62.50%] [G loss: 0.446230]\n",
      "epoch:10 step:9547 [D loss: 0.249148, acc.: 50.00%] [G loss: 0.432351]\n",
      "epoch:10 step:9548 [D loss: 0.227449, acc.: 64.06%] [G loss: 0.456955]\n",
      "epoch:10 step:9549 [D loss: 0.227394, acc.: 57.03%] [G loss: 0.453591]\n",
      "epoch:10 step:9550 [D loss: 0.247389, acc.: 64.84%] [G loss: 0.446865]\n",
      "epoch:10 step:9551 [D loss: 0.211576, acc.: 68.75%] [G loss: 0.433292]\n",
      "epoch:10 step:9552 [D loss: 0.238296, acc.: 57.81%] [G loss: 0.444625]\n",
      "epoch:10 step:9553 [D loss: 0.253870, acc.: 56.25%] [G loss: 0.448130]\n",
      "epoch:10 step:9554 [D loss: 0.215628, acc.: 64.84%] [G loss: 0.454294]\n",
      "epoch:10 step:9555 [D loss: 0.223659, acc.: 61.72%] [G loss: 0.461192]\n",
      "epoch:10 step:9556 [D loss: 0.272580, acc.: 55.47%] [G loss: 0.469736]\n",
      "epoch:10 step:9557 [D loss: 0.226414, acc.: 64.84%] [G loss: 0.473284]\n",
      "epoch:10 step:9558 [D loss: 0.229773, acc.: 56.25%] [G loss: 0.416986]\n",
      "epoch:10 step:9559 [D loss: 0.256698, acc.: 50.00%] [G loss: 0.400148]\n",
      "epoch:10 step:9560 [D loss: 0.211996, acc.: 63.28%] [G loss: 0.435821]\n",
      "epoch:10 step:9561 [D loss: 0.235680, acc.: 63.28%] [G loss: 0.437747]\n",
      "epoch:10 step:9562 [D loss: 0.192442, acc.: 71.09%] [G loss: 0.523848]\n",
      "epoch:10 step:9563 [D loss: 0.220228, acc.: 58.59%] [G loss: 0.465956]\n",
      "epoch:10 step:9564 [D loss: 0.201042, acc.: 71.88%] [G loss: 0.446288]\n",
      "epoch:10 step:9565 [D loss: 0.183945, acc.: 71.88%] [G loss: 0.497104]\n",
      "epoch:10 step:9566 [D loss: 0.229595, acc.: 59.38%] [G loss: 0.472427]\n",
      "epoch:10 step:9567 [D loss: 0.213233, acc.: 70.31%] [G loss: 0.467431]\n",
      "epoch:10 step:9568 [D loss: 0.181762, acc.: 72.66%] [G loss: 0.495682]\n",
      "epoch:10 step:9569 [D loss: 0.253776, acc.: 57.81%] [G loss: 0.433259]\n",
      "epoch:10 step:9570 [D loss: 0.251202, acc.: 60.16%] [G loss: 0.427352]\n",
      "epoch:10 step:9571 [D loss: 0.224748, acc.: 60.16%] [G loss: 0.485543]\n",
      "epoch:10 step:9572 [D loss: 0.233571, acc.: 64.06%] [G loss: 0.461941]\n",
      "epoch:10 step:9573 [D loss: 0.243839, acc.: 60.16%] [G loss: 0.460665]\n",
      "epoch:10 step:9574 [D loss: 0.219526, acc.: 65.62%] [G loss: 0.473187]\n",
      "epoch:10 step:9575 [D loss: 0.199339, acc.: 69.53%] [G loss: 0.487244]\n",
      "epoch:10 step:9576 [D loss: 0.225427, acc.: 61.72%] [G loss: 0.467439]\n",
      "epoch:10 step:9577 [D loss: 0.185474, acc.: 78.12%] [G loss: 0.551274]\n",
      "epoch:10 step:9578 [D loss: 0.213620, acc.: 70.31%] [G loss: 0.497756]\n",
      "epoch:10 step:9579 [D loss: 0.188418, acc.: 67.97%] [G loss: 0.502608]\n",
      "epoch:10 step:9580 [D loss: 0.268457, acc.: 50.78%] [G loss: 0.428699]\n",
      "epoch:10 step:9581 [D loss: 0.233199, acc.: 57.03%] [G loss: 0.460617]\n",
      "epoch:10 step:9582 [D loss: 0.247611, acc.: 56.25%] [G loss: 0.444226]\n",
      "epoch:10 step:9583 [D loss: 0.209785, acc.: 66.41%] [G loss: 0.477191]\n",
      "epoch:10 step:9584 [D loss: 0.275113, acc.: 54.69%] [G loss: 0.429395]\n",
      "epoch:10 step:9585 [D loss: 0.239622, acc.: 56.25%] [G loss: 0.388844]\n",
      "epoch:10 step:9586 [D loss: 0.220164, acc.: 62.50%] [G loss: 0.455584]\n",
      "epoch:10 step:9587 [D loss: 0.209861, acc.: 65.62%] [G loss: 0.479585]\n",
      "epoch:10 step:9588 [D loss: 0.205683, acc.: 67.19%] [G loss: 0.490207]\n",
      "epoch:10 step:9589 [D loss: 0.197259, acc.: 67.19%] [G loss: 0.441051]\n",
      "epoch:10 step:9590 [D loss: 0.249637, acc.: 58.59%] [G loss: 0.461393]\n",
      "epoch:10 step:9591 [D loss: 0.189384, acc.: 71.09%] [G loss: 0.493584]\n",
      "epoch:10 step:9592 [D loss: 0.194967, acc.: 68.75%] [G loss: 0.542195]\n",
      "epoch:10 step:9593 [D loss: 0.200342, acc.: 73.44%] [G loss: 0.548171]\n",
      "epoch:10 step:9594 [D loss: 0.274459, acc.: 53.91%] [G loss: 0.439418]\n",
      "epoch:10 step:9595 [D loss: 0.261634, acc.: 53.12%] [G loss: 0.433486]\n",
      "epoch:10 step:9596 [D loss: 0.233003, acc.: 60.94%] [G loss: 0.424610]\n",
      "epoch:10 step:9597 [D loss: 0.225128, acc.: 66.41%] [G loss: 0.458888]\n",
      "epoch:10 step:9598 [D loss: 0.285751, acc.: 52.34%] [G loss: 0.413980]\n",
      "epoch:10 step:9599 [D loss: 0.223233, acc.: 59.38%] [G loss: 0.449303]\n",
      "epoch:10 step:9600 [D loss: 0.202771, acc.: 68.75%] [G loss: 0.484798]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.503321\n",
      "FID: 14.314391\n",
      "0 = 11.911859289836803\n",
      "1 = 0.04813101000228784\n",
      "2 = 0.9067999720573425\n",
      "3 = 0.880299985408783\n",
      "4 = 0.9333000183105469\n",
      "5 = 0.9295670390129089\n",
      "6 = 0.880299985408783\n",
      "7 = 6.780971528881776\n",
      "8 = 0.08532985524698943\n",
      "9 = 0.7249500155448914\n",
      "10 = 0.7202000021934509\n",
      "11 = 0.7297000288963318\n",
      "12 = 0.7271075248718262\n",
      "13 = 0.7202000021934509\n",
      "14 = 7.5033860206604\n",
      "15 = 9.524481773376465\n",
      "16 = 0.12157445400953293\n",
      "17 = 7.503321170806885\n",
      "18 = 14.314391136169434\n",
      "epoch:10 step:9601 [D loss: 0.199336, acc.: 69.53%] [G loss: 0.530143]\n",
      "epoch:10 step:9602 [D loss: 0.186187, acc.: 74.22%] [G loss: 0.550596]\n",
      "epoch:10 step:9603 [D loss: 0.254046, acc.: 59.38%] [G loss: 0.458338]\n",
      "epoch:10 step:9604 [D loss: 0.242041, acc.: 56.25%] [G loss: 0.432542]\n",
      "epoch:10 step:9605 [D loss: 0.249479, acc.: 55.47%] [G loss: 0.428865]\n",
      "epoch:10 step:9606 [D loss: 0.221889, acc.: 60.94%] [G loss: 0.480019]\n",
      "epoch:10 step:9607 [D loss: 0.215754, acc.: 65.62%] [G loss: 0.452995]\n",
      "epoch:10 step:9608 [D loss: 0.220511, acc.: 64.06%] [G loss: 0.427186]\n",
      "epoch:10 step:9609 [D loss: 0.224084, acc.: 62.50%] [G loss: 0.482203]\n",
      "epoch:10 step:9610 [D loss: 0.205928, acc.: 65.62%] [G loss: 0.448556]\n",
      "epoch:10 step:9611 [D loss: 0.214148, acc.: 65.62%] [G loss: 0.506654]\n",
      "epoch:10 step:9612 [D loss: 0.222905, acc.: 64.06%] [G loss: 0.481921]\n",
      "epoch:10 step:9613 [D loss: 0.200633, acc.: 65.62%] [G loss: 0.490760]\n",
      "epoch:10 step:9614 [D loss: 0.220668, acc.: 61.72%] [G loss: 0.492212]\n",
      "epoch:10 step:9615 [D loss: 0.208009, acc.: 65.62%] [G loss: 0.491047]\n",
      "epoch:10 step:9616 [D loss: 0.227424, acc.: 65.62%] [G loss: 0.459307]\n",
      "epoch:10 step:9617 [D loss: 0.228847, acc.: 63.28%] [G loss: 0.486026]\n",
      "epoch:10 step:9618 [D loss: 0.220976, acc.: 63.28%] [G loss: 0.521535]\n",
      "epoch:10 step:9619 [D loss: 0.275569, acc.: 53.12%] [G loss: 0.465695]\n",
      "epoch:10 step:9620 [D loss: 0.281502, acc.: 50.00%] [G loss: 0.475672]\n",
      "epoch:10 step:9621 [D loss: 0.238022, acc.: 59.38%] [G loss: 0.422482]\n",
      "epoch:10 step:9622 [D loss: 0.237394, acc.: 60.16%] [G loss: 0.482562]\n",
      "epoch:10 step:9623 [D loss: 0.222307, acc.: 57.81%] [G loss: 0.468829]\n",
      "epoch:10 step:9624 [D loss: 0.212151, acc.: 67.97%] [G loss: 0.466417]\n",
      "epoch:10 step:9625 [D loss: 0.203213, acc.: 70.31%] [G loss: 0.438733]\n",
      "epoch:10 step:9626 [D loss: 0.221313, acc.: 64.84%] [G loss: 0.449338]\n",
      "epoch:10 step:9627 [D loss: 0.216757, acc.: 65.62%] [G loss: 0.448287]\n",
      "epoch:10 step:9628 [D loss: 0.212619, acc.: 68.75%] [G loss: 0.493296]\n",
      "epoch:10 step:9629 [D loss: 0.211245, acc.: 67.19%] [G loss: 0.496388]\n",
      "epoch:10 step:9630 [D loss: 0.196691, acc.: 68.75%] [G loss: 0.499952]\n",
      "epoch:10 step:9631 [D loss: 0.215793, acc.: 69.53%] [G loss: 0.505350]\n",
      "epoch:10 step:9632 [D loss: 0.207346, acc.: 64.06%] [G loss: 0.475916]\n",
      "epoch:10 step:9633 [D loss: 0.241541, acc.: 58.59%] [G loss: 0.478792]\n",
      "epoch:10 step:9634 [D loss: 0.197152, acc.: 71.88%] [G loss: 0.460206]\n",
      "epoch:10 step:9635 [D loss: 0.272830, acc.: 46.09%] [G loss: 0.458481]\n",
      "epoch:10 step:9636 [D loss: 0.241002, acc.: 64.84%] [G loss: 0.467555]\n",
      "epoch:10 step:9637 [D loss: 0.232290, acc.: 63.28%] [G loss: 0.440625]\n",
      "epoch:10 step:9638 [D loss: 0.210377, acc.: 64.06%] [G loss: 0.511735]\n",
      "epoch:10 step:9639 [D loss: 0.206411, acc.: 66.41%] [G loss: 0.485694]\n",
      "epoch:10 step:9640 [D loss: 0.225242, acc.: 63.28%] [G loss: 0.426311]\n",
      "epoch:10 step:9641 [D loss: 0.189359, acc.: 72.66%] [G loss: 0.498302]\n",
      "epoch:10 step:9642 [D loss: 0.230599, acc.: 61.72%] [G loss: 0.450063]\n",
      "epoch:10 step:9643 [D loss: 0.212403, acc.: 66.41%] [G loss: 0.468198]\n",
      "epoch:10 step:9644 [D loss: 0.205417, acc.: 66.41%] [G loss: 0.453132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9645 [D loss: 0.239976, acc.: 62.50%] [G loss: 0.466240]\n",
      "epoch:10 step:9646 [D loss: 0.178653, acc.: 75.78%] [G loss: 0.508449]\n",
      "epoch:10 step:9647 [D loss: 0.271817, acc.: 50.78%] [G loss: 0.431691]\n",
      "epoch:10 step:9648 [D loss: 0.240410, acc.: 53.91%] [G loss: 0.451628]\n",
      "epoch:10 step:9649 [D loss: 0.208935, acc.: 72.66%] [G loss: 0.460815]\n",
      "epoch:10 step:9650 [D loss: 0.210268, acc.: 64.06%] [G loss: 0.538031]\n",
      "epoch:10 step:9651 [D loss: 0.248926, acc.: 57.03%] [G loss: 0.451207]\n",
      "epoch:10 step:9652 [D loss: 0.205662, acc.: 71.09%] [G loss: 0.432699]\n",
      "epoch:10 step:9653 [D loss: 0.196790, acc.: 69.53%] [G loss: 0.471623]\n",
      "epoch:10 step:9654 [D loss: 0.230404, acc.: 58.59%] [G loss: 0.429091]\n",
      "epoch:10 step:9655 [D loss: 0.221417, acc.: 63.28%] [G loss: 0.459088]\n",
      "epoch:10 step:9656 [D loss: 0.230182, acc.: 60.16%] [G loss: 0.484806]\n",
      "epoch:10 step:9657 [D loss: 0.230465, acc.: 59.38%] [G loss: 0.512248]\n",
      "epoch:10 step:9658 [D loss: 0.247495, acc.: 57.03%] [G loss: 0.466788]\n",
      "epoch:10 step:9659 [D loss: 0.212497, acc.: 70.31%] [G loss: 0.474144]\n",
      "epoch:10 step:9660 [D loss: 0.210197, acc.: 64.06%] [G loss: 0.472229]\n",
      "epoch:10 step:9661 [D loss: 0.274559, acc.: 53.12%] [G loss: 0.408899]\n",
      "epoch:10 step:9662 [D loss: 0.216816, acc.: 64.06%] [G loss: 0.435778]\n",
      "epoch:10 step:9663 [D loss: 0.238125, acc.: 58.59%] [G loss: 0.460317]\n",
      "epoch:10 step:9664 [D loss: 0.232714, acc.: 59.38%] [G loss: 0.455977]\n",
      "epoch:10 step:9665 [D loss: 0.216707, acc.: 66.41%] [G loss: 0.430954]\n",
      "epoch:10 step:9666 [D loss: 0.196547, acc.: 70.31%] [G loss: 0.443875]\n",
      "epoch:10 step:9667 [D loss: 0.226902, acc.: 60.94%] [G loss: 0.462503]\n",
      "epoch:10 step:9668 [D loss: 0.188721, acc.: 76.56%] [G loss: 0.484155]\n",
      "epoch:10 step:9669 [D loss: 0.207758, acc.: 69.53%] [G loss: 0.478979]\n",
      "epoch:10 step:9670 [D loss: 0.203775, acc.: 71.88%] [G loss: 0.476280]\n",
      "epoch:10 step:9671 [D loss: 0.279366, acc.: 50.00%] [G loss: 0.441604]\n",
      "epoch:10 step:9672 [D loss: 0.210994, acc.: 66.41%] [G loss: 0.443755]\n",
      "epoch:10 step:9673 [D loss: 0.211010, acc.: 68.75%] [G loss: 0.451312]\n",
      "epoch:10 step:9674 [D loss: 0.216569, acc.: 64.84%] [G loss: 0.451493]\n",
      "epoch:10 step:9675 [D loss: 0.216671, acc.: 61.72%] [G loss: 0.507533]\n",
      "epoch:10 step:9676 [D loss: 0.240465, acc.: 63.28%] [G loss: 0.497166]\n",
      "epoch:10 step:9677 [D loss: 0.237000, acc.: 59.38%] [G loss: 0.499870]\n",
      "epoch:10 step:9678 [D loss: 0.241079, acc.: 58.59%] [G loss: 0.464740]\n",
      "epoch:10 step:9679 [D loss: 0.192786, acc.: 73.44%] [G loss: 0.482133]\n",
      "epoch:10 step:9680 [D loss: 0.198988, acc.: 71.09%] [G loss: 0.483747]\n",
      "epoch:10 step:9681 [D loss: 0.210330, acc.: 65.62%] [G loss: 0.478678]\n",
      "epoch:10 step:9682 [D loss: 0.155094, acc.: 80.47%] [G loss: 0.511955]\n",
      "epoch:10 step:9683 [D loss: 0.190845, acc.: 73.44%] [G loss: 0.503411]\n",
      "epoch:10 step:9684 [D loss: 0.200583, acc.: 68.75%] [G loss: 0.530771]\n",
      "epoch:10 step:9685 [D loss: 0.183254, acc.: 73.44%] [G loss: 0.579792]\n",
      "epoch:10 step:9686 [D loss: 0.280673, acc.: 55.47%] [G loss: 0.429525]\n",
      "epoch:10 step:9687 [D loss: 0.247477, acc.: 50.78%] [G loss: 0.433498]\n",
      "epoch:10 step:9688 [D loss: 0.196949, acc.: 68.75%] [G loss: 0.474927]\n",
      "epoch:10 step:9689 [D loss: 0.195325, acc.: 71.88%] [G loss: 0.465660]\n",
      "epoch:10 step:9690 [D loss: 0.194640, acc.: 67.97%] [G loss: 0.464437]\n",
      "epoch:10 step:9691 [D loss: 0.173371, acc.: 72.66%] [G loss: 0.459255]\n",
      "epoch:10 step:9692 [D loss: 0.227727, acc.: 60.94%] [G loss: 0.530646]\n",
      "epoch:10 step:9693 [D loss: 0.257801, acc.: 54.69%] [G loss: 0.460589]\n",
      "epoch:10 step:9694 [D loss: 0.227517, acc.: 64.84%] [G loss: 0.458246]\n",
      "epoch:10 step:9695 [D loss: 0.225149, acc.: 62.50%] [G loss: 0.471804]\n",
      "epoch:10 step:9696 [D loss: 0.202965, acc.: 69.53%] [G loss: 0.496492]\n",
      "epoch:10 step:9697 [D loss: 0.230941, acc.: 64.84%] [G loss: 0.474446]\n",
      "epoch:10 step:9698 [D loss: 0.203137, acc.: 67.97%] [G loss: 0.472562]\n",
      "epoch:10 step:9699 [D loss: 0.244463, acc.: 57.03%] [G loss: 0.469739]\n",
      "epoch:10 step:9700 [D loss: 0.210803, acc.: 68.75%] [G loss: 0.499171]\n",
      "epoch:10 step:9701 [D loss: 0.218921, acc.: 67.19%] [G loss: 0.456154]\n",
      "epoch:10 step:9702 [D loss: 0.190175, acc.: 76.56%] [G loss: 0.465247]\n",
      "epoch:10 step:9703 [D loss: 0.207217, acc.: 64.06%] [G loss: 0.468115]\n",
      "epoch:10 step:9704 [D loss: 0.194679, acc.: 68.75%] [G loss: 0.464794]\n",
      "epoch:10 step:9705 [D loss: 0.207228, acc.: 68.75%] [G loss: 0.486518]\n",
      "epoch:10 step:9706 [D loss: 0.196802, acc.: 73.44%] [G loss: 0.492128]\n",
      "epoch:10 step:9707 [D loss: 0.222578, acc.: 67.19%] [G loss: 0.478902]\n",
      "epoch:10 step:9708 [D loss: 0.220971, acc.: 64.06%] [G loss: 0.509047]\n",
      "epoch:10 step:9709 [D loss: 0.226365, acc.: 64.06%] [G loss: 0.504228]\n",
      "epoch:10 step:9710 [D loss: 0.231540, acc.: 58.59%] [G loss: 0.490113]\n",
      "epoch:10 step:9711 [D loss: 0.264977, acc.: 53.12%] [G loss: 0.474693]\n",
      "epoch:10 step:9712 [D loss: 0.250296, acc.: 56.25%] [G loss: 0.444019]\n",
      "epoch:10 step:9713 [D loss: 0.201149, acc.: 70.31%] [G loss: 0.511655]\n",
      "epoch:10 step:9714 [D loss: 0.233372, acc.: 63.28%] [G loss: 0.519062]\n",
      "epoch:10 step:9715 [D loss: 0.177276, acc.: 75.00%] [G loss: 0.489080]\n",
      "epoch:10 step:9716 [D loss: 0.150184, acc.: 74.22%] [G loss: 0.523683]\n",
      "epoch:10 step:9717 [D loss: 0.179426, acc.: 76.56%] [G loss: 0.517992]\n",
      "epoch:10 step:9718 [D loss: 0.295238, acc.: 53.91%] [G loss: 0.428901]\n",
      "epoch:10 step:9719 [D loss: 0.262229, acc.: 50.78%] [G loss: 0.462758]\n",
      "epoch:10 step:9720 [D loss: 0.233981, acc.: 60.94%] [G loss: 0.419717]\n",
      "epoch:10 step:9721 [D loss: 0.207870, acc.: 67.97%] [G loss: 0.498027]\n",
      "epoch:10 step:9722 [D loss: 0.237116, acc.: 61.72%] [G loss: 0.478050]\n",
      "epoch:10 step:9723 [D loss: 0.221915, acc.: 63.28%] [G loss: 0.527840]\n",
      "epoch:10 step:9724 [D loss: 0.170863, acc.: 75.00%] [G loss: 0.494626]\n",
      "epoch:10 step:9725 [D loss: 0.225193, acc.: 67.19%] [G loss: 0.500192]\n",
      "epoch:10 step:9726 [D loss: 0.252915, acc.: 62.50%] [G loss: 0.436643]\n",
      "epoch:10 step:9727 [D loss: 0.232465, acc.: 64.84%] [G loss: 0.489060]\n",
      "epoch:10 step:9728 [D loss: 0.187837, acc.: 71.09%] [G loss: 0.490174]\n",
      "epoch:10 step:9729 [D loss: 0.212805, acc.: 66.41%] [G loss: 0.484895]\n",
      "epoch:10 step:9730 [D loss: 0.209107, acc.: 67.97%] [G loss: 0.481005]\n",
      "epoch:10 step:9731 [D loss: 0.205998, acc.: 66.41%] [G loss: 0.470021]\n",
      "epoch:10 step:9732 [D loss: 0.257288, acc.: 57.81%] [G loss: 0.449855]\n",
      "epoch:10 step:9733 [D loss: 0.221961, acc.: 64.84%] [G loss: 0.449828]\n",
      "epoch:10 step:9734 [D loss: 0.208733, acc.: 71.88%] [G loss: 0.467581]\n",
      "epoch:10 step:9735 [D loss: 0.212256, acc.: 63.28%] [G loss: 0.484771]\n",
      "epoch:10 step:9736 [D loss: 0.180600, acc.: 75.00%] [G loss: 0.488921]\n",
      "epoch:10 step:9737 [D loss: 0.194398, acc.: 69.53%] [G loss: 0.451510]\n",
      "epoch:10 step:9738 [D loss: 0.224509, acc.: 61.72%] [G loss: 0.423514]\n",
      "epoch:10 step:9739 [D loss: 0.234939, acc.: 59.38%] [G loss: 0.467960]\n",
      "epoch:10 step:9740 [D loss: 0.212825, acc.: 65.62%] [G loss: 0.454586]\n",
      "epoch:10 step:9741 [D loss: 0.176452, acc.: 72.66%] [G loss: 0.491765]\n",
      "epoch:10 step:9742 [D loss: 0.260819, acc.: 57.81%] [G loss: 0.457636]\n",
      "epoch:10 step:9743 [D loss: 0.255606, acc.: 53.91%] [G loss: 0.446041]\n",
      "epoch:10 step:9744 [D loss: 0.224830, acc.: 65.62%] [G loss: 0.464922]\n",
      "epoch:10 step:9745 [D loss: 0.223300, acc.: 63.28%] [G loss: 0.457805]\n",
      "epoch:10 step:9746 [D loss: 0.287175, acc.: 46.09%] [G loss: 0.358724]\n",
      "epoch:10 step:9747 [D loss: 0.228506, acc.: 64.84%] [G loss: 0.440770]\n",
      "epoch:10 step:9748 [D loss: 0.212720, acc.: 67.19%] [G loss: 0.420671]\n",
      "epoch:10 step:9749 [D loss: 0.229556, acc.: 59.38%] [G loss: 0.450787]\n",
      "epoch:10 step:9750 [D loss: 0.240437, acc.: 60.94%] [G loss: 0.441716]\n",
      "epoch:10 step:9751 [D loss: 0.188838, acc.: 71.09%] [G loss: 0.489592]\n",
      "epoch:10 step:9752 [D loss: 0.214915, acc.: 65.62%] [G loss: 0.452387]\n",
      "epoch:10 step:9753 [D loss: 0.210351, acc.: 66.41%] [G loss: 0.480898]\n",
      "epoch:10 step:9754 [D loss: 0.226632, acc.: 63.28%] [G loss: 0.438873]\n",
      "epoch:10 step:9755 [D loss: 0.222442, acc.: 63.28%] [G loss: 0.469224]\n",
      "epoch:10 step:9756 [D loss: 0.248042, acc.: 57.03%] [G loss: 0.468012]\n",
      "epoch:10 step:9757 [D loss: 0.228176, acc.: 62.50%] [G loss: 0.470443]\n",
      "epoch:10 step:9758 [D loss: 0.222975, acc.: 64.06%] [G loss: 0.499165]\n",
      "epoch:10 step:9759 [D loss: 0.219856, acc.: 63.28%] [G loss: 0.486794]\n",
      "epoch:10 step:9760 [D loss: 0.228630, acc.: 64.06%] [G loss: 0.426471]\n",
      "epoch:10 step:9761 [D loss: 0.205260, acc.: 66.41%] [G loss: 0.458180]\n",
      "epoch:10 step:9762 [D loss: 0.252528, acc.: 57.81%] [G loss: 0.445258]\n",
      "epoch:10 step:9763 [D loss: 0.244438, acc.: 62.50%] [G loss: 0.495001]\n",
      "epoch:10 step:9764 [D loss: 0.250037, acc.: 63.28%] [G loss: 0.446140]\n",
      "epoch:10 step:9765 [D loss: 0.186232, acc.: 71.88%] [G loss: 0.489210]\n",
      "epoch:10 step:9766 [D loss: 0.252534, acc.: 50.78%] [G loss: 0.531712]\n",
      "epoch:10 step:9767 [D loss: 0.216302, acc.: 66.41%] [G loss: 0.495337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9768 [D loss: 0.163753, acc.: 78.91%] [G loss: 0.559990]\n",
      "epoch:10 step:9769 [D loss: 0.212485, acc.: 67.97%] [G loss: 0.539055]\n",
      "epoch:10 step:9770 [D loss: 0.253621, acc.: 58.59%] [G loss: 0.484377]\n",
      "epoch:10 step:9771 [D loss: 0.264198, acc.: 53.91%] [G loss: 0.444282]\n",
      "epoch:10 step:9772 [D loss: 0.206185, acc.: 68.75%] [G loss: 0.424360]\n",
      "epoch:10 step:9773 [D loss: 0.228914, acc.: 62.50%] [G loss: 0.431274]\n",
      "epoch:10 step:9774 [D loss: 0.235034, acc.: 60.16%] [G loss: 0.492808]\n",
      "epoch:10 step:9775 [D loss: 0.194190, acc.: 71.09%] [G loss: 0.475559]\n",
      "epoch:10 step:9776 [D loss: 0.202507, acc.: 68.75%] [G loss: 0.505432]\n",
      "epoch:10 step:9777 [D loss: 0.234261, acc.: 58.59%] [G loss: 0.475431]\n",
      "epoch:10 step:9778 [D loss: 0.237305, acc.: 63.28%] [G loss: 0.464808]\n",
      "epoch:10 step:9779 [D loss: 0.211835, acc.: 63.28%] [G loss: 0.490031]\n",
      "epoch:10 step:9780 [D loss: 0.248530, acc.: 61.72%] [G loss: 0.400569]\n",
      "epoch:10 step:9781 [D loss: 0.208929, acc.: 66.41%] [G loss: 0.408204]\n",
      "epoch:10 step:9782 [D loss: 0.228904, acc.: 62.50%] [G loss: 0.419169]\n",
      "epoch:10 step:9783 [D loss: 0.239346, acc.: 56.25%] [G loss: 0.427174]\n",
      "epoch:10 step:9784 [D loss: 0.209179, acc.: 67.97%] [G loss: 0.452701]\n",
      "epoch:10 step:9785 [D loss: 0.223944, acc.: 60.94%] [G loss: 0.467713]\n",
      "epoch:10 step:9786 [D loss: 0.221658, acc.: 64.06%] [G loss: 0.519572]\n",
      "epoch:10 step:9787 [D loss: 0.239133, acc.: 57.81%] [G loss: 0.474941]\n",
      "epoch:10 step:9788 [D loss: 0.255520, acc.: 57.03%] [G loss: 0.482170]\n",
      "epoch:10 step:9789 [D loss: 0.240680, acc.: 60.16%] [G loss: 0.457903]\n",
      "epoch:10 step:9790 [D loss: 0.214705, acc.: 64.84%] [G loss: 0.463949]\n",
      "epoch:10 step:9791 [D loss: 0.238161, acc.: 61.72%] [G loss: 0.422137]\n",
      "epoch:10 step:9792 [D loss: 0.239025, acc.: 64.84%] [G loss: 0.451116]\n",
      "epoch:10 step:9793 [D loss: 0.230729, acc.: 64.06%] [G loss: 0.463020]\n",
      "epoch:10 step:9794 [D loss: 0.244578, acc.: 53.91%] [G loss: 0.417777]\n",
      "epoch:10 step:9795 [D loss: 0.197498, acc.: 69.53%] [G loss: 0.462742]\n",
      "epoch:10 step:9796 [D loss: 0.221048, acc.: 64.84%] [G loss: 0.455785]\n",
      "epoch:10 step:9797 [D loss: 0.188752, acc.: 71.88%] [G loss: 0.499775]\n",
      "epoch:10 step:9798 [D loss: 0.171494, acc.: 72.66%] [G loss: 0.528169]\n",
      "epoch:10 step:9799 [D loss: 0.226512, acc.: 61.72%] [G loss: 0.518624]\n",
      "epoch:10 step:9800 [D loss: 0.213850, acc.: 64.06%] [G loss: 0.495149]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.655683\n",
      "FID: 9.747541\n",
      "0 = 11.931673083233798\n",
      "1 = 0.05230880253895211\n",
      "2 = 0.8981500267982483\n",
      "3 = 0.8744999766349792\n",
      "4 = 0.9218000173568726\n",
      "5 = 0.9179174900054932\n",
      "6 = 0.8744999766349792\n",
      "7 = 6.312336259418698\n",
      "8 = 0.06909999192233666\n",
      "9 = 0.7098000049591064\n",
      "10 = 0.7062000036239624\n",
      "11 = 0.7134000062942505\n",
      "12 = 0.7113215327262878\n",
      "13 = 0.7062000036239624\n",
      "14 = 7.6557512283325195\n",
      "15 = 9.52828311920166\n",
      "16 = 0.11580953747034073\n",
      "17 = 7.655682563781738\n",
      "18 = 9.747541427612305\n",
      "epoch:10 step:9801 [D loss: 0.219542, acc.: 65.62%] [G loss: 0.486816]\n",
      "epoch:10 step:9802 [D loss: 0.246130, acc.: 58.59%] [G loss: 0.432569]\n",
      "epoch:10 step:9803 [D loss: 0.254463, acc.: 57.81%] [G loss: 0.459576]\n",
      "epoch:10 step:9804 [D loss: 0.208967, acc.: 65.62%] [G loss: 0.461692]\n",
      "epoch:10 step:9805 [D loss: 0.223566, acc.: 60.16%] [G loss: 0.467560]\n",
      "epoch:10 step:9806 [D loss: 0.203228, acc.: 67.97%] [G loss: 0.518168]\n",
      "epoch:10 step:9807 [D loss: 0.273053, acc.: 53.91%] [G loss: 0.455292]\n",
      "epoch:10 step:9808 [D loss: 0.221525, acc.: 61.72%] [G loss: 0.408358]\n",
      "epoch:10 step:9809 [D loss: 0.217969, acc.: 71.09%] [G loss: 0.458149]\n",
      "epoch:10 step:9810 [D loss: 0.231090, acc.: 60.94%] [G loss: 0.433168]\n",
      "epoch:10 step:9811 [D loss: 0.222626, acc.: 66.41%] [G loss: 0.500335]\n",
      "epoch:10 step:9812 [D loss: 0.232307, acc.: 61.72%] [G loss: 0.506765]\n",
      "epoch:10 step:9813 [D loss: 0.247108, acc.: 60.94%] [G loss: 0.437634]\n",
      "epoch:10 step:9814 [D loss: 0.191726, acc.: 74.22%] [G loss: 0.482887]\n",
      "epoch:10 step:9815 [D loss: 0.216636, acc.: 61.72%] [G loss: 0.469972]\n",
      "epoch:10 step:9816 [D loss: 0.226517, acc.: 64.06%] [G loss: 0.455242]\n",
      "epoch:10 step:9817 [D loss: 0.208804, acc.: 66.41%] [G loss: 0.513742]\n",
      "epoch:10 step:9818 [D loss: 0.255094, acc.: 63.28%] [G loss: 0.455076]\n",
      "epoch:10 step:9819 [D loss: 0.207027, acc.: 68.75%] [G loss: 0.454342]\n",
      "epoch:10 step:9820 [D loss: 0.220926, acc.: 63.28%] [G loss: 0.436181]\n",
      "epoch:10 step:9821 [D loss: 0.180592, acc.: 78.12%] [G loss: 0.478616]\n",
      "epoch:10 step:9822 [D loss: 0.227840, acc.: 66.41%] [G loss: 0.454105]\n",
      "epoch:10 step:9823 [D loss: 0.217064, acc.: 60.94%] [G loss: 0.509875]\n",
      "epoch:10 step:9824 [D loss: 0.208832, acc.: 68.75%] [G loss: 0.511032]\n",
      "epoch:10 step:9825 [D loss: 0.241669, acc.: 57.03%] [G loss: 0.496083]\n",
      "epoch:10 step:9826 [D loss: 0.226517, acc.: 66.41%] [G loss: 0.433081]\n",
      "epoch:10 step:9827 [D loss: 0.179336, acc.: 77.34%] [G loss: 0.499595]\n",
      "epoch:10 step:9828 [D loss: 0.261473, acc.: 54.69%] [G loss: 0.424056]\n",
      "epoch:10 step:9829 [D loss: 0.230377, acc.: 60.94%] [G loss: 0.470214]\n",
      "epoch:10 step:9830 [D loss: 0.244523, acc.: 55.47%] [G loss: 0.424850]\n",
      "epoch:10 step:9831 [D loss: 0.207062, acc.: 68.75%] [G loss: 0.456033]\n",
      "epoch:10 step:9832 [D loss: 0.264774, acc.: 53.12%] [G loss: 0.444274]\n",
      "epoch:10 step:9833 [D loss: 0.244982, acc.: 58.59%] [G loss: 0.425741]\n",
      "epoch:10 step:9834 [D loss: 0.226172, acc.: 69.53%] [G loss: 0.478670]\n",
      "epoch:10 step:9835 [D loss: 0.228056, acc.: 61.72%] [G loss: 0.451267]\n",
      "epoch:10 step:9836 [D loss: 0.229649, acc.: 60.16%] [G loss: 0.424750]\n",
      "epoch:10 step:9837 [D loss: 0.218311, acc.: 63.28%] [G loss: 0.474865]\n",
      "epoch:10 step:9838 [D loss: 0.234717, acc.: 64.06%] [G loss: 0.451984]\n",
      "epoch:10 step:9839 [D loss: 0.201723, acc.: 69.53%] [G loss: 0.479090]\n",
      "epoch:10 step:9840 [D loss: 0.210478, acc.: 68.75%] [G loss: 0.463858]\n",
      "epoch:10 step:9841 [D loss: 0.203388, acc.: 71.88%] [G loss: 0.518068]\n",
      "epoch:10 step:9842 [D loss: 0.202790, acc.: 67.19%] [G loss: 0.531388]\n",
      "epoch:10 step:9843 [D loss: 0.270968, acc.: 51.56%] [G loss: 0.430596]\n",
      "epoch:10 step:9844 [D loss: 0.201822, acc.: 68.75%] [G loss: 0.481142]\n",
      "epoch:10 step:9845 [D loss: 0.202910, acc.: 68.75%] [G loss: 0.500377]\n",
      "epoch:10 step:9846 [D loss: 0.247411, acc.: 60.94%] [G loss: 0.496774]\n",
      "epoch:10 step:9847 [D loss: 0.276851, acc.: 50.00%] [G loss: 0.469638]\n",
      "epoch:10 step:9848 [D loss: 0.242812, acc.: 57.81%] [G loss: 0.423577]\n",
      "epoch:10 step:9849 [D loss: 0.217995, acc.: 63.28%] [G loss: 0.427018]\n",
      "epoch:10 step:9850 [D loss: 0.228738, acc.: 59.38%] [G loss: 0.423780]\n",
      "epoch:10 step:9851 [D loss: 0.203754, acc.: 69.53%] [G loss: 0.487314]\n",
      "epoch:10 step:9852 [D loss: 0.278518, acc.: 48.44%] [G loss: 0.437169]\n",
      "epoch:10 step:9853 [D loss: 0.265985, acc.: 54.69%] [G loss: 0.432689]\n",
      "epoch:10 step:9854 [D loss: 0.208897, acc.: 67.19%] [G loss: 0.441490]\n",
      "epoch:10 step:9855 [D loss: 0.211605, acc.: 69.53%] [G loss: 0.501752]\n",
      "epoch:10 step:9856 [D loss: 0.242218, acc.: 58.59%] [G loss: 0.465345]\n",
      "epoch:10 step:9857 [D loss: 0.212245, acc.: 65.62%] [G loss: 0.472551]\n",
      "epoch:10 step:9858 [D loss: 0.211362, acc.: 66.41%] [G loss: 0.454670]\n",
      "epoch:10 step:9859 [D loss: 0.246321, acc.: 60.94%] [G loss: 0.462570]\n",
      "epoch:10 step:9860 [D loss: 0.234794, acc.: 60.94%] [G loss: 0.447976]\n",
      "epoch:10 step:9861 [D loss: 0.204093, acc.: 67.19%] [G loss: 0.479754]\n",
      "epoch:10 step:9862 [D loss: 0.246328, acc.: 54.69%] [G loss: 0.435002]\n",
      "epoch:10 step:9863 [D loss: 0.239315, acc.: 60.16%] [G loss: 0.422287]\n",
      "epoch:10 step:9864 [D loss: 0.218013, acc.: 62.50%] [G loss: 0.468232]\n",
      "epoch:10 step:9865 [D loss: 0.184114, acc.: 72.66%] [G loss: 0.500238]\n",
      "epoch:10 step:9866 [D loss: 0.234180, acc.: 60.16%] [G loss: 0.467041]\n",
      "epoch:10 step:9867 [D loss: 0.220175, acc.: 60.94%] [G loss: 0.466877]\n",
      "epoch:10 step:9868 [D loss: 0.207003, acc.: 65.62%] [G loss: 0.501959]\n",
      "epoch:10 step:9869 [D loss: 0.175880, acc.: 76.56%] [G loss: 0.547740]\n",
      "epoch:10 step:9870 [D loss: 0.275777, acc.: 60.16%] [G loss: 0.488600]\n",
      "epoch:10 step:9871 [D loss: 0.281807, acc.: 46.88%] [G loss: 0.460675]\n",
      "epoch:10 step:9872 [D loss: 0.229132, acc.: 63.28%] [G loss: 0.434637]\n",
      "epoch:10 step:9873 [D loss: 0.199541, acc.: 63.28%] [G loss: 0.443853]\n",
      "epoch:10 step:9874 [D loss: 0.217867, acc.: 64.06%] [G loss: 0.443425]\n",
      "epoch:10 step:9875 [D loss: 0.213048, acc.: 67.97%] [G loss: 0.498325]\n",
      "epoch:10 step:9876 [D loss: 0.220824, acc.: 61.72%] [G loss: 0.513305]\n",
      "epoch:10 step:9877 [D loss: 0.218382, acc.: 64.84%] [G loss: 0.466849]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9878 [D loss: 0.177033, acc.: 71.09%] [G loss: 0.480322]\n",
      "epoch:10 step:9879 [D loss: 0.230517, acc.: 59.38%] [G loss: 0.489183]\n",
      "epoch:10 step:9880 [D loss: 0.219353, acc.: 63.28%] [G loss: 0.462224]\n",
      "epoch:10 step:9881 [D loss: 0.243593, acc.: 57.81%] [G loss: 0.440794]\n",
      "epoch:10 step:9882 [D loss: 0.207626, acc.: 64.84%] [G loss: 0.425782]\n",
      "epoch:10 step:9883 [D loss: 0.207012, acc.: 67.97%] [G loss: 0.446790]\n",
      "epoch:10 step:9884 [D loss: 0.193029, acc.: 72.66%] [G loss: 0.482827]\n",
      "epoch:10 step:9885 [D loss: 0.161633, acc.: 82.81%] [G loss: 0.532009]\n",
      "epoch:10 step:9886 [D loss: 0.197523, acc.: 72.66%] [G loss: 0.524793]\n",
      "epoch:10 step:9887 [D loss: 0.253490, acc.: 52.34%] [G loss: 0.489357]\n",
      "epoch:10 step:9888 [D loss: 0.256292, acc.: 58.59%] [G loss: 0.458667]\n",
      "epoch:10 step:9889 [D loss: 0.240414, acc.: 59.38%] [G loss: 0.441420]\n",
      "epoch:10 step:9890 [D loss: 0.198323, acc.: 68.75%] [G loss: 0.517947]\n",
      "epoch:10 step:9891 [D loss: 0.221326, acc.: 63.28%] [G loss: 0.474636]\n",
      "epoch:10 step:9892 [D loss: 0.208368, acc.: 66.41%] [G loss: 0.494638]\n",
      "epoch:10 step:9893 [D loss: 0.213467, acc.: 61.72%] [G loss: 0.446122]\n",
      "epoch:10 step:9894 [D loss: 0.225483, acc.: 62.50%] [G loss: 0.489474]\n",
      "epoch:10 step:9895 [D loss: 0.232045, acc.: 58.59%] [G loss: 0.458908]\n",
      "epoch:10 step:9896 [D loss: 0.199103, acc.: 72.66%] [G loss: 0.488685]\n",
      "epoch:10 step:9897 [D loss: 0.233326, acc.: 60.94%] [G loss: 0.472379]\n",
      "epoch:10 step:9898 [D loss: 0.260680, acc.: 52.34%] [G loss: 0.452887]\n",
      "epoch:10 step:9899 [D loss: 0.236582, acc.: 58.59%] [G loss: 0.495143]\n",
      "epoch:10 step:9900 [D loss: 0.237331, acc.: 63.28%] [G loss: 0.481752]\n",
      "epoch:10 step:9901 [D loss: 0.224499, acc.: 59.38%] [G loss: 0.472859]\n",
      "epoch:10 step:9902 [D loss: 0.190887, acc.: 70.31%] [G loss: 0.476432]\n",
      "epoch:10 step:9903 [D loss: 0.202154, acc.: 67.19%] [G loss: 0.484856]\n",
      "epoch:10 step:9904 [D loss: 0.192607, acc.: 68.75%] [G loss: 0.498760]\n",
      "epoch:10 step:9905 [D loss: 0.239924, acc.: 57.03%] [G loss: 0.456345]\n",
      "epoch:10 step:9906 [D loss: 0.200120, acc.: 71.88%] [G loss: 0.459789]\n",
      "epoch:10 step:9907 [D loss: 0.232885, acc.: 64.84%] [G loss: 0.472296]\n",
      "epoch:10 step:9908 [D loss: 0.233856, acc.: 60.94%] [G loss: 0.482864]\n",
      "epoch:10 step:9909 [D loss: 0.228877, acc.: 58.59%] [G loss: 0.464773]\n",
      "epoch:10 step:9910 [D loss: 0.218317, acc.: 58.59%] [G loss: 0.458596]\n",
      "epoch:10 step:9911 [D loss: 0.232443, acc.: 60.94%] [G loss: 0.451044]\n",
      "epoch:10 step:9912 [D loss: 0.246817, acc.: 56.25%] [G loss: 0.424820]\n",
      "epoch:10 step:9913 [D loss: 0.214821, acc.: 70.31%] [G loss: 0.425639]\n",
      "epoch:10 step:9914 [D loss: 0.231731, acc.: 58.59%] [G loss: 0.424347]\n",
      "epoch:10 step:9915 [D loss: 0.201923, acc.: 67.97%] [G loss: 0.473367]\n",
      "epoch:10 step:9916 [D loss: 0.232649, acc.: 60.16%] [G loss: 0.436099]\n",
      "epoch:10 step:9917 [D loss: 0.209331, acc.: 64.06%] [G loss: 0.506352]\n",
      "epoch:10 step:9918 [D loss: 0.221244, acc.: 66.41%] [G loss: 0.460628]\n",
      "epoch:10 step:9919 [D loss: 0.201082, acc.: 67.19%] [G loss: 0.456397]\n",
      "epoch:10 step:9920 [D loss: 0.198188, acc.: 69.53%] [G loss: 0.519417]\n",
      "epoch:10 step:9921 [D loss: 0.201948, acc.: 66.41%] [G loss: 0.508924]\n",
      "epoch:10 step:9922 [D loss: 0.211736, acc.: 67.97%] [G loss: 0.478542]\n",
      "epoch:10 step:9923 [D loss: 0.232062, acc.: 63.28%] [G loss: 0.461083]\n",
      "epoch:10 step:9924 [D loss: 0.187386, acc.: 71.88%] [G loss: 0.455605]\n",
      "epoch:10 step:9925 [D loss: 0.208415, acc.: 67.19%] [G loss: 0.496074]\n",
      "epoch:10 step:9926 [D loss: 0.223690, acc.: 65.62%] [G loss: 0.495190]\n",
      "epoch:10 step:9927 [D loss: 0.198906, acc.: 66.41%] [G loss: 0.517938]\n",
      "epoch:10 step:9928 [D loss: 0.194579, acc.: 71.88%] [G loss: 0.494424]\n",
      "epoch:10 step:9929 [D loss: 0.249770, acc.: 57.81%] [G loss: 0.448648]\n",
      "epoch:10 step:9930 [D loss: 0.216547, acc.: 66.41%] [G loss: 0.437917]\n",
      "epoch:10 step:9931 [D loss: 0.212289, acc.: 64.06%] [G loss: 0.511987]\n",
      "epoch:10 step:9932 [D loss: 0.226096, acc.: 67.97%] [G loss: 0.477156]\n",
      "epoch:10 step:9933 [D loss: 0.211491, acc.: 64.84%] [G loss: 0.495291]\n",
      "epoch:10 step:9934 [D loss: 0.192457, acc.: 70.31%] [G loss: 0.549275]\n",
      "epoch:10 step:9935 [D loss: 0.234134, acc.: 61.72%] [G loss: 0.493150]\n",
      "epoch:10 step:9936 [D loss: 0.262883, acc.: 58.59%] [G loss: 0.447883]\n",
      "epoch:10 step:9937 [D loss: 0.184598, acc.: 70.31%] [G loss: 0.526652]\n",
      "epoch:10 step:9938 [D loss: 0.210692, acc.: 67.97%] [G loss: 0.496801]\n",
      "epoch:10 step:9939 [D loss: 0.247612, acc.: 56.25%] [G loss: 0.455012]\n",
      "epoch:10 step:9940 [D loss: 0.244820, acc.: 57.03%] [G loss: 0.407523]\n",
      "epoch:10 step:9941 [D loss: 0.193304, acc.: 69.53%] [G loss: 0.486930]\n",
      "epoch:10 step:9942 [D loss: 0.222232, acc.: 64.84%] [G loss: 0.446508]\n",
      "epoch:10 step:9943 [D loss: 0.222913, acc.: 61.72%] [G loss: 0.436811]\n",
      "epoch:10 step:9944 [D loss: 0.170785, acc.: 69.53%] [G loss: 0.472512]\n",
      "epoch:10 step:9945 [D loss: 0.202618, acc.: 66.41%] [G loss: 0.511275]\n",
      "epoch:10 step:9946 [D loss: 0.245754, acc.: 59.38%] [G loss: 0.483231]\n",
      "epoch:10 step:9947 [D loss: 0.243454, acc.: 66.41%] [G loss: 0.423585]\n",
      "epoch:10 step:9948 [D loss: 0.235765, acc.: 64.06%] [G loss: 0.446887]\n",
      "epoch:10 step:9949 [D loss: 0.227512, acc.: 60.16%] [G loss: 0.446795]\n",
      "epoch:10 step:9950 [D loss: 0.232333, acc.: 60.16%] [G loss: 0.455433]\n",
      "epoch:10 step:9951 [D loss: 0.222274, acc.: 61.72%] [G loss: 0.459540]\n",
      "epoch:10 step:9952 [D loss: 0.209795, acc.: 64.84%] [G loss: 0.479485]\n",
      "epoch:10 step:9953 [D loss: 0.206526, acc.: 67.19%] [G loss: 0.491829]\n",
      "epoch:10 step:9954 [D loss: 0.240088, acc.: 55.47%] [G loss: 0.438497]\n",
      "epoch:10 step:9955 [D loss: 0.237320, acc.: 58.59%] [G loss: 0.469476]\n",
      "epoch:10 step:9956 [D loss: 0.265330, acc.: 58.59%] [G loss: 0.451356]\n",
      "epoch:10 step:9957 [D loss: 0.227031, acc.: 65.62%] [G loss: 0.449283]\n",
      "epoch:10 step:9958 [D loss: 0.202104, acc.: 63.28%] [G loss: 0.517733]\n",
      "epoch:10 step:9959 [D loss: 0.179588, acc.: 76.56%] [G loss: 0.511613]\n",
      "epoch:10 step:9960 [D loss: 0.286698, acc.: 50.00%] [G loss: 0.456641]\n",
      "epoch:10 step:9961 [D loss: 0.222831, acc.: 64.06%] [G loss: 0.441244]\n",
      "epoch:10 step:9962 [D loss: 0.206723, acc.: 64.06%] [G loss: 0.484514]\n",
      "epoch:10 step:9963 [D loss: 0.223837, acc.: 60.94%] [G loss: 0.438907]\n",
      "epoch:10 step:9964 [D loss: 0.237867, acc.: 60.94%] [G loss: 0.438215]\n",
      "epoch:10 step:9965 [D loss: 0.203109, acc.: 65.62%] [G loss: 0.490975]\n",
      "epoch:10 step:9966 [D loss: 0.234163, acc.: 65.62%] [G loss: 0.466954]\n",
      "epoch:10 step:9967 [D loss: 0.258128, acc.: 58.59%] [G loss: 0.434907]\n",
      "epoch:10 step:9968 [D loss: 0.221551, acc.: 64.84%] [G loss: 0.452491]\n",
      "epoch:10 step:9969 [D loss: 0.223565, acc.: 61.72%] [G loss: 0.472217]\n",
      "epoch:10 step:9970 [D loss: 0.240406, acc.: 58.59%] [G loss: 0.444349]\n",
      "epoch:10 step:9971 [D loss: 0.231777, acc.: 61.72%] [G loss: 0.412014]\n",
      "epoch:10 step:9972 [D loss: 0.240907, acc.: 60.94%] [G loss: 0.443974]\n",
      "epoch:10 step:9973 [D loss: 0.208505, acc.: 68.75%] [G loss: 0.482119]\n",
      "epoch:10 step:9974 [D loss: 0.246850, acc.: 55.47%] [G loss: 0.419454]\n",
      "epoch:10 step:9975 [D loss: 0.230931, acc.: 59.38%] [G loss: 0.432101]\n",
      "epoch:10 step:9976 [D loss: 0.218652, acc.: 58.59%] [G loss: 0.437088]\n",
      "epoch:10 step:9977 [D loss: 0.223980, acc.: 57.03%] [G loss: 0.472904]\n",
      "epoch:10 step:9978 [D loss: 0.240849, acc.: 56.25%] [G loss: 0.401347]\n",
      "epoch:10 step:9979 [D loss: 0.194034, acc.: 72.66%] [G loss: 0.453235]\n",
      "epoch:10 step:9980 [D loss: 0.212839, acc.: 66.41%] [G loss: 0.459106]\n",
      "epoch:10 step:9981 [D loss: 0.198682, acc.: 69.53%] [G loss: 0.490474]\n",
      "epoch:10 step:9982 [D loss: 0.223021, acc.: 63.28%] [G loss: 0.439002]\n",
      "epoch:10 step:9983 [D loss: 0.206654, acc.: 67.97%] [G loss: 0.471516]\n",
      "epoch:10 step:9984 [D loss: 0.245318, acc.: 53.91%] [G loss: 0.401483]\n",
      "epoch:10 step:9985 [D loss: 0.249786, acc.: 59.38%] [G loss: 0.452415]\n",
      "epoch:10 step:9986 [D loss: 0.219392, acc.: 67.97%] [G loss: 0.475083]\n",
      "epoch:10 step:9987 [D loss: 0.215067, acc.: 64.06%] [G loss: 0.441189]\n",
      "epoch:10 step:9988 [D loss: 0.218252, acc.: 65.62%] [G loss: 0.471917]\n",
      "epoch:10 step:9989 [D loss: 0.243594, acc.: 60.94%] [G loss: 0.441419]\n",
      "epoch:10 step:9990 [D loss: 0.196019, acc.: 68.75%] [G loss: 0.505765]\n",
      "epoch:10 step:9991 [D loss: 0.227543, acc.: 58.59%] [G loss: 0.472830]\n",
      "epoch:10 step:9992 [D loss: 0.217054, acc.: 60.94%] [G loss: 0.494403]\n",
      "epoch:10 step:9993 [D loss: 0.222556, acc.: 60.94%] [G loss: 0.447996]\n",
      "epoch:10 step:9994 [D loss: 0.196515, acc.: 71.09%] [G loss: 0.451320]\n",
      "epoch:10 step:9995 [D loss: 0.251403, acc.: 53.91%] [G loss: 0.442263]\n",
      "epoch:10 step:9996 [D loss: 0.243444, acc.: 57.03%] [G loss: 0.420673]\n",
      "epoch:10 step:9997 [D loss: 0.212342, acc.: 65.62%] [G loss: 0.460953]\n",
      "epoch:10 step:9998 [D loss: 0.217889, acc.: 60.94%] [G loss: 0.464897]\n",
      "epoch:10 step:9999 [D loss: 0.220139, acc.: 67.19%] [G loss: 0.417237]\n",
      "epoch:10 step:10000 [D loss: 0.211590, acc.: 67.97%] [G loss: 0.492472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.534550\n",
      "FID: 12.042650\n",
      "0 = 11.88433836514955\n",
      "1 = 0.05323721310389777\n",
      "2 = 0.8974499702453613\n",
      "3 = 0.8744000196456909\n",
      "4 = 0.9204999804496765\n",
      "5 = 0.9166579246520996\n",
      "6 = 0.8744000196456909\n",
      "7 = 6.651078641247728\n",
      "8 = 0.07619534644387509\n",
      "9 = 0.7195500135421753\n",
      "10 = 0.7200999855995178\n",
      "11 = 0.718999981880188\n",
      "12 = 0.7193087339401245\n",
      "13 = 0.7200999855995178\n",
      "14 = 7.534615993499756\n",
      "15 = 9.536858558654785\n",
      "16 = 0.11686006188392639\n",
      "17 = 7.534550189971924\n",
      "18 = 12.04265022277832\n",
      "epoch:10 step:10001 [D loss: 0.199821, acc.: 69.53%] [G loss: 0.468601]\n",
      "epoch:10 step:10002 [D loss: 0.207869, acc.: 67.19%] [G loss: 0.459468]\n",
      "epoch:10 step:10003 [D loss: 0.213322, acc.: 66.41%] [G loss: 0.476795]\n",
      "epoch:10 step:10004 [D loss: 0.193986, acc.: 73.44%] [G loss: 0.468741]\n",
      "epoch:10 step:10005 [D loss: 0.179296, acc.: 75.78%] [G loss: 0.502128]\n",
      "epoch:10 step:10006 [D loss: 0.238362, acc.: 63.28%] [G loss: 0.429102]\n",
      "epoch:10 step:10007 [D loss: 0.187344, acc.: 74.22%] [G loss: 0.498779]\n",
      "epoch:10 step:10008 [D loss: 0.202218, acc.: 64.06%] [G loss: 0.480483]\n",
      "epoch:10 step:10009 [D loss: 0.244418, acc.: 55.47%] [G loss: 0.417956]\n",
      "epoch:10 step:10010 [D loss: 0.211595, acc.: 68.75%] [G loss: 0.432080]\n",
      "epoch:10 step:10011 [D loss: 0.213455, acc.: 65.62%] [G loss: 0.462731]\n",
      "epoch:10 step:10012 [D loss: 0.179278, acc.: 73.44%] [G loss: 0.528090]\n",
      "epoch:10 step:10013 [D loss: 0.204879, acc.: 62.50%] [G loss: 0.508680]\n",
      "epoch:10 step:10014 [D loss: 0.243416, acc.: 57.03%] [G loss: 0.484060]\n",
      "epoch:10 step:10015 [D loss: 0.216325, acc.: 65.62%] [G loss: 0.440359]\n",
      "epoch:10 step:10016 [D loss: 0.240227, acc.: 57.81%] [G loss: 0.442553]\n",
      "epoch:10 step:10017 [D loss: 0.197048, acc.: 69.53%] [G loss: 0.538591]\n",
      "epoch:10 step:10018 [D loss: 0.186551, acc.: 73.44%] [G loss: 0.560222]\n",
      "epoch:10 step:10019 [D loss: 0.197208, acc.: 72.66%] [G loss: 0.523733]\n",
      "epoch:10 step:10020 [D loss: 0.238745, acc.: 64.06%] [G loss: 0.503089]\n",
      "epoch:10 step:10021 [D loss: 0.223127, acc.: 64.06%] [G loss: 0.465475]\n",
      "epoch:10 step:10022 [D loss: 0.245786, acc.: 62.50%] [G loss: 0.436171]\n",
      "epoch:10 step:10023 [D loss: 0.214526, acc.: 65.62%] [G loss: 0.439347]\n",
      "epoch:10 step:10024 [D loss: 0.239662, acc.: 63.28%] [G loss: 0.471097]\n",
      "epoch:10 step:10025 [D loss: 0.243370, acc.: 60.94%] [G loss: 0.473370]\n",
      "epoch:10 step:10026 [D loss: 0.194328, acc.: 71.09%] [G loss: 0.472337]\n",
      "epoch:10 step:10027 [D loss: 0.221557, acc.: 57.81%] [G loss: 0.466702]\n",
      "epoch:10 step:10028 [D loss: 0.261232, acc.: 52.34%] [G loss: 0.433229]\n",
      "epoch:10 step:10029 [D loss: 0.203935, acc.: 71.88%] [G loss: 0.490826]\n",
      "epoch:10 step:10030 [D loss: 0.199191, acc.: 67.19%] [G loss: 0.469742]\n",
      "epoch:10 step:10031 [D loss: 0.184146, acc.: 77.34%] [G loss: 0.483728]\n",
      "epoch:10 step:10032 [D loss: 0.211830, acc.: 65.62%] [G loss: 0.496828]\n",
      "epoch:10 step:10033 [D loss: 0.212444, acc.: 64.84%] [G loss: 0.445910]\n",
      "epoch:10 step:10034 [D loss: 0.221019, acc.: 65.62%] [G loss: 0.462027]\n",
      "epoch:10 step:10035 [D loss: 0.219764, acc.: 67.19%] [G loss: 0.480196]\n",
      "epoch:10 step:10036 [D loss: 0.222077, acc.: 64.06%] [G loss: 0.477671]\n",
      "epoch:10 step:10037 [D loss: 0.207551, acc.: 67.19%] [G loss: 0.474627]\n",
      "epoch:10 step:10038 [D loss: 0.223798, acc.: 63.28%] [G loss: 0.469438]\n",
      "epoch:10 step:10039 [D loss: 0.214019, acc.: 64.84%] [G loss: 0.461189]\n",
      "epoch:10 step:10040 [D loss: 0.237005, acc.: 60.16%] [G loss: 0.448932]\n",
      "epoch:10 step:10041 [D loss: 0.252992, acc.: 60.16%] [G loss: 0.448636]\n",
      "epoch:10 step:10042 [D loss: 0.267653, acc.: 56.25%] [G loss: 0.437612]\n",
      "epoch:10 step:10043 [D loss: 0.209558, acc.: 71.09%] [G loss: 0.486178]\n",
      "epoch:10 step:10044 [D loss: 0.228388, acc.: 65.62%] [G loss: 0.469477]\n",
      "epoch:10 step:10045 [D loss: 0.253465, acc.: 61.72%] [G loss: 0.466793]\n",
      "epoch:10 step:10046 [D loss: 0.226164, acc.: 62.50%] [G loss: 0.452794]\n",
      "epoch:10 step:10047 [D loss: 0.208535, acc.: 66.41%] [G loss: 0.450573]\n",
      "epoch:10 step:10048 [D loss: 0.222397, acc.: 63.28%] [G loss: 0.479577]\n",
      "epoch:10 step:10049 [D loss: 0.230826, acc.: 62.50%] [G loss: 0.449231]\n",
      "epoch:10 step:10050 [D loss: 0.229480, acc.: 62.50%] [G loss: 0.459556]\n",
      "epoch:10 step:10051 [D loss: 0.202960, acc.: 71.09%] [G loss: 0.464167]\n",
      "epoch:10 step:10052 [D loss: 0.242021, acc.: 56.25%] [G loss: 0.446770]\n",
      "epoch:10 step:10053 [D loss: 0.227729, acc.: 59.38%] [G loss: 0.465041]\n",
      "epoch:10 step:10054 [D loss: 0.227241, acc.: 61.72%] [G loss: 0.436264]\n",
      "epoch:10 step:10055 [D loss: 0.235521, acc.: 64.06%] [G loss: 0.438817]\n",
      "epoch:10 step:10056 [D loss: 0.210821, acc.: 70.31%] [G loss: 0.486498]\n",
      "epoch:10 step:10057 [D loss: 0.223905, acc.: 71.09%] [G loss: 0.427719]\n",
      "epoch:10 step:10058 [D loss: 0.214785, acc.: 66.41%] [G loss: 0.458988]\n",
      "epoch:10 step:10059 [D loss: 0.219771, acc.: 64.84%] [G loss: 0.501217]\n",
      "epoch:10 step:10060 [D loss: 0.186157, acc.: 72.66%] [G loss: 0.540116]\n",
      "epoch:10 step:10061 [D loss: 0.214631, acc.: 66.41%] [G loss: 0.491537]\n",
      "epoch:10 step:10062 [D loss: 0.211554, acc.: 67.97%] [G loss: 0.504073]\n",
      "epoch:10 step:10063 [D loss: 0.215006, acc.: 63.28%] [G loss: 0.476203]\n",
      "epoch:10 step:10064 [D loss: 0.202855, acc.: 65.62%] [G loss: 0.526633]\n",
      "epoch:10 step:10065 [D loss: 0.209962, acc.: 67.19%] [G loss: 0.498268]\n",
      "epoch:10 step:10066 [D loss: 0.213418, acc.: 61.72%] [G loss: 0.529225]\n",
      "epoch:10 step:10067 [D loss: 0.233396, acc.: 60.16%] [G loss: 0.449160]\n",
      "epoch:10 step:10068 [D loss: 0.217218, acc.: 65.62%] [G loss: 0.453993]\n",
      "epoch:10 step:10069 [D loss: 0.210360, acc.: 71.88%] [G loss: 0.458339]\n",
      "epoch:10 step:10070 [D loss: 0.221052, acc.: 66.41%] [G loss: 0.479425]\n",
      "epoch:10 step:10071 [D loss: 0.206003, acc.: 71.09%] [G loss: 0.493926]\n",
      "epoch:10 step:10072 [D loss: 0.241290, acc.: 61.72%] [G loss: 0.465369]\n",
      "epoch:10 step:10073 [D loss: 0.255334, acc.: 53.12%] [G loss: 0.405537]\n",
      "epoch:10 step:10074 [D loss: 0.244012, acc.: 61.72%] [G loss: 0.427854]\n",
      "epoch:10 step:10075 [D loss: 0.224985, acc.: 67.97%] [G loss: 0.442305]\n",
      "epoch:10 step:10076 [D loss: 0.216249, acc.: 66.41%] [G loss: 0.459946]\n",
      "epoch:10 step:10077 [D loss: 0.216935, acc.: 67.19%] [G loss: 0.463022]\n",
      "epoch:10 step:10078 [D loss: 0.193412, acc.: 71.09%] [G loss: 0.496808]\n",
      "epoch:10 step:10079 [D loss: 0.181192, acc.: 71.09%] [G loss: 0.526452]\n",
      "epoch:10 step:10080 [D loss: 0.282452, acc.: 54.69%] [G loss: 0.447317]\n",
      "epoch:10 step:10081 [D loss: 0.236601, acc.: 60.94%] [G loss: 0.471883]\n",
      "epoch:10 step:10082 [D loss: 0.218527, acc.: 64.06%] [G loss: 0.510226]\n",
      "epoch:10 step:10083 [D loss: 0.225206, acc.: 63.28%] [G loss: 0.462276]\n",
      "epoch:10 step:10084 [D loss: 0.194928, acc.: 73.44%] [G loss: 0.470697]\n",
      "epoch:10 step:10085 [D loss: 0.228185, acc.: 62.50%] [G loss: 0.468244]\n",
      "epoch:10 step:10086 [D loss: 0.249141, acc.: 55.47%] [G loss: 0.447254]\n",
      "epoch:10 step:10087 [D loss: 0.223730, acc.: 64.84%] [G loss: 0.454168]\n",
      "epoch:10 step:10088 [D loss: 0.225207, acc.: 64.06%] [G loss: 0.463823]\n",
      "epoch:10 step:10089 [D loss: 0.181504, acc.: 75.78%] [G loss: 0.498265]\n",
      "epoch:10 step:10090 [D loss: 0.219761, acc.: 60.94%] [G loss: 0.489346]\n",
      "epoch:10 step:10091 [D loss: 0.233312, acc.: 57.81%] [G loss: 0.448092]\n",
      "epoch:10 step:10092 [D loss: 0.235085, acc.: 61.72%] [G loss: 0.434463]\n",
      "epoch:10 step:10093 [D loss: 0.221776, acc.: 66.41%] [G loss: 0.471921]\n",
      "epoch:10 step:10094 [D loss: 0.219834, acc.: 64.84%] [G loss: 0.489725]\n",
      "epoch:10 step:10095 [D loss: 0.205015, acc.: 69.53%] [G loss: 0.453481]\n",
      "epoch:10 step:10096 [D loss: 0.218382, acc.: 61.72%] [G loss: 0.484060]\n",
      "epoch:10 step:10097 [D loss: 0.228969, acc.: 58.59%] [G loss: 0.456184]\n",
      "epoch:10 step:10098 [D loss: 0.225793, acc.: 63.28%] [G loss: 0.445638]\n",
      "epoch:10 step:10099 [D loss: 0.223794, acc.: 60.94%] [G loss: 0.434291]\n",
      "epoch:10 step:10100 [D loss: 0.184769, acc.: 71.88%] [G loss: 0.510566]\n",
      "epoch:10 step:10101 [D loss: 0.232523, acc.: 64.06%] [G loss: 0.477133]\n",
      "epoch:10 step:10102 [D loss: 0.221181, acc.: 67.19%] [G loss: 0.463651]\n",
      "epoch:10 step:10103 [D loss: 0.211585, acc.: 70.31%] [G loss: 0.514933]\n",
      "epoch:10 step:10104 [D loss: 0.252482, acc.: 60.16%] [G loss: 0.467862]\n",
      "epoch:10 step:10105 [D loss: 0.221231, acc.: 62.50%] [G loss: 0.433842]\n",
      "epoch:10 step:10106 [D loss: 0.222369, acc.: 61.72%] [G loss: 0.410960]\n",
      "epoch:10 step:10107 [D loss: 0.187797, acc.: 67.97%] [G loss: 0.472942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10108 [D loss: 0.227828, acc.: 62.50%] [G loss: 0.436528]\n",
      "epoch:10 step:10109 [D loss: 0.264205, acc.: 56.25%] [G loss: 0.463479]\n",
      "epoch:10 step:10110 [D loss: 0.252369, acc.: 56.25%] [G loss: 0.445668]\n",
      "epoch:10 step:10111 [D loss: 0.249074, acc.: 53.12%] [G loss: 0.471360]\n",
      "epoch:10 step:10112 [D loss: 0.216600, acc.: 61.72%] [G loss: 0.455100]\n",
      "epoch:10 step:10113 [D loss: 0.188322, acc.: 67.19%] [G loss: 0.470952]\n",
      "epoch:10 step:10114 [D loss: 0.246727, acc.: 56.25%] [G loss: 0.439709]\n",
      "epoch:10 step:10115 [D loss: 0.248869, acc.: 60.16%] [G loss: 0.486423]\n",
      "epoch:10 step:10116 [D loss: 0.225529, acc.: 67.19%] [G loss: 0.493400]\n",
      "epoch:10 step:10117 [D loss: 0.193715, acc.: 69.53%] [G loss: 0.488264]\n",
      "epoch:10 step:10118 [D loss: 0.257580, acc.: 57.81%] [G loss: 0.479044]\n",
      "epoch:10 step:10119 [D loss: 0.202809, acc.: 67.19%] [G loss: 0.479144]\n",
      "epoch:10 step:10120 [D loss: 0.197159, acc.: 71.09%] [G loss: 0.478870]\n",
      "epoch:10 step:10121 [D loss: 0.208646, acc.: 65.62%] [G loss: 0.471500]\n",
      "epoch:10 step:10122 [D loss: 0.218654, acc.: 67.97%] [G loss: 0.447837]\n",
      "epoch:10 step:10123 [D loss: 0.221623, acc.: 63.28%] [G loss: 0.492065]\n",
      "epoch:10 step:10124 [D loss: 0.200565, acc.: 70.31%] [G loss: 0.454791]\n",
      "epoch:10 step:10125 [D loss: 0.232252, acc.: 64.06%] [G loss: 0.463177]\n",
      "epoch:10 step:10126 [D loss: 0.234449, acc.: 60.94%] [G loss: 0.470083]\n",
      "epoch:10 step:10127 [D loss: 0.227831, acc.: 62.50%] [G loss: 0.435229]\n",
      "epoch:10 step:10128 [D loss: 0.216597, acc.: 68.75%] [G loss: 0.431340]\n",
      "epoch:10 step:10129 [D loss: 0.237222, acc.: 57.81%] [G loss: 0.437832]\n",
      "epoch:10 step:10130 [D loss: 0.247578, acc.: 53.12%] [G loss: 0.460476]\n",
      "epoch:10 step:10131 [D loss: 0.243977, acc.: 62.50%] [G loss: 0.469169]\n",
      "epoch:10 step:10132 [D loss: 0.220191, acc.: 64.06%] [G loss: 0.460806]\n",
      "epoch:10 step:10133 [D loss: 0.213421, acc.: 64.84%] [G loss: 0.446654]\n",
      "epoch:10 step:10134 [D loss: 0.230176, acc.: 62.50%] [G loss: 0.441572]\n",
      "epoch:10 step:10135 [D loss: 0.279370, acc.: 52.34%] [G loss: 0.510120]\n",
      "epoch:10 step:10136 [D loss: 0.238142, acc.: 58.59%] [G loss: 0.460555]\n",
      "epoch:10 step:10137 [D loss: 0.223617, acc.: 67.97%] [G loss: 0.452880]\n",
      "epoch:10 step:10138 [D loss: 0.228973, acc.: 62.50%] [G loss: 0.442560]\n",
      "epoch:10 step:10139 [D loss: 0.175055, acc.: 78.91%] [G loss: 0.504932]\n",
      "epoch:10 step:10140 [D loss: 0.212607, acc.: 67.19%] [G loss: 0.498825]\n",
      "epoch:10 step:10141 [D loss: 0.236823, acc.: 59.38%] [G loss: 0.516948]\n",
      "epoch:10 step:10142 [D loss: 0.234026, acc.: 60.94%] [G loss: 0.452221]\n",
      "epoch:10 step:10143 [D loss: 0.226286, acc.: 62.50%] [G loss: 0.441005]\n",
      "epoch:10 step:10144 [D loss: 0.221550, acc.: 67.19%] [G loss: 0.460340]\n",
      "epoch:10 step:10145 [D loss: 0.223015, acc.: 60.16%] [G loss: 0.455760]\n",
      "epoch:10 step:10146 [D loss: 0.229356, acc.: 60.16%] [G loss: 0.471424]\n",
      "epoch:10 step:10147 [D loss: 0.235956, acc.: 60.94%] [G loss: 0.467619]\n",
      "epoch:10 step:10148 [D loss: 0.236647, acc.: 58.59%] [G loss: 0.465845]\n",
      "epoch:10 step:10149 [D loss: 0.209510, acc.: 72.66%] [G loss: 0.481939]\n",
      "epoch:10 step:10150 [D loss: 0.219596, acc.: 67.19%] [G loss: 0.425029]\n",
      "epoch:10 step:10151 [D loss: 0.203738, acc.: 66.41%] [G loss: 0.483756]\n",
      "epoch:10 step:10152 [D loss: 0.213005, acc.: 68.75%] [G loss: 0.509262]\n",
      "epoch:10 step:10153 [D loss: 0.231802, acc.: 61.72%] [G loss: 0.523041]\n",
      "epoch:10 step:10154 [D loss: 0.273205, acc.: 50.00%] [G loss: 0.432208]\n",
      "epoch:10 step:10155 [D loss: 0.226129, acc.: 64.84%] [G loss: 0.410455]\n",
      "epoch:10 step:10156 [D loss: 0.199344, acc.: 69.53%] [G loss: 0.483804]\n",
      "epoch:10 step:10157 [D loss: 0.245617, acc.: 60.16%] [G loss: 0.446579]\n",
      "epoch:10 step:10158 [D loss: 0.249046, acc.: 57.03%] [G loss: 0.451336]\n",
      "epoch:10 step:10159 [D loss: 0.204697, acc.: 70.31%] [G loss: 0.420160]\n",
      "epoch:10 step:10160 [D loss: 0.208221, acc.: 65.62%] [G loss: 0.478308]\n",
      "epoch:10 step:10161 [D loss: 0.226511, acc.: 61.72%] [G loss: 0.446122]\n",
      "epoch:10 step:10162 [D loss: 0.220278, acc.: 66.41%] [G loss: 0.452212]\n",
      "epoch:10 step:10163 [D loss: 0.220629, acc.: 66.41%] [G loss: 0.508487]\n",
      "epoch:10 step:10164 [D loss: 0.237005, acc.: 60.94%] [G loss: 0.483912]\n",
      "epoch:10 step:10165 [D loss: 0.243858, acc.: 59.38%] [G loss: 0.468312]\n",
      "epoch:10 step:10166 [D loss: 0.210584, acc.: 64.84%] [G loss: 0.449858]\n",
      "epoch:10 step:10167 [D loss: 0.239371, acc.: 60.94%] [G loss: 0.473841]\n",
      "epoch:10 step:10168 [D loss: 0.239195, acc.: 65.62%] [G loss: 0.463888]\n",
      "epoch:10 step:10169 [D loss: 0.207872, acc.: 64.84%] [G loss: 0.459246]\n",
      "epoch:10 step:10170 [D loss: 0.234541, acc.: 57.81%] [G loss: 0.459251]\n",
      "epoch:10 step:10171 [D loss: 0.184335, acc.: 73.44%] [G loss: 0.492520]\n",
      "epoch:10 step:10172 [D loss: 0.197042, acc.: 65.62%] [G loss: 0.523257]\n",
      "epoch:10 step:10173 [D loss: 0.216516, acc.: 63.28%] [G loss: 0.506257]\n",
      "epoch:10 step:10174 [D loss: 0.201122, acc.: 66.41%] [G loss: 0.476263]\n",
      "epoch:10 step:10175 [D loss: 0.235329, acc.: 64.06%] [G loss: 0.431670]\n",
      "epoch:10 step:10176 [D loss: 0.236853, acc.: 64.84%] [G loss: 0.458291]\n",
      "epoch:10 step:10177 [D loss: 0.191909, acc.: 74.22%] [G loss: 0.441727]\n",
      "epoch:10 step:10178 [D loss: 0.209562, acc.: 68.75%] [G loss: 0.439970]\n",
      "epoch:10 step:10179 [D loss: 0.227613, acc.: 62.50%] [G loss: 0.458330]\n",
      "epoch:10 step:10180 [D loss: 0.230692, acc.: 65.62%] [G loss: 0.446539]\n",
      "epoch:10 step:10181 [D loss: 0.248730, acc.: 53.91%] [G loss: 0.440106]\n",
      "epoch:10 step:10182 [D loss: 0.231477, acc.: 60.16%] [G loss: 0.481088]\n",
      "epoch:10 step:10183 [D loss: 0.246336, acc.: 60.94%] [G loss: 0.437351]\n",
      "epoch:10 step:10184 [D loss: 0.214731, acc.: 67.19%] [G loss: 0.469128]\n",
      "epoch:10 step:10185 [D loss: 0.201768, acc.: 67.97%] [G loss: 0.516998]\n",
      "epoch:10 step:10186 [D loss: 0.227175, acc.: 62.50%] [G loss: 0.473150]\n",
      "epoch:10 step:10187 [D loss: 0.226749, acc.: 69.53%] [G loss: 0.440908]\n",
      "epoch:10 step:10188 [D loss: 0.235160, acc.: 58.59%] [G loss: 0.464281]\n",
      "epoch:10 step:10189 [D loss: 0.211719, acc.: 66.41%] [G loss: 0.446762]\n",
      "epoch:10 step:10190 [D loss: 0.275903, acc.: 52.34%] [G loss: 0.447746]\n",
      "epoch:10 step:10191 [D loss: 0.225253, acc.: 63.28%] [G loss: 0.439333]\n",
      "epoch:10 step:10192 [D loss: 0.249328, acc.: 55.47%] [G loss: 0.404747]\n",
      "epoch:10 step:10193 [D loss: 0.198550, acc.: 67.97%] [G loss: 0.495091]\n",
      "epoch:10 step:10194 [D loss: 0.254482, acc.: 53.12%] [G loss: 0.454843]\n",
      "epoch:10 step:10195 [D loss: 0.199588, acc.: 64.84%] [G loss: 0.473852]\n",
      "epoch:10 step:10196 [D loss: 0.211947, acc.: 63.28%] [G loss: 0.450959]\n",
      "epoch:10 step:10197 [D loss: 0.252237, acc.: 52.34%] [G loss: 0.436968]\n",
      "epoch:10 step:10198 [D loss: 0.241159, acc.: 55.47%] [G loss: 0.442768]\n",
      "epoch:10 step:10199 [D loss: 0.216009, acc.: 62.50%] [G loss: 0.475612]\n",
      "epoch:10 step:10200 [D loss: 0.205796, acc.: 63.28%] [G loss: 0.488628]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.574794\n",
      "FID: 14.147326\n",
      "0 = 11.909522492337219\n",
      "1 = 0.05149573425107296\n",
      "2 = 0.9000499844551086\n",
      "3 = 0.8791999816894531\n",
      "4 = 0.9208999872207642\n",
      "5 = 0.9174579977989197\n",
      "6 = 0.8791999816894531\n",
      "7 = 6.73321741024257\n",
      "8 = 0.08526207184549144\n",
      "9 = 0.7238500118255615\n",
      "10 = 0.7177000045776367\n",
      "11 = 0.7300000190734863\n",
      "12 = 0.7266376614570618\n",
      "13 = 0.7177000045776367\n",
      "14 = 7.57485818862915\n",
      "15 = 9.553030014038086\n",
      "16 = 0.11159311980009079\n",
      "17 = 7.574793815612793\n",
      "18 = 14.14732551574707\n",
      "epoch:10 step:10201 [D loss: 0.227417, acc.: 62.50%] [G loss: 0.471460]\n",
      "epoch:10 step:10202 [D loss: 0.232730, acc.: 62.50%] [G loss: 0.442055]\n",
      "epoch:10 step:10203 [D loss: 0.232816, acc.: 64.84%] [G loss: 0.443124]\n",
      "epoch:10 step:10204 [D loss: 0.213674, acc.: 60.16%] [G loss: 0.456195]\n",
      "epoch:10 step:10205 [D loss: 0.214174, acc.: 64.84%] [G loss: 0.451494]\n",
      "epoch:10 step:10206 [D loss: 0.212076, acc.: 64.06%] [G loss: 0.440914]\n",
      "epoch:10 step:10207 [D loss: 0.179672, acc.: 75.78%] [G loss: 0.493922]\n",
      "epoch:10 step:10208 [D loss: 0.226135, acc.: 67.19%] [G loss: 0.483020]\n",
      "epoch:10 step:10209 [D loss: 0.225152, acc.: 62.50%] [G loss: 0.437588]\n",
      "epoch:10 step:10210 [D loss: 0.243531, acc.: 57.03%] [G loss: 0.453520]\n",
      "epoch:10 step:10211 [D loss: 0.216585, acc.: 65.62%] [G loss: 0.444820]\n",
      "epoch:10 step:10212 [D loss: 0.194503, acc.: 71.09%] [G loss: 0.479427]\n",
      "epoch:10 step:10213 [D loss: 0.234721, acc.: 57.81%] [G loss: 0.443058]\n",
      "epoch:10 step:10214 [D loss: 0.219577, acc.: 67.19%] [G loss: 0.487018]\n",
      "epoch:10 step:10215 [D loss: 0.225939, acc.: 60.94%] [G loss: 0.456839]\n",
      "epoch:10 step:10216 [D loss: 0.224256, acc.: 60.94%] [G loss: 0.434388]\n",
      "epoch:10 step:10217 [D loss: 0.223296, acc.: 57.81%] [G loss: 0.427522]\n",
      "epoch:10 step:10218 [D loss: 0.226395, acc.: 61.72%] [G loss: 0.417214]\n",
      "epoch:10 step:10219 [D loss: 0.202434, acc.: 71.88%] [G loss: 0.448040]\n",
      "epoch:10 step:10220 [D loss: 0.227783, acc.: 57.81%] [G loss: 0.451303]\n",
      "epoch:10 step:10221 [D loss: 0.226288, acc.: 61.72%] [G loss: 0.425560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10222 [D loss: 0.221949, acc.: 66.41%] [G loss: 0.452579]\n",
      "epoch:10 step:10223 [D loss: 0.185263, acc.: 72.66%] [G loss: 0.512396]\n",
      "epoch:10 step:10224 [D loss: 0.233850, acc.: 67.19%] [G loss: 0.490946]\n",
      "epoch:10 step:10225 [D loss: 0.217099, acc.: 64.84%] [G loss: 0.481239]\n",
      "epoch:10 step:10226 [D loss: 0.242514, acc.: 60.16%] [G loss: 0.425937]\n",
      "epoch:10 step:10227 [D loss: 0.210199, acc.: 68.75%] [G loss: 0.422174]\n",
      "epoch:10 step:10228 [D loss: 0.286796, acc.: 44.53%] [G loss: 0.416260]\n",
      "epoch:10 step:10229 [D loss: 0.248449, acc.: 53.12%] [G loss: 0.450564]\n",
      "epoch:10 step:10230 [D loss: 0.191679, acc.: 72.66%] [G loss: 0.481157]\n",
      "epoch:10 step:10231 [D loss: 0.248974, acc.: 53.12%] [G loss: 0.404909]\n",
      "epoch:10 step:10232 [D loss: 0.224657, acc.: 66.41%] [G loss: 0.454257]\n",
      "epoch:10 step:10233 [D loss: 0.230514, acc.: 57.81%] [G loss: 0.457656]\n",
      "epoch:10 step:10234 [D loss: 0.239365, acc.: 60.16%] [G loss: 0.458581]\n",
      "epoch:10 step:10235 [D loss: 0.234704, acc.: 63.28%] [G loss: 0.418981]\n",
      "epoch:10 step:10236 [D loss: 0.232846, acc.: 63.28%] [G loss: 0.440223]\n",
      "epoch:10 step:10237 [D loss: 0.222332, acc.: 62.50%] [G loss: 0.458259]\n",
      "epoch:10 step:10238 [D loss: 0.226133, acc.: 62.50%] [G loss: 0.463208]\n",
      "epoch:10 step:10239 [D loss: 0.219726, acc.: 63.28%] [G loss: 0.466757]\n",
      "epoch:10 step:10240 [D loss: 0.226374, acc.: 62.50%] [G loss: 0.452418]\n",
      "epoch:10 step:10241 [D loss: 0.205962, acc.: 67.19%] [G loss: 0.481928]\n",
      "epoch:10 step:10242 [D loss: 0.215299, acc.: 67.19%] [G loss: 0.476122]\n",
      "epoch:10 step:10243 [D loss: 0.222715, acc.: 67.19%] [G loss: 0.452309]\n",
      "epoch:10 step:10244 [D loss: 0.220950, acc.: 65.62%] [G loss: 0.477749]\n",
      "epoch:10 step:10245 [D loss: 0.197852, acc.: 70.31%] [G loss: 0.483266]\n",
      "epoch:10 step:10246 [D loss: 0.210843, acc.: 66.41%] [G loss: 0.472480]\n",
      "epoch:10 step:10247 [D loss: 0.222383, acc.: 62.50%] [G loss: 0.445157]\n",
      "epoch:10 step:10248 [D loss: 0.243611, acc.: 59.38%] [G loss: 0.458784]\n",
      "epoch:10 step:10249 [D loss: 0.245520, acc.: 62.50%] [G loss: 0.399014]\n",
      "epoch:10 step:10250 [D loss: 0.216197, acc.: 65.62%] [G loss: 0.483600]\n",
      "epoch:10 step:10251 [D loss: 0.232183, acc.: 60.16%] [G loss: 0.454552]\n",
      "epoch:10 step:10252 [D loss: 0.211277, acc.: 62.50%] [G loss: 0.457532]\n",
      "epoch:10 step:10253 [D loss: 0.247372, acc.: 57.81%] [G loss: 0.457060]\n",
      "epoch:10 step:10254 [D loss: 0.204965, acc.: 66.41%] [G loss: 0.509482]\n",
      "epoch:10 step:10255 [D loss: 0.224719, acc.: 64.84%] [G loss: 0.490314]\n",
      "epoch:10 step:10256 [D loss: 0.220952, acc.: 68.75%] [G loss: 0.523065]\n",
      "epoch:10 step:10257 [D loss: 0.245896, acc.: 57.81%] [G loss: 0.494887]\n",
      "epoch:10 step:10258 [D loss: 0.207101, acc.: 67.19%] [G loss: 0.454632]\n",
      "epoch:10 step:10259 [D loss: 0.214351, acc.: 62.50%] [G loss: 0.468804]\n",
      "epoch:10 step:10260 [D loss: 0.211554, acc.: 66.41%] [G loss: 0.519861]\n",
      "epoch:10 step:10261 [D loss: 0.276609, acc.: 51.56%] [G loss: 0.447116]\n",
      "epoch:10 step:10262 [D loss: 0.262523, acc.: 52.34%] [G loss: 0.437554]\n",
      "epoch:10 step:10263 [D loss: 0.218109, acc.: 64.06%] [G loss: 0.424677]\n",
      "epoch:10 step:10264 [D loss: 0.188376, acc.: 71.88%] [G loss: 0.505509]\n",
      "epoch:10 step:10265 [D loss: 0.205743, acc.: 63.28%] [G loss: 0.503417]\n",
      "epoch:10 step:10266 [D loss: 0.237608, acc.: 61.72%] [G loss: 0.472782]\n",
      "epoch:10 step:10267 [D loss: 0.177560, acc.: 75.00%] [G loss: 0.505425]\n",
      "epoch:10 step:10268 [D loss: 0.225191, acc.: 60.94%] [G loss: 0.452889]\n",
      "epoch:10 step:10269 [D loss: 0.184531, acc.: 71.88%] [G loss: 0.525241]\n",
      "epoch:10 step:10270 [D loss: 0.199080, acc.: 71.09%] [G loss: 0.513753]\n",
      "epoch:10 step:10271 [D loss: 0.211664, acc.: 63.28%] [G loss: 0.452986]\n",
      "epoch:10 step:10272 [D loss: 0.226483, acc.: 61.72%] [G loss: 0.491954]\n",
      "epoch:10 step:10273 [D loss: 0.226597, acc.: 64.84%] [G loss: 0.452857]\n",
      "epoch:10 step:10274 [D loss: 0.226635, acc.: 61.72%] [G loss: 0.467324]\n",
      "epoch:10 step:10275 [D loss: 0.207366, acc.: 70.31%] [G loss: 0.487021]\n",
      "epoch:10 step:10276 [D loss: 0.183257, acc.: 72.66%] [G loss: 0.473921]\n",
      "epoch:10 step:10277 [D loss: 0.221126, acc.: 67.19%] [G loss: 0.469884]\n",
      "epoch:10 step:10278 [D loss: 0.207406, acc.: 69.53%] [G loss: 0.450485]\n",
      "epoch:10 step:10279 [D loss: 0.160078, acc.: 76.56%] [G loss: 0.489863]\n",
      "epoch:10 step:10280 [D loss: 0.241007, acc.: 61.72%] [G loss: 0.455379]\n",
      "epoch:10 step:10281 [D loss: 0.196446, acc.: 72.66%] [G loss: 0.492531]\n",
      "epoch:10 step:10282 [D loss: 0.189792, acc.: 71.88%] [G loss: 0.478933]\n",
      "epoch:10 step:10283 [D loss: 0.218396, acc.: 68.75%] [G loss: 0.519349]\n",
      "epoch:10 step:10284 [D loss: 0.207225, acc.: 71.88%] [G loss: 0.521257]\n",
      "epoch:10 step:10285 [D loss: 0.228312, acc.: 64.84%] [G loss: 0.433496]\n",
      "epoch:10 step:10286 [D loss: 0.222924, acc.: 63.28%] [G loss: 0.436154]\n",
      "epoch:10 step:10287 [D loss: 0.260482, acc.: 57.81%] [G loss: 0.475993]\n",
      "epoch:10 step:10288 [D loss: 0.160263, acc.: 81.25%] [G loss: 0.540480]\n",
      "epoch:10 step:10289 [D loss: 0.189229, acc.: 71.09%] [G loss: 0.538986]\n",
      "epoch:10 step:10290 [D loss: 0.303192, acc.: 51.56%] [G loss: 0.486160]\n",
      "epoch:10 step:10291 [D loss: 0.201103, acc.: 65.62%] [G loss: 0.499046]\n",
      "epoch:10 step:10292 [D loss: 0.242792, acc.: 56.25%] [G loss: 0.468971]\n",
      "epoch:10 step:10293 [D loss: 0.196907, acc.: 68.75%] [G loss: 0.493947]\n",
      "epoch:10 step:10294 [D loss: 0.155528, acc.: 80.47%] [G loss: 0.540442]\n",
      "epoch:10 step:10295 [D loss: 0.154145, acc.: 81.25%] [G loss: 0.582021]\n",
      "epoch:10 step:10296 [D loss: 0.174359, acc.: 77.34%] [G loss: 0.571475]\n",
      "epoch:10 step:10297 [D loss: 0.196054, acc.: 72.66%] [G loss: 0.599539]\n",
      "epoch:10 step:10298 [D loss: 0.301207, acc.: 60.94%] [G loss: 0.551321]\n",
      "epoch:10 step:10299 [D loss: 0.208823, acc.: 68.75%] [G loss: 0.635622]\n",
      "epoch:10 step:10300 [D loss: 0.236933, acc.: 65.62%] [G loss: 0.534387]\n",
      "epoch:10 step:10301 [D loss: 0.245957, acc.: 56.25%] [G loss: 0.424695]\n",
      "epoch:10 step:10302 [D loss: 0.263800, acc.: 53.91%] [G loss: 0.394602]\n",
      "epoch:10 step:10303 [D loss: 0.210287, acc.: 73.44%] [G loss: 0.488506]\n",
      "epoch:10 step:10304 [D loss: 0.224327, acc.: 64.84%] [G loss: 0.454195]\n",
      "epoch:10 step:10305 [D loss: 0.187177, acc.: 70.31%] [G loss: 0.532620]\n",
      "epoch:10 step:10306 [D loss: 0.182197, acc.: 78.91%] [G loss: 0.540891]\n",
      "epoch:10 step:10307 [D loss: 0.151698, acc.: 79.69%] [G loss: 0.553771]\n",
      "epoch:11 step:10308 [D loss: 0.272823, acc.: 57.81%] [G loss: 0.486035]\n",
      "epoch:11 step:10309 [D loss: 0.279377, acc.: 57.81%] [G loss: 0.473103]\n",
      "epoch:11 step:10310 [D loss: 0.233741, acc.: 59.38%] [G loss: 0.501473]\n",
      "epoch:11 step:10311 [D loss: 0.209695, acc.: 65.62%] [G loss: 0.487074]\n",
      "epoch:11 step:10312 [D loss: 0.233690, acc.: 59.38%] [G loss: 0.483829]\n",
      "epoch:11 step:10313 [D loss: 0.203221, acc.: 68.75%] [G loss: 0.500716]\n",
      "epoch:11 step:10314 [D loss: 0.195857, acc.: 71.88%] [G loss: 0.490386]\n",
      "epoch:11 step:10315 [D loss: 0.206121, acc.: 64.84%] [G loss: 0.461866]\n",
      "epoch:11 step:10316 [D loss: 0.197298, acc.: 72.66%] [G loss: 0.512372]\n",
      "epoch:11 step:10317 [D loss: 0.235874, acc.: 66.41%] [G loss: 0.460406]\n",
      "epoch:11 step:10318 [D loss: 0.214372, acc.: 64.06%] [G loss: 0.541917]\n",
      "epoch:11 step:10319 [D loss: 0.220416, acc.: 66.41%] [G loss: 0.481116]\n",
      "epoch:11 step:10320 [D loss: 0.213298, acc.: 65.62%] [G loss: 0.519737]\n",
      "epoch:11 step:10321 [D loss: 0.223938, acc.: 64.06%] [G loss: 0.452735]\n",
      "epoch:11 step:10322 [D loss: 0.196946, acc.: 65.62%] [G loss: 0.511320]\n",
      "epoch:11 step:10323 [D loss: 0.187225, acc.: 72.66%] [G loss: 0.523638]\n",
      "epoch:11 step:10324 [D loss: 0.245716, acc.: 57.81%] [G loss: 0.467409]\n",
      "epoch:11 step:10325 [D loss: 0.249524, acc.: 59.38%] [G loss: 0.466252]\n",
      "epoch:11 step:10326 [D loss: 0.257854, acc.: 58.59%] [G loss: 0.496944]\n",
      "epoch:11 step:10327 [D loss: 0.247444, acc.: 58.59%] [G loss: 0.518879]\n",
      "epoch:11 step:10328 [D loss: 0.213094, acc.: 66.41%] [G loss: 0.510732]\n",
      "epoch:11 step:10329 [D loss: 0.213737, acc.: 67.19%] [G loss: 0.532709]\n",
      "epoch:11 step:10330 [D loss: 0.265615, acc.: 52.34%] [G loss: 0.510930]\n",
      "epoch:11 step:10331 [D loss: 0.258951, acc.: 54.69%] [G loss: 0.457221]\n",
      "epoch:11 step:10332 [D loss: 0.204858, acc.: 65.62%] [G loss: 0.479981]\n",
      "epoch:11 step:10333 [D loss: 0.231702, acc.: 60.94%] [G loss: 0.444044]\n",
      "epoch:11 step:10334 [D loss: 0.229365, acc.: 60.16%] [G loss: 0.441017]\n",
      "epoch:11 step:10335 [D loss: 0.210375, acc.: 72.66%] [G loss: 0.456058]\n",
      "epoch:11 step:10336 [D loss: 0.209805, acc.: 64.84%] [G loss: 0.500019]\n",
      "epoch:11 step:10337 [D loss: 0.238741, acc.: 59.38%] [G loss: 0.457622]\n",
      "epoch:11 step:10338 [D loss: 0.250081, acc.: 60.16%] [G loss: 0.476036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10339 [D loss: 0.237720, acc.: 53.91%] [G loss: 0.447669]\n",
      "epoch:11 step:10340 [D loss: 0.202021, acc.: 69.53%] [G loss: 0.440878]\n",
      "epoch:11 step:10341 [D loss: 0.251229, acc.: 55.47%] [G loss: 0.402039]\n",
      "epoch:11 step:10342 [D loss: 0.203951, acc.: 66.41%] [G loss: 0.482152]\n",
      "epoch:11 step:10343 [D loss: 0.217330, acc.: 64.06%] [G loss: 0.454178]\n",
      "epoch:11 step:10344 [D loss: 0.223403, acc.: 68.75%] [G loss: 0.438222]\n",
      "epoch:11 step:10345 [D loss: 0.242363, acc.: 53.91%] [G loss: 0.403819]\n",
      "epoch:11 step:10346 [D loss: 0.204633, acc.: 66.41%] [G loss: 0.461138]\n",
      "epoch:11 step:10347 [D loss: 0.177012, acc.: 75.78%] [G loss: 0.507172]\n",
      "epoch:11 step:10348 [D loss: 0.239777, acc.: 59.38%] [G loss: 0.421508]\n",
      "epoch:11 step:10349 [D loss: 0.235695, acc.: 59.38%] [G loss: 0.445094]\n",
      "epoch:11 step:10350 [D loss: 0.219419, acc.: 64.06%] [G loss: 0.449929]\n",
      "epoch:11 step:10351 [D loss: 0.245103, acc.: 57.03%] [G loss: 0.461255]\n",
      "epoch:11 step:10352 [D loss: 0.184833, acc.: 72.66%] [G loss: 0.493814]\n",
      "epoch:11 step:10353 [D loss: 0.210685, acc.: 67.97%] [G loss: 0.459787]\n",
      "epoch:11 step:10354 [D loss: 0.241416, acc.: 60.16%] [G loss: 0.471649]\n",
      "epoch:11 step:10355 [D loss: 0.194356, acc.: 71.88%] [G loss: 0.460262]\n",
      "epoch:11 step:10356 [D loss: 0.212441, acc.: 66.41%] [G loss: 0.488614]\n",
      "epoch:11 step:10357 [D loss: 0.213798, acc.: 64.84%] [G loss: 0.488706]\n",
      "epoch:11 step:10358 [D loss: 0.259271, acc.: 57.81%] [G loss: 0.478665]\n",
      "epoch:11 step:10359 [D loss: 0.243053, acc.: 57.03%] [G loss: 0.459295]\n",
      "epoch:11 step:10360 [D loss: 0.199223, acc.: 72.66%] [G loss: 0.483047]\n",
      "epoch:11 step:10361 [D loss: 0.227050, acc.: 60.94%] [G loss: 0.504712]\n",
      "epoch:11 step:10362 [D loss: 0.204226, acc.: 63.28%] [G loss: 0.431728]\n",
      "epoch:11 step:10363 [D loss: 0.221301, acc.: 63.28%] [G loss: 0.420766]\n",
      "epoch:11 step:10364 [D loss: 0.211232, acc.: 63.28%] [G loss: 0.421100]\n",
      "epoch:11 step:10365 [D loss: 0.210068, acc.: 69.53%] [G loss: 0.453424]\n",
      "epoch:11 step:10366 [D loss: 0.204978, acc.: 70.31%] [G loss: 0.476094]\n",
      "epoch:11 step:10367 [D loss: 0.222280, acc.: 63.28%] [G loss: 0.455192]\n",
      "epoch:11 step:10368 [D loss: 0.231226, acc.: 65.62%] [G loss: 0.455519]\n",
      "epoch:11 step:10369 [D loss: 0.231670, acc.: 66.41%] [G loss: 0.435718]\n",
      "epoch:11 step:10370 [D loss: 0.224338, acc.: 64.06%] [G loss: 0.447210]\n",
      "epoch:11 step:10371 [D loss: 0.248736, acc.: 59.38%] [G loss: 0.463547]\n",
      "epoch:11 step:10372 [D loss: 0.250700, acc.: 57.81%] [G loss: 0.432788]\n",
      "epoch:11 step:10373 [D loss: 0.220702, acc.: 64.06%] [G loss: 0.458651]\n",
      "epoch:11 step:10374 [D loss: 0.232900, acc.: 58.59%] [G loss: 0.408404]\n",
      "epoch:11 step:10375 [D loss: 0.234531, acc.: 58.59%] [G loss: 0.430519]\n",
      "epoch:11 step:10376 [D loss: 0.203313, acc.: 65.62%] [G loss: 0.456795]\n",
      "epoch:11 step:10377 [D loss: 0.201981, acc.: 66.41%] [G loss: 0.510816]\n",
      "epoch:11 step:10378 [D loss: 0.253018, acc.: 58.59%] [G loss: 0.419232]\n",
      "epoch:11 step:10379 [D loss: 0.222339, acc.: 65.62%] [G loss: 0.444291]\n",
      "epoch:11 step:10380 [D loss: 0.231712, acc.: 57.03%] [G loss: 0.452477]\n",
      "epoch:11 step:10381 [D loss: 0.212519, acc.: 68.75%] [G loss: 0.465604]\n",
      "epoch:11 step:10382 [D loss: 0.233760, acc.: 59.38%] [G loss: 0.497802]\n",
      "epoch:11 step:10383 [D loss: 0.162854, acc.: 76.56%] [G loss: 0.490727]\n",
      "epoch:11 step:10384 [D loss: 0.175837, acc.: 72.66%] [G loss: 0.496853]\n",
      "epoch:11 step:10385 [D loss: 0.310005, acc.: 46.09%] [G loss: 0.436807]\n",
      "epoch:11 step:10386 [D loss: 0.227036, acc.: 60.16%] [G loss: 0.448890]\n",
      "epoch:11 step:10387 [D loss: 0.225180, acc.: 62.50%] [G loss: 0.446259]\n",
      "epoch:11 step:10388 [D loss: 0.249845, acc.: 57.03%] [G loss: 0.487673]\n",
      "epoch:11 step:10389 [D loss: 0.238875, acc.: 57.03%] [G loss: 0.450324]\n",
      "epoch:11 step:10390 [D loss: 0.202497, acc.: 68.75%] [G loss: 0.511040]\n",
      "epoch:11 step:10391 [D loss: 0.210210, acc.: 69.53%] [G loss: 0.463306]\n",
      "epoch:11 step:10392 [D loss: 0.223833, acc.: 64.06%] [G loss: 0.442047]\n",
      "epoch:11 step:10393 [D loss: 0.211713, acc.: 66.41%] [G loss: 0.444363]\n",
      "epoch:11 step:10394 [D loss: 0.223891, acc.: 63.28%] [G loss: 0.444096]\n",
      "epoch:11 step:10395 [D loss: 0.215255, acc.: 67.19%] [G loss: 0.468516]\n",
      "epoch:11 step:10396 [D loss: 0.221463, acc.: 67.97%] [G loss: 0.459929]\n",
      "epoch:11 step:10397 [D loss: 0.224977, acc.: 61.72%] [G loss: 0.449558]\n",
      "epoch:11 step:10398 [D loss: 0.257173, acc.: 54.69%] [G loss: 0.464091]\n",
      "epoch:11 step:10399 [D loss: 0.190612, acc.: 73.44%] [G loss: 0.502084]\n",
      "epoch:11 step:10400 [D loss: 0.220433, acc.: 64.06%] [G loss: 0.462366]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.591406\n",
      "FID: 12.689172\n",
      "0 = 11.978370549654912\n",
      "1 = 0.0534472843604366\n",
      "2 = 0.9006500244140625\n",
      "3 = 0.8812000155448914\n",
      "4 = 0.9200999736785889\n",
      "5 = 0.9168660640716553\n",
      "6 = 0.8812000155448914\n",
      "7 = 6.5969362471282595\n",
      "8 = 0.07860721541038834\n",
      "9 = 0.7060499787330627\n",
      "10 = 0.705299973487854\n",
      "11 = 0.7067999839782715\n",
      "12 = 0.7063595652580261\n",
      "13 = 0.705299973487854\n",
      "14 = 7.59146785736084\n",
      "15 = 9.446297645568848\n",
      "16 = 0.13878478109836578\n",
      "17 = 7.591406345367432\n",
      "18 = 12.68917179107666\n",
      "epoch:11 step:10401 [D loss: 0.258162, acc.: 57.81%] [G loss: 0.465906]\n",
      "epoch:11 step:10402 [D loss: 0.226289, acc.: 67.19%] [G loss: 0.481478]\n",
      "epoch:11 step:10403 [D loss: 0.223188, acc.: 65.62%] [G loss: 0.488743]\n",
      "epoch:11 step:10404 [D loss: 0.208810, acc.: 67.19%] [G loss: 0.498357]\n",
      "epoch:11 step:10405 [D loss: 0.250912, acc.: 58.59%] [G loss: 0.467225]\n",
      "epoch:11 step:10406 [D loss: 0.233439, acc.: 56.25%] [G loss: 0.444582]\n",
      "epoch:11 step:10407 [D loss: 0.173822, acc.: 77.34%] [G loss: 0.429097]\n",
      "epoch:11 step:10408 [D loss: 0.217411, acc.: 61.72%] [G loss: 0.462822]\n",
      "epoch:11 step:10409 [D loss: 0.264341, acc.: 53.12%] [G loss: 0.456858]\n",
      "epoch:11 step:10410 [D loss: 0.222248, acc.: 62.50%] [G loss: 0.452011]\n",
      "epoch:11 step:10411 [D loss: 0.224242, acc.: 58.59%] [G loss: 0.455720]\n",
      "epoch:11 step:10412 [D loss: 0.228248, acc.: 62.50%] [G loss: 0.461642]\n",
      "epoch:11 step:10413 [D loss: 0.223281, acc.: 62.50%] [G loss: 0.429636]\n",
      "epoch:11 step:10414 [D loss: 0.208169, acc.: 67.97%] [G loss: 0.543262]\n",
      "epoch:11 step:10415 [D loss: 0.250541, acc.: 57.81%] [G loss: 0.528208]\n",
      "epoch:11 step:10416 [D loss: 0.269570, acc.: 53.91%] [G loss: 0.437306]\n",
      "epoch:11 step:10417 [D loss: 0.253085, acc.: 57.03%] [G loss: 0.424199]\n",
      "epoch:11 step:10418 [D loss: 0.190299, acc.: 69.53%] [G loss: 0.471694]\n",
      "epoch:11 step:10419 [D loss: 0.198920, acc.: 71.09%] [G loss: 0.449586]\n",
      "epoch:11 step:10420 [D loss: 0.220510, acc.: 66.41%] [G loss: 0.515215]\n",
      "epoch:11 step:10421 [D loss: 0.226902, acc.: 63.28%] [G loss: 0.458224]\n",
      "epoch:11 step:10422 [D loss: 0.206529, acc.: 68.75%] [G loss: 0.489991]\n",
      "epoch:11 step:10423 [D loss: 0.202897, acc.: 67.97%] [G loss: 0.463334]\n",
      "epoch:11 step:10424 [D loss: 0.209512, acc.: 67.19%] [G loss: 0.502870]\n",
      "epoch:11 step:10425 [D loss: 0.232612, acc.: 60.94%] [G loss: 0.491487]\n",
      "epoch:11 step:10426 [D loss: 0.210407, acc.: 71.09%] [G loss: 0.502448]\n",
      "epoch:11 step:10427 [D loss: 0.237122, acc.: 60.94%] [G loss: 0.494241]\n",
      "epoch:11 step:10428 [D loss: 0.226002, acc.: 61.72%] [G loss: 0.449095]\n",
      "epoch:11 step:10429 [D loss: 0.193851, acc.: 68.75%] [G loss: 0.476008]\n",
      "epoch:11 step:10430 [D loss: 0.193300, acc.: 70.31%] [G loss: 0.505812]\n",
      "epoch:11 step:10431 [D loss: 0.236359, acc.: 58.59%] [G loss: 0.479747]\n",
      "epoch:11 step:10432 [D loss: 0.231657, acc.: 62.50%] [G loss: 0.448999]\n",
      "epoch:11 step:10433 [D loss: 0.199610, acc.: 70.31%] [G loss: 0.453528]\n",
      "epoch:11 step:10434 [D loss: 0.227195, acc.: 62.50%] [G loss: 0.452077]\n",
      "epoch:11 step:10435 [D loss: 0.225223, acc.: 60.16%] [G loss: 0.433870]\n",
      "epoch:11 step:10436 [D loss: 0.222177, acc.: 63.28%] [G loss: 0.452641]\n",
      "epoch:11 step:10437 [D loss: 0.203157, acc.: 65.62%] [G loss: 0.450688]\n",
      "epoch:11 step:10438 [D loss: 0.232383, acc.: 62.50%] [G loss: 0.461140]\n",
      "epoch:11 step:10439 [D loss: 0.249196, acc.: 57.03%] [G loss: 0.485826]\n",
      "epoch:11 step:10440 [D loss: 0.237715, acc.: 57.03%] [G loss: 0.469545]\n",
      "epoch:11 step:10441 [D loss: 0.218343, acc.: 62.50%] [G loss: 0.471864]\n",
      "epoch:11 step:10442 [D loss: 0.197305, acc.: 69.53%] [G loss: 0.509869]\n",
      "epoch:11 step:10443 [D loss: 0.243576, acc.: 61.72%] [G loss: 0.447313]\n",
      "epoch:11 step:10444 [D loss: 0.262791, acc.: 53.12%] [G loss: 0.423322]\n",
      "epoch:11 step:10445 [D loss: 0.238551, acc.: 58.59%] [G loss: 0.438792]\n",
      "epoch:11 step:10446 [D loss: 0.225221, acc.: 61.72%] [G loss: 0.445696]\n",
      "epoch:11 step:10447 [D loss: 0.262328, acc.: 49.22%] [G loss: 0.420066]\n",
      "epoch:11 step:10448 [D loss: 0.233701, acc.: 62.50%] [G loss: 0.469816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10449 [D loss: 0.214860, acc.: 66.41%] [G loss: 0.462705]\n",
      "epoch:11 step:10450 [D loss: 0.238747, acc.: 51.56%] [G loss: 0.436179]\n",
      "epoch:11 step:10451 [D loss: 0.179763, acc.: 73.44%] [G loss: 0.518881]\n",
      "epoch:11 step:10452 [D loss: 0.251229, acc.: 53.91%] [G loss: 0.472468]\n",
      "epoch:11 step:10453 [D loss: 0.222498, acc.: 60.16%] [G loss: 0.452215]\n",
      "epoch:11 step:10454 [D loss: 0.250117, acc.: 57.03%] [G loss: 0.421713]\n",
      "epoch:11 step:10455 [D loss: 0.244646, acc.: 54.69%] [G loss: 0.478548]\n",
      "epoch:11 step:10456 [D loss: 0.232720, acc.: 58.59%] [G loss: 0.440712]\n",
      "epoch:11 step:10457 [D loss: 0.234844, acc.: 61.72%] [G loss: 0.421543]\n",
      "epoch:11 step:10458 [D loss: 0.221550, acc.: 67.97%] [G loss: 0.441445]\n",
      "epoch:11 step:10459 [D loss: 0.216445, acc.: 69.53%] [G loss: 0.450140]\n",
      "epoch:11 step:10460 [D loss: 0.237385, acc.: 57.81%] [G loss: 0.436931]\n",
      "epoch:11 step:10461 [D loss: 0.215040, acc.: 65.62%] [G loss: 0.450421]\n",
      "epoch:11 step:10462 [D loss: 0.197864, acc.: 67.97%] [G loss: 0.477072]\n",
      "epoch:11 step:10463 [D loss: 0.220105, acc.: 67.97%] [G loss: 0.475698]\n",
      "epoch:11 step:10464 [D loss: 0.225290, acc.: 65.62%] [G loss: 0.452890]\n",
      "epoch:11 step:10465 [D loss: 0.219840, acc.: 63.28%] [G loss: 0.444862]\n",
      "epoch:11 step:10466 [D loss: 0.216912, acc.: 69.53%] [G loss: 0.452042]\n",
      "epoch:11 step:10467 [D loss: 0.250436, acc.: 53.91%] [G loss: 0.443364]\n",
      "epoch:11 step:10468 [D loss: 0.204994, acc.: 63.28%] [G loss: 0.463098]\n",
      "epoch:11 step:10469 [D loss: 0.196504, acc.: 67.97%] [G loss: 0.506635]\n",
      "epoch:11 step:10470 [D loss: 0.215591, acc.: 63.28%] [G loss: 0.467965]\n",
      "epoch:11 step:10471 [D loss: 0.236506, acc.: 60.94%] [G loss: 0.464100]\n",
      "epoch:11 step:10472 [D loss: 0.200956, acc.: 71.88%] [G loss: 0.495873]\n",
      "epoch:11 step:10473 [D loss: 0.237664, acc.: 58.59%] [G loss: 0.421654]\n",
      "epoch:11 step:10474 [D loss: 0.217070, acc.: 67.97%] [G loss: 0.440288]\n",
      "epoch:11 step:10475 [D loss: 0.210234, acc.: 70.31%] [G loss: 0.466118]\n",
      "epoch:11 step:10476 [D loss: 0.212341, acc.: 68.75%] [G loss: 0.473389]\n",
      "epoch:11 step:10477 [D loss: 0.248233, acc.: 61.72%] [G loss: 0.402443]\n",
      "epoch:11 step:10478 [D loss: 0.194839, acc.: 72.66%] [G loss: 0.461530]\n",
      "epoch:11 step:10479 [D loss: 0.212784, acc.: 65.62%] [G loss: 0.474847]\n",
      "epoch:11 step:10480 [D loss: 0.220582, acc.: 63.28%] [G loss: 0.473967]\n",
      "epoch:11 step:10481 [D loss: 0.252353, acc.: 57.03%] [G loss: 0.454206]\n",
      "epoch:11 step:10482 [D loss: 0.254717, acc.: 57.03%] [G loss: 0.399925]\n",
      "epoch:11 step:10483 [D loss: 0.224781, acc.: 60.94%] [G loss: 0.425497]\n",
      "epoch:11 step:10484 [D loss: 0.196750, acc.: 69.53%] [G loss: 0.462564]\n",
      "epoch:11 step:10485 [D loss: 0.257125, acc.: 57.03%] [G loss: 0.408614]\n",
      "epoch:11 step:10486 [D loss: 0.242606, acc.: 58.59%] [G loss: 0.427023]\n",
      "epoch:11 step:10487 [D loss: 0.247512, acc.: 47.66%] [G loss: 0.416425]\n",
      "epoch:11 step:10488 [D loss: 0.238180, acc.: 62.50%] [G loss: 0.417414]\n",
      "epoch:11 step:10489 [D loss: 0.244394, acc.: 61.72%] [G loss: 0.440169]\n",
      "epoch:11 step:10490 [D loss: 0.241946, acc.: 53.91%] [G loss: 0.471987]\n",
      "epoch:11 step:10491 [D loss: 0.218690, acc.: 67.97%] [G loss: 0.447375]\n",
      "epoch:11 step:10492 [D loss: 0.227684, acc.: 66.41%] [G loss: 0.469337]\n",
      "epoch:11 step:10493 [D loss: 0.247143, acc.: 56.25%] [G loss: 0.459229]\n",
      "epoch:11 step:10494 [D loss: 0.240607, acc.: 57.81%] [G loss: 0.431395]\n",
      "epoch:11 step:10495 [D loss: 0.231536, acc.: 61.72%] [G loss: 0.437175]\n",
      "epoch:11 step:10496 [D loss: 0.259995, acc.: 50.00%] [G loss: 0.402944]\n",
      "epoch:11 step:10497 [D loss: 0.200594, acc.: 71.09%] [G loss: 0.448619]\n",
      "epoch:11 step:10498 [D loss: 0.219677, acc.: 63.28%] [G loss: 0.445911]\n",
      "epoch:11 step:10499 [D loss: 0.179037, acc.: 71.88%] [G loss: 0.480575]\n",
      "epoch:11 step:10500 [D loss: 0.223013, acc.: 60.16%] [G loss: 0.468518]\n",
      "epoch:11 step:10501 [D loss: 0.223345, acc.: 63.28%] [G loss: 0.458314]\n",
      "epoch:11 step:10502 [D loss: 0.214418, acc.: 70.31%] [G loss: 0.438722]\n",
      "epoch:11 step:10503 [D loss: 0.235079, acc.: 56.25%] [G loss: 0.421532]\n",
      "epoch:11 step:10504 [D loss: 0.243505, acc.: 57.81%] [G loss: 0.423041]\n",
      "epoch:11 step:10505 [D loss: 0.217747, acc.: 64.06%] [G loss: 0.460050]\n",
      "epoch:11 step:10506 [D loss: 0.217426, acc.: 64.84%] [G loss: 0.463663]\n",
      "epoch:11 step:10507 [D loss: 0.244994, acc.: 62.50%] [G loss: 0.444649]\n",
      "epoch:11 step:10508 [D loss: 0.235811, acc.: 64.06%] [G loss: 0.455647]\n",
      "epoch:11 step:10509 [D loss: 0.210514, acc.: 68.75%] [G loss: 0.470779]\n",
      "epoch:11 step:10510 [D loss: 0.242525, acc.: 57.81%] [G loss: 0.458441]\n",
      "epoch:11 step:10511 [D loss: 0.234478, acc.: 60.94%] [G loss: 0.440475]\n",
      "epoch:11 step:10512 [D loss: 0.225453, acc.: 65.62%] [G loss: 0.474495]\n",
      "epoch:11 step:10513 [D loss: 0.193160, acc.: 70.31%] [G loss: 0.484051]\n",
      "epoch:11 step:10514 [D loss: 0.196716, acc.: 73.44%] [G loss: 0.505726]\n",
      "epoch:11 step:10515 [D loss: 0.168084, acc.: 76.56%] [G loss: 0.526639]\n",
      "epoch:11 step:10516 [D loss: 0.194913, acc.: 67.97%] [G loss: 0.480201]\n",
      "epoch:11 step:10517 [D loss: 0.227230, acc.: 57.81%] [G loss: 0.514437]\n",
      "epoch:11 step:10518 [D loss: 0.244181, acc.: 57.81%] [G loss: 0.444566]\n",
      "epoch:11 step:10519 [D loss: 0.220436, acc.: 58.59%] [G loss: 0.458098]\n",
      "epoch:11 step:10520 [D loss: 0.245150, acc.: 58.59%] [G loss: 0.418603]\n",
      "epoch:11 step:10521 [D loss: 0.241367, acc.: 63.28%] [G loss: 0.421488]\n",
      "epoch:11 step:10522 [D loss: 0.235069, acc.: 57.81%] [G loss: 0.435165]\n",
      "epoch:11 step:10523 [D loss: 0.207232, acc.: 62.50%] [G loss: 0.473109]\n",
      "epoch:11 step:10524 [D loss: 0.223718, acc.: 65.62%] [G loss: 0.441840]\n",
      "epoch:11 step:10525 [D loss: 0.191615, acc.: 71.88%] [G loss: 0.476489]\n",
      "epoch:11 step:10526 [D loss: 0.197611, acc.: 70.31%] [G loss: 0.503610]\n",
      "epoch:11 step:10527 [D loss: 0.274089, acc.: 53.12%] [G loss: 0.444723]\n",
      "epoch:11 step:10528 [D loss: 0.187156, acc.: 71.88%] [G loss: 0.488829]\n",
      "epoch:11 step:10529 [D loss: 0.192918, acc.: 71.88%] [G loss: 0.487436]\n",
      "epoch:11 step:10530 [D loss: 0.208497, acc.: 68.75%] [G loss: 0.499998]\n",
      "epoch:11 step:10531 [D loss: 0.240753, acc.: 64.84%] [G loss: 0.438980]\n",
      "epoch:11 step:10532 [D loss: 0.242619, acc.: 60.16%] [G loss: 0.464225]\n",
      "epoch:11 step:10533 [D loss: 0.246906, acc.: 53.12%] [G loss: 0.440460]\n",
      "epoch:11 step:10534 [D loss: 0.212040, acc.: 72.66%] [G loss: 0.439797]\n",
      "epoch:11 step:10535 [D loss: 0.230597, acc.: 64.84%] [G loss: 0.426861]\n",
      "epoch:11 step:10536 [D loss: 0.218633, acc.: 66.41%] [G loss: 0.456154]\n",
      "epoch:11 step:10537 [D loss: 0.196278, acc.: 69.53%] [G loss: 0.478290]\n",
      "epoch:11 step:10538 [D loss: 0.189437, acc.: 69.53%] [G loss: 0.561890]\n",
      "epoch:11 step:10539 [D loss: 0.175371, acc.: 76.56%] [G loss: 0.583223]\n",
      "epoch:11 step:10540 [D loss: 0.252594, acc.: 57.81%] [G loss: 0.431479]\n",
      "epoch:11 step:10541 [D loss: 0.278793, acc.: 50.78%] [G loss: 0.421720]\n",
      "epoch:11 step:10542 [D loss: 0.231426, acc.: 57.81%] [G loss: 0.449786]\n",
      "epoch:11 step:10543 [D loss: 0.218976, acc.: 67.19%] [G loss: 0.427576]\n",
      "epoch:11 step:10544 [D loss: 0.242905, acc.: 56.25%] [G loss: 0.439320]\n",
      "epoch:11 step:10545 [D loss: 0.202541, acc.: 71.09%] [G loss: 0.435550]\n",
      "epoch:11 step:10546 [D loss: 0.215340, acc.: 64.84%] [G loss: 0.426792]\n",
      "epoch:11 step:10547 [D loss: 0.212737, acc.: 66.41%] [G loss: 0.444823]\n",
      "epoch:11 step:10548 [D loss: 0.203176, acc.: 73.44%] [G loss: 0.503705]\n",
      "epoch:11 step:10549 [D loss: 0.202925, acc.: 69.53%] [G loss: 0.484364]\n",
      "epoch:11 step:10550 [D loss: 0.224267, acc.: 60.16%] [G loss: 0.487060]\n",
      "epoch:11 step:10551 [D loss: 0.220956, acc.: 66.41%] [G loss: 0.510807]\n",
      "epoch:11 step:10552 [D loss: 0.198980, acc.: 66.41%] [G loss: 0.460360]\n",
      "epoch:11 step:10553 [D loss: 0.237507, acc.: 63.28%] [G loss: 0.501036]\n",
      "epoch:11 step:10554 [D loss: 0.235007, acc.: 59.38%] [G loss: 0.473839]\n",
      "epoch:11 step:10555 [D loss: 0.210749, acc.: 72.66%] [G loss: 0.498693]\n",
      "epoch:11 step:10556 [D loss: 0.261839, acc.: 52.34%] [G loss: 0.432140]\n",
      "epoch:11 step:10557 [D loss: 0.251338, acc.: 58.59%] [G loss: 0.445684]\n",
      "epoch:11 step:10558 [D loss: 0.242463, acc.: 59.38%] [G loss: 0.445996]\n",
      "epoch:11 step:10559 [D loss: 0.216640, acc.: 64.06%] [G loss: 0.485093]\n",
      "epoch:11 step:10560 [D loss: 0.210788, acc.: 68.75%] [G loss: 0.459181]\n",
      "epoch:11 step:10561 [D loss: 0.233231, acc.: 60.16%] [G loss: 0.384491]\n",
      "epoch:11 step:10562 [D loss: 0.212161, acc.: 64.84%] [G loss: 0.454686]\n",
      "epoch:11 step:10563 [D loss: 0.229952, acc.: 61.72%] [G loss: 0.460273]\n",
      "epoch:11 step:10564 [D loss: 0.220809, acc.: 60.94%] [G loss: 0.509025]\n",
      "epoch:11 step:10565 [D loss: 0.214236, acc.: 69.53%] [G loss: 0.448487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10566 [D loss: 0.231170, acc.: 58.59%] [G loss: 0.426231]\n",
      "epoch:11 step:10567 [D loss: 0.200914, acc.: 68.75%] [G loss: 0.464783]\n",
      "epoch:11 step:10568 [D loss: 0.218713, acc.: 63.28%] [G loss: 0.470301]\n",
      "epoch:11 step:10569 [D loss: 0.197981, acc.: 67.97%] [G loss: 0.483957]\n",
      "epoch:11 step:10570 [D loss: 0.230533, acc.: 65.62%] [G loss: 0.479510]\n",
      "epoch:11 step:10571 [D loss: 0.217169, acc.: 68.75%] [G loss: 0.436574]\n",
      "epoch:11 step:10572 [D loss: 0.210566, acc.: 65.62%] [G loss: 0.445858]\n",
      "epoch:11 step:10573 [D loss: 0.238992, acc.: 58.59%] [G loss: 0.445600]\n",
      "epoch:11 step:10574 [D loss: 0.240944, acc.: 58.59%] [G loss: 0.461436]\n",
      "epoch:11 step:10575 [D loss: 0.233645, acc.: 64.84%] [G loss: 0.423811]\n",
      "epoch:11 step:10576 [D loss: 0.218175, acc.: 66.41%] [G loss: 0.451317]\n",
      "epoch:11 step:10577 [D loss: 0.229472, acc.: 62.50%] [G loss: 0.456270]\n",
      "epoch:11 step:10578 [D loss: 0.192197, acc.: 72.66%] [G loss: 0.483714]\n",
      "epoch:11 step:10579 [D loss: 0.222252, acc.: 60.94%] [G loss: 0.463201]\n",
      "epoch:11 step:10580 [D loss: 0.199986, acc.: 74.22%] [G loss: 0.463966]\n",
      "epoch:11 step:10581 [D loss: 0.200096, acc.: 69.53%] [G loss: 0.461509]\n",
      "epoch:11 step:10582 [D loss: 0.226475, acc.: 60.94%] [G loss: 0.463104]\n",
      "epoch:11 step:10583 [D loss: 0.211347, acc.: 65.62%] [G loss: 0.475566]\n",
      "epoch:11 step:10584 [D loss: 0.250862, acc.: 57.03%] [G loss: 0.454469]\n",
      "epoch:11 step:10585 [D loss: 0.227665, acc.: 61.72%] [G loss: 0.438126]\n",
      "epoch:11 step:10586 [D loss: 0.235116, acc.: 61.72%] [G loss: 0.435302]\n",
      "epoch:11 step:10587 [D loss: 0.183795, acc.: 74.22%] [G loss: 0.492152]\n",
      "epoch:11 step:10588 [D loss: 0.252352, acc.: 54.69%] [G loss: 0.453178]\n",
      "epoch:11 step:10589 [D loss: 0.217219, acc.: 67.97%] [G loss: 0.425944]\n",
      "epoch:11 step:10590 [D loss: 0.207405, acc.: 73.44%] [G loss: 0.453063]\n",
      "epoch:11 step:10591 [D loss: 0.238573, acc.: 60.16%] [G loss: 0.457456]\n",
      "epoch:11 step:10592 [D loss: 0.214505, acc.: 67.97%] [G loss: 0.502782]\n",
      "epoch:11 step:10593 [D loss: 0.188946, acc.: 72.66%] [G loss: 0.522537]\n",
      "epoch:11 step:10594 [D loss: 0.229548, acc.: 64.06%] [G loss: 0.447891]\n",
      "epoch:11 step:10595 [D loss: 0.245907, acc.: 60.16%] [G loss: 0.476845]\n",
      "epoch:11 step:10596 [D loss: 0.239982, acc.: 61.72%] [G loss: 0.459133]\n",
      "epoch:11 step:10597 [D loss: 0.224617, acc.: 64.06%] [G loss: 0.467987]\n",
      "epoch:11 step:10598 [D loss: 0.243244, acc.: 57.03%] [G loss: 0.451942]\n",
      "epoch:11 step:10599 [D loss: 0.232873, acc.: 59.38%] [G loss: 0.456505]\n",
      "epoch:11 step:10600 [D loss: 0.223075, acc.: 60.16%] [G loss: 0.419230]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.682003\n",
      "FID: 10.773531\n",
      "0 = 11.962842124056825\n",
      "1 = 0.04737460600757075\n",
      "2 = 0.8972499966621399\n",
      "3 = 0.8841999769210815\n",
      "4 = 0.9103000164031982\n",
      "5 = 0.907896101474762\n",
      "6 = 0.8841999769210815\n",
      "7 = 6.386251723331216\n",
      "8 = 0.0677339554260001\n",
      "9 = 0.7213500142097473\n",
      "10 = 0.722000002861023\n",
      "11 = 0.7207000255584717\n",
      "12 = 0.7210626006126404\n",
      "13 = 0.722000002861023\n",
      "14 = 7.682069301605225\n",
      "15 = 9.550370216369629\n",
      "16 = 0.11680839210748672\n",
      "17 = 7.682003498077393\n",
      "18 = 10.773530960083008\n",
      "epoch:11 step:10601 [D loss: 0.244572, acc.: 54.69%] [G loss: 0.444087]\n",
      "epoch:11 step:10602 [D loss: 0.213135, acc.: 67.19%] [G loss: 0.435619]\n",
      "epoch:11 step:10603 [D loss: 0.207956, acc.: 71.09%] [G loss: 0.450166]\n",
      "epoch:11 step:10604 [D loss: 0.208054, acc.: 66.41%] [G loss: 0.510183]\n",
      "epoch:11 step:10605 [D loss: 0.200634, acc.: 68.75%] [G loss: 0.527948]\n",
      "epoch:11 step:10606 [D loss: 0.194415, acc.: 75.78%] [G loss: 0.496231]\n",
      "epoch:11 step:10607 [D loss: 0.204197, acc.: 68.75%] [G loss: 0.525922]\n",
      "epoch:11 step:10608 [D loss: 0.273257, acc.: 55.47%] [G loss: 0.445359]\n",
      "epoch:11 step:10609 [D loss: 0.217331, acc.: 64.84%] [G loss: 0.491372]\n",
      "epoch:11 step:10610 [D loss: 0.229430, acc.: 61.72%] [G loss: 0.497415]\n",
      "epoch:11 step:10611 [D loss: 0.184101, acc.: 76.56%] [G loss: 0.497465]\n",
      "epoch:11 step:10612 [D loss: 0.237278, acc.: 62.50%] [G loss: 0.473980]\n",
      "epoch:11 step:10613 [D loss: 0.192186, acc.: 66.41%] [G loss: 0.528031]\n",
      "epoch:11 step:10614 [D loss: 0.221137, acc.: 67.19%] [G loss: 0.476113]\n",
      "epoch:11 step:10615 [D loss: 0.275341, acc.: 47.66%] [G loss: 0.418643]\n",
      "epoch:11 step:10616 [D loss: 0.209208, acc.: 69.53%] [G loss: 0.436213]\n",
      "epoch:11 step:10617 [D loss: 0.219812, acc.: 64.06%] [G loss: 0.452456]\n",
      "epoch:11 step:10618 [D loss: 0.202295, acc.: 67.19%] [G loss: 0.462698]\n",
      "epoch:11 step:10619 [D loss: 0.162142, acc.: 77.34%] [G loss: 0.501937]\n",
      "epoch:11 step:10620 [D loss: 0.203842, acc.: 67.97%] [G loss: 0.531044]\n",
      "epoch:11 step:10621 [D loss: 0.185396, acc.: 72.66%] [G loss: 0.561681]\n",
      "epoch:11 step:10622 [D loss: 0.217210, acc.: 71.09%] [G loss: 0.512641]\n",
      "epoch:11 step:10623 [D loss: 0.282455, acc.: 54.69%] [G loss: 0.444145]\n",
      "epoch:11 step:10624 [D loss: 0.251910, acc.: 57.03%] [G loss: 0.407701]\n",
      "epoch:11 step:10625 [D loss: 0.203802, acc.: 67.97%] [G loss: 0.455393]\n",
      "epoch:11 step:10626 [D loss: 0.219066, acc.: 64.06%] [G loss: 0.452892]\n",
      "epoch:11 step:10627 [D loss: 0.204633, acc.: 65.62%] [G loss: 0.452516]\n",
      "epoch:11 step:10628 [D loss: 0.185486, acc.: 71.88%] [G loss: 0.488737]\n",
      "epoch:11 step:10629 [D loss: 0.227830, acc.: 59.38%] [G loss: 0.446769]\n",
      "epoch:11 step:10630 [D loss: 0.253221, acc.: 51.56%] [G loss: 0.449327]\n",
      "epoch:11 step:10631 [D loss: 0.221318, acc.: 68.75%] [G loss: 0.473704]\n",
      "epoch:11 step:10632 [D loss: 0.219570, acc.: 64.84%] [G loss: 0.487028]\n",
      "epoch:11 step:10633 [D loss: 0.236461, acc.: 61.72%] [G loss: 0.478650]\n",
      "epoch:11 step:10634 [D loss: 0.238460, acc.: 59.38%] [G loss: 0.452232]\n",
      "epoch:11 step:10635 [D loss: 0.205214, acc.: 67.19%] [G loss: 0.467910]\n",
      "epoch:11 step:10636 [D loss: 0.228961, acc.: 62.50%] [G loss: 0.497093]\n",
      "epoch:11 step:10637 [D loss: 0.221815, acc.: 60.16%] [G loss: 0.441004]\n",
      "epoch:11 step:10638 [D loss: 0.193401, acc.: 71.09%] [G loss: 0.453799]\n",
      "epoch:11 step:10639 [D loss: 0.191332, acc.: 71.09%] [G loss: 0.450715]\n",
      "epoch:11 step:10640 [D loss: 0.207459, acc.: 67.97%] [G loss: 0.442799]\n",
      "epoch:11 step:10641 [D loss: 0.191613, acc.: 71.09%] [G loss: 0.499768]\n",
      "epoch:11 step:10642 [D loss: 0.182238, acc.: 73.44%] [G loss: 0.518300]\n",
      "epoch:11 step:10643 [D loss: 0.200155, acc.: 67.97%] [G loss: 0.505823]\n",
      "epoch:11 step:10644 [D loss: 0.205201, acc.: 71.09%] [G loss: 0.469774]\n",
      "epoch:11 step:10645 [D loss: 0.236310, acc.: 57.81%] [G loss: 0.454741]\n",
      "epoch:11 step:10646 [D loss: 0.184606, acc.: 74.22%] [G loss: 0.471142]\n",
      "epoch:11 step:10647 [D loss: 0.215415, acc.: 62.50%] [G loss: 0.434597]\n",
      "epoch:11 step:10648 [D loss: 0.272877, acc.: 57.81%] [G loss: 0.465656]\n",
      "epoch:11 step:10649 [D loss: 0.217575, acc.: 64.06%] [G loss: 0.518133]\n",
      "epoch:11 step:10650 [D loss: 0.192978, acc.: 69.53%] [G loss: 0.502800]\n",
      "epoch:11 step:10651 [D loss: 0.203927, acc.: 63.28%] [G loss: 0.496752]\n",
      "epoch:11 step:10652 [D loss: 0.217248, acc.: 71.88%] [G loss: 0.449829]\n",
      "epoch:11 step:10653 [D loss: 0.195177, acc.: 65.62%] [G loss: 0.521796]\n",
      "epoch:11 step:10654 [D loss: 0.175675, acc.: 73.44%] [G loss: 0.537253]\n",
      "epoch:11 step:10655 [D loss: 0.278730, acc.: 57.81%] [G loss: 0.500049]\n",
      "epoch:11 step:10656 [D loss: 0.254731, acc.: 55.47%] [G loss: 0.407074]\n",
      "epoch:11 step:10657 [D loss: 0.211646, acc.: 67.97%] [G loss: 0.428294]\n",
      "epoch:11 step:10658 [D loss: 0.209395, acc.: 66.41%] [G loss: 0.450679]\n",
      "epoch:11 step:10659 [D loss: 0.219588, acc.: 60.94%] [G loss: 0.495100]\n",
      "epoch:11 step:10660 [D loss: 0.205358, acc.: 69.53%] [G loss: 0.511915]\n",
      "epoch:11 step:10661 [D loss: 0.167336, acc.: 79.69%] [G loss: 0.575319]\n",
      "epoch:11 step:10662 [D loss: 0.207889, acc.: 67.97%] [G loss: 0.482190]\n",
      "epoch:11 step:10663 [D loss: 0.240740, acc.: 64.06%] [G loss: 0.450359]\n",
      "epoch:11 step:10664 [D loss: 0.209129, acc.: 69.53%] [G loss: 0.434404]\n",
      "epoch:11 step:10665 [D loss: 0.192993, acc.: 72.66%] [G loss: 0.438230]\n",
      "epoch:11 step:10666 [D loss: 0.211749, acc.: 68.75%] [G loss: 0.461304]\n",
      "epoch:11 step:10667 [D loss: 0.207755, acc.: 67.19%] [G loss: 0.470895]\n",
      "epoch:11 step:10668 [D loss: 0.210845, acc.: 67.97%] [G loss: 0.509537]\n",
      "epoch:11 step:10669 [D loss: 0.216994, acc.: 63.28%] [G loss: 0.464039]\n",
      "epoch:11 step:10670 [D loss: 0.225371, acc.: 57.81%] [G loss: 0.456551]\n",
      "epoch:11 step:10671 [D loss: 0.207933, acc.: 65.62%] [G loss: 0.445562]\n",
      "epoch:11 step:10672 [D loss: 0.228301, acc.: 60.94%] [G loss: 0.436624]\n",
      "epoch:11 step:10673 [D loss: 0.224715, acc.: 62.50%] [G loss: 0.470320]\n",
      "epoch:11 step:10674 [D loss: 0.217314, acc.: 66.41%] [G loss: 0.513994]\n",
      "epoch:11 step:10675 [D loss: 0.214053, acc.: 65.62%] [G loss: 0.490344]\n",
      "epoch:11 step:10676 [D loss: 0.234630, acc.: 57.81%] [G loss: 0.430117]\n",
      "epoch:11 step:10677 [D loss: 0.217913, acc.: 67.97%] [G loss: 0.447153]\n",
      "epoch:11 step:10678 [D loss: 0.205025, acc.: 68.75%] [G loss: 0.460748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10679 [D loss: 0.226152, acc.: 64.06%] [G loss: 0.485363]\n",
      "epoch:11 step:10680 [D loss: 0.255193, acc.: 57.03%] [G loss: 0.468261]\n",
      "epoch:11 step:10681 [D loss: 0.213289, acc.: 67.97%] [G loss: 0.518577]\n",
      "epoch:11 step:10682 [D loss: 0.229613, acc.: 64.06%] [G loss: 0.443058]\n",
      "epoch:11 step:10683 [D loss: 0.255465, acc.: 57.81%] [G loss: 0.433942]\n",
      "epoch:11 step:10684 [D loss: 0.234786, acc.: 61.72%] [G loss: 0.448678]\n",
      "epoch:11 step:10685 [D loss: 0.229848, acc.: 63.28%] [G loss: 0.433296]\n",
      "epoch:11 step:10686 [D loss: 0.222438, acc.: 63.28%] [G loss: 0.470306]\n",
      "epoch:11 step:10687 [D loss: 0.242202, acc.: 58.59%] [G loss: 0.471560]\n",
      "epoch:11 step:10688 [D loss: 0.218244, acc.: 65.62%] [G loss: 0.464900]\n",
      "epoch:11 step:10689 [D loss: 0.211696, acc.: 64.06%] [G loss: 0.451076]\n",
      "epoch:11 step:10690 [D loss: 0.227714, acc.: 60.16%] [G loss: 0.425412]\n",
      "epoch:11 step:10691 [D loss: 0.232983, acc.: 65.62%] [G loss: 0.452763]\n",
      "epoch:11 step:10692 [D loss: 0.208171, acc.: 67.19%] [G loss: 0.490416]\n",
      "epoch:11 step:10693 [D loss: 0.250259, acc.: 57.81%] [G loss: 0.449077]\n",
      "epoch:11 step:10694 [D loss: 0.209862, acc.: 69.53%] [G loss: 0.489322]\n",
      "epoch:11 step:10695 [D loss: 0.201897, acc.: 68.75%] [G loss: 0.474825]\n",
      "epoch:11 step:10696 [D loss: 0.218699, acc.: 60.94%] [G loss: 0.439376]\n",
      "epoch:11 step:10697 [D loss: 0.247116, acc.: 58.59%] [G loss: 0.430392]\n",
      "epoch:11 step:10698 [D loss: 0.239452, acc.: 64.06%] [G loss: 0.424896]\n",
      "epoch:11 step:10699 [D loss: 0.228000, acc.: 62.50%] [G loss: 0.459018]\n",
      "epoch:11 step:10700 [D loss: 0.224939, acc.: 63.28%] [G loss: 0.453799]\n",
      "epoch:11 step:10701 [D loss: 0.255270, acc.: 60.16%] [G loss: 0.449068]\n",
      "epoch:11 step:10702 [D loss: 0.209248, acc.: 66.41%] [G loss: 0.472498]\n",
      "epoch:11 step:10703 [D loss: 0.265080, acc.: 50.00%] [G loss: 0.481816]\n",
      "epoch:11 step:10704 [D loss: 0.220588, acc.: 65.62%] [G loss: 0.461823]\n",
      "epoch:11 step:10705 [D loss: 0.204958, acc.: 70.31%] [G loss: 0.491043]\n",
      "epoch:11 step:10706 [D loss: 0.206167, acc.: 67.97%] [G loss: 0.528395]\n",
      "epoch:11 step:10707 [D loss: 0.265260, acc.: 55.47%] [G loss: 0.437605]\n",
      "epoch:11 step:10708 [D loss: 0.233660, acc.: 60.16%] [G loss: 0.441242]\n",
      "epoch:11 step:10709 [D loss: 0.199239, acc.: 68.75%] [G loss: 0.464502]\n",
      "epoch:11 step:10710 [D loss: 0.261214, acc.: 60.94%] [G loss: 0.428849]\n",
      "epoch:11 step:10711 [D loss: 0.227593, acc.: 65.62%] [G loss: 0.500236]\n",
      "epoch:11 step:10712 [D loss: 0.215162, acc.: 64.06%] [G loss: 0.454925]\n",
      "epoch:11 step:10713 [D loss: 0.198898, acc.: 70.31%] [G loss: 0.493982]\n",
      "epoch:11 step:10714 [D loss: 0.234525, acc.: 66.41%] [G loss: 0.488150]\n",
      "epoch:11 step:10715 [D loss: 0.237248, acc.: 61.72%] [G loss: 0.440331]\n",
      "epoch:11 step:10716 [D loss: 0.218539, acc.: 65.62%] [G loss: 0.422734]\n",
      "epoch:11 step:10717 [D loss: 0.234223, acc.: 58.59%] [G loss: 0.410606]\n",
      "epoch:11 step:10718 [D loss: 0.233609, acc.: 62.50%] [G loss: 0.440181]\n",
      "epoch:11 step:10719 [D loss: 0.229081, acc.: 64.06%] [G loss: 0.432655]\n",
      "epoch:11 step:10720 [D loss: 0.234988, acc.: 60.16%] [G loss: 0.423807]\n",
      "epoch:11 step:10721 [D loss: 0.229810, acc.: 61.72%] [G loss: 0.479971]\n",
      "epoch:11 step:10722 [D loss: 0.221301, acc.: 64.06%] [G loss: 0.450036]\n",
      "epoch:11 step:10723 [D loss: 0.215031, acc.: 64.06%] [G loss: 0.489433]\n",
      "epoch:11 step:10724 [D loss: 0.228179, acc.: 67.97%] [G loss: 0.491951]\n",
      "epoch:11 step:10725 [D loss: 0.276891, acc.: 47.66%] [G loss: 0.427211]\n",
      "epoch:11 step:10726 [D loss: 0.242235, acc.: 60.94%] [G loss: 0.411632]\n",
      "epoch:11 step:10727 [D loss: 0.248166, acc.: 55.47%] [G loss: 0.441079]\n",
      "epoch:11 step:10728 [D loss: 0.243908, acc.: 56.25%] [G loss: 0.415662]\n",
      "epoch:11 step:10729 [D loss: 0.227188, acc.: 64.06%] [G loss: 0.451817]\n",
      "epoch:11 step:10730 [D loss: 0.206264, acc.: 67.19%] [G loss: 0.489674]\n",
      "epoch:11 step:10731 [D loss: 0.233356, acc.: 64.84%] [G loss: 0.426537]\n",
      "epoch:11 step:10732 [D loss: 0.222275, acc.: 61.72%] [G loss: 0.434142]\n",
      "epoch:11 step:10733 [D loss: 0.203960, acc.: 69.53%] [G loss: 0.469363]\n",
      "epoch:11 step:10734 [D loss: 0.215315, acc.: 60.94%] [G loss: 0.468558]\n",
      "epoch:11 step:10735 [D loss: 0.220644, acc.: 64.06%] [G loss: 0.499218]\n",
      "epoch:11 step:10736 [D loss: 0.204850, acc.: 65.62%] [G loss: 0.490043]\n",
      "epoch:11 step:10737 [D loss: 0.236478, acc.: 59.38%] [G loss: 0.494683]\n",
      "epoch:11 step:10738 [D loss: 0.212849, acc.: 66.41%] [G loss: 0.514788]\n",
      "epoch:11 step:10739 [D loss: 0.233754, acc.: 57.81%] [G loss: 0.460659]\n",
      "epoch:11 step:10740 [D loss: 0.222409, acc.: 63.28%] [G loss: 0.449725]\n",
      "epoch:11 step:10741 [D loss: 0.217434, acc.: 62.50%] [G loss: 0.469910]\n",
      "epoch:11 step:10742 [D loss: 0.214738, acc.: 62.50%] [G loss: 0.456970]\n",
      "epoch:11 step:10743 [D loss: 0.212421, acc.: 68.75%] [G loss: 0.508685]\n",
      "epoch:11 step:10744 [D loss: 0.283183, acc.: 50.78%] [G loss: 0.471669]\n",
      "epoch:11 step:10745 [D loss: 0.249255, acc.: 57.81%] [G loss: 0.416682]\n",
      "epoch:11 step:10746 [D loss: 0.216223, acc.: 67.19%] [G loss: 0.433945]\n",
      "epoch:11 step:10747 [D loss: 0.229648, acc.: 62.50%] [G loss: 0.448328]\n",
      "epoch:11 step:10748 [D loss: 0.231873, acc.: 64.06%] [G loss: 0.437132]\n",
      "epoch:11 step:10749 [D loss: 0.241563, acc.: 60.94%] [G loss: 0.499228]\n",
      "epoch:11 step:10750 [D loss: 0.220057, acc.: 60.94%] [G loss: 0.460451]\n",
      "epoch:11 step:10751 [D loss: 0.233172, acc.: 59.38%] [G loss: 0.418516]\n",
      "epoch:11 step:10752 [D loss: 0.196380, acc.: 66.41%] [G loss: 0.502081]\n",
      "epoch:11 step:10753 [D loss: 0.241818, acc.: 55.47%] [G loss: 0.436787]\n",
      "epoch:11 step:10754 [D loss: 0.186716, acc.: 71.88%] [G loss: 0.488618]\n",
      "epoch:11 step:10755 [D loss: 0.267170, acc.: 57.81%] [G loss: 0.393046]\n",
      "epoch:11 step:10756 [D loss: 0.220657, acc.: 67.19%] [G loss: 0.449689]\n",
      "epoch:11 step:10757 [D loss: 0.195459, acc.: 75.00%] [G loss: 0.456064]\n",
      "epoch:11 step:10758 [D loss: 0.198248, acc.: 71.09%] [G loss: 0.487276]\n",
      "epoch:11 step:10759 [D loss: 0.225388, acc.: 64.84%] [G loss: 0.493140]\n",
      "epoch:11 step:10760 [D loss: 0.227794, acc.: 55.47%] [G loss: 0.469611]\n",
      "epoch:11 step:10761 [D loss: 0.219255, acc.: 64.84%] [G loss: 0.483414]\n",
      "epoch:11 step:10762 [D loss: 0.244480, acc.: 56.25%] [G loss: 0.454031]\n",
      "epoch:11 step:10763 [D loss: 0.240877, acc.: 60.16%] [G loss: 0.500556]\n",
      "epoch:11 step:10764 [D loss: 0.218943, acc.: 68.75%] [G loss: 0.488949]\n",
      "epoch:11 step:10765 [D loss: 0.294162, acc.: 48.44%] [G loss: 0.446492]\n",
      "epoch:11 step:10766 [D loss: 0.234646, acc.: 59.38%] [G loss: 0.444172]\n",
      "epoch:11 step:10767 [D loss: 0.235661, acc.: 60.16%] [G loss: 0.421829]\n",
      "epoch:11 step:10768 [D loss: 0.222823, acc.: 65.62%] [G loss: 0.459646]\n",
      "epoch:11 step:10769 [D loss: 0.234410, acc.: 57.81%] [G loss: 0.446563]\n",
      "epoch:11 step:10770 [D loss: 0.226998, acc.: 60.16%] [G loss: 0.469853]\n",
      "epoch:11 step:10771 [D loss: 0.228528, acc.: 61.72%] [G loss: 0.435129]\n",
      "epoch:11 step:10772 [D loss: 0.212120, acc.: 67.97%] [G loss: 0.453446]\n",
      "epoch:11 step:10773 [D loss: 0.240125, acc.: 59.38%] [G loss: 0.406288]\n",
      "epoch:11 step:10774 [D loss: 0.216949, acc.: 64.06%] [G loss: 0.409858]\n",
      "epoch:11 step:10775 [D loss: 0.224613, acc.: 65.62%] [G loss: 0.464542]\n",
      "epoch:11 step:10776 [D loss: 0.208612, acc.: 70.31%] [G loss: 0.466624]\n",
      "epoch:11 step:10777 [D loss: 0.212102, acc.: 67.97%] [G loss: 0.476666]\n",
      "epoch:11 step:10778 [D loss: 0.183333, acc.: 69.53%] [G loss: 0.502191]\n",
      "epoch:11 step:10779 [D loss: 0.217654, acc.: 67.19%] [G loss: 0.512594]\n",
      "epoch:11 step:10780 [D loss: 0.269323, acc.: 48.44%] [G loss: 0.453445]\n",
      "epoch:11 step:10781 [D loss: 0.215923, acc.: 67.97%] [G loss: 0.454721]\n",
      "epoch:11 step:10782 [D loss: 0.219857, acc.: 62.50%] [G loss: 0.494780]\n",
      "epoch:11 step:10783 [D loss: 0.212051, acc.: 68.75%] [G loss: 0.538423]\n",
      "epoch:11 step:10784 [D loss: 0.260822, acc.: 50.00%] [G loss: 0.422946]\n",
      "epoch:11 step:10785 [D loss: 0.245594, acc.: 57.03%] [G loss: 0.438042]\n",
      "epoch:11 step:10786 [D loss: 0.235537, acc.: 60.94%] [G loss: 0.422081]\n",
      "epoch:11 step:10787 [D loss: 0.226746, acc.: 66.41%] [G loss: 0.404550]\n",
      "epoch:11 step:10788 [D loss: 0.194892, acc.: 73.44%] [G loss: 0.456873]\n",
      "epoch:11 step:10789 [D loss: 0.263423, acc.: 51.56%] [G loss: 0.422268]\n",
      "epoch:11 step:10790 [D loss: 0.245240, acc.: 55.47%] [G loss: 0.424559]\n",
      "epoch:11 step:10791 [D loss: 0.187913, acc.: 73.44%] [G loss: 0.454316]\n",
      "epoch:11 step:10792 [D loss: 0.238917, acc.: 64.06%] [G loss: 0.433770]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10793 [D loss: 0.229122, acc.: 57.81%] [G loss: 0.423678]\n",
      "epoch:11 step:10794 [D loss: 0.233979, acc.: 62.50%] [G loss: 0.421053]\n",
      "epoch:11 step:10795 [D loss: 0.176275, acc.: 77.34%] [G loss: 0.460489]\n",
      "epoch:11 step:10796 [D loss: 0.249246, acc.: 56.25%] [G loss: 0.438461]\n",
      "epoch:11 step:10797 [D loss: 0.207189, acc.: 66.41%] [G loss: 0.412465]\n",
      "epoch:11 step:10798 [D loss: 0.214629, acc.: 62.50%] [G loss: 0.438275]\n",
      "epoch:11 step:10799 [D loss: 0.239992, acc.: 57.81%] [G loss: 0.499867]\n",
      "epoch:11 step:10800 [D loss: 0.220215, acc.: 67.19%] [G loss: 0.475119]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.564535\n",
      "FID: 12.680777\n",
      "0 = 11.881255740785615\n",
      "1 = 0.05183721246135984\n",
      "2 = 0.8952000141143799\n",
      "3 = 0.8715000152587891\n",
      "4 = 0.9189000129699707\n",
      "5 = 0.9148645997047424\n",
      "6 = 0.8715000152587891\n",
      "7 = 6.611342152202116\n",
      "8 = 0.07749301878956971\n",
      "9 = 0.7116000056266785\n",
      "10 = 0.7095999717712402\n",
      "11 = 0.7135999798774719\n",
      "12 = 0.7124497890472412\n",
      "13 = 0.7095999717712402\n",
      "14 = 7.564596176147461\n",
      "15 = 9.487808227539062\n",
      "16 = 0.12715421617031097\n",
      "17 = 7.564534664154053\n",
      "18 = 12.680776596069336\n",
      "epoch:11 step:10801 [D loss: 0.222080, acc.: 59.38%] [G loss: 0.475430]\n",
      "epoch:11 step:10802 [D loss: 0.196169, acc.: 71.88%] [G loss: 0.461441]\n",
      "epoch:11 step:10803 [D loss: 0.222176, acc.: 65.62%] [G loss: 0.485699]\n",
      "epoch:11 step:10804 [D loss: 0.229679, acc.: 61.72%] [G loss: 0.454686]\n",
      "epoch:11 step:10805 [D loss: 0.193124, acc.: 74.22%] [G loss: 0.493026]\n",
      "epoch:11 step:10806 [D loss: 0.181457, acc.: 78.91%] [G loss: 0.447858]\n",
      "epoch:11 step:10807 [D loss: 0.247415, acc.: 58.59%] [G loss: 0.414890]\n",
      "epoch:11 step:10808 [D loss: 0.274029, acc.: 48.44%] [G loss: 0.381530]\n",
      "epoch:11 step:10809 [D loss: 0.231279, acc.: 61.72%] [G loss: 0.437569]\n",
      "epoch:11 step:10810 [D loss: 0.195952, acc.: 67.19%] [G loss: 0.455045]\n",
      "epoch:11 step:10811 [D loss: 0.196114, acc.: 68.75%] [G loss: 0.458456]\n",
      "epoch:11 step:10812 [D loss: 0.212737, acc.: 64.06%] [G loss: 0.437477]\n",
      "epoch:11 step:10813 [D loss: 0.221791, acc.: 65.62%] [G loss: 0.501774]\n",
      "epoch:11 step:10814 [D loss: 0.208037, acc.: 64.06%] [G loss: 0.542440]\n",
      "epoch:11 step:10815 [D loss: 0.186986, acc.: 71.09%] [G loss: 0.560402]\n",
      "epoch:11 step:10816 [D loss: 0.240546, acc.: 64.06%] [G loss: 0.458304]\n",
      "epoch:11 step:10817 [D loss: 0.235245, acc.: 58.59%] [G loss: 0.449915]\n",
      "epoch:11 step:10818 [D loss: 0.243554, acc.: 60.16%] [G loss: 0.434043]\n",
      "epoch:11 step:10819 [D loss: 0.226226, acc.: 60.16%] [G loss: 0.450693]\n",
      "epoch:11 step:10820 [D loss: 0.213520, acc.: 64.84%] [G loss: 0.468582]\n",
      "epoch:11 step:10821 [D loss: 0.206649, acc.: 64.84%] [G loss: 0.461939]\n",
      "epoch:11 step:10822 [D loss: 0.183384, acc.: 73.44%] [G loss: 0.480491]\n",
      "epoch:11 step:10823 [D loss: 0.202729, acc.: 70.31%] [G loss: 0.503165]\n",
      "epoch:11 step:10824 [D loss: 0.245794, acc.: 59.38%] [G loss: 0.449560]\n",
      "epoch:11 step:10825 [D loss: 0.233326, acc.: 57.81%] [G loss: 0.448689]\n",
      "epoch:11 step:10826 [D loss: 0.210423, acc.: 69.53%] [G loss: 0.461832]\n",
      "epoch:11 step:10827 [D loss: 0.210520, acc.: 64.84%] [G loss: 0.504915]\n",
      "epoch:11 step:10828 [D loss: 0.206760, acc.: 68.75%] [G loss: 0.479859]\n",
      "epoch:11 step:10829 [D loss: 0.210774, acc.: 67.97%] [G loss: 0.467560]\n",
      "epoch:11 step:10830 [D loss: 0.210217, acc.: 71.09%] [G loss: 0.442698]\n",
      "epoch:11 step:10831 [D loss: 0.228438, acc.: 60.94%] [G loss: 0.437707]\n",
      "epoch:11 step:10832 [D loss: 0.224420, acc.: 60.16%] [G loss: 0.444998]\n",
      "epoch:11 step:10833 [D loss: 0.206530, acc.: 67.97%] [G loss: 0.478977]\n",
      "epoch:11 step:10834 [D loss: 0.247012, acc.: 60.16%] [G loss: 0.474444]\n",
      "epoch:11 step:10835 [D loss: 0.240272, acc.: 56.25%] [G loss: 0.452246]\n",
      "epoch:11 step:10836 [D loss: 0.243211, acc.: 59.38%] [G loss: 0.462367]\n",
      "epoch:11 step:10837 [D loss: 0.206765, acc.: 70.31%] [G loss: 0.441520]\n",
      "epoch:11 step:10838 [D loss: 0.242646, acc.: 60.16%] [G loss: 0.446731]\n",
      "epoch:11 step:10839 [D loss: 0.218319, acc.: 61.72%] [G loss: 0.484338]\n",
      "epoch:11 step:10840 [D loss: 0.211513, acc.: 65.62%] [G loss: 0.427387]\n",
      "epoch:11 step:10841 [D loss: 0.185700, acc.: 72.66%] [G loss: 0.488121]\n",
      "epoch:11 step:10842 [D loss: 0.275726, acc.: 52.34%] [G loss: 0.405956]\n",
      "epoch:11 step:10843 [D loss: 0.197467, acc.: 67.19%] [G loss: 0.423177]\n",
      "epoch:11 step:10844 [D loss: 0.213881, acc.: 61.72%] [G loss: 0.480616]\n",
      "epoch:11 step:10845 [D loss: 0.239365, acc.: 62.50%] [G loss: 0.481834]\n",
      "epoch:11 step:10846 [D loss: 0.209624, acc.: 67.97%] [G loss: 0.460748]\n",
      "epoch:11 step:10847 [D loss: 0.205825, acc.: 67.19%] [G loss: 0.452325]\n",
      "epoch:11 step:10848 [D loss: 0.223769, acc.: 66.41%] [G loss: 0.470735]\n",
      "epoch:11 step:10849 [D loss: 0.274420, acc.: 47.66%] [G loss: 0.426654]\n",
      "epoch:11 step:10850 [D loss: 0.252659, acc.: 60.94%] [G loss: 0.405566]\n",
      "epoch:11 step:10851 [D loss: 0.224248, acc.: 66.41%] [G loss: 0.442587]\n",
      "epoch:11 step:10852 [D loss: 0.218030, acc.: 64.84%] [G loss: 0.482023]\n",
      "epoch:11 step:10853 [D loss: 0.214811, acc.: 64.06%] [G loss: 0.477713]\n",
      "epoch:11 step:10854 [D loss: 0.171582, acc.: 78.12%] [G loss: 0.512940]\n",
      "epoch:11 step:10855 [D loss: 0.222030, acc.: 64.84%] [G loss: 0.480503]\n",
      "epoch:11 step:10856 [D loss: 0.191941, acc.: 66.41%] [G loss: 0.497155]\n",
      "epoch:11 step:10857 [D loss: 0.191368, acc.: 70.31%] [G loss: 0.451598]\n",
      "epoch:11 step:10858 [D loss: 0.202005, acc.: 69.53%] [G loss: 0.466402]\n",
      "epoch:11 step:10859 [D loss: 0.211467, acc.: 64.84%] [G loss: 0.531237]\n",
      "epoch:11 step:10860 [D loss: 0.250975, acc.: 55.47%] [G loss: 0.440831]\n",
      "epoch:11 step:10861 [D loss: 0.187608, acc.: 70.31%] [G loss: 0.476431]\n",
      "epoch:11 step:10862 [D loss: 0.178930, acc.: 74.22%] [G loss: 0.477137]\n",
      "epoch:11 step:10863 [D loss: 0.214228, acc.: 67.97%] [G loss: 0.470876]\n",
      "epoch:11 step:10864 [D loss: 0.213766, acc.: 66.41%] [G loss: 0.490057]\n",
      "epoch:11 step:10865 [D loss: 0.202567, acc.: 67.97%] [G loss: 0.510223]\n",
      "epoch:11 step:10866 [D loss: 0.236477, acc.: 60.16%] [G loss: 0.506120]\n",
      "epoch:11 step:10867 [D loss: 0.235718, acc.: 57.81%] [G loss: 0.427016]\n",
      "epoch:11 step:10868 [D loss: 0.205420, acc.: 64.84%] [G loss: 0.456359]\n",
      "epoch:11 step:10869 [D loss: 0.227378, acc.: 59.38%] [G loss: 0.442084]\n",
      "epoch:11 step:10870 [D loss: 0.245502, acc.: 60.16%] [G loss: 0.451576]\n",
      "epoch:11 step:10871 [D loss: 0.191683, acc.: 68.75%] [G loss: 0.512439]\n",
      "epoch:11 step:10872 [D loss: 0.250145, acc.: 53.91%] [G loss: 0.491464]\n",
      "epoch:11 step:10873 [D loss: 0.269730, acc.: 52.34%] [G loss: 0.422185]\n",
      "epoch:11 step:10874 [D loss: 0.219159, acc.: 69.53%] [G loss: 0.457318]\n",
      "epoch:11 step:10875 [D loss: 0.189465, acc.: 75.00%] [G loss: 0.470118]\n",
      "epoch:11 step:10876 [D loss: 0.266799, acc.: 53.91%] [G loss: 0.460202]\n",
      "epoch:11 step:10877 [D loss: 0.216568, acc.: 65.62%] [G loss: 0.395710]\n",
      "epoch:11 step:10878 [D loss: 0.200629, acc.: 71.09%] [G loss: 0.424406]\n",
      "epoch:11 step:10879 [D loss: 0.234143, acc.: 57.03%] [G loss: 0.414836]\n",
      "epoch:11 step:10880 [D loss: 0.247783, acc.: 53.12%] [G loss: 0.408939]\n",
      "epoch:11 step:10881 [D loss: 0.188072, acc.: 71.88%] [G loss: 0.468754]\n",
      "epoch:11 step:10882 [D loss: 0.210482, acc.: 64.06%] [G loss: 0.504748]\n",
      "epoch:11 step:10883 [D loss: 0.229899, acc.: 62.50%] [G loss: 0.494747]\n",
      "epoch:11 step:10884 [D loss: 0.257094, acc.: 60.16%] [G loss: 0.436926]\n",
      "epoch:11 step:10885 [D loss: 0.230076, acc.: 60.16%] [G loss: 0.404805]\n",
      "epoch:11 step:10886 [D loss: 0.224834, acc.: 62.50%] [G loss: 0.457935]\n",
      "epoch:11 step:10887 [D loss: 0.256840, acc.: 60.16%] [G loss: 0.437869]\n",
      "epoch:11 step:10888 [D loss: 0.210955, acc.: 68.75%] [G loss: 0.480093]\n",
      "epoch:11 step:10889 [D loss: 0.224025, acc.: 60.16%] [G loss: 0.482551]\n",
      "epoch:11 step:10890 [D loss: 0.223159, acc.: 64.84%] [G loss: 0.447162]\n",
      "epoch:11 step:10891 [D loss: 0.255539, acc.: 54.69%] [G loss: 0.457647]\n",
      "epoch:11 step:10892 [D loss: 0.211987, acc.: 64.84%] [G loss: 0.452918]\n",
      "epoch:11 step:10893 [D loss: 0.234013, acc.: 59.38%] [G loss: 0.461656]\n",
      "epoch:11 step:10894 [D loss: 0.224935, acc.: 65.62%] [G loss: 0.463776]\n",
      "epoch:11 step:10895 [D loss: 0.212981, acc.: 70.31%] [G loss: 0.446323]\n",
      "epoch:11 step:10896 [D loss: 0.203474, acc.: 69.53%] [G loss: 0.514171]\n",
      "epoch:11 step:10897 [D loss: 0.243910, acc.: 62.50%] [G loss: 0.491736]\n",
      "epoch:11 step:10898 [D loss: 0.233862, acc.: 60.94%] [G loss: 0.443488]\n",
      "epoch:11 step:10899 [D loss: 0.214557, acc.: 66.41%] [G loss: 0.437996]\n",
      "epoch:11 step:10900 [D loss: 0.213016, acc.: 65.62%] [G loss: 0.457166]\n",
      "epoch:11 step:10901 [D loss: 0.214039, acc.: 68.75%] [G loss: 0.459905]\n",
      "epoch:11 step:10902 [D loss: 0.229053, acc.: 64.06%] [G loss: 0.436775]\n",
      "epoch:11 step:10903 [D loss: 0.245839, acc.: 58.59%] [G loss: 0.468219]\n",
      "epoch:11 step:10904 [D loss: 0.243417, acc.: 62.50%] [G loss: 0.438147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10905 [D loss: 0.201566, acc.: 69.53%] [G loss: 0.452692]\n",
      "epoch:11 step:10906 [D loss: 0.226495, acc.: 61.72%] [G loss: 0.434862]\n",
      "epoch:11 step:10907 [D loss: 0.277838, acc.: 46.09%] [G loss: 0.405618]\n",
      "epoch:11 step:10908 [D loss: 0.221121, acc.: 64.84%] [G loss: 0.451094]\n",
      "epoch:11 step:10909 [D loss: 0.218597, acc.: 63.28%] [G loss: 0.446310]\n",
      "epoch:11 step:10910 [D loss: 0.222704, acc.: 62.50%] [G loss: 0.475367]\n",
      "epoch:11 step:10911 [D loss: 0.224130, acc.: 63.28%] [G loss: 0.458216]\n",
      "epoch:11 step:10912 [D loss: 0.197232, acc.: 72.66%] [G loss: 0.468439]\n",
      "epoch:11 step:10913 [D loss: 0.235915, acc.: 56.25%] [G loss: 0.398993]\n",
      "epoch:11 step:10914 [D loss: 0.211913, acc.: 62.50%] [G loss: 0.465955]\n",
      "epoch:11 step:10915 [D loss: 0.216533, acc.: 61.72%] [G loss: 0.510888]\n",
      "epoch:11 step:10916 [D loss: 0.223240, acc.: 60.94%] [G loss: 0.465777]\n",
      "epoch:11 step:10917 [D loss: 0.205345, acc.: 70.31%] [G loss: 0.469448]\n",
      "epoch:11 step:10918 [D loss: 0.206329, acc.: 68.75%] [G loss: 0.442509]\n",
      "epoch:11 step:10919 [D loss: 0.224769, acc.: 64.84%] [G loss: 0.435181]\n",
      "epoch:11 step:10920 [D loss: 0.194970, acc.: 75.78%] [G loss: 0.413771]\n",
      "epoch:11 step:10921 [D loss: 0.266608, acc.: 51.56%] [G loss: 0.394082]\n",
      "epoch:11 step:10922 [D loss: 0.266934, acc.: 53.91%] [G loss: 0.407053]\n",
      "epoch:11 step:10923 [D loss: 0.221387, acc.: 64.06%] [G loss: 0.450696]\n",
      "epoch:11 step:10924 [D loss: 0.232859, acc.: 64.06%] [G loss: 0.443743]\n",
      "epoch:11 step:10925 [D loss: 0.221942, acc.: 68.75%] [G loss: 0.488511]\n",
      "epoch:11 step:10926 [D loss: 0.203955, acc.: 67.19%] [G loss: 0.453264]\n",
      "epoch:11 step:10927 [D loss: 0.224919, acc.: 64.06%] [G loss: 0.476822]\n",
      "epoch:11 step:10928 [D loss: 0.223995, acc.: 60.94%] [G loss: 0.498528]\n",
      "epoch:11 step:10929 [D loss: 0.239303, acc.: 58.59%] [G loss: 0.448288]\n",
      "epoch:11 step:10930 [D loss: 0.212999, acc.: 65.62%] [G loss: 0.485285]\n",
      "epoch:11 step:10931 [D loss: 0.187895, acc.: 72.66%] [G loss: 0.539437]\n",
      "epoch:11 step:10932 [D loss: 0.247431, acc.: 64.84%] [G loss: 0.460026]\n",
      "epoch:11 step:10933 [D loss: 0.228669, acc.: 60.94%] [G loss: 0.445379]\n",
      "epoch:11 step:10934 [D loss: 0.214146, acc.: 61.72%] [G loss: 0.437234]\n",
      "epoch:11 step:10935 [D loss: 0.224471, acc.: 65.62%] [G loss: 0.483290]\n",
      "epoch:11 step:10936 [D loss: 0.214430, acc.: 64.84%] [G loss: 0.461580]\n",
      "epoch:11 step:10937 [D loss: 0.220769, acc.: 61.72%] [G loss: 0.439573]\n",
      "epoch:11 step:10938 [D loss: 0.183897, acc.: 73.44%] [G loss: 0.483137]\n",
      "epoch:11 step:10939 [D loss: 0.222969, acc.: 61.72%] [G loss: 0.461249]\n",
      "epoch:11 step:10940 [D loss: 0.199911, acc.: 67.19%] [G loss: 0.507105]\n",
      "epoch:11 step:10941 [D loss: 0.204835, acc.: 67.19%] [G loss: 0.503920]\n",
      "epoch:11 step:10942 [D loss: 0.209025, acc.: 71.09%] [G loss: 0.489995]\n",
      "epoch:11 step:10943 [D loss: 0.232920, acc.: 57.81%] [G loss: 0.491152]\n",
      "epoch:11 step:10944 [D loss: 0.205007, acc.: 70.31%] [G loss: 0.508910]\n",
      "epoch:11 step:10945 [D loss: 0.232236, acc.: 62.50%] [G loss: 0.436310]\n",
      "epoch:11 step:10946 [D loss: 0.219654, acc.: 63.28%] [G loss: 0.433167]\n",
      "epoch:11 step:10947 [D loss: 0.187552, acc.: 69.53%] [G loss: 0.472700]\n",
      "epoch:11 step:10948 [D loss: 0.222572, acc.: 59.38%] [G loss: 0.432839]\n",
      "epoch:11 step:10949 [D loss: 0.194085, acc.: 72.66%] [G loss: 0.532256]\n",
      "epoch:11 step:10950 [D loss: 0.222879, acc.: 62.50%] [G loss: 0.509295]\n",
      "epoch:11 step:10951 [D loss: 0.235679, acc.: 55.47%] [G loss: 0.476493]\n",
      "epoch:11 step:10952 [D loss: 0.212304, acc.: 64.84%] [G loss: 0.493128]\n",
      "epoch:11 step:10953 [D loss: 0.220725, acc.: 64.06%] [G loss: 0.453350]\n",
      "epoch:11 step:10954 [D loss: 0.217288, acc.: 66.41%] [G loss: 0.489738]\n",
      "epoch:11 step:10955 [D loss: 0.186690, acc.: 76.56%] [G loss: 0.550599]\n",
      "epoch:11 step:10956 [D loss: 0.191638, acc.: 68.75%] [G loss: 0.483310]\n",
      "epoch:11 step:10957 [D loss: 0.228407, acc.: 64.06%] [G loss: 0.463207]\n",
      "epoch:11 step:10958 [D loss: 0.215069, acc.: 63.28%] [G loss: 0.481058]\n",
      "epoch:11 step:10959 [D loss: 0.252867, acc.: 55.47%] [G loss: 0.447974]\n",
      "epoch:11 step:10960 [D loss: 0.215963, acc.: 70.31%] [G loss: 0.504604]\n",
      "epoch:11 step:10961 [D loss: 0.210823, acc.: 64.06%] [G loss: 0.463197]\n",
      "epoch:11 step:10962 [D loss: 0.244038, acc.: 56.25%] [G loss: 0.476810]\n",
      "epoch:11 step:10963 [D loss: 0.213135, acc.: 69.53%] [G loss: 0.463856]\n",
      "epoch:11 step:10964 [D loss: 0.227999, acc.: 60.16%] [G loss: 0.452850]\n",
      "epoch:11 step:10965 [D loss: 0.226215, acc.: 60.16%] [G loss: 0.473638]\n",
      "epoch:11 step:10966 [D loss: 0.226268, acc.: 60.16%] [G loss: 0.460790]\n",
      "epoch:11 step:10967 [D loss: 0.227042, acc.: 66.41%] [G loss: 0.441899]\n",
      "epoch:11 step:10968 [D loss: 0.206516, acc.: 71.88%] [G loss: 0.494424]\n",
      "epoch:11 step:10969 [D loss: 0.234574, acc.: 61.72%] [G loss: 0.463291]\n",
      "epoch:11 step:10970 [D loss: 0.215075, acc.: 65.62%] [G loss: 0.458309]\n",
      "epoch:11 step:10971 [D loss: 0.219294, acc.: 64.84%] [G loss: 0.466143]\n",
      "epoch:11 step:10972 [D loss: 0.235824, acc.: 64.06%] [G loss: 0.478026]\n",
      "epoch:11 step:10973 [D loss: 0.201916, acc.: 64.06%] [G loss: 0.476753]\n",
      "epoch:11 step:10974 [D loss: 0.226614, acc.: 68.75%] [G loss: 0.450075]\n",
      "epoch:11 step:10975 [D loss: 0.219337, acc.: 67.19%] [G loss: 0.452219]\n",
      "epoch:11 step:10976 [D loss: 0.204672, acc.: 69.53%] [G loss: 0.464954]\n",
      "epoch:11 step:10977 [D loss: 0.255939, acc.: 54.69%] [G loss: 0.454600]\n",
      "epoch:11 step:10978 [D loss: 0.217365, acc.: 64.84%] [G loss: 0.475619]\n",
      "epoch:11 step:10979 [D loss: 0.246261, acc.: 60.16%] [G loss: 0.414742]\n",
      "epoch:11 step:10980 [D loss: 0.246768, acc.: 60.94%] [G loss: 0.456576]\n",
      "epoch:11 step:10981 [D loss: 0.225627, acc.: 64.84%] [G loss: 0.464480]\n",
      "epoch:11 step:10982 [D loss: 0.219391, acc.: 65.62%] [G loss: 0.435856]\n",
      "epoch:11 step:10983 [D loss: 0.224170, acc.: 60.94%] [G loss: 0.459689]\n",
      "epoch:11 step:10984 [D loss: 0.193392, acc.: 71.88%] [G loss: 0.456057]\n",
      "epoch:11 step:10985 [D loss: 0.250385, acc.: 57.03%] [G loss: 0.466069]\n",
      "epoch:11 step:10986 [D loss: 0.201962, acc.: 71.09%] [G loss: 0.462810]\n",
      "epoch:11 step:10987 [D loss: 0.206164, acc.: 65.62%] [G loss: 0.462760]\n",
      "epoch:11 step:10988 [D loss: 0.196582, acc.: 71.09%] [G loss: 0.484860]\n",
      "epoch:11 step:10989 [D loss: 0.237904, acc.: 61.72%] [G loss: 0.415264]\n",
      "epoch:11 step:10990 [D loss: 0.239818, acc.: 60.16%] [G loss: 0.444407]\n",
      "epoch:11 step:10991 [D loss: 0.223019, acc.: 64.06%] [G loss: 0.419340]\n",
      "epoch:11 step:10992 [D loss: 0.223359, acc.: 64.06%] [G loss: 0.445427]\n",
      "epoch:11 step:10993 [D loss: 0.224924, acc.: 58.59%] [G loss: 0.467646]\n",
      "epoch:11 step:10994 [D loss: 0.222641, acc.: 64.06%] [G loss: 0.469088]\n",
      "epoch:11 step:10995 [D loss: 0.213521, acc.: 64.84%] [G loss: 0.475517]\n",
      "epoch:11 step:10996 [D loss: 0.213545, acc.: 67.19%] [G loss: 0.500082]\n",
      "epoch:11 step:10997 [D loss: 0.197411, acc.: 67.97%] [G loss: 0.474892]\n",
      "epoch:11 step:10998 [D loss: 0.214073, acc.: 62.50%] [G loss: 0.504097]\n",
      "epoch:11 step:10999 [D loss: 0.190222, acc.: 71.09%] [G loss: 0.537514]\n",
      "epoch:11 step:11000 [D loss: 0.203984, acc.: 64.84%] [G loss: 0.483953]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.804075\n",
      "FID: 10.590171\n",
      "0 = 11.895696275711053\n",
      "1 = 0.0466583330797015\n",
      "2 = 0.9003000259399414\n",
      "3 = 0.8813999891281128\n",
      "4 = 0.9192000031471252\n",
      "5 = 0.9160257577896118\n",
      "6 = 0.8813999891281128\n",
      "7 = 6.345266872727863\n",
      "8 = 0.07253154594495782\n",
      "9 = 0.7010999917984009\n",
      "10 = 0.6998000144958496\n",
      "11 = 0.7024000287055969\n",
      "12 = 0.7016242146492004\n",
      "13 = 0.6998000144958496\n",
      "14 = 7.804149150848389\n",
      "15 = 9.52898120880127\n",
      "16 = 0.11188285797834396\n",
      "17 = 7.804074764251709\n",
      "18 = 10.590170860290527\n",
      "epoch:11 step:11001 [D loss: 0.187600, acc.: 71.09%] [G loss: 0.446607]\n",
      "epoch:11 step:11002 [D loss: 0.221169, acc.: 65.62%] [G loss: 0.504677]\n",
      "epoch:11 step:11003 [D loss: 0.252363, acc.: 57.81%] [G loss: 0.499039]\n",
      "epoch:11 step:11004 [D loss: 0.202181, acc.: 68.75%] [G loss: 0.477587]\n",
      "epoch:11 step:11005 [D loss: 0.245226, acc.: 60.94%] [G loss: 0.428395]\n",
      "epoch:11 step:11006 [D loss: 0.196208, acc.: 71.09%] [G loss: 0.447030]\n",
      "epoch:11 step:11007 [D loss: 0.187447, acc.: 73.44%] [G loss: 0.509424]\n",
      "epoch:11 step:11008 [D loss: 0.206362, acc.: 67.19%] [G loss: 0.469924]\n",
      "epoch:11 step:11009 [D loss: 0.245536, acc.: 54.69%] [G loss: 0.437624]\n",
      "epoch:11 step:11010 [D loss: 0.253529, acc.: 55.47%] [G loss: 0.422940]\n",
      "epoch:11 step:11011 [D loss: 0.243545, acc.: 55.47%] [G loss: 0.452841]\n",
      "epoch:11 step:11012 [D loss: 0.221848, acc.: 66.41%] [G loss: 0.477594]\n",
      "epoch:11 step:11013 [D loss: 0.204029, acc.: 66.41%] [G loss: 0.479600]\n",
      "epoch:11 step:11014 [D loss: 0.211503, acc.: 65.62%] [G loss: 0.440680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11015 [D loss: 0.183525, acc.: 75.78%] [G loss: 0.477782]\n",
      "epoch:11 step:11016 [D loss: 0.175642, acc.: 70.31%] [G loss: 0.509003]\n",
      "epoch:11 step:11017 [D loss: 0.255391, acc.: 53.91%] [G loss: 0.442192]\n",
      "epoch:11 step:11018 [D loss: 0.222204, acc.: 61.72%] [G loss: 0.458768]\n",
      "epoch:11 step:11019 [D loss: 0.219918, acc.: 62.50%] [G loss: 0.489677]\n",
      "epoch:11 step:11020 [D loss: 0.234329, acc.: 60.16%] [G loss: 0.443720]\n",
      "epoch:11 step:11021 [D loss: 0.254959, acc.: 51.56%] [G loss: 0.436728]\n",
      "epoch:11 step:11022 [D loss: 0.210740, acc.: 67.97%] [G loss: 0.458551]\n",
      "epoch:11 step:11023 [D loss: 0.257565, acc.: 58.59%] [G loss: 0.406259]\n",
      "epoch:11 step:11024 [D loss: 0.237054, acc.: 61.72%] [G loss: 0.443719]\n",
      "epoch:11 step:11025 [D loss: 0.245544, acc.: 58.59%] [G loss: 0.435268]\n",
      "epoch:11 step:11026 [D loss: 0.184866, acc.: 72.66%] [G loss: 0.508133]\n",
      "epoch:11 step:11027 [D loss: 0.215788, acc.: 63.28%] [G loss: 0.512725]\n",
      "epoch:11 step:11028 [D loss: 0.231271, acc.: 61.72%] [G loss: 0.458822]\n",
      "epoch:11 step:11029 [D loss: 0.253067, acc.: 51.56%] [G loss: 0.434333]\n",
      "epoch:11 step:11030 [D loss: 0.243582, acc.: 52.34%] [G loss: 0.450279]\n",
      "epoch:11 step:11031 [D loss: 0.211712, acc.: 67.97%] [G loss: 0.462939]\n",
      "epoch:11 step:11032 [D loss: 0.218058, acc.: 63.28%] [G loss: 0.504770]\n",
      "epoch:11 step:11033 [D loss: 0.230325, acc.: 60.94%] [G loss: 0.464470]\n",
      "epoch:11 step:11034 [D loss: 0.250007, acc.: 60.16%] [G loss: 0.422683]\n",
      "epoch:11 step:11035 [D loss: 0.226131, acc.: 56.25%] [G loss: 0.421481]\n",
      "epoch:11 step:11036 [D loss: 0.212860, acc.: 69.53%] [G loss: 0.446485]\n",
      "epoch:11 step:11037 [D loss: 0.184007, acc.: 67.97%] [G loss: 0.450432]\n",
      "epoch:11 step:11038 [D loss: 0.214532, acc.: 67.97%] [G loss: 0.481665]\n",
      "epoch:11 step:11039 [D loss: 0.210154, acc.: 68.75%] [G loss: 0.478795]\n",
      "epoch:11 step:11040 [D loss: 0.215382, acc.: 66.41%] [G loss: 0.477269]\n",
      "epoch:11 step:11041 [D loss: 0.253131, acc.: 57.03%] [G loss: 0.414785]\n",
      "epoch:11 step:11042 [D loss: 0.216096, acc.: 66.41%] [G loss: 0.472887]\n",
      "epoch:11 step:11043 [D loss: 0.206492, acc.: 69.53%] [G loss: 0.463929]\n",
      "epoch:11 step:11044 [D loss: 0.198185, acc.: 69.53%] [G loss: 0.462506]\n",
      "epoch:11 step:11045 [D loss: 0.239524, acc.: 57.81%] [G loss: 0.448762]\n",
      "epoch:11 step:11046 [D loss: 0.238019, acc.: 60.16%] [G loss: 0.455292]\n",
      "epoch:11 step:11047 [D loss: 0.233623, acc.: 61.72%] [G loss: 0.434157]\n",
      "epoch:11 step:11048 [D loss: 0.255973, acc.: 56.25%] [G loss: 0.415180]\n",
      "epoch:11 step:11049 [D loss: 0.234631, acc.: 60.94%] [G loss: 0.457750]\n",
      "epoch:11 step:11050 [D loss: 0.225514, acc.: 62.50%] [G loss: 0.473004]\n",
      "epoch:11 step:11051 [D loss: 0.226372, acc.: 61.72%] [G loss: 0.451126]\n",
      "epoch:11 step:11052 [D loss: 0.235706, acc.: 57.81%] [G loss: 0.472020]\n",
      "epoch:11 step:11053 [D loss: 0.195375, acc.: 67.97%] [G loss: 0.468919]\n",
      "epoch:11 step:11054 [D loss: 0.206863, acc.: 67.19%] [G loss: 0.492503]\n",
      "epoch:11 step:11055 [D loss: 0.213471, acc.: 67.97%] [G loss: 0.453234]\n",
      "epoch:11 step:11056 [D loss: 0.225264, acc.: 60.94%] [G loss: 0.435243]\n",
      "epoch:11 step:11057 [D loss: 0.212452, acc.: 66.41%] [G loss: 0.466979]\n",
      "epoch:11 step:11058 [D loss: 0.221988, acc.: 64.84%] [G loss: 0.424398]\n",
      "epoch:11 step:11059 [D loss: 0.220794, acc.: 64.84%] [G loss: 0.467628]\n",
      "epoch:11 step:11060 [D loss: 0.227792, acc.: 63.28%] [G loss: 0.460745]\n",
      "epoch:11 step:11061 [D loss: 0.217708, acc.: 60.94%] [G loss: 0.422749]\n",
      "epoch:11 step:11062 [D loss: 0.223351, acc.: 62.50%] [G loss: 0.431420]\n",
      "epoch:11 step:11063 [D loss: 0.202733, acc.: 70.31%] [G loss: 0.452937]\n",
      "epoch:11 step:11064 [D loss: 0.226286, acc.: 67.19%] [G loss: 0.469151]\n",
      "epoch:11 step:11065 [D loss: 0.202744, acc.: 72.66%] [G loss: 0.469847]\n",
      "epoch:11 step:11066 [D loss: 0.265631, acc.: 57.03%] [G loss: 0.413267]\n",
      "epoch:11 step:11067 [D loss: 0.253865, acc.: 57.81%] [G loss: 0.411352]\n",
      "epoch:11 step:11068 [D loss: 0.214803, acc.: 70.31%] [G loss: 0.499857]\n",
      "epoch:11 step:11069 [D loss: 0.233693, acc.: 62.50%] [G loss: 0.442336]\n",
      "epoch:11 step:11070 [D loss: 0.225149, acc.: 64.84%] [G loss: 0.447289]\n",
      "epoch:11 step:11071 [D loss: 0.226354, acc.: 61.72%] [G loss: 0.435357]\n",
      "epoch:11 step:11072 [D loss: 0.268845, acc.: 55.47%] [G loss: 0.462223]\n",
      "epoch:11 step:11073 [D loss: 0.242572, acc.: 58.59%] [G loss: 0.501237]\n",
      "epoch:11 step:11074 [D loss: 0.202234, acc.: 67.19%] [G loss: 0.510775]\n",
      "epoch:11 step:11075 [D loss: 0.277672, acc.: 55.47%] [G loss: 0.481046]\n",
      "epoch:11 step:11076 [D loss: 0.214478, acc.: 68.75%] [G loss: 0.487501]\n",
      "epoch:11 step:11077 [D loss: 0.230025, acc.: 58.59%] [G loss: 0.454967]\n",
      "epoch:11 step:11078 [D loss: 0.230855, acc.: 62.50%] [G loss: 0.425054]\n",
      "epoch:11 step:11079 [D loss: 0.213224, acc.: 63.28%] [G loss: 0.433311]\n",
      "epoch:11 step:11080 [D loss: 0.235275, acc.: 60.16%] [G loss: 0.435425]\n",
      "epoch:11 step:11081 [D loss: 0.217929, acc.: 65.62%] [G loss: 0.448081]\n",
      "epoch:11 step:11082 [D loss: 0.199453, acc.: 70.31%] [G loss: 0.448625]\n",
      "epoch:11 step:11083 [D loss: 0.221947, acc.: 63.28%] [G loss: 0.474829]\n",
      "epoch:11 step:11084 [D loss: 0.222097, acc.: 64.84%] [G loss: 0.467038]\n",
      "epoch:11 step:11085 [D loss: 0.221689, acc.: 60.94%] [G loss: 0.491552]\n",
      "epoch:11 step:11086 [D loss: 0.247763, acc.: 56.25%] [G loss: 0.441628]\n",
      "epoch:11 step:11087 [D loss: 0.216958, acc.: 65.62%] [G loss: 0.441778]\n",
      "epoch:11 step:11088 [D loss: 0.206499, acc.: 67.19%] [G loss: 0.461167]\n",
      "epoch:11 step:11089 [D loss: 0.185185, acc.: 74.22%] [G loss: 0.496732]\n",
      "epoch:11 step:11090 [D loss: 0.204219, acc.: 70.31%] [G loss: 0.488541]\n",
      "epoch:11 step:11091 [D loss: 0.274788, acc.: 53.12%] [G loss: 0.416979]\n",
      "epoch:11 step:11092 [D loss: 0.226998, acc.: 61.72%] [G loss: 0.387691]\n",
      "epoch:11 step:11093 [D loss: 0.207946, acc.: 64.84%] [G loss: 0.452005]\n",
      "epoch:11 step:11094 [D loss: 0.243483, acc.: 64.84%] [G loss: 0.470599]\n",
      "epoch:11 step:11095 [D loss: 0.236696, acc.: 61.72%] [G loss: 0.475935]\n",
      "epoch:11 step:11096 [D loss: 0.234588, acc.: 63.28%] [G loss: 0.421099]\n",
      "epoch:11 step:11097 [D loss: 0.230197, acc.: 63.28%] [G loss: 0.441560]\n",
      "epoch:11 step:11098 [D loss: 0.232183, acc.: 67.97%] [G loss: 0.443008]\n",
      "epoch:11 step:11099 [D loss: 0.215316, acc.: 65.62%] [G loss: 0.456501]\n",
      "epoch:11 step:11100 [D loss: 0.214914, acc.: 66.41%] [G loss: 0.471262]\n",
      "epoch:11 step:11101 [D loss: 0.258765, acc.: 57.81%] [G loss: 0.452475]\n",
      "epoch:11 step:11102 [D loss: 0.240603, acc.: 60.16%] [G loss: 0.494161]\n",
      "epoch:11 step:11103 [D loss: 0.225317, acc.: 65.62%] [G loss: 0.458998]\n",
      "epoch:11 step:11104 [D loss: 0.245192, acc.: 59.38%] [G loss: 0.467514]\n",
      "epoch:11 step:11105 [D loss: 0.210622, acc.: 63.28%] [G loss: 0.407715]\n",
      "epoch:11 step:11106 [D loss: 0.226258, acc.: 64.06%] [G loss: 0.454325]\n",
      "epoch:11 step:11107 [D loss: 0.243420, acc.: 60.94%] [G loss: 0.407727]\n",
      "epoch:11 step:11108 [D loss: 0.187176, acc.: 72.66%] [G loss: 0.485883]\n",
      "epoch:11 step:11109 [D loss: 0.199089, acc.: 67.97%] [G loss: 0.494175]\n",
      "epoch:11 step:11110 [D loss: 0.237099, acc.: 60.16%] [G loss: 0.496221]\n",
      "epoch:11 step:11111 [D loss: 0.228689, acc.: 62.50%] [G loss: 0.444852]\n",
      "epoch:11 step:11112 [D loss: 0.181918, acc.: 74.22%] [G loss: 0.484024]\n",
      "epoch:11 step:11113 [D loss: 0.232048, acc.: 60.16%] [G loss: 0.394824]\n",
      "epoch:11 step:11114 [D loss: 0.193459, acc.: 68.75%] [G loss: 0.452966]\n",
      "epoch:11 step:11115 [D loss: 0.248181, acc.: 57.81%] [G loss: 0.467881]\n",
      "epoch:11 step:11116 [D loss: 0.217050, acc.: 64.84%] [G loss: 0.455528]\n",
      "epoch:11 step:11117 [D loss: 0.253811, acc.: 60.94%] [G loss: 0.427139]\n",
      "epoch:11 step:11118 [D loss: 0.221401, acc.: 61.72%] [G loss: 0.468454]\n",
      "epoch:11 step:11119 [D loss: 0.234513, acc.: 55.47%] [G loss: 0.453343]\n",
      "epoch:11 step:11120 [D loss: 0.238618, acc.: 66.41%] [G loss: 0.479459]\n",
      "epoch:11 step:11121 [D loss: 0.202928, acc.: 70.31%] [G loss: 0.491051]\n",
      "epoch:11 step:11122 [D loss: 0.229993, acc.: 58.59%] [G loss: 0.474829]\n",
      "epoch:11 step:11123 [D loss: 0.235750, acc.: 61.72%] [G loss: 0.510567]\n",
      "epoch:11 step:11124 [D loss: 0.227343, acc.: 60.94%] [G loss: 0.464886]\n",
      "epoch:11 step:11125 [D loss: 0.250077, acc.: 57.81%] [G loss: 0.457885]\n",
      "epoch:11 step:11126 [D loss: 0.219423, acc.: 66.41%] [G loss: 0.470555]\n",
      "epoch:11 step:11127 [D loss: 0.245908, acc.: 59.38%] [G loss: 0.417678]\n",
      "epoch:11 step:11128 [D loss: 0.231817, acc.: 60.94%] [G loss: 0.437108]\n",
      "epoch:11 step:11129 [D loss: 0.214537, acc.: 64.84%] [G loss: 0.431831]\n",
      "epoch:11 step:11130 [D loss: 0.201728, acc.: 67.19%] [G loss: 0.476885]\n",
      "epoch:11 step:11131 [D loss: 0.234328, acc.: 58.59%] [G loss: 0.429216]\n",
      "epoch:11 step:11132 [D loss: 0.219710, acc.: 64.84%] [G loss: 0.418931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11133 [D loss: 0.213671, acc.: 64.84%] [G loss: 0.478471]\n",
      "epoch:11 step:11134 [D loss: 0.234954, acc.: 58.59%] [G loss: 0.431025]\n",
      "epoch:11 step:11135 [D loss: 0.258474, acc.: 51.56%] [G loss: 0.412403]\n",
      "epoch:11 step:11136 [D loss: 0.211289, acc.: 69.53%] [G loss: 0.485408]\n",
      "epoch:11 step:11137 [D loss: 0.220438, acc.: 66.41%] [G loss: 0.454563]\n",
      "epoch:11 step:11138 [D loss: 0.222883, acc.: 67.97%] [G loss: 0.416616]\n",
      "epoch:11 step:11139 [D loss: 0.230458, acc.: 60.16%] [G loss: 0.426326]\n",
      "epoch:11 step:11140 [D loss: 0.209650, acc.: 67.97%] [G loss: 0.495968]\n",
      "epoch:11 step:11141 [D loss: 0.244994, acc.: 60.16%] [G loss: 0.427223]\n",
      "epoch:11 step:11142 [D loss: 0.212370, acc.: 67.19%] [G loss: 0.450205]\n",
      "epoch:11 step:11143 [D loss: 0.255620, acc.: 59.38%] [G loss: 0.495244]\n",
      "epoch:11 step:11144 [D loss: 0.184001, acc.: 76.56%] [G loss: 0.484734]\n",
      "epoch:11 step:11145 [D loss: 0.218201, acc.: 60.94%] [G loss: 0.472904]\n",
      "epoch:11 step:11146 [D loss: 0.203550, acc.: 67.19%] [G loss: 0.478119]\n",
      "epoch:11 step:11147 [D loss: 0.224228, acc.: 67.97%] [G loss: 0.415827]\n",
      "epoch:11 step:11148 [D loss: 0.215427, acc.: 63.28%] [G loss: 0.470109]\n",
      "epoch:11 step:11149 [D loss: 0.205663, acc.: 64.84%] [G loss: 0.473962]\n",
      "epoch:11 step:11150 [D loss: 0.224724, acc.: 73.44%] [G loss: 0.470780]\n",
      "epoch:11 step:11151 [D loss: 0.197865, acc.: 67.97%] [G loss: 0.500067]\n",
      "epoch:11 step:11152 [D loss: 0.188298, acc.: 68.75%] [G loss: 0.436332]\n",
      "epoch:11 step:11153 [D loss: 0.245622, acc.: 57.03%] [G loss: 0.459368]\n",
      "epoch:11 step:11154 [D loss: 0.237616, acc.: 59.38%] [G loss: 0.459604]\n",
      "epoch:11 step:11155 [D loss: 0.217288, acc.: 67.97%] [G loss: 0.448350]\n",
      "epoch:11 step:11156 [D loss: 0.202057, acc.: 67.97%] [G loss: 0.489368]\n",
      "epoch:11 step:11157 [D loss: 0.236085, acc.: 64.84%] [G loss: 0.440547]\n",
      "epoch:11 step:11158 [D loss: 0.227185, acc.: 63.28%] [G loss: 0.469835]\n",
      "epoch:11 step:11159 [D loss: 0.194321, acc.: 71.09%] [G loss: 0.433118]\n",
      "epoch:11 step:11160 [D loss: 0.206509, acc.: 71.09%] [G loss: 0.486579]\n",
      "epoch:11 step:11161 [D loss: 0.216417, acc.: 64.84%] [G loss: 0.462720]\n",
      "epoch:11 step:11162 [D loss: 0.225822, acc.: 64.06%] [G loss: 0.481559]\n",
      "epoch:11 step:11163 [D loss: 0.243449, acc.: 61.72%] [G loss: 0.419113]\n",
      "epoch:11 step:11164 [D loss: 0.212293, acc.: 67.19%] [G loss: 0.454459]\n",
      "epoch:11 step:11165 [D loss: 0.272016, acc.: 51.56%] [G loss: 0.410693]\n",
      "epoch:11 step:11166 [D loss: 0.271680, acc.: 51.56%] [G loss: 0.424637]\n",
      "epoch:11 step:11167 [D loss: 0.208415, acc.: 64.84%] [G loss: 0.482422]\n",
      "epoch:11 step:11168 [D loss: 0.254682, acc.: 53.12%] [G loss: 0.417312]\n",
      "epoch:11 step:11169 [D loss: 0.238156, acc.: 56.25%] [G loss: 0.444918]\n",
      "epoch:11 step:11170 [D loss: 0.245814, acc.: 50.78%] [G loss: 0.418420]\n",
      "epoch:11 step:11171 [D loss: 0.238555, acc.: 58.59%] [G loss: 0.408711]\n",
      "epoch:11 step:11172 [D loss: 0.237644, acc.: 57.03%] [G loss: 0.415165]\n",
      "epoch:11 step:11173 [D loss: 0.223059, acc.: 62.50%] [G loss: 0.409045]\n",
      "epoch:11 step:11174 [D loss: 0.237857, acc.: 58.59%] [G loss: 0.426455]\n",
      "epoch:11 step:11175 [D loss: 0.263458, acc.: 55.47%] [G loss: 0.401573]\n",
      "epoch:11 step:11176 [D loss: 0.236537, acc.: 59.38%] [G loss: 0.419951]\n",
      "epoch:11 step:11177 [D loss: 0.238499, acc.: 56.25%] [G loss: 0.419782]\n",
      "epoch:11 step:11178 [D loss: 0.202887, acc.: 67.19%] [G loss: 0.426594]\n",
      "epoch:11 step:11179 [D loss: 0.227872, acc.: 62.50%] [G loss: 0.486511]\n",
      "epoch:11 step:11180 [D loss: 0.248533, acc.: 58.59%] [G loss: 0.434612]\n",
      "epoch:11 step:11181 [D loss: 0.206208, acc.: 67.97%] [G loss: 0.456457]\n",
      "epoch:11 step:11182 [D loss: 0.196736, acc.: 68.75%] [G loss: 0.446782]\n",
      "epoch:11 step:11183 [D loss: 0.219444, acc.: 71.09%] [G loss: 0.450821]\n",
      "epoch:11 step:11184 [D loss: 0.217329, acc.: 62.50%] [G loss: 0.426913]\n",
      "epoch:11 step:11185 [D loss: 0.228968, acc.: 62.50%] [G loss: 0.434242]\n",
      "epoch:11 step:11186 [D loss: 0.238447, acc.: 53.91%] [G loss: 0.418925]\n",
      "epoch:11 step:11187 [D loss: 0.246337, acc.: 53.12%] [G loss: 0.419936]\n",
      "epoch:11 step:11188 [D loss: 0.239794, acc.: 59.38%] [G loss: 0.431042]\n",
      "epoch:11 step:11189 [D loss: 0.210345, acc.: 69.53%] [G loss: 0.485592]\n",
      "epoch:11 step:11190 [D loss: 0.230460, acc.: 66.41%] [G loss: 0.466952]\n",
      "epoch:11 step:11191 [D loss: 0.208989, acc.: 68.75%] [G loss: 0.501325]\n",
      "epoch:11 step:11192 [D loss: 0.219853, acc.: 64.06%] [G loss: 0.525219]\n",
      "epoch:11 step:11193 [D loss: 0.212698, acc.: 62.50%] [G loss: 0.482270]\n",
      "epoch:11 step:11194 [D loss: 0.227541, acc.: 66.41%] [G loss: 0.460389]\n",
      "epoch:11 step:11195 [D loss: 0.193733, acc.: 69.53%] [G loss: 0.464349]\n",
      "epoch:11 step:11196 [D loss: 0.201236, acc.: 64.84%] [G loss: 0.424285]\n",
      "epoch:11 step:11197 [D loss: 0.205927, acc.: 69.53%] [G loss: 0.455424]\n",
      "epoch:11 step:11198 [D loss: 0.262335, acc.: 50.78%] [G loss: 0.406851]\n",
      "epoch:11 step:11199 [D loss: 0.266561, acc.: 50.00%] [G loss: 0.431442]\n",
      "epoch:11 step:11200 [D loss: 0.202493, acc.: 71.88%] [G loss: 0.453952]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.807641\n",
      "FID: 10.657323\n",
      "0 = 11.955253853440286\n",
      "1 = 0.05406819219886872\n",
      "2 = 0.8960999846458435\n",
      "3 = 0.8744000196456909\n",
      "4 = 0.9178000092506409\n",
      "5 = 0.9140706658363342\n",
      "6 = 0.8744000196456909\n",
      "7 = 6.402302160763738\n",
      "8 = 0.07240323548016073\n",
      "9 = 0.7086499929428101\n",
      "10 = 0.7056000232696533\n",
      "11 = 0.7117000222206116\n",
      "12 = 0.7099305987358093\n",
      "13 = 0.7056000232696533\n",
      "14 = 7.807707786560059\n",
      "15 = 9.489489555358887\n",
      "16 = 0.12339963763952255\n",
      "17 = 7.80764102935791\n",
      "18 = 10.657322883605957\n",
      "epoch:11 step:11201 [D loss: 0.200752, acc.: 71.09%] [G loss: 0.476370]\n",
      "epoch:11 step:11202 [D loss: 0.228325, acc.: 58.59%] [G loss: 0.458662]\n",
      "epoch:11 step:11203 [D loss: 0.209590, acc.: 70.31%] [G loss: 0.467972]\n",
      "epoch:11 step:11204 [D loss: 0.203107, acc.: 71.09%] [G loss: 0.462508]\n",
      "epoch:11 step:11205 [D loss: 0.212048, acc.: 64.06%] [G loss: 0.460465]\n",
      "epoch:11 step:11206 [D loss: 0.166944, acc.: 75.78%] [G loss: 0.499677]\n",
      "epoch:11 step:11207 [D loss: 0.215215, acc.: 59.38%] [G loss: 0.479806]\n",
      "epoch:11 step:11208 [D loss: 0.239711, acc.: 60.94%] [G loss: 0.456250]\n",
      "epoch:11 step:11209 [D loss: 0.248591, acc.: 56.25%] [G loss: 0.423877]\n",
      "epoch:11 step:11210 [D loss: 0.213747, acc.: 68.75%] [G loss: 0.437009]\n",
      "epoch:11 step:11211 [D loss: 0.232879, acc.: 63.28%] [G loss: 0.419649]\n",
      "epoch:11 step:11212 [D loss: 0.223937, acc.: 65.62%] [G loss: 0.490917]\n",
      "epoch:11 step:11213 [D loss: 0.207357, acc.: 71.09%] [G loss: 0.509408]\n",
      "epoch:11 step:11214 [D loss: 0.236134, acc.: 60.94%] [G loss: 0.459094]\n",
      "epoch:11 step:11215 [D loss: 0.235072, acc.: 66.41%] [G loss: 0.440980]\n",
      "epoch:11 step:11216 [D loss: 0.219323, acc.: 66.41%] [G loss: 0.434137]\n",
      "epoch:11 step:11217 [D loss: 0.243874, acc.: 57.03%] [G loss: 0.480622]\n",
      "epoch:11 step:11218 [D loss: 0.195411, acc.: 73.44%] [G loss: 0.441676]\n",
      "epoch:11 step:11219 [D loss: 0.200858, acc.: 67.19%] [G loss: 0.473460]\n",
      "epoch:11 step:11220 [D loss: 0.209864, acc.: 64.84%] [G loss: 0.468507]\n",
      "epoch:11 step:11221 [D loss: 0.243333, acc.: 57.03%] [G loss: 0.456651]\n",
      "epoch:11 step:11222 [D loss: 0.247967, acc.: 53.91%] [G loss: 0.438398]\n",
      "epoch:11 step:11223 [D loss: 0.200422, acc.: 70.31%] [G loss: 0.440425]\n",
      "epoch:11 step:11224 [D loss: 0.215418, acc.: 64.84%] [G loss: 0.505063]\n",
      "epoch:11 step:11225 [D loss: 0.198812, acc.: 73.44%] [G loss: 0.560462]\n",
      "epoch:11 step:11226 [D loss: 0.211218, acc.: 67.19%] [G loss: 0.544662]\n",
      "epoch:11 step:11227 [D loss: 0.280079, acc.: 57.81%] [G loss: 0.506578]\n",
      "epoch:11 step:11228 [D loss: 0.224066, acc.: 62.50%] [G loss: 0.507115]\n",
      "epoch:11 step:11229 [D loss: 0.234984, acc.: 63.28%] [G loss: 0.476093]\n",
      "epoch:11 step:11230 [D loss: 0.198547, acc.: 65.62%] [G loss: 0.484212]\n",
      "epoch:11 step:11231 [D loss: 0.175381, acc.: 80.47%] [G loss: 0.470443]\n",
      "epoch:11 step:11232 [D loss: 0.168320, acc.: 74.22%] [G loss: 0.535040]\n",
      "epoch:11 step:11233 [D loss: 0.158075, acc.: 78.12%] [G loss: 0.603927]\n",
      "epoch:11 step:11234 [D loss: 0.201203, acc.: 66.41%] [G loss: 0.572294]\n",
      "epoch:11 step:11235 [D loss: 0.346045, acc.: 50.00%] [G loss: 0.518302]\n",
      "epoch:11 step:11236 [D loss: 0.199947, acc.: 66.41%] [G loss: 0.527702]\n",
      "epoch:11 step:11237 [D loss: 0.216415, acc.: 65.62%] [G loss: 0.526131]\n",
      "epoch:11 step:11238 [D loss: 0.277802, acc.: 52.34%] [G loss: 0.440363]\n",
      "epoch:11 step:11239 [D loss: 0.261058, acc.: 57.03%] [G loss: 0.408290]\n",
      "epoch:11 step:11240 [D loss: 0.213134, acc.: 68.75%] [G loss: 0.428429]\n",
      "epoch:11 step:11241 [D loss: 0.226106, acc.: 67.19%] [G loss: 0.468324]\n",
      "epoch:11 step:11242 [D loss: 0.199878, acc.: 72.66%] [G loss: 0.489034]\n",
      "epoch:11 step:11243 [D loss: 0.145112, acc.: 80.47%] [G loss: 0.552923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11244 [D loss: 0.210205, acc.: 69.53%] [G loss: 0.574299]\n",
      "epoch:12 step:11245 [D loss: 0.241133, acc.: 60.94%] [G loss: 0.568483]\n",
      "epoch:12 step:11246 [D loss: 0.268276, acc.: 60.16%] [G loss: 0.486530]\n",
      "epoch:12 step:11247 [D loss: 0.228389, acc.: 60.16%] [G loss: 0.489892]\n",
      "epoch:12 step:11248 [D loss: 0.229528, acc.: 67.19%] [G loss: 0.466690]\n",
      "epoch:12 step:11249 [D loss: 0.237409, acc.: 57.03%] [G loss: 0.441987]\n",
      "epoch:12 step:11250 [D loss: 0.221541, acc.: 65.62%] [G loss: 0.465067]\n",
      "epoch:12 step:11251 [D loss: 0.221366, acc.: 63.28%] [G loss: 0.492908]\n",
      "epoch:12 step:11252 [D loss: 0.231877, acc.: 64.84%] [G loss: 0.480260]\n",
      "epoch:12 step:11253 [D loss: 0.212657, acc.: 66.41%] [G loss: 0.447877]\n",
      "epoch:12 step:11254 [D loss: 0.209066, acc.: 67.19%] [G loss: 0.464380]\n",
      "epoch:12 step:11255 [D loss: 0.217782, acc.: 60.94%] [G loss: 0.467262]\n",
      "epoch:12 step:11256 [D loss: 0.216560, acc.: 64.06%] [G loss: 0.520405]\n",
      "epoch:12 step:11257 [D loss: 0.208168, acc.: 64.84%] [G loss: 0.450010]\n",
      "epoch:12 step:11258 [D loss: 0.207897, acc.: 67.19%] [G loss: 0.452127]\n",
      "epoch:12 step:11259 [D loss: 0.193692, acc.: 69.53%] [G loss: 0.505302]\n",
      "epoch:12 step:11260 [D loss: 0.185478, acc.: 71.09%] [G loss: 0.535060]\n",
      "epoch:12 step:11261 [D loss: 0.265780, acc.: 57.03%] [G loss: 0.459635]\n",
      "epoch:12 step:11262 [D loss: 0.238143, acc.: 60.16%] [G loss: 0.444946]\n",
      "epoch:12 step:11263 [D loss: 0.266274, acc.: 51.56%] [G loss: 0.436280]\n",
      "epoch:12 step:11264 [D loss: 0.255079, acc.: 50.78%] [G loss: 0.485373]\n",
      "epoch:12 step:11265 [D loss: 0.242033, acc.: 60.94%] [G loss: 0.465532]\n",
      "epoch:12 step:11266 [D loss: 0.215705, acc.: 65.62%] [G loss: 0.480982]\n",
      "epoch:12 step:11267 [D loss: 0.249961, acc.: 57.03%] [G loss: 0.438161]\n",
      "epoch:12 step:11268 [D loss: 0.206320, acc.: 71.09%] [G loss: 0.464489]\n",
      "epoch:12 step:11269 [D loss: 0.182778, acc.: 77.34%] [G loss: 0.455216]\n",
      "epoch:12 step:11270 [D loss: 0.228809, acc.: 60.16%] [G loss: 0.428344]\n",
      "epoch:12 step:11271 [D loss: 0.244604, acc.: 58.59%] [G loss: 0.408417]\n",
      "epoch:12 step:11272 [D loss: 0.219751, acc.: 64.84%] [G loss: 0.460783]\n",
      "epoch:12 step:11273 [D loss: 0.230102, acc.: 64.06%] [G loss: 0.439073]\n",
      "epoch:12 step:11274 [D loss: 0.224680, acc.: 64.06%] [G loss: 0.475301]\n",
      "epoch:12 step:11275 [D loss: 0.236913, acc.: 58.59%] [G loss: 0.416882]\n",
      "epoch:12 step:11276 [D loss: 0.205816, acc.: 65.62%] [G loss: 0.473628]\n",
      "epoch:12 step:11277 [D loss: 0.241643, acc.: 59.38%] [G loss: 0.455989]\n",
      "epoch:12 step:11278 [D loss: 0.245981, acc.: 56.25%] [G loss: 0.479593]\n",
      "epoch:12 step:11279 [D loss: 0.207476, acc.: 67.19%] [G loss: 0.445158]\n",
      "epoch:12 step:11280 [D loss: 0.216030, acc.: 65.62%] [G loss: 0.438527]\n",
      "epoch:12 step:11281 [D loss: 0.241894, acc.: 60.16%] [G loss: 0.437759]\n",
      "epoch:12 step:11282 [D loss: 0.231717, acc.: 62.50%] [G loss: 0.423180]\n",
      "epoch:12 step:11283 [D loss: 0.208513, acc.: 70.31%] [G loss: 0.450059]\n",
      "epoch:12 step:11284 [D loss: 0.175068, acc.: 68.75%] [G loss: 0.494826]\n",
      "epoch:12 step:11285 [D loss: 0.219864, acc.: 60.16%] [G loss: 0.509563]\n",
      "epoch:12 step:11286 [D loss: 0.211191, acc.: 65.62%] [G loss: 0.483053]\n",
      "epoch:12 step:11287 [D loss: 0.222015, acc.: 61.72%] [G loss: 0.423406]\n",
      "epoch:12 step:11288 [D loss: 0.258602, acc.: 53.12%] [G loss: 0.447539]\n",
      "epoch:12 step:11289 [D loss: 0.221150, acc.: 60.94%] [G loss: 0.456412]\n",
      "epoch:12 step:11290 [D loss: 0.224433, acc.: 58.59%] [G loss: 0.485341]\n",
      "epoch:12 step:11291 [D loss: 0.226775, acc.: 64.84%] [G loss: 0.466830]\n",
      "epoch:12 step:11292 [D loss: 0.200786, acc.: 72.66%] [G loss: 0.500161]\n",
      "epoch:12 step:11293 [D loss: 0.226094, acc.: 61.72%] [G loss: 0.476520]\n",
      "epoch:12 step:11294 [D loss: 0.216662, acc.: 64.06%] [G loss: 0.465284]\n",
      "epoch:12 step:11295 [D loss: 0.242696, acc.: 56.25%] [G loss: 0.466904]\n",
      "epoch:12 step:11296 [D loss: 0.236366, acc.: 63.28%] [G loss: 0.481446]\n",
      "epoch:12 step:11297 [D loss: 0.215878, acc.: 62.50%] [G loss: 0.461852]\n",
      "epoch:12 step:11298 [D loss: 0.216108, acc.: 63.28%] [G loss: 0.480413]\n",
      "epoch:12 step:11299 [D loss: 0.204712, acc.: 69.53%] [G loss: 0.468918]\n",
      "epoch:12 step:11300 [D loss: 0.209159, acc.: 69.53%] [G loss: 0.470059]\n",
      "epoch:12 step:11301 [D loss: 0.195531, acc.: 68.75%] [G loss: 0.415068]\n",
      "epoch:12 step:11302 [D loss: 0.222892, acc.: 60.16%] [G loss: 0.428918]\n",
      "epoch:12 step:11303 [D loss: 0.220015, acc.: 61.72%] [G loss: 0.441855]\n",
      "epoch:12 step:11304 [D loss: 0.212640, acc.: 67.97%] [G loss: 0.436668]\n",
      "epoch:12 step:11305 [D loss: 0.272474, acc.: 49.22%] [G loss: 0.409237]\n",
      "epoch:12 step:11306 [D loss: 0.241704, acc.: 60.16%] [G loss: 0.414334]\n",
      "epoch:12 step:11307 [D loss: 0.238246, acc.: 60.16%] [G loss: 0.439013]\n",
      "epoch:12 step:11308 [D loss: 0.232337, acc.: 60.16%] [G loss: 0.450681]\n",
      "epoch:12 step:11309 [D loss: 0.237436, acc.: 63.28%] [G loss: 0.427621]\n",
      "epoch:12 step:11310 [D loss: 0.215760, acc.: 66.41%] [G loss: 0.433846]\n",
      "epoch:12 step:11311 [D loss: 0.224537, acc.: 61.72%] [G loss: 0.419008]\n",
      "epoch:12 step:11312 [D loss: 0.231688, acc.: 60.16%] [G loss: 0.426571]\n",
      "epoch:12 step:11313 [D loss: 0.178421, acc.: 78.91%] [G loss: 0.486435]\n",
      "epoch:12 step:11314 [D loss: 0.212358, acc.: 67.19%] [G loss: 0.427058]\n",
      "epoch:12 step:11315 [D loss: 0.237843, acc.: 62.50%] [G loss: 0.449589]\n",
      "epoch:12 step:11316 [D loss: 0.245357, acc.: 54.69%] [G loss: 0.444514]\n",
      "epoch:12 step:11317 [D loss: 0.208233, acc.: 65.62%] [G loss: 0.456912]\n",
      "epoch:12 step:11318 [D loss: 0.202892, acc.: 71.09%] [G loss: 0.454999]\n",
      "epoch:12 step:11319 [D loss: 0.204394, acc.: 64.84%] [G loss: 0.472181]\n",
      "epoch:12 step:11320 [D loss: 0.209082, acc.: 65.62%] [G loss: 0.503300]\n",
      "epoch:12 step:11321 [D loss: 0.188393, acc.: 71.88%] [G loss: 0.527281]\n",
      "epoch:12 step:11322 [D loss: 0.289416, acc.: 51.56%] [G loss: 0.425357]\n",
      "epoch:12 step:11323 [D loss: 0.223763, acc.: 62.50%] [G loss: 0.444966]\n",
      "epoch:12 step:11324 [D loss: 0.218645, acc.: 63.28%] [G loss: 0.440308]\n",
      "epoch:12 step:11325 [D loss: 0.207254, acc.: 65.62%] [G loss: 0.463320]\n",
      "epoch:12 step:11326 [D loss: 0.230317, acc.: 57.81%] [G loss: 0.430305]\n",
      "epoch:12 step:11327 [D loss: 0.211124, acc.: 66.41%] [G loss: 0.472414]\n",
      "epoch:12 step:11328 [D loss: 0.220290, acc.: 64.84%] [G loss: 0.448881]\n",
      "epoch:12 step:11329 [D loss: 0.242121, acc.: 60.16%] [G loss: 0.446028]\n",
      "epoch:12 step:11330 [D loss: 0.209928, acc.: 65.62%] [G loss: 0.470808]\n",
      "epoch:12 step:11331 [D loss: 0.209606, acc.: 69.53%] [G loss: 0.462573]\n",
      "epoch:12 step:11332 [D loss: 0.222663, acc.: 63.28%] [G loss: 0.434062]\n",
      "epoch:12 step:11333 [D loss: 0.225904, acc.: 64.06%] [G loss: 0.446044]\n",
      "epoch:12 step:11334 [D loss: 0.207874, acc.: 66.41%] [G loss: 0.484493]\n",
      "epoch:12 step:11335 [D loss: 0.239364, acc.: 58.59%] [G loss: 0.454625]\n",
      "epoch:12 step:11336 [D loss: 0.180567, acc.: 71.88%] [G loss: 0.482477]\n",
      "epoch:12 step:11337 [D loss: 0.211460, acc.: 65.62%] [G loss: 0.478622]\n",
      "epoch:12 step:11338 [D loss: 0.222549, acc.: 60.94%] [G loss: 0.489306]\n",
      "epoch:12 step:11339 [D loss: 0.202654, acc.: 67.97%] [G loss: 0.488928]\n",
      "epoch:12 step:11340 [D loss: 0.202732, acc.: 70.31%] [G loss: 0.462288]\n",
      "epoch:12 step:11341 [D loss: 0.185839, acc.: 71.09%] [G loss: 0.488601]\n",
      "epoch:12 step:11342 [D loss: 0.243916, acc.: 59.38%] [G loss: 0.475567]\n",
      "epoch:12 step:11343 [D loss: 0.203087, acc.: 66.41%] [G loss: 0.479718]\n",
      "epoch:12 step:11344 [D loss: 0.160838, acc.: 78.91%] [G loss: 0.530579]\n",
      "epoch:12 step:11345 [D loss: 0.221911, acc.: 64.06%] [G loss: 0.454990]\n",
      "epoch:12 step:11346 [D loss: 0.242197, acc.: 58.59%] [G loss: 0.432043]\n",
      "epoch:12 step:11347 [D loss: 0.228251, acc.: 57.81%] [G loss: 0.433178]\n",
      "epoch:12 step:11348 [D loss: 0.207198, acc.: 68.75%] [G loss: 0.453245]\n",
      "epoch:12 step:11349 [D loss: 0.266545, acc.: 53.91%] [G loss: 0.444250]\n",
      "epoch:12 step:11350 [D loss: 0.213187, acc.: 67.19%] [G loss: 0.520992]\n",
      "epoch:12 step:11351 [D loss: 0.186221, acc.: 76.56%] [G loss: 0.561354]\n",
      "epoch:12 step:11352 [D loss: 0.288057, acc.: 48.44%] [G loss: 0.471554]\n",
      "epoch:12 step:11353 [D loss: 0.248277, acc.: 53.91%] [G loss: 0.447865]\n",
      "epoch:12 step:11354 [D loss: 0.232953, acc.: 57.81%] [G loss: 0.435429]\n",
      "epoch:12 step:11355 [D loss: 0.204393, acc.: 66.41%] [G loss: 0.455750]\n",
      "epoch:12 step:11356 [D loss: 0.190806, acc.: 71.09%] [G loss: 0.455071]\n",
      "epoch:12 step:11357 [D loss: 0.225955, acc.: 61.72%] [G loss: 0.453891]\n",
      "epoch:12 step:11358 [D loss: 0.219251, acc.: 66.41%] [G loss: 0.508081]\n",
      "epoch:12 step:11359 [D loss: 0.205849, acc.: 68.75%] [G loss: 0.514177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11360 [D loss: 0.199317, acc.: 68.75%] [G loss: 0.509718]\n",
      "epoch:12 step:11361 [D loss: 0.224965, acc.: 64.84%] [G loss: 0.479857]\n",
      "epoch:12 step:11362 [D loss: 0.238349, acc.: 60.16%] [G loss: 0.489637]\n",
      "epoch:12 step:11363 [D loss: 0.171944, acc.: 76.56%] [G loss: 0.502752]\n",
      "epoch:12 step:11364 [D loss: 0.246601, acc.: 57.03%] [G loss: 0.499888]\n",
      "epoch:12 step:11365 [D loss: 0.208957, acc.: 70.31%] [G loss: 0.494381]\n",
      "epoch:12 step:11366 [D loss: 0.178642, acc.: 73.44%] [G loss: 0.499746]\n",
      "epoch:12 step:11367 [D loss: 0.207206, acc.: 68.75%] [G loss: 0.479236]\n",
      "epoch:12 step:11368 [D loss: 0.238651, acc.: 58.59%] [G loss: 0.446435]\n",
      "epoch:12 step:11369 [D loss: 0.215791, acc.: 64.06%] [G loss: 0.474943]\n",
      "epoch:12 step:11370 [D loss: 0.201156, acc.: 68.75%] [G loss: 0.437455]\n",
      "epoch:12 step:11371 [D loss: 0.211178, acc.: 64.06%] [G loss: 0.471627]\n",
      "epoch:12 step:11372 [D loss: 0.236252, acc.: 58.59%] [G loss: 0.461719]\n",
      "epoch:12 step:11373 [D loss: 0.209408, acc.: 67.19%] [G loss: 0.472505]\n",
      "epoch:12 step:11374 [D loss: 0.193103, acc.: 75.00%] [G loss: 0.459975]\n",
      "epoch:12 step:11375 [D loss: 0.203359, acc.: 67.97%] [G loss: 0.477514]\n",
      "epoch:12 step:11376 [D loss: 0.214394, acc.: 67.19%] [G loss: 0.479629]\n",
      "epoch:12 step:11377 [D loss: 0.224431, acc.: 61.72%] [G loss: 0.466024]\n",
      "epoch:12 step:11378 [D loss: 0.220520, acc.: 62.50%] [G loss: 0.473513]\n",
      "epoch:12 step:11379 [D loss: 0.235983, acc.: 61.72%] [G loss: 0.497258]\n",
      "epoch:12 step:11380 [D loss: 0.220935, acc.: 65.62%] [G loss: 0.475367]\n",
      "epoch:12 step:11381 [D loss: 0.249269, acc.: 53.12%] [G loss: 0.431426]\n",
      "epoch:12 step:11382 [D loss: 0.247750, acc.: 59.38%] [G loss: 0.416277]\n",
      "epoch:12 step:11383 [D loss: 0.229632, acc.: 61.72%] [G loss: 0.441939]\n",
      "epoch:12 step:11384 [D loss: 0.205835, acc.: 67.19%] [G loss: 0.487801]\n",
      "epoch:12 step:11385 [D loss: 0.231176, acc.: 57.81%] [G loss: 0.437400]\n",
      "epoch:12 step:11386 [D loss: 0.216955, acc.: 60.94%] [G loss: 0.474111]\n",
      "epoch:12 step:11387 [D loss: 0.214018, acc.: 66.41%] [G loss: 0.490359]\n",
      "epoch:12 step:11388 [D loss: 0.218293, acc.: 69.53%] [G loss: 0.453084]\n",
      "epoch:12 step:11389 [D loss: 0.215246, acc.: 64.06%] [G loss: 0.452891]\n",
      "epoch:12 step:11390 [D loss: 0.208643, acc.: 67.97%] [G loss: 0.457629]\n",
      "epoch:12 step:11391 [D loss: 0.246952, acc.: 58.59%] [G loss: 0.431328]\n",
      "epoch:12 step:11392 [D loss: 0.275755, acc.: 52.34%] [G loss: 0.466114]\n",
      "epoch:12 step:11393 [D loss: 0.205838, acc.: 68.75%] [G loss: 0.493382]\n",
      "epoch:12 step:11394 [D loss: 0.246257, acc.: 54.69%] [G loss: 0.408249]\n",
      "epoch:12 step:11395 [D loss: 0.181584, acc.: 79.69%] [G loss: 0.454566]\n",
      "epoch:12 step:11396 [D loss: 0.234365, acc.: 60.16%] [G loss: 0.437362]\n",
      "epoch:12 step:11397 [D loss: 0.236453, acc.: 61.72%] [G loss: 0.499198]\n",
      "epoch:12 step:11398 [D loss: 0.241767, acc.: 57.03%] [G loss: 0.433664]\n",
      "epoch:12 step:11399 [D loss: 0.217347, acc.: 62.50%] [G loss: 0.452463]\n",
      "epoch:12 step:11400 [D loss: 0.219399, acc.: 64.84%] [G loss: 0.480316]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.808773\n",
      "FID: 9.775707\n",
      "0 = 11.964888046431552\n",
      "1 = 0.04702734034310354\n",
      "2 = 0.8952000141143799\n",
      "3 = 0.8842999935150146\n",
      "4 = 0.9060999751091003\n",
      "5 = 0.9040073752403259\n",
      "6 = 0.8842999935150146\n",
      "7 = 6.234097901326407\n",
      "8 = 0.0658781708564054\n",
      "9 = 0.6953999996185303\n",
      "10 = 0.7016000151634216\n",
      "11 = 0.6891999840736389\n",
      "12 = 0.693006694316864\n",
      "13 = 0.7016000151634216\n",
      "14 = 7.808834075927734\n",
      "15 = 9.484612464904785\n",
      "16 = 0.12730473279953003\n",
      "17 = 7.808772563934326\n",
      "18 = 9.775707244873047\n",
      "epoch:12 step:11401 [D loss: 0.226519, acc.: 63.28%] [G loss: 0.500951]\n",
      "epoch:12 step:11402 [D loss: 0.220461, acc.: 62.50%] [G loss: 0.437544]\n",
      "epoch:12 step:11403 [D loss: 0.205122, acc.: 70.31%] [G loss: 0.461202]\n",
      "epoch:12 step:11404 [D loss: 0.252801, acc.: 57.81%] [G loss: 0.465687]\n",
      "epoch:12 step:11405 [D loss: 0.225466, acc.: 64.06%] [G loss: 0.438231]\n",
      "epoch:12 step:11406 [D loss: 0.240757, acc.: 58.59%] [G loss: 0.426805]\n",
      "epoch:12 step:11407 [D loss: 0.249591, acc.: 54.69%] [G loss: 0.515121]\n",
      "epoch:12 step:11408 [D loss: 0.224385, acc.: 65.62%] [G loss: 0.444011]\n",
      "epoch:12 step:11409 [D loss: 0.215181, acc.: 67.97%] [G loss: 0.450194]\n",
      "epoch:12 step:11410 [D loss: 0.227433, acc.: 57.81%] [G loss: 0.405114]\n",
      "epoch:12 step:11411 [D loss: 0.213939, acc.: 67.97%] [G loss: 0.425951]\n",
      "epoch:12 step:11412 [D loss: 0.214410, acc.: 64.84%] [G loss: 0.458852]\n",
      "epoch:12 step:11413 [D loss: 0.219645, acc.: 63.28%] [G loss: 0.481694]\n",
      "epoch:12 step:11414 [D loss: 0.269315, acc.: 53.12%] [G loss: 0.419673]\n",
      "epoch:12 step:11415 [D loss: 0.226815, acc.: 54.69%] [G loss: 0.450220]\n",
      "epoch:12 step:11416 [D loss: 0.213745, acc.: 63.28%] [G loss: 0.429336]\n",
      "epoch:12 step:11417 [D loss: 0.234516, acc.: 56.25%] [G loss: 0.429838]\n",
      "epoch:12 step:11418 [D loss: 0.252912, acc.: 50.00%] [G loss: 0.443154]\n",
      "epoch:12 step:11419 [D loss: 0.227028, acc.: 63.28%] [G loss: 0.425085]\n",
      "epoch:12 step:11420 [D loss: 0.208679, acc.: 67.97%] [G loss: 0.431407]\n",
      "epoch:12 step:11421 [D loss: 0.229245, acc.: 58.59%] [G loss: 0.446112]\n",
      "epoch:12 step:11422 [D loss: 0.247496, acc.: 57.81%] [G loss: 0.451855]\n",
      "epoch:12 step:11423 [D loss: 0.236613, acc.: 60.16%] [G loss: 0.415615]\n",
      "epoch:12 step:11424 [D loss: 0.243941, acc.: 53.91%] [G loss: 0.422377]\n",
      "epoch:12 step:11425 [D loss: 0.249551, acc.: 62.50%] [G loss: 0.408962]\n",
      "epoch:12 step:11426 [D loss: 0.240377, acc.: 63.28%] [G loss: 0.425937]\n",
      "epoch:12 step:11427 [D loss: 0.233465, acc.: 64.06%] [G loss: 0.497856]\n",
      "epoch:12 step:11428 [D loss: 0.227935, acc.: 60.16%] [G loss: 0.463568]\n",
      "epoch:12 step:11429 [D loss: 0.223785, acc.: 64.06%] [G loss: 0.456105]\n",
      "epoch:12 step:11430 [D loss: 0.247400, acc.: 62.50%] [G loss: 0.415673]\n",
      "epoch:12 step:11431 [D loss: 0.227013, acc.: 60.94%] [G loss: 0.431897]\n",
      "epoch:12 step:11432 [D loss: 0.222566, acc.: 58.59%] [G loss: 0.453764]\n",
      "epoch:12 step:11433 [D loss: 0.235613, acc.: 60.16%] [G loss: 0.440152]\n",
      "epoch:12 step:11434 [D loss: 0.200956, acc.: 67.19%] [G loss: 0.423291]\n",
      "epoch:12 step:11435 [D loss: 0.211131, acc.: 64.06%] [G loss: 0.460048]\n",
      "epoch:12 step:11436 [D loss: 0.192042, acc.: 75.00%] [G loss: 0.469211]\n",
      "epoch:12 step:11437 [D loss: 0.216945, acc.: 65.62%] [G loss: 0.481240]\n",
      "epoch:12 step:11438 [D loss: 0.187760, acc.: 71.88%] [G loss: 0.489639]\n",
      "epoch:12 step:11439 [D loss: 0.221068, acc.: 67.19%] [G loss: 0.493687]\n",
      "epoch:12 step:11440 [D loss: 0.218333, acc.: 66.41%] [G loss: 0.506724]\n",
      "epoch:12 step:11441 [D loss: 0.217342, acc.: 67.97%] [G loss: 0.437480]\n",
      "epoch:12 step:11442 [D loss: 0.211043, acc.: 67.19%] [G loss: 0.447843]\n",
      "epoch:12 step:11443 [D loss: 0.228602, acc.: 64.06%] [G loss: 0.468339]\n",
      "epoch:12 step:11444 [D loss: 0.247050, acc.: 59.38%] [G loss: 0.428474]\n",
      "epoch:12 step:11445 [D loss: 0.232831, acc.: 62.50%] [G loss: 0.446463]\n",
      "epoch:12 step:11446 [D loss: 0.210612, acc.: 70.31%] [G loss: 0.422719]\n",
      "epoch:12 step:11447 [D loss: 0.260435, acc.: 61.72%] [G loss: 0.414961]\n",
      "epoch:12 step:11448 [D loss: 0.212222, acc.: 64.84%] [G loss: 0.444366]\n",
      "epoch:12 step:11449 [D loss: 0.230090, acc.: 63.28%] [G loss: 0.500294]\n",
      "epoch:12 step:11450 [D loss: 0.190619, acc.: 71.88%] [G loss: 0.512372]\n",
      "epoch:12 step:11451 [D loss: 0.211161, acc.: 67.19%] [G loss: 0.454914]\n",
      "epoch:12 step:11452 [D loss: 0.190091, acc.: 74.22%] [G loss: 0.454162]\n",
      "epoch:12 step:11453 [D loss: 0.203347, acc.: 66.41%] [G loss: 0.514440]\n",
      "epoch:12 step:11454 [D loss: 0.254871, acc.: 52.34%] [G loss: 0.447199]\n",
      "epoch:12 step:11455 [D loss: 0.231351, acc.: 62.50%] [G loss: 0.416639]\n",
      "epoch:12 step:11456 [D loss: 0.233899, acc.: 60.16%] [G loss: 0.404714]\n",
      "epoch:12 step:11457 [D loss: 0.207227, acc.: 69.53%] [G loss: 0.484991]\n",
      "epoch:12 step:11458 [D loss: 0.254679, acc.: 57.81%] [G loss: 0.411321]\n",
      "epoch:12 step:11459 [D loss: 0.248097, acc.: 58.59%] [G loss: 0.425960]\n",
      "epoch:12 step:11460 [D loss: 0.211814, acc.: 64.06%] [G loss: 0.437960]\n",
      "epoch:12 step:11461 [D loss: 0.228038, acc.: 68.75%] [G loss: 0.467951]\n",
      "epoch:12 step:11462 [D loss: 0.215960, acc.: 64.84%] [G loss: 0.497226]\n",
      "epoch:12 step:11463 [D loss: 0.178997, acc.: 75.78%] [G loss: 0.518579]\n",
      "epoch:12 step:11464 [D loss: 0.293478, acc.: 52.34%] [G loss: 0.438234]\n",
      "epoch:12 step:11465 [D loss: 0.181643, acc.: 73.44%] [G loss: 0.501641]\n",
      "epoch:12 step:11466 [D loss: 0.199637, acc.: 64.84%] [G loss: 0.540937]\n",
      "epoch:12 step:11467 [D loss: 0.206421, acc.: 70.31%] [G loss: 0.531859]\n",
      "epoch:12 step:11468 [D loss: 0.245113, acc.: 60.94%] [G loss: 0.468831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11469 [D loss: 0.239255, acc.: 58.59%] [G loss: 0.433143]\n",
      "epoch:12 step:11470 [D loss: 0.253895, acc.: 58.59%] [G loss: 0.448080]\n",
      "epoch:12 step:11471 [D loss: 0.249519, acc.: 57.03%] [G loss: 0.435519]\n",
      "epoch:12 step:11472 [D loss: 0.222921, acc.: 68.75%] [G loss: 0.485508]\n",
      "epoch:12 step:11473 [D loss: 0.204031, acc.: 69.53%] [G loss: 0.475161]\n",
      "epoch:12 step:11474 [D loss: 0.231899, acc.: 64.84%] [G loss: 0.463479]\n",
      "epoch:12 step:11475 [D loss: 0.184087, acc.: 71.88%] [G loss: 0.520823]\n",
      "epoch:12 step:11476 [D loss: 0.168987, acc.: 75.78%] [G loss: 0.600126]\n",
      "epoch:12 step:11477 [D loss: 0.241650, acc.: 63.28%] [G loss: 0.462848]\n",
      "epoch:12 step:11478 [D loss: 0.231256, acc.: 57.03%] [G loss: 0.454561]\n",
      "epoch:12 step:11479 [D loss: 0.214445, acc.: 67.19%] [G loss: 0.460619]\n",
      "epoch:12 step:11480 [D loss: 0.211558, acc.: 66.41%] [G loss: 0.453606]\n",
      "epoch:12 step:11481 [D loss: 0.226140, acc.: 65.62%] [G loss: 0.440147]\n",
      "epoch:12 step:11482 [D loss: 0.228373, acc.: 61.72%] [G loss: 0.428097]\n",
      "epoch:12 step:11483 [D loss: 0.224437, acc.: 60.94%] [G loss: 0.503492]\n",
      "epoch:12 step:11484 [D loss: 0.224225, acc.: 63.28%] [G loss: 0.475112]\n",
      "epoch:12 step:11485 [D loss: 0.203387, acc.: 71.88%] [G loss: 0.512413]\n",
      "epoch:12 step:11486 [D loss: 0.212019, acc.: 64.84%] [G loss: 0.488730]\n",
      "epoch:12 step:11487 [D loss: 0.223887, acc.: 60.94%] [G loss: 0.498150]\n",
      "epoch:12 step:11488 [D loss: 0.229942, acc.: 58.59%] [G loss: 0.449234]\n",
      "epoch:12 step:11489 [D loss: 0.218007, acc.: 66.41%] [G loss: 0.464368]\n",
      "epoch:12 step:11490 [D loss: 0.220604, acc.: 63.28%] [G loss: 0.466534]\n",
      "epoch:12 step:11491 [D loss: 0.200153, acc.: 69.53%] [G loss: 0.478544]\n",
      "epoch:12 step:11492 [D loss: 0.229106, acc.: 60.16%] [G loss: 0.492982]\n",
      "epoch:12 step:11493 [D loss: 0.290505, acc.: 48.44%] [G loss: 0.424071]\n",
      "epoch:12 step:11494 [D loss: 0.268408, acc.: 50.78%] [G loss: 0.433623]\n",
      "epoch:12 step:11495 [D loss: 0.219403, acc.: 63.28%] [G loss: 0.434572]\n",
      "epoch:12 step:11496 [D loss: 0.220287, acc.: 64.06%] [G loss: 0.441211]\n",
      "epoch:12 step:11497 [D loss: 0.221012, acc.: 59.38%] [G loss: 0.470122]\n",
      "epoch:12 step:11498 [D loss: 0.217702, acc.: 65.62%] [G loss: 0.464914]\n",
      "epoch:12 step:11499 [D loss: 0.224501, acc.: 62.50%] [G loss: 0.496998]\n",
      "epoch:12 step:11500 [D loss: 0.215219, acc.: 64.84%] [G loss: 0.468388]\n",
      "epoch:12 step:11501 [D loss: 0.243074, acc.: 59.38%] [G loss: 0.396946]\n",
      "epoch:12 step:11502 [D loss: 0.207976, acc.: 71.88%] [G loss: 0.451510]\n",
      "epoch:12 step:11503 [D loss: 0.222953, acc.: 62.50%] [G loss: 0.436204]\n",
      "epoch:12 step:11504 [D loss: 0.229574, acc.: 62.50%] [G loss: 0.432871]\n",
      "epoch:12 step:11505 [D loss: 0.214834, acc.: 63.28%] [G loss: 0.512691]\n",
      "epoch:12 step:11506 [D loss: 0.219753, acc.: 64.06%] [G loss: 0.467749]\n",
      "epoch:12 step:11507 [D loss: 0.269865, acc.: 60.16%] [G loss: 0.482506]\n",
      "epoch:12 step:11508 [D loss: 0.221048, acc.: 63.28%] [G loss: 0.483920]\n",
      "epoch:12 step:11509 [D loss: 0.265824, acc.: 58.59%] [G loss: 0.457310]\n",
      "epoch:12 step:11510 [D loss: 0.245861, acc.: 60.16%] [G loss: 0.452269]\n",
      "epoch:12 step:11511 [D loss: 0.227554, acc.: 63.28%] [G loss: 0.459803]\n",
      "epoch:12 step:11512 [D loss: 0.216429, acc.: 63.28%] [G loss: 0.427981]\n",
      "epoch:12 step:11513 [D loss: 0.217470, acc.: 64.84%] [G loss: 0.457805]\n",
      "epoch:12 step:11514 [D loss: 0.206208, acc.: 67.19%] [G loss: 0.454239]\n",
      "epoch:12 step:11515 [D loss: 0.202328, acc.: 71.09%] [G loss: 0.461878]\n",
      "epoch:12 step:11516 [D loss: 0.221114, acc.: 68.75%] [G loss: 0.461344]\n",
      "epoch:12 step:11517 [D loss: 0.225130, acc.: 64.06%] [G loss: 0.474009]\n",
      "epoch:12 step:11518 [D loss: 0.207995, acc.: 65.62%] [G loss: 0.477811]\n",
      "epoch:12 step:11519 [D loss: 0.186752, acc.: 68.75%] [G loss: 0.496711]\n",
      "epoch:12 step:11520 [D loss: 0.186921, acc.: 71.88%] [G loss: 0.498172]\n",
      "epoch:12 step:11521 [D loss: 0.229632, acc.: 57.81%] [G loss: 0.431037]\n",
      "epoch:12 step:11522 [D loss: 0.209804, acc.: 67.19%] [G loss: 0.454365]\n",
      "epoch:12 step:11523 [D loss: 0.225892, acc.: 62.50%] [G loss: 0.459484]\n",
      "epoch:12 step:11524 [D loss: 0.193099, acc.: 72.66%] [G loss: 0.455632]\n",
      "epoch:12 step:11525 [D loss: 0.291629, acc.: 49.22%] [G loss: 0.426139]\n",
      "epoch:12 step:11526 [D loss: 0.199047, acc.: 74.22%] [G loss: 0.457042]\n",
      "epoch:12 step:11527 [D loss: 0.205092, acc.: 69.53%] [G loss: 0.454058]\n",
      "epoch:12 step:11528 [D loss: 0.245199, acc.: 53.12%] [G loss: 0.428284]\n",
      "epoch:12 step:11529 [D loss: 0.197584, acc.: 71.09%] [G loss: 0.429888]\n",
      "epoch:12 step:11530 [D loss: 0.213137, acc.: 67.19%] [G loss: 0.457010]\n",
      "epoch:12 step:11531 [D loss: 0.226320, acc.: 62.50%] [G loss: 0.479104]\n",
      "epoch:12 step:11532 [D loss: 0.216040, acc.: 64.84%] [G loss: 0.474114]\n",
      "epoch:12 step:11533 [D loss: 0.226068, acc.: 61.72%] [G loss: 0.463565]\n",
      "epoch:12 step:11534 [D loss: 0.220999, acc.: 62.50%] [G loss: 0.470681]\n",
      "epoch:12 step:11535 [D loss: 0.253018, acc.: 59.38%] [G loss: 0.420358]\n",
      "epoch:12 step:11536 [D loss: 0.228696, acc.: 59.38%] [G loss: 0.432492]\n",
      "epoch:12 step:11537 [D loss: 0.235978, acc.: 56.25%] [G loss: 0.446640]\n",
      "epoch:12 step:11538 [D loss: 0.243067, acc.: 59.38%] [G loss: 0.407751]\n",
      "epoch:12 step:11539 [D loss: 0.233920, acc.: 57.81%] [G loss: 0.415318]\n",
      "epoch:12 step:11540 [D loss: 0.199831, acc.: 69.53%] [G loss: 0.446544]\n",
      "epoch:12 step:11541 [D loss: 0.209690, acc.: 65.62%] [G loss: 0.488330]\n",
      "epoch:12 step:11542 [D loss: 0.168734, acc.: 78.12%] [G loss: 0.475757]\n",
      "epoch:12 step:11543 [D loss: 0.199368, acc.: 66.41%] [G loss: 0.462518]\n",
      "epoch:12 step:11544 [D loss: 0.217580, acc.: 63.28%] [G loss: 0.462873]\n",
      "epoch:12 step:11545 [D loss: 0.261952, acc.: 56.25%] [G loss: 0.436857]\n",
      "epoch:12 step:11546 [D loss: 0.225203, acc.: 61.72%] [G loss: 0.452615]\n",
      "epoch:12 step:11547 [D loss: 0.211656, acc.: 68.75%] [G loss: 0.454493]\n",
      "epoch:12 step:11548 [D loss: 0.212403, acc.: 64.84%] [G loss: 0.465825]\n",
      "epoch:12 step:11549 [D loss: 0.242822, acc.: 62.50%] [G loss: 0.469236]\n",
      "epoch:12 step:11550 [D loss: 0.208821, acc.: 67.97%] [G loss: 0.471043]\n",
      "epoch:12 step:11551 [D loss: 0.220578, acc.: 64.84%] [G loss: 0.455969]\n",
      "epoch:12 step:11552 [D loss: 0.240022, acc.: 58.59%] [G loss: 0.435346]\n",
      "epoch:12 step:11553 [D loss: 0.200849, acc.: 71.09%] [G loss: 0.409335]\n",
      "epoch:12 step:11554 [D loss: 0.198452, acc.: 66.41%] [G loss: 0.426201]\n",
      "epoch:12 step:11555 [D loss: 0.230988, acc.: 60.94%] [G loss: 0.432959]\n",
      "epoch:12 step:11556 [D loss: 0.171845, acc.: 75.00%] [G loss: 0.501184]\n",
      "epoch:12 step:11557 [D loss: 0.191597, acc.: 72.66%] [G loss: 0.541369]\n",
      "epoch:12 step:11558 [D loss: 0.177645, acc.: 75.78%] [G loss: 0.550736]\n",
      "epoch:12 step:11559 [D loss: 0.202066, acc.: 71.88%] [G loss: 0.543498]\n",
      "epoch:12 step:11560 [D loss: 0.269887, acc.: 53.12%] [G loss: 0.413355]\n",
      "epoch:12 step:11561 [D loss: 0.247560, acc.: 57.81%] [G loss: 0.434556]\n",
      "epoch:12 step:11562 [D loss: 0.230569, acc.: 62.50%] [G loss: 0.415239]\n",
      "epoch:12 step:11563 [D loss: 0.211485, acc.: 67.19%] [G loss: 0.446264]\n",
      "epoch:12 step:11564 [D loss: 0.225815, acc.: 63.28%] [G loss: 0.464121]\n",
      "epoch:12 step:11565 [D loss: 0.208653, acc.: 67.19%] [G loss: 0.437182]\n",
      "epoch:12 step:11566 [D loss: 0.239986, acc.: 61.72%] [G loss: 0.424622]\n",
      "epoch:12 step:11567 [D loss: 0.238887, acc.: 60.94%] [G loss: 0.458415]\n",
      "epoch:12 step:11568 [D loss: 0.217569, acc.: 68.75%] [G loss: 0.423937]\n",
      "epoch:12 step:11569 [D loss: 0.208497, acc.: 65.62%] [G loss: 0.446380]\n",
      "epoch:12 step:11570 [D loss: 0.210314, acc.: 70.31%] [G loss: 0.436111]\n",
      "epoch:12 step:11571 [D loss: 0.212742, acc.: 64.06%] [G loss: 0.512157]\n",
      "epoch:12 step:11572 [D loss: 0.216534, acc.: 66.41%] [G loss: 0.472125]\n",
      "epoch:12 step:11573 [D loss: 0.205811, acc.: 68.75%] [G loss: 0.487841]\n",
      "epoch:12 step:11574 [D loss: 0.221121, acc.: 61.72%] [G loss: 0.449338]\n",
      "epoch:12 step:11575 [D loss: 0.204053, acc.: 67.19%] [G loss: 0.449418]\n",
      "epoch:12 step:11576 [D loss: 0.207668, acc.: 66.41%] [G loss: 0.491649]\n",
      "epoch:12 step:11577 [D loss: 0.206865, acc.: 67.19%] [G loss: 0.474844]\n",
      "epoch:12 step:11578 [D loss: 0.222229, acc.: 67.97%] [G loss: 0.485110]\n",
      "epoch:12 step:11579 [D loss: 0.195513, acc.: 71.09%] [G loss: 0.465138]\n",
      "epoch:12 step:11580 [D loss: 0.195663, acc.: 75.00%] [G loss: 0.486957]\n",
      "epoch:12 step:11581 [D loss: 0.193622, acc.: 71.88%] [G loss: 0.489590]\n",
      "epoch:12 step:11582 [D loss: 0.230715, acc.: 61.72%] [G loss: 0.470621]\n",
      "epoch:12 step:11583 [D loss: 0.206005, acc.: 67.19%] [G loss: 0.478990]\n",
      "epoch:12 step:11584 [D loss: 0.214476, acc.: 61.72%] [G loss: 0.476494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11585 [D loss: 0.260605, acc.: 61.72%] [G loss: 0.423671]\n",
      "epoch:12 step:11586 [D loss: 0.232257, acc.: 58.59%] [G loss: 0.463366]\n",
      "epoch:12 step:11587 [D loss: 0.220331, acc.: 67.19%] [G loss: 0.501422]\n",
      "epoch:12 step:11588 [D loss: 0.208541, acc.: 64.06%] [G loss: 0.487913]\n",
      "epoch:12 step:11589 [D loss: 0.222580, acc.: 64.84%] [G loss: 0.484282]\n",
      "epoch:12 step:11590 [D loss: 0.179478, acc.: 70.31%] [G loss: 0.501584]\n",
      "epoch:12 step:11591 [D loss: 0.187982, acc.: 70.31%] [G loss: 0.556192]\n",
      "epoch:12 step:11592 [D loss: 0.282220, acc.: 57.03%] [G loss: 0.470171]\n",
      "epoch:12 step:11593 [D loss: 0.243519, acc.: 61.72%] [G loss: 0.450435]\n",
      "epoch:12 step:11594 [D loss: 0.224623, acc.: 66.41%] [G loss: 0.417786]\n",
      "epoch:12 step:11595 [D loss: 0.212743, acc.: 68.75%] [G loss: 0.442264]\n",
      "epoch:12 step:11596 [D loss: 0.226367, acc.: 60.16%] [G loss: 0.470505]\n",
      "epoch:12 step:11597 [D loss: 0.189938, acc.: 71.88%] [G loss: 0.482824]\n",
      "epoch:12 step:11598 [D loss: 0.188896, acc.: 72.66%] [G loss: 0.469135]\n",
      "epoch:12 step:11599 [D loss: 0.225513, acc.: 62.50%] [G loss: 0.465364]\n",
      "epoch:12 step:11600 [D loss: 0.222929, acc.: 64.06%] [G loss: 0.475129]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.673244\n",
      "FID: 10.767250\n",
      "0 = 11.889487922501608\n",
      "1 = 0.04824604219797262\n",
      "2 = 0.8928999900817871\n",
      "3 = 0.8705000281333923\n",
      "4 = 0.9153000116348267\n",
      "5 = 0.9113274812698364\n",
      "6 = 0.8705000281333923\n",
      "7 = 6.401128887802352\n",
      "8 = 0.07145191560477093\n",
      "9 = 0.7067999839782715\n",
      "10 = 0.7106000185012817\n",
      "11 = 0.703000009059906\n",
      "12 = 0.7052401900291443\n",
      "13 = 0.7106000185012817\n",
      "14 = 7.673308372497559\n",
      "15 = 9.509626388549805\n",
      "16 = 0.12239440530538559\n",
      "17 = 7.673243999481201\n",
      "18 = 10.767250061035156\n",
      "epoch:12 step:11601 [D loss: 0.191656, acc.: 70.31%] [G loss: 0.460555]\n",
      "epoch:12 step:11602 [D loss: 0.213630, acc.: 64.84%] [G loss: 0.441652]\n",
      "epoch:12 step:11603 [D loss: 0.213188, acc.: 65.62%] [G loss: 0.458567]\n",
      "epoch:12 step:11604 [D loss: 0.211799, acc.: 67.19%] [G loss: 0.441771]\n",
      "epoch:12 step:11605 [D loss: 0.223459, acc.: 64.84%] [G loss: 0.461495]\n",
      "epoch:12 step:11606 [D loss: 0.214040, acc.: 64.06%] [G loss: 0.472060]\n",
      "epoch:12 step:11607 [D loss: 0.242537, acc.: 53.91%] [G loss: 0.424768]\n",
      "epoch:12 step:11608 [D loss: 0.220114, acc.: 63.28%] [G loss: 0.470020]\n",
      "epoch:12 step:11609 [D loss: 0.215358, acc.: 64.06%] [G loss: 0.466015]\n",
      "epoch:12 step:11610 [D loss: 0.233290, acc.: 63.28%] [G loss: 0.466716]\n",
      "epoch:12 step:11611 [D loss: 0.215787, acc.: 62.50%] [G loss: 0.448684]\n",
      "epoch:12 step:11612 [D loss: 0.240838, acc.: 61.72%] [G loss: 0.431527]\n",
      "epoch:12 step:11613 [D loss: 0.243156, acc.: 57.03%] [G loss: 0.448570]\n",
      "epoch:12 step:11614 [D loss: 0.202987, acc.: 70.31%] [G loss: 0.464901]\n",
      "epoch:12 step:11615 [D loss: 0.206318, acc.: 67.97%] [G loss: 0.489249]\n",
      "epoch:12 step:11616 [D loss: 0.250566, acc.: 57.81%] [G loss: 0.470377]\n",
      "epoch:12 step:11617 [D loss: 0.257878, acc.: 54.69%] [G loss: 0.475267]\n",
      "epoch:12 step:11618 [D loss: 0.200130, acc.: 68.75%] [G loss: 0.459354]\n",
      "epoch:12 step:11619 [D loss: 0.245061, acc.: 54.69%] [G loss: 0.484162]\n",
      "epoch:12 step:11620 [D loss: 0.270444, acc.: 50.78%] [G loss: 0.405643]\n",
      "epoch:12 step:11621 [D loss: 0.258156, acc.: 49.22%] [G loss: 0.431064]\n",
      "epoch:12 step:11622 [D loss: 0.217320, acc.: 65.62%] [G loss: 0.430863]\n",
      "epoch:12 step:11623 [D loss: 0.223845, acc.: 60.16%] [G loss: 0.441297]\n",
      "epoch:12 step:11624 [D loss: 0.250266, acc.: 57.03%] [G loss: 0.415447]\n",
      "epoch:12 step:11625 [D loss: 0.201284, acc.: 70.31%] [G loss: 0.461170]\n",
      "epoch:12 step:11626 [D loss: 0.220539, acc.: 65.62%] [G loss: 0.433148]\n",
      "epoch:12 step:11627 [D loss: 0.217783, acc.: 60.16%] [G loss: 0.447411]\n",
      "epoch:12 step:11628 [D loss: 0.194066, acc.: 70.31%] [G loss: 0.457255]\n",
      "epoch:12 step:11629 [D loss: 0.214538, acc.: 66.41%] [G loss: 0.477370]\n",
      "epoch:12 step:11630 [D loss: 0.236418, acc.: 60.94%] [G loss: 0.463454]\n",
      "epoch:12 step:11631 [D loss: 0.210049, acc.: 69.53%] [G loss: 0.463671]\n",
      "epoch:12 step:11632 [D loss: 0.244370, acc.: 54.69%] [G loss: 0.440447]\n",
      "epoch:12 step:11633 [D loss: 0.219269, acc.: 65.62%] [G loss: 0.502484]\n",
      "epoch:12 step:11634 [D loss: 0.252555, acc.: 57.81%] [G loss: 0.424439]\n",
      "epoch:12 step:11635 [D loss: 0.222593, acc.: 59.38%] [G loss: 0.462929]\n",
      "epoch:12 step:11636 [D loss: 0.230403, acc.: 63.28%] [G loss: 0.407088]\n",
      "epoch:12 step:11637 [D loss: 0.220914, acc.: 66.41%] [G loss: 0.429075]\n",
      "epoch:12 step:11638 [D loss: 0.226612, acc.: 63.28%] [G loss: 0.477317]\n",
      "epoch:12 step:11639 [D loss: 0.213227, acc.: 64.84%] [G loss: 0.454029]\n",
      "epoch:12 step:11640 [D loss: 0.260898, acc.: 53.91%] [G loss: 0.480887]\n",
      "epoch:12 step:11641 [D loss: 0.215985, acc.: 66.41%] [G loss: 0.486653]\n",
      "epoch:12 step:11642 [D loss: 0.187545, acc.: 75.00%] [G loss: 0.497607]\n",
      "epoch:12 step:11643 [D loss: 0.215189, acc.: 64.84%] [G loss: 0.470592]\n",
      "epoch:12 step:11644 [D loss: 0.283205, acc.: 56.25%] [G loss: 0.448238]\n",
      "epoch:12 step:11645 [D loss: 0.230306, acc.: 57.03%] [G loss: 0.413242]\n",
      "epoch:12 step:11646 [D loss: 0.200771, acc.: 70.31%] [G loss: 0.437192]\n",
      "epoch:12 step:11647 [D loss: 0.241557, acc.: 56.25%] [G loss: 0.461454]\n",
      "epoch:12 step:11648 [D loss: 0.225504, acc.: 63.28%] [G loss: 0.467539]\n",
      "epoch:12 step:11649 [D loss: 0.196678, acc.: 68.75%] [G loss: 0.498762]\n",
      "epoch:12 step:11650 [D loss: 0.199268, acc.: 75.00%] [G loss: 0.541747]\n",
      "epoch:12 step:11651 [D loss: 0.262135, acc.: 54.69%] [G loss: 0.501339]\n",
      "epoch:12 step:11652 [D loss: 0.239900, acc.: 59.38%] [G loss: 0.470110]\n",
      "epoch:12 step:11653 [D loss: 0.206057, acc.: 67.97%] [G loss: 0.444872]\n",
      "epoch:12 step:11654 [D loss: 0.263535, acc.: 59.38%] [G loss: 0.481192]\n",
      "epoch:12 step:11655 [D loss: 0.241699, acc.: 63.28%] [G loss: 0.441063]\n",
      "epoch:12 step:11656 [D loss: 0.247987, acc.: 55.47%] [G loss: 0.415704]\n",
      "epoch:12 step:11657 [D loss: 0.233229, acc.: 60.16%] [G loss: 0.414194]\n",
      "epoch:12 step:11658 [D loss: 0.227731, acc.: 66.41%] [G loss: 0.431668]\n",
      "epoch:12 step:11659 [D loss: 0.202217, acc.: 66.41%] [G loss: 0.472541]\n",
      "epoch:12 step:11660 [D loss: 0.209136, acc.: 64.84%] [G loss: 0.471588]\n",
      "epoch:12 step:11661 [D loss: 0.233854, acc.: 66.41%] [G loss: 0.503063]\n",
      "epoch:12 step:11662 [D loss: 0.244003, acc.: 61.72%] [G loss: 0.448804]\n",
      "epoch:12 step:11663 [D loss: 0.241772, acc.: 61.72%] [G loss: 0.439645]\n",
      "epoch:12 step:11664 [D loss: 0.249701, acc.: 57.81%] [G loss: 0.481539]\n",
      "epoch:12 step:11665 [D loss: 0.242737, acc.: 56.25%] [G loss: 0.424546]\n",
      "epoch:12 step:11666 [D loss: 0.232099, acc.: 62.50%] [G loss: 0.417615]\n",
      "epoch:12 step:11667 [D loss: 0.227679, acc.: 64.84%] [G loss: 0.416521]\n",
      "epoch:12 step:11668 [D loss: 0.217373, acc.: 67.19%] [G loss: 0.432231]\n",
      "epoch:12 step:11669 [D loss: 0.206168, acc.: 65.62%] [G loss: 0.446497]\n",
      "epoch:12 step:11670 [D loss: 0.216169, acc.: 67.19%] [G loss: 0.451277]\n",
      "epoch:12 step:11671 [D loss: 0.216372, acc.: 70.31%] [G loss: 0.501407]\n",
      "epoch:12 step:11672 [D loss: 0.190022, acc.: 71.09%] [G loss: 0.489354]\n",
      "epoch:12 step:11673 [D loss: 0.205431, acc.: 72.66%] [G loss: 0.497291]\n",
      "epoch:12 step:11674 [D loss: 0.204733, acc.: 67.19%] [G loss: 0.508970]\n",
      "epoch:12 step:11675 [D loss: 0.205241, acc.: 67.19%] [G loss: 0.454860]\n",
      "epoch:12 step:11676 [D loss: 0.233647, acc.: 57.03%] [G loss: 0.437647]\n",
      "epoch:12 step:11677 [D loss: 0.228694, acc.: 60.94%] [G loss: 0.415303]\n",
      "epoch:12 step:11678 [D loss: 0.192192, acc.: 71.09%] [G loss: 0.453827]\n",
      "epoch:12 step:11679 [D loss: 0.216312, acc.: 67.19%] [G loss: 0.490719]\n",
      "epoch:12 step:11680 [D loss: 0.197450, acc.: 69.53%] [G loss: 0.460763]\n",
      "epoch:12 step:11681 [D loss: 0.294602, acc.: 50.78%] [G loss: 0.424233]\n",
      "epoch:12 step:11682 [D loss: 0.248484, acc.: 56.25%] [G loss: 0.428599]\n",
      "epoch:12 step:11683 [D loss: 0.199941, acc.: 71.88%] [G loss: 0.494231]\n",
      "epoch:12 step:11684 [D loss: 0.207333, acc.: 65.62%] [G loss: 0.465555]\n",
      "epoch:12 step:11685 [D loss: 0.223601, acc.: 66.41%] [G loss: 0.490391]\n",
      "epoch:12 step:11686 [D loss: 0.216729, acc.: 62.50%] [G loss: 0.436035]\n",
      "epoch:12 step:11687 [D loss: 0.243812, acc.: 60.16%] [G loss: 0.452844]\n",
      "epoch:12 step:11688 [D loss: 0.204622, acc.: 71.09%] [G loss: 0.451054]\n",
      "epoch:12 step:11689 [D loss: 0.223690, acc.: 61.72%] [G loss: 0.443957]\n",
      "epoch:12 step:11690 [D loss: 0.236000, acc.: 59.38%] [G loss: 0.474780]\n",
      "epoch:12 step:11691 [D loss: 0.216563, acc.: 64.84%] [G loss: 0.477834]\n",
      "epoch:12 step:11692 [D loss: 0.258069, acc.: 53.12%] [G loss: 0.453856]\n",
      "epoch:12 step:11693 [D loss: 0.230930, acc.: 60.94%] [G loss: 0.450685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11694 [D loss: 0.207585, acc.: 73.44%] [G loss: 0.443233]\n",
      "epoch:12 step:11695 [D loss: 0.195936, acc.: 69.53%] [G loss: 0.464590]\n",
      "epoch:12 step:11696 [D loss: 0.216941, acc.: 63.28%] [G loss: 0.498227]\n",
      "epoch:12 step:11697 [D loss: 0.218504, acc.: 64.84%] [G loss: 0.490038]\n",
      "epoch:12 step:11698 [D loss: 0.238455, acc.: 60.16%] [G loss: 0.472633]\n",
      "epoch:12 step:11699 [D loss: 0.227067, acc.: 64.06%] [G loss: 0.462302]\n",
      "epoch:12 step:11700 [D loss: 0.214103, acc.: 67.19%] [G loss: 0.509172]\n",
      "epoch:12 step:11701 [D loss: 0.227746, acc.: 64.84%] [G loss: 0.450990]\n",
      "epoch:12 step:11702 [D loss: 0.275902, acc.: 56.25%] [G loss: 0.451535]\n",
      "epoch:12 step:11703 [D loss: 0.231648, acc.: 64.84%] [G loss: 0.438995]\n",
      "epoch:12 step:11704 [D loss: 0.228701, acc.: 61.72%] [G loss: 0.421898]\n",
      "epoch:12 step:11705 [D loss: 0.206821, acc.: 65.62%] [G loss: 0.462563]\n",
      "epoch:12 step:11706 [D loss: 0.237834, acc.: 57.03%] [G loss: 0.433083]\n",
      "epoch:12 step:11707 [D loss: 0.217014, acc.: 64.06%] [G loss: 0.480074]\n",
      "epoch:12 step:11708 [D loss: 0.196125, acc.: 62.50%] [G loss: 0.473680]\n",
      "epoch:12 step:11709 [D loss: 0.239392, acc.: 60.94%] [G loss: 0.436782]\n",
      "epoch:12 step:11710 [D loss: 0.237767, acc.: 62.50%] [G loss: 0.384112]\n",
      "epoch:12 step:11711 [D loss: 0.222805, acc.: 64.06%] [G loss: 0.455600]\n",
      "epoch:12 step:11712 [D loss: 0.235738, acc.: 63.28%] [G loss: 0.434554]\n",
      "epoch:12 step:11713 [D loss: 0.211196, acc.: 68.75%] [G loss: 0.433555]\n",
      "epoch:12 step:11714 [D loss: 0.221098, acc.: 66.41%] [G loss: 0.460142]\n",
      "epoch:12 step:11715 [D loss: 0.164477, acc.: 78.12%] [G loss: 0.493523]\n",
      "epoch:12 step:11716 [D loss: 0.206863, acc.: 71.88%] [G loss: 0.498765]\n",
      "epoch:12 step:11717 [D loss: 0.261974, acc.: 49.22%] [G loss: 0.426795]\n",
      "epoch:12 step:11718 [D loss: 0.218771, acc.: 65.62%] [G loss: 0.454821]\n",
      "epoch:12 step:11719 [D loss: 0.197520, acc.: 66.41%] [G loss: 0.501885]\n",
      "epoch:12 step:11720 [D loss: 0.200743, acc.: 71.09%] [G loss: 0.455731]\n",
      "epoch:12 step:11721 [D loss: 0.264120, acc.: 53.12%] [G loss: 0.415572]\n",
      "epoch:12 step:11722 [D loss: 0.271180, acc.: 54.69%] [G loss: 0.383363]\n",
      "epoch:12 step:11723 [D loss: 0.221070, acc.: 64.84%] [G loss: 0.441631]\n",
      "epoch:12 step:11724 [D loss: 0.220507, acc.: 64.06%] [G loss: 0.437513]\n",
      "epoch:12 step:11725 [D loss: 0.199597, acc.: 67.97%] [G loss: 0.486436]\n",
      "epoch:12 step:11726 [D loss: 0.273236, acc.: 49.22%] [G loss: 0.435210]\n",
      "epoch:12 step:11727 [D loss: 0.241511, acc.: 60.94%] [G loss: 0.419671]\n",
      "epoch:12 step:11728 [D loss: 0.218397, acc.: 66.41%] [G loss: 0.441508]\n",
      "epoch:12 step:11729 [D loss: 0.216031, acc.: 67.19%] [G loss: 0.426927]\n",
      "epoch:12 step:11730 [D loss: 0.237753, acc.: 53.91%] [G loss: 0.416802]\n",
      "epoch:12 step:11731 [D loss: 0.202160, acc.: 69.53%] [G loss: 0.470084]\n",
      "epoch:12 step:11732 [D loss: 0.196280, acc.: 64.06%] [G loss: 0.456463]\n",
      "epoch:12 step:11733 [D loss: 0.242456, acc.: 59.38%] [G loss: 0.452643]\n",
      "epoch:12 step:11734 [D loss: 0.248314, acc.: 51.56%] [G loss: 0.428299]\n",
      "epoch:12 step:11735 [D loss: 0.207606, acc.: 69.53%] [G loss: 0.449145]\n",
      "epoch:12 step:11736 [D loss: 0.224151, acc.: 62.50%] [G loss: 0.448960]\n",
      "epoch:12 step:11737 [D loss: 0.208902, acc.: 66.41%] [G loss: 0.449233]\n",
      "epoch:12 step:11738 [D loss: 0.219445, acc.: 65.62%] [G loss: 0.417670]\n",
      "epoch:12 step:11739 [D loss: 0.200874, acc.: 68.75%] [G loss: 0.415723]\n",
      "epoch:12 step:11740 [D loss: 0.230489, acc.: 64.84%] [G loss: 0.458254]\n",
      "epoch:12 step:11741 [D loss: 0.247135, acc.: 57.81%] [G loss: 0.418796]\n",
      "epoch:12 step:11742 [D loss: 0.211375, acc.: 68.75%] [G loss: 0.490509]\n",
      "epoch:12 step:11743 [D loss: 0.199788, acc.: 69.53%] [G loss: 0.493640]\n",
      "epoch:12 step:11744 [D loss: 0.253511, acc.: 57.03%] [G loss: 0.492461]\n",
      "epoch:12 step:11745 [D loss: 0.279407, acc.: 59.38%] [G loss: 0.456474]\n",
      "epoch:12 step:11746 [D loss: 0.229425, acc.: 60.94%] [G loss: 0.418870]\n",
      "epoch:12 step:11747 [D loss: 0.251928, acc.: 56.25%] [G loss: 0.400956]\n",
      "epoch:12 step:11748 [D loss: 0.198884, acc.: 68.75%] [G loss: 0.469560]\n",
      "epoch:12 step:11749 [D loss: 0.208217, acc.: 67.97%] [G loss: 0.451881]\n",
      "epoch:12 step:11750 [D loss: 0.209484, acc.: 67.97%] [G loss: 0.462571]\n",
      "epoch:12 step:11751 [D loss: 0.227543, acc.: 65.62%] [G loss: 0.480538]\n",
      "epoch:12 step:11752 [D loss: 0.183026, acc.: 72.66%] [G loss: 0.506329]\n",
      "epoch:12 step:11753 [D loss: 0.250420, acc.: 57.03%] [G loss: 0.460209]\n",
      "epoch:12 step:11754 [D loss: 0.222894, acc.: 63.28%] [G loss: 0.450797]\n",
      "epoch:12 step:11755 [D loss: 0.264204, acc.: 50.78%] [G loss: 0.394252]\n",
      "epoch:12 step:11756 [D loss: 0.230107, acc.: 63.28%] [G loss: 0.435304]\n",
      "epoch:12 step:11757 [D loss: 0.211024, acc.: 71.09%] [G loss: 0.413071]\n",
      "epoch:12 step:11758 [D loss: 0.222033, acc.: 61.72%] [G loss: 0.493568]\n",
      "epoch:12 step:11759 [D loss: 0.202581, acc.: 70.31%] [G loss: 0.495838]\n",
      "epoch:12 step:11760 [D loss: 0.195033, acc.: 68.75%] [G loss: 0.481380]\n",
      "epoch:12 step:11761 [D loss: 0.247039, acc.: 62.50%] [G loss: 0.435664]\n",
      "epoch:12 step:11762 [D loss: 0.239674, acc.: 58.59%] [G loss: 0.443204]\n",
      "epoch:12 step:11763 [D loss: 0.216256, acc.: 65.62%] [G loss: 0.479335]\n",
      "epoch:12 step:11764 [D loss: 0.210030, acc.: 64.84%] [G loss: 0.486490]\n",
      "epoch:12 step:11765 [D loss: 0.212201, acc.: 70.31%] [G loss: 0.463623]\n",
      "epoch:12 step:11766 [D loss: 0.199293, acc.: 71.09%] [G loss: 0.466514]\n",
      "epoch:12 step:11767 [D loss: 0.227483, acc.: 60.94%] [G loss: 0.465546]\n",
      "epoch:12 step:11768 [D loss: 0.238281, acc.: 64.06%] [G loss: 0.445390]\n",
      "epoch:12 step:11769 [D loss: 0.230905, acc.: 61.72%] [G loss: 0.448582]\n",
      "epoch:12 step:11770 [D loss: 0.202380, acc.: 65.62%] [G loss: 0.468294]\n",
      "epoch:12 step:11771 [D loss: 0.224589, acc.: 61.72%] [G loss: 0.460340]\n",
      "epoch:12 step:11772 [D loss: 0.251967, acc.: 53.91%] [G loss: 0.454573]\n",
      "epoch:12 step:11773 [D loss: 0.236783, acc.: 60.16%] [G loss: 0.444140]\n",
      "epoch:12 step:11774 [D loss: 0.243542, acc.: 60.94%] [G loss: 0.399076]\n",
      "epoch:12 step:11775 [D loss: 0.238083, acc.: 60.94%] [G loss: 0.443749]\n",
      "epoch:12 step:11776 [D loss: 0.231518, acc.: 58.59%] [G loss: 0.440071]\n",
      "epoch:12 step:11777 [D loss: 0.211402, acc.: 66.41%] [G loss: 0.440000]\n",
      "epoch:12 step:11778 [D loss: 0.184404, acc.: 70.31%] [G loss: 0.492643]\n",
      "epoch:12 step:11779 [D loss: 0.258099, acc.: 56.25%] [G loss: 0.449594]\n",
      "epoch:12 step:11780 [D loss: 0.203953, acc.: 67.19%] [G loss: 0.451284]\n",
      "epoch:12 step:11781 [D loss: 0.231109, acc.: 54.69%] [G loss: 0.433847]\n",
      "epoch:12 step:11782 [D loss: 0.218760, acc.: 64.06%] [G loss: 0.467575]\n",
      "epoch:12 step:11783 [D loss: 0.207996, acc.: 71.88%] [G loss: 0.482811]\n",
      "epoch:12 step:11784 [D loss: 0.224925, acc.: 61.72%] [G loss: 0.428978]\n",
      "epoch:12 step:11785 [D loss: 0.237352, acc.: 63.28%] [G loss: 0.417949]\n",
      "epoch:12 step:11786 [D loss: 0.252679, acc.: 57.03%] [G loss: 0.434640]\n",
      "epoch:12 step:11787 [D loss: 0.243320, acc.: 53.12%] [G loss: 0.419924]\n",
      "epoch:12 step:11788 [D loss: 0.235305, acc.: 58.59%] [G loss: 0.435225]\n",
      "epoch:12 step:11789 [D loss: 0.220294, acc.: 64.06%] [G loss: 0.442046]\n",
      "epoch:12 step:11790 [D loss: 0.222632, acc.: 63.28%] [G loss: 0.463032]\n",
      "epoch:12 step:11791 [D loss: 0.186058, acc.: 76.56%] [G loss: 0.529311]\n",
      "epoch:12 step:11792 [D loss: 0.232782, acc.: 59.38%] [G loss: 0.450642]\n",
      "epoch:12 step:11793 [D loss: 0.199863, acc.: 71.09%] [G loss: 0.492791]\n",
      "epoch:12 step:11794 [D loss: 0.199370, acc.: 70.31%] [G loss: 0.448262]\n",
      "epoch:12 step:11795 [D loss: 0.221075, acc.: 69.53%] [G loss: 0.513953]\n",
      "epoch:12 step:11796 [D loss: 0.208909, acc.: 68.75%] [G loss: 0.484959]\n",
      "epoch:12 step:11797 [D loss: 0.254628, acc.: 57.03%] [G loss: 0.468794]\n",
      "epoch:12 step:11798 [D loss: 0.186275, acc.: 69.53%] [G loss: 0.477866]\n",
      "epoch:12 step:11799 [D loss: 0.187020, acc.: 69.53%] [G loss: 0.498609]\n",
      "epoch:12 step:11800 [D loss: 0.245230, acc.: 60.94%] [G loss: 0.492659]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.761528\n",
      "FID: 9.489305\n",
      "0 = 11.937467389225956\n",
      "1 = 0.04776304192713547\n",
      "2 = 0.8927500247955322\n",
      "3 = 0.8791000247001648\n",
      "4 = 0.9064000248908997\n",
      "5 = 0.9037730097770691\n",
      "6 = 0.8791000247001648\n",
      "7 = 6.269745619273195\n",
      "8 = 0.06550525457839525\n",
      "9 = 0.705049991607666\n",
      "10 = 0.7045000195503235\n",
      "11 = 0.7056000232696533\n",
      "12 = 0.7052757740020752\n",
      "13 = 0.7045000195503235\n",
      "14 = 7.761593341827393\n",
      "15 = 9.552423477172852\n",
      "16 = 0.11275291442871094\n",
      "17 = 7.7615275382995605\n",
      "18 = 9.48930549621582\n",
      "epoch:12 step:11801 [D loss: 0.194660, acc.: 71.09%] [G loss: 0.494944]\n",
      "epoch:12 step:11802 [D loss: 0.210521, acc.: 66.41%] [G loss: 0.484671]\n",
      "epoch:12 step:11803 [D loss: 0.244796, acc.: 55.47%] [G loss: 0.453214]\n",
      "epoch:12 step:11804 [D loss: 0.238271, acc.: 56.25%] [G loss: 0.433149]\n",
      "epoch:12 step:11805 [D loss: 0.206546, acc.: 67.19%] [G loss: 0.488647]\n",
      "epoch:12 step:11806 [D loss: 0.216030, acc.: 66.41%] [G loss: 0.446929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11807 [D loss: 0.195770, acc.: 68.75%] [G loss: 0.511690]\n",
      "epoch:12 step:11808 [D loss: 0.184603, acc.: 75.00%] [G loss: 0.509752]\n",
      "epoch:12 step:11809 [D loss: 0.255673, acc.: 54.69%] [G loss: 0.470507]\n",
      "epoch:12 step:11810 [D loss: 0.277087, acc.: 56.25%] [G loss: 0.441348]\n",
      "epoch:12 step:11811 [D loss: 0.200934, acc.: 71.09%] [G loss: 0.458857]\n",
      "epoch:12 step:11812 [D loss: 0.184261, acc.: 77.34%] [G loss: 0.494211]\n",
      "epoch:12 step:11813 [D loss: 0.256364, acc.: 58.59%] [G loss: 0.448705]\n",
      "epoch:12 step:11814 [D loss: 0.213595, acc.: 66.41%] [G loss: 0.403381]\n",
      "epoch:12 step:11815 [D loss: 0.210304, acc.: 64.84%] [G loss: 0.425693]\n",
      "epoch:12 step:11816 [D loss: 0.230253, acc.: 62.50%] [G loss: 0.476344]\n",
      "epoch:12 step:11817 [D loss: 0.209316, acc.: 66.41%] [G loss: 0.465186]\n",
      "epoch:12 step:11818 [D loss: 0.186394, acc.: 75.78%] [G loss: 0.526992]\n",
      "epoch:12 step:11819 [D loss: 0.189109, acc.: 67.97%] [G loss: 0.516356]\n",
      "epoch:12 step:11820 [D loss: 0.218818, acc.: 63.28%] [G loss: 0.497847]\n",
      "epoch:12 step:11821 [D loss: 0.253785, acc.: 60.16%] [G loss: 0.411997]\n",
      "epoch:12 step:11822 [D loss: 0.233772, acc.: 60.94%] [G loss: 0.417091]\n",
      "epoch:12 step:11823 [D loss: 0.227973, acc.: 60.16%] [G loss: 0.456556]\n",
      "epoch:12 step:11824 [D loss: 0.262704, acc.: 52.34%] [G loss: 0.403872]\n",
      "epoch:12 step:11825 [D loss: 0.192365, acc.: 74.22%] [G loss: 0.482150]\n",
      "epoch:12 step:11826 [D loss: 0.209026, acc.: 69.53%] [G loss: 0.421948]\n",
      "epoch:12 step:11827 [D loss: 0.215504, acc.: 63.28%] [G loss: 0.414154]\n",
      "epoch:12 step:11828 [D loss: 0.251917, acc.: 57.03%] [G loss: 0.429114]\n",
      "epoch:12 step:11829 [D loss: 0.215470, acc.: 63.28%] [G loss: 0.447161]\n",
      "epoch:12 step:11830 [D loss: 0.232572, acc.: 61.72%] [G loss: 0.443121]\n",
      "epoch:12 step:11831 [D loss: 0.252942, acc.: 56.25%] [G loss: 0.440567]\n",
      "epoch:12 step:11832 [D loss: 0.229416, acc.: 57.03%] [G loss: 0.444623]\n",
      "epoch:12 step:11833 [D loss: 0.194633, acc.: 64.84%] [G loss: 0.469129]\n",
      "epoch:12 step:11834 [D loss: 0.232623, acc.: 64.06%] [G loss: 0.477510]\n",
      "epoch:12 step:11835 [D loss: 0.249142, acc.: 59.38%] [G loss: 0.454459]\n",
      "epoch:12 step:11836 [D loss: 0.210080, acc.: 64.84%] [G loss: 0.442555]\n",
      "epoch:12 step:11837 [D loss: 0.204127, acc.: 70.31%] [G loss: 0.483175]\n",
      "epoch:12 step:11838 [D loss: 0.223503, acc.: 62.50%] [G loss: 0.445375]\n",
      "epoch:12 step:11839 [D loss: 0.228690, acc.: 60.16%] [G loss: 0.466766]\n",
      "epoch:12 step:11840 [D loss: 0.214064, acc.: 64.06%] [G loss: 0.445994]\n",
      "epoch:12 step:11841 [D loss: 0.248257, acc.: 62.50%] [G loss: 0.459344]\n",
      "epoch:12 step:11842 [D loss: 0.214624, acc.: 68.75%] [G loss: 0.480958]\n",
      "epoch:12 step:11843 [D loss: 0.224778, acc.: 61.72%] [G loss: 0.482538]\n",
      "epoch:12 step:11844 [D loss: 0.246637, acc.: 53.91%] [G loss: 0.433248]\n",
      "epoch:12 step:11845 [D loss: 0.254929, acc.: 53.12%] [G loss: 0.417118]\n",
      "epoch:12 step:11846 [D loss: 0.218034, acc.: 60.16%] [G loss: 0.384764]\n",
      "epoch:12 step:11847 [D loss: 0.207327, acc.: 70.31%] [G loss: 0.445844]\n",
      "epoch:12 step:11848 [D loss: 0.243737, acc.: 62.50%] [G loss: 0.425742]\n",
      "epoch:12 step:11849 [D loss: 0.207620, acc.: 69.53%] [G loss: 0.433084]\n",
      "epoch:12 step:11850 [D loss: 0.216605, acc.: 61.72%] [G loss: 0.439607]\n",
      "epoch:12 step:11851 [D loss: 0.204894, acc.: 64.06%] [G loss: 0.453653]\n",
      "epoch:12 step:11852 [D loss: 0.208836, acc.: 66.41%] [G loss: 0.446527]\n",
      "epoch:12 step:11853 [D loss: 0.209085, acc.: 64.84%] [G loss: 0.431236]\n",
      "epoch:12 step:11854 [D loss: 0.220340, acc.: 63.28%] [G loss: 0.418560]\n",
      "epoch:12 step:11855 [D loss: 0.199464, acc.: 67.19%] [G loss: 0.466731]\n",
      "epoch:12 step:11856 [D loss: 0.241056, acc.: 64.84%] [G loss: 0.416287]\n",
      "epoch:12 step:11857 [D loss: 0.194447, acc.: 71.09%] [G loss: 0.455334]\n",
      "epoch:12 step:11858 [D loss: 0.235574, acc.: 56.25%] [G loss: 0.440263]\n",
      "epoch:12 step:11859 [D loss: 0.249742, acc.: 51.56%] [G loss: 0.414719]\n",
      "epoch:12 step:11860 [D loss: 0.246871, acc.: 57.81%] [G loss: 0.441132]\n",
      "epoch:12 step:11861 [D loss: 0.232411, acc.: 60.94%] [G loss: 0.468782]\n",
      "epoch:12 step:11862 [D loss: 0.215897, acc.: 67.97%] [G loss: 0.508056]\n",
      "epoch:12 step:11863 [D loss: 0.219506, acc.: 62.50%] [G loss: 0.462291]\n",
      "epoch:12 step:11864 [D loss: 0.203109, acc.: 66.41%] [G loss: 0.470273]\n",
      "epoch:12 step:11865 [D loss: 0.242770, acc.: 58.59%] [G loss: 0.471427]\n",
      "epoch:12 step:11866 [D loss: 0.229532, acc.: 59.38%] [G loss: 0.461370]\n",
      "epoch:12 step:11867 [D loss: 0.221063, acc.: 64.06%] [G loss: 0.474908]\n",
      "epoch:12 step:11868 [D loss: 0.201976, acc.: 71.09%] [G loss: 0.545319]\n",
      "epoch:12 step:11869 [D loss: 0.273018, acc.: 51.56%] [G loss: 0.456949]\n",
      "epoch:12 step:11870 [D loss: 0.235412, acc.: 60.16%] [G loss: 0.466164]\n",
      "epoch:12 step:11871 [D loss: 0.232905, acc.: 63.28%] [G loss: 0.442771]\n",
      "epoch:12 step:11872 [D loss: 0.257276, acc.: 51.56%] [G loss: 0.413194]\n",
      "epoch:12 step:11873 [D loss: 0.224330, acc.: 64.84%] [G loss: 0.440201]\n",
      "epoch:12 step:11874 [D loss: 0.203096, acc.: 67.19%] [G loss: 0.470587]\n",
      "epoch:12 step:11875 [D loss: 0.184371, acc.: 75.78%] [G loss: 0.437945]\n",
      "epoch:12 step:11876 [D loss: 0.218292, acc.: 63.28%] [G loss: 0.482666]\n",
      "epoch:12 step:11877 [D loss: 0.224653, acc.: 64.06%] [G loss: 0.450922]\n",
      "epoch:12 step:11878 [D loss: 0.192327, acc.: 71.09%] [G loss: 0.472076]\n",
      "epoch:12 step:11879 [D loss: 0.215877, acc.: 67.97%] [G loss: 0.505984]\n",
      "epoch:12 step:11880 [D loss: 0.227459, acc.: 58.59%] [G loss: 0.461949]\n",
      "epoch:12 step:11881 [D loss: 0.207635, acc.: 71.88%] [G loss: 0.466527]\n",
      "epoch:12 step:11882 [D loss: 0.216238, acc.: 67.97%] [G loss: 0.452687]\n",
      "epoch:12 step:11883 [D loss: 0.234642, acc.: 64.06%] [G loss: 0.432374]\n",
      "epoch:12 step:11884 [D loss: 0.227121, acc.: 60.16%] [G loss: 0.434249]\n",
      "epoch:12 step:11885 [D loss: 0.208631, acc.: 67.97%] [G loss: 0.465342]\n",
      "epoch:12 step:11886 [D loss: 0.161091, acc.: 77.34%] [G loss: 0.533184]\n",
      "epoch:12 step:11887 [D loss: 0.222803, acc.: 66.41%] [G loss: 0.467544]\n",
      "epoch:12 step:11888 [D loss: 0.259300, acc.: 54.69%] [G loss: 0.453727]\n",
      "epoch:12 step:11889 [D loss: 0.224461, acc.: 60.94%] [G loss: 0.478416]\n",
      "epoch:12 step:11890 [D loss: 0.239634, acc.: 60.16%] [G loss: 0.467560]\n",
      "epoch:12 step:11891 [D loss: 0.214892, acc.: 69.53%] [G loss: 0.471604]\n",
      "epoch:12 step:11892 [D loss: 0.184857, acc.: 71.88%] [G loss: 0.500025]\n",
      "epoch:12 step:11893 [D loss: 0.208450, acc.: 65.62%] [G loss: 0.496255]\n",
      "epoch:12 step:11894 [D loss: 0.221103, acc.: 64.06%] [G loss: 0.477721]\n",
      "epoch:12 step:11895 [D loss: 0.222274, acc.: 61.72%] [G loss: 0.470158]\n",
      "epoch:12 step:11896 [D loss: 0.234877, acc.: 55.47%] [G loss: 0.387429]\n",
      "epoch:12 step:11897 [D loss: 0.228153, acc.: 62.50%] [G loss: 0.426648]\n",
      "epoch:12 step:11898 [D loss: 0.207602, acc.: 67.97%] [G loss: 0.474951]\n",
      "epoch:12 step:11899 [D loss: 0.241226, acc.: 60.94%] [G loss: 0.454210]\n",
      "epoch:12 step:11900 [D loss: 0.217909, acc.: 62.50%] [G loss: 0.428231]\n",
      "epoch:12 step:11901 [D loss: 0.233528, acc.: 60.16%] [G loss: 0.426341]\n",
      "epoch:12 step:11902 [D loss: 0.222317, acc.: 62.50%] [G loss: 0.417865]\n",
      "epoch:12 step:11903 [D loss: 0.210538, acc.: 64.84%] [G loss: 0.482538]\n",
      "epoch:12 step:11904 [D loss: 0.185719, acc.: 71.09%] [G loss: 0.468332]\n",
      "epoch:12 step:11905 [D loss: 0.202637, acc.: 69.53%] [G loss: 0.479839]\n",
      "epoch:12 step:11906 [D loss: 0.210587, acc.: 69.53%] [G loss: 0.483540]\n",
      "epoch:12 step:11907 [D loss: 0.205712, acc.: 66.41%] [G loss: 0.428008]\n",
      "epoch:12 step:11908 [D loss: 0.237551, acc.: 62.50%] [G loss: 0.458668]\n",
      "epoch:12 step:11909 [D loss: 0.211735, acc.: 68.75%] [G loss: 0.483935]\n",
      "epoch:12 step:11910 [D loss: 0.195343, acc.: 68.75%] [G loss: 0.482101]\n",
      "epoch:12 step:11911 [D loss: 0.228751, acc.: 62.50%] [G loss: 0.483578]\n",
      "epoch:12 step:11912 [D loss: 0.225404, acc.: 63.28%] [G loss: 0.450467]\n",
      "epoch:12 step:11913 [D loss: 0.193006, acc.: 70.31%] [G loss: 0.448915]\n",
      "epoch:12 step:11914 [D loss: 0.246439, acc.: 52.34%] [G loss: 0.424406]\n",
      "epoch:12 step:11915 [D loss: 0.216409, acc.: 63.28%] [G loss: 0.422161]\n",
      "epoch:12 step:11916 [D loss: 0.246409, acc.: 60.16%] [G loss: 0.417476]\n",
      "epoch:12 step:11917 [D loss: 0.231169, acc.: 61.72%] [G loss: 0.415203]\n",
      "epoch:12 step:11918 [D loss: 0.232326, acc.: 54.69%] [G loss: 0.476006]\n",
      "epoch:12 step:11919 [D loss: 0.228963, acc.: 61.72%] [G loss: 0.454231]\n",
      "epoch:12 step:11920 [D loss: 0.212589, acc.: 66.41%] [G loss: 0.451703]\n",
      "epoch:12 step:11921 [D loss: 0.199976, acc.: 67.97%] [G loss: 0.479574]\n",
      "epoch:12 step:11922 [D loss: 0.213132, acc.: 64.84%] [G loss: 0.459819]\n",
      "epoch:12 step:11923 [D loss: 0.224174, acc.: 65.62%] [G loss: 0.477066]\n",
      "epoch:12 step:11924 [D loss: 0.229235, acc.: 64.84%] [G loss: 0.444816]\n",
      "epoch:12 step:11925 [D loss: 0.199975, acc.: 70.31%] [G loss: 0.445577]\n",
      "epoch:12 step:11926 [D loss: 0.244548, acc.: 56.25%] [G loss: 0.444417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11927 [D loss: 0.235135, acc.: 59.38%] [G loss: 0.418632]\n",
      "epoch:12 step:11928 [D loss: 0.210459, acc.: 68.75%] [G loss: 0.436131]\n",
      "epoch:12 step:11929 [D loss: 0.233520, acc.: 57.81%] [G loss: 0.429244]\n",
      "epoch:12 step:11930 [D loss: 0.223898, acc.: 66.41%] [G loss: 0.443652]\n",
      "epoch:12 step:11931 [D loss: 0.236624, acc.: 53.12%] [G loss: 0.472250]\n",
      "epoch:12 step:11932 [D loss: 0.215455, acc.: 68.75%] [G loss: 0.480112]\n",
      "epoch:12 step:11933 [D loss: 0.224623, acc.: 59.38%] [G loss: 0.463467]\n",
      "epoch:12 step:11934 [D loss: 0.195890, acc.: 70.31%] [G loss: 0.519079]\n",
      "epoch:12 step:11935 [D loss: 0.188708, acc.: 68.75%] [G loss: 0.496261]\n",
      "epoch:12 step:11936 [D loss: 0.204205, acc.: 68.75%] [G loss: 0.492297]\n",
      "epoch:12 step:11937 [D loss: 0.191147, acc.: 69.53%] [G loss: 0.536634]\n",
      "epoch:12 step:11938 [D loss: 0.196968, acc.: 71.88%] [G loss: 0.549074]\n",
      "epoch:12 step:11939 [D loss: 0.193454, acc.: 70.31%] [G loss: 0.506154]\n",
      "epoch:12 step:11940 [D loss: 0.225129, acc.: 62.50%] [G loss: 0.451960]\n",
      "epoch:12 step:11941 [D loss: 0.223281, acc.: 59.38%] [G loss: 0.418823]\n",
      "epoch:12 step:11942 [D loss: 0.213833, acc.: 64.84%] [G loss: 0.459412]\n",
      "epoch:12 step:11943 [D loss: 0.228635, acc.: 65.62%] [G loss: 0.418989]\n",
      "epoch:12 step:11944 [D loss: 0.209400, acc.: 66.41%] [G loss: 0.464191]\n",
      "epoch:12 step:11945 [D loss: 0.237035, acc.: 60.16%] [G loss: 0.464756]\n",
      "epoch:12 step:11946 [D loss: 0.219851, acc.: 63.28%] [G loss: 0.458032]\n",
      "epoch:12 step:11947 [D loss: 0.251660, acc.: 53.12%] [G loss: 0.436850]\n",
      "epoch:12 step:11948 [D loss: 0.225728, acc.: 60.94%] [G loss: 0.484646]\n",
      "epoch:12 step:11949 [D loss: 0.228401, acc.: 61.72%] [G loss: 0.456435]\n",
      "epoch:12 step:11950 [D loss: 0.214713, acc.: 60.94%] [G loss: 0.443436]\n",
      "epoch:12 step:11951 [D loss: 0.209082, acc.: 72.66%] [G loss: 0.460130]\n",
      "epoch:12 step:11952 [D loss: 0.200677, acc.: 69.53%] [G loss: 0.479235]\n",
      "epoch:12 step:11953 [D loss: 0.187166, acc.: 72.66%] [G loss: 0.491476]\n",
      "epoch:12 step:11954 [D loss: 0.250823, acc.: 59.38%] [G loss: 0.455069]\n",
      "epoch:12 step:11955 [D loss: 0.233936, acc.: 57.81%] [G loss: 0.470367]\n",
      "epoch:12 step:11956 [D loss: 0.196477, acc.: 67.97%] [G loss: 0.483791]\n",
      "epoch:12 step:11957 [D loss: 0.222002, acc.: 60.94%] [G loss: 0.435005]\n",
      "epoch:12 step:11958 [D loss: 0.238862, acc.: 61.72%] [G loss: 0.420334]\n",
      "epoch:12 step:11959 [D loss: 0.234770, acc.: 59.38%] [G loss: 0.443140]\n",
      "epoch:12 step:11960 [D loss: 0.257748, acc.: 50.00%] [G loss: 0.421051]\n",
      "epoch:12 step:11961 [D loss: 0.227433, acc.: 64.84%] [G loss: 0.475195]\n",
      "epoch:12 step:11962 [D loss: 0.224498, acc.: 63.28%] [G loss: 0.453723]\n",
      "epoch:12 step:11963 [D loss: 0.203692, acc.: 64.84%] [G loss: 0.462344]\n",
      "epoch:12 step:11964 [D loss: 0.230089, acc.: 63.28%] [G loss: 0.500219]\n",
      "epoch:12 step:11965 [D loss: 0.256263, acc.: 50.00%] [G loss: 0.459689]\n",
      "epoch:12 step:11966 [D loss: 0.233044, acc.: 57.81%] [G loss: 0.451472]\n",
      "epoch:12 step:11967 [D loss: 0.217540, acc.: 66.41%] [G loss: 0.437995]\n",
      "epoch:12 step:11968 [D loss: 0.226703, acc.: 60.16%] [G loss: 0.476921]\n",
      "epoch:12 step:11969 [D loss: 0.206306, acc.: 69.53%] [G loss: 0.469846]\n",
      "epoch:12 step:11970 [D loss: 0.223342, acc.: 64.06%] [G loss: 0.475624]\n",
      "epoch:12 step:11971 [D loss: 0.222783, acc.: 64.06%] [G loss: 0.460310]\n",
      "epoch:12 step:11972 [D loss: 0.198383, acc.: 71.88%] [G loss: 0.435792]\n",
      "epoch:12 step:11973 [D loss: 0.255869, acc.: 53.12%] [G loss: 0.429864]\n",
      "epoch:12 step:11974 [D loss: 0.208434, acc.: 69.53%] [G loss: 0.455698]\n",
      "epoch:12 step:11975 [D loss: 0.199844, acc.: 68.75%] [G loss: 0.500877]\n",
      "epoch:12 step:11976 [D loss: 0.223825, acc.: 66.41%] [G loss: 0.482387]\n",
      "epoch:12 step:11977 [D loss: 0.207387, acc.: 70.31%] [G loss: 0.469294]\n",
      "epoch:12 step:11978 [D loss: 0.226794, acc.: 60.94%] [G loss: 0.413154]\n",
      "epoch:12 step:11979 [D loss: 0.215346, acc.: 64.84%] [G loss: 0.449346]\n",
      "epoch:12 step:11980 [D loss: 0.228610, acc.: 66.41%] [G loss: 0.420924]\n",
      "epoch:12 step:11981 [D loss: 0.202953, acc.: 70.31%] [G loss: 0.496256]\n",
      "epoch:12 step:11982 [D loss: 0.251924, acc.: 56.25%] [G loss: 0.441185]\n",
      "epoch:12 step:11983 [D loss: 0.270353, acc.: 50.00%] [G loss: 0.444766]\n",
      "epoch:12 step:11984 [D loss: 0.230159, acc.: 59.38%] [G loss: 0.467242]\n",
      "epoch:12 step:11985 [D loss: 0.244876, acc.: 57.81%] [G loss: 0.444116]\n",
      "epoch:12 step:11986 [D loss: 0.245783, acc.: 59.38%] [G loss: 0.446326]\n",
      "epoch:12 step:11987 [D loss: 0.212637, acc.: 60.94%] [G loss: 0.484714]\n",
      "epoch:12 step:11988 [D loss: 0.232816, acc.: 64.06%] [G loss: 0.468917]\n",
      "epoch:12 step:11989 [D loss: 0.236100, acc.: 62.50%] [G loss: 0.458897]\n",
      "epoch:12 step:11990 [D loss: 0.198926, acc.: 65.62%] [G loss: 0.455499]\n",
      "epoch:12 step:11991 [D loss: 0.213529, acc.: 68.75%] [G loss: 0.471759]\n",
      "epoch:12 step:11992 [D loss: 0.227229, acc.: 58.59%] [G loss: 0.465021]\n",
      "epoch:12 step:11993 [D loss: 0.231245, acc.: 63.28%] [G loss: 0.427501]\n",
      "epoch:12 step:11994 [D loss: 0.209012, acc.: 66.41%] [G loss: 0.444524]\n",
      "epoch:12 step:11995 [D loss: 0.206097, acc.: 67.97%] [G loss: 0.455142]\n",
      "epoch:12 step:11996 [D loss: 0.223383, acc.: 61.72%] [G loss: 0.479919]\n",
      "epoch:12 step:11997 [D loss: 0.204447, acc.: 67.19%] [G loss: 0.409985]\n",
      "epoch:12 step:11998 [D loss: 0.205387, acc.: 71.88%] [G loss: 0.473272]\n",
      "epoch:12 step:11999 [D loss: 0.253371, acc.: 57.81%] [G loss: 0.407287]\n",
      "epoch:12 step:12000 [D loss: 0.203099, acc.: 67.19%] [G loss: 0.445014]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.872162\n",
      "FID: 8.787756\n",
      "0 = 11.800965522289252\n",
      "1 = 0.047277628737883506\n",
      "2 = 0.8885999917984009\n",
      "3 = 0.876800000667572\n",
      "4 = 0.9003999829292297\n",
      "5 = 0.8979926109313965\n",
      "6 = 0.876800000667572\n",
      "7 = 6.067531121823187\n",
      "8 = 0.0661769723561531\n",
      "9 = 0.6923499703407288\n",
      "10 = 0.6915000081062317\n",
      "11 = 0.6931999921798706\n",
      "12 = 0.6926775574684143\n",
      "13 = 0.6915000081062317\n",
      "14 = 7.872231960296631\n",
      "15 = 9.554874420166016\n",
      "16 = 0.11014413088560104\n",
      "17 = 7.872162342071533\n",
      "18 = 8.787755966186523\n",
      "epoch:12 step:12001 [D loss: 0.243186, acc.: 60.94%] [G loss: 0.421702]\n",
      "epoch:12 step:12002 [D loss: 0.246231, acc.: 62.50%] [G loss: 0.446391]\n",
      "epoch:12 step:12003 [D loss: 0.233592, acc.: 60.16%] [G loss: 0.447758]\n",
      "epoch:12 step:12004 [D loss: 0.219850, acc.: 64.84%] [G loss: 0.436743]\n",
      "epoch:12 step:12005 [D loss: 0.220022, acc.: 64.84%] [G loss: 0.466173]\n",
      "epoch:12 step:12006 [D loss: 0.244477, acc.: 54.69%] [G loss: 0.449488]\n",
      "epoch:12 step:12007 [D loss: 0.229379, acc.: 58.59%] [G loss: 0.418715]\n",
      "epoch:12 step:12008 [D loss: 0.220247, acc.: 58.59%] [G loss: 0.456341]\n",
      "epoch:12 step:12009 [D loss: 0.283687, acc.: 50.00%] [G loss: 0.400895]\n",
      "epoch:12 step:12010 [D loss: 0.229837, acc.: 56.25%] [G loss: 0.484084]\n",
      "epoch:12 step:12011 [D loss: 0.221897, acc.: 64.06%] [G loss: 0.432031]\n",
      "epoch:12 step:12012 [D loss: 0.249883, acc.: 59.38%] [G loss: 0.472320]\n",
      "epoch:12 step:12013 [D loss: 0.204669, acc.: 68.75%] [G loss: 0.498374]\n",
      "epoch:12 step:12014 [D loss: 0.222575, acc.: 66.41%] [G loss: 0.458411]\n",
      "epoch:12 step:12015 [D loss: 0.226320, acc.: 55.47%] [G loss: 0.448401]\n",
      "epoch:12 step:12016 [D loss: 0.244234, acc.: 57.03%] [G loss: 0.429652]\n",
      "epoch:12 step:12017 [D loss: 0.210406, acc.: 63.28%] [G loss: 0.444841]\n",
      "epoch:12 step:12018 [D loss: 0.227183, acc.: 61.72%] [G loss: 0.462343]\n",
      "epoch:12 step:12019 [D loss: 0.204907, acc.: 74.22%] [G loss: 0.492321]\n",
      "epoch:12 step:12020 [D loss: 0.248520, acc.: 56.25%] [G loss: 0.456027]\n",
      "epoch:12 step:12021 [D loss: 0.231788, acc.: 56.25%] [G loss: 0.456226]\n",
      "epoch:12 step:12022 [D loss: 0.200698, acc.: 72.66%] [G loss: 0.448227]\n",
      "epoch:12 step:12023 [D loss: 0.225506, acc.: 67.19%] [G loss: 0.447686]\n",
      "epoch:12 step:12024 [D loss: 0.214808, acc.: 65.62%] [G loss: 0.459134]\n",
      "epoch:12 step:12025 [D loss: 0.222554, acc.: 65.62%] [G loss: 0.509313]\n",
      "epoch:12 step:12026 [D loss: 0.186046, acc.: 73.44%] [G loss: 0.533840]\n",
      "epoch:12 step:12027 [D loss: 0.259943, acc.: 57.03%] [G loss: 0.496529]\n",
      "epoch:12 step:12028 [D loss: 0.276248, acc.: 50.00%] [G loss: 0.400202]\n",
      "epoch:12 step:12029 [D loss: 0.232388, acc.: 61.72%] [G loss: 0.413340]\n",
      "epoch:12 step:12030 [D loss: 0.181488, acc.: 72.66%] [G loss: 0.471974]\n",
      "epoch:12 step:12031 [D loss: 0.253549, acc.: 56.25%] [G loss: 0.425968]\n",
      "epoch:12 step:12032 [D loss: 0.249309, acc.: 53.12%] [G loss: 0.421809]\n",
      "epoch:12 step:12033 [D loss: 0.244263, acc.: 58.59%] [G loss: 0.444243]\n",
      "epoch:12 step:12034 [D loss: 0.205902, acc.: 68.75%] [G loss: 0.467791]\n",
      "epoch:12 step:12035 [D loss: 0.275145, acc.: 48.44%] [G loss: 0.437201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:12036 [D loss: 0.219163, acc.: 67.97%] [G loss: 0.432883]\n",
      "epoch:12 step:12037 [D loss: 0.214311, acc.: 64.06%] [G loss: 0.473355]\n",
      "epoch:12 step:12038 [D loss: 0.256810, acc.: 48.44%] [G loss: 0.443388]\n",
      "epoch:12 step:12039 [D loss: 0.258306, acc.: 57.81%] [G loss: 0.488185]\n",
      "epoch:12 step:12040 [D loss: 0.191421, acc.: 70.31%] [G loss: 0.479928]\n",
      "epoch:12 step:12041 [D loss: 0.254249, acc.: 57.81%] [G loss: 0.457562]\n",
      "epoch:12 step:12042 [D loss: 0.245591, acc.: 60.16%] [G loss: 0.429811]\n",
      "epoch:12 step:12043 [D loss: 0.239040, acc.: 62.50%] [G loss: 0.461363]\n",
      "epoch:12 step:12044 [D loss: 0.222046, acc.: 61.72%] [G loss: 0.471763]\n",
      "epoch:12 step:12045 [D loss: 0.190938, acc.: 74.22%] [G loss: 0.500386]\n",
      "epoch:12 step:12046 [D loss: 0.211526, acc.: 69.53%] [G loss: 0.551953]\n",
      "epoch:12 step:12047 [D loss: 0.235759, acc.: 62.50%] [G loss: 0.480819]\n",
      "epoch:12 step:12048 [D loss: 0.234376, acc.: 65.62%] [G loss: 0.469341]\n",
      "epoch:12 step:12049 [D loss: 0.224021, acc.: 67.19%] [G loss: 0.449916]\n",
      "epoch:12 step:12050 [D loss: 0.234235, acc.: 60.16%] [G loss: 0.438129]\n",
      "epoch:12 step:12051 [D loss: 0.215406, acc.: 64.06%] [G loss: 0.438616]\n",
      "epoch:12 step:12052 [D loss: 0.233496, acc.: 59.38%] [G loss: 0.450837]\n",
      "epoch:12 step:12053 [D loss: 0.243517, acc.: 57.03%] [G loss: 0.430411]\n",
      "epoch:12 step:12054 [D loss: 0.221151, acc.: 61.72%] [G loss: 0.414844]\n",
      "epoch:12 step:12055 [D loss: 0.210447, acc.: 70.31%] [G loss: 0.491669]\n",
      "epoch:12 step:12056 [D loss: 0.229607, acc.: 58.59%] [G loss: 0.442994]\n",
      "epoch:12 step:12057 [D loss: 0.243542, acc.: 59.38%] [G loss: 0.465117]\n",
      "epoch:12 step:12058 [D loss: 0.226741, acc.: 54.69%] [G loss: 0.488779]\n",
      "epoch:12 step:12059 [D loss: 0.200631, acc.: 66.41%] [G loss: 0.471656]\n",
      "epoch:12 step:12060 [D loss: 0.215640, acc.: 64.06%] [G loss: 0.495628]\n",
      "epoch:12 step:12061 [D loss: 0.229770, acc.: 62.50%] [G loss: 0.469932]\n",
      "epoch:12 step:12062 [D loss: 0.241443, acc.: 63.28%] [G loss: 0.458514]\n",
      "epoch:12 step:12063 [D loss: 0.190780, acc.: 75.00%] [G loss: 0.473036]\n",
      "epoch:12 step:12064 [D loss: 0.238171, acc.: 62.50%] [G loss: 0.442453]\n",
      "epoch:12 step:12065 [D loss: 0.236288, acc.: 53.91%] [G loss: 0.472669]\n",
      "epoch:12 step:12066 [D loss: 0.218329, acc.: 63.28%] [G loss: 0.451764]\n",
      "epoch:12 step:12067 [D loss: 0.217318, acc.: 64.06%] [G loss: 0.466118]\n",
      "epoch:12 step:12068 [D loss: 0.247133, acc.: 60.16%] [G loss: 0.455677]\n",
      "epoch:12 step:12069 [D loss: 0.208716, acc.: 68.75%] [G loss: 0.460808]\n",
      "epoch:12 step:12070 [D loss: 0.231829, acc.: 60.16%] [G loss: 0.453162]\n",
      "epoch:12 step:12071 [D loss: 0.265847, acc.: 49.22%] [G loss: 0.472580]\n",
      "epoch:12 step:12072 [D loss: 0.271105, acc.: 51.56%] [G loss: 0.437083]\n",
      "epoch:12 step:12073 [D loss: 0.238331, acc.: 57.81%] [G loss: 0.439928]\n",
      "epoch:12 step:12074 [D loss: 0.214483, acc.: 66.41%] [G loss: 0.482061]\n",
      "epoch:12 step:12075 [D loss: 0.209680, acc.: 67.97%] [G loss: 0.426315]\n",
      "epoch:12 step:12076 [D loss: 0.236192, acc.: 60.94%] [G loss: 0.440527]\n",
      "epoch:12 step:12077 [D loss: 0.202476, acc.: 70.31%] [G loss: 0.472702]\n",
      "epoch:12 step:12078 [D loss: 0.230079, acc.: 57.81%] [G loss: 0.450424]\n",
      "epoch:12 step:12079 [D loss: 0.208127, acc.: 67.97%] [G loss: 0.471395]\n",
      "epoch:12 step:12080 [D loss: 0.220611, acc.: 64.84%] [G loss: 0.470487]\n",
      "epoch:12 step:12081 [D loss: 0.209418, acc.: 64.06%] [G loss: 0.437253]\n",
      "epoch:12 step:12082 [D loss: 0.196554, acc.: 67.97%] [G loss: 0.468668]\n",
      "epoch:12 step:12083 [D loss: 0.223635, acc.: 59.38%] [G loss: 0.446145]\n",
      "epoch:12 step:12084 [D loss: 0.216158, acc.: 64.84%] [G loss: 0.436657]\n",
      "epoch:12 step:12085 [D loss: 0.212322, acc.: 64.06%] [G loss: 0.462072]\n",
      "epoch:12 step:12086 [D loss: 0.198552, acc.: 70.31%] [G loss: 0.450301]\n",
      "epoch:12 step:12087 [D loss: 0.214876, acc.: 64.84%] [G loss: 0.488631]\n",
      "epoch:12 step:12088 [D loss: 0.235604, acc.: 54.69%] [G loss: 0.412387]\n",
      "epoch:12 step:12089 [D loss: 0.218397, acc.: 66.41%] [G loss: 0.431591]\n",
      "epoch:12 step:12090 [D loss: 0.253331, acc.: 57.81%] [G loss: 0.431079]\n",
      "epoch:12 step:12091 [D loss: 0.233797, acc.: 56.25%] [G loss: 0.438449]\n",
      "epoch:12 step:12092 [D loss: 0.239369, acc.: 60.16%] [G loss: 0.422672]\n",
      "epoch:12 step:12093 [D loss: 0.215495, acc.: 64.06%] [G loss: 0.403770]\n",
      "epoch:12 step:12094 [D loss: 0.238831, acc.: 58.59%] [G loss: 0.417375]\n",
      "epoch:12 step:12095 [D loss: 0.246090, acc.: 54.69%] [G loss: 0.424089]\n",
      "epoch:12 step:12096 [D loss: 0.202743, acc.: 67.97%] [G loss: 0.473466]\n",
      "epoch:12 step:12097 [D loss: 0.199054, acc.: 67.97%] [G loss: 0.502737]\n",
      "epoch:12 step:12098 [D loss: 0.236982, acc.: 61.72%] [G loss: 0.422264]\n",
      "epoch:12 step:12099 [D loss: 0.224272, acc.: 63.28%] [G loss: 0.418440]\n",
      "epoch:12 step:12100 [D loss: 0.238862, acc.: 60.16%] [G loss: 0.438343]\n",
      "epoch:12 step:12101 [D loss: 0.219180, acc.: 63.28%] [G loss: 0.424697]\n",
      "epoch:12 step:12102 [D loss: 0.273927, acc.: 45.31%] [G loss: 0.373576]\n",
      "epoch:12 step:12103 [D loss: 0.255081, acc.: 55.47%] [G loss: 0.404724]\n",
      "epoch:12 step:12104 [D loss: 0.223111, acc.: 62.50%] [G loss: 0.410502]\n",
      "epoch:12 step:12105 [D loss: 0.247914, acc.: 56.25%] [G loss: 0.400387]\n",
      "epoch:12 step:12106 [D loss: 0.224730, acc.: 61.72%] [G loss: 0.406245]\n",
      "epoch:12 step:12107 [D loss: 0.200249, acc.: 66.41%] [G loss: 0.468659]\n",
      "epoch:12 step:12108 [D loss: 0.257741, acc.: 58.59%] [G loss: 0.412601]\n",
      "epoch:12 step:12109 [D loss: 0.250664, acc.: 60.94%] [G loss: 0.410719]\n",
      "epoch:12 step:12110 [D loss: 0.222569, acc.: 65.62%] [G loss: 0.459302]\n",
      "epoch:12 step:12111 [D loss: 0.247016, acc.: 56.25%] [G loss: 0.409245]\n",
      "epoch:12 step:12112 [D loss: 0.238183, acc.: 58.59%] [G loss: 0.392684]\n",
      "epoch:12 step:12113 [D loss: 0.249381, acc.: 57.81%] [G loss: 0.455070]\n",
      "epoch:12 step:12114 [D loss: 0.224742, acc.: 61.72%] [G loss: 0.469447]\n",
      "epoch:12 step:12115 [D loss: 0.217566, acc.: 64.06%] [G loss: 0.470027]\n",
      "epoch:12 step:12116 [D loss: 0.235062, acc.: 60.16%] [G loss: 0.482745]\n",
      "epoch:12 step:12117 [D loss: 0.214704, acc.: 67.19%] [G loss: 0.460678]\n",
      "epoch:12 step:12118 [D loss: 0.204634, acc.: 68.75%] [G loss: 0.470550]\n",
      "epoch:12 step:12119 [D loss: 0.205638, acc.: 65.62%] [G loss: 0.452955]\n",
      "epoch:12 step:12120 [D loss: 0.253227, acc.: 56.25%] [G loss: 0.455848]\n",
      "epoch:12 step:12121 [D loss: 0.225476, acc.: 64.06%] [G loss: 0.466050]\n",
      "epoch:12 step:12122 [D loss: 0.231706, acc.: 57.03%] [G loss: 0.427917]\n",
      "epoch:12 step:12123 [D loss: 0.240513, acc.: 57.03%] [G loss: 0.422565]\n",
      "epoch:12 step:12124 [D loss: 0.228281, acc.: 59.38%] [G loss: 0.435583]\n",
      "epoch:12 step:12125 [D loss: 0.232885, acc.: 57.03%] [G loss: 0.417016]\n",
      "epoch:12 step:12126 [D loss: 0.213298, acc.: 62.50%] [G loss: 0.418202]\n",
      "epoch:12 step:12127 [D loss: 0.255191, acc.: 57.03%] [G loss: 0.407892]\n",
      "epoch:12 step:12128 [D loss: 0.207163, acc.: 65.62%] [G loss: 0.477781]\n",
      "epoch:12 step:12129 [D loss: 0.201349, acc.: 68.75%] [G loss: 0.524145]\n",
      "epoch:12 step:12130 [D loss: 0.191963, acc.: 77.34%] [G loss: 0.495094]\n",
      "epoch:12 step:12131 [D loss: 0.233997, acc.: 67.19%] [G loss: 0.469813]\n",
      "epoch:12 step:12132 [D loss: 0.210586, acc.: 64.06%] [G loss: 0.488313]\n",
      "epoch:12 step:12133 [D loss: 0.211584, acc.: 64.84%] [G loss: 0.467464]\n",
      "epoch:12 step:12134 [D loss: 0.178213, acc.: 72.66%] [G loss: 0.469534]\n",
      "epoch:12 step:12135 [D loss: 0.283081, acc.: 48.44%] [G loss: 0.405507]\n",
      "epoch:12 step:12136 [D loss: 0.288912, acc.: 48.44%] [G loss: 0.414096]\n",
      "epoch:12 step:12137 [D loss: 0.194969, acc.: 70.31%] [G loss: 0.466496]\n",
      "epoch:12 step:12138 [D loss: 0.194684, acc.: 67.19%] [G loss: 0.476684]\n",
      "epoch:12 step:12139 [D loss: 0.229452, acc.: 61.72%] [G loss: 0.502457]\n",
      "epoch:12 step:12140 [D loss: 0.210762, acc.: 66.41%] [G loss: 0.471266]\n",
      "epoch:12 step:12141 [D loss: 0.193218, acc.: 69.53%] [G loss: 0.499553]\n",
      "epoch:12 step:12142 [D loss: 0.229671, acc.: 60.16%] [G loss: 0.405638]\n",
      "epoch:12 step:12143 [D loss: 0.195839, acc.: 71.09%] [G loss: 0.473004]\n",
      "epoch:12 step:12144 [D loss: 0.181711, acc.: 72.66%] [G loss: 0.527131]\n",
      "epoch:12 step:12145 [D loss: 0.215360, acc.: 67.19%] [G loss: 0.480199]\n",
      "epoch:12 step:12146 [D loss: 0.239792, acc.: 60.16%] [G loss: 0.424284]\n",
      "epoch:12 step:12147 [D loss: 0.214515, acc.: 66.41%] [G loss: 0.475084]\n",
      "epoch:12 step:12148 [D loss: 0.220594, acc.: 68.75%] [G loss: 0.424425]\n",
      "epoch:12 step:12149 [D loss: 0.212307, acc.: 64.84%] [G loss: 0.455035]\n",
      "epoch:12 step:12150 [D loss: 0.188351, acc.: 70.31%] [G loss: 0.498281]\n",
      "epoch:12 step:12151 [D loss: 0.262127, acc.: 54.69%] [G loss: 0.477501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:12152 [D loss: 0.211648, acc.: 66.41%] [G loss: 0.454276]\n",
      "epoch:12 step:12153 [D loss: 0.194996, acc.: 71.09%] [G loss: 0.464235]\n",
      "epoch:12 step:12154 [D loss: 0.235397, acc.: 58.59%] [G loss: 0.462230]\n",
      "epoch:12 step:12155 [D loss: 0.217761, acc.: 63.28%] [G loss: 0.407798]\n",
      "epoch:12 step:12156 [D loss: 0.217530, acc.: 64.06%] [G loss: 0.479020]\n",
      "epoch:12 step:12157 [D loss: 0.257557, acc.: 58.59%] [G loss: 0.451607]\n",
      "epoch:12 step:12158 [D loss: 0.233963, acc.: 62.50%] [G loss: 0.473011]\n",
      "epoch:12 step:12159 [D loss: 0.262819, acc.: 56.25%] [G loss: 0.412476]\n",
      "epoch:12 step:12160 [D loss: 0.208737, acc.: 66.41%] [G loss: 0.456916]\n",
      "epoch:12 step:12161 [D loss: 0.225190, acc.: 65.62%] [G loss: 0.481128]\n",
      "epoch:12 step:12162 [D loss: 0.186178, acc.: 72.66%] [G loss: 0.511151]\n",
      "epoch:12 step:12163 [D loss: 0.205844, acc.: 69.53%] [G loss: 0.511817]\n",
      "epoch:12 step:12164 [D loss: 0.279527, acc.: 53.12%] [G loss: 0.423789]\n",
      "epoch:12 step:12165 [D loss: 0.207714, acc.: 67.19%] [G loss: 0.450215]\n",
      "epoch:12 step:12166 [D loss: 0.220308, acc.: 67.19%] [G loss: 0.462115]\n",
      "epoch:12 step:12167 [D loss: 0.188581, acc.: 71.09%] [G loss: 0.435873]\n",
      "epoch:12 step:12168 [D loss: 0.182026, acc.: 75.78%] [G loss: 0.508325]\n",
      "epoch:12 step:12169 [D loss: 0.174810, acc.: 75.00%] [G loss: 0.509154]\n",
      "epoch:12 step:12170 [D loss: 0.182689, acc.: 75.00%] [G loss: 0.567030]\n",
      "epoch:12 step:12171 [D loss: 0.212385, acc.: 64.06%] [G loss: 0.497783]\n",
      "epoch:12 step:12172 [D loss: 0.320817, acc.: 56.25%] [G loss: 0.495741]\n",
      "epoch:12 step:12173 [D loss: 0.232626, acc.: 63.28%] [G loss: 0.544176]\n",
      "epoch:12 step:12174 [D loss: 0.222735, acc.: 66.41%] [G loss: 0.503676]\n",
      "epoch:12 step:12175 [D loss: 0.258164, acc.: 60.16%] [G loss: 0.461307]\n",
      "epoch:12 step:12176 [D loss: 0.243053, acc.: 62.50%] [G loss: 0.451388]\n",
      "epoch:12 step:12177 [D loss: 0.215102, acc.: 71.88%] [G loss: 0.473067]\n",
      "epoch:12 step:12178 [D loss: 0.202912, acc.: 72.66%] [G loss: 0.484802]\n",
      "epoch:12 step:12179 [D loss: 0.198104, acc.: 71.88%] [G loss: 0.468226]\n",
      "epoch:12 step:12180 [D loss: 0.165490, acc.: 78.12%] [G loss: 0.519361]\n",
      "epoch:12 step:12181 [D loss: 0.194123, acc.: 75.00%] [G loss: 0.595124]\n",
      "epoch:13 step:12182 [D loss: 0.257370, acc.: 63.28%] [G loss: 0.558431]\n",
      "epoch:13 step:12183 [D loss: 0.257171, acc.: 59.38%] [G loss: 0.467483]\n",
      "epoch:13 step:12184 [D loss: 0.246216, acc.: 60.94%] [G loss: 0.472470]\n",
      "epoch:13 step:12185 [D loss: 0.231223, acc.: 60.16%] [G loss: 0.436940]\n",
      "epoch:13 step:12186 [D loss: 0.231820, acc.: 60.16%] [G loss: 0.478153]\n",
      "epoch:13 step:12187 [D loss: 0.210865, acc.: 65.62%] [G loss: 0.456037]\n",
      "epoch:13 step:12188 [D loss: 0.198941, acc.: 67.19%] [G loss: 0.534546]\n",
      "epoch:13 step:12189 [D loss: 0.219942, acc.: 61.72%] [G loss: 0.451133]\n",
      "epoch:13 step:12190 [D loss: 0.199771, acc.: 67.19%] [G loss: 0.480633]\n",
      "epoch:13 step:12191 [D loss: 0.209216, acc.: 68.75%] [G loss: 0.440851]\n",
      "epoch:13 step:12192 [D loss: 0.219066, acc.: 63.28%] [G loss: 0.513511]\n",
      "epoch:13 step:12193 [D loss: 0.239362, acc.: 62.50%] [G loss: 0.453877]\n",
      "epoch:13 step:12194 [D loss: 0.227266, acc.: 64.84%] [G loss: 0.438039]\n",
      "epoch:13 step:12195 [D loss: 0.230168, acc.: 65.62%] [G loss: 0.443769]\n",
      "epoch:13 step:12196 [D loss: 0.201356, acc.: 69.53%] [G loss: 0.447795]\n",
      "epoch:13 step:12197 [D loss: 0.191056, acc.: 73.44%] [G loss: 0.536640]\n",
      "epoch:13 step:12198 [D loss: 0.253300, acc.: 54.69%] [G loss: 0.497008]\n",
      "epoch:13 step:12199 [D loss: 0.237378, acc.: 59.38%] [G loss: 0.444037]\n",
      "epoch:13 step:12200 [D loss: 0.240459, acc.: 62.50%] [G loss: 0.448856]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.718908\n",
      "FID: 12.010916\n",
      "0 = 11.957694910025612\n",
      "1 = 0.05268914437584457\n",
      "2 = 0.8959000110626221\n",
      "3 = 0.8751000165939331\n",
      "4 = 0.916700005531311\n",
      "5 = 0.9130843281745911\n",
      "6 = 0.8751000165939331\n",
      "7 = 6.4231315408885505\n",
      "8 = 0.07711752743124857\n",
      "9 = 0.699150025844574\n",
      "10 = 0.6956999897956848\n",
      "11 = 0.7026000022888184\n",
      "12 = 0.7005336880683899\n",
      "13 = 0.6956999897956848\n",
      "14 = 7.718973636627197\n",
      "15 = 9.384610176086426\n",
      "16 = 0.14791616797447205\n",
      "17 = 7.718908309936523\n",
      "18 = 12.010915756225586\n",
      "epoch:13 step:12201 [D loss: 0.236349, acc.: 57.81%] [G loss: 0.463306]\n",
      "epoch:13 step:12202 [D loss: 0.226099, acc.: 60.94%] [G loss: 0.468809]\n",
      "epoch:13 step:12203 [D loss: 0.215050, acc.: 71.09%] [G loss: 0.518759]\n",
      "epoch:13 step:12204 [D loss: 0.221772, acc.: 63.28%] [G loss: 0.438889]\n",
      "epoch:13 step:12205 [D loss: 0.237839, acc.: 59.38%] [G loss: 0.451469]\n",
      "epoch:13 step:12206 [D loss: 0.196090, acc.: 71.88%] [G loss: 0.468128]\n",
      "epoch:13 step:12207 [D loss: 0.229494, acc.: 60.16%] [G loss: 0.448370]\n",
      "epoch:13 step:12208 [D loss: 0.224753, acc.: 64.84%] [G loss: 0.425462]\n",
      "epoch:13 step:12209 [D loss: 0.254888, acc.: 59.38%] [G loss: 0.430968]\n",
      "epoch:13 step:12210 [D loss: 0.193759, acc.: 69.53%] [G loss: 0.472255]\n",
      "epoch:13 step:12211 [D loss: 0.237025, acc.: 55.47%] [G loss: 0.423556]\n",
      "epoch:13 step:12212 [D loss: 0.222156, acc.: 63.28%] [G loss: 0.485739]\n",
      "epoch:13 step:12213 [D loss: 0.208231, acc.: 67.97%] [G loss: 0.455206]\n",
      "epoch:13 step:12214 [D loss: 0.223272, acc.: 61.72%] [G loss: 0.466516]\n",
      "epoch:13 step:12215 [D loss: 0.232398, acc.: 64.84%] [G loss: 0.416094]\n",
      "epoch:13 step:12216 [D loss: 0.239762, acc.: 61.72%] [G loss: 0.428719]\n",
      "epoch:13 step:12217 [D loss: 0.204546, acc.: 71.09%] [G loss: 0.472726]\n",
      "epoch:13 step:12218 [D loss: 0.220707, acc.: 65.62%] [G loss: 0.482787]\n",
      "epoch:13 step:12219 [D loss: 0.264640, acc.: 52.34%] [G loss: 0.400428]\n",
      "epoch:13 step:12220 [D loss: 0.217145, acc.: 67.97%] [G loss: 0.448560]\n",
      "epoch:13 step:12221 [D loss: 0.194590, acc.: 70.31%] [G loss: 0.469711]\n",
      "epoch:13 step:12222 [D loss: 0.225399, acc.: 67.97%] [G loss: 0.475224]\n",
      "epoch:13 step:12223 [D loss: 0.197927, acc.: 63.28%] [G loss: 0.448495]\n",
      "epoch:13 step:12224 [D loss: 0.225634, acc.: 64.06%] [G loss: 0.376338]\n",
      "epoch:13 step:12225 [D loss: 0.238752, acc.: 58.59%] [G loss: 0.422124]\n",
      "epoch:13 step:12226 [D loss: 0.199044, acc.: 68.75%] [G loss: 0.428408]\n",
      "epoch:13 step:12227 [D loss: 0.217417, acc.: 57.81%] [G loss: 0.478521]\n",
      "epoch:13 step:12228 [D loss: 0.217138, acc.: 64.06%] [G loss: 0.457462]\n",
      "epoch:13 step:12229 [D loss: 0.197774, acc.: 72.66%] [G loss: 0.496263]\n",
      "epoch:13 step:12230 [D loss: 0.219436, acc.: 64.84%] [G loss: 0.471699]\n",
      "epoch:13 step:12231 [D loss: 0.206964, acc.: 62.50%] [G loss: 0.483490]\n",
      "epoch:13 step:12232 [D loss: 0.244340, acc.: 59.38%] [G loss: 0.500971]\n",
      "epoch:13 step:12233 [D loss: 0.204394, acc.: 69.53%] [G loss: 0.465508]\n",
      "epoch:13 step:12234 [D loss: 0.223172, acc.: 68.75%] [G loss: 0.471921]\n",
      "epoch:13 step:12235 [D loss: 0.201440, acc.: 67.97%] [G loss: 0.462049]\n",
      "epoch:13 step:12236 [D loss: 0.243922, acc.: 57.81%] [G loss: 0.463433]\n",
      "epoch:13 step:12237 [D loss: 0.241056, acc.: 61.72%] [G loss: 0.435672]\n",
      "epoch:13 step:12238 [D loss: 0.229793, acc.: 61.72%] [G loss: 0.417382]\n",
      "epoch:13 step:12239 [D loss: 0.224851, acc.: 64.06%] [G loss: 0.439345]\n",
      "epoch:13 step:12240 [D loss: 0.222095, acc.: 63.28%] [G loss: 0.432505]\n",
      "epoch:13 step:12241 [D loss: 0.250313, acc.: 56.25%] [G loss: 0.460057]\n",
      "epoch:13 step:12242 [D loss: 0.249584, acc.: 58.59%] [G loss: 0.425169]\n",
      "epoch:13 step:12243 [D loss: 0.241954, acc.: 61.72%] [G loss: 0.409719]\n",
      "epoch:13 step:12244 [D loss: 0.197262, acc.: 75.00%] [G loss: 0.441578]\n",
      "epoch:13 step:12245 [D loss: 0.227390, acc.: 58.59%] [G loss: 0.431154]\n",
      "epoch:13 step:12246 [D loss: 0.222239, acc.: 61.72%] [G loss: 0.452130]\n",
      "epoch:13 step:12247 [D loss: 0.223435, acc.: 61.72%] [G loss: 0.462067]\n",
      "epoch:13 step:12248 [D loss: 0.212695, acc.: 67.97%] [G loss: 0.421216]\n",
      "epoch:13 step:12249 [D loss: 0.211173, acc.: 67.19%] [G loss: 0.417420]\n",
      "epoch:13 step:12250 [D loss: 0.205197, acc.: 67.97%] [G loss: 0.455690]\n",
      "epoch:13 step:12251 [D loss: 0.207146, acc.: 65.62%] [G loss: 0.462132]\n",
      "epoch:13 step:12252 [D loss: 0.233006, acc.: 61.72%] [G loss: 0.491424]\n",
      "epoch:13 step:12253 [D loss: 0.232740, acc.: 63.28%] [G loss: 0.445886]\n",
      "epoch:13 step:12254 [D loss: 0.233207, acc.: 56.25%] [G loss: 0.422263]\n",
      "epoch:13 step:12255 [D loss: 0.221014, acc.: 64.84%] [G loss: 0.422264]\n",
      "epoch:13 step:12256 [D loss: 0.189922, acc.: 67.19%] [G loss: 0.512442]\n",
      "epoch:13 step:12257 [D loss: 0.210833, acc.: 64.84%] [G loss: 0.542933]\n",
      "epoch:13 step:12258 [D loss: 0.173185, acc.: 75.00%] [G loss: 0.486103]\n",
      "epoch:13 step:12259 [D loss: 0.244322, acc.: 55.47%] [G loss: 0.416861]\n",
      "epoch:13 step:12260 [D loss: 0.265850, acc.: 57.81%] [G loss: 0.398637]\n",
      "epoch:13 step:12261 [D loss: 0.194461, acc.: 71.09%] [G loss: 0.421278]\n",
      "epoch:13 step:12262 [D loss: 0.216752, acc.: 64.84%] [G loss: 0.444104]\n",
      "epoch:13 step:12263 [D loss: 0.245661, acc.: 60.94%] [G loss: 0.395407]\n",
      "epoch:13 step:12264 [D loss: 0.219210, acc.: 69.53%] [G loss: 0.438627]\n",
      "epoch:13 step:12265 [D loss: 0.226540, acc.: 62.50%] [G loss: 0.448502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12266 [D loss: 0.228549, acc.: 58.59%] [G loss: 0.471170]\n",
      "epoch:13 step:12267 [D loss: 0.212125, acc.: 70.31%] [G loss: 0.439752]\n",
      "epoch:13 step:12268 [D loss: 0.232701, acc.: 62.50%] [G loss: 0.452834]\n",
      "epoch:13 step:12269 [D loss: 0.208465, acc.: 68.75%] [G loss: 0.457722]\n",
      "epoch:13 step:12270 [D loss: 0.218225, acc.: 63.28%] [G loss: 0.438071]\n",
      "epoch:13 step:12271 [D loss: 0.225757, acc.: 67.19%] [G loss: 0.458332]\n",
      "epoch:13 step:12272 [D loss: 0.228721, acc.: 66.41%] [G loss: 0.485871]\n",
      "epoch:13 step:12273 [D loss: 0.196252, acc.: 73.44%] [G loss: 0.457978]\n",
      "epoch:13 step:12274 [D loss: 0.188081, acc.: 74.22%] [G loss: 0.446310]\n",
      "epoch:13 step:12275 [D loss: 0.247199, acc.: 57.81%] [G loss: 0.491520]\n",
      "epoch:13 step:12276 [D loss: 0.210169, acc.: 64.84%] [G loss: 0.474536]\n",
      "epoch:13 step:12277 [D loss: 0.212945, acc.: 66.41%] [G loss: 0.457153]\n",
      "epoch:13 step:12278 [D loss: 0.205179, acc.: 71.88%] [G loss: 0.469086]\n",
      "epoch:13 step:12279 [D loss: 0.218732, acc.: 64.06%] [G loss: 0.472068]\n",
      "epoch:13 step:12280 [D loss: 0.209663, acc.: 67.97%] [G loss: 0.483227]\n",
      "epoch:13 step:12281 [D loss: 0.188825, acc.: 68.75%] [G loss: 0.488447]\n",
      "epoch:13 step:12282 [D loss: 0.228534, acc.: 63.28%] [G loss: 0.466842]\n",
      "epoch:13 step:12283 [D loss: 0.228540, acc.: 59.38%] [G loss: 0.459864]\n",
      "epoch:13 step:12284 [D loss: 0.219604, acc.: 64.84%] [G loss: 0.415029]\n",
      "epoch:13 step:12285 [D loss: 0.218848, acc.: 63.28%] [G loss: 0.427460]\n",
      "epoch:13 step:12286 [D loss: 0.241000, acc.: 57.81%] [G loss: 0.415719]\n",
      "epoch:13 step:12287 [D loss: 0.217362, acc.: 64.84%] [G loss: 0.456094]\n",
      "epoch:13 step:12288 [D loss: 0.224124, acc.: 65.62%] [G loss: 0.516140]\n",
      "epoch:13 step:12289 [D loss: 0.275030, acc.: 54.69%] [G loss: 0.481160]\n",
      "epoch:13 step:12290 [D loss: 0.257459, acc.: 57.03%] [G loss: 0.425602]\n",
      "epoch:13 step:12291 [D loss: 0.229197, acc.: 58.59%] [G loss: 0.430833]\n",
      "epoch:13 step:12292 [D loss: 0.216169, acc.: 65.62%] [G loss: 0.463932]\n",
      "epoch:13 step:12293 [D loss: 0.208837, acc.: 69.53%] [G loss: 0.462164]\n",
      "epoch:13 step:12294 [D loss: 0.207437, acc.: 66.41%] [G loss: 0.475337]\n",
      "epoch:13 step:12295 [D loss: 0.210079, acc.: 67.97%] [G loss: 0.480388]\n",
      "epoch:13 step:12296 [D loss: 0.190579, acc.: 71.09%] [G loss: 0.499984]\n",
      "epoch:13 step:12297 [D loss: 0.184945, acc.: 75.00%] [G loss: 0.487043]\n",
      "epoch:13 step:12298 [D loss: 0.238010, acc.: 60.16%] [G loss: 0.483621]\n",
      "epoch:13 step:12299 [D loss: 0.232960, acc.: 65.62%] [G loss: 0.498943]\n",
      "epoch:13 step:12300 [D loss: 0.149501, acc.: 82.03%] [G loss: 0.540473]\n",
      "epoch:13 step:12301 [D loss: 0.262353, acc.: 58.59%] [G loss: 0.459763]\n",
      "epoch:13 step:12302 [D loss: 0.236978, acc.: 64.06%] [G loss: 0.459394]\n",
      "epoch:13 step:12303 [D loss: 0.188248, acc.: 71.09%] [G loss: 0.501631]\n",
      "epoch:13 step:12304 [D loss: 0.187886, acc.: 74.22%] [G loss: 0.509238]\n",
      "epoch:13 step:12305 [D loss: 0.280400, acc.: 46.88%] [G loss: 0.446877]\n",
      "epoch:13 step:12306 [D loss: 0.222699, acc.: 61.72%] [G loss: 0.436646]\n",
      "epoch:13 step:12307 [D loss: 0.210559, acc.: 64.84%] [G loss: 0.428153]\n",
      "epoch:13 step:12308 [D loss: 0.244322, acc.: 57.03%] [G loss: 0.393292]\n",
      "epoch:13 step:12309 [D loss: 0.241692, acc.: 53.91%] [G loss: 0.400723]\n",
      "epoch:13 step:12310 [D loss: 0.207515, acc.: 71.09%] [G loss: 0.447735]\n",
      "epoch:13 step:12311 [D loss: 0.220766, acc.: 63.28%] [G loss: 0.441073]\n",
      "epoch:13 step:12312 [D loss: 0.217315, acc.: 61.72%] [G loss: 0.505541]\n",
      "epoch:13 step:12313 [D loss: 0.218676, acc.: 66.41%] [G loss: 0.510966]\n",
      "epoch:13 step:12314 [D loss: 0.243254, acc.: 58.59%] [G loss: 0.467451]\n",
      "epoch:13 step:12315 [D loss: 0.217818, acc.: 63.28%] [G loss: 0.457651]\n",
      "epoch:13 step:12316 [D loss: 0.224061, acc.: 63.28%] [G loss: 0.465788]\n",
      "epoch:13 step:12317 [D loss: 0.201550, acc.: 70.31%] [G loss: 0.474548]\n",
      "epoch:13 step:12318 [D loss: 0.248339, acc.: 56.25%] [G loss: 0.428940]\n",
      "epoch:13 step:12319 [D loss: 0.228492, acc.: 63.28%] [G loss: 0.414982]\n",
      "epoch:13 step:12320 [D loss: 0.208532, acc.: 67.19%] [G loss: 0.440184]\n",
      "epoch:13 step:12321 [D loss: 0.219765, acc.: 64.84%] [G loss: 0.425949]\n",
      "epoch:13 step:12322 [D loss: 0.218834, acc.: 65.62%] [G loss: 0.440718]\n",
      "epoch:13 step:12323 [D loss: 0.234243, acc.: 59.38%] [G loss: 0.418214]\n",
      "epoch:13 step:12324 [D loss: 0.231338, acc.: 60.94%] [G loss: 0.447225]\n",
      "epoch:13 step:12325 [D loss: 0.227782, acc.: 59.38%] [G loss: 0.446833]\n",
      "epoch:13 step:12326 [D loss: 0.226396, acc.: 57.81%] [G loss: 0.441788]\n",
      "epoch:13 step:12327 [D loss: 0.220468, acc.: 58.59%] [G loss: 0.443814]\n",
      "epoch:13 step:12328 [D loss: 0.229729, acc.: 62.50%] [G loss: 0.464974]\n",
      "epoch:13 step:12329 [D loss: 0.247305, acc.: 57.03%] [G loss: 0.401866]\n",
      "epoch:13 step:12330 [D loss: 0.185099, acc.: 72.66%] [G loss: 0.450101]\n",
      "epoch:13 step:12331 [D loss: 0.233925, acc.: 58.59%] [G loss: 0.419223]\n",
      "epoch:13 step:12332 [D loss: 0.217387, acc.: 67.97%] [G loss: 0.406406]\n",
      "epoch:13 step:12333 [D loss: 0.219836, acc.: 62.50%] [G loss: 0.435526]\n",
      "epoch:13 step:12334 [D loss: 0.255092, acc.: 57.81%] [G loss: 0.441035]\n",
      "epoch:13 step:12335 [D loss: 0.217044, acc.: 59.38%] [G loss: 0.465147]\n",
      "epoch:13 step:12336 [D loss: 0.202713, acc.: 67.19%] [G loss: 0.458429]\n",
      "epoch:13 step:12337 [D loss: 0.216460, acc.: 63.28%] [G loss: 0.479955]\n",
      "epoch:13 step:12338 [D loss: 0.226568, acc.: 64.06%] [G loss: 0.451814]\n",
      "epoch:13 step:12339 [D loss: 0.234562, acc.: 56.25%] [G loss: 0.469777]\n",
      "epoch:13 step:12340 [D loss: 0.207763, acc.: 66.41%] [G loss: 0.478806]\n",
      "epoch:13 step:12341 [D loss: 0.267525, acc.: 54.69%] [G loss: 0.478438]\n",
      "epoch:13 step:12342 [D loss: 0.212226, acc.: 63.28%] [G loss: 0.464985]\n",
      "epoch:13 step:12343 [D loss: 0.238265, acc.: 61.72%] [G loss: 0.444419]\n",
      "epoch:13 step:12344 [D loss: 0.249474, acc.: 57.03%] [G loss: 0.468235]\n",
      "epoch:13 step:12345 [D loss: 0.211007, acc.: 62.50%] [G loss: 0.487672]\n",
      "epoch:13 step:12346 [D loss: 0.215615, acc.: 63.28%] [G loss: 0.456408]\n",
      "epoch:13 step:12347 [D loss: 0.202152, acc.: 71.09%] [G loss: 0.434859]\n",
      "epoch:13 step:12348 [D loss: 0.207258, acc.: 68.75%] [G loss: 0.441995]\n",
      "epoch:13 step:12349 [D loss: 0.217686, acc.: 66.41%] [G loss: 0.459252]\n",
      "epoch:13 step:12350 [D loss: 0.243816, acc.: 60.94%] [G loss: 0.449168]\n",
      "epoch:13 step:12351 [D loss: 0.247032, acc.: 57.03%] [G loss: 0.457208]\n",
      "epoch:13 step:12352 [D loss: 0.237106, acc.: 57.81%] [G loss: 0.447788]\n",
      "epoch:13 step:12353 [D loss: 0.207592, acc.: 66.41%] [G loss: 0.420324]\n",
      "epoch:13 step:12354 [D loss: 0.242360, acc.: 58.59%] [G loss: 0.430551]\n",
      "epoch:13 step:12355 [D loss: 0.252823, acc.: 56.25%] [G loss: 0.461249]\n",
      "epoch:13 step:12356 [D loss: 0.239275, acc.: 54.69%] [G loss: 0.419780]\n",
      "epoch:13 step:12357 [D loss: 0.197552, acc.: 70.31%] [G loss: 0.484469]\n",
      "epoch:13 step:12358 [D loss: 0.226372, acc.: 64.84%] [G loss: 0.415948]\n",
      "epoch:13 step:12359 [D loss: 0.246134, acc.: 53.12%] [G loss: 0.452706]\n",
      "epoch:13 step:12360 [D loss: 0.224681, acc.: 62.50%] [G loss: 0.440368]\n",
      "epoch:13 step:12361 [D loss: 0.237348, acc.: 57.03%] [G loss: 0.421788]\n",
      "epoch:13 step:12362 [D loss: 0.228502, acc.: 64.06%] [G loss: 0.454189]\n",
      "epoch:13 step:12363 [D loss: 0.255843, acc.: 57.03%] [G loss: 0.452802]\n",
      "epoch:13 step:12364 [D loss: 0.229487, acc.: 61.72%] [G loss: 0.419409]\n",
      "epoch:13 step:12365 [D loss: 0.217726, acc.: 65.62%] [G loss: 0.469017]\n",
      "epoch:13 step:12366 [D loss: 0.219334, acc.: 66.41%] [G loss: 0.448887]\n",
      "epoch:13 step:12367 [D loss: 0.256089, acc.: 55.47%] [G loss: 0.419119]\n",
      "epoch:13 step:12368 [D loss: 0.228530, acc.: 57.03%] [G loss: 0.457642]\n",
      "epoch:13 step:12369 [D loss: 0.232401, acc.: 61.72%] [G loss: 0.433877]\n",
      "epoch:13 step:12370 [D loss: 0.236619, acc.: 63.28%] [G loss: 0.420211]\n",
      "epoch:13 step:12371 [D loss: 0.201728, acc.: 71.09%] [G loss: 0.414508]\n",
      "epoch:13 step:12372 [D loss: 0.199741, acc.: 66.41%] [G loss: 0.440976]\n",
      "epoch:13 step:12373 [D loss: 0.191105, acc.: 71.09%] [G loss: 0.453730]\n",
      "epoch:13 step:12374 [D loss: 0.226861, acc.: 68.75%] [G loss: 0.439347]\n",
      "epoch:13 step:12375 [D loss: 0.210864, acc.: 65.62%] [G loss: 0.468494]\n",
      "epoch:13 step:12376 [D loss: 0.211212, acc.: 67.19%] [G loss: 0.477709]\n",
      "epoch:13 step:12377 [D loss: 0.214374, acc.: 60.16%] [G loss: 0.473118]\n",
      "epoch:13 step:12378 [D loss: 0.220841, acc.: 69.53%] [G loss: 0.458947]\n",
      "epoch:13 step:12379 [D loss: 0.195126, acc.: 69.53%] [G loss: 0.493450]\n",
      "epoch:13 step:12380 [D loss: 0.242885, acc.: 61.72%] [G loss: 0.422777]\n",
      "epoch:13 step:12381 [D loss: 0.228749, acc.: 60.16%] [G loss: 0.470264]\n",
      "epoch:13 step:12382 [D loss: 0.209369, acc.: 64.06%] [G loss: 0.440686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12383 [D loss: 0.222756, acc.: 64.06%] [G loss: 0.426334]\n",
      "epoch:13 step:12384 [D loss: 0.270429, acc.: 55.47%] [G loss: 0.423274]\n",
      "epoch:13 step:12385 [D loss: 0.223881, acc.: 62.50%] [G loss: 0.447475]\n",
      "epoch:13 step:12386 [D loss: 0.211388, acc.: 62.50%] [G loss: 0.466246]\n",
      "epoch:13 step:12387 [D loss: 0.211735, acc.: 68.75%] [G loss: 0.506171]\n",
      "epoch:13 step:12388 [D loss: 0.197903, acc.: 72.66%] [G loss: 0.498465]\n",
      "epoch:13 step:12389 [D loss: 0.185255, acc.: 69.53%] [G loss: 0.502734]\n",
      "epoch:13 step:12390 [D loss: 0.181568, acc.: 75.00%] [G loss: 0.503021]\n",
      "epoch:13 step:12391 [D loss: 0.261446, acc.: 54.69%] [G loss: 0.447874]\n",
      "epoch:13 step:12392 [D loss: 0.219471, acc.: 64.06%] [G loss: 0.427222]\n",
      "epoch:13 step:12393 [D loss: 0.222636, acc.: 64.06%] [G loss: 0.440261]\n",
      "epoch:13 step:12394 [D loss: 0.243285, acc.: 57.81%] [G loss: 0.402964]\n",
      "epoch:13 step:12395 [D loss: 0.274367, acc.: 50.00%] [G loss: 0.405076]\n",
      "epoch:13 step:12396 [D loss: 0.243369, acc.: 57.03%] [G loss: 0.435798]\n",
      "epoch:13 step:12397 [D loss: 0.197463, acc.: 74.22%] [G loss: 0.449786]\n",
      "epoch:13 step:12398 [D loss: 0.239744, acc.: 60.94%] [G loss: 0.403182]\n",
      "epoch:13 step:12399 [D loss: 0.190566, acc.: 68.75%] [G loss: 0.500060]\n",
      "epoch:13 step:12400 [D loss: 0.177374, acc.: 72.66%] [G loss: 0.539698]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.725139\n",
      "FID: 12.149993\n",
      "0 = 12.07184157574177\n",
      "1 = 0.05479420741853993\n",
      "2 = 0.8948500156402588\n",
      "3 = 0.8871999979019165\n",
      "4 = 0.9024999737739563\n",
      "5 = 0.900985062122345\n",
      "6 = 0.8871999979019165\n",
      "7 = 6.5612997825682\n",
      "8 = 0.08181224835199015\n",
      "9 = 0.7106500267982483\n",
      "10 = 0.7143999934196472\n",
      "11 = 0.7069000005722046\n",
      "12 = 0.7090818881988525\n",
      "13 = 0.7143999934196472\n",
      "14 = 7.725211143493652\n",
      "15 = 9.474166870117188\n",
      "16 = 0.12866449356079102\n",
      "17 = 7.7251386642456055\n",
      "18 = 12.149992942810059\n",
      "epoch:13 step:12401 [D loss: 0.286512, acc.: 51.56%] [G loss: 0.456117]\n",
      "epoch:13 step:12402 [D loss: 0.206294, acc.: 66.41%] [G loss: 0.477382]\n",
      "epoch:13 step:12403 [D loss: 0.207671, acc.: 67.19%] [G loss: 0.461679]\n",
      "epoch:13 step:12404 [D loss: 0.196773, acc.: 71.09%] [G loss: 0.484664]\n",
      "epoch:13 step:12405 [D loss: 0.266418, acc.: 53.91%] [G loss: 0.445552]\n",
      "epoch:13 step:12406 [D loss: 0.233336, acc.: 59.38%] [G loss: 0.450968]\n",
      "epoch:13 step:12407 [D loss: 0.240851, acc.: 60.16%] [G loss: 0.387747]\n",
      "epoch:13 step:12408 [D loss: 0.229397, acc.: 62.50%] [G loss: 0.409875]\n",
      "epoch:13 step:12409 [D loss: 0.256286, acc.: 57.03%] [G loss: 0.390173]\n",
      "epoch:13 step:12410 [D loss: 0.194222, acc.: 69.53%] [G loss: 0.421590]\n",
      "epoch:13 step:12411 [D loss: 0.217837, acc.: 62.50%] [G loss: 0.447984]\n",
      "epoch:13 step:12412 [D loss: 0.186854, acc.: 73.44%] [G loss: 0.483239]\n",
      "epoch:13 step:12413 [D loss: 0.180252, acc.: 75.78%] [G loss: 0.549170]\n",
      "epoch:13 step:12414 [D loss: 0.245190, acc.: 57.03%] [G loss: 0.487424]\n",
      "epoch:13 step:12415 [D loss: 0.227182, acc.: 63.28%] [G loss: 0.415604]\n",
      "epoch:13 step:12416 [D loss: 0.259056, acc.: 53.91%] [G loss: 0.407564]\n",
      "epoch:13 step:12417 [D loss: 0.226296, acc.: 61.72%] [G loss: 0.452217]\n",
      "epoch:13 step:12418 [D loss: 0.219408, acc.: 64.06%] [G loss: 0.511340]\n",
      "epoch:13 step:12419 [D loss: 0.202073, acc.: 70.31%] [G loss: 0.430364]\n",
      "epoch:13 step:12420 [D loss: 0.213945, acc.: 65.62%] [G loss: 0.399134]\n",
      "epoch:13 step:12421 [D loss: 0.216798, acc.: 67.97%] [G loss: 0.443148]\n",
      "epoch:13 step:12422 [D loss: 0.226404, acc.: 60.94%] [G loss: 0.477026]\n",
      "epoch:13 step:12423 [D loss: 0.201488, acc.: 69.53%] [G loss: 0.455610]\n",
      "epoch:13 step:12424 [D loss: 0.228253, acc.: 61.72%] [G loss: 0.485174]\n",
      "epoch:13 step:12425 [D loss: 0.216675, acc.: 67.19%] [G loss: 0.490352]\n",
      "epoch:13 step:12426 [D loss: 0.239910, acc.: 57.81%] [G loss: 0.463381]\n",
      "epoch:13 step:12427 [D loss: 0.203349, acc.: 74.22%] [G loss: 0.486132]\n",
      "epoch:13 step:12428 [D loss: 0.236948, acc.: 60.16%] [G loss: 0.488660]\n",
      "epoch:13 step:12429 [D loss: 0.217714, acc.: 61.72%] [G loss: 0.501094]\n",
      "epoch:13 step:12430 [D loss: 0.268646, acc.: 50.78%] [G loss: 0.444464]\n",
      "epoch:13 step:12431 [D loss: 0.265824, acc.: 49.22%] [G loss: 0.433607]\n",
      "epoch:13 step:12432 [D loss: 0.238082, acc.: 56.25%] [G loss: 0.474432]\n",
      "epoch:13 step:12433 [D loss: 0.225943, acc.: 62.50%] [G loss: 0.467554]\n",
      "epoch:13 step:12434 [D loss: 0.209019, acc.: 68.75%] [G loss: 0.472735]\n",
      "epoch:13 step:12435 [D loss: 0.219784, acc.: 61.72%] [G loss: 0.446349]\n",
      "epoch:13 step:12436 [D loss: 0.226824, acc.: 59.38%] [G loss: 0.416121]\n",
      "epoch:13 step:12437 [D loss: 0.209674, acc.: 67.19%] [G loss: 0.444256]\n",
      "epoch:13 step:12438 [D loss: 0.239423, acc.: 60.16%] [G loss: 0.436302]\n",
      "epoch:13 step:12439 [D loss: 0.230540, acc.: 59.38%] [G loss: 0.431986]\n",
      "epoch:13 step:12440 [D loss: 0.215926, acc.: 61.72%] [G loss: 0.460293]\n",
      "epoch:13 step:12441 [D loss: 0.223525, acc.: 63.28%] [G loss: 0.453955]\n",
      "epoch:13 step:12442 [D loss: 0.219487, acc.: 62.50%] [G loss: 0.461946]\n",
      "epoch:13 step:12443 [D loss: 0.187164, acc.: 70.31%] [G loss: 0.498652]\n",
      "epoch:13 step:12444 [D loss: 0.236110, acc.: 60.16%] [G loss: 0.467349]\n",
      "epoch:13 step:12445 [D loss: 0.209801, acc.: 69.53%] [G loss: 0.439986]\n",
      "epoch:13 step:12446 [D loss: 0.229253, acc.: 60.16%] [G loss: 0.439709]\n",
      "epoch:13 step:12447 [D loss: 0.246244, acc.: 57.03%] [G loss: 0.454663]\n",
      "epoch:13 step:12448 [D loss: 0.210438, acc.: 66.41%] [G loss: 0.453242]\n",
      "epoch:13 step:12449 [D loss: 0.221069, acc.: 64.06%] [G loss: 0.453083]\n",
      "epoch:13 step:12450 [D loss: 0.202372, acc.: 70.31%] [G loss: 0.441137]\n",
      "epoch:13 step:12451 [D loss: 0.209553, acc.: 66.41%] [G loss: 0.417866]\n",
      "epoch:13 step:12452 [D loss: 0.209691, acc.: 64.84%] [G loss: 0.435166]\n",
      "epoch:13 step:12453 [D loss: 0.209321, acc.: 66.41%] [G loss: 0.498359]\n",
      "epoch:13 step:12454 [D loss: 0.189295, acc.: 72.66%] [G loss: 0.484667]\n",
      "epoch:13 step:12455 [D loss: 0.213214, acc.: 69.53%] [G loss: 0.478100]\n",
      "epoch:13 step:12456 [D loss: 0.217095, acc.: 64.84%] [G loss: 0.507970]\n",
      "epoch:13 step:12457 [D loss: 0.216322, acc.: 65.62%] [G loss: 0.464718]\n",
      "epoch:13 step:12458 [D loss: 0.240659, acc.: 56.25%] [G loss: 0.450579]\n",
      "epoch:13 step:12459 [D loss: 0.251383, acc.: 50.78%] [G loss: 0.417213]\n",
      "epoch:13 step:12460 [D loss: 0.229620, acc.: 60.16%] [G loss: 0.482362]\n",
      "epoch:13 step:12461 [D loss: 0.188029, acc.: 74.22%] [G loss: 0.497153]\n",
      "epoch:13 step:12462 [D loss: 0.267916, acc.: 56.25%] [G loss: 0.420785]\n",
      "epoch:13 step:12463 [D loss: 0.232437, acc.: 60.16%] [G loss: 0.408632]\n",
      "epoch:13 step:12464 [D loss: 0.198567, acc.: 71.88%] [G loss: 0.446436]\n",
      "epoch:13 step:12465 [D loss: 0.235465, acc.: 63.28%] [G loss: 0.462595]\n",
      "epoch:13 step:12466 [D loss: 0.218800, acc.: 65.62%] [G loss: 0.429306]\n",
      "epoch:13 step:12467 [D loss: 0.208660, acc.: 72.66%] [G loss: 0.470406]\n",
      "epoch:13 step:12468 [D loss: 0.209359, acc.: 70.31%] [G loss: 0.430635]\n",
      "epoch:13 step:12469 [D loss: 0.228829, acc.: 60.94%] [G loss: 0.458692]\n",
      "epoch:13 step:12470 [D loss: 0.207112, acc.: 66.41%] [G loss: 0.492661]\n",
      "epoch:13 step:12471 [D loss: 0.222876, acc.: 61.72%] [G loss: 0.476952]\n",
      "epoch:13 step:12472 [D loss: 0.241332, acc.: 60.16%] [G loss: 0.411224]\n",
      "epoch:13 step:12473 [D loss: 0.205079, acc.: 67.97%] [G loss: 0.488615]\n",
      "epoch:13 step:12474 [D loss: 0.233401, acc.: 57.03%] [G loss: 0.418117]\n",
      "epoch:13 step:12475 [D loss: 0.236236, acc.: 60.94%] [G loss: 0.409261]\n",
      "epoch:13 step:12476 [D loss: 0.229603, acc.: 58.59%] [G loss: 0.454240]\n",
      "epoch:13 step:12477 [D loss: 0.221918, acc.: 64.06%] [G loss: 0.398371]\n",
      "epoch:13 step:12478 [D loss: 0.196729, acc.: 71.09%] [G loss: 0.446906]\n",
      "epoch:13 step:12479 [D loss: 0.178130, acc.: 75.78%] [G loss: 0.478440]\n",
      "epoch:13 step:12480 [D loss: 0.189572, acc.: 70.31%] [G loss: 0.490897]\n",
      "epoch:13 step:12481 [D loss: 0.213051, acc.: 63.28%] [G loss: 0.486033]\n",
      "epoch:13 step:12482 [D loss: 0.263772, acc.: 60.16%] [G loss: 0.452387]\n",
      "epoch:13 step:12483 [D loss: 0.256916, acc.: 53.91%] [G loss: 0.456871]\n",
      "epoch:13 step:12484 [D loss: 0.215549, acc.: 65.62%] [G loss: 0.484579]\n",
      "epoch:13 step:12485 [D loss: 0.210712, acc.: 63.28%] [G loss: 0.422282]\n",
      "epoch:13 step:12486 [D loss: 0.226767, acc.: 60.16%] [G loss: 0.419043]\n",
      "epoch:13 step:12487 [D loss: 0.210070, acc.: 66.41%] [G loss: 0.439967]\n",
      "epoch:13 step:12488 [D loss: 0.198342, acc.: 67.97%] [G loss: 0.500810]\n",
      "epoch:13 step:12489 [D loss: 0.235077, acc.: 63.28%] [G loss: 0.419755]\n",
      "epoch:13 step:12490 [D loss: 0.185841, acc.: 74.22%] [G loss: 0.434266]\n",
      "epoch:13 step:12491 [D loss: 0.217824, acc.: 69.53%] [G loss: 0.451535]\n",
      "epoch:13 step:12492 [D loss: 0.235950, acc.: 61.72%] [G loss: 0.458560]\n",
      "epoch:13 step:12493 [D loss: 0.178935, acc.: 71.88%] [G loss: 0.495996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12494 [D loss: 0.205823, acc.: 67.19%] [G loss: 0.532041]\n",
      "epoch:13 step:12495 [D loss: 0.184194, acc.: 72.66%] [G loss: 0.520758]\n",
      "epoch:13 step:12496 [D loss: 0.204909, acc.: 64.06%] [G loss: 0.475938]\n",
      "epoch:13 step:12497 [D loss: 0.237534, acc.: 60.16%] [G loss: 0.448421]\n",
      "epoch:13 step:12498 [D loss: 0.239865, acc.: 53.12%] [G loss: 0.404571]\n",
      "epoch:13 step:12499 [D loss: 0.215574, acc.: 66.41%] [G loss: 0.417073]\n",
      "epoch:13 step:12500 [D loss: 0.215550, acc.: 65.62%] [G loss: 0.485999]\n",
      "epoch:13 step:12501 [D loss: 0.205612, acc.: 67.97%] [G loss: 0.465756]\n",
      "epoch:13 step:12502 [D loss: 0.198581, acc.: 68.75%] [G loss: 0.452350]\n",
      "epoch:13 step:12503 [D loss: 0.221326, acc.: 60.94%] [G loss: 0.472193]\n",
      "epoch:13 step:12504 [D loss: 0.263640, acc.: 54.69%] [G loss: 0.442134]\n",
      "epoch:13 step:12505 [D loss: 0.217831, acc.: 65.62%] [G loss: 0.387864]\n",
      "epoch:13 step:12506 [D loss: 0.229256, acc.: 64.84%] [G loss: 0.439809]\n",
      "epoch:13 step:12507 [D loss: 0.223562, acc.: 60.16%] [G loss: 0.421317]\n",
      "epoch:13 step:12508 [D loss: 0.238734, acc.: 61.72%] [G loss: 0.450482]\n",
      "epoch:13 step:12509 [D loss: 0.212153, acc.: 70.31%] [G loss: 0.432713]\n",
      "epoch:13 step:12510 [D loss: 0.197334, acc.: 71.09%] [G loss: 0.463728]\n",
      "epoch:13 step:12511 [D loss: 0.212729, acc.: 65.62%] [G loss: 0.440098]\n",
      "epoch:13 step:12512 [D loss: 0.222900, acc.: 64.84%] [G loss: 0.422773]\n",
      "epoch:13 step:12513 [D loss: 0.211110, acc.: 67.97%] [G loss: 0.472560]\n",
      "epoch:13 step:12514 [D loss: 0.191363, acc.: 72.66%] [G loss: 0.475952]\n",
      "epoch:13 step:12515 [D loss: 0.211991, acc.: 67.97%] [G loss: 0.492194]\n",
      "epoch:13 step:12516 [D loss: 0.230433, acc.: 64.84%] [G loss: 0.453036]\n",
      "epoch:13 step:12517 [D loss: 0.192713, acc.: 71.09%] [G loss: 0.489184]\n",
      "epoch:13 step:12518 [D loss: 0.218516, acc.: 61.72%] [G loss: 0.475680]\n",
      "epoch:13 step:12519 [D loss: 0.235142, acc.: 64.06%] [G loss: 0.419880]\n",
      "epoch:13 step:12520 [D loss: 0.213537, acc.: 67.97%] [G loss: 0.489669]\n",
      "epoch:13 step:12521 [D loss: 0.215234, acc.: 64.84%] [G loss: 0.518700]\n",
      "epoch:13 step:12522 [D loss: 0.321900, acc.: 50.00%] [G loss: 0.442085]\n",
      "epoch:13 step:12523 [D loss: 0.248772, acc.: 50.00%] [G loss: 0.448779]\n",
      "epoch:13 step:12524 [D loss: 0.206307, acc.: 69.53%] [G loss: 0.450687]\n",
      "epoch:13 step:12525 [D loss: 0.225471, acc.: 60.94%] [G loss: 0.482369]\n",
      "epoch:13 step:12526 [D loss: 0.221643, acc.: 62.50%] [G loss: 0.517982]\n",
      "epoch:13 step:12527 [D loss: 0.184239, acc.: 71.88%] [G loss: 0.557249]\n",
      "epoch:13 step:12528 [D loss: 0.163606, acc.: 76.56%] [G loss: 0.598134]\n",
      "epoch:13 step:12529 [D loss: 0.306716, acc.: 56.25%] [G loss: 0.430109]\n",
      "epoch:13 step:12530 [D loss: 0.277721, acc.: 46.09%] [G loss: 0.389795]\n",
      "epoch:13 step:12531 [D loss: 0.203581, acc.: 67.19%] [G loss: 0.406412]\n",
      "epoch:13 step:12532 [D loss: 0.233373, acc.: 60.16%] [G loss: 0.416479]\n",
      "epoch:13 step:12533 [D loss: 0.205791, acc.: 67.19%] [G loss: 0.471369]\n",
      "epoch:13 step:12534 [D loss: 0.214042, acc.: 62.50%] [G loss: 0.515186]\n",
      "epoch:13 step:12535 [D loss: 0.178440, acc.: 73.44%] [G loss: 0.527351]\n",
      "epoch:13 step:12536 [D loss: 0.231410, acc.: 64.84%] [G loss: 0.490916]\n",
      "epoch:13 step:12537 [D loss: 0.240852, acc.: 57.81%] [G loss: 0.428043]\n",
      "epoch:13 step:12538 [D loss: 0.233621, acc.: 60.16%] [G loss: 0.439851]\n",
      "epoch:13 step:12539 [D loss: 0.180710, acc.: 75.78%] [G loss: 0.462147]\n",
      "epoch:13 step:12540 [D loss: 0.206911, acc.: 67.19%] [G loss: 0.470600]\n",
      "epoch:13 step:12541 [D loss: 0.208897, acc.: 67.19%] [G loss: 0.482529]\n",
      "epoch:13 step:12542 [D loss: 0.173772, acc.: 75.00%] [G loss: 0.462396]\n",
      "epoch:13 step:12543 [D loss: 0.241258, acc.: 57.81%] [G loss: 0.447292]\n",
      "epoch:13 step:12544 [D loss: 0.225830, acc.: 64.84%] [G loss: 0.438251]\n",
      "epoch:13 step:12545 [D loss: 0.201159, acc.: 67.97%] [G loss: 0.444390]\n",
      "epoch:13 step:12546 [D loss: 0.221157, acc.: 64.84%] [G loss: 0.446713]\n",
      "epoch:13 step:12547 [D loss: 0.239517, acc.: 60.94%] [G loss: 0.410455]\n",
      "epoch:13 step:12548 [D loss: 0.206689, acc.: 66.41%] [G loss: 0.486688]\n",
      "epoch:13 step:12549 [D loss: 0.207966, acc.: 68.75%] [G loss: 0.469181]\n",
      "epoch:13 step:12550 [D loss: 0.254640, acc.: 51.56%] [G loss: 0.424105]\n",
      "epoch:13 step:12551 [D loss: 0.213839, acc.: 64.84%] [G loss: 0.444059]\n",
      "epoch:13 step:12552 [D loss: 0.218562, acc.: 70.31%] [G loss: 0.456468]\n",
      "epoch:13 step:12553 [D loss: 0.226011, acc.: 65.62%] [G loss: 0.524506]\n",
      "epoch:13 step:12554 [D loss: 0.246186, acc.: 59.38%] [G loss: 0.479482]\n",
      "epoch:13 step:12555 [D loss: 0.187868, acc.: 71.09%] [G loss: 0.435401]\n",
      "epoch:13 step:12556 [D loss: 0.247387, acc.: 60.94%] [G loss: 0.430460]\n",
      "epoch:13 step:12557 [D loss: 0.261099, acc.: 51.56%] [G loss: 0.427862]\n",
      "epoch:13 step:12558 [D loss: 0.263933, acc.: 54.69%] [G loss: 0.359967]\n",
      "epoch:13 step:12559 [D loss: 0.221980, acc.: 58.59%] [G loss: 0.447877]\n",
      "epoch:13 step:12560 [D loss: 0.233512, acc.: 63.28%] [G loss: 0.467550]\n",
      "epoch:13 step:12561 [D loss: 0.235107, acc.: 57.81%] [G loss: 0.447032]\n",
      "epoch:13 step:12562 [D loss: 0.198093, acc.: 67.19%] [G loss: 0.492463]\n",
      "epoch:13 step:12563 [D loss: 0.234479, acc.: 63.28%] [G loss: 0.408000]\n",
      "epoch:13 step:12564 [D loss: 0.189719, acc.: 72.66%] [G loss: 0.436504]\n",
      "epoch:13 step:12565 [D loss: 0.216053, acc.: 64.84%] [G loss: 0.448027]\n",
      "epoch:13 step:12566 [D loss: 0.229099, acc.: 61.72%] [G loss: 0.427128]\n",
      "epoch:13 step:12567 [D loss: 0.248990, acc.: 57.81%] [G loss: 0.447401]\n",
      "epoch:13 step:12568 [D loss: 0.228666, acc.: 60.94%] [G loss: 0.474359]\n",
      "epoch:13 step:12569 [D loss: 0.225052, acc.: 67.97%] [G loss: 0.463387]\n",
      "epoch:13 step:12570 [D loss: 0.222347, acc.: 64.84%] [G loss: 0.452254]\n",
      "epoch:13 step:12571 [D loss: 0.261882, acc.: 47.66%] [G loss: 0.435173]\n",
      "epoch:13 step:12572 [D loss: 0.222005, acc.: 65.62%] [G loss: 0.455583]\n",
      "epoch:13 step:12573 [D loss: 0.212538, acc.: 64.06%] [G loss: 0.451116]\n",
      "epoch:13 step:12574 [D loss: 0.228948, acc.: 61.72%] [G loss: 0.421360]\n",
      "epoch:13 step:12575 [D loss: 0.252444, acc.: 64.84%] [G loss: 0.455419]\n",
      "epoch:13 step:12576 [D loss: 0.199872, acc.: 67.19%] [G loss: 0.474028]\n",
      "epoch:13 step:12577 [D loss: 0.260907, acc.: 52.34%] [G loss: 0.482078]\n",
      "epoch:13 step:12578 [D loss: 0.220480, acc.: 63.28%] [G loss: 0.473608]\n",
      "epoch:13 step:12579 [D loss: 0.183999, acc.: 70.31%] [G loss: 0.470686]\n",
      "epoch:13 step:12580 [D loss: 0.204437, acc.: 66.41%] [G loss: 0.466413]\n",
      "epoch:13 step:12581 [D loss: 0.262767, acc.: 56.25%] [G loss: 0.445004]\n",
      "epoch:13 step:12582 [D loss: 0.238885, acc.: 55.47%] [G loss: 0.435026]\n",
      "epoch:13 step:12583 [D loss: 0.243939, acc.: 60.16%] [G loss: 0.501513]\n",
      "epoch:13 step:12584 [D loss: 0.233451, acc.: 57.81%] [G loss: 0.461686]\n",
      "epoch:13 step:12585 [D loss: 0.235347, acc.: 57.03%] [G loss: 0.433391]\n",
      "epoch:13 step:12586 [D loss: 0.200895, acc.: 71.09%] [G loss: 0.429091]\n",
      "epoch:13 step:12587 [D loss: 0.178563, acc.: 79.69%] [G loss: 0.506040]\n",
      "epoch:13 step:12588 [D loss: 0.241371, acc.: 57.03%] [G loss: 0.464386]\n",
      "epoch:13 step:12589 [D loss: 0.242644, acc.: 60.16%] [G loss: 0.466367]\n",
      "epoch:13 step:12590 [D loss: 0.225871, acc.: 64.84%] [G loss: 0.432925]\n",
      "epoch:13 step:12591 [D loss: 0.254424, acc.: 59.38%] [G loss: 0.408855]\n",
      "epoch:13 step:12592 [D loss: 0.249569, acc.: 57.03%] [G loss: 0.428567]\n",
      "epoch:13 step:12593 [D loss: 0.203136, acc.: 64.06%] [G loss: 0.463946]\n",
      "epoch:13 step:12594 [D loss: 0.236304, acc.: 65.62%] [G loss: 0.412075]\n",
      "epoch:13 step:12595 [D loss: 0.197081, acc.: 71.09%] [G loss: 0.462621]\n",
      "epoch:13 step:12596 [D loss: 0.203755, acc.: 67.19%] [G loss: 0.455199]\n",
      "epoch:13 step:12597 [D loss: 0.191953, acc.: 72.66%] [G loss: 0.512451]\n",
      "epoch:13 step:12598 [D loss: 0.229607, acc.: 61.72%] [G loss: 0.498139]\n",
      "epoch:13 step:12599 [D loss: 0.251143, acc.: 52.34%] [G loss: 0.460145]\n",
      "epoch:13 step:12600 [D loss: 0.231552, acc.: 63.28%] [G loss: 0.416462]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.792818\n",
      "FID: 8.682689\n",
      "0 = 11.850069131946533\n",
      "1 = 0.050212313967781086\n",
      "2 = 0.890500009059906\n",
      "3 = 0.8744000196456909\n",
      "4 = 0.9065999984741211\n",
      "5 = 0.9034924507141113\n",
      "6 = 0.8744000196456909\n",
      "7 = 6.130801137465244\n",
      "8 = 0.06381557607382221\n",
      "9 = 0.7053499817848206\n",
      "10 = 0.7034000158309937\n",
      "11 = 0.7073000073432922\n",
      "12 = 0.7061539888381958\n",
      "13 = 0.7034000158309937\n",
      "14 = 7.792880058288574\n",
      "15 = 9.603204727172852\n",
      "16 = 0.09304206818342209\n",
      "17 = 7.79281759262085\n",
      "18 = 8.68268871307373\n",
      "epoch:13 step:12601 [D loss: 0.216634, acc.: 60.94%] [G loss: 0.416458]\n",
      "epoch:13 step:12602 [D loss: 0.265326, acc.: 57.03%] [G loss: 0.439438]\n",
      "epoch:13 step:12603 [D loss: 0.228284, acc.: 61.72%] [G loss: 0.439724]\n",
      "epoch:13 step:12604 [D loss: 0.226775, acc.: 65.62%] [G loss: 0.446339]\n",
      "epoch:13 step:12605 [D loss: 0.229308, acc.: 63.28%] [G loss: 0.398242]\n",
      "epoch:13 step:12606 [D loss: 0.206223, acc.: 67.97%] [G loss: 0.446881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12607 [D loss: 0.224748, acc.: 64.84%] [G loss: 0.447379]\n",
      "epoch:13 step:12608 [D loss: 0.208986, acc.: 65.62%] [G loss: 0.454509]\n",
      "epoch:13 step:12609 [D loss: 0.213408, acc.: 64.84%] [G loss: 0.478788]\n",
      "epoch:13 step:12610 [D loss: 0.208869, acc.: 71.09%] [G loss: 0.471129]\n",
      "epoch:13 step:12611 [D loss: 0.193422, acc.: 72.66%] [G loss: 0.476088]\n",
      "epoch:13 step:12612 [D loss: 0.217170, acc.: 61.72%] [G loss: 0.452202]\n",
      "epoch:13 step:12613 [D loss: 0.232917, acc.: 58.59%] [G loss: 0.477493]\n",
      "epoch:13 step:12614 [D loss: 0.233030, acc.: 57.03%] [G loss: 0.431101]\n",
      "epoch:13 step:12615 [D loss: 0.193343, acc.: 71.09%] [G loss: 0.426470]\n",
      "epoch:13 step:12616 [D loss: 0.228396, acc.: 63.28%] [G loss: 0.437082]\n",
      "epoch:13 step:12617 [D loss: 0.200304, acc.: 67.19%] [G loss: 0.504644]\n",
      "epoch:13 step:12618 [D loss: 0.291627, acc.: 50.78%] [G loss: 0.439479]\n",
      "epoch:13 step:12619 [D loss: 0.253148, acc.: 59.38%] [G loss: 0.457759]\n",
      "epoch:13 step:12620 [D loss: 0.206972, acc.: 67.97%] [G loss: 0.470567]\n",
      "epoch:13 step:12621 [D loss: 0.226353, acc.: 65.62%] [G loss: 0.449828]\n",
      "epoch:13 step:12622 [D loss: 0.245406, acc.: 60.16%] [G loss: 0.461611]\n",
      "epoch:13 step:12623 [D loss: 0.241069, acc.: 57.81%] [G loss: 0.454054]\n",
      "epoch:13 step:12624 [D loss: 0.239027, acc.: 55.47%] [G loss: 0.416933]\n",
      "epoch:13 step:12625 [D loss: 0.230587, acc.: 60.94%] [G loss: 0.428646]\n",
      "epoch:13 step:12626 [D loss: 0.204800, acc.: 68.75%] [G loss: 0.459920]\n",
      "epoch:13 step:12627 [D loss: 0.229662, acc.: 62.50%] [G loss: 0.472118]\n",
      "epoch:13 step:12628 [D loss: 0.200445, acc.: 71.09%] [G loss: 0.487157]\n",
      "epoch:13 step:12629 [D loss: 0.268105, acc.: 53.91%] [G loss: 0.428273]\n",
      "epoch:13 step:12630 [D loss: 0.218441, acc.: 62.50%] [G loss: 0.420538]\n",
      "epoch:13 step:12631 [D loss: 0.190945, acc.: 71.09%] [G loss: 0.461764]\n",
      "epoch:13 step:12632 [D loss: 0.217172, acc.: 69.53%] [G loss: 0.449887]\n",
      "epoch:13 step:12633 [D loss: 0.204166, acc.: 70.31%] [G loss: 0.478091]\n",
      "epoch:13 step:12634 [D loss: 0.187863, acc.: 69.53%] [G loss: 0.480655]\n",
      "epoch:13 step:12635 [D loss: 0.230000, acc.: 61.72%] [G loss: 0.467348]\n",
      "epoch:13 step:12636 [D loss: 0.225483, acc.: 64.84%] [G loss: 0.446970]\n",
      "epoch:13 step:12637 [D loss: 0.244499, acc.: 59.38%] [G loss: 0.482529]\n",
      "epoch:13 step:12638 [D loss: 0.226881, acc.: 69.53%] [G loss: 0.472768]\n",
      "epoch:13 step:12639 [D loss: 0.255012, acc.: 53.12%] [G loss: 0.479550]\n",
      "epoch:13 step:12640 [D loss: 0.230062, acc.: 60.94%] [G loss: 0.438959]\n",
      "epoch:13 step:12641 [D loss: 0.238951, acc.: 57.03%] [G loss: 0.433583]\n",
      "epoch:13 step:12642 [D loss: 0.215054, acc.: 67.19%] [G loss: 0.428817]\n",
      "epoch:13 step:12643 [D loss: 0.243322, acc.: 56.25%] [G loss: 0.421497]\n",
      "epoch:13 step:12644 [D loss: 0.235370, acc.: 62.50%] [G loss: 0.441408]\n",
      "epoch:13 step:12645 [D loss: 0.227581, acc.: 57.81%] [G loss: 0.442305]\n",
      "epoch:13 step:12646 [D loss: 0.237097, acc.: 57.03%] [G loss: 0.445441]\n",
      "epoch:13 step:12647 [D loss: 0.232691, acc.: 59.38%] [G loss: 0.435716]\n",
      "epoch:13 step:12648 [D loss: 0.207177, acc.: 71.88%] [G loss: 0.470607]\n",
      "epoch:13 step:12649 [D loss: 0.223064, acc.: 60.16%] [G loss: 0.469807]\n",
      "epoch:13 step:12650 [D loss: 0.229537, acc.: 64.06%] [G loss: 0.460753]\n",
      "epoch:13 step:12651 [D loss: 0.216739, acc.: 65.62%] [G loss: 0.482091]\n",
      "epoch:13 step:12652 [D loss: 0.193178, acc.: 73.44%] [G loss: 0.507439]\n",
      "epoch:13 step:12653 [D loss: 0.211285, acc.: 66.41%] [G loss: 0.518030]\n",
      "epoch:13 step:12654 [D loss: 0.268750, acc.: 50.00%] [G loss: 0.422367]\n",
      "epoch:13 step:12655 [D loss: 0.229277, acc.: 62.50%] [G loss: 0.438327]\n",
      "epoch:13 step:12656 [D loss: 0.204979, acc.: 70.31%] [G loss: 0.506838]\n",
      "epoch:13 step:12657 [D loss: 0.265995, acc.: 58.59%] [G loss: 0.477795]\n",
      "epoch:13 step:12658 [D loss: 0.256721, acc.: 54.69%] [G loss: 0.409140]\n",
      "epoch:13 step:12659 [D loss: 0.264387, acc.: 54.69%] [G loss: 0.403732]\n",
      "epoch:13 step:12660 [D loss: 0.225566, acc.: 62.50%] [G loss: 0.402225]\n",
      "epoch:13 step:12661 [D loss: 0.209043, acc.: 65.62%] [G loss: 0.457529]\n",
      "epoch:13 step:12662 [D loss: 0.199106, acc.: 71.09%] [G loss: 0.452418]\n",
      "epoch:13 step:12663 [D loss: 0.301869, acc.: 46.88%] [G loss: 0.436247]\n",
      "epoch:13 step:12664 [D loss: 0.235380, acc.: 61.72%] [G loss: 0.436035]\n",
      "epoch:13 step:12665 [D loss: 0.227602, acc.: 67.19%] [G loss: 0.458043]\n",
      "epoch:13 step:12666 [D loss: 0.239456, acc.: 58.59%] [G loss: 0.430758]\n",
      "epoch:13 step:12667 [D loss: 0.232101, acc.: 65.62%] [G loss: 0.455301]\n",
      "epoch:13 step:12668 [D loss: 0.218897, acc.: 67.19%] [G loss: 0.445787]\n",
      "epoch:13 step:12669 [D loss: 0.200123, acc.: 70.31%] [G loss: 0.488191]\n",
      "epoch:13 step:12670 [D loss: 0.229928, acc.: 62.50%] [G loss: 0.457501]\n",
      "epoch:13 step:12671 [D loss: 0.224718, acc.: 60.94%] [G loss: 0.460062]\n",
      "epoch:13 step:12672 [D loss: 0.223812, acc.: 64.84%] [G loss: 0.447964]\n",
      "epoch:13 step:12673 [D loss: 0.236834, acc.: 63.28%] [G loss: 0.407304]\n",
      "epoch:13 step:12674 [D loss: 0.197438, acc.: 71.09%] [G loss: 0.473790]\n",
      "epoch:13 step:12675 [D loss: 0.225543, acc.: 67.19%] [G loss: 0.447998]\n",
      "epoch:13 step:12676 [D loss: 0.206063, acc.: 67.97%] [G loss: 0.446249]\n",
      "epoch:13 step:12677 [D loss: 0.219895, acc.: 66.41%] [G loss: 0.503886]\n",
      "epoch:13 step:12678 [D loss: 0.209562, acc.: 65.62%] [G loss: 0.499844]\n",
      "epoch:13 step:12679 [D loss: 0.216217, acc.: 66.41%] [G loss: 0.489656]\n",
      "epoch:13 step:12680 [D loss: 0.208758, acc.: 69.53%] [G loss: 0.514428]\n",
      "epoch:13 step:12681 [D loss: 0.254517, acc.: 53.12%] [G loss: 0.470898]\n",
      "epoch:13 step:12682 [D loss: 0.294272, acc.: 53.91%] [G loss: 0.363659]\n",
      "epoch:13 step:12683 [D loss: 0.223886, acc.: 59.38%] [G loss: 0.439283]\n",
      "epoch:13 step:12684 [D loss: 0.232348, acc.: 59.38%] [G loss: 0.399805]\n",
      "epoch:13 step:12685 [D loss: 0.208028, acc.: 61.72%] [G loss: 0.462214]\n",
      "epoch:13 step:12686 [D loss: 0.212923, acc.: 71.09%] [G loss: 0.517828]\n",
      "epoch:13 step:12687 [D loss: 0.209663, acc.: 66.41%] [G loss: 0.515280]\n",
      "epoch:13 step:12688 [D loss: 0.193169, acc.: 67.19%] [G loss: 0.498185]\n",
      "epoch:13 step:12689 [D loss: 0.187416, acc.: 69.53%] [G loss: 0.534003]\n",
      "epoch:13 step:12690 [D loss: 0.261716, acc.: 50.78%] [G loss: 0.393915]\n",
      "epoch:13 step:12691 [D loss: 0.234122, acc.: 59.38%] [G loss: 0.439883]\n",
      "epoch:13 step:12692 [D loss: 0.230145, acc.: 60.16%] [G loss: 0.426493]\n",
      "epoch:13 step:12693 [D loss: 0.217806, acc.: 63.28%] [G loss: 0.461017]\n",
      "epoch:13 step:12694 [D loss: 0.203213, acc.: 62.50%] [G loss: 0.475741]\n",
      "epoch:13 step:12695 [D loss: 0.201421, acc.: 65.62%] [G loss: 0.465222]\n",
      "epoch:13 step:12696 [D loss: 0.197498, acc.: 75.00%] [G loss: 0.483516]\n",
      "epoch:13 step:12697 [D loss: 0.199348, acc.: 70.31%] [G loss: 0.499061]\n",
      "epoch:13 step:12698 [D loss: 0.251901, acc.: 60.94%] [G loss: 0.484315]\n",
      "epoch:13 step:12699 [D loss: 0.234540, acc.: 59.38%] [G loss: 0.435276]\n",
      "epoch:13 step:12700 [D loss: 0.233067, acc.: 60.16%] [G loss: 0.394408]\n",
      "epoch:13 step:12701 [D loss: 0.222810, acc.: 62.50%] [G loss: 0.421886]\n",
      "epoch:13 step:12702 [D loss: 0.175342, acc.: 78.12%] [G loss: 0.472571]\n",
      "epoch:13 step:12703 [D loss: 0.227290, acc.: 62.50%] [G loss: 0.406478]\n",
      "epoch:13 step:12704 [D loss: 0.198328, acc.: 66.41%] [G loss: 0.435306]\n",
      "epoch:13 step:12705 [D loss: 0.215239, acc.: 71.09%] [G loss: 0.426636]\n",
      "epoch:13 step:12706 [D loss: 0.224581, acc.: 63.28%] [G loss: 0.479017]\n",
      "epoch:13 step:12707 [D loss: 0.203129, acc.: 65.62%] [G loss: 0.462080]\n",
      "epoch:13 step:12708 [D loss: 0.222338, acc.: 65.62%] [G loss: 0.483088]\n",
      "epoch:13 step:12709 [D loss: 0.280592, acc.: 46.88%] [G loss: 0.378500]\n",
      "epoch:13 step:12710 [D loss: 0.226819, acc.: 60.16%] [G loss: 0.452453]\n",
      "epoch:13 step:12711 [D loss: 0.222727, acc.: 61.72%] [G loss: 0.431308]\n",
      "epoch:13 step:12712 [D loss: 0.250823, acc.: 60.16%] [G loss: 0.453679]\n",
      "epoch:13 step:12713 [D loss: 0.240228, acc.: 56.25%] [G loss: 0.409046]\n",
      "epoch:13 step:12714 [D loss: 0.221190, acc.: 64.06%] [G loss: 0.453127]\n",
      "epoch:13 step:12715 [D loss: 0.181147, acc.: 73.44%] [G loss: 0.485117]\n",
      "epoch:13 step:12716 [D loss: 0.258302, acc.: 53.91%] [G loss: 0.413433]\n",
      "epoch:13 step:12717 [D loss: 0.212127, acc.: 68.75%] [G loss: 0.433348]\n",
      "epoch:13 step:12718 [D loss: 0.215412, acc.: 67.97%] [G loss: 0.472318]\n",
      "epoch:13 step:12719 [D loss: 0.267784, acc.: 57.81%] [G loss: 0.425186]\n",
      "epoch:13 step:12720 [D loss: 0.224960, acc.: 60.94%] [G loss: 0.434670]\n",
      "epoch:13 step:12721 [D loss: 0.225739, acc.: 62.50%] [G loss: 0.474393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12722 [D loss: 0.244533, acc.: 60.94%] [G loss: 0.443115]\n",
      "epoch:13 step:12723 [D loss: 0.256891, acc.: 56.25%] [G loss: 0.434200]\n",
      "epoch:13 step:12724 [D loss: 0.234185, acc.: 58.59%] [G loss: 0.408734]\n",
      "epoch:13 step:12725 [D loss: 0.240971, acc.: 60.16%] [G loss: 0.440239]\n",
      "epoch:13 step:12726 [D loss: 0.207640, acc.: 67.97%] [G loss: 0.459843]\n",
      "epoch:13 step:12727 [D loss: 0.217593, acc.: 64.84%] [G loss: 0.463340]\n",
      "epoch:13 step:12728 [D loss: 0.206523, acc.: 63.28%] [G loss: 0.483227]\n",
      "epoch:13 step:12729 [D loss: 0.200754, acc.: 65.62%] [G loss: 0.505908]\n",
      "epoch:13 step:12730 [D loss: 0.202547, acc.: 65.62%] [G loss: 0.462556]\n",
      "epoch:13 step:12731 [D loss: 0.196731, acc.: 70.31%] [G loss: 0.501532]\n",
      "epoch:13 step:12732 [D loss: 0.224333, acc.: 67.19%] [G loss: 0.480647]\n",
      "epoch:13 step:12733 [D loss: 0.218744, acc.: 64.06%] [G loss: 0.488598]\n",
      "epoch:13 step:12734 [D loss: 0.262638, acc.: 52.34%] [G loss: 0.427986]\n",
      "epoch:13 step:12735 [D loss: 0.193696, acc.: 68.75%] [G loss: 0.451382]\n",
      "epoch:13 step:12736 [D loss: 0.187692, acc.: 75.00%] [G loss: 0.476009]\n",
      "epoch:13 step:12737 [D loss: 0.199424, acc.: 71.88%] [G loss: 0.472421]\n",
      "epoch:13 step:12738 [D loss: 0.201567, acc.: 67.19%] [G loss: 0.453578]\n",
      "epoch:13 step:12739 [D loss: 0.213966, acc.: 63.28%] [G loss: 0.468190]\n",
      "epoch:13 step:12740 [D loss: 0.244204, acc.: 57.03%] [G loss: 0.483157]\n",
      "epoch:13 step:12741 [D loss: 0.236915, acc.: 63.28%] [G loss: 0.427088]\n",
      "epoch:13 step:12742 [D loss: 0.210484, acc.: 68.75%] [G loss: 0.450293]\n",
      "epoch:13 step:12743 [D loss: 0.222666, acc.: 60.16%] [G loss: 0.413229]\n",
      "epoch:13 step:12744 [D loss: 0.225787, acc.: 62.50%] [G loss: 0.491438]\n",
      "epoch:13 step:12745 [D loss: 0.182874, acc.: 73.44%] [G loss: 0.532933]\n",
      "epoch:13 step:12746 [D loss: 0.232662, acc.: 58.59%] [G loss: 0.466945]\n",
      "epoch:13 step:12747 [D loss: 0.251513, acc.: 57.03%] [G loss: 0.452935]\n",
      "epoch:13 step:12748 [D loss: 0.205580, acc.: 70.31%] [G loss: 0.489575]\n",
      "epoch:13 step:12749 [D loss: 0.197094, acc.: 72.66%] [G loss: 0.472979]\n",
      "epoch:13 step:12750 [D loss: 0.293117, acc.: 46.09%] [G loss: 0.394203]\n",
      "epoch:13 step:12751 [D loss: 0.230425, acc.: 58.59%] [G loss: 0.417098]\n",
      "epoch:13 step:12752 [D loss: 0.202772, acc.: 63.28%] [G loss: 0.452034]\n",
      "epoch:13 step:12753 [D loss: 0.201028, acc.: 69.53%] [G loss: 0.455755]\n",
      "epoch:13 step:12754 [D loss: 0.254348, acc.: 57.03%] [G loss: 0.462425]\n",
      "epoch:13 step:12755 [D loss: 0.167753, acc.: 73.44%] [G loss: 0.524869]\n",
      "epoch:13 step:12756 [D loss: 0.177556, acc.: 76.56%] [G loss: 0.541752]\n",
      "epoch:13 step:12757 [D loss: 0.245086, acc.: 57.81%] [G loss: 0.452665]\n",
      "epoch:13 step:12758 [D loss: 0.256804, acc.: 53.12%] [G loss: 0.418782]\n",
      "epoch:13 step:12759 [D loss: 0.239578, acc.: 58.59%] [G loss: 0.452445]\n",
      "epoch:13 step:12760 [D loss: 0.246797, acc.: 53.12%] [G loss: 0.412815]\n",
      "epoch:13 step:12761 [D loss: 0.226953, acc.: 59.38%] [G loss: 0.472367]\n",
      "epoch:13 step:12762 [D loss: 0.221608, acc.: 63.28%] [G loss: 0.442656]\n",
      "epoch:13 step:12763 [D loss: 0.203014, acc.: 67.19%] [G loss: 0.465584]\n",
      "epoch:13 step:12764 [D loss: 0.213345, acc.: 66.41%] [G loss: 0.445406]\n",
      "epoch:13 step:12765 [D loss: 0.246616, acc.: 59.38%] [G loss: 0.422133]\n",
      "epoch:13 step:12766 [D loss: 0.234587, acc.: 58.59%] [G loss: 0.458742]\n",
      "epoch:13 step:12767 [D loss: 0.236618, acc.: 58.59%] [G loss: 0.446257]\n",
      "epoch:13 step:12768 [D loss: 0.248126, acc.: 51.56%] [G loss: 0.408422]\n",
      "epoch:13 step:12769 [D loss: 0.219536, acc.: 64.06%] [G loss: 0.435047]\n",
      "epoch:13 step:12770 [D loss: 0.201184, acc.: 73.44%] [G loss: 0.434033]\n",
      "epoch:13 step:12771 [D loss: 0.235719, acc.: 64.84%] [G loss: 0.454253]\n",
      "epoch:13 step:12772 [D loss: 0.231413, acc.: 61.72%] [G loss: 0.415030]\n",
      "epoch:13 step:12773 [D loss: 0.212555, acc.: 64.06%] [G loss: 0.477583]\n",
      "epoch:13 step:12774 [D loss: 0.219944, acc.: 69.53%] [G loss: 0.501528]\n",
      "epoch:13 step:12775 [D loss: 0.245312, acc.: 54.69%] [G loss: 0.421429]\n",
      "epoch:13 step:12776 [D loss: 0.225300, acc.: 58.59%] [G loss: 0.436718]\n",
      "epoch:13 step:12777 [D loss: 0.242758, acc.: 57.81%] [G loss: 0.429113]\n",
      "epoch:13 step:12778 [D loss: 0.216467, acc.: 67.97%] [G loss: 0.413425]\n",
      "epoch:13 step:12779 [D loss: 0.196333, acc.: 70.31%] [G loss: 0.457995]\n",
      "epoch:13 step:12780 [D loss: 0.211663, acc.: 66.41%] [G loss: 0.439268]\n",
      "epoch:13 step:12781 [D loss: 0.240222, acc.: 57.03%] [G loss: 0.436311]\n",
      "epoch:13 step:12782 [D loss: 0.256383, acc.: 58.59%] [G loss: 0.467669]\n",
      "epoch:13 step:12783 [D loss: 0.233795, acc.: 64.84%] [G loss: 0.469181]\n",
      "epoch:13 step:12784 [D loss: 0.222528, acc.: 64.06%] [G loss: 0.451381]\n",
      "epoch:13 step:12785 [D loss: 0.249918, acc.: 58.59%] [G loss: 0.441651]\n",
      "epoch:13 step:12786 [D loss: 0.204430, acc.: 67.19%] [G loss: 0.444567]\n",
      "epoch:13 step:12787 [D loss: 0.235255, acc.: 55.47%] [G loss: 0.425306]\n",
      "epoch:13 step:12788 [D loss: 0.209662, acc.: 64.06%] [G loss: 0.486084]\n",
      "epoch:13 step:12789 [D loss: 0.215423, acc.: 61.72%] [G loss: 0.445492]\n",
      "epoch:13 step:12790 [D loss: 0.226380, acc.: 63.28%] [G loss: 0.445098]\n",
      "epoch:13 step:12791 [D loss: 0.215138, acc.: 62.50%] [G loss: 0.497072]\n",
      "epoch:13 step:12792 [D loss: 0.202743, acc.: 72.66%] [G loss: 0.422638]\n",
      "epoch:13 step:12793 [D loss: 0.209521, acc.: 65.62%] [G loss: 0.459871]\n",
      "epoch:13 step:12794 [D loss: 0.197515, acc.: 67.19%] [G loss: 0.403364]\n",
      "epoch:13 step:12795 [D loss: 0.247821, acc.: 60.16%] [G loss: 0.438272]\n",
      "epoch:13 step:12796 [D loss: 0.247062, acc.: 61.72%] [G loss: 0.423578]\n",
      "epoch:13 step:12797 [D loss: 0.235884, acc.: 64.84%] [G loss: 0.465037]\n",
      "epoch:13 step:12798 [D loss: 0.212264, acc.: 64.84%] [G loss: 0.472655]\n",
      "epoch:13 step:12799 [D loss: 0.207176, acc.: 68.75%] [G loss: 0.442851]\n",
      "epoch:13 step:12800 [D loss: 0.230923, acc.: 63.28%] [G loss: 0.441769]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.872869\n",
      "FID: 7.140935\n",
      "0 = 11.781422442436204\n",
      "1 = 0.04321909714433183\n",
      "2 = 0.8773000240325928\n",
      "3 = 0.8641999959945679\n",
      "4 = 0.8903999924659729\n",
      "5 = 0.8874512314796448\n",
      "6 = 0.8641999959945679\n",
      "7 = 6.0920879356324615\n",
      "8 = 0.06033287376242827\n",
      "9 = 0.6942499876022339\n",
      "10 = 0.7010999917984009\n",
      "11 = 0.6873999834060669\n",
      "12 = 0.6916247606277466\n",
      "13 = 0.7010999917984009\n",
      "14 = 7.872933864593506\n",
      "15 = 9.554656028747559\n",
      "16 = 0.10390797257423401\n",
      "17 = 7.872868537902832\n",
      "18 = 7.14093542098999\n",
      "epoch:13 step:12801 [D loss: 0.195207, acc.: 68.75%] [G loss: 0.464915]\n",
      "epoch:13 step:12802 [D loss: 0.216251, acc.: 62.50%] [G loss: 0.447620]\n",
      "epoch:13 step:12803 [D loss: 0.233437, acc.: 63.28%] [G loss: 0.433039]\n",
      "epoch:13 step:12804 [D loss: 0.226755, acc.: 60.94%] [G loss: 0.459692]\n",
      "epoch:13 step:12805 [D loss: 0.188739, acc.: 71.09%] [G loss: 0.517318]\n",
      "epoch:13 step:12806 [D loss: 0.261937, acc.: 52.34%] [G loss: 0.455895]\n",
      "epoch:13 step:12807 [D loss: 0.239013, acc.: 58.59%] [G loss: 0.438331]\n",
      "epoch:13 step:12808 [D loss: 0.223158, acc.: 60.94%] [G loss: 0.417194]\n",
      "epoch:13 step:12809 [D loss: 0.237689, acc.: 59.38%] [G loss: 0.437759]\n",
      "epoch:13 step:12810 [D loss: 0.222425, acc.: 65.62%] [G loss: 0.461132]\n",
      "epoch:13 step:12811 [D loss: 0.204785, acc.: 71.09%] [G loss: 0.442930]\n",
      "epoch:13 step:12812 [D loss: 0.211374, acc.: 67.19%] [G loss: 0.449931]\n",
      "epoch:13 step:12813 [D loss: 0.201180, acc.: 67.19%] [G loss: 0.471820]\n",
      "epoch:13 step:12814 [D loss: 0.214828, acc.: 63.28%] [G loss: 0.467980]\n",
      "epoch:13 step:12815 [D loss: 0.219292, acc.: 65.62%] [G loss: 0.468138]\n",
      "epoch:13 step:12816 [D loss: 0.198957, acc.: 70.31%] [G loss: 0.495960]\n",
      "epoch:13 step:12817 [D loss: 0.222582, acc.: 59.38%] [G loss: 0.466670]\n",
      "epoch:13 step:12818 [D loss: 0.221923, acc.: 62.50%] [G loss: 0.426969]\n",
      "epoch:13 step:12819 [D loss: 0.223221, acc.: 63.28%] [G loss: 0.474123]\n",
      "epoch:13 step:12820 [D loss: 0.229523, acc.: 62.50%] [G loss: 0.445772]\n",
      "epoch:13 step:12821 [D loss: 0.241725, acc.: 57.03%] [G loss: 0.385236]\n",
      "epoch:13 step:12822 [D loss: 0.195244, acc.: 67.19%] [G loss: 0.461325]\n",
      "epoch:13 step:12823 [D loss: 0.198904, acc.: 68.75%] [G loss: 0.488041]\n",
      "epoch:13 step:12824 [D loss: 0.212722, acc.: 64.06%] [G loss: 0.458001]\n",
      "epoch:13 step:12825 [D loss: 0.243781, acc.: 53.91%] [G loss: 0.458250]\n",
      "epoch:13 step:12826 [D loss: 0.230020, acc.: 59.38%] [G loss: 0.430068]\n",
      "epoch:13 step:12827 [D loss: 0.228889, acc.: 62.50%] [G loss: 0.413390]\n",
      "epoch:13 step:12828 [D loss: 0.224216, acc.: 62.50%] [G loss: 0.460944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12829 [D loss: 0.177826, acc.: 76.56%] [G loss: 0.535474]\n",
      "epoch:13 step:12830 [D loss: 0.198780, acc.: 66.41%] [G loss: 0.527012]\n",
      "epoch:13 step:12831 [D loss: 0.208236, acc.: 71.09%] [G loss: 0.494985]\n",
      "epoch:13 step:12832 [D loss: 0.222221, acc.: 63.28%] [G loss: 0.476972]\n",
      "epoch:13 step:12833 [D loss: 0.244984, acc.: 57.81%] [G loss: 0.438805]\n",
      "epoch:13 step:12834 [D loss: 0.250841, acc.: 57.81%] [G loss: 0.413589]\n",
      "epoch:13 step:12835 [D loss: 0.193803, acc.: 70.31%] [G loss: 0.493709]\n",
      "epoch:13 step:12836 [D loss: 0.266556, acc.: 59.38%] [G loss: 0.451394]\n",
      "epoch:13 step:12837 [D loss: 0.228967, acc.: 59.38%] [G loss: 0.436803]\n",
      "epoch:13 step:12838 [D loss: 0.227983, acc.: 63.28%] [G loss: 0.422121]\n",
      "epoch:13 step:12839 [D loss: 0.227558, acc.: 60.16%] [G loss: 0.451619]\n",
      "epoch:13 step:12840 [D loss: 0.206679, acc.: 71.09%] [G loss: 0.478813]\n",
      "epoch:13 step:12841 [D loss: 0.200975, acc.: 67.19%] [G loss: 0.498060]\n",
      "epoch:13 step:12842 [D loss: 0.186440, acc.: 78.12%] [G loss: 0.488686]\n",
      "epoch:13 step:12843 [D loss: 0.217728, acc.: 61.72%] [G loss: 0.503715]\n",
      "epoch:13 step:12844 [D loss: 0.222824, acc.: 61.72%] [G loss: 0.397214]\n",
      "epoch:13 step:12845 [D loss: 0.238158, acc.: 58.59%] [G loss: 0.432292]\n",
      "epoch:13 step:12846 [D loss: 0.223668, acc.: 61.72%] [G loss: 0.480011]\n",
      "epoch:13 step:12847 [D loss: 0.218784, acc.: 66.41%] [G loss: 0.450883]\n",
      "epoch:13 step:12848 [D loss: 0.241105, acc.: 65.62%] [G loss: 0.476523]\n",
      "epoch:13 step:12849 [D loss: 0.233338, acc.: 63.28%] [G loss: 0.442118]\n",
      "epoch:13 step:12850 [D loss: 0.222697, acc.: 65.62%] [G loss: 0.453461]\n",
      "epoch:13 step:12851 [D loss: 0.249632, acc.: 53.12%] [G loss: 0.450226]\n",
      "epoch:13 step:12852 [D loss: 0.225401, acc.: 64.84%] [G loss: 0.454143]\n",
      "epoch:13 step:12853 [D loss: 0.258086, acc.: 53.12%] [G loss: 0.442506]\n",
      "epoch:13 step:12854 [D loss: 0.205537, acc.: 72.66%] [G loss: 0.441378]\n",
      "epoch:13 step:12855 [D loss: 0.207107, acc.: 72.66%] [G loss: 0.472674]\n",
      "epoch:13 step:12856 [D loss: 0.244628, acc.: 56.25%] [G loss: 0.411955]\n",
      "epoch:13 step:12857 [D loss: 0.212927, acc.: 65.62%] [G loss: 0.462311]\n",
      "epoch:13 step:12858 [D loss: 0.211428, acc.: 67.19%] [G loss: 0.462298]\n",
      "epoch:13 step:12859 [D loss: 0.228224, acc.: 65.62%] [G loss: 0.441879]\n",
      "epoch:13 step:12860 [D loss: 0.220541, acc.: 65.62%] [G loss: 0.457705]\n",
      "epoch:13 step:12861 [D loss: 0.249504, acc.: 63.28%] [G loss: 0.451193]\n",
      "epoch:13 step:12862 [D loss: 0.220374, acc.: 63.28%] [G loss: 0.460694]\n",
      "epoch:13 step:12863 [D loss: 0.232046, acc.: 62.50%] [G loss: 0.457466]\n",
      "epoch:13 step:12864 [D loss: 0.221699, acc.: 66.41%] [G loss: 0.398649]\n",
      "epoch:13 step:12865 [D loss: 0.213201, acc.: 67.19%] [G loss: 0.436208]\n",
      "epoch:13 step:12866 [D loss: 0.234233, acc.: 63.28%] [G loss: 0.411743]\n",
      "epoch:13 step:12867 [D loss: 0.219801, acc.: 64.06%] [G loss: 0.424514]\n",
      "epoch:13 step:12868 [D loss: 0.246451, acc.: 58.59%] [G loss: 0.438923]\n",
      "epoch:13 step:12869 [D loss: 0.205911, acc.: 70.31%] [G loss: 0.443498]\n",
      "epoch:13 step:12870 [D loss: 0.232831, acc.: 60.94%] [G loss: 0.429292]\n",
      "epoch:13 step:12871 [D loss: 0.195277, acc.: 75.00%] [G loss: 0.472621]\n",
      "epoch:13 step:12872 [D loss: 0.216805, acc.: 67.97%] [G loss: 0.504104]\n",
      "epoch:13 step:12873 [D loss: 0.209047, acc.: 64.06%] [G loss: 0.504320]\n",
      "epoch:13 step:12874 [D loss: 0.226990, acc.: 60.94%] [G loss: 0.493611]\n",
      "epoch:13 step:12875 [D loss: 0.195829, acc.: 71.09%] [G loss: 0.451556]\n",
      "epoch:13 step:12876 [D loss: 0.208737, acc.: 70.31%] [G loss: 0.510629]\n",
      "epoch:13 step:12877 [D loss: 0.236704, acc.: 62.50%] [G loss: 0.434698]\n",
      "epoch:13 step:12878 [D loss: 0.242128, acc.: 56.25%] [G loss: 0.434602]\n",
      "epoch:13 step:12879 [D loss: 0.220168, acc.: 60.16%] [G loss: 0.434646]\n",
      "epoch:13 step:12880 [D loss: 0.214856, acc.: 61.72%] [G loss: 0.449604]\n",
      "epoch:13 step:12881 [D loss: 0.204109, acc.: 65.62%] [G loss: 0.493338]\n",
      "epoch:13 step:12882 [D loss: 0.222516, acc.: 61.72%] [G loss: 0.492654]\n",
      "epoch:13 step:12883 [D loss: 0.269194, acc.: 51.56%] [G loss: 0.441212]\n",
      "epoch:13 step:12884 [D loss: 0.263915, acc.: 53.91%] [G loss: 0.435644]\n",
      "epoch:13 step:12885 [D loss: 0.254559, acc.: 57.03%] [G loss: 0.409534]\n",
      "epoch:13 step:12886 [D loss: 0.236565, acc.: 55.47%] [G loss: 0.439001]\n",
      "epoch:13 step:12887 [D loss: 0.229909, acc.: 57.03%] [G loss: 0.448418]\n",
      "epoch:13 step:12888 [D loss: 0.202034, acc.: 68.75%] [G loss: 0.469978]\n",
      "epoch:13 step:12889 [D loss: 0.159094, acc.: 79.69%] [G loss: 0.486036]\n",
      "epoch:13 step:12890 [D loss: 0.221157, acc.: 67.97%] [G loss: 0.467499]\n",
      "epoch:13 step:12891 [D loss: 0.230060, acc.: 61.72%] [G loss: 0.442285]\n",
      "epoch:13 step:12892 [D loss: 0.193676, acc.: 67.19%] [G loss: 0.445552]\n",
      "epoch:13 step:12893 [D loss: 0.210471, acc.: 65.62%] [G loss: 0.460930]\n",
      "epoch:13 step:12894 [D loss: 0.228501, acc.: 60.94%] [G loss: 0.433642]\n",
      "epoch:13 step:12895 [D loss: 0.227186, acc.: 66.41%] [G loss: 0.432173]\n",
      "epoch:13 step:12896 [D loss: 0.218171, acc.: 64.84%] [G loss: 0.433402]\n",
      "epoch:13 step:12897 [D loss: 0.250008, acc.: 58.59%] [G loss: 0.390264]\n",
      "epoch:13 step:12898 [D loss: 0.232931, acc.: 64.84%] [G loss: 0.461675]\n",
      "epoch:13 step:12899 [D loss: 0.227000, acc.: 62.50%] [G loss: 0.424774]\n",
      "epoch:13 step:12900 [D loss: 0.209672, acc.: 61.72%] [G loss: 0.485303]\n",
      "epoch:13 step:12901 [D loss: 0.215439, acc.: 65.62%] [G loss: 0.486579]\n",
      "epoch:13 step:12902 [D loss: 0.244752, acc.: 53.12%] [G loss: 0.448472]\n",
      "epoch:13 step:12903 [D loss: 0.239502, acc.: 58.59%] [G loss: 0.419512]\n",
      "epoch:13 step:12904 [D loss: 0.216650, acc.: 66.41%] [G loss: 0.474353]\n",
      "epoch:13 step:12905 [D loss: 0.219043, acc.: 64.06%] [G loss: 0.439872]\n",
      "epoch:13 step:12906 [D loss: 0.213898, acc.: 67.97%] [G loss: 0.461268]\n",
      "epoch:13 step:12907 [D loss: 0.228877, acc.: 60.94%] [G loss: 0.457989]\n",
      "epoch:13 step:12908 [D loss: 0.217653, acc.: 63.28%] [G loss: 0.456793]\n",
      "epoch:13 step:12909 [D loss: 0.203846, acc.: 68.75%] [G loss: 0.437801]\n",
      "epoch:13 step:12910 [D loss: 0.232219, acc.: 62.50%] [G loss: 0.435627]\n",
      "epoch:13 step:12911 [D loss: 0.200910, acc.: 67.19%] [G loss: 0.423716]\n",
      "epoch:13 step:12912 [D loss: 0.223594, acc.: 64.06%] [G loss: 0.459298]\n",
      "epoch:13 step:12913 [D loss: 0.220949, acc.: 61.72%] [G loss: 0.464949]\n",
      "epoch:13 step:12914 [D loss: 0.210267, acc.: 66.41%] [G loss: 0.467337]\n",
      "epoch:13 step:12915 [D loss: 0.238521, acc.: 58.59%] [G loss: 0.480529]\n",
      "epoch:13 step:12916 [D loss: 0.239241, acc.: 60.16%] [G loss: 0.460841]\n",
      "epoch:13 step:12917 [D loss: 0.215132, acc.: 62.50%] [G loss: 0.471183]\n",
      "epoch:13 step:12918 [D loss: 0.223207, acc.: 62.50%] [G loss: 0.450268]\n",
      "epoch:13 step:12919 [D loss: 0.241149, acc.: 59.38%] [G loss: 0.446871]\n",
      "epoch:13 step:12920 [D loss: 0.268314, acc.: 54.69%] [G loss: 0.423633]\n",
      "epoch:13 step:12921 [D loss: 0.216012, acc.: 64.06%] [G loss: 0.440666]\n",
      "epoch:13 step:12922 [D loss: 0.257389, acc.: 66.41%] [G loss: 0.426237]\n",
      "epoch:13 step:12923 [D loss: 0.230483, acc.: 60.94%] [G loss: 0.436646]\n",
      "epoch:13 step:12924 [D loss: 0.193478, acc.: 74.22%] [G loss: 0.474445]\n",
      "epoch:13 step:12925 [D loss: 0.225285, acc.: 59.38%] [G loss: 0.437794]\n",
      "epoch:13 step:12926 [D loss: 0.247487, acc.: 53.12%] [G loss: 0.386009]\n",
      "epoch:13 step:12927 [D loss: 0.218961, acc.: 65.62%] [G loss: 0.446856]\n",
      "epoch:13 step:12928 [D loss: 0.205846, acc.: 62.50%] [G loss: 0.417117]\n",
      "epoch:13 step:12929 [D loss: 0.220991, acc.: 61.72%] [G loss: 0.439647]\n",
      "epoch:13 step:12930 [D loss: 0.195870, acc.: 70.31%] [G loss: 0.473755]\n",
      "epoch:13 step:12931 [D loss: 0.212262, acc.: 63.28%] [G loss: 0.468644]\n",
      "epoch:13 step:12932 [D loss: 0.218918, acc.: 69.53%] [G loss: 0.430970]\n",
      "epoch:13 step:12933 [D loss: 0.256946, acc.: 56.25%] [G loss: 0.438361]\n",
      "epoch:13 step:12934 [D loss: 0.208021, acc.: 67.19%] [G loss: 0.481197]\n",
      "epoch:13 step:12935 [D loss: 0.218046, acc.: 65.62%] [G loss: 0.460109]\n",
      "epoch:13 step:12936 [D loss: 0.234396, acc.: 62.50%] [G loss: 0.467017]\n",
      "epoch:13 step:12937 [D loss: 0.206061, acc.: 64.06%] [G loss: 0.458447]\n",
      "epoch:13 step:12938 [D loss: 0.211739, acc.: 67.97%] [G loss: 0.449035]\n",
      "epoch:13 step:12939 [D loss: 0.226495, acc.: 58.59%] [G loss: 0.407150]\n",
      "epoch:13 step:12940 [D loss: 0.220688, acc.: 61.72%] [G loss: 0.428224]\n",
      "epoch:13 step:12941 [D loss: 0.247929, acc.: 58.59%] [G loss: 0.411754]\n",
      "epoch:13 step:12942 [D loss: 0.221761, acc.: 61.72%] [G loss: 0.435753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12943 [D loss: 0.232829, acc.: 59.38%] [G loss: 0.428530]\n",
      "epoch:13 step:12944 [D loss: 0.248287, acc.: 58.59%] [G loss: 0.427314]\n",
      "epoch:13 step:12945 [D loss: 0.241586, acc.: 56.25%] [G loss: 0.458101]\n",
      "epoch:13 step:12946 [D loss: 0.230059, acc.: 61.72%] [G loss: 0.450741]\n",
      "epoch:13 step:12947 [D loss: 0.245975, acc.: 53.91%] [G loss: 0.433275]\n",
      "epoch:13 step:12948 [D loss: 0.228746, acc.: 67.97%] [G loss: 0.455186]\n",
      "epoch:13 step:12949 [D loss: 0.232594, acc.: 64.84%] [G loss: 0.447263]\n",
      "epoch:13 step:12950 [D loss: 0.211206, acc.: 69.53%] [G loss: 0.477210]\n",
      "epoch:13 step:12951 [D loss: 0.210879, acc.: 66.41%] [G loss: 0.473367]\n",
      "epoch:13 step:12952 [D loss: 0.224266, acc.: 60.16%] [G loss: 0.414424]\n",
      "epoch:13 step:12953 [D loss: 0.239322, acc.: 60.94%] [G loss: 0.396269]\n",
      "epoch:13 step:12954 [D loss: 0.205070, acc.: 67.19%] [G loss: 0.422151]\n",
      "epoch:13 step:12955 [D loss: 0.216634, acc.: 67.97%] [G loss: 0.451672]\n",
      "epoch:13 step:12956 [D loss: 0.208380, acc.: 67.97%] [G loss: 0.439876]\n",
      "epoch:13 step:12957 [D loss: 0.228742, acc.: 63.28%] [G loss: 0.449548]\n",
      "epoch:13 step:12958 [D loss: 0.238863, acc.: 62.50%] [G loss: 0.437622]\n",
      "epoch:13 step:12959 [D loss: 0.237625, acc.: 57.81%] [G loss: 0.444866]\n",
      "epoch:13 step:12960 [D loss: 0.207372, acc.: 67.97%] [G loss: 0.445581]\n",
      "epoch:13 step:12961 [D loss: 0.217663, acc.: 64.06%] [G loss: 0.414120]\n",
      "epoch:13 step:12962 [D loss: 0.189892, acc.: 70.31%] [G loss: 0.490867]\n",
      "epoch:13 step:12963 [D loss: 0.165747, acc.: 79.69%] [G loss: 0.542515]\n",
      "epoch:13 step:12964 [D loss: 0.249654, acc.: 60.16%] [G loss: 0.426319]\n",
      "epoch:13 step:12965 [D loss: 0.277585, acc.: 53.12%] [G loss: 0.422199]\n",
      "epoch:13 step:12966 [D loss: 0.244325, acc.: 66.41%] [G loss: 0.395129]\n",
      "epoch:13 step:12967 [D loss: 0.197165, acc.: 71.09%] [G loss: 0.464949]\n",
      "epoch:13 step:12968 [D loss: 0.241917, acc.: 59.38%] [G loss: 0.457591]\n",
      "epoch:13 step:12969 [D loss: 0.243127, acc.: 58.59%] [G loss: 0.423570]\n",
      "epoch:13 step:12970 [D loss: 0.219762, acc.: 61.72%] [G loss: 0.422101]\n",
      "epoch:13 step:12971 [D loss: 0.227191, acc.: 65.62%] [G loss: 0.465697]\n",
      "epoch:13 step:12972 [D loss: 0.273396, acc.: 50.78%] [G loss: 0.441567]\n",
      "epoch:13 step:12973 [D loss: 0.179788, acc.: 75.78%] [G loss: 0.432643]\n",
      "epoch:13 step:12974 [D loss: 0.208109, acc.: 60.94%] [G loss: 0.410357]\n",
      "epoch:13 step:12975 [D loss: 0.253437, acc.: 54.69%] [G loss: 0.461392]\n",
      "epoch:13 step:12976 [D loss: 0.255835, acc.: 54.69%] [G loss: 0.440541]\n",
      "epoch:13 step:12977 [D loss: 0.209007, acc.: 65.62%] [G loss: 0.461629]\n",
      "epoch:13 step:12978 [D loss: 0.231936, acc.: 56.25%] [G loss: 0.448486]\n",
      "epoch:13 step:12979 [D loss: 0.230598, acc.: 63.28%] [G loss: 0.447835]\n",
      "epoch:13 step:12980 [D loss: 0.216590, acc.: 66.41%] [G loss: 0.447053]\n",
      "epoch:13 step:12981 [D loss: 0.222210, acc.: 60.16%] [G loss: 0.426069]\n",
      "epoch:13 step:12982 [D loss: 0.197278, acc.: 67.19%] [G loss: 0.486954]\n",
      "epoch:13 step:12983 [D loss: 0.190371, acc.: 72.66%] [G loss: 0.517911]\n",
      "epoch:13 step:12984 [D loss: 0.248168, acc.: 57.81%] [G loss: 0.425804]\n",
      "epoch:13 step:12985 [D loss: 0.223333, acc.: 64.84%] [G loss: 0.437742]\n",
      "epoch:13 step:12986 [D loss: 0.222846, acc.: 62.50%] [G loss: 0.448242]\n",
      "epoch:13 step:12987 [D loss: 0.233788, acc.: 60.16%] [G loss: 0.456618]\n",
      "epoch:13 step:12988 [D loss: 0.207747, acc.: 67.97%] [G loss: 0.425879]\n",
      "epoch:13 step:12989 [D loss: 0.248466, acc.: 53.91%] [G loss: 0.447875]\n",
      "epoch:13 step:12990 [D loss: 0.217282, acc.: 63.28%] [G loss: 0.429141]\n",
      "epoch:13 step:12991 [D loss: 0.242957, acc.: 56.25%] [G loss: 0.425136]\n",
      "epoch:13 step:12992 [D loss: 0.213355, acc.: 64.06%] [G loss: 0.450177]\n",
      "epoch:13 step:12993 [D loss: 0.222461, acc.: 66.41%] [G loss: 0.434290]\n",
      "epoch:13 step:12994 [D loss: 0.240387, acc.: 60.94%] [G loss: 0.420002]\n",
      "epoch:13 step:12995 [D loss: 0.210558, acc.: 64.06%] [G loss: 0.503581]\n",
      "epoch:13 step:12996 [D loss: 0.201668, acc.: 67.19%] [G loss: 0.547955]\n",
      "epoch:13 step:12997 [D loss: 0.226832, acc.: 60.16%] [G loss: 0.466827]\n",
      "epoch:13 step:12998 [D loss: 0.221116, acc.: 64.84%] [G loss: 0.423043]\n",
      "epoch:13 step:12999 [D loss: 0.251859, acc.: 57.81%] [G loss: 0.464423]\n",
      "epoch:13 step:13000 [D loss: 0.220170, acc.: 63.28%] [G loss: 0.419865]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.835468\n",
      "FID: 8.743467\n",
      "0 = 11.842296580982234\n",
      "1 = 0.04385162539454669\n",
      "2 = 0.892300009727478\n",
      "3 = 0.8772000074386597\n",
      "4 = 0.9074000120162964\n",
      "5 = 0.9045163989067078\n",
      "6 = 0.8772000074386597\n",
      "7 = 6.233327904522428\n",
      "8 = 0.06172944953983247\n",
      "9 = 0.7056000232696533\n",
      "10 = 0.7046999931335449\n",
      "11 = 0.7064999938011169\n",
      "12 = 0.7059707641601562\n",
      "13 = 0.7046999931335449\n",
      "14 = 7.8355326652526855\n",
      "15 = 9.560334205627441\n",
      "16 = 0.10508455336093903\n",
      "17 = 7.83546781539917\n",
      "18 = 8.743467330932617\n",
      "epoch:13 step:13001 [D loss: 0.242239, acc.: 58.59%] [G loss: 0.427970]\n",
      "epoch:13 step:13002 [D loss: 0.220300, acc.: 60.16%] [G loss: 0.485055]\n",
      "epoch:13 step:13003 [D loss: 0.223094, acc.: 64.06%] [G loss: 0.445379]\n",
      "epoch:13 step:13004 [D loss: 0.205235, acc.: 64.84%] [G loss: 0.468575]\n",
      "epoch:13 step:13005 [D loss: 0.234214, acc.: 62.50%] [G loss: 0.426275]\n",
      "epoch:13 step:13006 [D loss: 0.196133, acc.: 71.88%] [G loss: 0.459086]\n",
      "epoch:13 step:13007 [D loss: 0.214333, acc.: 61.72%] [G loss: 0.492600]\n",
      "epoch:13 step:13008 [D loss: 0.246516, acc.: 60.94%] [G loss: 0.440579]\n",
      "epoch:13 step:13009 [D loss: 0.263662, acc.: 50.78%] [G loss: 0.397662]\n",
      "epoch:13 step:13010 [D loss: 0.223615, acc.: 67.97%] [G loss: 0.436678]\n",
      "epoch:13 step:13011 [D loss: 0.238243, acc.: 61.72%] [G loss: 0.465513]\n",
      "epoch:13 step:13012 [D loss: 0.205277, acc.: 69.53%] [G loss: 0.437877]\n",
      "epoch:13 step:13013 [D loss: 0.223056, acc.: 59.38%] [G loss: 0.421824]\n",
      "epoch:13 step:13014 [D loss: 0.226461, acc.: 64.06%] [G loss: 0.428421]\n",
      "epoch:13 step:13015 [D loss: 0.211931, acc.: 63.28%] [G loss: 0.418734]\n",
      "epoch:13 step:13016 [D loss: 0.221485, acc.: 58.59%] [G loss: 0.449311]\n",
      "epoch:13 step:13017 [D loss: 0.246369, acc.: 56.25%] [G loss: 0.419165]\n",
      "epoch:13 step:13018 [D loss: 0.204432, acc.: 69.53%] [G loss: 0.490397]\n",
      "epoch:13 step:13019 [D loss: 0.213913, acc.: 64.84%] [G loss: 0.450866]\n",
      "epoch:13 step:13020 [D loss: 0.214919, acc.: 68.75%] [G loss: 0.476316]\n",
      "epoch:13 step:13021 [D loss: 0.229720, acc.: 63.28%] [G loss: 0.457582]\n",
      "epoch:13 step:13022 [D loss: 0.202966, acc.: 71.09%] [G loss: 0.465692]\n",
      "epoch:13 step:13023 [D loss: 0.223339, acc.: 61.72%] [G loss: 0.424325]\n",
      "epoch:13 step:13024 [D loss: 0.227252, acc.: 66.41%] [G loss: 0.466609]\n",
      "epoch:13 step:13025 [D loss: 0.211948, acc.: 65.62%] [G loss: 0.467027]\n",
      "epoch:13 step:13026 [D loss: 0.216548, acc.: 64.84%] [G loss: 0.439115]\n",
      "epoch:13 step:13027 [D loss: 0.247474, acc.: 59.38%] [G loss: 0.442205]\n",
      "epoch:13 step:13028 [D loss: 0.227629, acc.: 56.25%] [G loss: 0.461957]\n",
      "epoch:13 step:13029 [D loss: 0.225283, acc.: 65.62%] [G loss: 0.433428]\n",
      "epoch:13 step:13030 [D loss: 0.203246, acc.: 71.09%] [G loss: 0.436752]\n",
      "epoch:13 step:13031 [D loss: 0.255101, acc.: 52.34%] [G loss: 0.402296]\n",
      "epoch:13 step:13032 [D loss: 0.238138, acc.: 57.81%] [G loss: 0.468634]\n",
      "epoch:13 step:13033 [D loss: 0.218770, acc.: 67.19%] [G loss: 0.430817]\n",
      "epoch:13 step:13034 [D loss: 0.206069, acc.: 67.97%] [G loss: 0.485157]\n",
      "epoch:13 step:13035 [D loss: 0.244515, acc.: 61.72%] [G loss: 0.431068]\n",
      "epoch:13 step:13036 [D loss: 0.233321, acc.: 62.50%] [G loss: 0.413254]\n",
      "epoch:13 step:13037 [D loss: 0.214789, acc.: 69.53%] [G loss: 0.428336]\n",
      "epoch:13 step:13038 [D loss: 0.234981, acc.: 62.50%] [G loss: 0.432285]\n",
      "epoch:13 step:13039 [D loss: 0.273770, acc.: 53.12%] [G loss: 0.437848]\n",
      "epoch:13 step:13040 [D loss: 0.265920, acc.: 54.69%] [G loss: 0.431978]\n",
      "epoch:13 step:13041 [D loss: 0.201680, acc.: 72.66%] [G loss: 0.492858]\n",
      "epoch:13 step:13042 [D loss: 0.262337, acc.: 51.56%] [G loss: 0.424236]\n",
      "epoch:13 step:13043 [D loss: 0.252671, acc.: 52.34%] [G loss: 0.420778]\n",
      "epoch:13 step:13044 [D loss: 0.218008, acc.: 64.06%] [G loss: 0.398147]\n",
      "epoch:13 step:13045 [D loss: 0.226198, acc.: 60.94%] [G loss: 0.421306]\n",
      "epoch:13 step:13046 [D loss: 0.270262, acc.: 44.53%] [G loss: 0.392148]\n",
      "epoch:13 step:13047 [D loss: 0.207464, acc.: 71.88%] [G loss: 0.436663]\n",
      "epoch:13 step:13048 [D loss: 0.253215, acc.: 54.69%] [G loss: 0.428870]\n",
      "epoch:13 step:13049 [D loss: 0.240909, acc.: 60.94%] [G loss: 0.421034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:13050 [D loss: 0.263798, acc.: 53.91%] [G loss: 0.460374]\n",
      "epoch:13 step:13051 [D loss: 0.218704, acc.: 64.06%] [G loss: 0.443675]\n",
      "epoch:13 step:13052 [D loss: 0.226880, acc.: 63.28%] [G loss: 0.451062]\n",
      "epoch:13 step:13053 [D loss: 0.236733, acc.: 64.06%] [G loss: 0.452048]\n",
      "epoch:13 step:13054 [D loss: 0.210417, acc.: 70.31%] [G loss: 0.457665]\n",
      "epoch:13 step:13055 [D loss: 0.214858, acc.: 62.50%] [G loss: 0.460445]\n",
      "epoch:13 step:13056 [D loss: 0.205358, acc.: 69.53%] [G loss: 0.463834]\n",
      "epoch:13 step:13057 [D loss: 0.232871, acc.: 60.94%] [G loss: 0.393299]\n",
      "epoch:13 step:13058 [D loss: 0.212312, acc.: 67.19%] [G loss: 0.479251]\n",
      "epoch:13 step:13059 [D loss: 0.216703, acc.: 64.84%] [G loss: 0.415469]\n",
      "epoch:13 step:13060 [D loss: 0.247361, acc.: 57.81%] [G loss: 0.418379]\n",
      "epoch:13 step:13061 [D loss: 0.252002, acc.: 57.03%] [G loss: 0.468226]\n",
      "epoch:13 step:13062 [D loss: 0.223350, acc.: 63.28%] [G loss: 0.425994]\n",
      "epoch:13 step:13063 [D loss: 0.206131, acc.: 69.53%] [G loss: 0.455999]\n",
      "epoch:13 step:13064 [D loss: 0.229138, acc.: 64.84%] [G loss: 0.444791]\n",
      "epoch:13 step:13065 [D loss: 0.210949, acc.: 64.06%] [G loss: 0.478547]\n",
      "epoch:13 step:13066 [D loss: 0.222931, acc.: 62.50%] [G loss: 0.473972]\n",
      "epoch:13 step:13067 [D loss: 0.190704, acc.: 69.53%] [G loss: 0.481604]\n",
      "epoch:13 step:13068 [D loss: 0.226905, acc.: 60.16%] [G loss: 0.472702]\n",
      "epoch:13 step:13069 [D loss: 0.243244, acc.: 60.94%] [G loss: 0.452285]\n",
      "epoch:13 step:13070 [D loss: 0.228203, acc.: 59.38%] [G loss: 0.456230]\n",
      "epoch:13 step:13071 [D loss: 0.180238, acc.: 78.12%] [G loss: 0.502279]\n",
      "epoch:13 step:13072 [D loss: 0.284612, acc.: 46.09%] [G loss: 0.431747]\n",
      "epoch:13 step:13073 [D loss: 0.272681, acc.: 44.53%] [G loss: 0.416821]\n",
      "epoch:13 step:13074 [D loss: 0.214596, acc.: 67.19%] [G loss: 0.409126]\n",
      "epoch:13 step:13075 [D loss: 0.216059, acc.: 65.62%] [G loss: 0.452202]\n",
      "epoch:13 step:13076 [D loss: 0.209339, acc.: 60.16%] [G loss: 0.453136]\n",
      "epoch:13 step:13077 [D loss: 0.192477, acc.: 68.75%] [G loss: 0.500349]\n",
      "epoch:13 step:13078 [D loss: 0.228577, acc.: 58.59%] [G loss: 0.461181]\n",
      "epoch:13 step:13079 [D loss: 0.216218, acc.: 64.06%] [G loss: 0.464006]\n",
      "epoch:13 step:13080 [D loss: 0.191046, acc.: 70.31%] [G loss: 0.489103]\n",
      "epoch:13 step:13081 [D loss: 0.203729, acc.: 72.66%] [G loss: 0.475863]\n",
      "epoch:13 step:13082 [D loss: 0.215537, acc.: 66.41%] [G loss: 0.486457]\n",
      "epoch:13 step:13083 [D loss: 0.241998, acc.: 59.38%] [G loss: 0.430164]\n",
      "epoch:13 step:13084 [D loss: 0.246345, acc.: 56.25%] [G loss: 0.444709]\n",
      "epoch:13 step:13085 [D loss: 0.234158, acc.: 54.69%] [G loss: 0.465262]\n",
      "epoch:13 step:13086 [D loss: 0.210716, acc.: 67.19%] [G loss: 0.481131]\n",
      "epoch:13 step:13087 [D loss: 0.201038, acc.: 69.53%] [G loss: 0.465442]\n",
      "epoch:13 step:13088 [D loss: 0.234083, acc.: 61.72%] [G loss: 0.444407]\n",
      "epoch:13 step:13089 [D loss: 0.213736, acc.: 68.75%] [G loss: 0.462640]\n",
      "epoch:13 step:13090 [D loss: 0.210531, acc.: 71.88%] [G loss: 0.434184]\n",
      "epoch:13 step:13091 [D loss: 0.218576, acc.: 64.84%] [G loss: 0.449145]\n",
      "epoch:13 step:13092 [D loss: 0.213620, acc.: 65.62%] [G loss: 0.458505]\n",
      "epoch:13 step:13093 [D loss: 0.203952, acc.: 63.28%] [G loss: 0.478383]\n",
      "epoch:13 step:13094 [D loss: 0.209404, acc.: 65.62%] [G loss: 0.513489]\n",
      "epoch:13 step:13095 [D loss: 0.211815, acc.: 64.84%] [G loss: 0.460309]\n",
      "epoch:13 step:13096 [D loss: 0.250432, acc.: 63.28%] [G loss: 0.411377]\n",
      "epoch:13 step:13097 [D loss: 0.251138, acc.: 55.47%] [G loss: 0.408042]\n",
      "epoch:13 step:13098 [D loss: 0.214691, acc.: 65.62%] [G loss: 0.467110]\n",
      "epoch:13 step:13099 [D loss: 0.179406, acc.: 75.78%] [G loss: 0.525723]\n",
      "epoch:13 step:13100 [D loss: 0.203528, acc.: 66.41%] [G loss: 0.515163]\n",
      "epoch:13 step:13101 [D loss: 0.299223, acc.: 46.88%] [G loss: 0.441069]\n",
      "epoch:13 step:13102 [D loss: 0.168275, acc.: 77.34%] [G loss: 0.453743]\n",
      "epoch:13 step:13103 [D loss: 0.228546, acc.: 65.62%] [G loss: 0.458127]\n",
      "epoch:13 step:13104 [D loss: 0.174972, acc.: 78.12%] [G loss: 0.485652]\n",
      "epoch:13 step:13105 [D loss: 0.194983, acc.: 71.09%] [G loss: 0.483080]\n",
      "epoch:13 step:13106 [D loss: 0.172059, acc.: 74.22%] [G loss: 0.511834]\n",
      "epoch:13 step:13107 [D loss: 0.195754, acc.: 73.44%] [G loss: 0.529301]\n",
      "epoch:13 step:13108 [D loss: 0.204827, acc.: 68.75%] [G loss: 0.572174]\n",
      "epoch:13 step:13109 [D loss: 0.259459, acc.: 62.50%] [G loss: 0.573403]\n",
      "epoch:13 step:13110 [D loss: 0.203954, acc.: 71.88%] [G loss: 0.652858]\n",
      "epoch:13 step:13111 [D loss: 0.246559, acc.: 63.28%] [G loss: 0.524229]\n",
      "epoch:13 step:13112 [D loss: 0.265582, acc.: 57.03%] [G loss: 0.410266]\n",
      "epoch:13 step:13113 [D loss: 0.259538, acc.: 54.69%] [G loss: 0.396648]\n",
      "epoch:13 step:13114 [D loss: 0.212885, acc.: 65.62%] [G loss: 0.462324]\n",
      "epoch:13 step:13115 [D loss: 0.220633, acc.: 64.84%] [G loss: 0.499778]\n",
      "epoch:13 step:13116 [D loss: 0.187345, acc.: 71.09%] [G loss: 0.517677]\n",
      "epoch:13 step:13117 [D loss: 0.178380, acc.: 70.31%] [G loss: 0.545486]\n",
      "epoch:13 step:13118 [D loss: 0.186555, acc.: 74.22%] [G loss: 0.557616]\n",
      "epoch:14 step:13119 [D loss: 0.252825, acc.: 60.94%] [G loss: 0.489845]\n",
      "epoch:14 step:13120 [D loss: 0.247281, acc.: 53.91%] [G loss: 0.479754]\n",
      "epoch:14 step:13121 [D loss: 0.258496, acc.: 51.56%] [G loss: 0.426825]\n",
      "epoch:14 step:13122 [D loss: 0.238669, acc.: 59.38%] [G loss: 0.491189]\n",
      "epoch:14 step:13123 [D loss: 0.223916, acc.: 61.72%] [G loss: 0.480250]\n",
      "epoch:14 step:13124 [D loss: 0.216613, acc.: 65.62%] [G loss: 0.454250]\n",
      "epoch:14 step:13125 [D loss: 0.212982, acc.: 67.97%] [G loss: 0.450195]\n",
      "epoch:14 step:13126 [D loss: 0.228670, acc.: 60.16%] [G loss: 0.464063]\n",
      "epoch:14 step:13127 [D loss: 0.199240, acc.: 71.09%] [G loss: 0.486009]\n",
      "epoch:14 step:13128 [D loss: 0.226344, acc.: 64.06%] [G loss: 0.490994]\n",
      "epoch:14 step:13129 [D loss: 0.208906, acc.: 66.41%] [G loss: 0.480482]\n",
      "epoch:14 step:13130 [D loss: 0.241711, acc.: 57.03%] [G loss: 0.455224]\n",
      "epoch:14 step:13131 [D loss: 0.237859, acc.: 56.25%] [G loss: 0.451253]\n",
      "epoch:14 step:13132 [D loss: 0.222198, acc.: 65.62%] [G loss: 0.469303]\n",
      "epoch:14 step:13133 [D loss: 0.208508, acc.: 71.88%] [G loss: 0.501920]\n",
      "epoch:14 step:13134 [D loss: 0.179091, acc.: 71.09%] [G loss: 0.485006]\n",
      "epoch:14 step:13135 [D loss: 0.272691, acc.: 45.31%] [G loss: 0.424825]\n",
      "epoch:14 step:13136 [D loss: 0.220671, acc.: 67.97%] [G loss: 0.458655]\n",
      "epoch:14 step:13137 [D loss: 0.243312, acc.: 61.72%] [G loss: 0.402402]\n",
      "epoch:14 step:13138 [D loss: 0.244525, acc.: 59.38%] [G loss: 0.432646]\n",
      "epoch:14 step:13139 [D loss: 0.256597, acc.: 50.00%] [G loss: 0.478013]\n",
      "epoch:14 step:13140 [D loss: 0.223851, acc.: 60.94%] [G loss: 0.485520]\n",
      "epoch:14 step:13141 [D loss: 0.235663, acc.: 59.38%] [G loss: 0.486788]\n",
      "epoch:14 step:13142 [D loss: 0.206729, acc.: 66.41%] [G loss: 0.454622]\n",
      "epoch:14 step:13143 [D loss: 0.196123, acc.: 67.97%] [G loss: 0.460015]\n",
      "epoch:14 step:13144 [D loss: 0.224475, acc.: 61.72%] [G loss: 0.451441]\n",
      "epoch:14 step:13145 [D loss: 0.240544, acc.: 55.47%] [G loss: 0.425870]\n",
      "epoch:14 step:13146 [D loss: 0.223521, acc.: 61.72%] [G loss: 0.459582]\n",
      "epoch:14 step:13147 [D loss: 0.220926, acc.: 65.62%] [G loss: 0.460788]\n",
      "epoch:14 step:13148 [D loss: 0.233189, acc.: 59.38%] [G loss: 0.425791]\n",
      "epoch:14 step:13149 [D loss: 0.249015, acc.: 57.81%] [G loss: 0.398178]\n",
      "epoch:14 step:13150 [D loss: 0.204334, acc.: 69.53%] [G loss: 0.494370]\n",
      "epoch:14 step:13151 [D loss: 0.226999, acc.: 60.16%] [G loss: 0.448851]\n",
      "epoch:14 step:13152 [D loss: 0.218837, acc.: 65.62%] [G loss: 0.424088]\n",
      "epoch:14 step:13153 [D loss: 0.229179, acc.: 63.28%] [G loss: 0.445239]\n",
      "epoch:14 step:13154 [D loss: 0.218492, acc.: 63.28%] [G loss: 0.475906]\n",
      "epoch:14 step:13155 [D loss: 0.223460, acc.: 60.94%] [G loss: 0.459234]\n",
      "epoch:14 step:13156 [D loss: 0.260378, acc.: 55.47%] [G loss: 0.430169]\n",
      "epoch:14 step:13157 [D loss: 0.246217, acc.: 56.25%] [G loss: 0.396604]\n",
      "epoch:14 step:13158 [D loss: 0.184514, acc.: 74.22%] [G loss: 0.464024]\n",
      "epoch:14 step:13159 [D loss: 0.236649, acc.: 59.38%] [G loss: 0.424399]\n",
      "epoch:14 step:13160 [D loss: 0.192256, acc.: 73.44%] [G loss: 0.460261]\n",
      "epoch:14 step:13161 [D loss: 0.214675, acc.: 68.75%] [G loss: 0.461447]\n",
      "epoch:14 step:13162 [D loss: 0.233897, acc.: 62.50%] [G loss: 0.418043]\n",
      "epoch:14 step:13163 [D loss: 0.219555, acc.: 64.84%] [G loss: 0.387755]\n",
      "epoch:14 step:13164 [D loss: 0.214745, acc.: 62.50%] [G loss: 0.431263]\n",
      "epoch:14 step:13165 [D loss: 0.238133, acc.: 58.59%] [G loss: 0.427549]\n",
      "epoch:14 step:13166 [D loss: 0.211607, acc.: 65.62%] [G loss: 0.428977]\n",
      "epoch:14 step:13167 [D loss: 0.197330, acc.: 72.66%] [G loss: 0.507489]\n",
      "epoch:14 step:13168 [D loss: 0.195370, acc.: 71.09%] [G loss: 0.430741]\n",
      "epoch:14 step:13169 [D loss: 0.230406, acc.: 62.50%] [G loss: 0.421326]\n",
      "epoch:14 step:13170 [D loss: 0.237951, acc.: 60.94%] [G loss: 0.461828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13171 [D loss: 0.212097, acc.: 67.19%] [G loss: 0.481723]\n",
      "epoch:14 step:13172 [D loss: 0.212668, acc.: 64.84%] [G loss: 0.493786]\n",
      "epoch:14 step:13173 [D loss: 0.213341, acc.: 63.28%] [G loss: 0.474327]\n",
      "epoch:14 step:13174 [D loss: 0.230124, acc.: 65.62%] [G loss: 0.460486]\n",
      "epoch:14 step:13175 [D loss: 0.216022, acc.: 71.88%] [G loss: 0.465060]\n",
      "epoch:14 step:13176 [D loss: 0.219228, acc.: 64.84%] [G loss: 0.508270]\n",
      "epoch:14 step:13177 [D loss: 0.220352, acc.: 61.72%] [G loss: 0.453927]\n",
      "epoch:14 step:13178 [D loss: 0.238150, acc.: 62.50%] [G loss: 0.436085]\n",
      "epoch:14 step:13179 [D loss: 0.228732, acc.: 63.28%] [G loss: 0.439876]\n",
      "epoch:14 step:13180 [D loss: 0.258863, acc.: 57.81%] [G loss: 0.409277]\n",
      "epoch:14 step:13181 [D loss: 0.224578, acc.: 64.06%] [G loss: 0.471143]\n",
      "epoch:14 step:13182 [D loss: 0.256647, acc.: 54.69%] [G loss: 0.450787]\n",
      "epoch:14 step:13183 [D loss: 0.231912, acc.: 57.81%] [G loss: 0.424572]\n",
      "epoch:14 step:13184 [D loss: 0.206080, acc.: 62.50%] [G loss: 0.442283]\n",
      "epoch:14 step:13185 [D loss: 0.220018, acc.: 62.50%] [G loss: 0.481869]\n",
      "epoch:14 step:13186 [D loss: 0.226854, acc.: 64.06%] [G loss: 0.470440]\n",
      "epoch:14 step:13187 [D loss: 0.191671, acc.: 68.75%] [G loss: 0.466186]\n",
      "epoch:14 step:13188 [D loss: 0.194367, acc.: 70.31%] [G loss: 0.494022]\n",
      "epoch:14 step:13189 [D loss: 0.246668, acc.: 55.47%] [G loss: 0.444196]\n",
      "epoch:14 step:13190 [D loss: 0.238823, acc.: 62.50%] [G loss: 0.448988]\n",
      "epoch:14 step:13191 [D loss: 0.230452, acc.: 60.16%] [G loss: 0.415521]\n",
      "epoch:14 step:13192 [D loss: 0.190859, acc.: 72.66%] [G loss: 0.445786]\n",
      "epoch:14 step:13193 [D loss: 0.234158, acc.: 61.72%] [G loss: 0.473767]\n",
      "epoch:14 step:13194 [D loss: 0.187404, acc.: 73.44%] [G loss: 0.471977]\n",
      "epoch:14 step:13195 [D loss: 0.187837, acc.: 72.66%] [G loss: 0.493974]\n",
      "epoch:14 step:13196 [D loss: 0.253287, acc.: 55.47%] [G loss: 0.441642]\n",
      "epoch:14 step:13197 [D loss: 0.249281, acc.: 54.69%] [G loss: 0.389254]\n",
      "epoch:14 step:13198 [D loss: 0.203356, acc.: 63.28%] [G loss: 0.447478]\n",
      "epoch:14 step:13199 [D loss: 0.241518, acc.: 60.94%] [G loss: 0.405189]\n",
      "epoch:14 step:13200 [D loss: 0.210995, acc.: 63.28%] [G loss: 0.445072]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.861584\n",
      "FID: 8.679165\n",
      "0 = 11.809620910501485\n",
      "1 = 0.047943649434707865\n",
      "2 = 0.8849999904632568\n",
      "3 = 0.8719000220298767\n",
      "4 = 0.8981000185012817\n",
      "5 = 0.8953583836555481\n",
      "6 = 0.8719000220298767\n",
      "7 = 6.0662010310620005\n",
      "8 = 0.06416673321741129\n",
      "9 = 0.7027000188827515\n",
      "10 = 0.7021999955177307\n",
      "11 = 0.7031999826431274\n",
      "12 = 0.7029029130935669\n",
      "13 = 0.7021999955177307\n",
      "14 = 7.861655235290527\n",
      "15 = 9.521064758300781\n",
      "16 = 0.11493969708681107\n",
      "17 = 7.861583709716797\n",
      "18 = 8.67916488647461\n",
      "epoch:14 step:13201 [D loss: 0.220158, acc.: 64.84%] [G loss: 0.430234]\n",
      "epoch:14 step:13202 [D loss: 0.219725, acc.: 64.06%] [G loss: 0.444110]\n",
      "epoch:14 step:13203 [D loss: 0.252085, acc.: 57.81%] [G loss: 0.431887]\n",
      "epoch:14 step:13204 [D loss: 0.218889, acc.: 67.19%] [G loss: 0.435386]\n",
      "epoch:14 step:13205 [D loss: 0.231629, acc.: 64.06%] [G loss: 0.471774]\n",
      "epoch:14 step:13206 [D loss: 0.201705, acc.: 70.31%] [G loss: 0.469186]\n",
      "epoch:14 step:13207 [D loss: 0.199209, acc.: 68.75%] [G loss: 0.455733]\n",
      "epoch:14 step:13208 [D loss: 0.217221, acc.: 65.62%] [G loss: 0.412656]\n",
      "epoch:14 step:13209 [D loss: 0.251348, acc.: 58.59%] [G loss: 0.417706]\n",
      "epoch:14 step:13210 [D loss: 0.235736, acc.: 62.50%] [G loss: 0.418518]\n",
      "epoch:14 step:13211 [D loss: 0.206832, acc.: 70.31%] [G loss: 0.445595]\n",
      "epoch:14 step:13212 [D loss: 0.232543, acc.: 60.94%] [G loss: 0.476265]\n",
      "epoch:14 step:13213 [D loss: 0.223164, acc.: 64.06%] [G loss: 0.425468]\n",
      "epoch:14 step:13214 [D loss: 0.189279, acc.: 68.75%] [G loss: 0.478878]\n",
      "epoch:14 step:13215 [D loss: 0.220554, acc.: 67.19%] [G loss: 0.507284]\n",
      "epoch:14 step:13216 [D loss: 0.239194, acc.: 59.38%] [G loss: 0.487152]\n",
      "epoch:14 step:13217 [D loss: 0.224052, acc.: 64.84%] [G loss: 0.471236]\n",
      "epoch:14 step:13218 [D loss: 0.205566, acc.: 71.88%] [G loss: 0.478187]\n",
      "epoch:14 step:13219 [D loss: 0.228973, acc.: 57.81%] [G loss: 0.475192]\n",
      "epoch:14 step:13220 [D loss: 0.249936, acc.: 60.94%] [G loss: 0.450961]\n",
      "epoch:14 step:13221 [D loss: 0.239608, acc.: 61.72%] [G loss: 0.429291]\n",
      "epoch:14 step:13222 [D loss: 0.250497, acc.: 50.78%] [G loss: 0.415220]\n",
      "epoch:14 step:13223 [D loss: 0.232463, acc.: 59.38%] [G loss: 0.462528]\n",
      "epoch:14 step:13224 [D loss: 0.213955, acc.: 67.19%] [G loss: 0.428799]\n",
      "epoch:14 step:13225 [D loss: 0.196222, acc.: 73.44%] [G loss: 0.519635]\n",
      "epoch:14 step:13226 [D loss: 0.275173, acc.: 53.12%] [G loss: 0.418543]\n",
      "epoch:14 step:13227 [D loss: 0.275714, acc.: 48.44%] [G loss: 0.392031]\n",
      "epoch:14 step:13228 [D loss: 0.256029, acc.: 56.25%] [G loss: 0.403487]\n",
      "epoch:14 step:13229 [D loss: 0.207413, acc.: 69.53%] [G loss: 0.463864]\n",
      "epoch:14 step:13230 [D loss: 0.216873, acc.: 67.97%] [G loss: 0.495507]\n",
      "epoch:14 step:13231 [D loss: 0.230378, acc.: 60.16%] [G loss: 0.491454]\n",
      "epoch:14 step:13232 [D loss: 0.205207, acc.: 72.66%] [G loss: 0.515227]\n",
      "epoch:14 step:13233 [D loss: 0.200912, acc.: 69.53%] [G loss: 0.498173]\n",
      "epoch:14 step:13234 [D loss: 0.207406, acc.: 67.19%] [G loss: 0.511095]\n",
      "epoch:14 step:13235 [D loss: 0.197256, acc.: 70.31%] [G loss: 0.482876]\n",
      "epoch:14 step:13236 [D loss: 0.234701, acc.: 62.50%] [G loss: 0.459814]\n",
      "epoch:14 step:13237 [D loss: 0.172131, acc.: 78.12%] [G loss: 0.493259]\n",
      "epoch:14 step:13238 [D loss: 0.255623, acc.: 63.28%] [G loss: 0.468441]\n",
      "epoch:14 step:13239 [D loss: 0.243227, acc.: 58.59%] [G loss: 0.451091]\n",
      "epoch:14 step:13240 [D loss: 0.177701, acc.: 75.00%] [G loss: 0.488690]\n",
      "epoch:14 step:13241 [D loss: 0.213411, acc.: 60.94%] [G loss: 0.456709]\n",
      "epoch:14 step:13242 [D loss: 0.273723, acc.: 50.78%] [G loss: 0.468878]\n",
      "epoch:14 step:13243 [D loss: 0.230096, acc.: 60.16%] [G loss: 0.446523]\n",
      "epoch:14 step:13244 [D loss: 0.195191, acc.: 71.09%] [G loss: 0.473844]\n",
      "epoch:14 step:13245 [D loss: 0.247164, acc.: 55.47%] [G loss: 0.422900]\n",
      "epoch:14 step:13246 [D loss: 0.232019, acc.: 55.47%] [G loss: 0.400927]\n",
      "epoch:14 step:13247 [D loss: 0.231770, acc.: 62.50%] [G loss: 0.410318]\n",
      "epoch:14 step:13248 [D loss: 0.221058, acc.: 60.94%] [G loss: 0.423773]\n",
      "epoch:14 step:13249 [D loss: 0.208484, acc.: 64.84%] [G loss: 0.508879]\n",
      "epoch:14 step:13250 [D loss: 0.225540, acc.: 65.62%] [G loss: 0.471696]\n",
      "epoch:14 step:13251 [D loss: 0.235580, acc.: 63.28%] [G loss: 0.481210]\n",
      "epoch:14 step:13252 [D loss: 0.207515, acc.: 66.41%] [G loss: 0.503145]\n",
      "epoch:14 step:13253 [D loss: 0.220100, acc.: 65.62%] [G loss: 0.452841]\n",
      "epoch:14 step:13254 [D loss: 0.197885, acc.: 69.53%] [G loss: 0.483070]\n",
      "epoch:14 step:13255 [D loss: 0.267103, acc.: 53.91%] [G loss: 0.458528]\n",
      "epoch:14 step:13256 [D loss: 0.236290, acc.: 53.91%] [G loss: 0.442637]\n",
      "epoch:14 step:13257 [D loss: 0.227224, acc.: 65.62%] [G loss: 0.428185]\n",
      "epoch:14 step:13258 [D loss: 0.225484, acc.: 64.06%] [G loss: 0.444897]\n",
      "epoch:14 step:13259 [D loss: 0.243018, acc.: 55.47%] [G loss: 0.437086]\n",
      "epoch:14 step:13260 [D loss: 0.238784, acc.: 59.38%] [G loss: 0.433950]\n",
      "epoch:14 step:13261 [D loss: 0.226818, acc.: 58.59%] [G loss: 0.440166]\n",
      "epoch:14 step:13262 [D loss: 0.190982, acc.: 67.19%] [G loss: 0.448393]\n",
      "epoch:14 step:13263 [D loss: 0.226725, acc.: 66.41%] [G loss: 0.446023]\n",
      "epoch:14 step:13264 [D loss: 0.221560, acc.: 59.38%] [G loss: 0.459444]\n",
      "epoch:14 step:13265 [D loss: 0.236132, acc.: 60.16%] [G loss: 0.433354]\n",
      "epoch:14 step:13266 [D loss: 0.280110, acc.: 52.34%] [G loss: 0.398517]\n",
      "epoch:14 step:13267 [D loss: 0.197560, acc.: 69.53%] [G loss: 0.428627]\n",
      "epoch:14 step:13268 [D loss: 0.207610, acc.: 63.28%] [G loss: 0.449338]\n",
      "epoch:14 step:13269 [D loss: 0.224234, acc.: 63.28%] [G loss: 0.429186]\n",
      "epoch:14 step:13270 [D loss: 0.217639, acc.: 67.97%] [G loss: 0.486830]\n",
      "epoch:14 step:13271 [D loss: 0.249424, acc.: 59.38%] [G loss: 0.422667]\n",
      "epoch:14 step:13272 [D loss: 0.234360, acc.: 64.84%] [G loss: 0.448114]\n",
      "epoch:14 step:13273 [D loss: 0.219010, acc.: 58.59%] [G loss: 0.440480]\n",
      "epoch:14 step:13274 [D loss: 0.220448, acc.: 64.84%] [G loss: 0.434443]\n",
      "epoch:14 step:13275 [D loss: 0.219801, acc.: 64.84%] [G loss: 0.465523]\n",
      "epoch:14 step:13276 [D loss: 0.232747, acc.: 64.84%] [G loss: 0.438883]\n",
      "epoch:14 step:13277 [D loss: 0.232551, acc.: 59.38%] [G loss: 0.484199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13278 [D loss: 0.293696, acc.: 48.44%] [G loss: 0.420367]\n",
      "epoch:14 step:13279 [D loss: 0.224888, acc.: 65.62%] [G loss: 0.469481]\n",
      "epoch:14 step:13280 [D loss: 0.218972, acc.: 66.41%] [G loss: 0.487147]\n",
      "epoch:14 step:13281 [D loss: 0.262781, acc.: 53.91%] [G loss: 0.463508]\n",
      "epoch:14 step:13282 [D loss: 0.231239, acc.: 64.84%] [G loss: 0.467890]\n",
      "epoch:14 step:13283 [D loss: 0.223557, acc.: 65.62%] [G loss: 0.417201]\n",
      "epoch:14 step:13284 [D loss: 0.231860, acc.: 62.50%] [G loss: 0.416446]\n",
      "epoch:14 step:13285 [D loss: 0.224843, acc.: 65.62%] [G loss: 0.454354]\n",
      "epoch:14 step:13286 [D loss: 0.208347, acc.: 67.19%] [G loss: 0.438777]\n",
      "epoch:14 step:13287 [D loss: 0.210301, acc.: 63.28%] [G loss: 0.452201]\n",
      "epoch:14 step:13288 [D loss: 0.248322, acc.: 53.12%] [G loss: 0.414240]\n",
      "epoch:14 step:13289 [D loss: 0.208767, acc.: 66.41%] [G loss: 0.423845]\n",
      "epoch:14 step:13290 [D loss: 0.228198, acc.: 57.03%] [G loss: 0.456168]\n",
      "epoch:14 step:13291 [D loss: 0.197938, acc.: 71.88%] [G loss: 0.479329]\n",
      "epoch:14 step:13292 [D loss: 0.261588, acc.: 52.34%] [G loss: 0.426969]\n",
      "epoch:14 step:13293 [D loss: 0.237945, acc.: 59.38%] [G loss: 0.436040]\n",
      "epoch:14 step:13294 [D loss: 0.217124, acc.: 67.19%] [G loss: 0.443624]\n",
      "epoch:14 step:13295 [D loss: 0.251210, acc.: 56.25%] [G loss: 0.412423]\n",
      "epoch:14 step:13296 [D loss: 0.245797, acc.: 55.47%] [G loss: 0.412164]\n",
      "epoch:14 step:13297 [D loss: 0.245084, acc.: 57.81%] [G loss: 0.437613]\n",
      "epoch:14 step:13298 [D loss: 0.244520, acc.: 55.47%] [G loss: 0.446961]\n",
      "epoch:14 step:13299 [D loss: 0.229033, acc.: 64.84%] [G loss: 0.410162]\n",
      "epoch:14 step:13300 [D loss: 0.226554, acc.: 61.72%] [G loss: 0.441075]\n",
      "epoch:14 step:13301 [D loss: 0.241627, acc.: 60.16%] [G loss: 0.406728]\n",
      "epoch:14 step:13302 [D loss: 0.238006, acc.: 64.06%] [G loss: 0.424836]\n",
      "epoch:14 step:13303 [D loss: 0.214635, acc.: 67.97%] [G loss: 0.460688]\n",
      "epoch:14 step:13304 [D loss: 0.234781, acc.: 60.16%] [G loss: 0.439606]\n",
      "epoch:14 step:13305 [D loss: 0.235632, acc.: 60.16%] [G loss: 0.425505]\n",
      "epoch:14 step:13306 [D loss: 0.211625, acc.: 66.41%] [G loss: 0.442875]\n",
      "epoch:14 step:13307 [D loss: 0.231125, acc.: 59.38%] [G loss: 0.441904]\n",
      "epoch:14 step:13308 [D loss: 0.234299, acc.: 62.50%] [G loss: 0.412130]\n",
      "epoch:14 step:13309 [D loss: 0.212619, acc.: 64.06%] [G loss: 0.448067]\n",
      "epoch:14 step:13310 [D loss: 0.202340, acc.: 67.97%] [G loss: 0.436006]\n",
      "epoch:14 step:13311 [D loss: 0.236226, acc.: 58.59%] [G loss: 0.403878]\n",
      "epoch:14 step:13312 [D loss: 0.185135, acc.: 75.78%] [G loss: 0.467603]\n",
      "epoch:14 step:13313 [D loss: 0.206572, acc.: 68.75%] [G loss: 0.461029]\n",
      "epoch:14 step:13314 [D loss: 0.237230, acc.: 57.81%] [G loss: 0.447901]\n",
      "epoch:14 step:13315 [D loss: 0.207337, acc.: 67.97%] [G loss: 0.431720]\n",
      "epoch:14 step:13316 [D loss: 0.203006, acc.: 67.97%] [G loss: 0.469469]\n",
      "epoch:14 step:13317 [D loss: 0.213370, acc.: 64.84%] [G loss: 0.454524]\n",
      "epoch:14 step:13318 [D loss: 0.244771, acc.: 55.47%] [G loss: 0.440244]\n",
      "epoch:14 step:13319 [D loss: 0.222984, acc.: 58.59%] [G loss: 0.440190]\n",
      "epoch:14 step:13320 [D loss: 0.238389, acc.: 65.62%] [G loss: 0.455455]\n",
      "epoch:14 step:13321 [D loss: 0.248567, acc.: 57.03%] [G loss: 0.447886]\n",
      "epoch:14 step:13322 [D loss: 0.244633, acc.: 60.16%] [G loss: 0.413873]\n",
      "epoch:14 step:13323 [D loss: 0.218157, acc.: 61.72%] [G loss: 0.453031]\n",
      "epoch:14 step:13324 [D loss: 0.195847, acc.: 71.09%] [G loss: 0.460482]\n",
      "epoch:14 step:13325 [D loss: 0.183777, acc.: 71.09%] [G loss: 0.482273]\n",
      "epoch:14 step:13326 [D loss: 0.196378, acc.: 68.75%] [G loss: 0.459721]\n",
      "epoch:14 step:13327 [D loss: 0.186206, acc.: 67.97%] [G loss: 0.500933]\n",
      "epoch:14 step:13328 [D loss: 0.245378, acc.: 54.69%] [G loss: 0.450402]\n",
      "epoch:14 step:13329 [D loss: 0.269188, acc.: 50.00%] [G loss: 0.403630]\n",
      "epoch:14 step:13330 [D loss: 0.204990, acc.: 62.50%] [G loss: 0.476280]\n",
      "epoch:14 step:13331 [D loss: 0.230207, acc.: 61.72%] [G loss: 0.422818]\n",
      "epoch:14 step:13332 [D loss: 0.252122, acc.: 60.16%] [G loss: 0.398992]\n",
      "epoch:14 step:13333 [D loss: 0.234667, acc.: 60.16%] [G loss: 0.446122]\n",
      "epoch:14 step:13334 [D loss: 0.213368, acc.: 64.84%] [G loss: 0.429451]\n",
      "epoch:14 step:13335 [D loss: 0.206335, acc.: 67.19%] [G loss: 0.423306]\n",
      "epoch:14 step:13336 [D loss: 0.192433, acc.: 70.31%] [G loss: 0.454816]\n",
      "epoch:14 step:13337 [D loss: 0.209826, acc.: 65.62%] [G loss: 0.466314]\n",
      "epoch:14 step:13338 [D loss: 0.268513, acc.: 53.91%] [G loss: 0.435471]\n",
      "epoch:14 step:13339 [D loss: 0.195231, acc.: 73.44%] [G loss: 0.511433]\n",
      "epoch:14 step:13340 [D loss: 0.219765, acc.: 59.38%] [G loss: 0.491386]\n",
      "epoch:14 step:13341 [D loss: 0.190787, acc.: 72.66%] [G loss: 0.526200]\n",
      "epoch:14 step:13342 [D loss: 0.259198, acc.: 55.47%] [G loss: 0.440156]\n",
      "epoch:14 step:13343 [D loss: 0.224065, acc.: 64.84%] [G loss: 0.435978]\n",
      "epoch:14 step:13344 [D loss: 0.263253, acc.: 53.12%] [G loss: 0.397752]\n",
      "epoch:14 step:13345 [D loss: 0.225834, acc.: 63.28%] [G loss: 0.464414]\n",
      "epoch:14 step:13346 [D loss: 0.252522, acc.: 53.91%] [G loss: 0.421106]\n",
      "epoch:14 step:13347 [D loss: 0.221973, acc.: 66.41%] [G loss: 0.444236]\n",
      "epoch:14 step:13348 [D loss: 0.228154, acc.: 60.94%] [G loss: 0.446166]\n",
      "epoch:14 step:13349 [D loss: 0.169852, acc.: 75.78%] [G loss: 0.506385]\n",
      "epoch:14 step:13350 [D loss: 0.199713, acc.: 75.00%] [G loss: 0.517177]\n",
      "epoch:14 step:13351 [D loss: 0.239032, acc.: 60.16%] [G loss: 0.500007]\n",
      "epoch:14 step:13352 [D loss: 0.258367, acc.: 53.91%] [G loss: 0.449009]\n",
      "epoch:14 step:13353 [D loss: 0.231220, acc.: 63.28%] [G loss: 0.414065]\n",
      "epoch:14 step:13354 [D loss: 0.229463, acc.: 60.94%] [G loss: 0.391489]\n",
      "epoch:14 step:13355 [D loss: 0.218340, acc.: 63.28%] [G loss: 0.466725]\n",
      "epoch:14 step:13356 [D loss: 0.249164, acc.: 59.38%] [G loss: 0.472101]\n",
      "epoch:14 step:13357 [D loss: 0.205332, acc.: 68.75%] [G loss: 0.436593]\n",
      "epoch:14 step:13358 [D loss: 0.241184, acc.: 60.16%] [G loss: 0.441277]\n",
      "epoch:14 step:13359 [D loss: 0.213916, acc.: 64.06%] [G loss: 0.458234]\n",
      "epoch:14 step:13360 [D loss: 0.200786, acc.: 69.53%] [G loss: 0.455860]\n",
      "epoch:14 step:13361 [D loss: 0.221785, acc.: 64.06%] [G loss: 0.483768]\n",
      "epoch:14 step:13362 [D loss: 0.233317, acc.: 59.38%] [G loss: 0.430103]\n",
      "epoch:14 step:13363 [D loss: 0.204710, acc.: 68.75%] [G loss: 0.468281]\n",
      "epoch:14 step:13364 [D loss: 0.201077, acc.: 64.06%] [G loss: 0.481471]\n",
      "epoch:14 step:13365 [D loss: 0.232050, acc.: 63.28%] [G loss: 0.449216]\n",
      "epoch:14 step:13366 [D loss: 0.228468, acc.: 57.81%] [G loss: 0.488617]\n",
      "epoch:14 step:13367 [D loss: 0.261579, acc.: 54.69%] [G loss: 0.463468]\n",
      "epoch:14 step:13368 [D loss: 0.256258, acc.: 54.69%] [G loss: 0.450664]\n",
      "epoch:14 step:13369 [D loss: 0.247369, acc.: 57.81%] [G loss: 0.497118]\n",
      "epoch:14 step:13370 [D loss: 0.247309, acc.: 59.38%] [G loss: 0.460887]\n",
      "epoch:14 step:13371 [D loss: 0.216722, acc.: 71.09%] [G loss: 0.449127]\n",
      "epoch:14 step:13372 [D loss: 0.204220, acc.: 68.75%] [G loss: 0.453706]\n",
      "epoch:14 step:13373 [D loss: 0.216027, acc.: 64.84%] [G loss: 0.425227]\n",
      "epoch:14 step:13374 [D loss: 0.226443, acc.: 60.94%] [G loss: 0.439459]\n",
      "epoch:14 step:13375 [D loss: 0.232004, acc.: 63.28%] [G loss: 0.443485]\n",
      "epoch:14 step:13376 [D loss: 0.216449, acc.: 62.50%] [G loss: 0.415513]\n",
      "epoch:14 step:13377 [D loss: 0.226448, acc.: 66.41%] [G loss: 0.438255]\n",
      "epoch:14 step:13378 [D loss: 0.245992, acc.: 58.59%] [G loss: 0.425481]\n",
      "epoch:14 step:13379 [D loss: 0.196067, acc.: 71.88%] [G loss: 0.480379]\n",
      "epoch:14 step:13380 [D loss: 0.202953, acc.: 69.53%] [G loss: 0.467947]\n",
      "epoch:14 step:13381 [D loss: 0.243166, acc.: 63.28%] [G loss: 0.458730]\n",
      "epoch:14 step:13382 [D loss: 0.188653, acc.: 74.22%] [G loss: 0.448355]\n",
      "epoch:14 step:13383 [D loss: 0.239467, acc.: 60.16%] [G loss: 0.399208]\n",
      "epoch:14 step:13384 [D loss: 0.243928, acc.: 62.50%] [G loss: 0.400920]\n",
      "epoch:14 step:13385 [D loss: 0.213704, acc.: 63.28%] [G loss: 0.473826]\n",
      "epoch:14 step:13386 [D loss: 0.228177, acc.: 64.84%] [G loss: 0.440311]\n",
      "epoch:14 step:13387 [D loss: 0.211532, acc.: 64.84%] [G loss: 0.449327]\n",
      "epoch:14 step:13388 [D loss: 0.219345, acc.: 66.41%] [G loss: 0.479671]\n",
      "epoch:14 step:13389 [D loss: 0.219783, acc.: 67.19%] [G loss: 0.440328]\n",
      "epoch:14 step:13390 [D loss: 0.228070, acc.: 60.94%] [G loss: 0.482779]\n",
      "epoch:14 step:13391 [D loss: 0.220699, acc.: 66.41%] [G loss: 0.450462]\n",
      "epoch:14 step:13392 [D loss: 0.196019, acc.: 68.75%] [G loss: 0.438987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13393 [D loss: 0.215522, acc.: 64.84%] [G loss: 0.418281]\n",
      "epoch:14 step:13394 [D loss: 0.205194, acc.: 68.75%] [G loss: 0.488100]\n",
      "epoch:14 step:13395 [D loss: 0.247815, acc.: 54.69%] [G loss: 0.461845]\n",
      "epoch:14 step:13396 [D loss: 0.247756, acc.: 50.00%] [G loss: 0.451346]\n",
      "epoch:14 step:13397 [D loss: 0.223697, acc.: 65.62%] [G loss: 0.469503]\n",
      "epoch:14 step:13398 [D loss: 0.205429, acc.: 73.44%] [G loss: 0.501670]\n",
      "epoch:14 step:13399 [D loss: 0.282731, acc.: 47.66%] [G loss: 0.416795]\n",
      "epoch:14 step:13400 [D loss: 0.213931, acc.: 64.06%] [G loss: 0.410042]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.745287\n",
      "FID: 10.020569\n",
      "0 = 11.898255222797372\n",
      "1 = 0.054864868271479315\n",
      "2 = 0.8903499841690063\n",
      "3 = 0.871399998664856\n",
      "4 = 0.9093000292778015\n",
      "5 = 0.9057270288467407\n",
      "6 = 0.871399998664856\n",
      "7 = 6.246098731952898\n",
      "8 = 0.06755596525224186\n",
      "9 = 0.7005000114440918\n",
      "10 = 0.6988000273704529\n",
      "11 = 0.7021999955177307\n",
      "12 = 0.7011840343475342\n",
      "13 = 0.6988000273704529\n",
      "14 = 7.745351791381836\n",
      "15 = 9.515116691589355\n",
      "16 = 0.11964843422174454\n",
      "17 = 7.74528694152832\n",
      "18 = 10.02056884765625\n",
      "epoch:14 step:13401 [D loss: 0.199093, acc.: 68.75%] [G loss: 0.450633]\n",
      "epoch:14 step:13402 [D loss: 0.241286, acc.: 59.38%] [G loss: 0.414288]\n",
      "epoch:14 step:13403 [D loss: 0.249795, acc.: 54.69%] [G loss: 0.387933]\n",
      "epoch:14 step:13404 [D loss: 0.209085, acc.: 70.31%] [G loss: 0.454929]\n",
      "epoch:14 step:13405 [D loss: 0.215924, acc.: 62.50%] [G loss: 0.439660]\n",
      "epoch:14 step:13406 [D loss: 0.211456, acc.: 64.06%] [G loss: 0.478846]\n",
      "epoch:14 step:13407 [D loss: 0.208144, acc.: 69.53%] [G loss: 0.445089]\n",
      "epoch:14 step:13408 [D loss: 0.227747, acc.: 64.06%] [G loss: 0.454994]\n",
      "epoch:14 step:13409 [D loss: 0.251355, acc.: 55.47%] [G loss: 0.457051]\n",
      "epoch:14 step:13410 [D loss: 0.215496, acc.: 64.06%] [G loss: 0.457179]\n",
      "epoch:14 step:13411 [D loss: 0.224989, acc.: 63.28%] [G loss: 0.439986]\n",
      "epoch:14 step:13412 [D loss: 0.245742, acc.: 53.12%] [G loss: 0.390481]\n",
      "epoch:14 step:13413 [D loss: 0.217608, acc.: 64.84%] [G loss: 0.419047]\n",
      "epoch:14 step:13414 [D loss: 0.196062, acc.: 71.88%] [G loss: 0.449612]\n",
      "epoch:14 step:13415 [D loss: 0.223790, acc.: 60.16%] [G loss: 0.420205]\n",
      "epoch:14 step:13416 [D loss: 0.209479, acc.: 67.97%] [G loss: 0.415758]\n",
      "epoch:14 step:13417 [D loss: 0.185137, acc.: 73.44%] [G loss: 0.523245]\n",
      "epoch:14 step:13418 [D loss: 0.210112, acc.: 70.31%] [G loss: 0.473017]\n",
      "epoch:14 step:13419 [D loss: 0.264708, acc.: 56.25%] [G loss: 0.452570]\n",
      "epoch:14 step:13420 [D loss: 0.227939, acc.: 60.16%] [G loss: 0.463528]\n",
      "epoch:14 step:13421 [D loss: 0.215298, acc.: 65.62%] [G loss: 0.491732]\n",
      "epoch:14 step:13422 [D loss: 0.211453, acc.: 65.62%] [G loss: 0.463819]\n",
      "epoch:14 step:13423 [D loss: 0.220271, acc.: 67.19%] [G loss: 0.460386]\n",
      "epoch:14 step:13424 [D loss: 0.229297, acc.: 63.28%] [G loss: 0.414573]\n",
      "epoch:14 step:13425 [D loss: 0.218170, acc.: 63.28%] [G loss: 0.409864]\n",
      "epoch:14 step:13426 [D loss: 0.238882, acc.: 60.94%] [G loss: 0.423928]\n",
      "epoch:14 step:13427 [D loss: 0.243771, acc.: 57.81%] [G loss: 0.448465]\n",
      "epoch:14 step:13428 [D loss: 0.208245, acc.: 66.41%] [G loss: 0.428081]\n",
      "epoch:14 step:13429 [D loss: 0.210084, acc.: 67.19%] [G loss: 0.443031]\n",
      "epoch:14 step:13430 [D loss: 0.171714, acc.: 75.00%] [G loss: 0.483446]\n",
      "epoch:14 step:13431 [D loss: 0.206786, acc.: 64.06%] [G loss: 0.521282]\n",
      "epoch:14 step:13432 [D loss: 0.163361, acc.: 77.34%] [G loss: 0.551730]\n",
      "epoch:14 step:13433 [D loss: 0.209685, acc.: 67.19%] [G loss: 0.482786]\n",
      "epoch:14 step:13434 [D loss: 0.291816, acc.: 47.66%] [G loss: 0.427388]\n",
      "epoch:14 step:13435 [D loss: 0.243551, acc.: 57.81%] [G loss: 0.430637]\n",
      "epoch:14 step:13436 [D loss: 0.212989, acc.: 64.84%] [G loss: 0.450792]\n",
      "epoch:14 step:13437 [D loss: 0.240469, acc.: 57.03%] [G loss: 0.466183]\n",
      "epoch:14 step:13438 [D loss: 0.213553, acc.: 67.97%] [G loss: 0.459988]\n",
      "epoch:14 step:13439 [D loss: 0.223787, acc.: 60.16%] [G loss: 0.450219]\n",
      "epoch:14 step:13440 [D loss: 0.240583, acc.: 57.81%] [G loss: 0.445823]\n",
      "epoch:14 step:13441 [D loss: 0.276330, acc.: 53.12%] [G loss: 0.434180]\n",
      "epoch:14 step:13442 [D loss: 0.205741, acc.: 68.75%] [G loss: 0.451112]\n",
      "epoch:14 step:13443 [D loss: 0.222487, acc.: 64.84%] [G loss: 0.445357]\n",
      "epoch:14 step:13444 [D loss: 0.221584, acc.: 63.28%] [G loss: 0.442614]\n",
      "epoch:14 step:13445 [D loss: 0.201579, acc.: 70.31%] [G loss: 0.445987]\n",
      "epoch:14 step:13446 [D loss: 0.207713, acc.: 68.75%] [G loss: 0.475093]\n",
      "epoch:14 step:13447 [D loss: 0.234967, acc.: 60.16%] [G loss: 0.441797]\n",
      "epoch:14 step:13448 [D loss: 0.214999, acc.: 65.62%] [G loss: 0.440422]\n",
      "epoch:14 step:13449 [D loss: 0.207310, acc.: 71.09%] [G loss: 0.455816]\n",
      "epoch:14 step:13450 [D loss: 0.201079, acc.: 69.53%] [G loss: 0.457676]\n",
      "epoch:14 step:13451 [D loss: 0.211894, acc.: 60.94%] [G loss: 0.473532]\n",
      "epoch:14 step:13452 [D loss: 0.211362, acc.: 69.53%] [G loss: 0.489582]\n",
      "epoch:14 step:13453 [D loss: 0.215839, acc.: 65.62%] [G loss: 0.501095]\n",
      "epoch:14 step:13454 [D loss: 0.221371, acc.: 69.53%] [G loss: 0.492243]\n",
      "epoch:14 step:13455 [D loss: 0.209783, acc.: 67.97%] [G loss: 0.479604]\n",
      "epoch:14 step:13456 [D loss: 0.214600, acc.: 71.09%] [G loss: 0.425519]\n",
      "epoch:14 step:13457 [D loss: 0.188603, acc.: 67.19%] [G loss: 0.472333]\n",
      "epoch:14 step:13458 [D loss: 0.204981, acc.: 67.97%] [G loss: 0.461899]\n",
      "epoch:14 step:13459 [D loss: 0.262911, acc.: 60.16%] [G loss: 0.447959]\n",
      "epoch:14 step:13460 [D loss: 0.253493, acc.: 54.69%] [G loss: 0.406021]\n",
      "epoch:14 step:13461 [D loss: 0.210858, acc.: 69.53%] [G loss: 0.451715]\n",
      "epoch:14 step:13462 [D loss: 0.240106, acc.: 61.72%] [G loss: 0.438542]\n",
      "epoch:14 step:13463 [D loss: 0.230790, acc.: 57.81%] [G loss: 0.450531]\n",
      "epoch:14 step:13464 [D loss: 0.199949, acc.: 65.62%] [G loss: 0.458582]\n",
      "epoch:14 step:13465 [D loss: 0.198875, acc.: 66.41%] [G loss: 0.546132]\n",
      "epoch:14 step:13466 [D loss: 0.265701, acc.: 58.59%] [G loss: 0.515430]\n",
      "epoch:14 step:13467 [D loss: 0.267467, acc.: 51.56%] [G loss: 0.408097]\n",
      "epoch:14 step:13468 [D loss: 0.215526, acc.: 64.06%] [G loss: 0.404615]\n",
      "epoch:14 step:13469 [D loss: 0.219311, acc.: 61.72%] [G loss: 0.408321]\n",
      "epoch:14 step:13470 [D loss: 0.215996, acc.: 66.41%] [G loss: 0.475811]\n",
      "epoch:14 step:13471 [D loss: 0.203893, acc.: 65.62%] [G loss: 0.480329]\n",
      "epoch:14 step:13472 [D loss: 0.201272, acc.: 69.53%] [G loss: 0.488483]\n",
      "epoch:14 step:13473 [D loss: 0.202890, acc.: 68.75%] [G loss: 0.525338]\n",
      "epoch:14 step:13474 [D loss: 0.226959, acc.: 64.84%] [G loss: 0.411688]\n",
      "epoch:14 step:13475 [D loss: 0.222541, acc.: 63.28%] [G loss: 0.442440]\n",
      "epoch:14 step:13476 [D loss: 0.202300, acc.: 69.53%] [G loss: 0.451106]\n",
      "epoch:14 step:13477 [D loss: 0.212115, acc.: 64.06%] [G loss: 0.478302]\n",
      "epoch:14 step:13478 [D loss: 0.199580, acc.: 66.41%] [G loss: 0.464206]\n",
      "epoch:14 step:13479 [D loss: 0.217514, acc.: 66.41%] [G loss: 0.490700]\n",
      "epoch:14 step:13480 [D loss: 0.223331, acc.: 64.84%] [G loss: 0.441736]\n",
      "epoch:14 step:13481 [D loss: 0.233704, acc.: 62.50%] [G loss: 0.467285]\n",
      "epoch:14 step:13482 [D loss: 0.205056, acc.: 71.09%] [G loss: 0.465600]\n",
      "epoch:14 step:13483 [D loss: 0.246615, acc.: 56.25%] [G loss: 0.446850]\n",
      "epoch:14 step:13484 [D loss: 0.205082, acc.: 71.88%] [G loss: 0.438769]\n",
      "epoch:14 step:13485 [D loss: 0.219497, acc.: 64.06%] [G loss: 0.479640]\n",
      "epoch:14 step:13486 [D loss: 0.243116, acc.: 56.25%] [G loss: 0.436599]\n",
      "epoch:14 step:13487 [D loss: 0.237660, acc.: 57.03%] [G loss: 0.438254]\n",
      "epoch:14 step:13488 [D loss: 0.204718, acc.: 67.19%] [G loss: 0.434659]\n",
      "epoch:14 step:13489 [D loss: 0.200103, acc.: 69.53%] [G loss: 0.454263]\n",
      "epoch:14 step:13490 [D loss: 0.198423, acc.: 67.97%] [G loss: 0.483097]\n",
      "epoch:14 step:13491 [D loss: 0.246881, acc.: 58.59%] [G loss: 0.411313]\n",
      "epoch:14 step:13492 [D loss: 0.181671, acc.: 75.00%] [G loss: 0.443235]\n",
      "epoch:14 step:13493 [D loss: 0.239929, acc.: 60.94%] [G loss: 0.445289]\n",
      "epoch:14 step:13494 [D loss: 0.253413, acc.: 53.12%] [G loss: 0.433577]\n",
      "epoch:14 step:13495 [D loss: 0.232085, acc.: 59.38%] [G loss: 0.382191]\n",
      "epoch:14 step:13496 [D loss: 0.227649, acc.: 63.28%] [G loss: 0.402142]\n",
      "epoch:14 step:13497 [D loss: 0.242379, acc.: 59.38%] [G loss: 0.436028]\n",
      "epoch:14 step:13498 [D loss: 0.213445, acc.: 65.62%] [G loss: 0.453828]\n",
      "epoch:14 step:13499 [D loss: 0.217570, acc.: 63.28%] [G loss: 0.457941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13500 [D loss: 0.229160, acc.: 60.16%] [G loss: 0.481003]\n",
      "epoch:14 step:13501 [D loss: 0.233621, acc.: 60.16%] [G loss: 0.427507]\n",
      "epoch:14 step:13502 [D loss: 0.221130, acc.: 63.28%] [G loss: 0.435458]\n",
      "epoch:14 step:13503 [D loss: 0.205484, acc.: 67.97%] [G loss: 0.470907]\n",
      "epoch:14 step:13504 [D loss: 0.256650, acc.: 57.81%] [G loss: 0.432854]\n",
      "epoch:14 step:13505 [D loss: 0.234421, acc.: 57.03%] [G loss: 0.440846]\n",
      "epoch:14 step:13506 [D loss: 0.229218, acc.: 61.72%] [G loss: 0.406905]\n",
      "epoch:14 step:13507 [D loss: 0.228645, acc.: 60.94%] [G loss: 0.454831]\n",
      "epoch:14 step:13508 [D loss: 0.254706, acc.: 56.25%] [G loss: 0.422445]\n",
      "epoch:14 step:13509 [D loss: 0.222287, acc.: 61.72%] [G loss: 0.439288]\n",
      "epoch:14 step:13510 [D loss: 0.216274, acc.: 64.84%] [G loss: 0.423402]\n",
      "epoch:14 step:13511 [D loss: 0.240117, acc.: 55.47%] [G loss: 0.435301]\n",
      "epoch:14 step:13512 [D loss: 0.241791, acc.: 64.06%] [G loss: 0.401538]\n",
      "epoch:14 step:13513 [D loss: 0.192016, acc.: 70.31%] [G loss: 0.476104]\n",
      "epoch:14 step:13514 [D loss: 0.268058, acc.: 50.78%] [G loss: 0.446644]\n",
      "epoch:14 step:13515 [D loss: 0.231640, acc.: 60.94%] [G loss: 0.470178]\n",
      "epoch:14 step:13516 [D loss: 0.175468, acc.: 77.34%] [G loss: 0.493608]\n",
      "epoch:14 step:13517 [D loss: 0.207170, acc.: 70.31%] [G loss: 0.508262]\n",
      "epoch:14 step:13518 [D loss: 0.270671, acc.: 52.34%] [G loss: 0.412311]\n",
      "epoch:14 step:13519 [D loss: 0.223708, acc.: 64.06%] [G loss: 0.446082]\n",
      "epoch:14 step:13520 [D loss: 0.212489, acc.: 68.75%] [G loss: 0.437539]\n",
      "epoch:14 step:13521 [D loss: 0.234235, acc.: 60.94%] [G loss: 0.410746]\n",
      "epoch:14 step:13522 [D loss: 0.242041, acc.: 60.16%] [G loss: 0.463920]\n",
      "epoch:14 step:13523 [D loss: 0.201149, acc.: 69.53%] [G loss: 0.508748]\n",
      "epoch:14 step:13524 [D loss: 0.198154, acc.: 70.31%] [G loss: 0.522384]\n",
      "epoch:14 step:13525 [D loss: 0.262100, acc.: 56.25%] [G loss: 0.484860]\n",
      "epoch:14 step:13526 [D loss: 0.226643, acc.: 66.41%] [G loss: 0.431164]\n",
      "epoch:14 step:13527 [D loss: 0.241563, acc.: 62.50%] [G loss: 0.419927]\n",
      "epoch:14 step:13528 [D loss: 0.249626, acc.: 58.59%] [G loss: 0.411230]\n",
      "epoch:14 step:13529 [D loss: 0.245738, acc.: 54.69%] [G loss: 0.388593]\n",
      "epoch:14 step:13530 [D loss: 0.234542, acc.: 60.16%] [G loss: 0.391017]\n",
      "epoch:14 step:13531 [D loss: 0.225973, acc.: 60.16%] [G loss: 0.424253]\n",
      "epoch:14 step:13532 [D loss: 0.225530, acc.: 61.72%] [G loss: 0.451230]\n",
      "epoch:14 step:13533 [D loss: 0.215183, acc.: 64.84%] [G loss: 0.420539]\n",
      "epoch:14 step:13534 [D loss: 0.216666, acc.: 63.28%] [G loss: 0.466065]\n",
      "epoch:14 step:13535 [D loss: 0.237442, acc.: 56.25%] [G loss: 0.462742]\n",
      "epoch:14 step:13536 [D loss: 0.259119, acc.: 57.03%] [G loss: 0.423927]\n",
      "epoch:14 step:13537 [D loss: 0.221593, acc.: 60.94%] [G loss: 0.416526]\n",
      "epoch:14 step:13538 [D loss: 0.243977, acc.: 57.03%] [G loss: 0.420520]\n",
      "epoch:14 step:13539 [D loss: 0.239422, acc.: 60.94%] [G loss: 0.435752]\n",
      "epoch:14 step:13540 [D loss: 0.243817, acc.: 54.69%] [G loss: 0.429436]\n",
      "epoch:14 step:13541 [D loss: 0.214586, acc.: 64.84%] [G loss: 0.447000]\n",
      "epoch:14 step:13542 [D loss: 0.227601, acc.: 57.03%] [G loss: 0.402448]\n",
      "epoch:14 step:13543 [D loss: 0.223341, acc.: 61.72%] [G loss: 0.455301]\n",
      "epoch:14 step:13544 [D loss: 0.209641, acc.: 64.84%] [G loss: 0.457493]\n",
      "epoch:14 step:13545 [D loss: 0.205932, acc.: 68.75%] [G loss: 0.402918]\n",
      "epoch:14 step:13546 [D loss: 0.182852, acc.: 74.22%] [G loss: 0.465955]\n",
      "epoch:14 step:13547 [D loss: 0.188285, acc.: 71.09%] [G loss: 0.510254]\n",
      "epoch:14 step:13548 [D loss: 0.219328, acc.: 64.06%] [G loss: 0.421816]\n",
      "epoch:14 step:13549 [D loss: 0.233751, acc.: 61.72%] [G loss: 0.471385]\n",
      "epoch:14 step:13550 [D loss: 0.229801, acc.: 57.81%] [G loss: 0.435940]\n",
      "epoch:14 step:13551 [D loss: 0.231170, acc.: 60.16%] [G loss: 0.439848]\n",
      "epoch:14 step:13552 [D loss: 0.202389, acc.: 68.75%] [G loss: 0.420789]\n",
      "epoch:14 step:13553 [D loss: 0.209450, acc.: 68.75%] [G loss: 0.462137]\n",
      "epoch:14 step:13554 [D loss: 0.180864, acc.: 72.66%] [G loss: 0.525538]\n",
      "epoch:14 step:13555 [D loss: 0.278681, acc.: 46.88%] [G loss: 0.449718]\n",
      "epoch:14 step:13556 [D loss: 0.273126, acc.: 53.91%] [G loss: 0.456925]\n",
      "epoch:14 step:13557 [D loss: 0.218093, acc.: 67.19%] [G loss: 0.474573]\n",
      "epoch:14 step:13558 [D loss: 0.247325, acc.: 56.25%] [G loss: 0.457463]\n",
      "epoch:14 step:13559 [D loss: 0.217446, acc.: 64.06%] [G loss: 0.476201]\n",
      "epoch:14 step:13560 [D loss: 0.224723, acc.: 62.50%] [G loss: 0.451379]\n",
      "epoch:14 step:13561 [D loss: 0.246279, acc.: 57.03%] [G loss: 0.466644]\n",
      "epoch:14 step:13562 [D loss: 0.231803, acc.: 63.28%] [G loss: 0.433550]\n",
      "epoch:14 step:13563 [D loss: 0.210320, acc.: 61.72%] [G loss: 0.530981]\n",
      "epoch:14 step:13564 [D loss: 0.239999, acc.: 59.38%] [G loss: 0.467144]\n",
      "epoch:14 step:13565 [D loss: 0.193443, acc.: 73.44%] [G loss: 0.475707]\n",
      "epoch:14 step:13566 [D loss: 0.254010, acc.: 56.25%] [G loss: 0.417975]\n",
      "epoch:14 step:13567 [D loss: 0.212332, acc.: 67.97%] [G loss: 0.443370]\n",
      "epoch:14 step:13568 [D loss: 0.207609, acc.: 67.97%] [G loss: 0.431891]\n",
      "epoch:14 step:13569 [D loss: 0.225386, acc.: 64.06%] [G loss: 0.454490]\n",
      "epoch:14 step:13570 [D loss: 0.220928, acc.: 67.97%] [G loss: 0.467855]\n",
      "epoch:14 step:13571 [D loss: 0.229633, acc.: 65.62%] [G loss: 0.425646]\n",
      "epoch:14 step:13572 [D loss: 0.230072, acc.: 66.41%] [G loss: 0.449888]\n",
      "epoch:14 step:13573 [D loss: 0.240821, acc.: 60.16%] [G loss: 0.449308]\n",
      "epoch:14 step:13574 [D loss: 0.251322, acc.: 59.38%] [G loss: 0.415574]\n",
      "epoch:14 step:13575 [D loss: 0.183031, acc.: 72.66%] [G loss: 0.493685]\n",
      "epoch:14 step:13576 [D loss: 0.274823, acc.: 50.00%] [G loss: 0.431979]\n",
      "epoch:14 step:13577 [D loss: 0.208015, acc.: 68.75%] [G loss: 0.466762]\n",
      "epoch:14 step:13578 [D loss: 0.251893, acc.: 57.03%] [G loss: 0.412229]\n",
      "epoch:14 step:13579 [D loss: 0.215147, acc.: 63.28%] [G loss: 0.431110]\n",
      "epoch:14 step:13580 [D loss: 0.254932, acc.: 56.25%] [G loss: 0.418033]\n",
      "epoch:14 step:13581 [D loss: 0.238763, acc.: 60.16%] [G loss: 0.426354]\n",
      "epoch:14 step:13582 [D loss: 0.241330, acc.: 61.72%] [G loss: 0.422342]\n",
      "epoch:14 step:13583 [D loss: 0.213453, acc.: 68.75%] [G loss: 0.430253]\n",
      "epoch:14 step:13584 [D loss: 0.223143, acc.: 60.16%] [G loss: 0.412860]\n",
      "epoch:14 step:13585 [D loss: 0.226937, acc.: 62.50%] [G loss: 0.414376]\n",
      "epoch:14 step:13586 [D loss: 0.220266, acc.: 65.62%] [G loss: 0.456089]\n",
      "epoch:14 step:13587 [D loss: 0.206138, acc.: 64.84%] [G loss: 0.495532]\n",
      "epoch:14 step:13588 [D loss: 0.197879, acc.: 71.88%] [G loss: 0.461273]\n",
      "epoch:14 step:13589 [D loss: 0.169490, acc.: 77.34%] [G loss: 0.526671]\n",
      "epoch:14 step:13590 [D loss: 0.207523, acc.: 67.97%] [G loss: 0.519401]\n",
      "epoch:14 step:13591 [D loss: 0.261148, acc.: 60.94%] [G loss: 0.445538]\n",
      "epoch:14 step:13592 [D loss: 0.204615, acc.: 66.41%] [G loss: 0.503384]\n",
      "epoch:14 step:13593 [D loss: 0.207170, acc.: 65.62%] [G loss: 0.507301]\n",
      "epoch:14 step:13594 [D loss: 0.238704, acc.: 63.28%] [G loss: 0.476166]\n",
      "epoch:14 step:13595 [D loss: 0.237986, acc.: 57.03%] [G loss: 0.420224]\n",
      "epoch:14 step:13596 [D loss: 0.227343, acc.: 62.50%] [G loss: 0.451232]\n",
      "epoch:14 step:13597 [D loss: 0.225030, acc.: 62.50%] [G loss: 0.375122]\n",
      "epoch:14 step:13598 [D loss: 0.219824, acc.: 62.50%] [G loss: 0.430765]\n",
      "epoch:14 step:13599 [D loss: 0.194537, acc.: 71.88%] [G loss: 0.494920]\n",
      "epoch:14 step:13600 [D loss: 0.293682, acc.: 47.66%] [G loss: 0.411890]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.755059\n",
      "FID: 9.803893\n",
      "0 = 11.815321409583115\n",
      "1 = 0.05278860059273273\n",
      "2 = 0.8847000002861023\n",
      "3 = 0.8665000200271606\n",
      "4 = 0.902899980545044\n",
      "5 = 0.899232029914856\n",
      "6 = 0.8665000200271606\n",
      "7 = 6.208867412561146\n",
      "8 = 0.07047338094106631\n",
      "9 = 0.6965000033378601\n",
      "10 = 0.6937000155448914\n",
      "11 = 0.6992999911308289\n",
      "12 = 0.69760662317276\n",
      "13 = 0.6937000155448914\n",
      "14 = 7.755126953125\n",
      "15 = 9.554054260253906\n",
      "16 = 0.108157679438591\n",
      "17 = 7.755058765411377\n",
      "18 = 9.803893089294434\n",
      "epoch:14 step:13601 [D loss: 0.228904, acc.: 58.59%] [G loss: 0.429127]\n",
      "epoch:14 step:13602 [D loss: 0.202282, acc.: 69.53%] [G loss: 0.483501]\n",
      "epoch:14 step:13603 [D loss: 0.217030, acc.: 65.62%] [G loss: 0.438752]\n",
      "epoch:14 step:13604 [D loss: 0.243198, acc.: 61.72%] [G loss: 0.417589]\n",
      "epoch:14 step:13605 [D loss: 0.214752, acc.: 64.06%] [G loss: 0.411879]\n",
      "epoch:14 step:13606 [D loss: 0.194159, acc.: 71.88%] [G loss: 0.410769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13607 [D loss: 0.224718, acc.: 66.41%] [G loss: 0.412028]\n",
      "epoch:14 step:13608 [D loss: 0.228906, acc.: 60.94%] [G loss: 0.421553]\n",
      "epoch:14 step:13609 [D loss: 0.219089, acc.: 63.28%] [G loss: 0.438983]\n",
      "epoch:14 step:13610 [D loss: 0.232770, acc.: 60.16%] [G loss: 0.454823]\n",
      "epoch:14 step:13611 [D loss: 0.224478, acc.: 64.06%] [G loss: 0.459931]\n",
      "epoch:14 step:13612 [D loss: 0.235972, acc.: 59.38%] [G loss: 0.410348]\n",
      "epoch:14 step:13613 [D loss: 0.199521, acc.: 69.53%] [G loss: 0.454729]\n",
      "epoch:14 step:13614 [D loss: 0.204968, acc.: 70.31%] [G loss: 0.448977]\n",
      "epoch:14 step:13615 [D loss: 0.204410, acc.: 66.41%] [G loss: 0.491834]\n",
      "epoch:14 step:13616 [D loss: 0.215512, acc.: 68.75%] [G loss: 0.516025]\n",
      "epoch:14 step:13617 [D loss: 0.188525, acc.: 76.56%] [G loss: 0.500123]\n",
      "epoch:14 step:13618 [D loss: 0.232341, acc.: 58.59%] [G loss: 0.440475]\n",
      "epoch:14 step:13619 [D loss: 0.294581, acc.: 50.00%] [G loss: 0.394828]\n",
      "epoch:14 step:13620 [D loss: 0.225088, acc.: 61.72%] [G loss: 0.433914]\n",
      "epoch:14 step:13621 [D loss: 0.246463, acc.: 54.69%] [G loss: 0.394270]\n",
      "epoch:14 step:13622 [D loss: 0.201667, acc.: 72.66%] [G loss: 0.452037]\n",
      "epoch:14 step:13623 [D loss: 0.190844, acc.: 73.44%] [G loss: 0.470138]\n",
      "epoch:14 step:13624 [D loss: 0.224018, acc.: 61.72%] [G loss: 0.449112]\n",
      "epoch:14 step:13625 [D loss: 0.174953, acc.: 71.09%] [G loss: 0.535775]\n",
      "epoch:14 step:13626 [D loss: 0.185962, acc.: 75.00%] [G loss: 0.490180]\n",
      "epoch:14 step:13627 [D loss: 0.244457, acc.: 58.59%] [G loss: 0.436769]\n",
      "epoch:14 step:13628 [D loss: 0.224281, acc.: 65.62%] [G loss: 0.430222]\n",
      "epoch:14 step:13629 [D loss: 0.245646, acc.: 58.59%] [G loss: 0.424445]\n",
      "epoch:14 step:13630 [D loss: 0.252984, acc.: 50.00%] [G loss: 0.399285]\n",
      "epoch:14 step:13631 [D loss: 0.206453, acc.: 67.19%] [G loss: 0.426825]\n",
      "epoch:14 step:13632 [D loss: 0.213943, acc.: 67.19%] [G loss: 0.429689]\n",
      "epoch:14 step:13633 [D loss: 0.206515, acc.: 67.19%] [G loss: 0.470298]\n",
      "epoch:14 step:13634 [D loss: 0.213328, acc.: 64.06%] [G loss: 0.448492]\n",
      "epoch:14 step:13635 [D loss: 0.239973, acc.: 64.84%] [G loss: 0.438039]\n",
      "epoch:14 step:13636 [D loss: 0.240330, acc.: 57.81%] [G loss: 0.420258]\n",
      "epoch:14 step:13637 [D loss: 0.192011, acc.: 71.88%] [G loss: 0.466329]\n",
      "epoch:14 step:13638 [D loss: 0.223284, acc.: 62.50%] [G loss: 0.473135]\n",
      "epoch:14 step:13639 [D loss: 0.207849, acc.: 64.84%] [G loss: 0.453474]\n",
      "epoch:14 step:13640 [D loss: 0.199297, acc.: 67.19%] [G loss: 0.470715]\n",
      "epoch:14 step:13641 [D loss: 0.200938, acc.: 63.28%] [G loss: 0.460566]\n",
      "epoch:14 step:13642 [D loss: 0.230659, acc.: 64.84%] [G loss: 0.412836]\n",
      "epoch:14 step:13643 [D loss: 0.235857, acc.: 60.94%] [G loss: 0.382852]\n",
      "epoch:14 step:13644 [D loss: 0.244217, acc.: 59.38%] [G loss: 0.418700]\n",
      "epoch:14 step:13645 [D loss: 0.247873, acc.: 57.81%] [G loss: 0.457447]\n",
      "epoch:14 step:13646 [D loss: 0.274698, acc.: 52.34%] [G loss: 0.424305]\n",
      "epoch:14 step:13647 [D loss: 0.229390, acc.: 62.50%] [G loss: 0.442411]\n",
      "epoch:14 step:13648 [D loss: 0.239503, acc.: 60.94%] [G loss: 0.416210]\n",
      "epoch:14 step:13649 [D loss: 0.232975, acc.: 59.38%] [G loss: 0.434997]\n",
      "epoch:14 step:13650 [D loss: 0.231193, acc.: 54.69%] [G loss: 0.433640]\n",
      "epoch:14 step:13651 [D loss: 0.216304, acc.: 62.50%] [G loss: 0.464969]\n",
      "epoch:14 step:13652 [D loss: 0.200399, acc.: 73.44%] [G loss: 0.490150]\n",
      "epoch:14 step:13653 [D loss: 0.264601, acc.: 53.91%] [G loss: 0.413189]\n",
      "epoch:14 step:13654 [D loss: 0.207917, acc.: 71.09%] [G loss: 0.458604]\n",
      "epoch:14 step:13655 [D loss: 0.229334, acc.: 64.06%] [G loss: 0.414207]\n",
      "epoch:14 step:13656 [D loss: 0.247681, acc.: 60.16%] [G loss: 0.433518]\n",
      "epoch:14 step:13657 [D loss: 0.218589, acc.: 60.94%] [G loss: 0.461729]\n",
      "epoch:14 step:13658 [D loss: 0.237002, acc.: 62.50%] [G loss: 0.452018]\n",
      "epoch:14 step:13659 [D loss: 0.239494, acc.: 62.50%] [G loss: 0.420996]\n",
      "epoch:14 step:13660 [D loss: 0.271028, acc.: 55.47%] [G loss: 0.391683]\n",
      "epoch:14 step:13661 [D loss: 0.220592, acc.: 64.06%] [G loss: 0.450087]\n",
      "epoch:14 step:13662 [D loss: 0.232236, acc.: 61.72%] [G loss: 0.464470]\n",
      "epoch:14 step:13663 [D loss: 0.211189, acc.: 67.19%] [G loss: 0.455542]\n",
      "epoch:14 step:13664 [D loss: 0.209791, acc.: 70.31%] [G loss: 0.450019]\n",
      "epoch:14 step:13665 [D loss: 0.220864, acc.: 66.41%] [G loss: 0.446480]\n",
      "epoch:14 step:13666 [D loss: 0.243124, acc.: 57.03%] [G loss: 0.433443]\n",
      "epoch:14 step:13667 [D loss: 0.197098, acc.: 71.88%] [G loss: 0.477650]\n",
      "epoch:14 step:13668 [D loss: 0.199455, acc.: 71.88%] [G loss: 0.468196]\n",
      "epoch:14 step:13669 [D loss: 0.202190, acc.: 68.75%] [G loss: 0.475792]\n",
      "epoch:14 step:13670 [D loss: 0.188183, acc.: 73.44%] [G loss: 0.471596]\n",
      "epoch:14 step:13671 [D loss: 0.246949, acc.: 56.25%] [G loss: 0.429719]\n",
      "epoch:14 step:13672 [D loss: 0.197279, acc.: 67.97%] [G loss: 0.458025]\n",
      "epoch:14 step:13673 [D loss: 0.195965, acc.: 71.88%] [G loss: 0.483520]\n",
      "epoch:14 step:13674 [D loss: 0.223780, acc.: 65.62%] [G loss: 0.476092]\n",
      "epoch:14 step:13675 [D loss: 0.213680, acc.: 67.97%] [G loss: 0.459691]\n",
      "epoch:14 step:13676 [D loss: 0.201740, acc.: 67.97%] [G loss: 0.501569]\n",
      "epoch:14 step:13677 [D loss: 0.245114, acc.: 60.16%] [G loss: 0.443373]\n",
      "epoch:14 step:13678 [D loss: 0.250304, acc.: 53.12%] [G loss: 0.426456]\n",
      "epoch:14 step:13679 [D loss: 0.218376, acc.: 64.06%] [G loss: 0.434194]\n",
      "epoch:14 step:13680 [D loss: 0.204728, acc.: 62.50%] [G loss: 0.449901]\n",
      "epoch:14 step:13681 [D loss: 0.227603, acc.: 66.41%] [G loss: 0.425650]\n",
      "epoch:14 step:13682 [D loss: 0.209973, acc.: 70.31%] [G loss: 0.497357]\n",
      "epoch:14 step:13683 [D loss: 0.233356, acc.: 61.72%] [G loss: 0.479349]\n",
      "epoch:14 step:13684 [D loss: 0.276306, acc.: 52.34%] [G loss: 0.419263]\n",
      "epoch:14 step:13685 [D loss: 0.210862, acc.: 67.97%] [G loss: 0.454290]\n",
      "epoch:14 step:13686 [D loss: 0.208651, acc.: 69.53%] [G loss: 0.489432]\n",
      "epoch:14 step:13687 [D loss: 0.258293, acc.: 52.34%] [G loss: 0.429183]\n",
      "epoch:14 step:13688 [D loss: 0.231587, acc.: 60.94%] [G loss: 0.372624]\n",
      "epoch:14 step:13689 [D loss: 0.224402, acc.: 64.06%] [G loss: 0.411425]\n",
      "epoch:14 step:13690 [D loss: 0.214976, acc.: 68.75%] [G loss: 0.435167]\n",
      "epoch:14 step:13691 [D loss: 0.195680, acc.: 67.19%] [G loss: 0.480059]\n",
      "epoch:14 step:13692 [D loss: 0.159536, acc.: 75.78%] [G loss: 0.544135]\n",
      "epoch:14 step:13693 [D loss: 0.173189, acc.: 76.56%] [G loss: 0.536651]\n",
      "epoch:14 step:13694 [D loss: 0.260419, acc.: 54.69%] [G loss: 0.448997]\n",
      "epoch:14 step:13695 [D loss: 0.235560, acc.: 56.25%] [G loss: 0.432686]\n",
      "epoch:14 step:13696 [D loss: 0.231362, acc.: 61.72%] [G loss: 0.438380]\n",
      "epoch:14 step:13697 [D loss: 0.252431, acc.: 54.69%] [G loss: 0.447197]\n",
      "epoch:14 step:13698 [D loss: 0.197810, acc.: 70.31%] [G loss: 0.462692]\n",
      "epoch:14 step:13699 [D loss: 0.201768, acc.: 64.84%] [G loss: 0.466370]\n",
      "epoch:14 step:13700 [D loss: 0.198691, acc.: 71.88%] [G loss: 0.469407]\n",
      "epoch:14 step:13701 [D loss: 0.221050, acc.: 66.41%] [G loss: 0.487758]\n",
      "epoch:14 step:13702 [D loss: 0.240921, acc.: 57.03%] [G loss: 0.445535]\n",
      "epoch:14 step:13703 [D loss: 0.222392, acc.: 60.16%] [G loss: 0.431778]\n",
      "epoch:14 step:13704 [D loss: 0.209443, acc.: 67.97%] [G loss: 0.426775]\n",
      "epoch:14 step:13705 [D loss: 0.254124, acc.: 55.47%] [G loss: 0.389224]\n",
      "epoch:14 step:13706 [D loss: 0.236213, acc.: 58.59%] [G loss: 0.437972]\n",
      "epoch:14 step:13707 [D loss: 0.221206, acc.: 63.28%] [G loss: 0.447123]\n",
      "epoch:14 step:13708 [D loss: 0.241900, acc.: 61.72%] [G loss: 0.433043]\n",
      "epoch:14 step:13709 [D loss: 0.232534, acc.: 62.50%] [G loss: 0.434782]\n",
      "epoch:14 step:13710 [D loss: 0.187891, acc.: 73.44%] [G loss: 0.449221]\n",
      "epoch:14 step:13711 [D loss: 0.230521, acc.: 64.84%] [G loss: 0.458900]\n",
      "epoch:14 step:13712 [D loss: 0.254244, acc.: 54.69%] [G loss: 0.405721]\n",
      "epoch:14 step:13713 [D loss: 0.209536, acc.: 71.09%] [G loss: 0.443017]\n",
      "epoch:14 step:13714 [D loss: 0.248377, acc.: 55.47%] [G loss: 0.439354]\n",
      "epoch:14 step:13715 [D loss: 0.228421, acc.: 66.41%] [G loss: 0.461022]\n",
      "epoch:14 step:13716 [D loss: 0.209449, acc.: 67.19%] [G loss: 0.447707]\n",
      "epoch:14 step:13717 [D loss: 0.208388, acc.: 67.97%] [G loss: 0.462235]\n",
      "epoch:14 step:13718 [D loss: 0.229522, acc.: 62.50%] [G loss: 0.433634]\n",
      "epoch:14 step:13719 [D loss: 0.242025, acc.: 57.81%] [G loss: 0.420419]\n",
      "epoch:14 step:13720 [D loss: 0.219217, acc.: 64.06%] [G loss: 0.417729]\n",
      "epoch:14 step:13721 [D loss: 0.214640, acc.: 64.06%] [G loss: 0.451823]\n",
      "epoch:14 step:13722 [D loss: 0.234223, acc.: 59.38%] [G loss: 0.421777]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13723 [D loss: 0.213315, acc.: 67.19%] [G loss: 0.447241]\n",
      "epoch:14 step:13724 [D loss: 0.230283, acc.: 62.50%] [G loss: 0.479536]\n",
      "epoch:14 step:13725 [D loss: 0.214453, acc.: 64.06%] [G loss: 0.452061]\n",
      "epoch:14 step:13726 [D loss: 0.233338, acc.: 59.38%] [G loss: 0.391662]\n",
      "epoch:14 step:13727 [D loss: 0.212712, acc.: 70.31%] [G loss: 0.448236]\n",
      "epoch:14 step:13728 [D loss: 0.233712, acc.: 56.25%] [G loss: 0.397226]\n",
      "epoch:14 step:13729 [D loss: 0.223971, acc.: 60.94%] [G loss: 0.416237]\n",
      "epoch:14 step:13730 [D loss: 0.208728, acc.: 65.62%] [G loss: 0.496828]\n",
      "epoch:14 step:13731 [D loss: 0.210502, acc.: 63.28%] [G loss: 0.424236]\n",
      "epoch:14 step:13732 [D loss: 0.240058, acc.: 56.25%] [G loss: 0.427634]\n",
      "epoch:14 step:13733 [D loss: 0.238386, acc.: 63.28%] [G loss: 0.429561]\n",
      "epoch:14 step:13734 [D loss: 0.219891, acc.: 66.41%] [G loss: 0.397016]\n",
      "epoch:14 step:13735 [D loss: 0.222663, acc.: 67.97%] [G loss: 0.410748]\n",
      "epoch:14 step:13736 [D loss: 0.233704, acc.: 65.62%] [G loss: 0.479117]\n",
      "epoch:14 step:13737 [D loss: 0.238078, acc.: 58.59%] [G loss: 0.425683]\n",
      "epoch:14 step:13738 [D loss: 0.216456, acc.: 65.62%] [G loss: 0.476760]\n",
      "epoch:14 step:13739 [D loss: 0.238390, acc.: 60.94%] [G loss: 0.461183]\n",
      "epoch:14 step:13740 [D loss: 0.206514, acc.: 64.84%] [G loss: 0.469624]\n",
      "epoch:14 step:13741 [D loss: 0.221270, acc.: 64.84%] [G loss: 0.450267]\n",
      "epoch:14 step:13742 [D loss: 0.198213, acc.: 67.19%] [G loss: 0.478804]\n",
      "epoch:14 step:13743 [D loss: 0.237578, acc.: 58.59%] [G loss: 0.453817]\n",
      "epoch:14 step:13744 [D loss: 0.268106, acc.: 58.59%] [G loss: 0.414854]\n",
      "epoch:14 step:13745 [D loss: 0.222259, acc.: 65.62%] [G loss: 0.414237]\n",
      "epoch:14 step:13746 [D loss: 0.266722, acc.: 50.00%] [G loss: 0.401987]\n",
      "epoch:14 step:13747 [D loss: 0.206363, acc.: 71.09%] [G loss: 0.467170]\n",
      "epoch:14 step:13748 [D loss: 0.229289, acc.: 62.50%] [G loss: 0.442497]\n",
      "epoch:14 step:13749 [D loss: 0.209286, acc.: 65.62%] [G loss: 0.437923]\n",
      "epoch:14 step:13750 [D loss: 0.243435, acc.: 57.03%] [G loss: 0.407574]\n",
      "epoch:14 step:13751 [D loss: 0.222353, acc.: 61.72%] [G loss: 0.448552]\n",
      "epoch:14 step:13752 [D loss: 0.187654, acc.: 75.78%] [G loss: 0.496762]\n",
      "epoch:14 step:13753 [D loss: 0.195348, acc.: 70.31%] [G loss: 0.500601]\n",
      "epoch:14 step:13754 [D loss: 0.232492, acc.: 60.94%] [G loss: 0.462789]\n",
      "epoch:14 step:13755 [D loss: 0.212319, acc.: 67.19%] [G loss: 0.445962]\n",
      "epoch:14 step:13756 [D loss: 0.216480, acc.: 66.41%] [G loss: 0.456376]\n",
      "epoch:14 step:13757 [D loss: 0.240779, acc.: 58.59%] [G loss: 0.446303]\n",
      "epoch:14 step:13758 [D loss: 0.202650, acc.: 64.84%] [G loss: 0.457622]\n",
      "epoch:14 step:13759 [D loss: 0.228547, acc.: 64.06%] [G loss: 0.445147]\n",
      "epoch:14 step:13760 [D loss: 0.195297, acc.: 67.19%] [G loss: 0.531273]\n",
      "epoch:14 step:13761 [D loss: 0.224815, acc.: 60.94%] [G loss: 0.542140]\n",
      "epoch:14 step:13762 [D loss: 0.258665, acc.: 53.91%] [G loss: 0.434875]\n",
      "epoch:14 step:13763 [D loss: 0.207096, acc.: 66.41%] [G loss: 0.434031]\n",
      "epoch:14 step:13764 [D loss: 0.225172, acc.: 64.06%] [G loss: 0.448672]\n",
      "epoch:14 step:13765 [D loss: 0.214008, acc.: 66.41%] [G loss: 0.462153]\n",
      "epoch:14 step:13766 [D loss: 0.164962, acc.: 80.47%] [G loss: 0.526898]\n",
      "epoch:14 step:13767 [D loss: 0.226763, acc.: 62.50%] [G loss: 0.501554]\n",
      "epoch:14 step:13768 [D loss: 0.200101, acc.: 73.44%] [G loss: 0.485094]\n",
      "epoch:14 step:13769 [D loss: 0.221709, acc.: 61.72%] [G loss: 0.464827]\n",
      "epoch:14 step:13770 [D loss: 0.237918, acc.: 59.38%] [G loss: 0.436784]\n",
      "epoch:14 step:13771 [D loss: 0.230660, acc.: 64.84%] [G loss: 0.457193]\n",
      "epoch:14 step:13772 [D loss: 0.176101, acc.: 75.78%] [G loss: 0.472038]\n",
      "epoch:14 step:13773 [D loss: 0.277833, acc.: 52.34%] [G loss: 0.427003]\n",
      "epoch:14 step:13774 [D loss: 0.223128, acc.: 69.53%] [G loss: 0.411315]\n",
      "epoch:14 step:13775 [D loss: 0.231379, acc.: 61.72%] [G loss: 0.484450]\n",
      "epoch:14 step:13776 [D loss: 0.231168, acc.: 60.16%] [G loss: 0.419156]\n",
      "epoch:14 step:13777 [D loss: 0.211680, acc.: 66.41%] [G loss: 0.435715]\n",
      "epoch:14 step:13778 [D loss: 0.166674, acc.: 74.22%] [G loss: 0.456378]\n",
      "epoch:14 step:13779 [D loss: 0.194560, acc.: 67.97%] [G loss: 0.480177]\n",
      "epoch:14 step:13780 [D loss: 0.209995, acc.: 66.41%] [G loss: 0.429404]\n",
      "epoch:14 step:13781 [D loss: 0.243456, acc.: 57.81%] [G loss: 0.405955]\n",
      "epoch:14 step:13782 [D loss: 0.241230, acc.: 57.03%] [G loss: 0.462567]\n",
      "epoch:14 step:13783 [D loss: 0.231444, acc.: 58.59%] [G loss: 0.459020]\n",
      "epoch:14 step:13784 [D loss: 0.187175, acc.: 70.31%] [G loss: 0.488945]\n",
      "epoch:14 step:13785 [D loss: 0.210073, acc.: 66.41%] [G loss: 0.484417]\n",
      "epoch:14 step:13786 [D loss: 0.245191, acc.: 57.81%] [G loss: 0.452871]\n",
      "epoch:14 step:13787 [D loss: 0.207894, acc.: 66.41%] [G loss: 0.419664]\n",
      "epoch:14 step:13788 [D loss: 0.241170, acc.: 57.81%] [G loss: 0.419118]\n",
      "epoch:14 step:13789 [D loss: 0.244686, acc.: 57.81%] [G loss: 0.393872]\n",
      "epoch:14 step:13790 [D loss: 0.236020, acc.: 60.94%] [G loss: 0.451199]\n",
      "epoch:14 step:13791 [D loss: 0.228153, acc.: 59.38%] [G loss: 0.448685]\n",
      "epoch:14 step:13792 [D loss: 0.220877, acc.: 61.72%] [G loss: 0.500427]\n",
      "epoch:14 step:13793 [D loss: 0.239733, acc.: 60.16%] [G loss: 0.471330]\n",
      "epoch:14 step:13794 [D loss: 0.265564, acc.: 46.88%] [G loss: 0.390880]\n",
      "epoch:14 step:13795 [D loss: 0.193509, acc.: 72.66%] [G loss: 0.456092]\n",
      "epoch:14 step:13796 [D loss: 0.239594, acc.: 57.03%] [G loss: 0.412262]\n",
      "epoch:14 step:13797 [D loss: 0.217700, acc.: 61.72%] [G loss: 0.423442]\n",
      "epoch:14 step:13798 [D loss: 0.229874, acc.: 62.50%] [G loss: 0.410334]\n",
      "epoch:14 step:13799 [D loss: 0.198374, acc.: 67.97%] [G loss: 0.446549]\n",
      "epoch:14 step:13800 [D loss: 0.214849, acc.: 63.28%] [G loss: 0.442043]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.956768\n",
      "FID: 7.240803\n",
      "0 = 11.803069304418559\n",
      "1 = 0.04682538267550538\n",
      "2 = 0.8852999806404114\n",
      "3 = 0.868399977684021\n",
      "4 = 0.9021999835968018\n",
      "5 = 0.8987787365913391\n",
      "6 = 0.868399977684021\n",
      "7 = 5.92877073221207\n",
      "8 = 0.05978804024610379\n",
      "9 = 0.6917999982833862\n",
      "10 = 0.6906999945640564\n",
      "11 = 0.6929000020027161\n",
      "12 = 0.6922228932380676\n",
      "13 = 0.6906999945640564\n",
      "14 = 7.9568352699279785\n",
      "15 = 9.608242988586426\n",
      "16 = 0.08831951022148132\n",
      "17 = 7.956768035888672\n",
      "18 = 7.240802764892578\n",
      "epoch:14 step:13801 [D loss: 0.254160, acc.: 59.38%] [G loss: 0.399886]\n",
      "epoch:14 step:13802 [D loss: 0.210587, acc.: 63.28%] [G loss: 0.427773]\n",
      "epoch:14 step:13803 [D loss: 0.219949, acc.: 60.94%] [G loss: 0.396179]\n",
      "epoch:14 step:13804 [D loss: 0.215810, acc.: 70.31%] [G loss: 0.448370]\n",
      "epoch:14 step:13805 [D loss: 0.227758, acc.: 57.81%] [G loss: 0.436402]\n",
      "epoch:14 step:13806 [D loss: 0.213005, acc.: 67.97%] [G loss: 0.468139]\n",
      "epoch:14 step:13807 [D loss: 0.212207, acc.: 68.75%] [G loss: 0.524579]\n",
      "epoch:14 step:13808 [D loss: 0.193429, acc.: 69.53%] [G loss: 0.502016]\n",
      "epoch:14 step:13809 [D loss: 0.190980, acc.: 71.88%] [G loss: 0.492062]\n",
      "epoch:14 step:13810 [D loss: 0.205717, acc.: 69.53%] [G loss: 0.520103]\n",
      "epoch:14 step:13811 [D loss: 0.198846, acc.: 67.97%] [G loss: 0.479722]\n",
      "epoch:14 step:13812 [D loss: 0.194408, acc.: 69.53%] [G loss: 0.486233]\n",
      "epoch:14 step:13813 [D loss: 0.224722, acc.: 65.62%] [G loss: 0.498334]\n",
      "epoch:14 step:13814 [D loss: 0.244214, acc.: 58.59%] [G loss: 0.464704]\n",
      "epoch:14 step:13815 [D loss: 0.225128, acc.: 63.28%] [G loss: 0.438658]\n",
      "epoch:14 step:13816 [D loss: 0.209818, acc.: 66.41%] [G loss: 0.455021]\n",
      "epoch:14 step:13817 [D loss: 0.204193, acc.: 70.31%] [G loss: 0.466180]\n",
      "epoch:14 step:13818 [D loss: 0.193186, acc.: 68.75%] [G loss: 0.483797]\n",
      "epoch:14 step:13819 [D loss: 0.207265, acc.: 67.97%] [G loss: 0.481903]\n",
      "epoch:14 step:13820 [D loss: 0.238336, acc.: 58.59%] [G loss: 0.443714]\n",
      "epoch:14 step:13821 [D loss: 0.229624, acc.: 57.81%] [G loss: 0.425720]\n",
      "epoch:14 step:13822 [D loss: 0.230836, acc.: 60.94%] [G loss: 0.416123]\n",
      "epoch:14 step:13823 [D loss: 0.235148, acc.: 57.03%] [G loss: 0.420276]\n",
      "epoch:14 step:13824 [D loss: 0.222123, acc.: 61.72%] [G loss: 0.429961]\n",
      "epoch:14 step:13825 [D loss: 0.206178, acc.: 68.75%] [G loss: 0.449131]\n",
      "epoch:14 step:13826 [D loss: 0.171041, acc.: 75.00%] [G loss: 0.463272]\n",
      "epoch:14 step:13827 [D loss: 0.179906, acc.: 76.56%] [G loss: 0.499042]\n",
      "epoch:14 step:13828 [D loss: 0.280508, acc.: 49.22%] [G loss: 0.403111]\n",
      "epoch:14 step:13829 [D loss: 0.224171, acc.: 64.84%] [G loss: 0.425723]\n",
      "epoch:14 step:13830 [D loss: 0.215998, acc.: 64.06%] [G loss: 0.479822]\n",
      "epoch:14 step:13831 [D loss: 0.225470, acc.: 59.38%] [G loss: 0.430916]\n",
      "epoch:14 step:13832 [D loss: 0.227883, acc.: 62.50%] [G loss: 0.431093]\n",
      "epoch:14 step:13833 [D loss: 0.246155, acc.: 57.03%] [G loss: 0.454780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13834 [D loss: 0.237627, acc.: 66.41%] [G loss: 0.469750]\n",
      "epoch:14 step:13835 [D loss: 0.224903, acc.: 61.72%] [G loss: 0.425526]\n",
      "epoch:14 step:13836 [D loss: 0.230191, acc.: 59.38%] [G loss: 0.448348]\n",
      "epoch:14 step:13837 [D loss: 0.189960, acc.: 71.88%] [G loss: 0.489770]\n",
      "epoch:14 step:13838 [D loss: 0.232809, acc.: 57.03%] [G loss: 0.464533]\n",
      "epoch:14 step:13839 [D loss: 0.227361, acc.: 57.81%] [G loss: 0.418054]\n",
      "epoch:14 step:13840 [D loss: 0.256299, acc.: 54.69%] [G loss: 0.460809]\n",
      "epoch:14 step:13841 [D loss: 0.236191, acc.: 57.81%] [G loss: 0.452883]\n",
      "epoch:14 step:13842 [D loss: 0.216196, acc.: 64.06%] [G loss: 0.463625]\n",
      "epoch:14 step:13843 [D loss: 0.216291, acc.: 65.62%] [G loss: 0.462385]\n",
      "epoch:14 step:13844 [D loss: 0.217693, acc.: 66.41%] [G loss: 0.469300]\n",
      "epoch:14 step:13845 [D loss: 0.225294, acc.: 64.06%] [G loss: 0.445114]\n",
      "epoch:14 step:13846 [D loss: 0.213626, acc.: 66.41%] [G loss: 0.437649]\n",
      "epoch:14 step:13847 [D loss: 0.230033, acc.: 59.38%] [G loss: 0.405331]\n",
      "epoch:14 step:13848 [D loss: 0.185309, acc.: 71.88%] [G loss: 0.464459]\n",
      "epoch:14 step:13849 [D loss: 0.215508, acc.: 62.50%] [G loss: 0.492818]\n",
      "epoch:14 step:13850 [D loss: 0.213045, acc.: 65.62%] [G loss: 0.463299]\n",
      "epoch:14 step:13851 [D loss: 0.191698, acc.: 74.22%] [G loss: 0.509317]\n",
      "epoch:14 step:13852 [D loss: 0.224727, acc.: 60.16%] [G loss: 0.446072]\n",
      "epoch:14 step:13853 [D loss: 0.240633, acc.: 56.25%] [G loss: 0.442967]\n",
      "epoch:14 step:13854 [D loss: 0.225181, acc.: 60.94%] [G loss: 0.456981]\n",
      "epoch:14 step:13855 [D loss: 0.221134, acc.: 64.84%] [G loss: 0.449043]\n",
      "epoch:14 step:13856 [D loss: 0.235565, acc.: 56.25%] [G loss: 0.446571]\n",
      "epoch:14 step:13857 [D loss: 0.251200, acc.: 57.03%] [G loss: 0.434418]\n",
      "epoch:14 step:13858 [D loss: 0.226868, acc.: 63.28%] [G loss: 0.443674]\n",
      "epoch:14 step:13859 [D loss: 0.251923, acc.: 57.81%] [G loss: 0.441338]\n",
      "epoch:14 step:13860 [D loss: 0.216015, acc.: 64.84%] [G loss: 0.420832]\n",
      "epoch:14 step:13861 [D loss: 0.208115, acc.: 66.41%] [G loss: 0.489550]\n",
      "epoch:14 step:13862 [D loss: 0.237969, acc.: 61.72%] [G loss: 0.488218]\n",
      "epoch:14 step:13863 [D loss: 0.242754, acc.: 53.12%] [G loss: 0.470246]\n",
      "epoch:14 step:13864 [D loss: 0.216422, acc.: 61.72%] [G loss: 0.436714]\n",
      "epoch:14 step:13865 [D loss: 0.204882, acc.: 66.41%] [G loss: 0.454708]\n",
      "epoch:14 step:13866 [D loss: 0.201065, acc.: 67.19%] [G loss: 0.433779]\n",
      "epoch:14 step:13867 [D loss: 0.233873, acc.: 61.72%] [G loss: 0.448183]\n",
      "epoch:14 step:13868 [D loss: 0.213600, acc.: 61.72%] [G loss: 0.465789]\n",
      "epoch:14 step:13869 [D loss: 0.218710, acc.: 62.50%] [G loss: 0.441529]\n",
      "epoch:14 step:13870 [D loss: 0.224775, acc.: 63.28%] [G loss: 0.457915]\n",
      "epoch:14 step:13871 [D loss: 0.207352, acc.: 69.53%] [G loss: 0.448353]\n",
      "epoch:14 step:13872 [D loss: 0.200341, acc.: 64.84%] [G loss: 0.430083]\n",
      "epoch:14 step:13873 [D loss: 0.218216, acc.: 66.41%] [G loss: 0.470827]\n",
      "epoch:14 step:13874 [D loss: 0.184873, acc.: 69.53%] [G loss: 0.496899]\n",
      "epoch:14 step:13875 [D loss: 0.225099, acc.: 58.59%] [G loss: 0.432435]\n",
      "epoch:14 step:13876 [D loss: 0.254872, acc.: 57.81%] [G loss: 0.404652]\n",
      "epoch:14 step:13877 [D loss: 0.220049, acc.: 60.94%] [G loss: 0.443847]\n",
      "epoch:14 step:13878 [D loss: 0.238286, acc.: 59.38%] [G loss: 0.384331]\n",
      "epoch:14 step:13879 [D loss: 0.233568, acc.: 57.81%] [G loss: 0.431185]\n",
      "epoch:14 step:13880 [D loss: 0.239364, acc.: 60.16%] [G loss: 0.400419]\n",
      "epoch:14 step:13881 [D loss: 0.244709, acc.: 57.81%] [G loss: 0.418723]\n",
      "epoch:14 step:13882 [D loss: 0.230987, acc.: 63.28%] [G loss: 0.446164]\n",
      "epoch:14 step:13883 [D loss: 0.259064, acc.: 53.12%] [G loss: 0.457950]\n",
      "epoch:14 step:13884 [D loss: 0.237857, acc.: 61.72%] [G loss: 0.436580]\n",
      "epoch:14 step:13885 [D loss: 0.238196, acc.: 58.59%] [G loss: 0.472667]\n",
      "epoch:14 step:13886 [D loss: 0.231517, acc.: 59.38%] [G loss: 0.450357]\n",
      "epoch:14 step:13887 [D loss: 0.215516, acc.: 63.28%] [G loss: 0.470587]\n",
      "epoch:14 step:13888 [D loss: 0.216575, acc.: 61.72%] [G loss: 0.485110]\n",
      "epoch:14 step:13889 [D loss: 0.232644, acc.: 57.03%] [G loss: 0.468482]\n",
      "epoch:14 step:13890 [D loss: 0.233515, acc.: 64.06%] [G loss: 0.413852]\n",
      "epoch:14 step:13891 [D loss: 0.223634, acc.: 65.62%] [G loss: 0.421890]\n",
      "epoch:14 step:13892 [D loss: 0.233049, acc.: 64.84%] [G loss: 0.417289]\n",
      "epoch:14 step:13893 [D loss: 0.220967, acc.: 59.38%] [G loss: 0.486362]\n",
      "epoch:14 step:13894 [D loss: 0.216247, acc.: 64.06%] [G loss: 0.485585]\n",
      "epoch:14 step:13895 [D loss: 0.220875, acc.: 64.84%] [G loss: 0.423578]\n",
      "epoch:14 step:13896 [D loss: 0.228374, acc.: 58.59%] [G loss: 0.404806]\n",
      "epoch:14 step:13897 [D loss: 0.217006, acc.: 69.53%] [G loss: 0.432713]\n",
      "epoch:14 step:13898 [D loss: 0.207934, acc.: 70.31%] [G loss: 0.449610]\n",
      "epoch:14 step:13899 [D loss: 0.202854, acc.: 72.66%] [G loss: 0.449034]\n",
      "epoch:14 step:13900 [D loss: 0.204870, acc.: 66.41%] [G loss: 0.491765]\n",
      "epoch:14 step:13901 [D loss: 0.270142, acc.: 54.69%] [G loss: 0.493800]\n",
      "epoch:14 step:13902 [D loss: 0.263084, acc.: 51.56%] [G loss: 0.406981]\n",
      "epoch:14 step:13903 [D loss: 0.256993, acc.: 59.38%] [G loss: 0.402857]\n",
      "epoch:14 step:13904 [D loss: 0.195496, acc.: 73.44%] [G loss: 0.434802]\n",
      "epoch:14 step:13905 [D loss: 0.248400, acc.: 55.47%] [G loss: 0.452497]\n",
      "epoch:14 step:13906 [D loss: 0.265021, acc.: 52.34%] [G loss: 0.414287]\n",
      "epoch:14 step:13907 [D loss: 0.205942, acc.: 67.19%] [G loss: 0.425103]\n",
      "epoch:14 step:13908 [D loss: 0.224800, acc.: 63.28%] [G loss: 0.433176]\n",
      "epoch:14 step:13909 [D loss: 0.261789, acc.: 52.34%] [G loss: 0.429507]\n",
      "epoch:14 step:13910 [D loss: 0.213200, acc.: 67.97%] [G loss: 0.448213]\n",
      "epoch:14 step:13911 [D loss: 0.240729, acc.: 58.59%] [G loss: 0.457994]\n",
      "epoch:14 step:13912 [D loss: 0.256596, acc.: 52.34%] [G loss: 0.458224]\n",
      "epoch:14 step:13913 [D loss: 0.248943, acc.: 55.47%] [G loss: 0.441004]\n",
      "epoch:14 step:13914 [D loss: 0.211453, acc.: 70.31%] [G loss: 0.467772]\n",
      "epoch:14 step:13915 [D loss: 0.271173, acc.: 57.03%] [G loss: 0.439519]\n",
      "epoch:14 step:13916 [D loss: 0.226610, acc.: 61.72%] [G loss: 0.415191]\n",
      "epoch:14 step:13917 [D loss: 0.231204, acc.: 64.84%] [G loss: 0.406079]\n",
      "epoch:14 step:13918 [D loss: 0.232301, acc.: 59.38%] [G loss: 0.416034]\n",
      "epoch:14 step:13919 [D loss: 0.198688, acc.: 69.53%] [G loss: 0.442160]\n",
      "epoch:14 step:13920 [D loss: 0.200013, acc.: 68.75%] [G loss: 0.493777]\n",
      "epoch:14 step:13921 [D loss: 0.210877, acc.: 67.97%] [G loss: 0.473613]\n",
      "epoch:14 step:13922 [D loss: 0.226771, acc.: 59.38%] [G loss: 0.426046]\n",
      "epoch:14 step:13923 [D loss: 0.220608, acc.: 64.06%] [G loss: 0.429479]\n",
      "epoch:14 step:13924 [D loss: 0.231445, acc.: 57.03%] [G loss: 0.398889]\n",
      "epoch:14 step:13925 [D loss: 0.223581, acc.: 64.84%] [G loss: 0.405605]\n",
      "epoch:14 step:13926 [D loss: 0.239320, acc.: 60.94%] [G loss: 0.435768]\n",
      "epoch:14 step:13927 [D loss: 0.236292, acc.: 57.81%] [G loss: 0.457402]\n",
      "epoch:14 step:13928 [D loss: 0.222430, acc.: 57.81%] [G loss: 0.452282]\n",
      "epoch:14 step:13929 [D loss: 0.227209, acc.: 67.19%] [G loss: 0.451176]\n",
      "epoch:14 step:13930 [D loss: 0.247096, acc.: 54.69%] [G loss: 0.477057]\n",
      "epoch:14 step:13931 [D loss: 0.237335, acc.: 62.50%] [G loss: 0.450700]\n",
      "epoch:14 step:13932 [D loss: 0.230752, acc.: 60.94%] [G loss: 0.457712]\n",
      "epoch:14 step:13933 [D loss: 0.178807, acc.: 78.91%] [G loss: 0.493093]\n",
      "epoch:14 step:13934 [D loss: 0.223144, acc.: 62.50%] [G loss: 0.492082]\n",
      "epoch:14 step:13935 [D loss: 0.258040, acc.: 55.47%] [G loss: 0.520544]\n",
      "epoch:14 step:13936 [D loss: 0.252466, acc.: 56.25%] [G loss: 0.451310]\n",
      "epoch:14 step:13937 [D loss: 0.208016, acc.: 67.19%] [G loss: 0.437353]\n",
      "epoch:14 step:13938 [D loss: 0.271438, acc.: 55.47%] [G loss: 0.436483]\n",
      "epoch:14 step:13939 [D loss: 0.212289, acc.: 64.06%] [G loss: 0.456067]\n",
      "epoch:14 step:13940 [D loss: 0.218564, acc.: 64.06%] [G loss: 0.420063]\n",
      "epoch:14 step:13941 [D loss: 0.199262, acc.: 67.19%] [G loss: 0.437429]\n",
      "epoch:14 step:13942 [D loss: 0.233800, acc.: 60.16%] [G loss: 0.447327]\n",
      "epoch:14 step:13943 [D loss: 0.196217, acc.: 72.66%] [G loss: 0.449824]\n",
      "epoch:14 step:13944 [D loss: 0.234977, acc.: 64.06%] [G loss: 0.437241]\n",
      "epoch:14 step:13945 [D loss: 0.238965, acc.: 56.25%] [G loss: 0.446537]\n",
      "epoch:14 step:13946 [D loss: 0.277413, acc.: 53.91%] [G loss: 0.411893]\n",
      "epoch:14 step:13947 [D loss: 0.257227, acc.: 50.78%] [G loss: 0.419417]\n",
      "epoch:14 step:13948 [D loss: 0.227202, acc.: 61.72%] [G loss: 0.440616]\n",
      "epoch:14 step:13949 [D loss: 0.236333, acc.: 60.94%] [G loss: 0.436509]\n",
      "epoch:14 step:13950 [D loss: 0.199707, acc.: 72.66%] [G loss: 0.428837]\n",
      "epoch:14 step:13951 [D loss: 0.221320, acc.: 60.16%] [G loss: 0.455152]\n",
      "epoch:14 step:13952 [D loss: 0.217140, acc.: 65.62%] [G loss: 0.461933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13953 [D loss: 0.216712, acc.: 64.06%] [G loss: 0.454489]\n",
      "epoch:14 step:13954 [D loss: 0.219248, acc.: 64.84%] [G loss: 0.455520]\n",
      "epoch:14 step:13955 [D loss: 0.217856, acc.: 62.50%] [G loss: 0.441213]\n",
      "epoch:14 step:13956 [D loss: 0.207599, acc.: 70.31%] [G loss: 0.458907]\n",
      "epoch:14 step:13957 [D loss: 0.217355, acc.: 64.84%] [G loss: 0.461786]\n",
      "epoch:14 step:13958 [D loss: 0.231654, acc.: 60.94%] [G loss: 0.450440]\n",
      "epoch:14 step:13959 [D loss: 0.219834, acc.: 62.50%] [G loss: 0.456101]\n",
      "epoch:14 step:13960 [D loss: 0.206534, acc.: 64.84%] [G loss: 0.450204]\n",
      "epoch:14 step:13961 [D loss: 0.227238, acc.: 63.28%] [G loss: 0.430387]\n",
      "epoch:14 step:13962 [D loss: 0.208801, acc.: 67.19%] [G loss: 0.485637]\n",
      "epoch:14 step:13963 [D loss: 0.213723, acc.: 65.62%] [G loss: 0.450459]\n",
      "epoch:14 step:13964 [D loss: 0.257659, acc.: 49.22%] [G loss: 0.429410]\n",
      "epoch:14 step:13965 [D loss: 0.223309, acc.: 64.06%] [G loss: 0.428330]\n",
      "epoch:14 step:13966 [D loss: 0.238870, acc.: 58.59%] [G loss: 0.449768]\n",
      "epoch:14 step:13967 [D loss: 0.215647, acc.: 64.84%] [G loss: 0.416845]\n",
      "epoch:14 step:13968 [D loss: 0.226721, acc.: 62.50%] [G loss: 0.471944]\n",
      "epoch:14 step:13969 [D loss: 0.233337, acc.: 58.59%] [G loss: 0.432920]\n",
      "epoch:14 step:13970 [D loss: 0.223844, acc.: 62.50%] [G loss: 0.460393]\n",
      "epoch:14 step:13971 [D loss: 0.194931, acc.: 67.19%] [G loss: 0.509831]\n",
      "epoch:14 step:13972 [D loss: 0.220497, acc.: 62.50%] [G loss: 0.402813]\n",
      "epoch:14 step:13973 [D loss: 0.272364, acc.: 53.12%] [G loss: 0.399758]\n",
      "epoch:14 step:13974 [D loss: 0.226567, acc.: 64.84%] [G loss: 0.446746]\n",
      "epoch:14 step:13975 [D loss: 0.227544, acc.: 65.62%] [G loss: 0.407657]\n",
      "epoch:14 step:13976 [D loss: 0.279659, acc.: 49.22%] [G loss: 0.409371]\n",
      "epoch:14 step:13977 [D loss: 0.228152, acc.: 63.28%] [G loss: 0.429819]\n",
      "epoch:14 step:13978 [D loss: 0.207736, acc.: 69.53%] [G loss: 0.474237]\n",
      "epoch:14 step:13979 [D loss: 0.269762, acc.: 55.47%] [G loss: 0.425404]\n",
      "epoch:14 step:13980 [D loss: 0.243782, acc.: 56.25%] [G loss: 0.403818]\n",
      "epoch:14 step:13981 [D loss: 0.221710, acc.: 60.94%] [G loss: 0.412942]\n",
      "epoch:14 step:13982 [D loss: 0.231444, acc.: 59.38%] [G loss: 0.446922]\n",
      "epoch:14 step:13983 [D loss: 0.245503, acc.: 61.72%] [G loss: 0.430132]\n",
      "epoch:14 step:13984 [D loss: 0.203351, acc.: 66.41%] [G loss: 0.435895]\n",
      "epoch:14 step:13985 [D loss: 0.260469, acc.: 49.22%] [G loss: 0.385695]\n",
      "epoch:14 step:13986 [D loss: 0.217706, acc.: 64.84%] [G loss: 0.435824]\n",
      "epoch:14 step:13987 [D loss: 0.245306, acc.: 59.38%] [G loss: 0.451472]\n",
      "epoch:14 step:13988 [D loss: 0.200774, acc.: 67.19%] [G loss: 0.478649]\n",
      "epoch:14 step:13989 [D loss: 0.214839, acc.: 65.62%] [G loss: 0.457740]\n",
      "epoch:14 step:13990 [D loss: 0.246358, acc.: 60.16%] [G loss: 0.440851]\n",
      "epoch:14 step:13991 [D loss: 0.235331, acc.: 58.59%] [G loss: 0.433854]\n",
      "epoch:14 step:13992 [D loss: 0.229295, acc.: 64.06%] [G loss: 0.453248]\n",
      "epoch:14 step:13993 [D loss: 0.188564, acc.: 73.44%] [G loss: 0.445316]\n",
      "epoch:14 step:13994 [D loss: 0.219309, acc.: 64.06%] [G loss: 0.435979]\n",
      "epoch:14 step:13995 [D loss: 0.228366, acc.: 58.59%] [G loss: 0.396597]\n",
      "epoch:14 step:13996 [D loss: 0.225346, acc.: 61.72%] [G loss: 0.433151]\n",
      "epoch:14 step:13997 [D loss: 0.220677, acc.: 67.97%] [G loss: 0.450470]\n",
      "epoch:14 step:13998 [D loss: 0.254773, acc.: 55.47%] [G loss: 0.394144]\n",
      "epoch:14 step:13999 [D loss: 0.234871, acc.: 60.94%] [G loss: 0.409482]\n",
      "epoch:14 step:14000 [D loss: 0.237289, acc.: 61.72%] [G loss: 0.433223]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.996518\n",
      "FID: 8.293356\n",
      "0 = 11.693758944582914\n",
      "1 = 0.04849249885991349\n",
      "2 = 0.8694000244140625\n",
      "3 = 0.8503000140190125\n",
      "4 = 0.8884999752044678\n",
      "5 = 0.8840715289115906\n",
      "6 = 0.8503000140190125\n",
      "7 = 5.958132736450434\n",
      "8 = 0.06463412676765717\n",
      "9 = 0.6826500296592712\n",
      "10 = 0.6823999881744385\n",
      "11 = 0.6829000115394592\n",
      "12 = 0.6827413439750671\n",
      "13 = 0.6823999881744385\n",
      "14 = 7.996588706970215\n",
      "15 = 9.540563583374023\n",
      "16 = 0.10736142843961716\n",
      "17 = 7.996518135070801\n",
      "18 = 8.293355941772461\n",
      "epoch:14 step:14001 [D loss: 0.235611, acc.: 59.38%] [G loss: 0.450982]\n",
      "epoch:14 step:14002 [D loss: 0.208104, acc.: 67.19%] [G loss: 0.484097]\n",
      "epoch:14 step:14003 [D loss: 0.216180, acc.: 69.53%] [G loss: 0.456480]\n",
      "epoch:14 step:14004 [D loss: 0.216363, acc.: 65.62%] [G loss: 0.467851]\n",
      "epoch:14 step:14005 [D loss: 0.221055, acc.: 68.75%] [G loss: 0.474537]\n",
      "epoch:14 step:14006 [D loss: 0.254088, acc.: 56.25%] [G loss: 0.472888]\n",
      "epoch:14 step:14007 [D loss: 0.195606, acc.: 68.75%] [G loss: 0.472833]\n",
      "epoch:14 step:14008 [D loss: 0.217634, acc.: 64.06%] [G loss: 0.477969]\n",
      "epoch:14 step:14009 [D loss: 0.277027, acc.: 53.12%] [G loss: 0.402092]\n",
      "epoch:14 step:14010 [D loss: 0.240485, acc.: 58.59%] [G loss: 0.417616]\n",
      "epoch:14 step:14011 [D loss: 0.200348, acc.: 69.53%] [G loss: 0.478825]\n",
      "epoch:14 step:14012 [D loss: 0.206439, acc.: 73.44%] [G loss: 0.450455]\n",
      "epoch:14 step:14013 [D loss: 0.197626, acc.: 67.19%] [G loss: 0.476060]\n",
      "epoch:14 step:14014 [D loss: 0.200905, acc.: 71.88%] [G loss: 0.438503]\n",
      "epoch:14 step:14015 [D loss: 0.196478, acc.: 71.09%] [G loss: 0.470919]\n",
      "epoch:14 step:14016 [D loss: 0.208601, acc.: 70.31%] [G loss: 0.450603]\n",
      "epoch:14 step:14017 [D loss: 0.200083, acc.: 64.06%] [G loss: 0.452034]\n",
      "epoch:14 step:14018 [D loss: 0.197400, acc.: 69.53%] [G loss: 0.479763]\n",
      "epoch:14 step:14019 [D loss: 0.218300, acc.: 64.84%] [G loss: 0.453460]\n",
      "epoch:14 step:14020 [D loss: 0.238680, acc.: 61.72%] [G loss: 0.443306]\n",
      "epoch:14 step:14021 [D loss: 0.216287, acc.: 65.62%] [G loss: 0.401245]\n",
      "epoch:14 step:14022 [D loss: 0.215850, acc.: 67.19%] [G loss: 0.431180]\n",
      "epoch:14 step:14023 [D loss: 0.205611, acc.: 68.75%] [G loss: 0.412026]\n",
      "epoch:14 step:14024 [D loss: 0.208061, acc.: 68.75%] [G loss: 0.445528]\n",
      "epoch:14 step:14025 [D loss: 0.244793, acc.: 57.03%] [G loss: 0.427425]\n",
      "epoch:14 step:14026 [D loss: 0.199136, acc.: 78.12%] [G loss: 0.481074]\n",
      "epoch:14 step:14027 [D loss: 0.179764, acc.: 78.12%] [G loss: 0.477439]\n",
      "epoch:14 step:14028 [D loss: 0.234017, acc.: 59.38%] [G loss: 0.472938]\n",
      "epoch:14 step:14029 [D loss: 0.201628, acc.: 67.19%] [G loss: 0.463611]\n",
      "epoch:14 step:14030 [D loss: 0.201464, acc.: 68.75%] [G loss: 0.456107]\n",
      "epoch:14 step:14031 [D loss: 0.226139, acc.: 60.94%] [G loss: 0.458273]\n",
      "epoch:14 step:14032 [D loss: 0.221628, acc.: 60.16%] [G loss: 0.468568]\n",
      "epoch:14 step:14033 [D loss: 0.225280, acc.: 65.62%] [G loss: 0.448799]\n",
      "epoch:14 step:14034 [D loss: 0.221672, acc.: 59.38%] [G loss: 0.416615]\n",
      "epoch:14 step:14035 [D loss: 0.223058, acc.: 67.97%] [G loss: 0.454105]\n",
      "epoch:14 step:14036 [D loss: 0.177480, acc.: 79.69%] [G loss: 0.430935]\n",
      "epoch:14 step:14037 [D loss: 0.191439, acc.: 72.66%] [G loss: 0.475053]\n",
      "epoch:14 step:14038 [D loss: 0.268995, acc.: 47.66%] [G loss: 0.462553]\n",
      "epoch:14 step:14039 [D loss: 0.190916, acc.: 70.31%] [G loss: 0.532805]\n",
      "epoch:14 step:14040 [D loss: 0.251624, acc.: 59.38%] [G loss: 0.452907]\n",
      "epoch:14 step:14041 [D loss: 0.186753, acc.: 73.44%] [G loss: 0.458387]\n",
      "epoch:14 step:14042 [D loss: 0.197930, acc.: 71.09%] [G loss: 0.445040]\n",
      "epoch:14 step:14043 [D loss: 0.175357, acc.: 76.56%] [G loss: 0.512607]\n",
      "epoch:14 step:14044 [D loss: 0.179474, acc.: 75.78%] [G loss: 0.545295]\n",
      "epoch:14 step:14045 [D loss: 0.205531, acc.: 67.97%] [G loss: 0.507190]\n",
      "epoch:14 step:14046 [D loss: 0.330454, acc.: 56.25%] [G loss: 0.505950]\n",
      "epoch:14 step:14047 [D loss: 0.225199, acc.: 67.97%] [G loss: 0.637506]\n",
      "epoch:14 step:14048 [D loss: 0.216446, acc.: 64.84%] [G loss: 0.516958]\n",
      "epoch:14 step:14049 [D loss: 0.239353, acc.: 60.94%] [G loss: 0.442190]\n",
      "epoch:14 step:14050 [D loss: 0.261296, acc.: 53.91%] [G loss: 0.387842]\n",
      "epoch:14 step:14051 [D loss: 0.222789, acc.: 64.84%] [G loss: 0.430142]\n",
      "epoch:14 step:14052 [D loss: 0.218028, acc.: 64.84%] [G loss: 0.396485]\n",
      "epoch:14 step:14053 [D loss: 0.200668, acc.: 74.22%] [G loss: 0.455359]\n",
      "epoch:14 step:14054 [D loss: 0.161200, acc.: 79.69%] [G loss: 0.491306]\n",
      "epoch:14 step:14055 [D loss: 0.169927, acc.: 74.22%] [G loss: 0.582055]\n",
      "epoch:15 step:14056 [D loss: 0.232663, acc.: 63.28%] [G loss: 0.527718]\n",
      "epoch:15 step:14057 [D loss: 0.284870, acc.: 47.66%] [G loss: 0.493958]\n",
      "epoch:15 step:14058 [D loss: 0.236692, acc.: 57.03%] [G loss: 0.478337]\n",
      "epoch:15 step:14059 [D loss: 0.232557, acc.: 62.50%] [G loss: 0.494652]\n",
      "epoch:15 step:14060 [D loss: 0.251821, acc.: 53.91%] [G loss: 0.427076]\n",
      "epoch:15 step:14061 [D loss: 0.194342, acc.: 69.53%] [G loss: 0.528217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14062 [D loss: 0.220040, acc.: 67.19%] [G loss: 0.473961]\n",
      "epoch:15 step:14063 [D loss: 0.245321, acc.: 58.59%] [G loss: 0.434864]\n",
      "epoch:15 step:14064 [D loss: 0.212304, acc.: 63.28%] [G loss: 0.493488]\n",
      "epoch:15 step:14065 [D loss: 0.206093, acc.: 71.88%] [G loss: 0.499298]\n",
      "epoch:15 step:14066 [D loss: 0.195142, acc.: 66.41%] [G loss: 0.496524]\n",
      "epoch:15 step:14067 [D loss: 0.207599, acc.: 69.53%] [G loss: 0.466985]\n",
      "epoch:15 step:14068 [D loss: 0.233669, acc.: 60.16%] [G loss: 0.427539]\n",
      "epoch:15 step:14069 [D loss: 0.213996, acc.: 67.19%] [G loss: 0.454189]\n",
      "epoch:15 step:14070 [D loss: 0.218458, acc.: 63.28%] [G loss: 0.454419]\n",
      "epoch:15 step:14071 [D loss: 0.192704, acc.: 72.66%] [G loss: 0.496186]\n",
      "epoch:15 step:14072 [D loss: 0.231051, acc.: 61.72%] [G loss: 0.504449]\n",
      "epoch:15 step:14073 [D loss: 0.223630, acc.: 62.50%] [G loss: 0.442360]\n",
      "epoch:15 step:14074 [D loss: 0.237757, acc.: 64.84%] [G loss: 0.521485]\n",
      "epoch:15 step:14075 [D loss: 0.273424, acc.: 49.22%] [G loss: 0.552943]\n",
      "epoch:15 step:14076 [D loss: 0.252661, acc.: 57.81%] [G loss: 0.488238]\n",
      "epoch:15 step:14077 [D loss: 0.232782, acc.: 64.84%] [G loss: 0.529584]\n",
      "epoch:15 step:14078 [D loss: 0.245968, acc.: 53.91%] [G loss: 0.418615]\n",
      "epoch:15 step:14079 [D loss: 0.228034, acc.: 63.28%] [G loss: 0.417522]\n",
      "epoch:15 step:14080 [D loss: 0.200240, acc.: 71.88%] [G loss: 0.424382]\n",
      "epoch:15 step:14081 [D loss: 0.226738, acc.: 60.16%] [G loss: 0.436074]\n",
      "epoch:15 step:14082 [D loss: 0.222963, acc.: 64.06%] [G loss: 0.405390]\n",
      "epoch:15 step:14083 [D loss: 0.236630, acc.: 64.84%] [G loss: 0.417962]\n",
      "epoch:15 step:14084 [D loss: 0.209174, acc.: 71.88%] [G loss: 0.451160]\n",
      "epoch:15 step:14085 [D loss: 0.225170, acc.: 57.03%] [G loss: 0.410810]\n",
      "epoch:15 step:14086 [D loss: 0.225660, acc.: 60.94%] [G loss: 0.426067]\n",
      "epoch:15 step:14087 [D loss: 0.230767, acc.: 58.59%] [G loss: 0.485617]\n",
      "epoch:15 step:14088 [D loss: 0.223792, acc.: 60.94%] [G loss: 0.447149]\n",
      "epoch:15 step:14089 [D loss: 0.246779, acc.: 55.47%] [G loss: 0.399790]\n",
      "epoch:15 step:14090 [D loss: 0.203804, acc.: 67.19%] [G loss: 0.430132]\n",
      "epoch:15 step:14091 [D loss: 0.238278, acc.: 63.28%] [G loss: 0.424218]\n",
      "epoch:15 step:14092 [D loss: 0.234853, acc.: 57.03%] [G loss: 0.428172]\n",
      "epoch:15 step:14093 [D loss: 0.249513, acc.: 52.34%] [G loss: 0.438797]\n",
      "epoch:15 step:14094 [D loss: 0.215004, acc.: 62.50%] [G loss: 0.427460]\n",
      "epoch:15 step:14095 [D loss: 0.188318, acc.: 75.78%] [G loss: 0.452598]\n",
      "epoch:15 step:14096 [D loss: 0.241939, acc.: 59.38%] [G loss: 0.415540]\n",
      "epoch:15 step:14097 [D loss: 0.211405, acc.: 65.62%] [G loss: 0.429012]\n",
      "epoch:15 step:14098 [D loss: 0.225783, acc.: 67.97%] [G loss: 0.454474]\n",
      "epoch:15 step:14099 [D loss: 0.223267, acc.: 63.28%] [G loss: 0.486319]\n",
      "epoch:15 step:14100 [D loss: 0.236794, acc.: 59.38%] [G loss: 0.415150]\n",
      "epoch:15 step:14101 [D loss: 0.212056, acc.: 66.41%] [G loss: 0.447088]\n",
      "epoch:15 step:14102 [D loss: 0.192360, acc.: 71.88%] [G loss: 0.481603]\n",
      "epoch:15 step:14103 [D loss: 0.226895, acc.: 62.50%] [G loss: 0.461990]\n",
      "epoch:15 step:14104 [D loss: 0.189639, acc.: 73.44%] [G loss: 0.525581]\n",
      "epoch:15 step:14105 [D loss: 0.218792, acc.: 65.62%] [G loss: 0.472642]\n",
      "epoch:15 step:14106 [D loss: 0.254693, acc.: 57.81%] [G loss: 0.434042]\n",
      "epoch:15 step:14107 [D loss: 0.248022, acc.: 57.03%] [G loss: 0.388601]\n",
      "epoch:15 step:14108 [D loss: 0.222058, acc.: 64.06%] [G loss: 0.423086]\n",
      "epoch:15 step:14109 [D loss: 0.219401, acc.: 63.28%] [G loss: 0.446762]\n",
      "epoch:15 step:14110 [D loss: 0.194754, acc.: 72.66%] [G loss: 0.458816]\n",
      "epoch:15 step:14111 [D loss: 0.224637, acc.: 64.06%] [G loss: 0.433438]\n",
      "epoch:15 step:14112 [D loss: 0.232340, acc.: 60.94%] [G loss: 0.434491]\n",
      "epoch:15 step:14113 [D loss: 0.216011, acc.: 64.06%] [G loss: 0.464096]\n",
      "epoch:15 step:14114 [D loss: 0.211031, acc.: 66.41%] [G loss: 0.465797]\n",
      "epoch:15 step:14115 [D loss: 0.230039, acc.: 62.50%] [G loss: 0.469612]\n",
      "epoch:15 step:14116 [D loss: 0.245615, acc.: 54.69%] [G loss: 0.423122]\n",
      "epoch:15 step:14117 [D loss: 0.235100, acc.: 63.28%] [G loss: 0.406879]\n",
      "epoch:15 step:14118 [D loss: 0.220337, acc.: 68.75%] [G loss: 0.422726]\n",
      "epoch:15 step:14119 [D loss: 0.208352, acc.: 65.62%] [G loss: 0.478999]\n",
      "epoch:15 step:14120 [D loss: 0.222213, acc.: 57.03%] [G loss: 0.426519]\n",
      "epoch:15 step:14121 [D loss: 0.209193, acc.: 70.31%] [G loss: 0.427645]\n",
      "epoch:15 step:14122 [D loss: 0.217399, acc.: 64.06%] [G loss: 0.451102]\n",
      "epoch:15 step:14123 [D loss: 0.243337, acc.: 60.94%] [G loss: 0.430195]\n",
      "epoch:15 step:14124 [D loss: 0.219831, acc.: 66.41%] [G loss: 0.475533]\n",
      "epoch:15 step:14125 [D loss: 0.186548, acc.: 71.88%] [G loss: 0.501193]\n",
      "epoch:15 step:14126 [D loss: 0.240153, acc.: 64.06%] [G loss: 0.465016]\n",
      "epoch:15 step:14127 [D loss: 0.254268, acc.: 60.16%] [G loss: 0.394515]\n",
      "epoch:15 step:14128 [D loss: 0.232561, acc.: 60.16%] [G loss: 0.413587]\n",
      "epoch:15 step:14129 [D loss: 0.193463, acc.: 73.44%] [G loss: 0.405360]\n",
      "epoch:15 step:14130 [D loss: 0.208315, acc.: 69.53%] [G loss: 0.442408]\n",
      "epoch:15 step:14131 [D loss: 0.211619, acc.: 66.41%] [G loss: 0.478189]\n",
      "epoch:15 step:14132 [D loss: 0.169233, acc.: 74.22%] [G loss: 0.521768]\n",
      "epoch:15 step:14133 [D loss: 0.288625, acc.: 52.34%] [G loss: 0.424267]\n",
      "epoch:15 step:14134 [D loss: 0.254680, acc.: 62.50%] [G loss: 0.416283]\n",
      "epoch:15 step:14135 [D loss: 0.240951, acc.: 57.81%] [G loss: 0.430489]\n",
      "epoch:15 step:14136 [D loss: 0.240674, acc.: 64.84%] [G loss: 0.450780]\n",
      "epoch:15 step:14137 [D loss: 0.234312, acc.: 59.38%] [G loss: 0.391246]\n",
      "epoch:15 step:14138 [D loss: 0.207296, acc.: 71.09%] [G loss: 0.504543]\n",
      "epoch:15 step:14139 [D loss: 0.219056, acc.: 63.28%] [G loss: 0.458352]\n",
      "epoch:15 step:14140 [D loss: 0.246983, acc.: 63.28%] [G loss: 0.461032]\n",
      "epoch:15 step:14141 [D loss: 0.212887, acc.: 67.19%] [G loss: 0.421525]\n",
      "epoch:15 step:14142 [D loss: 0.229187, acc.: 58.59%] [G loss: 0.432322]\n",
      "epoch:15 step:14143 [D loss: 0.203055, acc.: 71.09%] [G loss: 0.491996]\n",
      "epoch:15 step:14144 [D loss: 0.230908, acc.: 60.16%] [G loss: 0.416707]\n",
      "epoch:15 step:14145 [D loss: 0.223783, acc.: 64.06%] [G loss: 0.466753]\n",
      "epoch:15 step:14146 [D loss: 0.234691, acc.: 60.16%] [G loss: 0.404004]\n",
      "epoch:15 step:14147 [D loss: 0.209930, acc.: 67.97%] [G loss: 0.430151]\n",
      "epoch:15 step:14148 [D loss: 0.208326, acc.: 66.41%] [G loss: 0.462114]\n",
      "epoch:15 step:14149 [D loss: 0.232738, acc.: 59.38%] [G loss: 0.461447]\n",
      "epoch:15 step:14150 [D loss: 0.221993, acc.: 68.75%] [G loss: 0.456276]\n",
      "epoch:15 step:14151 [D loss: 0.212409, acc.: 60.94%] [G loss: 0.423527]\n",
      "epoch:15 step:14152 [D loss: 0.200062, acc.: 69.53%] [G loss: 0.500040]\n",
      "epoch:15 step:14153 [D loss: 0.214781, acc.: 64.84%] [G loss: 0.484760]\n",
      "epoch:15 step:14154 [D loss: 0.238409, acc.: 57.81%] [G loss: 0.451927]\n",
      "epoch:15 step:14155 [D loss: 0.181521, acc.: 73.44%] [G loss: 0.452581]\n",
      "epoch:15 step:14156 [D loss: 0.235413, acc.: 57.03%] [G loss: 0.420265]\n",
      "epoch:15 step:14157 [D loss: 0.229080, acc.: 60.94%] [G loss: 0.429629]\n",
      "epoch:15 step:14158 [D loss: 0.232579, acc.: 59.38%] [G loss: 0.388930]\n",
      "epoch:15 step:14159 [D loss: 0.234442, acc.: 60.94%] [G loss: 0.446508]\n",
      "epoch:15 step:14160 [D loss: 0.244243, acc.: 58.59%] [G loss: 0.427627]\n",
      "epoch:15 step:14161 [D loss: 0.215628, acc.: 64.84%] [G loss: 0.433349]\n",
      "epoch:15 step:14162 [D loss: 0.183067, acc.: 78.12%] [G loss: 0.499410]\n",
      "epoch:15 step:14163 [D loss: 0.256329, acc.: 50.78%] [G loss: 0.454169]\n",
      "epoch:15 step:14164 [D loss: 0.261342, acc.: 50.00%] [G loss: 0.422445]\n",
      "epoch:15 step:14165 [D loss: 0.270518, acc.: 50.00%] [G loss: 0.413863]\n",
      "epoch:15 step:14166 [D loss: 0.188863, acc.: 71.09%] [G loss: 0.454718]\n",
      "epoch:15 step:14167 [D loss: 0.202954, acc.: 70.31%] [G loss: 0.448212]\n",
      "epoch:15 step:14168 [D loss: 0.233870, acc.: 60.16%] [G loss: 0.454833]\n",
      "epoch:15 step:14169 [D loss: 0.201839, acc.: 66.41%] [G loss: 0.504250]\n",
      "epoch:15 step:14170 [D loss: 0.237283, acc.: 58.59%] [G loss: 0.461215]\n",
      "epoch:15 step:14171 [D loss: 0.215376, acc.: 62.50%] [G loss: 0.483096]\n",
      "epoch:15 step:14172 [D loss: 0.228773, acc.: 60.16%] [G loss: 0.445235]\n",
      "epoch:15 step:14173 [D loss: 0.238725, acc.: 57.81%] [G loss: 0.418673]\n",
      "epoch:15 step:14174 [D loss: 0.171591, acc.: 76.56%] [G loss: 0.483752]\n",
      "epoch:15 step:14175 [D loss: 0.232254, acc.: 59.38%] [G loss: 0.484481]\n",
      "epoch:15 step:14176 [D loss: 0.215861, acc.: 67.19%] [G loss: 0.414698]\n",
      "epoch:15 step:14177 [D loss: 0.162050, acc.: 76.56%] [G loss: 0.495126]\n",
      "epoch:15 step:14178 [D loss: 0.237988, acc.: 67.97%] [G loss: 0.450588]\n",
      "epoch:15 step:14179 [D loss: 0.234684, acc.: 59.38%] [G loss: 0.463407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14180 [D loss: 0.253260, acc.: 62.50%] [G loss: 0.391803]\n",
      "epoch:15 step:14181 [D loss: 0.220367, acc.: 63.28%] [G loss: 0.434868]\n",
      "epoch:15 step:14182 [D loss: 0.219589, acc.: 60.94%] [G loss: 0.425679]\n",
      "epoch:15 step:14183 [D loss: 0.249840, acc.: 63.28%] [G loss: 0.408135]\n",
      "epoch:15 step:14184 [D loss: 0.208764, acc.: 67.19%] [G loss: 0.440934]\n",
      "epoch:15 step:14185 [D loss: 0.227381, acc.: 70.31%] [G loss: 0.416777]\n",
      "epoch:15 step:14186 [D loss: 0.224283, acc.: 61.72%] [G loss: 0.453744]\n",
      "epoch:15 step:14187 [D loss: 0.234169, acc.: 63.28%] [G loss: 0.432246]\n",
      "epoch:15 step:14188 [D loss: 0.215787, acc.: 61.72%] [G loss: 0.443336]\n",
      "epoch:15 step:14189 [D loss: 0.238873, acc.: 62.50%] [G loss: 0.467637]\n",
      "epoch:15 step:14190 [D loss: 0.235831, acc.: 60.94%] [G loss: 0.453317]\n",
      "epoch:15 step:14191 [D loss: 0.210853, acc.: 67.19%] [G loss: 0.494770]\n",
      "epoch:15 step:14192 [D loss: 0.280971, acc.: 54.69%] [G loss: 0.419243]\n",
      "epoch:15 step:14193 [D loss: 0.212641, acc.: 60.16%] [G loss: 0.447546]\n",
      "epoch:15 step:14194 [D loss: 0.235851, acc.: 58.59%] [G loss: 0.403493]\n",
      "epoch:15 step:14195 [D loss: 0.234659, acc.: 59.38%] [G loss: 0.406731]\n",
      "epoch:15 step:14196 [D loss: 0.244773, acc.: 56.25%] [G loss: 0.386360]\n",
      "epoch:15 step:14197 [D loss: 0.226478, acc.: 60.94%] [G loss: 0.440642]\n",
      "epoch:15 step:14198 [D loss: 0.224670, acc.: 57.03%] [G loss: 0.460662]\n",
      "epoch:15 step:14199 [D loss: 0.221705, acc.: 57.03%] [G loss: 0.428173]\n",
      "epoch:15 step:14200 [D loss: 0.220817, acc.: 62.50%] [G loss: 0.457403]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.824134\n",
      "FID: 11.980923\n",
      "0 = 11.82447122054097\n",
      "1 = 0.05092307606182235\n",
      "2 = 0.8781999945640564\n",
      "3 = 0.8611999750137329\n",
      "4 = 0.8952000141143799\n",
      "5 = 0.8915113806724548\n",
      "6 = 0.8611999750137329\n",
      "7 = 6.386349375766522\n",
      "8 = 0.07592184651105982\n",
      "9 = 0.6887999773025513\n",
      "10 = 0.6869000196456909\n",
      "11 = 0.6906999945640564\n",
      "12 = 0.6895201802253723\n",
      "13 = 0.6869000196456909\n",
      "14 = 7.824206829071045\n",
      "15 = 9.39132308959961\n",
      "16 = 0.14640365540981293\n",
      "17 = 7.824134349822998\n",
      "18 = 11.98092269897461\n",
      "epoch:15 step:14201 [D loss: 0.210013, acc.: 65.62%] [G loss: 0.427056]\n",
      "epoch:15 step:14202 [D loss: 0.266389, acc.: 55.47%] [G loss: 0.419016]\n",
      "epoch:15 step:14203 [D loss: 0.234956, acc.: 60.16%] [G loss: 0.452116]\n",
      "epoch:15 step:14204 [D loss: 0.209616, acc.: 64.84%] [G loss: 0.399577]\n",
      "epoch:15 step:14205 [D loss: 0.214899, acc.: 64.06%] [G loss: 0.465114]\n",
      "epoch:15 step:14206 [D loss: 0.230585, acc.: 66.41%] [G loss: 0.439873]\n",
      "epoch:15 step:14207 [D loss: 0.221950, acc.: 67.97%] [G loss: 0.429983]\n",
      "epoch:15 step:14208 [D loss: 0.246809, acc.: 60.16%] [G loss: 0.469269]\n",
      "epoch:15 step:14209 [D loss: 0.238383, acc.: 59.38%] [G loss: 0.426605]\n",
      "epoch:15 step:14210 [D loss: 0.207359, acc.: 64.84%] [G loss: 0.433768]\n",
      "epoch:15 step:14211 [D loss: 0.226864, acc.: 61.72%] [G loss: 0.421657]\n",
      "epoch:15 step:14212 [D loss: 0.216463, acc.: 64.84%] [G loss: 0.423766]\n",
      "epoch:15 step:14213 [D loss: 0.234673, acc.: 66.41%] [G loss: 0.420133]\n",
      "epoch:15 step:14214 [D loss: 0.220949, acc.: 63.28%] [G loss: 0.439692]\n",
      "epoch:15 step:14215 [D loss: 0.275342, acc.: 57.03%] [G loss: 0.429759]\n",
      "epoch:15 step:14216 [D loss: 0.242522, acc.: 52.34%] [G loss: 0.426213]\n",
      "epoch:15 step:14217 [D loss: 0.234011, acc.: 57.03%] [G loss: 0.469801]\n",
      "epoch:15 step:14218 [D loss: 0.259066, acc.: 60.16%] [G loss: 0.438612]\n",
      "epoch:15 step:14219 [D loss: 0.196129, acc.: 70.31%] [G loss: 0.443763]\n",
      "epoch:15 step:14220 [D loss: 0.223734, acc.: 67.19%] [G loss: 0.415833]\n",
      "epoch:15 step:14221 [D loss: 0.190122, acc.: 75.00%] [G loss: 0.455285]\n",
      "epoch:15 step:14222 [D loss: 0.210788, acc.: 65.62%] [G loss: 0.459909]\n",
      "epoch:15 step:14223 [D loss: 0.231423, acc.: 62.50%] [G loss: 0.430741]\n",
      "epoch:15 step:14224 [D loss: 0.238616, acc.: 58.59%] [G loss: 0.426895]\n",
      "epoch:15 step:14225 [D loss: 0.247160, acc.: 55.47%] [G loss: 0.437622]\n",
      "epoch:15 step:14226 [D loss: 0.211522, acc.: 68.75%] [G loss: 0.458339]\n",
      "epoch:15 step:14227 [D loss: 0.217163, acc.: 61.72%] [G loss: 0.427866]\n",
      "epoch:15 step:14228 [D loss: 0.218170, acc.: 66.41%] [G loss: 0.417269]\n",
      "epoch:15 step:14229 [D loss: 0.259078, acc.: 53.91%] [G loss: 0.445231]\n",
      "epoch:15 step:14230 [D loss: 0.222112, acc.: 64.06%] [G loss: 0.416413]\n",
      "epoch:15 step:14231 [D loss: 0.200407, acc.: 70.31%] [G loss: 0.433397]\n",
      "epoch:15 step:14232 [D loss: 0.205424, acc.: 72.66%] [G loss: 0.448773]\n",
      "epoch:15 step:14233 [D loss: 0.241834, acc.: 59.38%] [G loss: 0.416863]\n",
      "epoch:15 step:14234 [D loss: 0.250597, acc.: 55.47%] [G loss: 0.433921]\n",
      "epoch:15 step:14235 [D loss: 0.226087, acc.: 63.28%] [G loss: 0.430888]\n",
      "epoch:15 step:14236 [D loss: 0.243945, acc.: 60.16%] [G loss: 0.407812]\n",
      "epoch:15 step:14237 [D loss: 0.253571, acc.: 56.25%] [G loss: 0.446793]\n",
      "epoch:15 step:14238 [D loss: 0.230253, acc.: 64.06%] [G loss: 0.472919]\n",
      "epoch:15 step:14239 [D loss: 0.221098, acc.: 68.75%] [G loss: 0.491347]\n",
      "epoch:15 step:14240 [D loss: 0.221411, acc.: 66.41%] [G loss: 0.429559]\n",
      "epoch:15 step:14241 [D loss: 0.239883, acc.: 59.38%] [G loss: 0.427000]\n",
      "epoch:15 step:14242 [D loss: 0.225482, acc.: 59.38%] [G loss: 0.447275]\n",
      "epoch:15 step:14243 [D loss: 0.226813, acc.: 59.38%] [G loss: 0.437577]\n",
      "epoch:15 step:14244 [D loss: 0.216248, acc.: 65.62%] [G loss: 0.460683]\n",
      "epoch:15 step:14245 [D loss: 0.210877, acc.: 71.09%] [G loss: 0.449108]\n",
      "epoch:15 step:14246 [D loss: 0.234542, acc.: 57.81%] [G loss: 0.417464]\n",
      "epoch:15 step:14247 [D loss: 0.187878, acc.: 71.09%] [G loss: 0.422020]\n",
      "epoch:15 step:14248 [D loss: 0.225784, acc.: 61.72%] [G loss: 0.449441]\n",
      "epoch:15 step:14249 [D loss: 0.198699, acc.: 73.44%] [G loss: 0.462615]\n",
      "epoch:15 step:14250 [D loss: 0.208135, acc.: 69.53%] [G loss: 0.458144]\n",
      "epoch:15 step:14251 [D loss: 0.222691, acc.: 60.94%] [G loss: 0.430074]\n",
      "epoch:15 step:14252 [D loss: 0.217024, acc.: 65.62%] [G loss: 0.430179]\n",
      "epoch:15 step:14253 [D loss: 0.211012, acc.: 65.62%] [G loss: 0.443359]\n",
      "epoch:15 step:14254 [D loss: 0.229440, acc.: 59.38%] [G loss: 0.468038]\n",
      "epoch:15 step:14255 [D loss: 0.229704, acc.: 60.94%] [G loss: 0.426395]\n",
      "epoch:15 step:14256 [D loss: 0.236472, acc.: 55.47%] [G loss: 0.455923]\n",
      "epoch:15 step:14257 [D loss: 0.224583, acc.: 64.06%] [G loss: 0.433294]\n",
      "epoch:15 step:14258 [D loss: 0.249489, acc.: 60.94%] [G loss: 0.454484]\n",
      "epoch:15 step:14259 [D loss: 0.202465, acc.: 71.09%] [G loss: 0.424881]\n",
      "epoch:15 step:14260 [D loss: 0.209698, acc.: 65.62%] [G loss: 0.459882]\n",
      "epoch:15 step:14261 [D loss: 0.212212, acc.: 67.97%] [G loss: 0.456487]\n",
      "epoch:15 step:14262 [D loss: 0.196306, acc.: 67.97%] [G loss: 0.485138]\n",
      "epoch:15 step:14263 [D loss: 0.198999, acc.: 67.19%] [G loss: 0.457789]\n",
      "epoch:15 step:14264 [D loss: 0.199588, acc.: 67.97%] [G loss: 0.511882]\n",
      "epoch:15 step:14265 [D loss: 0.263717, acc.: 49.22%] [G loss: 0.430759]\n",
      "epoch:15 step:14266 [D loss: 0.216058, acc.: 64.84%] [G loss: 0.483398]\n",
      "epoch:15 step:14267 [D loss: 0.212187, acc.: 65.62%] [G loss: 0.431745]\n",
      "epoch:15 step:14268 [D loss: 0.235820, acc.: 58.59%] [G loss: 0.425175]\n",
      "epoch:15 step:14269 [D loss: 0.246014, acc.: 52.34%] [G loss: 0.439123]\n",
      "epoch:15 step:14270 [D loss: 0.258930, acc.: 51.56%] [G loss: 0.387815]\n",
      "epoch:15 step:14271 [D loss: 0.204492, acc.: 67.97%] [G loss: 0.452854]\n",
      "epoch:15 step:14272 [D loss: 0.229580, acc.: 61.72%] [G loss: 0.425230]\n",
      "epoch:15 step:14273 [D loss: 0.195522, acc.: 70.31%] [G loss: 0.411797]\n",
      "epoch:15 step:14274 [D loss: 0.184377, acc.: 71.09%] [G loss: 0.450657]\n",
      "epoch:15 step:14275 [D loss: 0.306726, acc.: 49.22%] [G loss: 0.438618]\n",
      "epoch:15 step:14276 [D loss: 0.184661, acc.: 72.66%] [G loss: 0.483700]\n",
      "epoch:15 step:14277 [D loss: 0.178319, acc.: 74.22%] [G loss: 0.484633]\n",
      "epoch:15 step:14278 [D loss: 0.203985, acc.: 71.09%] [G loss: 0.455803]\n",
      "epoch:15 step:14279 [D loss: 0.243426, acc.: 61.72%] [G loss: 0.463010]\n",
      "epoch:15 step:14280 [D loss: 0.243849, acc.: 56.25%] [G loss: 0.410234]\n",
      "epoch:15 step:14281 [D loss: 0.246841, acc.: 52.34%] [G loss: 0.442620]\n",
      "epoch:15 step:14282 [D loss: 0.245268, acc.: 57.81%] [G loss: 0.409907]\n",
      "epoch:15 step:14283 [D loss: 0.234090, acc.: 64.06%] [G loss: 0.414209]\n",
      "epoch:15 step:14284 [D loss: 0.190949, acc.: 72.66%] [G loss: 0.456557]\n",
      "epoch:15 step:14285 [D loss: 0.210812, acc.: 65.62%] [G loss: 0.424254]\n",
      "epoch:15 step:14286 [D loss: 0.152407, acc.: 81.25%] [G loss: 0.520156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14287 [D loss: 0.196118, acc.: 69.53%] [G loss: 0.496370]\n",
      "epoch:15 step:14288 [D loss: 0.240841, acc.: 58.59%] [G loss: 0.472072]\n",
      "epoch:15 step:14289 [D loss: 0.234649, acc.: 60.16%] [G loss: 0.526683]\n",
      "epoch:15 step:14290 [D loss: 0.228194, acc.: 63.28%] [G loss: 0.418028]\n",
      "epoch:15 step:14291 [D loss: 0.220617, acc.: 64.06%] [G loss: 0.415330]\n",
      "epoch:15 step:14292 [D loss: 0.228477, acc.: 62.50%] [G loss: 0.422826]\n",
      "epoch:15 step:14293 [D loss: 0.210189, acc.: 64.06%] [G loss: 0.445946]\n",
      "epoch:15 step:14294 [D loss: 0.244191, acc.: 54.69%] [G loss: 0.425947]\n",
      "epoch:15 step:14295 [D loss: 0.222222, acc.: 71.09%] [G loss: 0.455681]\n",
      "epoch:15 step:14296 [D loss: 0.210021, acc.: 67.97%] [G loss: 0.462135]\n",
      "epoch:15 step:14297 [D loss: 0.206718, acc.: 64.84%] [G loss: 0.490299]\n",
      "epoch:15 step:14298 [D loss: 0.217782, acc.: 66.41%] [G loss: 0.474607]\n",
      "epoch:15 step:14299 [D loss: 0.216115, acc.: 67.19%] [G loss: 0.456893]\n",
      "epoch:15 step:14300 [D loss: 0.211977, acc.: 66.41%] [G loss: 0.448517]\n",
      "epoch:15 step:14301 [D loss: 0.200195, acc.: 69.53%] [G loss: 0.478938]\n",
      "epoch:15 step:14302 [D loss: 0.235659, acc.: 58.59%] [G loss: 0.503110]\n",
      "epoch:15 step:14303 [D loss: 0.234909, acc.: 64.06%] [G loss: 0.467043]\n",
      "epoch:15 step:14304 [D loss: 0.286021, acc.: 49.22%] [G loss: 0.453970]\n",
      "epoch:15 step:14305 [D loss: 0.268912, acc.: 47.66%] [G loss: 0.448936]\n",
      "epoch:15 step:14306 [D loss: 0.253627, acc.: 53.91%] [G loss: 0.431292]\n",
      "epoch:15 step:14307 [D loss: 0.224716, acc.: 60.94%] [G loss: 0.414420]\n",
      "epoch:15 step:14308 [D loss: 0.220522, acc.: 64.84%] [G loss: 0.426679]\n",
      "epoch:15 step:14309 [D loss: 0.209656, acc.: 65.62%] [G loss: 0.458168]\n",
      "epoch:15 step:14310 [D loss: 0.234571, acc.: 60.16%] [G loss: 0.425318]\n",
      "epoch:15 step:14311 [D loss: 0.238893, acc.: 53.91%] [G loss: 0.437709]\n",
      "epoch:15 step:14312 [D loss: 0.242149, acc.: 63.28%] [G loss: 0.419441]\n",
      "epoch:15 step:14313 [D loss: 0.227941, acc.: 60.16%] [G loss: 0.392965]\n",
      "epoch:15 step:14314 [D loss: 0.220406, acc.: 64.06%] [G loss: 0.433084]\n",
      "epoch:15 step:14315 [D loss: 0.226285, acc.: 62.50%] [G loss: 0.445205]\n",
      "epoch:15 step:14316 [D loss: 0.189850, acc.: 67.19%] [G loss: 0.438190]\n",
      "epoch:15 step:14317 [D loss: 0.205940, acc.: 65.62%] [G loss: 0.442096]\n",
      "epoch:15 step:14318 [D loss: 0.260834, acc.: 55.47%] [G loss: 0.443219]\n",
      "epoch:15 step:14319 [D loss: 0.219851, acc.: 64.84%] [G loss: 0.448515]\n",
      "epoch:15 step:14320 [D loss: 0.225560, acc.: 62.50%] [G loss: 0.476884]\n",
      "epoch:15 step:14321 [D loss: 0.216665, acc.: 64.06%] [G loss: 0.410666]\n",
      "epoch:15 step:14322 [D loss: 0.245710, acc.: 58.59%] [G loss: 0.429005]\n",
      "epoch:15 step:14323 [D loss: 0.205591, acc.: 66.41%] [G loss: 0.459146]\n",
      "epoch:15 step:14324 [D loss: 0.215299, acc.: 67.97%] [G loss: 0.446634]\n",
      "epoch:15 step:14325 [D loss: 0.208906, acc.: 66.41%] [G loss: 0.497133]\n",
      "epoch:15 step:14326 [D loss: 0.204135, acc.: 67.97%] [G loss: 0.463722]\n",
      "epoch:15 step:14327 [D loss: 0.221739, acc.: 63.28%] [G loss: 0.451546]\n",
      "epoch:15 step:14328 [D loss: 0.225540, acc.: 64.06%] [G loss: 0.429306]\n",
      "epoch:15 step:14329 [D loss: 0.214536, acc.: 68.75%] [G loss: 0.456954]\n",
      "epoch:15 step:14330 [D loss: 0.208812, acc.: 65.62%] [G loss: 0.469937]\n",
      "epoch:15 step:14331 [D loss: 0.221333, acc.: 65.62%] [G loss: 0.459292]\n",
      "epoch:15 step:14332 [D loss: 0.235830, acc.: 58.59%] [G loss: 0.454498]\n",
      "epoch:15 step:14333 [D loss: 0.227490, acc.: 65.62%] [G loss: 0.464239]\n",
      "epoch:15 step:14334 [D loss: 0.235069, acc.: 60.16%] [G loss: 0.436710]\n",
      "epoch:15 step:14335 [D loss: 0.212315, acc.: 66.41%] [G loss: 0.489050]\n",
      "epoch:15 step:14336 [D loss: 0.256449, acc.: 56.25%] [G loss: 0.430506]\n",
      "epoch:15 step:14337 [D loss: 0.219924, acc.: 67.19%] [G loss: 0.407721]\n",
      "epoch:15 step:14338 [D loss: 0.200798, acc.: 68.75%] [G loss: 0.451495]\n",
      "epoch:15 step:14339 [D loss: 0.240324, acc.: 60.16%] [G loss: 0.419765]\n",
      "epoch:15 step:14340 [D loss: 0.219569, acc.: 64.84%] [G loss: 0.416897]\n",
      "epoch:15 step:14341 [D loss: 0.209004, acc.: 66.41%] [G loss: 0.456990]\n",
      "epoch:15 step:14342 [D loss: 0.247436, acc.: 59.38%] [G loss: 0.411493]\n",
      "epoch:15 step:14343 [D loss: 0.229384, acc.: 64.84%] [G loss: 0.417944]\n",
      "epoch:15 step:14344 [D loss: 0.233384, acc.: 65.62%] [G loss: 0.428511]\n",
      "epoch:15 step:14345 [D loss: 0.215684, acc.: 65.62%] [G loss: 0.424357]\n",
      "epoch:15 step:14346 [D loss: 0.245156, acc.: 57.81%] [G loss: 0.422974]\n",
      "epoch:15 step:14347 [D loss: 0.225462, acc.: 61.72%] [G loss: 0.414497]\n",
      "epoch:15 step:14348 [D loss: 0.252396, acc.: 52.34%] [G loss: 0.404573]\n",
      "epoch:15 step:14349 [D loss: 0.235102, acc.: 60.94%] [G loss: 0.421363]\n",
      "epoch:15 step:14350 [D loss: 0.239511, acc.: 53.12%] [G loss: 0.410061]\n",
      "epoch:15 step:14351 [D loss: 0.209298, acc.: 66.41%] [G loss: 0.419894]\n",
      "epoch:15 step:14352 [D loss: 0.224884, acc.: 60.94%] [G loss: 0.458816]\n",
      "epoch:15 step:14353 [D loss: 0.206402, acc.: 67.19%] [G loss: 0.466887]\n",
      "epoch:15 step:14354 [D loss: 0.209665, acc.: 71.09%] [G loss: 0.496311]\n",
      "epoch:15 step:14355 [D loss: 0.215948, acc.: 65.62%] [G loss: 0.446777]\n",
      "epoch:15 step:14356 [D loss: 0.267976, acc.: 61.72%] [G loss: 0.449222]\n",
      "epoch:15 step:14357 [D loss: 0.243735, acc.: 60.16%] [G loss: 0.449626]\n",
      "epoch:15 step:14358 [D loss: 0.211962, acc.: 68.75%] [G loss: 0.413035]\n",
      "epoch:15 step:14359 [D loss: 0.254551, acc.: 56.25%] [G loss: 0.441617]\n",
      "epoch:15 step:14360 [D loss: 0.207858, acc.: 67.19%] [G loss: 0.466344]\n",
      "epoch:15 step:14361 [D loss: 0.229920, acc.: 60.16%] [G loss: 0.421725]\n",
      "epoch:15 step:14362 [D loss: 0.222776, acc.: 69.53%] [G loss: 0.443960]\n",
      "epoch:15 step:14363 [D loss: 0.249866, acc.: 53.91%] [G loss: 0.416755]\n",
      "epoch:15 step:14364 [D loss: 0.209762, acc.: 68.75%] [G loss: 0.437743]\n",
      "epoch:15 step:14365 [D loss: 0.210213, acc.: 67.97%] [G loss: 0.438683]\n",
      "epoch:15 step:14366 [D loss: 0.206945, acc.: 67.97%] [G loss: 0.452108]\n",
      "epoch:15 step:14367 [D loss: 0.188539, acc.: 68.75%] [G loss: 0.489020]\n",
      "epoch:15 step:14368 [D loss: 0.206387, acc.: 64.06%] [G loss: 0.472664]\n",
      "epoch:15 step:14369 [D loss: 0.184443, acc.: 75.00%] [G loss: 0.501285]\n",
      "epoch:15 step:14370 [D loss: 0.194383, acc.: 68.75%] [G loss: 0.502527]\n",
      "epoch:15 step:14371 [D loss: 0.274138, acc.: 52.34%] [G loss: 0.403281]\n",
      "epoch:15 step:14372 [D loss: 0.227994, acc.: 62.50%] [G loss: 0.420148]\n",
      "epoch:15 step:14373 [D loss: 0.229357, acc.: 59.38%] [G loss: 0.422201]\n",
      "epoch:15 step:14374 [D loss: 0.204026, acc.: 66.41%] [G loss: 0.463532]\n",
      "epoch:15 step:14375 [D loss: 0.211261, acc.: 67.19%] [G loss: 0.484417]\n",
      "epoch:15 step:14376 [D loss: 0.233524, acc.: 60.16%] [G loss: 0.398163]\n",
      "epoch:15 step:14377 [D loss: 0.225925, acc.: 60.94%] [G loss: 0.465779]\n",
      "epoch:15 step:14378 [D loss: 0.252618, acc.: 58.59%] [G loss: 0.434316]\n",
      "epoch:15 step:14379 [D loss: 0.213139, acc.: 66.41%] [G loss: 0.452016]\n",
      "epoch:15 step:14380 [D loss: 0.222668, acc.: 65.62%] [G loss: 0.443134]\n",
      "epoch:15 step:14381 [D loss: 0.216441, acc.: 67.97%] [G loss: 0.447007]\n",
      "epoch:15 step:14382 [D loss: 0.198963, acc.: 71.88%] [G loss: 0.445854]\n",
      "epoch:15 step:14383 [D loss: 0.195316, acc.: 67.97%] [G loss: 0.448001]\n",
      "epoch:15 step:14384 [D loss: 0.244298, acc.: 57.81%] [G loss: 0.438930]\n",
      "epoch:15 step:14385 [D loss: 0.227659, acc.: 69.53%] [G loss: 0.445487]\n",
      "epoch:15 step:14386 [D loss: 0.220341, acc.: 64.84%] [G loss: 0.463153]\n",
      "epoch:15 step:14387 [D loss: 0.195726, acc.: 69.53%] [G loss: 0.467477]\n",
      "epoch:15 step:14388 [D loss: 0.201602, acc.: 68.75%] [G loss: 0.483052]\n",
      "epoch:15 step:14389 [D loss: 0.213350, acc.: 69.53%] [G loss: 0.492313]\n",
      "epoch:15 step:14390 [D loss: 0.207877, acc.: 60.94%] [G loss: 0.485780]\n",
      "epoch:15 step:14391 [D loss: 0.208489, acc.: 65.62%] [G loss: 0.463781]\n",
      "epoch:15 step:14392 [D loss: 0.231655, acc.: 65.62%] [G loss: 0.417938]\n",
      "epoch:15 step:14393 [D loss: 0.223516, acc.: 64.06%] [G loss: 0.452210]\n",
      "epoch:15 step:14394 [D loss: 0.235365, acc.: 60.16%] [G loss: 0.452745]\n",
      "epoch:15 step:14395 [D loss: 0.211566, acc.: 67.97%] [G loss: 0.497763]\n",
      "epoch:15 step:14396 [D loss: 0.263196, acc.: 55.47%] [G loss: 0.432837]\n",
      "epoch:15 step:14397 [D loss: 0.253548, acc.: 50.00%] [G loss: 0.451876]\n",
      "epoch:15 step:14398 [D loss: 0.199256, acc.: 72.66%] [G loss: 0.442082]\n",
      "epoch:15 step:14399 [D loss: 0.200801, acc.: 71.09%] [G loss: 0.471423]\n",
      "epoch:15 step:14400 [D loss: 0.206501, acc.: 64.06%] [G loss: 0.451183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.964293\n",
      "FID: 8.005226\n",
      "0 = 11.778996272087081\n",
      "1 = 0.04348034179932168\n",
      "2 = 0.8817999958992004\n",
      "3 = 0.8636999726295471\n",
      "4 = 0.8999000191688538\n",
      "5 = 0.8961402773857117\n",
      "6 = 0.8636999726295471\n",
      "7 = 6.045099274921447\n",
      "8 = 0.06219024192094405\n",
      "9 = 0.6963499784469604\n",
      "10 = 0.6926000118255615\n",
      "11 = 0.7001000046730042\n",
      "12 = 0.697833776473999\n",
      "13 = 0.6926000118255615\n",
      "14 = 7.964364528656006\n",
      "15 = 9.57384967803955\n",
      "16 = 0.10016966611146927\n",
      "17 = 7.964293003082275\n",
      "18 = 8.005226135253906\n",
      "epoch:15 step:14401 [D loss: 0.201640, acc.: 71.88%] [G loss: 0.463793]\n",
      "epoch:15 step:14402 [D loss: 0.165042, acc.: 82.03%] [G loss: 0.531256]\n",
      "epoch:15 step:14403 [D loss: 0.269813, acc.: 55.47%] [G loss: 0.498453]\n",
      "epoch:15 step:14404 [D loss: 0.269972, acc.: 52.34%] [G loss: 0.431120]\n",
      "epoch:15 step:14405 [D loss: 0.223988, acc.: 65.62%] [G loss: 0.387448]\n",
      "epoch:15 step:14406 [D loss: 0.224609, acc.: 62.50%] [G loss: 0.412036]\n",
      "epoch:15 step:14407 [D loss: 0.230316, acc.: 62.50%] [G loss: 0.453502]\n",
      "epoch:15 step:14408 [D loss: 0.207232, acc.: 65.62%] [G loss: 0.473824]\n",
      "epoch:15 step:14409 [D loss: 0.195324, acc.: 71.88%] [G loss: 0.506804]\n",
      "epoch:15 step:14410 [D loss: 0.232293, acc.: 61.72%] [G loss: 0.483109]\n",
      "epoch:15 step:14411 [D loss: 0.256144, acc.: 54.69%] [G loss: 0.421441]\n",
      "epoch:15 step:14412 [D loss: 0.199701, acc.: 75.78%] [G loss: 0.461640]\n",
      "epoch:15 step:14413 [D loss: 0.228267, acc.: 60.16%] [G loss: 0.453288]\n",
      "epoch:15 step:14414 [D loss: 0.206965, acc.: 64.84%] [G loss: 0.459347]\n",
      "epoch:15 step:14415 [D loss: 0.202736, acc.: 64.84%] [G loss: 0.469492]\n",
      "epoch:15 step:14416 [D loss: 0.213302, acc.: 62.50%] [G loss: 0.445689]\n",
      "epoch:15 step:14417 [D loss: 0.220062, acc.: 65.62%] [G loss: 0.443309]\n",
      "epoch:15 step:14418 [D loss: 0.225132, acc.: 60.16%] [G loss: 0.472113]\n",
      "epoch:15 step:14419 [D loss: 0.215389, acc.: 67.97%] [G loss: 0.437805]\n",
      "epoch:15 step:14420 [D loss: 0.231814, acc.: 57.03%] [G loss: 0.432721]\n",
      "epoch:15 step:14421 [D loss: 0.212976, acc.: 64.06%] [G loss: 0.453673]\n",
      "epoch:15 step:14422 [D loss: 0.202087, acc.: 67.19%] [G loss: 0.458837]\n",
      "epoch:15 step:14423 [D loss: 0.208852, acc.: 64.06%] [G loss: 0.469143]\n",
      "epoch:15 step:14424 [D loss: 0.240764, acc.: 54.69%] [G loss: 0.438594]\n",
      "epoch:15 step:14425 [D loss: 0.231570, acc.: 65.62%] [G loss: 0.421299]\n",
      "epoch:15 step:14426 [D loss: 0.196415, acc.: 69.53%] [G loss: 0.462015]\n",
      "epoch:15 step:14427 [D loss: 0.202681, acc.: 67.19%] [G loss: 0.490608]\n",
      "epoch:15 step:14428 [D loss: 0.231641, acc.: 62.50%] [G loss: 0.484850]\n",
      "epoch:15 step:14429 [D loss: 0.186975, acc.: 72.66%] [G loss: 0.451029]\n",
      "epoch:15 step:14430 [D loss: 0.237434, acc.: 58.59%] [G loss: 0.436081]\n",
      "epoch:15 step:14431 [D loss: 0.257814, acc.: 55.47%] [G loss: 0.414628]\n",
      "epoch:15 step:14432 [D loss: 0.238642, acc.: 61.72%] [G loss: 0.435521]\n",
      "epoch:15 step:14433 [D loss: 0.224567, acc.: 62.50%] [G loss: 0.447767]\n",
      "epoch:15 step:14434 [D loss: 0.219370, acc.: 64.06%] [G loss: 0.443396]\n",
      "epoch:15 step:14435 [D loss: 0.221470, acc.: 61.72%] [G loss: 0.446115]\n",
      "epoch:15 step:14436 [D loss: 0.197560, acc.: 68.75%] [G loss: 0.404779]\n",
      "epoch:15 step:14437 [D loss: 0.217185, acc.: 65.62%] [G loss: 0.434796]\n",
      "epoch:15 step:14438 [D loss: 0.207135, acc.: 65.62%] [G loss: 0.418956]\n",
      "epoch:15 step:14439 [D loss: 0.210596, acc.: 67.97%] [G loss: 0.436063]\n",
      "epoch:15 step:14440 [D loss: 0.210149, acc.: 67.97%] [G loss: 0.473814]\n",
      "epoch:15 step:14441 [D loss: 0.254567, acc.: 53.91%] [G loss: 0.433139]\n",
      "epoch:15 step:14442 [D loss: 0.207427, acc.: 71.09%] [G loss: 0.484201]\n",
      "epoch:15 step:14443 [D loss: 0.228211, acc.: 60.94%] [G loss: 0.477092]\n",
      "epoch:15 step:14444 [D loss: 0.211597, acc.: 64.06%] [G loss: 0.438909]\n",
      "epoch:15 step:14445 [D loss: 0.246271, acc.: 57.03%] [G loss: 0.430029]\n",
      "epoch:15 step:14446 [D loss: 0.243019, acc.: 55.47%] [G loss: 0.423732]\n",
      "epoch:15 step:14447 [D loss: 0.220914, acc.: 66.41%] [G loss: 0.452453]\n",
      "epoch:15 step:14448 [D loss: 0.257394, acc.: 54.69%] [G loss: 0.442019]\n",
      "epoch:15 step:14449 [D loss: 0.216088, acc.: 67.19%] [G loss: 0.452287]\n",
      "epoch:15 step:14450 [D loss: 0.232392, acc.: 57.81%] [G loss: 0.438010]\n",
      "epoch:15 step:14451 [D loss: 0.268839, acc.: 51.56%] [G loss: 0.455617]\n",
      "epoch:15 step:14452 [D loss: 0.231059, acc.: 59.38%] [G loss: 0.441031]\n",
      "epoch:15 step:14453 [D loss: 0.180150, acc.: 73.44%] [G loss: 0.468565]\n",
      "epoch:15 step:14454 [D loss: 0.203169, acc.: 69.53%] [G loss: 0.447112]\n",
      "epoch:15 step:14455 [D loss: 0.261909, acc.: 55.47%] [G loss: 0.416221]\n",
      "epoch:15 step:14456 [D loss: 0.223232, acc.: 66.41%] [G loss: 0.444060]\n",
      "epoch:15 step:14457 [D loss: 0.220489, acc.: 67.97%] [G loss: 0.430469]\n",
      "epoch:15 step:14458 [D loss: 0.215853, acc.: 64.84%] [G loss: 0.464086]\n",
      "epoch:15 step:14459 [D loss: 0.240092, acc.: 61.72%] [G loss: 0.440750]\n",
      "epoch:15 step:14460 [D loss: 0.202648, acc.: 70.31%] [G loss: 0.463702]\n",
      "epoch:15 step:14461 [D loss: 0.220911, acc.: 63.28%] [G loss: 0.506648]\n",
      "epoch:15 step:14462 [D loss: 0.239680, acc.: 57.03%] [G loss: 0.457905]\n",
      "epoch:15 step:14463 [D loss: 0.239461, acc.: 62.50%] [G loss: 0.420498]\n",
      "epoch:15 step:14464 [D loss: 0.226696, acc.: 65.62%] [G loss: 0.473335]\n",
      "epoch:15 step:14465 [D loss: 0.241853, acc.: 63.28%] [G loss: 0.428992]\n",
      "epoch:15 step:14466 [D loss: 0.256298, acc.: 52.34%] [G loss: 0.387523]\n",
      "epoch:15 step:14467 [D loss: 0.228550, acc.: 60.16%] [G loss: 0.409620]\n",
      "epoch:15 step:14468 [D loss: 0.250656, acc.: 60.94%] [G loss: 0.419394]\n",
      "epoch:15 step:14469 [D loss: 0.221336, acc.: 67.97%] [G loss: 0.466596]\n",
      "epoch:15 step:14470 [D loss: 0.210739, acc.: 63.28%] [G loss: 0.447087]\n",
      "epoch:15 step:14471 [D loss: 0.201772, acc.: 74.22%] [G loss: 0.459803]\n",
      "epoch:15 step:14472 [D loss: 0.228074, acc.: 60.94%] [G loss: 0.457012]\n",
      "epoch:15 step:14473 [D loss: 0.240320, acc.: 55.47%] [G loss: 0.469249]\n",
      "epoch:15 step:14474 [D loss: 0.245619, acc.: 60.16%] [G loss: 0.402638]\n",
      "epoch:15 step:14475 [D loss: 0.240927, acc.: 60.16%] [G loss: 0.479402]\n",
      "epoch:15 step:14476 [D loss: 0.262175, acc.: 55.47%] [G loss: 0.436664]\n",
      "epoch:15 step:14477 [D loss: 0.247591, acc.: 60.16%] [G loss: 0.394111]\n",
      "epoch:15 step:14478 [D loss: 0.239222, acc.: 55.47%] [G loss: 0.437937]\n",
      "epoch:15 step:14479 [D loss: 0.213026, acc.: 67.19%] [G loss: 0.451930]\n",
      "epoch:15 step:14480 [D loss: 0.220477, acc.: 65.62%] [G loss: 0.414845]\n",
      "epoch:15 step:14481 [D loss: 0.220100, acc.: 62.50%] [G loss: 0.400216]\n",
      "epoch:15 step:14482 [D loss: 0.200922, acc.: 66.41%] [G loss: 0.465833]\n",
      "epoch:15 step:14483 [D loss: 0.212026, acc.: 61.72%] [G loss: 0.435743]\n",
      "epoch:15 step:14484 [D loss: 0.191621, acc.: 75.00%] [G loss: 0.467962]\n",
      "epoch:15 step:14485 [D loss: 0.213520, acc.: 68.75%] [G loss: 0.500205]\n",
      "epoch:15 step:14486 [D loss: 0.240403, acc.: 57.03%] [G loss: 0.435024]\n",
      "epoch:15 step:14487 [D loss: 0.237127, acc.: 57.03%] [G loss: 0.431547]\n",
      "epoch:15 step:14488 [D loss: 0.235804, acc.: 59.38%] [G loss: 0.438794]\n",
      "epoch:15 step:14489 [D loss: 0.204676, acc.: 72.66%] [G loss: 0.448897]\n",
      "epoch:15 step:14490 [D loss: 0.224632, acc.: 64.06%] [G loss: 0.461156]\n",
      "epoch:15 step:14491 [D loss: 0.203207, acc.: 66.41%] [G loss: 0.464712]\n",
      "epoch:15 step:14492 [D loss: 0.282729, acc.: 50.78%] [G loss: 0.388071]\n",
      "epoch:15 step:14493 [D loss: 0.271332, acc.: 48.44%] [G loss: 0.416661]\n",
      "epoch:15 step:14494 [D loss: 0.214265, acc.: 69.53%] [G loss: 0.480720]\n",
      "epoch:15 step:14495 [D loss: 0.229755, acc.: 66.41%] [G loss: 0.428768]\n",
      "epoch:15 step:14496 [D loss: 0.228993, acc.: 60.94%] [G loss: 0.440703]\n",
      "epoch:15 step:14497 [D loss: 0.225811, acc.: 63.28%] [G loss: 0.468321]\n",
      "epoch:15 step:14498 [D loss: 0.220915, acc.: 64.84%] [G loss: 0.394477]\n",
      "epoch:15 step:14499 [D loss: 0.239780, acc.: 60.94%] [G loss: 0.430624]\n",
      "epoch:15 step:14500 [D loss: 0.244205, acc.: 58.59%] [G loss: 0.425303]\n",
      "epoch:15 step:14501 [D loss: 0.226837, acc.: 60.16%] [G loss: 0.479474]\n",
      "epoch:15 step:14502 [D loss: 0.210080, acc.: 65.62%] [G loss: 0.474659]\n",
      "epoch:15 step:14503 [D loss: 0.254053, acc.: 54.69%] [G loss: 0.448912]\n",
      "epoch:15 step:14504 [D loss: 0.220220, acc.: 67.19%] [G loss: 0.438657]\n",
      "epoch:15 step:14505 [D loss: 0.212508, acc.: 67.97%] [G loss: 0.443781]\n",
      "epoch:15 step:14506 [D loss: 0.198828, acc.: 70.31%] [G loss: 0.457692]\n",
      "epoch:15 step:14507 [D loss: 0.218193, acc.: 64.06%] [G loss: 0.457588]\n",
      "epoch:15 step:14508 [D loss: 0.210985, acc.: 66.41%] [G loss: 0.479726]\n",
      "epoch:15 step:14509 [D loss: 0.221693, acc.: 63.28%] [G loss: 0.489388]\n",
      "epoch:15 step:14510 [D loss: 0.228118, acc.: 59.38%] [G loss: 0.505064]\n",
      "epoch:15 step:14511 [D loss: 0.224005, acc.: 65.62%] [G loss: 0.449203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14512 [D loss: 0.207408, acc.: 67.19%] [G loss: 0.538640]\n",
      "epoch:15 step:14513 [D loss: 0.289767, acc.: 50.00%] [G loss: 0.452419]\n",
      "epoch:15 step:14514 [D loss: 0.233813, acc.: 59.38%] [G loss: 0.467293]\n",
      "epoch:15 step:14515 [D loss: 0.251652, acc.: 57.81%] [G loss: 0.404044]\n",
      "epoch:15 step:14516 [D loss: 0.228061, acc.: 65.62%] [G loss: 0.392972]\n",
      "epoch:15 step:14517 [D loss: 0.243121, acc.: 63.28%] [G loss: 0.442123]\n",
      "epoch:15 step:14518 [D loss: 0.254875, acc.: 56.25%] [G loss: 0.374692]\n",
      "epoch:15 step:14519 [D loss: 0.240891, acc.: 60.94%] [G loss: 0.397411]\n",
      "epoch:15 step:14520 [D loss: 0.240114, acc.: 57.81%] [G loss: 0.408983]\n",
      "epoch:15 step:14521 [D loss: 0.222402, acc.: 62.50%] [G loss: 0.405780]\n",
      "epoch:15 step:14522 [D loss: 0.227771, acc.: 65.62%] [G loss: 0.436283]\n",
      "epoch:15 step:14523 [D loss: 0.218372, acc.: 64.06%] [G loss: 0.425035]\n",
      "epoch:15 step:14524 [D loss: 0.228538, acc.: 63.28%] [G loss: 0.439371]\n",
      "epoch:15 step:14525 [D loss: 0.211739, acc.: 70.31%] [G loss: 0.449825]\n",
      "epoch:15 step:14526 [D loss: 0.194709, acc.: 68.75%] [G loss: 0.489983]\n",
      "epoch:15 step:14527 [D loss: 0.229230, acc.: 65.62%] [G loss: 0.510032]\n",
      "epoch:15 step:14528 [D loss: 0.271301, acc.: 50.78%] [G loss: 0.424049]\n",
      "epoch:15 step:14529 [D loss: 0.214796, acc.: 66.41%] [G loss: 0.453169]\n",
      "epoch:15 step:14530 [D loss: 0.195019, acc.: 72.66%] [G loss: 0.447409]\n",
      "epoch:15 step:14531 [D loss: 0.231410, acc.: 66.41%] [G loss: 0.476138]\n",
      "epoch:15 step:14532 [D loss: 0.252529, acc.: 54.69%] [G loss: 0.430005]\n",
      "epoch:15 step:14533 [D loss: 0.246866, acc.: 57.81%] [G loss: 0.436897]\n",
      "epoch:15 step:14534 [D loss: 0.235634, acc.: 60.16%] [G loss: 0.415971]\n",
      "epoch:15 step:14535 [D loss: 0.213759, acc.: 66.41%] [G loss: 0.443284]\n",
      "epoch:15 step:14536 [D loss: 0.194854, acc.: 71.09%] [G loss: 0.431135]\n",
      "epoch:15 step:14537 [D loss: 0.274171, acc.: 47.66%] [G loss: 0.410209]\n",
      "epoch:15 step:14538 [D loss: 0.210317, acc.: 66.41%] [G loss: 0.438090]\n",
      "epoch:15 step:14539 [D loss: 0.200939, acc.: 70.31%] [G loss: 0.454653]\n",
      "epoch:15 step:14540 [D loss: 0.197069, acc.: 73.44%] [G loss: 0.440951]\n",
      "epoch:15 step:14541 [D loss: 0.242196, acc.: 57.03%] [G loss: 0.428278]\n",
      "epoch:15 step:14542 [D loss: 0.227282, acc.: 60.94%] [G loss: 0.441739]\n",
      "epoch:15 step:14543 [D loss: 0.201245, acc.: 66.41%] [G loss: 0.437864]\n",
      "epoch:15 step:14544 [D loss: 0.227245, acc.: 60.94%] [G loss: 0.442952]\n",
      "epoch:15 step:14545 [D loss: 0.238210, acc.: 58.59%] [G loss: 0.413068]\n",
      "epoch:15 step:14546 [D loss: 0.219606, acc.: 64.84%] [G loss: 0.433584]\n",
      "epoch:15 step:14547 [D loss: 0.216338, acc.: 67.19%] [G loss: 0.440918]\n",
      "epoch:15 step:14548 [D loss: 0.232018, acc.: 60.16%] [G loss: 0.445435]\n",
      "epoch:15 step:14549 [D loss: 0.217359, acc.: 64.84%] [G loss: 0.453244]\n",
      "epoch:15 step:14550 [D loss: 0.217389, acc.: 67.19%] [G loss: 0.466672]\n",
      "epoch:15 step:14551 [D loss: 0.212905, acc.: 67.19%] [G loss: 0.473783]\n",
      "epoch:15 step:14552 [D loss: 0.208963, acc.: 67.19%] [G loss: 0.481850]\n",
      "epoch:15 step:14553 [D loss: 0.203755, acc.: 72.66%] [G loss: 0.485610]\n",
      "epoch:15 step:14554 [D loss: 0.191391, acc.: 73.44%] [G loss: 0.459029]\n",
      "epoch:15 step:14555 [D loss: 0.257643, acc.: 53.91%] [G loss: 0.442456]\n",
      "epoch:15 step:14556 [D loss: 0.293921, acc.: 50.78%] [G loss: 0.397101]\n",
      "epoch:15 step:14557 [D loss: 0.240459, acc.: 56.25%] [G loss: 0.419725]\n",
      "epoch:15 step:14558 [D loss: 0.213771, acc.: 64.84%] [G loss: 0.431571]\n",
      "epoch:15 step:14559 [D loss: 0.207807, acc.: 67.19%] [G loss: 0.500918]\n",
      "epoch:15 step:14560 [D loss: 0.186382, acc.: 70.31%] [G loss: 0.514152]\n",
      "epoch:15 step:14561 [D loss: 0.229841, acc.: 58.59%] [G loss: 0.449900]\n",
      "epoch:15 step:14562 [D loss: 0.197031, acc.: 67.19%] [G loss: 0.484431]\n",
      "epoch:15 step:14563 [D loss: 0.199093, acc.: 68.75%] [G loss: 0.457227]\n",
      "epoch:15 step:14564 [D loss: 0.261072, acc.: 53.91%] [G loss: 0.423082]\n",
      "epoch:15 step:14565 [D loss: 0.243638, acc.: 56.25%] [G loss: 0.422043]\n",
      "epoch:15 step:14566 [D loss: 0.256163, acc.: 48.44%] [G loss: 0.398038]\n",
      "epoch:15 step:14567 [D loss: 0.218100, acc.: 64.06%] [G loss: 0.416267]\n",
      "epoch:15 step:14568 [D loss: 0.226236, acc.: 60.94%] [G loss: 0.420281]\n",
      "epoch:15 step:14569 [D loss: 0.247716, acc.: 57.81%] [G loss: 0.442488]\n",
      "epoch:15 step:14570 [D loss: 0.191985, acc.: 72.66%] [G loss: 0.503005]\n",
      "epoch:15 step:14571 [D loss: 0.203479, acc.: 70.31%] [G loss: 0.496943]\n",
      "epoch:15 step:14572 [D loss: 0.235942, acc.: 61.72%] [G loss: 0.444450]\n",
      "epoch:15 step:14573 [D loss: 0.250982, acc.: 59.38%] [G loss: 0.442393]\n",
      "epoch:15 step:14574 [D loss: 0.209806, acc.: 64.06%] [G loss: 0.486242]\n",
      "epoch:15 step:14575 [D loss: 0.212119, acc.: 65.62%] [G loss: 0.457917]\n",
      "epoch:15 step:14576 [D loss: 0.184974, acc.: 74.22%] [G loss: 0.442397]\n",
      "epoch:15 step:14577 [D loss: 0.218068, acc.: 67.97%] [G loss: 0.475610]\n",
      "epoch:15 step:14578 [D loss: 0.211755, acc.: 62.50%] [G loss: 0.466646]\n",
      "epoch:15 step:14579 [D loss: 0.237438, acc.: 61.72%] [G loss: 0.462219]\n",
      "epoch:15 step:14580 [D loss: 0.210419, acc.: 70.31%] [G loss: 0.444577]\n",
      "epoch:15 step:14581 [D loss: 0.196052, acc.: 69.53%] [G loss: 0.471417]\n",
      "epoch:15 step:14582 [D loss: 0.249837, acc.: 59.38%] [G loss: 0.473910]\n",
      "epoch:15 step:14583 [D loss: 0.265853, acc.: 53.12%] [G loss: 0.432455]\n",
      "epoch:15 step:14584 [D loss: 0.262821, acc.: 51.56%] [G loss: 0.409100]\n",
      "epoch:15 step:14585 [D loss: 0.213621, acc.: 67.97%] [G loss: 0.485302]\n",
      "epoch:15 step:14586 [D loss: 0.249017, acc.: 58.59%] [G loss: 0.422223]\n",
      "epoch:15 step:14587 [D loss: 0.229109, acc.: 58.59%] [G loss: 0.413332]\n",
      "epoch:15 step:14588 [D loss: 0.209525, acc.: 64.84%] [G loss: 0.436499]\n",
      "epoch:15 step:14589 [D loss: 0.185041, acc.: 72.66%] [G loss: 0.483784]\n",
      "epoch:15 step:14590 [D loss: 0.267986, acc.: 55.47%] [G loss: 0.418479]\n",
      "epoch:15 step:14591 [D loss: 0.219797, acc.: 67.19%] [G loss: 0.446706]\n",
      "epoch:15 step:14592 [D loss: 0.215470, acc.: 62.50%] [G loss: 0.465475]\n",
      "epoch:15 step:14593 [D loss: 0.241246, acc.: 60.16%] [G loss: 0.444779]\n",
      "epoch:15 step:14594 [D loss: 0.226653, acc.: 60.94%] [G loss: 0.430443]\n",
      "epoch:15 step:14595 [D loss: 0.241251, acc.: 57.81%] [G loss: 0.464906]\n",
      "epoch:15 step:14596 [D loss: 0.218367, acc.: 66.41%] [G loss: 0.481478]\n",
      "epoch:15 step:14597 [D loss: 0.262722, acc.: 51.56%] [G loss: 0.432064]\n",
      "epoch:15 step:14598 [D loss: 0.243332, acc.: 57.81%] [G loss: 0.404406]\n",
      "epoch:15 step:14599 [D loss: 0.228902, acc.: 59.38%] [G loss: 0.382038]\n",
      "epoch:15 step:14600 [D loss: 0.199763, acc.: 70.31%] [G loss: 0.453133]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.970685\n",
      "FID: 7.535193\n",
      "0 = 11.820489637565647\n",
      "1 = 0.04637258213865757\n",
      "2 = 0.8732500076293945\n",
      "3 = 0.8621000051498413\n",
      "4 = 0.8844000101089478\n",
      "5 = 0.8817633390426636\n",
      "6 = 0.8621000051498413\n",
      "7 = 5.924316766512378\n",
      "8 = 0.06036250438077954\n",
      "9 = 0.6774500012397766\n",
      "10 = 0.6780999898910522\n",
      "11 = 0.676800012588501\n",
      "12 = 0.6772196292877197\n",
      "13 = 0.6780999898910522\n",
      "14 = 7.970756530761719\n",
      "15 = 9.53944206237793\n",
      "16 = 0.10944050550460815\n",
      "17 = 7.970685005187988\n",
      "18 = 7.535192966461182\n",
      "epoch:15 step:14601 [D loss: 0.247102, acc.: 57.03%] [G loss: 0.437354]\n",
      "epoch:15 step:14602 [D loss: 0.192949, acc.: 68.75%] [G loss: 0.480900]\n",
      "epoch:15 step:14603 [D loss: 0.210901, acc.: 64.84%] [G loss: 0.422149]\n",
      "epoch:15 step:14604 [D loss: 0.201019, acc.: 69.53%] [G loss: 0.475647]\n",
      "epoch:15 step:14605 [D loss: 0.183168, acc.: 74.22%] [G loss: 0.494753]\n",
      "epoch:15 step:14606 [D loss: 0.185491, acc.: 77.34%] [G loss: 0.466183]\n",
      "epoch:15 step:14607 [D loss: 0.200797, acc.: 67.97%] [G loss: 0.477861]\n",
      "epoch:15 step:14608 [D loss: 0.235294, acc.: 58.59%] [G loss: 0.461500]\n",
      "epoch:15 step:14609 [D loss: 0.190441, acc.: 70.31%] [G loss: 0.458559]\n",
      "epoch:15 step:14610 [D loss: 0.195894, acc.: 69.53%] [G loss: 0.462976]\n",
      "epoch:15 step:14611 [D loss: 0.216493, acc.: 67.97%] [G loss: 0.474905]\n",
      "epoch:15 step:14612 [D loss: 0.220104, acc.: 64.06%] [G loss: 0.469413]\n",
      "epoch:15 step:14613 [D loss: 0.216181, acc.: 63.28%] [G loss: 0.474376]\n",
      "epoch:15 step:14614 [D loss: 0.240581, acc.: 57.81%] [G loss: 0.438033]\n",
      "epoch:15 step:14615 [D loss: 0.248056, acc.: 51.56%] [G loss: 0.424514]\n",
      "epoch:15 step:14616 [D loss: 0.224992, acc.: 60.16%] [G loss: 0.410676]\n",
      "epoch:15 step:14617 [D loss: 0.196194, acc.: 70.31%] [G loss: 0.453609]\n",
      "epoch:15 step:14618 [D loss: 0.190527, acc.: 71.09%] [G loss: 0.473084]\n",
      "epoch:15 step:14619 [D loss: 0.200032, acc.: 75.00%] [G loss: 0.444203]\n",
      "epoch:15 step:14620 [D loss: 0.233371, acc.: 61.72%] [G loss: 0.444375]\n",
      "epoch:15 step:14621 [D loss: 0.274065, acc.: 52.34%] [G loss: 0.431618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14622 [D loss: 0.207493, acc.: 71.88%] [G loss: 0.467027]\n",
      "epoch:15 step:14623 [D loss: 0.194268, acc.: 75.78%] [G loss: 0.456747]\n",
      "epoch:15 step:14624 [D loss: 0.263252, acc.: 53.91%] [G loss: 0.419324]\n",
      "epoch:15 step:14625 [D loss: 0.198705, acc.: 71.09%] [G loss: 0.421075]\n",
      "epoch:15 step:14626 [D loss: 0.215142, acc.: 61.72%] [G loss: 0.383564]\n",
      "epoch:15 step:14627 [D loss: 0.197810, acc.: 71.09%] [G loss: 0.438531]\n",
      "epoch:15 step:14628 [D loss: 0.236333, acc.: 59.38%] [G loss: 0.406757]\n",
      "epoch:15 step:14629 [D loss: 0.185018, acc.: 72.66%] [G loss: 0.468498]\n",
      "epoch:15 step:14630 [D loss: 0.191994, acc.: 72.66%] [G loss: 0.458282]\n",
      "epoch:15 step:14631 [D loss: 0.243920, acc.: 59.38%] [G loss: 0.455048]\n",
      "epoch:15 step:14632 [D loss: 0.255709, acc.: 50.00%] [G loss: 0.422052]\n",
      "epoch:15 step:14633 [D loss: 0.232917, acc.: 60.16%] [G loss: 0.437536]\n",
      "epoch:15 step:14634 [D loss: 0.221224, acc.: 64.06%] [G loss: 0.469912]\n",
      "epoch:15 step:14635 [D loss: 0.242435, acc.: 54.69%] [G loss: 0.476778]\n",
      "epoch:15 step:14636 [D loss: 0.215479, acc.: 67.97%] [G loss: 0.446038]\n",
      "epoch:15 step:14637 [D loss: 0.215235, acc.: 70.31%] [G loss: 0.480128]\n",
      "epoch:15 step:14638 [D loss: 0.213310, acc.: 64.06%] [G loss: 0.456385]\n",
      "epoch:15 step:14639 [D loss: 0.231105, acc.: 65.62%] [G loss: 0.444464]\n",
      "epoch:15 step:14640 [D loss: 0.206814, acc.: 67.19%] [G loss: 0.446399]\n",
      "epoch:15 step:14641 [D loss: 0.236396, acc.: 60.94%] [G loss: 0.403866]\n",
      "epoch:15 step:14642 [D loss: 0.234668, acc.: 64.84%] [G loss: 0.418012]\n",
      "epoch:15 step:14643 [D loss: 0.226043, acc.: 60.94%] [G loss: 0.435485]\n",
      "epoch:15 step:14644 [D loss: 0.220205, acc.: 63.28%] [G loss: 0.464130]\n",
      "epoch:15 step:14645 [D loss: 0.239981, acc.: 62.50%] [G loss: 0.425577]\n",
      "epoch:15 step:14646 [D loss: 0.210093, acc.: 68.75%] [G loss: 0.463654]\n",
      "epoch:15 step:14647 [D loss: 0.210029, acc.: 71.88%] [G loss: 0.462696]\n",
      "epoch:15 step:14648 [D loss: 0.216562, acc.: 68.75%] [G loss: 0.462379]\n",
      "epoch:15 step:14649 [D loss: 0.221142, acc.: 63.28%] [G loss: 0.445571]\n",
      "epoch:15 step:14650 [D loss: 0.222239, acc.: 62.50%] [G loss: 0.441019]\n",
      "epoch:15 step:14651 [D loss: 0.248993, acc.: 54.69%] [G loss: 0.422491]\n",
      "epoch:15 step:14652 [D loss: 0.230791, acc.: 63.28%] [G loss: 0.425239]\n",
      "epoch:15 step:14653 [D loss: 0.213105, acc.: 66.41%] [G loss: 0.452741]\n",
      "epoch:15 step:14654 [D loss: 0.234421, acc.: 62.50%] [G loss: 0.405660]\n",
      "epoch:15 step:14655 [D loss: 0.220149, acc.: 64.06%] [G loss: 0.465410]\n",
      "epoch:15 step:14656 [D loss: 0.257673, acc.: 52.34%] [G loss: 0.451320]\n",
      "epoch:15 step:14657 [D loss: 0.227480, acc.: 64.84%] [G loss: 0.421265]\n",
      "epoch:15 step:14658 [D loss: 0.229573, acc.: 63.28%] [G loss: 0.434958]\n",
      "epoch:15 step:14659 [D loss: 0.222238, acc.: 60.16%] [G loss: 0.468387]\n",
      "epoch:15 step:14660 [D loss: 0.200694, acc.: 75.00%] [G loss: 0.428077]\n",
      "epoch:15 step:14661 [D loss: 0.217380, acc.: 58.59%] [G loss: 0.407964]\n",
      "epoch:15 step:14662 [D loss: 0.236423, acc.: 62.50%] [G loss: 0.351876]\n",
      "epoch:15 step:14663 [D loss: 0.234941, acc.: 55.47%] [G loss: 0.405126]\n",
      "epoch:15 step:14664 [D loss: 0.209384, acc.: 70.31%] [G loss: 0.486900]\n",
      "epoch:15 step:14665 [D loss: 0.236394, acc.: 57.81%] [G loss: 0.435700]\n",
      "epoch:15 step:14666 [D loss: 0.224073, acc.: 59.38%] [G loss: 0.419316]\n",
      "epoch:15 step:14667 [D loss: 0.240729, acc.: 57.81%] [G loss: 0.429407]\n",
      "epoch:15 step:14668 [D loss: 0.205275, acc.: 64.06%] [G loss: 0.451379]\n",
      "epoch:15 step:14669 [D loss: 0.243999, acc.: 56.25%] [G loss: 0.414661]\n",
      "epoch:15 step:14670 [D loss: 0.277149, acc.: 49.22%] [G loss: 0.389286]\n",
      "epoch:15 step:14671 [D loss: 0.245217, acc.: 57.81%] [G loss: 0.420015]\n",
      "epoch:15 step:14672 [D loss: 0.241287, acc.: 61.72%] [G loss: 0.427817]\n",
      "epoch:15 step:14673 [D loss: 0.241634, acc.: 57.03%] [G loss: 0.430414]\n",
      "epoch:15 step:14674 [D loss: 0.245236, acc.: 59.38%] [G loss: 0.428186]\n",
      "epoch:15 step:14675 [D loss: 0.199507, acc.: 67.19%] [G loss: 0.411019]\n",
      "epoch:15 step:14676 [D loss: 0.236370, acc.: 57.81%] [G loss: 0.421853]\n",
      "epoch:15 step:14677 [D loss: 0.237799, acc.: 59.38%] [G loss: 0.434214]\n",
      "epoch:15 step:14678 [D loss: 0.210201, acc.: 68.75%] [G loss: 0.448775]\n",
      "epoch:15 step:14679 [D loss: 0.205341, acc.: 67.19%] [G loss: 0.434216]\n",
      "epoch:15 step:14680 [D loss: 0.258602, acc.: 49.22%] [G loss: 0.464323]\n",
      "epoch:15 step:14681 [D loss: 0.241106, acc.: 51.56%] [G loss: 0.434501]\n",
      "epoch:15 step:14682 [D loss: 0.224788, acc.: 63.28%] [G loss: 0.417362]\n",
      "epoch:15 step:14683 [D loss: 0.231384, acc.: 63.28%] [G loss: 0.409525]\n",
      "epoch:15 step:14684 [D loss: 0.235363, acc.: 61.72%] [G loss: 0.420674]\n",
      "epoch:15 step:14685 [D loss: 0.226054, acc.: 61.72%] [G loss: 0.460449]\n",
      "epoch:15 step:14686 [D loss: 0.192882, acc.: 75.78%] [G loss: 0.452228]\n",
      "epoch:15 step:14687 [D loss: 0.230766, acc.: 57.81%] [G loss: 0.489011]\n",
      "epoch:15 step:14688 [D loss: 0.203317, acc.: 68.75%] [G loss: 0.444438]\n",
      "epoch:15 step:14689 [D loss: 0.183996, acc.: 74.22%] [G loss: 0.463114]\n",
      "epoch:15 step:14690 [D loss: 0.206271, acc.: 68.75%] [G loss: 0.445451]\n",
      "epoch:15 step:14691 [D loss: 0.225101, acc.: 62.50%] [G loss: 0.429745]\n",
      "epoch:15 step:14692 [D loss: 0.204913, acc.: 71.88%] [G loss: 0.401623]\n",
      "epoch:15 step:14693 [D loss: 0.218612, acc.: 63.28%] [G loss: 0.393457]\n",
      "epoch:15 step:14694 [D loss: 0.237823, acc.: 57.03%] [G loss: 0.447932]\n",
      "epoch:15 step:14695 [D loss: 0.250198, acc.: 53.91%] [G loss: 0.438561]\n",
      "epoch:15 step:14696 [D loss: 0.204276, acc.: 67.97%] [G loss: 0.476751]\n",
      "epoch:15 step:14697 [D loss: 0.176003, acc.: 78.91%] [G loss: 0.477013]\n",
      "epoch:15 step:14698 [D loss: 0.224659, acc.: 63.28%] [G loss: 0.467765]\n",
      "epoch:15 step:14699 [D loss: 0.265280, acc.: 53.91%] [G loss: 0.424525]\n",
      "epoch:15 step:14700 [D loss: 0.218720, acc.: 67.97%] [G loss: 0.453697]\n",
      "epoch:15 step:14701 [D loss: 0.210056, acc.: 66.41%] [G loss: 0.458545]\n",
      "epoch:15 step:14702 [D loss: 0.184290, acc.: 74.22%] [G loss: 0.494252]\n",
      "epoch:15 step:14703 [D loss: 0.163530, acc.: 80.47%] [G loss: 0.516182]\n",
      "epoch:15 step:14704 [D loss: 0.195840, acc.: 68.75%] [G loss: 0.498403]\n",
      "epoch:15 step:14705 [D loss: 0.221971, acc.: 60.16%] [G loss: 0.482652]\n",
      "epoch:15 step:14706 [D loss: 0.236263, acc.: 59.38%] [G loss: 0.496504]\n",
      "epoch:15 step:14707 [D loss: 0.236024, acc.: 62.50%] [G loss: 0.427379]\n",
      "epoch:15 step:14708 [D loss: 0.214550, acc.: 68.75%] [G loss: 0.421742]\n",
      "epoch:15 step:14709 [D loss: 0.209875, acc.: 70.31%] [G loss: 0.473645]\n",
      "epoch:15 step:14710 [D loss: 0.267825, acc.: 50.78%] [G loss: 0.444279]\n",
      "epoch:15 step:14711 [D loss: 0.236457, acc.: 60.94%] [G loss: 0.454963]\n",
      "epoch:15 step:14712 [D loss: 0.240832, acc.: 61.72%] [G loss: 0.441592]\n",
      "epoch:15 step:14713 [D loss: 0.249303, acc.: 57.81%] [G loss: 0.479229]\n",
      "epoch:15 step:14714 [D loss: 0.234202, acc.: 57.03%] [G loss: 0.425877]\n",
      "epoch:15 step:14715 [D loss: 0.211788, acc.: 64.06%] [G loss: 0.427786]\n",
      "epoch:15 step:14716 [D loss: 0.198527, acc.: 67.97%] [G loss: 0.452973]\n",
      "epoch:15 step:14717 [D loss: 0.226349, acc.: 66.41%] [G loss: 0.431780]\n",
      "epoch:15 step:14718 [D loss: 0.233912, acc.: 61.72%] [G loss: 0.402256]\n",
      "epoch:15 step:14719 [D loss: 0.225907, acc.: 60.16%] [G loss: 0.479763]\n",
      "epoch:15 step:14720 [D loss: 0.240090, acc.: 56.25%] [G loss: 0.483759]\n",
      "epoch:15 step:14721 [D loss: 0.214391, acc.: 61.72%] [G loss: 0.458422]\n",
      "epoch:15 step:14722 [D loss: 0.230890, acc.: 64.84%] [G loss: 0.504292]\n",
      "epoch:15 step:14723 [D loss: 0.246707, acc.: 52.34%] [G loss: 0.436767]\n",
      "epoch:15 step:14724 [D loss: 0.221566, acc.: 66.41%] [G loss: 0.428208]\n",
      "epoch:15 step:14725 [D loss: 0.251252, acc.: 53.91%] [G loss: 0.430400]\n",
      "epoch:15 step:14726 [D loss: 0.241462, acc.: 60.16%] [G loss: 0.415581]\n",
      "epoch:15 step:14727 [D loss: 0.246451, acc.: 60.16%] [G loss: 0.434097]\n",
      "epoch:15 step:14728 [D loss: 0.224333, acc.: 64.84%] [G loss: 0.412274]\n",
      "epoch:15 step:14729 [D loss: 0.212439, acc.: 65.62%] [G loss: 0.456760]\n",
      "epoch:15 step:14730 [D loss: 0.249018, acc.: 62.50%] [G loss: 0.433865]\n",
      "epoch:15 step:14731 [D loss: 0.218147, acc.: 64.84%] [G loss: 0.427279]\n",
      "epoch:15 step:14732 [D loss: 0.191140, acc.: 68.75%] [G loss: 0.461539]\n",
      "epoch:15 step:14733 [D loss: 0.224969, acc.: 61.72%] [G loss: 0.422468]\n",
      "epoch:15 step:14734 [D loss: 0.217292, acc.: 63.28%] [G loss: 0.457972]\n",
      "epoch:15 step:14735 [D loss: 0.215681, acc.: 68.75%] [G loss: 0.441930]\n",
      "epoch:15 step:14736 [D loss: 0.201821, acc.: 68.75%] [G loss: 0.483959]\n",
      "epoch:15 step:14737 [D loss: 0.238333, acc.: 63.28%] [G loss: 0.433695]\n",
      "epoch:15 step:14738 [D loss: 0.223775, acc.: 62.50%] [G loss: 0.446533]\n",
      "epoch:15 step:14739 [D loss: 0.235940, acc.: 56.25%] [G loss: 0.409339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14740 [D loss: 0.221921, acc.: 61.72%] [G loss: 0.445191]\n",
      "epoch:15 step:14741 [D loss: 0.240288, acc.: 58.59%] [G loss: 0.443487]\n",
      "epoch:15 step:14742 [D loss: 0.239081, acc.: 54.69%] [G loss: 0.389874]\n",
      "epoch:15 step:14743 [D loss: 0.203866, acc.: 71.88%] [G loss: 0.434767]\n",
      "epoch:15 step:14744 [D loss: 0.208073, acc.: 69.53%] [G loss: 0.452628]\n",
      "epoch:15 step:14745 [D loss: 0.192345, acc.: 72.66%] [G loss: 0.486141]\n",
      "epoch:15 step:14746 [D loss: 0.207485, acc.: 64.84%] [G loss: 0.487738]\n",
      "epoch:15 step:14747 [D loss: 0.213645, acc.: 63.28%] [G loss: 0.473335]\n",
      "epoch:15 step:14748 [D loss: 0.225990, acc.: 67.19%] [G loss: 0.467963]\n",
      "epoch:15 step:14749 [D loss: 0.182626, acc.: 69.53%] [G loss: 0.467340]\n",
      "epoch:15 step:14750 [D loss: 0.210137, acc.: 67.19%] [G loss: 0.457458]\n",
      "epoch:15 step:14751 [D loss: 0.242844, acc.: 60.94%] [G loss: 0.501367]\n",
      "epoch:15 step:14752 [D loss: 0.224982, acc.: 56.25%] [G loss: 0.451065]\n",
      "epoch:15 step:14753 [D loss: 0.248073, acc.: 57.81%] [G loss: 0.411343]\n",
      "epoch:15 step:14754 [D loss: 0.209353, acc.: 67.97%] [G loss: 0.416465]\n",
      "epoch:15 step:14755 [D loss: 0.197347, acc.: 71.88%] [G loss: 0.469012]\n",
      "epoch:15 step:14756 [D loss: 0.202954, acc.: 63.28%] [G loss: 0.469911]\n",
      "epoch:15 step:14757 [D loss: 0.244363, acc.: 58.59%] [G loss: 0.472012]\n",
      "epoch:15 step:14758 [D loss: 0.252769, acc.: 54.69%] [G loss: 0.437026]\n",
      "epoch:15 step:14759 [D loss: 0.237342, acc.: 62.50%] [G loss: 0.432882]\n",
      "epoch:15 step:14760 [D loss: 0.238731, acc.: 53.91%] [G loss: 0.449195]\n",
      "epoch:15 step:14761 [D loss: 0.220915, acc.: 61.72%] [G loss: 0.403991]\n",
      "epoch:15 step:14762 [D loss: 0.200871, acc.: 69.53%] [G loss: 0.438694]\n",
      "epoch:15 step:14763 [D loss: 0.205213, acc.: 69.53%] [G loss: 0.474881]\n",
      "epoch:15 step:14764 [D loss: 0.183758, acc.: 72.66%] [G loss: 0.510534]\n",
      "epoch:15 step:14765 [D loss: 0.250219, acc.: 56.25%] [G loss: 0.414405]\n",
      "epoch:15 step:14766 [D loss: 0.235959, acc.: 60.16%] [G loss: 0.444339]\n",
      "epoch:15 step:14767 [D loss: 0.225750, acc.: 60.94%] [G loss: 0.409622]\n",
      "epoch:15 step:14768 [D loss: 0.234769, acc.: 57.81%] [G loss: 0.436080]\n",
      "epoch:15 step:14769 [D loss: 0.235323, acc.: 59.38%] [G loss: 0.415088]\n",
      "epoch:15 step:14770 [D loss: 0.210519, acc.: 66.41%] [G loss: 0.453029]\n",
      "epoch:15 step:14771 [D loss: 0.258928, acc.: 52.34%] [G loss: 0.408276]\n",
      "epoch:15 step:14772 [D loss: 0.230750, acc.: 60.94%] [G loss: 0.436473]\n",
      "epoch:15 step:14773 [D loss: 0.220958, acc.: 62.50%] [G loss: 0.495860]\n",
      "epoch:15 step:14774 [D loss: 0.188406, acc.: 69.53%] [G loss: 0.510358]\n",
      "epoch:15 step:14775 [D loss: 0.237652, acc.: 63.28%] [G loss: 0.483756]\n",
      "epoch:15 step:14776 [D loss: 0.242799, acc.: 58.59%] [G loss: 0.479648]\n",
      "epoch:15 step:14777 [D loss: 0.248773, acc.: 55.47%] [G loss: 0.404732]\n",
      "epoch:15 step:14778 [D loss: 0.209237, acc.: 62.50%] [G loss: 0.437781]\n",
      "epoch:15 step:14779 [D loss: 0.228613, acc.: 61.72%] [G loss: 0.447321]\n",
      "epoch:15 step:14780 [D loss: 0.205002, acc.: 69.53%] [G loss: 0.481113]\n",
      "epoch:15 step:14781 [D loss: 0.218648, acc.: 60.16%] [G loss: 0.450530]\n",
      "epoch:15 step:14782 [D loss: 0.229465, acc.: 63.28%] [G loss: 0.420566]\n",
      "epoch:15 step:14783 [D loss: 0.233335, acc.: 61.72%] [G loss: 0.414034]\n",
      "epoch:15 step:14784 [D loss: 0.245919, acc.: 57.03%] [G loss: 0.440046]\n",
      "epoch:15 step:14785 [D loss: 0.206740, acc.: 65.62%] [G loss: 0.456579]\n",
      "epoch:15 step:14786 [D loss: 0.224391, acc.: 62.50%] [G loss: 0.468971]\n",
      "epoch:15 step:14787 [D loss: 0.227210, acc.: 64.06%] [G loss: 0.451840]\n",
      "epoch:15 step:14788 [D loss: 0.207070, acc.: 62.50%] [G loss: 0.399371]\n",
      "epoch:15 step:14789 [D loss: 0.244813, acc.: 59.38%] [G loss: 0.446419]\n",
      "epoch:15 step:14790 [D loss: 0.238191, acc.: 59.38%] [G loss: 0.444448]\n",
      "epoch:15 step:14791 [D loss: 0.224946, acc.: 63.28%] [G loss: 0.476463]\n",
      "epoch:15 step:14792 [D loss: 0.228667, acc.: 61.72%] [G loss: 0.457933]\n",
      "epoch:15 step:14793 [D loss: 0.233909, acc.: 57.03%] [G loss: 0.458821]\n",
      "epoch:15 step:14794 [D loss: 0.247374, acc.: 53.91%] [G loss: 0.433401]\n",
      "epoch:15 step:14795 [D loss: 0.220980, acc.: 67.97%] [G loss: 0.441397]\n",
      "epoch:15 step:14796 [D loss: 0.250309, acc.: 53.12%] [G loss: 0.394475]\n",
      "epoch:15 step:14797 [D loss: 0.232561, acc.: 56.25%] [G loss: 0.420877]\n",
      "epoch:15 step:14798 [D loss: 0.200392, acc.: 70.31%] [G loss: 0.468525]\n",
      "epoch:15 step:14799 [D loss: 0.240711, acc.: 58.59%] [G loss: 0.457212]\n",
      "epoch:15 step:14800 [D loss: 0.239954, acc.: 55.47%] [G loss: 0.456308]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.993147\n",
      "FID: 7.369137\n",
      "0 = 11.748233012914667\n",
      "1 = 0.049710052937993784\n",
      "2 = 0.8751999735832214\n",
      "3 = 0.8596000075340271\n",
      "4 = 0.8907999992370605\n",
      "5 = 0.8872832655906677\n",
      "6 = 0.8596000075340271\n",
      "7 = 5.946586968648437\n",
      "8 = 0.06035460430491797\n",
      "9 = 0.6954500079154968\n",
      "10 = 0.6901000142097473\n",
      "11 = 0.7008000016212463\n",
      "12 = 0.6975639462471008\n",
      "13 = 0.6901000142097473\n",
      "14 = 7.993212699890137\n",
      "15 = 9.541975975036621\n",
      "16 = 0.10492677241563797\n",
      "17 = 7.993146896362305\n",
      "18 = 7.369136810302734\n",
      "epoch:15 step:14801 [D loss: 0.205787, acc.: 64.06%] [G loss: 0.433220]\n",
      "epoch:15 step:14802 [D loss: 0.194193, acc.: 70.31%] [G loss: 0.447734]\n",
      "epoch:15 step:14803 [D loss: 0.219968, acc.: 68.75%] [G loss: 0.433820]\n",
      "epoch:15 step:14804 [D loss: 0.216490, acc.: 62.50%] [G loss: 0.395850]\n",
      "epoch:15 step:14805 [D loss: 0.215338, acc.: 65.62%] [G loss: 0.411865]\n",
      "epoch:15 step:14806 [D loss: 0.215066, acc.: 64.84%] [G loss: 0.458141]\n",
      "epoch:15 step:14807 [D loss: 0.243750, acc.: 61.72%] [G loss: 0.462781]\n",
      "epoch:15 step:14808 [D loss: 0.229505, acc.: 64.06%] [G loss: 0.419721]\n",
      "epoch:15 step:14809 [D loss: 0.208355, acc.: 62.50%] [G loss: 0.466787]\n",
      "epoch:15 step:14810 [D loss: 0.231768, acc.: 60.16%] [G loss: 0.438380]\n",
      "epoch:15 step:14811 [D loss: 0.227534, acc.: 64.84%] [G loss: 0.435522]\n",
      "epoch:15 step:14812 [D loss: 0.215880, acc.: 64.06%] [G loss: 0.412423]\n",
      "epoch:15 step:14813 [D loss: 0.250619, acc.: 60.94%] [G loss: 0.446101]\n",
      "epoch:15 step:14814 [D loss: 0.223769, acc.: 61.72%] [G loss: 0.440211]\n",
      "epoch:15 step:14815 [D loss: 0.226235, acc.: 64.84%] [G loss: 0.439006]\n",
      "epoch:15 step:14816 [D loss: 0.221833, acc.: 62.50%] [G loss: 0.411217]\n",
      "epoch:15 step:14817 [D loss: 0.210835, acc.: 68.75%] [G loss: 0.445345]\n",
      "epoch:15 step:14818 [D loss: 0.238999, acc.: 61.72%] [G loss: 0.416386]\n",
      "epoch:15 step:14819 [D loss: 0.221061, acc.: 60.16%] [G loss: 0.433405]\n",
      "epoch:15 step:14820 [D loss: 0.263545, acc.: 57.81%] [G loss: 0.423527]\n",
      "epoch:15 step:14821 [D loss: 0.234188, acc.: 57.03%] [G loss: 0.463246]\n",
      "epoch:15 step:14822 [D loss: 0.238911, acc.: 57.03%] [G loss: 0.415450]\n",
      "epoch:15 step:14823 [D loss: 0.209766, acc.: 65.62%] [G loss: 0.465308]\n",
      "epoch:15 step:14824 [D loss: 0.203318, acc.: 70.31%] [G loss: 0.479202]\n",
      "epoch:15 step:14825 [D loss: 0.231602, acc.: 60.94%] [G loss: 0.465031]\n",
      "epoch:15 step:14826 [D loss: 0.249224, acc.: 54.69%] [G loss: 0.467307]\n",
      "epoch:15 step:14827 [D loss: 0.238980, acc.: 59.38%] [G loss: 0.465330]\n",
      "epoch:15 step:14828 [D loss: 0.244756, acc.: 54.69%] [G loss: 0.422268]\n",
      "epoch:15 step:14829 [D loss: 0.233890, acc.: 57.03%] [G loss: 0.410130]\n",
      "epoch:15 step:14830 [D loss: 0.183685, acc.: 74.22%] [G loss: 0.513739]\n",
      "epoch:15 step:14831 [D loss: 0.254902, acc.: 56.25%] [G loss: 0.458693]\n",
      "epoch:15 step:14832 [D loss: 0.220983, acc.: 66.41%] [G loss: 0.433047]\n",
      "epoch:15 step:14833 [D loss: 0.207833, acc.: 71.88%] [G loss: 0.464541]\n",
      "epoch:15 step:14834 [D loss: 0.204581, acc.: 71.09%] [G loss: 0.438824]\n",
      "epoch:15 step:14835 [D loss: 0.214099, acc.: 67.97%] [G loss: 0.402597]\n",
      "epoch:15 step:14836 [D loss: 0.205267, acc.: 71.09%] [G loss: 0.482128]\n",
      "epoch:15 step:14837 [D loss: 0.175068, acc.: 75.00%] [G loss: 0.565993]\n",
      "epoch:15 step:14838 [D loss: 0.253930, acc.: 54.69%] [G loss: 0.458831]\n",
      "epoch:15 step:14839 [D loss: 0.247798, acc.: 53.91%] [G loss: 0.419758]\n",
      "epoch:15 step:14840 [D loss: 0.230480, acc.: 59.38%] [G loss: 0.449294]\n",
      "epoch:15 step:14841 [D loss: 0.204398, acc.: 67.97%] [G loss: 0.458107]\n",
      "epoch:15 step:14842 [D loss: 0.246790, acc.: 54.69%] [G loss: 0.439540]\n",
      "epoch:15 step:14843 [D loss: 0.243483, acc.: 58.59%] [G loss: 0.432687]\n",
      "epoch:15 step:14844 [D loss: 0.216598, acc.: 61.72%] [G loss: 0.442210]\n",
      "epoch:15 step:14845 [D loss: 0.229807, acc.: 61.72%] [G loss: 0.420164]\n",
      "epoch:15 step:14846 [D loss: 0.239381, acc.: 53.91%] [G loss: 0.451232]\n",
      "epoch:15 step:14847 [D loss: 0.209606, acc.: 65.62%] [G loss: 0.440243]\n",
      "epoch:15 step:14848 [D loss: 0.214365, acc.: 62.50%] [G loss: 0.460511]\n",
      "epoch:15 step:14849 [D loss: 0.275034, acc.: 51.56%] [G loss: 0.451540]\n",
      "epoch:15 step:14850 [D loss: 0.240819, acc.: 58.59%] [G loss: 0.425574]\n",
      "epoch:15 step:14851 [D loss: 0.206568, acc.: 67.97%] [G loss: 0.492980]\n",
      "epoch:15 step:14852 [D loss: 0.244845, acc.: 59.38%] [G loss: 0.449991]\n",
      "epoch:15 step:14853 [D loss: 0.225399, acc.: 61.72%] [G loss: 0.419051]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14854 [D loss: 0.211684, acc.: 62.50%] [G loss: 0.388987]\n",
      "epoch:15 step:14855 [D loss: 0.258553, acc.: 53.91%] [G loss: 0.410816]\n",
      "epoch:15 step:14856 [D loss: 0.212716, acc.: 62.50%] [G loss: 0.424572]\n",
      "epoch:15 step:14857 [D loss: 0.204943, acc.: 64.84%] [G loss: 0.444222]\n",
      "epoch:15 step:14858 [D loss: 0.215813, acc.: 64.84%] [G loss: 0.438459]\n",
      "epoch:15 step:14859 [D loss: 0.219218, acc.: 61.72%] [G loss: 0.414785]\n",
      "epoch:15 step:14860 [D loss: 0.207689, acc.: 66.41%] [G loss: 0.449668]\n",
      "epoch:15 step:14861 [D loss: 0.223584, acc.: 64.84%] [G loss: 0.439634]\n",
      "epoch:15 step:14862 [D loss: 0.220368, acc.: 67.97%] [G loss: 0.440549]\n",
      "epoch:15 step:14863 [D loss: 0.236427, acc.: 53.91%] [G loss: 0.441258]\n",
      "epoch:15 step:14864 [D loss: 0.239545, acc.: 64.84%] [G loss: 0.435736]\n",
      "epoch:15 step:14865 [D loss: 0.229505, acc.: 60.94%] [G loss: 0.417188]\n",
      "epoch:15 step:14866 [D loss: 0.217465, acc.: 62.50%] [G loss: 0.497083]\n",
      "epoch:15 step:14867 [D loss: 0.231519, acc.: 61.72%] [G loss: 0.435856]\n",
      "epoch:15 step:14868 [D loss: 0.234264, acc.: 60.16%] [G loss: 0.472735]\n",
      "epoch:15 step:14869 [D loss: 0.232240, acc.: 59.38%] [G loss: 0.445844]\n",
      "epoch:15 step:14870 [D loss: 0.202842, acc.: 71.09%] [G loss: 0.505568]\n",
      "epoch:15 step:14871 [D loss: 0.229480, acc.: 61.72%] [G loss: 0.469117]\n",
      "epoch:15 step:14872 [D loss: 0.239077, acc.: 60.94%] [G loss: 0.498400]\n",
      "epoch:15 step:14873 [D loss: 0.255202, acc.: 59.38%] [G loss: 0.458834]\n",
      "epoch:15 step:14874 [D loss: 0.194602, acc.: 75.78%] [G loss: 0.447388]\n",
      "epoch:15 step:14875 [D loss: 0.247752, acc.: 53.12%] [G loss: 0.455028]\n",
      "epoch:15 step:14876 [D loss: 0.234690, acc.: 60.16%] [G loss: 0.450045]\n",
      "epoch:15 step:14877 [D loss: 0.215583, acc.: 62.50%] [G loss: 0.465460]\n",
      "epoch:15 step:14878 [D loss: 0.219041, acc.: 62.50%] [G loss: 0.432673]\n",
      "epoch:15 step:14879 [D loss: 0.253912, acc.: 51.56%] [G loss: 0.424001]\n",
      "epoch:15 step:14880 [D loss: 0.206716, acc.: 71.88%] [G loss: 0.459237]\n",
      "epoch:15 step:14881 [D loss: 0.224325, acc.: 58.59%] [G loss: 0.449533]\n",
      "epoch:15 step:14882 [D loss: 0.262165, acc.: 51.56%] [G loss: 0.438719]\n",
      "epoch:15 step:14883 [D loss: 0.242199, acc.: 60.94%] [G loss: 0.457360]\n",
      "epoch:15 step:14884 [D loss: 0.227808, acc.: 61.72%] [G loss: 0.442668]\n",
      "epoch:15 step:14885 [D loss: 0.235770, acc.: 60.16%] [G loss: 0.447664]\n",
      "epoch:15 step:14886 [D loss: 0.220948, acc.: 60.94%] [G loss: 0.425905]\n",
      "epoch:15 step:14887 [D loss: 0.201454, acc.: 64.06%] [G loss: 0.414670]\n",
      "epoch:15 step:14888 [D loss: 0.182448, acc.: 74.22%] [G loss: 0.484948]\n",
      "epoch:15 step:14889 [D loss: 0.253183, acc.: 55.47%] [G loss: 0.377630]\n",
      "epoch:15 step:14890 [D loss: 0.222002, acc.: 65.62%] [G loss: 0.417613]\n",
      "epoch:15 step:14891 [D loss: 0.230976, acc.: 63.28%] [G loss: 0.447177]\n",
      "epoch:15 step:14892 [D loss: 0.190131, acc.: 71.09%] [G loss: 0.436747]\n",
      "epoch:15 step:14893 [D loss: 0.220584, acc.: 67.19%] [G loss: 0.434818]\n",
      "epoch:15 step:14894 [D loss: 0.224086, acc.: 62.50%] [G loss: 0.430630]\n",
      "epoch:15 step:14895 [D loss: 0.224446, acc.: 70.31%] [G loss: 0.455839]\n",
      "epoch:15 step:14896 [D loss: 0.203315, acc.: 75.00%] [G loss: 0.476893]\n",
      "epoch:15 step:14897 [D loss: 0.189515, acc.: 72.66%] [G loss: 0.517620]\n",
      "epoch:15 step:14898 [D loss: 0.226316, acc.: 64.06%] [G loss: 0.451841]\n",
      "epoch:15 step:14899 [D loss: 0.227165, acc.: 59.38%] [G loss: 0.452986]\n",
      "epoch:15 step:14900 [D loss: 0.215345, acc.: 67.19%] [G loss: 0.431333]\n",
      "epoch:15 step:14901 [D loss: 0.240527, acc.: 55.47%] [G loss: 0.451132]\n",
      "epoch:15 step:14902 [D loss: 0.255240, acc.: 53.91%] [G loss: 0.415465]\n",
      "epoch:15 step:14903 [D loss: 0.218763, acc.: 59.38%] [G loss: 0.430397]\n",
      "epoch:15 step:14904 [D loss: 0.232356, acc.: 61.72%] [G loss: 0.401616]\n",
      "epoch:15 step:14905 [D loss: 0.266930, acc.: 49.22%] [G loss: 0.441451]\n",
      "epoch:15 step:14906 [D loss: 0.228483, acc.: 58.59%] [G loss: 0.481917]\n",
      "epoch:15 step:14907 [D loss: 0.210247, acc.: 64.06%] [G loss: 0.500167]\n",
      "epoch:15 step:14908 [D loss: 0.198978, acc.: 70.31%] [G loss: 0.479669]\n",
      "epoch:15 step:14909 [D loss: 0.225617, acc.: 59.38%] [G loss: 0.438401]\n",
      "epoch:15 step:14910 [D loss: 0.241128, acc.: 59.38%] [G loss: 0.429545]\n",
      "epoch:15 step:14911 [D loss: 0.246014, acc.: 55.47%] [G loss: 0.374567]\n",
      "epoch:15 step:14912 [D loss: 0.207561, acc.: 67.19%] [G loss: 0.432051]\n",
      "epoch:15 step:14913 [D loss: 0.300407, acc.: 46.88%] [G loss: 0.410528]\n",
      "epoch:15 step:14914 [D loss: 0.219002, acc.: 68.75%] [G loss: 0.450399]\n",
      "epoch:15 step:14915 [D loss: 0.236617, acc.: 59.38%] [G loss: 0.429472]\n",
      "epoch:15 step:14916 [D loss: 0.251303, acc.: 55.47%] [G loss: 0.413265]\n",
      "epoch:15 step:14917 [D loss: 0.223445, acc.: 60.94%] [G loss: 0.408485]\n",
      "epoch:15 step:14918 [D loss: 0.211619, acc.: 66.41%] [G loss: 0.360185]\n",
      "epoch:15 step:14919 [D loss: 0.240859, acc.: 58.59%] [G loss: 0.407243]\n",
      "epoch:15 step:14920 [D loss: 0.248986, acc.: 53.91%] [G loss: 0.408902]\n",
      "epoch:15 step:14921 [D loss: 0.214169, acc.: 65.62%] [G loss: 0.380006]\n",
      "epoch:15 step:14922 [D loss: 0.264486, acc.: 53.12%] [G loss: 0.408255]\n",
      "epoch:15 step:14923 [D loss: 0.224877, acc.: 64.84%] [G loss: 0.429749]\n",
      "epoch:15 step:14924 [D loss: 0.230933, acc.: 60.94%] [G loss: 0.427967]\n",
      "epoch:15 step:14925 [D loss: 0.218730, acc.: 62.50%] [G loss: 0.440512]\n",
      "epoch:15 step:14926 [D loss: 0.208037, acc.: 62.50%] [G loss: 0.457996]\n",
      "epoch:15 step:14927 [D loss: 0.250528, acc.: 57.81%] [G loss: 0.454072]\n",
      "epoch:15 step:14928 [D loss: 0.232120, acc.: 67.97%] [G loss: 0.431346]\n",
      "epoch:15 step:14929 [D loss: 0.239510, acc.: 57.03%] [G loss: 0.447033]\n",
      "epoch:15 step:14930 [D loss: 0.189870, acc.: 73.44%] [G loss: 0.478078]\n",
      "epoch:15 step:14931 [D loss: 0.233252, acc.: 64.06%] [G loss: 0.449052]\n",
      "epoch:15 step:14932 [D loss: 0.240096, acc.: 60.16%] [G loss: 0.418631]\n",
      "epoch:15 step:14933 [D loss: 0.223802, acc.: 59.38%] [G loss: 0.457478]\n",
      "epoch:15 step:14934 [D loss: 0.240835, acc.: 57.03%] [G loss: 0.455789]\n",
      "epoch:15 step:14935 [D loss: 0.228576, acc.: 60.94%] [G loss: 0.456081]\n",
      "epoch:15 step:14936 [D loss: 0.230057, acc.: 55.47%] [G loss: 0.410921]\n",
      "epoch:15 step:14937 [D loss: 0.229401, acc.: 59.38%] [G loss: 0.437605]\n",
      "epoch:15 step:14938 [D loss: 0.215066, acc.: 71.09%] [G loss: 0.489524]\n",
      "epoch:15 step:14939 [D loss: 0.213355, acc.: 64.06%] [G loss: 0.488918]\n",
      "epoch:15 step:14940 [D loss: 0.236137, acc.: 61.72%] [G loss: 0.495283]\n",
      "epoch:15 step:14941 [D loss: 0.213011, acc.: 67.19%] [G loss: 0.522661]\n",
      "epoch:15 step:14942 [D loss: 0.220165, acc.: 67.97%] [G loss: 0.498749]\n",
      "epoch:15 step:14943 [D loss: 0.214318, acc.: 63.28%] [G loss: 0.467004]\n",
      "epoch:15 step:14944 [D loss: 0.256955, acc.: 57.03%] [G loss: 0.456527]\n",
      "epoch:15 step:14945 [D loss: 0.173436, acc.: 76.56%] [G loss: 0.512815]\n",
      "epoch:15 step:14946 [D loss: 0.265302, acc.: 53.12%] [G loss: 0.449490]\n",
      "epoch:15 step:14947 [D loss: 0.259932, acc.: 54.69%] [G loss: 0.395748]\n",
      "epoch:15 step:14948 [D loss: 0.214185, acc.: 62.50%] [G loss: 0.419002]\n",
      "epoch:15 step:14949 [D loss: 0.215569, acc.: 64.84%] [G loss: 0.419318]\n",
      "epoch:15 step:14950 [D loss: 0.211969, acc.: 67.19%] [G loss: 0.414868]\n",
      "epoch:15 step:14951 [D loss: 0.206845, acc.: 67.97%] [G loss: 0.497197]\n",
      "epoch:15 step:14952 [D loss: 0.188845, acc.: 71.88%] [G loss: 0.476983]\n",
      "epoch:15 step:14953 [D loss: 0.195179, acc.: 71.09%] [G loss: 0.448389]\n",
      "epoch:15 step:14954 [D loss: 0.199158, acc.: 68.75%] [G loss: 0.476854]\n",
      "epoch:15 step:14955 [D loss: 0.188568, acc.: 71.09%] [G loss: 0.498709]\n",
      "epoch:15 step:14956 [D loss: 0.214032, acc.: 67.19%] [G loss: 0.458891]\n",
      "epoch:15 step:14957 [D loss: 0.251710, acc.: 54.69%] [G loss: 0.413128]\n",
      "epoch:15 step:14958 [D loss: 0.225679, acc.: 63.28%] [G loss: 0.432915]\n",
      "epoch:15 step:14959 [D loss: 0.224524, acc.: 64.06%] [G loss: 0.442217]\n",
      "epoch:15 step:14960 [D loss: 0.207511, acc.: 73.44%] [G loss: 0.456680]\n",
      "epoch:15 step:14961 [D loss: 0.221077, acc.: 71.09%] [G loss: 0.437236]\n",
      "epoch:15 step:14962 [D loss: 0.237330, acc.: 57.03%] [G loss: 0.438756]\n",
      "epoch:15 step:14963 [D loss: 0.219424, acc.: 64.06%] [G loss: 0.397987]\n",
      "epoch:15 step:14964 [D loss: 0.199767, acc.: 71.09%] [G loss: 0.454694]\n",
      "epoch:15 step:14965 [D loss: 0.202195, acc.: 65.62%] [G loss: 0.476386]\n",
      "epoch:15 step:14966 [D loss: 0.193312, acc.: 72.66%] [G loss: 0.471253]\n",
      "epoch:15 step:14967 [D loss: 0.229400, acc.: 62.50%] [G loss: 0.459439]\n",
      "epoch:15 step:14968 [D loss: 0.222237, acc.: 61.72%] [G loss: 0.473847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14969 [D loss: 0.226865, acc.: 57.81%] [G loss: 0.480178]\n",
      "epoch:15 step:14970 [D loss: 0.271998, acc.: 53.12%] [G loss: 0.412473]\n",
      "epoch:15 step:14971 [D loss: 0.239281, acc.: 60.94%] [G loss: 0.430230]\n",
      "epoch:15 step:14972 [D loss: 0.246361, acc.: 63.28%] [G loss: 0.474076]\n",
      "epoch:15 step:14973 [D loss: 0.186200, acc.: 73.44%] [G loss: 0.500654]\n",
      "epoch:15 step:14974 [D loss: 0.193757, acc.: 70.31%] [G loss: 0.500447]\n",
      "epoch:15 step:14975 [D loss: 0.286243, acc.: 45.31%] [G loss: 0.432376]\n",
      "epoch:15 step:14976 [D loss: 0.205146, acc.: 70.31%] [G loss: 0.470953]\n",
      "epoch:15 step:14977 [D loss: 0.241772, acc.: 54.69%] [G loss: 0.460782]\n",
      "epoch:15 step:14978 [D loss: 0.196950, acc.: 70.31%] [G loss: 0.460999]\n",
      "epoch:15 step:14979 [D loss: 0.162978, acc.: 81.25%] [G loss: 0.472481]\n",
      "epoch:15 step:14980 [D loss: 0.187125, acc.: 69.53%] [G loss: 0.511816]\n",
      "epoch:15 step:14981 [D loss: 0.173787, acc.: 78.91%] [G loss: 0.528241]\n",
      "epoch:15 step:14982 [D loss: 0.196689, acc.: 71.88%] [G loss: 0.516282]\n",
      "epoch:15 step:14983 [D loss: 0.312825, acc.: 50.78%] [G loss: 0.483362]\n",
      "epoch:15 step:14984 [D loss: 0.204156, acc.: 71.88%] [G loss: 0.605375]\n",
      "epoch:15 step:14985 [D loss: 0.210246, acc.: 67.19%] [G loss: 0.560815]\n",
      "epoch:15 step:14986 [D loss: 0.218230, acc.: 57.03%] [G loss: 0.446665]\n",
      "epoch:15 step:14987 [D loss: 0.247451, acc.: 53.12%] [G loss: 0.400387]\n",
      "epoch:15 step:14988 [D loss: 0.224831, acc.: 64.84%] [G loss: 0.449842]\n",
      "epoch:15 step:14989 [D loss: 0.239325, acc.: 69.53%] [G loss: 0.469612]\n",
      "epoch:15 step:14990 [D loss: 0.240224, acc.: 64.84%] [G loss: 0.480457]\n",
      "epoch:15 step:14991 [D loss: 0.166465, acc.: 77.34%] [G loss: 0.576371]\n",
      "epoch:15 step:14992 [D loss: 0.202771, acc.: 70.31%] [G loss: 0.553033]\n",
      "epoch:16 step:14993 [D loss: 0.256401, acc.: 60.16%] [G loss: 0.509748]\n",
      "epoch:16 step:14994 [D loss: 0.255056, acc.: 59.38%] [G loss: 0.460385]\n",
      "epoch:16 step:14995 [D loss: 0.228614, acc.: 58.59%] [G loss: 0.443921]\n",
      "epoch:16 step:14996 [D loss: 0.225195, acc.: 64.84%] [G loss: 0.481485]\n",
      "epoch:16 step:14997 [D loss: 0.212511, acc.: 67.97%] [G loss: 0.477130]\n",
      "epoch:16 step:14998 [D loss: 0.219084, acc.: 64.06%] [G loss: 0.492900]\n",
      "epoch:16 step:14999 [D loss: 0.220967, acc.: 59.38%] [G loss: 0.450774]\n",
      "epoch:16 step:15000 [D loss: 0.222060, acc.: 64.06%] [G loss: 0.417810]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 8.001406\n",
      "FID: 7.470464\n",
      "0 = 11.926243275189423\n",
      "1 = 0.052495344046357195\n",
      "2 = 0.8873000144958496\n",
      "3 = 0.8820000290870667\n",
      "4 = 0.8925999999046326\n",
      "5 = 0.8914493918418884\n",
      "6 = 0.8820000290870667\n",
      "7 = 5.889000153774025\n",
      "8 = 0.054747412424472564\n",
      "9 = 0.6985499858856201\n",
      "10 = 0.6952000260353088\n",
      "11 = 0.7019000053405762\n",
      "12 = 0.6998892426490784\n",
      "13 = 0.6952000260353088\n",
      "14 = 8.001472473144531\n",
      "15 = 9.4560546875\n",
      "16 = 0.13270895183086395\n",
      "17 = 8.001405715942383\n",
      "18 = 7.470463752746582\n",
      "epoch:16 step:15001 [D loss: 0.220080, acc.: 62.50%] [G loss: 0.419283]\n",
      "epoch:16 step:15002 [D loss: 0.205533, acc.: 69.53%] [G loss: 0.465340]\n",
      "epoch:16 step:15003 [D loss: 0.207147, acc.: 66.41%] [G loss: 0.485047]\n",
      "epoch:16 step:15004 [D loss: 0.224271, acc.: 62.50%] [G loss: 0.480678]\n",
      "epoch:16 step:15005 [D loss: 0.231456, acc.: 61.72%] [G loss: 0.490235]\n",
      "epoch:16 step:15006 [D loss: 0.192671, acc.: 72.66%] [G loss: 0.489560]\n",
      "epoch:16 step:15007 [D loss: 0.180994, acc.: 75.78%] [G loss: 0.445370]\n",
      "epoch:16 step:15008 [D loss: 0.182321, acc.: 71.09%] [G loss: 0.493694]\n",
      "epoch:16 step:15009 [D loss: 0.250925, acc.: 58.59%] [G loss: 0.443758]\n",
      "epoch:16 step:15010 [D loss: 0.208794, acc.: 65.62%] [G loss: 0.457512]\n",
      "epoch:16 step:15011 [D loss: 0.236428, acc.: 59.38%] [G loss: 0.436587]\n",
      "epoch:16 step:15012 [D loss: 0.224753, acc.: 64.06%] [G loss: 0.476061]\n",
      "epoch:16 step:15013 [D loss: 0.240556, acc.: 61.72%] [G loss: 0.434037]\n",
      "epoch:16 step:15014 [D loss: 0.196802, acc.: 74.22%] [G loss: 0.475072]\n",
      "epoch:16 step:15015 [D loss: 0.245123, acc.: 56.25%] [G loss: 0.433035]\n",
      "epoch:16 step:15016 [D loss: 0.228163, acc.: 67.19%] [G loss: 0.428389]\n",
      "epoch:16 step:15017 [D loss: 0.193325, acc.: 71.88%] [G loss: 0.464552]\n",
      "epoch:16 step:15018 [D loss: 0.241763, acc.: 52.34%] [G loss: 0.427254]\n",
      "epoch:16 step:15019 [D loss: 0.228595, acc.: 62.50%] [G loss: 0.454471]\n",
      "epoch:16 step:15020 [D loss: 0.199100, acc.: 69.53%] [G loss: 0.429993]\n",
      "epoch:16 step:15021 [D loss: 0.221782, acc.: 66.41%] [G loss: 0.433706]\n",
      "epoch:16 step:15022 [D loss: 0.252490, acc.: 54.69%] [G loss: 0.382106]\n",
      "epoch:16 step:15023 [D loss: 0.228396, acc.: 57.81%] [G loss: 0.457505]\n",
      "epoch:16 step:15024 [D loss: 0.217921, acc.: 64.84%] [G loss: 0.466893]\n",
      "epoch:16 step:15025 [D loss: 0.211943, acc.: 64.06%] [G loss: 0.432277]\n",
      "epoch:16 step:15026 [D loss: 0.257734, acc.: 53.12%] [G loss: 0.394378]\n",
      "epoch:16 step:15027 [D loss: 0.206155, acc.: 67.97%] [G loss: 0.408329]\n",
      "epoch:16 step:15028 [D loss: 0.229352, acc.: 65.62%] [G loss: 0.391575]\n",
      "epoch:16 step:15029 [D loss: 0.215422, acc.: 64.06%] [G loss: 0.449910]\n",
      "epoch:16 step:15030 [D loss: 0.235017, acc.: 60.16%] [G loss: 0.461393]\n",
      "epoch:16 step:15031 [D loss: 0.211616, acc.: 69.53%] [G loss: 0.424449]\n",
      "epoch:16 step:15032 [D loss: 0.205935, acc.: 64.84%] [G loss: 0.454909]\n",
      "epoch:16 step:15033 [D loss: 0.227571, acc.: 67.97%] [G loss: 0.426547]\n",
      "epoch:16 step:15034 [D loss: 0.220829, acc.: 60.94%] [G loss: 0.421575]\n",
      "epoch:16 step:15035 [D loss: 0.211987, acc.: 70.31%] [G loss: 0.470362]\n",
      "epoch:16 step:15036 [D loss: 0.242945, acc.: 57.81%] [G loss: 0.458903]\n",
      "epoch:16 step:15037 [D loss: 0.214146, acc.: 64.84%] [G loss: 0.407509]\n",
      "epoch:16 step:15038 [D loss: 0.224768, acc.: 65.62%] [G loss: 0.407284]\n",
      "epoch:16 step:15039 [D loss: 0.224428, acc.: 67.19%] [G loss: 0.400943]\n",
      "epoch:16 step:15040 [D loss: 0.192544, acc.: 73.44%] [G loss: 0.481266]\n",
      "epoch:16 step:15041 [D loss: 0.184824, acc.: 75.00%] [G loss: 0.475972]\n",
      "epoch:16 step:15042 [D loss: 0.190296, acc.: 69.53%] [G loss: 0.495901]\n",
      "epoch:16 step:15043 [D loss: 0.233475, acc.: 60.94%] [G loss: 0.432194]\n",
      "epoch:16 step:15044 [D loss: 0.265324, acc.: 53.12%] [G loss: 0.427429]\n",
      "epoch:16 step:15045 [D loss: 0.237248, acc.: 59.38%] [G loss: 0.447070]\n",
      "epoch:16 step:15046 [D loss: 0.223384, acc.: 64.06%] [G loss: 0.441903]\n",
      "epoch:16 step:15047 [D loss: 0.220399, acc.: 67.19%] [G loss: 0.446166]\n",
      "epoch:16 step:15048 [D loss: 0.210126, acc.: 67.97%] [G loss: 0.448300]\n",
      "epoch:16 step:15049 [D loss: 0.228819, acc.: 64.84%] [G loss: 0.394111]\n",
      "epoch:16 step:15050 [D loss: 0.226764, acc.: 60.16%] [G loss: 0.442406]\n",
      "epoch:16 step:15051 [D loss: 0.232083, acc.: 60.16%] [G loss: 0.410901]\n",
      "epoch:16 step:15052 [D loss: 0.225137, acc.: 63.28%] [G loss: 0.460743]\n",
      "epoch:16 step:15053 [D loss: 0.249463, acc.: 57.81%] [G loss: 0.355968]\n",
      "epoch:16 step:15054 [D loss: 0.234888, acc.: 61.72%] [G loss: 0.423113]\n",
      "epoch:16 step:15055 [D loss: 0.228504, acc.: 60.94%] [G loss: 0.415185]\n",
      "epoch:16 step:15056 [D loss: 0.228282, acc.: 57.81%] [G loss: 0.455556]\n",
      "epoch:16 step:15057 [D loss: 0.237790, acc.: 57.03%] [G loss: 0.404796]\n",
      "epoch:16 step:15058 [D loss: 0.224290, acc.: 68.75%] [G loss: 0.451712]\n",
      "epoch:16 step:15059 [D loss: 0.228309, acc.: 63.28%] [G loss: 0.426257]\n",
      "epoch:16 step:15060 [D loss: 0.229404, acc.: 60.16%] [G loss: 0.447933]\n",
      "epoch:16 step:15061 [D loss: 0.181830, acc.: 71.09%] [G loss: 0.493372]\n",
      "epoch:16 step:15062 [D loss: 0.193696, acc.: 67.97%] [G loss: 0.472177]\n",
      "epoch:16 step:15063 [D loss: 0.249965, acc.: 58.59%] [G loss: 0.476445]\n",
      "epoch:16 step:15064 [D loss: 0.244016, acc.: 57.03%] [G loss: 0.391978]\n",
      "epoch:16 step:15065 [D loss: 0.229621, acc.: 63.28%] [G loss: 0.426662]\n",
      "epoch:16 step:15066 [D loss: 0.211403, acc.: 71.09%] [G loss: 0.465526]\n",
      "epoch:16 step:15067 [D loss: 0.222895, acc.: 64.84%] [G loss: 0.415290]\n",
      "epoch:16 step:15068 [D loss: 0.215127, acc.: 64.84%] [G loss: 0.467868]\n",
      "epoch:16 step:15069 [D loss: 0.195033, acc.: 68.75%] [G loss: 0.470184]\n",
      "epoch:16 step:15070 [D loss: 0.271853, acc.: 57.81%] [G loss: 0.461433]\n",
      "epoch:16 step:15071 [D loss: 0.246772, acc.: 61.72%] [G loss: 0.404950]\n",
      "epoch:16 step:15072 [D loss: 0.220034, acc.: 63.28%] [G loss: 0.448445]\n",
      "epoch:16 step:15073 [D loss: 0.255106, acc.: 57.03%] [G loss: 0.432310]\n",
      "epoch:16 step:15074 [D loss: 0.228429, acc.: 63.28%] [G loss: 0.425906]\n",
      "epoch:16 step:15075 [D loss: 0.217426, acc.: 63.28%] [G loss: 0.443325]\n",
      "epoch:16 step:15076 [D loss: 0.217446, acc.: 66.41%] [G loss: 0.466139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15077 [D loss: 0.229528, acc.: 61.72%] [G loss: 0.430788]\n",
      "epoch:16 step:15078 [D loss: 0.228726, acc.: 62.50%] [G loss: 0.436575]\n",
      "epoch:16 step:15079 [D loss: 0.216000, acc.: 65.62%] [G loss: 0.420551]\n",
      "epoch:16 step:15080 [D loss: 0.217863, acc.: 64.84%] [G loss: 0.432375]\n",
      "epoch:16 step:15081 [D loss: 0.226309, acc.: 63.28%] [G loss: 0.453607]\n",
      "epoch:16 step:15082 [D loss: 0.235005, acc.: 60.16%] [G loss: 0.413413]\n",
      "epoch:16 step:15083 [D loss: 0.232160, acc.: 64.06%] [G loss: 0.438583]\n",
      "epoch:16 step:15084 [D loss: 0.205463, acc.: 68.75%] [G loss: 0.474368]\n",
      "epoch:16 step:15085 [D loss: 0.211671, acc.: 67.19%] [G loss: 0.447519]\n",
      "epoch:16 step:15086 [D loss: 0.245693, acc.: 57.03%] [G loss: 0.486149]\n",
      "epoch:16 step:15087 [D loss: 0.227269, acc.: 57.81%] [G loss: 0.509772]\n",
      "epoch:16 step:15088 [D loss: 0.246748, acc.: 54.69%] [G loss: 0.407495]\n",
      "epoch:16 step:15089 [D loss: 0.181167, acc.: 75.00%] [G loss: 0.506212]\n",
      "epoch:16 step:15090 [D loss: 0.215704, acc.: 64.06%] [G loss: 0.481039]\n",
      "epoch:16 step:15091 [D loss: 0.236512, acc.: 69.53%] [G loss: 0.460197]\n",
      "epoch:16 step:15092 [D loss: 0.200523, acc.: 71.09%] [G loss: 0.535864]\n",
      "epoch:16 step:15093 [D loss: 0.235585, acc.: 56.25%] [G loss: 0.483980]\n",
      "epoch:16 step:15094 [D loss: 0.230980, acc.: 58.59%] [G loss: 0.406110]\n",
      "epoch:16 step:15095 [D loss: 0.236770, acc.: 59.38%] [G loss: 0.392559]\n",
      "epoch:16 step:15096 [D loss: 0.270075, acc.: 52.34%] [G loss: 0.424743]\n",
      "epoch:16 step:15097 [D loss: 0.230893, acc.: 59.38%] [G loss: 0.426164]\n",
      "epoch:16 step:15098 [D loss: 0.211296, acc.: 70.31%] [G loss: 0.426461]\n",
      "epoch:16 step:15099 [D loss: 0.215376, acc.: 67.97%] [G loss: 0.443465]\n",
      "epoch:16 step:15100 [D loss: 0.257710, acc.: 60.94%] [G loss: 0.450010]\n",
      "epoch:16 step:15101 [D loss: 0.267830, acc.: 52.34%] [G loss: 0.452769]\n",
      "epoch:16 step:15102 [D loss: 0.242262, acc.: 60.16%] [G loss: 0.420444]\n",
      "epoch:16 step:15103 [D loss: 0.206129, acc.: 66.41%] [G loss: 0.436824]\n",
      "epoch:16 step:15104 [D loss: 0.203432, acc.: 70.31%] [G loss: 0.449472]\n",
      "epoch:16 step:15105 [D loss: 0.233642, acc.: 53.12%] [G loss: 0.455043]\n",
      "epoch:16 step:15106 [D loss: 0.217405, acc.: 70.31%] [G loss: 0.421785]\n",
      "epoch:16 step:15107 [D loss: 0.202915, acc.: 70.31%] [G loss: 0.491471]\n",
      "epoch:16 step:15108 [D loss: 0.219938, acc.: 58.59%] [G loss: 0.485437]\n",
      "epoch:16 step:15109 [D loss: 0.215697, acc.: 58.59%] [G loss: 0.508088]\n",
      "epoch:16 step:15110 [D loss: 0.235977, acc.: 60.94%] [G loss: 0.447785]\n",
      "epoch:16 step:15111 [D loss: 0.166840, acc.: 77.34%] [G loss: 0.489865]\n",
      "epoch:16 step:15112 [D loss: 0.253037, acc.: 57.81%] [G loss: 0.472079]\n",
      "epoch:16 step:15113 [D loss: 0.217199, acc.: 64.06%] [G loss: 0.433865]\n",
      "epoch:16 step:15114 [D loss: 0.164016, acc.: 81.25%] [G loss: 0.458718]\n",
      "epoch:16 step:15115 [D loss: 0.237658, acc.: 59.38%] [G loss: 0.433815]\n",
      "epoch:16 step:15116 [D loss: 0.261840, acc.: 54.69%] [G loss: 0.467765]\n",
      "epoch:16 step:15117 [D loss: 0.235954, acc.: 61.72%] [G loss: 0.466824]\n",
      "epoch:16 step:15118 [D loss: 0.202573, acc.: 71.88%] [G loss: 0.446765]\n",
      "epoch:16 step:15119 [D loss: 0.237441, acc.: 56.25%] [G loss: 0.431376]\n",
      "epoch:16 step:15120 [D loss: 0.230774, acc.: 58.59%] [G loss: 0.426622]\n",
      "epoch:16 step:15121 [D loss: 0.227692, acc.: 65.62%] [G loss: 0.413194]\n",
      "epoch:16 step:15122 [D loss: 0.209316, acc.: 71.88%] [G loss: 0.466839]\n",
      "epoch:16 step:15123 [D loss: 0.219363, acc.: 60.16%] [G loss: 0.443345]\n",
      "epoch:16 step:15124 [D loss: 0.217795, acc.: 66.41%] [G loss: 0.454697]\n",
      "epoch:16 step:15125 [D loss: 0.250006, acc.: 56.25%] [G loss: 0.447288]\n",
      "epoch:16 step:15126 [D loss: 0.204451, acc.: 64.84%] [G loss: 0.477601]\n",
      "epoch:16 step:15127 [D loss: 0.250866, acc.: 54.69%] [G loss: 0.415748]\n",
      "epoch:16 step:15128 [D loss: 0.207937, acc.: 71.88%] [G loss: 0.459798]\n",
      "epoch:16 step:15129 [D loss: 0.266272, acc.: 50.78%] [G loss: 0.430906]\n",
      "epoch:16 step:15130 [D loss: 0.261680, acc.: 50.78%] [G loss: 0.414771]\n",
      "epoch:16 step:15131 [D loss: 0.228148, acc.: 61.72%] [G loss: 0.439470]\n",
      "epoch:16 step:15132 [D loss: 0.237728, acc.: 58.59%] [G loss: 0.431398]\n",
      "epoch:16 step:15133 [D loss: 0.219677, acc.: 59.38%] [G loss: 0.404557]\n",
      "epoch:16 step:15134 [D loss: 0.206310, acc.: 67.19%] [G loss: 0.411248]\n",
      "epoch:16 step:15135 [D loss: 0.249862, acc.: 55.47%] [G loss: 0.408504]\n",
      "epoch:16 step:15136 [D loss: 0.231817, acc.: 61.72%] [G loss: 0.448554]\n",
      "epoch:16 step:15137 [D loss: 0.217383, acc.: 62.50%] [G loss: 0.443459]\n",
      "epoch:16 step:15138 [D loss: 0.218731, acc.: 64.06%] [G loss: 0.403966]\n",
      "epoch:16 step:15139 [D loss: 0.234817, acc.: 59.38%] [G loss: 0.417044]\n",
      "epoch:16 step:15140 [D loss: 0.221048, acc.: 61.72%] [G loss: 0.431481]\n",
      "epoch:16 step:15141 [D loss: 0.189544, acc.: 71.09%] [G loss: 0.473559]\n",
      "epoch:16 step:15142 [D loss: 0.213829, acc.: 64.84%] [G loss: 0.439233]\n",
      "epoch:16 step:15143 [D loss: 0.214717, acc.: 68.75%] [G loss: 0.441015]\n",
      "epoch:16 step:15144 [D loss: 0.224429, acc.: 65.62%] [G loss: 0.419404]\n",
      "epoch:16 step:15145 [D loss: 0.255159, acc.: 53.12%] [G loss: 0.440368]\n",
      "epoch:16 step:15146 [D loss: 0.227355, acc.: 65.62%] [G loss: 0.420349]\n",
      "epoch:16 step:15147 [D loss: 0.215625, acc.: 63.28%] [G loss: 0.452498]\n",
      "epoch:16 step:15148 [D loss: 0.220317, acc.: 65.62%] [G loss: 0.464990]\n",
      "epoch:16 step:15149 [D loss: 0.220128, acc.: 65.62%] [G loss: 0.473990]\n",
      "epoch:16 step:15150 [D loss: 0.239477, acc.: 60.94%] [G loss: 0.432658]\n",
      "epoch:16 step:15151 [D loss: 0.213930, acc.: 69.53%] [G loss: 0.462284]\n",
      "epoch:16 step:15152 [D loss: 0.264471, acc.: 55.47%] [G loss: 0.444994]\n",
      "epoch:16 step:15153 [D loss: 0.270557, acc.: 53.12%] [G loss: 0.397822]\n",
      "epoch:16 step:15154 [D loss: 0.209153, acc.: 67.97%] [G loss: 0.478659]\n",
      "epoch:16 step:15155 [D loss: 0.218533, acc.: 61.72%] [G loss: 0.459736]\n",
      "epoch:16 step:15156 [D loss: 0.233522, acc.: 61.72%] [G loss: 0.418231]\n",
      "epoch:16 step:15157 [D loss: 0.229529, acc.: 64.84%] [G loss: 0.438870]\n",
      "epoch:16 step:15158 [D loss: 0.217752, acc.: 59.38%] [G loss: 0.421696]\n",
      "epoch:16 step:15159 [D loss: 0.198598, acc.: 69.53%] [G loss: 0.443204]\n",
      "epoch:16 step:15160 [D loss: 0.191478, acc.: 72.66%] [G loss: 0.441595]\n",
      "epoch:16 step:15161 [D loss: 0.248820, acc.: 59.38%] [G loss: 0.436057]\n",
      "epoch:16 step:15162 [D loss: 0.251961, acc.: 53.91%] [G loss: 0.389454]\n",
      "epoch:16 step:15163 [D loss: 0.227105, acc.: 57.81%] [G loss: 0.414116]\n",
      "epoch:16 step:15164 [D loss: 0.189468, acc.: 69.53%] [G loss: 0.441320]\n",
      "epoch:16 step:15165 [D loss: 0.208554, acc.: 66.41%] [G loss: 0.442333]\n",
      "epoch:16 step:15166 [D loss: 0.225614, acc.: 61.72%] [G loss: 0.411083]\n",
      "epoch:16 step:15167 [D loss: 0.228479, acc.: 64.84%] [G loss: 0.459418]\n",
      "epoch:16 step:15168 [D loss: 0.238005, acc.: 57.03%] [G loss: 0.438733]\n",
      "epoch:16 step:15169 [D loss: 0.221129, acc.: 60.94%] [G loss: 0.432340]\n",
      "epoch:16 step:15170 [D loss: 0.222979, acc.: 64.06%] [G loss: 0.444989]\n",
      "epoch:16 step:15171 [D loss: 0.237710, acc.: 56.25%] [G loss: 0.423031]\n",
      "epoch:16 step:15172 [D loss: 0.229360, acc.: 63.28%] [G loss: 0.415889]\n",
      "epoch:16 step:15173 [D loss: 0.218355, acc.: 60.94%] [G loss: 0.409280]\n",
      "epoch:16 step:15174 [D loss: 0.248767, acc.: 55.47%] [G loss: 0.424755]\n",
      "epoch:16 step:15175 [D loss: 0.234477, acc.: 59.38%] [G loss: 0.433168]\n",
      "epoch:16 step:15176 [D loss: 0.218609, acc.: 67.19%] [G loss: 0.458445]\n",
      "epoch:16 step:15177 [D loss: 0.226867, acc.: 60.16%] [G loss: 0.523817]\n",
      "epoch:16 step:15178 [D loss: 0.263713, acc.: 56.25%] [G loss: 0.444432]\n",
      "epoch:16 step:15179 [D loss: 0.224627, acc.: 64.84%] [G loss: 0.446780]\n",
      "epoch:16 step:15180 [D loss: 0.243611, acc.: 60.16%] [G loss: 0.405686]\n",
      "epoch:16 step:15181 [D loss: 0.255426, acc.: 59.38%] [G loss: 0.398732]\n",
      "epoch:16 step:15182 [D loss: 0.224384, acc.: 62.50%] [G loss: 0.455248]\n",
      "epoch:16 step:15183 [D loss: 0.206831, acc.: 67.97%] [G loss: 0.443469]\n",
      "epoch:16 step:15184 [D loss: 0.211235, acc.: 67.97%] [G loss: 0.460753]\n",
      "epoch:16 step:15185 [D loss: 0.220889, acc.: 65.62%] [G loss: 0.461232]\n",
      "epoch:16 step:15186 [D loss: 0.199996, acc.: 71.88%] [G loss: 0.463220]\n",
      "epoch:16 step:15187 [D loss: 0.245945, acc.: 60.94%] [G loss: 0.422035]\n",
      "epoch:16 step:15188 [D loss: 0.237563, acc.: 60.94%] [G loss: 0.467210]\n",
      "epoch:16 step:15189 [D loss: 0.228978, acc.: 61.72%] [G loss: 0.451753]\n",
      "epoch:16 step:15190 [D loss: 0.209419, acc.: 69.53%] [G loss: 0.463944]\n",
      "epoch:16 step:15191 [D loss: 0.237897, acc.: 58.59%] [G loss: 0.446321]\n",
      "epoch:16 step:15192 [D loss: 0.249211, acc.: 54.69%] [G loss: 0.425274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15193 [D loss: 0.230920, acc.: 64.06%] [G loss: 0.428148]\n",
      "epoch:16 step:15194 [D loss: 0.219711, acc.: 67.19%] [G loss: 0.437276]\n",
      "epoch:16 step:15195 [D loss: 0.241753, acc.: 59.38%] [G loss: 0.425045]\n",
      "epoch:16 step:15196 [D loss: 0.222316, acc.: 63.28%] [G loss: 0.420814]\n",
      "epoch:16 step:15197 [D loss: 0.207774, acc.: 65.62%] [G loss: 0.434839]\n",
      "epoch:16 step:15198 [D loss: 0.212078, acc.: 64.84%] [G loss: 0.452982]\n",
      "epoch:16 step:15199 [D loss: 0.210506, acc.: 64.06%] [G loss: 0.459726]\n",
      "epoch:16 step:15200 [D loss: 0.197572, acc.: 71.09%] [G loss: 0.502597]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.941229\n",
      "FID: 7.539829\n",
      "0 = 11.856942341232276\n",
      "1 = 0.04393853029261337\n",
      "2 = 0.8754500150680542\n",
      "3 = 0.8698999881744385\n",
      "4 = 0.8809999823570251\n",
      "5 = 0.8796643018722534\n",
      "6 = 0.8698999881744385\n",
      "7 = 5.942952509307866\n",
      "8 = 0.06099803558688578\n",
      "9 = 0.6866999864578247\n",
      "10 = 0.6913999915122986\n",
      "11 = 0.6819999814033508\n",
      "12 = 0.6849613785743713\n",
      "13 = 0.6913999915122986\n",
      "14 = 7.941302299499512\n",
      "15 = 9.580909729003906\n",
      "16 = 0.09890620410442352\n",
      "17 = 7.941229343414307\n",
      "18 = 7.539828777313232\n",
      "epoch:16 step:15201 [D loss: 0.183550, acc.: 75.78%] [G loss: 0.513615]\n",
      "epoch:16 step:15202 [D loss: 0.262911, acc.: 56.25%] [G loss: 0.422694]\n",
      "epoch:16 step:15203 [D loss: 0.257688, acc.: 57.81%] [G loss: 0.418425]\n",
      "epoch:16 step:15204 [D loss: 0.238372, acc.: 61.72%] [G loss: 0.436991]\n",
      "epoch:16 step:15205 [D loss: 0.230668, acc.: 61.72%] [G loss: 0.415886]\n",
      "epoch:16 step:15206 [D loss: 0.264707, acc.: 46.88%] [G loss: 0.433017]\n",
      "epoch:16 step:15207 [D loss: 0.259735, acc.: 55.47%] [G loss: 0.428393]\n",
      "epoch:16 step:15208 [D loss: 0.217373, acc.: 62.50%] [G loss: 0.451562]\n",
      "epoch:16 step:15209 [D loss: 0.201931, acc.: 71.88%] [G loss: 0.433658]\n",
      "epoch:16 step:15210 [D loss: 0.209942, acc.: 67.97%] [G loss: 0.418541]\n",
      "epoch:16 step:15211 [D loss: 0.162914, acc.: 79.69%] [G loss: 0.477887]\n",
      "epoch:16 step:15212 [D loss: 0.291091, acc.: 50.78%] [G loss: 0.447198]\n",
      "epoch:16 step:15213 [D loss: 0.206513, acc.: 68.75%] [G loss: 0.472609]\n",
      "epoch:16 step:15214 [D loss: 0.213113, acc.: 63.28%] [G loss: 0.515712]\n",
      "epoch:16 step:15215 [D loss: 0.219366, acc.: 66.41%] [G loss: 0.486264]\n",
      "epoch:16 step:15216 [D loss: 0.248458, acc.: 62.50%] [G loss: 0.456867]\n",
      "epoch:16 step:15217 [D loss: 0.245776, acc.: 60.16%] [G loss: 0.431528]\n",
      "epoch:16 step:15218 [D loss: 0.236608, acc.: 58.59%] [G loss: 0.413680]\n",
      "epoch:16 step:15219 [D loss: 0.217842, acc.: 69.53%] [G loss: 0.420526]\n",
      "epoch:16 step:15220 [D loss: 0.248862, acc.: 60.16%] [G loss: 0.408945]\n",
      "epoch:16 step:15221 [D loss: 0.203583, acc.: 73.44%] [G loss: 0.418771]\n",
      "epoch:16 step:15222 [D loss: 0.193982, acc.: 67.19%] [G loss: 0.466017]\n",
      "epoch:16 step:15223 [D loss: 0.156202, acc.: 78.91%] [G loss: 0.541502]\n",
      "epoch:16 step:15224 [D loss: 0.183881, acc.: 75.00%] [G loss: 0.524415]\n",
      "epoch:16 step:15225 [D loss: 0.246216, acc.: 59.38%] [G loss: 0.475648]\n",
      "epoch:16 step:15226 [D loss: 0.255753, acc.: 55.47%] [G loss: 0.416248]\n",
      "epoch:16 step:15227 [D loss: 0.214007, acc.: 62.50%] [G loss: 0.423253]\n",
      "epoch:16 step:15228 [D loss: 0.220038, acc.: 65.62%] [G loss: 0.429675]\n",
      "epoch:16 step:15229 [D loss: 0.209138, acc.: 68.75%] [G loss: 0.442071]\n",
      "epoch:16 step:15230 [D loss: 0.208926, acc.: 64.84%] [G loss: 0.411918]\n",
      "epoch:16 step:15231 [D loss: 0.203018, acc.: 67.97%] [G loss: 0.439389]\n",
      "epoch:16 step:15232 [D loss: 0.219870, acc.: 64.06%] [G loss: 0.449444]\n",
      "epoch:16 step:15233 [D loss: 0.205579, acc.: 67.97%] [G loss: 0.471610]\n",
      "epoch:16 step:15234 [D loss: 0.221755, acc.: 62.50%] [G loss: 0.475802]\n",
      "epoch:16 step:15235 [D loss: 0.211465, acc.: 66.41%] [G loss: 0.457695]\n",
      "epoch:16 step:15236 [D loss: 0.223679, acc.: 59.38%] [G loss: 0.429875]\n",
      "epoch:16 step:15237 [D loss: 0.207008, acc.: 71.09%] [G loss: 0.481148]\n",
      "epoch:16 step:15238 [D loss: 0.198503, acc.: 71.09%] [G loss: 0.427460]\n",
      "epoch:16 step:15239 [D loss: 0.252506, acc.: 55.47%] [G loss: 0.426242]\n",
      "epoch:16 step:15240 [D loss: 0.201454, acc.: 67.19%] [G loss: 0.462783]\n",
      "epoch:16 step:15241 [D loss: 0.256670, acc.: 58.59%] [G loss: 0.450323]\n",
      "epoch:16 step:15242 [D loss: 0.249143, acc.: 56.25%] [G loss: 0.398844]\n",
      "epoch:16 step:15243 [D loss: 0.253355, acc.: 51.56%] [G loss: 0.433576]\n",
      "epoch:16 step:15244 [D loss: 0.238835, acc.: 66.41%] [G loss: 0.444128]\n",
      "epoch:16 step:15245 [D loss: 0.207475, acc.: 70.31%] [G loss: 0.454971]\n",
      "epoch:16 step:15246 [D loss: 0.218385, acc.: 62.50%] [G loss: 0.457088]\n",
      "epoch:16 step:15247 [D loss: 0.220560, acc.: 58.59%] [G loss: 0.432907]\n",
      "epoch:16 step:15248 [D loss: 0.262487, acc.: 47.66%] [G loss: 0.424391]\n",
      "epoch:16 step:15249 [D loss: 0.215649, acc.: 67.97%] [G loss: 0.458891]\n",
      "epoch:16 step:15250 [D loss: 0.209868, acc.: 65.62%] [G loss: 0.456260]\n",
      "epoch:16 step:15251 [D loss: 0.244472, acc.: 56.25%] [G loss: 0.387699]\n",
      "epoch:16 step:15252 [D loss: 0.232604, acc.: 63.28%] [G loss: 0.462728]\n",
      "epoch:16 step:15253 [D loss: 0.215830, acc.: 62.50%] [G loss: 0.426499]\n",
      "epoch:16 step:15254 [D loss: 0.215537, acc.: 59.38%] [G loss: 0.424636]\n",
      "epoch:16 step:15255 [D loss: 0.262932, acc.: 51.56%] [G loss: 0.439572]\n",
      "epoch:16 step:15256 [D loss: 0.196883, acc.: 67.19%] [G loss: 0.511052]\n",
      "epoch:16 step:15257 [D loss: 0.240351, acc.: 59.38%] [G loss: 0.438816]\n",
      "epoch:16 step:15258 [D loss: 0.255567, acc.: 54.69%] [G loss: 0.453165]\n",
      "epoch:16 step:15259 [D loss: 0.213372, acc.: 68.75%] [G loss: 0.424957]\n",
      "epoch:16 step:15260 [D loss: 0.229828, acc.: 64.06%] [G loss: 0.422105]\n",
      "epoch:16 step:15261 [D loss: 0.211299, acc.: 69.53%] [G loss: 0.451780]\n",
      "epoch:16 step:15262 [D loss: 0.233805, acc.: 64.06%] [G loss: 0.446151]\n",
      "epoch:16 step:15263 [D loss: 0.196173, acc.: 71.88%] [G loss: 0.477918]\n",
      "epoch:16 step:15264 [D loss: 0.227023, acc.: 65.62%] [G loss: 0.463523]\n",
      "epoch:16 step:15265 [D loss: 0.205481, acc.: 67.97%] [G loss: 0.455147]\n",
      "epoch:16 step:15266 [D loss: 0.210830, acc.: 67.97%] [G loss: 0.427061]\n",
      "epoch:16 step:15267 [D loss: 0.210327, acc.: 65.62%] [G loss: 0.447755]\n",
      "epoch:16 step:15268 [D loss: 0.221239, acc.: 64.84%] [G loss: 0.446987]\n",
      "epoch:16 step:15269 [D loss: 0.267252, acc.: 53.12%] [G loss: 0.435966]\n",
      "epoch:16 step:15270 [D loss: 0.246234, acc.: 55.47%] [G loss: 0.401337]\n",
      "epoch:16 step:15271 [D loss: 0.213193, acc.: 66.41%] [G loss: 0.466611]\n",
      "epoch:16 step:15272 [D loss: 0.196326, acc.: 70.31%] [G loss: 0.472203]\n",
      "epoch:16 step:15273 [D loss: 0.245390, acc.: 64.06%] [G loss: 0.423260]\n",
      "epoch:16 step:15274 [D loss: 0.226762, acc.: 60.94%] [G loss: 0.381851]\n",
      "epoch:16 step:15275 [D loss: 0.205808, acc.: 68.75%] [G loss: 0.436660]\n",
      "epoch:16 step:15276 [D loss: 0.225277, acc.: 60.16%] [G loss: 0.431256]\n",
      "epoch:16 step:15277 [D loss: 0.215332, acc.: 63.28%] [G loss: 0.417666]\n",
      "epoch:16 step:15278 [D loss: 0.206882, acc.: 64.84%] [G loss: 0.443886]\n",
      "epoch:16 step:15279 [D loss: 0.203309, acc.: 69.53%] [G loss: 0.425598]\n",
      "epoch:16 step:15280 [D loss: 0.219622, acc.: 66.41%] [G loss: 0.473833]\n",
      "epoch:16 step:15281 [D loss: 0.240268, acc.: 58.59%] [G loss: 0.447146]\n",
      "epoch:16 step:15282 [D loss: 0.225323, acc.: 67.97%] [G loss: 0.436893]\n",
      "epoch:16 step:15283 [D loss: 0.251705, acc.: 59.38%] [G loss: 0.408127]\n",
      "epoch:16 step:15284 [D loss: 0.227755, acc.: 60.16%] [G loss: 0.425633]\n",
      "epoch:16 step:15285 [D loss: 0.219028, acc.: 67.19%] [G loss: 0.415849]\n",
      "epoch:16 step:15286 [D loss: 0.240146, acc.: 60.16%] [G loss: 0.447640]\n",
      "epoch:16 step:15287 [D loss: 0.235241, acc.: 58.59%] [G loss: 0.435503]\n",
      "epoch:16 step:15288 [D loss: 0.211839, acc.: 64.06%] [G loss: 0.413508]\n",
      "epoch:16 step:15289 [D loss: 0.233221, acc.: 62.50%] [G loss: 0.432067]\n",
      "epoch:16 step:15290 [D loss: 0.180132, acc.: 78.12%] [G loss: 0.479776]\n",
      "epoch:16 step:15291 [D loss: 0.212998, acc.: 65.62%] [G loss: 0.451805]\n",
      "epoch:16 step:15292 [D loss: 0.207827, acc.: 71.09%] [G loss: 0.455830]\n",
      "epoch:16 step:15293 [D loss: 0.259880, acc.: 53.12%] [G loss: 0.450875]\n",
      "epoch:16 step:15294 [D loss: 0.235654, acc.: 62.50%] [G loss: 0.441603]\n",
      "epoch:16 step:15295 [D loss: 0.236162, acc.: 65.62%] [G loss: 0.473235]\n",
      "epoch:16 step:15296 [D loss: 0.215786, acc.: 65.62%] [G loss: 0.459141]\n",
      "epoch:16 step:15297 [D loss: 0.235151, acc.: 61.72%] [G loss: 0.439713]\n",
      "epoch:16 step:15298 [D loss: 0.219330, acc.: 62.50%] [G loss: 0.460467]\n",
      "epoch:16 step:15299 [D loss: 0.210431, acc.: 67.97%] [G loss: 0.485969]\n",
      "epoch:16 step:15300 [D loss: 0.229653, acc.: 60.16%] [G loss: 0.426320]\n",
      "epoch:16 step:15301 [D loss: 0.217119, acc.: 61.72%] [G loss: 0.433225]\n",
      "epoch:16 step:15302 [D loss: 0.211910, acc.: 63.28%] [G loss: 0.428102]\n",
      "epoch:16 step:15303 [D loss: 0.224230, acc.: 61.72%] [G loss: 0.429173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15304 [D loss: 0.180654, acc.: 74.22%] [G loss: 0.440229]\n",
      "epoch:16 step:15305 [D loss: 0.208707, acc.: 65.62%] [G loss: 0.492522]\n",
      "epoch:16 step:15306 [D loss: 0.198766, acc.: 66.41%] [G loss: 0.484012]\n",
      "epoch:16 step:15307 [D loss: 0.191431, acc.: 71.88%] [G loss: 0.491266]\n",
      "epoch:16 step:15308 [D loss: 0.279186, acc.: 46.88%] [G loss: 0.435930]\n",
      "epoch:16 step:15309 [D loss: 0.246573, acc.: 58.59%] [G loss: 0.389646]\n",
      "epoch:16 step:15310 [D loss: 0.206544, acc.: 69.53%] [G loss: 0.406264]\n",
      "epoch:16 step:15311 [D loss: 0.213780, acc.: 67.97%] [G loss: 0.447027]\n",
      "epoch:16 step:15312 [D loss: 0.210881, acc.: 69.53%] [G loss: 0.462037]\n",
      "epoch:16 step:15313 [D loss: 0.210899, acc.: 67.97%] [G loss: 0.469213]\n",
      "epoch:16 step:15314 [D loss: 0.238777, acc.: 60.16%] [G loss: 0.448568]\n",
      "epoch:16 step:15315 [D loss: 0.252233, acc.: 59.38%] [G loss: 0.412686]\n",
      "epoch:16 step:15316 [D loss: 0.195873, acc.: 71.88%] [G loss: 0.439562]\n",
      "epoch:16 step:15317 [D loss: 0.230775, acc.: 63.28%] [G loss: 0.435561]\n",
      "epoch:16 step:15318 [D loss: 0.217001, acc.: 60.94%] [G loss: 0.459832]\n",
      "epoch:16 step:15319 [D loss: 0.214026, acc.: 64.06%] [G loss: 0.469879]\n",
      "epoch:16 step:15320 [D loss: 0.214353, acc.: 61.72%] [G loss: 0.426062]\n",
      "epoch:16 step:15321 [D loss: 0.243603, acc.: 57.81%] [G loss: 0.452936]\n",
      "epoch:16 step:15322 [D loss: 0.215516, acc.: 60.94%] [G loss: 0.458684]\n",
      "epoch:16 step:15323 [D loss: 0.201451, acc.: 69.53%] [G loss: 0.481939]\n",
      "epoch:16 step:15324 [D loss: 0.195380, acc.: 66.41%] [G loss: 0.479643]\n",
      "epoch:16 step:15325 [D loss: 0.213860, acc.: 62.50%] [G loss: 0.408377]\n",
      "epoch:16 step:15326 [D loss: 0.220258, acc.: 66.41%] [G loss: 0.445403]\n",
      "epoch:16 step:15327 [D loss: 0.219746, acc.: 63.28%] [G loss: 0.416679]\n",
      "epoch:16 step:15328 [D loss: 0.219462, acc.: 63.28%] [G loss: 0.475700]\n",
      "epoch:16 step:15329 [D loss: 0.237588, acc.: 62.50%] [G loss: 0.443240]\n",
      "epoch:16 step:15330 [D loss: 0.239152, acc.: 56.25%] [G loss: 0.405715]\n",
      "epoch:16 step:15331 [D loss: 0.201299, acc.: 71.88%] [G loss: 0.452554]\n",
      "epoch:16 step:15332 [D loss: 0.219049, acc.: 59.38%] [G loss: 0.431068]\n",
      "epoch:16 step:15333 [D loss: 0.291017, acc.: 50.00%] [G loss: 0.398449]\n",
      "epoch:16 step:15334 [D loss: 0.250654, acc.: 52.34%] [G loss: 0.418738]\n",
      "epoch:16 step:15335 [D loss: 0.221039, acc.: 64.84%] [G loss: 0.456168]\n",
      "epoch:16 step:15336 [D loss: 0.217427, acc.: 62.50%] [G loss: 0.469789]\n",
      "epoch:16 step:15337 [D loss: 0.224099, acc.: 57.81%] [G loss: 0.475478]\n",
      "epoch:16 step:15338 [D loss: 0.183808, acc.: 70.31%] [G loss: 0.507209]\n",
      "epoch:16 step:15339 [D loss: 0.179110, acc.: 75.00%] [G loss: 0.508976]\n",
      "epoch:16 step:15340 [D loss: 0.263785, acc.: 57.81%] [G loss: 0.371114]\n",
      "epoch:16 step:15341 [D loss: 0.298083, acc.: 42.97%] [G loss: 0.372712]\n",
      "epoch:16 step:15342 [D loss: 0.216394, acc.: 65.62%] [G loss: 0.414474]\n",
      "epoch:16 step:15343 [D loss: 0.210680, acc.: 69.53%] [G loss: 0.465027]\n",
      "epoch:16 step:15344 [D loss: 0.258130, acc.: 57.81%] [G loss: 0.440557]\n",
      "epoch:16 step:15345 [D loss: 0.212693, acc.: 64.84%] [G loss: 0.456051]\n",
      "epoch:16 step:15346 [D loss: 0.187373, acc.: 74.22%] [G loss: 0.451850]\n",
      "epoch:16 step:15347 [D loss: 0.240231, acc.: 62.50%] [G loss: 0.438629]\n",
      "epoch:16 step:15348 [D loss: 0.228455, acc.: 61.72%] [G loss: 0.443070]\n",
      "epoch:16 step:15349 [D loss: 0.192825, acc.: 73.44%] [G loss: 0.486340]\n",
      "epoch:16 step:15350 [D loss: 0.207205, acc.: 69.53%] [G loss: 0.456048]\n",
      "epoch:16 step:15351 [D loss: 0.225626, acc.: 60.16%] [G loss: 0.455898]\n",
      "epoch:16 step:15352 [D loss: 0.207548, acc.: 65.62%] [G loss: 0.452221]\n",
      "epoch:16 step:15353 [D loss: 0.195987, acc.: 68.75%] [G loss: 0.461214]\n",
      "epoch:16 step:15354 [D loss: 0.233369, acc.: 53.12%] [G loss: 0.462926]\n",
      "epoch:16 step:15355 [D loss: 0.236024, acc.: 56.25%] [G loss: 0.422221]\n",
      "epoch:16 step:15356 [D loss: 0.211293, acc.: 63.28%] [G loss: 0.442460]\n",
      "epoch:16 step:15357 [D loss: 0.232233, acc.: 60.94%] [G loss: 0.417165]\n",
      "epoch:16 step:15358 [D loss: 0.220067, acc.: 66.41%] [G loss: 0.429702]\n",
      "epoch:16 step:15359 [D loss: 0.194660, acc.: 72.66%] [G loss: 0.430326]\n",
      "epoch:16 step:15360 [D loss: 0.217148, acc.: 64.06%] [G loss: 0.439892]\n",
      "epoch:16 step:15361 [D loss: 0.236836, acc.: 55.47%] [G loss: 0.423724]\n",
      "epoch:16 step:15362 [D loss: 0.204781, acc.: 66.41%] [G loss: 0.443253]\n",
      "epoch:16 step:15363 [D loss: 0.201524, acc.: 72.66%] [G loss: 0.494955]\n",
      "epoch:16 step:15364 [D loss: 0.227364, acc.: 62.50%] [G loss: 0.439854]\n",
      "epoch:16 step:15365 [D loss: 0.254092, acc.: 51.56%] [G loss: 0.425387]\n",
      "epoch:16 step:15366 [D loss: 0.218017, acc.: 62.50%] [G loss: 0.471733]\n",
      "epoch:16 step:15367 [D loss: 0.223607, acc.: 64.06%] [G loss: 0.452008]\n",
      "epoch:16 step:15368 [D loss: 0.243061, acc.: 57.81%] [G loss: 0.432241]\n",
      "epoch:16 step:15369 [D loss: 0.259492, acc.: 50.78%] [G loss: 0.428527]\n",
      "epoch:16 step:15370 [D loss: 0.235984, acc.: 57.81%] [G loss: 0.403887]\n",
      "epoch:16 step:15371 [D loss: 0.222156, acc.: 63.28%] [G loss: 0.446149]\n",
      "epoch:16 step:15372 [D loss: 0.239178, acc.: 58.59%] [G loss: 0.409108]\n",
      "epoch:16 step:15373 [D loss: 0.199687, acc.: 64.84%] [G loss: 0.420688]\n",
      "epoch:16 step:15374 [D loss: 0.227873, acc.: 62.50%] [G loss: 0.410908]\n",
      "epoch:16 step:15375 [D loss: 0.226690, acc.: 62.50%] [G loss: 0.443121]\n",
      "epoch:16 step:15376 [D loss: 0.214831, acc.: 65.62%] [G loss: 0.417127]\n",
      "epoch:16 step:15377 [D loss: 0.216000, acc.: 61.72%] [G loss: 0.422718]\n",
      "epoch:16 step:15378 [D loss: 0.236257, acc.: 60.16%] [G loss: 0.457683]\n",
      "epoch:16 step:15379 [D loss: 0.235797, acc.: 64.06%] [G loss: 0.413588]\n",
      "epoch:16 step:15380 [D loss: 0.228614, acc.: 62.50%] [G loss: 0.448711]\n",
      "epoch:16 step:15381 [D loss: 0.208566, acc.: 65.62%] [G loss: 0.463938]\n",
      "epoch:16 step:15382 [D loss: 0.246232, acc.: 57.03%] [G loss: 0.447895]\n",
      "epoch:16 step:15383 [D loss: 0.226255, acc.: 67.97%] [G loss: 0.441312]\n",
      "epoch:16 step:15384 [D loss: 0.226067, acc.: 60.94%] [G loss: 0.461694]\n",
      "epoch:16 step:15385 [D loss: 0.248622, acc.: 57.81%] [G loss: 0.417892]\n",
      "epoch:16 step:15386 [D loss: 0.231263, acc.: 62.50%] [G loss: 0.421185]\n",
      "epoch:16 step:15387 [D loss: 0.214348, acc.: 65.62%] [G loss: 0.459757]\n",
      "epoch:16 step:15388 [D loss: 0.251236, acc.: 53.12%] [G loss: 0.461921]\n",
      "epoch:16 step:15389 [D loss: 0.218464, acc.: 67.19%] [G loss: 0.450522]\n",
      "epoch:16 step:15390 [D loss: 0.218303, acc.: 63.28%] [G loss: 0.455689]\n",
      "epoch:16 step:15391 [D loss: 0.221100, acc.: 64.84%] [G loss: 0.515027]\n",
      "epoch:16 step:15392 [D loss: 0.269253, acc.: 50.78%] [G loss: 0.435664]\n",
      "epoch:16 step:15393 [D loss: 0.235957, acc.: 60.16%] [G loss: 0.416663]\n",
      "epoch:16 step:15394 [D loss: 0.213252, acc.: 67.19%] [G loss: 0.393241]\n",
      "epoch:16 step:15395 [D loss: 0.229015, acc.: 61.72%] [G loss: 0.435270]\n",
      "epoch:16 step:15396 [D loss: 0.226490, acc.: 64.84%] [G loss: 0.469147]\n",
      "epoch:16 step:15397 [D loss: 0.214435, acc.: 71.09%] [G loss: 0.499468]\n",
      "epoch:16 step:15398 [D loss: 0.208334, acc.: 69.53%] [G loss: 0.502494]\n",
      "epoch:16 step:15399 [D loss: 0.248663, acc.: 51.56%] [G loss: 0.486280]\n",
      "epoch:16 step:15400 [D loss: 0.234495, acc.: 65.62%] [G loss: 0.444833]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.995326\n",
      "FID: 7.346269\n",
      "0 = 11.788303141784667\n",
      "1 = 0.04713621472548958\n",
      "2 = 0.8762500286102295\n",
      "3 = 0.8672000169754028\n",
      "4 = 0.8852999806404114\n",
      "5 = 0.8831856846809387\n",
      "6 = 0.8672000169754028\n",
      "7 = 5.888364469259963\n",
      "8 = 0.060478910632439646\n",
      "9 = 0.6736000180244446\n",
      "10 = 0.6751999855041504\n",
      "11 = 0.671999990940094\n",
      "12 = 0.6730462312698364\n",
      "13 = 0.6751999855041504\n",
      "14 = 7.995397090911865\n",
      "15 = 9.518735885620117\n",
      "16 = 0.11170383542776108\n",
      "17 = 7.995326042175293\n",
      "18 = 7.346268653869629\n",
      "epoch:16 step:15401 [D loss: 0.206700, acc.: 68.75%] [G loss: 0.420429]\n",
      "epoch:16 step:15402 [D loss: 0.217278, acc.: 65.62%] [G loss: 0.458027]\n",
      "epoch:16 step:15403 [D loss: 0.236613, acc.: 63.28%] [G loss: 0.387077]\n",
      "epoch:16 step:15404 [D loss: 0.234427, acc.: 57.81%] [G loss: 0.412142]\n",
      "epoch:16 step:15405 [D loss: 0.228755, acc.: 70.31%] [G loss: 0.432296]\n",
      "epoch:16 step:15406 [D loss: 0.209760, acc.: 64.84%] [G loss: 0.471437]\n",
      "epoch:16 step:15407 [D loss: 0.228895, acc.: 60.94%] [G loss: 0.457944]\n",
      "epoch:16 step:15408 [D loss: 0.197409, acc.: 69.53%] [G loss: 0.452930]\n",
      "epoch:16 step:15409 [D loss: 0.230286, acc.: 64.84%] [G loss: 0.489424]\n",
      "epoch:16 step:15410 [D loss: 0.260674, acc.: 50.00%] [G loss: 0.405024]\n",
      "epoch:16 step:15411 [D loss: 0.243627, acc.: 59.38%] [G loss: 0.427828]\n",
      "epoch:16 step:15412 [D loss: 0.236625, acc.: 64.06%] [G loss: 0.477405]\n",
      "epoch:16 step:15413 [D loss: 0.259591, acc.: 57.03%] [G loss: 0.438664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15414 [D loss: 0.238828, acc.: 58.59%] [G loss: 0.441449]\n",
      "epoch:16 step:15415 [D loss: 0.238597, acc.: 57.03%] [G loss: 0.414511]\n",
      "epoch:16 step:15416 [D loss: 0.225525, acc.: 57.81%] [G loss: 0.426430]\n",
      "epoch:16 step:15417 [D loss: 0.199548, acc.: 71.09%] [G loss: 0.442444]\n",
      "epoch:16 step:15418 [D loss: 0.252452, acc.: 60.16%] [G loss: 0.419164]\n",
      "epoch:16 step:15419 [D loss: 0.204256, acc.: 67.19%] [G loss: 0.456085]\n",
      "epoch:16 step:15420 [D loss: 0.214003, acc.: 63.28%] [G loss: 0.444611]\n",
      "epoch:16 step:15421 [D loss: 0.230539, acc.: 64.06%] [G loss: 0.455593]\n",
      "epoch:16 step:15422 [D loss: 0.196815, acc.: 71.88%] [G loss: 0.447663]\n",
      "epoch:16 step:15423 [D loss: 0.219259, acc.: 60.16%] [G loss: 0.466269]\n",
      "epoch:16 step:15424 [D loss: 0.240234, acc.: 58.59%] [G loss: 0.441982]\n",
      "epoch:16 step:15425 [D loss: 0.232942, acc.: 64.84%] [G loss: 0.405172]\n",
      "epoch:16 step:15426 [D loss: 0.216032, acc.: 62.50%] [G loss: 0.462404]\n",
      "epoch:16 step:15427 [D loss: 0.214614, acc.: 66.41%] [G loss: 0.440697]\n",
      "epoch:16 step:15428 [D loss: 0.192033, acc.: 71.09%] [G loss: 0.470012]\n",
      "epoch:16 step:15429 [D loss: 0.261759, acc.: 57.81%] [G loss: 0.475575]\n",
      "epoch:16 step:15430 [D loss: 0.244817, acc.: 55.47%] [G loss: 0.422743]\n",
      "epoch:16 step:15431 [D loss: 0.219907, acc.: 65.62%] [G loss: 0.435855]\n",
      "epoch:16 step:15432 [D loss: 0.224982, acc.: 62.50%] [G loss: 0.458678]\n",
      "epoch:16 step:15433 [D loss: 0.261491, acc.: 55.47%] [G loss: 0.418499]\n",
      "epoch:16 step:15434 [D loss: 0.235059, acc.: 57.03%] [G loss: 0.418278]\n",
      "epoch:16 step:15435 [D loss: 0.241359, acc.: 60.94%] [G loss: 0.422434]\n",
      "epoch:16 step:15436 [D loss: 0.200835, acc.: 71.09%] [G loss: 0.420983]\n",
      "epoch:16 step:15437 [D loss: 0.218484, acc.: 63.28%] [G loss: 0.439199]\n",
      "epoch:16 step:15438 [D loss: 0.239555, acc.: 60.16%] [G loss: 0.443242]\n",
      "epoch:16 step:15439 [D loss: 0.220126, acc.: 65.62%] [G loss: 0.445158]\n",
      "epoch:16 step:15440 [D loss: 0.245759, acc.: 58.59%] [G loss: 0.464984]\n",
      "epoch:16 step:15441 [D loss: 0.226733, acc.: 60.94%] [G loss: 0.409620]\n",
      "epoch:16 step:15442 [D loss: 0.202847, acc.: 65.62%] [G loss: 0.434392]\n",
      "epoch:16 step:15443 [D loss: 0.218822, acc.: 64.06%] [G loss: 0.448714]\n",
      "epoch:16 step:15444 [D loss: 0.224935, acc.: 61.72%] [G loss: 0.452464]\n",
      "epoch:16 step:15445 [D loss: 0.224194, acc.: 60.94%] [G loss: 0.470513]\n",
      "epoch:16 step:15446 [D loss: 0.221127, acc.: 67.19%] [G loss: 0.480721]\n",
      "epoch:16 step:15447 [D loss: 0.243495, acc.: 58.59%] [G loss: 0.453859]\n",
      "epoch:16 step:15448 [D loss: 0.228610, acc.: 64.84%] [G loss: 0.474523]\n",
      "epoch:16 step:15449 [D loss: 0.216750, acc.: 68.75%] [G loss: 0.464622]\n",
      "epoch:16 step:15450 [D loss: 0.243656, acc.: 57.03%] [G loss: 0.498603]\n",
      "epoch:16 step:15451 [D loss: 0.251454, acc.: 54.69%] [G loss: 0.458141]\n",
      "epoch:16 step:15452 [D loss: 0.231582, acc.: 59.38%] [G loss: 0.453621]\n",
      "epoch:16 step:15453 [D loss: 0.234514, acc.: 64.06%] [G loss: 0.430850]\n",
      "epoch:16 step:15454 [D loss: 0.270581, acc.: 56.25%] [G loss: 0.394419]\n",
      "epoch:16 step:15455 [D loss: 0.230995, acc.: 64.06%] [G loss: 0.443629]\n",
      "epoch:16 step:15456 [D loss: 0.217958, acc.: 64.06%] [G loss: 0.410117]\n",
      "epoch:16 step:15457 [D loss: 0.257486, acc.: 53.12%] [G loss: 0.409428]\n",
      "epoch:16 step:15458 [D loss: 0.234008, acc.: 64.84%] [G loss: 0.486653]\n",
      "epoch:16 step:15459 [D loss: 0.221826, acc.: 64.84%] [G loss: 0.435836]\n",
      "epoch:16 step:15460 [D loss: 0.213356, acc.: 61.72%] [G loss: 0.487911]\n",
      "epoch:16 step:15461 [D loss: 0.211561, acc.: 67.97%] [G loss: 0.472250]\n",
      "epoch:16 step:15462 [D loss: 0.254742, acc.: 57.81%] [G loss: 0.447008]\n",
      "epoch:16 step:15463 [D loss: 0.181130, acc.: 77.34%] [G loss: 0.524367]\n",
      "epoch:16 step:15464 [D loss: 0.232731, acc.: 63.28%] [G loss: 0.486968]\n",
      "epoch:16 step:15465 [D loss: 0.271202, acc.: 50.00%] [G loss: 0.463066]\n",
      "epoch:16 step:15466 [D loss: 0.200222, acc.: 71.09%] [G loss: 0.470194]\n",
      "epoch:16 step:15467 [D loss: 0.219315, acc.: 64.06%] [G loss: 0.460643]\n",
      "epoch:16 step:15468 [D loss: 0.242742, acc.: 58.59%] [G loss: 0.426024]\n",
      "epoch:16 step:15469 [D loss: 0.262741, acc.: 52.34%] [G loss: 0.392240]\n",
      "epoch:16 step:15470 [D loss: 0.231304, acc.: 64.84%] [G loss: 0.395946]\n",
      "epoch:16 step:15471 [D loss: 0.240015, acc.: 64.06%] [G loss: 0.398907]\n",
      "epoch:16 step:15472 [D loss: 0.225537, acc.: 60.16%] [G loss: 0.423342]\n",
      "epoch:16 step:15473 [D loss: 0.184884, acc.: 75.00%] [G loss: 0.466939]\n",
      "epoch:16 step:15474 [D loss: 0.256650, acc.: 52.34%] [G loss: 0.417778]\n",
      "epoch:16 step:15475 [D loss: 0.229137, acc.: 59.38%] [G loss: 0.431980]\n",
      "epoch:16 step:15476 [D loss: 0.210384, acc.: 67.97%] [G loss: 0.450958]\n",
      "epoch:16 step:15477 [D loss: 0.203493, acc.: 69.53%] [G loss: 0.490407]\n",
      "epoch:16 step:15478 [D loss: 0.231718, acc.: 57.81%] [G loss: 0.398718]\n",
      "epoch:16 step:15479 [D loss: 0.226716, acc.: 59.38%] [G loss: 0.398231]\n",
      "epoch:16 step:15480 [D loss: 0.201846, acc.: 70.31%] [G loss: 0.430107]\n",
      "epoch:16 step:15481 [D loss: 0.253126, acc.: 60.16%] [G loss: 0.423245]\n",
      "epoch:16 step:15482 [D loss: 0.228660, acc.: 61.72%] [G loss: 0.433332]\n",
      "epoch:16 step:15483 [D loss: 0.241346, acc.: 57.81%] [G loss: 0.400444]\n",
      "epoch:16 step:15484 [D loss: 0.213837, acc.: 64.06%] [G loss: 0.452058]\n",
      "epoch:16 step:15485 [D loss: 0.221368, acc.: 60.94%] [G loss: 0.444930]\n",
      "epoch:16 step:15486 [D loss: 0.221732, acc.: 64.84%] [G loss: 0.441352]\n",
      "epoch:16 step:15487 [D loss: 0.191484, acc.: 70.31%] [G loss: 0.477741]\n",
      "epoch:16 step:15488 [D loss: 0.208353, acc.: 71.09%] [G loss: 0.464223]\n",
      "epoch:16 step:15489 [D loss: 0.216496, acc.: 66.41%] [G loss: 0.443393]\n",
      "epoch:16 step:15490 [D loss: 0.199787, acc.: 69.53%] [G loss: 0.482498]\n",
      "epoch:16 step:15491 [D loss: 0.196251, acc.: 73.44%] [G loss: 0.474300]\n",
      "epoch:16 step:15492 [D loss: 0.248753, acc.: 61.72%] [G loss: 0.425252]\n",
      "epoch:16 step:15493 [D loss: 0.274516, acc.: 46.88%] [G loss: 0.412359]\n",
      "epoch:16 step:15494 [D loss: 0.220055, acc.: 63.28%] [G loss: 0.402822]\n",
      "epoch:16 step:15495 [D loss: 0.220475, acc.: 65.62%] [G loss: 0.380730]\n",
      "epoch:16 step:15496 [D loss: 0.207300, acc.: 68.75%] [G loss: 0.452789]\n",
      "epoch:16 step:15497 [D loss: 0.205722, acc.: 68.75%] [G loss: 0.473471]\n",
      "epoch:16 step:15498 [D loss: 0.211025, acc.: 64.84%] [G loss: 0.450731]\n",
      "epoch:16 step:15499 [D loss: 0.189859, acc.: 68.75%] [G loss: 0.454969]\n",
      "epoch:16 step:15500 [D loss: 0.206866, acc.: 64.84%] [G loss: 0.457558]\n",
      "epoch:16 step:15501 [D loss: 0.244164, acc.: 58.59%] [G loss: 0.466448]\n",
      "epoch:16 step:15502 [D loss: 0.237115, acc.: 56.25%] [G loss: 0.442103]\n",
      "epoch:16 step:15503 [D loss: 0.242342, acc.: 57.81%] [G loss: 0.412148]\n",
      "epoch:16 step:15504 [D loss: 0.240079, acc.: 54.69%] [G loss: 0.416288]\n",
      "epoch:16 step:15505 [D loss: 0.200833, acc.: 68.75%] [G loss: 0.423732]\n",
      "epoch:16 step:15506 [D loss: 0.213204, acc.: 68.75%] [G loss: 0.471838]\n",
      "epoch:16 step:15507 [D loss: 0.198121, acc.: 72.66%] [G loss: 0.472688]\n",
      "epoch:16 step:15508 [D loss: 0.209683, acc.: 67.19%] [G loss: 0.477748]\n",
      "epoch:16 step:15509 [D loss: 0.268211, acc.: 57.81%] [G loss: 0.431469]\n",
      "epoch:16 step:15510 [D loss: 0.247082, acc.: 59.38%] [G loss: 0.417452]\n",
      "epoch:16 step:15511 [D loss: 0.199219, acc.: 70.31%] [G loss: 0.483392]\n",
      "epoch:16 step:15512 [D loss: 0.224638, acc.: 64.84%] [G loss: 0.429108]\n",
      "epoch:16 step:15513 [D loss: 0.201695, acc.: 68.75%] [G loss: 0.449333]\n",
      "epoch:16 step:15514 [D loss: 0.220208, acc.: 67.97%] [G loss: 0.424596]\n",
      "epoch:16 step:15515 [D loss: 0.201718, acc.: 68.75%] [G loss: 0.482057]\n",
      "epoch:16 step:15516 [D loss: 0.225252, acc.: 63.28%] [G loss: 0.427480]\n",
      "epoch:16 step:15517 [D loss: 0.249011, acc.: 57.81%] [G loss: 0.429943]\n",
      "epoch:16 step:15518 [D loss: 0.230825, acc.: 60.94%] [G loss: 0.471815]\n",
      "epoch:16 step:15519 [D loss: 0.225229, acc.: 61.72%] [G loss: 0.440809]\n",
      "epoch:16 step:15520 [D loss: 0.235590, acc.: 62.50%] [G loss: 0.447527]\n",
      "epoch:16 step:15521 [D loss: 0.249809, acc.: 52.34%] [G loss: 0.429501]\n",
      "epoch:16 step:15522 [D loss: 0.200511, acc.: 68.75%] [G loss: 0.480139]\n",
      "epoch:16 step:15523 [D loss: 0.246893, acc.: 61.72%] [G loss: 0.407018]\n",
      "epoch:16 step:15524 [D loss: 0.219027, acc.: 61.72%] [G loss: 0.415444]\n",
      "epoch:16 step:15525 [D loss: 0.198606, acc.: 69.53%] [G loss: 0.448923]\n",
      "epoch:16 step:15526 [D loss: 0.194972, acc.: 70.31%] [G loss: 0.458430]\n",
      "epoch:16 step:15527 [D loss: 0.248689, acc.: 54.69%] [G loss: 0.448241]\n",
      "epoch:16 step:15528 [D loss: 0.222617, acc.: 63.28%] [G loss: 0.423480]\n",
      "epoch:16 step:15529 [D loss: 0.245795, acc.: 55.47%] [G loss: 0.421386]\n",
      "epoch:16 step:15530 [D loss: 0.229027, acc.: 64.06%] [G loss: 0.428936]\n",
      "epoch:16 step:15531 [D loss: 0.233661, acc.: 66.41%] [G loss: 0.439763]\n",
      "epoch:16 step:15532 [D loss: 0.235093, acc.: 59.38%] [G loss: 0.465331]\n",
      "epoch:16 step:15533 [D loss: 0.237134, acc.: 59.38%] [G loss: 0.398338]\n",
      "epoch:16 step:15534 [D loss: 0.268829, acc.: 52.34%] [G loss: 0.412655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15535 [D loss: 0.215500, acc.: 64.84%] [G loss: 0.448818]\n",
      "epoch:16 step:15536 [D loss: 0.233453, acc.: 61.72%] [G loss: 0.434011]\n",
      "epoch:16 step:15537 [D loss: 0.220073, acc.: 63.28%] [G loss: 0.437567]\n",
      "epoch:16 step:15538 [D loss: 0.229212, acc.: 64.06%] [G loss: 0.429375]\n",
      "epoch:16 step:15539 [D loss: 0.205858, acc.: 67.19%] [G loss: 0.457468]\n",
      "epoch:16 step:15540 [D loss: 0.190112, acc.: 74.22%] [G loss: 0.490850]\n",
      "epoch:16 step:15541 [D loss: 0.191467, acc.: 71.88%] [G loss: 0.449870]\n",
      "epoch:16 step:15542 [D loss: 0.231467, acc.: 63.28%] [G loss: 0.420847]\n",
      "epoch:16 step:15543 [D loss: 0.209758, acc.: 67.97%] [G loss: 0.452782]\n",
      "epoch:16 step:15544 [D loss: 0.200343, acc.: 67.97%] [G loss: 0.423315]\n",
      "epoch:16 step:15545 [D loss: 0.242716, acc.: 57.81%] [G loss: 0.440317]\n",
      "epoch:16 step:15546 [D loss: 0.202383, acc.: 71.09%] [G loss: 0.451675]\n",
      "epoch:16 step:15547 [D loss: 0.218610, acc.: 63.28%] [G loss: 0.447087]\n",
      "epoch:16 step:15548 [D loss: 0.191732, acc.: 72.66%] [G loss: 0.491817]\n",
      "epoch:16 step:15549 [D loss: 0.212718, acc.: 68.75%] [G loss: 0.457225]\n",
      "epoch:16 step:15550 [D loss: 0.213487, acc.: 65.62%] [G loss: 0.420858]\n",
      "epoch:16 step:15551 [D loss: 0.243594, acc.: 57.03%] [G loss: 0.447976]\n",
      "epoch:16 step:15552 [D loss: 0.247714, acc.: 57.03%] [G loss: 0.411452]\n",
      "epoch:16 step:15553 [D loss: 0.234481, acc.: 59.38%] [G loss: 0.427072]\n",
      "epoch:16 step:15554 [D loss: 0.212942, acc.: 66.41%] [G loss: 0.452391]\n",
      "epoch:16 step:15555 [D loss: 0.211923, acc.: 67.19%] [G loss: 0.503082]\n",
      "epoch:16 step:15556 [D loss: 0.198460, acc.: 71.09%] [G loss: 0.504355]\n",
      "epoch:16 step:15557 [D loss: 0.255565, acc.: 55.47%] [G loss: 0.478750]\n",
      "epoch:16 step:15558 [D loss: 0.270280, acc.: 51.56%] [G loss: 0.468268]\n",
      "epoch:16 step:15559 [D loss: 0.226256, acc.: 63.28%] [G loss: 0.437767]\n",
      "epoch:16 step:15560 [D loss: 0.178150, acc.: 75.00%] [G loss: 0.488073]\n",
      "epoch:16 step:15561 [D loss: 0.251917, acc.: 60.16%] [G loss: 0.379025]\n",
      "epoch:16 step:15562 [D loss: 0.226998, acc.: 60.94%] [G loss: 0.372993]\n",
      "epoch:16 step:15563 [D loss: 0.209121, acc.: 67.97%] [G loss: 0.430362]\n",
      "epoch:16 step:15564 [D loss: 0.234874, acc.: 58.59%] [G loss: 0.414495]\n",
      "epoch:16 step:15565 [D loss: 0.225603, acc.: 59.38%] [G loss: 0.418929]\n",
      "epoch:16 step:15566 [D loss: 0.184212, acc.: 71.88%] [G loss: 0.489689]\n",
      "epoch:16 step:15567 [D loss: 0.200911, acc.: 69.53%] [G loss: 0.496540]\n",
      "epoch:16 step:15568 [D loss: 0.222253, acc.: 62.50%] [G loss: 0.492895]\n",
      "epoch:16 step:15569 [D loss: 0.260033, acc.: 48.44%] [G loss: 0.406018]\n",
      "epoch:16 step:15570 [D loss: 0.225490, acc.: 60.16%] [G loss: 0.423133]\n",
      "epoch:16 step:15571 [D loss: 0.206172, acc.: 67.19%] [G loss: 0.452477]\n",
      "epoch:16 step:15572 [D loss: 0.249742, acc.: 49.22%] [G loss: 0.386981]\n",
      "epoch:16 step:15573 [D loss: 0.226412, acc.: 67.19%] [G loss: 0.460533]\n",
      "epoch:16 step:15574 [D loss: 0.222978, acc.: 68.75%] [G loss: 0.500494]\n",
      "epoch:16 step:15575 [D loss: 0.239562, acc.: 60.94%] [G loss: 0.473480]\n",
      "epoch:16 step:15576 [D loss: 0.239492, acc.: 60.16%] [G loss: 0.458851]\n",
      "epoch:16 step:15577 [D loss: 0.227707, acc.: 63.28%] [G loss: 0.455156]\n",
      "epoch:16 step:15578 [D loss: 0.234081, acc.: 59.38%] [G loss: 0.453645]\n",
      "epoch:16 step:15579 [D loss: 0.242498, acc.: 58.59%] [G loss: 0.432499]\n",
      "epoch:16 step:15580 [D loss: 0.207728, acc.: 67.19%] [G loss: 0.468335]\n",
      "epoch:16 step:15581 [D loss: 0.226264, acc.: 64.06%] [G loss: 0.446320]\n",
      "epoch:16 step:15582 [D loss: 0.256826, acc.: 53.91%] [G loss: 0.432257]\n",
      "epoch:16 step:15583 [D loss: 0.223135, acc.: 60.16%] [G loss: 0.442514]\n",
      "epoch:16 step:15584 [D loss: 0.205241, acc.: 65.62%] [G loss: 0.434447]\n",
      "epoch:16 step:15585 [D loss: 0.221853, acc.: 66.41%] [G loss: 0.435067]\n",
      "epoch:16 step:15586 [D loss: 0.256499, acc.: 55.47%] [G loss: 0.424655]\n",
      "epoch:16 step:15587 [D loss: 0.256490, acc.: 56.25%] [G loss: 0.399168]\n",
      "epoch:16 step:15588 [D loss: 0.228312, acc.: 57.81%] [G loss: 0.437829]\n",
      "epoch:16 step:15589 [D loss: 0.247047, acc.: 56.25%] [G loss: 0.464034]\n",
      "epoch:16 step:15590 [D loss: 0.183968, acc.: 75.78%] [G loss: 0.496001]\n",
      "epoch:16 step:15591 [D loss: 0.232141, acc.: 64.06%] [G loss: 0.430403]\n",
      "epoch:16 step:15592 [D loss: 0.235276, acc.: 58.59%] [G loss: 0.446567]\n",
      "epoch:16 step:15593 [D loss: 0.242042, acc.: 57.81%] [G loss: 0.421657]\n",
      "epoch:16 step:15594 [D loss: 0.211354, acc.: 63.28%] [G loss: 0.421436]\n",
      "epoch:16 step:15595 [D loss: 0.227459, acc.: 64.84%] [G loss: 0.409418]\n",
      "epoch:16 step:15596 [D loss: 0.237896, acc.: 59.38%] [G loss: 0.458773]\n",
      "epoch:16 step:15597 [D loss: 0.210155, acc.: 67.19%] [G loss: 0.433068]\n",
      "epoch:16 step:15598 [D loss: 0.232755, acc.: 59.38%] [G loss: 0.421777]\n",
      "epoch:16 step:15599 [D loss: 0.202590, acc.: 67.19%] [G loss: 0.458219]\n",
      "epoch:16 step:15600 [D loss: 0.231580, acc.: 57.81%] [G loss: 0.386578]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.857979\n",
      "FID: 9.390117\n",
      "0 = 11.74529874789715\n",
      "1 = 0.047093201449919866\n",
      "2 = 0.8769500255584717\n",
      "3 = 0.8622000217437744\n",
      "4 = 0.891700029373169\n",
      "5 = 0.8884080648422241\n",
      "6 = 0.8622000217437744\n",
      "7 = 6.146661606246243\n",
      "8 = 0.06751801568256746\n",
      "9 = 0.687749981880188\n",
      "10 = 0.6858999729156494\n",
      "11 = 0.6895999908447266\n",
      "12 = 0.6884472370147705\n",
      "13 = 0.6858999729156494\n",
      "14 = 7.858047962188721\n",
      "15 = 9.507343292236328\n",
      "16 = 0.12317611277103424\n",
      "17 = 7.857978820800781\n",
      "18 = 9.390116691589355\n",
      "epoch:16 step:15601 [D loss: 0.197874, acc.: 67.97%] [G loss: 0.455405]\n",
      "epoch:16 step:15602 [D loss: 0.233777, acc.: 59.38%] [G loss: 0.419331]\n",
      "epoch:16 step:15603 [D loss: 0.226713, acc.: 63.28%] [G loss: 0.405855]\n",
      "epoch:16 step:15604 [D loss: 0.232599, acc.: 58.59%] [G loss: 0.397012]\n",
      "epoch:16 step:15605 [D loss: 0.199903, acc.: 69.53%] [G loss: 0.431170]\n",
      "epoch:16 step:15606 [D loss: 0.231271, acc.: 58.59%] [G loss: 0.435078]\n",
      "epoch:16 step:15607 [D loss: 0.255073, acc.: 55.47%] [G loss: 0.405291]\n",
      "epoch:16 step:15608 [D loss: 0.219511, acc.: 70.31%] [G loss: 0.432179]\n",
      "epoch:16 step:15609 [D loss: 0.239841, acc.: 64.06%] [G loss: 0.454811]\n",
      "epoch:16 step:15610 [D loss: 0.225857, acc.: 66.41%] [G loss: 0.452788]\n",
      "epoch:16 step:15611 [D loss: 0.222955, acc.: 66.41%] [G loss: 0.417385]\n",
      "epoch:16 step:15612 [D loss: 0.209125, acc.: 66.41%] [G loss: 0.428605]\n",
      "epoch:16 step:15613 [D loss: 0.255715, acc.: 58.59%] [G loss: 0.402144]\n",
      "epoch:16 step:15614 [D loss: 0.244040, acc.: 61.72%] [G loss: 0.467155]\n",
      "epoch:16 step:15615 [D loss: 0.235199, acc.: 62.50%] [G loss: 0.440212]\n",
      "epoch:16 step:15616 [D loss: 0.197099, acc.: 73.44%] [G loss: 0.464200]\n",
      "epoch:16 step:15617 [D loss: 0.248613, acc.: 53.12%] [G loss: 0.444326]\n",
      "epoch:16 step:15618 [D loss: 0.220975, acc.: 65.62%] [G loss: 0.478074]\n",
      "epoch:16 step:15619 [D loss: 0.219879, acc.: 63.28%] [G loss: 0.421030]\n",
      "epoch:16 step:15620 [D loss: 0.232786, acc.: 60.94%] [G loss: 0.438833]\n",
      "epoch:16 step:15621 [D loss: 0.219796, acc.: 64.84%] [G loss: 0.435255]\n",
      "epoch:16 step:15622 [D loss: 0.214327, acc.: 64.06%] [G loss: 0.456854]\n",
      "epoch:16 step:15623 [D loss: 0.209358, acc.: 64.84%] [G loss: 0.412920]\n",
      "epoch:16 step:15624 [D loss: 0.207015, acc.: 66.41%] [G loss: 0.443847]\n",
      "epoch:16 step:15625 [D loss: 0.215679, acc.: 60.16%] [G loss: 0.455173]\n",
      "epoch:16 step:15626 [D loss: 0.187400, acc.: 74.22%] [G loss: 0.457715]\n",
      "epoch:16 step:15627 [D loss: 0.204052, acc.: 72.66%] [G loss: 0.472132]\n",
      "epoch:16 step:15628 [D loss: 0.205291, acc.: 68.75%] [G loss: 0.440596]\n",
      "epoch:16 step:15629 [D loss: 0.226218, acc.: 60.16%] [G loss: 0.433141]\n",
      "epoch:16 step:15630 [D loss: 0.226754, acc.: 60.16%] [G loss: 0.452459]\n",
      "epoch:16 step:15631 [D loss: 0.246635, acc.: 59.38%] [G loss: 0.436729]\n",
      "epoch:16 step:15632 [D loss: 0.226105, acc.: 60.94%] [G loss: 0.427601]\n",
      "epoch:16 step:15633 [D loss: 0.207990, acc.: 64.84%] [G loss: 0.470180]\n",
      "epoch:16 step:15634 [D loss: 0.204498, acc.: 68.75%] [G loss: 0.464372]\n",
      "epoch:16 step:15635 [D loss: 0.224535, acc.: 64.06%] [G loss: 0.454894]\n",
      "epoch:16 step:15636 [D loss: 0.247446, acc.: 58.59%] [G loss: 0.418543]\n",
      "epoch:16 step:15637 [D loss: 0.245108, acc.: 57.03%] [G loss: 0.429662]\n",
      "epoch:16 step:15638 [D loss: 0.196544, acc.: 69.53%] [G loss: 0.444499]\n",
      "epoch:16 step:15639 [D loss: 0.189655, acc.: 75.78%] [G loss: 0.468616]\n",
      "epoch:16 step:15640 [D loss: 0.177697, acc.: 73.44%] [G loss: 0.507641]\n",
      "epoch:16 step:15641 [D loss: 0.215274, acc.: 63.28%] [G loss: 0.474615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15642 [D loss: 0.235266, acc.: 62.50%] [G loss: 0.484163]\n",
      "epoch:16 step:15643 [D loss: 0.216204, acc.: 64.06%] [G loss: 0.481736]\n",
      "epoch:16 step:15644 [D loss: 0.245339, acc.: 53.91%] [G loss: 0.387734]\n",
      "epoch:16 step:15645 [D loss: 0.229900, acc.: 64.06%] [G loss: 0.425035]\n",
      "epoch:16 step:15646 [D loss: 0.217062, acc.: 67.19%] [G loss: 0.457957]\n",
      "epoch:16 step:15647 [D loss: 0.231025, acc.: 61.72%] [G loss: 0.453645]\n",
      "epoch:16 step:15648 [D loss: 0.242122, acc.: 63.28%] [G loss: 0.436990]\n",
      "epoch:16 step:15649 [D loss: 0.227581, acc.: 61.72%] [G loss: 0.421466]\n",
      "epoch:16 step:15650 [D loss: 0.235275, acc.: 58.59%] [G loss: 0.419997]\n",
      "epoch:16 step:15651 [D loss: 0.193240, acc.: 71.09%] [G loss: 0.467832]\n",
      "epoch:16 step:15652 [D loss: 0.197656, acc.: 69.53%] [G loss: 0.490271]\n",
      "epoch:16 step:15653 [D loss: 0.201506, acc.: 67.97%] [G loss: 0.424462]\n",
      "epoch:16 step:15654 [D loss: 0.225719, acc.: 61.72%] [G loss: 0.433965]\n",
      "epoch:16 step:15655 [D loss: 0.218769, acc.: 61.72%] [G loss: 0.439366]\n",
      "epoch:16 step:15656 [D loss: 0.251953, acc.: 55.47%] [G loss: 0.435822]\n",
      "epoch:16 step:15657 [D loss: 0.231175, acc.: 61.72%] [G loss: 0.445805]\n",
      "epoch:16 step:15658 [D loss: 0.184064, acc.: 78.91%] [G loss: 0.445071]\n",
      "epoch:16 step:15659 [D loss: 0.217564, acc.: 67.19%] [G loss: 0.442830]\n",
      "epoch:16 step:15660 [D loss: 0.226852, acc.: 58.59%] [G loss: 0.407453]\n",
      "epoch:16 step:15661 [D loss: 0.221004, acc.: 60.94%] [G loss: 0.445131]\n",
      "epoch:16 step:15662 [D loss: 0.236709, acc.: 56.25%] [G loss: 0.434290]\n",
      "epoch:16 step:15663 [D loss: 0.242572, acc.: 58.59%] [G loss: 0.423543]\n",
      "epoch:16 step:15664 [D loss: 0.236211, acc.: 57.03%] [G loss: 0.443735]\n",
      "epoch:16 step:15665 [D loss: 0.239637, acc.: 53.91%] [G loss: 0.430786]\n",
      "epoch:16 step:15666 [D loss: 0.229635, acc.: 61.72%] [G loss: 0.423963]\n",
      "epoch:16 step:15667 [D loss: 0.227890, acc.: 63.28%] [G loss: 0.466219]\n",
      "epoch:16 step:15668 [D loss: 0.236361, acc.: 62.50%] [G loss: 0.438813]\n",
      "epoch:16 step:15669 [D loss: 0.200757, acc.: 70.31%] [G loss: 0.449033]\n",
      "epoch:16 step:15670 [D loss: 0.226880, acc.: 60.94%] [G loss: 0.422939]\n",
      "epoch:16 step:15671 [D loss: 0.206325, acc.: 65.62%] [G loss: 0.443022]\n",
      "epoch:16 step:15672 [D loss: 0.221569, acc.: 64.06%] [G loss: 0.414707]\n",
      "epoch:16 step:15673 [D loss: 0.193082, acc.: 71.88%] [G loss: 0.429466]\n",
      "epoch:16 step:15674 [D loss: 0.209775, acc.: 71.09%] [G loss: 0.441968]\n",
      "epoch:16 step:15675 [D loss: 0.248121, acc.: 57.03%] [G loss: 0.448917]\n",
      "epoch:16 step:15676 [D loss: 0.245595, acc.: 55.47%] [G loss: 0.422109]\n",
      "epoch:16 step:15677 [D loss: 0.232497, acc.: 63.28%] [G loss: 0.444299]\n",
      "epoch:16 step:15678 [D loss: 0.226723, acc.: 62.50%] [G loss: 0.462875]\n",
      "epoch:16 step:15679 [D loss: 0.247980, acc.: 57.03%] [G loss: 0.427427]\n",
      "epoch:16 step:15680 [D loss: 0.206377, acc.: 71.88%] [G loss: 0.454288]\n",
      "epoch:16 step:15681 [D loss: 0.211540, acc.: 64.84%] [G loss: 0.499878]\n",
      "epoch:16 step:15682 [D loss: 0.221922, acc.: 63.28%] [G loss: 0.426736]\n",
      "epoch:16 step:15683 [D loss: 0.213395, acc.: 67.97%] [G loss: 0.457116]\n",
      "epoch:16 step:15684 [D loss: 0.234793, acc.: 60.16%] [G loss: 0.459660]\n",
      "epoch:16 step:15685 [D loss: 0.207454, acc.: 64.84%] [G loss: 0.467162]\n",
      "epoch:16 step:15686 [D loss: 0.192102, acc.: 75.00%] [G loss: 0.487734]\n",
      "epoch:16 step:15687 [D loss: 0.208033, acc.: 69.53%] [G loss: 0.511996]\n",
      "epoch:16 step:15688 [D loss: 0.242762, acc.: 57.03%] [G loss: 0.457218]\n",
      "epoch:16 step:15689 [D loss: 0.232405, acc.: 59.38%] [G loss: 0.426250]\n",
      "epoch:16 step:15690 [D loss: 0.218917, acc.: 62.50%] [G loss: 0.419100]\n",
      "epoch:16 step:15691 [D loss: 0.232210, acc.: 60.16%] [G loss: 0.385506]\n",
      "epoch:16 step:15692 [D loss: 0.202597, acc.: 70.31%] [G loss: 0.429449]\n",
      "epoch:16 step:15693 [D loss: 0.232361, acc.: 60.16%] [G loss: 0.465206]\n",
      "epoch:16 step:15694 [D loss: 0.229409, acc.: 62.50%] [G loss: 0.453652]\n",
      "epoch:16 step:15695 [D loss: 0.250229, acc.: 56.25%] [G loss: 0.448930]\n",
      "epoch:16 step:15696 [D loss: 0.243162, acc.: 59.38%] [G loss: 0.429675]\n",
      "epoch:16 step:15697 [D loss: 0.225057, acc.: 62.50%] [G loss: 0.414499]\n",
      "epoch:16 step:15698 [D loss: 0.210059, acc.: 68.75%] [G loss: 0.404362]\n",
      "epoch:16 step:15699 [D loss: 0.236416, acc.: 66.41%] [G loss: 0.424756]\n",
      "epoch:16 step:15700 [D loss: 0.203554, acc.: 67.97%] [G loss: 0.456969]\n",
      "epoch:16 step:15701 [D loss: 0.215441, acc.: 61.72%] [G loss: 0.462811]\n",
      "epoch:16 step:15702 [D loss: 0.263523, acc.: 52.34%] [G loss: 0.481927]\n",
      "epoch:16 step:15703 [D loss: 0.223503, acc.: 64.84%] [G loss: 0.425903]\n",
      "epoch:16 step:15704 [D loss: 0.237409, acc.: 57.81%] [G loss: 0.442841]\n",
      "epoch:16 step:15705 [D loss: 0.255742, acc.: 53.91%] [G loss: 0.476082]\n",
      "epoch:16 step:15706 [D loss: 0.259508, acc.: 56.25%] [G loss: 0.427932]\n",
      "epoch:16 step:15707 [D loss: 0.235761, acc.: 62.50%] [G loss: 0.422708]\n",
      "epoch:16 step:15708 [D loss: 0.249654, acc.: 56.25%] [G loss: 0.399881]\n",
      "epoch:16 step:15709 [D loss: 0.220200, acc.: 63.28%] [G loss: 0.444833]\n",
      "epoch:16 step:15710 [D loss: 0.225008, acc.: 62.50%] [G loss: 0.430551]\n",
      "epoch:16 step:15711 [D loss: 0.203129, acc.: 69.53%] [G loss: 0.462485]\n",
      "epoch:16 step:15712 [D loss: 0.242453, acc.: 64.06%] [G loss: 0.480008]\n",
      "epoch:16 step:15713 [D loss: 0.232033, acc.: 57.03%] [G loss: 0.395477]\n",
      "epoch:16 step:15714 [D loss: 0.249853, acc.: 53.12%] [G loss: 0.426736]\n",
      "epoch:16 step:15715 [D loss: 0.217699, acc.: 60.16%] [G loss: 0.449677]\n",
      "epoch:16 step:15716 [D loss: 0.215393, acc.: 66.41%] [G loss: 0.486403]\n",
      "epoch:16 step:15717 [D loss: 0.190641, acc.: 71.88%] [G loss: 0.467593]\n",
      "epoch:16 step:15718 [D loss: 0.211112, acc.: 66.41%] [G loss: 0.458551]\n",
      "epoch:16 step:15719 [D loss: 0.257966, acc.: 52.34%] [G loss: 0.429862]\n",
      "epoch:16 step:15720 [D loss: 0.211459, acc.: 69.53%] [G loss: 0.444887]\n",
      "epoch:16 step:15721 [D loss: 0.212049, acc.: 64.06%] [G loss: 0.418234]\n",
      "epoch:16 step:15722 [D loss: 0.200040, acc.: 66.41%] [G loss: 0.429102]\n",
      "epoch:16 step:15723 [D loss: 0.211382, acc.: 69.53%] [G loss: 0.478297]\n",
      "epoch:16 step:15724 [D loss: 0.226321, acc.: 64.84%] [G loss: 0.494726]\n",
      "epoch:16 step:15725 [D loss: 0.198498, acc.: 69.53%] [G loss: 0.477975]\n",
      "epoch:16 step:15726 [D loss: 0.232897, acc.: 59.38%] [G loss: 0.416726]\n",
      "epoch:16 step:15727 [D loss: 0.221198, acc.: 64.84%] [G loss: 0.423074]\n",
      "epoch:16 step:15728 [D loss: 0.209432, acc.: 68.75%] [G loss: 0.450520]\n",
      "epoch:16 step:15729 [D loss: 0.235466, acc.: 60.94%] [G loss: 0.417253]\n",
      "epoch:16 step:15730 [D loss: 0.224410, acc.: 59.38%] [G loss: 0.464799]\n",
      "epoch:16 step:15731 [D loss: 0.252069, acc.: 53.12%] [G loss: 0.393507]\n",
      "epoch:16 step:15732 [D loss: 0.234863, acc.: 59.38%] [G loss: 0.419670]\n",
      "epoch:16 step:15733 [D loss: 0.252317, acc.: 51.56%] [G loss: 0.428856]\n",
      "epoch:16 step:15734 [D loss: 0.223239, acc.: 64.84%] [G loss: 0.420494]\n",
      "epoch:16 step:15735 [D loss: 0.215214, acc.: 61.72%] [G loss: 0.450740]\n",
      "epoch:16 step:15736 [D loss: 0.241939, acc.: 58.59%] [G loss: 0.447522]\n",
      "epoch:16 step:15737 [D loss: 0.243140, acc.: 58.59%] [G loss: 0.426968]\n",
      "epoch:16 step:15738 [D loss: 0.204893, acc.: 65.62%] [G loss: 0.395711]\n",
      "epoch:16 step:15739 [D loss: 0.184496, acc.: 74.22%] [G loss: 0.445791]\n",
      "epoch:16 step:15740 [D loss: 0.229859, acc.: 61.72%] [G loss: 0.434373]\n",
      "epoch:16 step:15741 [D loss: 0.222976, acc.: 60.16%] [G loss: 0.423490]\n",
      "epoch:16 step:15742 [D loss: 0.207596, acc.: 66.41%] [G loss: 0.399004]\n",
      "epoch:16 step:15743 [D loss: 0.215088, acc.: 70.31%] [G loss: 0.457194]\n",
      "epoch:16 step:15744 [D loss: 0.231927, acc.: 63.28%] [G loss: 0.447152]\n",
      "epoch:16 step:15745 [D loss: 0.224351, acc.: 67.19%] [G loss: 0.442783]\n",
      "epoch:16 step:15746 [D loss: 0.200968, acc.: 69.53%] [G loss: 0.461973]\n",
      "epoch:16 step:15747 [D loss: 0.214999, acc.: 67.19%] [G loss: 0.468733]\n",
      "epoch:16 step:15748 [D loss: 0.209195, acc.: 65.62%] [G loss: 0.471975]\n",
      "epoch:16 step:15749 [D loss: 0.221269, acc.: 66.41%] [G loss: 0.443997]\n",
      "epoch:16 step:15750 [D loss: 0.228776, acc.: 62.50%] [G loss: 0.451934]\n",
      "epoch:16 step:15751 [D loss: 0.221756, acc.: 60.94%] [G loss: 0.419002]\n",
      "epoch:16 step:15752 [D loss: 0.238780, acc.: 57.81%] [G loss: 0.419471]\n",
      "epoch:16 step:15753 [D loss: 0.243228, acc.: 59.38%] [G loss: 0.424146]\n",
      "epoch:16 step:15754 [D loss: 0.232155, acc.: 64.06%] [G loss: 0.439083]\n",
      "epoch:16 step:15755 [D loss: 0.244403, acc.: 62.50%] [G loss: 0.397334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15756 [D loss: 0.234828, acc.: 62.50%] [G loss: 0.461270]\n",
      "epoch:16 step:15757 [D loss: 0.271142, acc.: 53.91%] [G loss: 0.491487]\n",
      "epoch:16 step:15758 [D loss: 0.235867, acc.: 63.28%] [G loss: 0.489689]\n",
      "epoch:16 step:15759 [D loss: 0.228958, acc.: 66.41%] [G loss: 0.444029]\n",
      "epoch:16 step:15760 [D loss: 0.250303, acc.: 60.94%] [G loss: 0.436136]\n",
      "epoch:16 step:15761 [D loss: 0.236964, acc.: 64.06%] [G loss: 0.476574]\n",
      "epoch:16 step:15762 [D loss: 0.233811, acc.: 57.03%] [G loss: 0.456066]\n",
      "epoch:16 step:15763 [D loss: 0.235841, acc.: 60.16%] [G loss: 0.456279]\n",
      "epoch:16 step:15764 [D loss: 0.241684, acc.: 57.81%] [G loss: 0.417182]\n",
      "epoch:16 step:15765 [D loss: 0.217696, acc.: 64.06%] [G loss: 0.396443]\n",
      "epoch:16 step:15766 [D loss: 0.217788, acc.: 62.50%] [G loss: 0.446656]\n",
      "epoch:16 step:15767 [D loss: 0.201918, acc.: 67.19%] [G loss: 0.473480]\n",
      "epoch:16 step:15768 [D loss: 0.238998, acc.: 62.50%] [G loss: 0.445896]\n",
      "epoch:16 step:15769 [D loss: 0.207028, acc.: 69.53%] [G loss: 0.467313]\n",
      "epoch:16 step:15770 [D loss: 0.225201, acc.: 64.84%] [G loss: 0.442833]\n",
      "epoch:16 step:15771 [D loss: 0.210650, acc.: 60.94%] [G loss: 0.436288]\n",
      "epoch:16 step:15772 [D loss: 0.226451, acc.: 60.16%] [G loss: 0.431275]\n",
      "epoch:16 step:15773 [D loss: 0.201118, acc.: 64.06%] [G loss: 0.415420]\n",
      "epoch:16 step:15774 [D loss: 0.196553, acc.: 70.31%] [G loss: 0.505539]\n",
      "epoch:16 step:15775 [D loss: 0.236548, acc.: 55.47%] [G loss: 0.437324]\n",
      "epoch:16 step:15776 [D loss: 0.255713, acc.: 53.12%] [G loss: 0.416383]\n",
      "epoch:16 step:15777 [D loss: 0.226507, acc.: 67.19%] [G loss: 0.386238]\n",
      "epoch:16 step:15778 [D loss: 0.195717, acc.: 67.97%] [G loss: 0.419771]\n",
      "epoch:16 step:15779 [D loss: 0.242529, acc.: 58.59%] [G loss: 0.403196]\n",
      "epoch:16 step:15780 [D loss: 0.247498, acc.: 60.94%] [G loss: 0.390793]\n",
      "epoch:16 step:15781 [D loss: 0.238445, acc.: 60.94%] [G loss: 0.422581]\n",
      "epoch:16 step:15782 [D loss: 0.221946, acc.: 62.50%] [G loss: 0.447146]\n",
      "epoch:16 step:15783 [D loss: 0.219609, acc.: 61.72%] [G loss: 0.442912]\n",
      "epoch:16 step:15784 [D loss: 0.220391, acc.: 63.28%] [G loss: 0.401883]\n",
      "epoch:16 step:15785 [D loss: 0.236392, acc.: 60.16%] [G loss: 0.461282]\n",
      "epoch:16 step:15786 [D loss: 0.281065, acc.: 53.12%] [G loss: 0.480204]\n",
      "epoch:16 step:15787 [D loss: 0.223563, acc.: 63.28%] [G loss: 0.519950]\n",
      "epoch:16 step:15788 [D loss: 0.214301, acc.: 57.03%] [G loss: 0.471679]\n",
      "epoch:16 step:15789 [D loss: 0.254659, acc.: 57.03%] [G loss: 0.442275]\n",
      "epoch:16 step:15790 [D loss: 0.231745, acc.: 59.38%] [G loss: 0.404623]\n",
      "epoch:16 step:15791 [D loss: 0.201814, acc.: 68.75%] [G loss: 0.449888]\n",
      "epoch:16 step:15792 [D loss: 0.253464, acc.: 53.12%] [G loss: 0.437991]\n",
      "epoch:16 step:15793 [D loss: 0.201873, acc.: 69.53%] [G loss: 0.471004]\n",
      "epoch:16 step:15794 [D loss: 0.214569, acc.: 69.53%] [G loss: 0.470121]\n",
      "epoch:16 step:15795 [D loss: 0.218364, acc.: 61.72%] [G loss: 0.460380]\n",
      "epoch:16 step:15796 [D loss: 0.219544, acc.: 63.28%] [G loss: 0.448802]\n",
      "epoch:16 step:15797 [D loss: 0.205441, acc.: 69.53%] [G loss: 0.450494]\n",
      "epoch:16 step:15798 [D loss: 0.270376, acc.: 54.69%] [G loss: 0.411440]\n",
      "epoch:16 step:15799 [D loss: 0.219062, acc.: 63.28%] [G loss: 0.439282]\n",
      "epoch:16 step:15800 [D loss: 0.262329, acc.: 55.47%] [G loss: 0.459844]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 8.067720\n",
      "FID: 6.775568\n",
      "0 = 11.627065330147769\n",
      "1 = 0.03927098902394052\n",
      "2 = 0.867900013923645\n",
      "3 = 0.849399983882904\n",
      "4 = 0.8863999843597412\n",
      "5 = 0.8820353150367737\n",
      "6 = 0.849399983882904\n",
      "7 = 5.752351447534537\n",
      "8 = 0.053250646667852154\n",
      "9 = 0.6777999997138977\n",
      "10 = 0.682699978351593\n",
      "11 = 0.6729000210762024\n",
      "12 = 0.6760744452476501\n",
      "13 = 0.682699978351593\n",
      "14 = 8.067790985107422\n",
      "15 = 9.585648536682129\n",
      "16 = 0.09583845734596252\n",
      "17 = 8.067720413208008\n",
      "18 = 6.775567531585693\n",
      "epoch:16 step:15801 [D loss: 0.226168, acc.: 63.28%] [G loss: 0.424643]\n",
      "epoch:16 step:15802 [D loss: 0.234962, acc.: 57.81%] [G loss: 0.404527]\n",
      "epoch:16 step:15803 [D loss: 0.240622, acc.: 60.16%] [G loss: 0.403919]\n",
      "epoch:16 step:15804 [D loss: 0.235993, acc.: 56.25%] [G loss: 0.427057]\n",
      "epoch:16 step:15805 [D loss: 0.229761, acc.: 61.72%] [G loss: 0.448756]\n",
      "epoch:16 step:15806 [D loss: 0.233095, acc.: 61.72%] [G loss: 0.457167]\n",
      "epoch:16 step:15807 [D loss: 0.231881, acc.: 62.50%] [G loss: 0.495169]\n",
      "epoch:16 step:15808 [D loss: 0.216906, acc.: 64.84%] [G loss: 0.502019]\n",
      "epoch:16 step:15809 [D loss: 0.258464, acc.: 57.81%] [G loss: 0.433071]\n",
      "epoch:16 step:15810 [D loss: 0.250471, acc.: 55.47%] [G loss: 0.420231]\n",
      "epoch:16 step:15811 [D loss: 0.207494, acc.: 69.53%] [G loss: 0.423707]\n",
      "epoch:16 step:15812 [D loss: 0.248792, acc.: 53.91%] [G loss: 0.405253]\n",
      "epoch:16 step:15813 [D loss: 0.227480, acc.: 57.81%] [G loss: 0.427908]\n",
      "epoch:16 step:15814 [D loss: 0.212762, acc.: 62.50%] [G loss: 0.423120]\n",
      "epoch:16 step:15815 [D loss: 0.207358, acc.: 68.75%] [G loss: 0.424526]\n",
      "epoch:16 step:15816 [D loss: 0.235658, acc.: 62.50%] [G loss: 0.455628]\n",
      "epoch:16 step:15817 [D loss: 0.213589, acc.: 66.41%] [G loss: 0.407999]\n",
      "epoch:16 step:15818 [D loss: 0.236966, acc.: 57.03%] [G loss: 0.417545]\n",
      "epoch:16 step:15819 [D loss: 0.243813, acc.: 50.78%] [G loss: 0.386218]\n",
      "epoch:16 step:15820 [D loss: 0.260215, acc.: 50.78%] [G loss: 0.415864]\n",
      "epoch:16 step:15821 [D loss: 0.244030, acc.: 53.91%] [G loss: 0.407973]\n",
      "epoch:16 step:15822 [D loss: 0.209380, acc.: 68.75%] [G loss: 0.437170]\n",
      "epoch:16 step:15823 [D loss: 0.220481, acc.: 64.06%] [G loss: 0.451297]\n",
      "epoch:16 step:15824 [D loss: 0.227608, acc.: 64.06%] [G loss: 0.436772]\n",
      "epoch:16 step:15825 [D loss: 0.200719, acc.: 68.75%] [G loss: 0.436479]\n",
      "epoch:16 step:15826 [D loss: 0.241893, acc.: 60.16%] [G loss: 0.426496]\n",
      "epoch:16 step:15827 [D loss: 0.223918, acc.: 60.94%] [G loss: 0.421137]\n",
      "epoch:16 step:15828 [D loss: 0.214040, acc.: 70.31%] [G loss: 0.444949]\n",
      "epoch:16 step:15829 [D loss: 0.203209, acc.: 66.41%] [G loss: 0.431888]\n",
      "epoch:16 step:15830 [D loss: 0.205036, acc.: 67.97%] [G loss: 0.439601]\n",
      "epoch:16 step:15831 [D loss: 0.205664, acc.: 65.62%] [G loss: 0.445783]\n",
      "epoch:16 step:15832 [D loss: 0.232258, acc.: 63.28%] [G loss: 0.416103]\n",
      "epoch:16 step:15833 [D loss: 0.193179, acc.: 71.88%] [G loss: 0.473635]\n",
      "epoch:16 step:15834 [D loss: 0.197549, acc.: 72.66%] [G loss: 0.487252]\n",
      "epoch:16 step:15835 [D loss: 0.235378, acc.: 57.03%] [G loss: 0.456883]\n",
      "epoch:16 step:15836 [D loss: 0.252518, acc.: 57.03%] [G loss: 0.447790]\n",
      "epoch:16 step:15837 [D loss: 0.210773, acc.: 58.59%] [G loss: 0.449728]\n",
      "epoch:16 step:15838 [D loss: 0.232714, acc.: 60.94%] [G loss: 0.459497]\n",
      "epoch:16 step:15839 [D loss: 0.248615, acc.: 50.78%] [G loss: 0.413704]\n",
      "epoch:16 step:15840 [D loss: 0.243203, acc.: 57.03%] [G loss: 0.386595]\n",
      "epoch:16 step:15841 [D loss: 0.236047, acc.: 57.81%] [G loss: 0.427121]\n",
      "epoch:16 step:15842 [D loss: 0.267641, acc.: 51.56%] [G loss: 0.415950]\n",
      "epoch:16 step:15843 [D loss: 0.220925, acc.: 61.72%] [G loss: 0.448949]\n",
      "epoch:16 step:15844 [D loss: 0.221653, acc.: 67.19%] [G loss: 0.432834]\n",
      "epoch:16 step:15845 [D loss: 0.228565, acc.: 65.62%] [G loss: 0.463029]\n",
      "epoch:16 step:15846 [D loss: 0.236698, acc.: 57.81%] [G loss: 0.403103]\n",
      "epoch:16 step:15847 [D loss: 0.235489, acc.: 59.38%] [G loss: 0.453344]\n",
      "epoch:16 step:15848 [D loss: 0.262740, acc.: 51.56%] [G loss: 0.418568]\n",
      "epoch:16 step:15849 [D loss: 0.209624, acc.: 67.97%] [G loss: 0.423433]\n",
      "epoch:16 step:15850 [D loss: 0.290356, acc.: 50.78%] [G loss: 0.405209]\n",
      "epoch:16 step:15851 [D loss: 0.258992, acc.: 53.91%] [G loss: 0.418723]\n",
      "epoch:16 step:15852 [D loss: 0.196111, acc.: 72.66%] [G loss: 0.452006]\n",
      "epoch:16 step:15853 [D loss: 0.244797, acc.: 57.81%] [G loss: 0.417075]\n",
      "epoch:16 step:15854 [D loss: 0.234927, acc.: 59.38%] [G loss: 0.415956]\n",
      "epoch:16 step:15855 [D loss: 0.199769, acc.: 67.97%] [G loss: 0.410093]\n",
      "epoch:16 step:15856 [D loss: 0.252887, acc.: 56.25%] [G loss: 0.383703]\n",
      "epoch:16 step:15857 [D loss: 0.236094, acc.: 59.38%] [G loss: 0.408532]\n",
      "epoch:16 step:15858 [D loss: 0.237835, acc.: 60.16%] [G loss: 0.401854]\n",
      "epoch:16 step:15859 [D loss: 0.244042, acc.: 57.03%] [G loss: 0.396766]\n",
      "epoch:16 step:15860 [D loss: 0.221575, acc.: 65.62%] [G loss: 0.426693]\n",
      "epoch:16 step:15861 [D loss: 0.238325, acc.: 59.38%] [G loss: 0.427160]\n",
      "epoch:16 step:15862 [D loss: 0.230376, acc.: 61.72%] [G loss: 0.409242]\n",
      "epoch:16 step:15863 [D loss: 0.219400, acc.: 66.41%] [G loss: 0.429993]\n",
      "epoch:16 step:15864 [D loss: 0.220002, acc.: 63.28%] [G loss: 0.411117]\n",
      "epoch:16 step:15865 [D loss: 0.248319, acc.: 60.16%] [G loss: 0.387250]\n",
      "epoch:16 step:15866 [D loss: 0.240158, acc.: 58.59%] [G loss: 0.429939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15867 [D loss: 0.201999, acc.: 68.75%] [G loss: 0.494149]\n",
      "epoch:16 step:15868 [D loss: 0.236035, acc.: 62.50%] [G loss: 0.443308]\n",
      "epoch:16 step:15869 [D loss: 0.236534, acc.: 58.59%] [G loss: 0.444567]\n",
      "epoch:16 step:15870 [D loss: 0.233274, acc.: 61.72%] [G loss: 0.397706]\n",
      "epoch:16 step:15871 [D loss: 0.224267, acc.: 62.50%] [G loss: 0.426304]\n",
      "epoch:16 step:15872 [D loss: 0.250191, acc.: 52.34%] [G loss: 0.386847]\n",
      "epoch:16 step:15873 [D loss: 0.227539, acc.: 53.12%] [G loss: 0.408969]\n",
      "epoch:16 step:15874 [D loss: 0.209834, acc.: 68.75%] [G loss: 0.419186]\n",
      "epoch:16 step:15875 [D loss: 0.226471, acc.: 69.53%] [G loss: 0.429034]\n",
      "epoch:16 step:15876 [D loss: 0.201059, acc.: 66.41%] [G loss: 0.456725]\n",
      "epoch:16 step:15877 [D loss: 0.202866, acc.: 72.66%] [G loss: 0.497684]\n",
      "epoch:16 step:15878 [D loss: 0.234580, acc.: 62.50%] [G loss: 0.483503]\n",
      "epoch:16 step:15879 [D loss: 0.244405, acc.: 56.25%] [G loss: 0.454101]\n",
      "epoch:16 step:15880 [D loss: 0.228949, acc.: 64.06%] [G loss: 0.416984]\n",
      "epoch:16 step:15881 [D loss: 0.212466, acc.: 64.84%] [G loss: 0.471614]\n",
      "epoch:16 step:15882 [D loss: 0.179331, acc.: 77.34%] [G loss: 0.455919]\n",
      "epoch:16 step:15883 [D loss: 0.277378, acc.: 51.56%] [G loss: 0.383979]\n",
      "epoch:16 step:15884 [D loss: 0.260186, acc.: 49.22%] [G loss: 0.374492]\n",
      "epoch:16 step:15885 [D loss: 0.209110, acc.: 70.31%] [G loss: 0.445083]\n",
      "epoch:16 step:15886 [D loss: 0.222104, acc.: 62.50%] [G loss: 0.466426]\n",
      "epoch:16 step:15887 [D loss: 0.215794, acc.: 63.28%] [G loss: 0.472760]\n",
      "epoch:16 step:15888 [D loss: 0.205340, acc.: 67.97%] [G loss: 0.484682]\n",
      "epoch:16 step:15889 [D loss: 0.212272, acc.: 66.41%] [G loss: 0.484831]\n",
      "epoch:16 step:15890 [D loss: 0.213284, acc.: 67.19%] [G loss: 0.467712]\n",
      "epoch:16 step:15891 [D loss: 0.194699, acc.: 67.19%] [G loss: 0.428414]\n",
      "epoch:16 step:15892 [D loss: 0.216665, acc.: 67.97%] [G loss: 0.458630]\n",
      "epoch:16 step:15893 [D loss: 0.226338, acc.: 65.62%] [G loss: 0.507494]\n",
      "epoch:16 step:15894 [D loss: 0.230809, acc.: 58.59%] [G loss: 0.428192]\n",
      "epoch:16 step:15895 [D loss: 0.234340, acc.: 67.97%] [G loss: 0.425016]\n",
      "epoch:16 step:15896 [D loss: 0.238298, acc.: 60.94%] [G loss: 0.411749]\n",
      "epoch:16 step:15897 [D loss: 0.197423, acc.: 71.88%] [G loss: 0.437225]\n",
      "epoch:16 step:15898 [D loss: 0.221710, acc.: 71.09%] [G loss: 0.417805]\n",
      "epoch:16 step:15899 [D loss: 0.236641, acc.: 56.25%] [G loss: 0.436719]\n",
      "epoch:16 step:15900 [D loss: 0.235562, acc.: 62.50%] [G loss: 0.416386]\n",
      "epoch:16 step:15901 [D loss: 0.190202, acc.: 78.91%] [G loss: 0.436476]\n",
      "epoch:16 step:15902 [D loss: 0.232999, acc.: 56.25%] [G loss: 0.440557]\n",
      "epoch:16 step:15903 [D loss: 0.213887, acc.: 69.53%] [G loss: 0.455606]\n",
      "epoch:16 step:15904 [D loss: 0.177186, acc.: 71.09%] [G loss: 0.459978]\n",
      "epoch:16 step:15905 [D loss: 0.213294, acc.: 65.62%] [G loss: 0.456765]\n",
      "epoch:16 step:15906 [D loss: 0.238709, acc.: 62.50%] [G loss: 0.477278]\n",
      "epoch:16 step:15907 [D loss: 0.245441, acc.: 60.16%] [G loss: 0.446108]\n",
      "epoch:16 step:15908 [D loss: 0.245442, acc.: 59.38%] [G loss: 0.449252]\n",
      "epoch:16 step:15909 [D loss: 0.217274, acc.: 67.19%] [G loss: 0.472520]\n",
      "epoch:16 step:15910 [D loss: 0.179354, acc.: 76.56%] [G loss: 0.542870]\n",
      "epoch:16 step:15911 [D loss: 0.233794, acc.: 59.38%] [G loss: 0.506432]\n",
      "epoch:16 step:15912 [D loss: 0.279986, acc.: 50.00%] [G loss: 0.399276]\n",
      "epoch:16 step:15913 [D loss: 0.221021, acc.: 62.50%] [G loss: 0.456266]\n",
      "epoch:16 step:15914 [D loss: 0.252028, acc.: 57.03%] [G loss: 0.435594]\n",
      "epoch:16 step:15915 [D loss: 0.213245, acc.: 68.75%] [G loss: 0.434187]\n",
      "epoch:16 step:15916 [D loss: 0.192641, acc.: 68.75%] [G loss: 0.483159]\n",
      "epoch:16 step:15917 [D loss: 0.171349, acc.: 78.12%] [G loss: 0.500907]\n",
      "epoch:16 step:15918 [D loss: 0.173033, acc.: 76.56%] [G loss: 0.527121]\n",
      "epoch:16 step:15919 [D loss: 0.189820, acc.: 75.00%] [G loss: 0.564188]\n",
      "epoch:16 step:15920 [D loss: 0.307860, acc.: 51.56%] [G loss: 0.576169]\n",
      "epoch:16 step:15921 [D loss: 0.210233, acc.: 68.75%] [G loss: 0.671179]\n",
      "epoch:16 step:15922 [D loss: 0.190247, acc.: 68.75%] [G loss: 0.475351]\n",
      "epoch:16 step:15923 [D loss: 0.234048, acc.: 63.28%] [G loss: 0.406953]\n",
      "epoch:16 step:15924 [D loss: 0.290429, acc.: 47.66%] [G loss: 0.419017]\n",
      "epoch:16 step:15925 [D loss: 0.218159, acc.: 65.62%] [G loss: 0.473364]\n",
      "epoch:16 step:15926 [D loss: 0.222530, acc.: 59.38%] [G loss: 0.420341]\n",
      "epoch:16 step:15927 [D loss: 0.199178, acc.: 70.31%] [G loss: 0.444559]\n",
      "epoch:16 step:15928 [D loss: 0.152620, acc.: 78.12%] [G loss: 0.509693]\n",
      "epoch:16 step:15929 [D loss: 0.202984, acc.: 67.19%] [G loss: 0.546231]\n",
      "epoch:17 step:15930 [D loss: 0.242174, acc.: 65.62%] [G loss: 0.524386]\n",
      "epoch:17 step:15931 [D loss: 0.249884, acc.: 54.69%] [G loss: 0.472887]\n",
      "epoch:17 step:15932 [D loss: 0.264691, acc.: 54.69%] [G loss: 0.480360]\n",
      "epoch:17 step:15933 [D loss: 0.246039, acc.: 57.03%] [G loss: 0.451929]\n",
      "epoch:17 step:15934 [D loss: 0.225488, acc.: 61.72%] [G loss: 0.446457]\n",
      "epoch:17 step:15935 [D loss: 0.217362, acc.: 65.62%] [G loss: 0.452464]\n",
      "epoch:17 step:15936 [D loss: 0.216361, acc.: 67.19%] [G loss: 0.436709]\n",
      "epoch:17 step:15937 [D loss: 0.206924, acc.: 71.88%] [G loss: 0.463407]\n",
      "epoch:17 step:15938 [D loss: 0.214772, acc.: 68.75%] [G loss: 0.421616]\n",
      "epoch:17 step:15939 [D loss: 0.194914, acc.: 72.66%] [G loss: 0.492980]\n",
      "epoch:17 step:15940 [D loss: 0.224114, acc.: 61.72%] [G loss: 0.435495]\n",
      "epoch:17 step:15941 [D loss: 0.228074, acc.: 61.72%] [G loss: 0.444079]\n",
      "epoch:17 step:15942 [D loss: 0.217937, acc.: 65.62%] [G loss: 0.428636]\n",
      "epoch:17 step:15943 [D loss: 0.217115, acc.: 61.72%] [G loss: 0.430319]\n",
      "epoch:17 step:15944 [D loss: 0.193666, acc.: 69.53%] [G loss: 0.460057]\n",
      "epoch:17 step:15945 [D loss: 0.187442, acc.: 71.09%] [G loss: 0.470441]\n",
      "epoch:17 step:15946 [D loss: 0.263166, acc.: 53.12%] [G loss: 0.446588]\n",
      "epoch:17 step:15947 [D loss: 0.236698, acc.: 60.94%] [G loss: 0.441258]\n",
      "epoch:17 step:15948 [D loss: 0.239333, acc.: 60.16%] [G loss: 0.436162]\n",
      "epoch:17 step:15949 [D loss: 0.252840, acc.: 49.22%] [G loss: 0.496343]\n",
      "epoch:17 step:15950 [D loss: 0.225357, acc.: 57.03%] [G loss: 0.451672]\n",
      "epoch:17 step:15951 [D loss: 0.206297, acc.: 67.97%] [G loss: 0.575572]\n",
      "epoch:17 step:15952 [D loss: 0.281317, acc.: 54.69%] [G loss: 0.390742]\n",
      "epoch:17 step:15953 [D loss: 0.239600, acc.: 66.41%] [G loss: 0.417234]\n",
      "epoch:17 step:15954 [D loss: 0.218050, acc.: 65.62%] [G loss: 0.406538]\n",
      "epoch:17 step:15955 [D loss: 0.231002, acc.: 60.94%] [G loss: 0.412479]\n",
      "epoch:17 step:15956 [D loss: 0.238692, acc.: 57.81%] [G loss: 0.414386]\n",
      "epoch:17 step:15957 [D loss: 0.201371, acc.: 64.84%] [G loss: 0.426022]\n",
      "epoch:17 step:15958 [D loss: 0.205350, acc.: 65.62%] [G loss: 0.471290]\n",
      "epoch:17 step:15959 [D loss: 0.216434, acc.: 62.50%] [G loss: 0.450862]\n",
      "epoch:17 step:15960 [D loss: 0.214153, acc.: 70.31%] [G loss: 0.457701]\n",
      "epoch:17 step:15961 [D loss: 0.225715, acc.: 63.28%] [G loss: 0.423934]\n",
      "epoch:17 step:15962 [D loss: 0.227636, acc.: 59.38%] [G loss: 0.418682]\n",
      "epoch:17 step:15963 [D loss: 0.257318, acc.: 53.91%] [G loss: 0.410743]\n",
      "epoch:17 step:15964 [D loss: 0.210982, acc.: 67.19%] [G loss: 0.425726]\n",
      "epoch:17 step:15965 [D loss: 0.217741, acc.: 62.50%] [G loss: 0.458081]\n",
      "epoch:17 step:15966 [D loss: 0.223878, acc.: 64.06%] [G loss: 0.407698]\n",
      "epoch:17 step:15967 [D loss: 0.260084, acc.: 52.34%] [G loss: 0.376073]\n",
      "epoch:17 step:15968 [D loss: 0.217274, acc.: 66.41%] [G loss: 0.441214]\n",
      "epoch:17 step:15969 [D loss: 0.191162, acc.: 68.75%] [G loss: 0.483208]\n",
      "epoch:17 step:15970 [D loss: 0.248343, acc.: 60.94%] [G loss: 0.442616]\n",
      "epoch:17 step:15971 [D loss: 0.242942, acc.: 61.72%] [G loss: 0.423809]\n",
      "epoch:17 step:15972 [D loss: 0.208263, acc.: 68.75%] [G loss: 0.430370]\n",
      "epoch:17 step:15973 [D loss: 0.255413, acc.: 53.12%] [G loss: 0.404118]\n",
      "epoch:17 step:15974 [D loss: 0.216403, acc.: 61.72%] [G loss: 0.428554]\n",
      "epoch:17 step:15975 [D loss: 0.222309, acc.: 60.16%] [G loss: 0.444740]\n",
      "epoch:17 step:15976 [D loss: 0.214059, acc.: 68.75%] [G loss: 0.421976]\n",
      "epoch:17 step:15977 [D loss: 0.201012, acc.: 67.19%] [G loss: 0.455925]\n",
      "epoch:17 step:15978 [D loss: 0.204874, acc.: 67.19%] [G loss: 0.477921]\n",
      "epoch:17 step:15979 [D loss: 0.202665, acc.: 70.31%] [G loss: 0.481252]\n",
      "epoch:17 step:15980 [D loss: 0.248864, acc.: 58.59%] [G loss: 0.439789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:15981 [D loss: 0.219614, acc.: 60.94%] [G loss: 0.478928]\n",
      "epoch:17 step:15982 [D loss: 0.226181, acc.: 60.94%] [G loss: 0.420174]\n",
      "epoch:17 step:15983 [D loss: 0.221665, acc.: 57.03%] [G loss: 0.461500]\n",
      "epoch:17 step:15984 [D loss: 0.202911, acc.: 68.75%] [G loss: 0.471698]\n",
      "epoch:17 step:15985 [D loss: 0.230285, acc.: 64.84%] [G loss: 0.486160]\n",
      "epoch:17 step:15986 [D loss: 0.224663, acc.: 63.28%] [G loss: 0.455058]\n",
      "epoch:17 step:15987 [D loss: 0.221052, acc.: 62.50%] [G loss: 0.435202]\n",
      "epoch:17 step:15988 [D loss: 0.222703, acc.: 66.41%] [G loss: 0.434453]\n",
      "epoch:17 step:15989 [D loss: 0.245825, acc.: 62.50%] [G loss: 0.412033]\n",
      "epoch:17 step:15990 [D loss: 0.225614, acc.: 64.06%] [G loss: 0.398827]\n",
      "epoch:17 step:15991 [D loss: 0.223201, acc.: 67.19%] [G loss: 0.429378]\n",
      "epoch:17 step:15992 [D loss: 0.226709, acc.: 65.62%] [G loss: 0.438464]\n",
      "epoch:17 step:15993 [D loss: 0.257870, acc.: 60.94%] [G loss: 0.435560]\n",
      "epoch:17 step:15994 [D loss: 0.227537, acc.: 64.06%] [G loss: 0.441873]\n",
      "epoch:17 step:15995 [D loss: 0.215992, acc.: 65.62%] [G loss: 0.428362]\n",
      "epoch:17 step:15996 [D loss: 0.231391, acc.: 63.28%] [G loss: 0.416893]\n",
      "epoch:17 step:15997 [D loss: 0.243967, acc.: 61.72%] [G loss: 0.411940]\n",
      "epoch:17 step:15998 [D loss: 0.206012, acc.: 69.53%] [G loss: 0.458568]\n",
      "epoch:17 step:15999 [D loss: 0.201225, acc.: 68.75%] [G loss: 0.463264]\n",
      "epoch:17 step:16000 [D loss: 0.245389, acc.: 56.25%] [G loss: 0.444886]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.972402\n",
      "FID: 7.722383\n",
      "0 = 11.753259523177096\n",
      "1 = 0.046201315335131846\n",
      "2 = 0.871150016784668\n",
      "3 = 0.859499990940094\n",
      "4 = 0.8827999830245972\n",
      "5 = 0.8800041079521179\n",
      "6 = 0.859499990940094\n",
      "7 = 5.88152781899575\n",
      "8 = 0.05757061644085784\n",
      "9 = 0.6851000189781189\n",
      "10 = 0.6840999722480774\n",
      "11 = 0.6861000061035156\n",
      "12 = 0.6854709386825562\n",
      "13 = 0.6840999722480774\n",
      "14 = 7.972465515136719\n",
      "15 = 9.51933765411377\n",
      "16 = 0.11428327113389969\n",
      "17 = 7.972402095794678\n",
      "18 = 7.722383499145508\n",
      "epoch:17 step:16001 [D loss: 0.229399, acc.: 62.50%] [G loss: 0.426727]\n",
      "epoch:17 step:16002 [D loss: 0.235389, acc.: 58.59%] [G loss: 0.424774]\n",
      "epoch:17 step:16003 [D loss: 0.219129, acc.: 64.06%] [G loss: 0.443916]\n",
      "epoch:17 step:16004 [D loss: 0.213621, acc.: 64.84%] [G loss: 0.446188]\n",
      "epoch:17 step:16005 [D loss: 0.209203, acc.: 64.84%] [G loss: 0.436875]\n",
      "epoch:17 step:16006 [D loss: 0.183083, acc.: 74.22%] [G loss: 0.524374]\n",
      "epoch:17 step:16007 [D loss: 0.287342, acc.: 50.00%] [G loss: 0.427778]\n",
      "epoch:17 step:16008 [D loss: 0.249292, acc.: 57.81%] [G loss: 0.400902]\n",
      "epoch:17 step:16009 [D loss: 0.245115, acc.: 55.47%] [G loss: 0.391823]\n",
      "epoch:17 step:16010 [D loss: 0.228058, acc.: 64.84%] [G loss: 0.415839]\n",
      "epoch:17 step:16011 [D loss: 0.224577, acc.: 60.94%] [G loss: 0.453632]\n",
      "epoch:17 step:16012 [D loss: 0.218470, acc.: 63.28%] [G loss: 0.423272]\n",
      "epoch:17 step:16013 [D loss: 0.205145, acc.: 68.75%] [G loss: 0.437034]\n",
      "epoch:17 step:16014 [D loss: 0.261191, acc.: 49.22%] [G loss: 0.431678]\n",
      "epoch:17 step:16015 [D loss: 0.215820, acc.: 63.28%] [G loss: 0.448169]\n",
      "epoch:17 step:16016 [D loss: 0.221150, acc.: 66.41%] [G loss: 0.454592]\n",
      "epoch:17 step:16017 [D loss: 0.228159, acc.: 57.81%] [G loss: 0.445144]\n",
      "epoch:17 step:16018 [D loss: 0.216495, acc.: 66.41%] [G loss: 0.432953]\n",
      "epoch:17 step:16019 [D loss: 0.209665, acc.: 67.19%] [G loss: 0.440374]\n",
      "epoch:17 step:16020 [D loss: 0.250390, acc.: 59.38%] [G loss: 0.443659]\n",
      "epoch:17 step:16021 [D loss: 0.221283, acc.: 64.06%] [G loss: 0.418111]\n",
      "epoch:17 step:16022 [D loss: 0.207008, acc.: 66.41%] [G loss: 0.487344]\n",
      "epoch:17 step:16023 [D loss: 0.222100, acc.: 64.06%] [G loss: 0.426196]\n",
      "epoch:17 step:16024 [D loss: 0.237522, acc.: 55.47%] [G loss: 0.420022]\n",
      "epoch:17 step:16025 [D loss: 0.221655, acc.: 64.84%] [G loss: 0.458535]\n",
      "epoch:17 step:16026 [D loss: 0.213387, acc.: 67.19%] [G loss: 0.479507]\n",
      "epoch:17 step:16027 [D loss: 0.226070, acc.: 65.62%] [G loss: 0.472247]\n",
      "epoch:17 step:16028 [D loss: 0.220054, acc.: 63.28%] [G loss: 0.476261]\n",
      "epoch:17 step:16029 [D loss: 0.191408, acc.: 67.97%] [G loss: 0.456802]\n",
      "epoch:17 step:16030 [D loss: 0.239013, acc.: 57.81%] [G loss: 0.442763]\n",
      "epoch:17 step:16031 [D loss: 0.230763, acc.: 62.50%] [G loss: 0.463990]\n",
      "epoch:17 step:16032 [D loss: 0.242921, acc.: 57.03%] [G loss: 0.413010]\n",
      "epoch:17 step:16033 [D loss: 0.240615, acc.: 58.59%] [G loss: 0.422415]\n",
      "epoch:17 step:16034 [D loss: 0.251236, acc.: 54.69%] [G loss: 0.421505]\n",
      "epoch:17 step:16035 [D loss: 0.236607, acc.: 61.72%] [G loss: 0.409905]\n",
      "epoch:17 step:16036 [D loss: 0.210103, acc.: 71.88%] [G loss: 0.436587]\n",
      "epoch:17 step:16037 [D loss: 0.258135, acc.: 60.94%] [G loss: 0.456592]\n",
      "epoch:17 step:16038 [D loss: 0.274453, acc.: 48.44%] [G loss: 0.407871]\n",
      "epoch:17 step:16039 [D loss: 0.246248, acc.: 58.59%] [G loss: 0.448161]\n",
      "epoch:17 step:16040 [D loss: 0.203972, acc.: 71.09%] [G loss: 0.450084]\n",
      "epoch:17 step:16041 [D loss: 0.207726, acc.: 73.44%] [G loss: 0.452159]\n",
      "epoch:17 step:16042 [D loss: 0.218234, acc.: 67.19%] [G loss: 0.445279]\n",
      "epoch:17 step:16043 [D loss: 0.233898, acc.: 63.28%] [G loss: 0.471192]\n",
      "epoch:17 step:16044 [D loss: 0.216223, acc.: 66.41%] [G loss: 0.500367]\n",
      "epoch:17 step:16045 [D loss: 0.219404, acc.: 65.62%] [G loss: 0.508334]\n",
      "epoch:17 step:16046 [D loss: 0.212995, acc.: 62.50%] [G loss: 0.447518]\n",
      "epoch:17 step:16047 [D loss: 0.218110, acc.: 64.06%] [G loss: 0.448832]\n",
      "epoch:17 step:16048 [D loss: 0.195326, acc.: 72.66%] [G loss: 0.506139]\n",
      "epoch:17 step:16049 [D loss: 0.213797, acc.: 67.19%] [G loss: 0.494221]\n",
      "epoch:17 step:16050 [D loss: 0.225110, acc.: 64.06%] [G loss: 0.443337]\n",
      "epoch:17 step:16051 [D loss: 0.196782, acc.: 73.44%] [G loss: 0.465524]\n",
      "epoch:17 step:16052 [D loss: 0.213881, acc.: 67.19%] [G loss: 0.463852]\n",
      "epoch:17 step:16053 [D loss: 0.261284, acc.: 53.12%] [G loss: 0.409109]\n",
      "epoch:17 step:16054 [D loss: 0.203950, acc.: 70.31%] [G loss: 0.444745]\n",
      "epoch:17 step:16055 [D loss: 0.197388, acc.: 71.09%] [G loss: 0.442107]\n",
      "epoch:17 step:16056 [D loss: 0.224506, acc.: 60.94%] [G loss: 0.435193]\n",
      "epoch:17 step:16057 [D loss: 0.226579, acc.: 61.72%] [G loss: 0.417876]\n",
      "epoch:17 step:16058 [D loss: 0.222197, acc.: 64.06%] [G loss: 0.449979]\n",
      "epoch:17 step:16059 [D loss: 0.200475, acc.: 70.31%] [G loss: 0.467436]\n",
      "epoch:17 step:16060 [D loss: 0.237507, acc.: 60.94%] [G loss: 0.397433]\n",
      "epoch:17 step:16061 [D loss: 0.221095, acc.: 60.16%] [G loss: 0.435904]\n",
      "epoch:17 step:16062 [D loss: 0.263034, acc.: 57.03%] [G loss: 0.430294]\n",
      "epoch:17 step:16063 [D loss: 0.230240, acc.: 60.94%] [G loss: 0.443495]\n",
      "epoch:17 step:16064 [D loss: 0.207963, acc.: 69.53%] [G loss: 0.475420]\n",
      "epoch:17 step:16065 [D loss: 0.207613, acc.: 71.88%] [G loss: 0.487561]\n",
      "epoch:17 step:16066 [D loss: 0.263863, acc.: 50.78%] [G loss: 0.412644]\n",
      "epoch:17 step:16067 [D loss: 0.244953, acc.: 57.03%] [G loss: 0.414153]\n",
      "epoch:17 step:16068 [D loss: 0.204605, acc.: 65.62%] [G loss: 0.462610]\n",
      "epoch:17 step:16069 [D loss: 0.233432, acc.: 60.16%] [G loss: 0.402316]\n",
      "epoch:17 step:16070 [D loss: 0.245707, acc.: 54.69%] [G loss: 0.379027]\n",
      "epoch:17 step:16071 [D loss: 0.225740, acc.: 57.81%] [G loss: 0.426828]\n",
      "epoch:17 step:16072 [D loss: 0.226940, acc.: 58.59%] [G loss: 0.391434]\n",
      "epoch:17 step:16073 [D loss: 0.207600, acc.: 65.62%] [G loss: 0.433189]\n",
      "epoch:17 step:16074 [D loss: 0.234529, acc.: 59.38%] [G loss: 0.403931]\n",
      "epoch:17 step:16075 [D loss: 0.224191, acc.: 65.62%] [G loss: 0.445641]\n",
      "epoch:17 step:16076 [D loss: 0.251468, acc.: 58.59%] [G loss: 0.428291]\n",
      "epoch:17 step:16077 [D loss: 0.248147, acc.: 57.03%] [G loss: 0.382857]\n",
      "epoch:17 step:16078 [D loss: 0.233516, acc.: 63.28%] [G loss: 0.408339]\n",
      "epoch:17 step:16079 [D loss: 0.225470, acc.: 64.06%] [G loss: 0.468445]\n",
      "epoch:17 step:16080 [D loss: 0.214894, acc.: 68.75%] [G loss: 0.419267]\n",
      "epoch:17 step:16081 [D loss: 0.238313, acc.: 64.84%] [G loss: 0.418967]\n",
      "epoch:17 step:16082 [D loss: 0.252878, acc.: 56.25%] [G loss: 0.412412]\n",
      "epoch:17 step:16083 [D loss: 0.232315, acc.: 60.94%] [G loss: 0.428459]\n",
      "epoch:17 step:16084 [D loss: 0.207787, acc.: 64.06%] [G loss: 0.438140]\n",
      "epoch:17 step:16085 [D loss: 0.235600, acc.: 59.38%] [G loss: 0.443258]\n",
      "epoch:17 step:16086 [D loss: 0.222354, acc.: 65.62%] [G loss: 0.435090]\n",
      "epoch:17 step:16087 [D loss: 0.219352, acc.: 65.62%] [G loss: 0.460486]\n",
      "epoch:17 step:16088 [D loss: 0.197000, acc.: 70.31%] [G loss: 0.445459]\n",
      "epoch:17 step:16089 [D loss: 0.253646, acc.: 57.03%] [G loss: 0.440487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16090 [D loss: 0.231571, acc.: 60.16%] [G loss: 0.445358]\n",
      "epoch:17 step:16091 [D loss: 0.240235, acc.: 60.94%] [G loss: 0.419578]\n",
      "epoch:17 step:16092 [D loss: 0.245587, acc.: 55.47%] [G loss: 0.451441]\n",
      "epoch:17 step:16093 [D loss: 0.207691, acc.: 66.41%] [G loss: 0.469495]\n",
      "epoch:17 step:16094 [D loss: 0.240174, acc.: 58.59%] [G loss: 0.426580]\n",
      "epoch:17 step:16095 [D loss: 0.219441, acc.: 70.31%] [G loss: 0.413851]\n",
      "epoch:17 step:16096 [D loss: 0.222912, acc.: 61.72%] [G loss: 0.399859]\n",
      "epoch:17 step:16097 [D loss: 0.211346, acc.: 65.62%] [G loss: 0.422777]\n",
      "epoch:17 step:16098 [D loss: 0.219717, acc.: 63.28%] [G loss: 0.440780]\n",
      "epoch:17 step:16099 [D loss: 0.243828, acc.: 56.25%] [G loss: 0.434411]\n",
      "epoch:17 step:16100 [D loss: 0.245822, acc.: 53.91%] [G loss: 0.437743]\n",
      "epoch:17 step:16101 [D loss: 0.208024, acc.: 60.94%] [G loss: 0.489428]\n",
      "epoch:17 step:16102 [D loss: 0.226845, acc.: 61.72%] [G loss: 0.440496]\n",
      "epoch:17 step:16103 [D loss: 0.254218, acc.: 52.34%] [G loss: 0.402653]\n",
      "epoch:17 step:16104 [D loss: 0.226283, acc.: 64.06%] [G loss: 0.465271]\n",
      "epoch:17 step:16105 [D loss: 0.200156, acc.: 71.88%] [G loss: 0.441911]\n",
      "epoch:17 step:16106 [D loss: 0.230558, acc.: 63.28%] [G loss: 0.396744]\n",
      "epoch:17 step:16107 [D loss: 0.261177, acc.: 56.25%] [G loss: 0.397416]\n",
      "epoch:17 step:16108 [D loss: 0.237083, acc.: 61.72%] [G loss: 0.412515]\n",
      "epoch:17 step:16109 [D loss: 0.233009, acc.: 60.16%] [G loss: 0.458363]\n",
      "epoch:17 step:16110 [D loss: 0.222609, acc.: 64.84%] [G loss: 0.407336]\n",
      "epoch:17 step:16111 [D loss: 0.260987, acc.: 53.91%] [G loss: 0.377027]\n",
      "epoch:17 step:16112 [D loss: 0.244779, acc.: 53.91%] [G loss: 0.425127]\n",
      "epoch:17 step:16113 [D loss: 0.224524, acc.: 61.72%] [G loss: 0.424548]\n",
      "epoch:17 step:16114 [D loss: 0.225891, acc.: 66.41%] [G loss: 0.458630]\n",
      "epoch:17 step:16115 [D loss: 0.238937, acc.: 57.81%] [G loss: 0.426264]\n",
      "epoch:17 step:16116 [D loss: 0.237998, acc.: 58.59%] [G loss: 0.443849]\n",
      "epoch:17 step:16117 [D loss: 0.227512, acc.: 60.16%] [G loss: 0.441864]\n",
      "epoch:17 step:16118 [D loss: 0.247723, acc.: 53.91%] [G loss: 0.405560]\n",
      "epoch:17 step:16119 [D loss: 0.227625, acc.: 66.41%] [G loss: 0.404091]\n",
      "epoch:17 step:16120 [D loss: 0.209390, acc.: 66.41%] [G loss: 0.476812]\n",
      "epoch:17 step:16121 [D loss: 0.200375, acc.: 71.88%] [G loss: 0.445556]\n",
      "epoch:17 step:16122 [D loss: 0.229660, acc.: 62.50%] [G loss: 0.431548]\n",
      "epoch:17 step:16123 [D loss: 0.183893, acc.: 75.78%] [G loss: 0.478938]\n",
      "epoch:17 step:16124 [D loss: 0.217821, acc.: 67.19%] [G loss: 0.458960]\n",
      "epoch:17 step:16125 [D loss: 0.219831, acc.: 66.41%] [G loss: 0.451633]\n",
      "epoch:17 step:16126 [D loss: 0.221984, acc.: 64.84%] [G loss: 0.412689]\n",
      "epoch:17 step:16127 [D loss: 0.218197, acc.: 66.41%] [G loss: 0.406282]\n",
      "epoch:17 step:16128 [D loss: 0.219104, acc.: 62.50%] [G loss: 0.448993]\n",
      "epoch:17 step:16129 [D loss: 0.240212, acc.: 54.69%] [G loss: 0.412066]\n",
      "epoch:17 step:16130 [D loss: 0.236089, acc.: 58.59%] [G loss: 0.422808]\n",
      "epoch:17 step:16131 [D loss: 0.226645, acc.: 62.50%] [G loss: 0.422492]\n",
      "epoch:17 step:16132 [D loss: 0.255587, acc.: 53.12%] [G loss: 0.427215]\n",
      "epoch:17 step:16133 [D loss: 0.217352, acc.: 65.62%] [G loss: 0.450102]\n",
      "epoch:17 step:16134 [D loss: 0.212006, acc.: 64.84%] [G loss: 0.475050]\n",
      "epoch:17 step:16135 [D loss: 0.228230, acc.: 60.16%] [G loss: 0.483518]\n",
      "epoch:17 step:16136 [D loss: 0.203343, acc.: 69.53%] [G loss: 0.488213]\n",
      "epoch:17 step:16137 [D loss: 0.208466, acc.: 65.62%] [G loss: 0.458872]\n",
      "epoch:17 step:16138 [D loss: 0.196652, acc.: 70.31%] [G loss: 0.489638]\n",
      "epoch:17 step:16139 [D loss: 0.269068, acc.: 50.00%] [G loss: 0.426151]\n",
      "epoch:17 step:16140 [D loss: 0.256137, acc.: 53.91%] [G loss: 0.446459]\n",
      "epoch:17 step:16141 [D loss: 0.227974, acc.: 65.62%] [G loss: 0.406255]\n",
      "epoch:17 step:16142 [D loss: 0.240728, acc.: 60.16%] [G loss: 0.436193]\n",
      "epoch:17 step:16143 [D loss: 0.241827, acc.: 57.03%] [G loss: 0.401618]\n",
      "epoch:17 step:16144 [D loss: 0.256337, acc.: 53.91%] [G loss: 0.426058]\n",
      "epoch:17 step:16145 [D loss: 0.214400, acc.: 64.06%] [G loss: 0.417142]\n",
      "epoch:17 step:16146 [D loss: 0.249569, acc.: 58.59%] [G loss: 0.405741]\n",
      "epoch:17 step:16147 [D loss: 0.196869, acc.: 71.88%] [G loss: 0.457695]\n",
      "epoch:17 step:16148 [D loss: 0.175826, acc.: 75.78%] [G loss: 0.483889]\n",
      "epoch:17 step:16149 [D loss: 0.280075, acc.: 53.91%] [G loss: 0.442676]\n",
      "epoch:17 step:16150 [D loss: 0.206132, acc.: 66.41%] [G loss: 0.450663]\n",
      "epoch:17 step:16151 [D loss: 0.177492, acc.: 75.78%] [G loss: 0.490984]\n",
      "epoch:17 step:16152 [D loss: 0.221214, acc.: 67.97%] [G loss: 0.488810]\n",
      "epoch:17 step:16153 [D loss: 0.236619, acc.: 63.28%] [G loss: 0.476633]\n",
      "epoch:17 step:16154 [D loss: 0.238141, acc.: 60.16%] [G loss: 0.411810]\n",
      "epoch:17 step:16155 [D loss: 0.225641, acc.: 57.81%] [G loss: 0.412073]\n",
      "epoch:17 step:16156 [D loss: 0.226920, acc.: 60.16%] [G loss: 0.427484]\n",
      "epoch:17 step:16157 [D loss: 0.237114, acc.: 59.38%] [G loss: 0.416584]\n",
      "epoch:17 step:16158 [D loss: 0.190335, acc.: 71.88%] [G loss: 0.451197]\n",
      "epoch:17 step:16159 [D loss: 0.203410, acc.: 66.41%] [G loss: 0.428410]\n",
      "epoch:17 step:16160 [D loss: 0.174970, acc.: 76.56%] [G loss: 0.491720]\n",
      "epoch:17 step:16161 [D loss: 0.179882, acc.: 69.53%] [G loss: 0.510609]\n",
      "epoch:17 step:16162 [D loss: 0.234628, acc.: 59.38%] [G loss: 0.466043]\n",
      "epoch:17 step:16163 [D loss: 0.258858, acc.: 53.12%] [G loss: 0.405443]\n",
      "epoch:17 step:16164 [D loss: 0.226163, acc.: 59.38%] [G loss: 0.429364]\n",
      "epoch:17 step:16165 [D loss: 0.234156, acc.: 61.72%] [G loss: 0.381708]\n",
      "epoch:17 step:16166 [D loss: 0.223780, acc.: 61.72%] [G loss: 0.415781]\n",
      "epoch:17 step:16167 [D loss: 0.223902, acc.: 64.84%] [G loss: 0.428949]\n",
      "epoch:17 step:16168 [D loss: 0.203171, acc.: 65.62%] [G loss: 0.426987]\n",
      "epoch:17 step:16169 [D loss: 0.187393, acc.: 74.22%] [G loss: 0.467702]\n",
      "epoch:17 step:16170 [D loss: 0.189248, acc.: 71.09%] [G loss: 0.431418]\n",
      "epoch:17 step:16171 [D loss: 0.189274, acc.: 73.44%] [G loss: 0.457582]\n",
      "epoch:17 step:16172 [D loss: 0.211692, acc.: 66.41%] [G loss: 0.457043]\n",
      "epoch:17 step:16173 [D loss: 0.222674, acc.: 68.75%] [G loss: 0.443596]\n",
      "epoch:17 step:16174 [D loss: 0.217580, acc.: 64.84%] [G loss: 0.456752]\n",
      "epoch:17 step:16175 [D loss: 0.193234, acc.: 71.88%] [G loss: 0.478462]\n",
      "epoch:17 step:16176 [D loss: 0.232115, acc.: 58.59%] [G loss: 0.469026]\n",
      "epoch:17 step:16177 [D loss: 0.211215, acc.: 69.53%] [G loss: 0.483287]\n",
      "epoch:17 step:16178 [D loss: 0.260122, acc.: 53.12%] [G loss: 0.457251]\n",
      "epoch:17 step:16179 [D loss: 0.271138, acc.: 48.44%] [G loss: 0.403776]\n",
      "epoch:17 step:16180 [D loss: 0.273749, acc.: 51.56%] [G loss: 0.400969]\n",
      "epoch:17 step:16181 [D loss: 0.225113, acc.: 63.28%] [G loss: 0.464613]\n",
      "epoch:17 step:16182 [D loss: 0.210035, acc.: 66.41%] [G loss: 0.450375]\n",
      "epoch:17 step:16183 [D loss: 0.204976, acc.: 67.97%] [G loss: 0.470556]\n",
      "epoch:17 step:16184 [D loss: 0.231911, acc.: 64.84%] [G loss: 0.443226]\n",
      "epoch:17 step:16185 [D loss: 0.234986, acc.: 60.16%] [G loss: 0.433987]\n",
      "epoch:17 step:16186 [D loss: 0.227487, acc.: 67.19%] [G loss: 0.471187]\n",
      "epoch:17 step:16187 [D loss: 0.205784, acc.: 63.28%] [G loss: 0.424660]\n",
      "epoch:17 step:16188 [D loss: 0.233227, acc.: 61.72%] [G loss: 0.385678]\n",
      "epoch:17 step:16189 [D loss: 0.212797, acc.: 64.84%] [G loss: 0.424915]\n",
      "epoch:17 step:16190 [D loss: 0.217192, acc.: 64.84%] [G loss: 0.461776]\n",
      "epoch:17 step:16191 [D loss: 0.199441, acc.: 67.19%] [G loss: 0.447635]\n",
      "epoch:17 step:16192 [D loss: 0.232169, acc.: 66.41%] [G loss: 0.448671]\n",
      "epoch:17 step:16193 [D loss: 0.218869, acc.: 62.50%] [G loss: 0.426782]\n",
      "epoch:17 step:16194 [D loss: 0.236550, acc.: 61.72%] [G loss: 0.434922]\n",
      "epoch:17 step:16195 [D loss: 0.236306, acc.: 60.94%] [G loss: 0.420117]\n",
      "epoch:17 step:16196 [D loss: 0.211656, acc.: 63.28%] [G loss: 0.470999]\n",
      "epoch:17 step:16197 [D loss: 0.233017, acc.: 63.28%] [G loss: 0.404441]\n",
      "epoch:17 step:16198 [D loss: 0.231300, acc.: 60.94%] [G loss: 0.429131]\n",
      "epoch:17 step:16199 [D loss: 0.222021, acc.: 66.41%] [G loss: 0.455516]\n",
      "epoch:17 step:16200 [D loss: 0.202015, acc.: 70.31%] [G loss: 0.455875]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.923739\n",
      "FID: 10.382184\n",
      "0 = 11.77446312708855\n",
      "1 = 0.05213988730526339\n",
      "2 = 0.8660500049591064\n",
      "3 = 0.853600025177002\n",
      "4 = 0.8784999847412109\n",
      "5 = 0.8753973841667175\n",
      "6 = 0.853600025177002\n",
      "7 = 6.176987979686242\n",
      "8 = 0.07144107028052665\n",
      "9 = 0.683650016784668\n",
      "10 = 0.6895999908447266\n",
      "11 = 0.6776999831199646\n",
      "12 = 0.6814902424812317\n",
      "13 = 0.6895999908447266\n",
      "14 = 7.92380428314209\n",
      "15 = 9.455828666687012\n",
      "16 = 0.13033518195152283\n",
      "17 = 7.923739433288574\n",
      "18 = 10.382184028625488\n",
      "epoch:17 step:16201 [D loss: 0.213629, acc.: 68.75%] [G loss: 0.450040]\n",
      "epoch:17 step:16202 [D loss: 0.216794, acc.: 71.09%] [G loss: 0.441652]\n",
      "epoch:17 step:16203 [D loss: 0.190116, acc.: 71.88%] [G loss: 0.451256]\n",
      "epoch:17 step:16204 [D loss: 0.199394, acc.: 68.75%] [G loss: 0.459465]\n",
      "epoch:17 step:16205 [D loss: 0.204174, acc.: 71.09%] [G loss: 0.438110]\n",
      "epoch:17 step:16206 [D loss: 0.255862, acc.: 50.00%] [G loss: 0.398771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16207 [D loss: 0.246203, acc.: 57.03%] [G loss: 0.387309]\n",
      "epoch:17 step:16208 [D loss: 0.211947, acc.: 65.62%] [G loss: 0.449602]\n",
      "epoch:17 step:16209 [D loss: 0.201391, acc.: 69.53%] [G loss: 0.450912]\n",
      "epoch:17 step:16210 [D loss: 0.244663, acc.: 57.81%] [G loss: 0.452878]\n",
      "epoch:17 step:16211 [D loss: 0.207602, acc.: 64.06%] [G loss: 0.439755]\n",
      "epoch:17 step:16212 [D loss: 0.210523, acc.: 72.66%] [G loss: 0.429359]\n",
      "epoch:17 step:16213 [D loss: 0.257252, acc.: 53.12%] [G loss: 0.414784]\n",
      "epoch:17 step:16214 [D loss: 0.230841, acc.: 61.72%] [G loss: 0.418230]\n",
      "epoch:17 step:16215 [D loss: 0.216331, acc.: 64.84%] [G loss: 0.422623]\n",
      "epoch:17 step:16216 [D loss: 0.211201, acc.: 66.41%] [G loss: 0.462664]\n",
      "epoch:17 step:16217 [D loss: 0.222304, acc.: 66.41%] [G loss: 0.411174]\n",
      "epoch:17 step:16218 [D loss: 0.200988, acc.: 69.53%] [G loss: 0.450018]\n",
      "epoch:17 step:16219 [D loss: 0.234078, acc.: 60.16%] [G loss: 0.470249]\n",
      "epoch:17 step:16220 [D loss: 0.255064, acc.: 60.16%] [G loss: 0.434115]\n",
      "epoch:17 step:16221 [D loss: 0.222975, acc.: 60.16%] [G loss: 0.416966]\n",
      "epoch:17 step:16222 [D loss: 0.235779, acc.: 63.28%] [G loss: 0.422658]\n",
      "epoch:17 step:16223 [D loss: 0.257875, acc.: 56.25%] [G loss: 0.391127]\n",
      "epoch:17 step:16224 [D loss: 0.248919, acc.: 50.00%] [G loss: 0.425580]\n",
      "epoch:17 step:16225 [D loss: 0.214418, acc.: 63.28%] [G loss: 0.440382]\n",
      "epoch:17 step:16226 [D loss: 0.222030, acc.: 59.38%] [G loss: 0.440652]\n",
      "epoch:17 step:16227 [D loss: 0.193268, acc.: 73.44%] [G loss: 0.466106]\n",
      "epoch:17 step:16228 [D loss: 0.219219, acc.: 64.06%] [G loss: 0.419719]\n",
      "epoch:17 step:16229 [D loss: 0.185326, acc.: 75.78%] [G loss: 0.480136]\n",
      "epoch:17 step:16230 [D loss: 0.264047, acc.: 55.47%] [G loss: 0.414163]\n",
      "epoch:17 step:16231 [D loss: 0.226568, acc.: 64.06%] [G loss: 0.434546]\n",
      "epoch:17 step:16232 [D loss: 0.234683, acc.: 57.81%] [G loss: 0.408702]\n",
      "epoch:17 step:16233 [D loss: 0.203828, acc.: 67.19%] [G loss: 0.468742]\n",
      "epoch:17 step:16234 [D loss: 0.234650, acc.: 58.59%] [G loss: 0.466719]\n",
      "epoch:17 step:16235 [D loss: 0.220948, acc.: 63.28%] [G loss: 0.502691]\n",
      "epoch:17 step:16236 [D loss: 0.235325, acc.: 61.72%] [G loss: 0.424703]\n",
      "epoch:17 step:16237 [D loss: 0.215700, acc.: 63.28%] [G loss: 0.438882]\n",
      "epoch:17 step:16238 [D loss: 0.212404, acc.: 64.06%] [G loss: 0.449076]\n",
      "epoch:17 step:16239 [D loss: 0.218611, acc.: 67.19%] [G loss: 0.433235]\n",
      "epoch:17 step:16240 [D loss: 0.223252, acc.: 60.94%] [G loss: 0.445638]\n",
      "epoch:17 step:16241 [D loss: 0.175541, acc.: 74.22%] [G loss: 0.465086]\n",
      "epoch:17 step:16242 [D loss: 0.197761, acc.: 68.75%] [G loss: 0.486033]\n",
      "epoch:17 step:16243 [D loss: 0.210542, acc.: 67.19%] [G loss: 0.458586]\n",
      "epoch:17 step:16244 [D loss: 0.237086, acc.: 63.28%] [G loss: 0.481490]\n",
      "epoch:17 step:16245 [D loss: 0.261572, acc.: 57.81%] [G loss: 0.457660]\n",
      "epoch:17 step:16246 [D loss: 0.247868, acc.: 51.56%] [G loss: 0.392943]\n",
      "epoch:17 step:16247 [D loss: 0.212012, acc.: 67.97%] [G loss: 0.443515]\n",
      "epoch:17 step:16248 [D loss: 0.220681, acc.: 57.81%] [G loss: 0.419596]\n",
      "epoch:17 step:16249 [D loss: 0.230403, acc.: 61.72%] [G loss: 0.383798]\n",
      "epoch:17 step:16250 [D loss: 0.228916, acc.: 64.06%] [G loss: 0.421165]\n",
      "epoch:17 step:16251 [D loss: 0.223745, acc.: 59.38%] [G loss: 0.443310]\n",
      "epoch:17 step:16252 [D loss: 0.232184, acc.: 60.94%] [G loss: 0.427022]\n",
      "epoch:17 step:16253 [D loss: 0.205594, acc.: 65.62%] [G loss: 0.450083]\n",
      "epoch:17 step:16254 [D loss: 0.219321, acc.: 64.84%] [G loss: 0.460957]\n",
      "epoch:17 step:16255 [D loss: 0.224696, acc.: 65.62%] [G loss: 0.419146]\n",
      "epoch:17 step:16256 [D loss: 0.235181, acc.: 60.94%] [G loss: 0.487681]\n",
      "epoch:17 step:16257 [D loss: 0.191641, acc.: 70.31%] [G loss: 0.510943]\n",
      "epoch:17 step:16258 [D loss: 0.232260, acc.: 63.28%] [G loss: 0.447630]\n",
      "epoch:17 step:16259 [D loss: 0.250278, acc.: 51.56%] [G loss: 0.426766]\n",
      "epoch:17 step:16260 [D loss: 0.237921, acc.: 63.28%] [G loss: 0.408712]\n",
      "epoch:17 step:16261 [D loss: 0.207207, acc.: 67.97%] [G loss: 0.447565]\n",
      "epoch:17 step:16262 [D loss: 0.206004, acc.: 66.41%] [G loss: 0.432607]\n",
      "epoch:17 step:16263 [D loss: 0.196143, acc.: 71.09%] [G loss: 0.440965]\n",
      "epoch:17 step:16264 [D loss: 0.222041, acc.: 63.28%] [G loss: 0.456856]\n",
      "epoch:17 step:16265 [D loss: 0.196871, acc.: 70.31%] [G loss: 0.488976]\n",
      "epoch:17 step:16266 [D loss: 0.223761, acc.: 62.50%] [G loss: 0.410199]\n",
      "epoch:17 step:16267 [D loss: 0.232644, acc.: 61.72%] [G loss: 0.447857]\n",
      "epoch:17 step:16268 [D loss: 0.229283, acc.: 61.72%] [G loss: 0.453803]\n",
      "epoch:17 step:16269 [D loss: 0.221160, acc.: 64.84%] [G loss: 0.461520]\n",
      "epoch:17 step:16270 [D loss: 0.262685, acc.: 55.47%] [G loss: 0.418135]\n",
      "epoch:17 step:16271 [D loss: 0.249239, acc.: 54.69%] [G loss: 0.430709]\n",
      "epoch:17 step:16272 [D loss: 0.216310, acc.: 68.75%] [G loss: 0.442031]\n",
      "epoch:17 step:16273 [D loss: 0.228390, acc.: 67.97%] [G loss: 0.463117]\n",
      "epoch:17 step:16274 [D loss: 0.223298, acc.: 64.06%] [G loss: 0.436000]\n",
      "epoch:17 step:16275 [D loss: 0.187634, acc.: 73.44%] [G loss: 0.476892]\n",
      "epoch:17 step:16276 [D loss: 0.179708, acc.: 72.66%] [G loss: 0.550321]\n",
      "epoch:17 step:16277 [D loss: 0.263466, acc.: 59.38%] [G loss: 0.436409]\n",
      "epoch:17 step:16278 [D loss: 0.276433, acc.: 43.75%] [G loss: 0.382351]\n",
      "epoch:17 step:16279 [D loss: 0.205609, acc.: 71.09%] [G loss: 0.404858]\n",
      "epoch:17 step:16280 [D loss: 0.229880, acc.: 65.62%] [G loss: 0.447894]\n",
      "epoch:17 step:16281 [D loss: 0.230623, acc.: 59.38%] [G loss: 0.466977]\n",
      "epoch:17 step:16282 [D loss: 0.193748, acc.: 65.62%] [G loss: 0.478133]\n",
      "epoch:17 step:16283 [D loss: 0.173836, acc.: 78.12%] [G loss: 0.496667]\n",
      "epoch:17 step:16284 [D loss: 0.243007, acc.: 59.38%] [G loss: 0.447988]\n",
      "epoch:17 step:16285 [D loss: 0.225631, acc.: 64.84%] [G loss: 0.412317]\n",
      "epoch:17 step:16286 [D loss: 0.198766, acc.: 65.62%] [G loss: 0.461812]\n",
      "epoch:17 step:16287 [D loss: 0.194281, acc.: 71.09%] [G loss: 0.496539]\n",
      "epoch:17 step:16288 [D loss: 0.225452, acc.: 67.97%] [G loss: 0.461625]\n",
      "epoch:17 step:16289 [D loss: 0.210613, acc.: 64.84%] [G loss: 0.456394]\n",
      "epoch:17 step:16290 [D loss: 0.219780, acc.: 67.19%] [G loss: 0.409608]\n",
      "epoch:17 step:16291 [D loss: 0.230996, acc.: 60.94%] [G loss: 0.423771]\n",
      "epoch:17 step:16292 [D loss: 0.235990, acc.: 54.69%] [G loss: 0.426133]\n",
      "epoch:17 step:16293 [D loss: 0.211172, acc.: 62.50%] [G loss: 0.428969]\n",
      "epoch:17 step:16294 [D loss: 0.231030, acc.: 63.28%] [G loss: 0.443852]\n",
      "epoch:17 step:16295 [D loss: 0.236642, acc.: 57.03%] [G loss: 0.431949]\n",
      "epoch:17 step:16296 [D loss: 0.210801, acc.: 67.97%] [G loss: 0.435084]\n",
      "epoch:17 step:16297 [D loss: 0.221230, acc.: 63.28%] [G loss: 0.425831]\n",
      "epoch:17 step:16298 [D loss: 0.249961, acc.: 57.03%] [G loss: 0.406575]\n",
      "epoch:17 step:16299 [D loss: 0.181598, acc.: 75.78%] [G loss: 0.456060]\n",
      "epoch:17 step:16300 [D loss: 0.202491, acc.: 69.53%] [G loss: 0.465901]\n",
      "epoch:17 step:16301 [D loss: 0.209861, acc.: 68.75%] [G loss: 0.460258]\n",
      "epoch:17 step:16302 [D loss: 0.249577, acc.: 53.91%] [G loss: 0.436485]\n",
      "epoch:17 step:16303 [D loss: 0.206229, acc.: 67.19%] [G loss: 0.432342]\n",
      "epoch:17 step:16304 [D loss: 0.232295, acc.: 60.94%] [G loss: 0.450044]\n",
      "epoch:17 step:16305 [D loss: 0.260663, acc.: 53.12%] [G loss: 0.432146]\n",
      "epoch:17 step:16306 [D loss: 0.247310, acc.: 57.03%] [G loss: 0.414183]\n",
      "epoch:17 step:16307 [D loss: 0.231292, acc.: 64.84%] [G loss: 0.436712]\n",
      "epoch:17 step:16308 [D loss: 0.218166, acc.: 60.94%] [G loss: 0.436030]\n",
      "epoch:17 step:16309 [D loss: 0.235994, acc.: 57.03%] [G loss: 0.390190]\n",
      "epoch:17 step:16310 [D loss: 0.198805, acc.: 73.44%] [G loss: 0.442687]\n",
      "epoch:17 step:16311 [D loss: 0.240127, acc.: 60.94%] [G loss: 0.427979]\n",
      "epoch:17 step:16312 [D loss: 0.217138, acc.: 64.84%] [G loss: 0.416418]\n",
      "epoch:17 step:16313 [D loss: 0.223446, acc.: 62.50%] [G loss: 0.393690]\n",
      "epoch:17 step:16314 [D loss: 0.196186, acc.: 72.66%] [G loss: 0.409625]\n",
      "epoch:17 step:16315 [D loss: 0.228293, acc.: 61.72%] [G loss: 0.448231]\n",
      "epoch:17 step:16316 [D loss: 0.225623, acc.: 63.28%] [G loss: 0.429741]\n",
      "epoch:17 step:16317 [D loss: 0.235744, acc.: 59.38%] [G loss: 0.411473]\n",
      "epoch:17 step:16318 [D loss: 0.232632, acc.: 59.38%] [G loss: 0.409450]\n",
      "epoch:17 step:16319 [D loss: 0.238441, acc.: 60.16%] [G loss: 0.454840]\n",
      "epoch:17 step:16320 [D loss: 0.227586, acc.: 64.06%] [G loss: 0.408518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16321 [D loss: 0.234326, acc.: 62.50%] [G loss: 0.439788]\n",
      "epoch:17 step:16322 [D loss: 0.205537, acc.: 68.75%] [G loss: 0.450237]\n",
      "epoch:17 step:16323 [D loss: 0.216562, acc.: 69.53%] [G loss: 0.461870]\n",
      "epoch:17 step:16324 [D loss: 0.205972, acc.: 69.53%] [G loss: 0.447195]\n",
      "epoch:17 step:16325 [D loss: 0.237511, acc.: 59.38%] [G loss: 0.462325]\n",
      "epoch:17 step:16326 [D loss: 0.220730, acc.: 65.62%] [G loss: 0.484642]\n",
      "epoch:17 step:16327 [D loss: 0.215666, acc.: 70.31%] [G loss: 0.461112]\n",
      "epoch:17 step:16328 [D loss: 0.177269, acc.: 78.91%] [G loss: 0.472009]\n",
      "epoch:17 step:16329 [D loss: 0.292043, acc.: 49.22%] [G loss: 0.391531]\n",
      "epoch:17 step:16330 [D loss: 0.226002, acc.: 60.16%] [G loss: 0.435625]\n",
      "epoch:17 step:16331 [D loss: 0.195320, acc.: 71.88%] [G loss: 0.435887]\n",
      "epoch:17 step:16332 [D loss: 0.215001, acc.: 63.28%] [G loss: 0.476811]\n",
      "epoch:17 step:16333 [D loss: 0.240630, acc.: 59.38%] [G loss: 0.419927]\n",
      "epoch:17 step:16334 [D loss: 0.204414, acc.: 67.97%] [G loss: 0.430261]\n",
      "epoch:17 step:16335 [D loss: 0.212668, acc.: 64.84%] [G loss: 0.500377]\n",
      "epoch:17 step:16336 [D loss: 0.222541, acc.: 57.81%] [G loss: 0.458763]\n",
      "epoch:17 step:16337 [D loss: 0.240738, acc.: 58.59%] [G loss: 0.456508]\n",
      "epoch:17 step:16338 [D loss: 0.215373, acc.: 67.19%] [G loss: 0.431623]\n",
      "epoch:17 step:16339 [D loss: 0.269802, acc.: 52.34%] [G loss: 0.392439]\n",
      "epoch:17 step:16340 [D loss: 0.233269, acc.: 64.06%] [G loss: 0.415671]\n",
      "epoch:17 step:16341 [D loss: 0.226202, acc.: 58.59%] [G loss: 0.426388]\n",
      "epoch:17 step:16342 [D loss: 0.230773, acc.: 64.84%] [G loss: 0.414981]\n",
      "epoch:17 step:16343 [D loss: 0.215233, acc.: 65.62%] [G loss: 0.405040]\n",
      "epoch:17 step:16344 [D loss: 0.210838, acc.: 64.06%] [G loss: 0.447606]\n",
      "epoch:17 step:16345 [D loss: 0.193974, acc.: 75.00%] [G loss: 0.500055]\n",
      "epoch:17 step:16346 [D loss: 0.222162, acc.: 67.19%] [G loss: 0.488498]\n",
      "epoch:17 step:16347 [D loss: 0.261846, acc.: 53.12%] [G loss: 0.413222]\n",
      "epoch:17 step:16348 [D loss: 0.235639, acc.: 65.62%] [G loss: 0.449230]\n",
      "epoch:17 step:16349 [D loss: 0.245783, acc.: 60.94%] [G loss: 0.445149]\n",
      "epoch:17 step:16350 [D loss: 0.281575, acc.: 52.34%] [G loss: 0.427059]\n",
      "epoch:17 step:16351 [D loss: 0.214920, acc.: 63.28%] [G loss: 0.437602]\n",
      "epoch:17 step:16352 [D loss: 0.217170, acc.: 69.53%] [G loss: 0.406681]\n",
      "epoch:17 step:16353 [D loss: 0.252739, acc.: 59.38%] [G loss: 0.382209]\n",
      "epoch:17 step:16354 [D loss: 0.202946, acc.: 67.19%] [G loss: 0.478805]\n",
      "epoch:17 step:16355 [D loss: 0.228574, acc.: 59.38%] [G loss: 0.446347]\n",
      "epoch:17 step:16356 [D loss: 0.209403, acc.: 70.31%] [G loss: 0.418788]\n",
      "epoch:17 step:16357 [D loss: 0.201502, acc.: 69.53%] [G loss: 0.457129]\n",
      "epoch:17 step:16358 [D loss: 0.206994, acc.: 71.09%] [G loss: 0.496885]\n",
      "epoch:17 step:16359 [D loss: 0.203268, acc.: 69.53%] [G loss: 0.512065]\n",
      "epoch:17 step:16360 [D loss: 0.235303, acc.: 58.59%] [G loss: 0.460370]\n",
      "epoch:17 step:16361 [D loss: 0.234602, acc.: 56.25%] [G loss: 0.452423]\n",
      "epoch:17 step:16362 [D loss: 0.225744, acc.: 63.28%] [G loss: 0.431205]\n",
      "epoch:17 step:16363 [D loss: 0.223705, acc.: 61.72%] [G loss: 0.419459]\n",
      "epoch:17 step:16364 [D loss: 0.239007, acc.: 63.28%] [G loss: 0.460647]\n",
      "epoch:17 step:16365 [D loss: 0.179492, acc.: 75.00%] [G loss: 0.480293]\n",
      "epoch:17 step:16366 [D loss: 0.251671, acc.: 56.25%] [G loss: 0.435816]\n",
      "epoch:17 step:16367 [D loss: 0.233951, acc.: 58.59%] [G loss: 0.453933]\n",
      "epoch:17 step:16368 [D loss: 0.209354, acc.: 69.53%] [G loss: 0.431922]\n",
      "epoch:17 step:16369 [D loss: 0.212327, acc.: 67.97%] [G loss: 0.418988]\n",
      "epoch:17 step:16370 [D loss: 0.236507, acc.: 62.50%] [G loss: 0.472967]\n",
      "epoch:17 step:16371 [D loss: 0.221691, acc.: 61.72%] [G loss: 0.457141]\n",
      "epoch:17 step:16372 [D loss: 0.239079, acc.: 56.25%] [G loss: 0.408114]\n",
      "epoch:17 step:16373 [D loss: 0.226829, acc.: 64.06%] [G loss: 0.461454]\n",
      "epoch:17 step:16374 [D loss: 0.206705, acc.: 65.62%] [G loss: 0.451013]\n",
      "epoch:17 step:16375 [D loss: 0.217459, acc.: 64.06%] [G loss: 0.456223]\n",
      "epoch:17 step:16376 [D loss: 0.219885, acc.: 64.84%] [G loss: 0.461883]\n",
      "epoch:17 step:16377 [D loss: 0.237272, acc.: 60.94%] [G loss: 0.437085]\n",
      "epoch:17 step:16378 [D loss: 0.218246, acc.: 66.41%] [G loss: 0.421852]\n",
      "epoch:17 step:16379 [D loss: 0.208828, acc.: 67.19%] [G loss: 0.431558]\n",
      "epoch:17 step:16380 [D loss: 0.237669, acc.: 62.50%] [G loss: 0.433565]\n",
      "epoch:17 step:16381 [D loss: 0.207325, acc.: 71.88%] [G loss: 0.460424]\n",
      "epoch:17 step:16382 [D loss: 0.225392, acc.: 60.94%] [G loss: 0.468213]\n",
      "epoch:17 step:16383 [D loss: 0.203225, acc.: 74.22%] [G loss: 0.460710]\n",
      "epoch:17 step:16384 [D loss: 0.244816, acc.: 60.16%] [G loss: 0.458420]\n",
      "epoch:17 step:16385 [D loss: 0.231796, acc.: 60.94%] [G loss: 0.439877]\n",
      "epoch:17 step:16386 [D loss: 0.228789, acc.: 63.28%] [G loss: 0.455103]\n",
      "epoch:17 step:16387 [D loss: 0.270663, acc.: 49.22%] [G loss: 0.418457]\n",
      "epoch:17 step:16388 [D loss: 0.217169, acc.: 66.41%] [G loss: 0.439729]\n",
      "epoch:17 step:16389 [D loss: 0.232553, acc.: 58.59%] [G loss: 0.414821]\n",
      "epoch:17 step:16390 [D loss: 0.238037, acc.: 57.03%] [G loss: 0.394115]\n",
      "epoch:17 step:16391 [D loss: 0.254113, acc.: 53.12%] [G loss: 0.416591]\n",
      "epoch:17 step:16392 [D loss: 0.248123, acc.: 56.25%] [G loss: 0.409060]\n",
      "epoch:17 step:16393 [D loss: 0.202459, acc.: 67.97%] [G loss: 0.425597]\n",
      "epoch:17 step:16394 [D loss: 0.231446, acc.: 64.06%] [G loss: 0.398401]\n",
      "epoch:17 step:16395 [D loss: 0.208844, acc.: 66.41%] [G loss: 0.440500]\n",
      "epoch:17 step:16396 [D loss: 0.226116, acc.: 67.19%] [G loss: 0.418041]\n",
      "epoch:17 step:16397 [D loss: 0.209821, acc.: 69.53%] [G loss: 0.477936]\n",
      "epoch:17 step:16398 [D loss: 0.212778, acc.: 64.06%] [G loss: 0.463993]\n",
      "epoch:17 step:16399 [D loss: 0.223280, acc.: 69.53%] [G loss: 0.488302]\n",
      "epoch:17 step:16400 [D loss: 0.184103, acc.: 75.00%] [G loss: 0.514711]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.996509\n",
      "FID: 6.328317\n",
      "0 = 11.711193908476835\n",
      "1 = 0.046202605424216496\n",
      "2 = 0.8672999739646912\n",
      "3 = 0.859000027179718\n",
      "4 = 0.8755999803543091\n",
      "5 = 0.8735001087188721\n",
      "6 = 0.859000027179718\n",
      "7 = 5.777520985686764\n",
      "8 = 0.05362430869059242\n",
      "9 = 0.6747999787330627\n",
      "10 = 0.6747000217437744\n",
      "11 = 0.6748999953269958\n",
      "12 = 0.6748349666595459\n",
      "13 = 0.6747000217437744\n",
      "14 = 7.996577262878418\n",
      "15 = 9.604528427124023\n",
      "16 = 0.08985215425491333\n",
      "17 = 7.996509075164795\n",
      "18 = 6.328316688537598\n",
      "epoch:17 step:16401 [D loss: 0.227131, acc.: 64.06%] [G loss: 0.486670]\n",
      "epoch:17 step:16402 [D loss: 0.253647, acc.: 57.81%] [G loss: 0.422850]\n",
      "epoch:17 step:16403 [D loss: 0.205988, acc.: 64.06%] [G loss: 0.475790]\n",
      "epoch:17 step:16404 [D loss: 0.185155, acc.: 78.12%] [G loss: 0.471381]\n",
      "epoch:17 step:16405 [D loss: 0.225029, acc.: 63.28%] [G loss: 0.437440]\n",
      "epoch:17 step:16406 [D loss: 0.261941, acc.: 50.00%] [G loss: 0.395721]\n",
      "epoch:17 step:16407 [D loss: 0.253783, acc.: 53.91%] [G loss: 0.391392]\n",
      "epoch:17 step:16408 [D loss: 0.234993, acc.: 62.50%] [G loss: 0.422453]\n",
      "epoch:17 step:16409 [D loss: 0.210672, acc.: 66.41%] [G loss: 0.423134]\n",
      "epoch:17 step:16410 [D loss: 0.182584, acc.: 75.00%] [G loss: 0.440162]\n",
      "epoch:17 step:16411 [D loss: 0.280891, acc.: 46.88%] [G loss: 0.450263]\n",
      "epoch:17 step:16412 [D loss: 0.224610, acc.: 59.38%] [G loss: 0.396162]\n",
      "epoch:17 step:16413 [D loss: 0.241498, acc.: 60.94%] [G loss: 0.420882]\n",
      "epoch:17 step:16414 [D loss: 0.228705, acc.: 58.59%] [G loss: 0.411664]\n",
      "epoch:17 step:16415 [D loss: 0.244556, acc.: 58.59%] [G loss: 0.424190]\n",
      "epoch:17 step:16416 [D loss: 0.223619, acc.: 61.72%] [G loss: 0.429113]\n",
      "epoch:17 step:16417 [D loss: 0.212941, acc.: 64.06%] [G loss: 0.406339]\n",
      "epoch:17 step:16418 [D loss: 0.240385, acc.: 60.16%] [G loss: 0.436032]\n",
      "epoch:17 step:16419 [D loss: 0.221058, acc.: 65.62%] [G loss: 0.443808]\n",
      "epoch:17 step:16420 [D loss: 0.213068, acc.: 66.41%] [G loss: 0.428773]\n",
      "epoch:17 step:16421 [D loss: 0.219222, acc.: 62.50%] [G loss: 0.442924]\n",
      "epoch:17 step:16422 [D loss: 0.213657, acc.: 64.84%] [G loss: 0.437224]\n",
      "epoch:17 step:16423 [D loss: 0.201940, acc.: 73.44%] [G loss: 0.446871]\n",
      "epoch:17 step:16424 [D loss: 0.179444, acc.: 74.22%] [G loss: 0.426360]\n",
      "epoch:17 step:16425 [D loss: 0.217902, acc.: 64.84%] [G loss: 0.475150]\n",
      "epoch:17 step:16426 [D loss: 0.217906, acc.: 60.16%] [G loss: 0.475448]\n",
      "epoch:17 step:16427 [D loss: 0.206507, acc.: 69.53%] [G loss: 0.452174]\n",
      "epoch:17 step:16428 [D loss: 0.216103, acc.: 62.50%] [G loss: 0.471557]\n",
      "epoch:17 step:16429 [D loss: 0.241140, acc.: 64.06%] [G loss: 0.456270]\n",
      "epoch:17 step:16430 [D loss: 0.278197, acc.: 53.91%] [G loss: 0.395153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16431 [D loss: 0.240926, acc.: 60.16%] [G loss: 0.370600]\n",
      "epoch:17 step:16432 [D loss: 0.211839, acc.: 69.53%] [G loss: 0.421791]\n",
      "epoch:17 step:16433 [D loss: 0.226864, acc.: 63.28%] [G loss: 0.405528]\n",
      "epoch:17 step:16434 [D loss: 0.182436, acc.: 72.66%] [G loss: 0.496756]\n",
      "epoch:17 step:16435 [D loss: 0.233490, acc.: 57.81%] [G loss: 0.451461]\n",
      "epoch:17 step:16436 [D loss: 0.221944, acc.: 63.28%] [G loss: 0.438624]\n",
      "epoch:17 step:16437 [D loss: 0.173565, acc.: 78.12%] [G loss: 0.524474]\n",
      "epoch:17 step:16438 [D loss: 0.228056, acc.: 60.16%] [G loss: 0.483718]\n",
      "epoch:17 step:16439 [D loss: 0.234649, acc.: 60.94%] [G loss: 0.442596]\n",
      "epoch:17 step:16440 [D loss: 0.267292, acc.: 46.09%] [G loss: 0.401680]\n",
      "epoch:17 step:16441 [D loss: 0.230407, acc.: 61.72%] [G loss: 0.413474]\n",
      "epoch:17 step:16442 [D loss: 0.203111, acc.: 69.53%] [G loss: 0.467859]\n",
      "epoch:17 step:16443 [D loss: 0.207603, acc.: 64.84%] [G loss: 0.484388]\n",
      "epoch:17 step:16444 [D loss: 0.210262, acc.: 67.97%] [G loss: 0.475883]\n",
      "epoch:17 step:16445 [D loss: 0.218324, acc.: 64.06%] [G loss: 0.480647]\n",
      "epoch:17 step:16446 [D loss: 0.257762, acc.: 53.91%] [G loss: 0.449014]\n",
      "epoch:17 step:16447 [D loss: 0.225404, acc.: 61.72%] [G loss: 0.448070]\n",
      "epoch:17 step:16448 [D loss: 0.215241, acc.: 63.28%] [G loss: 0.423003]\n",
      "epoch:17 step:16449 [D loss: 0.214923, acc.: 64.84%] [G loss: 0.425649]\n",
      "epoch:17 step:16450 [D loss: 0.220303, acc.: 60.94%] [G loss: 0.451399]\n",
      "epoch:17 step:16451 [D loss: 0.195828, acc.: 71.09%] [G loss: 0.441031]\n",
      "epoch:17 step:16452 [D loss: 0.180716, acc.: 74.22%] [G loss: 0.436826]\n",
      "epoch:17 step:16453 [D loss: 0.236420, acc.: 60.16%] [G loss: 0.420891]\n",
      "epoch:17 step:16454 [D loss: 0.222509, acc.: 64.06%] [G loss: 0.405614]\n",
      "epoch:17 step:16455 [D loss: 0.218654, acc.: 61.72%] [G loss: 0.419779]\n",
      "epoch:17 step:16456 [D loss: 0.252731, acc.: 56.25%] [G loss: 0.429056]\n",
      "epoch:17 step:16457 [D loss: 0.267075, acc.: 54.69%] [G loss: 0.444595]\n",
      "epoch:17 step:16458 [D loss: 0.246661, acc.: 54.69%] [G loss: 0.435999]\n",
      "epoch:17 step:16459 [D loss: 0.227029, acc.: 58.59%] [G loss: 0.456703]\n",
      "epoch:17 step:16460 [D loss: 0.253045, acc.: 53.91%] [G loss: 0.425292]\n",
      "epoch:17 step:16461 [D loss: 0.236885, acc.: 57.03%] [G loss: 0.376501]\n",
      "epoch:17 step:16462 [D loss: 0.223328, acc.: 61.72%] [G loss: 0.421276]\n",
      "epoch:17 step:16463 [D loss: 0.188583, acc.: 74.22%] [G loss: 0.479038]\n",
      "epoch:17 step:16464 [D loss: 0.252417, acc.: 51.56%] [G loss: 0.422217]\n",
      "epoch:17 step:16465 [D loss: 0.224330, acc.: 66.41%] [G loss: 0.404923]\n",
      "epoch:17 step:16466 [D loss: 0.248511, acc.: 58.59%] [G loss: 0.435380]\n",
      "epoch:17 step:16467 [D loss: 0.230846, acc.: 62.50%] [G loss: 0.454197]\n",
      "epoch:17 step:16468 [D loss: 0.205993, acc.: 65.62%] [G loss: 0.445089]\n",
      "epoch:17 step:16469 [D loss: 0.227192, acc.: 60.94%] [G loss: 0.413126]\n",
      "epoch:17 step:16470 [D loss: 0.235474, acc.: 59.38%] [G loss: 0.379564]\n",
      "epoch:17 step:16471 [D loss: 0.271055, acc.: 48.44%] [G loss: 0.403719]\n",
      "epoch:17 step:16472 [D loss: 0.239324, acc.: 62.50%] [G loss: 0.377257]\n",
      "epoch:17 step:16473 [D loss: 0.202477, acc.: 68.75%] [G loss: 0.422795]\n",
      "epoch:17 step:16474 [D loss: 0.224270, acc.: 64.84%] [G loss: 0.420038]\n",
      "epoch:17 step:16475 [D loss: 0.228118, acc.: 60.16%] [G loss: 0.427293]\n",
      "epoch:17 step:16476 [D loss: 0.213814, acc.: 64.06%] [G loss: 0.476343]\n",
      "epoch:17 step:16477 [D loss: 0.201593, acc.: 67.97%] [G loss: 0.490568]\n",
      "epoch:17 step:16478 [D loss: 0.217334, acc.: 64.84%] [G loss: 0.448356]\n",
      "epoch:17 step:16479 [D loss: 0.217194, acc.: 67.97%] [G loss: 0.450002]\n",
      "epoch:17 step:16480 [D loss: 0.188210, acc.: 71.09%] [G loss: 0.481101]\n",
      "epoch:17 step:16481 [D loss: 0.214888, acc.: 64.06%] [G loss: 0.498280]\n",
      "epoch:17 step:16482 [D loss: 0.246544, acc.: 59.38%] [G loss: 0.445467]\n",
      "epoch:17 step:16483 [D loss: 0.192539, acc.: 72.66%] [G loss: 0.451817]\n",
      "epoch:17 step:16484 [D loss: 0.180306, acc.: 75.00%] [G loss: 0.440936]\n",
      "epoch:17 step:16485 [D loss: 0.215003, acc.: 71.09%] [G loss: 0.437238]\n",
      "epoch:17 step:16486 [D loss: 0.238450, acc.: 59.38%] [G loss: 0.439877]\n",
      "epoch:17 step:16487 [D loss: 0.206239, acc.: 64.84%] [G loss: 0.448898]\n",
      "epoch:17 step:16488 [D loss: 0.270067, acc.: 56.25%] [G loss: 0.418500]\n",
      "epoch:17 step:16489 [D loss: 0.238721, acc.: 53.12%] [G loss: 0.426136]\n",
      "epoch:17 step:16490 [D loss: 0.228161, acc.: 63.28%] [G loss: 0.440487]\n",
      "epoch:17 step:16491 [D loss: 0.217998, acc.: 67.19%] [G loss: 0.433200]\n",
      "epoch:17 step:16492 [D loss: 0.195500, acc.: 71.09%] [G loss: 0.489638]\n",
      "epoch:17 step:16493 [D loss: 0.200962, acc.: 71.88%] [G loss: 0.465062]\n",
      "epoch:17 step:16494 [D loss: 0.266036, acc.: 56.25%] [G loss: 0.431003]\n",
      "epoch:17 step:16495 [D loss: 0.297600, acc.: 48.44%] [G loss: 0.472092]\n",
      "epoch:17 step:16496 [D loss: 0.207348, acc.: 72.66%] [G loss: 0.434970]\n",
      "epoch:17 step:16497 [D loss: 0.199282, acc.: 71.88%] [G loss: 0.466532]\n",
      "epoch:17 step:16498 [D loss: 0.254643, acc.: 55.47%] [G loss: 0.410358]\n",
      "epoch:17 step:16499 [D loss: 0.206130, acc.: 61.72%] [G loss: 0.428199]\n",
      "epoch:17 step:16500 [D loss: 0.220882, acc.: 65.62%] [G loss: 0.416555]\n",
      "epoch:17 step:16501 [D loss: 0.222505, acc.: 66.41%] [G loss: 0.420868]\n",
      "epoch:17 step:16502 [D loss: 0.235788, acc.: 56.25%] [G loss: 0.445037]\n",
      "epoch:17 step:16503 [D loss: 0.169389, acc.: 80.47%] [G loss: 0.457581]\n",
      "epoch:17 step:16504 [D loss: 0.210981, acc.: 64.84%] [G loss: 0.480022]\n",
      "epoch:17 step:16505 [D loss: 0.229851, acc.: 62.50%] [G loss: 0.477554]\n",
      "epoch:17 step:16506 [D loss: 0.247340, acc.: 55.47%] [G loss: 0.427991]\n",
      "epoch:17 step:16507 [D loss: 0.224364, acc.: 58.59%] [G loss: 0.420989]\n",
      "epoch:17 step:16508 [D loss: 0.229933, acc.: 60.16%] [G loss: 0.419850]\n",
      "epoch:17 step:16509 [D loss: 0.232480, acc.: 57.03%] [G loss: 0.419527]\n",
      "epoch:17 step:16510 [D loss: 0.232296, acc.: 60.94%] [G loss: 0.416133]\n",
      "epoch:17 step:16511 [D loss: 0.218645, acc.: 67.19%] [G loss: 0.429157]\n",
      "epoch:17 step:16512 [D loss: 0.234762, acc.: 65.62%] [G loss: 0.458704]\n",
      "epoch:17 step:16513 [D loss: 0.237186, acc.: 62.50%] [G loss: 0.458693]\n",
      "epoch:17 step:16514 [D loss: 0.230708, acc.: 61.72%] [G loss: 0.448135]\n",
      "epoch:17 step:16515 [D loss: 0.228168, acc.: 60.16%] [G loss: 0.459627]\n",
      "epoch:17 step:16516 [D loss: 0.245985, acc.: 56.25%] [G loss: 0.408480]\n",
      "epoch:17 step:16517 [D loss: 0.229592, acc.: 60.16%] [G loss: 0.422713]\n",
      "epoch:17 step:16518 [D loss: 0.216374, acc.: 65.62%] [G loss: 0.420042]\n",
      "epoch:17 step:16519 [D loss: 0.234817, acc.: 60.16%] [G loss: 0.479047]\n",
      "epoch:17 step:16520 [D loss: 0.252862, acc.: 55.47%] [G loss: 0.447803]\n",
      "epoch:17 step:16521 [D loss: 0.189256, acc.: 75.00%] [G loss: 0.468328]\n",
      "epoch:17 step:16522 [D loss: 0.222570, acc.: 64.84%] [G loss: 0.492352]\n",
      "epoch:17 step:16523 [D loss: 0.231294, acc.: 61.72%] [G loss: 0.437087]\n",
      "epoch:17 step:16524 [D loss: 0.205276, acc.: 67.19%] [G loss: 0.473722]\n",
      "epoch:17 step:16525 [D loss: 0.237256, acc.: 58.59%] [G loss: 0.454392]\n",
      "epoch:17 step:16526 [D loss: 0.239401, acc.: 57.03%] [G loss: 0.409799]\n",
      "epoch:17 step:16527 [D loss: 0.208745, acc.: 71.88%] [G loss: 0.415066]\n",
      "epoch:17 step:16528 [D loss: 0.222719, acc.: 63.28%] [G loss: 0.445004]\n",
      "epoch:17 step:16529 [D loss: 0.230924, acc.: 57.81%] [G loss: 0.428332]\n",
      "epoch:17 step:16530 [D loss: 0.255696, acc.: 52.34%] [G loss: 0.416865]\n",
      "epoch:17 step:16531 [D loss: 0.248976, acc.: 60.94%] [G loss: 0.437069]\n",
      "epoch:17 step:16532 [D loss: 0.204162, acc.: 71.88%] [G loss: 0.455824]\n",
      "epoch:17 step:16533 [D loss: 0.225367, acc.: 64.84%] [G loss: 0.415277]\n",
      "epoch:17 step:16534 [D loss: 0.219295, acc.: 65.62%] [G loss: 0.424746]\n",
      "epoch:17 step:16535 [D loss: 0.232843, acc.: 58.59%] [G loss: 0.463153]\n",
      "epoch:17 step:16536 [D loss: 0.207067, acc.: 64.06%] [G loss: 0.403362]\n",
      "epoch:17 step:16537 [D loss: 0.222941, acc.: 61.72%] [G loss: 0.428699]\n",
      "epoch:17 step:16538 [D loss: 0.205845, acc.: 69.53%] [G loss: 0.472286]\n",
      "epoch:17 step:16539 [D loss: 0.240283, acc.: 56.25%] [G loss: 0.439689]\n",
      "epoch:17 step:16540 [D loss: 0.219848, acc.: 64.06%] [G loss: 0.456095]\n",
      "epoch:17 step:16541 [D loss: 0.236151, acc.: 61.72%] [G loss: 0.390943]\n",
      "epoch:17 step:16542 [D loss: 0.202505, acc.: 69.53%] [G loss: 0.425364]\n",
      "epoch:17 step:16543 [D loss: 0.262776, acc.: 53.12%] [G loss: 0.415016]\n",
      "epoch:17 step:16544 [D loss: 0.263102, acc.: 53.91%] [G loss: 0.390220]\n",
      "epoch:17 step:16545 [D loss: 0.228463, acc.: 62.50%] [G loss: 0.395994]\n",
      "epoch:17 step:16546 [D loss: 0.202391, acc.: 71.09%] [G loss: 0.405103]\n",
      "epoch:17 step:16547 [D loss: 0.230894, acc.: 57.03%] [G loss: 0.397582]\n",
      "epoch:17 step:16548 [D loss: 0.253270, acc.: 56.25%] [G loss: 0.433008]\n",
      "epoch:17 step:16549 [D loss: 0.201401, acc.: 71.09%] [G loss: 0.484715]\n",
      "epoch:17 step:16550 [D loss: 0.241353, acc.: 55.47%] [G loss: 0.437050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16551 [D loss: 0.237305, acc.: 55.47%] [G loss: 0.464327]\n",
      "epoch:17 step:16552 [D loss: 0.245628, acc.: 62.50%] [G loss: 0.431511]\n",
      "epoch:17 step:16553 [D loss: 0.201875, acc.: 70.31%] [G loss: 0.478085]\n",
      "epoch:17 step:16554 [D loss: 0.246504, acc.: 59.38%] [G loss: 0.428581]\n",
      "epoch:17 step:16555 [D loss: 0.235669, acc.: 56.25%] [G loss: 0.430985]\n",
      "epoch:17 step:16556 [D loss: 0.212674, acc.: 71.88%] [G loss: 0.461875]\n",
      "epoch:17 step:16557 [D loss: 0.254376, acc.: 58.59%] [G loss: 0.429546]\n",
      "epoch:17 step:16558 [D loss: 0.219330, acc.: 64.06%] [G loss: 0.438988]\n",
      "epoch:17 step:16559 [D loss: 0.220038, acc.: 63.28%] [G loss: 0.423143]\n",
      "epoch:17 step:16560 [D loss: 0.218813, acc.: 63.28%] [G loss: 0.408366]\n",
      "epoch:17 step:16561 [D loss: 0.204288, acc.: 67.97%] [G loss: 0.427824]\n",
      "epoch:17 step:16562 [D loss: 0.195985, acc.: 72.66%] [G loss: 0.463128]\n",
      "epoch:17 step:16563 [D loss: 0.212338, acc.: 66.41%] [G loss: 0.451431]\n",
      "epoch:17 step:16564 [D loss: 0.219929, acc.: 64.06%] [G loss: 0.486974]\n",
      "epoch:17 step:16565 [D loss: 0.241746, acc.: 60.94%] [G loss: 0.427704]\n",
      "epoch:17 step:16566 [D loss: 0.209081, acc.: 71.88%] [G loss: 0.427458]\n",
      "epoch:17 step:16567 [D loss: 0.211439, acc.: 67.19%] [G loss: 0.429978]\n",
      "epoch:17 step:16568 [D loss: 0.251287, acc.: 55.47%] [G loss: 0.423836]\n",
      "epoch:17 step:16569 [D loss: 0.224996, acc.: 57.81%] [G loss: 0.439653]\n",
      "epoch:17 step:16570 [D loss: 0.203073, acc.: 67.19%] [G loss: 0.411285]\n",
      "epoch:17 step:16571 [D loss: 0.208988, acc.: 66.41%] [G loss: 0.451316]\n",
      "epoch:17 step:16572 [D loss: 0.232412, acc.: 60.94%] [G loss: 0.447380]\n",
      "epoch:17 step:16573 [D loss: 0.231211, acc.: 59.38%] [G loss: 0.415324]\n",
      "epoch:17 step:16574 [D loss: 0.209221, acc.: 72.66%] [G loss: 0.449216]\n",
      "epoch:17 step:16575 [D loss: 0.204941, acc.: 71.88%] [G loss: 0.424018]\n",
      "epoch:17 step:16576 [D loss: 0.206122, acc.: 67.19%] [G loss: 0.434667]\n",
      "epoch:17 step:16577 [D loss: 0.178558, acc.: 72.66%] [G loss: 0.515756]\n",
      "epoch:17 step:16578 [D loss: 0.206964, acc.: 68.75%] [G loss: 0.521836]\n",
      "epoch:17 step:16579 [D loss: 0.190338, acc.: 73.44%] [G loss: 0.471593]\n",
      "epoch:17 step:16580 [D loss: 0.210933, acc.: 66.41%] [G loss: 0.481399]\n",
      "epoch:17 step:16581 [D loss: 0.237133, acc.: 56.25%] [G loss: 0.446144]\n",
      "epoch:17 step:16582 [D loss: 0.218040, acc.: 63.28%] [G loss: 0.463148]\n",
      "epoch:17 step:16583 [D loss: 0.208647, acc.: 63.28%] [G loss: 0.470003]\n",
      "epoch:17 step:16584 [D loss: 0.244080, acc.: 59.38%] [G loss: 0.422375]\n",
      "epoch:17 step:16585 [D loss: 0.227065, acc.: 61.72%] [G loss: 0.445287]\n",
      "epoch:17 step:16586 [D loss: 0.234226, acc.: 56.25%] [G loss: 0.442508]\n",
      "epoch:17 step:16587 [D loss: 0.229739, acc.: 64.06%] [G loss: 0.436983]\n",
      "epoch:17 step:16588 [D loss: 0.199424, acc.: 65.62%] [G loss: 0.500089]\n",
      "epoch:17 step:16589 [D loss: 0.196654, acc.: 74.22%] [G loss: 0.478774]\n",
      "epoch:17 step:16590 [D loss: 0.203014, acc.: 67.97%] [G loss: 0.467558]\n",
      "epoch:17 step:16591 [D loss: 0.201850, acc.: 67.97%] [G loss: 0.481726]\n",
      "epoch:17 step:16592 [D loss: 0.203413, acc.: 71.09%] [G loss: 0.450501]\n",
      "epoch:17 step:16593 [D loss: 0.216775, acc.: 65.62%] [G loss: 0.445588]\n",
      "epoch:17 step:16594 [D loss: 0.234791, acc.: 55.47%] [G loss: 0.400442]\n",
      "epoch:17 step:16595 [D loss: 0.208120, acc.: 70.31%] [G loss: 0.463072]\n",
      "epoch:17 step:16596 [D loss: 0.223602, acc.: 64.84%] [G loss: 0.417554]\n",
      "epoch:17 step:16597 [D loss: 0.213474, acc.: 64.84%] [G loss: 0.470621]\n",
      "epoch:17 step:16598 [D loss: 0.221875, acc.: 60.94%] [G loss: 0.426156]\n",
      "epoch:17 step:16599 [D loss: 0.249557, acc.: 48.44%] [G loss: 0.411684]\n",
      "epoch:17 step:16600 [D loss: 0.223787, acc.: 64.06%] [G loss: 0.430897]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.887713\n",
      "FID: 10.390389\n",
      "0 = 11.884116549134212\n",
      "1 = 0.04934178067820015\n",
      "2 = 0.8810499906539917\n",
      "3 = 0.8754000067710876\n",
      "4 = 0.8866999745368958\n",
      "5 = 0.8854050636291504\n",
      "6 = 0.8754000067710876\n",
      "7 = 6.212188000738603\n",
      "8 = 0.07258576286211972\n",
      "9 = 0.6858000159263611\n",
      "10 = 0.6812000274658203\n",
      "11 = 0.6904000043869019\n",
      "12 = 0.68752521276474\n",
      "13 = 0.6812000274658203\n",
      "14 = 7.887785911560059\n",
      "15 = 9.475187301635742\n",
      "16 = 0.1275942325592041\n",
      "17 = 7.887713432312012\n",
      "18 = 10.390389442443848\n",
      "epoch:17 step:16601 [D loss: 0.244066, acc.: 56.25%] [G loss: 0.409709]\n",
      "epoch:17 step:16602 [D loss: 0.225742, acc.: 61.72%] [G loss: 0.425651]\n",
      "epoch:17 step:16603 [D loss: 0.194097, acc.: 69.53%] [G loss: 0.470786]\n",
      "epoch:17 step:16604 [D loss: 0.242504, acc.: 60.16%] [G loss: 0.426415]\n",
      "epoch:17 step:16605 [D loss: 0.219979, acc.: 63.28%] [G loss: 0.376058]\n",
      "epoch:17 step:16606 [D loss: 0.195875, acc.: 71.09%] [G loss: 0.432272]\n",
      "epoch:17 step:16607 [D loss: 0.203315, acc.: 68.75%] [G loss: 0.481192]\n",
      "epoch:17 step:16608 [D loss: 0.223655, acc.: 59.38%] [G loss: 0.421200]\n",
      "epoch:17 step:16609 [D loss: 0.221291, acc.: 64.06%] [G loss: 0.416529]\n",
      "epoch:17 step:16610 [D loss: 0.220281, acc.: 64.84%] [G loss: 0.451521]\n",
      "epoch:17 step:16611 [D loss: 0.241563, acc.: 59.38%] [G loss: 0.405520]\n",
      "epoch:17 step:16612 [D loss: 0.229619, acc.: 62.50%] [G loss: 0.433661]\n",
      "epoch:17 step:16613 [D loss: 0.227961, acc.: 63.28%] [G loss: 0.406236]\n",
      "epoch:17 step:16614 [D loss: 0.217581, acc.: 62.50%] [G loss: 0.395334]\n",
      "epoch:17 step:16615 [D loss: 0.236336, acc.: 53.12%] [G loss: 0.442840]\n",
      "epoch:17 step:16616 [D loss: 0.216226, acc.: 68.75%] [G loss: 0.418217]\n",
      "epoch:17 step:16617 [D loss: 0.220495, acc.: 67.19%] [G loss: 0.417034]\n",
      "epoch:17 step:16618 [D loss: 0.244693, acc.: 60.94%] [G loss: 0.457400]\n",
      "epoch:17 step:16619 [D loss: 0.202374, acc.: 71.09%] [G loss: 0.453351]\n",
      "epoch:17 step:16620 [D loss: 0.206036, acc.: 70.31%] [G loss: 0.472601]\n",
      "epoch:17 step:16621 [D loss: 0.203228, acc.: 64.84%] [G loss: 0.480079]\n",
      "epoch:17 step:16622 [D loss: 0.212917, acc.: 64.06%] [G loss: 0.491334]\n",
      "epoch:17 step:16623 [D loss: 0.192728, acc.: 71.09%] [G loss: 0.487602]\n",
      "epoch:17 step:16624 [D loss: 0.196080, acc.: 67.97%] [G loss: 0.497655]\n",
      "epoch:17 step:16625 [D loss: 0.249384, acc.: 56.25%] [G loss: 0.439148]\n",
      "epoch:17 step:16626 [D loss: 0.203166, acc.: 70.31%] [G loss: 0.448222]\n",
      "epoch:17 step:16627 [D loss: 0.201862, acc.: 71.88%] [G loss: 0.447028]\n",
      "epoch:17 step:16628 [D loss: 0.206879, acc.: 69.53%] [G loss: 0.420104]\n",
      "epoch:17 step:16629 [D loss: 0.188389, acc.: 69.53%] [G loss: 0.455445]\n",
      "epoch:17 step:16630 [D loss: 0.211641, acc.: 64.84%] [G loss: 0.460191]\n",
      "epoch:17 step:16631 [D loss: 0.232243, acc.: 59.38%] [G loss: 0.485250]\n",
      "epoch:17 step:16632 [D loss: 0.247080, acc.: 55.47%] [G loss: 0.448593]\n",
      "epoch:17 step:16633 [D loss: 0.219801, acc.: 63.28%] [G loss: 0.402904]\n",
      "epoch:17 step:16634 [D loss: 0.240548, acc.: 53.91%] [G loss: 0.422701]\n",
      "epoch:17 step:16635 [D loss: 0.207623, acc.: 63.28%] [G loss: 0.443786]\n",
      "epoch:17 step:16636 [D loss: 0.193805, acc.: 71.88%] [G loss: 0.441003]\n",
      "epoch:17 step:16637 [D loss: 0.204917, acc.: 67.97%] [G loss: 0.451571]\n",
      "epoch:17 step:16638 [D loss: 0.211721, acc.: 67.19%] [G loss: 0.436739]\n",
      "epoch:17 step:16639 [D loss: 0.253096, acc.: 57.81%] [G loss: 0.412258]\n",
      "epoch:17 step:16640 [D loss: 0.253660, acc.: 53.12%] [G loss: 0.409180]\n",
      "epoch:17 step:16641 [D loss: 0.206237, acc.: 64.06%] [G loss: 0.429359]\n",
      "epoch:17 step:16642 [D loss: 0.222944, acc.: 64.84%] [G loss: 0.440428]\n",
      "epoch:17 step:16643 [D loss: 0.233754, acc.: 64.06%] [G loss: 0.437601]\n",
      "epoch:17 step:16644 [D loss: 0.219609, acc.: 64.06%] [G loss: 0.449261]\n",
      "epoch:17 step:16645 [D loss: 0.271384, acc.: 46.88%] [G loss: 0.386029]\n",
      "epoch:17 step:16646 [D loss: 0.240413, acc.: 57.03%] [G loss: 0.426037]\n",
      "epoch:17 step:16647 [D loss: 0.210580, acc.: 66.41%] [G loss: 0.430782]\n",
      "epoch:17 step:16648 [D loss: 0.204105, acc.: 69.53%] [G loss: 0.479716]\n",
      "epoch:17 step:16649 [D loss: 0.234820, acc.: 62.50%] [G loss: 0.458366]\n",
      "epoch:17 step:16650 [D loss: 0.224353, acc.: 60.94%] [G loss: 0.414336]\n",
      "epoch:17 step:16651 [D loss: 0.248503, acc.: 53.91%] [G loss: 0.409085]\n",
      "epoch:17 step:16652 [D loss: 0.228462, acc.: 60.94%] [G loss: 0.425359]\n",
      "epoch:17 step:16653 [D loss: 0.218594, acc.: 61.72%] [G loss: 0.467119]\n",
      "epoch:17 step:16654 [D loss: 0.218075, acc.: 66.41%] [G loss: 0.439596]\n",
      "epoch:17 step:16655 [D loss: 0.238855, acc.: 62.50%] [G loss: 0.454663]\n",
      "epoch:17 step:16656 [D loss: 0.230370, acc.: 58.59%] [G loss: 0.437963]\n",
      "epoch:17 step:16657 [D loss: 0.216324, acc.: 66.41%] [G loss: 0.443284]\n",
      "epoch:17 step:16658 [D loss: 0.224469, acc.: 60.16%] [G loss: 0.419214]\n",
      "epoch:17 step:16659 [D loss: 0.202512, acc.: 67.19%] [G loss: 0.469006]\n",
      "epoch:17 step:16660 [D loss: 0.205471, acc.: 67.97%] [G loss: 0.495030]\n",
      "epoch:17 step:16661 [D loss: 0.214171, acc.: 65.62%] [G loss: 0.436670]\n",
      "epoch:17 step:16662 [D loss: 0.196852, acc.: 71.88%] [G loss: 0.469211]\n",
      "epoch:17 step:16663 [D loss: 0.234834, acc.: 60.16%] [G loss: 0.434186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16664 [D loss: 0.238325, acc.: 57.81%] [G loss: 0.444600]\n",
      "epoch:17 step:16665 [D loss: 0.223865, acc.: 64.84%] [G loss: 0.420386]\n",
      "epoch:17 step:16666 [D loss: 0.205229, acc.: 67.19%] [G loss: 0.484478]\n",
      "epoch:17 step:16667 [D loss: 0.232662, acc.: 62.50%] [G loss: 0.402403]\n",
      "epoch:17 step:16668 [D loss: 0.229745, acc.: 63.28%] [G loss: 0.436955]\n",
      "epoch:17 step:16669 [D loss: 0.245286, acc.: 52.34%] [G loss: 0.409048]\n",
      "epoch:17 step:16670 [D loss: 0.222134, acc.: 60.94%] [G loss: 0.414927]\n",
      "epoch:17 step:16671 [D loss: 0.229150, acc.: 60.16%] [G loss: 0.431046]\n",
      "epoch:17 step:16672 [D loss: 0.222334, acc.: 67.19%] [G loss: 0.466161]\n",
      "epoch:17 step:16673 [D loss: 0.258343, acc.: 54.69%] [G loss: 0.424392]\n",
      "epoch:17 step:16674 [D loss: 0.232192, acc.: 60.16%] [G loss: 0.433451]\n",
      "epoch:17 step:16675 [D loss: 0.252595, acc.: 57.03%] [G loss: 0.383389]\n",
      "epoch:17 step:16676 [D loss: 0.207802, acc.: 67.97%] [G loss: 0.445829]\n",
      "epoch:17 step:16677 [D loss: 0.233410, acc.: 62.50%] [G loss: 0.471582]\n",
      "epoch:17 step:16678 [D loss: 0.236097, acc.: 59.38%] [G loss: 0.410659]\n",
      "epoch:17 step:16679 [D loss: 0.196618, acc.: 73.44%] [G loss: 0.422519]\n",
      "epoch:17 step:16680 [D loss: 0.195669, acc.: 73.44%] [G loss: 0.448055]\n",
      "epoch:17 step:16681 [D loss: 0.248067, acc.: 56.25%] [G loss: 0.419092]\n",
      "epoch:17 step:16682 [D loss: 0.235395, acc.: 57.81%] [G loss: 0.395325]\n",
      "epoch:17 step:16683 [D loss: 0.218818, acc.: 60.16%] [G loss: 0.454771]\n",
      "epoch:17 step:16684 [D loss: 0.231404, acc.: 62.50%] [G loss: 0.478905]\n",
      "epoch:17 step:16685 [D loss: 0.231154, acc.: 64.06%] [G loss: 0.423361]\n",
      "epoch:17 step:16686 [D loss: 0.245652, acc.: 60.94%] [G loss: 0.431219]\n",
      "epoch:17 step:16687 [D loss: 0.234501, acc.: 63.28%] [G loss: 0.433990]\n",
      "epoch:17 step:16688 [D loss: 0.251224, acc.: 56.25%] [G loss: 0.450744]\n",
      "epoch:17 step:16689 [D loss: 0.254023, acc.: 52.34%] [G loss: 0.437669]\n",
      "epoch:17 step:16690 [D loss: 0.240141, acc.: 55.47%] [G loss: 0.426375]\n",
      "epoch:17 step:16691 [D loss: 0.254187, acc.: 58.59%] [G loss: 0.439209]\n",
      "epoch:17 step:16692 [D loss: 0.238140, acc.: 57.03%] [G loss: 0.436469]\n",
      "epoch:17 step:16693 [D loss: 0.242074, acc.: 52.34%] [G loss: 0.447052]\n",
      "epoch:17 step:16694 [D loss: 0.242247, acc.: 57.03%] [G loss: 0.434993]\n",
      "epoch:17 step:16695 [D loss: 0.226657, acc.: 64.06%] [G loss: 0.430550]\n",
      "epoch:17 step:16696 [D loss: 0.231948, acc.: 61.72%] [G loss: 0.432718]\n",
      "epoch:17 step:16697 [D loss: 0.224862, acc.: 64.84%] [G loss: 0.451060]\n",
      "epoch:17 step:16698 [D loss: 0.183756, acc.: 77.34%] [G loss: 0.504773]\n",
      "epoch:17 step:16699 [D loss: 0.243181, acc.: 64.84%] [G loss: 0.459347]\n",
      "epoch:17 step:16700 [D loss: 0.255786, acc.: 55.47%] [G loss: 0.432066]\n",
      "epoch:17 step:16701 [D loss: 0.249324, acc.: 56.25%] [G loss: 0.419213]\n",
      "epoch:17 step:16702 [D loss: 0.237610, acc.: 57.03%] [G loss: 0.401503]\n",
      "epoch:17 step:16703 [D loss: 0.242198, acc.: 57.03%] [G loss: 0.410129]\n",
      "epoch:17 step:16704 [D loss: 0.197634, acc.: 66.41%] [G loss: 0.462297]\n",
      "epoch:17 step:16705 [D loss: 0.223144, acc.: 61.72%] [G loss: 0.468756]\n",
      "epoch:17 step:16706 [D loss: 0.219500, acc.: 63.28%] [G loss: 0.415168]\n",
      "epoch:17 step:16707 [D loss: 0.240020, acc.: 57.81%] [G loss: 0.408763]\n",
      "epoch:17 step:16708 [D loss: 0.223186, acc.: 65.62%] [G loss: 0.432533]\n",
      "epoch:17 step:16709 [D loss: 0.216920, acc.: 64.06%] [G loss: 0.418183]\n",
      "epoch:17 step:16710 [D loss: 0.223895, acc.: 64.06%] [G loss: 0.476915]\n",
      "epoch:17 step:16711 [D loss: 0.205823, acc.: 69.53%] [G loss: 0.481965]\n",
      "epoch:17 step:16712 [D loss: 0.235986, acc.: 59.38%] [G loss: 0.471628]\n",
      "epoch:17 step:16713 [D loss: 0.289939, acc.: 40.62%] [G loss: 0.385560]\n",
      "epoch:17 step:16714 [D loss: 0.214230, acc.: 69.53%] [G loss: 0.431960]\n",
      "epoch:17 step:16715 [D loss: 0.199956, acc.: 68.75%] [G loss: 0.431561]\n",
      "epoch:17 step:16716 [D loss: 0.242390, acc.: 57.03%] [G loss: 0.434917]\n",
      "epoch:17 step:16717 [D loss: 0.270886, acc.: 52.34%] [G loss: 0.395378]\n",
      "epoch:17 step:16718 [D loss: 0.208378, acc.: 63.28%] [G loss: 0.438456]\n",
      "epoch:17 step:16719 [D loss: 0.220724, acc.: 62.50%] [G loss: 0.425723]\n",
      "epoch:17 step:16720 [D loss: 0.230398, acc.: 62.50%] [G loss: 0.460638]\n",
      "epoch:17 step:16721 [D loss: 0.206387, acc.: 67.19%] [G loss: 0.475364]\n",
      "epoch:17 step:16722 [D loss: 0.219755, acc.: 64.84%] [G loss: 0.465707]\n",
      "epoch:17 step:16723 [D loss: 0.251934, acc.: 53.12%] [G loss: 0.460743]\n",
      "epoch:17 step:16724 [D loss: 0.243781, acc.: 64.84%] [G loss: 0.426432]\n",
      "epoch:17 step:16725 [D loss: 0.222124, acc.: 59.38%] [G loss: 0.428380]\n",
      "epoch:17 step:16726 [D loss: 0.234142, acc.: 59.38%] [G loss: 0.445165]\n",
      "epoch:17 step:16727 [D loss: 0.234184, acc.: 58.59%] [G loss: 0.415301]\n",
      "epoch:17 step:16728 [D loss: 0.226706, acc.: 64.06%] [G loss: 0.440677]\n",
      "epoch:17 step:16729 [D loss: 0.254828, acc.: 51.56%] [G loss: 0.419998]\n",
      "epoch:17 step:16730 [D loss: 0.206368, acc.: 70.31%] [G loss: 0.438509]\n",
      "epoch:17 step:16731 [D loss: 0.215232, acc.: 69.53%] [G loss: 0.477567]\n",
      "epoch:17 step:16732 [D loss: 0.218616, acc.: 65.62%] [G loss: 0.494928]\n",
      "epoch:17 step:16733 [D loss: 0.216313, acc.: 64.06%] [G loss: 0.420409]\n",
      "epoch:17 step:16734 [D loss: 0.228867, acc.: 57.81%] [G loss: 0.426066]\n",
      "epoch:17 step:16735 [D loss: 0.207574, acc.: 68.75%] [G loss: 0.411704]\n",
      "epoch:17 step:16736 [D loss: 0.203489, acc.: 68.75%] [G loss: 0.439911]\n",
      "epoch:17 step:16737 [D loss: 0.244547, acc.: 55.47%] [G loss: 0.386589]\n",
      "epoch:17 step:16738 [D loss: 0.223630, acc.: 65.62%] [G loss: 0.407828]\n",
      "epoch:17 step:16739 [D loss: 0.230503, acc.: 66.41%] [G loss: 0.413316]\n",
      "epoch:17 step:16740 [D loss: 0.204264, acc.: 64.84%] [G loss: 0.434035]\n",
      "epoch:17 step:16741 [D loss: 0.249510, acc.: 55.47%] [G loss: 0.436706]\n",
      "epoch:17 step:16742 [D loss: 0.236448, acc.: 61.72%] [G loss: 0.463699]\n",
      "epoch:17 step:16743 [D loss: 0.219568, acc.: 63.28%] [G loss: 0.434014]\n",
      "epoch:17 step:16744 [D loss: 0.211353, acc.: 65.62%] [G loss: 0.459845]\n",
      "epoch:17 step:16745 [D loss: 0.205325, acc.: 66.41%] [G loss: 0.493075]\n",
      "epoch:17 step:16746 [D loss: 0.225980, acc.: 61.72%] [G loss: 0.462765]\n",
      "epoch:17 step:16747 [D loss: 0.264287, acc.: 53.12%] [G loss: 0.437024]\n",
      "epoch:17 step:16748 [D loss: 0.217159, acc.: 68.75%] [G loss: 0.447303]\n",
      "epoch:17 step:16749 [D loss: 0.241164, acc.: 60.94%] [G loss: 0.449368]\n",
      "epoch:17 step:16750 [D loss: 0.231060, acc.: 60.94%] [G loss: 0.452983]\n",
      "epoch:17 step:16751 [D loss: 0.192176, acc.: 71.88%] [G loss: 0.446857]\n",
      "epoch:17 step:16752 [D loss: 0.228259, acc.: 61.72%] [G loss: 0.395340]\n",
      "epoch:17 step:16753 [D loss: 0.254849, acc.: 54.69%] [G loss: 0.394087]\n",
      "epoch:17 step:16754 [D loss: 0.233475, acc.: 57.81%] [G loss: 0.429778]\n",
      "epoch:17 step:16755 [D loss: 0.223794, acc.: 62.50%] [G loss: 0.456598]\n",
      "epoch:17 step:16756 [D loss: 0.236872, acc.: 58.59%] [G loss: 0.439285]\n",
      "epoch:17 step:16757 [D loss: 0.265520, acc.: 49.22%] [G loss: 0.409736]\n",
      "epoch:17 step:16758 [D loss: 0.231684, acc.: 60.94%] [G loss: 0.409202]\n",
      "epoch:17 step:16759 [D loss: 0.219337, acc.: 64.84%] [G loss: 0.415027]\n",
      "epoch:17 step:16760 [D loss: 0.225062, acc.: 66.41%] [G loss: 0.432887]\n",
      "epoch:17 step:16761 [D loss: 0.223660, acc.: 64.06%] [G loss: 0.388815]\n",
      "epoch:17 step:16762 [D loss: 0.208492, acc.: 64.84%] [G loss: 0.454391]\n",
      "epoch:17 step:16763 [D loss: 0.219671, acc.: 67.19%] [G loss: 0.432244]\n",
      "epoch:17 step:16764 [D loss: 0.226129, acc.: 61.72%] [G loss: 0.450898]\n",
      "epoch:17 step:16765 [D loss: 0.229708, acc.: 60.16%] [G loss: 0.411539]\n",
      "epoch:17 step:16766 [D loss: 0.209531, acc.: 69.53%] [G loss: 0.422312]\n",
      "epoch:17 step:16767 [D loss: 0.208805, acc.: 66.41%] [G loss: 0.426909]\n",
      "epoch:17 step:16768 [D loss: 0.212304, acc.: 63.28%] [G loss: 0.455041]\n",
      "epoch:17 step:16769 [D loss: 0.226569, acc.: 65.62%] [G loss: 0.455088]\n",
      "epoch:17 step:16770 [D loss: 0.222738, acc.: 64.84%] [G loss: 0.460123]\n",
      "epoch:17 step:16771 [D loss: 0.187299, acc.: 74.22%] [G loss: 0.487847]\n",
      "epoch:17 step:16772 [D loss: 0.238510, acc.: 59.38%] [G loss: 0.416424]\n",
      "epoch:17 step:16773 [D loss: 0.221813, acc.: 63.28%] [G loss: 0.415871]\n",
      "epoch:17 step:16774 [D loss: 0.215030, acc.: 66.41%] [G loss: 0.428254]\n",
      "epoch:17 step:16775 [D loss: 0.227159, acc.: 60.16%] [G loss: 0.430492]\n",
      "epoch:17 step:16776 [D loss: 0.271172, acc.: 50.78%] [G loss: 0.400878]\n",
      "epoch:17 step:16777 [D loss: 0.220619, acc.: 63.28%] [G loss: 0.398327]\n",
      "epoch:17 step:16778 [D loss: 0.209644, acc.: 68.75%] [G loss: 0.414697]\n",
      "epoch:17 step:16779 [D loss: 0.230679, acc.: 64.06%] [G loss: 0.457356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16780 [D loss: 0.218021, acc.: 64.84%] [G loss: 0.402504]\n",
      "epoch:17 step:16781 [D loss: 0.210547, acc.: 67.19%] [G loss: 0.428199]\n",
      "epoch:17 step:16782 [D loss: 0.213760, acc.: 67.19%] [G loss: 0.462111]\n",
      "epoch:17 step:16783 [D loss: 0.238663, acc.: 59.38%] [G loss: 0.455559]\n",
      "epoch:17 step:16784 [D loss: 0.230361, acc.: 55.47%] [G loss: 0.476807]\n",
      "epoch:17 step:16785 [D loss: 0.225069, acc.: 60.16%] [G loss: 0.447461]\n",
      "epoch:17 step:16786 [D loss: 0.230679, acc.: 57.81%] [G loss: 0.410755]\n",
      "epoch:17 step:16787 [D loss: 0.283288, acc.: 51.56%] [G loss: 0.404637]\n",
      "epoch:17 step:16788 [D loss: 0.278084, acc.: 50.78%] [G loss: 0.374519]\n",
      "epoch:17 step:16789 [D loss: 0.212849, acc.: 65.62%] [G loss: 0.421518]\n",
      "epoch:17 step:16790 [D loss: 0.238822, acc.: 62.50%] [G loss: 0.399521]\n",
      "epoch:17 step:16791 [D loss: 0.230501, acc.: 64.06%] [G loss: 0.407867]\n",
      "epoch:17 step:16792 [D loss: 0.233928, acc.: 57.81%] [G loss: 0.390994]\n",
      "epoch:17 step:16793 [D loss: 0.235799, acc.: 64.84%] [G loss: 0.402068]\n",
      "epoch:17 step:16794 [D loss: 0.248610, acc.: 57.81%] [G loss: 0.427008]\n",
      "epoch:17 step:16795 [D loss: 0.219639, acc.: 61.72%] [G loss: 0.429327]\n",
      "epoch:17 step:16796 [D loss: 0.227840, acc.: 57.81%] [G loss: 0.405555]\n",
      "epoch:17 step:16797 [D loss: 0.225600, acc.: 62.50%] [G loss: 0.463858]\n",
      "epoch:17 step:16798 [D loss: 0.257117, acc.: 49.22%] [G loss: 0.413191]\n",
      "epoch:17 step:16799 [D loss: 0.198657, acc.: 75.00%] [G loss: 0.466792]\n",
      "epoch:17 step:16800 [D loss: 0.215722, acc.: 65.62%] [G loss: 0.402546]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.982834\n",
      "FID: 8.578436\n",
      "0 = 11.783762198066734\n",
      "1 = 0.04819783631529609\n",
      "2 = 0.8753499984741211\n",
      "3 = 0.8614000082015991\n",
      "4 = 0.8892999887466431\n",
      "5 = 0.8861228227615356\n",
      "6 = 0.8614000082015991\n",
      "7 = 6.000071512770653\n",
      "8 = 0.06199153060901574\n",
      "9 = 0.6793500185012817\n",
      "10 = 0.6807000041007996\n",
      "11 = 0.6779999732971191\n",
      "12 = 0.6788670420646667\n",
      "13 = 0.6807000041007996\n",
      "14 = 7.982901573181152\n",
      "15 = 9.486106872558594\n",
      "16 = 0.12008354812860489\n",
      "17 = 7.9828338623046875\n",
      "18 = 8.578435897827148\n",
      "epoch:17 step:16801 [D loss: 0.226198, acc.: 59.38%] [G loss: 0.404843]\n",
      "epoch:17 step:16802 [D loss: 0.233133, acc.: 56.25%] [G loss: 0.428269]\n",
      "epoch:17 step:16803 [D loss: 0.209725, acc.: 65.62%] [G loss: 0.420445]\n",
      "epoch:17 step:16804 [D loss: 0.189291, acc.: 70.31%] [G loss: 0.482201]\n",
      "epoch:17 step:16805 [D loss: 0.236582, acc.: 64.84%] [G loss: 0.458969]\n",
      "epoch:17 step:16806 [D loss: 0.225553, acc.: 60.94%] [G loss: 0.429534]\n",
      "epoch:17 step:16807 [D loss: 0.226790, acc.: 63.28%] [G loss: 0.418242]\n",
      "epoch:17 step:16808 [D loss: 0.248696, acc.: 53.91%] [G loss: 0.404756]\n",
      "epoch:17 step:16809 [D loss: 0.230554, acc.: 58.59%] [G loss: 0.434571]\n",
      "epoch:17 step:16810 [D loss: 0.210811, acc.: 60.16%] [G loss: 0.418794]\n",
      "epoch:17 step:16811 [D loss: 0.222623, acc.: 60.16%] [G loss: 0.418922]\n",
      "epoch:17 step:16812 [D loss: 0.253210, acc.: 55.47%] [G loss: 0.441403]\n",
      "epoch:17 step:16813 [D loss: 0.221010, acc.: 61.72%] [G loss: 0.463784]\n",
      "epoch:17 step:16814 [D loss: 0.219742, acc.: 65.62%] [G loss: 0.496424]\n",
      "epoch:17 step:16815 [D loss: 0.225867, acc.: 61.72%] [G loss: 0.463350]\n",
      "epoch:17 step:16816 [D loss: 0.229265, acc.: 65.62%] [G loss: 0.472769]\n",
      "epoch:17 step:16817 [D loss: 0.226228, acc.: 62.50%] [G loss: 0.416206]\n",
      "epoch:17 step:16818 [D loss: 0.193354, acc.: 73.44%] [G loss: 0.473585]\n",
      "epoch:17 step:16819 [D loss: 0.200999, acc.: 71.09%] [G loss: 0.473225]\n",
      "epoch:17 step:16820 [D loss: 0.266747, acc.: 50.78%] [G loss: 0.393537]\n",
      "epoch:17 step:16821 [D loss: 0.269311, acc.: 49.22%] [G loss: 0.387451]\n",
      "epoch:17 step:16822 [D loss: 0.204542, acc.: 67.19%] [G loss: 0.422753]\n",
      "epoch:17 step:16823 [D loss: 0.193457, acc.: 74.22%] [G loss: 0.483358]\n",
      "epoch:17 step:16824 [D loss: 0.216669, acc.: 67.97%] [G loss: 0.453283]\n",
      "epoch:17 step:16825 [D loss: 0.213715, acc.: 67.97%] [G loss: 0.430672]\n",
      "epoch:17 step:16826 [D loss: 0.206307, acc.: 67.19%] [G loss: 0.450199]\n",
      "epoch:17 step:16827 [D loss: 0.198928, acc.: 72.66%] [G loss: 0.465538]\n",
      "epoch:17 step:16828 [D loss: 0.184769, acc.: 70.31%] [G loss: 0.492837]\n",
      "epoch:17 step:16829 [D loss: 0.217916, acc.: 61.72%] [G loss: 0.462471]\n",
      "epoch:17 step:16830 [D loss: 0.229791, acc.: 61.72%] [G loss: 0.436467]\n",
      "epoch:17 step:16831 [D loss: 0.246680, acc.: 57.81%] [G loss: 0.445261]\n",
      "epoch:17 step:16832 [D loss: 0.244071, acc.: 61.72%] [G loss: 0.438684]\n",
      "epoch:17 step:16833 [D loss: 0.225742, acc.: 61.72%] [G loss: 0.448195]\n",
      "epoch:17 step:16834 [D loss: 0.201787, acc.: 67.19%] [G loss: 0.456581]\n",
      "epoch:17 step:16835 [D loss: 0.230585, acc.: 65.62%] [G loss: 0.501742]\n",
      "epoch:17 step:16836 [D loss: 0.224586, acc.: 65.62%] [G loss: 0.455522]\n",
      "epoch:17 step:16837 [D loss: 0.195973, acc.: 75.00%] [G loss: 0.434009]\n",
      "epoch:17 step:16838 [D loss: 0.199825, acc.: 71.09%] [G loss: 0.441028]\n",
      "epoch:17 step:16839 [D loss: 0.200977, acc.: 69.53%] [G loss: 0.452779]\n",
      "epoch:17 step:16840 [D loss: 0.220825, acc.: 60.16%] [G loss: 0.442509]\n",
      "epoch:17 step:16841 [D loss: 0.221867, acc.: 62.50%] [G loss: 0.455277]\n",
      "epoch:17 step:16842 [D loss: 0.232540, acc.: 60.16%] [G loss: 0.452353]\n",
      "epoch:17 step:16843 [D loss: 0.211061, acc.: 61.72%] [G loss: 0.476889]\n",
      "epoch:17 step:16844 [D loss: 0.249125, acc.: 59.38%] [G loss: 0.420552]\n",
      "epoch:17 step:16845 [D loss: 0.220318, acc.: 62.50%] [G loss: 0.418155]\n",
      "epoch:17 step:16846 [D loss: 0.211382, acc.: 66.41%] [G loss: 0.462289]\n",
      "epoch:17 step:16847 [D loss: 0.166652, acc.: 80.47%] [G loss: 0.508112]\n",
      "epoch:17 step:16848 [D loss: 0.204605, acc.: 69.53%] [G loss: 0.504393]\n",
      "epoch:17 step:16849 [D loss: 0.314673, acc.: 50.00%] [G loss: 0.436908]\n",
      "epoch:17 step:16850 [D loss: 0.211786, acc.: 62.50%] [G loss: 0.418007]\n",
      "epoch:17 step:16851 [D loss: 0.241448, acc.: 57.81%] [G loss: 0.399719]\n",
      "epoch:17 step:16852 [D loss: 0.168220, acc.: 75.78%] [G loss: 0.410782]\n",
      "epoch:17 step:16853 [D loss: 0.177481, acc.: 75.00%] [G loss: 0.466549]\n",
      "epoch:17 step:16854 [D loss: 0.171459, acc.: 76.56%] [G loss: 0.491822]\n",
      "epoch:17 step:16855 [D loss: 0.180883, acc.: 77.34%] [G loss: 0.511028]\n",
      "epoch:17 step:16856 [D loss: 0.208127, acc.: 68.75%] [G loss: 0.539864]\n",
      "epoch:17 step:16857 [D loss: 0.306677, acc.: 54.69%] [G loss: 0.539173]\n",
      "epoch:17 step:16858 [D loss: 0.219404, acc.: 68.75%] [G loss: 0.643947]\n",
      "epoch:17 step:16859 [D loss: 0.237288, acc.: 59.38%] [G loss: 0.542457]\n",
      "epoch:17 step:16860 [D loss: 0.234681, acc.: 66.41%] [G loss: 0.444293]\n",
      "epoch:17 step:16861 [D loss: 0.241189, acc.: 57.03%] [G loss: 0.434129]\n",
      "epoch:17 step:16862 [D loss: 0.235630, acc.: 56.25%] [G loss: 0.417953]\n",
      "epoch:17 step:16863 [D loss: 0.216780, acc.: 62.50%] [G loss: 0.454049]\n",
      "epoch:17 step:16864 [D loss: 0.202035, acc.: 64.06%] [G loss: 0.491217]\n",
      "epoch:17 step:16865 [D loss: 0.205737, acc.: 70.31%] [G loss: 0.496158]\n",
      "epoch:17 step:16866 [D loss: 0.181040, acc.: 71.09%] [G loss: 0.555281]\n",
      "epoch:18 step:16867 [D loss: 0.241574, acc.: 60.16%] [G loss: 0.478551]\n",
      "epoch:18 step:16868 [D loss: 0.272413, acc.: 55.47%] [G loss: 0.518839]\n",
      "epoch:18 step:16869 [D loss: 0.244062, acc.: 59.38%] [G loss: 0.448427]\n",
      "epoch:18 step:16870 [D loss: 0.250227, acc.: 57.03%] [G loss: 0.452693]\n",
      "epoch:18 step:16871 [D loss: 0.226153, acc.: 64.06%] [G loss: 0.484279]\n",
      "epoch:18 step:16872 [D loss: 0.228138, acc.: 63.28%] [G loss: 0.481841]\n",
      "epoch:18 step:16873 [D loss: 0.218637, acc.: 66.41%] [G loss: 0.491913]\n",
      "epoch:18 step:16874 [D loss: 0.221826, acc.: 60.16%] [G loss: 0.489357]\n",
      "epoch:18 step:16875 [D loss: 0.226992, acc.: 60.94%] [G loss: 0.440591]\n",
      "epoch:18 step:16876 [D loss: 0.222037, acc.: 59.38%] [G loss: 0.502133]\n",
      "epoch:18 step:16877 [D loss: 0.208196, acc.: 71.09%] [G loss: 0.489469]\n",
      "epoch:18 step:16878 [D loss: 0.220135, acc.: 64.84%] [G loss: 0.447160]\n",
      "epoch:18 step:16879 [D loss: 0.210306, acc.: 64.06%] [G loss: 0.466072]\n",
      "epoch:18 step:16880 [D loss: 0.230822, acc.: 64.84%] [G loss: 0.423741]\n",
      "epoch:18 step:16881 [D loss: 0.192275, acc.: 67.97%] [G loss: 0.449447]\n",
      "epoch:18 step:16882 [D loss: 0.178547, acc.: 76.56%] [G loss: 0.481178]\n",
      "epoch:18 step:16883 [D loss: 0.258059, acc.: 52.34%] [G loss: 0.455461]\n",
      "epoch:18 step:16884 [D loss: 0.226483, acc.: 58.59%] [G loss: 0.440681]\n",
      "epoch:18 step:16885 [D loss: 0.262263, acc.: 55.47%] [G loss: 0.465049]\n",
      "epoch:18 step:16886 [D loss: 0.250293, acc.: 58.59%] [G loss: 0.478754]\n",
      "epoch:18 step:16887 [D loss: 0.254558, acc.: 55.47%] [G loss: 0.479312]\n",
      "epoch:18 step:16888 [D loss: 0.205497, acc.: 71.88%] [G loss: 0.510555]\n",
      "epoch:18 step:16889 [D loss: 0.221788, acc.: 66.41%] [G loss: 0.417965]\n",
      "epoch:18 step:16890 [D loss: 0.221420, acc.: 63.28%] [G loss: 0.397130]\n",
      "epoch:18 step:16891 [D loss: 0.191200, acc.: 71.88%] [G loss: 0.438651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:16892 [D loss: 0.220645, acc.: 64.06%] [G loss: 0.417730]\n",
      "epoch:18 step:16893 [D loss: 0.232690, acc.: 59.38%] [G loss: 0.429793]\n",
      "epoch:18 step:16894 [D loss: 0.231028, acc.: 61.72%] [G loss: 0.432074]\n",
      "epoch:18 step:16895 [D loss: 0.200167, acc.: 67.19%] [G loss: 0.471613]\n",
      "epoch:18 step:16896 [D loss: 0.237231, acc.: 56.25%] [G loss: 0.393990]\n",
      "epoch:18 step:16897 [D loss: 0.230994, acc.: 58.59%] [G loss: 0.470465]\n",
      "epoch:18 step:16898 [D loss: 0.225187, acc.: 67.97%] [G loss: 0.424150]\n",
      "epoch:18 step:16899 [D loss: 0.217825, acc.: 60.94%] [G loss: 0.467279]\n",
      "epoch:18 step:16900 [D loss: 0.244254, acc.: 56.25%] [G loss: 0.437375]\n",
      "epoch:18 step:16901 [D loss: 0.245003, acc.: 60.94%] [G loss: 0.434876]\n",
      "epoch:18 step:16902 [D loss: 0.207726, acc.: 64.06%] [G loss: 0.510499]\n",
      "epoch:18 step:16903 [D loss: 0.235713, acc.: 58.59%] [G loss: 0.461852]\n",
      "epoch:18 step:16904 [D loss: 0.254996, acc.: 50.00%] [G loss: 0.398445]\n",
      "epoch:18 step:16905 [D loss: 0.213693, acc.: 65.62%] [G loss: 0.440541]\n",
      "epoch:18 step:16906 [D loss: 0.201840, acc.: 65.62%] [G loss: 0.450908]\n",
      "epoch:18 step:16907 [D loss: 0.243563, acc.: 64.06%] [G loss: 0.475997]\n",
      "epoch:18 step:16908 [D loss: 0.205601, acc.: 67.97%] [G loss: 0.500021]\n",
      "epoch:18 step:16909 [D loss: 0.213179, acc.: 66.41%] [G loss: 0.440718]\n",
      "epoch:18 step:16910 [D loss: 0.234641, acc.: 57.03%] [G loss: 0.440204]\n",
      "epoch:18 step:16911 [D loss: 0.232839, acc.: 60.94%] [G loss: 0.433110]\n",
      "epoch:18 step:16912 [D loss: 0.238175, acc.: 57.81%] [G loss: 0.415989]\n",
      "epoch:18 step:16913 [D loss: 0.233214, acc.: 60.94%] [G loss: 0.427231]\n",
      "epoch:18 step:16914 [D loss: 0.192695, acc.: 75.00%] [G loss: 0.453618]\n",
      "epoch:18 step:16915 [D loss: 0.205195, acc.: 65.62%] [G loss: 0.424807]\n",
      "epoch:18 step:16916 [D loss: 0.209523, acc.: 63.28%] [G loss: 0.446107]\n",
      "epoch:18 step:16917 [D loss: 0.214313, acc.: 65.62%] [G loss: 0.416343]\n",
      "epoch:18 step:16918 [D loss: 0.236255, acc.: 62.50%] [G loss: 0.418237]\n",
      "epoch:18 step:16919 [D loss: 0.194539, acc.: 74.22%] [G loss: 0.447327]\n",
      "epoch:18 step:16920 [D loss: 0.221588, acc.: 64.84%] [G loss: 0.433768]\n",
      "epoch:18 step:16921 [D loss: 0.229433, acc.: 61.72%] [G loss: 0.421349]\n",
      "epoch:18 step:16922 [D loss: 0.202072, acc.: 69.53%] [G loss: 0.472943]\n",
      "epoch:18 step:16923 [D loss: 0.215277, acc.: 66.41%] [G loss: 0.449004]\n",
      "epoch:18 step:16924 [D loss: 0.228355, acc.: 63.28%] [G loss: 0.433077]\n",
      "epoch:18 step:16925 [D loss: 0.242540, acc.: 57.03%] [G loss: 0.442126]\n",
      "epoch:18 step:16926 [D loss: 0.229133, acc.: 63.28%] [G loss: 0.480095]\n",
      "epoch:18 step:16927 [D loss: 0.236847, acc.: 60.94%] [G loss: 0.410429]\n",
      "epoch:18 step:16928 [D loss: 0.259686, acc.: 57.03%] [G loss: 0.405637]\n",
      "epoch:18 step:16929 [D loss: 0.222871, acc.: 60.16%] [G loss: 0.417691]\n",
      "epoch:18 step:16930 [D loss: 0.236650, acc.: 57.03%] [G loss: 0.434959]\n",
      "epoch:18 step:16931 [D loss: 0.218329, acc.: 65.62%] [G loss: 0.436891]\n",
      "epoch:18 step:16932 [D loss: 0.234487, acc.: 64.84%] [G loss: 0.399612]\n",
      "epoch:18 step:16933 [D loss: 0.195655, acc.: 70.31%] [G loss: 0.459504]\n",
      "epoch:18 step:16934 [D loss: 0.230022, acc.: 60.94%] [G loss: 0.445956]\n",
      "epoch:18 step:16935 [D loss: 0.214180, acc.: 61.72%] [G loss: 0.443447]\n",
      "epoch:18 step:16936 [D loss: 0.185732, acc.: 72.66%] [G loss: 0.500199]\n",
      "epoch:18 step:16937 [D loss: 0.253565, acc.: 64.84%] [G loss: 0.424098]\n",
      "epoch:18 step:16938 [D loss: 0.240208, acc.: 59.38%] [G loss: 0.413015]\n",
      "epoch:18 step:16939 [D loss: 0.237500, acc.: 59.38%] [G loss: 0.382249]\n",
      "epoch:18 step:16940 [D loss: 0.226675, acc.: 58.59%] [G loss: 0.456853]\n",
      "epoch:18 step:16941 [D loss: 0.224135, acc.: 64.06%] [G loss: 0.440041]\n",
      "epoch:18 step:16942 [D loss: 0.191951, acc.: 71.88%] [G loss: 0.461012]\n",
      "epoch:18 step:16943 [D loss: 0.198447, acc.: 67.97%] [G loss: 0.451559]\n",
      "epoch:18 step:16944 [D loss: 0.272653, acc.: 48.44%] [G loss: 0.421468]\n",
      "epoch:18 step:16945 [D loss: 0.242504, acc.: 57.03%] [G loss: 0.388453]\n",
      "epoch:18 step:16946 [D loss: 0.234034, acc.: 59.38%] [G loss: 0.426384]\n",
      "epoch:18 step:16947 [D loss: 0.235831, acc.: 61.72%] [G loss: 0.443307]\n",
      "epoch:18 step:16948 [D loss: 0.216746, acc.: 64.06%] [G loss: 0.471265]\n",
      "epoch:18 step:16949 [D loss: 0.209475, acc.: 67.97%] [G loss: 0.458150]\n",
      "epoch:18 step:16950 [D loss: 0.216159, acc.: 64.06%] [G loss: 0.444186]\n",
      "epoch:18 step:16951 [D loss: 0.232297, acc.: 59.38%] [G loss: 0.429503]\n",
      "epoch:18 step:16952 [D loss: 0.208793, acc.: 64.84%] [G loss: 0.484245]\n",
      "epoch:18 step:16953 [D loss: 0.225069, acc.: 64.84%] [G loss: 0.440667]\n",
      "epoch:18 step:16954 [D loss: 0.216466, acc.: 65.62%] [G loss: 0.453730]\n",
      "epoch:18 step:16955 [D loss: 0.213084, acc.: 67.97%] [G loss: 0.453241]\n",
      "epoch:18 step:16956 [D loss: 0.219608, acc.: 65.62%] [G loss: 0.415284]\n",
      "epoch:18 step:16957 [D loss: 0.225613, acc.: 66.41%] [G loss: 0.427436]\n",
      "epoch:18 step:16958 [D loss: 0.194682, acc.: 71.09%] [G loss: 0.464402]\n",
      "epoch:18 step:16959 [D loss: 0.185864, acc.: 76.56%] [G loss: 0.496534]\n",
      "epoch:18 step:16960 [D loss: 0.233530, acc.: 62.50%] [G loss: 0.521670]\n",
      "epoch:18 step:16961 [D loss: 0.227390, acc.: 64.06%] [G loss: 0.502196]\n",
      "epoch:18 step:16962 [D loss: 0.240492, acc.: 63.28%] [G loss: 0.443811]\n",
      "epoch:18 step:16963 [D loss: 0.188859, acc.: 72.66%] [G loss: 0.455265]\n",
      "epoch:18 step:16964 [D loss: 0.207067, acc.: 67.97%] [G loss: 0.437321]\n",
      "epoch:18 step:16965 [D loss: 0.241030, acc.: 59.38%] [G loss: 0.419295]\n",
      "epoch:18 step:16966 [D loss: 0.174200, acc.: 75.00%] [G loss: 0.496024]\n",
      "epoch:18 step:16967 [D loss: 0.213104, acc.: 63.28%] [G loss: 0.470646]\n",
      "epoch:18 step:16968 [D loss: 0.231592, acc.: 58.59%] [G loss: 0.421569]\n",
      "epoch:18 step:16969 [D loss: 0.240658, acc.: 56.25%] [G loss: 0.387446]\n",
      "epoch:18 step:16970 [D loss: 0.219798, acc.: 66.41%] [G loss: 0.388884]\n",
      "epoch:18 step:16971 [D loss: 0.249826, acc.: 51.56%] [G loss: 0.406391]\n",
      "epoch:18 step:16972 [D loss: 0.224978, acc.: 63.28%] [G loss: 0.389459]\n",
      "epoch:18 step:16973 [D loss: 0.218960, acc.: 65.62%] [G loss: 0.445196]\n",
      "epoch:18 step:16974 [D loss: 0.260846, acc.: 54.69%] [G loss: 0.479520]\n",
      "epoch:18 step:16975 [D loss: 0.263208, acc.: 55.47%] [G loss: 0.447474]\n",
      "epoch:18 step:16976 [D loss: 0.250556, acc.: 54.69%] [G loss: 0.446213]\n",
      "epoch:18 step:16977 [D loss: 0.204493, acc.: 67.19%] [G loss: 0.465522]\n",
      "epoch:18 step:16978 [D loss: 0.216767, acc.: 66.41%] [G loss: 0.420344]\n",
      "epoch:18 step:16979 [D loss: 0.221582, acc.: 62.50%] [G loss: 0.446220]\n",
      "epoch:18 step:16980 [D loss: 0.206421, acc.: 68.75%] [G loss: 0.491843]\n",
      "epoch:18 step:16981 [D loss: 0.207349, acc.: 67.97%] [G loss: 0.489029]\n",
      "epoch:18 step:16982 [D loss: 0.226262, acc.: 63.28%] [G loss: 0.474154]\n",
      "epoch:18 step:16983 [D loss: 0.223671, acc.: 66.41%] [G loss: 0.455249]\n",
      "epoch:18 step:16984 [D loss: 0.231121, acc.: 60.94%] [G loss: 0.462580]\n",
      "epoch:18 step:16985 [D loss: 0.184328, acc.: 71.09%] [G loss: 0.506876]\n",
      "epoch:18 step:16986 [D loss: 0.241134, acc.: 62.50%] [G loss: 0.466434]\n",
      "epoch:18 step:16987 [D loss: 0.268603, acc.: 55.47%] [G loss: 0.414404]\n",
      "epoch:18 step:16988 [D loss: 0.180564, acc.: 75.00%] [G loss: 0.505496]\n",
      "epoch:18 step:16989 [D loss: 0.231090, acc.: 61.72%] [G loss: 0.453786]\n",
      "epoch:18 step:16990 [D loss: 0.260884, acc.: 51.56%] [G loss: 0.437446]\n",
      "epoch:18 step:16991 [D loss: 0.258524, acc.: 55.47%] [G loss: 0.413996]\n",
      "epoch:18 step:16992 [D loss: 0.187870, acc.: 73.44%] [G loss: 0.441195]\n",
      "epoch:18 step:16993 [D loss: 0.240657, acc.: 59.38%] [G loss: 0.385246]\n",
      "epoch:18 step:16994 [D loss: 0.212540, acc.: 67.19%] [G loss: 0.401102]\n",
      "epoch:18 step:16995 [D loss: 0.240484, acc.: 57.81%] [G loss: 0.417272]\n",
      "epoch:18 step:16996 [D loss: 0.212228, acc.: 67.97%] [G loss: 0.460670]\n",
      "epoch:18 step:16997 [D loss: 0.236617, acc.: 64.84%] [G loss: 0.429889]\n",
      "epoch:18 step:16998 [D loss: 0.225519, acc.: 60.94%] [G loss: 0.430856]\n",
      "epoch:18 step:16999 [D loss: 0.270200, acc.: 53.12%] [G loss: 0.421852]\n",
      "epoch:18 step:17000 [D loss: 0.220569, acc.: 63.28%] [G loss: 0.411552]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.988786\n",
      "FID: 8.124151\n",
      "0 = 11.810030295610435\n",
      "1 = 0.05532142524047268\n",
      "2 = 0.8760499954223633\n",
      "3 = 0.8672000169754028\n",
      "4 = 0.8848999738693237\n",
      "5 = 0.882826030254364\n",
      "6 = 0.8672000169754028\n",
      "7 = 5.909731796276554\n",
      "8 = 0.05872990365956941\n",
      "9 = 0.6828500032424927\n",
      "10 = 0.6812999844551086\n",
      "11 = 0.6844000220298767\n",
      "12 = 0.6834185719490051\n",
      "13 = 0.6812999844551086\n",
      "14 = 7.988857269287109\n",
      "15 = 9.527937889099121\n",
      "16 = 0.1157458946108818\n",
      "17 = 7.988785743713379\n",
      "18 = 8.124151229858398\n",
      "epoch:18 step:17001 [D loss: 0.198044, acc.: 72.66%] [G loss: 0.434516]\n",
      "epoch:18 step:17002 [D loss: 0.214907, acc.: 63.28%] [G loss: 0.443332]\n",
      "epoch:18 step:17003 [D loss: 0.282177, acc.: 51.56%] [G loss: 0.406716]\n",
      "epoch:18 step:17004 [D loss: 0.250590, acc.: 52.34%] [G loss: 0.436432]\n",
      "epoch:18 step:17005 [D loss: 0.223394, acc.: 60.94%] [G loss: 0.449042]\n",
      "epoch:18 step:17006 [D loss: 0.248320, acc.: 57.81%] [G loss: 0.378584]\n",
      "epoch:18 step:17007 [D loss: 0.256802, acc.: 53.12%] [G loss: 0.370982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17008 [D loss: 0.228442, acc.: 61.72%] [G loss: 0.431765]\n",
      "epoch:18 step:17009 [D loss: 0.232878, acc.: 55.47%] [G loss: 0.420965]\n",
      "epoch:18 step:17010 [D loss: 0.229103, acc.: 61.72%] [G loss: 0.411549]\n",
      "epoch:18 step:17011 [D loss: 0.230479, acc.: 61.72%] [G loss: 0.405459]\n",
      "epoch:18 step:17012 [D loss: 0.214506, acc.: 67.19%] [G loss: 0.433350]\n",
      "epoch:18 step:17013 [D loss: 0.246722, acc.: 59.38%] [G loss: 0.426122]\n",
      "epoch:18 step:17014 [D loss: 0.239133, acc.: 61.72%] [G loss: 0.418792]\n",
      "epoch:18 step:17015 [D loss: 0.212340, acc.: 66.41%] [G loss: 0.393827]\n",
      "epoch:18 step:17016 [D loss: 0.230197, acc.: 62.50%] [G loss: 0.452036]\n",
      "epoch:18 step:17017 [D loss: 0.220006, acc.: 64.06%] [G loss: 0.416975]\n",
      "epoch:18 step:17018 [D loss: 0.219172, acc.: 64.06%] [G loss: 0.420957]\n",
      "epoch:18 step:17019 [D loss: 0.249914, acc.: 55.47%] [G loss: 0.419297]\n",
      "epoch:18 step:17020 [D loss: 0.223905, acc.: 63.28%] [G loss: 0.450966]\n",
      "epoch:18 step:17021 [D loss: 0.212477, acc.: 62.50%] [G loss: 0.459846]\n",
      "epoch:18 step:17022 [D loss: 0.232539, acc.: 62.50%] [G loss: 0.425136]\n",
      "epoch:18 step:17023 [D loss: 0.219624, acc.: 59.38%] [G loss: 0.414630]\n",
      "epoch:18 step:17024 [D loss: 0.238005, acc.: 60.94%] [G loss: 0.421731]\n",
      "epoch:18 step:17025 [D loss: 0.224688, acc.: 67.97%] [G loss: 0.428218]\n",
      "epoch:18 step:17026 [D loss: 0.243418, acc.: 60.94%] [G loss: 0.423866]\n",
      "epoch:18 step:17027 [D loss: 0.241460, acc.: 54.69%] [G loss: 0.422707]\n",
      "epoch:18 step:17028 [D loss: 0.235049, acc.: 59.38%] [G loss: 0.441737]\n",
      "epoch:18 step:17029 [D loss: 0.229608, acc.: 63.28%] [G loss: 0.423779]\n",
      "epoch:18 step:17030 [D loss: 0.222737, acc.: 63.28%] [G loss: 0.454728]\n",
      "epoch:18 step:17031 [D loss: 0.225920, acc.: 67.97%] [G loss: 0.449559]\n",
      "epoch:18 step:17032 [D loss: 0.202819, acc.: 71.09%] [G loss: 0.470909]\n",
      "epoch:18 step:17033 [D loss: 0.226629, acc.: 62.50%] [G loss: 0.405782]\n",
      "epoch:18 step:17034 [D loss: 0.226194, acc.: 63.28%] [G loss: 0.391429]\n",
      "epoch:18 step:17035 [D loss: 0.219613, acc.: 63.28%] [G loss: 0.445637]\n",
      "epoch:18 step:17036 [D loss: 0.225232, acc.: 58.59%] [G loss: 0.450274]\n",
      "epoch:18 step:17037 [D loss: 0.210810, acc.: 64.84%] [G loss: 0.447645]\n",
      "epoch:18 step:17038 [D loss: 0.226676, acc.: 56.25%] [G loss: 0.442929]\n",
      "epoch:18 step:17039 [D loss: 0.225682, acc.: 61.72%] [G loss: 0.411979]\n",
      "epoch:18 step:17040 [D loss: 0.257034, acc.: 52.34%] [G loss: 0.410897]\n",
      "epoch:18 step:17041 [D loss: 0.242693, acc.: 49.22%] [G loss: 0.392182]\n",
      "epoch:18 step:17042 [D loss: 0.202171, acc.: 65.62%] [G loss: 0.457631]\n",
      "epoch:18 step:17043 [D loss: 0.247343, acc.: 59.38%] [G loss: 0.424291]\n",
      "epoch:18 step:17044 [D loss: 0.238431, acc.: 64.06%] [G loss: 0.407626]\n",
      "epoch:18 step:17045 [D loss: 0.234401, acc.: 62.50%] [G loss: 0.493487]\n",
      "epoch:18 step:17046 [D loss: 0.219079, acc.: 66.41%] [G loss: 0.421347]\n",
      "epoch:18 step:17047 [D loss: 0.213900, acc.: 70.31%] [G loss: 0.403589]\n",
      "epoch:18 step:17048 [D loss: 0.226567, acc.: 60.94%] [G loss: 0.433169]\n",
      "epoch:18 step:17049 [D loss: 0.250953, acc.: 58.59%] [G loss: 0.422707]\n",
      "epoch:18 step:17050 [D loss: 0.230096, acc.: 59.38%] [G loss: 0.470535]\n",
      "epoch:18 step:17051 [D loss: 0.218102, acc.: 63.28%] [G loss: 0.450486]\n",
      "epoch:18 step:17052 [D loss: 0.239995, acc.: 52.34%] [G loss: 0.429432]\n",
      "epoch:18 step:17053 [D loss: 0.243352, acc.: 57.81%] [G loss: 0.367677]\n",
      "epoch:18 step:17054 [D loss: 0.246637, acc.: 59.38%] [G loss: 0.393461]\n",
      "epoch:18 step:17055 [D loss: 0.236319, acc.: 60.94%] [G loss: 0.390901]\n",
      "epoch:18 step:17056 [D loss: 0.207786, acc.: 67.19%] [G loss: 0.413511]\n",
      "epoch:18 step:17057 [D loss: 0.208836, acc.: 67.19%] [G loss: 0.421660]\n",
      "epoch:18 step:17058 [D loss: 0.231103, acc.: 62.50%] [G loss: 0.426965]\n",
      "epoch:18 step:17059 [D loss: 0.217420, acc.: 61.72%] [G loss: 0.449565]\n",
      "epoch:18 step:17060 [D loss: 0.211725, acc.: 70.31%] [G loss: 0.424252]\n",
      "epoch:18 step:17061 [D loss: 0.214211, acc.: 68.75%] [G loss: 0.439086]\n",
      "epoch:18 step:17062 [D loss: 0.221784, acc.: 54.69%] [G loss: 0.434892]\n",
      "epoch:18 step:17063 [D loss: 0.218485, acc.: 63.28%] [G loss: 0.436161]\n",
      "epoch:18 step:17064 [D loss: 0.175391, acc.: 74.22%] [G loss: 0.490374]\n",
      "epoch:18 step:17065 [D loss: 0.230626, acc.: 61.72%] [G loss: 0.441154]\n",
      "epoch:18 step:17066 [D loss: 0.254154, acc.: 56.25%] [G loss: 0.447880]\n",
      "epoch:18 step:17067 [D loss: 0.245317, acc.: 54.69%] [G loss: 0.400094]\n",
      "epoch:18 step:17068 [D loss: 0.210394, acc.: 67.97%] [G loss: 0.435327]\n",
      "epoch:18 step:17069 [D loss: 0.267841, acc.: 46.88%] [G loss: 0.409428]\n",
      "epoch:18 step:17070 [D loss: 0.204780, acc.: 67.97%] [G loss: 0.469163]\n",
      "epoch:18 step:17071 [D loss: 0.226764, acc.: 64.06%] [G loss: 0.424625]\n",
      "epoch:18 step:17072 [D loss: 0.223596, acc.: 60.16%] [G loss: 0.446080]\n",
      "epoch:18 step:17073 [D loss: 0.225155, acc.: 60.16%] [G loss: 0.466949]\n",
      "epoch:18 step:17074 [D loss: 0.213960, acc.: 63.28%] [G loss: 0.444905]\n",
      "epoch:18 step:17075 [D loss: 0.187082, acc.: 73.44%] [G loss: 0.473690]\n",
      "epoch:18 step:17076 [D loss: 0.282627, acc.: 53.12%] [G loss: 0.410903]\n",
      "epoch:18 step:17077 [D loss: 0.238354, acc.: 57.03%] [G loss: 0.423119]\n",
      "epoch:18 step:17078 [D loss: 0.221814, acc.: 62.50%] [G loss: 0.416047]\n",
      "epoch:18 step:17079 [D loss: 0.210108, acc.: 62.50%] [G loss: 0.414485]\n",
      "epoch:18 step:17080 [D loss: 0.254162, acc.: 53.91%] [G loss: 0.411070]\n",
      "epoch:18 step:17081 [D loss: 0.246051, acc.: 57.81%] [G loss: 0.423706]\n",
      "epoch:18 step:17082 [D loss: 0.215623, acc.: 68.75%] [G loss: 0.423804]\n",
      "epoch:18 step:17083 [D loss: 0.218132, acc.: 68.75%] [G loss: 0.406285]\n",
      "epoch:18 step:17084 [D loss: 0.211314, acc.: 63.28%] [G loss: 0.446474]\n",
      "epoch:18 step:17085 [D loss: 0.191228, acc.: 71.88%] [G loss: 0.478686]\n",
      "epoch:18 step:17086 [D loss: 0.253963, acc.: 57.81%] [G loss: 0.442152]\n",
      "epoch:18 step:17087 [D loss: 0.185369, acc.: 69.53%] [G loss: 0.469263]\n",
      "epoch:18 step:17088 [D loss: 0.196838, acc.: 71.88%] [G loss: 0.445106]\n",
      "epoch:18 step:17089 [D loss: 0.196135, acc.: 70.31%] [G loss: 0.503329]\n",
      "epoch:18 step:17090 [D loss: 0.278618, acc.: 55.47%] [G loss: 0.404303]\n",
      "epoch:18 step:17091 [D loss: 0.232934, acc.: 60.94%] [G loss: 0.439039]\n",
      "epoch:18 step:17092 [D loss: 0.227871, acc.: 55.47%] [G loss: 0.453968]\n",
      "epoch:18 step:17093 [D loss: 0.201801, acc.: 69.53%] [G loss: 0.428449]\n",
      "epoch:18 step:17094 [D loss: 0.243742, acc.: 57.81%] [G loss: 0.440735]\n",
      "epoch:18 step:17095 [D loss: 0.214457, acc.: 67.97%] [G loss: 0.461408]\n",
      "epoch:18 step:17096 [D loss: 0.198960, acc.: 65.62%] [G loss: 0.498208]\n",
      "epoch:18 step:17097 [D loss: 0.160885, acc.: 76.56%] [G loss: 0.517947]\n",
      "epoch:18 step:17098 [D loss: 0.200957, acc.: 66.41%] [G loss: 0.529626]\n",
      "epoch:18 step:17099 [D loss: 0.239106, acc.: 55.47%] [G loss: 0.450309]\n",
      "epoch:18 step:17100 [D loss: 0.241218, acc.: 60.16%] [G loss: 0.430099]\n",
      "epoch:18 step:17101 [D loss: 0.248712, acc.: 55.47%] [G loss: 0.419405]\n",
      "epoch:18 step:17102 [D loss: 0.234850, acc.: 60.94%] [G loss: 0.409285]\n",
      "epoch:18 step:17103 [D loss: 0.224241, acc.: 64.06%] [G loss: 0.404037]\n",
      "epoch:18 step:17104 [D loss: 0.207574, acc.: 68.75%] [G loss: 0.413446]\n",
      "epoch:18 step:17105 [D loss: 0.216949, acc.: 62.50%] [G loss: 0.413989]\n",
      "epoch:18 step:17106 [D loss: 0.217289, acc.: 64.84%] [G loss: 0.464789]\n",
      "epoch:18 step:17107 [D loss: 0.209366, acc.: 63.28%] [G loss: 0.456706]\n",
      "epoch:18 step:17108 [D loss: 0.194350, acc.: 69.53%] [G loss: 0.491572]\n",
      "epoch:18 step:17109 [D loss: 0.215327, acc.: 62.50%] [G loss: 0.497770]\n",
      "epoch:18 step:17110 [D loss: 0.207879, acc.: 68.75%] [G loss: 0.469141]\n",
      "epoch:18 step:17111 [D loss: 0.191451, acc.: 75.78%] [G loss: 0.432780]\n",
      "epoch:18 step:17112 [D loss: 0.211266, acc.: 70.31%] [G loss: 0.430718]\n",
      "epoch:18 step:17113 [D loss: 0.247656, acc.: 63.28%] [G loss: 0.457826]\n",
      "epoch:18 step:17114 [D loss: 0.222835, acc.: 63.28%] [G loss: 0.436945]\n",
      "epoch:18 step:17115 [D loss: 0.271267, acc.: 50.78%] [G loss: 0.460645]\n",
      "epoch:18 step:17116 [D loss: 0.258048, acc.: 51.56%] [G loss: 0.442766]\n",
      "epoch:18 step:17117 [D loss: 0.245615, acc.: 53.12%] [G loss: 0.421881]\n",
      "epoch:18 step:17118 [D loss: 0.220520, acc.: 66.41%] [G loss: 0.429611]\n",
      "epoch:18 step:17119 [D loss: 0.231099, acc.: 58.59%] [G loss: 0.430157]\n",
      "epoch:18 step:17120 [D loss: 0.213714, acc.: 65.62%] [G loss: 0.447448]\n",
      "epoch:18 step:17121 [D loss: 0.209767, acc.: 70.31%] [G loss: 0.433542]\n",
      "epoch:18 step:17122 [D loss: 0.227310, acc.: 63.28%] [G loss: 0.461191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17123 [D loss: 0.235023, acc.: 60.94%] [G loss: 0.430457]\n",
      "epoch:18 step:17124 [D loss: 0.198691, acc.: 65.62%] [G loss: 0.450117]\n",
      "epoch:18 step:17125 [D loss: 0.206621, acc.: 66.41%] [G loss: 0.401993]\n",
      "epoch:18 step:17126 [D loss: 0.225236, acc.: 60.94%] [G loss: 0.402638]\n",
      "epoch:18 step:17127 [D loss: 0.210364, acc.: 66.41%] [G loss: 0.435489]\n",
      "epoch:18 step:17128 [D loss: 0.210990, acc.: 65.62%] [G loss: 0.468827]\n",
      "epoch:18 step:17129 [D loss: 0.241313, acc.: 62.50%] [G loss: 0.457164]\n",
      "epoch:18 step:17130 [D loss: 0.214660, acc.: 67.19%] [G loss: 0.427025]\n",
      "epoch:18 step:17131 [D loss: 0.244226, acc.: 59.38%] [G loss: 0.436118]\n",
      "epoch:18 step:17132 [D loss: 0.222685, acc.: 60.94%] [G loss: 0.416567]\n",
      "epoch:18 step:17133 [D loss: 0.213639, acc.: 67.97%] [G loss: 0.453647]\n",
      "epoch:18 step:17134 [D loss: 0.238354, acc.: 59.38%] [G loss: 0.424030]\n",
      "epoch:18 step:17135 [D loss: 0.208659, acc.: 67.19%] [G loss: 0.460084]\n",
      "epoch:18 step:17136 [D loss: 0.236848, acc.: 58.59%] [G loss: 0.398359]\n",
      "epoch:18 step:17137 [D loss: 0.220900, acc.: 64.06%] [G loss: 0.414028]\n",
      "epoch:18 step:17138 [D loss: 0.227852, acc.: 60.94%] [G loss: 0.468428]\n",
      "epoch:18 step:17139 [D loss: 0.228889, acc.: 60.94%] [G loss: 0.450885]\n",
      "epoch:18 step:17140 [D loss: 0.206443, acc.: 68.75%] [G loss: 0.468320]\n",
      "epoch:18 step:17141 [D loss: 0.216376, acc.: 72.66%] [G loss: 0.472676]\n",
      "epoch:18 step:17142 [D loss: 0.207345, acc.: 71.09%] [G loss: 0.463192]\n",
      "epoch:18 step:17143 [D loss: 0.250185, acc.: 57.81%] [G loss: 0.448206]\n",
      "epoch:18 step:17144 [D loss: 0.267113, acc.: 49.22%] [G loss: 0.413553]\n",
      "epoch:18 step:17145 [D loss: 0.227159, acc.: 60.94%] [G loss: 0.409273]\n",
      "epoch:18 step:17146 [D loss: 0.208075, acc.: 67.19%] [G loss: 0.428750]\n",
      "epoch:18 step:17147 [D loss: 0.258756, acc.: 55.47%] [G loss: 0.381618]\n",
      "epoch:18 step:17148 [D loss: 0.234132, acc.: 57.81%] [G loss: 0.404111]\n",
      "epoch:18 step:17149 [D loss: 0.210208, acc.: 67.19%] [G loss: 0.419030]\n",
      "epoch:18 step:17150 [D loss: 0.242818, acc.: 51.56%] [G loss: 0.445728]\n",
      "epoch:18 step:17151 [D loss: 0.208843, acc.: 64.06%] [G loss: 0.424111]\n",
      "epoch:18 step:17152 [D loss: 0.207161, acc.: 67.97%] [G loss: 0.433347]\n",
      "epoch:18 step:17153 [D loss: 0.227172, acc.: 62.50%] [G loss: 0.438855]\n",
      "epoch:18 step:17154 [D loss: 0.222238, acc.: 60.16%] [G loss: 0.438029]\n",
      "epoch:18 step:17155 [D loss: 0.220797, acc.: 60.94%] [G loss: 0.453640]\n",
      "epoch:18 step:17156 [D loss: 0.227903, acc.: 56.25%] [G loss: 0.433354]\n",
      "epoch:18 step:17157 [D loss: 0.262524, acc.: 52.34%] [G loss: 0.408141]\n",
      "epoch:18 step:17158 [D loss: 0.216108, acc.: 67.19%] [G loss: 0.406445]\n",
      "epoch:18 step:17159 [D loss: 0.226313, acc.: 60.94%] [G loss: 0.420197]\n",
      "epoch:18 step:17160 [D loss: 0.253674, acc.: 50.78%] [G loss: 0.416222]\n",
      "epoch:18 step:17161 [D loss: 0.248417, acc.: 60.16%] [G loss: 0.442520]\n",
      "epoch:18 step:17162 [D loss: 0.208346, acc.: 66.41%] [G loss: 0.434288]\n",
      "epoch:18 step:17163 [D loss: 0.228571, acc.: 61.72%] [G loss: 0.408188]\n",
      "epoch:18 step:17164 [D loss: 0.193470, acc.: 72.66%] [G loss: 0.479524]\n",
      "epoch:18 step:17165 [D loss: 0.208409, acc.: 69.53%] [G loss: 0.449815]\n",
      "epoch:18 step:17166 [D loss: 0.208049, acc.: 68.75%] [G loss: 0.454972]\n",
      "epoch:18 step:17167 [D loss: 0.256349, acc.: 57.03%] [G loss: 0.454334]\n",
      "epoch:18 step:17168 [D loss: 0.236859, acc.: 60.94%] [G loss: 0.445582]\n",
      "epoch:18 step:17169 [D loss: 0.231200, acc.: 61.72%] [G loss: 0.420097]\n",
      "epoch:18 step:17170 [D loss: 0.201129, acc.: 70.31%] [G loss: 0.436073]\n",
      "epoch:18 step:17171 [D loss: 0.216545, acc.: 66.41%] [G loss: 0.468412]\n",
      "epoch:18 step:17172 [D loss: 0.211409, acc.: 61.72%] [G loss: 0.465362]\n",
      "epoch:18 step:17173 [D loss: 0.248483, acc.: 53.91%] [G loss: 0.438213]\n",
      "epoch:18 step:17174 [D loss: 0.215897, acc.: 59.38%] [G loss: 0.412033]\n",
      "epoch:18 step:17175 [D loss: 0.221156, acc.: 67.97%] [G loss: 0.442879]\n",
      "epoch:18 step:17176 [D loss: 0.210294, acc.: 65.62%] [G loss: 0.458147]\n",
      "epoch:18 step:17177 [D loss: 0.233573, acc.: 59.38%] [G loss: 0.436716]\n",
      "epoch:18 step:17178 [D loss: 0.199539, acc.: 67.97%] [G loss: 0.455490]\n",
      "epoch:18 step:17179 [D loss: 0.203005, acc.: 68.75%] [G loss: 0.476932]\n",
      "epoch:18 step:17180 [D loss: 0.196218, acc.: 66.41%] [G loss: 0.488776]\n",
      "epoch:18 step:17181 [D loss: 0.217328, acc.: 63.28%] [G loss: 0.430220]\n",
      "epoch:18 step:17182 [D loss: 0.267888, acc.: 52.34%] [G loss: 0.427379]\n",
      "epoch:18 step:17183 [D loss: 0.249642, acc.: 56.25%] [G loss: 0.429986]\n",
      "epoch:18 step:17184 [D loss: 0.214919, acc.: 67.19%] [G loss: 0.415525]\n",
      "epoch:18 step:17185 [D loss: 0.230911, acc.: 57.03%] [G loss: 0.419022]\n",
      "epoch:18 step:17186 [D loss: 0.207660, acc.: 65.62%] [G loss: 0.410551]\n",
      "epoch:18 step:17187 [D loss: 0.212117, acc.: 64.06%] [G loss: 0.441497]\n",
      "epoch:18 step:17188 [D loss: 0.221632, acc.: 66.41%] [G loss: 0.459850]\n",
      "epoch:18 step:17189 [D loss: 0.226713, acc.: 64.06%] [G loss: 0.426247]\n",
      "epoch:18 step:17190 [D loss: 0.207513, acc.: 69.53%] [G loss: 0.428725]\n",
      "epoch:18 step:17191 [D loss: 0.233061, acc.: 63.28%] [G loss: 0.423164]\n",
      "epoch:18 step:17192 [D loss: 0.195722, acc.: 71.09%] [G loss: 0.427336]\n",
      "epoch:18 step:17193 [D loss: 0.218980, acc.: 67.19%] [G loss: 0.470218]\n",
      "epoch:18 step:17194 [D loss: 0.203974, acc.: 64.06%] [G loss: 0.475449]\n",
      "epoch:18 step:17195 [D loss: 0.213023, acc.: 65.62%] [G loss: 0.473590]\n",
      "epoch:18 step:17196 [D loss: 0.214273, acc.: 68.75%] [G loss: 0.436471]\n",
      "epoch:18 step:17197 [D loss: 0.199383, acc.: 66.41%] [G loss: 0.428772]\n",
      "epoch:18 step:17198 [D loss: 0.215522, acc.: 65.62%] [G loss: 0.390289]\n",
      "epoch:18 step:17199 [D loss: 0.201657, acc.: 76.56%] [G loss: 0.414124]\n",
      "epoch:18 step:17200 [D loss: 0.229788, acc.: 63.28%] [G loss: 0.416657]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 8.112236\n",
      "FID: 5.215371\n",
      "0 = 11.671088083028831\n",
      "1 = 0.042070183288885836\n",
      "2 = 0.8611000180244446\n",
      "3 = 0.8536999821662903\n",
      "4 = 0.8684999942779541\n",
      "5 = 0.8665245771408081\n",
      "6 = 0.8536999821662903\n",
      "7 = 5.602989293366664\n",
      "8 = 0.0469296454754421\n",
      "9 = 0.6794000267982483\n",
      "10 = 0.6894000172615051\n",
      "11 = 0.6693999767303467\n",
      "12 = 0.6758823394775391\n",
      "13 = 0.6894000172615051\n",
      "14 = 8.112311363220215\n",
      "15 = 9.593621253967285\n",
      "16 = 0.09188591688871384\n",
      "17 = 8.112236022949219\n",
      "18 = 5.215371131896973\n",
      "epoch:18 step:17201 [D loss: 0.209828, acc.: 63.28%] [G loss: 0.471113]\n",
      "epoch:18 step:17202 [D loss: 0.219321, acc.: 65.62%] [G loss: 0.450845]\n",
      "epoch:18 step:17203 [D loss: 0.219473, acc.: 64.84%] [G loss: 0.435436]\n",
      "epoch:18 step:17204 [D loss: 0.209721, acc.: 64.84%] [G loss: 0.452554]\n",
      "epoch:18 step:17205 [D loss: 0.220187, acc.: 67.19%] [G loss: 0.446394]\n",
      "epoch:18 step:17206 [D loss: 0.242169, acc.: 57.81%] [G loss: 0.475053]\n",
      "epoch:18 step:17207 [D loss: 0.273760, acc.: 53.91%] [G loss: 0.459956]\n",
      "epoch:18 step:17208 [D loss: 0.227666, acc.: 59.38%] [G loss: 0.439356]\n",
      "epoch:18 step:17209 [D loss: 0.221366, acc.: 64.84%] [G loss: 0.459409]\n",
      "epoch:18 step:17210 [D loss: 0.207285, acc.: 69.53%] [G loss: 0.478016]\n",
      "epoch:18 step:17211 [D loss: 0.195239, acc.: 69.53%] [G loss: 0.499950]\n",
      "epoch:18 step:17212 [D loss: 0.172742, acc.: 77.34%] [G loss: 0.509560]\n",
      "epoch:18 step:17213 [D loss: 0.173075, acc.: 74.22%] [G loss: 0.550788]\n",
      "epoch:18 step:17214 [D loss: 0.264935, acc.: 57.81%] [G loss: 0.454157]\n",
      "epoch:18 step:17215 [D loss: 0.263063, acc.: 53.91%] [G loss: 0.406930]\n",
      "epoch:18 step:17216 [D loss: 0.231397, acc.: 66.41%] [G loss: 0.402147]\n",
      "epoch:18 step:17217 [D loss: 0.210272, acc.: 70.31%] [G loss: 0.425963]\n",
      "epoch:18 step:17218 [D loss: 0.246103, acc.: 56.25%] [G loss: 0.419464]\n",
      "epoch:18 step:17219 [D loss: 0.201854, acc.: 67.19%] [G loss: 0.452877]\n",
      "epoch:18 step:17220 [D loss: 0.181491, acc.: 71.09%] [G loss: 0.466958]\n",
      "epoch:18 step:17221 [D loss: 0.231779, acc.: 59.38%] [G loss: 0.467070]\n",
      "epoch:18 step:17222 [D loss: 0.225548, acc.: 64.06%] [G loss: 0.435719]\n",
      "epoch:18 step:17223 [D loss: 0.213403, acc.: 64.84%] [G loss: 0.444209]\n",
      "epoch:18 step:17224 [D loss: 0.182527, acc.: 73.44%] [G loss: 0.488881]\n",
      "epoch:18 step:17225 [D loss: 0.212585, acc.: 67.97%] [G loss: 0.420412]\n",
      "epoch:18 step:17226 [D loss: 0.202948, acc.: 67.19%] [G loss: 0.476030]\n",
      "epoch:18 step:17227 [D loss: 0.204829, acc.: 73.44%] [G loss: 0.473207]\n",
      "epoch:18 step:17228 [D loss: 0.231031, acc.: 62.50%] [G loss: 0.424599]\n",
      "epoch:18 step:17229 [D loss: 0.218606, acc.: 62.50%] [G loss: 0.440904]\n",
      "epoch:18 step:17230 [D loss: 0.199619, acc.: 67.97%] [G loss: 0.446552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17231 [D loss: 0.215606, acc.: 69.53%] [G loss: 0.437874]\n",
      "epoch:18 step:17232 [D loss: 0.254021, acc.: 56.25%] [G loss: 0.413480]\n",
      "epoch:18 step:17233 [D loss: 0.211063, acc.: 66.41%] [G loss: 0.432119]\n",
      "epoch:18 step:17234 [D loss: 0.240473, acc.: 56.25%] [G loss: 0.441812]\n",
      "epoch:18 step:17235 [D loss: 0.256714, acc.: 53.12%] [G loss: 0.419336]\n",
      "epoch:18 step:17236 [D loss: 0.201952, acc.: 63.28%] [G loss: 0.460839]\n",
      "epoch:18 step:17237 [D loss: 0.191456, acc.: 73.44%] [G loss: 0.457968]\n",
      "epoch:18 step:17238 [D loss: 0.213208, acc.: 63.28%] [G loss: 0.440128]\n",
      "epoch:18 step:17239 [D loss: 0.251880, acc.: 56.25%] [G loss: 0.468255]\n",
      "epoch:18 step:17240 [D loss: 0.208534, acc.: 64.06%] [G loss: 0.424984]\n",
      "epoch:18 step:17241 [D loss: 0.216046, acc.: 65.62%] [G loss: 0.449586]\n",
      "epoch:18 step:17242 [D loss: 0.241572, acc.: 59.38%] [G loss: 0.462544]\n",
      "epoch:18 step:17243 [D loss: 0.262813, acc.: 50.00%] [G loss: 0.442206]\n",
      "epoch:18 step:17244 [D loss: 0.227789, acc.: 62.50%] [G loss: 0.362881]\n",
      "epoch:18 step:17245 [D loss: 0.213537, acc.: 67.97%] [G loss: 0.403027]\n",
      "epoch:18 step:17246 [D loss: 0.248244, acc.: 56.25%] [G loss: 0.411033]\n",
      "epoch:18 step:17247 [D loss: 0.197554, acc.: 67.97%] [G loss: 0.429202]\n",
      "epoch:18 step:17248 [D loss: 0.246433, acc.: 57.81%] [G loss: 0.446449]\n",
      "epoch:18 step:17249 [D loss: 0.226296, acc.: 60.16%] [G loss: 0.458595]\n",
      "epoch:18 step:17250 [D loss: 0.217770, acc.: 65.62%] [G loss: 0.422604]\n",
      "epoch:18 step:17251 [D loss: 0.229600, acc.: 61.72%] [G loss: 0.426503]\n",
      "epoch:18 step:17252 [D loss: 0.236546, acc.: 59.38%] [G loss: 0.438094]\n",
      "epoch:18 step:17253 [D loss: 0.217130, acc.: 70.31%] [G loss: 0.438389]\n",
      "epoch:18 step:17254 [D loss: 0.217412, acc.: 61.72%] [G loss: 0.403707]\n",
      "epoch:18 step:17255 [D loss: 0.221489, acc.: 66.41%] [G loss: 0.457188]\n",
      "epoch:18 step:17256 [D loss: 0.227185, acc.: 64.06%] [G loss: 0.422246]\n",
      "epoch:18 step:17257 [D loss: 0.230110, acc.: 64.84%] [G loss: 0.393901]\n",
      "epoch:18 step:17258 [D loss: 0.241695, acc.: 60.16%] [G loss: 0.423376]\n",
      "epoch:18 step:17259 [D loss: 0.231039, acc.: 58.59%] [G loss: 0.437098]\n",
      "epoch:18 step:17260 [D loss: 0.222269, acc.: 60.16%] [G loss: 0.432348]\n",
      "epoch:18 step:17261 [D loss: 0.229094, acc.: 57.81%] [G loss: 0.396418]\n",
      "epoch:18 step:17262 [D loss: 0.252742, acc.: 57.03%] [G loss: 0.429690]\n",
      "epoch:18 step:17263 [D loss: 0.233234, acc.: 62.50%] [G loss: 0.459378]\n",
      "epoch:18 step:17264 [D loss: 0.208997, acc.: 68.75%] [G loss: 0.466810]\n",
      "epoch:18 step:17265 [D loss: 0.230995, acc.: 64.06%] [G loss: 0.473560]\n",
      "epoch:18 step:17266 [D loss: 0.289231, acc.: 46.09%] [G loss: 0.412378]\n",
      "epoch:18 step:17267 [D loss: 0.224699, acc.: 60.16%] [G loss: 0.425503]\n",
      "epoch:18 step:17268 [D loss: 0.194796, acc.: 76.56%] [G loss: 0.419844]\n",
      "epoch:18 step:17269 [D loss: 0.209474, acc.: 64.06%] [G loss: 0.424304]\n",
      "epoch:18 step:17270 [D loss: 0.257771, acc.: 56.25%] [G loss: 0.419528]\n",
      "epoch:18 step:17271 [D loss: 0.229656, acc.: 66.41%] [G loss: 0.477163]\n",
      "epoch:18 step:17272 [D loss: 0.202415, acc.: 66.41%] [G loss: 0.494476]\n",
      "epoch:18 step:17273 [D loss: 0.235989, acc.: 59.38%] [G loss: 0.488235]\n",
      "epoch:18 step:17274 [D loss: 0.255411, acc.: 53.91%] [G loss: 0.421264]\n",
      "epoch:18 step:17275 [D loss: 0.235867, acc.: 64.06%] [G loss: 0.387965]\n",
      "epoch:18 step:17276 [D loss: 0.238304, acc.: 57.03%] [G loss: 0.398739]\n",
      "epoch:18 step:17277 [D loss: 0.241387, acc.: 61.72%] [G loss: 0.408111]\n",
      "epoch:18 step:17278 [D loss: 0.215963, acc.: 65.62%] [G loss: 0.411921]\n",
      "epoch:18 step:17279 [D loss: 0.231226, acc.: 62.50%] [G loss: 0.407156]\n",
      "epoch:18 step:17280 [D loss: 0.227626, acc.: 63.28%] [G loss: 0.417348]\n",
      "epoch:18 step:17281 [D loss: 0.213428, acc.: 62.50%] [G loss: 0.421725]\n",
      "epoch:18 step:17282 [D loss: 0.209223, acc.: 66.41%] [G loss: 0.454328]\n",
      "epoch:18 step:17283 [D loss: 0.242075, acc.: 60.16%] [G loss: 0.475515]\n",
      "epoch:18 step:17284 [D loss: 0.246452, acc.: 53.12%] [G loss: 0.419880]\n",
      "epoch:18 step:17285 [D loss: 0.227728, acc.: 64.84%] [G loss: 0.429442]\n",
      "epoch:18 step:17286 [D loss: 0.244664, acc.: 60.16%] [G loss: 0.407490]\n",
      "epoch:18 step:17287 [D loss: 0.250932, acc.: 58.59%] [G loss: 0.414981]\n",
      "epoch:18 step:17288 [D loss: 0.257151, acc.: 53.12%] [G loss: 0.389773]\n",
      "epoch:18 step:17289 [D loss: 0.214819, acc.: 66.41%] [G loss: 0.423797]\n",
      "epoch:18 step:17290 [D loss: 0.252811, acc.: 56.25%] [G loss: 0.375769]\n",
      "epoch:18 step:17291 [D loss: 0.222876, acc.: 62.50%] [G loss: 0.404983]\n",
      "epoch:18 step:17292 [D loss: 0.237634, acc.: 60.16%] [G loss: 0.404461]\n",
      "epoch:18 step:17293 [D loss: 0.198270, acc.: 67.19%] [G loss: 0.426046]\n",
      "epoch:18 step:17294 [D loss: 0.217361, acc.: 66.41%] [G loss: 0.435590]\n",
      "epoch:18 step:17295 [D loss: 0.189699, acc.: 75.00%] [G loss: 0.492055]\n",
      "epoch:18 step:17296 [D loss: 0.215248, acc.: 63.28%] [G loss: 0.469606]\n",
      "epoch:18 step:17297 [D loss: 0.232018, acc.: 59.38%] [G loss: 0.472319]\n",
      "epoch:18 step:17298 [D loss: 0.260750, acc.: 56.25%] [G loss: 0.422415]\n",
      "epoch:18 step:17299 [D loss: 0.228743, acc.: 60.16%] [G loss: 0.404054]\n",
      "epoch:18 step:17300 [D loss: 0.213911, acc.: 66.41%] [G loss: 0.414408]\n",
      "epoch:18 step:17301 [D loss: 0.230047, acc.: 61.72%] [G loss: 0.409718]\n",
      "epoch:18 step:17302 [D loss: 0.210980, acc.: 64.06%] [G loss: 0.438026]\n",
      "epoch:18 step:17303 [D loss: 0.292065, acc.: 46.09%] [G loss: 0.406538]\n",
      "epoch:18 step:17304 [D loss: 0.233082, acc.: 60.16%] [G loss: 0.470720]\n",
      "epoch:18 step:17305 [D loss: 0.217741, acc.: 65.62%] [G loss: 0.451651]\n",
      "epoch:18 step:17306 [D loss: 0.217467, acc.: 70.31%] [G loss: 0.435159]\n",
      "epoch:18 step:17307 [D loss: 0.221187, acc.: 64.06%] [G loss: 0.460106]\n",
      "epoch:18 step:17308 [D loss: 0.230564, acc.: 60.16%] [G loss: 0.380048]\n",
      "epoch:18 step:17309 [D loss: 0.244836, acc.: 57.81%] [G loss: 0.427489]\n",
      "epoch:18 step:17310 [D loss: 0.221035, acc.: 66.41%] [G loss: 0.449888]\n",
      "epoch:18 step:17311 [D loss: 0.221656, acc.: 65.62%] [G loss: 0.409473]\n",
      "epoch:18 step:17312 [D loss: 0.291674, acc.: 52.34%] [G loss: 0.416927]\n",
      "epoch:18 step:17313 [D loss: 0.201436, acc.: 66.41%] [G loss: 0.509699]\n",
      "epoch:18 step:17314 [D loss: 0.264409, acc.: 57.81%] [G loss: 0.417133]\n",
      "epoch:18 step:17315 [D loss: 0.215239, acc.: 69.53%] [G loss: 0.410944]\n",
      "epoch:18 step:17316 [D loss: 0.219380, acc.: 64.06%] [G loss: 0.414891]\n",
      "epoch:18 step:17317 [D loss: 0.224080, acc.: 62.50%] [G loss: 0.426159]\n",
      "epoch:18 step:17318 [D loss: 0.212795, acc.: 66.41%] [G loss: 0.481852]\n",
      "epoch:18 step:17319 [D loss: 0.206221, acc.: 64.84%] [G loss: 0.411990]\n",
      "epoch:18 step:17320 [D loss: 0.223974, acc.: 68.75%] [G loss: 0.419003]\n",
      "epoch:18 step:17321 [D loss: 0.239219, acc.: 61.72%] [G loss: 0.429543]\n",
      "epoch:18 step:17322 [D loss: 0.230975, acc.: 63.28%] [G loss: 0.480184]\n",
      "epoch:18 step:17323 [D loss: 0.238561, acc.: 57.03%] [G loss: 0.439607]\n",
      "epoch:18 step:17324 [D loss: 0.263221, acc.: 53.12%] [G loss: 0.420200]\n",
      "epoch:18 step:17325 [D loss: 0.249505, acc.: 57.03%] [G loss: 0.434250]\n",
      "epoch:18 step:17326 [D loss: 0.234419, acc.: 58.59%] [G loss: 0.388919]\n",
      "epoch:18 step:17327 [D loss: 0.241950, acc.: 58.59%] [G loss: 0.408164]\n",
      "epoch:18 step:17328 [D loss: 0.219186, acc.: 69.53%] [G loss: 0.417028]\n",
      "epoch:18 step:17329 [D loss: 0.255347, acc.: 55.47%] [G loss: 0.364574]\n",
      "epoch:18 step:17330 [D loss: 0.220193, acc.: 65.62%] [G loss: 0.429395]\n",
      "epoch:18 step:17331 [D loss: 0.220328, acc.: 61.72%] [G loss: 0.442013]\n",
      "epoch:18 step:17332 [D loss: 0.219443, acc.: 58.59%] [G loss: 0.417665]\n",
      "epoch:18 step:17333 [D loss: 0.214545, acc.: 67.19%] [G loss: 0.417699]\n",
      "epoch:18 step:17334 [D loss: 0.223451, acc.: 60.16%] [G loss: 0.453762]\n",
      "epoch:18 step:17335 [D loss: 0.197080, acc.: 69.53%] [G loss: 0.521744]\n",
      "epoch:18 step:17336 [D loss: 0.215371, acc.: 65.62%] [G loss: 0.458526]\n",
      "epoch:18 step:17337 [D loss: 0.189868, acc.: 73.44%] [G loss: 0.501898]\n",
      "epoch:18 step:17338 [D loss: 0.241391, acc.: 57.81%] [G loss: 0.488511]\n",
      "epoch:18 step:17339 [D loss: 0.278692, acc.: 47.66%] [G loss: 0.414364]\n",
      "epoch:18 step:17340 [D loss: 0.196336, acc.: 72.66%] [G loss: 0.474574]\n",
      "epoch:18 step:17341 [D loss: 0.227600, acc.: 63.28%] [G loss: 0.474534]\n",
      "epoch:18 step:17342 [D loss: 0.223550, acc.: 64.84%] [G loss: 0.488578]\n",
      "epoch:18 step:17343 [D loss: 0.249031, acc.: 56.25%] [G loss: 0.418611]\n",
      "epoch:18 step:17344 [D loss: 0.251847, acc.: 56.25%] [G loss: 0.374712]\n",
      "epoch:18 step:17345 [D loss: 0.224754, acc.: 60.94%] [G loss: 0.411377]\n",
      "epoch:18 step:17346 [D loss: 0.200955, acc.: 71.88%] [G loss: 0.426572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17347 [D loss: 0.187523, acc.: 77.34%] [G loss: 0.446358]\n",
      "epoch:18 step:17348 [D loss: 0.272067, acc.: 48.44%] [G loss: 0.410298]\n",
      "epoch:18 step:17349 [D loss: 0.218207, acc.: 64.06%] [G loss: 0.446354]\n",
      "epoch:18 step:17350 [D loss: 0.219472, acc.: 64.06%] [G loss: 0.418074]\n",
      "epoch:18 step:17351 [D loss: 0.239636, acc.: 63.28%] [G loss: 0.475941]\n",
      "epoch:18 step:17352 [D loss: 0.247174, acc.: 57.81%] [G loss: 0.453748]\n",
      "epoch:18 step:17353 [D loss: 0.242115, acc.: 59.38%] [G loss: 0.436391]\n",
      "epoch:18 step:17354 [D loss: 0.222510, acc.: 61.72%] [G loss: 0.461051]\n",
      "epoch:18 step:17355 [D loss: 0.228555, acc.: 68.75%] [G loss: 0.445947]\n",
      "epoch:18 step:17356 [D loss: 0.233014, acc.: 54.69%] [G loss: 0.432923]\n",
      "epoch:18 step:17357 [D loss: 0.225527, acc.: 67.19%] [G loss: 0.427113]\n",
      "epoch:18 step:17358 [D loss: 0.219001, acc.: 64.84%] [G loss: 0.435149]\n",
      "epoch:18 step:17359 [D loss: 0.237564, acc.: 60.94%] [G loss: 0.392048]\n",
      "epoch:18 step:17360 [D loss: 0.218286, acc.: 67.19%] [G loss: 0.468918]\n",
      "epoch:18 step:17361 [D loss: 0.189017, acc.: 73.44%] [G loss: 0.466316]\n",
      "epoch:18 step:17362 [D loss: 0.204927, acc.: 72.66%] [G loss: 0.511680]\n",
      "epoch:18 step:17363 [D loss: 0.205784, acc.: 67.19%] [G loss: 0.479707]\n",
      "epoch:18 step:17364 [D loss: 0.224953, acc.: 66.41%] [G loss: 0.465010]\n",
      "epoch:18 step:17365 [D loss: 0.205264, acc.: 67.19%] [G loss: 0.478009]\n",
      "epoch:18 step:17366 [D loss: 0.254506, acc.: 57.81%] [G loss: 0.449536]\n",
      "epoch:18 step:17367 [D loss: 0.263762, acc.: 53.12%] [G loss: 0.448970]\n",
      "epoch:18 step:17368 [D loss: 0.239436, acc.: 52.34%] [G loss: 0.427818]\n",
      "epoch:18 step:17369 [D loss: 0.227563, acc.: 67.19%] [G loss: 0.383536]\n",
      "epoch:18 step:17370 [D loss: 0.196093, acc.: 72.66%] [G loss: 0.425266]\n",
      "epoch:18 step:17371 [D loss: 0.194282, acc.: 71.88%] [G loss: 0.450905]\n",
      "epoch:18 step:17372 [D loss: 0.210940, acc.: 63.28%] [G loss: 0.458235]\n",
      "epoch:18 step:17373 [D loss: 0.218099, acc.: 60.16%] [G loss: 0.496491]\n",
      "epoch:18 step:17374 [D loss: 0.180523, acc.: 75.78%] [G loss: 0.489445]\n",
      "epoch:18 step:17375 [D loss: 0.259662, acc.: 57.81%] [G loss: 0.408667]\n",
      "epoch:18 step:17376 [D loss: 0.250893, acc.: 56.25%] [G loss: 0.420507]\n",
      "epoch:18 step:17377 [D loss: 0.218064, acc.: 62.50%] [G loss: 0.411824]\n",
      "epoch:18 step:17378 [D loss: 0.254422, acc.: 57.81%] [G loss: 0.415372]\n",
      "epoch:18 step:17379 [D loss: 0.191803, acc.: 72.66%] [G loss: 0.454775]\n",
      "epoch:18 step:17380 [D loss: 0.228776, acc.: 59.38%] [G loss: 0.430752]\n",
      "epoch:18 step:17381 [D loss: 0.211908, acc.: 67.97%] [G loss: 0.434666]\n",
      "epoch:18 step:17382 [D loss: 0.200815, acc.: 69.53%] [G loss: 0.496067]\n",
      "epoch:18 step:17383 [D loss: 0.225484, acc.: 61.72%] [G loss: 0.449035]\n",
      "epoch:18 step:17384 [D loss: 0.238526, acc.: 58.59%] [G loss: 0.431298]\n",
      "epoch:18 step:17385 [D loss: 0.207543, acc.: 66.41%] [G loss: 0.440993]\n",
      "epoch:18 step:17386 [D loss: 0.219141, acc.: 67.19%] [G loss: 0.451722]\n",
      "epoch:18 step:17387 [D loss: 0.214547, acc.: 67.97%] [G loss: 0.416708]\n",
      "epoch:18 step:17388 [D loss: 0.205478, acc.: 69.53%] [G loss: 0.427706]\n",
      "epoch:18 step:17389 [D loss: 0.183323, acc.: 78.12%] [G loss: 0.442189]\n",
      "epoch:18 step:17390 [D loss: 0.235136, acc.: 64.06%] [G loss: 0.435308]\n",
      "epoch:18 step:17391 [D loss: 0.205835, acc.: 71.88%] [G loss: 0.447777]\n",
      "epoch:18 step:17392 [D loss: 0.200017, acc.: 69.53%] [G loss: 0.500477]\n",
      "epoch:18 step:17393 [D loss: 0.219887, acc.: 65.62%] [G loss: 0.435036]\n",
      "epoch:18 step:17394 [D loss: 0.267708, acc.: 54.69%] [G loss: 0.418151]\n",
      "epoch:18 step:17395 [D loss: 0.238204, acc.: 59.38%] [G loss: 0.424825]\n",
      "epoch:18 step:17396 [D loss: 0.204571, acc.: 65.62%] [G loss: 0.526103]\n",
      "epoch:18 step:17397 [D loss: 0.240586, acc.: 58.59%] [G loss: 0.405912]\n",
      "epoch:18 step:17398 [D loss: 0.243562, acc.: 59.38%] [G loss: 0.425155]\n",
      "epoch:18 step:17399 [D loss: 0.224710, acc.: 62.50%] [G loss: 0.474028]\n",
      "epoch:18 step:17400 [D loss: 0.210616, acc.: 66.41%] [G loss: 0.482054]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.974999\n",
      "FID: 7.068206\n",
      "0 = 11.716946984458001\n",
      "1 = 0.04426694366720772\n",
      "2 = 0.875\n",
      "3 = 0.8657000064849854\n",
      "4 = 0.8842999935150146\n",
      "5 = 0.8821071982383728\n",
      "6 = 0.8657000064849854\n",
      "7 = 5.8658653172314\n",
      "8 = 0.058519443443400015\n",
      "9 = 0.6807500123977661\n",
      "10 = 0.6747000217437744\n",
      "11 = 0.6868000030517578\n",
      "12 = 0.6829638481140137\n",
      "13 = 0.6747000217437744\n",
      "14 = 7.975062370300293\n",
      "15 = 9.60798454284668\n",
      "16 = 0.08800852298736572\n",
      "17 = 7.97499942779541\n",
      "18 = 7.068205833435059\n",
      "epoch:18 step:17401 [D loss: 0.237871, acc.: 59.38%] [G loss: 0.442412]\n",
      "epoch:18 step:17402 [D loss: 0.236454, acc.: 63.28%] [G loss: 0.412111]\n",
      "epoch:18 step:17403 [D loss: 0.234540, acc.: 60.94%] [G loss: 0.415968]\n",
      "epoch:18 step:17404 [D loss: 0.247486, acc.: 55.47%] [G loss: 0.460999]\n",
      "epoch:18 step:17405 [D loss: 0.208578, acc.: 64.06%] [G loss: 0.456924]\n",
      "epoch:18 step:17406 [D loss: 0.230519, acc.: 59.38%] [G loss: 0.417556]\n",
      "epoch:18 step:17407 [D loss: 0.232390, acc.: 62.50%] [G loss: 0.412331]\n",
      "epoch:18 step:17408 [D loss: 0.265482, acc.: 52.34%] [G loss: 0.382053]\n",
      "epoch:18 step:17409 [D loss: 0.234226, acc.: 61.72%] [G loss: 0.421081]\n",
      "epoch:18 step:17410 [D loss: 0.217724, acc.: 60.16%] [G loss: 0.415993]\n",
      "epoch:18 step:17411 [D loss: 0.227125, acc.: 60.16%] [G loss: 0.425666]\n",
      "epoch:18 step:17412 [D loss: 0.225732, acc.: 65.62%] [G loss: 0.439364]\n",
      "epoch:18 step:17413 [D loss: 0.212475, acc.: 65.62%] [G loss: 0.468091]\n",
      "epoch:18 step:17414 [D loss: 0.219069, acc.: 65.62%] [G loss: 0.464011]\n",
      "epoch:18 step:17415 [D loss: 0.210126, acc.: 69.53%] [G loss: 0.456164]\n",
      "epoch:18 step:17416 [D loss: 0.196765, acc.: 68.75%] [G loss: 0.448826]\n",
      "epoch:18 step:17417 [D loss: 0.202613, acc.: 68.75%] [G loss: 0.462366]\n",
      "epoch:18 step:17418 [D loss: 0.222019, acc.: 64.06%] [G loss: 0.461339]\n",
      "epoch:18 step:17419 [D loss: 0.232554, acc.: 57.03%] [G loss: 0.437582]\n",
      "epoch:18 step:17420 [D loss: 0.201852, acc.: 68.75%] [G loss: 0.486673]\n",
      "epoch:18 step:17421 [D loss: 0.213698, acc.: 60.94%] [G loss: 0.466472]\n",
      "epoch:18 step:17422 [D loss: 0.208296, acc.: 71.09%] [G loss: 0.486355]\n",
      "epoch:18 step:17423 [D loss: 0.204607, acc.: 67.19%] [G loss: 0.450326]\n",
      "epoch:18 step:17424 [D loss: 0.226103, acc.: 61.72%] [G loss: 0.475680]\n",
      "epoch:18 step:17425 [D loss: 0.277099, acc.: 59.38%] [G loss: 0.431999]\n",
      "epoch:18 step:17426 [D loss: 0.263507, acc.: 53.91%] [G loss: 0.395959]\n",
      "epoch:18 step:17427 [D loss: 0.205102, acc.: 67.97%] [G loss: 0.508266]\n",
      "epoch:18 step:17428 [D loss: 0.191819, acc.: 72.66%] [G loss: 0.453082]\n",
      "epoch:18 step:17429 [D loss: 0.211714, acc.: 66.41%] [G loss: 0.480629]\n",
      "epoch:18 step:17430 [D loss: 0.192874, acc.: 73.44%] [G loss: 0.485897]\n",
      "epoch:18 step:17431 [D loss: 0.257170, acc.: 56.25%] [G loss: 0.457596]\n",
      "epoch:18 step:17432 [D loss: 0.278427, acc.: 52.34%] [G loss: 0.435431]\n",
      "epoch:18 step:17433 [D loss: 0.210545, acc.: 69.53%] [G loss: 0.463942]\n",
      "epoch:18 step:17434 [D loss: 0.193361, acc.: 72.66%] [G loss: 0.494886]\n",
      "epoch:18 step:17435 [D loss: 0.281987, acc.: 46.88%] [G loss: 0.394739]\n",
      "epoch:18 step:17436 [D loss: 0.210861, acc.: 60.16%] [G loss: 0.422250]\n",
      "epoch:18 step:17437 [D loss: 0.193007, acc.: 73.44%] [G loss: 0.456166]\n",
      "epoch:18 step:17438 [D loss: 0.247901, acc.: 57.81%] [G loss: 0.384314]\n",
      "epoch:18 step:17439 [D loss: 0.202218, acc.: 66.41%] [G loss: 0.434769]\n",
      "epoch:18 step:17440 [D loss: 0.161873, acc.: 77.34%] [G loss: 0.507379]\n",
      "epoch:18 step:17441 [D loss: 0.200423, acc.: 67.19%] [G loss: 0.491495]\n",
      "epoch:18 step:17442 [D loss: 0.247859, acc.: 53.91%] [G loss: 0.467910]\n",
      "epoch:18 step:17443 [D loss: 0.234239, acc.: 58.59%] [G loss: 0.464245]\n",
      "epoch:18 step:17444 [D loss: 0.228333, acc.: 60.16%] [G loss: 0.439465]\n",
      "epoch:18 step:17445 [D loss: 0.226032, acc.: 61.72%] [G loss: 0.443043]\n",
      "epoch:18 step:17446 [D loss: 0.223381, acc.: 63.28%] [G loss: 0.437215]\n",
      "epoch:18 step:17447 [D loss: 0.222271, acc.: 65.62%] [G loss: 0.414736]\n",
      "epoch:18 step:17448 [D loss: 0.194961, acc.: 75.78%] [G loss: 0.427151]\n",
      "epoch:18 step:17449 [D loss: 0.215643, acc.: 67.97%] [G loss: 0.489805]\n",
      "epoch:18 step:17450 [D loss: 0.238296, acc.: 59.38%] [G loss: 0.438923]\n",
      "epoch:18 step:17451 [D loss: 0.222069, acc.: 67.19%] [G loss: 0.402662]\n",
      "epoch:18 step:17452 [D loss: 0.229286, acc.: 60.94%] [G loss: 0.444090]\n",
      "epoch:18 step:17453 [D loss: 0.224443, acc.: 61.72%] [G loss: 0.444083]\n",
      "epoch:18 step:17454 [D loss: 0.225409, acc.: 60.94%] [G loss: 0.410059]\n",
      "epoch:18 step:17455 [D loss: 0.202435, acc.: 69.53%] [G loss: 0.438502]\n",
      "epoch:18 step:17456 [D loss: 0.224746, acc.: 60.16%] [G loss: 0.457306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17457 [D loss: 0.249560, acc.: 56.25%] [G loss: 0.391775]\n",
      "epoch:18 step:17458 [D loss: 0.178152, acc.: 76.56%] [G loss: 0.451839]\n",
      "epoch:18 step:17459 [D loss: 0.220359, acc.: 64.84%] [G loss: 0.424783]\n",
      "epoch:18 step:17460 [D loss: 0.236113, acc.: 60.94%] [G loss: 0.416136]\n",
      "epoch:18 step:17461 [D loss: 0.220393, acc.: 64.06%] [G loss: 0.395621]\n",
      "epoch:18 step:17462 [D loss: 0.239719, acc.: 59.38%] [G loss: 0.424649]\n",
      "epoch:18 step:17463 [D loss: 0.248049, acc.: 57.03%] [G loss: 0.431986]\n",
      "epoch:18 step:17464 [D loss: 0.197432, acc.: 73.44%] [G loss: 0.458441]\n",
      "epoch:18 step:17465 [D loss: 0.226213, acc.: 67.97%] [G loss: 0.420175]\n",
      "epoch:18 step:17466 [D loss: 0.217492, acc.: 64.84%] [G loss: 0.456185]\n",
      "epoch:18 step:17467 [D loss: 0.232426, acc.: 57.81%] [G loss: 0.450150]\n",
      "epoch:18 step:17468 [D loss: 0.248611, acc.: 55.47%] [G loss: 0.459213]\n",
      "epoch:18 step:17469 [D loss: 0.217441, acc.: 67.19%] [G loss: 0.426103]\n",
      "epoch:18 step:17470 [D loss: 0.232154, acc.: 62.50%] [G loss: 0.405675]\n",
      "epoch:18 step:17471 [D loss: 0.212081, acc.: 63.28%] [G loss: 0.439934]\n",
      "epoch:18 step:17472 [D loss: 0.237976, acc.: 61.72%] [G loss: 0.431121]\n",
      "epoch:18 step:17473 [D loss: 0.218712, acc.: 62.50%] [G loss: 0.406981]\n",
      "epoch:18 step:17474 [D loss: 0.245867, acc.: 54.69%] [G loss: 0.436185]\n",
      "epoch:18 step:17475 [D loss: 0.209730, acc.: 67.19%] [G loss: 0.428059]\n",
      "epoch:18 step:17476 [D loss: 0.226262, acc.: 67.19%] [G loss: 0.414455]\n",
      "epoch:18 step:17477 [D loss: 0.219914, acc.: 65.62%] [G loss: 0.392284]\n",
      "epoch:18 step:17478 [D loss: 0.232343, acc.: 60.16%] [G loss: 0.439277]\n",
      "epoch:18 step:17479 [D loss: 0.215625, acc.: 66.41%] [G loss: 0.416913]\n",
      "epoch:18 step:17480 [D loss: 0.233770, acc.: 55.47%] [G loss: 0.407445]\n",
      "epoch:18 step:17481 [D loss: 0.268061, acc.: 48.44%] [G loss: 0.378504]\n",
      "epoch:18 step:17482 [D loss: 0.208548, acc.: 67.19%] [G loss: 0.436395]\n",
      "epoch:18 step:17483 [D loss: 0.218708, acc.: 66.41%] [G loss: 0.450334]\n",
      "epoch:18 step:17484 [D loss: 0.229192, acc.: 58.59%] [G loss: 0.460333]\n",
      "epoch:18 step:17485 [D loss: 0.259201, acc.: 56.25%] [G loss: 0.460152]\n",
      "epoch:18 step:17486 [D loss: 0.227187, acc.: 60.94%] [G loss: 0.457332]\n",
      "epoch:18 step:17487 [D loss: 0.246907, acc.: 52.34%] [G loss: 0.419242]\n",
      "epoch:18 step:17488 [D loss: 0.249776, acc.: 58.59%] [G loss: 0.442577]\n",
      "epoch:18 step:17489 [D loss: 0.212518, acc.: 69.53%] [G loss: 0.454116]\n",
      "epoch:18 step:17490 [D loss: 0.194344, acc.: 75.00%] [G loss: 0.457594]\n",
      "epoch:18 step:17491 [D loss: 0.240733, acc.: 60.16%] [G loss: 0.414212]\n",
      "epoch:18 step:17492 [D loss: 0.242294, acc.: 56.25%] [G loss: 0.440610]\n",
      "epoch:18 step:17493 [D loss: 0.216334, acc.: 65.62%] [G loss: 0.405820]\n",
      "epoch:18 step:17494 [D loss: 0.222682, acc.: 64.06%] [G loss: 0.429664]\n",
      "epoch:18 step:17495 [D loss: 0.224653, acc.: 66.41%] [G loss: 0.411812]\n",
      "epoch:18 step:17496 [D loss: 0.241513, acc.: 60.16%] [G loss: 0.402414]\n",
      "epoch:18 step:17497 [D loss: 0.210422, acc.: 66.41%] [G loss: 0.429680]\n",
      "epoch:18 step:17498 [D loss: 0.215207, acc.: 67.97%] [G loss: 0.441679]\n",
      "epoch:18 step:17499 [D loss: 0.221838, acc.: 61.72%] [G loss: 0.408528]\n",
      "epoch:18 step:17500 [D loss: 0.207343, acc.: 65.62%] [G loss: 0.433299]\n",
      "epoch:18 step:17501 [D loss: 0.187795, acc.: 74.22%] [G loss: 0.478009]\n",
      "epoch:18 step:17502 [D loss: 0.240953, acc.: 59.38%] [G loss: 0.461778]\n",
      "epoch:18 step:17503 [D loss: 0.228348, acc.: 64.06%] [G loss: 0.435451]\n",
      "epoch:18 step:17504 [D loss: 0.219926, acc.: 64.84%] [G loss: 0.472743]\n",
      "epoch:18 step:17505 [D loss: 0.216451, acc.: 67.19%] [G loss: 0.448443]\n",
      "epoch:18 step:17506 [D loss: 0.227240, acc.: 62.50%] [G loss: 0.452382]\n",
      "epoch:18 step:17507 [D loss: 0.196883, acc.: 70.31%] [G loss: 0.475340]\n",
      "epoch:18 step:17508 [D loss: 0.187532, acc.: 74.22%] [G loss: 0.476449]\n",
      "epoch:18 step:17509 [D loss: 0.222975, acc.: 58.59%] [G loss: 0.432658]\n",
      "epoch:18 step:17510 [D loss: 0.238831, acc.: 58.59%] [G loss: 0.444247]\n",
      "epoch:18 step:17511 [D loss: 0.221378, acc.: 60.94%] [G loss: 0.414030]\n",
      "epoch:18 step:17512 [D loss: 0.200992, acc.: 67.19%] [G loss: 0.449019]\n",
      "epoch:18 step:17513 [D loss: 0.184389, acc.: 74.22%] [G loss: 0.441477]\n",
      "epoch:18 step:17514 [D loss: 0.178906, acc.: 70.31%] [G loss: 0.481915]\n",
      "epoch:18 step:17515 [D loss: 0.234546, acc.: 63.28%] [G loss: 0.514724]\n",
      "epoch:18 step:17516 [D loss: 0.227840, acc.: 66.41%] [G loss: 0.472219]\n",
      "epoch:18 step:17517 [D loss: 0.212136, acc.: 62.50%] [G loss: 0.471353]\n",
      "epoch:18 step:17518 [D loss: 0.231754, acc.: 61.72%] [G loss: 0.438072]\n",
      "epoch:18 step:17519 [D loss: 0.231647, acc.: 65.62%] [G loss: 0.437824]\n",
      "epoch:18 step:17520 [D loss: 0.185368, acc.: 75.00%] [G loss: 0.460946]\n",
      "epoch:18 step:17521 [D loss: 0.260592, acc.: 51.56%] [G loss: 0.444349]\n",
      "epoch:18 step:17522 [D loss: 0.237950, acc.: 57.03%] [G loss: 0.433572]\n",
      "epoch:18 step:17523 [D loss: 0.220864, acc.: 60.16%] [G loss: 0.461117]\n",
      "epoch:18 step:17524 [D loss: 0.224728, acc.: 60.16%] [G loss: 0.429947]\n",
      "epoch:18 step:17525 [D loss: 0.240557, acc.: 60.94%] [G loss: 0.434201]\n",
      "epoch:18 step:17526 [D loss: 0.187367, acc.: 71.88%] [G loss: 0.469618]\n",
      "epoch:18 step:17527 [D loss: 0.214214, acc.: 69.53%] [G loss: 0.429791]\n",
      "epoch:18 step:17528 [D loss: 0.201171, acc.: 73.44%] [G loss: 0.452280]\n",
      "epoch:18 step:17529 [D loss: 0.218863, acc.: 64.06%] [G loss: 0.457127]\n",
      "epoch:18 step:17530 [D loss: 0.230577, acc.: 64.06%] [G loss: 0.464756]\n",
      "epoch:18 step:17531 [D loss: 0.222011, acc.: 59.38%] [G loss: 0.437211]\n",
      "epoch:18 step:17532 [D loss: 0.220322, acc.: 63.28%] [G loss: 0.451919]\n",
      "epoch:18 step:17533 [D loss: 0.225458, acc.: 60.94%] [G loss: 0.465476]\n",
      "epoch:18 step:17534 [D loss: 0.223608, acc.: 62.50%] [G loss: 0.459426]\n",
      "epoch:18 step:17535 [D loss: 0.226365, acc.: 62.50%] [G loss: 0.428990]\n",
      "epoch:18 step:17536 [D loss: 0.249303, acc.: 55.47%] [G loss: 0.392614]\n",
      "epoch:18 step:17537 [D loss: 0.229615, acc.: 63.28%] [G loss: 0.387945]\n",
      "epoch:18 step:17538 [D loss: 0.293275, acc.: 52.34%] [G loss: 0.388099]\n",
      "epoch:18 step:17539 [D loss: 0.224448, acc.: 60.16%] [G loss: 0.470476]\n",
      "epoch:18 step:17540 [D loss: 0.221628, acc.: 60.16%] [G loss: 0.448949]\n",
      "epoch:18 step:17541 [D loss: 0.234649, acc.: 57.03%] [G loss: 0.463669]\n",
      "epoch:18 step:17542 [D loss: 0.229436, acc.: 63.28%] [G loss: 0.415327]\n",
      "epoch:18 step:17543 [D loss: 0.180652, acc.: 74.22%] [G loss: 0.463088]\n",
      "epoch:18 step:17544 [D loss: 0.222422, acc.: 63.28%] [G loss: 0.408930]\n",
      "epoch:18 step:17545 [D loss: 0.195346, acc.: 68.75%] [G loss: 0.430544]\n",
      "epoch:18 step:17546 [D loss: 0.245671, acc.: 58.59%] [G loss: 0.381811]\n",
      "epoch:18 step:17547 [D loss: 0.196553, acc.: 68.75%] [G loss: 0.449357]\n",
      "epoch:18 step:17548 [D loss: 0.233397, acc.: 59.38%] [G loss: 0.443570]\n",
      "epoch:18 step:17549 [D loss: 0.242789, acc.: 56.25%] [G loss: 0.414874]\n",
      "epoch:18 step:17550 [D loss: 0.244640, acc.: 52.34%] [G loss: 0.440249]\n",
      "epoch:18 step:17551 [D loss: 0.228124, acc.: 61.72%] [G loss: 0.433076]\n",
      "epoch:18 step:17552 [D loss: 0.236943, acc.: 60.16%] [G loss: 0.405431]\n",
      "epoch:18 step:17553 [D loss: 0.226900, acc.: 57.03%] [G loss: 0.439263]\n",
      "epoch:18 step:17554 [D loss: 0.215304, acc.: 65.62%] [G loss: 0.454762]\n",
      "epoch:18 step:17555 [D loss: 0.237828, acc.: 58.59%] [G loss: 0.425908]\n",
      "epoch:18 step:17556 [D loss: 0.195987, acc.: 70.31%] [G loss: 0.478460]\n",
      "epoch:18 step:17557 [D loss: 0.203631, acc.: 69.53%] [G loss: 0.498727]\n",
      "epoch:18 step:17558 [D loss: 0.198743, acc.: 69.53%] [G loss: 0.516822]\n",
      "epoch:18 step:17559 [D loss: 0.193001, acc.: 65.62%] [G loss: 0.492509]\n",
      "epoch:18 step:17560 [D loss: 0.187912, acc.: 70.31%] [G loss: 0.483373]\n",
      "epoch:18 step:17561 [D loss: 0.206153, acc.: 67.19%] [G loss: 0.456491]\n",
      "epoch:18 step:17562 [D loss: 0.268189, acc.: 57.03%] [G loss: 0.438986]\n",
      "epoch:18 step:17563 [D loss: 0.219921, acc.: 64.84%] [G loss: 0.470648]\n",
      "epoch:18 step:17564 [D loss: 0.216780, acc.: 66.41%] [G loss: 0.433655]\n",
      "epoch:18 step:17565 [D loss: 0.197230, acc.: 75.78%] [G loss: 0.445221]\n",
      "epoch:18 step:17566 [D loss: 0.196074, acc.: 66.41%] [G loss: 0.459943]\n",
      "epoch:18 step:17567 [D loss: 0.210950, acc.: 64.06%] [G loss: 0.466496]\n",
      "epoch:18 step:17568 [D loss: 0.239360, acc.: 59.38%] [G loss: 0.453381]\n",
      "epoch:18 step:17569 [D loss: 0.246726, acc.: 48.44%] [G loss: 0.405226]\n",
      "epoch:18 step:17570 [D loss: 0.237391, acc.: 58.59%] [G loss: 0.423090]\n",
      "epoch:18 step:17571 [D loss: 0.249680, acc.: 53.12%] [G loss: 0.441574]\n",
      "epoch:18 step:17572 [D loss: 0.219213, acc.: 68.75%] [G loss: 0.421055]\n",
      "epoch:18 step:17573 [D loss: 0.219334, acc.: 64.84%] [G loss: 0.457433]\n",
      "epoch:18 step:17574 [D loss: 0.198425, acc.: 72.66%] [G loss: 0.438732]\n",
      "epoch:18 step:17575 [D loss: 0.189869, acc.: 71.88%] [G loss: 0.490959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17576 [D loss: 0.280395, acc.: 53.91%] [G loss: 0.436313]\n",
      "epoch:18 step:17577 [D loss: 0.223410, acc.: 61.72%] [G loss: 0.420042]\n",
      "epoch:18 step:17578 [D loss: 0.216348, acc.: 63.28%] [G loss: 0.440739]\n",
      "epoch:18 step:17579 [D loss: 0.219714, acc.: 60.94%] [G loss: 0.430225]\n",
      "epoch:18 step:17580 [D loss: 0.253110, acc.: 57.03%] [G loss: 0.397829]\n",
      "epoch:18 step:17581 [D loss: 0.260130, acc.: 52.34%] [G loss: 0.408164]\n",
      "epoch:18 step:17582 [D loss: 0.246944, acc.: 53.91%] [G loss: 0.383126]\n",
      "epoch:18 step:17583 [D loss: 0.234853, acc.: 60.16%] [G loss: 0.424643]\n",
      "epoch:18 step:17584 [D loss: 0.242846, acc.: 60.16%] [G loss: 0.430123]\n",
      "epoch:18 step:17585 [D loss: 0.186360, acc.: 71.88%] [G loss: 0.489560]\n",
      "epoch:18 step:17586 [D loss: 0.229903, acc.: 60.94%] [G loss: 0.507609]\n",
      "epoch:18 step:17587 [D loss: 0.247227, acc.: 55.47%] [G loss: 0.435180]\n",
      "epoch:18 step:17588 [D loss: 0.242572, acc.: 59.38%] [G loss: 0.433947]\n",
      "epoch:18 step:17589 [D loss: 0.227420, acc.: 59.38%] [G loss: 0.416289]\n",
      "epoch:18 step:17590 [D loss: 0.202450, acc.: 70.31%] [G loss: 0.413365]\n",
      "epoch:18 step:17591 [D loss: 0.232360, acc.: 65.62%] [G loss: 0.440897]\n",
      "epoch:18 step:17592 [D loss: 0.202282, acc.: 65.62%] [G loss: 0.456105]\n",
      "epoch:18 step:17593 [D loss: 0.225755, acc.: 65.62%] [G loss: 0.423266]\n",
      "epoch:18 step:17594 [D loss: 0.226179, acc.: 64.06%] [G loss: 0.410029]\n",
      "epoch:18 step:17595 [D loss: 0.224673, acc.: 60.94%] [G loss: 0.416900]\n",
      "epoch:18 step:17596 [D loss: 0.191814, acc.: 71.88%] [G loss: 0.441300]\n",
      "epoch:18 step:17597 [D loss: 0.203770, acc.: 65.62%] [G loss: 0.494700]\n",
      "epoch:18 step:17598 [D loss: 0.223518, acc.: 64.06%] [G loss: 0.439491]\n",
      "epoch:18 step:17599 [D loss: 0.217578, acc.: 62.50%] [G loss: 0.426536]\n",
      "epoch:18 step:17600 [D loss: 0.228542, acc.: 63.28%] [G loss: 0.462086]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 8.098065\n",
      "FID: 5.992418\n",
      "0 = 11.662430582380328\n",
      "1 = 0.04518514691949369\n",
      "2 = 0.8608499765396118\n",
      "3 = 0.8485000133514404\n",
      "4 = 0.873199999332428\n",
      "5 = 0.869988739490509\n",
      "6 = 0.8485000133514404\n",
      "7 = 5.662741556990144\n",
      "8 = 0.05486160900464117\n",
      "9 = 0.6832000017166138\n",
      "10 = 0.6825000047683716\n",
      "11 = 0.683899998664856\n",
      "12 = 0.6834568381309509\n",
      "13 = 0.6825000047683716\n",
      "14 = 8.098127365112305\n",
      "15 = 9.611652374267578\n",
      "16 = 0.0823633074760437\n",
      "17 = 8.098065376281738\n",
      "18 = 5.99241828918457\n",
      "epoch:18 step:17601 [D loss: 0.237453, acc.: 57.81%] [G loss: 0.429389]\n",
      "epoch:18 step:17602 [D loss: 0.222244, acc.: 67.97%] [G loss: 0.456024]\n",
      "epoch:18 step:17603 [D loss: 0.217568, acc.: 64.06%] [G loss: 0.403782]\n",
      "epoch:18 step:17604 [D loss: 0.242130, acc.: 57.81%] [G loss: 0.391144]\n",
      "epoch:18 step:17605 [D loss: 0.236307, acc.: 61.72%] [G loss: 0.433568]\n",
      "epoch:18 step:17606 [D loss: 0.237241, acc.: 60.94%] [G loss: 0.432340]\n",
      "epoch:18 step:17607 [D loss: 0.230189, acc.: 61.72%] [G loss: 0.418435]\n",
      "epoch:18 step:17608 [D loss: 0.225931, acc.: 63.28%] [G loss: 0.402365]\n",
      "epoch:18 step:17609 [D loss: 0.207563, acc.: 65.62%] [G loss: 0.486265]\n",
      "epoch:18 step:17610 [D loss: 0.216714, acc.: 65.62%] [G loss: 0.443280]\n",
      "epoch:18 step:17611 [D loss: 0.261911, acc.: 51.56%] [G loss: 0.398999]\n",
      "epoch:18 step:17612 [D loss: 0.203994, acc.: 63.28%] [G loss: 0.454110]\n",
      "epoch:18 step:17613 [D loss: 0.199463, acc.: 67.97%] [G loss: 0.440988]\n",
      "epoch:18 step:17614 [D loss: 0.246805, acc.: 53.91%] [G loss: 0.406852]\n",
      "epoch:18 step:17615 [D loss: 0.216047, acc.: 61.72%] [G loss: 0.454060]\n",
      "epoch:18 step:17616 [D loss: 0.231684, acc.: 61.72%] [G loss: 0.404161]\n",
      "epoch:18 step:17617 [D loss: 0.221290, acc.: 64.06%] [G loss: 0.451633]\n",
      "epoch:18 step:17618 [D loss: 0.238155, acc.: 57.81%] [G loss: 0.414689]\n",
      "epoch:18 step:17619 [D loss: 0.207297, acc.: 65.62%] [G loss: 0.453724]\n",
      "epoch:18 step:17620 [D loss: 0.216000, acc.: 67.97%] [G loss: 0.436417]\n",
      "epoch:18 step:17621 [D loss: 0.227688, acc.: 65.62%] [G loss: 0.432692]\n",
      "epoch:18 step:17622 [D loss: 0.222604, acc.: 67.97%] [G loss: 0.452758]\n",
      "epoch:18 step:17623 [D loss: 0.234556, acc.: 61.72%] [G loss: 0.435157]\n",
      "epoch:18 step:17624 [D loss: 0.230590, acc.: 64.84%] [G loss: 0.408714]\n",
      "epoch:18 step:17625 [D loss: 0.229989, acc.: 61.72%] [G loss: 0.420300]\n",
      "epoch:18 step:17626 [D loss: 0.270034, acc.: 50.78%] [G loss: 0.389654]\n",
      "epoch:18 step:17627 [D loss: 0.234506, acc.: 61.72%] [G loss: 0.419694]\n",
      "epoch:18 step:17628 [D loss: 0.234198, acc.: 59.38%] [G loss: 0.427500]\n",
      "epoch:18 step:17629 [D loss: 0.224024, acc.: 60.16%] [G loss: 0.421716]\n",
      "epoch:18 step:17630 [D loss: 0.257562, acc.: 51.56%] [G loss: 0.417065]\n",
      "epoch:18 step:17631 [D loss: 0.248014, acc.: 59.38%] [G loss: 0.410887]\n",
      "epoch:18 step:17632 [D loss: 0.209632, acc.: 71.09%] [G loss: 0.425624]\n",
      "epoch:18 step:17633 [D loss: 0.214087, acc.: 66.41%] [G loss: 0.427911]\n",
      "epoch:18 step:17634 [D loss: 0.239823, acc.: 58.59%] [G loss: 0.447700]\n",
      "epoch:18 step:17635 [D loss: 0.203160, acc.: 74.22%] [G loss: 0.464217]\n",
      "epoch:18 step:17636 [D loss: 0.228493, acc.: 62.50%] [G loss: 0.457218]\n",
      "epoch:18 step:17637 [D loss: 0.238960, acc.: 58.59%] [G loss: 0.452635]\n",
      "epoch:18 step:17638 [D loss: 0.249582, acc.: 56.25%] [G loss: 0.404769]\n",
      "epoch:18 step:17639 [D loss: 0.217202, acc.: 65.62%] [G loss: 0.398180]\n",
      "epoch:18 step:17640 [D loss: 0.214752, acc.: 64.84%] [G loss: 0.438862]\n",
      "epoch:18 step:17641 [D loss: 0.216602, acc.: 67.19%] [G loss: 0.415711]\n",
      "epoch:18 step:17642 [D loss: 0.221015, acc.: 65.62%] [G loss: 0.458181]\n",
      "epoch:18 step:17643 [D loss: 0.215964, acc.: 66.41%] [G loss: 0.445995]\n",
      "epoch:18 step:17644 [D loss: 0.210038, acc.: 68.75%] [G loss: 0.484801]\n",
      "epoch:18 step:17645 [D loss: 0.229340, acc.: 65.62%] [G loss: 0.423878]\n",
      "epoch:18 step:17646 [D loss: 0.206098, acc.: 67.97%] [G loss: 0.441092]\n",
      "epoch:18 step:17647 [D loss: 0.200428, acc.: 73.44%] [G loss: 0.474752]\n",
      "epoch:18 step:17648 [D loss: 0.186323, acc.: 71.09%] [G loss: 0.509030]\n",
      "epoch:18 step:17649 [D loss: 0.258335, acc.: 58.59%] [G loss: 0.446426]\n",
      "epoch:18 step:17650 [D loss: 0.251687, acc.: 53.91%] [G loss: 0.408541]\n",
      "epoch:18 step:17651 [D loss: 0.234522, acc.: 64.06%] [G loss: 0.382747]\n",
      "epoch:18 step:17652 [D loss: 0.195958, acc.: 71.88%] [G loss: 0.437343]\n",
      "epoch:18 step:17653 [D loss: 0.239871, acc.: 57.81%] [G loss: 0.425132]\n",
      "epoch:18 step:17654 [D loss: 0.247628, acc.: 60.16%] [G loss: 0.406313]\n",
      "epoch:18 step:17655 [D loss: 0.207586, acc.: 68.75%] [G loss: 0.449121]\n",
      "epoch:18 step:17656 [D loss: 0.236413, acc.: 59.38%] [G loss: 0.440295]\n",
      "epoch:18 step:17657 [D loss: 0.258451, acc.: 52.34%] [G loss: 0.424125]\n",
      "epoch:18 step:17658 [D loss: 0.195040, acc.: 72.66%] [G loss: 0.443335]\n",
      "epoch:18 step:17659 [D loss: 0.219459, acc.: 63.28%] [G loss: 0.434745]\n",
      "epoch:18 step:17660 [D loss: 0.271010, acc.: 49.22%] [G loss: 0.417889]\n",
      "epoch:18 step:17661 [D loss: 0.219991, acc.: 62.50%] [G loss: 0.484632]\n",
      "epoch:18 step:17662 [D loss: 0.203464, acc.: 67.19%] [G loss: 0.427458]\n",
      "epoch:18 step:17663 [D loss: 0.238519, acc.: 62.50%] [G loss: 0.416046]\n",
      "epoch:18 step:17664 [D loss: 0.233334, acc.: 60.94%] [G loss: 0.416294]\n",
      "epoch:18 step:17665 [D loss: 0.236279, acc.: 63.28%] [G loss: 0.446298]\n",
      "epoch:18 step:17666 [D loss: 0.244066, acc.: 60.94%] [G loss: 0.431794]\n",
      "epoch:18 step:17667 [D loss: 0.218960, acc.: 64.84%] [G loss: 0.429857]\n",
      "epoch:18 step:17668 [D loss: 0.208309, acc.: 69.53%] [G loss: 0.483091]\n",
      "epoch:18 step:17669 [D loss: 0.235323, acc.: 58.59%] [G loss: 0.453272]\n",
      "epoch:18 step:17670 [D loss: 0.217319, acc.: 61.72%] [G loss: 0.420764]\n",
      "epoch:18 step:17671 [D loss: 0.223933, acc.: 67.19%] [G loss: 0.401166]\n",
      "epoch:18 step:17672 [D loss: 0.225730, acc.: 60.94%] [G loss: 0.455488]\n",
      "epoch:18 step:17673 [D loss: 0.198229, acc.: 75.00%] [G loss: 0.427447]\n",
      "epoch:18 step:17674 [D loss: 0.216823, acc.: 66.41%] [G loss: 0.433085]\n",
      "epoch:18 step:17675 [D loss: 0.251168, acc.: 56.25%] [G loss: 0.371419]\n",
      "epoch:18 step:17676 [D loss: 0.222330, acc.: 60.94%] [G loss: 0.439158]\n",
      "epoch:18 step:17677 [D loss: 0.224834, acc.: 60.94%] [G loss: 0.450581]\n",
      "epoch:18 step:17678 [D loss: 0.226668, acc.: 61.72%] [G loss: 0.455125]\n",
      "epoch:18 step:17679 [D loss: 0.223232, acc.: 62.50%] [G loss: 0.439694]\n",
      "epoch:18 step:17680 [D loss: 0.206494, acc.: 68.75%] [G loss: 0.468281]\n",
      "epoch:18 step:17681 [D loss: 0.182383, acc.: 72.66%] [G loss: 0.538332]\n",
      "epoch:18 step:17682 [D loss: 0.203959, acc.: 68.75%] [G loss: 0.471200]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17683 [D loss: 0.218903, acc.: 61.72%] [G loss: 0.465394]\n",
      "epoch:18 step:17684 [D loss: 0.262902, acc.: 55.47%] [G loss: 0.428308]\n",
      "epoch:18 step:17685 [D loss: 0.215770, acc.: 65.62%] [G loss: 0.454360]\n",
      "epoch:18 step:17686 [D loss: 0.262864, acc.: 52.34%] [G loss: 0.443949]\n",
      "epoch:18 step:17687 [D loss: 0.251740, acc.: 52.34%] [G loss: 0.429772]\n",
      "epoch:18 step:17688 [D loss: 0.215545, acc.: 63.28%] [G loss: 0.494709]\n",
      "epoch:18 step:17689 [D loss: 0.200435, acc.: 69.53%] [G loss: 0.476328]\n",
      "epoch:18 step:17690 [D loss: 0.264479, acc.: 54.69%] [G loss: 0.413297]\n",
      "epoch:18 step:17691 [D loss: 0.220516, acc.: 60.16%] [G loss: 0.422595]\n",
      "epoch:18 step:17692 [D loss: 0.209076, acc.: 66.41%] [G loss: 0.419640]\n",
      "epoch:18 step:17693 [D loss: 0.258550, acc.: 54.69%] [G loss: 0.414610]\n",
      "epoch:18 step:17694 [D loss: 0.248211, acc.: 58.59%] [G loss: 0.396506]\n",
      "epoch:18 step:17695 [D loss: 0.242792, acc.: 60.16%] [G loss: 0.407188]\n",
      "epoch:18 step:17696 [D loss: 0.199330, acc.: 68.75%] [G loss: 0.447454]\n",
      "epoch:18 step:17697 [D loss: 0.235669, acc.: 60.16%] [G loss: 0.416757]\n",
      "epoch:18 step:17698 [D loss: 0.212699, acc.: 66.41%] [G loss: 0.406581]\n",
      "epoch:18 step:17699 [D loss: 0.215133, acc.: 68.75%] [G loss: 0.444934]\n",
      "epoch:18 step:17700 [D loss: 0.224886, acc.: 58.59%] [G loss: 0.428275]\n",
      "epoch:18 step:17701 [D loss: 0.227401, acc.: 62.50%] [G loss: 0.399956]\n",
      "epoch:18 step:17702 [D loss: 0.213508, acc.: 66.41%] [G loss: 0.405182]\n",
      "epoch:18 step:17703 [D loss: 0.225599, acc.: 61.72%] [G loss: 0.405346]\n",
      "epoch:18 step:17704 [D loss: 0.207582, acc.: 68.75%] [G loss: 0.452942]\n",
      "epoch:18 step:17705 [D loss: 0.218203, acc.: 66.41%] [G loss: 0.436590]\n",
      "epoch:18 step:17706 [D loss: 0.216630, acc.: 68.75%] [G loss: 0.441522]\n",
      "epoch:18 step:17707 [D loss: 0.210265, acc.: 60.16%] [G loss: 0.407047]\n",
      "epoch:18 step:17708 [D loss: 0.204507, acc.: 70.31%] [G loss: 0.429458]\n",
      "epoch:18 step:17709 [D loss: 0.236648, acc.: 60.94%] [G loss: 0.438009]\n",
      "epoch:18 step:17710 [D loss: 0.216375, acc.: 64.06%] [G loss: 0.444159]\n",
      "epoch:18 step:17711 [D loss: 0.222455, acc.: 63.28%] [G loss: 0.466036]\n",
      "epoch:18 step:17712 [D loss: 0.237166, acc.: 55.47%] [G loss: 0.433169]\n",
      "epoch:18 step:17713 [D loss: 0.256105, acc.: 53.91%] [G loss: 0.443907]\n",
      "epoch:18 step:17714 [D loss: 0.240878, acc.: 57.03%] [G loss: 0.403655]\n",
      "epoch:18 step:17715 [D loss: 0.207180, acc.: 68.75%] [G loss: 0.434053]\n",
      "epoch:18 step:17716 [D loss: 0.250463, acc.: 60.16%] [G loss: 0.437413]\n",
      "epoch:18 step:17717 [D loss: 0.222200, acc.: 62.50%] [G loss: 0.455648]\n",
      "epoch:18 step:17718 [D loss: 0.219816, acc.: 62.50%] [G loss: 0.426979]\n",
      "epoch:18 step:17719 [D loss: 0.222724, acc.: 59.38%] [G loss: 0.446422]\n",
      "epoch:18 step:17720 [D loss: 0.247723, acc.: 61.72%] [G loss: 0.430208]\n",
      "epoch:18 step:17721 [D loss: 0.240736, acc.: 61.72%] [G loss: 0.435075]\n",
      "epoch:18 step:17722 [D loss: 0.242264, acc.: 56.25%] [G loss: 0.413560]\n",
      "epoch:18 step:17723 [D loss: 0.203605, acc.: 67.97%] [G loss: 0.458699]\n",
      "epoch:18 step:17724 [D loss: 0.270515, acc.: 50.00%] [G loss: 0.410282]\n",
      "epoch:18 step:17725 [D loss: 0.231823, acc.: 64.06%] [G loss: 0.438383]\n",
      "epoch:18 step:17726 [D loss: 0.206468, acc.: 72.66%] [G loss: 0.390599]\n",
      "epoch:18 step:17727 [D loss: 0.229791, acc.: 61.72%] [G loss: 0.449737]\n",
      "epoch:18 step:17728 [D loss: 0.241317, acc.: 56.25%] [G loss: 0.368820]\n",
      "epoch:18 step:17729 [D loss: 0.227519, acc.: 63.28%] [G loss: 0.395520]\n",
      "epoch:18 step:17730 [D loss: 0.229975, acc.: 57.81%] [G loss: 0.403436]\n",
      "epoch:18 step:17731 [D loss: 0.243647, acc.: 62.50%] [G loss: 0.399014]\n",
      "epoch:18 step:17732 [D loss: 0.224520, acc.: 63.28%] [G loss: 0.376718]\n",
      "epoch:18 step:17733 [D loss: 0.234656, acc.: 57.81%] [G loss: 0.404999]\n",
      "epoch:18 step:17734 [D loss: 0.226518, acc.: 64.06%] [G loss: 0.378829]\n",
      "epoch:18 step:17735 [D loss: 0.229731, acc.: 60.94%] [G loss: 0.412472]\n",
      "epoch:18 step:17736 [D loss: 0.214539, acc.: 62.50%] [G loss: 0.441582]\n",
      "epoch:18 step:17737 [D loss: 0.222535, acc.: 63.28%] [G loss: 0.424045]\n",
      "epoch:18 step:17738 [D loss: 0.238555, acc.: 63.28%] [G loss: 0.432339]\n",
      "epoch:18 step:17739 [D loss: 0.224402, acc.: 65.62%] [G loss: 0.428037]\n",
      "epoch:18 step:17740 [D loss: 0.218418, acc.: 61.72%] [G loss: 0.413738]\n",
      "epoch:18 step:17741 [D loss: 0.187683, acc.: 70.31%] [G loss: 0.464648]\n",
      "epoch:18 step:17742 [D loss: 0.214725, acc.: 67.97%] [G loss: 0.449823]\n",
      "epoch:18 step:17743 [D loss: 0.249007, acc.: 49.22%] [G loss: 0.441810]\n",
      "epoch:18 step:17744 [D loss: 0.224906, acc.: 64.84%] [G loss: 0.470030]\n",
      "epoch:18 step:17745 [D loss: 0.239992, acc.: 57.03%] [G loss: 0.421463]\n",
      "epoch:18 step:17746 [D loss: 0.251722, acc.: 53.91%] [G loss: 0.403302]\n",
      "epoch:18 step:17747 [D loss: 0.228550, acc.: 57.03%] [G loss: 0.415893]\n",
      "epoch:18 step:17748 [D loss: 0.206903, acc.: 67.97%] [G loss: 0.465793]\n",
      "epoch:18 step:17749 [D loss: 0.243895, acc.: 59.38%] [G loss: 0.410689]\n",
      "epoch:18 step:17750 [D loss: 0.227153, acc.: 60.94%] [G loss: 0.419339]\n",
      "epoch:18 step:17751 [D loss: 0.208475, acc.: 71.88%] [G loss: 0.459942]\n",
      "epoch:18 step:17752 [D loss: 0.226172, acc.: 66.41%] [G loss: 0.420269]\n",
      "epoch:18 step:17753 [D loss: 0.240179, acc.: 60.94%] [G loss: 0.419317]\n",
      "epoch:18 step:17754 [D loss: 0.216692, acc.: 64.84%] [G loss: 0.429686]\n",
      "epoch:18 step:17755 [D loss: 0.190567, acc.: 67.97%] [G loss: 0.434407]\n",
      "epoch:18 step:17756 [D loss: 0.195845, acc.: 68.75%] [G loss: 0.465848]\n",
      "epoch:18 step:17757 [D loss: 0.259288, acc.: 51.56%] [G loss: 0.421719]\n",
      "epoch:18 step:17758 [D loss: 0.252180, acc.: 56.25%] [G loss: 0.414783]\n",
      "epoch:18 step:17759 [D loss: 0.202269, acc.: 67.19%] [G loss: 0.431387]\n",
      "epoch:18 step:17760 [D loss: 0.215015, acc.: 64.84%] [G loss: 0.422800]\n",
      "epoch:18 step:17761 [D loss: 0.221308, acc.: 58.59%] [G loss: 0.455533]\n",
      "epoch:18 step:17762 [D loss: 0.216252, acc.: 64.06%] [G loss: 0.435189]\n",
      "epoch:18 step:17763 [D loss: 0.198110, acc.: 67.97%] [G loss: 0.469505]\n",
      "epoch:18 step:17764 [D loss: 0.209409, acc.: 66.41%] [G loss: 0.453034]\n",
      "epoch:18 step:17765 [D loss: 0.204253, acc.: 65.62%] [G loss: 0.436042]\n",
      "epoch:18 step:17766 [D loss: 0.193043, acc.: 70.31%] [G loss: 0.495019]\n",
      "epoch:18 step:17767 [D loss: 0.212465, acc.: 70.31%] [G loss: 0.463109]\n",
      "epoch:18 step:17768 [D loss: 0.232326, acc.: 66.41%] [G loss: 0.410892]\n",
      "epoch:18 step:17769 [D loss: 0.248477, acc.: 56.25%] [G loss: 0.418806]\n",
      "epoch:18 step:17770 [D loss: 0.201502, acc.: 70.31%] [G loss: 0.477127]\n",
      "epoch:18 step:17771 [D loss: 0.190996, acc.: 71.88%] [G loss: 0.477664]\n",
      "epoch:18 step:17772 [D loss: 0.215721, acc.: 66.41%] [G loss: 0.463095]\n",
      "epoch:18 step:17773 [D loss: 0.243358, acc.: 60.16%] [G loss: 0.429952]\n",
      "epoch:18 step:17774 [D loss: 0.224494, acc.: 66.41%] [G loss: 0.433303]\n",
      "epoch:18 step:17775 [D loss: 0.192067, acc.: 71.09%] [G loss: 0.449072]\n",
      "epoch:18 step:17776 [D loss: 0.231888, acc.: 56.25%] [G loss: 0.469999]\n",
      "epoch:18 step:17777 [D loss: 0.224223, acc.: 63.28%] [G loss: 0.441800]\n",
      "epoch:18 step:17778 [D loss: 0.201058, acc.: 67.19%] [G loss: 0.464888]\n",
      "epoch:18 step:17779 [D loss: 0.222071, acc.: 58.59%] [G loss: 0.469827]\n",
      "epoch:18 step:17780 [D loss: 0.211083, acc.: 63.28%] [G loss: 0.480526]\n",
      "epoch:18 step:17781 [D loss: 0.273301, acc.: 52.34%] [G loss: 0.417764]\n",
      "epoch:18 step:17782 [D loss: 0.234731, acc.: 63.28%] [G loss: 0.409695]\n",
      "epoch:18 step:17783 [D loss: 0.224840, acc.: 61.72%] [G loss: 0.429883]\n",
      "epoch:18 step:17784 [D loss: 0.168854, acc.: 77.34%] [G loss: 0.479475]\n",
      "epoch:18 step:17785 [D loss: 0.222972, acc.: 64.06%] [G loss: 0.498962]\n",
      "epoch:18 step:17786 [D loss: 0.274097, acc.: 49.22%] [G loss: 0.468556]\n",
      "epoch:18 step:17787 [D loss: 0.206996, acc.: 64.84%] [G loss: 0.470062]\n",
      "epoch:18 step:17788 [D loss: 0.263119, acc.: 50.78%] [G loss: 0.422690]\n",
      "epoch:18 step:17789 [D loss: 0.214895, acc.: 64.84%] [G loss: 0.420498]\n",
      "epoch:18 step:17790 [D loss: 0.187165, acc.: 75.00%] [G loss: 0.474557]\n",
      "epoch:18 step:17791 [D loss: 0.183484, acc.: 72.66%] [G loss: 0.483517]\n",
      "epoch:18 step:17792 [D loss: 0.163357, acc.: 78.91%] [G loss: 0.510238]\n",
      "epoch:18 step:17793 [D loss: 0.184533, acc.: 74.22%] [G loss: 0.506197]\n",
      "epoch:18 step:17794 [D loss: 0.344937, acc.: 50.00%] [G loss: 0.453508]\n",
      "epoch:18 step:17795 [D loss: 0.218935, acc.: 63.28%] [G loss: 0.563102]\n",
      "epoch:18 step:17796 [D loss: 0.202203, acc.: 66.41%] [G loss: 0.521739]\n",
      "epoch:18 step:17797 [D loss: 0.227525, acc.: 64.84%] [G loss: 0.439705]\n",
      "epoch:18 step:17798 [D loss: 0.281264, acc.: 52.34%] [G loss: 0.374851]\n",
      "epoch:18 step:17799 [D loss: 0.223720, acc.: 63.28%] [G loss: 0.410076]\n",
      "epoch:18 step:17800 [D loss: 0.207722, acc.: 69.53%] [G loss: 0.510005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 8.105740\n",
      "FID: 8.223183\n",
      "0 = 11.789068194985385\n",
      "1 = 0.04871569086420671\n",
      "2 = 0.8714500069618225\n",
      "3 = 0.8683000206947327\n",
      "4 = 0.8745999932289124\n",
      "5 = 0.8738049864768982\n",
      "6 = 0.8683000206947327\n",
      "7 = 5.90080263830423\n",
      "8 = 0.05958932041774869\n",
      "9 = 0.6863999962806702\n",
      "10 = 0.6873000264167786\n",
      "11 = 0.6855000257492065\n",
      "12 = 0.6860650777816772\n",
      "13 = 0.6873000264167786\n",
      "14 = 8.105809211730957\n",
      "15 = 9.428533554077148\n",
      "16 = 0.13652952015399933\n",
      "17 = 8.10573959350586\n",
      "18 = 8.223182678222656\n",
      "epoch:18 step:17801 [D loss: 0.213177, acc.: 64.84%] [G loss: 0.487318]\n",
      "epoch:18 step:17802 [D loss: 0.189531, acc.: 74.22%] [G loss: 0.527963]\n",
      "epoch:18 step:17803 [D loss: 0.206746, acc.: 67.97%] [G loss: 0.569824]\n",
      "epoch:19 step:17804 [D loss: 0.229644, acc.: 63.28%] [G loss: 0.486559]\n",
      "epoch:19 step:17805 [D loss: 0.240760, acc.: 57.03%] [G loss: 0.421147]\n",
      "epoch:19 step:17806 [D loss: 0.259318, acc.: 60.94%] [G loss: 0.427263]\n",
      "epoch:19 step:17807 [D loss: 0.257822, acc.: 57.81%] [G loss: 0.469980]\n",
      "epoch:19 step:17808 [D loss: 0.257693, acc.: 55.47%] [G loss: 0.420648]\n",
      "epoch:19 step:17809 [D loss: 0.221398, acc.: 60.16%] [G loss: 0.442228]\n",
      "epoch:19 step:17810 [D loss: 0.239235, acc.: 58.59%] [G loss: 0.423990]\n",
      "epoch:19 step:17811 [D loss: 0.245063, acc.: 54.69%] [G loss: 0.423768]\n",
      "epoch:19 step:17812 [D loss: 0.204549, acc.: 65.62%] [G loss: 0.434983]\n",
      "epoch:19 step:17813 [D loss: 0.200731, acc.: 67.19%] [G loss: 0.448378]\n",
      "epoch:19 step:17814 [D loss: 0.216377, acc.: 65.62%] [G loss: 0.431298]\n",
      "epoch:19 step:17815 [D loss: 0.206875, acc.: 67.97%] [G loss: 0.443764]\n",
      "epoch:19 step:17816 [D loss: 0.215617, acc.: 67.97%] [G loss: 0.437303]\n",
      "epoch:19 step:17817 [D loss: 0.198489, acc.: 67.19%] [G loss: 0.457005]\n",
      "epoch:19 step:17818 [D loss: 0.206452, acc.: 64.84%] [G loss: 0.487695]\n",
      "epoch:19 step:17819 [D loss: 0.181183, acc.: 72.66%] [G loss: 0.496608]\n",
      "epoch:19 step:17820 [D loss: 0.263807, acc.: 57.81%] [G loss: 0.438971]\n",
      "epoch:19 step:17821 [D loss: 0.234995, acc.: 58.59%] [G loss: 0.450158]\n",
      "epoch:19 step:17822 [D loss: 0.246076, acc.: 62.50%] [G loss: 0.488027]\n",
      "epoch:19 step:17823 [D loss: 0.234321, acc.: 60.94%] [G loss: 0.440407]\n",
      "epoch:19 step:17824 [D loss: 0.247327, acc.: 61.72%] [G loss: 0.444281]\n",
      "epoch:19 step:17825 [D loss: 0.204478, acc.: 65.62%] [G loss: 0.496559]\n",
      "epoch:19 step:17826 [D loss: 0.226194, acc.: 62.50%] [G loss: 0.458444]\n",
      "epoch:19 step:17827 [D loss: 0.234671, acc.: 64.06%] [G loss: 0.415521]\n",
      "epoch:19 step:17828 [D loss: 0.195240, acc.: 67.97%] [G loss: 0.440211]\n",
      "epoch:19 step:17829 [D loss: 0.232126, acc.: 57.03%] [G loss: 0.397080]\n",
      "epoch:19 step:17830 [D loss: 0.230749, acc.: 60.16%] [G loss: 0.421675]\n",
      "epoch:19 step:17831 [D loss: 0.237781, acc.: 59.38%] [G loss: 0.417486]\n",
      "epoch:19 step:17832 [D loss: 0.239529, acc.: 61.72%] [G loss: 0.428610]\n",
      "epoch:19 step:17833 [D loss: 0.242583, acc.: 56.25%] [G loss: 0.431975]\n",
      "epoch:19 step:17834 [D loss: 0.234823, acc.: 55.47%] [G loss: 0.446550]\n",
      "epoch:19 step:17835 [D loss: 0.239589, acc.: 53.91%] [G loss: 0.423713]\n",
      "epoch:19 step:17836 [D loss: 0.208150, acc.: 64.06%] [G loss: 0.427391]\n",
      "epoch:19 step:17837 [D loss: 0.234737, acc.: 62.50%] [G loss: 0.440759]\n",
      "epoch:19 step:17838 [D loss: 0.211247, acc.: 70.31%] [G loss: 0.433784]\n",
      "epoch:19 step:17839 [D loss: 0.204669, acc.: 66.41%] [G loss: 0.461679]\n",
      "epoch:19 step:17840 [D loss: 0.223026, acc.: 64.06%] [G loss: 0.425709]\n",
      "epoch:19 step:17841 [D loss: 0.274041, acc.: 54.69%] [G loss: 0.381410]\n",
      "epoch:19 step:17842 [D loss: 0.220856, acc.: 61.72%] [G loss: 0.380108]\n",
      "epoch:19 step:17843 [D loss: 0.194167, acc.: 72.66%] [G loss: 0.438732]\n",
      "epoch:19 step:17844 [D loss: 0.242489, acc.: 60.16%] [G loss: 0.416415]\n",
      "epoch:19 step:17845 [D loss: 0.204491, acc.: 67.97%] [G loss: 0.462204]\n",
      "epoch:19 step:17846 [D loss: 0.206686, acc.: 70.31%] [G loss: 0.454905]\n",
      "epoch:19 step:17847 [D loss: 0.237318, acc.: 60.16%] [G loss: 0.438276]\n",
      "epoch:19 step:17848 [D loss: 0.218863, acc.: 63.28%] [G loss: 0.425930]\n",
      "epoch:19 step:17849 [D loss: 0.222693, acc.: 63.28%] [G loss: 0.427240]\n",
      "epoch:19 step:17850 [D loss: 0.236563, acc.: 60.94%] [G loss: 0.414494]\n",
      "epoch:19 step:17851 [D loss: 0.203767, acc.: 70.31%] [G loss: 0.429639]\n",
      "epoch:19 step:17852 [D loss: 0.219477, acc.: 65.62%] [G loss: 0.455542]\n",
      "epoch:19 step:17853 [D loss: 0.192962, acc.: 71.88%] [G loss: 0.442565]\n",
      "epoch:19 step:17854 [D loss: 0.252402, acc.: 56.25%] [G loss: 0.434611]\n",
      "epoch:19 step:17855 [D loss: 0.229707, acc.: 63.28%] [G loss: 0.437986]\n",
      "epoch:19 step:17856 [D loss: 0.210855, acc.: 68.75%] [G loss: 0.426344]\n",
      "epoch:19 step:17857 [D loss: 0.208515, acc.: 67.97%] [G loss: 0.483107]\n",
      "epoch:19 step:17858 [D loss: 0.207799, acc.: 65.62%] [G loss: 0.473038]\n",
      "epoch:19 step:17859 [D loss: 0.232423, acc.: 62.50%] [G loss: 0.442370]\n",
      "epoch:19 step:17860 [D loss: 0.240716, acc.: 57.81%] [G loss: 0.450224]\n",
      "epoch:19 step:17861 [D loss: 0.229389, acc.: 59.38%] [G loss: 0.436636]\n",
      "epoch:19 step:17862 [D loss: 0.210335, acc.: 65.62%] [G loss: 0.385033]\n",
      "epoch:19 step:17863 [D loss: 0.229213, acc.: 59.38%] [G loss: 0.414967]\n",
      "epoch:19 step:17864 [D loss: 0.242869, acc.: 60.16%] [G loss: 0.379958]\n",
      "epoch:19 step:17865 [D loss: 0.243011, acc.: 60.94%] [G loss: 0.421037]\n",
      "epoch:19 step:17866 [D loss: 0.218849, acc.: 64.84%] [G loss: 0.440232]\n",
      "epoch:19 step:17867 [D loss: 0.241803, acc.: 63.28%] [G loss: 0.431716]\n",
      "epoch:19 step:17868 [D loss: 0.225273, acc.: 59.38%] [G loss: 0.433570]\n",
      "epoch:19 step:17869 [D loss: 0.231912, acc.: 61.72%] [G loss: 0.415919]\n",
      "epoch:19 step:17870 [D loss: 0.202823, acc.: 65.62%] [G loss: 0.449997]\n",
      "epoch:19 step:17871 [D loss: 0.233853, acc.: 60.94%] [G loss: 0.435457]\n",
      "epoch:19 step:17872 [D loss: 0.183361, acc.: 71.88%] [G loss: 0.480240]\n",
      "epoch:19 step:17873 [D loss: 0.195752, acc.: 71.88%] [G loss: 0.468878]\n",
      "epoch:19 step:17874 [D loss: 0.272532, acc.: 52.34%] [G loss: 0.421015]\n",
      "epoch:19 step:17875 [D loss: 0.225497, acc.: 58.59%] [G loss: 0.421874]\n",
      "epoch:19 step:17876 [D loss: 0.234275, acc.: 64.06%] [G loss: 0.405412]\n",
      "epoch:19 step:17877 [D loss: 0.201599, acc.: 71.88%] [G loss: 0.471424]\n",
      "epoch:19 step:17878 [D loss: 0.228592, acc.: 62.50%] [G loss: 0.436854]\n",
      "epoch:19 step:17879 [D loss: 0.205823, acc.: 68.75%] [G loss: 0.498503]\n",
      "epoch:19 step:17880 [D loss: 0.187003, acc.: 73.44%] [G loss: 0.487192]\n",
      "epoch:19 step:17881 [D loss: 0.280819, acc.: 52.34%] [G loss: 0.425988]\n",
      "epoch:19 step:17882 [D loss: 0.235127, acc.: 55.47%] [G loss: 0.423190]\n",
      "epoch:19 step:17883 [D loss: 0.233109, acc.: 59.38%] [G loss: 0.389017]\n",
      "epoch:19 step:17884 [D loss: 0.250746, acc.: 55.47%] [G loss: 0.394413]\n",
      "epoch:19 step:17885 [D loss: 0.221216, acc.: 61.72%] [G loss: 0.449466]\n",
      "epoch:19 step:17886 [D loss: 0.219056, acc.: 62.50%] [G loss: 0.424930]\n",
      "epoch:19 step:17887 [D loss: 0.213412, acc.: 64.06%] [G loss: 0.439848]\n",
      "epoch:19 step:17888 [D loss: 0.236005, acc.: 57.03%] [G loss: 0.441276]\n",
      "epoch:19 step:17889 [D loss: 0.217984, acc.: 67.97%] [G loss: 0.424885]\n",
      "epoch:19 step:17890 [D loss: 0.222380, acc.: 61.72%] [G loss: 0.411772]\n",
      "epoch:19 step:17891 [D loss: 0.218295, acc.: 64.06%] [G loss: 0.437226]\n",
      "epoch:19 step:17892 [D loss: 0.214086, acc.: 66.41%] [G loss: 0.390389]\n",
      "epoch:19 step:17893 [D loss: 0.212786, acc.: 63.28%] [G loss: 0.425930]\n",
      "epoch:19 step:17894 [D loss: 0.251464, acc.: 56.25%] [G loss: 0.437364]\n",
      "epoch:19 step:17895 [D loss: 0.219300, acc.: 61.72%] [G loss: 0.434558]\n",
      "epoch:19 step:17896 [D loss: 0.186316, acc.: 72.66%] [G loss: 0.458242]\n",
      "epoch:19 step:17897 [D loss: 0.234060, acc.: 66.41%] [G loss: 0.461213]\n",
      "epoch:19 step:17898 [D loss: 0.222810, acc.: 65.62%] [G loss: 0.460203]\n",
      "epoch:19 step:17899 [D loss: 0.234981, acc.: 61.72%] [G loss: 0.450883]\n",
      "epoch:19 step:17900 [D loss: 0.183152, acc.: 69.53%] [G loss: 0.517799]\n",
      "epoch:19 step:17901 [D loss: 0.218645, acc.: 63.28%] [G loss: 0.487713]\n",
      "epoch:19 step:17902 [D loss: 0.257024, acc.: 55.47%] [G loss: 0.442432]\n",
      "epoch:19 step:17903 [D loss: 0.184298, acc.: 75.00%] [G loss: 0.478743]\n",
      "epoch:19 step:17904 [D loss: 0.242988, acc.: 57.81%] [G loss: 0.421365]\n",
      "epoch:19 step:17905 [D loss: 0.250774, acc.: 58.59%] [G loss: 0.439551]\n",
      "epoch:19 step:17906 [D loss: 0.215213, acc.: 60.16%] [G loss: 0.421212]\n",
      "epoch:19 step:17907 [D loss: 0.243465, acc.: 61.72%] [G loss: 0.413843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:17908 [D loss: 0.243331, acc.: 57.03%] [G loss: 0.422854]\n",
      "epoch:19 step:17909 [D loss: 0.201022, acc.: 70.31%] [G loss: 0.427271]\n",
      "epoch:19 step:17910 [D loss: 0.207762, acc.: 68.75%] [G loss: 0.447456]\n",
      "epoch:19 step:17911 [D loss: 0.284418, acc.: 49.22%] [G loss: 0.461507]\n",
      "epoch:19 step:17912 [D loss: 0.263223, acc.: 50.78%] [G loss: 0.463924]\n",
      "epoch:19 step:17913 [D loss: 0.242637, acc.: 57.03%] [G loss: 0.413469]\n",
      "epoch:19 step:17914 [D loss: 0.196058, acc.: 67.97%] [G loss: 0.410514]\n",
      "epoch:19 step:17915 [D loss: 0.212587, acc.: 68.75%] [G loss: 0.433519]\n",
      "epoch:19 step:17916 [D loss: 0.222494, acc.: 60.94%] [G loss: 0.453902]\n",
      "epoch:19 step:17917 [D loss: 0.238357, acc.: 62.50%] [G loss: 0.456942]\n",
      "epoch:19 step:17918 [D loss: 0.215261, acc.: 69.53%] [G loss: 0.461035]\n",
      "epoch:19 step:17919 [D loss: 0.199395, acc.: 70.31%] [G loss: 0.499674]\n",
      "epoch:19 step:17920 [D loss: 0.187891, acc.: 72.66%] [G loss: 0.424287]\n",
      "epoch:19 step:17921 [D loss: 0.234389, acc.: 64.06%] [G loss: 0.456089]\n",
      "epoch:19 step:17922 [D loss: 0.194696, acc.: 68.75%] [G loss: 0.517208]\n",
      "epoch:19 step:17923 [D loss: 0.239053, acc.: 57.81%] [G loss: 0.498202]\n",
      "epoch:19 step:17924 [D loss: 0.229027, acc.: 57.03%] [G loss: 0.465817]\n",
      "epoch:19 step:17925 [D loss: 0.162977, acc.: 79.69%] [G loss: 0.476283]\n",
      "epoch:19 step:17926 [D loss: 0.214231, acc.: 64.84%] [G loss: 0.459557]\n",
      "epoch:19 step:17927 [D loss: 0.260583, acc.: 48.44%] [G loss: 0.431568]\n",
      "epoch:19 step:17928 [D loss: 0.242575, acc.: 60.16%] [G loss: 0.416632]\n",
      "epoch:19 step:17929 [D loss: 0.197132, acc.: 73.44%] [G loss: 0.427013]\n",
      "epoch:19 step:17930 [D loss: 0.226488, acc.: 57.03%] [G loss: 0.429016]\n",
      "epoch:19 step:17931 [D loss: 0.236737, acc.: 57.81%] [G loss: 0.432888]\n",
      "epoch:19 step:17932 [D loss: 0.213803, acc.: 64.84%] [G loss: 0.393840]\n",
      "epoch:19 step:17933 [D loss: 0.215451, acc.: 65.62%] [G loss: 0.412293]\n",
      "epoch:19 step:17934 [D loss: 0.227819, acc.: 60.16%] [G loss: 0.442859]\n",
      "epoch:19 step:17935 [D loss: 0.225761, acc.: 64.84%] [G loss: 0.436780]\n",
      "epoch:19 step:17936 [D loss: 0.260729, acc.: 49.22%] [G loss: 0.428853]\n",
      "epoch:19 step:17937 [D loss: 0.217500, acc.: 63.28%] [G loss: 0.444259]\n",
      "epoch:19 step:17938 [D loss: 0.208401, acc.: 67.19%] [G loss: 0.424404]\n",
      "epoch:19 step:17939 [D loss: 0.202637, acc.: 72.66%] [G loss: 0.479838]\n",
      "epoch:19 step:17940 [D loss: 0.254350, acc.: 62.50%] [G loss: 0.363889]\n",
      "epoch:19 step:17941 [D loss: 0.266347, acc.: 51.56%] [G loss: 0.388500]\n",
      "epoch:19 step:17942 [D loss: 0.216564, acc.: 68.75%] [G loss: 0.404702]\n",
      "epoch:19 step:17943 [D loss: 0.218037, acc.: 64.06%] [G loss: 0.410730]\n",
      "epoch:19 step:17944 [D loss: 0.229674, acc.: 63.28%] [G loss: 0.404568]\n",
      "epoch:19 step:17945 [D loss: 0.229810, acc.: 58.59%] [G loss: 0.410102]\n",
      "epoch:19 step:17946 [D loss: 0.239737, acc.: 55.47%] [G loss: 0.449656]\n",
      "epoch:19 step:17947 [D loss: 0.193444, acc.: 70.31%] [G loss: 0.441019]\n",
      "epoch:19 step:17948 [D loss: 0.232755, acc.: 60.16%] [G loss: 0.423249]\n",
      "epoch:19 step:17949 [D loss: 0.233233, acc.: 60.94%] [G loss: 0.417582]\n",
      "epoch:19 step:17950 [D loss: 0.254966, acc.: 59.38%] [G loss: 0.423304]\n",
      "epoch:19 step:17951 [D loss: 0.249698, acc.: 60.16%] [G loss: 0.401508]\n",
      "epoch:19 step:17952 [D loss: 0.208968, acc.: 67.19%] [G loss: 0.405383]\n",
      "epoch:19 step:17953 [D loss: 0.231857, acc.: 59.38%] [G loss: 0.436856]\n",
      "epoch:19 step:17954 [D loss: 0.216539, acc.: 67.97%] [G loss: 0.466525]\n",
      "epoch:19 step:17955 [D loss: 0.227724, acc.: 63.28%] [G loss: 0.437019]\n",
      "epoch:19 step:17956 [D loss: 0.247394, acc.: 53.91%] [G loss: 0.416519]\n",
      "epoch:19 step:17957 [D loss: 0.243486, acc.: 57.81%] [G loss: 0.425849]\n",
      "epoch:19 step:17958 [D loss: 0.205702, acc.: 64.06%] [G loss: 0.441037]\n",
      "epoch:19 step:17959 [D loss: 0.242685, acc.: 56.25%] [G loss: 0.371831]\n",
      "epoch:19 step:17960 [D loss: 0.222250, acc.: 64.06%] [G loss: 0.445903]\n",
      "epoch:19 step:17961 [D loss: 0.229013, acc.: 68.75%] [G loss: 0.434845]\n",
      "epoch:19 step:17962 [D loss: 0.221920, acc.: 70.31%] [G loss: 0.472152]\n",
      "epoch:19 step:17963 [D loss: 0.235088, acc.: 62.50%] [G loss: 0.434132]\n",
      "epoch:19 step:17964 [D loss: 0.233946, acc.: 57.81%] [G loss: 0.448458]\n",
      "epoch:19 step:17965 [D loss: 0.225363, acc.: 67.19%] [G loss: 0.433903]\n",
      "epoch:19 step:17966 [D loss: 0.242110, acc.: 62.50%] [G loss: 0.448569]\n",
      "epoch:19 step:17967 [D loss: 0.202882, acc.: 71.88%] [G loss: 0.419260]\n",
      "epoch:19 step:17968 [D loss: 0.214814, acc.: 67.19%] [G loss: 0.449834]\n",
      "epoch:19 step:17969 [D loss: 0.211156, acc.: 64.84%] [G loss: 0.445836]\n",
      "epoch:19 step:17970 [D loss: 0.226996, acc.: 64.84%] [G loss: 0.394180]\n",
      "epoch:19 step:17971 [D loss: 0.228153, acc.: 64.06%] [G loss: 0.423383]\n",
      "epoch:19 step:17972 [D loss: 0.244953, acc.: 53.91%] [G loss: 0.443307]\n",
      "epoch:19 step:17973 [D loss: 0.260561, acc.: 54.69%] [G loss: 0.391585]\n",
      "epoch:19 step:17974 [D loss: 0.216545, acc.: 64.84%] [G loss: 0.414635]\n",
      "epoch:19 step:17975 [D loss: 0.205551, acc.: 67.97%] [G loss: 0.461493]\n",
      "epoch:19 step:17976 [D loss: 0.201344, acc.: 72.66%] [G loss: 0.439722]\n",
      "epoch:19 step:17977 [D loss: 0.249025, acc.: 56.25%] [G loss: 0.423444]\n",
      "epoch:19 step:17978 [D loss: 0.218593, acc.: 60.94%] [G loss: 0.417407]\n",
      "epoch:19 step:17979 [D loss: 0.219524, acc.: 61.72%] [G loss: 0.425180]\n",
      "epoch:19 step:17980 [D loss: 0.222399, acc.: 59.38%] [G loss: 0.393606]\n",
      "epoch:19 step:17981 [D loss: 0.240040, acc.: 57.03%] [G loss: 0.381632]\n",
      "epoch:19 step:17982 [D loss: 0.245018, acc.: 58.59%] [G loss: 0.408521]\n",
      "epoch:19 step:17983 [D loss: 0.238120, acc.: 56.25%] [G loss: 0.430327]\n",
      "epoch:19 step:17984 [D loss: 0.224942, acc.: 64.84%] [G loss: 0.409244]\n",
      "epoch:19 step:17985 [D loss: 0.246829, acc.: 53.12%] [G loss: 0.438722]\n",
      "epoch:19 step:17986 [D loss: 0.223384, acc.: 65.62%] [G loss: 0.444723]\n",
      "epoch:19 step:17987 [D loss: 0.240372, acc.: 60.16%] [G loss: 0.419978]\n",
      "epoch:19 step:17988 [D loss: 0.233635, acc.: 57.81%] [G loss: 0.440957]\n",
      "epoch:19 step:17989 [D loss: 0.221816, acc.: 60.94%] [G loss: 0.449980]\n",
      "epoch:19 step:17990 [D loss: 0.248841, acc.: 54.69%] [G loss: 0.388645]\n",
      "epoch:19 step:17991 [D loss: 0.222448, acc.: 62.50%] [G loss: 0.401281]\n",
      "epoch:19 step:17992 [D loss: 0.229617, acc.: 60.16%] [G loss: 0.422398]\n",
      "epoch:19 step:17993 [D loss: 0.208362, acc.: 66.41%] [G loss: 0.395209]\n",
      "epoch:19 step:17994 [D loss: 0.212251, acc.: 64.06%] [G loss: 0.432317]\n",
      "epoch:19 step:17995 [D loss: 0.205632, acc.: 70.31%] [G loss: 0.414542]\n",
      "epoch:19 step:17996 [D loss: 0.216166, acc.: 66.41%] [G loss: 0.422047]\n",
      "epoch:19 step:17997 [D loss: 0.200668, acc.: 70.31%] [G loss: 0.446866]\n",
      "epoch:19 step:17998 [D loss: 0.202043, acc.: 66.41%] [G loss: 0.474125]\n",
      "epoch:19 step:17999 [D loss: 0.205039, acc.: 67.19%] [G loss: 0.473171]\n",
      "epoch:19 step:18000 [D loss: 0.226872, acc.: 64.06%] [G loss: 0.453155]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 8.094847\n",
      "FID: 5.002955\n",
      "0 = 11.831583887648579\n",
      "1 = 0.04548884311589448\n",
      "2 = 0.8654000163078308\n",
      "3 = 0.8629999756813049\n",
      "4 = 0.8677999973297119\n",
      "5 = 0.8671624064445496\n",
      "6 = 0.8629999756813049\n",
      "7 = 5.606227470955244\n",
      "8 = 0.048798819330660104\n",
      "9 = 0.6808000206947327\n",
      "10 = 0.6830000281333923\n",
      "11 = 0.678600013256073\n",
      "12 = 0.6800079941749573\n",
      "13 = 0.6830000281333923\n",
      "14 = 8.094921112060547\n",
      "15 = 9.57701301574707\n",
      "16 = 0.0963958278298378\n",
      "17 = 8.094846725463867\n",
      "18 = 5.002954959869385\n",
      "epoch:19 step:18001 [D loss: 0.215847, acc.: 69.53%] [G loss: 0.478569]\n",
      "epoch:19 step:18002 [D loss: 0.224188, acc.: 63.28%] [G loss: 0.475392]\n",
      "epoch:19 step:18003 [D loss: 0.257797, acc.: 50.00%] [G loss: 0.418760]\n",
      "epoch:19 step:18004 [D loss: 0.218705, acc.: 64.06%] [G loss: 0.484809]\n",
      "epoch:19 step:18005 [D loss: 0.236090, acc.: 60.94%] [G loss: 0.432416]\n",
      "epoch:19 step:18006 [D loss: 0.265447, acc.: 51.56%] [G loss: 0.392388]\n",
      "epoch:19 step:18007 [D loss: 0.216179, acc.: 67.19%] [G loss: 0.446107]\n",
      "epoch:19 step:18008 [D loss: 0.210999, acc.: 67.19%] [G loss: 0.440082]\n",
      "epoch:19 step:18009 [D loss: 0.199572, acc.: 70.31%] [G loss: 0.469325]\n",
      "epoch:19 step:18010 [D loss: 0.195805, acc.: 71.09%] [G loss: 0.457307]\n",
      "epoch:19 step:18011 [D loss: 0.192248, acc.: 72.66%] [G loss: 0.491465]\n",
      "epoch:19 step:18012 [D loss: 0.193364, acc.: 69.53%] [G loss: 0.479723]\n",
      "epoch:19 step:18013 [D loss: 0.271002, acc.: 50.00%] [G loss: 0.411264]\n",
      "epoch:19 step:18014 [D loss: 0.229353, acc.: 60.16%] [G loss: 0.431524]\n",
      "epoch:19 step:18015 [D loss: 0.254423, acc.: 60.94%] [G loss: 0.385291]\n",
      "epoch:19 step:18016 [D loss: 0.215257, acc.: 64.06%] [G loss: 0.469093]\n",
      "epoch:19 step:18017 [D loss: 0.262771, acc.: 53.91%] [G loss: 0.383790]\n",
      "epoch:19 step:18018 [D loss: 0.236097, acc.: 59.38%] [G loss: 0.405016]\n",
      "epoch:19 step:18019 [D loss: 0.194546, acc.: 72.66%] [G loss: 0.435846]\n",
      "epoch:19 step:18020 [D loss: 0.215695, acc.: 67.19%] [G loss: 0.420283]\n",
      "epoch:19 step:18021 [D loss: 0.192517, acc.: 71.09%] [G loss: 0.466471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18022 [D loss: 0.187898, acc.: 72.66%] [G loss: 0.479934]\n",
      "epoch:19 step:18023 [D loss: 0.290873, acc.: 57.03%] [G loss: 0.441144]\n",
      "epoch:19 step:18024 [D loss: 0.204356, acc.: 66.41%] [G loss: 0.510845]\n",
      "epoch:19 step:18025 [D loss: 0.205518, acc.: 65.62%] [G loss: 0.475459]\n",
      "epoch:19 step:18026 [D loss: 0.210846, acc.: 69.53%] [G loss: 0.481959]\n",
      "epoch:19 step:18027 [D loss: 0.254582, acc.: 58.59%] [G loss: 0.412391]\n",
      "epoch:19 step:18028 [D loss: 0.232260, acc.: 58.59%] [G loss: 0.428139]\n",
      "epoch:19 step:18029 [D loss: 0.245320, acc.: 54.69%] [G loss: 0.401345]\n",
      "epoch:19 step:18030 [D loss: 0.217651, acc.: 64.84%] [G loss: 0.427980]\n",
      "epoch:19 step:18031 [D loss: 0.247406, acc.: 55.47%] [G loss: 0.416819]\n",
      "epoch:19 step:18032 [D loss: 0.201064, acc.: 76.56%] [G loss: 0.415692]\n",
      "epoch:19 step:18033 [D loss: 0.202582, acc.: 65.62%] [G loss: 0.443276]\n",
      "epoch:19 step:18034 [D loss: 0.173341, acc.: 79.69%] [G loss: 0.486818]\n",
      "epoch:19 step:18035 [D loss: 0.172681, acc.: 74.22%] [G loss: 0.522263]\n",
      "epoch:19 step:18036 [D loss: 0.282129, acc.: 46.09%] [G loss: 0.424456]\n",
      "epoch:19 step:18037 [D loss: 0.228327, acc.: 60.94%] [G loss: 0.418174]\n",
      "epoch:19 step:18038 [D loss: 0.212534, acc.: 67.97%] [G loss: 0.430519]\n",
      "epoch:19 step:18039 [D loss: 0.206269, acc.: 71.88%] [G loss: 0.443925]\n",
      "epoch:19 step:18040 [D loss: 0.229248, acc.: 60.16%] [G loss: 0.429230]\n",
      "epoch:19 step:18041 [D loss: 0.213550, acc.: 64.06%] [G loss: 0.420562]\n",
      "epoch:19 step:18042 [D loss: 0.228326, acc.: 60.16%] [G loss: 0.432682]\n",
      "epoch:19 step:18043 [D loss: 0.222949, acc.: 61.72%] [G loss: 0.458041]\n",
      "epoch:19 step:18044 [D loss: 0.213197, acc.: 69.53%] [G loss: 0.466894]\n",
      "epoch:19 step:18045 [D loss: 0.226965, acc.: 63.28%] [G loss: 0.444768]\n",
      "epoch:19 step:18046 [D loss: 0.221537, acc.: 61.72%] [G loss: 0.431013]\n",
      "epoch:19 step:18047 [D loss: 0.211722, acc.: 67.97%] [G loss: 0.451380]\n",
      "epoch:19 step:18048 [D loss: 0.235555, acc.: 61.72%] [G loss: 0.436299]\n",
      "epoch:19 step:18049 [D loss: 0.232063, acc.: 61.72%] [G loss: 0.434505]\n",
      "epoch:19 step:18050 [D loss: 0.235903, acc.: 63.28%] [G loss: 0.431573]\n",
      "epoch:19 step:18051 [D loss: 0.218404, acc.: 66.41%] [G loss: 0.449322]\n",
      "epoch:19 step:18052 [D loss: 0.259777, acc.: 53.12%] [G loss: 0.419849]\n",
      "epoch:19 step:18053 [D loss: 0.287085, acc.: 47.66%] [G loss: 0.423968]\n",
      "epoch:19 step:18054 [D loss: 0.251485, acc.: 56.25%] [G loss: 0.465874]\n",
      "epoch:19 step:18055 [D loss: 0.233231, acc.: 58.59%] [G loss: 0.465896]\n",
      "epoch:19 step:18056 [D loss: 0.210782, acc.: 64.06%] [G loss: 0.406369]\n",
      "epoch:19 step:18057 [D loss: 0.228339, acc.: 65.62%] [G loss: 0.432426]\n",
      "epoch:19 step:18058 [D loss: 0.227195, acc.: 62.50%] [G loss: 0.424498]\n",
      "epoch:19 step:18059 [D loss: 0.230087, acc.: 60.16%] [G loss: 0.415808]\n",
      "epoch:19 step:18060 [D loss: 0.221817, acc.: 64.84%] [G loss: 0.424578]\n",
      "epoch:19 step:18061 [D loss: 0.205625, acc.: 66.41%] [G loss: 0.420100]\n",
      "epoch:19 step:18062 [D loss: 0.220474, acc.: 65.62%] [G loss: 0.452225]\n",
      "epoch:19 step:18063 [D loss: 0.228368, acc.: 56.25%] [G loss: 0.440342]\n",
      "epoch:19 step:18064 [D loss: 0.221553, acc.: 64.06%] [G loss: 0.387349]\n",
      "epoch:19 step:18065 [D loss: 0.211979, acc.: 65.62%] [G loss: 0.452207]\n",
      "epoch:19 step:18066 [D loss: 0.227787, acc.: 60.94%] [G loss: 0.467795]\n",
      "epoch:19 step:18067 [D loss: 0.235474, acc.: 60.94%] [G loss: 0.447698]\n",
      "epoch:19 step:18068 [D loss: 0.221664, acc.: 62.50%] [G loss: 0.457145]\n",
      "epoch:19 step:18069 [D loss: 0.233624, acc.: 60.94%] [G loss: 0.409603]\n",
      "epoch:19 step:18070 [D loss: 0.213467, acc.: 63.28%] [G loss: 0.435045]\n",
      "epoch:19 step:18071 [D loss: 0.219862, acc.: 68.75%] [G loss: 0.452865]\n",
      "epoch:19 step:18072 [D loss: 0.210397, acc.: 71.88%] [G loss: 0.417864]\n",
      "epoch:19 step:18073 [D loss: 0.220761, acc.: 64.06%] [G loss: 0.480512]\n",
      "epoch:19 step:18074 [D loss: 0.225349, acc.: 66.41%] [G loss: 0.484841]\n",
      "epoch:19 step:18075 [D loss: 0.208500, acc.: 68.75%] [G loss: 0.445121]\n",
      "epoch:19 step:18076 [D loss: 0.215176, acc.: 58.59%] [G loss: 0.412288]\n",
      "epoch:19 step:18077 [D loss: 0.193667, acc.: 66.41%] [G loss: 0.426903]\n",
      "epoch:19 step:18078 [D loss: 0.205253, acc.: 71.09%] [G loss: 0.424128]\n",
      "epoch:19 step:18079 [D loss: 0.206041, acc.: 71.09%] [G loss: 0.427991]\n",
      "epoch:19 step:18080 [D loss: 0.236987, acc.: 54.69%] [G loss: 0.435153]\n",
      "epoch:19 step:18081 [D loss: 0.262632, acc.: 52.34%] [G loss: 0.439185]\n",
      "epoch:19 step:18082 [D loss: 0.226797, acc.: 64.84%] [G loss: 0.439309]\n",
      "epoch:19 step:18083 [D loss: 0.196565, acc.: 74.22%] [G loss: 0.407024]\n",
      "epoch:19 step:18084 [D loss: 0.246618, acc.: 58.59%] [G loss: 0.418212]\n",
      "epoch:19 step:18085 [D loss: 0.226643, acc.: 65.62%] [G loss: 0.444224]\n",
      "epoch:19 step:18086 [D loss: 0.202432, acc.: 70.31%] [G loss: 0.444174]\n",
      "epoch:19 step:18087 [D loss: 0.218879, acc.: 60.16%] [G loss: 0.443458]\n",
      "epoch:19 step:18088 [D loss: 0.197661, acc.: 69.53%] [G loss: 0.423261]\n",
      "epoch:19 step:18089 [D loss: 0.205595, acc.: 65.62%] [G loss: 0.410587]\n",
      "epoch:19 step:18090 [D loss: 0.203952, acc.: 68.75%] [G loss: 0.409947]\n",
      "epoch:19 step:18091 [D loss: 0.215167, acc.: 68.75%] [G loss: 0.438397]\n",
      "epoch:19 step:18092 [D loss: 0.217022, acc.: 66.41%] [G loss: 0.464029]\n",
      "epoch:19 step:18093 [D loss: 0.223072, acc.: 61.72%] [G loss: 0.440791]\n",
      "epoch:19 step:18094 [D loss: 0.244740, acc.: 58.59%] [G loss: 0.409605]\n",
      "epoch:19 step:18095 [D loss: 0.229206, acc.: 59.38%] [G loss: 0.435971]\n",
      "epoch:19 step:18096 [D loss: 0.208934, acc.: 64.06%] [G loss: 0.445866]\n",
      "epoch:19 step:18097 [D loss: 0.227141, acc.: 65.62%] [G loss: 0.397886]\n",
      "epoch:19 step:18098 [D loss: 0.226003, acc.: 60.94%] [G loss: 0.428596]\n",
      "epoch:19 step:18099 [D loss: 0.210037, acc.: 65.62%] [G loss: 0.443159]\n",
      "epoch:19 step:18100 [D loss: 0.222716, acc.: 61.72%] [G loss: 0.408205]\n",
      "epoch:19 step:18101 [D loss: 0.200619, acc.: 69.53%] [G loss: 0.399152]\n",
      "epoch:19 step:18102 [D loss: 0.226099, acc.: 62.50%] [G loss: 0.437089]\n",
      "epoch:19 step:18103 [D loss: 0.208910, acc.: 65.62%] [G loss: 0.472570]\n",
      "epoch:19 step:18104 [D loss: 0.252766, acc.: 55.47%] [G loss: 0.454016]\n",
      "epoch:19 step:18105 [D loss: 0.230555, acc.: 59.38%] [G loss: 0.421445]\n",
      "epoch:19 step:18106 [D loss: 0.247823, acc.: 61.72%] [G loss: 0.464908]\n",
      "epoch:19 step:18107 [D loss: 0.236843, acc.: 57.03%] [G loss: 0.494723]\n",
      "epoch:19 step:18108 [D loss: 0.218437, acc.: 65.62%] [G loss: 0.466225]\n",
      "epoch:19 step:18109 [D loss: 0.212512, acc.: 65.62%] [G loss: 0.427396]\n",
      "epoch:19 step:18110 [D loss: 0.227105, acc.: 66.41%] [G loss: 0.450351]\n",
      "epoch:19 step:18111 [D loss: 0.212446, acc.: 65.62%] [G loss: 0.416779]\n",
      "epoch:19 step:18112 [D loss: 0.206290, acc.: 66.41%] [G loss: 0.434131]\n",
      "epoch:19 step:18113 [D loss: 0.197510, acc.: 71.09%] [G loss: 0.429924]\n",
      "epoch:19 step:18114 [D loss: 0.207771, acc.: 66.41%] [G loss: 0.407734]\n",
      "epoch:19 step:18115 [D loss: 0.196199, acc.: 68.75%] [G loss: 0.458160]\n",
      "epoch:19 step:18116 [D loss: 0.195754, acc.: 71.09%] [G loss: 0.487928]\n",
      "epoch:19 step:18117 [D loss: 0.201279, acc.: 67.97%] [G loss: 0.502920]\n",
      "epoch:19 step:18118 [D loss: 0.219285, acc.: 66.41%] [G loss: 0.495046]\n",
      "epoch:19 step:18119 [D loss: 0.253947, acc.: 57.03%] [G loss: 0.455639]\n",
      "epoch:19 step:18120 [D loss: 0.258268, acc.: 52.34%] [G loss: 0.438635]\n",
      "epoch:19 step:18121 [D loss: 0.219425, acc.: 67.19%] [G loss: 0.436712]\n",
      "epoch:19 step:18122 [D loss: 0.221094, acc.: 59.38%] [G loss: 0.402956]\n",
      "epoch:19 step:18123 [D loss: 0.213074, acc.: 68.75%] [G loss: 0.468282]\n",
      "epoch:19 step:18124 [D loss: 0.221588, acc.: 64.06%] [G loss: 0.438592]\n",
      "epoch:19 step:18125 [D loss: 0.206220, acc.: 64.84%] [G loss: 0.467668]\n",
      "epoch:19 step:18126 [D loss: 0.231617, acc.: 60.16%] [G loss: 0.443631]\n",
      "epoch:19 step:18127 [D loss: 0.218477, acc.: 65.62%] [G loss: 0.438362]\n",
      "epoch:19 step:18128 [D loss: 0.214486, acc.: 67.19%] [G loss: 0.457632]\n",
      "epoch:19 step:18129 [D loss: 0.218414, acc.: 64.84%] [G loss: 0.439165]\n",
      "epoch:19 step:18130 [D loss: 0.247725, acc.: 62.50%] [G loss: 0.445698]\n",
      "epoch:19 step:18131 [D loss: 0.204400, acc.: 65.62%] [G loss: 0.461806]\n",
      "epoch:19 step:18132 [D loss: 0.244045, acc.: 56.25%] [G loss: 0.485529]\n",
      "epoch:19 step:18133 [D loss: 0.231485, acc.: 63.28%] [G loss: 0.421868]\n",
      "epoch:19 step:18134 [D loss: 0.215488, acc.: 67.19%] [G loss: 0.442668]\n",
      "epoch:19 step:18135 [D loss: 0.199764, acc.: 68.75%] [G loss: 0.415356]\n",
      "epoch:19 step:18136 [D loss: 0.208666, acc.: 64.06%] [G loss: 0.414513]\n",
      "epoch:19 step:18137 [D loss: 0.212486, acc.: 65.62%] [G loss: 0.442392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18138 [D loss: 0.218615, acc.: 65.62%] [G loss: 0.475339]\n",
      "epoch:19 step:18139 [D loss: 0.197146, acc.: 69.53%] [G loss: 0.436126]\n",
      "epoch:19 step:18140 [D loss: 0.220715, acc.: 62.50%] [G loss: 0.444294]\n",
      "epoch:19 step:18141 [D loss: 0.232352, acc.: 62.50%] [G loss: 0.408740]\n",
      "epoch:19 step:18142 [D loss: 0.214590, acc.: 68.75%] [G loss: 0.465353]\n",
      "epoch:19 step:18143 [D loss: 0.220918, acc.: 63.28%] [G loss: 0.488090]\n",
      "epoch:19 step:18144 [D loss: 0.260042, acc.: 54.69%] [G loss: 0.457543]\n",
      "epoch:19 step:18145 [D loss: 0.257675, acc.: 54.69%] [G loss: 0.422141]\n",
      "epoch:19 step:18146 [D loss: 0.221121, acc.: 64.06%] [G loss: 0.488023]\n",
      "epoch:19 step:18147 [D loss: 0.205501, acc.: 69.53%] [G loss: 0.451815]\n",
      "epoch:19 step:18148 [D loss: 0.191390, acc.: 69.53%] [G loss: 0.466317]\n",
      "epoch:19 step:18149 [D loss: 0.188565, acc.: 71.88%] [G loss: 0.492768]\n",
      "epoch:19 step:18150 [D loss: 0.177304, acc.: 75.78%] [G loss: 0.554924]\n",
      "epoch:19 step:18151 [D loss: 0.286251, acc.: 52.34%] [G loss: 0.410337]\n",
      "epoch:19 step:18152 [D loss: 0.275501, acc.: 50.00%] [G loss: 0.382167]\n",
      "epoch:19 step:18153 [D loss: 0.211771, acc.: 66.41%] [G loss: 0.438444]\n",
      "epoch:19 step:18154 [D loss: 0.220787, acc.: 64.06%] [G loss: 0.414326]\n",
      "epoch:19 step:18155 [D loss: 0.260837, acc.: 51.56%] [G loss: 0.417192]\n",
      "epoch:19 step:18156 [D loss: 0.211388, acc.: 66.41%] [G loss: 0.466270]\n",
      "epoch:19 step:18157 [D loss: 0.194540, acc.: 68.75%] [G loss: 0.490821]\n",
      "epoch:19 step:18158 [D loss: 0.260325, acc.: 57.03%] [G loss: 0.458086]\n",
      "epoch:19 step:18159 [D loss: 0.248684, acc.: 59.38%] [G loss: 0.437186]\n",
      "epoch:19 step:18160 [D loss: 0.218056, acc.: 65.62%] [G loss: 0.381188]\n",
      "epoch:19 step:18161 [D loss: 0.191985, acc.: 69.53%] [G loss: 0.511530]\n",
      "epoch:19 step:18162 [D loss: 0.221150, acc.: 64.06%] [G loss: 0.432106]\n",
      "epoch:19 step:18163 [D loss: 0.237030, acc.: 58.59%] [G loss: 0.408800]\n",
      "epoch:19 step:18164 [D loss: 0.213185, acc.: 65.62%] [G loss: 0.453924]\n",
      "epoch:19 step:18165 [D loss: 0.232471, acc.: 59.38%] [G loss: 0.441347]\n",
      "epoch:19 step:18166 [D loss: 0.206299, acc.: 66.41%] [G loss: 0.429281]\n",
      "epoch:19 step:18167 [D loss: 0.199290, acc.: 68.75%] [G loss: 0.406288]\n",
      "epoch:19 step:18168 [D loss: 0.209237, acc.: 69.53%] [G loss: 0.448593]\n",
      "epoch:19 step:18169 [D loss: 0.217579, acc.: 67.97%] [G loss: 0.409386]\n",
      "epoch:19 step:18170 [D loss: 0.212666, acc.: 67.97%] [G loss: 0.435604]\n",
      "epoch:19 step:18171 [D loss: 0.233839, acc.: 58.59%] [G loss: 0.441044]\n",
      "epoch:19 step:18172 [D loss: 0.243354, acc.: 53.91%] [G loss: 0.445837]\n",
      "epoch:19 step:18173 [D loss: 0.217749, acc.: 67.19%] [G loss: 0.435285]\n",
      "epoch:19 step:18174 [D loss: 0.196566, acc.: 71.88%] [G loss: 0.494037]\n",
      "epoch:19 step:18175 [D loss: 0.235135, acc.: 58.59%] [G loss: 0.450348]\n",
      "epoch:19 step:18176 [D loss: 0.239592, acc.: 58.59%] [G loss: 0.435591]\n",
      "epoch:19 step:18177 [D loss: 0.194565, acc.: 68.75%] [G loss: 0.482477]\n",
      "epoch:19 step:18178 [D loss: 0.223500, acc.: 61.72%] [G loss: 0.440343]\n",
      "epoch:19 step:18179 [D loss: 0.242223, acc.: 56.25%] [G loss: 0.429966]\n",
      "epoch:19 step:18180 [D loss: 0.274434, acc.: 46.88%] [G loss: 0.401257]\n",
      "epoch:19 step:18181 [D loss: 0.245795, acc.: 58.59%] [G loss: 0.419037]\n",
      "epoch:19 step:18182 [D loss: 0.220644, acc.: 65.62%] [G loss: 0.441834]\n",
      "epoch:19 step:18183 [D loss: 0.227625, acc.: 66.41%] [G loss: 0.433588]\n",
      "epoch:19 step:18184 [D loss: 0.199922, acc.: 67.97%] [G loss: 0.447235]\n",
      "epoch:19 step:18185 [D loss: 0.219718, acc.: 64.84%] [G loss: 0.440743]\n",
      "epoch:19 step:18186 [D loss: 0.244346, acc.: 56.25%] [G loss: 0.432931]\n",
      "epoch:19 step:18187 [D loss: 0.208603, acc.: 69.53%] [G loss: 0.444559]\n",
      "epoch:19 step:18188 [D loss: 0.219295, acc.: 64.06%] [G loss: 0.442177]\n",
      "epoch:19 step:18189 [D loss: 0.233467, acc.: 64.06%] [G loss: 0.408967]\n",
      "epoch:19 step:18190 [D loss: 0.228432, acc.: 63.28%] [G loss: 0.430933]\n",
      "epoch:19 step:18191 [D loss: 0.213768, acc.: 63.28%] [G loss: 0.412265]\n",
      "epoch:19 step:18192 [D loss: 0.235182, acc.: 60.16%] [G loss: 0.405747]\n",
      "epoch:19 step:18193 [D loss: 0.230482, acc.: 63.28%] [G loss: 0.409116]\n",
      "epoch:19 step:18194 [D loss: 0.221703, acc.: 60.94%] [G loss: 0.409473]\n",
      "epoch:19 step:18195 [D loss: 0.220248, acc.: 65.62%] [G loss: 0.485926]\n",
      "epoch:19 step:18196 [D loss: 0.253630, acc.: 57.81%] [G loss: 0.419970]\n",
      "epoch:19 step:18197 [D loss: 0.223476, acc.: 57.03%] [G loss: 0.428337]\n",
      "epoch:19 step:18198 [D loss: 0.204739, acc.: 63.28%] [G loss: 0.463001]\n",
      "epoch:19 step:18199 [D loss: 0.245540, acc.: 57.81%] [G loss: 0.448781]\n",
      "epoch:19 step:18200 [D loss: 0.233896, acc.: 67.19%] [G loss: 0.448397]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 8.055018\n",
      "FID: 7.912412\n",
      "0 = 11.7428199137211\n",
      "1 = 0.04525500629192777\n",
      "2 = 0.8676000237464905\n",
      "3 = 0.8605999946594238\n",
      "4 = 0.8745999932289124\n",
      "5 = 0.8728194832801819\n",
      "6 = 0.8605999946594238\n",
      "7 = 5.845470108771332\n",
      "8 = 0.061766130633978136\n",
      "9 = 0.677049994468689\n",
      "10 = 0.6787999868392944\n",
      "11 = 0.6753000020980835\n",
      "12 = 0.6764324903488159\n",
      "13 = 0.6787999868392944\n",
      "14 = 8.05508804321289\n",
      "15 = 9.54355239868164\n",
      "16 = 0.10726238787174225\n",
      "17 = 8.055018424987793\n",
      "18 = 7.912412166595459\n",
      "epoch:19 step:18201 [D loss: 0.201933, acc.: 69.53%] [G loss: 0.446328]\n",
      "epoch:19 step:18202 [D loss: 0.180402, acc.: 74.22%] [G loss: 0.468015]\n",
      "epoch:19 step:18203 [D loss: 0.267949, acc.: 53.12%] [G loss: 0.407065]\n",
      "epoch:19 step:18204 [D loss: 0.253227, acc.: 56.25%] [G loss: 0.447005]\n",
      "epoch:19 step:18205 [D loss: 0.205310, acc.: 67.97%] [G loss: 0.448791]\n",
      "epoch:19 step:18206 [D loss: 0.236484, acc.: 60.94%] [G loss: 0.432906]\n",
      "epoch:19 step:18207 [D loss: 0.226434, acc.: 63.28%] [G loss: 0.411884]\n",
      "epoch:19 step:18208 [D loss: 0.204932, acc.: 65.62%] [G loss: 0.444929]\n",
      "epoch:19 step:18209 [D loss: 0.198082, acc.: 71.88%] [G loss: 0.502581]\n",
      "epoch:19 step:18210 [D loss: 0.230922, acc.: 57.03%] [G loss: 0.472985]\n",
      "epoch:19 step:18211 [D loss: 0.244193, acc.: 61.72%] [G loss: 0.471509]\n",
      "epoch:19 step:18212 [D loss: 0.247267, acc.: 59.38%] [G loss: 0.434300]\n",
      "epoch:19 step:18213 [D loss: 0.242441, acc.: 54.69%] [G loss: 0.430735]\n",
      "epoch:19 step:18214 [D loss: 0.248508, acc.: 60.16%] [G loss: 0.387855]\n",
      "epoch:19 step:18215 [D loss: 0.229570, acc.: 62.50%] [G loss: 0.428887]\n",
      "epoch:19 step:18216 [D loss: 0.239067, acc.: 57.81%] [G loss: 0.434740]\n",
      "epoch:19 step:18217 [D loss: 0.240640, acc.: 54.69%] [G loss: 0.417910]\n",
      "epoch:19 step:18218 [D loss: 0.207965, acc.: 67.97%] [G loss: 0.448677]\n",
      "epoch:19 step:18219 [D loss: 0.194037, acc.: 71.88%] [G loss: 0.507638]\n",
      "epoch:19 step:18220 [D loss: 0.245746, acc.: 55.47%] [G loss: 0.471538]\n",
      "epoch:19 step:18221 [D loss: 0.254094, acc.: 53.91%] [G loss: 0.391845]\n",
      "epoch:19 step:18222 [D loss: 0.251388, acc.: 58.59%] [G loss: 0.420632]\n",
      "epoch:19 step:18223 [D loss: 0.221703, acc.: 60.94%] [G loss: 0.473915]\n",
      "epoch:19 step:18224 [D loss: 0.236870, acc.: 63.28%] [G loss: 0.438473]\n",
      "epoch:19 step:18225 [D loss: 0.242541, acc.: 50.78%] [G loss: 0.412247]\n",
      "epoch:19 step:18226 [D loss: 0.211014, acc.: 64.84%] [G loss: 0.407224]\n",
      "epoch:19 step:18227 [D loss: 0.232167, acc.: 59.38%] [G loss: 0.452416]\n",
      "epoch:19 step:18228 [D loss: 0.210910, acc.: 65.62%] [G loss: 0.445915]\n",
      "epoch:19 step:18229 [D loss: 0.210442, acc.: 72.66%] [G loss: 0.411077]\n",
      "epoch:19 step:18230 [D loss: 0.216773, acc.: 61.72%] [G loss: 0.413925]\n",
      "epoch:19 step:18231 [D loss: 0.185856, acc.: 71.09%] [G loss: 0.483730]\n",
      "epoch:19 step:18232 [D loss: 0.218490, acc.: 67.19%] [G loss: 0.452038]\n",
      "epoch:19 step:18233 [D loss: 0.218468, acc.: 64.06%] [G loss: 0.450884]\n",
      "epoch:19 step:18234 [D loss: 0.238582, acc.: 56.25%] [G loss: 0.461553]\n",
      "epoch:19 step:18235 [D loss: 0.240221, acc.: 58.59%] [G loss: 0.445020]\n",
      "epoch:19 step:18236 [D loss: 0.213725, acc.: 66.41%] [G loss: 0.446691]\n",
      "epoch:19 step:18237 [D loss: 0.218085, acc.: 63.28%] [G loss: 0.433936]\n",
      "epoch:19 step:18238 [D loss: 0.244482, acc.: 55.47%] [G loss: 0.421943]\n",
      "epoch:19 step:18239 [D loss: 0.196681, acc.: 70.31%] [G loss: 0.457355]\n",
      "epoch:19 step:18240 [D loss: 0.285343, acc.: 49.22%] [G loss: 0.436790]\n",
      "epoch:19 step:18241 [D loss: 0.237691, acc.: 59.38%] [G loss: 0.441394]\n",
      "epoch:19 step:18242 [D loss: 0.245071, acc.: 55.47%] [G loss: 0.435499]\n",
      "epoch:19 step:18243 [D loss: 0.236701, acc.: 59.38%] [G loss: 0.456680]\n",
      "epoch:19 step:18244 [D loss: 0.244374, acc.: 57.03%] [G loss: 0.421835]\n",
      "epoch:19 step:18245 [D loss: 0.216492, acc.: 67.19%] [G loss: 0.446504]\n",
      "epoch:19 step:18246 [D loss: 0.245463, acc.: 60.16%] [G loss: 0.373358]\n",
      "epoch:19 step:18247 [D loss: 0.245123, acc.: 59.38%] [G loss: 0.410172]\n",
      "epoch:19 step:18248 [D loss: 0.221450, acc.: 61.72%] [G loss: 0.452930]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18249 [D loss: 0.214838, acc.: 67.97%] [G loss: 0.420501]\n",
      "epoch:19 step:18250 [D loss: 0.209099, acc.: 69.53%] [G loss: 0.442400]\n",
      "epoch:19 step:18251 [D loss: 0.260417, acc.: 53.91%] [G loss: 0.408228]\n",
      "epoch:19 step:18252 [D loss: 0.213341, acc.: 69.53%] [G loss: 0.442573]\n",
      "epoch:19 step:18253 [D loss: 0.224591, acc.: 61.72%] [G loss: 0.428166]\n",
      "epoch:19 step:18254 [D loss: 0.198922, acc.: 69.53%] [G loss: 0.423707]\n",
      "epoch:19 step:18255 [D loss: 0.219607, acc.: 61.72%] [G loss: 0.416220]\n",
      "epoch:19 step:18256 [D loss: 0.202651, acc.: 70.31%] [G loss: 0.444455]\n",
      "epoch:19 step:18257 [D loss: 0.220346, acc.: 67.97%] [G loss: 0.489824]\n",
      "epoch:19 step:18258 [D loss: 0.220481, acc.: 64.06%] [G loss: 0.450669]\n",
      "epoch:19 step:18259 [D loss: 0.221193, acc.: 62.50%] [G loss: 0.449562]\n",
      "epoch:19 step:18260 [D loss: 0.208968, acc.: 64.06%] [G loss: 0.478733]\n",
      "epoch:19 step:18261 [D loss: 0.262431, acc.: 53.12%] [G loss: 0.427691]\n",
      "epoch:19 step:18262 [D loss: 0.220860, acc.: 63.28%] [G loss: 0.462207]\n",
      "epoch:19 step:18263 [D loss: 0.241678, acc.: 57.81%] [G loss: 0.416153]\n",
      "epoch:19 step:18264 [D loss: 0.215854, acc.: 64.84%] [G loss: 0.426899]\n",
      "epoch:19 step:18265 [D loss: 0.247048, acc.: 57.81%] [G loss: 0.440687]\n",
      "epoch:19 step:18266 [D loss: 0.228902, acc.: 61.72%] [G loss: 0.431550]\n",
      "epoch:19 step:18267 [D loss: 0.240361, acc.: 57.81%] [G loss: 0.419443]\n",
      "epoch:19 step:18268 [D loss: 0.225170, acc.: 58.59%] [G loss: 0.416650]\n",
      "epoch:19 step:18269 [D loss: 0.222820, acc.: 64.06%] [G loss: 0.431798]\n",
      "epoch:19 step:18270 [D loss: 0.198555, acc.: 65.62%] [G loss: 0.459865]\n",
      "epoch:19 step:18271 [D loss: 0.222913, acc.: 59.38%] [G loss: 0.448832]\n",
      "epoch:19 step:18272 [D loss: 0.199188, acc.: 69.53%] [G loss: 0.492166]\n",
      "epoch:19 step:18273 [D loss: 0.225319, acc.: 64.06%] [G loss: 0.475515]\n",
      "epoch:19 step:18274 [D loss: 0.199131, acc.: 69.53%] [G loss: 0.482171]\n",
      "epoch:19 step:18275 [D loss: 0.205739, acc.: 68.75%] [G loss: 0.452808]\n",
      "epoch:19 step:18276 [D loss: 0.238405, acc.: 60.94%] [G loss: 0.431454]\n",
      "epoch:19 step:18277 [D loss: 0.194340, acc.: 71.88%] [G loss: 0.412683]\n",
      "epoch:19 step:18278 [D loss: 0.208586, acc.: 67.97%] [G loss: 0.427653]\n",
      "epoch:19 step:18279 [D loss: 0.214763, acc.: 69.53%] [G loss: 0.450222]\n",
      "epoch:19 step:18280 [D loss: 0.268560, acc.: 53.91%] [G loss: 0.413535]\n",
      "epoch:19 step:18281 [D loss: 0.220846, acc.: 64.06%] [G loss: 0.428132]\n",
      "epoch:19 step:18282 [D loss: 0.230153, acc.: 63.28%] [G loss: 0.358826]\n",
      "epoch:19 step:18283 [D loss: 0.221101, acc.: 63.28%] [G loss: 0.382936]\n",
      "epoch:19 step:18284 [D loss: 0.185724, acc.: 74.22%] [G loss: 0.438459]\n",
      "epoch:19 step:18285 [D loss: 0.270910, acc.: 52.34%] [G loss: 0.389400]\n",
      "epoch:19 step:18286 [D loss: 0.232903, acc.: 60.94%] [G loss: 0.433850]\n",
      "epoch:19 step:18287 [D loss: 0.203728, acc.: 71.09%] [G loss: 0.453487]\n",
      "epoch:19 step:18288 [D loss: 0.216111, acc.: 64.84%] [G loss: 0.424201]\n",
      "epoch:19 step:18289 [D loss: 0.227995, acc.: 57.03%] [G loss: 0.422090]\n",
      "epoch:19 step:18290 [D loss: 0.225710, acc.: 59.38%] [G loss: 0.382632]\n",
      "epoch:19 step:18291 [D loss: 0.208123, acc.: 65.62%] [G loss: 0.406436]\n",
      "epoch:19 step:18292 [D loss: 0.228615, acc.: 64.06%] [G loss: 0.423523]\n",
      "epoch:19 step:18293 [D loss: 0.230529, acc.: 59.38%] [G loss: 0.409039]\n",
      "epoch:19 step:18294 [D loss: 0.223846, acc.: 64.84%] [G loss: 0.421329]\n",
      "epoch:19 step:18295 [D loss: 0.235797, acc.: 59.38%] [G loss: 0.410225]\n",
      "epoch:19 step:18296 [D loss: 0.218776, acc.: 60.16%] [G loss: 0.441023]\n",
      "epoch:19 step:18297 [D loss: 0.212192, acc.: 70.31%] [G loss: 0.474738]\n",
      "epoch:19 step:18298 [D loss: 0.188870, acc.: 78.12%] [G loss: 0.433443]\n",
      "epoch:19 step:18299 [D loss: 0.202183, acc.: 69.53%] [G loss: 0.458850]\n",
      "epoch:19 step:18300 [D loss: 0.228296, acc.: 63.28%] [G loss: 0.430515]\n",
      "epoch:19 step:18301 [D loss: 0.202669, acc.: 72.66%] [G loss: 0.455651]\n",
      "epoch:19 step:18302 [D loss: 0.218901, acc.: 67.19%] [G loss: 0.429989]\n",
      "epoch:19 step:18303 [D loss: 0.232552, acc.: 61.72%] [G loss: 0.433267]\n",
      "epoch:19 step:18304 [D loss: 0.275109, acc.: 57.03%] [G loss: 0.452721]\n",
      "epoch:19 step:18305 [D loss: 0.223632, acc.: 61.72%] [G loss: 0.426433]\n",
      "epoch:19 step:18306 [D loss: 0.229285, acc.: 62.50%] [G loss: 0.389296]\n",
      "epoch:19 step:18307 [D loss: 0.186826, acc.: 73.44%] [G loss: 0.433513]\n",
      "epoch:19 step:18308 [D loss: 0.194340, acc.: 69.53%] [G loss: 0.451762]\n",
      "epoch:19 step:18309 [D loss: 0.194889, acc.: 69.53%] [G loss: 0.491296]\n",
      "epoch:19 step:18310 [D loss: 0.216856, acc.: 60.94%] [G loss: 0.490762]\n",
      "epoch:19 step:18311 [D loss: 0.198793, acc.: 64.06%] [G loss: 0.543779]\n",
      "epoch:19 step:18312 [D loss: 0.247430, acc.: 60.16%] [G loss: 0.449962]\n",
      "epoch:19 step:18313 [D loss: 0.255019, acc.: 55.47%] [G loss: 0.403623]\n",
      "epoch:19 step:18314 [D loss: 0.270140, acc.: 44.53%] [G loss: 0.409290]\n",
      "epoch:19 step:18315 [D loss: 0.226870, acc.: 60.94%] [G loss: 0.432964]\n",
      "epoch:19 step:18316 [D loss: 0.202681, acc.: 71.09%] [G loss: 0.426096]\n",
      "epoch:19 step:18317 [D loss: 0.229677, acc.: 63.28%] [G loss: 0.425789]\n",
      "epoch:19 step:18318 [D loss: 0.175144, acc.: 80.47%] [G loss: 0.478320]\n",
      "epoch:19 step:18319 [D loss: 0.196843, acc.: 71.88%] [G loss: 0.475924]\n",
      "epoch:19 step:18320 [D loss: 0.226937, acc.: 60.94%] [G loss: 0.404612]\n",
      "epoch:19 step:18321 [D loss: 0.250105, acc.: 57.81%] [G loss: 0.426110]\n",
      "epoch:19 step:18322 [D loss: 0.215404, acc.: 60.16%] [G loss: 0.453663]\n",
      "epoch:19 step:18323 [D loss: 0.200159, acc.: 67.19%] [G loss: 0.433154]\n",
      "epoch:19 step:18324 [D loss: 0.199962, acc.: 68.75%] [G loss: 0.426651]\n",
      "epoch:19 step:18325 [D loss: 0.215201, acc.: 67.19%] [G loss: 0.444229]\n",
      "epoch:19 step:18326 [D loss: 0.191403, acc.: 75.78%] [G loss: 0.460767]\n",
      "epoch:19 step:18327 [D loss: 0.228536, acc.: 59.38%] [G loss: 0.424440]\n",
      "epoch:19 step:18328 [D loss: 0.245152, acc.: 57.03%] [G loss: 0.429765]\n",
      "epoch:19 step:18329 [D loss: 0.222018, acc.: 64.84%] [G loss: 0.410360]\n",
      "epoch:19 step:18330 [D loss: 0.209967, acc.: 71.88%] [G loss: 0.455186]\n",
      "epoch:19 step:18331 [D loss: 0.270485, acc.: 49.22%] [G loss: 0.412692]\n",
      "epoch:19 step:18332 [D loss: 0.245534, acc.: 53.12%] [G loss: 0.442660]\n",
      "epoch:19 step:18333 [D loss: 0.200426, acc.: 66.41%] [G loss: 0.507748]\n",
      "epoch:19 step:18334 [D loss: 0.232902, acc.: 57.81%] [G loss: 0.413104]\n",
      "epoch:19 step:18335 [D loss: 0.220782, acc.: 61.72%] [G loss: 0.392087]\n",
      "epoch:19 step:18336 [D loss: 0.217849, acc.: 63.28%] [G loss: 0.393000]\n",
      "epoch:19 step:18337 [D loss: 0.197825, acc.: 66.41%] [G loss: 0.436941]\n",
      "epoch:19 step:18338 [D loss: 0.252029, acc.: 57.03%] [G loss: 0.420479]\n",
      "epoch:19 step:18339 [D loss: 0.246854, acc.: 61.72%] [G loss: 0.408928]\n",
      "epoch:19 step:18340 [D loss: 0.211603, acc.: 64.06%] [G loss: 0.458088]\n",
      "epoch:19 step:18341 [D loss: 0.260903, acc.: 59.38%] [G loss: 0.424911]\n",
      "epoch:19 step:18342 [D loss: 0.204582, acc.: 68.75%] [G loss: 0.416393]\n",
      "epoch:19 step:18343 [D loss: 0.224691, acc.: 67.19%] [G loss: 0.445016]\n",
      "epoch:19 step:18344 [D loss: 0.242623, acc.: 58.59%] [G loss: 0.397043]\n",
      "epoch:19 step:18345 [D loss: 0.270896, acc.: 46.09%] [G loss: 0.425246]\n",
      "epoch:19 step:18346 [D loss: 0.224412, acc.: 61.72%] [G loss: 0.404009]\n",
      "epoch:19 step:18347 [D loss: 0.224512, acc.: 63.28%] [G loss: 0.419316]\n",
      "epoch:19 step:18348 [D loss: 0.194192, acc.: 69.53%] [G loss: 0.463407]\n",
      "epoch:19 step:18349 [D loss: 0.207426, acc.: 69.53%] [G loss: 0.441273]\n",
      "epoch:19 step:18350 [D loss: 0.196621, acc.: 69.53%] [G loss: 0.446451]\n",
      "epoch:19 step:18351 [D loss: 0.210260, acc.: 66.41%] [G loss: 0.431890]\n",
      "epoch:19 step:18352 [D loss: 0.223461, acc.: 64.84%] [G loss: 0.435390]\n",
      "epoch:19 step:18353 [D loss: 0.214445, acc.: 69.53%] [G loss: 0.480834]\n",
      "epoch:19 step:18354 [D loss: 0.202622, acc.: 68.75%] [G loss: 0.461255]\n",
      "epoch:19 step:18355 [D loss: 0.209729, acc.: 64.06%] [G loss: 0.465026]\n",
      "epoch:19 step:18356 [D loss: 0.227124, acc.: 60.16%] [G loss: 0.425527]\n",
      "epoch:19 step:18357 [D loss: 0.198128, acc.: 67.97%] [G loss: 0.388527]\n",
      "epoch:19 step:18358 [D loss: 0.194259, acc.: 71.09%] [G loss: 0.507465]\n",
      "epoch:19 step:18359 [D loss: 0.231434, acc.: 64.84%] [G loss: 0.446659]\n",
      "epoch:19 step:18360 [D loss: 0.222037, acc.: 64.06%] [G loss: 0.450428]\n",
      "epoch:19 step:18361 [D loss: 0.208613, acc.: 69.53%] [G loss: 0.482222]\n",
      "epoch:19 step:18362 [D loss: 0.265971, acc.: 53.12%] [G loss: 0.412435]\n",
      "epoch:19 step:18363 [D loss: 0.235888, acc.: 60.94%] [G loss: 0.444352]\n",
      "epoch:19 step:18364 [D loss: 0.230173, acc.: 63.28%] [G loss: 0.393641]\n",
      "epoch:19 step:18365 [D loss: 0.217110, acc.: 63.28%] [G loss: 0.450920]\n",
      "epoch:19 step:18366 [D loss: 0.223161, acc.: 60.94%] [G loss: 0.441903]\n",
      "epoch:19 step:18367 [D loss: 0.186207, acc.: 69.53%] [G loss: 0.484203]\n",
      "epoch:19 step:18368 [D loss: 0.249728, acc.: 55.47%] [G loss: 0.442795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18369 [D loss: 0.281806, acc.: 50.00%] [G loss: 0.458545]\n",
      "epoch:19 step:18370 [D loss: 0.204853, acc.: 68.75%] [G loss: 0.460613]\n",
      "epoch:19 step:18371 [D loss: 0.205392, acc.: 72.66%] [G loss: 0.452427]\n",
      "epoch:19 step:18372 [D loss: 0.302593, acc.: 45.31%] [G loss: 0.389146]\n",
      "epoch:19 step:18373 [D loss: 0.219800, acc.: 65.62%] [G loss: 0.402439]\n",
      "epoch:19 step:18374 [D loss: 0.230253, acc.: 62.50%] [G loss: 0.409846]\n",
      "epoch:19 step:18375 [D loss: 0.206221, acc.: 67.19%] [G loss: 0.428122]\n",
      "epoch:19 step:18376 [D loss: 0.204709, acc.: 68.75%] [G loss: 0.415275]\n",
      "epoch:19 step:18377 [D loss: 0.177277, acc.: 75.78%] [G loss: 0.440139]\n",
      "epoch:19 step:18378 [D loss: 0.190038, acc.: 67.19%] [G loss: 0.476828]\n",
      "epoch:19 step:18379 [D loss: 0.245431, acc.: 56.25%] [G loss: 0.463898]\n",
      "epoch:19 step:18380 [D loss: 0.235858, acc.: 60.94%] [G loss: 0.445694]\n",
      "epoch:19 step:18381 [D loss: 0.213047, acc.: 62.50%] [G loss: 0.430715]\n",
      "epoch:19 step:18382 [D loss: 0.230007, acc.: 57.81%] [G loss: 0.400993]\n",
      "epoch:19 step:18383 [D loss: 0.228244, acc.: 59.38%] [G loss: 0.453960]\n",
      "epoch:19 step:18384 [D loss: 0.203710, acc.: 69.53%] [G loss: 0.450595]\n",
      "epoch:19 step:18385 [D loss: 0.188389, acc.: 71.09%] [G loss: 0.489407]\n",
      "epoch:19 step:18386 [D loss: 0.213847, acc.: 65.62%] [G loss: 0.465438]\n",
      "epoch:19 step:18387 [D loss: 0.243561, acc.: 56.25%] [G loss: 0.450869]\n",
      "epoch:19 step:18388 [D loss: 0.213722, acc.: 63.28%] [G loss: 0.411507]\n",
      "epoch:19 step:18389 [D loss: 0.232150, acc.: 60.94%] [G loss: 0.429825]\n",
      "epoch:19 step:18390 [D loss: 0.239060, acc.: 57.03%] [G loss: 0.401861]\n",
      "epoch:19 step:18391 [D loss: 0.228847, acc.: 60.94%] [G loss: 0.414575]\n",
      "epoch:19 step:18392 [D loss: 0.202915, acc.: 65.62%] [G loss: 0.429172]\n",
      "epoch:19 step:18393 [D loss: 0.232884, acc.: 64.06%] [G loss: 0.425421]\n",
      "epoch:19 step:18394 [D loss: 0.227708, acc.: 63.28%] [G loss: 0.433408]\n",
      "epoch:19 step:18395 [D loss: 0.208749, acc.: 68.75%] [G loss: 0.431747]\n",
      "epoch:19 step:18396 [D loss: 0.213688, acc.: 64.06%] [G loss: 0.441749]\n",
      "epoch:19 step:18397 [D loss: 0.231578, acc.: 60.16%] [G loss: 0.443290]\n",
      "epoch:19 step:18398 [D loss: 0.220259, acc.: 64.06%] [G loss: 0.440496]\n",
      "epoch:19 step:18399 [D loss: 0.234448, acc.: 55.47%] [G loss: 0.421460]\n",
      "epoch:19 step:18400 [D loss: 0.229059, acc.: 57.81%] [G loss: 0.439402]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 8.115244\n",
      "FID: 8.008303\n",
      "0 = 11.706583434867834\n",
      "1 = 0.047681857873959774\n",
      "2 = 0.8585500121116638\n",
      "3 = 0.8393999934196472\n",
      "4 = 0.8776999711990356\n",
      "5 = 0.8728293776512146\n",
      "6 = 0.8393999934196472\n",
      "7 = 5.827985695385939\n",
      "8 = 0.06061279645970418\n",
      "9 = 0.6796500086784363\n",
      "10 = 0.6769000291824341\n",
      "11 = 0.6823999881744385\n",
      "12 = 0.6806435585021973\n",
      "13 = 0.6769000291824341\n",
      "14 = 8.115317344665527\n",
      "15 = 9.518688201904297\n",
      "16 = 0.11287999898195267\n",
      "17 = 8.115243911743164\n",
      "18 = 8.008302688598633\n",
      "epoch:19 step:18401 [D loss: 0.202300, acc.: 70.31%] [G loss: 0.427856]\n",
      "epoch:19 step:18402 [D loss: 0.189441, acc.: 69.53%] [G loss: 0.465470]\n",
      "epoch:19 step:18403 [D loss: 0.252838, acc.: 58.59%] [G loss: 0.388189]\n",
      "epoch:19 step:18404 [D loss: 0.223211, acc.: 61.72%] [G loss: 0.439554]\n",
      "epoch:19 step:18405 [D loss: 0.217397, acc.: 63.28%] [G loss: 0.438263]\n",
      "epoch:19 step:18406 [D loss: 0.217523, acc.: 65.62%] [G loss: 0.412326]\n",
      "epoch:19 step:18407 [D loss: 0.237484, acc.: 61.72%] [G loss: 0.407265]\n",
      "epoch:19 step:18408 [D loss: 0.209275, acc.: 69.53%] [G loss: 0.450020]\n",
      "epoch:19 step:18409 [D loss: 0.232299, acc.: 62.50%] [G loss: 0.454786]\n",
      "epoch:19 step:18410 [D loss: 0.199910, acc.: 64.84%] [G loss: 0.430190]\n",
      "epoch:19 step:18411 [D loss: 0.244853, acc.: 59.38%] [G loss: 0.401053]\n",
      "epoch:19 step:18412 [D loss: 0.226482, acc.: 60.16%] [G loss: 0.417850]\n",
      "epoch:19 step:18413 [D loss: 0.231917, acc.: 60.16%] [G loss: 0.437411]\n",
      "epoch:19 step:18414 [D loss: 0.221771, acc.: 64.84%] [G loss: 0.455856]\n",
      "epoch:19 step:18415 [D loss: 0.222340, acc.: 60.16%] [G loss: 0.417111]\n",
      "epoch:19 step:18416 [D loss: 0.228304, acc.: 64.06%] [G loss: 0.440451]\n",
      "epoch:19 step:18417 [D loss: 0.251525, acc.: 57.03%] [G loss: 0.413772]\n",
      "epoch:19 step:18418 [D loss: 0.251840, acc.: 51.56%] [G loss: 0.457024]\n",
      "epoch:19 step:18419 [D loss: 0.220864, acc.: 64.84%] [G loss: 0.455116]\n",
      "epoch:19 step:18420 [D loss: 0.211667, acc.: 65.62%] [G loss: 0.440522]\n",
      "epoch:19 step:18421 [D loss: 0.206465, acc.: 71.09%] [G loss: 0.492179]\n",
      "epoch:19 step:18422 [D loss: 0.234322, acc.: 60.94%] [G loss: 0.469631]\n",
      "epoch:19 step:18423 [D loss: 0.199424, acc.: 65.62%] [G loss: 0.425994]\n",
      "epoch:19 step:18424 [D loss: 0.233949, acc.: 57.03%] [G loss: 0.433020]\n",
      "epoch:19 step:18425 [D loss: 0.228042, acc.: 64.84%] [G loss: 0.436756]\n",
      "epoch:19 step:18426 [D loss: 0.221963, acc.: 60.94%] [G loss: 0.424622]\n",
      "epoch:19 step:18427 [D loss: 0.184257, acc.: 75.00%] [G loss: 0.424024]\n",
      "epoch:19 step:18428 [D loss: 0.254294, acc.: 53.91%] [G loss: 0.475205]\n",
      "epoch:19 step:18429 [D loss: 0.231171, acc.: 61.72%] [G loss: 0.423373]\n",
      "epoch:19 step:18430 [D loss: 0.231538, acc.: 57.03%] [G loss: 0.388020]\n",
      "epoch:19 step:18431 [D loss: 0.236023, acc.: 54.69%] [G loss: 0.410120]\n",
      "epoch:19 step:18432 [D loss: 0.214686, acc.: 64.84%] [G loss: 0.433254]\n",
      "epoch:19 step:18433 [D loss: 0.240317, acc.: 58.59%] [G loss: 0.425647]\n",
      "epoch:19 step:18434 [D loss: 0.199456, acc.: 71.88%] [G loss: 0.424770]\n",
      "epoch:19 step:18435 [D loss: 0.209986, acc.: 63.28%] [G loss: 0.467056]\n",
      "epoch:19 step:18436 [D loss: 0.238583, acc.: 57.81%] [G loss: 0.435861]\n",
      "epoch:19 step:18437 [D loss: 0.197260, acc.: 73.44%] [G loss: 0.463367]\n",
      "epoch:19 step:18438 [D loss: 0.215753, acc.: 67.19%] [G loss: 0.449930]\n",
      "epoch:19 step:18439 [D loss: 0.219173, acc.: 61.72%] [G loss: 0.442301]\n",
      "epoch:19 step:18440 [D loss: 0.201353, acc.: 70.31%] [G loss: 0.449463]\n",
      "epoch:19 step:18441 [D loss: 0.226420, acc.: 62.50%] [G loss: 0.443929]\n",
      "epoch:19 step:18442 [D loss: 0.229934, acc.: 58.59%] [G loss: 0.429976]\n",
      "epoch:19 step:18443 [D loss: 0.228983, acc.: 56.25%] [G loss: 0.424876]\n",
      "epoch:19 step:18444 [D loss: 0.209006, acc.: 64.84%] [G loss: 0.461787]\n",
      "epoch:19 step:18445 [D loss: 0.168336, acc.: 78.12%] [G loss: 0.510987]\n",
      "epoch:19 step:18446 [D loss: 0.210383, acc.: 70.31%] [G loss: 0.457208]\n",
      "epoch:19 step:18447 [D loss: 0.231798, acc.: 60.16%] [G loss: 0.421844]\n",
      "epoch:19 step:18448 [D loss: 0.228391, acc.: 60.94%] [G loss: 0.402120]\n",
      "epoch:19 step:18449 [D loss: 0.215167, acc.: 71.09%] [G loss: 0.423599]\n",
      "epoch:19 step:18450 [D loss: 0.195612, acc.: 69.53%] [G loss: 0.512806]\n",
      "epoch:19 step:18451 [D loss: 0.189315, acc.: 69.53%] [G loss: 0.463826]\n",
      "epoch:19 step:18452 [D loss: 0.225322, acc.: 63.28%] [G loss: 0.475337]\n",
      "epoch:19 step:18453 [D loss: 0.228137, acc.: 68.75%] [G loss: 0.449882]\n",
      "epoch:19 step:18454 [D loss: 0.236928, acc.: 55.47%] [G loss: 0.451317]\n",
      "epoch:19 step:18455 [D loss: 0.220151, acc.: 64.84%] [G loss: 0.482752]\n",
      "epoch:19 step:18456 [D loss: 0.231967, acc.: 62.50%] [G loss: 0.429574]\n",
      "epoch:19 step:18457 [D loss: 0.217703, acc.: 60.94%] [G loss: 0.487645]\n",
      "epoch:19 step:18458 [D loss: 0.240640, acc.: 60.94%] [G loss: 0.455132]\n",
      "epoch:19 step:18459 [D loss: 0.229296, acc.: 61.72%] [G loss: 0.365418]\n",
      "epoch:19 step:18460 [D loss: 0.230310, acc.: 57.03%] [G loss: 0.403252]\n",
      "epoch:19 step:18461 [D loss: 0.229409, acc.: 64.06%] [G loss: 0.415361]\n",
      "epoch:19 step:18462 [D loss: 0.230148, acc.: 60.16%] [G loss: 0.406048]\n",
      "epoch:19 step:18463 [D loss: 0.190741, acc.: 67.19%] [G loss: 0.463730]\n",
      "epoch:19 step:18464 [D loss: 0.204680, acc.: 67.97%] [G loss: 0.460042]\n",
      "epoch:19 step:18465 [D loss: 0.226415, acc.: 61.72%] [G loss: 0.440124]\n",
      "epoch:19 step:18466 [D loss: 0.239179, acc.: 57.81%] [G loss: 0.410798]\n",
      "epoch:19 step:18467 [D loss: 0.219872, acc.: 63.28%] [G loss: 0.439077]\n",
      "epoch:19 step:18468 [D loss: 0.228908, acc.: 60.94%] [G loss: 0.452091]\n",
      "epoch:19 step:18469 [D loss: 0.207814, acc.: 72.66%] [G loss: 0.429995]\n",
      "epoch:19 step:18470 [D loss: 0.234672, acc.: 60.94%] [G loss: 0.473551]\n",
      "epoch:19 step:18471 [D loss: 0.208865, acc.: 64.84%] [G loss: 0.446169]\n",
      "epoch:19 step:18472 [D loss: 0.195434, acc.: 72.66%] [G loss: 0.444021]\n",
      "epoch:19 step:18473 [D loss: 0.256386, acc.: 52.34%] [G loss: 0.395061]\n",
      "epoch:19 step:18474 [D loss: 0.239803, acc.: 56.25%] [G loss: 0.398319]\n",
      "epoch:19 step:18475 [D loss: 0.249883, acc.: 53.91%] [G loss: 0.414761]\n",
      "epoch:19 step:18476 [D loss: 0.225439, acc.: 60.94%] [G loss: 0.438524]\n",
      "epoch:19 step:18477 [D loss: 0.219651, acc.: 64.84%] [G loss: 0.437919]\n",
      "epoch:19 step:18478 [D loss: 0.230903, acc.: 61.72%] [G loss: 0.464257]\n",
      "epoch:19 step:18479 [D loss: 0.233147, acc.: 58.59%] [G loss: 0.409401]\n",
      "epoch:19 step:18480 [D loss: 0.206533, acc.: 68.75%] [G loss: 0.414163]\n",
      "epoch:19 step:18481 [D loss: 0.225774, acc.: 57.81%] [G loss: 0.407087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18482 [D loss: 0.221347, acc.: 64.06%] [G loss: 0.453319]\n",
      "epoch:19 step:18483 [D loss: 0.227572, acc.: 60.16%] [G loss: 0.452149]\n",
      "epoch:19 step:18484 [D loss: 0.198405, acc.: 67.97%] [G loss: 0.473192]\n",
      "epoch:19 step:18485 [D loss: 0.234994, acc.: 50.78%] [G loss: 0.444066]\n",
      "epoch:19 step:18486 [D loss: 0.245963, acc.: 57.03%] [G loss: 0.422366]\n",
      "epoch:19 step:18487 [D loss: 0.205784, acc.: 64.84%] [G loss: 0.453961]\n",
      "epoch:19 step:18488 [D loss: 0.237359, acc.: 60.94%] [G loss: 0.401640]\n",
      "epoch:19 step:18489 [D loss: 0.239617, acc.: 62.50%] [G loss: 0.401218]\n",
      "epoch:19 step:18490 [D loss: 0.203279, acc.: 65.62%] [G loss: 0.405810]\n",
      "epoch:19 step:18491 [D loss: 0.224157, acc.: 64.06%] [G loss: 0.430240]\n",
      "epoch:19 step:18492 [D loss: 0.214298, acc.: 66.41%] [G loss: 0.434384]\n",
      "epoch:19 step:18493 [D loss: 0.207978, acc.: 68.75%] [G loss: 0.486194]\n",
      "epoch:19 step:18494 [D loss: 0.213561, acc.: 68.75%] [G loss: 0.485493]\n",
      "epoch:19 step:18495 [D loss: 0.210619, acc.: 66.41%] [G loss: 0.479113]\n",
      "epoch:19 step:18496 [D loss: 0.216710, acc.: 63.28%] [G loss: 0.450690]\n",
      "epoch:19 step:18497 [D loss: 0.204543, acc.: 72.66%] [G loss: 0.483196]\n",
      "epoch:19 step:18498 [D loss: 0.191848, acc.: 71.88%] [G loss: 0.459553]\n",
      "epoch:19 step:18499 [D loss: 0.257840, acc.: 51.56%] [G loss: 0.400542]\n",
      "epoch:19 step:18500 [D loss: 0.224854, acc.: 61.72%] [G loss: 0.430335]\n",
      "epoch:19 step:18501 [D loss: 0.225868, acc.: 62.50%] [G loss: 0.411439]\n",
      "epoch:19 step:18502 [D loss: 0.197557, acc.: 75.78%] [G loss: 0.425679]\n",
      "epoch:19 step:18503 [D loss: 0.213654, acc.: 64.06%] [G loss: 0.442920]\n",
      "epoch:19 step:18504 [D loss: 0.225877, acc.: 62.50%] [G loss: 0.466079]\n",
      "epoch:19 step:18505 [D loss: 0.250526, acc.: 55.47%] [G loss: 0.414197]\n",
      "epoch:19 step:18506 [D loss: 0.271001, acc.: 57.81%] [G loss: 0.409007]\n",
      "epoch:19 step:18507 [D loss: 0.248500, acc.: 53.12%] [G loss: 0.440224]\n",
      "epoch:19 step:18508 [D loss: 0.227978, acc.: 60.94%] [G loss: 0.414049]\n",
      "epoch:19 step:18509 [D loss: 0.209880, acc.: 63.28%] [G loss: 0.439354]\n",
      "epoch:19 step:18510 [D loss: 0.217216, acc.: 64.84%] [G loss: 0.415055]\n",
      "epoch:19 step:18511 [D loss: 0.183542, acc.: 67.97%] [G loss: 0.442516]\n",
      "epoch:19 step:18512 [D loss: 0.197904, acc.: 70.31%] [G loss: 0.462043]\n",
      "epoch:19 step:18513 [D loss: 0.242389, acc.: 59.38%] [G loss: 0.389531]\n",
      "epoch:19 step:18514 [D loss: 0.222485, acc.: 62.50%] [G loss: 0.406570]\n",
      "epoch:19 step:18515 [D loss: 0.220752, acc.: 60.16%] [G loss: 0.438288]\n",
      "epoch:19 step:18516 [D loss: 0.223025, acc.: 62.50%] [G loss: 0.455557]\n",
      "epoch:19 step:18517 [D loss: 0.228609, acc.: 61.72%] [G loss: 0.448397]\n",
      "epoch:19 step:18518 [D loss: 0.215937, acc.: 64.06%] [G loss: 0.453084]\n",
      "epoch:19 step:18519 [D loss: 0.256516, acc.: 57.03%] [G loss: 0.406584]\n",
      "epoch:19 step:18520 [D loss: 0.236688, acc.: 60.16%] [G loss: 0.398228]\n",
      "epoch:19 step:18521 [D loss: 0.239757, acc.: 57.81%] [G loss: 0.411164]\n",
      "epoch:19 step:18522 [D loss: 0.203406, acc.: 67.97%] [G loss: 0.502569]\n",
      "epoch:19 step:18523 [D loss: 0.223390, acc.: 60.94%] [G loss: 0.466000]\n",
      "epoch:19 step:18524 [D loss: 0.225127, acc.: 64.06%] [G loss: 0.452532]\n",
      "epoch:19 step:18525 [D loss: 0.241008, acc.: 59.38%] [G loss: 0.411850]\n",
      "epoch:19 step:18526 [D loss: 0.217793, acc.: 64.84%] [G loss: 0.424612]\n",
      "epoch:19 step:18527 [D loss: 0.217546, acc.: 60.94%] [G loss: 0.433667]\n",
      "epoch:19 step:18528 [D loss: 0.206565, acc.: 69.53%] [G loss: 0.446570]\n",
      "epoch:19 step:18529 [D loss: 0.218064, acc.: 64.06%] [G loss: 0.447920]\n",
      "epoch:19 step:18530 [D loss: 0.230221, acc.: 61.72%] [G loss: 0.418887]\n",
      "epoch:19 step:18531 [D loss: 0.209850, acc.: 65.62%] [G loss: 0.450121]\n",
      "epoch:19 step:18532 [D loss: 0.217162, acc.: 65.62%] [G loss: 0.424161]\n",
      "epoch:19 step:18533 [D loss: 0.217005, acc.: 64.06%] [G loss: 0.447100]\n",
      "epoch:19 step:18534 [D loss: 0.227514, acc.: 58.59%] [G loss: 0.437897]\n",
      "epoch:19 step:18535 [D loss: 0.200303, acc.: 71.88%] [G loss: 0.439106]\n",
      "epoch:19 step:18536 [D loss: 0.198890, acc.: 70.31%] [G loss: 0.439671]\n",
      "epoch:19 step:18537 [D loss: 0.243876, acc.: 61.72%] [G loss: 0.445207]\n",
      "epoch:19 step:18538 [D loss: 0.215945, acc.: 66.41%] [G loss: 0.451688]\n",
      "epoch:19 step:18539 [D loss: 0.208448, acc.: 66.41%] [G loss: 0.450345]\n",
      "epoch:19 step:18540 [D loss: 0.225788, acc.: 62.50%] [G loss: 0.405695]\n",
      "epoch:19 step:18541 [D loss: 0.241973, acc.: 60.94%] [G loss: 0.418561]\n",
      "epoch:19 step:18542 [D loss: 0.255434, acc.: 50.00%] [G loss: 0.407288]\n",
      "epoch:19 step:18543 [D loss: 0.245242, acc.: 56.25%] [G loss: 0.435188]\n",
      "epoch:19 step:18544 [D loss: 0.234353, acc.: 60.94%] [G loss: 0.414025]\n",
      "epoch:19 step:18545 [D loss: 0.217771, acc.: 64.06%] [G loss: 0.426033]\n",
      "epoch:19 step:18546 [D loss: 0.214443, acc.: 64.84%] [G loss: 0.424140]\n",
      "epoch:19 step:18547 [D loss: 0.255138, acc.: 53.91%] [G loss: 0.408339]\n",
      "epoch:19 step:18548 [D loss: 0.235904, acc.: 64.06%] [G loss: 0.429163]\n",
      "epoch:19 step:18549 [D loss: 0.220113, acc.: 62.50%] [G loss: 0.412615]\n",
      "epoch:19 step:18550 [D loss: 0.219291, acc.: 65.62%] [G loss: 0.394607]\n",
      "epoch:19 step:18551 [D loss: 0.233976, acc.: 64.06%] [G loss: 0.413770]\n",
      "epoch:19 step:18552 [D loss: 0.247070, acc.: 60.94%] [G loss: 0.437840]\n",
      "epoch:19 step:18553 [D loss: 0.210618, acc.: 64.84%] [G loss: 0.459257]\n",
      "epoch:19 step:18554 [D loss: 0.245139, acc.: 64.84%] [G loss: 0.432503]\n",
      "epoch:19 step:18555 [D loss: 0.228243, acc.: 62.50%] [G loss: 0.433227]\n",
      "epoch:19 step:18556 [D loss: 0.236052, acc.: 60.16%] [G loss: 0.400370]\n",
      "epoch:19 step:18557 [D loss: 0.220866, acc.: 60.94%] [G loss: 0.407220]\n",
      "epoch:19 step:18558 [D loss: 0.220441, acc.: 64.06%] [G loss: 0.442286]\n",
      "epoch:19 step:18559 [D loss: 0.230039, acc.: 66.41%] [G loss: 0.433448]\n",
      "epoch:19 step:18560 [D loss: 0.244306, acc.: 58.59%] [G loss: 0.434421]\n",
      "epoch:19 step:18561 [D loss: 0.257711, acc.: 55.47%] [G loss: 0.433618]\n",
      "epoch:19 step:18562 [D loss: 0.242796, acc.: 59.38%] [G loss: 0.402132]\n",
      "epoch:19 step:18563 [D loss: 0.247441, acc.: 55.47%] [G loss: 0.414211]\n",
      "epoch:19 step:18564 [D loss: 0.245587, acc.: 54.69%] [G loss: 0.424965]\n",
      "epoch:19 step:18565 [D loss: 0.226759, acc.: 60.16%] [G loss: 0.462479]\n",
      "epoch:19 step:18566 [D loss: 0.217061, acc.: 64.84%] [G loss: 0.429909]\n",
      "epoch:19 step:18567 [D loss: 0.227712, acc.: 60.16%] [G loss: 0.464817]\n",
      "epoch:19 step:18568 [D loss: 0.226275, acc.: 63.28%] [G loss: 0.413873]\n",
      "epoch:19 step:18569 [D loss: 0.245806, acc.: 52.34%] [G loss: 0.438805]\n",
      "epoch:19 step:18570 [D loss: 0.246242, acc.: 55.47%] [G loss: 0.399744]\n",
      "epoch:19 step:18571 [D loss: 0.213692, acc.: 67.97%] [G loss: 0.495593]\n",
      "epoch:19 step:18572 [D loss: 0.213602, acc.: 63.28%] [G loss: 0.480550]\n",
      "epoch:19 step:18573 [D loss: 0.196563, acc.: 68.75%] [G loss: 0.484420]\n",
      "epoch:19 step:18574 [D loss: 0.222457, acc.: 62.50%] [G loss: 0.432548]\n",
      "epoch:19 step:18575 [D loss: 0.259510, acc.: 58.59%] [G loss: 0.429282]\n",
      "epoch:19 step:18576 [D loss: 0.210126, acc.: 63.28%] [G loss: 0.395778]\n",
      "epoch:19 step:18577 [D loss: 0.219149, acc.: 59.38%] [G loss: 0.405995]\n",
      "epoch:19 step:18578 [D loss: 0.191248, acc.: 71.09%] [G loss: 0.490925]\n",
      "epoch:19 step:18579 [D loss: 0.250210, acc.: 59.38%] [G loss: 0.475604]\n",
      "epoch:19 step:18580 [D loss: 0.202867, acc.: 69.53%] [G loss: 0.448409]\n",
      "epoch:19 step:18581 [D loss: 0.239075, acc.: 57.03%] [G loss: 0.435179]\n",
      "epoch:19 step:18582 [D loss: 0.219976, acc.: 65.62%] [G loss: 0.411039]\n",
      "epoch:19 step:18583 [D loss: 0.217589, acc.: 66.41%] [G loss: 0.415273]\n",
      "epoch:19 step:18584 [D loss: 0.200714, acc.: 66.41%] [G loss: 0.462977]\n",
      "epoch:19 step:18585 [D loss: 0.209666, acc.: 67.97%] [G loss: 0.496650]\n",
      "epoch:19 step:18586 [D loss: 0.237568, acc.: 61.72%] [G loss: 0.464743]\n",
      "epoch:19 step:18587 [D loss: 0.233577, acc.: 50.00%] [G loss: 0.427710]\n",
      "epoch:19 step:18588 [D loss: 0.231665, acc.: 61.72%] [G loss: 0.399335]\n",
      "epoch:19 step:18589 [D loss: 0.202096, acc.: 68.75%] [G loss: 0.431320]\n",
      "epoch:19 step:18590 [D loss: 0.258640, acc.: 53.12%] [G loss: 0.450280]\n",
      "epoch:19 step:18591 [D loss: 0.239509, acc.: 61.72%] [G loss: 0.462080]\n",
      "epoch:19 step:18592 [D loss: 0.233554, acc.: 62.50%] [G loss: 0.402653]\n",
      "epoch:19 step:18593 [D loss: 0.243922, acc.: 56.25%] [G loss: 0.399237]\n",
      "epoch:19 step:18594 [D loss: 0.233497, acc.: 52.34%] [G loss: 0.414713]\n",
      "epoch:19 step:18595 [D loss: 0.198939, acc.: 70.31%] [G loss: 0.445869]\n",
      "epoch:19 step:18596 [D loss: 0.234666, acc.: 58.59%] [G loss: 0.444755]\n",
      "epoch:19 step:18597 [D loss: 0.288750, acc.: 49.22%] [G loss: 0.426427]\n",
      "epoch:19 step:18598 [D loss: 0.226656, acc.: 60.16%] [G loss: 0.433370]\n",
      "epoch:19 step:18599 [D loss: 0.215597, acc.: 68.75%] [G loss: 0.420190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18600 [D loss: 0.248918, acc.: 57.81%] [G loss: 0.439834]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 8.034595\n",
      "FID: 8.597926\n",
      "0 = 11.750230149722066\n",
      "1 = 0.04363770378106735\n",
      "2 = 0.8639000058174133\n",
      "3 = 0.8640999794006348\n",
      "4 = 0.8636999726295471\n",
      "5 = 0.8637545108795166\n",
      "6 = 0.8640999794006348\n",
      "7 = 6.0179635235428846\n",
      "8 = 0.06123697103383646\n",
      "9 = 0.6821500062942505\n",
      "10 = 0.6841999888420105\n",
      "11 = 0.6801000237464905\n",
      "12 = 0.6814062595367432\n",
      "13 = 0.6841999888420105\n",
      "14 = 8.034667015075684\n",
      "15 = 9.495246887207031\n",
      "16 = 0.12209251523017883\n",
      "17 = 8.034594535827637\n",
      "18 = 8.597926139831543\n",
      "epoch:19 step:18601 [D loss: 0.228543, acc.: 61.72%] [G loss: 0.381845]\n",
      "epoch:19 step:18602 [D loss: 0.213893, acc.: 66.41%] [G loss: 0.432379]\n",
      "epoch:19 step:18603 [D loss: 0.268137, acc.: 48.44%] [G loss: 0.406122]\n",
      "epoch:19 step:18604 [D loss: 0.196857, acc.: 66.41%] [G loss: 0.445505]\n",
      "epoch:19 step:18605 [D loss: 0.209773, acc.: 71.09%] [G loss: 0.481366]\n",
      "epoch:19 step:18606 [D loss: 0.206378, acc.: 68.75%] [G loss: 0.498945]\n",
      "epoch:19 step:18607 [D loss: 0.260204, acc.: 50.78%] [G loss: 0.443549]\n",
      "epoch:19 step:18608 [D loss: 0.213389, acc.: 67.97%] [G loss: 0.428501]\n",
      "epoch:19 step:18609 [D loss: 0.231316, acc.: 61.72%] [G loss: 0.437436]\n",
      "epoch:19 step:18610 [D loss: 0.214318, acc.: 66.41%] [G loss: 0.442552]\n",
      "epoch:19 step:18611 [D loss: 0.228402, acc.: 59.38%] [G loss: 0.447290]\n",
      "epoch:19 step:18612 [D loss: 0.247541, acc.: 57.81%] [G loss: 0.405615]\n",
      "epoch:19 step:18613 [D loss: 0.248728, acc.: 57.81%] [G loss: 0.402844]\n",
      "epoch:19 step:18614 [D loss: 0.228328, acc.: 61.72%] [G loss: 0.401402]\n",
      "epoch:19 step:18615 [D loss: 0.247751, acc.: 54.69%] [G loss: 0.394591]\n",
      "epoch:19 step:18616 [D loss: 0.233580, acc.: 57.03%] [G loss: 0.429625]\n",
      "epoch:19 step:18617 [D loss: 0.215065, acc.: 63.28%] [G loss: 0.425031]\n",
      "epoch:19 step:18618 [D loss: 0.219742, acc.: 67.19%] [G loss: 0.455558]\n",
      "epoch:19 step:18619 [D loss: 0.204165, acc.: 66.41%] [G loss: 0.468062]\n",
      "epoch:19 step:18620 [D loss: 0.257508, acc.: 53.12%] [G loss: 0.470260]\n",
      "epoch:19 step:18621 [D loss: 0.263570, acc.: 53.91%] [G loss: 0.435460]\n",
      "epoch:19 step:18622 [D loss: 0.220268, acc.: 64.84%] [G loss: 0.412298]\n",
      "epoch:19 step:18623 [D loss: 0.243471, acc.: 55.47%] [G loss: 0.418236]\n",
      "epoch:19 step:18624 [D loss: 0.235873, acc.: 60.94%] [G loss: 0.419833]\n",
      "epoch:19 step:18625 [D loss: 0.215133, acc.: 64.06%] [G loss: 0.427853]\n",
      "epoch:19 step:18626 [D loss: 0.183252, acc.: 72.66%] [G loss: 0.442430]\n",
      "epoch:19 step:18627 [D loss: 0.240038, acc.: 60.16%] [G loss: 0.373743]\n",
      "epoch:19 step:18628 [D loss: 0.214402, acc.: 69.53%] [G loss: 0.397111]\n",
      "epoch:19 step:18629 [D loss: 0.231852, acc.: 60.16%] [G loss: 0.434148]\n",
      "epoch:19 step:18630 [D loss: 0.247336, acc.: 57.03%] [G loss: 0.412030]\n",
      "epoch:19 step:18631 [D loss: 0.245520, acc.: 56.25%] [G loss: 0.441053]\n",
      "epoch:19 step:18632 [D loss: 0.240908, acc.: 57.81%] [G loss: 0.433848]\n",
      "epoch:19 step:18633 [D loss: 0.218687, acc.: 64.84%] [G loss: 0.472174]\n",
      "epoch:19 step:18634 [D loss: 0.196901, acc.: 71.88%] [G loss: 0.433711]\n",
      "epoch:19 step:18635 [D loss: 0.221149, acc.: 63.28%] [G loss: 0.416113]\n",
      "epoch:19 step:18636 [D loss: 0.199235, acc.: 70.31%] [G loss: 0.438033]\n",
      "epoch:19 step:18637 [D loss: 0.245179, acc.: 58.59%] [G loss: 0.370992]\n",
      "epoch:19 step:18638 [D loss: 0.213325, acc.: 67.97%] [G loss: 0.440488]\n",
      "epoch:19 step:18639 [D loss: 0.223124, acc.: 60.16%] [G loss: 0.416384]\n",
      "epoch:19 step:18640 [D loss: 0.204677, acc.: 69.53%] [G loss: 0.427146]\n",
      "epoch:19 step:18641 [D loss: 0.211927, acc.: 67.97%] [G loss: 0.438648]\n",
      "epoch:19 step:18642 [D loss: 0.217070, acc.: 62.50%] [G loss: 0.401012]\n",
      "epoch:19 step:18643 [D loss: 0.218457, acc.: 66.41%] [G loss: 0.453210]\n",
      "epoch:19 step:18644 [D loss: 0.205900, acc.: 68.75%] [G loss: 0.478638]\n",
      "epoch:19 step:18645 [D loss: 0.185218, acc.: 78.12%] [G loss: 0.473653]\n",
      "epoch:19 step:18646 [D loss: 0.262705, acc.: 51.56%] [G loss: 0.406969]\n",
      "epoch:19 step:18647 [D loss: 0.231303, acc.: 62.50%] [G loss: 0.408145]\n",
      "epoch:19 step:18648 [D loss: 0.215327, acc.: 61.72%] [G loss: 0.447670]\n",
      "epoch:19 step:18649 [D loss: 0.239378, acc.: 53.91%] [G loss: 0.428207]\n",
      "epoch:19 step:18650 [D loss: 0.259042, acc.: 52.34%] [G loss: 0.397236]\n",
      "epoch:19 step:18651 [D loss: 0.222786, acc.: 63.28%] [G loss: 0.422691]\n",
      "epoch:19 step:18652 [D loss: 0.217260, acc.: 60.94%] [G loss: 0.461236]\n",
      "epoch:19 step:18653 [D loss: 0.243862, acc.: 53.91%] [G loss: 0.447273]\n",
      "epoch:19 step:18654 [D loss: 0.231273, acc.: 58.59%] [G loss: 0.434927]\n",
      "epoch:19 step:18655 [D loss: 0.223586, acc.: 64.84%] [G loss: 0.413712]\n",
      "epoch:19 step:18656 [D loss: 0.225812, acc.: 57.03%] [G loss: 0.444104]\n",
      "epoch:19 step:18657 [D loss: 0.220021, acc.: 66.41%] [G loss: 0.425737]\n",
      "epoch:19 step:18658 [D loss: 0.230398, acc.: 57.03%] [G loss: 0.395988]\n",
      "epoch:19 step:18659 [D loss: 0.230353, acc.: 64.84%] [G loss: 0.401925]\n",
      "epoch:19 step:18660 [D loss: 0.203418, acc.: 71.88%] [G loss: 0.462786]\n",
      "epoch:19 step:18661 [D loss: 0.260963, acc.: 48.44%] [G loss: 0.409108]\n",
      "epoch:19 step:18662 [D loss: 0.252841, acc.: 54.69%] [G loss: 0.430533]\n",
      "epoch:19 step:18663 [D loss: 0.228514, acc.: 64.06%] [G loss: 0.427270]\n",
      "epoch:19 step:18664 [D loss: 0.264617, acc.: 53.91%] [G loss: 0.387047]\n",
      "epoch:19 step:18665 [D loss: 0.238204, acc.: 53.12%] [G loss: 0.396056]\n",
      "epoch:19 step:18666 [D loss: 0.198640, acc.: 71.88%] [G loss: 0.423611]\n",
      "epoch:19 step:18667 [D loss: 0.235913, acc.: 58.59%] [G loss: 0.433205]\n",
      "epoch:19 step:18668 [D loss: 0.230071, acc.: 57.81%] [G loss: 0.415225]\n",
      "epoch:19 step:18669 [D loss: 0.216891, acc.: 65.62%] [G loss: 0.421258]\n",
      "epoch:19 step:18670 [D loss: 0.257656, acc.: 53.91%] [G loss: 0.394874]\n",
      "epoch:19 step:18671 [D loss: 0.243118, acc.: 57.81%] [G loss: 0.401779]\n",
      "epoch:19 step:18672 [D loss: 0.223701, acc.: 64.84%] [G loss: 0.425303]\n",
      "epoch:19 step:18673 [D loss: 0.217882, acc.: 64.84%] [G loss: 0.436648]\n",
      "epoch:19 step:18674 [D loss: 0.205122, acc.: 67.97%] [G loss: 0.446015]\n",
      "epoch:19 step:18675 [D loss: 0.222180, acc.: 65.62%] [G loss: 0.428251]\n",
      "epoch:19 step:18676 [D loss: 0.236100, acc.: 57.03%] [G loss: 0.442746]\n",
      "epoch:19 step:18677 [D loss: 0.211831, acc.: 63.28%] [G loss: 0.451937]\n",
      "epoch:19 step:18678 [D loss: 0.206404, acc.: 64.84%] [G loss: 0.449733]\n",
      "epoch:19 step:18679 [D loss: 0.242574, acc.: 56.25%] [G loss: 0.416406]\n",
      "epoch:19 step:18680 [D loss: 0.223855, acc.: 66.41%] [G loss: 0.404382]\n",
      "epoch:19 step:18681 [D loss: 0.217487, acc.: 65.62%] [G loss: 0.409532]\n",
      "epoch:19 step:18682 [D loss: 0.240232, acc.: 57.81%] [G loss: 0.395642]\n",
      "epoch:19 step:18683 [D loss: 0.237037, acc.: 57.81%] [G loss: 0.409516]\n",
      "epoch:19 step:18684 [D loss: 0.236639, acc.: 54.69%] [G loss: 0.411692]\n",
      "epoch:19 step:18685 [D loss: 0.224482, acc.: 60.16%] [G loss: 0.403747]\n",
      "epoch:19 step:18686 [D loss: 0.233804, acc.: 63.28%] [G loss: 0.408350]\n",
      "epoch:19 step:18687 [D loss: 0.211639, acc.: 59.38%] [G loss: 0.447012]\n",
      "epoch:19 step:18688 [D loss: 0.204098, acc.: 68.75%] [G loss: 0.450079]\n",
      "epoch:19 step:18689 [D loss: 0.217125, acc.: 60.16%] [G loss: 0.477850]\n",
      "epoch:19 step:18690 [D loss: 0.230448, acc.: 57.03%] [G loss: 0.432747]\n",
      "epoch:19 step:18691 [D loss: 0.217898, acc.: 60.16%] [G loss: 0.473021]\n",
      "epoch:19 step:18692 [D loss: 0.203847, acc.: 67.19%] [G loss: 0.451750]\n",
      "epoch:19 step:18693 [D loss: 0.195952, acc.: 69.53%] [G loss: 0.455218]\n",
      "epoch:19 step:18694 [D loss: 0.265663, acc.: 53.12%] [G loss: 0.444990]\n",
      "epoch:19 step:18695 [D loss: 0.249253, acc.: 58.59%] [G loss: 0.403313]\n",
      "epoch:19 step:18696 [D loss: 0.232919, acc.: 61.72%] [G loss: 0.423160]\n",
      "epoch:19 step:18697 [D loss: 0.230486, acc.: 60.16%] [G loss: 0.422937]\n",
      "epoch:19 step:18698 [D loss: 0.201622, acc.: 68.75%] [G loss: 0.430151]\n",
      "epoch:19 step:18699 [D loss: 0.223478, acc.: 65.62%] [G loss: 0.435320]\n",
      "epoch:19 step:18700 [D loss: 0.214857, acc.: 62.50%] [G loss: 0.428228]\n",
      "epoch:19 step:18701 [D loss: 0.206166, acc.: 68.75%] [G loss: 0.440878]\n",
      "epoch:19 step:18702 [D loss: 0.204371, acc.: 71.88%] [G loss: 0.451411]\n",
      "epoch:19 step:18703 [D loss: 0.201158, acc.: 67.97%] [G loss: 0.478090]\n",
      "epoch:19 step:18704 [D loss: 0.214227, acc.: 61.72%] [G loss: 0.443172]\n",
      "epoch:19 step:18705 [D loss: 0.246694, acc.: 54.69%] [G loss: 0.413127]\n",
      "epoch:19 step:18706 [D loss: 0.246702, acc.: 58.59%] [G loss: 0.426492]\n",
      "epoch:19 step:18707 [D loss: 0.202482, acc.: 68.75%] [G loss: 0.469622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18708 [D loss: 0.209754, acc.: 68.75%] [G loss: 0.489374]\n",
      "epoch:19 step:18709 [D loss: 0.192014, acc.: 72.66%] [G loss: 0.464290]\n",
      "epoch:19 step:18710 [D loss: 0.239914, acc.: 58.59%] [G loss: 0.431464]\n",
      "epoch:19 step:18711 [D loss: 0.222892, acc.: 63.28%] [G loss: 0.445209]\n",
      "epoch:19 step:18712 [D loss: 0.204650, acc.: 70.31%] [G loss: 0.471649]\n",
      "epoch:19 step:18713 [D loss: 0.254148, acc.: 54.69%] [G loss: 0.442458]\n",
      "epoch:19 step:18714 [D loss: 0.227134, acc.: 59.38%] [G loss: 0.437453]\n",
      "epoch:19 step:18715 [D loss: 0.228482, acc.: 60.16%] [G loss: 0.450032]\n",
      "epoch:19 step:18716 [D loss: 0.223791, acc.: 59.38%] [G loss: 0.453589]\n",
      "epoch:19 step:18717 [D loss: 0.235493, acc.: 53.91%] [G loss: 0.427673]\n",
      "epoch:19 step:18718 [D loss: 0.253912, acc.: 54.69%] [G loss: 0.427318]\n",
      "epoch:19 step:18719 [D loss: 0.205811, acc.: 70.31%] [G loss: 0.403357]\n",
      "epoch:19 step:18720 [D loss: 0.249477, acc.: 62.50%] [G loss: 0.408459]\n",
      "epoch:19 step:18721 [D loss: 0.177456, acc.: 78.12%] [G loss: 0.525486]\n",
      "epoch:19 step:18722 [D loss: 0.185991, acc.: 78.12%] [G loss: 0.542252]\n",
      "epoch:19 step:18723 [D loss: 0.287170, acc.: 50.78%] [G loss: 0.455074]\n",
      "epoch:19 step:18724 [D loss: 0.203400, acc.: 68.75%] [G loss: 0.483841]\n",
      "epoch:19 step:18725 [D loss: 0.237999, acc.: 62.50%] [G loss: 0.413109]\n",
      "epoch:19 step:18726 [D loss: 0.199729, acc.: 68.75%] [G loss: 0.430651]\n",
      "epoch:19 step:18727 [D loss: 0.176919, acc.: 74.22%] [G loss: 0.485323]\n",
      "epoch:19 step:18728 [D loss: 0.189208, acc.: 75.00%] [G loss: 0.490462]\n",
      "epoch:19 step:18729 [D loss: 0.185800, acc.: 69.53%] [G loss: 0.495636]\n",
      "epoch:19 step:18730 [D loss: 0.199241, acc.: 68.75%] [G loss: 0.475228]\n",
      "epoch:19 step:18731 [D loss: 0.306429, acc.: 58.59%] [G loss: 0.508192]\n",
      "epoch:19 step:18732 [D loss: 0.220736, acc.: 67.19%] [G loss: 0.492732]\n",
      "epoch:19 step:18733 [D loss: 0.211943, acc.: 69.53%] [G loss: 0.538318]\n",
      "epoch:19 step:18734 [D loss: 0.221918, acc.: 64.84%] [G loss: 0.514643]\n",
      "epoch:19 step:18735 [D loss: 0.275135, acc.: 52.34%] [G loss: 0.430996]\n",
      "epoch:19 step:18736 [D loss: 0.198033, acc.: 71.88%] [G loss: 0.462418]\n",
      "epoch:19 step:18737 [D loss: 0.227313, acc.: 64.06%] [G loss: 0.459983]\n",
      "epoch:19 step:18738 [D loss: 0.222111, acc.: 66.41%] [G loss: 0.477479]\n",
      "epoch:19 step:18739 [D loss: 0.177985, acc.: 73.44%] [G loss: 0.503263]\n",
      "epoch:19 step:18740 [D loss: 0.180990, acc.: 71.88%] [G loss: 0.545628]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "from __future__ import print_function, division\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "import util\n",
    "import utils\n",
    "import tensorflow.contrib.gan as tfgan\n",
    "num_images_to_eval = 500\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, imgs, transform=None):\n",
    "        # super().__init__()\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = torch.from_numpy(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import ot\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.models as models\n",
    "\n",
    "from scipy import linalg\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def giveName(iter):  # 7 digit name.\n",
    "    ans = str(iter)\n",
    "    return ans.zfill(7)\n",
    "\n",
    "def make_dataset(dataset, dataroot, imageSize):\n",
    "    \"\"\"\n",
    "    :param dataset: must be in 'cifar10 | lsun | imagenet | folder | lfw | fake'\n",
    "    :return: pytorch dataset for DataLoader to utilize\n",
    "    \"\"\"\n",
    "    if dataset in ['imagenet', 'folder', 'lfw']:\n",
    "        print(os.getcwd() + dataroot)  # \n",
    "        # folder dataset\n",
    "        # dataset = dset.ImageFolder(root=dataroot,\n",
    "        dataset = dset.ImageFolder(root=os.getcwd() + dataroot,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.Resize(imageSize),\n",
    "                                       # transforms.CenterCrop(imageSize),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(\n",
    "                                           (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                   ]))\n",
    "    elif dataset == 'lsun':\n",
    "        dataset = dset.LSUN(db_path=dataroot, classes=['bedroom_train'],\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.Resize(imageSize),\n",
    "                                transforms.CenterCrop(imageSize),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                            ]))\n",
    "    elif dataset == 'cifar10':\n",
    "        dataset = dset.CIFAR10(root=dataroot, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Resize(imageSize),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize(\n",
    "                                       (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                               ]))\n",
    "    elif dataset == 'celeba':\n",
    "        dataset = dset.ImageFolder(root=dataroot,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.CenterCrop(138),\n",
    "                                       transforms.Resize(imageSize),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(\n",
    "                                           (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                   ]))\n",
    "    else:\n",
    "        raise Exception('--dataset must be in cifar10 | lsun | imagenet | folder | lfw | fake')\n",
    "    assert dataset\n",
    "    return dataset\n",
    "\n",
    "MNIST_CLASSIFIER_FROZEN_GRAPH = './classify_mnist_graph_def.pb'\n",
    "INPUT_TENSOR = 'inputs:0'\n",
    "OUTPUT_TENSOR = 'logits:0'\n",
    "# CONV_TENSOR = 'fc3/Relu:0'\n",
    "CONV_TENSOR = 'fc4/BiasAdd:0'\n",
    "class ConvNetFeatureSaver(object):\n",
    "    def __init__(self, model='cnn', workers=4, batchSize=64):\n",
    "        '''\n",
    "        model: inception_v3, vgg13, vgg16, vgg19, resnet18, resnet34,\n",
    "               resnet50, resnet101, or resnet152\n",
    "        '''\n",
    "        self.model = model\n",
    "        self.batch_size = batchSize\n",
    "        self.workers = workers\n",
    "        if self.model.find('tfgan') >= 0:\n",
    "            print('tfgan')\n",
    "\n",
    "        elif self.model.find('vgg') >= 0:\n",
    "            self.vgg = getattr(models, model)(pretrained=True).cuda().eval()\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                     (0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "        elif self.model.find('resnet') >= 0:\n",
    "            resnet = getattr(models, model)(pretrained=True)\n",
    "            resnet.cuda().eval()\n",
    "            resnet_feature = nn.Sequential(resnet.conv1, resnet.bn1,\n",
    "                                           resnet.relu,\n",
    "                                           resnet.maxpool, resnet.layer1,\n",
    "                                           resnet.layer2, resnet.layer3,\n",
    "                                           resnet.layer4).cuda().eval()\n",
    "            self.resnet = resnet\n",
    "            self.resnet_feature = resnet_feature\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                     (0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "        elif self.model == 'inception' or self.model == 'inception_v3':\n",
    "            inception = models.inception_v3(\n",
    "                pretrained=True, transform_input=False).cuda().eval()\n",
    "            inception_feature = nn.Sequential(inception.Conv2d_1a_3x3,\n",
    "                                              inception.Conv2d_2a_3x3,\n",
    "                                              inception.Conv2d_2b_3x3,\n",
    "                                              nn.MaxPool2d(3, 2),\n",
    "                                              inception.Conv2d_3b_1x1,\n",
    "                                              inception.Conv2d_4a_3x3,\n",
    "                                              nn.MaxPool2d(3, 2),\n",
    "                                              inception.Mixed_5b,\n",
    "                                              inception.Mixed_5c,\n",
    "                                              inception.Mixed_5d,\n",
    "                                              inception.Mixed_6a,\n",
    "                                              inception.Mixed_6b,\n",
    "                                              inception.Mixed_6c,\n",
    "                                              inception.Mixed_6d,\n",
    "                                              inception.Mixed_7a,\n",
    "                                              inception.Mixed_7b,\n",
    "                                              inception.Mixed_7c,\n",
    "                                              ).cuda().eval()\n",
    "            self.inception = inception\n",
    "            self.inception_feature = inception_feature\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.Resize(299),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def save(self, imgFolder, dataloader, save2disk=False):\n",
    "        feature_pixl, feature_conv, feature_smax, feature_logit = [], [], [], []\n",
    "\n",
    "        for img in dataloader:\n",
    "            with torch.no_grad():\n",
    "                input = img.cuda()\n",
    "                if self.model == 'tfgan':\n",
    "                    gen_imgs = np.array(img)\n",
    "                    eval_images = tf.convert_to_tensor(gen_imgs)\n",
    "                    flogit = util.mnist_logits(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH, INPUT_TENSOR, OUTPUT_TENSOR)\n",
    "                    fconv = util.mnist_logits(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH, INPUT_TENSOR, CONV_TENSOR)\n",
    "                    flogit,fconv=tf.Session().run([flogit,fconv])\n",
    "\n",
    "                    flogit=torch.from_numpy(flogit)\n",
    "                    fconv=torch.from_numpy(fconv)\n",
    "                elif self.model == 'vgg' or self.model == 'vgg16':\n",
    "                    print(self.vgg.features(input).shape)\n",
    "                    fconv = self.vgg.features(input).view(input.size(0), -1)  # reshape\n",
    "                    flogit = self.vgg.classifier(fconv)\n",
    "                    # flogit = self.vgg.logitifier(fconv)\n",
    "                elif self.model.find('resnet') >= 0:\n",
    "                    fconv = self.resnet_feature(\n",
    "                        input).mean(3).mean(2).squeeze()\n",
    "                    flogit = self.resnet.fc(fconv)\n",
    "                elif self.model == 'inception' or self.model == 'inception_v3':\n",
    "                    fconv = self.inception_feature(\n",
    "                        input).mean(3).mean(2).squeeze()\n",
    "                    flogit = self.inception.fc(fconv)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                fsmax = F.softmax(flogit)\n",
    "                '''\n",
    "                1.feature_pixl 2.feature_conv 3.feature_logit 4.feature_smax\n",
    "                '''\n",
    "                feature_pixl.append(img)\n",
    "                feature_conv.append(fconv.data.cpu())\n",
    "                feature_logit.append(flogit.data.cpu())\n",
    "                feature_smax.append(fsmax.data.cpu())\n",
    "\n",
    "        feature_pixl = torch.cat(feature_pixl, 0).to('cpu')\n",
    "        feature_conv = torch.cat(feature_conv, 0).to('cpu')\n",
    "        feature_logit = torch.cat(feature_logit, 0).to('cpu')\n",
    "        feature_smax = torch.cat(feature_smax, 0).to('cpu')\n",
    "\n",
    "        return feature_pixl, feature_conv, feature_logit, feature_smax\n",
    "\n",
    "    # return feature_pixl, feature_conv, feature_logit, feature_smax\n",
    "\n",
    "\n",
    "def distance(X, Y, sqrt):\n",
    "    nX = X.size(0)\n",
    "    nY = Y.size(0)\n",
    "    X = X.view(nX, -1)\n",
    "    X2 = (X * X).sum(1).resize_(nX, 1)\n",
    "    Y = Y.view(nY, -1)\n",
    "    Y2 = (Y * Y).sum(1).resize_(nY, 1)\n",
    "\n",
    "    M = torch.zeros(nX, nY)\n",
    "    M.copy_(X2.expand(nX, nY) + Y2.expand(nY, nX).transpose(0, 1) -\n",
    "            2 * torch.mm(X, Y.transpose(0, 1)))\n",
    "\n",
    "    del X, X2, Y, Y2\n",
    "\n",
    "    if sqrt:\n",
    "        M = ((M + M.abs()) / 2).sqrt()\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "def wasserstein(M, sqrt):\n",
    "    if sqrt:\n",
    "        M = M.abs().sqrt()\n",
    "    emd = ot.emd2([], [], M.numpy())\n",
    "\n",
    "    return emd\n",
    "\n",
    "\n",
    "class Score_knn:\n",
    "    acc = 0\n",
    "    acc_real = 0\n",
    "    acc_fake = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    ft = 0\n",
    "\n",
    "\n",
    "def knn(Mxx, Mxy, Myy, k, sqrt):\n",
    "    n0 = Mxx.size(0)\n",
    "    n1 = Myy.size(0)\n",
    "    label = torch.cat((torch.ones(n0), torch.zeros(n1)))\n",
    "    M = torch.cat((torch.cat((Mxx, Mxy), 1), torch.cat(\n",
    "        (Mxy.transpose(0, 1), Myy), 1)), 0)\n",
    "    if sqrt:\n",
    "        M = M.abs().sqrt()\n",
    "    INFINITY = float('inf')\n",
    "    val, idx = (M + torch.diag(INFINITY * torch.ones(n0 + n1))\n",
    "                ).topk(k, 0, False)\n",
    "\n",
    "    count = torch.zeros(n0 + n1)\n",
    "    for i in range(0, k):\n",
    "        count = count + label.index_select(0, idx[i])\n",
    "    pred = torch.ge(count, (float(k) / 2) * torch.ones(n0 + n1)).float()\n",
    "\n",
    "    s = Score_knn()\n",
    "    s.tp = (pred * label).sum()\n",
    "    s.fp = (pred * (1 - label)).sum()\n",
    "    s.fn = ((1 - pred) * label).sum()\n",
    "    s.tn = ((1 - pred) * (1 - label)).sum()\n",
    "    s.precision = s.tp / (s.tp + s.fp + 1e-10)\n",
    "    s.recall = s.tp / (s.tp + s.fn + 1e-10)\n",
    "    s.acc_t = s.tp / (s.tp + s.fn)\n",
    "    s.acc_f = s.tn / (s.tn + s.fp)\n",
    "    s.acc = torch.eq(label, pred).float().mean()\n",
    "    s.k = k\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def mmd(Mxx, Mxy, Myy, sigma):\n",
    "    scale = Mxx.mean()\n",
    "    Mxx = torch.exp(-Mxx / (scale * 2 * sigma * sigma))\n",
    "    Mxy = torch.exp(-Mxy / (scale * 2 * sigma * sigma))\n",
    "    Myy = torch.exp(-Myy / (scale * 2 * sigma * sigma))\n",
    "    mmd = math.sqrt(Mxx.mean() + Myy.mean() - 2 * Mxy.mean())\n",
    "\n",
    "    return mmd\n",
    "\n",
    "\n",
    "def entropy_score(X, Y, epsilons):\n",
    "    Mxy = distance(X, Y, False)\n",
    "    scores = []\n",
    "    for epsilon in epsilons:\n",
    "        scores.append(ent(Mxy.t(), epsilon))\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def ent(M, epsilon):\n",
    "    n0 = M.size(0)\n",
    "    n1 = M.size(1)\n",
    "    neighbors = M.lt(epsilon).float()\n",
    "    sums = neighbors.sum(0).repeat(n0, 1)\n",
    "    sums[sums.eq(0)] = 1\n",
    "    neighbors = neighbors.div(sums)\n",
    "    probs = neighbors.sum(1) / n1\n",
    "    rem = 1 - probs.sum()\n",
    "    if rem < 0:\n",
    "        rem = 0\n",
    "    probs = torch.cat((probs, rem * torch.ones(1)), 0)\n",
    "    e = {}\n",
    "    e['probs'] = probs\n",
    "    probs = probs[probs.gt(0)]\n",
    "    e['ent'] = -probs.mul(probs.log()).sum()\n",
    "\n",
    "    return e\n",
    "\n",
    "\n",
    "eps = 1e-20\n",
    "\n",
    "\n",
    "def inception_score(X):\n",
    "    kl = X * ((X + eps).log() - (X.mean(0) + eps).log().expand_as(X))\n",
    "    score = np.exp(kl.sum(1).mean())\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def mode_score(X, Y):\n",
    "    kl1 = X * ((X + eps).log() - (X.mean(0) + eps).log().expand_as(X))\n",
    "    kl2 = X.mean(0) * ((X.mean(0) + eps).log() - (Y.mean(0) + eps).log())\n",
    "    score = np.exp(kl1.sum(1).mean() - kl2.sum())\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def fid(X, Y):\n",
    "    m = X.mean(0)\n",
    "    m_w = Y.mean(0)\n",
    "    X_np = X.numpy()\n",
    "    Y_np = Y.numpy()\n",
    "\n",
    "    C = np.cov(X_np.transpose())\n",
    "    C_w = np.cov(Y_np.transpose())\n",
    "    C_C_w_sqrt = linalg.sqrtm(C.dot(C_w), True).real\n",
    "\n",
    "    score = m.dot(m) + m_w.dot(m_w) - 2 * m_w.dot(m) + \\\n",
    "            np.trace(C + C_w - 2 * C_C_w_sqrt)\n",
    "    return np.sqrt(score)\n",
    "\n",
    "\n",
    "class Score:\n",
    "    emd = 0\n",
    "    mmd = 0\n",
    "    knn = None\n",
    "\n",
    "\n",
    "def compute_score(real, fake, k=1, sigma=1, sqrt=True):\n",
    "    Mxx = distance(real, real, False)\n",
    "    Mxy = distance(real, fake, False)\n",
    "    Myy = distance(fake, fake, False)\n",
    "\n",
    "    s = Score()\n",
    "    s.emd = wasserstein(Mxy, sqrt)\n",
    "    s.mmd = mmd(Mxx, Mxy, Myy, sigma)\n",
    "    s.knn = knn(Mxx, Mxy, Myy, k, sqrt)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "dataset:path\n",
    "imageSize:\n",
    "dataroot_real:path\n",
    "batchSize\n",
    "saveFolder_r:\n",
    "conv_model:\n",
    "'''\n",
    "\n",
    "\n",
    "def compute_score_raw(real_dataloader, fake_dataloader, batchSize, saveFolder_r, saveFolder_f, conv_model='resnet34',\n",
    "                      workers=4):\n",
    "    convnet_feature_saver = ConvNetFeatureSaver(model=conv_model,\n",
    "                                                batchSize=batchSize, workers=workers)\n",
    "    print(saveFolder_r)\n",
    "    print(saveFolder_f)\n",
    "    feature_r = convnet_feature_saver.save(saveFolder_r, real_dataloader, False)\n",
    "    feature_f = convnet_feature_saver.save(saveFolder_f, fake_dataloader, False)\n",
    "\n",
    "    # 4 feature spaces and 7 scores + incep + modescore + fid\n",
    "    score = np.zeros(2 * 7 + 5)\n",
    "    for i in range(0, 2):\n",
    "        print('compute score in space: ' + str(i))\n",
    "        Mxx = distance(feature_r[i], feature_r[i], False)\n",
    "        Mxy = distance(feature_r[i], feature_f[i], False)\n",
    "        Myy = distance(feature_f[i], feature_f[i], False)\n",
    "\n",
    "        score[i * 7] = wasserstein(Mxy, True)\n",
    "        score[i * 7 + 1] = mmd(Mxx, Mxy, Myy, 1)\n",
    "        tmp = knn(Mxx, Mxy, Myy, 1, False)\n",
    "        score[(i * 7 + 2):(i * 7 + 7)] = \\\n",
    "            tmp.acc, tmp.acc_t, tmp.acc_f, tmp.precision, tmp.recall\n",
    "\n",
    "\n",
    "    score[14] = inception_score(feature_f[3])\n",
    "    score[15] = mode_score(feature_r[3], feature_f[3])\n",
    "    score[16] = fid(feature_r[3], feature_f[3])\n",
    "\n",
    "    return score\n",
    "labels_name=['w_pixl','mmd_pixl','acc_pixl','acc_t_pixl','acc_f_pixl','acc_precision_pixl','acc_recall_pixl',\n",
    "             'w_conv','mmd_conv','acc_conv','acc_t_conv','acc_f_conv','acc_precision_conv','acc_recall_conv',\n",
    "             'is','mode_score','fid' ,'tf_is','tf_fid']\n",
    "if not os.path.isdir('saved_models_{}'.format('lsgan')):\n",
    "    os.mkdir('saved_models_{}'.format('lsgan'))\n",
    "f = open('saved_models_{}/log_collapse1.txt'.format('lsgan'), mode='w')\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class LSGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "        self.x = []\n",
    "        self.y = np.zeros((31, 1), dtype=np.int)\n",
    "        self.y = list(self.y)\n",
    "        for i in range(31):\n",
    "            self.y[i] = []\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        # (!!!) Optimize w.r.t. MSE loss instead of crossentropy\n",
    "        self.combined.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        # (!!!) No softmax\n",
    "        model.add(Dense(1))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (X_test, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        # Rescale -1 to 1\n",
    "        X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "        # X_test = X_test / 127.5 - 1.\n",
    "        X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                imgs = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"epoch:%d step:%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, global_step,d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                sampleSize = 10000\n",
    "                # If at save interval => save generated image samples\n",
    "                if global_step % sample_interval == 0:\n",
    "                    s = self.metrics(global_step, X_test, sampleSize)\n",
    "        for i in range(len(s)):\n",
    "            self.y[i] = [float(j) / max(self.y[i]) for j in self.y[i]]#\n",
    "\n",
    "        for i in range(len(s)):\n",
    "            font1={'size':8}\n",
    "\n",
    "            plt.plot(self.x, self.y[i], label=labels_name[i])\n",
    "            plt.legend(loc='lower right',prop=font1)\n",
    "            plt.savefig('saved_models_acgan/{}.png'.format(labels_name[i]))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "    def metrics(self, epoch, X_test, sampleSize):\n",
    "        self.x.append(epoch)\n",
    "        r, c = 10, sampleSize // 10\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "#         sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise])\n",
    "        x_dataset = MyDataset(X_test[:sampleSize])\n",
    "        # print(x_dataset[0].shape)\n",
    "        x_real_loader = Data.DataLoader(dataset=x_dataset, batch_size=2000, shuffle=True)\n",
    "        x_fake_dataset = MyDataset(gen_imgs)\n",
    "        x_fake_loader = Data.DataLoader(dataset=x_fake_dataset, batch_size=2000, shuffle=True)\n",
    "        s = compute_score_raw(x_real_loader, x_fake_loader, 256, '/real/', './fake', conv_model='tfgan',\n",
    "                              workers=int(1))\n",
    "        real_images = tf.convert_to_tensor(X_test)  # real images\n",
    "        # MNIST_CLASSIFIER_FROZEN_GRAPH = '.\\classify_mnist_graph_def.pb'\n",
    "        gen_imgs = np.array(gen_imgs)\n",
    "        eval_images = tf.convert_to_tensor(gen_imgs)\n",
    "        eval_score = utils.mnist_score(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH)  # IS score\n",
    "        frechet_distance = utils.mnist_frechet_distance(real_images, eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH)\n",
    "        mnist_score, f_distance = sess.run([eval_score, frechet_distance])\n",
    "        # print(mnist_score)\n",
    "        # print(f_distance)\n",
    "        # s[14]=mnist_score\n",
    "        # s[16]=f_distance\n",
    "        s[17] = mnist_score\n",
    "        s[18] = f_distance\n",
    "        print('IS socre: %f' % mnist_score)\n",
    "        print('FID: %f' % f_distance)\n",
    "\n",
    "        for i in range(len(s)):\n",
    "            print(i, \"=\", s[i])\n",
    "        for i in range(len(s)):\n",
    "            self.y[i].append(s[i])\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('epoch:' + str(epoch))\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('%.8f' % (i) for i in s)\n",
    "        f.writelines('\\n')\n",
    "        return s\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = LSGAN()\n",
    "    gan.train(epochs=20, batch_size=64, sample_interval=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
