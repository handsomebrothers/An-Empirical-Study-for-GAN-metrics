{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "from __future__ import print_function, division\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "import util\n",
    "import utils\n",
    "import tensorflow.contrib.gan as tfgan\n",
    "num_images_to_eval = 500\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, imgs, transform=None):\n",
    "        # super().__init__()\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = torch.from_numpy(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import ot\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.models as models\n",
    "\n",
    "from scipy import linalg\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def giveName(iter):  # 7 digit name.\n",
    "    ans = str(iter)\n",
    "    return ans.zfill(7)\n",
    "\n",
    "def make_dataset(dataset, dataroot, imageSize):\n",
    "    \"\"\"\n",
    "    :param dataset: must be in 'cifar10 | lsun | imagenet | folder | lfw | fake'\n",
    "    :return: pytorch dataset for DataLoader to utilize\n",
    "    \"\"\"\n",
    "    if dataset in ['imagenet', 'folder', 'lfw']:\n",
    "        print(os.getcwd() + dataroot)  # 函数的作用是用于返回当前工作目录\n",
    "        # folder dataset\n",
    "        # dataset = dset.ImageFolder(root=dataroot,\n",
    "        dataset = dset.ImageFolder(root=os.getcwd() + dataroot,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.Resize(imageSize),\n",
    "                                       # transforms.CenterCrop(imageSize),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(\n",
    "                                           (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                   ]))\n",
    "    elif dataset == 'lsun':\n",
    "        dataset = dset.LSUN(db_path=dataroot, classes=['bedroom_train'],\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.Resize(imageSize),\n",
    "                                transforms.CenterCrop(imageSize),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                            ]))\n",
    "    elif dataset == 'cifar10':\n",
    "        dataset = dset.CIFAR10(root=dataroot, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Resize(imageSize),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize(\n",
    "                                       (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                               ]))\n",
    "    elif dataset == 'celeba':\n",
    "        dataset = dset.ImageFolder(root=dataroot,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.CenterCrop(138),\n",
    "                                       transforms.Resize(imageSize),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(\n",
    "                                           (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                   ]))\n",
    "    else:\n",
    "        raise Exception('--dataset must be in cifar10 | lsun | imagenet | folder | lfw | fake')\n",
    "    assert dataset\n",
    "    return dataset\n",
    "\n",
    "MNIST_CLASSIFIER_FROZEN_GRAPH = './classify_mnist_graph_def.pb'\n",
    "INPUT_TENSOR = 'inputs:0'\n",
    "OUTPUT_TENSOR = 'logits:0'\n",
    "# CONV_TENSOR = 'fc3/Relu:0'\n",
    "CONV_TENSOR = 'fc4/BiasAdd:0'\n",
    "class ConvNetFeatureSaver(object):\n",
    "    def __init__(self, model='cnn', workers=4, batchSize=64):\n",
    "        '''\n",
    "        model: inception_v3, vgg13, vgg16, vgg19, resnet18, resnet34,\n",
    "               resnet50, resnet101, or resnet152\n",
    "        '''\n",
    "        self.model = model\n",
    "        self.batch_size = batchSize\n",
    "        self.workers = workers\n",
    "        if self.model.find('tfgan') >= 0:\n",
    "            print('tfgan')\n",
    "\n",
    "        elif self.model.find('vgg') >= 0:\n",
    "            self.vgg = getattr(models, model)(pretrained=True).cuda().eval()\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                     (0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "        elif self.model.find('resnet') >= 0:\n",
    "            resnet = getattr(models, model)(pretrained=True)\n",
    "            resnet.cuda().eval()\n",
    "            resnet_feature = nn.Sequential(resnet.conv1, resnet.bn1,\n",
    "                                           resnet.relu,\n",
    "                                           resnet.maxpool, resnet.layer1,\n",
    "                                           resnet.layer2, resnet.layer3,\n",
    "                                           resnet.layer4).cuda().eval()\n",
    "            self.resnet = resnet\n",
    "            self.resnet_feature = resnet_feature\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                     (0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "        elif self.model == 'inception' or self.model == 'inception_v3':\n",
    "            inception = models.inception_v3(\n",
    "                pretrained=True, transform_input=False).cuda().eval()\n",
    "            inception_feature = nn.Sequential(inception.Conv2d_1a_3x3,\n",
    "                                              inception.Conv2d_2a_3x3,\n",
    "                                              inception.Conv2d_2b_3x3,\n",
    "                                              nn.MaxPool2d(3, 2),\n",
    "                                              inception.Conv2d_3b_1x1,\n",
    "                                              inception.Conv2d_4a_3x3,\n",
    "                                              nn.MaxPool2d(3, 2),\n",
    "                                              inception.Mixed_5b,\n",
    "                                              inception.Mixed_5c,\n",
    "                                              inception.Mixed_5d,\n",
    "                                              inception.Mixed_6a,\n",
    "                                              inception.Mixed_6b,\n",
    "                                              inception.Mixed_6c,\n",
    "                                              inception.Mixed_6d,\n",
    "                                              inception.Mixed_7a,\n",
    "                                              inception.Mixed_7b,\n",
    "                                              inception.Mixed_7c,\n",
    "                                              ).cuda().eval()\n",
    "            self.inception = inception\n",
    "            self.inception_feature = inception_feature\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.Resize(299),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def save(self, imgFolder, dataloader, save2disk=False):\n",
    "        feature_pixl, feature_conv, feature_smax, feature_logit = [], [], [], []\n",
    "\n",
    "        for img in dataloader:\n",
    "            with torch.no_grad():\n",
    "                input = img.cuda()\n",
    "                if self.model == 'tfgan':\n",
    "                    gen_imgs = np.array(img)\n",
    "                    eval_images = tf.convert_to_tensor(gen_imgs)\n",
    "                    flogit = util.mnist_logits(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH, INPUT_TENSOR, OUTPUT_TENSOR)\n",
    "                    fconv = util.mnist_logits(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH, INPUT_TENSOR, CONV_TENSOR)\n",
    "                    flogit,fconv=tf.Session().run([flogit,fconv])\n",
    "\n",
    "                    flogit=torch.from_numpy(flogit)\n",
    "                    fconv=torch.from_numpy(fconv)\n",
    "                elif self.model == 'vgg' or self.model == 'vgg16':\n",
    "                    print(self.vgg.features(input).shape)\n",
    "                    fconv = self.vgg.features(input).view(input.size(0), -1)  # 相当于reshape\n",
    "                    flogit = self.vgg.classifier(fconv)\n",
    "                    # flogit = self.vgg.logitifier(fconv)\n",
    "                elif self.model.find('resnet') >= 0:\n",
    "                    fconv = self.resnet_feature(\n",
    "                        input).mean(3).mean(2).squeeze()\n",
    "                    flogit = self.resnet.fc(fconv)\n",
    "                elif self.model == 'inception' or self.model == 'inception_v3':\n",
    "                    fconv = self.inception_feature(\n",
    "                        input).mean(3).mean(2).squeeze()\n",
    "                    flogit = self.inception.fc(fconv)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                fsmax = F.softmax(flogit)\n",
    "                '''\n",
    "                总共有四个空间：1.feature_pixl 2.feature_conv 3.feature_logit 4.feature_smax\n",
    "                '''\n",
    "                feature_pixl.append(img)\n",
    "                feature_conv.append(fconv.data.cpu())\n",
    "                feature_logit.append(flogit.data.cpu())\n",
    "                feature_smax.append(fsmax.data.cpu())\n",
    "\n",
    "        feature_pixl = torch.cat(feature_pixl, 0).to('cpu')\n",
    "        feature_conv = torch.cat(feature_conv, 0).to('cpu')\n",
    "        feature_logit = torch.cat(feature_logit, 0).to('cpu')\n",
    "        feature_smax = torch.cat(feature_smax, 0).to('cpu')\n",
    "\n",
    "        return feature_pixl, feature_conv, feature_logit, feature_smax\n",
    "\n",
    "    # return feature_pixl, feature_conv, feature_logit, feature_smax\n",
    "\n",
    "\n",
    "def distance(X, Y, sqrt):\n",
    "    nX = X.size(0)\n",
    "    nY = Y.size(0)\n",
    "    X = X.view(nX, -1)\n",
    "    X2 = (X * X).sum(1).resize_(nX, 1)\n",
    "    Y = Y.view(nY, -1)\n",
    "    Y2 = (Y * Y).sum(1).resize_(nY, 1)\n",
    "\n",
    "    M = torch.zeros(nX, nY)\n",
    "    M.copy_(X2.expand(nX, nY) + Y2.expand(nY, nX).transpose(0, 1) -\n",
    "            2 * torch.mm(X, Y.transpose(0, 1)))\n",
    "\n",
    "    del X, X2, Y, Y2\n",
    "\n",
    "    if sqrt:\n",
    "        M = ((M + M.abs()) / 2).sqrt()\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "def wasserstein(M, sqrt):\n",
    "    if sqrt:\n",
    "        M = M.abs().sqrt()\n",
    "    emd = ot.emd2([], [], M.numpy())\n",
    "\n",
    "    return emd\n",
    "\n",
    "\n",
    "class Score_knn:\n",
    "    acc = 0\n",
    "    acc_real = 0\n",
    "    acc_fake = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    ft = 0\n",
    "\n",
    "\n",
    "def knn(Mxx, Mxy, Myy, k, sqrt):\n",
    "    n0 = Mxx.size(0)\n",
    "    n1 = Myy.size(0)\n",
    "    label = torch.cat((torch.ones(n0), torch.zeros(n1)))\n",
    "    M = torch.cat((torch.cat((Mxx, Mxy), 1), torch.cat(\n",
    "        (Mxy.transpose(0, 1), Myy), 1)), 0)\n",
    "    if sqrt:\n",
    "        M = M.abs().sqrt()\n",
    "    INFINITY = float('inf')\n",
    "    val, idx = (M + torch.diag(INFINITY * torch.ones(n0 + n1))\n",
    "                ).topk(k, 0, False)\n",
    "\n",
    "    count = torch.zeros(n0 + n1)\n",
    "    for i in range(0, k):\n",
    "        count = count + label.index_select(0, idx[i])\n",
    "    pred = torch.ge(count, (float(k) / 2) * torch.ones(n0 + n1)).float()\n",
    "\n",
    "    s = Score_knn()\n",
    "    s.tp = (pred * label).sum()\n",
    "    s.fp = (pred * (1 - label)).sum()\n",
    "    s.fn = ((1 - pred) * label).sum()\n",
    "    s.tn = ((1 - pred) * (1 - label)).sum()\n",
    "    s.precision = s.tp / (s.tp + s.fp + 1e-10)\n",
    "    s.recall = s.tp / (s.tp + s.fn + 1e-10)\n",
    "    s.acc_t = s.tp / (s.tp + s.fn)\n",
    "    s.acc_f = s.tn / (s.tn + s.fp)\n",
    "    s.acc = torch.eq(label, pred).float().mean()\n",
    "    s.k = k\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def mmd(Mxx, Mxy, Myy, sigma):\n",
    "    scale = Mxx.mean()\n",
    "    Mxx = torch.exp(-Mxx / (scale * 2 * sigma * sigma))\n",
    "    Mxy = torch.exp(-Mxy / (scale * 2 * sigma * sigma))\n",
    "    Myy = torch.exp(-Myy / (scale * 2 * sigma * sigma))\n",
    "    mmd = math.sqrt(Mxx.mean() + Myy.mean() - 2 * Mxy.mean())\n",
    "\n",
    "    return mmd\n",
    "\n",
    "\n",
    "def entropy_score(X, Y, epsilons):\n",
    "    Mxy = distance(X, Y, False)\n",
    "    scores = []\n",
    "    for epsilon in epsilons:\n",
    "        scores.append(ent(Mxy.t(), epsilon))\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def ent(M, epsilon):\n",
    "    n0 = M.size(0)\n",
    "    n1 = M.size(1)\n",
    "    neighbors = M.lt(epsilon).float()\n",
    "    sums = neighbors.sum(0).repeat(n0, 1)\n",
    "    sums[sums.eq(0)] = 1\n",
    "    neighbors = neighbors.div(sums)\n",
    "    probs = neighbors.sum(1) / n1\n",
    "    rem = 1 - probs.sum()\n",
    "    if rem < 0:\n",
    "        rem = 0\n",
    "    probs = torch.cat((probs, rem * torch.ones(1)), 0)\n",
    "    e = {}\n",
    "    e['probs'] = probs\n",
    "    probs = probs[probs.gt(0)]\n",
    "    e['ent'] = -probs.mul(probs.log()).sum()\n",
    "\n",
    "    return e\n",
    "\n",
    "\n",
    "eps = 1e-20\n",
    "\n",
    "\n",
    "def inception_score(X):\n",
    "    kl = X * ((X + eps).log() - (X.mean(0) + eps).log().expand_as(X))\n",
    "    score = np.exp(kl.sum(1).mean())\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def mode_score(X, Y):\n",
    "    kl1 = X * ((X + eps).log() - (X.mean(0) + eps).log().expand_as(X))\n",
    "    kl2 = X.mean(0) * ((X.mean(0) + eps).log() - (Y.mean(0) + eps).log())\n",
    "    score = np.exp(kl1.sum(1).mean() - kl2.sum())\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def fid(X, Y):\n",
    "    m = X.mean(0)\n",
    "    m_w = Y.mean(0)\n",
    "    X_np = X.numpy()\n",
    "    Y_np = Y.numpy()\n",
    "\n",
    "    C = np.cov(X_np.transpose())\n",
    "    C_w = np.cov(Y_np.transpose())\n",
    "    C_C_w_sqrt = linalg.sqrtm(C.dot(C_w), True).real\n",
    "\n",
    "    score = m.dot(m) + m_w.dot(m_w) - 2 * m_w.dot(m) + \\\n",
    "            np.trace(C + C_w - 2 * C_C_w_sqrt)\n",
    "    return np.sqrt(score)\n",
    "\n",
    "\n",
    "class Score:\n",
    "    emd = 0\n",
    "    mmd = 0\n",
    "    knn = None\n",
    "\n",
    "\n",
    "def compute_score(real, fake, k=1, sigma=1, sqrt=True):\n",
    "    Mxx = distance(real, real, False)\n",
    "    Mxy = distance(real, fake, False)\n",
    "    Myy = distance(fake, fake, False)\n",
    "\n",
    "    s = Score()\n",
    "    s.emd = wasserstein(Mxy, sqrt)\n",
    "    s.mmd = mmd(Mxx, Mxy, Myy, sigma)\n",
    "    s.knn = knn(Mxx, Mxy, Myy, k, sqrt)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "'''\n",
    "参数说明：\n",
    "dataset:真实数据集的path\n",
    "imageSize:图片的大小\n",
    "dataroot_real:真实数据所在的path\n",
    "batchSize\n",
    "saveFolder_r:真实数据的保存位置\n",
    "conv_model:卷积模型\n",
    "'''\n",
    "\n",
    "\n",
    "def compute_score_raw(real_dataloader, fake_dataloader, batchSize, saveFolder_r, saveFolder_f, conv_model='resnet34',\n",
    "                      workers=4):\n",
    "    convnet_feature_saver = ConvNetFeatureSaver(model=conv_model,\n",
    "                                                batchSize=batchSize, workers=workers)\n",
    "    print(saveFolder_r)\n",
    "    print(saveFolder_f)\n",
    "    feature_r = convnet_feature_saver.save(saveFolder_r, real_dataloader, False)\n",
    "    feature_f = convnet_feature_saver.save(saveFolder_f, fake_dataloader, False)\n",
    "\n",
    "    # 4 feature spaces and 7 scores + incep + modescore + fid\n",
    "    score = np.zeros(2 * 7 +5)\n",
    "    for i in range(0, 2):\n",
    "        print('compute score in space: ' + str(i))\n",
    "        Mxx = distance(feature_r[i], feature_r[i], False)\n",
    "        Mxy = distance(feature_r[i], feature_f[i], False)\n",
    "        Myy = distance(feature_f[i], feature_f[i], False)\n",
    "\n",
    "        score[i * 7] = wasserstein(Mxy, True)\n",
    "        score[i * 7 + 1] = mmd(Mxx, Mxy, Myy, 1)\n",
    "        tmp = knn(Mxx, Mxy, Myy, 1, False)\n",
    "        score[(i * 7 + 2):(i * 7 + 7)] = \\\n",
    "            tmp.acc, tmp.acc_t, tmp.acc_f, tmp.precision, tmp.recall\n",
    "\n",
    "\n",
    "    score[14] = inception_score(feature_f[3])\n",
    "    score[15] = mode_score(feature_r[3], feature_f[3])\n",
    "    score[16] = fid(feature_r[3], feature_f[3])\n",
    "\n",
    "    return score\n",
    "labels_name=['w_pixl','mmd_pixl','acc_pixl','acc_t_pixl','acc_f_pixl','acc_precision_pixl','acc_recall_pixl',\n",
    "             'w_conv','mmd_conv','acc_conv','acc_t_conv','acc_f_conv','acc_precision_conv','acc_recall_conv',\n",
    "             'is','mode_score','fid','tf_is' ,'tf_id']\n",
    "if not os.path.isdir('saved_models_{}'.format('bgan')):\n",
    "    os.mkdir('saved_models_{}'.format('bgan'))\n",
    "f = open('saved_models_{}/log_collapse1.txt'.format('bgan'), mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:1 [D loss: 0.934562, acc.: 3.91%] [G loss: 0.108778]\n",
      "epoch:0 step:2 [D loss: 0.447958, acc.: 69.53%] [G loss: 0.124142]\n",
      "epoch:0 step:3 [D loss: 0.329848, acc.: 82.03%] [G loss: 0.213751]\n",
      "epoch:0 step:4 [D loss: 0.242822, acc.: 96.09%] [G loss: 0.323146]\n",
      "epoch:0 step:5 [D loss: 0.216138, acc.: 97.66%] [G loss: 0.569473]\n",
      "epoch:0 step:6 [D loss: 0.194916, acc.: 98.44%] [G loss: 0.649422]\n",
      "epoch:0 step:7 [D loss: 0.180510, acc.: 99.22%] [G loss: 0.764949]\n",
      "epoch:0 step:8 [D loss: 0.171768, acc.: 99.22%] [G loss: 0.958265]\n",
      "epoch:0 step:9 [D loss: 0.144499, acc.: 100.00%] [G loss: 1.030847]\n",
      "epoch:0 step:10 [D loss: 0.127038, acc.: 100.00%] [G loss: 1.351921]\n",
      "epoch:0 step:11 [D loss: 0.120841, acc.: 100.00%] [G loss: 1.316136]\n",
      "epoch:0 step:12 [D loss: 0.114124, acc.: 100.00%] [G loss: 1.543276]\n",
      "epoch:0 step:13 [D loss: 0.115436, acc.: 100.00%] [G loss: 1.539082]\n",
      "epoch:0 step:14 [D loss: 0.098335, acc.: 100.00%] [G loss: 1.695999]\n",
      "epoch:0 step:15 [D loss: 0.091759, acc.: 100.00%] [G loss: 1.907974]\n",
      "epoch:0 step:16 [D loss: 0.080270, acc.: 100.00%] [G loss: 2.080670]\n",
      "epoch:0 step:17 [D loss: 0.096008, acc.: 100.00%] [G loss: 2.160395]\n",
      "epoch:0 step:18 [D loss: 0.084580, acc.: 100.00%] [G loss: 2.415860]\n",
      "epoch:0 step:19 [D loss: 0.072419, acc.: 100.00%] [G loss: 2.481134]\n",
      "epoch:0 step:20 [D loss: 0.068009, acc.: 100.00%] [G loss: 2.763773]\n",
      "epoch:0 step:21 [D loss: 0.071191, acc.: 100.00%] [G loss: 2.811243]\n",
      "epoch:0 step:22 [D loss: 0.070013, acc.: 100.00%] [G loss: 3.023939]\n",
      "epoch:0 step:23 [D loss: 0.056730, acc.: 100.00%] [G loss: 3.279730]\n",
      "epoch:0 step:24 [D loss: 0.056914, acc.: 100.00%] [G loss: 3.386643]\n",
      "epoch:0 step:25 [D loss: 0.046463, acc.: 100.00%] [G loss: 3.605335]\n",
      "epoch:0 step:26 [D loss: 0.042858, acc.: 100.00%] [G loss: 3.915881]\n",
      "epoch:0 step:27 [D loss: 0.047411, acc.: 100.00%] [G loss: 3.809504]\n",
      "epoch:0 step:28 [D loss: 0.041243, acc.: 100.00%] [G loss: 3.691290]\n",
      "epoch:0 step:29 [D loss: 0.042348, acc.: 100.00%] [G loss: 3.977818]\n",
      "epoch:0 step:30 [D loss: 0.039354, acc.: 100.00%] [G loss: 4.147614]\n",
      "epoch:0 step:31 [D loss: 0.038026, acc.: 100.00%] [G loss: 4.256677]\n",
      "epoch:0 step:32 [D loss: 0.032926, acc.: 100.00%] [G loss: 4.573868]\n",
      "epoch:0 step:33 [D loss: 0.041037, acc.: 100.00%] [G loss: 4.201356]\n",
      "epoch:0 step:34 [D loss: 0.035124, acc.: 100.00%] [G loss: 4.653516]\n",
      "epoch:0 step:35 [D loss: 0.029260, acc.: 100.00%] [G loss: 5.039865]\n",
      "epoch:0 step:36 [D loss: 0.032149, acc.: 100.00%] [G loss: 4.876858]\n",
      "epoch:0 step:37 [D loss: 0.034415, acc.: 100.00%] [G loss: 5.017227]\n",
      "epoch:0 step:38 [D loss: 0.030241, acc.: 100.00%] [G loss: 5.097940]\n",
      "epoch:0 step:39 [D loss: 0.030739, acc.: 100.00%] [G loss: 5.578309]\n",
      "epoch:0 step:40 [D loss: 0.025085, acc.: 100.00%] [G loss: 5.343553]\n",
      "epoch:0 step:41 [D loss: 0.025374, acc.: 100.00%] [G loss: 5.636444]\n",
      "epoch:0 step:42 [D loss: 0.027532, acc.: 100.00%] [G loss: 5.617662]\n",
      "epoch:0 step:43 [D loss: 0.030052, acc.: 100.00%] [G loss: 5.797374]\n",
      "epoch:0 step:44 [D loss: 0.036353, acc.: 100.00%] [G loss: 5.979676]\n",
      "epoch:0 step:45 [D loss: 0.021820, acc.: 100.00%] [G loss: 5.759971]\n",
      "epoch:0 step:46 [D loss: 0.029456, acc.: 100.00%] [G loss: 6.066267]\n",
      "epoch:0 step:47 [D loss: 0.023246, acc.: 100.00%] [G loss: 5.961103]\n",
      "epoch:0 step:48 [D loss: 0.024270, acc.: 100.00%] [G loss: 6.921487]\n",
      "epoch:0 step:49 [D loss: 0.022550, acc.: 100.00%] [G loss: 6.337838]\n",
      "epoch:0 step:50 [D loss: 0.019855, acc.: 100.00%] [G loss: 6.304087]\n",
      "epoch:0 step:51 [D loss: 0.021168, acc.: 100.00%] [G loss: 6.835666]\n",
      "epoch:0 step:52 [D loss: 0.022998, acc.: 100.00%] [G loss: 6.348286]\n",
      "epoch:0 step:53 [D loss: 0.022981, acc.: 100.00%] [G loss: 6.863216]\n",
      "epoch:0 step:54 [D loss: 0.021606, acc.: 100.00%] [G loss: 6.394692]\n",
      "epoch:0 step:55 [D loss: 0.020595, acc.: 100.00%] [G loss: 6.774108]\n",
      "epoch:0 step:56 [D loss: 0.021962, acc.: 100.00%] [G loss: 6.659281]\n",
      "epoch:0 step:57 [D loss: 0.021058, acc.: 100.00%] [G loss: 7.089970]\n",
      "epoch:0 step:58 [D loss: 0.025043, acc.: 100.00%] [G loss: 7.034189]\n",
      "epoch:0 step:59 [D loss: 0.022356, acc.: 100.00%] [G loss: 7.273208]\n",
      "epoch:0 step:60 [D loss: 0.017065, acc.: 100.00%] [G loss: 7.053572]\n",
      "epoch:0 step:61 [D loss: 0.022322, acc.: 100.00%] [G loss: 6.925237]\n",
      "epoch:0 step:62 [D loss: 0.019283, acc.: 100.00%] [G loss: 7.320344]\n",
      "epoch:0 step:63 [D loss: 0.019395, acc.: 100.00%] [G loss: 7.618723]\n",
      "epoch:0 step:64 [D loss: 0.022057, acc.: 100.00%] [G loss: 7.337636]\n",
      "epoch:0 step:65 [D loss: 0.018721, acc.: 100.00%] [G loss: 7.263569]\n",
      "epoch:0 step:66 [D loss: 0.022144, acc.: 100.00%] [G loss: 7.935495]\n",
      "epoch:0 step:67 [D loss: 0.022296, acc.: 100.00%] [G loss: 7.804813]\n",
      "epoch:0 step:68 [D loss: 0.020897, acc.: 100.00%] [G loss: 7.771770]\n",
      "epoch:0 step:69 [D loss: 0.019616, acc.: 100.00%] [G loss: 7.971549]\n",
      "epoch:0 step:70 [D loss: 0.017521, acc.: 100.00%] [G loss: 8.012424]\n",
      "epoch:0 step:71 [D loss: 0.020602, acc.: 100.00%] [G loss: 7.441216]\n",
      "epoch:0 step:72 [D loss: 0.017214, acc.: 100.00%] [G loss: 8.033012]\n",
      "epoch:0 step:73 [D loss: 0.014437, acc.: 100.00%] [G loss: 7.201018]\n",
      "epoch:0 step:74 [D loss: 0.020046, acc.: 100.00%] [G loss: 7.602300]\n",
      "epoch:0 step:75 [D loss: 0.020255, acc.: 100.00%] [G loss: 6.907487]\n",
      "epoch:0 step:76 [D loss: 0.020706, acc.: 100.00%] [G loss: 7.314464]\n",
      "epoch:0 step:77 [D loss: 0.022674, acc.: 100.00%] [G loss: 7.406610]\n",
      "epoch:0 step:78 [D loss: 0.023826, acc.: 100.00%] [G loss: 7.954853]\n",
      "epoch:0 step:79 [D loss: 0.019362, acc.: 100.00%] [G loss: 8.449524]\n",
      "epoch:0 step:80 [D loss: 0.018155, acc.: 100.00%] [G loss: 8.549711]\n",
      "epoch:0 step:81 [D loss: 0.023303, acc.: 100.00%] [G loss: 7.802629]\n",
      "epoch:0 step:82 [D loss: 0.022408, acc.: 100.00%] [G loss: 8.507401]\n",
      "epoch:0 step:83 [D loss: 0.018550, acc.: 100.00%] [G loss: 8.760172]\n",
      "epoch:0 step:84 [D loss: 0.018624, acc.: 100.00%] [G loss: 8.061659]\n",
      "epoch:0 step:85 [D loss: 0.022547, acc.: 100.00%] [G loss: 8.808570]\n",
      "epoch:0 step:86 [D loss: 0.022516, acc.: 100.00%] [G loss: 8.755816]\n",
      "epoch:0 step:87 [D loss: 0.027828, acc.: 100.00%] [G loss: 9.481002]\n",
      "epoch:0 step:88 [D loss: 0.028549, acc.: 100.00%] [G loss: 9.697010]\n",
      "epoch:0 step:89 [D loss: 0.032661, acc.: 100.00%] [G loss: 9.396296]\n",
      "epoch:0 step:90 [D loss: 0.026604, acc.: 100.00%] [G loss: 8.662327]\n",
      "epoch:0 step:91 [D loss: 0.020262, acc.: 100.00%] [G loss: 8.121666]\n",
      "epoch:0 step:92 [D loss: 0.024289, acc.: 100.00%] [G loss: 8.947908]\n",
      "epoch:0 step:93 [D loss: 0.037007, acc.: 100.00%] [G loss: 8.457398]\n",
      "epoch:0 step:94 [D loss: 0.109634, acc.: 98.44%] [G loss: 12.757969]\n",
      "epoch:0 step:95 [D loss: 0.066961, acc.: 100.00%] [G loss: 14.864687]\n",
      "epoch:0 step:96 [D loss: 1.002600, acc.: 73.44%] [G loss: 22.776222]\n",
      "epoch:0 step:97 [D loss: 0.351491, acc.: 81.25%] [G loss: 18.094833]\n",
      "epoch:0 step:98 [D loss: 0.082823, acc.: 96.88%] [G loss: 13.348542]\n",
      "epoch:0 step:99 [D loss: 0.028121, acc.: 100.00%] [G loss: 13.407468]\n",
      "epoch:0 step:100 [D loss: 0.016707, acc.: 100.00%] [G loss: 12.491751]\n",
      "epoch:0 step:101 [D loss: 0.022460, acc.: 100.00%] [G loss: 10.594316]\n",
      "epoch:0 step:102 [D loss: 0.019216, acc.: 100.00%] [G loss: 10.143409]\n",
      "epoch:0 step:103 [D loss: 0.025420, acc.: 100.00%] [G loss: 9.327922]\n",
      "epoch:0 step:104 [D loss: 0.025733, acc.: 100.00%] [G loss: 8.720167]\n",
      "epoch:0 step:105 [D loss: 0.025254, acc.: 100.00%] [G loss: 7.998932]\n",
      "epoch:0 step:106 [D loss: 0.028362, acc.: 100.00%] [G loss: 7.872263]\n",
      "epoch:0 step:107 [D loss: 0.033472, acc.: 100.00%] [G loss: 7.964936]\n",
      "epoch:0 step:108 [D loss: 0.064406, acc.: 99.22%] [G loss: 6.392546]\n",
      "epoch:0 step:109 [D loss: 0.036030, acc.: 100.00%] [G loss: 6.427527]\n",
      "epoch:0 step:110 [D loss: 0.059353, acc.: 100.00%] [G loss: 8.115749]\n",
      "epoch:0 step:111 [D loss: 0.073487, acc.: 98.44%] [G loss: 6.082765]\n",
      "epoch:0 step:112 [D loss: 0.045905, acc.: 100.00%] [G loss: 7.180034]\n",
      "epoch:0 step:113 [D loss: 0.106210, acc.: 97.66%] [G loss: 6.758611]\n",
      "epoch:0 step:114 [D loss: 0.095659, acc.: 97.66%] [G loss: 8.572210]\n",
      "epoch:0 step:115 [D loss: 0.086326, acc.: 99.22%] [G loss: 8.146139]\n",
      "epoch:0 step:116 [D loss: 0.172468, acc.: 92.97%] [G loss: 8.440734]\n",
      "epoch:0 step:117 [D loss: 0.066266, acc.: 99.22%] [G loss: 8.743080]\n",
      "epoch:0 step:118 [D loss: 0.145425, acc.: 93.75%] [G loss: 11.518913]\n",
      "epoch:0 step:119 [D loss: 1.356411, acc.: 63.28%] [G loss: 14.775854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:120 [D loss: 0.326455, acc.: 85.94%] [G loss: 20.204994]\n",
      "epoch:0 step:121 [D loss: 0.056985, acc.: 99.22%] [G loss: 8.469729]\n",
      "epoch:0 step:122 [D loss: 0.067518, acc.: 97.66%] [G loss: 9.269920]\n",
      "epoch:0 step:123 [D loss: 0.104010, acc.: 96.88%] [G loss: 9.067365]\n",
      "epoch:0 step:124 [D loss: 0.240887, acc.: 90.62%] [G loss: 5.302743]\n",
      "epoch:0 step:125 [D loss: 0.089934, acc.: 98.44%] [G loss: 7.885641]\n",
      "epoch:0 step:126 [D loss: 0.066246, acc.: 99.22%] [G loss: 7.985581]\n",
      "epoch:0 step:127 [D loss: 0.110010, acc.: 96.88%] [G loss: 7.417695]\n",
      "epoch:0 step:128 [D loss: 0.118739, acc.: 96.09%] [G loss: 7.098672]\n",
      "epoch:0 step:129 [D loss: 0.088724, acc.: 99.22%] [G loss: 6.233447]\n",
      "epoch:0 step:130 [D loss: 0.266093, acc.: 88.28%] [G loss: 9.155222]\n",
      "epoch:0 step:131 [D loss: 0.734213, acc.: 75.78%] [G loss: 5.959568]\n",
      "epoch:0 step:132 [D loss: 0.080292, acc.: 99.22%] [G loss: 7.164620]\n",
      "epoch:0 step:133 [D loss: 0.378081, acc.: 82.03%] [G loss: 4.157262]\n",
      "epoch:0 step:134 [D loss: 0.099095, acc.: 99.22%] [G loss: 6.717943]\n",
      "epoch:0 step:135 [D loss: 0.379725, acc.: 79.69%] [G loss: 4.438497]\n",
      "epoch:0 step:136 [D loss: 0.109647, acc.: 98.44%] [G loss: 5.288774]\n",
      "epoch:0 step:137 [D loss: 0.110370, acc.: 99.22%] [G loss: 5.104901]\n",
      "epoch:0 step:138 [D loss: 0.115753, acc.: 97.66%] [G loss: 6.275641]\n",
      "epoch:0 step:139 [D loss: 0.374952, acc.: 82.81%] [G loss: 7.289474]\n",
      "epoch:0 step:140 [D loss: 0.496992, acc.: 78.12%] [G loss: 3.847547]\n",
      "epoch:0 step:141 [D loss: 0.098816, acc.: 99.22%] [G loss: 5.474468]\n",
      "epoch:0 step:142 [D loss: 0.142818, acc.: 97.66%] [G loss: 5.293464]\n",
      "epoch:0 step:143 [D loss: 0.186427, acc.: 95.31%] [G loss: 6.046230]\n",
      "epoch:0 step:144 [D loss: 0.592561, acc.: 75.78%] [G loss: 3.045465]\n",
      "epoch:0 step:145 [D loss: 0.135757, acc.: 97.66%] [G loss: 6.503590]\n",
      "epoch:0 step:146 [D loss: 0.334251, acc.: 82.81%] [G loss: 6.777790]\n",
      "epoch:0 step:147 [D loss: 0.384445, acc.: 85.16%] [G loss: 4.762152]\n",
      "epoch:0 step:148 [D loss: 0.235309, acc.: 95.31%] [G loss: 3.710998]\n",
      "epoch:0 step:149 [D loss: 0.239341, acc.: 92.97%] [G loss: 4.438320]\n",
      "epoch:0 step:150 [D loss: 0.171557, acc.: 96.88%] [G loss: 5.236214]\n",
      "epoch:0 step:151 [D loss: 0.293744, acc.: 85.94%] [G loss: 6.456000]\n",
      "epoch:0 step:152 [D loss: 0.938155, acc.: 57.81%] [G loss: 3.727109]\n",
      "epoch:0 step:153 [D loss: 0.164128, acc.: 96.88%] [G loss: 4.680498]\n",
      "epoch:0 step:154 [D loss: 0.663779, acc.: 67.97%] [G loss: 4.138041]\n",
      "epoch:0 step:155 [D loss: 0.136391, acc.: 95.31%] [G loss: 6.072064]\n",
      "epoch:0 step:156 [D loss: 0.229339, acc.: 93.75%] [G loss: 3.307840]\n",
      "epoch:0 step:157 [D loss: 0.132583, acc.: 99.22%] [G loss: 4.302813]\n",
      "epoch:0 step:158 [D loss: 0.115974, acc.: 100.00%] [G loss: 4.962457]\n",
      "epoch:0 step:159 [D loss: 0.203050, acc.: 92.19%] [G loss: 5.671314]\n",
      "epoch:0 step:160 [D loss: 0.580257, acc.: 68.75%] [G loss: 2.088521]\n",
      "epoch:0 step:161 [D loss: 0.117475, acc.: 98.44%] [G loss: 5.034783]\n",
      "epoch:0 step:162 [D loss: 0.456997, acc.: 76.56%] [G loss: 6.329814]\n",
      "epoch:0 step:163 [D loss: 0.225812, acc.: 96.09%] [G loss: 4.042903]\n",
      "epoch:0 step:164 [D loss: 0.145377, acc.: 98.44%] [G loss: 4.052229]\n",
      "epoch:0 step:165 [D loss: 0.249400, acc.: 92.19%] [G loss: 4.632414]\n",
      "epoch:0 step:166 [D loss: 0.310250, acc.: 84.38%] [G loss: 5.288346]\n",
      "epoch:0 step:167 [D loss: 0.316005, acc.: 85.94%] [G loss: 3.729378]\n",
      "epoch:0 step:168 [D loss: 0.165852, acc.: 95.31%] [G loss: 4.483820]\n",
      "epoch:0 step:169 [D loss: 0.175219, acc.: 93.75%] [G loss: 5.325727]\n",
      "epoch:0 step:170 [D loss: 0.632858, acc.: 73.44%] [G loss: 4.422453]\n",
      "epoch:0 step:171 [D loss: 0.215466, acc.: 96.09%] [G loss: 4.311699]\n",
      "epoch:0 step:172 [D loss: 0.580050, acc.: 67.19%] [G loss: 4.371665]\n",
      "epoch:0 step:173 [D loss: 0.299230, acc.: 89.84%] [G loss: 3.344662]\n",
      "epoch:0 step:174 [D loss: 0.352311, acc.: 82.03%] [G loss: 4.777834]\n",
      "epoch:0 step:175 [D loss: 0.200980, acc.: 96.09%] [G loss: 5.877703]\n",
      "epoch:0 step:176 [D loss: 0.762258, acc.: 66.41%] [G loss: 2.196897]\n",
      "epoch:0 step:177 [D loss: 0.247670, acc.: 83.59%] [G loss: 5.593558]\n",
      "epoch:0 step:178 [D loss: 0.326231, acc.: 85.16%] [G loss: 7.807318]\n",
      "epoch:0 step:179 [D loss: 0.556280, acc.: 64.84%] [G loss: 3.126450]\n",
      "epoch:0 step:180 [D loss: 0.191425, acc.: 96.88%] [G loss: 5.157488]\n",
      "epoch:0 step:181 [D loss: 0.397733, acc.: 79.69%] [G loss: 3.975335]\n",
      "epoch:0 step:182 [D loss: 0.305875, acc.: 85.94%] [G loss: 3.710974]\n",
      "epoch:0 step:183 [D loss: 0.306388, acc.: 86.72%] [G loss: 3.118885]\n",
      "epoch:0 step:184 [D loss: 0.237224, acc.: 94.53%] [G loss: 4.108172]\n",
      "epoch:0 step:185 [D loss: 0.448511, acc.: 75.78%] [G loss: 2.544988]\n",
      "epoch:0 step:186 [D loss: 0.256837, acc.: 92.97%] [G loss: 5.674352]\n",
      "epoch:0 step:187 [D loss: 0.876344, acc.: 62.50%] [G loss: 3.780523]\n",
      "epoch:0 step:188 [D loss: 0.280081, acc.: 91.41%] [G loss: 3.949578]\n",
      "epoch:0 step:189 [D loss: 0.345411, acc.: 82.81%] [G loss: 2.681570]\n",
      "epoch:0 step:190 [D loss: 0.246560, acc.: 92.19%] [G loss: 3.414443]\n",
      "epoch:0 step:191 [D loss: 0.347743, acc.: 89.84%] [G loss: 4.046709]\n",
      "epoch:0 step:192 [D loss: 0.574080, acc.: 70.31%] [G loss: 3.398940]\n",
      "epoch:0 step:193 [D loss: 0.380399, acc.: 78.91%] [G loss: 5.053761]\n",
      "epoch:0 step:194 [D loss: 1.022285, acc.: 53.91%] [G loss: 1.591935]\n",
      "epoch:0 step:195 [D loss: 0.362906, acc.: 75.78%] [G loss: 3.748315]\n",
      "epoch:0 step:196 [D loss: 0.378244, acc.: 86.72%] [G loss: 2.391903]\n",
      "epoch:0 step:197 [D loss: 0.229476, acc.: 98.44%] [G loss: 2.716514]\n",
      "epoch:0 step:198 [D loss: 0.290249, acc.: 92.97%] [G loss: 2.822536]\n",
      "epoch:0 step:199 [D loss: 0.379443, acc.: 85.94%] [G loss: 3.352798]\n",
      "epoch:0 step:200 [D loss: 0.479694, acc.: 73.44%] [G loss: 1.641201]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "WARNING:tensorflow:From /home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:185: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:208: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute score in space: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ot/lp/__init__.py:211: UserWarning: numItermax reached before optimality. Try to increase numItermax.\n",
      "  check_result(result_code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute score in space: 1\n",
      "IS socre: 1.256496\n",
      "FID: 270.756348\n",
      "0 = 21.957644178962653\n",
      "1 = 0.45969922826808823\n",
      "2 = 1.0\n",
      "3 = 1.0\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 1.0\n",
      "7 = 16.001187260675422\n",
      "8 = 0.26175609720701604\n",
      "9 = 0.998199999332428\n",
      "10 = 0.996399998664856\n",
      "11 = 1.0\n",
      "12 = 1.0\n",
      "13 = 0.996399998664856\n",
      "14 = 1.2564960718154907\n",
      "15 = 5.833425521850586\n",
      "16 = 0.8470408320426941\n",
      "17 = 1.2564960718154907\n",
      "18 = 270.75634765625\n",
      "epoch:0 step:201 [D loss: 0.198550, acc.: 95.31%] [G loss: 4.309289]\n",
      "epoch:0 step:202 [D loss: 0.750339, acc.: 62.50%] [G loss: 1.018375]\n",
      "epoch:0 step:203 [D loss: 0.294978, acc.: 89.84%] [G loss: 4.445893]\n",
      "epoch:0 step:204 [D loss: 0.502216, acc.: 75.00%] [G loss: 1.266027]\n",
      "epoch:0 step:205 [D loss: 0.287956, acc.: 89.06%] [G loss: 4.552553]\n",
      "epoch:0 step:206 [D loss: 0.412115, acc.: 78.12%] [G loss: 2.791561]\n",
      "epoch:0 step:207 [D loss: 0.393909, acc.: 85.16%] [G loss: 1.610561]\n",
      "epoch:0 step:208 [D loss: 0.330352, acc.: 89.06%] [G loss: 3.265699]\n",
      "epoch:0 step:209 [D loss: 0.451330, acc.: 75.78%] [G loss: 2.218311]\n",
      "epoch:0 step:210 [D loss: 0.403129, acc.: 84.38%] [G loss: 2.558216]\n",
      "epoch:0 step:211 [D loss: 0.539957, acc.: 68.75%] [G loss: 2.023751]\n",
      "epoch:0 step:212 [D loss: 0.488939, acc.: 69.53%] [G loss: 2.150280]\n",
      "epoch:0 step:213 [D loss: 0.406255, acc.: 85.16%] [G loss: 2.823257]\n",
      "epoch:0 step:214 [D loss: 0.570301, acc.: 69.53%] [G loss: 1.844134]\n",
      "epoch:0 step:215 [D loss: 0.281482, acc.: 96.88%] [G loss: 4.170253]\n",
      "epoch:0 step:216 [D loss: 0.941680, acc.: 40.62%] [G loss: 0.558288]\n",
      "epoch:0 step:217 [D loss: 0.265273, acc.: 93.75%] [G loss: 5.679381]\n",
      "epoch:0 step:218 [D loss: 1.092972, acc.: 36.72%] [G loss: 0.977245]\n",
      "epoch:0 step:219 [D loss: 0.497429, acc.: 62.50%] [G loss: 2.026645]\n",
      "epoch:0 step:220 [D loss: 0.893437, acc.: 46.88%] [G loss: 0.758759]\n",
      "epoch:0 step:221 [D loss: 0.399955, acc.: 84.38%] [G loss: 2.257089]\n",
      "epoch:0 step:222 [D loss: 0.509377, acc.: 79.69%] [G loss: 1.532003]\n",
      "epoch:0 step:223 [D loss: 0.344635, acc.: 91.41%] [G loss: 2.225074]\n",
      "epoch:0 step:224 [D loss: 0.682800, acc.: 57.03%] [G loss: 0.651652]\n",
      "epoch:0 step:225 [D loss: 0.419174, acc.: 82.03%] [G loss: 1.880176]\n",
      "epoch:0 step:226 [D loss: 0.704101, acc.: 51.56%] [G loss: 0.953451]\n",
      "epoch:0 step:227 [D loss: 0.462196, acc.: 75.78%] [G loss: 2.178748]\n",
      "epoch:0 step:228 [D loss: 0.489115, acc.: 82.03%] [G loss: 1.335172]\n",
      "epoch:0 step:229 [D loss: 0.749159, acc.: 46.09%] [G loss: 0.811914]\n",
      "epoch:0 step:230 [D loss: 0.361983, acc.: 92.97%] [G loss: 2.482426]\n",
      "epoch:0 step:231 [D loss: 0.576580, acc.: 76.56%] [G loss: 1.132938]\n",
      "epoch:0 step:232 [D loss: 0.400761, acc.: 86.72%] [G loss: 2.065066]\n",
      "epoch:0 step:233 [D loss: 1.278538, acc.: 26.56%] [G loss: 0.154991]\n",
      "epoch:0 step:234 [D loss: 0.531439, acc.: 55.47%] [G loss: 1.382299]\n",
      "epoch:0 step:235 [D loss: 0.608341, acc.: 67.97%] [G loss: 0.946059]\n",
      "epoch:0 step:236 [D loss: 0.746096, acc.: 49.22%] [G loss: 0.323198]\n",
      "epoch:0 step:237 [D loss: 0.624162, acc.: 51.56%] [G loss: 0.915114]\n",
      "epoch:0 step:238 [D loss: 0.532508, acc.: 71.88%] [G loss: 0.967297]\n",
      "epoch:0 step:239 [D loss: 0.708923, acc.: 47.66%] [G loss: 0.444105]\n",
      "epoch:0 step:240 [D loss: 0.421587, acc.: 85.16%] [G loss: 1.367153]\n",
      "epoch:0 step:241 [D loss: 0.750849, acc.: 43.75%] [G loss: 0.268638]\n",
      "epoch:0 step:242 [D loss: 0.494996, acc.: 69.53%] [G loss: 1.453786]\n",
      "epoch:0 step:243 [D loss: 0.794152, acc.: 37.50%] [G loss: 0.274572]\n",
      "epoch:0 step:244 [D loss: 0.547700, acc.: 64.06%] [G loss: 0.849815]\n",
      "epoch:0 step:245 [D loss: 0.750908, acc.: 49.22%] [G loss: 0.485019]\n",
      "epoch:0 step:246 [D loss: 0.730628, acc.: 49.22%] [G loss: 0.352455]\n",
      "epoch:0 step:247 [D loss: 0.617809, acc.: 54.69%] [G loss: 0.584119]\n",
      "epoch:0 step:248 [D loss: 0.642898, acc.: 56.25%] [G loss: 0.598219]\n",
      "epoch:0 step:249 [D loss: 0.767184, acc.: 41.41%] [G loss: 0.190303]\n",
      "epoch:0 step:250 [D loss: 0.542422, acc.: 65.62%] [G loss: 0.604053]\n",
      "epoch:0 step:251 [D loss: 0.802197, acc.: 39.84%] [G loss: 0.163296]\n",
      "epoch:0 step:252 [D loss: 0.625390, acc.: 50.00%] [G loss: 0.391911]\n",
      "epoch:0 step:253 [D loss: 0.672599, acc.: 53.12%] [G loss: 0.411943]\n",
      "epoch:0 step:254 [D loss: 0.685585, acc.: 58.59%] [G loss: 0.322439]\n",
      "epoch:0 step:255 [D loss: 0.640473, acc.: 60.16%] [G loss: 0.236834]\n",
      "epoch:0 step:256 [D loss: 0.691533, acc.: 42.19%] [G loss: 0.275180]\n",
      "epoch:0 step:257 [D loss: 0.518632, acc.: 68.75%] [G loss: 0.644268]\n",
      "epoch:0 step:258 [D loss: 0.698368, acc.: 52.34%] [G loss: 0.312486]\n",
      "epoch:0 step:259 [D loss: 0.693630, acc.: 44.53%] [G loss: 0.220086]\n",
      "epoch:0 step:260 [D loss: 0.609542, acc.: 53.91%] [G loss: 0.498754]\n",
      "epoch:0 step:261 [D loss: 0.657427, acc.: 53.12%] [G loss: 0.383324]\n",
      "epoch:0 step:262 [D loss: 0.661095, acc.: 49.22%] [G loss: 0.307600]\n",
      "epoch:0 step:263 [D loss: 0.851685, acc.: 33.59%] [G loss: 0.083644]\n",
      "epoch:0 step:264 [D loss: 0.579784, acc.: 55.47%] [G loss: 0.269874]\n",
      "epoch:0 step:265 [D loss: 0.645482, acc.: 59.38%] [G loss: 0.380818]\n",
      "epoch:0 step:266 [D loss: 0.662736, acc.: 56.25%] [G loss: 0.217729]\n",
      "epoch:0 step:267 [D loss: 0.570235, acc.: 60.94%] [G loss: 0.429391]\n",
      "epoch:0 step:268 [D loss: 0.780419, acc.: 36.72%] [G loss: 0.100621]\n",
      "epoch:0 step:269 [D loss: 0.774353, acc.: 35.16%] [G loss: 0.061435]\n",
      "epoch:0 step:270 [D loss: 0.606658, acc.: 50.00%] [G loss: 0.250699]\n",
      "epoch:0 step:271 [D loss: 0.716225, acc.: 42.19%] [G loss: 0.165721]\n",
      "epoch:0 step:272 [D loss: 0.649154, acc.: 54.69%] [G loss: 0.158858]\n",
      "epoch:0 step:273 [D loss: 0.582746, acc.: 58.59%] [G loss: 0.281926]\n",
      "epoch:0 step:274 [D loss: 0.705636, acc.: 46.88%] [G loss: 0.161963]\n",
      "epoch:0 step:275 [D loss: 0.658874, acc.: 50.00%] [G loss: 0.141640]\n",
      "epoch:0 step:276 [D loss: 0.642795, acc.: 49.22%] [G loss: 0.198532]\n",
      "epoch:0 step:277 [D loss: 0.615240, acc.: 54.69%] [G loss: 0.208041]\n",
      "epoch:0 step:278 [D loss: 0.587698, acc.: 67.97%] [G loss: 0.175125]\n",
      "epoch:0 step:279 [D loss: 0.618646, acc.: 57.81%] [G loss: 0.281598]\n",
      "epoch:0 step:280 [D loss: 0.584722, acc.: 64.84%] [G loss: 0.254553]\n",
      "epoch:0 step:281 [D loss: 0.686334, acc.: 48.44%] [G loss: 0.122643]\n",
      "epoch:0 step:282 [D loss: 0.613324, acc.: 55.47%] [G loss: 0.196454]\n",
      "epoch:0 step:283 [D loss: 0.586426, acc.: 64.06%] [G loss: 0.223534]\n",
      "epoch:0 step:284 [D loss: 0.622443, acc.: 55.47%] [G loss: 0.279066]\n",
      "epoch:0 step:285 [D loss: 0.632369, acc.: 54.69%] [G loss: 0.193025]\n",
      "epoch:0 step:286 [D loss: 0.654734, acc.: 50.78%] [G loss: 0.192541]\n",
      "epoch:0 step:287 [D loss: 0.554556, acc.: 63.28%] [G loss: 0.279792]\n",
      "epoch:0 step:288 [D loss: 0.657316, acc.: 53.91%] [G loss: 0.163875]\n",
      "epoch:0 step:289 [D loss: 0.563960, acc.: 69.53%] [G loss: 0.328068]\n",
      "epoch:0 step:290 [D loss: 0.736025, acc.: 42.97%] [G loss: 0.059094]\n",
      "epoch:0 step:291 [D loss: 0.751613, acc.: 40.62%] [G loss: 0.048335]\n",
      "epoch:0 step:292 [D loss: 0.653862, acc.: 46.88%] [G loss: 0.102046]\n",
      "epoch:0 step:293 [D loss: 0.672187, acc.: 43.75%] [G loss: 0.067843]\n",
      "epoch:0 step:294 [D loss: 0.650750, acc.: 48.44%] [G loss: 0.132322]\n",
      "epoch:0 step:295 [D loss: 0.654430, acc.: 53.12%] [G loss: 0.105843]\n",
      "epoch:0 step:296 [D loss: 0.616497, acc.: 51.56%] [G loss: 0.126933]\n",
      "epoch:0 step:297 [D loss: 0.614760, acc.: 54.69%] [G loss: 0.140278]\n",
      "epoch:0 step:298 [D loss: 0.626925, acc.: 57.81%] [G loss: 0.114488]\n",
      "epoch:0 step:299 [D loss: 0.594739, acc.: 63.28%] [G loss: 0.127274]\n",
      "epoch:0 step:300 [D loss: 0.624927, acc.: 52.34%] [G loss: 0.125621]\n",
      "epoch:0 step:301 [D loss: 0.773710, acc.: 35.16%] [G loss: 0.039616]\n",
      "epoch:0 step:302 [D loss: 0.625385, acc.: 50.78%] [G loss: 0.088211]\n",
      "epoch:0 step:303 [D loss: 0.681619, acc.: 46.09%] [G loss: 0.071518]\n",
      "epoch:0 step:304 [D loss: 0.634541, acc.: 47.66%] [G loss: 0.097458]\n",
      "epoch:0 step:305 [D loss: 0.605909, acc.: 60.94%] [G loss: 0.160985]\n",
      "epoch:0 step:306 [D loss: 0.625956, acc.: 57.03%] [G loss: 0.163117]\n",
      "epoch:0 step:307 [D loss: 0.563014, acc.: 70.31%] [G loss: 0.190057]\n",
      "epoch:0 step:308 [D loss: 0.675827, acc.: 50.78%] [G loss: 0.096522]\n",
      "epoch:0 step:309 [D loss: 0.649779, acc.: 47.66%] [G loss: 0.085817]\n",
      "epoch:0 step:310 [D loss: 0.623956, acc.: 50.78%] [G loss: 0.153006]\n",
      "epoch:0 step:311 [D loss: 0.628954, acc.: 58.59%] [G loss: 0.110458]\n",
      "epoch:0 step:312 [D loss: 0.807145, acc.: 25.78%] [G loss: 0.029877]\n",
      "epoch:0 step:313 [D loss: 0.651021, acc.: 45.31%] [G loss: 0.027430]\n",
      "epoch:0 step:314 [D loss: 0.566968, acc.: 62.50%] [G loss: 0.141126]\n",
      "epoch:0 step:315 [D loss: 0.614472, acc.: 58.59%] [G loss: 0.119385]\n",
      "epoch:0 step:316 [D loss: 0.758216, acc.: 36.72%] [G loss: 0.023388]\n",
      "epoch:0 step:317 [D loss: 0.629822, acc.: 48.44%] [G loss: 0.047853]\n",
      "epoch:0 step:318 [D loss: 0.626072, acc.: 50.00%] [G loss: 0.066008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:319 [D loss: 0.628179, acc.: 55.47%] [G loss: 0.116319]\n",
      "epoch:0 step:320 [D loss: 0.590723, acc.: 55.47%] [G loss: 0.099409]\n",
      "epoch:0 step:321 [D loss: 0.656616, acc.: 52.34%] [G loss: 0.061459]\n",
      "epoch:0 step:322 [D loss: 0.610793, acc.: 58.59%] [G loss: 0.097836]\n",
      "epoch:0 step:323 [D loss: 0.611249, acc.: 57.81%] [G loss: 0.092736]\n",
      "epoch:0 step:324 [D loss: 0.570041, acc.: 70.31%] [G loss: 0.127334]\n",
      "epoch:0 step:325 [D loss: 0.611273, acc.: 59.38%] [G loss: 0.087019]\n",
      "epoch:0 step:326 [D loss: 0.620018, acc.: 53.91%] [G loss: 0.084594]\n",
      "epoch:0 step:327 [D loss: 0.590208, acc.: 62.50%] [G loss: 0.124350]\n",
      "epoch:0 step:328 [D loss: 0.662778, acc.: 49.22%] [G loss: 0.061558]\n",
      "epoch:0 step:329 [D loss: 0.588015, acc.: 59.38%] [G loss: 0.105331]\n",
      "epoch:0 step:330 [D loss: 0.620231, acc.: 52.34%] [G loss: 0.085674]\n",
      "epoch:0 step:331 [D loss: 0.587954, acc.: 66.41%] [G loss: 0.089251]\n",
      "epoch:0 step:332 [D loss: 0.619277, acc.: 60.16%] [G loss: 0.096048]\n",
      "epoch:0 step:333 [D loss: 0.622373, acc.: 57.81%] [G loss: 0.081336]\n",
      "epoch:0 step:334 [D loss: 0.648042, acc.: 47.66%] [G loss: 0.054573]\n",
      "epoch:0 step:335 [D loss: 0.597028, acc.: 59.38%] [G loss: 0.097021]\n",
      "epoch:0 step:336 [D loss: 0.570095, acc.: 69.53%] [G loss: 0.151678]\n",
      "epoch:0 step:337 [D loss: 0.578361, acc.: 74.22%] [G loss: 0.129847]\n",
      "epoch:0 step:338 [D loss: 0.586776, acc.: 64.84%] [G loss: 0.089072]\n",
      "epoch:0 step:339 [D loss: 0.584248, acc.: 59.38%] [G loss: 0.098693]\n",
      "epoch:0 step:340 [D loss: 0.565241, acc.: 66.41%] [G loss: 0.161220]\n",
      "epoch:0 step:341 [D loss: 0.672525, acc.: 50.78%] [G loss: 0.087412]\n",
      "epoch:0 step:342 [D loss: 0.591215, acc.: 60.94%] [G loss: 0.108140]\n",
      "epoch:0 step:343 [D loss: 0.582662, acc.: 69.53%] [G loss: 0.133817]\n",
      "epoch:0 step:344 [D loss: 0.563886, acc.: 72.66%] [G loss: 0.164788]\n",
      "epoch:0 step:345 [D loss: 0.724166, acc.: 39.84%] [G loss: 0.046541]\n",
      "epoch:0 step:346 [D loss: 0.665617, acc.: 48.44%] [G loss: 0.024361]\n",
      "epoch:0 step:347 [D loss: 0.586104, acc.: 57.03%] [G loss: 0.072338]\n",
      "epoch:0 step:348 [D loss: 0.638834, acc.: 52.34%] [G loss: 0.052810]\n",
      "epoch:0 step:349 [D loss: 0.678167, acc.: 46.09%] [G loss: 0.018203]\n",
      "epoch:0 step:350 [D loss: 0.600754, acc.: 52.34%] [G loss: 0.061372]\n",
      "epoch:0 step:351 [D loss: 0.629522, acc.: 53.12%] [G loss: 0.047170]\n",
      "epoch:0 step:352 [D loss: 0.664021, acc.: 40.62%] [G loss: 0.032399]\n",
      "epoch:0 step:353 [D loss: 0.611627, acc.: 51.56%] [G loss: 0.071098]\n",
      "epoch:0 step:354 [D loss: 0.612113, acc.: 57.03%] [G loss: 0.088348]\n",
      "epoch:0 step:355 [D loss: 0.587644, acc.: 68.75%] [G loss: 0.131427]\n",
      "epoch:0 step:356 [D loss: 0.605490, acc.: 59.38%] [G loss: 0.110968]\n",
      "epoch:0 step:357 [D loss: 0.579458, acc.: 66.41%] [G loss: 0.097645]\n",
      "epoch:0 step:358 [D loss: 0.558521, acc.: 69.53%] [G loss: 0.126405]\n",
      "epoch:0 step:359 [D loss: 0.607700, acc.: 63.28%] [G loss: 0.092877]\n",
      "epoch:0 step:360 [D loss: 0.615741, acc.: 55.47%] [G loss: 0.057792]\n",
      "epoch:0 step:361 [D loss: 0.579297, acc.: 70.31%] [G loss: 0.099279]\n",
      "epoch:0 step:362 [D loss: 0.610248, acc.: 56.25%] [G loss: 0.086091]\n",
      "epoch:0 step:363 [D loss: 0.565572, acc.: 64.84%] [G loss: 0.083657]\n",
      "epoch:0 step:364 [D loss: 0.584582, acc.: 60.94%] [G loss: 0.120095]\n",
      "epoch:0 step:365 [D loss: 0.607391, acc.: 64.84%] [G loss: 0.071949]\n",
      "epoch:0 step:366 [D loss: 0.600658, acc.: 57.81%] [G loss: 0.080594]\n",
      "epoch:0 step:367 [D loss: 0.571244, acc.: 65.62%] [G loss: 0.086280]\n",
      "epoch:0 step:368 [D loss: 0.598983, acc.: 64.06%] [G loss: 0.081172]\n",
      "epoch:0 step:369 [D loss: 0.647817, acc.: 53.91%] [G loss: 0.041699]\n",
      "epoch:0 step:370 [D loss: 0.629579, acc.: 52.34%] [G loss: 0.048898]\n",
      "epoch:0 step:371 [D loss: 0.612405, acc.: 59.38%] [G loss: 0.069256]\n",
      "epoch:0 step:372 [D loss: 0.622614, acc.: 50.78%] [G loss: 0.057266]\n",
      "epoch:0 step:373 [D loss: 0.616537, acc.: 55.47%] [G loss: 0.049442]\n",
      "epoch:0 step:374 [D loss: 0.612199, acc.: 59.38%] [G loss: 0.051102]\n",
      "epoch:0 step:375 [D loss: 0.562710, acc.: 67.97%] [G loss: 0.087052]\n",
      "epoch:0 step:376 [D loss: 0.574866, acc.: 70.31%] [G loss: 0.108088]\n",
      "epoch:0 step:377 [D loss: 0.542623, acc.: 78.12%] [G loss: 0.102107]\n",
      "epoch:0 step:378 [D loss: 0.517704, acc.: 89.06%] [G loss: 0.140834]\n",
      "epoch:0 step:379 [D loss: 0.566972, acc.: 78.12%] [G loss: 0.117561]\n",
      "epoch:0 step:380 [D loss: 0.604012, acc.: 64.06%] [G loss: 0.078229]\n",
      "epoch:0 step:381 [D loss: 0.553758, acc.: 71.88%] [G loss: 0.105043]\n",
      "epoch:0 step:382 [D loss: 0.621538, acc.: 61.72%] [G loss: 0.104182]\n",
      "epoch:0 step:383 [D loss: 0.613383, acc.: 59.38%] [G loss: 0.056786]\n",
      "epoch:0 step:384 [D loss: 0.592606, acc.: 66.41%] [G loss: 0.069155]\n",
      "epoch:0 step:385 [D loss: 0.617224, acc.: 57.03%] [G loss: 0.053609]\n",
      "epoch:0 step:386 [D loss: 0.529968, acc.: 79.69%] [G loss: 0.117846]\n",
      "epoch:0 step:387 [D loss: 0.563979, acc.: 75.78%] [G loss: 0.114647]\n",
      "epoch:0 step:388 [D loss: 0.518965, acc.: 82.03%] [G loss: 0.119066]\n",
      "epoch:0 step:389 [D loss: 0.584575, acc.: 67.19%] [G loss: 0.097090]\n",
      "epoch:0 step:390 [D loss: 0.590264, acc.: 62.50%] [G loss: 0.090382]\n",
      "epoch:0 step:391 [D loss: 0.606588, acc.: 61.72%] [G loss: 0.080416]\n",
      "epoch:0 step:392 [D loss: 0.557237, acc.: 71.09%] [G loss: 0.081385]\n",
      "epoch:0 step:393 [D loss: 0.603760, acc.: 65.62%] [G loss: 0.082216]\n",
      "epoch:0 step:394 [D loss: 0.633648, acc.: 55.47%] [G loss: 0.035947]\n",
      "epoch:0 step:395 [D loss: 0.590455, acc.: 59.38%] [G loss: 0.053019]\n",
      "epoch:0 step:396 [D loss: 0.634206, acc.: 51.56%] [G loss: 0.070460]\n",
      "epoch:0 step:397 [D loss: 0.571618, acc.: 72.66%] [G loss: 0.085671]\n",
      "epoch:0 step:398 [D loss: 0.554468, acc.: 75.78%] [G loss: 0.099973]\n",
      "epoch:0 step:399 [D loss: 0.554080, acc.: 80.47%] [G loss: 0.115234]\n",
      "epoch:0 step:400 [D loss: 0.643947, acc.: 56.25%] [G loss: 0.042637]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 1.369799\n",
      "FID: 268.015930\n",
      "0 = 16.126035541534424\n",
      "1 = 0.31238286684166244\n",
      "2 = 1.0\n",
      "3 = 1.0\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 1.0\n",
      "7 = 15.900492986679064\n",
      "8 = 0.2606349538827934\n",
      "9 = 0.9976999759674072\n",
      "10 = 0.9954000115394592\n",
      "11 = 1.0\n",
      "12 = 1.0\n",
      "13 = 0.9954000115394592\n",
      "14 = 1.3697996139526367\n",
      "15 = 5.893270969390869\n",
      "16 = 0.798244833946228\n",
      "17 = 1.3697993755340576\n",
      "18 = 268.01593017578125\n",
      "epoch:0 step:401 [D loss: 0.591356, acc.: 60.94%] [G loss: 0.086624]\n",
      "epoch:0 step:402 [D loss: 0.594958, acc.: 73.44%] [G loss: 0.072650]\n",
      "epoch:0 step:403 [D loss: 0.581168, acc.: 64.06%] [G loss: 0.078236]\n",
      "epoch:0 step:404 [D loss: 0.624350, acc.: 60.16%] [G loss: 0.058224]\n",
      "epoch:0 step:405 [D loss: 0.549045, acc.: 71.09%] [G loss: 0.066201]\n",
      "epoch:0 step:406 [D loss: 0.596636, acc.: 63.28%] [G loss: 0.065860]\n",
      "epoch:0 step:407 [D loss: 0.594542, acc.: 69.53%] [G loss: 0.085409]\n",
      "epoch:0 step:408 [D loss: 0.564513, acc.: 78.91%] [G loss: 0.125311]\n",
      "epoch:0 step:409 [D loss: 0.593183, acc.: 70.31%] [G loss: 0.078133]\n",
      "epoch:0 step:410 [D loss: 0.562760, acc.: 76.56%] [G loss: 0.097517]\n",
      "epoch:0 step:411 [D loss: 0.598499, acc.: 62.50%] [G loss: 0.093907]\n",
      "epoch:0 step:412 [D loss: 0.586405, acc.: 71.09%] [G loss: 0.084209]\n",
      "epoch:0 step:413 [D loss: 0.588659, acc.: 66.41%] [G loss: 0.078575]\n",
      "epoch:0 step:414 [D loss: 0.608235, acc.: 63.28%] [G loss: 0.053760]\n",
      "epoch:0 step:415 [D loss: 0.593703, acc.: 64.84%] [G loss: 0.056790]\n",
      "epoch:0 step:416 [D loss: 0.603768, acc.: 64.06%] [G loss: 0.081950]\n",
      "epoch:0 step:417 [D loss: 0.583921, acc.: 69.53%] [G loss: 0.079635]\n",
      "epoch:0 step:418 [D loss: 0.571452, acc.: 71.88%] [G loss: 0.090234]\n",
      "epoch:0 step:419 [D loss: 0.549744, acc.: 75.00%] [G loss: 0.104195]\n",
      "epoch:0 step:420 [D loss: 0.552319, acc.: 79.69%] [G loss: 0.127808]\n",
      "epoch:0 step:421 [D loss: 0.624821, acc.: 60.94%] [G loss: 0.072583]\n",
      "epoch:0 step:422 [D loss: 0.643004, acc.: 58.59%] [G loss: 0.041507]\n",
      "epoch:0 step:423 [D loss: 0.605518, acc.: 62.50%] [G loss: 0.055599]\n",
      "epoch:0 step:424 [D loss: 0.624818, acc.: 61.72%] [G loss: 0.061274]\n",
      "epoch:0 step:425 [D loss: 0.614577, acc.: 65.62%] [G loss: 0.050710]\n",
      "epoch:0 step:426 [D loss: 0.581861, acc.: 65.62%] [G loss: 0.076114]\n",
      "epoch:0 step:427 [D loss: 0.619255, acc.: 60.16%] [G loss: 0.051606]\n",
      "epoch:0 step:428 [D loss: 0.592095, acc.: 63.28%] [G loss: 0.053176]\n",
      "epoch:0 step:429 [D loss: 0.592370, acc.: 73.44%] [G loss: 0.067509]\n",
      "epoch:0 step:430 [D loss: 0.573953, acc.: 70.31%] [G loss: 0.067569]\n",
      "epoch:0 step:431 [D loss: 0.604492, acc.: 66.41%] [G loss: 0.060340]\n",
      "epoch:0 step:432 [D loss: 0.635820, acc.: 54.69%] [G loss: 0.050689]\n",
      "epoch:0 step:433 [D loss: 0.596455, acc.: 64.06%] [G loss: 0.042705]\n",
      "epoch:0 step:434 [D loss: 0.605177, acc.: 59.38%] [G loss: 0.050556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:435 [D loss: 0.592172, acc.: 72.66%] [G loss: 0.054277]\n",
      "epoch:0 step:436 [D loss: 0.588086, acc.: 65.62%] [G loss: 0.050510]\n",
      "epoch:0 step:437 [D loss: 0.614242, acc.: 57.03%] [G loss: 0.049432]\n",
      "epoch:0 step:438 [D loss: 0.562630, acc.: 68.75%] [G loss: 0.074458]\n",
      "epoch:0 step:439 [D loss: 0.586138, acc.: 72.66%] [G loss: 0.069660]\n",
      "epoch:0 step:440 [D loss: 0.566584, acc.: 72.66%] [G loss: 0.088086]\n",
      "epoch:0 step:441 [D loss: 0.582875, acc.: 78.91%] [G loss: 0.074061]\n",
      "epoch:0 step:442 [D loss: 0.592306, acc.: 70.31%] [G loss: 0.086680]\n",
      "epoch:0 step:443 [D loss: 0.593101, acc.: 70.31%] [G loss: 0.064995]\n",
      "epoch:0 step:444 [D loss: 0.594129, acc.: 71.88%] [G loss: 0.048257]\n",
      "epoch:0 step:445 [D loss: 0.578513, acc.: 71.09%] [G loss: 0.061777]\n",
      "epoch:0 step:446 [D loss: 0.580120, acc.: 69.53%] [G loss: 0.059937]\n",
      "epoch:0 step:447 [D loss: 0.526800, acc.: 79.69%] [G loss: 0.091633]\n",
      "epoch:0 step:448 [D loss: 0.680967, acc.: 56.25%] [G loss: 0.044674]\n",
      "epoch:0 step:449 [D loss: 0.614755, acc.: 62.50%] [G loss: 0.056356]\n",
      "epoch:0 step:450 [D loss: 0.561526, acc.: 68.75%] [G loss: 0.069548]\n",
      "epoch:0 step:451 [D loss: 0.607342, acc.: 64.84%] [G loss: 0.052806]\n",
      "epoch:0 step:452 [D loss: 0.603224, acc.: 64.84%] [G loss: 0.046928]\n",
      "epoch:0 step:453 [D loss: 0.583535, acc.: 71.88%] [G loss: 0.043318]\n",
      "epoch:0 step:454 [D loss: 0.604136, acc.: 64.84%] [G loss: 0.037409]\n",
      "epoch:0 step:455 [D loss: 0.596582, acc.: 67.19%] [G loss: 0.042890]\n",
      "epoch:0 step:456 [D loss: 0.607472, acc.: 65.62%] [G loss: 0.041801]\n",
      "epoch:0 step:457 [D loss: 0.599092, acc.: 70.31%] [G loss: 0.054454]\n",
      "epoch:0 step:458 [D loss: 0.540438, acc.: 79.69%] [G loss: 0.067594]\n",
      "epoch:0 step:459 [D loss: 0.546462, acc.: 85.94%] [G loss: 0.112475]\n",
      "epoch:0 step:460 [D loss: 0.538082, acc.: 86.72%] [G loss: 0.143346]\n",
      "epoch:0 step:461 [D loss: 0.580118, acc.: 81.25%] [G loss: 0.106365]\n",
      "epoch:0 step:462 [D loss: 0.576887, acc.: 71.88%] [G loss: 0.041000]\n",
      "epoch:0 step:463 [D loss: 0.565401, acc.: 71.09%] [G loss: 0.057244]\n",
      "epoch:0 step:464 [D loss: 0.558887, acc.: 73.44%] [G loss: 0.076032]\n",
      "epoch:0 step:465 [D loss: 0.576151, acc.: 78.12%] [G loss: 0.067127]\n",
      "epoch:0 step:466 [D loss: 0.554372, acc.: 74.22%] [G loss: 0.080107]\n",
      "epoch:0 step:467 [D loss: 0.556314, acc.: 82.03%] [G loss: 0.090684]\n",
      "epoch:0 step:468 [D loss: 0.553551, acc.: 76.56%] [G loss: 0.111009]\n",
      "epoch:0 step:469 [D loss: 0.564581, acc.: 78.91%] [G loss: 0.079375]\n",
      "epoch:0 step:470 [D loss: 0.582183, acc.: 69.53%] [G loss: 0.079744]\n",
      "epoch:0 step:471 [D loss: 0.524452, acc.: 80.47%] [G loss: 0.108876]\n",
      "epoch:0 step:472 [D loss: 0.560723, acc.: 73.44%] [G loss: 0.132380]\n",
      "epoch:0 step:473 [D loss: 0.594276, acc.: 71.09%] [G loss: 0.086385]\n",
      "epoch:0 step:474 [D loss: 0.578164, acc.: 69.53%] [G loss: 0.075449]\n",
      "epoch:0 step:475 [D loss: 0.597152, acc.: 64.06%] [G loss: 0.083257]\n",
      "epoch:0 step:476 [D loss: 0.539806, acc.: 80.47%] [G loss: 0.115988]\n",
      "epoch:0 step:477 [D loss: 0.581206, acc.: 70.31%] [G loss: 0.073489]\n",
      "epoch:0 step:478 [D loss: 0.581957, acc.: 69.53%] [G loss: 0.079471]\n",
      "epoch:0 step:479 [D loss: 0.553316, acc.: 78.12%] [G loss: 0.069739]\n",
      "epoch:0 step:480 [D loss: 0.563631, acc.: 71.88%] [G loss: 0.074117]\n",
      "epoch:0 step:481 [D loss: 0.554053, acc.: 71.88%] [G loss: 0.103321]\n",
      "epoch:0 step:482 [D loss: 0.560131, acc.: 78.91%] [G loss: 0.117088]\n",
      "epoch:0 step:483 [D loss: 0.565902, acc.: 76.56%] [G loss: 0.108408]\n",
      "epoch:0 step:484 [D loss: 0.542830, acc.: 76.56%] [G loss: 0.102733]\n",
      "epoch:0 step:485 [D loss: 0.544845, acc.: 78.91%] [G loss: 0.110135]\n",
      "epoch:0 step:486 [D loss: 0.603303, acc.: 66.41%] [G loss: 0.091448]\n",
      "epoch:0 step:487 [D loss: 0.561729, acc.: 69.53%] [G loss: 0.081601]\n",
      "epoch:0 step:488 [D loss: 0.555026, acc.: 79.69%] [G loss: 0.102420]\n",
      "epoch:0 step:489 [D loss: 0.581555, acc.: 75.00%] [G loss: 0.094538]\n",
      "epoch:0 step:490 [D loss: 0.553704, acc.: 80.47%] [G loss: 0.078885]\n",
      "epoch:0 step:491 [D loss: 0.548133, acc.: 75.00%] [G loss: 0.107338]\n",
      "epoch:0 step:492 [D loss: 0.580037, acc.: 74.22%] [G loss: 0.088412]\n",
      "epoch:0 step:493 [D loss: 0.564843, acc.: 70.31%] [G loss: 0.084263]\n",
      "epoch:0 step:494 [D loss: 0.558485, acc.: 71.88%] [G loss: 0.104367]\n",
      "epoch:0 step:495 [D loss: 0.548873, acc.: 80.47%] [G loss: 0.101074]\n",
      "epoch:0 step:496 [D loss: 0.600679, acc.: 62.50%] [G loss: 0.121486]\n",
      "epoch:0 step:497 [D loss: 0.562215, acc.: 77.34%] [G loss: 0.107347]\n",
      "epoch:0 step:498 [D loss: 0.553673, acc.: 75.78%] [G loss: 0.120454]\n",
      "epoch:0 step:499 [D loss: 0.520700, acc.: 80.47%] [G loss: 0.130384]\n",
      "epoch:0 step:500 [D loss: 0.593716, acc.: 73.44%] [G loss: 0.090285]\n",
      "epoch:0 step:501 [D loss: 0.583424, acc.: 72.66%] [G loss: 0.084820]\n",
      "epoch:0 step:502 [D loss: 0.570926, acc.: 72.66%] [G loss: 0.084202]\n",
      "epoch:0 step:503 [D loss: 0.553993, acc.: 82.03%] [G loss: 0.094372]\n",
      "epoch:0 step:504 [D loss: 0.564758, acc.: 72.66%] [G loss: 0.123009]\n",
      "epoch:0 step:505 [D loss: 0.596850, acc.: 64.06%] [G loss: 0.108817]\n",
      "epoch:0 step:506 [D loss: 0.531117, acc.: 80.47%] [G loss: 0.127146]\n",
      "epoch:0 step:507 [D loss: 0.559323, acc.: 77.34%] [G loss: 0.130951]\n",
      "epoch:0 step:508 [D loss: 0.523206, acc.: 83.59%] [G loss: 0.140640]\n",
      "epoch:0 step:509 [D loss: 0.594463, acc.: 65.62%] [G loss: 0.097206]\n",
      "epoch:0 step:510 [D loss: 0.615648, acc.: 68.75%] [G loss: 0.062322]\n",
      "epoch:0 step:511 [D loss: 0.568982, acc.: 75.00%] [G loss: 0.064372]\n",
      "epoch:0 step:512 [D loss: 0.539272, acc.: 78.91%] [G loss: 0.086330]\n",
      "epoch:0 step:513 [D loss: 0.546046, acc.: 76.56%] [G loss: 0.111595]\n",
      "epoch:0 step:514 [D loss: 0.539716, acc.: 81.25%] [G loss: 0.126854]\n",
      "epoch:0 step:515 [D loss: 0.550689, acc.: 80.47%] [G loss: 0.103600]\n",
      "epoch:0 step:516 [D loss: 0.540396, acc.: 84.38%] [G loss: 0.097095]\n",
      "epoch:0 step:517 [D loss: 0.552165, acc.: 83.59%] [G loss: 0.094300]\n",
      "epoch:0 step:518 [D loss: 0.526437, acc.: 91.41%] [G loss: 0.116334]\n",
      "epoch:0 step:519 [D loss: 0.539318, acc.: 79.69%] [G loss: 0.104747]\n",
      "epoch:0 step:520 [D loss: 0.548702, acc.: 83.59%] [G loss: 0.122583]\n",
      "epoch:0 step:521 [D loss: 0.569331, acc.: 78.12%] [G loss: 0.092186]\n",
      "epoch:0 step:522 [D loss: 0.528077, acc.: 87.50%] [G loss: 0.128612]\n",
      "epoch:0 step:523 [D loss: 0.520356, acc.: 78.91%] [G loss: 0.149971]\n",
      "epoch:0 step:524 [D loss: 0.565510, acc.: 80.47%] [G loss: 0.112368]\n",
      "epoch:0 step:525 [D loss: 0.562687, acc.: 71.09%] [G loss: 0.095051]\n",
      "epoch:0 step:526 [D loss: 0.515957, acc.: 84.38%] [G loss: 0.148349]\n",
      "epoch:0 step:527 [D loss: 0.524740, acc.: 87.50%] [G loss: 0.176241]\n",
      "epoch:0 step:528 [D loss: 0.542752, acc.: 84.38%] [G loss: 0.124641]\n",
      "epoch:0 step:529 [D loss: 0.541552, acc.: 79.69%] [G loss: 0.126694]\n",
      "epoch:0 step:530 [D loss: 0.511144, acc.: 87.50%] [G loss: 0.161400]\n",
      "epoch:0 step:531 [D loss: 0.583311, acc.: 73.44%] [G loss: 0.126307]\n",
      "epoch:0 step:532 [D loss: 0.532476, acc.: 83.59%] [G loss: 0.116478]\n",
      "epoch:0 step:533 [D loss: 0.544813, acc.: 79.69%] [G loss: 0.133434]\n",
      "epoch:0 step:534 [D loss: 0.565590, acc.: 75.00%] [G loss: 0.145294]\n",
      "epoch:0 step:535 [D loss: 0.576162, acc.: 71.88%] [G loss: 0.119798]\n",
      "epoch:0 step:536 [D loss: 0.589729, acc.: 71.88%] [G loss: 0.078091]\n",
      "epoch:0 step:537 [D loss: 0.579291, acc.: 68.75%] [G loss: 0.088298]\n",
      "epoch:0 step:538 [D loss: 0.536777, acc.: 78.12%] [G loss: 0.136150]\n",
      "epoch:0 step:539 [D loss: 0.571817, acc.: 73.44%] [G loss: 0.125534]\n",
      "epoch:0 step:540 [D loss: 0.573721, acc.: 71.09%] [G loss: 0.098840]\n",
      "epoch:0 step:541 [D loss: 0.559634, acc.: 74.22%] [G loss: 0.113703]\n",
      "epoch:0 step:542 [D loss: 0.597515, acc.: 71.09%] [G loss: 0.101095]\n",
      "epoch:0 step:543 [D loss: 0.578303, acc.: 75.78%] [G loss: 0.094151]\n",
      "epoch:0 step:544 [D loss: 0.549976, acc.: 75.00%] [G loss: 0.112254]\n",
      "epoch:0 step:545 [D loss: 0.550759, acc.: 75.00%] [G loss: 0.107286]\n",
      "epoch:0 step:546 [D loss: 0.537257, acc.: 79.69%] [G loss: 0.119606]\n",
      "epoch:0 step:547 [D loss: 0.528834, acc.: 83.59%] [G loss: 0.160238]\n",
      "epoch:0 step:548 [D loss: 0.565646, acc.: 75.78%] [G loss: 0.167326]\n",
      "epoch:0 step:549 [D loss: 0.574731, acc.: 73.44%] [G loss: 0.119849]\n",
      "epoch:0 step:550 [D loss: 0.580087, acc.: 70.31%] [G loss: 0.095368]\n",
      "epoch:0 step:551 [D loss: 0.567527, acc.: 69.53%] [G loss: 0.105721]\n",
      "epoch:0 step:552 [D loss: 0.574379, acc.: 67.97%] [G loss: 0.125179]\n",
      "epoch:0 step:553 [D loss: 0.579117, acc.: 72.66%] [G loss: 0.144257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:554 [D loss: 0.556585, acc.: 73.44%] [G loss: 0.143628]\n",
      "epoch:0 step:555 [D loss: 0.587446, acc.: 73.44%] [G loss: 0.125198]\n",
      "epoch:0 step:556 [D loss: 0.575673, acc.: 70.31%] [G loss: 0.096030]\n",
      "epoch:0 step:557 [D loss: 0.564744, acc.: 70.31%] [G loss: 0.131575]\n",
      "epoch:0 step:558 [D loss: 0.561544, acc.: 71.88%] [G loss: 0.150684]\n",
      "epoch:0 step:559 [D loss: 0.694065, acc.: 51.56%] [G loss: 0.075621]\n",
      "epoch:0 step:560 [D loss: 0.571284, acc.: 75.00%] [G loss: 0.104753]\n",
      "epoch:0 step:561 [D loss: 0.644332, acc.: 62.50%] [G loss: 0.097936]\n",
      "epoch:0 step:562 [D loss: 0.618292, acc.: 59.38%] [G loss: 0.097849]\n",
      "epoch:0 step:563 [D loss: 0.589646, acc.: 67.19%] [G loss: 0.101816]\n",
      "epoch:0 step:564 [D loss: 0.562746, acc.: 75.78%] [G loss: 0.112784]\n",
      "epoch:0 step:565 [D loss: 0.576516, acc.: 75.00%] [G loss: 0.140503]\n",
      "epoch:0 step:566 [D loss: 0.624586, acc.: 67.19%] [G loss: 0.104449]\n",
      "epoch:0 step:567 [D loss: 0.580145, acc.: 71.88%] [G loss: 0.150428]\n",
      "epoch:0 step:568 [D loss: 0.593787, acc.: 75.00%] [G loss: 0.132760]\n",
      "epoch:0 step:569 [D loss: 0.610840, acc.: 67.19%] [G loss: 0.101397]\n",
      "epoch:0 step:570 [D loss: 0.604318, acc.: 67.97%] [G loss: 0.098996]\n",
      "epoch:0 step:571 [D loss: 0.575968, acc.: 71.88%] [G loss: 0.072021]\n",
      "epoch:0 step:572 [D loss: 0.649987, acc.: 53.91%] [G loss: 0.061864]\n",
      "epoch:0 step:573 [D loss: 0.541758, acc.: 78.12%] [G loss: 0.117795]\n",
      "epoch:0 step:574 [D loss: 0.593387, acc.: 74.22%] [G loss: 0.149139]\n",
      "epoch:0 step:575 [D loss: 0.550186, acc.: 84.38%] [G loss: 0.151430]\n",
      "epoch:0 step:576 [D loss: 0.599258, acc.: 71.88%] [G loss: 0.098278]\n",
      "epoch:0 step:577 [D loss: 0.620168, acc.: 64.06%] [G loss: 0.072279]\n",
      "epoch:0 step:578 [D loss: 0.558085, acc.: 70.31%] [G loss: 0.100160]\n",
      "epoch:0 step:579 [D loss: 0.551181, acc.: 79.69%] [G loss: 0.109430]\n",
      "epoch:0 step:580 [D loss: 0.560498, acc.: 77.34%] [G loss: 0.123494]\n",
      "epoch:0 step:581 [D loss: 0.555501, acc.: 75.78%] [G loss: 0.151047]\n",
      "epoch:0 step:582 [D loss: 0.506783, acc.: 88.28%] [G loss: 0.236548]\n",
      "epoch:0 step:583 [D loss: 0.609396, acc.: 73.44%] [G loss: 0.134955]\n",
      "epoch:0 step:584 [D loss: 0.636285, acc.: 60.16%] [G loss: 0.057623]\n",
      "epoch:0 step:585 [D loss: 0.593086, acc.: 68.75%] [G loss: 0.053974]\n",
      "epoch:0 step:586 [D loss: 0.603368, acc.: 66.41%] [G loss: 0.066439]\n",
      "epoch:0 step:587 [D loss: 0.652428, acc.: 53.12%] [G loss: 0.069403]\n",
      "epoch:0 step:588 [D loss: 0.607762, acc.: 64.84%] [G loss: 0.079341]\n",
      "epoch:0 step:589 [D loss: 0.577113, acc.: 71.88%] [G loss: 0.091903]\n",
      "epoch:0 step:590 [D loss: 0.572603, acc.: 79.69%] [G loss: 0.099687]\n",
      "epoch:0 step:591 [D loss: 0.588721, acc.: 75.78%] [G loss: 0.093211]\n",
      "epoch:0 step:592 [D loss: 0.562140, acc.: 79.69%] [G loss: 0.123701]\n",
      "epoch:0 step:593 [D loss: 0.542737, acc.: 82.03%] [G loss: 0.133333]\n",
      "epoch:0 step:594 [D loss: 0.555973, acc.: 73.44%] [G loss: 0.138847]\n",
      "epoch:0 step:595 [D loss: 0.534671, acc.: 81.25%] [G loss: 0.163136]\n",
      "epoch:0 step:596 [D loss: 0.543863, acc.: 78.91%] [G loss: 0.181057]\n",
      "epoch:0 step:597 [D loss: 0.569914, acc.: 79.69%] [G loss: 0.151694]\n",
      "epoch:0 step:598 [D loss: 0.536564, acc.: 83.59%] [G loss: 0.138659]\n",
      "epoch:0 step:599 [D loss: 0.542096, acc.: 79.69%] [G loss: 0.142207]\n",
      "epoch:0 step:600 [D loss: 0.593574, acc.: 71.88%] [G loss: 0.094520]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 2.218159\n",
      "FID: 194.538712\n",
      "0 = 15.829904230499304\n",
      "1 = 0.2990891058715406\n",
      "2 = 0.9998000264167786\n",
      "3 = 0.9995999932289124\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9995999932289124\n",
      "7 = 14.02757152481077\n",
      "8 = 0.2333948621914966\n",
      "9 = 0.9943000078201294\n",
      "10 = 0.9886000156402588\n",
      "11 = 1.0\n",
      "12 = 1.0\n",
      "13 = 0.9886000156402588\n",
      "14 = 2.218158483505249\n",
      "15 = 6.90326452255249\n",
      "16 = 0.6366875171661377\n",
      "17 = 2.2181591987609863\n",
      "18 = 194.53871154785156\n",
      "epoch:0 step:601 [D loss: 0.594182, acc.: 61.72%] [G loss: 0.112462]\n",
      "epoch:0 step:602 [D loss: 0.523287, acc.: 87.50%] [G loss: 0.135829]\n",
      "epoch:0 step:603 [D loss: 0.533613, acc.: 84.38%] [G loss: 0.172237]\n",
      "epoch:0 step:604 [D loss: 0.603518, acc.: 71.88%] [G loss: 0.175309]\n",
      "epoch:0 step:605 [D loss: 0.573750, acc.: 75.78%] [G loss: 0.144747]\n",
      "epoch:0 step:606 [D loss: 0.606177, acc.: 67.19%] [G loss: 0.098460]\n",
      "epoch:0 step:607 [D loss: 0.583266, acc.: 72.66%] [G loss: 0.099289]\n",
      "epoch:0 step:608 [D loss: 0.565793, acc.: 81.25%] [G loss: 0.101905]\n",
      "epoch:0 step:609 [D loss: 0.568934, acc.: 80.47%] [G loss: 0.103243]\n",
      "epoch:0 step:610 [D loss: 0.538360, acc.: 81.25%] [G loss: 0.113501]\n",
      "epoch:0 step:611 [D loss: 0.536448, acc.: 82.81%] [G loss: 0.125510]\n",
      "epoch:0 step:612 [D loss: 0.544690, acc.: 84.38%] [G loss: 0.124212]\n",
      "epoch:0 step:613 [D loss: 0.531804, acc.: 84.38%] [G loss: 0.154066]\n",
      "epoch:0 step:614 [D loss: 0.532070, acc.: 85.94%] [G loss: 0.141765]\n",
      "epoch:0 step:615 [D loss: 0.582427, acc.: 71.88%] [G loss: 0.115102]\n",
      "epoch:0 step:616 [D loss: 0.542092, acc.: 75.00%] [G loss: 0.174472]\n",
      "epoch:0 step:617 [D loss: 0.530330, acc.: 84.38%] [G loss: 0.158057]\n",
      "epoch:0 step:618 [D loss: 0.542161, acc.: 82.81%] [G loss: 0.148350]\n",
      "epoch:0 step:619 [D loss: 0.555214, acc.: 76.56%] [G loss: 0.134736]\n",
      "epoch:0 step:620 [D loss: 0.545406, acc.: 82.81%] [G loss: 0.153660]\n",
      "epoch:0 step:621 [D loss: 0.562454, acc.: 82.03%] [G loss: 0.125256]\n",
      "epoch:0 step:622 [D loss: 0.634191, acc.: 58.59%] [G loss: 0.061710]\n",
      "epoch:0 step:623 [D loss: 0.586481, acc.: 60.94%] [G loss: 0.067967]\n",
      "epoch:0 step:624 [D loss: 0.523007, acc.: 83.59%] [G loss: 0.111827]\n",
      "epoch:0 step:625 [D loss: 0.517473, acc.: 79.69%] [G loss: 0.159180]\n",
      "epoch:0 step:626 [D loss: 0.518335, acc.: 85.16%] [G loss: 0.177445]\n",
      "epoch:0 step:627 [D loss: 0.548494, acc.: 82.03%] [G loss: 0.146552]\n",
      "epoch:0 step:628 [D loss: 0.519373, acc.: 82.03%] [G loss: 0.170438]\n",
      "epoch:0 step:629 [D loss: 0.522221, acc.: 82.03%] [G loss: 0.145935]\n",
      "epoch:0 step:630 [D loss: 0.521687, acc.: 82.03%] [G loss: 0.148344]\n",
      "epoch:0 step:631 [D loss: 0.520860, acc.: 81.25%] [G loss: 0.176541]\n",
      "epoch:0 step:632 [D loss: 0.535604, acc.: 82.81%] [G loss: 0.156948]\n",
      "epoch:0 step:633 [D loss: 0.522591, acc.: 81.25%] [G loss: 0.175936]\n",
      "epoch:0 step:634 [D loss: 0.512920, acc.: 82.81%] [G loss: 0.232715]\n",
      "epoch:0 step:635 [D loss: 0.531341, acc.: 87.50%] [G loss: 0.184651]\n",
      "epoch:0 step:636 [D loss: 0.583571, acc.: 77.34%] [G loss: 0.103723]\n",
      "epoch:0 step:637 [D loss: 0.557946, acc.: 73.44%] [G loss: 0.131811]\n",
      "epoch:0 step:638 [D loss: 0.508565, acc.: 82.03%] [G loss: 0.177755]\n",
      "epoch:0 step:639 [D loss: 0.564324, acc.: 79.69%] [G loss: 0.152223]\n",
      "epoch:0 step:640 [D loss: 0.592063, acc.: 70.31%] [G loss: 0.114683]\n",
      "epoch:0 step:641 [D loss: 0.560712, acc.: 72.66%] [G loss: 0.118877]\n",
      "epoch:0 step:642 [D loss: 0.538436, acc.: 81.25%] [G loss: 0.154336]\n",
      "epoch:0 step:643 [D loss: 0.538735, acc.: 80.47%] [G loss: 0.132470]\n",
      "epoch:0 step:644 [D loss: 0.535656, acc.: 75.78%] [G loss: 0.137957]\n",
      "epoch:0 step:645 [D loss: 0.546338, acc.: 80.47%] [G loss: 0.145254]\n",
      "epoch:0 step:646 [D loss: 0.554903, acc.: 75.00%] [G loss: 0.153403]\n",
      "epoch:0 step:647 [D loss: 0.597731, acc.: 71.09%] [G loss: 0.135891]\n",
      "epoch:0 step:648 [D loss: 0.517989, acc.: 75.00%] [G loss: 0.176133]\n",
      "epoch:0 step:649 [D loss: 0.551715, acc.: 78.91%] [G loss: 0.176791]\n",
      "epoch:0 step:650 [D loss: 0.555322, acc.: 71.09%] [G loss: 0.148729]\n",
      "epoch:0 step:651 [D loss: 0.542668, acc.: 79.69%] [G loss: 0.150308]\n",
      "epoch:0 step:652 [D loss: 0.570727, acc.: 78.12%] [G loss: 0.120157]\n",
      "epoch:0 step:653 [D loss: 0.553093, acc.: 74.22%] [G loss: 0.127719]\n",
      "epoch:0 step:654 [D loss: 0.533542, acc.: 76.56%] [G loss: 0.163153]\n",
      "epoch:0 step:655 [D loss: 0.540702, acc.: 82.03%] [G loss: 0.190454]\n",
      "epoch:0 step:656 [D loss: 0.547926, acc.: 82.81%] [G loss: 0.140434]\n",
      "epoch:0 step:657 [D loss: 0.545273, acc.: 75.00%] [G loss: 0.138901]\n",
      "epoch:0 step:658 [D loss: 0.567961, acc.: 71.09%] [G loss: 0.141572]\n",
      "epoch:0 step:659 [D loss: 0.518921, acc.: 84.38%] [G loss: 0.160424]\n",
      "epoch:0 step:660 [D loss: 0.531690, acc.: 79.69%] [G loss: 0.183243]\n",
      "epoch:0 step:661 [D loss: 0.548921, acc.: 76.56%] [G loss: 0.166384]\n",
      "epoch:0 step:662 [D loss: 0.594667, acc.: 69.53%] [G loss: 0.125063]\n",
      "epoch:0 step:663 [D loss: 0.615273, acc.: 60.16%] [G loss: 0.136198]\n",
      "epoch:0 step:664 [D loss: 0.564971, acc.: 78.91%] [G loss: 0.177935]\n",
      "epoch:0 step:665 [D loss: 0.587913, acc.: 69.53%] [G loss: 0.198212]\n",
      "epoch:0 step:666 [D loss: 0.574673, acc.: 67.19%] [G loss: 0.237002]\n",
      "epoch:0 step:667 [D loss: 0.575917, acc.: 75.00%] [G loss: 0.221897]\n",
      "epoch:0 step:668 [D loss: 0.564109, acc.: 82.81%] [G loss: 0.167979]\n",
      "epoch:0 step:669 [D loss: 0.535350, acc.: 76.56%] [G loss: 0.155708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:670 [D loss: 0.563219, acc.: 73.44%] [G loss: 0.132915]\n",
      "epoch:0 step:671 [D loss: 0.575069, acc.: 69.53%] [G loss: 0.150313]\n",
      "epoch:0 step:672 [D loss: 0.578415, acc.: 71.88%] [G loss: 0.140907]\n",
      "epoch:0 step:673 [D loss: 0.526289, acc.: 83.59%] [G loss: 0.178528]\n",
      "epoch:0 step:674 [D loss: 0.519521, acc.: 83.59%] [G loss: 0.232384]\n",
      "epoch:0 step:675 [D loss: 0.602062, acc.: 67.97%] [G loss: 0.121500]\n",
      "epoch:0 step:676 [D loss: 0.602623, acc.: 63.28%] [G loss: 0.135275]\n",
      "epoch:0 step:677 [D loss: 0.523393, acc.: 76.56%] [G loss: 0.178519]\n",
      "epoch:0 step:678 [D loss: 0.600963, acc.: 68.75%] [G loss: 0.132058]\n",
      "epoch:0 step:679 [D loss: 0.576415, acc.: 74.22%] [G loss: 0.119401]\n",
      "epoch:0 step:680 [D loss: 0.586167, acc.: 66.41%] [G loss: 0.130158]\n",
      "epoch:0 step:681 [D loss: 0.567596, acc.: 78.12%] [G loss: 0.140671]\n",
      "epoch:0 step:682 [D loss: 0.584559, acc.: 67.19%] [G loss: 0.124411]\n",
      "epoch:0 step:683 [D loss: 0.571768, acc.: 67.97%] [G loss: 0.121048]\n",
      "epoch:0 step:684 [D loss: 0.550462, acc.: 77.34%] [G loss: 0.118718]\n",
      "epoch:0 step:685 [D loss: 0.585258, acc.: 69.53%] [G loss: 0.117300]\n",
      "epoch:0 step:686 [D loss: 0.592577, acc.: 66.41%] [G loss: 0.129788]\n",
      "epoch:0 step:687 [D loss: 0.551070, acc.: 72.66%] [G loss: 0.157629]\n",
      "epoch:0 step:688 [D loss: 0.572141, acc.: 71.09%] [G loss: 0.142211]\n",
      "epoch:0 step:689 [D loss: 0.588846, acc.: 67.19%] [G loss: 0.116233]\n",
      "epoch:0 step:690 [D loss: 0.590928, acc.: 68.75%] [G loss: 0.127100]\n",
      "epoch:0 step:691 [D loss: 0.584739, acc.: 70.31%] [G loss: 0.146565]\n",
      "epoch:0 step:692 [D loss: 0.595632, acc.: 68.75%] [G loss: 0.145190]\n",
      "epoch:0 step:693 [D loss: 0.579118, acc.: 75.78%] [G loss: 0.150032]\n",
      "epoch:0 step:694 [D loss: 0.525691, acc.: 83.59%] [G loss: 0.164984]\n",
      "epoch:0 step:695 [D loss: 0.603153, acc.: 69.53%] [G loss: 0.110950]\n",
      "epoch:0 step:696 [D loss: 0.642338, acc.: 60.94%] [G loss: 0.111102]\n",
      "epoch:0 step:697 [D loss: 0.592101, acc.: 73.44%] [G loss: 0.095448]\n",
      "epoch:0 step:698 [D loss: 0.568036, acc.: 72.66%] [G loss: 0.101521]\n",
      "epoch:0 step:699 [D loss: 0.555796, acc.: 76.56%] [G loss: 0.125163]\n",
      "epoch:0 step:700 [D loss: 0.557467, acc.: 78.12%] [G loss: 0.135106]\n",
      "epoch:0 step:701 [D loss: 0.548098, acc.: 81.25%] [G loss: 0.140310]\n",
      "epoch:0 step:702 [D loss: 0.554280, acc.: 78.91%] [G loss: 0.134300]\n",
      "epoch:0 step:703 [D loss: 0.540695, acc.: 85.16%] [G loss: 0.142220]\n",
      "epoch:0 step:704 [D loss: 0.559852, acc.: 73.44%] [G loss: 0.145392]\n",
      "epoch:0 step:705 [D loss: 0.518571, acc.: 83.59%] [G loss: 0.140536]\n",
      "epoch:0 step:706 [D loss: 0.540638, acc.: 77.34%] [G loss: 0.176137]\n",
      "epoch:0 step:707 [D loss: 0.522879, acc.: 86.72%] [G loss: 0.204482]\n",
      "epoch:0 step:708 [D loss: 0.534514, acc.: 84.38%] [G loss: 0.204184]\n",
      "epoch:0 step:709 [D loss: 0.490618, acc.: 91.41%] [G loss: 0.242773]\n",
      "epoch:0 step:710 [D loss: 0.633548, acc.: 65.62%] [G loss: 0.119357]\n",
      "epoch:0 step:711 [D loss: 0.601220, acc.: 64.84%] [G loss: 0.104135]\n",
      "epoch:0 step:712 [D loss: 0.541752, acc.: 74.22%] [G loss: 0.146760]\n",
      "epoch:0 step:713 [D loss: 0.532082, acc.: 78.12%] [G loss: 0.187181]\n",
      "epoch:0 step:714 [D loss: 0.577790, acc.: 71.09%] [G loss: 0.159953]\n",
      "epoch:0 step:715 [D loss: 0.573713, acc.: 76.56%] [G loss: 0.117447]\n",
      "epoch:0 step:716 [D loss: 0.566514, acc.: 71.88%] [G loss: 0.116994]\n",
      "epoch:0 step:717 [D loss: 0.544045, acc.: 82.81%] [G loss: 0.137534]\n",
      "epoch:0 step:718 [D loss: 0.558406, acc.: 76.56%] [G loss: 0.112519]\n",
      "epoch:0 step:719 [D loss: 0.553222, acc.: 76.56%] [G loss: 0.133027]\n",
      "epoch:0 step:720 [D loss: 0.568818, acc.: 78.12%] [G loss: 0.118875]\n",
      "epoch:0 step:721 [D loss: 0.560441, acc.: 76.56%] [G loss: 0.165380]\n",
      "epoch:0 step:722 [D loss: 0.531123, acc.: 85.16%] [G loss: 0.184467]\n",
      "epoch:0 step:723 [D loss: 0.601019, acc.: 71.88%] [G loss: 0.131327]\n",
      "epoch:0 step:724 [D loss: 0.547920, acc.: 76.56%] [G loss: 0.131600]\n",
      "epoch:0 step:725 [D loss: 0.597633, acc.: 65.62%] [G loss: 0.166316]\n",
      "epoch:0 step:726 [D loss: 0.574567, acc.: 78.12%] [G loss: 0.188708]\n",
      "epoch:0 step:727 [D loss: 0.544553, acc.: 84.38%] [G loss: 0.155244]\n",
      "epoch:0 step:728 [D loss: 0.532203, acc.: 82.03%] [G loss: 0.146825]\n",
      "epoch:0 step:729 [D loss: 0.577455, acc.: 72.66%] [G loss: 0.102437]\n",
      "epoch:0 step:730 [D loss: 0.558699, acc.: 75.78%] [G loss: 0.166609]\n",
      "epoch:0 step:731 [D loss: 0.537040, acc.: 85.94%] [G loss: 0.155370]\n",
      "epoch:0 step:732 [D loss: 0.552806, acc.: 78.12%] [G loss: 0.147408]\n",
      "epoch:0 step:733 [D loss: 0.526022, acc.: 86.72%] [G loss: 0.147777]\n",
      "epoch:0 step:734 [D loss: 0.565037, acc.: 76.56%] [G loss: 0.121742]\n",
      "epoch:0 step:735 [D loss: 0.575497, acc.: 77.34%] [G loss: 0.101316]\n",
      "epoch:0 step:736 [D loss: 0.519885, acc.: 79.69%] [G loss: 0.145069]\n",
      "epoch:0 step:737 [D loss: 0.541609, acc.: 80.47%] [G loss: 0.149384]\n",
      "epoch:0 step:738 [D loss: 0.548428, acc.: 85.16%] [G loss: 0.136457]\n",
      "epoch:0 step:739 [D loss: 0.582871, acc.: 72.66%] [G loss: 0.128873]\n",
      "epoch:0 step:740 [D loss: 0.549874, acc.: 78.91%] [G loss: 0.127011]\n",
      "epoch:0 step:741 [D loss: 0.535054, acc.: 81.25%] [G loss: 0.154599]\n",
      "epoch:0 step:742 [D loss: 0.534564, acc.: 83.59%] [G loss: 0.167230]\n",
      "epoch:0 step:743 [D loss: 0.519605, acc.: 84.38%] [G loss: 0.185477]\n",
      "epoch:0 step:744 [D loss: 0.556865, acc.: 76.56%] [G loss: 0.177398]\n",
      "epoch:0 step:745 [D loss: 0.568883, acc.: 75.78%] [G loss: 0.161466]\n",
      "epoch:0 step:746 [D loss: 0.525377, acc.: 82.03%] [G loss: 0.145909]\n",
      "epoch:0 step:747 [D loss: 0.510010, acc.: 91.41%] [G loss: 0.171996]\n",
      "epoch:0 step:748 [D loss: 0.535684, acc.: 82.03%] [G loss: 0.168906]\n",
      "epoch:0 step:749 [D loss: 0.510801, acc.: 88.28%] [G loss: 0.195583]\n",
      "epoch:0 step:750 [D loss: 0.534328, acc.: 80.47%] [G loss: 0.147302]\n",
      "epoch:0 step:751 [D loss: 0.536340, acc.: 78.12%] [G loss: 0.153146]\n",
      "epoch:0 step:752 [D loss: 0.477330, acc.: 92.19%] [G loss: 0.225081]\n",
      "epoch:0 step:753 [D loss: 0.511699, acc.: 83.59%] [G loss: 0.202788]\n",
      "epoch:0 step:754 [D loss: 0.517480, acc.: 82.81%] [G loss: 0.194439]\n",
      "epoch:0 step:755 [D loss: 0.529110, acc.: 81.25%] [G loss: 0.194730]\n",
      "epoch:0 step:756 [D loss: 0.517954, acc.: 85.16%] [G loss: 0.198744]\n",
      "epoch:0 step:757 [D loss: 0.514601, acc.: 86.72%] [G loss: 0.197629]\n",
      "epoch:0 step:758 [D loss: 0.527901, acc.: 84.38%] [G loss: 0.157143]\n",
      "epoch:0 step:759 [D loss: 0.521425, acc.: 84.38%] [G loss: 0.166888]\n",
      "epoch:0 step:760 [D loss: 0.504544, acc.: 84.38%] [G loss: 0.194272]\n",
      "epoch:0 step:761 [D loss: 0.526113, acc.: 82.81%] [G loss: 0.162514]\n",
      "epoch:0 step:762 [D loss: 0.526216, acc.: 79.69%] [G loss: 0.173624]\n",
      "epoch:0 step:763 [D loss: 0.499590, acc.: 82.03%] [G loss: 0.199609]\n",
      "epoch:0 step:764 [D loss: 0.482555, acc.: 85.94%] [G loss: 0.203892]\n",
      "epoch:0 step:765 [D loss: 0.615846, acc.: 69.53%] [G loss: 0.122512]\n",
      "epoch:0 step:766 [D loss: 0.606136, acc.: 63.28%] [G loss: 0.097932]\n",
      "epoch:0 step:767 [D loss: 0.538574, acc.: 73.44%] [G loss: 0.166899]\n",
      "epoch:0 step:768 [D loss: 0.488151, acc.: 87.50%] [G loss: 0.204717]\n",
      "epoch:0 step:769 [D loss: 0.473732, acc.: 85.16%] [G loss: 0.248769]\n",
      "epoch:0 step:770 [D loss: 0.497506, acc.: 81.25%] [G loss: 0.255261]\n",
      "epoch:0 step:771 [D loss: 0.560746, acc.: 74.22%] [G loss: 0.150487]\n",
      "epoch:0 step:772 [D loss: 0.538960, acc.: 74.22%] [G loss: 0.164112]\n",
      "epoch:0 step:773 [D loss: 0.510744, acc.: 82.03%] [G loss: 0.216646]\n",
      "epoch:0 step:774 [D loss: 0.475384, acc.: 89.84%] [G loss: 0.265223]\n",
      "epoch:0 step:775 [D loss: 0.497378, acc.: 82.81%] [G loss: 0.256826]\n",
      "epoch:0 step:776 [D loss: 0.539382, acc.: 76.56%] [G loss: 0.200691]\n",
      "epoch:0 step:777 [D loss: 0.521632, acc.: 81.25%] [G loss: 0.196265]\n",
      "epoch:0 step:778 [D loss: 0.551796, acc.: 75.00%] [G loss: 0.203914]\n",
      "epoch:0 step:779 [D loss: 0.594823, acc.: 64.84%] [G loss: 0.149085]\n",
      "epoch:0 step:780 [D loss: 0.587707, acc.: 68.75%] [G loss: 0.192784]\n",
      "epoch:0 step:781 [D loss: 0.520809, acc.: 76.56%] [G loss: 0.236455]\n",
      "epoch:0 step:782 [D loss: 0.552938, acc.: 75.78%] [G loss: 0.220852]\n",
      "epoch:0 step:783 [D loss: 0.562099, acc.: 78.12%] [G loss: 0.179948]\n",
      "epoch:0 step:784 [D loss: 0.583021, acc.: 67.97%] [G loss: 0.144612]\n",
      "epoch:0 step:785 [D loss: 0.559715, acc.: 75.78%] [G loss: 0.165594]\n",
      "epoch:0 step:786 [D loss: 0.512671, acc.: 85.16%] [G loss: 0.188932]\n",
      "epoch:0 step:787 [D loss: 0.582869, acc.: 71.09%] [G loss: 0.196614]\n",
      "epoch:0 step:788 [D loss: 0.562206, acc.: 79.69%] [G loss: 0.196869]\n",
      "epoch:0 step:789 [D loss: 0.591245, acc.: 70.31%] [G loss: 0.167001]\n",
      "epoch:0 step:790 [D loss: 0.547383, acc.: 75.78%] [G loss: 0.188754]\n",
      "epoch:0 step:791 [D loss: 0.516321, acc.: 82.03%] [G loss: 0.208969]\n",
      "epoch:0 step:792 [D loss: 0.515368, acc.: 76.56%] [G loss: 0.315491]\n",
      "epoch:0 step:793 [D loss: 0.493161, acc.: 82.81%] [G loss: 0.303241]\n",
      "epoch:0 step:794 [D loss: 0.544163, acc.: 76.56%] [G loss: 0.220117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:795 [D loss: 0.504486, acc.: 80.47%] [G loss: 0.226010]\n",
      "epoch:0 step:796 [D loss: 0.476369, acc.: 91.41%] [G loss: 0.249400]\n",
      "epoch:0 step:797 [D loss: 0.567536, acc.: 82.81%] [G loss: 0.169728]\n",
      "epoch:0 step:798 [D loss: 0.559790, acc.: 76.56%] [G loss: 0.178716]\n",
      "epoch:0 step:799 [D loss: 0.529409, acc.: 82.03%] [G loss: 0.177607]\n",
      "epoch:0 step:800 [D loss: 0.605481, acc.: 68.75%] [G loss: 0.172082]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 2.622868\n",
      "FID: 175.531754\n",
      "0 = 15.484608063316353\n",
      "1 = 0.27390554424745517\n",
      "2 = 0.9997000098228455\n",
      "3 = 0.9994000196456909\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9994000196456909\n",
      "7 = 13.558479006052062\n",
      "8 = 0.2308493881518225\n",
      "9 = 0.9919999837875366\n",
      "10 = 0.984000027179718\n",
      "11 = 1.0\n",
      "12 = 1.0\n",
      "13 = 0.984000027179718\n",
      "14 = 2.622868537902832\n",
      "15 = 7.387226104736328\n",
      "16 = 0.5455951690673828\n",
      "17 = 2.622868299484253\n",
      "18 = 175.53175354003906\n",
      "epoch:0 step:801 [D loss: 0.514636, acc.: 86.72%] [G loss: 0.263392]\n",
      "epoch:0 step:802 [D loss: 0.517091, acc.: 87.50%] [G loss: 0.304088]\n",
      "epoch:0 step:803 [D loss: 0.515993, acc.: 82.03%] [G loss: 0.265688]\n",
      "epoch:0 step:804 [D loss: 0.583137, acc.: 71.88%] [G loss: 0.150948]\n",
      "epoch:0 step:805 [D loss: 0.579524, acc.: 71.09%] [G loss: 0.145033]\n",
      "epoch:0 step:806 [D loss: 0.535730, acc.: 75.00%] [G loss: 0.199003]\n",
      "epoch:0 step:807 [D loss: 0.544971, acc.: 75.00%] [G loss: 0.206780]\n",
      "epoch:0 step:808 [D loss: 0.593538, acc.: 70.31%] [G loss: 0.155368]\n",
      "epoch:0 step:809 [D loss: 0.567294, acc.: 73.44%] [G loss: 0.219930]\n",
      "epoch:0 step:810 [D loss: 0.548269, acc.: 74.22%] [G loss: 0.236507]\n",
      "epoch:0 step:811 [D loss: 0.543900, acc.: 82.81%] [G loss: 0.208374]\n",
      "epoch:0 step:812 [D loss: 0.624606, acc.: 60.94%] [G loss: 0.167886]\n",
      "epoch:0 step:813 [D loss: 0.553963, acc.: 73.44%] [G loss: 0.195610]\n",
      "epoch:0 step:814 [D loss: 0.588879, acc.: 67.19%] [G loss: 0.208979]\n",
      "epoch:0 step:815 [D loss: 0.594801, acc.: 71.88%] [G loss: 0.175470]\n",
      "epoch:0 step:816 [D loss: 0.506835, acc.: 82.03%] [G loss: 0.229365]\n",
      "epoch:0 step:817 [D loss: 0.513625, acc.: 71.88%] [G loss: 0.283809]\n",
      "epoch:0 step:818 [D loss: 0.559200, acc.: 76.56%] [G loss: 0.280717]\n",
      "epoch:0 step:819 [D loss: 0.536213, acc.: 84.38%] [G loss: 0.247463]\n",
      "epoch:0 step:820 [D loss: 0.633936, acc.: 61.72%] [G loss: 0.149768]\n",
      "epoch:0 step:821 [D loss: 0.578027, acc.: 68.75%] [G loss: 0.131364]\n",
      "epoch:0 step:822 [D loss: 0.548791, acc.: 79.69%] [G loss: 0.214852]\n",
      "epoch:0 step:823 [D loss: 0.581274, acc.: 73.44%] [G loss: 0.164272]\n",
      "epoch:0 step:824 [D loss: 0.579839, acc.: 71.09%] [G loss: 0.155701]\n",
      "epoch:0 step:825 [D loss: 0.589230, acc.: 70.31%] [G loss: 0.150670]\n",
      "epoch:0 step:826 [D loss: 0.580977, acc.: 72.66%] [G loss: 0.161143]\n",
      "epoch:0 step:827 [D loss: 0.633278, acc.: 63.28%] [G loss: 0.146741]\n",
      "epoch:0 step:828 [D loss: 0.610337, acc.: 67.97%] [G loss: 0.174333]\n",
      "epoch:0 step:829 [D loss: 0.589722, acc.: 71.09%] [G loss: 0.181355]\n",
      "epoch:0 step:830 [D loss: 0.564359, acc.: 74.22%] [G loss: 0.144684]\n",
      "epoch:0 step:831 [D loss: 0.595644, acc.: 71.09%] [G loss: 0.124177]\n",
      "epoch:0 step:832 [D loss: 0.560336, acc.: 71.09%] [G loss: 0.171361]\n",
      "epoch:0 step:833 [D loss: 0.610780, acc.: 67.97%] [G loss: 0.182549]\n",
      "epoch:0 step:834 [D loss: 0.576678, acc.: 79.69%] [G loss: 0.179455]\n",
      "epoch:0 step:835 [D loss: 0.566580, acc.: 71.88%] [G loss: 0.150916]\n",
      "epoch:0 step:836 [D loss: 0.578215, acc.: 73.44%] [G loss: 0.135771]\n",
      "epoch:0 step:837 [D loss: 0.529795, acc.: 83.59%] [G loss: 0.158600]\n",
      "epoch:0 step:838 [D loss: 0.530408, acc.: 82.81%] [G loss: 0.152651]\n",
      "epoch:0 step:839 [D loss: 0.542481, acc.: 78.12%] [G loss: 0.196590]\n",
      "epoch:0 step:840 [D loss: 0.539215, acc.: 78.12%] [G loss: 0.188876]\n",
      "epoch:0 step:841 [D loss: 0.494176, acc.: 87.50%] [G loss: 0.227286]\n",
      "epoch:0 step:842 [D loss: 0.529648, acc.: 82.81%] [G loss: 0.231462]\n",
      "epoch:0 step:843 [D loss: 0.539145, acc.: 79.69%] [G loss: 0.196998]\n",
      "epoch:0 step:844 [D loss: 0.577835, acc.: 70.31%] [G loss: 0.147015]\n",
      "epoch:0 step:845 [D loss: 0.561089, acc.: 77.34%] [G loss: 0.161020]\n",
      "epoch:0 step:846 [D loss: 0.513156, acc.: 85.94%] [G loss: 0.188111]\n",
      "epoch:0 step:847 [D loss: 0.544791, acc.: 82.81%] [G loss: 0.161178]\n",
      "epoch:0 step:848 [D loss: 0.537244, acc.: 80.47%] [G loss: 0.150584]\n",
      "epoch:0 step:849 [D loss: 0.520837, acc.: 82.81%] [G loss: 0.208606]\n",
      "epoch:0 step:850 [D loss: 0.516060, acc.: 92.19%] [G loss: 0.181801]\n",
      "epoch:0 step:851 [D loss: 0.529616, acc.: 83.59%] [G loss: 0.166400]\n",
      "epoch:0 step:852 [D loss: 0.519381, acc.: 85.94%] [G loss: 0.180716]\n",
      "epoch:0 step:853 [D loss: 0.515714, acc.: 82.03%] [G loss: 0.183660]\n",
      "epoch:0 step:854 [D loss: 0.489287, acc.: 87.50%] [G loss: 0.212253]\n",
      "epoch:0 step:855 [D loss: 0.498689, acc.: 84.38%] [G loss: 0.199245]\n",
      "epoch:0 step:856 [D loss: 0.507027, acc.: 84.38%] [G loss: 0.187557]\n",
      "epoch:0 step:857 [D loss: 0.499658, acc.: 82.81%] [G loss: 0.206077]\n",
      "epoch:0 step:858 [D loss: 0.607201, acc.: 71.09%] [G loss: 0.126358]\n",
      "epoch:0 step:859 [D loss: 0.539166, acc.: 80.47%] [G loss: 0.156937]\n",
      "epoch:0 step:860 [D loss: 0.495473, acc.: 84.38%] [G loss: 0.221643]\n",
      "epoch:0 step:861 [D loss: 0.527793, acc.: 82.81%] [G loss: 0.187862]\n",
      "epoch:0 step:862 [D loss: 0.502959, acc.: 86.72%] [G loss: 0.191195]\n",
      "epoch:0 step:863 [D loss: 0.491483, acc.: 83.59%] [G loss: 0.225563]\n",
      "epoch:0 step:864 [D loss: 0.508820, acc.: 83.59%] [G loss: 0.190951]\n",
      "epoch:0 step:865 [D loss: 0.534963, acc.: 78.91%] [G loss: 0.180378]\n",
      "epoch:0 step:866 [D loss: 0.504099, acc.: 85.16%] [G loss: 0.190891]\n",
      "epoch:0 step:867 [D loss: 0.532150, acc.: 82.03%] [G loss: 0.192358]\n",
      "epoch:0 step:868 [D loss: 0.539543, acc.: 81.25%] [G loss: 0.180127]\n",
      "epoch:0 step:869 [D loss: 0.505503, acc.: 81.25%] [G loss: 0.216626]\n",
      "epoch:0 step:870 [D loss: 0.523333, acc.: 79.69%] [G loss: 0.251337]\n",
      "epoch:0 step:871 [D loss: 0.528889, acc.: 78.91%] [G loss: 0.221577]\n",
      "epoch:0 step:872 [D loss: 0.512394, acc.: 82.81%] [G loss: 0.219968]\n",
      "epoch:0 step:873 [D loss: 0.552473, acc.: 75.00%] [G loss: 0.188528]\n",
      "epoch:0 step:874 [D loss: 0.573154, acc.: 68.75%] [G loss: 0.155265]\n",
      "epoch:0 step:875 [D loss: 0.554545, acc.: 76.56%] [G loss: 0.161596]\n",
      "epoch:0 step:876 [D loss: 0.561225, acc.: 75.00%] [G loss: 0.163339]\n",
      "epoch:0 step:877 [D loss: 0.555332, acc.: 72.66%] [G loss: 0.188879]\n",
      "epoch:0 step:878 [D loss: 0.547480, acc.: 80.47%] [G loss: 0.172108]\n",
      "epoch:0 step:879 [D loss: 0.619651, acc.: 65.62%] [G loss: 0.136558]\n",
      "epoch:0 step:880 [D loss: 0.582817, acc.: 73.44%] [G loss: 0.124593]\n",
      "epoch:0 step:881 [D loss: 0.566072, acc.: 71.09%] [G loss: 0.160884]\n",
      "epoch:0 step:882 [D loss: 0.557472, acc.: 68.75%] [G loss: 0.190308]\n",
      "epoch:0 step:883 [D loss: 0.590632, acc.: 73.44%] [G loss: 0.145369]\n",
      "epoch:0 step:884 [D loss: 0.592216, acc.: 67.97%] [G loss: 0.152265]\n",
      "epoch:0 step:885 [D loss: 0.526528, acc.: 78.12%] [G loss: 0.308915]\n",
      "epoch:0 step:886 [D loss: 0.532079, acc.: 78.91%] [G loss: 0.311283]\n",
      "epoch:0 step:887 [D loss: 0.549734, acc.: 71.09%] [G loss: 0.268064]\n",
      "epoch:0 step:888 [D loss: 0.535041, acc.: 75.78%] [G loss: 0.237364]\n",
      "epoch:0 step:889 [D loss: 0.592537, acc.: 70.31%] [G loss: 0.244414]\n",
      "epoch:0 step:890 [D loss: 0.525889, acc.: 81.25%] [G loss: 0.306997]\n",
      "epoch:0 step:891 [D loss: 0.638440, acc.: 64.06%] [G loss: 0.133922]\n",
      "epoch:0 step:892 [D loss: 0.591767, acc.: 71.09%] [G loss: 0.125675]\n",
      "epoch:0 step:893 [D loss: 0.640594, acc.: 57.03%] [G loss: 0.205798]\n",
      "epoch:0 step:894 [D loss: 0.583535, acc.: 70.31%] [G loss: 0.271798]\n",
      "epoch:0 step:895 [D loss: 0.609311, acc.: 68.75%] [G loss: 0.145616]\n",
      "epoch:0 step:896 [D loss: 0.599917, acc.: 67.97%] [G loss: 0.136810]\n",
      "epoch:0 step:897 [D loss: 0.536874, acc.: 80.47%] [G loss: 0.168887]\n",
      "epoch:0 step:898 [D loss: 0.578888, acc.: 70.31%] [G loss: 0.197606]\n",
      "epoch:0 step:899 [D loss: 0.580830, acc.: 73.44%] [G loss: 0.200968]\n",
      "epoch:0 step:900 [D loss: 0.595684, acc.: 74.22%] [G loss: 0.143486]\n",
      "epoch:0 step:901 [D loss: 0.559432, acc.: 73.44%] [G loss: 0.154679]\n",
      "epoch:0 step:902 [D loss: 0.578013, acc.: 78.91%] [G loss: 0.128262]\n",
      "epoch:0 step:903 [D loss: 0.577305, acc.: 68.75%] [G loss: 0.117773]\n",
      "epoch:0 step:904 [D loss: 0.599456, acc.: 70.31%] [G loss: 0.144366]\n",
      "epoch:0 step:905 [D loss: 0.580130, acc.: 78.12%] [G loss: 0.161616]\n",
      "epoch:0 step:906 [D loss: 0.538127, acc.: 76.56%] [G loss: 0.158024]\n",
      "epoch:0 step:907 [D loss: 0.588206, acc.: 75.78%] [G loss: 0.153988]\n",
      "epoch:0 step:908 [D loss: 0.539479, acc.: 85.94%] [G loss: 0.150485]\n",
      "epoch:0 step:909 [D loss: 0.560007, acc.: 78.12%] [G loss: 0.129759]\n",
      "epoch:0 step:910 [D loss: 0.555767, acc.: 75.00%] [G loss: 0.177548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:911 [D loss: 0.573435, acc.: 68.75%] [G loss: 0.170340]\n",
      "epoch:0 step:912 [D loss: 0.497666, acc.: 87.50%] [G loss: 0.228919]\n",
      "epoch:0 step:913 [D loss: 0.575997, acc.: 75.00%] [G loss: 0.180196]\n",
      "epoch:0 step:914 [D loss: 0.538839, acc.: 78.12%] [G loss: 0.185722]\n",
      "epoch:0 step:915 [D loss: 0.567312, acc.: 77.34%] [G loss: 0.160422]\n",
      "epoch:0 step:916 [D loss: 0.544440, acc.: 77.34%] [G loss: 0.148090]\n",
      "epoch:0 step:917 [D loss: 0.529955, acc.: 82.03%] [G loss: 0.170531]\n",
      "epoch:0 step:918 [D loss: 0.504941, acc.: 89.06%] [G loss: 0.185360]\n",
      "epoch:0 step:919 [D loss: 0.525795, acc.: 85.16%] [G loss: 0.177697]\n",
      "epoch:0 step:920 [D loss: 0.578105, acc.: 78.12%] [G loss: 0.185812]\n",
      "epoch:0 step:921 [D loss: 0.491483, acc.: 84.38%] [G loss: 0.224765]\n",
      "epoch:0 step:922 [D loss: 0.516963, acc.: 77.34%] [G loss: 0.201680]\n",
      "epoch:0 step:923 [D loss: 0.480101, acc.: 83.59%] [G loss: 0.210241]\n",
      "epoch:0 step:924 [D loss: 0.503617, acc.: 88.28%] [G loss: 0.210899]\n",
      "epoch:0 step:925 [D loss: 0.472832, acc.: 85.16%] [G loss: 0.244387]\n",
      "epoch:0 step:926 [D loss: 0.487887, acc.: 82.03%] [G loss: 0.260764]\n",
      "epoch:0 step:927 [D loss: 0.469092, acc.: 87.50%] [G loss: 0.318210]\n",
      "epoch:0 step:928 [D loss: 0.666832, acc.: 67.19%] [G loss: 0.161370]\n",
      "epoch:0 step:929 [D loss: 0.529285, acc.: 80.47%] [G loss: 0.285185]\n",
      "epoch:0 step:930 [D loss: 0.469884, acc.: 85.16%] [G loss: 0.338708]\n",
      "epoch:0 step:931 [D loss: 0.558881, acc.: 75.78%] [G loss: 0.191855]\n",
      "epoch:0 step:932 [D loss: 0.568344, acc.: 75.00%] [G loss: 0.117024]\n",
      "epoch:0 step:933 [D loss: 0.533874, acc.: 77.34%] [G loss: 0.156688]\n",
      "epoch:0 step:934 [D loss: 0.501783, acc.: 82.03%] [G loss: 0.203926]\n",
      "epoch:0 step:935 [D loss: 0.398944, acc.: 94.53%] [G loss: 0.293188]\n",
      "epoch:0 step:936 [D loss: 0.389232, acc.: 89.84%] [G loss: 0.416476]\n",
      "epoch:0 step:937 [D loss: 0.569831, acc.: 72.66%] [G loss: 0.290590]\n",
      "epoch:1 step:938 [D loss: 0.527436, acc.: 78.91%] [G loss: 0.233388]\n",
      "epoch:1 step:939 [D loss: 0.554517, acc.: 68.75%] [G loss: 0.256318]\n",
      "epoch:1 step:940 [D loss: 0.534442, acc.: 79.69%] [G loss: 0.235754]\n",
      "epoch:1 step:941 [D loss: 0.511050, acc.: 78.91%] [G loss: 0.241272]\n",
      "epoch:1 step:942 [D loss: 0.512869, acc.: 82.03%] [G loss: 0.232814]\n",
      "epoch:1 step:943 [D loss: 0.513688, acc.: 82.03%] [G loss: 0.232356]\n",
      "epoch:1 step:944 [D loss: 0.498798, acc.: 88.28%] [G loss: 0.227533]\n",
      "epoch:1 step:945 [D loss: 0.547348, acc.: 78.12%] [G loss: 0.220899]\n",
      "epoch:1 step:946 [D loss: 0.547089, acc.: 77.34%] [G loss: 0.192487]\n",
      "epoch:1 step:947 [D loss: 0.535342, acc.: 79.69%] [G loss: 0.206524]\n",
      "epoch:1 step:948 [D loss: 0.510407, acc.: 82.03%] [G loss: 0.229159]\n",
      "epoch:1 step:949 [D loss: 0.519206, acc.: 82.81%] [G loss: 0.259865]\n",
      "epoch:1 step:950 [D loss: 0.471016, acc.: 88.28%] [G loss: 0.299624]\n",
      "epoch:1 step:951 [D loss: 0.523039, acc.: 83.59%] [G loss: 0.242145]\n",
      "epoch:1 step:952 [D loss: 0.528015, acc.: 79.69%] [G loss: 0.188734]\n",
      "epoch:1 step:953 [D loss: 0.507446, acc.: 83.59%] [G loss: 0.209701]\n",
      "epoch:1 step:954 [D loss: 0.539906, acc.: 75.78%] [G loss: 0.177402]\n",
      "epoch:1 step:955 [D loss: 0.558014, acc.: 74.22%] [G loss: 0.205788]\n",
      "epoch:1 step:956 [D loss: 0.520407, acc.: 83.59%] [G loss: 0.223217]\n",
      "epoch:1 step:957 [D loss: 0.564254, acc.: 74.22%] [G loss: 0.191410]\n",
      "epoch:1 step:958 [D loss: 0.504917, acc.: 86.72%] [G loss: 0.250376]\n",
      "epoch:1 step:959 [D loss: 0.473708, acc.: 89.06%] [G loss: 0.282664]\n",
      "epoch:1 step:960 [D loss: 0.577693, acc.: 78.91%] [G loss: 0.165484]\n",
      "epoch:1 step:961 [D loss: 0.539492, acc.: 79.69%] [G loss: 0.175822]\n",
      "epoch:1 step:962 [D loss: 0.528474, acc.: 78.12%] [G loss: 0.220547]\n",
      "epoch:1 step:963 [D loss: 0.556590, acc.: 72.66%] [G loss: 0.173827]\n",
      "epoch:1 step:964 [D loss: 0.496596, acc.: 84.38%] [G loss: 0.260799]\n",
      "epoch:1 step:965 [D loss: 0.553427, acc.: 79.69%] [G loss: 0.222508]\n",
      "epoch:1 step:966 [D loss: 0.512111, acc.: 83.59%] [G loss: 0.225722]\n",
      "epoch:1 step:967 [D loss: 0.539961, acc.: 77.34%] [G loss: 0.187242]\n",
      "epoch:1 step:968 [D loss: 0.473885, acc.: 83.59%] [G loss: 0.259683]\n",
      "epoch:1 step:969 [D loss: 0.511485, acc.: 82.81%] [G loss: 0.302617]\n",
      "epoch:1 step:970 [D loss: 0.480802, acc.: 89.06%] [G loss: 0.291275]\n",
      "epoch:1 step:971 [D loss: 0.508995, acc.: 83.59%] [G loss: 0.296960]\n",
      "epoch:1 step:972 [D loss: 0.532773, acc.: 79.69%] [G loss: 0.290588]\n",
      "epoch:1 step:973 [D loss: 0.504630, acc.: 85.16%] [G loss: 0.367622]\n",
      "epoch:1 step:974 [D loss: 0.593912, acc.: 72.66%] [G loss: 0.194855]\n",
      "epoch:1 step:975 [D loss: 0.571860, acc.: 74.22%] [G loss: 0.164451]\n",
      "epoch:1 step:976 [D loss: 0.504216, acc.: 81.25%] [G loss: 0.226565]\n",
      "epoch:1 step:977 [D loss: 0.492049, acc.: 82.81%] [G loss: 0.274223]\n",
      "epoch:1 step:978 [D loss: 0.545168, acc.: 75.00%] [G loss: 0.220157]\n",
      "epoch:1 step:979 [D loss: 0.584447, acc.: 71.88%] [G loss: 0.187694]\n",
      "epoch:1 step:980 [D loss: 0.550570, acc.: 75.78%] [G loss: 0.183130]\n",
      "epoch:1 step:981 [D loss: 0.525900, acc.: 79.69%] [G loss: 0.246306]\n",
      "epoch:1 step:982 [D loss: 0.513869, acc.: 79.69%] [G loss: 0.252357]\n",
      "epoch:1 step:983 [D loss: 0.549907, acc.: 75.78%] [G loss: 0.217211]\n",
      "epoch:1 step:984 [D loss: 0.558639, acc.: 76.56%] [G loss: 0.179252]\n",
      "epoch:1 step:985 [D loss: 0.541037, acc.: 75.00%] [G loss: 0.240089]\n",
      "epoch:1 step:986 [D loss: 0.543490, acc.: 76.56%] [G loss: 0.245091]\n",
      "epoch:1 step:987 [D loss: 0.558929, acc.: 75.78%] [G loss: 0.167586]\n",
      "epoch:1 step:988 [D loss: 0.520890, acc.: 75.78%] [G loss: 0.191112]\n",
      "epoch:1 step:989 [D loss: 0.501579, acc.: 86.72%] [G loss: 0.230295]\n",
      "epoch:1 step:990 [D loss: 0.524901, acc.: 80.47%] [G loss: 0.248718]\n",
      "epoch:1 step:991 [D loss: 0.518890, acc.: 81.25%] [G loss: 0.284061]\n",
      "epoch:1 step:992 [D loss: 0.535228, acc.: 76.56%] [G loss: 0.292464]\n",
      "epoch:1 step:993 [D loss: 0.560928, acc.: 73.44%] [G loss: 0.240584]\n",
      "epoch:1 step:994 [D loss: 0.538719, acc.: 78.91%] [G loss: 0.213261]\n",
      "epoch:1 step:995 [D loss: 0.587721, acc.: 67.19%] [G loss: 0.152386]\n",
      "epoch:1 step:996 [D loss: 0.510233, acc.: 80.47%] [G loss: 0.251490]\n",
      "epoch:1 step:997 [D loss: 0.481019, acc.: 85.94%] [G loss: 0.320344]\n",
      "epoch:1 step:998 [D loss: 0.527795, acc.: 74.22%] [G loss: 0.290979]\n",
      "epoch:1 step:999 [D loss: 0.585209, acc.: 65.62%] [G loss: 0.184449]\n",
      "epoch:1 step:1000 [D loss: 0.532084, acc.: 78.12%] [G loss: 0.199118]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 3.204543\n",
      "FID: 157.395477\n",
      "0 = 14.953765780258122\n",
      "1 = 0.24853347747775362\n",
      "2 = 0.9972000122070312\n",
      "3 = 0.9944000244140625\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9944000244140625\n",
      "7 = 12.91490811595913\n",
      "8 = 0.21628406807560402\n",
      "9 = 0.9887999892234802\n",
      "10 = 0.9778000116348267\n",
      "11 = 0.9998000264167786\n",
      "12 = 0.9997954964637756\n",
      "13 = 0.9778000116348267\n",
      "14 = 3.2045438289642334\n",
      "15 = 6.963834285736084\n",
      "16 = 0.51902174949646\n",
      "17 = 3.204543113708496\n",
      "18 = 157.39547729492188\n",
      "epoch:1 step:1001 [D loss: 0.547228, acc.: 72.66%] [G loss: 0.200315]\n",
      "epoch:1 step:1002 [D loss: 0.529222, acc.: 74.22%] [G loss: 0.244914]\n",
      "epoch:1 step:1003 [D loss: 0.511061, acc.: 82.81%] [G loss: 0.236819]\n",
      "epoch:1 step:1004 [D loss: 0.519729, acc.: 86.72%] [G loss: 0.229402]\n",
      "epoch:1 step:1005 [D loss: 0.533347, acc.: 76.56%] [G loss: 0.219121]\n",
      "epoch:1 step:1006 [D loss: 0.535191, acc.: 82.81%] [G loss: 0.206931]\n",
      "epoch:1 step:1007 [D loss: 0.495956, acc.: 82.03%] [G loss: 0.245652]\n",
      "epoch:1 step:1008 [D loss: 0.548866, acc.: 71.09%] [G loss: 0.218150]\n",
      "epoch:1 step:1009 [D loss: 0.472002, acc.: 86.72%] [G loss: 0.311011]\n",
      "epoch:1 step:1010 [D loss: 0.469981, acc.: 85.94%] [G loss: 0.323461]\n",
      "epoch:1 step:1011 [D loss: 0.550085, acc.: 78.91%] [G loss: 0.234000]\n",
      "epoch:1 step:1012 [D loss: 0.531904, acc.: 80.47%] [G loss: 0.198158]\n",
      "epoch:1 step:1013 [D loss: 0.537683, acc.: 78.91%] [G loss: 0.224312]\n",
      "epoch:1 step:1014 [D loss: 0.484720, acc.: 79.69%] [G loss: 0.297764]\n",
      "epoch:1 step:1015 [D loss: 0.596163, acc.: 69.53%] [G loss: 0.205911]\n",
      "epoch:1 step:1016 [D loss: 0.509759, acc.: 82.03%] [G loss: 0.209782]\n",
      "epoch:1 step:1017 [D loss: 0.563616, acc.: 71.09%] [G loss: 0.189371]\n",
      "epoch:1 step:1018 [D loss: 0.508116, acc.: 82.03%] [G loss: 0.227847]\n",
      "epoch:1 step:1019 [D loss: 0.470749, acc.: 88.28%] [G loss: 0.258624]\n",
      "epoch:1 step:1020 [D loss: 0.532907, acc.: 81.25%] [G loss: 0.236515]\n",
      "epoch:1 step:1021 [D loss: 0.506907, acc.: 85.94%] [G loss: 0.215402]\n",
      "epoch:1 step:1022 [D loss: 0.564717, acc.: 75.00%] [G loss: 0.223116]\n",
      "epoch:1 step:1023 [D loss: 0.490182, acc.: 85.16%] [G loss: 0.245216]\n",
      "epoch:1 step:1024 [D loss: 0.472291, acc.: 91.41%] [G loss: 0.297765]\n",
      "epoch:1 step:1025 [D loss: 0.468492, acc.: 88.28%] [G loss: 0.293490]\n",
      "epoch:1 step:1026 [D loss: 0.521723, acc.: 83.59%] [G loss: 0.224482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1027 [D loss: 0.506239, acc.: 82.81%] [G loss: 0.278108]\n",
      "epoch:1 step:1028 [D loss: 0.509131, acc.: 82.03%] [G loss: 0.273149]\n",
      "epoch:1 step:1029 [D loss: 0.511128, acc.: 84.38%] [G loss: 0.244406]\n",
      "epoch:1 step:1030 [D loss: 0.488529, acc.: 82.03%] [G loss: 0.278776]\n",
      "epoch:1 step:1031 [D loss: 0.530634, acc.: 81.25%] [G loss: 0.289278]\n",
      "epoch:1 step:1032 [D loss: 0.533888, acc.: 80.47%] [G loss: 0.299961]\n",
      "epoch:1 step:1033 [D loss: 0.491363, acc.: 85.94%] [G loss: 0.325678]\n",
      "epoch:1 step:1034 [D loss: 0.553266, acc.: 75.78%] [G loss: 0.244266]\n",
      "epoch:1 step:1035 [D loss: 0.544386, acc.: 71.88%] [G loss: 0.255367]\n",
      "epoch:1 step:1036 [D loss: 0.493475, acc.: 85.94%] [G loss: 0.271350]\n",
      "epoch:1 step:1037 [D loss: 0.514547, acc.: 78.91%] [G loss: 0.301993]\n",
      "epoch:1 step:1038 [D loss: 0.544493, acc.: 81.25%] [G loss: 0.238016]\n",
      "epoch:1 step:1039 [D loss: 0.602275, acc.: 73.44%] [G loss: 0.169711]\n",
      "epoch:1 step:1040 [D loss: 0.490225, acc.: 86.72%] [G loss: 0.256577]\n",
      "epoch:1 step:1041 [D loss: 0.498854, acc.: 84.38%] [G loss: 0.296189]\n",
      "epoch:1 step:1042 [D loss: 0.610465, acc.: 68.75%] [G loss: 0.166650]\n",
      "epoch:1 step:1043 [D loss: 0.549043, acc.: 80.47%] [G loss: 0.233842]\n",
      "epoch:1 step:1044 [D loss: 0.559838, acc.: 78.91%] [G loss: 0.192685]\n",
      "epoch:1 step:1045 [D loss: 0.548616, acc.: 75.78%] [G loss: 0.239131]\n",
      "epoch:1 step:1046 [D loss: 0.527570, acc.: 85.16%] [G loss: 0.269552]\n",
      "epoch:1 step:1047 [D loss: 0.510709, acc.: 86.72%] [G loss: 0.281012]\n",
      "epoch:1 step:1048 [D loss: 0.559766, acc.: 76.56%] [G loss: 0.229685]\n",
      "epoch:1 step:1049 [D loss: 0.558828, acc.: 78.91%] [G loss: 0.183241]\n",
      "epoch:1 step:1050 [D loss: 0.524698, acc.: 88.28%] [G loss: 0.220091]\n",
      "epoch:1 step:1051 [D loss: 0.557665, acc.: 80.47%] [G loss: 0.174025]\n",
      "epoch:1 step:1052 [D loss: 0.523425, acc.: 80.47%] [G loss: 0.226537]\n",
      "epoch:1 step:1053 [D loss: 0.538609, acc.: 74.22%] [G loss: 0.292909]\n",
      "epoch:1 step:1054 [D loss: 0.539431, acc.: 80.47%] [G loss: 0.289459]\n",
      "epoch:1 step:1055 [D loss: 0.500629, acc.: 89.06%] [G loss: 0.330578]\n",
      "epoch:1 step:1056 [D loss: 0.522771, acc.: 79.69%] [G loss: 0.311700]\n",
      "epoch:1 step:1057 [D loss: 0.609476, acc.: 69.53%] [G loss: 0.201605]\n",
      "epoch:1 step:1058 [D loss: 0.538440, acc.: 78.12%] [G loss: 0.199993]\n",
      "epoch:1 step:1059 [D loss: 0.527964, acc.: 82.03%] [G loss: 0.220422]\n",
      "epoch:1 step:1060 [D loss: 0.551109, acc.: 80.47%] [G loss: 0.217553]\n",
      "epoch:1 step:1061 [D loss: 0.549193, acc.: 82.03%] [G loss: 0.196673]\n",
      "epoch:1 step:1062 [D loss: 0.500212, acc.: 82.81%] [G loss: 0.254178]\n",
      "epoch:1 step:1063 [D loss: 0.555205, acc.: 76.56%] [G loss: 0.195259]\n",
      "epoch:1 step:1064 [D loss: 0.522220, acc.: 84.38%] [G loss: 0.216823]\n",
      "epoch:1 step:1065 [D loss: 0.529733, acc.: 84.38%] [G loss: 0.201380]\n",
      "epoch:1 step:1066 [D loss: 0.515034, acc.: 85.94%] [G loss: 0.181293]\n",
      "epoch:1 step:1067 [D loss: 0.503860, acc.: 80.47%] [G loss: 0.225806]\n",
      "epoch:1 step:1068 [D loss: 0.452263, acc.: 89.06%] [G loss: 0.302457]\n",
      "epoch:1 step:1069 [D loss: 0.511188, acc.: 89.06%] [G loss: 0.274064]\n",
      "epoch:1 step:1070 [D loss: 0.554347, acc.: 80.47%] [G loss: 0.191812]\n",
      "epoch:1 step:1071 [D loss: 0.481623, acc.: 85.16%] [G loss: 0.237325]\n",
      "epoch:1 step:1072 [D loss: 0.491112, acc.: 85.16%] [G loss: 0.270524]\n",
      "epoch:1 step:1073 [D loss: 0.473225, acc.: 90.62%] [G loss: 0.300041]\n",
      "epoch:1 step:1074 [D loss: 0.516350, acc.: 82.81%] [G loss: 0.216167]\n",
      "epoch:1 step:1075 [D loss: 0.481152, acc.: 88.28%] [G loss: 0.209945]\n",
      "epoch:1 step:1076 [D loss: 0.542290, acc.: 78.91%] [G loss: 0.185551]\n",
      "epoch:1 step:1077 [D loss: 0.501514, acc.: 78.91%] [G loss: 0.217481]\n",
      "epoch:1 step:1078 [D loss: 0.482760, acc.: 86.72%] [G loss: 0.242581]\n",
      "epoch:1 step:1079 [D loss: 0.473035, acc.: 89.06%] [G loss: 0.263599]\n",
      "epoch:1 step:1080 [D loss: 0.526522, acc.: 75.78%] [G loss: 0.226875]\n",
      "epoch:1 step:1081 [D loss: 0.494376, acc.: 82.03%] [G loss: 0.245551]\n",
      "epoch:1 step:1082 [D loss: 0.484300, acc.: 86.72%] [G loss: 0.278450]\n",
      "epoch:1 step:1083 [D loss: 0.463731, acc.: 85.94%] [G loss: 0.316709]\n",
      "epoch:1 step:1084 [D loss: 0.521413, acc.: 81.25%] [G loss: 0.254101]\n",
      "epoch:1 step:1085 [D loss: 0.463440, acc.: 92.97%] [G loss: 0.268402]\n",
      "epoch:1 step:1086 [D loss: 0.469071, acc.: 86.72%] [G loss: 0.323861]\n",
      "epoch:1 step:1087 [D loss: 0.552957, acc.: 78.12%] [G loss: 0.219002]\n",
      "epoch:1 step:1088 [D loss: 0.433507, acc.: 90.62%] [G loss: 0.317282]\n",
      "epoch:1 step:1089 [D loss: 0.400009, acc.: 92.19%] [G loss: 0.462809]\n",
      "epoch:1 step:1090 [D loss: 0.518803, acc.: 78.91%] [G loss: 0.357811]\n",
      "epoch:1 step:1091 [D loss: 0.563716, acc.: 73.44%] [G loss: 0.206016]\n",
      "epoch:1 step:1092 [D loss: 0.498861, acc.: 79.69%] [G loss: 0.239065]\n",
      "epoch:1 step:1093 [D loss: 0.485889, acc.: 89.84%] [G loss: 0.256465]\n",
      "epoch:1 step:1094 [D loss: 0.462762, acc.: 89.06%] [G loss: 0.306057]\n",
      "epoch:1 step:1095 [D loss: 0.519948, acc.: 77.34%] [G loss: 0.257807]\n",
      "epoch:1 step:1096 [D loss: 0.539482, acc.: 77.34%] [G loss: 0.234571]\n",
      "epoch:1 step:1097 [D loss: 0.554181, acc.: 74.22%] [G loss: 0.202431]\n",
      "epoch:1 step:1098 [D loss: 0.535029, acc.: 78.91%] [G loss: 0.215246]\n",
      "epoch:1 step:1099 [D loss: 0.490048, acc.: 79.69%] [G loss: 0.257514]\n",
      "epoch:1 step:1100 [D loss: 0.527561, acc.: 78.12%] [G loss: 0.237799]\n",
      "epoch:1 step:1101 [D loss: 0.561480, acc.: 69.53%] [G loss: 0.214346]\n",
      "epoch:1 step:1102 [D loss: 0.500298, acc.: 79.69%] [G loss: 0.306093]\n",
      "epoch:1 step:1103 [D loss: 0.534040, acc.: 78.12%] [G loss: 0.293431]\n",
      "epoch:1 step:1104 [D loss: 0.606465, acc.: 66.41%] [G loss: 0.184560]\n",
      "epoch:1 step:1105 [D loss: 0.531270, acc.: 77.34%] [G loss: 0.227823]\n",
      "epoch:1 step:1106 [D loss: 0.559761, acc.: 71.09%] [G loss: 0.293691]\n",
      "epoch:1 step:1107 [D loss: 0.531638, acc.: 78.12%] [G loss: 0.330783]\n",
      "epoch:1 step:1108 [D loss: 0.560299, acc.: 77.34%] [G loss: 0.226747]\n",
      "epoch:1 step:1109 [D loss: 0.534805, acc.: 81.25%] [G loss: 0.251351]\n",
      "epoch:1 step:1110 [D loss: 0.503380, acc.: 81.25%] [G loss: 0.285401]\n",
      "epoch:1 step:1111 [D loss: 0.570589, acc.: 72.66%] [G loss: 0.238189]\n",
      "epoch:1 step:1112 [D loss: 0.549580, acc.: 75.78%] [G loss: 0.275550]\n",
      "epoch:1 step:1113 [D loss: 0.548064, acc.: 82.03%] [G loss: 0.245676]\n",
      "epoch:1 step:1114 [D loss: 0.529212, acc.: 80.47%] [G loss: 0.264001]\n",
      "epoch:1 step:1115 [D loss: 0.489391, acc.: 87.50%] [G loss: 0.276592]\n",
      "epoch:1 step:1116 [D loss: 0.548326, acc.: 75.00%] [G loss: 0.236136]\n",
      "epoch:1 step:1117 [D loss: 0.533943, acc.: 82.81%] [G loss: 0.258187]\n",
      "epoch:1 step:1118 [D loss: 0.510064, acc.: 82.81%] [G loss: 0.255371]\n",
      "epoch:1 step:1119 [D loss: 0.518420, acc.: 82.03%] [G loss: 0.310058]\n",
      "epoch:1 step:1120 [D loss: 0.501200, acc.: 88.28%] [G loss: 0.315904]\n",
      "epoch:1 step:1121 [D loss: 0.509607, acc.: 85.16%] [G loss: 0.296808]\n",
      "epoch:1 step:1122 [D loss: 0.539074, acc.: 77.34%] [G loss: 0.274224]\n",
      "epoch:1 step:1123 [D loss: 0.507976, acc.: 82.81%] [G loss: 0.249536]\n",
      "epoch:1 step:1124 [D loss: 0.527082, acc.: 80.47%] [G loss: 0.225581]\n",
      "epoch:1 step:1125 [D loss: 0.502155, acc.: 83.59%] [G loss: 0.259598]\n",
      "epoch:1 step:1126 [D loss: 0.534875, acc.: 81.25%] [G loss: 0.229473]\n",
      "epoch:1 step:1127 [D loss: 0.456449, acc.: 89.06%] [G loss: 0.296892]\n",
      "epoch:1 step:1128 [D loss: 0.493904, acc.: 85.16%] [G loss: 0.309887]\n",
      "epoch:1 step:1129 [D loss: 0.512384, acc.: 79.69%] [G loss: 0.287217]\n",
      "epoch:1 step:1130 [D loss: 0.543298, acc.: 78.12%] [G loss: 0.203754]\n",
      "epoch:1 step:1131 [D loss: 0.499476, acc.: 85.16%] [G loss: 0.207626]\n",
      "epoch:1 step:1132 [D loss: 0.492958, acc.: 86.72%] [G loss: 0.296469]\n",
      "epoch:1 step:1133 [D loss: 0.514593, acc.: 84.38%] [G loss: 0.271583]\n",
      "epoch:1 step:1134 [D loss: 0.544800, acc.: 78.12%] [G loss: 0.198432]\n",
      "epoch:1 step:1135 [D loss: 0.492413, acc.: 85.94%] [G loss: 0.264261]\n",
      "epoch:1 step:1136 [D loss: 0.530929, acc.: 78.91%] [G loss: 0.279170]\n",
      "epoch:1 step:1137 [D loss: 0.526710, acc.: 79.69%] [G loss: 0.258328]\n",
      "epoch:1 step:1138 [D loss: 0.512698, acc.: 82.81%] [G loss: 0.231826]\n",
      "epoch:1 step:1139 [D loss: 0.503926, acc.: 83.59%] [G loss: 0.248015]\n",
      "epoch:1 step:1140 [D loss: 0.529984, acc.: 79.69%] [G loss: 0.212419]\n",
      "epoch:1 step:1141 [D loss: 0.462633, acc.: 89.84%] [G loss: 0.295564]\n",
      "epoch:1 step:1142 [D loss: 0.458893, acc.: 90.62%] [G loss: 0.295747]\n",
      "epoch:1 step:1143 [D loss: 0.452659, acc.: 90.62%] [G loss: 0.323372]\n",
      "epoch:1 step:1144 [D loss: 0.507182, acc.: 84.38%] [G loss: 0.301932]\n",
      "epoch:1 step:1145 [D loss: 0.497019, acc.: 82.03%] [G loss: 0.281214]\n",
      "epoch:1 step:1146 [D loss: 0.505988, acc.: 82.03%] [G loss: 0.288092]\n",
      "epoch:1 step:1147 [D loss: 0.496705, acc.: 82.81%] [G loss: 0.243863]\n",
      "epoch:1 step:1148 [D loss: 0.529323, acc.: 79.69%] [G loss: 0.208319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1149 [D loss: 0.510595, acc.: 82.81%] [G loss: 0.244010]\n",
      "epoch:1 step:1150 [D loss: 0.476826, acc.: 87.50%] [G loss: 0.311994]\n",
      "epoch:1 step:1151 [D loss: 0.590550, acc.: 74.22%] [G loss: 0.211492]\n",
      "epoch:1 step:1152 [D loss: 0.542595, acc.: 77.34%] [G loss: 0.208580]\n",
      "epoch:1 step:1153 [D loss: 0.549983, acc.: 80.47%] [G loss: 0.208462]\n",
      "epoch:1 step:1154 [D loss: 0.477358, acc.: 89.06%] [G loss: 0.281296]\n",
      "epoch:1 step:1155 [D loss: 0.447361, acc.: 95.31%] [G loss: 0.417964]\n",
      "epoch:1 step:1156 [D loss: 0.497612, acc.: 89.06%] [G loss: 0.356333]\n",
      "epoch:1 step:1157 [D loss: 0.566235, acc.: 78.91%] [G loss: 0.188341]\n",
      "epoch:1 step:1158 [D loss: 0.479034, acc.: 86.72%] [G loss: 0.294446]\n",
      "epoch:1 step:1159 [D loss: 0.485237, acc.: 85.94%] [G loss: 0.380942]\n",
      "epoch:1 step:1160 [D loss: 0.468183, acc.: 88.28%] [G loss: 0.336571]\n",
      "epoch:1 step:1161 [D loss: 0.503325, acc.: 75.78%] [G loss: 0.328731]\n",
      "epoch:1 step:1162 [D loss: 0.520611, acc.: 82.81%] [G loss: 0.280398]\n",
      "epoch:1 step:1163 [D loss: 0.511835, acc.: 83.59%] [G loss: 0.268905]\n",
      "epoch:1 step:1164 [D loss: 0.527709, acc.: 79.69%] [G loss: 0.278341]\n",
      "epoch:1 step:1165 [D loss: 0.487625, acc.: 85.16%] [G loss: 0.296368]\n",
      "epoch:1 step:1166 [D loss: 0.511192, acc.: 87.50%] [G loss: 0.368666]\n",
      "epoch:1 step:1167 [D loss: 0.492367, acc.: 79.69%] [G loss: 0.441540]\n",
      "epoch:1 step:1168 [D loss: 0.451940, acc.: 86.72%] [G loss: 0.496043]\n",
      "epoch:1 step:1169 [D loss: 0.452735, acc.: 85.16%] [G loss: 0.566636]\n",
      "epoch:1 step:1170 [D loss: 0.590198, acc.: 67.97%] [G loss: 0.401272]\n",
      "epoch:1 step:1171 [D loss: 0.540778, acc.: 78.12%] [G loss: 0.262549]\n",
      "epoch:1 step:1172 [D loss: 0.508710, acc.: 82.03%] [G loss: 0.289054]\n",
      "epoch:1 step:1173 [D loss: 0.505800, acc.: 82.03%] [G loss: 0.356625]\n",
      "epoch:1 step:1174 [D loss: 0.551114, acc.: 77.34%] [G loss: 0.327668]\n",
      "epoch:1 step:1175 [D loss: 0.526479, acc.: 82.03%] [G loss: 0.296203]\n",
      "epoch:1 step:1176 [D loss: 0.541837, acc.: 72.66%] [G loss: 0.316570]\n",
      "epoch:1 step:1177 [D loss: 0.511651, acc.: 80.47%] [G loss: 0.335550]\n",
      "epoch:1 step:1178 [D loss: 0.524912, acc.: 76.56%] [G loss: 0.363120]\n",
      "epoch:1 step:1179 [D loss: 0.561916, acc.: 77.34%] [G loss: 0.285994]\n",
      "epoch:1 step:1180 [D loss: 0.505378, acc.: 80.47%] [G loss: 0.321680]\n",
      "epoch:1 step:1181 [D loss: 0.511578, acc.: 78.91%] [G loss: 0.296279]\n",
      "epoch:1 step:1182 [D loss: 0.514407, acc.: 78.91%] [G loss: 0.408344]\n",
      "epoch:1 step:1183 [D loss: 0.622685, acc.: 66.41%] [G loss: 0.263112]\n",
      "epoch:1 step:1184 [D loss: 0.602422, acc.: 61.72%] [G loss: 0.226433]\n",
      "epoch:1 step:1185 [D loss: 0.532090, acc.: 78.12%] [G loss: 0.315205]\n",
      "epoch:1 step:1186 [D loss: 0.560493, acc.: 73.44%] [G loss: 0.320764]\n",
      "epoch:1 step:1187 [D loss: 0.574273, acc.: 77.34%] [G loss: 0.246241]\n",
      "epoch:1 step:1188 [D loss: 0.545448, acc.: 82.81%] [G loss: 0.259228]\n",
      "epoch:1 step:1189 [D loss: 0.550874, acc.: 71.09%] [G loss: 0.265985]\n",
      "epoch:1 step:1190 [D loss: 0.514995, acc.: 82.03%] [G loss: 0.295882]\n",
      "epoch:1 step:1191 [D loss: 0.552789, acc.: 73.44%] [G loss: 0.261502]\n",
      "epoch:1 step:1192 [D loss: 0.520151, acc.: 79.69%] [G loss: 0.293724]\n",
      "epoch:1 step:1193 [D loss: 0.522424, acc.: 83.59%] [G loss: 0.309575]\n",
      "epoch:1 step:1194 [D loss: 0.482926, acc.: 85.94%] [G loss: 0.417597]\n",
      "epoch:1 step:1195 [D loss: 0.465610, acc.: 89.84%] [G loss: 0.432669]\n",
      "epoch:1 step:1196 [D loss: 0.517962, acc.: 76.56%] [G loss: 0.384480]\n",
      "epoch:1 step:1197 [D loss: 0.515326, acc.: 80.47%] [G loss: 0.388969]\n",
      "epoch:1 step:1198 [D loss: 0.523030, acc.: 76.56%] [G loss: 0.340922]\n",
      "epoch:1 step:1199 [D loss: 0.533791, acc.: 74.22%] [G loss: 0.337884]\n",
      "epoch:1 step:1200 [D loss: 0.641076, acc.: 61.72%] [G loss: 0.231096]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 3.161528\n",
      "FID: 137.886810\n",
      "0 = 14.870068560409576\n",
      "1 = 0.23342448452331557\n",
      "2 = 0.9973999857902527\n",
      "3 = 0.9947999715805054\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9947999715805054\n",
      "7 = 12.412873501443878\n",
      "8 = 0.21154218116904563\n",
      "9 = 0.9829999804496765\n",
      "10 = 0.9667999744415283\n",
      "11 = 0.9991999864578247\n",
      "12 = 0.9991732239723206\n",
      "13 = 0.9667999744415283\n",
      "14 = 3.1615302562713623\n",
      "15 = 7.955793857574463\n",
      "16 = 0.4737933278083801\n",
      "17 = 3.161527633666992\n",
      "18 = 137.88681030273438\n",
      "epoch:1 step:1201 [D loss: 0.603898, acc.: 67.97%] [G loss: 0.217464]\n",
      "epoch:1 step:1202 [D loss: 0.540272, acc.: 76.56%] [G loss: 0.297879]\n",
      "epoch:1 step:1203 [D loss: 0.590847, acc.: 75.78%] [G loss: 0.229126]\n",
      "epoch:1 step:1204 [D loss: 0.547157, acc.: 76.56%] [G loss: 0.236953]\n",
      "epoch:1 step:1205 [D loss: 0.575758, acc.: 71.09%] [G loss: 0.231288]\n",
      "epoch:1 step:1206 [D loss: 0.606711, acc.: 70.31%] [G loss: 0.204027]\n",
      "epoch:1 step:1207 [D loss: 0.532773, acc.: 75.00%] [G loss: 0.239513]\n",
      "epoch:1 step:1208 [D loss: 0.487626, acc.: 84.38%] [G loss: 0.336100]\n",
      "epoch:1 step:1209 [D loss: 0.522700, acc.: 82.81%] [G loss: 0.281847]\n",
      "epoch:1 step:1210 [D loss: 0.491711, acc.: 87.50%] [G loss: 0.270968]\n",
      "epoch:1 step:1211 [D loss: 0.537868, acc.: 75.00%] [G loss: 0.242790]\n",
      "epoch:1 step:1212 [D loss: 0.585260, acc.: 70.31%] [G loss: 0.214204]\n",
      "epoch:1 step:1213 [D loss: 0.553425, acc.: 78.12%] [G loss: 0.253050]\n",
      "epoch:1 step:1214 [D loss: 0.555290, acc.: 71.09%] [G loss: 0.244929]\n",
      "epoch:1 step:1215 [D loss: 0.531770, acc.: 80.47%] [G loss: 0.295860]\n",
      "epoch:1 step:1216 [D loss: 0.466849, acc.: 89.84%] [G loss: 0.379304]\n",
      "epoch:1 step:1217 [D loss: 0.512361, acc.: 78.12%] [G loss: 0.399481]\n",
      "epoch:1 step:1218 [D loss: 0.519138, acc.: 82.03%] [G loss: 0.320753]\n",
      "epoch:1 step:1219 [D loss: 0.545765, acc.: 78.91%] [G loss: 0.282690]\n",
      "epoch:1 step:1220 [D loss: 0.510067, acc.: 86.72%] [G loss: 0.273340]\n",
      "epoch:1 step:1221 [D loss: 0.486405, acc.: 82.81%] [G loss: 0.288756]\n",
      "epoch:1 step:1222 [D loss: 0.522431, acc.: 82.81%] [G loss: 0.252791]\n",
      "epoch:1 step:1223 [D loss: 0.501789, acc.: 84.38%] [G loss: 0.340153]\n",
      "epoch:1 step:1224 [D loss: 0.500655, acc.: 81.25%] [G loss: 0.296518]\n",
      "epoch:1 step:1225 [D loss: 0.550259, acc.: 76.56%] [G loss: 0.230776]\n",
      "epoch:1 step:1226 [D loss: 0.510768, acc.: 81.25%] [G loss: 0.267672]\n",
      "epoch:1 step:1227 [D loss: 0.497345, acc.: 85.94%] [G loss: 0.290004]\n",
      "epoch:1 step:1228 [D loss: 0.549554, acc.: 73.44%] [G loss: 0.287386]\n",
      "epoch:1 step:1229 [D loss: 0.543674, acc.: 77.34%] [G loss: 0.312627]\n",
      "epoch:1 step:1230 [D loss: 0.515244, acc.: 76.56%] [G loss: 0.313782]\n",
      "epoch:1 step:1231 [D loss: 0.490772, acc.: 83.59%] [G loss: 0.327500]\n",
      "epoch:1 step:1232 [D loss: 0.508213, acc.: 88.28%] [G loss: 0.354499]\n",
      "epoch:1 step:1233 [D loss: 0.508284, acc.: 85.16%] [G loss: 0.297021]\n",
      "epoch:1 step:1234 [D loss: 0.525046, acc.: 80.47%] [G loss: 0.246068]\n",
      "epoch:1 step:1235 [D loss: 0.558279, acc.: 77.34%] [G loss: 0.254413]\n",
      "epoch:1 step:1236 [D loss: 0.529594, acc.: 82.81%] [G loss: 0.297769]\n",
      "epoch:1 step:1237 [D loss: 0.517948, acc.: 82.03%] [G loss: 0.255480]\n",
      "epoch:1 step:1238 [D loss: 0.598646, acc.: 73.44%] [G loss: 0.225150]\n",
      "epoch:1 step:1239 [D loss: 0.502491, acc.: 85.94%] [G loss: 0.270535]\n",
      "epoch:1 step:1240 [D loss: 0.511623, acc.: 80.47%] [G loss: 0.305117]\n",
      "epoch:1 step:1241 [D loss: 0.447497, acc.: 87.50%] [G loss: 0.380191]\n",
      "epoch:1 step:1242 [D loss: 0.457533, acc.: 89.06%] [G loss: 0.368936]\n",
      "epoch:1 step:1243 [D loss: 0.521983, acc.: 80.47%] [G loss: 0.294290]\n",
      "epoch:1 step:1244 [D loss: 0.488279, acc.: 85.16%] [G loss: 0.330766]\n",
      "epoch:1 step:1245 [D loss: 0.431723, acc.: 88.28%] [G loss: 0.412008]\n",
      "epoch:1 step:1246 [D loss: 0.480394, acc.: 81.25%] [G loss: 0.460197]\n",
      "epoch:1 step:1247 [D loss: 0.478530, acc.: 83.59%] [G loss: 0.431298]\n",
      "epoch:1 step:1248 [D loss: 0.483513, acc.: 82.03%] [G loss: 0.374036]\n",
      "epoch:1 step:1249 [D loss: 0.437183, acc.: 85.94%] [G loss: 0.536565]\n",
      "epoch:1 step:1250 [D loss: 0.451751, acc.: 79.69%] [G loss: 0.555131]\n",
      "epoch:1 step:1251 [D loss: 0.460342, acc.: 81.25%] [G loss: 0.596618]\n",
      "epoch:1 step:1252 [D loss: 0.421619, acc.: 87.50%] [G loss: 0.568467]\n",
      "epoch:1 step:1253 [D loss: 0.656177, acc.: 65.62%] [G loss: 0.212706]\n",
      "epoch:1 step:1254 [D loss: 0.540473, acc.: 72.66%] [G loss: 0.237872]\n",
      "epoch:1 step:1255 [D loss: 0.536569, acc.: 75.00%] [G loss: 0.282093]\n",
      "epoch:1 step:1256 [D loss: 0.496306, acc.: 85.16%] [G loss: 0.328729]\n",
      "epoch:1 step:1257 [D loss: 0.559542, acc.: 74.22%] [G loss: 0.232758]\n",
      "epoch:1 step:1258 [D loss: 0.531576, acc.: 75.78%] [G loss: 0.281021]\n",
      "epoch:1 step:1259 [D loss: 0.501772, acc.: 81.25%] [G loss: 0.305302]\n",
      "epoch:1 step:1260 [D loss: 0.475338, acc.: 86.72%] [G loss: 0.351926]\n",
      "epoch:1 step:1261 [D loss: 0.481556, acc.: 85.94%] [G loss: 0.360652]\n",
      "epoch:1 step:1262 [D loss: 0.531703, acc.: 78.12%] [G loss: 0.290783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1263 [D loss: 0.480972, acc.: 90.62%] [G loss: 0.337981]\n",
      "epoch:1 step:1264 [D loss: 0.481169, acc.: 85.94%] [G loss: 0.388926]\n",
      "epoch:1 step:1265 [D loss: 0.472390, acc.: 84.38%] [G loss: 0.391693]\n",
      "epoch:1 step:1266 [D loss: 0.478459, acc.: 82.81%] [G loss: 0.424726]\n",
      "epoch:1 step:1267 [D loss: 0.530418, acc.: 78.91%] [G loss: 0.338596]\n",
      "epoch:1 step:1268 [D loss: 0.488006, acc.: 82.03%] [G loss: 0.353794]\n",
      "epoch:1 step:1269 [D loss: 0.509260, acc.: 82.03%] [G loss: 0.360225]\n",
      "epoch:1 step:1270 [D loss: 0.483349, acc.: 85.94%] [G loss: 0.364681]\n",
      "epoch:1 step:1271 [D loss: 0.524757, acc.: 74.22%] [G loss: 0.353218]\n",
      "epoch:1 step:1272 [D loss: 0.466870, acc.: 88.28%] [G loss: 0.399288]\n",
      "epoch:1 step:1273 [D loss: 0.512233, acc.: 79.69%] [G loss: 0.392064]\n",
      "epoch:1 step:1274 [D loss: 0.480434, acc.: 85.16%] [G loss: 0.404993]\n",
      "epoch:1 step:1275 [D loss: 0.574854, acc.: 72.66%] [G loss: 0.282177]\n",
      "epoch:1 step:1276 [D loss: 0.573131, acc.: 69.53%] [G loss: 0.310380]\n",
      "epoch:1 step:1277 [D loss: 0.507805, acc.: 86.72%] [G loss: 0.375719]\n",
      "epoch:1 step:1278 [D loss: 0.571986, acc.: 74.22%] [G loss: 0.319758]\n",
      "epoch:1 step:1279 [D loss: 0.485243, acc.: 85.16%] [G loss: 0.418332]\n",
      "epoch:1 step:1280 [D loss: 0.441950, acc.: 83.59%] [G loss: 0.554560]\n",
      "epoch:1 step:1281 [D loss: 0.477938, acc.: 85.94%] [G loss: 0.505086]\n",
      "epoch:1 step:1282 [D loss: 0.523020, acc.: 76.56%] [G loss: 0.446088]\n",
      "epoch:1 step:1283 [D loss: 0.491330, acc.: 77.34%] [G loss: 0.512598]\n",
      "epoch:1 step:1284 [D loss: 0.492182, acc.: 80.47%] [G loss: 0.711766]\n",
      "epoch:1 step:1285 [D loss: 0.587209, acc.: 71.88%] [G loss: 0.396983]\n",
      "epoch:1 step:1286 [D loss: 0.672410, acc.: 60.16%] [G loss: 0.176707]\n",
      "epoch:1 step:1287 [D loss: 0.508424, acc.: 82.03%] [G loss: 0.280663]\n",
      "epoch:1 step:1288 [D loss: 0.537285, acc.: 75.00%] [G loss: 0.380654]\n",
      "epoch:1 step:1289 [D loss: 0.558586, acc.: 75.00%] [G loss: 0.381138]\n",
      "epoch:1 step:1290 [D loss: 0.546896, acc.: 72.66%] [G loss: 0.413342]\n",
      "epoch:1 step:1291 [D loss: 0.497002, acc.: 80.47%] [G loss: 0.393112]\n",
      "epoch:1 step:1292 [D loss: 0.483454, acc.: 87.50%] [G loss: 0.390547]\n",
      "epoch:1 step:1293 [D loss: 0.537180, acc.: 74.22%] [G loss: 0.301262]\n",
      "epoch:1 step:1294 [D loss: 0.516586, acc.: 81.25%] [G loss: 0.345049]\n",
      "epoch:1 step:1295 [D loss: 0.450853, acc.: 89.06%] [G loss: 0.445294]\n",
      "epoch:1 step:1296 [D loss: 0.513472, acc.: 83.59%] [G loss: 0.320938]\n",
      "epoch:1 step:1297 [D loss: 0.509131, acc.: 80.47%] [G loss: 0.288679]\n",
      "epoch:1 step:1298 [D loss: 0.482063, acc.: 84.38%] [G loss: 0.344902]\n",
      "epoch:1 step:1299 [D loss: 0.525520, acc.: 82.03%] [G loss: 0.294377]\n",
      "epoch:1 step:1300 [D loss: 0.467048, acc.: 91.41%] [G loss: 0.356927]\n",
      "epoch:1 step:1301 [D loss: 0.423443, acc.: 95.31%] [G loss: 0.398421]\n",
      "epoch:1 step:1302 [D loss: 0.468536, acc.: 88.28%] [G loss: 0.342623]\n",
      "epoch:1 step:1303 [D loss: 0.424184, acc.: 92.97%] [G loss: 0.436649]\n",
      "epoch:1 step:1304 [D loss: 0.461044, acc.: 84.38%] [G loss: 0.467865]\n",
      "epoch:1 step:1305 [D loss: 0.477669, acc.: 87.50%] [G loss: 0.376890]\n",
      "epoch:1 step:1306 [D loss: 0.486322, acc.: 85.16%] [G loss: 0.370653]\n",
      "epoch:1 step:1307 [D loss: 0.515554, acc.: 78.12%] [G loss: 0.301252]\n",
      "epoch:1 step:1308 [D loss: 0.477211, acc.: 85.94%] [G loss: 0.325801]\n",
      "epoch:1 step:1309 [D loss: 0.448041, acc.: 88.28%] [G loss: 0.399429]\n",
      "epoch:1 step:1310 [D loss: 0.507099, acc.: 82.81%] [G loss: 0.361919]\n",
      "epoch:1 step:1311 [D loss: 0.413825, acc.: 90.62%] [G loss: 0.468399]\n",
      "epoch:1 step:1312 [D loss: 0.441435, acc.: 87.50%] [G loss: 0.477361]\n",
      "epoch:1 step:1313 [D loss: 0.484049, acc.: 85.94%] [G loss: 0.343388]\n",
      "epoch:1 step:1314 [D loss: 0.470227, acc.: 87.50%] [G loss: 0.311798]\n",
      "epoch:1 step:1315 [D loss: 0.415839, acc.: 90.62%] [G loss: 0.428761]\n",
      "epoch:1 step:1316 [D loss: 0.437919, acc.: 91.41%] [G loss: 0.489582]\n",
      "epoch:1 step:1317 [D loss: 0.524799, acc.: 82.81%] [G loss: 0.303579]\n",
      "epoch:1 step:1318 [D loss: 0.458654, acc.: 89.84%] [G loss: 0.335934]\n",
      "epoch:1 step:1319 [D loss: 0.479228, acc.: 85.16%] [G loss: 0.387112]\n",
      "epoch:1 step:1320 [D loss: 0.473869, acc.: 87.50%] [G loss: 0.370589]\n",
      "epoch:1 step:1321 [D loss: 0.500078, acc.: 79.69%] [G loss: 0.315471]\n",
      "epoch:1 step:1322 [D loss: 0.436108, acc.: 90.62%] [G loss: 0.449348]\n",
      "epoch:1 step:1323 [D loss: 0.474401, acc.: 85.94%] [G loss: 0.362421]\n",
      "epoch:1 step:1324 [D loss: 0.485114, acc.: 85.94%] [G loss: 0.341508]\n",
      "epoch:1 step:1325 [D loss: 0.461474, acc.: 89.84%] [G loss: 0.359355]\n",
      "epoch:1 step:1326 [D loss: 0.520571, acc.: 82.03%] [G loss: 0.378858]\n",
      "epoch:1 step:1327 [D loss: 0.523091, acc.: 77.34%] [G loss: 0.313449]\n",
      "epoch:1 step:1328 [D loss: 0.480241, acc.: 85.94%] [G loss: 0.331867]\n",
      "epoch:1 step:1329 [D loss: 0.413879, acc.: 86.72%] [G loss: 0.453806]\n",
      "epoch:1 step:1330 [D loss: 0.497197, acc.: 80.47%] [G loss: 0.419902]\n",
      "epoch:1 step:1331 [D loss: 0.514769, acc.: 84.38%] [G loss: 0.331617]\n",
      "epoch:1 step:1332 [D loss: 0.470777, acc.: 86.72%] [G loss: 0.371317]\n",
      "epoch:1 step:1333 [D loss: 0.470745, acc.: 84.38%] [G loss: 0.506362]\n",
      "epoch:1 step:1334 [D loss: 0.400930, acc.: 89.06%] [G loss: 0.629803]\n",
      "epoch:1 step:1335 [D loss: 0.407144, acc.: 85.94%] [G loss: 0.670977]\n",
      "epoch:1 step:1336 [D loss: 0.407270, acc.: 88.28%] [G loss: 0.657162]\n",
      "epoch:1 step:1337 [D loss: 0.572089, acc.: 73.44%] [G loss: 0.415337]\n",
      "epoch:1 step:1338 [D loss: 0.489377, acc.: 78.91%] [G loss: 0.440320]\n",
      "epoch:1 step:1339 [D loss: 0.429457, acc.: 87.50%] [G loss: 0.567803]\n",
      "epoch:1 step:1340 [D loss: 0.480841, acc.: 82.81%] [G loss: 0.510840]\n",
      "epoch:1 step:1341 [D loss: 0.532938, acc.: 78.91%] [G loss: 0.398000]\n",
      "epoch:1 step:1342 [D loss: 0.494610, acc.: 82.03%] [G loss: 0.419789]\n",
      "epoch:1 step:1343 [D loss: 0.465392, acc.: 82.81%] [G loss: 0.570113]\n",
      "epoch:1 step:1344 [D loss: 0.435525, acc.: 89.84%] [G loss: 0.533437]\n",
      "epoch:1 step:1345 [D loss: 0.502635, acc.: 79.69%] [G loss: 0.470854]\n",
      "epoch:1 step:1346 [D loss: 0.473646, acc.: 83.59%] [G loss: 0.459999]\n",
      "epoch:1 step:1347 [D loss: 0.508875, acc.: 84.38%] [G loss: 0.417648]\n",
      "epoch:1 step:1348 [D loss: 0.530716, acc.: 79.69%] [G loss: 0.382055]\n",
      "epoch:1 step:1349 [D loss: 0.447895, acc.: 89.06%] [G loss: 0.553853]\n",
      "epoch:1 step:1350 [D loss: 0.483219, acc.: 85.94%] [G loss: 0.456032]\n",
      "epoch:1 step:1351 [D loss: 0.525857, acc.: 84.38%] [G loss: 0.356259]\n",
      "epoch:1 step:1352 [D loss: 0.529971, acc.: 75.00%] [G loss: 0.390730]\n",
      "epoch:1 step:1353 [D loss: 0.455327, acc.: 85.16%] [G loss: 0.530572]\n",
      "epoch:1 step:1354 [D loss: 0.496100, acc.: 80.47%] [G loss: 0.461170]\n",
      "epoch:1 step:1355 [D loss: 0.511554, acc.: 78.91%] [G loss: 0.348534]\n",
      "epoch:1 step:1356 [D loss: 0.489389, acc.: 83.59%] [G loss: 0.374983]\n",
      "epoch:1 step:1357 [D loss: 0.452297, acc.: 82.03%] [G loss: 0.542299]\n",
      "epoch:1 step:1358 [D loss: 0.462733, acc.: 84.38%] [G loss: 0.490929]\n",
      "epoch:1 step:1359 [D loss: 0.482744, acc.: 89.84%] [G loss: 0.382586]\n",
      "epoch:1 step:1360 [D loss: 0.507591, acc.: 78.91%] [G loss: 0.345976]\n",
      "epoch:1 step:1361 [D loss: 0.441176, acc.: 85.16%] [G loss: 0.500173]\n",
      "epoch:1 step:1362 [D loss: 0.422129, acc.: 85.16%] [G loss: 0.662202]\n",
      "epoch:1 step:1363 [D loss: 0.436012, acc.: 88.28%] [G loss: 0.571740]\n",
      "epoch:1 step:1364 [D loss: 0.455013, acc.: 85.16%] [G loss: 0.519824]\n",
      "epoch:1 step:1365 [D loss: 0.478295, acc.: 81.25%] [G loss: 0.560304]\n",
      "epoch:1 step:1366 [D loss: 0.465247, acc.: 83.59%] [G loss: 0.555675]\n",
      "epoch:1 step:1367 [D loss: 0.474979, acc.: 84.38%] [G loss: 0.578325]\n",
      "epoch:1 step:1368 [D loss: 0.536630, acc.: 81.25%] [G loss: 0.414773]\n",
      "epoch:1 step:1369 [D loss: 0.542257, acc.: 77.34%] [G loss: 0.346277]\n",
      "epoch:1 step:1370 [D loss: 0.533875, acc.: 73.44%] [G loss: 0.380080]\n",
      "epoch:1 step:1371 [D loss: 0.503436, acc.: 78.91%] [G loss: 0.457176]\n",
      "epoch:1 step:1372 [D loss: 0.503823, acc.: 82.03%] [G loss: 0.384302]\n",
      "epoch:1 step:1373 [D loss: 0.539624, acc.: 78.12%] [G loss: 0.350474]\n",
      "epoch:1 step:1374 [D loss: 0.518723, acc.: 76.56%] [G loss: 0.353368]\n",
      "epoch:1 step:1375 [D loss: 0.487985, acc.: 82.03%] [G loss: 0.388366]\n",
      "epoch:1 step:1376 [D loss: 0.513095, acc.: 75.00%] [G loss: 0.422767]\n",
      "epoch:1 step:1377 [D loss: 0.513405, acc.: 78.91%] [G loss: 0.441734]\n",
      "epoch:1 step:1378 [D loss: 0.467428, acc.: 87.50%] [G loss: 0.439681]\n",
      "epoch:1 step:1379 [D loss: 0.499383, acc.: 77.34%] [G loss: 0.455378]\n",
      "epoch:1 step:1380 [D loss: 0.493302, acc.: 82.03%] [G loss: 0.408901]\n",
      "epoch:1 step:1381 [D loss: 0.553560, acc.: 72.66%] [G loss: 0.370652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1382 [D loss: 0.492346, acc.: 82.81%] [G loss: 0.449356]\n",
      "epoch:1 step:1383 [D loss: 0.480465, acc.: 79.69%] [G loss: 0.483861]\n",
      "epoch:1 step:1384 [D loss: 0.452906, acc.: 86.72%] [G loss: 0.460949]\n",
      "epoch:1 step:1385 [D loss: 0.596289, acc.: 68.75%] [G loss: 0.328361]\n",
      "epoch:1 step:1386 [D loss: 0.524453, acc.: 81.25%] [G loss: 0.379490]\n",
      "epoch:1 step:1387 [D loss: 0.479110, acc.: 84.38%] [G loss: 0.469905]\n",
      "epoch:1 step:1388 [D loss: 0.490410, acc.: 78.12%] [G loss: 0.419368]\n",
      "epoch:1 step:1389 [D loss: 0.499835, acc.: 81.25%] [G loss: 0.465239]\n",
      "epoch:1 step:1390 [D loss: 0.518100, acc.: 81.25%] [G loss: 0.471752]\n",
      "epoch:1 step:1391 [D loss: 0.515202, acc.: 79.69%] [G loss: 0.404391]\n",
      "epoch:1 step:1392 [D loss: 0.545019, acc.: 78.91%] [G loss: 0.357841]\n",
      "epoch:1 step:1393 [D loss: 0.529421, acc.: 80.47%] [G loss: 0.342554]\n",
      "epoch:1 step:1394 [D loss: 0.471484, acc.: 86.72%] [G loss: 0.461246]\n",
      "epoch:1 step:1395 [D loss: 0.512156, acc.: 81.25%] [G loss: 0.440976]\n",
      "epoch:1 step:1396 [D loss: 0.530469, acc.: 74.22%] [G loss: 0.389830]\n",
      "epoch:1 step:1397 [D loss: 0.473853, acc.: 82.81%] [G loss: 0.500669]\n",
      "epoch:1 step:1398 [D loss: 0.463931, acc.: 85.16%] [G loss: 0.491733]\n",
      "epoch:1 step:1399 [D loss: 0.515986, acc.: 82.03%] [G loss: 0.353763]\n",
      "epoch:1 step:1400 [D loss: 0.565193, acc.: 78.12%] [G loss: 0.324176]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 3.367952\n",
      "FID: 141.061142\n",
      "0 = 14.441930594825736\n",
      "1 = 0.20716565645584867\n",
      "2 = 0.9940000176429749\n",
      "3 = 0.9879999756813049\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9879999756813049\n",
      "7 = 12.571954205942166\n",
      "8 = 0.2158158936199205\n",
      "9 = 0.9818999767303467\n",
      "10 = 0.9670000076293945\n",
      "11 = 0.9968000054359436\n",
      "12 = 0.996701717376709\n",
      "13 = 0.9670000076293945\n",
      "14 = 3.3679535388946533\n",
      "15 = 8.412113189697266\n",
      "16 = 0.3960179090499878\n",
      "17 = 3.3679518699645996\n",
      "18 = 141.06114196777344\n",
      "epoch:1 step:1401 [D loss: 0.519611, acc.: 76.56%] [G loss: 0.421313]\n",
      "epoch:1 step:1402 [D loss: 0.546334, acc.: 76.56%] [G loss: 0.348195]\n",
      "epoch:1 step:1403 [D loss: 0.498959, acc.: 78.91%] [G loss: 0.342801]\n",
      "epoch:1 step:1404 [D loss: 0.516955, acc.: 81.25%] [G loss: 0.363620]\n",
      "epoch:1 step:1405 [D loss: 0.520656, acc.: 83.59%] [G loss: 0.393206]\n",
      "epoch:1 step:1406 [D loss: 0.568149, acc.: 70.31%] [G loss: 0.327206]\n",
      "epoch:1 step:1407 [D loss: 0.511296, acc.: 75.78%] [G loss: 0.415626]\n",
      "epoch:1 step:1408 [D loss: 0.484975, acc.: 82.03%] [G loss: 0.451007]\n",
      "epoch:1 step:1409 [D loss: 0.461249, acc.: 85.94%] [G loss: 0.487515]\n",
      "epoch:1 step:1410 [D loss: 0.573253, acc.: 73.44%] [G loss: 0.412707]\n",
      "epoch:1 step:1411 [D loss: 0.511126, acc.: 79.69%] [G loss: 0.472430]\n",
      "epoch:1 step:1412 [D loss: 0.447257, acc.: 84.38%] [G loss: 0.676237]\n",
      "epoch:1 step:1413 [D loss: 0.524800, acc.: 75.00%] [G loss: 0.575137]\n",
      "epoch:1 step:1414 [D loss: 0.639880, acc.: 58.59%] [G loss: 0.285266]\n",
      "epoch:1 step:1415 [D loss: 0.537542, acc.: 74.22%] [G loss: 0.326494]\n",
      "epoch:1 step:1416 [D loss: 0.512488, acc.: 78.12%] [G loss: 0.463444]\n",
      "epoch:1 step:1417 [D loss: 0.502693, acc.: 80.47%] [G loss: 0.459602]\n",
      "epoch:1 step:1418 [D loss: 0.509213, acc.: 82.03%] [G loss: 0.464553]\n",
      "epoch:1 step:1419 [D loss: 0.545651, acc.: 76.56%] [G loss: 0.375374]\n",
      "epoch:1 step:1420 [D loss: 0.527281, acc.: 78.91%] [G loss: 0.365457]\n",
      "epoch:1 step:1421 [D loss: 0.523598, acc.: 77.34%] [G loss: 0.380353]\n",
      "epoch:1 step:1422 [D loss: 0.525428, acc.: 80.47%] [G loss: 0.500212]\n",
      "epoch:1 step:1423 [D loss: 0.556679, acc.: 78.12%] [G loss: 0.349178]\n",
      "epoch:1 step:1424 [D loss: 0.494075, acc.: 82.81%] [G loss: 0.390784]\n",
      "epoch:1 step:1425 [D loss: 0.467478, acc.: 85.16%] [G loss: 0.577464]\n",
      "epoch:1 step:1426 [D loss: 0.528754, acc.: 82.03%] [G loss: 0.379262]\n",
      "epoch:1 step:1427 [D loss: 0.584157, acc.: 73.44%] [G loss: 0.251846]\n",
      "epoch:1 step:1428 [D loss: 0.464562, acc.: 85.94%] [G loss: 0.366913]\n",
      "epoch:1 step:1429 [D loss: 0.508780, acc.: 85.16%] [G loss: 0.347420]\n",
      "epoch:1 step:1430 [D loss: 0.525183, acc.: 76.56%] [G loss: 0.343526]\n",
      "epoch:1 step:1431 [D loss: 0.500415, acc.: 85.16%] [G loss: 0.347977]\n",
      "epoch:1 step:1432 [D loss: 0.555290, acc.: 76.56%] [G loss: 0.372805]\n",
      "epoch:1 step:1433 [D loss: 0.531193, acc.: 82.81%] [G loss: 0.321015]\n",
      "epoch:1 step:1434 [D loss: 0.469807, acc.: 82.03%] [G loss: 0.425050]\n",
      "epoch:1 step:1435 [D loss: 0.447157, acc.: 85.94%] [G loss: 0.598153]\n",
      "epoch:1 step:1436 [D loss: 0.446733, acc.: 87.50%] [G loss: 0.636965]\n",
      "epoch:1 step:1437 [D loss: 0.529842, acc.: 75.00%] [G loss: 0.467588]\n",
      "epoch:1 step:1438 [D loss: 0.553566, acc.: 75.00%] [G loss: 0.293866]\n",
      "epoch:1 step:1439 [D loss: 0.500057, acc.: 86.72%] [G loss: 0.325479]\n",
      "epoch:1 step:1440 [D loss: 0.452342, acc.: 89.06%] [G loss: 0.503353]\n",
      "epoch:1 step:1441 [D loss: 0.477772, acc.: 86.72%] [G loss: 0.466172]\n",
      "epoch:1 step:1442 [D loss: 0.566780, acc.: 69.53%] [G loss: 0.342915]\n",
      "epoch:1 step:1443 [D loss: 0.445628, acc.: 91.41%] [G loss: 0.416780]\n",
      "epoch:1 step:1444 [D loss: 0.433432, acc.: 87.50%] [G loss: 0.556568]\n",
      "epoch:1 step:1445 [D loss: 0.462140, acc.: 82.81%] [G loss: 0.601545]\n",
      "epoch:1 step:1446 [D loss: 0.518186, acc.: 77.34%] [G loss: 0.444939]\n",
      "epoch:1 step:1447 [D loss: 0.477870, acc.: 87.50%] [G loss: 0.342749]\n",
      "epoch:1 step:1448 [D loss: 0.497491, acc.: 81.25%] [G loss: 0.398683]\n",
      "epoch:1 step:1449 [D loss: 0.469775, acc.: 88.28%] [G loss: 0.394490]\n",
      "epoch:1 step:1450 [D loss: 0.481808, acc.: 77.34%] [G loss: 0.525226]\n",
      "epoch:1 step:1451 [D loss: 0.445945, acc.: 82.81%] [G loss: 0.581270]\n",
      "epoch:1 step:1452 [D loss: 0.505604, acc.: 73.44%] [G loss: 0.532402]\n",
      "epoch:1 step:1453 [D loss: 0.554744, acc.: 76.56%] [G loss: 0.447562]\n",
      "epoch:1 step:1454 [D loss: 0.597659, acc.: 67.19%] [G loss: 0.335158]\n",
      "epoch:1 step:1455 [D loss: 0.527173, acc.: 76.56%] [G loss: 0.307834]\n",
      "epoch:1 step:1456 [D loss: 0.510261, acc.: 82.81%] [G loss: 0.351579]\n",
      "epoch:1 step:1457 [D loss: 0.476664, acc.: 82.03%] [G loss: 0.484918]\n",
      "epoch:1 step:1458 [D loss: 0.518790, acc.: 78.91%] [G loss: 0.435263]\n",
      "epoch:1 step:1459 [D loss: 0.523931, acc.: 81.25%] [G loss: 0.425280]\n",
      "epoch:1 step:1460 [D loss: 0.557382, acc.: 71.09%] [G loss: 0.361184]\n",
      "epoch:1 step:1461 [D loss: 0.510130, acc.: 79.69%] [G loss: 0.453514]\n",
      "epoch:1 step:1462 [D loss: 0.546590, acc.: 75.78%] [G loss: 0.433511]\n",
      "epoch:1 step:1463 [D loss: 0.511907, acc.: 75.78%] [G loss: 0.457305]\n",
      "epoch:1 step:1464 [D loss: 0.507851, acc.: 80.47%] [G loss: 0.376527]\n",
      "epoch:1 step:1465 [D loss: 0.533891, acc.: 78.91%] [G loss: 0.433351]\n",
      "epoch:1 step:1466 [D loss: 0.491511, acc.: 81.25%] [G loss: 0.574412]\n",
      "epoch:1 step:1467 [D loss: 0.435582, acc.: 86.72%] [G loss: 0.625296]\n",
      "epoch:1 step:1468 [D loss: 0.568667, acc.: 70.31%] [G loss: 0.379679]\n",
      "epoch:1 step:1469 [D loss: 0.577882, acc.: 71.09%] [G loss: 0.341116]\n",
      "epoch:1 step:1470 [D loss: 0.551171, acc.: 74.22%] [G loss: 0.439871]\n",
      "epoch:1 step:1471 [D loss: 0.519260, acc.: 75.00%] [G loss: 0.513939]\n",
      "epoch:1 step:1472 [D loss: 0.491209, acc.: 85.94%] [G loss: 0.376610]\n",
      "epoch:1 step:1473 [D loss: 0.490569, acc.: 85.94%] [G loss: 0.373774]\n",
      "epoch:1 step:1474 [D loss: 0.457018, acc.: 85.94%] [G loss: 0.458317]\n",
      "epoch:1 step:1475 [D loss: 0.464300, acc.: 86.72%] [G loss: 0.410921]\n",
      "epoch:1 step:1476 [D loss: 0.518276, acc.: 82.03%] [G loss: 0.451302]\n",
      "epoch:1 step:1477 [D loss: 0.467411, acc.: 79.69%] [G loss: 0.429628]\n",
      "epoch:1 step:1478 [D loss: 0.458984, acc.: 87.50%] [G loss: 0.435042]\n",
      "epoch:1 step:1479 [D loss: 0.533103, acc.: 78.12%] [G loss: 0.409511]\n",
      "epoch:1 step:1480 [D loss: 0.515588, acc.: 77.34%] [G loss: 0.384530]\n",
      "epoch:1 step:1481 [D loss: 0.496860, acc.: 81.25%] [G loss: 0.360411]\n",
      "epoch:1 step:1482 [D loss: 0.493510, acc.: 81.25%] [G loss: 0.449524]\n",
      "epoch:1 step:1483 [D loss: 0.457554, acc.: 85.16%] [G loss: 0.518993]\n",
      "epoch:1 step:1484 [D loss: 0.454458, acc.: 85.16%] [G loss: 0.455194]\n",
      "epoch:1 step:1485 [D loss: 0.485971, acc.: 83.59%] [G loss: 0.413199]\n",
      "epoch:1 step:1486 [D loss: 0.503809, acc.: 77.34%] [G loss: 0.438451]\n",
      "epoch:1 step:1487 [D loss: 0.511509, acc.: 78.91%] [G loss: 0.401860]\n",
      "epoch:1 step:1488 [D loss: 0.510082, acc.: 80.47%] [G loss: 0.435744]\n",
      "epoch:1 step:1489 [D loss: 0.457416, acc.: 91.41%] [G loss: 0.401054]\n",
      "epoch:1 step:1490 [D loss: 0.468711, acc.: 89.06%] [G loss: 0.370088]\n",
      "epoch:1 step:1491 [D loss: 0.433444, acc.: 88.28%] [G loss: 0.426244]\n",
      "epoch:1 step:1492 [D loss: 0.463805, acc.: 82.03%] [G loss: 0.435816]\n",
      "epoch:1 step:1493 [D loss: 0.463325, acc.: 86.72%] [G loss: 0.427188]\n",
      "epoch:1 step:1494 [D loss: 0.466603, acc.: 86.72%] [G loss: 0.488222]\n",
      "epoch:1 step:1495 [D loss: 0.430008, acc.: 89.84%] [G loss: 0.472665]\n",
      "epoch:1 step:1496 [D loss: 0.553323, acc.: 75.00%] [G loss: 0.324851]\n",
      "epoch:1 step:1497 [D loss: 0.507320, acc.: 80.47%] [G loss: 0.375213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1498 [D loss: 0.464125, acc.: 87.50%] [G loss: 0.433188]\n",
      "epoch:1 step:1499 [D loss: 0.530721, acc.: 75.00%] [G loss: 0.472739]\n",
      "epoch:1 step:1500 [D loss: 0.523045, acc.: 78.12%] [G loss: 0.371761]\n",
      "epoch:1 step:1501 [D loss: 0.503237, acc.: 79.69%] [G loss: 0.405119]\n",
      "epoch:1 step:1502 [D loss: 0.515802, acc.: 80.47%] [G loss: 0.395498]\n",
      "epoch:1 step:1503 [D loss: 0.537017, acc.: 75.00%] [G loss: 0.489695]\n",
      "epoch:1 step:1504 [D loss: 0.434101, acc.: 86.72%] [G loss: 0.535355]\n",
      "epoch:1 step:1505 [D loss: 0.480637, acc.: 84.38%] [G loss: 0.526336]\n",
      "epoch:1 step:1506 [D loss: 0.552610, acc.: 78.12%] [G loss: 0.330568]\n",
      "epoch:1 step:1507 [D loss: 0.461230, acc.: 89.06%] [G loss: 0.355613]\n",
      "epoch:1 step:1508 [D loss: 0.492321, acc.: 82.81%] [G loss: 0.368736]\n",
      "epoch:1 step:1509 [D loss: 0.490122, acc.: 84.38%] [G loss: 0.359060]\n",
      "epoch:1 step:1510 [D loss: 0.456026, acc.: 82.81%] [G loss: 0.487066]\n",
      "epoch:1 step:1511 [D loss: 0.468581, acc.: 78.91%] [G loss: 0.550871]\n",
      "epoch:1 step:1512 [D loss: 0.418526, acc.: 89.06%] [G loss: 0.633341]\n",
      "epoch:1 step:1513 [D loss: 0.522580, acc.: 77.34%] [G loss: 0.499964]\n",
      "epoch:1 step:1514 [D loss: 0.527612, acc.: 76.56%] [G loss: 0.423094]\n",
      "epoch:1 step:1515 [D loss: 0.471235, acc.: 83.59%] [G loss: 0.475808]\n",
      "epoch:1 step:1516 [D loss: 0.469401, acc.: 84.38%] [G loss: 0.565504]\n",
      "epoch:1 step:1517 [D loss: 0.502715, acc.: 82.03%] [G loss: 0.488821]\n",
      "epoch:1 step:1518 [D loss: 0.446493, acc.: 85.16%] [G loss: 0.551061]\n",
      "epoch:1 step:1519 [D loss: 0.399056, acc.: 89.06%] [G loss: 0.704327]\n",
      "epoch:1 step:1520 [D loss: 0.523885, acc.: 76.56%] [G loss: 0.643878]\n",
      "epoch:1 step:1521 [D loss: 0.490497, acc.: 79.69%] [G loss: 0.564654]\n",
      "epoch:1 step:1522 [D loss: 0.488673, acc.: 82.03%] [G loss: 0.571252]\n",
      "epoch:1 step:1523 [D loss: 0.470517, acc.: 86.72%] [G loss: 0.576515]\n",
      "epoch:1 step:1524 [D loss: 0.557120, acc.: 73.44%] [G loss: 0.467816]\n",
      "epoch:1 step:1525 [D loss: 0.496070, acc.: 82.03%] [G loss: 0.490208]\n",
      "epoch:1 step:1526 [D loss: 0.445095, acc.: 83.59%] [G loss: 0.736991]\n",
      "epoch:1 step:1527 [D loss: 0.510137, acc.: 75.78%] [G loss: 0.517669]\n",
      "epoch:1 step:1528 [D loss: 0.547289, acc.: 77.34%] [G loss: 0.446969]\n",
      "epoch:1 step:1529 [D loss: 0.440099, acc.: 83.59%] [G loss: 0.543045]\n",
      "epoch:1 step:1530 [D loss: 0.541545, acc.: 75.78%] [G loss: 0.416819]\n",
      "epoch:1 step:1531 [D loss: 0.469459, acc.: 84.38%] [G loss: 0.536116]\n",
      "epoch:1 step:1532 [D loss: 0.539627, acc.: 71.09%] [G loss: 0.447580]\n",
      "epoch:1 step:1533 [D loss: 0.506650, acc.: 83.59%] [G loss: 0.401282]\n",
      "epoch:1 step:1534 [D loss: 0.508762, acc.: 79.69%] [G loss: 0.356885]\n",
      "epoch:1 step:1535 [D loss: 0.466095, acc.: 82.81%] [G loss: 0.393444]\n",
      "epoch:1 step:1536 [D loss: 0.501886, acc.: 78.91%] [G loss: 0.427114]\n",
      "epoch:1 step:1537 [D loss: 0.532643, acc.: 76.56%] [G loss: 0.446737]\n",
      "epoch:1 step:1538 [D loss: 0.539951, acc.: 77.34%] [G loss: 0.347702]\n",
      "epoch:1 step:1539 [D loss: 0.461784, acc.: 82.81%] [G loss: 0.390942]\n",
      "epoch:1 step:1540 [D loss: 0.485067, acc.: 76.56%] [G loss: 0.454225]\n",
      "epoch:1 step:1541 [D loss: 0.536263, acc.: 75.00%] [G loss: 0.345066]\n",
      "epoch:1 step:1542 [D loss: 0.450963, acc.: 84.38%] [G loss: 0.478452]\n",
      "epoch:1 step:1543 [D loss: 0.468066, acc.: 86.72%] [G loss: 0.563427]\n",
      "epoch:1 step:1544 [D loss: 0.521438, acc.: 76.56%] [G loss: 0.431827]\n",
      "epoch:1 step:1545 [D loss: 0.439607, acc.: 88.28%] [G loss: 0.442341]\n",
      "epoch:1 step:1546 [D loss: 0.427247, acc.: 87.50%] [G loss: 0.546234]\n",
      "epoch:1 step:1547 [D loss: 0.517851, acc.: 78.91%] [G loss: 0.448570]\n",
      "epoch:1 step:1548 [D loss: 0.425665, acc.: 89.84%] [G loss: 0.549465]\n",
      "epoch:1 step:1549 [D loss: 0.451016, acc.: 86.72%] [G loss: 0.540672]\n",
      "epoch:1 step:1550 [D loss: 0.443095, acc.: 85.16%] [G loss: 0.551481]\n",
      "epoch:1 step:1551 [D loss: 0.458078, acc.: 87.50%] [G loss: 0.476189]\n",
      "epoch:1 step:1552 [D loss: 0.538454, acc.: 80.47%] [G loss: 0.432145]\n",
      "epoch:1 step:1553 [D loss: 0.481308, acc.: 82.03%] [G loss: 0.387439]\n",
      "epoch:1 step:1554 [D loss: 0.502583, acc.: 78.91%] [G loss: 0.518815]\n",
      "epoch:1 step:1555 [D loss: 0.479031, acc.: 81.25%] [G loss: 0.496470]\n",
      "epoch:1 step:1556 [D loss: 0.443496, acc.: 86.72%] [G loss: 0.566547]\n",
      "epoch:1 step:1557 [D loss: 0.436977, acc.: 87.50%] [G loss: 0.558847]\n",
      "epoch:1 step:1558 [D loss: 0.503034, acc.: 81.25%] [G loss: 0.459858]\n",
      "epoch:1 step:1559 [D loss: 0.619221, acc.: 65.62%] [G loss: 0.324981]\n",
      "epoch:1 step:1560 [D loss: 0.486986, acc.: 81.25%] [G loss: 0.364466]\n",
      "epoch:1 step:1561 [D loss: 0.461364, acc.: 80.47%] [G loss: 0.608721]\n",
      "epoch:1 step:1562 [D loss: 0.493886, acc.: 78.12%] [G loss: 0.528172]\n",
      "epoch:1 step:1563 [D loss: 0.456865, acc.: 82.81%] [G loss: 0.521833]\n",
      "epoch:1 step:1564 [D loss: 0.426695, acc.: 86.72%] [G loss: 0.597909]\n",
      "epoch:1 step:1565 [D loss: 0.473298, acc.: 84.38%] [G loss: 0.453956]\n",
      "epoch:1 step:1566 [D loss: 0.503863, acc.: 79.69%] [G loss: 0.457565]\n",
      "epoch:1 step:1567 [D loss: 0.472979, acc.: 82.81%] [G loss: 0.476486]\n",
      "epoch:1 step:1568 [D loss: 0.456001, acc.: 85.94%] [G loss: 0.554394]\n",
      "epoch:1 step:1569 [D loss: 0.470893, acc.: 85.16%] [G loss: 0.520512]\n",
      "epoch:1 step:1570 [D loss: 0.441052, acc.: 88.28%] [G loss: 0.532191]\n",
      "epoch:1 step:1571 [D loss: 0.480508, acc.: 82.03%] [G loss: 0.438885]\n",
      "epoch:1 step:1572 [D loss: 0.420661, acc.: 88.28%] [G loss: 0.548864]\n",
      "epoch:1 step:1573 [D loss: 0.519408, acc.: 76.56%] [G loss: 0.480292]\n",
      "epoch:1 step:1574 [D loss: 0.498346, acc.: 75.00%] [G loss: 0.495384]\n",
      "epoch:1 step:1575 [D loss: 0.454653, acc.: 85.94%] [G loss: 0.462956]\n",
      "epoch:1 step:1576 [D loss: 0.430777, acc.: 85.94%] [G loss: 0.548687]\n",
      "epoch:1 step:1577 [D loss: 0.474472, acc.: 84.38%] [G loss: 0.539889]\n",
      "epoch:1 step:1578 [D loss: 0.434683, acc.: 85.16%] [G loss: 0.671793]\n",
      "epoch:1 step:1579 [D loss: 0.454448, acc.: 77.34%] [G loss: 0.586753]\n",
      "epoch:1 step:1580 [D loss: 0.488184, acc.: 82.03%] [G loss: 0.554277]\n",
      "epoch:1 step:1581 [D loss: 0.500218, acc.: 80.47%] [G loss: 0.441240]\n",
      "epoch:1 step:1582 [D loss: 0.536688, acc.: 72.66%] [G loss: 0.345486]\n",
      "epoch:1 step:1583 [D loss: 0.482080, acc.: 82.81%] [G loss: 0.477053]\n",
      "epoch:1 step:1584 [D loss: 0.480939, acc.: 78.91%] [G loss: 0.447898]\n",
      "epoch:1 step:1585 [D loss: 0.468397, acc.: 82.03%] [G loss: 0.494924]\n",
      "epoch:1 step:1586 [D loss: 0.460666, acc.: 82.03%] [G loss: 0.499230]\n",
      "epoch:1 step:1587 [D loss: 0.513416, acc.: 77.34%] [G loss: 0.427683]\n",
      "epoch:1 step:1588 [D loss: 0.482992, acc.: 83.59%] [G loss: 0.483639]\n",
      "epoch:1 step:1589 [D loss: 0.559647, acc.: 67.97%] [G loss: 0.363650]\n",
      "epoch:1 step:1590 [D loss: 0.557695, acc.: 73.44%] [G loss: 0.359259]\n",
      "epoch:1 step:1591 [D loss: 0.443220, acc.: 90.62%] [G loss: 0.501727]\n",
      "epoch:1 step:1592 [D loss: 0.550774, acc.: 77.34%] [G loss: 0.418580]\n",
      "epoch:1 step:1593 [D loss: 0.520405, acc.: 81.25%] [G loss: 0.471162]\n",
      "epoch:1 step:1594 [D loss: 0.482061, acc.: 85.16%] [G loss: 0.448708]\n",
      "epoch:1 step:1595 [D loss: 0.469167, acc.: 84.38%] [G loss: 0.469123]\n",
      "epoch:1 step:1596 [D loss: 0.438275, acc.: 87.50%] [G loss: 0.567573]\n",
      "epoch:1 step:1597 [D loss: 0.425761, acc.: 92.19%] [G loss: 0.487721]\n",
      "epoch:1 step:1598 [D loss: 0.420780, acc.: 91.41%] [G loss: 0.545903]\n",
      "epoch:1 step:1599 [D loss: 0.572862, acc.: 67.97%] [G loss: 0.393895]\n",
      "epoch:1 step:1600 [D loss: 0.472235, acc.: 79.69%] [G loss: 0.469761]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 3.508344\n",
      "FID: 127.440155\n",
      "0 = 14.257149666786159\n",
      "1 = 0.1900173319892131\n",
      "2 = 0.9926999807357788\n",
      "3 = 0.9854000210762024\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9854000210762024\n",
      "7 = 12.17036000747682\n",
      "8 = 0.20525098509093284\n",
      "9 = 0.9789000153541565\n",
      "10 = 0.9613999724388123\n",
      "11 = 0.996399998664856\n",
      "12 = 0.9962694048881531\n",
      "13 = 0.9613999724388123\n",
      "14 = 3.5083465576171875\n",
      "15 = 8.395915985107422\n",
      "16 = 0.39057862758636475\n",
      "17 = 3.5083439350128174\n",
      "18 = 127.44015502929688\n",
      "epoch:1 step:1601 [D loss: 0.456434, acc.: 81.25%] [G loss: 0.554406]\n",
      "epoch:1 step:1602 [D loss: 0.489267, acc.: 77.34%] [G loss: 0.599276]\n",
      "epoch:1 step:1603 [D loss: 0.407362, acc.: 85.16%] [G loss: 0.671909]\n",
      "epoch:1 step:1604 [D loss: 0.458975, acc.: 83.59%] [G loss: 0.554775]\n",
      "epoch:1 step:1605 [D loss: 0.488126, acc.: 80.47%] [G loss: 0.430452]\n",
      "epoch:1 step:1606 [D loss: 0.465506, acc.: 80.47%] [G loss: 0.510891]\n",
      "epoch:1 step:1607 [D loss: 0.435454, acc.: 85.94%] [G loss: 0.592058]\n",
      "epoch:1 step:1608 [D loss: 0.490711, acc.: 79.69%] [G loss: 0.511245]\n",
      "epoch:1 step:1609 [D loss: 0.537302, acc.: 75.00%] [G loss: 0.407403]\n",
      "epoch:1 step:1610 [D loss: 0.534070, acc.: 79.69%] [G loss: 0.382641]\n",
      "epoch:1 step:1611 [D loss: 0.509085, acc.: 79.69%] [G loss: 0.428676]\n",
      "epoch:1 step:1612 [D loss: 0.435987, acc.: 85.16%] [G loss: 0.496842]\n",
      "epoch:1 step:1613 [D loss: 0.474008, acc.: 82.81%] [G loss: 0.519023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1614 [D loss: 0.475765, acc.: 84.38%] [G loss: 0.514444]\n",
      "epoch:1 step:1615 [D loss: 0.407247, acc.: 89.06%] [G loss: 0.650936]\n",
      "epoch:1 step:1616 [D loss: 0.454854, acc.: 85.16%] [G loss: 0.516060]\n",
      "epoch:1 step:1617 [D loss: 0.375117, acc.: 95.31%] [G loss: 0.577886]\n",
      "epoch:1 step:1618 [D loss: 0.458007, acc.: 87.50%] [G loss: 0.512845]\n",
      "epoch:1 step:1619 [D loss: 0.433661, acc.: 89.06%] [G loss: 0.452165]\n",
      "epoch:1 step:1620 [D loss: 0.452138, acc.: 85.94%] [G loss: 0.485684]\n",
      "epoch:1 step:1621 [D loss: 0.473873, acc.: 82.03%] [G loss: 0.491582]\n",
      "epoch:1 step:1622 [D loss: 0.469618, acc.: 85.94%] [G loss: 0.514357]\n",
      "epoch:1 step:1623 [D loss: 0.505474, acc.: 79.69%] [G loss: 0.405173]\n",
      "epoch:1 step:1624 [D loss: 0.448696, acc.: 85.16%] [G loss: 0.462110]\n",
      "epoch:1 step:1625 [D loss: 0.467848, acc.: 85.16%] [G loss: 0.457052]\n",
      "epoch:1 step:1626 [D loss: 0.488798, acc.: 82.81%] [G loss: 0.480909]\n",
      "epoch:1 step:1627 [D loss: 0.422969, acc.: 89.06%] [G loss: 0.617667]\n",
      "epoch:1 step:1628 [D loss: 0.425612, acc.: 87.50%] [G loss: 0.583442]\n",
      "epoch:1 step:1629 [D loss: 0.478177, acc.: 79.69%] [G loss: 0.540433]\n",
      "epoch:1 step:1630 [D loss: 0.419984, acc.: 88.28%] [G loss: 0.557797]\n",
      "epoch:1 step:1631 [D loss: 0.438508, acc.: 89.84%] [G loss: 0.531243]\n",
      "epoch:1 step:1632 [D loss: 0.447745, acc.: 85.94%] [G loss: 0.463749]\n",
      "epoch:1 step:1633 [D loss: 0.435631, acc.: 89.06%] [G loss: 0.450521]\n",
      "epoch:1 step:1634 [D loss: 0.416811, acc.: 85.94%] [G loss: 0.654890]\n",
      "epoch:1 step:1635 [D loss: 0.437508, acc.: 88.28%] [G loss: 0.544656]\n",
      "epoch:1 step:1636 [D loss: 0.452387, acc.: 81.25%] [G loss: 0.525068]\n",
      "epoch:1 step:1637 [D loss: 0.483781, acc.: 74.22%] [G loss: 0.522706]\n",
      "epoch:1 step:1638 [D loss: 0.466634, acc.: 82.03%] [G loss: 0.583028]\n",
      "epoch:1 step:1639 [D loss: 0.480623, acc.: 78.12%] [G loss: 0.605199]\n",
      "epoch:1 step:1640 [D loss: 0.460910, acc.: 86.72%] [G loss: 0.497282]\n",
      "epoch:1 step:1641 [D loss: 0.491562, acc.: 82.81%] [G loss: 0.393455]\n",
      "epoch:1 step:1642 [D loss: 0.443737, acc.: 84.38%] [G loss: 0.545229]\n",
      "epoch:1 step:1643 [D loss: 0.452149, acc.: 85.16%] [G loss: 0.511796]\n",
      "epoch:1 step:1644 [D loss: 0.362011, acc.: 92.19%] [G loss: 0.778794]\n",
      "epoch:1 step:1645 [D loss: 0.397892, acc.: 85.94%] [G loss: 0.710558]\n",
      "epoch:1 step:1646 [D loss: 0.395308, acc.: 86.72%] [G loss: 0.774222]\n",
      "epoch:1 step:1647 [D loss: 0.616333, acc.: 72.66%] [G loss: 0.376757]\n",
      "epoch:1 step:1648 [D loss: 0.557114, acc.: 74.22%] [G loss: 0.396024]\n",
      "epoch:1 step:1649 [D loss: 0.433266, acc.: 82.81%] [G loss: 0.633441]\n",
      "epoch:1 step:1650 [D loss: 0.476803, acc.: 82.81%] [G loss: 0.643823]\n",
      "epoch:1 step:1651 [D loss: 0.432573, acc.: 84.38%] [G loss: 0.726304]\n",
      "epoch:1 step:1652 [D loss: 0.457050, acc.: 82.03%] [G loss: 0.586762]\n",
      "epoch:1 step:1653 [D loss: 0.497817, acc.: 82.03%] [G loss: 0.463092]\n",
      "epoch:1 step:1654 [D loss: 0.499123, acc.: 79.69%] [G loss: 0.465161]\n",
      "epoch:1 step:1655 [D loss: 0.529306, acc.: 75.78%] [G loss: 0.490196]\n",
      "epoch:1 step:1656 [D loss: 0.491025, acc.: 82.03%] [G loss: 0.551473]\n",
      "epoch:1 step:1657 [D loss: 0.530723, acc.: 78.12%] [G loss: 0.426896]\n",
      "epoch:1 step:1658 [D loss: 0.505793, acc.: 76.56%] [G loss: 0.377765]\n",
      "epoch:1 step:1659 [D loss: 0.470447, acc.: 78.91%] [G loss: 0.546828]\n",
      "epoch:1 step:1660 [D loss: 0.571698, acc.: 69.53%] [G loss: 0.485867]\n",
      "epoch:1 step:1661 [D loss: 0.483879, acc.: 81.25%] [G loss: 0.555794]\n",
      "epoch:1 step:1662 [D loss: 0.480194, acc.: 82.03%] [G loss: 0.556077]\n",
      "epoch:1 step:1663 [D loss: 0.507575, acc.: 78.91%] [G loss: 0.525259]\n",
      "epoch:1 step:1664 [D loss: 0.575555, acc.: 72.66%] [G loss: 0.353193]\n",
      "epoch:1 step:1665 [D loss: 0.489113, acc.: 80.47%] [G loss: 0.498633]\n",
      "epoch:1 step:1666 [D loss: 0.488336, acc.: 80.47%] [G loss: 0.580698]\n",
      "epoch:1 step:1667 [D loss: 0.438111, acc.: 86.72%] [G loss: 0.545554]\n",
      "epoch:1 step:1668 [D loss: 0.453633, acc.: 80.47%] [G loss: 0.622514]\n",
      "epoch:1 step:1669 [D loss: 0.383558, acc.: 88.28%] [G loss: 0.702317]\n",
      "epoch:1 step:1670 [D loss: 0.441809, acc.: 82.81%] [G loss: 0.757429]\n",
      "epoch:1 step:1671 [D loss: 0.521515, acc.: 76.56%] [G loss: 0.622917]\n",
      "epoch:1 step:1672 [D loss: 0.544594, acc.: 75.78%] [G loss: 0.547035]\n",
      "epoch:1 step:1673 [D loss: 0.458196, acc.: 85.16%] [G loss: 0.545357]\n",
      "epoch:1 step:1674 [D loss: 0.469597, acc.: 77.34%] [G loss: 0.542847]\n",
      "epoch:1 step:1675 [D loss: 0.496649, acc.: 75.78%] [G loss: 0.646225]\n",
      "epoch:1 step:1676 [D loss: 0.520183, acc.: 78.91%] [G loss: 0.489289]\n",
      "epoch:1 step:1677 [D loss: 0.530072, acc.: 78.12%] [G loss: 0.411434]\n",
      "epoch:1 step:1678 [D loss: 0.514080, acc.: 77.34%] [G loss: 0.382243]\n",
      "epoch:1 step:1679 [D loss: 0.535612, acc.: 72.66%] [G loss: 0.444893]\n",
      "epoch:1 step:1680 [D loss: 0.504304, acc.: 78.91%] [G loss: 0.446383]\n",
      "epoch:1 step:1681 [D loss: 0.516262, acc.: 76.56%] [G loss: 0.458795]\n",
      "epoch:1 step:1682 [D loss: 0.491358, acc.: 82.81%] [G loss: 0.553517]\n",
      "epoch:1 step:1683 [D loss: 0.435265, acc.: 82.81%] [G loss: 0.610273]\n",
      "epoch:1 step:1684 [D loss: 0.410347, acc.: 85.16%] [G loss: 0.672089]\n",
      "epoch:1 step:1685 [D loss: 0.433296, acc.: 85.94%] [G loss: 0.457354]\n",
      "epoch:1 step:1686 [D loss: 0.499863, acc.: 79.69%] [G loss: 0.494597]\n",
      "epoch:1 step:1687 [D loss: 0.484442, acc.: 81.25%] [G loss: 0.459536]\n",
      "epoch:1 step:1688 [D loss: 0.471789, acc.: 82.81%] [G loss: 0.454364]\n",
      "epoch:1 step:1689 [D loss: 0.459604, acc.: 85.16%] [G loss: 0.623267]\n",
      "epoch:1 step:1690 [D loss: 0.409397, acc.: 86.72%] [G loss: 0.765251]\n",
      "epoch:1 step:1691 [D loss: 0.465723, acc.: 84.38%] [G loss: 0.655529]\n",
      "epoch:1 step:1692 [D loss: 0.464927, acc.: 84.38%] [G loss: 0.633098]\n",
      "epoch:1 step:1693 [D loss: 0.534580, acc.: 76.56%] [G loss: 0.523545]\n",
      "epoch:1 step:1694 [D loss: 0.434455, acc.: 89.06%] [G loss: 0.632712]\n",
      "epoch:1 step:1695 [D loss: 0.513140, acc.: 76.56%] [G loss: 0.532354]\n",
      "epoch:1 step:1696 [D loss: 0.494173, acc.: 78.12%] [G loss: 0.563431]\n",
      "epoch:1 step:1697 [D loss: 0.473349, acc.: 82.81%] [G loss: 0.543176]\n",
      "epoch:1 step:1698 [D loss: 0.498522, acc.: 73.44%] [G loss: 0.551313]\n",
      "epoch:1 step:1699 [D loss: 0.484114, acc.: 79.69%] [G loss: 0.573506]\n",
      "epoch:1 step:1700 [D loss: 0.449682, acc.: 82.03%] [G loss: 0.647389]\n",
      "epoch:1 step:1701 [D loss: 0.492288, acc.: 79.69%] [G loss: 0.610065]\n",
      "epoch:1 step:1702 [D loss: 0.618377, acc.: 67.19%] [G loss: 0.434626]\n",
      "epoch:1 step:1703 [D loss: 0.581918, acc.: 71.88%] [G loss: 0.415971]\n",
      "epoch:1 step:1704 [D loss: 0.512146, acc.: 75.78%] [G loss: 0.479239]\n",
      "epoch:1 step:1705 [D loss: 0.444391, acc.: 85.16%] [G loss: 0.704099]\n",
      "epoch:1 step:1706 [D loss: 0.449303, acc.: 83.59%] [G loss: 0.679835]\n",
      "epoch:1 step:1707 [D loss: 0.490646, acc.: 78.12%] [G loss: 0.694166]\n",
      "epoch:1 step:1708 [D loss: 0.452627, acc.: 82.03%] [G loss: 0.703482]\n",
      "epoch:1 step:1709 [D loss: 0.443404, acc.: 85.16%] [G loss: 0.645056]\n",
      "epoch:1 step:1710 [D loss: 0.459754, acc.: 83.59%] [G loss: 0.677535]\n",
      "epoch:1 step:1711 [D loss: 0.514959, acc.: 74.22%] [G loss: 0.614859]\n",
      "epoch:1 step:1712 [D loss: 0.461825, acc.: 85.94%] [G loss: 0.701570]\n",
      "epoch:1 step:1713 [D loss: 0.532762, acc.: 79.69%] [G loss: 0.536159]\n",
      "epoch:1 step:1714 [D loss: 0.484624, acc.: 79.69%] [G loss: 0.515044]\n",
      "epoch:1 step:1715 [D loss: 0.528520, acc.: 75.78%] [G loss: 0.528351]\n",
      "epoch:1 step:1716 [D loss: 0.467872, acc.: 85.16%] [G loss: 0.613188]\n",
      "epoch:1 step:1717 [D loss: 0.528500, acc.: 75.00%] [G loss: 0.473186]\n",
      "epoch:1 step:1718 [D loss: 0.429932, acc.: 83.59%] [G loss: 0.640913]\n",
      "epoch:1 step:1719 [D loss: 0.426878, acc.: 85.16%] [G loss: 0.679345]\n",
      "epoch:1 step:1720 [D loss: 0.517748, acc.: 71.88%] [G loss: 0.586057]\n",
      "epoch:1 step:1721 [D loss: 0.450164, acc.: 86.72%] [G loss: 0.603567]\n",
      "epoch:1 step:1722 [D loss: 0.504106, acc.: 78.12%] [G loss: 0.650801]\n",
      "epoch:1 step:1723 [D loss: 0.429546, acc.: 85.16%] [G loss: 0.673424]\n",
      "epoch:1 step:1724 [D loss: 0.538001, acc.: 74.22%] [G loss: 0.488618]\n",
      "epoch:1 step:1725 [D loss: 0.544076, acc.: 78.12%] [G loss: 0.439079]\n",
      "epoch:1 step:1726 [D loss: 0.439455, acc.: 85.16%] [G loss: 0.590531]\n",
      "epoch:1 step:1727 [D loss: 0.472771, acc.: 81.25%] [G loss: 0.529066]\n",
      "epoch:1 step:1728 [D loss: 0.473887, acc.: 78.91%] [G loss: 0.581734]\n",
      "epoch:1 step:1729 [D loss: 0.416584, acc.: 85.94%] [G loss: 0.786653]\n",
      "epoch:1 step:1730 [D loss: 0.517652, acc.: 81.25%] [G loss: 0.530578]\n",
      "epoch:1 step:1731 [D loss: 0.462066, acc.: 85.16%] [G loss: 0.550872]\n",
      "epoch:1 step:1732 [D loss: 0.499509, acc.: 77.34%] [G loss: 0.545137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1733 [D loss: 0.472556, acc.: 77.34%] [G loss: 0.751274]\n",
      "epoch:1 step:1734 [D loss: 0.495467, acc.: 79.69%] [G loss: 0.566049]\n",
      "epoch:1 step:1735 [D loss: 0.452156, acc.: 82.81%] [G loss: 0.573276]\n",
      "epoch:1 step:1736 [D loss: 0.540714, acc.: 74.22%] [G loss: 0.564646]\n",
      "epoch:1 step:1737 [D loss: 0.571098, acc.: 75.00%] [G loss: 0.412712]\n",
      "epoch:1 step:1738 [D loss: 0.493746, acc.: 81.25%] [G loss: 0.490880]\n",
      "epoch:1 step:1739 [D loss: 0.487243, acc.: 78.91%] [G loss: 0.549895]\n",
      "epoch:1 step:1740 [D loss: 0.481057, acc.: 79.69%] [G loss: 0.624452]\n",
      "epoch:1 step:1741 [D loss: 0.474833, acc.: 81.25%] [G loss: 0.505491]\n",
      "epoch:1 step:1742 [D loss: 0.435339, acc.: 84.38%] [G loss: 0.493577]\n",
      "epoch:1 step:1743 [D loss: 0.413766, acc.: 85.94%] [G loss: 0.602401]\n",
      "epoch:1 step:1744 [D loss: 0.478503, acc.: 77.34%] [G loss: 0.580287]\n",
      "epoch:1 step:1745 [D loss: 0.504616, acc.: 78.12%] [G loss: 0.545552]\n",
      "epoch:1 step:1746 [D loss: 0.521571, acc.: 73.44%] [G loss: 0.508461]\n",
      "epoch:1 step:1747 [D loss: 0.441350, acc.: 85.94%] [G loss: 0.456607]\n",
      "epoch:1 step:1748 [D loss: 0.504427, acc.: 82.03%] [G loss: 0.430297]\n",
      "epoch:1 step:1749 [D loss: 0.510143, acc.: 78.12%] [G loss: 0.480845]\n",
      "epoch:1 step:1750 [D loss: 0.489056, acc.: 78.91%] [G loss: 0.488794]\n",
      "epoch:1 step:1751 [D loss: 0.498497, acc.: 75.00%] [G loss: 0.483486]\n",
      "epoch:1 step:1752 [D loss: 0.509819, acc.: 78.12%] [G loss: 0.595614]\n",
      "epoch:1 step:1753 [D loss: 0.462778, acc.: 82.03%] [G loss: 0.644336]\n",
      "epoch:1 step:1754 [D loss: 0.516813, acc.: 79.69%] [G loss: 0.463032]\n",
      "epoch:1 step:1755 [D loss: 0.458429, acc.: 87.50%] [G loss: 0.434800]\n",
      "epoch:1 step:1756 [D loss: 0.452565, acc.: 85.94%] [G loss: 0.538556]\n",
      "epoch:1 step:1757 [D loss: 0.467263, acc.: 79.69%] [G loss: 0.556132]\n",
      "epoch:1 step:1758 [D loss: 0.423351, acc.: 85.16%] [G loss: 0.513296]\n",
      "epoch:1 step:1759 [D loss: 0.412532, acc.: 87.50%] [G loss: 0.628355]\n",
      "epoch:1 step:1760 [D loss: 0.449792, acc.: 81.25%] [G loss: 0.588179]\n",
      "epoch:1 step:1761 [D loss: 0.600297, acc.: 66.41%] [G loss: 0.368153]\n",
      "epoch:1 step:1762 [D loss: 0.487172, acc.: 80.47%] [G loss: 0.355941]\n",
      "epoch:1 step:1763 [D loss: 0.500103, acc.: 75.78%] [G loss: 0.450267]\n",
      "epoch:1 step:1764 [D loss: 0.501396, acc.: 76.56%] [G loss: 0.431526]\n",
      "epoch:1 step:1765 [D loss: 0.490919, acc.: 75.78%] [G loss: 0.510143]\n",
      "epoch:1 step:1766 [D loss: 0.417250, acc.: 84.38%] [G loss: 0.519875]\n",
      "epoch:1 step:1767 [D loss: 0.441397, acc.: 85.16%] [G loss: 0.588899]\n",
      "epoch:1 step:1768 [D loss: 0.438973, acc.: 84.38%] [G loss: 0.557961]\n",
      "epoch:1 step:1769 [D loss: 0.438650, acc.: 83.59%] [G loss: 0.564562]\n",
      "epoch:1 step:1770 [D loss: 0.452266, acc.: 87.50%] [G loss: 0.544593]\n",
      "epoch:1 step:1771 [D loss: 0.425125, acc.: 88.28%] [G loss: 0.568195]\n",
      "epoch:1 step:1772 [D loss: 0.453734, acc.: 85.16%] [G loss: 0.573458]\n",
      "epoch:1 step:1773 [D loss: 0.514309, acc.: 75.78%] [G loss: 0.441831]\n",
      "epoch:1 step:1774 [D loss: 0.454889, acc.: 85.16%] [G loss: 0.433433]\n",
      "epoch:1 step:1775 [D loss: 0.485732, acc.: 81.25%] [G loss: 0.463644]\n",
      "epoch:1 step:1776 [D loss: 0.474258, acc.: 81.25%] [G loss: 0.555899]\n",
      "epoch:1 step:1777 [D loss: 0.519230, acc.: 78.12%] [G loss: 0.529023]\n",
      "epoch:1 step:1778 [D loss: 0.442024, acc.: 84.38%] [G loss: 0.622732]\n",
      "epoch:1 step:1779 [D loss: 0.491966, acc.: 81.25%] [G loss: 0.542018]\n",
      "epoch:1 step:1780 [D loss: 0.523582, acc.: 77.34%] [G loss: 0.568753]\n",
      "epoch:1 step:1781 [D loss: 0.541269, acc.: 79.69%] [G loss: 0.499588]\n",
      "epoch:1 step:1782 [D loss: 0.493605, acc.: 81.25%] [G loss: 0.446309]\n",
      "epoch:1 step:1783 [D loss: 0.461950, acc.: 86.72%] [G loss: 0.559302]\n",
      "epoch:1 step:1784 [D loss: 0.433318, acc.: 86.72%] [G loss: 0.596760]\n",
      "epoch:1 step:1785 [D loss: 0.482661, acc.: 82.03%] [G loss: 0.569154]\n",
      "epoch:1 step:1786 [D loss: 0.483797, acc.: 82.03%] [G loss: 0.620734]\n",
      "epoch:1 step:1787 [D loss: 0.450470, acc.: 86.72%] [G loss: 0.687310]\n",
      "epoch:1 step:1788 [D loss: 0.429271, acc.: 86.72%] [G loss: 0.714908]\n",
      "epoch:1 step:1789 [D loss: 0.403500, acc.: 85.16%] [G loss: 0.802645]\n",
      "epoch:1 step:1790 [D loss: 0.396673, acc.: 88.28%] [G loss: 0.830630]\n",
      "epoch:1 step:1791 [D loss: 0.368077, acc.: 89.06%] [G loss: 0.812126]\n",
      "epoch:1 step:1792 [D loss: 0.403212, acc.: 86.72%] [G loss: 0.775668]\n",
      "epoch:1 step:1793 [D loss: 0.415242, acc.: 88.28%] [G loss: 0.707441]\n",
      "epoch:1 step:1794 [D loss: 0.422387, acc.: 83.59%] [G loss: 0.758659]\n",
      "epoch:1 step:1795 [D loss: 0.611439, acc.: 67.97%] [G loss: 0.460292]\n",
      "epoch:1 step:1796 [D loss: 0.502476, acc.: 81.25%] [G loss: 0.543868]\n",
      "epoch:1 step:1797 [D loss: 0.448313, acc.: 82.81%] [G loss: 0.623842]\n",
      "epoch:1 step:1798 [D loss: 0.486954, acc.: 79.69%] [G loss: 0.541915]\n",
      "epoch:1 step:1799 [D loss: 0.456939, acc.: 82.81%] [G loss: 0.564642]\n",
      "epoch:1 step:1800 [D loss: 0.494638, acc.: 79.69%] [G loss: 0.576196]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 3.984018\n",
      "FID: 114.155777\n",
      "0 = 14.3227261699676\n",
      "1 = 0.17964105420452445\n",
      "2 = 0.9887999892234802\n",
      "3 = 0.9775999784469604\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9775999784469604\n",
      "7 = 11.623113004136101\n",
      "8 = 0.1992115692639694\n",
      "9 = 0.9682000279426575\n",
      "10 = 0.9441999793052673\n",
      "11 = 0.9922000169754028\n",
      "12 = 0.9918067455291748\n",
      "13 = 0.9441999793052673\n",
      "14 = 3.9840216636657715\n",
      "15 = 7.99962043762207\n",
      "16 = 0.40553954243659973\n",
      "17 = 3.984018087387085\n",
      "18 = 114.15577697753906\n",
      "epoch:1 step:1801 [D loss: 0.503055, acc.: 78.91%] [G loss: 0.444056]\n",
      "epoch:1 step:1802 [D loss: 0.469988, acc.: 79.69%] [G loss: 0.497073]\n",
      "epoch:1 step:1803 [D loss: 0.469130, acc.: 85.94%] [G loss: 0.511150]\n",
      "epoch:1 step:1804 [D loss: 0.534176, acc.: 73.44%] [G loss: 0.480846]\n",
      "epoch:1 step:1805 [D loss: 0.462781, acc.: 80.47%] [G loss: 0.602368]\n",
      "epoch:1 step:1806 [D loss: 0.439137, acc.: 86.72%] [G loss: 0.592363]\n",
      "epoch:1 step:1807 [D loss: 0.465003, acc.: 81.25%] [G loss: 0.722568]\n",
      "epoch:1 step:1808 [D loss: 0.419130, acc.: 85.16%] [G loss: 0.660179]\n",
      "epoch:1 step:1809 [D loss: 0.429809, acc.: 82.03%] [G loss: 0.694065]\n",
      "epoch:1 step:1810 [D loss: 0.498379, acc.: 78.12%] [G loss: 0.649780]\n",
      "epoch:1 step:1811 [D loss: 0.463306, acc.: 79.69%] [G loss: 0.609230]\n",
      "epoch:1 step:1812 [D loss: 0.468475, acc.: 80.47%] [G loss: 0.669287]\n",
      "epoch:1 step:1813 [D loss: 0.494177, acc.: 76.56%] [G loss: 0.564373]\n",
      "epoch:1 step:1814 [D loss: 0.500212, acc.: 77.34%] [G loss: 0.469970]\n",
      "epoch:1 step:1815 [D loss: 0.533828, acc.: 72.66%] [G loss: 0.480004]\n",
      "epoch:1 step:1816 [D loss: 0.541161, acc.: 71.09%] [G loss: 0.452109]\n",
      "epoch:1 step:1817 [D loss: 0.549650, acc.: 76.56%] [G loss: 0.378007]\n",
      "epoch:1 step:1818 [D loss: 0.527648, acc.: 75.00%] [G loss: 0.418731]\n",
      "epoch:1 step:1819 [D loss: 0.546217, acc.: 73.44%] [G loss: 0.500414]\n",
      "epoch:1 step:1820 [D loss: 0.497893, acc.: 76.56%] [G loss: 0.581221]\n",
      "epoch:1 step:1821 [D loss: 0.439708, acc.: 86.72%] [G loss: 0.624057]\n",
      "epoch:1 step:1822 [D loss: 0.425537, acc.: 86.72%] [G loss: 0.623847]\n",
      "epoch:1 step:1823 [D loss: 0.402646, acc.: 87.50%] [G loss: 0.804448]\n",
      "epoch:1 step:1824 [D loss: 0.367466, acc.: 89.06%] [G loss: 0.970074]\n",
      "epoch:1 step:1825 [D loss: 0.454050, acc.: 80.47%] [G loss: 0.769600]\n",
      "epoch:1 step:1826 [D loss: 0.459815, acc.: 82.03%] [G loss: 0.831349]\n",
      "epoch:1 step:1827 [D loss: 0.387884, acc.: 89.06%] [G loss: 0.905599]\n",
      "epoch:1 step:1828 [D loss: 0.564748, acc.: 68.75%] [G loss: 0.445942]\n",
      "epoch:1 step:1829 [D loss: 0.631030, acc.: 64.84%] [G loss: 0.387380]\n",
      "epoch:1 step:1830 [D loss: 0.492641, acc.: 82.03%] [G loss: 0.538089]\n",
      "epoch:1 step:1831 [D loss: 0.457863, acc.: 83.59%] [G loss: 0.570548]\n",
      "epoch:1 step:1832 [D loss: 0.543678, acc.: 72.66%] [G loss: 0.449548]\n",
      "epoch:1 step:1833 [D loss: 0.493498, acc.: 78.91%] [G loss: 0.540576]\n",
      "epoch:1 step:1834 [D loss: 0.398016, acc.: 86.72%] [G loss: 0.683422]\n",
      "epoch:1 step:1835 [D loss: 0.465641, acc.: 86.72%] [G loss: 0.574898]\n",
      "epoch:1 step:1836 [D loss: 0.501741, acc.: 81.25%] [G loss: 0.524426]\n",
      "epoch:1 step:1837 [D loss: 0.481700, acc.: 78.91%] [G loss: 0.623704]\n",
      "epoch:1 step:1838 [D loss: 0.475376, acc.: 78.12%] [G loss: 0.709977]\n",
      "epoch:1 step:1839 [D loss: 0.536767, acc.: 71.88%] [G loss: 0.442549]\n",
      "epoch:1 step:1840 [D loss: 0.516358, acc.: 69.53%] [G loss: 0.468016]\n",
      "epoch:1 step:1841 [D loss: 0.442855, acc.: 86.72%] [G loss: 0.614136]\n",
      "epoch:1 step:1842 [D loss: 0.510701, acc.: 72.66%] [G loss: 0.626059]\n",
      "epoch:1 step:1843 [D loss: 0.484258, acc.: 74.22%] [G loss: 0.669995]\n",
      "epoch:1 step:1844 [D loss: 0.473023, acc.: 78.12%] [G loss: 0.715623]\n",
      "epoch:1 step:1845 [D loss: 0.480567, acc.: 78.12%] [G loss: 0.606989]\n",
      "epoch:1 step:1846 [D loss: 0.477219, acc.: 82.03%] [G loss: 0.545721]\n",
      "epoch:1 step:1847 [D loss: 0.431116, acc.: 85.16%] [G loss: 0.747760]\n",
      "epoch:1 step:1848 [D loss: 0.475828, acc.: 85.16%] [G loss: 0.520042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1849 [D loss: 0.531441, acc.: 78.91%] [G loss: 0.471064]\n",
      "epoch:1 step:1850 [D loss: 0.548216, acc.: 67.97%] [G loss: 0.491135]\n",
      "epoch:1 step:1851 [D loss: 0.464869, acc.: 79.69%] [G loss: 0.672445]\n",
      "epoch:1 step:1852 [D loss: 0.562092, acc.: 74.22%] [G loss: 0.502404]\n",
      "epoch:1 step:1853 [D loss: 0.474915, acc.: 80.47%] [G loss: 0.495856]\n",
      "epoch:1 step:1854 [D loss: 0.583669, acc.: 73.44%] [G loss: 0.446875]\n",
      "epoch:1 step:1855 [D loss: 0.429201, acc.: 89.84%] [G loss: 0.540789]\n",
      "epoch:1 step:1856 [D loss: 0.443312, acc.: 86.72%] [G loss: 0.628849]\n",
      "epoch:1 step:1857 [D loss: 0.486237, acc.: 80.47%] [G loss: 0.538562]\n",
      "epoch:1 step:1858 [D loss: 0.381563, acc.: 89.06%] [G loss: 0.719034]\n",
      "epoch:1 step:1859 [D loss: 0.593770, acc.: 70.31%] [G loss: 0.424902]\n",
      "epoch:1 step:1860 [D loss: 0.411479, acc.: 86.72%] [G loss: 0.575333]\n",
      "epoch:1 step:1861 [D loss: 0.379527, acc.: 89.84%] [G loss: 0.753473]\n",
      "epoch:1 step:1862 [D loss: 0.406803, acc.: 88.28%] [G loss: 0.740940]\n",
      "epoch:1 step:1863 [D loss: 0.429304, acc.: 83.59%] [G loss: 0.835319]\n",
      "epoch:1 step:1864 [D loss: 0.460177, acc.: 80.47%] [G loss: 0.867270]\n",
      "epoch:1 step:1865 [D loss: 0.616457, acc.: 71.88%] [G loss: 0.583774]\n",
      "epoch:1 step:1866 [D loss: 0.377781, acc.: 86.72%] [G loss: 0.870033]\n",
      "epoch:1 step:1867 [D loss: 0.381014, acc.: 85.16%] [G loss: 0.922159]\n",
      "epoch:1 step:1868 [D loss: 0.583387, acc.: 71.09%] [G loss: 0.537859]\n",
      "epoch:1 step:1869 [D loss: 0.507244, acc.: 78.91%] [G loss: 0.488985]\n",
      "epoch:1 step:1870 [D loss: 0.442757, acc.: 82.03%] [G loss: 0.559014]\n",
      "epoch:1 step:1871 [D loss: 0.480079, acc.: 79.69%] [G loss: 0.511226]\n",
      "epoch:1 step:1872 [D loss: 0.393999, acc.: 90.62%] [G loss: 0.724857]\n",
      "epoch:1 step:1873 [D loss: 0.304508, acc.: 96.88%] [G loss: 0.905995]\n",
      "epoch:1 step:1874 [D loss: 0.525174, acc.: 71.09%] [G loss: 0.688555]\n",
      "epoch:2 step:1875 [D loss: 0.494311, acc.: 78.91%] [G loss: 0.635559]\n",
      "epoch:2 step:1876 [D loss: 0.486044, acc.: 78.12%] [G loss: 0.727428]\n",
      "epoch:2 step:1877 [D loss: 0.525605, acc.: 78.91%] [G loss: 0.615622]\n",
      "epoch:2 step:1878 [D loss: 0.487663, acc.: 77.34%] [G loss: 0.571237]\n",
      "epoch:2 step:1879 [D loss: 0.532754, acc.: 72.66%] [G loss: 0.516479]\n",
      "epoch:2 step:1880 [D loss: 0.477677, acc.: 81.25%] [G loss: 0.558794]\n",
      "epoch:2 step:1881 [D loss: 0.437505, acc.: 83.59%] [G loss: 0.682025]\n",
      "epoch:2 step:1882 [D loss: 0.451682, acc.: 85.16%] [G loss: 0.601671]\n",
      "epoch:2 step:1883 [D loss: 0.529323, acc.: 71.09%] [G loss: 0.533390]\n",
      "epoch:2 step:1884 [D loss: 0.467550, acc.: 80.47%] [G loss: 0.666659]\n",
      "epoch:2 step:1885 [D loss: 0.484164, acc.: 82.81%] [G loss: 0.587114]\n",
      "epoch:2 step:1886 [D loss: 0.475870, acc.: 78.12%] [G loss: 0.605133]\n",
      "epoch:2 step:1887 [D loss: 0.481043, acc.: 75.78%] [G loss: 0.516326]\n",
      "epoch:2 step:1888 [D loss: 0.486405, acc.: 82.03%] [G loss: 0.581822]\n",
      "epoch:2 step:1889 [D loss: 0.463540, acc.: 84.38%] [G loss: 0.552771]\n",
      "epoch:2 step:1890 [D loss: 0.461299, acc.: 86.72%] [G loss: 0.592207]\n",
      "epoch:2 step:1891 [D loss: 0.539433, acc.: 71.88%] [G loss: 0.514021]\n",
      "epoch:2 step:1892 [D loss: 0.531948, acc.: 74.22%] [G loss: 0.454586]\n",
      "epoch:2 step:1893 [D loss: 0.489566, acc.: 77.34%] [G loss: 0.488713]\n",
      "epoch:2 step:1894 [D loss: 0.571712, acc.: 68.75%] [G loss: 0.390937]\n",
      "epoch:2 step:1895 [D loss: 0.492293, acc.: 79.69%] [G loss: 0.453144]\n",
      "epoch:2 step:1896 [D loss: 0.432746, acc.: 82.03%] [G loss: 0.731961]\n",
      "epoch:2 step:1897 [D loss: 0.533926, acc.: 75.78%] [G loss: 0.574034]\n",
      "epoch:2 step:1898 [D loss: 0.505223, acc.: 74.22%] [G loss: 0.516232]\n",
      "epoch:2 step:1899 [D loss: 0.472135, acc.: 81.25%] [G loss: 0.565632]\n",
      "epoch:2 step:1900 [D loss: 0.515114, acc.: 78.12%] [G loss: 0.421843]\n",
      "epoch:2 step:1901 [D loss: 0.424263, acc.: 85.16%] [G loss: 0.630322]\n",
      "epoch:2 step:1902 [D loss: 0.450709, acc.: 87.50%] [G loss: 0.615605]\n",
      "epoch:2 step:1903 [D loss: 0.503084, acc.: 74.22%] [G loss: 0.455558]\n",
      "epoch:2 step:1904 [D loss: 0.485540, acc.: 79.69%] [G loss: 0.473279]\n",
      "epoch:2 step:1905 [D loss: 0.520364, acc.: 77.34%] [G loss: 0.504424]\n",
      "epoch:2 step:1906 [D loss: 0.489479, acc.: 81.25%] [G loss: 0.526074]\n",
      "epoch:2 step:1907 [D loss: 0.437914, acc.: 81.25%] [G loss: 0.578320]\n",
      "epoch:2 step:1908 [D loss: 0.460226, acc.: 82.03%] [G loss: 0.715073]\n",
      "epoch:2 step:1909 [D loss: 0.412698, acc.: 89.06%] [G loss: 0.633714]\n",
      "epoch:2 step:1910 [D loss: 0.403418, acc.: 84.38%] [G loss: 0.958615]\n",
      "epoch:2 step:1911 [D loss: 0.501423, acc.: 78.12%] [G loss: 0.570838]\n",
      "epoch:2 step:1912 [D loss: 0.637033, acc.: 65.62%] [G loss: 0.407566]\n",
      "epoch:2 step:1913 [D loss: 0.446782, acc.: 88.28%] [G loss: 0.481428]\n",
      "epoch:2 step:1914 [D loss: 0.479119, acc.: 80.47%] [G loss: 0.630520]\n",
      "epoch:2 step:1915 [D loss: 0.431654, acc.: 84.38%] [G loss: 0.568065]\n",
      "epoch:2 step:1916 [D loss: 0.445031, acc.: 83.59%] [G loss: 0.647758]\n",
      "epoch:2 step:1917 [D loss: 0.463211, acc.: 75.78%] [G loss: 0.568997]\n",
      "epoch:2 step:1918 [D loss: 0.482456, acc.: 78.91%] [G loss: 0.602138]\n",
      "epoch:2 step:1919 [D loss: 0.458225, acc.: 82.81%] [G loss: 0.555936]\n",
      "epoch:2 step:1920 [D loss: 0.413517, acc.: 82.81%] [G loss: 0.588215]\n",
      "epoch:2 step:1921 [D loss: 0.474834, acc.: 80.47%] [G loss: 0.598442]\n",
      "epoch:2 step:1922 [D loss: 0.499138, acc.: 82.03%] [G loss: 0.533316]\n",
      "epoch:2 step:1923 [D loss: 0.495156, acc.: 82.03%] [G loss: 0.571780]\n",
      "epoch:2 step:1924 [D loss: 0.424452, acc.: 85.94%] [G loss: 0.643828]\n",
      "epoch:2 step:1925 [D loss: 0.449633, acc.: 84.38%] [G loss: 0.484923]\n",
      "epoch:2 step:1926 [D loss: 0.473893, acc.: 81.25%] [G loss: 0.562570]\n",
      "epoch:2 step:1927 [D loss: 0.439391, acc.: 82.03%] [G loss: 0.692411]\n",
      "epoch:2 step:1928 [D loss: 0.395211, acc.: 88.28%] [G loss: 0.684507]\n",
      "epoch:2 step:1929 [D loss: 0.442547, acc.: 82.81%] [G loss: 0.538718]\n",
      "epoch:2 step:1930 [D loss: 0.524508, acc.: 73.44%] [G loss: 0.573415]\n",
      "epoch:2 step:1931 [D loss: 0.452857, acc.: 85.94%] [G loss: 0.572930]\n",
      "epoch:2 step:1932 [D loss: 0.420224, acc.: 89.06%] [G loss: 0.627465]\n",
      "epoch:2 step:1933 [D loss: 0.386059, acc.: 87.50%] [G loss: 0.798297]\n",
      "epoch:2 step:1934 [D loss: 0.437900, acc.: 85.16%] [G loss: 0.762232]\n",
      "epoch:2 step:1935 [D loss: 0.476971, acc.: 80.47%] [G loss: 0.588128]\n",
      "epoch:2 step:1936 [D loss: 0.496011, acc.: 78.12%] [G loss: 0.580510]\n",
      "epoch:2 step:1937 [D loss: 0.458246, acc.: 80.47%] [G loss: 0.630326]\n",
      "epoch:2 step:1938 [D loss: 0.463846, acc.: 89.06%] [G loss: 0.633739]\n",
      "epoch:2 step:1939 [D loss: 0.474287, acc.: 82.03%] [G loss: 0.538176]\n",
      "epoch:2 step:1940 [D loss: 0.425276, acc.: 85.16%] [G loss: 0.621876]\n",
      "epoch:2 step:1941 [D loss: 0.480591, acc.: 83.59%] [G loss: 0.582056]\n",
      "epoch:2 step:1942 [D loss: 0.494594, acc.: 80.47%] [G loss: 0.536223]\n",
      "epoch:2 step:1943 [D loss: 0.487623, acc.: 79.69%] [G loss: 0.620571]\n",
      "epoch:2 step:1944 [D loss: 0.448190, acc.: 85.94%] [G loss: 0.620167]\n",
      "epoch:2 step:1945 [D loss: 0.411209, acc.: 85.94%] [G loss: 0.760282]\n",
      "epoch:2 step:1946 [D loss: 0.395289, acc.: 86.72%] [G loss: 0.742223]\n",
      "epoch:2 step:1947 [D loss: 0.431736, acc.: 84.38%] [G loss: 0.798214]\n",
      "epoch:2 step:1948 [D loss: 0.440501, acc.: 85.94%] [G loss: 0.607208]\n",
      "epoch:2 step:1949 [D loss: 0.433159, acc.: 82.81%] [G loss: 0.698593]\n",
      "epoch:2 step:1950 [D loss: 0.469319, acc.: 80.47%] [G loss: 0.708515]\n",
      "epoch:2 step:1951 [D loss: 0.333157, acc.: 92.97%] [G loss: 0.932692]\n",
      "epoch:2 step:1952 [D loss: 0.569922, acc.: 71.88%] [G loss: 0.590775]\n",
      "epoch:2 step:1953 [D loss: 0.499151, acc.: 76.56%] [G loss: 0.517889]\n",
      "epoch:2 step:1954 [D loss: 0.520321, acc.: 75.78%] [G loss: 0.455795]\n",
      "epoch:2 step:1955 [D loss: 0.482370, acc.: 78.91%] [G loss: 0.518756]\n",
      "epoch:2 step:1956 [D loss: 0.392792, acc.: 89.06%] [G loss: 0.650098]\n",
      "epoch:2 step:1957 [D loss: 0.470914, acc.: 82.81%] [G loss: 0.667394]\n",
      "epoch:2 step:1958 [D loss: 0.456813, acc.: 82.03%] [G loss: 0.653930]\n",
      "epoch:2 step:1959 [D loss: 0.478054, acc.: 81.25%] [G loss: 0.597433]\n",
      "epoch:2 step:1960 [D loss: 0.455491, acc.: 79.69%] [G loss: 0.596769]\n",
      "epoch:2 step:1961 [D loss: 0.411912, acc.: 85.94%] [G loss: 0.739788]\n",
      "epoch:2 step:1962 [D loss: 0.490297, acc.: 78.91%] [G loss: 0.617726]\n",
      "epoch:2 step:1963 [D loss: 0.488581, acc.: 76.56%] [G loss: 0.639721]\n",
      "epoch:2 step:1964 [D loss: 0.490004, acc.: 78.91%] [G loss: 0.635196]\n",
      "epoch:2 step:1965 [D loss: 0.539744, acc.: 75.78%] [G loss: 0.574099]\n",
      "epoch:2 step:1966 [D loss: 0.530954, acc.: 74.22%] [G loss: 0.509151]\n",
      "epoch:2 step:1967 [D loss: 0.486266, acc.: 79.69%] [G loss: 0.574516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1968 [D loss: 0.523735, acc.: 71.88%] [G loss: 0.663423]\n",
      "epoch:2 step:1969 [D loss: 0.483672, acc.: 77.34%] [G loss: 0.580960]\n",
      "epoch:2 step:1970 [D loss: 0.389279, acc.: 89.06%] [G loss: 0.703907]\n",
      "epoch:2 step:1971 [D loss: 0.510829, acc.: 71.88%] [G loss: 0.652466]\n",
      "epoch:2 step:1972 [D loss: 0.495985, acc.: 78.91%] [G loss: 0.484963]\n",
      "epoch:2 step:1973 [D loss: 0.529516, acc.: 70.31%] [G loss: 0.656294]\n",
      "epoch:2 step:1974 [D loss: 0.446049, acc.: 82.03%] [G loss: 0.676971]\n",
      "epoch:2 step:1975 [D loss: 0.483875, acc.: 81.25%] [G loss: 0.735193]\n",
      "epoch:2 step:1976 [D loss: 0.527379, acc.: 78.12%] [G loss: 0.572560]\n",
      "epoch:2 step:1977 [D loss: 0.439330, acc.: 82.03%] [G loss: 0.723643]\n",
      "epoch:2 step:1978 [D loss: 0.432633, acc.: 84.38%] [G loss: 0.763736]\n",
      "epoch:2 step:1979 [D loss: 0.472306, acc.: 82.81%] [G loss: 0.659544]\n",
      "epoch:2 step:1980 [D loss: 0.522243, acc.: 77.34%] [G loss: 0.629603]\n",
      "epoch:2 step:1981 [D loss: 0.580197, acc.: 68.75%] [G loss: 0.624422]\n",
      "epoch:2 step:1982 [D loss: 0.472395, acc.: 78.91%] [G loss: 0.788462]\n",
      "epoch:2 step:1983 [D loss: 0.501780, acc.: 76.56%] [G loss: 0.592919]\n",
      "epoch:2 step:1984 [D loss: 0.507963, acc.: 75.78%] [G loss: 0.516968]\n",
      "epoch:2 step:1985 [D loss: 0.493887, acc.: 78.12%] [G loss: 0.619369]\n",
      "epoch:2 step:1986 [D loss: 0.460135, acc.: 86.72%] [G loss: 0.645588]\n",
      "epoch:2 step:1987 [D loss: 0.504422, acc.: 78.91%] [G loss: 0.656319]\n",
      "epoch:2 step:1988 [D loss: 0.536534, acc.: 71.88%] [G loss: 0.526027]\n",
      "epoch:2 step:1989 [D loss: 0.509188, acc.: 79.69%] [G loss: 0.499104]\n",
      "epoch:2 step:1990 [D loss: 0.465531, acc.: 82.81%] [G loss: 0.606522]\n",
      "epoch:2 step:1991 [D loss: 0.472304, acc.: 82.03%] [G loss: 0.727004]\n",
      "epoch:2 step:1992 [D loss: 0.430176, acc.: 85.16%] [G loss: 0.663061]\n",
      "epoch:2 step:1993 [D loss: 0.464466, acc.: 80.47%] [G loss: 0.654633]\n",
      "epoch:2 step:1994 [D loss: 0.543195, acc.: 78.12%] [G loss: 0.489478]\n",
      "epoch:2 step:1995 [D loss: 0.528523, acc.: 76.56%] [G loss: 0.449304]\n",
      "epoch:2 step:1996 [D loss: 0.534229, acc.: 74.22%] [G loss: 0.510658]\n",
      "epoch:2 step:1997 [D loss: 0.472641, acc.: 82.81%] [G loss: 0.525464]\n",
      "epoch:2 step:1998 [D loss: 0.492332, acc.: 82.03%] [G loss: 0.444492]\n",
      "epoch:2 step:1999 [D loss: 0.437739, acc.: 85.94%] [G loss: 0.549666]\n",
      "epoch:2 step:2000 [D loss: 0.456912, acc.: 80.47%] [G loss: 0.636576]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 3.639117\n",
      "FID: 111.862297\n",
      "0 = 14.360400360870328\n",
      "1 = 0.16858612863533437\n",
      "2 = 0.986299991607666\n",
      "3 = 0.972599983215332\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.972599983215332\n",
      "7 = 11.575020438909565\n",
      "8 = 0.20052671217454354\n",
      "9 = 0.9616000056266785\n",
      "10 = 0.9376000165939331\n",
      "11 = 0.9855999946594238\n",
      "12 = 0.9848739504814148\n",
      "13 = 0.9376000165939331\n",
      "14 = 3.6391208171844482\n",
      "15 = 7.496689319610596\n",
      "16 = 0.4373641908168793\n",
      "17 = 3.6391165256500244\n",
      "18 = 111.86229705810547\n",
      "epoch:2 step:2001 [D loss: 0.450575, acc.: 86.72%] [G loss: 0.645761]\n",
      "epoch:2 step:2002 [D loss: 0.516467, acc.: 75.78%] [G loss: 0.506133]\n",
      "epoch:2 step:2003 [D loss: 0.484730, acc.: 79.69%] [G loss: 0.483752]\n",
      "epoch:2 step:2004 [D loss: 0.444093, acc.: 84.38%] [G loss: 0.578744]\n",
      "epoch:2 step:2005 [D loss: 0.383011, acc.: 89.84%] [G loss: 0.726450]\n",
      "epoch:2 step:2006 [D loss: 0.472436, acc.: 78.91%] [G loss: 0.575619]\n",
      "epoch:2 step:2007 [D loss: 0.475441, acc.: 78.12%] [G loss: 0.630832]\n",
      "epoch:2 step:2008 [D loss: 0.379710, acc.: 86.72%] [G loss: 0.731253]\n",
      "epoch:2 step:2009 [D loss: 0.405657, acc.: 87.50%] [G loss: 0.749011]\n",
      "epoch:2 step:2010 [D loss: 0.467442, acc.: 81.25%] [G loss: 0.808680]\n",
      "epoch:2 step:2011 [D loss: 0.504439, acc.: 78.12%] [G loss: 0.706262]\n",
      "epoch:2 step:2012 [D loss: 0.455463, acc.: 83.59%] [G loss: 0.564567]\n",
      "epoch:2 step:2013 [D loss: 0.525515, acc.: 75.00%] [G loss: 0.561162]\n",
      "epoch:2 step:2014 [D loss: 0.453562, acc.: 78.91%] [G loss: 0.564613]\n",
      "epoch:2 step:2015 [D loss: 0.434207, acc.: 82.81%] [G loss: 0.744687]\n",
      "epoch:2 step:2016 [D loss: 0.436712, acc.: 78.91%] [G loss: 0.716757]\n",
      "epoch:2 step:2017 [D loss: 0.478845, acc.: 79.69%] [G loss: 0.680396]\n",
      "epoch:2 step:2018 [D loss: 0.422980, acc.: 76.56%] [G loss: 0.776427]\n",
      "epoch:2 step:2019 [D loss: 0.466403, acc.: 81.25%] [G loss: 0.691959]\n",
      "epoch:2 step:2020 [D loss: 0.466512, acc.: 79.69%] [G loss: 0.680294]\n",
      "epoch:2 step:2021 [D loss: 0.477859, acc.: 82.03%] [G loss: 0.565374]\n",
      "epoch:2 step:2022 [D loss: 0.448463, acc.: 86.72%] [G loss: 0.712280]\n",
      "epoch:2 step:2023 [D loss: 0.404858, acc.: 87.50%] [G loss: 0.730050]\n",
      "epoch:2 step:2024 [D loss: 0.533564, acc.: 76.56%] [G loss: 0.679831]\n",
      "epoch:2 step:2025 [D loss: 0.376370, acc.: 87.50%] [G loss: 0.745625]\n",
      "epoch:2 step:2026 [D loss: 0.393240, acc.: 89.06%] [G loss: 0.923733]\n",
      "epoch:2 step:2027 [D loss: 0.490711, acc.: 78.12%] [G loss: 0.652998]\n",
      "epoch:2 step:2028 [D loss: 0.486727, acc.: 81.25%] [G loss: 0.581624]\n",
      "epoch:2 step:2029 [D loss: 0.458268, acc.: 79.69%] [G loss: 0.561897]\n",
      "epoch:2 step:2030 [D loss: 0.448213, acc.: 85.16%] [G loss: 0.731975]\n",
      "epoch:2 step:2031 [D loss: 0.434258, acc.: 85.16%] [G loss: 0.733418]\n",
      "epoch:2 step:2032 [D loss: 0.452628, acc.: 82.81%] [G loss: 0.615928]\n",
      "epoch:2 step:2033 [D loss: 0.452827, acc.: 78.91%] [G loss: 0.603353]\n",
      "epoch:2 step:2034 [D loss: 0.484674, acc.: 76.56%] [G loss: 0.603763]\n",
      "epoch:2 step:2035 [D loss: 0.436776, acc.: 83.59%] [G loss: 0.769935]\n",
      "epoch:2 step:2036 [D loss: 0.409644, acc.: 83.59%] [G loss: 0.928490]\n",
      "epoch:2 step:2037 [D loss: 0.437523, acc.: 83.59%] [G loss: 0.826936]\n",
      "epoch:2 step:2038 [D loss: 0.483576, acc.: 78.91%] [G loss: 0.796096]\n",
      "epoch:2 step:2039 [D loss: 0.434985, acc.: 82.81%] [G loss: 0.823071]\n",
      "epoch:2 step:2040 [D loss: 0.480401, acc.: 77.34%] [G loss: 0.663418]\n",
      "epoch:2 step:2041 [D loss: 0.531280, acc.: 74.22%] [G loss: 0.491445]\n",
      "epoch:2 step:2042 [D loss: 0.489014, acc.: 78.12%] [G loss: 0.577184]\n",
      "epoch:2 step:2043 [D loss: 0.506227, acc.: 79.69%] [G loss: 0.573244]\n",
      "epoch:2 step:2044 [D loss: 0.395444, acc.: 88.28%] [G loss: 0.743286]\n",
      "epoch:2 step:2045 [D loss: 0.439476, acc.: 85.16%] [G loss: 0.646040]\n",
      "epoch:2 step:2046 [D loss: 0.431633, acc.: 82.03%] [G loss: 0.760646]\n",
      "epoch:2 step:2047 [D loss: 0.444953, acc.: 82.03%] [G loss: 0.744265]\n",
      "epoch:2 step:2048 [D loss: 0.455107, acc.: 82.03%] [G loss: 0.662811]\n",
      "epoch:2 step:2049 [D loss: 0.426163, acc.: 86.72%] [G loss: 0.637167]\n",
      "epoch:2 step:2050 [D loss: 0.461785, acc.: 78.12%] [G loss: 0.716111]\n",
      "epoch:2 step:2051 [D loss: 0.431300, acc.: 85.16%] [G loss: 0.630651]\n",
      "epoch:2 step:2052 [D loss: 0.462548, acc.: 82.03%] [G loss: 0.566067]\n",
      "epoch:2 step:2053 [D loss: 0.475012, acc.: 78.12%] [G loss: 0.776762]\n",
      "epoch:2 step:2054 [D loss: 0.430795, acc.: 82.81%] [G loss: 0.734980]\n",
      "epoch:2 step:2055 [D loss: 0.435899, acc.: 83.59%] [G loss: 0.773192]\n",
      "epoch:2 step:2056 [D loss: 0.486357, acc.: 74.22%] [G loss: 0.691293]\n",
      "epoch:2 step:2057 [D loss: 0.425052, acc.: 85.16%] [G loss: 0.759458]\n",
      "epoch:2 step:2058 [D loss: 0.510254, acc.: 75.78%] [G loss: 0.782871]\n",
      "epoch:2 step:2059 [D loss: 0.473882, acc.: 79.69%] [G loss: 0.688035]\n",
      "epoch:2 step:2060 [D loss: 0.475275, acc.: 82.81%] [G loss: 0.614002]\n",
      "epoch:2 step:2061 [D loss: 0.505906, acc.: 76.56%] [G loss: 0.524895]\n",
      "epoch:2 step:2062 [D loss: 0.419709, acc.: 87.50%] [G loss: 0.790271]\n",
      "epoch:2 step:2063 [D loss: 0.445386, acc.: 85.16%] [G loss: 0.640904]\n",
      "epoch:2 step:2064 [D loss: 0.390973, acc.: 86.72%] [G loss: 0.780136]\n",
      "epoch:2 step:2065 [D loss: 0.420624, acc.: 87.50%] [G loss: 0.784876]\n",
      "epoch:2 step:2066 [D loss: 0.478504, acc.: 80.47%] [G loss: 0.653980]\n",
      "epoch:2 step:2067 [D loss: 0.475069, acc.: 84.38%] [G loss: 0.675062]\n",
      "epoch:2 step:2068 [D loss: 0.420061, acc.: 88.28%] [G loss: 0.814985]\n",
      "epoch:2 step:2069 [D loss: 0.441056, acc.: 85.16%] [G loss: 0.728581]\n",
      "epoch:2 step:2070 [D loss: 0.486023, acc.: 79.69%] [G loss: 0.548759]\n",
      "epoch:2 step:2071 [D loss: 0.468298, acc.: 81.25%] [G loss: 0.656352]\n",
      "epoch:2 step:2072 [D loss: 0.462203, acc.: 81.25%] [G loss: 0.632781]\n",
      "epoch:2 step:2073 [D loss: 0.452098, acc.: 82.81%] [G loss: 0.733714]\n",
      "epoch:2 step:2074 [D loss: 0.477846, acc.: 82.81%] [G loss: 0.575512]\n",
      "epoch:2 step:2075 [D loss: 0.455439, acc.: 82.81%] [G loss: 0.732331]\n",
      "epoch:2 step:2076 [D loss: 0.450296, acc.: 81.25%] [G loss: 0.752045]\n",
      "epoch:2 step:2077 [D loss: 0.532913, acc.: 75.78%] [G loss: 0.620753]\n",
      "epoch:2 step:2078 [D loss: 0.448084, acc.: 78.12%] [G loss: 0.674999]\n",
      "epoch:2 step:2079 [D loss: 0.443732, acc.: 83.59%] [G loss: 0.764424]\n",
      "epoch:2 step:2080 [D loss: 0.469435, acc.: 75.78%] [G loss: 0.698825]\n",
      "epoch:2 step:2081 [D loss: 0.442799, acc.: 81.25%] [G loss: 0.626695]\n",
      "epoch:2 step:2082 [D loss: 0.400619, acc.: 84.38%] [G loss: 0.765730]\n",
      "epoch:2 step:2083 [D loss: 0.524444, acc.: 75.78%] [G loss: 0.614557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2084 [D loss: 0.453083, acc.: 81.25%] [G loss: 0.696277]\n",
      "epoch:2 step:2085 [D loss: 0.501185, acc.: 77.34%] [G loss: 0.501053]\n",
      "epoch:2 step:2086 [D loss: 0.479801, acc.: 78.12%] [G loss: 0.556857]\n",
      "epoch:2 step:2087 [D loss: 0.433120, acc.: 82.81%] [G loss: 0.800937]\n",
      "epoch:2 step:2088 [D loss: 0.605347, acc.: 64.06%] [G loss: 0.595439]\n",
      "epoch:2 step:2089 [D loss: 0.532098, acc.: 74.22%] [G loss: 0.631091]\n",
      "epoch:2 step:2090 [D loss: 0.531778, acc.: 74.22%] [G loss: 0.451457]\n",
      "epoch:2 step:2091 [D loss: 0.421408, acc.: 85.16%] [G loss: 0.659988]\n",
      "epoch:2 step:2092 [D loss: 0.491943, acc.: 82.03%] [G loss: 0.602643]\n",
      "epoch:2 step:2093 [D loss: 0.521932, acc.: 79.69%] [G loss: 0.448909]\n",
      "epoch:2 step:2094 [D loss: 0.543961, acc.: 71.88%] [G loss: 0.416759]\n",
      "epoch:2 step:2095 [D loss: 0.391746, acc.: 85.94%] [G loss: 0.657955]\n",
      "epoch:2 step:2096 [D loss: 0.427276, acc.: 82.03%] [G loss: 0.786475]\n",
      "epoch:2 step:2097 [D loss: 0.453056, acc.: 82.81%] [G loss: 0.698393]\n",
      "epoch:2 step:2098 [D loss: 0.483716, acc.: 79.69%] [G loss: 0.660393]\n",
      "epoch:2 step:2099 [D loss: 0.498071, acc.: 82.03%] [G loss: 0.491484]\n",
      "epoch:2 step:2100 [D loss: 0.474075, acc.: 78.91%] [G loss: 0.571277]\n",
      "epoch:2 step:2101 [D loss: 0.483222, acc.: 76.56%] [G loss: 0.626005]\n",
      "epoch:2 step:2102 [D loss: 0.492987, acc.: 82.03%] [G loss: 0.599583]\n",
      "epoch:2 step:2103 [D loss: 0.494212, acc.: 80.47%] [G loss: 0.643529]\n",
      "epoch:2 step:2104 [D loss: 0.404540, acc.: 85.94%] [G loss: 0.825348]\n",
      "epoch:2 step:2105 [D loss: 0.429219, acc.: 82.03%] [G loss: 0.936458]\n",
      "epoch:2 step:2106 [D loss: 0.373560, acc.: 86.72%] [G loss: 1.018843]\n",
      "epoch:2 step:2107 [D loss: 0.483613, acc.: 80.47%] [G loss: 0.686719]\n",
      "epoch:2 step:2108 [D loss: 0.482839, acc.: 76.56%] [G loss: 0.602119]\n",
      "epoch:2 step:2109 [D loss: 0.447053, acc.: 85.94%] [G loss: 0.673156]\n",
      "epoch:2 step:2110 [D loss: 0.459310, acc.: 81.25%] [G loss: 0.689415]\n",
      "epoch:2 step:2111 [D loss: 0.487129, acc.: 81.25%] [G loss: 0.645861]\n",
      "epoch:2 step:2112 [D loss: 0.461410, acc.: 78.91%] [G loss: 0.547753]\n",
      "epoch:2 step:2113 [D loss: 0.496052, acc.: 77.34%] [G loss: 0.688673]\n",
      "epoch:2 step:2114 [D loss: 0.436560, acc.: 83.59%] [G loss: 0.691151]\n",
      "epoch:2 step:2115 [D loss: 0.450444, acc.: 85.16%] [G loss: 0.693631]\n",
      "epoch:2 step:2116 [D loss: 0.437044, acc.: 85.16%] [G loss: 0.695722]\n",
      "epoch:2 step:2117 [D loss: 0.458979, acc.: 82.03%] [G loss: 0.575892]\n",
      "epoch:2 step:2118 [D loss: 0.440244, acc.: 82.81%] [G loss: 0.639144]\n",
      "epoch:2 step:2119 [D loss: 0.432493, acc.: 84.38%] [G loss: 0.637070]\n",
      "epoch:2 step:2120 [D loss: 0.483013, acc.: 80.47%] [G loss: 0.656115]\n",
      "epoch:2 step:2121 [D loss: 0.553797, acc.: 75.78%] [G loss: 0.561265]\n",
      "epoch:2 step:2122 [D loss: 0.526089, acc.: 78.12%] [G loss: 0.509943]\n",
      "epoch:2 step:2123 [D loss: 0.485643, acc.: 81.25%] [G loss: 0.586584]\n",
      "epoch:2 step:2124 [D loss: 0.533773, acc.: 75.00%] [G loss: 0.560287]\n",
      "epoch:2 step:2125 [D loss: 0.464325, acc.: 83.59%] [G loss: 0.641662]\n",
      "epoch:2 step:2126 [D loss: 0.477815, acc.: 78.91%] [G loss: 0.629436]\n",
      "epoch:2 step:2127 [D loss: 0.412820, acc.: 88.28%] [G loss: 0.667683]\n",
      "epoch:2 step:2128 [D loss: 0.496607, acc.: 75.00%] [G loss: 0.545868]\n",
      "epoch:2 step:2129 [D loss: 0.462997, acc.: 82.81%] [G loss: 0.619720]\n",
      "epoch:2 step:2130 [D loss: 0.470206, acc.: 78.12%] [G loss: 0.682228]\n",
      "epoch:2 step:2131 [D loss: 0.468985, acc.: 78.91%] [G loss: 0.808455]\n",
      "epoch:2 step:2132 [D loss: 0.429519, acc.: 85.16%] [G loss: 0.782534]\n",
      "epoch:2 step:2133 [D loss: 0.441547, acc.: 78.91%] [G loss: 0.682941]\n",
      "epoch:2 step:2134 [D loss: 0.447153, acc.: 80.47%] [G loss: 0.617363]\n",
      "epoch:2 step:2135 [D loss: 0.437330, acc.: 83.59%] [G loss: 0.644050]\n",
      "epoch:2 step:2136 [D loss: 0.460004, acc.: 78.91%] [G loss: 0.761048]\n",
      "epoch:2 step:2137 [D loss: 0.555397, acc.: 75.00%] [G loss: 0.672495]\n",
      "epoch:2 step:2138 [D loss: 0.498047, acc.: 75.78%] [G loss: 0.622035]\n",
      "epoch:2 step:2139 [D loss: 0.556046, acc.: 71.09%] [G loss: 0.549376]\n",
      "epoch:2 step:2140 [D loss: 0.448651, acc.: 85.16%] [G loss: 0.619478]\n",
      "epoch:2 step:2141 [D loss: 0.523313, acc.: 79.69%] [G loss: 0.543281]\n",
      "epoch:2 step:2142 [D loss: 0.506101, acc.: 78.91%] [G loss: 0.485770]\n",
      "epoch:2 step:2143 [D loss: 0.516649, acc.: 75.00%] [G loss: 0.473543]\n",
      "epoch:2 step:2144 [D loss: 0.496490, acc.: 79.69%] [G loss: 0.479124]\n",
      "epoch:2 step:2145 [D loss: 0.450282, acc.: 79.69%] [G loss: 0.596968]\n",
      "epoch:2 step:2146 [D loss: 0.497523, acc.: 80.47%] [G loss: 0.535463]\n",
      "epoch:2 step:2147 [D loss: 0.429126, acc.: 85.16%] [G loss: 0.642524]\n",
      "epoch:2 step:2148 [D loss: 0.522650, acc.: 71.09%] [G loss: 0.507036]\n",
      "epoch:2 step:2149 [D loss: 0.531748, acc.: 78.12%] [G loss: 0.514845]\n",
      "epoch:2 step:2150 [D loss: 0.532416, acc.: 74.22%] [G loss: 0.668835]\n",
      "epoch:2 step:2151 [D loss: 0.524488, acc.: 78.12%] [G loss: 0.509837]\n",
      "epoch:2 step:2152 [D loss: 0.486375, acc.: 77.34%] [G loss: 0.565177]\n",
      "epoch:2 step:2153 [D loss: 0.416879, acc.: 85.94%] [G loss: 0.640221]\n",
      "epoch:2 step:2154 [D loss: 0.496090, acc.: 77.34%] [G loss: 0.572395]\n",
      "epoch:2 step:2155 [D loss: 0.405965, acc.: 84.38%] [G loss: 0.732918]\n",
      "epoch:2 step:2156 [D loss: 0.527542, acc.: 75.78%] [G loss: 0.492156]\n",
      "epoch:2 step:2157 [D loss: 0.442028, acc.: 85.16%] [G loss: 0.541435]\n",
      "epoch:2 step:2158 [D loss: 0.450908, acc.: 81.25%] [G loss: 0.562374]\n",
      "epoch:2 step:2159 [D loss: 0.387556, acc.: 87.50%] [G loss: 0.623608]\n",
      "epoch:2 step:2160 [D loss: 0.441777, acc.: 85.16%] [G loss: 0.683133]\n",
      "epoch:2 step:2161 [D loss: 0.542797, acc.: 69.53%] [G loss: 0.420407]\n",
      "epoch:2 step:2162 [D loss: 0.522828, acc.: 73.44%] [G loss: 0.556172]\n",
      "epoch:2 step:2163 [D loss: 0.478288, acc.: 79.69%] [G loss: 0.574204]\n",
      "epoch:2 step:2164 [D loss: 0.463129, acc.: 84.38%] [G loss: 0.549346]\n",
      "epoch:2 step:2165 [D loss: 0.532949, acc.: 73.44%] [G loss: 0.602322]\n",
      "epoch:2 step:2166 [D loss: 0.528583, acc.: 76.56%] [G loss: 0.614991]\n",
      "epoch:2 step:2167 [D loss: 0.458668, acc.: 78.91%] [G loss: 0.497307]\n",
      "epoch:2 step:2168 [D loss: 0.515933, acc.: 74.22%] [G loss: 0.523269]\n",
      "epoch:2 step:2169 [D loss: 0.500416, acc.: 79.69%] [G loss: 0.520175]\n",
      "epoch:2 step:2170 [D loss: 0.471045, acc.: 85.94%] [G loss: 0.518174]\n",
      "epoch:2 step:2171 [D loss: 0.498600, acc.: 79.69%] [G loss: 0.511933]\n",
      "epoch:2 step:2172 [D loss: 0.476250, acc.: 82.03%] [G loss: 0.502659]\n",
      "epoch:2 step:2173 [D loss: 0.529146, acc.: 73.44%] [G loss: 0.456870]\n",
      "epoch:2 step:2174 [D loss: 0.442768, acc.: 83.59%] [G loss: 0.609716]\n",
      "epoch:2 step:2175 [D loss: 0.540712, acc.: 78.12%] [G loss: 0.549074]\n",
      "epoch:2 step:2176 [D loss: 0.462666, acc.: 78.91%] [G loss: 0.542023]\n",
      "epoch:2 step:2177 [D loss: 0.514161, acc.: 77.34%] [G loss: 0.500384]\n",
      "epoch:2 step:2178 [D loss: 0.431139, acc.: 82.81%] [G loss: 0.571358]\n",
      "epoch:2 step:2179 [D loss: 0.477102, acc.: 78.91%] [G loss: 0.696363]\n",
      "epoch:2 step:2180 [D loss: 0.594459, acc.: 70.31%] [G loss: 0.579937]\n",
      "epoch:2 step:2181 [D loss: 0.448549, acc.: 80.47%] [G loss: 0.663104]\n",
      "epoch:2 step:2182 [D loss: 0.492112, acc.: 73.44%] [G loss: 0.595720]\n",
      "epoch:2 step:2183 [D loss: 0.426367, acc.: 83.59%] [G loss: 0.686412]\n",
      "epoch:2 step:2184 [D loss: 0.460236, acc.: 82.03%] [G loss: 0.707370]\n",
      "epoch:2 step:2185 [D loss: 0.479872, acc.: 78.91%] [G loss: 0.615212]\n",
      "epoch:2 step:2186 [D loss: 0.413801, acc.: 82.03%] [G loss: 0.856774]\n",
      "epoch:2 step:2187 [D loss: 0.447991, acc.: 82.81%] [G loss: 0.829979]\n",
      "epoch:2 step:2188 [D loss: 0.421686, acc.: 84.38%] [G loss: 1.043919]\n",
      "epoch:2 step:2189 [D loss: 0.469510, acc.: 82.81%] [G loss: 0.924327]\n",
      "epoch:2 step:2190 [D loss: 0.617651, acc.: 68.75%] [G loss: 0.448706]\n",
      "epoch:2 step:2191 [D loss: 0.469713, acc.: 78.12%] [G loss: 0.567342]\n",
      "epoch:2 step:2192 [D loss: 0.497605, acc.: 75.78%] [G loss: 0.534959]\n",
      "epoch:2 step:2193 [D loss: 0.528172, acc.: 76.56%] [G loss: 0.548068]\n",
      "epoch:2 step:2194 [D loss: 0.510583, acc.: 73.44%] [G loss: 0.592084]\n",
      "epoch:2 step:2195 [D loss: 0.498318, acc.: 78.91%] [G loss: 0.591881]\n",
      "epoch:2 step:2196 [D loss: 0.507691, acc.: 75.00%] [G loss: 0.564593]\n",
      "epoch:2 step:2197 [D loss: 0.483025, acc.: 75.78%] [G loss: 0.664419]\n",
      "epoch:2 step:2198 [D loss: 0.474863, acc.: 84.38%] [G loss: 0.578101]\n",
      "epoch:2 step:2199 [D loss: 0.486003, acc.: 77.34%] [G loss: 0.635091]\n",
      "epoch:2 step:2200 [D loss: 0.478148, acc.: 78.91%] [G loss: 0.591613]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.361175\n",
      "FID: 99.383362\n",
      "0 = 13.853596839714022\n",
      "1 = 0.14938015023135467\n",
      "2 = 0.9779999852180481\n",
      "3 = 0.9559999704360962\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9559999704360962\n",
      "7 = 10.98428174962999\n",
      "8 = 0.19404526638596292\n",
      "9 = 0.9466000199317932\n",
      "10 = 0.9151999950408936\n",
      "11 = 0.9779999852180481\n",
      "12 = 0.9765258431434631\n",
      "13 = 0.9151999950408936\n",
      "14 = 4.36118221282959\n",
      "15 = 7.623469829559326\n",
      "16 = 0.39358195662498474\n",
      "17 = 4.361174583435059\n",
      "18 = 99.38336181640625\n",
      "epoch:2 step:2201 [D loss: 0.496893, acc.: 78.12%] [G loss: 0.702138]\n",
      "epoch:2 step:2202 [D loss: 0.475890, acc.: 77.34%] [G loss: 0.561985]\n",
      "epoch:2 step:2203 [D loss: 0.505784, acc.: 78.12%] [G loss: 0.607100]\n",
      "epoch:2 step:2204 [D loss: 0.510454, acc.: 73.44%] [G loss: 0.566874]\n",
      "epoch:2 step:2205 [D loss: 0.453254, acc.: 82.03%] [G loss: 0.513078]\n",
      "epoch:2 step:2206 [D loss: 0.472233, acc.: 77.34%] [G loss: 0.554139]\n",
      "epoch:2 step:2207 [D loss: 0.442259, acc.: 85.94%] [G loss: 0.624370]\n",
      "epoch:2 step:2208 [D loss: 0.486201, acc.: 80.47%] [G loss: 0.615598]\n",
      "epoch:2 step:2209 [D loss: 0.452188, acc.: 82.03%] [G loss: 0.679354]\n",
      "epoch:2 step:2210 [D loss: 0.494106, acc.: 78.12%] [G loss: 0.663936]\n",
      "epoch:2 step:2211 [D loss: 0.507143, acc.: 75.78%] [G loss: 0.633995]\n",
      "epoch:2 step:2212 [D loss: 0.470419, acc.: 80.47%] [G loss: 0.681594]\n",
      "epoch:2 step:2213 [D loss: 0.521510, acc.: 72.66%] [G loss: 0.546677]\n",
      "epoch:2 step:2214 [D loss: 0.474466, acc.: 81.25%] [G loss: 0.600330]\n",
      "epoch:2 step:2215 [D loss: 0.533457, acc.: 74.22%] [G loss: 0.538259]\n",
      "epoch:2 step:2216 [D loss: 0.475833, acc.: 79.69%] [G loss: 0.700747]\n",
      "epoch:2 step:2217 [D loss: 0.388672, acc.: 82.03%] [G loss: 0.833779]\n",
      "epoch:2 step:2218 [D loss: 0.414961, acc.: 82.03%] [G loss: 0.840156]\n",
      "epoch:2 step:2219 [D loss: 0.541536, acc.: 74.22%] [G loss: 0.946006]\n",
      "epoch:2 step:2220 [D loss: 0.435976, acc.: 81.25%] [G loss: 0.789406]\n",
      "epoch:2 step:2221 [D loss: 0.452401, acc.: 80.47%] [G loss: 0.968608]\n",
      "epoch:2 step:2222 [D loss: 0.571408, acc.: 66.41%] [G loss: 0.505234]\n",
      "epoch:2 step:2223 [D loss: 0.630273, acc.: 67.19%] [G loss: 0.435570]\n",
      "epoch:2 step:2224 [D loss: 0.423098, acc.: 83.59%] [G loss: 0.652702]\n",
      "epoch:2 step:2225 [D loss: 0.464001, acc.: 82.81%] [G loss: 0.722944]\n",
      "epoch:2 step:2226 [D loss: 0.520426, acc.: 75.78%] [G loss: 0.553698]\n",
      "epoch:2 step:2227 [D loss: 0.505863, acc.: 73.44%] [G loss: 0.565796]\n",
      "epoch:2 step:2228 [D loss: 0.392271, acc.: 87.50%] [G loss: 0.694398]\n",
      "epoch:2 step:2229 [D loss: 0.501238, acc.: 77.34%] [G loss: 0.546956]\n",
      "epoch:2 step:2230 [D loss: 0.503800, acc.: 75.78%] [G loss: 0.628696]\n",
      "epoch:2 step:2231 [D loss: 0.466473, acc.: 75.00%] [G loss: 0.574443]\n",
      "epoch:2 step:2232 [D loss: 0.433594, acc.: 80.47%] [G loss: 0.755019]\n",
      "epoch:2 step:2233 [D loss: 0.466796, acc.: 80.47%] [G loss: 0.784897]\n",
      "epoch:2 step:2234 [D loss: 0.479286, acc.: 76.56%] [G loss: 0.672989]\n",
      "epoch:2 step:2235 [D loss: 0.491572, acc.: 78.91%] [G loss: 0.581683]\n",
      "epoch:2 step:2236 [D loss: 0.467453, acc.: 82.03%] [G loss: 0.691679]\n",
      "epoch:2 step:2237 [D loss: 0.489452, acc.: 78.12%] [G loss: 0.643995]\n",
      "epoch:2 step:2238 [D loss: 0.461032, acc.: 80.47%] [G loss: 0.639571]\n",
      "epoch:2 step:2239 [D loss: 0.479526, acc.: 78.91%] [G loss: 0.581013]\n",
      "epoch:2 step:2240 [D loss: 0.478852, acc.: 75.78%] [G loss: 0.638865]\n",
      "epoch:2 step:2241 [D loss: 0.531430, acc.: 75.00%] [G loss: 0.573234]\n",
      "epoch:2 step:2242 [D loss: 0.465473, acc.: 80.47%] [G loss: 0.560743]\n",
      "epoch:2 step:2243 [D loss: 0.470392, acc.: 84.38%] [G loss: 0.646867]\n",
      "epoch:2 step:2244 [D loss: 0.530859, acc.: 76.56%] [G loss: 0.506778]\n",
      "epoch:2 step:2245 [D loss: 0.478969, acc.: 82.81%] [G loss: 0.704529]\n",
      "epoch:2 step:2246 [D loss: 0.436498, acc.: 84.38%] [G loss: 0.738106]\n",
      "epoch:2 step:2247 [D loss: 0.553604, acc.: 70.31%] [G loss: 0.625749]\n",
      "epoch:2 step:2248 [D loss: 0.507129, acc.: 77.34%] [G loss: 0.693467]\n",
      "epoch:2 step:2249 [D loss: 0.548093, acc.: 71.09%] [G loss: 0.467758]\n",
      "epoch:2 step:2250 [D loss: 0.555816, acc.: 75.00%] [G loss: 0.493293]\n",
      "epoch:2 step:2251 [D loss: 0.461902, acc.: 80.47%] [G loss: 0.591981]\n",
      "epoch:2 step:2252 [D loss: 0.419421, acc.: 84.38%] [G loss: 0.759349]\n",
      "epoch:2 step:2253 [D loss: 0.496605, acc.: 78.91%] [G loss: 0.648801]\n",
      "epoch:2 step:2254 [D loss: 0.499519, acc.: 77.34%] [G loss: 0.604049]\n",
      "epoch:2 step:2255 [D loss: 0.395545, acc.: 85.94%] [G loss: 0.728772]\n",
      "epoch:2 step:2256 [D loss: 0.536881, acc.: 72.66%] [G loss: 0.575708]\n",
      "epoch:2 step:2257 [D loss: 0.513557, acc.: 70.31%] [G loss: 0.569759]\n",
      "epoch:2 step:2258 [D loss: 0.509104, acc.: 75.78%] [G loss: 0.558384]\n",
      "epoch:2 step:2259 [D loss: 0.560210, acc.: 70.31%] [G loss: 0.578642]\n",
      "epoch:2 step:2260 [D loss: 0.531487, acc.: 69.53%] [G loss: 0.581941]\n",
      "epoch:2 step:2261 [D loss: 0.479499, acc.: 77.34%] [G loss: 0.746140]\n",
      "epoch:2 step:2262 [D loss: 0.463160, acc.: 77.34%] [G loss: 0.648695]\n",
      "epoch:2 step:2263 [D loss: 0.466094, acc.: 78.12%] [G loss: 0.635063]\n",
      "epoch:2 step:2264 [D loss: 0.510865, acc.: 73.44%] [G loss: 0.556613]\n",
      "epoch:2 step:2265 [D loss: 0.525318, acc.: 78.12%] [G loss: 0.586248]\n",
      "epoch:2 step:2266 [D loss: 0.408033, acc.: 83.59%] [G loss: 0.693199]\n",
      "epoch:2 step:2267 [D loss: 0.526579, acc.: 73.44%] [G loss: 0.496962]\n",
      "epoch:2 step:2268 [D loss: 0.509740, acc.: 75.78%] [G loss: 0.577215]\n",
      "epoch:2 step:2269 [D loss: 0.480233, acc.: 78.12%] [G loss: 0.668143]\n",
      "epoch:2 step:2270 [D loss: 0.536667, acc.: 72.66%] [G loss: 0.642908]\n",
      "epoch:2 step:2271 [D loss: 0.413011, acc.: 83.59%] [G loss: 0.826487]\n",
      "epoch:2 step:2272 [D loss: 0.375540, acc.: 86.72%] [G loss: 0.786829]\n",
      "epoch:2 step:2273 [D loss: 0.446040, acc.: 80.47%] [G loss: 0.919844]\n",
      "epoch:2 step:2274 [D loss: 0.587161, acc.: 65.62%] [G loss: 0.663018]\n",
      "epoch:2 step:2275 [D loss: 0.508053, acc.: 76.56%] [G loss: 0.577430]\n",
      "epoch:2 step:2276 [D loss: 0.392108, acc.: 89.06%] [G loss: 0.616421]\n",
      "epoch:2 step:2277 [D loss: 0.470821, acc.: 79.69%] [G loss: 0.789138]\n",
      "epoch:2 step:2278 [D loss: 0.559867, acc.: 69.53%] [G loss: 0.607724]\n",
      "epoch:2 step:2279 [D loss: 0.460779, acc.: 82.03%] [G loss: 0.666320]\n",
      "epoch:2 step:2280 [D loss: 0.524193, acc.: 76.56%] [G loss: 0.572617]\n",
      "epoch:2 step:2281 [D loss: 0.453504, acc.: 75.00%] [G loss: 0.610114]\n",
      "epoch:2 step:2282 [D loss: 0.478916, acc.: 76.56%] [G loss: 0.673169]\n",
      "epoch:2 step:2283 [D loss: 0.487273, acc.: 74.22%] [G loss: 0.619074]\n",
      "epoch:2 step:2284 [D loss: 0.512250, acc.: 76.56%] [G loss: 0.470151]\n",
      "epoch:2 step:2285 [D loss: 0.455568, acc.: 88.28%] [G loss: 0.512920]\n",
      "epoch:2 step:2286 [D loss: 0.566864, acc.: 68.75%] [G loss: 0.490293]\n",
      "epoch:2 step:2287 [D loss: 0.507130, acc.: 77.34%] [G loss: 0.548463]\n",
      "epoch:2 step:2288 [D loss: 0.455409, acc.: 81.25%] [G loss: 0.606231]\n",
      "epoch:2 step:2289 [D loss: 0.517828, acc.: 75.78%] [G loss: 0.520149]\n",
      "epoch:2 step:2290 [D loss: 0.506196, acc.: 71.09%] [G loss: 0.582562]\n",
      "epoch:2 step:2291 [D loss: 0.546477, acc.: 72.66%] [G loss: 0.631708]\n",
      "epoch:2 step:2292 [D loss: 0.529798, acc.: 75.78%] [G loss: 0.457745]\n",
      "epoch:2 step:2293 [D loss: 0.416958, acc.: 85.94%] [G loss: 0.634707]\n",
      "epoch:2 step:2294 [D loss: 0.473876, acc.: 79.69%] [G loss: 0.639417]\n",
      "epoch:2 step:2295 [D loss: 0.500363, acc.: 74.22%] [G loss: 0.538664]\n",
      "epoch:2 step:2296 [D loss: 0.507290, acc.: 72.66%] [G loss: 0.633279]\n",
      "epoch:2 step:2297 [D loss: 0.489634, acc.: 76.56%] [G loss: 0.747710]\n",
      "epoch:2 step:2298 [D loss: 0.526385, acc.: 75.78%] [G loss: 0.675773]\n",
      "epoch:2 step:2299 [D loss: 0.505538, acc.: 76.56%] [G loss: 0.580509]\n",
      "epoch:2 step:2300 [D loss: 0.475384, acc.: 78.91%] [G loss: 0.671168]\n",
      "epoch:2 step:2301 [D loss: 0.478392, acc.: 78.91%] [G loss: 0.714355]\n",
      "epoch:2 step:2302 [D loss: 0.453928, acc.: 79.69%] [G loss: 0.666479]\n",
      "epoch:2 step:2303 [D loss: 0.452221, acc.: 83.59%] [G loss: 0.753030]\n",
      "epoch:2 step:2304 [D loss: 0.471750, acc.: 76.56%] [G loss: 0.634204]\n",
      "epoch:2 step:2305 [D loss: 0.491307, acc.: 74.22%] [G loss: 0.707676]\n",
      "epoch:2 step:2306 [D loss: 0.466722, acc.: 81.25%] [G loss: 0.539629]\n",
      "epoch:2 step:2307 [D loss: 0.509284, acc.: 79.69%] [G loss: 0.527500]\n",
      "epoch:2 step:2308 [D loss: 0.502410, acc.: 77.34%] [G loss: 0.554447]\n",
      "epoch:2 step:2309 [D loss: 0.508157, acc.: 78.91%] [G loss: 0.583683]\n",
      "epoch:2 step:2310 [D loss: 0.429458, acc.: 84.38%] [G loss: 0.733954]\n",
      "epoch:2 step:2311 [D loss: 0.517604, acc.: 74.22%] [G loss: 0.512165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2312 [D loss: 0.502999, acc.: 74.22%] [G loss: 0.510482]\n",
      "epoch:2 step:2313 [D loss: 0.475147, acc.: 78.12%] [G loss: 0.626108]\n",
      "epoch:2 step:2314 [D loss: 0.468387, acc.: 78.12%] [G loss: 0.725622]\n",
      "epoch:2 step:2315 [D loss: 0.480078, acc.: 78.91%] [G loss: 0.611298]\n",
      "epoch:2 step:2316 [D loss: 0.522108, acc.: 74.22%] [G loss: 0.539798]\n",
      "epoch:2 step:2317 [D loss: 0.485506, acc.: 75.00%] [G loss: 0.558550]\n",
      "epoch:2 step:2318 [D loss: 0.476632, acc.: 78.12%] [G loss: 0.562806]\n",
      "epoch:2 step:2319 [D loss: 0.450983, acc.: 81.25%] [G loss: 0.617288]\n",
      "epoch:2 step:2320 [D loss: 0.427147, acc.: 87.50%] [G loss: 0.641707]\n",
      "epoch:2 step:2321 [D loss: 0.439827, acc.: 83.59%] [G loss: 0.565400]\n",
      "epoch:2 step:2322 [D loss: 0.483648, acc.: 79.69%] [G loss: 0.596107]\n",
      "epoch:2 step:2323 [D loss: 0.488008, acc.: 75.78%] [G loss: 0.691966]\n",
      "epoch:2 step:2324 [D loss: 0.434135, acc.: 79.69%] [G loss: 0.646829]\n",
      "epoch:2 step:2325 [D loss: 0.412569, acc.: 85.16%] [G loss: 0.815522]\n",
      "epoch:2 step:2326 [D loss: 0.451958, acc.: 82.03%] [G loss: 0.738205]\n",
      "epoch:2 step:2327 [D loss: 0.519287, acc.: 71.88%] [G loss: 0.614206]\n",
      "epoch:2 step:2328 [D loss: 0.512411, acc.: 79.69%] [G loss: 0.484479]\n",
      "epoch:2 step:2329 [D loss: 0.546537, acc.: 71.88%] [G loss: 0.454698]\n",
      "epoch:2 step:2330 [D loss: 0.559418, acc.: 68.75%] [G loss: 0.479223]\n",
      "epoch:2 step:2331 [D loss: 0.447204, acc.: 82.81%] [G loss: 0.559075]\n",
      "epoch:2 step:2332 [D loss: 0.456821, acc.: 79.69%] [G loss: 0.573676]\n",
      "epoch:2 step:2333 [D loss: 0.514827, acc.: 75.00%] [G loss: 0.611504]\n",
      "epoch:2 step:2334 [D loss: 0.424658, acc.: 84.38%] [G loss: 0.627335]\n",
      "epoch:2 step:2335 [D loss: 0.456902, acc.: 78.12%] [G loss: 0.739609]\n",
      "epoch:2 step:2336 [D loss: 0.464268, acc.: 84.38%] [G loss: 0.769382]\n",
      "epoch:2 step:2337 [D loss: 0.539449, acc.: 71.88%] [G loss: 0.540973]\n",
      "epoch:2 step:2338 [D loss: 0.502906, acc.: 75.00%] [G loss: 0.533666]\n",
      "epoch:2 step:2339 [D loss: 0.561790, acc.: 71.88%] [G loss: 0.446448]\n",
      "epoch:2 step:2340 [D loss: 0.507070, acc.: 71.88%] [G loss: 0.556584]\n",
      "epoch:2 step:2341 [D loss: 0.457464, acc.: 83.59%] [G loss: 0.527399]\n",
      "epoch:2 step:2342 [D loss: 0.508098, acc.: 76.56%] [G loss: 0.628198]\n",
      "epoch:2 step:2343 [D loss: 0.459852, acc.: 82.03%] [G loss: 0.709030]\n",
      "epoch:2 step:2344 [D loss: 0.544449, acc.: 74.22%] [G loss: 0.472205]\n",
      "epoch:2 step:2345 [D loss: 0.440886, acc.: 81.25%] [G loss: 0.644785]\n",
      "epoch:2 step:2346 [D loss: 0.392361, acc.: 89.84%] [G loss: 0.754566]\n",
      "epoch:2 step:2347 [D loss: 0.557878, acc.: 76.56%] [G loss: 0.612084]\n",
      "epoch:2 step:2348 [D loss: 0.424964, acc.: 79.69%] [G loss: 0.799231]\n",
      "epoch:2 step:2349 [D loss: 0.496125, acc.: 75.00%] [G loss: 0.777173]\n",
      "epoch:2 step:2350 [D loss: 0.505135, acc.: 80.47%] [G loss: 0.624549]\n",
      "epoch:2 step:2351 [D loss: 0.642303, acc.: 67.19%] [G loss: 0.401302]\n",
      "epoch:2 step:2352 [D loss: 0.513912, acc.: 77.34%] [G loss: 0.419586]\n",
      "epoch:2 step:2353 [D loss: 0.481432, acc.: 80.47%] [G loss: 0.498273]\n",
      "epoch:2 step:2354 [D loss: 0.473048, acc.: 82.03%] [G loss: 0.541320]\n",
      "epoch:2 step:2355 [D loss: 0.509384, acc.: 78.91%] [G loss: 0.501951]\n",
      "epoch:2 step:2356 [D loss: 0.561173, acc.: 70.31%] [G loss: 0.445681]\n",
      "epoch:2 step:2357 [D loss: 0.542865, acc.: 71.09%] [G loss: 0.423010]\n",
      "epoch:2 step:2358 [D loss: 0.471412, acc.: 79.69%] [G loss: 0.561033]\n",
      "epoch:2 step:2359 [D loss: 0.482254, acc.: 79.69%] [G loss: 0.536920]\n",
      "epoch:2 step:2360 [D loss: 0.484492, acc.: 76.56%] [G loss: 0.566859]\n",
      "epoch:2 step:2361 [D loss: 0.397722, acc.: 89.06%] [G loss: 0.629427]\n",
      "epoch:2 step:2362 [D loss: 0.471376, acc.: 79.69%] [G loss: 0.579954]\n",
      "epoch:2 step:2363 [D loss: 0.550517, acc.: 75.78%] [G loss: 0.526349]\n",
      "epoch:2 step:2364 [D loss: 0.505748, acc.: 75.78%] [G loss: 0.569240]\n",
      "epoch:2 step:2365 [D loss: 0.483068, acc.: 78.91%] [G loss: 0.577610]\n",
      "epoch:2 step:2366 [D loss: 0.496977, acc.: 78.12%] [G loss: 0.578514]\n",
      "epoch:2 step:2367 [D loss: 0.546250, acc.: 72.66%] [G loss: 0.519071]\n",
      "epoch:2 step:2368 [D loss: 0.499301, acc.: 72.66%] [G loss: 0.563446]\n",
      "epoch:2 step:2369 [D loss: 0.531898, acc.: 75.78%] [G loss: 0.595111]\n",
      "epoch:2 step:2370 [D loss: 0.582163, acc.: 68.75%] [G loss: 0.388303]\n",
      "epoch:2 step:2371 [D loss: 0.450924, acc.: 78.91%] [G loss: 0.666353]\n",
      "epoch:2 step:2372 [D loss: 0.397733, acc.: 85.94%] [G loss: 0.710808]\n",
      "epoch:2 step:2373 [D loss: 0.423627, acc.: 85.16%] [G loss: 0.880066]\n",
      "epoch:2 step:2374 [D loss: 0.647875, acc.: 65.62%] [G loss: 0.429709]\n",
      "epoch:2 step:2375 [D loss: 0.635522, acc.: 64.06%] [G loss: 0.296341]\n",
      "epoch:2 step:2376 [D loss: 0.491203, acc.: 80.47%] [G loss: 0.392422]\n",
      "epoch:2 step:2377 [D loss: 0.421805, acc.: 84.38%] [G loss: 0.471163]\n",
      "epoch:2 step:2378 [D loss: 0.487371, acc.: 81.25%] [G loss: 0.516262]\n",
      "epoch:2 step:2379 [D loss: 0.499276, acc.: 76.56%] [G loss: 0.526561]\n",
      "epoch:2 step:2380 [D loss: 0.430451, acc.: 84.38%] [G loss: 0.498481]\n",
      "epoch:2 step:2381 [D loss: 0.440304, acc.: 85.16%] [G loss: 0.636436]\n",
      "epoch:2 step:2382 [D loss: 0.395327, acc.: 88.28%] [G loss: 0.776227]\n",
      "epoch:2 step:2383 [D loss: 0.502371, acc.: 72.66%] [G loss: 0.553707]\n",
      "epoch:2 step:2384 [D loss: 0.478735, acc.: 78.91%] [G loss: 0.602563]\n",
      "epoch:2 step:2385 [D loss: 0.523605, acc.: 71.88%] [G loss: 0.554834]\n",
      "epoch:2 step:2386 [D loss: 0.470629, acc.: 81.25%] [G loss: 0.581427]\n",
      "epoch:2 step:2387 [D loss: 0.461290, acc.: 80.47%] [G loss: 0.566582]\n",
      "epoch:2 step:2388 [D loss: 0.406953, acc.: 83.59%] [G loss: 0.608533]\n",
      "epoch:2 step:2389 [D loss: 0.452833, acc.: 79.69%] [G loss: 0.691680]\n",
      "epoch:2 step:2390 [D loss: 0.445783, acc.: 86.72%] [G loss: 0.682656]\n",
      "epoch:2 step:2391 [D loss: 0.541087, acc.: 72.66%] [G loss: 0.535043]\n",
      "epoch:2 step:2392 [D loss: 0.407210, acc.: 87.50%] [G loss: 0.565891]\n",
      "epoch:2 step:2393 [D loss: 0.481443, acc.: 77.34%] [G loss: 0.618653]\n",
      "epoch:2 step:2394 [D loss: 0.394140, acc.: 87.50%] [G loss: 0.710376]\n",
      "epoch:2 step:2395 [D loss: 0.448949, acc.: 82.81%] [G loss: 0.657120]\n",
      "epoch:2 step:2396 [D loss: 0.485039, acc.: 78.12%] [G loss: 0.616625]\n",
      "epoch:2 step:2397 [D loss: 0.491308, acc.: 72.66%] [G loss: 0.693357]\n",
      "epoch:2 step:2398 [D loss: 0.519953, acc.: 72.66%] [G loss: 0.615780]\n",
      "epoch:2 step:2399 [D loss: 0.501345, acc.: 80.47%] [G loss: 0.631953]\n",
      "epoch:2 step:2400 [D loss: 0.429305, acc.: 85.16%] [G loss: 0.570739]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.395320\n",
      "FID: 91.213989\n",
      "0 = 13.742203829765248\n",
      "1 = 0.14078766633546952\n",
      "2 = 0.9803000092506409\n",
      "3 = 0.9606000185012817\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9606000185012817\n",
      "7 = 10.76220603132248\n",
      "8 = 0.1854623702509859\n",
      "9 = 0.9394000172615051\n",
      "10 = 0.9064000248908997\n",
      "11 = 0.9724000096321106\n",
      "12 = 0.970449686050415\n",
      "13 = 0.9064000248908997\n",
      "14 = 4.395331382751465\n",
      "15 = 7.990861415863037\n",
      "16 = 0.37020060420036316\n",
      "17 = 4.395319938659668\n",
      "18 = 91.2139892578125\n",
      "epoch:2 step:2401 [D loss: 0.472753, acc.: 80.47%] [G loss: 0.542518]\n",
      "epoch:2 step:2402 [D loss: 0.503677, acc.: 75.78%] [G loss: 0.513966]\n",
      "epoch:2 step:2403 [D loss: 0.465230, acc.: 83.59%] [G loss: 0.588856]\n",
      "epoch:2 step:2404 [D loss: 0.409294, acc.: 85.16%] [G loss: 0.714763]\n",
      "epoch:2 step:2405 [D loss: 0.495632, acc.: 79.69%] [G loss: 0.532105]\n",
      "epoch:2 step:2406 [D loss: 0.475093, acc.: 82.81%] [G loss: 0.560787]\n",
      "epoch:2 step:2407 [D loss: 0.521461, acc.: 72.66%] [G loss: 0.527524]\n",
      "epoch:2 step:2408 [D loss: 0.443700, acc.: 81.25%] [G loss: 0.622985]\n",
      "epoch:2 step:2409 [D loss: 0.497421, acc.: 79.69%] [G loss: 0.599052]\n",
      "epoch:2 step:2410 [D loss: 0.507068, acc.: 78.91%] [G loss: 0.528527]\n",
      "epoch:2 step:2411 [D loss: 0.522583, acc.: 74.22%] [G loss: 0.628977]\n",
      "epoch:2 step:2412 [D loss: 0.481033, acc.: 78.12%] [G loss: 0.596847]\n",
      "epoch:2 step:2413 [D loss: 0.481229, acc.: 80.47%] [G loss: 0.596359]\n",
      "epoch:2 step:2414 [D loss: 0.491655, acc.: 75.78%] [G loss: 0.579867]\n",
      "epoch:2 step:2415 [D loss: 0.458987, acc.: 79.69%] [G loss: 0.662341]\n",
      "epoch:2 step:2416 [D loss: 0.559971, acc.: 75.78%] [G loss: 0.524693]\n",
      "epoch:2 step:2417 [D loss: 0.516781, acc.: 77.34%] [G loss: 0.548279]\n",
      "epoch:2 step:2418 [D loss: 0.483938, acc.: 75.00%] [G loss: 0.628186]\n",
      "epoch:2 step:2419 [D loss: 0.456432, acc.: 80.47%] [G loss: 0.794372]\n",
      "epoch:2 step:2420 [D loss: 0.389395, acc.: 85.16%] [G loss: 0.781296]\n",
      "epoch:2 step:2421 [D loss: 0.462711, acc.: 81.25%] [G loss: 0.657246]\n",
      "epoch:2 step:2422 [D loss: 0.512789, acc.: 72.66%] [G loss: 0.706033]\n",
      "epoch:2 step:2423 [D loss: 0.482955, acc.: 74.22%] [G loss: 0.630991]\n",
      "epoch:2 step:2424 [D loss: 0.516882, acc.: 74.22%] [G loss: 0.591351]\n",
      "epoch:2 step:2425 [D loss: 0.490497, acc.: 77.34%] [G loss: 0.727058]\n",
      "epoch:2 step:2426 [D loss: 0.475799, acc.: 82.03%] [G loss: 0.700256]\n",
      "epoch:2 step:2427 [D loss: 0.498562, acc.: 74.22%] [G loss: 0.579152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2428 [D loss: 0.476909, acc.: 77.34%] [G loss: 0.631676]\n",
      "epoch:2 step:2429 [D loss: 0.466941, acc.: 78.91%] [G loss: 0.607936]\n",
      "epoch:2 step:2430 [D loss: 0.457702, acc.: 80.47%] [G loss: 0.631467]\n",
      "epoch:2 step:2431 [D loss: 0.471874, acc.: 77.34%] [G loss: 0.619841]\n",
      "epoch:2 step:2432 [D loss: 0.414700, acc.: 85.16%] [G loss: 0.708428]\n",
      "epoch:2 step:2433 [D loss: 0.552765, acc.: 71.88%] [G loss: 0.585200]\n",
      "epoch:2 step:2434 [D loss: 0.511323, acc.: 74.22%] [G loss: 0.579077]\n",
      "epoch:2 step:2435 [D loss: 0.541399, acc.: 70.31%] [G loss: 0.602572]\n",
      "epoch:2 step:2436 [D loss: 0.521902, acc.: 73.44%] [G loss: 0.601542]\n",
      "epoch:2 step:2437 [D loss: 0.499918, acc.: 76.56%] [G loss: 0.576171]\n",
      "epoch:2 step:2438 [D loss: 0.530445, acc.: 71.88%] [G loss: 0.597028]\n",
      "epoch:2 step:2439 [D loss: 0.490345, acc.: 82.03%] [G loss: 0.684970]\n",
      "epoch:2 step:2440 [D loss: 0.585438, acc.: 71.88%] [G loss: 0.639688]\n",
      "epoch:2 step:2441 [D loss: 0.486617, acc.: 75.78%] [G loss: 0.694054]\n",
      "epoch:2 step:2442 [D loss: 0.490334, acc.: 78.12%] [G loss: 0.659162]\n",
      "epoch:2 step:2443 [D loss: 0.466694, acc.: 78.91%] [G loss: 0.537893]\n",
      "epoch:2 step:2444 [D loss: 0.487028, acc.: 77.34%] [G loss: 0.599897]\n",
      "epoch:2 step:2445 [D loss: 0.492522, acc.: 79.69%] [G loss: 0.702302]\n",
      "epoch:2 step:2446 [D loss: 0.537771, acc.: 71.09%] [G loss: 0.607284]\n",
      "epoch:2 step:2447 [D loss: 0.497884, acc.: 76.56%] [G loss: 0.590899]\n",
      "epoch:2 step:2448 [D loss: 0.495532, acc.: 82.03%] [G loss: 0.785913]\n",
      "epoch:2 step:2449 [D loss: 0.402790, acc.: 85.94%] [G loss: 0.775418]\n",
      "epoch:2 step:2450 [D loss: 0.483505, acc.: 73.44%] [G loss: 0.700069]\n",
      "epoch:2 step:2451 [D loss: 0.539957, acc.: 75.00%] [G loss: 0.543849]\n",
      "epoch:2 step:2452 [D loss: 0.494352, acc.: 75.78%] [G loss: 0.556028]\n",
      "epoch:2 step:2453 [D loss: 0.502409, acc.: 77.34%] [G loss: 0.643325]\n",
      "epoch:2 step:2454 [D loss: 0.538591, acc.: 71.88%] [G loss: 0.640453]\n",
      "epoch:2 step:2455 [D loss: 0.468552, acc.: 78.12%] [G loss: 0.692789]\n",
      "epoch:2 step:2456 [D loss: 0.413070, acc.: 85.94%] [G loss: 0.830713]\n",
      "epoch:2 step:2457 [D loss: 0.518936, acc.: 75.78%] [G loss: 0.612462]\n",
      "epoch:2 step:2458 [D loss: 0.515770, acc.: 76.56%] [G loss: 0.684062]\n",
      "epoch:2 step:2459 [D loss: 0.502296, acc.: 77.34%] [G loss: 0.665637]\n",
      "epoch:2 step:2460 [D loss: 0.481825, acc.: 81.25%] [G loss: 0.649211]\n",
      "epoch:2 step:2461 [D loss: 0.551966, acc.: 69.53%] [G loss: 0.481123]\n",
      "epoch:2 step:2462 [D loss: 0.481946, acc.: 78.12%] [G loss: 0.691715]\n",
      "epoch:2 step:2463 [D loss: 0.414372, acc.: 82.03%] [G loss: 0.885112]\n",
      "epoch:2 step:2464 [D loss: 0.535423, acc.: 67.97%] [G loss: 0.638712]\n",
      "epoch:2 step:2465 [D loss: 0.494957, acc.: 81.25%] [G loss: 0.526069]\n",
      "epoch:2 step:2466 [D loss: 0.437074, acc.: 79.69%] [G loss: 0.718937]\n",
      "epoch:2 step:2467 [D loss: 0.491616, acc.: 75.78%] [G loss: 0.591128]\n",
      "epoch:2 step:2468 [D loss: 0.482470, acc.: 78.12%] [G loss: 0.613334]\n",
      "epoch:2 step:2469 [D loss: 0.530738, acc.: 75.00%] [G loss: 0.573751]\n",
      "epoch:2 step:2470 [D loss: 0.467168, acc.: 79.69%] [G loss: 0.642387]\n",
      "epoch:2 step:2471 [D loss: 0.474433, acc.: 79.69%] [G loss: 0.689754]\n",
      "epoch:2 step:2472 [D loss: 0.444990, acc.: 81.25%] [G loss: 0.664314]\n",
      "epoch:2 step:2473 [D loss: 0.543181, acc.: 71.88%] [G loss: 0.538981]\n",
      "epoch:2 step:2474 [D loss: 0.589134, acc.: 64.06%] [G loss: 0.457020]\n",
      "epoch:2 step:2475 [D loss: 0.509426, acc.: 73.44%] [G loss: 0.623688]\n",
      "epoch:2 step:2476 [D loss: 0.468330, acc.: 79.69%] [G loss: 0.573212]\n",
      "epoch:2 step:2477 [D loss: 0.468823, acc.: 78.91%] [G loss: 0.573024]\n",
      "epoch:2 step:2478 [D loss: 0.541315, acc.: 73.44%] [G loss: 0.498253]\n",
      "epoch:2 step:2479 [D loss: 0.431230, acc.: 82.03%] [G loss: 0.497033]\n",
      "epoch:2 step:2480 [D loss: 0.540236, acc.: 69.53%] [G loss: 0.581516]\n",
      "epoch:2 step:2481 [D loss: 0.521576, acc.: 74.22%] [G loss: 0.538595]\n",
      "epoch:2 step:2482 [D loss: 0.447985, acc.: 80.47%] [G loss: 0.603831]\n",
      "epoch:2 step:2483 [D loss: 0.427945, acc.: 80.47%] [G loss: 0.542866]\n",
      "epoch:2 step:2484 [D loss: 0.483267, acc.: 77.34%] [G loss: 0.696909]\n",
      "epoch:2 step:2485 [D loss: 0.434698, acc.: 83.59%] [G loss: 0.699623]\n",
      "epoch:2 step:2486 [D loss: 0.473900, acc.: 77.34%] [G loss: 0.683580]\n",
      "epoch:2 step:2487 [D loss: 0.457341, acc.: 79.69%] [G loss: 0.650539]\n",
      "epoch:2 step:2488 [D loss: 0.481987, acc.: 79.69%] [G loss: 0.597391]\n",
      "epoch:2 step:2489 [D loss: 0.519416, acc.: 78.12%] [G loss: 0.550110]\n",
      "epoch:2 step:2490 [D loss: 0.507907, acc.: 75.00%] [G loss: 0.713612]\n",
      "epoch:2 step:2491 [D loss: 0.528096, acc.: 71.09%] [G loss: 0.546140]\n",
      "epoch:2 step:2492 [D loss: 0.481674, acc.: 76.56%] [G loss: 0.633521]\n",
      "epoch:2 step:2493 [D loss: 0.471245, acc.: 79.69%] [G loss: 0.578690]\n",
      "epoch:2 step:2494 [D loss: 0.474839, acc.: 80.47%] [G loss: 0.668586]\n",
      "epoch:2 step:2495 [D loss: 0.584294, acc.: 63.28%] [G loss: 0.503085]\n",
      "epoch:2 step:2496 [D loss: 0.586078, acc.: 67.19%] [G loss: 0.556348]\n",
      "epoch:2 step:2497 [D loss: 0.457443, acc.: 82.81%] [G loss: 0.625614]\n",
      "epoch:2 step:2498 [D loss: 0.512534, acc.: 74.22%] [G loss: 0.646288]\n",
      "epoch:2 step:2499 [D loss: 0.516930, acc.: 75.78%] [G loss: 0.532547]\n",
      "epoch:2 step:2500 [D loss: 0.497507, acc.: 78.12%] [G loss: 0.468446]\n",
      "epoch:2 step:2501 [D loss: 0.474238, acc.: 82.03%] [G loss: 0.467749]\n",
      "epoch:2 step:2502 [D loss: 0.488039, acc.: 75.78%] [G loss: 0.566453]\n",
      "epoch:2 step:2503 [D loss: 0.486832, acc.: 78.12%] [G loss: 0.705598]\n",
      "epoch:2 step:2504 [D loss: 0.485816, acc.: 81.25%] [G loss: 0.659633]\n",
      "epoch:2 step:2505 [D loss: 0.443523, acc.: 81.25%] [G loss: 0.715004]\n",
      "epoch:2 step:2506 [D loss: 0.495709, acc.: 76.56%] [G loss: 0.606051]\n",
      "epoch:2 step:2507 [D loss: 0.447334, acc.: 79.69%] [G loss: 0.648295]\n",
      "epoch:2 step:2508 [D loss: 0.492656, acc.: 75.00%] [G loss: 0.577446]\n",
      "epoch:2 step:2509 [D loss: 0.461165, acc.: 81.25%] [G loss: 0.588159]\n",
      "epoch:2 step:2510 [D loss: 0.547395, acc.: 69.53%] [G loss: 0.519175]\n",
      "epoch:2 step:2511 [D loss: 0.534487, acc.: 76.56%] [G loss: 0.540605]\n",
      "epoch:2 step:2512 [D loss: 0.460913, acc.: 77.34%] [G loss: 0.632829]\n",
      "epoch:2 step:2513 [D loss: 0.462569, acc.: 77.34%] [G loss: 0.700046]\n",
      "epoch:2 step:2514 [D loss: 0.464913, acc.: 83.59%] [G loss: 0.581478]\n",
      "epoch:2 step:2515 [D loss: 0.472280, acc.: 78.12%] [G loss: 0.655446]\n",
      "epoch:2 step:2516 [D loss: 0.537164, acc.: 71.88%] [G loss: 0.779668]\n",
      "epoch:2 step:2517 [D loss: 0.572513, acc.: 69.53%] [G loss: 0.588724]\n",
      "epoch:2 step:2518 [D loss: 0.526724, acc.: 77.34%] [G loss: 0.500730]\n",
      "epoch:2 step:2519 [D loss: 0.497113, acc.: 78.91%] [G loss: 0.501213]\n",
      "epoch:2 step:2520 [D loss: 0.512569, acc.: 78.12%] [G loss: 0.578974]\n",
      "epoch:2 step:2521 [D loss: 0.485733, acc.: 78.12%] [G loss: 0.615435]\n",
      "epoch:2 step:2522 [D loss: 0.479480, acc.: 77.34%] [G loss: 0.772378]\n",
      "epoch:2 step:2523 [D loss: 0.459497, acc.: 78.91%] [G loss: 0.828370]\n",
      "epoch:2 step:2524 [D loss: 0.441055, acc.: 82.81%] [G loss: 0.837051]\n",
      "epoch:2 step:2525 [D loss: 0.503995, acc.: 76.56%] [G loss: 0.589678]\n",
      "epoch:2 step:2526 [D loss: 0.568341, acc.: 68.75%] [G loss: 0.570549]\n",
      "epoch:2 step:2527 [D loss: 0.602730, acc.: 64.84%] [G loss: 0.440002]\n",
      "epoch:2 step:2528 [D loss: 0.472488, acc.: 79.69%] [G loss: 0.577114]\n",
      "epoch:2 step:2529 [D loss: 0.532547, acc.: 71.09%] [G loss: 0.496724]\n",
      "epoch:2 step:2530 [D loss: 0.480759, acc.: 80.47%] [G loss: 0.578474]\n",
      "epoch:2 step:2531 [D loss: 0.516312, acc.: 72.66%] [G loss: 0.562145]\n",
      "epoch:2 step:2532 [D loss: 0.508218, acc.: 74.22%] [G loss: 0.561305]\n",
      "epoch:2 step:2533 [D loss: 0.454058, acc.: 77.34%] [G loss: 0.545096]\n",
      "epoch:2 step:2534 [D loss: 0.512866, acc.: 75.78%] [G loss: 0.465858]\n",
      "epoch:2 step:2535 [D loss: 0.444883, acc.: 81.25%] [G loss: 0.723799]\n",
      "epoch:2 step:2536 [D loss: 0.527898, acc.: 75.78%] [G loss: 0.582768]\n",
      "epoch:2 step:2537 [D loss: 0.450839, acc.: 78.12%] [G loss: 0.727069]\n",
      "epoch:2 step:2538 [D loss: 0.505537, acc.: 75.00%] [G loss: 0.834136]\n",
      "epoch:2 step:2539 [D loss: 0.478117, acc.: 75.78%] [G loss: 0.778436]\n",
      "epoch:2 step:2540 [D loss: 0.490794, acc.: 75.00%] [G loss: 0.604294]\n",
      "epoch:2 step:2541 [D loss: 0.519871, acc.: 74.22%] [G loss: 0.517376]\n",
      "epoch:2 step:2542 [D loss: 0.520664, acc.: 77.34%] [G loss: 0.507238]\n",
      "epoch:2 step:2543 [D loss: 0.484326, acc.: 78.12%] [G loss: 0.652858]\n",
      "epoch:2 step:2544 [D loss: 0.464976, acc.: 79.69%] [G loss: 0.549347]\n",
      "epoch:2 step:2545 [D loss: 0.533221, acc.: 74.22%] [G loss: 0.609720]\n",
      "epoch:2 step:2546 [D loss: 0.566887, acc.: 72.66%] [G loss: 0.526057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2547 [D loss: 0.515374, acc.: 75.00%] [G loss: 0.494383]\n",
      "epoch:2 step:2548 [D loss: 0.470426, acc.: 83.59%] [G loss: 0.591754]\n",
      "epoch:2 step:2549 [D loss: 0.507326, acc.: 70.31%] [G loss: 0.631337]\n",
      "epoch:2 step:2550 [D loss: 0.511652, acc.: 78.91%] [G loss: 0.534600]\n",
      "epoch:2 step:2551 [D loss: 0.441864, acc.: 81.25%] [G loss: 0.591720]\n",
      "epoch:2 step:2552 [D loss: 0.438168, acc.: 82.81%] [G loss: 0.648936]\n",
      "epoch:2 step:2553 [D loss: 0.450927, acc.: 83.59%] [G loss: 0.560214]\n",
      "epoch:2 step:2554 [D loss: 0.399782, acc.: 86.72%] [G loss: 0.574808]\n",
      "epoch:2 step:2555 [D loss: 0.494888, acc.: 78.12%] [G loss: 0.565138]\n",
      "epoch:2 step:2556 [D loss: 0.466359, acc.: 82.81%] [G loss: 0.526407]\n",
      "epoch:2 step:2557 [D loss: 0.483212, acc.: 77.34%] [G loss: 0.592083]\n",
      "epoch:2 step:2558 [D loss: 0.478687, acc.: 75.78%] [G loss: 0.575109]\n",
      "epoch:2 step:2559 [D loss: 0.539570, acc.: 71.09%] [G loss: 0.578308]\n",
      "epoch:2 step:2560 [D loss: 0.515519, acc.: 78.91%] [G loss: 0.529291]\n",
      "epoch:2 step:2561 [D loss: 0.497299, acc.: 82.03%] [G loss: 0.453736]\n",
      "epoch:2 step:2562 [D loss: 0.517232, acc.: 77.34%] [G loss: 0.484631]\n",
      "epoch:2 step:2563 [D loss: 0.474321, acc.: 82.03%] [G loss: 0.568509]\n",
      "epoch:2 step:2564 [D loss: 0.462338, acc.: 80.47%] [G loss: 0.668012]\n",
      "epoch:2 step:2565 [D loss: 0.479495, acc.: 78.12%] [G loss: 0.721210]\n",
      "epoch:2 step:2566 [D loss: 0.517751, acc.: 72.66%] [G loss: 0.635494]\n",
      "epoch:2 step:2567 [D loss: 0.494924, acc.: 76.56%] [G loss: 0.562233]\n",
      "epoch:2 step:2568 [D loss: 0.491002, acc.: 78.12%] [G loss: 0.493002]\n",
      "epoch:2 step:2569 [D loss: 0.486597, acc.: 72.66%] [G loss: 0.518891]\n",
      "epoch:2 step:2570 [D loss: 0.553489, acc.: 71.09%] [G loss: 0.474363]\n",
      "epoch:2 step:2571 [D loss: 0.494751, acc.: 75.78%] [G loss: 0.478173]\n",
      "epoch:2 step:2572 [D loss: 0.473382, acc.: 78.12%] [G loss: 0.540617]\n",
      "epoch:2 step:2573 [D loss: 0.470390, acc.: 78.91%] [G loss: 0.578743]\n",
      "epoch:2 step:2574 [D loss: 0.518084, acc.: 75.00%] [G loss: 0.544648]\n",
      "epoch:2 step:2575 [D loss: 0.509275, acc.: 71.88%] [G loss: 0.575700]\n",
      "epoch:2 step:2576 [D loss: 0.514443, acc.: 74.22%] [G loss: 0.585684]\n",
      "epoch:2 step:2577 [D loss: 0.474997, acc.: 79.69%] [G loss: 0.525082]\n",
      "epoch:2 step:2578 [D loss: 0.567793, acc.: 71.88%] [G loss: 0.400268]\n",
      "epoch:2 step:2579 [D loss: 0.434807, acc.: 84.38%] [G loss: 0.653487]\n",
      "epoch:2 step:2580 [D loss: 0.450782, acc.: 81.25%] [G loss: 0.599306]\n",
      "epoch:2 step:2581 [D loss: 0.383756, acc.: 89.84%] [G loss: 0.788216]\n",
      "epoch:2 step:2582 [D loss: 0.440876, acc.: 78.91%] [G loss: 0.879978]\n",
      "epoch:2 step:2583 [D loss: 0.454840, acc.: 78.12%] [G loss: 0.743057]\n",
      "epoch:2 step:2584 [D loss: 0.630702, acc.: 68.75%] [G loss: 0.461053]\n",
      "epoch:2 step:2585 [D loss: 0.596553, acc.: 67.19%] [G loss: 0.495585]\n",
      "epoch:2 step:2586 [D loss: 0.507829, acc.: 79.69%] [G loss: 0.595707]\n",
      "epoch:2 step:2587 [D loss: 0.503225, acc.: 77.34%] [G loss: 0.553905]\n",
      "epoch:2 step:2588 [D loss: 0.500585, acc.: 76.56%] [G loss: 0.563296]\n",
      "epoch:2 step:2589 [D loss: 0.493126, acc.: 78.91%] [G loss: 0.629007]\n",
      "epoch:2 step:2590 [D loss: 0.536177, acc.: 73.44%] [G loss: 0.506193]\n",
      "epoch:2 step:2591 [D loss: 0.579171, acc.: 69.53%] [G loss: 0.495590]\n",
      "epoch:2 step:2592 [D loss: 0.528769, acc.: 77.34%] [G loss: 0.513728]\n",
      "epoch:2 step:2593 [D loss: 0.471138, acc.: 80.47%] [G loss: 0.591500]\n",
      "epoch:2 step:2594 [D loss: 0.601416, acc.: 66.41%] [G loss: 0.449386]\n",
      "epoch:2 step:2595 [D loss: 0.553282, acc.: 72.66%] [G loss: 0.445976]\n",
      "epoch:2 step:2596 [D loss: 0.504372, acc.: 82.03%] [G loss: 0.467281]\n",
      "epoch:2 step:2597 [D loss: 0.508426, acc.: 75.00%] [G loss: 0.477916]\n",
      "epoch:2 step:2598 [D loss: 0.447031, acc.: 82.81%] [G loss: 0.565423]\n",
      "epoch:2 step:2599 [D loss: 0.472633, acc.: 84.38%] [G loss: 0.585486]\n",
      "epoch:2 step:2600 [D loss: 0.536637, acc.: 75.00%] [G loss: 0.573038]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.697263\n",
      "FID: 81.979256\n",
      "0 = 13.428616412925678\n",
      "1 = 0.1304702667330906\n",
      "2 = 0.9684000015258789\n",
      "3 = 0.9368000030517578\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9368000030517578\n",
      "7 = 10.272624968194965\n",
      "8 = 0.17719598066968315\n",
      "9 = 0.9190999865531921\n",
      "10 = 0.8795999884605408\n",
      "11 = 0.9585999846458435\n",
      "12 = 0.9550488591194153\n",
      "13 = 0.8795999884605408\n",
      "14 = 4.697278022766113\n",
      "15 = 7.808682918548584\n",
      "16 = 0.3660140931606293\n",
      "17 = 4.697263240814209\n",
      "18 = 81.97925567626953\n",
      "epoch:2 step:2601 [D loss: 0.525603, acc.: 75.00%] [G loss: 0.503657]\n",
      "epoch:2 step:2602 [D loss: 0.546680, acc.: 75.00%] [G loss: 0.417855]\n",
      "epoch:2 step:2603 [D loss: 0.522210, acc.: 75.00%] [G loss: 0.479962]\n",
      "epoch:2 step:2604 [D loss: 0.459711, acc.: 82.81%] [G loss: 0.526531]\n",
      "epoch:2 step:2605 [D loss: 0.566360, acc.: 67.97%] [G loss: 0.635931]\n",
      "epoch:2 step:2606 [D loss: 0.452873, acc.: 82.81%] [G loss: 0.643908]\n",
      "epoch:2 step:2607 [D loss: 0.455092, acc.: 81.25%] [G loss: 0.702663]\n",
      "epoch:2 step:2608 [D loss: 0.465465, acc.: 79.69%] [G loss: 0.808387]\n",
      "epoch:2 step:2609 [D loss: 0.573531, acc.: 70.31%] [G loss: 0.573625]\n",
      "epoch:2 step:2610 [D loss: 0.460924, acc.: 75.78%] [G loss: 0.661944]\n",
      "epoch:2 step:2611 [D loss: 0.469864, acc.: 79.69%] [G loss: 0.658573]\n",
      "epoch:2 step:2612 [D loss: 0.526446, acc.: 73.44%] [G loss: 0.508843]\n",
      "epoch:2 step:2613 [D loss: 0.563258, acc.: 67.19%] [G loss: 0.489633]\n",
      "epoch:2 step:2614 [D loss: 0.522115, acc.: 76.56%] [G loss: 0.486015]\n",
      "epoch:2 step:2615 [D loss: 0.488139, acc.: 78.12%] [G loss: 0.441052]\n",
      "epoch:2 step:2616 [D loss: 0.561159, acc.: 72.66%] [G loss: 0.463409]\n",
      "epoch:2 step:2617 [D loss: 0.554839, acc.: 71.88%] [G loss: 0.460474]\n",
      "epoch:2 step:2618 [D loss: 0.509050, acc.: 78.91%] [G loss: 0.587391]\n",
      "epoch:2 step:2619 [D loss: 0.515289, acc.: 74.22%] [G loss: 0.556233]\n",
      "epoch:2 step:2620 [D loss: 0.428984, acc.: 79.69%] [G loss: 0.620839]\n",
      "epoch:2 step:2621 [D loss: 0.423724, acc.: 83.59%] [G loss: 0.649578]\n",
      "epoch:2 step:2622 [D loss: 0.418455, acc.: 86.72%] [G loss: 0.698376]\n",
      "epoch:2 step:2623 [D loss: 0.479529, acc.: 78.12%] [G loss: 0.582431]\n",
      "epoch:2 step:2624 [D loss: 0.487592, acc.: 78.12%] [G loss: 0.546111]\n",
      "epoch:2 step:2625 [D loss: 0.532755, acc.: 71.88%] [G loss: 0.618844]\n",
      "epoch:2 step:2626 [D loss: 0.404754, acc.: 87.50%] [G loss: 0.726282]\n",
      "epoch:2 step:2627 [D loss: 0.466157, acc.: 80.47%] [G loss: 0.713975]\n",
      "epoch:2 step:2628 [D loss: 0.438762, acc.: 82.03%] [G loss: 0.748327]\n",
      "epoch:2 step:2629 [D loss: 0.488611, acc.: 75.78%] [G loss: 0.663002]\n",
      "epoch:2 step:2630 [D loss: 0.484783, acc.: 78.91%] [G loss: 0.650394]\n",
      "epoch:2 step:2631 [D loss: 0.467992, acc.: 79.69%] [G loss: 0.512141]\n",
      "epoch:2 step:2632 [D loss: 0.448169, acc.: 78.91%] [G loss: 0.590981]\n",
      "epoch:2 step:2633 [D loss: 0.459752, acc.: 81.25%] [G loss: 0.601208]\n",
      "epoch:2 step:2634 [D loss: 0.518321, acc.: 71.09%] [G loss: 0.594067]\n",
      "epoch:2 step:2635 [D loss: 0.492630, acc.: 76.56%] [G loss: 0.509704]\n",
      "epoch:2 step:2636 [D loss: 0.520798, acc.: 73.44%] [G loss: 0.543597]\n",
      "epoch:2 step:2637 [D loss: 0.511590, acc.: 77.34%] [G loss: 0.618195]\n",
      "epoch:2 step:2638 [D loss: 0.468062, acc.: 79.69%] [G loss: 0.658657]\n",
      "epoch:2 step:2639 [D loss: 0.585379, acc.: 64.84%] [G loss: 0.546620]\n",
      "epoch:2 step:2640 [D loss: 0.671138, acc.: 60.16%] [G loss: 0.312320]\n",
      "epoch:2 step:2641 [D loss: 0.485174, acc.: 80.47%] [G loss: 0.544373]\n",
      "epoch:2 step:2642 [D loss: 0.469271, acc.: 76.56%] [G loss: 0.602747]\n",
      "epoch:2 step:2643 [D loss: 0.455537, acc.: 81.25%] [G loss: 0.729533]\n",
      "epoch:2 step:2644 [D loss: 0.494762, acc.: 75.00%] [G loss: 0.658729]\n",
      "epoch:2 step:2645 [D loss: 0.488208, acc.: 76.56%] [G loss: 0.515185]\n",
      "epoch:2 step:2646 [D loss: 0.520755, acc.: 74.22%] [G loss: 0.671389]\n",
      "epoch:2 step:2647 [D loss: 0.445732, acc.: 82.81%] [G loss: 0.645326]\n",
      "epoch:2 step:2648 [D loss: 0.589167, acc.: 71.09%] [G loss: 0.482406]\n",
      "epoch:2 step:2649 [D loss: 0.466674, acc.: 81.25%] [G loss: 0.539450]\n",
      "epoch:2 step:2650 [D loss: 0.543013, acc.: 71.88%] [G loss: 0.538662]\n",
      "epoch:2 step:2651 [D loss: 0.489065, acc.: 75.00%] [G loss: 0.567380]\n",
      "epoch:2 step:2652 [D loss: 0.570591, acc.: 67.19%] [G loss: 0.423658]\n",
      "epoch:2 step:2653 [D loss: 0.511400, acc.: 75.00%] [G loss: 0.586796]\n",
      "epoch:2 step:2654 [D loss: 0.501443, acc.: 75.78%] [G loss: 0.625566]\n",
      "epoch:2 step:2655 [D loss: 0.450086, acc.: 80.47%] [G loss: 0.730828]\n",
      "epoch:2 step:2656 [D loss: 0.526746, acc.: 75.00%] [G loss: 0.709948]\n",
      "epoch:2 step:2657 [D loss: 0.549427, acc.: 71.88%] [G loss: 0.649237]\n",
      "epoch:2 step:2658 [D loss: 0.468102, acc.: 79.69%] [G loss: 0.497400]\n",
      "epoch:2 step:2659 [D loss: 0.536457, acc.: 71.88%] [G loss: 0.544056]\n",
      "epoch:2 step:2660 [D loss: 0.497619, acc.: 77.34%] [G loss: 0.531096]\n",
      "epoch:2 step:2661 [D loss: 0.566761, acc.: 71.09%] [G loss: 0.526651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2662 [D loss: 0.567319, acc.: 73.44%] [G loss: 0.501686]\n",
      "epoch:2 step:2663 [D loss: 0.489104, acc.: 82.03%] [G loss: 0.479217]\n",
      "epoch:2 step:2664 [D loss: 0.458377, acc.: 83.59%] [G loss: 0.559159]\n",
      "epoch:2 step:2665 [D loss: 0.489904, acc.: 79.69%] [G loss: 0.445193]\n",
      "epoch:2 step:2666 [D loss: 0.433632, acc.: 85.16%] [G loss: 0.653898]\n",
      "epoch:2 step:2667 [D loss: 0.563888, acc.: 72.66%] [G loss: 0.529654]\n",
      "epoch:2 step:2668 [D loss: 0.512145, acc.: 77.34%] [G loss: 0.519628]\n",
      "epoch:2 step:2669 [D loss: 0.507578, acc.: 75.00%] [G loss: 0.502905]\n",
      "epoch:2 step:2670 [D loss: 0.481422, acc.: 79.69%] [G loss: 0.496320]\n",
      "epoch:2 step:2671 [D loss: 0.448500, acc.: 84.38%] [G loss: 0.599956]\n",
      "epoch:2 step:2672 [D loss: 0.462562, acc.: 76.56%] [G loss: 0.736016]\n",
      "epoch:2 step:2673 [D loss: 0.499376, acc.: 74.22%] [G loss: 0.544022]\n",
      "epoch:2 step:2674 [D loss: 0.549836, acc.: 67.97%] [G loss: 0.614270]\n",
      "epoch:2 step:2675 [D loss: 0.457761, acc.: 83.59%] [G loss: 0.688486]\n",
      "epoch:2 step:2676 [D loss: 0.492915, acc.: 74.22%] [G loss: 0.734284]\n",
      "epoch:2 step:2677 [D loss: 0.476068, acc.: 79.69%] [G loss: 0.761543]\n",
      "epoch:2 step:2678 [D loss: 0.434131, acc.: 80.47%] [G loss: 0.654854]\n",
      "epoch:2 step:2679 [D loss: 0.472828, acc.: 76.56%] [G loss: 0.535618]\n",
      "epoch:2 step:2680 [D loss: 0.444291, acc.: 79.69%] [G loss: 0.684224]\n",
      "epoch:2 step:2681 [D loss: 0.502510, acc.: 77.34%] [G loss: 0.535462]\n",
      "epoch:2 step:2682 [D loss: 0.508811, acc.: 73.44%] [G loss: 0.611377]\n",
      "epoch:2 step:2683 [D loss: 0.542485, acc.: 73.44%] [G loss: 0.548350]\n",
      "epoch:2 step:2684 [D loss: 0.467177, acc.: 80.47%] [G loss: 0.543611]\n",
      "epoch:2 step:2685 [D loss: 0.542790, acc.: 74.22%] [G loss: 0.539326]\n",
      "epoch:2 step:2686 [D loss: 0.528338, acc.: 75.78%] [G loss: 0.591472]\n",
      "epoch:2 step:2687 [D loss: 0.505792, acc.: 74.22%] [G loss: 0.433858]\n",
      "epoch:2 step:2688 [D loss: 0.500648, acc.: 78.12%] [G loss: 0.644708]\n",
      "epoch:2 step:2689 [D loss: 0.475367, acc.: 79.69%] [G loss: 0.681524]\n",
      "epoch:2 step:2690 [D loss: 0.493262, acc.: 74.22%] [G loss: 0.679849]\n",
      "epoch:2 step:2691 [D loss: 0.562564, acc.: 71.09%] [G loss: 0.581211]\n",
      "epoch:2 step:2692 [D loss: 0.490410, acc.: 84.38%] [G loss: 0.550270]\n",
      "epoch:2 step:2693 [D loss: 0.479602, acc.: 78.91%] [G loss: 0.579127]\n",
      "epoch:2 step:2694 [D loss: 0.531336, acc.: 68.75%] [G loss: 0.568386]\n",
      "epoch:2 step:2695 [D loss: 0.448980, acc.: 87.50%] [G loss: 0.581312]\n",
      "epoch:2 step:2696 [D loss: 0.426072, acc.: 84.38%] [G loss: 0.710871]\n",
      "epoch:2 step:2697 [D loss: 0.502436, acc.: 75.78%] [G loss: 0.734802]\n",
      "epoch:2 step:2698 [D loss: 0.603541, acc.: 65.62%] [G loss: 0.676330]\n",
      "epoch:2 step:2699 [D loss: 0.536610, acc.: 71.88%] [G loss: 0.568112]\n",
      "epoch:2 step:2700 [D loss: 0.551102, acc.: 66.41%] [G loss: 0.578718]\n",
      "epoch:2 step:2701 [D loss: 0.540600, acc.: 71.09%] [G loss: 0.558837]\n",
      "epoch:2 step:2702 [D loss: 0.602170, acc.: 64.06%] [G loss: 0.405617]\n",
      "epoch:2 step:2703 [D loss: 0.484893, acc.: 77.34%] [G loss: 0.569243]\n",
      "epoch:2 step:2704 [D loss: 0.471448, acc.: 83.59%] [G loss: 0.693587]\n",
      "epoch:2 step:2705 [D loss: 0.501722, acc.: 78.12%] [G loss: 0.552871]\n",
      "epoch:2 step:2706 [D loss: 0.495765, acc.: 77.34%] [G loss: 0.623421]\n",
      "epoch:2 step:2707 [D loss: 0.424184, acc.: 83.59%] [G loss: 0.619369]\n",
      "epoch:2 step:2708 [D loss: 0.441361, acc.: 82.81%] [G loss: 0.695663]\n",
      "epoch:2 step:2709 [D loss: 0.478705, acc.: 80.47%] [G loss: 0.606608]\n",
      "epoch:2 step:2710 [D loss: 0.511363, acc.: 74.22%] [G loss: 0.531802]\n",
      "epoch:2 step:2711 [D loss: 0.490986, acc.: 81.25%] [G loss: 0.564495]\n",
      "epoch:2 step:2712 [D loss: 0.500643, acc.: 75.00%] [G loss: 0.503424]\n",
      "epoch:2 step:2713 [D loss: 0.488543, acc.: 82.81%] [G loss: 0.516136]\n",
      "epoch:2 step:2714 [D loss: 0.513069, acc.: 75.00%] [G loss: 0.531762]\n",
      "epoch:2 step:2715 [D loss: 0.477903, acc.: 75.00%] [G loss: 0.631962]\n",
      "epoch:2 step:2716 [D loss: 0.472228, acc.: 79.69%] [G loss: 0.570731]\n",
      "epoch:2 step:2717 [D loss: 0.476457, acc.: 78.91%] [G loss: 0.620422]\n",
      "epoch:2 step:2718 [D loss: 0.523631, acc.: 79.69%] [G loss: 0.600340]\n",
      "epoch:2 step:2719 [D loss: 0.551497, acc.: 71.09%] [G loss: 0.436318]\n",
      "epoch:2 step:2720 [D loss: 0.484399, acc.: 78.91%] [G loss: 0.489997]\n",
      "epoch:2 step:2721 [D loss: 0.505621, acc.: 75.00%] [G loss: 0.547101]\n",
      "epoch:2 step:2722 [D loss: 0.479255, acc.: 81.25%] [G loss: 0.514440]\n",
      "epoch:2 step:2723 [D loss: 0.495153, acc.: 76.56%] [G loss: 0.535752]\n",
      "epoch:2 step:2724 [D loss: 0.503023, acc.: 81.25%] [G loss: 0.413380]\n",
      "epoch:2 step:2725 [D loss: 0.499384, acc.: 73.44%] [G loss: 0.584211]\n",
      "epoch:2 step:2726 [D loss: 0.453893, acc.: 81.25%] [G loss: 0.582069]\n",
      "epoch:2 step:2727 [D loss: 0.466713, acc.: 80.47%] [G loss: 0.800400]\n",
      "epoch:2 step:2728 [D loss: 0.434366, acc.: 80.47%] [G loss: 0.831318]\n",
      "epoch:2 step:2729 [D loss: 0.507105, acc.: 76.56%] [G loss: 0.654788]\n",
      "epoch:2 step:2730 [D loss: 0.496573, acc.: 79.69%] [G loss: 0.704939]\n",
      "epoch:2 step:2731 [D loss: 0.463209, acc.: 75.00%] [G loss: 0.663397]\n",
      "epoch:2 step:2732 [D loss: 0.625404, acc.: 61.72%] [G loss: 0.477260]\n",
      "epoch:2 step:2733 [D loss: 0.472024, acc.: 78.91%] [G loss: 0.540514]\n",
      "epoch:2 step:2734 [D loss: 0.443377, acc.: 77.34%] [G loss: 0.707858]\n",
      "epoch:2 step:2735 [D loss: 0.605700, acc.: 64.84%] [G loss: 0.431485]\n",
      "epoch:2 step:2736 [D loss: 0.503682, acc.: 76.56%] [G loss: 0.588753]\n",
      "epoch:2 step:2737 [D loss: 0.489390, acc.: 77.34%] [G loss: 0.491036]\n",
      "epoch:2 step:2738 [D loss: 0.525802, acc.: 74.22%] [G loss: 0.542068]\n",
      "epoch:2 step:2739 [D loss: 0.564546, acc.: 73.44%] [G loss: 0.400959]\n",
      "epoch:2 step:2740 [D loss: 0.465154, acc.: 77.34%] [G loss: 0.590400]\n",
      "epoch:2 step:2741 [D loss: 0.622418, acc.: 63.28%] [G loss: 0.452638]\n",
      "epoch:2 step:2742 [D loss: 0.487886, acc.: 80.47%] [G loss: 0.582008]\n",
      "epoch:2 step:2743 [D loss: 0.561045, acc.: 71.09%] [G loss: 0.468762]\n",
      "epoch:2 step:2744 [D loss: 0.481100, acc.: 76.56%] [G loss: 0.610866]\n",
      "epoch:2 step:2745 [D loss: 0.441447, acc.: 80.47%] [G loss: 0.707345]\n",
      "epoch:2 step:2746 [D loss: 0.481639, acc.: 75.78%] [G loss: 0.696436]\n",
      "epoch:2 step:2747 [D loss: 0.538876, acc.: 67.97%] [G loss: 0.613387]\n",
      "epoch:2 step:2748 [D loss: 0.482100, acc.: 76.56%] [G loss: 0.624657]\n",
      "epoch:2 step:2749 [D loss: 0.494147, acc.: 71.09%] [G loss: 0.692731]\n",
      "epoch:2 step:2750 [D loss: 0.477611, acc.: 78.12%] [G loss: 0.794334]\n",
      "epoch:2 step:2751 [D loss: 0.500457, acc.: 78.12%] [G loss: 0.638324]\n",
      "epoch:2 step:2752 [D loss: 0.523044, acc.: 75.78%] [G loss: 0.490655]\n",
      "epoch:2 step:2753 [D loss: 0.550256, acc.: 71.88%] [G loss: 0.447117]\n",
      "epoch:2 step:2754 [D loss: 0.584396, acc.: 67.19%] [G loss: 0.473333]\n",
      "epoch:2 step:2755 [D loss: 0.500769, acc.: 78.12%] [G loss: 0.424343]\n",
      "epoch:2 step:2756 [D loss: 0.515478, acc.: 75.00%] [G loss: 0.510354]\n",
      "epoch:2 step:2757 [D loss: 0.521441, acc.: 76.56%] [G loss: 0.444463]\n",
      "epoch:2 step:2758 [D loss: 0.465125, acc.: 79.69%] [G loss: 0.513821]\n",
      "epoch:2 step:2759 [D loss: 0.471590, acc.: 77.34%] [G loss: 0.665905]\n",
      "epoch:2 step:2760 [D loss: 0.431702, acc.: 83.59%] [G loss: 0.848664]\n",
      "epoch:2 step:2761 [D loss: 0.521127, acc.: 75.00%] [G loss: 0.732003]\n",
      "epoch:2 step:2762 [D loss: 0.479314, acc.: 82.81%] [G loss: 0.648674]\n",
      "epoch:2 step:2763 [D loss: 0.502702, acc.: 78.91%] [G loss: 0.496852]\n",
      "epoch:2 step:2764 [D loss: 0.470654, acc.: 76.56%] [G loss: 0.689486]\n",
      "epoch:2 step:2765 [D loss: 0.530508, acc.: 71.09%] [G loss: 0.532326]\n",
      "epoch:2 step:2766 [D loss: 0.579293, acc.: 67.19%] [G loss: 0.509125]\n",
      "epoch:2 step:2767 [D loss: 0.521802, acc.: 75.78%] [G loss: 0.501943]\n",
      "epoch:2 step:2768 [D loss: 0.461043, acc.: 80.47%] [G loss: 0.596175]\n",
      "epoch:2 step:2769 [D loss: 0.516663, acc.: 75.00%] [G loss: 0.583712]\n",
      "epoch:2 step:2770 [D loss: 0.511949, acc.: 74.22%] [G loss: 0.535180]\n",
      "epoch:2 step:2771 [D loss: 0.446152, acc.: 85.16%] [G loss: 0.575638]\n",
      "epoch:2 step:2772 [D loss: 0.438485, acc.: 78.91%] [G loss: 0.528366]\n",
      "epoch:2 step:2773 [D loss: 0.494971, acc.: 79.69%] [G loss: 0.557273]\n",
      "epoch:2 step:2774 [D loss: 0.463577, acc.: 81.25%] [G loss: 0.646550]\n",
      "epoch:2 step:2775 [D loss: 0.485713, acc.: 75.00%] [G loss: 0.658620]\n",
      "epoch:2 step:2776 [D loss: 0.584171, acc.: 67.97%] [G loss: 0.455756]\n",
      "epoch:2 step:2777 [D loss: 0.565396, acc.: 71.09%] [G loss: 0.425893]\n",
      "epoch:2 step:2778 [D loss: 0.507570, acc.: 75.00%] [G loss: 0.400558]\n",
      "epoch:2 step:2779 [D loss: 0.568035, acc.: 70.31%] [G loss: 0.435012]\n",
      "epoch:2 step:2780 [D loss: 0.478741, acc.: 78.12%] [G loss: 0.434562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2781 [D loss: 0.516227, acc.: 74.22%] [G loss: 0.510323]\n",
      "epoch:2 step:2782 [D loss: 0.454406, acc.: 86.72%] [G loss: 0.544786]\n",
      "epoch:2 step:2783 [D loss: 0.459523, acc.: 82.03%] [G loss: 0.684017]\n",
      "epoch:2 step:2784 [D loss: 0.470946, acc.: 82.81%] [G loss: 0.626662]\n",
      "epoch:2 step:2785 [D loss: 0.497649, acc.: 75.78%] [G loss: 0.669661]\n",
      "epoch:2 step:2786 [D loss: 0.423532, acc.: 85.16%] [G loss: 0.653721]\n",
      "epoch:2 step:2787 [D loss: 0.587116, acc.: 68.75%] [G loss: 0.506092]\n",
      "epoch:2 step:2788 [D loss: 0.485572, acc.: 74.22%] [G loss: 0.510128]\n",
      "epoch:2 step:2789 [D loss: 0.608162, acc.: 70.31%] [G loss: 0.510139]\n",
      "epoch:2 step:2790 [D loss: 0.502179, acc.: 77.34%] [G loss: 0.567913]\n",
      "epoch:2 step:2791 [D loss: 0.527926, acc.: 79.69%] [G loss: 0.608932]\n",
      "epoch:2 step:2792 [D loss: 0.485703, acc.: 82.03%] [G loss: 0.643428]\n",
      "epoch:2 step:2793 [D loss: 0.486576, acc.: 78.91%] [G loss: 0.610163]\n",
      "epoch:2 step:2794 [D loss: 0.655958, acc.: 64.06%] [G loss: 0.381869]\n",
      "epoch:2 step:2795 [D loss: 0.418703, acc.: 84.38%] [G loss: 0.692658]\n",
      "epoch:2 step:2796 [D loss: 0.486516, acc.: 76.56%] [G loss: 0.615066]\n",
      "epoch:2 step:2797 [D loss: 0.396122, acc.: 86.72%] [G loss: 0.793610]\n",
      "epoch:2 step:2798 [D loss: 0.420005, acc.: 82.81%] [G loss: 0.721321]\n",
      "epoch:2 step:2799 [D loss: 0.440105, acc.: 81.25%] [G loss: 0.811875]\n",
      "epoch:2 step:2800 [D loss: 0.452374, acc.: 78.91%] [G loss: 0.843805]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.712716\n",
      "FID: 75.783539\n",
      "0 = 13.576247192192088\n",
      "1 = 0.13918285519370052\n",
      "2 = 0.972000002861023\n",
      "3 = 0.9440000057220459\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9440000057220459\n",
      "7 = 10.028258092308057\n",
      "8 = 0.17090262271661022\n",
      "9 = 0.9205999970436096\n",
      "10 = 0.8873999714851379\n",
      "11 = 0.9538000226020813\n",
      "12 = 0.9505141377449036\n",
      "13 = 0.8873999714851379\n",
      "14 = 4.712728023529053\n",
      "15 = 6.862506866455078\n",
      "16 = 0.4088720977306366\n",
      "17 = 4.712716102600098\n",
      "18 = 75.78353881835938\n",
      "epoch:2 step:2801 [D loss: 0.502657, acc.: 71.09%] [G loss: 0.999744]\n",
      "epoch:2 step:2802 [D loss: 0.819118, acc.: 62.50%] [G loss: 0.742511]\n",
      "epoch:2 step:2803 [D loss: 0.475689, acc.: 76.56%] [G loss: 1.013942]\n",
      "epoch:2 step:2804 [D loss: 0.373244, acc.: 86.72%] [G loss: 0.971383]\n",
      "epoch:2 step:2805 [D loss: 0.489004, acc.: 75.00%] [G loss: 0.877149]\n",
      "epoch:2 step:2806 [D loss: 0.557899, acc.: 67.97%] [G loss: 0.581499]\n",
      "epoch:2 step:2807 [D loss: 0.511043, acc.: 74.22%] [G loss: 0.645426]\n",
      "epoch:2 step:2808 [D loss: 0.491200, acc.: 75.78%] [G loss: 0.701998]\n",
      "epoch:2 step:2809 [D loss: 0.452418, acc.: 79.69%] [G loss: 0.729228]\n",
      "epoch:2 step:2810 [D loss: 0.346406, acc.: 88.28%] [G loss: 0.906064]\n",
      "epoch:2 step:2811 [D loss: 0.482942, acc.: 81.25%] [G loss: 0.916308]\n",
      "epoch:3 step:2812 [D loss: 0.508578, acc.: 75.78%] [G loss: 0.853252]\n",
      "epoch:3 step:2813 [D loss: 0.451237, acc.: 77.34%] [G loss: 0.738943]\n",
      "epoch:3 step:2814 [D loss: 0.542626, acc.: 72.66%] [G loss: 0.675305]\n",
      "epoch:3 step:2815 [D loss: 0.503029, acc.: 74.22%] [G loss: 0.605487]\n",
      "epoch:3 step:2816 [D loss: 0.530719, acc.: 72.66%] [G loss: 0.644608]\n",
      "epoch:3 step:2817 [D loss: 0.514690, acc.: 69.53%] [G loss: 0.610878]\n",
      "epoch:3 step:2818 [D loss: 0.457152, acc.: 80.47%] [G loss: 0.696369]\n",
      "epoch:3 step:2819 [D loss: 0.477079, acc.: 76.56%] [G loss: 0.662344]\n",
      "epoch:3 step:2820 [D loss: 0.516984, acc.: 75.78%] [G loss: 0.512700]\n",
      "epoch:3 step:2821 [D loss: 0.515271, acc.: 78.12%] [G loss: 0.529694]\n",
      "epoch:3 step:2822 [D loss: 0.492752, acc.: 77.34%] [G loss: 0.605827]\n",
      "epoch:3 step:2823 [D loss: 0.530612, acc.: 71.09%] [G loss: 0.617716]\n",
      "epoch:3 step:2824 [D loss: 0.512267, acc.: 71.09%] [G loss: 0.561987]\n",
      "epoch:3 step:2825 [D loss: 0.515301, acc.: 72.66%] [G loss: 0.520673]\n",
      "epoch:3 step:2826 [D loss: 0.483475, acc.: 82.81%] [G loss: 0.523433]\n",
      "epoch:3 step:2827 [D loss: 0.451007, acc.: 83.59%] [G loss: 0.694123]\n",
      "epoch:3 step:2828 [D loss: 0.518231, acc.: 73.44%] [G loss: 0.581128]\n",
      "epoch:3 step:2829 [D loss: 0.577384, acc.: 71.09%] [G loss: 0.524153]\n",
      "epoch:3 step:2830 [D loss: 0.540676, acc.: 68.75%] [G loss: 0.519946]\n",
      "epoch:3 step:2831 [D loss: 0.577238, acc.: 71.88%] [G loss: 0.464774]\n",
      "epoch:3 step:2832 [D loss: 0.483760, acc.: 82.03%] [G loss: 0.520807]\n",
      "epoch:3 step:2833 [D loss: 0.440716, acc.: 79.69%] [G loss: 0.629052]\n",
      "epoch:3 step:2834 [D loss: 0.495363, acc.: 77.34%] [G loss: 0.641940]\n",
      "epoch:3 step:2835 [D loss: 0.441801, acc.: 85.16%] [G loss: 0.685033]\n",
      "epoch:3 step:2836 [D loss: 0.495549, acc.: 79.69%] [G loss: 0.644679]\n",
      "epoch:3 step:2837 [D loss: 0.544522, acc.: 72.66%] [G loss: 0.467215]\n",
      "epoch:3 step:2838 [D loss: 0.479423, acc.: 75.00%] [G loss: 0.559696]\n",
      "epoch:3 step:2839 [D loss: 0.466712, acc.: 76.56%] [G loss: 0.604974]\n",
      "epoch:3 step:2840 [D loss: 0.463518, acc.: 82.81%] [G loss: 0.663125]\n",
      "epoch:3 step:2841 [D loss: 0.530441, acc.: 74.22%] [G loss: 0.438177]\n",
      "epoch:3 step:2842 [D loss: 0.520589, acc.: 75.78%] [G loss: 0.549448]\n",
      "epoch:3 step:2843 [D loss: 0.536713, acc.: 74.22%] [G loss: 0.562043]\n",
      "epoch:3 step:2844 [D loss: 0.532987, acc.: 71.09%] [G loss: 0.577767]\n",
      "epoch:3 step:2845 [D loss: 0.402698, acc.: 89.06%] [G loss: 0.670548]\n",
      "epoch:3 step:2846 [D loss: 0.476297, acc.: 76.56%] [G loss: 0.672106]\n",
      "epoch:3 step:2847 [D loss: 0.519958, acc.: 71.88%] [G loss: 0.667930]\n",
      "epoch:3 step:2848 [D loss: 0.480413, acc.: 80.47%] [G loss: 0.648262]\n",
      "epoch:3 step:2849 [D loss: 0.544260, acc.: 75.78%] [G loss: 0.593865]\n",
      "epoch:3 step:2850 [D loss: 0.469174, acc.: 79.69%] [G loss: 0.589246]\n",
      "epoch:3 step:2851 [D loss: 0.432116, acc.: 85.16%] [G loss: 0.632350]\n",
      "epoch:3 step:2852 [D loss: 0.499687, acc.: 77.34%] [G loss: 0.450181]\n",
      "epoch:3 step:2853 [D loss: 0.463732, acc.: 82.81%] [G loss: 0.549009]\n",
      "epoch:3 step:2854 [D loss: 0.440695, acc.: 82.03%] [G loss: 0.611328]\n",
      "epoch:3 step:2855 [D loss: 0.473108, acc.: 77.34%] [G loss: 0.585475]\n",
      "epoch:3 step:2856 [D loss: 0.467914, acc.: 78.91%] [G loss: 0.529246]\n",
      "epoch:3 step:2857 [D loss: 0.476866, acc.: 78.12%] [G loss: 0.658954]\n",
      "epoch:3 step:2858 [D loss: 0.527794, acc.: 69.53%] [G loss: 0.499604]\n",
      "epoch:3 step:2859 [D loss: 0.499882, acc.: 75.78%] [G loss: 0.558145]\n",
      "epoch:3 step:2860 [D loss: 0.524394, acc.: 76.56%] [G loss: 0.484326]\n",
      "epoch:3 step:2861 [D loss: 0.505508, acc.: 79.69%] [G loss: 0.594848]\n",
      "epoch:3 step:2862 [D loss: 0.510799, acc.: 75.78%] [G loss: 0.432220]\n",
      "epoch:3 step:2863 [D loss: 0.531729, acc.: 74.22%] [G loss: 0.531162]\n",
      "epoch:3 step:2864 [D loss: 0.462539, acc.: 85.16%] [G loss: 0.595996]\n",
      "epoch:3 step:2865 [D loss: 0.440031, acc.: 83.59%] [G loss: 0.669065]\n",
      "epoch:3 step:2866 [D loss: 0.469526, acc.: 78.91%] [G loss: 0.676242]\n",
      "epoch:3 step:2867 [D loss: 0.519388, acc.: 73.44%] [G loss: 0.468711]\n",
      "epoch:3 step:2868 [D loss: 0.463731, acc.: 81.25%] [G loss: 0.570396]\n",
      "epoch:3 step:2869 [D loss: 0.497137, acc.: 78.91%] [G loss: 0.526670]\n",
      "epoch:3 step:2870 [D loss: 0.487339, acc.: 77.34%] [G loss: 0.713559]\n",
      "epoch:3 step:2871 [D loss: 0.466623, acc.: 79.69%] [G loss: 0.673741]\n",
      "epoch:3 step:2872 [D loss: 0.528046, acc.: 78.12%] [G loss: 0.557709]\n",
      "epoch:3 step:2873 [D loss: 0.567096, acc.: 71.88%] [G loss: 0.514173]\n",
      "epoch:3 step:2874 [D loss: 0.533821, acc.: 76.56%] [G loss: 0.466571]\n",
      "epoch:3 step:2875 [D loss: 0.508352, acc.: 78.91%] [G loss: 0.511886]\n",
      "epoch:3 step:2876 [D loss: 0.477224, acc.: 82.03%] [G loss: 0.497010]\n",
      "epoch:3 step:2877 [D loss: 0.489798, acc.: 79.69%] [G loss: 0.432009]\n",
      "epoch:3 step:2878 [D loss: 0.514123, acc.: 76.56%] [G loss: 0.607157]\n",
      "epoch:3 step:2879 [D loss: 0.498791, acc.: 75.78%] [G loss: 0.539939]\n",
      "epoch:3 step:2880 [D loss: 0.476628, acc.: 79.69%] [G loss: 0.528141]\n",
      "epoch:3 step:2881 [D loss: 0.511755, acc.: 74.22%] [G loss: 0.496798]\n",
      "epoch:3 step:2882 [D loss: 0.513035, acc.: 71.09%] [G loss: 0.470570]\n",
      "epoch:3 step:2883 [D loss: 0.426869, acc.: 84.38%] [G loss: 0.548518]\n",
      "epoch:3 step:2884 [D loss: 0.521323, acc.: 73.44%] [G loss: 0.490120]\n",
      "epoch:3 step:2885 [D loss: 0.462252, acc.: 78.91%] [G loss: 0.647249]\n",
      "epoch:3 step:2886 [D loss: 0.459934, acc.: 76.56%] [G loss: 0.625660]\n",
      "epoch:3 step:2887 [D loss: 0.529137, acc.: 72.66%] [G loss: 0.631072]\n",
      "epoch:3 step:2888 [D loss: 0.404128, acc.: 82.03%] [G loss: 0.760586]\n",
      "epoch:3 step:2889 [D loss: 0.537861, acc.: 72.66%] [G loss: 0.556671]\n",
      "epoch:3 step:2890 [D loss: 0.514140, acc.: 73.44%] [G loss: 0.510480]\n",
      "epoch:3 step:2891 [D loss: 0.505609, acc.: 77.34%] [G loss: 0.465787]\n",
      "epoch:3 step:2892 [D loss: 0.509021, acc.: 74.22%] [G loss: 0.524795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2893 [D loss: 0.458080, acc.: 76.56%] [G loss: 0.529490]\n",
      "epoch:3 step:2894 [D loss: 0.481393, acc.: 77.34%] [G loss: 0.626533]\n",
      "epoch:3 step:2895 [D loss: 0.528488, acc.: 73.44%] [G loss: 0.621748]\n",
      "epoch:3 step:2896 [D loss: 0.511142, acc.: 73.44%] [G loss: 0.509865]\n",
      "epoch:3 step:2897 [D loss: 0.496055, acc.: 76.56%] [G loss: 0.560691]\n",
      "epoch:3 step:2898 [D loss: 0.418577, acc.: 85.94%] [G loss: 0.550386]\n",
      "epoch:3 step:2899 [D loss: 0.445381, acc.: 81.25%] [G loss: 0.642455]\n",
      "epoch:3 step:2900 [D loss: 0.448626, acc.: 79.69%] [G loss: 0.735013]\n",
      "epoch:3 step:2901 [D loss: 0.509331, acc.: 76.56%] [G loss: 0.490686]\n",
      "epoch:3 step:2902 [D loss: 0.485139, acc.: 76.56%] [G loss: 0.665938]\n",
      "epoch:3 step:2903 [D loss: 0.491137, acc.: 71.88%] [G loss: 0.661659]\n",
      "epoch:3 step:2904 [D loss: 0.489978, acc.: 78.91%] [G loss: 0.529256]\n",
      "epoch:3 step:2905 [D loss: 0.475418, acc.: 78.12%] [G loss: 0.604137]\n",
      "epoch:3 step:2906 [D loss: 0.510686, acc.: 75.78%] [G loss: 0.690765]\n",
      "epoch:3 step:2907 [D loss: 0.392387, acc.: 85.94%] [G loss: 0.636092]\n",
      "epoch:3 step:2908 [D loss: 0.479684, acc.: 77.34%] [G loss: 0.658475]\n",
      "epoch:3 step:2909 [D loss: 0.490590, acc.: 78.91%] [G loss: 0.610935]\n",
      "epoch:3 step:2910 [D loss: 0.481071, acc.: 80.47%] [G loss: 0.639036]\n",
      "epoch:3 step:2911 [D loss: 0.450717, acc.: 77.34%] [G loss: 0.620029]\n",
      "epoch:3 step:2912 [D loss: 0.468863, acc.: 85.16%] [G loss: 0.690613]\n",
      "epoch:3 step:2913 [D loss: 0.523219, acc.: 74.22%] [G loss: 0.712710]\n",
      "epoch:3 step:2914 [D loss: 0.417810, acc.: 82.81%] [G loss: 0.620307]\n",
      "epoch:3 step:2915 [D loss: 0.472050, acc.: 81.25%] [G loss: 0.598075]\n",
      "epoch:3 step:2916 [D loss: 0.497709, acc.: 75.78%] [G loss: 0.475841]\n",
      "epoch:3 step:2917 [D loss: 0.586684, acc.: 65.62%] [G loss: 0.471931]\n",
      "epoch:3 step:2918 [D loss: 0.569564, acc.: 67.19%] [G loss: 0.450528]\n",
      "epoch:3 step:2919 [D loss: 0.535571, acc.: 74.22%] [G loss: 0.448426]\n",
      "epoch:3 step:2920 [D loss: 0.548698, acc.: 67.97%] [G loss: 0.503639]\n",
      "epoch:3 step:2921 [D loss: 0.500199, acc.: 77.34%] [G loss: 0.504238]\n",
      "epoch:3 step:2922 [D loss: 0.462850, acc.: 79.69%] [G loss: 0.540779]\n",
      "epoch:3 step:2923 [D loss: 0.487191, acc.: 81.25%] [G loss: 0.549462]\n",
      "epoch:3 step:2924 [D loss: 0.567282, acc.: 68.75%] [G loss: 0.469999]\n",
      "epoch:3 step:2925 [D loss: 0.478411, acc.: 81.25%] [G loss: 0.601819]\n",
      "epoch:3 step:2926 [D loss: 0.517312, acc.: 77.34%] [G loss: 0.657398]\n",
      "epoch:3 step:2927 [D loss: 0.495609, acc.: 78.91%] [G loss: 0.719892]\n",
      "epoch:3 step:2928 [D loss: 0.484217, acc.: 78.91%] [G loss: 0.675680]\n",
      "epoch:3 step:2929 [D loss: 0.509115, acc.: 78.91%] [G loss: 0.736655]\n",
      "epoch:3 step:2930 [D loss: 0.433771, acc.: 86.72%] [G loss: 0.784220]\n",
      "epoch:3 step:2931 [D loss: 0.574638, acc.: 68.75%] [G loss: 0.604020]\n",
      "epoch:3 step:2932 [D loss: 0.510673, acc.: 75.00%] [G loss: 0.644646]\n",
      "epoch:3 step:2933 [D loss: 0.541063, acc.: 72.66%] [G loss: 0.613730]\n",
      "epoch:3 step:2934 [D loss: 0.538178, acc.: 67.19%] [G loss: 0.585134]\n",
      "epoch:3 step:2935 [D loss: 0.565178, acc.: 69.53%] [G loss: 0.500185]\n",
      "epoch:3 step:2936 [D loss: 0.536437, acc.: 71.88%] [G loss: 0.573677]\n",
      "epoch:3 step:2937 [D loss: 0.465510, acc.: 76.56%] [G loss: 0.762009]\n",
      "epoch:3 step:2938 [D loss: 0.559360, acc.: 69.53%] [G loss: 0.557678]\n",
      "epoch:3 step:2939 [D loss: 0.458468, acc.: 78.91%] [G loss: 0.534017]\n",
      "epoch:3 step:2940 [D loss: 0.516283, acc.: 76.56%] [G loss: 0.500331]\n",
      "epoch:3 step:2941 [D loss: 0.465181, acc.: 77.34%] [G loss: 0.732935]\n",
      "epoch:3 step:2942 [D loss: 0.388477, acc.: 85.16%] [G loss: 0.756184]\n",
      "epoch:3 step:2943 [D loss: 0.492833, acc.: 78.12%] [G loss: 0.603795]\n",
      "epoch:3 step:2944 [D loss: 0.582452, acc.: 70.31%] [G loss: 0.589058]\n",
      "epoch:3 step:2945 [D loss: 0.465328, acc.: 77.34%] [G loss: 0.578470]\n",
      "epoch:3 step:2946 [D loss: 0.546616, acc.: 70.31%] [G loss: 0.557195]\n",
      "epoch:3 step:2947 [D loss: 0.527511, acc.: 70.31%] [G loss: 0.761214]\n",
      "epoch:3 step:2948 [D loss: 0.573958, acc.: 71.09%] [G loss: 0.518536]\n",
      "epoch:3 step:2949 [D loss: 0.516110, acc.: 71.88%] [G loss: 0.620559]\n",
      "epoch:3 step:2950 [D loss: 0.594006, acc.: 67.97%] [G loss: 0.610939]\n",
      "epoch:3 step:2951 [D loss: 0.535106, acc.: 74.22%] [G loss: 0.450819]\n",
      "epoch:3 step:2952 [D loss: 0.505299, acc.: 75.00%] [G loss: 0.478215]\n",
      "epoch:3 step:2953 [D loss: 0.484784, acc.: 80.47%] [G loss: 0.720218]\n",
      "epoch:3 step:2954 [D loss: 0.578925, acc.: 68.75%] [G loss: 0.548995]\n",
      "epoch:3 step:2955 [D loss: 0.460604, acc.: 84.38%] [G loss: 0.733464]\n",
      "epoch:3 step:2956 [D loss: 0.519038, acc.: 77.34%] [G loss: 0.571305]\n",
      "epoch:3 step:2957 [D loss: 0.488398, acc.: 78.12%] [G loss: 0.591331]\n",
      "epoch:3 step:2958 [D loss: 0.567930, acc.: 73.44%] [G loss: 0.528984]\n",
      "epoch:3 step:2959 [D loss: 0.491827, acc.: 78.12%] [G loss: 0.492221]\n",
      "epoch:3 step:2960 [D loss: 0.427812, acc.: 80.47%] [G loss: 0.741820]\n",
      "epoch:3 step:2961 [D loss: 0.607451, acc.: 61.72%] [G loss: 0.536213]\n",
      "epoch:3 step:2962 [D loss: 0.479160, acc.: 77.34%] [G loss: 0.836081]\n",
      "epoch:3 step:2963 [D loss: 0.415531, acc.: 81.25%] [G loss: 0.749993]\n",
      "epoch:3 step:2964 [D loss: 0.541256, acc.: 68.75%] [G loss: 0.593729]\n",
      "epoch:3 step:2965 [D loss: 0.459850, acc.: 79.69%] [G loss: 0.581777]\n",
      "epoch:3 step:2966 [D loss: 0.428575, acc.: 82.81%] [G loss: 0.791885]\n",
      "epoch:3 step:2967 [D loss: 0.507847, acc.: 77.34%] [G loss: 0.561607]\n",
      "epoch:3 step:2968 [D loss: 0.477247, acc.: 78.12%] [G loss: 0.545618]\n",
      "epoch:3 step:2969 [D loss: 0.528416, acc.: 73.44%] [G loss: 0.475684]\n",
      "epoch:3 step:2970 [D loss: 0.445140, acc.: 82.03%] [G loss: 0.609766]\n",
      "epoch:3 step:2971 [D loss: 0.543793, acc.: 71.09%] [G loss: 0.611802]\n",
      "epoch:3 step:2972 [D loss: 0.531595, acc.: 69.53%] [G loss: 0.584872]\n",
      "epoch:3 step:2973 [D loss: 0.429507, acc.: 82.81%] [G loss: 0.525207]\n",
      "epoch:3 step:2974 [D loss: 0.432913, acc.: 85.16%] [G loss: 0.680663]\n",
      "epoch:3 step:2975 [D loss: 0.507185, acc.: 74.22%] [G loss: 0.631203]\n",
      "epoch:3 step:2976 [D loss: 0.487226, acc.: 78.91%] [G loss: 0.554121]\n",
      "epoch:3 step:2977 [D loss: 0.463432, acc.: 80.47%] [G loss: 0.624152]\n",
      "epoch:3 step:2978 [D loss: 0.501269, acc.: 77.34%] [G loss: 0.569590]\n",
      "epoch:3 step:2979 [D loss: 0.510844, acc.: 75.00%] [G loss: 0.578017]\n",
      "epoch:3 step:2980 [D loss: 0.533439, acc.: 73.44%] [G loss: 0.549148]\n",
      "epoch:3 step:2981 [D loss: 0.473830, acc.: 78.91%] [G loss: 0.580735]\n",
      "epoch:3 step:2982 [D loss: 0.436197, acc.: 82.81%] [G loss: 0.545143]\n",
      "epoch:3 step:2983 [D loss: 0.497630, acc.: 78.12%] [G loss: 0.631869]\n",
      "epoch:3 step:2984 [D loss: 0.455119, acc.: 78.12%] [G loss: 0.550570]\n",
      "epoch:3 step:2985 [D loss: 0.531361, acc.: 72.66%] [G loss: 0.512215]\n",
      "epoch:3 step:2986 [D loss: 0.489186, acc.: 78.91%] [G loss: 0.556862]\n",
      "epoch:3 step:2987 [D loss: 0.507495, acc.: 76.56%] [G loss: 0.622126]\n",
      "epoch:3 step:2988 [D loss: 0.521202, acc.: 71.09%] [G loss: 0.628466]\n",
      "epoch:3 step:2989 [D loss: 0.487349, acc.: 76.56%] [G loss: 0.648695]\n",
      "epoch:3 step:2990 [D loss: 0.485923, acc.: 78.91%] [G loss: 0.605241]\n",
      "epoch:3 step:2991 [D loss: 0.544542, acc.: 70.31%] [G loss: 0.451876]\n",
      "epoch:3 step:2992 [D loss: 0.514176, acc.: 77.34%] [G loss: 0.472494]\n",
      "epoch:3 step:2993 [D loss: 0.479548, acc.: 78.91%] [G loss: 0.580204]\n",
      "epoch:3 step:2994 [D loss: 0.491124, acc.: 75.78%] [G loss: 0.640160]\n",
      "epoch:3 step:2995 [D loss: 0.529939, acc.: 69.53%] [G loss: 0.561947]\n",
      "epoch:3 step:2996 [D loss: 0.532889, acc.: 76.56%] [G loss: 0.462844]\n",
      "epoch:3 step:2997 [D loss: 0.540077, acc.: 73.44%] [G loss: 0.546567]\n",
      "epoch:3 step:2998 [D loss: 0.628604, acc.: 67.97%] [G loss: 0.445978]\n",
      "epoch:3 step:2999 [D loss: 0.515158, acc.: 73.44%] [G loss: 0.542552]\n",
      "epoch:3 step:3000 [D loss: 0.481950, acc.: 82.03%] [G loss: 0.545604]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.189616\n",
      "FID: 64.560898\n",
      "0 = 13.200225973510754\n",
      "1 = 0.12454235223653981\n",
      "2 = 0.9607999920845032\n",
      "3 = 0.9215999841690063\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9215999841690063\n",
      "7 = 9.48574992971422\n",
      "8 = 0.15773827346284433\n",
      "9 = 0.8895999789237976\n",
      "10 = 0.8514000177383423\n",
      "11 = 0.9277999997138977\n",
      "12 = 0.9218276143074036\n",
      "13 = 0.8514000177383423\n",
      "14 = 5.189630508422852\n",
      "15 = 7.882967472076416\n",
      "16 = 0.3350611627101898\n",
      "17 = 5.1896162033081055\n",
      "18 = 64.56089782714844\n",
      "epoch:3 step:3001 [D loss: 0.408283, acc.: 86.72%] [G loss: 0.625439]\n",
      "epoch:3 step:3002 [D loss: 0.505416, acc.: 77.34%] [G loss: 0.663934]\n",
      "epoch:3 step:3003 [D loss: 0.463081, acc.: 80.47%] [G loss: 0.675594]\n",
      "epoch:3 step:3004 [D loss: 0.463073, acc.: 78.91%] [G loss: 0.509486]\n",
      "epoch:3 step:3005 [D loss: 0.414974, acc.: 84.38%] [G loss: 0.653133]\n",
      "epoch:3 step:3006 [D loss: 0.465150, acc.: 78.12%] [G loss: 0.650099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3007 [D loss: 0.577198, acc.: 67.97%] [G loss: 0.671251]\n",
      "epoch:3 step:3008 [D loss: 0.502990, acc.: 75.78%] [G loss: 0.686883]\n",
      "epoch:3 step:3009 [D loss: 0.438202, acc.: 78.91%] [G loss: 0.719008]\n",
      "epoch:3 step:3010 [D loss: 0.497617, acc.: 74.22%] [G loss: 0.767622]\n",
      "epoch:3 step:3011 [D loss: 0.534662, acc.: 74.22%] [G loss: 0.501142]\n",
      "epoch:3 step:3012 [D loss: 0.508394, acc.: 75.00%] [G loss: 0.494099]\n",
      "epoch:3 step:3013 [D loss: 0.521798, acc.: 71.88%] [G loss: 0.437056]\n",
      "epoch:3 step:3014 [D loss: 0.538815, acc.: 78.12%] [G loss: 0.469422]\n",
      "epoch:3 step:3015 [D loss: 0.483240, acc.: 79.69%] [G loss: 0.567862]\n",
      "epoch:3 step:3016 [D loss: 0.458176, acc.: 81.25%] [G loss: 0.528595]\n",
      "epoch:3 step:3017 [D loss: 0.491899, acc.: 77.34%] [G loss: 0.617933]\n",
      "epoch:3 step:3018 [D loss: 0.439918, acc.: 79.69%] [G loss: 0.755855]\n",
      "epoch:3 step:3019 [D loss: 0.392136, acc.: 85.94%] [G loss: 0.897913]\n",
      "epoch:3 step:3020 [D loss: 0.500222, acc.: 75.78%] [G loss: 0.759552]\n",
      "epoch:3 step:3021 [D loss: 0.556150, acc.: 71.09%] [G loss: 0.630429]\n",
      "epoch:3 step:3022 [D loss: 0.545042, acc.: 72.66%] [G loss: 0.480017]\n",
      "epoch:3 step:3023 [D loss: 0.481746, acc.: 78.12%] [G loss: 0.590089]\n",
      "epoch:3 step:3024 [D loss: 0.483669, acc.: 78.91%] [G loss: 0.639813]\n",
      "epoch:3 step:3025 [D loss: 0.628956, acc.: 62.50%] [G loss: 0.402136]\n",
      "epoch:3 step:3026 [D loss: 0.511217, acc.: 78.12%] [G loss: 0.575410]\n",
      "epoch:3 step:3027 [D loss: 0.503767, acc.: 75.00%] [G loss: 0.497066]\n",
      "epoch:3 step:3028 [D loss: 0.446595, acc.: 84.38%] [G loss: 0.587803]\n",
      "epoch:3 step:3029 [D loss: 0.477029, acc.: 79.69%] [G loss: 0.598221]\n",
      "epoch:3 step:3030 [D loss: 0.532318, acc.: 74.22%] [G loss: 0.514050]\n",
      "epoch:3 step:3031 [D loss: 0.512150, acc.: 75.78%] [G loss: 0.620281]\n",
      "epoch:3 step:3032 [D loss: 0.463859, acc.: 80.47%] [G loss: 0.641240]\n",
      "epoch:3 step:3033 [D loss: 0.441154, acc.: 80.47%] [G loss: 0.740112]\n",
      "epoch:3 step:3034 [D loss: 0.479416, acc.: 81.25%] [G loss: 0.700286]\n",
      "epoch:3 step:3035 [D loss: 0.500133, acc.: 77.34%] [G loss: 0.656946]\n",
      "epoch:3 step:3036 [D loss: 0.584328, acc.: 70.31%] [G loss: 0.568067]\n",
      "epoch:3 step:3037 [D loss: 0.573468, acc.: 67.97%] [G loss: 0.356804]\n",
      "epoch:3 step:3038 [D loss: 0.535368, acc.: 73.44%] [G loss: 0.401841]\n",
      "epoch:3 step:3039 [D loss: 0.525055, acc.: 72.66%] [G loss: 0.466291]\n",
      "epoch:3 step:3040 [D loss: 0.521722, acc.: 76.56%] [G loss: 0.511084]\n",
      "epoch:3 step:3041 [D loss: 0.450134, acc.: 82.03%] [G loss: 0.603965]\n",
      "epoch:3 step:3042 [D loss: 0.468568, acc.: 80.47%] [G loss: 0.792206]\n",
      "epoch:3 step:3043 [D loss: 0.407163, acc.: 85.16%] [G loss: 0.881518]\n",
      "epoch:3 step:3044 [D loss: 0.516714, acc.: 77.34%] [G loss: 0.627874]\n",
      "epoch:3 step:3045 [D loss: 0.496958, acc.: 74.22%] [G loss: 0.578429]\n",
      "epoch:3 step:3046 [D loss: 0.514394, acc.: 73.44%] [G loss: 0.541062]\n",
      "epoch:3 step:3047 [D loss: 0.514654, acc.: 73.44%] [G loss: 0.531292]\n",
      "epoch:3 step:3048 [D loss: 0.525522, acc.: 72.66%] [G loss: 0.542614]\n",
      "epoch:3 step:3049 [D loss: 0.500355, acc.: 77.34%] [G loss: 0.484513]\n",
      "epoch:3 step:3050 [D loss: 0.504113, acc.: 78.91%] [G loss: 0.497892]\n",
      "epoch:3 step:3051 [D loss: 0.478165, acc.: 76.56%] [G loss: 0.577805]\n",
      "epoch:3 step:3052 [D loss: 0.486504, acc.: 77.34%] [G loss: 0.527706]\n",
      "epoch:3 step:3053 [D loss: 0.546386, acc.: 75.78%] [G loss: 0.384568]\n",
      "epoch:3 step:3054 [D loss: 0.494257, acc.: 80.47%] [G loss: 0.505725]\n",
      "epoch:3 step:3055 [D loss: 0.420207, acc.: 82.81%] [G loss: 0.753372]\n",
      "epoch:3 step:3056 [D loss: 0.537010, acc.: 71.88%] [G loss: 0.543700]\n",
      "epoch:3 step:3057 [D loss: 0.532793, acc.: 74.22%] [G loss: 0.492814]\n",
      "epoch:3 step:3058 [D loss: 0.567253, acc.: 68.75%] [G loss: 0.558860]\n",
      "epoch:3 step:3059 [D loss: 0.569783, acc.: 69.53%] [G loss: 0.561541]\n",
      "epoch:3 step:3060 [D loss: 0.467842, acc.: 79.69%] [G loss: 0.645524]\n",
      "epoch:3 step:3061 [D loss: 0.559547, acc.: 67.19%] [G loss: 0.532204]\n",
      "epoch:3 step:3062 [D loss: 0.540085, acc.: 75.78%] [G loss: 0.538385]\n",
      "epoch:3 step:3063 [D loss: 0.523340, acc.: 74.22%] [G loss: 0.446887]\n",
      "epoch:3 step:3064 [D loss: 0.500239, acc.: 76.56%] [G loss: 0.488043]\n",
      "epoch:3 step:3065 [D loss: 0.491441, acc.: 75.00%] [G loss: 0.590015]\n",
      "epoch:3 step:3066 [D loss: 0.560355, acc.: 71.09%] [G loss: 0.618508]\n",
      "epoch:3 step:3067 [D loss: 0.503156, acc.: 78.91%] [G loss: 0.655822]\n",
      "epoch:3 step:3068 [D loss: 0.509692, acc.: 72.66%] [G loss: 0.682907]\n",
      "epoch:3 step:3069 [D loss: 0.537848, acc.: 70.31%] [G loss: 0.647078]\n",
      "epoch:3 step:3070 [D loss: 0.505700, acc.: 76.56%] [G loss: 0.661674]\n",
      "epoch:3 step:3071 [D loss: 0.546660, acc.: 73.44%] [G loss: 0.552847]\n",
      "epoch:3 step:3072 [D loss: 0.511137, acc.: 75.78%] [G loss: 0.553660]\n",
      "epoch:3 step:3073 [D loss: 0.468252, acc.: 82.81%] [G loss: 0.524799]\n",
      "epoch:3 step:3074 [D loss: 0.578678, acc.: 72.66%] [G loss: 0.467818]\n",
      "epoch:3 step:3075 [D loss: 0.503419, acc.: 74.22%] [G loss: 0.397099]\n",
      "epoch:3 step:3076 [D loss: 0.569325, acc.: 64.84%] [G loss: 0.591030]\n",
      "epoch:3 step:3077 [D loss: 0.546946, acc.: 72.66%] [G loss: 0.468773]\n",
      "epoch:3 step:3078 [D loss: 0.544828, acc.: 70.31%] [G loss: 0.481943]\n",
      "epoch:3 step:3079 [D loss: 0.536315, acc.: 73.44%] [G loss: 0.422829]\n",
      "epoch:3 step:3080 [D loss: 0.522662, acc.: 77.34%] [G loss: 0.491997]\n",
      "epoch:3 step:3081 [D loss: 0.562041, acc.: 68.75%] [G loss: 0.500738]\n",
      "epoch:3 step:3082 [D loss: 0.467407, acc.: 78.12%] [G loss: 0.574823]\n",
      "epoch:3 step:3083 [D loss: 0.524685, acc.: 74.22%] [G loss: 0.668388]\n",
      "epoch:3 step:3084 [D loss: 0.492004, acc.: 75.00%] [G loss: 0.548874]\n",
      "epoch:3 step:3085 [D loss: 0.526670, acc.: 74.22%] [G loss: 0.529798]\n",
      "epoch:3 step:3086 [D loss: 0.547361, acc.: 73.44%] [G loss: 0.479561]\n",
      "epoch:3 step:3087 [D loss: 0.497765, acc.: 77.34%] [G loss: 0.560014]\n",
      "epoch:3 step:3088 [D loss: 0.543921, acc.: 75.78%] [G loss: 0.496119]\n",
      "epoch:3 step:3089 [D loss: 0.566724, acc.: 70.31%] [G loss: 0.423759]\n",
      "epoch:3 step:3090 [D loss: 0.511840, acc.: 76.56%] [G loss: 0.435958]\n",
      "epoch:3 step:3091 [D loss: 0.518052, acc.: 73.44%] [G loss: 0.525273]\n",
      "epoch:3 step:3092 [D loss: 0.535052, acc.: 71.88%] [G loss: 0.446301]\n",
      "epoch:3 step:3093 [D loss: 0.527169, acc.: 78.12%] [G loss: 0.467766]\n",
      "epoch:3 step:3094 [D loss: 0.453552, acc.: 87.50%] [G loss: 0.508867]\n",
      "epoch:3 step:3095 [D loss: 0.459872, acc.: 83.59%] [G loss: 0.452677]\n",
      "epoch:3 step:3096 [D loss: 0.438311, acc.: 85.16%] [G loss: 0.642480]\n",
      "epoch:3 step:3097 [D loss: 0.473987, acc.: 77.34%] [G loss: 0.634888]\n",
      "epoch:3 step:3098 [D loss: 0.534989, acc.: 71.88%] [G loss: 0.581377]\n",
      "epoch:3 step:3099 [D loss: 0.597344, acc.: 62.50%] [G loss: 0.477905]\n",
      "epoch:3 step:3100 [D loss: 0.540334, acc.: 76.56%] [G loss: 0.491936]\n",
      "epoch:3 step:3101 [D loss: 0.494889, acc.: 75.00%] [G loss: 0.510031]\n",
      "epoch:3 step:3102 [D loss: 0.555592, acc.: 75.00%] [G loss: 0.498944]\n",
      "epoch:3 step:3103 [D loss: 0.526229, acc.: 75.78%] [G loss: 0.455601]\n",
      "epoch:3 step:3104 [D loss: 0.475175, acc.: 76.56%] [G loss: 0.525993]\n",
      "epoch:3 step:3105 [D loss: 0.489541, acc.: 77.34%] [G loss: 0.584308]\n",
      "epoch:3 step:3106 [D loss: 0.481465, acc.: 78.12%] [G loss: 0.462256]\n",
      "epoch:3 step:3107 [D loss: 0.454472, acc.: 82.81%] [G loss: 0.536567]\n",
      "epoch:3 step:3108 [D loss: 0.498397, acc.: 78.91%] [G loss: 0.487280]\n",
      "epoch:3 step:3109 [D loss: 0.486812, acc.: 81.25%] [G loss: 0.513211]\n",
      "epoch:3 step:3110 [D loss: 0.490521, acc.: 78.91%] [G loss: 0.421435]\n",
      "epoch:3 step:3111 [D loss: 0.480727, acc.: 75.78%] [G loss: 0.512365]\n",
      "epoch:3 step:3112 [D loss: 0.520286, acc.: 78.91%] [G loss: 0.461158]\n",
      "epoch:3 step:3113 [D loss: 0.472309, acc.: 78.12%] [G loss: 0.639355]\n",
      "epoch:3 step:3114 [D loss: 0.495979, acc.: 79.69%] [G loss: 0.559296]\n",
      "epoch:3 step:3115 [D loss: 0.445933, acc.: 80.47%] [G loss: 0.638883]\n",
      "epoch:3 step:3116 [D loss: 0.467244, acc.: 78.12%] [G loss: 0.616787]\n",
      "epoch:3 step:3117 [D loss: 0.526104, acc.: 75.78%] [G loss: 0.511586]\n",
      "epoch:3 step:3118 [D loss: 0.443332, acc.: 78.91%] [G loss: 0.616871]\n",
      "epoch:3 step:3119 [D loss: 0.514754, acc.: 73.44%] [G loss: 0.517819]\n",
      "epoch:3 step:3120 [D loss: 0.406155, acc.: 84.38%] [G loss: 0.759075]\n",
      "epoch:3 step:3121 [D loss: 0.511792, acc.: 73.44%] [G loss: 0.590351]\n",
      "epoch:3 step:3122 [D loss: 0.428381, acc.: 82.03%] [G loss: 0.683777]\n",
      "epoch:3 step:3123 [D loss: 0.449448, acc.: 77.34%] [G loss: 0.729767]\n",
      "epoch:3 step:3124 [D loss: 0.440835, acc.: 84.38%] [G loss: 0.785454]\n",
      "epoch:3 step:3125 [D loss: 0.375796, acc.: 86.72%] [G loss: 0.976840]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3126 [D loss: 0.418885, acc.: 84.38%] [G loss: 0.879441]\n",
      "epoch:3 step:3127 [D loss: 0.649468, acc.: 65.62%] [G loss: 0.583726]\n",
      "epoch:3 step:3128 [D loss: 0.531569, acc.: 72.66%] [G loss: 0.431083]\n",
      "epoch:3 step:3129 [D loss: 0.488847, acc.: 78.12%] [G loss: 0.515096]\n",
      "epoch:3 step:3130 [D loss: 0.522920, acc.: 72.66%] [G loss: 0.545753]\n",
      "epoch:3 step:3131 [D loss: 0.492282, acc.: 74.22%] [G loss: 0.560353]\n",
      "epoch:3 step:3132 [D loss: 0.497803, acc.: 78.91%] [G loss: 0.535095]\n",
      "epoch:3 step:3133 [D loss: 0.504235, acc.: 75.00%] [G loss: 0.606503]\n",
      "epoch:3 step:3134 [D loss: 0.567200, acc.: 67.19%] [G loss: 0.501302]\n",
      "epoch:3 step:3135 [D loss: 0.517113, acc.: 75.00%] [G loss: 0.581211]\n",
      "epoch:3 step:3136 [D loss: 0.411335, acc.: 89.06%] [G loss: 0.658738]\n",
      "epoch:3 step:3137 [D loss: 0.456322, acc.: 82.03%] [G loss: 0.589313]\n",
      "epoch:3 step:3138 [D loss: 0.493964, acc.: 75.78%] [G loss: 0.551745]\n",
      "epoch:3 step:3139 [D loss: 0.436088, acc.: 84.38%] [G loss: 0.676880]\n",
      "epoch:3 step:3140 [D loss: 0.505865, acc.: 76.56%] [G loss: 0.632307]\n",
      "epoch:3 step:3141 [D loss: 0.519219, acc.: 72.66%] [G loss: 0.562671]\n",
      "epoch:3 step:3142 [D loss: 0.488022, acc.: 79.69%] [G loss: 0.579237]\n",
      "epoch:3 step:3143 [D loss: 0.464433, acc.: 80.47%] [G loss: 0.679645]\n",
      "epoch:3 step:3144 [D loss: 0.430257, acc.: 86.72%] [G loss: 0.624127]\n",
      "epoch:3 step:3145 [D loss: 0.498628, acc.: 75.78%] [G loss: 0.602004]\n",
      "epoch:3 step:3146 [D loss: 0.498481, acc.: 75.00%] [G loss: 0.679971]\n",
      "epoch:3 step:3147 [D loss: 0.523970, acc.: 71.88%] [G loss: 0.626505]\n",
      "epoch:3 step:3148 [D loss: 0.438157, acc.: 81.25%] [G loss: 0.709557]\n",
      "epoch:3 step:3149 [D loss: 0.563231, acc.: 71.09%] [G loss: 0.658766]\n",
      "epoch:3 step:3150 [D loss: 0.519142, acc.: 73.44%] [G loss: 0.529973]\n",
      "epoch:3 step:3151 [D loss: 0.481692, acc.: 79.69%] [G loss: 0.680823]\n",
      "epoch:3 step:3152 [D loss: 0.550264, acc.: 73.44%] [G loss: 0.526875]\n",
      "epoch:3 step:3153 [D loss: 0.597910, acc.: 67.19%] [G loss: 0.464776]\n",
      "epoch:3 step:3154 [D loss: 0.490997, acc.: 75.78%] [G loss: 0.527029]\n",
      "epoch:3 step:3155 [D loss: 0.449619, acc.: 78.91%] [G loss: 0.756459]\n",
      "epoch:3 step:3156 [D loss: 0.498786, acc.: 76.56%] [G loss: 0.861581]\n",
      "epoch:3 step:3157 [D loss: 0.473761, acc.: 77.34%] [G loss: 0.839079]\n",
      "epoch:3 step:3158 [D loss: 0.436711, acc.: 79.69%] [G loss: 0.849104]\n",
      "epoch:3 step:3159 [D loss: 0.604667, acc.: 67.97%] [G loss: 0.615844]\n",
      "epoch:3 step:3160 [D loss: 0.665197, acc.: 57.81%] [G loss: 0.408936]\n",
      "epoch:3 step:3161 [D loss: 0.522782, acc.: 75.00%] [G loss: 0.482098]\n",
      "epoch:3 step:3162 [D loss: 0.480800, acc.: 78.12%] [G loss: 0.694483]\n",
      "epoch:3 step:3163 [D loss: 0.477962, acc.: 78.12%] [G loss: 0.635454]\n",
      "epoch:3 step:3164 [D loss: 0.527613, acc.: 75.00%] [G loss: 0.624882]\n",
      "epoch:3 step:3165 [D loss: 0.449249, acc.: 78.12%] [G loss: 0.692684]\n",
      "epoch:3 step:3166 [D loss: 0.473699, acc.: 75.78%] [G loss: 0.663325]\n",
      "epoch:3 step:3167 [D loss: 0.531393, acc.: 71.09%] [G loss: 0.491893]\n",
      "epoch:3 step:3168 [D loss: 0.438298, acc.: 82.03%] [G loss: 0.582885]\n",
      "epoch:3 step:3169 [D loss: 0.409439, acc.: 83.59%] [G loss: 0.655412]\n",
      "epoch:3 step:3170 [D loss: 0.448976, acc.: 78.12%] [G loss: 0.643712]\n",
      "epoch:3 step:3171 [D loss: 0.467953, acc.: 75.00%] [G loss: 0.596893]\n",
      "epoch:3 step:3172 [D loss: 0.487048, acc.: 77.34%] [G loss: 0.541446]\n",
      "epoch:3 step:3173 [D loss: 0.506269, acc.: 73.44%] [G loss: 0.588916]\n",
      "epoch:3 step:3174 [D loss: 0.504127, acc.: 74.22%] [G loss: 0.611789]\n",
      "epoch:3 step:3175 [D loss: 0.439171, acc.: 79.69%] [G loss: 0.603513]\n",
      "epoch:3 step:3176 [D loss: 0.548349, acc.: 72.66%] [G loss: 0.507429]\n",
      "epoch:3 step:3177 [D loss: 0.494541, acc.: 75.78%] [G loss: 0.499581]\n",
      "epoch:3 step:3178 [D loss: 0.569903, acc.: 69.53%] [G loss: 0.647478]\n",
      "epoch:3 step:3179 [D loss: 0.494460, acc.: 75.78%] [G loss: 0.602361]\n",
      "epoch:3 step:3180 [D loss: 0.545429, acc.: 71.09%] [G loss: 0.493304]\n",
      "epoch:3 step:3181 [D loss: 0.502505, acc.: 81.25%] [G loss: 0.502070]\n",
      "epoch:3 step:3182 [D loss: 0.508503, acc.: 78.91%] [G loss: 0.560408]\n",
      "epoch:3 step:3183 [D loss: 0.528508, acc.: 71.88%] [G loss: 0.623228]\n",
      "epoch:3 step:3184 [D loss: 0.512781, acc.: 74.22%] [G loss: 0.696167]\n",
      "epoch:3 step:3185 [D loss: 0.513660, acc.: 75.00%] [G loss: 0.617540]\n",
      "epoch:3 step:3186 [D loss: 0.520627, acc.: 76.56%] [G loss: 0.625429]\n",
      "epoch:3 step:3187 [D loss: 0.628041, acc.: 62.50%] [G loss: 0.421441]\n",
      "epoch:3 step:3188 [D loss: 0.512509, acc.: 75.00%] [G loss: 0.462780]\n",
      "epoch:3 step:3189 [D loss: 0.458040, acc.: 82.03%] [G loss: 0.549893]\n",
      "epoch:3 step:3190 [D loss: 0.521456, acc.: 77.34%] [G loss: 0.444769]\n",
      "epoch:3 step:3191 [D loss: 0.556647, acc.: 75.78%] [G loss: 0.427005]\n",
      "epoch:3 step:3192 [D loss: 0.451658, acc.: 82.03%] [G loss: 0.527906]\n",
      "epoch:3 step:3193 [D loss: 0.461045, acc.: 81.25%] [G loss: 0.573131]\n",
      "epoch:3 step:3194 [D loss: 0.487068, acc.: 78.12%] [G loss: 0.514541]\n",
      "epoch:3 step:3195 [D loss: 0.443576, acc.: 81.25%] [G loss: 0.562307]\n",
      "epoch:3 step:3196 [D loss: 0.477312, acc.: 74.22%] [G loss: 0.645939]\n",
      "epoch:3 step:3197 [D loss: 0.613323, acc.: 64.06%] [G loss: 0.590397]\n",
      "epoch:3 step:3198 [D loss: 0.549265, acc.: 71.88%] [G loss: 0.653571]\n",
      "epoch:3 step:3199 [D loss: 0.566607, acc.: 74.22%] [G loss: 0.576695]\n",
      "epoch:3 step:3200 [D loss: 0.569004, acc.: 69.53%] [G loss: 0.429005]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.136649\n",
      "FID: 67.670364\n",
      "0 = 13.226402902126333\n",
      "1 = 0.12155653598149026\n",
      "2 = 0.9585999846458435\n",
      "3 = 0.9172000288963318\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9172000288963318\n",
      "7 = 9.610302368021008\n",
      "8 = 0.16116351210864346\n",
      "9 = 0.8894000053405762\n",
      "10 = 0.857200026512146\n",
      "11 = 0.9215999841690063\n",
      "12 = 0.916203498840332\n",
      "13 = 0.857200026512146\n",
      "14 = 5.136664867401123\n",
      "15 = 8.0233154296875\n",
      "16 = 0.33446890115737915\n",
      "17 = 5.136649131774902\n",
      "18 = 67.67036437988281\n",
      "epoch:3 step:3201 [D loss: 0.495682, acc.: 75.78%] [G loss: 0.544617]\n",
      "epoch:3 step:3202 [D loss: 0.502528, acc.: 71.88%] [G loss: 0.602110]\n",
      "epoch:3 step:3203 [D loss: 0.456407, acc.: 82.81%] [G loss: 0.501933]\n",
      "epoch:3 step:3204 [D loss: 0.506006, acc.: 77.34%] [G loss: 0.572125]\n",
      "epoch:3 step:3205 [D loss: 0.475656, acc.: 82.81%] [G loss: 0.501535]\n",
      "epoch:3 step:3206 [D loss: 0.480767, acc.: 78.12%] [G loss: 0.533680]\n",
      "epoch:3 step:3207 [D loss: 0.545386, acc.: 68.75%] [G loss: 0.619381]\n",
      "epoch:3 step:3208 [D loss: 0.440748, acc.: 82.03%] [G loss: 0.676867]\n",
      "epoch:3 step:3209 [D loss: 0.421776, acc.: 83.59%] [G loss: 0.706160]\n",
      "epoch:3 step:3210 [D loss: 0.457234, acc.: 81.25%] [G loss: 0.766420]\n",
      "epoch:3 step:3211 [D loss: 0.599552, acc.: 71.09%] [G loss: 0.502302]\n",
      "epoch:3 step:3212 [D loss: 0.563686, acc.: 65.62%] [G loss: 0.398216]\n",
      "epoch:3 step:3213 [D loss: 0.484918, acc.: 78.12%] [G loss: 0.564934]\n",
      "epoch:3 step:3214 [D loss: 0.469043, acc.: 77.34%] [G loss: 0.632512]\n",
      "epoch:3 step:3215 [D loss: 0.587367, acc.: 67.97%] [G loss: 0.500207]\n",
      "epoch:3 step:3216 [D loss: 0.527996, acc.: 74.22%] [G loss: 0.494209]\n",
      "epoch:3 step:3217 [D loss: 0.506356, acc.: 75.00%] [G loss: 0.679832]\n",
      "epoch:3 step:3218 [D loss: 0.521288, acc.: 75.00%] [G loss: 0.690828]\n",
      "epoch:3 step:3219 [D loss: 0.491924, acc.: 75.78%] [G loss: 0.543378]\n",
      "epoch:3 step:3220 [D loss: 0.486353, acc.: 78.91%] [G loss: 0.597695]\n",
      "epoch:3 step:3221 [D loss: 0.528039, acc.: 73.44%] [G loss: 0.483568]\n",
      "epoch:3 step:3222 [D loss: 0.472534, acc.: 82.81%] [G loss: 0.459641]\n",
      "epoch:3 step:3223 [D loss: 0.559060, acc.: 75.00%] [G loss: 0.516375]\n",
      "epoch:3 step:3224 [D loss: 0.534441, acc.: 78.12%] [G loss: 0.504767]\n",
      "epoch:3 step:3225 [D loss: 0.511813, acc.: 82.03%] [G loss: 0.505436]\n",
      "epoch:3 step:3226 [D loss: 0.552887, acc.: 73.44%] [G loss: 0.444050]\n",
      "epoch:3 step:3227 [D loss: 0.492624, acc.: 78.12%] [G loss: 0.610884]\n",
      "epoch:3 step:3228 [D loss: 0.624777, acc.: 68.75%] [G loss: 0.464134]\n",
      "epoch:3 step:3229 [D loss: 0.572248, acc.: 65.62%] [G loss: 0.440247]\n",
      "epoch:3 step:3230 [D loss: 0.551602, acc.: 71.09%] [G loss: 0.297833]\n",
      "epoch:3 step:3231 [D loss: 0.511416, acc.: 75.78%] [G loss: 0.527093]\n",
      "epoch:3 step:3232 [D loss: 0.511257, acc.: 73.44%] [G loss: 0.512638]\n",
      "epoch:3 step:3233 [D loss: 0.475110, acc.: 82.03%] [G loss: 0.542089]\n",
      "epoch:3 step:3234 [D loss: 0.525966, acc.: 76.56%] [G loss: 0.600834]\n",
      "epoch:3 step:3235 [D loss: 0.577833, acc.: 70.31%] [G loss: 0.401403]\n",
      "epoch:3 step:3236 [D loss: 0.456929, acc.: 78.12%] [G loss: 0.614486]\n",
      "epoch:3 step:3237 [D loss: 0.482423, acc.: 81.25%] [G loss: 0.583931]\n",
      "epoch:3 step:3238 [D loss: 0.451172, acc.: 81.25%] [G loss: 0.775226]\n",
      "epoch:3 step:3239 [D loss: 0.441145, acc.: 82.81%] [G loss: 0.751544]\n",
      "epoch:3 step:3240 [D loss: 0.470307, acc.: 78.91%] [G loss: 0.665578]\n",
      "epoch:3 step:3241 [D loss: 0.495171, acc.: 78.12%] [G loss: 0.675352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3242 [D loss: 0.500471, acc.: 77.34%] [G loss: 0.545017]\n",
      "epoch:3 step:3243 [D loss: 0.519613, acc.: 73.44%] [G loss: 0.520039]\n",
      "epoch:3 step:3244 [D loss: 0.557029, acc.: 67.97%] [G loss: 0.513845]\n",
      "epoch:3 step:3245 [D loss: 0.510934, acc.: 75.78%] [G loss: 0.474139]\n",
      "epoch:3 step:3246 [D loss: 0.524780, acc.: 80.47%] [G loss: 0.613086]\n",
      "epoch:3 step:3247 [D loss: 0.506583, acc.: 81.25%] [G loss: 0.442152]\n",
      "epoch:3 step:3248 [D loss: 0.648414, acc.: 64.06%] [G loss: 0.559312]\n",
      "epoch:3 step:3249 [D loss: 0.526240, acc.: 72.66%] [G loss: 0.480863]\n",
      "epoch:3 step:3250 [D loss: 0.498666, acc.: 75.00%] [G loss: 0.544297]\n",
      "epoch:3 step:3251 [D loss: 0.488867, acc.: 78.12%] [G loss: 0.529738]\n",
      "epoch:3 step:3252 [D loss: 0.558610, acc.: 74.22%] [G loss: 0.527128]\n",
      "epoch:3 step:3253 [D loss: 0.503996, acc.: 77.34%] [G loss: 0.472615]\n",
      "epoch:3 step:3254 [D loss: 0.470784, acc.: 77.34%] [G loss: 0.409338]\n",
      "epoch:3 step:3255 [D loss: 0.564396, acc.: 67.19%] [G loss: 0.444205]\n",
      "epoch:3 step:3256 [D loss: 0.501063, acc.: 76.56%] [G loss: 0.541522]\n",
      "epoch:3 step:3257 [D loss: 0.541490, acc.: 71.09%] [G loss: 0.555604]\n",
      "epoch:3 step:3258 [D loss: 0.474851, acc.: 74.22%] [G loss: 0.675111]\n",
      "epoch:3 step:3259 [D loss: 0.557518, acc.: 69.53%] [G loss: 0.704112]\n",
      "epoch:3 step:3260 [D loss: 0.488747, acc.: 76.56%] [G loss: 0.659146]\n",
      "epoch:3 step:3261 [D loss: 0.473286, acc.: 75.00%] [G loss: 0.579809]\n",
      "epoch:3 step:3262 [D loss: 0.442020, acc.: 79.69%] [G loss: 0.785988]\n",
      "epoch:3 step:3263 [D loss: 0.512524, acc.: 75.78%] [G loss: 0.736768]\n",
      "epoch:3 step:3264 [D loss: 0.517450, acc.: 74.22%] [G loss: 0.732002]\n",
      "epoch:3 step:3265 [D loss: 0.536226, acc.: 78.12%] [G loss: 0.432088]\n",
      "epoch:3 step:3266 [D loss: 0.591890, acc.: 69.53%] [G loss: 0.358496]\n",
      "epoch:3 step:3267 [D loss: 0.608553, acc.: 67.97%] [G loss: 0.411638]\n",
      "epoch:3 step:3268 [D loss: 0.479522, acc.: 81.25%] [G loss: 0.524199]\n",
      "epoch:3 step:3269 [D loss: 0.508290, acc.: 77.34%] [G loss: 0.485225]\n",
      "epoch:3 step:3270 [D loss: 0.539143, acc.: 72.66%] [G loss: 0.543408]\n",
      "epoch:3 step:3271 [D loss: 0.529457, acc.: 72.66%] [G loss: 0.565478]\n",
      "epoch:3 step:3272 [D loss: 0.528435, acc.: 67.97%] [G loss: 0.544080]\n",
      "epoch:3 step:3273 [D loss: 0.544230, acc.: 71.09%] [G loss: 0.468589]\n",
      "epoch:3 step:3274 [D loss: 0.594098, acc.: 68.75%] [G loss: 0.468202]\n",
      "epoch:3 step:3275 [D loss: 0.512820, acc.: 75.00%] [G loss: 0.461698]\n",
      "epoch:3 step:3276 [D loss: 0.592553, acc.: 64.84%] [G loss: 0.358909]\n",
      "epoch:3 step:3277 [D loss: 0.477901, acc.: 75.00%] [G loss: 0.504274]\n",
      "epoch:3 step:3278 [D loss: 0.532394, acc.: 68.75%] [G loss: 0.542971]\n",
      "epoch:3 step:3279 [D loss: 0.491337, acc.: 73.44%] [G loss: 0.627114]\n",
      "epoch:3 step:3280 [D loss: 0.497820, acc.: 74.22%] [G loss: 0.562979]\n",
      "epoch:3 step:3281 [D loss: 0.522991, acc.: 77.34%] [G loss: 0.495575]\n",
      "epoch:3 step:3282 [D loss: 0.455400, acc.: 81.25%] [G loss: 0.610712]\n",
      "epoch:3 step:3283 [D loss: 0.437540, acc.: 76.56%] [G loss: 0.631588]\n",
      "epoch:3 step:3284 [D loss: 0.625322, acc.: 67.97%] [G loss: 0.611042]\n",
      "epoch:3 step:3285 [D loss: 0.460929, acc.: 78.91%] [G loss: 0.800711]\n",
      "epoch:3 step:3286 [D loss: 0.483476, acc.: 78.91%] [G loss: 0.874662]\n",
      "epoch:3 step:3287 [D loss: 0.529603, acc.: 77.34%] [G loss: 0.699634]\n",
      "epoch:3 step:3288 [D loss: 0.645455, acc.: 64.06%] [G loss: 0.421302]\n",
      "epoch:3 step:3289 [D loss: 0.552011, acc.: 75.00%] [G loss: 0.378951]\n",
      "epoch:3 step:3290 [D loss: 0.527943, acc.: 74.22%] [G loss: 0.467525]\n",
      "epoch:3 step:3291 [D loss: 0.529411, acc.: 77.34%] [G loss: 0.540246]\n",
      "epoch:3 step:3292 [D loss: 0.520530, acc.: 73.44%] [G loss: 0.609486]\n",
      "epoch:3 step:3293 [D loss: 0.558228, acc.: 70.31%] [G loss: 0.460727]\n",
      "epoch:3 step:3294 [D loss: 0.542874, acc.: 72.66%] [G loss: 0.475020]\n",
      "epoch:3 step:3295 [D loss: 0.473842, acc.: 78.12%] [G loss: 0.663463]\n",
      "epoch:3 step:3296 [D loss: 0.465201, acc.: 82.81%] [G loss: 0.647404]\n",
      "epoch:3 step:3297 [D loss: 0.522245, acc.: 74.22%] [G loss: 0.540769]\n",
      "epoch:3 step:3298 [D loss: 0.507539, acc.: 78.91%] [G loss: 0.641810]\n",
      "epoch:3 step:3299 [D loss: 0.441809, acc.: 83.59%] [G loss: 0.626075]\n",
      "epoch:3 step:3300 [D loss: 0.568251, acc.: 71.09%] [G loss: 0.529127]\n",
      "epoch:3 step:3301 [D loss: 0.557674, acc.: 69.53%] [G loss: 0.486326]\n",
      "epoch:3 step:3302 [D loss: 0.481722, acc.: 75.00%] [G loss: 0.607462]\n",
      "epoch:3 step:3303 [D loss: 0.580437, acc.: 67.97%] [G loss: 0.557350]\n",
      "epoch:3 step:3304 [D loss: 0.509021, acc.: 75.00%] [G loss: 0.506829]\n",
      "epoch:3 step:3305 [D loss: 0.512069, acc.: 76.56%] [G loss: 0.523650]\n",
      "epoch:3 step:3306 [D loss: 0.513988, acc.: 78.12%] [G loss: 0.509354]\n",
      "epoch:3 step:3307 [D loss: 0.548484, acc.: 72.66%] [G loss: 0.640188]\n",
      "epoch:3 step:3308 [D loss: 0.506549, acc.: 75.78%] [G loss: 0.552881]\n",
      "epoch:3 step:3309 [D loss: 0.463219, acc.: 78.12%] [G loss: 0.685817]\n",
      "epoch:3 step:3310 [D loss: 0.422674, acc.: 85.94%] [G loss: 1.013025]\n",
      "epoch:3 step:3311 [D loss: 0.690945, acc.: 62.50%] [G loss: 0.466190]\n",
      "epoch:3 step:3312 [D loss: 0.649232, acc.: 67.19%] [G loss: 0.396426]\n",
      "epoch:3 step:3313 [D loss: 0.542095, acc.: 71.88%] [G loss: 0.390320]\n",
      "epoch:3 step:3314 [D loss: 0.544411, acc.: 71.09%] [G loss: 0.430789]\n",
      "epoch:3 step:3315 [D loss: 0.523758, acc.: 75.00%] [G loss: 0.499820]\n",
      "epoch:3 step:3316 [D loss: 0.585313, acc.: 67.19%] [G loss: 0.497607]\n",
      "epoch:3 step:3317 [D loss: 0.500269, acc.: 78.12%] [G loss: 0.570219]\n",
      "epoch:3 step:3318 [D loss: 0.519072, acc.: 73.44%] [G loss: 0.486772]\n",
      "epoch:3 step:3319 [D loss: 0.468459, acc.: 82.03%] [G loss: 0.841031]\n",
      "epoch:3 step:3320 [D loss: 0.495847, acc.: 75.00%] [G loss: 0.652733]\n",
      "epoch:3 step:3321 [D loss: 0.553988, acc.: 74.22%] [G loss: 0.489052]\n",
      "epoch:3 step:3322 [D loss: 0.554857, acc.: 71.09%] [G loss: 0.488485]\n",
      "epoch:3 step:3323 [D loss: 0.566681, acc.: 72.66%] [G loss: 0.440465]\n",
      "epoch:3 step:3324 [D loss: 0.508589, acc.: 72.66%] [G loss: 0.466321]\n",
      "epoch:3 step:3325 [D loss: 0.468948, acc.: 77.34%] [G loss: 0.604327]\n",
      "epoch:3 step:3326 [D loss: 0.497222, acc.: 75.78%] [G loss: 0.539527]\n",
      "epoch:3 step:3327 [D loss: 0.506919, acc.: 78.12%] [G loss: 0.597191]\n",
      "epoch:3 step:3328 [D loss: 0.581672, acc.: 67.19%] [G loss: 0.515248]\n",
      "epoch:3 step:3329 [D loss: 0.495647, acc.: 76.56%] [G loss: 0.500342]\n",
      "epoch:3 step:3330 [D loss: 0.519990, acc.: 73.44%] [G loss: 0.593695]\n",
      "epoch:3 step:3331 [D loss: 0.451601, acc.: 82.81%] [G loss: 0.565194]\n",
      "epoch:3 step:3332 [D loss: 0.431178, acc.: 86.72%] [G loss: 0.694443]\n",
      "epoch:3 step:3333 [D loss: 0.525981, acc.: 76.56%] [G loss: 0.644206]\n",
      "epoch:3 step:3334 [D loss: 0.473822, acc.: 80.47%] [G loss: 0.695187]\n",
      "epoch:3 step:3335 [D loss: 0.565914, acc.: 70.31%] [G loss: 0.472458]\n",
      "epoch:3 step:3336 [D loss: 0.580145, acc.: 67.19%] [G loss: 0.465550]\n",
      "epoch:3 step:3337 [D loss: 0.515021, acc.: 71.09%] [G loss: 0.552563]\n",
      "epoch:3 step:3338 [D loss: 0.543312, acc.: 74.22%] [G loss: 0.548290]\n",
      "epoch:3 step:3339 [D loss: 0.583378, acc.: 64.06%] [G loss: 0.456523]\n",
      "epoch:3 step:3340 [D loss: 0.581842, acc.: 63.28%] [G loss: 0.455584]\n",
      "epoch:3 step:3341 [D loss: 0.459393, acc.: 82.81%] [G loss: 0.666417]\n",
      "epoch:3 step:3342 [D loss: 0.564796, acc.: 75.78%] [G loss: 0.418021]\n",
      "epoch:3 step:3343 [D loss: 0.481994, acc.: 84.38%] [G loss: 0.404992]\n",
      "epoch:3 step:3344 [D loss: 0.513075, acc.: 75.78%] [G loss: 0.497693]\n",
      "epoch:3 step:3345 [D loss: 0.477424, acc.: 78.91%] [G loss: 0.493014]\n",
      "epoch:3 step:3346 [D loss: 0.523951, acc.: 72.66%] [G loss: 0.533286]\n",
      "epoch:3 step:3347 [D loss: 0.542405, acc.: 76.56%] [G loss: 0.548863]\n",
      "epoch:3 step:3348 [D loss: 0.512224, acc.: 76.56%] [G loss: 0.569986]\n",
      "epoch:3 step:3349 [D loss: 0.531332, acc.: 71.88%] [G loss: 0.591569]\n",
      "epoch:3 step:3350 [D loss: 0.494664, acc.: 76.56%] [G loss: 0.503388]\n",
      "epoch:3 step:3351 [D loss: 0.545748, acc.: 70.31%] [G loss: 0.439461]\n",
      "epoch:3 step:3352 [D loss: 0.468138, acc.: 81.25%] [G loss: 0.509720]\n",
      "epoch:3 step:3353 [D loss: 0.552789, acc.: 73.44%] [G loss: 0.532829]\n",
      "epoch:3 step:3354 [D loss: 0.571361, acc.: 71.09%] [G loss: 0.533898]\n",
      "epoch:3 step:3355 [D loss: 0.505890, acc.: 76.56%] [G loss: 0.566666]\n",
      "epoch:3 step:3356 [D loss: 0.498806, acc.: 75.78%] [G loss: 0.673992]\n",
      "epoch:3 step:3357 [D loss: 0.455003, acc.: 79.69%] [G loss: 0.556934]\n",
      "epoch:3 step:3358 [D loss: 0.473043, acc.: 79.69%] [G loss: 0.518842]\n",
      "epoch:3 step:3359 [D loss: 0.471039, acc.: 73.44%] [G loss: 0.531926]\n",
      "epoch:3 step:3360 [D loss: 0.511075, acc.: 76.56%] [G loss: 0.652108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3361 [D loss: 0.550250, acc.: 72.66%] [G loss: 0.502431]\n",
      "epoch:3 step:3362 [D loss: 0.561421, acc.: 73.44%] [G loss: 0.615780]\n",
      "epoch:3 step:3363 [D loss: 0.451830, acc.: 82.03%] [G loss: 0.603417]\n",
      "epoch:3 step:3364 [D loss: 0.524820, acc.: 70.31%] [G loss: 0.517279]\n",
      "epoch:3 step:3365 [D loss: 0.444332, acc.: 85.16%] [G loss: 0.522896]\n",
      "epoch:3 step:3366 [D loss: 0.505932, acc.: 75.00%] [G loss: 0.461798]\n",
      "epoch:3 step:3367 [D loss: 0.463471, acc.: 82.81%] [G loss: 0.539686]\n",
      "epoch:3 step:3368 [D loss: 0.524102, acc.: 75.78%] [G loss: 0.549729]\n",
      "epoch:3 step:3369 [D loss: 0.469906, acc.: 78.12%] [G loss: 0.755384]\n",
      "epoch:3 step:3370 [D loss: 0.544473, acc.: 74.22%] [G loss: 0.600246]\n",
      "epoch:3 step:3371 [D loss: 0.552701, acc.: 71.88%] [G loss: 0.452769]\n",
      "epoch:3 step:3372 [D loss: 0.483948, acc.: 82.03%] [G loss: 0.506193]\n",
      "epoch:3 step:3373 [D loss: 0.543830, acc.: 70.31%] [G loss: 0.585103]\n",
      "epoch:3 step:3374 [D loss: 0.538651, acc.: 75.00%] [G loss: 0.488335]\n",
      "epoch:3 step:3375 [D loss: 0.515189, acc.: 75.78%] [G loss: 0.504860]\n",
      "epoch:3 step:3376 [D loss: 0.478108, acc.: 78.12%] [G loss: 0.583801]\n",
      "epoch:3 step:3377 [D loss: 0.584162, acc.: 70.31%] [G loss: 0.427190]\n",
      "epoch:3 step:3378 [D loss: 0.451475, acc.: 85.94%] [G loss: 0.555174]\n",
      "epoch:3 step:3379 [D loss: 0.495045, acc.: 78.91%] [G loss: 0.569389]\n",
      "epoch:3 step:3380 [D loss: 0.559531, acc.: 68.75%] [G loss: 0.492297]\n",
      "epoch:3 step:3381 [D loss: 0.468971, acc.: 82.81%] [G loss: 0.536267]\n",
      "epoch:3 step:3382 [D loss: 0.460890, acc.: 76.56%] [G loss: 0.600514]\n",
      "epoch:3 step:3383 [D loss: 0.560812, acc.: 73.44%] [G loss: 0.574126]\n",
      "epoch:3 step:3384 [D loss: 0.520970, acc.: 72.66%] [G loss: 0.504992]\n",
      "epoch:3 step:3385 [D loss: 0.510528, acc.: 75.00%] [G loss: 0.600017]\n",
      "epoch:3 step:3386 [D loss: 0.378238, acc.: 87.50%] [G loss: 0.861394]\n",
      "epoch:3 step:3387 [D loss: 0.496804, acc.: 78.91%] [G loss: 0.563437]\n",
      "epoch:3 step:3388 [D loss: 0.570420, acc.: 69.53%] [G loss: 0.537121]\n",
      "epoch:3 step:3389 [D loss: 0.506540, acc.: 79.69%] [G loss: 0.617632]\n",
      "epoch:3 step:3390 [D loss: 0.520703, acc.: 71.88%] [G loss: 0.429917]\n",
      "epoch:3 step:3391 [D loss: 0.495954, acc.: 78.12%] [G loss: 0.615981]\n",
      "epoch:3 step:3392 [D loss: 0.482747, acc.: 77.34%] [G loss: 0.626832]\n",
      "epoch:3 step:3393 [D loss: 0.409165, acc.: 83.59%] [G loss: 0.709274]\n",
      "epoch:3 step:3394 [D loss: 0.537517, acc.: 67.19%] [G loss: 0.674885]\n",
      "epoch:3 step:3395 [D loss: 0.584651, acc.: 67.97%] [G loss: 0.533392]\n",
      "epoch:3 step:3396 [D loss: 0.482201, acc.: 77.34%] [G loss: 0.508587]\n",
      "epoch:3 step:3397 [D loss: 0.559408, acc.: 71.88%] [G loss: 0.342204]\n",
      "epoch:3 step:3398 [D loss: 0.527512, acc.: 71.09%] [G loss: 0.474406]\n",
      "epoch:3 step:3399 [D loss: 0.548970, acc.: 71.09%] [G loss: 0.593586]\n",
      "epoch:3 step:3400 [D loss: 0.454496, acc.: 83.59%] [G loss: 0.774601]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.139782\n",
      "FID: 65.044975\n",
      "0 = 13.234443551826477\n",
      "1 = 0.11943293498770693\n",
      "2 = 0.9617000222206116\n",
      "3 = 0.9233999848365784\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9233999848365784\n",
      "7 = 9.395802523946726\n",
      "8 = 0.16066308189814413\n",
      "9 = 0.8751000165939331\n",
      "10 = 0.8374000191688538\n",
      "11 = 0.9128000140190125\n",
      "12 = 0.9056889414787292\n",
      "13 = 0.8374000191688538\n",
      "14 = 5.139799118041992\n",
      "15 = 7.82813835144043\n",
      "16 = 0.3495407700538635\n",
      "17 = 5.139782428741455\n",
      "18 = 65.04497528076172\n",
      "epoch:3 step:3401 [D loss: 0.545453, acc.: 72.66%] [G loss: 0.564814]\n",
      "epoch:3 step:3402 [D loss: 0.559037, acc.: 69.53%] [G loss: 0.538430]\n",
      "epoch:3 step:3403 [D loss: 0.442443, acc.: 83.59%] [G loss: 0.553790]\n",
      "epoch:3 step:3404 [D loss: 0.582671, acc.: 68.75%] [G loss: 0.574055]\n",
      "epoch:3 step:3405 [D loss: 0.492254, acc.: 82.81%] [G loss: 0.450370]\n",
      "epoch:3 step:3406 [D loss: 0.505345, acc.: 77.34%] [G loss: 0.454748]\n",
      "epoch:3 step:3407 [D loss: 0.490647, acc.: 79.69%] [G loss: 0.596638]\n",
      "epoch:3 step:3408 [D loss: 0.515603, acc.: 78.12%] [G loss: 0.540653]\n",
      "epoch:3 step:3409 [D loss: 0.505437, acc.: 75.00%] [G loss: 0.541919]\n",
      "epoch:3 step:3410 [D loss: 0.525450, acc.: 76.56%] [G loss: 0.543552]\n",
      "epoch:3 step:3411 [D loss: 0.588703, acc.: 69.53%] [G loss: 0.525615]\n",
      "epoch:3 step:3412 [D loss: 0.531590, acc.: 78.12%] [G loss: 0.488084]\n",
      "epoch:3 step:3413 [D loss: 0.536271, acc.: 72.66%] [G loss: 0.505399]\n",
      "epoch:3 step:3414 [D loss: 0.462426, acc.: 82.81%] [G loss: 0.582842]\n",
      "epoch:3 step:3415 [D loss: 0.558441, acc.: 73.44%] [G loss: 0.590740]\n",
      "epoch:3 step:3416 [D loss: 0.483439, acc.: 80.47%] [G loss: 0.547608]\n",
      "epoch:3 step:3417 [D loss: 0.473638, acc.: 82.03%] [G loss: 0.627451]\n",
      "epoch:3 step:3418 [D loss: 0.514389, acc.: 78.12%] [G loss: 0.503840]\n",
      "epoch:3 step:3419 [D loss: 0.519545, acc.: 73.44%] [G loss: 0.458339]\n",
      "epoch:3 step:3420 [D loss: 0.470627, acc.: 80.47%] [G loss: 0.622212]\n",
      "epoch:3 step:3421 [D loss: 0.516491, acc.: 81.25%] [G loss: 0.467281]\n",
      "epoch:3 step:3422 [D loss: 0.479805, acc.: 79.69%] [G loss: 0.559078]\n",
      "epoch:3 step:3423 [D loss: 0.495246, acc.: 75.78%] [G loss: 0.460841]\n",
      "epoch:3 step:3424 [D loss: 0.479003, acc.: 78.12%] [G loss: 0.568968]\n",
      "epoch:3 step:3425 [D loss: 0.502320, acc.: 75.00%] [G loss: 0.562492]\n",
      "epoch:3 step:3426 [D loss: 0.532606, acc.: 75.00%] [G loss: 0.543785]\n",
      "epoch:3 step:3427 [D loss: 0.521943, acc.: 73.44%] [G loss: 0.562466]\n",
      "epoch:3 step:3428 [D loss: 0.566948, acc.: 67.97%] [G loss: 0.534598]\n",
      "epoch:3 step:3429 [D loss: 0.468582, acc.: 83.59%] [G loss: 0.466914]\n",
      "epoch:3 step:3430 [D loss: 0.515651, acc.: 74.22%] [G loss: 0.571531]\n",
      "epoch:3 step:3431 [D loss: 0.511361, acc.: 77.34%] [G loss: 0.565971]\n",
      "epoch:3 step:3432 [D loss: 0.561530, acc.: 71.09%] [G loss: 0.490423]\n",
      "epoch:3 step:3433 [D loss: 0.594234, acc.: 69.53%] [G loss: 0.488238]\n",
      "epoch:3 step:3434 [D loss: 0.513024, acc.: 74.22%] [G loss: 0.629317]\n",
      "epoch:3 step:3435 [D loss: 0.522186, acc.: 75.78%] [G loss: 0.568037]\n",
      "epoch:3 step:3436 [D loss: 0.529114, acc.: 70.31%] [G loss: 0.588526]\n",
      "epoch:3 step:3437 [D loss: 0.505005, acc.: 77.34%] [G loss: 0.499652]\n",
      "epoch:3 step:3438 [D loss: 0.500635, acc.: 76.56%] [G loss: 0.518609]\n",
      "epoch:3 step:3439 [D loss: 0.505166, acc.: 75.00%] [G loss: 0.530721]\n",
      "epoch:3 step:3440 [D loss: 0.460959, acc.: 79.69%] [G loss: 0.574921]\n",
      "epoch:3 step:3441 [D loss: 0.478566, acc.: 75.78%] [G loss: 0.641084]\n",
      "epoch:3 step:3442 [D loss: 0.474146, acc.: 79.69%] [G loss: 0.645164]\n",
      "epoch:3 step:3443 [D loss: 0.517553, acc.: 74.22%] [G loss: 0.492667]\n",
      "epoch:3 step:3444 [D loss: 0.489424, acc.: 75.78%] [G loss: 0.578370]\n",
      "epoch:3 step:3445 [D loss: 0.432042, acc.: 82.03%] [G loss: 0.612716]\n",
      "epoch:3 step:3446 [D loss: 0.523974, acc.: 70.31%] [G loss: 0.619532]\n",
      "epoch:3 step:3447 [D loss: 0.615930, acc.: 65.62%] [G loss: 0.410628]\n",
      "epoch:3 step:3448 [D loss: 0.481198, acc.: 80.47%] [G loss: 0.573846]\n",
      "epoch:3 step:3449 [D loss: 0.463913, acc.: 81.25%] [G loss: 0.537020]\n",
      "epoch:3 step:3450 [D loss: 0.425738, acc.: 83.59%] [G loss: 0.618214]\n",
      "epoch:3 step:3451 [D loss: 0.494887, acc.: 75.00%] [G loss: 0.543066]\n",
      "epoch:3 step:3452 [D loss: 0.474001, acc.: 79.69%] [G loss: 0.535539]\n",
      "epoch:3 step:3453 [D loss: 0.454993, acc.: 80.47%] [G loss: 0.690255]\n",
      "epoch:3 step:3454 [D loss: 0.514890, acc.: 75.78%] [G loss: 0.544376]\n",
      "epoch:3 step:3455 [D loss: 0.563156, acc.: 70.31%] [G loss: 0.512746]\n",
      "epoch:3 step:3456 [D loss: 0.500184, acc.: 75.00%] [G loss: 0.649615]\n",
      "epoch:3 step:3457 [D loss: 0.514223, acc.: 71.88%] [G loss: 0.670434]\n",
      "epoch:3 step:3458 [D loss: 0.464088, acc.: 78.91%] [G loss: 0.621794]\n",
      "epoch:3 step:3459 [D loss: 0.447863, acc.: 82.03%] [G loss: 0.637629]\n",
      "epoch:3 step:3460 [D loss: 0.460253, acc.: 78.91%] [G loss: 0.800050]\n",
      "epoch:3 step:3461 [D loss: 0.468427, acc.: 78.12%] [G loss: 0.582879]\n",
      "epoch:3 step:3462 [D loss: 0.518926, acc.: 75.00%] [G loss: 0.570273]\n",
      "epoch:3 step:3463 [D loss: 0.584484, acc.: 68.75%] [G loss: 0.542179]\n",
      "epoch:3 step:3464 [D loss: 0.550786, acc.: 70.31%] [G loss: 0.570659]\n",
      "epoch:3 step:3465 [D loss: 0.505511, acc.: 74.22%] [G loss: 0.576381]\n",
      "epoch:3 step:3466 [D loss: 0.496911, acc.: 77.34%] [G loss: 0.497452]\n",
      "epoch:3 step:3467 [D loss: 0.476262, acc.: 81.25%] [G loss: 0.610725]\n",
      "epoch:3 step:3468 [D loss: 0.506872, acc.: 76.56%] [G loss: 0.494381]\n",
      "epoch:3 step:3469 [D loss: 0.527962, acc.: 79.69%] [G loss: 0.489018]\n",
      "epoch:3 step:3470 [D loss: 0.523170, acc.: 74.22%] [G loss: 0.460655]\n",
      "epoch:3 step:3471 [D loss: 0.516129, acc.: 73.44%] [G loss: 0.586061]\n",
      "epoch:3 step:3472 [D loss: 0.454968, acc.: 77.34%] [G loss: 0.562846]\n",
      "epoch:3 step:3473 [D loss: 0.592209, acc.: 72.66%] [G loss: 0.456451]\n",
      "epoch:3 step:3474 [D loss: 0.456123, acc.: 80.47%] [G loss: 0.626494]\n",
      "epoch:3 step:3475 [D loss: 0.509544, acc.: 77.34%] [G loss: 0.711517]\n",
      "epoch:3 step:3476 [D loss: 0.489943, acc.: 75.00%] [G loss: 0.684001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3477 [D loss: 0.533119, acc.: 75.78%] [G loss: 0.677715]\n",
      "epoch:3 step:3478 [D loss: 0.482635, acc.: 81.25%] [G loss: 0.655603]\n",
      "epoch:3 step:3479 [D loss: 0.584705, acc.: 65.62%] [G loss: 0.393421]\n",
      "epoch:3 step:3480 [D loss: 0.469603, acc.: 79.69%] [G loss: 0.606947]\n",
      "epoch:3 step:3481 [D loss: 0.479126, acc.: 75.78%] [G loss: 0.586063]\n",
      "epoch:3 step:3482 [D loss: 0.515392, acc.: 73.44%] [G loss: 0.545077]\n",
      "epoch:3 step:3483 [D loss: 0.578363, acc.: 67.19%] [G loss: 0.590030]\n",
      "epoch:3 step:3484 [D loss: 0.485698, acc.: 76.56%] [G loss: 0.534742]\n",
      "epoch:3 step:3485 [D loss: 0.580298, acc.: 74.22%] [G loss: 0.488028]\n",
      "epoch:3 step:3486 [D loss: 0.516964, acc.: 73.44%] [G loss: 0.475532]\n",
      "epoch:3 step:3487 [D loss: 0.494724, acc.: 81.25%] [G loss: 0.639183]\n",
      "epoch:3 step:3488 [D loss: 0.448413, acc.: 82.81%] [G loss: 0.690253]\n",
      "epoch:3 step:3489 [D loss: 0.440558, acc.: 80.47%] [G loss: 0.665753]\n",
      "epoch:3 step:3490 [D loss: 0.554225, acc.: 72.66%] [G loss: 0.558677]\n",
      "epoch:3 step:3491 [D loss: 0.406633, acc.: 88.28%] [G loss: 0.614339]\n",
      "epoch:3 step:3492 [D loss: 0.550808, acc.: 69.53%] [G loss: 0.600114]\n",
      "epoch:3 step:3493 [D loss: 0.511678, acc.: 76.56%] [G loss: 0.528455]\n",
      "epoch:3 step:3494 [D loss: 0.520130, acc.: 76.56%] [G loss: 0.640528]\n",
      "epoch:3 step:3495 [D loss: 0.538591, acc.: 68.75%] [G loss: 0.457066]\n",
      "epoch:3 step:3496 [D loss: 0.519150, acc.: 72.66%] [G loss: 0.521500]\n",
      "epoch:3 step:3497 [D loss: 0.545534, acc.: 72.66%] [G loss: 0.507358]\n",
      "epoch:3 step:3498 [D loss: 0.544906, acc.: 74.22%] [G loss: 0.458797]\n",
      "epoch:3 step:3499 [D loss: 0.546507, acc.: 73.44%] [G loss: 0.518264]\n",
      "epoch:3 step:3500 [D loss: 0.510845, acc.: 78.91%] [G loss: 0.619804]\n",
      "epoch:3 step:3501 [D loss: 0.549149, acc.: 71.09%] [G loss: 0.662860]\n",
      "epoch:3 step:3502 [D loss: 0.480264, acc.: 79.69%] [G loss: 0.598292]\n",
      "epoch:3 step:3503 [D loss: 0.502714, acc.: 77.34%] [G loss: 0.511565]\n",
      "epoch:3 step:3504 [D loss: 0.515469, acc.: 76.56%] [G loss: 0.525251]\n",
      "epoch:3 step:3505 [D loss: 0.527650, acc.: 72.66%] [G loss: 0.563458]\n",
      "epoch:3 step:3506 [D loss: 0.473318, acc.: 79.69%] [G loss: 0.686001]\n",
      "epoch:3 step:3507 [D loss: 0.551816, acc.: 68.75%] [G loss: 0.481686]\n",
      "epoch:3 step:3508 [D loss: 0.493048, acc.: 78.12%] [G loss: 0.624300]\n",
      "epoch:3 step:3509 [D loss: 0.553368, acc.: 68.75%] [G loss: 0.560471]\n",
      "epoch:3 step:3510 [D loss: 0.483711, acc.: 78.91%] [G loss: 0.581965]\n",
      "epoch:3 step:3511 [D loss: 0.543945, acc.: 69.53%] [G loss: 0.624023]\n",
      "epoch:3 step:3512 [D loss: 0.495546, acc.: 79.69%] [G loss: 0.633920]\n",
      "epoch:3 step:3513 [D loss: 0.564031, acc.: 71.09%] [G loss: 0.423670]\n",
      "epoch:3 step:3514 [D loss: 0.544984, acc.: 72.66%] [G loss: 0.441307]\n",
      "epoch:3 step:3515 [D loss: 0.565946, acc.: 72.66%] [G loss: 0.504237]\n",
      "epoch:3 step:3516 [D loss: 0.523600, acc.: 75.00%] [G loss: 0.478145]\n",
      "epoch:3 step:3517 [D loss: 0.532561, acc.: 71.09%] [G loss: 0.395172]\n",
      "epoch:3 step:3518 [D loss: 0.451988, acc.: 84.38%] [G loss: 0.555576]\n",
      "epoch:3 step:3519 [D loss: 0.496389, acc.: 78.12%] [G loss: 0.623060]\n",
      "epoch:3 step:3520 [D loss: 0.502824, acc.: 79.69%] [G loss: 0.717362]\n",
      "epoch:3 step:3521 [D loss: 0.590454, acc.: 70.31%] [G loss: 0.543763]\n",
      "epoch:3 step:3522 [D loss: 0.594240, acc.: 67.97%] [G loss: 0.331637]\n",
      "epoch:3 step:3523 [D loss: 0.483045, acc.: 77.34%] [G loss: 0.557804]\n",
      "epoch:3 step:3524 [D loss: 0.487481, acc.: 81.25%] [G loss: 0.664797]\n",
      "epoch:3 step:3525 [D loss: 0.500581, acc.: 75.00%] [G loss: 0.562590]\n",
      "epoch:3 step:3526 [D loss: 0.497682, acc.: 77.34%] [G loss: 0.631113]\n",
      "epoch:3 step:3527 [D loss: 0.562250, acc.: 71.88%] [G loss: 0.430815]\n",
      "epoch:3 step:3528 [D loss: 0.554213, acc.: 75.00%] [G loss: 0.547030]\n",
      "epoch:3 step:3529 [D loss: 0.577173, acc.: 72.66%] [G loss: 0.480056]\n",
      "epoch:3 step:3530 [D loss: 0.520797, acc.: 71.88%] [G loss: 0.509852]\n",
      "epoch:3 step:3531 [D loss: 0.577997, acc.: 71.09%] [G loss: 0.496894]\n",
      "epoch:3 step:3532 [D loss: 0.518748, acc.: 75.78%] [G loss: 0.462120]\n",
      "epoch:3 step:3533 [D loss: 0.486391, acc.: 78.12%] [G loss: 0.571522]\n",
      "epoch:3 step:3534 [D loss: 0.560158, acc.: 70.31%] [G loss: 0.493502]\n",
      "epoch:3 step:3535 [D loss: 0.495234, acc.: 76.56%] [G loss: 0.623092]\n",
      "epoch:3 step:3536 [D loss: 0.478871, acc.: 81.25%] [G loss: 0.485652]\n",
      "epoch:3 step:3537 [D loss: 0.553790, acc.: 72.66%] [G loss: 0.443214]\n",
      "epoch:3 step:3538 [D loss: 0.495915, acc.: 81.25%] [G loss: 0.439695]\n",
      "epoch:3 step:3539 [D loss: 0.537552, acc.: 75.78%] [G loss: 0.520041]\n",
      "epoch:3 step:3540 [D loss: 0.534084, acc.: 77.34%] [G loss: 0.462376]\n",
      "epoch:3 step:3541 [D loss: 0.467412, acc.: 78.91%] [G loss: 0.521648]\n",
      "epoch:3 step:3542 [D loss: 0.522156, acc.: 72.66%] [G loss: 0.590270]\n",
      "epoch:3 step:3543 [D loss: 0.453120, acc.: 81.25%] [G loss: 0.706305]\n",
      "epoch:3 step:3544 [D loss: 0.488309, acc.: 75.78%] [G loss: 0.688647]\n",
      "epoch:3 step:3545 [D loss: 0.475116, acc.: 78.12%] [G loss: 0.651651]\n",
      "epoch:3 step:3546 [D loss: 0.576776, acc.: 68.75%] [G loss: 0.464538]\n",
      "epoch:3 step:3547 [D loss: 0.464797, acc.: 78.91%] [G loss: 0.597236]\n",
      "epoch:3 step:3548 [D loss: 0.495518, acc.: 78.12%] [G loss: 0.634165]\n",
      "epoch:3 step:3549 [D loss: 0.513939, acc.: 73.44%] [G loss: 0.585471]\n",
      "epoch:3 step:3550 [D loss: 0.564404, acc.: 67.19%] [G loss: 0.413985]\n",
      "epoch:3 step:3551 [D loss: 0.545203, acc.: 76.56%] [G loss: 0.426266]\n",
      "epoch:3 step:3552 [D loss: 0.497379, acc.: 78.91%] [G loss: 0.508411]\n",
      "epoch:3 step:3553 [D loss: 0.537320, acc.: 74.22%] [G loss: 0.587845]\n",
      "epoch:3 step:3554 [D loss: 0.506434, acc.: 78.12%] [G loss: 0.653705]\n",
      "epoch:3 step:3555 [D loss: 0.525666, acc.: 74.22%] [G loss: 0.528271]\n",
      "epoch:3 step:3556 [D loss: 0.505916, acc.: 71.88%] [G loss: 0.602208]\n",
      "epoch:3 step:3557 [D loss: 0.445522, acc.: 77.34%] [G loss: 0.625058]\n",
      "epoch:3 step:3558 [D loss: 0.445899, acc.: 79.69%] [G loss: 0.671294]\n",
      "epoch:3 step:3559 [D loss: 0.472376, acc.: 78.12%] [G loss: 0.717741]\n",
      "epoch:3 step:3560 [D loss: 0.521462, acc.: 71.09%] [G loss: 0.505105]\n",
      "epoch:3 step:3561 [D loss: 0.461475, acc.: 81.25%] [G loss: 0.567610]\n",
      "epoch:3 step:3562 [D loss: 0.498373, acc.: 75.78%] [G loss: 0.608143]\n",
      "epoch:3 step:3563 [D loss: 0.446833, acc.: 80.47%] [G loss: 0.659949]\n",
      "epoch:3 step:3564 [D loss: 0.461345, acc.: 83.59%] [G loss: 0.621799]\n",
      "epoch:3 step:3565 [D loss: 0.433835, acc.: 78.12%] [G loss: 0.754464]\n",
      "epoch:3 step:3566 [D loss: 0.448931, acc.: 78.91%] [G loss: 0.742322]\n",
      "epoch:3 step:3567 [D loss: 0.508313, acc.: 78.12%] [G loss: 0.688317]\n",
      "epoch:3 step:3568 [D loss: 0.472841, acc.: 80.47%] [G loss: 0.613339]\n",
      "epoch:3 step:3569 [D loss: 0.501756, acc.: 76.56%] [G loss: 0.625481]\n",
      "epoch:3 step:3570 [D loss: 0.558012, acc.: 73.44%] [G loss: 0.579155]\n",
      "epoch:3 step:3571 [D loss: 0.528593, acc.: 71.09%] [G loss: 0.602237]\n",
      "epoch:3 step:3572 [D loss: 0.456087, acc.: 81.25%] [G loss: 0.793216]\n",
      "epoch:3 step:3573 [D loss: 0.546369, acc.: 70.31%] [G loss: 0.510394]\n",
      "epoch:3 step:3574 [D loss: 0.496007, acc.: 74.22%] [G loss: 0.681288]\n",
      "epoch:3 step:3575 [D loss: 0.472516, acc.: 75.78%] [G loss: 0.630836]\n",
      "epoch:3 step:3576 [D loss: 0.605927, acc.: 68.75%] [G loss: 0.485224]\n",
      "epoch:3 step:3577 [D loss: 0.668789, acc.: 62.50%] [G loss: 0.379311]\n",
      "epoch:3 step:3578 [D loss: 0.587822, acc.: 66.41%] [G loss: 0.329674]\n",
      "epoch:3 step:3579 [D loss: 0.501541, acc.: 77.34%] [G loss: 0.558029]\n",
      "epoch:3 step:3580 [D loss: 0.466387, acc.: 78.91%] [G loss: 0.628946]\n",
      "epoch:3 step:3581 [D loss: 0.526712, acc.: 72.66%] [G loss: 0.655479]\n",
      "epoch:3 step:3582 [D loss: 0.533593, acc.: 71.09%] [G loss: 0.549695]\n",
      "epoch:3 step:3583 [D loss: 0.486997, acc.: 78.12%] [G loss: 0.691129]\n",
      "epoch:3 step:3584 [D loss: 0.495894, acc.: 80.47%] [G loss: 0.530336]\n",
      "epoch:3 step:3585 [D loss: 0.530758, acc.: 77.34%] [G loss: 0.537265]\n",
      "epoch:3 step:3586 [D loss: 0.470852, acc.: 78.91%] [G loss: 0.642805]\n",
      "epoch:3 step:3587 [D loss: 0.574769, acc.: 67.19%] [G loss: 0.433597]\n",
      "epoch:3 step:3588 [D loss: 0.513763, acc.: 76.56%] [G loss: 0.446870]\n",
      "epoch:3 step:3589 [D loss: 0.527054, acc.: 71.88%] [G loss: 0.427894]\n",
      "epoch:3 step:3590 [D loss: 0.534258, acc.: 70.31%] [G loss: 0.526479]\n",
      "epoch:3 step:3591 [D loss: 0.484947, acc.: 73.44%] [G loss: 0.608680]\n",
      "epoch:3 step:3592 [D loss: 0.475292, acc.: 78.91%] [G loss: 0.923887]\n",
      "epoch:3 step:3593 [D loss: 0.523948, acc.: 75.78%] [G loss: 0.736678]\n",
      "epoch:3 step:3594 [D loss: 0.564649, acc.: 69.53%] [G loss: 0.660797]\n",
      "epoch:3 step:3595 [D loss: 0.535793, acc.: 74.22%] [G loss: 0.459113]\n",
      "epoch:3 step:3596 [D loss: 0.543029, acc.: 71.09%] [G loss: 0.401966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3597 [D loss: 0.521536, acc.: 68.75%] [G loss: 0.605445]\n",
      "epoch:3 step:3598 [D loss: 0.546243, acc.: 70.31%] [G loss: 0.587002]\n",
      "epoch:3 step:3599 [D loss: 0.553815, acc.: 75.00%] [G loss: 0.547850]\n",
      "epoch:3 step:3600 [D loss: 0.448131, acc.: 84.38%] [G loss: 0.591262]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.036515\n",
      "FID: 69.362602\n",
      "0 = 13.190743389034255\n",
      "1 = 0.10817551243314422\n",
      "2 = 0.9635000228881836\n",
      "3 = 0.9269999861717224\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9269999861717224\n",
      "7 = 9.642630028271695\n",
      "8 = 0.17074839953989193\n",
      "9 = 0.8805000185966492\n",
      "10 = 0.8460000157356262\n",
      "11 = 0.9150000214576721\n",
      "12 = 0.9087003469467163\n",
      "13 = 0.8460000157356262\n",
      "14 = 5.036532402038574\n",
      "15 = 7.662153244018555\n",
      "16 = 0.3569512665271759\n",
      "17 = 5.036514759063721\n",
      "18 = 69.36260223388672\n",
      "epoch:3 step:3601 [D loss: 0.514873, acc.: 75.00%] [G loss: 0.486845]\n",
      "epoch:3 step:3602 [D loss: 0.515504, acc.: 75.78%] [G loss: 0.515011]\n",
      "epoch:3 step:3603 [D loss: 0.433302, acc.: 81.25%] [G loss: 0.650338]\n",
      "epoch:3 step:3604 [D loss: 0.570201, acc.: 71.88%] [G loss: 0.587886]\n",
      "epoch:3 step:3605 [D loss: 0.606663, acc.: 65.62%] [G loss: 0.507357]\n",
      "epoch:3 step:3606 [D loss: 0.514525, acc.: 74.22%] [G loss: 0.554715]\n",
      "epoch:3 step:3607 [D loss: 0.502465, acc.: 74.22%] [G loss: 0.659013]\n",
      "epoch:3 step:3608 [D loss: 0.508163, acc.: 77.34%] [G loss: 0.633301]\n",
      "epoch:3 step:3609 [D loss: 0.515886, acc.: 74.22%] [G loss: 0.676952]\n",
      "epoch:3 step:3610 [D loss: 0.529369, acc.: 75.78%] [G loss: 0.540693]\n",
      "epoch:3 step:3611 [D loss: 0.508391, acc.: 78.12%] [G loss: 0.515485]\n",
      "epoch:3 step:3612 [D loss: 0.500391, acc.: 75.78%] [G loss: 0.579806]\n",
      "epoch:3 step:3613 [D loss: 0.503805, acc.: 78.12%] [G loss: 0.744891]\n",
      "epoch:3 step:3614 [D loss: 0.426042, acc.: 83.59%] [G loss: 0.613964]\n",
      "epoch:3 step:3615 [D loss: 0.503837, acc.: 72.66%] [G loss: 0.537953]\n",
      "epoch:3 step:3616 [D loss: 0.459474, acc.: 78.91%] [G loss: 0.529922]\n",
      "epoch:3 step:3617 [D loss: 0.450232, acc.: 80.47%] [G loss: 0.558767]\n",
      "epoch:3 step:3618 [D loss: 0.492330, acc.: 79.69%] [G loss: 0.686510]\n",
      "epoch:3 step:3619 [D loss: 0.521913, acc.: 73.44%] [G loss: 0.698812]\n",
      "epoch:3 step:3620 [D loss: 0.535555, acc.: 72.66%] [G loss: 0.519244]\n",
      "epoch:3 step:3621 [D loss: 0.492915, acc.: 74.22%] [G loss: 0.527448]\n",
      "epoch:3 step:3622 [D loss: 0.501898, acc.: 73.44%] [G loss: 0.600321]\n",
      "epoch:3 step:3623 [D loss: 0.586573, acc.: 67.19%] [G loss: 0.454155]\n",
      "epoch:3 step:3624 [D loss: 0.530214, acc.: 75.78%] [G loss: 0.400349]\n",
      "epoch:3 step:3625 [D loss: 0.487970, acc.: 79.69%] [G loss: 0.466875]\n",
      "epoch:3 step:3626 [D loss: 0.518930, acc.: 78.91%] [G loss: 0.659405]\n",
      "epoch:3 step:3627 [D loss: 0.506377, acc.: 78.12%] [G loss: 0.823184]\n",
      "epoch:3 step:3628 [D loss: 0.603456, acc.: 64.84%] [G loss: 0.593237]\n",
      "epoch:3 step:3629 [D loss: 0.491731, acc.: 75.78%] [G loss: 0.632975]\n",
      "epoch:3 step:3630 [D loss: 0.519060, acc.: 71.88%] [G loss: 0.494987]\n",
      "epoch:3 step:3631 [D loss: 0.537189, acc.: 71.09%] [G loss: 0.548508]\n",
      "epoch:3 step:3632 [D loss: 0.494699, acc.: 77.34%] [G loss: 0.475080]\n",
      "epoch:3 step:3633 [D loss: 0.454453, acc.: 82.03%] [G loss: 0.608349]\n",
      "epoch:3 step:3634 [D loss: 0.500736, acc.: 75.78%] [G loss: 0.496213]\n",
      "epoch:3 step:3635 [D loss: 0.606278, acc.: 65.62%] [G loss: 0.514096]\n",
      "epoch:3 step:3636 [D loss: 0.568467, acc.: 71.09%] [G loss: 0.390293]\n",
      "epoch:3 step:3637 [D loss: 0.567522, acc.: 64.06%] [G loss: 0.491317]\n",
      "epoch:3 step:3638 [D loss: 0.558716, acc.: 67.97%] [G loss: 0.453732]\n",
      "epoch:3 step:3639 [D loss: 0.562835, acc.: 67.19%] [G loss: 0.559286]\n",
      "epoch:3 step:3640 [D loss: 0.495243, acc.: 75.78%] [G loss: 0.722889]\n",
      "epoch:3 step:3641 [D loss: 0.468928, acc.: 78.12%] [G loss: 0.714915]\n",
      "epoch:3 step:3642 [D loss: 0.484964, acc.: 78.91%] [G loss: 0.661074]\n",
      "epoch:3 step:3643 [D loss: 0.531689, acc.: 74.22%] [G loss: 0.538812]\n",
      "epoch:3 step:3644 [D loss: 0.532561, acc.: 75.00%] [G loss: 0.427339]\n",
      "epoch:3 step:3645 [D loss: 0.454086, acc.: 82.81%] [G loss: 0.609420]\n",
      "epoch:3 step:3646 [D loss: 0.513600, acc.: 74.22%] [G loss: 0.667411]\n",
      "epoch:3 step:3647 [D loss: 0.530660, acc.: 75.78%] [G loss: 0.574950]\n",
      "epoch:3 step:3648 [D loss: 0.470380, acc.: 79.69%] [G loss: 0.563217]\n",
      "epoch:3 step:3649 [D loss: 0.462085, acc.: 81.25%] [G loss: 0.554396]\n",
      "epoch:3 step:3650 [D loss: 0.539270, acc.: 70.31%] [G loss: 0.532199]\n",
      "epoch:3 step:3651 [D loss: 0.540335, acc.: 73.44%] [G loss: 0.509128]\n",
      "epoch:3 step:3652 [D loss: 0.468170, acc.: 78.91%] [G loss: 0.502248]\n",
      "epoch:3 step:3653 [D loss: 0.485934, acc.: 78.12%] [G loss: 0.514019]\n",
      "epoch:3 step:3654 [D loss: 0.538610, acc.: 75.78%] [G loss: 0.504154]\n",
      "epoch:3 step:3655 [D loss: 0.621326, acc.: 64.84%] [G loss: 0.498523]\n",
      "epoch:3 step:3656 [D loss: 0.504958, acc.: 82.03%] [G loss: 0.580124]\n",
      "epoch:3 step:3657 [D loss: 0.547995, acc.: 71.88%] [G loss: 0.438317]\n",
      "epoch:3 step:3658 [D loss: 0.590487, acc.: 67.97%] [G loss: 0.483205]\n",
      "epoch:3 step:3659 [D loss: 0.550690, acc.: 70.31%] [G loss: 0.454052]\n",
      "epoch:3 step:3660 [D loss: 0.544982, acc.: 74.22%] [G loss: 0.420562]\n",
      "epoch:3 step:3661 [D loss: 0.514014, acc.: 75.00%] [G loss: 0.452473]\n",
      "epoch:3 step:3662 [D loss: 0.548564, acc.: 73.44%] [G loss: 0.439399]\n",
      "epoch:3 step:3663 [D loss: 0.471999, acc.: 82.03%] [G loss: 0.552368]\n",
      "epoch:3 step:3664 [D loss: 0.466763, acc.: 79.69%] [G loss: 0.578801]\n",
      "epoch:3 step:3665 [D loss: 0.460687, acc.: 77.34%] [G loss: 0.813917]\n",
      "epoch:3 step:3666 [D loss: 0.505877, acc.: 75.00%] [G loss: 0.678778]\n",
      "epoch:3 step:3667 [D loss: 0.481095, acc.: 80.47%] [G loss: 0.555973]\n",
      "epoch:3 step:3668 [D loss: 0.480729, acc.: 78.12%] [G loss: 0.568072]\n",
      "epoch:3 step:3669 [D loss: 0.653377, acc.: 64.84%] [G loss: 0.539134]\n",
      "epoch:3 step:3670 [D loss: 0.471412, acc.: 81.25%] [G loss: 0.577776]\n",
      "epoch:3 step:3671 [D loss: 0.460404, acc.: 78.91%] [G loss: 0.537476]\n",
      "epoch:3 step:3672 [D loss: 0.593711, acc.: 62.50%] [G loss: 0.552455]\n",
      "epoch:3 step:3673 [D loss: 0.513564, acc.: 71.09%] [G loss: 0.477765]\n",
      "epoch:3 step:3674 [D loss: 0.493570, acc.: 77.34%] [G loss: 0.463303]\n",
      "epoch:3 step:3675 [D loss: 0.507758, acc.: 75.00%] [G loss: 0.461169]\n",
      "epoch:3 step:3676 [D loss: 0.497479, acc.: 80.47%] [G loss: 0.402025]\n",
      "epoch:3 step:3677 [D loss: 0.559081, acc.: 67.97%] [G loss: 0.479657]\n",
      "epoch:3 step:3678 [D loss: 0.571120, acc.: 71.09%] [G loss: 0.452198]\n",
      "epoch:3 step:3679 [D loss: 0.482023, acc.: 81.25%] [G loss: 0.555414]\n",
      "epoch:3 step:3680 [D loss: 0.486731, acc.: 75.78%] [G loss: 0.544283]\n",
      "epoch:3 step:3681 [D loss: 0.473409, acc.: 79.69%] [G loss: 0.522058]\n",
      "epoch:3 step:3682 [D loss: 0.452917, acc.: 81.25%] [G loss: 0.696529]\n",
      "epoch:3 step:3683 [D loss: 0.557157, acc.: 64.84%] [G loss: 0.554143]\n",
      "epoch:3 step:3684 [D loss: 0.571415, acc.: 72.66%] [G loss: 0.538427]\n",
      "epoch:3 step:3685 [D loss: 0.516290, acc.: 75.00%] [G loss: 0.611773]\n",
      "epoch:3 step:3686 [D loss: 0.475698, acc.: 79.69%] [G loss: 0.579513]\n",
      "epoch:3 step:3687 [D loss: 0.481895, acc.: 77.34%] [G loss: 0.639712]\n",
      "epoch:3 step:3688 [D loss: 0.490224, acc.: 82.03%] [G loss: 0.501872]\n",
      "epoch:3 step:3689 [D loss: 0.554187, acc.: 71.09%] [G loss: 0.435255]\n",
      "epoch:3 step:3690 [D loss: 0.525054, acc.: 72.66%] [G loss: 0.499950]\n",
      "epoch:3 step:3691 [D loss: 0.574809, acc.: 71.09%] [G loss: 0.497125]\n",
      "epoch:3 step:3692 [D loss: 0.541462, acc.: 73.44%] [G loss: 0.516778]\n",
      "epoch:3 step:3693 [D loss: 0.541651, acc.: 72.66%] [G loss: 0.548106]\n",
      "epoch:3 step:3694 [D loss: 0.535242, acc.: 75.78%] [G loss: 0.445151]\n",
      "epoch:3 step:3695 [D loss: 0.455155, acc.: 78.12%] [G loss: 0.571827]\n",
      "epoch:3 step:3696 [D loss: 0.474128, acc.: 80.47%] [G loss: 0.744010]\n",
      "epoch:3 step:3697 [D loss: 0.514225, acc.: 76.56%] [G loss: 0.685824]\n",
      "epoch:3 step:3698 [D loss: 0.503495, acc.: 79.69%] [G loss: 0.613033]\n",
      "epoch:3 step:3699 [D loss: 0.530266, acc.: 74.22%] [G loss: 0.573517]\n",
      "epoch:3 step:3700 [D loss: 0.474184, acc.: 80.47%] [G loss: 0.626967]\n",
      "epoch:3 step:3701 [D loss: 0.419589, acc.: 84.38%] [G loss: 0.780424]\n",
      "epoch:3 step:3702 [D loss: 0.504722, acc.: 75.78%] [G loss: 0.693619]\n",
      "epoch:3 step:3703 [D loss: 0.578728, acc.: 68.75%] [G loss: 0.591273]\n",
      "epoch:3 step:3704 [D loss: 0.530712, acc.: 73.44%] [G loss: 0.543867]\n",
      "epoch:3 step:3705 [D loss: 0.489402, acc.: 75.00%] [G loss: 0.616703]\n",
      "epoch:3 step:3706 [D loss: 0.464971, acc.: 77.34%] [G loss: 0.612557]\n",
      "epoch:3 step:3707 [D loss: 0.568007, acc.: 70.31%] [G loss: 0.669063]\n",
      "epoch:3 step:3708 [D loss: 0.498914, acc.: 75.78%] [G loss: 0.632257]\n",
      "epoch:3 step:3709 [D loss: 0.469804, acc.: 78.91%] [G loss: 0.613418]\n",
      "epoch:3 step:3710 [D loss: 0.487022, acc.: 78.91%] [G loss: 0.731556]\n",
      "epoch:3 step:3711 [D loss: 0.521576, acc.: 71.09%] [G loss: 0.650506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3712 [D loss: 0.488672, acc.: 74.22%] [G loss: 0.757744]\n",
      "epoch:3 step:3713 [D loss: 0.586199, acc.: 67.97%] [G loss: 0.386895]\n",
      "epoch:3 step:3714 [D loss: 0.521229, acc.: 74.22%] [G loss: 0.512139]\n",
      "epoch:3 step:3715 [D loss: 0.492144, acc.: 77.34%] [G loss: 0.522422]\n",
      "epoch:3 step:3716 [D loss: 0.513722, acc.: 75.00%] [G loss: 0.655706]\n",
      "epoch:3 step:3717 [D loss: 0.461698, acc.: 82.81%] [G loss: 0.682052]\n",
      "epoch:3 step:3718 [D loss: 0.570189, acc.: 69.53%] [G loss: 0.542344]\n",
      "epoch:3 step:3719 [D loss: 0.506013, acc.: 78.12%] [G loss: 0.548923]\n",
      "epoch:3 step:3720 [D loss: 0.462200, acc.: 76.56%] [G loss: 0.626415]\n",
      "epoch:3 step:3721 [D loss: 0.445165, acc.: 83.59%] [G loss: 0.582635]\n",
      "epoch:3 step:3722 [D loss: 0.415617, acc.: 85.94%] [G loss: 0.745235]\n",
      "epoch:3 step:3723 [D loss: 0.445452, acc.: 83.59%] [G loss: 0.721273]\n",
      "epoch:3 step:3724 [D loss: 0.576545, acc.: 71.88%] [G loss: 0.585911]\n",
      "epoch:3 step:3725 [D loss: 0.494552, acc.: 77.34%] [G loss: 0.667454]\n",
      "epoch:3 step:3726 [D loss: 0.532738, acc.: 73.44%] [G loss: 0.695377]\n",
      "epoch:3 step:3727 [D loss: 0.555198, acc.: 71.88%] [G loss: 0.538411]\n",
      "epoch:3 step:3728 [D loss: 0.593139, acc.: 62.50%] [G loss: 0.416993]\n",
      "epoch:3 step:3729 [D loss: 0.444295, acc.: 81.25%] [G loss: 0.684314]\n",
      "epoch:3 step:3730 [D loss: 0.478991, acc.: 78.12%] [G loss: 0.661031]\n",
      "epoch:3 step:3731 [D loss: 0.698546, acc.: 60.16%] [G loss: 0.441015]\n",
      "epoch:3 step:3732 [D loss: 0.419214, acc.: 85.94%] [G loss: 0.604539]\n",
      "epoch:3 step:3733 [D loss: 0.505382, acc.: 79.69%] [G loss: 0.535165]\n",
      "epoch:3 step:3734 [D loss: 0.408980, acc.: 86.72%] [G loss: 0.564106]\n",
      "epoch:3 step:3735 [D loss: 0.438565, acc.: 82.81%] [G loss: 0.536043]\n",
      "epoch:3 step:3736 [D loss: 0.404427, acc.: 85.94%] [G loss: 0.794197]\n",
      "epoch:3 step:3737 [D loss: 0.473309, acc.: 79.69%] [G loss: 0.912227]\n",
      "epoch:3 step:3738 [D loss: 0.463821, acc.: 72.66%] [G loss: 1.050154]\n",
      "epoch:3 step:3739 [D loss: 0.754764, acc.: 64.06%] [G loss: 0.848569]\n",
      "epoch:3 step:3740 [D loss: 0.419772, acc.: 82.03%] [G loss: 0.888275]\n",
      "epoch:3 step:3741 [D loss: 0.375998, acc.: 85.16%] [G loss: 1.086819]\n",
      "epoch:3 step:3742 [D loss: 0.480729, acc.: 79.69%] [G loss: 0.984431]\n",
      "epoch:3 step:3743 [D loss: 0.526491, acc.: 74.22%] [G loss: 0.510626]\n",
      "epoch:3 step:3744 [D loss: 0.460033, acc.: 81.25%] [G loss: 0.921198]\n",
      "epoch:3 step:3745 [D loss: 0.546647, acc.: 71.88%] [G loss: 0.799627]\n",
      "epoch:3 step:3746 [D loss: 0.480367, acc.: 75.78%] [G loss: 0.942956]\n",
      "epoch:3 step:3747 [D loss: 0.340411, acc.: 85.16%] [G loss: 1.033484]\n",
      "epoch:3 step:3748 [D loss: 0.482852, acc.: 78.91%] [G loss: 1.058363]\n",
      "epoch:4 step:3749 [D loss: 0.552981, acc.: 71.09%] [G loss: 0.957815]\n",
      "epoch:4 step:3750 [D loss: 0.473188, acc.: 77.34%] [G loss: 0.930571]\n",
      "epoch:4 step:3751 [D loss: 0.612554, acc.: 66.41%] [G loss: 0.640998]\n",
      "epoch:4 step:3752 [D loss: 0.495461, acc.: 75.78%] [G loss: 0.728159]\n",
      "epoch:4 step:3753 [D loss: 0.539967, acc.: 73.44%] [G loss: 0.619579]\n",
      "epoch:4 step:3754 [D loss: 0.509456, acc.: 71.88%] [G loss: 0.601954]\n",
      "epoch:4 step:3755 [D loss: 0.466065, acc.: 81.25%] [G loss: 0.724567]\n",
      "epoch:4 step:3756 [D loss: 0.510266, acc.: 71.88%] [G loss: 0.654982]\n",
      "epoch:4 step:3757 [D loss: 0.522477, acc.: 73.44%] [G loss: 0.679823]\n",
      "epoch:4 step:3758 [D loss: 0.556352, acc.: 75.78%] [G loss: 0.567925]\n",
      "epoch:4 step:3759 [D loss: 0.497221, acc.: 75.78%] [G loss: 0.628394]\n",
      "epoch:4 step:3760 [D loss: 0.592370, acc.: 68.75%] [G loss: 0.511483]\n",
      "epoch:4 step:3761 [D loss: 0.477567, acc.: 82.03%] [G loss: 0.558107]\n",
      "epoch:4 step:3762 [D loss: 0.485307, acc.: 73.44%] [G loss: 0.542524]\n",
      "epoch:4 step:3763 [D loss: 0.507953, acc.: 76.56%] [G loss: 0.580533]\n",
      "epoch:4 step:3764 [D loss: 0.430710, acc.: 83.59%] [G loss: 0.768781]\n",
      "epoch:4 step:3765 [D loss: 0.501779, acc.: 76.56%] [G loss: 0.623936]\n",
      "epoch:4 step:3766 [D loss: 0.631733, acc.: 61.72%] [G loss: 0.507135]\n",
      "epoch:4 step:3767 [D loss: 0.604790, acc.: 67.19%] [G loss: 0.399432]\n",
      "epoch:4 step:3768 [D loss: 0.584563, acc.: 71.88%] [G loss: 0.487419]\n",
      "epoch:4 step:3769 [D loss: 0.537669, acc.: 75.78%] [G loss: 0.565005]\n",
      "epoch:4 step:3770 [D loss: 0.457891, acc.: 81.25%] [G loss: 0.792055]\n",
      "epoch:4 step:3771 [D loss: 0.503835, acc.: 78.12%] [G loss: 0.753403]\n",
      "epoch:4 step:3772 [D loss: 0.483965, acc.: 75.78%] [G loss: 0.688412]\n",
      "epoch:4 step:3773 [D loss: 0.507613, acc.: 73.44%] [G loss: 0.562236]\n",
      "epoch:4 step:3774 [D loss: 0.590717, acc.: 67.97%] [G loss: 0.530690]\n",
      "epoch:4 step:3775 [D loss: 0.515418, acc.: 75.78%] [G loss: 0.525361]\n",
      "epoch:4 step:3776 [D loss: 0.531869, acc.: 71.88%] [G loss: 0.487949]\n",
      "epoch:4 step:3777 [D loss: 0.485360, acc.: 81.25%] [G loss: 0.634371]\n",
      "epoch:4 step:3778 [D loss: 0.545987, acc.: 70.31%] [G loss: 0.556150]\n",
      "epoch:4 step:3779 [D loss: 0.566047, acc.: 72.66%] [G loss: 0.440987]\n",
      "epoch:4 step:3780 [D loss: 0.540037, acc.: 75.00%] [G loss: 0.519364]\n",
      "epoch:4 step:3781 [D loss: 0.503354, acc.: 72.66%] [G loss: 0.520911]\n",
      "epoch:4 step:3782 [D loss: 0.478383, acc.: 75.00%] [G loss: 0.544656]\n",
      "epoch:4 step:3783 [D loss: 0.523633, acc.: 75.78%] [G loss: 0.659117]\n",
      "epoch:4 step:3784 [D loss: 0.443891, acc.: 85.16%] [G loss: 0.817654]\n",
      "epoch:4 step:3785 [D loss: 0.493158, acc.: 81.25%] [G loss: 0.653405]\n",
      "epoch:4 step:3786 [D loss: 0.548500, acc.: 75.00%] [G loss: 0.626240]\n",
      "epoch:4 step:3787 [D loss: 0.476490, acc.: 77.34%] [G loss: 0.583918]\n",
      "epoch:4 step:3788 [D loss: 0.444231, acc.: 80.47%] [G loss: 0.572383]\n",
      "epoch:4 step:3789 [D loss: 0.545867, acc.: 69.53%] [G loss: 0.612093]\n",
      "epoch:4 step:3790 [D loss: 0.497101, acc.: 78.91%] [G loss: 0.497065]\n",
      "epoch:4 step:3791 [D loss: 0.479481, acc.: 78.12%] [G loss: 0.491215]\n",
      "epoch:4 step:3792 [D loss: 0.555562, acc.: 73.44%] [G loss: 0.493126]\n",
      "epoch:4 step:3793 [D loss: 0.455283, acc.: 82.81%] [G loss: 0.719884]\n",
      "epoch:4 step:3794 [D loss: 0.444843, acc.: 81.25%] [G loss: 0.683995]\n",
      "epoch:4 step:3795 [D loss: 0.509248, acc.: 75.78%] [G loss: 0.659892]\n",
      "epoch:4 step:3796 [D loss: 0.495944, acc.: 74.22%] [G loss: 0.617436]\n",
      "epoch:4 step:3797 [D loss: 0.471474, acc.: 81.25%] [G loss: 0.603815]\n",
      "epoch:4 step:3798 [D loss: 0.465735, acc.: 85.16%] [G loss: 0.666432]\n",
      "epoch:4 step:3799 [D loss: 0.585559, acc.: 67.97%] [G loss: 0.512047]\n",
      "epoch:4 step:3800 [D loss: 0.505664, acc.: 78.91%] [G loss: 0.550309]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.439101\n",
      "FID: 57.133339\n",
      "0 = 13.418821625518811\n",
      "1 = 0.12116411687239857\n",
      "2 = 0.9679999947547913\n",
      "3 = 0.9362000226974487\n",
      "4 = 0.9998000264167786\n",
      "5 = 0.9997864365577698\n",
      "6 = 0.9362000226974487\n",
      "7 = 9.02031233150958\n",
      "8 = 0.15198376501115285\n",
      "9 = 0.8592000007629395\n",
      "10 = 0.8289999961853027\n",
      "11 = 0.8894000053405762\n",
      "12 = 0.8822903633117676\n",
      "13 = 0.8289999961853027\n",
      "14 = 5.439116954803467\n",
      "15 = 7.6754655838012695\n",
      "16 = 0.33908042311668396\n",
      "17 = 5.439101219177246\n",
      "18 = 57.133338928222656\n",
      "epoch:4 step:3801 [D loss: 0.500859, acc.: 78.12%] [G loss: 0.653757]\n",
      "epoch:4 step:3802 [D loss: 0.447183, acc.: 85.16%] [G loss: 0.779942]\n",
      "epoch:4 step:3803 [D loss: 0.507608, acc.: 75.78%] [G loss: 0.657060]\n",
      "epoch:4 step:3804 [D loss: 0.481234, acc.: 80.47%] [G loss: 0.566137]\n",
      "epoch:4 step:3805 [D loss: 0.543895, acc.: 72.66%] [G loss: 0.500759]\n",
      "epoch:4 step:3806 [D loss: 0.505375, acc.: 74.22%] [G loss: 0.583287]\n",
      "epoch:4 step:3807 [D loss: 0.538785, acc.: 68.75%] [G loss: 0.556213]\n",
      "epoch:4 step:3808 [D loss: 0.502834, acc.: 73.44%] [G loss: 0.758095]\n",
      "epoch:4 step:3809 [D loss: 0.526332, acc.: 71.88%] [G loss: 0.717786]\n",
      "epoch:4 step:3810 [D loss: 0.591241, acc.: 65.62%] [G loss: 0.499161]\n",
      "epoch:4 step:3811 [D loss: 0.596075, acc.: 64.84%] [G loss: 0.579251]\n",
      "epoch:4 step:3812 [D loss: 0.582970, acc.: 70.31%] [G loss: 0.516294]\n",
      "epoch:4 step:3813 [D loss: 0.560441, acc.: 75.78%] [G loss: 0.453785]\n",
      "epoch:4 step:3814 [D loss: 0.524306, acc.: 75.78%] [G loss: 0.518194]\n",
      "epoch:4 step:3815 [D loss: 0.517367, acc.: 77.34%] [G loss: 0.599640]\n",
      "epoch:4 step:3816 [D loss: 0.490777, acc.: 75.00%] [G loss: 0.593932]\n",
      "epoch:4 step:3817 [D loss: 0.507677, acc.: 75.78%] [G loss: 0.596087]\n",
      "epoch:4 step:3818 [D loss: 0.526237, acc.: 75.00%] [G loss: 0.568519]\n",
      "epoch:4 step:3819 [D loss: 0.494057, acc.: 76.56%] [G loss: 0.483577]\n",
      "epoch:4 step:3820 [D loss: 0.470789, acc.: 78.91%] [G loss: 0.512620]\n",
      "epoch:4 step:3821 [D loss: 0.464481, acc.: 79.69%] [G loss: 0.553256]\n",
      "epoch:4 step:3822 [D loss: 0.491251, acc.: 78.12%] [G loss: 0.541267]\n",
      "epoch:4 step:3823 [D loss: 0.466102, acc.: 75.00%] [G loss: 0.665915]\n",
      "epoch:4 step:3824 [D loss: 0.480177, acc.: 75.00%] [G loss: 0.669482]\n",
      "epoch:4 step:3825 [D loss: 0.415434, acc.: 82.81%] [G loss: 0.620190]\n",
      "epoch:4 step:3826 [D loss: 0.561188, acc.: 69.53%] [G loss: 0.657959]\n",
      "epoch:4 step:3827 [D loss: 0.569209, acc.: 66.41%] [G loss: 0.466088]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3828 [D loss: 0.525392, acc.: 78.12%] [G loss: 0.522904]\n",
      "epoch:4 step:3829 [D loss: 0.535955, acc.: 75.00%] [G loss: 0.568828]\n",
      "epoch:4 step:3830 [D loss: 0.470337, acc.: 82.81%] [G loss: 0.590542]\n",
      "epoch:4 step:3831 [D loss: 0.502214, acc.: 78.12%] [G loss: 0.522581]\n",
      "epoch:4 step:3832 [D loss: 0.516821, acc.: 78.12%] [G loss: 0.611421]\n",
      "epoch:4 step:3833 [D loss: 0.529199, acc.: 73.44%] [G loss: 0.539842]\n",
      "epoch:4 step:3834 [D loss: 0.542876, acc.: 71.88%] [G loss: 0.579481]\n",
      "epoch:4 step:3835 [D loss: 0.473591, acc.: 82.03%] [G loss: 0.618802]\n",
      "epoch:4 step:3836 [D loss: 0.479183, acc.: 78.12%] [G loss: 0.656754]\n",
      "epoch:4 step:3837 [D loss: 0.478636, acc.: 77.34%] [G loss: 0.669340]\n",
      "epoch:4 step:3838 [D loss: 0.526688, acc.: 75.78%] [G loss: 0.580879]\n",
      "epoch:4 step:3839 [D loss: 0.536578, acc.: 75.00%] [G loss: 0.541687]\n",
      "epoch:4 step:3840 [D loss: 0.460311, acc.: 79.69%] [G loss: 0.527679]\n",
      "epoch:4 step:3841 [D loss: 0.530582, acc.: 75.78%] [G loss: 0.621267]\n",
      "epoch:4 step:3842 [D loss: 0.494901, acc.: 79.69%] [G loss: 0.656214]\n",
      "epoch:4 step:3843 [D loss: 0.493487, acc.: 80.47%] [G loss: 0.548980]\n",
      "epoch:4 step:3844 [D loss: 0.423904, acc.: 82.03%] [G loss: 0.608009]\n",
      "epoch:4 step:3845 [D loss: 0.533904, acc.: 73.44%] [G loss: 0.605689]\n",
      "epoch:4 step:3846 [D loss: 0.518123, acc.: 76.56%] [G loss: 0.659127]\n",
      "epoch:4 step:3847 [D loss: 0.530971, acc.: 71.88%] [G loss: 0.475694]\n",
      "epoch:4 step:3848 [D loss: 0.466772, acc.: 75.78%] [G loss: 0.745607]\n",
      "epoch:4 step:3849 [D loss: 0.512868, acc.: 74.22%] [G loss: 0.666494]\n",
      "epoch:4 step:3850 [D loss: 0.543853, acc.: 77.34%] [G loss: 0.601663]\n",
      "epoch:4 step:3851 [D loss: 0.447501, acc.: 80.47%] [G loss: 0.508635]\n",
      "epoch:4 step:3852 [D loss: 0.522473, acc.: 78.12%] [G loss: 0.519170]\n",
      "epoch:4 step:3853 [D loss: 0.538594, acc.: 69.53%] [G loss: 0.587572]\n",
      "epoch:4 step:3854 [D loss: 0.524370, acc.: 76.56%] [G loss: 0.515262]\n",
      "epoch:4 step:3855 [D loss: 0.531714, acc.: 78.12%] [G loss: 0.598270]\n",
      "epoch:4 step:3856 [D loss: 0.552702, acc.: 71.88%] [G loss: 0.547475]\n",
      "epoch:4 step:3857 [D loss: 0.585149, acc.: 70.31%] [G loss: 0.455406]\n",
      "epoch:4 step:3858 [D loss: 0.560055, acc.: 69.53%] [G loss: 0.408632]\n",
      "epoch:4 step:3859 [D loss: 0.479956, acc.: 78.91%] [G loss: 0.544683]\n",
      "epoch:4 step:3860 [D loss: 0.484935, acc.: 76.56%] [G loss: 0.599236]\n",
      "epoch:4 step:3861 [D loss: 0.599789, acc.: 66.41%] [G loss: 0.422651]\n",
      "epoch:4 step:3862 [D loss: 0.504725, acc.: 81.25%] [G loss: 0.545868]\n",
      "epoch:4 step:3863 [D loss: 0.489839, acc.: 75.00%] [G loss: 0.594266]\n",
      "epoch:4 step:3864 [D loss: 0.530927, acc.: 72.66%] [G loss: 0.608194]\n",
      "epoch:4 step:3865 [D loss: 0.538208, acc.: 71.88%] [G loss: 0.586556]\n",
      "epoch:4 step:3866 [D loss: 0.503110, acc.: 74.22%] [G loss: 0.883675]\n",
      "epoch:4 step:3867 [D loss: 0.470373, acc.: 82.81%] [G loss: 0.826091]\n",
      "epoch:4 step:3868 [D loss: 0.615638, acc.: 66.41%] [G loss: 0.647945]\n",
      "epoch:4 step:3869 [D loss: 0.565870, acc.: 68.75%] [G loss: 0.484543]\n",
      "epoch:4 step:3870 [D loss: 0.516453, acc.: 74.22%] [G loss: 0.627415]\n",
      "epoch:4 step:3871 [D loss: 0.516897, acc.: 75.78%] [G loss: 0.475945]\n",
      "epoch:4 step:3872 [D loss: 0.539852, acc.: 75.00%] [G loss: 0.515381]\n",
      "epoch:4 step:3873 [D loss: 0.547926, acc.: 66.41%] [G loss: 0.476246]\n",
      "epoch:4 step:3874 [D loss: 0.537667, acc.: 68.75%] [G loss: 0.474772]\n",
      "epoch:4 step:3875 [D loss: 0.544028, acc.: 75.00%] [G loss: 0.570359]\n",
      "epoch:4 step:3876 [D loss: 0.531236, acc.: 73.44%] [G loss: 0.480688]\n",
      "epoch:4 step:3877 [D loss: 0.512322, acc.: 79.69%] [G loss: 0.611412]\n",
      "epoch:4 step:3878 [D loss: 0.496376, acc.: 73.44%] [G loss: 0.484598]\n",
      "epoch:4 step:3879 [D loss: 0.529676, acc.: 78.12%] [G loss: 0.643934]\n",
      "epoch:4 step:3880 [D loss: 0.535765, acc.: 73.44%] [G loss: 0.622594]\n",
      "epoch:4 step:3881 [D loss: 0.545562, acc.: 75.00%] [G loss: 0.581694]\n",
      "epoch:4 step:3882 [D loss: 0.505280, acc.: 72.66%] [G loss: 0.552307]\n",
      "epoch:4 step:3883 [D loss: 0.451987, acc.: 82.81%] [G loss: 0.663723]\n",
      "epoch:4 step:3884 [D loss: 0.521647, acc.: 75.78%] [G loss: 0.646492]\n",
      "epoch:4 step:3885 [D loss: 0.526552, acc.: 74.22%] [G loss: 0.557390]\n",
      "epoch:4 step:3886 [D loss: 0.483938, acc.: 74.22%] [G loss: 0.562179]\n",
      "epoch:4 step:3887 [D loss: 0.559377, acc.: 70.31%] [G loss: 0.571274]\n",
      "epoch:4 step:3888 [D loss: 0.509748, acc.: 75.00%] [G loss: 0.616020]\n",
      "epoch:4 step:3889 [D loss: 0.493585, acc.: 75.00%] [G loss: 0.591345]\n",
      "epoch:4 step:3890 [D loss: 0.564772, acc.: 64.84%] [G loss: 0.522971]\n",
      "epoch:4 step:3891 [D loss: 0.582386, acc.: 68.75%] [G loss: 0.440598]\n",
      "epoch:4 step:3892 [D loss: 0.537752, acc.: 73.44%] [G loss: 0.552664]\n",
      "epoch:4 step:3893 [D loss: 0.574185, acc.: 69.53%] [G loss: 0.660583]\n",
      "epoch:4 step:3894 [D loss: 0.518929, acc.: 77.34%] [G loss: 0.721593]\n",
      "epoch:4 step:3895 [D loss: 0.591947, acc.: 69.53%] [G loss: 0.397924]\n",
      "epoch:4 step:3896 [D loss: 0.571896, acc.: 71.09%] [G loss: 0.489957]\n",
      "epoch:4 step:3897 [D loss: 0.523945, acc.: 72.66%] [G loss: 0.488879]\n",
      "epoch:4 step:3898 [D loss: 0.534280, acc.: 74.22%] [G loss: 0.535995]\n",
      "epoch:4 step:3899 [D loss: 0.517221, acc.: 77.34%] [G loss: 0.520304]\n",
      "epoch:4 step:3900 [D loss: 0.536574, acc.: 75.00%] [G loss: 0.710208]\n",
      "epoch:4 step:3901 [D loss: 0.623134, acc.: 62.50%] [G loss: 0.559112]\n",
      "epoch:4 step:3902 [D loss: 0.529635, acc.: 75.00%] [G loss: 0.501418]\n",
      "epoch:4 step:3903 [D loss: 0.440761, acc.: 81.25%] [G loss: 0.661624]\n",
      "epoch:4 step:3904 [D loss: 0.487221, acc.: 78.91%] [G loss: 0.676600]\n",
      "epoch:4 step:3905 [D loss: 0.542222, acc.: 73.44%] [G loss: 0.602567]\n",
      "epoch:4 step:3906 [D loss: 0.517057, acc.: 74.22%] [G loss: 0.525860]\n",
      "epoch:4 step:3907 [D loss: 0.548785, acc.: 69.53%] [G loss: 0.584910]\n",
      "epoch:4 step:3908 [D loss: 0.455931, acc.: 80.47%] [G loss: 0.682014]\n",
      "epoch:4 step:3909 [D loss: 0.473286, acc.: 76.56%] [G loss: 0.550697]\n",
      "epoch:4 step:3910 [D loss: 0.461405, acc.: 80.47%] [G loss: 0.609452]\n",
      "epoch:4 step:3911 [D loss: 0.495046, acc.: 74.22%] [G loss: 0.746158]\n",
      "epoch:4 step:3912 [D loss: 0.503641, acc.: 80.47%] [G loss: 0.578643]\n",
      "epoch:4 step:3913 [D loss: 0.489839, acc.: 76.56%] [G loss: 0.610982]\n",
      "epoch:4 step:3914 [D loss: 0.486183, acc.: 82.03%] [G loss: 0.704992]\n",
      "epoch:4 step:3915 [D loss: 0.541046, acc.: 70.31%] [G loss: 0.545282]\n",
      "epoch:4 step:3916 [D loss: 0.501054, acc.: 77.34%] [G loss: 0.532556]\n",
      "epoch:4 step:3917 [D loss: 0.538999, acc.: 69.53%] [G loss: 0.577520]\n",
      "epoch:4 step:3918 [D loss: 0.474542, acc.: 76.56%] [G loss: 0.553698]\n",
      "epoch:4 step:3919 [D loss: 0.485893, acc.: 79.69%] [G loss: 0.654512]\n",
      "epoch:4 step:3920 [D loss: 0.453989, acc.: 79.69%] [G loss: 0.803300]\n",
      "epoch:4 step:3921 [D loss: 0.469513, acc.: 79.69%] [G loss: 0.566624]\n",
      "epoch:4 step:3922 [D loss: 0.554738, acc.: 68.75%] [G loss: 0.554521]\n",
      "epoch:4 step:3923 [D loss: 0.472064, acc.: 82.03%] [G loss: 0.599158]\n",
      "epoch:4 step:3924 [D loss: 0.468717, acc.: 77.34%] [G loss: 0.565118]\n",
      "epoch:4 step:3925 [D loss: 0.538052, acc.: 71.09%] [G loss: 0.482566]\n",
      "epoch:4 step:3926 [D loss: 0.488575, acc.: 77.34%] [G loss: 0.571807]\n",
      "epoch:4 step:3927 [D loss: 0.494499, acc.: 76.56%] [G loss: 0.659278]\n",
      "epoch:4 step:3928 [D loss: 0.570655, acc.: 67.19%] [G loss: 0.549360]\n",
      "epoch:4 step:3929 [D loss: 0.524391, acc.: 74.22%] [G loss: 0.506819]\n",
      "epoch:4 step:3930 [D loss: 0.529062, acc.: 71.09%] [G loss: 0.561824]\n",
      "epoch:4 step:3931 [D loss: 0.563580, acc.: 68.75%] [G loss: 0.633667]\n",
      "epoch:4 step:3932 [D loss: 0.558397, acc.: 68.75%] [G loss: 0.539642]\n",
      "epoch:4 step:3933 [D loss: 0.578961, acc.: 71.09%] [G loss: 0.503300]\n",
      "epoch:4 step:3934 [D loss: 0.554522, acc.: 75.78%] [G loss: 0.504644]\n",
      "epoch:4 step:3935 [D loss: 0.539589, acc.: 75.00%] [G loss: 0.565367]\n",
      "epoch:4 step:3936 [D loss: 0.513304, acc.: 71.88%] [G loss: 0.513959]\n",
      "epoch:4 step:3937 [D loss: 0.561500, acc.: 70.31%] [G loss: 0.489535]\n",
      "epoch:4 step:3938 [D loss: 0.411223, acc.: 84.38%] [G loss: 0.738806]\n",
      "epoch:4 step:3939 [D loss: 0.530619, acc.: 74.22%] [G loss: 0.603510]\n",
      "epoch:4 step:3940 [D loss: 0.516855, acc.: 73.44%] [G loss: 0.573225]\n",
      "epoch:4 step:3941 [D loss: 0.494628, acc.: 80.47%] [G loss: 0.576639]\n",
      "epoch:4 step:3942 [D loss: 0.441768, acc.: 82.81%] [G loss: 0.712049]\n",
      "epoch:4 step:3943 [D loss: 0.521723, acc.: 75.00%] [G loss: 0.629725]\n",
      "epoch:4 step:3944 [D loss: 0.546938, acc.: 71.09%] [G loss: 0.533294]\n",
      "epoch:4 step:3945 [D loss: 0.488448, acc.: 77.34%] [G loss: 0.654591]\n",
      "epoch:4 step:3946 [D loss: 0.451130, acc.: 82.03%] [G loss: 0.588197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3947 [D loss: 0.494734, acc.: 77.34%] [G loss: 0.697426]\n",
      "epoch:4 step:3948 [D loss: 0.601340, acc.: 71.09%] [G loss: 0.638820]\n",
      "epoch:4 step:3949 [D loss: 0.564982, acc.: 71.88%] [G loss: 0.529077]\n",
      "epoch:4 step:3950 [D loss: 0.509402, acc.: 74.22%] [G loss: 0.703329]\n",
      "epoch:4 step:3951 [D loss: 0.578629, acc.: 69.53%] [G loss: 0.510880]\n",
      "epoch:4 step:3952 [D loss: 0.525128, acc.: 77.34%] [G loss: 0.485391]\n",
      "epoch:4 step:3953 [D loss: 0.484880, acc.: 75.78%] [G loss: 0.570943]\n",
      "epoch:4 step:3954 [D loss: 0.527186, acc.: 75.00%] [G loss: 0.625593]\n",
      "epoch:4 step:3955 [D loss: 0.420250, acc.: 82.81%] [G loss: 0.686178]\n",
      "epoch:4 step:3956 [D loss: 0.468792, acc.: 79.69%] [G loss: 0.619318]\n",
      "epoch:4 step:3957 [D loss: 0.553799, acc.: 67.19%] [G loss: 0.646360]\n",
      "epoch:4 step:3958 [D loss: 0.532135, acc.: 72.66%] [G loss: 0.628991]\n",
      "epoch:4 step:3959 [D loss: 0.529540, acc.: 75.00%] [G loss: 0.415981]\n",
      "epoch:4 step:3960 [D loss: 0.512143, acc.: 75.00%] [G loss: 0.470158]\n",
      "epoch:4 step:3961 [D loss: 0.484181, acc.: 77.34%] [G loss: 0.584940]\n",
      "epoch:4 step:3962 [D loss: 0.637226, acc.: 60.94%] [G loss: 0.424054]\n",
      "epoch:4 step:3963 [D loss: 0.553651, acc.: 72.66%] [G loss: 0.492567]\n",
      "epoch:4 step:3964 [D loss: 0.483731, acc.: 78.91%] [G loss: 0.567838]\n",
      "epoch:4 step:3965 [D loss: 0.499296, acc.: 75.78%] [G loss: 0.527868]\n",
      "epoch:4 step:3966 [D loss: 0.509685, acc.: 75.78%] [G loss: 0.595587]\n",
      "epoch:4 step:3967 [D loss: 0.537283, acc.: 75.78%] [G loss: 0.476648]\n",
      "epoch:4 step:3968 [D loss: 0.673377, acc.: 57.81%] [G loss: 0.387804]\n",
      "epoch:4 step:3969 [D loss: 0.444780, acc.: 82.81%] [G loss: 0.564254]\n",
      "epoch:4 step:3970 [D loss: 0.474421, acc.: 77.34%] [G loss: 0.663156]\n",
      "epoch:4 step:3971 [D loss: 0.457983, acc.: 80.47%] [G loss: 0.674533]\n",
      "epoch:4 step:3972 [D loss: 0.555590, acc.: 72.66%] [G loss: 0.630076]\n",
      "epoch:4 step:3973 [D loss: 0.542033, acc.: 72.66%] [G loss: 0.559433]\n",
      "epoch:4 step:3974 [D loss: 0.543643, acc.: 72.66%] [G loss: 0.621959]\n",
      "epoch:4 step:3975 [D loss: 0.580581, acc.: 68.75%] [G loss: 0.510569]\n",
      "epoch:4 step:3976 [D loss: 0.575040, acc.: 67.97%] [G loss: 0.403471]\n",
      "epoch:4 step:3977 [D loss: 0.548157, acc.: 76.56%] [G loss: 0.513700]\n",
      "epoch:4 step:3978 [D loss: 0.501609, acc.: 75.78%] [G loss: 0.550831]\n",
      "epoch:4 step:3979 [D loss: 0.490733, acc.: 80.47%] [G loss: 0.638732]\n",
      "epoch:4 step:3980 [D loss: 0.450268, acc.: 80.47%] [G loss: 0.691503]\n",
      "epoch:4 step:3981 [D loss: 0.549451, acc.: 71.09%] [G loss: 0.672406]\n",
      "epoch:4 step:3982 [D loss: 0.517094, acc.: 73.44%] [G loss: 0.571673]\n",
      "epoch:4 step:3983 [D loss: 0.521665, acc.: 71.88%] [G loss: 0.627154]\n",
      "epoch:4 step:3984 [D loss: 0.519342, acc.: 70.31%] [G loss: 0.503468]\n",
      "epoch:4 step:3985 [D loss: 0.595955, acc.: 68.75%] [G loss: 0.510167]\n",
      "epoch:4 step:3986 [D loss: 0.523230, acc.: 74.22%] [G loss: 0.591938]\n",
      "epoch:4 step:3987 [D loss: 0.491238, acc.: 78.12%] [G loss: 0.598337]\n",
      "epoch:4 step:3988 [D loss: 0.537918, acc.: 71.88%] [G loss: 0.549912]\n",
      "epoch:4 step:3989 [D loss: 0.558612, acc.: 70.31%] [G loss: 0.532106]\n",
      "epoch:4 step:3990 [D loss: 0.524417, acc.: 74.22%] [G loss: 0.398700]\n",
      "epoch:4 step:3991 [D loss: 0.550480, acc.: 74.22%] [G loss: 0.519558]\n",
      "epoch:4 step:3992 [D loss: 0.513846, acc.: 75.00%] [G loss: 0.619684]\n",
      "epoch:4 step:3993 [D loss: 0.478078, acc.: 79.69%] [G loss: 0.563253]\n",
      "epoch:4 step:3994 [D loss: 0.534043, acc.: 71.09%] [G loss: 0.595669]\n",
      "epoch:4 step:3995 [D loss: 0.606942, acc.: 66.41%] [G loss: 0.591480]\n",
      "epoch:4 step:3996 [D loss: 0.513191, acc.: 77.34%] [G loss: 0.557398]\n",
      "epoch:4 step:3997 [D loss: 0.524468, acc.: 74.22%] [G loss: 0.610838]\n",
      "epoch:4 step:3998 [D loss: 0.550216, acc.: 67.19%] [G loss: 0.554009]\n",
      "epoch:4 step:3999 [D loss: 0.593260, acc.: 67.19%] [G loss: 0.617250]\n",
      "epoch:4 step:4000 [D loss: 0.498967, acc.: 78.12%] [G loss: 0.626182]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.197130\n",
      "FID: 64.151985\n",
      "0 = 13.323276966094976\n",
      "1 = 0.10988823501328947\n",
      "2 = 0.9598000049591064\n",
      "3 = 0.9197999835014343\n",
      "4 = 0.9998000264167786\n",
      "5 = 0.9997826218605042\n",
      "6 = 0.9197999835014343\n",
      "7 = 9.487704921817738\n",
      "8 = 0.1606152168589817\n",
      "9 = 0.8720999956130981\n",
      "10 = 0.8410000205039978\n",
      "11 = 0.9031999707221985\n",
      "12 = 0.8967797160148621\n",
      "13 = 0.8410000205039978\n",
      "14 = 5.197142601013184\n",
      "15 = 7.917108058929443\n",
      "16 = 0.34397920966148376\n",
      "17 = 5.197129726409912\n",
      "18 = 64.15198516845703\n",
      "epoch:4 step:4001 [D loss: 0.531983, acc.: 77.34%] [G loss: 0.538589]\n",
      "epoch:4 step:4002 [D loss: 0.485066, acc.: 78.91%] [G loss: 0.700072]\n",
      "epoch:4 step:4003 [D loss: 0.533697, acc.: 72.66%] [G loss: 0.524623]\n",
      "epoch:4 step:4004 [D loss: 0.471545, acc.: 82.81%] [G loss: 0.579645]\n",
      "epoch:4 step:4005 [D loss: 0.544430, acc.: 71.09%] [G loss: 0.517824]\n",
      "epoch:4 step:4006 [D loss: 0.497374, acc.: 75.78%] [G loss: 0.521824]\n",
      "epoch:4 step:4007 [D loss: 0.454145, acc.: 78.91%] [G loss: 0.686949]\n",
      "epoch:4 step:4008 [D loss: 0.532672, acc.: 75.78%] [G loss: 0.596206]\n",
      "epoch:4 step:4009 [D loss: 0.486948, acc.: 78.12%] [G loss: 0.610590]\n",
      "epoch:4 step:4010 [D loss: 0.516679, acc.: 77.34%] [G loss: 0.445202]\n",
      "epoch:4 step:4011 [D loss: 0.587261, acc.: 66.41%] [G loss: 0.486683]\n",
      "epoch:4 step:4012 [D loss: 0.463576, acc.: 85.94%] [G loss: 0.572858]\n",
      "epoch:4 step:4013 [D loss: 0.539865, acc.: 72.66%] [G loss: 0.570778]\n",
      "epoch:4 step:4014 [D loss: 0.491702, acc.: 76.56%] [G loss: 0.499534]\n",
      "epoch:4 step:4015 [D loss: 0.553345, acc.: 70.31%] [G loss: 0.463979]\n",
      "epoch:4 step:4016 [D loss: 0.513915, acc.: 77.34%] [G loss: 0.515098]\n",
      "epoch:4 step:4017 [D loss: 0.532953, acc.: 73.44%] [G loss: 0.529157]\n",
      "epoch:4 step:4018 [D loss: 0.482183, acc.: 78.91%] [G loss: 0.563583]\n",
      "epoch:4 step:4019 [D loss: 0.462311, acc.: 77.34%] [G loss: 0.654567]\n",
      "epoch:4 step:4020 [D loss: 0.529041, acc.: 75.00%] [G loss: 0.751858]\n",
      "epoch:4 step:4021 [D loss: 0.475867, acc.: 82.03%] [G loss: 0.542063]\n",
      "epoch:4 step:4022 [D loss: 0.625845, acc.: 67.97%] [G loss: 0.533942]\n",
      "epoch:4 step:4023 [D loss: 0.562643, acc.: 69.53%] [G loss: 0.474970]\n",
      "epoch:4 step:4024 [D loss: 0.548412, acc.: 66.41%] [G loss: 0.507210]\n",
      "epoch:4 step:4025 [D loss: 0.624530, acc.: 60.94%] [G loss: 0.477816]\n",
      "epoch:4 step:4026 [D loss: 0.558849, acc.: 71.88%] [G loss: 0.501082]\n",
      "epoch:4 step:4027 [D loss: 0.517808, acc.: 75.78%] [G loss: 0.522581]\n",
      "epoch:4 step:4028 [D loss: 0.538789, acc.: 75.00%] [G loss: 0.480917]\n",
      "epoch:4 step:4029 [D loss: 0.665079, acc.: 59.38%] [G loss: 0.360953]\n",
      "epoch:4 step:4030 [D loss: 0.537145, acc.: 74.22%] [G loss: 0.529779]\n",
      "epoch:4 step:4031 [D loss: 0.461632, acc.: 83.59%] [G loss: 0.617250]\n",
      "epoch:4 step:4032 [D loss: 0.494760, acc.: 79.69%] [G loss: 0.543135]\n",
      "epoch:4 step:4033 [D loss: 0.497239, acc.: 77.34%] [G loss: 0.531247]\n",
      "epoch:4 step:4034 [D loss: 0.460353, acc.: 82.81%] [G loss: 0.600924]\n",
      "epoch:4 step:4035 [D loss: 0.567133, acc.: 66.41%] [G loss: 0.507344]\n",
      "epoch:4 step:4036 [D loss: 0.582180, acc.: 71.09%] [G loss: 0.538926]\n",
      "epoch:4 step:4037 [D loss: 0.514962, acc.: 70.31%] [G loss: 0.504442]\n",
      "epoch:4 step:4038 [D loss: 0.529923, acc.: 71.09%] [G loss: 0.523350]\n",
      "epoch:4 step:4039 [D loss: 0.588493, acc.: 66.41%] [G loss: 0.457315]\n",
      "epoch:4 step:4040 [D loss: 0.572546, acc.: 64.84%] [G loss: 0.488327]\n",
      "epoch:4 step:4041 [D loss: 0.505947, acc.: 75.00%] [G loss: 0.505801]\n",
      "epoch:4 step:4042 [D loss: 0.543088, acc.: 71.09%] [G loss: 0.520518]\n",
      "epoch:4 step:4043 [D loss: 0.511485, acc.: 74.22%] [G loss: 0.518752]\n",
      "epoch:4 step:4044 [D loss: 0.458629, acc.: 82.81%] [G loss: 0.559087]\n",
      "epoch:4 step:4045 [D loss: 0.563363, acc.: 67.97%] [G loss: 0.554194]\n",
      "epoch:4 step:4046 [D loss: 0.507742, acc.: 75.78%] [G loss: 0.575737]\n",
      "epoch:4 step:4047 [D loss: 0.564735, acc.: 70.31%] [G loss: 0.545240]\n",
      "epoch:4 step:4048 [D loss: 0.481011, acc.: 80.47%] [G loss: 0.584087]\n",
      "epoch:4 step:4049 [D loss: 0.546942, acc.: 75.00%] [G loss: 0.481629]\n",
      "epoch:4 step:4050 [D loss: 0.487104, acc.: 76.56%] [G loss: 0.457287]\n",
      "epoch:4 step:4051 [D loss: 0.514409, acc.: 74.22%] [G loss: 0.501464]\n",
      "epoch:4 step:4052 [D loss: 0.462374, acc.: 80.47%] [G loss: 0.648683]\n",
      "epoch:4 step:4053 [D loss: 0.497488, acc.: 77.34%] [G loss: 0.695369]\n",
      "epoch:4 step:4054 [D loss: 0.532547, acc.: 73.44%] [G loss: 0.589932]\n",
      "epoch:4 step:4055 [D loss: 0.491862, acc.: 73.44%] [G loss: 0.473613]\n",
      "epoch:4 step:4056 [D loss: 0.534973, acc.: 66.41%] [G loss: 0.496523]\n",
      "epoch:4 step:4057 [D loss: 0.447470, acc.: 82.03%] [G loss: 0.648517]\n",
      "epoch:4 step:4058 [D loss: 0.469240, acc.: 78.12%] [G loss: 0.673949]\n",
      "epoch:4 step:4059 [D loss: 0.526564, acc.: 75.78%] [G loss: 0.573336]\n",
      "epoch:4 step:4060 [D loss: 0.411121, acc.: 80.47%] [G loss: 0.762171]\n",
      "epoch:4 step:4061 [D loss: 0.444607, acc.: 74.22%] [G loss: 0.918653]\n",
      "epoch:4 step:4062 [D loss: 0.461914, acc.: 76.56%] [G loss: 0.929745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4063 [D loss: 0.460399, acc.: 81.25%] [G loss: 0.902376]\n",
      "epoch:4 step:4064 [D loss: 0.689434, acc.: 62.50%] [G loss: 0.564653]\n",
      "epoch:4 step:4065 [D loss: 0.571641, acc.: 71.09%] [G loss: 0.470804]\n",
      "epoch:4 step:4066 [D loss: 0.499350, acc.: 77.34%] [G loss: 0.534537]\n",
      "epoch:4 step:4067 [D loss: 0.562382, acc.: 69.53%] [G loss: 0.509126]\n",
      "epoch:4 step:4068 [D loss: 0.486282, acc.: 77.34%] [G loss: 0.625343]\n",
      "epoch:4 step:4069 [D loss: 0.500604, acc.: 76.56%] [G loss: 0.662736]\n",
      "epoch:4 step:4070 [D loss: 0.530892, acc.: 75.78%] [G loss: 0.608147]\n",
      "epoch:4 step:4071 [D loss: 0.593613, acc.: 70.31%] [G loss: 0.405471]\n",
      "epoch:4 step:4072 [D loss: 0.543947, acc.: 73.44%] [G loss: 0.473092]\n",
      "epoch:4 step:4073 [D loss: 0.462423, acc.: 82.03%] [G loss: 0.598067]\n",
      "epoch:4 step:4074 [D loss: 0.499881, acc.: 77.34%] [G loss: 0.540281]\n",
      "epoch:4 step:4075 [D loss: 0.488595, acc.: 75.00%] [G loss: 0.637458]\n",
      "epoch:4 step:4076 [D loss: 0.506021, acc.: 75.00%] [G loss: 0.586578]\n",
      "epoch:4 step:4077 [D loss: 0.535667, acc.: 69.53%] [G loss: 0.605844]\n",
      "epoch:4 step:4078 [D loss: 0.499031, acc.: 76.56%] [G loss: 0.647625]\n",
      "epoch:4 step:4079 [D loss: 0.526391, acc.: 73.44%] [G loss: 0.468012]\n",
      "epoch:4 step:4080 [D loss: 0.476070, acc.: 77.34%] [G loss: 0.542372]\n",
      "epoch:4 step:4081 [D loss: 0.515744, acc.: 76.56%] [G loss: 0.528737]\n",
      "epoch:4 step:4082 [D loss: 0.593320, acc.: 67.19%] [G loss: 0.547507]\n",
      "epoch:4 step:4083 [D loss: 0.521554, acc.: 71.88%] [G loss: 0.587966]\n",
      "epoch:4 step:4084 [D loss: 0.478113, acc.: 76.56%] [G loss: 0.585605]\n",
      "epoch:4 step:4085 [D loss: 0.483206, acc.: 77.34%] [G loss: 0.607504]\n",
      "epoch:4 step:4086 [D loss: 0.507926, acc.: 77.34%] [G loss: 0.713964]\n",
      "epoch:4 step:4087 [D loss: 0.527170, acc.: 73.44%] [G loss: 0.497890]\n",
      "epoch:4 step:4088 [D loss: 0.542214, acc.: 75.00%] [G loss: 0.609357]\n",
      "epoch:4 step:4089 [D loss: 0.561935, acc.: 71.88%] [G loss: 0.535138]\n",
      "epoch:4 step:4090 [D loss: 0.608233, acc.: 63.28%] [G loss: 0.541246]\n",
      "epoch:4 step:4091 [D loss: 0.481099, acc.: 78.91%] [G loss: 0.573513]\n",
      "epoch:4 step:4092 [D loss: 0.488954, acc.: 77.34%] [G loss: 0.776622]\n",
      "epoch:4 step:4093 [D loss: 0.505499, acc.: 74.22%] [G loss: 0.641770]\n",
      "epoch:4 step:4094 [D loss: 0.522936, acc.: 72.66%] [G loss: 0.600942]\n",
      "epoch:4 step:4095 [D loss: 0.475744, acc.: 78.91%] [G loss: 0.828197]\n",
      "epoch:4 step:4096 [D loss: 0.677768, acc.: 60.16%] [G loss: 0.608104]\n",
      "epoch:4 step:4097 [D loss: 0.695953, acc.: 55.47%] [G loss: 0.553579]\n",
      "epoch:4 step:4098 [D loss: 0.487839, acc.: 76.56%] [G loss: 0.487894]\n",
      "epoch:4 step:4099 [D loss: 0.536443, acc.: 71.09%] [G loss: 0.680165]\n",
      "epoch:4 step:4100 [D loss: 0.540646, acc.: 72.66%] [G loss: 0.555774]\n",
      "epoch:4 step:4101 [D loss: 0.562182, acc.: 64.84%] [G loss: 0.450172]\n",
      "epoch:4 step:4102 [D loss: 0.487529, acc.: 75.78%] [G loss: 0.611594]\n",
      "epoch:4 step:4103 [D loss: 0.532360, acc.: 71.09%] [G loss: 0.592223]\n",
      "epoch:4 step:4104 [D loss: 0.582682, acc.: 64.84%] [G loss: 0.495706]\n",
      "epoch:4 step:4105 [D loss: 0.451805, acc.: 78.91%] [G loss: 0.508468]\n",
      "epoch:4 step:4106 [D loss: 0.429828, acc.: 84.38%] [G loss: 0.632816]\n",
      "epoch:4 step:4107 [D loss: 0.457643, acc.: 77.34%] [G loss: 0.699083]\n",
      "epoch:4 step:4108 [D loss: 0.483990, acc.: 79.69%] [G loss: 0.601313]\n",
      "epoch:4 step:4109 [D loss: 0.503436, acc.: 77.34%] [G loss: 0.600138]\n",
      "epoch:4 step:4110 [D loss: 0.508333, acc.: 73.44%] [G loss: 0.605830]\n",
      "epoch:4 step:4111 [D loss: 0.563646, acc.: 65.62%] [G loss: 0.569213]\n",
      "epoch:4 step:4112 [D loss: 0.461583, acc.: 78.91%] [G loss: 0.583629]\n",
      "epoch:4 step:4113 [D loss: 0.503359, acc.: 75.00%] [G loss: 0.631624]\n",
      "epoch:4 step:4114 [D loss: 0.469502, acc.: 81.25%] [G loss: 0.713068]\n",
      "epoch:4 step:4115 [D loss: 0.565647, acc.: 71.88%] [G loss: 0.651809]\n",
      "epoch:4 step:4116 [D loss: 0.482546, acc.: 79.69%] [G loss: 0.749582]\n",
      "epoch:4 step:4117 [D loss: 0.567218, acc.: 71.09%] [G loss: 0.606804]\n",
      "epoch:4 step:4118 [D loss: 0.529037, acc.: 76.56%] [G loss: 0.462926]\n",
      "epoch:4 step:4119 [D loss: 0.560402, acc.: 75.00%] [G loss: 0.586151]\n",
      "epoch:4 step:4120 [D loss: 0.491036, acc.: 81.25%] [G loss: 0.603197]\n",
      "epoch:4 step:4121 [D loss: 0.564917, acc.: 72.66%] [G loss: 0.591101]\n",
      "epoch:4 step:4122 [D loss: 0.477540, acc.: 77.34%] [G loss: 0.633009]\n",
      "epoch:4 step:4123 [D loss: 0.542428, acc.: 76.56%] [G loss: 0.667675]\n",
      "epoch:4 step:4124 [D loss: 0.683064, acc.: 57.81%] [G loss: 0.518104]\n",
      "epoch:4 step:4125 [D loss: 0.557038, acc.: 67.97%] [G loss: 0.479914]\n",
      "epoch:4 step:4126 [D loss: 0.502490, acc.: 77.34%] [G loss: 0.650510]\n",
      "epoch:4 step:4127 [D loss: 0.490414, acc.: 82.03%] [G loss: 0.516651]\n",
      "epoch:4 step:4128 [D loss: 0.531938, acc.: 72.66%] [G loss: 0.502143]\n",
      "epoch:4 step:4129 [D loss: 0.466175, acc.: 81.25%] [G loss: 0.517281]\n",
      "epoch:4 step:4130 [D loss: 0.482044, acc.: 78.12%] [G loss: 0.652932]\n",
      "epoch:4 step:4131 [D loss: 0.542730, acc.: 71.09%] [G loss: 0.577811]\n",
      "epoch:4 step:4132 [D loss: 0.485531, acc.: 78.12%] [G loss: 0.623895]\n",
      "epoch:4 step:4133 [D loss: 0.524619, acc.: 72.66%] [G loss: 0.507356]\n",
      "epoch:4 step:4134 [D loss: 0.594893, acc.: 71.09%] [G loss: 0.552864]\n",
      "epoch:4 step:4135 [D loss: 0.527736, acc.: 74.22%] [G loss: 0.515782]\n",
      "epoch:4 step:4136 [D loss: 0.497134, acc.: 75.00%] [G loss: 0.601927]\n",
      "epoch:4 step:4137 [D loss: 0.562462, acc.: 71.88%] [G loss: 0.517244]\n",
      "epoch:4 step:4138 [D loss: 0.560828, acc.: 71.09%] [G loss: 0.490710]\n",
      "epoch:4 step:4139 [D loss: 0.509463, acc.: 79.69%] [G loss: 0.531507]\n",
      "epoch:4 step:4140 [D loss: 0.483953, acc.: 75.00%] [G loss: 0.583609]\n",
      "epoch:4 step:4141 [D loss: 0.528267, acc.: 74.22%] [G loss: 0.628195]\n",
      "epoch:4 step:4142 [D loss: 0.545034, acc.: 75.00%] [G loss: 0.515718]\n",
      "epoch:4 step:4143 [D loss: 0.518730, acc.: 74.22%] [G loss: 0.544676]\n",
      "epoch:4 step:4144 [D loss: 0.571608, acc.: 64.84%] [G loss: 0.616598]\n",
      "epoch:4 step:4145 [D loss: 0.512006, acc.: 75.78%] [G loss: 0.688595]\n",
      "epoch:4 step:4146 [D loss: 0.437093, acc.: 83.59%] [G loss: 0.757800]\n",
      "epoch:4 step:4147 [D loss: 0.472554, acc.: 79.69%] [G loss: 0.905618]\n",
      "epoch:4 step:4148 [D loss: 0.625808, acc.: 60.16%] [G loss: 0.555436]\n",
      "epoch:4 step:4149 [D loss: 0.555016, acc.: 71.09%] [G loss: 0.557627]\n",
      "epoch:4 step:4150 [D loss: 0.475105, acc.: 82.03%] [G loss: 0.533680]\n",
      "epoch:4 step:4151 [D loss: 0.502595, acc.: 75.78%] [G loss: 0.726374]\n",
      "epoch:4 step:4152 [D loss: 0.647921, acc.: 66.41%] [G loss: 0.514545]\n",
      "epoch:4 step:4153 [D loss: 0.518397, acc.: 75.00%] [G loss: 0.646718]\n",
      "epoch:4 step:4154 [D loss: 0.505390, acc.: 71.09%] [G loss: 0.649022]\n",
      "epoch:4 step:4155 [D loss: 0.525602, acc.: 71.88%] [G loss: 0.651454]\n",
      "epoch:4 step:4156 [D loss: 0.558761, acc.: 71.88%] [G loss: 0.485706]\n",
      "epoch:4 step:4157 [D loss: 0.499110, acc.: 75.00%] [G loss: 0.601817]\n",
      "epoch:4 step:4158 [D loss: 0.576942, acc.: 69.53%] [G loss: 0.575733]\n",
      "epoch:4 step:4159 [D loss: 0.488719, acc.: 80.47%] [G loss: 0.488573]\n",
      "epoch:4 step:4160 [D loss: 0.541647, acc.: 74.22%] [G loss: 0.545014]\n",
      "epoch:4 step:4161 [D loss: 0.545471, acc.: 72.66%] [G loss: 0.466860]\n",
      "epoch:4 step:4162 [D loss: 0.575810, acc.: 71.09%] [G loss: 0.559804]\n",
      "epoch:4 step:4163 [D loss: 0.606502, acc.: 71.88%] [G loss: 0.655877]\n",
      "epoch:4 step:4164 [D loss: 0.514857, acc.: 71.09%] [G loss: 0.705362]\n",
      "epoch:4 step:4165 [D loss: 0.618955, acc.: 67.19%] [G loss: 0.435229]\n",
      "epoch:4 step:4166 [D loss: 0.616539, acc.: 62.50%] [G loss: 0.395835]\n",
      "epoch:4 step:4167 [D loss: 0.500622, acc.: 78.12%] [G loss: 0.522083]\n",
      "epoch:4 step:4168 [D loss: 0.591042, acc.: 64.84%] [G loss: 0.608732]\n",
      "epoch:4 step:4169 [D loss: 0.556700, acc.: 71.09%] [G loss: 0.510850]\n",
      "epoch:4 step:4170 [D loss: 0.584315, acc.: 67.97%] [G loss: 0.518988]\n",
      "epoch:4 step:4171 [D loss: 0.507887, acc.: 77.34%] [G loss: 0.477650]\n",
      "epoch:4 step:4172 [D loss: 0.591419, acc.: 68.75%] [G loss: 0.517920]\n",
      "epoch:4 step:4173 [D loss: 0.514535, acc.: 75.00%] [G loss: 0.655905]\n",
      "epoch:4 step:4174 [D loss: 0.459596, acc.: 78.12%] [G loss: 0.670513]\n",
      "epoch:4 step:4175 [D loss: 0.479520, acc.: 81.25%] [G loss: 0.835300]\n",
      "epoch:4 step:4176 [D loss: 0.497227, acc.: 76.56%] [G loss: 0.692563]\n",
      "epoch:4 step:4177 [D loss: 0.459898, acc.: 80.47%] [G loss: 0.667808]\n",
      "epoch:4 step:4178 [D loss: 0.498829, acc.: 75.00%] [G loss: 0.719859]\n",
      "epoch:4 step:4179 [D loss: 0.488707, acc.: 78.91%] [G loss: 0.647257]\n",
      "epoch:4 step:4180 [D loss: 0.562346, acc.: 67.19%] [G loss: 0.434969]\n",
      "epoch:4 step:4181 [D loss: 0.589255, acc.: 68.75%] [G loss: 0.502861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4182 [D loss: 0.476674, acc.: 78.91%] [G loss: 0.623776]\n",
      "epoch:4 step:4183 [D loss: 0.591747, acc.: 70.31%] [G loss: 0.481307]\n",
      "epoch:4 step:4184 [D loss: 0.507544, acc.: 78.12%] [G loss: 0.551535]\n",
      "epoch:4 step:4185 [D loss: 0.646688, acc.: 65.62%] [G loss: 0.422618]\n",
      "epoch:4 step:4186 [D loss: 0.573112, acc.: 69.53%] [G loss: 0.500331]\n",
      "epoch:4 step:4187 [D loss: 0.481048, acc.: 75.78%] [G loss: 0.505314]\n",
      "epoch:4 step:4188 [D loss: 0.498424, acc.: 78.91%] [G loss: 0.532937]\n",
      "epoch:4 step:4189 [D loss: 0.545989, acc.: 73.44%] [G loss: 0.682327]\n",
      "epoch:4 step:4190 [D loss: 0.522825, acc.: 75.78%] [G loss: 0.582297]\n",
      "epoch:4 step:4191 [D loss: 0.505633, acc.: 81.25%] [G loss: 0.576543]\n",
      "epoch:4 step:4192 [D loss: 0.497161, acc.: 76.56%] [G loss: 0.553870]\n",
      "epoch:4 step:4193 [D loss: 0.474344, acc.: 76.56%] [G loss: 0.657120]\n",
      "epoch:4 step:4194 [D loss: 0.487305, acc.: 77.34%] [G loss: 0.698304]\n",
      "epoch:4 step:4195 [D loss: 0.526142, acc.: 72.66%] [G loss: 0.650111]\n",
      "epoch:4 step:4196 [D loss: 0.544028, acc.: 75.00%] [G loss: 0.684614]\n",
      "epoch:4 step:4197 [D loss: 0.485346, acc.: 79.69%] [G loss: 0.755588]\n",
      "epoch:4 step:4198 [D loss: 0.493376, acc.: 75.78%] [G loss: 0.660872]\n",
      "epoch:4 step:4199 [D loss: 0.438245, acc.: 78.91%] [G loss: 0.767369]\n",
      "epoch:4 step:4200 [D loss: 0.517775, acc.: 73.44%] [G loss: 0.761970]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.738070\n",
      "FID: 47.662472\n",
      "0 = 12.978793992137906\n",
      "1 = 0.10678633910687561\n",
      "2 = 0.9555000066757202\n",
      "3 = 0.9115999937057495\n",
      "4 = 0.9994000196456909\n",
      "5 = 0.9993422627449036\n",
      "6 = 0.9115999937057495\n",
      "7 = 8.603125644397744\n",
      "8 = 0.1317667461661046\n",
      "9 = 0.8461999893188477\n",
      "10 = 0.8149999976158142\n",
      "11 = 0.8773999810218811\n",
      "12 = 0.8692406415939331\n",
      "13 = 0.8149999976158142\n",
      "14 = 5.738086223602295\n",
      "15 = 8.36428165435791\n",
      "16 = 0.29249662160873413\n",
      "17 = 5.738069534301758\n",
      "18 = 47.662471771240234\n",
      "epoch:4 step:4201 [D loss: 0.540721, acc.: 71.09%] [G loss: 0.649278]\n",
      "epoch:4 step:4202 [D loss: 0.559920, acc.: 71.88%] [G loss: 0.584167]\n",
      "epoch:4 step:4203 [D loss: 0.593666, acc.: 65.62%] [G loss: 0.492343]\n",
      "epoch:4 step:4204 [D loss: 0.563197, acc.: 73.44%] [G loss: 0.545592]\n",
      "epoch:4 step:4205 [D loss: 0.528935, acc.: 71.09%] [G loss: 0.521764]\n",
      "epoch:4 step:4206 [D loss: 0.500162, acc.: 78.12%] [G loss: 0.535747]\n",
      "epoch:4 step:4207 [D loss: 0.572914, acc.: 64.06%] [G loss: 0.530047]\n",
      "epoch:4 step:4208 [D loss: 0.484705, acc.: 78.12%] [G loss: 0.619585]\n",
      "epoch:4 step:4209 [D loss: 0.511904, acc.: 78.12%] [G loss: 0.514426]\n",
      "epoch:4 step:4210 [D loss: 0.544520, acc.: 64.84%] [G loss: 0.551555]\n",
      "epoch:4 step:4211 [D loss: 0.592567, acc.: 69.53%] [G loss: 0.502722]\n",
      "epoch:4 step:4212 [D loss: 0.489884, acc.: 76.56%] [G loss: 0.538828]\n",
      "epoch:4 step:4213 [D loss: 0.602273, acc.: 65.62%] [G loss: 0.528988]\n",
      "epoch:4 step:4214 [D loss: 0.550510, acc.: 69.53%] [G loss: 0.442176]\n",
      "epoch:4 step:4215 [D loss: 0.490018, acc.: 76.56%] [G loss: 0.557946]\n",
      "epoch:4 step:4216 [D loss: 0.577378, acc.: 69.53%] [G loss: 0.530128]\n",
      "epoch:4 step:4217 [D loss: 0.495022, acc.: 77.34%] [G loss: 0.647571]\n",
      "epoch:4 step:4218 [D loss: 0.604344, acc.: 68.75%] [G loss: 0.559973]\n",
      "epoch:4 step:4219 [D loss: 0.463798, acc.: 82.81%] [G loss: 0.740217]\n",
      "epoch:4 step:4220 [D loss: 0.430833, acc.: 81.25%] [G loss: 0.781017]\n",
      "epoch:4 step:4221 [D loss: 0.549386, acc.: 75.78%] [G loss: 0.653278]\n",
      "epoch:4 step:4222 [D loss: 0.449186, acc.: 82.81%] [G loss: 0.707384]\n",
      "epoch:4 step:4223 [D loss: 0.511081, acc.: 74.22%] [G loss: 0.585742]\n",
      "epoch:4 step:4224 [D loss: 0.516706, acc.: 74.22%] [G loss: 0.609353]\n",
      "epoch:4 step:4225 [D loss: 0.627903, acc.: 64.84%] [G loss: 0.449549]\n",
      "epoch:4 step:4226 [D loss: 0.588005, acc.: 68.75%] [G loss: 0.404102]\n",
      "epoch:4 step:4227 [D loss: 0.490311, acc.: 81.25%] [G loss: 0.426358]\n",
      "epoch:4 step:4228 [D loss: 0.610368, acc.: 66.41%] [G loss: 0.526945]\n",
      "epoch:4 step:4229 [D loss: 0.538377, acc.: 73.44%] [G loss: 0.544515]\n",
      "epoch:4 step:4230 [D loss: 0.612626, acc.: 66.41%] [G loss: 0.447946]\n",
      "epoch:4 step:4231 [D loss: 0.605843, acc.: 63.28%] [G loss: 0.454912]\n",
      "epoch:4 step:4232 [D loss: 0.534808, acc.: 73.44%] [G loss: 0.530923]\n",
      "epoch:4 step:4233 [D loss: 0.501907, acc.: 81.25%] [G loss: 0.642478]\n",
      "epoch:4 step:4234 [D loss: 0.606771, acc.: 65.62%] [G loss: 0.487119]\n",
      "epoch:4 step:4235 [D loss: 0.503484, acc.: 75.00%] [G loss: 0.578720]\n",
      "epoch:4 step:4236 [D loss: 0.468297, acc.: 78.12%] [G loss: 0.545195]\n",
      "epoch:4 step:4237 [D loss: 0.551134, acc.: 70.31%] [G loss: 0.506590]\n",
      "epoch:4 step:4238 [D loss: 0.538554, acc.: 71.88%] [G loss: 0.468393]\n",
      "epoch:4 step:4239 [D loss: 0.497152, acc.: 78.91%] [G loss: 0.521413]\n",
      "epoch:4 step:4240 [D loss: 0.565359, acc.: 72.66%] [G loss: 0.500025]\n",
      "epoch:4 step:4241 [D loss: 0.583700, acc.: 64.84%] [G loss: 0.471890]\n",
      "epoch:4 step:4242 [D loss: 0.581448, acc.: 68.75%] [G loss: 0.501734]\n",
      "epoch:4 step:4243 [D loss: 0.508339, acc.: 78.12%] [G loss: 0.694048]\n",
      "epoch:4 step:4244 [D loss: 0.563881, acc.: 73.44%] [G loss: 0.561198]\n",
      "epoch:4 step:4245 [D loss: 0.548006, acc.: 71.88%] [G loss: 0.629886]\n",
      "epoch:4 step:4246 [D loss: 0.421933, acc.: 80.47%] [G loss: 0.756994]\n",
      "epoch:4 step:4247 [D loss: 0.446391, acc.: 84.38%] [G loss: 0.558259]\n",
      "epoch:4 step:4248 [D loss: 0.651673, acc.: 63.28%] [G loss: 0.420592]\n",
      "epoch:4 step:4249 [D loss: 0.659526, acc.: 57.81%] [G loss: 0.385498]\n",
      "epoch:4 step:4250 [D loss: 0.598475, acc.: 61.72%] [G loss: 0.388789]\n",
      "epoch:4 step:4251 [D loss: 0.483245, acc.: 78.91%] [G loss: 0.566941]\n",
      "epoch:4 step:4252 [D loss: 0.518171, acc.: 74.22%] [G loss: 0.626426]\n",
      "epoch:4 step:4253 [D loss: 0.584337, acc.: 72.66%] [G loss: 0.653680]\n",
      "epoch:4 step:4254 [D loss: 0.526336, acc.: 75.00%] [G loss: 0.543391]\n",
      "epoch:4 step:4255 [D loss: 0.570639, acc.: 70.31%] [G loss: 0.562310]\n",
      "epoch:4 step:4256 [D loss: 0.443791, acc.: 82.81%] [G loss: 0.675148]\n",
      "epoch:4 step:4257 [D loss: 0.542459, acc.: 72.66%] [G loss: 0.572043]\n",
      "epoch:4 step:4258 [D loss: 0.536486, acc.: 72.66%] [G loss: 0.615207]\n",
      "epoch:4 step:4259 [D loss: 0.636977, acc.: 63.28%] [G loss: 0.408006]\n",
      "epoch:4 step:4260 [D loss: 0.564867, acc.: 66.41%] [G loss: 0.478981]\n",
      "epoch:4 step:4261 [D loss: 0.529855, acc.: 73.44%] [G loss: 0.397967]\n",
      "epoch:4 step:4262 [D loss: 0.469040, acc.: 79.69%] [G loss: 0.565752]\n",
      "epoch:4 step:4263 [D loss: 0.460123, acc.: 78.12%] [G loss: 0.667453]\n",
      "epoch:4 step:4264 [D loss: 0.521837, acc.: 74.22%] [G loss: 0.639522]\n",
      "epoch:4 step:4265 [D loss: 0.547336, acc.: 75.78%] [G loss: 0.577040]\n",
      "epoch:4 step:4266 [D loss: 0.480268, acc.: 77.34%] [G loss: 0.616831]\n",
      "epoch:4 step:4267 [D loss: 0.511853, acc.: 76.56%] [G loss: 0.604379]\n",
      "epoch:4 step:4268 [D loss: 0.478832, acc.: 78.91%] [G loss: 0.676461]\n",
      "epoch:4 step:4269 [D loss: 0.487550, acc.: 77.34%] [G loss: 0.670118]\n",
      "epoch:4 step:4270 [D loss: 0.490521, acc.: 75.78%] [G loss: 0.588502]\n",
      "epoch:4 step:4271 [D loss: 0.535187, acc.: 73.44%] [G loss: 0.586478]\n",
      "epoch:4 step:4272 [D loss: 0.543674, acc.: 69.53%] [G loss: 0.469004]\n",
      "epoch:4 step:4273 [D loss: 0.539724, acc.: 75.00%] [G loss: 0.586166]\n",
      "epoch:4 step:4274 [D loss: 0.535492, acc.: 71.88%] [G loss: 0.488769]\n",
      "epoch:4 step:4275 [D loss: 0.552947, acc.: 69.53%] [G loss: 0.496671]\n",
      "epoch:4 step:4276 [D loss: 0.612799, acc.: 64.84%] [G loss: 0.497178]\n",
      "epoch:4 step:4277 [D loss: 0.616402, acc.: 64.06%] [G loss: 0.458047]\n",
      "epoch:4 step:4278 [D loss: 0.504505, acc.: 76.56%] [G loss: 0.497389]\n",
      "epoch:4 step:4279 [D loss: 0.591340, acc.: 67.19%] [G loss: 0.460051]\n",
      "epoch:4 step:4280 [D loss: 0.568133, acc.: 68.75%] [G loss: 0.476749]\n",
      "epoch:4 step:4281 [D loss: 0.543540, acc.: 73.44%] [G loss: 0.436929]\n",
      "epoch:4 step:4282 [D loss: 0.548063, acc.: 74.22%] [G loss: 0.530621]\n",
      "epoch:4 step:4283 [D loss: 0.587770, acc.: 69.53%] [G loss: 0.444423]\n",
      "epoch:4 step:4284 [D loss: 0.470571, acc.: 83.59%] [G loss: 0.519205]\n",
      "epoch:4 step:4285 [D loss: 0.534343, acc.: 71.09%] [G loss: 0.563248]\n",
      "epoch:4 step:4286 [D loss: 0.605334, acc.: 68.75%] [G loss: 0.521741]\n",
      "epoch:4 step:4287 [D loss: 0.575326, acc.: 75.78%] [G loss: 0.461559]\n",
      "epoch:4 step:4288 [D loss: 0.611465, acc.: 67.97%] [G loss: 0.454635]\n",
      "epoch:4 step:4289 [D loss: 0.548726, acc.: 71.88%] [G loss: 0.427099]\n",
      "epoch:4 step:4290 [D loss: 0.559666, acc.: 72.66%] [G loss: 0.525770]\n",
      "epoch:4 step:4291 [D loss: 0.623768, acc.: 65.62%] [G loss: 0.413350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4292 [D loss: 0.520980, acc.: 75.78%] [G loss: 0.484432]\n",
      "epoch:4 step:4293 [D loss: 0.577109, acc.: 67.19%] [G loss: 0.542188]\n",
      "epoch:4 step:4294 [D loss: 0.496338, acc.: 81.25%] [G loss: 0.564969]\n",
      "epoch:4 step:4295 [D loss: 0.485104, acc.: 80.47%] [G loss: 0.490887]\n",
      "epoch:4 step:4296 [D loss: 0.483294, acc.: 75.00%] [G loss: 0.548973]\n",
      "epoch:4 step:4297 [D loss: 0.542737, acc.: 71.88%] [G loss: 0.618515]\n",
      "epoch:4 step:4298 [D loss: 0.508695, acc.: 75.78%] [G loss: 0.519421]\n",
      "epoch:4 step:4299 [D loss: 0.575760, acc.: 67.97%] [G loss: 0.485004]\n",
      "epoch:4 step:4300 [D loss: 0.468815, acc.: 82.03%] [G loss: 0.604780]\n",
      "epoch:4 step:4301 [D loss: 0.548342, acc.: 70.31%] [G loss: 0.528351]\n",
      "epoch:4 step:4302 [D loss: 0.414018, acc.: 84.38%] [G loss: 0.625072]\n",
      "epoch:4 step:4303 [D loss: 0.524317, acc.: 73.44%] [G loss: 0.628448]\n",
      "epoch:4 step:4304 [D loss: 0.529292, acc.: 75.78%] [G loss: 0.484542]\n",
      "epoch:4 step:4305 [D loss: 0.492390, acc.: 78.91%] [G loss: 0.521396]\n",
      "epoch:4 step:4306 [D loss: 0.496797, acc.: 75.78%] [G loss: 0.536199]\n",
      "epoch:4 step:4307 [D loss: 0.554709, acc.: 69.53%] [G loss: 0.543229]\n",
      "epoch:4 step:4308 [D loss: 0.541689, acc.: 70.31%] [G loss: 0.456663]\n",
      "epoch:4 step:4309 [D loss: 0.502136, acc.: 78.91%] [G loss: 0.655787]\n",
      "epoch:4 step:4310 [D loss: 0.590611, acc.: 68.75%] [G loss: 0.411243]\n",
      "epoch:4 step:4311 [D loss: 0.572102, acc.: 71.09%] [G loss: 0.331097]\n",
      "epoch:4 step:4312 [D loss: 0.505927, acc.: 71.09%] [G loss: 0.553171]\n",
      "epoch:4 step:4313 [D loss: 0.565268, acc.: 71.88%] [G loss: 0.387508]\n",
      "epoch:4 step:4314 [D loss: 0.611498, acc.: 70.31%] [G loss: 0.596242]\n",
      "epoch:4 step:4315 [D loss: 0.490083, acc.: 76.56%] [G loss: 0.619050]\n",
      "epoch:4 step:4316 [D loss: 0.528437, acc.: 77.34%] [G loss: 0.605462]\n",
      "epoch:4 step:4317 [D loss: 0.543907, acc.: 71.09%] [G loss: 0.545650]\n",
      "epoch:4 step:4318 [D loss: 0.487965, acc.: 77.34%] [G loss: 0.633602]\n",
      "epoch:4 step:4319 [D loss: 0.543949, acc.: 75.00%] [G loss: 0.552987]\n",
      "epoch:4 step:4320 [D loss: 0.558536, acc.: 71.09%] [G loss: 0.548121]\n",
      "epoch:4 step:4321 [D loss: 0.530257, acc.: 72.66%] [G loss: 0.431368]\n",
      "epoch:4 step:4322 [D loss: 0.491947, acc.: 74.22%] [G loss: 0.582439]\n",
      "epoch:4 step:4323 [D loss: 0.498397, acc.: 75.78%] [G loss: 0.584604]\n",
      "epoch:4 step:4324 [D loss: 0.553338, acc.: 69.53%] [G loss: 0.562597]\n",
      "epoch:4 step:4325 [D loss: 0.583363, acc.: 67.97%] [G loss: 0.580741]\n",
      "epoch:4 step:4326 [D loss: 0.508014, acc.: 75.00%] [G loss: 0.570193]\n",
      "epoch:4 step:4327 [D loss: 0.527229, acc.: 75.78%] [G loss: 0.522799]\n",
      "epoch:4 step:4328 [D loss: 0.488474, acc.: 77.34%] [G loss: 0.595566]\n",
      "epoch:4 step:4329 [D loss: 0.482219, acc.: 80.47%] [G loss: 0.577481]\n",
      "epoch:4 step:4330 [D loss: 0.457759, acc.: 78.12%] [G loss: 0.589910]\n",
      "epoch:4 step:4331 [D loss: 0.540239, acc.: 71.88%] [G loss: 0.642404]\n",
      "epoch:4 step:4332 [D loss: 0.603556, acc.: 65.62%] [G loss: 0.504878]\n",
      "epoch:4 step:4333 [D loss: 0.553308, acc.: 71.88%] [G loss: 0.338502]\n",
      "epoch:4 step:4334 [D loss: 0.544782, acc.: 71.09%] [G loss: 0.511263]\n",
      "epoch:4 step:4335 [D loss: 0.573538, acc.: 67.97%] [G loss: 0.547034]\n",
      "epoch:4 step:4336 [D loss: 0.591271, acc.: 67.97%] [G loss: 0.593228]\n",
      "epoch:4 step:4337 [D loss: 0.485468, acc.: 77.34%] [G loss: 0.673979]\n",
      "epoch:4 step:4338 [D loss: 0.554811, acc.: 73.44%] [G loss: 0.527785]\n",
      "epoch:4 step:4339 [D loss: 0.588948, acc.: 71.88%] [G loss: 0.481012]\n",
      "epoch:4 step:4340 [D loss: 0.493446, acc.: 74.22%] [G loss: 0.510195]\n",
      "epoch:4 step:4341 [D loss: 0.524565, acc.: 75.00%] [G loss: 0.600719]\n",
      "epoch:4 step:4342 [D loss: 0.561608, acc.: 70.31%] [G loss: 0.479432]\n",
      "epoch:4 step:4343 [D loss: 0.550683, acc.: 71.88%] [G loss: 0.418154]\n",
      "epoch:4 step:4344 [D loss: 0.568213, acc.: 71.09%] [G loss: 0.499695]\n",
      "epoch:4 step:4345 [D loss: 0.489904, acc.: 78.91%] [G loss: 0.536379]\n",
      "epoch:4 step:4346 [D loss: 0.515509, acc.: 76.56%] [G loss: 0.548672]\n",
      "epoch:4 step:4347 [D loss: 0.584526, acc.: 68.75%] [G loss: 0.542767]\n",
      "epoch:4 step:4348 [D loss: 0.579719, acc.: 66.41%] [G loss: 0.446865]\n",
      "epoch:4 step:4349 [D loss: 0.522538, acc.: 69.53%] [G loss: 0.472783]\n",
      "epoch:4 step:4350 [D loss: 0.547021, acc.: 67.97%] [G loss: 0.614322]\n",
      "epoch:4 step:4351 [D loss: 0.546542, acc.: 71.88%] [G loss: 0.527862]\n",
      "epoch:4 step:4352 [D loss: 0.598938, acc.: 67.19%] [G loss: 0.546724]\n",
      "epoch:4 step:4353 [D loss: 0.542162, acc.: 74.22%] [G loss: 0.531769]\n",
      "epoch:4 step:4354 [D loss: 0.511916, acc.: 71.88%] [G loss: 0.540054]\n",
      "epoch:4 step:4355 [D loss: 0.534008, acc.: 70.31%] [G loss: 0.479245]\n",
      "epoch:4 step:4356 [D loss: 0.533341, acc.: 73.44%] [G loss: 0.570870]\n",
      "epoch:4 step:4357 [D loss: 0.523380, acc.: 70.31%] [G loss: 0.582859]\n",
      "epoch:4 step:4358 [D loss: 0.574490, acc.: 71.09%] [G loss: 0.467583]\n",
      "epoch:4 step:4359 [D loss: 0.504658, acc.: 75.00%] [G loss: 0.533463]\n",
      "epoch:4 step:4360 [D loss: 0.549794, acc.: 69.53%] [G loss: 0.453256]\n",
      "epoch:4 step:4361 [D loss: 0.471484, acc.: 78.91%] [G loss: 0.521428]\n",
      "epoch:4 step:4362 [D loss: 0.541554, acc.: 72.66%] [G loss: 0.554888]\n",
      "epoch:4 step:4363 [D loss: 0.559189, acc.: 68.75%] [G loss: 0.542390]\n",
      "epoch:4 step:4364 [D loss: 0.541143, acc.: 75.00%] [G loss: 0.526651]\n",
      "epoch:4 step:4365 [D loss: 0.560490, acc.: 69.53%] [G loss: 0.443270]\n",
      "epoch:4 step:4366 [D loss: 0.492769, acc.: 79.69%] [G loss: 0.565653]\n",
      "epoch:4 step:4367 [D loss: 0.537089, acc.: 75.00%] [G loss: 0.580689]\n",
      "epoch:4 step:4368 [D loss: 0.517823, acc.: 76.56%] [G loss: 0.638898]\n",
      "epoch:4 step:4369 [D loss: 0.562163, acc.: 67.97%] [G loss: 0.535953]\n",
      "epoch:4 step:4370 [D loss: 0.652234, acc.: 62.50%] [G loss: 0.426057]\n",
      "epoch:4 step:4371 [D loss: 0.527872, acc.: 74.22%] [G loss: 0.478483]\n",
      "epoch:4 step:4372 [D loss: 0.497918, acc.: 78.12%] [G loss: 0.571494]\n",
      "epoch:4 step:4373 [D loss: 0.585908, acc.: 63.28%] [G loss: 0.433279]\n",
      "epoch:4 step:4374 [D loss: 0.496134, acc.: 77.34%] [G loss: 0.552514]\n",
      "epoch:4 step:4375 [D loss: 0.534827, acc.: 75.78%] [G loss: 0.636054]\n",
      "epoch:4 step:4376 [D loss: 0.567171, acc.: 69.53%] [G loss: 0.411060]\n",
      "epoch:4 step:4377 [D loss: 0.513553, acc.: 75.00%] [G loss: 0.535152]\n",
      "epoch:4 step:4378 [D loss: 0.484374, acc.: 81.25%] [G loss: 0.606108]\n",
      "epoch:4 step:4379 [D loss: 0.492041, acc.: 76.56%] [G loss: 0.459513]\n",
      "epoch:4 step:4380 [D loss: 0.498337, acc.: 75.00%] [G loss: 0.536868]\n",
      "epoch:4 step:4381 [D loss: 0.513417, acc.: 73.44%] [G loss: 0.619325]\n",
      "epoch:4 step:4382 [D loss: 0.500930, acc.: 75.00%] [G loss: 0.615763]\n",
      "epoch:4 step:4383 [D loss: 0.513866, acc.: 77.34%] [G loss: 0.503784]\n",
      "epoch:4 step:4384 [D loss: 0.598435, acc.: 67.19%] [G loss: 0.471982]\n",
      "epoch:4 step:4385 [D loss: 0.507626, acc.: 73.44%] [G loss: 0.561578]\n",
      "epoch:4 step:4386 [D loss: 0.530598, acc.: 71.09%] [G loss: 0.595708]\n",
      "epoch:4 step:4387 [D loss: 0.445601, acc.: 82.03%] [G loss: 0.651168]\n",
      "epoch:4 step:4388 [D loss: 0.471829, acc.: 76.56%] [G loss: 0.664008]\n",
      "epoch:4 step:4389 [D loss: 0.461571, acc.: 78.91%] [G loss: 0.625052]\n",
      "epoch:4 step:4390 [D loss: 0.480659, acc.: 78.91%] [G loss: 0.688666]\n",
      "epoch:4 step:4391 [D loss: 0.544270, acc.: 73.44%] [G loss: 0.606708]\n",
      "epoch:4 step:4392 [D loss: 0.561598, acc.: 70.31%] [G loss: 0.534938]\n",
      "epoch:4 step:4393 [D loss: 0.530960, acc.: 74.22%] [G loss: 0.527846]\n",
      "epoch:4 step:4394 [D loss: 0.523243, acc.: 72.66%] [G loss: 0.569874]\n",
      "epoch:4 step:4395 [D loss: 0.464531, acc.: 79.69%] [G loss: 0.557272]\n",
      "epoch:4 step:4396 [D loss: 0.482669, acc.: 77.34%] [G loss: 0.801989]\n",
      "epoch:4 step:4397 [D loss: 0.503217, acc.: 75.78%] [G loss: 0.671831]\n",
      "epoch:4 step:4398 [D loss: 0.503126, acc.: 78.12%] [G loss: 0.679554]\n",
      "epoch:4 step:4399 [D loss: 0.511036, acc.: 71.09%] [G loss: 0.607226]\n",
      "epoch:4 step:4400 [D loss: 0.647517, acc.: 63.28%] [G loss: 0.378187]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.677644\n",
      "FID: 48.943703\n",
      "0 = 12.999243165969842\n",
      "1 = 0.10517389974088887\n",
      "2 = 0.9477999806404114\n",
      "3 = 0.8971999883651733\n",
      "4 = 0.9983999729156494\n",
      "5 = 0.9982198476791382\n",
      "6 = 0.8971999883651733\n",
      "7 = 8.587562637114523\n",
      "8 = 0.13681593574263315\n",
      "9 = 0.8341000080108643\n",
      "10 = 0.7996000051498413\n",
      "11 = 0.8686000108718872\n",
      "12 = 0.8588614463806152\n",
      "13 = 0.7996000051498413\n",
      "14 = 5.677665710449219\n",
      "15 = 8.04576587677002\n",
      "16 = 0.31044936180114746\n",
      "17 = 5.6776442527771\n",
      "18 = 48.943702697753906\n",
      "epoch:4 step:4401 [D loss: 0.601834, acc.: 63.28%] [G loss: 0.399360]\n",
      "epoch:4 step:4402 [D loss: 0.514955, acc.: 76.56%] [G loss: 0.498656]\n",
      "epoch:4 step:4403 [D loss: 0.536793, acc.: 73.44%] [G loss: 0.466939]\n",
      "epoch:4 step:4404 [D loss: 0.507441, acc.: 75.00%] [G loss: 0.507830]\n",
      "epoch:4 step:4405 [D loss: 0.531035, acc.: 75.78%] [G loss: 0.470093]\n",
      "epoch:4 step:4406 [D loss: 0.536110, acc.: 71.09%] [G loss: 0.557706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4407 [D loss: 0.547037, acc.: 70.31%] [G loss: 0.530538]\n",
      "epoch:4 step:4408 [D loss: 0.484668, acc.: 79.69%] [G loss: 0.624999]\n",
      "epoch:4 step:4409 [D loss: 0.494668, acc.: 75.00%] [G loss: 0.594916]\n",
      "epoch:4 step:4410 [D loss: 0.628959, acc.: 64.06%] [G loss: 0.553358]\n",
      "epoch:4 step:4411 [D loss: 0.523957, acc.: 73.44%] [G loss: 0.510253]\n",
      "epoch:4 step:4412 [D loss: 0.521915, acc.: 73.44%] [G loss: 0.681740]\n",
      "epoch:4 step:4413 [D loss: 0.539518, acc.: 71.88%] [G loss: 0.720703]\n",
      "epoch:4 step:4414 [D loss: 0.550074, acc.: 67.19%] [G loss: 0.637624]\n",
      "epoch:4 step:4415 [D loss: 0.589570, acc.: 68.75%] [G loss: 0.464557]\n",
      "epoch:4 step:4416 [D loss: 0.575100, acc.: 71.88%] [G loss: 0.464179]\n",
      "epoch:4 step:4417 [D loss: 0.550105, acc.: 74.22%] [G loss: 0.406184]\n",
      "epoch:4 step:4418 [D loss: 0.514490, acc.: 71.09%] [G loss: 0.576563]\n",
      "epoch:4 step:4419 [D loss: 0.543419, acc.: 74.22%] [G loss: 0.610113]\n",
      "epoch:4 step:4420 [D loss: 0.561379, acc.: 70.31%] [G loss: 0.603871]\n",
      "epoch:4 step:4421 [D loss: 0.563231, acc.: 69.53%] [G loss: 0.535707]\n",
      "epoch:4 step:4422 [D loss: 0.529697, acc.: 75.78%] [G loss: 0.498879]\n",
      "epoch:4 step:4423 [D loss: 0.594265, acc.: 67.19%] [G loss: 0.513271]\n",
      "epoch:4 step:4424 [D loss: 0.545430, acc.: 72.66%] [G loss: 0.485064]\n",
      "epoch:4 step:4425 [D loss: 0.520389, acc.: 76.56%] [G loss: 0.468206]\n",
      "epoch:4 step:4426 [D loss: 0.537582, acc.: 75.78%] [G loss: 0.422634]\n",
      "epoch:4 step:4427 [D loss: 0.516931, acc.: 77.34%] [G loss: 0.563903]\n",
      "epoch:4 step:4428 [D loss: 0.528482, acc.: 71.09%] [G loss: 0.409611]\n",
      "epoch:4 step:4429 [D loss: 0.476996, acc.: 76.56%] [G loss: 0.668743]\n",
      "epoch:4 step:4430 [D loss: 0.503422, acc.: 75.78%] [G loss: 0.574714]\n",
      "epoch:4 step:4431 [D loss: 0.605164, acc.: 64.84%] [G loss: 0.451976]\n",
      "epoch:4 step:4432 [D loss: 0.557575, acc.: 70.31%] [G loss: 0.586864]\n",
      "epoch:4 step:4433 [D loss: 0.495150, acc.: 78.91%] [G loss: 0.592243]\n",
      "epoch:4 step:4434 [D loss: 0.579041, acc.: 69.53%] [G loss: 0.529317]\n",
      "epoch:4 step:4435 [D loss: 0.554620, acc.: 74.22%] [G loss: 0.451162]\n",
      "epoch:4 step:4436 [D loss: 0.566786, acc.: 68.75%] [G loss: 0.507556]\n",
      "epoch:4 step:4437 [D loss: 0.543040, acc.: 73.44%] [G loss: 0.468753]\n",
      "epoch:4 step:4438 [D loss: 0.527635, acc.: 67.19%] [G loss: 0.558175]\n",
      "epoch:4 step:4439 [D loss: 0.541331, acc.: 75.78%] [G loss: 0.664575]\n",
      "epoch:4 step:4440 [D loss: 0.533237, acc.: 77.34%] [G loss: 0.660824]\n",
      "epoch:4 step:4441 [D loss: 0.511534, acc.: 75.00%] [G loss: 0.591414]\n",
      "epoch:4 step:4442 [D loss: 0.536243, acc.: 71.88%] [G loss: 0.505705]\n",
      "epoch:4 step:4443 [D loss: 0.530445, acc.: 71.09%] [G loss: 0.486950]\n",
      "epoch:4 step:4444 [D loss: 0.560371, acc.: 71.88%] [G loss: 0.441957]\n",
      "epoch:4 step:4445 [D loss: 0.574374, acc.: 71.09%] [G loss: 0.628768]\n",
      "epoch:4 step:4446 [D loss: 0.552130, acc.: 71.88%] [G loss: 0.470342]\n",
      "epoch:4 step:4447 [D loss: 0.478513, acc.: 78.91%] [G loss: 0.692391]\n",
      "epoch:4 step:4448 [D loss: 0.587137, acc.: 65.62%] [G loss: 0.486112]\n",
      "epoch:4 step:4449 [D loss: 0.542629, acc.: 71.09%] [G loss: 0.638712]\n",
      "epoch:4 step:4450 [D loss: 0.609328, acc.: 66.41%] [G loss: 0.596923]\n",
      "epoch:4 step:4451 [D loss: 0.574700, acc.: 71.88%] [G loss: 0.580105]\n",
      "epoch:4 step:4452 [D loss: 0.608226, acc.: 66.41%] [G loss: 0.639453]\n",
      "epoch:4 step:4453 [D loss: 0.511215, acc.: 73.44%] [G loss: 0.562635]\n",
      "epoch:4 step:4454 [D loss: 0.508852, acc.: 74.22%] [G loss: 0.559889]\n",
      "epoch:4 step:4455 [D loss: 0.449227, acc.: 80.47%] [G loss: 0.776615]\n",
      "epoch:4 step:4456 [D loss: 0.499432, acc.: 78.12%] [G loss: 0.719224]\n",
      "epoch:4 step:4457 [D loss: 0.494083, acc.: 76.56%] [G loss: 0.683445]\n",
      "epoch:4 step:4458 [D loss: 0.638158, acc.: 64.06%] [G loss: 0.486385]\n",
      "epoch:4 step:4459 [D loss: 0.646494, acc.: 64.06%] [G loss: 0.388975]\n",
      "epoch:4 step:4460 [D loss: 0.511504, acc.: 76.56%] [G loss: 0.582261]\n",
      "epoch:4 step:4461 [D loss: 0.594387, acc.: 66.41%] [G loss: 0.410463]\n",
      "epoch:4 step:4462 [D loss: 0.561044, acc.: 72.66%] [G loss: 0.503104]\n",
      "epoch:4 step:4463 [D loss: 0.533767, acc.: 75.00%] [G loss: 0.421592]\n",
      "epoch:4 step:4464 [D loss: 0.617275, acc.: 61.72%] [G loss: 0.359845]\n",
      "epoch:4 step:4465 [D loss: 0.625429, acc.: 63.28%] [G loss: 0.494520]\n",
      "epoch:4 step:4466 [D loss: 0.569993, acc.: 67.97%] [G loss: 0.508205]\n",
      "epoch:4 step:4467 [D loss: 0.545171, acc.: 71.09%] [G loss: 0.493020]\n",
      "epoch:4 step:4468 [D loss: 0.575682, acc.: 71.88%] [G loss: 0.512090]\n",
      "epoch:4 step:4469 [D loss: 0.540156, acc.: 72.66%] [G loss: 0.467954]\n",
      "epoch:4 step:4470 [D loss: 0.515248, acc.: 68.75%] [G loss: 0.574867]\n",
      "epoch:4 step:4471 [D loss: 0.549388, acc.: 66.41%] [G loss: 0.399258]\n",
      "epoch:4 step:4472 [D loss: 0.489834, acc.: 78.91%] [G loss: 0.449936]\n",
      "epoch:4 step:4473 [D loss: 0.482770, acc.: 78.12%] [G loss: 0.524932]\n",
      "epoch:4 step:4474 [D loss: 0.602462, acc.: 67.97%] [G loss: 0.542725]\n",
      "epoch:4 step:4475 [D loss: 0.625991, acc.: 62.50%] [G loss: 0.492910]\n",
      "epoch:4 step:4476 [D loss: 0.536572, acc.: 71.88%] [G loss: 0.542215]\n",
      "epoch:4 step:4477 [D loss: 0.617302, acc.: 65.62%] [G loss: 0.410686]\n",
      "epoch:4 step:4478 [D loss: 0.534811, acc.: 75.78%] [G loss: 0.503837]\n",
      "epoch:4 step:4479 [D loss: 0.556965, acc.: 69.53%] [G loss: 0.485699]\n",
      "epoch:4 step:4480 [D loss: 0.537082, acc.: 70.31%] [G loss: 0.431528]\n",
      "epoch:4 step:4481 [D loss: 0.498969, acc.: 75.00%] [G loss: 0.532793]\n",
      "epoch:4 step:4482 [D loss: 0.514861, acc.: 76.56%] [G loss: 0.515914]\n",
      "epoch:4 step:4483 [D loss: 0.656148, acc.: 59.38%] [G loss: 0.464797]\n",
      "epoch:4 step:4484 [D loss: 0.457597, acc.: 74.22%] [G loss: 0.537198]\n",
      "epoch:4 step:4485 [D loss: 0.481490, acc.: 78.91%] [G loss: 0.553521]\n",
      "epoch:4 step:4486 [D loss: 0.550039, acc.: 67.19%] [G loss: 0.379826]\n",
      "epoch:4 step:4487 [D loss: 0.580306, acc.: 69.53%] [G loss: 0.418493]\n",
      "epoch:4 step:4488 [D loss: 0.587428, acc.: 67.97%] [G loss: 0.420073]\n",
      "epoch:4 step:4489 [D loss: 0.505904, acc.: 74.22%] [G loss: 0.508456]\n",
      "epoch:4 step:4490 [D loss: 0.548602, acc.: 75.00%] [G loss: 0.393593]\n",
      "epoch:4 step:4491 [D loss: 0.485891, acc.: 76.56%] [G loss: 0.681861]\n",
      "epoch:4 step:4492 [D loss: 0.517588, acc.: 71.88%] [G loss: 0.540838]\n",
      "epoch:4 step:4493 [D loss: 0.543407, acc.: 74.22%] [G loss: 0.609254]\n",
      "epoch:4 step:4494 [D loss: 0.472035, acc.: 82.03%] [G loss: 0.621892]\n",
      "epoch:4 step:4495 [D loss: 0.465991, acc.: 79.69%] [G loss: 0.601467]\n",
      "epoch:4 step:4496 [D loss: 0.478542, acc.: 78.12%] [G loss: 0.646781]\n",
      "epoch:4 step:4497 [D loss: 0.499681, acc.: 75.00%] [G loss: 0.563588]\n",
      "epoch:4 step:4498 [D loss: 0.482589, acc.: 80.47%] [G loss: 0.608190]\n",
      "epoch:4 step:4499 [D loss: 0.557066, acc.: 73.44%] [G loss: 0.442423]\n",
      "epoch:4 step:4500 [D loss: 0.505964, acc.: 75.78%] [G loss: 0.546081]\n",
      "epoch:4 step:4501 [D loss: 0.516509, acc.: 72.66%] [G loss: 0.682668]\n",
      "epoch:4 step:4502 [D loss: 0.476229, acc.: 75.00%] [G loss: 0.684586]\n",
      "epoch:4 step:4503 [D loss: 0.505684, acc.: 73.44%] [G loss: 0.811356]\n",
      "epoch:4 step:4504 [D loss: 0.577760, acc.: 70.31%] [G loss: 0.546870]\n",
      "epoch:4 step:4505 [D loss: 0.516312, acc.: 71.88%] [G loss: 0.521772]\n",
      "epoch:4 step:4506 [D loss: 0.542866, acc.: 71.09%] [G loss: 0.555910]\n",
      "epoch:4 step:4507 [D loss: 0.620280, acc.: 65.62%] [G loss: 0.513836]\n",
      "epoch:4 step:4508 [D loss: 0.526592, acc.: 74.22%] [G loss: 0.564600]\n",
      "epoch:4 step:4509 [D loss: 0.549859, acc.: 65.62%] [G loss: 0.513987]\n",
      "epoch:4 step:4510 [D loss: 0.571351, acc.: 69.53%] [G loss: 0.412991]\n",
      "epoch:4 step:4511 [D loss: 0.548030, acc.: 74.22%] [G loss: 0.513016]\n",
      "epoch:4 step:4512 [D loss: 0.527949, acc.: 74.22%] [G loss: 0.639356]\n",
      "epoch:4 step:4513 [D loss: 0.667520, acc.: 64.84%] [G loss: 0.490544]\n",
      "epoch:4 step:4514 [D loss: 0.628685, acc.: 64.06%] [G loss: 0.392692]\n",
      "epoch:4 step:4515 [D loss: 0.568886, acc.: 68.75%] [G loss: 0.455522]\n",
      "epoch:4 step:4516 [D loss: 0.561591, acc.: 63.28%] [G loss: 0.542584]\n",
      "epoch:4 step:4517 [D loss: 0.470147, acc.: 81.25%] [G loss: 0.731944]\n",
      "epoch:4 step:4518 [D loss: 0.584200, acc.: 66.41%] [G loss: 0.481929]\n",
      "epoch:4 step:4519 [D loss: 0.514148, acc.: 71.88%] [G loss: 0.572165]\n",
      "epoch:4 step:4520 [D loss: 0.520174, acc.: 75.00%] [G loss: 0.518764]\n",
      "epoch:4 step:4521 [D loss: 0.537597, acc.: 78.91%] [G loss: 0.593191]\n",
      "epoch:4 step:4522 [D loss: 0.595224, acc.: 72.66%] [G loss: 0.471350]\n",
      "epoch:4 step:4523 [D loss: 0.553829, acc.: 68.75%] [G loss: 0.480031]\n",
      "epoch:4 step:4524 [D loss: 0.552865, acc.: 68.75%] [G loss: 0.444067]\n",
      "epoch:4 step:4525 [D loss: 0.516620, acc.: 70.31%] [G loss: 0.513783]\n",
      "epoch:4 step:4526 [D loss: 0.578682, acc.: 64.84%] [G loss: 0.479857]\n",
      "epoch:4 step:4527 [D loss: 0.559755, acc.: 70.31%] [G loss: 0.693898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4528 [D loss: 0.535319, acc.: 77.34%] [G loss: 0.566510]\n",
      "epoch:4 step:4529 [D loss: 0.473497, acc.: 78.91%] [G loss: 0.812171]\n",
      "epoch:4 step:4530 [D loss: 0.510455, acc.: 75.00%] [G loss: 0.868122]\n",
      "epoch:4 step:4531 [D loss: 0.583493, acc.: 71.09%] [G loss: 0.513695]\n",
      "epoch:4 step:4532 [D loss: 0.569076, acc.: 70.31%] [G loss: 0.422108]\n",
      "epoch:4 step:4533 [D loss: 0.540578, acc.: 74.22%] [G loss: 0.489763]\n",
      "epoch:4 step:4534 [D loss: 0.525743, acc.: 71.09%] [G loss: 0.486558]\n",
      "epoch:4 step:4535 [D loss: 0.579158, acc.: 68.75%] [G loss: 0.507629]\n",
      "epoch:4 step:4536 [D loss: 0.624109, acc.: 64.06%] [G loss: 0.434545]\n",
      "epoch:4 step:4537 [D loss: 0.522332, acc.: 75.00%] [G loss: 0.435517]\n",
      "epoch:4 step:4538 [D loss: 0.519927, acc.: 75.78%] [G loss: 0.421242]\n",
      "epoch:4 step:4539 [D loss: 0.548450, acc.: 72.66%] [G loss: 0.512125]\n",
      "epoch:4 step:4540 [D loss: 0.464786, acc.: 76.56%] [G loss: 0.516456]\n",
      "epoch:4 step:4541 [D loss: 0.561959, acc.: 73.44%] [G loss: 0.582795]\n",
      "epoch:4 step:4542 [D loss: 0.566843, acc.: 68.75%] [G loss: 0.464168]\n",
      "epoch:4 step:4543 [D loss: 0.524934, acc.: 73.44%] [G loss: 0.577742]\n",
      "epoch:4 step:4544 [D loss: 0.469949, acc.: 80.47%] [G loss: 0.573911]\n",
      "epoch:4 step:4545 [D loss: 0.562917, acc.: 71.09%] [G loss: 0.434744]\n",
      "epoch:4 step:4546 [D loss: 0.518936, acc.: 78.12%] [G loss: 0.578626]\n",
      "epoch:4 step:4547 [D loss: 0.562732, acc.: 71.88%] [G loss: 0.597878]\n",
      "epoch:4 step:4548 [D loss: 0.563232, acc.: 67.97%] [G loss: 0.438167]\n",
      "epoch:4 step:4549 [D loss: 0.486596, acc.: 80.47%] [G loss: 0.592145]\n",
      "epoch:4 step:4550 [D loss: 0.554611, acc.: 71.88%] [G loss: 0.570442]\n",
      "epoch:4 step:4551 [D loss: 0.472887, acc.: 79.69%] [G loss: 0.652311]\n",
      "epoch:4 step:4552 [D loss: 0.543774, acc.: 70.31%] [G loss: 0.495676]\n",
      "epoch:4 step:4553 [D loss: 0.481479, acc.: 74.22%] [G loss: 0.477707]\n",
      "epoch:4 step:4554 [D loss: 0.492366, acc.: 78.12%] [G loss: 0.561685]\n",
      "epoch:4 step:4555 [D loss: 0.507003, acc.: 71.88%] [G loss: 0.641564]\n",
      "epoch:4 step:4556 [D loss: 0.597843, acc.: 68.75%] [G loss: 0.589932]\n",
      "epoch:4 step:4557 [D loss: 0.591486, acc.: 64.06%] [G loss: 0.552516]\n",
      "epoch:4 step:4558 [D loss: 0.495991, acc.: 79.69%] [G loss: 0.542518]\n",
      "epoch:4 step:4559 [D loss: 0.554630, acc.: 71.09%] [G loss: 0.456576]\n",
      "epoch:4 step:4560 [D loss: 0.649421, acc.: 60.16%] [G loss: 0.411498]\n",
      "epoch:4 step:4561 [D loss: 0.535396, acc.: 75.78%] [G loss: 0.549674]\n",
      "epoch:4 step:4562 [D loss: 0.489141, acc.: 76.56%] [G loss: 0.511592]\n",
      "epoch:4 step:4563 [D loss: 0.496840, acc.: 78.12%] [G loss: 0.615953]\n",
      "epoch:4 step:4564 [D loss: 0.534728, acc.: 71.09%] [G loss: 0.761127]\n",
      "epoch:4 step:4565 [D loss: 0.610447, acc.: 60.94%] [G loss: 0.638209]\n",
      "epoch:4 step:4566 [D loss: 0.519685, acc.: 73.44%] [G loss: 0.572965]\n",
      "epoch:4 step:4567 [D loss: 0.526860, acc.: 75.00%] [G loss: 0.430712]\n",
      "epoch:4 step:4568 [D loss: 0.587046, acc.: 67.19%] [G loss: 0.482310]\n",
      "epoch:4 step:4569 [D loss: 0.559933, acc.: 70.31%] [G loss: 0.431836]\n",
      "epoch:4 step:4570 [D loss: 0.489798, acc.: 78.91%] [G loss: 0.442865]\n",
      "epoch:4 step:4571 [D loss: 0.550698, acc.: 67.97%] [G loss: 0.491653]\n",
      "epoch:4 step:4572 [D loss: 0.600435, acc.: 67.19%] [G loss: 0.449885]\n",
      "epoch:4 step:4573 [D loss: 0.561780, acc.: 70.31%] [G loss: 0.492591]\n",
      "epoch:4 step:4574 [D loss: 0.580812, acc.: 71.88%] [G loss: 0.389015]\n",
      "epoch:4 step:4575 [D loss: 0.575049, acc.: 67.19%] [G loss: 0.586281]\n",
      "epoch:4 step:4576 [D loss: 0.583343, acc.: 64.06%] [G loss: 0.434123]\n",
      "epoch:4 step:4577 [D loss: 0.487014, acc.: 78.12%] [G loss: 0.570386]\n",
      "epoch:4 step:4578 [D loss: 0.537590, acc.: 71.88%] [G loss: 0.455606]\n",
      "epoch:4 step:4579 [D loss: 0.491921, acc.: 75.00%] [G loss: 0.606729]\n",
      "epoch:4 step:4580 [D loss: 0.575894, acc.: 67.19%] [G loss: 0.668898]\n",
      "epoch:4 step:4581 [D loss: 0.460615, acc.: 78.12%] [G loss: 0.664436]\n",
      "epoch:4 step:4582 [D loss: 0.495386, acc.: 78.91%] [G loss: 0.539377]\n",
      "epoch:4 step:4583 [D loss: 0.510414, acc.: 75.78%] [G loss: 0.548378]\n",
      "epoch:4 step:4584 [D loss: 0.535910, acc.: 71.09%] [G loss: 0.562610]\n",
      "epoch:4 step:4585 [D loss: 0.523032, acc.: 75.00%] [G loss: 0.595355]\n",
      "epoch:4 step:4586 [D loss: 0.562808, acc.: 71.09%] [G loss: 0.473405]\n",
      "epoch:4 step:4587 [D loss: 0.546013, acc.: 71.09%] [G loss: 0.447490]\n",
      "epoch:4 step:4588 [D loss: 0.623491, acc.: 65.62%] [G loss: 0.382941]\n",
      "epoch:4 step:4589 [D loss: 0.499173, acc.: 78.91%] [G loss: 0.560019]\n",
      "epoch:4 step:4590 [D loss: 0.483114, acc.: 75.78%] [G loss: 0.684226]\n",
      "epoch:4 step:4591 [D loss: 0.600965, acc.: 66.41%] [G loss: 0.474214]\n",
      "epoch:4 step:4592 [D loss: 0.555823, acc.: 78.12%] [G loss: 0.403181]\n",
      "epoch:4 step:4593 [D loss: 0.575153, acc.: 71.88%] [G loss: 0.450004]\n",
      "epoch:4 step:4594 [D loss: 0.550818, acc.: 69.53%] [G loss: 0.374471]\n",
      "epoch:4 step:4595 [D loss: 0.590925, acc.: 67.97%] [G loss: 0.467059]\n",
      "epoch:4 step:4596 [D loss: 0.543502, acc.: 73.44%] [G loss: 0.405926]\n",
      "epoch:4 step:4597 [D loss: 0.574917, acc.: 67.97%] [G loss: 0.454643]\n",
      "epoch:4 step:4598 [D loss: 0.551399, acc.: 68.75%] [G loss: 0.465868]\n",
      "epoch:4 step:4599 [D loss: 0.571445, acc.: 71.88%] [G loss: 0.485853]\n",
      "epoch:4 step:4600 [D loss: 0.560843, acc.: 75.78%] [G loss: 0.395161]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.881305\n",
      "FID: 47.346092\n",
      "0 = 12.823773054599696\n",
      "1 = 0.0993385372804214\n",
      "2 = 0.9391000270843506\n",
      "3 = 0.8808000087738037\n",
      "4 = 0.9973999857902527\n",
      "5 = 0.9970568418502808\n",
      "6 = 0.8808000087738037\n",
      "7 = 8.589527684378591\n",
      "8 = 0.13773037078628037\n",
      "9 = 0.823199987411499\n",
      "10 = 0.79339998960495\n",
      "11 = 0.8529999852180481\n",
      "12 = 0.8436835408210754\n",
      "13 = 0.79339998960495\n",
      "14 = 5.881326675415039\n",
      "15 = 8.107017517089844\n",
      "16 = 0.3080780506134033\n",
      "17 = 5.88130521774292\n",
      "18 = 47.346092224121094\n",
      "epoch:4 step:4601 [D loss: 0.487044, acc.: 81.25%] [G loss: 0.572520]\n",
      "epoch:4 step:4602 [D loss: 0.476000, acc.: 81.25%] [G loss: 0.578230]\n",
      "epoch:4 step:4603 [D loss: 0.524005, acc.: 75.00%] [G loss: 0.636944]\n",
      "epoch:4 step:4604 [D loss: 0.596563, acc.: 65.62%] [G loss: 0.567286]\n",
      "epoch:4 step:4605 [D loss: 0.545113, acc.: 70.31%] [G loss: 0.673101]\n",
      "epoch:4 step:4606 [D loss: 0.659884, acc.: 62.50%] [G loss: 0.443369]\n",
      "epoch:4 step:4607 [D loss: 0.487424, acc.: 77.34%] [G loss: 0.569151]\n",
      "epoch:4 step:4608 [D loss: 0.482096, acc.: 77.34%] [G loss: 0.674560]\n",
      "epoch:4 step:4609 [D loss: 0.565857, acc.: 67.19%] [G loss: 0.486947]\n",
      "epoch:4 step:4610 [D loss: 0.545430, acc.: 76.56%] [G loss: 0.411255]\n",
      "epoch:4 step:4611 [D loss: 0.561484, acc.: 68.75%] [G loss: 0.397171]\n",
      "epoch:4 step:4612 [D loss: 0.537272, acc.: 70.31%] [G loss: 0.460230]\n",
      "epoch:4 step:4613 [D loss: 0.565722, acc.: 68.75%] [G loss: 0.511101]\n",
      "epoch:4 step:4614 [D loss: 0.535779, acc.: 70.31%] [G loss: 0.495975]\n",
      "epoch:4 step:4615 [D loss: 0.631308, acc.: 64.84%] [G loss: 0.435265]\n",
      "epoch:4 step:4616 [D loss: 0.531410, acc.: 73.44%] [G loss: 0.377654]\n",
      "epoch:4 step:4617 [D loss: 0.556273, acc.: 68.75%] [G loss: 0.445988]\n",
      "epoch:4 step:4618 [D loss: 0.504828, acc.: 75.78%] [G loss: 0.457564]\n",
      "epoch:4 step:4619 [D loss: 0.514736, acc.: 75.78%] [G loss: 0.537168]\n",
      "epoch:4 step:4620 [D loss: 0.540789, acc.: 71.88%] [G loss: 0.563419]\n",
      "epoch:4 step:4621 [D loss: 0.593691, acc.: 62.50%] [G loss: 0.502984]\n",
      "epoch:4 step:4622 [D loss: 0.554621, acc.: 70.31%] [G loss: 0.512290]\n",
      "epoch:4 step:4623 [D loss: 0.477935, acc.: 79.69%] [G loss: 0.594722]\n",
      "epoch:4 step:4624 [D loss: 0.481515, acc.: 77.34%] [G loss: 0.560095]\n",
      "epoch:4 step:4625 [D loss: 0.503491, acc.: 82.81%] [G loss: 0.482473]\n",
      "epoch:4 step:4626 [D loss: 0.532798, acc.: 76.56%] [G loss: 0.536603]\n",
      "epoch:4 step:4627 [D loss: 0.583484, acc.: 67.97%] [G loss: 0.493558]\n",
      "epoch:4 step:4628 [D loss: 0.579588, acc.: 67.19%] [G loss: 0.443902]\n",
      "epoch:4 step:4629 [D loss: 0.525664, acc.: 73.44%] [G loss: 0.496935]\n",
      "epoch:4 step:4630 [D loss: 0.639833, acc.: 65.62%] [G loss: 0.456065]\n",
      "epoch:4 step:4631 [D loss: 0.598498, acc.: 67.19%] [G loss: 0.505801]\n",
      "epoch:4 step:4632 [D loss: 0.471380, acc.: 78.12%] [G loss: 0.634237]\n",
      "epoch:4 step:4633 [D loss: 0.462885, acc.: 77.34%] [G loss: 0.635071]\n",
      "epoch:4 step:4634 [D loss: 0.456114, acc.: 77.34%] [G loss: 0.712109]\n",
      "epoch:4 step:4635 [D loss: 0.558341, acc.: 70.31%] [G loss: 0.611196]\n",
      "epoch:4 step:4636 [D loss: 0.533402, acc.: 78.12%] [G loss: 0.589409]\n",
      "epoch:4 step:4637 [D loss: 0.580016, acc.: 69.53%] [G loss: 0.577122]\n",
      "epoch:4 step:4638 [D loss: 0.435070, acc.: 84.38%] [G loss: 0.639963]\n",
      "epoch:4 step:4639 [D loss: 0.544350, acc.: 73.44%] [G loss: 0.535756]\n",
      "epoch:4 step:4640 [D loss: 0.616449, acc.: 60.16%] [G loss: 0.519099]\n",
      "epoch:4 step:4641 [D loss: 0.562348, acc.: 69.53%] [G loss: 0.532645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4642 [D loss: 0.549364, acc.: 72.66%] [G loss: 0.542093]\n",
      "epoch:4 step:4643 [D loss: 0.527188, acc.: 75.00%] [G loss: 0.669343]\n",
      "epoch:4 step:4644 [D loss: 0.540492, acc.: 76.56%] [G loss: 0.643180]\n",
      "epoch:4 step:4645 [D loss: 0.528050, acc.: 73.44%] [G loss: 0.576864]\n",
      "epoch:4 step:4646 [D loss: 0.482799, acc.: 78.12%] [G loss: 0.601686]\n",
      "epoch:4 step:4647 [D loss: 0.500650, acc.: 75.78%] [G loss: 0.602894]\n",
      "epoch:4 step:4648 [D loss: 0.520714, acc.: 75.78%] [G loss: 0.536864]\n",
      "epoch:4 step:4649 [D loss: 0.558711, acc.: 67.19%] [G loss: 0.645356]\n",
      "epoch:4 step:4650 [D loss: 0.580611, acc.: 68.75%] [G loss: 0.506624]\n",
      "epoch:4 step:4651 [D loss: 0.563138, acc.: 67.19%] [G loss: 0.562438]\n",
      "epoch:4 step:4652 [D loss: 0.562489, acc.: 68.75%] [G loss: 0.379440]\n",
      "epoch:4 step:4653 [D loss: 0.569573, acc.: 71.09%] [G loss: 0.438948]\n",
      "epoch:4 step:4654 [D loss: 0.483325, acc.: 78.91%] [G loss: 0.536331]\n",
      "epoch:4 step:4655 [D loss: 0.572965, acc.: 66.41%] [G loss: 0.519971]\n",
      "epoch:4 step:4656 [D loss: 0.531720, acc.: 73.44%] [G loss: 0.570090]\n",
      "epoch:4 step:4657 [D loss: 0.471920, acc.: 80.47%] [G loss: 0.613768]\n",
      "epoch:4 step:4658 [D loss: 0.492268, acc.: 72.66%] [G loss: 0.651414]\n",
      "epoch:4 step:4659 [D loss: 0.447154, acc.: 81.25%] [G loss: 0.736815]\n",
      "epoch:4 step:4660 [D loss: 0.472299, acc.: 76.56%] [G loss: 0.733719]\n",
      "epoch:4 step:4661 [D loss: 0.563157, acc.: 72.66%] [G loss: 0.611558]\n",
      "epoch:4 step:4662 [D loss: 0.506429, acc.: 74.22%] [G loss: 0.563527]\n",
      "epoch:4 step:4663 [D loss: 0.622720, acc.: 62.50%] [G loss: 0.509218]\n",
      "epoch:4 step:4664 [D loss: 0.601713, acc.: 67.19%] [G loss: 0.493245]\n",
      "epoch:4 step:4665 [D loss: 0.598470, acc.: 67.97%] [G loss: 0.556387]\n",
      "epoch:4 step:4666 [D loss: 0.487963, acc.: 74.22%] [G loss: 0.529254]\n",
      "epoch:4 step:4667 [D loss: 0.545724, acc.: 67.97%] [G loss: 0.581540]\n",
      "epoch:4 step:4668 [D loss: 0.652314, acc.: 61.72%] [G loss: 0.648576]\n",
      "epoch:4 step:4669 [D loss: 0.494717, acc.: 74.22%] [G loss: 0.606643]\n",
      "epoch:4 step:4670 [D loss: 0.524067, acc.: 76.56%] [G loss: 0.504777]\n",
      "epoch:4 step:4671 [D loss: 0.448045, acc.: 76.56%] [G loss: 0.626763]\n",
      "epoch:4 step:4672 [D loss: 0.400402, acc.: 85.16%] [G loss: 0.651555]\n",
      "epoch:4 step:4673 [D loss: 0.430471, acc.: 82.03%] [G loss: 0.872814]\n",
      "epoch:4 step:4674 [D loss: 0.465522, acc.: 79.69%] [G loss: 1.032183]\n",
      "epoch:4 step:4675 [D loss: 0.555664, acc.: 70.31%] [G loss: 0.963832]\n",
      "epoch:4 step:4676 [D loss: 0.759119, acc.: 61.72%] [G loss: 0.752373]\n",
      "epoch:4 step:4677 [D loss: 0.411911, acc.: 81.25%] [G loss: 1.021340]\n",
      "epoch:4 step:4678 [D loss: 0.445491, acc.: 82.03%] [G loss: 1.222980]\n",
      "epoch:4 step:4679 [D loss: 0.630311, acc.: 60.16%] [G loss: 0.684971]\n",
      "epoch:4 step:4680 [D loss: 0.659525, acc.: 61.72%] [G loss: 0.551600]\n",
      "epoch:4 step:4681 [D loss: 0.513805, acc.: 77.34%] [G loss: 0.636355]\n",
      "epoch:4 step:4682 [D loss: 0.553042, acc.: 70.31%] [G loss: 0.641670]\n",
      "epoch:4 step:4683 [D loss: 0.459196, acc.: 77.34%] [G loss: 0.678178]\n",
      "epoch:4 step:4684 [D loss: 0.419008, acc.: 82.03%] [G loss: 0.877658]\n",
      "epoch:4 step:4685 [D loss: 0.371611, acc.: 84.38%] [G loss: 0.822842]\n",
      "epoch:5 step:4686 [D loss: 0.637717, acc.: 66.41%] [G loss: 0.797216]\n",
      "epoch:5 step:4687 [D loss: 0.540951, acc.: 73.44%] [G loss: 0.785018]\n",
      "epoch:5 step:4688 [D loss: 0.595361, acc.: 67.19%] [G loss: 0.795711]\n",
      "epoch:5 step:4689 [D loss: 0.549001, acc.: 73.44%] [G loss: 0.561562]\n",
      "epoch:5 step:4690 [D loss: 0.560653, acc.: 69.53%] [G loss: 0.620887]\n",
      "epoch:5 step:4691 [D loss: 0.523036, acc.: 66.41%] [G loss: 0.593211]\n",
      "epoch:5 step:4692 [D loss: 0.507907, acc.: 75.78%] [G loss: 0.578234]\n",
      "epoch:5 step:4693 [D loss: 0.519842, acc.: 70.31%] [G loss: 0.521367]\n",
      "epoch:5 step:4694 [D loss: 0.528715, acc.: 72.66%] [G loss: 0.626602]\n",
      "epoch:5 step:4695 [D loss: 0.560367, acc.: 73.44%] [G loss: 0.488932]\n",
      "epoch:5 step:4696 [D loss: 0.495849, acc.: 77.34%] [G loss: 0.614981]\n",
      "epoch:5 step:4697 [D loss: 0.560422, acc.: 67.97%] [G loss: 0.512975]\n",
      "epoch:5 step:4698 [D loss: 0.549416, acc.: 71.09%] [G loss: 0.552097]\n",
      "epoch:5 step:4699 [D loss: 0.534110, acc.: 76.56%] [G loss: 0.500986]\n",
      "epoch:5 step:4700 [D loss: 0.472013, acc.: 78.91%] [G loss: 0.579707]\n",
      "epoch:5 step:4701 [D loss: 0.469886, acc.: 79.69%] [G loss: 0.666822]\n",
      "epoch:5 step:4702 [D loss: 0.565460, acc.: 67.97%] [G loss: 0.521610]\n",
      "epoch:5 step:4703 [D loss: 0.612317, acc.: 63.28%] [G loss: 0.582503]\n",
      "epoch:5 step:4704 [D loss: 0.547588, acc.: 71.09%] [G loss: 0.472926]\n",
      "epoch:5 step:4705 [D loss: 0.604413, acc.: 68.75%] [G loss: 0.495744]\n",
      "epoch:5 step:4706 [D loss: 0.526233, acc.: 75.78%] [G loss: 0.574715]\n",
      "epoch:5 step:4707 [D loss: 0.513882, acc.: 78.91%] [G loss: 0.562544]\n",
      "epoch:5 step:4708 [D loss: 0.530580, acc.: 71.09%] [G loss: 0.574985]\n",
      "epoch:5 step:4709 [D loss: 0.499197, acc.: 73.44%] [G loss: 0.598778]\n",
      "epoch:5 step:4710 [D loss: 0.500337, acc.: 78.91%] [G loss: 0.612597]\n",
      "epoch:5 step:4711 [D loss: 0.599238, acc.: 64.06%] [G loss: 0.520038]\n",
      "epoch:5 step:4712 [D loss: 0.496336, acc.: 77.34%] [G loss: 0.517167]\n",
      "epoch:5 step:4713 [D loss: 0.533400, acc.: 71.88%] [G loss: 0.517634]\n",
      "epoch:5 step:4714 [D loss: 0.504504, acc.: 75.00%] [G loss: 0.507265]\n",
      "epoch:5 step:4715 [D loss: 0.550244, acc.: 73.44%] [G loss: 0.668162]\n",
      "epoch:5 step:4716 [D loss: 0.610949, acc.: 65.62%] [G loss: 0.503280]\n",
      "epoch:5 step:4717 [D loss: 0.597045, acc.: 58.59%] [G loss: 0.450214]\n",
      "epoch:5 step:4718 [D loss: 0.550254, acc.: 71.09%] [G loss: 0.447026]\n",
      "epoch:5 step:4719 [D loss: 0.486471, acc.: 78.12%] [G loss: 0.555543]\n",
      "epoch:5 step:4720 [D loss: 0.532605, acc.: 73.44%] [G loss: 0.480840]\n",
      "epoch:5 step:4721 [D loss: 0.496182, acc.: 74.22%] [G loss: 0.593949]\n",
      "epoch:5 step:4722 [D loss: 0.468249, acc.: 82.03%] [G loss: 0.519660]\n",
      "epoch:5 step:4723 [D loss: 0.608929, acc.: 71.09%] [G loss: 0.436858]\n",
      "epoch:5 step:4724 [D loss: 0.505556, acc.: 79.69%] [G loss: 0.475469]\n",
      "epoch:5 step:4725 [D loss: 0.463655, acc.: 77.34%] [G loss: 0.535120]\n",
      "epoch:5 step:4726 [D loss: 0.521975, acc.: 73.44%] [G loss: 0.606625]\n",
      "epoch:5 step:4727 [D loss: 0.507497, acc.: 72.66%] [G loss: 0.593937]\n",
      "epoch:5 step:4728 [D loss: 0.498051, acc.: 71.88%] [G loss: 0.538310]\n",
      "epoch:5 step:4729 [D loss: 0.568941, acc.: 71.09%] [G loss: 0.612954]\n",
      "epoch:5 step:4730 [D loss: 0.501612, acc.: 78.12%] [G loss: 0.594444]\n",
      "epoch:5 step:4731 [D loss: 0.548890, acc.: 67.97%] [G loss: 0.584428]\n",
      "epoch:5 step:4732 [D loss: 0.530121, acc.: 70.31%] [G loss: 0.504423]\n",
      "epoch:5 step:4733 [D loss: 0.530171, acc.: 71.09%] [G loss: 0.597995]\n",
      "epoch:5 step:4734 [D loss: 0.541465, acc.: 68.75%] [G loss: 0.618904]\n",
      "epoch:5 step:4735 [D loss: 0.542696, acc.: 76.56%] [G loss: 0.547393]\n",
      "epoch:5 step:4736 [D loss: 0.589465, acc.: 69.53%] [G loss: 0.480020]\n",
      "epoch:5 step:4737 [D loss: 0.536116, acc.: 72.66%] [G loss: 0.531307]\n",
      "epoch:5 step:4738 [D loss: 0.569295, acc.: 72.66%] [G loss: 0.666709]\n",
      "epoch:5 step:4739 [D loss: 0.460862, acc.: 78.91%] [G loss: 0.800524]\n",
      "epoch:5 step:4740 [D loss: 0.545529, acc.: 71.88%] [G loss: 0.572030]\n",
      "epoch:5 step:4741 [D loss: 0.586308, acc.: 67.97%] [G loss: 0.610328]\n",
      "epoch:5 step:4742 [D loss: 0.556378, acc.: 67.97%] [G loss: 0.514372]\n",
      "epoch:5 step:4743 [D loss: 0.583564, acc.: 65.62%] [G loss: 0.471955]\n",
      "epoch:5 step:4744 [D loss: 0.538475, acc.: 67.97%] [G loss: 0.511935]\n",
      "epoch:5 step:4745 [D loss: 0.560698, acc.: 68.75%] [G loss: 0.488935]\n",
      "epoch:5 step:4746 [D loss: 0.492942, acc.: 78.91%] [G loss: 0.624147]\n",
      "epoch:5 step:4747 [D loss: 0.598270, acc.: 69.53%] [G loss: 0.460214]\n",
      "epoch:5 step:4748 [D loss: 0.575982, acc.: 67.97%] [G loss: 0.430457]\n",
      "epoch:5 step:4749 [D loss: 0.577692, acc.: 69.53%] [G loss: 0.479680]\n",
      "epoch:5 step:4750 [D loss: 0.529091, acc.: 78.12%] [G loss: 0.479153]\n",
      "epoch:5 step:4751 [D loss: 0.540568, acc.: 77.34%] [G loss: 0.454430]\n",
      "epoch:5 step:4752 [D loss: 0.545981, acc.: 71.09%] [G loss: 0.510618]\n",
      "epoch:5 step:4753 [D loss: 0.488448, acc.: 76.56%] [G loss: 0.637593]\n",
      "epoch:5 step:4754 [D loss: 0.520093, acc.: 75.78%] [G loss: 0.540973]\n",
      "epoch:5 step:4755 [D loss: 0.516346, acc.: 69.53%] [G loss: 0.619990]\n",
      "epoch:5 step:4756 [D loss: 0.532832, acc.: 69.53%] [G loss: 0.522429]\n",
      "epoch:5 step:4757 [D loss: 0.469751, acc.: 80.47%] [G loss: 0.527314]\n",
      "epoch:5 step:4758 [D loss: 0.559339, acc.: 75.78%] [G loss: 0.504009]\n",
      "epoch:5 step:4759 [D loss: 0.495837, acc.: 77.34%] [G loss: 0.502004]\n",
      "epoch:5 step:4760 [D loss: 0.487534, acc.: 78.12%] [G loss: 0.653311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4761 [D loss: 0.510558, acc.: 71.88%] [G loss: 0.561657]\n",
      "epoch:5 step:4762 [D loss: 0.436813, acc.: 82.81%] [G loss: 0.712993]\n",
      "epoch:5 step:4763 [D loss: 0.646640, acc.: 64.84%] [G loss: 0.544548]\n",
      "epoch:5 step:4764 [D loss: 0.546875, acc.: 71.88%] [G loss: 0.438865]\n",
      "epoch:5 step:4765 [D loss: 0.570517, acc.: 66.41%] [G loss: 0.480567]\n",
      "epoch:5 step:4766 [D loss: 0.541512, acc.: 67.19%] [G loss: 0.507559]\n",
      "epoch:5 step:4767 [D loss: 0.500416, acc.: 71.09%] [G loss: 0.629218]\n",
      "epoch:5 step:4768 [D loss: 0.527841, acc.: 72.66%] [G loss: 0.520544]\n",
      "epoch:5 step:4769 [D loss: 0.524354, acc.: 71.09%] [G loss: 0.577049]\n",
      "epoch:5 step:4770 [D loss: 0.608487, acc.: 65.62%] [G loss: 0.418949]\n",
      "epoch:5 step:4771 [D loss: 0.549382, acc.: 71.09%] [G loss: 0.472153]\n",
      "epoch:5 step:4772 [D loss: 0.493118, acc.: 75.00%] [G loss: 0.568529]\n",
      "epoch:5 step:4773 [D loss: 0.496704, acc.: 75.00%] [G loss: 0.598299]\n",
      "epoch:5 step:4774 [D loss: 0.503889, acc.: 76.56%] [G loss: 0.601173]\n",
      "epoch:5 step:4775 [D loss: 0.538546, acc.: 73.44%] [G loss: 0.458416]\n",
      "epoch:5 step:4776 [D loss: 0.594428, acc.: 67.19%] [G loss: 0.517894]\n",
      "epoch:5 step:4777 [D loss: 0.519726, acc.: 73.44%] [G loss: 0.666399]\n",
      "epoch:5 step:4778 [D loss: 0.501735, acc.: 74.22%] [G loss: 0.572998]\n",
      "epoch:5 step:4779 [D loss: 0.510424, acc.: 75.78%] [G loss: 0.545825]\n",
      "epoch:5 step:4780 [D loss: 0.535929, acc.: 70.31%] [G loss: 0.571110]\n",
      "epoch:5 step:4781 [D loss: 0.477210, acc.: 75.78%] [G loss: 0.587513]\n",
      "epoch:5 step:4782 [D loss: 0.508241, acc.: 73.44%] [G loss: 0.659580]\n",
      "epoch:5 step:4783 [D loss: 0.531995, acc.: 74.22%] [G loss: 0.578314]\n",
      "epoch:5 step:4784 [D loss: 0.566075, acc.: 71.88%] [G loss: 0.664877]\n",
      "epoch:5 step:4785 [D loss: 0.454086, acc.: 77.34%] [G loss: 0.600133]\n",
      "epoch:5 step:4786 [D loss: 0.512905, acc.: 74.22%] [G loss: 0.642697]\n",
      "epoch:5 step:4787 [D loss: 0.523587, acc.: 75.00%] [G loss: 0.613047]\n",
      "epoch:5 step:4788 [D loss: 0.474813, acc.: 78.12%] [G loss: 0.605416]\n",
      "epoch:5 step:4789 [D loss: 0.571695, acc.: 64.84%] [G loss: 0.499452]\n",
      "epoch:5 step:4790 [D loss: 0.507089, acc.: 77.34%] [G loss: 0.485770]\n",
      "epoch:5 step:4791 [D loss: 0.557950, acc.: 71.88%] [G loss: 0.498791]\n",
      "epoch:5 step:4792 [D loss: 0.593983, acc.: 65.62%] [G loss: 0.613088]\n",
      "epoch:5 step:4793 [D loss: 0.600925, acc.: 67.97%] [G loss: 0.505275]\n",
      "epoch:5 step:4794 [D loss: 0.662955, acc.: 60.94%] [G loss: 0.406043]\n",
      "epoch:5 step:4795 [D loss: 0.548597, acc.: 74.22%] [G loss: 0.570055]\n",
      "epoch:5 step:4796 [D loss: 0.516177, acc.: 75.78%] [G loss: 0.543946]\n",
      "epoch:5 step:4797 [D loss: 0.575175, acc.: 71.09%] [G loss: 0.420989]\n",
      "epoch:5 step:4798 [D loss: 0.567589, acc.: 68.75%] [G loss: 0.506263]\n",
      "epoch:5 step:4799 [D loss: 0.607898, acc.: 69.53%] [G loss: 0.458739]\n",
      "epoch:5 step:4800 [D loss: 0.547792, acc.: 75.00%] [G loss: 0.503915]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.001296\n",
      "FID: 46.241108\n",
      "0 = 13.023387108039882\n",
      "1 = 0.11419728297861235\n",
      "2 = 0.9404000043869019\n",
      "3 = 0.8838000297546387\n",
      "4 = 0.996999979019165\n",
      "5 = 0.9966170787811279\n",
      "6 = 0.8838000297546387\n",
      "7 = 8.556393553423895\n",
      "8 = 0.13646958671157836\n",
      "9 = 0.8203999996185303\n",
      "10 = 0.7990000247955322\n",
      "11 = 0.8417999744415283\n",
      "12 = 0.8347262740135193\n",
      "13 = 0.7990000247955322\n",
      "14 = 6.001312255859375\n",
      "15 = 8.001972198486328\n",
      "16 = 0.3105694055557251\n",
      "17 = 6.001295566558838\n",
      "18 = 46.24110794067383\n",
      "epoch:5 step:4801 [D loss: 0.568588, acc.: 67.19%] [G loss: 0.557822]\n",
      "epoch:5 step:4802 [D loss: 0.528961, acc.: 79.69%] [G loss: 0.486489]\n",
      "epoch:5 step:4803 [D loss: 0.496631, acc.: 78.91%] [G loss: 0.636035]\n",
      "epoch:5 step:4804 [D loss: 0.469715, acc.: 82.03%] [G loss: 0.706547]\n",
      "epoch:5 step:4805 [D loss: 0.583298, acc.: 68.75%] [G loss: 0.496529]\n",
      "epoch:5 step:4806 [D loss: 0.559974, acc.: 73.44%] [G loss: 0.451848]\n",
      "epoch:5 step:4807 [D loss: 0.506778, acc.: 78.91%] [G loss: 0.606460]\n",
      "epoch:5 step:4808 [D loss: 0.584420, acc.: 66.41%] [G loss: 0.638205]\n",
      "epoch:5 step:4809 [D loss: 0.544047, acc.: 71.09%] [G loss: 0.607671]\n",
      "epoch:5 step:4810 [D loss: 0.538556, acc.: 68.75%] [G loss: 0.624256]\n",
      "epoch:5 step:4811 [D loss: 0.525455, acc.: 75.00%] [G loss: 0.537410]\n",
      "epoch:5 step:4812 [D loss: 0.532759, acc.: 71.88%] [G loss: 0.531560]\n",
      "epoch:5 step:4813 [D loss: 0.547802, acc.: 67.19%] [G loss: 0.384823]\n",
      "epoch:5 step:4814 [D loss: 0.597665, acc.: 71.09%] [G loss: 0.400585]\n",
      "epoch:5 step:4815 [D loss: 0.536326, acc.: 71.09%] [G loss: 0.584631]\n",
      "epoch:5 step:4816 [D loss: 0.507527, acc.: 76.56%] [G loss: 0.692993]\n",
      "epoch:5 step:4817 [D loss: 0.522765, acc.: 76.56%] [G loss: 0.560415]\n",
      "epoch:5 step:4818 [D loss: 0.641628, acc.: 60.94%] [G loss: 0.482925]\n",
      "epoch:5 step:4819 [D loss: 0.545636, acc.: 70.31%] [G loss: 0.516578]\n",
      "epoch:5 step:4820 [D loss: 0.486668, acc.: 71.88%] [G loss: 0.650802]\n",
      "epoch:5 step:4821 [D loss: 0.511402, acc.: 74.22%] [G loss: 0.473237]\n",
      "epoch:5 step:4822 [D loss: 0.603180, acc.: 67.19%] [G loss: 0.550954]\n",
      "epoch:5 step:4823 [D loss: 0.575755, acc.: 68.75%] [G loss: 0.516361]\n",
      "epoch:5 step:4824 [D loss: 0.617685, acc.: 66.41%] [G loss: 0.517139]\n",
      "epoch:5 step:4825 [D loss: 0.557730, acc.: 70.31%] [G loss: 0.469750]\n",
      "epoch:5 step:4826 [D loss: 0.536721, acc.: 67.19%] [G loss: 0.493886]\n",
      "epoch:5 step:4827 [D loss: 0.542084, acc.: 71.09%] [G loss: 0.509313]\n",
      "epoch:5 step:4828 [D loss: 0.599007, acc.: 66.41%] [G loss: 0.422842]\n",
      "epoch:5 step:4829 [D loss: 0.546858, acc.: 75.00%] [G loss: 0.569072]\n",
      "epoch:5 step:4830 [D loss: 0.575899, acc.: 64.84%] [G loss: 0.544600]\n",
      "epoch:5 step:4831 [D loss: 0.486267, acc.: 78.91%] [G loss: 0.578422]\n",
      "epoch:5 step:4832 [D loss: 0.594202, acc.: 73.44%] [G loss: 0.558735]\n",
      "epoch:5 step:4833 [D loss: 0.575711, acc.: 71.88%] [G loss: 0.492674]\n",
      "epoch:5 step:4834 [D loss: 0.506704, acc.: 77.34%] [G loss: 0.536617]\n",
      "epoch:5 step:4835 [D loss: 0.619484, acc.: 66.41%] [G loss: 0.454358]\n",
      "epoch:5 step:4836 [D loss: 0.558860, acc.: 66.41%] [G loss: 0.480578]\n",
      "epoch:5 step:4837 [D loss: 0.466300, acc.: 78.91%] [G loss: 0.599590]\n",
      "epoch:5 step:4838 [D loss: 0.574630, acc.: 68.75%] [G loss: 0.515849]\n",
      "epoch:5 step:4839 [D loss: 0.505064, acc.: 75.78%] [G loss: 0.477439]\n",
      "epoch:5 step:4840 [D loss: 0.480694, acc.: 77.34%] [G loss: 0.549164]\n",
      "epoch:5 step:4841 [D loss: 0.521594, acc.: 75.00%] [G loss: 0.535270]\n",
      "epoch:5 step:4842 [D loss: 0.571684, acc.: 71.09%] [G loss: 0.504562]\n",
      "epoch:5 step:4843 [D loss: 0.566579, acc.: 74.22%] [G loss: 0.491039]\n",
      "epoch:5 step:4844 [D loss: 0.542069, acc.: 71.88%] [G loss: 0.564701]\n",
      "epoch:5 step:4845 [D loss: 0.530161, acc.: 76.56%] [G loss: 0.552873]\n",
      "epoch:5 step:4846 [D loss: 0.496460, acc.: 75.78%] [G loss: 0.629871]\n",
      "epoch:5 step:4847 [D loss: 0.499767, acc.: 75.00%] [G loss: 0.570863]\n",
      "epoch:5 step:4848 [D loss: 0.544368, acc.: 71.88%] [G loss: 0.668589]\n",
      "epoch:5 step:4849 [D loss: 0.559614, acc.: 71.09%] [G loss: 0.556225]\n",
      "epoch:5 step:4850 [D loss: 0.532861, acc.: 75.00%] [G loss: 0.605636]\n",
      "epoch:5 step:4851 [D loss: 0.518575, acc.: 78.12%] [G loss: 0.536081]\n",
      "epoch:5 step:4852 [D loss: 0.545990, acc.: 71.88%] [G loss: 0.470670]\n",
      "epoch:5 step:4853 [D loss: 0.512191, acc.: 73.44%] [G loss: 0.572470]\n",
      "epoch:5 step:4854 [D loss: 0.564361, acc.: 70.31%] [G loss: 0.460448]\n",
      "epoch:5 step:4855 [D loss: 0.554110, acc.: 74.22%] [G loss: 0.452608]\n",
      "epoch:5 step:4856 [D loss: 0.516987, acc.: 73.44%] [G loss: 0.433364]\n",
      "epoch:5 step:4857 [D loss: 0.525393, acc.: 71.88%] [G loss: 0.556027]\n",
      "epoch:5 step:4858 [D loss: 0.526932, acc.: 72.66%] [G loss: 0.589046]\n",
      "epoch:5 step:4859 [D loss: 0.553042, acc.: 72.66%] [G loss: 0.436376]\n",
      "epoch:5 step:4860 [D loss: 0.534219, acc.: 78.91%] [G loss: 0.452325]\n",
      "epoch:5 step:4861 [D loss: 0.568463, acc.: 67.97%] [G loss: 0.480037]\n",
      "epoch:5 step:4862 [D loss: 0.512181, acc.: 74.22%] [G loss: 0.484256]\n",
      "epoch:5 step:4863 [D loss: 0.540147, acc.: 67.97%] [G loss: 0.581300]\n",
      "epoch:5 step:4864 [D loss: 0.558618, acc.: 71.88%] [G loss: 0.537848]\n",
      "epoch:5 step:4865 [D loss: 0.649367, acc.: 62.50%] [G loss: 0.442415]\n",
      "epoch:5 step:4866 [D loss: 0.526431, acc.: 75.00%] [G loss: 0.454530]\n",
      "epoch:5 step:4867 [D loss: 0.580123, acc.: 70.31%] [G loss: 0.447485]\n",
      "epoch:5 step:4868 [D loss: 0.568243, acc.: 72.66%] [G loss: 0.512284]\n",
      "epoch:5 step:4869 [D loss: 0.534504, acc.: 71.88%] [G loss: 0.564135]\n",
      "epoch:5 step:4870 [D loss: 0.608441, acc.: 64.06%] [G loss: 0.458194]\n",
      "epoch:5 step:4871 [D loss: 0.557787, acc.: 76.56%] [G loss: 0.588499]\n",
      "epoch:5 step:4872 [D loss: 0.643245, acc.: 60.94%] [G loss: 0.362023]\n",
      "epoch:5 step:4873 [D loss: 0.537237, acc.: 72.66%] [G loss: 0.454761]\n",
      "epoch:5 step:4874 [D loss: 0.512623, acc.: 76.56%] [G loss: 0.618190]\n",
      "epoch:5 step:4875 [D loss: 0.469532, acc.: 76.56%] [G loss: 0.537327]\n",
      "epoch:5 step:4876 [D loss: 0.529170, acc.: 75.00%] [G loss: 0.464867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4877 [D loss: 0.542623, acc.: 74.22%] [G loss: 0.597197]\n",
      "epoch:5 step:4878 [D loss: 0.498009, acc.: 79.69%] [G loss: 0.626766]\n",
      "epoch:5 step:4879 [D loss: 0.472592, acc.: 78.91%] [G loss: 0.419383]\n",
      "epoch:5 step:4880 [D loss: 0.573214, acc.: 65.62%] [G loss: 0.551716]\n",
      "epoch:5 step:4881 [D loss: 0.623685, acc.: 60.94%] [G loss: 0.474149]\n",
      "epoch:5 step:4882 [D loss: 0.511643, acc.: 76.56%] [G loss: 0.713025]\n",
      "epoch:5 step:4883 [D loss: 0.471931, acc.: 74.22%] [G loss: 0.629884]\n",
      "epoch:5 step:4884 [D loss: 0.501870, acc.: 76.56%] [G loss: 0.614101]\n",
      "epoch:5 step:4885 [D loss: 0.624717, acc.: 65.62%] [G loss: 0.479517]\n",
      "epoch:5 step:4886 [D loss: 0.565032, acc.: 71.88%] [G loss: 0.379391]\n",
      "epoch:5 step:4887 [D loss: 0.558617, acc.: 69.53%] [G loss: 0.434593]\n",
      "epoch:5 step:4888 [D loss: 0.589400, acc.: 71.09%] [G loss: 0.465812]\n",
      "epoch:5 step:4889 [D loss: 0.499662, acc.: 75.78%] [G loss: 0.583219]\n",
      "epoch:5 step:4890 [D loss: 0.511034, acc.: 74.22%] [G loss: 0.597845]\n",
      "epoch:5 step:4891 [D loss: 0.521603, acc.: 75.78%] [G loss: 0.752397]\n",
      "epoch:5 step:4892 [D loss: 0.427334, acc.: 80.47%] [G loss: 0.696157]\n",
      "epoch:5 step:4893 [D loss: 0.453982, acc.: 82.03%] [G loss: 0.700053]\n",
      "epoch:5 step:4894 [D loss: 0.537306, acc.: 73.44%] [G loss: 0.580714]\n",
      "epoch:5 step:4895 [D loss: 0.577582, acc.: 71.88%] [G loss: 0.526752]\n",
      "epoch:5 step:4896 [D loss: 0.547277, acc.: 75.00%] [G loss: 0.539010]\n",
      "epoch:5 step:4897 [D loss: 0.535736, acc.: 69.53%] [G loss: 0.454345]\n",
      "epoch:5 step:4898 [D loss: 0.533923, acc.: 70.31%] [G loss: 0.501331]\n",
      "epoch:5 step:4899 [D loss: 0.681781, acc.: 54.69%] [G loss: 0.430475]\n",
      "epoch:5 step:4900 [D loss: 0.632736, acc.: 63.28%] [G loss: 0.427592]\n",
      "epoch:5 step:4901 [D loss: 0.540181, acc.: 71.88%] [G loss: 0.495305]\n",
      "epoch:5 step:4902 [D loss: 0.467034, acc.: 80.47%] [G loss: 0.565715]\n",
      "epoch:5 step:4903 [D loss: 0.564835, acc.: 71.88%] [G loss: 0.596008]\n",
      "epoch:5 step:4904 [D loss: 0.568072, acc.: 72.66%] [G loss: 0.602298]\n",
      "epoch:5 step:4905 [D loss: 0.725833, acc.: 61.72%] [G loss: 0.505912]\n",
      "epoch:5 step:4906 [D loss: 0.527035, acc.: 73.44%] [G loss: 0.557950]\n",
      "epoch:5 step:4907 [D loss: 0.453548, acc.: 79.69%] [G loss: 0.671589]\n",
      "epoch:5 step:4908 [D loss: 0.512217, acc.: 75.78%] [G loss: 0.692209]\n",
      "epoch:5 step:4909 [D loss: 0.592455, acc.: 71.88%] [G loss: 0.586011]\n",
      "epoch:5 step:4910 [D loss: 0.535743, acc.: 75.00%] [G loss: 0.409709]\n",
      "epoch:5 step:4911 [D loss: 0.632873, acc.: 60.94%] [G loss: 0.518602]\n",
      "epoch:5 step:4912 [D loss: 0.627704, acc.: 61.72%] [G loss: 0.437296]\n",
      "epoch:5 step:4913 [D loss: 0.594296, acc.: 67.97%] [G loss: 0.423063]\n",
      "epoch:5 step:4914 [D loss: 0.569747, acc.: 68.75%] [G loss: 0.383401]\n",
      "epoch:5 step:4915 [D loss: 0.514920, acc.: 74.22%] [G loss: 0.593567]\n",
      "epoch:5 step:4916 [D loss: 0.498700, acc.: 71.88%] [G loss: 0.624985]\n",
      "epoch:5 step:4917 [D loss: 0.491716, acc.: 79.69%] [G loss: 0.876929]\n",
      "epoch:5 step:4918 [D loss: 0.602723, acc.: 67.97%] [G loss: 0.649299]\n",
      "epoch:5 step:4919 [D loss: 0.557423, acc.: 71.88%] [G loss: 0.495977]\n",
      "epoch:5 step:4920 [D loss: 0.589549, acc.: 61.72%] [G loss: 0.587072]\n",
      "epoch:5 step:4921 [D loss: 0.513666, acc.: 78.91%] [G loss: 0.569773]\n",
      "epoch:5 step:4922 [D loss: 0.540604, acc.: 75.78%] [G loss: 0.538024]\n",
      "epoch:5 step:4923 [D loss: 0.549467, acc.: 75.00%] [G loss: 0.425716]\n",
      "epoch:5 step:4924 [D loss: 0.523797, acc.: 71.09%] [G loss: 0.444630]\n",
      "epoch:5 step:4925 [D loss: 0.519495, acc.: 74.22%] [G loss: 0.411925]\n",
      "epoch:5 step:4926 [D loss: 0.549311, acc.: 74.22%] [G loss: 0.529208]\n",
      "epoch:5 step:4927 [D loss: 0.532406, acc.: 71.09%] [G loss: 0.562989]\n",
      "epoch:5 step:4928 [D loss: 0.537646, acc.: 71.88%] [G loss: 0.445978]\n",
      "epoch:5 step:4929 [D loss: 0.507435, acc.: 76.56%] [G loss: 0.612017]\n",
      "epoch:5 step:4930 [D loss: 0.509131, acc.: 77.34%] [G loss: 0.578762]\n",
      "epoch:5 step:4931 [D loss: 0.587491, acc.: 71.09%] [G loss: 0.633326]\n",
      "epoch:5 step:4932 [D loss: 0.622228, acc.: 64.06%] [G loss: 0.661213]\n",
      "epoch:5 step:4933 [D loss: 0.571281, acc.: 66.41%] [G loss: 0.513545]\n",
      "epoch:5 step:4934 [D loss: 0.503605, acc.: 71.88%] [G loss: 0.778555]\n",
      "epoch:5 step:4935 [D loss: 0.616195, acc.: 60.94%] [G loss: 0.522280]\n",
      "epoch:5 step:4936 [D loss: 0.690573, acc.: 55.47%] [G loss: 0.575327]\n",
      "epoch:5 step:4937 [D loss: 0.535984, acc.: 69.53%] [G loss: 0.462094]\n",
      "epoch:5 step:4938 [D loss: 0.604242, acc.: 69.53%] [G loss: 0.446150]\n",
      "epoch:5 step:4939 [D loss: 0.485286, acc.: 78.12%] [G loss: 0.529681]\n",
      "epoch:5 step:4940 [D loss: 0.557614, acc.: 71.09%] [G loss: 0.488415]\n",
      "epoch:5 step:4941 [D loss: 0.534834, acc.: 72.66%] [G loss: 0.479956]\n",
      "epoch:5 step:4942 [D loss: 0.589957, acc.: 71.09%] [G loss: 0.491790]\n",
      "epoch:5 step:4943 [D loss: 0.524520, acc.: 76.56%] [G loss: 0.499092]\n",
      "epoch:5 step:4944 [D loss: 0.496753, acc.: 75.78%] [G loss: 0.546723]\n",
      "epoch:5 step:4945 [D loss: 0.561494, acc.: 67.97%] [G loss: 0.521090]\n",
      "epoch:5 step:4946 [D loss: 0.522984, acc.: 75.78%] [G loss: 0.638816]\n",
      "epoch:5 step:4947 [D loss: 0.519746, acc.: 74.22%] [G loss: 0.598844]\n",
      "epoch:5 step:4948 [D loss: 0.673659, acc.: 62.50%] [G loss: 0.362775]\n",
      "epoch:5 step:4949 [D loss: 0.493966, acc.: 76.56%] [G loss: 0.612228]\n",
      "epoch:5 step:4950 [D loss: 0.533798, acc.: 77.34%] [G loss: 0.578719]\n",
      "epoch:5 step:4951 [D loss: 0.548063, acc.: 73.44%] [G loss: 0.476517]\n",
      "epoch:5 step:4952 [D loss: 0.563594, acc.: 71.09%] [G loss: 0.525624]\n",
      "epoch:5 step:4953 [D loss: 0.558660, acc.: 70.31%] [G loss: 0.496019]\n",
      "epoch:5 step:4954 [D loss: 0.615946, acc.: 64.84%] [G loss: 0.435713]\n",
      "epoch:5 step:4955 [D loss: 0.525909, acc.: 75.00%] [G loss: 0.572765]\n",
      "epoch:5 step:4956 [D loss: 0.524833, acc.: 67.97%] [G loss: 0.547303]\n",
      "epoch:5 step:4957 [D loss: 0.504297, acc.: 76.56%] [G loss: 0.615099]\n",
      "epoch:5 step:4958 [D loss: 0.566668, acc.: 71.88%] [G loss: 0.505603]\n",
      "epoch:5 step:4959 [D loss: 0.581159, acc.: 67.97%] [G loss: 0.517499]\n",
      "epoch:5 step:4960 [D loss: 0.610273, acc.: 64.06%] [G loss: 0.415777]\n",
      "epoch:5 step:4961 [D loss: 0.524564, acc.: 76.56%] [G loss: 0.460769]\n",
      "epoch:5 step:4962 [D loss: 0.605992, acc.: 66.41%] [G loss: 0.436978]\n",
      "epoch:5 step:4963 [D loss: 0.598726, acc.: 69.53%] [G loss: 0.387258]\n",
      "epoch:5 step:4964 [D loss: 0.581414, acc.: 67.97%] [G loss: 0.460792]\n",
      "epoch:5 step:4965 [D loss: 0.586097, acc.: 66.41%] [G loss: 0.512612]\n",
      "epoch:5 step:4966 [D loss: 0.602153, acc.: 69.53%] [G loss: 0.528290]\n",
      "epoch:5 step:4967 [D loss: 0.545556, acc.: 71.09%] [G loss: 0.516212]\n",
      "epoch:5 step:4968 [D loss: 0.498738, acc.: 84.38%] [G loss: 0.482652]\n",
      "epoch:5 step:4969 [D loss: 0.495744, acc.: 78.12%] [G loss: 0.598412]\n",
      "epoch:5 step:4970 [D loss: 0.479842, acc.: 77.34%] [G loss: 0.566814]\n",
      "epoch:5 step:4971 [D loss: 0.494698, acc.: 75.78%] [G loss: 0.580036]\n",
      "epoch:5 step:4972 [D loss: 0.604275, acc.: 66.41%] [G loss: 0.522121]\n",
      "epoch:5 step:4973 [D loss: 0.620490, acc.: 63.28%] [G loss: 0.426843]\n",
      "epoch:5 step:4974 [D loss: 0.549096, acc.: 72.66%] [G loss: 0.553119]\n",
      "epoch:5 step:4975 [D loss: 0.550901, acc.: 67.97%] [G loss: 0.515664]\n",
      "epoch:5 step:4976 [D loss: 0.570282, acc.: 69.53%] [G loss: 0.456489]\n",
      "epoch:5 step:4977 [D loss: 0.517916, acc.: 77.34%] [G loss: 0.593148]\n",
      "epoch:5 step:4978 [D loss: 0.537607, acc.: 73.44%] [G loss: 0.487472]\n",
      "epoch:5 step:4979 [D loss: 0.605158, acc.: 67.19%] [G loss: 0.488699]\n",
      "epoch:5 step:4980 [D loss: 0.565158, acc.: 70.31%] [G loss: 0.360659]\n",
      "epoch:5 step:4981 [D loss: 0.495302, acc.: 77.34%] [G loss: 0.479335]\n",
      "epoch:5 step:4982 [D loss: 0.523961, acc.: 78.91%] [G loss: 0.548186]\n",
      "epoch:5 step:4983 [D loss: 0.507419, acc.: 75.78%] [G loss: 0.484301]\n",
      "epoch:5 step:4984 [D loss: 0.549248, acc.: 75.00%] [G loss: 0.589225]\n",
      "epoch:5 step:4985 [D loss: 0.481157, acc.: 80.47%] [G loss: 0.566923]\n",
      "epoch:5 step:4986 [D loss: 0.635275, acc.: 64.06%] [G loss: 0.475485]\n",
      "epoch:5 step:4987 [D loss: 0.515415, acc.: 69.53%] [G loss: 0.586800]\n",
      "epoch:5 step:4988 [D loss: 0.544559, acc.: 69.53%] [G loss: 0.658460]\n",
      "epoch:5 step:4989 [D loss: 0.498001, acc.: 72.66%] [G loss: 0.546061]\n",
      "epoch:5 step:4990 [D loss: 0.473481, acc.: 79.69%] [G loss: 0.545639]\n",
      "epoch:5 step:4991 [D loss: 0.542029, acc.: 73.44%] [G loss: 0.543919]\n",
      "epoch:5 step:4992 [D loss: 0.570113, acc.: 74.22%] [G loss: 0.531748]\n",
      "epoch:5 step:4993 [D loss: 0.584507, acc.: 64.06%] [G loss: 0.549082]\n",
      "epoch:5 step:4994 [D loss: 0.475905, acc.: 75.00%] [G loss: 0.557255]\n",
      "epoch:5 step:4995 [D loss: 0.543288, acc.: 73.44%] [G loss: 0.544351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4996 [D loss: 0.518046, acc.: 68.75%] [G loss: 0.606255]\n",
      "epoch:5 step:4997 [D loss: 0.451534, acc.: 78.12%] [G loss: 0.686163]\n",
      "epoch:5 step:4998 [D loss: 0.515144, acc.: 72.66%] [G loss: 0.701053]\n",
      "epoch:5 step:4999 [D loss: 0.436548, acc.: 82.03%] [G loss: 0.699175]\n",
      "epoch:5 step:5000 [D loss: 0.429151, acc.: 80.47%] [G loss: 0.757905]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.877627\n",
      "FID: 47.590199\n",
      "0 = 12.787790679454805\n",
      "1 = 0.09441394936898477\n",
      "2 = 0.9315999746322632\n",
      "3 = 0.8668000102043152\n",
      "4 = 0.996399998664856\n",
      "5 = 0.9958639740943909\n",
      "6 = 0.8668000102043152\n",
      "7 = 8.479480159807222\n",
      "8 = 0.14119942223752177\n",
      "9 = 0.8090000152587891\n",
      "10 = 0.7839999794960022\n",
      "11 = 0.8339999914169312\n",
      "12 = 0.8252631425857544\n",
      "13 = 0.7839999794960022\n",
      "14 = 5.877645015716553\n",
      "15 = 7.950189113616943\n",
      "16 = 0.317531943321228\n",
      "17 = 5.877627372741699\n",
      "18 = 47.5901985168457\n",
      "epoch:5 step:5001 [D loss: 0.685846, acc.: 61.72%] [G loss: 0.555926]\n",
      "epoch:5 step:5002 [D loss: 0.616809, acc.: 64.84%] [G loss: 0.505640]\n",
      "epoch:5 step:5003 [D loss: 0.525596, acc.: 75.00%] [G loss: 0.527214]\n",
      "epoch:5 step:5004 [D loss: 0.524724, acc.: 72.66%] [G loss: 0.593576]\n",
      "epoch:5 step:5005 [D loss: 0.578161, acc.: 70.31%] [G loss: 0.504913]\n",
      "epoch:5 step:5006 [D loss: 0.551547, acc.: 75.00%] [G loss: 0.639638]\n",
      "epoch:5 step:5007 [D loss: 0.617454, acc.: 60.94%] [G loss: 0.595865]\n",
      "epoch:5 step:5008 [D loss: 0.615419, acc.: 62.50%] [G loss: 0.560621]\n",
      "epoch:5 step:5009 [D loss: 0.594220, acc.: 68.75%] [G loss: 0.442411]\n",
      "epoch:5 step:5010 [D loss: 0.556655, acc.: 71.88%] [G loss: 0.478894]\n",
      "epoch:5 step:5011 [D loss: 0.498582, acc.: 81.25%] [G loss: 0.555737]\n",
      "epoch:5 step:5012 [D loss: 0.534530, acc.: 72.66%] [G loss: 0.700607]\n",
      "epoch:5 step:5013 [D loss: 0.524714, acc.: 73.44%] [G loss: 0.554301]\n",
      "epoch:5 step:5014 [D loss: 0.536197, acc.: 69.53%] [G loss: 0.541463]\n",
      "epoch:5 step:5015 [D loss: 0.556721, acc.: 67.19%] [G loss: 0.517990]\n",
      "epoch:5 step:5016 [D loss: 0.535163, acc.: 73.44%] [G loss: 0.543279]\n",
      "epoch:5 step:5017 [D loss: 0.567122, acc.: 71.09%] [G loss: 0.441148]\n",
      "epoch:5 step:5018 [D loss: 0.496350, acc.: 75.00%] [G loss: 0.559339]\n",
      "epoch:5 step:5019 [D loss: 0.529630, acc.: 75.00%] [G loss: 0.616266]\n",
      "epoch:5 step:5020 [D loss: 0.557121, acc.: 68.75%] [G loss: 0.650815]\n",
      "epoch:5 step:5021 [D loss: 0.543152, acc.: 69.53%] [G loss: 0.504806]\n",
      "epoch:5 step:5022 [D loss: 0.486300, acc.: 76.56%] [G loss: 0.640101]\n",
      "epoch:5 step:5023 [D loss: 0.526599, acc.: 71.09%] [G loss: 0.658784]\n",
      "epoch:5 step:5024 [D loss: 0.525807, acc.: 71.88%] [G loss: 0.517230]\n",
      "epoch:5 step:5025 [D loss: 0.526973, acc.: 71.09%] [G loss: 0.600689]\n",
      "epoch:5 step:5026 [D loss: 0.595860, acc.: 66.41%] [G loss: 0.472007]\n",
      "epoch:5 step:5027 [D loss: 0.606920, acc.: 64.06%] [G loss: 0.654072]\n",
      "epoch:5 step:5028 [D loss: 0.494680, acc.: 73.44%] [G loss: 0.637555]\n",
      "epoch:5 step:5029 [D loss: 0.463870, acc.: 80.47%] [G loss: 0.688750]\n",
      "epoch:5 step:5030 [D loss: 0.522333, acc.: 72.66%] [G loss: 0.615424]\n",
      "epoch:5 step:5031 [D loss: 0.506386, acc.: 74.22%] [G loss: 0.732077]\n",
      "epoch:5 step:5032 [D loss: 0.446548, acc.: 77.34%] [G loss: 0.772277]\n",
      "epoch:5 step:5033 [D loss: 0.662029, acc.: 65.62%] [G loss: 0.551863]\n",
      "epoch:5 step:5034 [D loss: 0.699456, acc.: 54.69%] [G loss: 0.423012]\n",
      "epoch:5 step:5035 [D loss: 0.472456, acc.: 79.69%] [G loss: 0.553436]\n",
      "epoch:5 step:5036 [D loss: 0.554815, acc.: 71.09%] [G loss: 0.672688]\n",
      "epoch:5 step:5037 [D loss: 0.631999, acc.: 63.28%] [G loss: 0.542199]\n",
      "epoch:5 step:5038 [D loss: 0.545677, acc.: 71.09%] [G loss: 0.647263]\n",
      "epoch:5 step:5039 [D loss: 0.450040, acc.: 78.12%] [G loss: 0.631099]\n",
      "epoch:5 step:5040 [D loss: 0.563743, acc.: 65.62%] [G loss: 0.617060]\n",
      "epoch:5 step:5041 [D loss: 0.544262, acc.: 71.09%] [G loss: 0.607780]\n",
      "epoch:5 step:5042 [D loss: 0.496414, acc.: 78.91%] [G loss: 0.557558]\n",
      "epoch:5 step:5043 [D loss: 0.479321, acc.: 75.78%] [G loss: 0.742280]\n",
      "epoch:5 step:5044 [D loss: 0.499079, acc.: 76.56%] [G loss: 0.681405]\n",
      "epoch:5 step:5045 [D loss: 0.490024, acc.: 75.00%] [G loss: 0.651905]\n",
      "epoch:5 step:5046 [D loss: 0.535050, acc.: 74.22%] [G loss: 0.669721]\n",
      "epoch:5 step:5047 [D loss: 0.525879, acc.: 72.66%] [G loss: 0.696221]\n",
      "epoch:5 step:5048 [D loss: 0.551049, acc.: 73.44%] [G loss: 0.541018]\n",
      "epoch:5 step:5049 [D loss: 0.495389, acc.: 75.78%] [G loss: 0.610680]\n",
      "epoch:5 step:5050 [D loss: 0.569835, acc.: 64.06%] [G loss: 0.511703]\n",
      "epoch:5 step:5051 [D loss: 0.489035, acc.: 77.34%] [G loss: 0.636653]\n",
      "epoch:5 step:5052 [D loss: 0.569467, acc.: 69.53%] [G loss: 0.573566]\n",
      "epoch:5 step:5053 [D loss: 0.536020, acc.: 70.31%] [G loss: 0.636365]\n",
      "epoch:5 step:5054 [D loss: 0.542231, acc.: 76.56%] [G loss: 0.527201]\n",
      "epoch:5 step:5055 [D loss: 0.550394, acc.: 76.56%] [G loss: 0.459985]\n",
      "epoch:5 step:5056 [D loss: 0.536441, acc.: 72.66%] [G loss: 0.519527]\n",
      "epoch:5 step:5057 [D loss: 0.545244, acc.: 74.22%] [G loss: 0.545830]\n",
      "epoch:5 step:5058 [D loss: 0.599922, acc.: 69.53%] [G loss: 0.589458]\n",
      "epoch:5 step:5059 [D loss: 0.508114, acc.: 71.88%] [G loss: 0.691166]\n",
      "epoch:5 step:5060 [D loss: 0.602821, acc.: 64.84%] [G loss: 0.508010]\n",
      "epoch:5 step:5061 [D loss: 0.672559, acc.: 57.81%] [G loss: 0.427145]\n",
      "epoch:5 step:5062 [D loss: 0.624719, acc.: 67.97%] [G loss: 0.355126]\n",
      "epoch:5 step:5063 [D loss: 0.521152, acc.: 76.56%] [G loss: 0.412030]\n",
      "epoch:5 step:5064 [D loss: 0.605923, acc.: 62.50%] [G loss: 0.464497]\n",
      "epoch:5 step:5065 [D loss: 0.581524, acc.: 67.19%] [G loss: 0.404458]\n",
      "epoch:5 step:5066 [D loss: 0.508129, acc.: 75.00%] [G loss: 0.454192]\n",
      "epoch:5 step:5067 [D loss: 0.522461, acc.: 75.00%] [G loss: 0.544711]\n",
      "epoch:5 step:5068 [D loss: 0.547789, acc.: 71.09%] [G loss: 0.541180]\n",
      "epoch:5 step:5069 [D loss: 0.530005, acc.: 68.75%] [G loss: 0.462069]\n",
      "epoch:5 step:5070 [D loss: 0.508859, acc.: 78.12%] [G loss: 0.591153]\n",
      "epoch:5 step:5071 [D loss: 0.605353, acc.: 60.16%] [G loss: 0.479051]\n",
      "epoch:5 step:5072 [D loss: 0.559644, acc.: 67.97%] [G loss: 0.524088]\n",
      "epoch:5 step:5073 [D loss: 0.567502, acc.: 71.09%] [G loss: 0.680729]\n",
      "epoch:5 step:5074 [D loss: 0.604550, acc.: 64.84%] [G loss: 0.565395]\n",
      "epoch:5 step:5075 [D loss: 0.569438, acc.: 72.66%] [G loss: 0.528746]\n",
      "epoch:5 step:5076 [D loss: 0.568820, acc.: 69.53%] [G loss: 0.424746]\n",
      "epoch:5 step:5077 [D loss: 0.480715, acc.: 76.56%] [G loss: 0.500446]\n",
      "epoch:5 step:5078 [D loss: 0.560583, acc.: 69.53%] [G loss: 0.444489]\n",
      "epoch:5 step:5079 [D loss: 0.529798, acc.: 75.78%] [G loss: 0.554393]\n",
      "epoch:5 step:5080 [D loss: 0.526230, acc.: 73.44%] [G loss: 0.450502]\n",
      "epoch:5 step:5081 [D loss: 0.625363, acc.: 64.84%] [G loss: 0.432511]\n",
      "epoch:5 step:5082 [D loss: 0.575307, acc.: 64.06%] [G loss: 0.546979]\n",
      "epoch:5 step:5083 [D loss: 0.521695, acc.: 70.31%] [G loss: 0.651927]\n",
      "epoch:5 step:5084 [D loss: 0.480758, acc.: 76.56%] [G loss: 0.763523]\n",
      "epoch:5 step:5085 [D loss: 0.641727, acc.: 63.28%] [G loss: 0.423674]\n",
      "epoch:5 step:5086 [D loss: 0.602055, acc.: 67.19%] [G loss: 0.539640]\n",
      "epoch:5 step:5087 [D loss: 0.494597, acc.: 75.78%] [G loss: 0.502508]\n",
      "epoch:5 step:5088 [D loss: 0.476573, acc.: 78.91%] [G loss: 0.534749]\n",
      "epoch:5 step:5089 [D loss: 0.633333, acc.: 63.28%] [G loss: 0.508332]\n",
      "epoch:5 step:5090 [D loss: 0.572773, acc.: 68.75%] [G loss: 0.693062]\n",
      "epoch:5 step:5091 [D loss: 0.459404, acc.: 77.34%] [G loss: 0.652604]\n",
      "epoch:5 step:5092 [D loss: 0.603475, acc.: 65.62%] [G loss: 0.590524]\n",
      "epoch:5 step:5093 [D loss: 0.564336, acc.: 75.00%] [G loss: 0.499082]\n",
      "epoch:5 step:5094 [D loss: 0.541665, acc.: 72.66%] [G loss: 0.486823]\n",
      "epoch:5 step:5095 [D loss: 0.589300, acc.: 63.28%] [G loss: 0.405304]\n",
      "epoch:5 step:5096 [D loss: 0.571133, acc.: 67.97%] [G loss: 0.481443]\n",
      "epoch:5 step:5097 [D loss: 0.607816, acc.: 66.41%] [G loss: 0.388482]\n",
      "epoch:5 step:5098 [D loss: 0.565396, acc.: 69.53%] [G loss: 0.444684]\n",
      "epoch:5 step:5099 [D loss: 0.556956, acc.: 70.31%] [G loss: 0.390352]\n",
      "epoch:5 step:5100 [D loss: 0.500796, acc.: 79.69%] [G loss: 0.632206]\n",
      "epoch:5 step:5101 [D loss: 0.518432, acc.: 72.66%] [G loss: 0.522456]\n",
      "epoch:5 step:5102 [D loss: 0.600565, acc.: 68.75%] [G loss: 0.568644]\n",
      "epoch:5 step:5103 [D loss: 0.664194, acc.: 57.03%] [G loss: 0.382911]\n",
      "epoch:5 step:5104 [D loss: 0.579002, acc.: 65.62%] [G loss: 0.391172]\n",
      "epoch:5 step:5105 [D loss: 0.538217, acc.: 71.09%] [G loss: 0.563007]\n",
      "epoch:5 step:5106 [D loss: 0.547986, acc.: 73.44%] [G loss: 0.474324]\n",
      "epoch:5 step:5107 [D loss: 0.606402, acc.: 66.41%] [G loss: 0.483706]\n",
      "epoch:5 step:5108 [D loss: 0.583303, acc.: 69.53%] [G loss: 0.569982]\n",
      "epoch:5 step:5109 [D loss: 0.559731, acc.: 74.22%] [G loss: 0.435756]\n",
      "epoch:5 step:5110 [D loss: 0.509403, acc.: 75.00%] [G loss: 0.563584]\n",
      "epoch:5 step:5111 [D loss: 0.480611, acc.: 79.69%] [G loss: 0.524697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5112 [D loss: 0.441268, acc.: 80.47%] [G loss: 0.543538]\n",
      "epoch:5 step:5113 [D loss: 0.478227, acc.: 76.56%] [G loss: 0.589234]\n",
      "epoch:5 step:5114 [D loss: 0.448473, acc.: 82.03%] [G loss: 0.861713]\n",
      "epoch:5 step:5115 [D loss: 0.510489, acc.: 72.66%] [G loss: 0.712069]\n",
      "epoch:5 step:5116 [D loss: 0.524627, acc.: 73.44%] [G loss: 0.701820]\n",
      "epoch:5 step:5117 [D loss: 0.573423, acc.: 69.53%] [G loss: 0.570159]\n",
      "epoch:5 step:5118 [D loss: 0.592558, acc.: 64.84%] [G loss: 0.500785]\n",
      "epoch:5 step:5119 [D loss: 0.488411, acc.: 81.25%] [G loss: 0.512627]\n",
      "epoch:5 step:5120 [D loss: 0.622115, acc.: 70.31%] [G loss: 0.579811]\n",
      "epoch:5 step:5121 [D loss: 0.482123, acc.: 81.25%] [G loss: 0.625678]\n",
      "epoch:5 step:5122 [D loss: 0.641051, acc.: 68.75%] [G loss: 0.559138]\n",
      "epoch:5 step:5123 [D loss: 0.569419, acc.: 66.41%] [G loss: 0.530781]\n",
      "epoch:5 step:5124 [D loss: 0.502028, acc.: 77.34%] [G loss: 0.538063]\n",
      "epoch:5 step:5125 [D loss: 0.549243, acc.: 70.31%] [G loss: 0.436999]\n",
      "epoch:5 step:5126 [D loss: 0.574881, acc.: 67.19%] [G loss: 0.513681]\n",
      "epoch:5 step:5127 [D loss: 0.555850, acc.: 74.22%] [G loss: 0.463907]\n",
      "epoch:5 step:5128 [D loss: 0.547253, acc.: 71.88%] [G loss: 0.453348]\n",
      "epoch:5 step:5129 [D loss: 0.502924, acc.: 76.56%] [G loss: 0.505611]\n",
      "epoch:5 step:5130 [D loss: 0.558023, acc.: 70.31%] [G loss: 0.530962]\n",
      "epoch:5 step:5131 [D loss: 0.562230, acc.: 67.19%] [G loss: 0.556928]\n",
      "epoch:5 step:5132 [D loss: 0.530352, acc.: 68.75%] [G loss: 0.599557]\n",
      "epoch:5 step:5133 [D loss: 0.541398, acc.: 67.19%] [G loss: 0.589738]\n",
      "epoch:5 step:5134 [D loss: 0.497558, acc.: 73.44%] [G loss: 0.637957]\n",
      "epoch:5 step:5135 [D loss: 0.487523, acc.: 81.25%] [G loss: 0.655488]\n",
      "epoch:5 step:5136 [D loss: 0.457624, acc.: 78.12%] [G loss: 0.702860]\n",
      "epoch:5 step:5137 [D loss: 0.519658, acc.: 74.22%] [G loss: 0.653502]\n",
      "epoch:5 step:5138 [D loss: 0.539682, acc.: 70.31%] [G loss: 0.526662]\n",
      "epoch:5 step:5139 [D loss: 0.581455, acc.: 66.41%] [G loss: 0.549041]\n",
      "epoch:5 step:5140 [D loss: 0.599795, acc.: 64.84%] [G loss: 0.463319]\n",
      "epoch:5 step:5141 [D loss: 0.624017, acc.: 63.28%] [G loss: 0.534998]\n",
      "epoch:5 step:5142 [D loss: 0.523791, acc.: 75.00%] [G loss: 0.557212]\n",
      "epoch:5 step:5143 [D loss: 0.587201, acc.: 69.53%] [G loss: 0.486198]\n",
      "epoch:5 step:5144 [D loss: 0.553721, acc.: 72.66%] [G loss: 0.496551]\n",
      "epoch:5 step:5145 [D loss: 0.520017, acc.: 78.12%] [G loss: 0.519145]\n",
      "epoch:5 step:5146 [D loss: 0.534368, acc.: 74.22%] [G loss: 0.475958]\n",
      "epoch:5 step:5147 [D loss: 0.582251, acc.: 66.41%] [G loss: 0.516083]\n",
      "epoch:5 step:5148 [D loss: 0.579534, acc.: 65.62%] [G loss: 0.457594]\n",
      "epoch:5 step:5149 [D loss: 0.530028, acc.: 76.56%] [G loss: 0.542661]\n",
      "epoch:5 step:5150 [D loss: 0.618335, acc.: 63.28%] [G loss: 0.435357]\n",
      "epoch:5 step:5151 [D loss: 0.561735, acc.: 71.09%] [G loss: 0.542062]\n",
      "epoch:5 step:5152 [D loss: 0.489496, acc.: 76.56%] [G loss: 0.542247]\n",
      "epoch:5 step:5153 [D loss: 0.560222, acc.: 72.66%] [G loss: 0.641357]\n",
      "epoch:5 step:5154 [D loss: 0.527014, acc.: 73.44%] [G loss: 0.581544]\n",
      "epoch:5 step:5155 [D loss: 0.544088, acc.: 71.88%] [G loss: 0.629129]\n",
      "epoch:5 step:5156 [D loss: 0.530458, acc.: 72.66%] [G loss: 0.573570]\n",
      "epoch:5 step:5157 [D loss: 0.474237, acc.: 76.56%] [G loss: 0.802640]\n",
      "epoch:5 step:5158 [D loss: 0.610080, acc.: 67.19%] [G loss: 0.573415]\n",
      "epoch:5 step:5159 [D loss: 0.549852, acc.: 73.44%] [G loss: 0.518789]\n",
      "epoch:5 step:5160 [D loss: 0.430446, acc.: 82.81%] [G loss: 0.723180]\n",
      "epoch:5 step:5161 [D loss: 0.578009, acc.: 68.75%] [G loss: 0.543115]\n",
      "epoch:5 step:5162 [D loss: 0.713166, acc.: 61.72%] [G loss: 0.429848]\n",
      "epoch:5 step:5163 [D loss: 0.588235, acc.: 67.97%] [G loss: 0.420579]\n",
      "epoch:5 step:5164 [D loss: 0.543930, acc.: 74.22%] [G loss: 0.460977]\n",
      "epoch:5 step:5165 [D loss: 0.588584, acc.: 71.09%] [G loss: 0.545584]\n",
      "epoch:5 step:5166 [D loss: 0.536823, acc.: 76.56%] [G loss: 0.548804]\n",
      "epoch:5 step:5167 [D loss: 0.654960, acc.: 63.28%] [G loss: 0.499882]\n",
      "epoch:5 step:5168 [D loss: 0.602963, acc.: 64.06%] [G loss: 0.435111]\n",
      "epoch:5 step:5169 [D loss: 0.539183, acc.: 73.44%] [G loss: 0.582430]\n",
      "epoch:5 step:5170 [D loss: 0.575155, acc.: 64.84%] [G loss: 0.460940]\n",
      "epoch:5 step:5171 [D loss: 0.593233, acc.: 62.50%] [G loss: 0.533837]\n",
      "epoch:5 step:5172 [D loss: 0.534383, acc.: 73.44%] [G loss: 0.539496]\n",
      "epoch:5 step:5173 [D loss: 0.497432, acc.: 75.78%] [G loss: 0.522722]\n",
      "epoch:5 step:5174 [D loss: 0.586290, acc.: 70.31%] [G loss: 0.480687]\n",
      "epoch:5 step:5175 [D loss: 0.582858, acc.: 62.50%] [G loss: 0.403724]\n",
      "epoch:5 step:5176 [D loss: 0.513032, acc.: 75.00%] [G loss: 0.607955]\n",
      "epoch:5 step:5177 [D loss: 0.585926, acc.: 68.75%] [G loss: 0.415295]\n",
      "epoch:5 step:5178 [D loss: 0.593880, acc.: 64.06%] [G loss: 0.422281]\n",
      "epoch:5 step:5179 [D loss: 0.531064, acc.: 75.00%] [G loss: 0.453274]\n",
      "epoch:5 step:5180 [D loss: 0.530500, acc.: 71.88%] [G loss: 0.480895]\n",
      "epoch:5 step:5181 [D loss: 0.547890, acc.: 71.88%] [G loss: 0.524272]\n",
      "epoch:5 step:5182 [D loss: 0.544965, acc.: 73.44%] [G loss: 0.520112]\n",
      "epoch:5 step:5183 [D loss: 0.466941, acc.: 82.03%] [G loss: 0.606576]\n",
      "epoch:5 step:5184 [D loss: 0.467872, acc.: 78.91%] [G loss: 0.641042]\n",
      "epoch:5 step:5185 [D loss: 0.679455, acc.: 62.50%] [G loss: 0.514096]\n",
      "epoch:5 step:5186 [D loss: 0.662516, acc.: 57.81%] [G loss: 0.391846]\n",
      "epoch:5 step:5187 [D loss: 0.562157, acc.: 71.88%] [G loss: 0.423374]\n",
      "epoch:5 step:5188 [D loss: 0.500572, acc.: 71.88%] [G loss: 0.584676]\n",
      "epoch:5 step:5189 [D loss: 0.482790, acc.: 76.56%] [G loss: 0.664434]\n",
      "epoch:5 step:5190 [D loss: 0.601225, acc.: 66.41%] [G loss: 0.555776]\n",
      "epoch:5 step:5191 [D loss: 0.565824, acc.: 67.97%] [G loss: 0.597936]\n",
      "epoch:5 step:5192 [D loss: 0.523517, acc.: 74.22%] [G loss: 0.613854]\n",
      "epoch:5 step:5193 [D loss: 0.417812, acc.: 83.59%] [G loss: 0.576465]\n",
      "epoch:5 step:5194 [D loss: 0.566817, acc.: 67.97%] [G loss: 0.570761]\n",
      "epoch:5 step:5195 [D loss: 0.616361, acc.: 66.41%] [G loss: 0.450162]\n",
      "epoch:5 step:5196 [D loss: 0.677838, acc.: 54.69%] [G loss: 0.468213]\n",
      "epoch:5 step:5197 [D loss: 0.599185, acc.: 66.41%] [G loss: 0.343109]\n",
      "epoch:5 step:5198 [D loss: 0.576265, acc.: 67.19%] [G loss: 0.384220]\n",
      "epoch:5 step:5199 [D loss: 0.514131, acc.: 71.09%] [G loss: 0.570321]\n",
      "epoch:5 step:5200 [D loss: 0.536952, acc.: 73.44%] [G loss: 0.532356]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.072478\n",
      "FID: 44.067329\n",
      "0 = 12.882863899612452\n",
      "1 = 0.0975475469221162\n",
      "2 = 0.9355000257492065\n",
      "3 = 0.8730000257492065\n",
      "4 = 0.9980000257492065\n",
      "5 = 0.9977142810821533\n",
      "6 = 0.8730000257492065\n",
      "7 = 8.27960537674428\n",
      "8 = 0.13339062691606204\n",
      "9 = 0.8137999773025513\n",
      "10 = 0.7882000207901001\n",
      "11 = 0.8393999934196472\n",
      "12 = 0.8307335376739502\n",
      "13 = 0.7882000207901001\n",
      "14 = 6.0725016593933105\n",
      "15 = 7.9338836669921875\n",
      "16 = 0.3075351119041443\n",
      "17 = 6.0724778175354\n",
      "18 = 44.06732940673828\n",
      "epoch:5 step:5201 [D loss: 0.505629, acc.: 82.03%] [G loss: 0.515909]\n",
      "epoch:5 step:5202 [D loss: 0.567196, acc.: 69.53%] [G loss: 0.540290]\n",
      "epoch:5 step:5203 [D loss: 0.506233, acc.: 75.78%] [G loss: 0.449912]\n",
      "epoch:5 step:5204 [D loss: 0.548147, acc.: 71.88%] [G loss: 0.532965]\n",
      "epoch:5 step:5205 [D loss: 0.543163, acc.: 71.09%] [G loss: 0.469862]\n",
      "epoch:5 step:5206 [D loss: 0.512878, acc.: 81.25%] [G loss: 0.589472]\n",
      "epoch:5 step:5207 [D loss: 0.549285, acc.: 69.53%] [G loss: 0.502379]\n",
      "epoch:5 step:5208 [D loss: 0.529807, acc.: 72.66%] [G loss: 0.559726]\n",
      "epoch:5 step:5209 [D loss: 0.615085, acc.: 63.28%] [G loss: 0.450344]\n",
      "epoch:5 step:5210 [D loss: 0.567692, acc.: 66.41%] [G loss: 0.472454]\n",
      "epoch:5 step:5211 [D loss: 0.499106, acc.: 73.44%] [G loss: 0.440918]\n",
      "epoch:5 step:5212 [D loss: 0.576370, acc.: 72.66%] [G loss: 0.452983]\n",
      "epoch:5 step:5213 [D loss: 0.592771, acc.: 67.19%] [G loss: 0.433794]\n",
      "epoch:5 step:5214 [D loss: 0.570624, acc.: 74.22%] [G loss: 0.374962]\n",
      "epoch:5 step:5215 [D loss: 0.516896, acc.: 73.44%] [G loss: 0.435822]\n",
      "epoch:5 step:5216 [D loss: 0.600814, acc.: 67.97%] [G loss: 0.381916]\n",
      "epoch:5 step:5217 [D loss: 0.557566, acc.: 73.44%] [G loss: 0.431058]\n",
      "epoch:5 step:5218 [D loss: 0.554460, acc.: 67.19%] [G loss: 0.543636]\n",
      "epoch:5 step:5219 [D loss: 0.500223, acc.: 76.56%] [G loss: 0.433196]\n",
      "epoch:5 step:5220 [D loss: 0.621779, acc.: 62.50%] [G loss: 0.425110]\n",
      "epoch:5 step:5221 [D loss: 0.502957, acc.: 75.78%] [G loss: 0.470424]\n",
      "epoch:5 step:5222 [D loss: 0.596724, acc.: 67.19%] [G loss: 0.481263]\n",
      "epoch:5 step:5223 [D loss: 0.604499, acc.: 69.53%] [G loss: 0.564797]\n",
      "epoch:5 step:5224 [D loss: 0.571004, acc.: 67.97%] [G loss: 0.572201]\n",
      "epoch:5 step:5225 [D loss: 0.599588, acc.: 67.19%] [G loss: 0.411525]\n",
      "epoch:5 step:5226 [D loss: 0.569327, acc.: 68.75%] [G loss: 0.514529]\n",
      "epoch:5 step:5227 [D loss: 0.559343, acc.: 70.31%] [G loss: 0.442377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5228 [D loss: 0.582988, acc.: 67.97%] [G loss: 0.479763]\n",
      "epoch:5 step:5229 [D loss: 0.526437, acc.: 75.78%] [G loss: 0.515503]\n",
      "epoch:5 step:5230 [D loss: 0.548163, acc.: 71.88%] [G loss: 0.538427]\n",
      "epoch:5 step:5231 [D loss: 0.486768, acc.: 75.78%] [G loss: 0.522694]\n",
      "epoch:5 step:5232 [D loss: 0.504527, acc.: 72.66%] [G loss: 0.525370]\n",
      "epoch:5 step:5233 [D loss: 0.488546, acc.: 74.22%] [G loss: 0.585572]\n",
      "epoch:5 step:5234 [D loss: 0.535559, acc.: 74.22%] [G loss: 0.514112]\n",
      "epoch:5 step:5235 [D loss: 0.549327, acc.: 75.00%] [G loss: 0.434842]\n",
      "epoch:5 step:5236 [D loss: 0.522724, acc.: 77.34%] [G loss: 0.561065]\n",
      "epoch:5 step:5237 [D loss: 0.510834, acc.: 74.22%] [G loss: 0.481219]\n",
      "epoch:5 step:5238 [D loss: 0.535977, acc.: 71.09%] [G loss: 0.501450]\n",
      "epoch:5 step:5239 [D loss: 0.467516, acc.: 76.56%] [G loss: 0.571905]\n",
      "epoch:5 step:5240 [D loss: 0.509978, acc.: 75.78%] [G loss: 0.569210]\n",
      "epoch:5 step:5241 [D loss: 0.489241, acc.: 77.34%] [G loss: 0.682381]\n",
      "epoch:5 step:5242 [D loss: 0.488175, acc.: 75.00%] [G loss: 0.544061]\n",
      "epoch:5 step:5243 [D loss: 0.480175, acc.: 75.78%] [G loss: 0.657723]\n",
      "epoch:5 step:5244 [D loss: 0.584099, acc.: 64.84%] [G loss: 0.560211]\n",
      "epoch:5 step:5245 [D loss: 0.582273, acc.: 70.31%] [G loss: 0.503039]\n",
      "epoch:5 step:5246 [D loss: 0.569092, acc.: 72.66%] [G loss: 0.488386]\n",
      "epoch:5 step:5247 [D loss: 0.634480, acc.: 64.84%] [G loss: 0.401050]\n",
      "epoch:5 step:5248 [D loss: 0.556194, acc.: 73.44%] [G loss: 0.400247]\n",
      "epoch:5 step:5249 [D loss: 0.533165, acc.: 75.00%] [G loss: 0.529958]\n",
      "epoch:5 step:5250 [D loss: 0.592377, acc.: 71.88%] [G loss: 0.486213]\n",
      "epoch:5 step:5251 [D loss: 0.632508, acc.: 65.62%] [G loss: 0.438613]\n",
      "epoch:5 step:5252 [D loss: 0.507370, acc.: 76.56%] [G loss: 0.497031]\n",
      "epoch:5 step:5253 [D loss: 0.515551, acc.: 73.44%] [G loss: 0.525358]\n",
      "epoch:5 step:5254 [D loss: 0.543035, acc.: 71.09%] [G loss: 0.566436]\n",
      "epoch:5 step:5255 [D loss: 0.564501, acc.: 75.78%] [G loss: 0.445009]\n",
      "epoch:5 step:5256 [D loss: 0.519357, acc.: 73.44%] [G loss: 0.558909]\n",
      "epoch:5 step:5257 [D loss: 0.591948, acc.: 72.66%] [G loss: 0.591107]\n",
      "epoch:5 step:5258 [D loss: 0.534416, acc.: 70.31%] [G loss: 0.538646]\n",
      "epoch:5 step:5259 [D loss: 0.499747, acc.: 77.34%] [G loss: 0.597182]\n",
      "epoch:5 step:5260 [D loss: 0.471358, acc.: 75.00%] [G loss: 0.764937]\n",
      "epoch:5 step:5261 [D loss: 0.569554, acc.: 67.97%] [G loss: 0.635604]\n",
      "epoch:5 step:5262 [D loss: 0.607214, acc.: 65.62%] [G loss: 0.510851]\n",
      "epoch:5 step:5263 [D loss: 0.561838, acc.: 71.09%] [G loss: 0.524971]\n",
      "epoch:5 step:5264 [D loss: 0.541123, acc.: 71.88%] [G loss: 0.406784]\n",
      "epoch:5 step:5265 [D loss: 0.581143, acc.: 70.31%] [G loss: 0.649798]\n",
      "epoch:5 step:5266 [D loss: 0.528355, acc.: 73.44%] [G loss: 0.567053]\n",
      "epoch:5 step:5267 [D loss: 0.516309, acc.: 71.88%] [G loss: 0.618134]\n",
      "epoch:5 step:5268 [D loss: 0.570424, acc.: 67.97%] [G loss: 0.508482]\n",
      "epoch:5 step:5269 [D loss: 0.607720, acc.: 63.28%] [G loss: 0.568068]\n",
      "epoch:5 step:5270 [D loss: 0.556689, acc.: 67.97%] [G loss: 0.513290]\n",
      "epoch:5 step:5271 [D loss: 0.612530, acc.: 65.62%] [G loss: 0.369962]\n",
      "epoch:5 step:5272 [D loss: 0.597399, acc.: 66.41%] [G loss: 0.407090]\n",
      "epoch:5 step:5273 [D loss: 0.579610, acc.: 67.19%] [G loss: 0.455383]\n",
      "epoch:5 step:5274 [D loss: 0.505871, acc.: 74.22%] [G loss: 0.744885]\n",
      "epoch:5 step:5275 [D loss: 0.579540, acc.: 67.19%] [G loss: 0.509854]\n",
      "epoch:5 step:5276 [D loss: 0.557827, acc.: 70.31%] [G loss: 0.508745]\n",
      "epoch:5 step:5277 [D loss: 0.478066, acc.: 80.47%] [G loss: 0.600538]\n",
      "epoch:5 step:5278 [D loss: 0.546874, acc.: 71.09%] [G loss: 0.549455]\n",
      "epoch:5 step:5279 [D loss: 0.542432, acc.: 69.53%] [G loss: 0.406120]\n",
      "epoch:5 step:5280 [D loss: 0.561641, acc.: 71.09%] [G loss: 0.370404]\n",
      "epoch:5 step:5281 [D loss: 0.562451, acc.: 75.00%] [G loss: 0.505055]\n",
      "epoch:5 step:5282 [D loss: 0.553127, acc.: 68.75%] [G loss: 0.525225]\n",
      "epoch:5 step:5283 [D loss: 0.485233, acc.: 78.12%] [G loss: 0.627904]\n",
      "epoch:5 step:5284 [D loss: 0.589761, acc.: 71.88%] [G loss: 0.522347]\n",
      "epoch:5 step:5285 [D loss: 0.641716, acc.: 61.72%] [G loss: 0.361109]\n",
      "epoch:5 step:5286 [D loss: 0.525946, acc.: 69.53%] [G loss: 0.461773]\n",
      "epoch:5 step:5287 [D loss: 0.509978, acc.: 74.22%] [G loss: 0.522850]\n",
      "epoch:5 step:5288 [D loss: 0.506475, acc.: 75.78%] [G loss: 0.571711]\n",
      "epoch:5 step:5289 [D loss: 0.549739, acc.: 71.09%] [G loss: 0.649133]\n",
      "epoch:5 step:5290 [D loss: 0.473522, acc.: 80.47%] [G loss: 0.621400]\n",
      "epoch:5 step:5291 [D loss: 0.627537, acc.: 64.84%] [G loss: 0.424329]\n",
      "epoch:5 step:5292 [D loss: 0.608237, acc.: 64.06%] [G loss: 0.473417]\n",
      "epoch:5 step:5293 [D loss: 0.537222, acc.: 71.09%] [G loss: 0.463146]\n",
      "epoch:5 step:5294 [D loss: 0.478907, acc.: 76.56%] [G loss: 0.471376]\n",
      "epoch:5 step:5295 [D loss: 0.591209, acc.: 64.06%] [G loss: 0.358263]\n",
      "epoch:5 step:5296 [D loss: 0.522120, acc.: 73.44%] [G loss: 0.478570]\n",
      "epoch:5 step:5297 [D loss: 0.557687, acc.: 71.88%] [G loss: 0.476620]\n",
      "epoch:5 step:5298 [D loss: 0.483205, acc.: 78.12%] [G loss: 0.501217]\n",
      "epoch:5 step:5299 [D loss: 0.585300, acc.: 66.41%] [G loss: 0.458588]\n",
      "epoch:5 step:5300 [D loss: 0.584093, acc.: 68.75%] [G loss: 0.662094]\n",
      "epoch:5 step:5301 [D loss: 0.563473, acc.: 67.97%] [G loss: 0.511696]\n",
      "epoch:5 step:5302 [D loss: 0.533878, acc.: 69.53%] [G loss: 0.574018]\n",
      "epoch:5 step:5303 [D loss: 0.505684, acc.: 76.56%] [G loss: 0.604199]\n",
      "epoch:5 step:5304 [D loss: 0.484781, acc.: 79.69%] [G loss: 0.618000]\n",
      "epoch:5 step:5305 [D loss: 0.521850, acc.: 75.78%] [G loss: 0.477613]\n",
      "epoch:5 step:5306 [D loss: 0.557758, acc.: 75.00%] [G loss: 0.503047]\n",
      "epoch:5 step:5307 [D loss: 0.607914, acc.: 66.41%] [G loss: 0.404765]\n",
      "epoch:5 step:5308 [D loss: 0.512339, acc.: 75.00%] [G loss: 0.422816]\n",
      "epoch:5 step:5309 [D loss: 0.517400, acc.: 72.66%] [G loss: 0.568371]\n",
      "epoch:5 step:5310 [D loss: 0.546998, acc.: 71.09%] [G loss: 0.539733]\n",
      "epoch:5 step:5311 [D loss: 0.491383, acc.: 73.44%] [G loss: 0.512647]\n",
      "epoch:5 step:5312 [D loss: 0.553276, acc.: 69.53%] [G loss: 0.463169]\n",
      "epoch:5 step:5313 [D loss: 0.584157, acc.: 63.28%] [G loss: 0.481171]\n",
      "epoch:5 step:5314 [D loss: 0.544073, acc.: 69.53%] [G loss: 0.476599]\n",
      "epoch:5 step:5315 [D loss: 0.551102, acc.: 73.44%] [G loss: 0.594138]\n",
      "epoch:5 step:5316 [D loss: 0.506053, acc.: 77.34%] [G loss: 0.465188]\n",
      "epoch:5 step:5317 [D loss: 0.512062, acc.: 75.78%] [G loss: 0.606369]\n",
      "epoch:5 step:5318 [D loss: 0.541826, acc.: 69.53%] [G loss: 0.540704]\n",
      "epoch:5 step:5319 [D loss: 0.503173, acc.: 77.34%] [G loss: 0.639723]\n",
      "epoch:5 step:5320 [D loss: 0.517133, acc.: 75.00%] [G loss: 0.584874]\n",
      "epoch:5 step:5321 [D loss: 0.610321, acc.: 63.28%] [G loss: 0.511068]\n",
      "epoch:5 step:5322 [D loss: 0.548607, acc.: 76.56%] [G loss: 0.516062]\n",
      "epoch:5 step:5323 [D loss: 0.525749, acc.: 74.22%] [G loss: 0.372988]\n",
      "epoch:5 step:5324 [D loss: 0.471085, acc.: 78.91%] [G loss: 0.531424]\n",
      "epoch:5 step:5325 [D loss: 0.501461, acc.: 71.88%] [G loss: 0.585742]\n",
      "epoch:5 step:5326 [D loss: 0.448331, acc.: 81.25%] [G loss: 0.713135]\n",
      "epoch:5 step:5327 [D loss: 0.498772, acc.: 76.56%] [G loss: 0.664741]\n",
      "epoch:5 step:5328 [D loss: 0.561586, acc.: 67.97%] [G loss: 0.851040]\n",
      "epoch:5 step:5329 [D loss: 0.577833, acc.: 69.53%] [G loss: 0.544863]\n",
      "epoch:5 step:5330 [D loss: 0.607354, acc.: 65.62%] [G loss: 0.577240]\n",
      "epoch:5 step:5331 [D loss: 0.573743, acc.: 70.31%] [G loss: 0.659724]\n",
      "epoch:5 step:5332 [D loss: 0.459779, acc.: 82.03%] [G loss: 0.638117]\n",
      "epoch:5 step:5333 [D loss: 0.442841, acc.: 82.03%] [G loss: 0.667230]\n",
      "epoch:5 step:5334 [D loss: 0.532636, acc.: 73.44%] [G loss: 0.672505]\n",
      "epoch:5 step:5335 [D loss: 0.509279, acc.: 78.12%] [G loss: 0.623734]\n",
      "epoch:5 step:5336 [D loss: 0.514069, acc.: 75.00%] [G loss: 0.656838]\n",
      "epoch:5 step:5337 [D loss: 0.592006, acc.: 68.75%] [G loss: 0.605510]\n",
      "epoch:5 step:5338 [D loss: 0.625103, acc.: 62.50%] [G loss: 0.373592]\n",
      "epoch:5 step:5339 [D loss: 0.543301, acc.: 71.88%] [G loss: 0.415006]\n",
      "epoch:5 step:5340 [D loss: 0.578524, acc.: 73.44%] [G loss: 0.389063]\n",
      "epoch:5 step:5341 [D loss: 0.518393, acc.: 77.34%] [G loss: 0.449709]\n",
      "epoch:5 step:5342 [D loss: 0.507082, acc.: 75.00%] [G loss: 0.502636]\n",
      "epoch:5 step:5343 [D loss: 0.580503, acc.: 70.31%] [G loss: 0.543261]\n",
      "epoch:5 step:5344 [D loss: 0.572503, acc.: 70.31%] [G loss: 0.472889]\n",
      "epoch:5 step:5345 [D loss: 0.544137, acc.: 76.56%] [G loss: 0.473996]\n",
      "epoch:5 step:5346 [D loss: 0.517842, acc.: 75.00%] [G loss: 0.586199]\n",
      "epoch:5 step:5347 [D loss: 0.514790, acc.: 76.56%] [G loss: 0.628212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5348 [D loss: 0.579577, acc.: 70.31%] [G loss: 0.529746]\n",
      "epoch:5 step:5349 [D loss: 0.574712, acc.: 64.06%] [G loss: 0.609548]\n",
      "epoch:5 step:5350 [D loss: 0.513860, acc.: 75.00%] [G loss: 0.663778]\n",
      "epoch:5 step:5351 [D loss: 0.623169, acc.: 64.06%] [G loss: 0.612934]\n",
      "epoch:5 step:5352 [D loss: 0.574642, acc.: 67.19%] [G loss: 0.536810]\n",
      "epoch:5 step:5353 [D loss: 0.596689, acc.: 69.53%] [G loss: 0.394608]\n",
      "epoch:5 step:5354 [D loss: 0.523062, acc.: 71.09%] [G loss: 0.494990]\n",
      "epoch:5 step:5355 [D loss: 0.541580, acc.: 70.31%] [G loss: 0.517628]\n",
      "epoch:5 step:5356 [D loss: 0.582161, acc.: 67.19%] [G loss: 0.492439]\n",
      "epoch:5 step:5357 [D loss: 0.621306, acc.: 64.84%] [G loss: 0.450477]\n",
      "epoch:5 step:5358 [D loss: 0.576290, acc.: 71.88%] [G loss: 0.498514]\n",
      "epoch:5 step:5359 [D loss: 0.565337, acc.: 71.88%] [G loss: 0.434040]\n",
      "epoch:5 step:5360 [D loss: 0.574196, acc.: 67.97%] [G loss: 0.526723]\n",
      "epoch:5 step:5361 [D loss: 0.560925, acc.: 68.75%] [G loss: 0.560237]\n",
      "epoch:5 step:5362 [D loss: 0.518715, acc.: 71.88%] [G loss: 0.473640]\n",
      "epoch:5 step:5363 [D loss: 0.516459, acc.: 71.09%] [G loss: 0.599992]\n",
      "epoch:5 step:5364 [D loss: 0.542583, acc.: 75.78%] [G loss: 0.582839]\n",
      "epoch:5 step:5365 [D loss: 0.514964, acc.: 77.34%] [G loss: 0.662959]\n",
      "epoch:5 step:5366 [D loss: 0.520468, acc.: 74.22%] [G loss: 0.584866]\n",
      "epoch:5 step:5367 [D loss: 0.515403, acc.: 78.91%] [G loss: 0.476744]\n",
      "epoch:5 step:5368 [D loss: 0.554432, acc.: 74.22%] [G loss: 0.498828]\n",
      "epoch:5 step:5369 [D loss: 0.532219, acc.: 74.22%] [G loss: 0.577632]\n",
      "epoch:5 step:5370 [D loss: 0.566967, acc.: 67.19%] [G loss: 0.540822]\n",
      "epoch:5 step:5371 [D loss: 0.560958, acc.: 69.53%] [G loss: 0.514122]\n",
      "epoch:5 step:5372 [D loss: 0.546508, acc.: 75.00%] [G loss: 0.455678]\n",
      "epoch:5 step:5373 [D loss: 0.553467, acc.: 71.88%] [G loss: 0.486644]\n",
      "epoch:5 step:5374 [D loss: 0.554208, acc.: 66.41%] [G loss: 0.679166]\n",
      "epoch:5 step:5375 [D loss: 0.535780, acc.: 73.44%] [G loss: 0.548365]\n",
      "epoch:5 step:5376 [D loss: 0.470949, acc.: 78.91%] [G loss: 0.712206]\n",
      "epoch:5 step:5377 [D loss: 0.548513, acc.: 77.34%] [G loss: 0.577189]\n",
      "epoch:5 step:5378 [D loss: 0.560062, acc.: 67.19%] [G loss: 0.536757]\n",
      "epoch:5 step:5379 [D loss: 0.511306, acc.: 77.34%] [G loss: 0.507933]\n",
      "epoch:5 step:5380 [D loss: 0.521208, acc.: 75.78%] [G loss: 0.640038]\n",
      "epoch:5 step:5381 [D loss: 0.591951, acc.: 67.19%] [G loss: 0.544703]\n",
      "epoch:5 step:5382 [D loss: 0.548105, acc.: 70.31%] [G loss: 0.722779]\n",
      "epoch:5 step:5383 [D loss: 0.507783, acc.: 70.31%] [G loss: 0.641017]\n",
      "epoch:5 step:5384 [D loss: 0.550163, acc.: 71.09%] [G loss: 0.546283]\n",
      "epoch:5 step:5385 [D loss: 0.492006, acc.: 75.78%] [G loss: 0.741515]\n",
      "epoch:5 step:5386 [D loss: 0.549139, acc.: 72.66%] [G loss: 0.595036]\n",
      "epoch:5 step:5387 [D loss: 0.604624, acc.: 62.50%] [G loss: 0.454554]\n",
      "epoch:5 step:5388 [D loss: 0.603927, acc.: 65.62%] [G loss: 0.440271]\n",
      "epoch:5 step:5389 [D loss: 0.589407, acc.: 70.31%] [G loss: 0.552042]\n",
      "epoch:5 step:5390 [D loss: 0.543420, acc.: 71.88%] [G loss: 0.424928]\n",
      "epoch:5 step:5391 [D loss: 0.534603, acc.: 73.44%] [G loss: 0.453916]\n",
      "epoch:5 step:5392 [D loss: 0.447578, acc.: 85.16%] [G loss: 0.671139]\n",
      "epoch:5 step:5393 [D loss: 0.516323, acc.: 75.78%] [G loss: 0.584008]\n",
      "epoch:5 step:5394 [D loss: 0.528476, acc.: 73.44%] [G loss: 0.685285]\n",
      "epoch:5 step:5395 [D loss: 0.648626, acc.: 65.62%] [G loss: 0.586580]\n",
      "epoch:5 step:5396 [D loss: 0.601867, acc.: 68.75%] [G loss: 0.462393]\n",
      "epoch:5 step:5397 [D loss: 0.548107, acc.: 67.97%] [G loss: 0.516743]\n",
      "epoch:5 step:5398 [D loss: 0.663728, acc.: 60.94%] [G loss: 0.417548]\n",
      "epoch:5 step:5399 [D loss: 0.539083, acc.: 69.53%] [G loss: 0.504990]\n",
      "epoch:5 step:5400 [D loss: 0.500313, acc.: 75.78%] [G loss: 0.602088]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.871399\n",
      "FID: 55.118130\n",
      "0 = 12.9452243801117\n",
      "1 = 0.10137805329227355\n",
      "2 = 0.9416999816894531\n",
      "3 = 0.8855999708175659\n",
      "4 = 0.9977999925613403\n",
      "5 = 0.9975219368934631\n",
      "6 = 0.8855999708175659\n",
      "7 = 8.7941881661892\n",
      "8 = 0.15684168366847479\n",
      "9 = 0.8176000118255615\n",
      "10 = 0.7870000004768372\n",
      "11 = 0.8482000231742859\n",
      "12 = 0.8383042216300964\n",
      "13 = 0.7870000004768372\n",
      "14 = 5.8714189529418945\n",
      "15 = 7.524579048156738\n",
      "16 = 0.350204199552536\n",
      "17 = 5.87139892578125\n",
      "18 = 55.11812973022461\n",
      "epoch:5 step:5401 [D loss: 0.641896, acc.: 62.50%] [G loss: 0.560311]\n",
      "epoch:5 step:5402 [D loss: 0.593355, acc.: 64.06%] [G loss: 0.448757]\n",
      "epoch:5 step:5403 [D loss: 0.603242, acc.: 65.62%] [G loss: 0.381463]\n",
      "epoch:5 step:5404 [D loss: 0.522547, acc.: 75.78%] [G loss: 0.547156]\n",
      "epoch:5 step:5405 [D loss: 0.547225, acc.: 78.12%] [G loss: 0.545910]\n",
      "epoch:5 step:5406 [D loss: 0.583647, acc.: 64.06%] [G loss: 0.464411]\n",
      "epoch:5 step:5407 [D loss: 0.553192, acc.: 75.00%] [G loss: 0.418952]\n",
      "epoch:5 step:5408 [D loss: 0.557618, acc.: 73.44%] [G loss: 0.434630]\n",
      "epoch:5 step:5409 [D loss: 0.540811, acc.: 71.88%] [G loss: 0.499788]\n",
      "epoch:5 step:5410 [D loss: 0.538139, acc.: 75.00%] [G loss: 0.530597]\n",
      "epoch:5 step:5411 [D loss: 0.535563, acc.: 78.12%] [G loss: 0.568714]\n",
      "epoch:5 step:5412 [D loss: 0.611346, acc.: 63.28%] [G loss: 0.447511]\n",
      "epoch:5 step:5413 [D loss: 0.541870, acc.: 74.22%] [G loss: 0.473757]\n",
      "epoch:5 step:5414 [D loss: 0.595085, acc.: 71.88%] [G loss: 0.527694]\n",
      "epoch:5 step:5415 [D loss: 0.536206, acc.: 78.12%] [G loss: 0.495937]\n",
      "epoch:5 step:5416 [D loss: 0.534526, acc.: 73.44%] [G loss: 0.551459]\n",
      "epoch:5 step:5417 [D loss: 0.480360, acc.: 75.78%] [G loss: 0.609403]\n",
      "epoch:5 step:5418 [D loss: 0.492089, acc.: 78.91%] [G loss: 0.574324]\n",
      "epoch:5 step:5419 [D loss: 0.528357, acc.: 72.66%] [G loss: 0.459809]\n",
      "epoch:5 step:5420 [D loss: 0.615569, acc.: 65.62%] [G loss: 0.463111]\n",
      "epoch:5 step:5421 [D loss: 0.446461, acc.: 79.69%] [G loss: 0.422989]\n",
      "epoch:5 step:5422 [D loss: 0.553962, acc.: 72.66%] [G loss: 0.511850]\n",
      "epoch:5 step:5423 [D loss: 0.556127, acc.: 71.09%] [G loss: 0.444586]\n",
      "epoch:5 step:5424 [D loss: 0.590099, acc.: 67.19%] [G loss: 0.414043]\n",
      "epoch:5 step:5425 [D loss: 0.644903, acc.: 60.16%] [G loss: 0.434797]\n",
      "epoch:5 step:5426 [D loss: 0.569627, acc.: 69.53%] [G loss: 0.392821]\n",
      "epoch:5 step:5427 [D loss: 0.578181, acc.: 71.88%] [G loss: 0.461251]\n",
      "epoch:5 step:5428 [D loss: 0.515683, acc.: 76.56%] [G loss: 0.469829]\n",
      "epoch:5 step:5429 [D loss: 0.538211, acc.: 69.53%] [G loss: 0.624852]\n",
      "epoch:5 step:5430 [D loss: 0.552379, acc.: 69.53%] [G loss: 0.510369]\n",
      "epoch:5 step:5431 [D loss: 0.466572, acc.: 80.47%] [G loss: 0.662005]\n",
      "epoch:5 step:5432 [D loss: 0.445878, acc.: 80.47%] [G loss: 0.637347]\n",
      "epoch:5 step:5433 [D loss: 0.537589, acc.: 71.09%] [G loss: 0.561506]\n",
      "epoch:5 step:5434 [D loss: 0.594292, acc.: 67.97%] [G loss: 0.500269]\n",
      "epoch:5 step:5435 [D loss: 0.527267, acc.: 73.44%] [G loss: 0.552263]\n",
      "epoch:5 step:5436 [D loss: 0.510903, acc.: 72.66%] [G loss: 0.600639]\n",
      "epoch:5 step:5437 [D loss: 0.509549, acc.: 73.44%] [G loss: 0.609378]\n",
      "epoch:5 step:5438 [D loss: 0.482368, acc.: 74.22%] [G loss: 0.613638]\n",
      "epoch:5 step:5439 [D loss: 0.509365, acc.: 76.56%] [G loss: 0.500588]\n",
      "epoch:5 step:5440 [D loss: 0.527677, acc.: 68.75%] [G loss: 0.695225]\n",
      "epoch:5 step:5441 [D loss: 0.578910, acc.: 70.31%] [G loss: 0.568260]\n",
      "epoch:5 step:5442 [D loss: 0.517263, acc.: 75.78%] [G loss: 0.583213]\n",
      "epoch:5 step:5443 [D loss: 0.552686, acc.: 71.09%] [G loss: 0.498477]\n",
      "epoch:5 step:5444 [D loss: 0.598988, acc.: 61.72%] [G loss: 0.535467]\n",
      "epoch:5 step:5445 [D loss: 0.527641, acc.: 74.22%] [G loss: 0.513422]\n",
      "epoch:5 step:5446 [D loss: 0.551104, acc.: 73.44%] [G loss: 0.578896]\n",
      "epoch:5 step:5447 [D loss: 0.566876, acc.: 66.41%] [G loss: 0.440238]\n",
      "epoch:5 step:5448 [D loss: 0.534072, acc.: 74.22%] [G loss: 0.536981]\n",
      "epoch:5 step:5449 [D loss: 0.594202, acc.: 67.97%] [G loss: 0.491350]\n",
      "epoch:5 step:5450 [D loss: 0.634557, acc.: 64.84%] [G loss: 0.459355]\n",
      "epoch:5 step:5451 [D loss: 0.656515, acc.: 61.72%] [G loss: 0.464649]\n",
      "epoch:5 step:5452 [D loss: 0.582236, acc.: 66.41%] [G loss: 0.376883]\n",
      "epoch:5 step:5453 [D loss: 0.598041, acc.: 67.97%] [G loss: 0.531520]\n",
      "epoch:5 step:5454 [D loss: 0.483689, acc.: 78.91%] [G loss: 0.527630]\n",
      "epoch:5 step:5455 [D loss: 0.515930, acc.: 75.00%] [G loss: 0.570649]\n",
      "epoch:5 step:5456 [D loss: 0.581410, acc.: 69.53%] [G loss: 0.566295]\n",
      "epoch:5 step:5457 [D loss: 0.504728, acc.: 75.78%] [G loss: 0.574168]\n",
      "epoch:5 step:5458 [D loss: 0.541882, acc.: 76.56%] [G loss: 0.523956]\n",
      "epoch:5 step:5459 [D loss: 0.584489, acc.: 71.09%] [G loss: 0.546775]\n",
      "epoch:5 step:5460 [D loss: 0.520736, acc.: 74.22%] [G loss: 0.498234]\n",
      "epoch:5 step:5461 [D loss: 0.576861, acc.: 69.53%] [G loss: 0.535946]\n",
      "epoch:5 step:5462 [D loss: 0.585641, acc.: 65.62%] [G loss: 0.417119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5463 [D loss: 0.539619, acc.: 75.00%] [G loss: 0.540607]\n",
      "epoch:5 step:5464 [D loss: 0.621208, acc.: 62.50%] [G loss: 0.400131]\n",
      "epoch:5 step:5465 [D loss: 0.535472, acc.: 73.44%] [G loss: 0.597337]\n",
      "epoch:5 step:5466 [D loss: 0.471590, acc.: 76.56%] [G loss: 0.749747]\n",
      "epoch:5 step:5467 [D loss: 0.502746, acc.: 78.12%] [G loss: 0.836551]\n",
      "epoch:5 step:5468 [D loss: 0.606745, acc.: 63.28%] [G loss: 0.558814]\n",
      "epoch:5 step:5469 [D loss: 0.606157, acc.: 66.41%] [G loss: 0.399867]\n",
      "epoch:5 step:5470 [D loss: 0.564855, acc.: 70.31%] [G loss: 0.507590]\n",
      "epoch:5 step:5471 [D loss: 0.496990, acc.: 75.78%] [G loss: 0.461963]\n",
      "epoch:5 step:5472 [D loss: 0.612303, acc.: 71.88%] [G loss: 0.493043]\n",
      "epoch:5 step:5473 [D loss: 0.620670, acc.: 64.84%] [G loss: 0.465562]\n",
      "epoch:5 step:5474 [D loss: 0.531005, acc.: 77.34%] [G loss: 0.463522]\n",
      "epoch:5 step:5475 [D loss: 0.592327, acc.: 65.62%] [G loss: 0.517013]\n",
      "epoch:5 step:5476 [D loss: 0.601614, acc.: 65.62%] [G loss: 0.462973]\n",
      "epoch:5 step:5477 [D loss: 0.514326, acc.: 71.88%] [G loss: 0.616262]\n",
      "epoch:5 step:5478 [D loss: 0.637856, acc.: 60.94%] [G loss: 0.389498]\n",
      "epoch:5 step:5479 [D loss: 0.587265, acc.: 65.62%] [G loss: 0.517923]\n",
      "epoch:5 step:5480 [D loss: 0.525716, acc.: 69.53%] [G loss: 0.528699]\n",
      "epoch:5 step:5481 [D loss: 0.522231, acc.: 70.31%] [G loss: 0.521220]\n",
      "epoch:5 step:5482 [D loss: 0.564920, acc.: 70.31%] [G loss: 0.554386]\n",
      "epoch:5 step:5483 [D loss: 0.512420, acc.: 72.66%] [G loss: 0.598576]\n",
      "epoch:5 step:5484 [D loss: 0.562517, acc.: 69.53%] [G loss: 0.487817]\n",
      "epoch:5 step:5485 [D loss: 0.505840, acc.: 76.56%] [G loss: 0.506617]\n",
      "epoch:5 step:5486 [D loss: 0.488712, acc.: 80.47%] [G loss: 0.548206]\n",
      "epoch:5 step:5487 [D loss: 0.536383, acc.: 71.88%] [G loss: 0.760217]\n",
      "epoch:5 step:5488 [D loss: 0.470456, acc.: 82.81%] [G loss: 0.720754]\n",
      "epoch:5 step:5489 [D loss: 0.515065, acc.: 72.66%] [G loss: 0.615617]\n",
      "epoch:5 step:5490 [D loss: 0.536463, acc.: 68.75%] [G loss: 0.454449]\n",
      "epoch:5 step:5491 [D loss: 0.501978, acc.: 75.00%] [G loss: 0.586113]\n",
      "epoch:5 step:5492 [D loss: 0.503371, acc.: 72.66%] [G loss: 0.569757]\n",
      "epoch:5 step:5493 [D loss: 0.605463, acc.: 65.62%] [G loss: 0.423082]\n",
      "epoch:5 step:5494 [D loss: 0.579939, acc.: 69.53%] [G loss: 0.361986]\n",
      "epoch:5 step:5495 [D loss: 0.502631, acc.: 75.00%] [G loss: 0.571604]\n",
      "epoch:5 step:5496 [D loss: 0.594906, acc.: 64.06%] [G loss: 0.533029]\n",
      "epoch:5 step:5497 [D loss: 0.634083, acc.: 65.62%] [G loss: 0.392948]\n",
      "epoch:5 step:5498 [D loss: 0.544774, acc.: 72.66%] [G loss: 0.446390]\n",
      "epoch:5 step:5499 [D loss: 0.524981, acc.: 72.66%] [G loss: 0.543495]\n",
      "epoch:5 step:5500 [D loss: 0.533592, acc.: 74.22%] [G loss: 0.691588]\n",
      "epoch:5 step:5501 [D loss: 0.558115, acc.: 69.53%] [G loss: 0.689070]\n",
      "epoch:5 step:5502 [D loss: 0.564231, acc.: 71.88%] [G loss: 0.621227]\n",
      "epoch:5 step:5503 [D loss: 0.600107, acc.: 64.84%] [G loss: 0.651393]\n",
      "epoch:5 step:5504 [D loss: 0.520039, acc.: 76.56%] [G loss: 0.652637]\n",
      "epoch:5 step:5505 [D loss: 0.606805, acc.: 64.84%] [G loss: 0.449343]\n",
      "epoch:5 step:5506 [D loss: 0.526792, acc.: 73.44%] [G loss: 0.504070]\n",
      "epoch:5 step:5507 [D loss: 0.553009, acc.: 68.75%] [G loss: 0.576401]\n",
      "epoch:5 step:5508 [D loss: 0.498963, acc.: 74.22%] [G loss: 0.528230]\n",
      "epoch:5 step:5509 [D loss: 0.583008, acc.: 69.53%] [G loss: 0.457797]\n",
      "epoch:5 step:5510 [D loss: 0.580862, acc.: 68.75%] [G loss: 0.370633]\n",
      "epoch:5 step:5511 [D loss: 0.559622, acc.: 74.22%] [G loss: 0.398626]\n",
      "epoch:5 step:5512 [D loss: 0.594137, acc.: 61.72%] [G loss: 0.427860]\n",
      "epoch:5 step:5513 [D loss: 0.581634, acc.: 66.41%] [G loss: 0.410827]\n",
      "epoch:5 step:5514 [D loss: 0.566417, acc.: 68.75%] [G loss: 0.414183]\n",
      "epoch:5 step:5515 [D loss: 0.539694, acc.: 70.31%] [G loss: 0.503071]\n",
      "epoch:5 step:5516 [D loss: 0.537519, acc.: 75.00%] [G loss: 0.619814]\n",
      "epoch:5 step:5517 [D loss: 0.539874, acc.: 75.00%] [G loss: 0.562021]\n",
      "epoch:5 step:5518 [D loss: 0.543426, acc.: 71.09%] [G loss: 0.508291]\n",
      "epoch:5 step:5519 [D loss: 0.544800, acc.: 71.88%] [G loss: 0.511308]\n",
      "epoch:5 step:5520 [D loss: 0.504545, acc.: 74.22%] [G loss: 0.520320]\n",
      "epoch:5 step:5521 [D loss: 0.555852, acc.: 67.19%] [G loss: 0.569043]\n",
      "epoch:5 step:5522 [D loss: 0.517249, acc.: 78.12%] [G loss: 0.585083]\n",
      "epoch:5 step:5523 [D loss: 0.538725, acc.: 70.31%] [G loss: 0.588742]\n",
      "epoch:5 step:5524 [D loss: 0.535687, acc.: 70.31%] [G loss: 0.580625]\n",
      "epoch:5 step:5525 [D loss: 0.627607, acc.: 63.28%] [G loss: 0.413398]\n",
      "epoch:5 step:5526 [D loss: 0.534427, acc.: 72.66%] [G loss: 0.475426]\n",
      "epoch:5 step:5527 [D loss: 0.484674, acc.: 78.91%] [G loss: 0.514208]\n",
      "epoch:5 step:5528 [D loss: 0.578717, acc.: 71.09%] [G loss: 0.501491]\n",
      "epoch:5 step:5529 [D loss: 0.603216, acc.: 66.41%] [G loss: 0.419796]\n",
      "epoch:5 step:5530 [D loss: 0.590706, acc.: 65.62%] [G loss: 0.436900]\n",
      "epoch:5 step:5531 [D loss: 0.549988, acc.: 69.53%] [G loss: 0.488092]\n",
      "epoch:5 step:5532 [D loss: 0.608606, acc.: 65.62%] [G loss: 0.339062]\n",
      "epoch:5 step:5533 [D loss: 0.577239, acc.: 63.28%] [G loss: 0.412953]\n",
      "epoch:5 step:5534 [D loss: 0.596087, acc.: 71.88%] [G loss: 0.330582]\n",
      "epoch:5 step:5535 [D loss: 0.503250, acc.: 75.00%] [G loss: 0.514101]\n",
      "epoch:5 step:5536 [D loss: 0.628354, acc.: 60.94%] [G loss: 0.415704]\n",
      "epoch:5 step:5537 [D loss: 0.505678, acc.: 75.00%] [G loss: 0.473632]\n",
      "epoch:5 step:5538 [D loss: 0.552726, acc.: 69.53%] [G loss: 0.473422]\n",
      "epoch:5 step:5539 [D loss: 0.479259, acc.: 80.47%] [G loss: 0.602395]\n",
      "epoch:5 step:5540 [D loss: 0.534453, acc.: 77.34%] [G loss: 0.605007]\n",
      "epoch:5 step:5541 [D loss: 0.611330, acc.: 64.84%] [G loss: 0.612209]\n",
      "epoch:5 step:5542 [D loss: 0.488252, acc.: 75.78%] [G loss: 0.537585]\n",
      "epoch:5 step:5543 [D loss: 0.650218, acc.: 60.16%] [G loss: 0.486963]\n",
      "epoch:5 step:5544 [D loss: 0.596296, acc.: 67.97%] [G loss: 0.567400]\n",
      "epoch:5 step:5545 [D loss: 0.487013, acc.: 78.91%] [G loss: 0.559358]\n",
      "epoch:5 step:5546 [D loss: 0.595952, acc.: 63.28%] [G loss: 0.462168]\n",
      "epoch:5 step:5547 [D loss: 0.581579, acc.: 67.97%] [G loss: 0.418001]\n",
      "epoch:5 step:5548 [D loss: 0.573465, acc.: 65.62%] [G loss: 0.411238]\n",
      "epoch:5 step:5549 [D loss: 0.531386, acc.: 73.44%] [G loss: 0.379049]\n",
      "epoch:5 step:5550 [D loss: 0.579370, acc.: 71.09%] [G loss: 0.445336]\n",
      "epoch:5 step:5551 [D loss: 0.558070, acc.: 72.66%] [G loss: 0.445239]\n",
      "epoch:5 step:5552 [D loss: 0.593790, acc.: 67.19%] [G loss: 0.449171]\n",
      "epoch:5 step:5553 [D loss: 0.527471, acc.: 73.44%] [G loss: 0.447871]\n",
      "epoch:5 step:5554 [D loss: 0.561868, acc.: 67.97%] [G loss: 0.310563]\n",
      "epoch:5 step:5555 [D loss: 0.531607, acc.: 74.22%] [G loss: 0.468443]\n",
      "epoch:5 step:5556 [D loss: 0.531746, acc.: 74.22%] [G loss: 0.545394]\n",
      "epoch:5 step:5557 [D loss: 0.583844, acc.: 66.41%] [G loss: 0.470383]\n",
      "epoch:5 step:5558 [D loss: 0.554041, acc.: 72.66%] [G loss: 0.536607]\n",
      "epoch:5 step:5559 [D loss: 0.585730, acc.: 63.28%] [G loss: 0.370561]\n",
      "epoch:5 step:5560 [D loss: 0.485929, acc.: 75.78%] [G loss: 0.704304]\n",
      "epoch:5 step:5561 [D loss: 0.551125, acc.: 67.19%] [G loss: 0.566071]\n",
      "epoch:5 step:5562 [D loss: 0.559439, acc.: 72.66%] [G loss: 0.502333]\n",
      "epoch:5 step:5563 [D loss: 0.612087, acc.: 61.72%] [G loss: 0.473653]\n",
      "epoch:5 step:5564 [D loss: 0.574477, acc.: 63.28%] [G loss: 0.470446]\n",
      "epoch:5 step:5565 [D loss: 0.617255, acc.: 60.94%] [G loss: 0.385988]\n",
      "epoch:5 step:5566 [D loss: 0.634622, acc.: 65.62%] [G loss: 0.335663]\n",
      "epoch:5 step:5567 [D loss: 0.565119, acc.: 70.31%] [G loss: 0.394114]\n",
      "epoch:5 step:5568 [D loss: 0.578089, acc.: 67.97%] [G loss: 0.509750]\n",
      "epoch:5 step:5569 [D loss: 0.486952, acc.: 78.91%] [G loss: 0.566140]\n",
      "epoch:5 step:5570 [D loss: 0.470382, acc.: 78.12%] [G loss: 0.590704]\n",
      "epoch:5 step:5571 [D loss: 0.517228, acc.: 78.12%] [G loss: 0.616202]\n",
      "epoch:5 step:5572 [D loss: 0.590266, acc.: 67.19%] [G loss: 0.608855]\n",
      "epoch:5 step:5573 [D loss: 0.519930, acc.: 78.91%] [G loss: 0.527843]\n",
      "epoch:5 step:5574 [D loss: 0.528558, acc.: 73.44%] [G loss: 0.492861]\n",
      "epoch:5 step:5575 [D loss: 0.522915, acc.: 75.78%] [G loss: 0.601035]\n",
      "epoch:5 step:5576 [D loss: 0.553730, acc.: 70.31%] [G loss: 0.506238]\n",
      "epoch:5 step:5577 [D loss: 0.581450, acc.: 67.19%] [G loss: 0.620198]\n",
      "epoch:5 step:5578 [D loss: 0.557041, acc.: 70.31%] [G loss: 0.499155]\n",
      "epoch:5 step:5579 [D loss: 0.478828, acc.: 75.78%] [G loss: 0.714725]\n",
      "epoch:5 step:5580 [D loss: 0.547775, acc.: 72.66%] [G loss: 0.702561]\n",
      "epoch:5 step:5581 [D loss: 0.559953, acc.: 73.44%] [G loss: 0.790193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5582 [D loss: 0.493410, acc.: 78.12%] [G loss: 0.665417]\n",
      "epoch:5 step:5583 [D loss: 0.475574, acc.: 79.69%] [G loss: 0.567305]\n",
      "epoch:5 step:5584 [D loss: 0.520079, acc.: 72.66%] [G loss: 0.613790]\n",
      "epoch:5 step:5585 [D loss: 0.474408, acc.: 78.91%] [G loss: 0.731328]\n",
      "epoch:5 step:5586 [D loss: 0.535666, acc.: 72.66%] [G loss: 0.637684]\n",
      "epoch:5 step:5587 [D loss: 0.585418, acc.: 66.41%] [G loss: 0.432797]\n",
      "epoch:5 step:5588 [D loss: 0.568007, acc.: 67.19%] [G loss: 0.456746]\n",
      "epoch:5 step:5589 [D loss: 0.530948, acc.: 70.31%] [G loss: 0.537203]\n",
      "epoch:5 step:5590 [D loss: 0.572190, acc.: 70.31%] [G loss: 0.630939]\n",
      "epoch:5 step:5591 [D loss: 0.517414, acc.: 78.91%] [G loss: 0.641355]\n",
      "epoch:5 step:5592 [D loss: 0.525457, acc.: 73.44%] [G loss: 0.654593]\n",
      "epoch:5 step:5593 [D loss: 0.556745, acc.: 72.66%] [G loss: 0.519503]\n",
      "epoch:5 step:5594 [D loss: 0.501926, acc.: 75.00%] [G loss: 0.627652]\n",
      "epoch:5 step:5595 [D loss: 0.492411, acc.: 75.00%] [G loss: 0.604999]\n",
      "epoch:5 step:5596 [D loss: 0.505045, acc.: 77.34%] [G loss: 0.692993]\n",
      "epoch:5 step:5597 [D loss: 0.458166, acc.: 78.12%] [G loss: 0.884498]\n",
      "epoch:5 step:5598 [D loss: 0.605492, acc.: 68.75%] [G loss: 0.786517]\n",
      "epoch:5 step:5599 [D loss: 0.443012, acc.: 83.59%] [G loss: 0.789310]\n",
      "epoch:5 step:5600 [D loss: 0.655752, acc.: 63.28%] [G loss: 0.549821]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.926800\n",
      "FID: 45.982071\n",
      "0 = 12.81149310922623\n",
      "1 = 0.09731385287775869\n",
      "2 = 0.9316999912261963\n",
      "3 = 0.8668000102043152\n",
      "4 = 0.9965999722480774\n",
      "5 = 0.9960928559303284\n",
      "6 = 0.8668000102043152\n",
      "7 = 8.372662473678593\n",
      "8 = 0.1366742742948662\n",
      "9 = 0.8097000122070312\n",
      "10 = 0.7838000059127808\n",
      "11 = 0.8356000185012817\n",
      "12 = 0.8266188502311707\n",
      "13 = 0.7838000059127808\n",
      "14 = 5.926823139190674\n",
      "15 = 7.45681619644165\n",
      "16 = 0.35221871733665466\n",
      "17 = 5.92680025100708\n",
      "18 = 45.98207092285156\n",
      "epoch:5 step:5601 [D loss: 0.533953, acc.: 74.22%] [G loss: 0.480921]\n",
      "epoch:5 step:5602 [D loss: 0.606930, acc.: 67.19%] [G loss: 0.456702]\n",
      "epoch:5 step:5603 [D loss: 0.487437, acc.: 78.91%] [G loss: 0.709032]\n",
      "epoch:5 step:5604 [D loss: 0.502236, acc.: 71.88%] [G loss: 0.641213]\n",
      "epoch:5 step:5605 [D loss: 0.734110, acc.: 62.50%] [G loss: 0.585520]\n",
      "epoch:5 step:5606 [D loss: 0.426240, acc.: 81.25%] [G loss: 0.710502]\n",
      "epoch:5 step:5607 [D loss: 0.521780, acc.: 77.34%] [G loss: 0.576298]\n",
      "epoch:5 step:5608 [D loss: 0.468082, acc.: 76.56%] [G loss: 0.601214]\n",
      "epoch:5 step:5609 [D loss: 0.425130, acc.: 82.03%] [G loss: 0.692632]\n",
      "epoch:5 step:5610 [D loss: 0.463320, acc.: 82.03%] [G loss: 0.867263]\n",
      "epoch:5 step:5611 [D loss: 0.485031, acc.: 75.78%] [G loss: 0.971397]\n",
      "epoch:5 step:5612 [D loss: 0.541670, acc.: 70.31%] [G loss: 0.883041]\n",
      "epoch:5 step:5613 [D loss: 0.752544, acc.: 62.50%] [G loss: 0.747197]\n",
      "epoch:5 step:5614 [D loss: 0.506197, acc.: 72.66%] [G loss: 0.914269]\n",
      "epoch:5 step:5615 [D loss: 0.495622, acc.: 76.56%] [G loss: 0.977260]\n",
      "epoch:5 step:5616 [D loss: 0.549700, acc.: 70.31%] [G loss: 0.698361]\n",
      "epoch:5 step:5617 [D loss: 0.633149, acc.: 65.62%] [G loss: 0.607586]\n",
      "epoch:5 step:5618 [D loss: 0.529370, acc.: 74.22%] [G loss: 0.741974]\n",
      "epoch:5 step:5619 [D loss: 0.597048, acc.: 68.75%] [G loss: 0.629579]\n",
      "epoch:5 step:5620 [D loss: 0.463671, acc.: 78.12%] [G loss: 0.971773]\n",
      "epoch:5 step:5621 [D loss: 0.401414, acc.: 82.03%] [G loss: 0.874532]\n",
      "epoch:5 step:5622 [D loss: 0.451277, acc.: 82.81%] [G loss: 1.183505]\n",
      "epoch:6 step:5623 [D loss: 0.547840, acc.: 75.78%] [G loss: 0.658361]\n",
      "epoch:6 step:5624 [D loss: 0.492467, acc.: 77.34%] [G loss: 0.768406]\n",
      "epoch:6 step:5625 [D loss: 0.598795, acc.: 67.97%] [G loss: 0.513002]\n",
      "epoch:6 step:5626 [D loss: 0.528404, acc.: 73.44%] [G loss: 0.627686]\n",
      "epoch:6 step:5627 [D loss: 0.536383, acc.: 69.53%] [G loss: 0.530042]\n",
      "epoch:6 step:5628 [D loss: 0.538891, acc.: 73.44%] [G loss: 0.597353]\n",
      "epoch:6 step:5629 [D loss: 0.491603, acc.: 78.12%] [G loss: 0.607259]\n",
      "epoch:6 step:5630 [D loss: 0.561784, acc.: 68.75%] [G loss: 0.491062]\n",
      "epoch:6 step:5631 [D loss: 0.545529, acc.: 71.09%] [G loss: 0.724308]\n",
      "epoch:6 step:5632 [D loss: 0.552458, acc.: 73.44%] [G loss: 0.554085]\n",
      "epoch:6 step:5633 [D loss: 0.489849, acc.: 78.12%] [G loss: 0.513194]\n",
      "epoch:6 step:5634 [D loss: 0.571190, acc.: 65.62%] [G loss: 0.515443]\n",
      "epoch:6 step:5635 [D loss: 0.480701, acc.: 75.00%] [G loss: 0.510617]\n",
      "epoch:6 step:5636 [D loss: 0.616880, acc.: 61.72%] [G loss: 0.367835]\n",
      "epoch:6 step:5637 [D loss: 0.523881, acc.: 77.34%] [G loss: 0.571715]\n",
      "epoch:6 step:5638 [D loss: 0.505417, acc.: 74.22%] [G loss: 0.560686]\n",
      "epoch:6 step:5639 [D loss: 0.568407, acc.: 68.75%] [G loss: 0.468303]\n",
      "epoch:6 step:5640 [D loss: 0.629761, acc.: 57.81%] [G loss: 0.345371]\n",
      "epoch:6 step:5641 [D loss: 0.570708, acc.: 71.88%] [G loss: 0.416297]\n",
      "epoch:6 step:5642 [D loss: 0.578046, acc.: 70.31%] [G loss: 0.525572]\n",
      "epoch:6 step:5643 [D loss: 0.529038, acc.: 71.88%] [G loss: 0.523813]\n",
      "epoch:6 step:5644 [D loss: 0.467478, acc.: 82.03%] [G loss: 0.627582]\n",
      "epoch:6 step:5645 [D loss: 0.537690, acc.: 72.66%] [G loss: 0.496054]\n",
      "epoch:6 step:5646 [D loss: 0.521346, acc.: 69.53%] [G loss: 0.619564]\n",
      "epoch:6 step:5647 [D loss: 0.514691, acc.: 75.78%] [G loss: 0.504297]\n",
      "epoch:6 step:5648 [D loss: 0.592189, acc.: 64.06%] [G loss: 0.399752]\n",
      "epoch:6 step:5649 [D loss: 0.522953, acc.: 75.00%] [G loss: 0.611639]\n",
      "epoch:6 step:5650 [D loss: 0.506951, acc.: 77.34%] [G loss: 0.547530]\n",
      "epoch:6 step:5651 [D loss: 0.510615, acc.: 75.00%] [G loss: 0.473216]\n",
      "epoch:6 step:5652 [D loss: 0.532261, acc.: 74.22%] [G loss: 0.509561]\n",
      "epoch:6 step:5653 [D loss: 0.597545, acc.: 70.31%] [G loss: 0.441610]\n",
      "epoch:6 step:5654 [D loss: 0.582338, acc.: 67.97%] [G loss: 0.432598]\n",
      "epoch:6 step:5655 [D loss: 0.524342, acc.: 73.44%] [G loss: 0.393137]\n",
      "epoch:6 step:5656 [D loss: 0.502565, acc.: 75.00%] [G loss: 0.545341]\n",
      "epoch:6 step:5657 [D loss: 0.553209, acc.: 68.75%] [G loss: 0.568450]\n",
      "epoch:6 step:5658 [D loss: 0.555458, acc.: 68.75%] [G loss: 0.658085]\n",
      "epoch:6 step:5659 [D loss: 0.475896, acc.: 78.91%] [G loss: 0.622912]\n",
      "epoch:6 step:5660 [D loss: 0.618016, acc.: 71.88%] [G loss: 0.474053]\n",
      "epoch:6 step:5661 [D loss: 0.516015, acc.: 73.44%] [G loss: 0.506832]\n",
      "epoch:6 step:5662 [D loss: 0.478067, acc.: 76.56%] [G loss: 0.489458]\n",
      "epoch:6 step:5663 [D loss: 0.524275, acc.: 69.53%] [G loss: 0.491664]\n",
      "epoch:6 step:5664 [D loss: 0.511001, acc.: 75.78%] [G loss: 0.469128]\n",
      "epoch:6 step:5665 [D loss: 0.530146, acc.: 69.53%] [G loss: 0.620790]\n",
      "epoch:6 step:5666 [D loss: 0.555856, acc.: 68.75%] [G loss: 0.551610]\n",
      "epoch:6 step:5667 [D loss: 0.516216, acc.: 74.22%] [G loss: 0.551058]\n",
      "epoch:6 step:5668 [D loss: 0.561888, acc.: 68.75%] [G loss: 0.548858]\n",
      "epoch:6 step:5669 [D loss: 0.521792, acc.: 72.66%] [G loss: 0.459369]\n",
      "epoch:6 step:5670 [D loss: 0.570494, acc.: 67.97%] [G loss: 0.543675]\n",
      "epoch:6 step:5671 [D loss: 0.470792, acc.: 79.69%] [G loss: 0.605998]\n",
      "epoch:6 step:5672 [D loss: 0.549022, acc.: 75.78%] [G loss: 0.584978]\n",
      "epoch:6 step:5673 [D loss: 0.623920, acc.: 67.19%] [G loss: 0.452558]\n",
      "epoch:6 step:5674 [D loss: 0.541713, acc.: 69.53%] [G loss: 0.497613]\n",
      "epoch:6 step:5675 [D loss: 0.511769, acc.: 78.91%] [G loss: 0.449529]\n",
      "epoch:6 step:5676 [D loss: 0.487961, acc.: 75.78%] [G loss: 0.752539]\n",
      "epoch:6 step:5677 [D loss: 0.531372, acc.: 68.75%] [G loss: 0.556516]\n",
      "epoch:6 step:5678 [D loss: 0.512039, acc.: 74.22%] [G loss: 0.576712]\n",
      "epoch:6 step:5679 [D loss: 0.513005, acc.: 71.88%] [G loss: 0.580508]\n",
      "epoch:6 step:5680 [D loss: 0.585671, acc.: 64.84%] [G loss: 0.471869]\n",
      "epoch:6 step:5681 [D loss: 0.501940, acc.: 77.34%] [G loss: 0.780092]\n",
      "epoch:6 step:5682 [D loss: 0.552373, acc.: 70.31%] [G loss: 0.687612]\n",
      "epoch:6 step:5683 [D loss: 0.566234, acc.: 67.19%] [G loss: 0.591470]\n",
      "epoch:6 step:5684 [D loss: 0.549866, acc.: 74.22%] [G loss: 0.543061]\n",
      "epoch:6 step:5685 [D loss: 0.573249, acc.: 65.62%] [G loss: 0.426770]\n",
      "epoch:6 step:5686 [D loss: 0.541209, acc.: 73.44%] [G loss: 0.450967]\n",
      "epoch:6 step:5687 [D loss: 0.560788, acc.: 68.75%] [G loss: 0.435999]\n",
      "epoch:6 step:5688 [D loss: 0.521076, acc.: 71.88%] [G loss: 0.532243]\n",
      "epoch:6 step:5689 [D loss: 0.567507, acc.: 64.06%] [G loss: 0.494475]\n",
      "epoch:6 step:5690 [D loss: 0.514843, acc.: 74.22%] [G loss: 0.542920]\n",
      "epoch:6 step:5691 [D loss: 0.543924, acc.: 73.44%] [G loss: 0.593220]\n",
      "epoch:6 step:5692 [D loss: 0.485313, acc.: 80.47%] [G loss: 0.551002]\n",
      "epoch:6 step:5693 [D loss: 0.543878, acc.: 67.19%] [G loss: 0.590245]\n",
      "epoch:6 step:5694 [D loss: 0.492729, acc.: 80.47%] [G loss: 0.433214]\n",
      "epoch:6 step:5695 [D loss: 0.538764, acc.: 71.09%] [G loss: 0.432085]\n",
      "epoch:6 step:5696 [D loss: 0.493989, acc.: 75.78%] [G loss: 0.627189]\n",
      "epoch:6 step:5697 [D loss: 0.601170, acc.: 65.62%] [G loss: 0.569073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5698 [D loss: 0.515702, acc.: 75.00%] [G loss: 0.601916]\n",
      "epoch:6 step:5699 [D loss: 0.457139, acc.: 78.12%] [G loss: 0.704981]\n",
      "epoch:6 step:5700 [D loss: 0.581594, acc.: 68.75%] [G loss: 0.576759]\n",
      "epoch:6 step:5701 [D loss: 0.560335, acc.: 72.66%] [G loss: 0.500640]\n",
      "epoch:6 step:5702 [D loss: 0.545092, acc.: 73.44%] [G loss: 0.605332]\n",
      "epoch:6 step:5703 [D loss: 0.594674, acc.: 60.94%] [G loss: 0.553472]\n",
      "epoch:6 step:5704 [D loss: 0.512829, acc.: 72.66%] [G loss: 0.518216]\n",
      "epoch:6 step:5705 [D loss: 0.527723, acc.: 71.88%] [G loss: 0.521444]\n",
      "epoch:6 step:5706 [D loss: 0.550796, acc.: 73.44%] [G loss: 0.553565]\n",
      "epoch:6 step:5707 [D loss: 0.556643, acc.: 73.44%] [G loss: 0.412013]\n",
      "epoch:6 step:5708 [D loss: 0.562317, acc.: 75.00%] [G loss: 0.468376]\n",
      "epoch:6 step:5709 [D loss: 0.492565, acc.: 76.56%] [G loss: 0.518814]\n",
      "epoch:6 step:5710 [D loss: 0.503101, acc.: 78.12%] [G loss: 0.453660]\n",
      "epoch:6 step:5711 [D loss: 0.484329, acc.: 78.91%] [G loss: 0.505488]\n",
      "epoch:6 step:5712 [D loss: 0.531029, acc.: 77.34%] [G loss: 0.507699]\n",
      "epoch:6 step:5713 [D loss: 0.552970, acc.: 70.31%] [G loss: 0.634347]\n",
      "epoch:6 step:5714 [D loss: 0.538302, acc.: 72.66%] [G loss: 0.565409]\n",
      "epoch:6 step:5715 [D loss: 0.525169, acc.: 69.53%] [G loss: 0.616628]\n",
      "epoch:6 step:5716 [D loss: 0.509639, acc.: 75.00%] [G loss: 0.702830]\n",
      "epoch:6 step:5717 [D loss: 0.551753, acc.: 74.22%] [G loss: 0.681888]\n",
      "epoch:6 step:5718 [D loss: 0.444170, acc.: 78.12%] [G loss: 0.590324]\n",
      "epoch:6 step:5719 [D loss: 0.512724, acc.: 75.78%] [G loss: 0.563989]\n",
      "epoch:6 step:5720 [D loss: 0.545723, acc.: 66.41%] [G loss: 0.743397]\n",
      "epoch:6 step:5721 [D loss: 0.526283, acc.: 75.00%] [G loss: 0.715637]\n",
      "epoch:6 step:5722 [D loss: 0.466248, acc.: 77.34%] [G loss: 0.741442]\n",
      "epoch:6 step:5723 [D loss: 0.497800, acc.: 76.56%] [G loss: 0.626803]\n",
      "epoch:6 step:5724 [D loss: 0.573822, acc.: 71.88%] [G loss: 0.536859]\n",
      "epoch:6 step:5725 [D loss: 0.564545, acc.: 68.75%] [G loss: 0.473312]\n",
      "epoch:6 step:5726 [D loss: 0.492885, acc.: 77.34%] [G loss: 0.569098]\n",
      "epoch:6 step:5727 [D loss: 0.569197, acc.: 64.06%] [G loss: 0.537433]\n",
      "epoch:6 step:5728 [D loss: 0.550570, acc.: 75.78%] [G loss: 0.450105]\n",
      "epoch:6 step:5729 [D loss: 0.574296, acc.: 69.53%] [G loss: 0.412416]\n",
      "epoch:6 step:5730 [D loss: 0.607160, acc.: 63.28%] [G loss: 0.447983]\n",
      "epoch:6 step:5731 [D loss: 0.594868, acc.: 64.84%] [G loss: 0.498428]\n",
      "epoch:6 step:5732 [D loss: 0.558775, acc.: 73.44%] [G loss: 0.565121]\n",
      "epoch:6 step:5733 [D loss: 0.502897, acc.: 77.34%] [G loss: 0.515489]\n",
      "epoch:6 step:5734 [D loss: 0.507580, acc.: 77.34%] [G loss: 0.626408]\n",
      "epoch:6 step:5735 [D loss: 0.568383, acc.: 69.53%] [G loss: 0.585157]\n",
      "epoch:6 step:5736 [D loss: 0.546380, acc.: 69.53%] [G loss: 0.525157]\n",
      "epoch:6 step:5737 [D loss: 0.521551, acc.: 73.44%] [G loss: 0.585278]\n",
      "epoch:6 step:5738 [D loss: 0.487043, acc.: 74.22%] [G loss: 0.624279]\n",
      "epoch:6 step:5739 [D loss: 0.542416, acc.: 70.31%] [G loss: 0.657183]\n",
      "epoch:6 step:5740 [D loss: 0.536566, acc.: 74.22%] [G loss: 0.571419]\n",
      "epoch:6 step:5741 [D loss: 0.452996, acc.: 82.03%] [G loss: 0.728916]\n",
      "epoch:6 step:5742 [D loss: 0.641840, acc.: 62.50%] [G loss: 0.556402]\n",
      "epoch:6 step:5743 [D loss: 0.602996, acc.: 69.53%] [G loss: 0.634336]\n",
      "epoch:6 step:5744 [D loss: 0.487038, acc.: 75.78%] [G loss: 0.684925]\n",
      "epoch:6 step:5745 [D loss: 0.555518, acc.: 66.41%] [G loss: 0.643802]\n",
      "epoch:6 step:5746 [D loss: 0.571967, acc.: 75.78%] [G loss: 0.637973]\n",
      "epoch:6 step:5747 [D loss: 0.559081, acc.: 66.41%] [G loss: 0.487680]\n",
      "epoch:6 step:5748 [D loss: 0.598459, acc.: 70.31%] [G loss: 0.479075]\n",
      "epoch:6 step:5749 [D loss: 0.520002, acc.: 71.88%] [G loss: 0.590269]\n",
      "epoch:6 step:5750 [D loss: 0.547423, acc.: 72.66%] [G loss: 0.464425]\n",
      "epoch:6 step:5751 [D loss: 0.564216, acc.: 64.84%] [G loss: 0.529159]\n",
      "epoch:6 step:5752 [D loss: 0.543023, acc.: 72.66%] [G loss: 0.438814]\n",
      "epoch:6 step:5753 [D loss: 0.500056, acc.: 76.56%] [G loss: 0.522230]\n",
      "epoch:6 step:5754 [D loss: 0.552262, acc.: 71.88%] [G loss: 0.461020]\n",
      "epoch:6 step:5755 [D loss: 0.605062, acc.: 69.53%] [G loss: 0.562390]\n",
      "epoch:6 step:5756 [D loss: 0.520616, acc.: 70.31%] [G loss: 0.589144]\n",
      "epoch:6 step:5757 [D loss: 0.540614, acc.: 71.09%] [G loss: 0.591667]\n",
      "epoch:6 step:5758 [D loss: 0.512320, acc.: 75.78%] [G loss: 0.614459]\n",
      "epoch:6 step:5759 [D loss: 0.644932, acc.: 57.81%] [G loss: 0.481548]\n",
      "epoch:6 step:5760 [D loss: 0.578129, acc.: 65.62%] [G loss: 0.582528]\n",
      "epoch:6 step:5761 [D loss: 0.613716, acc.: 66.41%] [G loss: 0.611608]\n",
      "epoch:6 step:5762 [D loss: 0.588313, acc.: 67.19%] [G loss: 0.603967]\n",
      "epoch:6 step:5763 [D loss: 0.482626, acc.: 79.69%] [G loss: 0.594941]\n",
      "epoch:6 step:5764 [D loss: 0.580231, acc.: 65.62%] [G loss: 0.377696]\n",
      "epoch:6 step:5765 [D loss: 0.604451, acc.: 65.62%] [G loss: 0.380953]\n",
      "epoch:6 step:5766 [D loss: 0.537348, acc.: 69.53%] [G loss: 0.489237]\n",
      "epoch:6 step:5767 [D loss: 0.552143, acc.: 68.75%] [G loss: 0.530929]\n",
      "epoch:6 step:5768 [D loss: 0.545058, acc.: 76.56%] [G loss: 0.572133]\n",
      "epoch:6 step:5769 [D loss: 0.639621, acc.: 66.41%] [G loss: 0.572629]\n",
      "epoch:6 step:5770 [D loss: 0.579775, acc.: 68.75%] [G loss: 0.450688]\n",
      "epoch:6 step:5771 [D loss: 0.498418, acc.: 74.22%] [G loss: 0.566936]\n",
      "epoch:6 step:5772 [D loss: 0.612950, acc.: 65.62%] [G loss: 0.555364]\n",
      "epoch:6 step:5773 [D loss: 0.569639, acc.: 74.22%] [G loss: 0.543541]\n",
      "epoch:6 step:5774 [D loss: 0.489303, acc.: 77.34%] [G loss: 0.790749]\n",
      "epoch:6 step:5775 [D loss: 0.615153, acc.: 64.84%] [G loss: 0.496623]\n",
      "epoch:6 step:5776 [D loss: 0.513353, acc.: 76.56%] [G loss: 0.565166]\n",
      "epoch:6 step:5777 [D loss: 0.452061, acc.: 80.47%] [G loss: 0.595139]\n",
      "epoch:6 step:5778 [D loss: 0.532195, acc.: 71.88%] [G loss: 0.629358]\n",
      "epoch:6 step:5779 [D loss: 0.567780, acc.: 69.53%] [G loss: 0.542005]\n",
      "epoch:6 step:5780 [D loss: 0.533589, acc.: 77.34%] [G loss: 0.586865]\n",
      "epoch:6 step:5781 [D loss: 0.547627, acc.: 71.09%] [G loss: 0.518306]\n",
      "epoch:6 step:5782 [D loss: 0.591110, acc.: 67.97%] [G loss: 0.608768]\n",
      "epoch:6 step:5783 [D loss: 0.514089, acc.: 71.88%] [G loss: 0.613008]\n",
      "epoch:6 step:5784 [D loss: 0.476800, acc.: 76.56%] [G loss: 0.580201]\n",
      "epoch:6 step:5785 [D loss: 0.565284, acc.: 69.53%] [G loss: 0.410501]\n",
      "epoch:6 step:5786 [D loss: 0.494693, acc.: 78.12%] [G loss: 0.646197]\n",
      "epoch:6 step:5787 [D loss: 0.506157, acc.: 73.44%] [G loss: 0.554064]\n",
      "epoch:6 step:5788 [D loss: 0.548375, acc.: 68.75%] [G loss: 0.516990]\n",
      "epoch:6 step:5789 [D loss: 0.622685, acc.: 60.16%] [G loss: 0.580031]\n",
      "epoch:6 step:5790 [D loss: 0.535738, acc.: 75.00%] [G loss: 0.485554]\n",
      "epoch:6 step:5791 [D loss: 0.570880, acc.: 71.09%] [G loss: 0.426535]\n",
      "epoch:6 step:5792 [D loss: 0.537700, acc.: 71.09%] [G loss: 0.436917]\n",
      "epoch:6 step:5793 [D loss: 0.510226, acc.: 71.88%] [G loss: 0.511635]\n",
      "epoch:6 step:5794 [D loss: 0.552695, acc.: 73.44%] [G loss: 0.501139]\n",
      "epoch:6 step:5795 [D loss: 0.503481, acc.: 78.12%] [G loss: 0.433077]\n",
      "epoch:6 step:5796 [D loss: 0.548147, acc.: 70.31%] [G loss: 0.425306]\n",
      "epoch:6 step:5797 [D loss: 0.556549, acc.: 71.09%] [G loss: 0.468665]\n",
      "epoch:6 step:5798 [D loss: 0.560589, acc.: 71.09%] [G loss: 0.484177]\n",
      "epoch:6 step:5799 [D loss: 0.526444, acc.: 75.00%] [G loss: 0.464950]\n",
      "epoch:6 step:5800 [D loss: 0.541953, acc.: 71.09%] [G loss: 0.592385]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.260027\n",
      "FID: 37.922749\n",
      "0 = 12.96808666353225\n",
      "1 = 0.10707396608539742\n",
      "2 = 0.9301999807357788\n",
      "3 = 0.8654000163078308\n",
      "4 = 0.9950000047683716\n",
      "5 = 0.994255542755127\n",
      "6 = 0.8654000163078308\n",
      "7 = 7.969539223885522\n",
      "8 = 0.12317807907551524\n",
      "9 = 0.8019999861717224\n",
      "10 = 0.7710000276565552\n",
      "11 = 0.8330000042915344\n",
      "12 = 0.8219616413116455\n",
      "13 = 0.7710000276565552\n",
      "14 = 6.2600531578063965\n",
      "15 = 8.150823593139648\n",
      "16 = 0.3015543818473816\n",
      "17 = 6.260026931762695\n",
      "18 = 37.92274856567383\n",
      "epoch:6 step:5801 [D loss: 0.524091, acc.: 75.78%] [G loss: 0.695732]\n",
      "epoch:6 step:5802 [D loss: 0.590515, acc.: 64.84%] [G loss: 0.479055]\n",
      "epoch:6 step:5803 [D loss: 0.536184, acc.: 71.09%] [G loss: 0.405012]\n",
      "epoch:6 step:5804 [D loss: 0.615665, acc.: 71.09%] [G loss: 0.499145]\n",
      "epoch:6 step:5805 [D loss: 0.622115, acc.: 67.19%] [G loss: 0.579177]\n",
      "epoch:6 step:5806 [D loss: 0.582277, acc.: 71.09%] [G loss: 0.608592]\n",
      "epoch:6 step:5807 [D loss: 0.636316, acc.: 67.19%] [G loss: 0.493377]\n",
      "epoch:6 step:5808 [D loss: 0.551162, acc.: 74.22%] [G loss: 0.492330]\n",
      "epoch:6 step:5809 [D loss: 0.635167, acc.: 64.06%] [G loss: 0.398820]\n",
      "epoch:6 step:5810 [D loss: 0.516838, acc.: 71.88%] [G loss: 0.539711]\n",
      "epoch:6 step:5811 [D loss: 0.523248, acc.: 75.78%] [G loss: 0.437281]\n",
      "epoch:6 step:5812 [D loss: 0.530581, acc.: 75.78%] [G loss: 0.532382]\n",
      "epoch:6 step:5813 [D loss: 0.494282, acc.: 74.22%] [G loss: 0.598094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5814 [D loss: 0.530240, acc.: 75.78%] [G loss: 0.507191]\n",
      "epoch:6 step:5815 [D loss: 0.545809, acc.: 69.53%] [G loss: 0.445953]\n",
      "epoch:6 step:5816 [D loss: 0.455123, acc.: 78.12%] [G loss: 0.484841]\n",
      "epoch:6 step:5817 [D loss: 0.607074, acc.: 67.97%] [G loss: 0.531573]\n",
      "epoch:6 step:5818 [D loss: 0.601810, acc.: 60.94%] [G loss: 0.463547]\n",
      "epoch:6 step:5819 [D loss: 0.515519, acc.: 75.78%] [G loss: 0.528438]\n",
      "epoch:6 step:5820 [D loss: 0.432022, acc.: 81.25%] [G loss: 0.626653]\n",
      "epoch:6 step:5821 [D loss: 0.545242, acc.: 66.41%] [G loss: 0.501475]\n",
      "epoch:6 step:5822 [D loss: 0.565670, acc.: 69.53%] [G loss: 0.560161]\n",
      "epoch:6 step:5823 [D loss: 0.556719, acc.: 74.22%] [G loss: 0.530521]\n",
      "epoch:6 step:5824 [D loss: 0.536072, acc.: 72.66%] [G loss: 0.478574]\n",
      "epoch:6 step:5825 [D loss: 0.623378, acc.: 70.31%] [G loss: 0.496728]\n",
      "epoch:6 step:5826 [D loss: 0.568781, acc.: 72.66%] [G loss: 0.664686]\n",
      "epoch:6 step:5827 [D loss: 0.516300, acc.: 73.44%] [G loss: 0.622191]\n",
      "epoch:6 step:5828 [D loss: 0.504479, acc.: 76.56%] [G loss: 0.670472]\n",
      "epoch:6 step:5829 [D loss: 0.489920, acc.: 75.78%] [G loss: 0.577671]\n",
      "epoch:6 step:5830 [D loss: 0.458874, acc.: 75.00%] [G loss: 0.643077]\n",
      "epoch:6 step:5831 [D loss: 0.506377, acc.: 75.78%] [G loss: 0.707217]\n",
      "epoch:6 step:5832 [D loss: 0.608951, acc.: 62.50%] [G loss: 0.489651]\n",
      "epoch:6 step:5833 [D loss: 0.567884, acc.: 69.53%] [G loss: 0.568741]\n",
      "epoch:6 step:5834 [D loss: 0.557837, acc.: 68.75%] [G loss: 0.474949]\n",
      "epoch:6 step:5835 [D loss: 0.512470, acc.: 74.22%] [G loss: 0.528558]\n",
      "epoch:6 step:5836 [D loss: 0.656173, acc.: 61.72%] [G loss: 0.352911]\n",
      "epoch:6 step:5837 [D loss: 0.572661, acc.: 67.19%] [G loss: 0.402603]\n",
      "epoch:6 step:5838 [D loss: 0.560348, acc.: 72.66%] [G loss: 0.466217]\n",
      "epoch:6 step:5839 [D loss: 0.492073, acc.: 73.44%] [G loss: 0.557624]\n",
      "epoch:6 step:5840 [D loss: 0.546337, acc.: 74.22%] [G loss: 0.437354]\n",
      "epoch:6 step:5841 [D loss: 0.514769, acc.: 76.56%] [G loss: 0.609713]\n",
      "epoch:6 step:5842 [D loss: 0.676986, acc.: 61.72%] [G loss: 0.440888]\n",
      "epoch:6 step:5843 [D loss: 0.509568, acc.: 75.78%] [G loss: 0.567517]\n",
      "epoch:6 step:5844 [D loss: 0.480447, acc.: 77.34%] [G loss: 0.749937]\n",
      "epoch:6 step:5845 [D loss: 0.471761, acc.: 80.47%] [G loss: 0.715655]\n",
      "epoch:6 step:5846 [D loss: 0.580771, acc.: 71.09%] [G loss: 0.527470]\n",
      "epoch:6 step:5847 [D loss: 0.571283, acc.: 67.19%] [G loss: 0.472451]\n",
      "epoch:6 step:5848 [D loss: 0.599343, acc.: 66.41%] [G loss: 0.381309]\n",
      "epoch:6 step:5849 [D loss: 0.577329, acc.: 70.31%] [G loss: 0.485761]\n",
      "epoch:6 step:5850 [D loss: 0.631259, acc.: 62.50%] [G loss: 0.351060]\n",
      "epoch:6 step:5851 [D loss: 0.550388, acc.: 75.78%] [G loss: 0.419988]\n",
      "epoch:6 step:5852 [D loss: 0.524526, acc.: 74.22%] [G loss: 0.364142]\n",
      "epoch:6 step:5853 [D loss: 0.524276, acc.: 71.88%] [G loss: 0.537526]\n",
      "epoch:6 step:5854 [D loss: 0.449885, acc.: 81.25%] [G loss: 0.690105]\n",
      "epoch:6 step:5855 [D loss: 0.590309, acc.: 64.84%] [G loss: 0.629754]\n",
      "epoch:6 step:5856 [D loss: 0.492341, acc.: 74.22%] [G loss: 0.571772]\n",
      "epoch:6 step:5857 [D loss: 0.548452, acc.: 68.75%] [G loss: 0.623587]\n",
      "epoch:6 step:5858 [D loss: 0.547162, acc.: 70.31%] [G loss: 0.575994]\n",
      "epoch:6 step:5859 [D loss: 0.539048, acc.: 70.31%] [G loss: 0.621497]\n",
      "epoch:6 step:5860 [D loss: 0.595723, acc.: 61.72%] [G loss: 0.508752]\n",
      "epoch:6 step:5861 [D loss: 0.511307, acc.: 74.22%] [G loss: 0.471270]\n",
      "epoch:6 step:5862 [D loss: 0.574108, acc.: 67.19%] [G loss: 0.451302]\n",
      "epoch:6 step:5863 [D loss: 0.529767, acc.: 74.22%] [G loss: 0.594956]\n",
      "epoch:6 step:5864 [D loss: 0.537429, acc.: 70.31%] [G loss: 0.543919]\n",
      "epoch:6 step:5865 [D loss: 0.537466, acc.: 75.78%] [G loss: 0.539434]\n",
      "epoch:6 step:5866 [D loss: 0.471506, acc.: 75.78%] [G loss: 0.635139]\n",
      "epoch:6 step:5867 [D loss: 0.547937, acc.: 71.88%] [G loss: 0.497612]\n",
      "epoch:6 step:5868 [D loss: 0.546663, acc.: 71.09%] [G loss: 0.560716]\n",
      "epoch:6 step:5869 [D loss: 0.641675, acc.: 60.16%] [G loss: 0.414835]\n",
      "epoch:6 step:5870 [D loss: 0.602279, acc.: 67.97%] [G loss: 0.527900]\n",
      "epoch:6 step:5871 [D loss: 0.510986, acc.: 74.22%] [G loss: 0.621219]\n",
      "epoch:6 step:5872 [D loss: 0.588929, acc.: 62.50%] [G loss: 0.559830]\n",
      "epoch:6 step:5873 [D loss: 0.618854, acc.: 61.72%] [G loss: 0.572679]\n",
      "epoch:6 step:5874 [D loss: 0.633514, acc.: 63.28%] [G loss: 0.414481]\n",
      "epoch:6 step:5875 [D loss: 0.574002, acc.: 71.09%] [G loss: 0.484370]\n",
      "epoch:6 step:5876 [D loss: 0.461494, acc.: 79.69%] [G loss: 0.667411]\n",
      "epoch:6 step:5877 [D loss: 0.542457, acc.: 67.19%] [G loss: 0.632393]\n",
      "epoch:6 step:5878 [D loss: 0.497253, acc.: 78.91%] [G loss: 0.642058]\n",
      "epoch:6 step:5879 [D loss: 0.565513, acc.: 67.97%] [G loss: 0.445544]\n",
      "epoch:6 step:5880 [D loss: 0.506331, acc.: 78.12%] [G loss: 0.565082]\n",
      "epoch:6 step:5881 [D loss: 0.537516, acc.: 74.22%] [G loss: 0.471406]\n",
      "epoch:6 step:5882 [D loss: 0.572513, acc.: 71.09%] [G loss: 0.572398]\n",
      "epoch:6 step:5883 [D loss: 0.552749, acc.: 75.00%] [G loss: 0.560699]\n",
      "epoch:6 step:5884 [D loss: 0.542987, acc.: 73.44%] [G loss: 0.526796]\n",
      "epoch:6 step:5885 [D loss: 0.633020, acc.: 65.62%] [G loss: 0.548252]\n",
      "epoch:6 step:5886 [D loss: 0.512243, acc.: 75.78%] [G loss: 0.664924]\n",
      "epoch:6 step:5887 [D loss: 0.601859, acc.: 64.06%] [G loss: 0.533156]\n",
      "epoch:6 step:5888 [D loss: 0.531557, acc.: 77.34%] [G loss: 0.439521]\n",
      "epoch:6 step:5889 [D loss: 0.561612, acc.: 68.75%] [G loss: 0.496927]\n",
      "epoch:6 step:5890 [D loss: 0.542240, acc.: 70.31%] [G loss: 0.481607]\n",
      "epoch:6 step:5891 [D loss: 0.524453, acc.: 75.78%] [G loss: 0.508328]\n",
      "epoch:6 step:5892 [D loss: 0.517746, acc.: 72.66%] [G loss: 0.486586]\n",
      "epoch:6 step:5893 [D loss: 0.544538, acc.: 73.44%] [G loss: 0.501991]\n",
      "epoch:6 step:5894 [D loss: 0.553135, acc.: 71.88%] [G loss: 0.598904]\n",
      "epoch:6 step:5895 [D loss: 0.524204, acc.: 73.44%] [G loss: 0.694257]\n",
      "epoch:6 step:5896 [D loss: 0.604353, acc.: 66.41%] [G loss: 0.562321]\n",
      "epoch:6 step:5897 [D loss: 0.594211, acc.: 67.97%] [G loss: 0.404161]\n",
      "epoch:6 step:5898 [D loss: 0.544163, acc.: 69.53%] [G loss: 0.421978]\n",
      "epoch:6 step:5899 [D loss: 0.649627, acc.: 67.19%] [G loss: 0.445958]\n",
      "epoch:6 step:5900 [D loss: 0.588002, acc.: 67.97%] [G loss: 0.445059]\n",
      "epoch:6 step:5901 [D loss: 0.529530, acc.: 78.12%] [G loss: 0.466677]\n",
      "epoch:6 step:5902 [D loss: 0.567267, acc.: 69.53%] [G loss: 0.811091]\n",
      "epoch:6 step:5903 [D loss: 0.620273, acc.: 65.62%] [G loss: 0.423234]\n",
      "epoch:6 step:5904 [D loss: 0.544928, acc.: 71.09%] [G loss: 0.494926]\n",
      "epoch:6 step:5905 [D loss: 0.561656, acc.: 67.19%] [G loss: 0.450878]\n",
      "epoch:6 step:5906 [D loss: 0.505958, acc.: 75.00%] [G loss: 0.446450]\n",
      "epoch:6 step:5907 [D loss: 0.490805, acc.: 71.88%] [G loss: 0.643736]\n",
      "epoch:6 step:5908 [D loss: 0.468451, acc.: 82.03%] [G loss: 0.697118]\n",
      "epoch:6 step:5909 [D loss: 0.555804, acc.: 69.53%] [G loss: 0.607122]\n",
      "epoch:6 step:5910 [D loss: 0.612085, acc.: 65.62%] [G loss: 0.450484]\n",
      "epoch:6 step:5911 [D loss: 0.525700, acc.: 75.78%] [G loss: 0.524233]\n",
      "epoch:6 step:5912 [D loss: 0.592511, acc.: 67.97%] [G loss: 0.451642]\n",
      "epoch:6 step:5913 [D loss: 0.566460, acc.: 67.97%] [G loss: 0.515341]\n",
      "epoch:6 step:5914 [D loss: 0.565069, acc.: 65.62%] [G loss: 0.552870]\n",
      "epoch:6 step:5915 [D loss: 0.537781, acc.: 70.31%] [G loss: 0.469900]\n",
      "epoch:6 step:5916 [D loss: 0.584381, acc.: 65.62%] [G loss: 0.407167]\n",
      "epoch:6 step:5917 [D loss: 0.518493, acc.: 75.00%] [G loss: 0.584036]\n",
      "epoch:6 step:5918 [D loss: 0.499668, acc.: 75.00%] [G loss: 0.601883]\n",
      "epoch:6 step:5919 [D loss: 0.564006, acc.: 66.41%] [G loss: 0.548640]\n",
      "epoch:6 step:5920 [D loss: 0.538702, acc.: 71.09%] [G loss: 0.621289]\n",
      "epoch:6 step:5921 [D loss: 0.513852, acc.: 78.91%] [G loss: 0.475729]\n",
      "epoch:6 step:5922 [D loss: 0.507977, acc.: 78.12%] [G loss: 0.556592]\n",
      "epoch:6 step:5923 [D loss: 0.576488, acc.: 72.66%] [G loss: 0.539464]\n",
      "epoch:6 step:5924 [D loss: 0.518804, acc.: 67.97%] [G loss: 0.482166]\n",
      "epoch:6 step:5925 [D loss: 0.532124, acc.: 74.22%] [G loss: 0.470807]\n",
      "epoch:6 step:5926 [D loss: 0.504633, acc.: 74.22%] [G loss: 0.551187]\n",
      "epoch:6 step:5927 [D loss: 0.469733, acc.: 80.47%] [G loss: 0.607107]\n",
      "epoch:6 step:5928 [D loss: 0.579511, acc.: 65.62%] [G loss: 0.538854]\n",
      "epoch:6 step:5929 [D loss: 0.503046, acc.: 75.78%] [G loss: 0.570589]\n",
      "epoch:6 step:5930 [D loss: 0.545681, acc.: 71.09%] [G loss: 0.509031]\n",
      "epoch:6 step:5931 [D loss: 0.525357, acc.: 71.09%] [G loss: 0.702032]\n",
      "epoch:6 step:5932 [D loss: 0.519728, acc.: 74.22%] [G loss: 0.590281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5933 [D loss: 0.500066, acc.: 75.00%] [G loss: 0.722201]\n",
      "epoch:6 step:5934 [D loss: 0.470720, acc.: 78.12%] [G loss: 0.763438]\n",
      "epoch:6 step:5935 [D loss: 0.483737, acc.: 77.34%] [G loss: 0.968956]\n",
      "epoch:6 step:5936 [D loss: 0.510587, acc.: 70.31%] [G loss: 0.826589]\n",
      "epoch:6 step:5937 [D loss: 0.446455, acc.: 80.47%] [G loss: 0.780960]\n",
      "epoch:6 step:5938 [D loss: 0.731280, acc.: 59.38%] [G loss: 0.525563]\n",
      "epoch:6 step:5939 [D loss: 0.580237, acc.: 66.41%] [G loss: 0.530399]\n",
      "epoch:6 step:5940 [D loss: 0.568356, acc.: 65.62%] [G loss: 0.437651]\n",
      "epoch:6 step:5941 [D loss: 0.581493, acc.: 69.53%] [G loss: 0.440266]\n",
      "epoch:6 step:5942 [D loss: 0.575401, acc.: 67.97%] [G loss: 0.434361]\n",
      "epoch:6 step:5943 [D loss: 0.486491, acc.: 79.69%] [G loss: 0.512986]\n",
      "epoch:6 step:5944 [D loss: 0.550688, acc.: 72.66%] [G loss: 0.561910]\n",
      "epoch:6 step:5945 [D loss: 0.579685, acc.: 64.84%] [G loss: 0.498435]\n",
      "epoch:6 step:5946 [D loss: 0.524366, acc.: 73.44%] [G loss: 0.534333]\n",
      "epoch:6 step:5947 [D loss: 0.535554, acc.: 70.31%] [G loss: 0.507511]\n",
      "epoch:6 step:5948 [D loss: 0.523715, acc.: 75.00%] [G loss: 0.722025]\n",
      "epoch:6 step:5949 [D loss: 0.517172, acc.: 75.78%] [G loss: 0.615897]\n",
      "epoch:6 step:5950 [D loss: 0.581868, acc.: 67.97%] [G loss: 0.705279]\n",
      "epoch:6 step:5951 [D loss: 0.521345, acc.: 71.88%] [G loss: 0.590329]\n",
      "epoch:6 step:5952 [D loss: 0.542003, acc.: 67.97%] [G loss: 0.560363]\n",
      "epoch:6 step:5953 [D loss: 0.533876, acc.: 69.53%] [G loss: 0.536339]\n",
      "epoch:6 step:5954 [D loss: 0.510874, acc.: 75.00%] [G loss: 0.537103]\n",
      "epoch:6 step:5955 [D loss: 0.497629, acc.: 74.22%] [G loss: 0.554379]\n",
      "epoch:6 step:5956 [D loss: 0.523613, acc.: 70.31%] [G loss: 0.693653]\n",
      "epoch:6 step:5957 [D loss: 0.537964, acc.: 67.97%] [G loss: 0.574462]\n",
      "epoch:6 step:5958 [D loss: 0.556212, acc.: 71.88%] [G loss: 0.748433]\n",
      "epoch:6 step:5959 [D loss: 0.570257, acc.: 68.75%] [G loss: 0.646572]\n",
      "epoch:6 step:5960 [D loss: 0.550379, acc.: 66.41%] [G loss: 0.570132]\n",
      "epoch:6 step:5961 [D loss: 0.553382, acc.: 69.53%] [G loss: 0.539689]\n",
      "epoch:6 step:5962 [D loss: 0.511990, acc.: 75.00%] [G loss: 0.579042]\n",
      "epoch:6 step:5963 [D loss: 0.601485, acc.: 71.88%] [G loss: 0.529093]\n",
      "epoch:6 step:5964 [D loss: 0.606507, acc.: 69.53%] [G loss: 0.539409]\n",
      "epoch:6 step:5965 [D loss: 0.502142, acc.: 71.09%] [G loss: 0.763676]\n",
      "epoch:6 step:5966 [D loss: 0.449647, acc.: 81.25%] [G loss: 0.763479]\n",
      "epoch:6 step:5967 [D loss: 0.582936, acc.: 66.41%] [G loss: 0.678174]\n",
      "epoch:6 step:5968 [D loss: 0.570905, acc.: 68.75%] [G loss: 0.784211]\n",
      "epoch:6 step:5969 [D loss: 0.494166, acc.: 76.56%] [G loss: 0.759538]\n",
      "epoch:6 step:5970 [D loss: 0.706756, acc.: 62.50%] [G loss: 0.539984]\n",
      "epoch:6 step:5971 [D loss: 0.678518, acc.: 59.38%] [G loss: 0.490464]\n",
      "epoch:6 step:5972 [D loss: 0.543092, acc.: 74.22%] [G loss: 0.494579]\n",
      "epoch:6 step:5973 [D loss: 0.557777, acc.: 72.66%] [G loss: 0.493047]\n",
      "epoch:6 step:5974 [D loss: 0.520064, acc.: 71.09%] [G loss: 0.658713]\n",
      "epoch:6 step:5975 [D loss: 0.597245, acc.: 67.19%] [G loss: 0.603334]\n",
      "epoch:6 step:5976 [D loss: 0.419441, acc.: 81.25%] [G loss: 0.808710]\n",
      "epoch:6 step:5977 [D loss: 0.545475, acc.: 73.44%] [G loss: 0.691492]\n",
      "epoch:6 step:5978 [D loss: 0.547465, acc.: 74.22%] [G loss: 0.589106]\n",
      "epoch:6 step:5979 [D loss: 0.500066, acc.: 78.12%] [G loss: 0.670230]\n",
      "epoch:6 step:5980 [D loss: 0.450524, acc.: 75.00%] [G loss: 0.747230]\n",
      "epoch:6 step:5981 [D loss: 0.456038, acc.: 81.25%] [G loss: 0.728285]\n",
      "epoch:6 step:5982 [D loss: 0.512563, acc.: 73.44%] [G loss: 0.583182]\n",
      "epoch:6 step:5983 [D loss: 0.605015, acc.: 70.31%] [G loss: 0.638929]\n",
      "epoch:6 step:5984 [D loss: 0.550243, acc.: 70.31%] [G loss: 0.470865]\n",
      "epoch:6 step:5985 [D loss: 0.512832, acc.: 78.12%] [G loss: 0.541872]\n",
      "epoch:6 step:5986 [D loss: 0.569658, acc.: 67.97%] [G loss: 0.529342]\n",
      "epoch:6 step:5987 [D loss: 0.536258, acc.: 72.66%] [G loss: 0.557651]\n",
      "epoch:6 step:5988 [D loss: 0.550110, acc.: 68.75%] [G loss: 0.473270]\n",
      "epoch:6 step:5989 [D loss: 0.544204, acc.: 75.00%] [G loss: 0.623777]\n",
      "epoch:6 step:5990 [D loss: 0.503985, acc.: 78.91%] [G loss: 0.709110]\n",
      "epoch:6 step:5991 [D loss: 0.559439, acc.: 76.56%] [G loss: 0.580024]\n",
      "epoch:6 step:5992 [D loss: 0.556585, acc.: 71.09%] [G loss: 0.558045]\n",
      "epoch:6 step:5993 [D loss: 0.532832, acc.: 74.22%] [G loss: 0.631458]\n",
      "epoch:6 step:5994 [D loss: 0.543468, acc.: 72.66%] [G loss: 0.585076]\n",
      "epoch:6 step:5995 [D loss: 0.556911, acc.: 72.66%] [G loss: 0.590165]\n",
      "epoch:6 step:5996 [D loss: 0.475452, acc.: 77.34%] [G loss: 0.607136]\n",
      "epoch:6 step:5997 [D loss: 0.580097, acc.: 67.97%] [G loss: 0.580313]\n",
      "epoch:6 step:5998 [D loss: 0.642803, acc.: 64.84%] [G loss: 0.471370]\n",
      "epoch:6 step:5999 [D loss: 0.569795, acc.: 67.19%] [G loss: 0.399967]\n",
      "epoch:6 step:6000 [D loss: 0.508357, acc.: 77.34%] [G loss: 0.537814]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.070099\n",
      "FID: 44.568851\n",
      "0 = 13.086460699653584\n",
      "1 = 0.1046113377701063\n",
      "2 = 0.9427000284194946\n",
      "3 = 0.8903999924659729\n",
      "4 = 0.9950000047683716\n",
      "5 = 0.9944158792495728\n",
      "6 = 0.8903999924659729\n",
      "7 = 8.222050615978265\n",
      "8 = 0.13347862592603194\n",
      "9 = 0.7960000038146973\n",
      "10 = 0.7742000222206116\n",
      "11 = 0.817799985408783\n",
      "12 = 0.8094939589500427\n",
      "13 = 0.7742000222206116\n",
      "14 = 6.070123195648193\n",
      "15 = 7.73895788192749\n",
      "16 = 0.3329174518585205\n",
      "17 = 6.070099353790283\n",
      "18 = 44.568851470947266\n",
      "epoch:6 step:6001 [D loss: 0.572378, acc.: 70.31%] [G loss: 0.483447]\n",
      "epoch:6 step:6002 [D loss: 0.640404, acc.: 67.19%] [G loss: 0.447524]\n",
      "epoch:6 step:6003 [D loss: 0.468655, acc.: 75.00%] [G loss: 0.614848]\n",
      "epoch:6 step:6004 [D loss: 0.501523, acc.: 77.34%] [G loss: 0.634482]\n",
      "epoch:6 step:6005 [D loss: 0.551663, acc.: 67.19%] [G loss: 0.482245]\n",
      "epoch:6 step:6006 [D loss: 0.559659, acc.: 70.31%] [G loss: 0.559660]\n",
      "epoch:6 step:6007 [D loss: 0.518416, acc.: 74.22%] [G loss: 0.616033]\n",
      "epoch:6 step:6008 [D loss: 0.600634, acc.: 67.19%] [G loss: 0.577763]\n",
      "epoch:6 step:6009 [D loss: 0.539380, acc.: 73.44%] [G loss: 0.472059]\n",
      "epoch:6 step:6010 [D loss: 0.538774, acc.: 73.44%] [G loss: 0.553421]\n",
      "epoch:6 step:6011 [D loss: 0.537898, acc.: 75.00%] [G loss: 0.624153]\n",
      "epoch:6 step:6012 [D loss: 0.567030, acc.: 71.09%] [G loss: 0.488092]\n",
      "epoch:6 step:6013 [D loss: 0.531239, acc.: 72.66%] [G loss: 0.465345]\n",
      "epoch:6 step:6014 [D loss: 0.431853, acc.: 83.59%] [G loss: 0.679044]\n",
      "epoch:6 step:6015 [D loss: 0.625498, acc.: 64.84%] [G loss: 0.471471]\n",
      "epoch:6 step:6016 [D loss: 0.576582, acc.: 71.09%] [G loss: 0.517538]\n",
      "epoch:6 step:6017 [D loss: 0.545680, acc.: 74.22%] [G loss: 0.434231]\n",
      "epoch:6 step:6018 [D loss: 0.594797, acc.: 63.28%] [G loss: 0.574563]\n",
      "epoch:6 step:6019 [D loss: 0.543878, acc.: 64.84%] [G loss: 0.593096]\n",
      "epoch:6 step:6020 [D loss: 0.460761, acc.: 78.91%] [G loss: 0.742610]\n",
      "epoch:6 step:6021 [D loss: 0.482751, acc.: 78.91%] [G loss: 0.775453]\n",
      "epoch:6 step:6022 [D loss: 0.692294, acc.: 57.81%] [G loss: 0.487879]\n",
      "epoch:6 step:6023 [D loss: 0.584093, acc.: 63.28%] [G loss: 0.387359]\n",
      "epoch:6 step:6024 [D loss: 0.479770, acc.: 78.91%] [G loss: 0.419240]\n",
      "epoch:6 step:6025 [D loss: 0.497053, acc.: 78.12%] [G loss: 0.547756]\n",
      "epoch:6 step:6026 [D loss: 0.577744, acc.: 67.19%] [G loss: 0.419123]\n",
      "epoch:6 step:6027 [D loss: 0.542470, acc.: 73.44%] [G loss: 0.548391]\n",
      "epoch:6 step:6028 [D loss: 0.514840, acc.: 71.88%] [G loss: 0.694910]\n",
      "epoch:6 step:6029 [D loss: 0.589880, acc.: 65.62%] [G loss: 0.556407]\n",
      "epoch:6 step:6030 [D loss: 0.552555, acc.: 70.31%] [G loss: 0.619762]\n",
      "epoch:6 step:6031 [D loss: 0.533601, acc.: 71.88%] [G loss: 0.480648]\n",
      "epoch:6 step:6032 [D loss: 0.631233, acc.: 67.97%] [G loss: 0.539810]\n",
      "epoch:6 step:6033 [D loss: 0.541715, acc.: 71.88%] [G loss: 0.633626]\n",
      "epoch:6 step:6034 [D loss: 0.613648, acc.: 61.72%] [G loss: 0.462700]\n",
      "epoch:6 step:6035 [D loss: 0.574697, acc.: 71.88%] [G loss: 0.563027]\n",
      "epoch:6 step:6036 [D loss: 0.606481, acc.: 69.53%] [G loss: 0.545936]\n",
      "epoch:6 step:6037 [D loss: 0.511530, acc.: 73.44%] [G loss: 0.608054]\n",
      "epoch:6 step:6038 [D loss: 0.533511, acc.: 73.44%] [G loss: 0.613267]\n",
      "epoch:6 step:6039 [D loss: 0.626456, acc.: 60.94%] [G loss: 0.515442]\n",
      "epoch:6 step:6040 [D loss: 0.619983, acc.: 63.28%] [G loss: 0.367232]\n",
      "epoch:6 step:6041 [D loss: 0.565973, acc.: 67.19%] [G loss: 0.507750]\n",
      "epoch:6 step:6042 [D loss: 0.581086, acc.: 69.53%] [G loss: 0.522164]\n",
      "epoch:6 step:6043 [D loss: 0.626977, acc.: 64.06%] [G loss: 0.441009]\n",
      "epoch:6 step:6044 [D loss: 0.599238, acc.: 64.84%] [G loss: 0.457343]\n",
      "epoch:6 step:6045 [D loss: 0.559519, acc.: 67.97%] [G loss: 0.454779]\n",
      "epoch:6 step:6046 [D loss: 0.590383, acc.: 64.84%] [G loss: 0.503044]\n",
      "epoch:6 step:6047 [D loss: 0.552677, acc.: 75.78%] [G loss: 0.530874]\n",
      "epoch:6 step:6048 [D loss: 0.446757, acc.: 82.03%] [G loss: 0.500517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6049 [D loss: 0.542798, acc.: 71.09%] [G loss: 0.648196]\n",
      "epoch:6 step:6050 [D loss: 0.463449, acc.: 78.91%] [G loss: 0.771419]\n",
      "epoch:6 step:6051 [D loss: 0.467344, acc.: 75.78%] [G loss: 0.770435]\n",
      "epoch:6 step:6052 [D loss: 0.543951, acc.: 73.44%] [G loss: 0.661278]\n",
      "epoch:6 step:6053 [D loss: 0.559228, acc.: 69.53%] [G loss: 0.603905]\n",
      "epoch:6 step:6054 [D loss: 0.578475, acc.: 70.31%] [G loss: 0.595836]\n",
      "epoch:6 step:6055 [D loss: 0.588035, acc.: 66.41%] [G loss: 0.452640]\n",
      "epoch:6 step:6056 [D loss: 0.519462, acc.: 76.56%] [G loss: 0.464758]\n",
      "epoch:6 step:6057 [D loss: 0.559048, acc.: 75.78%] [G loss: 0.537453]\n",
      "epoch:6 step:6058 [D loss: 0.482108, acc.: 78.91%] [G loss: 0.586925]\n",
      "epoch:6 step:6059 [D loss: 0.657309, acc.: 62.50%] [G loss: 0.399961]\n",
      "epoch:6 step:6060 [D loss: 0.601230, acc.: 61.72%] [G loss: 0.454433]\n",
      "epoch:6 step:6061 [D loss: 0.503206, acc.: 74.22%] [G loss: 0.613053]\n",
      "epoch:6 step:6062 [D loss: 0.542414, acc.: 67.97%] [G loss: 0.564852]\n",
      "epoch:6 step:6063 [D loss: 0.580995, acc.: 71.09%] [G loss: 0.617825]\n",
      "epoch:6 step:6064 [D loss: 0.538309, acc.: 70.31%] [G loss: 0.465887]\n",
      "epoch:6 step:6065 [D loss: 0.536754, acc.: 74.22%] [G loss: 0.535063]\n",
      "epoch:6 step:6066 [D loss: 0.545465, acc.: 72.66%] [G loss: 0.411669]\n",
      "epoch:6 step:6067 [D loss: 0.542090, acc.: 73.44%] [G loss: 0.565870]\n",
      "epoch:6 step:6068 [D loss: 0.541452, acc.: 69.53%] [G loss: 0.679783]\n",
      "epoch:6 step:6069 [D loss: 0.544916, acc.: 73.44%] [G loss: 0.593658]\n",
      "epoch:6 step:6070 [D loss: 0.555978, acc.: 77.34%] [G loss: 0.629753]\n",
      "epoch:6 step:6071 [D loss: 0.507092, acc.: 74.22%] [G loss: 0.623902]\n",
      "epoch:6 step:6072 [D loss: 0.488327, acc.: 75.00%] [G loss: 0.674443]\n",
      "epoch:6 step:6073 [D loss: 0.446864, acc.: 79.69%] [G loss: 0.567170]\n",
      "epoch:6 step:6074 [D loss: 0.498411, acc.: 77.34%] [G loss: 0.660547]\n",
      "epoch:6 step:6075 [D loss: 0.536043, acc.: 71.09%] [G loss: 0.596732]\n",
      "epoch:6 step:6076 [D loss: 0.557058, acc.: 71.88%] [G loss: 0.623600]\n",
      "epoch:6 step:6077 [D loss: 0.596960, acc.: 67.97%] [G loss: 0.555083]\n",
      "epoch:6 step:6078 [D loss: 0.624737, acc.: 68.75%] [G loss: 0.451851]\n",
      "epoch:6 step:6079 [D loss: 0.571659, acc.: 67.19%] [G loss: 0.467009]\n",
      "epoch:6 step:6080 [D loss: 0.551388, acc.: 71.88%] [G loss: 0.522655]\n",
      "epoch:6 step:6081 [D loss: 0.561134, acc.: 71.88%] [G loss: 0.496740]\n",
      "epoch:6 step:6082 [D loss: 0.479934, acc.: 76.56%] [G loss: 0.555060]\n",
      "epoch:6 step:6083 [D loss: 0.530687, acc.: 71.88%] [G loss: 0.532275]\n",
      "epoch:6 step:6084 [D loss: 0.591900, acc.: 61.72%] [G loss: 0.653511]\n",
      "epoch:6 step:6085 [D loss: 0.573865, acc.: 71.88%] [G loss: 0.528051]\n",
      "epoch:6 step:6086 [D loss: 0.571609, acc.: 68.75%] [G loss: 0.565653]\n",
      "epoch:6 step:6087 [D loss: 0.627145, acc.: 64.06%] [G loss: 0.477789]\n",
      "epoch:6 step:6088 [D loss: 0.600272, acc.: 67.19%] [G loss: 0.522830]\n",
      "epoch:6 step:6089 [D loss: 0.524039, acc.: 75.78%] [G loss: 0.659850]\n",
      "epoch:6 step:6090 [D loss: 0.581548, acc.: 71.09%] [G loss: 0.486793]\n",
      "epoch:6 step:6091 [D loss: 0.545310, acc.: 69.53%] [G loss: 0.565053]\n",
      "epoch:6 step:6092 [D loss: 0.539454, acc.: 71.09%] [G loss: 0.623854]\n",
      "epoch:6 step:6093 [D loss: 0.477679, acc.: 78.12%] [G loss: 0.607806]\n",
      "epoch:6 step:6094 [D loss: 0.415638, acc.: 82.81%] [G loss: 0.747192]\n",
      "epoch:6 step:6095 [D loss: 0.619231, acc.: 64.06%] [G loss: 0.657388]\n",
      "epoch:6 step:6096 [D loss: 0.532643, acc.: 71.88%] [G loss: 0.539459]\n",
      "epoch:6 step:6097 [D loss: 0.449828, acc.: 81.25%] [G loss: 0.819058]\n",
      "epoch:6 step:6098 [D loss: 0.552341, acc.: 74.22%] [G loss: 0.536900]\n",
      "epoch:6 step:6099 [D loss: 0.703702, acc.: 53.12%] [G loss: 0.454279]\n",
      "epoch:6 step:6100 [D loss: 0.598198, acc.: 65.62%] [G loss: 0.354030]\n",
      "epoch:6 step:6101 [D loss: 0.518730, acc.: 79.69%] [G loss: 0.411618]\n",
      "epoch:6 step:6102 [D loss: 0.549413, acc.: 76.56%] [G loss: 0.411980]\n",
      "epoch:6 step:6103 [D loss: 0.491379, acc.: 77.34%] [G loss: 0.436033]\n",
      "epoch:6 step:6104 [D loss: 0.654508, acc.: 61.72%] [G loss: 0.453232]\n",
      "epoch:6 step:6105 [D loss: 0.587599, acc.: 62.50%] [G loss: 0.457615]\n",
      "epoch:6 step:6106 [D loss: 0.520671, acc.: 75.78%] [G loss: 0.623804]\n",
      "epoch:6 step:6107 [D loss: 0.547058, acc.: 67.97%] [G loss: 0.613833]\n",
      "epoch:6 step:6108 [D loss: 0.588845, acc.: 68.75%] [G loss: 0.612270]\n",
      "epoch:6 step:6109 [D loss: 0.524024, acc.: 76.56%] [G loss: 0.502343]\n",
      "epoch:6 step:6110 [D loss: 0.521938, acc.: 69.53%] [G loss: 0.489325]\n",
      "epoch:6 step:6111 [D loss: 0.588820, acc.: 66.41%] [G loss: 0.468040]\n",
      "epoch:6 step:6112 [D loss: 0.590844, acc.: 71.09%] [G loss: 0.485517]\n",
      "epoch:6 step:6113 [D loss: 0.502377, acc.: 75.78%] [G loss: 0.605834]\n",
      "epoch:6 step:6114 [D loss: 0.543508, acc.: 73.44%] [G loss: 0.529833]\n",
      "epoch:6 step:6115 [D loss: 0.562182, acc.: 70.31%] [G loss: 0.449918]\n",
      "epoch:6 step:6116 [D loss: 0.585297, acc.: 65.62%] [G loss: 0.572693]\n",
      "epoch:6 step:6117 [D loss: 0.551715, acc.: 69.53%] [G loss: 0.434084]\n",
      "epoch:6 step:6118 [D loss: 0.568021, acc.: 67.97%] [G loss: 0.552376]\n",
      "epoch:6 step:6119 [D loss: 0.600746, acc.: 63.28%] [G loss: 0.540166]\n",
      "epoch:6 step:6120 [D loss: 0.478806, acc.: 76.56%] [G loss: 0.631553]\n",
      "epoch:6 step:6121 [D loss: 0.438332, acc.: 83.59%] [G loss: 0.745273]\n",
      "epoch:6 step:6122 [D loss: 0.618290, acc.: 58.59%] [G loss: 0.600694]\n",
      "epoch:6 step:6123 [D loss: 0.631390, acc.: 67.19%] [G loss: 0.345795]\n",
      "epoch:6 step:6124 [D loss: 0.577328, acc.: 66.41%] [G loss: 0.471528]\n",
      "epoch:6 step:6125 [D loss: 0.469364, acc.: 75.00%] [G loss: 0.577484]\n",
      "epoch:6 step:6126 [D loss: 0.474725, acc.: 75.00%] [G loss: 0.712866]\n",
      "epoch:6 step:6127 [D loss: 0.569031, acc.: 74.22%] [G loss: 0.590693]\n",
      "epoch:6 step:6128 [D loss: 0.512694, acc.: 73.44%] [G loss: 0.549918]\n",
      "epoch:6 step:6129 [D loss: 0.490746, acc.: 74.22%] [G loss: 0.655443]\n",
      "epoch:6 step:6130 [D loss: 0.448604, acc.: 81.25%] [G loss: 0.744376]\n",
      "epoch:6 step:6131 [D loss: 0.578938, acc.: 66.41%] [G loss: 0.557468]\n",
      "epoch:6 step:6132 [D loss: 0.603310, acc.: 65.62%] [G loss: 0.659390]\n",
      "epoch:6 step:6133 [D loss: 0.707735, acc.: 57.03%] [G loss: 0.555876]\n",
      "epoch:6 step:6134 [D loss: 0.609147, acc.: 64.06%] [G loss: 0.420310]\n",
      "epoch:6 step:6135 [D loss: 0.544854, acc.: 74.22%] [G loss: 0.520560]\n",
      "epoch:6 step:6136 [D loss: 0.590750, acc.: 64.84%] [G loss: 0.653857]\n",
      "epoch:6 step:6137 [D loss: 0.559587, acc.: 77.34%] [G loss: 0.742677]\n",
      "epoch:6 step:6138 [D loss: 0.498922, acc.: 75.78%] [G loss: 0.666740]\n",
      "epoch:6 step:6139 [D loss: 0.575339, acc.: 75.00%] [G loss: 0.673128]\n",
      "epoch:6 step:6140 [D loss: 0.529458, acc.: 71.09%] [G loss: 0.526721]\n",
      "epoch:6 step:6141 [D loss: 0.535696, acc.: 72.66%] [G loss: 0.526935]\n",
      "epoch:6 step:6142 [D loss: 0.501165, acc.: 72.66%] [G loss: 0.647628]\n",
      "epoch:6 step:6143 [D loss: 0.518048, acc.: 76.56%] [G loss: 0.571844]\n",
      "epoch:6 step:6144 [D loss: 0.544878, acc.: 65.62%] [G loss: 0.550351]\n",
      "epoch:6 step:6145 [D loss: 0.514193, acc.: 73.44%] [G loss: 0.600885]\n",
      "epoch:6 step:6146 [D loss: 0.545745, acc.: 67.19%] [G loss: 0.587032]\n",
      "epoch:6 step:6147 [D loss: 0.655842, acc.: 64.84%] [G loss: 0.502832]\n",
      "epoch:6 step:6148 [D loss: 0.535999, acc.: 71.09%] [G loss: 0.441382]\n",
      "epoch:6 step:6149 [D loss: 0.573584, acc.: 70.31%] [G loss: 0.412327]\n",
      "epoch:6 step:6150 [D loss: 0.609105, acc.: 64.06%] [G loss: 0.526911]\n",
      "epoch:6 step:6151 [D loss: 0.579054, acc.: 68.75%] [G loss: 0.456400]\n",
      "epoch:6 step:6152 [D loss: 0.523334, acc.: 75.78%] [G loss: 0.621833]\n",
      "epoch:6 step:6153 [D loss: 0.576274, acc.: 69.53%] [G loss: 0.518518]\n",
      "epoch:6 step:6154 [D loss: 0.555758, acc.: 70.31%] [G loss: 0.469786]\n",
      "epoch:6 step:6155 [D loss: 0.572810, acc.: 69.53%] [G loss: 0.501821]\n",
      "epoch:6 step:6156 [D loss: 0.529835, acc.: 71.88%] [G loss: 0.623206]\n",
      "epoch:6 step:6157 [D loss: 0.574629, acc.: 69.53%] [G loss: 0.512000]\n",
      "epoch:6 step:6158 [D loss: 0.534706, acc.: 69.53%] [G loss: 0.509950]\n",
      "epoch:6 step:6159 [D loss: 0.532402, acc.: 73.44%] [G loss: 0.630657]\n",
      "epoch:6 step:6160 [D loss: 0.555665, acc.: 68.75%] [G loss: 0.515408]\n",
      "epoch:6 step:6161 [D loss: 0.554716, acc.: 67.97%] [G loss: 0.478930]\n",
      "epoch:6 step:6162 [D loss: 0.566140, acc.: 67.19%] [G loss: 0.422392]\n",
      "epoch:6 step:6163 [D loss: 0.536033, acc.: 73.44%] [G loss: 0.433693]\n",
      "epoch:6 step:6164 [D loss: 0.597913, acc.: 69.53%] [G loss: 0.416263]\n",
      "epoch:6 step:6165 [D loss: 0.612824, acc.: 67.97%] [G loss: 0.419089]\n",
      "epoch:6 step:6166 [D loss: 0.572558, acc.: 71.88%] [G loss: 0.456086]\n",
      "epoch:6 step:6167 [D loss: 0.561543, acc.: 67.19%] [G loss: 0.630639]\n",
      "epoch:6 step:6168 [D loss: 0.441305, acc.: 82.81%] [G loss: 0.712705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6169 [D loss: 0.481456, acc.: 74.22%] [G loss: 0.705507]\n",
      "epoch:6 step:6170 [D loss: 0.531425, acc.: 72.66%] [G loss: 0.611468]\n",
      "epoch:6 step:6171 [D loss: 0.564854, acc.: 68.75%] [G loss: 0.493199]\n",
      "epoch:6 step:6172 [D loss: 0.572878, acc.: 71.88%] [G loss: 0.437643]\n",
      "epoch:6 step:6173 [D loss: 0.487805, acc.: 78.12%] [G loss: 0.556361]\n",
      "epoch:6 step:6174 [D loss: 0.464278, acc.: 80.47%] [G loss: 0.588693]\n",
      "epoch:6 step:6175 [D loss: 0.555916, acc.: 69.53%] [G loss: 0.447210]\n",
      "epoch:6 step:6176 [D loss: 0.458633, acc.: 78.91%] [G loss: 0.609463]\n",
      "epoch:6 step:6177 [D loss: 0.508803, acc.: 76.56%] [G loss: 0.622090]\n",
      "epoch:6 step:6178 [D loss: 0.488240, acc.: 78.91%] [G loss: 0.675457]\n",
      "epoch:6 step:6179 [D loss: 0.513206, acc.: 75.78%] [G loss: 0.618572]\n",
      "epoch:6 step:6180 [D loss: 0.463229, acc.: 80.47%] [G loss: 0.607778]\n",
      "epoch:6 step:6181 [D loss: 0.562422, acc.: 70.31%] [G loss: 0.535045]\n",
      "epoch:6 step:6182 [D loss: 0.598389, acc.: 63.28%] [G loss: 0.423466]\n",
      "epoch:6 step:6183 [D loss: 0.569381, acc.: 74.22%] [G loss: 0.524858]\n",
      "epoch:6 step:6184 [D loss: 0.633401, acc.: 63.28%] [G loss: 0.418757]\n",
      "epoch:6 step:6185 [D loss: 0.537673, acc.: 72.66%] [G loss: 0.487403]\n",
      "epoch:6 step:6186 [D loss: 0.499120, acc.: 75.00%] [G loss: 0.618135]\n",
      "epoch:6 step:6187 [D loss: 0.531334, acc.: 78.12%] [G loss: 0.551419]\n",
      "epoch:6 step:6188 [D loss: 0.700878, acc.: 61.72%] [G loss: 0.404383]\n",
      "epoch:6 step:6189 [D loss: 0.511064, acc.: 75.00%] [G loss: 0.450566]\n",
      "epoch:6 step:6190 [D loss: 0.560803, acc.: 71.09%] [G loss: 0.531420]\n",
      "epoch:6 step:6191 [D loss: 0.487925, acc.: 75.00%] [G loss: 0.739481]\n",
      "epoch:6 step:6192 [D loss: 0.567714, acc.: 68.75%] [G loss: 0.480710]\n",
      "epoch:6 step:6193 [D loss: 0.584688, acc.: 70.31%] [G loss: 0.543527]\n",
      "epoch:6 step:6194 [D loss: 0.546649, acc.: 72.66%] [G loss: 0.640442]\n",
      "epoch:6 step:6195 [D loss: 0.539969, acc.: 70.31%] [G loss: 0.492813]\n",
      "epoch:6 step:6196 [D loss: 0.511679, acc.: 75.78%] [G loss: 0.656347]\n",
      "epoch:6 step:6197 [D loss: 0.483875, acc.: 75.00%] [G loss: 0.732120]\n",
      "epoch:6 step:6198 [D loss: 0.624086, acc.: 64.84%] [G loss: 0.497682]\n",
      "epoch:6 step:6199 [D loss: 0.569288, acc.: 71.88%] [G loss: 0.579440]\n",
      "epoch:6 step:6200 [D loss: 0.594089, acc.: 65.62%] [G loss: 0.490963]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.097932\n",
      "FID: 44.199398\n",
      "0 = 12.86072589368822\n",
      "1 = 0.09450355313196128\n",
      "2 = 0.9283000230789185\n",
      "3 = 0.8655999898910522\n",
      "4 = 0.9909999966621399\n",
      "5 = 0.9897095561027527\n",
      "6 = 0.8655999898910522\n",
      "7 = 8.123847870564454\n",
      "8 = 0.13064420860000897\n",
      "9 = 0.7763000130653381\n",
      "10 = 0.7552000284194946\n",
      "11 = 0.7973999977111816\n",
      "12 = 0.7884736061096191\n",
      "13 = 0.7552000284194946\n",
      "14 = 6.097952842712402\n",
      "15 = 7.559650897979736\n",
      "16 = 0.3371804654598236\n",
      "17 = 6.097931861877441\n",
      "18 = 44.199398040771484\n",
      "epoch:6 step:6201 [D loss: 0.539287, acc.: 70.31%] [G loss: 0.510105]\n",
      "epoch:6 step:6202 [D loss: 0.527023, acc.: 72.66%] [G loss: 0.533789]\n",
      "epoch:6 step:6203 [D loss: 0.550909, acc.: 70.31%] [G loss: 0.578717]\n",
      "epoch:6 step:6204 [D loss: 0.442825, acc.: 81.25%] [G loss: 0.749704]\n",
      "epoch:6 step:6205 [D loss: 0.605390, acc.: 65.62%] [G loss: 0.624856]\n",
      "epoch:6 step:6206 [D loss: 0.625851, acc.: 64.84%] [G loss: 0.550228]\n",
      "epoch:6 step:6207 [D loss: 0.549772, acc.: 71.88%] [G loss: 0.505540]\n",
      "epoch:6 step:6208 [D loss: 0.580842, acc.: 68.75%] [G loss: 0.449278]\n",
      "epoch:6 step:6209 [D loss: 0.624237, acc.: 61.72%] [G loss: 0.387005]\n",
      "epoch:6 step:6210 [D loss: 0.581893, acc.: 67.19%] [G loss: 0.467222]\n",
      "epoch:6 step:6211 [D loss: 0.537190, acc.: 70.31%] [G loss: 0.558001]\n",
      "epoch:6 step:6212 [D loss: 0.559722, acc.: 71.09%] [G loss: 0.600947]\n",
      "epoch:6 step:6213 [D loss: 0.535248, acc.: 71.88%] [G loss: 0.573507]\n",
      "epoch:6 step:6214 [D loss: 0.482183, acc.: 79.69%] [G loss: 0.609261]\n",
      "epoch:6 step:6215 [D loss: 0.553789, acc.: 68.75%] [G loss: 0.556101]\n",
      "epoch:6 step:6216 [D loss: 0.566328, acc.: 70.31%] [G loss: 0.498213]\n",
      "epoch:6 step:6217 [D loss: 0.534911, acc.: 69.53%] [G loss: 0.539839]\n",
      "epoch:6 step:6218 [D loss: 0.590091, acc.: 68.75%] [G loss: 0.392766]\n",
      "epoch:6 step:6219 [D loss: 0.570804, acc.: 66.41%] [G loss: 0.529855]\n",
      "epoch:6 step:6220 [D loss: 0.503669, acc.: 78.91%] [G loss: 0.546246]\n",
      "epoch:6 step:6221 [D loss: 0.562694, acc.: 69.53%] [G loss: 0.595455]\n",
      "epoch:6 step:6222 [D loss: 0.606733, acc.: 69.53%] [G loss: 0.341333]\n",
      "epoch:6 step:6223 [D loss: 0.553871, acc.: 71.09%] [G loss: 0.510404]\n",
      "epoch:6 step:6224 [D loss: 0.515095, acc.: 75.00%] [G loss: 0.517294]\n",
      "epoch:6 step:6225 [D loss: 0.533044, acc.: 76.56%] [G loss: 0.489335]\n",
      "epoch:6 step:6226 [D loss: 0.591001, acc.: 68.75%] [G loss: 0.500355]\n",
      "epoch:6 step:6227 [D loss: 0.485036, acc.: 71.88%] [G loss: 0.558893]\n",
      "epoch:6 step:6228 [D loss: 0.589824, acc.: 64.84%] [G loss: 0.472478]\n",
      "epoch:6 step:6229 [D loss: 0.577662, acc.: 62.50%] [G loss: 0.410297]\n",
      "epoch:6 step:6230 [D loss: 0.560793, acc.: 70.31%] [G loss: 0.512440]\n",
      "epoch:6 step:6231 [D loss: 0.507093, acc.: 75.00%] [G loss: 0.521254]\n",
      "epoch:6 step:6232 [D loss: 0.563853, acc.: 75.00%] [G loss: 0.375765]\n",
      "epoch:6 step:6233 [D loss: 0.494331, acc.: 74.22%] [G loss: 0.592290]\n",
      "epoch:6 step:6234 [D loss: 0.607649, acc.: 64.06%] [G loss: 0.368889]\n",
      "epoch:6 step:6235 [D loss: 0.489192, acc.: 75.78%] [G loss: 0.500276]\n",
      "epoch:6 step:6236 [D loss: 0.560634, acc.: 69.53%] [G loss: 0.576987]\n",
      "epoch:6 step:6237 [D loss: 0.613095, acc.: 66.41%] [G loss: 0.511950]\n",
      "epoch:6 step:6238 [D loss: 0.557038, acc.: 71.88%] [G loss: 0.597059]\n",
      "epoch:6 step:6239 [D loss: 0.525700, acc.: 71.88%] [G loss: 0.720441]\n",
      "epoch:6 step:6240 [D loss: 0.504481, acc.: 73.44%] [G loss: 0.725334]\n",
      "epoch:6 step:6241 [D loss: 0.534581, acc.: 71.88%] [G loss: 0.544607]\n",
      "epoch:6 step:6242 [D loss: 0.539943, acc.: 72.66%] [G loss: 0.640139]\n",
      "epoch:6 step:6243 [D loss: 0.628230, acc.: 67.97%] [G loss: 0.424812]\n",
      "epoch:6 step:6244 [D loss: 0.614416, acc.: 67.97%] [G loss: 0.505225]\n",
      "epoch:6 step:6245 [D loss: 0.511791, acc.: 76.56%] [G loss: 0.520032]\n",
      "epoch:6 step:6246 [D loss: 0.490066, acc.: 75.00%] [G loss: 0.544402]\n",
      "epoch:6 step:6247 [D loss: 0.565687, acc.: 70.31%] [G loss: 0.431049]\n",
      "epoch:6 step:6248 [D loss: 0.537332, acc.: 70.31%] [G loss: 0.456213]\n",
      "epoch:6 step:6249 [D loss: 0.557329, acc.: 69.53%] [G loss: 0.516848]\n",
      "epoch:6 step:6250 [D loss: 0.598755, acc.: 64.84%] [G loss: 0.481452]\n",
      "epoch:6 step:6251 [D loss: 0.487774, acc.: 76.56%] [G loss: 0.520149]\n",
      "epoch:6 step:6252 [D loss: 0.540504, acc.: 71.88%] [G loss: 0.622425]\n",
      "epoch:6 step:6253 [D loss: 0.543876, acc.: 75.78%] [G loss: 0.478910]\n",
      "epoch:6 step:6254 [D loss: 0.511546, acc.: 73.44%] [G loss: 0.547721]\n",
      "epoch:6 step:6255 [D loss: 0.481892, acc.: 78.91%] [G loss: 0.651754]\n",
      "epoch:6 step:6256 [D loss: 0.478607, acc.: 77.34%] [G loss: 0.666386]\n",
      "epoch:6 step:6257 [D loss: 0.489093, acc.: 72.66%] [G loss: 0.697124]\n",
      "epoch:6 step:6258 [D loss: 0.586832, acc.: 71.09%] [G loss: 0.432438]\n",
      "epoch:6 step:6259 [D loss: 0.546201, acc.: 71.09%] [G loss: 0.500862]\n",
      "epoch:6 step:6260 [D loss: 0.515126, acc.: 74.22%] [G loss: 0.461546]\n",
      "epoch:6 step:6261 [D loss: 0.494479, acc.: 73.44%] [G loss: 0.510877]\n",
      "epoch:6 step:6262 [D loss: 0.519474, acc.: 71.09%] [G loss: 0.735015]\n",
      "epoch:6 step:6263 [D loss: 0.487940, acc.: 78.12%] [G loss: 0.596414]\n",
      "epoch:6 step:6264 [D loss: 0.467912, acc.: 82.81%] [G loss: 0.698760]\n",
      "epoch:6 step:6265 [D loss: 0.519780, acc.: 72.66%] [G loss: 0.585619]\n",
      "epoch:6 step:6266 [D loss: 0.562137, acc.: 71.88%] [G loss: 0.516657]\n",
      "epoch:6 step:6267 [D loss: 0.566678, acc.: 72.66%] [G loss: 0.548466]\n",
      "epoch:6 step:6268 [D loss: 0.537903, acc.: 73.44%] [G loss: 0.474205]\n",
      "epoch:6 step:6269 [D loss: 0.456519, acc.: 81.25%] [G loss: 0.716443]\n",
      "epoch:6 step:6270 [D loss: 0.407594, acc.: 82.81%] [G loss: 0.803463]\n",
      "epoch:6 step:6271 [D loss: 0.458655, acc.: 79.69%] [G loss: 0.791996]\n",
      "epoch:6 step:6272 [D loss: 0.498702, acc.: 75.00%] [G loss: 0.828927]\n",
      "epoch:6 step:6273 [D loss: 0.541069, acc.: 75.00%] [G loss: 0.684387]\n",
      "epoch:6 step:6274 [D loss: 0.676484, acc.: 61.72%] [G loss: 0.494502]\n",
      "epoch:6 step:6275 [D loss: 0.610677, acc.: 64.84%] [G loss: 0.454891]\n",
      "epoch:6 step:6276 [D loss: 0.479521, acc.: 76.56%] [G loss: 0.550128]\n",
      "epoch:6 step:6277 [D loss: 0.560862, acc.: 70.31%] [G loss: 0.470608]\n",
      "epoch:6 step:6278 [D loss: 0.542016, acc.: 71.09%] [G loss: 0.467428]\n",
      "epoch:6 step:6279 [D loss: 0.549847, acc.: 70.31%] [G loss: 0.615690]\n",
      "epoch:6 step:6280 [D loss: 0.562351, acc.: 69.53%] [G loss: 0.571232]\n",
      "epoch:6 step:6281 [D loss: 0.520903, acc.: 71.09%] [G loss: 0.647676]\n",
      "epoch:6 step:6282 [D loss: 0.523334, acc.: 74.22%] [G loss: 0.582143]\n",
      "epoch:6 step:6283 [D loss: 0.497353, acc.: 78.12%] [G loss: 0.536801]\n",
      "epoch:6 step:6284 [D loss: 0.558419, acc.: 71.88%] [G loss: 0.569084]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6285 [D loss: 0.548378, acc.: 75.00%] [G loss: 0.494783]\n",
      "epoch:6 step:6286 [D loss: 0.544865, acc.: 71.88%] [G loss: 0.495084]\n",
      "epoch:6 step:6287 [D loss: 0.598899, acc.: 66.41%] [G loss: 0.416863]\n",
      "epoch:6 step:6288 [D loss: 0.540908, acc.: 75.00%] [G loss: 0.475293]\n",
      "epoch:6 step:6289 [D loss: 0.611884, acc.: 63.28%] [G loss: 0.542263]\n",
      "epoch:6 step:6290 [D loss: 0.570498, acc.: 69.53%] [G loss: 0.527099]\n",
      "epoch:6 step:6291 [D loss: 0.530720, acc.: 73.44%] [G loss: 0.525823]\n",
      "epoch:6 step:6292 [D loss: 0.515247, acc.: 70.31%] [G loss: 0.524675]\n",
      "epoch:6 step:6293 [D loss: 0.555188, acc.: 67.97%] [G loss: 0.494345]\n",
      "epoch:6 step:6294 [D loss: 0.589087, acc.: 66.41%] [G loss: 0.505805]\n",
      "epoch:6 step:6295 [D loss: 0.585851, acc.: 65.62%] [G loss: 0.617309]\n",
      "epoch:6 step:6296 [D loss: 0.618866, acc.: 67.19%] [G loss: 0.557951]\n",
      "epoch:6 step:6297 [D loss: 0.526891, acc.: 71.88%] [G loss: 0.515429]\n",
      "epoch:6 step:6298 [D loss: 0.573302, acc.: 64.84%] [G loss: 0.477130]\n",
      "epoch:6 step:6299 [D loss: 0.510684, acc.: 72.66%] [G loss: 0.468137]\n",
      "epoch:6 step:6300 [D loss: 0.539518, acc.: 68.75%] [G loss: 0.513876]\n",
      "epoch:6 step:6301 [D loss: 0.535778, acc.: 71.88%] [G loss: 0.415515]\n",
      "epoch:6 step:6302 [D loss: 0.553043, acc.: 67.97%] [G loss: 0.533749]\n",
      "epoch:6 step:6303 [D loss: 0.469818, acc.: 78.91%] [G loss: 0.665095]\n",
      "epoch:6 step:6304 [D loss: 0.558196, acc.: 69.53%] [G loss: 0.551550]\n",
      "epoch:6 step:6305 [D loss: 0.612474, acc.: 64.84%] [G loss: 0.422291]\n",
      "epoch:6 step:6306 [D loss: 0.590195, acc.: 63.28%] [G loss: 0.649398]\n",
      "epoch:6 step:6307 [D loss: 0.514032, acc.: 74.22%] [G loss: 0.617857]\n",
      "epoch:6 step:6308 [D loss: 0.556529, acc.: 75.78%] [G loss: 0.511548]\n",
      "epoch:6 step:6309 [D loss: 0.548246, acc.: 73.44%] [G loss: 0.501253]\n",
      "epoch:6 step:6310 [D loss: 0.605797, acc.: 64.84%] [G loss: 0.464176]\n",
      "epoch:6 step:6311 [D loss: 0.533211, acc.: 68.75%] [G loss: 0.637996]\n",
      "epoch:6 step:6312 [D loss: 0.524806, acc.: 74.22%] [G loss: 0.607008]\n",
      "epoch:6 step:6313 [D loss: 0.491515, acc.: 75.78%] [G loss: 0.644064]\n",
      "epoch:6 step:6314 [D loss: 0.592009, acc.: 72.66%] [G loss: 0.659012]\n",
      "epoch:6 step:6315 [D loss: 0.487622, acc.: 81.25%] [G loss: 0.665660]\n",
      "epoch:6 step:6316 [D loss: 0.542283, acc.: 74.22%] [G loss: 0.655217]\n",
      "epoch:6 step:6317 [D loss: 0.549801, acc.: 76.56%] [G loss: 0.561337]\n",
      "epoch:6 step:6318 [D loss: 0.607297, acc.: 61.72%] [G loss: 0.486388]\n",
      "epoch:6 step:6319 [D loss: 0.521824, acc.: 73.44%] [G loss: 0.600348]\n",
      "epoch:6 step:6320 [D loss: 0.530397, acc.: 75.78%] [G loss: 0.518138]\n",
      "epoch:6 step:6321 [D loss: 0.531488, acc.: 73.44%] [G loss: 0.527909]\n",
      "epoch:6 step:6322 [D loss: 0.548165, acc.: 73.44%] [G loss: 0.510072]\n",
      "epoch:6 step:6323 [D loss: 0.543947, acc.: 75.78%] [G loss: 0.717471]\n",
      "epoch:6 step:6324 [D loss: 0.552952, acc.: 69.53%] [G loss: 0.616132]\n",
      "epoch:6 step:6325 [D loss: 0.558204, acc.: 67.97%] [G loss: 0.607177]\n",
      "epoch:6 step:6326 [D loss: 0.598514, acc.: 67.19%] [G loss: 0.448386]\n",
      "epoch:6 step:6327 [D loss: 0.557703, acc.: 71.09%] [G loss: 0.577807]\n",
      "epoch:6 step:6328 [D loss: 0.558821, acc.: 75.00%] [G loss: 0.603505]\n",
      "epoch:6 step:6329 [D loss: 0.517468, acc.: 75.00%] [G loss: 0.543004]\n",
      "epoch:6 step:6330 [D loss: 0.529613, acc.: 75.78%] [G loss: 0.764493]\n",
      "epoch:6 step:6331 [D loss: 0.497993, acc.: 76.56%] [G loss: 0.627239]\n",
      "epoch:6 step:6332 [D loss: 0.622755, acc.: 65.62%] [G loss: 0.365119]\n",
      "epoch:6 step:6333 [D loss: 0.600293, acc.: 65.62%] [G loss: 0.468424]\n",
      "epoch:6 step:6334 [D loss: 0.528488, acc.: 71.09%] [G loss: 0.616348]\n",
      "epoch:6 step:6335 [D loss: 0.553404, acc.: 71.09%] [G loss: 0.511273]\n",
      "epoch:6 step:6336 [D loss: 0.589344, acc.: 66.41%] [G loss: 0.476647]\n",
      "epoch:6 step:6337 [D loss: 0.543435, acc.: 71.88%] [G loss: 0.528553]\n",
      "epoch:6 step:6338 [D loss: 0.677611, acc.: 58.59%] [G loss: 0.515262]\n",
      "epoch:6 step:6339 [D loss: 0.607912, acc.: 65.62%] [G loss: 0.425948]\n",
      "epoch:6 step:6340 [D loss: 0.579207, acc.: 64.84%] [G loss: 0.574704]\n",
      "epoch:6 step:6341 [D loss: 0.534651, acc.: 74.22%] [G loss: 0.501150]\n",
      "epoch:6 step:6342 [D loss: 0.577330, acc.: 72.66%] [G loss: 0.478197]\n",
      "epoch:6 step:6343 [D loss: 0.555563, acc.: 73.44%] [G loss: 0.457541]\n",
      "epoch:6 step:6344 [D loss: 0.595245, acc.: 64.06%] [G loss: 0.417696]\n",
      "epoch:6 step:6345 [D loss: 0.551079, acc.: 68.75%] [G loss: 0.475549]\n",
      "epoch:6 step:6346 [D loss: 0.546495, acc.: 66.41%] [G loss: 0.458412]\n",
      "epoch:6 step:6347 [D loss: 0.488626, acc.: 75.00%] [G loss: 0.726219]\n",
      "epoch:6 step:6348 [D loss: 0.496842, acc.: 75.78%] [G loss: 0.480123]\n",
      "epoch:6 step:6349 [D loss: 0.582990, acc.: 70.31%] [G loss: 0.405968]\n",
      "epoch:6 step:6350 [D loss: 0.583887, acc.: 70.31%] [G loss: 0.444928]\n",
      "epoch:6 step:6351 [D loss: 0.584116, acc.: 67.19%] [G loss: 0.428656]\n",
      "epoch:6 step:6352 [D loss: 0.490906, acc.: 81.25%] [G loss: 0.451257]\n",
      "epoch:6 step:6353 [D loss: 0.589361, acc.: 62.50%] [G loss: 0.497105]\n",
      "epoch:6 step:6354 [D loss: 0.539817, acc.: 74.22%] [G loss: 0.512517]\n",
      "epoch:6 step:6355 [D loss: 0.530217, acc.: 71.88%] [G loss: 0.605157]\n",
      "epoch:6 step:6356 [D loss: 0.549330, acc.: 70.31%] [G loss: 0.509794]\n",
      "epoch:6 step:6357 [D loss: 0.577603, acc.: 69.53%] [G loss: 0.489857]\n",
      "epoch:6 step:6358 [D loss: 0.466244, acc.: 73.44%] [G loss: 0.494438]\n",
      "epoch:6 step:6359 [D loss: 0.542170, acc.: 71.09%] [G loss: 0.573546]\n",
      "epoch:6 step:6360 [D loss: 0.526824, acc.: 66.41%] [G loss: 0.475053]\n",
      "epoch:6 step:6361 [D loss: 0.599271, acc.: 65.62%] [G loss: 0.469425]\n",
      "epoch:6 step:6362 [D loss: 0.637650, acc.: 62.50%] [G loss: 0.496515]\n",
      "epoch:6 step:6363 [D loss: 0.538198, acc.: 72.66%] [G loss: 0.550516]\n",
      "epoch:6 step:6364 [D loss: 0.571244, acc.: 69.53%] [G loss: 0.589623]\n",
      "epoch:6 step:6365 [D loss: 0.501000, acc.: 75.00%] [G loss: 0.610174]\n",
      "epoch:6 step:6366 [D loss: 0.596949, acc.: 71.88%] [G loss: 0.563917]\n",
      "epoch:6 step:6367 [D loss: 0.657411, acc.: 58.59%] [G loss: 0.565729]\n",
      "epoch:6 step:6368 [D loss: 0.478955, acc.: 80.47%] [G loss: 0.608728]\n",
      "epoch:6 step:6369 [D loss: 0.443548, acc.: 78.91%] [G loss: 0.722524]\n",
      "epoch:6 step:6370 [D loss: 0.565024, acc.: 67.19%] [G loss: 0.552570]\n",
      "epoch:6 step:6371 [D loss: 0.539487, acc.: 72.66%] [G loss: 0.502334]\n",
      "epoch:6 step:6372 [D loss: 0.502129, acc.: 76.56%] [G loss: 0.480933]\n",
      "epoch:6 step:6373 [D loss: 0.516402, acc.: 71.88%] [G loss: 0.539997]\n",
      "epoch:6 step:6374 [D loss: 0.585627, acc.: 69.53%] [G loss: 0.611984]\n",
      "epoch:6 step:6375 [D loss: 0.477147, acc.: 76.56%] [G loss: 0.800274]\n",
      "epoch:6 step:6376 [D loss: 0.552266, acc.: 67.19%] [G loss: 0.691307]\n",
      "epoch:6 step:6377 [D loss: 0.552076, acc.: 68.75%] [G loss: 0.625398]\n",
      "epoch:6 step:6378 [D loss: 0.545384, acc.: 74.22%] [G loss: 0.609214]\n",
      "epoch:6 step:6379 [D loss: 0.536377, acc.: 71.88%] [G loss: 0.509610]\n",
      "epoch:6 step:6380 [D loss: 0.517400, acc.: 75.00%] [G loss: 0.571186]\n",
      "epoch:6 step:6381 [D loss: 0.573871, acc.: 70.31%] [G loss: 0.483003]\n",
      "epoch:6 step:6382 [D loss: 0.525118, acc.: 73.44%] [G loss: 0.513663]\n",
      "epoch:6 step:6383 [D loss: 0.569320, acc.: 69.53%] [G loss: 0.468032]\n",
      "epoch:6 step:6384 [D loss: 0.587560, acc.: 67.97%] [G loss: 0.543495]\n",
      "epoch:6 step:6385 [D loss: 0.538774, acc.: 71.09%] [G loss: 0.595190]\n",
      "epoch:6 step:6386 [D loss: 0.548863, acc.: 71.88%] [G loss: 0.517230]\n",
      "epoch:6 step:6387 [D loss: 0.608240, acc.: 63.28%] [G loss: 0.478459]\n",
      "epoch:6 step:6388 [D loss: 0.703761, acc.: 54.69%] [G loss: 0.336912]\n",
      "epoch:6 step:6389 [D loss: 0.499912, acc.: 73.44%] [G loss: 0.532628]\n",
      "epoch:6 step:6390 [D loss: 0.571407, acc.: 73.44%] [G loss: 0.678439]\n",
      "epoch:6 step:6391 [D loss: 0.438827, acc.: 85.94%] [G loss: 0.732995]\n",
      "epoch:6 step:6392 [D loss: 0.517474, acc.: 75.00%] [G loss: 0.687348]\n",
      "epoch:6 step:6393 [D loss: 0.515206, acc.: 73.44%] [G loss: 0.726707]\n",
      "epoch:6 step:6394 [D loss: 0.565555, acc.: 69.53%] [G loss: 0.493180]\n",
      "epoch:6 step:6395 [D loss: 0.539326, acc.: 67.97%] [G loss: 0.553534]\n",
      "epoch:6 step:6396 [D loss: 0.533495, acc.: 78.12%] [G loss: 0.519489]\n",
      "epoch:6 step:6397 [D loss: 0.553252, acc.: 70.31%] [G loss: 0.550486]\n",
      "epoch:6 step:6398 [D loss: 0.605813, acc.: 64.84%] [G loss: 0.591673]\n",
      "epoch:6 step:6399 [D loss: 0.524240, acc.: 74.22%] [G loss: 0.601838]\n",
      "epoch:6 step:6400 [D loss: 0.539835, acc.: 74.22%] [G loss: 0.602428]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.041002\n",
      "FID: 45.607643\n",
      "0 = 12.999441561031361\n",
      "1 = 0.09900256055352702\n",
      "2 = 0.9243999719619751\n",
      "3 = 0.8600000143051147\n",
      "4 = 0.9887999892234802\n",
      "5 = 0.9871441721916199\n",
      "6 = 0.8600000143051147\n",
      "7 = 8.30742110729217\n",
      "8 = 0.13636909459489832\n",
      "9 = 0.7806000113487244\n",
      "10 = 0.7620000243186951\n",
      "11 = 0.7991999983787537\n",
      "12 = 0.7914416193962097\n",
      "13 = 0.7620000243186951\n",
      "14 = 6.041026592254639\n",
      "15 = 7.269363880157471\n",
      "16 = 0.3706147074699402\n",
      "17 = 6.041001796722412\n",
      "18 = 45.607643127441406\n",
      "epoch:6 step:6401 [D loss: 0.608209, acc.: 64.06%] [G loss: 0.540462]\n",
      "epoch:6 step:6402 [D loss: 0.540212, acc.: 73.44%] [G loss: 0.506014]\n",
      "epoch:6 step:6403 [D loss: 0.496490, acc.: 78.91%] [G loss: 0.639983]\n",
      "epoch:6 step:6404 [D loss: 0.555260, acc.: 66.41%] [G loss: 0.660587]\n",
      "epoch:6 step:6405 [D loss: 0.550786, acc.: 70.31%] [G loss: 0.559446]\n",
      "epoch:6 step:6406 [D loss: 0.578737, acc.: 71.09%] [G loss: 0.620556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6407 [D loss: 0.580278, acc.: 67.97%] [G loss: 0.531848]\n",
      "epoch:6 step:6408 [D loss: 0.530032, acc.: 70.31%] [G loss: 0.574542]\n",
      "epoch:6 step:6409 [D loss: 0.581840, acc.: 67.19%] [G loss: 0.487650]\n",
      "epoch:6 step:6410 [D loss: 0.634699, acc.: 67.19%] [G loss: 0.470761]\n",
      "epoch:6 step:6411 [D loss: 0.536151, acc.: 71.88%] [G loss: 0.440466]\n",
      "epoch:6 step:6412 [D loss: 0.496506, acc.: 79.69%] [G loss: 0.546845]\n",
      "epoch:6 step:6413 [D loss: 0.522218, acc.: 75.78%] [G loss: 0.545809]\n",
      "epoch:6 step:6414 [D loss: 0.478589, acc.: 74.22%] [G loss: 0.617697]\n",
      "epoch:6 step:6415 [D loss: 0.620735, acc.: 65.62%] [G loss: 0.597123]\n",
      "epoch:6 step:6416 [D loss: 0.607373, acc.: 67.97%] [G loss: 0.515488]\n",
      "epoch:6 step:6417 [D loss: 0.533761, acc.: 73.44%] [G loss: 0.479622]\n",
      "epoch:6 step:6418 [D loss: 0.522317, acc.: 74.22%] [G loss: 0.621757]\n",
      "epoch:6 step:6419 [D loss: 0.557964, acc.: 70.31%] [G loss: 0.596191]\n",
      "epoch:6 step:6420 [D loss: 0.539710, acc.: 71.88%] [G loss: 0.545830]\n",
      "epoch:6 step:6421 [D loss: 0.581390, acc.: 70.31%] [G loss: 0.512015]\n",
      "epoch:6 step:6422 [D loss: 0.578318, acc.: 71.88%] [G loss: 0.402709]\n",
      "epoch:6 step:6423 [D loss: 0.517187, acc.: 72.66%] [G loss: 0.610092]\n",
      "epoch:6 step:6424 [D loss: 0.483724, acc.: 75.78%] [G loss: 0.724808]\n",
      "epoch:6 step:6425 [D loss: 0.476321, acc.: 78.91%] [G loss: 0.671325]\n",
      "epoch:6 step:6426 [D loss: 0.594814, acc.: 71.09%] [G loss: 0.511755]\n",
      "epoch:6 step:6427 [D loss: 0.570211, acc.: 70.31%] [G loss: 0.532647]\n",
      "epoch:6 step:6428 [D loss: 0.556518, acc.: 67.97%] [G loss: 0.561605]\n",
      "epoch:6 step:6429 [D loss: 0.504834, acc.: 75.78%] [G loss: 0.552101]\n",
      "epoch:6 step:6430 [D loss: 0.557901, acc.: 71.09%] [G loss: 0.551568]\n",
      "epoch:6 step:6431 [D loss: 0.552683, acc.: 70.31%] [G loss: 0.572887]\n",
      "epoch:6 step:6432 [D loss: 0.539708, acc.: 71.88%] [G loss: 0.675634]\n",
      "epoch:6 step:6433 [D loss: 0.539814, acc.: 75.00%] [G loss: 0.545034]\n",
      "epoch:6 step:6434 [D loss: 0.636425, acc.: 61.72%] [G loss: 0.493479]\n",
      "epoch:6 step:6435 [D loss: 0.568548, acc.: 62.50%] [G loss: 0.440701]\n",
      "epoch:6 step:6436 [D loss: 0.513228, acc.: 75.78%] [G loss: 0.556873]\n",
      "epoch:6 step:6437 [D loss: 0.522670, acc.: 71.88%] [G loss: 0.699304]\n",
      "epoch:6 step:6438 [D loss: 0.550317, acc.: 67.19%] [G loss: 0.533877]\n",
      "epoch:6 step:6439 [D loss: 0.606966, acc.: 62.50%] [G loss: 0.469401]\n",
      "epoch:6 step:6440 [D loss: 0.565776, acc.: 71.88%] [G loss: 0.445800]\n",
      "epoch:6 step:6441 [D loss: 0.534443, acc.: 71.88%] [G loss: 0.392615]\n",
      "epoch:6 step:6442 [D loss: 0.570931, acc.: 70.31%] [G loss: 0.453315]\n",
      "epoch:6 step:6443 [D loss: 0.544631, acc.: 71.09%] [G loss: 0.545316]\n",
      "epoch:6 step:6444 [D loss: 0.559103, acc.: 67.19%] [G loss: 0.404004]\n",
      "epoch:6 step:6445 [D loss: 0.452308, acc.: 75.78%] [G loss: 0.651221]\n",
      "epoch:6 step:6446 [D loss: 0.577821, acc.: 72.66%] [G loss: 0.553094]\n",
      "epoch:6 step:6447 [D loss: 0.602694, acc.: 71.09%] [G loss: 0.500657]\n",
      "epoch:6 step:6448 [D loss: 0.603487, acc.: 67.19%] [G loss: 0.420260]\n",
      "epoch:6 step:6449 [D loss: 0.611583, acc.: 68.75%] [G loss: 0.371566]\n",
      "epoch:6 step:6450 [D loss: 0.621012, acc.: 59.38%] [G loss: 0.387916]\n",
      "epoch:6 step:6451 [D loss: 0.539874, acc.: 73.44%] [G loss: 0.508881]\n",
      "epoch:6 step:6452 [D loss: 0.515981, acc.: 78.91%] [G loss: 0.532316]\n",
      "epoch:6 step:6453 [D loss: 0.528767, acc.: 71.09%] [G loss: 0.611078]\n",
      "epoch:6 step:6454 [D loss: 0.498131, acc.: 78.91%] [G loss: 0.478431]\n",
      "epoch:6 step:6455 [D loss: 0.549158, acc.: 71.88%] [G loss: 0.597714]\n",
      "epoch:6 step:6456 [D loss: 0.546837, acc.: 71.09%] [G loss: 0.622734]\n",
      "epoch:6 step:6457 [D loss: 0.528899, acc.: 71.88%] [G loss: 0.557409]\n",
      "epoch:6 step:6458 [D loss: 0.535210, acc.: 69.53%] [G loss: 0.512330]\n",
      "epoch:6 step:6459 [D loss: 0.571316, acc.: 71.09%] [G loss: 0.477404]\n",
      "epoch:6 step:6460 [D loss: 0.533301, acc.: 73.44%] [G loss: 0.511189]\n",
      "epoch:6 step:6461 [D loss: 0.609895, acc.: 64.84%] [G loss: 0.554415]\n",
      "epoch:6 step:6462 [D loss: 0.612683, acc.: 62.50%] [G loss: 0.408824]\n",
      "epoch:6 step:6463 [D loss: 0.538729, acc.: 75.00%] [G loss: 0.511648]\n",
      "epoch:6 step:6464 [D loss: 0.516868, acc.: 71.09%] [G loss: 0.503204]\n",
      "epoch:6 step:6465 [D loss: 0.587063, acc.: 72.66%] [G loss: 0.481135]\n",
      "epoch:6 step:6466 [D loss: 0.588781, acc.: 67.97%] [G loss: 0.511237]\n",
      "epoch:6 step:6467 [D loss: 0.600007, acc.: 64.06%] [G loss: 0.618683]\n",
      "epoch:6 step:6468 [D loss: 0.584334, acc.: 60.16%] [G loss: 0.431942]\n",
      "epoch:6 step:6469 [D loss: 0.594306, acc.: 65.62%] [G loss: 0.365881]\n",
      "epoch:6 step:6470 [D loss: 0.568460, acc.: 72.66%] [G loss: 0.503728]\n",
      "epoch:6 step:6471 [D loss: 0.641459, acc.: 60.94%] [G loss: 0.351800]\n",
      "epoch:6 step:6472 [D loss: 0.553366, acc.: 71.88%] [G loss: 0.417861]\n",
      "epoch:6 step:6473 [D loss: 0.615585, acc.: 60.16%] [G loss: 0.441379]\n",
      "epoch:6 step:6474 [D loss: 0.530241, acc.: 71.09%] [G loss: 0.539850]\n",
      "epoch:6 step:6475 [D loss: 0.511910, acc.: 75.78%] [G loss: 0.533743]\n",
      "epoch:6 step:6476 [D loss: 0.539809, acc.: 67.97%] [G loss: 0.467433]\n",
      "epoch:6 step:6477 [D loss: 0.557210, acc.: 73.44%] [G loss: 0.528001]\n",
      "epoch:6 step:6478 [D loss: 0.611232, acc.: 64.06%] [G loss: 0.558337]\n",
      "epoch:6 step:6479 [D loss: 0.490402, acc.: 73.44%] [G loss: 0.511305]\n",
      "epoch:6 step:6480 [D loss: 0.618058, acc.: 64.06%] [G loss: 0.494002]\n",
      "epoch:6 step:6481 [D loss: 0.578907, acc.: 71.88%] [G loss: 0.612028]\n",
      "epoch:6 step:6482 [D loss: 0.496120, acc.: 72.66%] [G loss: 0.570533]\n",
      "epoch:6 step:6483 [D loss: 0.624937, acc.: 63.28%] [G loss: 0.526430]\n",
      "epoch:6 step:6484 [D loss: 0.539085, acc.: 71.09%] [G loss: 0.508998]\n",
      "epoch:6 step:6485 [D loss: 0.585458, acc.: 63.28%] [G loss: 0.511845]\n",
      "epoch:6 step:6486 [D loss: 0.567201, acc.: 67.19%] [G loss: 0.536618]\n",
      "epoch:6 step:6487 [D loss: 0.537837, acc.: 70.31%] [G loss: 0.505807]\n",
      "epoch:6 step:6488 [D loss: 0.541672, acc.: 71.88%] [G loss: 0.493633]\n",
      "epoch:6 step:6489 [D loss: 0.652267, acc.: 64.06%] [G loss: 0.462202]\n",
      "epoch:6 step:6490 [D loss: 0.521531, acc.: 75.00%] [G loss: 0.508252]\n",
      "epoch:6 step:6491 [D loss: 0.552310, acc.: 71.09%] [G loss: 0.493996]\n",
      "epoch:6 step:6492 [D loss: 0.499025, acc.: 75.78%] [G loss: 0.586234]\n",
      "epoch:6 step:6493 [D loss: 0.503735, acc.: 71.88%] [G loss: 0.568991]\n",
      "epoch:6 step:6494 [D loss: 0.537310, acc.: 75.00%] [G loss: 0.572138]\n",
      "epoch:6 step:6495 [D loss: 0.567585, acc.: 71.88%] [G loss: 0.474033]\n",
      "epoch:6 step:6496 [D loss: 0.615616, acc.: 61.72%] [G loss: 0.445367]\n",
      "epoch:6 step:6497 [D loss: 0.511763, acc.: 72.66%] [G loss: 0.513062]\n",
      "epoch:6 step:6498 [D loss: 0.565729, acc.: 69.53%] [G loss: 0.532328]\n",
      "epoch:6 step:6499 [D loss: 0.555114, acc.: 71.88%] [G loss: 0.419502]\n",
      "epoch:6 step:6500 [D loss: 0.567232, acc.: 72.66%] [G loss: 0.555739]\n",
      "epoch:6 step:6501 [D loss: 0.564799, acc.: 66.41%] [G loss: 0.463340]\n",
      "epoch:6 step:6502 [D loss: 0.679936, acc.: 56.25%] [G loss: 0.430729]\n",
      "epoch:6 step:6503 [D loss: 0.572410, acc.: 62.50%] [G loss: 0.420072]\n",
      "epoch:6 step:6504 [D loss: 0.548174, acc.: 75.78%] [G loss: 0.456143]\n",
      "epoch:6 step:6505 [D loss: 0.558489, acc.: 71.88%] [G loss: 0.498073]\n",
      "epoch:6 step:6506 [D loss: 0.535746, acc.: 73.44%] [G loss: 0.542643]\n",
      "epoch:6 step:6507 [D loss: 0.507926, acc.: 74.22%] [G loss: 0.698809]\n",
      "epoch:6 step:6508 [D loss: 0.581114, acc.: 63.28%] [G loss: 0.509869]\n",
      "epoch:6 step:6509 [D loss: 0.509852, acc.: 76.56%] [G loss: 0.663566]\n",
      "epoch:6 step:6510 [D loss: 0.564561, acc.: 69.53%] [G loss: 0.561067]\n",
      "epoch:6 step:6511 [D loss: 0.527126, acc.: 71.88%] [G loss: 0.555764]\n",
      "epoch:6 step:6512 [D loss: 0.459585, acc.: 83.59%] [G loss: 0.545792]\n",
      "epoch:6 step:6513 [D loss: 0.583336, acc.: 67.97%] [G loss: 0.542402]\n",
      "epoch:6 step:6514 [D loss: 0.693968, acc.: 60.16%] [G loss: 0.353474]\n",
      "epoch:6 step:6515 [D loss: 0.534795, acc.: 70.31%] [G loss: 0.623450]\n",
      "epoch:6 step:6516 [D loss: 0.468372, acc.: 74.22%] [G loss: 0.621572]\n",
      "epoch:6 step:6517 [D loss: 0.547042, acc.: 72.66%] [G loss: 0.538644]\n",
      "epoch:6 step:6518 [D loss: 0.486456, acc.: 78.12%] [G loss: 0.665230]\n",
      "epoch:6 step:6519 [D loss: 0.523446, acc.: 69.53%] [G loss: 0.617463]\n",
      "epoch:6 step:6520 [D loss: 0.463692, acc.: 80.47%] [G loss: 0.581756]\n",
      "epoch:6 step:6521 [D loss: 0.517568, acc.: 78.12%] [G loss: 0.616739]\n",
      "epoch:6 step:6522 [D loss: 0.535488, acc.: 75.78%] [G loss: 0.773911]\n",
      "epoch:6 step:6523 [D loss: 0.503544, acc.: 71.09%] [G loss: 0.680564]\n",
      "epoch:6 step:6524 [D loss: 0.606432, acc.: 67.19%] [G loss: 0.566373]\n",
      "epoch:6 step:6525 [D loss: 0.573768, acc.: 69.53%] [G loss: 0.571755]\n",
      "epoch:6 step:6526 [D loss: 0.585920, acc.: 65.62%] [G loss: 0.382545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6527 [D loss: 0.627330, acc.: 67.97%] [G loss: 0.377875]\n",
      "epoch:6 step:6528 [D loss: 0.485483, acc.: 77.34%] [G loss: 0.600855]\n",
      "epoch:6 step:6529 [D loss: 0.501540, acc.: 72.66%] [G loss: 0.515713]\n",
      "epoch:6 step:6530 [D loss: 0.577062, acc.: 67.19%] [G loss: 0.391394]\n",
      "epoch:6 step:6531 [D loss: 0.562044, acc.: 72.66%] [G loss: 0.543662]\n",
      "epoch:6 step:6532 [D loss: 0.475749, acc.: 76.56%] [G loss: 0.626214]\n",
      "epoch:6 step:6533 [D loss: 0.507760, acc.: 75.78%] [G loss: 0.623224]\n",
      "epoch:6 step:6534 [D loss: 0.485729, acc.: 75.78%] [G loss: 0.694805]\n",
      "epoch:6 step:6535 [D loss: 0.590619, acc.: 71.88%] [G loss: 0.699310]\n",
      "epoch:6 step:6536 [D loss: 0.482756, acc.: 75.00%] [G loss: 0.652712]\n",
      "epoch:6 step:6537 [D loss: 0.640549, acc.: 66.41%] [G loss: 0.559131]\n",
      "epoch:6 step:6538 [D loss: 0.577639, acc.: 67.19%] [G loss: 0.558188]\n",
      "epoch:6 step:6539 [D loss: 0.623354, acc.: 63.28%] [G loss: 0.554458]\n",
      "epoch:6 step:6540 [D loss: 0.476934, acc.: 77.34%] [G loss: 0.589745]\n",
      "epoch:6 step:6541 [D loss: 0.512405, acc.: 73.44%] [G loss: 0.529623]\n",
      "epoch:6 step:6542 [D loss: 0.740785, acc.: 60.16%] [G loss: 0.563286]\n",
      "epoch:6 step:6543 [D loss: 0.509614, acc.: 73.44%] [G loss: 0.576186]\n",
      "epoch:6 step:6544 [D loss: 0.604949, acc.: 64.06%] [G loss: 0.426679]\n",
      "epoch:6 step:6545 [D loss: 0.493892, acc.: 74.22%] [G loss: 0.719569]\n",
      "epoch:6 step:6546 [D loss: 0.438557, acc.: 78.12%] [G loss: 0.815313]\n",
      "epoch:6 step:6547 [D loss: 0.385374, acc.: 87.50%] [G loss: 0.867152]\n",
      "epoch:6 step:6548 [D loss: 0.501851, acc.: 78.12%] [G loss: 0.945433]\n",
      "epoch:6 step:6549 [D loss: 0.528383, acc.: 68.75%] [G loss: 1.193209]\n",
      "epoch:6 step:6550 [D loss: 0.782126, acc.: 59.38%] [G loss: 0.855021]\n",
      "epoch:6 step:6551 [D loss: 0.555983, acc.: 71.09%] [G loss: 0.818344]\n",
      "epoch:6 step:6552 [D loss: 0.448282, acc.: 75.78%] [G loss: 0.963034]\n",
      "epoch:6 step:6553 [D loss: 0.564606, acc.: 66.41%] [G loss: 0.772174]\n",
      "epoch:6 step:6554 [D loss: 0.571112, acc.: 66.41%] [G loss: 0.570020]\n",
      "epoch:6 step:6555 [D loss: 0.513184, acc.: 75.78%] [G loss: 0.626161]\n",
      "epoch:6 step:6556 [D loss: 0.563168, acc.: 70.31%] [G loss: 0.784235]\n",
      "epoch:6 step:6557 [D loss: 0.458274, acc.: 73.44%] [G loss: 0.898604]\n",
      "epoch:6 step:6558 [D loss: 0.423186, acc.: 75.78%] [G loss: 1.041487]\n",
      "epoch:6 step:6559 [D loss: 0.446472, acc.: 80.47%] [G loss: 1.243043]\n",
      "epoch:7 step:6560 [D loss: 0.554056, acc.: 71.88%] [G loss: 1.008612]\n",
      "epoch:7 step:6561 [D loss: 0.497937, acc.: 77.34%] [G loss: 0.878809]\n",
      "epoch:7 step:6562 [D loss: 0.568921, acc.: 75.00%] [G loss: 0.752314]\n",
      "epoch:7 step:6563 [D loss: 0.534957, acc.: 73.44%] [G loss: 0.670833]\n",
      "epoch:7 step:6564 [D loss: 0.520630, acc.: 72.66%] [G loss: 0.489348]\n",
      "epoch:7 step:6565 [D loss: 0.568934, acc.: 67.19%] [G loss: 0.496325]\n",
      "epoch:7 step:6566 [D loss: 0.492691, acc.: 75.00%] [G loss: 0.653301]\n",
      "epoch:7 step:6567 [D loss: 0.546378, acc.: 71.88%] [G loss: 0.626545]\n",
      "epoch:7 step:6568 [D loss: 0.512005, acc.: 75.00%] [G loss: 0.721842]\n",
      "epoch:7 step:6569 [D loss: 0.558800, acc.: 71.09%] [G loss: 0.665159]\n",
      "epoch:7 step:6570 [D loss: 0.505653, acc.: 75.00%] [G loss: 0.634686]\n",
      "epoch:7 step:6571 [D loss: 0.583811, acc.: 64.06%] [G loss: 0.440932]\n",
      "epoch:7 step:6572 [D loss: 0.552903, acc.: 71.09%] [G loss: 0.533173]\n",
      "epoch:7 step:6573 [D loss: 0.556692, acc.: 71.09%] [G loss: 0.471862]\n",
      "epoch:7 step:6574 [D loss: 0.505481, acc.: 78.91%] [G loss: 0.588835]\n",
      "epoch:7 step:6575 [D loss: 0.514409, acc.: 72.66%] [G loss: 0.530053]\n",
      "epoch:7 step:6576 [D loss: 0.588558, acc.: 66.41%] [G loss: 0.566764]\n",
      "epoch:7 step:6577 [D loss: 0.583024, acc.: 67.19%] [G loss: 0.553218]\n",
      "epoch:7 step:6578 [D loss: 0.568103, acc.: 67.97%] [G loss: 0.610057]\n",
      "epoch:7 step:6579 [D loss: 0.604816, acc.: 66.41%] [G loss: 0.530680]\n",
      "epoch:7 step:6580 [D loss: 0.519091, acc.: 74.22%] [G loss: 0.517047]\n",
      "epoch:7 step:6581 [D loss: 0.546109, acc.: 72.66%] [G loss: 0.604247]\n",
      "epoch:7 step:6582 [D loss: 0.491499, acc.: 78.91%] [G loss: 0.518390]\n",
      "epoch:7 step:6583 [D loss: 0.523432, acc.: 71.88%] [G loss: 0.557177]\n",
      "epoch:7 step:6584 [D loss: 0.517889, acc.: 75.00%] [G loss: 0.608180]\n",
      "epoch:7 step:6585 [D loss: 0.603291, acc.: 65.62%] [G loss: 0.452323]\n",
      "epoch:7 step:6586 [D loss: 0.507221, acc.: 77.34%] [G loss: 0.509043]\n",
      "epoch:7 step:6587 [D loss: 0.587630, acc.: 70.31%] [G loss: 0.457434]\n",
      "epoch:7 step:6588 [D loss: 0.507474, acc.: 75.00%] [G loss: 0.523262]\n",
      "epoch:7 step:6589 [D loss: 0.592427, acc.: 64.84%] [G loss: 0.567973]\n",
      "epoch:7 step:6590 [D loss: 0.580324, acc.: 61.72%] [G loss: 0.597549]\n",
      "epoch:7 step:6591 [D loss: 0.616569, acc.: 60.94%] [G loss: 0.533822]\n",
      "epoch:7 step:6592 [D loss: 0.538660, acc.: 71.09%] [G loss: 0.685376]\n",
      "epoch:7 step:6593 [D loss: 0.552633, acc.: 68.75%] [G loss: 0.565449]\n",
      "epoch:7 step:6594 [D loss: 0.517729, acc.: 78.12%] [G loss: 0.617899]\n",
      "epoch:7 step:6595 [D loss: 0.547341, acc.: 67.97%] [G loss: 0.698994]\n",
      "epoch:7 step:6596 [D loss: 0.517711, acc.: 81.25%] [G loss: 0.578236]\n",
      "epoch:7 step:6597 [D loss: 0.664787, acc.: 64.06%] [G loss: 0.456597]\n",
      "epoch:7 step:6598 [D loss: 0.528547, acc.: 71.09%] [G loss: 0.505785]\n",
      "epoch:7 step:6599 [D loss: 0.437405, acc.: 78.12%] [G loss: 0.655573]\n",
      "epoch:7 step:6600 [D loss: 0.567333, acc.: 67.19%] [G loss: 0.494219]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.104661\n",
      "FID: 50.685345\n",
      "0 = 13.096194580364283\n",
      "1 = 0.10141685018990387\n",
      "2 = 0.9355000257492065\n",
      "3 = 0.8776000142097473\n",
      "4 = 0.993399977684021\n",
      "5 = 0.9925356507301331\n",
      "6 = 0.8776000142097473\n",
      "7 = 8.58494266428947\n",
      "8 = 0.14998065505852637\n",
      "9 = 0.79339998960495\n",
      "10 = 0.7721999883651733\n",
      "11 = 0.8145999908447266\n",
      "12 = 0.8063910007476807\n",
      "13 = 0.7721999883651733\n",
      "14 = 6.104687213897705\n",
      "15 = 7.480928897857666\n",
      "16 = 0.35466453433036804\n",
      "17 = 6.104660511016846\n",
      "18 = 50.68534469604492\n",
      "epoch:7 step:6601 [D loss: 0.556236, acc.: 69.53%] [G loss: 0.529130]\n",
      "epoch:7 step:6602 [D loss: 0.483487, acc.: 75.00%] [G loss: 0.563490]\n",
      "epoch:7 step:6603 [D loss: 0.561119, acc.: 71.09%] [G loss: 0.542772]\n",
      "epoch:7 step:6604 [D loss: 0.497779, acc.: 74.22%] [G loss: 0.502414]\n",
      "epoch:7 step:6605 [D loss: 0.579085, acc.: 67.97%] [G loss: 0.498856]\n",
      "epoch:7 step:6606 [D loss: 0.552022, acc.: 71.09%] [G loss: 0.440935]\n",
      "epoch:7 step:6607 [D loss: 0.545810, acc.: 71.09%] [G loss: 0.553807]\n",
      "epoch:7 step:6608 [D loss: 0.515637, acc.: 75.00%] [G loss: 0.648483]\n",
      "epoch:7 step:6609 [D loss: 0.541606, acc.: 74.22%] [G loss: 0.545855]\n",
      "epoch:7 step:6610 [D loss: 0.559907, acc.: 71.88%] [G loss: 0.491554]\n",
      "epoch:7 step:6611 [D loss: 0.528107, acc.: 71.88%] [G loss: 0.690641]\n",
      "epoch:7 step:6612 [D loss: 0.564356, acc.: 73.44%] [G loss: 0.568739]\n",
      "epoch:7 step:6613 [D loss: 0.441421, acc.: 80.47%] [G loss: 0.638015]\n",
      "epoch:7 step:6614 [D loss: 0.525863, acc.: 69.53%] [G loss: 0.511201]\n",
      "epoch:7 step:6615 [D loss: 0.499702, acc.: 76.56%] [G loss: 0.626282]\n",
      "epoch:7 step:6616 [D loss: 0.522222, acc.: 68.75%] [G loss: 0.575082]\n",
      "epoch:7 step:6617 [D loss: 0.563988, acc.: 68.75%] [G loss: 0.793126]\n",
      "epoch:7 step:6618 [D loss: 0.481252, acc.: 75.78%] [G loss: 0.564697]\n",
      "epoch:7 step:6619 [D loss: 0.544122, acc.: 72.66%] [G loss: 0.505831]\n",
      "epoch:7 step:6620 [D loss: 0.557754, acc.: 69.53%] [G loss: 0.594596]\n",
      "epoch:7 step:6621 [D loss: 0.548230, acc.: 72.66%] [G loss: 0.668717]\n",
      "epoch:7 step:6622 [D loss: 0.554151, acc.: 64.06%] [G loss: 0.550455]\n",
      "epoch:7 step:6623 [D loss: 0.554205, acc.: 72.66%] [G loss: 0.491993]\n",
      "epoch:7 step:6624 [D loss: 0.536777, acc.: 73.44%] [G loss: 0.509073]\n",
      "epoch:7 step:6625 [D loss: 0.528501, acc.: 70.31%] [G loss: 0.457031]\n",
      "epoch:7 step:6626 [D loss: 0.563124, acc.: 68.75%] [G loss: 0.436222]\n",
      "epoch:7 step:6627 [D loss: 0.511792, acc.: 75.00%] [G loss: 0.482561]\n",
      "epoch:7 step:6628 [D loss: 0.554249, acc.: 69.53%] [G loss: 0.487556]\n",
      "epoch:7 step:6629 [D loss: 0.553110, acc.: 72.66%] [G loss: 0.604258]\n",
      "epoch:7 step:6630 [D loss: 0.541592, acc.: 67.19%] [G loss: 0.501779]\n",
      "epoch:7 step:6631 [D loss: 0.521311, acc.: 68.75%] [G loss: 0.543021]\n",
      "epoch:7 step:6632 [D loss: 0.542939, acc.: 68.75%] [G loss: 0.508593]\n",
      "epoch:7 step:6633 [D loss: 0.438335, acc.: 82.03%] [G loss: 0.733707]\n",
      "epoch:7 step:6634 [D loss: 0.589334, acc.: 67.97%] [G loss: 0.621311]\n",
      "epoch:7 step:6635 [D loss: 0.539139, acc.: 68.75%] [G loss: 0.644266]\n",
      "epoch:7 step:6636 [D loss: 0.434361, acc.: 81.25%] [G loss: 0.783868]\n",
      "epoch:7 step:6637 [D loss: 0.645378, acc.: 64.06%] [G loss: 0.532644]\n",
      "epoch:7 step:6638 [D loss: 0.584339, acc.: 65.62%] [G loss: 0.538877]\n",
      "epoch:7 step:6639 [D loss: 0.556851, acc.: 74.22%] [G loss: 0.560668]\n",
      "epoch:7 step:6640 [D loss: 0.567020, acc.: 71.88%] [G loss: 0.607968]\n",
      "epoch:7 step:6641 [D loss: 0.482193, acc.: 75.00%] [G loss: 0.540131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6642 [D loss: 0.540084, acc.: 70.31%] [G loss: 0.568843]\n",
      "epoch:7 step:6643 [D loss: 0.552238, acc.: 71.09%] [G loss: 0.610877]\n",
      "epoch:7 step:6644 [D loss: 0.569107, acc.: 70.31%] [G loss: 0.509090]\n",
      "epoch:7 step:6645 [D loss: 0.576074, acc.: 67.97%] [G loss: 0.412754]\n",
      "epoch:7 step:6646 [D loss: 0.530207, acc.: 73.44%] [G loss: 0.420638]\n",
      "epoch:7 step:6647 [D loss: 0.493735, acc.: 76.56%] [G loss: 0.539154]\n",
      "epoch:7 step:6648 [D loss: 0.522389, acc.: 72.66%] [G loss: 0.643916]\n",
      "epoch:7 step:6649 [D loss: 0.490596, acc.: 78.91%] [G loss: 0.569371]\n",
      "epoch:7 step:6650 [D loss: 0.533270, acc.: 74.22%] [G loss: 0.599702]\n",
      "epoch:7 step:6651 [D loss: 0.494579, acc.: 75.00%] [G loss: 0.574803]\n",
      "epoch:7 step:6652 [D loss: 0.518993, acc.: 77.34%] [G loss: 0.548868]\n",
      "epoch:7 step:6653 [D loss: 0.527498, acc.: 76.56%] [G loss: 0.503993]\n",
      "epoch:7 step:6654 [D loss: 0.568523, acc.: 67.97%] [G loss: 0.611112]\n",
      "epoch:7 step:6655 [D loss: 0.464884, acc.: 79.69%] [G loss: 0.607967]\n",
      "epoch:7 step:6656 [D loss: 0.521832, acc.: 73.44%] [G loss: 0.548938]\n",
      "epoch:7 step:6657 [D loss: 0.552554, acc.: 71.09%] [G loss: 0.547691]\n",
      "epoch:7 step:6658 [D loss: 0.595992, acc.: 63.28%] [G loss: 0.444580]\n",
      "epoch:7 step:6659 [D loss: 0.424325, acc.: 83.59%] [G loss: 0.705522]\n",
      "epoch:7 step:6660 [D loss: 0.579405, acc.: 67.97%] [G loss: 0.653998]\n",
      "epoch:7 step:6661 [D loss: 0.620953, acc.: 64.84%] [G loss: 0.500993]\n",
      "epoch:7 step:6662 [D loss: 0.544942, acc.: 67.19%] [G loss: 0.502027]\n",
      "epoch:7 step:6663 [D loss: 0.494278, acc.: 71.88%] [G loss: 0.572242]\n",
      "epoch:7 step:6664 [D loss: 0.570106, acc.: 70.31%] [G loss: 0.614742]\n",
      "epoch:7 step:6665 [D loss: 0.530385, acc.: 79.69%] [G loss: 0.505968]\n",
      "epoch:7 step:6666 [D loss: 0.615092, acc.: 64.84%] [G loss: 0.447792]\n",
      "epoch:7 step:6667 [D loss: 0.581913, acc.: 65.62%] [G loss: 0.561903]\n",
      "epoch:7 step:6668 [D loss: 0.623538, acc.: 64.06%] [G loss: 0.504198]\n",
      "epoch:7 step:6669 [D loss: 0.568882, acc.: 71.88%] [G loss: 0.479927]\n",
      "epoch:7 step:6670 [D loss: 0.476085, acc.: 78.12%] [G loss: 0.570324]\n",
      "epoch:7 step:6671 [D loss: 0.579797, acc.: 70.31%] [G loss: 0.540950]\n",
      "epoch:7 step:6672 [D loss: 0.641983, acc.: 66.41%] [G loss: 0.499457]\n",
      "epoch:7 step:6673 [D loss: 0.584078, acc.: 70.31%] [G loss: 0.637515]\n",
      "epoch:7 step:6674 [D loss: 0.544106, acc.: 73.44%] [G loss: 0.553646]\n",
      "epoch:7 step:6675 [D loss: 0.499114, acc.: 74.22%] [G loss: 0.666023]\n",
      "epoch:7 step:6676 [D loss: 0.553379, acc.: 71.88%] [G loss: 0.664760]\n",
      "epoch:7 step:6677 [D loss: 0.545281, acc.: 71.09%] [G loss: 0.515932]\n",
      "epoch:7 step:6678 [D loss: 0.506340, acc.: 74.22%] [G loss: 0.582144]\n",
      "epoch:7 step:6679 [D loss: 0.615251, acc.: 64.84%] [G loss: 0.722728]\n",
      "epoch:7 step:6680 [D loss: 0.619723, acc.: 64.06%] [G loss: 0.549593]\n",
      "epoch:7 step:6681 [D loss: 0.549809, acc.: 71.88%] [G loss: 0.649337]\n",
      "epoch:7 step:6682 [D loss: 0.519212, acc.: 69.53%] [G loss: 0.621483]\n",
      "epoch:7 step:6683 [D loss: 0.578396, acc.: 71.88%] [G loss: 0.531864]\n",
      "epoch:7 step:6684 [D loss: 0.495371, acc.: 78.91%] [G loss: 0.544129]\n",
      "epoch:7 step:6685 [D loss: 0.538065, acc.: 73.44%] [G loss: 0.459737]\n",
      "epoch:7 step:6686 [D loss: 0.505961, acc.: 73.44%] [G loss: 0.560833]\n",
      "epoch:7 step:6687 [D loss: 0.521821, acc.: 71.09%] [G loss: 0.460723]\n",
      "epoch:7 step:6688 [D loss: 0.600141, acc.: 70.31%] [G loss: 0.456655]\n",
      "epoch:7 step:6689 [D loss: 0.545704, acc.: 74.22%] [G loss: 0.513973]\n",
      "epoch:7 step:6690 [D loss: 0.536704, acc.: 71.88%] [G loss: 0.451163]\n",
      "epoch:7 step:6691 [D loss: 0.521418, acc.: 76.56%] [G loss: 0.537897]\n",
      "epoch:7 step:6692 [D loss: 0.578581, acc.: 67.97%] [G loss: 0.563652]\n",
      "epoch:7 step:6693 [D loss: 0.538141, acc.: 74.22%] [G loss: 0.543852]\n",
      "epoch:7 step:6694 [D loss: 0.534285, acc.: 74.22%] [G loss: 0.588521]\n",
      "epoch:7 step:6695 [D loss: 0.554777, acc.: 73.44%] [G loss: 0.564124]\n",
      "epoch:7 step:6696 [D loss: 0.639933, acc.: 64.84%] [G loss: 0.401468]\n",
      "epoch:7 step:6697 [D loss: 0.597304, acc.: 67.19%] [G loss: 0.582416]\n",
      "epoch:7 step:6698 [D loss: 0.562064, acc.: 71.09%] [G loss: 0.576776]\n",
      "epoch:7 step:6699 [D loss: 0.623453, acc.: 60.16%] [G loss: 0.530992]\n",
      "epoch:7 step:6700 [D loss: 0.556712, acc.: 68.75%] [G loss: 0.504854]\n",
      "epoch:7 step:6701 [D loss: 0.583771, acc.: 64.06%] [G loss: 0.557402]\n",
      "epoch:7 step:6702 [D loss: 0.652935, acc.: 62.50%] [G loss: 0.513798]\n",
      "epoch:7 step:6703 [D loss: 0.529382, acc.: 67.19%] [G loss: 0.487701]\n",
      "epoch:7 step:6704 [D loss: 0.568707, acc.: 69.53%] [G loss: 0.640231]\n",
      "epoch:7 step:6705 [D loss: 0.539581, acc.: 74.22%] [G loss: 0.533425]\n",
      "epoch:7 step:6706 [D loss: 0.676885, acc.: 60.94%] [G loss: 0.399006]\n",
      "epoch:7 step:6707 [D loss: 0.548647, acc.: 69.53%] [G loss: 0.579581]\n",
      "epoch:7 step:6708 [D loss: 0.485306, acc.: 74.22%] [G loss: 0.642890]\n",
      "epoch:7 step:6709 [D loss: 0.697119, acc.: 57.03%] [G loss: 0.390009]\n",
      "epoch:7 step:6710 [D loss: 0.542782, acc.: 71.09%] [G loss: 0.546152]\n",
      "epoch:7 step:6711 [D loss: 0.462615, acc.: 81.25%] [G loss: 0.569268]\n",
      "epoch:7 step:6712 [D loss: 0.570512, acc.: 67.97%] [G loss: 0.571315]\n",
      "epoch:7 step:6713 [D loss: 0.584826, acc.: 67.97%] [G loss: 0.492908]\n",
      "epoch:7 step:6714 [D loss: 0.452078, acc.: 81.25%] [G loss: 0.520736]\n",
      "epoch:7 step:6715 [D loss: 0.552980, acc.: 70.31%] [G loss: 0.627387]\n",
      "epoch:7 step:6716 [D loss: 0.549363, acc.: 71.09%] [G loss: 0.592387]\n",
      "epoch:7 step:6717 [D loss: 0.609073, acc.: 64.84%] [G loss: 0.418914]\n",
      "epoch:7 step:6718 [D loss: 0.521520, acc.: 76.56%] [G loss: 0.504730]\n",
      "epoch:7 step:6719 [D loss: 0.598193, acc.: 65.62%] [G loss: 0.634642]\n",
      "epoch:7 step:6720 [D loss: 0.541708, acc.: 72.66%] [G loss: 0.542233]\n",
      "epoch:7 step:6721 [D loss: 0.488090, acc.: 74.22%] [G loss: 0.552506]\n",
      "epoch:7 step:6722 [D loss: 0.548322, acc.: 67.97%] [G loss: 0.613172]\n",
      "epoch:7 step:6723 [D loss: 0.529139, acc.: 77.34%] [G loss: 0.550941]\n",
      "epoch:7 step:6724 [D loss: 0.568614, acc.: 63.28%] [G loss: 0.413812]\n",
      "epoch:7 step:6725 [D loss: 0.519926, acc.: 71.09%] [G loss: 0.642938]\n",
      "epoch:7 step:6726 [D loss: 0.561282, acc.: 69.53%] [G loss: 0.466975]\n",
      "epoch:7 step:6727 [D loss: 0.530964, acc.: 73.44%] [G loss: 0.480669]\n",
      "epoch:7 step:6728 [D loss: 0.562904, acc.: 70.31%] [G loss: 0.485666]\n",
      "epoch:7 step:6729 [D loss: 0.545613, acc.: 64.06%] [G loss: 0.532120]\n",
      "epoch:7 step:6730 [D loss: 0.490649, acc.: 75.00%] [G loss: 0.499223]\n",
      "epoch:7 step:6731 [D loss: 0.543727, acc.: 71.09%] [G loss: 0.448244]\n",
      "epoch:7 step:6732 [D loss: 0.506806, acc.: 73.44%] [G loss: 0.633142]\n",
      "epoch:7 step:6733 [D loss: 0.594954, acc.: 66.41%] [G loss: 0.429884]\n",
      "epoch:7 step:6734 [D loss: 0.542067, acc.: 70.31%] [G loss: 0.485701]\n",
      "epoch:7 step:6735 [D loss: 0.527993, acc.: 71.09%] [G loss: 0.416288]\n",
      "epoch:7 step:6736 [D loss: 0.540253, acc.: 71.09%] [G loss: 0.507484]\n",
      "epoch:7 step:6737 [D loss: 0.545676, acc.: 67.19%] [G loss: 0.541692]\n",
      "epoch:7 step:6738 [D loss: 0.537724, acc.: 68.75%] [G loss: 0.518392]\n",
      "epoch:7 step:6739 [D loss: 0.647612, acc.: 67.19%] [G loss: 0.441240]\n",
      "epoch:7 step:6740 [D loss: 0.546419, acc.: 71.88%] [G loss: 0.523724]\n",
      "epoch:7 step:6741 [D loss: 0.519560, acc.: 79.69%] [G loss: 0.526901]\n",
      "epoch:7 step:6742 [D loss: 0.566289, acc.: 70.31%] [G loss: 0.519354]\n",
      "epoch:7 step:6743 [D loss: 0.553276, acc.: 71.09%] [G loss: 0.559371]\n",
      "epoch:7 step:6744 [D loss: 0.597385, acc.: 62.50%] [G loss: 0.499802]\n",
      "epoch:7 step:6745 [D loss: 0.603239, acc.: 64.84%] [G loss: 0.455978]\n",
      "epoch:7 step:6746 [D loss: 0.626330, acc.: 62.50%] [G loss: 0.502955]\n",
      "epoch:7 step:6747 [D loss: 0.531199, acc.: 71.88%] [G loss: 0.521153]\n",
      "epoch:7 step:6748 [D loss: 0.643193, acc.: 64.06%] [G loss: 0.514596]\n",
      "epoch:7 step:6749 [D loss: 0.449322, acc.: 78.91%] [G loss: 0.684463]\n",
      "epoch:7 step:6750 [D loss: 0.536103, acc.: 74.22%] [G loss: 0.490712]\n",
      "epoch:7 step:6751 [D loss: 0.534656, acc.: 75.78%] [G loss: 0.568983]\n",
      "epoch:7 step:6752 [D loss: 0.622958, acc.: 67.19%] [G loss: 0.657518]\n",
      "epoch:7 step:6753 [D loss: 0.486413, acc.: 75.00%] [G loss: 0.656526]\n",
      "epoch:7 step:6754 [D loss: 0.557047, acc.: 71.88%] [G loss: 0.639261]\n",
      "epoch:7 step:6755 [D loss: 0.571627, acc.: 67.19%] [G loss: 0.522678]\n",
      "epoch:7 step:6756 [D loss: 0.527474, acc.: 74.22%] [G loss: 0.547101]\n",
      "epoch:7 step:6757 [D loss: 0.513274, acc.: 70.31%] [G loss: 0.580171]\n",
      "epoch:7 step:6758 [D loss: 0.517655, acc.: 71.88%] [G loss: 0.532228]\n",
      "epoch:7 step:6759 [D loss: 0.615690, acc.: 67.19%] [G loss: 0.463236]\n",
      "epoch:7 step:6760 [D loss: 0.568892, acc.: 68.75%] [G loss: 0.510414]\n",
      "epoch:7 step:6761 [D loss: 0.553564, acc.: 68.75%] [G loss: 0.508847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6762 [D loss: 0.610610, acc.: 65.62%] [G loss: 0.522248]\n",
      "epoch:7 step:6763 [D loss: 0.520470, acc.: 75.00%] [G loss: 0.541052]\n",
      "epoch:7 step:6764 [D loss: 0.570452, acc.: 68.75%] [G loss: 0.610591]\n",
      "epoch:7 step:6765 [D loss: 0.535395, acc.: 74.22%] [G loss: 0.572412]\n",
      "epoch:7 step:6766 [D loss: 0.456029, acc.: 79.69%] [G loss: 0.639371]\n",
      "epoch:7 step:6767 [D loss: 0.430326, acc.: 81.25%] [G loss: 0.689557]\n",
      "epoch:7 step:6768 [D loss: 0.546600, acc.: 71.88%] [G loss: 0.712566]\n",
      "epoch:7 step:6769 [D loss: 0.636567, acc.: 62.50%] [G loss: 0.516135]\n",
      "epoch:7 step:6770 [D loss: 0.581499, acc.: 65.62%] [G loss: 0.523031]\n",
      "epoch:7 step:6771 [D loss: 0.572684, acc.: 67.19%] [G loss: 0.533483]\n",
      "epoch:7 step:6772 [D loss: 0.514283, acc.: 71.88%] [G loss: 0.615203]\n",
      "epoch:7 step:6773 [D loss: 0.655368, acc.: 57.81%] [G loss: 0.403642]\n",
      "epoch:7 step:6774 [D loss: 0.637375, acc.: 62.50%] [G loss: 0.478436]\n",
      "epoch:7 step:6775 [D loss: 0.556070, acc.: 68.75%] [G loss: 0.445194]\n",
      "epoch:7 step:6776 [D loss: 0.516813, acc.: 76.56%] [G loss: 0.436845]\n",
      "epoch:7 step:6777 [D loss: 0.518573, acc.: 77.34%] [G loss: 0.514067]\n",
      "epoch:7 step:6778 [D loss: 0.527774, acc.: 76.56%] [G loss: 0.674652]\n",
      "epoch:7 step:6779 [D loss: 0.573832, acc.: 70.31%] [G loss: 0.802863]\n",
      "epoch:7 step:6780 [D loss: 0.561833, acc.: 71.09%] [G loss: 0.610084]\n",
      "epoch:7 step:6781 [D loss: 0.485160, acc.: 75.00%] [G loss: 0.662382]\n",
      "epoch:7 step:6782 [D loss: 0.495308, acc.: 78.91%] [G loss: 0.690788]\n",
      "epoch:7 step:6783 [D loss: 0.617656, acc.: 64.84%] [G loss: 0.599098]\n",
      "epoch:7 step:6784 [D loss: 0.546059, acc.: 67.97%] [G loss: 0.543794]\n",
      "epoch:7 step:6785 [D loss: 0.617486, acc.: 59.38%] [G loss: 0.384622]\n",
      "epoch:7 step:6786 [D loss: 0.561448, acc.: 71.88%] [G loss: 0.390822]\n",
      "epoch:7 step:6787 [D loss: 0.593800, acc.: 66.41%] [G loss: 0.394790]\n",
      "epoch:7 step:6788 [D loss: 0.547530, acc.: 70.31%] [G loss: 0.438345]\n",
      "epoch:7 step:6789 [D loss: 0.583852, acc.: 64.06%] [G loss: 0.448869]\n",
      "epoch:7 step:6790 [D loss: 0.550896, acc.: 68.75%] [G loss: 0.513548]\n",
      "epoch:7 step:6791 [D loss: 0.484990, acc.: 77.34%] [G loss: 0.602651]\n",
      "epoch:7 step:6792 [D loss: 0.585183, acc.: 64.84%] [G loss: 0.596722]\n",
      "epoch:7 step:6793 [D loss: 0.530653, acc.: 71.09%] [G loss: 0.660574]\n",
      "epoch:7 step:6794 [D loss: 0.542619, acc.: 67.19%] [G loss: 0.540122]\n",
      "epoch:7 step:6795 [D loss: 0.538032, acc.: 77.34%] [G loss: 0.473332]\n",
      "epoch:7 step:6796 [D loss: 0.555506, acc.: 70.31%] [G loss: 0.441191]\n",
      "epoch:7 step:6797 [D loss: 0.572605, acc.: 70.31%] [G loss: 0.571717]\n",
      "epoch:7 step:6798 [D loss: 0.532605, acc.: 71.09%] [G loss: 0.496109]\n",
      "epoch:7 step:6799 [D loss: 0.501709, acc.: 76.56%] [G loss: 0.557910]\n",
      "epoch:7 step:6800 [D loss: 0.548358, acc.: 72.66%] [G loss: 0.524336]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.053918\n",
      "FID: 47.373028\n",
      "0 = 13.009076743602755\n",
      "1 = 0.09948363502146954\n",
      "2 = 0.9236999750137329\n",
      "3 = 0.8565999865531921\n",
      "4 = 0.9908000230789185\n",
      "5 = 0.9893739819526672\n",
      "6 = 0.8565999865531921\n",
      "7 = 8.260629324626924\n",
      "8 = 0.14359827742220826\n",
      "9 = 0.7843999862670898\n",
      "10 = 0.7591999769210815\n",
      "11 = 0.8095999956130981\n",
      "12 = 0.7994945049285889\n",
      "13 = 0.7591999769210815\n",
      "14 = 6.053940296173096\n",
      "15 = 7.675827980041504\n",
      "16 = 0.34114885330200195\n",
      "17 = 6.05391788482666\n",
      "18 = 47.37302780151367\n",
      "epoch:7 step:6801 [D loss: 0.559662, acc.: 70.31%] [G loss: 0.675768]\n",
      "epoch:7 step:6802 [D loss: 0.554027, acc.: 63.28%] [G loss: 0.568354]\n",
      "epoch:7 step:6803 [D loss: 0.507207, acc.: 75.78%] [G loss: 0.635401]\n",
      "epoch:7 step:6804 [D loss: 0.561627, acc.: 67.97%] [G loss: 0.587639]\n",
      "epoch:7 step:6805 [D loss: 0.566001, acc.: 73.44%] [G loss: 0.577323]\n",
      "epoch:7 step:6806 [D loss: 0.612474, acc.: 65.62%] [G loss: 0.613079]\n",
      "epoch:7 step:6807 [D loss: 0.560403, acc.: 71.09%] [G loss: 0.590667]\n",
      "epoch:7 step:6808 [D loss: 0.531940, acc.: 75.00%] [G loss: 0.572068]\n",
      "epoch:7 step:6809 [D loss: 0.580385, acc.: 67.19%] [G loss: 0.740688]\n",
      "epoch:7 step:6810 [D loss: 0.654881, acc.: 60.94%] [G loss: 0.513284]\n",
      "epoch:7 step:6811 [D loss: 0.580753, acc.: 67.19%] [G loss: 0.544064]\n",
      "epoch:7 step:6812 [D loss: 0.583096, acc.: 71.88%] [G loss: 0.579635]\n",
      "epoch:7 step:6813 [D loss: 0.496642, acc.: 75.78%] [G loss: 0.632961]\n",
      "epoch:7 step:6814 [D loss: 0.553710, acc.: 67.97%] [G loss: 0.523036]\n",
      "epoch:7 step:6815 [D loss: 0.508843, acc.: 73.44%] [G loss: 0.622698]\n",
      "epoch:7 step:6816 [D loss: 0.580994, acc.: 69.53%] [G loss: 0.456073]\n",
      "epoch:7 step:6817 [D loss: 0.554339, acc.: 70.31%] [G loss: 0.426298]\n",
      "epoch:7 step:6818 [D loss: 0.496870, acc.: 78.12%] [G loss: 0.491426]\n",
      "epoch:7 step:6819 [D loss: 0.581498, acc.: 71.88%] [G loss: 0.491133]\n",
      "epoch:7 step:6820 [D loss: 0.557400, acc.: 72.66%] [G loss: 0.439614]\n",
      "epoch:7 step:6821 [D loss: 0.536074, acc.: 71.09%] [G loss: 0.542331]\n",
      "epoch:7 step:6822 [D loss: 0.650032, acc.: 64.06%] [G loss: 0.337529]\n",
      "epoch:7 step:6823 [D loss: 0.552013, acc.: 69.53%] [G loss: 0.476669]\n",
      "epoch:7 step:6824 [D loss: 0.559530, acc.: 70.31%] [G loss: 0.470986]\n",
      "epoch:7 step:6825 [D loss: 0.586060, acc.: 69.53%] [G loss: 0.471842]\n",
      "epoch:7 step:6826 [D loss: 0.564287, acc.: 68.75%] [G loss: 0.489464]\n",
      "epoch:7 step:6827 [D loss: 0.576687, acc.: 64.84%] [G loss: 0.478759]\n",
      "epoch:7 step:6828 [D loss: 0.574395, acc.: 69.53%] [G loss: 0.486550]\n",
      "epoch:7 step:6829 [D loss: 0.518341, acc.: 75.78%] [G loss: 0.471768]\n",
      "epoch:7 step:6830 [D loss: 0.506096, acc.: 75.00%] [G loss: 0.479406]\n",
      "epoch:7 step:6831 [D loss: 0.508146, acc.: 78.12%] [G loss: 0.558309]\n",
      "epoch:7 step:6832 [D loss: 0.493536, acc.: 78.91%] [G loss: 0.501223]\n",
      "epoch:7 step:6833 [D loss: 0.568988, acc.: 70.31%] [G loss: 0.555612]\n",
      "epoch:7 step:6834 [D loss: 0.647781, acc.: 64.06%] [G loss: 0.504196]\n",
      "epoch:7 step:6835 [D loss: 0.479610, acc.: 80.47%] [G loss: 0.609332]\n",
      "epoch:7 step:6836 [D loss: 0.618339, acc.: 68.75%] [G loss: 0.504297]\n",
      "epoch:7 step:6837 [D loss: 0.543151, acc.: 74.22%] [G loss: 0.518566]\n",
      "epoch:7 step:6838 [D loss: 0.595479, acc.: 62.50%] [G loss: 0.418858]\n",
      "epoch:7 step:6839 [D loss: 0.595955, acc.: 65.62%] [G loss: 0.554137]\n",
      "epoch:7 step:6840 [D loss: 0.574731, acc.: 72.66%] [G loss: 0.576943]\n",
      "epoch:7 step:6841 [D loss: 0.577990, acc.: 64.06%] [G loss: 0.617086]\n",
      "epoch:7 step:6842 [D loss: 0.526500, acc.: 71.88%] [G loss: 0.522977]\n",
      "epoch:7 step:6843 [D loss: 0.534648, acc.: 78.12%] [G loss: 0.411828]\n",
      "epoch:7 step:6844 [D loss: 0.513500, acc.: 71.09%] [G loss: 0.479710]\n",
      "epoch:7 step:6845 [D loss: 0.527518, acc.: 75.78%] [G loss: 0.523241]\n",
      "epoch:7 step:6846 [D loss: 0.575927, acc.: 67.97%] [G loss: 0.582593]\n",
      "epoch:7 step:6847 [D loss: 0.600390, acc.: 61.72%] [G loss: 0.531528]\n",
      "epoch:7 step:6848 [D loss: 0.556552, acc.: 70.31%] [G loss: 0.495263]\n",
      "epoch:7 step:6849 [D loss: 0.562303, acc.: 67.19%] [G loss: 0.567271]\n",
      "epoch:7 step:6850 [D loss: 0.624529, acc.: 60.16%] [G loss: 0.363826]\n",
      "epoch:7 step:6851 [D loss: 0.605833, acc.: 66.41%] [G loss: 0.508008]\n",
      "epoch:7 step:6852 [D loss: 0.511456, acc.: 75.00%] [G loss: 0.579403]\n",
      "epoch:7 step:6853 [D loss: 0.585167, acc.: 65.62%] [G loss: 0.507934]\n",
      "epoch:7 step:6854 [D loss: 0.538201, acc.: 68.75%] [G loss: 0.539243]\n",
      "epoch:7 step:6855 [D loss: 0.472495, acc.: 80.47%] [G loss: 0.665677]\n",
      "epoch:7 step:6856 [D loss: 0.564854, acc.: 70.31%] [G loss: 0.462162]\n",
      "epoch:7 step:6857 [D loss: 0.490067, acc.: 77.34%] [G loss: 0.584849]\n",
      "epoch:7 step:6858 [D loss: 0.486371, acc.: 75.78%] [G loss: 0.617103]\n",
      "epoch:7 step:6859 [D loss: 0.529648, acc.: 74.22%] [G loss: 0.584394]\n",
      "epoch:7 step:6860 [D loss: 0.616078, acc.: 70.31%] [G loss: 0.527676]\n",
      "epoch:7 step:6861 [D loss: 0.510547, acc.: 71.09%] [G loss: 0.534215]\n",
      "epoch:7 step:6862 [D loss: 0.581784, acc.: 66.41%] [G loss: 0.517173]\n",
      "epoch:7 step:6863 [D loss: 0.478650, acc.: 78.12%] [G loss: 0.643151]\n",
      "epoch:7 step:6864 [D loss: 0.533500, acc.: 74.22%] [G loss: 0.600727]\n",
      "epoch:7 step:6865 [D loss: 0.539215, acc.: 68.75%] [G loss: 0.620023]\n",
      "epoch:7 step:6866 [D loss: 0.494014, acc.: 76.56%] [G loss: 0.601853]\n",
      "epoch:7 step:6867 [D loss: 0.531672, acc.: 74.22%] [G loss: 0.594661]\n",
      "epoch:7 step:6868 [D loss: 0.460629, acc.: 81.25%] [G loss: 0.606604]\n",
      "epoch:7 step:6869 [D loss: 0.529732, acc.: 67.97%] [G loss: 0.681589]\n",
      "epoch:7 step:6870 [D loss: 0.506469, acc.: 71.88%] [G loss: 0.662472]\n",
      "epoch:7 step:6871 [D loss: 0.416268, acc.: 83.59%] [G loss: 0.795223]\n",
      "epoch:7 step:6872 [D loss: 0.478159, acc.: 78.91%] [G loss: 0.978554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6873 [D loss: 0.438664, acc.: 82.03%] [G loss: 0.956534]\n",
      "epoch:7 step:6874 [D loss: 0.412505, acc.: 81.25%] [G loss: 0.915209]\n",
      "epoch:7 step:6875 [D loss: 0.724846, acc.: 64.84%] [G loss: 0.580342]\n",
      "epoch:7 step:6876 [D loss: 0.556982, acc.: 69.53%] [G loss: 0.490273]\n",
      "epoch:7 step:6877 [D loss: 0.561648, acc.: 67.97%] [G loss: 0.411561]\n",
      "epoch:7 step:6878 [D loss: 0.556855, acc.: 74.22%] [G loss: 0.605471]\n",
      "epoch:7 step:6879 [D loss: 0.531307, acc.: 71.09%] [G loss: 0.544705]\n",
      "epoch:7 step:6880 [D loss: 0.535965, acc.: 75.00%] [G loss: 0.602462]\n",
      "epoch:7 step:6881 [D loss: 0.574679, acc.: 67.97%] [G loss: 0.654427]\n",
      "epoch:7 step:6882 [D loss: 0.609637, acc.: 64.06%] [G loss: 0.529368]\n",
      "epoch:7 step:6883 [D loss: 0.591064, acc.: 69.53%] [G loss: 0.577686]\n",
      "epoch:7 step:6884 [D loss: 0.560530, acc.: 75.00%] [G loss: 0.520624]\n",
      "epoch:7 step:6885 [D loss: 0.513326, acc.: 74.22%] [G loss: 0.519863]\n",
      "epoch:7 step:6886 [D loss: 0.514873, acc.: 73.44%] [G loss: 0.632105]\n",
      "epoch:7 step:6887 [D loss: 0.531716, acc.: 71.88%] [G loss: 0.592542]\n",
      "epoch:7 step:6888 [D loss: 0.553930, acc.: 71.88%] [G loss: 0.556406]\n",
      "epoch:7 step:6889 [D loss: 0.557754, acc.: 67.19%] [G loss: 0.475727]\n",
      "epoch:7 step:6890 [D loss: 0.555457, acc.: 70.31%] [G loss: 0.477261]\n",
      "epoch:7 step:6891 [D loss: 0.522182, acc.: 71.09%] [G loss: 0.499944]\n",
      "epoch:7 step:6892 [D loss: 0.492950, acc.: 75.78%] [G loss: 0.567899]\n",
      "epoch:7 step:6893 [D loss: 0.586380, acc.: 69.53%] [G loss: 0.698801]\n",
      "epoch:7 step:6894 [D loss: 0.545031, acc.: 68.75%] [G loss: 0.582181]\n",
      "epoch:7 step:6895 [D loss: 0.532105, acc.: 71.88%] [G loss: 0.725598]\n",
      "epoch:7 step:6896 [D loss: 0.511490, acc.: 73.44%] [G loss: 0.643437]\n",
      "epoch:7 step:6897 [D loss: 0.552120, acc.: 70.31%] [G loss: 0.610150]\n",
      "epoch:7 step:6898 [D loss: 0.509679, acc.: 75.00%] [G loss: 0.713746]\n",
      "epoch:7 step:6899 [D loss: 0.514170, acc.: 75.00%] [G loss: 0.626998]\n",
      "epoch:7 step:6900 [D loss: 0.568138, acc.: 69.53%] [G loss: 0.651153]\n",
      "epoch:7 step:6901 [D loss: 0.684618, acc.: 57.03%] [G loss: 0.408676]\n",
      "epoch:7 step:6902 [D loss: 0.486274, acc.: 75.78%] [G loss: 0.621112]\n",
      "epoch:7 step:6903 [D loss: 0.472761, acc.: 75.00%] [G loss: 0.725621]\n",
      "epoch:7 step:6904 [D loss: 0.552733, acc.: 71.09%] [G loss: 0.712983]\n",
      "epoch:7 step:6905 [D loss: 0.553815, acc.: 67.19%] [G loss: 0.741265]\n",
      "epoch:7 step:6906 [D loss: 0.448719, acc.: 82.81%] [G loss: 0.872353]\n",
      "epoch:7 step:6907 [D loss: 0.686568, acc.: 60.16%] [G loss: 0.534546]\n",
      "epoch:7 step:6908 [D loss: 0.663839, acc.: 64.06%] [G loss: 0.496975]\n",
      "epoch:7 step:6909 [D loss: 0.465638, acc.: 77.34%] [G loss: 0.459686]\n",
      "epoch:7 step:6910 [D loss: 0.609977, acc.: 67.97%] [G loss: 0.581792]\n",
      "epoch:7 step:6911 [D loss: 0.565915, acc.: 67.97%] [G loss: 0.673954]\n",
      "epoch:7 step:6912 [D loss: 0.533656, acc.: 71.88%] [G loss: 0.597908]\n",
      "epoch:7 step:6913 [D loss: 0.495768, acc.: 71.88%] [G loss: 0.572788]\n",
      "epoch:7 step:6914 [D loss: 0.571209, acc.: 68.75%] [G loss: 0.642700]\n",
      "epoch:7 step:6915 [D loss: 0.578216, acc.: 66.41%] [G loss: 0.503921]\n",
      "epoch:7 step:6916 [D loss: 0.472656, acc.: 79.69%] [G loss: 0.639901]\n",
      "epoch:7 step:6917 [D loss: 0.495935, acc.: 71.09%] [G loss: 0.609788]\n",
      "epoch:7 step:6918 [D loss: 0.468991, acc.: 77.34%] [G loss: 0.689494]\n",
      "epoch:7 step:6919 [D loss: 0.533194, acc.: 69.53%] [G loss: 0.534855]\n",
      "epoch:7 step:6920 [D loss: 0.500961, acc.: 78.12%] [G loss: 0.601246]\n",
      "epoch:7 step:6921 [D loss: 0.614997, acc.: 64.06%] [G loss: 0.527697]\n",
      "epoch:7 step:6922 [D loss: 0.529647, acc.: 71.88%] [G loss: 0.536646]\n",
      "epoch:7 step:6923 [D loss: 0.506591, acc.: 71.09%] [G loss: 0.545911]\n",
      "epoch:7 step:6924 [D loss: 0.576133, acc.: 65.62%] [G loss: 0.421764]\n",
      "epoch:7 step:6925 [D loss: 0.531266, acc.: 72.66%] [G loss: 0.624346]\n",
      "epoch:7 step:6926 [D loss: 0.588724, acc.: 66.41%] [G loss: 0.637515]\n",
      "epoch:7 step:6927 [D loss: 0.556790, acc.: 70.31%] [G loss: 0.609743]\n",
      "epoch:7 step:6928 [D loss: 0.555806, acc.: 66.41%] [G loss: 0.519970]\n",
      "epoch:7 step:6929 [D loss: 0.538354, acc.: 74.22%] [G loss: 0.562452]\n",
      "epoch:7 step:6930 [D loss: 0.530106, acc.: 75.78%] [G loss: 0.573932]\n",
      "epoch:7 step:6931 [D loss: 0.592055, acc.: 68.75%] [G loss: 0.603370]\n",
      "epoch:7 step:6932 [D loss: 0.617067, acc.: 63.28%] [G loss: 0.646158]\n",
      "epoch:7 step:6933 [D loss: 0.504473, acc.: 75.00%] [G loss: 0.641952]\n",
      "epoch:7 step:6934 [D loss: 0.550671, acc.: 73.44%] [G loss: 0.750320]\n",
      "epoch:7 step:6935 [D loss: 0.700108, acc.: 58.59%] [G loss: 0.499394]\n",
      "epoch:7 step:6936 [D loss: 0.616056, acc.: 61.72%] [G loss: 0.467793]\n",
      "epoch:7 step:6937 [D loss: 0.557869, acc.: 69.53%] [G loss: 0.507473]\n",
      "epoch:7 step:6938 [D loss: 0.576994, acc.: 68.75%] [G loss: 0.535831]\n",
      "epoch:7 step:6939 [D loss: 0.605280, acc.: 65.62%] [G loss: 0.459054]\n",
      "epoch:7 step:6940 [D loss: 0.472995, acc.: 77.34%] [G loss: 0.487747]\n",
      "epoch:7 step:6941 [D loss: 0.532403, acc.: 73.44%] [G loss: 0.520539]\n",
      "epoch:7 step:6942 [D loss: 0.567594, acc.: 64.06%] [G loss: 0.538609]\n",
      "epoch:7 step:6943 [D loss: 0.585312, acc.: 62.50%] [G loss: 0.552959]\n",
      "epoch:7 step:6944 [D loss: 0.544545, acc.: 71.88%] [G loss: 0.557991]\n",
      "epoch:7 step:6945 [D loss: 0.609124, acc.: 61.72%] [G loss: 0.434325]\n",
      "epoch:7 step:6946 [D loss: 0.524157, acc.: 78.12%] [G loss: 0.455817]\n",
      "epoch:7 step:6947 [D loss: 0.524320, acc.: 75.00%] [G loss: 0.449572]\n",
      "epoch:7 step:6948 [D loss: 0.492817, acc.: 76.56%] [G loss: 0.547761]\n",
      "epoch:7 step:6949 [D loss: 0.613959, acc.: 63.28%] [G loss: 0.561428]\n",
      "epoch:7 step:6950 [D loss: 0.548910, acc.: 72.66%] [G loss: 0.481683]\n",
      "epoch:7 step:6951 [D loss: 0.521955, acc.: 72.66%] [G loss: 0.478061]\n",
      "epoch:7 step:6952 [D loss: 0.622813, acc.: 64.06%] [G loss: 0.497984]\n",
      "epoch:7 step:6953 [D loss: 0.549712, acc.: 70.31%] [G loss: 0.440158]\n",
      "epoch:7 step:6954 [D loss: 0.542353, acc.: 74.22%] [G loss: 0.485715]\n",
      "epoch:7 step:6955 [D loss: 0.626201, acc.: 57.81%] [G loss: 0.559327]\n",
      "epoch:7 step:6956 [D loss: 0.577103, acc.: 67.19%] [G loss: 0.457707]\n",
      "epoch:7 step:6957 [D loss: 0.526784, acc.: 71.88%] [G loss: 0.726028]\n",
      "epoch:7 step:6958 [D loss: 0.508358, acc.: 76.56%] [G loss: 0.723681]\n",
      "epoch:7 step:6959 [D loss: 0.685453, acc.: 58.59%] [G loss: 0.491032]\n",
      "epoch:7 step:6960 [D loss: 0.599066, acc.: 62.50%] [G loss: 0.522603]\n",
      "epoch:7 step:6961 [D loss: 0.478137, acc.: 78.12%] [G loss: 0.469863]\n",
      "epoch:7 step:6962 [D loss: 0.497320, acc.: 75.78%] [G loss: 0.473499]\n",
      "epoch:7 step:6963 [D loss: 0.627208, acc.: 60.16%] [G loss: 0.519109]\n",
      "epoch:7 step:6964 [D loss: 0.532445, acc.: 72.66%] [G loss: 0.515352]\n",
      "epoch:7 step:6965 [D loss: 0.521931, acc.: 71.09%] [G loss: 0.702539]\n",
      "epoch:7 step:6966 [D loss: 0.621872, acc.: 65.62%] [G loss: 0.508810]\n",
      "epoch:7 step:6967 [D loss: 0.603103, acc.: 65.62%] [G loss: 0.536547]\n",
      "epoch:7 step:6968 [D loss: 0.566767, acc.: 70.31%] [G loss: 0.638880]\n",
      "epoch:7 step:6969 [D loss: 0.587585, acc.: 66.41%] [G loss: 0.543496]\n",
      "epoch:7 step:6970 [D loss: 0.546384, acc.: 69.53%] [G loss: 0.457657]\n",
      "epoch:7 step:6971 [D loss: 0.589828, acc.: 65.62%] [G loss: 0.442766]\n",
      "epoch:7 step:6972 [D loss: 0.552880, acc.: 70.31%] [G loss: 0.555389]\n",
      "epoch:7 step:6973 [D loss: 0.552150, acc.: 67.97%] [G loss: 0.489075]\n",
      "epoch:7 step:6974 [D loss: 0.584564, acc.: 65.62%] [G loss: 0.496830]\n",
      "epoch:7 step:6975 [D loss: 0.509638, acc.: 76.56%] [G loss: 0.585455]\n",
      "epoch:7 step:6976 [D loss: 0.570265, acc.: 66.41%] [G loss: 0.488711]\n",
      "epoch:7 step:6977 [D loss: 0.639364, acc.: 61.72%] [G loss: 0.447736]\n",
      "epoch:7 step:6978 [D loss: 0.577091, acc.: 66.41%] [G loss: 0.504041]\n",
      "epoch:7 step:6979 [D loss: 0.604425, acc.: 64.06%] [G loss: 0.588753]\n",
      "epoch:7 step:6980 [D loss: 0.574114, acc.: 74.22%] [G loss: 0.513296]\n",
      "epoch:7 step:6981 [D loss: 0.603630, acc.: 67.97%] [G loss: 0.511157]\n",
      "epoch:7 step:6982 [D loss: 0.591758, acc.: 70.31%] [G loss: 0.411172]\n",
      "epoch:7 step:6983 [D loss: 0.595681, acc.: 64.06%] [G loss: 0.444927]\n",
      "epoch:7 step:6984 [D loss: 0.546216, acc.: 71.09%] [G loss: 0.575619]\n",
      "epoch:7 step:6985 [D loss: 0.503604, acc.: 71.88%] [G loss: 0.717549]\n",
      "epoch:7 step:6986 [D loss: 0.565383, acc.: 71.88%] [G loss: 0.661138]\n",
      "epoch:7 step:6987 [D loss: 0.511726, acc.: 73.44%] [G loss: 0.675796]\n",
      "epoch:7 step:6988 [D loss: 0.464662, acc.: 75.00%] [G loss: 0.890785]\n",
      "epoch:7 step:6989 [D loss: 0.514820, acc.: 75.00%] [G loss: 0.640422]\n",
      "epoch:7 step:6990 [D loss: 0.559160, acc.: 75.78%] [G loss: 0.502701]\n",
      "epoch:7 step:6991 [D loss: 0.540910, acc.: 71.88%] [G loss: 0.508538]\n",
      "epoch:7 step:6992 [D loss: 0.586890, acc.: 68.75%] [G loss: 0.441646]\n",
      "epoch:7 step:6993 [D loss: 0.490702, acc.: 79.69%] [G loss: 0.653190]\n",
      "epoch:7 step:6994 [D loss: 0.537918, acc.: 71.88%] [G loss: 0.503308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6995 [D loss: 0.499264, acc.: 78.91%] [G loss: 0.589331]\n",
      "epoch:7 step:6996 [D loss: 0.724302, acc.: 60.16%] [G loss: 0.530630]\n",
      "epoch:7 step:6997 [D loss: 0.585179, acc.: 65.62%] [G loss: 0.484677]\n",
      "epoch:7 step:6998 [D loss: 0.493774, acc.: 76.56%] [G loss: 0.609446]\n",
      "epoch:7 step:6999 [D loss: 0.540565, acc.: 68.75%] [G loss: 0.630202]\n",
      "epoch:7 step:7000 [D loss: 0.496198, acc.: 75.78%] [G loss: 0.573574]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.183992\n",
      "FID: 44.908993\n",
      "0 = 12.885483188724479\n",
      "1 = 0.0967123573668011\n",
      "2 = 0.920799970626831\n",
      "3 = 0.8537999987602234\n",
      "4 = 0.9878000020980835\n",
      "5 = 0.9859122633934021\n",
      "6 = 0.8537999987602234\n",
      "7 = 8.10562458803653\n",
      "8 = 0.1376273345887843\n",
      "9 = 0.7688999772071838\n",
      "10 = 0.7427999973297119\n",
      "11 = 0.7950000166893005\n",
      "12 = 0.7837096452713013\n",
      "13 = 0.7427999973297119\n",
      "14 = 6.184017658233643\n",
      "15 = 7.456535339355469\n",
      "16 = 0.3478993773460388\n",
      "17 = 6.183992385864258\n",
      "18 = 44.908992767333984\n",
      "epoch:7 step:7001 [D loss: 0.583856, acc.: 67.97%] [G loss: 0.506340]\n",
      "epoch:7 step:7002 [D loss: 0.551274, acc.: 73.44%] [G loss: 0.638580]\n",
      "epoch:7 step:7003 [D loss: 0.572839, acc.: 71.09%] [G loss: 0.667520]\n",
      "epoch:7 step:7004 [D loss: 0.530146, acc.: 73.44%] [G loss: 0.548847]\n",
      "epoch:7 step:7005 [D loss: 0.570842, acc.: 69.53%] [G loss: 0.623948]\n",
      "epoch:7 step:7006 [D loss: 0.525697, acc.: 74.22%] [G loss: 0.498266]\n",
      "epoch:7 step:7007 [D loss: 0.543916, acc.: 71.88%] [G loss: 0.543313]\n",
      "epoch:7 step:7008 [D loss: 0.515508, acc.: 75.00%] [G loss: 0.607139]\n",
      "epoch:7 step:7009 [D loss: 0.501046, acc.: 72.66%] [G loss: 0.770856]\n",
      "epoch:7 step:7010 [D loss: 0.426611, acc.: 82.03%] [G loss: 0.827904]\n",
      "epoch:7 step:7011 [D loss: 0.464888, acc.: 78.12%] [G loss: 0.676625]\n",
      "epoch:7 step:7012 [D loss: 0.540469, acc.: 74.22%] [G loss: 0.592488]\n",
      "epoch:7 step:7013 [D loss: 0.609529, acc.: 67.97%] [G loss: 0.516973]\n",
      "epoch:7 step:7014 [D loss: 0.496666, acc.: 79.69%] [G loss: 0.536532]\n",
      "epoch:7 step:7015 [D loss: 0.634761, acc.: 61.72%] [G loss: 0.474419]\n",
      "epoch:7 step:7016 [D loss: 0.499798, acc.: 78.12%] [G loss: 0.590116]\n",
      "epoch:7 step:7017 [D loss: 0.721252, acc.: 57.81%] [G loss: 0.344076]\n",
      "epoch:7 step:7018 [D loss: 0.600088, acc.: 63.28%] [G loss: 0.451225]\n",
      "epoch:7 step:7019 [D loss: 0.527637, acc.: 73.44%] [G loss: 0.515378]\n",
      "epoch:7 step:7020 [D loss: 0.517034, acc.: 77.34%] [G loss: 0.633942]\n",
      "epoch:7 step:7021 [D loss: 0.589389, acc.: 64.06%] [G loss: 0.486211]\n",
      "epoch:7 step:7022 [D loss: 0.562560, acc.: 69.53%] [G loss: 0.557061]\n",
      "epoch:7 step:7023 [D loss: 0.547138, acc.: 72.66%] [G loss: 0.518308]\n",
      "epoch:7 step:7024 [D loss: 0.671607, acc.: 60.16%] [G loss: 0.361105]\n",
      "epoch:7 step:7025 [D loss: 0.582393, acc.: 64.84%] [G loss: 0.436763]\n",
      "epoch:7 step:7026 [D loss: 0.472120, acc.: 77.34%] [G loss: 0.593331]\n",
      "epoch:7 step:7027 [D loss: 0.599651, acc.: 67.19%] [G loss: 0.723320]\n",
      "epoch:7 step:7028 [D loss: 0.598219, acc.: 71.09%] [G loss: 0.560398]\n",
      "epoch:7 step:7029 [D loss: 0.541103, acc.: 67.97%] [G loss: 0.694749]\n",
      "epoch:7 step:7030 [D loss: 0.489054, acc.: 77.34%] [G loss: 0.738459]\n",
      "epoch:7 step:7031 [D loss: 0.479895, acc.: 75.78%] [G loss: 0.690052]\n",
      "epoch:7 step:7032 [D loss: 0.610488, acc.: 65.62%] [G loss: 0.625116]\n",
      "epoch:7 step:7033 [D loss: 0.557066, acc.: 71.88%] [G loss: 0.590718]\n",
      "epoch:7 step:7034 [D loss: 0.456505, acc.: 84.38%] [G loss: 0.655626]\n",
      "epoch:7 step:7035 [D loss: 0.603877, acc.: 64.84%] [G loss: 0.536777]\n",
      "epoch:7 step:7036 [D loss: 0.737197, acc.: 56.25%] [G loss: 0.468394]\n",
      "epoch:7 step:7037 [D loss: 0.567098, acc.: 67.19%] [G loss: 0.368077]\n",
      "epoch:7 step:7038 [D loss: 0.539801, acc.: 77.34%] [G loss: 0.559600]\n",
      "epoch:7 step:7039 [D loss: 0.549422, acc.: 72.66%] [G loss: 0.532839]\n",
      "epoch:7 step:7040 [D loss: 0.501897, acc.: 76.56%] [G loss: 0.577519]\n",
      "epoch:7 step:7041 [D loss: 0.620610, acc.: 66.41%] [G loss: 0.437572]\n",
      "epoch:7 step:7042 [D loss: 0.588976, acc.: 64.84%] [G loss: 0.477213]\n",
      "epoch:7 step:7043 [D loss: 0.506434, acc.: 77.34%] [G loss: 0.570647]\n",
      "epoch:7 step:7044 [D loss: 0.571326, acc.: 70.31%] [G loss: 0.588880]\n",
      "epoch:7 step:7045 [D loss: 0.589960, acc.: 72.66%] [G loss: 0.573764]\n",
      "epoch:7 step:7046 [D loss: 0.583057, acc.: 68.75%] [G loss: 0.516043]\n",
      "epoch:7 step:7047 [D loss: 0.534168, acc.: 70.31%] [G loss: 0.558338]\n",
      "epoch:7 step:7048 [D loss: 0.553240, acc.: 71.88%] [G loss: 0.537095]\n",
      "epoch:7 step:7049 [D loss: 0.618738, acc.: 64.84%] [G loss: 0.474680]\n",
      "epoch:7 step:7050 [D loss: 0.556258, acc.: 71.09%] [G loss: 0.608348]\n",
      "epoch:7 step:7051 [D loss: 0.595566, acc.: 64.06%] [G loss: 0.466348]\n",
      "epoch:7 step:7052 [D loss: 0.530244, acc.: 73.44%] [G loss: 0.575667]\n",
      "epoch:7 step:7053 [D loss: 0.576647, acc.: 71.09%] [G loss: 0.555378]\n",
      "epoch:7 step:7054 [D loss: 0.542032, acc.: 74.22%] [G loss: 0.453355]\n",
      "epoch:7 step:7055 [D loss: 0.575281, acc.: 74.22%] [G loss: 0.503795]\n",
      "epoch:7 step:7056 [D loss: 0.543073, acc.: 71.09%] [G loss: 0.565038]\n",
      "epoch:7 step:7057 [D loss: 0.471779, acc.: 79.69%] [G loss: 0.662924]\n",
      "epoch:7 step:7058 [D loss: 0.515097, acc.: 76.56%] [G loss: 0.702921]\n",
      "epoch:7 step:7059 [D loss: 0.649105, acc.: 60.16%] [G loss: 0.597500]\n",
      "epoch:7 step:7060 [D loss: 0.658733, acc.: 61.72%] [G loss: 0.402182]\n",
      "epoch:7 step:7061 [D loss: 0.586582, acc.: 67.97%] [G loss: 0.412141]\n",
      "epoch:7 step:7062 [D loss: 0.489788, acc.: 73.44%] [G loss: 0.453925]\n",
      "epoch:7 step:7063 [D loss: 0.507351, acc.: 76.56%] [G loss: 0.566423]\n",
      "epoch:7 step:7064 [D loss: 0.514396, acc.: 73.44%] [G loss: 0.544374]\n",
      "epoch:7 step:7065 [D loss: 0.567581, acc.: 70.31%] [G loss: 0.567842]\n",
      "epoch:7 step:7066 [D loss: 0.507769, acc.: 78.12%] [G loss: 0.640877]\n",
      "epoch:7 step:7067 [D loss: 0.447962, acc.: 82.03%] [G loss: 0.704075]\n",
      "epoch:7 step:7068 [D loss: 0.492381, acc.: 78.91%] [G loss: 0.617955]\n",
      "epoch:7 step:7069 [D loss: 0.545628, acc.: 68.75%] [G loss: 0.605337]\n",
      "epoch:7 step:7070 [D loss: 0.647947, acc.: 57.03%] [G loss: 0.357981]\n",
      "epoch:7 step:7071 [D loss: 0.592124, acc.: 67.97%] [G loss: 0.407351]\n",
      "epoch:7 step:7072 [D loss: 0.568332, acc.: 67.97%] [G loss: 0.432599]\n",
      "epoch:7 step:7073 [D loss: 0.499416, acc.: 74.22%] [G loss: 0.569651]\n",
      "epoch:7 step:7074 [D loss: 0.517427, acc.: 72.66%] [G loss: 0.590786]\n",
      "epoch:7 step:7075 [D loss: 0.467742, acc.: 75.78%] [G loss: 0.704649]\n",
      "epoch:7 step:7076 [D loss: 0.537340, acc.: 78.91%] [G loss: 0.571273]\n",
      "epoch:7 step:7077 [D loss: 0.506441, acc.: 76.56%] [G loss: 0.493101]\n",
      "epoch:7 step:7078 [D loss: 0.483996, acc.: 77.34%] [G loss: 0.709415]\n",
      "epoch:7 step:7079 [D loss: 0.494657, acc.: 74.22%] [G loss: 0.641465]\n",
      "epoch:7 step:7080 [D loss: 0.495499, acc.: 78.12%] [G loss: 0.568386]\n",
      "epoch:7 step:7081 [D loss: 0.582251, acc.: 69.53%] [G loss: 0.555858]\n",
      "epoch:7 step:7082 [D loss: 0.534885, acc.: 75.00%] [G loss: 0.577390]\n",
      "epoch:7 step:7083 [D loss: 0.510128, acc.: 73.44%] [G loss: 0.587579]\n",
      "epoch:7 step:7084 [D loss: 0.632710, acc.: 64.06%] [G loss: 0.475862]\n",
      "epoch:7 step:7085 [D loss: 0.540210, acc.: 69.53%] [G loss: 0.546817]\n",
      "epoch:7 step:7086 [D loss: 0.566668, acc.: 63.28%] [G loss: 0.479016]\n",
      "epoch:7 step:7087 [D loss: 0.625780, acc.: 67.97%] [G loss: 0.511962]\n",
      "epoch:7 step:7088 [D loss: 0.608019, acc.: 60.94%] [G loss: 0.439557]\n",
      "epoch:7 step:7089 [D loss: 0.516448, acc.: 71.88%] [G loss: 0.467269]\n",
      "epoch:7 step:7090 [D loss: 0.553545, acc.: 72.66%] [G loss: 0.497114]\n",
      "epoch:7 step:7091 [D loss: 0.580700, acc.: 68.75%] [G loss: 0.442376]\n",
      "epoch:7 step:7092 [D loss: 0.532200, acc.: 71.88%] [G loss: 0.655363]\n",
      "epoch:7 step:7093 [D loss: 0.521770, acc.: 73.44%] [G loss: 0.623231]\n",
      "epoch:7 step:7094 [D loss: 0.675643, acc.: 57.03%] [G loss: 0.438005]\n",
      "epoch:7 step:7095 [D loss: 0.503787, acc.: 75.00%] [G loss: 0.538993]\n",
      "epoch:7 step:7096 [D loss: 0.565248, acc.: 71.09%] [G loss: 0.571075]\n",
      "epoch:7 step:7097 [D loss: 0.575700, acc.: 65.62%] [G loss: 0.492732]\n",
      "epoch:7 step:7098 [D loss: 0.507942, acc.: 78.12%] [G loss: 0.565553]\n",
      "epoch:7 step:7099 [D loss: 0.576015, acc.: 68.75%] [G loss: 0.539676]\n",
      "epoch:7 step:7100 [D loss: 0.557837, acc.: 68.75%] [G loss: 0.537536]\n",
      "epoch:7 step:7101 [D loss: 0.582344, acc.: 68.75%] [G loss: 0.448584]\n",
      "epoch:7 step:7102 [D loss: 0.622297, acc.: 64.84%] [G loss: 0.542503]\n",
      "epoch:7 step:7103 [D loss: 0.584152, acc.: 64.84%] [G loss: 0.491585]\n",
      "epoch:7 step:7104 [D loss: 0.565714, acc.: 71.09%] [G loss: 0.531947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7105 [D loss: 0.527890, acc.: 72.66%] [G loss: 0.682759]\n",
      "epoch:7 step:7106 [D loss: 0.545535, acc.: 71.88%] [G loss: 0.758380]\n",
      "epoch:7 step:7107 [D loss: 0.540594, acc.: 67.19%] [G loss: 0.585820]\n",
      "epoch:7 step:7108 [D loss: 0.562371, acc.: 69.53%] [G loss: 0.687527]\n",
      "epoch:7 step:7109 [D loss: 0.582467, acc.: 67.97%] [G loss: 0.559197]\n",
      "epoch:7 step:7110 [D loss: 0.560199, acc.: 73.44%] [G loss: 0.429351]\n",
      "epoch:7 step:7111 [D loss: 0.525085, acc.: 74.22%] [G loss: 0.527345]\n",
      "epoch:7 step:7112 [D loss: 0.555484, acc.: 70.31%] [G loss: 0.470249]\n",
      "epoch:7 step:7113 [D loss: 0.444053, acc.: 78.12%] [G loss: 0.567626]\n",
      "epoch:7 step:7114 [D loss: 0.541920, acc.: 68.75%] [G loss: 0.599026]\n",
      "epoch:7 step:7115 [D loss: 0.520161, acc.: 73.44%] [G loss: 0.617943]\n",
      "epoch:7 step:7116 [D loss: 0.560036, acc.: 69.53%] [G loss: 0.692204]\n",
      "epoch:7 step:7117 [D loss: 0.487217, acc.: 76.56%] [G loss: 0.671479]\n",
      "epoch:7 step:7118 [D loss: 0.583449, acc.: 67.97%] [G loss: 0.621768]\n",
      "epoch:7 step:7119 [D loss: 0.563526, acc.: 65.62%] [G loss: 0.591888]\n",
      "epoch:7 step:7120 [D loss: 0.526390, acc.: 74.22%] [G loss: 0.631739]\n",
      "epoch:7 step:7121 [D loss: 0.606668, acc.: 61.72%] [G loss: 0.555346]\n",
      "epoch:7 step:7122 [D loss: 0.591095, acc.: 67.97%] [G loss: 0.459516]\n",
      "epoch:7 step:7123 [D loss: 0.498136, acc.: 78.12%] [G loss: 0.544468]\n",
      "epoch:7 step:7124 [D loss: 0.603168, acc.: 64.84%] [G loss: 0.640228]\n",
      "epoch:7 step:7125 [D loss: 0.671019, acc.: 63.28%] [G loss: 0.502178]\n",
      "epoch:7 step:7126 [D loss: 0.504864, acc.: 75.00%] [G loss: 0.504076]\n",
      "epoch:7 step:7127 [D loss: 0.571194, acc.: 70.31%] [G loss: 0.462992]\n",
      "epoch:7 step:7128 [D loss: 0.543920, acc.: 69.53%] [G loss: 0.404258]\n",
      "epoch:7 step:7129 [D loss: 0.530308, acc.: 74.22%] [G loss: 0.521570]\n",
      "epoch:7 step:7130 [D loss: 0.514462, acc.: 75.00%] [G loss: 0.572206]\n",
      "epoch:7 step:7131 [D loss: 0.566273, acc.: 68.75%] [G loss: 0.528728]\n",
      "epoch:7 step:7132 [D loss: 0.546233, acc.: 75.00%] [G loss: 0.419181]\n",
      "epoch:7 step:7133 [D loss: 0.550156, acc.: 71.88%] [G loss: 0.531394]\n",
      "epoch:7 step:7134 [D loss: 0.498469, acc.: 76.56%] [G loss: 0.625490]\n",
      "epoch:7 step:7135 [D loss: 0.602072, acc.: 64.06%] [G loss: 0.632932]\n",
      "epoch:7 step:7136 [D loss: 0.569207, acc.: 74.22%] [G loss: 0.518434]\n",
      "epoch:7 step:7137 [D loss: 0.594190, acc.: 66.41%] [G loss: 0.579583]\n",
      "epoch:7 step:7138 [D loss: 0.558097, acc.: 70.31%] [G loss: 0.455296]\n",
      "epoch:7 step:7139 [D loss: 0.545531, acc.: 73.44%] [G loss: 0.676731]\n",
      "epoch:7 step:7140 [D loss: 0.598864, acc.: 64.06%] [G loss: 0.534356]\n",
      "epoch:7 step:7141 [D loss: 0.491650, acc.: 77.34%] [G loss: 0.637820]\n",
      "epoch:7 step:7142 [D loss: 0.598035, acc.: 68.75%] [G loss: 0.645214]\n",
      "epoch:7 step:7143 [D loss: 0.706719, acc.: 55.47%] [G loss: 0.651048]\n",
      "epoch:7 step:7144 [D loss: 0.555155, acc.: 72.66%] [G loss: 0.509447]\n",
      "epoch:7 step:7145 [D loss: 0.601523, acc.: 64.84%] [G loss: 0.390099]\n",
      "epoch:7 step:7146 [D loss: 0.645704, acc.: 56.25%] [G loss: 0.437751]\n",
      "epoch:7 step:7147 [D loss: 0.559852, acc.: 68.75%] [G loss: 0.472297]\n",
      "epoch:7 step:7148 [D loss: 0.537966, acc.: 67.97%] [G loss: 0.536339]\n",
      "epoch:7 step:7149 [D loss: 0.583851, acc.: 67.97%] [G loss: 0.594463]\n",
      "epoch:7 step:7150 [D loss: 0.560426, acc.: 71.88%] [G loss: 0.500056]\n",
      "epoch:7 step:7151 [D loss: 0.505565, acc.: 74.22%] [G loss: 0.518972]\n",
      "epoch:7 step:7152 [D loss: 0.523364, acc.: 71.88%] [G loss: 0.599003]\n",
      "epoch:7 step:7153 [D loss: 0.525603, acc.: 76.56%] [G loss: 0.501571]\n",
      "epoch:7 step:7154 [D loss: 0.542279, acc.: 70.31%] [G loss: 0.489186]\n",
      "epoch:7 step:7155 [D loss: 0.524369, acc.: 72.66%] [G loss: 0.549962]\n",
      "epoch:7 step:7156 [D loss: 0.570749, acc.: 68.75%] [G loss: 0.485502]\n",
      "epoch:7 step:7157 [D loss: 0.513155, acc.: 78.12%] [G loss: 0.621128]\n",
      "epoch:7 step:7158 [D loss: 0.552358, acc.: 69.53%] [G loss: 0.658746]\n",
      "epoch:7 step:7159 [D loss: 0.615229, acc.: 64.06%] [G loss: 0.505874]\n",
      "epoch:7 step:7160 [D loss: 0.574609, acc.: 64.84%] [G loss: 0.464909]\n",
      "epoch:7 step:7161 [D loss: 0.510477, acc.: 73.44%] [G loss: 0.492333]\n",
      "epoch:7 step:7162 [D loss: 0.536525, acc.: 69.53%] [G loss: 0.650997]\n",
      "epoch:7 step:7163 [D loss: 0.572257, acc.: 73.44%] [G loss: 0.461835]\n",
      "epoch:7 step:7164 [D loss: 0.506947, acc.: 77.34%] [G loss: 0.690591]\n",
      "epoch:7 step:7165 [D loss: 0.580874, acc.: 71.88%] [G loss: 0.471529]\n",
      "epoch:7 step:7166 [D loss: 0.571875, acc.: 69.53%] [G loss: 0.393159]\n",
      "epoch:7 step:7167 [D loss: 0.581427, acc.: 66.41%] [G loss: 0.465198]\n",
      "epoch:7 step:7168 [D loss: 0.496877, acc.: 73.44%] [G loss: 0.477184]\n",
      "epoch:7 step:7169 [D loss: 0.554236, acc.: 73.44%] [G loss: 0.428503]\n",
      "epoch:7 step:7170 [D loss: 0.546415, acc.: 68.75%] [G loss: 0.447163]\n",
      "epoch:7 step:7171 [D loss: 0.595866, acc.: 65.62%] [G loss: 0.466481]\n",
      "epoch:7 step:7172 [D loss: 0.469157, acc.: 78.91%] [G loss: 0.478043]\n",
      "epoch:7 step:7173 [D loss: 0.599598, acc.: 62.50%] [G loss: 0.443668]\n",
      "epoch:7 step:7174 [D loss: 0.617882, acc.: 64.06%] [G loss: 0.679739]\n",
      "epoch:7 step:7175 [D loss: 0.601496, acc.: 64.84%] [G loss: 0.590427]\n",
      "epoch:7 step:7176 [D loss: 0.539607, acc.: 74.22%] [G loss: 0.523058]\n",
      "epoch:7 step:7177 [D loss: 0.564887, acc.: 67.97%] [G loss: 0.471428]\n",
      "epoch:7 step:7178 [D loss: 0.552774, acc.: 67.97%] [G loss: 0.545603]\n",
      "epoch:7 step:7179 [D loss: 0.560197, acc.: 70.31%] [G loss: 0.548130]\n",
      "epoch:7 step:7180 [D loss: 0.606918, acc.: 64.84%] [G loss: 0.490631]\n",
      "epoch:7 step:7181 [D loss: 0.572141, acc.: 72.66%] [G loss: 0.479787]\n",
      "epoch:7 step:7182 [D loss: 0.485309, acc.: 78.12%] [G loss: 0.616423]\n",
      "epoch:7 step:7183 [D loss: 0.517408, acc.: 75.00%] [G loss: 0.641745]\n",
      "epoch:7 step:7184 [D loss: 0.631468, acc.: 62.50%] [G loss: 0.496876]\n",
      "epoch:7 step:7185 [D loss: 0.500767, acc.: 71.88%] [G loss: 0.499910]\n",
      "epoch:7 step:7186 [D loss: 0.529487, acc.: 73.44%] [G loss: 0.440077]\n",
      "epoch:7 step:7187 [D loss: 0.594080, acc.: 64.84%] [G loss: 0.479777]\n",
      "epoch:7 step:7188 [D loss: 0.513565, acc.: 73.44%] [G loss: 0.644440]\n",
      "epoch:7 step:7189 [D loss: 0.556953, acc.: 72.66%] [G loss: 0.574876]\n",
      "epoch:7 step:7190 [D loss: 0.532301, acc.: 73.44%] [G loss: 0.499703]\n",
      "epoch:7 step:7191 [D loss: 0.541801, acc.: 73.44%] [G loss: 0.489750]\n",
      "epoch:7 step:7192 [D loss: 0.477711, acc.: 76.56%] [G loss: 0.646288]\n",
      "epoch:7 step:7193 [D loss: 0.476338, acc.: 81.25%] [G loss: 0.677333]\n",
      "epoch:7 step:7194 [D loss: 0.537127, acc.: 70.31%] [G loss: 0.561616]\n",
      "epoch:7 step:7195 [D loss: 0.571431, acc.: 72.66%] [G loss: 0.435695]\n",
      "epoch:7 step:7196 [D loss: 0.565700, acc.: 72.66%] [G loss: 0.619393]\n",
      "epoch:7 step:7197 [D loss: 0.547294, acc.: 71.88%] [G loss: 0.443753]\n",
      "epoch:7 step:7198 [D loss: 0.501768, acc.: 72.66%] [G loss: 0.646510]\n",
      "epoch:7 step:7199 [D loss: 0.524662, acc.: 71.09%] [G loss: 0.679887]\n",
      "epoch:7 step:7200 [D loss: 0.500972, acc.: 74.22%] [G loss: 0.714930]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.107912\n",
      "FID: 43.504871\n",
      "0 = 13.124977244853964\n",
      "1 = 0.0945546271052826\n",
      "2 = 0.9352999925613403\n",
      "3 = 0.8838000297546387\n",
      "4 = 0.9868000149726868\n",
      "5 = 0.9852842688560486\n",
      "6 = 0.8838000297546387\n",
      "7 = 8.223008236455916\n",
      "8 = 0.13539705378991004\n",
      "9 = 0.7766000032424927\n",
      "10 = 0.754800021648407\n",
      "11 = 0.7983999848365784\n",
      "12 = 0.7892095446586609\n",
      "13 = 0.754800021648407\n",
      "14 = 6.107933044433594\n",
      "15 = 7.758033752441406\n",
      "16 = 0.3400818407535553\n",
      "17 = 6.107911586761475\n",
      "18 = 43.5048713684082\n",
      "epoch:7 step:7201 [D loss: 0.486934, acc.: 75.78%] [G loss: 0.793950]\n",
      "epoch:7 step:7202 [D loss: 0.554354, acc.: 67.19%] [G loss: 0.666555]\n",
      "epoch:7 step:7203 [D loss: 0.595640, acc.: 66.41%] [G loss: 0.492006]\n",
      "epoch:7 step:7204 [D loss: 0.505319, acc.: 77.34%] [G loss: 0.530080]\n",
      "epoch:7 step:7205 [D loss: 0.600298, acc.: 64.84%] [G loss: 0.511941]\n",
      "epoch:7 step:7206 [D loss: 0.457566, acc.: 80.47%] [G loss: 0.727383]\n",
      "epoch:7 step:7207 [D loss: 0.399231, acc.: 82.03%] [G loss: 1.050632]\n",
      "epoch:7 step:7208 [D loss: 0.520233, acc.: 75.78%] [G loss: 0.767444]\n",
      "epoch:7 step:7209 [D loss: 0.488939, acc.: 74.22%] [G loss: 0.750246]\n",
      "epoch:7 step:7210 [D loss: 0.521835, acc.: 74.22%] [G loss: 0.634398]\n",
      "epoch:7 step:7211 [D loss: 0.640230, acc.: 60.94%] [G loss: 0.516674]\n",
      "epoch:7 step:7212 [D loss: 0.660949, acc.: 57.03%] [G loss: 0.457850]\n",
      "epoch:7 step:7213 [D loss: 0.503337, acc.: 72.66%] [G loss: 0.535825]\n",
      "epoch:7 step:7214 [D loss: 0.549791, acc.: 71.88%] [G loss: 0.497923]\n",
      "epoch:7 step:7215 [D loss: 0.552608, acc.: 66.41%] [G loss: 0.652916]\n",
      "epoch:7 step:7216 [D loss: 0.568269, acc.: 64.84%] [G loss: 0.527921]\n",
      "epoch:7 step:7217 [D loss: 0.622620, acc.: 65.62%] [G loss: 0.493074]\n",
      "epoch:7 step:7218 [D loss: 0.571666, acc.: 68.75%] [G loss: 0.471932]\n",
      "epoch:7 step:7219 [D loss: 0.544324, acc.: 71.88%] [G loss: 0.550973]\n",
      "epoch:7 step:7220 [D loss: 0.524754, acc.: 75.00%] [G loss: 0.534531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7221 [D loss: 0.622255, acc.: 66.41%] [G loss: 0.477180]\n",
      "epoch:7 step:7222 [D loss: 0.582758, acc.: 69.53%] [G loss: 0.531848]\n",
      "epoch:7 step:7223 [D loss: 0.590962, acc.: 65.62%] [G loss: 0.530942]\n",
      "epoch:7 step:7224 [D loss: 0.587878, acc.: 65.62%] [G loss: 0.525740]\n",
      "epoch:7 step:7225 [D loss: 0.536714, acc.: 75.00%] [G loss: 0.450545]\n",
      "epoch:7 step:7226 [D loss: 0.615312, acc.: 67.19%] [G loss: 0.572384]\n",
      "epoch:7 step:7227 [D loss: 0.575053, acc.: 76.56%] [G loss: 0.473170]\n",
      "epoch:7 step:7228 [D loss: 0.531864, acc.: 73.44%] [G loss: 0.618667]\n",
      "epoch:7 step:7229 [D loss: 0.546790, acc.: 73.44%] [G loss: 0.537433]\n",
      "epoch:7 step:7230 [D loss: 0.591589, acc.: 67.19%] [G loss: 0.512131]\n",
      "epoch:7 step:7231 [D loss: 0.631336, acc.: 64.06%] [G loss: 0.449718]\n",
      "epoch:7 step:7232 [D loss: 0.590263, acc.: 66.41%] [G loss: 0.544684]\n",
      "epoch:7 step:7233 [D loss: 0.614987, acc.: 68.75%] [G loss: 0.636389]\n",
      "epoch:7 step:7234 [D loss: 0.513553, acc.: 75.00%] [G loss: 0.520095]\n",
      "epoch:7 step:7235 [D loss: 0.506590, acc.: 73.44%] [G loss: 0.542261]\n",
      "epoch:7 step:7236 [D loss: 0.494647, acc.: 73.44%] [G loss: 0.461782]\n",
      "epoch:7 step:7237 [D loss: 0.574135, acc.: 66.41%] [G loss: 0.477996]\n",
      "epoch:7 step:7238 [D loss: 0.584714, acc.: 66.41%] [G loss: 0.522637]\n",
      "epoch:7 step:7239 [D loss: 0.563223, acc.: 76.56%] [G loss: 0.593491]\n",
      "epoch:7 step:7240 [D loss: 0.488182, acc.: 79.69%] [G loss: 0.532534]\n",
      "epoch:7 step:7241 [D loss: 0.483111, acc.: 81.25%] [G loss: 0.518580]\n",
      "epoch:7 step:7242 [D loss: 0.559395, acc.: 68.75%] [G loss: 0.505634]\n",
      "epoch:7 step:7243 [D loss: 0.565947, acc.: 70.31%] [G loss: 0.415883]\n",
      "epoch:7 step:7244 [D loss: 0.519668, acc.: 76.56%] [G loss: 0.515564]\n",
      "epoch:7 step:7245 [D loss: 0.582298, acc.: 70.31%] [G loss: 0.504588]\n",
      "epoch:7 step:7246 [D loss: 0.559598, acc.: 68.75%] [G loss: 0.581091]\n",
      "epoch:7 step:7247 [D loss: 0.569132, acc.: 67.97%] [G loss: 0.517995]\n",
      "epoch:7 step:7248 [D loss: 0.589438, acc.: 67.97%] [G loss: 0.520949]\n",
      "epoch:7 step:7249 [D loss: 0.553502, acc.: 69.53%] [G loss: 0.636557]\n",
      "epoch:7 step:7250 [D loss: 0.494951, acc.: 74.22%] [G loss: 0.679935]\n",
      "epoch:7 step:7251 [D loss: 0.585867, acc.: 71.09%] [G loss: 0.513617]\n",
      "epoch:7 step:7252 [D loss: 0.536606, acc.: 71.09%] [G loss: 0.462650]\n",
      "epoch:7 step:7253 [D loss: 0.528645, acc.: 70.31%] [G loss: 0.566826]\n",
      "epoch:7 step:7254 [D loss: 0.608367, acc.: 63.28%] [G loss: 0.630041]\n",
      "epoch:7 step:7255 [D loss: 0.627380, acc.: 60.94%] [G loss: 0.523578]\n",
      "epoch:7 step:7256 [D loss: 0.545895, acc.: 67.97%] [G loss: 0.585380]\n",
      "epoch:7 step:7257 [D loss: 0.540922, acc.: 67.97%] [G loss: 0.563941]\n",
      "epoch:7 step:7258 [D loss: 0.516212, acc.: 74.22%] [G loss: 0.599230]\n",
      "epoch:7 step:7259 [D loss: 0.519856, acc.: 71.09%] [G loss: 0.595827]\n",
      "epoch:7 step:7260 [D loss: 0.566300, acc.: 66.41%] [G loss: 0.507409]\n",
      "epoch:7 step:7261 [D loss: 0.591918, acc.: 67.19%] [G loss: 0.542574]\n",
      "epoch:7 step:7262 [D loss: 0.551776, acc.: 70.31%] [G loss: 0.541349]\n",
      "epoch:7 step:7263 [D loss: 0.613337, acc.: 60.16%] [G loss: 0.501192]\n",
      "epoch:7 step:7264 [D loss: 0.564704, acc.: 70.31%] [G loss: 0.629705]\n",
      "epoch:7 step:7265 [D loss: 0.583883, acc.: 64.84%] [G loss: 0.464596]\n",
      "epoch:7 step:7266 [D loss: 0.492814, acc.: 73.44%] [G loss: 0.615267]\n",
      "epoch:7 step:7267 [D loss: 0.547679, acc.: 71.88%] [G loss: 0.548497]\n",
      "epoch:7 step:7268 [D loss: 0.545413, acc.: 72.66%] [G loss: 0.629016]\n",
      "epoch:7 step:7269 [D loss: 0.579321, acc.: 68.75%] [G loss: 0.537121]\n",
      "epoch:7 step:7270 [D loss: 0.583099, acc.: 66.41%] [G loss: 0.503649]\n",
      "epoch:7 step:7271 [D loss: 0.558527, acc.: 70.31%] [G loss: 0.534863]\n",
      "epoch:7 step:7272 [D loss: 0.618691, acc.: 63.28%] [G loss: 0.533000]\n",
      "epoch:7 step:7273 [D loss: 0.528428, acc.: 71.09%] [G loss: 0.420262]\n",
      "epoch:7 step:7274 [D loss: 0.608562, acc.: 65.62%] [G loss: 0.489178]\n",
      "epoch:7 step:7275 [D loss: 0.589469, acc.: 67.19%] [G loss: 0.464063]\n",
      "epoch:7 step:7276 [D loss: 0.590011, acc.: 65.62%] [G loss: 0.361890]\n",
      "epoch:7 step:7277 [D loss: 0.581574, acc.: 64.06%] [G loss: 0.405978]\n",
      "epoch:7 step:7278 [D loss: 0.536446, acc.: 70.31%] [G loss: 0.562474]\n",
      "epoch:7 step:7279 [D loss: 0.610831, acc.: 68.75%] [G loss: 0.470206]\n",
      "epoch:7 step:7280 [D loss: 0.581300, acc.: 68.75%] [G loss: 0.430902]\n",
      "epoch:7 step:7281 [D loss: 0.588901, acc.: 70.31%] [G loss: 0.537431]\n",
      "epoch:7 step:7282 [D loss: 0.566798, acc.: 66.41%] [G loss: 0.486363]\n",
      "epoch:7 step:7283 [D loss: 0.532839, acc.: 70.31%] [G loss: 0.691615]\n",
      "epoch:7 step:7284 [D loss: 0.566675, acc.: 72.66%] [G loss: 0.465943]\n",
      "epoch:7 step:7285 [D loss: 0.519657, acc.: 77.34%] [G loss: 0.500580]\n",
      "epoch:7 step:7286 [D loss: 0.571592, acc.: 66.41%] [G loss: 0.479263]\n",
      "epoch:7 step:7287 [D loss: 0.546038, acc.: 68.75%] [G loss: 0.447028]\n",
      "epoch:7 step:7288 [D loss: 0.620019, acc.: 59.38%] [G loss: 0.451744]\n",
      "epoch:7 step:7289 [D loss: 0.542464, acc.: 74.22%] [G loss: 0.450412]\n",
      "epoch:7 step:7290 [D loss: 0.539410, acc.: 72.66%] [G loss: 0.551991]\n",
      "epoch:7 step:7291 [D loss: 0.520525, acc.: 75.00%] [G loss: 0.567261]\n",
      "epoch:7 step:7292 [D loss: 0.539049, acc.: 76.56%] [G loss: 0.526434]\n",
      "epoch:7 step:7293 [D loss: 0.533639, acc.: 71.09%] [G loss: 0.466559]\n",
      "epoch:7 step:7294 [D loss: 0.620080, acc.: 64.06%] [G loss: 0.428342]\n",
      "epoch:7 step:7295 [D loss: 0.483564, acc.: 73.44%] [G loss: 0.669866]\n",
      "epoch:7 step:7296 [D loss: 0.548548, acc.: 71.09%] [G loss: 0.601687]\n",
      "epoch:7 step:7297 [D loss: 0.593963, acc.: 67.97%] [G loss: 0.489165]\n",
      "epoch:7 step:7298 [D loss: 0.624784, acc.: 64.84%] [G loss: 0.376716]\n",
      "epoch:7 step:7299 [D loss: 0.628061, acc.: 64.06%] [G loss: 0.401161]\n",
      "epoch:7 step:7300 [D loss: 0.587032, acc.: 67.19%] [G loss: 0.319787]\n",
      "epoch:7 step:7301 [D loss: 0.564142, acc.: 68.75%] [G loss: 0.480078]\n",
      "epoch:7 step:7302 [D loss: 0.514210, acc.: 78.12%] [G loss: 0.628049]\n",
      "epoch:7 step:7303 [D loss: 0.551356, acc.: 71.88%] [G loss: 0.561105]\n",
      "epoch:7 step:7304 [D loss: 0.642244, acc.: 64.06%] [G loss: 0.484724]\n",
      "epoch:7 step:7305 [D loss: 0.487122, acc.: 75.78%] [G loss: 0.506293]\n",
      "epoch:7 step:7306 [D loss: 0.459032, acc.: 78.91%] [G loss: 0.656446]\n",
      "epoch:7 step:7307 [D loss: 0.582428, acc.: 65.62%] [G loss: 0.566786]\n",
      "epoch:7 step:7308 [D loss: 0.555806, acc.: 68.75%] [G loss: 0.602107]\n",
      "epoch:7 step:7309 [D loss: 0.505367, acc.: 75.00%] [G loss: 0.491279]\n",
      "epoch:7 step:7310 [D loss: 0.548905, acc.: 67.97%] [G loss: 0.507706]\n",
      "epoch:7 step:7311 [D loss: 0.549060, acc.: 67.97%] [G loss: 0.502279]\n",
      "epoch:7 step:7312 [D loss: 0.550260, acc.: 69.53%] [G loss: 0.578196]\n",
      "epoch:7 step:7313 [D loss: 0.524310, acc.: 75.78%] [G loss: 0.661750]\n",
      "epoch:7 step:7314 [D loss: 0.542626, acc.: 72.66%] [G loss: 0.524554]\n",
      "epoch:7 step:7315 [D loss: 0.557324, acc.: 68.75%] [G loss: 0.487575]\n",
      "epoch:7 step:7316 [D loss: 0.562777, acc.: 64.06%] [G loss: 0.567030]\n",
      "epoch:7 step:7317 [D loss: 0.598604, acc.: 69.53%] [G loss: 0.332590]\n",
      "epoch:7 step:7318 [D loss: 0.577026, acc.: 69.53%] [G loss: 0.491062]\n",
      "epoch:7 step:7319 [D loss: 0.526892, acc.: 70.31%] [G loss: 0.490526]\n",
      "epoch:7 step:7320 [D loss: 0.563742, acc.: 69.53%] [G loss: 0.474638]\n",
      "epoch:7 step:7321 [D loss: 0.559188, acc.: 71.09%] [G loss: 0.524156]\n",
      "epoch:7 step:7322 [D loss: 0.588569, acc.: 67.97%] [G loss: 0.401478]\n",
      "epoch:7 step:7323 [D loss: 0.579708, acc.: 68.75%] [G loss: 0.441609]\n",
      "epoch:7 step:7324 [D loss: 0.592022, acc.: 68.75%] [G loss: 0.376001]\n",
      "epoch:7 step:7325 [D loss: 0.717407, acc.: 56.25%] [G loss: 0.354533]\n",
      "epoch:7 step:7326 [D loss: 0.557781, acc.: 74.22%] [G loss: 0.346270]\n",
      "epoch:7 step:7327 [D loss: 0.513197, acc.: 77.34%] [G loss: 0.517172]\n",
      "epoch:7 step:7328 [D loss: 0.516290, acc.: 75.78%] [G loss: 0.600351]\n",
      "epoch:7 step:7329 [D loss: 0.561648, acc.: 65.62%] [G loss: 0.621072]\n",
      "epoch:7 step:7330 [D loss: 0.491659, acc.: 73.44%] [G loss: 0.625417]\n",
      "epoch:7 step:7331 [D loss: 0.567440, acc.: 65.62%] [G loss: 0.453873]\n",
      "epoch:7 step:7332 [D loss: 0.522700, acc.: 73.44%] [G loss: 0.602616]\n",
      "epoch:7 step:7333 [D loss: 0.572555, acc.: 69.53%] [G loss: 0.584527]\n",
      "epoch:7 step:7334 [D loss: 0.508282, acc.: 77.34%] [G loss: 0.653601]\n",
      "epoch:7 step:7335 [D loss: 0.626276, acc.: 62.50%] [G loss: 0.525227]\n",
      "epoch:7 step:7336 [D loss: 0.537959, acc.: 75.00%] [G loss: 0.477701]\n",
      "epoch:7 step:7337 [D loss: 0.562840, acc.: 72.66%] [G loss: 0.526025]\n",
      "epoch:7 step:7338 [D loss: 0.604886, acc.: 64.06%] [G loss: 0.524960]\n",
      "epoch:7 step:7339 [D loss: 0.564525, acc.: 72.66%] [G loss: 0.570378]\n",
      "epoch:7 step:7340 [D loss: 0.521226, acc.: 74.22%] [G loss: 0.795907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7341 [D loss: 0.471176, acc.: 75.78%] [G loss: 0.845203]\n",
      "epoch:7 step:7342 [D loss: 0.607193, acc.: 67.19%] [G loss: 0.509598]\n",
      "epoch:7 step:7343 [D loss: 0.641244, acc.: 57.81%] [G loss: 0.464401]\n",
      "epoch:7 step:7344 [D loss: 0.566483, acc.: 64.84%] [G loss: 0.545842]\n",
      "epoch:7 step:7345 [D loss: 0.576155, acc.: 70.31%] [G loss: 0.470860]\n",
      "epoch:7 step:7346 [D loss: 0.600740, acc.: 66.41%] [G loss: 0.553178]\n",
      "epoch:7 step:7347 [D loss: 0.632219, acc.: 61.72%] [G loss: 0.494904]\n",
      "epoch:7 step:7348 [D loss: 0.548465, acc.: 71.09%] [G loss: 0.505899]\n",
      "epoch:7 step:7349 [D loss: 0.552805, acc.: 73.44%] [G loss: 0.572808]\n",
      "epoch:7 step:7350 [D loss: 0.569986, acc.: 69.53%] [G loss: 0.548190]\n",
      "epoch:7 step:7351 [D loss: 0.473027, acc.: 74.22%] [G loss: 0.563381]\n",
      "epoch:7 step:7352 [D loss: 0.586090, acc.: 66.41%] [G loss: 0.583354]\n",
      "epoch:7 step:7353 [D loss: 0.579372, acc.: 73.44%] [G loss: 0.509966]\n",
      "epoch:7 step:7354 [D loss: 0.563036, acc.: 67.97%] [G loss: 0.427523]\n",
      "epoch:7 step:7355 [D loss: 0.528477, acc.: 72.66%] [G loss: 0.537071]\n",
      "epoch:7 step:7356 [D loss: 0.557174, acc.: 71.09%] [G loss: 0.562529]\n",
      "epoch:7 step:7357 [D loss: 0.603271, acc.: 66.41%] [G loss: 0.463371]\n",
      "epoch:7 step:7358 [D loss: 0.560744, acc.: 72.66%] [G loss: 0.496008]\n",
      "epoch:7 step:7359 [D loss: 0.534309, acc.: 74.22%] [G loss: 0.514307]\n",
      "epoch:7 step:7360 [D loss: 0.539288, acc.: 74.22%] [G loss: 0.561343]\n",
      "epoch:7 step:7361 [D loss: 0.515435, acc.: 76.56%] [G loss: 0.557839]\n",
      "epoch:7 step:7362 [D loss: 0.499590, acc.: 79.69%] [G loss: 0.582001]\n",
      "epoch:7 step:7363 [D loss: 0.547176, acc.: 70.31%] [G loss: 0.591231]\n",
      "epoch:7 step:7364 [D loss: 0.526127, acc.: 71.88%] [G loss: 0.653759]\n",
      "epoch:7 step:7365 [D loss: 0.535223, acc.: 72.66%] [G loss: 0.588680]\n",
      "epoch:7 step:7366 [D loss: 0.520803, acc.: 76.56%] [G loss: 0.596011]\n",
      "epoch:7 step:7367 [D loss: 0.576851, acc.: 71.09%] [G loss: 0.386687]\n",
      "epoch:7 step:7368 [D loss: 0.562494, acc.: 71.88%] [G loss: 0.469294]\n",
      "epoch:7 step:7369 [D loss: 0.527189, acc.: 74.22%] [G loss: 0.636846]\n",
      "epoch:7 step:7370 [D loss: 0.550890, acc.: 72.66%] [G loss: 0.603617]\n",
      "epoch:7 step:7371 [D loss: 0.684991, acc.: 59.38%] [G loss: 0.470092]\n",
      "epoch:7 step:7372 [D loss: 0.575203, acc.: 67.19%] [G loss: 0.462841]\n",
      "epoch:7 step:7373 [D loss: 0.571663, acc.: 68.75%] [G loss: 0.511270]\n",
      "epoch:7 step:7374 [D loss: 0.564095, acc.: 71.09%] [G loss: 0.692182]\n",
      "epoch:7 step:7375 [D loss: 0.547838, acc.: 67.19%] [G loss: 0.609780]\n",
      "epoch:7 step:7376 [D loss: 0.653357, acc.: 60.94%] [G loss: 0.572146]\n",
      "epoch:7 step:7377 [D loss: 0.566661, acc.: 67.19%] [G loss: 0.489182]\n",
      "epoch:7 step:7378 [D loss: 0.524294, acc.: 71.88%] [G loss: 0.545614]\n",
      "epoch:7 step:7379 [D loss: 0.624145, acc.: 61.72%] [G loss: 0.642690]\n",
      "epoch:7 step:7380 [D loss: 0.595159, acc.: 64.84%] [G loss: 0.558533]\n",
      "epoch:7 step:7381 [D loss: 0.535255, acc.: 69.53%] [G loss: 0.593106]\n",
      "epoch:7 step:7382 [D loss: 0.502837, acc.: 73.44%] [G loss: 0.627720]\n",
      "epoch:7 step:7383 [D loss: 0.625989, acc.: 61.72%] [G loss: 0.546536]\n",
      "epoch:7 step:7384 [D loss: 0.568177, acc.: 68.75%] [G loss: 0.527572]\n",
      "epoch:7 step:7385 [D loss: 0.561971, acc.: 70.31%] [G loss: 0.463729]\n",
      "epoch:7 step:7386 [D loss: 0.586708, acc.: 66.41%] [G loss: 0.394045]\n",
      "epoch:7 step:7387 [D loss: 0.637724, acc.: 60.16%] [G loss: 0.451768]\n",
      "epoch:7 step:7388 [D loss: 0.560839, acc.: 69.53%] [G loss: 0.451141]\n",
      "epoch:7 step:7389 [D loss: 0.542960, acc.: 76.56%] [G loss: 0.525753]\n",
      "epoch:7 step:7390 [D loss: 0.513379, acc.: 72.66%] [G loss: 0.612248]\n",
      "epoch:7 step:7391 [D loss: 0.565228, acc.: 67.97%] [G loss: 0.522531]\n",
      "epoch:7 step:7392 [D loss: 0.510890, acc.: 78.12%] [G loss: 0.435075]\n",
      "epoch:7 step:7393 [D loss: 0.565250, acc.: 65.62%] [G loss: 0.383505]\n",
      "epoch:7 step:7394 [D loss: 0.493704, acc.: 76.56%] [G loss: 0.508926]\n",
      "epoch:7 step:7395 [D loss: 0.553598, acc.: 70.31%] [G loss: 0.432887]\n",
      "epoch:7 step:7396 [D loss: 0.518745, acc.: 69.53%] [G loss: 0.535518]\n",
      "epoch:7 step:7397 [D loss: 0.565996, acc.: 71.88%] [G loss: 0.371707]\n",
      "epoch:7 step:7398 [D loss: 0.556334, acc.: 70.31%] [G loss: 0.477694]\n",
      "epoch:7 step:7399 [D loss: 0.593385, acc.: 66.41%] [G loss: 0.432176]\n",
      "epoch:7 step:7400 [D loss: 0.550102, acc.: 70.31%] [G loss: 0.439569]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.110732\n",
      "FID: 47.593506\n",
      "0 = 12.754285609817487\n",
      "1 = 0.08622969457466743\n",
      "2 = 0.9200999736785889\n",
      "3 = 0.8551999926567078\n",
      "4 = 0.9850000143051147\n",
      "5 = 0.9827625751495361\n",
      "6 = 0.8551999926567078\n",
      "7 = 8.333372318899633\n",
      "8 = 0.14274274128164496\n",
      "9 = 0.7803000211715698\n",
      "10 = 0.7581999897956848\n",
      "11 = 0.8023999929428101\n",
      "12 = 0.7932621836662292\n",
      "13 = 0.7581999897956848\n",
      "14 = 6.1107587814331055\n",
      "15 = 7.518272399902344\n",
      "16 = 0.35160818696022034\n",
      "17 = 6.110732078552246\n",
      "18 = 47.593505859375\n",
      "epoch:7 step:7401 [D loss: 0.527557, acc.: 71.09%] [G loss: 0.494913]\n",
      "epoch:7 step:7402 [D loss: 0.533655, acc.: 71.88%] [G loss: 0.545712]\n",
      "epoch:7 step:7403 [D loss: 0.579485, acc.: 73.44%] [G loss: 0.532483]\n",
      "epoch:7 step:7404 [D loss: 0.568467, acc.: 69.53%] [G loss: 0.470559]\n",
      "epoch:7 step:7405 [D loss: 0.562062, acc.: 67.97%] [G loss: 0.435427]\n",
      "epoch:7 step:7406 [D loss: 0.592575, acc.: 65.62%] [G loss: 0.420469]\n",
      "epoch:7 step:7407 [D loss: 0.550992, acc.: 67.97%] [G loss: 0.386249]\n",
      "epoch:7 step:7408 [D loss: 0.603035, acc.: 64.06%] [G loss: 0.349976]\n",
      "epoch:7 step:7409 [D loss: 0.542141, acc.: 67.97%] [G loss: 0.526348]\n",
      "epoch:7 step:7410 [D loss: 0.561073, acc.: 67.19%] [G loss: 0.510149]\n",
      "epoch:7 step:7411 [D loss: 0.525377, acc.: 71.88%] [G loss: 0.538045]\n",
      "epoch:7 step:7412 [D loss: 0.525293, acc.: 75.78%] [G loss: 0.481812]\n",
      "epoch:7 step:7413 [D loss: 0.489256, acc.: 77.34%] [G loss: 0.571843]\n",
      "epoch:7 step:7414 [D loss: 0.595638, acc.: 67.97%] [G loss: 0.516043]\n",
      "epoch:7 step:7415 [D loss: 0.633015, acc.: 64.84%] [G loss: 0.450270]\n",
      "epoch:7 step:7416 [D loss: 0.495806, acc.: 75.78%] [G loss: 0.634790]\n",
      "epoch:7 step:7417 [D loss: 0.672504, acc.: 65.62%] [G loss: 0.610211]\n",
      "epoch:7 step:7418 [D loss: 0.544720, acc.: 74.22%] [G loss: 0.696186]\n",
      "epoch:7 step:7419 [D loss: 0.503680, acc.: 75.78%] [G loss: 0.723742]\n",
      "epoch:7 step:7420 [D loss: 0.617082, acc.: 67.19%] [G loss: 0.509077]\n",
      "epoch:7 step:7421 [D loss: 0.602995, acc.: 62.50%] [G loss: 0.444919]\n",
      "epoch:7 step:7422 [D loss: 0.544906, acc.: 71.88%] [G loss: 0.538730]\n",
      "epoch:7 step:7423 [D loss: 0.574767, acc.: 67.97%] [G loss: 0.454495]\n",
      "epoch:7 step:7424 [D loss: 0.600702, acc.: 63.28%] [G loss: 0.443929]\n",
      "epoch:7 step:7425 [D loss: 0.558731, acc.: 67.97%] [G loss: 0.498652]\n",
      "epoch:7 step:7426 [D loss: 0.687614, acc.: 54.69%] [G loss: 0.437096]\n",
      "epoch:7 step:7427 [D loss: 0.500087, acc.: 76.56%] [G loss: 0.493013]\n",
      "epoch:7 step:7428 [D loss: 0.568728, acc.: 66.41%] [G loss: 0.419343]\n",
      "epoch:7 step:7429 [D loss: 0.465766, acc.: 78.91%] [G loss: 0.535987]\n",
      "epoch:7 step:7430 [D loss: 0.494713, acc.: 75.00%] [G loss: 0.546675]\n",
      "epoch:7 step:7431 [D loss: 0.524322, acc.: 75.78%] [G loss: 0.688757]\n",
      "epoch:7 step:7432 [D loss: 0.650966, acc.: 66.41%] [G loss: 0.419631]\n",
      "epoch:7 step:7433 [D loss: 0.573000, acc.: 71.09%] [G loss: 0.521055]\n",
      "epoch:7 step:7434 [D loss: 0.525310, acc.: 71.88%] [G loss: 0.561587]\n",
      "epoch:7 step:7435 [D loss: 0.563191, acc.: 73.44%] [G loss: 0.524498]\n",
      "epoch:7 step:7436 [D loss: 0.550044, acc.: 64.84%] [G loss: 0.427450]\n",
      "epoch:7 step:7437 [D loss: 0.555396, acc.: 67.97%] [G loss: 0.477853]\n",
      "epoch:7 step:7438 [D loss: 0.618424, acc.: 64.06%] [G loss: 0.492938]\n",
      "epoch:7 step:7439 [D loss: 0.633600, acc.: 57.81%] [G loss: 0.464350]\n",
      "epoch:7 step:7440 [D loss: 0.567520, acc.: 67.19%] [G loss: 0.481677]\n",
      "epoch:7 step:7441 [D loss: 0.611158, acc.: 64.06%] [G loss: 0.377508]\n",
      "epoch:7 step:7442 [D loss: 0.598051, acc.: 65.62%] [G loss: 0.435680]\n",
      "epoch:7 step:7443 [D loss: 0.525059, acc.: 74.22%] [G loss: 0.613508]\n",
      "epoch:7 step:7444 [D loss: 0.543967, acc.: 67.97%] [G loss: 0.684348]\n",
      "epoch:7 step:7445 [D loss: 0.544983, acc.: 71.88%] [G loss: 0.592118]\n",
      "epoch:7 step:7446 [D loss: 0.562726, acc.: 69.53%] [G loss: 0.623386]\n",
      "epoch:7 step:7447 [D loss: 0.647094, acc.: 60.16%] [G loss: 0.619962]\n",
      "epoch:7 step:7448 [D loss: 0.558860, acc.: 70.31%] [G loss: 0.618241]\n",
      "epoch:7 step:7449 [D loss: 0.494596, acc.: 76.56%] [G loss: 0.558847]\n",
      "epoch:7 step:7450 [D loss: 0.666653, acc.: 58.59%] [G loss: 0.575506]\n",
      "epoch:7 step:7451 [D loss: 0.628741, acc.: 63.28%] [G loss: 0.440421]\n",
      "epoch:7 step:7452 [D loss: 0.574892, acc.: 69.53%] [G loss: 0.413197]\n",
      "epoch:7 step:7453 [D loss: 0.506968, acc.: 75.78%] [G loss: 0.515881]\n",
      "epoch:7 step:7454 [D loss: 0.535273, acc.: 70.31%] [G loss: 0.826613]\n",
      "epoch:7 step:7455 [D loss: 0.522311, acc.: 76.56%] [G loss: 0.614416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7456 [D loss: 0.499616, acc.: 73.44%] [G loss: 0.590651]\n",
      "epoch:7 step:7457 [D loss: 0.528810, acc.: 71.88%] [G loss: 0.590267]\n",
      "epoch:7 step:7458 [D loss: 0.483560, acc.: 76.56%] [G loss: 0.581685]\n",
      "epoch:7 step:7459 [D loss: 0.523499, acc.: 75.00%] [G loss: 0.684596]\n",
      "epoch:7 step:7460 [D loss: 0.542864, acc.: 67.19%] [G loss: 0.611242]\n",
      "epoch:7 step:7461 [D loss: 0.596176, acc.: 67.19%] [G loss: 0.569368]\n",
      "epoch:7 step:7462 [D loss: 0.553093, acc.: 73.44%] [G loss: 0.557016]\n",
      "epoch:7 step:7463 [D loss: 0.602420, acc.: 64.84%] [G loss: 0.403981]\n",
      "epoch:7 step:7464 [D loss: 0.571103, acc.: 67.97%] [G loss: 0.531457]\n",
      "epoch:7 step:7465 [D loss: 0.558921, acc.: 66.41%] [G loss: 0.556414]\n",
      "epoch:7 step:7466 [D loss: 0.572473, acc.: 72.66%] [G loss: 0.599326]\n",
      "epoch:7 step:7467 [D loss: 0.531580, acc.: 73.44%] [G loss: 0.589885]\n",
      "epoch:7 step:7468 [D loss: 0.500206, acc.: 76.56%] [G loss: 0.526748]\n",
      "epoch:7 step:7469 [D loss: 0.502792, acc.: 72.66%] [G loss: 0.597155]\n",
      "epoch:7 step:7470 [D loss: 0.465095, acc.: 78.12%] [G loss: 0.796836]\n",
      "epoch:7 step:7471 [D loss: 0.505454, acc.: 76.56%] [G loss: 0.570065]\n",
      "epoch:7 step:7472 [D loss: 0.547505, acc.: 70.31%] [G loss: 0.814959]\n",
      "epoch:7 step:7473 [D loss: 0.487076, acc.: 74.22%] [G loss: 0.879942]\n",
      "epoch:7 step:7474 [D loss: 0.627997, acc.: 63.28%] [G loss: 0.675596]\n",
      "epoch:7 step:7475 [D loss: 0.543196, acc.: 71.09%] [G loss: 0.519609]\n",
      "epoch:7 step:7476 [D loss: 0.623221, acc.: 64.06%] [G loss: 0.489428]\n",
      "epoch:7 step:7477 [D loss: 0.506899, acc.: 74.22%] [G loss: 0.552053]\n",
      "epoch:7 step:7478 [D loss: 0.525347, acc.: 74.22%] [G loss: 0.583631]\n",
      "epoch:7 step:7479 [D loss: 0.750941, acc.: 57.03%] [G loss: 0.578074]\n",
      "epoch:7 step:7480 [D loss: 0.429983, acc.: 82.03%] [G loss: 0.775506]\n",
      "epoch:7 step:7481 [D loss: 0.540562, acc.: 76.56%] [G loss: 0.564307]\n",
      "epoch:7 step:7482 [D loss: 0.469550, acc.: 79.69%] [G loss: 0.622093]\n",
      "epoch:7 step:7483 [D loss: 0.482759, acc.: 78.91%] [G loss: 0.672403]\n",
      "epoch:7 step:7484 [D loss: 0.435178, acc.: 79.69%] [G loss: 0.672493]\n",
      "epoch:7 step:7485 [D loss: 0.418430, acc.: 82.81%] [G loss: 0.835660]\n",
      "epoch:7 step:7486 [D loss: 0.500897, acc.: 75.00%] [G loss: 0.868651]\n",
      "epoch:7 step:7487 [D loss: 0.743466, acc.: 61.72%] [G loss: 0.921099]\n",
      "epoch:7 step:7488 [D loss: 0.525835, acc.: 75.00%] [G loss: 1.085845]\n",
      "epoch:7 step:7489 [D loss: 0.451897, acc.: 78.12%] [G loss: 0.993071]\n",
      "epoch:7 step:7490 [D loss: 0.607614, acc.: 64.84%] [G loss: 0.654013]\n",
      "epoch:7 step:7491 [D loss: 0.690639, acc.: 63.28%] [G loss: 0.553944]\n",
      "epoch:7 step:7492 [D loss: 0.624302, acc.: 62.50%] [G loss: 0.670032]\n",
      "epoch:7 step:7493 [D loss: 0.566080, acc.: 71.09%] [G loss: 0.644979]\n",
      "epoch:7 step:7494 [D loss: 0.483204, acc.: 73.44%] [G loss: 0.821953]\n",
      "epoch:7 step:7495 [D loss: 0.386456, acc.: 82.03%] [G loss: 0.928722]\n",
      "epoch:7 step:7496 [D loss: 0.440620, acc.: 81.25%] [G loss: 1.138838]\n",
      "epoch:8 step:7497 [D loss: 0.607137, acc.: 67.97%] [G loss: 0.770382]\n",
      "epoch:8 step:7498 [D loss: 0.548731, acc.: 67.97%] [G loss: 0.767550]\n",
      "epoch:8 step:7499 [D loss: 0.585090, acc.: 69.53%] [G loss: 0.747280]\n",
      "epoch:8 step:7500 [D loss: 0.511974, acc.: 74.22%] [G loss: 0.576544]\n",
      "epoch:8 step:7501 [D loss: 0.514514, acc.: 76.56%] [G loss: 0.568509]\n",
      "epoch:8 step:7502 [D loss: 0.566432, acc.: 67.19%] [G loss: 0.503116]\n",
      "epoch:8 step:7503 [D loss: 0.465185, acc.: 81.25%] [G loss: 0.743529]\n",
      "epoch:8 step:7504 [D loss: 0.510148, acc.: 75.78%] [G loss: 0.752272]\n",
      "epoch:8 step:7505 [D loss: 0.538787, acc.: 71.09%] [G loss: 0.611648]\n",
      "epoch:8 step:7506 [D loss: 0.561555, acc.: 69.53%] [G loss: 0.679650]\n",
      "epoch:8 step:7507 [D loss: 0.456072, acc.: 79.69%] [G loss: 0.728830]\n",
      "epoch:8 step:7508 [D loss: 0.590469, acc.: 66.41%] [G loss: 0.546584]\n",
      "epoch:8 step:7509 [D loss: 0.546988, acc.: 70.31%] [G loss: 0.524387]\n",
      "epoch:8 step:7510 [D loss: 0.601255, acc.: 64.06%] [G loss: 0.522635]\n",
      "epoch:8 step:7511 [D loss: 0.448181, acc.: 79.69%] [G loss: 0.639511]\n",
      "epoch:8 step:7512 [D loss: 0.503773, acc.: 72.66%] [G loss: 0.656316]\n",
      "epoch:8 step:7513 [D loss: 0.550612, acc.: 68.75%] [G loss: 0.514404]\n",
      "epoch:8 step:7514 [D loss: 0.592065, acc.: 67.19%] [G loss: 0.495520]\n",
      "epoch:8 step:7515 [D loss: 0.600079, acc.: 68.75%] [G loss: 0.509617]\n",
      "epoch:8 step:7516 [D loss: 0.553253, acc.: 74.22%] [G loss: 0.576568]\n",
      "epoch:8 step:7517 [D loss: 0.632515, acc.: 61.72%] [G loss: 0.610829]\n",
      "epoch:8 step:7518 [D loss: 0.502468, acc.: 77.34%] [G loss: 0.738471]\n",
      "epoch:8 step:7519 [D loss: 0.565430, acc.: 71.88%] [G loss: 0.572307]\n",
      "epoch:8 step:7520 [D loss: 0.511466, acc.: 73.44%] [G loss: 0.506470]\n",
      "epoch:8 step:7521 [D loss: 0.501493, acc.: 78.12%] [G loss: 0.517109]\n",
      "epoch:8 step:7522 [D loss: 0.590467, acc.: 68.75%] [G loss: 0.527161]\n",
      "epoch:8 step:7523 [D loss: 0.541983, acc.: 64.06%] [G loss: 0.543765]\n",
      "epoch:8 step:7524 [D loss: 0.565467, acc.: 69.53%] [G loss: 0.546999]\n",
      "epoch:8 step:7525 [D loss: 0.522737, acc.: 74.22%] [G loss: 0.540485]\n",
      "epoch:8 step:7526 [D loss: 0.556484, acc.: 73.44%] [G loss: 0.544548]\n",
      "epoch:8 step:7527 [D loss: 0.618264, acc.: 64.06%] [G loss: 0.389656]\n",
      "epoch:8 step:7528 [D loss: 0.584774, acc.: 70.31%] [G loss: 0.566546]\n",
      "epoch:8 step:7529 [D loss: 0.512819, acc.: 77.34%] [G loss: 0.498913]\n",
      "epoch:8 step:7530 [D loss: 0.534787, acc.: 72.66%] [G loss: 0.607685]\n",
      "epoch:8 step:7531 [D loss: 0.517949, acc.: 75.78%] [G loss: 0.544517]\n",
      "epoch:8 step:7532 [D loss: 0.513214, acc.: 75.78%] [G loss: 0.618332]\n",
      "epoch:8 step:7533 [D loss: 0.496230, acc.: 76.56%] [G loss: 0.684021]\n",
      "epoch:8 step:7534 [D loss: 0.671288, acc.: 64.84%] [G loss: 0.421590]\n",
      "epoch:8 step:7535 [D loss: 0.507788, acc.: 73.44%] [G loss: 0.468295]\n",
      "epoch:8 step:7536 [D loss: 0.409158, acc.: 83.59%] [G loss: 0.629460]\n",
      "epoch:8 step:7537 [D loss: 0.532510, acc.: 72.66%] [G loss: 0.633306]\n",
      "epoch:8 step:7538 [D loss: 0.526816, acc.: 73.44%] [G loss: 0.514317]\n",
      "epoch:8 step:7539 [D loss: 0.497656, acc.: 77.34%] [G loss: 0.577985]\n",
      "epoch:8 step:7540 [D loss: 0.626158, acc.: 58.59%] [G loss: 0.497253]\n",
      "epoch:8 step:7541 [D loss: 0.505476, acc.: 78.12%] [G loss: 0.673465]\n",
      "epoch:8 step:7542 [D loss: 0.535639, acc.: 71.88%] [G loss: 0.666639]\n",
      "epoch:8 step:7543 [D loss: 0.529610, acc.: 73.44%] [G loss: 0.515502]\n",
      "epoch:8 step:7544 [D loss: 0.556425, acc.: 69.53%] [G loss: 0.611441]\n",
      "epoch:8 step:7545 [D loss: 0.494349, acc.: 77.34%] [G loss: 0.516769]\n",
      "epoch:8 step:7546 [D loss: 0.585595, acc.: 67.19%] [G loss: 0.535970]\n",
      "epoch:8 step:7547 [D loss: 0.588412, acc.: 72.66%] [G loss: 0.488093]\n",
      "epoch:8 step:7548 [D loss: 0.565907, acc.: 68.75%] [G loss: 0.502502]\n",
      "epoch:8 step:7549 [D loss: 0.524822, acc.: 71.09%] [G loss: 0.537401]\n",
      "epoch:8 step:7550 [D loss: 0.457382, acc.: 78.91%] [G loss: 0.570496]\n",
      "epoch:8 step:7551 [D loss: 0.538536, acc.: 72.66%] [G loss: 0.572230]\n",
      "epoch:8 step:7552 [D loss: 0.522451, acc.: 73.44%] [G loss: 0.565449]\n",
      "epoch:8 step:7553 [D loss: 0.523203, acc.: 73.44%] [G loss: 0.686922]\n",
      "epoch:8 step:7554 [D loss: 0.600282, acc.: 64.06%] [G loss: 0.611609]\n",
      "epoch:8 step:7555 [D loss: 0.497956, acc.: 76.56%] [G loss: 0.550045]\n",
      "epoch:8 step:7556 [D loss: 0.533907, acc.: 68.75%] [G loss: 0.776091]\n",
      "epoch:8 step:7557 [D loss: 0.550294, acc.: 67.97%] [G loss: 0.587377]\n",
      "epoch:8 step:7558 [D loss: 0.587057, acc.: 67.19%] [G loss: 0.544752]\n",
      "epoch:8 step:7559 [D loss: 0.586275, acc.: 67.97%] [G loss: 0.440848]\n",
      "epoch:8 step:7560 [D loss: 0.572515, acc.: 71.88%] [G loss: 0.577301]\n",
      "epoch:8 step:7561 [D loss: 0.585893, acc.: 68.75%] [G loss: 0.429065]\n",
      "epoch:8 step:7562 [D loss: 0.561541, acc.: 71.88%] [G loss: 0.485886]\n",
      "epoch:8 step:7563 [D loss: 0.605301, acc.: 64.84%] [G loss: 0.537610]\n",
      "epoch:8 step:7564 [D loss: 0.517929, acc.: 75.00%] [G loss: 0.553951]\n",
      "epoch:8 step:7565 [D loss: 0.529014, acc.: 71.88%] [G loss: 0.603066]\n",
      "epoch:8 step:7566 [D loss: 0.509975, acc.: 72.66%] [G loss: 0.640518]\n",
      "epoch:8 step:7567 [D loss: 0.533308, acc.: 69.53%] [G loss: 0.548445]\n",
      "epoch:8 step:7568 [D loss: 0.472140, acc.: 80.47%] [G loss: 0.513059]\n",
      "epoch:8 step:7569 [D loss: 0.528077, acc.: 74.22%] [G loss: 0.478379]\n",
      "epoch:8 step:7570 [D loss: 0.508227, acc.: 75.00%] [G loss: 0.575347]\n",
      "epoch:8 step:7571 [D loss: 0.573889, acc.: 68.75%] [G loss: 0.777332]\n",
      "epoch:8 step:7572 [D loss: 0.533485, acc.: 72.66%] [G loss: 0.706107]\n",
      "epoch:8 step:7573 [D loss: 0.470864, acc.: 77.34%] [G loss: 0.704342]\n",
      "epoch:8 step:7574 [D loss: 0.718821, acc.: 61.72%] [G loss: 0.446132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7575 [D loss: 0.587652, acc.: 67.19%] [G loss: 0.503192]\n",
      "epoch:8 step:7576 [D loss: 0.516828, acc.: 71.09%] [G loss: 0.679252]\n",
      "epoch:8 step:7577 [D loss: 0.559228, acc.: 67.97%] [G loss: 0.663451]\n",
      "epoch:8 step:7578 [D loss: 0.530886, acc.: 70.31%] [G loss: 0.707150]\n",
      "epoch:8 step:7579 [D loss: 0.536382, acc.: 72.66%] [G loss: 0.666697]\n",
      "epoch:8 step:7580 [D loss: 0.515953, acc.: 74.22%] [G loss: 0.625419]\n",
      "epoch:8 step:7581 [D loss: 0.571908, acc.: 71.88%] [G loss: 0.517861]\n",
      "epoch:8 step:7582 [D loss: 0.564975, acc.: 71.09%] [G loss: 0.494230]\n",
      "epoch:8 step:7583 [D loss: 0.534774, acc.: 72.66%] [G loss: 0.546003]\n",
      "epoch:8 step:7584 [D loss: 0.498214, acc.: 76.56%] [G loss: 0.540268]\n",
      "epoch:8 step:7585 [D loss: 0.511400, acc.: 75.00%] [G loss: 0.474715]\n",
      "epoch:8 step:7586 [D loss: 0.533416, acc.: 71.88%] [G loss: 0.596094]\n",
      "epoch:8 step:7587 [D loss: 0.571037, acc.: 71.09%] [G loss: 0.670435]\n",
      "epoch:8 step:7588 [D loss: 0.485128, acc.: 72.66%] [G loss: 0.549659]\n",
      "epoch:8 step:7589 [D loss: 0.541436, acc.: 75.00%] [G loss: 0.603404]\n",
      "epoch:8 step:7590 [D loss: 0.526689, acc.: 71.88%] [G loss: 0.717826]\n",
      "epoch:8 step:7591 [D loss: 0.523421, acc.: 76.56%] [G loss: 0.542481]\n",
      "epoch:8 step:7592 [D loss: 0.512616, acc.: 73.44%] [G loss: 0.498900]\n",
      "epoch:8 step:7593 [D loss: 0.521053, acc.: 69.53%] [G loss: 0.624702]\n",
      "epoch:8 step:7594 [D loss: 0.570177, acc.: 71.88%] [G loss: 0.497353]\n",
      "epoch:8 step:7595 [D loss: 0.554559, acc.: 70.31%] [G loss: 0.744489]\n",
      "epoch:8 step:7596 [D loss: 0.478005, acc.: 80.47%] [G loss: 0.549219]\n",
      "epoch:8 step:7597 [D loss: 0.518851, acc.: 74.22%] [G loss: 0.607260]\n",
      "epoch:8 step:7598 [D loss: 0.616211, acc.: 62.50%] [G loss: 0.486305]\n",
      "epoch:8 step:7599 [D loss: 0.561380, acc.: 70.31%] [G loss: 0.514651]\n",
      "epoch:8 step:7600 [D loss: 0.546209, acc.: 69.53%] [G loss: 0.433426]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.148164\n",
      "FID: 51.073608\n",
      "0 = 13.081633692073813\n",
      "1 = 0.10890751798266514\n",
      "2 = 0.9207000136375427\n",
      "3 = 0.8592000007629395\n",
      "4 = 0.982200026512146\n",
      "5 = 0.9797035455703735\n",
      "6 = 0.8592000007629395\n",
      "7 = 8.482427368879328\n",
      "8 = 0.15427987693158624\n",
      "9 = 0.7623000144958496\n",
      "10 = 0.7445999979972839\n",
      "11 = 0.7799999713897705\n",
      "12 = 0.7719261646270752\n",
      "13 = 0.7445999979972839\n",
      "14 = 6.148189067840576\n",
      "15 = 7.348232269287109\n",
      "16 = 0.3679824769496918\n",
      "17 = 6.14816427230835\n",
      "18 = 51.0736083984375\n",
      "epoch:8 step:7601 [D loss: 0.598393, acc.: 67.19%] [G loss: 0.441476]\n",
      "epoch:8 step:7602 [D loss: 0.578733, acc.: 72.66%] [G loss: 0.510232]\n",
      "epoch:8 step:7603 [D loss: 0.603620, acc.: 64.84%] [G loss: 0.519653]\n",
      "epoch:8 step:7604 [D loss: 0.608317, acc.: 63.28%] [G loss: 0.515239]\n",
      "epoch:8 step:7605 [D loss: 0.573017, acc.: 71.09%] [G loss: 0.536279]\n",
      "epoch:8 step:7606 [D loss: 0.575168, acc.: 71.09%] [G loss: 0.456329]\n",
      "epoch:8 step:7607 [D loss: 0.519367, acc.: 72.66%] [G loss: 0.487232]\n",
      "epoch:8 step:7608 [D loss: 0.566055, acc.: 71.88%] [G loss: 0.606822]\n",
      "epoch:8 step:7609 [D loss: 0.588203, acc.: 65.62%] [G loss: 0.508350]\n",
      "epoch:8 step:7610 [D loss: 0.544165, acc.: 76.56%] [G loss: 0.529832]\n",
      "epoch:8 step:7611 [D loss: 0.546337, acc.: 70.31%] [G loss: 0.523993]\n",
      "epoch:8 step:7612 [D loss: 0.529313, acc.: 67.97%] [G loss: 0.511879]\n",
      "epoch:8 step:7613 [D loss: 0.571376, acc.: 67.97%] [G loss: 0.570342]\n",
      "epoch:8 step:7614 [D loss: 0.524519, acc.: 78.12%] [G loss: 0.642626]\n",
      "epoch:8 step:7615 [D loss: 0.478077, acc.: 82.03%] [G loss: 0.732142]\n",
      "epoch:8 step:7616 [D loss: 0.590740, acc.: 67.97%] [G loss: 0.547877]\n",
      "epoch:8 step:7617 [D loss: 0.637422, acc.: 64.84%] [G loss: 0.590966]\n",
      "epoch:8 step:7618 [D loss: 0.539174, acc.: 72.66%] [G loss: 0.614672]\n",
      "epoch:8 step:7619 [D loss: 0.522634, acc.: 71.88%] [G loss: 0.606666]\n",
      "epoch:8 step:7620 [D loss: 0.564757, acc.: 69.53%] [G loss: 0.583176]\n",
      "epoch:8 step:7621 [D loss: 0.581476, acc.: 69.53%] [G loss: 0.525201]\n",
      "epoch:8 step:7622 [D loss: 0.551079, acc.: 72.66%] [G loss: 0.492160]\n",
      "epoch:8 step:7623 [D loss: 0.525382, acc.: 73.44%] [G loss: 0.507513]\n",
      "epoch:8 step:7624 [D loss: 0.552180, acc.: 69.53%] [G loss: 0.451893]\n",
      "epoch:8 step:7625 [D loss: 0.591674, acc.: 64.84%] [G loss: 0.465176]\n",
      "epoch:8 step:7626 [D loss: 0.539489, acc.: 71.88%] [G loss: 0.449259]\n",
      "epoch:8 step:7627 [D loss: 0.573102, acc.: 71.88%] [G loss: 0.531994]\n",
      "epoch:8 step:7628 [D loss: 0.562268, acc.: 71.88%] [G loss: 0.484840]\n",
      "epoch:8 step:7629 [D loss: 0.538431, acc.: 71.88%] [G loss: 0.540968]\n",
      "epoch:8 step:7630 [D loss: 0.581167, acc.: 63.28%] [G loss: 0.529030]\n",
      "epoch:8 step:7631 [D loss: 0.558130, acc.: 70.31%] [G loss: 0.510115]\n",
      "epoch:8 step:7632 [D loss: 0.560027, acc.: 64.06%] [G loss: 0.529432]\n",
      "epoch:8 step:7633 [D loss: 0.607764, acc.: 64.06%] [G loss: 0.528270]\n",
      "epoch:8 step:7634 [D loss: 0.596531, acc.: 64.84%] [G loss: 0.375042]\n",
      "epoch:8 step:7635 [D loss: 0.578365, acc.: 68.75%] [G loss: 0.511807]\n",
      "epoch:8 step:7636 [D loss: 0.553967, acc.: 72.66%] [G loss: 0.584052]\n",
      "epoch:8 step:7637 [D loss: 0.517412, acc.: 71.88%] [G loss: 0.475113]\n",
      "epoch:8 step:7638 [D loss: 0.585568, acc.: 65.62%] [G loss: 0.467199]\n",
      "epoch:8 step:7639 [D loss: 0.606302, acc.: 63.28%] [G loss: 0.461354]\n",
      "epoch:8 step:7640 [D loss: 0.476388, acc.: 78.91%] [G loss: 0.625130]\n",
      "epoch:8 step:7641 [D loss: 0.575395, acc.: 63.28%] [G loss: 0.547394]\n",
      "epoch:8 step:7642 [D loss: 0.447517, acc.: 84.38%] [G loss: 0.702164]\n",
      "epoch:8 step:7643 [D loss: 0.657773, acc.: 63.28%] [G loss: 0.467695]\n",
      "epoch:8 step:7644 [D loss: 0.577048, acc.: 67.97%] [G loss: 0.408824]\n",
      "epoch:8 step:7645 [D loss: 0.506931, acc.: 71.88%] [G loss: 0.509481]\n",
      "epoch:8 step:7646 [D loss: 0.662232, acc.: 64.06%] [G loss: 0.505616]\n",
      "epoch:8 step:7647 [D loss: 0.530676, acc.: 77.34%] [G loss: 0.530905]\n",
      "epoch:8 step:7648 [D loss: 0.525418, acc.: 75.00%] [G loss: 0.671247]\n",
      "epoch:8 step:7649 [D loss: 0.582371, acc.: 67.97%] [G loss: 0.626183]\n",
      "epoch:8 step:7650 [D loss: 0.558933, acc.: 70.31%] [G loss: 0.538772]\n",
      "epoch:8 step:7651 [D loss: 0.465428, acc.: 80.47%] [G loss: 0.588166]\n",
      "epoch:8 step:7652 [D loss: 0.524274, acc.: 72.66%] [G loss: 0.621854]\n",
      "epoch:8 step:7653 [D loss: 0.556184, acc.: 70.31%] [G loss: 0.462493]\n",
      "epoch:8 step:7654 [D loss: 0.589935, acc.: 70.31%] [G loss: 0.436394]\n",
      "epoch:8 step:7655 [D loss: 0.549941, acc.: 64.06%] [G loss: 0.372192]\n",
      "epoch:8 step:7656 [D loss: 0.591407, acc.: 65.62%] [G loss: 0.420659]\n",
      "epoch:8 step:7657 [D loss: 0.545343, acc.: 69.53%] [G loss: 0.589704]\n",
      "epoch:8 step:7658 [D loss: 0.475216, acc.: 81.25%] [G loss: 0.681739]\n",
      "epoch:8 step:7659 [D loss: 0.581080, acc.: 66.41%] [G loss: 0.650859]\n",
      "epoch:8 step:7660 [D loss: 0.536680, acc.: 75.78%] [G loss: 0.709285]\n",
      "epoch:8 step:7661 [D loss: 0.487327, acc.: 78.91%] [G loss: 0.660620]\n",
      "epoch:8 step:7662 [D loss: 0.555648, acc.: 68.75%] [G loss: 0.419803]\n",
      "epoch:8 step:7663 [D loss: 0.561910, acc.: 67.97%] [G loss: 0.517076]\n",
      "epoch:8 step:7664 [D loss: 0.580292, acc.: 70.31%] [G loss: 0.561339]\n",
      "epoch:8 step:7665 [D loss: 0.589135, acc.: 62.50%] [G loss: 0.607171]\n",
      "epoch:8 step:7666 [D loss: 0.553724, acc.: 67.19%] [G loss: 0.539024]\n",
      "epoch:8 step:7667 [D loss: 0.516259, acc.: 74.22%] [G loss: 0.536583]\n",
      "epoch:8 step:7668 [D loss: 0.578116, acc.: 67.19%] [G loss: 0.548095]\n",
      "epoch:8 step:7669 [D loss: 0.489940, acc.: 75.00%] [G loss: 0.523231]\n",
      "epoch:8 step:7670 [D loss: 0.625917, acc.: 57.03%] [G loss: 0.451049]\n",
      "epoch:8 step:7671 [D loss: 0.600513, acc.: 66.41%] [G loss: 0.366402]\n",
      "epoch:8 step:7672 [D loss: 0.543644, acc.: 72.66%] [G loss: 0.426821]\n",
      "epoch:8 step:7673 [D loss: 0.525189, acc.: 74.22%] [G loss: 0.492642]\n",
      "epoch:8 step:7674 [D loss: 0.579331, acc.: 68.75%] [G loss: 0.526139]\n",
      "epoch:8 step:7675 [D loss: 0.544357, acc.: 68.75%] [G loss: 0.555399]\n",
      "epoch:8 step:7676 [D loss: 0.575013, acc.: 67.19%] [G loss: 0.613403]\n",
      "epoch:8 step:7677 [D loss: 0.613179, acc.: 64.84%] [G loss: 0.476089]\n",
      "epoch:8 step:7678 [D loss: 0.552508, acc.: 76.56%] [G loss: 0.602350]\n",
      "epoch:8 step:7679 [D loss: 0.545962, acc.: 74.22%] [G loss: 0.635314]\n",
      "epoch:8 step:7680 [D loss: 0.527884, acc.: 74.22%] [G loss: 0.558380]\n",
      "epoch:8 step:7681 [D loss: 0.603885, acc.: 63.28%] [G loss: 0.458180]\n",
      "epoch:8 step:7682 [D loss: 0.573399, acc.: 61.72%] [G loss: 0.538635]\n",
      "epoch:8 step:7683 [D loss: 0.633448, acc.: 63.28%] [G loss: 0.366018]\n",
      "epoch:8 step:7684 [D loss: 0.547753, acc.: 67.97%] [G loss: 0.448472]\n",
      "epoch:8 step:7685 [D loss: 0.565486, acc.: 69.53%] [G loss: 0.504429]\n",
      "epoch:8 step:7686 [D loss: 0.454911, acc.: 80.47%] [G loss: 0.594017]\n",
      "epoch:8 step:7687 [D loss: 0.541740, acc.: 70.31%] [G loss: 0.525132]\n",
      "epoch:8 step:7688 [D loss: 0.494330, acc.: 74.22%] [G loss: 0.539310]\n",
      "epoch:8 step:7689 [D loss: 0.521034, acc.: 74.22%] [G loss: 0.558226]\n",
      "epoch:8 step:7690 [D loss: 0.449115, acc.: 80.47%] [G loss: 0.588135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7691 [D loss: 0.553530, acc.: 71.88%] [G loss: 0.722761]\n",
      "epoch:8 step:7692 [D loss: 0.612805, acc.: 67.19%] [G loss: 0.530599]\n",
      "epoch:8 step:7693 [D loss: 0.519919, acc.: 76.56%] [G loss: 0.601595]\n",
      "epoch:8 step:7694 [D loss: 0.506887, acc.: 74.22%] [G loss: 0.457643]\n",
      "epoch:8 step:7695 [D loss: 0.529705, acc.: 71.88%] [G loss: 0.579801]\n",
      "epoch:8 step:7696 [D loss: 0.640714, acc.: 59.38%] [G loss: 0.477939]\n",
      "epoch:8 step:7697 [D loss: 0.534583, acc.: 75.78%] [G loss: 0.582432]\n",
      "epoch:8 step:7698 [D loss: 0.546036, acc.: 70.31%] [G loss: 0.558678]\n",
      "epoch:8 step:7699 [D loss: 0.608658, acc.: 62.50%] [G loss: 0.459151]\n",
      "epoch:8 step:7700 [D loss: 0.598418, acc.: 64.84%] [G loss: 0.405199]\n",
      "epoch:8 step:7701 [D loss: 0.536996, acc.: 75.78%] [G loss: 0.686756]\n",
      "epoch:8 step:7702 [D loss: 0.535647, acc.: 73.44%] [G loss: 0.711222]\n",
      "epoch:8 step:7703 [D loss: 0.483248, acc.: 79.69%] [G loss: 0.601664]\n",
      "epoch:8 step:7704 [D loss: 0.462445, acc.: 79.69%] [G loss: 0.671715]\n",
      "epoch:8 step:7705 [D loss: 0.536294, acc.: 72.66%] [G loss: 0.633340]\n",
      "epoch:8 step:7706 [D loss: 0.624704, acc.: 64.84%] [G loss: 0.487395]\n",
      "epoch:8 step:7707 [D loss: 0.549028, acc.: 73.44%] [G loss: 0.539539]\n",
      "epoch:8 step:7708 [D loss: 0.604935, acc.: 71.88%] [G loss: 0.396920]\n",
      "epoch:8 step:7709 [D loss: 0.527943, acc.: 69.53%] [G loss: 0.554566]\n",
      "epoch:8 step:7710 [D loss: 0.687200, acc.: 57.81%] [G loss: 0.402934]\n",
      "epoch:8 step:7711 [D loss: 0.605873, acc.: 65.62%] [G loss: 0.444649]\n",
      "epoch:8 step:7712 [D loss: 0.558044, acc.: 64.84%] [G loss: 0.437003]\n",
      "epoch:8 step:7713 [D loss: 0.468075, acc.: 77.34%] [G loss: 0.591951]\n",
      "epoch:8 step:7714 [D loss: 0.515420, acc.: 76.56%] [G loss: 0.661642]\n",
      "epoch:8 step:7715 [D loss: 0.559248, acc.: 71.09%] [G loss: 0.593686]\n",
      "epoch:8 step:7716 [D loss: 0.659482, acc.: 65.62%] [G loss: 0.644763]\n",
      "epoch:8 step:7717 [D loss: 0.587751, acc.: 67.97%] [G loss: 0.542577]\n",
      "epoch:8 step:7718 [D loss: 0.481828, acc.: 79.69%] [G loss: 0.700713]\n",
      "epoch:8 step:7719 [D loss: 0.504807, acc.: 78.12%] [G loss: 0.769724]\n",
      "epoch:8 step:7720 [D loss: 0.599718, acc.: 67.97%] [G loss: 0.518523]\n",
      "epoch:8 step:7721 [D loss: 0.517766, acc.: 74.22%] [G loss: 0.667977]\n",
      "epoch:8 step:7722 [D loss: 0.606862, acc.: 67.19%] [G loss: 0.439217]\n",
      "epoch:8 step:7723 [D loss: 0.555180, acc.: 71.09%] [G loss: 0.338958]\n",
      "epoch:8 step:7724 [D loss: 0.599280, acc.: 63.28%] [G loss: 0.403699]\n",
      "epoch:8 step:7725 [D loss: 0.574924, acc.: 70.31%] [G loss: 0.332337]\n",
      "epoch:8 step:7726 [D loss: 0.502154, acc.: 78.12%] [G loss: 0.607112]\n",
      "epoch:8 step:7727 [D loss: 0.522830, acc.: 78.91%] [G loss: 0.707643]\n",
      "epoch:8 step:7728 [D loss: 0.500181, acc.: 78.91%] [G loss: 0.757348]\n",
      "epoch:8 step:7729 [D loss: 0.557604, acc.: 73.44%] [G loss: 0.705413]\n",
      "epoch:8 step:7730 [D loss: 0.557197, acc.: 71.09%] [G loss: 0.576734]\n",
      "epoch:8 step:7731 [D loss: 0.546342, acc.: 67.97%] [G loss: 0.627910]\n",
      "epoch:8 step:7732 [D loss: 0.565820, acc.: 74.22%] [G loss: 0.574408]\n",
      "epoch:8 step:7733 [D loss: 0.555455, acc.: 71.88%] [G loss: 0.518040]\n",
      "epoch:8 step:7734 [D loss: 0.570470, acc.: 69.53%] [G loss: 0.521615]\n",
      "epoch:8 step:7735 [D loss: 0.504292, acc.: 72.66%] [G loss: 0.510838]\n",
      "epoch:8 step:7736 [D loss: 0.557343, acc.: 70.31%] [G loss: 0.555928]\n",
      "epoch:8 step:7737 [D loss: 0.525913, acc.: 74.22%] [G loss: 0.531570]\n",
      "epoch:8 step:7738 [D loss: 0.543155, acc.: 68.75%] [G loss: 0.642489]\n",
      "epoch:8 step:7739 [D loss: 0.527793, acc.: 74.22%] [G loss: 0.514597]\n",
      "epoch:8 step:7740 [D loss: 0.483074, acc.: 71.88%] [G loss: 0.720516]\n",
      "epoch:8 step:7741 [D loss: 0.553256, acc.: 68.75%] [G loss: 0.527430]\n",
      "epoch:8 step:7742 [D loss: 0.557471, acc.: 69.53%] [G loss: 0.619866]\n",
      "epoch:8 step:7743 [D loss: 0.603711, acc.: 65.62%] [G loss: 0.664748]\n",
      "epoch:8 step:7744 [D loss: 0.535637, acc.: 71.09%] [G loss: 0.619343]\n",
      "epoch:8 step:7745 [D loss: 0.578258, acc.: 68.75%] [G loss: 0.639648]\n",
      "epoch:8 step:7746 [D loss: 0.573403, acc.: 66.41%] [G loss: 0.633475]\n",
      "epoch:8 step:7747 [D loss: 0.613937, acc.: 69.53%] [G loss: 0.487119]\n",
      "epoch:8 step:7748 [D loss: 0.630864, acc.: 66.41%] [G loss: 0.516845]\n",
      "epoch:8 step:7749 [D loss: 0.564685, acc.: 72.66%] [G loss: 0.564536]\n",
      "epoch:8 step:7750 [D loss: 0.528799, acc.: 71.88%] [G loss: 0.611163]\n",
      "epoch:8 step:7751 [D loss: 0.564913, acc.: 64.06%] [G loss: 0.532757]\n",
      "epoch:8 step:7752 [D loss: 0.519179, acc.: 71.88%] [G loss: 0.480349]\n",
      "epoch:8 step:7753 [D loss: 0.595286, acc.: 66.41%] [G loss: 0.550242]\n",
      "epoch:8 step:7754 [D loss: 0.525869, acc.: 72.66%] [G loss: 0.453131]\n",
      "epoch:8 step:7755 [D loss: 0.574399, acc.: 67.97%] [G loss: 0.419729]\n",
      "epoch:8 step:7756 [D loss: 0.595612, acc.: 68.75%] [G loss: 0.471638]\n",
      "epoch:8 step:7757 [D loss: 0.562747, acc.: 68.75%] [G loss: 0.513374]\n",
      "epoch:8 step:7758 [D loss: 0.548826, acc.: 73.44%] [G loss: 0.593996]\n",
      "epoch:8 step:7759 [D loss: 0.649832, acc.: 65.62%] [G loss: 0.375465]\n",
      "epoch:8 step:7760 [D loss: 0.508139, acc.: 76.56%] [G loss: 0.629766]\n",
      "epoch:8 step:7761 [D loss: 0.552196, acc.: 73.44%] [G loss: 0.514957]\n",
      "epoch:8 step:7762 [D loss: 0.578720, acc.: 67.19%] [G loss: 0.429840]\n",
      "epoch:8 step:7763 [D loss: 0.517690, acc.: 71.09%] [G loss: 0.481990]\n",
      "epoch:8 step:7764 [D loss: 0.609499, acc.: 61.72%] [G loss: 0.539490]\n",
      "epoch:8 step:7765 [D loss: 0.559025, acc.: 70.31%] [G loss: 0.440434]\n",
      "epoch:8 step:7766 [D loss: 0.534291, acc.: 71.88%] [G loss: 0.440046]\n",
      "epoch:8 step:7767 [D loss: 0.529331, acc.: 70.31%] [G loss: 0.451694]\n",
      "epoch:8 step:7768 [D loss: 0.544547, acc.: 73.44%] [G loss: 0.565967]\n",
      "epoch:8 step:7769 [D loss: 0.530181, acc.: 76.56%] [G loss: 0.526867]\n",
      "epoch:8 step:7770 [D loss: 0.545677, acc.: 70.31%] [G loss: 0.565468]\n",
      "epoch:8 step:7771 [D loss: 0.612386, acc.: 69.53%] [G loss: 0.424630]\n",
      "epoch:8 step:7772 [D loss: 0.553810, acc.: 68.75%] [G loss: 0.498672]\n",
      "epoch:8 step:7773 [D loss: 0.628254, acc.: 64.06%] [G loss: 0.489912]\n",
      "epoch:8 step:7774 [D loss: 0.619064, acc.: 59.38%] [G loss: 0.388652]\n",
      "epoch:8 step:7775 [D loss: 0.566134, acc.: 67.19%] [G loss: 0.639069]\n",
      "epoch:8 step:7776 [D loss: 0.579979, acc.: 65.62%] [G loss: 0.578383]\n",
      "epoch:8 step:7777 [D loss: 0.627620, acc.: 60.16%] [G loss: 0.461985]\n",
      "epoch:8 step:7778 [D loss: 0.585757, acc.: 68.75%] [G loss: 0.512561]\n",
      "epoch:8 step:7779 [D loss: 0.571398, acc.: 68.75%] [G loss: 0.453787]\n",
      "epoch:8 step:7780 [D loss: 0.524568, acc.: 71.88%] [G loss: 0.496145]\n",
      "epoch:8 step:7781 [D loss: 0.527738, acc.: 71.88%] [G loss: 0.540687]\n",
      "epoch:8 step:7782 [D loss: 0.490820, acc.: 78.12%] [G loss: 0.622521]\n",
      "epoch:8 step:7783 [D loss: 0.598318, acc.: 64.84%] [G loss: 0.582035]\n",
      "epoch:8 step:7784 [D loss: 0.595787, acc.: 66.41%] [G loss: 0.487093]\n",
      "epoch:8 step:7785 [D loss: 0.544882, acc.: 73.44%] [G loss: 0.693894]\n",
      "epoch:8 step:7786 [D loss: 0.575793, acc.: 64.84%] [G loss: 0.601995]\n",
      "epoch:8 step:7787 [D loss: 0.580509, acc.: 68.75%] [G loss: 0.518521]\n",
      "epoch:8 step:7788 [D loss: 0.540514, acc.: 69.53%] [G loss: 0.516427]\n",
      "epoch:8 step:7789 [D loss: 0.572437, acc.: 67.19%] [G loss: 0.628081]\n",
      "epoch:8 step:7790 [D loss: 0.593853, acc.: 67.19%] [G loss: 0.442177]\n",
      "epoch:8 step:7791 [D loss: 0.520521, acc.: 71.09%] [G loss: 0.551923]\n",
      "epoch:8 step:7792 [D loss: 0.496229, acc.: 73.44%] [G loss: 0.574507]\n",
      "epoch:8 step:7793 [D loss: 0.561033, acc.: 71.09%] [G loss: 0.421027]\n",
      "epoch:8 step:7794 [D loss: 0.489955, acc.: 78.12%] [G loss: 0.510815]\n",
      "epoch:8 step:7795 [D loss: 0.496419, acc.: 76.56%] [G loss: 0.503470]\n",
      "epoch:8 step:7796 [D loss: 0.529201, acc.: 71.88%] [G loss: 0.552490]\n",
      "epoch:8 step:7797 [D loss: 0.634590, acc.: 65.62%] [G loss: 0.489390]\n",
      "epoch:8 step:7798 [D loss: 0.544549, acc.: 69.53%] [G loss: 0.565518]\n",
      "epoch:8 step:7799 [D loss: 0.626457, acc.: 60.16%] [G loss: 0.471025]\n",
      "epoch:8 step:7800 [D loss: 0.452217, acc.: 78.91%] [G loss: 0.670197]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.216199\n",
      "FID: 42.672878\n",
      "0 = 12.844607948207878\n",
      "1 = 0.09023630379601169\n",
      "2 = 0.9089000225067139\n",
      "3 = 0.8374000191688538\n",
      "4 = 0.980400025844574\n",
      "5 = 0.9771295189857483\n",
      "6 = 0.8374000191688538\n",
      "7 = 8.065163919663425\n",
      "8 = 0.1354503100735596\n",
      "9 = 0.7537999749183655\n",
      "10 = 0.7368000149726868\n",
      "11 = 0.770799994468689\n",
      "12 = 0.7627329230308533\n",
      "13 = 0.7368000149726868\n",
      "14 = 6.2162251472473145\n",
      "15 = 7.777690887451172\n",
      "16 = 0.33011454343795776\n",
      "17 = 6.2161993980407715\n",
      "18 = 42.67287826538086\n",
      "epoch:8 step:7801 [D loss: 0.543556, acc.: 73.44%] [G loss: 0.481586]\n",
      "epoch:8 step:7802 [D loss: 0.596343, acc.: 65.62%] [G loss: 0.505338]\n",
      "epoch:8 step:7803 [D loss: 0.490805, acc.: 78.12%] [G loss: 0.688580]\n",
      "epoch:8 step:7804 [D loss: 0.531111, acc.: 70.31%] [G loss: 0.607656]\n",
      "epoch:8 step:7805 [D loss: 0.531856, acc.: 73.44%] [G loss: 0.708993]\n",
      "epoch:8 step:7806 [D loss: 0.574659, acc.: 70.31%] [G loss: 0.652779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7807 [D loss: 0.483607, acc.: 74.22%] [G loss: 0.606091]\n",
      "epoch:8 step:7808 [D loss: 0.461424, acc.: 78.91%] [G loss: 0.649327]\n",
      "epoch:8 step:7809 [D loss: 0.525425, acc.: 73.44%] [G loss: 0.830447]\n",
      "epoch:8 step:7810 [D loss: 0.453094, acc.: 78.91%] [G loss: 0.671934]\n",
      "epoch:8 step:7811 [D loss: 0.488596, acc.: 78.12%] [G loss: 0.812442]\n",
      "epoch:8 step:7812 [D loss: 0.646784, acc.: 64.84%] [G loss: 0.533288]\n",
      "epoch:8 step:7813 [D loss: 0.614483, acc.: 64.06%] [G loss: 0.553382]\n",
      "epoch:8 step:7814 [D loss: 0.551231, acc.: 70.31%] [G loss: 0.481730]\n",
      "epoch:8 step:7815 [D loss: 0.533729, acc.: 72.66%] [G loss: 0.687914]\n",
      "epoch:8 step:7816 [D loss: 0.536555, acc.: 70.31%] [G loss: 0.631590]\n",
      "epoch:8 step:7817 [D loss: 0.503290, acc.: 75.78%] [G loss: 0.872853]\n",
      "epoch:8 step:7818 [D loss: 0.551775, acc.: 67.97%] [G loss: 0.738818]\n",
      "epoch:8 step:7819 [D loss: 0.605115, acc.: 65.62%] [G loss: 0.577400]\n",
      "epoch:8 step:7820 [D loss: 0.572409, acc.: 69.53%] [G loss: 0.439693]\n",
      "epoch:8 step:7821 [D loss: 0.560267, acc.: 72.66%] [G loss: 0.459338]\n",
      "epoch:8 step:7822 [D loss: 0.526088, acc.: 69.53%] [G loss: 0.608158]\n",
      "epoch:8 step:7823 [D loss: 0.512988, acc.: 75.78%] [G loss: 0.662224]\n",
      "epoch:8 step:7824 [D loss: 0.533503, acc.: 68.75%] [G loss: 0.626137]\n",
      "epoch:8 step:7825 [D loss: 0.507118, acc.: 72.66%] [G loss: 0.636294]\n",
      "epoch:8 step:7826 [D loss: 0.544795, acc.: 71.09%] [G loss: 0.461362]\n",
      "epoch:8 step:7827 [D loss: 0.556932, acc.: 67.97%] [G loss: 0.526111]\n",
      "epoch:8 step:7828 [D loss: 0.531366, acc.: 74.22%] [G loss: 0.588379]\n",
      "epoch:8 step:7829 [D loss: 0.555850, acc.: 69.53%] [G loss: 0.558052]\n",
      "epoch:8 step:7830 [D loss: 0.565684, acc.: 69.53%] [G loss: 0.561978]\n",
      "epoch:8 step:7831 [D loss: 0.511751, acc.: 73.44%] [G loss: 0.620865]\n",
      "epoch:8 step:7832 [D loss: 0.523623, acc.: 72.66%] [G loss: 0.632602]\n",
      "epoch:8 step:7833 [D loss: 0.502406, acc.: 79.69%] [G loss: 0.631768]\n",
      "epoch:8 step:7834 [D loss: 0.550033, acc.: 71.88%] [G loss: 0.633939]\n",
      "epoch:8 step:7835 [D loss: 0.545881, acc.: 74.22%] [G loss: 0.569403]\n",
      "epoch:8 step:7836 [D loss: 0.548573, acc.: 67.19%] [G loss: 0.655568]\n",
      "epoch:8 step:7837 [D loss: 0.568967, acc.: 68.75%] [G loss: 0.707871]\n",
      "epoch:8 step:7838 [D loss: 0.613259, acc.: 65.62%] [G loss: 0.584810]\n",
      "epoch:8 step:7839 [D loss: 0.449200, acc.: 79.69%] [G loss: 0.649717]\n",
      "epoch:8 step:7840 [D loss: 0.496222, acc.: 76.56%] [G loss: 0.829864]\n",
      "epoch:8 step:7841 [D loss: 0.619307, acc.: 66.41%] [G loss: 0.742219]\n",
      "epoch:8 step:7842 [D loss: 0.601401, acc.: 67.19%] [G loss: 0.797565]\n",
      "epoch:8 step:7843 [D loss: 0.480014, acc.: 80.47%] [G loss: 0.857628]\n",
      "epoch:8 step:7844 [D loss: 0.660474, acc.: 63.28%] [G loss: 0.636221]\n",
      "epoch:8 step:7845 [D loss: 0.644076, acc.: 60.94%] [G loss: 0.569890]\n",
      "epoch:8 step:7846 [D loss: 0.571187, acc.: 71.88%] [G loss: 0.480600]\n",
      "epoch:8 step:7847 [D loss: 0.541422, acc.: 70.31%] [G loss: 0.577705]\n",
      "epoch:8 step:7848 [D loss: 0.599807, acc.: 70.31%] [G loss: 0.626640]\n",
      "epoch:8 step:7849 [D loss: 0.585142, acc.: 64.84%] [G loss: 0.522568]\n",
      "epoch:8 step:7850 [D loss: 0.460078, acc.: 77.34%] [G loss: 0.585539]\n",
      "epoch:8 step:7851 [D loss: 0.580582, acc.: 65.62%] [G loss: 0.502239]\n",
      "epoch:8 step:7852 [D loss: 0.573079, acc.: 68.75%] [G loss: 0.496946]\n",
      "epoch:8 step:7853 [D loss: 0.478563, acc.: 82.03%] [G loss: 0.584150]\n",
      "epoch:8 step:7854 [D loss: 0.499851, acc.: 78.12%] [G loss: 0.645389]\n",
      "epoch:8 step:7855 [D loss: 0.478574, acc.: 78.12%] [G loss: 0.741806]\n",
      "epoch:8 step:7856 [D loss: 0.508486, acc.: 74.22%] [G loss: 0.653047]\n",
      "epoch:8 step:7857 [D loss: 0.554279, acc.: 71.09%] [G loss: 0.696237]\n",
      "epoch:8 step:7858 [D loss: 0.589736, acc.: 67.19%] [G loss: 0.541473]\n",
      "epoch:8 step:7859 [D loss: 0.565903, acc.: 67.19%] [G loss: 0.495481]\n",
      "epoch:8 step:7860 [D loss: 0.560378, acc.: 68.75%] [G loss: 0.492624]\n",
      "epoch:8 step:7861 [D loss: 0.542547, acc.: 71.09%] [G loss: 0.517795]\n",
      "epoch:8 step:7862 [D loss: 0.541964, acc.: 69.53%] [G loss: 0.532278]\n",
      "epoch:8 step:7863 [D loss: 0.577739, acc.: 67.19%] [G loss: 0.600039]\n",
      "epoch:8 step:7864 [D loss: 0.557743, acc.: 71.88%] [G loss: 0.459818]\n",
      "epoch:8 step:7865 [D loss: 0.592278, acc.: 69.53%] [G loss: 0.543104]\n",
      "epoch:8 step:7866 [D loss: 0.563073, acc.: 71.09%] [G loss: 0.577585]\n",
      "epoch:8 step:7867 [D loss: 0.528484, acc.: 74.22%] [G loss: 0.626760]\n",
      "epoch:8 step:7868 [D loss: 0.548445, acc.: 69.53%] [G loss: 0.502519]\n",
      "epoch:8 step:7869 [D loss: 0.592639, acc.: 65.62%] [G loss: 0.448544]\n",
      "epoch:8 step:7870 [D loss: 0.466027, acc.: 81.25%] [G loss: 0.507250]\n",
      "epoch:8 step:7871 [D loss: 0.592795, acc.: 68.75%] [G loss: 0.535416]\n",
      "epoch:8 step:7872 [D loss: 0.709403, acc.: 60.16%] [G loss: 0.488206]\n",
      "epoch:8 step:7873 [D loss: 0.595834, acc.: 66.41%] [G loss: 0.561670]\n",
      "epoch:8 step:7874 [D loss: 0.553778, acc.: 67.97%] [G loss: 0.495325]\n",
      "epoch:8 step:7875 [D loss: 0.532835, acc.: 78.12%] [G loss: 0.508408]\n",
      "epoch:8 step:7876 [D loss: 0.591474, acc.: 66.41%] [G loss: 0.443861]\n",
      "epoch:8 step:7877 [D loss: 0.442959, acc.: 82.03%] [G loss: 0.559792]\n",
      "epoch:8 step:7878 [D loss: 0.488117, acc.: 78.91%] [G loss: 0.601284]\n",
      "epoch:8 step:7879 [D loss: 0.522248, acc.: 73.44%] [G loss: 0.633770]\n",
      "epoch:8 step:7880 [D loss: 0.521791, acc.: 71.09%] [G loss: 0.604163]\n",
      "epoch:8 step:7881 [D loss: 0.503594, acc.: 75.78%] [G loss: 0.669896]\n",
      "epoch:8 step:7882 [D loss: 0.599739, acc.: 64.84%] [G loss: 0.480998]\n",
      "epoch:8 step:7883 [D loss: 0.584642, acc.: 69.53%] [G loss: 0.440935]\n",
      "epoch:8 step:7884 [D loss: 0.531800, acc.: 75.00%] [G loss: 0.599049]\n",
      "epoch:8 step:7885 [D loss: 0.570334, acc.: 65.62%] [G loss: 0.585270]\n",
      "epoch:8 step:7886 [D loss: 0.563400, acc.: 70.31%] [G loss: 0.553838]\n",
      "epoch:8 step:7887 [D loss: 0.545299, acc.: 67.97%] [G loss: 0.648169]\n",
      "epoch:8 step:7888 [D loss: 0.483337, acc.: 76.56%] [G loss: 0.741774]\n",
      "epoch:8 step:7889 [D loss: 0.635631, acc.: 63.28%] [G loss: 0.441358]\n",
      "epoch:8 step:7890 [D loss: 0.520848, acc.: 75.00%] [G loss: 0.699113]\n",
      "epoch:8 step:7891 [D loss: 0.577897, acc.: 63.28%] [G loss: 0.409648]\n",
      "epoch:8 step:7892 [D loss: 0.590572, acc.: 68.75%] [G loss: 0.478013]\n",
      "epoch:8 step:7893 [D loss: 0.541889, acc.: 70.31%] [G loss: 0.545064]\n",
      "epoch:8 step:7894 [D loss: 0.520862, acc.: 74.22%] [G loss: 0.601132]\n",
      "epoch:8 step:7895 [D loss: 0.513311, acc.: 75.00%] [G loss: 0.621868]\n",
      "epoch:8 step:7896 [D loss: 0.670377, acc.: 60.94%] [G loss: 0.473440]\n",
      "epoch:8 step:7897 [D loss: 0.661851, acc.: 55.47%] [G loss: 0.461767]\n",
      "epoch:8 step:7898 [D loss: 0.530969, acc.: 77.34%] [G loss: 0.550684]\n",
      "epoch:8 step:7899 [D loss: 0.476952, acc.: 76.56%] [G loss: 0.608006]\n",
      "epoch:8 step:7900 [D loss: 0.555313, acc.: 67.97%] [G loss: 0.613001]\n",
      "epoch:8 step:7901 [D loss: 0.565407, acc.: 71.88%] [G loss: 0.522637]\n",
      "epoch:8 step:7902 [D loss: 0.521271, acc.: 70.31%] [G loss: 0.716255]\n",
      "epoch:8 step:7903 [D loss: 0.606163, acc.: 68.75%] [G loss: 0.765727]\n",
      "epoch:8 step:7904 [D loss: 0.571227, acc.: 71.88%] [G loss: 0.720010]\n",
      "epoch:8 step:7905 [D loss: 0.608323, acc.: 61.72%] [G loss: 0.661267]\n",
      "epoch:8 step:7906 [D loss: 0.524229, acc.: 73.44%] [G loss: 0.512836]\n",
      "epoch:8 step:7907 [D loss: 0.555042, acc.: 66.41%] [G loss: 0.509874]\n",
      "epoch:8 step:7908 [D loss: 0.590971, acc.: 71.09%] [G loss: 0.486996]\n",
      "epoch:8 step:7909 [D loss: 0.547043, acc.: 73.44%] [G loss: 0.589953]\n",
      "epoch:8 step:7910 [D loss: 0.574074, acc.: 68.75%] [G loss: 0.683490]\n",
      "epoch:8 step:7911 [D loss: 0.544555, acc.: 73.44%] [G loss: 0.773492]\n",
      "epoch:8 step:7912 [D loss: 0.487901, acc.: 78.91%] [G loss: 0.768834]\n",
      "epoch:8 step:7913 [D loss: 0.624708, acc.: 64.06%] [G loss: 0.720505]\n",
      "epoch:8 step:7914 [D loss: 0.658202, acc.: 60.16%] [G loss: 0.490876]\n",
      "epoch:8 step:7915 [D loss: 0.565952, acc.: 71.09%] [G loss: 0.713678]\n",
      "epoch:8 step:7916 [D loss: 0.570794, acc.: 67.97%] [G loss: 0.680297]\n",
      "epoch:8 step:7917 [D loss: 0.596095, acc.: 67.97%] [G loss: 0.444676]\n",
      "epoch:8 step:7918 [D loss: 0.604390, acc.: 63.28%] [G loss: 0.560008]\n",
      "epoch:8 step:7919 [D loss: 0.605480, acc.: 70.31%] [G loss: 0.432152]\n",
      "epoch:8 step:7920 [D loss: 0.601174, acc.: 66.41%] [G loss: 0.459221]\n",
      "epoch:8 step:7921 [D loss: 0.558193, acc.: 75.00%] [G loss: 0.544962]\n",
      "epoch:8 step:7922 [D loss: 0.543566, acc.: 70.31%] [G loss: 0.730821]\n",
      "epoch:8 step:7923 [D loss: 0.503018, acc.: 71.09%] [G loss: 0.676652]\n",
      "epoch:8 step:7924 [D loss: 0.536513, acc.: 70.31%] [G loss: 0.592051]\n",
      "epoch:8 step:7925 [D loss: 0.498508, acc.: 74.22%] [G loss: 0.661343]\n",
      "epoch:8 step:7926 [D loss: 0.550278, acc.: 67.19%] [G loss: 0.731987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7927 [D loss: 0.515234, acc.: 72.66%] [G loss: 0.575742]\n",
      "epoch:8 step:7928 [D loss: 0.587408, acc.: 65.62%] [G loss: 0.535832]\n",
      "epoch:8 step:7929 [D loss: 0.581779, acc.: 69.53%] [G loss: 0.523492]\n",
      "epoch:8 step:7930 [D loss: 0.520643, acc.: 77.34%] [G loss: 0.499214]\n",
      "epoch:8 step:7931 [D loss: 0.609760, acc.: 69.53%] [G loss: 0.523237]\n",
      "epoch:8 step:7932 [D loss: 0.471770, acc.: 77.34%] [G loss: 0.599374]\n",
      "epoch:8 step:7933 [D loss: 0.665252, acc.: 67.19%] [G loss: 0.484097]\n",
      "epoch:8 step:7934 [D loss: 0.567530, acc.: 61.72%] [G loss: 0.529851]\n",
      "epoch:8 step:7935 [D loss: 0.512811, acc.: 72.66%] [G loss: 0.598507]\n",
      "epoch:8 step:7936 [D loss: 0.561438, acc.: 70.31%] [G loss: 0.634418]\n",
      "epoch:8 step:7937 [D loss: 0.547988, acc.: 67.97%] [G loss: 0.633836]\n",
      "epoch:8 step:7938 [D loss: 0.581758, acc.: 71.88%] [G loss: 0.507163]\n",
      "epoch:8 step:7939 [D loss: 0.573319, acc.: 70.31%] [G loss: 0.515570]\n",
      "epoch:8 step:7940 [D loss: 0.551091, acc.: 72.66%] [G loss: 0.547515]\n",
      "epoch:8 step:7941 [D loss: 0.540130, acc.: 71.88%] [G loss: 0.650975]\n",
      "epoch:8 step:7942 [D loss: 0.577643, acc.: 68.75%] [G loss: 0.566658]\n",
      "epoch:8 step:7943 [D loss: 0.477595, acc.: 75.78%] [G loss: 0.632512]\n",
      "epoch:8 step:7944 [D loss: 0.512284, acc.: 74.22%] [G loss: 0.631174]\n",
      "epoch:8 step:7945 [D loss: 0.561015, acc.: 71.88%] [G loss: 0.561364]\n",
      "epoch:8 step:7946 [D loss: 0.538471, acc.: 71.88%] [G loss: 0.619403]\n",
      "epoch:8 step:7947 [D loss: 0.440553, acc.: 82.03%] [G loss: 0.738000]\n",
      "epoch:8 step:7948 [D loss: 0.501788, acc.: 77.34%] [G loss: 0.677604]\n",
      "epoch:8 step:7949 [D loss: 0.501824, acc.: 75.78%] [G loss: 0.611329]\n",
      "epoch:8 step:7950 [D loss: 0.584542, acc.: 71.88%] [G loss: 0.615325]\n",
      "epoch:8 step:7951 [D loss: 0.559162, acc.: 71.88%] [G loss: 0.546721]\n",
      "epoch:8 step:7952 [D loss: 0.652202, acc.: 62.50%] [G loss: 0.529551]\n",
      "epoch:8 step:7953 [D loss: 0.548118, acc.: 68.75%] [G loss: 0.629363]\n",
      "epoch:8 step:7954 [D loss: 0.619516, acc.: 65.62%] [G loss: 0.529010]\n",
      "epoch:8 step:7955 [D loss: 0.589303, acc.: 63.28%] [G loss: 0.503574]\n",
      "epoch:8 step:7956 [D loss: 0.529934, acc.: 70.31%] [G loss: 0.642596]\n",
      "epoch:8 step:7957 [D loss: 0.572467, acc.: 74.22%] [G loss: 0.561810]\n",
      "epoch:8 step:7958 [D loss: 0.622067, acc.: 60.16%] [G loss: 0.515421]\n",
      "epoch:8 step:7959 [D loss: 0.571441, acc.: 67.19%] [G loss: 0.459308]\n",
      "epoch:8 step:7960 [D loss: 0.572898, acc.: 70.31%] [G loss: 0.433203]\n",
      "epoch:8 step:7961 [D loss: 0.557573, acc.: 69.53%] [G loss: 0.548349]\n",
      "epoch:8 step:7962 [D loss: 0.566618, acc.: 67.19%] [G loss: 0.442795]\n",
      "epoch:8 step:7963 [D loss: 0.481584, acc.: 81.25%] [G loss: 0.602426]\n",
      "epoch:8 step:7964 [D loss: 0.579240, acc.: 71.88%] [G loss: 0.597999]\n",
      "epoch:8 step:7965 [D loss: 0.604689, acc.: 64.06%] [G loss: 0.546898]\n",
      "epoch:8 step:7966 [D loss: 0.544380, acc.: 69.53%] [G loss: 0.597673]\n",
      "epoch:8 step:7967 [D loss: 0.477165, acc.: 84.38%] [G loss: 0.742279]\n",
      "epoch:8 step:7968 [D loss: 0.421420, acc.: 82.03%] [G loss: 0.779358]\n",
      "epoch:8 step:7969 [D loss: 0.617766, acc.: 64.84%] [G loss: 0.605380]\n",
      "epoch:8 step:7970 [D loss: 0.511831, acc.: 76.56%] [G loss: 0.659150]\n",
      "epoch:8 step:7971 [D loss: 0.451146, acc.: 81.25%] [G loss: 0.693461]\n",
      "epoch:8 step:7972 [D loss: 0.602766, acc.: 65.62%] [G loss: 0.677176]\n",
      "epoch:8 step:7973 [D loss: 0.734799, acc.: 58.59%] [G loss: 0.431706]\n",
      "epoch:8 step:7974 [D loss: 0.569473, acc.: 74.22%] [G loss: 0.420491]\n",
      "epoch:8 step:7975 [D loss: 0.586223, acc.: 71.88%] [G loss: 0.390454]\n",
      "epoch:8 step:7976 [D loss: 0.583457, acc.: 68.75%] [G loss: 0.453974]\n",
      "epoch:8 step:7977 [D loss: 0.541415, acc.: 71.09%] [G loss: 0.616062]\n",
      "epoch:8 step:7978 [D loss: 0.604503, acc.: 63.28%] [G loss: 0.524660]\n",
      "epoch:8 step:7979 [D loss: 0.567261, acc.: 69.53%] [G loss: 0.603395]\n",
      "epoch:8 step:7980 [D loss: 0.533428, acc.: 76.56%] [G loss: 0.510960]\n",
      "epoch:8 step:7981 [D loss: 0.568650, acc.: 68.75%] [G loss: 0.565785]\n",
      "epoch:8 step:7982 [D loss: 0.565607, acc.: 69.53%] [G loss: 0.513029]\n",
      "epoch:8 step:7983 [D loss: 0.556921, acc.: 71.09%] [G loss: 0.501891]\n",
      "epoch:8 step:7984 [D loss: 0.497647, acc.: 71.09%] [G loss: 0.578173]\n",
      "epoch:8 step:7985 [D loss: 0.538463, acc.: 70.31%] [G loss: 0.495358]\n",
      "epoch:8 step:7986 [D loss: 0.549031, acc.: 72.66%] [G loss: 0.537598]\n",
      "epoch:8 step:7987 [D loss: 0.542967, acc.: 71.09%] [G loss: 0.502480]\n",
      "epoch:8 step:7988 [D loss: 0.588050, acc.: 64.06%] [G loss: 0.499497]\n",
      "epoch:8 step:7989 [D loss: 0.586009, acc.: 68.75%] [G loss: 0.400563]\n",
      "epoch:8 step:7990 [D loss: 0.573635, acc.: 74.22%] [G loss: 0.379399]\n",
      "epoch:8 step:7991 [D loss: 0.514740, acc.: 71.88%] [G loss: 0.555956]\n",
      "epoch:8 step:7992 [D loss: 0.589915, acc.: 67.19%] [G loss: 0.503151]\n",
      "epoch:8 step:7993 [D loss: 0.518122, acc.: 75.78%] [G loss: 0.675481]\n",
      "epoch:8 step:7994 [D loss: 0.531384, acc.: 75.00%] [G loss: 0.687355]\n",
      "epoch:8 step:7995 [D loss: 0.478163, acc.: 77.34%] [G loss: 0.784417]\n",
      "epoch:8 step:7996 [D loss: 0.637530, acc.: 62.50%] [G loss: 0.652750]\n",
      "epoch:8 step:7997 [D loss: 0.639660, acc.: 66.41%] [G loss: 0.423608]\n",
      "epoch:8 step:7998 [D loss: 0.620608, acc.: 64.06%] [G loss: 0.447581]\n",
      "epoch:8 step:7999 [D loss: 0.493337, acc.: 75.00%] [G loss: 0.614148]\n",
      "epoch:8 step:8000 [D loss: 0.482046, acc.: 77.34%] [G loss: 0.603116]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.203999\n",
      "FID: 41.255726\n",
      "0 = 12.799054458713504\n",
      "1 = 0.08080206420263494\n",
      "2 = 0.9172000288963318\n",
      "3 = 0.8537999987602234\n",
      "4 = 0.9805999994277954\n",
      "5 = 0.9777828454971313\n",
      "6 = 0.8537999987602234\n",
      "7 = 8.003781255030608\n",
      "8 = 0.12819498523300982\n",
      "9 = 0.776199996471405\n",
      "10 = 0.7545999884605408\n",
      "11 = 0.7978000044822693\n",
      "12 = 0.788670539855957\n",
      "13 = 0.7545999884605408\n",
      "14 = 6.204021453857422\n",
      "15 = 7.906357765197754\n",
      "16 = 0.3174947500228882\n",
      "17 = 6.203998565673828\n",
      "18 = 41.2557258605957\n",
      "epoch:8 step:8001 [D loss: 0.506701, acc.: 75.78%] [G loss: 0.587342]\n",
      "epoch:8 step:8002 [D loss: 0.538074, acc.: 73.44%] [G loss: 0.629396]\n",
      "epoch:8 step:8003 [D loss: 0.553078, acc.: 67.97%] [G loss: 0.625326]\n",
      "epoch:8 step:8004 [D loss: 0.454793, acc.: 80.47%] [G loss: 0.738312]\n",
      "epoch:8 step:8005 [D loss: 0.524228, acc.: 71.09%] [G loss: 0.710450]\n",
      "epoch:8 step:8006 [D loss: 0.603694, acc.: 61.72%] [G loss: 0.527329]\n",
      "epoch:8 step:8007 [D loss: 0.684955, acc.: 55.47%] [G loss: 0.513450]\n",
      "epoch:8 step:8008 [D loss: 0.620000, acc.: 63.28%] [G loss: 0.489017]\n",
      "epoch:8 step:8009 [D loss: 0.505020, acc.: 78.12%] [G loss: 0.571743]\n",
      "epoch:8 step:8010 [D loss: 0.524749, acc.: 71.88%] [G loss: 0.448046]\n",
      "epoch:8 step:8011 [D loss: 0.526803, acc.: 73.44%] [G loss: 0.725229]\n",
      "epoch:8 step:8012 [D loss: 0.479903, acc.: 77.34%] [G loss: 0.667521]\n",
      "epoch:8 step:8013 [D loss: 0.504856, acc.: 80.47%] [G loss: 0.672615]\n",
      "epoch:8 step:8014 [D loss: 0.490127, acc.: 75.00%] [G loss: 0.568992]\n",
      "epoch:8 step:8015 [D loss: 0.504879, acc.: 80.47%] [G loss: 0.733005]\n",
      "epoch:8 step:8016 [D loss: 0.476896, acc.: 77.34%] [G loss: 0.666704]\n",
      "epoch:8 step:8017 [D loss: 0.472249, acc.: 82.81%] [G loss: 0.617749]\n",
      "epoch:8 step:8018 [D loss: 0.502701, acc.: 71.09%] [G loss: 0.649316]\n",
      "epoch:8 step:8019 [D loss: 0.490356, acc.: 77.34%] [G loss: 0.588889]\n",
      "epoch:8 step:8020 [D loss: 0.575660, acc.: 69.53%] [G loss: 0.590160]\n",
      "epoch:8 step:8021 [D loss: 0.672741, acc.: 58.59%] [G loss: 0.440376]\n",
      "epoch:8 step:8022 [D loss: 0.554381, acc.: 67.19%] [G loss: 0.460448]\n",
      "epoch:8 step:8023 [D loss: 0.539191, acc.: 71.88%] [G loss: 0.603681]\n",
      "epoch:8 step:8024 [D loss: 0.616897, acc.: 66.41%] [G loss: 0.526248]\n",
      "epoch:8 step:8025 [D loss: 0.596319, acc.: 67.97%] [G loss: 0.485296]\n",
      "epoch:8 step:8026 [D loss: 0.537054, acc.: 71.88%] [G loss: 0.480225]\n",
      "epoch:8 step:8027 [D loss: 0.564321, acc.: 72.66%] [G loss: 0.560439]\n",
      "epoch:8 step:8028 [D loss: 0.553648, acc.: 70.31%] [G loss: 0.401091]\n",
      "epoch:8 step:8029 [D loss: 0.568554, acc.: 69.53%] [G loss: 0.483839]\n",
      "epoch:8 step:8030 [D loss: 0.478916, acc.: 75.78%] [G loss: 0.663444]\n",
      "epoch:8 step:8031 [D loss: 0.664536, acc.: 60.94%] [G loss: 0.418589]\n",
      "epoch:8 step:8032 [D loss: 0.513261, acc.: 73.44%] [G loss: 0.668258]\n",
      "epoch:8 step:8033 [D loss: 0.618585, acc.: 64.06%] [G loss: 0.524484]\n",
      "epoch:8 step:8034 [D loss: 0.560686, acc.: 67.19%] [G loss: 0.420921]\n",
      "epoch:8 step:8035 [D loss: 0.613077, acc.: 67.97%] [G loss: 0.483539]\n",
      "epoch:8 step:8036 [D loss: 0.582166, acc.: 67.19%] [G loss: 0.497426]\n",
      "epoch:8 step:8037 [D loss: 0.599292, acc.: 65.62%] [G loss: 0.514219]\n",
      "epoch:8 step:8038 [D loss: 0.593152, acc.: 70.31%] [G loss: 0.530259]\n",
      "epoch:8 step:8039 [D loss: 0.583682, acc.: 71.09%] [G loss: 0.504604]\n",
      "epoch:8 step:8040 [D loss: 0.619127, acc.: 66.41%] [G loss: 0.506142]\n",
      "epoch:8 step:8041 [D loss: 0.583366, acc.: 70.31%] [G loss: 0.697259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8042 [D loss: 0.536200, acc.: 74.22%] [G loss: 0.596221]\n",
      "epoch:8 step:8043 [D loss: 0.528176, acc.: 75.00%] [G loss: 0.679482]\n",
      "epoch:8 step:8044 [D loss: 0.584570, acc.: 69.53%] [G loss: 0.554800]\n",
      "epoch:8 step:8045 [D loss: 0.577754, acc.: 70.31%] [G loss: 0.480139]\n",
      "epoch:8 step:8046 [D loss: 0.566327, acc.: 71.09%] [G loss: 0.516121]\n",
      "epoch:8 step:8047 [D loss: 0.573351, acc.: 65.62%] [G loss: 0.501227]\n",
      "epoch:8 step:8048 [D loss: 0.538472, acc.: 70.31%] [G loss: 0.543607]\n",
      "epoch:8 step:8049 [D loss: 0.587569, acc.: 71.09%] [G loss: 0.578278]\n",
      "epoch:8 step:8050 [D loss: 0.483233, acc.: 78.91%] [G loss: 0.511198]\n",
      "epoch:8 step:8051 [D loss: 0.516669, acc.: 74.22%] [G loss: 0.556635]\n",
      "epoch:8 step:8052 [D loss: 0.545235, acc.: 73.44%] [G loss: 0.526746]\n",
      "epoch:8 step:8053 [D loss: 0.496257, acc.: 75.78%] [G loss: 0.568774]\n",
      "epoch:8 step:8054 [D loss: 0.501130, acc.: 74.22%] [G loss: 0.534270]\n",
      "epoch:8 step:8055 [D loss: 0.610561, acc.: 65.62%] [G loss: 0.611085]\n",
      "epoch:8 step:8056 [D loss: 0.570760, acc.: 70.31%] [G loss: 0.461648]\n",
      "epoch:8 step:8057 [D loss: 0.544489, acc.: 72.66%] [G loss: 0.479483]\n",
      "epoch:8 step:8058 [D loss: 0.609542, acc.: 64.06%] [G loss: 0.444057]\n",
      "epoch:8 step:8059 [D loss: 0.541273, acc.: 69.53%] [G loss: 0.489247]\n",
      "epoch:8 step:8060 [D loss: 0.535160, acc.: 74.22%] [G loss: 0.559044]\n",
      "epoch:8 step:8061 [D loss: 0.671921, acc.: 59.38%] [G loss: 0.420938]\n",
      "epoch:8 step:8062 [D loss: 0.652318, acc.: 60.94%] [G loss: 0.492115]\n",
      "epoch:8 step:8063 [D loss: 0.498135, acc.: 76.56%] [G loss: 0.613809]\n",
      "epoch:8 step:8064 [D loss: 0.559344, acc.: 71.09%] [G loss: 0.410710]\n",
      "epoch:8 step:8065 [D loss: 0.536623, acc.: 71.88%] [G loss: 0.503749]\n",
      "epoch:8 step:8066 [D loss: 0.530611, acc.: 67.97%] [G loss: 0.587350]\n",
      "epoch:8 step:8067 [D loss: 0.510578, acc.: 72.66%] [G loss: 0.565270]\n",
      "epoch:8 step:8068 [D loss: 0.531815, acc.: 75.00%] [G loss: 0.538171]\n",
      "epoch:8 step:8069 [D loss: 0.565383, acc.: 71.09%] [G loss: 0.585392]\n",
      "epoch:8 step:8070 [D loss: 0.500082, acc.: 76.56%] [G loss: 0.614208]\n",
      "epoch:8 step:8071 [D loss: 0.493734, acc.: 75.78%] [G loss: 0.591556]\n",
      "epoch:8 step:8072 [D loss: 0.619706, acc.: 64.84%] [G loss: 0.560781]\n",
      "epoch:8 step:8073 [D loss: 0.582619, acc.: 67.19%] [G loss: 0.709577]\n",
      "epoch:8 step:8074 [D loss: 0.562377, acc.: 73.44%] [G loss: 0.508933]\n",
      "epoch:8 step:8075 [D loss: 0.592629, acc.: 63.28%] [G loss: 0.653145]\n",
      "epoch:8 step:8076 [D loss: 0.602855, acc.: 64.06%] [G loss: 0.595720]\n",
      "epoch:8 step:8077 [D loss: 0.563574, acc.: 69.53%] [G loss: 0.610581]\n",
      "epoch:8 step:8078 [D loss: 0.510517, acc.: 73.44%] [G loss: 0.617906]\n",
      "epoch:8 step:8079 [D loss: 0.574189, acc.: 70.31%] [G loss: 0.642915]\n",
      "epoch:8 step:8080 [D loss: 0.675777, acc.: 58.59%] [G loss: 0.520993]\n",
      "epoch:8 step:8081 [D loss: 0.516802, acc.: 72.66%] [G loss: 0.517594]\n",
      "epoch:8 step:8082 [D loss: 0.557331, acc.: 71.09%] [G loss: 0.558874]\n",
      "epoch:8 step:8083 [D loss: 0.642656, acc.: 61.72%] [G loss: 0.489009]\n",
      "epoch:8 step:8084 [D loss: 0.539019, acc.: 70.31%] [G loss: 0.661688]\n",
      "epoch:8 step:8085 [D loss: 0.541198, acc.: 71.09%] [G loss: 0.601755]\n",
      "epoch:8 step:8086 [D loss: 0.512928, acc.: 76.56%] [G loss: 0.629250]\n",
      "epoch:8 step:8087 [D loss: 0.557905, acc.: 71.09%] [G loss: 0.503670]\n",
      "epoch:8 step:8088 [D loss: 0.492008, acc.: 79.69%] [G loss: 0.599209]\n",
      "epoch:8 step:8089 [D loss: 0.598732, acc.: 67.19%] [G loss: 0.564623]\n",
      "epoch:8 step:8090 [D loss: 0.550407, acc.: 68.75%] [G loss: 0.453581]\n",
      "epoch:8 step:8091 [D loss: 0.557428, acc.: 65.62%] [G loss: 0.484861]\n",
      "epoch:8 step:8092 [D loss: 0.517409, acc.: 74.22%] [G loss: 0.619861]\n",
      "epoch:8 step:8093 [D loss: 0.509195, acc.: 73.44%] [G loss: 0.577695]\n",
      "epoch:8 step:8094 [D loss: 0.559443, acc.: 71.88%] [G loss: 0.622564]\n",
      "epoch:8 step:8095 [D loss: 0.580777, acc.: 63.28%] [G loss: 0.557518]\n",
      "epoch:8 step:8096 [D loss: 0.648760, acc.: 59.38%] [G loss: 0.394069]\n",
      "epoch:8 step:8097 [D loss: 0.518152, acc.: 72.66%] [G loss: 0.588429]\n",
      "epoch:8 step:8098 [D loss: 0.539428, acc.: 67.19%] [G loss: 0.501887]\n",
      "epoch:8 step:8099 [D loss: 0.483077, acc.: 78.91%] [G loss: 0.600940]\n",
      "epoch:8 step:8100 [D loss: 0.568762, acc.: 68.75%] [G loss: 0.645637]\n",
      "epoch:8 step:8101 [D loss: 0.494760, acc.: 75.78%] [G loss: 0.634548]\n",
      "epoch:8 step:8102 [D loss: 0.605481, acc.: 66.41%] [G loss: 0.571117]\n",
      "epoch:8 step:8103 [D loss: 0.596891, acc.: 65.62%] [G loss: 0.482452]\n",
      "epoch:8 step:8104 [D loss: 0.562045, acc.: 67.97%] [G loss: 0.402387]\n",
      "epoch:8 step:8105 [D loss: 0.513095, acc.: 71.09%] [G loss: 0.486633]\n",
      "epoch:8 step:8106 [D loss: 0.550984, acc.: 71.09%] [G loss: 0.504309]\n",
      "epoch:8 step:8107 [D loss: 0.536778, acc.: 71.88%] [G loss: 0.443056]\n",
      "epoch:8 step:8108 [D loss: 0.573329, acc.: 70.31%] [G loss: 0.525886]\n",
      "epoch:8 step:8109 [D loss: 0.493892, acc.: 80.47%] [G loss: 0.381303]\n",
      "epoch:8 step:8110 [D loss: 0.560908, acc.: 68.75%] [G loss: 0.518189]\n",
      "epoch:8 step:8111 [D loss: 0.590275, acc.: 67.19%] [G loss: 0.628421]\n",
      "epoch:8 step:8112 [D loss: 0.597499, acc.: 61.72%] [G loss: 0.501893]\n",
      "epoch:8 step:8113 [D loss: 0.512635, acc.: 77.34%] [G loss: 0.590725]\n",
      "epoch:8 step:8114 [D loss: 0.541131, acc.: 68.75%] [G loss: 0.641210]\n",
      "epoch:8 step:8115 [D loss: 0.538393, acc.: 72.66%] [G loss: 0.662816]\n",
      "epoch:8 step:8116 [D loss: 0.544294, acc.: 69.53%] [G loss: 0.777181]\n",
      "epoch:8 step:8117 [D loss: 0.606767, acc.: 65.62%] [G loss: 0.673999]\n",
      "epoch:8 step:8118 [D loss: 0.655595, acc.: 64.84%] [G loss: 0.460599]\n",
      "epoch:8 step:8119 [D loss: 0.526347, acc.: 68.75%] [G loss: 0.554630]\n",
      "epoch:8 step:8120 [D loss: 0.538622, acc.: 73.44%] [G loss: 0.536640]\n",
      "epoch:8 step:8121 [D loss: 0.618535, acc.: 63.28%] [G loss: 0.401204]\n",
      "epoch:8 step:8122 [D loss: 0.595964, acc.: 67.97%] [G loss: 0.461182]\n",
      "epoch:8 step:8123 [D loss: 0.553932, acc.: 69.53%] [G loss: 0.628290]\n",
      "epoch:8 step:8124 [D loss: 0.611169, acc.: 66.41%] [G loss: 0.541241]\n",
      "epoch:8 step:8125 [D loss: 0.509193, acc.: 75.78%] [G loss: 0.511337]\n",
      "epoch:8 step:8126 [D loss: 0.524648, acc.: 75.00%] [G loss: 0.557616]\n",
      "epoch:8 step:8127 [D loss: 0.467120, acc.: 82.81%] [G loss: 0.648002]\n",
      "epoch:8 step:8128 [D loss: 0.485494, acc.: 77.34%] [G loss: 0.637091]\n",
      "epoch:8 step:8129 [D loss: 0.514825, acc.: 68.75%] [G loss: 0.611390]\n",
      "epoch:8 step:8130 [D loss: 0.527349, acc.: 75.00%] [G loss: 0.652008]\n",
      "epoch:8 step:8131 [D loss: 0.533022, acc.: 66.41%] [G loss: 0.470452]\n",
      "epoch:8 step:8132 [D loss: 0.599078, acc.: 66.41%] [G loss: 0.503598]\n",
      "epoch:8 step:8133 [D loss: 0.575258, acc.: 71.88%] [G loss: 0.380842]\n",
      "epoch:8 step:8134 [D loss: 0.531561, acc.: 71.09%] [G loss: 0.490901]\n",
      "epoch:8 step:8135 [D loss: 0.476991, acc.: 76.56%] [G loss: 0.619969]\n",
      "epoch:8 step:8136 [D loss: 0.547866, acc.: 64.06%] [G loss: 0.648902]\n",
      "epoch:8 step:8137 [D loss: 0.458322, acc.: 75.00%] [G loss: 0.696318]\n",
      "epoch:8 step:8138 [D loss: 0.498242, acc.: 77.34%] [G loss: 0.701429]\n",
      "epoch:8 step:8139 [D loss: 0.513187, acc.: 71.09%] [G loss: 0.835872]\n",
      "epoch:8 step:8140 [D loss: 0.584240, acc.: 72.66%] [G loss: 0.524132]\n",
      "epoch:8 step:8141 [D loss: 0.550782, acc.: 71.09%] [G loss: 0.508548]\n",
      "epoch:8 step:8142 [D loss: 0.511663, acc.: 75.00%] [G loss: 0.526456]\n",
      "epoch:8 step:8143 [D loss: 0.442844, acc.: 82.03%] [G loss: 0.703014]\n",
      "epoch:8 step:8144 [D loss: 0.454667, acc.: 79.69%] [G loss: 0.736973]\n",
      "epoch:8 step:8145 [D loss: 0.438592, acc.: 82.03%] [G loss: 0.864469]\n",
      "epoch:8 step:8146 [D loss: 0.520760, acc.: 71.88%] [G loss: 0.782180]\n",
      "epoch:8 step:8147 [D loss: 0.517807, acc.: 75.78%] [G loss: 0.813570]\n",
      "epoch:8 step:8148 [D loss: 0.636548, acc.: 60.16%] [G loss: 0.641594]\n",
      "epoch:8 step:8149 [D loss: 0.642414, acc.: 61.72%] [G loss: 0.564626]\n",
      "epoch:8 step:8150 [D loss: 0.565505, acc.: 67.97%] [G loss: 0.727732]\n",
      "epoch:8 step:8151 [D loss: 0.618376, acc.: 72.66%] [G loss: 0.612519]\n",
      "epoch:8 step:8152 [D loss: 0.536518, acc.: 75.78%] [G loss: 0.536583]\n",
      "epoch:8 step:8153 [D loss: 0.571012, acc.: 71.09%] [G loss: 0.466344]\n",
      "epoch:8 step:8154 [D loss: 0.575099, acc.: 63.28%] [G loss: 0.420724]\n",
      "epoch:8 step:8155 [D loss: 0.539018, acc.: 71.09%] [G loss: 0.534432]\n",
      "epoch:8 step:8156 [D loss: 0.545508, acc.: 69.53%] [G loss: 0.503065]\n",
      "epoch:8 step:8157 [D loss: 0.558588, acc.: 70.31%] [G loss: 0.485231]\n",
      "epoch:8 step:8158 [D loss: 0.591563, acc.: 64.84%] [G loss: 0.481025]\n",
      "epoch:8 step:8159 [D loss: 0.534111, acc.: 74.22%] [G loss: 0.536771]\n",
      "epoch:8 step:8160 [D loss: 0.596022, acc.: 64.84%] [G loss: 0.637463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8161 [D loss: 0.639634, acc.: 66.41%] [G loss: 0.501414]\n",
      "epoch:8 step:8162 [D loss: 0.601025, acc.: 65.62%] [G loss: 0.615107]\n",
      "epoch:8 step:8163 [D loss: 0.556106, acc.: 69.53%] [G loss: 0.612947]\n",
      "epoch:8 step:8164 [D loss: 0.570672, acc.: 71.88%] [G loss: 0.557989]\n",
      "epoch:8 step:8165 [D loss: 0.534138, acc.: 71.88%] [G loss: 0.519308]\n",
      "epoch:8 step:8166 [D loss: 0.590804, acc.: 66.41%] [G loss: 0.438076]\n",
      "epoch:8 step:8167 [D loss: 0.564347, acc.: 67.97%] [G loss: 0.485693]\n",
      "epoch:8 step:8168 [D loss: 0.611077, acc.: 66.41%] [G loss: 0.539129]\n",
      "epoch:8 step:8169 [D loss: 0.584991, acc.: 68.75%] [G loss: 0.436290]\n",
      "epoch:8 step:8170 [D loss: 0.611709, acc.: 71.88%] [G loss: 0.735440]\n",
      "epoch:8 step:8171 [D loss: 0.605587, acc.: 65.62%] [G loss: 0.513357]\n",
      "epoch:8 step:8172 [D loss: 0.576642, acc.: 68.75%] [G loss: 0.465079]\n",
      "epoch:8 step:8173 [D loss: 0.484597, acc.: 78.91%] [G loss: 0.567378]\n",
      "epoch:8 step:8174 [D loss: 0.576114, acc.: 68.75%] [G loss: 0.370312]\n",
      "epoch:8 step:8175 [D loss: 0.555884, acc.: 69.53%] [G loss: 0.562595]\n",
      "epoch:8 step:8176 [D loss: 0.534821, acc.: 75.00%] [G loss: 0.499193]\n",
      "epoch:8 step:8177 [D loss: 0.517297, acc.: 75.00%] [G loss: 0.530848]\n",
      "epoch:8 step:8178 [D loss: 0.546262, acc.: 71.09%] [G loss: 0.488605]\n",
      "epoch:8 step:8179 [D loss: 0.591553, acc.: 67.19%] [G loss: 0.417573]\n",
      "epoch:8 step:8180 [D loss: 0.613377, acc.: 64.06%] [G loss: 0.387939]\n",
      "epoch:8 step:8181 [D loss: 0.526317, acc.: 75.78%] [G loss: 0.575641]\n",
      "epoch:8 step:8182 [D loss: 0.582753, acc.: 66.41%] [G loss: 0.619260]\n",
      "epoch:8 step:8183 [D loss: 0.554876, acc.: 68.75%] [G loss: 0.522699]\n",
      "epoch:8 step:8184 [D loss: 0.594503, acc.: 67.19%] [G loss: 0.485268]\n",
      "epoch:8 step:8185 [D loss: 0.517385, acc.: 76.56%] [G loss: 0.721538]\n",
      "epoch:8 step:8186 [D loss: 0.543537, acc.: 71.09%] [G loss: 0.571074]\n",
      "epoch:8 step:8187 [D loss: 0.508387, acc.: 74.22%] [G loss: 0.699491]\n",
      "epoch:8 step:8188 [D loss: 0.576332, acc.: 71.09%] [G loss: 0.498156]\n",
      "epoch:8 step:8189 [D loss: 0.524920, acc.: 76.56%] [G loss: 0.643305]\n",
      "epoch:8 step:8190 [D loss: 0.503287, acc.: 75.00%] [G loss: 0.725214]\n",
      "epoch:8 step:8191 [D loss: 0.553009, acc.: 69.53%] [G loss: 0.610559]\n",
      "epoch:8 step:8192 [D loss: 0.577883, acc.: 65.62%] [G loss: 0.473075]\n",
      "epoch:8 step:8193 [D loss: 0.569492, acc.: 65.62%] [G loss: 0.519929]\n",
      "epoch:8 step:8194 [D loss: 0.568383, acc.: 69.53%] [G loss: 0.508013]\n",
      "epoch:8 step:8195 [D loss: 0.566749, acc.: 71.09%] [G loss: 0.611372]\n",
      "epoch:8 step:8196 [D loss: 0.574788, acc.: 67.19%] [G loss: 0.641666]\n",
      "epoch:8 step:8197 [D loss: 0.531119, acc.: 73.44%] [G loss: 0.687370]\n",
      "epoch:8 step:8198 [D loss: 0.596530, acc.: 67.19%] [G loss: 0.473335]\n",
      "epoch:8 step:8199 [D loss: 0.609941, acc.: 64.84%] [G loss: 0.469327]\n",
      "epoch:8 step:8200 [D loss: 0.571892, acc.: 67.97%] [G loss: 0.488895]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.254962\n",
      "FID: 47.444523\n",
      "0 = 13.009852411365484\n",
      "1 = 0.09706117521758709\n",
      "2 = 0.921500027179718\n",
      "3 = 0.8615999817848206\n",
      "4 = 0.9814000129699707\n",
      "5 = 0.9788684248924255\n",
      "6 = 0.8615999817848206\n",
      "7 = 8.302133133113378\n",
      "8 = 0.14612627126436056\n",
      "9 = 0.7502999901771545\n",
      "10 = 0.7390000224113464\n",
      "11 = 0.7616000175476074\n",
      "12 = 0.756087601184845\n",
      "13 = 0.7390000224113464\n",
      "14 = 6.254991054534912\n",
      "15 = 7.3361334800720215\n",
      "16 = 0.367736279964447\n",
      "17 = 6.25496244430542\n",
      "18 = 47.444522857666016\n",
      "epoch:8 step:8201 [D loss: 0.530720, acc.: 71.88%] [G loss: 0.430241]\n",
      "epoch:8 step:8202 [D loss: 0.531654, acc.: 70.31%] [G loss: 0.605974]\n",
      "epoch:8 step:8203 [D loss: 0.481438, acc.: 76.56%] [G loss: 0.733623]\n",
      "epoch:8 step:8204 [D loss: 0.558589, acc.: 69.53%] [G loss: 0.691157]\n",
      "epoch:8 step:8205 [D loss: 0.532881, acc.: 69.53%] [G loss: 0.761293]\n",
      "epoch:8 step:8206 [D loss: 0.681452, acc.: 63.28%] [G loss: 0.487257]\n",
      "epoch:8 step:8207 [D loss: 0.574084, acc.: 67.19%] [G loss: 0.546594]\n",
      "epoch:8 step:8208 [D loss: 0.573691, acc.: 67.19%] [G loss: 0.510492]\n",
      "epoch:8 step:8209 [D loss: 0.607560, acc.: 63.28%] [G loss: 0.493687]\n",
      "epoch:8 step:8210 [D loss: 0.556703, acc.: 68.75%] [G loss: 0.520842]\n",
      "epoch:8 step:8211 [D loss: 0.557853, acc.: 73.44%] [G loss: 0.510309]\n",
      "epoch:8 step:8212 [D loss: 0.621721, acc.: 64.84%] [G loss: 0.423879]\n",
      "epoch:8 step:8213 [D loss: 0.585605, acc.: 65.62%] [G loss: 0.424335]\n",
      "epoch:8 step:8214 [D loss: 0.589664, acc.: 67.19%] [G loss: 0.314655]\n",
      "epoch:8 step:8215 [D loss: 0.533520, acc.: 75.00%] [G loss: 0.426049]\n",
      "epoch:8 step:8216 [D loss: 0.598188, acc.: 65.62%] [G loss: 0.553864]\n",
      "epoch:8 step:8217 [D loss: 0.647920, acc.: 57.81%] [G loss: 0.434509]\n",
      "epoch:8 step:8218 [D loss: 0.545064, acc.: 71.88%] [G loss: 0.517146]\n",
      "epoch:8 step:8219 [D loss: 0.549186, acc.: 71.88%] [G loss: 0.452748]\n",
      "epoch:8 step:8220 [D loss: 0.527576, acc.: 71.88%] [G loss: 0.639968]\n",
      "epoch:8 step:8221 [D loss: 0.486591, acc.: 79.69%] [G loss: 0.573028]\n",
      "epoch:8 step:8222 [D loss: 0.601300, acc.: 67.97%] [G loss: 0.526721]\n",
      "epoch:8 step:8223 [D loss: 0.609129, acc.: 64.84%] [G loss: 0.506226]\n",
      "epoch:8 step:8224 [D loss: 0.553918, acc.: 70.31%] [G loss: 0.525058]\n",
      "epoch:8 step:8225 [D loss: 0.612827, acc.: 60.16%] [G loss: 0.522310]\n",
      "epoch:8 step:8226 [D loss: 0.553477, acc.: 74.22%] [G loss: 0.426445]\n",
      "epoch:8 step:8227 [D loss: 0.583627, acc.: 67.97%] [G loss: 0.539105]\n",
      "epoch:8 step:8228 [D loss: 0.565597, acc.: 68.75%] [G loss: 0.589530]\n",
      "epoch:8 step:8229 [D loss: 0.602524, acc.: 69.53%] [G loss: 0.529572]\n",
      "epoch:8 step:8230 [D loss: 0.582102, acc.: 67.97%] [G loss: 0.557305]\n",
      "epoch:8 step:8231 [D loss: 0.599188, acc.: 64.84%] [G loss: 0.303317]\n",
      "epoch:8 step:8232 [D loss: 0.487120, acc.: 71.88%] [G loss: 0.618795]\n",
      "epoch:8 step:8233 [D loss: 0.520673, acc.: 75.00%] [G loss: 0.516189]\n",
      "epoch:8 step:8234 [D loss: 0.648817, acc.: 62.50%] [G loss: 0.467173]\n",
      "epoch:8 step:8235 [D loss: 0.637424, acc.: 61.72%] [G loss: 0.422809]\n",
      "epoch:8 step:8236 [D loss: 0.671171, acc.: 63.28%] [G loss: 0.380457]\n",
      "epoch:8 step:8237 [D loss: 0.553275, acc.: 71.88%] [G loss: 0.385922]\n",
      "epoch:8 step:8238 [D loss: 0.545275, acc.: 73.44%] [G loss: 0.526122]\n",
      "epoch:8 step:8239 [D loss: 0.529182, acc.: 74.22%] [G loss: 0.627783]\n",
      "epoch:8 step:8240 [D loss: 0.536008, acc.: 74.22%] [G loss: 0.633307]\n",
      "epoch:8 step:8241 [D loss: 0.612815, acc.: 67.97%] [G loss: 0.629865]\n",
      "epoch:8 step:8242 [D loss: 0.448679, acc.: 75.78%] [G loss: 0.587288]\n",
      "epoch:8 step:8243 [D loss: 0.473360, acc.: 75.78%] [G loss: 0.627494]\n",
      "epoch:8 step:8244 [D loss: 0.590223, acc.: 67.97%] [G loss: 0.488454]\n",
      "epoch:8 step:8245 [D loss: 0.578787, acc.: 67.97%] [G loss: 0.601388]\n",
      "epoch:8 step:8246 [D loss: 0.539476, acc.: 70.31%] [G loss: 0.572939]\n",
      "epoch:8 step:8247 [D loss: 0.574119, acc.: 71.09%] [G loss: 0.592905]\n",
      "epoch:8 step:8248 [D loss: 0.535589, acc.: 71.09%] [G loss: 0.717065]\n",
      "epoch:8 step:8249 [D loss: 0.514861, acc.: 72.66%] [G loss: 0.625279]\n",
      "epoch:8 step:8250 [D loss: 0.570342, acc.: 70.31%] [G loss: 0.449747]\n",
      "epoch:8 step:8251 [D loss: 0.520949, acc.: 75.00%] [G loss: 0.666190]\n",
      "epoch:8 step:8252 [D loss: 0.580460, acc.: 68.75%] [G loss: 0.594433]\n",
      "epoch:8 step:8253 [D loss: 0.565723, acc.: 71.09%] [G loss: 0.534608]\n",
      "epoch:8 step:8254 [D loss: 0.519771, acc.: 75.00%] [G loss: 0.560393]\n",
      "epoch:8 step:8255 [D loss: 0.537875, acc.: 73.44%] [G loss: 0.607125]\n",
      "epoch:8 step:8256 [D loss: 0.555199, acc.: 69.53%] [G loss: 0.501865]\n",
      "epoch:8 step:8257 [D loss: 0.554243, acc.: 70.31%] [G loss: 0.555384]\n",
      "epoch:8 step:8258 [D loss: 0.559064, acc.: 72.66%] [G loss: 0.462771]\n",
      "epoch:8 step:8259 [D loss: 0.554220, acc.: 71.88%] [G loss: 0.526367]\n",
      "epoch:8 step:8260 [D loss: 0.532952, acc.: 74.22%] [G loss: 0.494407]\n",
      "epoch:8 step:8261 [D loss: 0.623016, acc.: 67.19%] [G loss: 0.575072]\n",
      "epoch:8 step:8262 [D loss: 0.728847, acc.: 53.12%] [G loss: 0.356525]\n",
      "epoch:8 step:8263 [D loss: 0.534197, acc.: 71.88%] [G loss: 0.489954]\n",
      "epoch:8 step:8264 [D loss: 0.568690, acc.: 68.75%] [G loss: 0.510027]\n",
      "epoch:8 step:8265 [D loss: 0.513012, acc.: 75.00%] [G loss: 0.709479]\n",
      "epoch:8 step:8266 [D loss: 0.493356, acc.: 73.44%] [G loss: 0.676872]\n",
      "epoch:8 step:8267 [D loss: 0.520733, acc.: 75.78%] [G loss: 0.647401]\n",
      "epoch:8 step:8268 [D loss: 0.588589, acc.: 64.84%] [G loss: 0.560491]\n",
      "epoch:8 step:8269 [D loss: 0.514985, acc.: 73.44%] [G loss: 0.692576]\n",
      "epoch:8 step:8270 [D loss: 0.614623, acc.: 68.75%] [G loss: 0.524242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8271 [D loss: 0.528237, acc.: 75.78%] [G loss: 0.644511]\n",
      "epoch:8 step:8272 [D loss: 0.601485, acc.: 67.19%] [G loss: 0.542175]\n",
      "epoch:8 step:8273 [D loss: 0.581476, acc.: 67.97%] [G loss: 0.511052]\n",
      "epoch:8 step:8274 [D loss: 0.519137, acc.: 74.22%] [G loss: 0.601727]\n",
      "epoch:8 step:8275 [D loss: 0.573873, acc.: 66.41%] [G loss: 0.533414]\n",
      "epoch:8 step:8276 [D loss: 0.541813, acc.: 72.66%] [G loss: 0.559256]\n",
      "epoch:8 step:8277 [D loss: 0.536166, acc.: 74.22%] [G loss: 0.697225]\n",
      "epoch:8 step:8278 [D loss: 0.534703, acc.: 71.09%] [G loss: 0.817563]\n",
      "epoch:8 step:8279 [D loss: 0.569215, acc.: 69.53%] [G loss: 0.592128]\n",
      "epoch:8 step:8280 [D loss: 0.627433, acc.: 61.72%] [G loss: 0.436867]\n",
      "epoch:8 step:8281 [D loss: 0.588698, acc.: 64.84%] [G loss: 0.496841]\n",
      "epoch:8 step:8282 [D loss: 0.537536, acc.: 75.78%] [G loss: 0.611865]\n",
      "epoch:8 step:8283 [D loss: 0.598571, acc.: 65.62%] [G loss: 0.512242]\n",
      "epoch:8 step:8284 [D loss: 0.636839, acc.: 62.50%] [G loss: 0.527127]\n",
      "epoch:8 step:8285 [D loss: 0.546355, acc.: 68.75%] [G loss: 0.623926]\n",
      "epoch:8 step:8286 [D loss: 0.496707, acc.: 75.00%] [G loss: 0.662950]\n",
      "epoch:8 step:8287 [D loss: 0.545492, acc.: 75.00%] [G loss: 0.514720]\n",
      "epoch:8 step:8288 [D loss: 0.523851, acc.: 72.66%] [G loss: 0.551000]\n",
      "epoch:8 step:8289 [D loss: 0.597161, acc.: 67.97%] [G loss: 0.522801]\n",
      "epoch:8 step:8290 [D loss: 0.592247, acc.: 63.28%] [G loss: 0.424219]\n",
      "epoch:8 step:8291 [D loss: 0.578498, acc.: 65.62%] [G loss: 0.461807]\n",
      "epoch:8 step:8292 [D loss: 0.457759, acc.: 79.69%] [G loss: 0.565098]\n",
      "epoch:8 step:8293 [D loss: 0.573177, acc.: 65.62%] [G loss: 0.534967]\n",
      "epoch:8 step:8294 [D loss: 0.538734, acc.: 71.88%] [G loss: 0.646412]\n",
      "epoch:8 step:8295 [D loss: 0.560326, acc.: 71.09%] [G loss: 0.533803]\n",
      "epoch:8 step:8296 [D loss: 0.592972, acc.: 68.75%] [G loss: 0.510785]\n",
      "epoch:8 step:8297 [D loss: 0.534605, acc.: 73.44%] [G loss: 0.672552]\n",
      "epoch:8 step:8298 [D loss: 0.506618, acc.: 75.00%] [G loss: 0.699837]\n",
      "epoch:8 step:8299 [D loss: 0.552588, acc.: 68.75%] [G loss: 0.602342]\n",
      "epoch:8 step:8300 [D loss: 0.581706, acc.: 62.50%] [G loss: 0.517699]\n",
      "epoch:8 step:8301 [D loss: 0.543484, acc.: 67.19%] [G loss: 0.494374]\n",
      "epoch:8 step:8302 [D loss: 0.537914, acc.: 71.88%] [G loss: 0.397400]\n",
      "epoch:8 step:8303 [D loss: 0.520560, acc.: 72.66%] [G loss: 0.497493]\n",
      "epoch:8 step:8304 [D loss: 0.556675, acc.: 68.75%] [G loss: 0.464008]\n",
      "epoch:8 step:8305 [D loss: 0.552952, acc.: 70.31%] [G loss: 0.454446]\n",
      "epoch:8 step:8306 [D loss: 0.482954, acc.: 78.12%] [G loss: 0.622954]\n",
      "epoch:8 step:8307 [D loss: 0.568610, acc.: 70.31%] [G loss: 0.566906]\n",
      "epoch:8 step:8308 [D loss: 0.625178, acc.: 64.84%] [G loss: 0.487016]\n",
      "epoch:8 step:8309 [D loss: 0.594091, acc.: 65.62%] [G loss: 0.460936]\n",
      "epoch:8 step:8310 [D loss: 0.567114, acc.: 66.41%] [G loss: 0.495038]\n",
      "epoch:8 step:8311 [D loss: 0.559282, acc.: 67.97%] [G loss: 0.519384]\n",
      "epoch:8 step:8312 [D loss: 0.642087, acc.: 60.94%] [G loss: 0.578958]\n",
      "epoch:8 step:8313 [D loss: 0.620270, acc.: 64.06%] [G loss: 0.510018]\n",
      "epoch:8 step:8314 [D loss: 0.612654, acc.: 64.84%] [G loss: 0.444791]\n",
      "epoch:8 step:8315 [D loss: 0.538207, acc.: 71.88%] [G loss: 0.527871]\n",
      "epoch:8 step:8316 [D loss: 0.595051, acc.: 71.09%] [G loss: 0.505650]\n",
      "epoch:8 step:8317 [D loss: 0.548385, acc.: 69.53%] [G loss: 0.442826]\n",
      "epoch:8 step:8318 [D loss: 0.523460, acc.: 71.88%] [G loss: 0.477250]\n",
      "epoch:8 step:8319 [D loss: 0.495627, acc.: 73.44%] [G loss: 0.487157]\n",
      "epoch:8 step:8320 [D loss: 0.627595, acc.: 64.06%] [G loss: 0.505711]\n",
      "epoch:8 step:8321 [D loss: 0.599519, acc.: 64.06%] [G loss: 0.403038]\n",
      "epoch:8 step:8322 [D loss: 0.589208, acc.: 64.06%] [G loss: 0.404266]\n",
      "epoch:8 step:8323 [D loss: 0.580264, acc.: 64.06%] [G loss: 0.522803]\n",
      "epoch:8 step:8324 [D loss: 0.624274, acc.: 63.28%] [G loss: 0.434977]\n",
      "epoch:8 step:8325 [D loss: 0.558983, acc.: 70.31%] [G loss: 0.525499]\n",
      "epoch:8 step:8326 [D loss: 0.546274, acc.: 69.53%] [G loss: 0.502916]\n",
      "epoch:8 step:8327 [D loss: 0.540036, acc.: 69.53%] [G loss: 0.572770]\n",
      "epoch:8 step:8328 [D loss: 0.548795, acc.: 70.31%] [G loss: 0.497581]\n",
      "epoch:8 step:8329 [D loss: 0.582546, acc.: 66.41%] [G loss: 0.453982]\n",
      "epoch:8 step:8330 [D loss: 0.547673, acc.: 72.66%] [G loss: 0.598310]\n",
      "epoch:8 step:8331 [D loss: 0.531872, acc.: 73.44%] [G loss: 0.542877]\n",
      "epoch:8 step:8332 [D loss: 0.558070, acc.: 73.44%] [G loss: 0.566013]\n",
      "epoch:8 step:8333 [D loss: 0.551484, acc.: 68.75%] [G loss: 0.382190]\n",
      "epoch:8 step:8334 [D loss: 0.546722, acc.: 66.41%] [G loss: 0.461631]\n",
      "epoch:8 step:8335 [D loss: 0.567853, acc.: 71.09%] [G loss: 0.496312]\n",
      "epoch:8 step:8336 [D loss: 0.600238, acc.: 67.97%] [G loss: 0.421283]\n",
      "epoch:8 step:8337 [D loss: 0.514069, acc.: 71.88%] [G loss: 0.502159]\n",
      "epoch:8 step:8338 [D loss: 0.518555, acc.: 73.44%] [G loss: 0.501296]\n",
      "epoch:8 step:8339 [D loss: 0.542936, acc.: 72.66%] [G loss: 0.463034]\n",
      "epoch:8 step:8340 [D loss: 0.561101, acc.: 67.97%] [G loss: 0.513698]\n",
      "epoch:8 step:8341 [D loss: 0.584977, acc.: 66.41%] [G loss: 0.525195]\n",
      "epoch:8 step:8342 [D loss: 0.591405, acc.: 64.84%] [G loss: 0.414277]\n",
      "epoch:8 step:8343 [D loss: 0.603566, acc.: 66.41%] [G loss: 0.357902]\n",
      "epoch:8 step:8344 [D loss: 0.591368, acc.: 60.94%] [G loss: 0.339220]\n",
      "epoch:8 step:8345 [D loss: 0.568921, acc.: 68.75%] [G loss: 0.508125]\n",
      "epoch:8 step:8346 [D loss: 0.581864, acc.: 67.97%] [G loss: 0.334838]\n",
      "epoch:8 step:8347 [D loss: 0.539560, acc.: 75.78%] [G loss: 0.459963]\n",
      "epoch:8 step:8348 [D loss: 0.535389, acc.: 71.09%] [G loss: 0.510555]\n",
      "epoch:8 step:8349 [D loss: 0.638638, acc.: 62.50%] [G loss: 0.529196]\n",
      "epoch:8 step:8350 [D loss: 0.511283, acc.: 71.88%] [G loss: 0.546636]\n",
      "epoch:8 step:8351 [D loss: 0.567445, acc.: 68.75%] [G loss: 0.521960]\n",
      "epoch:8 step:8352 [D loss: 0.633572, acc.: 57.03%] [G loss: 0.489300]\n",
      "epoch:8 step:8353 [D loss: 0.493221, acc.: 72.66%] [G loss: 0.564199]\n",
      "epoch:8 step:8354 [D loss: 0.652339, acc.: 62.50%] [G loss: 0.465479]\n",
      "epoch:8 step:8355 [D loss: 0.570032, acc.: 70.31%] [G loss: 0.521748]\n",
      "epoch:8 step:8356 [D loss: 0.453711, acc.: 78.12%] [G loss: 0.590254]\n",
      "epoch:8 step:8357 [D loss: 0.648066, acc.: 64.84%] [G loss: 0.556428]\n",
      "epoch:8 step:8358 [D loss: 0.591171, acc.: 65.62%] [G loss: 0.424364]\n",
      "epoch:8 step:8359 [D loss: 0.611877, acc.: 64.06%] [G loss: 0.426639]\n",
      "epoch:8 step:8360 [D loss: 0.586345, acc.: 64.06%] [G loss: 0.588754]\n",
      "epoch:8 step:8361 [D loss: 0.630874, acc.: 59.38%] [G loss: 0.422127]\n",
      "epoch:8 step:8362 [D loss: 0.543200, acc.: 67.97%] [G loss: 0.530803]\n",
      "epoch:8 step:8363 [D loss: 0.676560, acc.: 64.06%] [G loss: 0.352248]\n",
      "epoch:8 step:8364 [D loss: 0.551295, acc.: 71.88%] [G loss: 0.424472]\n",
      "epoch:8 step:8365 [D loss: 0.552333, acc.: 71.09%] [G loss: 0.493099]\n",
      "epoch:8 step:8366 [D loss: 0.531717, acc.: 75.00%] [G loss: 0.471669]\n",
      "epoch:8 step:8367 [D loss: 0.460636, acc.: 78.12%] [G loss: 0.618405]\n",
      "epoch:8 step:8368 [D loss: 0.593486, acc.: 65.62%] [G loss: 0.425827]\n",
      "epoch:8 step:8369 [D loss: 0.592350, acc.: 67.97%] [G loss: 0.733618]\n",
      "epoch:8 step:8370 [D loss: 0.609714, acc.: 60.16%] [G loss: 0.489683]\n",
      "epoch:8 step:8371 [D loss: 0.496510, acc.: 79.69%] [G loss: 0.533154]\n",
      "epoch:8 step:8372 [D loss: 0.588658, acc.: 72.66%] [G loss: 0.506149]\n",
      "epoch:8 step:8373 [D loss: 0.550910, acc.: 67.97%] [G loss: 0.497569]\n",
      "epoch:8 step:8374 [D loss: 0.546728, acc.: 72.66%] [G loss: 0.505779]\n",
      "epoch:8 step:8375 [D loss: 0.589854, acc.: 67.97%] [G loss: 0.502981]\n",
      "epoch:8 step:8376 [D loss: 0.640028, acc.: 59.38%] [G loss: 0.465248]\n",
      "epoch:8 step:8377 [D loss: 0.606386, acc.: 64.84%] [G loss: 0.368751]\n",
      "epoch:8 step:8378 [D loss: 0.587808, acc.: 64.84%] [G loss: 0.473882]\n",
      "epoch:8 step:8379 [D loss: 0.575069, acc.: 68.75%] [G loss: 0.368559]\n",
      "epoch:8 step:8380 [D loss: 0.482303, acc.: 79.69%] [G loss: 0.566868]\n",
      "epoch:8 step:8381 [D loss: 0.545311, acc.: 67.97%] [G loss: 0.546774]\n",
      "epoch:8 step:8382 [D loss: 0.538423, acc.: 68.75%] [G loss: 0.501704]\n",
      "epoch:8 step:8383 [D loss: 0.553311, acc.: 72.66%] [G loss: 0.715402]\n",
      "epoch:8 step:8384 [D loss: 0.579384, acc.: 67.97%] [G loss: 0.660007]\n",
      "epoch:8 step:8385 [D loss: 0.578660, acc.: 62.50%] [G loss: 0.487033]\n",
      "epoch:8 step:8386 [D loss: 0.504570, acc.: 73.44%] [G loss: 0.653592]\n",
      "epoch:8 step:8387 [D loss: 0.581968, acc.: 62.50%] [G loss: 0.529513]\n",
      "epoch:8 step:8388 [D loss: 0.635600, acc.: 64.06%] [G loss: 0.457966]\n",
      "epoch:8 step:8389 [D loss: 0.573385, acc.: 64.84%] [G loss: 0.572324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8390 [D loss: 0.511423, acc.: 72.66%] [G loss: 0.646214]\n",
      "epoch:8 step:8391 [D loss: 0.522129, acc.: 71.88%] [G loss: 0.698139]\n",
      "epoch:8 step:8392 [D loss: 0.570105, acc.: 68.75%] [G loss: 0.631085]\n",
      "epoch:8 step:8393 [D loss: 0.512473, acc.: 67.19%] [G loss: 0.673973]\n",
      "epoch:8 step:8394 [D loss: 0.524092, acc.: 71.09%] [G loss: 0.769085]\n",
      "epoch:8 step:8395 [D loss: 0.503785, acc.: 78.12%] [G loss: 0.634323]\n",
      "epoch:8 step:8396 [D loss: 0.619549, acc.: 64.84%] [G loss: 0.691378]\n",
      "epoch:8 step:8397 [D loss: 0.513696, acc.: 72.66%] [G loss: 0.552463]\n",
      "epoch:8 step:8398 [D loss: 0.650881, acc.: 61.72%] [G loss: 0.476013]\n",
      "epoch:8 step:8399 [D loss: 0.577801, acc.: 67.19%] [G loss: 0.536568]\n",
      "epoch:8 step:8400 [D loss: 0.577043, acc.: 64.84%] [G loss: 0.487679]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.246898\n",
      "FID: 47.921974\n",
      "0 = 12.7730947627068\n",
      "1 = 0.09114459808965802\n",
      "2 = 0.9099000096321106\n",
      "3 = 0.8375999927520752\n",
      "4 = 0.982200026512146\n",
      "5 = 0.97919100522995\n",
      "6 = 0.8375999927520752\n",
      "7 = 8.263464475941634\n",
      "8 = 0.14501912023826435\n",
      "9 = 0.7522000074386597\n",
      "10 = 0.7382000088691711\n",
      "11 = 0.7662000060081482\n",
      "12 = 0.7594650387763977\n",
      "13 = 0.7382000088691711\n",
      "14 = 6.246919631958008\n",
      "15 = 7.344934940338135\n",
      "16 = 0.36442455649375916\n",
      "17 = 6.2468976974487305\n",
      "18 = 47.921974182128906\n",
      "epoch:8 step:8401 [D loss: 0.591968, acc.: 64.84%] [G loss: 0.609257]\n",
      "epoch:8 step:8402 [D loss: 0.498634, acc.: 78.91%] [G loss: 0.767907]\n",
      "epoch:8 step:8403 [D loss: 0.608444, acc.: 63.28%] [G loss: 0.610740]\n",
      "epoch:8 step:8404 [D loss: 0.620788, acc.: 62.50%] [G loss: 0.510809]\n",
      "epoch:8 step:8405 [D loss: 0.521934, acc.: 72.66%] [G loss: 0.553155]\n",
      "epoch:8 step:8406 [D loss: 0.518288, acc.: 70.31%] [G loss: 0.796323]\n",
      "epoch:8 step:8407 [D loss: 0.496566, acc.: 73.44%] [G loss: 0.730294]\n",
      "epoch:8 step:8408 [D loss: 0.484516, acc.: 78.91%] [G loss: 0.608462]\n",
      "epoch:8 step:8409 [D loss: 0.552351, acc.: 66.41%] [G loss: 0.778648]\n",
      "epoch:8 step:8410 [D loss: 0.468801, acc.: 78.12%] [G loss: 0.664651]\n",
      "epoch:8 step:8411 [D loss: 0.613821, acc.: 62.50%] [G loss: 0.597566]\n",
      "epoch:8 step:8412 [D loss: 0.523210, acc.: 74.22%] [G loss: 0.544477]\n",
      "epoch:8 step:8413 [D loss: 0.659689, acc.: 57.81%] [G loss: 0.535228]\n",
      "epoch:8 step:8414 [D loss: 0.586891, acc.: 68.75%] [G loss: 0.596351]\n",
      "epoch:8 step:8415 [D loss: 0.478336, acc.: 77.34%] [G loss: 0.775990]\n",
      "epoch:8 step:8416 [D loss: 0.671593, acc.: 56.25%] [G loss: 0.572129]\n",
      "epoch:8 step:8417 [D loss: 0.467742, acc.: 82.03%] [G loss: 0.699554]\n",
      "epoch:8 step:8418 [D loss: 0.568512, acc.: 71.09%] [G loss: 0.568645]\n",
      "epoch:8 step:8419 [D loss: 0.483326, acc.: 75.00%] [G loss: 0.622585]\n",
      "epoch:8 step:8420 [D loss: 0.421411, acc.: 81.25%] [G loss: 0.763926]\n",
      "epoch:8 step:8421 [D loss: 0.472310, acc.: 78.12%] [G loss: 0.722342]\n",
      "epoch:8 step:8422 [D loss: 0.422118, acc.: 79.69%] [G loss: 1.069063]\n",
      "epoch:8 step:8423 [D loss: 0.508627, acc.: 77.34%] [G loss: 1.198697]\n",
      "epoch:8 step:8424 [D loss: 0.685920, acc.: 63.28%] [G loss: 0.917003]\n",
      "epoch:8 step:8425 [D loss: 0.503915, acc.: 74.22%] [G loss: 0.989452]\n",
      "epoch:8 step:8426 [D loss: 0.458772, acc.: 77.34%] [G loss: 0.857602]\n",
      "epoch:8 step:8427 [D loss: 0.603104, acc.: 66.41%] [G loss: 0.823070]\n",
      "epoch:8 step:8428 [D loss: 0.612809, acc.: 63.28%] [G loss: 0.620422]\n",
      "epoch:8 step:8429 [D loss: 0.509223, acc.: 75.00%] [G loss: 0.778766]\n",
      "epoch:8 step:8430 [D loss: 0.609248, acc.: 64.06%] [G loss: 0.755009]\n",
      "epoch:8 step:8431 [D loss: 0.500911, acc.: 75.78%] [G loss: 0.786387]\n",
      "epoch:8 step:8432 [D loss: 0.385291, acc.: 78.91%] [G loss: 1.131703]\n",
      "epoch:8 step:8433 [D loss: 0.433048, acc.: 82.81%] [G loss: 1.123893]\n",
      "epoch:9 step:8434 [D loss: 0.620978, acc.: 67.97%] [G loss: 1.022611]\n",
      "epoch:9 step:8435 [D loss: 0.483397, acc.: 78.12%] [G loss: 0.828097]\n",
      "epoch:9 step:8436 [D loss: 0.563064, acc.: 71.88%] [G loss: 0.544905]\n",
      "epoch:9 step:8437 [D loss: 0.556569, acc.: 71.09%] [G loss: 0.655874]\n",
      "epoch:9 step:8438 [D loss: 0.568876, acc.: 71.88%] [G loss: 0.616505]\n",
      "epoch:9 step:8439 [D loss: 0.579567, acc.: 69.53%] [G loss: 0.616325]\n",
      "epoch:9 step:8440 [D loss: 0.492688, acc.: 78.91%] [G loss: 0.633442]\n",
      "epoch:9 step:8441 [D loss: 0.497489, acc.: 78.12%] [G loss: 0.714819]\n",
      "epoch:9 step:8442 [D loss: 0.530155, acc.: 75.00%] [G loss: 0.686980]\n",
      "epoch:9 step:8443 [D loss: 0.554321, acc.: 71.09%] [G loss: 0.572565]\n",
      "epoch:9 step:8444 [D loss: 0.483894, acc.: 77.34%] [G loss: 0.582522]\n",
      "epoch:9 step:8445 [D loss: 0.593884, acc.: 61.72%] [G loss: 0.572703]\n",
      "epoch:9 step:8446 [D loss: 0.548101, acc.: 68.75%] [G loss: 0.448648]\n",
      "epoch:9 step:8447 [D loss: 0.581432, acc.: 64.84%] [G loss: 0.452996]\n",
      "epoch:9 step:8448 [D loss: 0.489478, acc.: 75.78%] [G loss: 0.503801]\n",
      "epoch:9 step:8449 [D loss: 0.500849, acc.: 75.78%] [G loss: 0.677186]\n",
      "epoch:9 step:8450 [D loss: 0.579921, acc.: 68.75%] [G loss: 0.601383]\n",
      "epoch:9 step:8451 [D loss: 0.625918, acc.: 65.62%] [G loss: 0.561102]\n",
      "epoch:9 step:8452 [D loss: 0.595511, acc.: 67.19%] [G loss: 0.489591]\n",
      "epoch:9 step:8453 [D loss: 0.582075, acc.: 72.66%] [G loss: 0.537750]\n",
      "epoch:9 step:8454 [D loss: 0.594614, acc.: 67.97%] [G loss: 0.557317]\n",
      "epoch:9 step:8455 [D loss: 0.492955, acc.: 76.56%] [G loss: 0.848218]\n",
      "epoch:9 step:8456 [D loss: 0.521628, acc.: 71.88%] [G loss: 0.494105]\n",
      "epoch:9 step:8457 [D loss: 0.521838, acc.: 71.09%] [G loss: 0.528251]\n",
      "epoch:9 step:8458 [D loss: 0.579440, acc.: 71.88%] [G loss: 0.454358]\n",
      "epoch:9 step:8459 [D loss: 0.593101, acc.: 65.62%] [G loss: 0.484199]\n",
      "epoch:9 step:8460 [D loss: 0.526892, acc.: 71.88%] [G loss: 0.492682]\n",
      "epoch:9 step:8461 [D loss: 0.589945, acc.: 65.62%] [G loss: 0.522046]\n",
      "epoch:9 step:8462 [D loss: 0.459740, acc.: 78.91%] [G loss: 0.720019]\n",
      "epoch:9 step:8463 [D loss: 0.550670, acc.: 71.09%] [G loss: 0.536128]\n",
      "epoch:9 step:8464 [D loss: 0.677738, acc.: 62.50%] [G loss: 0.447236]\n",
      "epoch:9 step:8465 [D loss: 0.550438, acc.: 72.66%] [G loss: 0.500133]\n",
      "epoch:9 step:8466 [D loss: 0.513195, acc.: 76.56%] [G loss: 0.530350]\n",
      "epoch:9 step:8467 [D loss: 0.545025, acc.: 64.84%] [G loss: 0.482417]\n",
      "epoch:9 step:8468 [D loss: 0.560639, acc.: 69.53%] [G loss: 0.540390]\n",
      "epoch:9 step:8469 [D loss: 0.463352, acc.: 77.34%] [G loss: 0.705199]\n",
      "epoch:9 step:8470 [D loss: 0.523502, acc.: 71.88%] [G loss: 0.491810]\n",
      "epoch:9 step:8471 [D loss: 0.620262, acc.: 69.53%] [G loss: 0.495221]\n",
      "epoch:9 step:8472 [D loss: 0.550022, acc.: 70.31%] [G loss: 0.481010]\n",
      "epoch:9 step:8473 [D loss: 0.482168, acc.: 75.00%] [G loss: 0.565851]\n",
      "epoch:9 step:8474 [D loss: 0.516641, acc.: 75.78%] [G loss: 0.495957]\n",
      "epoch:9 step:8475 [D loss: 0.513581, acc.: 71.88%] [G loss: 0.560729]\n",
      "epoch:9 step:8476 [D loss: 0.521806, acc.: 76.56%] [G loss: 0.547942]\n",
      "epoch:9 step:8477 [D loss: 0.586636, acc.: 70.31%] [G loss: 0.545838]\n",
      "epoch:9 step:8478 [D loss: 0.542950, acc.: 67.19%] [G loss: 0.554338]\n",
      "epoch:9 step:8479 [D loss: 0.557074, acc.: 68.75%] [G loss: 0.534683]\n",
      "epoch:9 step:8480 [D loss: 0.576149, acc.: 67.97%] [G loss: 0.422306]\n",
      "epoch:9 step:8481 [D loss: 0.503431, acc.: 76.56%] [G loss: 0.447976]\n",
      "epoch:9 step:8482 [D loss: 0.565114, acc.: 71.09%] [G loss: 0.501908]\n",
      "epoch:9 step:8483 [D loss: 0.565701, acc.: 65.62%] [G loss: 0.559621]\n",
      "epoch:9 step:8484 [D loss: 0.668415, acc.: 56.25%] [G loss: 0.517052]\n",
      "epoch:9 step:8485 [D loss: 0.578330, acc.: 71.09%] [G loss: 0.433384]\n",
      "epoch:9 step:8486 [D loss: 0.535104, acc.: 71.09%] [G loss: 0.510980]\n",
      "epoch:9 step:8487 [D loss: 0.469425, acc.: 78.91%] [G loss: 0.546390]\n",
      "epoch:9 step:8488 [D loss: 0.527629, acc.: 72.66%] [G loss: 0.620449]\n",
      "epoch:9 step:8489 [D loss: 0.524924, acc.: 69.53%] [G loss: 0.590297]\n",
      "epoch:9 step:8490 [D loss: 0.534192, acc.: 70.31%] [G loss: 0.461836]\n",
      "epoch:9 step:8491 [D loss: 0.580553, acc.: 61.72%] [G loss: 0.689010]\n",
      "epoch:9 step:8492 [D loss: 0.518689, acc.: 74.22%] [G loss: 0.622378]\n",
      "epoch:9 step:8493 [D loss: 0.522798, acc.: 71.88%] [G loss: 0.586536]\n",
      "epoch:9 step:8494 [D loss: 0.508284, acc.: 69.53%] [G loss: 0.685799]\n",
      "epoch:9 step:8495 [D loss: 0.566286, acc.: 70.31%] [G loss: 0.502510]\n",
      "epoch:9 step:8496 [D loss: 0.604574, acc.: 61.72%] [G loss: 0.434757]\n",
      "epoch:9 step:8497 [D loss: 0.596705, acc.: 66.41%] [G loss: 0.497902]\n",
      "epoch:9 step:8498 [D loss: 0.548208, acc.: 71.09%] [G loss: 0.613194]\n",
      "epoch:9 step:8499 [D loss: 0.535806, acc.: 70.31%] [G loss: 0.447407]\n",
      "epoch:9 step:8500 [D loss: 0.574950, acc.: 70.31%] [G loss: 0.454385]\n",
      "epoch:9 step:8501 [D loss: 0.540040, acc.: 66.41%] [G loss: 0.460838]\n",
      "epoch:9 step:8502 [D loss: 0.503949, acc.: 77.34%] [G loss: 0.515354]\n",
      "epoch:9 step:8503 [D loss: 0.522844, acc.: 75.00%] [G loss: 0.611458]\n",
      "epoch:9 step:8504 [D loss: 0.565050, acc.: 72.66%] [G loss: 0.480189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8505 [D loss: 0.520477, acc.: 71.09%] [G loss: 0.455841]\n",
      "epoch:9 step:8506 [D loss: 0.582954, acc.: 67.19%] [G loss: 0.531586]\n",
      "epoch:9 step:8507 [D loss: 0.496701, acc.: 78.91%] [G loss: 0.503496]\n",
      "epoch:9 step:8508 [D loss: 0.599645, acc.: 62.50%] [G loss: 0.669606]\n",
      "epoch:9 step:8509 [D loss: 0.565003, acc.: 68.75%] [G loss: 0.698544]\n",
      "epoch:9 step:8510 [D loss: 0.492452, acc.: 75.00%] [G loss: 0.689355]\n",
      "epoch:9 step:8511 [D loss: 0.656126, acc.: 62.50%] [G loss: 0.556475]\n",
      "epoch:9 step:8512 [D loss: 0.603674, acc.: 62.50%] [G loss: 0.446737]\n",
      "epoch:9 step:8513 [D loss: 0.532307, acc.: 78.12%] [G loss: 0.558797]\n",
      "epoch:9 step:8514 [D loss: 0.570935, acc.: 70.31%] [G loss: 0.563823]\n",
      "epoch:9 step:8515 [D loss: 0.525011, acc.: 71.09%] [G loss: 0.573292]\n",
      "epoch:9 step:8516 [D loss: 0.522048, acc.: 71.88%] [G loss: 0.497835]\n",
      "epoch:9 step:8517 [D loss: 0.573265, acc.: 67.97%] [G loss: 0.468463]\n",
      "epoch:9 step:8518 [D loss: 0.544012, acc.: 70.31%] [G loss: 0.508734]\n",
      "epoch:9 step:8519 [D loss: 0.565343, acc.: 67.97%] [G loss: 0.416172]\n",
      "epoch:9 step:8520 [D loss: 0.552126, acc.: 67.97%] [G loss: 0.553847]\n",
      "epoch:9 step:8521 [D loss: 0.514609, acc.: 73.44%] [G loss: 0.473245]\n",
      "epoch:9 step:8522 [D loss: 0.571076, acc.: 69.53%] [G loss: 0.507899]\n",
      "epoch:9 step:8523 [D loss: 0.512658, acc.: 75.00%] [G loss: 0.522517]\n",
      "epoch:9 step:8524 [D loss: 0.539652, acc.: 71.88%] [G loss: 0.562769]\n",
      "epoch:9 step:8525 [D loss: 0.473555, acc.: 78.12%] [G loss: 0.594125]\n",
      "epoch:9 step:8526 [D loss: 0.516168, acc.: 77.34%] [G loss: 0.577534]\n",
      "epoch:9 step:8527 [D loss: 0.552239, acc.: 71.09%] [G loss: 0.558266]\n",
      "epoch:9 step:8528 [D loss: 0.571026, acc.: 66.41%] [G loss: 0.638871]\n",
      "epoch:9 step:8529 [D loss: 0.505968, acc.: 77.34%] [G loss: 0.667063]\n",
      "epoch:9 step:8530 [D loss: 0.520059, acc.: 72.66%] [G loss: 0.549203]\n",
      "epoch:9 step:8531 [D loss: 0.505566, acc.: 73.44%] [G loss: 0.619102]\n",
      "epoch:9 step:8532 [D loss: 0.553867, acc.: 72.66%] [G loss: 0.564615]\n",
      "epoch:9 step:8533 [D loss: 0.469798, acc.: 78.91%] [G loss: 0.689180]\n",
      "epoch:9 step:8534 [D loss: 0.493034, acc.: 73.44%] [G loss: 0.613160]\n",
      "epoch:9 step:8535 [D loss: 0.664452, acc.: 60.94%] [G loss: 0.601255]\n",
      "epoch:9 step:8536 [D loss: 0.569502, acc.: 67.19%] [G loss: 0.493207]\n",
      "epoch:9 step:8537 [D loss: 0.559811, acc.: 67.97%] [G loss: 0.513336]\n",
      "epoch:9 step:8538 [D loss: 0.576098, acc.: 61.72%] [G loss: 0.535871]\n",
      "epoch:9 step:8539 [D loss: 0.542945, acc.: 77.34%] [G loss: 0.461989]\n",
      "epoch:9 step:8540 [D loss: 0.536159, acc.: 76.56%] [G loss: 0.461531]\n",
      "epoch:9 step:8541 [D loss: 0.638075, acc.: 65.62%] [G loss: 0.419628]\n",
      "epoch:9 step:8542 [D loss: 0.598293, acc.: 68.75%] [G loss: 0.464884]\n",
      "epoch:9 step:8543 [D loss: 0.541401, acc.: 76.56%] [G loss: 0.520530]\n",
      "epoch:9 step:8544 [D loss: 0.546502, acc.: 71.88%] [G loss: 0.540418]\n",
      "epoch:9 step:8545 [D loss: 0.559636, acc.: 69.53%] [G loss: 0.487523]\n",
      "epoch:9 step:8546 [D loss: 0.581569, acc.: 71.09%] [G loss: 0.542947]\n",
      "epoch:9 step:8547 [D loss: 0.586482, acc.: 70.31%] [G loss: 0.454313]\n",
      "epoch:9 step:8548 [D loss: 0.522377, acc.: 74.22%] [G loss: 0.577504]\n",
      "epoch:9 step:8549 [D loss: 0.584018, acc.: 60.16%] [G loss: 0.495790]\n",
      "epoch:9 step:8550 [D loss: 0.564170, acc.: 67.19%] [G loss: 0.601330]\n",
      "epoch:9 step:8551 [D loss: 0.524533, acc.: 73.44%] [G loss: 0.654998]\n",
      "epoch:9 step:8552 [D loss: 0.482631, acc.: 77.34%] [G loss: 0.727019]\n",
      "epoch:9 step:8553 [D loss: 0.560772, acc.: 67.19%] [G loss: 0.598424]\n",
      "epoch:9 step:8554 [D loss: 0.580418, acc.: 70.31%] [G loss: 0.545499]\n",
      "epoch:9 step:8555 [D loss: 0.522126, acc.: 78.91%] [G loss: 0.697410]\n",
      "epoch:9 step:8556 [D loss: 0.523246, acc.: 76.56%] [G loss: 0.612144]\n",
      "epoch:9 step:8557 [D loss: 0.539561, acc.: 73.44%] [G loss: 0.510537]\n",
      "epoch:9 step:8558 [D loss: 0.588333, acc.: 66.41%] [G loss: 0.565407]\n",
      "epoch:9 step:8559 [D loss: 0.547170, acc.: 76.56%] [G loss: 0.498940]\n",
      "epoch:9 step:8560 [D loss: 0.574183, acc.: 69.53%] [G loss: 0.640041]\n",
      "epoch:9 step:8561 [D loss: 0.504123, acc.: 74.22%] [G loss: 0.533680]\n",
      "epoch:9 step:8562 [D loss: 0.622994, acc.: 64.06%] [G loss: 0.491384]\n",
      "epoch:9 step:8563 [D loss: 0.562812, acc.: 71.88%] [G loss: 0.575648]\n",
      "epoch:9 step:8564 [D loss: 0.506437, acc.: 71.09%] [G loss: 0.658687]\n",
      "epoch:9 step:8565 [D loss: 0.548221, acc.: 75.78%] [G loss: 0.625699]\n",
      "epoch:9 step:8566 [D loss: 0.616017, acc.: 64.06%] [G loss: 0.687572]\n",
      "epoch:9 step:8567 [D loss: 0.532982, acc.: 67.19%] [G loss: 0.628169]\n",
      "epoch:9 step:8568 [D loss: 0.567629, acc.: 66.41%] [G loss: 0.492355]\n",
      "epoch:9 step:8569 [D loss: 0.578404, acc.: 69.53%] [G loss: 0.613570]\n",
      "epoch:9 step:8570 [D loss: 0.623849, acc.: 62.50%] [G loss: 0.487726]\n",
      "epoch:9 step:8571 [D loss: 0.635532, acc.: 61.72%] [G loss: 0.557130]\n",
      "epoch:9 step:8572 [D loss: 0.540933, acc.: 69.53%] [G loss: 0.561807]\n",
      "epoch:9 step:8573 [D loss: 0.564847, acc.: 67.97%] [G loss: 0.609274]\n",
      "epoch:9 step:8574 [D loss: 0.547339, acc.: 65.62%] [G loss: 0.491017]\n",
      "epoch:9 step:8575 [D loss: 0.578196, acc.: 67.19%] [G loss: 0.476010]\n",
      "epoch:9 step:8576 [D loss: 0.582900, acc.: 69.53%] [G loss: 0.555684]\n",
      "epoch:9 step:8577 [D loss: 0.516492, acc.: 73.44%] [G loss: 0.543283]\n",
      "epoch:9 step:8578 [D loss: 0.566992, acc.: 65.62%] [G loss: 0.664718]\n",
      "epoch:9 step:8579 [D loss: 0.513894, acc.: 75.00%] [G loss: 0.553354]\n",
      "epoch:9 step:8580 [D loss: 0.693275, acc.: 59.38%] [G loss: 0.484414]\n",
      "epoch:9 step:8581 [D loss: 0.600578, acc.: 67.97%] [G loss: 0.494789]\n",
      "epoch:9 step:8582 [D loss: 0.523765, acc.: 72.66%] [G loss: 0.654847]\n",
      "epoch:9 step:8583 [D loss: 0.606739, acc.: 65.62%] [G loss: 0.452275]\n",
      "epoch:9 step:8584 [D loss: 0.544273, acc.: 69.53%] [G loss: 0.518520]\n",
      "epoch:9 step:8585 [D loss: 0.552014, acc.: 67.97%] [G loss: 0.563220]\n",
      "epoch:9 step:8586 [D loss: 0.576828, acc.: 67.97%] [G loss: 0.525915]\n",
      "epoch:9 step:8587 [D loss: 0.551529, acc.: 71.88%] [G loss: 0.488883]\n",
      "epoch:9 step:8588 [D loss: 0.472876, acc.: 74.22%] [G loss: 0.550409]\n",
      "epoch:9 step:8589 [D loss: 0.515898, acc.: 73.44%] [G loss: 0.646982]\n",
      "epoch:9 step:8590 [D loss: 0.581294, acc.: 67.97%] [G loss: 0.458536]\n",
      "epoch:9 step:8591 [D loss: 0.614084, acc.: 62.50%] [G loss: 0.389485]\n",
      "epoch:9 step:8592 [D loss: 0.502516, acc.: 76.56%] [G loss: 0.485604]\n",
      "epoch:9 step:8593 [D loss: 0.626501, acc.: 67.97%] [G loss: 0.529811]\n",
      "epoch:9 step:8594 [D loss: 0.576412, acc.: 71.09%] [G loss: 0.527294]\n",
      "epoch:9 step:8595 [D loss: 0.479998, acc.: 76.56%] [G loss: 0.681161]\n",
      "epoch:9 step:8596 [D loss: 0.538931, acc.: 71.09%] [G loss: 0.610212]\n",
      "epoch:9 step:8597 [D loss: 0.578305, acc.: 68.75%] [G loss: 0.592286]\n",
      "epoch:9 step:8598 [D loss: 0.498321, acc.: 72.66%] [G loss: 0.604447]\n",
      "epoch:9 step:8599 [D loss: 0.553229, acc.: 71.88%] [G loss: 0.521779]\n",
      "epoch:9 step:8600 [D loss: 0.555224, acc.: 66.41%] [G loss: 0.526686]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.436646\n",
      "FID: 42.015377\n",
      "0 = 12.778820565795879\n",
      "1 = 0.08800245164577238\n",
      "2 = 0.9049999713897705\n",
      "3 = 0.8379999995231628\n",
      "4 = 0.972000002861023\n",
      "5 = 0.9676674604415894\n",
      "6 = 0.8379999995231628\n",
      "7 = 7.991877456080925\n",
      "8 = 0.13542434474990064\n",
      "9 = 0.7383000254631042\n",
      "10 = 0.7229999899864197\n",
      "11 = 0.753600001335144\n",
      "12 = 0.7458221316337585\n",
      "13 = 0.7229999899864197\n",
      "14 = 6.436672210693359\n",
      "15 = 7.59647798538208\n",
      "16 = 0.3394922614097595\n",
      "17 = 6.436645984649658\n",
      "18 = 42.015377044677734\n",
      "epoch:9 step:8601 [D loss: 0.495622, acc.: 79.69%] [G loss: 0.603352]\n",
      "epoch:9 step:8602 [D loss: 0.609141, acc.: 61.72%] [G loss: 0.489931]\n",
      "epoch:9 step:8603 [D loss: 0.557002, acc.: 70.31%] [G loss: 0.418730]\n",
      "epoch:9 step:8604 [D loss: 0.553463, acc.: 69.53%] [G loss: 0.488395]\n",
      "epoch:9 step:8605 [D loss: 0.527701, acc.: 74.22%] [G loss: 0.432794]\n",
      "epoch:9 step:8606 [D loss: 0.517651, acc.: 76.56%] [G loss: 0.470774]\n",
      "epoch:9 step:8607 [D loss: 0.631346, acc.: 61.72%] [G loss: 0.440104]\n",
      "epoch:9 step:8608 [D loss: 0.593837, acc.: 62.50%] [G loss: 0.525031]\n",
      "epoch:9 step:8609 [D loss: 0.561868, acc.: 67.19%] [G loss: 0.374350]\n",
      "epoch:9 step:8610 [D loss: 0.566305, acc.: 71.09%] [G loss: 0.428731]\n",
      "epoch:9 step:8611 [D loss: 0.589065, acc.: 64.06%] [G loss: 0.501038]\n",
      "epoch:9 step:8612 [D loss: 0.584637, acc.: 66.41%] [G loss: 0.444882]\n",
      "epoch:9 step:8613 [D loss: 0.668078, acc.: 64.06%] [G loss: 0.455236]\n",
      "epoch:9 step:8614 [D loss: 0.581785, acc.: 71.88%] [G loss: 0.405319]\n",
      "epoch:9 step:8615 [D loss: 0.558341, acc.: 71.09%] [G loss: 0.481089]\n",
      "epoch:9 step:8616 [D loss: 0.550967, acc.: 70.31%] [G loss: 0.504920]\n",
      "epoch:9 step:8617 [D loss: 0.538546, acc.: 71.09%] [G loss: 0.501298]\n",
      "epoch:9 step:8618 [D loss: 0.552292, acc.: 72.66%] [G loss: 0.535481]\n",
      "epoch:9 step:8619 [D loss: 0.608450, acc.: 65.62%] [G loss: 0.612983]\n",
      "epoch:9 step:8620 [D loss: 0.614254, acc.: 60.94%] [G loss: 0.541950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8621 [D loss: 0.569599, acc.: 66.41%] [G loss: 0.410586]\n",
      "epoch:9 step:8622 [D loss: 0.572109, acc.: 67.97%] [G loss: 0.433791]\n",
      "epoch:9 step:8623 [D loss: 0.460116, acc.: 78.12%] [G loss: 0.603956]\n",
      "epoch:9 step:8624 [D loss: 0.545887, acc.: 71.88%] [G loss: 0.446214]\n",
      "epoch:9 step:8625 [D loss: 0.504115, acc.: 74.22%] [G loss: 0.576274]\n",
      "epoch:9 step:8626 [D loss: 0.557804, acc.: 69.53%] [G loss: 0.584708]\n",
      "epoch:9 step:8627 [D loss: 0.475214, acc.: 75.78%] [G loss: 0.531659]\n",
      "epoch:9 step:8628 [D loss: 0.605293, acc.: 68.75%] [G loss: 0.589691]\n",
      "epoch:9 step:8629 [D loss: 0.641639, acc.: 64.06%] [G loss: 0.567986]\n",
      "epoch:9 step:8630 [D loss: 0.516233, acc.: 70.31%] [G loss: 0.601255]\n",
      "epoch:9 step:8631 [D loss: 0.495800, acc.: 72.66%] [G loss: 0.520357]\n",
      "epoch:9 step:8632 [D loss: 0.537529, acc.: 72.66%] [G loss: 0.614906]\n",
      "epoch:9 step:8633 [D loss: 0.606249, acc.: 62.50%] [G loss: 0.442406]\n",
      "epoch:9 step:8634 [D loss: 0.549237, acc.: 69.53%] [G loss: 0.547626]\n",
      "epoch:9 step:8635 [D loss: 0.579073, acc.: 68.75%] [G loss: 0.508140]\n",
      "epoch:9 step:8636 [D loss: 0.638316, acc.: 63.28%] [G loss: 0.559675]\n",
      "epoch:9 step:8637 [D loss: 0.581110, acc.: 69.53%] [G loss: 0.551876]\n",
      "epoch:9 step:8638 [D loss: 0.505562, acc.: 75.78%] [G loss: 0.678411]\n",
      "epoch:9 step:8639 [D loss: 0.561698, acc.: 71.88%] [G loss: 0.610659]\n",
      "epoch:9 step:8640 [D loss: 0.492207, acc.: 76.56%] [G loss: 0.799854]\n",
      "epoch:9 step:8641 [D loss: 0.482480, acc.: 77.34%] [G loss: 0.703407]\n",
      "epoch:9 step:8642 [D loss: 0.503814, acc.: 75.00%] [G loss: 0.779962]\n",
      "epoch:9 step:8643 [D loss: 0.610775, acc.: 66.41%] [G loss: 0.521311]\n",
      "epoch:9 step:8644 [D loss: 0.613559, acc.: 64.84%] [G loss: 0.528767]\n",
      "epoch:9 step:8645 [D loss: 0.549867, acc.: 71.88%] [G loss: 0.491000]\n",
      "epoch:9 step:8646 [D loss: 0.538839, acc.: 71.09%] [G loss: 0.536276]\n",
      "epoch:9 step:8647 [D loss: 0.661663, acc.: 57.81%] [G loss: 0.371642]\n",
      "epoch:9 step:8648 [D loss: 0.596631, acc.: 66.41%] [G loss: 0.498827]\n",
      "epoch:9 step:8649 [D loss: 0.571696, acc.: 67.97%] [G loss: 0.355927]\n",
      "epoch:9 step:8650 [D loss: 0.534645, acc.: 71.09%] [G loss: 0.438763]\n",
      "epoch:9 step:8651 [D loss: 0.508871, acc.: 75.00%] [G loss: 0.677892]\n",
      "epoch:9 step:8652 [D loss: 0.517333, acc.: 75.00%] [G loss: 0.715443]\n",
      "epoch:9 step:8653 [D loss: 0.662118, acc.: 61.72%] [G loss: 0.479565]\n",
      "epoch:9 step:8654 [D loss: 0.496795, acc.: 79.69%] [G loss: 0.578999]\n",
      "epoch:9 step:8655 [D loss: 0.475337, acc.: 79.69%] [G loss: 0.705818]\n",
      "epoch:9 step:8656 [D loss: 0.515180, acc.: 72.66%] [G loss: 0.685221]\n",
      "epoch:9 step:8657 [D loss: 0.596547, acc.: 71.09%] [G loss: 0.654928]\n",
      "epoch:9 step:8658 [D loss: 0.595447, acc.: 63.28%] [G loss: 0.485841]\n",
      "epoch:9 step:8659 [D loss: 0.568774, acc.: 64.84%] [G loss: 0.463919]\n",
      "epoch:9 step:8660 [D loss: 0.557977, acc.: 71.09%] [G loss: 0.463276]\n",
      "epoch:9 step:8661 [D loss: 0.574892, acc.: 66.41%] [G loss: 0.495983]\n",
      "epoch:9 step:8662 [D loss: 0.545380, acc.: 72.66%] [G loss: 0.483612]\n",
      "epoch:9 step:8663 [D loss: 0.538990, acc.: 74.22%] [G loss: 0.650535]\n",
      "epoch:9 step:8664 [D loss: 0.524569, acc.: 75.00%] [G loss: 0.587358]\n",
      "epoch:9 step:8665 [D loss: 0.494461, acc.: 77.34%] [G loss: 0.766502]\n",
      "epoch:9 step:8666 [D loss: 0.547951, acc.: 73.44%] [G loss: 0.703458]\n",
      "epoch:9 step:8667 [D loss: 0.539485, acc.: 71.09%] [G loss: 0.587602]\n",
      "epoch:9 step:8668 [D loss: 0.536362, acc.: 67.19%] [G loss: 0.624411]\n",
      "epoch:9 step:8669 [D loss: 0.551827, acc.: 70.31%] [G loss: 0.475154]\n",
      "epoch:9 step:8670 [D loss: 0.541213, acc.: 70.31%] [G loss: 0.492666]\n",
      "epoch:9 step:8671 [D loss: 0.559107, acc.: 71.88%] [G loss: 0.417400]\n",
      "epoch:9 step:8672 [D loss: 0.521346, acc.: 75.78%] [G loss: 0.525722]\n",
      "epoch:9 step:8673 [D loss: 0.589139, acc.: 66.41%] [G loss: 0.593215]\n",
      "epoch:9 step:8674 [D loss: 0.545898, acc.: 70.31%] [G loss: 0.606803]\n",
      "epoch:9 step:8675 [D loss: 0.533642, acc.: 71.09%] [G loss: 0.534615]\n",
      "epoch:9 step:8676 [D loss: 0.541704, acc.: 66.41%] [G loss: 0.504573]\n",
      "epoch:9 step:8677 [D loss: 0.522316, acc.: 71.88%] [G loss: 0.644107]\n",
      "epoch:9 step:8678 [D loss: 0.530036, acc.: 68.75%] [G loss: 0.696321]\n",
      "epoch:9 step:8679 [D loss: 0.537448, acc.: 71.09%] [G loss: 0.645640]\n",
      "epoch:9 step:8680 [D loss: 0.588499, acc.: 62.50%] [G loss: 0.591830]\n",
      "epoch:9 step:8681 [D loss: 0.554577, acc.: 65.62%] [G loss: 0.592424]\n",
      "epoch:9 step:8682 [D loss: 0.559852, acc.: 74.22%] [G loss: 0.588243]\n",
      "epoch:9 step:8683 [D loss: 0.578699, acc.: 69.53%] [G loss: 0.609449]\n",
      "epoch:9 step:8684 [D loss: 0.628336, acc.: 66.41%] [G loss: 0.585805]\n",
      "epoch:9 step:8685 [D loss: 0.571945, acc.: 67.97%] [G loss: 0.649929]\n",
      "epoch:9 step:8686 [D loss: 0.585151, acc.: 67.19%] [G loss: 0.549329]\n",
      "epoch:9 step:8687 [D loss: 0.506640, acc.: 75.78%] [G loss: 0.593866]\n",
      "epoch:9 step:8688 [D loss: 0.567994, acc.: 66.41%] [G loss: 0.478115]\n",
      "epoch:9 step:8689 [D loss: 0.512917, acc.: 73.44%] [G loss: 0.638867]\n",
      "epoch:9 step:8690 [D loss: 0.588649, acc.: 68.75%] [G loss: 0.524706]\n",
      "epoch:9 step:8691 [D loss: 0.552050, acc.: 72.66%] [G loss: 0.453372]\n",
      "epoch:9 step:8692 [D loss: 0.538023, acc.: 71.09%] [G loss: 0.480100]\n",
      "epoch:9 step:8693 [D loss: 0.565649, acc.: 71.88%] [G loss: 0.480865]\n",
      "epoch:9 step:8694 [D loss: 0.578537, acc.: 64.84%] [G loss: 0.511382]\n",
      "epoch:9 step:8695 [D loss: 0.575280, acc.: 68.75%] [G loss: 0.499109]\n",
      "epoch:9 step:8696 [D loss: 0.628841, acc.: 67.97%] [G loss: 0.443445]\n",
      "epoch:9 step:8697 [D loss: 0.535756, acc.: 70.31%] [G loss: 0.453058]\n",
      "epoch:9 step:8698 [D loss: 0.578008, acc.: 65.62%] [G loss: 0.627302]\n",
      "epoch:9 step:8699 [D loss: 0.593627, acc.: 67.19%] [G loss: 0.484436]\n",
      "epoch:9 step:8700 [D loss: 0.550194, acc.: 69.53%] [G loss: 0.476615]\n",
      "epoch:9 step:8701 [D loss: 0.537585, acc.: 74.22%] [G loss: 0.466346]\n",
      "epoch:9 step:8702 [D loss: 0.571366, acc.: 69.53%] [G loss: 0.396638]\n",
      "epoch:9 step:8703 [D loss: 0.534613, acc.: 70.31%] [G loss: 0.471440]\n",
      "epoch:9 step:8704 [D loss: 0.538946, acc.: 71.09%] [G loss: 0.496863]\n",
      "epoch:9 step:8705 [D loss: 0.517148, acc.: 78.91%] [G loss: 0.535062]\n",
      "epoch:9 step:8706 [D loss: 0.540817, acc.: 67.97%] [G loss: 0.533381]\n",
      "epoch:9 step:8707 [D loss: 0.513301, acc.: 77.34%] [G loss: 0.712139]\n",
      "epoch:9 step:8708 [D loss: 0.610355, acc.: 65.62%] [G loss: 0.489271]\n",
      "epoch:9 step:8709 [D loss: 0.453764, acc.: 82.81%] [G loss: 0.642244]\n",
      "epoch:9 step:8710 [D loss: 0.686195, acc.: 62.50%] [G loss: 0.493607]\n",
      "epoch:9 step:8711 [D loss: 0.582830, acc.: 69.53%] [G loss: 0.417292]\n",
      "epoch:9 step:8712 [D loss: 0.578475, acc.: 65.62%] [G loss: 0.594567]\n",
      "epoch:9 step:8713 [D loss: 0.545042, acc.: 68.75%] [G loss: 0.494887]\n",
      "epoch:9 step:8714 [D loss: 0.626626, acc.: 66.41%] [G loss: 0.393902]\n",
      "epoch:9 step:8715 [D loss: 0.545074, acc.: 69.53%] [G loss: 0.441646]\n",
      "epoch:9 step:8716 [D loss: 0.557446, acc.: 73.44%] [G loss: 0.508399]\n",
      "epoch:9 step:8717 [D loss: 0.513071, acc.: 73.44%] [G loss: 0.481560]\n",
      "epoch:9 step:8718 [D loss: 0.522097, acc.: 73.44%] [G loss: 0.519899]\n",
      "epoch:9 step:8719 [D loss: 0.450382, acc.: 81.25%] [G loss: 0.666031]\n",
      "epoch:9 step:8720 [D loss: 0.582240, acc.: 67.19%] [G loss: 0.624595]\n",
      "epoch:9 step:8721 [D loss: 0.610528, acc.: 64.06%] [G loss: 0.519146]\n",
      "epoch:9 step:8722 [D loss: 0.556925, acc.: 74.22%] [G loss: 0.510911]\n",
      "epoch:9 step:8723 [D loss: 0.590460, acc.: 70.31%] [G loss: 0.514452]\n",
      "epoch:9 step:8724 [D loss: 0.564741, acc.: 67.19%] [G loss: 0.458681]\n",
      "epoch:9 step:8725 [D loss: 0.577861, acc.: 73.44%] [G loss: 0.680523]\n",
      "epoch:9 step:8726 [D loss: 0.521085, acc.: 69.53%] [G loss: 0.483149]\n",
      "epoch:9 step:8727 [D loss: 0.626193, acc.: 58.59%] [G loss: 0.386838]\n",
      "epoch:9 step:8728 [D loss: 0.577207, acc.: 67.19%] [G loss: 0.523933]\n",
      "epoch:9 step:8729 [D loss: 0.511694, acc.: 75.00%] [G loss: 0.549997]\n",
      "epoch:9 step:8730 [D loss: 0.568294, acc.: 70.31%] [G loss: 0.562147]\n",
      "epoch:9 step:8731 [D loss: 0.510006, acc.: 75.78%] [G loss: 0.564014]\n",
      "epoch:9 step:8732 [D loss: 0.510989, acc.: 75.78%] [G loss: 0.528974]\n",
      "epoch:9 step:8733 [D loss: 0.489315, acc.: 75.00%] [G loss: 0.621490]\n",
      "epoch:9 step:8734 [D loss: 0.672581, acc.: 59.38%] [G loss: 0.467251]\n",
      "epoch:9 step:8735 [D loss: 0.529302, acc.: 68.75%] [G loss: 0.520953]\n",
      "epoch:9 step:8736 [D loss: 0.595399, acc.: 68.75%] [G loss: 0.561865]\n",
      "epoch:9 step:8737 [D loss: 0.497378, acc.: 72.66%] [G loss: 0.502198]\n",
      "epoch:9 step:8738 [D loss: 0.535118, acc.: 71.88%] [G loss: 0.540181]\n",
      "epoch:9 step:8739 [D loss: 0.539624, acc.: 67.19%] [G loss: 0.497549]\n",
      "epoch:9 step:8740 [D loss: 0.540850, acc.: 69.53%] [G loss: 0.517085]\n",
      "epoch:9 step:8741 [D loss: 0.576410, acc.: 64.84%] [G loss: 0.567523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8742 [D loss: 0.530108, acc.: 71.09%] [G loss: 0.562113]\n",
      "epoch:9 step:8743 [D loss: 0.536659, acc.: 75.00%] [G loss: 0.616405]\n",
      "epoch:9 step:8744 [D loss: 0.516376, acc.: 66.41%] [G loss: 0.555919]\n",
      "epoch:9 step:8745 [D loss: 0.498497, acc.: 77.34%] [G loss: 0.564873]\n",
      "epoch:9 step:8746 [D loss: 0.510827, acc.: 73.44%] [G loss: 0.831637]\n",
      "epoch:9 step:8747 [D loss: 0.448110, acc.: 79.69%] [G loss: 0.710280]\n",
      "epoch:9 step:8748 [D loss: 0.465412, acc.: 81.25%] [G loss: 0.731106]\n",
      "epoch:9 step:8749 [D loss: 0.674142, acc.: 62.50%] [G loss: 0.464453]\n",
      "epoch:9 step:8750 [D loss: 0.591831, acc.: 69.53%] [G loss: 0.487920]\n",
      "epoch:9 step:8751 [D loss: 0.506738, acc.: 76.56%] [G loss: 0.443612]\n",
      "epoch:9 step:8752 [D loss: 0.596125, acc.: 66.41%] [G loss: 0.484456]\n",
      "epoch:9 step:8753 [D loss: 0.583835, acc.: 64.06%] [G loss: 0.421308]\n",
      "epoch:9 step:8754 [D loss: 0.493199, acc.: 78.12%] [G loss: 0.598574]\n",
      "epoch:9 step:8755 [D loss: 0.554873, acc.: 67.97%] [G loss: 0.535374]\n",
      "epoch:9 step:8756 [D loss: 0.599046, acc.: 66.41%] [G loss: 0.482444]\n",
      "epoch:9 step:8757 [D loss: 0.575168, acc.: 69.53%] [G loss: 0.536779]\n",
      "epoch:9 step:8758 [D loss: 0.538693, acc.: 74.22%] [G loss: 0.501109]\n",
      "epoch:9 step:8759 [D loss: 0.470566, acc.: 78.91%] [G loss: 0.631096]\n",
      "epoch:9 step:8760 [D loss: 0.525072, acc.: 73.44%] [G loss: 0.692599]\n",
      "epoch:9 step:8761 [D loss: 0.536069, acc.: 71.09%] [G loss: 0.802229]\n",
      "epoch:9 step:8762 [D loss: 0.507951, acc.: 74.22%] [G loss: 0.619781]\n",
      "epoch:9 step:8763 [D loss: 0.538294, acc.: 68.75%] [G loss: 0.598991]\n",
      "epoch:9 step:8764 [D loss: 0.585132, acc.: 63.28%] [G loss: 0.527149]\n",
      "epoch:9 step:8765 [D loss: 0.537443, acc.: 71.09%] [G loss: 0.500612]\n",
      "epoch:9 step:8766 [D loss: 0.538309, acc.: 73.44%] [G loss: 0.655022]\n",
      "epoch:9 step:8767 [D loss: 0.584718, acc.: 69.53%] [G loss: 0.637430]\n",
      "epoch:9 step:8768 [D loss: 0.567157, acc.: 69.53%] [G loss: 0.637800]\n",
      "epoch:9 step:8769 [D loss: 0.545300, acc.: 66.41%] [G loss: 0.610574]\n",
      "epoch:9 step:8770 [D loss: 0.501365, acc.: 74.22%] [G loss: 0.696851]\n",
      "epoch:9 step:8771 [D loss: 0.586333, acc.: 65.62%] [G loss: 0.634861]\n",
      "epoch:9 step:8772 [D loss: 0.521344, acc.: 74.22%] [G loss: 0.651841]\n",
      "epoch:9 step:8773 [D loss: 0.486267, acc.: 79.69%] [G loss: 0.680786]\n",
      "epoch:9 step:8774 [D loss: 0.591455, acc.: 74.22%] [G loss: 0.732864]\n",
      "epoch:9 step:8775 [D loss: 0.688501, acc.: 55.47%] [G loss: 0.463281]\n",
      "epoch:9 step:8776 [D loss: 0.507956, acc.: 76.56%] [G loss: 0.550091]\n",
      "epoch:9 step:8777 [D loss: 0.488547, acc.: 76.56%] [G loss: 0.674993]\n",
      "epoch:9 step:8778 [D loss: 0.563336, acc.: 68.75%] [G loss: 0.632606]\n",
      "epoch:9 step:8779 [D loss: 0.532171, acc.: 74.22%] [G loss: 0.663490]\n",
      "epoch:9 step:8780 [D loss: 0.427207, acc.: 83.59%] [G loss: 1.011046]\n",
      "epoch:9 step:8781 [D loss: 0.648144, acc.: 63.28%] [G loss: 0.618142]\n",
      "epoch:9 step:8782 [D loss: 0.680418, acc.: 56.25%] [G loss: 0.339313]\n",
      "epoch:9 step:8783 [D loss: 0.544302, acc.: 75.78%] [G loss: 0.466755]\n",
      "epoch:9 step:8784 [D loss: 0.560357, acc.: 71.88%] [G loss: 0.512108]\n",
      "epoch:9 step:8785 [D loss: 0.539508, acc.: 71.88%] [G loss: 0.639870]\n",
      "epoch:9 step:8786 [D loss: 0.562098, acc.: 71.88%] [G loss: 0.690823]\n",
      "epoch:9 step:8787 [D loss: 0.414968, acc.: 83.59%] [G loss: 0.649267]\n",
      "epoch:9 step:8788 [D loss: 0.604669, acc.: 64.84%] [G loss: 0.700371]\n",
      "epoch:9 step:8789 [D loss: 0.558910, acc.: 71.09%] [G loss: 0.605185]\n",
      "epoch:9 step:8790 [D loss: 0.509567, acc.: 71.88%] [G loss: 0.586125]\n",
      "epoch:9 step:8791 [D loss: 0.463040, acc.: 78.91%] [G loss: 0.660847]\n",
      "epoch:9 step:8792 [D loss: 0.471000, acc.: 79.69%] [G loss: 0.641649]\n",
      "epoch:9 step:8793 [D loss: 0.505388, acc.: 71.88%] [G loss: 0.631383]\n",
      "epoch:9 step:8794 [D loss: 0.509117, acc.: 75.78%] [G loss: 0.675678]\n",
      "epoch:9 step:8795 [D loss: 0.667554, acc.: 61.72%] [G loss: 0.529150]\n",
      "epoch:9 step:8796 [D loss: 0.523080, acc.: 73.44%] [G loss: 0.533158]\n",
      "epoch:9 step:8797 [D loss: 0.503417, acc.: 75.00%] [G loss: 0.476904]\n",
      "epoch:9 step:8798 [D loss: 0.589698, acc.: 65.62%] [G loss: 0.720687]\n",
      "epoch:9 step:8799 [D loss: 0.538416, acc.: 70.31%] [G loss: 0.583711]\n",
      "epoch:9 step:8800 [D loss: 0.567648, acc.: 66.41%] [G loss: 0.537337]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.386132\n",
      "FID: 46.414860\n",
      "0 = 12.890432709312417\n",
      "1 = 0.09321760961977676\n",
      "2 = 0.9196000099182129\n",
      "3 = 0.8619999885559082\n",
      "4 = 0.9771999716758728\n",
      "5 = 0.974231481552124\n",
      "6 = 0.8619999885559082\n",
      "7 = 8.106266986274713\n",
      "8 = 0.14323961624405157\n",
      "9 = 0.7468000054359436\n",
      "10 = 0.7314000129699707\n",
      "11 = 0.7621999979019165\n",
      "12 = 0.7546430230140686\n",
      "13 = 0.7314000129699707\n",
      "14 = 6.3861589431762695\n",
      "15 = 7.466821670532227\n",
      "16 = 0.35301414132118225\n",
      "17 = 6.386131763458252\n",
      "18 = 46.414859771728516\n",
      "epoch:9 step:8801 [D loss: 0.571084, acc.: 69.53%] [G loss: 0.496351]\n",
      "epoch:9 step:8802 [D loss: 0.558060, acc.: 69.53%] [G loss: 0.504917]\n",
      "epoch:9 step:8803 [D loss: 0.591475, acc.: 71.09%] [G loss: 0.478980]\n",
      "epoch:9 step:8804 [D loss: 0.560356, acc.: 71.88%] [G loss: 0.698090]\n",
      "epoch:9 step:8805 [D loss: 0.585670, acc.: 60.94%] [G loss: 0.534433]\n",
      "epoch:9 step:8806 [D loss: 0.593747, acc.: 64.06%] [G loss: 0.470522]\n",
      "epoch:9 step:8807 [D loss: 0.479480, acc.: 72.66%] [G loss: 0.666494]\n",
      "epoch:9 step:8808 [D loss: 0.562465, acc.: 68.75%] [G loss: 0.635871]\n",
      "epoch:9 step:8809 [D loss: 0.651532, acc.: 57.03%] [G loss: 0.467037]\n",
      "epoch:9 step:8810 [D loss: 0.659221, acc.: 62.50%] [G loss: 0.540653]\n",
      "epoch:9 step:8811 [D loss: 0.540623, acc.: 67.97%] [G loss: 0.516603]\n",
      "epoch:9 step:8812 [D loss: 0.563300, acc.: 68.75%] [G loss: 0.456383]\n",
      "epoch:9 step:8813 [D loss: 0.542504, acc.: 72.66%] [G loss: 0.474358]\n",
      "epoch:9 step:8814 [D loss: 0.472006, acc.: 76.56%] [G loss: 0.450832]\n",
      "epoch:9 step:8815 [D loss: 0.520005, acc.: 74.22%] [G loss: 0.606611]\n",
      "epoch:9 step:8816 [D loss: 0.549702, acc.: 70.31%] [G loss: 0.634047]\n",
      "epoch:9 step:8817 [D loss: 0.590513, acc.: 63.28%] [G loss: 0.562662]\n",
      "epoch:9 step:8818 [D loss: 0.481592, acc.: 78.12%] [G loss: 0.582058]\n",
      "epoch:9 step:8819 [D loss: 0.658401, acc.: 62.50%] [G loss: 0.464780]\n",
      "epoch:9 step:8820 [D loss: 0.564300, acc.: 64.84%] [G loss: 0.516807]\n",
      "epoch:9 step:8821 [D loss: 0.529917, acc.: 75.78%] [G loss: 0.672504]\n",
      "epoch:9 step:8822 [D loss: 0.539602, acc.: 69.53%] [G loss: 0.579606]\n",
      "epoch:9 step:8823 [D loss: 0.636874, acc.: 63.28%] [G loss: 0.494183]\n",
      "epoch:9 step:8824 [D loss: 0.596777, acc.: 64.06%] [G loss: 0.491716]\n",
      "epoch:9 step:8825 [D loss: 0.472889, acc.: 75.78%] [G loss: 0.576735]\n",
      "epoch:9 step:8826 [D loss: 0.594764, acc.: 64.84%] [G loss: 0.491545]\n",
      "epoch:9 step:8827 [D loss: 0.574551, acc.: 67.97%] [G loss: 0.569818]\n",
      "epoch:9 step:8828 [D loss: 0.577054, acc.: 67.19%] [G loss: 0.476315]\n",
      "epoch:9 step:8829 [D loss: 0.584995, acc.: 64.84%] [G loss: 0.639386]\n",
      "epoch:9 step:8830 [D loss: 0.550439, acc.: 70.31%] [G loss: 0.555316]\n",
      "epoch:9 step:8831 [D loss: 0.509363, acc.: 70.31%] [G loss: 0.574916]\n",
      "epoch:9 step:8832 [D loss: 0.494163, acc.: 77.34%] [G loss: 0.614682]\n",
      "epoch:9 step:8833 [D loss: 0.674654, acc.: 53.12%] [G loss: 0.608520]\n",
      "epoch:9 step:8834 [D loss: 0.624223, acc.: 60.16%] [G loss: 0.384585]\n",
      "epoch:9 step:8835 [D loss: 0.538424, acc.: 77.34%] [G loss: 0.449407]\n",
      "epoch:9 step:8836 [D loss: 0.530219, acc.: 75.78%] [G loss: 0.526045]\n",
      "epoch:9 step:8837 [D loss: 0.620081, acc.: 63.28%] [G loss: 0.517385]\n",
      "epoch:9 step:8838 [D loss: 0.582757, acc.: 72.66%] [G loss: 0.535324]\n",
      "epoch:9 step:8839 [D loss: 0.531057, acc.: 73.44%] [G loss: 0.493188]\n",
      "epoch:9 step:8840 [D loss: 0.554654, acc.: 72.66%] [G loss: 0.562873]\n",
      "epoch:9 step:8841 [D loss: 0.595473, acc.: 65.62%] [G loss: 0.424820]\n",
      "epoch:9 step:8842 [D loss: 0.561284, acc.: 69.53%] [G loss: 0.512583]\n",
      "epoch:9 step:8843 [D loss: 0.563467, acc.: 66.41%] [G loss: 0.594168]\n",
      "epoch:9 step:8844 [D loss: 0.541688, acc.: 69.53%] [G loss: 0.455755]\n",
      "epoch:9 step:8845 [D loss: 0.619141, acc.: 66.41%] [G loss: 0.404435]\n",
      "epoch:9 step:8846 [D loss: 0.573776, acc.: 70.31%] [G loss: 0.433469]\n",
      "epoch:9 step:8847 [D loss: 0.521027, acc.: 72.66%] [G loss: 0.596708]\n",
      "epoch:9 step:8848 [D loss: 0.555355, acc.: 69.53%] [G loss: 0.517230]\n",
      "epoch:9 step:8849 [D loss: 0.520930, acc.: 75.78%] [G loss: 0.608535]\n",
      "epoch:9 step:8850 [D loss: 0.561870, acc.: 67.19%] [G loss: 0.603353]\n",
      "epoch:9 step:8851 [D loss: 0.676030, acc.: 58.59%] [G loss: 0.521358]\n",
      "epoch:9 step:8852 [D loss: 0.581911, acc.: 66.41%] [G loss: 0.534501]\n",
      "epoch:9 step:8853 [D loss: 0.615726, acc.: 59.38%] [G loss: 0.694194]\n",
      "epoch:9 step:8854 [D loss: 0.588583, acc.: 68.75%] [G loss: 0.596548]\n",
      "epoch:9 step:8855 [D loss: 0.580698, acc.: 69.53%] [G loss: 0.482826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8856 [D loss: 0.605903, acc.: 65.62%] [G loss: 0.415732]\n",
      "epoch:9 step:8857 [D loss: 0.594905, acc.: 61.72%] [G loss: 0.446129]\n",
      "epoch:9 step:8858 [D loss: 0.525724, acc.: 71.88%] [G loss: 0.508484]\n",
      "epoch:9 step:8859 [D loss: 0.487183, acc.: 74.22%] [G loss: 0.553320]\n",
      "epoch:9 step:8860 [D loss: 0.491643, acc.: 75.00%] [G loss: 0.675475]\n",
      "epoch:9 step:8861 [D loss: 0.545111, acc.: 71.88%] [G loss: 0.686530]\n",
      "epoch:9 step:8862 [D loss: 0.467357, acc.: 75.78%] [G loss: 0.786665]\n",
      "epoch:9 step:8863 [D loss: 0.553476, acc.: 71.09%] [G loss: 0.633730]\n",
      "epoch:9 step:8864 [D loss: 0.507898, acc.: 73.44%] [G loss: 0.698419]\n",
      "epoch:9 step:8865 [D loss: 0.533164, acc.: 72.66%] [G loss: 0.644050]\n",
      "epoch:9 step:8866 [D loss: 0.600308, acc.: 69.53%] [G loss: 0.604530]\n",
      "epoch:9 step:8867 [D loss: 0.565530, acc.: 75.78%] [G loss: 0.564729]\n",
      "epoch:9 step:8868 [D loss: 0.576639, acc.: 73.44%] [G loss: 0.472209]\n",
      "epoch:9 step:8869 [D loss: 0.503315, acc.: 72.66%] [G loss: 0.586057]\n",
      "epoch:9 step:8870 [D loss: 0.628824, acc.: 67.19%] [G loss: 0.557984]\n",
      "epoch:9 step:8871 [D loss: 0.569633, acc.: 69.53%] [G loss: 0.476440]\n",
      "epoch:9 step:8872 [D loss: 0.505129, acc.: 72.66%] [G loss: 0.518262]\n",
      "epoch:9 step:8873 [D loss: 0.560751, acc.: 65.62%] [G loss: 0.623844]\n",
      "epoch:9 step:8874 [D loss: 0.573125, acc.: 66.41%] [G loss: 0.666566]\n",
      "epoch:9 step:8875 [D loss: 0.548907, acc.: 71.09%] [G loss: 0.636055]\n",
      "epoch:9 step:8876 [D loss: 0.513385, acc.: 73.44%] [G loss: 0.545461]\n",
      "epoch:9 step:8877 [D loss: 0.538390, acc.: 73.44%] [G loss: 0.495474]\n",
      "epoch:9 step:8878 [D loss: 0.545495, acc.: 69.53%] [G loss: 0.558059]\n",
      "epoch:9 step:8879 [D loss: 0.485317, acc.: 78.12%] [G loss: 0.660434]\n",
      "epoch:9 step:8880 [D loss: 0.536707, acc.: 68.75%] [G loss: 0.730699]\n",
      "epoch:9 step:8881 [D loss: 0.596233, acc.: 63.28%] [G loss: 0.643844]\n",
      "epoch:9 step:8882 [D loss: 0.493124, acc.: 76.56%] [G loss: 0.573988]\n",
      "epoch:9 step:8883 [D loss: 0.479443, acc.: 78.12%] [G loss: 0.663265]\n",
      "epoch:9 step:8884 [D loss: 0.425018, acc.: 80.47%] [G loss: 0.924879]\n",
      "epoch:9 step:8885 [D loss: 0.544819, acc.: 72.66%] [G loss: 0.629020]\n",
      "epoch:9 step:8886 [D loss: 0.492982, acc.: 78.12%] [G loss: 0.670189]\n",
      "epoch:9 step:8887 [D loss: 0.601739, acc.: 68.75%] [G loss: 0.540446]\n",
      "epoch:9 step:8888 [D loss: 0.530557, acc.: 72.66%] [G loss: 0.587259]\n",
      "epoch:9 step:8889 [D loss: 0.669321, acc.: 58.59%] [G loss: 0.542236]\n",
      "epoch:9 step:8890 [D loss: 0.518147, acc.: 74.22%] [G loss: 0.524335]\n",
      "epoch:9 step:8891 [D loss: 0.604458, acc.: 67.19%] [G loss: 0.503865]\n",
      "epoch:9 step:8892 [D loss: 0.552761, acc.: 66.41%] [G loss: 0.624621]\n",
      "epoch:9 step:8893 [D loss: 0.522744, acc.: 72.66%] [G loss: 0.714100]\n",
      "epoch:9 step:8894 [D loss: 0.544739, acc.: 71.88%] [G loss: 0.578926]\n",
      "epoch:9 step:8895 [D loss: 0.666533, acc.: 57.81%] [G loss: 0.385071]\n",
      "epoch:9 step:8896 [D loss: 0.580602, acc.: 64.06%] [G loss: 0.508314]\n",
      "epoch:9 step:8897 [D loss: 0.557068, acc.: 71.88%] [G loss: 0.482708]\n",
      "epoch:9 step:8898 [D loss: 0.595203, acc.: 67.19%] [G loss: 0.482029]\n",
      "epoch:9 step:8899 [D loss: 0.556611, acc.: 60.94%] [G loss: 0.589167]\n",
      "epoch:9 step:8900 [D loss: 0.537458, acc.: 74.22%] [G loss: 0.649851]\n",
      "epoch:9 step:8901 [D loss: 0.572108, acc.: 68.75%] [G loss: 0.543887]\n",
      "epoch:9 step:8902 [D loss: 0.536910, acc.: 71.88%] [G loss: 0.526730]\n",
      "epoch:9 step:8903 [D loss: 0.530957, acc.: 74.22%] [G loss: 0.553639]\n",
      "epoch:9 step:8904 [D loss: 0.419884, acc.: 86.72%] [G loss: 0.753305]\n",
      "epoch:9 step:8905 [D loss: 0.483366, acc.: 80.47%] [G loss: 0.850496]\n",
      "epoch:9 step:8906 [D loss: 0.621834, acc.: 64.06%] [G loss: 0.595950]\n",
      "epoch:9 step:8907 [D loss: 0.578059, acc.: 69.53%] [G loss: 0.617515]\n",
      "epoch:9 step:8908 [D loss: 0.503333, acc.: 74.22%] [G loss: 0.661215]\n",
      "epoch:9 step:8909 [D loss: 0.550180, acc.: 75.78%] [G loss: 0.609095]\n",
      "epoch:9 step:8910 [D loss: 0.679397, acc.: 60.94%] [G loss: 0.469890]\n",
      "epoch:9 step:8911 [D loss: 0.589100, acc.: 60.16%] [G loss: 0.390744]\n",
      "epoch:9 step:8912 [D loss: 0.546798, acc.: 71.88%] [G loss: 0.410569]\n",
      "epoch:9 step:8913 [D loss: 0.597163, acc.: 68.75%] [G loss: 0.487264]\n",
      "epoch:9 step:8914 [D loss: 0.528810, acc.: 75.00%] [G loss: 0.524053]\n",
      "epoch:9 step:8915 [D loss: 0.668158, acc.: 57.03%] [G loss: 0.451228]\n",
      "epoch:9 step:8916 [D loss: 0.573758, acc.: 66.41%] [G loss: 0.590609]\n",
      "epoch:9 step:8917 [D loss: 0.551394, acc.: 71.88%] [G loss: 0.510754]\n",
      "epoch:9 step:8918 [D loss: 0.526060, acc.: 71.09%] [G loss: 0.645589]\n",
      "epoch:9 step:8919 [D loss: 0.592286, acc.: 66.41%] [G loss: 0.685088]\n",
      "epoch:9 step:8920 [D loss: 0.569659, acc.: 67.19%] [G loss: 0.452883]\n",
      "epoch:9 step:8921 [D loss: 0.504408, acc.: 72.66%] [G loss: 0.514249]\n",
      "epoch:9 step:8922 [D loss: 0.551163, acc.: 71.88%] [G loss: 0.521891]\n",
      "epoch:9 step:8923 [D loss: 0.531269, acc.: 79.69%] [G loss: 0.544215]\n",
      "epoch:9 step:8924 [D loss: 0.597978, acc.: 64.84%] [G loss: 0.618699]\n",
      "epoch:9 step:8925 [D loss: 0.613836, acc.: 68.75%] [G loss: 0.554275]\n",
      "epoch:9 step:8926 [D loss: 0.609451, acc.: 66.41%] [G loss: 0.533579]\n",
      "epoch:9 step:8927 [D loss: 0.603330, acc.: 67.19%] [G loss: 0.471699]\n",
      "epoch:9 step:8928 [D loss: 0.535119, acc.: 71.09%] [G loss: 0.524206]\n",
      "epoch:9 step:8929 [D loss: 0.580796, acc.: 69.53%] [G loss: 0.523097]\n",
      "epoch:9 step:8930 [D loss: 0.547415, acc.: 73.44%] [G loss: 0.554141]\n",
      "epoch:9 step:8931 [D loss: 0.527272, acc.: 71.88%] [G loss: 0.550289]\n",
      "epoch:9 step:8932 [D loss: 0.515483, acc.: 72.66%] [G loss: 0.556980]\n",
      "epoch:9 step:8933 [D loss: 0.622438, acc.: 64.84%] [G loss: 0.597714]\n",
      "epoch:9 step:8934 [D loss: 0.667691, acc.: 65.62%] [G loss: 0.422936]\n",
      "epoch:9 step:8935 [D loss: 0.596128, acc.: 67.97%] [G loss: 0.381826]\n",
      "epoch:9 step:8936 [D loss: 0.501988, acc.: 74.22%] [G loss: 0.449538]\n",
      "epoch:9 step:8937 [D loss: 0.511202, acc.: 75.00%] [G loss: 0.560259]\n",
      "epoch:9 step:8938 [D loss: 0.543544, acc.: 74.22%] [G loss: 0.552804]\n",
      "epoch:9 step:8939 [D loss: 0.496768, acc.: 75.00%] [G loss: 0.649498]\n",
      "epoch:9 step:8940 [D loss: 0.524506, acc.: 74.22%] [G loss: 0.592305]\n",
      "epoch:9 step:8941 [D loss: 0.483285, acc.: 80.47%] [G loss: 0.869933]\n",
      "epoch:9 step:8942 [D loss: 0.504014, acc.: 71.09%] [G loss: 0.755404]\n",
      "epoch:9 step:8943 [D loss: 0.632505, acc.: 64.06%] [G loss: 0.581813]\n",
      "epoch:9 step:8944 [D loss: 0.693153, acc.: 53.12%] [G loss: 0.356713]\n",
      "epoch:9 step:8945 [D loss: 0.572973, acc.: 70.31%] [G loss: 0.464533]\n",
      "epoch:9 step:8946 [D loss: 0.510200, acc.: 72.66%] [G loss: 0.452846]\n",
      "epoch:9 step:8947 [D loss: 0.524994, acc.: 72.66%] [G loss: 0.466445]\n",
      "epoch:9 step:8948 [D loss: 0.559445, acc.: 65.62%] [G loss: 0.578048]\n",
      "epoch:9 step:8949 [D loss: 0.473642, acc.: 76.56%] [G loss: 0.629696]\n",
      "epoch:9 step:8950 [D loss: 0.545333, acc.: 72.66%] [G loss: 0.569458]\n",
      "epoch:9 step:8951 [D loss: 0.536497, acc.: 71.09%] [G loss: 0.619901]\n",
      "epoch:9 step:8952 [D loss: 0.515992, acc.: 71.88%] [G loss: 0.548030]\n",
      "epoch:9 step:8953 [D loss: 0.493093, acc.: 78.12%] [G loss: 0.535562]\n",
      "epoch:9 step:8954 [D loss: 0.524182, acc.: 76.56%] [G loss: 0.526881]\n",
      "epoch:9 step:8955 [D loss: 0.521253, acc.: 74.22%] [G loss: 0.556819]\n",
      "epoch:9 step:8956 [D loss: 0.501225, acc.: 77.34%] [G loss: 0.679115]\n",
      "epoch:9 step:8957 [D loss: 0.572266, acc.: 72.66%] [G loss: 0.511432]\n",
      "epoch:9 step:8958 [D loss: 0.616180, acc.: 64.06%] [G loss: 0.511081]\n",
      "epoch:9 step:8959 [D loss: 0.606512, acc.: 60.94%] [G loss: 0.459019]\n",
      "epoch:9 step:8960 [D loss: 0.564699, acc.: 71.09%] [G loss: 0.585094]\n",
      "epoch:9 step:8961 [D loss: 0.695854, acc.: 59.38%] [G loss: 0.541529]\n",
      "epoch:9 step:8962 [D loss: 0.573832, acc.: 71.88%] [G loss: 0.435742]\n",
      "epoch:9 step:8963 [D loss: 0.529087, acc.: 72.66%] [G loss: 0.513308]\n",
      "epoch:9 step:8964 [D loss: 0.601072, acc.: 65.62%] [G loss: 0.458513]\n",
      "epoch:9 step:8965 [D loss: 0.529610, acc.: 74.22%] [G loss: 0.490432]\n",
      "epoch:9 step:8966 [D loss: 0.565180, acc.: 67.97%] [G loss: 0.551005]\n",
      "epoch:9 step:8967 [D loss: 0.495829, acc.: 78.12%] [G loss: 0.543832]\n",
      "epoch:9 step:8968 [D loss: 0.650451, acc.: 59.38%] [G loss: 0.462541]\n",
      "epoch:9 step:8969 [D loss: 0.507328, acc.: 75.00%] [G loss: 0.491512]\n",
      "epoch:9 step:8970 [D loss: 0.594711, acc.: 68.75%] [G loss: 0.425259]\n",
      "epoch:9 step:8971 [D loss: 0.607176, acc.: 61.72%] [G loss: 0.390420]\n",
      "epoch:9 step:8972 [D loss: 0.524473, acc.: 75.00%] [G loss: 0.525838]\n",
      "epoch:9 step:8973 [D loss: 0.590346, acc.: 68.75%] [G loss: 0.554720]\n",
      "epoch:9 step:8974 [D loss: 0.511807, acc.: 75.00%] [G loss: 0.604341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8975 [D loss: 0.559984, acc.: 70.31%] [G loss: 0.451724]\n",
      "epoch:9 step:8976 [D loss: 0.592542, acc.: 65.62%] [G loss: 0.538149]\n",
      "epoch:9 step:8977 [D loss: 0.576294, acc.: 67.19%] [G loss: 0.550759]\n",
      "epoch:9 step:8978 [D loss: 0.535158, acc.: 71.09%] [G loss: 0.674115]\n",
      "epoch:9 step:8979 [D loss: 0.525351, acc.: 75.00%] [G loss: 0.551048]\n",
      "epoch:9 step:8980 [D loss: 0.545234, acc.: 72.66%] [G loss: 0.646582]\n",
      "epoch:9 step:8981 [D loss: 0.562902, acc.: 70.31%] [G loss: 0.575187]\n",
      "epoch:9 step:8982 [D loss: 0.567852, acc.: 69.53%] [G loss: 0.538716]\n",
      "epoch:9 step:8983 [D loss: 0.535380, acc.: 73.44%] [G loss: 0.536517]\n",
      "epoch:9 step:8984 [D loss: 0.582959, acc.: 63.28%] [G loss: 0.506008]\n",
      "epoch:9 step:8985 [D loss: 0.505934, acc.: 75.78%] [G loss: 0.611282]\n",
      "epoch:9 step:8986 [D loss: 0.596099, acc.: 67.97%] [G loss: 0.518429]\n",
      "epoch:9 step:8987 [D loss: 0.459750, acc.: 77.34%] [G loss: 0.606984]\n",
      "epoch:9 step:8988 [D loss: 0.492189, acc.: 75.00%] [G loss: 0.615493]\n",
      "epoch:9 step:8989 [D loss: 0.518629, acc.: 71.88%] [G loss: 0.738383]\n",
      "epoch:9 step:8990 [D loss: 0.529223, acc.: 71.88%] [G loss: 0.565291]\n",
      "epoch:9 step:8991 [D loss: 0.512087, acc.: 71.88%] [G loss: 0.544168]\n",
      "epoch:9 step:8992 [D loss: 0.582929, acc.: 64.06%] [G loss: 0.529641]\n",
      "epoch:9 step:8993 [D loss: 0.608725, acc.: 64.06%] [G loss: 0.414991]\n",
      "epoch:9 step:8994 [D loss: 0.575180, acc.: 72.66%] [G loss: 0.540343]\n",
      "epoch:9 step:8995 [D loss: 0.584063, acc.: 69.53%] [G loss: 0.591746]\n",
      "epoch:9 step:8996 [D loss: 0.593845, acc.: 67.97%] [G loss: 0.473264]\n",
      "epoch:9 step:8997 [D loss: 0.555248, acc.: 65.62%] [G loss: 0.525553]\n",
      "epoch:9 step:8998 [D loss: 0.572919, acc.: 72.66%] [G loss: 0.610275]\n",
      "epoch:9 step:8999 [D loss: 0.699771, acc.: 56.25%] [G loss: 0.443841]\n",
      "epoch:9 step:9000 [D loss: 0.541735, acc.: 75.78%] [G loss: 0.437044]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.269304\n",
      "FID: 45.476624\n",
      "0 = 12.997840588092801\n",
      "1 = 0.08570900798625423\n",
      "2 = 0.9135000109672546\n",
      "3 = 0.8546000123023987\n",
      "4 = 0.9724000096321106\n",
      "5 = 0.9687145948410034\n",
      "6 = 0.8546000123023987\n",
      "7 = 8.206498425722133\n",
      "8 = 0.14077665840138123\n",
      "9 = 0.7526999711990356\n",
      "10 = 0.734000027179718\n",
      "11 = 0.771399974822998\n",
      "12 = 0.7625181674957275\n",
      "13 = 0.734000027179718\n",
      "14 = 6.269328594207764\n",
      "15 = 7.310634613037109\n",
      "16 = 0.3601200580596924\n",
      "17 = 6.269304275512695\n",
      "18 = 45.47662353515625\n",
      "epoch:9 step:9001 [D loss: 0.587690, acc.: 68.75%] [G loss: 0.565124]\n",
      "epoch:9 step:9002 [D loss: 0.544918, acc.: 70.31%] [G loss: 0.522092]\n",
      "epoch:9 step:9003 [D loss: 0.550583, acc.: 74.22%] [G loss: 0.470001]\n",
      "epoch:9 step:9004 [D loss: 0.517710, acc.: 68.75%] [G loss: 0.572571]\n",
      "epoch:9 step:9005 [D loss: 0.548028, acc.: 69.53%] [G loss: 0.472995]\n",
      "epoch:9 step:9006 [D loss: 0.592340, acc.: 67.97%] [G loss: 0.475552]\n",
      "epoch:9 step:9007 [D loss: 0.544834, acc.: 75.00%] [G loss: 0.552156]\n",
      "epoch:9 step:9008 [D loss: 0.474649, acc.: 78.12%] [G loss: 0.622871]\n",
      "epoch:9 step:9009 [D loss: 0.602066, acc.: 64.84%] [G loss: 0.505834]\n",
      "epoch:9 step:9010 [D loss: 0.565794, acc.: 72.66%] [G loss: 0.562717]\n",
      "epoch:9 step:9011 [D loss: 0.582448, acc.: 67.97%] [G loss: 0.558115]\n",
      "epoch:9 step:9012 [D loss: 0.503170, acc.: 75.00%] [G loss: 0.525821]\n",
      "epoch:9 step:9013 [D loss: 0.535490, acc.: 73.44%] [G loss: 0.602496]\n",
      "epoch:9 step:9014 [D loss: 0.527583, acc.: 74.22%] [G loss: 0.552476]\n",
      "epoch:9 step:9015 [D loss: 0.515692, acc.: 73.44%] [G loss: 0.633505]\n",
      "epoch:9 step:9016 [D loss: 0.551431, acc.: 68.75%] [G loss: 0.628485]\n",
      "epoch:9 step:9017 [D loss: 0.631904, acc.: 63.28%] [G loss: 0.676220]\n",
      "epoch:9 step:9018 [D loss: 0.542408, acc.: 70.31%] [G loss: 0.446923]\n",
      "epoch:9 step:9019 [D loss: 0.630124, acc.: 60.94%] [G loss: 0.416591]\n",
      "epoch:9 step:9020 [D loss: 0.569900, acc.: 64.84%] [G loss: 0.563825]\n",
      "epoch:9 step:9021 [D loss: 0.533881, acc.: 70.31%] [G loss: 0.544912]\n",
      "epoch:9 step:9022 [D loss: 0.562543, acc.: 64.84%] [G loss: 0.592555]\n",
      "epoch:9 step:9023 [D loss: 0.597208, acc.: 67.97%] [G loss: 0.444941]\n",
      "epoch:9 step:9024 [D loss: 0.558325, acc.: 71.88%] [G loss: 0.601604]\n",
      "epoch:9 step:9025 [D loss: 0.517980, acc.: 77.34%] [G loss: 0.548168]\n",
      "epoch:9 step:9026 [D loss: 0.529023, acc.: 71.88%] [G loss: 0.566475]\n",
      "epoch:9 step:9027 [D loss: 0.581208, acc.: 71.09%] [G loss: 0.512175]\n",
      "epoch:9 step:9028 [D loss: 0.627377, acc.: 63.28%] [G loss: 0.469616]\n",
      "epoch:9 step:9029 [D loss: 0.555433, acc.: 71.09%] [G loss: 0.475963]\n",
      "epoch:9 step:9030 [D loss: 0.546479, acc.: 71.88%] [G loss: 0.568999]\n",
      "epoch:9 step:9031 [D loss: 0.524867, acc.: 78.12%] [G loss: 0.657164]\n",
      "epoch:9 step:9032 [D loss: 0.601611, acc.: 66.41%] [G loss: 0.630121]\n",
      "epoch:9 step:9033 [D loss: 0.613551, acc.: 64.06%] [G loss: 0.554049]\n",
      "epoch:9 step:9034 [D loss: 0.569439, acc.: 66.41%] [G loss: 0.560538]\n",
      "epoch:9 step:9035 [D loss: 0.523895, acc.: 66.41%] [G loss: 0.537271]\n",
      "epoch:9 step:9036 [D loss: 0.486893, acc.: 78.12%] [G loss: 0.653540]\n",
      "epoch:9 step:9037 [D loss: 0.573190, acc.: 67.97%] [G loss: 0.571087]\n",
      "epoch:9 step:9038 [D loss: 0.527768, acc.: 74.22%] [G loss: 0.752979]\n",
      "epoch:9 step:9039 [D loss: 0.593299, acc.: 64.84%] [G loss: 0.664960]\n",
      "epoch:9 step:9040 [D loss: 0.557729, acc.: 67.19%] [G loss: 0.514613]\n",
      "epoch:9 step:9041 [D loss: 0.545997, acc.: 73.44%] [G loss: 0.457316]\n",
      "epoch:9 step:9042 [D loss: 0.514422, acc.: 75.00%] [G loss: 0.466571]\n",
      "epoch:9 step:9043 [D loss: 0.565313, acc.: 69.53%] [G loss: 0.487801]\n",
      "epoch:9 step:9044 [D loss: 0.502399, acc.: 75.00%] [G loss: 0.549495]\n",
      "epoch:9 step:9045 [D loss: 0.579512, acc.: 69.53%] [G loss: 0.590494]\n",
      "epoch:9 step:9046 [D loss: 0.507205, acc.: 74.22%] [G loss: 0.582733]\n",
      "epoch:9 step:9047 [D loss: 0.583691, acc.: 68.75%] [G loss: 0.543032]\n",
      "epoch:9 step:9048 [D loss: 0.595156, acc.: 61.72%] [G loss: 0.601774]\n",
      "epoch:9 step:9049 [D loss: 0.571365, acc.: 68.75%] [G loss: 0.607426]\n",
      "epoch:9 step:9050 [D loss: 0.567515, acc.: 67.97%] [G loss: 0.560257]\n",
      "epoch:9 step:9051 [D loss: 0.541925, acc.: 68.75%] [G loss: 0.713495]\n",
      "epoch:9 step:9052 [D loss: 0.591933, acc.: 69.53%] [G loss: 0.490022]\n",
      "epoch:9 step:9053 [D loss: 0.553799, acc.: 74.22%] [G loss: 0.666082]\n",
      "epoch:9 step:9054 [D loss: 0.549605, acc.: 69.53%] [G loss: 0.514625]\n",
      "epoch:9 step:9055 [D loss: 0.630997, acc.: 67.19%] [G loss: 0.507060]\n",
      "epoch:9 step:9056 [D loss: 0.468121, acc.: 78.12%] [G loss: 0.563097]\n",
      "epoch:9 step:9057 [D loss: 0.608929, acc.: 69.53%] [G loss: 0.530947]\n",
      "epoch:9 step:9058 [D loss: 0.616959, acc.: 63.28%] [G loss: 0.385178]\n",
      "epoch:9 step:9059 [D loss: 0.538233, acc.: 68.75%] [G loss: 0.449850]\n",
      "epoch:9 step:9060 [D loss: 0.526520, acc.: 70.31%] [G loss: 0.490469]\n",
      "epoch:9 step:9061 [D loss: 0.595214, acc.: 62.50%] [G loss: 0.464049]\n",
      "epoch:9 step:9062 [D loss: 0.484410, acc.: 76.56%] [G loss: 0.549535]\n",
      "epoch:9 step:9063 [D loss: 0.477283, acc.: 82.03%] [G loss: 0.618498]\n",
      "epoch:9 step:9064 [D loss: 0.575947, acc.: 72.66%] [G loss: 0.553545]\n",
      "epoch:9 step:9065 [D loss: 0.500052, acc.: 75.78%] [G loss: 0.702932]\n",
      "epoch:9 step:9066 [D loss: 0.517621, acc.: 75.78%] [G loss: 0.591897]\n",
      "epoch:9 step:9067 [D loss: 0.507427, acc.: 72.66%] [G loss: 0.742659]\n",
      "epoch:9 step:9068 [D loss: 0.485142, acc.: 73.44%] [G loss: 0.655973]\n",
      "epoch:9 step:9069 [D loss: 0.595281, acc.: 65.62%] [G loss: 0.569118]\n",
      "epoch:9 step:9070 [D loss: 0.564176, acc.: 68.75%] [G loss: 0.503268]\n",
      "epoch:9 step:9071 [D loss: 0.531168, acc.: 74.22%] [G loss: 0.566670]\n",
      "epoch:9 step:9072 [D loss: 0.469515, acc.: 82.03%] [G loss: 0.593741]\n",
      "epoch:9 step:9073 [D loss: 0.559270, acc.: 69.53%] [G loss: 0.573438]\n",
      "epoch:9 step:9074 [D loss: 0.487125, acc.: 76.56%] [G loss: 0.603294]\n",
      "epoch:9 step:9075 [D loss: 0.507404, acc.: 75.78%] [G loss: 0.635341]\n",
      "epoch:9 step:9076 [D loss: 0.552671, acc.: 72.66%] [G loss: 0.526671]\n",
      "epoch:9 step:9077 [D loss: 0.563594, acc.: 67.97%] [G loss: 0.491943]\n",
      "epoch:9 step:9078 [D loss: 0.508544, acc.: 74.22%] [G loss: 0.638829]\n",
      "epoch:9 step:9079 [D loss: 0.496882, acc.: 75.78%] [G loss: 0.621205]\n",
      "epoch:9 step:9080 [D loss: 0.447265, acc.: 85.16%] [G loss: 0.766535]\n",
      "epoch:9 step:9081 [D loss: 0.442027, acc.: 80.47%] [G loss: 0.773926]\n",
      "epoch:9 step:9082 [D loss: 0.486904, acc.: 78.91%] [G loss: 0.650354]\n",
      "epoch:9 step:9083 [D loss: 0.509695, acc.: 77.34%] [G loss: 0.756133]\n",
      "epoch:9 step:9084 [D loss: 0.534769, acc.: 74.22%] [G loss: 0.790547]\n",
      "epoch:9 step:9085 [D loss: 0.615277, acc.: 63.28%] [G loss: 0.507751]\n",
      "epoch:9 step:9086 [D loss: 0.631864, acc.: 64.06%] [G loss: 0.535216]\n",
      "epoch:9 step:9087 [D loss: 0.505774, acc.: 71.09%] [G loss: 0.589525]\n",
      "epoch:9 step:9088 [D loss: 0.567202, acc.: 68.75%] [G loss: 0.646649]\n",
      "epoch:9 step:9089 [D loss: 0.562237, acc.: 67.19%] [G loss: 0.544029]\n",
      "epoch:9 step:9090 [D loss: 0.523117, acc.: 75.00%] [G loss: 0.580773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9091 [D loss: 0.569644, acc.: 68.75%] [G loss: 0.677528]\n",
      "epoch:9 step:9092 [D loss: 0.579077, acc.: 70.31%] [G loss: 0.547928]\n",
      "epoch:9 step:9093 [D loss: 0.533521, acc.: 72.66%] [G loss: 0.491256]\n",
      "epoch:9 step:9094 [D loss: 0.510605, acc.: 71.88%] [G loss: 0.498255]\n",
      "epoch:9 step:9095 [D loss: 0.599229, acc.: 67.19%] [G loss: 0.519648]\n",
      "epoch:9 step:9096 [D loss: 0.572754, acc.: 72.66%] [G loss: 0.574619]\n",
      "epoch:9 step:9097 [D loss: 0.545174, acc.: 71.88%] [G loss: 0.546811]\n",
      "epoch:9 step:9098 [D loss: 0.599042, acc.: 61.72%] [G loss: 0.560507]\n",
      "epoch:9 step:9099 [D loss: 0.615658, acc.: 62.50%] [G loss: 0.541793]\n",
      "epoch:9 step:9100 [D loss: 0.596229, acc.: 64.06%] [G loss: 0.431973]\n",
      "epoch:9 step:9101 [D loss: 0.588574, acc.: 71.88%] [G loss: 0.559233]\n",
      "epoch:9 step:9102 [D loss: 0.520580, acc.: 74.22%] [G loss: 0.516607]\n",
      "epoch:9 step:9103 [D loss: 0.557607, acc.: 66.41%] [G loss: 0.398194]\n",
      "epoch:9 step:9104 [D loss: 0.585421, acc.: 64.06%] [G loss: 0.548720]\n",
      "epoch:9 step:9105 [D loss: 0.643900, acc.: 60.94%] [G loss: 0.462454]\n",
      "epoch:9 step:9106 [D loss: 0.604555, acc.: 66.41%] [G loss: 0.467857]\n",
      "epoch:9 step:9107 [D loss: 0.592652, acc.: 69.53%] [G loss: 0.518769]\n",
      "epoch:9 step:9108 [D loss: 0.581916, acc.: 64.84%] [G loss: 0.460543]\n",
      "epoch:9 step:9109 [D loss: 0.549685, acc.: 69.53%] [G loss: 0.635195]\n",
      "epoch:9 step:9110 [D loss: 0.505789, acc.: 78.12%] [G loss: 0.431090]\n",
      "epoch:9 step:9111 [D loss: 0.553496, acc.: 70.31%] [G loss: 0.490700]\n",
      "epoch:9 step:9112 [D loss: 0.549891, acc.: 70.31%] [G loss: 0.454045]\n",
      "epoch:9 step:9113 [D loss: 0.519085, acc.: 69.53%] [G loss: 0.533368]\n",
      "epoch:9 step:9114 [D loss: 0.508505, acc.: 75.00%] [G loss: 0.565915]\n",
      "epoch:9 step:9115 [D loss: 0.528628, acc.: 70.31%] [G loss: 0.553018]\n",
      "epoch:9 step:9116 [D loss: 0.573422, acc.: 69.53%] [G loss: 0.563150]\n",
      "epoch:9 step:9117 [D loss: 0.608462, acc.: 62.50%] [G loss: 0.435550]\n",
      "epoch:9 step:9118 [D loss: 0.527107, acc.: 71.88%] [G loss: 0.474291]\n",
      "epoch:9 step:9119 [D loss: 0.556980, acc.: 70.31%] [G loss: 0.429233]\n",
      "epoch:9 step:9120 [D loss: 0.567344, acc.: 67.19%] [G loss: 0.548630]\n",
      "epoch:9 step:9121 [D loss: 0.555695, acc.: 68.75%] [G loss: 0.557714]\n",
      "epoch:9 step:9122 [D loss: 0.520188, acc.: 71.88%] [G loss: 0.660169]\n",
      "epoch:9 step:9123 [D loss: 0.529258, acc.: 72.66%] [G loss: 0.662858]\n",
      "epoch:9 step:9124 [D loss: 0.532187, acc.: 71.09%] [G loss: 0.617917]\n",
      "epoch:9 step:9125 [D loss: 0.570117, acc.: 65.62%] [G loss: 0.570478]\n",
      "epoch:9 step:9126 [D loss: 0.517524, acc.: 70.31%] [G loss: 0.526514]\n",
      "epoch:9 step:9127 [D loss: 0.480878, acc.: 77.34%] [G loss: 0.674956]\n",
      "epoch:9 step:9128 [D loss: 0.535945, acc.: 75.78%] [G loss: 0.584805]\n",
      "epoch:9 step:9129 [D loss: 0.614124, acc.: 64.06%] [G loss: 0.437495]\n",
      "epoch:9 step:9130 [D loss: 0.544691, acc.: 64.84%] [G loss: 0.482909]\n",
      "epoch:9 step:9131 [D loss: 0.573837, acc.: 69.53%] [G loss: 0.439457]\n",
      "epoch:9 step:9132 [D loss: 0.518270, acc.: 71.09%] [G loss: 0.602969]\n",
      "epoch:9 step:9133 [D loss: 0.590642, acc.: 63.28%] [G loss: 0.643551]\n",
      "epoch:9 step:9134 [D loss: 0.551131, acc.: 71.09%] [G loss: 0.519490]\n",
      "epoch:9 step:9135 [D loss: 0.575490, acc.: 71.09%] [G loss: 0.535836]\n",
      "epoch:9 step:9136 [D loss: 0.587848, acc.: 65.62%] [G loss: 0.477059]\n",
      "epoch:9 step:9137 [D loss: 0.603364, acc.: 66.41%] [G loss: 0.402756]\n",
      "epoch:9 step:9138 [D loss: 0.526837, acc.: 74.22%] [G loss: 0.555106]\n",
      "epoch:9 step:9139 [D loss: 0.544547, acc.: 68.75%] [G loss: 0.542804]\n",
      "epoch:9 step:9140 [D loss: 0.511307, acc.: 74.22%] [G loss: 0.653894]\n",
      "epoch:9 step:9141 [D loss: 0.501650, acc.: 78.91%] [G loss: 0.557641]\n",
      "epoch:9 step:9142 [D loss: 0.535033, acc.: 74.22%] [G loss: 0.582479]\n",
      "epoch:9 step:9143 [D loss: 0.601420, acc.: 64.84%] [G loss: 0.619689]\n",
      "epoch:9 step:9144 [D loss: 0.594418, acc.: 62.50%] [G loss: 0.424578]\n",
      "epoch:9 step:9145 [D loss: 0.514273, acc.: 71.88%] [G loss: 0.717391]\n",
      "epoch:9 step:9146 [D loss: 0.584000, acc.: 64.84%] [G loss: 0.538115]\n",
      "epoch:9 step:9147 [D loss: 0.533557, acc.: 70.31%] [G loss: 0.464829]\n",
      "epoch:9 step:9148 [D loss: 0.545934, acc.: 71.88%] [G loss: 0.520184]\n",
      "epoch:9 step:9149 [D loss: 0.620750, acc.: 65.62%] [G loss: 0.583980]\n",
      "epoch:9 step:9150 [D loss: 0.618777, acc.: 67.97%] [G loss: 0.516803]\n",
      "epoch:9 step:9151 [D loss: 0.637140, acc.: 61.72%] [G loss: 0.468556]\n",
      "epoch:9 step:9152 [D loss: 0.546218, acc.: 75.00%] [G loss: 0.570225]\n",
      "epoch:9 step:9153 [D loss: 0.583917, acc.: 64.06%] [G loss: 0.546856]\n",
      "epoch:9 step:9154 [D loss: 0.602176, acc.: 67.97%] [G loss: 0.531267]\n",
      "epoch:9 step:9155 [D loss: 0.569845, acc.: 66.41%] [G loss: 0.507696]\n",
      "epoch:9 step:9156 [D loss: 0.575515, acc.: 68.75%] [G loss: 0.427823]\n",
      "epoch:9 step:9157 [D loss: 0.553378, acc.: 70.31%] [G loss: 0.523542]\n",
      "epoch:9 step:9158 [D loss: 0.518456, acc.: 75.78%] [G loss: 0.750952]\n",
      "epoch:9 step:9159 [D loss: 0.564204, acc.: 67.19%] [G loss: 0.713605]\n",
      "epoch:9 step:9160 [D loss: 0.612112, acc.: 66.41%] [G loss: 0.505242]\n",
      "epoch:9 step:9161 [D loss: 0.569104, acc.: 72.66%] [G loss: 0.462496]\n",
      "epoch:9 step:9162 [D loss: 0.576534, acc.: 70.31%] [G loss: 0.460033]\n",
      "epoch:9 step:9163 [D loss: 0.497008, acc.: 77.34%] [G loss: 0.538496]\n",
      "epoch:9 step:9164 [D loss: 0.559391, acc.: 67.97%] [G loss: 0.616898]\n",
      "epoch:9 step:9165 [D loss: 0.606441, acc.: 60.16%] [G loss: 0.594501]\n",
      "epoch:9 step:9166 [D loss: 0.530699, acc.: 71.88%] [G loss: 0.698511]\n",
      "epoch:9 step:9167 [D loss: 0.520986, acc.: 73.44%] [G loss: 0.611751]\n",
      "epoch:9 step:9168 [D loss: 0.558965, acc.: 68.75%] [G loss: 0.482735]\n",
      "epoch:9 step:9169 [D loss: 0.518452, acc.: 75.00%] [G loss: 0.443378]\n",
      "epoch:9 step:9170 [D loss: 0.516922, acc.: 71.09%] [G loss: 0.574418]\n",
      "epoch:9 step:9171 [D loss: 0.587282, acc.: 62.50%] [G loss: 0.516670]\n",
      "epoch:9 step:9172 [D loss: 0.598261, acc.: 64.84%] [G loss: 0.529140]\n",
      "epoch:9 step:9173 [D loss: 0.651426, acc.: 59.38%] [G loss: 0.475529]\n",
      "epoch:9 step:9174 [D loss: 0.539526, acc.: 71.88%] [G loss: 0.375525]\n",
      "epoch:9 step:9175 [D loss: 0.565906, acc.: 67.97%] [G loss: 0.480084]\n",
      "epoch:9 step:9176 [D loss: 0.518899, acc.: 74.22%] [G loss: 0.706295]\n",
      "epoch:9 step:9177 [D loss: 0.565165, acc.: 69.53%] [G loss: 0.621777]\n",
      "epoch:9 step:9178 [D loss: 0.597060, acc.: 67.97%] [G loss: 0.577010]\n",
      "epoch:9 step:9179 [D loss: 0.486116, acc.: 72.66%] [G loss: 0.641035]\n",
      "epoch:9 step:9180 [D loss: 0.482561, acc.: 72.66%] [G loss: 0.735225]\n",
      "epoch:9 step:9181 [D loss: 0.597586, acc.: 65.62%] [G loss: 0.618728]\n",
      "epoch:9 step:9182 [D loss: 0.566940, acc.: 67.19%] [G loss: 0.518335]\n",
      "epoch:9 step:9183 [D loss: 0.548922, acc.: 71.88%] [G loss: 0.416181]\n",
      "epoch:9 step:9184 [D loss: 0.550087, acc.: 72.66%] [G loss: 0.629462]\n",
      "epoch:9 step:9185 [D loss: 0.587117, acc.: 67.19%] [G loss: 0.528539]\n",
      "epoch:9 step:9186 [D loss: 0.535619, acc.: 71.09%] [G loss: 0.614531]\n",
      "epoch:9 step:9187 [D loss: 0.500924, acc.: 74.22%] [G loss: 0.584918]\n",
      "epoch:9 step:9188 [D loss: 0.575258, acc.: 65.62%] [G loss: 0.641042]\n",
      "epoch:9 step:9189 [D loss: 0.565357, acc.: 68.75%] [G loss: 0.449974]\n",
      "epoch:9 step:9190 [D loss: 0.597923, acc.: 71.09%] [G loss: 0.548046]\n",
      "epoch:9 step:9191 [D loss: 0.560485, acc.: 67.97%] [G loss: 0.435330]\n",
      "epoch:9 step:9192 [D loss: 0.585521, acc.: 69.53%] [G loss: 0.446123]\n",
      "epoch:9 step:9193 [D loss: 0.555547, acc.: 68.75%] [G loss: 0.462559]\n",
      "epoch:9 step:9194 [D loss: 0.568538, acc.: 67.97%] [G loss: 0.442443]\n",
      "epoch:9 step:9195 [D loss: 0.566159, acc.: 64.06%] [G loss: 0.506845]\n",
      "epoch:9 step:9196 [D loss: 0.577087, acc.: 67.19%] [G loss: 0.503715]\n",
      "epoch:9 step:9197 [D loss: 0.557777, acc.: 74.22%] [G loss: 0.716084]\n",
      "epoch:9 step:9198 [D loss: 0.693559, acc.: 57.81%] [G loss: 0.439194]\n",
      "epoch:9 step:9199 [D loss: 0.717241, acc.: 60.94%] [G loss: 0.520440]\n",
      "epoch:9 step:9200 [D loss: 0.574385, acc.: 68.75%] [G loss: 0.388662]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.624477\n",
      "FID: 38.939667\n",
      "0 = 12.861370245075209\n",
      "1 = 0.09516161873741481\n",
      "2 = 0.9092000126838684\n",
      "3 = 0.8461999893188477\n",
      "4 = 0.9721999764442444\n",
      "5 = 0.9681922197341919\n",
      "6 = 0.8461999893188477\n",
      "7 = 7.760531502592559\n",
      "8 = 0.1280898627505437\n",
      "9 = 0.7282999753952026\n",
      "10 = 0.7242000102996826\n",
      "11 = 0.7324000000953674\n",
      "12 = 0.7301875352859497\n",
      "13 = 0.7242000102996826\n",
      "14 = 6.624504566192627\n",
      "15 = 7.615898132324219\n",
      "16 = 0.3382549285888672\n",
      "17 = 6.624477386474609\n",
      "18 = 38.939666748046875\n",
      "epoch:9 step:9201 [D loss: 0.530219, acc.: 70.31%] [G loss: 0.474265]\n",
      "epoch:9 step:9202 [D loss: 0.535674, acc.: 69.53%] [G loss: 0.671177]\n",
      "epoch:9 step:9203 [D loss: 0.515563, acc.: 75.78%] [G loss: 0.695805]\n",
      "epoch:9 step:9204 [D loss: 0.518958, acc.: 73.44%] [G loss: 0.725250]\n",
      "epoch:9 step:9205 [D loss: 0.585427, acc.: 67.19%] [G loss: 0.601691]\n",
      "epoch:9 step:9206 [D loss: 0.551875, acc.: 67.19%] [G loss: 0.615319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9207 [D loss: 0.520843, acc.: 75.00%] [G loss: 0.515326]\n",
      "epoch:9 step:9208 [D loss: 0.556271, acc.: 71.09%] [G loss: 0.623248]\n",
      "epoch:9 step:9209 [D loss: 0.602416, acc.: 65.62%] [G loss: 0.703708]\n",
      "epoch:9 step:9210 [D loss: 0.558457, acc.: 70.31%] [G loss: 0.631338]\n",
      "epoch:9 step:9211 [D loss: 0.582456, acc.: 64.84%] [G loss: 0.582748]\n",
      "epoch:9 step:9212 [D loss: 0.606504, acc.: 67.19%] [G loss: 0.518869]\n",
      "epoch:9 step:9213 [D loss: 0.582580, acc.: 69.53%] [G loss: 0.494355]\n",
      "epoch:9 step:9214 [D loss: 0.535159, acc.: 70.31%] [G loss: 0.631601]\n",
      "epoch:9 step:9215 [D loss: 0.509191, acc.: 74.22%] [G loss: 0.811257]\n",
      "epoch:9 step:9216 [D loss: 0.633416, acc.: 60.94%] [G loss: 0.558806]\n",
      "epoch:9 step:9217 [D loss: 0.654792, acc.: 54.69%] [G loss: 0.492346]\n",
      "epoch:9 step:9218 [D loss: 0.572381, acc.: 67.19%] [G loss: 0.535610]\n",
      "epoch:9 step:9219 [D loss: 0.529295, acc.: 74.22%] [G loss: 0.471724]\n",
      "epoch:9 step:9220 [D loss: 0.608701, acc.: 64.84%] [G loss: 0.501681]\n",
      "epoch:9 step:9221 [D loss: 0.664893, acc.: 60.16%] [G loss: 0.370637]\n",
      "epoch:9 step:9222 [D loss: 0.544414, acc.: 71.09%] [G loss: 0.548855]\n",
      "epoch:9 step:9223 [D loss: 0.524546, acc.: 74.22%] [G loss: 0.525059]\n",
      "epoch:9 step:9224 [D loss: 0.546921, acc.: 71.09%] [G loss: 0.657048]\n",
      "epoch:9 step:9225 [D loss: 0.535250, acc.: 74.22%] [G loss: 0.536366]\n",
      "epoch:9 step:9226 [D loss: 0.585341, acc.: 69.53%] [G loss: 0.604035]\n",
      "epoch:9 step:9227 [D loss: 0.677604, acc.: 59.38%] [G loss: 0.509639]\n",
      "epoch:9 step:9228 [D loss: 0.572919, acc.: 71.09%] [G loss: 0.630700]\n",
      "epoch:9 step:9229 [D loss: 0.522298, acc.: 71.09%] [G loss: 0.612663]\n",
      "epoch:9 step:9230 [D loss: 0.530321, acc.: 72.66%] [G loss: 0.604380]\n",
      "epoch:9 step:9231 [D loss: 0.613765, acc.: 58.59%] [G loss: 0.486386]\n",
      "epoch:9 step:9232 [D loss: 0.544531, acc.: 71.09%] [G loss: 0.550421]\n",
      "epoch:9 step:9233 [D loss: 0.597513, acc.: 62.50%] [G loss: 0.538306]\n",
      "epoch:9 step:9234 [D loss: 0.494438, acc.: 75.78%] [G loss: 0.763008]\n",
      "epoch:9 step:9235 [D loss: 0.519220, acc.: 75.00%] [G loss: 0.665674]\n",
      "epoch:9 step:9236 [D loss: 0.535640, acc.: 71.09%] [G loss: 0.627138]\n",
      "epoch:9 step:9237 [D loss: 0.608396, acc.: 64.06%] [G loss: 0.474311]\n",
      "epoch:9 step:9238 [D loss: 0.514752, acc.: 73.44%] [G loss: 0.556600]\n",
      "epoch:9 step:9239 [D loss: 0.584139, acc.: 67.19%] [G loss: 0.426548]\n",
      "epoch:9 step:9240 [D loss: 0.538365, acc.: 69.53%] [G loss: 0.449928]\n",
      "epoch:9 step:9241 [D loss: 0.543901, acc.: 72.66%] [G loss: 0.641183]\n",
      "epoch:9 step:9242 [D loss: 0.514506, acc.: 73.44%] [G loss: 0.643828]\n",
      "epoch:9 step:9243 [D loss: 0.533093, acc.: 71.09%] [G loss: 0.515987]\n",
      "epoch:9 step:9244 [D loss: 0.578308, acc.: 69.53%] [G loss: 0.536057]\n",
      "epoch:9 step:9245 [D loss: 0.637913, acc.: 63.28%] [G loss: 0.470770]\n",
      "epoch:9 step:9246 [D loss: 0.569064, acc.: 68.75%] [G loss: 0.414294]\n",
      "epoch:9 step:9247 [D loss: 0.543944, acc.: 70.31%] [G loss: 0.498594]\n",
      "epoch:9 step:9248 [D loss: 0.514348, acc.: 73.44%] [G loss: 0.729497]\n",
      "epoch:9 step:9249 [D loss: 0.535040, acc.: 74.22%] [G loss: 0.639954]\n",
      "epoch:9 step:9250 [D loss: 0.616761, acc.: 60.16%] [G loss: 0.475401]\n",
      "epoch:9 step:9251 [D loss: 0.611674, acc.: 64.84%] [G loss: 0.490859]\n",
      "epoch:9 step:9252 [D loss: 0.528108, acc.: 76.56%] [G loss: 0.491464]\n",
      "epoch:9 step:9253 [D loss: 0.622984, acc.: 64.84%] [G loss: 0.601000]\n",
      "epoch:9 step:9254 [D loss: 0.549647, acc.: 69.53%] [G loss: 0.441461]\n",
      "epoch:9 step:9255 [D loss: 0.561128, acc.: 66.41%] [G loss: 0.407243]\n",
      "epoch:9 step:9256 [D loss: 0.514291, acc.: 71.09%] [G loss: 0.560796]\n",
      "epoch:9 step:9257 [D loss: 0.628306, acc.: 64.06%] [G loss: 0.526801]\n",
      "epoch:9 step:9258 [D loss: 0.567739, acc.: 69.53%] [G loss: 0.542226]\n",
      "epoch:9 step:9259 [D loss: 0.532077, acc.: 72.66%] [G loss: 0.565174]\n",
      "epoch:9 step:9260 [D loss: 0.649806, acc.: 60.16%] [G loss: 0.550582]\n",
      "epoch:9 step:9261 [D loss: 0.650447, acc.: 56.25%] [G loss: 0.445224]\n",
      "epoch:9 step:9262 [D loss: 0.540865, acc.: 72.66%] [G loss: 0.477113]\n",
      "epoch:9 step:9263 [D loss: 0.578579, acc.: 66.41%] [G loss: 0.497631]\n",
      "epoch:9 step:9264 [D loss: 0.587097, acc.: 71.09%] [G loss: 0.520944]\n",
      "epoch:9 step:9265 [D loss: 0.565570, acc.: 69.53%] [G loss: 0.484972]\n",
      "epoch:9 step:9266 [D loss: 0.538729, acc.: 69.53%] [G loss: 0.530608]\n",
      "epoch:9 step:9267 [D loss: 0.501358, acc.: 75.00%] [G loss: 0.456514]\n",
      "epoch:9 step:9268 [D loss: 0.573219, acc.: 63.28%] [G loss: 0.379546]\n",
      "epoch:9 step:9269 [D loss: 0.564951, acc.: 71.09%] [G loss: 0.459628]\n",
      "epoch:9 step:9270 [D loss: 0.538735, acc.: 75.78%] [G loss: 0.465374]\n",
      "epoch:9 step:9271 [D loss: 0.589484, acc.: 66.41%] [G loss: 0.519978]\n",
      "epoch:9 step:9272 [D loss: 0.559032, acc.: 67.19%] [G loss: 0.364607]\n",
      "epoch:9 step:9273 [D loss: 0.603130, acc.: 64.06%] [G loss: 0.450933]\n",
      "epoch:9 step:9274 [D loss: 0.545955, acc.: 68.75%] [G loss: 0.440079]\n",
      "epoch:9 step:9275 [D loss: 0.512104, acc.: 73.44%] [G loss: 0.572988]\n",
      "epoch:9 step:9276 [D loss: 0.528811, acc.: 72.66%] [G loss: 0.625988]\n",
      "epoch:9 step:9277 [D loss: 0.549668, acc.: 69.53%] [G loss: 0.621118]\n",
      "epoch:9 step:9278 [D loss: 0.589569, acc.: 66.41%] [G loss: 0.485524]\n",
      "epoch:9 step:9279 [D loss: 0.589581, acc.: 65.62%] [G loss: 0.610166]\n",
      "epoch:9 step:9280 [D loss: 0.607691, acc.: 65.62%] [G loss: 0.342251]\n",
      "epoch:9 step:9281 [D loss: 0.515235, acc.: 73.44%] [G loss: 0.408066]\n",
      "epoch:9 step:9282 [D loss: 0.559097, acc.: 69.53%] [G loss: 0.419085]\n",
      "epoch:9 step:9283 [D loss: 0.545475, acc.: 74.22%] [G loss: 0.445319]\n",
      "epoch:9 step:9284 [D loss: 0.604901, acc.: 64.84%] [G loss: 0.495979]\n",
      "epoch:9 step:9285 [D loss: 0.573034, acc.: 68.75%] [G loss: 0.516900]\n",
      "epoch:9 step:9286 [D loss: 0.548864, acc.: 67.97%] [G loss: 0.544940]\n",
      "epoch:9 step:9287 [D loss: 0.507335, acc.: 76.56%] [G loss: 0.524042]\n",
      "epoch:9 step:9288 [D loss: 0.501844, acc.: 71.88%] [G loss: 0.736559]\n",
      "epoch:9 step:9289 [D loss: 0.565621, acc.: 68.75%] [G loss: 0.451701]\n",
      "epoch:9 step:9290 [D loss: 0.543375, acc.: 67.19%] [G loss: 0.569587]\n",
      "epoch:9 step:9291 [D loss: 0.595542, acc.: 64.84%] [G loss: 0.535839]\n",
      "epoch:9 step:9292 [D loss: 0.577786, acc.: 70.31%] [G loss: 0.544785]\n",
      "epoch:9 step:9293 [D loss: 0.460083, acc.: 77.34%] [G loss: 0.574089]\n",
      "epoch:9 step:9294 [D loss: 0.593711, acc.: 69.53%] [G loss: 0.498272]\n",
      "epoch:9 step:9295 [D loss: 0.576162, acc.: 68.75%] [G loss: 0.406078]\n",
      "epoch:9 step:9296 [D loss: 0.636456, acc.: 60.16%] [G loss: 0.495506]\n",
      "epoch:9 step:9297 [D loss: 0.522270, acc.: 67.97%] [G loss: 0.444741]\n",
      "epoch:9 step:9298 [D loss: 0.631266, acc.: 58.59%] [G loss: 0.435744]\n",
      "epoch:9 step:9299 [D loss: 0.572307, acc.: 60.16%] [G loss: 0.447504]\n",
      "epoch:9 step:9300 [D loss: 0.674469, acc.: 60.16%] [G loss: 0.442360]\n",
      "epoch:9 step:9301 [D loss: 0.513081, acc.: 76.56%] [G loss: 0.457247]\n",
      "epoch:9 step:9302 [D loss: 0.602733, acc.: 63.28%] [G loss: 0.462602]\n",
      "epoch:9 step:9303 [D loss: 0.490959, acc.: 75.00%] [G loss: 0.535026]\n",
      "epoch:9 step:9304 [D loss: 0.509318, acc.: 75.78%] [G loss: 0.609317]\n",
      "epoch:9 step:9305 [D loss: 0.597330, acc.: 66.41%] [G loss: 0.515889]\n",
      "epoch:9 step:9306 [D loss: 0.607090, acc.: 64.06%] [G loss: 0.499660]\n",
      "epoch:9 step:9307 [D loss: 0.558288, acc.: 71.88%] [G loss: 0.508853]\n",
      "epoch:9 step:9308 [D loss: 0.505958, acc.: 76.56%] [G loss: 0.620790]\n",
      "epoch:9 step:9309 [D loss: 0.569184, acc.: 67.97%] [G loss: 0.503892]\n",
      "epoch:9 step:9310 [D loss: 0.589702, acc.: 64.84%] [G loss: 0.392811]\n",
      "epoch:9 step:9311 [D loss: 0.544302, acc.: 69.53%] [G loss: 0.476004]\n",
      "epoch:9 step:9312 [D loss: 0.563578, acc.: 68.75%] [G loss: 0.505688]\n",
      "epoch:9 step:9313 [D loss: 0.655330, acc.: 66.41%] [G loss: 0.294879]\n",
      "epoch:9 step:9314 [D loss: 0.579309, acc.: 64.06%] [G loss: 0.438544]\n",
      "epoch:9 step:9315 [D loss: 0.605239, acc.: 67.97%] [G loss: 0.353709]\n",
      "epoch:9 step:9316 [D loss: 0.590948, acc.: 68.75%] [G loss: 0.468944]\n",
      "epoch:9 step:9317 [D loss: 0.491555, acc.: 77.34%] [G loss: 0.563335]\n",
      "epoch:9 step:9318 [D loss: 0.517453, acc.: 73.44%] [G loss: 0.547811]\n",
      "epoch:9 step:9319 [D loss: 0.537414, acc.: 70.31%] [G loss: 0.579372]\n",
      "epoch:9 step:9320 [D loss: 0.528561, acc.: 73.44%] [G loss: 0.607026]\n",
      "epoch:9 step:9321 [D loss: 0.560934, acc.: 67.19%] [G loss: 0.596428]\n",
      "epoch:9 step:9322 [D loss: 0.484435, acc.: 78.12%] [G loss: 0.571096]\n",
      "epoch:9 step:9323 [D loss: 0.469352, acc.: 78.91%] [G loss: 0.609355]\n",
      "epoch:9 step:9324 [D loss: 0.612021, acc.: 61.72%] [G loss: 0.496168]\n",
      "epoch:9 step:9325 [D loss: 0.666230, acc.: 54.69%] [G loss: 0.445322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9326 [D loss: 0.556994, acc.: 70.31%] [G loss: 0.543889]\n",
      "epoch:9 step:9327 [D loss: 0.528349, acc.: 68.75%] [G loss: 0.622584]\n",
      "epoch:9 step:9328 [D loss: 0.519645, acc.: 72.66%] [G loss: 0.612004]\n",
      "epoch:9 step:9329 [D loss: 0.524702, acc.: 74.22%] [G loss: 0.610095]\n",
      "epoch:9 step:9330 [D loss: 0.504895, acc.: 72.66%] [G loss: 0.743904]\n",
      "epoch:9 step:9331 [D loss: 0.541609, acc.: 72.66%] [G loss: 0.656633]\n",
      "epoch:9 step:9332 [D loss: 0.468629, acc.: 76.56%] [G loss: 0.741720]\n",
      "epoch:9 step:9333 [D loss: 0.582478, acc.: 70.31%] [G loss: 0.587006]\n",
      "epoch:9 step:9334 [D loss: 0.509853, acc.: 73.44%] [G loss: 0.510469]\n",
      "epoch:9 step:9335 [D loss: 0.557802, acc.: 75.00%] [G loss: 0.602426]\n",
      "epoch:9 step:9336 [D loss: 0.568564, acc.: 71.09%] [G loss: 0.529963]\n",
      "epoch:9 step:9337 [D loss: 0.600151, acc.: 64.06%] [G loss: 0.523710]\n",
      "epoch:9 step:9338 [D loss: 0.562024, acc.: 67.97%] [G loss: 0.592954]\n",
      "epoch:9 step:9339 [D loss: 0.496184, acc.: 75.00%] [G loss: 0.517249]\n",
      "epoch:9 step:9340 [D loss: 0.541926, acc.: 69.53%] [G loss: 0.522634]\n",
      "epoch:9 step:9341 [D loss: 0.566129, acc.: 67.97%] [G loss: 0.563816]\n",
      "epoch:9 step:9342 [D loss: 0.506279, acc.: 72.66%] [G loss: 0.682762]\n",
      "epoch:9 step:9343 [D loss: 0.544605, acc.: 74.22%] [G loss: 0.633728]\n",
      "epoch:9 step:9344 [D loss: 0.509075, acc.: 78.12%] [G loss: 0.583376]\n",
      "epoch:9 step:9345 [D loss: 0.429993, acc.: 81.25%] [G loss: 0.639462]\n",
      "epoch:9 step:9346 [D loss: 0.567268, acc.: 68.75%] [G loss: 0.782426]\n",
      "epoch:9 step:9347 [D loss: 0.504735, acc.: 74.22%] [G loss: 0.800673]\n",
      "epoch:9 step:9348 [D loss: 0.684019, acc.: 60.16%] [G loss: 0.595728]\n",
      "epoch:9 step:9349 [D loss: 0.544064, acc.: 72.66%] [G loss: 0.549291]\n",
      "epoch:9 step:9350 [D loss: 0.643307, acc.: 64.84%] [G loss: 0.489486]\n",
      "epoch:9 step:9351 [D loss: 0.508777, acc.: 75.00%] [G loss: 0.571830]\n",
      "epoch:9 step:9352 [D loss: 0.496205, acc.: 77.34%] [G loss: 0.783742]\n",
      "epoch:9 step:9353 [D loss: 0.769099, acc.: 56.25%] [G loss: 0.513090]\n",
      "epoch:9 step:9354 [D loss: 0.520641, acc.: 69.53%] [G loss: 0.690914]\n",
      "epoch:9 step:9355 [D loss: 0.545760, acc.: 75.78%] [G loss: 0.547263]\n",
      "epoch:9 step:9356 [D loss: 0.477884, acc.: 79.69%] [G loss: 0.649459]\n",
      "epoch:9 step:9357 [D loss: 0.420673, acc.: 83.59%] [G loss: 0.765362]\n",
      "epoch:9 step:9358 [D loss: 0.417117, acc.: 82.81%] [G loss: 0.780853]\n",
      "epoch:9 step:9359 [D loss: 0.408132, acc.: 84.38%] [G loss: 0.884742]\n",
      "epoch:9 step:9360 [D loss: 0.552785, acc.: 69.53%] [G loss: 1.116165]\n",
      "epoch:9 step:9361 [D loss: 0.699892, acc.: 66.41%] [G loss: 0.955138]\n",
      "epoch:9 step:9362 [D loss: 0.544727, acc.: 75.78%] [G loss: 0.771983]\n",
      "epoch:9 step:9363 [D loss: 0.456749, acc.: 72.66%] [G loss: 0.951425]\n",
      "epoch:9 step:9364 [D loss: 0.599758, acc.: 63.28%] [G loss: 0.764945]\n",
      "epoch:9 step:9365 [D loss: 0.606776, acc.: 68.75%] [G loss: 0.681742]\n",
      "epoch:9 step:9366 [D loss: 0.498979, acc.: 75.00%] [G loss: 0.693518]\n",
      "epoch:9 step:9367 [D loss: 0.585508, acc.: 63.28%] [G loss: 0.625007]\n",
      "epoch:9 step:9368 [D loss: 0.472653, acc.: 77.34%] [G loss: 0.914692]\n",
      "epoch:9 step:9369 [D loss: 0.412824, acc.: 82.03%] [G loss: 1.057309]\n",
      "epoch:9 step:9370 [D loss: 0.436332, acc.: 80.47%] [G loss: 0.974758]\n",
      "epoch:10 step:9371 [D loss: 0.606873, acc.: 71.88%] [G loss: 0.759186]\n",
      "epoch:10 step:9372 [D loss: 0.472001, acc.: 78.91%] [G loss: 0.892724]\n",
      "epoch:10 step:9373 [D loss: 0.604104, acc.: 68.75%] [G loss: 0.685670]\n",
      "epoch:10 step:9374 [D loss: 0.514986, acc.: 75.00%] [G loss: 0.728369]\n",
      "epoch:10 step:9375 [D loss: 0.582218, acc.: 72.66%] [G loss: 0.647460]\n",
      "epoch:10 step:9376 [D loss: 0.603005, acc.: 64.84%] [G loss: 0.538973]\n",
      "epoch:10 step:9377 [D loss: 0.485313, acc.: 79.69%] [G loss: 0.752883]\n",
      "epoch:10 step:9378 [D loss: 0.469570, acc.: 79.69%] [G loss: 0.763676]\n",
      "epoch:10 step:9379 [D loss: 0.476074, acc.: 80.47%] [G loss: 0.707873]\n",
      "epoch:10 step:9380 [D loss: 0.582922, acc.: 70.31%] [G loss: 0.581672]\n",
      "epoch:10 step:9381 [D loss: 0.483072, acc.: 78.12%] [G loss: 0.702914]\n",
      "epoch:10 step:9382 [D loss: 0.578725, acc.: 69.53%] [G loss: 0.509085]\n",
      "epoch:10 step:9383 [D loss: 0.515653, acc.: 75.00%] [G loss: 0.550194]\n",
      "epoch:10 step:9384 [D loss: 0.559937, acc.: 69.53%] [G loss: 0.632522]\n",
      "epoch:10 step:9385 [D loss: 0.492682, acc.: 75.00%] [G loss: 0.577907]\n",
      "epoch:10 step:9386 [D loss: 0.463902, acc.: 77.34%] [G loss: 0.889830]\n",
      "epoch:10 step:9387 [D loss: 0.579853, acc.: 67.97%] [G loss: 0.572133]\n",
      "epoch:10 step:9388 [D loss: 0.628598, acc.: 64.84%] [G loss: 0.567659]\n",
      "epoch:10 step:9389 [D loss: 0.553704, acc.: 65.62%] [G loss: 0.555366]\n",
      "epoch:10 step:9390 [D loss: 0.680771, acc.: 60.16%] [G loss: 0.466293]\n",
      "epoch:10 step:9391 [D loss: 0.529294, acc.: 73.44%] [G loss: 0.568805]\n",
      "epoch:10 step:9392 [D loss: 0.465462, acc.: 80.47%] [G loss: 0.624825]\n",
      "epoch:10 step:9393 [D loss: 0.550457, acc.: 69.53%] [G loss: 0.505393]\n",
      "epoch:10 step:9394 [D loss: 0.555541, acc.: 73.44%] [G loss: 0.682617]\n",
      "epoch:10 step:9395 [D loss: 0.539577, acc.: 74.22%] [G loss: 0.618324]\n",
      "epoch:10 step:9396 [D loss: 0.586313, acc.: 63.28%] [G loss: 0.605500]\n",
      "epoch:10 step:9397 [D loss: 0.495474, acc.: 75.78%] [G loss: 0.644601]\n",
      "epoch:10 step:9398 [D loss: 0.583256, acc.: 67.19%] [G loss: 0.434802]\n",
      "epoch:10 step:9399 [D loss: 0.502631, acc.: 74.22%] [G loss: 0.666058]\n",
      "epoch:10 step:9400 [D loss: 0.540243, acc.: 69.53%] [G loss: 0.551607]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.519505\n",
      "FID: 41.733757\n",
      "0 = 13.261688314151797\n",
      "1 = 0.10248731488144709\n",
      "2 = 0.9229000210762024\n",
      "3 = 0.8704000115394592\n",
      "4 = 0.9753999710083008\n",
      "5 = 0.9725139737129211\n",
      "6 = 0.8704000115394592\n",
      "7 = 7.955168333864213\n",
      "8 = 0.13108237325159516\n",
      "9 = 0.7483000159263611\n",
      "10 = 0.7373999953269958\n",
      "11 = 0.7591999769210815\n",
      "12 = 0.7538335919380188\n",
      "13 = 0.7373999953269958\n",
      "14 = 6.519532203674316\n",
      "15 = 7.34609842300415\n",
      "16 = 0.35511037707328796\n",
      "17 = 6.519504547119141\n",
      "18 = 41.73375701904297\n",
      "epoch:10 step:9401 [D loss: 0.607531, acc.: 61.72%] [G loss: 0.597434]\n",
      "epoch:10 step:9402 [D loss: 0.584224, acc.: 65.62%] [G loss: 0.448608]\n",
      "epoch:10 step:9403 [D loss: 0.534728, acc.: 75.00%] [G loss: 0.581327]\n",
      "epoch:10 step:9404 [D loss: 0.516388, acc.: 75.00%] [G loss: 0.526801]\n",
      "epoch:10 step:9405 [D loss: 0.552274, acc.: 77.34%] [G loss: 0.439898]\n",
      "epoch:10 step:9406 [D loss: 0.534649, acc.: 71.88%] [G loss: 0.502105]\n",
      "epoch:10 step:9407 [D loss: 0.523947, acc.: 74.22%] [G loss: 0.670894]\n",
      "epoch:10 step:9408 [D loss: 0.600774, acc.: 65.62%] [G loss: 0.458992]\n",
      "epoch:10 step:9409 [D loss: 0.574811, acc.: 69.53%] [G loss: 0.640366]\n",
      "epoch:10 step:9410 [D loss: 0.420449, acc.: 79.69%] [G loss: 0.774105]\n",
      "epoch:10 step:9411 [D loss: 0.549908, acc.: 69.53%] [G loss: 0.597897]\n",
      "epoch:10 step:9412 [D loss: 0.530945, acc.: 74.22%] [G loss: 0.605967]\n",
      "epoch:10 step:9413 [D loss: 0.499348, acc.: 76.56%] [G loss: 0.615584]\n",
      "epoch:10 step:9414 [D loss: 0.613425, acc.: 63.28%] [G loss: 0.484566]\n",
      "epoch:10 step:9415 [D loss: 0.520192, acc.: 77.34%] [G loss: 0.541973]\n",
      "epoch:10 step:9416 [D loss: 0.560314, acc.: 69.53%] [G loss: 0.520152]\n",
      "epoch:10 step:9417 [D loss: 0.572510, acc.: 68.75%] [G loss: 0.505194]\n",
      "epoch:10 step:9418 [D loss: 0.535894, acc.: 70.31%] [G loss: 0.639772]\n",
      "epoch:10 step:9419 [D loss: 0.523194, acc.: 74.22%] [G loss: 0.644848]\n",
      "epoch:10 step:9420 [D loss: 0.562417, acc.: 70.31%] [G loss: 0.593887]\n",
      "epoch:10 step:9421 [D loss: 0.650286, acc.: 63.28%] [G loss: 0.436035]\n",
      "epoch:10 step:9422 [D loss: 0.558761, acc.: 66.41%] [G loss: 0.613271]\n",
      "epoch:10 step:9423 [D loss: 0.514228, acc.: 73.44%] [G loss: 0.564688]\n",
      "epoch:10 step:9424 [D loss: 0.489855, acc.: 71.88%] [G loss: 0.636852]\n",
      "epoch:10 step:9425 [D loss: 0.510887, acc.: 72.66%] [G loss: 0.520944]\n",
      "epoch:10 step:9426 [D loss: 0.557270, acc.: 70.31%] [G loss: 0.646238]\n",
      "epoch:10 step:9427 [D loss: 0.503134, acc.: 75.00%] [G loss: 0.663136]\n",
      "epoch:10 step:9428 [D loss: 0.567306, acc.: 68.75%] [G loss: 0.650131]\n",
      "epoch:10 step:9429 [D loss: 0.475394, acc.: 77.34%] [G loss: 0.726875]\n",
      "epoch:10 step:9430 [D loss: 0.536180, acc.: 74.22%] [G loss: 0.606442]\n",
      "epoch:10 step:9431 [D loss: 0.548830, acc.: 68.75%] [G loss: 0.471895]\n",
      "epoch:10 step:9432 [D loss: 0.626436, acc.: 66.41%] [G loss: 0.448359]\n",
      "epoch:10 step:9433 [D loss: 0.574687, acc.: 68.75%] [G loss: 0.583266]\n",
      "epoch:10 step:9434 [D loss: 0.594238, acc.: 68.75%] [G loss: 0.550537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9435 [D loss: 0.508503, acc.: 75.00%] [G loss: 0.592689]\n",
      "epoch:10 step:9436 [D loss: 0.565605, acc.: 68.75%] [G loss: 0.604069]\n",
      "epoch:10 step:9437 [D loss: 0.605384, acc.: 67.19%] [G loss: 0.359656]\n",
      "epoch:10 step:9438 [D loss: 0.503116, acc.: 74.22%] [G loss: 0.565205]\n",
      "epoch:10 step:9439 [D loss: 0.484703, acc.: 77.34%] [G loss: 0.551752]\n",
      "epoch:10 step:9440 [D loss: 0.520162, acc.: 69.53%] [G loss: 0.521199]\n",
      "epoch:10 step:9441 [D loss: 0.523784, acc.: 71.88%] [G loss: 0.555070]\n",
      "epoch:10 step:9442 [D loss: 0.516877, acc.: 71.88%] [G loss: 0.523567]\n",
      "epoch:10 step:9443 [D loss: 0.565906, acc.: 68.75%] [G loss: 0.439654]\n",
      "epoch:10 step:9444 [D loss: 0.476737, acc.: 78.12%] [G loss: 0.575465]\n",
      "epoch:10 step:9445 [D loss: 0.540970, acc.: 75.00%] [G loss: 0.556242]\n",
      "epoch:10 step:9446 [D loss: 0.550886, acc.: 67.97%] [G loss: 0.674390]\n",
      "epoch:10 step:9447 [D loss: 0.453066, acc.: 78.12%] [G loss: 0.650415]\n",
      "epoch:10 step:9448 [D loss: 0.645612, acc.: 64.84%] [G loss: 0.575411]\n",
      "epoch:10 step:9449 [D loss: 0.627868, acc.: 64.84%] [G loss: 0.593814]\n",
      "epoch:10 step:9450 [D loss: 0.576374, acc.: 71.88%] [G loss: 0.575717]\n",
      "epoch:10 step:9451 [D loss: 0.597600, acc.: 64.84%] [G loss: 0.579471]\n",
      "epoch:10 step:9452 [D loss: 0.518985, acc.: 72.66%] [G loss: 0.585561]\n",
      "epoch:10 step:9453 [D loss: 0.514294, acc.: 75.00%] [G loss: 0.497600]\n",
      "epoch:10 step:9454 [D loss: 0.539869, acc.: 66.41%] [G loss: 0.634899]\n",
      "epoch:10 step:9455 [D loss: 0.558721, acc.: 72.66%] [G loss: 0.529910]\n",
      "epoch:10 step:9456 [D loss: 0.549380, acc.: 73.44%] [G loss: 0.526971]\n",
      "epoch:10 step:9457 [D loss: 0.534496, acc.: 72.66%] [G loss: 0.525430]\n",
      "epoch:10 step:9458 [D loss: 0.526753, acc.: 74.22%] [G loss: 0.551244]\n",
      "epoch:10 step:9459 [D loss: 0.513145, acc.: 74.22%] [G loss: 0.615822]\n",
      "epoch:10 step:9460 [D loss: 0.506286, acc.: 76.56%] [G loss: 0.648833]\n",
      "epoch:10 step:9461 [D loss: 0.526899, acc.: 76.56%] [G loss: 0.626644]\n",
      "epoch:10 step:9462 [D loss: 0.459013, acc.: 75.78%] [G loss: 0.744099]\n",
      "epoch:10 step:9463 [D loss: 0.533753, acc.: 72.66%] [G loss: 0.578613]\n",
      "epoch:10 step:9464 [D loss: 0.526449, acc.: 75.00%] [G loss: 0.643590]\n",
      "epoch:10 step:9465 [D loss: 0.550027, acc.: 69.53%] [G loss: 0.572975]\n",
      "epoch:10 step:9466 [D loss: 0.516449, acc.: 75.78%] [G loss: 0.644863]\n",
      "epoch:10 step:9467 [D loss: 0.531763, acc.: 72.66%] [G loss: 0.610318]\n",
      "epoch:10 step:9468 [D loss: 0.550380, acc.: 69.53%] [G loss: 0.630612]\n",
      "epoch:10 step:9469 [D loss: 0.561743, acc.: 73.44%] [G loss: 0.636617]\n",
      "epoch:10 step:9470 [D loss: 0.482304, acc.: 71.88%] [G loss: 0.795467]\n",
      "epoch:10 step:9471 [D loss: 0.510027, acc.: 74.22%] [G loss: 0.694767]\n",
      "epoch:10 step:9472 [D loss: 0.635089, acc.: 58.59%] [G loss: 0.498057]\n",
      "epoch:10 step:9473 [D loss: 0.559000, acc.: 68.75%] [G loss: 0.538595]\n",
      "epoch:10 step:9474 [D loss: 0.496572, acc.: 73.44%] [G loss: 0.676454]\n",
      "epoch:10 step:9475 [D loss: 0.551717, acc.: 67.97%] [G loss: 0.462362]\n",
      "epoch:10 step:9476 [D loss: 0.540320, acc.: 72.66%] [G loss: 0.545717]\n",
      "epoch:10 step:9477 [D loss: 0.573996, acc.: 71.09%] [G loss: 0.589546]\n",
      "epoch:10 step:9478 [D loss: 0.620635, acc.: 63.28%] [G loss: 0.618630]\n",
      "epoch:10 step:9479 [D loss: 0.600461, acc.: 69.53%] [G loss: 0.542204]\n",
      "epoch:10 step:9480 [D loss: 0.578000, acc.: 65.62%] [G loss: 0.479227]\n",
      "epoch:10 step:9481 [D loss: 0.554848, acc.: 69.53%] [G loss: 0.563163]\n",
      "epoch:10 step:9482 [D loss: 0.568599, acc.: 70.31%] [G loss: 0.528201]\n",
      "epoch:10 step:9483 [D loss: 0.542572, acc.: 75.78%] [G loss: 0.510783]\n",
      "epoch:10 step:9484 [D loss: 0.573954, acc.: 68.75%] [G loss: 0.442252]\n",
      "epoch:10 step:9485 [D loss: 0.511056, acc.: 78.91%] [G loss: 0.527516]\n",
      "epoch:10 step:9486 [D loss: 0.546710, acc.: 70.31%] [G loss: 0.596216]\n",
      "epoch:10 step:9487 [D loss: 0.504108, acc.: 75.00%] [G loss: 0.656895]\n",
      "epoch:10 step:9488 [D loss: 0.524642, acc.: 71.88%] [G loss: 0.631636]\n",
      "epoch:10 step:9489 [D loss: 0.483078, acc.: 77.34%] [G loss: 0.941338]\n",
      "epoch:10 step:9490 [D loss: 0.562845, acc.: 71.88%] [G loss: 0.632452]\n",
      "epoch:10 step:9491 [D loss: 0.650011, acc.: 61.72%] [G loss: 0.491870]\n",
      "epoch:10 step:9492 [D loss: 0.499374, acc.: 80.47%] [G loss: 0.581305]\n",
      "epoch:10 step:9493 [D loss: 0.548485, acc.: 73.44%] [G loss: 0.625848]\n",
      "epoch:10 step:9494 [D loss: 0.549261, acc.: 70.31%] [G loss: 0.641245]\n",
      "epoch:10 step:9495 [D loss: 0.526076, acc.: 71.88%] [G loss: 0.593559]\n",
      "epoch:10 step:9496 [D loss: 0.510087, acc.: 71.09%] [G loss: 0.567636]\n",
      "epoch:10 step:9497 [D loss: 0.479863, acc.: 76.56%] [G loss: 0.670185]\n",
      "epoch:10 step:9498 [D loss: 0.496469, acc.: 78.91%] [G loss: 0.534221]\n",
      "epoch:10 step:9499 [D loss: 0.582057, acc.: 64.84%] [G loss: 0.554249]\n",
      "epoch:10 step:9500 [D loss: 0.573908, acc.: 67.19%] [G loss: 0.503620]\n",
      "epoch:10 step:9501 [D loss: 0.510975, acc.: 71.09%] [G loss: 0.559145]\n",
      "epoch:10 step:9502 [D loss: 0.566442, acc.: 70.31%] [G loss: 0.523793]\n",
      "epoch:10 step:9503 [D loss: 0.544596, acc.: 69.53%] [G loss: 0.467938]\n",
      "epoch:10 step:9504 [D loss: 0.515554, acc.: 75.00%] [G loss: 0.514310]\n",
      "epoch:10 step:9505 [D loss: 0.558947, acc.: 72.66%] [G loss: 0.583509]\n",
      "epoch:10 step:9506 [D loss: 0.613397, acc.: 66.41%] [G loss: 0.771963]\n",
      "epoch:10 step:9507 [D loss: 0.596621, acc.: 64.84%] [G loss: 0.658891]\n",
      "epoch:10 step:9508 [D loss: 0.648304, acc.: 66.41%] [G loss: 0.498394]\n",
      "epoch:10 step:9509 [D loss: 0.581093, acc.: 67.97%] [G loss: 0.548019]\n",
      "epoch:10 step:9510 [D loss: 0.600467, acc.: 65.62%] [G loss: 0.448801]\n",
      "epoch:10 step:9511 [D loss: 0.496724, acc.: 70.31%] [G loss: 0.757171]\n",
      "epoch:10 step:9512 [D loss: 0.586388, acc.: 63.28%] [G loss: 0.465401]\n",
      "epoch:10 step:9513 [D loss: 0.625181, acc.: 60.94%] [G loss: 0.466495]\n",
      "epoch:10 step:9514 [D loss: 0.507308, acc.: 76.56%] [G loss: 0.442434]\n",
      "epoch:10 step:9515 [D loss: 0.537856, acc.: 69.53%] [G loss: 0.615503]\n",
      "epoch:10 step:9516 [D loss: 0.481790, acc.: 77.34%] [G loss: 0.622624]\n",
      "epoch:10 step:9517 [D loss: 0.634295, acc.: 66.41%] [G loss: 0.482057]\n",
      "epoch:10 step:9518 [D loss: 0.563640, acc.: 68.75%] [G loss: 0.464077]\n",
      "epoch:10 step:9519 [D loss: 0.513402, acc.: 75.00%] [G loss: 0.626767]\n",
      "epoch:10 step:9520 [D loss: 0.637394, acc.: 66.41%] [G loss: 0.543894]\n",
      "epoch:10 step:9521 [D loss: 0.548601, acc.: 74.22%] [G loss: 0.450495]\n",
      "epoch:10 step:9522 [D loss: 0.493878, acc.: 78.12%] [G loss: 0.692547]\n",
      "epoch:10 step:9523 [D loss: 0.654009, acc.: 58.59%] [G loss: 0.447811]\n",
      "epoch:10 step:9524 [D loss: 0.577821, acc.: 68.75%] [G loss: 0.605127]\n",
      "epoch:10 step:9525 [D loss: 0.478939, acc.: 76.56%] [G loss: 0.603535]\n",
      "epoch:10 step:9526 [D loss: 0.542077, acc.: 73.44%] [G loss: 0.638097]\n",
      "epoch:10 step:9527 [D loss: 0.559350, acc.: 67.97%] [G loss: 0.534635]\n",
      "epoch:10 step:9528 [D loss: 0.601305, acc.: 69.53%] [G loss: 0.547111]\n",
      "epoch:10 step:9529 [D loss: 0.494596, acc.: 75.00%] [G loss: 0.693811]\n",
      "epoch:10 step:9530 [D loss: 0.585105, acc.: 68.75%] [G loss: 0.538705]\n",
      "epoch:10 step:9531 [D loss: 0.572576, acc.: 66.41%] [G loss: 0.605449]\n",
      "epoch:10 step:9532 [D loss: 0.461018, acc.: 76.56%] [G loss: 0.612224]\n",
      "epoch:10 step:9533 [D loss: 0.558161, acc.: 71.09%] [G loss: 0.535382]\n",
      "epoch:10 step:9534 [D loss: 0.570978, acc.: 68.75%] [G loss: 0.608507]\n",
      "epoch:10 step:9535 [D loss: 0.488205, acc.: 74.22%] [G loss: 0.681733]\n",
      "epoch:10 step:9536 [D loss: 0.555336, acc.: 70.31%] [G loss: 0.635857]\n",
      "epoch:10 step:9537 [D loss: 0.626960, acc.: 61.72%] [G loss: 0.597050]\n",
      "epoch:10 step:9538 [D loss: 0.570849, acc.: 68.75%] [G loss: 0.396855]\n",
      "epoch:10 step:9539 [D loss: 0.578876, acc.: 67.97%] [G loss: 0.549441]\n",
      "epoch:10 step:9540 [D loss: 0.548028, acc.: 70.31%] [G loss: 0.494919]\n",
      "epoch:10 step:9541 [D loss: 0.516117, acc.: 75.00%] [G loss: 0.518835]\n",
      "epoch:10 step:9542 [D loss: 0.583357, acc.: 70.31%] [G loss: 0.522999]\n",
      "epoch:10 step:9543 [D loss: 0.546721, acc.: 69.53%] [G loss: 0.551011]\n",
      "epoch:10 step:9544 [D loss: 0.608524, acc.: 64.84%] [G loss: 0.488393]\n",
      "epoch:10 step:9545 [D loss: 0.576875, acc.: 67.97%] [G loss: 0.518475]\n",
      "epoch:10 step:9546 [D loss: 0.561158, acc.: 68.75%] [G loss: 0.434395]\n",
      "epoch:10 step:9547 [D loss: 0.530014, acc.: 75.78%] [G loss: 0.455435]\n",
      "epoch:10 step:9548 [D loss: 0.563824, acc.: 67.97%] [G loss: 0.510338]\n",
      "epoch:10 step:9549 [D loss: 0.512263, acc.: 75.00%] [G loss: 0.460922]\n",
      "epoch:10 step:9550 [D loss: 0.644045, acc.: 62.50%] [G loss: 0.482891]\n",
      "epoch:10 step:9551 [D loss: 0.533446, acc.: 71.09%] [G loss: 0.455863]\n",
      "epoch:10 step:9552 [D loss: 0.582836, acc.: 74.22%] [G loss: 0.458264]\n",
      "epoch:10 step:9553 [D loss: 0.604112, acc.: 69.53%] [G loss: 0.638128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9554 [D loss: 0.516292, acc.: 72.66%] [G loss: 0.415908]\n",
      "epoch:10 step:9555 [D loss: 0.544272, acc.: 67.97%] [G loss: 0.605461]\n",
      "epoch:10 step:9556 [D loss: 0.555436, acc.: 73.44%] [G loss: 0.551118]\n",
      "epoch:10 step:9557 [D loss: 0.643388, acc.: 60.16%] [G loss: 0.586780]\n",
      "epoch:10 step:9558 [D loss: 0.510214, acc.: 72.66%] [G loss: 0.475926]\n",
      "epoch:10 step:9559 [D loss: 0.542864, acc.: 70.31%] [G loss: 0.568600]\n",
      "epoch:10 step:9560 [D loss: 0.512314, acc.: 75.78%] [G loss: 0.505966]\n",
      "epoch:10 step:9561 [D loss: 0.512087, acc.: 75.78%] [G loss: 0.574334]\n",
      "epoch:10 step:9562 [D loss: 0.500983, acc.: 75.00%] [G loss: 0.616749]\n",
      "epoch:10 step:9563 [D loss: 0.550548, acc.: 71.88%] [G loss: 0.568858]\n",
      "epoch:10 step:9564 [D loss: 0.456441, acc.: 78.91%] [G loss: 0.534896]\n",
      "epoch:10 step:9565 [D loss: 0.570365, acc.: 69.53%] [G loss: 0.606554]\n",
      "epoch:10 step:9566 [D loss: 0.648323, acc.: 60.94%] [G loss: 0.557927]\n",
      "epoch:10 step:9567 [D loss: 0.529158, acc.: 75.00%] [G loss: 0.611908]\n",
      "epoch:10 step:9568 [D loss: 0.495184, acc.: 78.91%] [G loss: 0.567297]\n",
      "epoch:10 step:9569 [D loss: 0.531220, acc.: 71.09%] [G loss: 0.618584]\n",
      "epoch:10 step:9570 [D loss: 0.595608, acc.: 63.28%] [G loss: 0.575762]\n",
      "epoch:10 step:9571 [D loss: 0.593014, acc.: 64.06%] [G loss: 0.558556]\n",
      "epoch:10 step:9572 [D loss: 0.559943, acc.: 65.62%] [G loss: 0.568621]\n",
      "epoch:10 step:9573 [D loss: 0.592490, acc.: 70.31%] [G loss: 0.537734]\n",
      "epoch:10 step:9574 [D loss: 0.548736, acc.: 73.44%] [G loss: 0.587121]\n",
      "epoch:10 step:9575 [D loss: 0.493410, acc.: 77.34%] [G loss: 0.640773]\n",
      "epoch:10 step:9576 [D loss: 0.475403, acc.: 76.56%] [G loss: 0.637542]\n",
      "epoch:10 step:9577 [D loss: 0.489643, acc.: 81.25%] [G loss: 0.771438]\n",
      "epoch:10 step:9578 [D loss: 0.458092, acc.: 78.12%] [G loss: 0.770215]\n",
      "epoch:10 step:9579 [D loss: 0.475497, acc.: 78.91%] [G loss: 0.675142]\n",
      "epoch:10 step:9580 [D loss: 0.739214, acc.: 52.34%] [G loss: 0.401392]\n",
      "epoch:10 step:9581 [D loss: 0.596494, acc.: 63.28%] [G loss: 0.465987]\n",
      "epoch:10 step:9582 [D loss: 0.572959, acc.: 72.66%] [G loss: 0.521485]\n",
      "epoch:10 step:9583 [D loss: 0.522993, acc.: 69.53%] [G loss: 0.631152]\n",
      "epoch:10 step:9584 [D loss: 0.640087, acc.: 58.59%] [G loss: 0.455305]\n",
      "epoch:10 step:9585 [D loss: 0.610299, acc.: 67.19%] [G loss: 0.403419]\n",
      "epoch:10 step:9586 [D loss: 0.574314, acc.: 65.62%] [G loss: 0.476621]\n",
      "epoch:10 step:9587 [D loss: 0.565198, acc.: 73.44%] [G loss: 0.532083]\n",
      "epoch:10 step:9588 [D loss: 0.501241, acc.: 75.00%] [G loss: 0.617399]\n",
      "epoch:10 step:9589 [D loss: 0.505621, acc.: 75.78%] [G loss: 0.664066]\n",
      "epoch:10 step:9590 [D loss: 0.638535, acc.: 67.19%] [G loss: 0.556973]\n",
      "epoch:10 step:9591 [D loss: 0.538399, acc.: 70.31%] [G loss: 0.533247]\n",
      "epoch:10 step:9592 [D loss: 0.485604, acc.: 75.78%] [G loss: 0.606672]\n",
      "epoch:10 step:9593 [D loss: 0.571524, acc.: 71.88%] [G loss: 0.646883]\n",
      "epoch:10 step:9594 [D loss: 0.572824, acc.: 72.66%] [G loss: 0.519514]\n",
      "epoch:10 step:9595 [D loss: 0.560593, acc.: 74.22%] [G loss: 0.658155]\n",
      "epoch:10 step:9596 [D loss: 0.576936, acc.: 67.19%] [G loss: 0.565675]\n",
      "epoch:10 step:9597 [D loss: 0.521978, acc.: 74.22%] [G loss: 0.550398]\n",
      "epoch:10 step:9598 [D loss: 0.619085, acc.: 61.72%] [G loss: 0.485099]\n",
      "epoch:10 step:9599 [D loss: 0.565918, acc.: 71.88%] [G loss: 0.515781]\n",
      "epoch:10 step:9600 [D loss: 0.543909, acc.: 71.09%] [G loss: 0.607910]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.337078\n",
      "FID: 45.791744\n",
      "0 = 12.930681906890896\n",
      "1 = 0.09027196588311266\n",
      "2 = 0.901199996471405\n",
      "3 = 0.8303999900817871\n",
      "4 = 0.972000002861023\n",
      "5 = 0.9673811793327332\n",
      "6 = 0.8303999900817871\n",
      "7 = 8.163796856999397\n",
      "8 = 0.1416541692419791\n",
      "9 = 0.7480000257492065\n",
      "10 = 0.7297999858856201\n",
      "11 = 0.7662000060081482\n",
      "12 = 0.7573682069778442\n",
      "13 = 0.7297999858856201\n",
      "14 = 6.337102890014648\n",
      "15 = 7.41011905670166\n",
      "16 = 0.36027127504348755\n",
      "17 = 6.337077617645264\n",
      "18 = 45.791744232177734\n",
      "epoch:10 step:9601 [D loss: 0.470556, acc.: 77.34%] [G loss: 0.623862]\n",
      "epoch:10 step:9602 [D loss: 0.501406, acc.: 78.12%] [G loss: 0.633048]\n",
      "epoch:10 step:9603 [D loss: 0.569529, acc.: 70.31%] [G loss: 0.580289]\n",
      "epoch:10 step:9604 [D loss: 0.544476, acc.: 69.53%] [G loss: 0.662931]\n",
      "epoch:10 step:9605 [D loss: 0.608709, acc.: 60.94%] [G loss: 0.529358]\n",
      "epoch:10 step:9606 [D loss: 0.541640, acc.: 75.78%] [G loss: 0.460535]\n",
      "epoch:10 step:9607 [D loss: 0.520022, acc.: 68.75%] [G loss: 0.510625]\n",
      "epoch:10 step:9608 [D loss: 0.617202, acc.: 67.97%] [G loss: 0.449471]\n",
      "epoch:10 step:9609 [D loss: 0.517221, acc.: 71.88%] [G loss: 0.562218]\n",
      "epoch:10 step:9610 [D loss: 0.549356, acc.: 73.44%] [G loss: 0.473285]\n",
      "epoch:10 step:9611 [D loss: 0.544422, acc.: 75.00%] [G loss: 0.555387]\n",
      "epoch:10 step:9612 [D loss: 0.533620, acc.: 71.88%] [G loss: 0.518808]\n",
      "epoch:10 step:9613 [D loss: 0.590661, acc.: 67.19%] [G loss: 0.439796]\n",
      "epoch:10 step:9614 [D loss: 0.504000, acc.: 71.09%] [G loss: 0.624568]\n",
      "epoch:10 step:9615 [D loss: 0.533857, acc.: 71.88%] [G loss: 0.569153]\n",
      "epoch:10 step:9616 [D loss: 0.612592, acc.: 64.06%] [G loss: 0.542157]\n",
      "epoch:10 step:9617 [D loss: 0.579923, acc.: 68.75%] [G loss: 0.613648]\n",
      "epoch:10 step:9618 [D loss: 0.520457, acc.: 72.66%] [G loss: 0.557284]\n",
      "epoch:10 step:9619 [D loss: 0.611198, acc.: 68.75%] [G loss: 0.441767]\n",
      "epoch:10 step:9620 [D loss: 0.621704, acc.: 60.94%] [G loss: 0.524362]\n",
      "epoch:10 step:9621 [D loss: 0.659196, acc.: 60.94%] [G loss: 0.542598]\n",
      "epoch:10 step:9622 [D loss: 0.563672, acc.: 71.88%] [G loss: 0.592040]\n",
      "epoch:10 step:9623 [D loss: 0.538631, acc.: 67.97%] [G loss: 0.567562]\n",
      "epoch:10 step:9624 [D loss: 0.550718, acc.: 73.44%] [G loss: 0.519436]\n",
      "epoch:10 step:9625 [D loss: 0.546096, acc.: 71.88%] [G loss: 0.534573]\n",
      "epoch:10 step:9626 [D loss: 0.517023, acc.: 70.31%] [G loss: 0.559980]\n",
      "epoch:10 step:9627 [D loss: 0.599597, acc.: 62.50%] [G loss: 0.468172]\n",
      "epoch:10 step:9628 [D loss: 0.554282, acc.: 68.75%] [G loss: 0.499751]\n",
      "epoch:10 step:9629 [D loss: 0.561272, acc.: 67.97%] [G loss: 0.435663]\n",
      "epoch:10 step:9630 [D loss: 0.589376, acc.: 64.84%] [G loss: 0.484635]\n",
      "epoch:10 step:9631 [D loss: 0.584248, acc.: 67.97%] [G loss: 0.503232]\n",
      "epoch:10 step:9632 [D loss: 0.570150, acc.: 69.53%] [G loss: 0.366222]\n",
      "epoch:10 step:9633 [D loss: 0.627073, acc.: 68.75%] [G loss: 0.422481]\n",
      "epoch:10 step:9634 [D loss: 0.537857, acc.: 74.22%] [G loss: 0.563209]\n",
      "epoch:10 step:9635 [D loss: 0.560030, acc.: 69.53%] [G loss: 0.483424]\n",
      "epoch:10 step:9636 [D loss: 0.613282, acc.: 61.72%] [G loss: 0.456696]\n",
      "epoch:10 step:9637 [D loss: 0.570036, acc.: 71.88%] [G loss: 0.353295]\n",
      "epoch:10 step:9638 [D loss: 0.560622, acc.: 66.41%] [G loss: 0.535263]\n",
      "epoch:10 step:9639 [D loss: 0.528539, acc.: 72.66%] [G loss: 0.555477]\n",
      "epoch:10 step:9640 [D loss: 0.504150, acc.: 78.12%] [G loss: 0.526585]\n",
      "epoch:10 step:9641 [D loss: 0.507600, acc.: 72.66%] [G loss: 0.607347]\n",
      "epoch:10 step:9642 [D loss: 0.571253, acc.: 71.09%] [G loss: 0.506489]\n",
      "epoch:10 step:9643 [D loss: 0.495936, acc.: 74.22%] [G loss: 0.621712]\n",
      "epoch:10 step:9644 [D loss: 0.601554, acc.: 65.62%] [G loss: 0.546258]\n",
      "epoch:10 step:9645 [D loss: 0.606858, acc.: 65.62%] [G loss: 0.537959]\n",
      "epoch:10 step:9646 [D loss: 0.493582, acc.: 78.91%] [G loss: 0.580839]\n",
      "epoch:10 step:9647 [D loss: 0.644345, acc.: 64.06%] [G loss: 0.428716]\n",
      "epoch:10 step:9648 [D loss: 0.604169, acc.: 65.62%] [G loss: 0.428735]\n",
      "epoch:10 step:9649 [D loss: 0.567479, acc.: 69.53%] [G loss: 0.342979]\n",
      "epoch:10 step:9650 [D loss: 0.557820, acc.: 70.31%] [G loss: 0.570489]\n",
      "epoch:10 step:9651 [D loss: 0.586134, acc.: 69.53%] [G loss: 0.509302]\n",
      "epoch:10 step:9652 [D loss: 0.579824, acc.: 69.53%] [G loss: 0.404763]\n",
      "epoch:10 step:9653 [D loss: 0.558093, acc.: 75.00%] [G loss: 0.448035]\n",
      "epoch:10 step:9654 [D loss: 0.510929, acc.: 77.34%] [G loss: 0.544647]\n",
      "epoch:10 step:9655 [D loss: 0.536239, acc.: 72.66%] [G loss: 0.516874]\n",
      "epoch:10 step:9656 [D loss: 0.469896, acc.: 78.12%] [G loss: 0.596717]\n",
      "epoch:10 step:9657 [D loss: 0.686518, acc.: 57.81%] [G loss: 0.464873]\n",
      "epoch:10 step:9658 [D loss: 0.564825, acc.: 67.97%] [G loss: 0.538463]\n",
      "epoch:10 step:9659 [D loss: 0.566726, acc.: 72.66%] [G loss: 0.523172]\n",
      "epoch:10 step:9660 [D loss: 0.621611, acc.: 57.81%] [G loss: 0.576468]\n",
      "epoch:10 step:9661 [D loss: 0.562929, acc.: 64.84%] [G loss: 0.553660]\n",
      "epoch:10 step:9662 [D loss: 0.542535, acc.: 68.75%] [G loss: 0.496228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9663 [D loss: 0.584450, acc.: 64.06%] [G loss: 0.512329]\n",
      "epoch:10 step:9664 [D loss: 0.548732, acc.: 69.53%] [G loss: 0.484062]\n",
      "epoch:10 step:9665 [D loss: 0.573255, acc.: 71.88%] [G loss: 0.621884]\n",
      "epoch:10 step:9666 [D loss: 0.515360, acc.: 71.09%] [G loss: 0.719145]\n",
      "epoch:10 step:9667 [D loss: 0.572283, acc.: 70.31%] [G loss: 0.502897]\n",
      "epoch:10 step:9668 [D loss: 0.493206, acc.: 76.56%] [G loss: 0.634302]\n",
      "epoch:10 step:9669 [D loss: 0.516469, acc.: 77.34%] [G loss: 0.850326]\n",
      "epoch:10 step:9670 [D loss: 0.540095, acc.: 71.88%] [G loss: 0.625255]\n",
      "epoch:10 step:9671 [D loss: 0.623658, acc.: 67.19%] [G loss: 0.505534]\n",
      "epoch:10 step:9672 [D loss: 0.561864, acc.: 66.41%] [G loss: 0.550717]\n",
      "epoch:10 step:9673 [D loss: 0.587166, acc.: 70.31%] [G loss: 0.516841]\n",
      "epoch:10 step:9674 [D loss: 0.513815, acc.: 69.53%] [G loss: 0.614577]\n",
      "epoch:10 step:9675 [D loss: 0.558813, acc.: 71.09%] [G loss: 0.607996]\n",
      "epoch:10 step:9676 [D loss: 0.553462, acc.: 68.75%] [G loss: 0.605244]\n",
      "epoch:10 step:9677 [D loss: 0.505609, acc.: 77.34%] [G loss: 0.633805]\n",
      "epoch:10 step:9678 [D loss: 0.627000, acc.: 66.41%] [G loss: 0.544462]\n",
      "epoch:10 step:9679 [D loss: 0.588721, acc.: 62.50%] [G loss: 0.617182]\n",
      "epoch:10 step:9680 [D loss: 0.544947, acc.: 73.44%] [G loss: 0.623467]\n",
      "epoch:10 step:9681 [D loss: 0.476319, acc.: 78.91%] [G loss: 0.572625]\n",
      "epoch:10 step:9682 [D loss: 0.450449, acc.: 81.25%] [G loss: 0.751058]\n",
      "epoch:10 step:9683 [D loss: 0.537535, acc.: 73.44%] [G loss: 0.838766]\n",
      "epoch:10 step:9684 [D loss: 0.431628, acc.: 81.25%] [G loss: 0.808099]\n",
      "epoch:10 step:9685 [D loss: 0.471222, acc.: 77.34%] [G loss: 0.812137]\n",
      "epoch:10 step:9686 [D loss: 0.717806, acc.: 62.50%] [G loss: 0.609173]\n",
      "epoch:10 step:9687 [D loss: 0.602403, acc.: 64.06%] [G loss: 0.515600]\n",
      "epoch:10 step:9688 [D loss: 0.558935, acc.: 68.75%] [G loss: 0.534960]\n",
      "epoch:10 step:9689 [D loss: 0.575722, acc.: 70.31%] [G loss: 0.468663]\n",
      "epoch:10 step:9690 [D loss: 0.540680, acc.: 70.31%] [G loss: 0.499845]\n",
      "epoch:10 step:9691 [D loss: 0.535114, acc.: 69.53%] [G loss: 0.601245]\n",
      "epoch:10 step:9692 [D loss: 0.558219, acc.: 67.97%] [G loss: 0.644800]\n",
      "epoch:10 step:9693 [D loss: 0.656526, acc.: 63.28%] [G loss: 0.575014]\n",
      "epoch:10 step:9694 [D loss: 0.544428, acc.: 75.00%] [G loss: 0.499051]\n",
      "epoch:10 step:9695 [D loss: 0.505176, acc.: 74.22%] [G loss: 0.575585]\n",
      "epoch:10 step:9696 [D loss: 0.479236, acc.: 77.34%] [G loss: 0.545576]\n",
      "epoch:10 step:9697 [D loss: 0.524936, acc.: 70.31%] [G loss: 0.602408]\n",
      "epoch:10 step:9698 [D loss: 0.525225, acc.: 73.44%] [G loss: 0.749805]\n",
      "epoch:10 step:9699 [D loss: 0.574890, acc.: 66.41%] [G loss: 0.669180]\n",
      "epoch:10 step:9700 [D loss: 0.558496, acc.: 67.19%] [G loss: 0.584102]\n",
      "epoch:10 step:9701 [D loss: 0.529529, acc.: 71.88%] [G loss: 0.607536]\n",
      "epoch:10 step:9702 [D loss: 0.531198, acc.: 69.53%] [G loss: 0.476446]\n",
      "epoch:10 step:9703 [D loss: 0.502723, acc.: 75.78%] [G loss: 0.542644]\n",
      "epoch:10 step:9704 [D loss: 0.508208, acc.: 74.22%] [G loss: 0.696838]\n",
      "epoch:10 step:9705 [D loss: 0.526734, acc.: 70.31%] [G loss: 0.678395]\n",
      "epoch:10 step:9706 [D loss: 0.501132, acc.: 78.12%] [G loss: 0.578231]\n",
      "epoch:10 step:9707 [D loss: 0.549567, acc.: 74.22%] [G loss: 0.618106]\n",
      "epoch:10 step:9708 [D loss: 0.545185, acc.: 72.66%] [G loss: 0.545628]\n",
      "epoch:10 step:9709 [D loss: 0.616423, acc.: 65.62%] [G loss: 0.608689]\n",
      "epoch:10 step:9710 [D loss: 0.532561, acc.: 71.09%] [G loss: 0.589045]\n",
      "epoch:10 step:9711 [D loss: 0.569217, acc.: 69.53%] [G loss: 0.656794]\n",
      "epoch:10 step:9712 [D loss: 0.662782, acc.: 59.38%] [G loss: 0.486454]\n",
      "epoch:10 step:9713 [D loss: 0.509781, acc.: 73.44%] [G loss: 0.717452]\n",
      "epoch:10 step:9714 [D loss: 0.494634, acc.: 75.78%] [G loss: 0.857122]\n",
      "epoch:10 step:9715 [D loss: 0.536979, acc.: 68.75%] [G loss: 0.774125]\n",
      "epoch:10 step:9716 [D loss: 0.544027, acc.: 68.75%] [G loss: 0.672067]\n",
      "epoch:10 step:9717 [D loss: 0.437057, acc.: 81.25%] [G loss: 0.899696]\n",
      "epoch:10 step:9718 [D loss: 0.641412, acc.: 67.19%] [G loss: 0.582496]\n",
      "epoch:10 step:9719 [D loss: 0.686217, acc.: 57.81%] [G loss: 0.431840]\n",
      "epoch:10 step:9720 [D loss: 0.484926, acc.: 77.34%] [G loss: 0.480809]\n",
      "epoch:10 step:9721 [D loss: 0.567384, acc.: 70.31%] [G loss: 0.522684]\n",
      "epoch:10 step:9722 [D loss: 0.549985, acc.: 69.53%] [G loss: 0.711567]\n",
      "epoch:10 step:9723 [D loss: 0.575884, acc.: 64.06%] [G loss: 0.492798]\n",
      "epoch:10 step:9724 [D loss: 0.397246, acc.: 85.16%] [G loss: 0.621644]\n",
      "epoch:10 step:9725 [D loss: 0.551482, acc.: 71.09%] [G loss: 0.697654]\n",
      "epoch:10 step:9726 [D loss: 0.582497, acc.: 64.84%] [G loss: 0.663855]\n",
      "epoch:10 step:9727 [D loss: 0.484134, acc.: 75.78%] [G loss: 0.712192]\n",
      "epoch:10 step:9728 [D loss: 0.471637, acc.: 77.34%] [G loss: 0.707314]\n",
      "epoch:10 step:9729 [D loss: 0.465423, acc.: 78.91%] [G loss: 0.774359]\n",
      "epoch:10 step:9730 [D loss: 0.504965, acc.: 74.22%] [G loss: 0.713188]\n",
      "epoch:10 step:9731 [D loss: 0.509275, acc.: 75.78%] [G loss: 0.800076]\n",
      "epoch:10 step:9732 [D loss: 0.632296, acc.: 61.72%] [G loss: 0.537037]\n",
      "epoch:10 step:9733 [D loss: 0.571872, acc.: 65.62%] [G loss: 0.549598]\n",
      "epoch:10 step:9734 [D loss: 0.514139, acc.: 74.22%] [G loss: 0.566174]\n",
      "epoch:10 step:9735 [D loss: 0.561657, acc.: 68.75%] [G loss: 0.408866]\n",
      "epoch:10 step:9736 [D loss: 0.503891, acc.: 75.00%] [G loss: 0.575941]\n",
      "epoch:10 step:9737 [D loss: 0.642974, acc.: 62.50%] [G loss: 0.499675]\n",
      "epoch:10 step:9738 [D loss: 0.581742, acc.: 64.84%] [G loss: 0.468400]\n",
      "epoch:10 step:9739 [D loss: 0.501180, acc.: 75.00%] [G loss: 0.596576]\n",
      "epoch:10 step:9740 [D loss: 0.563469, acc.: 72.66%] [G loss: 0.611146]\n",
      "epoch:10 step:9741 [D loss: 0.479349, acc.: 78.91%] [G loss: 0.690774]\n",
      "epoch:10 step:9742 [D loss: 0.566605, acc.: 67.19%] [G loss: 0.743764]\n",
      "epoch:10 step:9743 [D loss: 0.604219, acc.: 64.06%] [G loss: 0.479792]\n",
      "epoch:10 step:9744 [D loss: 0.417834, acc.: 82.81%] [G loss: 0.797446]\n",
      "epoch:10 step:9745 [D loss: 0.583369, acc.: 67.97%] [G loss: 0.601488]\n",
      "epoch:10 step:9746 [D loss: 0.734578, acc.: 50.78%] [G loss: 0.349565]\n",
      "epoch:10 step:9747 [D loss: 0.601197, acc.: 64.06%] [G loss: 0.374181]\n",
      "epoch:10 step:9748 [D loss: 0.534601, acc.: 71.09%] [G loss: 0.461623]\n",
      "epoch:10 step:9749 [D loss: 0.581463, acc.: 68.75%] [G loss: 0.446963]\n",
      "epoch:10 step:9750 [D loss: 0.599830, acc.: 69.53%] [G loss: 0.463431]\n",
      "epoch:10 step:9751 [D loss: 0.448711, acc.: 81.25%] [G loss: 0.655955]\n",
      "epoch:10 step:9752 [D loss: 0.521675, acc.: 71.88%] [G loss: 0.646282]\n",
      "epoch:10 step:9753 [D loss: 0.527756, acc.: 71.88%] [G loss: 0.633496]\n",
      "epoch:10 step:9754 [D loss: 0.540084, acc.: 72.66%] [G loss: 0.605585]\n",
      "epoch:10 step:9755 [D loss: 0.535139, acc.: 73.44%] [G loss: 0.490120]\n",
      "epoch:10 step:9756 [D loss: 0.608368, acc.: 63.28%] [G loss: 0.621710]\n",
      "epoch:10 step:9757 [D loss: 0.608814, acc.: 64.06%] [G loss: 0.566117]\n",
      "epoch:10 step:9758 [D loss: 0.592629, acc.: 68.75%] [G loss: 0.599610]\n",
      "epoch:10 step:9759 [D loss: 0.518765, acc.: 73.44%] [G loss: 0.626193]\n",
      "epoch:10 step:9760 [D loss: 0.613099, acc.: 68.75%] [G loss: 0.410589]\n",
      "epoch:10 step:9761 [D loss: 0.546786, acc.: 72.66%] [G loss: 0.555190]\n",
      "epoch:10 step:9762 [D loss: 0.469245, acc.: 75.78%] [G loss: 0.516880]\n",
      "epoch:10 step:9763 [D loss: 0.614369, acc.: 64.84%] [G loss: 0.408271]\n",
      "epoch:10 step:9764 [D loss: 0.580154, acc.: 67.19%] [G loss: 0.505034]\n",
      "epoch:10 step:9765 [D loss: 0.515794, acc.: 72.66%] [G loss: 0.453227]\n",
      "epoch:10 step:9766 [D loss: 0.644836, acc.: 62.50%] [G loss: 0.613249]\n",
      "epoch:10 step:9767 [D loss: 0.555944, acc.: 67.19%] [G loss: 0.505153]\n",
      "epoch:10 step:9768 [D loss: 0.461791, acc.: 74.22%] [G loss: 0.617356]\n",
      "epoch:10 step:9769 [D loss: 0.505194, acc.: 75.00%] [G loss: 0.564328]\n",
      "epoch:10 step:9770 [D loss: 0.689226, acc.: 57.03%] [G loss: 0.572594]\n",
      "epoch:10 step:9771 [D loss: 0.638703, acc.: 62.50%] [G loss: 0.445990]\n",
      "epoch:10 step:9772 [D loss: 0.538139, acc.: 71.09%] [G loss: 0.594198]\n",
      "epoch:10 step:9773 [D loss: 0.460628, acc.: 79.69%] [G loss: 0.592623]\n",
      "epoch:10 step:9774 [D loss: 0.618279, acc.: 57.03%] [G loss: 0.515567]\n",
      "epoch:10 step:9775 [D loss: 0.531608, acc.: 75.78%] [G loss: 0.513485]\n",
      "epoch:10 step:9776 [D loss: 0.486853, acc.: 78.12%] [G loss: 0.710819]\n",
      "epoch:10 step:9777 [D loss: 0.564429, acc.: 68.75%] [G loss: 0.616048]\n",
      "epoch:10 step:9778 [D loss: 0.643636, acc.: 64.06%] [G loss: 0.583370]\n",
      "epoch:10 step:9779 [D loss: 0.589460, acc.: 64.84%] [G loss: 0.644682]\n",
      "epoch:10 step:9780 [D loss: 0.545517, acc.: 68.75%] [G loss: 0.592504]\n",
      "epoch:10 step:9781 [D loss: 0.551189, acc.: 67.97%] [G loss: 0.552706]\n",
      "epoch:10 step:9782 [D loss: 0.609686, acc.: 64.84%] [G loss: 0.425464]\n",
      "epoch:10 step:9783 [D loss: 0.590456, acc.: 71.09%] [G loss: 0.505260]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9784 [D loss: 0.539395, acc.: 70.31%] [G loss: 0.597672]\n",
      "epoch:10 step:9785 [D loss: 0.606754, acc.: 66.41%] [G loss: 0.468353]\n",
      "epoch:10 step:9786 [D loss: 0.546597, acc.: 69.53%] [G loss: 0.548995]\n",
      "epoch:10 step:9787 [D loss: 0.594719, acc.: 60.94%] [G loss: 0.642303]\n",
      "epoch:10 step:9788 [D loss: 0.638814, acc.: 65.62%] [G loss: 0.506423]\n",
      "epoch:10 step:9789 [D loss: 0.555926, acc.: 68.75%] [G loss: 0.510821]\n",
      "epoch:10 step:9790 [D loss: 0.605826, acc.: 64.84%] [G loss: 0.477488]\n",
      "epoch:10 step:9791 [D loss: 0.608405, acc.: 68.75%] [G loss: 0.484114]\n",
      "epoch:10 step:9792 [D loss: 0.595239, acc.: 64.06%] [G loss: 0.552191]\n",
      "epoch:10 step:9793 [D loss: 0.572890, acc.: 67.97%] [G loss: 0.543318]\n",
      "epoch:10 step:9794 [D loss: 0.645701, acc.: 62.50%] [G loss: 0.426532]\n",
      "epoch:10 step:9795 [D loss: 0.563106, acc.: 70.31%] [G loss: 0.581519]\n",
      "epoch:10 step:9796 [D loss: 0.492396, acc.: 71.09%] [G loss: 0.644369]\n",
      "epoch:10 step:9797 [D loss: 0.532499, acc.: 72.66%] [G loss: 0.567642]\n",
      "epoch:10 step:9798 [D loss: 0.545878, acc.: 73.44%] [G loss: 0.610968]\n",
      "epoch:10 step:9799 [D loss: 0.469691, acc.: 75.78%] [G loss: 0.597364]\n",
      "epoch:10 step:9800 [D loss: 0.537739, acc.: 66.41%] [G loss: 0.605131]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.409464\n",
      "FID: 47.770222\n",
      "0 = 12.765771359443633\n",
      "1 = 0.08927071309090837\n",
      "2 = 0.8970000147819519\n",
      "3 = 0.8270000219345093\n",
      "4 = 0.9670000076293945\n",
      "5 = 0.9616279006004333\n",
      "6 = 0.8270000219345093\n",
      "7 = 8.225568985819802\n",
      "8 = 0.14988166591910607\n",
      "9 = 0.7336999773979187\n",
      "10 = 0.7164000272750854\n",
      "11 = 0.7509999871253967\n",
      "12 = 0.7420758008956909\n",
      "13 = 0.7164000272750854\n",
      "14 = 6.409492492675781\n",
      "15 = 7.375570297241211\n",
      "16 = 0.35950738191604614\n",
      "17 = 6.409463882446289\n",
      "18 = 47.77022171020508\n",
      "epoch:10 step:9801 [D loss: 0.550169, acc.: 72.66%] [G loss: 0.663692]\n",
      "epoch:10 step:9802 [D loss: 0.584699, acc.: 62.50%] [G loss: 0.528211]\n",
      "epoch:10 step:9803 [D loss: 0.569248, acc.: 69.53%] [G loss: 0.568905]\n",
      "epoch:10 step:9804 [D loss: 0.516787, acc.: 71.88%] [G loss: 0.602155]\n",
      "epoch:10 step:9805 [D loss: 0.600774, acc.: 67.97%] [G loss: 0.626745]\n",
      "epoch:10 step:9806 [D loss: 0.502718, acc.: 75.78%] [G loss: 0.515626]\n",
      "epoch:10 step:9807 [D loss: 0.634412, acc.: 67.19%] [G loss: 0.456662]\n",
      "epoch:10 step:9808 [D loss: 0.577339, acc.: 61.72%] [G loss: 0.501020]\n",
      "epoch:10 step:9809 [D loss: 0.526218, acc.: 68.75%] [G loss: 0.557083]\n",
      "epoch:10 step:9810 [D loss: 0.532134, acc.: 64.84%] [G loss: 0.524992]\n",
      "epoch:10 step:9811 [D loss: 0.554542, acc.: 66.41%] [G loss: 0.644241]\n",
      "epoch:10 step:9812 [D loss: 0.540385, acc.: 72.66%] [G loss: 0.484609]\n",
      "epoch:10 step:9813 [D loss: 0.523271, acc.: 71.09%] [G loss: 0.592938]\n",
      "epoch:10 step:9814 [D loss: 0.550874, acc.: 68.75%] [G loss: 0.682920]\n",
      "epoch:10 step:9815 [D loss: 0.534252, acc.: 71.88%] [G loss: 0.614248]\n",
      "epoch:10 step:9816 [D loss: 0.527225, acc.: 77.34%] [G loss: 0.662791]\n",
      "epoch:10 step:9817 [D loss: 0.508001, acc.: 75.00%] [G loss: 0.689268]\n",
      "epoch:10 step:9818 [D loss: 0.558354, acc.: 71.09%] [G loss: 0.656840]\n",
      "epoch:10 step:9819 [D loss: 0.522147, acc.: 75.78%] [G loss: 0.710094]\n",
      "epoch:10 step:9820 [D loss: 0.551826, acc.: 68.75%] [G loss: 0.557158]\n",
      "epoch:10 step:9821 [D loss: 0.472372, acc.: 77.34%] [G loss: 0.727761]\n",
      "epoch:10 step:9822 [D loss: 0.498965, acc.: 72.66%] [G loss: 0.656319]\n",
      "epoch:10 step:9823 [D loss: 0.526978, acc.: 72.66%] [G loss: 0.711264]\n",
      "epoch:10 step:9824 [D loss: 0.642590, acc.: 65.62%] [G loss: 0.543523]\n",
      "epoch:10 step:9825 [D loss: 0.572305, acc.: 71.88%] [G loss: 0.515225]\n",
      "epoch:10 step:9826 [D loss: 0.640460, acc.: 64.06%] [G loss: 0.584414]\n",
      "epoch:10 step:9827 [D loss: 0.577443, acc.: 67.97%] [G loss: 0.466510]\n",
      "epoch:10 step:9828 [D loss: 0.614864, acc.: 64.84%] [G loss: 0.502285]\n",
      "epoch:10 step:9829 [D loss: 0.531081, acc.: 72.66%] [G loss: 0.514701]\n",
      "epoch:10 step:9830 [D loss: 0.569710, acc.: 65.62%] [G loss: 0.442522]\n",
      "epoch:10 step:9831 [D loss: 0.551124, acc.: 69.53%] [G loss: 0.534881]\n",
      "epoch:10 step:9832 [D loss: 0.578603, acc.: 64.84%] [G loss: 0.571629]\n",
      "epoch:10 step:9833 [D loss: 0.554585, acc.: 66.41%] [G loss: 0.560334]\n",
      "epoch:10 step:9834 [D loss: 0.552304, acc.: 68.75%] [G loss: 0.661081]\n",
      "epoch:10 step:9835 [D loss: 0.599231, acc.: 70.31%] [G loss: 0.546413]\n",
      "epoch:10 step:9836 [D loss: 0.546710, acc.: 72.66%] [G loss: 0.574696]\n",
      "epoch:10 step:9837 [D loss: 0.585853, acc.: 70.31%] [G loss: 0.703010]\n",
      "epoch:10 step:9838 [D loss: 0.561665, acc.: 70.31%] [G loss: 0.602316]\n",
      "epoch:10 step:9839 [D loss: 0.539281, acc.: 69.53%] [G loss: 0.607129]\n",
      "epoch:10 step:9840 [D loss: 0.528994, acc.: 73.44%] [G loss: 0.675321]\n",
      "epoch:10 step:9841 [D loss: 0.481342, acc.: 74.22%] [G loss: 0.664249]\n",
      "epoch:10 step:9842 [D loss: 0.444842, acc.: 78.91%] [G loss: 0.799600]\n",
      "epoch:10 step:9843 [D loss: 0.654279, acc.: 61.72%] [G loss: 0.624439]\n",
      "epoch:10 step:9844 [D loss: 0.539267, acc.: 65.62%] [G loss: 0.664772]\n",
      "epoch:10 step:9845 [D loss: 0.457437, acc.: 80.47%] [G loss: 0.734478]\n",
      "epoch:10 step:9846 [D loss: 0.627764, acc.: 60.94%] [G loss: 0.597982]\n",
      "epoch:10 step:9847 [D loss: 0.691037, acc.: 63.28%] [G loss: 0.440539]\n",
      "epoch:10 step:9848 [D loss: 0.596727, acc.: 66.41%] [G loss: 0.457381]\n",
      "epoch:10 step:9849 [D loss: 0.516262, acc.: 77.34%] [G loss: 0.467954]\n",
      "epoch:10 step:9850 [D loss: 0.573625, acc.: 66.41%] [G loss: 0.522503]\n",
      "epoch:10 step:9851 [D loss: 0.534904, acc.: 75.00%] [G loss: 0.481949]\n",
      "epoch:10 step:9852 [D loss: 0.639681, acc.: 60.94%] [G loss: 0.422206]\n",
      "epoch:10 step:9853 [D loss: 0.523354, acc.: 71.88%] [G loss: 0.613354]\n",
      "epoch:10 step:9854 [D loss: 0.483265, acc.: 71.88%] [G loss: 0.598261]\n",
      "epoch:10 step:9855 [D loss: 0.519552, acc.: 76.56%] [G loss: 0.566154]\n",
      "epoch:10 step:9856 [D loss: 0.602354, acc.: 67.97%] [G loss: 0.687671]\n",
      "epoch:10 step:9857 [D loss: 0.564258, acc.: 71.09%] [G loss: 0.491260]\n",
      "epoch:10 step:9858 [D loss: 0.513007, acc.: 72.66%] [G loss: 0.594454]\n",
      "epoch:10 step:9859 [D loss: 0.532908, acc.: 75.78%] [G loss: 0.586031]\n",
      "epoch:10 step:9860 [D loss: 0.569560, acc.: 70.31%] [G loss: 0.500622]\n",
      "epoch:10 step:9861 [D loss: 0.553814, acc.: 68.75%] [G loss: 0.547783]\n",
      "epoch:10 step:9862 [D loss: 0.553254, acc.: 75.00%] [G loss: 0.458806]\n",
      "epoch:10 step:9863 [D loss: 0.568078, acc.: 65.62%] [G loss: 0.547575]\n",
      "epoch:10 step:9864 [D loss: 0.629191, acc.: 66.41%] [G loss: 0.447274]\n",
      "epoch:10 step:9865 [D loss: 0.463985, acc.: 81.25%] [G loss: 0.548969]\n",
      "epoch:10 step:9866 [D loss: 0.591860, acc.: 68.75%] [G loss: 0.499977]\n",
      "epoch:10 step:9867 [D loss: 0.577702, acc.: 67.97%] [G loss: 0.589596]\n",
      "epoch:10 step:9868 [D loss: 0.493774, acc.: 76.56%] [G loss: 0.638344]\n",
      "epoch:10 step:9869 [D loss: 0.500776, acc.: 70.31%] [G loss: 0.589810]\n",
      "epoch:10 step:9870 [D loss: 0.638316, acc.: 64.84%] [G loss: 0.579733]\n",
      "epoch:10 step:9871 [D loss: 0.635852, acc.: 65.62%] [G loss: 0.521182]\n",
      "epoch:10 step:9872 [D loss: 0.600375, acc.: 59.38%] [G loss: 0.442369]\n",
      "epoch:10 step:9873 [D loss: 0.485089, acc.: 78.91%] [G loss: 0.549827]\n",
      "epoch:10 step:9874 [D loss: 0.488088, acc.: 75.00%] [G loss: 0.612934]\n",
      "epoch:10 step:9875 [D loss: 0.551231, acc.: 71.09%] [G loss: 0.692743]\n",
      "epoch:10 step:9876 [D loss: 0.528918, acc.: 72.66%] [G loss: 0.710045]\n",
      "epoch:10 step:9877 [D loss: 0.536263, acc.: 72.66%] [G loss: 0.699964]\n",
      "epoch:10 step:9878 [D loss: 0.428181, acc.: 82.03%] [G loss: 0.759499]\n",
      "epoch:10 step:9879 [D loss: 0.509557, acc.: 75.78%] [G loss: 0.579216]\n",
      "epoch:10 step:9880 [D loss: 0.703130, acc.: 61.72%] [G loss: 0.623454]\n",
      "epoch:10 step:9881 [D loss: 0.659990, acc.: 61.72%] [G loss: 0.373365]\n",
      "epoch:10 step:9882 [D loss: 0.602992, acc.: 66.41%] [G loss: 0.433754]\n",
      "epoch:10 step:9883 [D loss: 0.546598, acc.: 73.44%] [G loss: 0.554703]\n",
      "epoch:10 step:9884 [D loss: 0.478149, acc.: 71.88%] [G loss: 0.597577]\n",
      "epoch:10 step:9885 [D loss: 0.540124, acc.: 74.22%] [G loss: 0.686595]\n",
      "epoch:10 step:9886 [D loss: 0.511404, acc.: 75.00%] [G loss: 0.527820]\n",
      "epoch:10 step:9887 [D loss: 0.564995, acc.: 67.97%] [G loss: 0.663744]\n",
      "epoch:10 step:9888 [D loss: 0.569448, acc.: 69.53%] [G loss: 0.729215]\n",
      "epoch:10 step:9889 [D loss: 0.476013, acc.: 73.44%] [G loss: 0.754135]\n",
      "epoch:10 step:9890 [D loss: 0.476197, acc.: 76.56%] [G loss: 0.672120]\n",
      "epoch:10 step:9891 [D loss: 0.508301, acc.: 75.00%] [G loss: 0.505594]\n",
      "epoch:10 step:9892 [D loss: 0.514191, acc.: 79.69%] [G loss: 0.765930]\n",
      "epoch:10 step:9893 [D loss: 0.479382, acc.: 75.00%] [G loss: 0.763823]\n",
      "epoch:10 step:9894 [D loss: 0.589057, acc.: 71.88%] [G loss: 0.586081]\n",
      "epoch:10 step:9895 [D loss: 0.596222, acc.: 71.88%] [G loss: 0.502224]\n",
      "epoch:10 step:9896 [D loss: 0.563034, acc.: 66.41%] [G loss: 0.439677]\n",
      "epoch:10 step:9897 [D loss: 0.583334, acc.: 67.19%] [G loss: 0.604904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9898 [D loss: 0.643033, acc.: 61.72%] [G loss: 0.509467]\n",
      "epoch:10 step:9899 [D loss: 0.583613, acc.: 63.28%] [G loss: 0.643218]\n",
      "epoch:10 step:9900 [D loss: 0.567939, acc.: 67.97%] [G loss: 0.536939]\n",
      "epoch:10 step:9901 [D loss: 0.595602, acc.: 67.19%] [G loss: 0.509335]\n",
      "epoch:10 step:9902 [D loss: 0.584071, acc.: 66.41%] [G loss: 0.508907]\n",
      "epoch:10 step:9903 [D loss: 0.538802, acc.: 71.09%] [G loss: 0.545644]\n",
      "epoch:10 step:9904 [D loss: 0.529501, acc.: 72.66%] [G loss: 0.583129]\n",
      "epoch:10 step:9905 [D loss: 0.603850, acc.: 67.19%] [G loss: 0.553913]\n",
      "epoch:10 step:9906 [D loss: 0.492191, acc.: 76.56%] [G loss: 0.510164]\n",
      "epoch:10 step:9907 [D loss: 0.567800, acc.: 67.97%] [G loss: 0.615760]\n",
      "epoch:10 step:9908 [D loss: 0.594420, acc.: 66.41%] [G loss: 0.499430]\n",
      "epoch:10 step:9909 [D loss: 0.551062, acc.: 71.88%] [G loss: 0.488763]\n",
      "epoch:10 step:9910 [D loss: 0.568739, acc.: 67.97%] [G loss: 0.543464]\n",
      "epoch:10 step:9911 [D loss: 0.588381, acc.: 67.97%] [G loss: 0.465489]\n",
      "epoch:10 step:9912 [D loss: 0.588545, acc.: 67.19%] [G loss: 0.572072]\n",
      "epoch:10 step:9913 [D loss: 0.576445, acc.: 64.84%] [G loss: 0.526520]\n",
      "epoch:10 step:9914 [D loss: 0.555474, acc.: 72.66%] [G loss: 0.553662]\n",
      "epoch:10 step:9915 [D loss: 0.581092, acc.: 67.97%] [G loss: 0.486675]\n",
      "epoch:10 step:9916 [D loss: 0.529990, acc.: 74.22%] [G loss: 0.542045]\n",
      "epoch:10 step:9917 [D loss: 0.576259, acc.: 73.44%] [G loss: 0.583594]\n",
      "epoch:10 step:9918 [D loss: 0.491974, acc.: 76.56%] [G loss: 0.641922]\n",
      "epoch:10 step:9919 [D loss: 0.541660, acc.: 71.09%] [G loss: 0.663294]\n",
      "epoch:10 step:9920 [D loss: 0.596276, acc.: 66.41%] [G loss: 0.539292]\n",
      "epoch:10 step:9921 [D loss: 0.522043, acc.: 71.88%] [G loss: 0.537159]\n",
      "epoch:10 step:9922 [D loss: 0.530849, acc.: 70.31%] [G loss: 0.512494]\n",
      "epoch:10 step:9923 [D loss: 0.610163, acc.: 65.62%] [G loss: 0.479250]\n",
      "epoch:10 step:9924 [D loss: 0.503664, acc.: 69.53%] [G loss: 0.638642]\n",
      "epoch:10 step:9925 [D loss: 0.512972, acc.: 78.12%] [G loss: 0.565429]\n",
      "epoch:10 step:9926 [D loss: 0.503523, acc.: 75.78%] [G loss: 0.593185]\n",
      "epoch:10 step:9927 [D loss: 0.551518, acc.: 69.53%] [G loss: 0.652286]\n",
      "epoch:10 step:9928 [D loss: 0.452869, acc.: 79.69%] [G loss: 0.718114]\n",
      "epoch:10 step:9929 [D loss: 0.578808, acc.: 68.75%] [G loss: 0.689211]\n",
      "epoch:10 step:9930 [D loss: 0.563283, acc.: 68.75%] [G loss: 0.559432]\n",
      "epoch:10 step:9931 [D loss: 0.554517, acc.: 69.53%] [G loss: 0.527383]\n",
      "epoch:10 step:9932 [D loss: 0.643618, acc.: 60.16%] [G loss: 0.528053]\n",
      "epoch:10 step:9933 [D loss: 0.570916, acc.: 68.75%] [G loss: 0.492437]\n",
      "epoch:10 step:9934 [D loss: 0.485438, acc.: 78.12%] [G loss: 0.606551]\n",
      "epoch:10 step:9935 [D loss: 0.567141, acc.: 67.19%] [G loss: 0.526562]\n",
      "epoch:10 step:9936 [D loss: 0.693631, acc.: 53.91%] [G loss: 0.459336]\n",
      "epoch:10 step:9937 [D loss: 0.483399, acc.: 75.78%] [G loss: 0.631979]\n",
      "epoch:10 step:9938 [D loss: 0.523650, acc.: 73.44%] [G loss: 0.540802]\n",
      "epoch:10 step:9939 [D loss: 0.540762, acc.: 71.88%] [G loss: 0.466853]\n",
      "epoch:10 step:9940 [D loss: 0.546251, acc.: 73.44%] [G loss: 0.536295]\n",
      "epoch:10 step:9941 [D loss: 0.509098, acc.: 71.88%] [G loss: 0.554101]\n",
      "epoch:10 step:9942 [D loss: 0.553264, acc.: 72.66%] [G loss: 0.527715]\n",
      "epoch:10 step:9943 [D loss: 0.564680, acc.: 71.88%] [G loss: 0.557499]\n",
      "epoch:10 step:9944 [D loss: 0.497151, acc.: 74.22%] [G loss: 0.683883]\n",
      "epoch:10 step:9945 [D loss: 0.491581, acc.: 75.00%] [G loss: 0.710516]\n",
      "epoch:10 step:9946 [D loss: 0.624307, acc.: 57.81%] [G loss: 0.588201]\n",
      "epoch:10 step:9947 [D loss: 0.641659, acc.: 64.84%] [G loss: 0.511082]\n",
      "epoch:10 step:9948 [D loss: 0.559715, acc.: 69.53%] [G loss: 0.604939]\n",
      "epoch:10 step:9949 [D loss: 0.559233, acc.: 67.97%] [G loss: 0.534117]\n",
      "epoch:10 step:9950 [D loss: 0.605980, acc.: 64.84%] [G loss: 0.540386]\n",
      "epoch:10 step:9951 [D loss: 0.494442, acc.: 80.47%] [G loss: 0.542986]\n",
      "epoch:10 step:9952 [D loss: 0.438021, acc.: 82.03%] [G loss: 0.785165]\n",
      "epoch:10 step:9953 [D loss: 0.623829, acc.: 64.84%] [G loss: 0.708293]\n",
      "epoch:10 step:9954 [D loss: 0.673674, acc.: 58.59%] [G loss: 0.623472]\n",
      "epoch:10 step:9955 [D loss: 0.571533, acc.: 67.97%] [G loss: 0.503729]\n",
      "epoch:10 step:9956 [D loss: 0.568317, acc.: 66.41%] [G loss: 0.562293]\n",
      "epoch:10 step:9957 [D loss: 0.587869, acc.: 65.62%] [G loss: 0.369742]\n",
      "epoch:10 step:9958 [D loss: 0.559471, acc.: 67.97%] [G loss: 0.494635]\n",
      "epoch:10 step:9959 [D loss: 0.521514, acc.: 69.53%] [G loss: 0.819686]\n",
      "epoch:10 step:9960 [D loss: 0.582302, acc.: 71.09%] [G loss: 0.590720]\n",
      "epoch:10 step:9961 [D loss: 0.545097, acc.: 74.22%] [G loss: 0.539530]\n",
      "epoch:10 step:9962 [D loss: 0.522172, acc.: 71.09%] [G loss: 0.683652]\n",
      "epoch:10 step:9963 [D loss: 0.504189, acc.: 77.34%] [G loss: 0.668545]\n",
      "epoch:10 step:9964 [D loss: 0.584151, acc.: 69.53%] [G loss: 0.455220]\n",
      "epoch:10 step:9965 [D loss: 0.586737, acc.: 67.19%] [G loss: 0.438683]\n",
      "epoch:10 step:9966 [D loss: 0.515188, acc.: 74.22%] [G loss: 0.614679]\n",
      "epoch:10 step:9967 [D loss: 0.525368, acc.: 72.66%] [G loss: 0.652592]\n",
      "epoch:10 step:9968 [D loss: 0.534780, acc.: 72.66%] [G loss: 0.594554]\n",
      "epoch:10 step:9969 [D loss: 0.594983, acc.: 68.75%] [G loss: 0.564925]\n",
      "epoch:10 step:9970 [D loss: 0.667269, acc.: 57.81%] [G loss: 0.492916]\n",
      "epoch:10 step:9971 [D loss: 0.513147, acc.: 75.78%] [G loss: 0.585186]\n",
      "epoch:10 step:9972 [D loss: 0.507521, acc.: 72.66%] [G loss: 0.637273]\n",
      "epoch:10 step:9973 [D loss: 0.515028, acc.: 76.56%] [G loss: 0.444568]\n",
      "epoch:10 step:9974 [D loss: 0.585641, acc.: 67.97%] [G loss: 0.410027]\n",
      "epoch:10 step:9975 [D loss: 0.476337, acc.: 78.12%] [G loss: 0.760238]\n",
      "epoch:10 step:9976 [D loss: 0.585100, acc.: 67.19%] [G loss: 0.555998]\n",
      "epoch:10 step:9977 [D loss: 0.542314, acc.: 70.31%] [G loss: 0.434063]\n",
      "epoch:10 step:9978 [D loss: 0.561361, acc.: 67.97%] [G loss: 0.504004]\n",
      "epoch:10 step:9979 [D loss: 0.518044, acc.: 69.53%] [G loss: 0.483816]\n",
      "epoch:10 step:9980 [D loss: 0.560244, acc.: 67.97%] [G loss: 0.469952]\n",
      "epoch:10 step:9981 [D loss: 0.515842, acc.: 73.44%] [G loss: 0.592121]\n",
      "epoch:10 step:9982 [D loss: 0.567661, acc.: 68.75%] [G loss: 0.599162]\n",
      "epoch:10 step:9983 [D loss: 0.514469, acc.: 72.66%] [G loss: 0.456444]\n",
      "epoch:10 step:9984 [D loss: 0.598157, acc.: 64.06%] [G loss: 0.366100]\n",
      "epoch:10 step:9985 [D loss: 0.625925, acc.: 64.84%] [G loss: 0.582984]\n",
      "epoch:10 step:9986 [D loss: 0.558255, acc.: 71.88%] [G loss: 0.610001]\n",
      "epoch:10 step:9987 [D loss: 0.556961, acc.: 71.88%] [G loss: 0.657269]\n",
      "epoch:10 step:9988 [D loss: 0.559900, acc.: 70.31%] [G loss: 0.485027]\n",
      "epoch:10 step:9989 [D loss: 0.524769, acc.: 73.44%] [G loss: 0.539058]\n",
      "epoch:10 step:9990 [D loss: 0.527804, acc.: 71.09%] [G loss: 0.614433]\n",
      "epoch:10 step:9991 [D loss: 0.591126, acc.: 67.97%] [G loss: 0.516348]\n",
      "epoch:10 step:9992 [D loss: 0.587745, acc.: 67.97%] [G loss: 0.478586]\n",
      "epoch:10 step:9993 [D loss: 0.510668, acc.: 72.66%] [G loss: 0.627968]\n",
      "epoch:10 step:9994 [D loss: 0.527243, acc.: 75.00%] [G loss: 0.579912]\n",
      "epoch:10 step:9995 [D loss: 0.590746, acc.: 66.41%] [G loss: 0.496315]\n",
      "epoch:10 step:9996 [D loss: 0.554239, acc.: 73.44%] [G loss: 0.435956]\n",
      "epoch:10 step:9997 [D loss: 0.595601, acc.: 64.84%] [G loss: 0.488859]\n",
      "epoch:10 step:9998 [D loss: 0.559264, acc.: 68.75%] [G loss: 0.540414]\n",
      "epoch:10 step:9999 [D loss: 0.550876, acc.: 69.53%] [G loss: 0.516810]\n",
      "epoch:10 step:10000 [D loss: 0.537189, acc.: 72.66%] [G loss: 0.535833]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.399171\n",
      "FID: 37.244503\n",
      "0 = 12.89481226568223\n",
      "1 = 0.09371566143587878\n",
      "2 = 0.8980000019073486\n",
      "3 = 0.8349999785423279\n",
      "4 = 0.9610000252723694\n",
      "5 = 0.9553775787353516\n",
      "6 = 0.8349999785423279\n",
      "7 = 7.76396489756108\n",
      "8 = 0.12072550248298024\n",
      "9 = 0.7386000156402588\n",
      "10 = 0.7242000102996826\n",
      "11 = 0.753000020980835\n",
      "12 = 0.7456754446029663\n",
      "13 = 0.7242000102996826\n",
      "14 = 6.399198055267334\n",
      "15 = 7.5534138679504395\n",
      "16 = 0.34296685457229614\n",
      "17 = 6.399171352386475\n",
      "18 = 37.244503021240234\n",
      "epoch:10 step:10001 [D loss: 0.536965, acc.: 74.22%] [G loss: 0.679329]\n",
      "epoch:10 step:10002 [D loss: 0.475905, acc.: 80.47%] [G loss: 0.704936]\n",
      "epoch:10 step:10003 [D loss: 0.514702, acc.: 76.56%] [G loss: 0.764782]\n",
      "epoch:10 step:10004 [D loss: 0.487946, acc.: 75.00%] [G loss: 0.715205]\n",
      "epoch:10 step:10005 [D loss: 0.476872, acc.: 74.22%] [G loss: 0.592056]\n",
      "epoch:10 step:10006 [D loss: 0.593194, acc.: 68.75%] [G loss: 0.511560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10007 [D loss: 0.527985, acc.: 74.22%] [G loss: 0.555616]\n",
      "epoch:10 step:10008 [D loss: 0.573138, acc.: 70.31%] [G loss: 0.444476]\n",
      "epoch:10 step:10009 [D loss: 0.555114, acc.: 70.31%] [G loss: 0.565722]\n",
      "epoch:10 step:10010 [D loss: 0.587078, acc.: 60.16%] [G loss: 0.602944]\n",
      "epoch:10 step:10011 [D loss: 0.543186, acc.: 71.09%] [G loss: 0.547546]\n",
      "epoch:10 step:10012 [D loss: 0.499460, acc.: 78.91%] [G loss: 0.761976]\n",
      "epoch:10 step:10013 [D loss: 0.534846, acc.: 72.66%] [G loss: 0.553752]\n",
      "epoch:10 step:10014 [D loss: 0.584858, acc.: 64.84%] [G loss: 0.449571]\n",
      "epoch:10 step:10015 [D loss: 0.568557, acc.: 71.09%] [G loss: 0.440245]\n",
      "epoch:10 step:10016 [D loss: 0.533151, acc.: 72.66%] [G loss: 0.503727]\n",
      "epoch:10 step:10017 [D loss: 0.453969, acc.: 78.12%] [G loss: 0.676022]\n",
      "epoch:10 step:10018 [D loss: 0.425971, acc.: 79.69%] [G loss: 0.728681]\n",
      "epoch:10 step:10019 [D loss: 0.507301, acc.: 75.00%] [G loss: 0.724025]\n",
      "epoch:10 step:10020 [D loss: 0.481325, acc.: 75.78%] [G loss: 0.812318]\n",
      "epoch:10 step:10021 [D loss: 0.536818, acc.: 73.44%] [G loss: 0.685486]\n",
      "epoch:10 step:10022 [D loss: 0.601486, acc.: 67.19%] [G loss: 0.525402]\n",
      "epoch:10 step:10023 [D loss: 0.655554, acc.: 60.94%] [G loss: 0.387545]\n",
      "epoch:10 step:10024 [D loss: 0.488733, acc.: 75.78%] [G loss: 0.592907]\n",
      "epoch:10 step:10025 [D loss: 0.568645, acc.: 68.75%] [G loss: 0.542105]\n",
      "epoch:10 step:10026 [D loss: 0.573402, acc.: 70.31%] [G loss: 0.567996]\n",
      "epoch:10 step:10027 [D loss: 0.505215, acc.: 71.09%] [G loss: 0.586095]\n",
      "epoch:10 step:10028 [D loss: 0.602500, acc.: 64.84%] [G loss: 0.547723]\n",
      "epoch:10 step:10029 [D loss: 0.527486, acc.: 72.66%] [G loss: 0.633575]\n",
      "epoch:10 step:10030 [D loss: 0.533877, acc.: 75.00%] [G loss: 0.638120]\n",
      "epoch:10 step:10031 [D loss: 0.488310, acc.: 75.00%] [G loss: 0.695740]\n",
      "epoch:10 step:10032 [D loss: 0.552312, acc.: 75.00%] [G loss: 0.686555]\n",
      "epoch:10 step:10033 [D loss: 0.578931, acc.: 70.31%] [G loss: 0.670338]\n",
      "epoch:10 step:10034 [D loss: 0.519213, acc.: 75.00%] [G loss: 0.620017]\n",
      "epoch:10 step:10035 [D loss: 0.611680, acc.: 64.06%] [G loss: 0.571631]\n",
      "epoch:10 step:10036 [D loss: 0.510345, acc.: 75.00%] [G loss: 0.600059]\n",
      "epoch:10 step:10037 [D loss: 0.619757, acc.: 64.84%] [G loss: 0.425469]\n",
      "epoch:10 step:10038 [D loss: 0.545451, acc.: 69.53%] [G loss: 0.659019]\n",
      "epoch:10 step:10039 [D loss: 0.581763, acc.: 66.41%] [G loss: 0.693979]\n",
      "epoch:10 step:10040 [D loss: 0.553889, acc.: 67.19%] [G loss: 0.499627]\n",
      "epoch:10 step:10041 [D loss: 0.553197, acc.: 72.66%] [G loss: 0.543685]\n",
      "epoch:10 step:10042 [D loss: 0.555791, acc.: 71.88%] [G loss: 0.575588]\n",
      "epoch:10 step:10043 [D loss: 0.632307, acc.: 64.84%] [G loss: 0.627068]\n",
      "epoch:10 step:10044 [D loss: 0.592128, acc.: 65.62%] [G loss: 0.495656]\n",
      "epoch:10 step:10045 [D loss: 0.596235, acc.: 64.06%] [G loss: 0.549086]\n",
      "epoch:10 step:10046 [D loss: 0.521052, acc.: 74.22%] [G loss: 0.580841]\n",
      "epoch:10 step:10047 [D loss: 0.520064, acc.: 68.75%] [G loss: 0.594675]\n",
      "epoch:10 step:10048 [D loss: 0.610511, acc.: 62.50%] [G loss: 0.525796]\n",
      "epoch:10 step:10049 [D loss: 0.542013, acc.: 69.53%] [G loss: 0.657634]\n",
      "epoch:10 step:10050 [D loss: 0.585146, acc.: 71.88%] [G loss: 0.599633]\n",
      "epoch:10 step:10051 [D loss: 0.524184, acc.: 71.88%] [G loss: 0.481710]\n",
      "epoch:10 step:10052 [D loss: 0.499964, acc.: 77.34%] [G loss: 0.626656]\n",
      "epoch:10 step:10053 [D loss: 0.588886, acc.: 65.62%] [G loss: 0.536619]\n",
      "epoch:10 step:10054 [D loss: 0.612358, acc.: 65.62%] [G loss: 0.405040]\n",
      "epoch:10 step:10055 [D loss: 0.544847, acc.: 71.88%] [G loss: 0.503158]\n",
      "epoch:10 step:10056 [D loss: 0.592178, acc.: 68.75%] [G loss: 0.463664]\n",
      "epoch:10 step:10057 [D loss: 0.555659, acc.: 71.88%] [G loss: 0.525247]\n",
      "epoch:10 step:10058 [D loss: 0.601480, acc.: 63.28%] [G loss: 0.457579]\n",
      "epoch:10 step:10059 [D loss: 0.562492, acc.: 61.72%] [G loss: 0.636546]\n",
      "epoch:10 step:10060 [D loss: 0.506997, acc.: 72.66%] [G loss: 0.687290]\n",
      "epoch:10 step:10061 [D loss: 0.509069, acc.: 73.44%] [G loss: 0.657922]\n",
      "epoch:10 step:10062 [D loss: 0.523278, acc.: 75.78%] [G loss: 0.580805]\n",
      "epoch:10 step:10063 [D loss: 0.466947, acc.: 77.34%] [G loss: 0.723195]\n",
      "epoch:10 step:10064 [D loss: 0.563736, acc.: 72.66%] [G loss: 0.490576]\n",
      "epoch:10 step:10065 [D loss: 0.605183, acc.: 69.53%] [G loss: 0.747481]\n",
      "epoch:10 step:10066 [D loss: 0.661991, acc.: 60.94%] [G loss: 0.562129]\n",
      "epoch:10 step:10067 [D loss: 0.543143, acc.: 70.31%] [G loss: 0.642028]\n",
      "epoch:10 step:10068 [D loss: 0.583926, acc.: 67.19%] [G loss: 0.546418]\n",
      "epoch:10 step:10069 [D loss: 0.512647, acc.: 75.00%] [G loss: 0.706659]\n",
      "epoch:10 step:10070 [D loss: 0.529427, acc.: 74.22%] [G loss: 0.603162]\n",
      "epoch:10 step:10071 [D loss: 0.565061, acc.: 69.53%] [G loss: 0.812347]\n",
      "epoch:10 step:10072 [D loss: 0.603846, acc.: 68.75%] [G loss: 0.499934]\n",
      "epoch:10 step:10073 [D loss: 0.596632, acc.: 66.41%] [G loss: 0.538188]\n",
      "epoch:10 step:10074 [D loss: 0.577803, acc.: 68.75%] [G loss: 0.544697]\n",
      "epoch:10 step:10075 [D loss: 0.498337, acc.: 76.56%] [G loss: 0.515860]\n",
      "epoch:10 step:10076 [D loss: 0.579799, acc.: 68.75%] [G loss: 0.588083]\n",
      "epoch:10 step:10077 [D loss: 0.571787, acc.: 67.97%] [G loss: 0.584696]\n",
      "epoch:10 step:10078 [D loss: 0.538738, acc.: 71.88%] [G loss: 0.540459]\n",
      "epoch:10 step:10079 [D loss: 0.603740, acc.: 67.97%] [G loss: 0.646430]\n",
      "epoch:10 step:10080 [D loss: 0.586018, acc.: 67.19%] [G loss: 0.485412]\n",
      "epoch:10 step:10081 [D loss: 0.552371, acc.: 69.53%] [G loss: 0.549456]\n",
      "epoch:10 step:10082 [D loss: 0.552082, acc.: 70.31%] [G loss: 0.431019]\n",
      "epoch:10 step:10083 [D loss: 0.582254, acc.: 69.53%] [G loss: 0.447105]\n",
      "epoch:10 step:10084 [D loss: 0.546746, acc.: 75.00%] [G loss: 0.475691]\n",
      "epoch:10 step:10085 [D loss: 0.598459, acc.: 64.84%] [G loss: 0.488576]\n",
      "epoch:10 step:10086 [D loss: 0.613620, acc.: 64.84%] [G loss: 0.508638]\n",
      "epoch:10 step:10087 [D loss: 0.582241, acc.: 71.09%] [G loss: 0.496751]\n",
      "epoch:10 step:10088 [D loss: 0.597736, acc.: 68.75%] [G loss: 0.552857]\n",
      "epoch:10 step:10089 [D loss: 0.492500, acc.: 73.44%] [G loss: 0.637767]\n",
      "epoch:10 step:10090 [D loss: 0.658655, acc.: 60.94%] [G loss: 0.542060]\n",
      "epoch:10 step:10091 [D loss: 0.599742, acc.: 64.84%] [G loss: 0.524688]\n",
      "epoch:10 step:10092 [D loss: 0.546988, acc.: 66.41%] [G loss: 0.602871]\n",
      "epoch:10 step:10093 [D loss: 0.594538, acc.: 63.28%] [G loss: 0.485756]\n",
      "epoch:10 step:10094 [D loss: 0.530614, acc.: 72.66%] [G loss: 0.462681]\n",
      "epoch:10 step:10095 [D loss: 0.517369, acc.: 75.00%] [G loss: 0.781062]\n",
      "epoch:10 step:10096 [D loss: 0.549048, acc.: 70.31%] [G loss: 0.627123]\n",
      "epoch:10 step:10097 [D loss: 0.594453, acc.: 68.75%] [G loss: 0.555949]\n",
      "epoch:10 step:10098 [D loss: 0.536306, acc.: 73.44%] [G loss: 0.527930]\n",
      "epoch:10 step:10099 [D loss: 0.604964, acc.: 67.19%] [G loss: 0.500077]\n",
      "epoch:10 step:10100 [D loss: 0.511230, acc.: 76.56%] [G loss: 0.558458]\n",
      "epoch:10 step:10101 [D loss: 0.615698, acc.: 60.16%] [G loss: 0.462550]\n",
      "epoch:10 step:10102 [D loss: 0.481610, acc.: 76.56%] [G loss: 0.668070]\n",
      "epoch:10 step:10103 [D loss: 0.587020, acc.: 66.41%] [G loss: 0.529807]\n",
      "epoch:10 step:10104 [D loss: 0.576859, acc.: 64.84%] [G loss: 0.495147]\n",
      "epoch:10 step:10105 [D loss: 0.626353, acc.: 64.06%] [G loss: 0.489444]\n",
      "epoch:10 step:10106 [D loss: 0.473265, acc.: 78.12%] [G loss: 0.692071]\n",
      "epoch:10 step:10107 [D loss: 0.518819, acc.: 75.00%] [G loss: 0.585735]\n",
      "epoch:10 step:10108 [D loss: 0.547082, acc.: 71.88%] [G loss: 0.582140]\n",
      "epoch:10 step:10109 [D loss: 0.693176, acc.: 58.59%] [G loss: 0.409202]\n",
      "epoch:10 step:10110 [D loss: 0.654893, acc.: 59.38%] [G loss: 0.399095]\n",
      "epoch:10 step:10111 [D loss: 0.492694, acc.: 78.12%] [G loss: 0.529540]\n",
      "epoch:10 step:10112 [D loss: 0.574974, acc.: 66.41%] [G loss: 0.570660]\n",
      "epoch:10 step:10113 [D loss: 0.563120, acc.: 71.88%] [G loss: 0.586940]\n",
      "epoch:10 step:10114 [D loss: 0.577172, acc.: 70.31%] [G loss: 0.531848]\n",
      "epoch:10 step:10115 [D loss: 0.571414, acc.: 67.19%] [G loss: 0.603540]\n",
      "epoch:10 step:10116 [D loss: 0.438385, acc.: 75.78%] [G loss: 0.714118]\n",
      "epoch:10 step:10117 [D loss: 0.470504, acc.: 75.78%] [G loss: 0.531821]\n",
      "epoch:10 step:10118 [D loss: 0.552534, acc.: 67.97%] [G loss: 0.545439]\n",
      "epoch:10 step:10119 [D loss: 0.564565, acc.: 71.88%] [G loss: 0.571960]\n",
      "epoch:10 step:10120 [D loss: 0.544496, acc.: 71.88%] [G loss: 0.656831]\n",
      "epoch:10 step:10121 [D loss: 0.518402, acc.: 71.09%] [G loss: 0.682564]\n",
      "epoch:10 step:10122 [D loss: 0.537763, acc.: 71.88%] [G loss: 0.663480]\n",
      "epoch:10 step:10123 [D loss: 0.529930, acc.: 74.22%] [G loss: 0.582459]\n",
      "epoch:10 step:10124 [D loss: 0.529373, acc.: 69.53%] [G loss: 0.628771]\n",
      "epoch:10 step:10125 [D loss: 0.533828, acc.: 67.97%] [G loss: 0.564084]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10126 [D loss: 0.608199, acc.: 63.28%] [G loss: 0.553575]\n",
      "epoch:10 step:10127 [D loss: 0.520932, acc.: 71.88%] [G loss: 0.632544]\n",
      "epoch:10 step:10128 [D loss: 0.556463, acc.: 70.31%] [G loss: 0.476386]\n",
      "epoch:10 step:10129 [D loss: 0.594312, acc.: 64.84%] [G loss: 0.489955]\n",
      "epoch:10 step:10130 [D loss: 0.611410, acc.: 60.16%] [G loss: 0.454723]\n",
      "epoch:10 step:10131 [D loss: 0.568266, acc.: 67.97%] [G loss: 0.500777]\n",
      "epoch:10 step:10132 [D loss: 0.584417, acc.: 66.41%] [G loss: 0.416972]\n",
      "epoch:10 step:10133 [D loss: 0.594638, acc.: 60.94%] [G loss: 0.552752]\n",
      "epoch:10 step:10134 [D loss: 0.545146, acc.: 70.31%] [G loss: 0.515514]\n",
      "epoch:10 step:10135 [D loss: 0.676588, acc.: 60.16%] [G loss: 0.414913]\n",
      "epoch:10 step:10136 [D loss: 0.674993, acc.: 54.69%] [G loss: 0.426164]\n",
      "epoch:10 step:10137 [D loss: 0.631072, acc.: 64.06%] [G loss: 0.527846]\n",
      "epoch:10 step:10138 [D loss: 0.547415, acc.: 74.22%] [G loss: 0.698424]\n",
      "epoch:10 step:10139 [D loss: 0.494887, acc.: 76.56%] [G loss: 0.709458]\n",
      "epoch:10 step:10140 [D loss: 0.576152, acc.: 65.62%] [G loss: 0.791426]\n",
      "epoch:10 step:10141 [D loss: 0.531283, acc.: 74.22%] [G loss: 0.618872]\n",
      "epoch:10 step:10142 [D loss: 0.541218, acc.: 66.41%] [G loss: 0.611381]\n",
      "epoch:10 step:10143 [D loss: 0.521473, acc.: 73.44%] [G loss: 0.593278]\n",
      "epoch:10 step:10144 [D loss: 0.561076, acc.: 72.66%] [G loss: 0.627931]\n",
      "epoch:10 step:10145 [D loss: 0.543549, acc.: 71.88%] [G loss: 0.742868]\n",
      "epoch:10 step:10146 [D loss: 0.623966, acc.: 64.84%] [G loss: 0.527481]\n",
      "epoch:10 step:10147 [D loss: 0.600237, acc.: 70.31%] [G loss: 0.556440]\n",
      "epoch:10 step:10148 [D loss: 0.603588, acc.: 66.41%] [G loss: 0.668238]\n",
      "epoch:10 step:10149 [D loss: 0.565038, acc.: 68.75%] [G loss: 0.582402]\n",
      "epoch:10 step:10150 [D loss: 0.566814, acc.: 67.19%] [G loss: 0.669775]\n",
      "epoch:10 step:10151 [D loss: 0.524018, acc.: 71.09%] [G loss: 0.641457]\n",
      "epoch:10 step:10152 [D loss: 0.571062, acc.: 68.75%] [G loss: 0.849522]\n",
      "epoch:10 step:10153 [D loss: 0.607127, acc.: 62.50%] [G loss: 0.731426]\n",
      "epoch:10 step:10154 [D loss: 0.626967, acc.: 66.41%] [G loss: 0.518421]\n",
      "epoch:10 step:10155 [D loss: 0.544586, acc.: 70.31%] [G loss: 0.507863]\n",
      "epoch:10 step:10156 [D loss: 0.527031, acc.: 75.00%] [G loss: 0.544336]\n",
      "epoch:10 step:10157 [D loss: 0.620621, acc.: 65.62%] [G loss: 0.518026]\n",
      "epoch:10 step:10158 [D loss: 0.670511, acc.: 64.06%] [G loss: 0.299850]\n",
      "epoch:10 step:10159 [D loss: 0.518624, acc.: 71.88%] [G loss: 0.469800]\n",
      "epoch:10 step:10160 [D loss: 0.515971, acc.: 75.00%] [G loss: 0.549294]\n",
      "epoch:10 step:10161 [D loss: 0.613210, acc.: 63.28%] [G loss: 0.458087]\n",
      "epoch:10 step:10162 [D loss: 0.481328, acc.: 73.44%] [G loss: 0.653429]\n",
      "epoch:10 step:10163 [D loss: 0.623549, acc.: 62.50%] [G loss: 0.523765]\n",
      "epoch:10 step:10164 [D loss: 0.611937, acc.: 64.84%] [G loss: 0.562616]\n",
      "epoch:10 step:10165 [D loss: 0.567563, acc.: 71.09%] [G loss: 0.515455]\n",
      "epoch:10 step:10166 [D loss: 0.566718, acc.: 66.41%] [G loss: 0.543724]\n",
      "epoch:10 step:10167 [D loss: 0.586493, acc.: 65.62%] [G loss: 0.688295]\n",
      "epoch:10 step:10168 [D loss: 0.567999, acc.: 63.28%] [G loss: 0.536038]\n",
      "epoch:10 step:10169 [D loss: 0.587597, acc.: 68.75%] [G loss: 0.465472]\n",
      "epoch:10 step:10170 [D loss: 0.546928, acc.: 71.88%] [G loss: 0.534500]\n",
      "epoch:10 step:10171 [D loss: 0.451717, acc.: 80.47%] [G loss: 0.624441]\n",
      "epoch:10 step:10172 [D loss: 0.484750, acc.: 75.78%] [G loss: 0.809112]\n",
      "epoch:10 step:10173 [D loss: 0.641925, acc.: 67.19%] [G loss: 0.561188]\n",
      "epoch:10 step:10174 [D loss: 0.570862, acc.: 69.53%] [G loss: 0.578548]\n",
      "epoch:10 step:10175 [D loss: 0.554247, acc.: 69.53%] [G loss: 0.605831]\n",
      "epoch:10 step:10176 [D loss: 0.565512, acc.: 67.97%] [G loss: 0.447944]\n",
      "epoch:10 step:10177 [D loss: 0.564173, acc.: 66.41%] [G loss: 0.492200]\n",
      "epoch:10 step:10178 [D loss: 0.604175, acc.: 64.06%] [G loss: 0.568639]\n",
      "epoch:10 step:10179 [D loss: 0.571242, acc.: 70.31%] [G loss: 0.403575]\n",
      "epoch:10 step:10180 [D loss: 0.573728, acc.: 66.41%] [G loss: 0.433243]\n",
      "epoch:10 step:10181 [D loss: 0.537584, acc.: 71.09%] [G loss: 0.515555]\n",
      "epoch:10 step:10182 [D loss: 0.600798, acc.: 64.84%] [G loss: 0.566131]\n",
      "epoch:10 step:10183 [D loss: 0.600535, acc.: 66.41%] [G loss: 0.347282]\n",
      "epoch:10 step:10184 [D loss: 0.531701, acc.: 70.31%] [G loss: 0.566006]\n",
      "epoch:10 step:10185 [D loss: 0.563068, acc.: 75.78%] [G loss: 0.588383]\n",
      "epoch:10 step:10186 [D loss: 0.580925, acc.: 68.75%] [G loss: 0.788908]\n",
      "epoch:10 step:10187 [D loss: 0.650148, acc.: 61.72%] [G loss: 0.692272]\n",
      "epoch:10 step:10188 [D loss: 0.549090, acc.: 71.88%] [G loss: 0.512393]\n",
      "epoch:10 step:10189 [D loss: 0.560802, acc.: 69.53%] [G loss: 0.438976]\n",
      "epoch:10 step:10190 [D loss: 0.600645, acc.: 67.97%] [G loss: 0.437498]\n",
      "epoch:10 step:10191 [D loss: 0.560346, acc.: 67.19%] [G loss: 0.457522]\n",
      "epoch:10 step:10192 [D loss: 0.549515, acc.: 66.41%] [G loss: 0.590656]\n",
      "epoch:10 step:10193 [D loss: 0.483931, acc.: 78.12%] [G loss: 0.602173]\n",
      "epoch:10 step:10194 [D loss: 0.579879, acc.: 67.19%] [G loss: 0.561343]\n",
      "epoch:10 step:10195 [D loss: 0.566791, acc.: 69.53%] [G loss: 0.512309]\n",
      "epoch:10 step:10196 [D loss: 0.547593, acc.: 66.41%] [G loss: 0.538596]\n",
      "epoch:10 step:10197 [D loss: 0.639503, acc.: 63.28%] [G loss: 0.512974]\n",
      "epoch:10 step:10198 [D loss: 0.642486, acc.: 58.59%] [G loss: 0.520736]\n",
      "epoch:10 step:10199 [D loss: 0.559468, acc.: 72.66%] [G loss: 0.491245]\n",
      "epoch:10 step:10200 [D loss: 0.558220, acc.: 67.97%] [G loss: 0.582317]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.435241\n",
      "FID: 43.971432\n",
      "0 = 12.789391459369643\n",
      "1 = 0.09197461391657233\n",
      "2 = 0.8938999772071838\n",
      "3 = 0.8226000070571899\n",
      "4 = 0.9652000069618225\n",
      "5 = 0.959412157535553\n",
      "6 = 0.8226000070571899\n",
      "7 = 8.005844200134263\n",
      "8 = 0.13406988590408903\n",
      "9 = 0.7279999852180481\n",
      "10 = 0.7135999798774719\n",
      "11 = 0.7423999905586243\n",
      "12 = 0.734761118888855\n",
      "13 = 0.7135999798774719\n",
      "14 = 6.435268402099609\n",
      "15 = 7.146385669708252\n",
      "16 = 0.3742992579936981\n",
      "17 = 6.435240745544434\n",
      "18 = 43.971431732177734\n",
      "epoch:10 step:10201 [D loss: 0.619317, acc.: 68.75%] [G loss: 0.556756]\n",
      "epoch:10 step:10202 [D loss: 0.508034, acc.: 72.66%] [G loss: 0.619286]\n",
      "epoch:10 step:10203 [D loss: 0.552954, acc.: 73.44%] [G loss: 0.458002]\n",
      "epoch:10 step:10204 [D loss: 0.514622, acc.: 74.22%] [G loss: 0.482963]\n",
      "epoch:10 step:10205 [D loss: 0.557651, acc.: 70.31%] [G loss: 0.462966]\n",
      "epoch:10 step:10206 [D loss: 0.551619, acc.: 69.53%] [G loss: 0.488486]\n",
      "epoch:10 step:10207 [D loss: 0.556081, acc.: 68.75%] [G loss: 0.483422]\n",
      "epoch:10 step:10208 [D loss: 0.515733, acc.: 75.00%] [G loss: 0.490146]\n",
      "epoch:10 step:10209 [D loss: 0.593124, acc.: 61.72%] [G loss: 0.517201]\n",
      "epoch:10 step:10210 [D loss: 0.602062, acc.: 64.06%] [G loss: 0.435206]\n",
      "epoch:10 step:10211 [D loss: 0.543880, acc.: 67.97%] [G loss: 0.426467]\n",
      "epoch:10 step:10212 [D loss: 0.519329, acc.: 74.22%] [G loss: 0.634668]\n",
      "epoch:10 step:10213 [D loss: 0.539998, acc.: 68.75%] [G loss: 0.561190]\n",
      "epoch:10 step:10214 [D loss: 0.565641, acc.: 67.19%] [G loss: 0.631925]\n",
      "epoch:10 step:10215 [D loss: 0.565450, acc.: 66.41%] [G loss: 0.524151]\n",
      "epoch:10 step:10216 [D loss: 0.597391, acc.: 64.84%] [G loss: 0.465925]\n",
      "epoch:10 step:10217 [D loss: 0.617725, acc.: 64.84%] [G loss: 0.411670]\n",
      "epoch:10 step:10218 [D loss: 0.556401, acc.: 66.41%] [G loss: 0.360483]\n",
      "epoch:10 step:10219 [D loss: 0.571206, acc.: 66.41%] [G loss: 0.352241]\n",
      "epoch:10 step:10220 [D loss: 0.540992, acc.: 71.09%] [G loss: 0.543314]\n",
      "epoch:10 step:10221 [D loss: 0.589325, acc.: 67.19%] [G loss: 0.496613]\n",
      "epoch:10 step:10222 [D loss: 0.559192, acc.: 70.31%] [G loss: 0.524149]\n",
      "epoch:10 step:10223 [D loss: 0.638373, acc.: 59.38%] [G loss: 0.576647]\n",
      "epoch:10 step:10224 [D loss: 0.572329, acc.: 61.72%] [G loss: 0.652699]\n",
      "epoch:10 step:10225 [D loss: 0.547642, acc.: 70.31%] [G loss: 0.641296]\n",
      "epoch:10 step:10226 [D loss: 0.609046, acc.: 66.41%] [G loss: 0.681752]\n",
      "epoch:10 step:10227 [D loss: 0.512671, acc.: 71.09%] [G loss: 0.603551]\n",
      "epoch:10 step:10228 [D loss: 0.664733, acc.: 60.16%] [G loss: 0.518592]\n",
      "epoch:10 step:10229 [D loss: 0.519087, acc.: 75.00%] [G loss: 0.598006]\n",
      "epoch:10 step:10230 [D loss: 0.457823, acc.: 78.12%] [G loss: 0.676035]\n",
      "epoch:10 step:10231 [D loss: 0.641790, acc.: 63.28%] [G loss: 0.517823]\n",
      "epoch:10 step:10232 [D loss: 0.589734, acc.: 64.84%] [G loss: 0.574580]\n",
      "epoch:10 step:10233 [D loss: 0.615795, acc.: 64.84%] [G loss: 0.352415]\n",
      "epoch:10 step:10234 [D loss: 0.562373, acc.: 67.19%] [G loss: 0.449993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10235 [D loss: 0.582907, acc.: 68.75%] [G loss: 0.376345]\n",
      "epoch:10 step:10236 [D loss: 0.551939, acc.: 67.97%] [G loss: 0.374736]\n",
      "epoch:10 step:10237 [D loss: 0.667699, acc.: 60.94%] [G loss: 0.457712]\n",
      "epoch:10 step:10238 [D loss: 0.597457, acc.: 65.62%] [G loss: 0.357718]\n",
      "epoch:10 step:10239 [D loss: 0.584001, acc.: 67.97%] [G loss: 0.457216]\n",
      "epoch:10 step:10240 [D loss: 0.457841, acc.: 81.25%] [G loss: 0.630931]\n",
      "epoch:10 step:10241 [D loss: 0.485538, acc.: 77.34%] [G loss: 0.589395]\n",
      "epoch:10 step:10242 [D loss: 0.579932, acc.: 62.50%] [G loss: 0.513762]\n",
      "epoch:10 step:10243 [D loss: 0.631984, acc.: 64.84%] [G loss: 0.523658]\n",
      "epoch:10 step:10244 [D loss: 0.602253, acc.: 69.53%] [G loss: 0.565776]\n",
      "epoch:10 step:10245 [D loss: 0.528107, acc.: 70.31%] [G loss: 0.636606]\n",
      "epoch:10 step:10246 [D loss: 0.587033, acc.: 66.41%] [G loss: 0.548130]\n",
      "epoch:10 step:10247 [D loss: 0.582148, acc.: 63.28%] [G loss: 0.583335]\n",
      "epoch:10 step:10248 [D loss: 0.560355, acc.: 67.19%] [G loss: 0.437384]\n",
      "epoch:10 step:10249 [D loss: 0.572580, acc.: 64.84%] [G loss: 0.435927]\n",
      "epoch:10 step:10250 [D loss: 0.656073, acc.: 62.50%] [G loss: 0.412847]\n",
      "epoch:10 step:10251 [D loss: 0.590442, acc.: 69.53%] [G loss: 0.375978]\n",
      "epoch:10 step:10252 [D loss: 0.616815, acc.: 62.50%] [G loss: 0.540635]\n",
      "epoch:10 step:10253 [D loss: 0.611445, acc.: 64.06%] [G loss: 0.501622]\n",
      "epoch:10 step:10254 [D loss: 0.509756, acc.: 75.78%] [G loss: 0.515253]\n",
      "epoch:10 step:10255 [D loss: 0.549904, acc.: 71.09%] [G loss: 0.599817]\n",
      "epoch:10 step:10256 [D loss: 0.500775, acc.: 75.78%] [G loss: 0.680117]\n",
      "epoch:10 step:10257 [D loss: 0.575949, acc.: 67.19%] [G loss: 0.587022]\n",
      "epoch:10 step:10258 [D loss: 0.596135, acc.: 64.84%] [G loss: 0.521629]\n",
      "epoch:10 step:10259 [D loss: 0.538148, acc.: 75.00%] [G loss: 0.547799]\n",
      "epoch:10 step:10260 [D loss: 0.503356, acc.: 76.56%] [G loss: 0.599005]\n",
      "epoch:10 step:10261 [D loss: 0.601030, acc.: 66.41%] [G loss: 0.577933]\n",
      "epoch:10 step:10262 [D loss: 0.644024, acc.: 58.59%] [G loss: 0.506261]\n",
      "epoch:10 step:10263 [D loss: 0.562874, acc.: 65.62%] [G loss: 0.543815]\n",
      "epoch:10 step:10264 [D loss: 0.497658, acc.: 78.12%] [G loss: 0.628195]\n",
      "epoch:10 step:10265 [D loss: 0.593182, acc.: 66.41%] [G loss: 0.614451]\n",
      "epoch:10 step:10266 [D loss: 0.530993, acc.: 74.22%] [G loss: 0.675593]\n",
      "epoch:10 step:10267 [D loss: 0.498034, acc.: 71.88%] [G loss: 0.608489]\n",
      "epoch:10 step:10268 [D loss: 0.466306, acc.: 82.03%] [G loss: 0.671857]\n",
      "epoch:10 step:10269 [D loss: 0.520716, acc.: 76.56%] [G loss: 0.677684]\n",
      "epoch:10 step:10270 [D loss: 0.542782, acc.: 73.44%] [G loss: 0.710986]\n",
      "epoch:10 step:10271 [D loss: 0.544357, acc.: 67.97%] [G loss: 0.596422]\n",
      "epoch:10 step:10272 [D loss: 0.580103, acc.: 67.19%] [G loss: 0.655010]\n",
      "epoch:10 step:10273 [D loss: 0.554300, acc.: 70.31%] [G loss: 0.572939]\n",
      "epoch:10 step:10274 [D loss: 0.588490, acc.: 64.84%] [G loss: 0.539591]\n",
      "epoch:10 step:10275 [D loss: 0.595819, acc.: 63.28%] [G loss: 0.604623]\n",
      "epoch:10 step:10276 [D loss: 0.530079, acc.: 75.78%] [G loss: 0.657695]\n",
      "epoch:10 step:10277 [D loss: 0.530303, acc.: 71.88%] [G loss: 0.795212]\n",
      "epoch:10 step:10278 [D loss: 0.565450, acc.: 69.53%] [G loss: 0.577819]\n",
      "epoch:10 step:10279 [D loss: 0.517392, acc.: 75.00%] [G loss: 0.541508]\n",
      "epoch:10 step:10280 [D loss: 0.500866, acc.: 71.09%] [G loss: 0.692661]\n",
      "epoch:10 step:10281 [D loss: 0.556065, acc.: 75.00%] [G loss: 0.729754]\n",
      "epoch:10 step:10282 [D loss: 0.426399, acc.: 83.59%] [G loss: 0.692846]\n",
      "epoch:10 step:10283 [D loss: 0.535109, acc.: 72.66%] [G loss: 0.720327]\n",
      "epoch:10 step:10284 [D loss: 0.472887, acc.: 78.91%] [G loss: 0.784513]\n",
      "epoch:10 step:10285 [D loss: 0.745086, acc.: 54.69%] [G loss: 0.733677]\n",
      "epoch:10 step:10286 [D loss: 0.547743, acc.: 71.09%] [G loss: 0.639374]\n",
      "epoch:10 step:10287 [D loss: 0.595370, acc.: 64.84%] [G loss: 0.588551]\n",
      "epoch:10 step:10288 [D loss: 0.490104, acc.: 78.91%] [G loss: 0.590334]\n",
      "epoch:10 step:10289 [D loss: 0.462479, acc.: 78.91%] [G loss: 0.682619]\n",
      "epoch:10 step:10290 [D loss: 0.717777, acc.: 56.25%] [G loss: 0.501799]\n",
      "epoch:10 step:10291 [D loss: 0.502467, acc.: 78.12%] [G loss: 0.727646]\n",
      "epoch:10 step:10292 [D loss: 0.540643, acc.: 69.53%] [G loss: 0.729848]\n",
      "epoch:10 step:10293 [D loss: 0.489489, acc.: 71.09%] [G loss: 0.629400]\n",
      "epoch:10 step:10294 [D loss: 0.434467, acc.: 81.25%] [G loss: 0.853691]\n",
      "epoch:10 step:10295 [D loss: 0.427822, acc.: 85.94%] [G loss: 0.965228]\n",
      "epoch:10 step:10296 [D loss: 0.409403, acc.: 82.03%] [G loss: 0.944425]\n",
      "epoch:10 step:10297 [D loss: 0.560680, acc.: 66.41%] [G loss: 1.103092]\n",
      "epoch:10 step:10298 [D loss: 0.707844, acc.: 65.62%] [G loss: 1.132134]\n",
      "epoch:10 step:10299 [D loss: 0.562901, acc.: 70.31%] [G loss: 1.093880]\n",
      "epoch:10 step:10300 [D loss: 0.445562, acc.: 76.56%] [G loss: 0.886716]\n",
      "epoch:10 step:10301 [D loss: 0.580198, acc.: 63.28%] [G loss: 0.640902]\n",
      "epoch:10 step:10302 [D loss: 0.601629, acc.: 66.41%] [G loss: 0.464561]\n",
      "epoch:10 step:10303 [D loss: 0.528225, acc.: 75.00%] [G loss: 0.762188]\n",
      "epoch:10 step:10304 [D loss: 0.558589, acc.: 71.09%] [G loss: 0.667628]\n",
      "epoch:10 step:10305 [D loss: 0.453858, acc.: 75.00%] [G loss: 0.951099]\n",
      "epoch:10 step:10306 [D loss: 0.386933, acc.: 80.47%] [G loss: 1.109059]\n",
      "epoch:10 step:10307 [D loss: 0.467708, acc.: 81.25%] [G loss: 1.011940]\n",
      "epoch:11 step:10308 [D loss: 0.622274, acc.: 67.97%] [G loss: 1.196024]\n",
      "epoch:11 step:10309 [D loss: 0.428615, acc.: 82.03%] [G loss: 0.989663]\n",
      "epoch:11 step:10310 [D loss: 0.578785, acc.: 70.31%] [G loss: 0.734925]\n",
      "epoch:11 step:10311 [D loss: 0.481775, acc.: 82.03%] [G loss: 0.705731]\n",
      "epoch:11 step:10312 [D loss: 0.569533, acc.: 71.09%] [G loss: 0.680339]\n",
      "epoch:11 step:10313 [D loss: 0.571390, acc.: 67.19%] [G loss: 0.683090]\n",
      "epoch:11 step:10314 [D loss: 0.529901, acc.: 75.00%] [G loss: 0.808903]\n",
      "epoch:11 step:10315 [D loss: 0.486740, acc.: 79.69%] [G loss: 0.749516]\n",
      "epoch:11 step:10316 [D loss: 0.488036, acc.: 77.34%] [G loss: 0.746210]\n",
      "epoch:11 step:10317 [D loss: 0.513101, acc.: 75.78%] [G loss: 0.650747]\n",
      "epoch:11 step:10318 [D loss: 0.504760, acc.: 74.22%] [G loss: 0.747165]\n",
      "epoch:11 step:10319 [D loss: 0.617559, acc.: 67.19%] [G loss: 0.491380]\n",
      "epoch:11 step:10320 [D loss: 0.562595, acc.: 67.97%] [G loss: 0.555714]\n",
      "epoch:11 step:10321 [D loss: 0.548500, acc.: 68.75%] [G loss: 0.410318]\n",
      "epoch:11 step:10322 [D loss: 0.489223, acc.: 78.12%] [G loss: 0.529917]\n",
      "epoch:11 step:10323 [D loss: 0.527132, acc.: 74.22%] [G loss: 0.675795]\n",
      "epoch:11 step:10324 [D loss: 0.570790, acc.: 71.09%] [G loss: 0.641461]\n",
      "epoch:11 step:10325 [D loss: 0.589171, acc.: 67.19%] [G loss: 0.630351]\n",
      "epoch:11 step:10326 [D loss: 0.521973, acc.: 73.44%] [G loss: 0.577806]\n",
      "epoch:11 step:10327 [D loss: 0.572673, acc.: 75.00%] [G loss: 0.514585]\n",
      "epoch:11 step:10328 [D loss: 0.630183, acc.: 62.50%] [G loss: 0.490222]\n",
      "epoch:11 step:10329 [D loss: 0.470850, acc.: 83.59%] [G loss: 0.685965]\n",
      "epoch:11 step:10330 [D loss: 0.475167, acc.: 79.69%] [G loss: 0.671007]\n",
      "epoch:11 step:10331 [D loss: 0.532248, acc.: 67.19%] [G loss: 0.533917]\n",
      "epoch:11 step:10332 [D loss: 0.533355, acc.: 78.12%] [G loss: 0.650291]\n",
      "epoch:11 step:10333 [D loss: 0.570490, acc.: 70.31%] [G loss: 0.467783]\n",
      "epoch:11 step:10334 [D loss: 0.479628, acc.: 76.56%] [G loss: 0.496481]\n",
      "epoch:11 step:10335 [D loss: 0.527550, acc.: 67.97%] [G loss: 0.575676]\n",
      "epoch:11 step:10336 [D loss: 0.507707, acc.: 70.31%] [G loss: 0.513051]\n",
      "epoch:11 step:10337 [D loss: 0.524723, acc.: 76.56%] [G loss: 0.548986]\n",
      "epoch:11 step:10338 [D loss: 0.620431, acc.: 62.50%] [G loss: 0.485710]\n",
      "epoch:11 step:10339 [D loss: 0.546597, acc.: 71.88%] [G loss: 0.479576]\n",
      "epoch:11 step:10340 [D loss: 0.546965, acc.: 67.19%] [G loss: 0.538562]\n",
      "epoch:11 step:10341 [D loss: 0.533401, acc.: 73.44%] [G loss: 0.625787]\n",
      "epoch:11 step:10342 [D loss: 0.600338, acc.: 63.28%] [G loss: 0.519091]\n",
      "epoch:11 step:10343 [D loss: 0.503258, acc.: 75.00%] [G loss: 0.614910]\n",
      "epoch:11 step:10344 [D loss: 0.495058, acc.: 78.12%] [G loss: 0.571634]\n",
      "epoch:11 step:10345 [D loss: 0.601770, acc.: 67.19%] [G loss: 0.443454]\n",
      "epoch:11 step:10346 [D loss: 0.523870, acc.: 74.22%] [G loss: 0.632649]\n",
      "epoch:11 step:10347 [D loss: 0.446789, acc.: 79.69%] [G loss: 0.592103]\n",
      "epoch:11 step:10348 [D loss: 0.553831, acc.: 64.84%] [G loss: 0.579775]\n",
      "epoch:11 step:10349 [D loss: 0.586951, acc.: 71.09%] [G loss: 0.561582]\n",
      "epoch:11 step:10350 [D loss: 0.526349, acc.: 68.75%] [G loss: 0.617089]\n",
      "epoch:11 step:10351 [D loss: 0.575109, acc.: 71.88%] [G loss: 0.540544]\n",
      "epoch:11 step:10352 [D loss: 0.506337, acc.: 69.53%] [G loss: 0.579516]\n",
      "epoch:11 step:10353 [D loss: 0.540126, acc.: 71.09%] [G loss: 0.674960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10354 [D loss: 0.612217, acc.: 61.72%] [G loss: 0.621175]\n",
      "epoch:11 step:10355 [D loss: 0.508039, acc.: 76.56%] [G loss: 0.651542]\n",
      "epoch:11 step:10356 [D loss: 0.539888, acc.: 72.66%] [G loss: 0.599974]\n",
      "epoch:11 step:10357 [D loss: 0.583765, acc.: 64.06%] [G loss: 0.630661]\n",
      "epoch:11 step:10358 [D loss: 0.702969, acc.: 57.81%] [G loss: 0.396934]\n",
      "epoch:11 step:10359 [D loss: 0.672682, acc.: 64.06%] [G loss: 0.408548]\n",
      "epoch:11 step:10360 [D loss: 0.543709, acc.: 71.88%] [G loss: 0.459090]\n",
      "epoch:11 step:10361 [D loss: 0.500975, acc.: 78.91%] [G loss: 0.526695]\n",
      "epoch:11 step:10362 [D loss: 0.565078, acc.: 68.75%] [G loss: 0.604611]\n",
      "epoch:11 step:10363 [D loss: 0.510544, acc.: 76.56%] [G loss: 0.563493]\n",
      "epoch:11 step:10364 [D loss: 0.554086, acc.: 67.97%] [G loss: 0.612029]\n",
      "epoch:11 step:10365 [D loss: 0.621109, acc.: 66.41%] [G loss: 0.446153]\n",
      "epoch:11 step:10366 [D loss: 0.507462, acc.: 73.44%] [G loss: 0.647546]\n",
      "epoch:11 step:10367 [D loss: 0.538164, acc.: 69.53%] [G loss: 0.585579]\n",
      "epoch:11 step:10368 [D loss: 0.562254, acc.: 71.09%] [G loss: 0.621250]\n",
      "epoch:11 step:10369 [D loss: 0.572170, acc.: 71.88%] [G loss: 0.584045]\n",
      "epoch:11 step:10370 [D loss: 0.572685, acc.: 69.53%] [G loss: 0.524650]\n",
      "epoch:11 step:10371 [D loss: 0.568882, acc.: 70.31%] [G loss: 0.454975]\n",
      "epoch:11 step:10372 [D loss: 0.518775, acc.: 73.44%] [G loss: 0.441982]\n",
      "epoch:11 step:10373 [D loss: 0.564582, acc.: 71.88%] [G loss: 0.482496]\n",
      "epoch:11 step:10374 [D loss: 0.571258, acc.: 71.09%] [G loss: 0.479223]\n",
      "epoch:11 step:10375 [D loss: 0.544403, acc.: 71.88%] [G loss: 0.483121]\n",
      "epoch:11 step:10376 [D loss: 0.558775, acc.: 77.34%] [G loss: 0.532495]\n",
      "epoch:11 step:10377 [D loss: 0.502746, acc.: 75.00%] [G loss: 0.588053]\n",
      "epoch:11 step:10378 [D loss: 0.522300, acc.: 73.44%] [G loss: 0.468084]\n",
      "epoch:11 step:10379 [D loss: 0.507435, acc.: 74.22%] [G loss: 0.532121]\n",
      "epoch:11 step:10380 [D loss: 0.575915, acc.: 69.53%] [G loss: 0.499845]\n",
      "epoch:11 step:10381 [D loss: 0.536811, acc.: 75.00%] [G loss: 0.460164]\n",
      "epoch:11 step:10382 [D loss: 0.496952, acc.: 72.66%] [G loss: 0.636133]\n",
      "epoch:11 step:10383 [D loss: 0.600678, acc.: 60.94%] [G loss: 0.651215]\n",
      "epoch:11 step:10384 [D loss: 0.455312, acc.: 75.78%] [G loss: 0.922948]\n",
      "epoch:11 step:10385 [D loss: 0.580606, acc.: 71.88%] [G loss: 0.492866]\n",
      "epoch:11 step:10386 [D loss: 0.576464, acc.: 65.62%] [G loss: 0.424508]\n",
      "epoch:11 step:10387 [D loss: 0.566233, acc.: 73.44%] [G loss: 0.546578]\n",
      "epoch:11 step:10388 [D loss: 0.512064, acc.: 72.66%] [G loss: 0.634974]\n",
      "epoch:11 step:10389 [D loss: 0.562727, acc.: 67.19%] [G loss: 0.582667]\n",
      "epoch:11 step:10390 [D loss: 0.559646, acc.: 71.88%] [G loss: 0.494097]\n",
      "epoch:11 step:10391 [D loss: 0.536846, acc.: 67.97%] [G loss: 0.651722]\n",
      "epoch:11 step:10392 [D loss: 0.542270, acc.: 72.66%] [G loss: 0.651015]\n",
      "epoch:11 step:10393 [D loss: 0.536417, acc.: 71.09%] [G loss: 0.434241]\n",
      "epoch:11 step:10394 [D loss: 0.526233, acc.: 71.88%] [G loss: 0.560230]\n",
      "epoch:11 step:10395 [D loss: 0.513645, acc.: 73.44%] [G loss: 0.397435]\n",
      "epoch:11 step:10396 [D loss: 0.542065, acc.: 74.22%] [G loss: 0.511503]\n",
      "epoch:11 step:10397 [D loss: 0.527732, acc.: 71.88%] [G loss: 0.611006]\n",
      "epoch:11 step:10398 [D loss: 0.530192, acc.: 75.78%] [G loss: 0.684113]\n",
      "epoch:11 step:10399 [D loss: 0.549255, acc.: 68.75%] [G loss: 0.603269]\n",
      "epoch:11 step:10400 [D loss: 0.496972, acc.: 77.34%] [G loss: 0.645756]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.367841\n",
      "FID: 50.066948\n",
      "0 = 12.982510269355771\n",
      "1 = 0.09615731966640526\n",
      "2 = 0.9071000218391418\n",
      "3 = 0.8432000279426575\n",
      "4 = 0.9710000157356262\n",
      "5 = 0.9667507410049438\n",
      "6 = 0.8432000279426575\n",
      "7 = 8.439776655006431\n",
      "8 = 0.1528796039677176\n",
      "9 = 0.7411999702453613\n",
      "10 = 0.7293999791145325\n",
      "11 = 0.753000020980835\n",
      "12 = 0.7470299005508423\n",
      "13 = 0.7293999791145325\n",
      "14 = 6.367865085601807\n",
      "15 = 7.073798179626465\n",
      "16 = 0.3802169859409332\n",
      "17 = 6.3678412437438965\n",
      "18 = 50.06694793701172\n",
      "epoch:11 step:10401 [D loss: 0.513164, acc.: 74.22%] [G loss: 0.582405]\n",
      "epoch:11 step:10402 [D loss: 0.567891, acc.: 66.41%] [G loss: 0.538717]\n",
      "epoch:11 step:10403 [D loss: 0.498780, acc.: 76.56%] [G loss: 0.713466]\n",
      "epoch:11 step:10404 [D loss: 0.542183, acc.: 74.22%] [G loss: 0.663515]\n",
      "epoch:11 step:10405 [D loss: 0.578105, acc.: 71.88%] [G loss: 0.646325]\n",
      "epoch:11 step:10406 [D loss: 0.518764, acc.: 77.34%] [G loss: 0.665775]\n",
      "epoch:11 step:10407 [D loss: 0.543180, acc.: 68.75%] [G loss: 0.702969]\n",
      "epoch:11 step:10408 [D loss: 0.551597, acc.: 69.53%] [G loss: 0.707370]\n",
      "epoch:11 step:10409 [D loss: 0.604755, acc.: 64.84%] [G loss: 0.639116]\n",
      "epoch:11 step:10410 [D loss: 0.548718, acc.: 68.75%] [G loss: 0.482719]\n",
      "epoch:11 step:10411 [D loss: 0.500475, acc.: 73.44%] [G loss: 0.502975]\n",
      "epoch:11 step:10412 [D loss: 0.576264, acc.: 72.66%] [G loss: 0.524778]\n",
      "epoch:11 step:10413 [D loss: 0.529488, acc.: 70.31%] [G loss: 0.541765]\n",
      "epoch:11 step:10414 [D loss: 0.577996, acc.: 74.22%] [G loss: 0.598346]\n",
      "epoch:11 step:10415 [D loss: 0.602880, acc.: 69.53%] [G loss: 0.473728]\n",
      "epoch:11 step:10416 [D loss: 0.584933, acc.: 75.00%] [G loss: 0.603152]\n",
      "epoch:11 step:10417 [D loss: 0.539109, acc.: 71.88%] [G loss: 0.474280]\n",
      "epoch:11 step:10418 [D loss: 0.547686, acc.: 67.19%] [G loss: 0.592897]\n",
      "epoch:11 step:10419 [D loss: 0.549257, acc.: 73.44%] [G loss: 0.517231]\n",
      "epoch:11 step:10420 [D loss: 0.544156, acc.: 74.22%] [G loss: 0.533407]\n",
      "epoch:11 step:10421 [D loss: 0.581925, acc.: 73.44%] [G loss: 0.560785]\n",
      "epoch:11 step:10422 [D loss: 0.508209, acc.: 78.91%] [G loss: 0.535762]\n",
      "epoch:11 step:10423 [D loss: 0.565299, acc.: 64.84%] [G loss: 0.606949]\n",
      "epoch:11 step:10424 [D loss: 0.496460, acc.: 74.22%] [G loss: 0.720577]\n",
      "epoch:11 step:10425 [D loss: 0.546249, acc.: 71.88%] [G loss: 0.779821]\n",
      "epoch:11 step:10426 [D loss: 0.452929, acc.: 81.25%] [G loss: 0.674097]\n",
      "epoch:11 step:10427 [D loss: 0.532194, acc.: 75.00%] [G loss: 0.715273]\n",
      "epoch:11 step:10428 [D loss: 0.561299, acc.: 69.53%] [G loss: 0.612766]\n",
      "epoch:11 step:10429 [D loss: 0.522010, acc.: 74.22%] [G loss: 0.724053]\n",
      "epoch:11 step:10430 [D loss: 0.470381, acc.: 78.12%] [G loss: 0.658397]\n",
      "epoch:11 step:10431 [D loss: 0.544810, acc.: 71.88%] [G loss: 0.637838]\n",
      "epoch:11 step:10432 [D loss: 0.568013, acc.: 71.88%] [G loss: 0.525417]\n",
      "epoch:11 step:10433 [D loss: 0.493923, acc.: 75.00%] [G loss: 0.591266]\n",
      "epoch:11 step:10434 [D loss: 0.521258, acc.: 72.66%] [G loss: 0.555031]\n",
      "epoch:11 step:10435 [D loss: 0.561558, acc.: 67.97%] [G loss: 0.507373]\n",
      "epoch:11 step:10436 [D loss: 0.615093, acc.: 64.06%] [G loss: 0.467266]\n",
      "epoch:11 step:10437 [D loss: 0.567635, acc.: 68.75%] [G loss: 0.566850]\n",
      "epoch:11 step:10438 [D loss: 0.491224, acc.: 77.34%] [G loss: 0.670063]\n",
      "epoch:11 step:10439 [D loss: 0.548714, acc.: 70.31%] [G loss: 0.593825]\n",
      "epoch:11 step:10440 [D loss: 0.585687, acc.: 64.06%] [G loss: 0.675018]\n",
      "epoch:11 step:10441 [D loss: 0.590470, acc.: 63.28%] [G loss: 0.515794]\n",
      "epoch:11 step:10442 [D loss: 0.559393, acc.: 73.44%] [G loss: 0.534181]\n",
      "epoch:11 step:10443 [D loss: 0.575801, acc.: 65.62%] [G loss: 0.524666]\n",
      "epoch:11 step:10444 [D loss: 0.634092, acc.: 64.84%] [G loss: 0.534394]\n",
      "epoch:11 step:10445 [D loss: 0.590892, acc.: 68.75%] [G loss: 0.585362]\n",
      "epoch:11 step:10446 [D loss: 0.562022, acc.: 67.97%] [G loss: 0.446345]\n",
      "epoch:11 step:10447 [D loss: 0.572620, acc.: 67.97%] [G loss: 0.473315]\n",
      "epoch:11 step:10448 [D loss: 0.502426, acc.: 74.22%] [G loss: 0.563450]\n",
      "epoch:11 step:10449 [D loss: 0.568532, acc.: 64.06%] [G loss: 0.539974]\n",
      "epoch:11 step:10450 [D loss: 0.629978, acc.: 67.97%] [G loss: 0.491128]\n",
      "epoch:11 step:10451 [D loss: 0.526947, acc.: 66.41%] [G loss: 0.679600]\n",
      "epoch:11 step:10452 [D loss: 0.566580, acc.: 69.53%] [G loss: 0.602052]\n",
      "epoch:11 step:10453 [D loss: 0.503804, acc.: 75.00%] [G loss: 0.584787]\n",
      "epoch:11 step:10454 [D loss: 0.722506, acc.: 57.03%] [G loss: 0.520365]\n",
      "epoch:11 step:10455 [D loss: 0.534088, acc.: 67.97%] [G loss: 0.529141]\n",
      "epoch:11 step:10456 [D loss: 0.549143, acc.: 73.44%] [G loss: 0.487231]\n",
      "epoch:11 step:10457 [D loss: 0.569061, acc.: 74.22%] [G loss: 0.496742]\n",
      "epoch:11 step:10458 [D loss: 0.556743, acc.: 67.97%] [G loss: 0.513164]\n",
      "epoch:11 step:10459 [D loss: 0.570566, acc.: 67.19%] [G loss: 0.553736]\n",
      "epoch:11 step:10460 [D loss: 0.636011, acc.: 59.38%] [G loss: 0.600209]\n",
      "epoch:11 step:10461 [D loss: 0.563649, acc.: 75.00%] [G loss: 0.554422]\n",
      "epoch:11 step:10462 [D loss: 0.491859, acc.: 75.00%] [G loss: 0.612917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10463 [D loss: 0.486095, acc.: 80.47%] [G loss: 0.593907]\n",
      "epoch:11 step:10464 [D loss: 0.568919, acc.: 69.53%] [G loss: 0.496863]\n",
      "epoch:11 step:10465 [D loss: 0.628948, acc.: 62.50%] [G loss: 0.577223]\n",
      "epoch:11 step:10466 [D loss: 0.517205, acc.: 75.78%] [G loss: 0.613229]\n",
      "epoch:11 step:10467 [D loss: 0.564819, acc.: 71.09%] [G loss: 0.609685]\n",
      "epoch:11 step:10468 [D loss: 0.556423, acc.: 67.19%] [G loss: 0.587202]\n",
      "epoch:11 step:10469 [D loss: 0.489606, acc.: 75.78%] [G loss: 0.661109]\n",
      "epoch:11 step:10470 [D loss: 0.585517, acc.: 70.31%] [G loss: 0.728194]\n",
      "epoch:11 step:10471 [D loss: 0.607848, acc.: 64.06%] [G loss: 0.538617]\n",
      "epoch:11 step:10472 [D loss: 0.527701, acc.: 74.22%] [G loss: 0.516128]\n",
      "epoch:11 step:10473 [D loss: 0.602338, acc.: 64.06%] [G loss: 0.573065]\n",
      "epoch:11 step:10474 [D loss: 0.537670, acc.: 72.66%] [G loss: 0.629538]\n",
      "epoch:11 step:10475 [D loss: 0.540387, acc.: 71.09%] [G loss: 0.499573]\n",
      "epoch:11 step:10476 [D loss: 0.574878, acc.: 67.19%] [G loss: 0.434894]\n",
      "epoch:11 step:10477 [D loss: 0.596880, acc.: 65.62%] [G loss: 0.355979]\n",
      "epoch:11 step:10478 [D loss: 0.534662, acc.: 69.53%] [G loss: 0.399292]\n",
      "epoch:11 step:10479 [D loss: 0.543592, acc.: 74.22%] [G loss: 0.520442]\n",
      "epoch:11 step:10480 [D loss: 0.572605, acc.: 64.84%] [G loss: 0.553716]\n",
      "epoch:11 step:10481 [D loss: 0.561830, acc.: 74.22%] [G loss: 0.604037]\n",
      "epoch:11 step:10482 [D loss: 0.600466, acc.: 62.50%] [G loss: 0.511937]\n",
      "epoch:11 step:10483 [D loss: 0.563832, acc.: 67.19%] [G loss: 0.496738]\n",
      "epoch:11 step:10484 [D loss: 0.502012, acc.: 76.56%] [G loss: 0.502427]\n",
      "epoch:11 step:10485 [D loss: 0.513464, acc.: 70.31%] [G loss: 0.630272]\n",
      "epoch:11 step:10486 [D loss: 0.598678, acc.: 64.84%] [G loss: 0.461336]\n",
      "epoch:11 step:10487 [D loss: 0.628700, acc.: 64.84%] [G loss: 0.504687]\n",
      "epoch:11 step:10488 [D loss: 0.595500, acc.: 71.09%] [G loss: 0.432456]\n",
      "epoch:11 step:10489 [D loss: 0.556655, acc.: 75.78%] [G loss: 0.595453]\n",
      "epoch:11 step:10490 [D loss: 0.556926, acc.: 68.75%] [G loss: 0.738882]\n",
      "epoch:11 step:10491 [D loss: 0.479314, acc.: 79.69%] [G loss: 0.792028]\n",
      "epoch:11 step:10492 [D loss: 0.559989, acc.: 67.97%] [G loss: 0.572456]\n",
      "epoch:11 step:10493 [D loss: 0.604770, acc.: 67.19%] [G loss: 0.488445]\n",
      "epoch:11 step:10494 [D loss: 0.619441, acc.: 65.62%] [G loss: 0.544919]\n",
      "epoch:11 step:10495 [D loss: 0.548732, acc.: 73.44%] [G loss: 0.491507]\n",
      "epoch:11 step:10496 [D loss: 0.561085, acc.: 67.97%] [G loss: 0.590577]\n",
      "epoch:11 step:10497 [D loss: 0.529496, acc.: 75.00%] [G loss: 0.472798]\n",
      "epoch:11 step:10498 [D loss: 0.510709, acc.: 76.56%] [G loss: 0.595569]\n",
      "epoch:11 step:10499 [D loss: 0.522683, acc.: 74.22%] [G loss: 0.683176]\n",
      "epoch:11 step:10500 [D loss: 0.524095, acc.: 71.09%] [G loss: 0.457201]\n",
      "epoch:11 step:10501 [D loss: 0.508548, acc.: 77.34%] [G loss: 0.558123]\n",
      "epoch:11 step:10502 [D loss: 0.621387, acc.: 64.84%] [G loss: 0.611027]\n",
      "epoch:11 step:10503 [D loss: 0.608852, acc.: 66.41%] [G loss: 0.569689]\n",
      "epoch:11 step:10504 [D loss: 0.515136, acc.: 75.00%] [G loss: 0.639850]\n",
      "epoch:11 step:10505 [D loss: 0.506375, acc.: 69.53%] [G loss: 0.734481]\n",
      "epoch:11 step:10506 [D loss: 0.566764, acc.: 68.75%] [G loss: 0.615180]\n",
      "epoch:11 step:10507 [D loss: 0.607400, acc.: 67.19%] [G loss: 0.501322]\n",
      "epoch:11 step:10508 [D loss: 0.567014, acc.: 70.31%] [G loss: 0.699826]\n",
      "epoch:11 step:10509 [D loss: 0.580052, acc.: 68.75%] [G loss: 0.617444]\n",
      "epoch:11 step:10510 [D loss: 0.655729, acc.: 60.94%] [G loss: 0.580758]\n",
      "epoch:11 step:10511 [D loss: 0.585765, acc.: 71.09%] [G loss: 0.604024]\n",
      "epoch:11 step:10512 [D loss: 0.559673, acc.: 67.97%] [G loss: 0.794338]\n",
      "epoch:11 step:10513 [D loss: 0.504571, acc.: 71.09%] [G loss: 0.654528]\n",
      "epoch:11 step:10514 [D loss: 0.467486, acc.: 80.47%] [G loss: 0.803974]\n",
      "epoch:11 step:10515 [D loss: 0.409398, acc.: 83.59%] [G loss: 0.798125]\n",
      "epoch:11 step:10516 [D loss: 0.497307, acc.: 78.12%] [G loss: 0.747663]\n",
      "epoch:11 step:10517 [D loss: 0.692238, acc.: 57.81%] [G loss: 0.601448]\n",
      "epoch:11 step:10518 [D loss: 0.583290, acc.: 67.19%] [G loss: 0.654566]\n",
      "epoch:11 step:10519 [D loss: 0.558031, acc.: 74.22%] [G loss: 0.551659]\n",
      "epoch:11 step:10520 [D loss: 0.530031, acc.: 71.09%] [G loss: 0.509305]\n",
      "epoch:11 step:10521 [D loss: 0.653126, acc.: 58.59%] [G loss: 0.499996]\n",
      "epoch:11 step:10522 [D loss: 0.586834, acc.: 64.06%] [G loss: 0.467053]\n",
      "epoch:11 step:10523 [D loss: 0.581424, acc.: 66.41%] [G loss: 0.503360]\n",
      "epoch:11 step:10524 [D loss: 0.562141, acc.: 74.22%] [G loss: 0.607468]\n",
      "epoch:11 step:10525 [D loss: 0.552850, acc.: 67.97%] [G loss: 0.466342]\n",
      "epoch:11 step:10526 [D loss: 0.502038, acc.: 75.78%] [G loss: 0.594850]\n",
      "epoch:11 step:10527 [D loss: 0.668075, acc.: 59.38%] [G loss: 0.562943]\n",
      "epoch:11 step:10528 [D loss: 0.523130, acc.: 71.09%] [G loss: 0.685437]\n",
      "epoch:11 step:10529 [D loss: 0.458661, acc.: 78.12%] [G loss: 0.655392]\n",
      "epoch:11 step:10530 [D loss: 0.538594, acc.: 73.44%] [G loss: 0.585640]\n",
      "epoch:11 step:10531 [D loss: 0.547932, acc.: 72.66%] [G loss: 0.553368]\n",
      "epoch:11 step:10532 [D loss: 0.567790, acc.: 70.31%] [G loss: 0.679334]\n",
      "epoch:11 step:10533 [D loss: 0.549969, acc.: 69.53%] [G loss: 0.522480]\n",
      "epoch:11 step:10534 [D loss: 0.552682, acc.: 66.41%] [G loss: 0.589382]\n",
      "epoch:11 step:10535 [D loss: 0.619721, acc.: 60.16%] [G loss: 0.514531]\n",
      "epoch:11 step:10536 [D loss: 0.534460, acc.: 73.44%] [G loss: 0.401277]\n",
      "epoch:11 step:10537 [D loss: 0.559181, acc.: 70.31%] [G loss: 0.562509]\n",
      "epoch:11 step:10538 [D loss: 0.466468, acc.: 82.81%] [G loss: 0.557741]\n",
      "epoch:11 step:10539 [D loss: 0.470995, acc.: 79.69%] [G loss: 0.704769]\n",
      "epoch:11 step:10540 [D loss: 0.607227, acc.: 68.75%] [G loss: 0.568989]\n",
      "epoch:11 step:10541 [D loss: 0.565890, acc.: 71.09%] [G loss: 0.629688]\n",
      "epoch:11 step:10542 [D loss: 0.531287, acc.: 70.31%] [G loss: 0.645591]\n",
      "epoch:11 step:10543 [D loss: 0.611114, acc.: 67.19%] [G loss: 0.478936]\n",
      "epoch:11 step:10544 [D loss: 0.572577, acc.: 66.41%] [G loss: 0.512968]\n",
      "epoch:11 step:10545 [D loss: 0.593148, acc.: 67.97%] [G loss: 0.581562]\n",
      "epoch:11 step:10546 [D loss: 0.501276, acc.: 76.56%] [G loss: 0.560456]\n",
      "epoch:11 step:10547 [D loss: 0.557742, acc.: 70.31%] [G loss: 0.513000]\n",
      "epoch:11 step:10548 [D loss: 0.610932, acc.: 69.53%] [G loss: 0.596785]\n",
      "epoch:11 step:10549 [D loss: 0.564243, acc.: 64.06%] [G loss: 0.474280]\n",
      "epoch:11 step:10550 [D loss: 0.560022, acc.: 67.19%] [G loss: 0.563485]\n",
      "epoch:11 step:10551 [D loss: 0.487371, acc.: 75.78%] [G loss: 0.593539]\n",
      "epoch:11 step:10552 [D loss: 0.537086, acc.: 68.75%] [G loss: 0.546884]\n",
      "epoch:11 step:10553 [D loss: 0.559036, acc.: 67.97%] [G loss: 0.459126]\n",
      "epoch:11 step:10554 [D loss: 0.549865, acc.: 69.53%] [G loss: 0.726367]\n",
      "epoch:11 step:10555 [D loss: 0.570489, acc.: 67.19%] [G loss: 0.614799]\n",
      "epoch:11 step:10556 [D loss: 0.562481, acc.: 73.44%] [G loss: 0.681558]\n",
      "epoch:11 step:10557 [D loss: 0.590190, acc.: 68.75%] [G loss: 0.571207]\n",
      "epoch:11 step:10558 [D loss: 0.605375, acc.: 70.31%] [G loss: 0.531892]\n",
      "epoch:11 step:10559 [D loss: 0.534382, acc.: 71.09%] [G loss: 0.598007]\n",
      "epoch:11 step:10560 [D loss: 0.537710, acc.: 67.97%] [G loss: 0.598948]\n",
      "epoch:11 step:10561 [D loss: 0.503009, acc.: 71.09%] [G loss: 0.691004]\n",
      "epoch:11 step:10562 [D loss: 0.553849, acc.: 73.44%] [G loss: 0.615888]\n",
      "epoch:11 step:10563 [D loss: 0.535237, acc.: 70.31%] [G loss: 0.590399]\n",
      "epoch:11 step:10564 [D loss: 0.544972, acc.: 71.88%] [G loss: 0.611310]\n",
      "epoch:11 step:10565 [D loss: 0.616291, acc.: 61.72%] [G loss: 0.513375]\n",
      "epoch:11 step:10566 [D loss: 0.569169, acc.: 68.75%] [G loss: 0.450368]\n",
      "epoch:11 step:10567 [D loss: 0.566359, acc.: 67.97%] [G loss: 0.478663]\n",
      "epoch:11 step:10568 [D loss: 0.567448, acc.: 68.75%] [G loss: 0.536049]\n",
      "epoch:11 step:10569 [D loss: 0.547110, acc.: 71.09%] [G loss: 0.498260]\n",
      "epoch:11 step:10570 [D loss: 0.624823, acc.: 66.41%] [G loss: 0.407063]\n",
      "epoch:11 step:10571 [D loss: 0.538388, acc.: 71.09%] [G loss: 0.494947]\n",
      "epoch:11 step:10572 [D loss: 0.573896, acc.: 67.97%] [G loss: 0.599782]\n",
      "epoch:11 step:10573 [D loss: 0.600318, acc.: 66.41%] [G loss: 0.410468]\n",
      "epoch:11 step:10574 [D loss: 0.592050, acc.: 64.06%] [G loss: 0.476814]\n",
      "epoch:11 step:10575 [D loss: 0.578869, acc.: 67.97%] [G loss: 0.503771]\n",
      "epoch:11 step:10576 [D loss: 0.542458, acc.: 71.09%] [G loss: 0.513419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10577 [D loss: 0.516539, acc.: 76.56%] [G loss: 0.666199]\n",
      "epoch:11 step:10578 [D loss: 0.586211, acc.: 69.53%] [G loss: 0.550133]\n",
      "epoch:11 step:10579 [D loss: 0.529517, acc.: 68.75%] [G loss: 0.574048]\n",
      "epoch:11 step:10580 [D loss: 0.492351, acc.: 75.78%] [G loss: 0.717624]\n",
      "epoch:11 step:10581 [D loss: 0.553646, acc.: 68.75%] [G loss: 0.578537]\n",
      "epoch:11 step:10582 [D loss: 0.651061, acc.: 67.19%] [G loss: 0.453885]\n",
      "epoch:11 step:10583 [D loss: 0.507242, acc.: 78.12%] [G loss: 0.459702]\n",
      "epoch:11 step:10584 [D loss: 0.635486, acc.: 65.62%] [G loss: 0.485619]\n",
      "epoch:11 step:10585 [D loss: 0.612889, acc.: 60.16%] [G loss: 0.428490]\n",
      "epoch:11 step:10586 [D loss: 0.617898, acc.: 60.94%] [G loss: 0.409721]\n",
      "epoch:11 step:10587 [D loss: 0.522972, acc.: 71.88%] [G loss: 0.570199]\n",
      "epoch:11 step:10588 [D loss: 0.611504, acc.: 64.06%] [G loss: 0.392790]\n",
      "epoch:11 step:10589 [D loss: 0.606927, acc.: 64.84%] [G loss: 0.437794]\n",
      "epoch:11 step:10590 [D loss: 0.569138, acc.: 70.31%] [G loss: 0.450642]\n",
      "epoch:11 step:10591 [D loss: 0.583682, acc.: 64.84%] [G loss: 0.474239]\n",
      "epoch:11 step:10592 [D loss: 0.582795, acc.: 67.19%] [G loss: 0.469733]\n",
      "epoch:11 step:10593 [D loss: 0.506147, acc.: 76.56%] [G loss: 0.706202]\n",
      "epoch:11 step:10594 [D loss: 0.572288, acc.: 69.53%] [G loss: 0.540147]\n",
      "epoch:11 step:10595 [D loss: 0.593400, acc.: 64.84%] [G loss: 0.544689]\n",
      "epoch:11 step:10596 [D loss: 0.529383, acc.: 72.66%] [G loss: 0.640641]\n",
      "epoch:11 step:10597 [D loss: 0.571957, acc.: 63.28%] [G loss: 0.520993]\n",
      "epoch:11 step:10598 [D loss: 0.583558, acc.: 64.06%] [G loss: 0.463767]\n",
      "epoch:11 step:10599 [D loss: 0.539922, acc.: 71.09%] [G loss: 0.509647]\n",
      "epoch:11 step:10600 [D loss: 0.558090, acc.: 70.31%] [G loss: 0.587751]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.334687\n",
      "FID: 43.449509\n",
      "0 = 12.771702041339887\n",
      "1 = 0.08739484190046977\n",
      "2 = 0.8949999809265137\n",
      "3 = 0.829200029373169\n",
      "4 = 0.9607999920845032\n",
      "5 = 0.9548594951629639\n",
      "6 = 0.829200029373169\n",
      "7 = 8.02199762918948\n",
      "8 = 0.1349706886401347\n",
      "9 = 0.7361999750137329\n",
      "10 = 0.7221999764442444\n",
      "11 = 0.7501999735832214\n",
      "12 = 0.7430041432380676\n",
      "13 = 0.7221999764442444\n",
      "14 = 6.334711074829102\n",
      "15 = 7.509540557861328\n",
      "16 = 0.34997737407684326\n",
      "17 = 6.334686756134033\n",
      "18 = 43.44950866699219\n",
      "epoch:11 step:10601 [D loss: 0.622502, acc.: 62.50%] [G loss: 0.486974]\n",
      "epoch:11 step:10602 [D loss: 0.565012, acc.: 69.53%] [G loss: 0.626984]\n",
      "epoch:11 step:10603 [D loss: 0.501653, acc.: 74.22%] [G loss: 0.581818]\n",
      "epoch:11 step:10604 [D loss: 0.528873, acc.: 73.44%] [G loss: 0.627302]\n",
      "epoch:11 step:10605 [D loss: 0.500450, acc.: 75.78%] [G loss: 0.588838]\n",
      "epoch:11 step:10606 [D loss: 0.485811, acc.: 75.00%] [G loss: 0.684977]\n",
      "epoch:11 step:10607 [D loss: 0.495657, acc.: 78.91%] [G loss: 0.682820]\n",
      "epoch:11 step:10608 [D loss: 0.679956, acc.: 64.84%] [G loss: 0.492299]\n",
      "epoch:11 step:10609 [D loss: 0.500997, acc.: 75.00%] [G loss: 0.545778]\n",
      "epoch:11 step:10610 [D loss: 0.612263, acc.: 67.97%] [G loss: 0.562665]\n",
      "epoch:11 step:10611 [D loss: 0.486489, acc.: 75.78%] [G loss: 0.541999]\n",
      "epoch:11 step:10612 [D loss: 0.539606, acc.: 70.31%] [G loss: 0.517200]\n",
      "epoch:11 step:10613 [D loss: 0.535035, acc.: 74.22%] [G loss: 0.674376]\n",
      "epoch:11 step:10614 [D loss: 0.534947, acc.: 74.22%] [G loss: 0.735359]\n",
      "epoch:11 step:10615 [D loss: 0.573740, acc.: 67.19%] [G loss: 0.550538]\n",
      "epoch:11 step:10616 [D loss: 0.496449, acc.: 69.53%] [G loss: 0.672489]\n",
      "epoch:11 step:10617 [D loss: 0.526656, acc.: 77.34%] [G loss: 0.561327]\n",
      "epoch:11 step:10618 [D loss: 0.522483, acc.: 66.41%] [G loss: 0.684616]\n",
      "epoch:11 step:10619 [D loss: 0.468604, acc.: 73.44%] [G loss: 0.833055]\n",
      "epoch:11 step:10620 [D loss: 0.553852, acc.: 70.31%] [G loss: 0.684016]\n",
      "epoch:11 step:10621 [D loss: 0.502863, acc.: 72.66%] [G loss: 0.917963]\n",
      "epoch:11 step:10622 [D loss: 0.529647, acc.: 75.00%] [G loss: 0.655792]\n",
      "epoch:11 step:10623 [D loss: 0.678584, acc.: 66.41%] [G loss: 0.542818]\n",
      "epoch:11 step:10624 [D loss: 0.622751, acc.: 64.06%] [G loss: 0.518599]\n",
      "epoch:11 step:10625 [D loss: 0.530605, acc.: 71.09%] [G loss: 0.508126]\n",
      "epoch:11 step:10626 [D loss: 0.518706, acc.: 71.09%] [G loss: 0.539159]\n",
      "epoch:11 step:10627 [D loss: 0.515833, acc.: 74.22%] [G loss: 0.572285]\n",
      "epoch:11 step:10628 [D loss: 0.502762, acc.: 75.78%] [G loss: 0.643342]\n",
      "epoch:11 step:10629 [D loss: 0.528330, acc.: 69.53%] [G loss: 0.579884]\n",
      "epoch:11 step:10630 [D loss: 0.645877, acc.: 64.06%] [G loss: 0.473027]\n",
      "epoch:11 step:10631 [D loss: 0.537605, acc.: 72.66%] [G loss: 0.457482]\n",
      "epoch:11 step:10632 [D loss: 0.580352, acc.: 65.62%] [G loss: 0.511215]\n",
      "epoch:11 step:10633 [D loss: 0.539534, acc.: 71.88%] [G loss: 0.688450]\n",
      "epoch:11 step:10634 [D loss: 0.568452, acc.: 70.31%] [G loss: 0.589875]\n",
      "epoch:11 step:10635 [D loss: 0.513719, acc.: 75.78%] [G loss: 0.818211]\n",
      "epoch:11 step:10636 [D loss: 0.600406, acc.: 69.53%] [G loss: 0.858662]\n",
      "epoch:11 step:10637 [D loss: 0.573702, acc.: 63.28%] [G loss: 0.509508]\n",
      "epoch:11 step:10638 [D loss: 0.588509, acc.: 66.41%] [G loss: 0.452414]\n",
      "epoch:11 step:10639 [D loss: 0.555100, acc.: 69.53%] [G loss: 0.519681]\n",
      "epoch:11 step:10640 [D loss: 0.481303, acc.: 75.00%] [G loss: 0.631754]\n",
      "epoch:11 step:10641 [D loss: 0.533224, acc.: 66.41%] [G loss: 0.720772]\n",
      "epoch:11 step:10642 [D loss: 0.479232, acc.: 75.78%] [G loss: 0.725241]\n",
      "epoch:11 step:10643 [D loss: 0.505838, acc.: 75.00%] [G loss: 0.622063]\n",
      "epoch:11 step:10644 [D loss: 0.500886, acc.: 77.34%] [G loss: 0.599991]\n",
      "epoch:11 step:10645 [D loss: 0.539774, acc.: 71.88%] [G loss: 0.592106]\n",
      "epoch:11 step:10646 [D loss: 0.592437, acc.: 71.88%] [G loss: 0.656244]\n",
      "epoch:11 step:10647 [D loss: 0.496721, acc.: 77.34%] [G loss: 0.540157]\n",
      "epoch:11 step:10648 [D loss: 0.583740, acc.: 71.09%] [G loss: 0.537622]\n",
      "epoch:11 step:10649 [D loss: 0.673429, acc.: 57.81%] [G loss: 0.465751]\n",
      "epoch:11 step:10650 [D loss: 0.488609, acc.: 75.78%] [G loss: 0.562077]\n",
      "epoch:11 step:10651 [D loss: 0.480186, acc.: 77.34%] [G loss: 0.657040]\n",
      "epoch:11 step:10652 [D loss: 0.592046, acc.: 64.84%] [G loss: 0.673544]\n",
      "epoch:11 step:10653 [D loss: 0.536613, acc.: 68.75%] [G loss: 0.709620]\n",
      "epoch:11 step:10654 [D loss: 0.451722, acc.: 78.12%] [G loss: 0.929260]\n",
      "epoch:11 step:10655 [D loss: 0.632948, acc.: 68.75%] [G loss: 0.567423]\n",
      "epoch:11 step:10656 [D loss: 0.688201, acc.: 59.38%] [G loss: 0.430327]\n",
      "epoch:11 step:10657 [D loss: 0.514938, acc.: 72.66%] [G loss: 0.575910]\n",
      "epoch:11 step:10658 [D loss: 0.570050, acc.: 71.09%] [G loss: 0.662508]\n",
      "epoch:11 step:10659 [D loss: 0.599491, acc.: 67.19%] [G loss: 0.606642]\n",
      "epoch:11 step:10660 [D loss: 0.575860, acc.: 64.84%] [G loss: 0.558153]\n",
      "epoch:11 step:10661 [D loss: 0.390242, acc.: 86.72%] [G loss: 0.689329]\n",
      "epoch:11 step:10662 [D loss: 0.566734, acc.: 73.44%] [G loss: 0.612092]\n",
      "epoch:11 step:10663 [D loss: 0.534270, acc.: 71.88%] [G loss: 0.632144]\n",
      "epoch:11 step:10664 [D loss: 0.468608, acc.: 75.78%] [G loss: 0.826223]\n",
      "epoch:11 step:10665 [D loss: 0.436064, acc.: 79.69%] [G loss: 0.686217]\n",
      "epoch:11 step:10666 [D loss: 0.468745, acc.: 79.69%] [G loss: 0.633163]\n",
      "epoch:11 step:10667 [D loss: 0.503618, acc.: 75.00%] [G loss: 0.804067]\n",
      "epoch:11 step:10668 [D loss: 0.531728, acc.: 71.88%] [G loss: 0.719092]\n",
      "epoch:11 step:10669 [D loss: 0.550239, acc.: 69.53%] [G loss: 0.596942]\n",
      "epoch:11 step:10670 [D loss: 0.582996, acc.: 63.28%] [G loss: 0.501337]\n",
      "epoch:11 step:10671 [D loss: 0.507308, acc.: 71.88%] [G loss: 0.583343]\n",
      "epoch:11 step:10672 [D loss: 0.571108, acc.: 67.19%] [G loss: 0.659228]\n",
      "epoch:11 step:10673 [D loss: 0.526227, acc.: 69.53%] [G loss: 0.605999]\n",
      "epoch:11 step:10674 [D loss: 0.616358, acc.: 64.84%] [G loss: 0.455144]\n",
      "epoch:11 step:10675 [D loss: 0.594760, acc.: 66.41%] [G loss: 0.568800]\n",
      "epoch:11 step:10676 [D loss: 0.532738, acc.: 72.66%] [G loss: 0.631945]\n",
      "epoch:11 step:10677 [D loss: 0.600845, acc.: 69.53%] [G loss: 0.592702]\n",
      "epoch:11 step:10678 [D loss: 0.553063, acc.: 71.88%] [G loss: 0.693403]\n",
      "epoch:11 step:10679 [D loss: 0.555359, acc.: 72.66%] [G loss: 0.631523]\n",
      "epoch:11 step:10680 [D loss: 0.561441, acc.: 66.41%] [G loss: 0.523630]\n",
      "epoch:11 step:10681 [D loss: 0.459593, acc.: 77.34%] [G loss: 0.548691]\n",
      "epoch:11 step:10682 [D loss: 0.541223, acc.: 73.44%] [G loss: 0.589215]\n",
      "epoch:11 step:10683 [D loss: 0.721810, acc.: 56.25%] [G loss: 0.341506]\n",
      "epoch:11 step:10684 [D loss: 0.626110, acc.: 62.50%] [G loss: 0.409601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10685 [D loss: 0.577187, acc.: 62.50%] [G loss: 0.508948]\n",
      "epoch:11 step:10686 [D loss: 0.565641, acc.: 71.88%] [G loss: 0.488610]\n",
      "epoch:11 step:10687 [D loss: 0.581534, acc.: 65.62%] [G loss: 0.549904]\n",
      "epoch:11 step:10688 [D loss: 0.449638, acc.: 81.25%] [G loss: 0.589546]\n",
      "epoch:11 step:10689 [D loss: 0.496980, acc.: 76.56%] [G loss: 0.657944]\n",
      "epoch:11 step:10690 [D loss: 0.592964, acc.: 67.97%] [G loss: 0.552278]\n",
      "epoch:11 step:10691 [D loss: 0.608051, acc.: 65.62%] [G loss: 0.557842]\n",
      "epoch:11 step:10692 [D loss: 0.544850, acc.: 71.09%] [G loss: 0.785892]\n",
      "epoch:11 step:10693 [D loss: 0.651074, acc.: 64.84%] [G loss: 0.484108]\n",
      "epoch:11 step:10694 [D loss: 0.593759, acc.: 66.41%] [G loss: 0.537659]\n",
      "epoch:11 step:10695 [D loss: 0.575921, acc.: 69.53%] [G loss: 0.481227]\n",
      "epoch:11 step:10696 [D loss: 0.501368, acc.: 70.31%] [G loss: 0.642218]\n",
      "epoch:11 step:10697 [D loss: 0.600929, acc.: 71.88%] [G loss: 0.530178]\n",
      "epoch:11 step:10698 [D loss: 0.573457, acc.: 64.84%] [G loss: 0.528060]\n",
      "epoch:11 step:10699 [D loss: 0.463023, acc.: 77.34%] [G loss: 0.651360]\n",
      "epoch:11 step:10700 [D loss: 0.609346, acc.: 64.06%] [G loss: 0.633994]\n",
      "epoch:11 step:10701 [D loss: 0.565340, acc.: 73.44%] [G loss: 0.503297]\n",
      "epoch:11 step:10702 [D loss: 0.555689, acc.: 64.06%] [G loss: 0.393648]\n",
      "epoch:11 step:10703 [D loss: 0.580556, acc.: 65.62%] [G loss: 0.420403]\n",
      "epoch:11 step:10704 [D loss: 0.566687, acc.: 65.62%] [G loss: 0.592221]\n",
      "epoch:11 step:10705 [D loss: 0.466054, acc.: 77.34%] [G loss: 0.619776]\n",
      "epoch:11 step:10706 [D loss: 0.511174, acc.: 75.78%] [G loss: 0.492522]\n",
      "epoch:11 step:10707 [D loss: 0.671250, acc.: 56.25%] [G loss: 0.436325]\n",
      "epoch:11 step:10708 [D loss: 0.639793, acc.: 57.81%] [G loss: 0.395153]\n",
      "epoch:11 step:10709 [D loss: 0.524399, acc.: 70.31%] [G loss: 0.557366]\n",
      "epoch:11 step:10710 [D loss: 0.542805, acc.: 71.09%] [G loss: 0.506188]\n",
      "epoch:11 step:10711 [D loss: 0.636787, acc.: 60.16%] [G loss: 0.411821]\n",
      "epoch:11 step:10712 [D loss: 0.543526, acc.: 71.09%] [G loss: 0.630389]\n",
      "epoch:11 step:10713 [D loss: 0.510248, acc.: 73.44%] [G loss: 0.702245]\n",
      "epoch:11 step:10714 [D loss: 0.637976, acc.: 63.28%] [G loss: 0.599446]\n",
      "epoch:11 step:10715 [D loss: 0.616850, acc.: 65.62%] [G loss: 0.490319]\n",
      "epoch:11 step:10716 [D loss: 0.562075, acc.: 67.97%] [G loss: 0.477294]\n",
      "epoch:11 step:10717 [D loss: 0.571304, acc.: 64.84%] [G loss: 0.558184]\n",
      "epoch:11 step:10718 [D loss: 0.564946, acc.: 69.53%] [G loss: 0.471685]\n",
      "epoch:11 step:10719 [D loss: 0.608721, acc.: 66.41%] [G loss: 0.524666]\n",
      "epoch:11 step:10720 [D loss: 0.551106, acc.: 72.66%] [G loss: 0.495244]\n",
      "epoch:11 step:10721 [D loss: 0.575153, acc.: 66.41%] [G loss: 0.464752]\n",
      "epoch:11 step:10722 [D loss: 0.544593, acc.: 73.44%] [G loss: 0.590758]\n",
      "epoch:11 step:10723 [D loss: 0.512836, acc.: 75.78%] [G loss: 0.631255]\n",
      "epoch:11 step:10724 [D loss: 0.550788, acc.: 71.09%] [G loss: 0.640264]\n",
      "epoch:11 step:10725 [D loss: 0.667323, acc.: 61.72%] [G loss: 0.554786]\n",
      "epoch:11 step:10726 [D loss: 0.584808, acc.: 67.97%] [G loss: 0.449687]\n",
      "epoch:11 step:10727 [D loss: 0.618162, acc.: 56.25%] [G loss: 0.589884]\n",
      "epoch:11 step:10728 [D loss: 0.602175, acc.: 64.06%] [G loss: 0.475320]\n",
      "epoch:11 step:10729 [D loss: 0.559996, acc.: 71.88%] [G loss: 0.534495]\n",
      "epoch:11 step:10730 [D loss: 0.559077, acc.: 71.09%] [G loss: 0.558134]\n",
      "epoch:11 step:10731 [D loss: 0.585195, acc.: 65.62%] [G loss: 0.558611]\n",
      "epoch:11 step:10732 [D loss: 0.522331, acc.: 75.78%] [G loss: 0.707058]\n",
      "epoch:11 step:10733 [D loss: 0.595412, acc.: 67.97%] [G loss: 0.508515]\n",
      "epoch:11 step:10734 [D loss: 0.517098, acc.: 75.78%] [G loss: 0.614418]\n",
      "epoch:11 step:10735 [D loss: 0.502874, acc.: 75.00%] [G loss: 0.563230]\n",
      "epoch:11 step:10736 [D loss: 0.483866, acc.: 75.78%] [G loss: 0.803731]\n",
      "epoch:11 step:10737 [D loss: 0.518952, acc.: 72.66%] [G loss: 0.747367]\n",
      "epoch:11 step:10738 [D loss: 0.529108, acc.: 74.22%] [G loss: 0.680668]\n",
      "epoch:11 step:10739 [D loss: 0.535277, acc.: 70.31%] [G loss: 0.756444]\n",
      "epoch:11 step:10740 [D loss: 0.606878, acc.: 67.19%] [G loss: 0.551542]\n",
      "epoch:11 step:10741 [D loss: 0.491543, acc.: 74.22%] [G loss: 0.643112]\n",
      "epoch:11 step:10742 [D loss: 0.565540, acc.: 67.97%] [G loss: 0.601203]\n",
      "epoch:11 step:10743 [D loss: 0.497623, acc.: 76.56%] [G loss: 0.556861]\n",
      "epoch:11 step:10744 [D loss: 0.654974, acc.: 60.94%] [G loss: 0.513112]\n",
      "epoch:11 step:10745 [D loss: 0.611634, acc.: 62.50%] [G loss: 0.522690]\n",
      "epoch:11 step:10746 [D loss: 0.572830, acc.: 71.88%] [G loss: 0.581806]\n",
      "epoch:11 step:10747 [D loss: 0.462232, acc.: 78.91%] [G loss: 0.759716]\n",
      "epoch:11 step:10748 [D loss: 0.559730, acc.: 69.53%] [G loss: 0.574209]\n",
      "epoch:11 step:10749 [D loss: 0.541252, acc.: 71.88%] [G loss: 0.646953]\n",
      "epoch:11 step:10750 [D loss: 0.573799, acc.: 70.31%] [G loss: 0.633042]\n",
      "epoch:11 step:10751 [D loss: 0.571617, acc.: 71.09%] [G loss: 0.565876]\n",
      "epoch:11 step:10752 [D loss: 0.594030, acc.: 62.50%] [G loss: 0.537452]\n",
      "epoch:11 step:10753 [D loss: 0.551334, acc.: 70.31%] [G loss: 0.686015]\n",
      "epoch:11 step:10754 [D loss: 0.525975, acc.: 70.31%] [G loss: 0.717331]\n",
      "epoch:11 step:10755 [D loss: 0.510168, acc.: 72.66%] [G loss: 0.696659]\n",
      "epoch:11 step:10756 [D loss: 0.505562, acc.: 74.22%] [G loss: 0.669914]\n",
      "epoch:11 step:10757 [D loss: 0.488713, acc.: 76.56%] [G loss: 0.924435]\n",
      "epoch:11 step:10758 [D loss: 0.425139, acc.: 82.81%] [G loss: 0.744759]\n",
      "epoch:11 step:10759 [D loss: 0.513013, acc.: 69.53%] [G loss: 0.793766]\n",
      "epoch:11 step:10760 [D loss: 0.500856, acc.: 71.88%] [G loss: 0.679790]\n",
      "epoch:11 step:10761 [D loss: 0.580916, acc.: 71.09%] [G loss: 0.559990]\n",
      "epoch:11 step:10762 [D loss: 0.534938, acc.: 74.22%] [G loss: 0.448609]\n",
      "epoch:11 step:10763 [D loss: 0.645509, acc.: 64.06%] [G loss: 0.511157]\n",
      "epoch:11 step:10764 [D loss: 0.530415, acc.: 74.22%] [G loss: 0.647984]\n",
      "epoch:11 step:10765 [D loss: 0.610055, acc.: 67.19%] [G loss: 0.618753]\n",
      "epoch:11 step:10766 [D loss: 0.558583, acc.: 65.62%] [G loss: 0.550509]\n",
      "epoch:11 step:10767 [D loss: 0.557132, acc.: 69.53%] [G loss: 0.645182]\n",
      "epoch:11 step:10768 [D loss: 0.568587, acc.: 67.97%] [G loss: 0.507153]\n",
      "epoch:11 step:10769 [D loss: 0.543928, acc.: 65.62%] [G loss: 0.591940]\n",
      "epoch:11 step:10770 [D loss: 0.547422, acc.: 69.53%] [G loss: 0.391203]\n",
      "epoch:11 step:10771 [D loss: 0.541898, acc.: 71.88%] [G loss: 0.564317]\n",
      "epoch:11 step:10772 [D loss: 0.592593, acc.: 67.97%] [G loss: 0.605925]\n",
      "epoch:11 step:10773 [D loss: 0.576780, acc.: 64.84%] [G loss: 0.538779]\n",
      "epoch:11 step:10774 [D loss: 0.615177, acc.: 69.53%] [G loss: 0.673281]\n",
      "epoch:11 step:10775 [D loss: 0.516170, acc.: 75.00%] [G loss: 0.575012]\n",
      "epoch:11 step:10776 [D loss: 0.504302, acc.: 76.56%] [G loss: 0.523750]\n",
      "epoch:11 step:10777 [D loss: 0.519888, acc.: 72.66%] [G loss: 0.605540]\n",
      "epoch:11 step:10778 [D loss: 0.488979, acc.: 76.56%] [G loss: 0.821630]\n",
      "epoch:11 step:10779 [D loss: 0.433696, acc.: 81.25%] [G loss: 0.819699]\n",
      "epoch:11 step:10780 [D loss: 0.629398, acc.: 61.72%] [G loss: 0.608755]\n",
      "epoch:11 step:10781 [D loss: 0.520947, acc.: 73.44%] [G loss: 0.639760]\n",
      "epoch:11 step:10782 [D loss: 0.521031, acc.: 74.22%] [G loss: 0.704537]\n",
      "epoch:11 step:10783 [D loss: 0.582524, acc.: 66.41%] [G loss: 0.583663]\n",
      "epoch:11 step:10784 [D loss: 0.733999, acc.: 56.25%] [G loss: 0.413639]\n",
      "epoch:11 step:10785 [D loss: 0.591341, acc.: 64.06%] [G loss: 0.382011]\n",
      "epoch:11 step:10786 [D loss: 0.493979, acc.: 76.56%] [G loss: 0.521606]\n",
      "epoch:11 step:10787 [D loss: 0.606986, acc.: 65.62%] [G loss: 0.542826]\n",
      "epoch:11 step:10788 [D loss: 0.517468, acc.: 75.78%] [G loss: 0.539564]\n",
      "epoch:11 step:10789 [D loss: 0.593445, acc.: 66.41%] [G loss: 0.451512]\n",
      "epoch:11 step:10790 [D loss: 0.545545, acc.: 73.44%] [G loss: 0.509439]\n",
      "epoch:11 step:10791 [D loss: 0.550289, acc.: 71.88%] [G loss: 0.644916]\n",
      "epoch:11 step:10792 [D loss: 0.544206, acc.: 72.66%] [G loss: 0.761966]\n",
      "epoch:11 step:10793 [D loss: 0.615489, acc.: 60.94%] [G loss: 0.657522]\n",
      "epoch:11 step:10794 [D loss: 0.615727, acc.: 63.28%] [G loss: 0.540932]\n",
      "epoch:11 step:10795 [D loss: 0.527511, acc.: 70.31%] [G loss: 0.572518]\n",
      "epoch:11 step:10796 [D loss: 0.540935, acc.: 73.44%] [G loss: 0.569304]\n",
      "epoch:11 step:10797 [D loss: 0.541373, acc.: 73.44%] [G loss: 0.567132]\n",
      "epoch:11 step:10798 [D loss: 0.543850, acc.: 67.97%] [G loss: 0.543496]\n",
      "epoch:11 step:10799 [D loss: 0.581419, acc.: 68.75%] [G loss: 0.475424]\n",
      "epoch:11 step:10800 [D loss: 0.582592, acc.: 68.75%] [G loss: 0.513966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.522903\n",
      "FID: 41.308891\n",
      "0 = 12.657835997009329\n",
      "1 = 0.08072752591998661\n",
      "2 = 0.894599974155426\n",
      "3 = 0.8267999887466431\n",
      "4 = 0.9624000191688538\n",
      "5 = 0.9565016031265259\n",
      "6 = 0.8267999887466431\n",
      "7 = 7.964543601155264\n",
      "8 = 0.1337293505548767\n",
      "9 = 0.7361000180244446\n",
      "10 = 0.7233999967575073\n",
      "11 = 0.7487999796867371\n",
      "12 = 0.7422532439231873\n",
      "13 = 0.7233999967575073\n",
      "14 = 6.522932052612305\n",
      "15 = 7.475067138671875\n",
      "16 = 0.34767699241638184\n",
      "17 = 6.522902965545654\n",
      "18 = 41.30889129638672\n",
      "epoch:11 step:10801 [D loss: 0.592109, acc.: 67.97%] [G loss: 0.586147]\n",
      "epoch:11 step:10802 [D loss: 0.498906, acc.: 76.56%] [G loss: 0.491062]\n",
      "epoch:11 step:10803 [D loss: 0.649774, acc.: 63.28%] [G loss: 0.496521]\n",
      "epoch:11 step:10804 [D loss: 0.551666, acc.: 74.22%] [G loss: 0.538712]\n",
      "epoch:11 step:10805 [D loss: 0.530230, acc.: 69.53%] [G loss: 0.469456]\n",
      "epoch:11 step:10806 [D loss: 0.501246, acc.: 70.31%] [G loss: 0.839322]\n",
      "epoch:11 step:10807 [D loss: 0.592118, acc.: 67.19%] [G loss: 0.563554]\n",
      "epoch:11 step:10808 [D loss: 0.660676, acc.: 62.50%] [G loss: 0.498901]\n",
      "epoch:11 step:10809 [D loss: 0.593220, acc.: 61.72%] [G loss: 0.424475]\n",
      "epoch:11 step:10810 [D loss: 0.549859, acc.: 72.66%] [G loss: 0.639699]\n",
      "epoch:11 step:10811 [D loss: 0.494146, acc.: 76.56%] [G loss: 0.662794]\n",
      "epoch:11 step:10812 [D loss: 0.500167, acc.: 73.44%] [G loss: 0.708512]\n",
      "epoch:11 step:10813 [D loss: 0.592020, acc.: 67.97%] [G loss: 0.726557]\n",
      "epoch:11 step:10814 [D loss: 0.558199, acc.: 74.22%] [G loss: 0.667545]\n",
      "epoch:11 step:10815 [D loss: 0.482391, acc.: 79.69%] [G loss: 0.872062]\n",
      "epoch:11 step:10816 [D loss: 0.514809, acc.: 76.56%] [G loss: 0.773358]\n",
      "epoch:11 step:10817 [D loss: 0.659822, acc.: 61.72%] [G loss: 0.526051]\n",
      "epoch:11 step:10818 [D loss: 0.701654, acc.: 58.59%] [G loss: 0.404949]\n",
      "epoch:11 step:10819 [D loss: 0.619659, acc.: 62.50%] [G loss: 0.401989]\n",
      "epoch:11 step:10820 [D loss: 0.530674, acc.: 74.22%] [G loss: 0.507504]\n",
      "epoch:11 step:10821 [D loss: 0.530141, acc.: 71.09%] [G loss: 0.536221]\n",
      "epoch:11 step:10822 [D loss: 0.553767, acc.: 71.88%] [G loss: 0.541250]\n",
      "epoch:11 step:10823 [D loss: 0.529439, acc.: 70.31%] [G loss: 0.480327]\n",
      "epoch:11 step:10824 [D loss: 0.514595, acc.: 78.91%] [G loss: 0.617907]\n",
      "epoch:11 step:10825 [D loss: 0.543011, acc.: 69.53%] [G loss: 0.540864]\n",
      "epoch:11 step:10826 [D loss: 0.506102, acc.: 73.44%] [G loss: 0.499509]\n",
      "epoch:11 step:10827 [D loss: 0.507814, acc.: 74.22%] [G loss: 0.615986]\n",
      "epoch:11 step:10828 [D loss: 0.560227, acc.: 66.41%] [G loss: 0.423656]\n",
      "epoch:11 step:10829 [D loss: 0.538025, acc.: 71.09%] [G loss: 0.652483]\n",
      "epoch:11 step:10830 [D loss: 0.455175, acc.: 81.25%] [G loss: 0.695404]\n",
      "epoch:11 step:10831 [D loss: 0.566315, acc.: 70.31%] [G loss: 0.468063]\n",
      "epoch:11 step:10832 [D loss: 0.597035, acc.: 69.53%] [G loss: 0.555485]\n",
      "epoch:11 step:10833 [D loss: 0.534568, acc.: 71.88%] [G loss: 0.486494]\n",
      "epoch:11 step:10834 [D loss: 0.595865, acc.: 63.28%] [G loss: 0.415482]\n",
      "epoch:11 step:10835 [D loss: 0.723334, acc.: 53.91%] [G loss: 0.374604]\n",
      "epoch:11 step:10836 [D loss: 0.576096, acc.: 69.53%] [G loss: 0.473830]\n",
      "epoch:11 step:10837 [D loss: 0.523575, acc.: 77.34%] [G loss: 0.716630]\n",
      "epoch:11 step:10838 [D loss: 0.597278, acc.: 62.50%] [G loss: 0.550753]\n",
      "epoch:11 step:10839 [D loss: 0.572693, acc.: 71.88%] [G loss: 0.536649]\n",
      "epoch:11 step:10840 [D loss: 0.553017, acc.: 68.75%] [G loss: 0.469969]\n",
      "epoch:11 step:10841 [D loss: 0.536431, acc.: 66.41%] [G loss: 0.488281]\n",
      "epoch:11 step:10842 [D loss: 0.634005, acc.: 60.16%] [G loss: 0.414393]\n",
      "epoch:11 step:10843 [D loss: 0.528320, acc.: 74.22%] [G loss: 0.575027]\n",
      "epoch:11 step:10844 [D loss: 0.575979, acc.: 71.09%] [G loss: 0.493494]\n",
      "epoch:11 step:10845 [D loss: 0.553119, acc.: 65.62%] [G loss: 0.567735]\n",
      "epoch:11 step:10846 [D loss: 0.557952, acc.: 69.53%] [G loss: 0.504681]\n",
      "epoch:11 step:10847 [D loss: 0.543933, acc.: 70.31%] [G loss: 0.621340]\n",
      "epoch:11 step:10848 [D loss: 0.561304, acc.: 62.50%] [G loss: 0.436670]\n",
      "epoch:11 step:10849 [D loss: 0.607575, acc.: 67.19%] [G loss: 0.603101]\n",
      "epoch:11 step:10850 [D loss: 0.569667, acc.: 67.97%] [G loss: 0.499938]\n",
      "epoch:11 step:10851 [D loss: 0.558939, acc.: 74.22%] [G loss: 0.512362]\n",
      "epoch:11 step:10852 [D loss: 0.539148, acc.: 71.09%] [G loss: 0.619094]\n",
      "epoch:11 step:10853 [D loss: 0.534556, acc.: 78.12%] [G loss: 0.593672]\n",
      "epoch:11 step:10854 [D loss: 0.555847, acc.: 71.88%] [G loss: 0.605405]\n",
      "epoch:11 step:10855 [D loss: 0.492450, acc.: 76.56%] [G loss: 0.705195]\n",
      "epoch:11 step:10856 [D loss: 0.559510, acc.: 69.53%] [G loss: 0.608017]\n",
      "epoch:11 step:10857 [D loss: 0.572198, acc.: 67.19%] [G loss: 0.700684]\n",
      "epoch:11 step:10858 [D loss: 0.562550, acc.: 71.88%] [G loss: 0.542688]\n",
      "epoch:11 step:10859 [D loss: 0.528784, acc.: 75.00%] [G loss: 0.611438]\n",
      "epoch:11 step:10860 [D loss: 0.569396, acc.: 70.31%] [G loss: 0.478565]\n",
      "epoch:11 step:10861 [D loss: 0.484168, acc.: 72.66%] [G loss: 0.444941]\n",
      "epoch:11 step:10862 [D loss: 0.475498, acc.: 75.78%] [G loss: 0.584868]\n",
      "epoch:11 step:10863 [D loss: 0.513213, acc.: 74.22%] [G loss: 0.619665]\n",
      "epoch:11 step:10864 [D loss: 0.532215, acc.: 72.66%] [G loss: 0.663704]\n",
      "epoch:11 step:10865 [D loss: 0.515242, acc.: 73.44%] [G loss: 0.680710]\n",
      "epoch:11 step:10866 [D loss: 0.554260, acc.: 71.09%] [G loss: 0.656008]\n",
      "epoch:11 step:10867 [D loss: 0.565149, acc.: 68.75%] [G loss: 0.641442]\n",
      "epoch:11 step:10868 [D loss: 0.531606, acc.: 75.00%] [G loss: 0.675706]\n",
      "epoch:11 step:10869 [D loss: 0.579687, acc.: 64.84%] [G loss: 0.566187]\n",
      "epoch:11 step:10870 [D loss: 0.561411, acc.: 74.22%] [G loss: 0.418097]\n",
      "epoch:11 step:10871 [D loss: 0.552436, acc.: 68.75%] [G loss: 0.506074]\n",
      "epoch:11 step:10872 [D loss: 0.582536, acc.: 67.97%] [G loss: 0.494591]\n",
      "epoch:11 step:10873 [D loss: 0.675545, acc.: 63.28%] [G loss: 0.497305]\n",
      "epoch:11 step:10874 [D loss: 0.497754, acc.: 76.56%] [G loss: 0.575819]\n",
      "epoch:11 step:10875 [D loss: 0.494821, acc.: 74.22%] [G loss: 0.591360]\n",
      "epoch:11 step:10876 [D loss: 0.542806, acc.: 71.88%] [G loss: 0.494632]\n",
      "epoch:11 step:10877 [D loss: 0.506241, acc.: 74.22%] [G loss: 0.518799]\n",
      "epoch:11 step:10878 [D loss: 0.560291, acc.: 67.97%] [G loss: 0.505972]\n",
      "epoch:11 step:10879 [D loss: 0.566132, acc.: 68.75%] [G loss: 0.429936]\n",
      "epoch:11 step:10880 [D loss: 0.523893, acc.: 78.91%] [G loss: 0.697512]\n",
      "epoch:11 step:10881 [D loss: 0.444156, acc.: 81.25%] [G loss: 0.832406]\n",
      "epoch:11 step:10882 [D loss: 0.528796, acc.: 71.88%] [G loss: 0.608761]\n",
      "epoch:11 step:10883 [D loss: 0.625874, acc.: 63.28%] [G loss: 0.562993]\n",
      "epoch:11 step:10884 [D loss: 0.527925, acc.: 73.44%] [G loss: 0.624581]\n",
      "epoch:11 step:10885 [D loss: 0.518297, acc.: 75.00%] [G loss: 0.552958]\n",
      "epoch:11 step:10886 [D loss: 0.548267, acc.: 72.66%] [G loss: 0.606770]\n",
      "epoch:11 step:10887 [D loss: 0.562275, acc.: 68.75%] [G loss: 0.554148]\n",
      "epoch:11 step:10888 [D loss: 0.546291, acc.: 67.97%] [G loss: 0.655728]\n",
      "epoch:11 step:10889 [D loss: 0.452636, acc.: 83.59%] [G loss: 0.659087]\n",
      "epoch:11 step:10890 [D loss: 0.618299, acc.: 65.62%] [G loss: 0.597935]\n",
      "epoch:11 step:10891 [D loss: 0.628973, acc.: 61.72%] [G loss: 0.566340]\n",
      "epoch:11 step:10892 [D loss: 0.646146, acc.: 65.62%] [G loss: 0.563354]\n",
      "epoch:11 step:10893 [D loss: 0.597847, acc.: 64.06%] [G loss: 0.586366]\n",
      "epoch:11 step:10894 [D loss: 0.558854, acc.: 69.53%] [G loss: 0.606402]\n",
      "epoch:11 step:10895 [D loss: 0.504975, acc.: 75.78%] [G loss: 0.604877]\n",
      "epoch:11 step:10896 [D loss: 0.555338, acc.: 66.41%] [G loss: 0.678132]\n",
      "epoch:11 step:10897 [D loss: 0.623863, acc.: 62.50%] [G loss: 0.520896]\n",
      "epoch:11 step:10898 [D loss: 0.549045, acc.: 74.22%] [G loss: 0.630041]\n",
      "epoch:11 step:10899 [D loss: 0.498112, acc.: 78.91%] [G loss: 0.575694]\n",
      "epoch:11 step:10900 [D loss: 0.519352, acc.: 70.31%] [G loss: 0.694864]\n",
      "epoch:11 step:10901 [D loss: 0.574842, acc.: 66.41%] [G loss: 0.591489]\n",
      "epoch:11 step:10902 [D loss: 0.606848, acc.: 64.84%] [G loss: 0.485677]\n",
      "epoch:11 step:10903 [D loss: 0.552209, acc.: 75.00%] [G loss: 0.631962]\n",
      "epoch:11 step:10904 [D loss: 0.553683, acc.: 71.09%] [G loss: 0.416765]\n",
      "epoch:11 step:10905 [D loss: 0.520305, acc.: 73.44%] [G loss: 0.708324]\n",
      "epoch:11 step:10906 [D loss: 0.558826, acc.: 68.75%] [G loss: 0.628137]\n",
      "epoch:11 step:10907 [D loss: 0.604016, acc.: 64.06%] [G loss: 0.592812]\n",
      "epoch:11 step:10908 [D loss: 0.502330, acc.: 76.56%] [G loss: 0.538743]\n",
      "epoch:11 step:10909 [D loss: 0.560758, acc.: 64.84%] [G loss: 0.519751]\n",
      "epoch:11 step:10910 [D loss: 0.503588, acc.: 73.44%] [G loss: 0.607032]\n",
      "epoch:11 step:10911 [D loss: 0.547920, acc.: 71.88%] [G loss: 0.584472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10912 [D loss: 0.471129, acc.: 76.56%] [G loss: 0.572661]\n",
      "epoch:11 step:10913 [D loss: 0.589666, acc.: 65.62%] [G loss: 0.547355]\n",
      "epoch:11 step:10914 [D loss: 0.532526, acc.: 71.88%] [G loss: 0.636906]\n",
      "epoch:11 step:10915 [D loss: 0.564408, acc.: 64.84%] [G loss: 0.439727]\n",
      "epoch:11 step:10916 [D loss: 0.520759, acc.: 74.22%] [G loss: 0.525551]\n",
      "epoch:11 step:10917 [D loss: 0.572510, acc.: 70.31%] [G loss: 0.648486]\n",
      "epoch:11 step:10918 [D loss: 0.527657, acc.: 74.22%] [G loss: 0.563352]\n",
      "epoch:11 step:10919 [D loss: 0.571115, acc.: 67.19%] [G loss: 0.415640]\n",
      "epoch:11 step:10920 [D loss: 0.491718, acc.: 75.00%] [G loss: 0.421124]\n",
      "epoch:11 step:10921 [D loss: 0.551481, acc.: 68.75%] [G loss: 0.559827]\n",
      "epoch:11 step:10922 [D loss: 0.575678, acc.: 64.06%] [G loss: 0.645704]\n",
      "epoch:11 step:10923 [D loss: 0.578233, acc.: 66.41%] [G loss: 0.562348]\n",
      "epoch:11 step:10924 [D loss: 0.559164, acc.: 70.31%] [G loss: 0.585809]\n",
      "epoch:11 step:10925 [D loss: 0.578390, acc.: 70.31%] [G loss: 0.657758]\n",
      "epoch:11 step:10926 [D loss: 0.562263, acc.: 71.09%] [G loss: 0.699693]\n",
      "epoch:11 step:10927 [D loss: 0.535907, acc.: 71.09%] [G loss: 0.536889]\n",
      "epoch:11 step:10928 [D loss: 0.649089, acc.: 64.06%] [G loss: 0.408377]\n",
      "epoch:11 step:10929 [D loss: 0.668903, acc.: 61.72%] [G loss: 0.411066]\n",
      "epoch:11 step:10930 [D loss: 0.511941, acc.: 74.22%] [G loss: 0.517767]\n",
      "epoch:11 step:10931 [D loss: 0.481361, acc.: 77.34%] [G loss: 0.670242]\n",
      "epoch:11 step:10932 [D loss: 0.642905, acc.: 60.94%] [G loss: 0.613274]\n",
      "epoch:11 step:10933 [D loss: 0.526738, acc.: 71.88%] [G loss: 0.586435]\n",
      "epoch:11 step:10934 [D loss: 0.547002, acc.: 67.97%] [G loss: 0.489930]\n",
      "epoch:11 step:10935 [D loss: 0.590364, acc.: 67.19%] [G loss: 0.490158]\n",
      "epoch:11 step:10936 [D loss: 0.542940, acc.: 71.09%] [G loss: 0.519321]\n",
      "epoch:11 step:10937 [D loss: 0.519885, acc.: 72.66%] [G loss: 0.556443]\n",
      "epoch:11 step:10938 [D loss: 0.527589, acc.: 75.00%] [G loss: 0.550966]\n",
      "epoch:11 step:10939 [D loss: 0.497085, acc.: 77.34%] [G loss: 0.524184]\n",
      "epoch:11 step:10940 [D loss: 0.499952, acc.: 78.12%] [G loss: 0.708110]\n",
      "epoch:11 step:10941 [D loss: 0.501647, acc.: 75.78%] [G loss: 0.802602]\n",
      "epoch:11 step:10942 [D loss: 0.525248, acc.: 71.88%] [G loss: 0.587040]\n",
      "epoch:11 step:10943 [D loss: 0.654757, acc.: 64.06%] [G loss: 0.503114]\n",
      "epoch:11 step:10944 [D loss: 0.547562, acc.: 69.53%] [G loss: 0.707964]\n",
      "epoch:11 step:10945 [D loss: 0.573133, acc.: 69.53%] [G loss: 0.505172]\n",
      "epoch:11 step:10946 [D loss: 0.537102, acc.: 73.44%] [G loss: 0.523442]\n",
      "epoch:11 step:10947 [D loss: 0.561517, acc.: 70.31%] [G loss: 0.545880]\n",
      "epoch:11 step:10948 [D loss: 0.518373, acc.: 72.66%] [G loss: 0.712759]\n",
      "epoch:11 step:10949 [D loss: 0.465728, acc.: 80.47%] [G loss: 0.683320]\n",
      "epoch:11 step:10950 [D loss: 0.568922, acc.: 71.09%] [G loss: 0.799217]\n",
      "epoch:11 step:10951 [D loss: 0.567833, acc.: 72.66%] [G loss: 0.525565]\n",
      "epoch:11 step:10952 [D loss: 0.589033, acc.: 64.84%] [G loss: 0.427872]\n",
      "epoch:11 step:10953 [D loss: 0.522399, acc.: 72.66%] [G loss: 0.562943]\n",
      "epoch:11 step:10954 [D loss: 0.496632, acc.: 78.91%] [G loss: 0.695971]\n",
      "epoch:11 step:10955 [D loss: 0.391669, acc.: 85.16%] [G loss: 0.726423]\n",
      "epoch:11 step:10956 [D loss: 0.510195, acc.: 75.00%] [G loss: 0.724201]\n",
      "epoch:11 step:10957 [D loss: 0.604795, acc.: 67.97%] [G loss: 0.698727]\n",
      "epoch:11 step:10958 [D loss: 0.530021, acc.: 70.31%] [G loss: 0.686285]\n",
      "epoch:11 step:10959 [D loss: 0.648015, acc.: 60.94%] [G loss: 0.472358]\n",
      "epoch:11 step:10960 [D loss: 0.608882, acc.: 64.06%] [G loss: 0.499934]\n",
      "epoch:11 step:10961 [D loss: 0.547568, acc.: 72.66%] [G loss: 0.601953]\n",
      "epoch:11 step:10962 [D loss: 0.620897, acc.: 66.41%] [G loss: 0.534091]\n",
      "epoch:11 step:10963 [D loss: 0.513286, acc.: 77.34%] [G loss: 0.540682]\n",
      "epoch:11 step:10964 [D loss: 0.550610, acc.: 66.41%] [G loss: 0.613236]\n",
      "epoch:11 step:10965 [D loss: 0.559747, acc.: 70.31%] [G loss: 0.587517]\n",
      "epoch:11 step:10966 [D loss: 0.526626, acc.: 71.88%] [G loss: 0.550833]\n",
      "epoch:11 step:10967 [D loss: 0.564150, acc.: 69.53%] [G loss: 0.666460]\n",
      "epoch:11 step:10968 [D loss: 0.562695, acc.: 67.97%] [G loss: 0.526006]\n",
      "epoch:11 step:10969 [D loss: 0.565378, acc.: 71.88%] [G loss: 0.612566]\n",
      "epoch:11 step:10970 [D loss: 0.590678, acc.: 67.97%] [G loss: 0.555126]\n",
      "epoch:11 step:10971 [D loss: 0.547668, acc.: 70.31%] [G loss: 0.631169]\n",
      "epoch:11 step:10972 [D loss: 0.595996, acc.: 65.62%] [G loss: 0.537590]\n",
      "epoch:11 step:10973 [D loss: 0.557947, acc.: 71.09%] [G loss: 0.562088]\n",
      "epoch:11 step:10974 [D loss: 0.559567, acc.: 67.97%] [G loss: 0.579552]\n",
      "epoch:11 step:10975 [D loss: 0.522821, acc.: 77.34%] [G loss: 0.525926]\n",
      "epoch:11 step:10976 [D loss: 0.520864, acc.: 72.66%] [G loss: 0.673064]\n",
      "epoch:11 step:10977 [D loss: 0.550504, acc.: 71.88%] [G loss: 0.633285]\n",
      "epoch:11 step:10978 [D loss: 0.576498, acc.: 69.53%] [G loss: 0.483026]\n",
      "epoch:11 step:10979 [D loss: 0.590354, acc.: 69.53%] [G loss: 0.565599]\n",
      "epoch:11 step:10980 [D loss: 0.551270, acc.: 70.31%] [G loss: 0.587435]\n",
      "epoch:11 step:10981 [D loss: 0.660247, acc.: 61.72%] [G loss: 0.563812]\n",
      "epoch:11 step:10982 [D loss: 0.550937, acc.: 74.22%] [G loss: 0.515733]\n",
      "epoch:11 step:10983 [D loss: 0.576994, acc.: 71.09%] [G loss: 0.650518]\n",
      "epoch:11 step:10984 [D loss: 0.492987, acc.: 76.56%] [G loss: 0.563441]\n",
      "epoch:11 step:10985 [D loss: 0.592571, acc.: 66.41%] [G loss: 0.503691]\n",
      "epoch:11 step:10986 [D loss: 0.528026, acc.: 70.31%] [G loss: 0.537487]\n",
      "epoch:11 step:10987 [D loss: 0.534181, acc.: 72.66%] [G loss: 0.749638]\n",
      "epoch:11 step:10988 [D loss: 0.507709, acc.: 76.56%] [G loss: 0.663178]\n",
      "epoch:11 step:10989 [D loss: 0.512513, acc.: 72.66%] [G loss: 0.543678]\n",
      "epoch:11 step:10990 [D loss: 0.560886, acc.: 67.19%] [G loss: 0.738667]\n",
      "epoch:11 step:10991 [D loss: 0.603717, acc.: 64.84%] [G loss: 0.499097]\n",
      "epoch:11 step:10992 [D loss: 0.559680, acc.: 71.88%] [G loss: 0.485434]\n",
      "epoch:11 step:10993 [D loss: 0.589645, acc.: 64.84%] [G loss: 0.479493]\n",
      "epoch:11 step:10994 [D loss: 0.539857, acc.: 76.56%] [G loss: 0.455892]\n",
      "epoch:11 step:10995 [D loss: 0.553125, acc.: 71.88%] [G loss: 0.616787]\n",
      "epoch:11 step:10996 [D loss: 0.571770, acc.: 67.97%] [G loss: 0.516141]\n",
      "epoch:11 step:10997 [D loss: 0.531645, acc.: 74.22%] [G loss: 0.663476]\n",
      "epoch:11 step:10998 [D loss: 0.515322, acc.: 76.56%] [G loss: 0.593425]\n",
      "epoch:11 step:10999 [D loss: 0.545951, acc.: 75.78%] [G loss: 0.552337]\n",
      "epoch:11 step:11000 [D loss: 0.479599, acc.: 75.00%] [G loss: 0.666572]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.608178\n",
      "FID: 39.583397\n",
      "0 = 12.99114517707826\n",
      "1 = 0.09395767098903157\n",
      "2 = 0.8971999883651733\n",
      "3 = 0.8325999975204468\n",
      "4 = 0.9617999792098999\n",
      "5 = 0.9561322927474976\n",
      "6 = 0.8325999975204468\n",
      "7 = 7.749144202136978\n",
      "8 = 0.12548258133942133\n",
      "9 = 0.7312999963760376\n",
      "10 = 0.7146000266075134\n",
      "11 = 0.7480000257492065\n",
      "12 = 0.7392923831939697\n",
      "13 = 0.7146000266075134\n",
      "14 = 6.608202934265137\n",
      "15 = 7.541398525238037\n",
      "16 = 0.3444713056087494\n",
      "17 = 6.608177661895752\n",
      "18 = 39.583396911621094\n",
      "epoch:11 step:11001 [D loss: 0.474209, acc.: 79.69%] [G loss: 0.512295]\n",
      "epoch:11 step:11002 [D loss: 0.595457, acc.: 67.97%] [G loss: 0.560201]\n",
      "epoch:11 step:11003 [D loss: 0.667245, acc.: 53.91%] [G loss: 0.486973]\n",
      "epoch:11 step:11004 [D loss: 0.542331, acc.: 71.88%] [G loss: 0.603059]\n",
      "epoch:11 step:11005 [D loss: 0.560501, acc.: 71.09%] [G loss: 0.770541]\n",
      "epoch:11 step:11006 [D loss: 0.481400, acc.: 76.56%] [G loss: 0.629450]\n",
      "epoch:11 step:11007 [D loss: 0.530682, acc.: 74.22%] [G loss: 0.810528]\n",
      "epoch:11 step:11008 [D loss: 0.571469, acc.: 67.97%] [G loss: 0.695470]\n",
      "epoch:11 step:11009 [D loss: 0.614242, acc.: 69.53%] [G loss: 0.593046]\n",
      "epoch:11 step:11010 [D loss: 0.584119, acc.: 66.41%] [G loss: 0.554639]\n",
      "epoch:11 step:11011 [D loss: 0.577356, acc.: 67.19%] [G loss: 0.593124]\n",
      "epoch:11 step:11012 [D loss: 0.517998, acc.: 71.88%] [G loss: 0.596800]\n",
      "epoch:11 step:11013 [D loss: 0.555863, acc.: 70.31%] [G loss: 0.512805]\n",
      "epoch:11 step:11014 [D loss: 0.499979, acc.: 75.00%] [G loss: 0.558085]\n",
      "epoch:11 step:11015 [D loss: 0.536522, acc.: 72.66%] [G loss: 0.624678]\n",
      "epoch:11 step:11016 [D loss: 0.524306, acc.: 71.09%] [G loss: 0.767745]\n",
      "epoch:11 step:11017 [D loss: 0.628046, acc.: 64.06%] [G loss: 0.659066]\n",
      "epoch:11 step:11018 [D loss: 0.608533, acc.: 61.72%] [G loss: 0.521812]\n",
      "epoch:11 step:11019 [D loss: 0.533229, acc.: 70.31%] [G loss: 0.543748]\n",
      "epoch:11 step:11020 [D loss: 0.605155, acc.: 60.94%] [G loss: 0.491067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11021 [D loss: 0.542298, acc.: 71.09%] [G loss: 0.499128]\n",
      "epoch:11 step:11022 [D loss: 0.550223, acc.: 70.31%] [G loss: 0.475106]\n",
      "epoch:11 step:11023 [D loss: 0.662350, acc.: 60.16%] [G loss: 0.418621]\n",
      "epoch:11 step:11024 [D loss: 0.549565, acc.: 71.09%] [G loss: 0.491487]\n",
      "epoch:11 step:11025 [D loss: 0.578168, acc.: 67.97%] [G loss: 0.534384]\n",
      "epoch:11 step:11026 [D loss: 0.521315, acc.: 75.78%] [G loss: 0.575789]\n",
      "epoch:11 step:11027 [D loss: 0.595086, acc.: 71.09%] [G loss: 0.541318]\n",
      "epoch:11 step:11028 [D loss: 0.598197, acc.: 67.19%] [G loss: 0.466163]\n",
      "epoch:11 step:11029 [D loss: 0.541986, acc.: 73.44%] [G loss: 0.447314]\n",
      "epoch:11 step:11030 [D loss: 0.592745, acc.: 61.72%] [G loss: 0.591724]\n",
      "epoch:11 step:11031 [D loss: 0.589682, acc.: 70.31%] [G loss: 0.428834]\n",
      "epoch:11 step:11032 [D loss: 0.516230, acc.: 73.44%] [G loss: 0.657210]\n",
      "epoch:11 step:11033 [D loss: 0.566683, acc.: 74.22%] [G loss: 0.628329]\n",
      "epoch:11 step:11034 [D loss: 0.557209, acc.: 67.97%] [G loss: 0.625379]\n",
      "epoch:11 step:11035 [D loss: 0.532934, acc.: 71.09%] [G loss: 0.584028]\n",
      "epoch:11 step:11036 [D loss: 0.623368, acc.: 57.81%] [G loss: 0.560884]\n",
      "epoch:11 step:11037 [D loss: 0.473778, acc.: 81.25%] [G loss: 0.607857]\n",
      "epoch:11 step:11038 [D loss: 0.634790, acc.: 57.81%] [G loss: 0.502411]\n",
      "epoch:11 step:11039 [D loss: 0.564885, acc.: 64.06%] [G loss: 0.652982]\n",
      "epoch:11 step:11040 [D loss: 0.526084, acc.: 75.78%] [G loss: 0.587322]\n",
      "epoch:11 step:11041 [D loss: 0.618393, acc.: 68.75%] [G loss: 0.471761]\n",
      "epoch:11 step:11042 [D loss: 0.547562, acc.: 68.75%] [G loss: 0.541710]\n",
      "epoch:11 step:11043 [D loss: 0.495380, acc.: 69.53%] [G loss: 0.630965]\n",
      "epoch:11 step:11044 [D loss: 0.518025, acc.: 71.09%] [G loss: 0.752013]\n",
      "epoch:11 step:11045 [D loss: 0.598231, acc.: 66.41%] [G loss: 0.653485]\n",
      "epoch:11 step:11046 [D loss: 0.623447, acc.: 64.84%] [G loss: 0.471830]\n",
      "epoch:11 step:11047 [D loss: 0.675753, acc.: 53.12%] [G loss: 0.359374]\n",
      "epoch:11 step:11048 [D loss: 0.529210, acc.: 67.19%] [G loss: 0.567456]\n",
      "epoch:11 step:11049 [D loss: 0.573339, acc.: 68.75%] [G loss: 0.454418]\n",
      "epoch:11 step:11050 [D loss: 0.504665, acc.: 75.78%] [G loss: 0.583439]\n",
      "epoch:11 step:11051 [D loss: 0.521675, acc.: 75.78%] [G loss: 0.512686]\n",
      "epoch:11 step:11052 [D loss: 0.642184, acc.: 60.16%] [G loss: 0.495244]\n",
      "epoch:11 step:11053 [D loss: 0.479862, acc.: 78.12%] [G loss: 0.583966]\n",
      "epoch:11 step:11054 [D loss: 0.467756, acc.: 72.66%] [G loss: 0.675912]\n",
      "epoch:11 step:11055 [D loss: 0.562867, acc.: 71.09%] [G loss: 0.699730]\n",
      "epoch:11 step:11056 [D loss: 0.573549, acc.: 68.75%] [G loss: 0.518486]\n",
      "epoch:11 step:11057 [D loss: 0.565626, acc.: 71.09%] [G loss: 0.696652]\n",
      "epoch:11 step:11058 [D loss: 0.570704, acc.: 66.41%] [G loss: 0.469429]\n",
      "epoch:11 step:11059 [D loss: 0.581026, acc.: 69.53%] [G loss: 0.563628]\n",
      "epoch:11 step:11060 [D loss: 0.586083, acc.: 67.19%] [G loss: 0.574150]\n",
      "epoch:11 step:11061 [D loss: 0.557971, acc.: 66.41%] [G loss: 0.522861]\n",
      "epoch:11 step:11062 [D loss: 0.539078, acc.: 69.53%] [G loss: 0.582396]\n",
      "epoch:11 step:11063 [D loss: 0.571459, acc.: 73.44%] [G loss: 0.597316]\n",
      "epoch:11 step:11064 [D loss: 0.527520, acc.: 67.19%] [G loss: 0.563320]\n",
      "epoch:11 step:11065 [D loss: 0.531228, acc.: 71.09%] [G loss: 0.527270]\n",
      "epoch:11 step:11066 [D loss: 0.624176, acc.: 60.94%] [G loss: 0.504342]\n",
      "epoch:11 step:11067 [D loss: 0.578011, acc.: 67.97%] [G loss: 0.470534]\n",
      "epoch:11 step:11068 [D loss: 0.582916, acc.: 64.84%] [G loss: 0.458597]\n",
      "epoch:11 step:11069 [D loss: 0.594329, acc.: 64.06%] [G loss: 0.518599]\n",
      "epoch:11 step:11070 [D loss: 0.520528, acc.: 75.00%] [G loss: 0.575555]\n",
      "epoch:11 step:11071 [D loss: 0.573110, acc.: 69.53%] [G loss: 0.467443]\n",
      "epoch:11 step:11072 [D loss: 0.664780, acc.: 64.06%] [G loss: 0.526971]\n",
      "epoch:11 step:11073 [D loss: 0.731785, acc.: 58.59%] [G loss: 0.377614]\n",
      "epoch:11 step:11074 [D loss: 0.556264, acc.: 70.31%] [G loss: 0.604136]\n",
      "epoch:11 step:11075 [D loss: 0.607278, acc.: 69.53%] [G loss: 0.581148]\n",
      "epoch:11 step:11076 [D loss: 0.470836, acc.: 77.34%] [G loss: 0.937485]\n",
      "epoch:11 step:11077 [D loss: 0.579480, acc.: 68.75%] [G loss: 0.778530]\n",
      "epoch:11 step:11078 [D loss: 0.503142, acc.: 75.78%] [G loss: 0.674066]\n",
      "epoch:11 step:11079 [D loss: 0.558859, acc.: 72.66%] [G loss: 0.632504]\n",
      "epoch:11 step:11080 [D loss: 0.567009, acc.: 70.31%] [G loss: 0.630165]\n",
      "epoch:11 step:11081 [D loss: 0.569600, acc.: 73.44%] [G loss: 0.524261]\n",
      "epoch:11 step:11082 [D loss: 0.552983, acc.: 67.97%] [G loss: 0.675807]\n",
      "epoch:11 step:11083 [D loss: 0.603852, acc.: 66.41%] [G loss: 0.516495]\n",
      "epoch:11 step:11084 [D loss: 0.542801, acc.: 71.09%] [G loss: 0.570396]\n",
      "epoch:11 step:11085 [D loss: 0.560125, acc.: 70.31%] [G loss: 0.466364]\n",
      "epoch:11 step:11086 [D loss: 0.544392, acc.: 67.19%] [G loss: 0.517478]\n",
      "epoch:11 step:11087 [D loss: 0.585766, acc.: 67.19%] [G loss: 0.407346]\n",
      "epoch:11 step:11088 [D loss: 0.552654, acc.: 65.62%] [G loss: 0.593764]\n",
      "epoch:11 step:11089 [D loss: 0.507021, acc.: 77.34%] [G loss: 0.659285]\n",
      "epoch:11 step:11090 [D loss: 0.589279, acc.: 70.31%] [G loss: 0.650894]\n",
      "epoch:11 step:11091 [D loss: 0.637072, acc.: 60.94%] [G loss: 0.591301]\n",
      "epoch:11 step:11092 [D loss: 0.549126, acc.: 70.31%] [G loss: 0.576347]\n",
      "epoch:11 step:11093 [D loss: 0.548533, acc.: 68.75%] [G loss: 0.447683]\n",
      "epoch:11 step:11094 [D loss: 0.567297, acc.: 71.88%] [G loss: 0.527035]\n",
      "epoch:11 step:11095 [D loss: 0.680264, acc.: 57.81%] [G loss: 0.436639]\n",
      "epoch:11 step:11096 [D loss: 0.519785, acc.: 72.66%] [G loss: 0.572331]\n",
      "epoch:11 step:11097 [D loss: 0.555232, acc.: 67.97%] [G loss: 0.469335]\n",
      "epoch:11 step:11098 [D loss: 0.565071, acc.: 64.06%] [G loss: 0.546356]\n",
      "epoch:11 step:11099 [D loss: 0.493038, acc.: 71.88%] [G loss: 0.624663]\n",
      "epoch:11 step:11100 [D loss: 0.649788, acc.: 57.03%] [G loss: 0.506995]\n",
      "epoch:11 step:11101 [D loss: 0.700465, acc.: 53.91%] [G loss: 0.456715]\n",
      "epoch:11 step:11102 [D loss: 0.564799, acc.: 67.19%] [G loss: 0.485408]\n",
      "epoch:11 step:11103 [D loss: 0.515436, acc.: 75.78%] [G loss: 0.673597]\n",
      "epoch:11 step:11104 [D loss: 0.587689, acc.: 64.06%] [G loss: 0.642068]\n",
      "epoch:11 step:11105 [D loss: 0.548734, acc.: 67.97%] [G loss: 0.494357]\n",
      "epoch:11 step:11106 [D loss: 0.594656, acc.: 68.75%] [G loss: 0.515199]\n",
      "epoch:11 step:11107 [D loss: 0.580883, acc.: 63.28%] [G loss: 0.526013]\n",
      "epoch:11 step:11108 [D loss: 0.483035, acc.: 79.69%] [G loss: 0.671276]\n",
      "epoch:11 step:11109 [D loss: 0.508150, acc.: 79.69%] [G loss: 0.696833]\n",
      "epoch:11 step:11110 [D loss: 0.539776, acc.: 77.34%] [G loss: 0.704879]\n",
      "epoch:11 step:11111 [D loss: 0.609326, acc.: 67.19%] [G loss: 0.463096]\n",
      "epoch:11 step:11112 [D loss: 0.533513, acc.: 73.44%] [G loss: 0.504949]\n",
      "epoch:11 step:11113 [D loss: 0.618345, acc.: 60.16%] [G loss: 0.477296]\n",
      "epoch:11 step:11114 [D loss: 0.528620, acc.: 71.09%] [G loss: 0.515193]\n",
      "epoch:11 step:11115 [D loss: 0.561746, acc.: 69.53%] [G loss: 0.504436]\n",
      "epoch:11 step:11116 [D loss: 0.540618, acc.: 73.44%] [G loss: 0.518078]\n",
      "epoch:11 step:11117 [D loss: 0.491547, acc.: 75.78%] [G loss: 0.598250]\n",
      "epoch:11 step:11118 [D loss: 0.575929, acc.: 67.19%] [G loss: 0.429045]\n",
      "epoch:11 step:11119 [D loss: 0.670763, acc.: 63.28%] [G loss: 0.504088]\n",
      "epoch:11 step:11120 [D loss: 0.603430, acc.: 63.28%] [G loss: 0.424720]\n",
      "epoch:11 step:11121 [D loss: 0.566892, acc.: 69.53%] [G loss: 0.515613]\n",
      "epoch:11 step:11122 [D loss: 0.527727, acc.: 72.66%] [G loss: 0.684216]\n",
      "epoch:11 step:11123 [D loss: 0.581809, acc.: 67.97%] [G loss: 0.709901]\n",
      "epoch:11 step:11124 [D loss: 0.646481, acc.: 61.72%] [G loss: 0.559875]\n",
      "epoch:11 step:11125 [D loss: 0.566452, acc.: 69.53%] [G loss: 0.611910]\n",
      "epoch:11 step:11126 [D loss: 0.523232, acc.: 74.22%] [G loss: 0.532293]\n",
      "epoch:11 step:11127 [D loss: 0.595863, acc.: 71.09%] [G loss: 0.481649]\n",
      "epoch:11 step:11128 [D loss: 0.542802, acc.: 74.22%] [G loss: 0.530850]\n",
      "epoch:11 step:11129 [D loss: 0.566031, acc.: 65.62%] [G loss: 0.468648]\n",
      "epoch:11 step:11130 [D loss: 0.478710, acc.: 73.44%] [G loss: 0.677903]\n",
      "epoch:11 step:11131 [D loss: 0.570071, acc.: 72.66%] [G loss: 0.484360]\n",
      "epoch:11 step:11132 [D loss: 0.556266, acc.: 71.09%] [G loss: 0.508431]\n",
      "epoch:11 step:11133 [D loss: 0.551805, acc.: 75.78%] [G loss: 0.448280]\n",
      "epoch:11 step:11134 [D loss: 0.557416, acc.: 70.31%] [G loss: 0.448290]\n",
      "epoch:11 step:11135 [D loss: 0.636382, acc.: 62.50%] [G loss: 0.505472]\n",
      "epoch:11 step:11136 [D loss: 0.518641, acc.: 75.00%] [G loss: 0.456160]\n",
      "epoch:11 step:11137 [D loss: 0.571445, acc.: 72.66%] [G loss: 0.462662]\n",
      "epoch:11 step:11138 [D loss: 0.566072, acc.: 73.44%] [G loss: 0.511277]\n",
      "epoch:11 step:11139 [D loss: 0.612938, acc.: 64.84%] [G loss: 0.479739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11140 [D loss: 0.503594, acc.: 77.34%] [G loss: 0.619176]\n",
      "epoch:11 step:11141 [D loss: 0.561517, acc.: 66.41%] [G loss: 0.504795]\n",
      "epoch:11 step:11142 [D loss: 0.564879, acc.: 68.75%] [G loss: 0.558128]\n",
      "epoch:11 step:11143 [D loss: 0.512116, acc.: 75.00%] [G loss: 0.549486]\n",
      "epoch:11 step:11144 [D loss: 0.542458, acc.: 67.97%] [G loss: 0.533258]\n",
      "epoch:11 step:11145 [D loss: 0.539416, acc.: 69.53%] [G loss: 0.461446]\n",
      "epoch:11 step:11146 [D loss: 0.585966, acc.: 64.84%] [G loss: 0.548888]\n",
      "epoch:11 step:11147 [D loss: 0.612107, acc.: 64.84%] [G loss: 0.479210]\n",
      "epoch:11 step:11148 [D loss: 0.564244, acc.: 71.09%] [G loss: 0.412137]\n",
      "epoch:11 step:11149 [D loss: 0.559433, acc.: 68.75%] [G loss: 0.496800]\n",
      "epoch:11 step:11150 [D loss: 0.544017, acc.: 75.00%] [G loss: 0.676093]\n",
      "epoch:11 step:11151 [D loss: 0.556520, acc.: 71.09%] [G loss: 0.722818]\n",
      "epoch:11 step:11152 [D loss: 0.621848, acc.: 60.16%] [G loss: 0.541489]\n",
      "epoch:11 step:11153 [D loss: 0.562114, acc.: 67.19%] [G loss: 0.553093]\n",
      "epoch:11 step:11154 [D loss: 0.620926, acc.: 65.62%] [G loss: 0.366758]\n",
      "epoch:11 step:11155 [D loss: 0.561522, acc.: 64.84%] [G loss: 0.365997]\n",
      "epoch:11 step:11156 [D loss: 0.557592, acc.: 72.66%] [G loss: 0.514068]\n",
      "epoch:11 step:11157 [D loss: 0.575839, acc.: 67.97%] [G loss: 0.420216]\n",
      "epoch:11 step:11158 [D loss: 0.626065, acc.: 60.94%] [G loss: 0.599054]\n",
      "epoch:11 step:11159 [D loss: 0.509549, acc.: 72.66%] [G loss: 0.579210]\n",
      "epoch:11 step:11160 [D loss: 0.560733, acc.: 70.31%] [G loss: 0.600743]\n",
      "epoch:11 step:11161 [D loss: 0.552873, acc.: 69.53%] [G loss: 0.590368]\n",
      "epoch:11 step:11162 [D loss: 0.576216, acc.: 66.41%] [G loss: 0.628254]\n",
      "epoch:11 step:11163 [D loss: 0.591773, acc.: 63.28%] [G loss: 0.525603]\n",
      "epoch:11 step:11164 [D loss: 0.452145, acc.: 80.47%] [G loss: 0.569784]\n",
      "epoch:11 step:11165 [D loss: 0.660533, acc.: 62.50%] [G loss: 0.475544]\n",
      "epoch:11 step:11166 [D loss: 0.588366, acc.: 66.41%] [G loss: 0.555490]\n",
      "epoch:11 step:11167 [D loss: 0.524863, acc.: 76.56%] [G loss: 0.660891]\n",
      "epoch:11 step:11168 [D loss: 0.627487, acc.: 61.72%] [G loss: 0.538037]\n",
      "epoch:11 step:11169 [D loss: 0.580329, acc.: 71.88%] [G loss: 0.481124]\n",
      "epoch:11 step:11170 [D loss: 0.559438, acc.: 68.75%] [G loss: 0.404386]\n",
      "epoch:11 step:11171 [D loss: 0.529031, acc.: 71.09%] [G loss: 0.476659]\n",
      "epoch:11 step:11172 [D loss: 0.574447, acc.: 68.75%] [G loss: 0.534449]\n",
      "epoch:11 step:11173 [D loss: 0.552371, acc.: 67.19%] [G loss: 0.489587]\n",
      "epoch:11 step:11174 [D loss: 0.644210, acc.: 63.28%] [G loss: 0.408103]\n",
      "epoch:11 step:11175 [D loss: 0.485586, acc.: 76.56%] [G loss: 0.565434]\n",
      "epoch:11 step:11176 [D loss: 0.537867, acc.: 71.88%] [G loss: 0.573016]\n",
      "epoch:11 step:11177 [D loss: 0.457239, acc.: 77.34%] [G loss: 0.694111]\n",
      "epoch:11 step:11178 [D loss: 0.472413, acc.: 76.56%] [G loss: 0.661533]\n",
      "epoch:11 step:11179 [D loss: 0.602838, acc.: 64.84%] [G loss: 0.483524]\n",
      "epoch:11 step:11180 [D loss: 0.633078, acc.: 67.19%] [G loss: 0.439231]\n",
      "epoch:11 step:11181 [D loss: 0.601070, acc.: 66.41%] [G loss: 0.661123]\n",
      "epoch:11 step:11182 [D loss: 0.499449, acc.: 77.34%] [G loss: 0.577811]\n",
      "epoch:11 step:11183 [D loss: 0.558870, acc.: 71.88%] [G loss: 0.503967]\n",
      "epoch:11 step:11184 [D loss: 0.598180, acc.: 61.72%] [G loss: 0.479278]\n",
      "epoch:11 step:11185 [D loss: 0.544967, acc.: 73.44%] [G loss: 0.424338]\n",
      "epoch:11 step:11186 [D loss: 0.577739, acc.: 67.97%] [G loss: 0.439052]\n",
      "epoch:11 step:11187 [D loss: 0.659229, acc.: 57.81%] [G loss: 0.334164]\n",
      "epoch:11 step:11188 [D loss: 0.639093, acc.: 57.03%] [G loss: 0.357824]\n",
      "epoch:11 step:11189 [D loss: 0.563593, acc.: 64.06%] [G loss: 0.487053]\n",
      "epoch:11 step:11190 [D loss: 0.588808, acc.: 67.97%] [G loss: 0.458651]\n",
      "epoch:11 step:11191 [D loss: 0.509021, acc.: 76.56%] [G loss: 0.578203]\n",
      "epoch:11 step:11192 [D loss: 0.541757, acc.: 67.97%] [G loss: 0.743052]\n",
      "epoch:11 step:11193 [D loss: 0.517790, acc.: 71.88%] [G loss: 0.717328]\n",
      "epoch:11 step:11194 [D loss: 0.598936, acc.: 65.62%] [G loss: 0.657160]\n",
      "epoch:11 step:11195 [D loss: 0.556112, acc.: 67.97%] [G loss: 0.777619]\n",
      "epoch:11 step:11196 [D loss: 0.544373, acc.: 70.31%] [G loss: 0.618957]\n",
      "epoch:11 step:11197 [D loss: 0.525594, acc.: 74.22%] [G loss: 0.672720]\n",
      "epoch:11 step:11198 [D loss: 0.597625, acc.: 67.19%] [G loss: 0.535955]\n",
      "epoch:11 step:11199 [D loss: 0.627010, acc.: 61.72%] [G loss: 0.534411]\n",
      "epoch:11 step:11200 [D loss: 0.546551, acc.: 72.66%] [G loss: 0.504580]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.608162\n",
      "FID: 41.425274\n",
      "0 = 12.7007059905052\n",
      "1 = 0.07910759043390092\n",
      "2 = 0.8895000219345093\n",
      "3 = 0.8184000253677368\n",
      "4 = 0.9606000185012817\n",
      "5 = 0.9540685415267944\n",
      "6 = 0.8184000253677368\n",
      "7 = 7.834027035427126\n",
      "8 = 0.13523936279548426\n",
      "9 = 0.7371000051498413\n",
      "10 = 0.7193999886512756\n",
      "11 = 0.754800021648407\n",
      "12 = 0.7458013892173767\n",
      "13 = 0.7193999886512756\n",
      "14 = 6.608188629150391\n",
      "15 = 7.471287727355957\n",
      "16 = 0.34381991624832153\n",
      "17 = 6.608161926269531\n",
      "18 = 41.42527389526367\n",
      "epoch:11 step:11201 [D loss: 0.534719, acc.: 67.97%] [G loss: 0.549166]\n",
      "epoch:11 step:11202 [D loss: 0.545024, acc.: 71.88%] [G loss: 0.718353]\n",
      "epoch:11 step:11203 [D loss: 0.515259, acc.: 76.56%] [G loss: 0.762080]\n",
      "epoch:11 step:11204 [D loss: 0.505128, acc.: 76.56%] [G loss: 0.611383]\n",
      "epoch:11 step:11205 [D loss: 0.514335, acc.: 78.91%] [G loss: 0.526355]\n",
      "epoch:11 step:11206 [D loss: 0.526324, acc.: 75.00%] [G loss: 0.684731]\n",
      "epoch:11 step:11207 [D loss: 0.533108, acc.: 74.22%] [G loss: 0.630897]\n",
      "epoch:11 step:11208 [D loss: 0.547828, acc.: 67.97%] [G loss: 0.630521]\n",
      "epoch:11 step:11209 [D loss: 0.593555, acc.: 66.41%] [G loss: 0.646771]\n",
      "epoch:11 step:11210 [D loss: 0.548912, acc.: 70.31%] [G loss: 0.678013]\n",
      "epoch:11 step:11211 [D loss: 0.599704, acc.: 66.41%] [G loss: 0.492971]\n",
      "epoch:11 step:11212 [D loss: 0.568172, acc.: 67.19%] [G loss: 0.580193]\n",
      "epoch:11 step:11213 [D loss: 0.510295, acc.: 74.22%] [G loss: 0.691674]\n",
      "epoch:11 step:11214 [D loss: 0.582923, acc.: 65.62%] [G loss: 0.644423]\n",
      "epoch:11 step:11215 [D loss: 0.561116, acc.: 68.75%] [G loss: 0.550854]\n",
      "epoch:11 step:11216 [D loss: 0.562360, acc.: 66.41%] [G loss: 0.595356]\n",
      "epoch:11 step:11217 [D loss: 0.539559, acc.: 69.53%] [G loss: 0.525008]\n",
      "epoch:11 step:11218 [D loss: 0.538979, acc.: 75.00%] [G loss: 0.614807]\n",
      "epoch:11 step:11219 [D loss: 0.438630, acc.: 75.00%] [G loss: 0.755178]\n",
      "epoch:11 step:11220 [D loss: 0.529447, acc.: 74.22%] [G loss: 0.763151]\n",
      "epoch:11 step:11221 [D loss: 0.503715, acc.: 74.22%] [G loss: 0.802452]\n",
      "epoch:11 step:11222 [D loss: 0.653453, acc.: 62.50%] [G loss: 0.549855]\n",
      "epoch:11 step:11223 [D loss: 0.522491, acc.: 75.78%] [G loss: 0.675834]\n",
      "epoch:11 step:11224 [D loss: 0.682669, acc.: 53.91%] [G loss: 0.517044]\n",
      "epoch:11 step:11225 [D loss: 0.534525, acc.: 72.66%] [G loss: 0.582084]\n",
      "epoch:11 step:11226 [D loss: 0.441808, acc.: 82.81%] [G loss: 0.860848]\n",
      "epoch:11 step:11227 [D loss: 0.714387, acc.: 62.50%] [G loss: 0.659811]\n",
      "epoch:11 step:11228 [D loss: 0.506806, acc.: 75.78%] [G loss: 0.774450]\n",
      "epoch:11 step:11229 [D loss: 0.567109, acc.: 68.75%] [G loss: 0.608814]\n",
      "epoch:11 step:11230 [D loss: 0.469999, acc.: 77.34%] [G loss: 0.672342]\n",
      "epoch:11 step:11231 [D loss: 0.462396, acc.: 78.12%] [G loss: 0.758548]\n",
      "epoch:11 step:11232 [D loss: 0.403856, acc.: 84.38%] [G loss: 0.812424]\n",
      "epoch:11 step:11233 [D loss: 0.426040, acc.: 84.38%] [G loss: 1.001207]\n",
      "epoch:11 step:11234 [D loss: 0.557011, acc.: 71.88%] [G loss: 1.103052]\n",
      "epoch:11 step:11235 [D loss: 0.644765, acc.: 67.19%] [G loss: 1.028337]\n",
      "epoch:11 step:11236 [D loss: 0.506472, acc.: 70.31%] [G loss: 1.138684]\n",
      "epoch:11 step:11237 [D loss: 0.484865, acc.: 72.66%] [G loss: 1.080565]\n",
      "epoch:11 step:11238 [D loss: 0.644247, acc.: 64.84%] [G loss: 0.757932]\n",
      "epoch:11 step:11239 [D loss: 0.694542, acc.: 57.03%] [G loss: 0.522253]\n",
      "epoch:11 step:11240 [D loss: 0.598112, acc.: 67.19%] [G loss: 0.597135]\n",
      "epoch:11 step:11241 [D loss: 0.518771, acc.: 75.78%] [G loss: 0.642278]\n",
      "epoch:11 step:11242 [D loss: 0.516678, acc.: 75.78%] [G loss: 0.899315]\n",
      "epoch:11 step:11243 [D loss: 0.380559, acc.: 81.25%] [G loss: 1.263390]\n",
      "epoch:11 step:11244 [D loss: 0.339821, acc.: 92.19%] [G loss: 1.338224]\n",
      "epoch:12 step:11245 [D loss: 0.593101, acc.: 68.75%] [G loss: 1.118424]\n",
      "epoch:12 step:11246 [D loss: 0.526651, acc.: 70.31%] [G loss: 1.093237]\n",
      "epoch:12 step:11247 [D loss: 0.558257, acc.: 67.97%] [G loss: 0.872560]\n",
      "epoch:12 step:11248 [D loss: 0.530864, acc.: 72.66%] [G loss: 0.682977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11249 [D loss: 0.552889, acc.: 71.88%] [G loss: 0.689695]\n",
      "epoch:12 step:11250 [D loss: 0.540344, acc.: 73.44%] [G loss: 0.658871]\n",
      "epoch:12 step:11251 [D loss: 0.524749, acc.: 74.22%] [G loss: 0.748931]\n",
      "epoch:12 step:11252 [D loss: 0.522946, acc.: 74.22%] [G loss: 0.679257]\n",
      "epoch:12 step:11253 [D loss: 0.497324, acc.: 78.12%] [G loss: 0.760902]\n",
      "epoch:12 step:11254 [D loss: 0.558097, acc.: 74.22%] [G loss: 0.727119]\n",
      "epoch:12 step:11255 [D loss: 0.474484, acc.: 78.91%] [G loss: 0.624524]\n",
      "epoch:12 step:11256 [D loss: 0.605829, acc.: 69.53%] [G loss: 0.557977]\n",
      "epoch:12 step:11257 [D loss: 0.515570, acc.: 71.88%] [G loss: 0.644014]\n",
      "epoch:12 step:11258 [D loss: 0.556698, acc.: 65.62%] [G loss: 0.504898]\n",
      "epoch:12 step:11259 [D loss: 0.457712, acc.: 77.34%] [G loss: 0.606951]\n",
      "epoch:12 step:11260 [D loss: 0.525525, acc.: 69.53%] [G loss: 0.686948]\n",
      "epoch:12 step:11261 [D loss: 0.581774, acc.: 71.09%] [G loss: 0.592125]\n",
      "epoch:12 step:11262 [D loss: 0.600654, acc.: 65.62%] [G loss: 0.660146]\n",
      "epoch:12 step:11263 [D loss: 0.593530, acc.: 68.75%] [G loss: 0.642833]\n",
      "epoch:12 step:11264 [D loss: 0.597060, acc.: 63.28%] [G loss: 0.539797]\n",
      "epoch:12 step:11265 [D loss: 0.576804, acc.: 66.41%] [G loss: 0.685249]\n",
      "epoch:12 step:11266 [D loss: 0.469410, acc.: 79.69%] [G loss: 0.745021]\n",
      "epoch:12 step:11267 [D loss: 0.551265, acc.: 71.88%] [G loss: 0.556812]\n",
      "epoch:12 step:11268 [D loss: 0.549840, acc.: 69.53%] [G loss: 0.494550]\n",
      "epoch:12 step:11269 [D loss: 0.538781, acc.: 75.00%] [G loss: 0.705964]\n",
      "epoch:12 step:11270 [D loss: 0.564041, acc.: 67.19%] [G loss: 0.553065]\n",
      "epoch:12 step:11271 [D loss: 0.452730, acc.: 77.34%] [G loss: 0.603967]\n",
      "epoch:12 step:11272 [D loss: 0.602400, acc.: 60.94%] [G loss: 0.586926]\n",
      "epoch:12 step:11273 [D loss: 0.490089, acc.: 72.66%] [G loss: 0.585718]\n",
      "epoch:12 step:11274 [D loss: 0.539372, acc.: 71.09%] [G loss: 0.539343]\n",
      "epoch:12 step:11275 [D loss: 0.569836, acc.: 67.97%] [G loss: 0.466832]\n",
      "epoch:12 step:11276 [D loss: 0.544645, acc.: 74.22%] [G loss: 0.524057]\n",
      "epoch:12 step:11277 [D loss: 0.532932, acc.: 71.88%] [G loss: 0.551221]\n",
      "epoch:12 step:11278 [D loss: 0.504255, acc.: 74.22%] [G loss: 0.555302]\n",
      "epoch:12 step:11279 [D loss: 0.548411, acc.: 74.22%] [G loss: 0.544767]\n",
      "epoch:12 step:11280 [D loss: 0.502869, acc.: 78.12%] [G loss: 0.615023]\n",
      "epoch:12 step:11281 [D loss: 0.498823, acc.: 73.44%] [G loss: 0.824880]\n",
      "epoch:12 step:11282 [D loss: 0.613521, acc.: 64.06%] [G loss: 0.553228]\n",
      "epoch:12 step:11283 [D loss: 0.561273, acc.: 67.97%] [G loss: 0.572091]\n",
      "epoch:12 step:11284 [D loss: 0.500739, acc.: 81.25%] [G loss: 0.634308]\n",
      "epoch:12 step:11285 [D loss: 0.516761, acc.: 72.66%] [G loss: 0.653657]\n",
      "epoch:12 step:11286 [D loss: 0.536680, acc.: 73.44%] [G loss: 0.561232]\n",
      "epoch:12 step:11287 [D loss: 0.558340, acc.: 69.53%] [G loss: 0.543153]\n",
      "epoch:12 step:11288 [D loss: 0.579204, acc.: 69.53%] [G loss: 0.607815]\n",
      "epoch:12 step:11289 [D loss: 0.504733, acc.: 75.78%] [G loss: 0.566611]\n",
      "epoch:12 step:11290 [D loss: 0.607619, acc.: 64.06%] [G loss: 0.554116]\n",
      "epoch:12 step:11291 [D loss: 0.579760, acc.: 69.53%] [G loss: 0.605233]\n",
      "epoch:12 step:11292 [D loss: 0.540690, acc.: 66.41%] [G loss: 0.598210]\n",
      "epoch:12 step:11293 [D loss: 0.522103, acc.: 77.34%] [G loss: 0.605003]\n",
      "epoch:12 step:11294 [D loss: 0.511927, acc.: 78.12%] [G loss: 0.584193]\n",
      "epoch:12 step:11295 [D loss: 0.664526, acc.: 59.38%] [G loss: 0.427055]\n",
      "epoch:12 step:11296 [D loss: 0.601897, acc.: 66.41%] [G loss: 0.462167]\n",
      "epoch:12 step:11297 [D loss: 0.541815, acc.: 75.00%] [G loss: 0.532114]\n",
      "epoch:12 step:11298 [D loss: 0.532842, acc.: 70.31%] [G loss: 0.588828]\n",
      "epoch:12 step:11299 [D loss: 0.519565, acc.: 72.66%] [G loss: 0.629901]\n",
      "epoch:12 step:11300 [D loss: 0.494859, acc.: 76.56%] [G loss: 0.676542]\n",
      "epoch:12 step:11301 [D loss: 0.543451, acc.: 69.53%] [G loss: 0.569121]\n",
      "epoch:12 step:11302 [D loss: 0.582185, acc.: 64.84%] [G loss: 0.597610]\n",
      "epoch:12 step:11303 [D loss: 0.514050, acc.: 75.78%] [G loss: 0.530652]\n",
      "epoch:12 step:11304 [D loss: 0.581321, acc.: 63.28%] [G loss: 0.639503]\n",
      "epoch:12 step:11305 [D loss: 0.576399, acc.: 66.41%] [G loss: 0.516411]\n",
      "epoch:12 step:11306 [D loss: 0.574133, acc.: 65.62%] [G loss: 0.516435]\n",
      "epoch:12 step:11307 [D loss: 0.554607, acc.: 67.97%] [G loss: 0.506312]\n",
      "epoch:12 step:11308 [D loss: 0.541647, acc.: 76.56%] [G loss: 0.503767]\n",
      "epoch:12 step:11309 [D loss: 0.529017, acc.: 71.88%] [G loss: 0.523263]\n",
      "epoch:12 step:11310 [D loss: 0.518500, acc.: 75.78%] [G loss: 0.607685]\n",
      "epoch:12 step:11311 [D loss: 0.519384, acc.: 67.97%] [G loss: 0.543230]\n",
      "epoch:12 step:11312 [D loss: 0.520888, acc.: 68.75%] [G loss: 0.622346]\n",
      "epoch:12 step:11313 [D loss: 0.517018, acc.: 74.22%] [G loss: 0.561699]\n",
      "epoch:12 step:11314 [D loss: 0.527141, acc.: 75.00%] [G loss: 0.535093]\n",
      "epoch:12 step:11315 [D loss: 0.567118, acc.: 68.75%] [G loss: 0.534795]\n",
      "epoch:12 step:11316 [D loss: 0.500025, acc.: 75.00%] [G loss: 0.593028]\n",
      "epoch:12 step:11317 [D loss: 0.571757, acc.: 69.53%] [G loss: 0.491943]\n",
      "epoch:12 step:11318 [D loss: 0.499258, acc.: 76.56%] [G loss: 0.449697]\n",
      "epoch:12 step:11319 [D loss: 0.530492, acc.: 72.66%] [G loss: 0.525614]\n",
      "epoch:12 step:11320 [D loss: 0.539448, acc.: 71.09%] [G loss: 0.819615]\n",
      "epoch:12 step:11321 [D loss: 0.414392, acc.: 79.69%] [G loss: 0.722236]\n",
      "epoch:12 step:11322 [D loss: 0.610109, acc.: 67.19%] [G loss: 0.482834]\n",
      "epoch:12 step:11323 [D loss: 0.559149, acc.: 67.97%] [G loss: 0.513056]\n",
      "epoch:12 step:11324 [D loss: 0.511656, acc.: 73.44%] [G loss: 0.476346]\n",
      "epoch:12 step:11325 [D loss: 0.573896, acc.: 67.97%] [G loss: 0.610625]\n",
      "epoch:12 step:11326 [D loss: 0.558340, acc.: 71.09%] [G loss: 0.555690]\n",
      "epoch:12 step:11327 [D loss: 0.569890, acc.: 71.88%] [G loss: 0.601958]\n",
      "epoch:12 step:11328 [D loss: 0.535237, acc.: 69.53%] [G loss: 0.631562]\n",
      "epoch:12 step:11329 [D loss: 0.558915, acc.: 70.31%] [G loss: 0.666986]\n",
      "epoch:12 step:11330 [D loss: 0.587625, acc.: 69.53%] [G loss: 0.551532]\n",
      "epoch:12 step:11331 [D loss: 0.533969, acc.: 70.31%] [G loss: 0.557834]\n",
      "epoch:12 step:11332 [D loss: 0.504225, acc.: 75.00%] [G loss: 0.578116]\n",
      "epoch:12 step:11333 [D loss: 0.552254, acc.: 72.66%] [G loss: 0.628331]\n",
      "epoch:12 step:11334 [D loss: 0.516360, acc.: 78.12%] [G loss: 0.609138]\n",
      "epoch:12 step:11335 [D loss: 0.569220, acc.: 71.88%] [G loss: 0.516820]\n",
      "epoch:12 step:11336 [D loss: 0.463198, acc.: 79.69%] [G loss: 0.540668]\n",
      "epoch:12 step:11337 [D loss: 0.522126, acc.: 75.00%] [G loss: 0.685688]\n",
      "epoch:12 step:11338 [D loss: 0.538773, acc.: 68.75%] [G loss: 0.610637]\n",
      "epoch:12 step:11339 [D loss: 0.554030, acc.: 72.66%] [G loss: 0.558238]\n",
      "epoch:12 step:11340 [D loss: 0.478601, acc.: 77.34%] [G loss: 0.745807]\n",
      "epoch:12 step:11341 [D loss: 0.513498, acc.: 75.00%] [G loss: 0.805063]\n",
      "epoch:12 step:11342 [D loss: 0.559981, acc.: 70.31%] [G loss: 0.749904]\n",
      "epoch:12 step:11343 [D loss: 0.566080, acc.: 70.31%] [G loss: 0.735950]\n",
      "epoch:12 step:11344 [D loss: 0.522980, acc.: 71.09%] [G loss: 0.732657]\n",
      "epoch:12 step:11345 [D loss: 0.539019, acc.: 72.66%] [G loss: 0.740984]\n",
      "epoch:12 step:11346 [D loss: 0.655519, acc.: 63.28%] [G loss: 0.473155]\n",
      "epoch:12 step:11347 [D loss: 0.542727, acc.: 66.41%] [G loss: 0.522907]\n",
      "epoch:12 step:11348 [D loss: 0.501975, acc.: 71.88%] [G loss: 0.718917]\n",
      "epoch:12 step:11349 [D loss: 0.596755, acc.: 68.75%] [G loss: 0.519309]\n",
      "epoch:12 step:11350 [D loss: 0.520663, acc.: 75.78%] [G loss: 0.636256]\n",
      "epoch:12 step:11351 [D loss: 0.543137, acc.: 75.00%] [G loss: 0.525969]\n",
      "epoch:12 step:11352 [D loss: 0.652701, acc.: 60.16%] [G loss: 0.422145]\n",
      "epoch:12 step:11353 [D loss: 0.567925, acc.: 70.31%] [G loss: 0.507321]\n",
      "epoch:12 step:11354 [D loss: 0.616366, acc.: 64.84%] [G loss: 0.517847]\n",
      "epoch:12 step:11355 [D loss: 0.495027, acc.: 74.22%] [G loss: 0.597739]\n",
      "epoch:12 step:11356 [D loss: 0.552077, acc.: 70.31%] [G loss: 0.499073]\n",
      "epoch:12 step:11357 [D loss: 0.532196, acc.: 70.31%] [G loss: 0.539520]\n",
      "epoch:12 step:11358 [D loss: 0.561544, acc.: 75.78%] [G loss: 0.408074]\n",
      "epoch:12 step:11359 [D loss: 0.568730, acc.: 72.66%] [G loss: 0.640586]\n",
      "epoch:12 step:11360 [D loss: 0.538773, acc.: 71.88%] [G loss: 0.639580]\n",
      "epoch:12 step:11361 [D loss: 0.500887, acc.: 77.34%] [G loss: 0.823362]\n",
      "epoch:12 step:11362 [D loss: 0.553907, acc.: 68.75%] [G loss: 0.686775]\n",
      "epoch:12 step:11363 [D loss: 0.455090, acc.: 80.47%] [G loss: 0.785396]\n",
      "epoch:12 step:11364 [D loss: 0.558641, acc.: 75.00%] [G loss: 0.605589]\n",
      "epoch:12 step:11365 [D loss: 0.536821, acc.: 71.88%] [G loss: 0.747962]\n",
      "epoch:12 step:11366 [D loss: 0.501957, acc.: 78.91%] [G loss: 0.805005]\n",
      "epoch:12 step:11367 [D loss: 0.558715, acc.: 72.66%] [G loss: 0.634875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11368 [D loss: 0.592973, acc.: 68.75%] [G loss: 0.563355]\n",
      "epoch:12 step:11369 [D loss: 0.606551, acc.: 70.31%] [G loss: 0.497079]\n",
      "epoch:12 step:11370 [D loss: 0.514580, acc.: 75.78%] [G loss: 0.601619]\n",
      "epoch:12 step:11371 [D loss: 0.530863, acc.: 71.88%] [G loss: 0.490473]\n",
      "epoch:12 step:11372 [D loss: 0.499721, acc.: 75.78%] [G loss: 0.561983]\n",
      "epoch:12 step:11373 [D loss: 0.582420, acc.: 68.75%] [G loss: 0.406168]\n",
      "epoch:12 step:11374 [D loss: 0.510736, acc.: 75.00%] [G loss: 0.475279]\n",
      "epoch:12 step:11375 [D loss: 0.455321, acc.: 79.69%] [G loss: 0.672670]\n",
      "epoch:12 step:11376 [D loss: 0.552010, acc.: 70.31%] [G loss: 0.667842]\n",
      "epoch:12 step:11377 [D loss: 0.586038, acc.: 69.53%] [G loss: 0.606298]\n",
      "epoch:12 step:11378 [D loss: 0.493015, acc.: 75.78%] [G loss: 0.736054]\n",
      "epoch:12 step:11379 [D loss: 0.550053, acc.: 75.78%] [G loss: 0.694147]\n",
      "epoch:12 step:11380 [D loss: 0.522474, acc.: 71.88%] [G loss: 0.750003]\n",
      "epoch:12 step:11381 [D loss: 0.579156, acc.: 71.09%] [G loss: 0.591317]\n",
      "epoch:12 step:11382 [D loss: 0.662302, acc.: 62.50%] [G loss: 0.462948]\n",
      "epoch:12 step:11383 [D loss: 0.649138, acc.: 64.06%] [G loss: 0.590863]\n",
      "epoch:12 step:11384 [D loss: 0.560293, acc.: 67.97%] [G loss: 0.449556]\n",
      "epoch:12 step:11385 [D loss: 0.573225, acc.: 65.62%] [G loss: 0.471486]\n",
      "epoch:12 step:11386 [D loss: 0.596788, acc.: 61.72%] [G loss: 0.513183]\n",
      "epoch:12 step:11387 [D loss: 0.638341, acc.: 67.19%] [G loss: 0.510350]\n",
      "epoch:12 step:11388 [D loss: 0.541513, acc.: 69.53%] [G loss: 0.503317]\n",
      "epoch:12 step:11389 [D loss: 0.569070, acc.: 67.19%] [G loss: 0.529891]\n",
      "epoch:12 step:11390 [D loss: 0.499549, acc.: 75.00%] [G loss: 0.703153]\n",
      "epoch:12 step:11391 [D loss: 0.634230, acc.: 67.97%] [G loss: 0.530017]\n",
      "epoch:12 step:11392 [D loss: 0.533508, acc.: 72.66%] [G loss: 0.637571]\n",
      "epoch:12 step:11393 [D loss: 0.571783, acc.: 70.31%] [G loss: 0.647125]\n",
      "epoch:12 step:11394 [D loss: 0.575551, acc.: 71.88%] [G loss: 0.532960]\n",
      "epoch:12 step:11395 [D loss: 0.612434, acc.: 60.16%] [G loss: 0.607958]\n",
      "epoch:12 step:11396 [D loss: 0.545107, acc.: 68.75%] [G loss: 0.659195]\n",
      "epoch:12 step:11397 [D loss: 0.612328, acc.: 64.06%] [G loss: 0.685619]\n",
      "epoch:12 step:11398 [D loss: 0.547471, acc.: 71.09%] [G loss: 0.585249]\n",
      "epoch:12 step:11399 [D loss: 0.542807, acc.: 71.88%] [G loss: 0.614269]\n",
      "epoch:12 step:11400 [D loss: 0.509555, acc.: 77.34%] [G loss: 0.661027]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.555603\n",
      "FID: 40.901867\n",
      "0 = 13.019533737182627\n",
      "1 = 0.09128639644962339\n",
      "2 = 0.9036999940872192\n",
      "3 = 0.853600025177002\n",
      "4 = 0.9538000226020813\n",
      "5 = 0.9486552476882935\n",
      "6 = 0.853600025177002\n",
      "7 = 7.858089895677541\n",
      "8 = 0.13088078110249615\n",
      "9 = 0.7322999835014343\n",
      "10 = 0.7174000144004822\n",
      "11 = 0.7472000122070312\n",
      "12 = 0.7394351959228516\n",
      "13 = 0.7174000144004822\n",
      "14 = 6.555628776550293\n",
      "15 = 7.170332908630371\n",
      "16 = 0.3685917556285858\n",
      "17 = 6.55560302734375\n",
      "18 = 40.9018669128418\n",
      "epoch:12 step:11401 [D loss: 0.599078, acc.: 60.16%] [G loss: 0.547998]\n",
      "epoch:12 step:11402 [D loss: 0.570615, acc.: 70.31%] [G loss: 0.603848]\n",
      "epoch:12 step:11403 [D loss: 0.528477, acc.: 75.78%] [G loss: 0.642389]\n",
      "epoch:12 step:11404 [D loss: 0.640996, acc.: 61.72%] [G loss: 0.532967]\n",
      "epoch:12 step:11405 [D loss: 0.542092, acc.: 69.53%] [G loss: 0.591552]\n",
      "epoch:12 step:11406 [D loss: 0.507397, acc.: 71.88%] [G loss: 0.690072]\n",
      "epoch:12 step:11407 [D loss: 0.561766, acc.: 71.09%] [G loss: 0.638607]\n",
      "epoch:12 step:11408 [D loss: 0.558585, acc.: 70.31%] [G loss: 0.536113]\n",
      "epoch:12 step:11409 [D loss: 0.510286, acc.: 72.66%] [G loss: 0.540215]\n",
      "epoch:12 step:11410 [D loss: 0.543090, acc.: 71.09%] [G loss: 0.494065]\n",
      "epoch:12 step:11411 [D loss: 0.547530, acc.: 67.97%] [G loss: 0.412706]\n",
      "epoch:12 step:11412 [D loss: 0.555717, acc.: 68.75%] [G loss: 0.553380]\n",
      "epoch:12 step:11413 [D loss: 0.578620, acc.: 67.97%] [G loss: 0.524343]\n",
      "epoch:12 step:11414 [D loss: 0.534683, acc.: 67.97%] [G loss: 0.569229]\n",
      "epoch:12 step:11415 [D loss: 0.533345, acc.: 68.75%] [G loss: 0.462871]\n",
      "epoch:12 step:11416 [D loss: 0.547276, acc.: 74.22%] [G loss: 0.475107]\n",
      "epoch:12 step:11417 [D loss: 0.501370, acc.: 73.44%] [G loss: 0.673247]\n",
      "epoch:12 step:11418 [D loss: 0.590638, acc.: 64.84%] [G loss: 0.525800]\n",
      "epoch:12 step:11419 [D loss: 0.571022, acc.: 64.06%] [G loss: 0.435615]\n",
      "epoch:12 step:11420 [D loss: 0.550913, acc.: 70.31%] [G loss: 0.577549]\n",
      "epoch:12 step:11421 [D loss: 0.594380, acc.: 66.41%] [G loss: 0.444738]\n",
      "epoch:12 step:11422 [D loss: 0.590572, acc.: 68.75%] [G loss: 0.472130]\n",
      "epoch:12 step:11423 [D loss: 0.577690, acc.: 66.41%] [G loss: 0.523901]\n",
      "epoch:12 step:11424 [D loss: 0.653230, acc.: 60.94%] [G loss: 0.408136]\n",
      "epoch:12 step:11425 [D loss: 0.584119, acc.: 69.53%] [G loss: 0.442092]\n",
      "epoch:12 step:11426 [D loss: 0.579500, acc.: 70.31%] [G loss: 0.742904]\n",
      "epoch:12 step:11427 [D loss: 0.600877, acc.: 67.97%] [G loss: 0.607062]\n",
      "epoch:12 step:11428 [D loss: 0.486777, acc.: 76.56%] [G loss: 0.525551]\n",
      "epoch:12 step:11429 [D loss: 0.556785, acc.: 71.09%] [G loss: 0.605006]\n",
      "epoch:12 step:11430 [D loss: 0.546699, acc.: 67.97%] [G loss: 0.673880]\n",
      "epoch:12 step:11431 [D loss: 0.620726, acc.: 63.28%] [G loss: 0.566211]\n",
      "epoch:12 step:11432 [D loss: 0.476378, acc.: 76.56%] [G loss: 0.563756]\n",
      "epoch:12 step:11433 [D loss: 0.592409, acc.: 68.75%] [G loss: 0.557791]\n",
      "epoch:12 step:11434 [D loss: 0.507811, acc.: 72.66%] [G loss: 0.607469]\n",
      "epoch:12 step:11435 [D loss: 0.465936, acc.: 76.56%] [G loss: 0.563726]\n",
      "epoch:12 step:11436 [D loss: 0.482543, acc.: 78.12%] [G loss: 0.614559]\n",
      "epoch:12 step:11437 [D loss: 0.570111, acc.: 64.84%] [G loss: 0.681055]\n",
      "epoch:12 step:11438 [D loss: 0.445455, acc.: 78.12%] [G loss: 0.706603]\n",
      "epoch:12 step:11439 [D loss: 0.574644, acc.: 70.31%] [G loss: 0.604896]\n",
      "epoch:12 step:11440 [D loss: 0.630107, acc.: 64.84%] [G loss: 0.616075]\n",
      "epoch:12 step:11441 [D loss: 0.552617, acc.: 71.09%] [G loss: 0.535585]\n",
      "epoch:12 step:11442 [D loss: 0.516625, acc.: 78.12%] [G loss: 0.568215]\n",
      "epoch:12 step:11443 [D loss: 0.531574, acc.: 72.66%] [G loss: 0.591827]\n",
      "epoch:12 step:11444 [D loss: 0.591558, acc.: 63.28%] [G loss: 0.627940]\n",
      "epoch:12 step:11445 [D loss: 0.574164, acc.: 68.75%] [G loss: 0.590920]\n",
      "epoch:12 step:11446 [D loss: 0.530506, acc.: 71.88%] [G loss: 0.577834]\n",
      "epoch:12 step:11447 [D loss: 0.598496, acc.: 67.97%] [G loss: 0.472681]\n",
      "epoch:12 step:11448 [D loss: 0.560130, acc.: 69.53%] [G loss: 0.491664]\n",
      "epoch:12 step:11449 [D loss: 0.484681, acc.: 76.56%] [G loss: 0.780985]\n",
      "epoch:12 step:11450 [D loss: 0.501175, acc.: 78.12%] [G loss: 0.823320]\n",
      "epoch:12 step:11451 [D loss: 0.496799, acc.: 75.00%] [G loss: 0.669710]\n",
      "epoch:12 step:11452 [D loss: 0.433935, acc.: 81.25%] [G loss: 0.703079]\n",
      "epoch:12 step:11453 [D loss: 0.506366, acc.: 71.09%] [G loss: 0.794873]\n",
      "epoch:12 step:11454 [D loss: 0.672701, acc.: 60.94%] [G loss: 0.537523]\n",
      "epoch:12 step:11455 [D loss: 0.616627, acc.: 63.28%] [G loss: 0.630252]\n",
      "epoch:12 step:11456 [D loss: 0.568458, acc.: 75.00%] [G loss: 0.534383]\n",
      "epoch:12 step:11457 [D loss: 0.512131, acc.: 73.44%] [G loss: 0.509879]\n",
      "epoch:12 step:11458 [D loss: 0.649517, acc.: 63.28%] [G loss: 0.548332]\n",
      "epoch:12 step:11459 [D loss: 0.653987, acc.: 59.38%] [G loss: 0.537873]\n",
      "epoch:12 step:11460 [D loss: 0.496457, acc.: 75.78%] [G loss: 0.451124]\n",
      "epoch:12 step:11461 [D loss: 0.585773, acc.: 66.41%] [G loss: 0.450808]\n",
      "epoch:12 step:11462 [D loss: 0.551411, acc.: 71.09%] [G loss: 0.567000]\n",
      "epoch:12 step:11463 [D loss: 0.543988, acc.: 73.44%] [G loss: 0.634643]\n",
      "epoch:12 step:11464 [D loss: 0.713439, acc.: 60.16%] [G loss: 0.578162]\n",
      "epoch:12 step:11465 [D loss: 0.551414, acc.: 68.75%] [G loss: 0.659988]\n",
      "epoch:12 step:11466 [D loss: 0.462533, acc.: 80.47%] [G loss: 0.608302]\n",
      "epoch:12 step:11467 [D loss: 0.584712, acc.: 64.84%] [G loss: 0.665828]\n",
      "epoch:12 step:11468 [D loss: 0.590193, acc.: 71.88%] [G loss: 0.531234]\n",
      "epoch:12 step:11469 [D loss: 0.594580, acc.: 67.97%] [G loss: 0.621692]\n",
      "epoch:12 step:11470 [D loss: 0.609384, acc.: 62.50%] [G loss: 0.519593]\n",
      "epoch:12 step:11471 [D loss: 0.538620, acc.: 75.78%] [G loss: 0.495910]\n",
      "epoch:12 step:11472 [D loss: 0.578177, acc.: 66.41%] [G loss: 0.445631]\n",
      "epoch:12 step:11473 [D loss: 0.556171, acc.: 77.34%] [G loss: 0.541012]\n",
      "epoch:12 step:11474 [D loss: 0.472674, acc.: 79.69%] [G loss: 0.505820]\n",
      "epoch:12 step:11475 [D loss: 0.532341, acc.: 71.88%] [G loss: 0.682162]\n",
      "epoch:12 step:11476 [D loss: 0.515473, acc.: 73.44%] [G loss: 0.771417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11477 [D loss: 0.568989, acc.: 66.41%] [G loss: 0.693074]\n",
      "epoch:12 step:11478 [D loss: 0.523156, acc.: 67.97%] [G loss: 0.809383]\n",
      "epoch:12 step:11479 [D loss: 0.507847, acc.: 70.31%] [G loss: 0.612800]\n",
      "epoch:12 step:11480 [D loss: 0.579228, acc.: 65.62%] [G loss: 0.416596]\n",
      "epoch:12 step:11481 [D loss: 0.497580, acc.: 73.44%] [G loss: 0.537425]\n",
      "epoch:12 step:11482 [D loss: 0.563817, acc.: 66.41%] [G loss: 0.530558]\n",
      "epoch:12 step:11483 [D loss: 0.520760, acc.: 71.09%] [G loss: 0.624797]\n",
      "epoch:12 step:11484 [D loss: 0.572592, acc.: 68.75%] [G loss: 0.518399]\n",
      "epoch:12 step:11485 [D loss: 0.563865, acc.: 69.53%] [G loss: 0.660139]\n",
      "epoch:12 step:11486 [D loss: 0.523489, acc.: 74.22%] [G loss: 0.605628]\n",
      "epoch:12 step:11487 [D loss: 0.570842, acc.: 68.75%] [G loss: 0.638821]\n",
      "epoch:12 step:11488 [D loss: 0.508281, acc.: 69.53%] [G loss: 0.612483]\n",
      "epoch:12 step:11489 [D loss: 0.531727, acc.: 76.56%] [G loss: 0.594109]\n",
      "epoch:12 step:11490 [D loss: 0.559221, acc.: 70.31%] [G loss: 0.700778]\n",
      "epoch:12 step:11491 [D loss: 0.551484, acc.: 68.75%] [G loss: 0.628232]\n",
      "epoch:12 step:11492 [D loss: 0.505577, acc.: 74.22%] [G loss: 0.650614]\n",
      "epoch:12 step:11493 [D loss: 0.563121, acc.: 71.09%] [G loss: 0.676561]\n",
      "epoch:12 step:11494 [D loss: 0.602696, acc.: 62.50%] [G loss: 0.474553]\n",
      "epoch:12 step:11495 [D loss: 0.613146, acc.: 65.62%] [G loss: 0.508023]\n",
      "epoch:12 step:11496 [D loss: 0.552762, acc.: 71.88%] [G loss: 0.648444]\n",
      "epoch:12 step:11497 [D loss: 0.566869, acc.: 71.88%] [G loss: 0.514540]\n",
      "epoch:12 step:11498 [D loss: 0.503825, acc.: 79.69%] [G loss: 0.586747]\n",
      "epoch:12 step:11499 [D loss: 0.588938, acc.: 64.84%] [G loss: 0.536846]\n",
      "epoch:12 step:11500 [D loss: 0.556812, acc.: 66.41%] [G loss: 0.500679]\n",
      "epoch:12 step:11501 [D loss: 0.599749, acc.: 63.28%] [G loss: 0.595404]\n",
      "epoch:12 step:11502 [D loss: 0.544087, acc.: 70.31%] [G loss: 0.520109]\n",
      "epoch:12 step:11503 [D loss: 0.557315, acc.: 66.41%] [G loss: 0.485102]\n",
      "epoch:12 step:11504 [D loss: 0.573433, acc.: 67.19%] [G loss: 0.488291]\n",
      "epoch:12 step:11505 [D loss: 0.552916, acc.: 65.62%] [G loss: 0.531540]\n",
      "epoch:12 step:11506 [D loss: 0.505734, acc.: 71.09%] [G loss: 0.571686]\n",
      "epoch:12 step:11507 [D loss: 0.609866, acc.: 68.75%] [G loss: 0.509595]\n",
      "epoch:12 step:11508 [D loss: 0.536862, acc.: 71.09%] [G loss: 0.662165]\n",
      "epoch:12 step:11509 [D loss: 0.526393, acc.: 75.00%] [G loss: 0.575381]\n",
      "epoch:12 step:11510 [D loss: 0.535129, acc.: 74.22%] [G loss: 0.540898]\n",
      "epoch:12 step:11511 [D loss: 0.585684, acc.: 64.84%] [G loss: 0.605699]\n",
      "epoch:12 step:11512 [D loss: 0.561049, acc.: 67.19%] [G loss: 0.534381]\n",
      "epoch:12 step:11513 [D loss: 0.491601, acc.: 75.78%] [G loss: 0.614874]\n",
      "epoch:12 step:11514 [D loss: 0.491842, acc.: 71.88%] [G loss: 0.674232]\n",
      "epoch:12 step:11515 [D loss: 0.563580, acc.: 66.41%] [G loss: 0.480893]\n",
      "epoch:12 step:11516 [D loss: 0.505474, acc.: 72.66%] [G loss: 0.725093]\n",
      "epoch:12 step:11517 [D loss: 0.494636, acc.: 75.00%] [G loss: 0.648646]\n",
      "epoch:12 step:11518 [D loss: 0.564383, acc.: 75.00%] [G loss: 0.656634]\n",
      "epoch:12 step:11519 [D loss: 0.601555, acc.: 62.50%] [G loss: 0.550440]\n",
      "epoch:12 step:11520 [D loss: 0.475247, acc.: 78.91%] [G loss: 0.570369]\n",
      "epoch:12 step:11521 [D loss: 0.641998, acc.: 63.28%] [G loss: 0.402536]\n",
      "epoch:12 step:11522 [D loss: 0.611551, acc.: 64.06%] [G loss: 0.499160]\n",
      "epoch:12 step:11523 [D loss: 0.598294, acc.: 60.16%] [G loss: 0.403341]\n",
      "epoch:12 step:11524 [D loss: 0.550800, acc.: 65.62%] [G loss: 0.598083]\n",
      "epoch:12 step:11525 [D loss: 0.656002, acc.: 60.16%] [G loss: 0.460252]\n",
      "epoch:12 step:11526 [D loss: 0.555946, acc.: 73.44%] [G loss: 0.578389]\n",
      "epoch:12 step:11527 [D loss: 0.510525, acc.: 76.56%] [G loss: 0.543402]\n",
      "epoch:12 step:11528 [D loss: 0.500393, acc.: 78.12%] [G loss: 0.503612]\n",
      "epoch:12 step:11529 [D loss: 0.534960, acc.: 72.66%] [G loss: 0.501213]\n",
      "epoch:12 step:11530 [D loss: 0.499771, acc.: 81.25%] [G loss: 0.625880]\n",
      "epoch:12 step:11531 [D loss: 0.547049, acc.: 71.09%] [G loss: 0.666903]\n",
      "epoch:12 step:11532 [D loss: 0.584790, acc.: 61.72%] [G loss: 0.570499]\n",
      "epoch:12 step:11533 [D loss: 0.538248, acc.: 71.88%] [G loss: 0.531692]\n",
      "epoch:12 step:11534 [D loss: 0.567115, acc.: 69.53%] [G loss: 0.507213]\n",
      "epoch:12 step:11535 [D loss: 0.599237, acc.: 67.97%] [G loss: 0.466801]\n",
      "epoch:12 step:11536 [D loss: 0.544832, acc.: 70.31%] [G loss: 0.433176]\n",
      "epoch:12 step:11537 [D loss: 0.566959, acc.: 69.53%] [G loss: 0.495163]\n",
      "epoch:12 step:11538 [D loss: 0.583020, acc.: 67.19%] [G loss: 0.518195]\n",
      "epoch:12 step:11539 [D loss: 0.536223, acc.: 72.66%] [G loss: 0.505445]\n",
      "epoch:12 step:11540 [D loss: 0.530287, acc.: 75.00%] [G loss: 0.525350]\n",
      "epoch:12 step:11541 [D loss: 0.581733, acc.: 70.31%] [G loss: 0.579994]\n",
      "epoch:12 step:11542 [D loss: 0.519547, acc.: 71.88%] [G loss: 0.667902]\n",
      "epoch:12 step:11543 [D loss: 0.535500, acc.: 72.66%] [G loss: 0.578158]\n",
      "epoch:12 step:11544 [D loss: 0.479511, acc.: 75.78%] [G loss: 0.755615]\n",
      "epoch:12 step:11545 [D loss: 0.624857, acc.: 67.19%] [G loss: 0.571093]\n",
      "epoch:12 step:11546 [D loss: 0.551688, acc.: 72.66%] [G loss: 0.577603]\n",
      "epoch:12 step:11547 [D loss: 0.571543, acc.: 71.88%] [G loss: 0.543173]\n",
      "epoch:12 step:11548 [D loss: 0.450229, acc.: 79.69%] [G loss: 0.570911]\n",
      "epoch:12 step:11549 [D loss: 0.550989, acc.: 70.31%] [G loss: 0.636599]\n",
      "epoch:12 step:11550 [D loss: 0.566093, acc.: 65.62%] [G loss: 0.585769]\n",
      "epoch:12 step:11551 [D loss: 0.487935, acc.: 73.44%] [G loss: 0.676872]\n",
      "epoch:12 step:11552 [D loss: 0.578892, acc.: 68.75%] [G loss: 0.577404]\n",
      "epoch:12 step:11553 [D loss: 0.505477, acc.: 72.66%] [G loss: 0.648053]\n",
      "epoch:12 step:11554 [D loss: 0.500111, acc.: 73.44%] [G loss: 0.711182]\n",
      "epoch:12 step:11555 [D loss: 0.517244, acc.: 72.66%] [G loss: 0.623328]\n",
      "epoch:12 step:11556 [D loss: 0.454740, acc.: 82.03%] [G loss: 0.828013]\n",
      "epoch:12 step:11557 [D loss: 0.543510, acc.: 71.88%] [G loss: 0.899654]\n",
      "epoch:12 step:11558 [D loss: 0.450467, acc.: 77.34%] [G loss: 0.914714]\n",
      "epoch:12 step:11559 [D loss: 0.457354, acc.: 80.47%] [G loss: 1.060386]\n",
      "epoch:12 step:11560 [D loss: 0.670432, acc.: 61.72%] [G loss: 0.653864]\n",
      "epoch:12 step:11561 [D loss: 0.625444, acc.: 65.62%] [G loss: 0.531605]\n",
      "epoch:12 step:11562 [D loss: 0.528940, acc.: 72.66%] [G loss: 0.529321]\n",
      "epoch:12 step:11563 [D loss: 0.556830, acc.: 71.09%] [G loss: 0.532330]\n",
      "epoch:12 step:11564 [D loss: 0.523289, acc.: 74.22%] [G loss: 0.578517]\n",
      "epoch:12 step:11565 [D loss: 0.494233, acc.: 76.56%] [G loss: 0.779754]\n",
      "epoch:12 step:11566 [D loss: 0.580121, acc.: 69.53%] [G loss: 0.636182]\n",
      "epoch:12 step:11567 [D loss: 0.640351, acc.: 60.16%] [G loss: 0.608037]\n",
      "epoch:12 step:11568 [D loss: 0.586371, acc.: 67.97%] [G loss: 0.580411]\n",
      "epoch:12 step:11569 [D loss: 0.598116, acc.: 70.31%] [G loss: 0.527659]\n",
      "epoch:12 step:11570 [D loss: 0.505610, acc.: 75.00%] [G loss: 0.661069]\n",
      "epoch:12 step:11571 [D loss: 0.511277, acc.: 78.12%] [G loss: 0.520377]\n",
      "epoch:12 step:11572 [D loss: 0.500522, acc.: 75.78%] [G loss: 0.677860]\n",
      "epoch:12 step:11573 [D loss: 0.568050, acc.: 67.97%] [G loss: 0.712300]\n",
      "epoch:12 step:11574 [D loss: 0.570231, acc.: 70.31%] [G loss: 0.502331]\n",
      "epoch:12 step:11575 [D loss: 0.545100, acc.: 70.31%] [G loss: 0.542396]\n",
      "epoch:12 step:11576 [D loss: 0.521467, acc.: 72.66%] [G loss: 0.511405]\n",
      "epoch:12 step:11577 [D loss: 0.489016, acc.: 75.00%] [G loss: 0.594340]\n",
      "epoch:12 step:11578 [D loss: 0.523328, acc.: 71.09%] [G loss: 0.582341]\n",
      "epoch:12 step:11579 [D loss: 0.520795, acc.: 73.44%] [G loss: 0.683460]\n",
      "epoch:12 step:11580 [D loss: 0.484145, acc.: 74.22%] [G loss: 0.809810]\n",
      "epoch:12 step:11581 [D loss: 0.535129, acc.: 75.00%] [G loss: 0.773190]\n",
      "epoch:12 step:11582 [D loss: 0.552510, acc.: 68.75%] [G loss: 0.548552]\n",
      "epoch:12 step:11583 [D loss: 0.538097, acc.: 75.78%] [G loss: 0.788163]\n",
      "epoch:12 step:11584 [D loss: 0.514024, acc.: 75.78%] [G loss: 0.612236]\n",
      "epoch:12 step:11585 [D loss: 0.650611, acc.: 63.28%] [G loss: 0.514916]\n",
      "epoch:12 step:11586 [D loss: 0.643354, acc.: 60.94%] [G loss: 0.442107]\n",
      "epoch:12 step:11587 [D loss: 0.497829, acc.: 76.56%] [G loss: 0.607944]\n",
      "epoch:12 step:11588 [D loss: 0.429798, acc.: 87.50%] [G loss: 0.715908]\n",
      "epoch:12 step:11589 [D loss: 0.632124, acc.: 66.41%] [G loss: 0.568574]\n",
      "epoch:12 step:11590 [D loss: 0.522185, acc.: 69.53%] [G loss: 0.922902]\n",
      "epoch:12 step:11591 [D loss: 0.455416, acc.: 82.03%] [G loss: 1.063577]\n",
      "epoch:12 step:11592 [D loss: 0.647784, acc.: 63.28%] [G loss: 0.691978]\n",
      "epoch:12 step:11593 [D loss: 0.725932, acc.: 54.69%] [G loss: 0.416882]\n",
      "epoch:12 step:11594 [D loss: 0.524739, acc.: 72.66%] [G loss: 0.530868]\n",
      "epoch:12 step:11595 [D loss: 0.516071, acc.: 75.78%] [G loss: 0.577595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11596 [D loss: 0.595156, acc.: 64.84%] [G loss: 0.607490]\n",
      "epoch:12 step:11597 [D loss: 0.596891, acc.: 62.50%] [G loss: 0.638107]\n",
      "epoch:12 step:11598 [D loss: 0.410888, acc.: 85.94%] [G loss: 0.763162]\n",
      "epoch:12 step:11599 [D loss: 0.553862, acc.: 71.09%] [G loss: 0.594502]\n",
      "epoch:12 step:11600 [D loss: 0.529666, acc.: 74.22%] [G loss: 0.659693]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.406376\n",
      "FID: 50.237263\n",
      "0 = 12.902365229129783\n",
      "1 = 0.09740752015740536\n",
      "2 = 0.8996999859809875\n",
      "3 = 0.8393999934196472\n",
      "4 = 0.9599999785423279\n",
      "5 = 0.9545144438743591\n",
      "6 = 0.8393999934196472\n",
      "7 = 8.381609325873855\n",
      "8 = 0.15553961437529235\n",
      "9 = 0.7422999739646912\n",
      "10 = 0.7253999710083008\n",
      "11 = 0.7591999769210815\n",
      "12 = 0.7507762312889099\n",
      "13 = 0.7253999710083008\n",
      "14 = 6.406402587890625\n",
      "15 = 7.339705944061279\n",
      "16 = 0.3704315721988678\n",
      "17 = 6.406375885009766\n",
      "18 = 50.23726272583008\n",
      "epoch:12 step:11601 [D loss: 0.449981, acc.: 78.91%] [G loss: 0.649847]\n",
      "epoch:12 step:11602 [D loss: 0.454634, acc.: 75.78%] [G loss: 0.884814]\n",
      "epoch:12 step:11603 [D loss: 0.474161, acc.: 77.34%] [G loss: 0.864778]\n",
      "epoch:12 step:11604 [D loss: 0.515299, acc.: 73.44%] [G loss: 0.717743]\n",
      "epoch:12 step:11605 [D loss: 0.558081, acc.: 70.31%] [G loss: 0.667629]\n",
      "epoch:12 step:11606 [D loss: 0.603821, acc.: 64.84%] [G loss: 0.606911]\n",
      "epoch:12 step:11607 [D loss: 0.536169, acc.: 71.88%] [G loss: 0.518086]\n",
      "epoch:12 step:11608 [D loss: 0.525953, acc.: 71.88%] [G loss: 0.526044]\n",
      "epoch:12 step:11609 [D loss: 0.561135, acc.: 71.88%] [G loss: 0.504220]\n",
      "epoch:12 step:11610 [D loss: 0.513292, acc.: 71.88%] [G loss: 0.623935]\n",
      "epoch:12 step:11611 [D loss: 0.574746, acc.: 65.62%] [G loss: 0.571639]\n",
      "epoch:12 step:11612 [D loss: 0.531520, acc.: 71.09%] [G loss: 0.665926]\n",
      "epoch:12 step:11613 [D loss: 0.522750, acc.: 72.66%] [G loss: 0.554332]\n",
      "epoch:12 step:11614 [D loss: 0.560015, acc.: 74.22%] [G loss: 0.656968]\n",
      "epoch:12 step:11615 [D loss: 0.486625, acc.: 75.78%] [G loss: 0.757256]\n",
      "epoch:12 step:11616 [D loss: 0.546931, acc.: 72.66%] [G loss: 0.576287]\n",
      "epoch:12 step:11617 [D loss: 0.566926, acc.: 68.75%] [G loss: 0.505240]\n",
      "epoch:12 step:11618 [D loss: 0.487746, acc.: 75.00%] [G loss: 0.734989]\n",
      "epoch:12 step:11619 [D loss: 0.612937, acc.: 67.19%] [G loss: 0.556070]\n",
      "epoch:12 step:11620 [D loss: 0.723647, acc.: 55.47%] [G loss: 0.460416]\n",
      "epoch:12 step:11621 [D loss: 0.604135, acc.: 62.50%] [G loss: 0.344734]\n",
      "epoch:12 step:11622 [D loss: 0.580916, acc.: 66.41%] [G loss: 0.494204]\n",
      "epoch:12 step:11623 [D loss: 0.545037, acc.: 73.44%] [G loss: 0.397238]\n",
      "epoch:12 step:11624 [D loss: 0.612162, acc.: 63.28%] [G loss: 0.447504]\n",
      "epoch:12 step:11625 [D loss: 0.453091, acc.: 77.34%] [G loss: 0.600849]\n",
      "epoch:12 step:11626 [D loss: 0.500271, acc.: 76.56%] [G loss: 0.593954]\n",
      "epoch:12 step:11627 [D loss: 0.571509, acc.: 67.97%] [G loss: 0.650471]\n",
      "epoch:12 step:11628 [D loss: 0.583581, acc.: 63.28%] [G loss: 0.547959]\n",
      "epoch:12 step:11629 [D loss: 0.442192, acc.: 79.69%] [G loss: 0.595396]\n",
      "epoch:12 step:11630 [D loss: 0.620846, acc.: 65.62%] [G loss: 0.495088]\n",
      "epoch:12 step:11631 [D loss: 0.587466, acc.: 64.06%] [G loss: 0.407026]\n",
      "epoch:12 step:11632 [D loss: 0.534579, acc.: 74.22%] [G loss: 0.571790]\n",
      "epoch:12 step:11633 [D loss: 0.573505, acc.: 68.75%] [G loss: 0.599171]\n",
      "epoch:12 step:11634 [D loss: 0.609636, acc.: 65.62%] [G loss: 0.647912]\n",
      "epoch:12 step:11635 [D loss: 0.541086, acc.: 73.44%] [G loss: 0.533288]\n",
      "epoch:12 step:11636 [D loss: 0.484389, acc.: 78.12%] [G loss: 0.673834]\n",
      "epoch:12 step:11637 [D loss: 0.580410, acc.: 69.53%] [G loss: 0.506151]\n",
      "epoch:12 step:11638 [D loss: 0.513962, acc.: 72.66%] [G loss: 0.543738]\n",
      "epoch:12 step:11639 [D loss: 0.571638, acc.: 69.53%] [G loss: 0.604337]\n",
      "epoch:12 step:11640 [D loss: 0.605184, acc.: 62.50%] [G loss: 0.580459]\n",
      "epoch:12 step:11641 [D loss: 0.562482, acc.: 69.53%] [G loss: 0.411617]\n",
      "epoch:12 step:11642 [D loss: 0.443724, acc.: 77.34%] [G loss: 0.746199]\n",
      "epoch:12 step:11643 [D loss: 0.526610, acc.: 75.00%] [G loss: 0.742886]\n",
      "epoch:12 step:11644 [D loss: 0.680800, acc.: 53.91%] [G loss: 0.463144]\n",
      "epoch:12 step:11645 [D loss: 0.669710, acc.: 53.91%] [G loss: 0.335500]\n",
      "epoch:12 step:11646 [D loss: 0.557177, acc.: 66.41%] [G loss: 0.445930]\n",
      "epoch:12 step:11647 [D loss: 0.502306, acc.: 79.69%] [G loss: 0.607356]\n",
      "epoch:12 step:11648 [D loss: 0.618101, acc.: 61.72%] [G loss: 0.495798]\n",
      "epoch:12 step:11649 [D loss: 0.502805, acc.: 75.00%] [G loss: 0.657658]\n",
      "epoch:12 step:11650 [D loss: 0.537390, acc.: 68.75%] [G loss: 0.642607]\n",
      "epoch:12 step:11651 [D loss: 0.623447, acc.: 60.94%] [G loss: 0.571500]\n",
      "epoch:12 step:11652 [D loss: 0.571077, acc.: 71.09%] [G loss: 0.662558]\n",
      "epoch:12 step:11653 [D loss: 0.589556, acc.: 63.28%] [G loss: 0.633494]\n",
      "epoch:12 step:11654 [D loss: 0.579359, acc.: 65.62%] [G loss: 0.535217]\n",
      "epoch:12 step:11655 [D loss: 0.557005, acc.: 66.41%] [G loss: 0.523352]\n",
      "epoch:12 step:11656 [D loss: 0.636942, acc.: 59.38%] [G loss: 0.483631]\n",
      "epoch:12 step:11657 [D loss: 0.557275, acc.: 67.97%] [G loss: 0.458468]\n",
      "epoch:12 step:11658 [D loss: 0.534055, acc.: 71.09%] [G loss: 0.609580]\n",
      "epoch:12 step:11659 [D loss: 0.499354, acc.: 73.44%] [G loss: 0.663045]\n",
      "epoch:12 step:11660 [D loss: 0.530283, acc.: 76.56%] [G loss: 0.636550]\n",
      "epoch:12 step:11661 [D loss: 0.560094, acc.: 71.88%] [G loss: 0.645815]\n",
      "epoch:12 step:11662 [D loss: 0.633072, acc.: 61.72%] [G loss: 0.542929]\n",
      "epoch:12 step:11663 [D loss: 0.559590, acc.: 67.19%] [G loss: 0.598758]\n",
      "epoch:12 step:11664 [D loss: 0.570536, acc.: 66.41%] [G loss: 0.478537]\n",
      "epoch:12 step:11665 [D loss: 0.585845, acc.: 63.28%] [G loss: 0.448082]\n",
      "epoch:12 step:11666 [D loss: 0.657306, acc.: 61.72%] [G loss: 0.655089]\n",
      "epoch:12 step:11667 [D loss: 0.545562, acc.: 74.22%] [G loss: 0.602968]\n",
      "epoch:12 step:11668 [D loss: 0.588259, acc.: 63.28%] [G loss: 0.520512]\n",
      "epoch:12 step:11669 [D loss: 0.576170, acc.: 69.53%] [G loss: 0.637808]\n",
      "epoch:12 step:11670 [D loss: 0.478876, acc.: 75.78%] [G loss: 0.619590]\n",
      "epoch:12 step:11671 [D loss: 0.508289, acc.: 71.88%] [G loss: 0.700700]\n",
      "epoch:12 step:11672 [D loss: 0.521109, acc.: 74.22%] [G loss: 0.718948]\n",
      "epoch:12 step:11673 [D loss: 0.458210, acc.: 77.34%] [G loss: 0.729453]\n",
      "epoch:12 step:11674 [D loss: 0.564296, acc.: 70.31%] [G loss: 0.627676]\n",
      "epoch:12 step:11675 [D loss: 0.567980, acc.: 70.31%] [G loss: 0.519631]\n",
      "epoch:12 step:11676 [D loss: 0.579252, acc.: 65.62%] [G loss: 0.677297]\n",
      "epoch:12 step:11677 [D loss: 0.560015, acc.: 68.75%] [G loss: 0.513230]\n",
      "epoch:12 step:11678 [D loss: 0.535424, acc.: 75.78%] [G loss: 0.465465]\n",
      "epoch:12 step:11679 [D loss: 0.558968, acc.: 74.22%] [G loss: 0.510123]\n",
      "epoch:12 step:11680 [D loss: 0.508813, acc.: 75.78%] [G loss: 0.739960]\n",
      "epoch:12 step:11681 [D loss: 0.648881, acc.: 66.41%] [G loss: 0.523062]\n",
      "epoch:12 step:11682 [D loss: 0.650058, acc.: 60.94%] [G loss: 0.549744]\n",
      "epoch:12 step:11683 [D loss: 0.524273, acc.: 73.44%] [G loss: 0.515904]\n",
      "epoch:12 step:11684 [D loss: 0.506995, acc.: 76.56%] [G loss: 0.612668]\n",
      "epoch:12 step:11685 [D loss: 0.550426, acc.: 72.66%] [G loss: 0.651673]\n",
      "epoch:12 step:11686 [D loss: 0.525666, acc.: 72.66%] [G loss: 0.709562]\n",
      "epoch:12 step:11687 [D loss: 0.546544, acc.: 71.88%] [G loss: 0.638093]\n",
      "epoch:12 step:11688 [D loss: 0.538751, acc.: 69.53%] [G loss: 0.783424]\n",
      "epoch:12 step:11689 [D loss: 0.586948, acc.: 64.84%] [G loss: 0.643116]\n",
      "epoch:12 step:11690 [D loss: 0.541389, acc.: 73.44%] [G loss: 0.630984]\n",
      "epoch:12 step:11691 [D loss: 0.540251, acc.: 71.09%] [G loss: 0.806787]\n",
      "epoch:12 step:11692 [D loss: 0.528304, acc.: 71.88%] [G loss: 0.570927]\n",
      "epoch:12 step:11693 [D loss: 0.466120, acc.: 75.00%] [G loss: 0.645735]\n",
      "epoch:12 step:11694 [D loss: 0.498756, acc.: 75.78%] [G loss: 0.706781]\n",
      "epoch:12 step:11695 [D loss: 0.466608, acc.: 75.78%] [G loss: 0.665184]\n",
      "epoch:12 step:11696 [D loss: 0.537475, acc.: 71.88%] [G loss: 0.662841]\n",
      "epoch:12 step:11697 [D loss: 0.578318, acc.: 65.62%] [G loss: 0.622407]\n",
      "epoch:12 step:11698 [D loss: 0.538735, acc.: 74.22%] [G loss: 0.663305]\n",
      "epoch:12 step:11699 [D loss: 0.560548, acc.: 67.97%] [G loss: 0.607444]\n",
      "epoch:12 step:11700 [D loss: 0.669251, acc.: 64.06%] [G loss: 0.633764]\n",
      "epoch:12 step:11701 [D loss: 0.501528, acc.: 76.56%] [G loss: 0.690992]\n",
      "epoch:12 step:11702 [D loss: 0.626955, acc.: 65.62%] [G loss: 0.697156]\n",
      "epoch:12 step:11703 [D loss: 0.535265, acc.: 71.09%] [G loss: 0.519077]\n",
      "epoch:12 step:11704 [D loss: 0.548829, acc.: 71.88%] [G loss: 0.530691]\n",
      "epoch:12 step:11705 [D loss: 0.521283, acc.: 71.88%] [G loss: 0.651194]\n",
      "epoch:12 step:11706 [D loss: 0.546637, acc.: 67.97%] [G loss: 0.554623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11707 [D loss: 0.576004, acc.: 65.62%] [G loss: 0.387951]\n",
      "epoch:12 step:11708 [D loss: 0.477261, acc.: 76.56%] [G loss: 0.562934]\n",
      "epoch:12 step:11709 [D loss: 0.601075, acc.: 67.19%] [G loss: 0.496061]\n",
      "epoch:12 step:11710 [D loss: 0.567303, acc.: 71.09%] [G loss: 0.538887]\n",
      "epoch:12 step:11711 [D loss: 0.542340, acc.: 71.88%] [G loss: 0.538768]\n",
      "epoch:12 step:11712 [D loss: 0.590651, acc.: 66.41%] [G loss: 0.578620]\n",
      "epoch:12 step:11713 [D loss: 0.526767, acc.: 71.88%] [G loss: 0.700745]\n",
      "epoch:12 step:11714 [D loss: 0.486553, acc.: 75.78%] [G loss: 0.670003]\n",
      "epoch:12 step:11715 [D loss: 0.433606, acc.: 80.47%] [G loss: 0.809613]\n",
      "epoch:12 step:11716 [D loss: 0.484443, acc.: 75.78%] [G loss: 0.785161]\n",
      "epoch:12 step:11717 [D loss: 0.668146, acc.: 61.72%] [G loss: 0.533934]\n",
      "epoch:12 step:11718 [D loss: 0.542120, acc.: 68.75%] [G loss: 0.614757]\n",
      "epoch:12 step:11719 [D loss: 0.499657, acc.: 75.78%] [G loss: 0.724960]\n",
      "epoch:12 step:11720 [D loss: 0.593825, acc.: 67.19%] [G loss: 0.740051]\n",
      "epoch:12 step:11721 [D loss: 0.679509, acc.: 62.50%] [G loss: 0.586317]\n",
      "epoch:12 step:11722 [D loss: 0.620651, acc.: 65.62%] [G loss: 0.497858]\n",
      "epoch:12 step:11723 [D loss: 0.539948, acc.: 69.53%] [G loss: 0.442072]\n",
      "epoch:12 step:11724 [D loss: 0.608217, acc.: 68.75%] [G loss: 0.393260]\n",
      "epoch:12 step:11725 [D loss: 0.515018, acc.: 77.34%] [G loss: 0.507780]\n",
      "epoch:12 step:11726 [D loss: 0.607116, acc.: 66.41%] [G loss: 0.585266]\n",
      "epoch:12 step:11727 [D loss: 0.518819, acc.: 72.66%] [G loss: 0.605433]\n",
      "epoch:12 step:11728 [D loss: 0.507604, acc.: 73.44%] [G loss: 0.635777]\n",
      "epoch:12 step:11729 [D loss: 0.557324, acc.: 71.09%] [G loss: 0.627622]\n",
      "epoch:12 step:11730 [D loss: 0.588166, acc.: 67.97%] [G loss: 0.626659]\n",
      "epoch:12 step:11731 [D loss: 0.559540, acc.: 66.41%] [G loss: 0.645076]\n",
      "epoch:12 step:11732 [D loss: 0.490060, acc.: 75.00%] [G loss: 0.543205]\n",
      "epoch:12 step:11733 [D loss: 0.552546, acc.: 70.31%] [G loss: 0.613730]\n",
      "epoch:12 step:11734 [D loss: 0.554689, acc.: 70.31%] [G loss: 0.696012]\n",
      "epoch:12 step:11735 [D loss: 0.532115, acc.: 68.75%] [G loss: 0.598567]\n",
      "epoch:12 step:11736 [D loss: 0.553737, acc.: 72.66%] [G loss: 0.598654]\n",
      "epoch:12 step:11737 [D loss: 0.551512, acc.: 70.31%] [G loss: 0.460874]\n",
      "epoch:12 step:11738 [D loss: 0.625316, acc.: 65.62%] [G loss: 0.364536]\n",
      "epoch:12 step:11739 [D loss: 0.487298, acc.: 76.56%] [G loss: 0.589014]\n",
      "epoch:12 step:11740 [D loss: 0.561035, acc.: 71.09%] [G loss: 0.646865]\n",
      "epoch:12 step:11741 [D loss: 0.561399, acc.: 68.75%] [G loss: 0.523329]\n",
      "epoch:12 step:11742 [D loss: 0.529947, acc.: 69.53%] [G loss: 0.586182]\n",
      "epoch:12 step:11743 [D loss: 0.493723, acc.: 72.66%] [G loss: 0.673748]\n",
      "epoch:12 step:11744 [D loss: 0.591292, acc.: 69.53%] [G loss: 0.589898]\n",
      "epoch:12 step:11745 [D loss: 0.649240, acc.: 61.72%] [G loss: 0.455837]\n",
      "epoch:12 step:11746 [D loss: 0.619122, acc.: 63.28%] [G loss: 0.388267]\n",
      "epoch:12 step:11747 [D loss: 0.515664, acc.: 70.31%] [G loss: 0.539697]\n",
      "epoch:12 step:11748 [D loss: 0.520558, acc.: 78.12%] [G loss: 0.733576]\n",
      "epoch:12 step:11749 [D loss: 0.514567, acc.: 75.00%] [G loss: 0.698431]\n",
      "epoch:12 step:11750 [D loss: 0.588439, acc.: 65.62%] [G loss: 0.790537]\n",
      "epoch:12 step:11751 [D loss: 0.545343, acc.: 75.78%] [G loss: 0.691600]\n",
      "epoch:12 step:11752 [D loss: 0.427301, acc.: 84.38%] [G loss: 0.809384]\n",
      "epoch:12 step:11753 [D loss: 0.614792, acc.: 69.53%] [G loss: 0.663802]\n",
      "epoch:12 step:11754 [D loss: 0.650281, acc.: 59.38%] [G loss: 0.773406]\n",
      "epoch:12 step:11755 [D loss: 0.632726, acc.: 64.06%] [G loss: 0.570127]\n",
      "epoch:12 step:11756 [D loss: 0.600971, acc.: 62.50%] [G loss: 0.608092]\n",
      "epoch:12 step:11757 [D loss: 0.525938, acc.: 71.88%] [G loss: 0.567332]\n",
      "epoch:12 step:11758 [D loss: 0.499893, acc.: 75.78%] [G loss: 0.676015]\n",
      "epoch:12 step:11759 [D loss: 0.528079, acc.: 72.66%] [G loss: 0.679546]\n",
      "epoch:12 step:11760 [D loss: 0.516587, acc.: 75.00%] [G loss: 0.604258]\n",
      "epoch:12 step:11761 [D loss: 0.514786, acc.: 75.00%] [G loss: 0.619128]\n",
      "epoch:12 step:11762 [D loss: 0.569178, acc.: 66.41%] [G loss: 0.505299]\n",
      "epoch:12 step:11763 [D loss: 0.458618, acc.: 77.34%] [G loss: 0.725600]\n",
      "epoch:12 step:11764 [D loss: 0.515202, acc.: 75.78%] [G loss: 0.684899]\n",
      "epoch:12 step:11765 [D loss: 0.514345, acc.: 72.66%] [G loss: 0.813188]\n",
      "epoch:12 step:11766 [D loss: 0.516630, acc.: 73.44%] [G loss: 0.795518]\n",
      "epoch:12 step:11767 [D loss: 0.540933, acc.: 71.09%] [G loss: 0.771215]\n",
      "epoch:12 step:11768 [D loss: 0.534585, acc.: 71.88%] [G loss: 0.508028]\n",
      "epoch:12 step:11769 [D loss: 0.610121, acc.: 65.62%] [G loss: 0.627402]\n",
      "epoch:12 step:11770 [D loss: 0.544071, acc.: 67.97%] [G loss: 0.373377]\n",
      "epoch:12 step:11771 [D loss: 0.584673, acc.: 65.62%] [G loss: 0.554621]\n",
      "epoch:12 step:11772 [D loss: 0.670643, acc.: 55.47%] [G loss: 0.511646]\n",
      "epoch:12 step:11773 [D loss: 0.589895, acc.: 67.19%] [G loss: 0.549320]\n",
      "epoch:12 step:11774 [D loss: 0.551587, acc.: 71.09%] [G loss: 0.503944]\n",
      "epoch:12 step:11775 [D loss: 0.631107, acc.: 64.84%] [G loss: 0.522697]\n",
      "epoch:12 step:11776 [D loss: 0.570394, acc.: 70.31%] [G loss: 0.624697]\n",
      "epoch:12 step:11777 [D loss: 0.541798, acc.: 71.09%] [G loss: 0.472816]\n",
      "epoch:12 step:11778 [D loss: 0.483822, acc.: 75.00%] [G loss: 0.651805]\n",
      "epoch:12 step:11779 [D loss: 0.657805, acc.: 60.16%] [G loss: 0.450959]\n",
      "epoch:12 step:11780 [D loss: 0.488876, acc.: 75.00%] [G loss: 0.574973]\n",
      "epoch:12 step:11781 [D loss: 0.599265, acc.: 67.97%] [G loss: 0.587038]\n",
      "epoch:12 step:11782 [D loss: 0.574069, acc.: 67.19%] [G loss: 0.634345]\n",
      "epoch:12 step:11783 [D loss: 0.595759, acc.: 69.53%] [G loss: 0.674074]\n",
      "epoch:12 step:11784 [D loss: 0.571462, acc.: 64.84%] [G loss: 0.456730]\n",
      "epoch:12 step:11785 [D loss: 0.563308, acc.: 65.62%] [G loss: 0.475754]\n",
      "epoch:12 step:11786 [D loss: 0.672902, acc.: 60.94%] [G loss: 0.522498]\n",
      "epoch:12 step:11787 [D loss: 0.588332, acc.: 67.97%] [G loss: 0.524778]\n",
      "epoch:12 step:11788 [D loss: 0.596893, acc.: 67.97%] [G loss: 0.477726]\n",
      "epoch:12 step:11789 [D loss: 0.596852, acc.: 66.41%] [G loss: 0.647806]\n",
      "epoch:12 step:11790 [D loss: 0.529351, acc.: 71.09%] [G loss: 0.671024]\n",
      "epoch:12 step:11791 [D loss: 0.505117, acc.: 75.00%] [G loss: 0.673145]\n",
      "epoch:12 step:11792 [D loss: 0.500906, acc.: 68.75%] [G loss: 0.561811]\n",
      "epoch:12 step:11793 [D loss: 0.582977, acc.: 66.41%] [G loss: 0.579198]\n",
      "epoch:12 step:11794 [D loss: 0.554295, acc.: 73.44%] [G loss: 0.761009]\n",
      "epoch:12 step:11795 [D loss: 0.526021, acc.: 73.44%] [G loss: 0.561916]\n",
      "epoch:12 step:11796 [D loss: 0.497506, acc.: 75.00%] [G loss: 0.668556]\n",
      "epoch:12 step:11797 [D loss: 0.583869, acc.: 67.97%] [G loss: 0.509675]\n",
      "epoch:12 step:11798 [D loss: 0.456887, acc.: 78.12%] [G loss: 0.664280]\n",
      "epoch:12 step:11799 [D loss: 0.494227, acc.: 77.34%] [G loss: 0.661894]\n",
      "epoch:12 step:11800 [D loss: 0.508073, acc.: 76.56%] [G loss: 0.549080]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.532032\n",
      "FID: 45.057770\n",
      "0 = 12.9746836767196\n",
      "1 = 0.0928935026937428\n",
      "2 = 0.8996999859809875\n",
      "3 = 0.8442000150680542\n",
      "4 = 0.9552000164985657\n",
      "5 = 0.9496062994003296\n",
      "6 = 0.8442000150680542\n",
      "7 = 8.098304174017901\n",
      "8 = 0.14258105131307763\n",
      "9 = 0.7250999808311462\n",
      "10 = 0.7164000272750854\n",
      "11 = 0.7337999939918518\n",
      "12 = 0.7290861010551453\n",
      "13 = 0.7164000272750854\n",
      "14 = 6.532058238983154\n",
      "15 = 7.319853782653809\n",
      "16 = 0.36488476395606995\n",
      "17 = 6.532032489776611\n",
      "18 = 45.057769775390625\n",
      "epoch:12 step:11801 [D loss: 0.514022, acc.: 75.00%] [G loss: 0.580845]\n",
      "epoch:12 step:11802 [D loss: 0.483619, acc.: 78.12%] [G loss: 0.622746]\n",
      "epoch:12 step:11803 [D loss: 0.651374, acc.: 64.06%] [G loss: 0.585947]\n",
      "epoch:12 step:11804 [D loss: 0.563034, acc.: 71.09%] [G loss: 0.563114]\n",
      "epoch:12 step:11805 [D loss: 0.551857, acc.: 74.22%] [G loss: 0.482299]\n",
      "epoch:12 step:11806 [D loss: 0.566389, acc.: 67.97%] [G loss: 0.576522]\n",
      "epoch:12 step:11807 [D loss: 0.584454, acc.: 67.19%] [G loss: 0.622220]\n",
      "epoch:12 step:11808 [D loss: 0.466911, acc.: 78.12%] [G loss: 0.634382]\n",
      "epoch:12 step:11809 [D loss: 0.573182, acc.: 67.19%] [G loss: 0.584249]\n",
      "epoch:12 step:11810 [D loss: 0.705610, acc.: 57.03%] [G loss: 0.533737]\n",
      "epoch:12 step:11811 [D loss: 0.543668, acc.: 73.44%] [G loss: 0.641121]\n",
      "epoch:12 step:11812 [D loss: 0.515668, acc.: 75.00%] [G loss: 0.581320]\n",
      "epoch:12 step:11813 [D loss: 0.540382, acc.: 73.44%] [G loss: 0.607648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11814 [D loss: 0.535126, acc.: 74.22%] [G loss: 0.591414]\n",
      "epoch:12 step:11815 [D loss: 0.550923, acc.: 68.75%] [G loss: 0.507758]\n",
      "epoch:12 step:11816 [D loss: 0.571507, acc.: 67.19%] [G loss: 0.466691]\n",
      "epoch:12 step:11817 [D loss: 0.587311, acc.: 68.75%] [G loss: 0.548030]\n",
      "epoch:12 step:11818 [D loss: 0.493686, acc.: 78.91%] [G loss: 0.576336]\n",
      "epoch:12 step:11819 [D loss: 0.488153, acc.: 72.66%] [G loss: 0.693659]\n",
      "epoch:12 step:11820 [D loss: 0.619236, acc.: 61.72%] [G loss: 0.710996]\n",
      "epoch:12 step:11821 [D loss: 0.530090, acc.: 73.44%] [G loss: 0.712763]\n",
      "epoch:12 step:11822 [D loss: 0.520888, acc.: 71.88%] [G loss: 0.521145]\n",
      "epoch:12 step:11823 [D loss: 0.514950, acc.: 72.66%] [G loss: 0.680406]\n",
      "epoch:12 step:11824 [D loss: 0.530831, acc.: 72.66%] [G loss: 0.778990]\n",
      "epoch:12 step:11825 [D loss: 0.563236, acc.: 66.41%] [G loss: 0.608959]\n",
      "epoch:12 step:11826 [D loss: 0.432470, acc.: 83.59%] [G loss: 0.754377]\n",
      "epoch:12 step:11827 [D loss: 0.492675, acc.: 74.22%] [G loss: 0.885160]\n",
      "epoch:12 step:11828 [D loss: 0.636693, acc.: 60.94%] [G loss: 0.562921]\n",
      "epoch:12 step:11829 [D loss: 0.518057, acc.: 74.22%] [G loss: 0.530229]\n",
      "epoch:12 step:11830 [D loss: 0.567897, acc.: 69.53%] [G loss: 0.423329]\n",
      "epoch:12 step:11831 [D loss: 0.563130, acc.: 67.19%] [G loss: 0.621654]\n",
      "epoch:12 step:11832 [D loss: 0.596629, acc.: 63.28%] [G loss: 0.462711]\n",
      "epoch:12 step:11833 [D loss: 0.524795, acc.: 68.75%] [G loss: 0.532965]\n",
      "epoch:12 step:11834 [D loss: 0.572921, acc.: 67.19%] [G loss: 0.704593]\n",
      "epoch:12 step:11835 [D loss: 0.550514, acc.: 68.75%] [G loss: 0.555996]\n",
      "epoch:12 step:11836 [D loss: 0.574751, acc.: 70.31%] [G loss: 0.633844]\n",
      "epoch:12 step:11837 [D loss: 0.541034, acc.: 71.88%] [G loss: 0.548732]\n",
      "epoch:12 step:11838 [D loss: 0.556618, acc.: 64.06%] [G loss: 0.626463]\n",
      "epoch:12 step:11839 [D loss: 0.583498, acc.: 73.44%] [G loss: 0.529216]\n",
      "epoch:12 step:11840 [D loss: 0.568992, acc.: 67.19%] [G loss: 0.502476]\n",
      "epoch:12 step:11841 [D loss: 0.581093, acc.: 70.31%] [G loss: 0.565593]\n",
      "epoch:12 step:11842 [D loss: 0.519805, acc.: 71.88%] [G loss: 0.812355]\n",
      "epoch:12 step:11843 [D loss: 0.572630, acc.: 68.75%] [G loss: 0.717654]\n",
      "epoch:12 step:11844 [D loss: 0.581058, acc.: 65.62%] [G loss: 0.469111]\n",
      "epoch:12 step:11845 [D loss: 0.578128, acc.: 65.62%] [G loss: 0.491776]\n",
      "epoch:12 step:11846 [D loss: 0.529661, acc.: 67.97%] [G loss: 0.727165]\n",
      "epoch:12 step:11847 [D loss: 0.509940, acc.: 77.34%] [G loss: 0.794445]\n",
      "epoch:12 step:11848 [D loss: 0.590837, acc.: 68.75%] [G loss: 0.746437]\n",
      "epoch:12 step:11849 [D loss: 0.483229, acc.: 78.12%] [G loss: 0.669742]\n",
      "epoch:12 step:11850 [D loss: 0.566126, acc.: 68.75%] [G loss: 0.586046]\n",
      "epoch:12 step:11851 [D loss: 0.557145, acc.: 71.09%] [G loss: 0.471754]\n",
      "epoch:12 step:11852 [D loss: 0.563351, acc.: 71.09%] [G loss: 0.511676]\n",
      "epoch:12 step:11853 [D loss: 0.532547, acc.: 75.00%] [G loss: 0.381710]\n",
      "epoch:12 step:11854 [D loss: 0.553247, acc.: 70.31%] [G loss: 0.417058]\n",
      "epoch:12 step:11855 [D loss: 0.529942, acc.: 70.31%] [G loss: 0.532718]\n",
      "epoch:12 step:11856 [D loss: 0.560694, acc.: 67.97%] [G loss: 0.489340]\n",
      "epoch:12 step:11857 [D loss: 0.551030, acc.: 71.88%] [G loss: 0.703217]\n",
      "epoch:12 step:11858 [D loss: 0.573532, acc.: 69.53%] [G loss: 0.587753]\n",
      "epoch:12 step:11859 [D loss: 0.575232, acc.: 65.62%] [G loss: 0.529103]\n",
      "epoch:12 step:11860 [D loss: 0.575049, acc.: 68.75%] [G loss: 0.594573]\n",
      "epoch:12 step:11861 [D loss: 0.582920, acc.: 68.75%] [G loss: 0.511507]\n",
      "epoch:12 step:11862 [D loss: 0.528516, acc.: 68.75%] [G loss: 0.683011]\n",
      "epoch:12 step:11863 [D loss: 0.612834, acc.: 64.06%] [G loss: 0.515473]\n",
      "epoch:12 step:11864 [D loss: 0.540357, acc.: 71.09%] [G loss: 0.664271]\n",
      "epoch:12 step:11865 [D loss: 0.555786, acc.: 66.41%] [G loss: 0.539783]\n",
      "epoch:12 step:11866 [D loss: 0.628570, acc.: 67.97%] [G loss: 0.525863]\n",
      "epoch:12 step:11867 [D loss: 0.501099, acc.: 70.31%] [G loss: 0.582064]\n",
      "epoch:12 step:11868 [D loss: 0.465687, acc.: 77.34%] [G loss: 0.758422]\n",
      "epoch:12 step:11869 [D loss: 0.618654, acc.: 65.62%] [G loss: 0.631784]\n",
      "epoch:12 step:11870 [D loss: 0.545045, acc.: 70.31%] [G loss: 0.682751]\n",
      "epoch:12 step:11871 [D loss: 0.557071, acc.: 65.62%] [G loss: 0.526635]\n",
      "epoch:12 step:11872 [D loss: 0.648709, acc.: 63.28%] [G loss: 0.550371]\n",
      "epoch:12 step:11873 [D loss: 0.540402, acc.: 70.31%] [G loss: 0.586181]\n",
      "epoch:12 step:11874 [D loss: 0.565189, acc.: 68.75%] [G loss: 0.573851]\n",
      "epoch:12 step:11875 [D loss: 0.518996, acc.: 75.78%] [G loss: 0.677802]\n",
      "epoch:12 step:11876 [D loss: 0.493314, acc.: 78.91%] [G loss: 0.605993]\n",
      "epoch:12 step:11877 [D loss: 0.513434, acc.: 75.78%] [G loss: 0.632107]\n",
      "epoch:12 step:11878 [D loss: 0.502338, acc.: 75.00%] [G loss: 0.756699]\n",
      "epoch:12 step:11879 [D loss: 0.482738, acc.: 72.66%] [G loss: 0.817397]\n",
      "epoch:12 step:11880 [D loss: 0.670887, acc.: 58.59%] [G loss: 0.398794]\n",
      "epoch:12 step:11881 [D loss: 0.535713, acc.: 73.44%] [G loss: 0.587221]\n",
      "epoch:12 step:11882 [D loss: 0.532971, acc.: 75.00%] [G loss: 0.462347]\n",
      "epoch:12 step:11883 [D loss: 0.530400, acc.: 67.19%] [G loss: 0.490604]\n",
      "epoch:12 step:11884 [D loss: 0.519210, acc.: 74.22%] [G loss: 0.521302]\n",
      "epoch:12 step:11885 [D loss: 0.480547, acc.: 73.44%] [G loss: 0.713384]\n",
      "epoch:12 step:11886 [D loss: 0.495626, acc.: 75.78%] [G loss: 0.822529]\n",
      "epoch:12 step:11887 [D loss: 0.565852, acc.: 69.53%] [G loss: 0.587039]\n",
      "epoch:12 step:11888 [D loss: 0.590163, acc.: 68.75%] [G loss: 0.546278]\n",
      "epoch:12 step:11889 [D loss: 0.547188, acc.: 66.41%] [G loss: 0.430856]\n",
      "epoch:12 step:11890 [D loss: 0.525633, acc.: 77.34%] [G loss: 0.505406]\n",
      "epoch:12 step:11891 [D loss: 0.478007, acc.: 76.56%] [G loss: 0.587352]\n",
      "epoch:12 step:11892 [D loss: 0.409493, acc.: 82.03%] [G loss: 0.841425]\n",
      "epoch:12 step:11893 [D loss: 0.510253, acc.: 71.88%] [G loss: 0.690679]\n",
      "epoch:12 step:11894 [D loss: 0.474113, acc.: 78.91%] [G loss: 0.723432]\n",
      "epoch:12 step:11895 [D loss: 0.564066, acc.: 71.09%] [G loss: 0.827745]\n",
      "epoch:12 step:11896 [D loss: 0.568496, acc.: 68.75%] [G loss: 0.558629]\n",
      "epoch:12 step:11897 [D loss: 0.620846, acc.: 63.28%] [G loss: 0.535010]\n",
      "epoch:12 step:11898 [D loss: 0.524877, acc.: 70.31%] [G loss: 0.685216]\n",
      "epoch:12 step:11899 [D loss: 0.544319, acc.: 74.22%] [G loss: 0.530828]\n",
      "epoch:12 step:11900 [D loss: 0.519399, acc.: 74.22%] [G loss: 0.534575]\n",
      "epoch:12 step:11901 [D loss: 0.533973, acc.: 67.19%] [G loss: 0.545041]\n",
      "epoch:12 step:11902 [D loss: 0.552118, acc.: 72.66%] [G loss: 0.594217]\n",
      "epoch:12 step:11903 [D loss: 0.532375, acc.: 71.88%] [G loss: 0.579675]\n",
      "epoch:12 step:11904 [D loss: 0.585053, acc.: 69.53%] [G loss: 0.523103]\n",
      "epoch:12 step:11905 [D loss: 0.511891, acc.: 76.56%] [G loss: 0.682138]\n",
      "epoch:12 step:11906 [D loss: 0.547607, acc.: 71.88%] [G loss: 0.567705]\n",
      "epoch:12 step:11907 [D loss: 0.544754, acc.: 67.97%] [G loss: 0.580596]\n",
      "epoch:12 step:11908 [D loss: 0.551257, acc.: 66.41%] [G loss: 0.637096]\n",
      "epoch:12 step:11909 [D loss: 0.619180, acc.: 60.94%] [G loss: 0.694798]\n",
      "epoch:12 step:11910 [D loss: 0.522428, acc.: 73.44%] [G loss: 0.570774]\n",
      "epoch:12 step:11911 [D loss: 0.574341, acc.: 67.19%] [G loss: 0.529573]\n",
      "epoch:12 step:11912 [D loss: 0.629023, acc.: 64.06%] [G loss: 0.490850]\n",
      "epoch:12 step:11913 [D loss: 0.550401, acc.: 71.09%] [G loss: 0.539083]\n",
      "epoch:12 step:11914 [D loss: 0.596001, acc.: 64.06%] [G loss: 0.514269]\n",
      "epoch:12 step:11915 [D loss: 0.584341, acc.: 67.97%] [G loss: 0.484960]\n",
      "epoch:12 step:11916 [D loss: 0.576412, acc.: 67.19%] [G loss: 0.480816]\n",
      "epoch:12 step:11917 [D loss: 0.624924, acc.: 61.72%] [G loss: 0.385488]\n",
      "epoch:12 step:11918 [D loss: 0.588072, acc.: 64.06%] [G loss: 0.549321]\n",
      "epoch:12 step:11919 [D loss: 0.553536, acc.: 65.62%] [G loss: 0.542066]\n",
      "epoch:12 step:11920 [D loss: 0.608758, acc.: 64.84%] [G loss: 0.514712]\n",
      "epoch:12 step:11921 [D loss: 0.515254, acc.: 71.88%] [G loss: 0.645781]\n",
      "epoch:12 step:11922 [D loss: 0.594798, acc.: 69.53%] [G loss: 0.608834]\n",
      "epoch:12 step:11923 [D loss: 0.526367, acc.: 72.66%] [G loss: 0.655241]\n",
      "epoch:12 step:11924 [D loss: 0.552740, acc.: 74.22%] [G loss: 0.572920]\n",
      "epoch:12 step:11925 [D loss: 0.524434, acc.: 71.88%] [G loss: 0.553457]\n",
      "epoch:12 step:11926 [D loss: 0.580716, acc.: 64.06%] [G loss: 0.573385]\n",
      "epoch:12 step:11927 [D loss: 0.565904, acc.: 67.19%] [G loss: 0.603548]\n",
      "epoch:12 step:11928 [D loss: 0.575051, acc.: 64.84%] [G loss: 0.463935]\n",
      "epoch:12 step:11929 [D loss: 0.517254, acc.: 73.44%] [G loss: 0.446502]\n",
      "epoch:12 step:11930 [D loss: 0.606090, acc.: 67.19%] [G loss: 0.414567]\n",
      "epoch:12 step:11931 [D loss: 0.550784, acc.: 67.97%] [G loss: 0.590565]\n",
      "epoch:12 step:11932 [D loss: 0.550398, acc.: 67.97%] [G loss: 0.537783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11933 [D loss: 0.577779, acc.: 67.97%] [G loss: 0.527585]\n",
      "epoch:12 step:11934 [D loss: 0.478671, acc.: 77.34%] [G loss: 0.638540]\n",
      "epoch:12 step:11935 [D loss: 0.498912, acc.: 78.12%] [G loss: 0.573049]\n",
      "epoch:12 step:11936 [D loss: 0.555450, acc.: 73.44%] [G loss: 0.520304]\n",
      "epoch:12 step:11937 [D loss: 0.453217, acc.: 78.91%] [G loss: 0.701719]\n",
      "epoch:12 step:11938 [D loss: 0.537420, acc.: 72.66%] [G loss: 0.630022]\n",
      "epoch:12 step:11939 [D loss: 0.568780, acc.: 71.88%] [G loss: 0.661746]\n",
      "epoch:12 step:11940 [D loss: 0.648022, acc.: 61.72%] [G loss: 0.459745]\n",
      "epoch:12 step:11941 [D loss: 0.577487, acc.: 67.19%] [G loss: 0.482206]\n",
      "epoch:12 step:11942 [D loss: 0.566027, acc.: 70.31%] [G loss: 0.576848]\n",
      "epoch:12 step:11943 [D loss: 0.556186, acc.: 67.19%] [G loss: 0.570505]\n",
      "epoch:12 step:11944 [D loss: 0.538352, acc.: 73.44%] [G loss: 0.576761]\n",
      "epoch:12 step:11945 [D loss: 0.529627, acc.: 70.31%] [G loss: 0.618743]\n",
      "epoch:12 step:11946 [D loss: 0.582640, acc.: 67.97%] [G loss: 0.637561]\n",
      "epoch:12 step:11947 [D loss: 0.579956, acc.: 71.09%] [G loss: 0.491500]\n",
      "epoch:12 step:11948 [D loss: 0.599383, acc.: 62.50%] [G loss: 0.569995]\n",
      "epoch:12 step:11949 [D loss: 0.479600, acc.: 75.78%] [G loss: 0.602171]\n",
      "epoch:12 step:11950 [D loss: 0.584605, acc.: 64.06%] [G loss: 0.561702]\n",
      "epoch:12 step:11951 [D loss: 0.557034, acc.: 69.53%] [G loss: 0.651601]\n",
      "epoch:12 step:11952 [D loss: 0.490309, acc.: 78.91%] [G loss: 0.621135]\n",
      "epoch:12 step:11953 [D loss: 0.551357, acc.: 68.75%] [G loss: 0.705839]\n",
      "epoch:12 step:11954 [D loss: 0.613197, acc.: 67.97%] [G loss: 0.666099]\n",
      "epoch:12 step:11955 [D loss: 0.568697, acc.: 72.66%] [G loss: 0.556735]\n",
      "epoch:12 step:11956 [D loss: 0.605171, acc.: 63.28%] [G loss: 0.654237]\n",
      "epoch:12 step:11957 [D loss: 0.641652, acc.: 61.72%] [G loss: 0.753132]\n",
      "epoch:12 step:11958 [D loss: 0.570646, acc.: 67.97%] [G loss: 0.598367]\n",
      "epoch:12 step:11959 [D loss: 0.554355, acc.: 66.41%] [G loss: 0.575062]\n",
      "epoch:12 step:11960 [D loss: 0.672363, acc.: 57.81%] [G loss: 0.551562]\n",
      "epoch:12 step:11961 [D loss: 0.541455, acc.: 70.31%] [G loss: 0.625799]\n",
      "epoch:12 step:11962 [D loss: 0.600026, acc.: 67.97%] [G loss: 0.585382]\n",
      "epoch:12 step:11963 [D loss: 0.507960, acc.: 72.66%] [G loss: 0.610160]\n",
      "epoch:12 step:11964 [D loss: 0.645780, acc.: 64.84%] [G loss: 0.582423]\n",
      "epoch:12 step:11965 [D loss: 0.606193, acc.: 67.97%] [G loss: 0.524393]\n",
      "epoch:12 step:11966 [D loss: 0.568044, acc.: 68.75%] [G loss: 0.661094]\n",
      "epoch:12 step:11967 [D loss: 0.574391, acc.: 65.62%] [G loss: 0.508074]\n",
      "epoch:12 step:11968 [D loss: 0.502549, acc.: 77.34%] [G loss: 0.529324]\n",
      "epoch:12 step:11969 [D loss: 0.504254, acc.: 75.78%] [G loss: 0.533421]\n",
      "epoch:12 step:11970 [D loss: 0.608417, acc.: 71.88%] [G loss: 0.651911]\n",
      "epoch:12 step:11971 [D loss: 0.581405, acc.: 65.62%] [G loss: 0.625641]\n",
      "epoch:12 step:11972 [D loss: 0.567435, acc.: 67.19%] [G loss: 0.521739]\n",
      "epoch:12 step:11973 [D loss: 0.557872, acc.: 68.75%] [G loss: 0.537043]\n",
      "epoch:12 step:11974 [D loss: 0.534786, acc.: 75.78%] [G loss: 0.468984]\n",
      "epoch:12 step:11975 [D loss: 0.580849, acc.: 68.75%] [G loss: 0.500727]\n",
      "epoch:12 step:11976 [D loss: 0.545939, acc.: 73.44%] [G loss: 0.515772]\n",
      "epoch:12 step:11977 [D loss: 0.575742, acc.: 67.97%] [G loss: 0.550214]\n",
      "epoch:12 step:11978 [D loss: 0.550475, acc.: 68.75%] [G loss: 0.426271]\n",
      "epoch:12 step:11979 [D loss: 0.545247, acc.: 67.97%] [G loss: 0.534963]\n",
      "epoch:12 step:11980 [D loss: 0.473531, acc.: 76.56%] [G loss: 0.607877]\n",
      "epoch:12 step:11981 [D loss: 0.545624, acc.: 70.31%] [G loss: 0.694240]\n",
      "epoch:12 step:11982 [D loss: 0.575251, acc.: 66.41%] [G loss: 0.564041]\n",
      "epoch:12 step:11983 [D loss: 0.591797, acc.: 66.41%] [G loss: 0.582847]\n",
      "epoch:12 step:11984 [D loss: 0.698936, acc.: 53.91%] [G loss: 0.330989]\n",
      "epoch:12 step:11985 [D loss: 0.532710, acc.: 69.53%] [G loss: 0.525253]\n",
      "epoch:12 step:11986 [D loss: 0.588332, acc.: 70.31%] [G loss: 0.537798]\n",
      "epoch:12 step:11987 [D loss: 0.560608, acc.: 67.97%] [G loss: 0.644420]\n",
      "epoch:12 step:11988 [D loss: 0.511644, acc.: 77.34%] [G loss: 0.573936]\n",
      "epoch:12 step:11989 [D loss: 0.546966, acc.: 72.66%] [G loss: 0.541506]\n",
      "epoch:12 step:11990 [D loss: 0.448803, acc.: 77.34%] [G loss: 0.591174]\n",
      "epoch:12 step:11991 [D loss: 0.446379, acc.: 75.00%] [G loss: 0.703666]\n",
      "epoch:12 step:11992 [D loss: 0.584956, acc.: 67.19%] [G loss: 0.644054]\n",
      "epoch:12 step:11993 [D loss: 0.527666, acc.: 69.53%] [G loss: 0.698231]\n",
      "epoch:12 step:11994 [D loss: 0.477072, acc.: 78.12%] [G loss: 0.614115]\n",
      "epoch:12 step:11995 [D loss: 0.493281, acc.: 73.44%] [G loss: 0.682994]\n",
      "epoch:12 step:11996 [D loss: 0.650336, acc.: 68.75%] [G loss: 0.637715]\n",
      "epoch:12 step:11997 [D loss: 0.526788, acc.: 73.44%] [G loss: 0.784013]\n",
      "epoch:12 step:11998 [D loss: 0.536127, acc.: 69.53%] [G loss: 0.721041]\n",
      "epoch:12 step:11999 [D loss: 0.538101, acc.: 71.09%] [G loss: 0.615136]\n",
      "epoch:12 step:12000 [D loss: 0.572265, acc.: 68.75%] [G loss: 0.599898]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.617731\n",
      "FID: 39.344452\n",
      "0 = 12.693138782691962\n",
      "1 = 0.08771140581731214\n",
      "2 = 0.8806999921798706\n",
      "3 = 0.8101999759674072\n",
      "4 = 0.951200008392334\n",
      "5 = 0.9431897401809692\n",
      "6 = 0.8101999759674072\n",
      "7 = 7.766167348456386\n",
      "8 = 0.1292737865289979\n",
      "9 = 0.7181000113487244\n",
      "10 = 0.7075999975204468\n",
      "11 = 0.728600025177002\n",
      "12 = 0.7227783203125\n",
      "13 = 0.7075999975204468\n",
      "14 = 6.6177568435668945\n",
      "15 = 7.510308742523193\n",
      "16 = 0.34881672263145447\n",
      "17 = 6.617731094360352\n",
      "18 = 39.344451904296875\n",
      "epoch:12 step:12001 [D loss: 0.544301, acc.: 70.31%] [G loss: 0.484214]\n",
      "epoch:12 step:12002 [D loss: 0.580371, acc.: 70.31%] [G loss: 0.616508]\n",
      "epoch:12 step:12003 [D loss: 0.553204, acc.: 71.09%] [G loss: 0.512499]\n",
      "epoch:12 step:12004 [D loss: 0.545409, acc.: 68.75%] [G loss: 0.525874]\n",
      "epoch:12 step:12005 [D loss: 0.616146, acc.: 64.06%] [G loss: 0.482035]\n",
      "epoch:12 step:12006 [D loss: 0.596005, acc.: 68.75%] [G loss: 0.472898]\n",
      "epoch:12 step:12007 [D loss: 0.554706, acc.: 67.19%] [G loss: 0.440424]\n",
      "epoch:12 step:12008 [D loss: 0.543835, acc.: 68.75%] [G loss: 0.532254]\n",
      "epoch:12 step:12009 [D loss: 0.637617, acc.: 65.62%] [G loss: 0.493374]\n",
      "epoch:12 step:12010 [D loss: 0.657161, acc.: 61.72%] [G loss: 0.433456]\n",
      "epoch:12 step:12011 [D loss: 0.525504, acc.: 73.44%] [G loss: 0.405723]\n",
      "epoch:12 step:12012 [D loss: 0.509969, acc.: 75.78%] [G loss: 0.734177]\n",
      "epoch:12 step:12013 [D loss: 0.490495, acc.: 76.56%] [G loss: 0.692700]\n",
      "epoch:12 step:12014 [D loss: 0.532204, acc.: 69.53%] [G loss: 0.791084]\n",
      "epoch:12 step:12015 [D loss: 0.549161, acc.: 71.09%] [G loss: 0.724653]\n",
      "epoch:12 step:12016 [D loss: 0.522248, acc.: 74.22%] [G loss: 0.632193]\n",
      "epoch:12 step:12017 [D loss: 0.519550, acc.: 72.66%] [G loss: 0.590596]\n",
      "epoch:12 step:12018 [D loss: 0.509824, acc.: 76.56%] [G loss: 0.717011]\n",
      "epoch:12 step:12019 [D loss: 0.533377, acc.: 71.88%] [G loss: 0.740391]\n",
      "epoch:12 step:12020 [D loss: 0.579951, acc.: 70.31%] [G loss: 0.670037]\n",
      "epoch:12 step:12021 [D loss: 0.547565, acc.: 70.31%] [G loss: 0.579736]\n",
      "epoch:12 step:12022 [D loss: 0.548200, acc.: 73.44%] [G loss: 0.552307]\n",
      "epoch:12 step:12023 [D loss: 0.577850, acc.: 68.75%] [G loss: 0.469532]\n",
      "epoch:12 step:12024 [D loss: 0.619602, acc.: 65.62%] [G loss: 0.689370]\n",
      "epoch:12 step:12025 [D loss: 0.544446, acc.: 66.41%] [G loss: 0.739352]\n",
      "epoch:12 step:12026 [D loss: 0.515139, acc.: 70.31%] [G loss: 0.941319]\n",
      "epoch:12 step:12027 [D loss: 0.617036, acc.: 60.94%] [G loss: 0.783581]\n",
      "epoch:12 step:12028 [D loss: 0.623263, acc.: 64.84%] [G loss: 0.586855]\n",
      "epoch:12 step:12029 [D loss: 0.575733, acc.: 64.06%] [G loss: 0.453586]\n",
      "epoch:12 step:12030 [D loss: 0.547035, acc.: 71.09%] [G loss: 0.470790]\n",
      "epoch:12 step:12031 [D loss: 0.553034, acc.: 70.31%] [G loss: 0.503600]\n",
      "epoch:12 step:12032 [D loss: 0.614433, acc.: 59.38%] [G loss: 0.507506]\n",
      "epoch:12 step:12033 [D loss: 0.527239, acc.: 73.44%] [G loss: 0.492054]\n",
      "epoch:12 step:12034 [D loss: 0.488209, acc.: 79.69%] [G loss: 0.517929]\n",
      "epoch:12 step:12035 [D loss: 0.549228, acc.: 67.19%] [G loss: 0.513022]\n",
      "epoch:12 step:12036 [D loss: 0.511454, acc.: 77.34%] [G loss: 0.535015]\n",
      "epoch:12 step:12037 [D loss: 0.591155, acc.: 64.84%] [G loss: 0.572055]\n",
      "epoch:12 step:12038 [D loss: 0.671691, acc.: 53.12%] [G loss: 0.543366]\n",
      "epoch:12 step:12039 [D loss: 0.574674, acc.: 67.19%] [G loss: 0.449417]\n",
      "epoch:12 step:12040 [D loss: 0.556626, acc.: 65.62%] [G loss: 0.695614]\n",
      "epoch:12 step:12041 [D loss: 0.584476, acc.: 67.97%] [G loss: 0.504682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:12042 [D loss: 0.566330, acc.: 66.41%] [G loss: 0.498391]\n",
      "epoch:12 step:12043 [D loss: 0.586930, acc.: 64.84%] [G loss: 0.630301]\n",
      "epoch:12 step:12044 [D loss: 0.597316, acc.: 66.41%] [G loss: 0.450423]\n",
      "epoch:12 step:12045 [D loss: 0.529731, acc.: 71.88%] [G loss: 0.642270]\n",
      "epoch:12 step:12046 [D loss: 0.506425, acc.: 72.66%] [G loss: 0.675545]\n",
      "epoch:12 step:12047 [D loss: 0.505320, acc.: 77.34%] [G loss: 0.605015]\n",
      "epoch:12 step:12048 [D loss: 0.576108, acc.: 67.97%] [G loss: 0.439645]\n",
      "epoch:12 step:12049 [D loss: 0.499427, acc.: 71.09%] [G loss: 0.504950]\n",
      "epoch:12 step:12050 [D loss: 0.560126, acc.: 69.53%] [G loss: 0.437996]\n",
      "epoch:12 step:12051 [D loss: 0.516709, acc.: 72.66%] [G loss: 0.564664]\n",
      "epoch:12 step:12052 [D loss: 0.568245, acc.: 71.09%] [G loss: 0.516204]\n",
      "epoch:12 step:12053 [D loss: 0.554693, acc.: 67.97%] [G loss: 0.554208]\n",
      "epoch:12 step:12054 [D loss: 0.523364, acc.: 71.09%] [G loss: 0.576115]\n",
      "epoch:12 step:12055 [D loss: 0.589703, acc.: 67.19%] [G loss: 0.638238]\n",
      "epoch:12 step:12056 [D loss: 0.597541, acc.: 69.53%] [G loss: 0.668121]\n",
      "epoch:12 step:12057 [D loss: 0.549425, acc.: 65.62%] [G loss: 0.527155]\n",
      "epoch:12 step:12058 [D loss: 0.543976, acc.: 71.88%] [G loss: 0.855675]\n",
      "epoch:12 step:12059 [D loss: 0.578600, acc.: 71.88%] [G loss: 0.974349]\n",
      "epoch:12 step:12060 [D loss: 0.549762, acc.: 71.09%] [G loss: 0.689346]\n",
      "epoch:12 step:12061 [D loss: 0.612291, acc.: 65.62%] [G loss: 0.633836]\n",
      "epoch:12 step:12062 [D loss: 0.591564, acc.: 60.94%] [G loss: 0.533275]\n",
      "epoch:12 step:12063 [D loss: 0.503061, acc.: 78.91%] [G loss: 0.568424]\n",
      "epoch:12 step:12064 [D loss: 0.606790, acc.: 65.62%] [G loss: 0.527654]\n",
      "epoch:12 step:12065 [D loss: 0.535646, acc.: 70.31%] [G loss: 0.685960]\n",
      "epoch:12 step:12066 [D loss: 0.517789, acc.: 71.09%] [G loss: 0.568076]\n",
      "epoch:12 step:12067 [D loss: 0.483897, acc.: 80.47%] [G loss: 0.739424]\n",
      "epoch:12 step:12068 [D loss: 0.578543, acc.: 69.53%] [G loss: 0.514259]\n",
      "epoch:12 step:12069 [D loss: 0.511379, acc.: 75.78%] [G loss: 0.504171]\n",
      "epoch:12 step:12070 [D loss: 0.542240, acc.: 71.09%] [G loss: 0.536734]\n",
      "epoch:12 step:12071 [D loss: 0.612454, acc.: 64.84%] [G loss: 0.511980]\n",
      "epoch:12 step:12072 [D loss: 0.669511, acc.: 56.25%] [G loss: 0.464170]\n",
      "epoch:12 step:12073 [D loss: 0.548711, acc.: 67.97%] [G loss: 0.649315]\n",
      "epoch:12 step:12074 [D loss: 0.537653, acc.: 69.53%] [G loss: 0.721492]\n",
      "epoch:12 step:12075 [D loss: 0.599369, acc.: 64.84%] [G loss: 0.439713]\n",
      "epoch:12 step:12076 [D loss: 0.539516, acc.: 71.88%] [G loss: 0.570889]\n",
      "epoch:12 step:12077 [D loss: 0.511757, acc.: 73.44%] [G loss: 0.593650]\n",
      "epoch:12 step:12078 [D loss: 0.552293, acc.: 67.19%] [G loss: 0.465554]\n",
      "epoch:12 step:12079 [D loss: 0.512807, acc.: 71.09%] [G loss: 0.470407]\n",
      "epoch:12 step:12080 [D loss: 0.539003, acc.: 71.09%] [G loss: 0.543833]\n",
      "epoch:12 step:12081 [D loss: 0.522851, acc.: 72.66%] [G loss: 0.622952]\n",
      "epoch:12 step:12082 [D loss: 0.526897, acc.: 79.69%] [G loss: 0.487302]\n",
      "epoch:12 step:12083 [D loss: 0.556532, acc.: 71.88%] [G loss: 0.549317]\n",
      "epoch:12 step:12084 [D loss: 0.632131, acc.: 61.72%] [G loss: 0.429604]\n",
      "epoch:12 step:12085 [D loss: 0.541455, acc.: 66.41%] [G loss: 0.520073]\n",
      "epoch:12 step:12086 [D loss: 0.538453, acc.: 72.66%] [G loss: 0.567071]\n",
      "epoch:12 step:12087 [D loss: 0.501979, acc.: 71.88%] [G loss: 0.689960]\n",
      "epoch:12 step:12088 [D loss: 0.551848, acc.: 71.09%] [G loss: 0.631432]\n",
      "epoch:12 step:12089 [D loss: 0.569482, acc.: 67.97%] [G loss: 0.508124]\n",
      "epoch:12 step:12090 [D loss: 0.583103, acc.: 69.53%] [G loss: 0.558692]\n",
      "epoch:12 step:12091 [D loss: 0.577668, acc.: 69.53%] [G loss: 0.434324]\n",
      "epoch:12 step:12092 [D loss: 0.576263, acc.: 64.84%] [G loss: 0.495307]\n",
      "epoch:12 step:12093 [D loss: 0.587402, acc.: 64.84%] [G loss: 0.561119]\n",
      "epoch:12 step:12094 [D loss: 0.554848, acc.: 69.53%] [G loss: 0.529580]\n",
      "epoch:12 step:12095 [D loss: 0.575820, acc.: 66.41%] [G loss: 0.526968]\n",
      "epoch:12 step:12096 [D loss: 0.569066, acc.: 71.09%] [G loss: 0.406584]\n",
      "epoch:12 step:12097 [D loss: 0.556392, acc.: 66.41%] [G loss: 0.541710]\n",
      "epoch:12 step:12098 [D loss: 0.524940, acc.: 67.97%] [G loss: 0.550508]\n",
      "epoch:12 step:12099 [D loss: 0.542784, acc.: 70.31%] [G loss: 0.626497]\n",
      "epoch:12 step:12100 [D loss: 0.593873, acc.: 70.31%] [G loss: 0.456577]\n",
      "epoch:12 step:12101 [D loss: 0.436317, acc.: 79.69%] [G loss: 0.560061]\n",
      "epoch:12 step:12102 [D loss: 0.607794, acc.: 67.19%] [G loss: 0.457656]\n",
      "epoch:12 step:12103 [D loss: 0.534978, acc.: 71.09%] [G loss: 0.535145]\n",
      "epoch:12 step:12104 [D loss: 0.453770, acc.: 82.81%] [G loss: 0.761624]\n",
      "epoch:12 step:12105 [D loss: 0.657554, acc.: 57.03%] [G loss: 0.544047]\n",
      "epoch:12 step:12106 [D loss: 0.565479, acc.: 66.41%] [G loss: 0.349168]\n",
      "epoch:12 step:12107 [D loss: 0.586067, acc.: 66.41%] [G loss: 0.377914]\n",
      "epoch:12 step:12108 [D loss: 0.553558, acc.: 69.53%] [G loss: 0.460098]\n",
      "epoch:12 step:12109 [D loss: 0.573293, acc.: 70.31%] [G loss: 0.503525]\n",
      "epoch:12 step:12110 [D loss: 0.556256, acc.: 70.31%] [G loss: 0.379256]\n",
      "epoch:12 step:12111 [D loss: 0.646679, acc.: 60.16%] [G loss: 0.418620]\n",
      "epoch:12 step:12112 [D loss: 0.492808, acc.: 77.34%] [G loss: 0.470094]\n",
      "epoch:12 step:12113 [D loss: 0.535840, acc.: 67.97%] [G loss: 0.567563]\n",
      "epoch:12 step:12114 [D loss: 0.455871, acc.: 79.69%] [G loss: 0.575749]\n",
      "epoch:12 step:12115 [D loss: 0.517343, acc.: 75.00%] [G loss: 0.730903]\n",
      "epoch:12 step:12116 [D loss: 0.506366, acc.: 73.44%] [G loss: 0.881617]\n",
      "epoch:12 step:12117 [D loss: 0.651595, acc.: 63.28%] [G loss: 0.516507]\n",
      "epoch:12 step:12118 [D loss: 0.533445, acc.: 75.00%] [G loss: 0.520046]\n",
      "epoch:12 step:12119 [D loss: 0.524579, acc.: 75.00%] [G loss: 0.590259]\n",
      "epoch:12 step:12120 [D loss: 0.552466, acc.: 70.31%] [G loss: 0.592818]\n",
      "epoch:12 step:12121 [D loss: 0.609458, acc.: 61.72%] [G loss: 0.479685]\n",
      "epoch:12 step:12122 [D loss: 0.526374, acc.: 73.44%] [G loss: 0.544407]\n",
      "epoch:12 step:12123 [D loss: 0.579748, acc.: 70.31%] [G loss: 0.482501]\n",
      "epoch:12 step:12124 [D loss: 0.679079, acc.: 57.81%] [G loss: 0.412153]\n",
      "epoch:12 step:12125 [D loss: 0.563453, acc.: 67.19%] [G loss: 0.436047]\n",
      "epoch:12 step:12126 [D loss: 0.584532, acc.: 62.50%] [G loss: 0.473095]\n",
      "epoch:12 step:12127 [D loss: 0.562251, acc.: 71.09%] [G loss: 0.443204]\n",
      "epoch:12 step:12128 [D loss: 0.481835, acc.: 76.56%] [G loss: 0.629747]\n",
      "epoch:12 step:12129 [D loss: 0.553259, acc.: 66.41%] [G loss: 0.755183]\n",
      "epoch:12 step:12130 [D loss: 0.559886, acc.: 68.75%] [G loss: 0.755345]\n",
      "epoch:12 step:12131 [D loss: 0.568903, acc.: 72.66%] [G loss: 0.748782]\n",
      "epoch:12 step:12132 [D loss: 0.596382, acc.: 68.75%] [G loss: 0.585102]\n",
      "epoch:12 step:12133 [D loss: 0.555126, acc.: 71.09%] [G loss: 0.650780]\n",
      "epoch:12 step:12134 [D loss: 0.454330, acc.: 83.59%] [G loss: 0.597464]\n",
      "epoch:12 step:12135 [D loss: 0.602073, acc.: 66.41%] [G loss: 0.637282]\n",
      "epoch:12 step:12136 [D loss: 0.614591, acc.: 63.28%] [G loss: 0.527207]\n",
      "epoch:12 step:12137 [D loss: 0.511269, acc.: 75.78%] [G loss: 0.623811]\n",
      "epoch:12 step:12138 [D loss: 0.503987, acc.: 78.12%] [G loss: 0.624732]\n",
      "epoch:12 step:12139 [D loss: 0.536297, acc.: 71.09%] [G loss: 0.625937]\n",
      "epoch:12 step:12140 [D loss: 0.505896, acc.: 76.56%] [G loss: 0.751470]\n",
      "epoch:12 step:12141 [D loss: 0.546313, acc.: 70.31%] [G loss: 0.732076]\n",
      "epoch:12 step:12142 [D loss: 0.477317, acc.: 77.34%] [G loss: 0.681435]\n",
      "epoch:12 step:12143 [D loss: 0.507135, acc.: 76.56%] [G loss: 0.620974]\n",
      "epoch:12 step:12144 [D loss: 0.528095, acc.: 75.00%] [G loss: 0.657253]\n",
      "epoch:12 step:12145 [D loss: 0.519432, acc.: 71.09%] [G loss: 0.600720]\n",
      "epoch:12 step:12146 [D loss: 0.552269, acc.: 71.88%] [G loss: 0.505038]\n",
      "epoch:12 step:12147 [D loss: 0.539775, acc.: 72.66%] [G loss: 0.519318]\n",
      "epoch:12 step:12148 [D loss: 0.567056, acc.: 67.97%] [G loss: 0.561188]\n",
      "epoch:12 step:12149 [D loss: 0.550124, acc.: 73.44%] [G loss: 0.656963]\n",
      "epoch:12 step:12150 [D loss: 0.505782, acc.: 77.34%] [G loss: 0.624515]\n",
      "epoch:12 step:12151 [D loss: 0.543110, acc.: 68.75%] [G loss: 0.573694]\n",
      "epoch:12 step:12152 [D loss: 0.502811, acc.: 80.47%] [G loss: 0.612468]\n",
      "epoch:12 step:12153 [D loss: 0.485890, acc.: 75.00%] [G loss: 0.700911]\n",
      "epoch:12 step:12154 [D loss: 0.598913, acc.: 64.06%] [G loss: 0.469763]\n",
      "epoch:12 step:12155 [D loss: 0.509277, acc.: 75.00%] [G loss: 0.643142]\n",
      "epoch:12 step:12156 [D loss: 0.452971, acc.: 78.12%] [G loss: 0.702624]\n",
      "epoch:12 step:12157 [D loss: 0.565179, acc.: 72.66%] [G loss: 0.720115]\n",
      "epoch:12 step:12158 [D loss: 0.506490, acc.: 76.56%] [G loss: 0.812908]\n",
      "epoch:12 step:12159 [D loss: 0.624254, acc.: 60.16%] [G loss: 0.650862]\n",
      "epoch:12 step:12160 [D loss: 0.534008, acc.: 72.66%] [G loss: 0.577900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:12161 [D loss: 0.589077, acc.: 67.97%] [G loss: 0.520279]\n",
      "epoch:12 step:12162 [D loss: 0.487864, acc.: 79.69%] [G loss: 0.774769]\n",
      "epoch:12 step:12163 [D loss: 0.452660, acc.: 80.47%] [G loss: 0.720722]\n",
      "epoch:12 step:12164 [D loss: 0.726623, acc.: 56.25%] [G loss: 0.506828]\n",
      "epoch:12 step:12165 [D loss: 0.474587, acc.: 75.78%] [G loss: 0.822610]\n",
      "epoch:12 step:12166 [D loss: 0.576186, acc.: 68.75%] [G loss: 0.631448]\n",
      "epoch:12 step:12167 [D loss: 0.484168, acc.: 71.88%] [G loss: 0.776970]\n",
      "epoch:12 step:12168 [D loss: 0.427843, acc.: 81.25%] [G loss: 0.761021]\n",
      "epoch:12 step:12169 [D loss: 0.416446, acc.: 81.25%] [G loss: 0.889022]\n",
      "epoch:12 step:12170 [D loss: 0.446343, acc.: 77.34%] [G loss: 0.959526]\n",
      "epoch:12 step:12171 [D loss: 0.551814, acc.: 67.19%] [G loss: 1.205899]\n",
      "epoch:12 step:12172 [D loss: 0.718856, acc.: 66.41%] [G loss: 1.105595]\n",
      "epoch:12 step:12173 [D loss: 0.512949, acc.: 75.00%] [G loss: 1.363379]\n",
      "epoch:12 step:12174 [D loss: 0.470721, acc.: 78.12%] [G loss: 1.053435]\n",
      "epoch:12 step:12175 [D loss: 0.649965, acc.: 62.50%] [G loss: 0.993808]\n",
      "epoch:12 step:12176 [D loss: 0.637121, acc.: 66.41%] [G loss: 0.556361]\n",
      "epoch:12 step:12177 [D loss: 0.524324, acc.: 71.09%] [G loss: 0.827295]\n",
      "epoch:12 step:12178 [D loss: 0.503039, acc.: 71.88%] [G loss: 0.720304]\n",
      "epoch:12 step:12179 [D loss: 0.472743, acc.: 78.12%] [G loss: 0.776793]\n",
      "epoch:12 step:12180 [D loss: 0.357757, acc.: 80.47%] [G loss: 1.177716]\n",
      "epoch:12 step:12181 [D loss: 0.406907, acc.: 85.16%] [G loss: 0.936332]\n",
      "epoch:13 step:12182 [D loss: 0.547319, acc.: 75.00%] [G loss: 1.023211]\n",
      "epoch:13 step:12183 [D loss: 0.480115, acc.: 74.22%] [G loss: 1.004775]\n",
      "epoch:13 step:12184 [D loss: 0.537556, acc.: 72.66%] [G loss: 0.910054]\n",
      "epoch:13 step:12185 [D loss: 0.520294, acc.: 75.00%] [G loss: 0.711214]\n",
      "epoch:13 step:12186 [D loss: 0.571818, acc.: 71.88%] [G loss: 0.638584]\n",
      "epoch:13 step:12187 [D loss: 0.573917, acc.: 67.97%] [G loss: 0.504301]\n",
      "epoch:13 step:12188 [D loss: 0.519204, acc.: 72.66%] [G loss: 0.803014]\n",
      "epoch:13 step:12189 [D loss: 0.505813, acc.: 77.34%] [G loss: 0.710523]\n",
      "epoch:13 step:12190 [D loss: 0.492121, acc.: 78.12%] [G loss: 0.812325]\n",
      "epoch:13 step:12191 [D loss: 0.541347, acc.: 72.66%] [G loss: 0.700962]\n",
      "epoch:13 step:12192 [D loss: 0.437498, acc.: 84.38%] [G loss: 0.631370]\n",
      "epoch:13 step:12193 [D loss: 0.567282, acc.: 66.41%] [G loss: 0.569111]\n",
      "epoch:13 step:12194 [D loss: 0.612046, acc.: 60.16%] [G loss: 0.517529]\n",
      "epoch:13 step:12195 [D loss: 0.533076, acc.: 70.31%] [G loss: 0.597746]\n",
      "epoch:13 step:12196 [D loss: 0.490695, acc.: 71.88%] [G loss: 0.678064]\n",
      "epoch:13 step:12197 [D loss: 0.492142, acc.: 71.88%] [G loss: 0.835136]\n",
      "epoch:13 step:12198 [D loss: 0.555243, acc.: 75.00%] [G loss: 0.616701]\n",
      "epoch:13 step:12199 [D loss: 0.582920, acc.: 67.19%] [G loss: 0.587930]\n",
      "epoch:13 step:12200 [D loss: 0.573662, acc.: 70.31%] [G loss: 0.658762]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.514983\n",
      "FID: 43.382080\n",
      "0 = 13.222360755825036\n",
      "1 = 0.10290635946657792\n",
      "2 = 0.9222000241279602\n",
      "3 = 0.8759999871253967\n",
      "4 = 0.9684000015258789\n",
      "5 = 0.965182900428772\n",
      "6 = 0.8759999871253967\n",
      "7 = 7.939348855519314\n",
      "8 = 0.1344973339965252\n",
      "9 = 0.7228000164031982\n",
      "10 = 0.7045999765396118\n",
      "11 = 0.7409999966621399\n",
      "12 = 0.7312162518501282\n",
      "13 = 0.7045999765396118\n",
      "14 = 6.5150065422058105\n",
      "15 = 7.122820854187012\n",
      "16 = 0.37228143215179443\n",
      "17 = 6.514983177185059\n",
      "18 = 43.382080078125\n",
      "epoch:13 step:12201 [D loss: 0.602070, acc.: 72.66%] [G loss: 0.601034]\n",
      "epoch:13 step:12202 [D loss: 0.556606, acc.: 69.53%] [G loss: 0.701445]\n",
      "epoch:13 step:12203 [D loss: 0.485578, acc.: 74.22%] [G loss: 0.698440]\n",
      "epoch:13 step:12204 [D loss: 0.505798, acc.: 75.00%] [G loss: 0.696632]\n",
      "epoch:13 step:12205 [D loss: 0.503530, acc.: 72.66%] [G loss: 0.717326]\n",
      "epoch:13 step:12206 [D loss: 0.533183, acc.: 67.19%] [G loss: 0.588598]\n",
      "epoch:13 step:12207 [D loss: 0.590908, acc.: 60.94%] [G loss: 0.598782]\n",
      "epoch:13 step:12208 [D loss: 0.488981, acc.: 75.78%] [G loss: 0.629995]\n",
      "epoch:13 step:12209 [D loss: 0.559451, acc.: 68.75%] [G loss: 0.644471]\n",
      "epoch:13 step:12210 [D loss: 0.496589, acc.: 76.56%] [G loss: 0.639398]\n",
      "epoch:13 step:12211 [D loss: 0.547510, acc.: 69.53%] [G loss: 0.559682]\n",
      "epoch:13 step:12212 [D loss: 0.636565, acc.: 60.16%] [G loss: 0.520155]\n",
      "epoch:13 step:12213 [D loss: 0.589492, acc.: 67.97%] [G loss: 0.579180]\n",
      "epoch:13 step:12214 [D loss: 0.535254, acc.: 70.31%] [G loss: 0.612940]\n",
      "epoch:13 step:12215 [D loss: 0.509241, acc.: 72.66%] [G loss: 0.708137]\n",
      "epoch:13 step:12216 [D loss: 0.493735, acc.: 73.44%] [G loss: 0.651714]\n",
      "epoch:13 step:12217 [D loss: 0.519118, acc.: 78.12%] [G loss: 0.503892]\n",
      "epoch:13 step:12218 [D loss: 0.502900, acc.: 77.34%] [G loss: 0.651566]\n",
      "epoch:13 step:12219 [D loss: 0.633030, acc.: 64.06%] [G loss: 0.538693]\n",
      "epoch:13 step:12220 [D loss: 0.542867, acc.: 70.31%] [G loss: 0.526525]\n",
      "epoch:13 step:12221 [D loss: 0.448487, acc.: 81.25%] [G loss: 0.686271]\n",
      "epoch:13 step:12222 [D loss: 0.586673, acc.: 65.62%] [G loss: 0.745697]\n",
      "epoch:13 step:12223 [D loss: 0.534096, acc.: 75.00%] [G loss: 0.727046]\n",
      "epoch:13 step:12224 [D loss: 0.487851, acc.: 72.66%] [G loss: 0.751773]\n",
      "epoch:13 step:12225 [D loss: 0.567551, acc.: 67.97%] [G loss: 0.634472]\n",
      "epoch:13 step:12226 [D loss: 0.540435, acc.: 71.88%] [G loss: 0.661417]\n",
      "epoch:13 step:12227 [D loss: 0.509157, acc.: 72.66%] [G loss: 0.708846]\n",
      "epoch:13 step:12228 [D loss: 0.559474, acc.: 68.75%] [G loss: 0.580975]\n",
      "epoch:13 step:12229 [D loss: 0.482208, acc.: 79.69%] [G loss: 0.679366]\n",
      "epoch:13 step:12230 [D loss: 0.543947, acc.: 75.00%] [G loss: 0.772101]\n",
      "epoch:13 step:12231 [D loss: 0.540335, acc.: 71.09%] [G loss: 0.585457]\n",
      "epoch:13 step:12232 [D loss: 0.652678, acc.: 60.94%] [G loss: 0.467644]\n",
      "epoch:13 step:12233 [D loss: 0.626108, acc.: 61.72%] [G loss: 0.595127]\n",
      "epoch:13 step:12234 [D loss: 0.543321, acc.: 71.88%] [G loss: 0.567058]\n",
      "epoch:13 step:12235 [D loss: 0.470714, acc.: 80.47%] [G loss: 0.724798]\n",
      "epoch:13 step:12236 [D loss: 0.522880, acc.: 74.22%] [G loss: 0.876850]\n",
      "epoch:13 step:12237 [D loss: 0.608125, acc.: 67.97%] [G loss: 0.787720]\n",
      "epoch:13 step:12238 [D loss: 0.536925, acc.: 71.09%] [G loss: 0.537050]\n",
      "epoch:13 step:12239 [D loss: 0.592735, acc.: 70.31%] [G loss: 0.528200]\n",
      "epoch:13 step:12240 [D loss: 0.538687, acc.: 71.88%] [G loss: 0.618467]\n",
      "epoch:13 step:12241 [D loss: 0.560966, acc.: 64.84%] [G loss: 0.628915]\n",
      "epoch:13 step:12242 [D loss: 0.594870, acc.: 64.84%] [G loss: 0.734393]\n",
      "epoch:13 step:12243 [D loss: 0.600817, acc.: 68.75%] [G loss: 0.520338]\n",
      "epoch:13 step:12244 [D loss: 0.583220, acc.: 67.97%] [G loss: 0.475093]\n",
      "epoch:13 step:12245 [D loss: 0.565680, acc.: 68.75%] [G loss: 0.513421]\n",
      "epoch:13 step:12246 [D loss: 0.592667, acc.: 67.97%] [G loss: 0.494647]\n",
      "epoch:13 step:12247 [D loss: 0.583190, acc.: 66.41%] [G loss: 0.523026]\n",
      "epoch:13 step:12248 [D loss: 0.558870, acc.: 64.06%] [G loss: 0.470745]\n",
      "epoch:13 step:12249 [D loss: 0.541238, acc.: 71.88%] [G loss: 0.543541]\n",
      "epoch:13 step:12250 [D loss: 0.554420, acc.: 71.09%] [G loss: 0.453209]\n",
      "epoch:13 step:12251 [D loss: 0.539533, acc.: 71.09%] [G loss: 0.677423]\n",
      "epoch:13 step:12252 [D loss: 0.517810, acc.: 73.44%] [G loss: 0.599264]\n",
      "epoch:13 step:12253 [D loss: 0.499608, acc.: 72.66%] [G loss: 0.535866]\n",
      "epoch:13 step:12254 [D loss: 0.561126, acc.: 69.53%] [G loss: 0.461277]\n",
      "epoch:13 step:12255 [D loss: 0.562193, acc.: 73.44%] [G loss: 0.561172]\n",
      "epoch:13 step:12256 [D loss: 0.548902, acc.: 70.31%] [G loss: 0.520508]\n",
      "epoch:13 step:12257 [D loss: 0.545114, acc.: 70.31%] [G loss: 0.789107]\n",
      "epoch:13 step:12258 [D loss: 0.442588, acc.: 74.22%] [G loss: 0.703292]\n",
      "epoch:13 step:12259 [D loss: 0.608746, acc.: 64.84%] [G loss: 0.616395]\n",
      "epoch:13 step:12260 [D loss: 0.582348, acc.: 64.06%] [G loss: 0.421716]\n",
      "epoch:13 step:12261 [D loss: 0.496792, acc.: 76.56%] [G loss: 0.581582]\n",
      "epoch:13 step:12262 [D loss: 0.525408, acc.: 75.00%] [G loss: 0.615716]\n",
      "epoch:13 step:12263 [D loss: 0.551282, acc.: 70.31%] [G loss: 0.605879]\n",
      "epoch:13 step:12264 [D loss: 0.521100, acc.: 71.88%] [G loss: 0.544826]\n",
      "epoch:13 step:12265 [D loss: 0.520577, acc.: 75.00%] [G loss: 0.669287]\n",
      "epoch:13 step:12266 [D loss: 0.574028, acc.: 71.88%] [G loss: 0.565053]\n",
      "epoch:13 step:12267 [D loss: 0.532142, acc.: 71.88%] [G loss: 0.514287]\n",
      "epoch:13 step:12268 [D loss: 0.557481, acc.: 67.97%] [G loss: 0.597952]\n",
      "epoch:13 step:12269 [D loss: 0.528786, acc.: 75.78%] [G loss: 0.537163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12270 [D loss: 0.547224, acc.: 67.97%] [G loss: 0.549044]\n",
      "epoch:13 step:12271 [D loss: 0.473683, acc.: 77.34%] [G loss: 0.576787]\n",
      "epoch:13 step:12272 [D loss: 0.583774, acc.: 67.19%] [G loss: 0.534811]\n",
      "epoch:13 step:12273 [D loss: 0.501229, acc.: 73.44%] [G loss: 0.609368]\n",
      "epoch:13 step:12274 [D loss: 0.532803, acc.: 72.66%] [G loss: 0.629460]\n",
      "epoch:13 step:12275 [D loss: 0.487069, acc.: 72.66%] [G loss: 0.816011]\n",
      "epoch:13 step:12276 [D loss: 0.552252, acc.: 71.88%] [G loss: 0.652320]\n",
      "epoch:13 step:12277 [D loss: 0.481417, acc.: 77.34%] [G loss: 0.667440]\n",
      "epoch:13 step:12278 [D loss: 0.542626, acc.: 68.75%] [G loss: 0.591031]\n",
      "epoch:13 step:12279 [D loss: 0.513556, acc.: 74.22%] [G loss: 0.629315]\n",
      "epoch:13 step:12280 [D loss: 0.547904, acc.: 72.66%] [G loss: 0.677369]\n",
      "epoch:13 step:12281 [D loss: 0.469403, acc.: 76.56%] [G loss: 0.810463]\n",
      "epoch:13 step:12282 [D loss: 0.496958, acc.: 75.00%] [G loss: 0.748125]\n",
      "epoch:13 step:12283 [D loss: 0.606296, acc.: 67.97%] [G loss: 0.602227]\n",
      "epoch:13 step:12284 [D loss: 0.568039, acc.: 63.28%] [G loss: 0.522467]\n",
      "epoch:13 step:12285 [D loss: 0.549379, acc.: 67.97%] [G loss: 0.549000]\n",
      "epoch:13 step:12286 [D loss: 0.574181, acc.: 64.84%] [G loss: 0.544075]\n",
      "epoch:13 step:12287 [D loss: 0.527783, acc.: 71.09%] [G loss: 0.543020]\n",
      "epoch:13 step:12288 [D loss: 0.574314, acc.: 68.75%] [G loss: 0.726534]\n",
      "epoch:13 step:12289 [D loss: 0.620846, acc.: 67.19%] [G loss: 0.550962]\n",
      "epoch:13 step:12290 [D loss: 0.582324, acc.: 66.41%] [G loss: 0.463157]\n",
      "epoch:13 step:12291 [D loss: 0.580167, acc.: 69.53%] [G loss: 0.495743]\n",
      "epoch:13 step:12292 [D loss: 0.500667, acc.: 74.22%] [G loss: 0.617690]\n",
      "epoch:13 step:12293 [D loss: 0.554858, acc.: 71.88%] [G loss: 0.651189]\n",
      "epoch:13 step:12294 [D loss: 0.563690, acc.: 68.75%] [G loss: 0.628309]\n",
      "epoch:13 step:12295 [D loss: 0.602730, acc.: 70.31%] [G loss: 0.572196]\n",
      "epoch:13 step:12296 [D loss: 0.494539, acc.: 78.91%] [G loss: 0.609660]\n",
      "epoch:13 step:12297 [D loss: 0.563656, acc.: 66.41%] [G loss: 0.637515]\n",
      "epoch:13 step:12298 [D loss: 0.575884, acc.: 65.62%] [G loss: 0.669484]\n",
      "epoch:13 step:12299 [D loss: 0.506689, acc.: 77.34%] [G loss: 0.671218]\n",
      "epoch:13 step:12300 [D loss: 0.477229, acc.: 78.91%] [G loss: 0.755105]\n",
      "epoch:13 step:12301 [D loss: 0.572366, acc.: 67.19%] [G loss: 0.857890]\n",
      "epoch:13 step:12302 [D loss: 0.554739, acc.: 70.31%] [G loss: 0.605905]\n",
      "epoch:13 step:12303 [D loss: 0.488504, acc.: 75.78%] [G loss: 0.862041]\n",
      "epoch:13 step:12304 [D loss: 0.575002, acc.: 68.75%] [G loss: 0.754175]\n",
      "epoch:13 step:12305 [D loss: 0.588716, acc.: 70.31%] [G loss: 0.665721]\n",
      "epoch:13 step:12306 [D loss: 0.574481, acc.: 71.09%] [G loss: 0.598102]\n",
      "epoch:13 step:12307 [D loss: 0.499652, acc.: 71.09%] [G loss: 0.635689]\n",
      "epoch:13 step:12308 [D loss: 0.497556, acc.: 71.88%] [G loss: 0.616349]\n",
      "epoch:13 step:12309 [D loss: 0.507211, acc.: 73.44%] [G loss: 0.504780]\n",
      "epoch:13 step:12310 [D loss: 0.624259, acc.: 67.19%] [G loss: 0.459881]\n",
      "epoch:13 step:12311 [D loss: 0.559248, acc.: 67.97%] [G loss: 0.508618]\n",
      "epoch:13 step:12312 [D loss: 0.541331, acc.: 68.75%] [G loss: 0.631210]\n",
      "epoch:13 step:12313 [D loss: 0.549169, acc.: 68.75%] [G loss: 0.612293]\n",
      "epoch:13 step:12314 [D loss: 0.520383, acc.: 72.66%] [G loss: 0.645126]\n",
      "epoch:13 step:12315 [D loss: 0.563065, acc.: 68.75%] [G loss: 0.630390]\n",
      "epoch:13 step:12316 [D loss: 0.569543, acc.: 68.75%] [G loss: 0.600713]\n",
      "epoch:13 step:12317 [D loss: 0.623783, acc.: 65.62%] [G loss: 0.713459]\n",
      "epoch:13 step:12318 [D loss: 0.608040, acc.: 70.31%] [G loss: 0.685494]\n",
      "epoch:13 step:12319 [D loss: 0.602652, acc.: 61.72%] [G loss: 0.602234]\n",
      "epoch:13 step:12320 [D loss: 0.571963, acc.: 69.53%] [G loss: 0.589539]\n",
      "epoch:13 step:12321 [D loss: 0.599379, acc.: 65.62%] [G loss: 0.466187]\n",
      "epoch:13 step:12322 [D loss: 0.543264, acc.: 69.53%] [G loss: 0.523874]\n",
      "epoch:13 step:12323 [D loss: 0.553029, acc.: 67.97%] [G loss: 0.459992]\n",
      "epoch:13 step:12324 [D loss: 0.569324, acc.: 68.75%] [G loss: 0.621282]\n",
      "epoch:13 step:12325 [D loss: 0.536193, acc.: 67.97%] [G loss: 0.562004]\n",
      "epoch:13 step:12326 [D loss: 0.556605, acc.: 71.09%] [G loss: 0.586847]\n",
      "epoch:13 step:12327 [D loss: 0.494452, acc.: 77.34%] [G loss: 0.698232]\n",
      "epoch:13 step:12328 [D loss: 0.643870, acc.: 64.06%] [G loss: 0.562106]\n",
      "epoch:13 step:12329 [D loss: 0.580650, acc.: 67.97%] [G loss: 0.552836]\n",
      "epoch:13 step:12330 [D loss: 0.509362, acc.: 70.31%] [G loss: 0.504006]\n",
      "epoch:13 step:12331 [D loss: 0.576077, acc.: 67.97%] [G loss: 0.565279]\n",
      "epoch:13 step:12332 [D loss: 0.579309, acc.: 64.84%] [G loss: 0.504989]\n",
      "epoch:13 step:12333 [D loss: 0.530663, acc.: 74.22%] [G loss: 0.663471]\n",
      "epoch:13 step:12334 [D loss: 0.589489, acc.: 65.62%] [G loss: 0.582395]\n",
      "epoch:13 step:12335 [D loss: 0.544309, acc.: 72.66%] [G loss: 0.627880]\n",
      "epoch:13 step:12336 [D loss: 0.481684, acc.: 79.69%] [G loss: 0.716900]\n",
      "epoch:13 step:12337 [D loss: 0.525643, acc.: 72.66%] [G loss: 0.680751]\n",
      "epoch:13 step:12338 [D loss: 0.641533, acc.: 60.16%] [G loss: 0.491428]\n",
      "epoch:13 step:12339 [D loss: 0.570539, acc.: 70.31%] [G loss: 0.708700]\n",
      "epoch:13 step:12340 [D loss: 0.545210, acc.: 70.31%] [G loss: 0.627802]\n",
      "epoch:13 step:12341 [D loss: 0.661858, acc.: 59.38%] [G loss: 0.693748]\n",
      "epoch:13 step:12342 [D loss: 0.482739, acc.: 81.25%] [G loss: 0.717605]\n",
      "epoch:13 step:12343 [D loss: 0.500933, acc.: 75.78%] [G loss: 0.634820]\n",
      "epoch:13 step:12344 [D loss: 0.584949, acc.: 67.97%] [G loss: 0.542857]\n",
      "epoch:13 step:12345 [D loss: 0.532731, acc.: 75.00%] [G loss: 0.519270]\n",
      "epoch:13 step:12346 [D loss: 0.551196, acc.: 69.53%] [G loss: 0.490758]\n",
      "epoch:13 step:12347 [D loss: 0.569145, acc.: 66.41%] [G loss: 0.578065]\n",
      "epoch:13 step:12348 [D loss: 0.571430, acc.: 68.75%] [G loss: 0.582310]\n",
      "epoch:13 step:12349 [D loss: 0.547317, acc.: 69.53%] [G loss: 0.496913]\n",
      "epoch:13 step:12350 [D loss: 0.578016, acc.: 66.41%] [G loss: 0.402921]\n",
      "epoch:13 step:12351 [D loss: 0.565352, acc.: 66.41%] [G loss: 0.434271]\n",
      "epoch:13 step:12352 [D loss: 0.515406, acc.: 70.31%] [G loss: 0.535980]\n",
      "epoch:13 step:12353 [D loss: 0.520359, acc.: 72.66%] [G loss: 0.652822]\n",
      "epoch:13 step:12354 [D loss: 0.514968, acc.: 73.44%] [G loss: 0.576549]\n",
      "epoch:13 step:12355 [D loss: 0.596345, acc.: 64.84%] [G loss: 0.615007]\n",
      "epoch:13 step:12356 [D loss: 0.552331, acc.: 68.75%] [G loss: 0.605338]\n",
      "epoch:13 step:12357 [D loss: 0.509275, acc.: 78.12%] [G loss: 0.651735]\n",
      "epoch:13 step:12358 [D loss: 0.555802, acc.: 68.75%] [G loss: 0.593739]\n",
      "epoch:13 step:12359 [D loss: 0.570009, acc.: 67.97%] [G loss: 0.482513]\n",
      "epoch:13 step:12360 [D loss: 0.549739, acc.: 70.31%] [G loss: 0.499977]\n",
      "epoch:13 step:12361 [D loss: 0.620349, acc.: 64.84%] [G loss: 0.422285]\n",
      "epoch:13 step:12362 [D loss: 0.586606, acc.: 68.75%] [G loss: 0.524473]\n",
      "epoch:13 step:12363 [D loss: 0.603026, acc.: 67.19%] [G loss: 0.454213]\n",
      "epoch:13 step:12364 [D loss: 0.575458, acc.: 67.19%] [G loss: 0.511087]\n",
      "epoch:13 step:12365 [D loss: 0.504552, acc.: 76.56%] [G loss: 0.562808]\n",
      "epoch:13 step:12366 [D loss: 0.600155, acc.: 66.41%] [G loss: 0.600316]\n",
      "epoch:13 step:12367 [D loss: 0.564352, acc.: 66.41%] [G loss: 0.487565]\n",
      "epoch:13 step:12368 [D loss: 0.642086, acc.: 64.84%] [G loss: 0.623489]\n",
      "epoch:13 step:12369 [D loss: 0.526598, acc.: 70.31%] [G loss: 0.464129]\n",
      "epoch:13 step:12370 [D loss: 0.605564, acc.: 64.06%] [G loss: 0.563543]\n",
      "epoch:13 step:12371 [D loss: 0.546808, acc.: 71.88%] [G loss: 0.502438]\n",
      "epoch:13 step:12372 [D loss: 0.522070, acc.: 74.22%] [G loss: 0.627071]\n",
      "epoch:13 step:12373 [D loss: 0.505536, acc.: 71.88%] [G loss: 0.600003]\n",
      "epoch:13 step:12374 [D loss: 0.523978, acc.: 71.88%] [G loss: 0.569015]\n",
      "epoch:13 step:12375 [D loss: 0.477834, acc.: 77.34%] [G loss: 0.718948]\n",
      "epoch:13 step:12376 [D loss: 0.563758, acc.: 70.31%] [G loss: 0.660303]\n",
      "epoch:13 step:12377 [D loss: 0.510836, acc.: 69.53%] [G loss: 0.575742]\n",
      "epoch:13 step:12378 [D loss: 0.521758, acc.: 72.66%] [G loss: 0.510730]\n",
      "epoch:13 step:12379 [D loss: 0.494343, acc.: 73.44%] [G loss: 0.647086]\n",
      "epoch:13 step:12380 [D loss: 0.549164, acc.: 68.75%] [G loss: 0.680041]\n",
      "epoch:13 step:12381 [D loss: 0.629569, acc.: 67.19%] [G loss: 0.577130]\n",
      "epoch:13 step:12382 [D loss: 0.605422, acc.: 67.97%] [G loss: 0.547201]\n",
      "epoch:13 step:12383 [D loss: 0.533955, acc.: 71.88%] [G loss: 0.592768]\n",
      "epoch:13 step:12384 [D loss: 0.603244, acc.: 67.97%] [G loss: 0.581446]\n",
      "epoch:13 step:12385 [D loss: 0.558022, acc.: 72.66%] [G loss: 0.516552]\n",
      "epoch:13 step:12386 [D loss: 0.564021, acc.: 73.44%] [G loss: 0.646166]\n",
      "epoch:13 step:12387 [D loss: 0.506783, acc.: 73.44%] [G loss: 0.752241]\n",
      "epoch:13 step:12388 [D loss: 0.459188, acc.: 79.69%] [G loss: 0.632644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12389 [D loss: 0.448735, acc.: 78.12%] [G loss: 0.842280]\n",
      "epoch:13 step:12390 [D loss: 0.516895, acc.: 73.44%] [G loss: 0.783317]\n",
      "epoch:13 step:12391 [D loss: 0.651445, acc.: 64.06%] [G loss: 0.567835]\n",
      "epoch:13 step:12392 [D loss: 0.610202, acc.: 63.28%] [G loss: 0.463076]\n",
      "epoch:13 step:12393 [D loss: 0.602078, acc.: 69.53%] [G loss: 0.557959]\n",
      "epoch:13 step:12394 [D loss: 0.540271, acc.: 69.53%] [G loss: 0.515811]\n",
      "epoch:13 step:12395 [D loss: 0.589675, acc.: 64.84%] [G loss: 0.575513]\n",
      "epoch:13 step:12396 [D loss: 0.581079, acc.: 71.09%] [G loss: 0.578271]\n",
      "epoch:13 step:12397 [D loss: 0.542255, acc.: 65.62%] [G loss: 0.552671]\n",
      "epoch:13 step:12398 [D loss: 0.591429, acc.: 69.53%] [G loss: 0.566009]\n",
      "epoch:13 step:12399 [D loss: 0.556337, acc.: 72.66%] [G loss: 0.747848]\n",
      "epoch:13 step:12400 [D loss: 0.486323, acc.: 78.91%] [G loss: 0.746992]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.410313\n",
      "FID: 51.328178\n",
      "0 = 13.000046106433864\n",
      "1 = 0.09078158301392503\n",
      "2 = 0.8935999870300293\n",
      "3 = 0.8306000232696533\n",
      "4 = 0.95660001039505\n",
      "5 = 0.9503432512283325\n",
      "6 = 0.8306000232696533\n",
      "7 = 8.422520693409442\n",
      "8 = 0.15904895117712797\n",
      "9 = 0.7416999936103821\n",
      "10 = 0.7337999939918518\n",
      "11 = 0.7495999932289124\n",
      "12 = 0.7455801963806152\n",
      "13 = 0.7337999939918518\n",
      "14 = 6.410339832305908\n",
      "15 = 7.180539608001709\n",
      "16 = 0.3748692572116852\n",
      "17 = 6.410313129425049\n",
      "18 = 51.32817840576172\n",
      "epoch:13 step:12401 [D loss: 0.602763, acc.: 62.50%] [G loss: 0.558476]\n",
      "epoch:13 step:12402 [D loss: 0.568841, acc.: 66.41%] [G loss: 0.525431]\n",
      "epoch:13 step:12403 [D loss: 0.466959, acc.: 77.34%] [G loss: 0.678035]\n",
      "epoch:13 step:12404 [D loss: 0.577184, acc.: 64.84%] [G loss: 0.665562]\n",
      "epoch:13 step:12405 [D loss: 0.541567, acc.: 72.66%] [G loss: 0.672110]\n",
      "epoch:13 step:12406 [D loss: 0.545448, acc.: 74.22%] [G loss: 0.626635]\n",
      "epoch:13 step:12407 [D loss: 0.582555, acc.: 66.41%] [G loss: 0.583838]\n",
      "epoch:13 step:12408 [D loss: 0.546711, acc.: 70.31%] [G loss: 0.475298]\n",
      "epoch:13 step:12409 [D loss: 0.641383, acc.: 61.72%] [G loss: 0.506269]\n",
      "epoch:13 step:12410 [D loss: 0.514843, acc.: 75.00%] [G loss: 0.527720]\n",
      "epoch:13 step:12411 [D loss: 0.562685, acc.: 71.88%] [G loss: 0.694447]\n",
      "epoch:13 step:12412 [D loss: 0.473008, acc.: 78.12%] [G loss: 0.683161]\n",
      "epoch:13 step:12413 [D loss: 0.488950, acc.: 75.78%] [G loss: 0.741663]\n",
      "epoch:13 step:12414 [D loss: 0.551478, acc.: 69.53%] [G loss: 0.616477]\n",
      "epoch:13 step:12415 [D loss: 0.555696, acc.: 68.75%] [G loss: 0.594282]\n",
      "epoch:13 step:12416 [D loss: 0.566208, acc.: 67.97%] [G loss: 0.468067]\n",
      "epoch:13 step:12417 [D loss: 0.572916, acc.: 69.53%] [G loss: 0.481624]\n",
      "epoch:13 step:12418 [D loss: 0.567725, acc.: 67.97%] [G loss: 0.506109]\n",
      "epoch:13 step:12419 [D loss: 0.558754, acc.: 66.41%] [G loss: 0.608151]\n",
      "epoch:13 step:12420 [D loss: 0.519862, acc.: 71.88%] [G loss: 0.553206]\n",
      "epoch:13 step:12421 [D loss: 0.576192, acc.: 68.75%] [G loss: 0.494348]\n",
      "epoch:13 step:12422 [D loss: 0.573554, acc.: 67.19%] [G loss: 0.516211]\n",
      "epoch:13 step:12423 [D loss: 0.527509, acc.: 71.09%] [G loss: 0.630430]\n",
      "epoch:13 step:12424 [D loss: 0.581901, acc.: 68.75%] [G loss: 0.595420]\n",
      "epoch:13 step:12425 [D loss: 0.449146, acc.: 74.22%] [G loss: 0.596481]\n",
      "epoch:13 step:12426 [D loss: 0.546191, acc.: 73.44%] [G loss: 0.651696]\n",
      "epoch:13 step:12427 [D loss: 0.587105, acc.: 64.06%] [G loss: 0.703561]\n",
      "epoch:13 step:12428 [D loss: 0.537216, acc.: 68.75%] [G loss: 0.585200]\n",
      "epoch:13 step:12429 [D loss: 0.522689, acc.: 70.31%] [G loss: 0.679230]\n",
      "epoch:13 step:12430 [D loss: 0.591515, acc.: 70.31%] [G loss: 0.693964]\n",
      "epoch:13 step:12431 [D loss: 0.585614, acc.: 66.41%] [G loss: 0.573699]\n",
      "epoch:13 step:12432 [D loss: 0.657846, acc.: 62.50%] [G loss: 0.490766]\n",
      "epoch:13 step:12433 [D loss: 0.544299, acc.: 72.66%] [G loss: 0.585491]\n",
      "epoch:13 step:12434 [D loss: 0.561556, acc.: 73.44%] [G loss: 0.482227]\n",
      "epoch:13 step:12435 [D loss: 0.499307, acc.: 78.12%] [G loss: 0.569194]\n",
      "epoch:13 step:12436 [D loss: 0.547311, acc.: 70.31%] [G loss: 0.431892]\n",
      "epoch:13 step:12437 [D loss: 0.528577, acc.: 71.88%] [G loss: 0.505305]\n",
      "epoch:13 step:12438 [D loss: 0.570881, acc.: 67.19%] [G loss: 0.524760]\n",
      "epoch:13 step:12439 [D loss: 0.573375, acc.: 62.50%] [G loss: 0.573753]\n",
      "epoch:13 step:12440 [D loss: 0.550791, acc.: 72.66%] [G loss: 0.532568]\n",
      "epoch:13 step:12441 [D loss: 0.602691, acc.: 62.50%] [G loss: 0.409637]\n",
      "epoch:13 step:12442 [D loss: 0.561372, acc.: 70.31%] [G loss: 0.407898]\n",
      "epoch:13 step:12443 [D loss: 0.566037, acc.: 71.09%] [G loss: 0.552757]\n",
      "epoch:13 step:12444 [D loss: 0.598049, acc.: 70.31%] [G loss: 0.606787]\n",
      "epoch:13 step:12445 [D loss: 0.565785, acc.: 69.53%] [G loss: 0.476483]\n",
      "epoch:13 step:12446 [D loss: 0.566924, acc.: 64.84%] [G loss: 0.485380]\n",
      "epoch:13 step:12447 [D loss: 0.538172, acc.: 70.31%] [G loss: 0.627507]\n",
      "epoch:13 step:12448 [D loss: 0.547828, acc.: 67.97%] [G loss: 0.503599]\n",
      "epoch:13 step:12449 [D loss: 0.595763, acc.: 61.72%] [G loss: 0.404636]\n",
      "epoch:13 step:12450 [D loss: 0.538232, acc.: 75.78%] [G loss: 0.525974]\n",
      "epoch:13 step:12451 [D loss: 0.489574, acc.: 76.56%] [G loss: 0.521021]\n",
      "epoch:13 step:12452 [D loss: 0.495418, acc.: 75.00%] [G loss: 0.551086]\n",
      "epoch:13 step:12453 [D loss: 0.558652, acc.: 67.97%] [G loss: 0.595840]\n",
      "epoch:13 step:12454 [D loss: 0.537218, acc.: 72.66%] [G loss: 0.524876]\n",
      "epoch:13 step:12455 [D loss: 0.513398, acc.: 75.78%] [G loss: 0.650775]\n",
      "epoch:13 step:12456 [D loss: 0.585940, acc.: 67.97%] [G loss: 0.600184]\n",
      "epoch:13 step:12457 [D loss: 0.497078, acc.: 76.56%] [G loss: 0.569198]\n",
      "epoch:13 step:12458 [D loss: 0.720346, acc.: 59.38%] [G loss: 0.479673]\n",
      "epoch:13 step:12459 [D loss: 0.650243, acc.: 60.94%] [G loss: 0.484745]\n",
      "epoch:13 step:12460 [D loss: 0.550897, acc.: 67.97%] [G loss: 0.490426]\n",
      "epoch:13 step:12461 [D loss: 0.565679, acc.: 66.41%] [G loss: 0.518710]\n",
      "epoch:13 step:12462 [D loss: 0.603260, acc.: 63.28%] [G loss: 0.474623]\n",
      "epoch:13 step:12463 [D loss: 0.601093, acc.: 70.31%] [G loss: 0.491046]\n",
      "epoch:13 step:12464 [D loss: 0.541938, acc.: 77.34%] [G loss: 0.561809]\n",
      "epoch:13 step:12465 [D loss: 0.562415, acc.: 70.31%] [G loss: 0.559646]\n",
      "epoch:13 step:12466 [D loss: 0.537432, acc.: 71.09%] [G loss: 0.510693]\n",
      "epoch:13 step:12467 [D loss: 0.459058, acc.: 81.25%] [G loss: 0.730961]\n",
      "epoch:13 step:12468 [D loss: 0.554342, acc.: 68.75%] [G loss: 0.492135]\n",
      "epoch:13 step:12469 [D loss: 0.577986, acc.: 64.06%] [G loss: 0.593867]\n",
      "epoch:13 step:12470 [D loss: 0.566267, acc.: 63.28%] [G loss: 0.631076]\n",
      "epoch:13 step:12471 [D loss: 0.609513, acc.: 65.62%] [G loss: 0.630619]\n",
      "epoch:13 step:12472 [D loss: 0.587318, acc.: 69.53%] [G loss: 0.683947]\n",
      "epoch:13 step:12473 [D loss: 0.568320, acc.: 71.88%] [G loss: 0.607207]\n",
      "epoch:13 step:12474 [D loss: 0.564907, acc.: 67.97%] [G loss: 0.460510]\n",
      "epoch:13 step:12475 [D loss: 0.571951, acc.: 67.19%] [G loss: 0.472647]\n",
      "epoch:13 step:12476 [D loss: 0.537958, acc.: 70.31%] [G loss: 0.582491]\n",
      "epoch:13 step:12477 [D loss: 0.504776, acc.: 75.78%] [G loss: 0.529685]\n",
      "epoch:13 step:12478 [D loss: 0.579181, acc.: 64.84%] [G loss: 0.515252]\n",
      "epoch:13 step:12479 [D loss: 0.479809, acc.: 76.56%] [G loss: 0.809996]\n",
      "epoch:13 step:12480 [D loss: 0.513906, acc.: 75.00%] [G loss: 0.730836]\n",
      "epoch:13 step:12481 [D loss: 0.478723, acc.: 78.12%] [G loss: 0.695619]\n",
      "epoch:13 step:12482 [D loss: 0.656193, acc.: 63.28%] [G loss: 0.574973]\n",
      "epoch:13 step:12483 [D loss: 0.543814, acc.: 74.22%] [G loss: 0.572660]\n",
      "epoch:13 step:12484 [D loss: 0.545684, acc.: 75.00%] [G loss: 0.622267]\n",
      "epoch:13 step:12485 [D loss: 0.483513, acc.: 77.34%] [G loss: 0.633898]\n",
      "epoch:13 step:12486 [D loss: 0.532597, acc.: 73.44%] [G loss: 0.566009]\n",
      "epoch:13 step:12487 [D loss: 0.533015, acc.: 68.75%] [G loss: 0.573389]\n",
      "epoch:13 step:12488 [D loss: 0.494227, acc.: 79.69%] [G loss: 0.562811]\n",
      "epoch:13 step:12489 [D loss: 0.566940, acc.: 64.84%] [G loss: 0.512655]\n",
      "epoch:13 step:12490 [D loss: 0.496651, acc.: 70.31%] [G loss: 0.640105]\n",
      "epoch:13 step:12491 [D loss: 0.560299, acc.: 69.53%] [G loss: 0.677797]\n",
      "epoch:13 step:12492 [D loss: 0.492691, acc.: 75.00%] [G loss: 0.611068]\n",
      "epoch:13 step:12493 [D loss: 0.491930, acc.: 77.34%] [G loss: 0.839994]\n",
      "epoch:13 step:12494 [D loss: 0.529819, acc.: 70.31%] [G loss: 0.672761]\n",
      "epoch:13 step:12495 [D loss: 0.481931, acc.: 74.22%] [G loss: 0.767984]\n",
      "epoch:13 step:12496 [D loss: 0.475266, acc.: 77.34%] [G loss: 0.772912]\n",
      "epoch:13 step:12497 [D loss: 0.692058, acc.: 62.50%] [G loss: 0.748188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12498 [D loss: 0.631221, acc.: 64.84%] [G loss: 0.520833]\n",
      "epoch:13 step:12499 [D loss: 0.533209, acc.: 73.44%] [G loss: 0.457492]\n",
      "epoch:13 step:12500 [D loss: 0.554530, acc.: 70.31%] [G loss: 0.520381]\n",
      "epoch:13 step:12501 [D loss: 0.596644, acc.: 62.50%] [G loss: 0.450160]\n",
      "epoch:13 step:12502 [D loss: 0.477964, acc.: 80.47%] [G loss: 0.647679]\n",
      "epoch:13 step:12503 [D loss: 0.552254, acc.: 71.09%] [G loss: 0.497119]\n",
      "epoch:13 step:12504 [D loss: 0.628708, acc.: 64.06%] [G loss: 0.441678]\n",
      "epoch:13 step:12505 [D loss: 0.604782, acc.: 67.97%] [G loss: 0.454262]\n",
      "epoch:13 step:12506 [D loss: 0.542494, acc.: 67.97%] [G loss: 0.442270]\n",
      "epoch:13 step:12507 [D loss: 0.483817, acc.: 74.22%] [G loss: 0.695025]\n",
      "epoch:13 step:12508 [D loss: 0.567750, acc.: 68.75%] [G loss: 0.504825]\n",
      "epoch:13 step:12509 [D loss: 0.521935, acc.: 72.66%] [G loss: 0.661642]\n",
      "epoch:13 step:12510 [D loss: 0.550385, acc.: 69.53%] [G loss: 0.519334]\n",
      "epoch:13 step:12511 [D loss: 0.580015, acc.: 64.84%] [G loss: 0.559959]\n",
      "epoch:13 step:12512 [D loss: 0.562857, acc.: 65.62%] [G loss: 0.589907]\n",
      "epoch:13 step:12513 [D loss: 0.551188, acc.: 69.53%] [G loss: 0.471015]\n",
      "epoch:13 step:12514 [D loss: 0.489895, acc.: 74.22%] [G loss: 0.579738]\n",
      "epoch:13 step:12515 [D loss: 0.551494, acc.: 65.62%] [G loss: 0.622703]\n",
      "epoch:13 step:12516 [D loss: 0.508283, acc.: 75.00%] [G loss: 0.765926]\n",
      "epoch:13 step:12517 [D loss: 0.517460, acc.: 76.56%] [G loss: 0.786845]\n",
      "epoch:13 step:12518 [D loss: 0.510826, acc.: 73.44%] [G loss: 0.875862]\n",
      "epoch:13 step:12519 [D loss: 0.561709, acc.: 71.09%] [G loss: 0.617517]\n",
      "epoch:13 step:12520 [D loss: 0.513588, acc.: 71.09%] [G loss: 0.777963]\n",
      "epoch:13 step:12521 [D loss: 0.518523, acc.: 75.00%] [G loss: 0.653953]\n",
      "epoch:13 step:12522 [D loss: 0.659801, acc.: 64.84%] [G loss: 0.606773]\n",
      "epoch:13 step:12523 [D loss: 0.635179, acc.: 58.59%] [G loss: 0.606682]\n",
      "epoch:13 step:12524 [D loss: 0.514989, acc.: 74.22%] [G loss: 0.602744]\n",
      "epoch:13 step:12525 [D loss: 0.464142, acc.: 78.91%] [G loss: 0.775595]\n",
      "epoch:13 step:12526 [D loss: 0.619956, acc.: 64.84%] [G loss: 0.742863]\n",
      "epoch:13 step:12527 [D loss: 0.511466, acc.: 75.78%] [G loss: 0.841304]\n",
      "epoch:13 step:12528 [D loss: 0.450643, acc.: 80.47%] [G loss: 0.855741]\n",
      "epoch:13 step:12529 [D loss: 0.634351, acc.: 64.84%] [G loss: 0.676828]\n",
      "epoch:13 step:12530 [D loss: 0.733672, acc.: 54.69%] [G loss: 0.370802]\n",
      "epoch:13 step:12531 [D loss: 0.512206, acc.: 75.78%] [G loss: 0.514096]\n",
      "epoch:13 step:12532 [D loss: 0.526540, acc.: 72.66%] [G loss: 0.720938]\n",
      "epoch:13 step:12533 [D loss: 0.511475, acc.: 72.66%] [G loss: 0.570825]\n",
      "epoch:13 step:12534 [D loss: 0.570478, acc.: 68.75%] [G loss: 0.580191]\n",
      "epoch:13 step:12535 [D loss: 0.397262, acc.: 83.59%] [G loss: 0.799397]\n",
      "epoch:13 step:12536 [D loss: 0.553155, acc.: 71.88%] [G loss: 0.688932]\n",
      "epoch:13 step:12537 [D loss: 0.541198, acc.: 73.44%] [G loss: 0.613606]\n",
      "epoch:13 step:12538 [D loss: 0.480484, acc.: 75.00%] [G loss: 0.709978]\n",
      "epoch:13 step:12539 [D loss: 0.499588, acc.: 78.12%] [G loss: 0.799774]\n",
      "epoch:13 step:12540 [D loss: 0.436079, acc.: 82.03%] [G loss: 0.927704]\n",
      "epoch:13 step:12541 [D loss: 0.523407, acc.: 71.09%] [G loss: 0.709928]\n",
      "epoch:13 step:12542 [D loss: 0.542336, acc.: 72.66%] [G loss: 0.776569]\n",
      "epoch:13 step:12543 [D loss: 0.547490, acc.: 74.22%] [G loss: 0.610900]\n",
      "epoch:13 step:12544 [D loss: 0.593376, acc.: 64.06%] [G loss: 0.470407]\n",
      "epoch:13 step:12545 [D loss: 0.502617, acc.: 72.66%] [G loss: 0.662936]\n",
      "epoch:13 step:12546 [D loss: 0.528128, acc.: 70.31%] [G loss: 0.696346]\n",
      "epoch:13 step:12547 [D loss: 0.591020, acc.: 71.88%] [G loss: 0.549284]\n",
      "epoch:13 step:12548 [D loss: 0.570439, acc.: 71.09%] [G loss: 0.499244]\n",
      "epoch:13 step:12549 [D loss: 0.593363, acc.: 67.19%] [G loss: 0.603272]\n",
      "epoch:13 step:12550 [D loss: 0.562725, acc.: 64.06%] [G loss: 0.579698]\n",
      "epoch:13 step:12551 [D loss: 0.548450, acc.: 73.44%] [G loss: 0.655195]\n",
      "epoch:13 step:12552 [D loss: 0.498516, acc.: 75.78%] [G loss: 0.644161]\n",
      "epoch:13 step:12553 [D loss: 0.559107, acc.: 68.75%] [G loss: 0.720630]\n",
      "epoch:13 step:12554 [D loss: 0.539210, acc.: 76.56%] [G loss: 0.716628]\n",
      "epoch:13 step:12555 [D loss: 0.459491, acc.: 75.00%] [G loss: 0.553359]\n",
      "epoch:13 step:12556 [D loss: 0.585290, acc.: 68.75%] [G loss: 0.559864]\n",
      "epoch:13 step:12557 [D loss: 0.669538, acc.: 60.94%] [G loss: 0.472204]\n",
      "epoch:13 step:12558 [D loss: 0.610221, acc.: 64.06%] [G loss: 0.539046]\n",
      "epoch:13 step:12559 [D loss: 0.540078, acc.: 71.88%] [G loss: 0.535556]\n",
      "epoch:13 step:12560 [D loss: 0.574500, acc.: 65.62%] [G loss: 0.475753]\n",
      "epoch:13 step:12561 [D loss: 0.597987, acc.: 69.53%] [G loss: 0.493279]\n",
      "epoch:13 step:12562 [D loss: 0.498475, acc.: 77.34%] [G loss: 0.564947]\n",
      "epoch:13 step:12563 [D loss: 0.470631, acc.: 79.69%] [G loss: 0.625219]\n",
      "epoch:13 step:12564 [D loss: 0.595305, acc.: 66.41%] [G loss: 0.485675]\n",
      "epoch:13 step:12565 [D loss: 0.531604, acc.: 74.22%] [G loss: 0.543977]\n",
      "epoch:13 step:12566 [D loss: 0.524486, acc.: 75.78%] [G loss: 0.749879]\n",
      "epoch:13 step:12567 [D loss: 0.586077, acc.: 64.06%] [G loss: 0.588071]\n",
      "epoch:13 step:12568 [D loss: 0.611942, acc.: 62.50%] [G loss: 0.451080]\n",
      "epoch:13 step:12569 [D loss: 0.539029, acc.: 71.88%] [G loss: 0.593278]\n",
      "epoch:13 step:12570 [D loss: 0.562319, acc.: 68.75%] [G loss: 0.641551]\n",
      "epoch:13 step:12571 [D loss: 0.592488, acc.: 60.94%] [G loss: 0.514187]\n",
      "epoch:13 step:12572 [D loss: 0.524892, acc.: 70.31%] [G loss: 0.660019]\n",
      "epoch:13 step:12573 [D loss: 0.499336, acc.: 73.44%] [G loss: 0.676898]\n",
      "epoch:13 step:12574 [D loss: 0.664436, acc.: 57.81%] [G loss: 0.566155]\n",
      "epoch:13 step:12575 [D loss: 0.570806, acc.: 67.19%] [G loss: 0.491411]\n",
      "epoch:13 step:12576 [D loss: 0.521795, acc.: 73.44%] [G loss: 0.520545]\n",
      "epoch:13 step:12577 [D loss: 0.554920, acc.: 67.97%] [G loss: 0.648521]\n",
      "epoch:13 step:12578 [D loss: 0.555069, acc.: 68.75%] [G loss: 0.456946]\n",
      "epoch:13 step:12579 [D loss: 0.504450, acc.: 72.66%] [G loss: 0.656212]\n",
      "epoch:13 step:12580 [D loss: 0.572586, acc.: 71.88%] [G loss: 0.557625]\n",
      "epoch:13 step:12581 [D loss: 0.664555, acc.: 59.38%] [G loss: 0.533206]\n",
      "epoch:13 step:12582 [D loss: 0.688999, acc.: 52.34%] [G loss: 0.530078]\n",
      "epoch:13 step:12583 [D loss: 0.486257, acc.: 75.00%] [G loss: 0.757456]\n",
      "epoch:13 step:12584 [D loss: 0.549473, acc.: 68.75%] [G loss: 0.717016]\n",
      "epoch:13 step:12585 [D loss: 0.615975, acc.: 61.72%] [G loss: 0.640168]\n",
      "epoch:13 step:12586 [D loss: 0.521363, acc.: 71.88%] [G loss: 0.615721]\n",
      "epoch:13 step:12587 [D loss: 0.484912, acc.: 74.22%] [G loss: 0.660693]\n",
      "epoch:13 step:12588 [D loss: 0.614133, acc.: 62.50%] [G loss: 0.562366]\n",
      "epoch:13 step:12589 [D loss: 0.607778, acc.: 67.19%] [G loss: 0.525084]\n",
      "epoch:13 step:12590 [D loss: 0.533824, acc.: 68.75%] [G loss: 0.620417]\n",
      "epoch:13 step:12591 [D loss: 0.597091, acc.: 64.06%] [G loss: 0.567481]\n",
      "epoch:13 step:12592 [D loss: 0.521520, acc.: 75.78%] [G loss: 0.425478]\n",
      "epoch:13 step:12593 [D loss: 0.621964, acc.: 65.62%] [G loss: 0.430588]\n",
      "epoch:13 step:12594 [D loss: 0.541910, acc.: 69.53%] [G loss: 0.436499]\n",
      "epoch:13 step:12595 [D loss: 0.569501, acc.: 70.31%] [G loss: 0.474541]\n",
      "epoch:13 step:12596 [D loss: 0.563907, acc.: 68.75%] [G loss: 0.573283]\n",
      "epoch:13 step:12597 [D loss: 0.568910, acc.: 64.06%] [G loss: 0.699200]\n",
      "epoch:13 step:12598 [D loss: 0.565606, acc.: 69.53%] [G loss: 0.686756]\n",
      "epoch:13 step:12599 [D loss: 0.674514, acc.: 59.38%] [G loss: 0.594036]\n",
      "epoch:13 step:12600 [D loss: 0.565455, acc.: 71.09%] [G loss: 0.688566]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.475203\n",
      "FID: 45.280319\n",
      "0 = 12.750864654064193\n",
      "1 = 0.08136222131950696\n",
      "2 = 0.8848999738693237\n",
      "3 = 0.8217999935150146\n",
      "4 = 0.9480000138282776\n",
      "5 = 0.9404898285865784\n",
      "6 = 0.8217999935150146\n",
      "7 = 8.081881768870357\n",
      "8 = 0.14012523431273444\n",
      "9 = 0.7207000255584717\n",
      "10 = 0.7134000062942505\n",
      "11 = 0.7279999852180481\n",
      "12 = 0.7239699363708496\n",
      "13 = 0.7134000062942505\n",
      "14 = 6.475226402282715\n",
      "15 = 7.209246635437012\n",
      "16 = 0.3732687830924988\n",
      "17 = 6.475203037261963\n",
      "18 = 45.28031921386719\n",
      "epoch:13 step:12601 [D loss: 0.620984, acc.: 60.94%] [G loss: 0.576745]\n",
      "epoch:13 step:12602 [D loss: 0.611324, acc.: 65.62%] [G loss: 0.570076]\n",
      "epoch:13 step:12603 [D loss: 0.605999, acc.: 67.97%] [G loss: 0.584553]\n",
      "epoch:13 step:12604 [D loss: 0.579514, acc.: 64.84%] [G loss: 0.504391]\n",
      "epoch:13 step:12605 [D loss: 0.592719, acc.: 66.41%] [G loss: 0.505629]\n",
      "epoch:13 step:12606 [D loss: 0.555167, acc.: 67.97%] [G loss: 0.493358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12607 [D loss: 0.473708, acc.: 75.00%] [G loss: 0.643579]\n",
      "epoch:13 step:12608 [D loss: 0.460765, acc.: 78.91%] [G loss: 0.758016]\n",
      "epoch:13 step:12609 [D loss: 0.576254, acc.: 68.75%] [G loss: 0.667700]\n",
      "epoch:13 step:12610 [D loss: 0.454369, acc.: 79.69%] [G loss: 0.821968]\n",
      "epoch:13 step:12611 [D loss: 0.482884, acc.: 74.22%] [G loss: 0.858276]\n",
      "epoch:13 step:12612 [D loss: 0.586536, acc.: 69.53%] [G loss: 0.758413]\n",
      "epoch:13 step:12613 [D loss: 0.532126, acc.: 69.53%] [G loss: 0.652433]\n",
      "epoch:13 step:12614 [D loss: 0.622607, acc.: 61.72%] [G loss: 0.609364]\n",
      "epoch:13 step:12615 [D loss: 0.527618, acc.: 71.09%] [G loss: 0.607097]\n",
      "epoch:13 step:12616 [D loss: 0.583251, acc.: 69.53%] [G loss: 0.608718]\n",
      "epoch:13 step:12617 [D loss: 0.486223, acc.: 77.34%] [G loss: 0.586518]\n",
      "epoch:13 step:12618 [D loss: 0.634842, acc.: 62.50%] [G loss: 0.613343]\n",
      "epoch:13 step:12619 [D loss: 0.561580, acc.: 65.62%] [G loss: 0.491364]\n",
      "epoch:13 step:12620 [D loss: 0.562424, acc.: 71.09%] [G loss: 0.543714]\n",
      "epoch:13 step:12621 [D loss: 0.472935, acc.: 77.34%] [G loss: 0.605454]\n",
      "epoch:13 step:12622 [D loss: 0.548016, acc.: 70.31%] [G loss: 0.725166]\n",
      "epoch:13 step:12623 [D loss: 0.525401, acc.: 75.00%] [G loss: 0.790752]\n",
      "epoch:13 step:12624 [D loss: 0.532973, acc.: 71.09%] [G loss: 0.594320]\n",
      "epoch:13 step:12625 [D loss: 0.493658, acc.: 75.00%] [G loss: 0.704862]\n",
      "epoch:13 step:12626 [D loss: 0.561414, acc.: 64.06%] [G loss: 0.582958]\n",
      "epoch:13 step:12627 [D loss: 0.481637, acc.: 77.34%] [G loss: 0.711964]\n",
      "epoch:13 step:12628 [D loss: 0.507396, acc.: 71.88%] [G loss: 0.808977]\n",
      "epoch:13 step:12629 [D loss: 0.545600, acc.: 72.66%] [G loss: 0.622559]\n",
      "epoch:13 step:12630 [D loss: 0.514673, acc.: 76.56%] [G loss: 0.572060]\n",
      "epoch:13 step:12631 [D loss: 0.548384, acc.: 73.44%] [G loss: 0.591192]\n",
      "epoch:13 step:12632 [D loss: 0.456801, acc.: 79.69%] [G loss: 0.730970]\n",
      "epoch:13 step:12633 [D loss: 0.467088, acc.: 79.69%] [G loss: 0.855920]\n",
      "epoch:13 step:12634 [D loss: 0.501603, acc.: 74.22%] [G loss: 0.863463]\n",
      "epoch:13 step:12635 [D loss: 0.576240, acc.: 69.53%] [G loss: 0.574121]\n",
      "epoch:13 step:12636 [D loss: 0.565376, acc.: 70.31%] [G loss: 0.615974]\n",
      "epoch:13 step:12637 [D loss: 0.667243, acc.: 60.94%] [G loss: 0.639587]\n",
      "epoch:13 step:12638 [D loss: 0.544258, acc.: 71.88%] [G loss: 0.604380]\n",
      "epoch:13 step:12639 [D loss: 0.600899, acc.: 67.19%] [G loss: 0.637805]\n",
      "epoch:13 step:12640 [D loss: 0.557676, acc.: 67.97%] [G loss: 0.597626]\n",
      "epoch:13 step:12641 [D loss: 0.484492, acc.: 77.34%] [G loss: 0.691705]\n",
      "epoch:13 step:12642 [D loss: 0.545938, acc.: 72.66%] [G loss: 0.666566]\n",
      "epoch:13 step:12643 [D loss: 0.574121, acc.: 64.84%] [G loss: 0.509301]\n",
      "epoch:13 step:12644 [D loss: 0.574484, acc.: 65.62%] [G loss: 0.544853]\n",
      "epoch:13 step:12645 [D loss: 0.505270, acc.: 75.78%] [G loss: 0.593042]\n",
      "epoch:13 step:12646 [D loss: 0.672243, acc.: 62.50%] [G loss: 0.417786]\n",
      "epoch:13 step:12647 [D loss: 0.613558, acc.: 64.84%] [G loss: 0.470142]\n",
      "epoch:13 step:12648 [D loss: 0.526145, acc.: 73.44%] [G loss: 0.566672]\n",
      "epoch:13 step:12649 [D loss: 0.514269, acc.: 76.56%] [G loss: 0.567747]\n",
      "epoch:13 step:12650 [D loss: 0.543470, acc.: 71.09%] [G loss: 0.558520]\n",
      "epoch:13 step:12651 [D loss: 0.555628, acc.: 69.53%] [G loss: 0.610706]\n",
      "epoch:13 step:12652 [D loss: 0.417483, acc.: 82.03%] [G loss: 0.947520]\n",
      "epoch:13 step:12653 [D loss: 0.451537, acc.: 77.34%] [G loss: 0.894487]\n",
      "epoch:13 step:12654 [D loss: 0.636800, acc.: 60.16%] [G loss: 0.695523]\n",
      "epoch:13 step:12655 [D loss: 0.565800, acc.: 60.94%] [G loss: 0.611319]\n",
      "epoch:13 step:12656 [D loss: 0.489718, acc.: 78.91%] [G loss: 0.785746]\n",
      "epoch:13 step:12657 [D loss: 0.618975, acc.: 67.19%] [G loss: 0.624571]\n",
      "epoch:13 step:12658 [D loss: 0.709593, acc.: 63.28%] [G loss: 0.387140]\n",
      "epoch:13 step:12659 [D loss: 0.554547, acc.: 74.22%] [G loss: 0.388773]\n",
      "epoch:13 step:12660 [D loss: 0.569957, acc.: 72.66%] [G loss: 0.427941]\n",
      "epoch:13 step:12661 [D loss: 0.573124, acc.: 68.75%] [G loss: 0.533101]\n",
      "epoch:13 step:12662 [D loss: 0.522770, acc.: 77.34%] [G loss: 0.604995]\n",
      "epoch:13 step:12663 [D loss: 0.618451, acc.: 68.75%] [G loss: 0.566784]\n",
      "epoch:13 step:12664 [D loss: 0.547631, acc.: 70.31%] [G loss: 0.519034]\n",
      "epoch:13 step:12665 [D loss: 0.496147, acc.: 76.56%] [G loss: 0.718983]\n",
      "epoch:13 step:12666 [D loss: 0.543826, acc.: 70.31%] [G loss: 0.648722]\n",
      "epoch:13 step:12667 [D loss: 0.592638, acc.: 71.09%] [G loss: 0.542246]\n",
      "epoch:13 step:12668 [D loss: 0.542933, acc.: 69.53%] [G loss: 0.582995]\n",
      "epoch:13 step:12669 [D loss: 0.522580, acc.: 69.53%] [G loss: 0.442051]\n",
      "epoch:13 step:12670 [D loss: 0.596900, acc.: 67.97%] [G loss: 0.579180]\n",
      "epoch:13 step:12671 [D loss: 0.524643, acc.: 73.44%] [G loss: 0.648679]\n",
      "epoch:13 step:12672 [D loss: 0.537827, acc.: 69.53%] [G loss: 0.596055]\n",
      "epoch:13 step:12673 [D loss: 0.575663, acc.: 67.97%] [G loss: 0.529336]\n",
      "epoch:13 step:12674 [D loss: 0.529753, acc.: 74.22%] [G loss: 0.525535]\n",
      "epoch:13 step:12675 [D loss: 0.652750, acc.: 64.84%] [G loss: 0.483224]\n",
      "epoch:13 step:12676 [D loss: 0.516760, acc.: 71.88%] [G loss: 0.529384]\n",
      "epoch:13 step:12677 [D loss: 0.568893, acc.: 71.09%] [G loss: 0.568842]\n",
      "epoch:13 step:12678 [D loss: 0.602494, acc.: 66.41%] [G loss: 0.534482]\n",
      "epoch:13 step:12679 [D loss: 0.540255, acc.: 70.31%] [G loss: 0.578049]\n",
      "epoch:13 step:12680 [D loss: 0.481282, acc.: 72.66%] [G loss: 0.843112]\n",
      "epoch:13 step:12681 [D loss: 0.635043, acc.: 64.84%] [G loss: 0.631854]\n",
      "epoch:13 step:12682 [D loss: 0.630424, acc.: 63.28%] [G loss: 0.497832]\n",
      "epoch:13 step:12683 [D loss: 0.628665, acc.: 61.72%] [G loss: 0.517586]\n",
      "epoch:13 step:12684 [D loss: 0.480463, acc.: 75.78%] [G loss: 0.494801]\n",
      "epoch:13 step:12685 [D loss: 0.476404, acc.: 77.34%] [G loss: 0.673452]\n",
      "epoch:13 step:12686 [D loss: 0.506076, acc.: 75.00%] [G loss: 0.647520]\n",
      "epoch:13 step:12687 [D loss: 0.603270, acc.: 67.19%] [G loss: 0.662348]\n",
      "epoch:13 step:12688 [D loss: 0.531631, acc.: 74.22%] [G loss: 0.577391]\n",
      "epoch:13 step:12689 [D loss: 0.443646, acc.: 82.03%] [G loss: 0.805674]\n",
      "epoch:13 step:12690 [D loss: 0.522715, acc.: 74.22%] [G loss: 0.701988]\n",
      "epoch:13 step:12691 [D loss: 0.636975, acc.: 62.50%] [G loss: 0.586012]\n",
      "epoch:13 step:12692 [D loss: 0.707775, acc.: 56.25%] [G loss: 0.449281]\n",
      "epoch:13 step:12693 [D loss: 0.575318, acc.: 64.06%] [G loss: 0.410513]\n",
      "epoch:13 step:12694 [D loss: 0.526215, acc.: 75.00%] [G loss: 0.583641]\n",
      "epoch:13 step:12695 [D loss: 0.584191, acc.: 68.75%] [G loss: 0.506101]\n",
      "epoch:13 step:12696 [D loss: 0.531840, acc.: 71.09%] [G loss: 0.639559]\n",
      "epoch:13 step:12697 [D loss: 0.479592, acc.: 78.91%] [G loss: 0.565283]\n",
      "epoch:13 step:12698 [D loss: 0.462275, acc.: 79.69%] [G loss: 0.695109]\n",
      "epoch:13 step:12699 [D loss: 0.554735, acc.: 71.88%] [G loss: 0.632174]\n",
      "epoch:13 step:12700 [D loss: 0.484353, acc.: 75.78%] [G loss: 0.520039]\n",
      "epoch:13 step:12701 [D loss: 0.545620, acc.: 71.09%] [G loss: 0.644375]\n",
      "epoch:13 step:12702 [D loss: 0.583490, acc.: 73.44%] [G loss: 0.518316]\n",
      "epoch:13 step:12703 [D loss: 0.537415, acc.: 73.44%] [G loss: 0.769289]\n",
      "epoch:13 step:12704 [D loss: 0.501701, acc.: 75.78%] [G loss: 0.625477]\n",
      "epoch:13 step:12705 [D loss: 0.560607, acc.: 68.75%] [G loss: 0.673771]\n",
      "epoch:13 step:12706 [D loss: 0.623848, acc.: 64.84%] [G loss: 0.590710]\n",
      "epoch:13 step:12707 [D loss: 0.523220, acc.: 71.09%] [G loss: 0.537046]\n",
      "epoch:13 step:12708 [D loss: 0.587356, acc.: 64.06%] [G loss: 0.597721]\n",
      "epoch:13 step:12709 [D loss: 0.732902, acc.: 47.66%] [G loss: 0.554349]\n",
      "epoch:13 step:12710 [D loss: 0.632329, acc.: 57.81%] [G loss: 0.417092]\n",
      "epoch:13 step:12711 [D loss: 0.543814, acc.: 70.31%] [G loss: 0.576967]\n",
      "epoch:13 step:12712 [D loss: 0.585833, acc.: 68.75%] [G loss: 0.611681]\n",
      "epoch:13 step:12713 [D loss: 0.596147, acc.: 71.88%] [G loss: 0.447203]\n",
      "epoch:13 step:12714 [D loss: 0.500361, acc.: 72.66%] [G loss: 0.687912]\n",
      "epoch:13 step:12715 [D loss: 0.566269, acc.: 70.31%] [G loss: 0.670642]\n",
      "epoch:13 step:12716 [D loss: 0.606464, acc.: 60.94%] [G loss: 0.547112]\n",
      "epoch:13 step:12717 [D loss: 0.512355, acc.: 70.31%] [G loss: 0.557971]\n",
      "epoch:13 step:12718 [D loss: 0.543137, acc.: 69.53%] [G loss: 0.537671]\n",
      "epoch:13 step:12719 [D loss: 0.589591, acc.: 62.50%] [G loss: 0.610875]\n",
      "epoch:13 step:12720 [D loss: 0.534525, acc.: 74.22%] [G loss: 0.597796]\n",
      "epoch:13 step:12721 [D loss: 0.589813, acc.: 63.28%] [G loss: 0.496223]\n",
      "epoch:13 step:12722 [D loss: 0.584749, acc.: 68.75%] [G loss: 0.633816]\n",
      "epoch:13 step:12723 [D loss: 0.599550, acc.: 67.97%] [G loss: 0.484054]\n",
      "epoch:13 step:12724 [D loss: 0.590748, acc.: 70.31%] [G loss: 0.543800]\n",
      "epoch:13 step:12725 [D loss: 0.535519, acc.: 70.31%] [G loss: 0.498733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12726 [D loss: 0.597842, acc.: 68.75%] [G loss: 0.530474]\n",
      "epoch:13 step:12727 [D loss: 0.531087, acc.: 72.66%] [G loss: 0.637998]\n",
      "epoch:13 step:12728 [D loss: 0.566123, acc.: 71.88%] [G loss: 0.654553]\n",
      "epoch:13 step:12729 [D loss: 0.518149, acc.: 72.66%] [G loss: 0.591661]\n",
      "epoch:13 step:12730 [D loss: 0.578287, acc.: 70.31%] [G loss: 0.528674]\n",
      "epoch:13 step:12731 [D loss: 0.555093, acc.: 71.09%] [G loss: 0.664296]\n",
      "epoch:13 step:12732 [D loss: 0.522466, acc.: 73.44%] [G loss: 0.627233]\n",
      "epoch:13 step:12733 [D loss: 0.486864, acc.: 76.56%] [G loss: 0.561492]\n",
      "epoch:13 step:12734 [D loss: 0.585505, acc.: 69.53%] [G loss: 0.637161]\n",
      "epoch:13 step:12735 [D loss: 0.449116, acc.: 78.91%] [G loss: 0.610061]\n",
      "epoch:13 step:12736 [D loss: 0.482206, acc.: 79.69%] [G loss: 0.856287]\n",
      "epoch:13 step:12737 [D loss: 0.510594, acc.: 75.00%] [G loss: 0.654079]\n",
      "epoch:13 step:12738 [D loss: 0.491401, acc.: 75.78%] [G loss: 0.647502]\n",
      "epoch:13 step:12739 [D loss: 0.493293, acc.: 75.00%] [G loss: 0.662803]\n",
      "epoch:13 step:12740 [D loss: 0.582731, acc.: 66.41%] [G loss: 0.672073]\n",
      "epoch:13 step:12741 [D loss: 0.583562, acc.: 64.06%] [G loss: 0.552271]\n",
      "epoch:13 step:12742 [D loss: 0.580369, acc.: 67.19%] [G loss: 0.543562]\n",
      "epoch:13 step:12743 [D loss: 0.568982, acc.: 64.84%] [G loss: 0.547410]\n",
      "epoch:13 step:12744 [D loss: 0.539304, acc.: 71.09%] [G loss: 0.638329]\n",
      "epoch:13 step:12745 [D loss: 0.488954, acc.: 74.22%] [G loss: 0.619415]\n",
      "epoch:13 step:12746 [D loss: 0.571819, acc.: 70.31%] [G loss: 0.684767]\n",
      "epoch:13 step:12747 [D loss: 0.682533, acc.: 60.16%] [G loss: 0.563107]\n",
      "epoch:13 step:12748 [D loss: 0.523836, acc.: 74.22%] [G loss: 0.770536]\n",
      "epoch:13 step:12749 [D loss: 0.508887, acc.: 77.34%] [G loss: 0.631069]\n",
      "epoch:13 step:12750 [D loss: 0.594399, acc.: 64.84%] [G loss: 0.487316]\n",
      "epoch:13 step:12751 [D loss: 0.516827, acc.: 75.78%] [G loss: 0.628426]\n",
      "epoch:13 step:12752 [D loss: 0.545896, acc.: 69.53%] [G loss: 0.522427]\n",
      "epoch:13 step:12753 [D loss: 0.585697, acc.: 66.41%] [G loss: 0.558202]\n",
      "epoch:13 step:12754 [D loss: 0.591991, acc.: 68.75%] [G loss: 0.656742]\n",
      "epoch:13 step:12755 [D loss: 0.521645, acc.: 71.88%] [G loss: 0.604714]\n",
      "epoch:13 step:12756 [D loss: 0.489385, acc.: 75.00%] [G loss: 0.650941]\n",
      "epoch:13 step:12757 [D loss: 0.633651, acc.: 64.06%] [G loss: 0.560547]\n",
      "epoch:13 step:12758 [D loss: 0.553549, acc.: 71.88%] [G loss: 0.623337]\n",
      "epoch:13 step:12759 [D loss: 0.541807, acc.: 71.09%] [G loss: 0.548551]\n",
      "epoch:13 step:12760 [D loss: 0.559921, acc.: 67.19%] [G loss: 0.574766]\n",
      "epoch:13 step:12761 [D loss: 0.588608, acc.: 63.28%] [G loss: 0.697453]\n",
      "epoch:13 step:12762 [D loss: 0.557873, acc.: 67.19%] [G loss: 0.648671]\n",
      "epoch:13 step:12763 [D loss: 0.498273, acc.: 75.78%] [G loss: 0.701783]\n",
      "epoch:13 step:12764 [D loss: 0.560470, acc.: 67.19%] [G loss: 0.604273]\n",
      "epoch:13 step:12765 [D loss: 0.652340, acc.: 57.03%] [G loss: 0.519244]\n",
      "epoch:13 step:12766 [D loss: 0.524537, acc.: 72.66%] [G loss: 0.524649]\n",
      "epoch:13 step:12767 [D loss: 0.579892, acc.: 67.19%] [G loss: 0.579188]\n",
      "epoch:13 step:12768 [D loss: 0.580974, acc.: 64.06%] [G loss: 0.584532]\n",
      "epoch:13 step:12769 [D loss: 0.542442, acc.: 69.53%] [G loss: 0.636839]\n",
      "epoch:13 step:12770 [D loss: 0.489216, acc.: 71.88%] [G loss: 0.844511]\n",
      "epoch:13 step:12771 [D loss: 0.608615, acc.: 58.59%] [G loss: 0.524338]\n",
      "epoch:13 step:12772 [D loss: 0.544436, acc.: 75.00%] [G loss: 0.507305]\n",
      "epoch:13 step:12773 [D loss: 0.550218, acc.: 67.19%] [G loss: 0.563582]\n",
      "epoch:13 step:12774 [D loss: 0.486030, acc.: 77.34%] [G loss: 0.635769]\n",
      "epoch:13 step:12775 [D loss: 0.574612, acc.: 67.19%] [G loss: 0.603110]\n",
      "epoch:13 step:12776 [D loss: 0.556067, acc.: 71.88%] [G loss: 0.517312]\n",
      "epoch:13 step:12777 [D loss: 0.559020, acc.: 68.75%] [G loss: 0.662543]\n",
      "epoch:13 step:12778 [D loss: 0.592296, acc.: 67.97%] [G loss: 0.576577]\n",
      "epoch:13 step:12779 [D loss: 0.548250, acc.: 71.88%] [G loss: 0.767531]\n",
      "epoch:13 step:12780 [D loss: 0.585813, acc.: 64.84%] [G loss: 0.665126]\n",
      "epoch:13 step:12781 [D loss: 0.559593, acc.: 70.31%] [G loss: 0.476315]\n",
      "epoch:13 step:12782 [D loss: 0.559056, acc.: 69.53%] [G loss: 0.634049]\n",
      "epoch:13 step:12783 [D loss: 0.502284, acc.: 72.66%] [G loss: 0.682618]\n",
      "epoch:13 step:12784 [D loss: 0.503900, acc.: 69.53%] [G loss: 0.588147]\n",
      "epoch:13 step:12785 [D loss: 0.561492, acc.: 73.44%] [G loss: 0.639491]\n",
      "epoch:13 step:12786 [D loss: 0.513694, acc.: 75.00%] [G loss: 0.542425]\n",
      "epoch:13 step:12787 [D loss: 0.611540, acc.: 64.84%] [G loss: 0.585584]\n",
      "epoch:13 step:12788 [D loss: 0.516297, acc.: 73.44%] [G loss: 0.656710]\n",
      "epoch:13 step:12789 [D loss: 0.548509, acc.: 66.41%] [G loss: 0.580736]\n",
      "epoch:13 step:12790 [D loss: 0.577896, acc.: 62.50%] [G loss: 0.522096]\n",
      "epoch:13 step:12791 [D loss: 0.588591, acc.: 64.06%] [G loss: 0.443548]\n",
      "epoch:13 step:12792 [D loss: 0.498464, acc.: 75.00%] [G loss: 0.504342]\n",
      "epoch:13 step:12793 [D loss: 0.566137, acc.: 66.41%] [G loss: 0.419604]\n",
      "epoch:13 step:12794 [D loss: 0.539920, acc.: 70.31%] [G loss: 0.516342]\n",
      "epoch:13 step:12795 [D loss: 0.599443, acc.: 62.50%] [G loss: 0.516840]\n",
      "epoch:13 step:12796 [D loss: 0.580269, acc.: 68.75%] [G loss: 0.519076]\n",
      "epoch:13 step:12797 [D loss: 0.568002, acc.: 65.62%] [G loss: 0.767461]\n",
      "epoch:13 step:12798 [D loss: 0.530700, acc.: 71.88%] [G loss: 0.739178]\n",
      "epoch:13 step:12799 [D loss: 0.562524, acc.: 70.31%] [G loss: 0.608697]\n",
      "epoch:13 step:12800 [D loss: 0.569252, acc.: 70.31%] [G loss: 0.464786]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.628599\n",
      "FID: 38.492409\n",
      "0 = 12.908330480670921\n",
      "1 = 0.08902064896116679\n",
      "2 = 0.8867999911308289\n",
      "3 = 0.8230000138282776\n",
      "4 = 0.9506000280380249\n",
      "5 = 0.9433745741844177\n",
      "6 = 0.8230000138282776\n",
      "7 = 7.789961862087255\n",
      "8 = 0.12906244236388317\n",
      "9 = 0.7217000126838684\n",
      "10 = 0.7124000191688538\n",
      "11 = 0.7310000061988831\n",
      "12 = 0.7259017825126648\n",
      "13 = 0.7124000191688538\n",
      "14 = 6.628625392913818\n",
      "15 = 7.522073268890381\n",
      "16 = 0.3479301929473877\n",
      "17 = 6.628599166870117\n",
      "18 = 38.492408752441406\n",
      "epoch:13 step:12801 [D loss: 0.564815, acc.: 64.84%] [G loss: 0.544431]\n",
      "epoch:13 step:12802 [D loss: 0.556082, acc.: 72.66%] [G loss: 0.553426]\n",
      "epoch:13 step:12803 [D loss: 0.563273, acc.: 73.44%] [G loss: 0.608705]\n",
      "epoch:13 step:12804 [D loss: 0.465188, acc.: 78.12%] [G loss: 0.657062]\n",
      "epoch:13 step:12805 [D loss: 0.523800, acc.: 74.22%] [G loss: 0.676693]\n",
      "epoch:13 step:12806 [D loss: 0.593709, acc.: 60.16%] [G loss: 0.510737]\n",
      "epoch:13 step:12807 [D loss: 0.567274, acc.: 63.28%] [G loss: 0.529144]\n",
      "epoch:13 step:12808 [D loss: 0.569109, acc.: 67.97%] [G loss: 0.509773]\n",
      "epoch:13 step:12809 [D loss: 0.651880, acc.: 58.59%] [G loss: 0.489338]\n",
      "epoch:13 step:12810 [D loss: 0.532132, acc.: 71.88%] [G loss: 0.457547]\n",
      "epoch:13 step:12811 [D loss: 0.517200, acc.: 75.78%] [G loss: 0.524824]\n",
      "epoch:13 step:12812 [D loss: 0.553542, acc.: 70.31%] [G loss: 0.641723]\n",
      "epoch:13 step:12813 [D loss: 0.480590, acc.: 80.47%] [G loss: 0.666238]\n",
      "epoch:13 step:12814 [D loss: 0.515082, acc.: 76.56%] [G loss: 0.697549]\n",
      "epoch:13 step:12815 [D loss: 0.490226, acc.: 77.34%] [G loss: 0.707861]\n",
      "epoch:13 step:12816 [D loss: 0.485726, acc.: 75.78%] [G loss: 0.752591]\n",
      "epoch:13 step:12817 [D loss: 0.669943, acc.: 60.16%] [G loss: 0.478763]\n",
      "epoch:13 step:12818 [D loss: 0.569740, acc.: 70.31%] [G loss: 0.560816]\n",
      "epoch:13 step:12819 [D loss: 0.515560, acc.: 73.44%] [G loss: 0.634837]\n",
      "epoch:13 step:12820 [D loss: 0.498620, acc.: 72.66%] [G loss: 0.626915]\n",
      "epoch:13 step:12821 [D loss: 0.546308, acc.: 68.75%] [G loss: 0.583300]\n",
      "epoch:13 step:12822 [D loss: 0.496071, acc.: 72.66%] [G loss: 0.685864]\n",
      "epoch:13 step:12823 [D loss: 0.516019, acc.: 75.00%] [G loss: 0.633986]\n",
      "epoch:13 step:12824 [D loss: 0.526346, acc.: 73.44%] [G loss: 0.560549]\n",
      "epoch:13 step:12825 [D loss: 0.554977, acc.: 67.19%] [G loss: 0.652250]\n",
      "epoch:13 step:12826 [D loss: 0.543485, acc.: 64.84%] [G loss: 0.579268]\n",
      "epoch:13 step:12827 [D loss: 0.530291, acc.: 74.22%] [G loss: 0.586535]\n",
      "epoch:13 step:12828 [D loss: 0.466051, acc.: 79.69%] [G loss: 0.628385]\n",
      "epoch:13 step:12829 [D loss: 0.381049, acc.: 82.81%] [G loss: 0.909908]\n",
      "epoch:13 step:12830 [D loss: 0.485680, acc.: 78.12%] [G loss: 0.830585]\n",
      "epoch:13 step:12831 [D loss: 0.521560, acc.: 74.22%] [G loss: 0.884712]\n",
      "epoch:13 step:12832 [D loss: 0.552894, acc.: 71.88%] [G loss: 0.729894]\n",
      "epoch:13 step:12833 [D loss: 0.650692, acc.: 60.16%] [G loss: 0.491447]\n",
      "epoch:13 step:12834 [D loss: 0.602603, acc.: 64.06%] [G loss: 0.549390]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12835 [D loss: 0.484887, acc.: 75.78%] [G loss: 0.744684]\n",
      "epoch:13 step:12836 [D loss: 0.608314, acc.: 67.97%] [G loss: 0.617034]\n",
      "epoch:13 step:12837 [D loss: 0.534369, acc.: 74.22%] [G loss: 0.662454]\n",
      "epoch:13 step:12838 [D loss: 0.551170, acc.: 65.62%] [G loss: 0.597345]\n",
      "epoch:13 step:12839 [D loss: 0.579843, acc.: 67.97%] [G loss: 0.709618]\n",
      "epoch:13 step:12840 [D loss: 0.532215, acc.: 67.19%] [G loss: 0.707598]\n",
      "epoch:13 step:12841 [D loss: 0.603672, acc.: 67.19%] [G loss: 0.628101]\n",
      "epoch:13 step:12842 [D loss: 0.553565, acc.: 70.31%] [G loss: 0.620045]\n",
      "epoch:13 step:12843 [D loss: 0.562035, acc.: 67.97%] [G loss: 0.548869]\n",
      "epoch:13 step:12844 [D loss: 0.577049, acc.: 70.31%] [G loss: 0.575350]\n",
      "epoch:13 step:12845 [D loss: 0.508168, acc.: 78.91%] [G loss: 0.644189]\n",
      "epoch:13 step:12846 [D loss: 0.549902, acc.: 65.62%] [G loss: 0.730175]\n",
      "epoch:13 step:12847 [D loss: 0.585624, acc.: 69.53%] [G loss: 0.562292]\n",
      "epoch:13 step:12848 [D loss: 0.598077, acc.: 63.28%] [G loss: 0.687447]\n",
      "epoch:13 step:12849 [D loss: 0.550130, acc.: 74.22%] [G loss: 0.574108]\n",
      "epoch:13 step:12850 [D loss: 0.539268, acc.: 67.19%] [G loss: 0.577895]\n",
      "epoch:13 step:12851 [D loss: 0.534373, acc.: 69.53%] [G loss: 0.493243]\n",
      "epoch:13 step:12852 [D loss: 0.552892, acc.: 67.19%] [G loss: 0.522064]\n",
      "epoch:13 step:12853 [D loss: 0.550580, acc.: 73.44%] [G loss: 0.541856]\n",
      "epoch:13 step:12854 [D loss: 0.603228, acc.: 64.84%] [G loss: 0.488098]\n",
      "epoch:13 step:12855 [D loss: 0.553421, acc.: 71.88%] [G loss: 0.753448]\n",
      "epoch:13 step:12856 [D loss: 0.571903, acc.: 67.19%] [G loss: 0.636328]\n",
      "epoch:13 step:12857 [D loss: 0.506289, acc.: 75.78%] [G loss: 0.683146]\n",
      "epoch:13 step:12858 [D loss: 0.477630, acc.: 82.03%] [G loss: 0.653558]\n",
      "epoch:13 step:12859 [D loss: 0.578031, acc.: 67.19%] [G loss: 0.492056]\n",
      "epoch:13 step:12860 [D loss: 0.522941, acc.: 75.00%] [G loss: 0.586409]\n",
      "epoch:13 step:12861 [D loss: 0.571541, acc.: 69.53%] [G loss: 0.590258]\n",
      "epoch:13 step:12862 [D loss: 0.486701, acc.: 75.78%] [G loss: 0.620150]\n",
      "epoch:13 step:12863 [D loss: 0.492145, acc.: 78.12%] [G loss: 0.590629]\n",
      "epoch:13 step:12864 [D loss: 0.580903, acc.: 65.62%] [G loss: 0.540619]\n",
      "epoch:13 step:12865 [D loss: 0.611206, acc.: 66.41%] [G loss: 0.583570]\n",
      "epoch:13 step:12866 [D loss: 0.498630, acc.: 78.91%] [G loss: 0.539771]\n",
      "epoch:13 step:12867 [D loss: 0.559215, acc.: 69.53%] [G loss: 0.442128]\n",
      "epoch:13 step:12868 [D loss: 0.565391, acc.: 68.75%] [G loss: 0.449307]\n",
      "epoch:13 step:12869 [D loss: 0.527420, acc.: 72.66%] [G loss: 0.533546]\n",
      "epoch:13 step:12870 [D loss: 0.561758, acc.: 67.97%] [G loss: 0.668346]\n",
      "epoch:13 step:12871 [D loss: 0.513845, acc.: 75.78%] [G loss: 0.691131]\n",
      "epoch:13 step:12872 [D loss: 0.576558, acc.: 68.75%] [G loss: 0.758666]\n",
      "epoch:13 step:12873 [D loss: 0.534482, acc.: 71.88%] [G loss: 0.674393]\n",
      "epoch:13 step:12874 [D loss: 0.505040, acc.: 74.22%] [G loss: 0.643583]\n",
      "epoch:13 step:12875 [D loss: 0.539938, acc.: 71.88%] [G loss: 0.750671]\n",
      "epoch:13 step:12876 [D loss: 0.508480, acc.: 75.00%] [G loss: 0.565142]\n",
      "epoch:13 step:12877 [D loss: 0.633258, acc.: 60.94%] [G loss: 0.465128]\n",
      "epoch:13 step:12878 [D loss: 0.598738, acc.: 65.62%] [G loss: 0.448605]\n",
      "epoch:13 step:12879 [D loss: 0.567896, acc.: 68.75%] [G loss: 0.476836]\n",
      "epoch:13 step:12880 [D loss: 0.539674, acc.: 71.88%] [G loss: 0.533731]\n",
      "epoch:13 step:12881 [D loss: 0.553078, acc.: 71.09%] [G loss: 0.730279]\n",
      "epoch:13 step:12882 [D loss: 0.552411, acc.: 70.31%] [G loss: 0.751548]\n",
      "epoch:13 step:12883 [D loss: 0.615434, acc.: 65.62%] [G loss: 0.456044]\n",
      "epoch:13 step:12884 [D loss: 0.578757, acc.: 72.66%] [G loss: 0.629604]\n",
      "epoch:13 step:12885 [D loss: 0.635464, acc.: 65.62%] [G loss: 0.480371]\n",
      "epoch:13 step:12886 [D loss: 0.488370, acc.: 80.47%] [G loss: 0.685012]\n",
      "epoch:13 step:12887 [D loss: 0.521101, acc.: 79.69%] [G loss: 0.499807]\n",
      "epoch:13 step:12888 [D loss: 0.583765, acc.: 66.41%] [G loss: 0.506556]\n",
      "epoch:13 step:12889 [D loss: 0.502840, acc.: 76.56%] [G loss: 0.645608]\n",
      "epoch:13 step:12890 [D loss: 0.584046, acc.: 67.19%] [G loss: 0.736828]\n",
      "epoch:13 step:12891 [D loss: 0.584483, acc.: 69.53%] [G loss: 0.561243]\n",
      "epoch:13 step:12892 [D loss: 0.598171, acc.: 65.62%] [G loss: 0.568219]\n",
      "epoch:13 step:12893 [D loss: 0.565743, acc.: 69.53%] [G loss: 0.551183]\n",
      "epoch:13 step:12894 [D loss: 0.643426, acc.: 59.38%] [G loss: 0.614938]\n",
      "epoch:13 step:12895 [D loss: 0.534605, acc.: 67.19%] [G loss: 0.500364]\n",
      "epoch:13 step:12896 [D loss: 0.547058, acc.: 72.66%] [G loss: 0.527486]\n",
      "epoch:13 step:12897 [D loss: 0.625418, acc.: 67.19%] [G loss: 0.623635]\n",
      "epoch:13 step:12898 [D loss: 0.601865, acc.: 64.06%] [G loss: 0.403679]\n",
      "epoch:13 step:12899 [D loss: 0.583865, acc.: 63.28%] [G loss: 0.561526]\n",
      "epoch:13 step:12900 [D loss: 0.547531, acc.: 68.75%] [G loss: 0.498922]\n",
      "epoch:13 step:12901 [D loss: 0.554323, acc.: 67.19%] [G loss: 0.579045]\n",
      "epoch:13 step:12902 [D loss: 0.611522, acc.: 61.72%] [G loss: 0.582964]\n",
      "epoch:13 step:12903 [D loss: 0.584311, acc.: 70.31%] [G loss: 0.550585]\n",
      "epoch:13 step:12904 [D loss: 0.571573, acc.: 67.97%] [G loss: 0.467223]\n",
      "epoch:13 step:12905 [D loss: 0.557064, acc.: 70.31%] [G loss: 0.648314]\n",
      "epoch:13 step:12906 [D loss: 0.580576, acc.: 64.84%] [G loss: 0.730206]\n",
      "epoch:13 step:12907 [D loss: 0.556817, acc.: 71.09%] [G loss: 0.663765]\n",
      "epoch:13 step:12908 [D loss: 0.602680, acc.: 67.97%] [G loss: 0.477992]\n",
      "epoch:13 step:12909 [D loss: 0.564713, acc.: 68.75%] [G loss: 0.539041]\n",
      "epoch:13 step:12910 [D loss: 0.617610, acc.: 65.62%] [G loss: 0.388029]\n",
      "epoch:13 step:12911 [D loss: 0.508354, acc.: 77.34%] [G loss: 0.458407]\n",
      "epoch:13 step:12912 [D loss: 0.602760, acc.: 56.25%] [G loss: 0.600362]\n",
      "epoch:13 step:12913 [D loss: 0.558592, acc.: 65.62%] [G loss: 0.644611]\n",
      "epoch:13 step:12914 [D loss: 0.531323, acc.: 71.09%] [G loss: 0.715837]\n",
      "epoch:13 step:12915 [D loss: 0.538949, acc.: 68.75%] [G loss: 0.591052]\n",
      "epoch:13 step:12916 [D loss: 0.575363, acc.: 65.62%] [G loss: 0.528778]\n",
      "epoch:13 step:12917 [D loss: 0.503705, acc.: 69.53%] [G loss: 0.598341]\n",
      "epoch:13 step:12918 [D loss: 0.495504, acc.: 74.22%] [G loss: 0.676787]\n",
      "epoch:13 step:12919 [D loss: 0.531733, acc.: 73.44%] [G loss: 0.522710]\n",
      "epoch:13 step:12920 [D loss: 0.632810, acc.: 60.94%] [G loss: 0.445406]\n",
      "epoch:13 step:12921 [D loss: 0.649161, acc.: 60.94%] [G loss: 0.479113]\n",
      "epoch:13 step:12922 [D loss: 0.509808, acc.: 75.00%] [G loss: 0.500626]\n",
      "epoch:13 step:12923 [D loss: 0.553101, acc.: 69.53%] [G loss: 0.503675]\n",
      "epoch:13 step:12924 [D loss: 0.538784, acc.: 70.31%] [G loss: 0.618277]\n",
      "epoch:13 step:12925 [D loss: 0.538529, acc.: 72.66%] [G loss: 0.680657]\n",
      "epoch:13 step:12926 [D loss: 0.563112, acc.: 70.31%] [G loss: 0.496254]\n",
      "epoch:13 step:12927 [D loss: 0.462503, acc.: 75.78%] [G loss: 0.576766]\n",
      "epoch:13 step:12928 [D loss: 0.423303, acc.: 80.47%] [G loss: 0.688496]\n",
      "epoch:13 step:12929 [D loss: 0.553391, acc.: 67.19%] [G loss: 0.609224]\n",
      "epoch:13 step:12930 [D loss: 0.528707, acc.: 72.66%] [G loss: 0.747424]\n",
      "epoch:13 step:12931 [D loss: 0.513291, acc.: 75.78%] [G loss: 0.575979]\n",
      "epoch:13 step:12932 [D loss: 0.478018, acc.: 77.34%] [G loss: 0.704846]\n",
      "epoch:13 step:12933 [D loss: 0.594820, acc.: 70.31%] [G loss: 0.678084]\n",
      "epoch:13 step:12934 [D loss: 0.480230, acc.: 78.12%] [G loss: 0.634726]\n",
      "epoch:13 step:12935 [D loss: 0.568485, acc.: 64.06%] [G loss: 0.538017]\n",
      "epoch:13 step:12936 [D loss: 0.536708, acc.: 73.44%] [G loss: 0.619843]\n",
      "epoch:13 step:12937 [D loss: 0.574852, acc.: 67.97%] [G loss: 0.596758]\n",
      "epoch:13 step:12938 [D loss: 0.504012, acc.: 75.78%] [G loss: 0.605452]\n",
      "epoch:13 step:12939 [D loss: 0.549248, acc.: 73.44%] [G loss: 0.508952]\n",
      "epoch:13 step:12940 [D loss: 0.611538, acc.: 63.28%] [G loss: 0.523216]\n",
      "epoch:13 step:12941 [D loss: 0.540683, acc.: 67.97%] [G loss: 0.554890]\n",
      "epoch:13 step:12942 [D loss: 0.551631, acc.: 67.97%] [G loss: 0.604821]\n",
      "epoch:13 step:12943 [D loss: 0.563984, acc.: 70.31%] [G loss: 0.558576]\n",
      "epoch:13 step:12944 [D loss: 0.557848, acc.: 69.53%] [G loss: 0.461382]\n",
      "epoch:13 step:12945 [D loss: 0.566030, acc.: 65.62%] [G loss: 0.591202]\n",
      "epoch:13 step:12946 [D loss: 0.588924, acc.: 71.88%] [G loss: 0.528833]\n",
      "epoch:13 step:12947 [D loss: 0.666274, acc.: 57.03%] [G loss: 0.361514]\n",
      "epoch:13 step:12948 [D loss: 0.506930, acc.: 75.78%] [G loss: 0.385664]\n",
      "epoch:13 step:12949 [D loss: 0.554931, acc.: 71.88%] [G loss: 0.526928]\n",
      "epoch:13 step:12950 [D loss: 0.508078, acc.: 75.00%] [G loss: 0.675455]\n",
      "epoch:13 step:12951 [D loss: 0.529351, acc.: 70.31%] [G loss: 0.626980]\n",
      "epoch:13 step:12952 [D loss: 0.525805, acc.: 68.75%] [G loss: 0.619394]\n",
      "epoch:13 step:12953 [D loss: 0.583765, acc.: 67.19%] [G loss: 0.542432]\n",
      "epoch:13 step:12954 [D loss: 0.586862, acc.: 64.84%] [G loss: 0.608197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12955 [D loss: 0.554843, acc.: 73.44%] [G loss: 0.599430]\n",
      "epoch:13 step:12956 [D loss: 0.506836, acc.: 75.78%] [G loss: 0.765342]\n",
      "epoch:13 step:12957 [D loss: 0.591461, acc.: 65.62%] [G loss: 0.667011]\n",
      "epoch:13 step:12958 [D loss: 0.556753, acc.: 71.88%] [G loss: 0.647496]\n",
      "epoch:13 step:12959 [D loss: 0.603033, acc.: 63.28%] [G loss: 0.540693]\n",
      "epoch:13 step:12960 [D loss: 0.608636, acc.: 61.72%] [G loss: 0.511961]\n",
      "epoch:13 step:12961 [D loss: 0.491644, acc.: 78.12%] [G loss: 0.715907]\n",
      "epoch:13 step:12962 [D loss: 0.510580, acc.: 70.31%] [G loss: 0.732168]\n",
      "epoch:13 step:12963 [D loss: 0.483841, acc.: 80.47%] [G loss: 0.766886]\n",
      "epoch:13 step:12964 [D loss: 0.623997, acc.: 57.81%] [G loss: 0.652589]\n",
      "epoch:13 step:12965 [D loss: 0.644000, acc.: 57.81%] [G loss: 0.687790]\n",
      "epoch:13 step:12966 [D loss: 0.548812, acc.: 70.31%] [G loss: 0.511151]\n",
      "epoch:13 step:12967 [D loss: 0.558319, acc.: 67.19%] [G loss: 0.487662]\n",
      "epoch:13 step:12968 [D loss: 0.579673, acc.: 63.28%] [G loss: 0.514434]\n",
      "epoch:13 step:12969 [D loss: 0.645938, acc.: 68.75%] [G loss: 0.456198]\n",
      "epoch:13 step:12970 [D loss: 0.550895, acc.: 71.09%] [G loss: 0.474749]\n",
      "epoch:13 step:12971 [D loss: 0.533293, acc.: 71.88%] [G loss: 0.546609]\n",
      "epoch:13 step:12972 [D loss: 0.509479, acc.: 78.12%] [G loss: 0.569299]\n",
      "epoch:13 step:12973 [D loss: 0.470209, acc.: 78.12%] [G loss: 0.674371]\n",
      "epoch:13 step:12974 [D loss: 0.633339, acc.: 60.94%] [G loss: 0.589256]\n",
      "epoch:13 step:12975 [D loss: 0.670926, acc.: 55.47%] [G loss: 0.587795]\n",
      "epoch:13 step:12976 [D loss: 0.563249, acc.: 70.31%] [G loss: 0.444758]\n",
      "epoch:13 step:12977 [D loss: 0.528566, acc.: 75.78%] [G loss: 0.758812]\n",
      "epoch:13 step:12978 [D loss: 0.569510, acc.: 72.66%] [G loss: 0.758349]\n",
      "epoch:13 step:12979 [D loss: 0.538566, acc.: 74.22%] [G loss: 0.794917]\n",
      "epoch:13 step:12980 [D loss: 0.583996, acc.: 67.19%] [G loss: 0.482642]\n",
      "epoch:13 step:12981 [D loss: 0.600248, acc.: 67.19%] [G loss: 0.541475]\n",
      "epoch:13 step:12982 [D loss: 0.517485, acc.: 74.22%] [G loss: 0.756770]\n",
      "epoch:13 step:12983 [D loss: 0.521707, acc.: 75.78%] [G loss: 0.586168]\n",
      "epoch:13 step:12984 [D loss: 0.527571, acc.: 74.22%] [G loss: 0.809564]\n",
      "epoch:13 step:12985 [D loss: 0.587209, acc.: 67.97%] [G loss: 0.672171]\n",
      "epoch:13 step:12986 [D loss: 0.563984, acc.: 68.75%] [G loss: 0.519822]\n",
      "epoch:13 step:12987 [D loss: 0.547277, acc.: 68.75%] [G loss: 0.473316]\n",
      "epoch:13 step:12988 [D loss: 0.522464, acc.: 75.78%] [G loss: 0.513642]\n",
      "epoch:13 step:12989 [D loss: 0.552262, acc.: 70.31%] [G loss: 0.460171]\n",
      "epoch:13 step:12990 [D loss: 0.562572, acc.: 67.97%] [G loss: 0.566352]\n",
      "epoch:13 step:12991 [D loss: 0.535464, acc.: 71.88%] [G loss: 0.574603]\n",
      "epoch:13 step:12992 [D loss: 0.547185, acc.: 71.88%] [G loss: 0.683338]\n",
      "epoch:13 step:12993 [D loss: 0.638368, acc.: 61.72%] [G loss: 0.536286]\n",
      "epoch:13 step:12994 [D loss: 0.582182, acc.: 68.75%] [G loss: 0.611808]\n",
      "epoch:13 step:12995 [D loss: 0.599581, acc.: 67.97%] [G loss: 0.635286]\n",
      "epoch:13 step:12996 [D loss: 0.546980, acc.: 67.19%] [G loss: 0.855245]\n",
      "epoch:13 step:12997 [D loss: 0.578702, acc.: 71.09%] [G loss: 0.639182]\n",
      "epoch:13 step:12998 [D loss: 0.623754, acc.: 58.59%] [G loss: 0.540418]\n",
      "epoch:13 step:12999 [D loss: 0.568384, acc.: 74.22%] [G loss: 0.555157]\n",
      "epoch:13 step:13000 [D loss: 0.540014, acc.: 69.53%] [G loss: 0.410220]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.547186\n",
      "FID: 45.350044\n",
      "0 = 12.898703738403285\n",
      "1 = 0.09242907408200483\n",
      "2 = 0.883899986743927\n",
      "3 = 0.8137999773025513\n",
      "4 = 0.9539999961853027\n",
      "5 = 0.9464991688728333\n",
      "6 = 0.8137999773025513\n",
      "7 = 8.181327245116224\n",
      "8 = 0.14287838607607042\n",
      "9 = 0.7247999906539917\n",
      "10 = 0.7098000049591064\n",
      "11 = 0.739799976348877\n",
      "12 = 0.7317525744438171\n",
      "13 = 0.7098000049591064\n",
      "14 = 6.547210216522217\n",
      "15 = 7.183682441711426\n",
      "16 = 0.3752639889717102\n",
      "17 = 6.547185897827148\n",
      "18 = 45.35004425048828\n",
      "epoch:13 step:13001 [D loss: 0.646545, acc.: 64.84%] [G loss: 0.593651]\n",
      "epoch:13 step:13002 [D loss: 0.542796, acc.: 68.75%] [G loss: 0.557912]\n",
      "epoch:13 step:13003 [D loss: 0.562072, acc.: 67.97%] [G loss: 0.467652]\n",
      "epoch:13 step:13004 [D loss: 0.426656, acc.: 81.25%] [G loss: 0.626894]\n",
      "epoch:13 step:13005 [D loss: 0.592990, acc.: 67.19%] [G loss: 0.576315]\n",
      "epoch:13 step:13006 [D loss: 0.544418, acc.: 71.09%] [G loss: 0.652265]\n",
      "epoch:13 step:13007 [D loss: 0.552053, acc.: 70.31%] [G loss: 0.573385]\n",
      "epoch:13 step:13008 [D loss: 0.583082, acc.: 71.88%] [G loss: 0.564120]\n",
      "epoch:13 step:13009 [D loss: 0.604582, acc.: 64.84%] [G loss: 0.591032]\n",
      "epoch:13 step:13010 [D loss: 0.581425, acc.: 69.53%] [G loss: 0.540614]\n",
      "epoch:13 step:13011 [D loss: 0.531907, acc.: 71.09%] [G loss: 0.589281]\n",
      "epoch:13 step:13012 [D loss: 0.552440, acc.: 72.66%] [G loss: 0.666225]\n",
      "epoch:13 step:13013 [D loss: 0.523716, acc.: 73.44%] [G loss: 0.558138]\n",
      "epoch:13 step:13014 [D loss: 0.556588, acc.: 71.88%] [G loss: 0.647269]\n",
      "epoch:13 step:13015 [D loss: 0.524696, acc.: 75.00%] [G loss: 0.516661]\n",
      "epoch:13 step:13016 [D loss: 0.553289, acc.: 67.19%] [G loss: 0.479204]\n",
      "epoch:13 step:13017 [D loss: 0.559661, acc.: 69.53%] [G loss: 0.532804]\n",
      "epoch:13 step:13018 [D loss: 0.537995, acc.: 75.00%] [G loss: 0.443659]\n",
      "epoch:13 step:13019 [D loss: 0.534234, acc.: 71.88%] [G loss: 0.492857]\n",
      "epoch:13 step:13020 [D loss: 0.594667, acc.: 62.50%] [G loss: 0.567361]\n",
      "epoch:13 step:13021 [D loss: 0.600559, acc.: 67.97%] [G loss: 0.451177]\n",
      "epoch:13 step:13022 [D loss: 0.535250, acc.: 68.75%] [G loss: 0.449961]\n",
      "epoch:13 step:13023 [D loss: 0.511371, acc.: 75.78%] [G loss: 0.524139]\n",
      "epoch:13 step:13024 [D loss: 0.549475, acc.: 70.31%] [G loss: 0.537184]\n",
      "epoch:13 step:13025 [D loss: 0.540719, acc.: 71.09%] [G loss: 0.565553]\n",
      "epoch:13 step:13026 [D loss: 0.572932, acc.: 66.41%] [G loss: 0.515056]\n",
      "epoch:13 step:13027 [D loss: 0.576277, acc.: 64.84%] [G loss: 0.490209]\n",
      "epoch:13 step:13028 [D loss: 0.582848, acc.: 68.75%] [G loss: 0.431245]\n",
      "epoch:13 step:13029 [D loss: 0.523915, acc.: 68.75%] [G loss: 0.501532]\n",
      "epoch:13 step:13030 [D loss: 0.581636, acc.: 65.62%] [G loss: 0.499687]\n",
      "epoch:13 step:13031 [D loss: 0.589223, acc.: 68.75%] [G loss: 0.396208]\n",
      "epoch:13 step:13032 [D loss: 0.597013, acc.: 69.53%] [G loss: 0.564203]\n",
      "epoch:13 step:13033 [D loss: 0.538234, acc.: 71.09%] [G loss: 0.701787]\n",
      "epoch:13 step:13034 [D loss: 0.556483, acc.: 69.53%] [G loss: 0.647429]\n",
      "epoch:13 step:13035 [D loss: 0.555666, acc.: 67.97%] [G loss: 0.744555]\n",
      "epoch:13 step:13036 [D loss: 0.524930, acc.: 75.78%] [G loss: 0.542242]\n",
      "epoch:13 step:13037 [D loss: 0.607692, acc.: 67.19%] [G loss: 0.513147]\n",
      "epoch:13 step:13038 [D loss: 0.513245, acc.: 67.19%] [G loss: 0.579484]\n",
      "epoch:13 step:13039 [D loss: 0.642335, acc.: 63.28%] [G loss: 0.537080]\n",
      "epoch:13 step:13040 [D loss: 0.514101, acc.: 76.56%] [G loss: 0.645624]\n",
      "epoch:13 step:13041 [D loss: 0.469016, acc.: 71.88%] [G loss: 0.688722]\n",
      "epoch:13 step:13042 [D loss: 0.627253, acc.: 66.41%] [G loss: 0.780310]\n",
      "epoch:13 step:13043 [D loss: 0.578138, acc.: 62.50%] [G loss: 0.638942]\n",
      "epoch:13 step:13044 [D loss: 0.593644, acc.: 70.31%] [G loss: 0.611043]\n",
      "epoch:13 step:13045 [D loss: 0.576041, acc.: 62.50%] [G loss: 0.497505]\n",
      "epoch:13 step:13046 [D loss: 0.602090, acc.: 60.16%] [G loss: 0.496647]\n",
      "epoch:13 step:13047 [D loss: 0.533207, acc.: 72.66%] [G loss: 0.556546]\n",
      "epoch:13 step:13048 [D loss: 0.653442, acc.: 63.28%] [G loss: 0.489810]\n",
      "epoch:13 step:13049 [D loss: 0.532183, acc.: 74.22%] [G loss: 0.472421]\n",
      "epoch:13 step:13050 [D loss: 0.601144, acc.: 64.06%] [G loss: 0.466765]\n",
      "epoch:13 step:13051 [D loss: 0.445792, acc.: 77.34%] [G loss: 0.556016]\n",
      "epoch:13 step:13052 [D loss: 0.498471, acc.: 76.56%] [G loss: 0.602092]\n",
      "epoch:13 step:13053 [D loss: 0.592353, acc.: 62.50%] [G loss: 0.642826]\n",
      "epoch:13 step:13054 [D loss: 0.568643, acc.: 75.00%] [G loss: 0.544598]\n",
      "epoch:13 step:13055 [D loss: 0.594778, acc.: 66.41%] [G loss: 0.611960]\n",
      "epoch:13 step:13056 [D loss: 0.505190, acc.: 75.78%] [G loss: 0.712863]\n",
      "epoch:13 step:13057 [D loss: 0.595549, acc.: 67.19%] [G loss: 0.526606]\n",
      "epoch:13 step:13058 [D loss: 0.605017, acc.: 67.19%] [G loss: 0.379780]\n",
      "epoch:13 step:13059 [D loss: 0.562473, acc.: 64.84%] [G loss: 0.412447]\n",
      "epoch:13 step:13060 [D loss: 0.540529, acc.: 71.88%] [G loss: 0.506583]\n",
      "epoch:13 step:13061 [D loss: 0.622682, acc.: 67.97%] [G loss: 0.424457]\n",
      "epoch:13 step:13062 [D loss: 0.598209, acc.: 63.28%] [G loss: 0.274517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:13063 [D loss: 0.589649, acc.: 67.19%] [G loss: 0.469359]\n",
      "epoch:13 step:13064 [D loss: 0.610011, acc.: 63.28%] [G loss: 0.479999]\n",
      "epoch:13 step:13065 [D loss: 0.479315, acc.: 78.91%] [G loss: 0.612264]\n",
      "epoch:13 step:13066 [D loss: 0.559661, acc.: 72.66%] [G loss: 0.519969]\n",
      "epoch:13 step:13067 [D loss: 0.553882, acc.: 73.44%] [G loss: 0.570661]\n",
      "epoch:13 step:13068 [D loss: 0.532482, acc.: 71.09%] [G loss: 0.673589]\n",
      "epoch:13 step:13069 [D loss: 0.575684, acc.: 65.62%] [G loss: 0.707618]\n",
      "epoch:13 step:13070 [D loss: 0.560174, acc.: 70.31%] [G loss: 0.698452]\n",
      "epoch:13 step:13071 [D loss: 0.452261, acc.: 76.56%] [G loss: 0.793702]\n",
      "epoch:13 step:13072 [D loss: 0.615118, acc.: 60.16%] [G loss: 0.513073]\n",
      "epoch:13 step:13073 [D loss: 0.596131, acc.: 64.06%] [G loss: 0.440889]\n",
      "epoch:13 step:13074 [D loss: 0.564278, acc.: 67.97%] [G loss: 0.415431]\n",
      "epoch:13 step:13075 [D loss: 0.513774, acc.: 78.91%] [G loss: 0.705887]\n",
      "epoch:13 step:13076 [D loss: 0.520196, acc.: 75.00%] [G loss: 0.626320]\n",
      "epoch:13 step:13077 [D loss: 0.514501, acc.: 74.22%] [G loss: 0.718173]\n",
      "epoch:13 step:13078 [D loss: 0.526949, acc.: 72.66%] [G loss: 0.626618]\n",
      "epoch:13 step:13079 [D loss: 0.465117, acc.: 75.78%] [G loss: 0.656025]\n",
      "epoch:13 step:13080 [D loss: 0.485219, acc.: 77.34%] [G loss: 0.697351]\n",
      "epoch:13 step:13081 [D loss: 0.529906, acc.: 75.78%] [G loss: 0.772538]\n",
      "epoch:13 step:13082 [D loss: 0.499609, acc.: 72.66%] [G loss: 0.687120]\n",
      "epoch:13 step:13083 [D loss: 0.617034, acc.: 62.50%] [G loss: 0.459130]\n",
      "epoch:13 step:13084 [D loss: 0.564176, acc.: 72.66%] [G loss: 0.493151]\n",
      "epoch:13 step:13085 [D loss: 0.604921, acc.: 66.41%] [G loss: 0.490315]\n",
      "epoch:13 step:13086 [D loss: 0.551538, acc.: 73.44%] [G loss: 0.595987]\n",
      "epoch:13 step:13087 [D loss: 0.527151, acc.: 77.34%] [G loss: 0.606557]\n",
      "epoch:13 step:13088 [D loss: 0.533428, acc.: 77.34%] [G loss: 0.692702]\n",
      "epoch:13 step:13089 [D loss: 0.567159, acc.: 71.09%] [G loss: 0.440476]\n",
      "epoch:13 step:13090 [D loss: 0.554010, acc.: 69.53%] [G loss: 0.625635]\n",
      "epoch:13 step:13091 [D loss: 0.559437, acc.: 71.88%] [G loss: 0.512414]\n",
      "epoch:13 step:13092 [D loss: 0.459012, acc.: 78.12%] [G loss: 0.740980]\n",
      "epoch:13 step:13093 [D loss: 0.489816, acc.: 74.22%] [G loss: 0.749458]\n",
      "epoch:13 step:13094 [D loss: 0.525929, acc.: 75.78%] [G loss: 0.801468]\n",
      "epoch:13 step:13095 [D loss: 0.501198, acc.: 78.91%] [G loss: 0.721985]\n",
      "epoch:13 step:13096 [D loss: 0.675844, acc.: 63.28%] [G loss: 0.525465]\n",
      "epoch:13 step:13097 [D loss: 0.528663, acc.: 74.22%] [G loss: 0.572512]\n",
      "epoch:13 step:13098 [D loss: 0.546141, acc.: 69.53%] [G loss: 0.549599]\n",
      "epoch:13 step:13099 [D loss: 0.465618, acc.: 78.12%] [G loss: 0.671643]\n",
      "epoch:13 step:13100 [D loss: 0.472720, acc.: 76.56%] [G loss: 0.751399]\n",
      "epoch:13 step:13101 [D loss: 0.724467, acc.: 57.03%] [G loss: 0.539311]\n",
      "epoch:13 step:13102 [D loss: 0.495018, acc.: 76.56%] [G loss: 0.605220]\n",
      "epoch:13 step:13103 [D loss: 0.538073, acc.: 73.44%] [G loss: 0.671565]\n",
      "epoch:13 step:13104 [D loss: 0.511469, acc.: 72.66%] [G loss: 0.546543]\n",
      "epoch:13 step:13105 [D loss: 0.544079, acc.: 71.88%] [G loss: 0.767793]\n",
      "epoch:13 step:13106 [D loss: 0.421562, acc.: 81.25%] [G loss: 0.953581]\n",
      "epoch:13 step:13107 [D loss: 0.433411, acc.: 76.56%] [G loss: 1.134564]\n",
      "epoch:13 step:13108 [D loss: 0.508665, acc.: 73.44%] [G loss: 1.080835]\n",
      "epoch:13 step:13109 [D loss: 0.711760, acc.: 58.59%] [G loss: 0.955433]\n",
      "epoch:13 step:13110 [D loss: 0.485110, acc.: 80.47%] [G loss: 1.345511]\n",
      "epoch:13 step:13111 [D loss: 0.464838, acc.: 76.56%] [G loss: 1.118151]\n",
      "epoch:13 step:13112 [D loss: 0.557533, acc.: 66.41%] [G loss: 0.787313]\n",
      "epoch:13 step:13113 [D loss: 0.653970, acc.: 57.81%] [G loss: 0.656043]\n",
      "epoch:13 step:13114 [D loss: 0.570707, acc.: 67.19%] [G loss: 0.666173]\n",
      "epoch:13 step:13115 [D loss: 0.497017, acc.: 75.78%] [G loss: 0.816722]\n",
      "epoch:13 step:13116 [D loss: 0.535879, acc.: 71.09%] [G loss: 0.838940]\n",
      "epoch:13 step:13117 [D loss: 0.397608, acc.: 81.25%] [G loss: 1.325299]\n",
      "epoch:13 step:13118 [D loss: 0.372783, acc.: 84.38%] [G loss: 1.252135]\n",
      "epoch:14 step:13119 [D loss: 0.590194, acc.: 68.75%] [G loss: 0.775680]\n",
      "epoch:14 step:13120 [D loss: 0.501569, acc.: 76.56%] [G loss: 1.037017]\n",
      "epoch:14 step:13121 [D loss: 0.577411, acc.: 67.97%] [G loss: 0.762886]\n",
      "epoch:14 step:13122 [D loss: 0.535786, acc.: 71.88%] [G loss: 0.724185]\n",
      "epoch:14 step:13123 [D loss: 0.530203, acc.: 74.22%] [G loss: 0.611251]\n",
      "epoch:14 step:13124 [D loss: 0.571650, acc.: 71.09%] [G loss: 0.731613]\n",
      "epoch:14 step:13125 [D loss: 0.532866, acc.: 76.56%] [G loss: 0.733989]\n",
      "epoch:14 step:13126 [D loss: 0.480704, acc.: 77.34%] [G loss: 0.764136]\n",
      "epoch:14 step:13127 [D loss: 0.501494, acc.: 76.56%] [G loss: 0.769250]\n",
      "epoch:14 step:13128 [D loss: 0.518998, acc.: 73.44%] [G loss: 0.838293]\n",
      "epoch:14 step:13129 [D loss: 0.482243, acc.: 80.47%] [G loss: 0.983628]\n",
      "epoch:14 step:13130 [D loss: 0.590605, acc.: 66.41%] [G loss: 0.608837]\n",
      "epoch:14 step:13131 [D loss: 0.543795, acc.: 74.22%] [G loss: 0.481519]\n",
      "epoch:14 step:13132 [D loss: 0.542602, acc.: 71.88%] [G loss: 0.624940]\n",
      "epoch:14 step:13133 [D loss: 0.485922, acc.: 71.88%] [G loss: 0.643897]\n",
      "epoch:14 step:13134 [D loss: 0.455027, acc.: 80.47%] [G loss: 0.686203]\n",
      "epoch:14 step:13135 [D loss: 0.541827, acc.: 73.44%] [G loss: 0.498092]\n",
      "epoch:14 step:13136 [D loss: 0.559670, acc.: 67.19%] [G loss: 0.597839]\n",
      "epoch:14 step:13137 [D loss: 0.550447, acc.: 68.75%] [G loss: 0.678585]\n",
      "epoch:14 step:13138 [D loss: 0.614796, acc.: 65.62%] [G loss: 0.649042]\n",
      "epoch:14 step:13139 [D loss: 0.539052, acc.: 71.09%] [G loss: 0.669931]\n",
      "epoch:14 step:13140 [D loss: 0.444400, acc.: 81.25%] [G loss: 0.832561]\n",
      "epoch:14 step:13141 [D loss: 0.532578, acc.: 74.22%] [G loss: 0.684100]\n",
      "epoch:14 step:13142 [D loss: 0.507045, acc.: 75.00%] [G loss: 0.711755]\n",
      "epoch:14 step:13143 [D loss: 0.497735, acc.: 71.88%] [G loss: 0.611452]\n",
      "epoch:14 step:13144 [D loss: 0.587815, acc.: 67.19%] [G loss: 0.576311]\n",
      "epoch:14 step:13145 [D loss: 0.489297, acc.: 73.44%] [G loss: 0.649080]\n",
      "epoch:14 step:13146 [D loss: 0.578877, acc.: 66.41%] [G loss: 0.677051]\n",
      "epoch:14 step:13147 [D loss: 0.512877, acc.: 74.22%] [G loss: 0.473687]\n",
      "epoch:14 step:13148 [D loss: 0.532496, acc.: 70.31%] [G loss: 0.532697]\n",
      "epoch:14 step:13149 [D loss: 0.600657, acc.: 67.97%] [G loss: 0.509326]\n",
      "epoch:14 step:13150 [D loss: 0.579675, acc.: 71.09%] [G loss: 0.476378]\n",
      "epoch:14 step:13151 [D loss: 0.588917, acc.: 67.19%] [G loss: 0.603771]\n",
      "epoch:14 step:13152 [D loss: 0.527290, acc.: 71.09%] [G loss: 0.624666]\n",
      "epoch:14 step:13153 [D loss: 0.551003, acc.: 74.22%] [G loss: 0.482182]\n",
      "epoch:14 step:13154 [D loss: 0.556343, acc.: 67.97%] [G loss: 0.665732]\n",
      "epoch:14 step:13155 [D loss: 0.488229, acc.: 80.47%] [G loss: 0.651695]\n",
      "epoch:14 step:13156 [D loss: 0.597860, acc.: 67.19%] [G loss: 0.545828]\n",
      "epoch:14 step:13157 [D loss: 0.480034, acc.: 79.69%] [G loss: 0.656854]\n",
      "epoch:14 step:13158 [D loss: 0.471256, acc.: 73.44%] [G loss: 0.538154]\n",
      "epoch:14 step:13159 [D loss: 0.522667, acc.: 72.66%] [G loss: 0.726051]\n",
      "epoch:14 step:13160 [D loss: 0.549704, acc.: 70.31%] [G loss: 0.622525]\n",
      "epoch:14 step:13161 [D loss: 0.527088, acc.: 71.88%] [G loss: 0.610528]\n",
      "epoch:14 step:13162 [D loss: 0.570147, acc.: 70.31%] [G loss: 0.588334]\n",
      "epoch:14 step:13163 [D loss: 0.478378, acc.: 77.34%] [G loss: 0.677951]\n",
      "epoch:14 step:13164 [D loss: 0.491875, acc.: 77.34%] [G loss: 0.655797]\n",
      "epoch:14 step:13165 [D loss: 0.481317, acc.: 74.22%] [G loss: 0.756966]\n",
      "epoch:14 step:13166 [D loss: 0.533554, acc.: 69.53%] [G loss: 0.647848]\n",
      "epoch:14 step:13167 [D loss: 0.545471, acc.: 74.22%] [G loss: 0.589990]\n",
      "epoch:14 step:13168 [D loss: 0.531442, acc.: 74.22%] [G loss: 0.598058]\n",
      "epoch:14 step:13169 [D loss: 0.611282, acc.: 64.84%] [G loss: 0.420318]\n",
      "epoch:14 step:13170 [D loss: 0.605168, acc.: 64.84%] [G loss: 0.503432]\n",
      "epoch:14 step:13171 [D loss: 0.548897, acc.: 68.75%] [G loss: 0.602420]\n",
      "epoch:14 step:13172 [D loss: 0.477225, acc.: 79.69%] [G loss: 0.589295]\n",
      "epoch:14 step:13173 [D loss: 0.531347, acc.: 72.66%] [G loss: 0.744778]\n",
      "epoch:14 step:13174 [D loss: 0.537980, acc.: 70.31%] [G loss: 0.679773]\n",
      "epoch:14 step:13175 [D loss: 0.532984, acc.: 70.31%] [G loss: 0.648603]\n",
      "epoch:14 step:13176 [D loss: 0.552765, acc.: 67.97%] [G loss: 0.751182]\n",
      "epoch:14 step:13177 [D loss: 0.527176, acc.: 76.56%] [G loss: 0.606060]\n",
      "epoch:14 step:13178 [D loss: 0.562664, acc.: 64.06%] [G loss: 0.591148]\n",
      "epoch:14 step:13179 [D loss: 0.563145, acc.: 67.97%] [G loss: 0.526114]\n",
      "epoch:14 step:13180 [D loss: 0.552529, acc.: 71.88%] [G loss: 0.620879]\n",
      "epoch:14 step:13181 [D loss: 0.615879, acc.: 62.50%] [G loss: 0.597920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13182 [D loss: 0.535313, acc.: 71.88%] [G loss: 0.581525]\n",
      "epoch:14 step:13183 [D loss: 0.585162, acc.: 71.09%] [G loss: 0.544940]\n",
      "epoch:14 step:13184 [D loss: 0.582159, acc.: 68.75%] [G loss: 0.697783]\n",
      "epoch:14 step:13185 [D loss: 0.544217, acc.: 68.75%] [G loss: 0.573812]\n",
      "epoch:14 step:13186 [D loss: 0.539400, acc.: 69.53%] [G loss: 0.659087]\n",
      "epoch:14 step:13187 [D loss: 0.520898, acc.: 73.44%] [G loss: 0.491553]\n",
      "epoch:14 step:13188 [D loss: 0.512066, acc.: 75.78%] [G loss: 0.678936]\n",
      "epoch:14 step:13189 [D loss: 0.566482, acc.: 69.53%] [G loss: 0.539200]\n",
      "epoch:14 step:13190 [D loss: 0.499064, acc.: 76.56%] [G loss: 0.545726]\n",
      "epoch:14 step:13191 [D loss: 0.528065, acc.: 68.75%] [G loss: 0.544813]\n",
      "epoch:14 step:13192 [D loss: 0.504489, acc.: 74.22%] [G loss: 0.599024]\n",
      "epoch:14 step:13193 [D loss: 0.551045, acc.: 67.97%] [G loss: 0.753923]\n",
      "epoch:14 step:13194 [D loss: 0.554297, acc.: 68.75%] [G loss: 0.693555]\n",
      "epoch:14 step:13195 [D loss: 0.496573, acc.: 75.00%] [G loss: 0.813209]\n",
      "epoch:14 step:13196 [D loss: 0.581455, acc.: 68.75%] [G loss: 0.652467]\n",
      "epoch:14 step:13197 [D loss: 0.590026, acc.: 64.06%] [G loss: 0.552571]\n",
      "epoch:14 step:13198 [D loss: 0.535780, acc.: 74.22%] [G loss: 0.598957]\n",
      "epoch:14 step:13199 [D loss: 0.577102, acc.: 68.75%] [G loss: 0.907669]\n",
      "epoch:14 step:13200 [D loss: 0.533262, acc.: 71.88%] [G loss: 0.615916]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.385429\n",
      "FID: 55.053436\n",
      "0 = 13.075296670055419\n",
      "1 = 0.09333646485206917\n",
      "2 = 0.890999972820282\n",
      "3 = 0.8325999975204468\n",
      "4 = 0.949400007724762\n",
      "5 = 0.9427083134651184\n",
      "6 = 0.8325999975204468\n",
      "7 = 8.641214319825176\n",
      "8 = 0.16092886299734327\n",
      "9 = 0.718500018119812\n",
      "10 = 0.7080000042915344\n",
      "11 = 0.7289999723434448\n",
      "12 = 0.7231869101524353\n",
      "13 = 0.7080000042915344\n",
      "14 = 6.385453224182129\n",
      "15 = 6.787984848022461\n",
      "16 = 0.4051440358161926\n",
      "17 = 6.385429382324219\n",
      "18 = 55.053436279296875\n",
      "epoch:14 step:13201 [D loss: 0.522278, acc.: 74.22%] [G loss: 0.659493]\n",
      "epoch:14 step:13202 [D loss: 0.523070, acc.: 70.31%] [G loss: 0.633522]\n",
      "epoch:14 step:13203 [D loss: 0.540384, acc.: 71.88%] [G loss: 0.585631]\n",
      "epoch:14 step:13204 [D loss: 0.527766, acc.: 73.44%] [G loss: 0.644077]\n",
      "epoch:14 step:13205 [D loss: 0.579177, acc.: 68.75%] [G loss: 0.625585]\n",
      "epoch:14 step:13206 [D loss: 0.533805, acc.: 74.22%] [G loss: 0.622602]\n",
      "epoch:14 step:13207 [D loss: 0.518118, acc.: 73.44%] [G loss: 0.657947]\n",
      "epoch:14 step:13208 [D loss: 0.557278, acc.: 69.53%] [G loss: 0.573608]\n",
      "epoch:14 step:13209 [D loss: 0.567040, acc.: 67.19%] [G loss: 0.559227]\n",
      "epoch:14 step:13210 [D loss: 0.452178, acc.: 80.47%] [G loss: 0.665769]\n",
      "epoch:14 step:13211 [D loss: 0.531996, acc.: 71.09%] [G loss: 0.603312]\n",
      "epoch:14 step:13212 [D loss: 0.512727, acc.: 74.22%] [G loss: 0.705478]\n",
      "epoch:14 step:13213 [D loss: 0.502392, acc.: 75.00%] [G loss: 0.760170]\n",
      "epoch:14 step:13214 [D loss: 0.514494, acc.: 75.78%] [G loss: 0.652998]\n",
      "epoch:14 step:13215 [D loss: 0.556240, acc.: 64.84%] [G loss: 0.745319]\n",
      "epoch:14 step:13216 [D loss: 0.508845, acc.: 75.00%] [G loss: 0.752826]\n",
      "epoch:14 step:13217 [D loss: 0.519935, acc.: 75.00%] [G loss: 0.555260]\n",
      "epoch:14 step:13218 [D loss: 0.452607, acc.: 80.47%] [G loss: 0.884343]\n",
      "epoch:14 step:13219 [D loss: 0.497134, acc.: 74.22%] [G loss: 0.859052]\n",
      "epoch:14 step:13220 [D loss: 0.626546, acc.: 68.75%] [G loss: 0.564336]\n",
      "epoch:14 step:13221 [D loss: 0.540660, acc.: 67.97%] [G loss: 0.467089]\n",
      "epoch:14 step:13222 [D loss: 0.544789, acc.: 67.19%] [G loss: 0.568004]\n",
      "epoch:14 step:13223 [D loss: 0.592579, acc.: 67.19%] [G loss: 0.724102]\n",
      "epoch:14 step:13224 [D loss: 0.569700, acc.: 71.88%] [G loss: 0.642168]\n",
      "epoch:14 step:13225 [D loss: 0.556714, acc.: 67.19%] [G loss: 0.688614]\n",
      "epoch:14 step:13226 [D loss: 0.630632, acc.: 61.72%] [G loss: 0.603387]\n",
      "epoch:14 step:13227 [D loss: 0.574944, acc.: 67.97%] [G loss: 0.604046]\n",
      "epoch:14 step:13228 [D loss: 0.536098, acc.: 75.00%] [G loss: 0.587869]\n",
      "epoch:14 step:13229 [D loss: 0.575985, acc.: 65.62%] [G loss: 0.470176]\n",
      "epoch:14 step:13230 [D loss: 0.520433, acc.: 73.44%] [G loss: 0.626548]\n",
      "epoch:14 step:13231 [D loss: 0.578415, acc.: 66.41%] [G loss: 0.604117]\n",
      "epoch:14 step:13232 [D loss: 0.544741, acc.: 71.09%] [G loss: 0.594355]\n",
      "epoch:14 step:13233 [D loss: 0.521094, acc.: 71.09%] [G loss: 0.651765]\n",
      "epoch:14 step:13234 [D loss: 0.545072, acc.: 67.19%] [G loss: 0.682217]\n",
      "epoch:14 step:13235 [D loss: 0.533564, acc.: 70.31%] [G loss: 0.611500]\n",
      "epoch:14 step:13236 [D loss: 0.539415, acc.: 67.97%] [G loss: 0.720050]\n",
      "epoch:14 step:13237 [D loss: 0.485732, acc.: 74.22%] [G loss: 1.006918]\n",
      "epoch:14 step:13238 [D loss: 0.541424, acc.: 76.56%] [G loss: 0.702617]\n",
      "epoch:14 step:13239 [D loss: 0.545552, acc.: 70.31%] [G loss: 0.667575]\n",
      "epoch:14 step:13240 [D loss: 0.518325, acc.: 75.78%] [G loss: 0.646591]\n",
      "epoch:14 step:13241 [D loss: 0.521598, acc.: 74.22%] [G loss: 0.776495]\n",
      "epoch:14 step:13242 [D loss: 0.608759, acc.: 65.62%] [G loss: 0.594357]\n",
      "epoch:14 step:13243 [D loss: 0.540223, acc.: 70.31%] [G loss: 0.592533]\n",
      "epoch:14 step:13244 [D loss: 0.538677, acc.: 71.88%] [G loss: 0.428457]\n",
      "epoch:14 step:13245 [D loss: 0.489577, acc.: 74.22%] [G loss: 0.582271]\n",
      "epoch:14 step:13246 [D loss: 0.492522, acc.: 75.00%] [G loss: 0.577469]\n",
      "epoch:14 step:13247 [D loss: 0.597630, acc.: 63.28%] [G loss: 0.495387]\n",
      "epoch:14 step:13248 [D loss: 0.521949, acc.: 74.22%] [G loss: 0.455478]\n",
      "epoch:14 step:13249 [D loss: 0.541704, acc.: 64.84%] [G loss: 0.524543]\n",
      "epoch:14 step:13250 [D loss: 0.561269, acc.: 68.75%] [G loss: 0.530261]\n",
      "epoch:14 step:13251 [D loss: 0.566596, acc.: 68.75%] [G loss: 0.524891]\n",
      "epoch:14 step:13252 [D loss: 0.523404, acc.: 72.66%] [G loss: 0.616589]\n",
      "epoch:14 step:13253 [D loss: 0.553643, acc.: 75.00%] [G loss: 0.620427]\n",
      "epoch:14 step:13254 [D loss: 0.584070, acc.: 67.19%] [G loss: 0.585983]\n",
      "epoch:14 step:13255 [D loss: 0.625531, acc.: 65.62%] [G loss: 0.564685]\n",
      "epoch:14 step:13256 [D loss: 0.599201, acc.: 60.94%] [G loss: 0.647418]\n",
      "epoch:14 step:13257 [D loss: 0.573532, acc.: 70.31%] [G loss: 0.562632]\n",
      "epoch:14 step:13258 [D loss: 0.574264, acc.: 66.41%] [G loss: 0.482685]\n",
      "epoch:14 step:13259 [D loss: 0.514669, acc.: 73.44%] [G loss: 0.661948]\n",
      "epoch:14 step:13260 [D loss: 0.549305, acc.: 68.75%] [G loss: 0.411047]\n",
      "epoch:14 step:13261 [D loss: 0.560585, acc.: 67.97%] [G loss: 0.570954]\n",
      "epoch:14 step:13262 [D loss: 0.534085, acc.: 70.31%] [G loss: 0.629350]\n",
      "epoch:14 step:13263 [D loss: 0.559300, acc.: 71.09%] [G loss: 0.619405]\n",
      "epoch:14 step:13264 [D loss: 0.563188, acc.: 71.88%] [G loss: 0.565704]\n",
      "epoch:14 step:13265 [D loss: 0.665538, acc.: 62.50%] [G loss: 0.639916]\n",
      "epoch:14 step:13266 [D loss: 0.608775, acc.: 65.62%] [G loss: 0.442309]\n",
      "epoch:14 step:13267 [D loss: 0.531947, acc.: 66.41%] [G loss: 0.630653]\n",
      "epoch:14 step:13268 [D loss: 0.609757, acc.: 65.62%] [G loss: 0.501501]\n",
      "epoch:14 step:13269 [D loss: 0.587126, acc.: 65.62%] [G loss: 0.661219]\n",
      "epoch:14 step:13270 [D loss: 0.535894, acc.: 72.66%] [G loss: 0.512523]\n",
      "epoch:14 step:13271 [D loss: 0.557817, acc.: 71.88%] [G loss: 0.520725]\n",
      "epoch:14 step:13272 [D loss: 0.535083, acc.: 69.53%] [G loss: 0.585579]\n",
      "epoch:14 step:13273 [D loss: 0.466081, acc.: 75.78%] [G loss: 0.730687]\n",
      "epoch:14 step:13274 [D loss: 0.552547, acc.: 71.09%] [G loss: 0.666595]\n",
      "epoch:14 step:13275 [D loss: 0.599088, acc.: 66.41%] [G loss: 0.529246]\n",
      "epoch:14 step:13276 [D loss: 0.597886, acc.: 66.41%] [G loss: 0.499970]\n",
      "epoch:14 step:13277 [D loss: 0.517227, acc.: 74.22%] [G loss: 0.579860]\n",
      "epoch:14 step:13278 [D loss: 0.568209, acc.: 67.19%] [G loss: 0.565886]\n",
      "epoch:14 step:13279 [D loss: 0.535652, acc.: 73.44%] [G loss: 0.556726]\n",
      "epoch:14 step:13280 [D loss: 0.490730, acc.: 78.12%] [G loss: 0.580335]\n",
      "epoch:14 step:13281 [D loss: 0.560037, acc.: 69.53%] [G loss: 0.641947]\n",
      "epoch:14 step:13282 [D loss: 0.548591, acc.: 70.31%] [G loss: 0.889379]\n",
      "epoch:14 step:13283 [D loss: 0.515835, acc.: 73.44%] [G loss: 0.624669]\n",
      "epoch:14 step:13284 [D loss: 0.542687, acc.: 71.88%] [G loss: 0.615803]\n",
      "epoch:14 step:13285 [D loss: 0.587349, acc.: 68.75%] [G loss: 0.574980]\n",
      "epoch:14 step:13286 [D loss: 0.540420, acc.: 77.34%] [G loss: 0.577612]\n",
      "epoch:14 step:13287 [D loss: 0.546901, acc.: 68.75%] [G loss: 0.588638]\n",
      "epoch:14 step:13288 [D loss: 0.537582, acc.: 70.31%] [G loss: 0.556107]\n",
      "epoch:14 step:13289 [D loss: 0.543524, acc.: 65.62%] [G loss: 0.539817]\n",
      "epoch:14 step:13290 [D loss: 0.536299, acc.: 74.22%] [G loss: 0.729845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13291 [D loss: 0.500241, acc.: 73.44%] [G loss: 0.683890]\n",
      "epoch:14 step:13292 [D loss: 0.597484, acc.: 67.97%] [G loss: 0.560990]\n",
      "epoch:14 step:13293 [D loss: 0.560257, acc.: 65.62%] [G loss: 0.562380]\n",
      "epoch:14 step:13294 [D loss: 0.520447, acc.: 71.88%] [G loss: 0.606922]\n",
      "epoch:14 step:13295 [D loss: 0.526582, acc.: 72.66%] [G loss: 0.535646]\n",
      "epoch:14 step:13296 [D loss: 0.612898, acc.: 61.72%] [G loss: 0.545019]\n",
      "epoch:14 step:13297 [D loss: 0.587671, acc.: 66.41%] [G loss: 0.610909]\n",
      "epoch:14 step:13298 [D loss: 0.622846, acc.: 63.28%] [G loss: 0.404706]\n",
      "epoch:14 step:13299 [D loss: 0.544822, acc.: 72.66%] [G loss: 0.568535]\n",
      "epoch:14 step:13300 [D loss: 0.534678, acc.: 72.66%] [G loss: 0.633600]\n",
      "epoch:14 step:13301 [D loss: 0.544714, acc.: 71.09%] [G loss: 0.585097]\n",
      "epoch:14 step:13302 [D loss: 0.549545, acc.: 67.19%] [G loss: 0.627316]\n",
      "epoch:14 step:13303 [D loss: 0.552032, acc.: 71.88%] [G loss: 0.627704]\n",
      "epoch:14 step:13304 [D loss: 0.563531, acc.: 71.09%] [G loss: 0.649673]\n",
      "epoch:14 step:13305 [D loss: 0.662377, acc.: 60.94%] [G loss: 0.549991]\n",
      "epoch:14 step:13306 [D loss: 0.484464, acc.: 75.78%] [G loss: 0.676711]\n",
      "epoch:14 step:13307 [D loss: 0.538194, acc.: 73.44%] [G loss: 0.708117]\n",
      "epoch:14 step:13308 [D loss: 0.521055, acc.: 73.44%] [G loss: 0.587401]\n",
      "epoch:14 step:13309 [D loss: 0.503286, acc.: 74.22%] [G loss: 0.576902]\n",
      "epoch:14 step:13310 [D loss: 0.541166, acc.: 68.75%] [G loss: 0.587413]\n",
      "epoch:14 step:13311 [D loss: 0.536677, acc.: 76.56%] [G loss: 0.611675]\n",
      "epoch:14 step:13312 [D loss: 0.497338, acc.: 77.34%] [G loss: 0.638632]\n",
      "epoch:14 step:13313 [D loss: 0.543011, acc.: 68.75%] [G loss: 0.666334]\n",
      "epoch:14 step:13314 [D loss: 0.617578, acc.: 63.28%] [G loss: 0.489224]\n",
      "epoch:14 step:13315 [D loss: 0.527231, acc.: 71.09%] [G loss: 0.676186]\n",
      "epoch:14 step:13316 [D loss: 0.497830, acc.: 72.66%] [G loss: 0.698273]\n",
      "epoch:14 step:13317 [D loss: 0.570426, acc.: 67.97%] [G loss: 0.685167]\n",
      "epoch:14 step:13318 [D loss: 0.637853, acc.: 67.97%] [G loss: 0.600475]\n",
      "epoch:14 step:13319 [D loss: 0.573988, acc.: 67.97%] [G loss: 0.624523]\n",
      "epoch:14 step:13320 [D loss: 0.561537, acc.: 68.75%] [G loss: 0.512463]\n",
      "epoch:14 step:13321 [D loss: 0.620718, acc.: 62.50%] [G loss: 0.506599]\n",
      "epoch:14 step:13322 [D loss: 0.502031, acc.: 75.78%] [G loss: 0.578440]\n",
      "epoch:14 step:13323 [D loss: 0.530348, acc.: 68.75%] [G loss: 0.753870]\n",
      "epoch:14 step:13324 [D loss: 0.511412, acc.: 73.44%] [G loss: 0.764660]\n",
      "epoch:14 step:13325 [D loss: 0.424894, acc.: 82.03%] [G loss: 0.989529]\n",
      "epoch:14 step:13326 [D loss: 0.476772, acc.: 80.47%] [G loss: 0.754437]\n",
      "epoch:14 step:13327 [D loss: 0.487891, acc.: 73.44%] [G loss: 0.793205]\n",
      "epoch:14 step:13328 [D loss: 0.639188, acc.: 58.59%] [G loss: 0.666446]\n",
      "epoch:14 step:13329 [D loss: 0.595085, acc.: 64.84%] [G loss: 0.556684]\n",
      "epoch:14 step:13330 [D loss: 0.546439, acc.: 72.66%] [G loss: 0.598293]\n",
      "epoch:14 step:13331 [D loss: 0.546715, acc.: 70.31%] [G loss: 0.404725]\n",
      "epoch:14 step:13332 [D loss: 0.648642, acc.: 66.41%] [G loss: 0.458788]\n",
      "epoch:14 step:13333 [D loss: 0.598053, acc.: 64.06%] [G loss: 0.474772]\n",
      "epoch:14 step:13334 [D loss: 0.524034, acc.: 71.88%] [G loss: 0.621878]\n",
      "epoch:14 step:13335 [D loss: 0.555049, acc.: 71.09%] [G loss: 0.506597]\n",
      "epoch:14 step:13336 [D loss: 0.512559, acc.: 76.56%] [G loss: 0.614821]\n",
      "epoch:14 step:13337 [D loss: 0.442439, acc.: 83.59%] [G loss: 0.812007]\n",
      "epoch:14 step:13338 [D loss: 0.644183, acc.: 69.53%] [G loss: 0.780051]\n",
      "epoch:14 step:13339 [D loss: 0.520171, acc.: 71.88%] [G loss: 0.645632]\n",
      "epoch:14 step:13340 [D loss: 0.503062, acc.: 73.44%] [G loss: 0.573468]\n",
      "epoch:14 step:13341 [D loss: 0.537679, acc.: 74.22%] [G loss: 0.687932]\n",
      "epoch:14 step:13342 [D loss: 0.548851, acc.: 73.44%] [G loss: 0.579911]\n",
      "epoch:14 step:13343 [D loss: 0.525271, acc.: 73.44%] [G loss: 0.592012]\n",
      "epoch:14 step:13344 [D loss: 0.577329, acc.: 63.28%] [G loss: 0.517417]\n",
      "epoch:14 step:13345 [D loss: 0.532367, acc.: 71.09%] [G loss: 0.599280]\n",
      "epoch:14 step:13346 [D loss: 0.581959, acc.: 64.84%] [G loss: 0.574002]\n",
      "epoch:14 step:13347 [D loss: 0.560259, acc.: 72.66%] [G loss: 0.630035]\n",
      "epoch:14 step:13348 [D loss: 0.521996, acc.: 72.66%] [G loss: 0.627882]\n",
      "epoch:14 step:13349 [D loss: 0.491361, acc.: 73.44%] [G loss: 0.615755]\n",
      "epoch:14 step:13350 [D loss: 0.458503, acc.: 78.91%] [G loss: 0.887848]\n",
      "epoch:14 step:13351 [D loss: 0.560669, acc.: 68.75%] [G loss: 0.613051]\n",
      "epoch:14 step:13352 [D loss: 0.592077, acc.: 65.62%] [G loss: 0.621743]\n",
      "epoch:14 step:13353 [D loss: 0.555793, acc.: 71.09%] [G loss: 0.615747]\n",
      "epoch:14 step:13354 [D loss: 0.556803, acc.: 76.56%] [G loss: 0.412763]\n",
      "epoch:14 step:13355 [D loss: 0.530771, acc.: 70.31%] [G loss: 0.615857]\n",
      "epoch:14 step:13356 [D loss: 0.620041, acc.: 65.62%] [G loss: 0.487821]\n",
      "epoch:14 step:13357 [D loss: 0.554842, acc.: 68.75%] [G loss: 0.616796]\n",
      "epoch:14 step:13358 [D loss: 0.578884, acc.: 66.41%] [G loss: 0.536910]\n",
      "epoch:14 step:13359 [D loss: 0.546250, acc.: 70.31%] [G loss: 0.643635]\n",
      "epoch:14 step:13360 [D loss: 0.559826, acc.: 72.66%] [G loss: 0.731371]\n",
      "epoch:14 step:13361 [D loss: 0.520458, acc.: 73.44%] [G loss: 0.726871]\n",
      "epoch:14 step:13362 [D loss: 0.502048, acc.: 71.88%] [G loss: 0.754088]\n",
      "epoch:14 step:13363 [D loss: 0.530880, acc.: 70.31%] [G loss: 0.574481]\n",
      "epoch:14 step:13364 [D loss: 0.566528, acc.: 67.97%] [G loss: 0.636414]\n",
      "epoch:14 step:13365 [D loss: 0.536649, acc.: 70.31%] [G loss: 0.671106]\n",
      "epoch:14 step:13366 [D loss: 0.485345, acc.: 72.66%] [G loss: 0.671444]\n",
      "epoch:14 step:13367 [D loss: 0.600212, acc.: 68.75%] [G loss: 0.588155]\n",
      "epoch:14 step:13368 [D loss: 0.598884, acc.: 65.62%] [G loss: 0.675076]\n",
      "epoch:14 step:13369 [D loss: 0.610323, acc.: 64.06%] [G loss: 0.615179]\n",
      "epoch:14 step:13370 [D loss: 0.576893, acc.: 64.84%] [G loss: 0.585001]\n",
      "epoch:14 step:13371 [D loss: 0.577306, acc.: 72.66%] [G loss: 0.613245]\n",
      "epoch:14 step:13372 [D loss: 0.524572, acc.: 70.31%] [G loss: 0.537040]\n",
      "epoch:14 step:13373 [D loss: 0.568824, acc.: 69.53%] [G loss: 0.576007]\n",
      "epoch:14 step:13374 [D loss: 0.582504, acc.: 68.75%] [G loss: 0.655394]\n",
      "epoch:14 step:13375 [D loss: 0.522795, acc.: 67.97%] [G loss: 0.750252]\n",
      "epoch:14 step:13376 [D loss: 0.545759, acc.: 67.19%] [G loss: 0.561546]\n",
      "epoch:14 step:13377 [D loss: 0.537759, acc.: 64.84%] [G loss: 0.590767]\n",
      "epoch:14 step:13378 [D loss: 0.595312, acc.: 64.06%] [G loss: 0.441890]\n",
      "epoch:14 step:13379 [D loss: 0.529824, acc.: 76.56%] [G loss: 0.504209]\n",
      "epoch:14 step:13380 [D loss: 0.520551, acc.: 71.88%] [G loss: 0.665127]\n",
      "epoch:14 step:13381 [D loss: 0.647261, acc.: 66.41%] [G loss: 0.511304]\n",
      "epoch:14 step:13382 [D loss: 0.512127, acc.: 74.22%] [G loss: 0.598931]\n",
      "epoch:14 step:13383 [D loss: 0.581023, acc.: 67.97%] [G loss: 0.471354]\n",
      "epoch:14 step:13384 [D loss: 0.530861, acc.: 71.09%] [G loss: 0.563877]\n",
      "epoch:14 step:13385 [D loss: 0.605028, acc.: 63.28%] [G loss: 0.565106]\n",
      "epoch:14 step:13386 [D loss: 0.515419, acc.: 73.44%] [G loss: 0.557738]\n",
      "epoch:14 step:13387 [D loss: 0.517304, acc.: 74.22%] [G loss: 0.502223]\n",
      "epoch:14 step:13388 [D loss: 0.566772, acc.: 69.53%] [G loss: 0.522305]\n",
      "epoch:14 step:13389 [D loss: 0.484797, acc.: 75.78%] [G loss: 0.600931]\n",
      "epoch:14 step:13390 [D loss: 0.555297, acc.: 70.31%] [G loss: 0.649336]\n",
      "epoch:14 step:13391 [D loss: 0.543033, acc.: 71.88%] [G loss: 0.699018]\n",
      "epoch:14 step:13392 [D loss: 0.532798, acc.: 73.44%] [G loss: 0.595222]\n",
      "epoch:14 step:13393 [D loss: 0.562594, acc.: 67.19%] [G loss: 0.543101]\n",
      "epoch:14 step:13394 [D loss: 0.471902, acc.: 78.91%] [G loss: 0.582303]\n",
      "epoch:14 step:13395 [D loss: 0.693493, acc.: 57.81%] [G loss: 0.525395]\n",
      "epoch:14 step:13396 [D loss: 0.641324, acc.: 57.81%] [G loss: 0.487707]\n",
      "epoch:14 step:13397 [D loss: 0.607041, acc.: 63.28%] [G loss: 0.519500]\n",
      "epoch:14 step:13398 [D loss: 0.531338, acc.: 73.44%] [G loss: 0.733176]\n",
      "epoch:14 step:13399 [D loss: 0.587227, acc.: 67.19%] [G loss: 0.563535]\n",
      "epoch:14 step:13400 [D loss: 0.558789, acc.: 74.22%] [G loss: 0.488851]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.305680\n",
      "FID: 59.940018\n",
      "0 = 12.954114649677262\n",
      "1 = 0.0872064024420919\n",
      "2 = 0.8955000042915344\n",
      "3 = 0.8399999737739563\n",
      "4 = 0.9509999752044678\n",
      "5 = 0.9448819160461426\n",
      "6 = 0.8399999737739563\n",
      "7 = 8.848133681607285\n",
      "8 = 0.17356904685860547\n",
      "9 = 0.7168999910354614\n",
      "10 = 0.7081999778747559\n",
      "11 = 0.725600004196167\n",
      "12 = 0.7207409143447876\n",
      "13 = 0.7081999778747559\n",
      "14 = 6.305701732635498\n",
      "15 = 6.861990928649902\n",
      "16 = 0.40213847160339355\n",
      "17 = 6.305680274963379\n",
      "18 = 59.94001770019531\n",
      "epoch:14 step:13401 [D loss: 0.500946, acc.: 75.00%] [G loss: 0.592862]\n",
      "epoch:14 step:13402 [D loss: 0.539052, acc.: 73.44%] [G loss: 0.586969]\n",
      "epoch:14 step:13403 [D loss: 0.554336, acc.: 65.62%] [G loss: 0.488122]\n",
      "epoch:14 step:13404 [D loss: 0.479330, acc.: 75.78%] [G loss: 0.574256]\n",
      "epoch:14 step:13405 [D loss: 0.534383, acc.: 71.09%] [G loss: 0.599422]\n",
      "epoch:14 step:13406 [D loss: 0.546993, acc.: 70.31%] [G loss: 0.501333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13407 [D loss: 0.592578, acc.: 69.53%] [G loss: 0.587687]\n",
      "epoch:14 step:13408 [D loss: 0.585930, acc.: 68.75%] [G loss: 0.574941]\n",
      "epoch:14 step:13409 [D loss: 0.578749, acc.: 70.31%] [G loss: 0.528497]\n",
      "epoch:14 step:13410 [D loss: 0.548376, acc.: 71.88%] [G loss: 0.560052]\n",
      "epoch:14 step:13411 [D loss: 0.557437, acc.: 68.75%] [G loss: 0.601560]\n",
      "epoch:14 step:13412 [D loss: 0.579164, acc.: 64.06%] [G loss: 0.405317]\n",
      "epoch:14 step:13413 [D loss: 0.546146, acc.: 71.88%] [G loss: 0.470675]\n",
      "epoch:14 step:13414 [D loss: 0.464533, acc.: 75.78%] [G loss: 0.599387]\n",
      "epoch:14 step:13415 [D loss: 0.584325, acc.: 64.06%] [G loss: 0.487188]\n",
      "epoch:14 step:13416 [D loss: 0.527366, acc.: 74.22%] [G loss: 0.664130]\n",
      "epoch:14 step:13417 [D loss: 0.521604, acc.: 69.53%] [G loss: 0.643004]\n",
      "epoch:14 step:13418 [D loss: 0.506035, acc.: 76.56%] [G loss: 0.607153]\n",
      "epoch:14 step:13419 [D loss: 0.629870, acc.: 69.53%] [G loss: 0.583099]\n",
      "epoch:14 step:13420 [D loss: 0.537640, acc.: 67.97%] [G loss: 0.611022]\n",
      "epoch:14 step:13421 [D loss: 0.590099, acc.: 71.88%] [G loss: 0.624100]\n",
      "epoch:14 step:13422 [D loss: 0.503769, acc.: 73.44%] [G loss: 0.657989]\n",
      "epoch:14 step:13423 [D loss: 0.518386, acc.: 74.22%] [G loss: 0.608022]\n",
      "epoch:14 step:13424 [D loss: 0.563555, acc.: 70.31%] [G loss: 0.741760]\n",
      "epoch:14 step:13425 [D loss: 0.493450, acc.: 77.34%] [G loss: 0.655960]\n",
      "epoch:14 step:13426 [D loss: 0.659091, acc.: 58.59%] [G loss: 0.518323]\n",
      "epoch:14 step:13427 [D loss: 0.494052, acc.: 78.12%] [G loss: 0.608106]\n",
      "epoch:14 step:13428 [D loss: 0.549739, acc.: 71.09%] [G loss: 0.769720]\n",
      "epoch:14 step:13429 [D loss: 0.503250, acc.: 72.66%] [G loss: 0.690655]\n",
      "epoch:14 step:13430 [D loss: 0.501123, acc.: 72.66%] [G loss: 0.764075]\n",
      "epoch:14 step:13431 [D loss: 0.505893, acc.: 75.78%] [G loss: 0.818483]\n",
      "epoch:14 step:13432 [D loss: 0.432976, acc.: 80.47%] [G loss: 0.930365]\n",
      "epoch:14 step:13433 [D loss: 0.492343, acc.: 73.44%] [G loss: 0.770625]\n",
      "epoch:14 step:13434 [D loss: 0.676793, acc.: 66.41%] [G loss: 0.648704]\n",
      "epoch:14 step:13435 [D loss: 0.618351, acc.: 62.50%] [G loss: 0.694934]\n",
      "epoch:14 step:13436 [D loss: 0.526980, acc.: 71.88%] [G loss: 0.575828]\n",
      "epoch:14 step:13437 [D loss: 0.551259, acc.: 67.97%] [G loss: 0.555239]\n",
      "epoch:14 step:13438 [D loss: 0.581463, acc.: 67.19%] [G loss: 0.689083]\n",
      "epoch:14 step:13439 [D loss: 0.452982, acc.: 79.69%] [G loss: 0.807589]\n",
      "epoch:14 step:13440 [D loss: 0.553127, acc.: 71.09%] [G loss: 0.716693]\n",
      "epoch:14 step:13441 [D loss: 0.621391, acc.: 61.72%] [G loss: 0.631111]\n",
      "epoch:14 step:13442 [D loss: 0.580831, acc.: 71.88%] [G loss: 0.532854]\n",
      "epoch:14 step:13443 [D loss: 0.487182, acc.: 73.44%] [G loss: 0.779066]\n",
      "epoch:14 step:13444 [D loss: 0.473300, acc.: 77.34%] [G loss: 0.780253]\n",
      "epoch:14 step:13445 [D loss: 0.587681, acc.: 70.31%] [G loss: 0.755364]\n",
      "epoch:14 step:13446 [D loss: 0.496706, acc.: 73.44%] [G loss: 0.819859]\n",
      "epoch:14 step:13447 [D loss: 0.582590, acc.: 66.41%] [G loss: 0.789419]\n",
      "epoch:14 step:13448 [D loss: 0.595325, acc.: 67.19%] [G loss: 0.580373]\n",
      "epoch:14 step:13449 [D loss: 0.563026, acc.: 68.75%] [G loss: 0.611727]\n",
      "epoch:14 step:13450 [D loss: 0.515081, acc.: 71.88%] [G loss: 0.506158]\n",
      "epoch:14 step:13451 [D loss: 0.489374, acc.: 71.88%] [G loss: 0.452938]\n",
      "epoch:14 step:13452 [D loss: 0.476481, acc.: 78.12%] [G loss: 0.615501]\n",
      "epoch:14 step:13453 [D loss: 0.521622, acc.: 72.66%] [G loss: 0.683859]\n",
      "epoch:14 step:13454 [D loss: 0.491581, acc.: 78.91%] [G loss: 0.662324]\n",
      "epoch:14 step:13455 [D loss: 0.545735, acc.: 71.09%] [G loss: 0.767092]\n",
      "epoch:14 step:13456 [D loss: 0.577625, acc.: 70.31%] [G loss: 0.605687]\n",
      "epoch:14 step:13457 [D loss: 0.535642, acc.: 69.53%] [G loss: 0.618403]\n",
      "epoch:14 step:13458 [D loss: 0.505605, acc.: 74.22%] [G loss: 0.692992]\n",
      "epoch:14 step:13459 [D loss: 0.563210, acc.: 69.53%] [G loss: 0.733662]\n",
      "epoch:14 step:13460 [D loss: 0.705290, acc.: 57.03%] [G loss: 0.359533]\n",
      "epoch:14 step:13461 [D loss: 0.532731, acc.: 68.75%] [G loss: 0.572467]\n",
      "epoch:14 step:13462 [D loss: 0.473653, acc.: 75.00%] [G loss: 0.735398]\n",
      "epoch:14 step:13463 [D loss: 0.574175, acc.: 67.19%] [G loss: 0.753978]\n",
      "epoch:14 step:13464 [D loss: 0.517738, acc.: 71.09%] [G loss: 1.051636]\n",
      "epoch:14 step:13465 [D loss: 0.422360, acc.: 78.12%] [G loss: 1.062562]\n",
      "epoch:14 step:13466 [D loss: 0.662817, acc.: 64.84%] [G loss: 0.790237]\n",
      "epoch:14 step:13467 [D loss: 0.683039, acc.: 57.03%] [G loss: 0.572891]\n",
      "epoch:14 step:13468 [D loss: 0.525054, acc.: 78.91%] [G loss: 0.567172]\n",
      "epoch:14 step:13469 [D loss: 0.558217, acc.: 70.31%] [G loss: 0.631415]\n",
      "epoch:14 step:13470 [D loss: 0.507563, acc.: 75.78%] [G loss: 0.592853]\n",
      "epoch:14 step:13471 [D loss: 0.583615, acc.: 65.62%] [G loss: 0.675663]\n",
      "epoch:14 step:13472 [D loss: 0.397737, acc.: 79.69%] [G loss: 0.868235]\n",
      "epoch:14 step:13473 [D loss: 0.541772, acc.: 71.88%] [G loss: 0.735667]\n",
      "epoch:14 step:13474 [D loss: 0.543607, acc.: 72.66%] [G loss: 0.710813]\n",
      "epoch:14 step:13475 [D loss: 0.446727, acc.: 76.56%] [G loss: 0.808384]\n",
      "epoch:14 step:13476 [D loss: 0.438379, acc.: 78.91%] [G loss: 0.759322]\n",
      "epoch:14 step:13477 [D loss: 0.473900, acc.: 77.34%] [G loss: 0.725624]\n",
      "epoch:14 step:13478 [D loss: 0.511725, acc.: 74.22%] [G loss: 0.738042]\n",
      "epoch:14 step:13479 [D loss: 0.530839, acc.: 73.44%] [G loss: 0.809769]\n",
      "epoch:14 step:13480 [D loss: 0.596551, acc.: 66.41%] [G loss: 0.659495]\n",
      "epoch:14 step:13481 [D loss: 0.595923, acc.: 65.62%] [G loss: 0.497065]\n",
      "epoch:14 step:13482 [D loss: 0.526003, acc.: 72.66%] [G loss: 0.690882]\n",
      "epoch:14 step:13483 [D loss: 0.543016, acc.: 69.53%] [G loss: 0.654209]\n",
      "epoch:14 step:13484 [D loss: 0.549829, acc.: 68.75%] [G loss: 0.565747]\n",
      "epoch:14 step:13485 [D loss: 0.587598, acc.: 69.53%] [G loss: 0.627577]\n",
      "epoch:14 step:13486 [D loss: 0.564646, acc.: 67.97%] [G loss: 0.629442]\n",
      "epoch:14 step:13487 [D loss: 0.517531, acc.: 72.66%] [G loss: 0.574785]\n",
      "epoch:14 step:13488 [D loss: 0.566822, acc.: 72.66%] [G loss: 0.628452]\n",
      "epoch:14 step:13489 [D loss: 0.462455, acc.: 77.34%] [G loss: 0.685555]\n",
      "epoch:14 step:13490 [D loss: 0.549852, acc.: 71.88%] [G loss: 0.700672]\n",
      "epoch:14 step:13491 [D loss: 0.568478, acc.: 64.84%] [G loss: 0.536675]\n",
      "epoch:14 step:13492 [D loss: 0.485609, acc.: 71.09%] [G loss: 0.664918]\n",
      "epoch:14 step:13493 [D loss: 0.597206, acc.: 66.41%] [G loss: 0.711325]\n",
      "epoch:14 step:13494 [D loss: 0.684626, acc.: 60.94%] [G loss: 0.471677]\n",
      "epoch:14 step:13495 [D loss: 0.544725, acc.: 72.66%] [G loss: 0.587986]\n",
      "epoch:14 step:13496 [D loss: 0.522443, acc.: 71.09%] [G loss: 0.538937]\n",
      "epoch:14 step:13497 [D loss: 0.585597, acc.: 65.62%] [G loss: 0.492660]\n",
      "epoch:14 step:13498 [D loss: 0.542366, acc.: 71.88%] [G loss: 0.549945]\n",
      "epoch:14 step:13499 [D loss: 0.493347, acc.: 72.66%] [G loss: 0.630919]\n",
      "epoch:14 step:13500 [D loss: 0.503092, acc.: 74.22%] [G loss: 0.601321]\n",
      "epoch:14 step:13501 [D loss: 0.565265, acc.: 73.44%] [G loss: 0.577437]\n",
      "epoch:14 step:13502 [D loss: 0.550053, acc.: 71.09%] [G loss: 0.665551]\n",
      "epoch:14 step:13503 [D loss: 0.473852, acc.: 74.22%] [G loss: 0.742987]\n",
      "epoch:14 step:13504 [D loss: 0.612443, acc.: 67.19%] [G loss: 0.583980]\n",
      "epoch:14 step:13505 [D loss: 0.538186, acc.: 70.31%] [G loss: 0.425585]\n",
      "epoch:14 step:13506 [D loss: 0.544096, acc.: 73.44%] [G loss: 0.579730]\n",
      "epoch:14 step:13507 [D loss: 0.538796, acc.: 69.53%] [G loss: 0.640182]\n",
      "epoch:14 step:13508 [D loss: 0.581907, acc.: 67.19%] [G loss: 0.449355]\n",
      "epoch:14 step:13509 [D loss: 0.533397, acc.: 71.09%] [G loss: 0.631183]\n",
      "epoch:14 step:13510 [D loss: 0.542032, acc.: 70.31%] [G loss: 0.712055]\n",
      "epoch:14 step:13511 [D loss: 0.585501, acc.: 70.31%] [G loss: 0.659907]\n",
      "epoch:14 step:13512 [D loss: 0.596471, acc.: 64.84%] [G loss: 0.500419]\n",
      "epoch:14 step:13513 [D loss: 0.510906, acc.: 74.22%] [G loss: 0.664134]\n",
      "epoch:14 step:13514 [D loss: 0.581773, acc.: 67.97%] [G loss: 0.531773]\n",
      "epoch:14 step:13515 [D loss: 0.593943, acc.: 65.62%] [G loss: 0.497939]\n",
      "epoch:14 step:13516 [D loss: 0.517124, acc.: 75.78%] [G loss: 0.888216]\n",
      "epoch:14 step:13517 [D loss: 0.569528, acc.: 69.53%] [G loss: 0.899126]\n",
      "epoch:14 step:13518 [D loss: 0.640677, acc.: 60.16%] [G loss: 0.559296]\n",
      "epoch:14 step:13519 [D loss: 0.583043, acc.: 59.38%] [G loss: 0.502926]\n",
      "epoch:14 step:13520 [D loss: 0.563615, acc.: 69.53%] [G loss: 0.522879]\n",
      "epoch:14 step:13521 [D loss: 0.472216, acc.: 78.91%] [G loss: 0.738520]\n",
      "epoch:14 step:13522 [D loss: 0.599181, acc.: 64.06%] [G loss: 0.528693]\n",
      "epoch:14 step:13523 [D loss: 0.548032, acc.: 68.75%] [G loss: 0.581040]\n",
      "epoch:14 step:13524 [D loss: 0.505095, acc.: 70.31%] [G loss: 0.652354]\n",
      "epoch:14 step:13525 [D loss: 0.593458, acc.: 64.06%] [G loss: 0.622941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13526 [D loss: 0.581974, acc.: 64.84%] [G loss: 0.655560]\n",
      "epoch:14 step:13527 [D loss: 0.579950, acc.: 65.62%] [G loss: 0.592341]\n",
      "epoch:14 step:13528 [D loss: 0.560972, acc.: 67.19%] [G loss: 0.618182]\n",
      "epoch:14 step:13529 [D loss: 0.577108, acc.: 68.75%] [G loss: 0.515066]\n",
      "epoch:14 step:13530 [D loss: 0.611187, acc.: 65.62%] [G loss: 0.524948]\n",
      "epoch:14 step:13531 [D loss: 0.524474, acc.: 71.88%] [G loss: 0.498014]\n",
      "epoch:14 step:13532 [D loss: 0.553406, acc.: 66.41%] [G loss: 0.592290]\n",
      "epoch:14 step:13533 [D loss: 0.551309, acc.: 71.88%] [G loss: 0.503502]\n",
      "epoch:14 step:13534 [D loss: 0.463222, acc.: 79.69%] [G loss: 0.718635]\n",
      "epoch:14 step:13535 [D loss: 0.557506, acc.: 67.19%] [G loss: 0.632797]\n",
      "epoch:14 step:13536 [D loss: 0.664082, acc.: 61.72%] [G loss: 0.469294]\n",
      "epoch:14 step:13537 [D loss: 0.583604, acc.: 67.97%] [G loss: 0.499409]\n",
      "epoch:14 step:13538 [D loss: 0.579585, acc.: 60.94%] [G loss: 0.507059]\n",
      "epoch:14 step:13539 [D loss: 0.613462, acc.: 67.97%] [G loss: 0.632225]\n",
      "epoch:14 step:13540 [D loss: 0.605424, acc.: 72.66%] [G loss: 0.640941]\n",
      "epoch:14 step:13541 [D loss: 0.591407, acc.: 65.62%] [G loss: 0.518710]\n",
      "epoch:14 step:13542 [D loss: 0.584958, acc.: 65.62%] [G loss: 0.569165]\n",
      "epoch:14 step:13543 [D loss: 0.499116, acc.: 78.12%] [G loss: 0.504153]\n",
      "epoch:14 step:13544 [D loss: 0.524266, acc.: 71.09%] [G loss: 0.602226]\n",
      "epoch:14 step:13545 [D loss: 0.539330, acc.: 67.97%] [G loss: 0.660026]\n",
      "epoch:14 step:13546 [D loss: 0.522464, acc.: 71.09%] [G loss: 0.773375]\n",
      "epoch:14 step:13547 [D loss: 0.471230, acc.: 78.91%] [G loss: 0.779245]\n",
      "epoch:14 step:13548 [D loss: 0.481688, acc.: 76.56%] [G loss: 0.883861]\n",
      "epoch:14 step:13549 [D loss: 0.541458, acc.: 72.66%] [G loss: 0.615873]\n",
      "epoch:14 step:13550 [D loss: 0.591296, acc.: 64.06%] [G loss: 0.642795]\n",
      "epoch:14 step:13551 [D loss: 0.625616, acc.: 61.72%] [G loss: 0.520878]\n",
      "epoch:14 step:13552 [D loss: 0.565499, acc.: 68.75%] [G loss: 0.524595]\n",
      "epoch:14 step:13553 [D loss: 0.560609, acc.: 74.22%] [G loss: 0.644287]\n",
      "epoch:14 step:13554 [D loss: 0.515250, acc.: 70.31%] [G loss: 0.689219]\n",
      "epoch:14 step:13555 [D loss: 0.612633, acc.: 63.28%] [G loss: 0.641014]\n",
      "epoch:14 step:13556 [D loss: 0.562415, acc.: 65.62%] [G loss: 0.595380]\n",
      "epoch:14 step:13557 [D loss: 0.512944, acc.: 74.22%] [G loss: 0.634267]\n",
      "epoch:14 step:13558 [D loss: 0.439790, acc.: 79.69%] [G loss: 0.838397]\n",
      "epoch:14 step:13559 [D loss: 0.555192, acc.: 69.53%] [G loss: 0.761984]\n",
      "epoch:14 step:13560 [D loss: 0.585054, acc.: 67.19%] [G loss: 0.674370]\n",
      "epoch:14 step:13561 [D loss: 0.536929, acc.: 72.66%] [G loss: 0.641214]\n",
      "epoch:14 step:13562 [D loss: 0.542187, acc.: 68.75%] [G loss: 0.752896]\n",
      "epoch:14 step:13563 [D loss: 0.570355, acc.: 66.41%] [G loss: 0.680626]\n",
      "epoch:14 step:13564 [D loss: 0.505045, acc.: 75.78%] [G loss: 0.583065]\n",
      "epoch:14 step:13565 [D loss: 0.581914, acc.: 66.41%] [G loss: 0.764934]\n",
      "epoch:14 step:13566 [D loss: 0.521121, acc.: 73.44%] [G loss: 0.885715]\n",
      "epoch:14 step:13567 [D loss: 0.540380, acc.: 75.00%] [G loss: 0.742238]\n",
      "epoch:14 step:13568 [D loss: 0.493556, acc.: 74.22%] [G loss: 0.729765]\n",
      "epoch:14 step:13569 [D loss: 0.410694, acc.: 82.81%] [G loss: 0.807043]\n",
      "epoch:14 step:13570 [D loss: 0.485401, acc.: 75.78%] [G loss: 0.862684]\n",
      "epoch:14 step:13571 [D loss: 0.532215, acc.: 73.44%] [G loss: 0.658245]\n",
      "epoch:14 step:13572 [D loss: 0.538684, acc.: 74.22%] [G loss: 0.694681]\n",
      "epoch:14 step:13573 [D loss: 0.603302, acc.: 60.16%] [G loss: 0.623740]\n",
      "epoch:14 step:13574 [D loss: 0.633847, acc.: 65.62%] [G loss: 0.584427]\n",
      "epoch:14 step:13575 [D loss: 0.514948, acc.: 75.00%] [G loss: 0.619201]\n",
      "epoch:14 step:13576 [D loss: 0.607280, acc.: 64.06%] [G loss: 0.529441]\n",
      "epoch:14 step:13577 [D loss: 0.567672, acc.: 64.84%] [G loss: 0.660657]\n",
      "epoch:14 step:13578 [D loss: 0.516439, acc.: 75.78%] [G loss: 0.660767]\n",
      "epoch:14 step:13579 [D loss: 0.533136, acc.: 67.97%] [G loss: 0.726894]\n",
      "epoch:14 step:13580 [D loss: 0.602587, acc.: 67.97%] [G loss: 0.530047]\n",
      "epoch:14 step:13581 [D loss: 0.573333, acc.: 65.62%] [G loss: 0.546919]\n",
      "epoch:14 step:13582 [D loss: 0.541529, acc.: 72.66%] [G loss: 0.677473]\n",
      "epoch:14 step:13583 [D loss: 0.601916, acc.: 67.97%] [G loss: 0.489212]\n",
      "epoch:14 step:13584 [D loss: 0.580102, acc.: 61.72%] [G loss: 0.439379]\n",
      "epoch:14 step:13585 [D loss: 0.534901, acc.: 70.31%] [G loss: 0.600050]\n",
      "epoch:14 step:13586 [D loss: 0.550343, acc.: 71.09%] [G loss: 0.724352]\n",
      "epoch:14 step:13587 [D loss: 0.581757, acc.: 63.28%] [G loss: 0.679548]\n",
      "epoch:14 step:13588 [D loss: 0.531477, acc.: 72.66%] [G loss: 0.693079]\n",
      "epoch:14 step:13589 [D loss: 0.435972, acc.: 79.69%] [G loss: 0.974270]\n",
      "epoch:14 step:13590 [D loss: 0.437367, acc.: 79.69%] [G loss: 0.918893]\n",
      "epoch:14 step:13591 [D loss: 0.664400, acc.: 56.25%] [G loss: 0.696388]\n",
      "epoch:14 step:13592 [D loss: 0.540227, acc.: 67.19%] [G loss: 0.742822]\n",
      "epoch:14 step:13593 [D loss: 0.466366, acc.: 79.69%] [G loss: 0.775830]\n",
      "epoch:14 step:13594 [D loss: 0.576352, acc.: 71.09%] [G loss: 0.784862]\n",
      "epoch:14 step:13595 [D loss: 0.722179, acc.: 53.91%] [G loss: 0.517028]\n",
      "epoch:14 step:13596 [D loss: 0.581297, acc.: 65.62%] [G loss: 0.508292]\n",
      "epoch:14 step:13597 [D loss: 0.518435, acc.: 74.22%] [G loss: 0.559335]\n",
      "epoch:14 step:13598 [D loss: 0.584405, acc.: 70.31%] [G loss: 0.636881]\n",
      "epoch:14 step:13599 [D loss: 0.493007, acc.: 79.69%] [G loss: 0.696642]\n",
      "epoch:14 step:13600 [D loss: 0.624448, acc.: 60.16%] [G loss: 0.431422]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.467085\n",
      "FID: 55.126923\n",
      "0 = 12.888563562107095\n",
      "1 = 0.09039139729980142\n",
      "2 = 0.878600001335144\n",
      "3 = 0.8185999989509583\n",
      "4 = 0.9386000037193298\n",
      "5 = 0.9302272796630859\n",
      "6 = 0.8185999989509583\n",
      "7 = 8.535086216187501\n",
      "8 = 0.161894506509001\n",
      "9 = 0.70169997215271\n",
      "10 = 0.6891999840736389\n",
      "11 = 0.7142000198364258\n",
      "12 = 0.7068718075752258\n",
      "13 = 0.6891999840736389\n",
      "14 = 6.467105865478516\n",
      "15 = 6.764516830444336\n",
      "16 = 0.4010275602340698\n",
      "17 = 6.467084884643555\n",
      "18 = 55.126922607421875\n",
      "epoch:14 step:13601 [D loss: 0.544062, acc.: 73.44%] [G loss: 0.391342]\n",
      "epoch:14 step:13602 [D loss: 0.482876, acc.: 78.12%] [G loss: 0.566958]\n",
      "epoch:14 step:13603 [D loss: 0.515881, acc.: 72.66%] [G loss: 0.666539]\n",
      "epoch:14 step:13604 [D loss: 0.574554, acc.: 67.19%] [G loss: 0.634647]\n",
      "epoch:14 step:13605 [D loss: 0.546411, acc.: 68.75%] [G loss: 0.702385]\n",
      "epoch:14 step:13606 [D loss: 0.510101, acc.: 68.75%] [G loss: 0.559244]\n",
      "epoch:14 step:13607 [D loss: 0.548570, acc.: 71.09%] [G loss: 0.561393]\n",
      "epoch:14 step:13608 [D loss: 0.553497, acc.: 70.31%] [G loss: 0.613711]\n",
      "epoch:14 step:13609 [D loss: 0.518240, acc.: 73.44%] [G loss: 0.581755]\n",
      "epoch:14 step:13610 [D loss: 0.569129, acc.: 72.66%] [G loss: 0.689741]\n",
      "epoch:14 step:13611 [D loss: 0.577986, acc.: 67.19%] [G loss: 0.482901]\n",
      "epoch:14 step:13612 [D loss: 0.630498, acc.: 64.84%] [G loss: 0.440908]\n",
      "epoch:14 step:13613 [D loss: 0.501823, acc.: 75.78%] [G loss: 0.708489]\n",
      "epoch:14 step:13614 [D loss: 0.599882, acc.: 64.84%] [G loss: 0.500094]\n",
      "epoch:14 step:13615 [D loss: 0.558204, acc.: 70.31%] [G loss: 0.578840]\n",
      "epoch:14 step:13616 [D loss: 0.537874, acc.: 72.66%] [G loss: 0.525196]\n",
      "epoch:14 step:13617 [D loss: 0.494722, acc.: 75.78%] [G loss: 0.638698]\n",
      "epoch:14 step:13618 [D loss: 0.617334, acc.: 64.84%] [G loss: 0.601569]\n",
      "epoch:14 step:13619 [D loss: 0.593566, acc.: 67.97%] [G loss: 0.584178]\n",
      "epoch:14 step:13620 [D loss: 0.629760, acc.: 64.84%] [G loss: 0.440944]\n",
      "epoch:14 step:13621 [D loss: 0.481191, acc.: 77.34%] [G loss: 0.610865]\n",
      "epoch:14 step:13622 [D loss: 0.540822, acc.: 72.66%] [G loss: 0.540569]\n",
      "epoch:14 step:13623 [D loss: 0.506808, acc.: 75.78%] [G loss: 0.666691]\n",
      "epoch:14 step:13624 [D loss: 0.509140, acc.: 72.66%] [G loss: 0.601916]\n",
      "epoch:14 step:13625 [D loss: 0.607137, acc.: 70.31%] [G loss: 0.656279]\n",
      "epoch:14 step:13626 [D loss: 0.453053, acc.: 82.03%] [G loss: 0.867376]\n",
      "epoch:14 step:13627 [D loss: 0.498359, acc.: 76.56%] [G loss: 0.740083]\n",
      "epoch:14 step:13628 [D loss: 0.638467, acc.: 64.06%] [G loss: 0.654809]\n",
      "epoch:14 step:13629 [D loss: 0.625027, acc.: 64.84%] [G loss: 0.504693]\n",
      "epoch:14 step:13630 [D loss: 0.665340, acc.: 59.38%] [G loss: 0.460220]\n",
      "epoch:14 step:13631 [D loss: 0.544787, acc.: 68.75%] [G loss: 0.588802]\n",
      "epoch:14 step:13632 [D loss: 0.536858, acc.: 69.53%] [G loss: 0.578564]\n",
      "epoch:14 step:13633 [D loss: 0.525360, acc.: 72.66%] [G loss: 0.698566]\n",
      "epoch:14 step:13634 [D loss: 0.504218, acc.: 75.00%] [G loss: 0.639258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13635 [D loss: 0.481791, acc.: 76.56%] [G loss: 0.671371]\n",
      "epoch:14 step:13636 [D loss: 0.602371, acc.: 66.41%] [G loss: 0.577333]\n",
      "epoch:14 step:13637 [D loss: 0.500138, acc.: 79.69%] [G loss: 0.596155]\n",
      "epoch:14 step:13638 [D loss: 0.549459, acc.: 72.66%] [G loss: 0.574924]\n",
      "epoch:14 step:13639 [D loss: 0.514274, acc.: 74.22%] [G loss: 0.599063]\n",
      "epoch:14 step:13640 [D loss: 0.506408, acc.: 75.78%] [G loss: 0.776354]\n",
      "epoch:14 step:13641 [D loss: 0.483413, acc.: 79.69%] [G loss: 0.660678]\n",
      "epoch:14 step:13642 [D loss: 0.572454, acc.: 64.06%] [G loss: 0.603845]\n",
      "epoch:14 step:13643 [D loss: 0.559009, acc.: 75.00%] [G loss: 0.524944]\n",
      "epoch:14 step:13644 [D loss: 0.556596, acc.: 68.75%] [G loss: 0.489662]\n",
      "epoch:14 step:13645 [D loss: 0.556697, acc.: 68.75%] [G loss: 0.521506]\n",
      "epoch:14 step:13646 [D loss: 0.664589, acc.: 56.25%] [G loss: 0.571172]\n",
      "epoch:14 step:13647 [D loss: 0.555828, acc.: 69.53%] [G loss: 0.729166]\n",
      "epoch:14 step:13648 [D loss: 0.578246, acc.: 67.97%] [G loss: 0.652061]\n",
      "epoch:14 step:13649 [D loss: 0.630917, acc.: 59.38%] [G loss: 0.460208]\n",
      "epoch:14 step:13650 [D loss: 0.546158, acc.: 70.31%] [G loss: 0.556928]\n",
      "epoch:14 step:13651 [D loss: 0.562782, acc.: 74.22%] [G loss: 0.601746]\n",
      "epoch:14 step:13652 [D loss: 0.488576, acc.: 75.78%] [G loss: 0.712134]\n",
      "epoch:14 step:13653 [D loss: 0.620336, acc.: 60.94%] [G loss: 0.469236]\n",
      "epoch:14 step:13654 [D loss: 0.525407, acc.: 70.31%] [G loss: 0.517676]\n",
      "epoch:14 step:13655 [D loss: 0.582649, acc.: 67.97%] [G loss: 0.535038]\n",
      "epoch:14 step:13656 [D loss: 0.573773, acc.: 65.62%] [G loss: 0.404986]\n",
      "epoch:14 step:13657 [D loss: 0.569630, acc.: 71.88%] [G loss: 0.499534]\n",
      "epoch:14 step:13658 [D loss: 0.584052, acc.: 64.84%] [G loss: 0.596699]\n",
      "epoch:14 step:13659 [D loss: 0.531598, acc.: 71.88%] [G loss: 0.616867]\n",
      "epoch:14 step:13660 [D loss: 0.625431, acc.: 69.53%] [G loss: 0.542272]\n",
      "epoch:14 step:13661 [D loss: 0.519273, acc.: 75.78%] [G loss: 0.593081]\n",
      "epoch:14 step:13662 [D loss: 0.539072, acc.: 74.22%] [G loss: 0.635081]\n",
      "epoch:14 step:13663 [D loss: 0.561948, acc.: 71.88%] [G loss: 0.784138]\n",
      "epoch:14 step:13664 [D loss: 0.546568, acc.: 68.75%] [G loss: 0.684302]\n",
      "epoch:14 step:13665 [D loss: 0.554899, acc.: 67.97%] [G loss: 0.731907]\n",
      "epoch:14 step:13666 [D loss: 0.530593, acc.: 70.31%] [G loss: 0.601098]\n",
      "epoch:14 step:13667 [D loss: 0.507990, acc.: 74.22%] [G loss: 0.702640]\n",
      "epoch:14 step:13668 [D loss: 0.586172, acc.: 69.53%] [G loss: 0.540794]\n",
      "epoch:14 step:13669 [D loss: 0.525035, acc.: 73.44%] [G loss: 0.558638]\n",
      "epoch:14 step:13670 [D loss: 0.513914, acc.: 71.88%] [G loss: 0.627842]\n",
      "epoch:14 step:13671 [D loss: 0.640706, acc.: 62.50%] [G loss: 0.489987]\n",
      "epoch:14 step:13672 [D loss: 0.498146, acc.: 75.00%] [G loss: 0.655768]\n",
      "epoch:14 step:13673 [D loss: 0.486683, acc.: 79.69%] [G loss: 0.591883]\n",
      "epoch:14 step:13674 [D loss: 0.509387, acc.: 74.22%] [G loss: 0.615037]\n",
      "epoch:14 step:13675 [D loss: 0.577147, acc.: 74.22%] [G loss: 0.776529]\n",
      "epoch:14 step:13676 [D loss: 0.447752, acc.: 77.34%] [G loss: 0.770690]\n",
      "epoch:14 step:13677 [D loss: 0.644387, acc.: 60.94%] [G loss: 0.467122]\n",
      "epoch:14 step:13678 [D loss: 0.554417, acc.: 70.31%] [G loss: 0.580032]\n",
      "epoch:14 step:13679 [D loss: 0.529962, acc.: 72.66%] [G loss: 0.833630]\n",
      "epoch:14 step:13680 [D loss: 0.520768, acc.: 73.44%] [G loss: 0.599730]\n",
      "epoch:14 step:13681 [D loss: 0.597890, acc.: 66.41%] [G loss: 0.625725]\n",
      "epoch:14 step:13682 [D loss: 0.528505, acc.: 69.53%] [G loss: 0.735424]\n",
      "epoch:14 step:13683 [D loss: 0.560500, acc.: 68.75%] [G loss: 0.640173]\n",
      "epoch:14 step:13684 [D loss: 0.698439, acc.: 60.16%] [G loss: 0.524823]\n",
      "epoch:14 step:13685 [D loss: 0.537573, acc.: 69.53%] [G loss: 0.523377]\n",
      "epoch:14 step:13686 [D loss: 0.527104, acc.: 69.53%] [G loss: 0.589980]\n",
      "epoch:14 step:13687 [D loss: 0.563258, acc.: 67.97%] [G loss: 0.555103]\n",
      "epoch:14 step:13688 [D loss: 0.552754, acc.: 67.97%] [G loss: 0.532208]\n",
      "epoch:14 step:13689 [D loss: 0.517632, acc.: 74.22%] [G loss: 0.592341]\n",
      "epoch:14 step:13690 [D loss: 0.541666, acc.: 71.88%] [G loss: 0.540484]\n",
      "epoch:14 step:13691 [D loss: 0.490763, acc.: 75.78%] [G loss: 0.743891]\n",
      "epoch:14 step:13692 [D loss: 0.480035, acc.: 75.00%] [G loss: 0.915979]\n",
      "epoch:14 step:13693 [D loss: 0.487607, acc.: 77.34%] [G loss: 0.838438]\n",
      "epoch:14 step:13694 [D loss: 0.595574, acc.: 65.62%] [G loss: 0.639243]\n",
      "epoch:14 step:13695 [D loss: 0.521902, acc.: 71.09%] [G loss: 0.616545]\n",
      "epoch:14 step:13696 [D loss: 0.514944, acc.: 72.66%] [G loss: 0.537395]\n",
      "epoch:14 step:13697 [D loss: 0.555157, acc.: 68.75%] [G loss: 0.609053]\n",
      "epoch:14 step:13698 [D loss: 0.550534, acc.: 71.88%] [G loss: 0.604524]\n",
      "epoch:14 step:13699 [D loss: 0.528049, acc.: 73.44%] [G loss: 0.688253]\n",
      "epoch:14 step:13700 [D loss: 0.456567, acc.: 81.25%] [G loss: 0.698003]\n",
      "epoch:14 step:13701 [D loss: 0.567870, acc.: 68.75%] [G loss: 0.677548]\n",
      "epoch:14 step:13702 [D loss: 0.602444, acc.: 64.06%] [G loss: 0.534722]\n",
      "epoch:14 step:13703 [D loss: 0.487081, acc.: 77.34%] [G loss: 0.643687]\n",
      "epoch:14 step:13704 [D loss: 0.584781, acc.: 62.50%] [G loss: 0.542812]\n",
      "epoch:14 step:13705 [D loss: 0.592470, acc.: 67.19%] [G loss: 0.536200]\n",
      "epoch:14 step:13706 [D loss: 0.567498, acc.: 66.41%] [G loss: 0.627476]\n",
      "epoch:14 step:13707 [D loss: 0.529065, acc.: 68.75%] [G loss: 0.714265]\n",
      "epoch:14 step:13708 [D loss: 0.578353, acc.: 67.97%] [G loss: 0.674658]\n",
      "epoch:14 step:13709 [D loss: 0.583951, acc.: 68.75%] [G loss: 0.436614]\n",
      "epoch:14 step:13710 [D loss: 0.565002, acc.: 71.88%] [G loss: 0.638968]\n",
      "epoch:14 step:13711 [D loss: 0.522339, acc.: 73.44%] [G loss: 0.691388]\n",
      "epoch:14 step:13712 [D loss: 0.542297, acc.: 68.75%] [G loss: 0.687732]\n",
      "epoch:14 step:13713 [D loss: 0.564695, acc.: 68.75%] [G loss: 0.616624]\n",
      "epoch:14 step:13714 [D loss: 0.581355, acc.: 64.06%] [G loss: 0.510910]\n",
      "epoch:14 step:13715 [D loss: 0.545525, acc.: 70.31%] [G loss: 0.431507]\n",
      "epoch:14 step:13716 [D loss: 0.514952, acc.: 75.00%] [G loss: 0.548975]\n",
      "epoch:14 step:13717 [D loss: 0.537837, acc.: 68.75%] [G loss: 0.602862]\n",
      "epoch:14 step:13718 [D loss: 0.584747, acc.: 64.84%] [G loss: 0.560972]\n",
      "epoch:14 step:13719 [D loss: 0.539008, acc.: 70.31%] [G loss: 0.610548]\n",
      "epoch:14 step:13720 [D loss: 0.521169, acc.: 71.88%] [G loss: 0.599730]\n",
      "epoch:14 step:13721 [D loss: 0.490339, acc.: 77.34%] [G loss: 0.540886]\n",
      "epoch:14 step:13722 [D loss: 0.613411, acc.: 66.41%] [G loss: 0.561300]\n",
      "epoch:14 step:13723 [D loss: 0.462771, acc.: 78.91%] [G loss: 0.486177]\n",
      "epoch:14 step:13724 [D loss: 0.614984, acc.: 64.06%] [G loss: 0.402910]\n",
      "epoch:14 step:13725 [D loss: 0.538265, acc.: 71.09%] [G loss: 0.480235]\n",
      "epoch:14 step:13726 [D loss: 0.569391, acc.: 65.62%] [G loss: 0.645958]\n",
      "epoch:14 step:13727 [D loss: 0.509744, acc.: 74.22%] [G loss: 0.509126]\n",
      "epoch:14 step:13728 [D loss: 0.548705, acc.: 67.97%] [G loss: 0.549276]\n",
      "epoch:14 step:13729 [D loss: 0.595303, acc.: 67.19%] [G loss: 0.483144]\n",
      "epoch:14 step:13730 [D loss: 0.569219, acc.: 68.75%] [G loss: 0.683408]\n",
      "epoch:14 step:13731 [D loss: 0.528824, acc.: 69.53%] [G loss: 0.541032]\n",
      "epoch:14 step:13732 [D loss: 0.610962, acc.: 63.28%] [G loss: 0.641580]\n",
      "epoch:14 step:13733 [D loss: 0.534891, acc.: 70.31%] [G loss: 0.706866]\n",
      "epoch:14 step:13734 [D loss: 0.539292, acc.: 74.22%] [G loss: 0.650435]\n",
      "epoch:14 step:13735 [D loss: 0.505493, acc.: 74.22%] [G loss: 0.693232]\n",
      "epoch:14 step:13736 [D loss: 0.611711, acc.: 62.50%] [G loss: 0.560717]\n",
      "epoch:14 step:13737 [D loss: 0.548289, acc.: 68.75%] [G loss: 0.587133]\n",
      "epoch:14 step:13738 [D loss: 0.534063, acc.: 73.44%] [G loss: 0.664132]\n",
      "epoch:14 step:13739 [D loss: 0.592092, acc.: 64.06%] [G loss: 0.516684]\n",
      "epoch:14 step:13740 [D loss: 0.545839, acc.: 72.66%] [G loss: 0.631654]\n",
      "epoch:14 step:13741 [D loss: 0.478097, acc.: 75.00%] [G loss: 0.470723]\n",
      "epoch:14 step:13742 [D loss: 0.516418, acc.: 75.00%] [G loss: 0.881571]\n",
      "epoch:14 step:13743 [D loss: 0.619658, acc.: 62.50%] [G loss: 0.615831]\n",
      "epoch:14 step:13744 [D loss: 0.583317, acc.: 66.41%] [G loss: 0.512679]\n",
      "epoch:14 step:13745 [D loss: 0.560741, acc.: 66.41%] [G loss: 0.621751]\n",
      "epoch:14 step:13746 [D loss: 0.592627, acc.: 64.84%] [G loss: 0.711034]\n",
      "epoch:14 step:13747 [D loss: 0.543898, acc.: 75.78%] [G loss: 0.586798]\n",
      "epoch:14 step:13748 [D loss: 0.561053, acc.: 69.53%] [G loss: 0.593120]\n",
      "epoch:14 step:13749 [D loss: 0.532346, acc.: 75.00%] [G loss: 0.777386]\n",
      "epoch:14 step:13750 [D loss: 0.484886, acc.: 79.69%] [G loss: 0.712556]\n",
      "epoch:14 step:13751 [D loss: 0.499788, acc.: 78.12%] [G loss: 0.667438]\n",
      "epoch:14 step:13752 [D loss: 0.464135, acc.: 78.12%] [G loss: 0.783154]\n",
      "epoch:14 step:13753 [D loss: 0.523005, acc.: 72.66%] [G loss: 0.725882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13754 [D loss: 0.569914, acc.: 68.75%] [G loss: 0.610771]\n",
      "epoch:14 step:13755 [D loss: 0.520153, acc.: 78.12%] [G loss: 0.679517]\n",
      "epoch:14 step:13756 [D loss: 0.565744, acc.: 68.75%] [G loss: 0.507111]\n",
      "epoch:14 step:13757 [D loss: 0.515564, acc.: 75.00%] [G loss: 0.623728]\n",
      "epoch:14 step:13758 [D loss: 0.563331, acc.: 72.66%] [G loss: 0.713634]\n",
      "epoch:14 step:13759 [D loss: 0.511472, acc.: 72.66%] [G loss: 0.618897]\n",
      "epoch:14 step:13760 [D loss: 0.501380, acc.: 75.00%] [G loss: 0.686309]\n",
      "epoch:14 step:13761 [D loss: 0.512119, acc.: 75.00%] [G loss: 0.665691]\n",
      "epoch:14 step:13762 [D loss: 0.570567, acc.: 67.19%] [G loss: 0.636550]\n",
      "epoch:14 step:13763 [D loss: 0.527781, acc.: 71.09%] [G loss: 0.518790]\n",
      "epoch:14 step:13764 [D loss: 0.482285, acc.: 77.34%] [G loss: 0.582731]\n",
      "epoch:14 step:13765 [D loss: 0.442934, acc.: 79.69%] [G loss: 0.685572]\n",
      "epoch:14 step:13766 [D loss: 0.367410, acc.: 84.38%] [G loss: 0.798879]\n",
      "epoch:14 step:13767 [D loss: 0.512982, acc.: 75.00%] [G loss: 0.783614]\n",
      "epoch:14 step:13768 [D loss: 0.551703, acc.: 70.31%] [G loss: 0.916337]\n",
      "epoch:14 step:13769 [D loss: 0.517171, acc.: 75.00%] [G loss: 0.847057]\n",
      "epoch:14 step:13770 [D loss: 0.619684, acc.: 64.06%] [G loss: 0.608544]\n",
      "epoch:14 step:13771 [D loss: 0.623327, acc.: 65.62%] [G loss: 0.498779]\n",
      "epoch:14 step:13772 [D loss: 0.497932, acc.: 75.00%] [G loss: 0.646008]\n",
      "epoch:14 step:13773 [D loss: 0.564512, acc.: 65.62%] [G loss: 0.645578]\n",
      "epoch:14 step:13774 [D loss: 0.600404, acc.: 67.19%] [G loss: 0.508593]\n",
      "epoch:14 step:13775 [D loss: 0.542625, acc.: 69.53%] [G loss: 0.605380]\n",
      "epoch:14 step:13776 [D loss: 0.575308, acc.: 68.75%] [G loss: 0.672957]\n",
      "epoch:14 step:13777 [D loss: 0.495702, acc.: 73.44%] [G loss: 0.631178]\n",
      "epoch:14 step:13778 [D loss: 0.507761, acc.: 71.88%] [G loss: 0.587160]\n",
      "epoch:14 step:13779 [D loss: 0.507894, acc.: 69.53%] [G loss: 0.755656]\n",
      "epoch:14 step:13780 [D loss: 0.536372, acc.: 68.75%] [G loss: 0.704698]\n",
      "epoch:14 step:13781 [D loss: 0.607797, acc.: 64.84%] [G loss: 0.575773]\n",
      "epoch:14 step:13782 [D loss: 0.576139, acc.: 66.41%] [G loss: 0.680667]\n",
      "epoch:14 step:13783 [D loss: 0.591164, acc.: 67.19%] [G loss: 0.608562]\n",
      "epoch:14 step:13784 [D loss: 0.567927, acc.: 73.44%] [G loss: 0.536346]\n",
      "epoch:14 step:13785 [D loss: 0.534445, acc.: 72.66%] [G loss: 0.629482]\n",
      "epoch:14 step:13786 [D loss: 0.550235, acc.: 71.09%] [G loss: 0.694521]\n",
      "epoch:14 step:13787 [D loss: 0.598751, acc.: 67.97%] [G loss: 0.560644]\n",
      "epoch:14 step:13788 [D loss: 0.577909, acc.: 67.19%] [G loss: 0.514113]\n",
      "epoch:14 step:13789 [D loss: 0.544059, acc.: 71.88%] [G loss: 0.623889]\n",
      "epoch:14 step:13790 [D loss: 0.568717, acc.: 67.19%] [G loss: 0.529450]\n",
      "epoch:14 step:13791 [D loss: 0.597280, acc.: 67.97%] [G loss: 0.471373]\n",
      "epoch:14 step:13792 [D loss: 0.594609, acc.: 68.75%] [G loss: 0.566108]\n",
      "epoch:14 step:13793 [D loss: 0.582242, acc.: 61.72%] [G loss: 0.665395]\n",
      "epoch:14 step:13794 [D loss: 0.579761, acc.: 67.97%] [G loss: 0.600882]\n",
      "epoch:14 step:13795 [D loss: 0.500766, acc.: 71.88%] [G loss: 0.767936]\n",
      "epoch:14 step:13796 [D loss: 0.594915, acc.: 67.97%] [G loss: 0.598294]\n",
      "epoch:14 step:13797 [D loss: 0.497132, acc.: 75.00%] [G loss: 0.661098]\n",
      "epoch:14 step:13798 [D loss: 0.560906, acc.: 65.62%] [G loss: 0.474741]\n",
      "epoch:14 step:13799 [D loss: 0.525174, acc.: 75.00%] [G loss: 0.467914]\n",
      "epoch:14 step:13800 [D loss: 0.497427, acc.: 75.00%] [G loss: 0.695741]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.530266\n",
      "FID: 48.987545\n",
      "0 = 12.973193322086352\n",
      "1 = 0.09573986360366887\n",
      "2 = 0.8862000107765198\n",
      "3 = 0.8312000036239624\n",
      "4 = 0.9412000179290771\n",
      "5 = 0.9339326024055481\n",
      "6 = 0.8312000036239624\n",
      "7 = 8.235163669157027\n",
      "8 = 0.14786821924110552\n",
      "9 = 0.7106999754905701\n",
      "10 = 0.6973999738693237\n",
      "11 = 0.7239999771118164\n",
      "12 = 0.7164577841758728\n",
      "13 = 0.6973999738693237\n",
      "14 = 6.530295372009277\n",
      "15 = 7.117222309112549\n",
      "16 = 0.38080376386642456\n",
      "17 = 6.530266284942627\n",
      "18 = 48.987545013427734\n",
      "epoch:14 step:13801 [D loss: 0.587487, acc.: 64.84%] [G loss: 0.397144]\n",
      "epoch:14 step:13802 [D loss: 0.578553, acc.: 66.41%] [G loss: 0.429322]\n",
      "epoch:14 step:13803 [D loss: 0.498750, acc.: 74.22%] [G loss: 0.615507]\n",
      "epoch:14 step:13804 [D loss: 0.548526, acc.: 72.66%] [G loss: 0.635680]\n",
      "epoch:14 step:13805 [D loss: 0.603284, acc.: 65.62%] [G loss: 0.498957]\n",
      "epoch:14 step:13806 [D loss: 0.529592, acc.: 73.44%] [G loss: 0.708218]\n",
      "epoch:14 step:13807 [D loss: 0.569351, acc.: 67.19%] [G loss: 0.737129]\n",
      "epoch:14 step:13808 [D loss: 0.554471, acc.: 71.88%] [G loss: 0.546570]\n",
      "epoch:14 step:13809 [D loss: 0.548399, acc.: 68.75%] [G loss: 0.560504]\n",
      "epoch:14 step:13810 [D loss: 0.567715, acc.: 70.31%] [G loss: 0.649261]\n",
      "epoch:14 step:13811 [D loss: 0.500574, acc.: 75.00%] [G loss: 0.636454]\n",
      "epoch:14 step:13812 [D loss: 0.483934, acc.: 76.56%] [G loss: 0.784243]\n",
      "epoch:14 step:13813 [D loss: 0.571421, acc.: 71.09%] [G loss: 0.872024]\n",
      "epoch:14 step:13814 [D loss: 0.640627, acc.: 63.28%] [G loss: 0.497303]\n",
      "epoch:14 step:13815 [D loss: 0.570989, acc.: 62.50%] [G loss: 0.614385]\n",
      "epoch:14 step:13816 [D loss: 0.593167, acc.: 64.84%] [G loss: 0.545823]\n",
      "epoch:14 step:13817 [D loss: 0.509424, acc.: 73.44%] [G loss: 0.557049]\n",
      "epoch:14 step:13818 [D loss: 0.519953, acc.: 75.00%] [G loss: 0.969504]\n",
      "epoch:14 step:13819 [D loss: 0.542709, acc.: 73.44%] [G loss: 0.765189]\n",
      "epoch:14 step:13820 [D loss: 0.655187, acc.: 61.72%] [G loss: 0.535659]\n",
      "epoch:14 step:13821 [D loss: 0.638361, acc.: 59.38%] [G loss: 0.456533]\n",
      "epoch:14 step:13822 [D loss: 0.607559, acc.: 67.19%] [G loss: 0.528900]\n",
      "epoch:14 step:13823 [D loss: 0.481449, acc.: 78.12%] [G loss: 0.647856]\n",
      "epoch:14 step:13824 [D loss: 0.566432, acc.: 68.75%] [G loss: 0.560369]\n",
      "epoch:14 step:13825 [D loss: 0.505994, acc.: 73.44%] [G loss: 0.659382]\n",
      "epoch:14 step:13826 [D loss: 0.458272, acc.: 81.25%] [G loss: 0.735128]\n",
      "epoch:14 step:13827 [D loss: 0.547023, acc.: 66.41%] [G loss: 0.592346]\n",
      "epoch:14 step:13828 [D loss: 0.552744, acc.: 73.44%] [G loss: 0.598776]\n",
      "epoch:14 step:13829 [D loss: 0.623270, acc.: 61.72%] [G loss: 0.600607]\n",
      "epoch:14 step:13830 [D loss: 0.585851, acc.: 67.97%] [G loss: 0.806194]\n",
      "epoch:14 step:13831 [D loss: 0.639632, acc.: 56.25%] [G loss: 0.575169]\n",
      "epoch:14 step:13832 [D loss: 0.542204, acc.: 68.75%] [G loss: 0.549032]\n",
      "epoch:14 step:13833 [D loss: 0.558056, acc.: 73.44%] [G loss: 0.574434]\n",
      "epoch:14 step:13834 [D loss: 0.738750, acc.: 53.91%] [G loss: 0.459872]\n",
      "epoch:14 step:13835 [D loss: 0.607432, acc.: 67.19%] [G loss: 0.537920]\n",
      "epoch:14 step:13836 [D loss: 0.610923, acc.: 66.41%] [G loss: 0.571164]\n",
      "epoch:14 step:13837 [D loss: 0.497665, acc.: 73.44%] [G loss: 0.616708]\n",
      "epoch:14 step:13838 [D loss: 0.554325, acc.: 70.31%] [G loss: 0.617953]\n",
      "epoch:14 step:13839 [D loss: 0.583570, acc.: 66.41%] [G loss: 0.539507]\n",
      "epoch:14 step:13840 [D loss: 0.561051, acc.: 69.53%] [G loss: 0.619223]\n",
      "epoch:14 step:13841 [D loss: 0.577488, acc.: 68.75%] [G loss: 0.496129]\n",
      "epoch:14 step:13842 [D loss: 0.478614, acc.: 79.69%] [G loss: 0.715200]\n",
      "epoch:14 step:13843 [D loss: 0.531389, acc.: 72.66%] [G loss: 0.636204]\n",
      "epoch:14 step:13844 [D loss: 0.558656, acc.: 68.75%] [G loss: 0.635410]\n",
      "epoch:14 step:13845 [D loss: 0.603140, acc.: 64.06%] [G loss: 0.626886]\n",
      "epoch:14 step:13846 [D loss: 0.566861, acc.: 71.09%] [G loss: 0.580757]\n",
      "epoch:14 step:13847 [D loss: 0.563489, acc.: 69.53%] [G loss: 0.578953]\n",
      "epoch:14 step:13848 [D loss: 0.503476, acc.: 76.56%] [G loss: 0.559441]\n",
      "epoch:14 step:13849 [D loss: 0.635229, acc.: 58.59%] [G loss: 0.536236]\n",
      "epoch:14 step:13850 [D loss: 0.544929, acc.: 68.75%] [G loss: 0.542370]\n",
      "epoch:14 step:13851 [D loss: 0.583242, acc.: 64.84%] [G loss: 0.578838]\n",
      "epoch:14 step:13852 [D loss: 0.538800, acc.: 72.66%] [G loss: 0.503574]\n",
      "epoch:14 step:13853 [D loss: 0.580590, acc.: 70.31%] [G loss: 0.494391]\n",
      "epoch:14 step:13854 [D loss: 0.501185, acc.: 75.00%] [G loss: 0.589291]\n",
      "epoch:14 step:13855 [D loss: 0.535594, acc.: 68.75%] [G loss: 0.684745]\n",
      "epoch:14 step:13856 [D loss: 0.549591, acc.: 71.09%] [G loss: 0.574811]\n",
      "epoch:14 step:13857 [D loss: 0.566323, acc.: 67.19%] [G loss: 0.566994]\n",
      "epoch:14 step:13858 [D loss: 0.619849, acc.: 57.81%] [G loss: 0.343201]\n",
      "epoch:14 step:13859 [D loss: 0.510219, acc.: 73.44%] [G loss: 0.510536]\n",
      "epoch:14 step:13860 [D loss: 0.512979, acc.: 75.78%] [G loss: 0.714304]\n",
      "epoch:14 step:13861 [D loss: 0.522280, acc.: 68.75%] [G loss: 0.667980]\n",
      "epoch:14 step:13862 [D loss: 0.522784, acc.: 76.56%] [G loss: 0.673783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13863 [D loss: 0.595414, acc.: 66.41%] [G loss: 0.614704]\n",
      "epoch:14 step:13864 [D loss: 0.492667, acc.: 75.78%] [G loss: 0.592629]\n",
      "epoch:14 step:13865 [D loss: 0.450414, acc.: 75.00%] [G loss: 0.662855]\n",
      "epoch:14 step:13866 [D loss: 0.535468, acc.: 67.19%] [G loss: 0.631572]\n",
      "epoch:14 step:13867 [D loss: 0.545360, acc.: 71.88%] [G loss: 0.594430]\n",
      "epoch:14 step:13868 [D loss: 0.590217, acc.: 61.72%] [G loss: 0.617834]\n",
      "epoch:14 step:13869 [D loss: 0.468850, acc.: 78.12%] [G loss: 0.622242]\n",
      "epoch:14 step:13870 [D loss: 0.619549, acc.: 64.06%] [G loss: 0.606927]\n",
      "epoch:14 step:13871 [D loss: 0.508876, acc.: 75.00%] [G loss: 0.624624]\n",
      "epoch:14 step:13872 [D loss: 0.529147, acc.: 69.53%] [G loss: 0.479938]\n",
      "epoch:14 step:13873 [D loss: 0.544589, acc.: 67.97%] [G loss: 0.578529]\n",
      "epoch:14 step:13874 [D loss: 0.557799, acc.: 68.75%] [G loss: 0.559263]\n",
      "epoch:14 step:13875 [D loss: 0.517685, acc.: 69.53%] [G loss: 0.579389]\n",
      "epoch:14 step:13876 [D loss: 0.550796, acc.: 70.31%] [G loss: 0.534948]\n",
      "epoch:14 step:13877 [D loss: 0.567739, acc.: 71.88%] [G loss: 0.584171]\n",
      "epoch:14 step:13878 [D loss: 0.506248, acc.: 73.44%] [G loss: 0.628513]\n",
      "epoch:14 step:13879 [D loss: 0.562098, acc.: 68.75%] [G loss: 0.597809]\n",
      "epoch:14 step:13880 [D loss: 0.567631, acc.: 70.31%] [G loss: 0.517048]\n",
      "epoch:14 step:13881 [D loss: 0.573406, acc.: 71.88%] [G loss: 0.646277]\n",
      "epoch:14 step:13882 [D loss: 0.575128, acc.: 67.97%] [G loss: 0.450610]\n",
      "epoch:14 step:13883 [D loss: 0.616430, acc.: 67.97%] [G loss: 0.519064]\n",
      "epoch:14 step:13884 [D loss: 0.680527, acc.: 57.81%] [G loss: 0.530709]\n",
      "epoch:14 step:13885 [D loss: 0.529626, acc.: 73.44%] [G loss: 0.619375]\n",
      "epoch:14 step:13886 [D loss: 0.571946, acc.: 69.53%] [G loss: 0.717378]\n",
      "epoch:14 step:13887 [D loss: 0.507601, acc.: 75.78%] [G loss: 0.847151]\n",
      "epoch:14 step:13888 [D loss: 0.554702, acc.: 73.44%] [G loss: 0.725956]\n",
      "epoch:14 step:13889 [D loss: 0.517463, acc.: 76.56%] [G loss: 0.752142]\n",
      "epoch:14 step:13890 [D loss: 0.562223, acc.: 70.31%] [G loss: 0.574859]\n",
      "epoch:14 step:13891 [D loss: 0.522028, acc.: 74.22%] [G loss: 0.605072]\n",
      "epoch:14 step:13892 [D loss: 0.560040, acc.: 69.53%] [G loss: 0.576875]\n",
      "epoch:14 step:13893 [D loss: 0.501509, acc.: 72.66%] [G loss: 0.655965]\n",
      "epoch:14 step:13894 [D loss: 0.576446, acc.: 66.41%] [G loss: 0.567234]\n",
      "epoch:14 step:13895 [D loss: 0.546148, acc.: 74.22%] [G loss: 0.515789]\n",
      "epoch:14 step:13896 [D loss: 0.557481, acc.: 73.44%] [G loss: 0.588846]\n",
      "epoch:14 step:13897 [D loss: 0.615914, acc.: 65.62%] [G loss: 0.564466]\n",
      "epoch:14 step:13898 [D loss: 0.575343, acc.: 67.97%] [G loss: 0.665184]\n",
      "epoch:14 step:13899 [D loss: 0.505393, acc.: 72.66%] [G loss: 0.691061]\n",
      "epoch:14 step:13900 [D loss: 0.520181, acc.: 72.66%] [G loss: 0.626413]\n",
      "epoch:14 step:13901 [D loss: 0.527798, acc.: 71.88%] [G loss: 0.679876]\n",
      "epoch:14 step:13902 [D loss: 0.671265, acc.: 62.50%] [G loss: 0.599749]\n",
      "epoch:14 step:13903 [D loss: 0.582636, acc.: 68.75%] [G loss: 0.522716]\n",
      "epoch:14 step:13904 [D loss: 0.537374, acc.: 71.09%] [G loss: 0.663519]\n",
      "epoch:14 step:13905 [D loss: 0.603759, acc.: 67.19%] [G loss: 0.551093]\n",
      "epoch:14 step:13906 [D loss: 0.635392, acc.: 61.72%] [G loss: 0.593023]\n",
      "epoch:14 step:13907 [D loss: 0.531441, acc.: 75.00%] [G loss: 0.663997]\n",
      "epoch:14 step:13908 [D loss: 0.501881, acc.: 73.44%] [G loss: 0.610725]\n",
      "epoch:14 step:13909 [D loss: 0.583379, acc.: 71.88%] [G loss: 0.716644]\n",
      "epoch:14 step:13910 [D loss: 0.481384, acc.: 75.78%] [G loss: 0.614552]\n",
      "epoch:14 step:13911 [D loss: 0.580901, acc.: 66.41%] [G loss: 0.560556]\n",
      "epoch:14 step:13912 [D loss: 0.634377, acc.: 63.28%] [G loss: 0.512035]\n",
      "epoch:14 step:13913 [D loss: 0.576153, acc.: 64.06%] [G loss: 0.594407]\n",
      "epoch:14 step:13914 [D loss: 0.562841, acc.: 72.66%] [G loss: 0.620095]\n",
      "epoch:14 step:13915 [D loss: 0.545846, acc.: 67.97%] [G loss: 0.699723]\n",
      "epoch:14 step:13916 [D loss: 0.536087, acc.: 67.19%] [G loss: 0.653945]\n",
      "epoch:14 step:13917 [D loss: 0.552267, acc.: 71.09%] [G loss: 0.594981]\n",
      "epoch:14 step:13918 [D loss: 0.580101, acc.: 63.28%] [G loss: 0.456898]\n",
      "epoch:14 step:13919 [D loss: 0.419700, acc.: 86.72%] [G loss: 0.676822]\n",
      "epoch:14 step:13920 [D loss: 0.587493, acc.: 67.19%] [G loss: 0.859726]\n",
      "epoch:14 step:13921 [D loss: 0.491175, acc.: 75.78%] [G loss: 0.880658]\n",
      "epoch:14 step:13922 [D loss: 0.586932, acc.: 63.28%] [G loss: 0.636143]\n",
      "epoch:14 step:13923 [D loss: 0.513518, acc.: 70.31%] [G loss: 0.612230]\n",
      "epoch:14 step:13924 [D loss: 0.590879, acc.: 64.84%] [G loss: 0.533686]\n",
      "epoch:14 step:13925 [D loss: 0.511292, acc.: 73.44%] [G loss: 0.536280]\n",
      "epoch:14 step:13926 [D loss: 0.510607, acc.: 69.53%] [G loss: 0.450579]\n",
      "epoch:14 step:13927 [D loss: 0.493836, acc.: 75.78%] [G loss: 0.686415]\n",
      "epoch:14 step:13928 [D loss: 0.536866, acc.: 71.88%] [G loss: 0.509982]\n",
      "epoch:14 step:13929 [D loss: 0.597968, acc.: 61.72%] [G loss: 0.525413]\n",
      "epoch:14 step:13930 [D loss: 0.636678, acc.: 61.72%] [G loss: 0.509875]\n",
      "epoch:14 step:13931 [D loss: 0.533223, acc.: 68.75%] [G loss: 0.554979]\n",
      "epoch:14 step:13932 [D loss: 0.514148, acc.: 72.66%] [G loss: 0.506031]\n",
      "epoch:14 step:13933 [D loss: 0.505742, acc.: 76.56%] [G loss: 0.658605]\n",
      "epoch:14 step:13934 [D loss: 0.530857, acc.: 71.88%] [G loss: 0.636638]\n",
      "epoch:14 step:13935 [D loss: 0.615340, acc.: 64.06%] [G loss: 0.579413]\n",
      "epoch:14 step:13936 [D loss: 0.580968, acc.: 67.19%] [G loss: 0.610151]\n",
      "epoch:14 step:13937 [D loss: 0.562324, acc.: 65.62%] [G loss: 0.501005]\n",
      "epoch:14 step:13938 [D loss: 0.658212, acc.: 63.28%] [G loss: 0.455384]\n",
      "epoch:14 step:13939 [D loss: 0.535411, acc.: 68.75%] [G loss: 0.726454]\n",
      "epoch:14 step:13940 [D loss: 0.544162, acc.: 64.84%] [G loss: 0.563594]\n",
      "epoch:14 step:13941 [D loss: 0.451776, acc.: 78.91%] [G loss: 0.517809]\n",
      "epoch:14 step:13942 [D loss: 0.627426, acc.: 63.28%] [G loss: 0.514805]\n",
      "epoch:14 step:13943 [D loss: 0.550154, acc.: 73.44%] [G loss: 0.458777]\n",
      "epoch:14 step:13944 [D loss: 0.549724, acc.: 64.84%] [G loss: 0.515962]\n",
      "epoch:14 step:13945 [D loss: 0.593584, acc.: 68.75%] [G loss: 0.538523]\n",
      "epoch:14 step:13946 [D loss: 0.613965, acc.: 64.06%] [G loss: 0.410661]\n",
      "epoch:14 step:13947 [D loss: 0.557929, acc.: 68.75%] [G loss: 0.617142]\n",
      "epoch:14 step:13948 [D loss: 0.529052, acc.: 71.88%] [G loss: 0.642080]\n",
      "epoch:14 step:13949 [D loss: 0.515228, acc.: 75.00%] [G loss: 0.608550]\n",
      "epoch:14 step:13950 [D loss: 0.485485, acc.: 79.69%] [G loss: 0.594182]\n",
      "epoch:14 step:13951 [D loss: 0.581334, acc.: 67.19%] [G loss: 0.594542]\n",
      "epoch:14 step:13952 [D loss: 0.537467, acc.: 67.19%] [G loss: 0.450038]\n",
      "epoch:14 step:13953 [D loss: 0.522719, acc.: 71.88%] [G loss: 0.473388]\n",
      "epoch:14 step:13954 [D loss: 0.538535, acc.: 73.44%] [G loss: 0.406380]\n",
      "epoch:14 step:13955 [D loss: 0.508624, acc.: 73.44%] [G loss: 0.661297]\n",
      "epoch:14 step:13956 [D loss: 0.517099, acc.: 72.66%] [G loss: 0.549271]\n",
      "epoch:14 step:13957 [D loss: 0.547556, acc.: 71.09%] [G loss: 0.520194]\n",
      "epoch:14 step:13958 [D loss: 0.570763, acc.: 72.66%] [G loss: 0.528158]\n",
      "epoch:14 step:13959 [D loss: 0.540702, acc.: 70.31%] [G loss: 0.596513]\n",
      "epoch:14 step:13960 [D loss: 0.508520, acc.: 73.44%] [G loss: 0.505158]\n",
      "epoch:14 step:13961 [D loss: 0.508503, acc.: 76.56%] [G loss: 0.604113]\n",
      "epoch:14 step:13962 [D loss: 0.518791, acc.: 71.09%] [G loss: 0.707427]\n",
      "epoch:14 step:13963 [D loss: 0.599214, acc.: 67.19%] [G loss: 0.600484]\n",
      "epoch:14 step:13964 [D loss: 0.574181, acc.: 67.19%] [G loss: 0.582578]\n",
      "epoch:14 step:13965 [D loss: 0.595351, acc.: 66.41%] [G loss: 0.487975]\n",
      "epoch:14 step:13966 [D loss: 0.563100, acc.: 64.84%] [G loss: 0.553991]\n",
      "epoch:14 step:13967 [D loss: 0.540248, acc.: 72.66%] [G loss: 0.439616]\n",
      "epoch:14 step:13968 [D loss: 0.612537, acc.: 59.38%] [G loss: 0.437621]\n",
      "epoch:14 step:13969 [D loss: 0.591975, acc.: 64.84%] [G loss: 0.463597]\n",
      "epoch:14 step:13970 [D loss: 0.566046, acc.: 66.41%] [G loss: 0.592996]\n",
      "epoch:14 step:13971 [D loss: 0.561258, acc.: 68.75%] [G loss: 0.631752]\n",
      "epoch:14 step:13972 [D loss: 0.570791, acc.: 68.75%] [G loss: 0.643004]\n",
      "epoch:14 step:13973 [D loss: 0.563383, acc.: 71.88%] [G loss: 0.651936]\n",
      "epoch:14 step:13974 [D loss: 0.615183, acc.: 62.50%] [G loss: 0.608877]\n",
      "epoch:14 step:13975 [D loss: 0.452042, acc.: 75.00%] [G loss: 0.492578]\n",
      "epoch:14 step:13976 [D loss: 0.631480, acc.: 58.59%] [G loss: 0.483801]\n",
      "epoch:14 step:13977 [D loss: 0.564022, acc.: 72.66%] [G loss: 0.553545]\n",
      "epoch:14 step:13978 [D loss: 0.458767, acc.: 80.47%] [G loss: 0.670451]\n",
      "epoch:14 step:13979 [D loss: 0.655631, acc.: 60.16%] [G loss: 0.706250]\n",
      "epoch:14 step:13980 [D loss: 0.593760, acc.: 63.28%] [G loss: 0.419473]\n",
      "epoch:14 step:13981 [D loss: 0.588759, acc.: 60.94%] [G loss: 0.494605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13982 [D loss: 0.553404, acc.: 67.97%] [G loss: 0.481844]\n",
      "epoch:14 step:13983 [D loss: 0.574220, acc.: 66.41%] [G loss: 0.462590]\n",
      "epoch:14 step:13984 [D loss: 0.560389, acc.: 67.19%] [G loss: 0.490097]\n",
      "epoch:14 step:13985 [D loss: 0.666445, acc.: 62.50%] [G loss: 0.461614]\n",
      "epoch:14 step:13986 [D loss: 0.518557, acc.: 76.56%] [G loss: 0.585733]\n",
      "epoch:14 step:13987 [D loss: 0.581993, acc.: 61.72%] [G loss: 0.560371]\n",
      "epoch:14 step:13988 [D loss: 0.452236, acc.: 78.91%] [G loss: 0.692559]\n",
      "epoch:14 step:13989 [D loss: 0.512116, acc.: 74.22%] [G loss: 0.616973]\n",
      "epoch:14 step:13990 [D loss: 0.552135, acc.: 67.97%] [G loss: 0.641191]\n",
      "epoch:14 step:13991 [D loss: 0.605198, acc.: 68.75%] [G loss: 0.506338]\n",
      "epoch:14 step:13992 [D loss: 0.586739, acc.: 68.75%] [G loss: 0.480875]\n",
      "epoch:14 step:13993 [D loss: 0.532404, acc.: 70.31%] [G loss: 0.541493]\n",
      "epoch:14 step:13994 [D loss: 0.585646, acc.: 67.19%] [G loss: 0.539992]\n",
      "epoch:14 step:13995 [D loss: 0.637329, acc.: 59.38%] [G loss: 0.510416]\n",
      "epoch:14 step:13996 [D loss: 0.500650, acc.: 75.00%] [G loss: 0.509540]\n",
      "epoch:14 step:13997 [D loss: 0.528972, acc.: 71.09%] [G loss: 0.570642]\n",
      "epoch:14 step:13998 [D loss: 0.707655, acc.: 57.03%] [G loss: 0.457042]\n",
      "epoch:14 step:13999 [D loss: 0.571865, acc.: 64.06%] [G loss: 0.427760]\n",
      "epoch:14 step:14000 [D loss: 0.535463, acc.: 70.31%] [G loss: 0.551953]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.628246\n",
      "FID: 48.265385\n",
      "0 = 12.927077036285416\n",
      "1 = 0.08876786602879806\n",
      "2 = 0.8791000247001648\n",
      "3 = 0.8230000138282776\n",
      "4 = 0.9351999759674072\n",
      "5 = 0.9270105957984924\n",
      "6 = 0.8230000138282776\n",
      "7 = 8.259232388210302\n",
      "8 = 0.14610750674180348\n",
      "9 = 0.7200999855995178\n",
      "10 = 0.7134000062942505\n",
      "11 = 0.7268000245094299\n",
      "12 = 0.7230893969535828\n",
      "13 = 0.7134000062942505\n",
      "14 = 6.62827730178833\n",
      "15 = 7.3561553955078125\n",
      "16 = 0.3635982275009155\n",
      "17 = 6.628245830535889\n",
      "18 = 48.265384674072266\n",
      "epoch:14 step:14001 [D loss: 0.576252, acc.: 67.97%] [G loss: 0.361720]\n",
      "epoch:14 step:14002 [D loss: 0.539216, acc.: 71.09%] [G loss: 0.529779]\n",
      "epoch:14 step:14003 [D loss: 0.567862, acc.: 70.31%] [G loss: 0.542975]\n",
      "epoch:14 step:14004 [D loss: 0.555444, acc.: 71.09%] [G loss: 0.704252]\n",
      "epoch:14 step:14005 [D loss: 0.541534, acc.: 68.75%] [G loss: 0.719113]\n",
      "epoch:14 step:14006 [D loss: 0.557771, acc.: 64.06%] [G loss: 0.530979]\n",
      "epoch:14 step:14007 [D loss: 0.500770, acc.: 75.78%] [G loss: 0.674748]\n",
      "epoch:14 step:14008 [D loss: 0.481656, acc.: 78.12%] [G loss: 0.581979]\n",
      "epoch:14 step:14009 [D loss: 0.600391, acc.: 64.84%] [G loss: 0.487339]\n",
      "epoch:14 step:14010 [D loss: 0.593338, acc.: 66.41%] [G loss: 0.449521]\n",
      "epoch:14 step:14011 [D loss: 0.558505, acc.: 71.09%] [G loss: 0.563601]\n",
      "epoch:14 step:14012 [D loss: 0.501776, acc.: 74.22%] [G loss: 0.487407]\n",
      "epoch:14 step:14013 [D loss: 0.555925, acc.: 70.31%] [G loss: 0.522530]\n",
      "epoch:14 step:14014 [D loss: 0.564055, acc.: 67.97%] [G loss: 0.822397]\n",
      "epoch:14 step:14015 [D loss: 0.504156, acc.: 74.22%] [G loss: 0.735784]\n",
      "epoch:14 step:14016 [D loss: 0.480429, acc.: 78.91%] [G loss: 0.709289]\n",
      "epoch:14 step:14017 [D loss: 0.473838, acc.: 79.69%] [G loss: 0.708935]\n",
      "epoch:14 step:14018 [D loss: 0.508094, acc.: 76.56%] [G loss: 0.639426]\n",
      "epoch:14 step:14019 [D loss: 0.523634, acc.: 68.75%] [G loss: 0.718639]\n",
      "epoch:14 step:14020 [D loss: 0.609777, acc.: 62.50%] [G loss: 0.459599]\n",
      "epoch:14 step:14021 [D loss: 0.569159, acc.: 67.19%] [G loss: 0.554794]\n",
      "epoch:14 step:14022 [D loss: 0.605640, acc.: 64.84%] [G loss: 0.514094]\n",
      "epoch:14 step:14023 [D loss: 0.563190, acc.: 71.88%] [G loss: 0.615609]\n",
      "epoch:14 step:14024 [D loss: 0.535324, acc.: 72.66%] [G loss: 0.709031]\n",
      "epoch:14 step:14025 [D loss: 0.537619, acc.: 71.09%] [G loss: 0.602731]\n",
      "epoch:14 step:14026 [D loss: 0.552532, acc.: 71.09%] [G loss: 0.545410]\n",
      "epoch:14 step:14027 [D loss: 0.539881, acc.: 70.31%] [G loss: 0.613173]\n",
      "epoch:14 step:14028 [D loss: 0.576083, acc.: 67.19%] [G loss: 0.516388]\n",
      "epoch:14 step:14029 [D loss: 0.483223, acc.: 77.34%] [G loss: 0.581109]\n",
      "epoch:14 step:14030 [D loss: 0.460918, acc.: 77.34%] [G loss: 0.654874]\n",
      "epoch:14 step:14031 [D loss: 0.576016, acc.: 71.09%] [G loss: 0.699009]\n",
      "epoch:14 step:14032 [D loss: 0.508869, acc.: 73.44%] [G loss: 0.977886]\n",
      "epoch:14 step:14033 [D loss: 0.687087, acc.: 64.84%] [G loss: 0.609903]\n",
      "epoch:14 step:14034 [D loss: 0.550509, acc.: 68.75%] [G loss: 0.634273]\n",
      "epoch:14 step:14035 [D loss: 0.612491, acc.: 62.50%] [G loss: 0.562129]\n",
      "epoch:14 step:14036 [D loss: 0.498052, acc.: 78.12%] [G loss: 0.670565]\n",
      "epoch:14 step:14037 [D loss: 0.452831, acc.: 82.03%] [G loss: 0.877518]\n",
      "epoch:14 step:14038 [D loss: 0.735393, acc.: 57.03%] [G loss: 0.707100]\n",
      "epoch:14 step:14039 [D loss: 0.531540, acc.: 75.00%] [G loss: 0.675026]\n",
      "epoch:14 step:14040 [D loss: 0.536133, acc.: 71.09%] [G loss: 0.655074]\n",
      "epoch:14 step:14041 [D loss: 0.457119, acc.: 78.91%] [G loss: 0.547746]\n",
      "epoch:14 step:14042 [D loss: 0.517485, acc.: 75.00%] [G loss: 0.694051]\n",
      "epoch:14 step:14043 [D loss: 0.466567, acc.: 76.56%] [G loss: 0.887635]\n",
      "epoch:14 step:14044 [D loss: 0.468804, acc.: 78.91%] [G loss: 0.951100]\n",
      "epoch:14 step:14045 [D loss: 0.498689, acc.: 70.31%] [G loss: 0.984967]\n",
      "epoch:14 step:14046 [D loss: 0.775469, acc.: 55.47%] [G loss: 0.908168]\n",
      "epoch:14 step:14047 [D loss: 0.511554, acc.: 74.22%] [G loss: 0.977162]\n",
      "epoch:14 step:14048 [D loss: 0.472242, acc.: 76.56%] [G loss: 1.013128]\n",
      "epoch:14 step:14049 [D loss: 0.581734, acc.: 67.97%] [G loss: 0.844179]\n",
      "epoch:14 step:14050 [D loss: 0.596729, acc.: 64.06%] [G loss: 0.612030]\n",
      "epoch:14 step:14051 [D loss: 0.502106, acc.: 71.88%] [G loss: 0.643580]\n",
      "epoch:14 step:14052 [D loss: 0.546566, acc.: 67.19%] [G loss: 0.822011]\n",
      "epoch:14 step:14053 [D loss: 0.492047, acc.: 71.88%] [G loss: 0.971347]\n",
      "epoch:14 step:14054 [D loss: 0.350170, acc.: 84.38%] [G loss: 0.941986]\n",
      "epoch:14 step:14055 [D loss: 0.384739, acc.: 89.84%] [G loss: 0.962380]\n",
      "epoch:15 step:14056 [D loss: 0.580602, acc.: 71.09%] [G loss: 0.908386]\n",
      "epoch:15 step:14057 [D loss: 0.486125, acc.: 77.34%] [G loss: 1.117493]\n",
      "epoch:15 step:14058 [D loss: 0.562771, acc.: 71.09%] [G loss: 0.777382]\n",
      "epoch:15 step:14059 [D loss: 0.495933, acc.: 74.22%] [G loss: 0.851661]\n",
      "epoch:15 step:14060 [D loss: 0.604254, acc.: 68.75%] [G loss: 0.716464]\n",
      "epoch:15 step:14061 [D loss: 0.567042, acc.: 70.31%] [G loss: 0.810098]\n",
      "epoch:15 step:14062 [D loss: 0.529926, acc.: 73.44%] [G loss: 0.654653]\n",
      "epoch:15 step:14063 [D loss: 0.489931, acc.: 80.47%] [G loss: 0.837355]\n",
      "epoch:15 step:14064 [D loss: 0.532464, acc.: 67.97%] [G loss: 0.999909]\n",
      "epoch:15 step:14065 [D loss: 0.527187, acc.: 71.09%] [G loss: 0.935073]\n",
      "epoch:15 step:14066 [D loss: 0.548870, acc.: 73.44%] [G loss: 0.824412]\n",
      "epoch:15 step:14067 [D loss: 0.631045, acc.: 66.41%] [G loss: 0.521770]\n",
      "epoch:15 step:14068 [D loss: 0.515445, acc.: 71.88%] [G loss: 0.666635]\n",
      "epoch:15 step:14069 [D loss: 0.517266, acc.: 74.22%] [G loss: 0.534122]\n",
      "epoch:15 step:14070 [D loss: 0.482721, acc.: 74.22%] [G loss: 0.452889]\n",
      "epoch:15 step:14071 [D loss: 0.548430, acc.: 70.31%] [G loss: 0.704306]\n",
      "epoch:15 step:14072 [D loss: 0.533161, acc.: 70.31%] [G loss: 0.644302]\n",
      "epoch:15 step:14073 [D loss: 0.560160, acc.: 67.19%] [G loss: 0.730159]\n",
      "epoch:15 step:14074 [D loss: 0.561828, acc.: 69.53%] [G loss: 0.549289]\n",
      "epoch:15 step:14075 [D loss: 0.677641, acc.: 57.81%] [G loss: 0.552025]\n",
      "epoch:15 step:14076 [D loss: 0.598163, acc.: 64.84%] [G loss: 0.613583]\n",
      "epoch:15 step:14077 [D loss: 0.462328, acc.: 77.34%] [G loss: 0.822494]\n",
      "epoch:15 step:14078 [D loss: 0.535153, acc.: 75.00%] [G loss: 0.645182]\n",
      "epoch:15 step:14079 [D loss: 0.466735, acc.: 79.69%] [G loss: 0.743533]\n",
      "epoch:15 step:14080 [D loss: 0.523013, acc.: 74.22%] [G loss: 0.726317]\n",
      "epoch:15 step:14081 [D loss: 0.610058, acc.: 65.62%] [G loss: 0.552110]\n",
      "epoch:15 step:14082 [D loss: 0.494549, acc.: 71.88%] [G loss: 0.615314]\n",
      "epoch:15 step:14083 [D loss: 0.610924, acc.: 64.84%] [G loss: 0.560335]\n",
      "epoch:15 step:14084 [D loss: 0.485353, acc.: 71.09%] [G loss: 0.591611]\n",
      "epoch:15 step:14085 [D loss: 0.544382, acc.: 71.88%] [G loss: 0.478365]\n",
      "epoch:15 step:14086 [D loss: 0.636951, acc.: 62.50%] [G loss: 0.583063]\n",
      "epoch:15 step:14087 [D loss: 0.540502, acc.: 68.75%] [G loss: 0.615806]\n",
      "epoch:15 step:14088 [D loss: 0.564234, acc.: 69.53%] [G loss: 0.649906]\n",
      "epoch:15 step:14089 [D loss: 0.517152, acc.: 70.31%] [G loss: 0.750855]\n",
      "epoch:15 step:14090 [D loss: 0.566763, acc.: 70.31%] [G loss: 0.551670]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14091 [D loss: 0.526624, acc.: 71.88%] [G loss: 0.757888]\n",
      "epoch:15 step:14092 [D loss: 0.515991, acc.: 78.91%] [G loss: 0.597822]\n",
      "epoch:15 step:14093 [D loss: 0.621333, acc.: 66.41%] [G loss: 0.590976]\n",
      "epoch:15 step:14094 [D loss: 0.570509, acc.: 70.31%] [G loss: 0.476927]\n",
      "epoch:15 step:14095 [D loss: 0.455870, acc.: 80.47%] [G loss: 0.753450]\n",
      "epoch:15 step:14096 [D loss: 0.545040, acc.: 74.22%] [G loss: 0.635730]\n",
      "epoch:15 step:14097 [D loss: 0.512238, acc.: 73.44%] [G loss: 0.657160]\n",
      "epoch:15 step:14098 [D loss: 0.549405, acc.: 68.75%] [G loss: 0.788679]\n",
      "epoch:15 step:14099 [D loss: 0.576193, acc.: 68.75%] [G loss: 0.608604]\n",
      "epoch:15 step:14100 [D loss: 0.539883, acc.: 75.78%] [G loss: 0.675959]\n",
      "epoch:15 step:14101 [D loss: 0.525329, acc.: 73.44%] [G loss: 0.759803]\n",
      "epoch:15 step:14102 [D loss: 0.563044, acc.: 69.53%] [G loss: 0.575222]\n",
      "epoch:15 step:14103 [D loss: 0.497004, acc.: 76.56%] [G loss: 0.681447]\n",
      "epoch:15 step:14104 [D loss: 0.499488, acc.: 75.78%] [G loss: 0.728214]\n",
      "epoch:15 step:14105 [D loss: 0.531679, acc.: 71.88%] [G loss: 0.748388]\n",
      "epoch:15 step:14106 [D loss: 0.662709, acc.: 64.84%] [G loss: 0.454324]\n",
      "epoch:15 step:14107 [D loss: 0.651905, acc.: 59.38%] [G loss: 0.553636]\n",
      "epoch:15 step:14108 [D loss: 0.533270, acc.: 69.53%] [G loss: 0.617692]\n",
      "epoch:15 step:14109 [D loss: 0.462278, acc.: 75.78%] [G loss: 0.747804]\n",
      "epoch:15 step:14110 [D loss: 0.597442, acc.: 70.31%] [G loss: 0.670788]\n",
      "epoch:15 step:14111 [D loss: 0.569141, acc.: 67.97%] [G loss: 0.624398]\n",
      "epoch:15 step:14112 [D loss: 0.582048, acc.: 67.97%] [G loss: 0.634755]\n",
      "epoch:15 step:14113 [D loss: 0.566295, acc.: 68.75%] [G loss: 0.536678]\n",
      "epoch:15 step:14114 [D loss: 0.508172, acc.: 76.56%] [G loss: 0.534167]\n",
      "epoch:15 step:14115 [D loss: 0.574680, acc.: 65.62%] [G loss: 0.655318]\n",
      "epoch:15 step:14116 [D loss: 0.595834, acc.: 63.28%] [G loss: 0.447198]\n",
      "epoch:15 step:14117 [D loss: 0.591531, acc.: 67.97%] [G loss: 0.430937]\n",
      "epoch:15 step:14118 [D loss: 0.592234, acc.: 69.53%] [G loss: 0.449446]\n",
      "epoch:15 step:14119 [D loss: 0.570297, acc.: 71.09%] [G loss: 0.453894]\n",
      "epoch:15 step:14120 [D loss: 0.493826, acc.: 78.12%] [G loss: 0.600067]\n",
      "epoch:15 step:14121 [D loss: 0.582721, acc.: 67.97%] [G loss: 0.501064]\n",
      "epoch:15 step:14122 [D loss: 0.594155, acc.: 66.41%] [G loss: 0.561470]\n",
      "epoch:15 step:14123 [D loss: 0.546040, acc.: 69.53%] [G loss: 0.628918]\n",
      "epoch:15 step:14124 [D loss: 0.545715, acc.: 70.31%] [G loss: 0.613779]\n",
      "epoch:15 step:14125 [D loss: 0.502649, acc.: 78.12%] [G loss: 0.690379]\n",
      "epoch:15 step:14126 [D loss: 0.546813, acc.: 69.53%] [G loss: 0.544887]\n",
      "epoch:15 step:14127 [D loss: 0.560233, acc.: 67.19%] [G loss: 0.616379]\n",
      "epoch:15 step:14128 [D loss: 0.539054, acc.: 75.00%] [G loss: 0.630934]\n",
      "epoch:15 step:14129 [D loss: 0.474058, acc.: 81.25%] [G loss: 0.596558]\n",
      "epoch:15 step:14130 [D loss: 0.556127, acc.: 71.09%] [G loss: 0.616289]\n",
      "epoch:15 step:14131 [D loss: 0.508921, acc.: 75.00%] [G loss: 0.632455]\n",
      "epoch:15 step:14132 [D loss: 0.460799, acc.: 75.00%] [G loss: 0.846119]\n",
      "epoch:15 step:14133 [D loss: 0.635254, acc.: 68.75%] [G loss: 0.483897]\n",
      "epoch:15 step:14134 [D loss: 0.614491, acc.: 65.62%] [G loss: 0.532174]\n",
      "epoch:15 step:14135 [D loss: 0.522193, acc.: 71.88%] [G loss: 0.621709]\n",
      "epoch:15 step:14136 [D loss: 0.555129, acc.: 68.75%] [G loss: 0.547489]\n",
      "epoch:15 step:14137 [D loss: 0.537374, acc.: 68.75%] [G loss: 0.510667]\n",
      "epoch:15 step:14138 [D loss: 0.520503, acc.: 72.66%] [G loss: 0.703665]\n",
      "epoch:15 step:14139 [D loss: 0.501117, acc.: 67.19%] [G loss: 0.810263]\n",
      "epoch:15 step:14140 [D loss: 0.563666, acc.: 67.19%] [G loss: 0.684652]\n",
      "epoch:15 step:14141 [D loss: 0.531829, acc.: 74.22%] [G loss: 0.595625]\n",
      "epoch:15 step:14142 [D loss: 0.567919, acc.: 71.88%] [G loss: 0.683638]\n",
      "epoch:15 step:14143 [D loss: 0.493329, acc.: 75.00%] [G loss: 0.650204]\n",
      "epoch:15 step:14144 [D loss: 0.507523, acc.: 71.09%] [G loss: 0.605839]\n",
      "epoch:15 step:14145 [D loss: 0.486031, acc.: 77.34%] [G loss: 0.757399]\n",
      "epoch:15 step:14146 [D loss: 0.553775, acc.: 70.31%] [G loss: 0.528629]\n",
      "epoch:15 step:14147 [D loss: 0.478146, acc.: 71.09%] [G loss: 0.746920]\n",
      "epoch:15 step:14148 [D loss: 0.531349, acc.: 69.53%] [G loss: 0.661419]\n",
      "epoch:15 step:14149 [D loss: 0.480609, acc.: 75.00%] [G loss: 0.609111]\n",
      "epoch:15 step:14150 [D loss: 0.527335, acc.: 71.09%] [G loss: 0.626751]\n",
      "epoch:15 step:14151 [D loss: 0.474475, acc.: 75.00%] [G loss: 0.558025]\n",
      "epoch:15 step:14152 [D loss: 0.514467, acc.: 75.00%] [G loss: 0.800699]\n",
      "epoch:15 step:14153 [D loss: 0.553729, acc.: 71.09%] [G loss: 0.827215]\n",
      "epoch:15 step:14154 [D loss: 0.561691, acc.: 71.88%] [G loss: 0.601469]\n",
      "epoch:15 step:14155 [D loss: 0.437758, acc.: 81.25%] [G loss: 0.746753]\n",
      "epoch:15 step:14156 [D loss: 0.531276, acc.: 71.88%] [G loss: 0.607795]\n",
      "epoch:15 step:14157 [D loss: 0.634773, acc.: 64.06%] [G loss: 0.600362]\n",
      "epoch:15 step:14158 [D loss: 0.493868, acc.: 71.09%] [G loss: 0.604838]\n",
      "epoch:15 step:14159 [D loss: 0.566801, acc.: 66.41%] [G loss: 0.554904]\n",
      "epoch:15 step:14160 [D loss: 0.600030, acc.: 62.50%] [G loss: 0.488676]\n",
      "epoch:15 step:14161 [D loss: 0.580115, acc.: 67.97%] [G loss: 0.553312]\n",
      "epoch:15 step:14162 [D loss: 0.554966, acc.: 70.31%] [G loss: 0.590055]\n",
      "epoch:15 step:14163 [D loss: 0.644150, acc.: 60.16%] [G loss: 0.516698]\n",
      "epoch:15 step:14164 [D loss: 0.525046, acc.: 71.88%] [G loss: 0.527771]\n",
      "epoch:15 step:14165 [D loss: 0.576070, acc.: 66.41%] [G loss: 0.511951]\n",
      "epoch:15 step:14166 [D loss: 0.513223, acc.: 71.88%] [G loss: 0.583587]\n",
      "epoch:15 step:14167 [D loss: 0.504272, acc.: 75.78%] [G loss: 0.683747]\n",
      "epoch:15 step:14168 [D loss: 0.614686, acc.: 62.50%] [G loss: 0.723045]\n",
      "epoch:15 step:14169 [D loss: 0.579154, acc.: 68.75%] [G loss: 0.522958]\n",
      "epoch:15 step:14170 [D loss: 0.531028, acc.: 75.00%] [G loss: 0.603089]\n",
      "epoch:15 step:14171 [D loss: 0.580308, acc.: 60.16%] [G loss: 0.545066]\n",
      "epoch:15 step:14172 [D loss: 0.572582, acc.: 72.66%] [G loss: 0.665215]\n",
      "epoch:15 step:14173 [D loss: 0.551711, acc.: 69.53%] [G loss: 0.693946]\n",
      "epoch:15 step:14174 [D loss: 0.468690, acc.: 81.25%] [G loss: 0.958951]\n",
      "epoch:15 step:14175 [D loss: 0.537675, acc.: 72.66%] [G loss: 0.713729]\n",
      "epoch:15 step:14176 [D loss: 0.510891, acc.: 72.66%] [G loss: 0.590649]\n",
      "epoch:15 step:14177 [D loss: 0.579669, acc.: 76.56%] [G loss: 0.658187]\n",
      "epoch:15 step:14178 [D loss: 0.497333, acc.: 76.56%] [G loss: 0.726550]\n",
      "epoch:15 step:14179 [D loss: 0.560876, acc.: 71.09%] [G loss: 0.710580]\n",
      "epoch:15 step:14180 [D loss: 0.558167, acc.: 70.31%] [G loss: 0.667713]\n",
      "epoch:15 step:14181 [D loss: 0.478377, acc.: 77.34%] [G loss: 0.636285]\n",
      "epoch:15 step:14182 [D loss: 0.465942, acc.: 78.12%] [G loss: 0.661898]\n",
      "epoch:15 step:14183 [D loss: 0.537770, acc.: 70.31%] [G loss: 0.546772]\n",
      "epoch:15 step:14184 [D loss: 0.609949, acc.: 60.94%] [G loss: 0.462157]\n",
      "epoch:15 step:14185 [D loss: 0.557746, acc.: 71.09%] [G loss: 0.558465]\n",
      "epoch:15 step:14186 [D loss: 0.532490, acc.: 64.84%] [G loss: 0.632519]\n",
      "epoch:15 step:14187 [D loss: 0.563293, acc.: 71.88%] [G loss: 0.543176]\n",
      "epoch:15 step:14188 [D loss: 0.543046, acc.: 73.44%] [G loss: 0.482494]\n",
      "epoch:15 step:14189 [D loss: 0.504928, acc.: 71.88%] [G loss: 0.587276]\n",
      "epoch:15 step:14190 [D loss: 0.566247, acc.: 69.53%] [G loss: 0.637915]\n",
      "epoch:15 step:14191 [D loss: 0.524655, acc.: 73.44%] [G loss: 0.588961]\n",
      "epoch:15 step:14192 [D loss: 0.642354, acc.: 68.75%] [G loss: 0.575662]\n",
      "epoch:15 step:14193 [D loss: 0.565509, acc.: 67.19%] [G loss: 0.565075]\n",
      "epoch:15 step:14194 [D loss: 0.600167, acc.: 71.88%] [G loss: 0.541040]\n",
      "epoch:15 step:14195 [D loss: 0.483662, acc.: 74.22%] [G loss: 0.670744]\n",
      "epoch:15 step:14196 [D loss: 0.585409, acc.: 65.62%] [G loss: 0.703606]\n",
      "epoch:15 step:14197 [D loss: 0.565131, acc.: 65.62%] [G loss: 0.735226]\n",
      "epoch:15 step:14198 [D loss: 0.675913, acc.: 63.28%] [G loss: 0.528075]\n",
      "epoch:15 step:14199 [D loss: 0.509601, acc.: 77.34%] [G loss: 0.631681]\n",
      "epoch:15 step:14200 [D loss: 0.554397, acc.: 68.75%] [G loss: 0.613572]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.699097\n",
      "FID: 46.140148\n",
      "0 = 12.925450069332099\n",
      "1 = 0.08914844314833258\n",
      "2 = 0.8925999999046326\n",
      "3 = 0.8411999940872192\n",
      "4 = 0.9440000057220459\n",
      "5 = 0.9375835657119751\n",
      "6 = 0.8411999940872192\n",
      "7 = 8.070428416728994\n",
      "8 = 0.144730301987578\n",
      "9 = 0.7226999998092651\n",
      "10 = 0.7113999724388123\n",
      "11 = 0.734000027179718\n",
      "12 = 0.7278494238853455\n",
      "13 = 0.7113999724388123\n",
      "14 = 6.699120998382568\n",
      "15 = 7.461191177368164\n",
      "16 = 0.3559403419494629\n",
      "17 = 6.699097156524658\n",
      "18 = 46.1401481628418\n",
      "epoch:15 step:14201 [D loss: 0.535007, acc.: 71.88%] [G loss: 0.544748]\n",
      "epoch:15 step:14202 [D loss: 0.655395, acc.: 67.97%] [G loss: 0.626550]\n",
      "epoch:15 step:14203 [D loss: 0.597995, acc.: 67.19%] [G loss: 0.671950]\n",
      "epoch:15 step:14204 [D loss: 0.526072, acc.: 71.88%] [G loss: 0.675176]\n",
      "epoch:15 step:14205 [D loss: 0.604669, acc.: 67.19%] [G loss: 0.575280]\n",
      "epoch:15 step:14206 [D loss: 0.609239, acc.: 66.41%] [G loss: 0.543510]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14207 [D loss: 0.504270, acc.: 75.78%] [G loss: 0.698241]\n",
      "epoch:15 step:14208 [D loss: 0.614768, acc.: 61.72%] [G loss: 0.464024]\n",
      "epoch:15 step:14209 [D loss: 0.591424, acc.: 67.19%] [G loss: 0.399913]\n",
      "epoch:15 step:14210 [D loss: 0.491127, acc.: 71.88%] [G loss: 0.579569]\n",
      "epoch:15 step:14211 [D loss: 0.465609, acc.: 76.56%] [G loss: 0.722125]\n",
      "epoch:15 step:14212 [D loss: 0.561305, acc.: 68.75%] [G loss: 0.585335]\n",
      "epoch:15 step:14213 [D loss: 0.624165, acc.: 65.62%] [G loss: 0.597366]\n",
      "epoch:15 step:14214 [D loss: 0.540239, acc.: 71.88%] [G loss: 0.698866]\n",
      "epoch:15 step:14215 [D loss: 0.601162, acc.: 68.75%] [G loss: 0.652903]\n",
      "epoch:15 step:14216 [D loss: 0.538923, acc.: 74.22%] [G loss: 0.668749]\n",
      "epoch:15 step:14217 [D loss: 0.472827, acc.: 79.69%] [G loss: 0.669054]\n",
      "epoch:15 step:14218 [D loss: 0.571592, acc.: 69.53%] [G loss: 0.696744]\n",
      "epoch:15 step:14219 [D loss: 0.566619, acc.: 71.09%] [G loss: 0.711543]\n",
      "epoch:15 step:14220 [D loss: 0.525287, acc.: 73.44%] [G loss: 0.600766]\n",
      "epoch:15 step:14221 [D loss: 0.560822, acc.: 70.31%] [G loss: 0.454704]\n",
      "epoch:15 step:14222 [D loss: 0.587693, acc.: 63.28%] [G loss: 0.499468]\n",
      "epoch:15 step:14223 [D loss: 0.556886, acc.: 70.31%] [G loss: 0.614272]\n",
      "epoch:15 step:14224 [D loss: 0.583445, acc.: 68.75%] [G loss: 0.673806]\n",
      "epoch:15 step:14225 [D loss: 0.557845, acc.: 69.53%] [G loss: 0.560570]\n",
      "epoch:15 step:14226 [D loss: 0.530195, acc.: 68.75%] [G loss: 0.575998]\n",
      "epoch:15 step:14227 [D loss: 0.504923, acc.: 75.00%] [G loss: 0.682222]\n",
      "epoch:15 step:14228 [D loss: 0.512469, acc.: 75.00%] [G loss: 0.760536]\n",
      "epoch:15 step:14229 [D loss: 0.547514, acc.: 68.75%] [G loss: 0.556288]\n",
      "epoch:15 step:14230 [D loss: 0.606517, acc.: 61.72%] [G loss: 0.485165]\n",
      "epoch:15 step:14231 [D loss: 0.516245, acc.: 70.31%] [G loss: 0.572969]\n",
      "epoch:15 step:14232 [D loss: 0.509474, acc.: 74.22%] [G loss: 0.546363]\n",
      "epoch:15 step:14233 [D loss: 0.577900, acc.: 64.84%] [G loss: 0.436764]\n",
      "epoch:15 step:14234 [D loss: 0.552098, acc.: 71.09%] [G loss: 0.609677]\n",
      "epoch:15 step:14235 [D loss: 0.591020, acc.: 67.19%] [G loss: 0.399207]\n",
      "epoch:15 step:14236 [D loss: 0.575040, acc.: 68.75%] [G loss: 0.525083]\n",
      "epoch:15 step:14237 [D loss: 0.529241, acc.: 71.88%] [G loss: 0.637863]\n",
      "epoch:15 step:14238 [D loss: 0.545250, acc.: 67.19%] [G loss: 0.581079]\n",
      "epoch:15 step:14239 [D loss: 0.546405, acc.: 72.66%] [G loss: 0.641022]\n",
      "epoch:15 step:14240 [D loss: 0.543517, acc.: 71.88%] [G loss: 0.592435]\n",
      "epoch:15 step:14241 [D loss: 0.561267, acc.: 70.31%] [G loss: 0.529938]\n",
      "epoch:15 step:14242 [D loss: 0.619761, acc.: 62.50%] [G loss: 0.687415]\n",
      "epoch:15 step:14243 [D loss: 0.524427, acc.: 70.31%] [G loss: 0.555016]\n",
      "epoch:15 step:14244 [D loss: 0.566530, acc.: 68.75%] [G loss: 0.491205]\n",
      "epoch:15 step:14245 [D loss: 0.483222, acc.: 73.44%] [G loss: 0.631580]\n",
      "epoch:15 step:14246 [D loss: 0.512246, acc.: 75.78%] [G loss: 0.628871]\n",
      "epoch:15 step:14247 [D loss: 0.506837, acc.: 72.66%] [G loss: 0.593113]\n",
      "epoch:15 step:14248 [D loss: 0.544595, acc.: 73.44%] [G loss: 0.592868]\n",
      "epoch:15 step:14249 [D loss: 0.470718, acc.: 75.78%] [G loss: 0.693426]\n",
      "epoch:15 step:14250 [D loss: 0.540964, acc.: 73.44%] [G loss: 0.580495]\n",
      "epoch:15 step:14251 [D loss: 0.585608, acc.: 64.06%] [G loss: 0.524078]\n",
      "epoch:15 step:14252 [D loss: 0.499869, acc.: 76.56%] [G loss: 0.599959]\n",
      "epoch:15 step:14253 [D loss: 0.453651, acc.: 78.91%] [G loss: 0.749985]\n",
      "epoch:15 step:14254 [D loss: 0.491747, acc.: 74.22%] [G loss: 0.678849]\n",
      "epoch:15 step:14255 [D loss: 0.559741, acc.: 69.53%] [G loss: 0.683148]\n",
      "epoch:15 step:14256 [D loss: 0.598104, acc.: 64.06%] [G loss: 0.484937]\n",
      "epoch:15 step:14257 [D loss: 0.557539, acc.: 70.31%] [G loss: 0.685310]\n",
      "epoch:15 step:14258 [D loss: 0.564766, acc.: 66.41%] [G loss: 0.662238]\n",
      "epoch:15 step:14259 [D loss: 0.541844, acc.: 71.09%] [G loss: 0.758317]\n",
      "epoch:15 step:14260 [D loss: 0.518674, acc.: 73.44%] [G loss: 0.655885]\n",
      "epoch:15 step:14261 [D loss: 0.509353, acc.: 75.78%] [G loss: 0.809429]\n",
      "epoch:15 step:14262 [D loss: 0.477236, acc.: 78.12%] [G loss: 0.838118]\n",
      "epoch:15 step:14263 [D loss: 0.446898, acc.: 79.69%] [G loss: 0.743018]\n",
      "epoch:15 step:14264 [D loss: 0.494927, acc.: 77.34%] [G loss: 0.866347]\n",
      "epoch:15 step:14265 [D loss: 0.616863, acc.: 65.62%] [G loss: 0.702839]\n",
      "epoch:15 step:14266 [D loss: 0.630667, acc.: 59.38%] [G loss: 0.439792]\n",
      "epoch:15 step:14267 [D loss: 0.567474, acc.: 71.88%] [G loss: 0.433489]\n",
      "epoch:15 step:14268 [D loss: 0.534157, acc.: 68.75%] [G loss: 0.583144]\n",
      "epoch:15 step:14269 [D loss: 0.633967, acc.: 63.28%] [G loss: 0.466617]\n",
      "epoch:15 step:14270 [D loss: 0.595444, acc.: 62.50%] [G loss: 0.545460]\n",
      "epoch:15 step:14271 [D loss: 0.567413, acc.: 65.62%] [G loss: 0.572042]\n",
      "epoch:15 step:14272 [D loss: 0.574037, acc.: 68.75%] [G loss: 0.613505]\n",
      "epoch:15 step:14273 [D loss: 0.528889, acc.: 75.00%] [G loss: 0.626139]\n",
      "epoch:15 step:14274 [D loss: 0.510907, acc.: 72.66%] [G loss: 0.597974]\n",
      "epoch:15 step:14275 [D loss: 0.683608, acc.: 60.94%] [G loss: 0.499806]\n",
      "epoch:15 step:14276 [D loss: 0.543419, acc.: 71.09%] [G loss: 0.572561]\n",
      "epoch:15 step:14277 [D loss: 0.525227, acc.: 69.53%] [G loss: 0.565674]\n",
      "epoch:15 step:14278 [D loss: 0.538254, acc.: 71.88%] [G loss: 0.593766]\n",
      "epoch:15 step:14279 [D loss: 0.554713, acc.: 70.31%] [G loss: 0.518058]\n",
      "epoch:15 step:14280 [D loss: 0.534669, acc.: 74.22%] [G loss: 0.487848]\n",
      "epoch:15 step:14281 [D loss: 0.604273, acc.: 59.38%] [G loss: 0.484530]\n",
      "epoch:15 step:14282 [D loss: 0.564701, acc.: 69.53%] [G loss: 0.507160]\n",
      "epoch:15 step:14283 [D loss: 0.599237, acc.: 61.72%] [G loss: 0.644739]\n",
      "epoch:15 step:14284 [D loss: 0.564522, acc.: 68.75%] [G loss: 0.537458]\n",
      "epoch:15 step:14285 [D loss: 0.544739, acc.: 69.53%] [G loss: 0.558456]\n",
      "epoch:15 step:14286 [D loss: 0.468404, acc.: 78.91%] [G loss: 0.698952]\n",
      "epoch:15 step:14287 [D loss: 0.473079, acc.: 78.12%] [G loss: 0.612391]\n",
      "epoch:15 step:14288 [D loss: 0.534385, acc.: 70.31%] [G loss: 0.764844]\n",
      "epoch:15 step:14289 [D loss: 0.557589, acc.: 71.88%] [G loss: 0.582149]\n",
      "epoch:15 step:14290 [D loss: 0.607805, acc.: 62.50%] [G loss: 0.570727]\n",
      "epoch:15 step:14291 [D loss: 0.533798, acc.: 74.22%] [G loss: 0.562067]\n",
      "epoch:15 step:14292 [D loss: 0.545605, acc.: 71.09%] [G loss: 0.645734]\n",
      "epoch:15 step:14293 [D loss: 0.573067, acc.: 63.28%] [G loss: 0.527540]\n",
      "epoch:15 step:14294 [D loss: 0.554107, acc.: 68.75%] [G loss: 0.545486]\n",
      "epoch:15 step:14295 [D loss: 0.563669, acc.: 72.66%] [G loss: 0.472186]\n",
      "epoch:15 step:14296 [D loss: 0.534226, acc.: 73.44%] [G loss: 0.578684]\n",
      "epoch:15 step:14297 [D loss: 0.493153, acc.: 76.56%] [G loss: 0.694718]\n",
      "epoch:15 step:14298 [D loss: 0.598657, acc.: 59.38%] [G loss: 0.675314]\n",
      "epoch:15 step:14299 [D loss: 0.496314, acc.: 71.88%] [G loss: 0.718499]\n",
      "epoch:15 step:14300 [D loss: 0.523784, acc.: 74.22%] [G loss: 0.663760]\n",
      "epoch:15 step:14301 [D loss: 0.558631, acc.: 71.09%] [G loss: 0.615996]\n",
      "epoch:15 step:14302 [D loss: 0.540667, acc.: 72.66%] [G loss: 0.645389]\n",
      "epoch:15 step:14303 [D loss: 0.542942, acc.: 70.31%] [G loss: 0.840266]\n",
      "epoch:15 step:14304 [D loss: 0.605154, acc.: 66.41%] [G loss: 0.469088]\n",
      "epoch:15 step:14305 [D loss: 0.568430, acc.: 70.31%] [G loss: 0.627282]\n",
      "epoch:15 step:14306 [D loss: 0.643312, acc.: 64.84%] [G loss: 0.535465]\n",
      "epoch:15 step:14307 [D loss: 0.541312, acc.: 72.66%] [G loss: 0.480468]\n",
      "epoch:15 step:14308 [D loss: 0.595169, acc.: 67.19%] [G loss: 0.494329]\n",
      "epoch:15 step:14309 [D loss: 0.541650, acc.: 73.44%] [G loss: 0.507780]\n",
      "epoch:15 step:14310 [D loss: 0.556458, acc.: 67.97%] [G loss: 0.575070]\n",
      "epoch:15 step:14311 [D loss: 0.537226, acc.: 69.53%] [G loss: 0.583523]\n",
      "epoch:15 step:14312 [D loss: 0.573618, acc.: 66.41%] [G loss: 0.601852]\n",
      "epoch:15 step:14313 [D loss: 0.542837, acc.: 71.09%] [G loss: 0.507998]\n",
      "epoch:15 step:14314 [D loss: 0.542503, acc.: 71.88%] [G loss: 0.551097]\n",
      "epoch:15 step:14315 [D loss: 0.554882, acc.: 68.75%] [G loss: 0.678475]\n",
      "epoch:15 step:14316 [D loss: 0.571282, acc.: 64.84%] [G loss: 0.522838]\n",
      "epoch:15 step:14317 [D loss: 0.535547, acc.: 72.66%] [G loss: 0.508489]\n",
      "epoch:15 step:14318 [D loss: 0.604973, acc.: 68.75%] [G loss: 0.541904]\n",
      "epoch:15 step:14319 [D loss: 0.509802, acc.: 73.44%] [G loss: 0.691707]\n",
      "epoch:15 step:14320 [D loss: 0.569672, acc.: 70.31%] [G loss: 0.546539]\n",
      "epoch:15 step:14321 [D loss: 0.607378, acc.: 67.97%] [G loss: 0.460928]\n",
      "epoch:15 step:14322 [D loss: 0.579919, acc.: 64.06%] [G loss: 0.491639]\n",
      "epoch:15 step:14323 [D loss: 0.538166, acc.: 71.88%] [G loss: 0.545496]\n",
      "epoch:15 step:14324 [D loss: 0.516489, acc.: 75.00%] [G loss: 0.565026]\n",
      "epoch:15 step:14325 [D loss: 0.538226, acc.: 74.22%] [G loss: 0.617638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14326 [D loss: 0.514602, acc.: 75.78%] [G loss: 0.486700]\n",
      "epoch:15 step:14327 [D loss: 0.530246, acc.: 72.66%] [G loss: 0.622265]\n",
      "epoch:15 step:14328 [D loss: 0.503347, acc.: 75.78%] [G loss: 0.614888]\n",
      "epoch:15 step:14329 [D loss: 0.496721, acc.: 78.12%] [G loss: 0.622956]\n",
      "epoch:15 step:14330 [D loss: 0.556731, acc.: 70.31%] [G loss: 0.588449]\n",
      "epoch:15 step:14331 [D loss: 0.472892, acc.: 78.91%] [G loss: 0.752170]\n",
      "epoch:15 step:14332 [D loss: 0.700882, acc.: 57.81%] [G loss: 0.472911]\n",
      "epoch:15 step:14333 [D loss: 0.630808, acc.: 56.25%] [G loss: 0.516255]\n",
      "epoch:15 step:14334 [D loss: 0.600737, acc.: 60.94%] [G loss: 0.495166]\n",
      "epoch:15 step:14335 [D loss: 0.534305, acc.: 68.75%] [G loss: 0.544607]\n",
      "epoch:15 step:14336 [D loss: 0.623168, acc.: 61.72%] [G loss: 0.492929]\n",
      "epoch:15 step:14337 [D loss: 0.558023, acc.: 74.22%] [G loss: 0.442534]\n",
      "epoch:15 step:14338 [D loss: 0.499678, acc.: 75.78%] [G loss: 0.589419]\n",
      "epoch:15 step:14339 [D loss: 0.521623, acc.: 72.66%] [G loss: 0.583724]\n",
      "epoch:15 step:14340 [D loss: 0.522180, acc.: 70.31%] [G loss: 0.644357]\n",
      "epoch:15 step:14341 [D loss: 0.513721, acc.: 72.66%] [G loss: 0.539536]\n",
      "epoch:15 step:14342 [D loss: 0.609639, acc.: 64.84%] [G loss: 0.550172]\n",
      "epoch:15 step:14343 [D loss: 0.572559, acc.: 66.41%] [G loss: 0.612492]\n",
      "epoch:15 step:14344 [D loss: 0.559198, acc.: 69.53%] [G loss: 0.609861]\n",
      "epoch:15 step:14345 [D loss: 0.572483, acc.: 69.53%] [G loss: 0.579816]\n",
      "epoch:15 step:14346 [D loss: 0.576642, acc.: 70.31%] [G loss: 0.521565]\n",
      "epoch:15 step:14347 [D loss: 0.526958, acc.: 75.78%] [G loss: 0.582805]\n",
      "epoch:15 step:14348 [D loss: 0.570203, acc.: 67.97%] [G loss: 0.638440]\n",
      "epoch:15 step:14349 [D loss: 0.616279, acc.: 60.16%] [G loss: 0.462593]\n",
      "epoch:15 step:14350 [D loss: 0.518758, acc.: 72.66%] [G loss: 0.574705]\n",
      "epoch:15 step:14351 [D loss: 0.474262, acc.: 76.56%] [G loss: 0.651835]\n",
      "epoch:15 step:14352 [D loss: 0.513277, acc.: 73.44%] [G loss: 0.669941]\n",
      "epoch:15 step:14353 [D loss: 0.471031, acc.: 79.69%] [G loss: 0.635554]\n",
      "epoch:15 step:14354 [D loss: 0.563577, acc.: 67.97%] [G loss: 0.539438]\n",
      "epoch:15 step:14355 [D loss: 0.501105, acc.: 78.91%] [G loss: 0.566926]\n",
      "epoch:15 step:14356 [D loss: 0.621564, acc.: 69.53%] [G loss: 0.520924]\n",
      "epoch:15 step:14357 [D loss: 0.546442, acc.: 67.19%] [G loss: 0.594246]\n",
      "epoch:15 step:14358 [D loss: 0.550743, acc.: 75.00%] [G loss: 0.596028]\n",
      "epoch:15 step:14359 [D loss: 0.498234, acc.: 74.22%] [G loss: 0.538792]\n",
      "epoch:15 step:14360 [D loss: 0.543853, acc.: 70.31%] [G loss: 0.634173]\n",
      "epoch:15 step:14361 [D loss: 0.541537, acc.: 72.66%] [G loss: 0.704695]\n",
      "epoch:15 step:14362 [D loss: 0.519663, acc.: 75.00%] [G loss: 0.699141]\n",
      "epoch:15 step:14363 [D loss: 0.611332, acc.: 62.50%] [G loss: 0.608218]\n",
      "epoch:15 step:14364 [D loss: 0.553220, acc.: 68.75%] [G loss: 0.618138]\n",
      "epoch:15 step:14365 [D loss: 0.526101, acc.: 75.00%] [G loss: 0.597235]\n",
      "epoch:15 step:14366 [D loss: 0.506991, acc.: 70.31%] [G loss: 0.608285]\n",
      "epoch:15 step:14367 [D loss: 0.490561, acc.: 77.34%] [G loss: 0.690507]\n",
      "epoch:15 step:14368 [D loss: 0.506273, acc.: 71.09%] [G loss: 0.927182]\n",
      "epoch:15 step:14369 [D loss: 0.441939, acc.: 78.12%] [G loss: 0.961872]\n",
      "epoch:15 step:14370 [D loss: 0.467676, acc.: 79.69%] [G loss: 0.981096]\n",
      "epoch:15 step:14371 [D loss: 0.752494, acc.: 53.12%] [G loss: 0.622895]\n",
      "epoch:15 step:14372 [D loss: 0.571068, acc.: 69.53%] [G loss: 0.609081]\n",
      "epoch:15 step:14373 [D loss: 0.522214, acc.: 71.88%] [G loss: 0.811143]\n",
      "epoch:15 step:14374 [D loss: 0.536401, acc.: 72.66%] [G loss: 0.592773]\n",
      "epoch:15 step:14375 [D loss: 0.557207, acc.: 67.97%] [G loss: 0.612405]\n",
      "epoch:15 step:14376 [D loss: 0.531596, acc.: 70.31%] [G loss: 0.629454]\n",
      "epoch:15 step:14377 [D loss: 0.561801, acc.: 69.53%] [G loss: 0.620772]\n",
      "epoch:15 step:14378 [D loss: 0.582651, acc.: 65.62%] [G loss: 0.700175]\n",
      "epoch:15 step:14379 [D loss: 0.588651, acc.: 66.41%] [G loss: 0.521123]\n",
      "epoch:15 step:14380 [D loss: 0.537740, acc.: 67.97%] [G loss: 0.587891]\n",
      "epoch:15 step:14381 [D loss: 0.518950, acc.: 74.22%] [G loss: 0.596874]\n",
      "epoch:15 step:14382 [D loss: 0.547050, acc.: 71.09%] [G loss: 0.781908]\n",
      "epoch:15 step:14383 [D loss: 0.524002, acc.: 72.66%] [G loss: 0.757216]\n",
      "epoch:15 step:14384 [D loss: 0.507900, acc.: 73.44%] [G loss: 0.730849]\n",
      "epoch:15 step:14385 [D loss: 0.556699, acc.: 68.75%] [G loss: 0.477749]\n",
      "epoch:15 step:14386 [D loss: 0.556264, acc.: 69.53%] [G loss: 0.525946]\n",
      "epoch:15 step:14387 [D loss: 0.551387, acc.: 67.97%] [G loss: 0.543971]\n",
      "epoch:15 step:14388 [D loss: 0.501011, acc.: 75.00%] [G loss: 0.683211]\n",
      "epoch:15 step:14389 [D loss: 0.494173, acc.: 74.22%] [G loss: 0.790646]\n",
      "epoch:15 step:14390 [D loss: 0.488059, acc.: 76.56%] [G loss: 0.649112]\n",
      "epoch:15 step:14391 [D loss: 0.497508, acc.: 75.00%] [G loss: 0.686054]\n",
      "epoch:15 step:14392 [D loss: 0.521729, acc.: 71.09%] [G loss: 0.662494]\n",
      "epoch:15 step:14393 [D loss: 0.614619, acc.: 66.41%] [G loss: 0.563820]\n",
      "epoch:15 step:14394 [D loss: 0.542103, acc.: 70.31%] [G loss: 0.578461]\n",
      "epoch:15 step:14395 [D loss: 0.517184, acc.: 73.44%] [G loss: 0.563900]\n",
      "epoch:15 step:14396 [D loss: 0.582208, acc.: 69.53%] [G loss: 0.579700]\n",
      "epoch:15 step:14397 [D loss: 0.630356, acc.: 63.28%] [G loss: 0.629465]\n",
      "epoch:15 step:14398 [D loss: 0.471980, acc.: 78.91%] [G loss: 0.616980]\n",
      "epoch:15 step:14399 [D loss: 0.492617, acc.: 75.00%] [G loss: 0.718895]\n",
      "epoch:15 step:14400 [D loss: 0.565132, acc.: 71.09%] [G loss: 0.761472]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.661851\n",
      "FID: 44.929760\n",
      "0 = 12.908643347644789\n",
      "1 = 0.09529556374086343\n",
      "2 = 0.8772000074386597\n",
      "3 = 0.8216000199317932\n",
      "4 = 0.9327999949455261\n",
      "5 = 0.9243924617767334\n",
      "6 = 0.8216000199317932\n",
      "7 = 8.12492311408517\n",
      "8 = 0.14591360153792904\n",
      "9 = 0.7093999981880188\n",
      "10 = 0.704200029373169\n",
      "11 = 0.7146000266075134\n",
      "12 = 0.711600661277771\n",
      "13 = 0.704200029373169\n",
      "14 = 6.661874771118164\n",
      "15 = 7.162635803222656\n",
      "16 = 0.37478408217430115\n",
      "17 = 6.661851406097412\n",
      "18 = 44.92975997924805\n",
      "epoch:15 step:14401 [D loss: 0.567750, acc.: 66.41%] [G loss: 0.641558]\n",
      "epoch:15 step:14402 [D loss: 0.463577, acc.: 77.34%] [G loss: 0.885435]\n",
      "epoch:15 step:14403 [D loss: 0.647699, acc.: 61.72%] [G loss: 0.599884]\n",
      "epoch:15 step:14404 [D loss: 0.669919, acc.: 62.50%] [G loss: 0.430124]\n",
      "epoch:15 step:14405 [D loss: 0.544086, acc.: 68.75%] [G loss: 0.478972]\n",
      "epoch:15 step:14406 [D loss: 0.555783, acc.: 71.88%] [G loss: 0.495199]\n",
      "epoch:15 step:14407 [D loss: 0.554898, acc.: 71.09%] [G loss: 0.662811]\n",
      "epoch:15 step:14408 [D loss: 0.538310, acc.: 71.09%] [G loss: 0.671941]\n",
      "epoch:15 step:14409 [D loss: 0.386836, acc.: 83.59%] [G loss: 0.664696]\n",
      "epoch:15 step:14410 [D loss: 0.625179, acc.: 65.62%] [G loss: 0.699046]\n",
      "epoch:15 step:14411 [D loss: 0.553293, acc.: 69.53%] [G loss: 0.735740]\n",
      "epoch:15 step:14412 [D loss: 0.438654, acc.: 82.81%] [G loss: 0.763756]\n",
      "epoch:15 step:14413 [D loss: 0.426752, acc.: 78.91%] [G loss: 0.648643]\n",
      "epoch:15 step:14414 [D loss: 0.455011, acc.: 79.69%] [G loss: 0.836947]\n",
      "epoch:15 step:14415 [D loss: 0.457976, acc.: 78.12%] [G loss: 0.841884]\n",
      "epoch:15 step:14416 [D loss: 0.503242, acc.: 75.78%] [G loss: 0.692361]\n",
      "epoch:15 step:14417 [D loss: 0.578961, acc.: 70.31%] [G loss: 0.651062]\n",
      "epoch:15 step:14418 [D loss: 0.592295, acc.: 67.97%] [G loss: 0.625001]\n",
      "epoch:15 step:14419 [D loss: 0.551521, acc.: 70.31%] [G loss: 0.499821]\n",
      "epoch:15 step:14420 [D loss: 0.522758, acc.: 71.88%] [G loss: 0.541772]\n",
      "epoch:15 step:14421 [D loss: 0.527473, acc.: 72.66%] [G loss: 0.735821]\n",
      "epoch:15 step:14422 [D loss: 0.674434, acc.: 58.59%] [G loss: 0.575268]\n",
      "epoch:15 step:14423 [D loss: 0.532528, acc.: 68.75%] [G loss: 0.692324]\n",
      "epoch:15 step:14424 [D loss: 0.487063, acc.: 73.44%] [G loss: 0.673611]\n",
      "epoch:15 step:14425 [D loss: 0.570502, acc.: 72.66%] [G loss: 0.639957]\n",
      "epoch:15 step:14426 [D loss: 0.504550, acc.: 75.00%] [G loss: 0.589009]\n",
      "epoch:15 step:14427 [D loss: 0.517692, acc.: 71.88%] [G loss: 0.573113]\n",
      "epoch:15 step:14428 [D loss: 0.555075, acc.: 66.41%] [G loss: 0.763832]\n",
      "epoch:15 step:14429 [D loss: 0.466109, acc.: 75.00%] [G loss: 0.637345]\n",
      "epoch:15 step:14430 [D loss: 0.584079, acc.: 65.62%] [G loss: 0.680118]\n",
      "epoch:15 step:14431 [D loss: 0.708015, acc.: 53.12%] [G loss: 0.413708]\n",
      "epoch:15 step:14432 [D loss: 0.563683, acc.: 64.06%] [G loss: 0.482796]\n",
      "epoch:15 step:14433 [D loss: 0.537463, acc.: 69.53%] [G loss: 0.560361]\n",
      "epoch:15 step:14434 [D loss: 0.494441, acc.: 74.22%] [G loss: 0.703503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14435 [D loss: 0.570392, acc.: 65.62%] [G loss: 0.649947]\n",
      "epoch:15 step:14436 [D loss: 0.438548, acc.: 81.25%] [G loss: 0.696655]\n",
      "epoch:15 step:14437 [D loss: 0.556894, acc.: 69.53%] [G loss: 0.669624]\n",
      "epoch:15 step:14438 [D loss: 0.534587, acc.: 70.31%] [G loss: 0.565207]\n",
      "epoch:15 step:14439 [D loss: 0.518981, acc.: 71.88%] [G loss: 0.573474]\n",
      "epoch:15 step:14440 [D loss: 0.482814, acc.: 75.00%] [G loss: 0.729992]\n",
      "epoch:15 step:14441 [D loss: 0.576665, acc.: 68.75%] [G loss: 0.659523]\n",
      "epoch:15 step:14442 [D loss: 0.576015, acc.: 71.88%] [G loss: 0.591104]\n",
      "epoch:15 step:14443 [D loss: 0.557613, acc.: 71.09%] [G loss: 0.684308]\n",
      "epoch:15 step:14444 [D loss: 0.569311, acc.: 67.97%] [G loss: 0.535191]\n",
      "epoch:15 step:14445 [D loss: 0.582528, acc.: 65.62%] [G loss: 0.547062]\n",
      "epoch:15 step:14446 [D loss: 0.537623, acc.: 67.97%] [G loss: 0.557042]\n",
      "epoch:15 step:14447 [D loss: 0.493969, acc.: 69.53%] [G loss: 0.773967]\n",
      "epoch:15 step:14448 [D loss: 0.602197, acc.: 67.19%] [G loss: 0.520563]\n",
      "epoch:15 step:14449 [D loss: 0.565474, acc.: 69.53%] [G loss: 0.519690]\n",
      "epoch:15 step:14450 [D loss: 0.549845, acc.: 71.09%] [G loss: 0.562805]\n",
      "epoch:15 step:14451 [D loss: 0.530064, acc.: 68.75%] [G loss: 0.653831]\n",
      "epoch:15 step:14452 [D loss: 0.620379, acc.: 61.72%] [G loss: 0.591805]\n",
      "epoch:15 step:14453 [D loss: 0.482538, acc.: 75.00%] [G loss: 0.790046]\n",
      "epoch:15 step:14454 [D loss: 0.543891, acc.: 71.09%] [G loss: 0.721303]\n",
      "epoch:15 step:14455 [D loss: 0.655133, acc.: 58.59%] [G loss: 0.612130]\n",
      "epoch:15 step:14456 [D loss: 0.691703, acc.: 53.12%] [G loss: 0.454834]\n",
      "epoch:15 step:14457 [D loss: 0.494356, acc.: 72.66%] [G loss: 0.568982]\n",
      "epoch:15 step:14458 [D loss: 0.487723, acc.: 74.22%] [G loss: 0.618947]\n",
      "epoch:15 step:14459 [D loss: 0.615830, acc.: 61.72%] [G loss: 0.539819]\n",
      "epoch:15 step:14460 [D loss: 0.581442, acc.: 61.72%] [G loss: 0.638776]\n",
      "epoch:15 step:14461 [D loss: 0.490734, acc.: 72.66%] [G loss: 0.718347]\n",
      "epoch:15 step:14462 [D loss: 0.601169, acc.: 64.06%] [G loss: 0.731487]\n",
      "epoch:15 step:14463 [D loss: 0.570724, acc.: 71.09%] [G loss: 0.645040]\n",
      "epoch:15 step:14464 [D loss: 0.555220, acc.: 67.19%] [G loss: 0.560013]\n",
      "epoch:15 step:14465 [D loss: 0.549711, acc.: 67.97%] [G loss: 0.575006]\n",
      "epoch:15 step:14466 [D loss: 0.570163, acc.: 67.19%] [G loss: 0.589848]\n",
      "epoch:15 step:14467 [D loss: 0.609562, acc.: 65.62%] [G loss: 0.555003]\n",
      "epoch:15 step:14468 [D loss: 0.548510, acc.: 74.22%] [G loss: 0.500301]\n",
      "epoch:15 step:14469 [D loss: 0.513080, acc.: 71.09%] [G loss: 0.671476]\n",
      "epoch:15 step:14470 [D loss: 0.520153, acc.: 73.44%] [G loss: 0.625727]\n",
      "epoch:15 step:14471 [D loss: 0.569672, acc.: 68.75%] [G loss: 0.717366]\n",
      "epoch:15 step:14472 [D loss: 0.548213, acc.: 69.53%] [G loss: 0.582737]\n",
      "epoch:15 step:14473 [D loss: 0.634344, acc.: 63.28%] [G loss: 0.697277]\n",
      "epoch:15 step:14474 [D loss: 0.597224, acc.: 66.41%] [G loss: 0.543075]\n",
      "epoch:15 step:14475 [D loss: 0.616589, acc.: 58.59%] [G loss: 0.559730]\n",
      "epoch:15 step:14476 [D loss: 0.551847, acc.: 71.09%] [G loss: 0.550244]\n",
      "epoch:15 step:14477 [D loss: 0.624840, acc.: 67.19%] [G loss: 0.475311]\n",
      "epoch:15 step:14478 [D loss: 0.576557, acc.: 69.53%] [G loss: 0.520974]\n",
      "epoch:15 step:14479 [D loss: 0.550170, acc.: 67.97%] [G loss: 0.462719]\n",
      "epoch:15 step:14480 [D loss: 0.492867, acc.: 75.00%] [G loss: 0.553729]\n",
      "epoch:15 step:14481 [D loss: 0.485409, acc.: 75.00%] [G loss: 0.720034]\n",
      "epoch:15 step:14482 [D loss: 0.434722, acc.: 80.47%] [G loss: 0.731110]\n",
      "epoch:15 step:14483 [D loss: 0.552045, acc.: 72.66%] [G loss: 0.701203]\n",
      "epoch:15 step:14484 [D loss: 0.469548, acc.: 79.69%] [G loss: 0.728548]\n",
      "epoch:15 step:14485 [D loss: 0.568223, acc.: 71.88%] [G loss: 0.867355]\n",
      "epoch:15 step:14486 [D loss: 0.578271, acc.: 67.97%] [G loss: 0.819125]\n",
      "epoch:15 step:14487 [D loss: 0.575111, acc.: 64.84%] [G loss: 0.712425]\n",
      "epoch:15 step:14488 [D loss: 0.610700, acc.: 68.75%] [G loss: 0.557452]\n",
      "epoch:15 step:14489 [D loss: 0.576859, acc.: 67.19%] [G loss: 0.520069]\n",
      "epoch:15 step:14490 [D loss: 0.528905, acc.: 72.66%] [G loss: 0.630118]\n",
      "epoch:15 step:14491 [D loss: 0.563457, acc.: 71.88%] [G loss: 0.622435]\n",
      "epoch:15 step:14492 [D loss: 0.644947, acc.: 64.84%] [G loss: 0.462938]\n",
      "epoch:15 step:14493 [D loss: 0.571432, acc.: 64.84%] [G loss: 0.597675]\n",
      "epoch:15 step:14494 [D loss: 0.474587, acc.: 79.69%] [G loss: 0.695519]\n",
      "epoch:15 step:14495 [D loss: 0.511990, acc.: 75.00%] [G loss: 0.545918]\n",
      "epoch:15 step:14496 [D loss: 0.517195, acc.: 70.31%] [G loss: 0.654857]\n",
      "epoch:15 step:14497 [D loss: 0.580123, acc.: 65.62%] [G loss: 0.783382]\n",
      "epoch:15 step:14498 [D loss: 0.550633, acc.: 70.31%] [G loss: 0.786169]\n",
      "epoch:15 step:14499 [D loss: 0.491731, acc.: 71.09%] [G loss: 0.660217]\n",
      "epoch:15 step:14500 [D loss: 0.576218, acc.: 64.06%] [G loss: 0.545405]\n",
      "epoch:15 step:14501 [D loss: 0.515131, acc.: 71.09%] [G loss: 0.652504]\n",
      "epoch:15 step:14502 [D loss: 0.589179, acc.: 65.62%] [G loss: 0.767972]\n",
      "epoch:15 step:14503 [D loss: 0.521822, acc.: 74.22%] [G loss: 0.688052]\n",
      "epoch:15 step:14504 [D loss: 0.509701, acc.: 75.00%] [G loss: 0.698432]\n",
      "epoch:15 step:14505 [D loss: 0.466686, acc.: 79.69%] [G loss: 0.899087]\n",
      "epoch:15 step:14506 [D loss: 0.449848, acc.: 82.03%] [G loss: 0.763669]\n",
      "epoch:15 step:14507 [D loss: 0.477277, acc.: 76.56%] [G loss: 0.675794]\n",
      "epoch:15 step:14508 [D loss: 0.504140, acc.: 75.00%] [G loss: 0.802998]\n",
      "epoch:15 step:14509 [D loss: 0.576800, acc.: 70.31%] [G loss: 0.684097]\n",
      "epoch:15 step:14510 [D loss: 0.599269, acc.: 68.75%] [G loss: 0.548560]\n",
      "epoch:15 step:14511 [D loss: 0.669510, acc.: 61.72%] [G loss: 0.513895]\n",
      "epoch:15 step:14512 [D loss: 0.541978, acc.: 67.97%] [G loss: 0.573053]\n",
      "epoch:15 step:14513 [D loss: 0.584345, acc.: 62.50%] [G loss: 0.516519]\n",
      "epoch:15 step:14514 [D loss: 0.548988, acc.: 67.19%] [G loss: 0.668505]\n",
      "epoch:15 step:14515 [D loss: 0.542493, acc.: 67.97%] [G loss: 0.643921]\n",
      "epoch:15 step:14516 [D loss: 0.546475, acc.: 71.88%] [G loss: 0.554339]\n",
      "epoch:15 step:14517 [D loss: 0.554896, acc.: 67.97%] [G loss: 0.530867]\n",
      "epoch:15 step:14518 [D loss: 0.577850, acc.: 67.19%] [G loss: 0.572583]\n",
      "epoch:15 step:14519 [D loss: 0.541677, acc.: 72.66%] [G loss: 0.519778]\n",
      "epoch:15 step:14520 [D loss: 0.602734, acc.: 70.31%] [G loss: 0.535346]\n",
      "epoch:15 step:14521 [D loss: 0.557910, acc.: 70.31%] [G loss: 0.605217]\n",
      "epoch:15 step:14522 [D loss: 0.506270, acc.: 74.22%] [G loss: 0.665432]\n",
      "epoch:15 step:14523 [D loss: 0.608638, acc.: 62.50%] [G loss: 0.638136]\n",
      "epoch:15 step:14524 [D loss: 0.521924, acc.: 71.88%] [G loss: 0.812133]\n",
      "epoch:15 step:14525 [D loss: 0.534153, acc.: 71.09%] [G loss: 0.749519]\n",
      "epoch:15 step:14526 [D loss: 0.428958, acc.: 80.47%] [G loss: 0.991765]\n",
      "epoch:15 step:14527 [D loss: 0.414866, acc.: 81.25%] [G loss: 0.974857]\n",
      "epoch:15 step:14528 [D loss: 0.661108, acc.: 57.81%] [G loss: 0.674711]\n",
      "epoch:15 step:14529 [D loss: 0.515940, acc.: 73.44%] [G loss: 0.469410]\n",
      "epoch:15 step:14530 [D loss: 0.475712, acc.: 77.34%] [G loss: 0.639358]\n",
      "epoch:15 step:14531 [D loss: 0.562965, acc.: 72.66%] [G loss: 0.639649]\n",
      "epoch:15 step:14532 [D loss: 0.691929, acc.: 60.94%] [G loss: 0.603815]\n",
      "epoch:15 step:14533 [D loss: 0.555222, acc.: 68.75%] [G loss: 0.513811]\n",
      "epoch:15 step:14534 [D loss: 0.538535, acc.: 73.44%] [G loss: 0.577247]\n",
      "epoch:15 step:14535 [D loss: 0.567614, acc.: 69.53%] [G loss: 0.439767]\n",
      "epoch:15 step:14536 [D loss: 0.494706, acc.: 78.91%] [G loss: 0.555830]\n",
      "epoch:15 step:14537 [D loss: 0.645955, acc.: 60.94%] [G loss: 0.478928]\n",
      "epoch:15 step:14538 [D loss: 0.522647, acc.: 72.66%] [G loss: 0.610298]\n",
      "epoch:15 step:14539 [D loss: 0.534032, acc.: 74.22%] [G loss: 0.546998]\n",
      "epoch:15 step:14540 [D loss: 0.518551, acc.: 75.00%] [G loss: 0.644005]\n",
      "epoch:15 step:14541 [D loss: 0.618201, acc.: 61.72%] [G loss: 0.714112]\n",
      "epoch:15 step:14542 [D loss: 0.574428, acc.: 70.31%] [G loss: 0.658892]\n",
      "epoch:15 step:14543 [D loss: 0.487104, acc.: 73.44%] [G loss: 0.688919]\n",
      "epoch:15 step:14544 [D loss: 0.547687, acc.: 68.75%] [G loss: 0.629233]\n",
      "epoch:15 step:14545 [D loss: 0.575922, acc.: 69.53%] [G loss: 0.519073]\n",
      "epoch:15 step:14546 [D loss: 0.548301, acc.: 70.31%] [G loss: 0.618496]\n",
      "epoch:15 step:14547 [D loss: 0.578978, acc.: 68.75%] [G loss: 0.555333]\n",
      "epoch:15 step:14548 [D loss: 0.575951, acc.: 65.62%] [G loss: 0.504077]\n",
      "epoch:15 step:14549 [D loss: 0.582201, acc.: 71.88%] [G loss: 0.652533]\n",
      "epoch:15 step:14550 [D loss: 0.485366, acc.: 76.56%] [G loss: 0.676329]\n",
      "epoch:15 step:14551 [D loss: 0.538857, acc.: 71.88%] [G loss: 0.672576]\n",
      "epoch:15 step:14552 [D loss: 0.542148, acc.: 71.88%] [G loss: 0.718946]\n",
      "epoch:15 step:14553 [D loss: 0.569613, acc.: 71.09%] [G loss: 0.545719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14554 [D loss: 0.490973, acc.: 76.56%] [G loss: 0.848518]\n",
      "epoch:15 step:14555 [D loss: 0.600868, acc.: 71.88%] [G loss: 0.463987]\n",
      "epoch:15 step:14556 [D loss: 0.684224, acc.: 60.94%] [G loss: 0.392205]\n",
      "epoch:15 step:14557 [D loss: 0.601025, acc.: 66.41%] [G loss: 0.415926]\n",
      "epoch:15 step:14558 [D loss: 0.529153, acc.: 72.66%] [G loss: 0.538800]\n",
      "epoch:15 step:14559 [D loss: 0.502760, acc.: 75.78%] [G loss: 0.731458]\n",
      "epoch:15 step:14560 [D loss: 0.493632, acc.: 73.44%] [G loss: 0.827215]\n",
      "epoch:15 step:14561 [D loss: 0.523560, acc.: 71.88%] [G loss: 0.662047]\n",
      "epoch:15 step:14562 [D loss: 0.538888, acc.: 73.44%] [G loss: 0.839443]\n",
      "epoch:15 step:14563 [D loss: 0.454710, acc.: 78.12%] [G loss: 0.838462]\n",
      "epoch:15 step:14564 [D loss: 0.458622, acc.: 77.34%] [G loss: 0.679636]\n",
      "epoch:15 step:14565 [D loss: 0.641039, acc.: 62.50%] [G loss: 0.626259]\n",
      "epoch:15 step:14566 [D loss: 0.615951, acc.: 64.06%] [G loss: 0.545681]\n",
      "epoch:15 step:14567 [D loss: 0.609209, acc.: 62.50%] [G loss: 0.609022]\n",
      "epoch:15 step:14568 [D loss: 0.522239, acc.: 74.22%] [G loss: 0.490158]\n",
      "epoch:15 step:14569 [D loss: 0.588991, acc.: 64.06%] [G loss: 0.569504]\n",
      "epoch:15 step:14570 [D loss: 0.545793, acc.: 68.75%] [G loss: 0.697347]\n",
      "epoch:15 step:14571 [D loss: 0.467015, acc.: 75.78%] [G loss: 0.714573]\n",
      "epoch:15 step:14572 [D loss: 0.510287, acc.: 75.00%] [G loss: 0.647322]\n",
      "epoch:15 step:14573 [D loss: 0.539072, acc.: 68.75%] [G loss: 0.598556]\n",
      "epoch:15 step:14574 [D loss: 0.438257, acc.: 82.03%] [G loss: 0.682711]\n",
      "epoch:15 step:14575 [D loss: 0.471918, acc.: 78.91%] [G loss: 0.787503]\n",
      "epoch:15 step:14576 [D loss: 0.516136, acc.: 73.44%] [G loss: 0.601339]\n",
      "epoch:15 step:14577 [D loss: 0.486100, acc.: 78.91%] [G loss: 0.718907]\n",
      "epoch:15 step:14578 [D loss: 0.476954, acc.: 75.00%] [G loss: 0.801964]\n",
      "epoch:15 step:14579 [D loss: 0.541332, acc.: 73.44%] [G loss: 0.673745]\n",
      "epoch:15 step:14580 [D loss: 0.625646, acc.: 64.06%] [G loss: 0.573383]\n",
      "epoch:15 step:14581 [D loss: 0.530656, acc.: 70.31%] [G loss: 0.652485]\n",
      "epoch:15 step:14582 [D loss: 0.553820, acc.: 69.53%] [G loss: 0.653087]\n",
      "epoch:15 step:14583 [D loss: 0.633739, acc.: 57.81%] [G loss: 0.485113]\n",
      "epoch:15 step:14584 [D loss: 0.586267, acc.: 63.28%] [G loss: 0.624734]\n",
      "epoch:15 step:14585 [D loss: 0.513104, acc.: 71.09%] [G loss: 0.692756]\n",
      "epoch:15 step:14586 [D loss: 0.593033, acc.: 64.06%] [G loss: 0.624148]\n",
      "epoch:15 step:14587 [D loss: 0.622219, acc.: 69.53%] [G loss: 0.658110]\n",
      "epoch:15 step:14588 [D loss: 0.543434, acc.: 69.53%] [G loss: 0.670761]\n",
      "epoch:15 step:14589 [D loss: 0.525446, acc.: 74.22%] [G loss: 0.681948]\n",
      "epoch:15 step:14590 [D loss: 0.601245, acc.: 64.84%] [G loss: 0.570807]\n",
      "epoch:15 step:14591 [D loss: 0.541492, acc.: 67.97%] [G loss: 0.462555]\n",
      "epoch:15 step:14592 [D loss: 0.585423, acc.: 67.97%] [G loss: 0.493446]\n",
      "epoch:15 step:14593 [D loss: 0.628013, acc.: 64.06%] [G loss: 0.551448]\n",
      "epoch:15 step:14594 [D loss: 0.553236, acc.: 74.22%] [G loss: 0.608883]\n",
      "epoch:15 step:14595 [D loss: 0.566725, acc.: 67.19%] [G loss: 0.502289]\n",
      "epoch:15 step:14596 [D loss: 0.472995, acc.: 80.47%] [G loss: 0.623541]\n",
      "epoch:15 step:14597 [D loss: 0.633138, acc.: 63.28%] [G loss: 0.587361]\n",
      "epoch:15 step:14598 [D loss: 0.540672, acc.: 69.53%] [G loss: 0.581357]\n",
      "epoch:15 step:14599 [D loss: 0.559506, acc.: 71.88%] [G loss: 0.539929]\n",
      "epoch:15 step:14600 [D loss: 0.505069, acc.: 75.78%] [G loss: 0.679742]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.660373\n",
      "FID: 41.562527\n",
      "0 = 12.773548534679415\n",
      "1 = 0.08446136575557994\n",
      "2 = 0.878600001335144\n",
      "3 = 0.8162000179290771\n",
      "4 = 0.9409999847412109\n",
      "5 = 0.9325868487358093\n",
      "6 = 0.8162000179290771\n",
      "7 = 7.885602665507801\n",
      "8 = 0.1332766332866516\n",
      "9 = 0.7197999954223633\n",
      "10 = 0.6991999745368958\n",
      "11 = 0.7404000163078308\n",
      "12 = 0.7292448878288269\n",
      "13 = 0.6991999745368958\n",
      "14 = 6.660402297973633\n",
      "15 = 7.373763084411621\n",
      "16 = 0.3593965470790863\n",
      "17 = 6.660373210906982\n",
      "18 = 41.56252670288086\n",
      "epoch:15 step:14601 [D loss: 0.536830, acc.: 68.75%] [G loss: 0.641700]\n",
      "epoch:15 step:14602 [D loss: 0.516396, acc.: 72.66%] [G loss: 0.603353]\n",
      "epoch:15 step:14603 [D loss: 0.526548, acc.: 73.44%] [G loss: 0.761943]\n",
      "epoch:15 step:14604 [D loss: 0.564093, acc.: 69.53%] [G loss: 0.658715]\n",
      "epoch:15 step:14605 [D loss: 0.535422, acc.: 75.78%] [G loss: 0.674589]\n",
      "epoch:15 step:14606 [D loss: 0.511232, acc.: 74.22%] [G loss: 0.684655]\n",
      "epoch:15 step:14607 [D loss: 0.452017, acc.: 80.47%] [G loss: 0.810045]\n",
      "epoch:15 step:14608 [D loss: 0.572271, acc.: 69.53%] [G loss: 0.607211]\n",
      "epoch:15 step:14609 [D loss: 0.466631, acc.: 78.12%] [G loss: 0.665906]\n",
      "epoch:15 step:14610 [D loss: 0.484830, acc.: 75.00%] [G loss: 0.760277]\n",
      "epoch:15 step:14611 [D loss: 0.549608, acc.: 72.66%] [G loss: 0.639688]\n",
      "epoch:15 step:14612 [D loss: 0.522143, acc.: 72.66%] [G loss: 0.724126]\n",
      "epoch:15 step:14613 [D loss: 0.517767, acc.: 75.00%] [G loss: 0.598389]\n",
      "epoch:15 step:14614 [D loss: 0.635573, acc.: 63.28%] [G loss: 0.614112]\n",
      "epoch:15 step:14615 [D loss: 0.552148, acc.: 73.44%] [G loss: 0.534820]\n",
      "epoch:15 step:14616 [D loss: 0.545160, acc.: 74.22%] [G loss: 0.506548]\n",
      "epoch:15 step:14617 [D loss: 0.551277, acc.: 65.62%] [G loss: 0.605599]\n",
      "epoch:15 step:14618 [D loss: 0.619448, acc.: 62.50%] [G loss: 0.578104]\n",
      "epoch:15 step:14619 [D loss: 0.468331, acc.: 81.25%] [G loss: 0.668357]\n",
      "epoch:15 step:14620 [D loss: 0.587109, acc.: 67.19%] [G loss: 0.646422]\n",
      "epoch:15 step:14621 [D loss: 0.677271, acc.: 59.38%] [G loss: 0.589008]\n",
      "epoch:15 step:14622 [D loss: 0.508028, acc.: 72.66%] [G loss: 0.635972]\n",
      "epoch:15 step:14623 [D loss: 0.525013, acc.: 69.53%] [G loss: 0.732801]\n",
      "epoch:15 step:14624 [D loss: 0.557157, acc.: 70.31%] [G loss: 0.611757]\n",
      "epoch:15 step:14625 [D loss: 0.511238, acc.: 75.78%] [G loss: 0.658706]\n",
      "epoch:15 step:14626 [D loss: 0.575478, acc.: 67.19%] [G loss: 0.517803]\n",
      "epoch:15 step:14627 [D loss: 0.587616, acc.: 68.75%] [G loss: 0.634881]\n",
      "epoch:15 step:14628 [D loss: 0.601729, acc.: 66.41%] [G loss: 0.653908]\n",
      "epoch:15 step:14629 [D loss: 0.537905, acc.: 69.53%] [G loss: 0.741342]\n",
      "epoch:15 step:14630 [D loss: 0.505914, acc.: 76.56%] [G loss: 0.712769]\n",
      "epoch:15 step:14631 [D loss: 0.638760, acc.: 63.28%] [G loss: 0.705512]\n",
      "epoch:15 step:14632 [D loss: 0.574838, acc.: 68.75%] [G loss: 0.642024]\n",
      "epoch:15 step:14633 [D loss: 0.537491, acc.: 71.88%] [G loss: 0.550945]\n",
      "epoch:15 step:14634 [D loss: 0.564839, acc.: 71.09%] [G loss: 0.769838]\n",
      "epoch:15 step:14635 [D loss: 0.555196, acc.: 71.09%] [G loss: 0.535317]\n",
      "epoch:15 step:14636 [D loss: 0.552493, acc.: 73.44%] [G loss: 0.614274]\n",
      "epoch:15 step:14637 [D loss: 0.478168, acc.: 79.69%] [G loss: 0.736130]\n",
      "epoch:15 step:14638 [D loss: 0.553159, acc.: 70.31%] [G loss: 0.745093]\n",
      "epoch:15 step:14639 [D loss: 0.632849, acc.: 60.16%] [G loss: 0.602584]\n",
      "epoch:15 step:14640 [D loss: 0.565960, acc.: 67.97%] [G loss: 0.537044]\n",
      "epoch:15 step:14641 [D loss: 0.619763, acc.: 60.16%] [G loss: 0.487723]\n",
      "epoch:15 step:14642 [D loss: 0.549777, acc.: 68.75%] [G loss: 0.495711]\n",
      "epoch:15 step:14643 [D loss: 0.545309, acc.: 67.19%] [G loss: 0.554034]\n",
      "epoch:15 step:14644 [D loss: 0.544992, acc.: 67.19%] [G loss: 0.618615]\n",
      "epoch:15 step:14645 [D loss: 0.560626, acc.: 71.88%] [G loss: 0.602360]\n",
      "epoch:15 step:14646 [D loss: 0.584319, acc.: 66.41%] [G loss: 0.553843]\n",
      "epoch:15 step:14647 [D loss: 0.494686, acc.: 75.00%] [G loss: 0.671132]\n",
      "epoch:15 step:14648 [D loss: 0.563056, acc.: 68.75%] [G loss: 0.619967]\n",
      "epoch:15 step:14649 [D loss: 0.655141, acc.: 58.59%] [G loss: 0.636931]\n",
      "epoch:15 step:14650 [D loss: 0.553456, acc.: 70.31%] [G loss: 0.609199]\n",
      "epoch:15 step:14651 [D loss: 0.575143, acc.: 71.88%] [G loss: 0.503513]\n",
      "epoch:15 step:14652 [D loss: 0.559528, acc.: 67.19%] [G loss: 0.590900]\n",
      "epoch:15 step:14653 [D loss: 0.558447, acc.: 72.66%] [G loss: 0.569621]\n",
      "epoch:15 step:14654 [D loss: 0.543157, acc.: 70.31%] [G loss: 0.667763]\n",
      "epoch:15 step:14655 [D loss: 0.622890, acc.: 59.38%] [G loss: 0.641065]\n",
      "epoch:15 step:14656 [D loss: 0.537517, acc.: 67.19%] [G loss: 0.533651]\n",
      "epoch:15 step:14657 [D loss: 0.541909, acc.: 64.06%] [G loss: 0.549908]\n",
      "epoch:15 step:14658 [D loss: 0.499994, acc.: 75.00%] [G loss: 0.656770]\n",
      "epoch:15 step:14659 [D loss: 0.589478, acc.: 67.97%] [G loss: 0.610958]\n",
      "epoch:15 step:14660 [D loss: 0.495679, acc.: 77.34%] [G loss: 0.769727]\n",
      "epoch:15 step:14661 [D loss: 0.613091, acc.: 64.84%] [G loss: 0.577656]\n",
      "epoch:15 step:14662 [D loss: 0.558898, acc.: 64.84%] [G loss: 0.588004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14663 [D loss: 0.551293, acc.: 65.62%] [G loss: 0.604413]\n",
      "epoch:15 step:14664 [D loss: 0.532127, acc.: 72.66%] [G loss: 0.527261]\n",
      "epoch:15 step:14665 [D loss: 0.574493, acc.: 63.28%] [G loss: 0.472105]\n",
      "epoch:15 step:14666 [D loss: 0.490940, acc.: 76.56%] [G loss: 0.611966]\n",
      "epoch:15 step:14667 [D loss: 0.562245, acc.: 72.66%] [G loss: 0.490637]\n",
      "epoch:15 step:14668 [D loss: 0.533685, acc.: 74.22%] [G loss: 0.611510]\n",
      "epoch:15 step:14669 [D loss: 0.624486, acc.: 64.06%] [G loss: 0.371804]\n",
      "epoch:15 step:14670 [D loss: 0.527876, acc.: 69.53%] [G loss: 0.628691]\n",
      "epoch:15 step:14671 [D loss: 0.607995, acc.: 67.97%] [G loss: 0.574802]\n",
      "epoch:15 step:14672 [D loss: 0.536959, acc.: 72.66%] [G loss: 0.615258]\n",
      "epoch:15 step:14673 [D loss: 0.531348, acc.: 68.75%] [G loss: 0.587266]\n",
      "epoch:15 step:14674 [D loss: 0.536592, acc.: 73.44%] [G loss: 0.616006]\n",
      "epoch:15 step:14675 [D loss: 0.587051, acc.: 69.53%] [G loss: 0.670598]\n",
      "epoch:15 step:14676 [D loss: 0.571744, acc.: 70.31%] [G loss: 0.673927]\n",
      "epoch:15 step:14677 [D loss: 0.565844, acc.: 68.75%] [G loss: 0.666875]\n",
      "epoch:15 step:14678 [D loss: 0.464269, acc.: 80.47%] [G loss: 0.642310]\n",
      "epoch:15 step:14679 [D loss: 0.488715, acc.: 77.34%] [G loss: 0.686009]\n",
      "epoch:15 step:14680 [D loss: 0.590205, acc.: 65.62%] [G loss: 0.548098]\n",
      "epoch:15 step:14681 [D loss: 0.529319, acc.: 68.75%] [G loss: 0.555399]\n",
      "epoch:15 step:14682 [D loss: 0.583138, acc.: 66.41%] [G loss: 0.614650]\n",
      "epoch:15 step:14683 [D loss: 0.611342, acc.: 64.06%] [G loss: 0.544843]\n",
      "epoch:15 step:14684 [D loss: 0.516007, acc.: 74.22%] [G loss: 0.576250]\n",
      "epoch:15 step:14685 [D loss: 0.561171, acc.: 69.53%] [G loss: 0.539357]\n",
      "epoch:15 step:14686 [D loss: 0.500496, acc.: 75.78%] [G loss: 0.708586]\n",
      "epoch:15 step:14687 [D loss: 0.501696, acc.: 78.91%] [G loss: 0.585539]\n",
      "epoch:15 step:14688 [D loss: 0.468113, acc.: 80.47%] [G loss: 0.689800]\n",
      "epoch:15 step:14689 [D loss: 0.522224, acc.: 73.44%] [G loss: 0.636530]\n",
      "epoch:15 step:14690 [D loss: 0.464227, acc.: 73.44%] [G loss: 0.757375]\n",
      "epoch:15 step:14691 [D loss: 0.576810, acc.: 69.53%] [G loss: 0.589104]\n",
      "epoch:15 step:14692 [D loss: 0.565371, acc.: 66.41%] [G loss: 0.611693]\n",
      "epoch:15 step:14693 [D loss: 0.522898, acc.: 70.31%] [G loss: 0.638094]\n",
      "epoch:15 step:14694 [D loss: 0.488175, acc.: 70.31%] [G loss: 0.630408]\n",
      "epoch:15 step:14695 [D loss: 0.605555, acc.: 62.50%] [G loss: 0.624815]\n",
      "epoch:15 step:14696 [D loss: 0.478055, acc.: 75.78%] [G loss: 0.763379]\n",
      "epoch:15 step:14697 [D loss: 0.468380, acc.: 78.12%] [G loss: 0.817647]\n",
      "epoch:15 step:14698 [D loss: 0.523835, acc.: 73.44%] [G loss: 0.859363]\n",
      "epoch:15 step:14699 [D loss: 0.610616, acc.: 60.94%] [G loss: 0.657769]\n",
      "epoch:15 step:14700 [D loss: 0.516088, acc.: 73.44%] [G loss: 0.542313]\n",
      "epoch:15 step:14701 [D loss: 0.552105, acc.: 69.53%] [G loss: 0.616421]\n",
      "epoch:15 step:14702 [D loss: 0.427992, acc.: 82.81%] [G loss: 0.793396]\n",
      "epoch:15 step:14703 [D loss: 0.471622, acc.: 80.47%] [G loss: 0.913582]\n",
      "epoch:15 step:14704 [D loss: 0.487848, acc.: 76.56%] [G loss: 0.840813]\n",
      "epoch:15 step:14705 [D loss: 0.502165, acc.: 74.22%] [G loss: 0.948573]\n",
      "epoch:15 step:14706 [D loss: 0.510046, acc.: 72.66%] [G loss: 0.855377]\n",
      "epoch:15 step:14707 [D loss: 0.618201, acc.: 67.19%] [G loss: 0.643949]\n",
      "epoch:15 step:14708 [D loss: 0.569247, acc.: 66.41%] [G loss: 0.647712]\n",
      "epoch:15 step:14709 [D loss: 0.475321, acc.: 77.34%] [G loss: 0.684386]\n",
      "epoch:15 step:14710 [D loss: 0.555872, acc.: 72.66%] [G loss: 0.709884]\n",
      "epoch:15 step:14711 [D loss: 0.528599, acc.: 75.00%] [G loss: 0.595905]\n",
      "epoch:15 step:14712 [D loss: 0.553308, acc.: 68.75%] [G loss: 0.515598]\n",
      "epoch:15 step:14713 [D loss: 0.593526, acc.: 65.62%] [G loss: 0.594984]\n",
      "epoch:15 step:14714 [D loss: 0.512165, acc.: 70.31%] [G loss: 0.697026]\n",
      "epoch:15 step:14715 [D loss: 0.517328, acc.: 75.78%] [G loss: 0.568744]\n",
      "epoch:15 step:14716 [D loss: 0.526232, acc.: 75.00%] [G loss: 0.672153]\n",
      "epoch:15 step:14717 [D loss: 0.538525, acc.: 72.66%] [G loss: 0.700641]\n",
      "epoch:15 step:14718 [D loss: 0.608782, acc.: 65.62%] [G loss: 0.559849]\n",
      "epoch:15 step:14719 [D loss: 0.531842, acc.: 69.53%] [G loss: 0.397100]\n",
      "epoch:15 step:14720 [D loss: 0.614416, acc.: 66.41%] [G loss: 0.531769]\n",
      "epoch:15 step:14721 [D loss: 0.542361, acc.: 71.09%] [G loss: 0.474269]\n",
      "epoch:15 step:14722 [D loss: 0.556605, acc.: 71.09%] [G loss: 0.595971]\n",
      "epoch:15 step:14723 [D loss: 0.583153, acc.: 67.97%] [G loss: 0.611414]\n",
      "epoch:15 step:14724 [D loss: 0.537150, acc.: 73.44%] [G loss: 0.560326]\n",
      "epoch:15 step:14725 [D loss: 0.540421, acc.: 72.66%] [G loss: 0.584877]\n",
      "epoch:15 step:14726 [D loss: 0.570206, acc.: 68.75%] [G loss: 0.507416]\n",
      "epoch:15 step:14727 [D loss: 0.534920, acc.: 72.66%] [G loss: 0.525401]\n",
      "epoch:15 step:14728 [D loss: 0.581883, acc.: 64.06%] [G loss: 0.560432]\n",
      "epoch:15 step:14729 [D loss: 0.537308, acc.: 69.53%] [G loss: 0.576803]\n",
      "epoch:15 step:14730 [D loss: 0.659776, acc.: 61.72%] [G loss: 0.476177]\n",
      "epoch:15 step:14731 [D loss: 0.511952, acc.: 71.88%] [G loss: 0.544778]\n",
      "epoch:15 step:14732 [D loss: 0.464891, acc.: 78.12%] [G loss: 0.747169]\n",
      "epoch:15 step:14733 [D loss: 0.584353, acc.: 67.97%] [G loss: 0.523941]\n",
      "epoch:15 step:14734 [D loss: 0.522121, acc.: 71.88%] [G loss: 0.692268]\n",
      "epoch:15 step:14735 [D loss: 0.539928, acc.: 77.34%] [G loss: 0.620111]\n",
      "epoch:15 step:14736 [D loss: 0.523822, acc.: 71.09%] [G loss: 0.663589]\n",
      "epoch:15 step:14737 [D loss: 0.491598, acc.: 75.78%] [G loss: 0.497475]\n",
      "epoch:15 step:14738 [D loss: 0.582425, acc.: 68.75%] [G loss: 0.524824]\n",
      "epoch:15 step:14739 [D loss: 0.590474, acc.: 64.84%] [G loss: 0.414919]\n",
      "epoch:15 step:14740 [D loss: 0.565631, acc.: 67.19%] [G loss: 0.633602]\n",
      "epoch:15 step:14741 [D loss: 0.571300, acc.: 64.06%] [G loss: 0.574081]\n",
      "epoch:15 step:14742 [D loss: 0.551319, acc.: 64.06%] [G loss: 0.549095]\n",
      "epoch:15 step:14743 [D loss: 0.515325, acc.: 77.34%] [G loss: 0.640073]\n",
      "epoch:15 step:14744 [D loss: 0.597552, acc.: 61.72%] [G loss: 0.554255]\n",
      "epoch:15 step:14745 [D loss: 0.572429, acc.: 70.31%] [G loss: 0.702363]\n",
      "epoch:15 step:14746 [D loss: 0.523761, acc.: 73.44%] [G loss: 0.635152]\n",
      "epoch:15 step:14747 [D loss: 0.522052, acc.: 73.44%] [G loss: 0.628542]\n",
      "epoch:15 step:14748 [D loss: 0.468657, acc.: 77.34%] [G loss: 0.639612]\n",
      "epoch:15 step:14749 [D loss: 0.507351, acc.: 70.31%] [G loss: 0.598550]\n",
      "epoch:15 step:14750 [D loss: 0.535295, acc.: 72.66%] [G loss: 0.638251]\n",
      "epoch:15 step:14751 [D loss: 0.640621, acc.: 60.16%] [G loss: 0.574498]\n",
      "epoch:15 step:14752 [D loss: 0.567034, acc.: 63.28%] [G loss: 0.530905]\n",
      "epoch:15 step:14753 [D loss: 0.522638, acc.: 81.25%] [G loss: 0.576890]\n",
      "epoch:15 step:14754 [D loss: 0.525617, acc.: 71.09%] [G loss: 0.624818]\n",
      "epoch:15 step:14755 [D loss: 0.544905, acc.: 71.09%] [G loss: 0.751775]\n",
      "epoch:15 step:14756 [D loss: 0.581118, acc.: 67.97%] [G loss: 0.595300]\n",
      "epoch:15 step:14757 [D loss: 0.601857, acc.: 67.19%] [G loss: 0.582072]\n",
      "epoch:15 step:14758 [D loss: 0.623254, acc.: 63.28%] [G loss: 0.485375]\n",
      "epoch:15 step:14759 [D loss: 0.590259, acc.: 67.19%] [G loss: 0.564013]\n",
      "epoch:15 step:14760 [D loss: 0.532579, acc.: 72.66%] [G loss: 0.514674]\n",
      "epoch:15 step:14761 [D loss: 0.556364, acc.: 69.53%] [G loss: 0.603636]\n",
      "epoch:15 step:14762 [D loss: 0.530893, acc.: 71.88%] [G loss: 0.532217]\n",
      "epoch:15 step:14763 [D loss: 0.469718, acc.: 78.91%] [G loss: 0.728856]\n",
      "epoch:15 step:14764 [D loss: 0.548222, acc.: 68.75%] [G loss: 0.561727]\n",
      "epoch:15 step:14765 [D loss: 0.590965, acc.: 63.28%] [G loss: 0.478828]\n",
      "epoch:15 step:14766 [D loss: 0.562182, acc.: 68.75%] [G loss: 0.634421]\n",
      "epoch:15 step:14767 [D loss: 0.544430, acc.: 67.97%] [G loss: 0.551720]\n",
      "epoch:15 step:14768 [D loss: 0.639242, acc.: 57.81%] [G loss: 0.525618]\n",
      "epoch:15 step:14769 [D loss: 0.535030, acc.: 70.31%] [G loss: 0.620050]\n",
      "epoch:15 step:14770 [D loss: 0.513243, acc.: 74.22%] [G loss: 0.616746]\n",
      "epoch:15 step:14771 [D loss: 0.689260, acc.: 62.50%] [G loss: 0.599418]\n",
      "epoch:15 step:14772 [D loss: 0.575578, acc.: 69.53%] [G loss: 0.520308]\n",
      "epoch:15 step:14773 [D loss: 0.563622, acc.: 71.88%] [G loss: 0.554321]\n",
      "epoch:15 step:14774 [D loss: 0.574076, acc.: 65.62%] [G loss: 0.584328]\n",
      "epoch:15 step:14775 [D loss: 0.582550, acc.: 72.66%] [G loss: 0.556228]\n",
      "epoch:15 step:14776 [D loss: 0.643790, acc.: 62.50%] [G loss: 0.536978]\n",
      "epoch:15 step:14777 [D loss: 0.561821, acc.: 70.31%] [G loss: 0.625754]\n",
      "epoch:15 step:14778 [D loss: 0.578404, acc.: 68.75%] [G loss: 0.549663]\n",
      "epoch:15 step:14779 [D loss: 0.521826, acc.: 73.44%] [G loss: 0.463812]\n",
      "epoch:15 step:14780 [D loss: 0.515762, acc.: 77.34%] [G loss: 0.654265]\n",
      "epoch:15 step:14781 [D loss: 0.487137, acc.: 73.44%] [G loss: 0.656281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14782 [D loss: 0.560190, acc.: 70.31%] [G loss: 0.626362]\n",
      "epoch:15 step:14783 [D loss: 0.544366, acc.: 74.22%] [G loss: 0.473660]\n",
      "epoch:15 step:14784 [D loss: 0.554906, acc.: 67.19%] [G loss: 0.485318]\n",
      "epoch:15 step:14785 [D loss: 0.495549, acc.: 77.34%] [G loss: 0.508823]\n",
      "epoch:15 step:14786 [D loss: 0.555142, acc.: 70.31%] [G loss: 0.515794]\n",
      "epoch:15 step:14787 [D loss: 0.555845, acc.: 69.53%] [G loss: 0.500827]\n",
      "epoch:15 step:14788 [D loss: 0.572649, acc.: 68.75%] [G loss: 0.415257]\n",
      "epoch:15 step:14789 [D loss: 0.524260, acc.: 72.66%] [G loss: 0.583447]\n",
      "epoch:15 step:14790 [D loss: 0.605492, acc.: 63.28%] [G loss: 0.599050]\n",
      "epoch:15 step:14791 [D loss: 0.481451, acc.: 74.22%] [G loss: 0.559370]\n",
      "epoch:15 step:14792 [D loss: 0.507017, acc.: 74.22%] [G loss: 0.597111]\n",
      "epoch:15 step:14793 [D loss: 0.565540, acc.: 68.75%] [G loss: 0.524446]\n",
      "epoch:15 step:14794 [D loss: 0.575258, acc.: 70.31%] [G loss: 0.506784]\n",
      "epoch:15 step:14795 [D loss: 0.636201, acc.: 62.50%] [G loss: 0.440153]\n",
      "epoch:15 step:14796 [D loss: 0.535543, acc.: 70.31%] [G loss: 0.449323]\n",
      "epoch:15 step:14797 [D loss: 0.535140, acc.: 73.44%] [G loss: 0.793363]\n",
      "epoch:15 step:14798 [D loss: 0.557020, acc.: 66.41%] [G loss: 0.685160]\n",
      "epoch:15 step:14799 [D loss: 0.552805, acc.: 72.66%] [G loss: 0.666616]\n",
      "epoch:15 step:14800 [D loss: 0.594704, acc.: 63.28%] [G loss: 0.585880]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.632643\n",
      "FID: 48.677345\n",
      "0 = 12.752632766151438\n",
      "1 = 0.08337965313739325\n",
      "2 = 0.870199978351593\n",
      "3 = 0.8108000159263611\n",
      "4 = 0.9296000003814697\n",
      "5 = 0.9201089143753052\n",
      "6 = 0.8108000159263611\n",
      "7 = 8.297006188774118\n",
      "8 = 0.14877762225479027\n",
      "9 = 0.707099974155426\n",
      "10 = 0.703000009059906\n",
      "11 = 0.7111999988555908\n",
      "12 = 0.7088122367858887\n",
      "13 = 0.703000009059906\n",
      "14 = 6.632668495178223\n",
      "15 = 7.235188007354736\n",
      "16 = 0.3737318217754364\n",
      "17 = 6.632643222808838\n",
      "18 = 48.677345275878906\n",
      "epoch:15 step:14801 [D loss: 0.496271, acc.: 71.88%] [G loss: 0.681668]\n",
      "epoch:15 step:14802 [D loss: 0.451789, acc.: 76.56%] [G loss: 0.723982]\n",
      "epoch:15 step:14803 [D loss: 0.579997, acc.: 67.19%] [G loss: 0.627799]\n",
      "epoch:15 step:14804 [D loss: 0.530361, acc.: 75.00%] [G loss: 0.628500]\n",
      "epoch:15 step:14805 [D loss: 0.508907, acc.: 77.34%] [G loss: 0.602506]\n",
      "epoch:15 step:14806 [D loss: 0.510810, acc.: 73.44%] [G loss: 0.712027]\n",
      "epoch:15 step:14807 [D loss: 0.594104, acc.: 70.31%] [G loss: 0.702655]\n",
      "epoch:15 step:14808 [D loss: 0.553441, acc.: 70.31%] [G loss: 0.482962]\n",
      "epoch:15 step:14809 [D loss: 0.508325, acc.: 74.22%] [G loss: 0.636162]\n",
      "epoch:15 step:14810 [D loss: 0.533657, acc.: 70.31%] [G loss: 0.736074]\n",
      "epoch:15 step:14811 [D loss: 0.558174, acc.: 68.75%] [G loss: 0.607097]\n",
      "epoch:15 step:14812 [D loss: 0.512126, acc.: 70.31%] [G loss: 0.618595]\n",
      "epoch:15 step:14813 [D loss: 0.547796, acc.: 71.09%] [G loss: 0.622811]\n",
      "epoch:15 step:14814 [D loss: 0.562306, acc.: 64.06%] [G loss: 0.511347]\n",
      "epoch:15 step:14815 [D loss: 0.521029, acc.: 71.09%] [G loss: 0.624826]\n",
      "epoch:15 step:14816 [D loss: 0.581918, acc.: 63.28%] [G loss: 0.584146]\n",
      "epoch:15 step:14817 [D loss: 0.615673, acc.: 65.62%] [G loss: 0.546491]\n",
      "epoch:15 step:14818 [D loss: 0.567533, acc.: 65.62%] [G loss: 0.723197]\n",
      "epoch:15 step:14819 [D loss: 0.550295, acc.: 67.19%] [G loss: 0.653090]\n",
      "epoch:15 step:14820 [D loss: 0.643798, acc.: 64.06%] [G loss: 0.480974]\n",
      "epoch:15 step:14821 [D loss: 0.654661, acc.: 60.16%] [G loss: 0.441281]\n",
      "epoch:15 step:14822 [D loss: 0.484564, acc.: 77.34%] [G loss: 0.582826]\n",
      "epoch:15 step:14823 [D loss: 0.529563, acc.: 68.75%] [G loss: 0.779362]\n",
      "epoch:15 step:14824 [D loss: 0.536263, acc.: 74.22%] [G loss: 0.820264]\n",
      "epoch:15 step:14825 [D loss: 0.565575, acc.: 67.19%] [G loss: 0.733198]\n",
      "epoch:15 step:14826 [D loss: 0.562264, acc.: 70.31%] [G loss: 0.709291]\n",
      "epoch:15 step:14827 [D loss: 0.525300, acc.: 73.44%] [G loss: 0.615975]\n",
      "epoch:15 step:14828 [D loss: 0.533247, acc.: 73.44%] [G loss: 0.450224]\n",
      "epoch:15 step:14829 [D loss: 0.533517, acc.: 73.44%] [G loss: 0.701352]\n",
      "epoch:15 step:14830 [D loss: 0.518285, acc.: 72.66%] [G loss: 0.717247]\n",
      "epoch:15 step:14831 [D loss: 0.551198, acc.: 68.75%] [G loss: 0.716795]\n",
      "epoch:15 step:14832 [D loss: 0.600714, acc.: 67.97%] [G loss: 0.673998]\n",
      "epoch:15 step:14833 [D loss: 0.560611, acc.: 68.75%] [G loss: 0.523370]\n",
      "epoch:15 step:14834 [D loss: 0.574900, acc.: 66.41%] [G loss: 0.514402]\n",
      "epoch:15 step:14835 [D loss: 0.561517, acc.: 67.19%] [G loss: 0.580963]\n",
      "epoch:15 step:14836 [D loss: 0.490173, acc.: 73.44%] [G loss: 0.670380]\n",
      "epoch:15 step:14837 [D loss: 0.542379, acc.: 71.09%] [G loss: 0.728347]\n",
      "epoch:15 step:14838 [D loss: 0.578258, acc.: 67.19%] [G loss: 0.772079]\n",
      "epoch:15 step:14839 [D loss: 0.604579, acc.: 66.41%] [G loss: 0.420318]\n",
      "epoch:15 step:14840 [D loss: 0.529706, acc.: 71.88%] [G loss: 0.585157]\n",
      "epoch:15 step:14841 [D loss: 0.519912, acc.: 72.66%] [G loss: 0.554839]\n",
      "epoch:15 step:14842 [D loss: 0.595978, acc.: 66.41%] [G loss: 0.573533]\n",
      "epoch:15 step:14843 [D loss: 0.603128, acc.: 67.97%] [G loss: 0.469708]\n",
      "epoch:15 step:14844 [D loss: 0.569764, acc.: 70.31%] [G loss: 0.535915]\n",
      "epoch:15 step:14845 [D loss: 0.556604, acc.: 67.19%] [G loss: 0.567089]\n",
      "epoch:15 step:14846 [D loss: 0.564347, acc.: 68.75%] [G loss: 0.695963]\n",
      "epoch:15 step:14847 [D loss: 0.467423, acc.: 74.22%] [G loss: 0.780637]\n",
      "epoch:15 step:14848 [D loss: 0.607582, acc.: 64.06%] [G loss: 0.575758]\n",
      "epoch:15 step:14849 [D loss: 0.647330, acc.: 57.81%] [G loss: 0.556402]\n",
      "epoch:15 step:14850 [D loss: 0.559609, acc.: 64.84%] [G loss: 0.539253]\n",
      "epoch:15 step:14851 [D loss: 0.468077, acc.: 75.78%] [G loss: 0.699712]\n",
      "epoch:15 step:14852 [D loss: 0.551129, acc.: 67.19%] [G loss: 0.618704]\n",
      "epoch:15 step:14853 [D loss: 0.554767, acc.: 67.19%] [G loss: 0.663694]\n",
      "epoch:15 step:14854 [D loss: 0.604343, acc.: 67.97%] [G loss: 0.526284]\n",
      "epoch:15 step:14855 [D loss: 0.553914, acc.: 66.41%] [G loss: 0.620098]\n",
      "epoch:15 step:14856 [D loss: 0.536627, acc.: 73.44%] [G loss: 0.651294]\n",
      "epoch:15 step:14857 [D loss: 0.512971, acc.: 75.78%] [G loss: 0.682033]\n",
      "epoch:15 step:14858 [D loss: 0.585217, acc.: 70.31%] [G loss: 0.559983]\n",
      "epoch:15 step:14859 [D loss: 0.587407, acc.: 65.62%] [G loss: 0.505080]\n",
      "epoch:15 step:14860 [D loss: 0.577047, acc.: 69.53%] [G loss: 0.536243]\n",
      "epoch:15 step:14861 [D loss: 0.577038, acc.: 60.16%] [G loss: 0.530916]\n",
      "epoch:15 step:14862 [D loss: 0.556989, acc.: 65.62%] [G loss: 0.572836]\n",
      "epoch:15 step:14863 [D loss: 0.572010, acc.: 62.50%] [G loss: 0.643767]\n",
      "epoch:15 step:14864 [D loss: 0.596835, acc.: 68.75%] [G loss: 0.532241]\n",
      "epoch:15 step:14865 [D loss: 0.544441, acc.: 73.44%] [G loss: 0.499016]\n",
      "epoch:15 step:14866 [D loss: 0.617009, acc.: 63.28%] [G loss: 0.563587]\n",
      "epoch:15 step:14867 [D loss: 0.612732, acc.: 60.94%] [G loss: 0.606830]\n",
      "epoch:15 step:14868 [D loss: 0.608381, acc.: 64.84%] [G loss: 0.438100]\n",
      "epoch:15 step:14869 [D loss: 0.487140, acc.: 78.12%] [G loss: 0.578129]\n",
      "epoch:15 step:14870 [D loss: 0.522229, acc.: 73.44%] [G loss: 0.669673]\n",
      "epoch:15 step:14871 [D loss: 0.519527, acc.: 75.00%] [G loss: 0.757344]\n",
      "epoch:15 step:14872 [D loss: 0.595281, acc.: 67.19%] [G loss: 0.626485]\n",
      "epoch:15 step:14873 [D loss: 0.626296, acc.: 62.50%] [G loss: 0.626434]\n",
      "epoch:15 step:14874 [D loss: 0.553670, acc.: 68.75%] [G loss: 0.551560]\n",
      "epoch:15 step:14875 [D loss: 0.662410, acc.: 71.88%] [G loss: 0.595447]\n",
      "epoch:15 step:14876 [D loss: 0.553083, acc.: 69.53%] [G loss: 0.463490]\n",
      "epoch:15 step:14877 [D loss: 0.569248, acc.: 67.19%] [G loss: 0.555159]\n",
      "epoch:15 step:14878 [D loss: 0.463863, acc.: 75.78%] [G loss: 0.598380]\n",
      "epoch:15 step:14879 [D loss: 0.612032, acc.: 67.97%] [G loss: 0.543597]\n",
      "epoch:15 step:14880 [D loss: 0.569376, acc.: 68.75%] [G loss: 0.514104]\n",
      "epoch:15 step:14881 [D loss: 0.561327, acc.: 71.09%] [G loss: 0.582648]\n",
      "epoch:15 step:14882 [D loss: 0.631940, acc.: 62.50%] [G loss: 0.477405]\n",
      "epoch:15 step:14883 [D loss: 0.633587, acc.: 61.72%] [G loss: 0.470524]\n",
      "epoch:15 step:14884 [D loss: 0.590808, acc.: 67.97%] [G loss: 0.543475]\n",
      "epoch:15 step:14885 [D loss: 0.555484, acc.: 70.31%] [G loss: 0.518402]\n",
      "epoch:15 step:14886 [D loss: 0.575129, acc.: 67.97%] [G loss: 0.548084]\n",
      "epoch:15 step:14887 [D loss: 0.546718, acc.: 69.53%] [G loss: 0.557732]\n",
      "epoch:15 step:14888 [D loss: 0.542401, acc.: 72.66%] [G loss: 0.489652]\n",
      "epoch:15 step:14889 [D loss: 0.538791, acc.: 69.53%] [G loss: 0.531835]\n",
      "epoch:15 step:14890 [D loss: 0.513608, acc.: 74.22%] [G loss: 0.526852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14891 [D loss: 0.553004, acc.: 72.66%] [G loss: 0.420247]\n",
      "epoch:15 step:14892 [D loss: 0.522718, acc.: 73.44%] [G loss: 0.468995]\n",
      "epoch:15 step:14893 [D loss: 0.550944, acc.: 70.31%] [G loss: 0.554798]\n",
      "epoch:15 step:14894 [D loss: 0.618794, acc.: 58.59%] [G loss: 0.458577]\n",
      "epoch:15 step:14895 [D loss: 0.609057, acc.: 62.50%] [G loss: 0.446461]\n",
      "epoch:15 step:14896 [D loss: 0.529568, acc.: 71.88%] [G loss: 0.501583]\n",
      "epoch:15 step:14897 [D loss: 0.518512, acc.: 67.19%] [G loss: 0.480981]\n",
      "epoch:15 step:14898 [D loss: 0.521057, acc.: 75.00%] [G loss: 0.639530]\n",
      "epoch:15 step:14899 [D loss: 0.573855, acc.: 68.75%] [G loss: 0.606792]\n",
      "epoch:15 step:14900 [D loss: 0.571327, acc.: 65.62%] [G loss: 0.491950]\n",
      "epoch:15 step:14901 [D loss: 0.596766, acc.: 66.41%] [G loss: 0.567808]\n",
      "epoch:15 step:14902 [D loss: 0.618312, acc.: 61.72%] [G loss: 0.346830]\n",
      "epoch:15 step:14903 [D loss: 0.541895, acc.: 68.75%] [G loss: 0.426740]\n",
      "epoch:15 step:14904 [D loss: 0.569205, acc.: 65.62%] [G loss: 0.506928]\n",
      "epoch:15 step:14905 [D loss: 0.563955, acc.: 67.97%] [G loss: 0.439918]\n",
      "epoch:15 step:14906 [D loss: 0.618589, acc.: 63.28%] [G loss: 0.666329]\n",
      "epoch:15 step:14907 [D loss: 0.525589, acc.: 74.22%] [G loss: 0.543515]\n",
      "epoch:15 step:14908 [D loss: 0.517029, acc.: 71.09%] [G loss: 0.755196]\n",
      "epoch:15 step:14909 [D loss: 0.563579, acc.: 67.97%] [G loss: 0.623267]\n",
      "epoch:15 step:14910 [D loss: 0.527930, acc.: 73.44%] [G loss: 0.524202]\n",
      "epoch:15 step:14911 [D loss: 0.589929, acc.: 68.75%] [G loss: 0.552183]\n",
      "epoch:15 step:14912 [D loss: 0.537325, acc.: 71.88%] [G loss: 0.558846]\n",
      "epoch:15 step:14913 [D loss: 0.629977, acc.: 64.84%] [G loss: 0.501050]\n",
      "epoch:15 step:14914 [D loss: 0.584840, acc.: 68.75%] [G loss: 0.663763]\n",
      "epoch:15 step:14915 [D loss: 0.452781, acc.: 77.34%] [G loss: 0.790701]\n",
      "epoch:15 step:14916 [D loss: 0.635765, acc.: 60.16%] [G loss: 0.619535]\n",
      "epoch:15 step:14917 [D loss: 0.557236, acc.: 64.84%] [G loss: 0.520858]\n",
      "epoch:15 step:14918 [D loss: 0.563798, acc.: 67.97%] [G loss: 0.577498]\n",
      "epoch:15 step:14919 [D loss: 0.578389, acc.: 62.50%] [G loss: 0.517373]\n",
      "epoch:15 step:14920 [D loss: 0.580189, acc.: 66.41%] [G loss: 0.511276]\n",
      "epoch:15 step:14921 [D loss: 0.594153, acc.: 65.62%] [G loss: 0.457214]\n",
      "epoch:15 step:14922 [D loss: 0.656721, acc.: 60.94%] [G loss: 0.447131]\n",
      "epoch:15 step:14923 [D loss: 0.566401, acc.: 70.31%] [G loss: 0.439710]\n",
      "epoch:15 step:14924 [D loss: 0.572076, acc.: 65.62%] [G loss: 0.494787]\n",
      "epoch:15 step:14925 [D loss: 0.429081, acc.: 82.81%] [G loss: 0.614833]\n",
      "epoch:15 step:14926 [D loss: 0.529954, acc.: 74.22%] [G loss: 0.715196]\n",
      "epoch:15 step:14927 [D loss: 0.524292, acc.: 71.09%] [G loss: 0.766580]\n",
      "epoch:15 step:14928 [D loss: 0.614650, acc.: 60.94%] [G loss: 0.616460]\n",
      "epoch:15 step:14929 [D loss: 0.548941, acc.: 71.09%] [G loss: 0.503150]\n",
      "epoch:15 step:14930 [D loss: 0.517461, acc.: 74.22%] [G loss: 0.619721]\n",
      "epoch:15 step:14931 [D loss: 0.573889, acc.: 70.31%] [G loss: 0.492894]\n",
      "epoch:15 step:14932 [D loss: 0.637319, acc.: 63.28%] [G loss: 0.386265]\n",
      "epoch:15 step:14933 [D loss: 0.589385, acc.: 63.28%] [G loss: 0.483111]\n",
      "epoch:15 step:14934 [D loss: 0.538326, acc.: 69.53%] [G loss: 0.673124]\n",
      "epoch:15 step:14935 [D loss: 0.662943, acc.: 57.81%] [G loss: 0.449208]\n",
      "epoch:15 step:14936 [D loss: 0.592359, acc.: 69.53%] [G loss: 0.466267]\n",
      "epoch:15 step:14937 [D loss: 0.570825, acc.: 67.19%] [G loss: 0.506635]\n",
      "epoch:15 step:14938 [D loss: 0.552536, acc.: 68.75%] [G loss: 0.501617]\n",
      "epoch:15 step:14939 [D loss: 0.530826, acc.: 71.09%] [G loss: 0.544911]\n",
      "epoch:15 step:14940 [D loss: 0.549122, acc.: 71.09%] [G loss: 0.571630]\n",
      "epoch:15 step:14941 [D loss: 0.567092, acc.: 73.44%] [G loss: 0.714042]\n",
      "epoch:15 step:14942 [D loss: 0.530408, acc.: 69.53%] [G loss: 0.747177]\n",
      "epoch:15 step:14943 [D loss: 0.557354, acc.: 67.19%] [G loss: 0.623903]\n",
      "epoch:15 step:14944 [D loss: 0.607255, acc.: 66.41%] [G loss: 0.621675]\n",
      "epoch:15 step:14945 [D loss: 0.507730, acc.: 73.44%] [G loss: 0.452632]\n",
      "epoch:15 step:14946 [D loss: 0.591496, acc.: 65.62%] [G loss: 0.547484]\n",
      "epoch:15 step:14947 [D loss: 0.571323, acc.: 64.06%] [G loss: 0.501923]\n",
      "epoch:15 step:14948 [D loss: 0.593679, acc.: 65.62%] [G loss: 0.542591]\n",
      "epoch:15 step:14949 [D loss: 0.546114, acc.: 69.53%] [G loss: 0.563413]\n",
      "epoch:15 step:14950 [D loss: 0.520479, acc.: 71.09%] [G loss: 0.684668]\n",
      "epoch:15 step:14951 [D loss: 0.465731, acc.: 78.12%] [G loss: 0.917230]\n",
      "epoch:15 step:14952 [D loss: 0.489246, acc.: 76.56%] [G loss: 0.795462]\n",
      "epoch:15 step:14953 [D loss: 0.507173, acc.: 77.34%] [G loss: 0.710271]\n",
      "epoch:15 step:14954 [D loss: 0.484546, acc.: 74.22%] [G loss: 0.872842]\n",
      "epoch:15 step:14955 [D loss: 0.533944, acc.: 71.88%] [G loss: 0.715155]\n",
      "epoch:15 step:14956 [D loss: 0.518166, acc.: 71.88%] [G loss: 0.765312]\n",
      "epoch:15 step:14957 [D loss: 0.590363, acc.: 63.28%] [G loss: 0.652975]\n",
      "epoch:15 step:14958 [D loss: 0.527186, acc.: 73.44%] [G loss: 0.499277]\n",
      "epoch:15 step:14959 [D loss: 0.583373, acc.: 70.31%] [G loss: 0.655145]\n",
      "epoch:15 step:14960 [D loss: 0.576062, acc.: 69.53%] [G loss: 0.618128]\n",
      "epoch:15 step:14961 [D loss: 0.532793, acc.: 71.88%] [G loss: 0.635968]\n",
      "epoch:15 step:14962 [D loss: 0.542890, acc.: 71.09%] [G loss: 0.714480]\n",
      "epoch:15 step:14963 [D loss: 0.570295, acc.: 67.19%] [G loss: 0.527408]\n",
      "epoch:15 step:14964 [D loss: 0.543289, acc.: 64.84%] [G loss: 0.654351]\n",
      "epoch:15 step:14965 [D loss: 0.538478, acc.: 74.22%] [G loss: 0.675079]\n",
      "epoch:15 step:14966 [D loss: 0.528247, acc.: 72.66%] [G loss: 0.607045]\n",
      "epoch:15 step:14967 [D loss: 0.483279, acc.: 81.25%] [G loss: 0.839206]\n",
      "epoch:15 step:14968 [D loss: 0.603371, acc.: 70.31%] [G loss: 0.629884]\n",
      "epoch:15 step:14969 [D loss: 0.505166, acc.: 77.34%] [G loss: 0.909648]\n",
      "epoch:15 step:14970 [D loss: 0.626716, acc.: 67.19%] [G loss: 0.766138]\n",
      "epoch:15 step:14971 [D loss: 0.525555, acc.: 73.44%] [G loss: 0.662062]\n",
      "epoch:15 step:14972 [D loss: 0.654954, acc.: 60.16%] [G loss: 0.525806]\n",
      "epoch:15 step:14973 [D loss: 0.463481, acc.: 79.69%] [G loss: 0.788502]\n",
      "epoch:15 step:14974 [D loss: 0.453922, acc.: 80.47%] [G loss: 0.836302]\n",
      "epoch:15 step:14975 [D loss: 0.616155, acc.: 64.06%] [G loss: 0.771646]\n",
      "epoch:15 step:14976 [D loss: 0.516389, acc.: 70.31%] [G loss: 0.799566]\n",
      "epoch:15 step:14977 [D loss: 0.554887, acc.: 66.41%] [G loss: 0.577183]\n",
      "epoch:15 step:14978 [D loss: 0.521534, acc.: 67.19%] [G loss: 0.602450]\n",
      "epoch:15 step:14979 [D loss: 0.458503, acc.: 76.56%] [G loss: 0.812250]\n",
      "epoch:15 step:14980 [D loss: 0.415251, acc.: 80.47%] [G loss: 0.865260]\n",
      "epoch:15 step:14981 [D loss: 0.450460, acc.: 78.91%] [G loss: 0.877234]\n",
      "epoch:15 step:14982 [D loss: 0.488281, acc.: 77.34%] [G loss: 0.976261]\n",
      "epoch:15 step:14983 [D loss: 0.641788, acc.: 64.84%] [G loss: 0.979685]\n",
      "epoch:15 step:14984 [D loss: 0.474632, acc.: 75.00%] [G loss: 1.237740]\n",
      "epoch:15 step:14985 [D loss: 0.508644, acc.: 71.88%] [G loss: 1.181154]\n",
      "epoch:15 step:14986 [D loss: 0.539435, acc.: 67.19%] [G loss: 0.839973]\n",
      "epoch:15 step:14987 [D loss: 0.625466, acc.: 64.84%] [G loss: 0.583707]\n",
      "epoch:15 step:14988 [D loss: 0.539689, acc.: 70.31%] [G loss: 0.841380]\n",
      "epoch:15 step:14989 [D loss: 0.527825, acc.: 75.78%] [G loss: 0.992232]\n",
      "epoch:15 step:14990 [D loss: 0.532949, acc.: 66.41%] [G loss: 0.847875]\n",
      "epoch:15 step:14991 [D loss: 0.426326, acc.: 82.03%] [G loss: 1.180680]\n",
      "epoch:15 step:14992 [D loss: 0.424686, acc.: 82.81%] [G loss: 1.456327]\n",
      "epoch:16 step:14993 [D loss: 0.568223, acc.: 67.97%] [G loss: 1.114277]\n",
      "epoch:16 step:14994 [D loss: 0.573446, acc.: 68.75%] [G loss: 1.040980]\n",
      "epoch:16 step:14995 [D loss: 0.539145, acc.: 72.66%] [G loss: 0.968921]\n",
      "epoch:16 step:14996 [D loss: 0.519828, acc.: 76.56%] [G loss: 0.614200]\n",
      "epoch:16 step:14997 [D loss: 0.629361, acc.: 60.94%] [G loss: 0.863609]\n",
      "epoch:16 step:14998 [D loss: 0.562073, acc.: 70.31%] [G loss: 0.670913]\n",
      "epoch:16 step:14999 [D loss: 0.520086, acc.: 76.56%] [G loss: 0.671782]\n",
      "epoch:16 step:15000 [D loss: 0.434819, acc.: 78.91%] [G loss: 0.802899]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.583112\n",
      "FID: 51.127663\n",
      "0 = 13.215632014369929\n",
      "1 = 0.10544557670045206\n",
      "2 = 0.9187999963760376\n",
      "3 = 0.8831999897956848\n",
      "4 = 0.9544000029563904\n",
      "5 = 0.950904369354248\n",
      "6 = 0.8831999897956848\n",
      "7 = 8.498842910039434\n",
      "8 = 0.1576479365384667\n",
      "9 = 0.7260000109672546\n",
      "10 = 0.7124000191688538\n",
      "11 = 0.7396000027656555\n",
      "12 = 0.7323190569877625\n",
      "13 = 0.7124000191688538\n",
      "14 = 6.583137035369873\n",
      "15 = 7.295544147491455\n",
      "16 = 0.3715217411518097\n",
      "17 = 6.583111763000488\n",
      "18 = 51.127662658691406\n",
      "epoch:16 step:15001 [D loss: 0.512736, acc.: 73.44%] [G loss: 0.850531]\n",
      "epoch:16 step:15002 [D loss: 0.605303, acc.: 71.09%] [G loss: 0.763869]\n",
      "epoch:16 step:15003 [D loss: 0.499217, acc.: 75.78%] [G loss: 0.765357]\n",
      "epoch:16 step:15004 [D loss: 0.585094, acc.: 69.53%] [G loss: 0.670708]\n",
      "epoch:16 step:15005 [D loss: 0.562788, acc.: 65.62%] [G loss: 0.675957]\n",
      "epoch:16 step:15006 [D loss: 0.517563, acc.: 71.88%] [G loss: 0.769226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15007 [D loss: 0.456502, acc.: 75.78%] [G loss: 0.607898]\n",
      "epoch:16 step:15008 [D loss: 0.470548, acc.: 75.78%] [G loss: 0.648688]\n",
      "epoch:16 step:15009 [D loss: 0.608515, acc.: 66.41%] [G loss: 0.624227]\n",
      "epoch:16 step:15010 [D loss: 0.606955, acc.: 68.75%] [G loss: 0.600155]\n",
      "epoch:16 step:15011 [D loss: 0.520588, acc.: 76.56%] [G loss: 0.877614]\n",
      "epoch:16 step:15012 [D loss: 0.641193, acc.: 58.59%] [G loss: 0.888089]\n",
      "epoch:16 step:15013 [D loss: 0.565550, acc.: 66.41%] [G loss: 0.771720]\n",
      "epoch:16 step:15014 [D loss: 0.461168, acc.: 77.34%] [G loss: 0.931477]\n",
      "epoch:16 step:15015 [D loss: 0.534422, acc.: 75.00%] [G loss: 0.685161]\n",
      "epoch:16 step:15016 [D loss: 0.510869, acc.: 71.88%] [G loss: 0.685152]\n",
      "epoch:16 step:15017 [D loss: 0.499912, acc.: 72.66%] [G loss: 0.740815]\n",
      "epoch:16 step:15018 [D loss: 0.591143, acc.: 67.97%] [G loss: 0.568377]\n",
      "epoch:16 step:15019 [D loss: 0.461848, acc.: 77.34%] [G loss: 0.589033]\n",
      "epoch:16 step:15020 [D loss: 0.544846, acc.: 71.09%] [G loss: 0.645009]\n",
      "epoch:16 step:15021 [D loss: 0.516484, acc.: 70.31%] [G loss: 0.529481]\n",
      "epoch:16 step:15022 [D loss: 0.598948, acc.: 63.28%] [G loss: 0.525076]\n",
      "epoch:16 step:15023 [D loss: 0.603900, acc.: 62.50%] [G loss: 0.522418]\n",
      "epoch:16 step:15024 [D loss: 0.545638, acc.: 76.56%] [G loss: 0.564133]\n",
      "epoch:16 step:15025 [D loss: 0.529951, acc.: 70.31%] [G loss: 0.549117]\n",
      "epoch:16 step:15026 [D loss: 0.549337, acc.: 67.97%] [G loss: 0.582096]\n",
      "epoch:16 step:15027 [D loss: 0.542321, acc.: 70.31%] [G loss: 0.646003]\n",
      "epoch:16 step:15028 [D loss: 0.526779, acc.: 69.53%] [G loss: 0.837490]\n",
      "epoch:16 step:15029 [D loss: 0.502713, acc.: 74.22%] [G loss: 0.649621]\n",
      "epoch:16 step:15030 [D loss: 0.591280, acc.: 71.09%] [G loss: 0.558659]\n",
      "epoch:16 step:15031 [D loss: 0.461927, acc.: 78.12%] [G loss: 0.700364]\n",
      "epoch:16 step:15032 [D loss: 0.479238, acc.: 77.34%] [G loss: 0.582622]\n",
      "epoch:16 step:15033 [D loss: 0.557584, acc.: 71.09%] [G loss: 0.659525]\n",
      "epoch:16 step:15034 [D loss: 0.569797, acc.: 63.28%] [G loss: 0.614383]\n",
      "epoch:16 step:15035 [D loss: 0.531200, acc.: 75.00%] [G loss: 0.632264]\n",
      "epoch:16 step:15036 [D loss: 0.599450, acc.: 63.28%] [G loss: 0.512726]\n",
      "epoch:16 step:15037 [D loss: 0.502019, acc.: 74.22%] [G loss: 0.726264]\n",
      "epoch:16 step:15038 [D loss: 0.515503, acc.: 72.66%] [G loss: 0.698111]\n",
      "epoch:16 step:15039 [D loss: 0.534087, acc.: 69.53%] [G loss: 0.696562]\n",
      "epoch:16 step:15040 [D loss: 0.521896, acc.: 71.09%] [G loss: 0.593419]\n",
      "epoch:16 step:15041 [D loss: 0.505127, acc.: 76.56%] [G loss: 0.725901]\n",
      "epoch:16 step:15042 [D loss: 0.556137, acc.: 68.75%] [G loss: 0.643392]\n",
      "epoch:16 step:15043 [D loss: 0.636814, acc.: 62.50%] [G loss: 0.554261]\n",
      "epoch:16 step:15044 [D loss: 0.615095, acc.: 64.84%] [G loss: 0.528364]\n",
      "epoch:16 step:15045 [D loss: 0.548220, acc.: 73.44%] [G loss: 0.604905]\n",
      "epoch:16 step:15046 [D loss: 0.536670, acc.: 71.88%] [G loss: 0.597733]\n",
      "epoch:16 step:15047 [D loss: 0.541686, acc.: 70.31%] [G loss: 0.684321]\n",
      "epoch:16 step:15048 [D loss: 0.553769, acc.: 71.09%] [G loss: 0.637498]\n",
      "epoch:16 step:15049 [D loss: 0.551886, acc.: 69.53%] [G loss: 0.691741]\n",
      "epoch:16 step:15050 [D loss: 0.596812, acc.: 65.62%] [G loss: 0.690102]\n",
      "epoch:16 step:15051 [D loss: 0.537357, acc.: 71.09%] [G loss: 0.747565]\n",
      "epoch:16 step:15052 [D loss: 0.588093, acc.: 64.06%] [G loss: 0.538296]\n",
      "epoch:16 step:15053 [D loss: 0.610529, acc.: 65.62%] [G loss: 0.517102]\n",
      "epoch:16 step:15054 [D loss: 0.574954, acc.: 71.88%] [G loss: 0.429058]\n",
      "epoch:16 step:15055 [D loss: 0.560601, acc.: 66.41%] [G loss: 0.436325]\n",
      "epoch:16 step:15056 [D loss: 0.514334, acc.: 73.44%] [G loss: 0.557983]\n",
      "epoch:16 step:15057 [D loss: 0.549232, acc.: 74.22%] [G loss: 0.597691]\n",
      "epoch:16 step:15058 [D loss: 0.526772, acc.: 71.88%] [G loss: 0.612082]\n",
      "epoch:16 step:15059 [D loss: 0.575146, acc.: 66.41%] [G loss: 0.628169]\n",
      "epoch:16 step:15060 [D loss: 0.567375, acc.: 70.31%] [G loss: 0.733012]\n",
      "epoch:16 step:15061 [D loss: 0.548456, acc.: 72.66%] [G loss: 0.525962]\n",
      "epoch:16 step:15062 [D loss: 0.505672, acc.: 77.34%] [G loss: 0.502630]\n",
      "epoch:16 step:15063 [D loss: 0.550424, acc.: 73.44%] [G loss: 0.587698]\n",
      "epoch:16 step:15064 [D loss: 0.502029, acc.: 75.00%] [G loss: 0.628427]\n",
      "epoch:16 step:15065 [D loss: 0.573820, acc.: 68.75%] [G loss: 0.538539]\n",
      "epoch:16 step:15066 [D loss: 0.547170, acc.: 71.88%] [G loss: 0.582569]\n",
      "epoch:16 step:15067 [D loss: 0.547015, acc.: 69.53%] [G loss: 0.632561]\n",
      "epoch:16 step:15068 [D loss: 0.503654, acc.: 77.34%] [G loss: 0.709073]\n",
      "epoch:16 step:15069 [D loss: 0.490072, acc.: 74.22%] [G loss: 0.657249]\n",
      "epoch:16 step:15070 [D loss: 0.589214, acc.: 67.97%] [G loss: 0.623147]\n",
      "epoch:16 step:15071 [D loss: 0.617170, acc.: 59.38%] [G loss: 0.451608]\n",
      "epoch:16 step:15072 [D loss: 0.527644, acc.: 75.00%] [G loss: 0.657474]\n",
      "epoch:16 step:15073 [D loss: 0.524067, acc.: 71.09%] [G loss: 0.678175]\n",
      "epoch:16 step:15074 [D loss: 0.551226, acc.: 67.19%] [G loss: 0.538982]\n",
      "epoch:16 step:15075 [D loss: 0.541746, acc.: 69.53%] [G loss: 0.702968]\n",
      "epoch:16 step:15076 [D loss: 0.556518, acc.: 67.19%] [G loss: 0.597888]\n",
      "epoch:16 step:15077 [D loss: 0.602042, acc.: 66.41%] [G loss: 0.622291]\n",
      "epoch:16 step:15078 [D loss: 0.599232, acc.: 65.62%] [G loss: 0.517588]\n",
      "epoch:16 step:15079 [D loss: 0.535640, acc.: 71.88%] [G loss: 0.645086]\n",
      "epoch:16 step:15080 [D loss: 0.584664, acc.: 67.19%] [G loss: 0.629676]\n",
      "epoch:16 step:15081 [D loss: 0.596126, acc.: 71.09%] [G loss: 0.620072]\n",
      "epoch:16 step:15082 [D loss: 0.556705, acc.: 65.62%] [G loss: 0.553986]\n",
      "epoch:16 step:15083 [D loss: 0.558137, acc.: 69.53%] [G loss: 0.544809]\n",
      "epoch:16 step:15084 [D loss: 0.458629, acc.: 79.69%] [G loss: 0.714429]\n",
      "epoch:16 step:15085 [D loss: 0.537915, acc.: 71.88%] [G loss: 0.669305]\n",
      "epoch:16 step:15086 [D loss: 0.513125, acc.: 74.22%] [G loss: 0.782550]\n",
      "epoch:16 step:15087 [D loss: 0.532967, acc.: 69.53%] [G loss: 0.785062]\n",
      "epoch:16 step:15088 [D loss: 0.536790, acc.: 70.31%] [G loss: 0.624423]\n",
      "epoch:16 step:15089 [D loss: 0.524520, acc.: 69.53%] [G loss: 0.684970]\n",
      "epoch:16 step:15090 [D loss: 0.620905, acc.: 62.50%] [G loss: 0.566405]\n",
      "epoch:16 step:15091 [D loss: 0.541146, acc.: 74.22%] [G loss: 0.624407]\n",
      "epoch:16 step:15092 [D loss: 0.477276, acc.: 72.66%] [G loss: 0.787974]\n",
      "epoch:16 step:15093 [D loss: 0.520970, acc.: 71.88%] [G loss: 0.654863]\n",
      "epoch:16 step:15094 [D loss: 0.604215, acc.: 63.28%] [G loss: 0.670883]\n",
      "epoch:16 step:15095 [D loss: 0.523239, acc.: 70.31%] [G loss: 0.536308]\n",
      "epoch:16 step:15096 [D loss: 0.517199, acc.: 71.88%] [G loss: 0.590648]\n",
      "epoch:16 step:15097 [D loss: 0.619374, acc.: 61.72%] [G loss: 0.467251]\n",
      "epoch:16 step:15098 [D loss: 0.584086, acc.: 67.97%] [G loss: 0.454123]\n",
      "epoch:16 step:15099 [D loss: 0.623609, acc.: 64.06%] [G loss: 0.692190]\n",
      "epoch:16 step:15100 [D loss: 0.674024, acc.: 58.59%] [G loss: 0.821634]\n",
      "epoch:16 step:15101 [D loss: 0.549819, acc.: 75.00%] [G loss: 0.522439]\n",
      "epoch:16 step:15102 [D loss: 0.586673, acc.: 68.75%] [G loss: 0.647410]\n",
      "epoch:16 step:15103 [D loss: 0.504824, acc.: 72.66%] [G loss: 0.539776]\n",
      "epoch:16 step:15104 [D loss: 0.571728, acc.: 70.31%] [G loss: 0.476405]\n",
      "epoch:16 step:15105 [D loss: 0.511693, acc.: 71.88%] [G loss: 0.618320]\n",
      "epoch:16 step:15106 [D loss: 0.598656, acc.: 66.41%] [G loss: 0.583456]\n",
      "epoch:16 step:15107 [D loss: 0.510504, acc.: 75.00%] [G loss: 0.666296]\n",
      "epoch:16 step:15108 [D loss: 0.534479, acc.: 70.31%] [G loss: 0.643904]\n",
      "epoch:16 step:15109 [D loss: 0.606403, acc.: 64.06%] [G loss: 0.668850]\n",
      "epoch:16 step:15110 [D loss: 0.546576, acc.: 72.66%] [G loss: 0.791734]\n",
      "epoch:16 step:15111 [D loss: 0.504465, acc.: 78.12%] [G loss: 0.911662]\n",
      "epoch:16 step:15112 [D loss: 0.604276, acc.: 67.97%] [G loss: 0.731909]\n",
      "epoch:16 step:15113 [D loss: 0.594336, acc.: 68.75%] [G loss: 0.478204]\n",
      "epoch:16 step:15114 [D loss: 0.529358, acc.: 75.00%] [G loss: 0.943558]\n",
      "epoch:16 step:15115 [D loss: 0.573660, acc.: 67.19%] [G loss: 0.719989]\n",
      "epoch:16 step:15116 [D loss: 0.541612, acc.: 71.09%] [G loss: 0.794630]\n",
      "epoch:16 step:15117 [D loss: 0.568300, acc.: 68.75%] [G loss: 0.588196]\n",
      "epoch:16 step:15118 [D loss: 0.502651, acc.: 75.00%] [G loss: 0.536029]\n",
      "epoch:16 step:15119 [D loss: 0.494574, acc.: 75.78%] [G loss: 0.641218]\n",
      "epoch:16 step:15120 [D loss: 0.548372, acc.: 71.09%] [G loss: 0.524274]\n",
      "epoch:16 step:15121 [D loss: 0.601817, acc.: 66.41%] [G loss: 0.596606]\n",
      "epoch:16 step:15122 [D loss: 0.509277, acc.: 70.31%] [G loss: 0.652391]\n",
      "epoch:16 step:15123 [D loss: 0.492023, acc.: 74.22%] [G loss: 0.657812]\n",
      "epoch:16 step:15124 [D loss: 0.577528, acc.: 71.09%] [G loss: 0.597583]\n",
      "epoch:16 step:15125 [D loss: 0.591232, acc.: 71.09%] [G loss: 0.544069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15126 [D loss: 0.580956, acc.: 67.19%] [G loss: 0.606620]\n",
      "epoch:16 step:15127 [D loss: 0.606629, acc.: 68.75%] [G loss: 0.657916]\n",
      "epoch:16 step:15128 [D loss: 0.537521, acc.: 71.09%] [G loss: 0.640578]\n",
      "epoch:16 step:15129 [D loss: 0.572245, acc.: 69.53%] [G loss: 0.597992]\n",
      "epoch:16 step:15130 [D loss: 0.565125, acc.: 71.09%] [G loss: 0.609313]\n",
      "epoch:16 step:15131 [D loss: 0.575819, acc.: 73.44%] [G loss: 0.629866]\n",
      "epoch:16 step:15132 [D loss: 0.585155, acc.: 64.06%] [G loss: 0.526827]\n",
      "epoch:16 step:15133 [D loss: 0.526691, acc.: 75.00%] [G loss: 0.546130]\n",
      "epoch:16 step:15134 [D loss: 0.593204, acc.: 67.97%] [G loss: 0.497726]\n",
      "epoch:16 step:15135 [D loss: 0.603821, acc.: 61.72%] [G loss: 0.684050]\n",
      "epoch:16 step:15136 [D loss: 0.539362, acc.: 68.75%] [G loss: 0.476600]\n",
      "epoch:16 step:15137 [D loss: 0.576073, acc.: 67.97%] [G loss: 0.610816]\n",
      "epoch:16 step:15138 [D loss: 0.482596, acc.: 78.12%] [G loss: 0.681164]\n",
      "epoch:16 step:15139 [D loss: 0.629869, acc.: 59.38%] [G loss: 0.621152]\n",
      "epoch:16 step:15140 [D loss: 0.606363, acc.: 64.06%] [G loss: 0.569945]\n",
      "epoch:16 step:15141 [D loss: 0.542378, acc.: 68.75%] [G loss: 0.636092]\n",
      "epoch:16 step:15142 [D loss: 0.603580, acc.: 67.19%] [G loss: 0.508968]\n",
      "epoch:16 step:15143 [D loss: 0.608839, acc.: 64.84%] [G loss: 0.569853]\n",
      "epoch:16 step:15144 [D loss: 0.515157, acc.: 71.09%] [G loss: 0.726005]\n",
      "epoch:16 step:15145 [D loss: 0.650465, acc.: 62.50%] [G loss: 0.585596]\n",
      "epoch:16 step:15146 [D loss: 0.545313, acc.: 70.31%] [G loss: 0.662596]\n",
      "epoch:16 step:15147 [D loss: 0.485869, acc.: 71.88%] [G loss: 0.636267]\n",
      "epoch:16 step:15148 [D loss: 0.488295, acc.: 77.34%] [G loss: 0.559769]\n",
      "epoch:16 step:15149 [D loss: 0.577172, acc.: 66.41%] [G loss: 0.672260]\n",
      "epoch:16 step:15150 [D loss: 0.593150, acc.: 66.41%] [G loss: 0.591987]\n",
      "epoch:16 step:15151 [D loss: 0.528223, acc.: 72.66%] [G loss: 0.626436]\n",
      "epoch:16 step:15152 [D loss: 0.562880, acc.: 72.66%] [G loss: 0.652300]\n",
      "epoch:16 step:15153 [D loss: 0.536314, acc.: 72.66%] [G loss: 0.880180]\n",
      "epoch:16 step:15154 [D loss: 0.507645, acc.: 75.00%] [G loss: 0.709651]\n",
      "epoch:16 step:15155 [D loss: 0.578046, acc.: 67.19%] [G loss: 0.612402]\n",
      "epoch:16 step:15156 [D loss: 0.550784, acc.: 72.66%] [G loss: 0.711957]\n",
      "epoch:16 step:15157 [D loss: 0.511331, acc.: 73.44%] [G loss: 0.595421]\n",
      "epoch:16 step:15158 [D loss: 0.605966, acc.: 64.84%] [G loss: 0.575295]\n",
      "epoch:16 step:15159 [D loss: 0.552292, acc.: 68.75%] [G loss: 0.478098]\n",
      "epoch:16 step:15160 [D loss: 0.535767, acc.: 68.75%] [G loss: 0.521901]\n",
      "epoch:16 step:15161 [D loss: 0.589571, acc.: 66.41%] [G loss: 0.550907]\n",
      "epoch:16 step:15162 [D loss: 0.549912, acc.: 70.31%] [G loss: 0.408987]\n",
      "epoch:16 step:15163 [D loss: 0.522716, acc.: 71.88%] [G loss: 0.535089]\n",
      "epoch:16 step:15164 [D loss: 0.480462, acc.: 75.78%] [G loss: 0.795406]\n",
      "epoch:16 step:15165 [D loss: 0.493763, acc.: 75.78%] [G loss: 0.673201]\n",
      "epoch:16 step:15166 [D loss: 0.578556, acc.: 64.06%] [G loss: 0.524241]\n",
      "epoch:16 step:15167 [D loss: 0.557858, acc.: 63.28%] [G loss: 0.634175]\n",
      "epoch:16 step:15168 [D loss: 0.577132, acc.: 70.31%] [G loss: 0.596116]\n",
      "epoch:16 step:15169 [D loss: 0.512974, acc.: 75.78%] [G loss: 0.625307]\n",
      "epoch:16 step:15170 [D loss: 0.558131, acc.: 72.66%] [G loss: 0.490377]\n",
      "epoch:16 step:15171 [D loss: 0.556658, acc.: 67.97%] [G loss: 0.550109]\n",
      "epoch:16 step:15172 [D loss: 0.638427, acc.: 63.28%] [G loss: 0.497040]\n",
      "epoch:16 step:15173 [D loss: 0.574363, acc.: 70.31%] [G loss: 0.507281]\n",
      "epoch:16 step:15174 [D loss: 0.530665, acc.: 75.78%] [G loss: 0.717697]\n",
      "epoch:16 step:15175 [D loss: 0.609681, acc.: 67.19%] [G loss: 0.655168]\n",
      "epoch:16 step:15176 [D loss: 0.529354, acc.: 71.09%] [G loss: 0.813607]\n",
      "epoch:16 step:15177 [D loss: 0.590476, acc.: 71.88%] [G loss: 0.541647]\n",
      "epoch:16 step:15178 [D loss: 0.609046, acc.: 64.06%] [G loss: 0.477531]\n",
      "epoch:16 step:15179 [D loss: 0.594797, acc.: 65.62%] [G loss: 0.599941]\n",
      "epoch:16 step:15180 [D loss: 0.528209, acc.: 74.22%] [G loss: 0.580346]\n",
      "epoch:16 step:15181 [D loss: 0.588597, acc.: 69.53%] [G loss: 0.638099]\n",
      "epoch:16 step:15182 [D loss: 0.487918, acc.: 78.91%] [G loss: 0.591365]\n",
      "epoch:16 step:15183 [D loss: 0.588572, acc.: 68.75%] [G loss: 0.576544]\n",
      "epoch:16 step:15184 [D loss: 0.522467, acc.: 71.09%] [G loss: 0.623697]\n",
      "epoch:16 step:15185 [D loss: 0.584601, acc.: 67.19%] [G loss: 0.691014]\n",
      "epoch:16 step:15186 [D loss: 0.474763, acc.: 76.56%] [G loss: 0.661015]\n",
      "epoch:16 step:15187 [D loss: 0.577997, acc.: 67.97%] [G loss: 0.599720]\n",
      "epoch:16 step:15188 [D loss: 0.556724, acc.: 68.75%] [G loss: 0.595496]\n",
      "epoch:16 step:15189 [D loss: 0.537638, acc.: 71.09%] [G loss: 0.627196]\n",
      "epoch:16 step:15190 [D loss: 0.455648, acc.: 79.69%] [G loss: 0.833575]\n",
      "epoch:16 step:15191 [D loss: 0.545630, acc.: 71.09%] [G loss: 0.770506]\n",
      "epoch:16 step:15192 [D loss: 0.606431, acc.: 67.97%] [G loss: 0.702632]\n",
      "epoch:16 step:15193 [D loss: 0.564638, acc.: 70.31%] [G loss: 0.640841]\n",
      "epoch:16 step:15194 [D loss: 0.493673, acc.: 75.00%] [G loss: 0.501431]\n",
      "epoch:16 step:15195 [D loss: 0.611176, acc.: 64.84%] [G loss: 0.465948]\n",
      "epoch:16 step:15196 [D loss: 0.520814, acc.: 75.00%] [G loss: 0.661617]\n",
      "epoch:16 step:15197 [D loss: 0.515294, acc.: 75.78%] [G loss: 0.835518]\n",
      "epoch:16 step:15198 [D loss: 0.513865, acc.: 73.44%] [G loss: 0.731170]\n",
      "epoch:16 step:15199 [D loss: 0.445062, acc.: 78.91%] [G loss: 0.857909]\n",
      "epoch:16 step:15200 [D loss: 0.429172, acc.: 82.03%] [G loss: 0.721203]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.473038\n",
      "FID: 47.152878\n",
      "0 = 13.067036478900858\n",
      "1 = 0.09571620306847839\n",
      "2 = 0.88919997215271\n",
      "3 = 0.8356000185012817\n",
      "4 = 0.942799985408783\n",
      "5 = 0.935931921005249\n",
      "6 = 0.8356000185012817\n",
      "7 = 8.110661464571972\n",
      "8 = 0.14563146879702715\n",
      "9 = 0.7267000079154968\n",
      "10 = 0.7193999886512756\n",
      "11 = 0.734000027179718\n",
      "12 = 0.7300588488578796\n",
      "13 = 0.7193999886512756\n",
      "14 = 6.473062992095947\n",
      "15 = 7.077977657318115\n",
      "16 = 0.3775412142276764\n",
      "17 = 6.4730377197265625\n",
      "18 = 47.15287780761719\n",
      "epoch:16 step:15201 [D loss: 0.486773, acc.: 71.88%] [G loss: 0.818767]\n",
      "epoch:16 step:15202 [D loss: 0.665452, acc.: 59.38%] [G loss: 0.586090]\n",
      "epoch:16 step:15203 [D loss: 0.591959, acc.: 64.06%] [G loss: 0.536196]\n",
      "epoch:16 step:15204 [D loss: 0.547403, acc.: 76.56%] [G loss: 0.449568]\n",
      "epoch:16 step:15205 [D loss: 0.545101, acc.: 74.22%] [G loss: 0.605971]\n",
      "epoch:16 step:15206 [D loss: 0.618832, acc.: 64.06%] [G loss: 0.508089]\n",
      "epoch:16 step:15207 [D loss: 0.605804, acc.: 58.59%] [G loss: 0.499380]\n",
      "epoch:16 step:15208 [D loss: 0.541837, acc.: 71.88%] [G loss: 0.714864]\n",
      "epoch:16 step:15209 [D loss: 0.558194, acc.: 67.97%] [G loss: 0.658133]\n",
      "epoch:16 step:15210 [D loss: 0.530360, acc.: 76.56%] [G loss: 0.574140]\n",
      "epoch:16 step:15211 [D loss: 0.449563, acc.: 80.47%] [G loss: 0.719750]\n",
      "epoch:16 step:15212 [D loss: 0.596922, acc.: 65.62%] [G loss: 0.640324]\n",
      "epoch:16 step:15213 [D loss: 0.505972, acc.: 74.22%] [G loss: 0.711456]\n",
      "epoch:16 step:15214 [D loss: 0.511392, acc.: 77.34%] [G loss: 0.914231]\n",
      "epoch:16 step:15215 [D loss: 0.581888, acc.: 66.41%] [G loss: 0.839537]\n",
      "epoch:16 step:15216 [D loss: 0.597609, acc.: 71.88%] [G loss: 0.654173]\n",
      "epoch:16 step:15217 [D loss: 0.569925, acc.: 64.84%] [G loss: 0.567678]\n",
      "epoch:16 step:15218 [D loss: 0.609370, acc.: 64.84%] [G loss: 0.547141]\n",
      "epoch:16 step:15219 [D loss: 0.554255, acc.: 70.31%] [G loss: 0.548539]\n",
      "epoch:16 step:15220 [D loss: 0.579831, acc.: 64.84%] [G loss: 0.502203]\n",
      "epoch:16 step:15221 [D loss: 0.592507, acc.: 69.53%] [G loss: 0.371033]\n",
      "epoch:16 step:15222 [D loss: 0.550268, acc.: 71.88%] [G loss: 0.536026]\n",
      "epoch:16 step:15223 [D loss: 0.461022, acc.: 78.12%] [G loss: 0.765575]\n",
      "epoch:16 step:15224 [D loss: 0.537832, acc.: 70.31%] [G loss: 0.930424]\n",
      "epoch:16 step:15225 [D loss: 0.523214, acc.: 71.09%] [G loss: 0.765713]\n",
      "epoch:16 step:15226 [D loss: 0.554837, acc.: 67.97%] [G loss: 0.670167]\n",
      "epoch:16 step:15227 [D loss: 0.575528, acc.: 65.62%] [G loss: 0.628781]\n",
      "epoch:16 step:15228 [D loss: 0.590774, acc.: 67.97%] [G loss: 0.525969]\n",
      "epoch:16 step:15229 [D loss: 0.564066, acc.: 68.75%] [G loss: 0.648021]\n",
      "epoch:16 step:15230 [D loss: 0.559351, acc.: 68.75%] [G loss: 0.518256]\n",
      "epoch:16 step:15231 [D loss: 0.541948, acc.: 71.88%] [G loss: 0.614614]\n",
      "epoch:16 step:15232 [D loss: 0.537113, acc.: 68.75%] [G loss: 0.679486]\n",
      "epoch:16 step:15233 [D loss: 0.515631, acc.: 74.22%] [G loss: 0.735770]\n",
      "epoch:16 step:15234 [D loss: 0.583583, acc.: 66.41%] [G loss: 0.691075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15235 [D loss: 0.584505, acc.: 63.28%] [G loss: 0.711888]\n",
      "epoch:16 step:15236 [D loss: 0.493778, acc.: 75.00%] [G loss: 0.720703]\n",
      "epoch:16 step:15237 [D loss: 0.554756, acc.: 68.75%] [G loss: 0.870633]\n",
      "epoch:16 step:15238 [D loss: 0.539911, acc.: 71.88%] [G loss: 0.632033]\n",
      "epoch:16 step:15239 [D loss: 0.527483, acc.: 71.88%] [G loss: 0.686383]\n",
      "epoch:16 step:15240 [D loss: 0.484491, acc.: 72.66%] [G loss: 0.757246]\n",
      "epoch:16 step:15241 [D loss: 0.548558, acc.: 75.78%] [G loss: 0.710508]\n",
      "epoch:16 step:15242 [D loss: 0.591324, acc.: 67.19%] [G loss: 0.492291]\n",
      "epoch:16 step:15243 [D loss: 0.625945, acc.: 64.84%] [G loss: 0.504630]\n",
      "epoch:16 step:15244 [D loss: 0.515950, acc.: 72.66%] [G loss: 0.605387]\n",
      "epoch:16 step:15245 [D loss: 0.567347, acc.: 70.31%] [G loss: 0.594999]\n",
      "epoch:16 step:15246 [D loss: 0.509389, acc.: 72.66%] [G loss: 0.677755]\n",
      "epoch:16 step:15247 [D loss: 0.555849, acc.: 71.09%] [G loss: 0.825242]\n",
      "epoch:16 step:15248 [D loss: 0.592115, acc.: 65.62%] [G loss: 0.594833]\n",
      "epoch:16 step:15249 [D loss: 0.526777, acc.: 71.09%] [G loss: 0.646750]\n",
      "epoch:16 step:15250 [D loss: 0.554298, acc.: 69.53%] [G loss: 0.551404]\n",
      "epoch:16 step:15251 [D loss: 0.535422, acc.: 68.75%] [G loss: 0.617685]\n",
      "epoch:16 step:15252 [D loss: 0.590816, acc.: 64.84%] [G loss: 0.481641]\n",
      "epoch:16 step:15253 [D loss: 0.539657, acc.: 69.53%] [G loss: 0.531829]\n",
      "epoch:16 step:15254 [D loss: 0.538356, acc.: 72.66%] [G loss: 0.511256]\n",
      "epoch:16 step:15255 [D loss: 0.571800, acc.: 68.75%] [G loss: 0.567354]\n",
      "epoch:16 step:15256 [D loss: 0.541932, acc.: 72.66%] [G loss: 0.696091]\n",
      "epoch:16 step:15257 [D loss: 0.525424, acc.: 72.66%] [G loss: 0.622968]\n",
      "epoch:16 step:15258 [D loss: 0.544576, acc.: 70.31%] [G loss: 0.701283]\n",
      "epoch:16 step:15259 [D loss: 0.554654, acc.: 66.41%] [G loss: 0.648530]\n",
      "epoch:16 step:15260 [D loss: 0.550699, acc.: 67.19%] [G loss: 0.612293]\n",
      "epoch:16 step:15261 [D loss: 0.492392, acc.: 76.56%] [G loss: 0.889543]\n",
      "epoch:16 step:15262 [D loss: 0.523886, acc.: 73.44%] [G loss: 0.672658]\n",
      "epoch:16 step:15263 [D loss: 0.517331, acc.: 71.88%] [G loss: 0.689905]\n",
      "epoch:16 step:15264 [D loss: 0.569467, acc.: 63.28%] [G loss: 0.589595]\n",
      "epoch:16 step:15265 [D loss: 0.547129, acc.: 69.53%] [G loss: 0.675534]\n",
      "epoch:16 step:15266 [D loss: 0.493798, acc.: 76.56%] [G loss: 0.725163]\n",
      "epoch:16 step:15267 [D loss: 0.568477, acc.: 69.53%] [G loss: 0.816527]\n",
      "epoch:16 step:15268 [D loss: 0.507578, acc.: 76.56%] [G loss: 0.688488]\n",
      "epoch:16 step:15269 [D loss: 0.714151, acc.: 62.50%] [G loss: 0.444488]\n",
      "epoch:16 step:15270 [D loss: 0.633519, acc.: 63.28%] [G loss: 0.451554]\n",
      "epoch:16 step:15271 [D loss: 0.580355, acc.: 67.19%] [G loss: 0.436801]\n",
      "epoch:16 step:15272 [D loss: 0.594128, acc.: 60.94%] [G loss: 0.484311]\n",
      "epoch:16 step:15273 [D loss: 0.572853, acc.: 65.62%] [G loss: 0.619113]\n",
      "epoch:16 step:15274 [D loss: 0.552484, acc.: 76.56%] [G loss: 0.520494]\n",
      "epoch:16 step:15275 [D loss: 0.521280, acc.: 74.22%] [G loss: 0.638410]\n",
      "epoch:16 step:15276 [D loss: 0.539718, acc.: 71.09%] [G loss: 0.572423]\n",
      "epoch:16 step:15277 [D loss: 0.544200, acc.: 70.31%] [G loss: 0.701236]\n",
      "epoch:16 step:15278 [D loss: 0.515890, acc.: 71.88%] [G loss: 0.526963]\n",
      "epoch:16 step:15279 [D loss: 0.587197, acc.: 68.75%] [G loss: 0.644168]\n",
      "epoch:16 step:15280 [D loss: 0.621616, acc.: 60.94%] [G loss: 0.665509]\n",
      "epoch:16 step:15281 [D loss: 0.561179, acc.: 67.19%] [G loss: 0.779209]\n",
      "epoch:16 step:15282 [D loss: 0.546923, acc.: 73.44%] [G loss: 0.670529]\n",
      "epoch:16 step:15283 [D loss: 0.549644, acc.: 70.31%] [G loss: 0.590890]\n",
      "epoch:16 step:15284 [D loss: 0.591041, acc.: 65.62%] [G loss: 0.577670]\n",
      "epoch:16 step:15285 [D loss: 0.532579, acc.: 68.75%] [G loss: 0.526347]\n",
      "epoch:16 step:15286 [D loss: 0.687252, acc.: 58.59%] [G loss: 0.479182]\n",
      "epoch:16 step:15287 [D loss: 0.586529, acc.: 71.09%] [G loss: 0.550808]\n",
      "epoch:16 step:15288 [D loss: 0.497772, acc.: 75.00%] [G loss: 0.675652]\n",
      "epoch:16 step:15289 [D loss: 0.545690, acc.: 73.44%] [G loss: 0.574376]\n",
      "epoch:16 step:15290 [D loss: 0.492973, acc.: 78.12%] [G loss: 0.619207]\n",
      "epoch:16 step:15291 [D loss: 0.441125, acc.: 79.69%] [G loss: 0.745898]\n",
      "epoch:16 step:15292 [D loss: 0.482543, acc.: 74.22%] [G loss: 0.645906]\n",
      "epoch:16 step:15293 [D loss: 0.611986, acc.: 67.97%] [G loss: 0.615370]\n",
      "epoch:16 step:15294 [D loss: 0.560803, acc.: 70.31%] [G loss: 0.577884]\n",
      "epoch:16 step:15295 [D loss: 0.573181, acc.: 67.97%] [G loss: 0.580101]\n",
      "epoch:16 step:15296 [D loss: 0.485900, acc.: 78.91%] [G loss: 0.659180]\n",
      "epoch:16 step:15297 [D loss: 0.536581, acc.: 71.88%] [G loss: 0.718499]\n",
      "epoch:16 step:15298 [D loss: 0.529544, acc.: 70.31%] [G loss: 0.677096]\n",
      "epoch:16 step:15299 [D loss: 0.503779, acc.: 78.12%] [G loss: 0.634511]\n",
      "epoch:16 step:15300 [D loss: 0.596213, acc.: 61.72%] [G loss: 0.465693]\n",
      "epoch:16 step:15301 [D loss: 0.521818, acc.: 73.44%] [G loss: 0.713410]\n",
      "epoch:16 step:15302 [D loss: 0.549329, acc.: 68.75%] [G loss: 0.653199]\n",
      "epoch:16 step:15303 [D loss: 0.504801, acc.: 67.19%] [G loss: 0.842433]\n",
      "epoch:16 step:15304 [D loss: 0.491576, acc.: 74.22%] [G loss: 0.778983]\n",
      "epoch:16 step:15305 [D loss: 0.521915, acc.: 69.53%] [G loss: 0.843409]\n",
      "epoch:16 step:15306 [D loss: 0.447444, acc.: 78.12%] [G loss: 1.114313]\n",
      "epoch:16 step:15307 [D loss: 0.500558, acc.: 76.56%] [G loss: 1.018439]\n",
      "epoch:16 step:15308 [D loss: 0.735731, acc.: 64.06%] [G loss: 0.550427]\n",
      "epoch:16 step:15309 [D loss: 0.586141, acc.: 67.19%] [G loss: 0.599647]\n",
      "epoch:16 step:15310 [D loss: 0.567427, acc.: 68.75%] [G loss: 0.676728]\n",
      "epoch:16 step:15311 [D loss: 0.496903, acc.: 70.31%] [G loss: 0.651969]\n",
      "epoch:16 step:15312 [D loss: 0.557706, acc.: 71.88%] [G loss: 0.591982]\n",
      "epoch:16 step:15313 [D loss: 0.507966, acc.: 78.12%] [G loss: 0.708622]\n",
      "epoch:16 step:15314 [D loss: 0.567388, acc.: 67.19%] [G loss: 0.818974]\n",
      "epoch:16 step:15315 [D loss: 0.622920, acc.: 66.41%] [G loss: 0.589553]\n",
      "epoch:16 step:15316 [D loss: 0.544823, acc.: 69.53%] [G loss: 0.569411]\n",
      "epoch:16 step:15317 [D loss: 0.538059, acc.: 76.56%] [G loss: 0.601972]\n",
      "epoch:16 step:15318 [D loss: 0.451864, acc.: 81.25%] [G loss: 0.725215]\n",
      "epoch:16 step:15319 [D loss: 0.502798, acc.: 75.00%] [G loss: 0.759170]\n",
      "epoch:16 step:15320 [D loss: 0.522351, acc.: 74.22%] [G loss: 0.709689]\n",
      "epoch:16 step:15321 [D loss: 0.577795, acc.: 71.88%] [G loss: 0.796585]\n",
      "epoch:16 step:15322 [D loss: 0.560226, acc.: 67.19%] [G loss: 0.631896]\n",
      "epoch:16 step:15323 [D loss: 0.544327, acc.: 71.88%] [G loss: 0.623610]\n",
      "epoch:16 step:15324 [D loss: 0.536745, acc.: 74.22%] [G loss: 0.590060]\n",
      "epoch:16 step:15325 [D loss: 0.509906, acc.: 71.88%] [G loss: 0.608269]\n",
      "epoch:16 step:15326 [D loss: 0.484614, acc.: 76.56%] [G loss: 0.569751]\n",
      "epoch:16 step:15327 [D loss: 0.536143, acc.: 69.53%] [G loss: 0.617601]\n",
      "epoch:16 step:15328 [D loss: 0.551396, acc.: 73.44%] [G loss: 0.666707]\n",
      "epoch:16 step:15329 [D loss: 0.545484, acc.: 71.88%] [G loss: 0.744078]\n",
      "epoch:16 step:15330 [D loss: 0.567106, acc.: 71.09%] [G loss: 0.612062]\n",
      "epoch:16 step:15331 [D loss: 0.525522, acc.: 71.88%] [G loss: 0.584672]\n",
      "epoch:16 step:15332 [D loss: 0.507766, acc.: 76.56%] [G loss: 0.677402]\n",
      "epoch:16 step:15333 [D loss: 0.606282, acc.: 64.84%] [G loss: 0.596803]\n",
      "epoch:16 step:15334 [D loss: 0.665757, acc.: 62.50%] [G loss: 0.623590]\n",
      "epoch:16 step:15335 [D loss: 0.589963, acc.: 64.06%] [G loss: 0.491634]\n",
      "epoch:16 step:15336 [D loss: 0.467872, acc.: 78.12%] [G loss: 0.974068]\n",
      "epoch:16 step:15337 [D loss: 0.547273, acc.: 70.31%] [G loss: 0.698491]\n",
      "epoch:16 step:15338 [D loss: 0.513501, acc.: 72.66%] [G loss: 0.837374]\n",
      "epoch:16 step:15339 [D loss: 0.458124, acc.: 78.91%] [G loss: 0.930705]\n",
      "epoch:16 step:15340 [D loss: 0.618691, acc.: 64.84%] [G loss: 0.563216]\n",
      "epoch:16 step:15341 [D loss: 0.664624, acc.: 62.50%] [G loss: 0.551999]\n",
      "epoch:16 step:15342 [D loss: 0.498251, acc.: 75.78%] [G loss: 0.619558]\n",
      "epoch:16 step:15343 [D loss: 0.532717, acc.: 74.22%] [G loss: 0.811983]\n",
      "epoch:16 step:15344 [D loss: 0.560827, acc.: 67.19%] [G loss: 0.685453]\n",
      "epoch:16 step:15345 [D loss: 0.571571, acc.: 67.19%] [G loss: 0.694856]\n",
      "epoch:16 step:15346 [D loss: 0.351957, acc.: 86.72%] [G loss: 0.685948]\n",
      "epoch:16 step:15347 [D loss: 0.577051, acc.: 70.31%] [G loss: 0.979922]\n",
      "epoch:16 step:15348 [D loss: 0.543466, acc.: 71.88%] [G loss: 0.752849]\n",
      "epoch:16 step:15349 [D loss: 0.448582, acc.: 77.34%] [G loss: 0.770277]\n",
      "epoch:16 step:15350 [D loss: 0.458652, acc.: 78.91%] [G loss: 0.856326]\n",
      "epoch:16 step:15351 [D loss: 0.417359, acc.: 81.25%] [G loss: 0.874897]\n",
      "epoch:16 step:15352 [D loss: 0.529291, acc.: 74.22%] [G loss: 0.783204]\n",
      "epoch:16 step:15353 [D loss: 0.485120, acc.: 73.44%] [G loss: 0.788974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15354 [D loss: 0.573387, acc.: 70.31%] [G loss: 0.606959]\n",
      "epoch:16 step:15355 [D loss: 0.584278, acc.: 68.75%] [G loss: 0.692109]\n",
      "epoch:16 step:15356 [D loss: 0.579259, acc.: 68.75%] [G loss: 0.608646]\n",
      "epoch:16 step:15357 [D loss: 0.573510, acc.: 71.09%] [G loss: 0.630332]\n",
      "epoch:16 step:15358 [D loss: 0.505514, acc.: 71.88%] [G loss: 0.710298]\n",
      "epoch:16 step:15359 [D loss: 0.585906, acc.: 67.97%] [G loss: 0.568135]\n",
      "epoch:16 step:15360 [D loss: 0.575700, acc.: 62.50%] [G loss: 0.582465]\n",
      "epoch:16 step:15361 [D loss: 0.527056, acc.: 70.31%] [G loss: 0.684819]\n",
      "epoch:16 step:15362 [D loss: 0.551847, acc.: 70.31%] [G loss: 0.691306]\n",
      "epoch:16 step:15363 [D loss: 0.520833, acc.: 75.78%] [G loss: 0.785260]\n",
      "epoch:16 step:15364 [D loss: 0.535690, acc.: 70.31%] [G loss: 0.647571]\n",
      "epoch:16 step:15365 [D loss: 0.538836, acc.: 74.22%] [G loss: 0.668513]\n",
      "epoch:16 step:15366 [D loss: 0.444172, acc.: 81.25%] [G loss: 0.870923]\n",
      "epoch:16 step:15367 [D loss: 0.602338, acc.: 65.62%] [G loss: 0.739873]\n",
      "epoch:16 step:15368 [D loss: 0.685124, acc.: 54.69%] [G loss: 0.573261]\n",
      "epoch:16 step:15369 [D loss: 0.590630, acc.: 70.31%] [G loss: 0.600780]\n",
      "epoch:16 step:15370 [D loss: 0.552841, acc.: 70.31%] [G loss: 0.663796]\n",
      "epoch:16 step:15371 [D loss: 0.576597, acc.: 70.31%] [G loss: 0.713200]\n",
      "epoch:16 step:15372 [D loss: 0.578751, acc.: 66.41%] [G loss: 0.555583]\n",
      "epoch:16 step:15373 [D loss: 0.462384, acc.: 80.47%] [G loss: 0.711101]\n",
      "epoch:16 step:15374 [D loss: 0.514060, acc.: 75.00%] [G loss: 0.732749]\n",
      "epoch:16 step:15375 [D loss: 0.515315, acc.: 72.66%] [G loss: 0.683935]\n",
      "epoch:16 step:15376 [D loss: 0.524980, acc.: 70.31%] [G loss: 0.783529]\n",
      "epoch:16 step:15377 [D loss: 0.537331, acc.: 69.53%] [G loss: 0.642650]\n",
      "epoch:16 step:15378 [D loss: 0.585304, acc.: 70.31%] [G loss: 0.564615]\n",
      "epoch:16 step:15379 [D loss: 0.520423, acc.: 75.78%] [G loss: 0.517618]\n",
      "epoch:16 step:15380 [D loss: 0.562492, acc.: 69.53%] [G loss: 0.672903]\n",
      "epoch:16 step:15381 [D loss: 0.529539, acc.: 74.22%] [G loss: 0.630104]\n",
      "epoch:16 step:15382 [D loss: 0.591186, acc.: 69.53%] [G loss: 0.638787]\n",
      "epoch:16 step:15383 [D loss: 0.533682, acc.: 75.00%] [G loss: 0.657364]\n",
      "epoch:16 step:15384 [D loss: 0.512803, acc.: 73.44%] [G loss: 0.588249]\n",
      "epoch:16 step:15385 [D loss: 0.561528, acc.: 68.75%] [G loss: 0.611525]\n",
      "epoch:16 step:15386 [D loss: 0.549026, acc.: 67.97%] [G loss: 0.498321]\n",
      "epoch:16 step:15387 [D loss: 0.521527, acc.: 68.75%] [G loss: 0.540329]\n",
      "epoch:16 step:15388 [D loss: 0.619254, acc.: 57.03%] [G loss: 0.577055]\n",
      "epoch:16 step:15389 [D loss: 0.580941, acc.: 64.06%] [G loss: 0.616853]\n",
      "epoch:16 step:15390 [D loss: 0.480274, acc.: 74.22%] [G loss: 0.713776]\n",
      "epoch:16 step:15391 [D loss: 0.521212, acc.: 73.44%] [G loss: 0.703692]\n",
      "epoch:16 step:15392 [D loss: 0.605317, acc.: 63.28%] [G loss: 0.725763]\n",
      "epoch:16 step:15393 [D loss: 0.697027, acc.: 53.12%] [G loss: 0.480832]\n",
      "epoch:16 step:15394 [D loss: 0.566895, acc.: 70.31%] [G loss: 0.518709]\n",
      "epoch:16 step:15395 [D loss: 0.542180, acc.: 68.75%] [G loss: 0.624445]\n",
      "epoch:16 step:15396 [D loss: 0.593270, acc.: 61.72%] [G loss: 0.837725]\n",
      "epoch:16 step:15397 [D loss: 0.574700, acc.: 67.19%] [G loss: 0.593565]\n",
      "epoch:16 step:15398 [D loss: 0.489778, acc.: 75.00%] [G loss: 0.683301]\n",
      "epoch:16 step:15399 [D loss: 0.563128, acc.: 68.75%] [G loss: 0.578300]\n",
      "epoch:16 step:15400 [D loss: 0.666219, acc.: 59.38%] [G loss: 0.752870]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.611862\n",
      "FID: 48.306137\n",
      "0 = 13.076280194282548\n",
      "1 = 0.09359028244600373\n",
      "2 = 0.8871999979019165\n",
      "3 = 0.8345999717712402\n",
      "4 = 0.9398000240325928\n",
      "5 = 0.9327223896980286\n",
      "6 = 0.8345999717712402\n",
      "7 = 8.317530719089483\n",
      "8 = 0.14899260475700743\n",
      "9 = 0.713699996471405\n",
      "10 = 0.7098000049591064\n",
      "11 = 0.7175999879837036\n",
      "12 = 0.7153799533843994\n",
      "13 = 0.7098000049591064\n",
      "14 = 6.611886024475098\n",
      "15 = 6.961577415466309\n",
      "16 = 0.38783398270606995\n",
      "17 = 6.611861705780029\n",
      "18 = 48.30613708496094\n",
      "epoch:16 step:15401 [D loss: 0.548288, acc.: 69.53%] [G loss: 0.576677]\n",
      "epoch:16 step:15402 [D loss: 0.599567, acc.: 64.06%] [G loss: 0.549934]\n",
      "epoch:16 step:15403 [D loss: 0.583892, acc.: 69.53%] [G loss: 0.502587]\n",
      "epoch:16 step:15404 [D loss: 0.648311, acc.: 59.38%] [G loss: 0.414287]\n",
      "epoch:16 step:15405 [D loss: 0.534248, acc.: 68.75%] [G loss: 0.572913]\n",
      "epoch:16 step:15406 [D loss: 0.576953, acc.: 65.62%] [G loss: 0.547710]\n",
      "epoch:16 step:15407 [D loss: 0.548415, acc.: 73.44%] [G loss: 0.641968]\n",
      "epoch:16 step:15408 [D loss: 0.486311, acc.: 78.12%] [G loss: 0.585396]\n",
      "epoch:16 step:15409 [D loss: 0.515843, acc.: 71.88%] [G loss: 0.626375]\n",
      "epoch:16 step:15410 [D loss: 0.633285, acc.: 62.50%] [G loss: 0.579712]\n",
      "epoch:16 step:15411 [D loss: 0.544424, acc.: 72.66%] [G loss: 0.636707]\n",
      "epoch:16 step:15412 [D loss: 0.643941, acc.: 60.16%] [G loss: 0.686291]\n",
      "epoch:16 step:15413 [D loss: 0.575116, acc.: 66.41%] [G loss: 0.664085]\n",
      "epoch:16 step:15414 [D loss: 0.646027, acc.: 66.41%] [G loss: 0.582990]\n",
      "epoch:16 step:15415 [D loss: 0.594759, acc.: 63.28%] [G loss: 0.505539]\n",
      "epoch:16 step:15416 [D loss: 0.546433, acc.: 69.53%] [G loss: 0.726075]\n",
      "epoch:16 step:15417 [D loss: 0.477906, acc.: 78.12%] [G loss: 0.726976]\n",
      "epoch:16 step:15418 [D loss: 0.491222, acc.: 76.56%] [G loss: 0.683728]\n",
      "epoch:16 step:15419 [D loss: 0.451566, acc.: 78.91%] [G loss: 0.689864]\n",
      "epoch:16 step:15420 [D loss: 0.496660, acc.: 73.44%] [G loss: 0.795789]\n",
      "epoch:16 step:15421 [D loss: 0.526107, acc.: 74.22%] [G loss: 0.807316]\n",
      "epoch:16 step:15422 [D loss: 0.456927, acc.: 81.25%] [G loss: 0.803812]\n",
      "epoch:16 step:15423 [D loss: 0.549975, acc.: 70.31%] [G loss: 0.783484]\n",
      "epoch:16 step:15424 [D loss: 0.549837, acc.: 71.09%] [G loss: 0.740461]\n",
      "epoch:16 step:15425 [D loss: 0.600653, acc.: 71.88%] [G loss: 0.659180]\n",
      "epoch:16 step:15426 [D loss: 0.526735, acc.: 68.75%] [G loss: 0.509320]\n",
      "epoch:16 step:15427 [D loss: 0.544802, acc.: 74.22%] [G loss: 0.702457]\n",
      "epoch:16 step:15428 [D loss: 0.523544, acc.: 73.44%] [G loss: 0.605712]\n",
      "epoch:16 step:15429 [D loss: 0.681276, acc.: 60.94%] [G loss: 0.622793]\n",
      "epoch:16 step:15430 [D loss: 0.589127, acc.: 67.97%] [G loss: 0.537411]\n",
      "epoch:16 step:15431 [D loss: 0.589749, acc.: 67.19%] [G loss: 0.460666]\n",
      "epoch:16 step:15432 [D loss: 0.496816, acc.: 73.44%] [G loss: 0.663236]\n",
      "epoch:16 step:15433 [D loss: 0.549290, acc.: 71.09%] [G loss: 0.803477]\n",
      "epoch:16 step:15434 [D loss: 0.518076, acc.: 71.88%] [G loss: 0.719795]\n",
      "epoch:16 step:15435 [D loss: 0.535376, acc.: 70.31%] [G loss: 0.771294]\n",
      "epoch:16 step:15436 [D loss: 0.535321, acc.: 67.97%] [G loss: 0.691145]\n",
      "epoch:16 step:15437 [D loss: 0.590610, acc.: 64.06%] [G loss: 0.759905]\n",
      "epoch:16 step:15438 [D loss: 0.523256, acc.: 67.97%] [G loss: 0.711208]\n",
      "epoch:16 step:15439 [D loss: 0.590791, acc.: 66.41%] [G loss: 0.725577]\n",
      "epoch:16 step:15440 [D loss: 0.542213, acc.: 75.00%] [G loss: 0.705942]\n",
      "epoch:16 step:15441 [D loss: 0.497425, acc.: 76.56%] [G loss: 0.616053]\n",
      "epoch:16 step:15442 [D loss: 0.493145, acc.: 75.00%] [G loss: 0.698033]\n",
      "epoch:16 step:15443 [D loss: 0.434278, acc.: 81.25%] [G loss: 0.780119]\n",
      "epoch:16 step:15444 [D loss: 0.506572, acc.: 74.22%] [G loss: 0.787548]\n",
      "epoch:16 step:15445 [D loss: 0.506037, acc.: 77.34%] [G loss: 0.768034]\n",
      "epoch:16 step:15446 [D loss: 0.581214, acc.: 70.31%] [G loss: 0.553694]\n",
      "epoch:16 step:15447 [D loss: 0.549372, acc.: 70.31%] [G loss: 0.708281]\n",
      "epoch:16 step:15448 [D loss: 0.651274, acc.: 65.62%] [G loss: 0.622381]\n",
      "epoch:16 step:15449 [D loss: 0.510469, acc.: 72.66%] [G loss: 0.583757]\n",
      "epoch:16 step:15450 [D loss: 0.645228, acc.: 63.28%] [G loss: 0.562890]\n",
      "epoch:16 step:15451 [D loss: 0.583494, acc.: 64.84%] [G loss: 0.463111]\n",
      "epoch:16 step:15452 [D loss: 0.548582, acc.: 69.53%] [G loss: 0.553414]\n",
      "epoch:16 step:15453 [D loss: 0.513546, acc.: 73.44%] [G loss: 0.773921]\n",
      "epoch:16 step:15454 [D loss: 0.581429, acc.: 64.06%] [G loss: 0.566101]\n",
      "epoch:16 step:15455 [D loss: 0.509992, acc.: 71.09%] [G loss: 0.583129]\n",
      "epoch:16 step:15456 [D loss: 0.552255, acc.: 67.97%] [G loss: 0.545019]\n",
      "epoch:16 step:15457 [D loss: 0.581065, acc.: 68.75%] [G loss: 0.541469]\n",
      "epoch:16 step:15458 [D loss: 0.662540, acc.: 61.72%] [G loss: 0.511391]\n",
      "epoch:16 step:15459 [D loss: 0.534274, acc.: 69.53%] [G loss: 0.539415]\n",
      "epoch:16 step:15460 [D loss: 0.546686, acc.: 74.22%] [G loss: 0.688686]\n",
      "epoch:16 step:15461 [D loss: 0.595037, acc.: 67.97%] [G loss: 0.644794]\n",
      "epoch:16 step:15462 [D loss: 0.542514, acc.: 71.09%] [G loss: 0.779614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15463 [D loss: 0.464249, acc.: 79.69%] [G loss: 0.882250]\n",
      "epoch:16 step:15464 [D loss: 0.433466, acc.: 80.47%] [G loss: 0.870857]\n",
      "epoch:16 step:15465 [D loss: 0.597134, acc.: 68.75%] [G loss: 0.840571]\n",
      "epoch:16 step:15466 [D loss: 0.539263, acc.: 71.88%] [G loss: 0.575326]\n",
      "epoch:16 step:15467 [D loss: 0.482815, acc.: 75.78%] [G loss: 0.874077]\n",
      "epoch:16 step:15468 [D loss: 0.659223, acc.: 60.16%] [G loss: 0.645553]\n",
      "epoch:16 step:15469 [D loss: 0.707937, acc.: 63.28%] [G loss: 0.556589]\n",
      "epoch:16 step:15470 [D loss: 0.590738, acc.: 66.41%] [G loss: 0.483900]\n",
      "epoch:16 step:15471 [D loss: 0.500966, acc.: 78.91%] [G loss: 0.523225]\n",
      "epoch:16 step:15472 [D loss: 0.561043, acc.: 72.66%] [G loss: 0.511413]\n",
      "epoch:16 step:15473 [D loss: 0.486176, acc.: 80.47%] [G loss: 0.598219]\n",
      "epoch:16 step:15474 [D loss: 0.583701, acc.: 66.41%] [G loss: 0.398460]\n",
      "epoch:16 step:15475 [D loss: 0.525425, acc.: 74.22%] [G loss: 0.583564]\n",
      "epoch:16 step:15476 [D loss: 0.544984, acc.: 73.44%] [G loss: 0.642386]\n",
      "epoch:16 step:15477 [D loss: 0.528487, acc.: 72.66%] [G loss: 0.690465]\n",
      "epoch:16 step:15478 [D loss: 0.608049, acc.: 65.62%] [G loss: 0.748760]\n",
      "epoch:16 step:15479 [D loss: 0.554778, acc.: 69.53%] [G loss: 0.577249]\n",
      "epoch:16 step:15480 [D loss: 0.501513, acc.: 71.09%] [G loss: 0.691387]\n",
      "epoch:16 step:15481 [D loss: 0.519734, acc.: 75.00%] [G loss: 0.587593]\n",
      "epoch:16 step:15482 [D loss: 0.564396, acc.: 69.53%] [G loss: 0.518066]\n",
      "epoch:16 step:15483 [D loss: 0.529229, acc.: 75.00%] [G loss: 0.525471]\n",
      "epoch:16 step:15484 [D loss: 0.577123, acc.: 70.31%] [G loss: 0.620275]\n",
      "epoch:16 step:15485 [D loss: 0.607535, acc.: 63.28%] [G loss: 0.599282]\n",
      "epoch:16 step:15486 [D loss: 0.631841, acc.: 64.84%] [G loss: 0.558058]\n",
      "epoch:16 step:15487 [D loss: 0.494112, acc.: 77.34%] [G loss: 0.678225]\n",
      "epoch:16 step:15488 [D loss: 0.548498, acc.: 71.09%] [G loss: 0.554309]\n",
      "epoch:16 step:15489 [D loss: 0.554062, acc.: 72.66%] [G loss: 0.515715]\n",
      "epoch:16 step:15490 [D loss: 0.553567, acc.: 71.88%] [G loss: 0.592962]\n",
      "epoch:16 step:15491 [D loss: 0.454560, acc.: 79.69%] [G loss: 0.620494]\n",
      "epoch:16 step:15492 [D loss: 0.595380, acc.: 71.09%] [G loss: 0.635213]\n",
      "epoch:16 step:15493 [D loss: 0.651243, acc.: 60.16%] [G loss: 0.476065]\n",
      "epoch:16 step:15494 [D loss: 0.589816, acc.: 67.97%] [G loss: 0.503410]\n",
      "epoch:16 step:15495 [D loss: 0.486489, acc.: 76.56%] [G loss: 0.641104]\n",
      "epoch:16 step:15496 [D loss: 0.489070, acc.: 75.78%] [G loss: 0.742245]\n",
      "epoch:16 step:15497 [D loss: 0.514368, acc.: 73.44%] [G loss: 0.661188]\n",
      "epoch:16 step:15498 [D loss: 0.528442, acc.: 67.19%] [G loss: 0.764655]\n",
      "epoch:16 step:15499 [D loss: 0.539648, acc.: 71.09%] [G loss: 0.752769]\n",
      "epoch:16 step:15500 [D loss: 0.407103, acc.: 81.25%] [G loss: 0.819117]\n",
      "epoch:16 step:15501 [D loss: 0.480471, acc.: 75.00%] [G loss: 0.839780]\n",
      "epoch:16 step:15502 [D loss: 0.593553, acc.: 62.50%] [G loss: 0.611771]\n",
      "epoch:16 step:15503 [D loss: 0.715518, acc.: 57.81%] [G loss: 0.501117]\n",
      "epoch:16 step:15504 [D loss: 0.602374, acc.: 64.06%] [G loss: 0.530358]\n",
      "epoch:16 step:15505 [D loss: 0.529722, acc.: 70.31%] [G loss: 0.596415]\n",
      "epoch:16 step:15506 [D loss: 0.489844, acc.: 77.34%] [G loss: 0.571455]\n",
      "epoch:16 step:15507 [D loss: 0.600218, acc.: 71.09%] [G loss: 0.628297]\n",
      "epoch:16 step:15508 [D loss: 0.449412, acc.: 80.47%] [G loss: 0.858355]\n",
      "epoch:16 step:15509 [D loss: 0.537890, acc.: 73.44%] [G loss: 0.568835]\n",
      "epoch:16 step:15510 [D loss: 0.511397, acc.: 73.44%] [G loss: 0.644209]\n",
      "epoch:16 step:15511 [D loss: 0.474777, acc.: 75.78%] [G loss: 0.670145]\n",
      "epoch:16 step:15512 [D loss: 0.495179, acc.: 71.09%] [G loss: 0.592230]\n",
      "epoch:16 step:15513 [D loss: 0.465880, acc.: 78.91%] [G loss: 0.752740]\n",
      "epoch:16 step:15514 [D loss: 0.510691, acc.: 70.31%] [G loss: 0.750671]\n",
      "epoch:16 step:15515 [D loss: 0.512347, acc.: 75.00%] [G loss: 0.752189]\n",
      "epoch:16 step:15516 [D loss: 0.538457, acc.: 68.75%] [G loss: 0.739500]\n",
      "epoch:16 step:15517 [D loss: 0.578318, acc.: 67.97%] [G loss: 0.702733]\n",
      "epoch:16 step:15518 [D loss: 0.519842, acc.: 71.88%] [G loss: 0.508125]\n",
      "epoch:16 step:15519 [D loss: 0.577514, acc.: 67.19%] [G loss: 0.444848]\n",
      "epoch:16 step:15520 [D loss: 0.700398, acc.: 53.12%] [G loss: 0.424088]\n",
      "epoch:16 step:15521 [D loss: 0.608958, acc.: 65.62%] [G loss: 0.509393]\n",
      "epoch:16 step:15522 [D loss: 0.544838, acc.: 67.97%] [G loss: 0.560377]\n",
      "epoch:16 step:15523 [D loss: 0.577287, acc.: 70.31%] [G loss: 0.580981]\n",
      "epoch:16 step:15524 [D loss: 0.583945, acc.: 70.31%] [G loss: 0.402961]\n",
      "epoch:16 step:15525 [D loss: 0.520298, acc.: 75.00%] [G loss: 0.498201]\n",
      "epoch:16 step:15526 [D loss: 0.486864, acc.: 74.22%] [G loss: 0.633190]\n",
      "epoch:16 step:15527 [D loss: 0.649195, acc.: 60.16%] [G loss: 0.505803]\n",
      "epoch:16 step:15528 [D loss: 0.542179, acc.: 71.09%] [G loss: 0.510403]\n",
      "epoch:16 step:15529 [D loss: 0.559120, acc.: 70.31%] [G loss: 0.570963]\n",
      "epoch:16 step:15530 [D loss: 0.545062, acc.: 69.53%] [G loss: 0.463198]\n",
      "epoch:16 step:15531 [D loss: 0.536470, acc.: 73.44%] [G loss: 0.498641]\n",
      "epoch:16 step:15532 [D loss: 0.545046, acc.: 68.75%] [G loss: 0.608960]\n",
      "epoch:16 step:15533 [D loss: 0.522008, acc.: 67.97%] [G loss: 0.495409]\n",
      "epoch:16 step:15534 [D loss: 0.661220, acc.: 59.38%] [G loss: 0.547018]\n",
      "epoch:16 step:15535 [D loss: 0.573533, acc.: 69.53%] [G loss: 0.552866]\n",
      "epoch:16 step:15536 [D loss: 0.488012, acc.: 72.66%] [G loss: 0.534072]\n",
      "epoch:16 step:15537 [D loss: 0.533231, acc.: 71.09%] [G loss: 0.625422]\n",
      "epoch:16 step:15538 [D loss: 0.469038, acc.: 78.12%] [G loss: 0.575779]\n",
      "epoch:16 step:15539 [D loss: 0.537789, acc.: 74.22%] [G loss: 0.584051]\n",
      "epoch:16 step:15540 [D loss: 0.496907, acc.: 75.78%] [G loss: 0.868273]\n",
      "epoch:16 step:15541 [D loss: 0.562784, acc.: 71.09%] [G loss: 0.697582]\n",
      "epoch:16 step:15542 [D loss: 0.588759, acc.: 70.31%] [G loss: 0.663724]\n",
      "epoch:16 step:15543 [D loss: 0.509360, acc.: 75.78%] [G loss: 0.637076]\n",
      "epoch:16 step:15544 [D loss: 0.494864, acc.: 74.22%] [G loss: 0.589412]\n",
      "epoch:16 step:15545 [D loss: 0.583576, acc.: 71.88%] [G loss: 0.622777]\n",
      "epoch:16 step:15546 [D loss: 0.427244, acc.: 82.03%] [G loss: 0.674929]\n",
      "epoch:16 step:15547 [D loss: 0.528655, acc.: 75.78%] [G loss: 0.541633]\n",
      "epoch:16 step:15548 [D loss: 0.558704, acc.: 69.53%] [G loss: 0.565199]\n",
      "epoch:16 step:15549 [D loss: 0.508796, acc.: 75.00%] [G loss: 0.541782]\n",
      "epoch:16 step:15550 [D loss: 0.474330, acc.: 75.78%] [G loss: 0.684408]\n",
      "epoch:16 step:15551 [D loss: 0.619176, acc.: 66.41%] [G loss: 0.665951]\n",
      "epoch:16 step:15552 [D loss: 0.514443, acc.: 71.09%] [G loss: 0.702988]\n",
      "epoch:16 step:15553 [D loss: 0.560594, acc.: 69.53%] [G loss: 0.713826]\n",
      "epoch:16 step:15554 [D loss: 0.578098, acc.: 63.28%] [G loss: 0.601015]\n",
      "epoch:16 step:15555 [D loss: 0.591480, acc.: 65.62%] [G loss: 0.595646]\n",
      "epoch:16 step:15556 [D loss: 0.501280, acc.: 76.56%] [G loss: 0.591774]\n",
      "epoch:16 step:15557 [D loss: 0.540115, acc.: 73.44%] [G loss: 0.594802]\n",
      "epoch:16 step:15558 [D loss: 0.672135, acc.: 61.72%] [G loss: 0.588808]\n",
      "epoch:16 step:15559 [D loss: 0.507132, acc.: 71.09%] [G loss: 0.552830]\n",
      "epoch:16 step:15560 [D loss: 0.521867, acc.: 72.66%] [G loss: 0.594051]\n",
      "epoch:16 step:15561 [D loss: 0.544147, acc.: 74.22%] [G loss: 0.612689]\n",
      "epoch:16 step:15562 [D loss: 0.562853, acc.: 68.75%] [G loss: 0.587790]\n",
      "epoch:16 step:15563 [D loss: 0.507863, acc.: 73.44%] [G loss: 0.605091]\n",
      "epoch:16 step:15564 [D loss: 0.545082, acc.: 73.44%] [G loss: 0.574189]\n",
      "epoch:16 step:15565 [D loss: 0.517319, acc.: 71.09%] [G loss: 0.624421]\n",
      "epoch:16 step:15566 [D loss: 0.468844, acc.: 75.78%] [G loss: 0.752657]\n",
      "epoch:16 step:15567 [D loss: 0.484432, acc.: 71.88%] [G loss: 0.769372]\n",
      "epoch:16 step:15568 [D loss: 0.623892, acc.: 67.19%] [G loss: 0.662728]\n",
      "epoch:16 step:15569 [D loss: 0.523250, acc.: 78.91%] [G loss: 0.819075]\n",
      "epoch:16 step:15570 [D loss: 0.577827, acc.: 67.19%] [G loss: 0.668761]\n",
      "epoch:16 step:15571 [D loss: 0.602469, acc.: 65.62%] [G loss: 0.719762]\n",
      "epoch:16 step:15572 [D loss: 0.551956, acc.: 70.31%] [G loss: 0.680858]\n",
      "epoch:16 step:15573 [D loss: 0.556727, acc.: 70.31%] [G loss: 0.513596]\n",
      "epoch:16 step:15574 [D loss: 0.458663, acc.: 80.47%] [G loss: 0.665986]\n",
      "epoch:16 step:15575 [D loss: 0.582057, acc.: 66.41%] [G loss: 0.628461]\n",
      "epoch:16 step:15576 [D loss: 0.631912, acc.: 64.06%] [G loss: 0.628047]\n",
      "epoch:16 step:15577 [D loss: 0.585299, acc.: 66.41%] [G loss: 0.520813]\n",
      "epoch:16 step:15578 [D loss: 0.544678, acc.: 74.22%] [G loss: 0.560152]\n",
      "epoch:16 step:15579 [D loss: 0.534721, acc.: 70.31%] [G loss: 0.565577]\n",
      "epoch:16 step:15580 [D loss: 0.565576, acc.: 64.84%] [G loss: 0.601913]\n",
      "epoch:16 step:15581 [D loss: 0.543897, acc.: 66.41%] [G loss: 0.733024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15582 [D loss: 0.564035, acc.: 67.97%] [G loss: 0.742016]\n",
      "epoch:16 step:15583 [D loss: 0.597062, acc.: 63.28%] [G loss: 0.460348]\n",
      "epoch:16 step:15584 [D loss: 0.492291, acc.: 78.91%] [G loss: 0.676645]\n",
      "epoch:16 step:15585 [D loss: 0.500393, acc.: 77.34%] [G loss: 0.637273]\n",
      "epoch:16 step:15586 [D loss: 0.605059, acc.: 63.28%] [G loss: 0.658090]\n",
      "epoch:16 step:15587 [D loss: 0.524448, acc.: 73.44%] [G loss: 0.661965]\n",
      "epoch:16 step:15588 [D loss: 0.531928, acc.: 72.66%] [G loss: 0.713967]\n",
      "epoch:16 step:15589 [D loss: 0.544440, acc.: 72.66%] [G loss: 0.600528]\n",
      "epoch:16 step:15590 [D loss: 0.514569, acc.: 72.66%] [G loss: 0.684853]\n",
      "epoch:16 step:15591 [D loss: 0.524348, acc.: 73.44%] [G loss: 0.722043]\n",
      "epoch:16 step:15592 [D loss: 0.615531, acc.: 63.28%] [G loss: 0.575270]\n",
      "epoch:16 step:15593 [D loss: 0.465591, acc.: 77.34%] [G loss: 0.757108]\n",
      "epoch:16 step:15594 [D loss: 0.527113, acc.: 74.22%] [G loss: 0.780500]\n",
      "epoch:16 step:15595 [D loss: 0.502367, acc.: 73.44%] [G loss: 0.697285]\n",
      "epoch:16 step:15596 [D loss: 0.633396, acc.: 64.06%] [G loss: 0.538693]\n",
      "epoch:16 step:15597 [D loss: 0.471466, acc.: 77.34%] [G loss: 0.704807]\n",
      "epoch:16 step:15598 [D loss: 0.582752, acc.: 67.97%] [G loss: 0.614110]\n",
      "epoch:16 step:15599 [D loss: 0.521370, acc.: 71.09%] [G loss: 0.613145]\n",
      "epoch:16 step:15600 [D loss: 0.569372, acc.: 65.62%] [G loss: 0.589639]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.433350\n",
      "FID: 52.197292\n",
      "0 = 13.055464172744713\n",
      "1 = 0.09376589322476894\n",
      "2 = 0.881600022315979\n",
      "3 = 0.8320000171661377\n",
      "4 = 0.9312000274658203\n",
      "5 = 0.9236234426498413\n",
      "6 = 0.8320000171661377\n",
      "7 = 8.392936633539192\n",
      "8 = 0.1578055200827243\n",
      "9 = 0.708299994468689\n",
      "10 = 0.699400007724762\n",
      "11 = 0.717199981212616\n",
      "12 = 0.7120749354362488\n",
      "13 = 0.699400007724762\n",
      "14 = 6.433374404907227\n",
      "15 = 6.962758541107178\n",
      "16 = 0.39236751198768616\n",
      "17 = 6.433350086212158\n",
      "18 = 52.19729232788086\n",
      "epoch:16 step:15601 [D loss: 0.510133, acc.: 75.00%] [G loss: 0.627312]\n",
      "epoch:16 step:15602 [D loss: 0.606678, acc.: 60.94%] [G loss: 0.557418]\n",
      "epoch:16 step:15603 [D loss: 0.490984, acc.: 76.56%] [G loss: 0.545012]\n",
      "epoch:16 step:15604 [D loss: 0.540900, acc.: 66.41%] [G loss: 0.590837]\n",
      "epoch:16 step:15605 [D loss: 0.475422, acc.: 75.00%] [G loss: 0.628694]\n",
      "epoch:16 step:15606 [D loss: 0.586325, acc.: 64.84%] [G loss: 0.624448]\n",
      "epoch:16 step:15607 [D loss: 0.581703, acc.: 67.97%] [G loss: 0.615477]\n",
      "epoch:16 step:15608 [D loss: 0.595536, acc.: 64.84%] [G loss: 0.668423]\n",
      "epoch:16 step:15609 [D loss: 0.522277, acc.: 78.12%] [G loss: 0.664995]\n",
      "epoch:16 step:15610 [D loss: 0.588549, acc.: 65.62%] [G loss: 0.637022]\n",
      "epoch:16 step:15611 [D loss: 0.521031, acc.: 68.75%] [G loss: 0.713286]\n",
      "epoch:16 step:15612 [D loss: 0.502886, acc.: 71.88%] [G loss: 0.769773]\n",
      "epoch:16 step:15613 [D loss: 0.628107, acc.: 63.28%] [G loss: 0.440840]\n",
      "epoch:16 step:15614 [D loss: 0.576925, acc.: 71.09%] [G loss: 0.567824]\n",
      "epoch:16 step:15615 [D loss: 0.552276, acc.: 70.31%] [G loss: 0.711114]\n",
      "epoch:16 step:15616 [D loss: 0.482608, acc.: 78.12%] [G loss: 0.797573]\n",
      "epoch:16 step:15617 [D loss: 0.578471, acc.: 65.62%] [G loss: 0.659044]\n",
      "epoch:16 step:15618 [D loss: 0.551311, acc.: 64.84%] [G loss: 0.589640]\n",
      "epoch:16 step:15619 [D loss: 0.583233, acc.: 65.62%] [G loss: 0.570347]\n",
      "epoch:16 step:15620 [D loss: 0.552227, acc.: 69.53%] [G loss: 0.697556]\n",
      "epoch:16 step:15621 [D loss: 0.564639, acc.: 70.31%] [G loss: 0.622257]\n",
      "epoch:16 step:15622 [D loss: 0.527002, acc.: 75.00%] [G loss: 0.608038]\n",
      "epoch:16 step:15623 [D loss: 0.483773, acc.: 78.12%] [G loss: 0.658426]\n",
      "epoch:16 step:15624 [D loss: 0.492310, acc.: 74.22%] [G loss: 0.594571]\n",
      "epoch:16 step:15625 [D loss: 0.490404, acc.: 77.34%] [G loss: 0.722259]\n",
      "epoch:16 step:15626 [D loss: 0.465456, acc.: 80.47%] [G loss: 0.914113]\n",
      "epoch:16 step:15627 [D loss: 0.483185, acc.: 75.00%] [G loss: 0.882083]\n",
      "epoch:16 step:15628 [D loss: 0.616060, acc.: 65.62%] [G loss: 0.528129]\n",
      "epoch:16 step:15629 [D loss: 0.549210, acc.: 71.09%] [G loss: 0.624528]\n",
      "epoch:16 step:15630 [D loss: 0.528941, acc.: 74.22%] [G loss: 0.461645]\n",
      "epoch:16 step:15631 [D loss: 0.555091, acc.: 65.62%] [G loss: 0.535379]\n",
      "epoch:16 step:15632 [D loss: 0.576353, acc.: 66.41%] [G loss: 0.584013]\n",
      "epoch:16 step:15633 [D loss: 0.486227, acc.: 75.00%] [G loss: 0.610267]\n",
      "epoch:16 step:15634 [D loss: 0.496453, acc.: 76.56%] [G loss: 0.807657]\n",
      "epoch:16 step:15635 [D loss: 0.564354, acc.: 67.97%] [G loss: 0.626270]\n",
      "epoch:16 step:15636 [D loss: 0.558317, acc.: 67.97%] [G loss: 0.622015]\n",
      "epoch:16 step:15637 [D loss: 0.512495, acc.: 73.44%] [G loss: 0.697387]\n",
      "epoch:16 step:15638 [D loss: 0.552079, acc.: 72.66%] [G loss: 0.650737]\n",
      "epoch:16 step:15639 [D loss: 0.479023, acc.: 77.34%] [G loss: 0.698489]\n",
      "epoch:16 step:15640 [D loss: 0.449823, acc.: 74.22%] [G loss: 1.122282]\n",
      "epoch:16 step:15641 [D loss: 0.523399, acc.: 73.44%] [G loss: 1.093883]\n",
      "epoch:16 step:15642 [D loss: 0.551183, acc.: 71.09%] [G loss: 0.833053]\n",
      "epoch:16 step:15643 [D loss: 0.534847, acc.: 71.88%] [G loss: 0.748147]\n",
      "epoch:16 step:15644 [D loss: 0.624331, acc.: 65.62%] [G loss: 0.595836]\n",
      "epoch:16 step:15645 [D loss: 0.587365, acc.: 70.31%] [G loss: 0.614657]\n",
      "epoch:16 step:15646 [D loss: 0.477619, acc.: 71.88%] [G loss: 0.693965]\n",
      "epoch:16 step:15647 [D loss: 0.577282, acc.: 70.31%] [G loss: 0.561584]\n",
      "epoch:16 step:15648 [D loss: 0.524916, acc.: 71.88%] [G loss: 0.530440]\n",
      "epoch:16 step:15649 [D loss: 0.536950, acc.: 75.78%] [G loss: 0.536359]\n",
      "epoch:16 step:15650 [D loss: 0.545278, acc.: 70.31%] [G loss: 0.625553]\n",
      "epoch:16 step:15651 [D loss: 0.529501, acc.: 71.09%] [G loss: 0.584405]\n",
      "epoch:16 step:15652 [D loss: 0.527790, acc.: 68.75%] [G loss: 0.714759]\n",
      "epoch:16 step:15653 [D loss: 0.554449, acc.: 69.53%] [G loss: 0.635787]\n",
      "epoch:16 step:15654 [D loss: 0.528538, acc.: 71.88%] [G loss: 0.824459]\n",
      "epoch:16 step:15655 [D loss: 0.583201, acc.: 71.09%] [G loss: 0.591607]\n",
      "epoch:16 step:15656 [D loss: 0.575349, acc.: 64.84%] [G loss: 0.487816]\n",
      "epoch:16 step:15657 [D loss: 0.550793, acc.: 71.88%] [G loss: 0.648873]\n",
      "epoch:16 step:15658 [D loss: 0.579972, acc.: 71.09%] [G loss: 0.485768]\n",
      "epoch:16 step:15659 [D loss: 0.537691, acc.: 69.53%] [G loss: 0.645890]\n",
      "epoch:16 step:15660 [D loss: 0.549146, acc.: 71.88%] [G loss: 0.566763]\n",
      "epoch:16 step:15661 [D loss: 0.513118, acc.: 75.00%] [G loss: 0.652285]\n",
      "epoch:16 step:15662 [D loss: 0.594638, acc.: 62.50%] [G loss: 0.565436]\n",
      "epoch:16 step:15663 [D loss: 0.572726, acc.: 67.19%] [G loss: 0.595609]\n",
      "epoch:16 step:15664 [D loss: 0.527219, acc.: 74.22%] [G loss: 0.608266]\n",
      "epoch:16 step:15665 [D loss: 0.601511, acc.: 64.06%] [G loss: 0.555572]\n",
      "epoch:16 step:15666 [D loss: 0.561147, acc.: 71.88%] [G loss: 0.550110]\n",
      "epoch:16 step:15667 [D loss: 0.622718, acc.: 62.50%] [G loss: 0.499099]\n",
      "epoch:16 step:15668 [D loss: 0.571495, acc.: 67.19%] [G loss: 0.623876]\n",
      "epoch:16 step:15669 [D loss: 0.483870, acc.: 80.47%] [G loss: 0.623717]\n",
      "epoch:16 step:15670 [D loss: 0.593364, acc.: 63.28%] [G loss: 0.665311]\n",
      "epoch:16 step:15671 [D loss: 0.531973, acc.: 73.44%] [G loss: 0.709720]\n",
      "epoch:16 step:15672 [D loss: 0.548039, acc.: 75.78%] [G loss: 0.573147]\n",
      "epoch:16 step:15673 [D loss: 0.540617, acc.: 69.53%] [G loss: 0.722447]\n",
      "epoch:16 step:15674 [D loss: 0.547308, acc.: 71.09%] [G loss: 0.549283]\n",
      "epoch:16 step:15675 [D loss: 0.558010, acc.: 69.53%] [G loss: 0.558298]\n",
      "epoch:16 step:15676 [D loss: 0.547598, acc.: 73.44%] [G loss: 0.468375]\n",
      "epoch:16 step:15677 [D loss: 0.505776, acc.: 75.00%] [G loss: 0.607517]\n",
      "epoch:16 step:15678 [D loss: 0.562654, acc.: 69.53%] [G loss: 0.654572]\n",
      "epoch:16 step:15679 [D loss: 0.576117, acc.: 66.41%] [G loss: 0.675918]\n",
      "epoch:16 step:15680 [D loss: 0.530312, acc.: 73.44%] [G loss: 0.569519]\n",
      "epoch:16 step:15681 [D loss: 0.575538, acc.: 65.62%] [G loss: 0.569688]\n",
      "epoch:16 step:15682 [D loss: 0.482406, acc.: 75.78%] [G loss: 0.758862]\n",
      "epoch:16 step:15683 [D loss: 0.510386, acc.: 75.78%] [G loss: 0.640921]\n",
      "epoch:16 step:15684 [D loss: 0.546158, acc.: 70.31%] [G loss: 0.784865]\n",
      "epoch:16 step:15685 [D loss: 0.477579, acc.: 77.34%] [G loss: 0.717677]\n",
      "epoch:16 step:15686 [D loss: 0.499930, acc.: 75.78%] [G loss: 0.651902]\n",
      "epoch:16 step:15687 [D loss: 0.509364, acc.: 77.34%] [G loss: 0.688650]\n",
      "epoch:16 step:15688 [D loss: 0.635293, acc.: 58.59%] [G loss: 0.431087]\n",
      "epoch:16 step:15689 [D loss: 0.558995, acc.: 65.62%] [G loss: 0.479845]\n",
      "epoch:16 step:15690 [D loss: 0.555230, acc.: 67.97%] [G loss: 0.546887]\n",
      "epoch:16 step:15691 [D loss: 0.523381, acc.: 71.88%] [G loss: 0.609807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15692 [D loss: 0.558648, acc.: 72.66%] [G loss: 0.947538]\n",
      "epoch:16 step:15693 [D loss: 0.512470, acc.: 75.78%] [G loss: 0.765500]\n",
      "epoch:16 step:15694 [D loss: 0.644894, acc.: 59.38%] [G loss: 0.601326]\n",
      "epoch:16 step:15695 [D loss: 0.583991, acc.: 71.09%] [G loss: 0.498651]\n",
      "epoch:16 step:15696 [D loss: 0.589363, acc.: 65.62%] [G loss: 0.571531]\n",
      "epoch:16 step:15697 [D loss: 0.505156, acc.: 71.88%] [G loss: 0.554496]\n",
      "epoch:16 step:15698 [D loss: 0.585385, acc.: 67.19%] [G loss: 0.646260]\n",
      "epoch:16 step:15699 [D loss: 0.544877, acc.: 72.66%] [G loss: 0.631707]\n",
      "epoch:16 step:15700 [D loss: 0.499026, acc.: 75.00%] [G loss: 0.716589]\n",
      "epoch:16 step:15701 [D loss: 0.529779, acc.: 67.97%] [G loss: 0.757307]\n",
      "epoch:16 step:15702 [D loss: 0.620360, acc.: 56.25%] [G loss: 0.467164]\n",
      "epoch:16 step:15703 [D loss: 0.526311, acc.: 72.66%] [G loss: 0.789371]\n",
      "epoch:16 step:15704 [D loss: 0.568511, acc.: 71.88%] [G loss: 0.667264]\n",
      "epoch:16 step:15705 [D loss: 0.592155, acc.: 61.72%] [G loss: 0.506482]\n",
      "epoch:16 step:15706 [D loss: 0.558704, acc.: 69.53%] [G loss: 0.493839]\n",
      "epoch:16 step:15707 [D loss: 0.535220, acc.: 72.66%] [G loss: 0.615248]\n",
      "epoch:16 step:15708 [D loss: 0.591715, acc.: 67.97%] [G loss: 0.670500]\n",
      "epoch:16 step:15709 [D loss: 0.568044, acc.: 69.53%] [G loss: 0.550197]\n",
      "epoch:16 step:15710 [D loss: 0.562818, acc.: 75.00%] [G loss: 0.474515]\n",
      "epoch:16 step:15711 [D loss: 0.515147, acc.: 71.09%] [G loss: 0.616552]\n",
      "epoch:16 step:15712 [D loss: 0.609935, acc.: 64.84%] [G loss: 0.726628]\n",
      "epoch:16 step:15713 [D loss: 0.564215, acc.: 67.19%] [G loss: 0.568853]\n",
      "epoch:16 step:15714 [D loss: 0.562803, acc.: 71.88%] [G loss: 0.527993]\n",
      "epoch:16 step:15715 [D loss: 0.548094, acc.: 72.66%] [G loss: 0.489516]\n",
      "epoch:16 step:15716 [D loss: 0.519735, acc.: 71.88%] [G loss: 0.548208]\n",
      "epoch:16 step:15717 [D loss: 0.536505, acc.: 77.34%] [G loss: 0.637375]\n",
      "epoch:16 step:15718 [D loss: 0.516886, acc.: 76.56%] [G loss: 0.636644]\n",
      "epoch:16 step:15719 [D loss: 0.534360, acc.: 70.31%] [G loss: 0.559944]\n",
      "epoch:16 step:15720 [D loss: 0.556450, acc.: 68.75%] [G loss: 0.547565]\n",
      "epoch:16 step:15721 [D loss: 0.607937, acc.: 62.50%] [G loss: 0.577617]\n",
      "epoch:16 step:15722 [D loss: 0.533559, acc.: 74.22%] [G loss: 0.542103]\n",
      "epoch:16 step:15723 [D loss: 0.604421, acc.: 60.94%] [G loss: 0.641698]\n",
      "epoch:16 step:15724 [D loss: 0.577925, acc.: 67.19%] [G loss: 0.593432]\n",
      "epoch:16 step:15725 [D loss: 0.567518, acc.: 68.75%] [G loss: 0.501379]\n",
      "epoch:16 step:15726 [D loss: 0.546422, acc.: 69.53%] [G loss: 0.519734]\n",
      "epoch:16 step:15727 [D loss: 0.545827, acc.: 71.09%] [G loss: 0.600208]\n",
      "epoch:16 step:15728 [D loss: 0.485132, acc.: 75.00%] [G loss: 0.576470]\n",
      "epoch:16 step:15729 [D loss: 0.529173, acc.: 71.09%] [G loss: 0.623678]\n",
      "epoch:16 step:15730 [D loss: 0.514879, acc.: 71.88%] [G loss: 0.559563]\n",
      "epoch:16 step:15731 [D loss: 0.567668, acc.: 68.75%] [G loss: 0.542305]\n",
      "epoch:16 step:15732 [D loss: 0.604335, acc.: 62.50%] [G loss: 0.417094]\n",
      "epoch:16 step:15733 [D loss: 0.542188, acc.: 69.53%] [G loss: 0.431096]\n",
      "epoch:16 step:15734 [D loss: 0.507636, acc.: 75.00%] [G loss: 0.594911]\n",
      "epoch:16 step:15735 [D loss: 0.478892, acc.: 76.56%] [G loss: 0.589717]\n",
      "epoch:16 step:15736 [D loss: 0.530338, acc.: 74.22%] [G loss: 0.672191]\n",
      "epoch:16 step:15737 [D loss: 0.552212, acc.: 66.41%] [G loss: 0.571167]\n",
      "epoch:16 step:15738 [D loss: 0.492464, acc.: 73.44%] [G loss: 0.637120]\n",
      "epoch:16 step:15739 [D loss: 0.437625, acc.: 81.25%] [G loss: 0.680611]\n",
      "epoch:16 step:15740 [D loss: 0.513378, acc.: 71.09%] [G loss: 0.608735]\n",
      "epoch:16 step:15741 [D loss: 0.530871, acc.: 73.44%] [G loss: 0.595665]\n",
      "epoch:16 step:15742 [D loss: 0.538725, acc.: 67.19%] [G loss: 0.720847]\n",
      "epoch:16 step:15743 [D loss: 0.468806, acc.: 74.22%] [G loss: 0.896290]\n",
      "epoch:16 step:15744 [D loss: 0.593527, acc.: 68.75%] [G loss: 0.608928]\n",
      "epoch:16 step:15745 [D loss: 0.526714, acc.: 76.56%] [G loss: 0.667006]\n",
      "epoch:16 step:15746 [D loss: 0.530154, acc.: 70.31%] [G loss: 0.632685]\n",
      "epoch:16 step:15747 [D loss: 0.556030, acc.: 69.53%] [G loss: 0.879465]\n",
      "epoch:16 step:15748 [D loss: 0.562948, acc.: 69.53%] [G loss: 0.601806]\n",
      "epoch:16 step:15749 [D loss: 0.518833, acc.: 72.66%] [G loss: 0.595667]\n",
      "epoch:16 step:15750 [D loss: 0.564063, acc.: 71.09%] [G loss: 0.641673]\n",
      "epoch:16 step:15751 [D loss: 0.568936, acc.: 64.84%] [G loss: 0.584558]\n",
      "epoch:16 step:15752 [D loss: 0.520194, acc.: 73.44%] [G loss: 0.521692]\n",
      "epoch:16 step:15753 [D loss: 0.637351, acc.: 64.84%] [G loss: 0.470241]\n",
      "epoch:16 step:15754 [D loss: 0.563994, acc.: 67.97%] [G loss: 0.579601]\n",
      "epoch:16 step:15755 [D loss: 0.634354, acc.: 60.94%] [G loss: 0.529276]\n",
      "epoch:16 step:15756 [D loss: 0.547429, acc.: 71.09%] [G loss: 0.639196]\n",
      "epoch:16 step:15757 [D loss: 0.630400, acc.: 60.94%] [G loss: 0.558680]\n",
      "epoch:16 step:15758 [D loss: 0.692498, acc.: 59.38%] [G loss: 0.478480]\n",
      "epoch:16 step:15759 [D loss: 0.491900, acc.: 77.34%] [G loss: 0.623335]\n",
      "epoch:16 step:15760 [D loss: 0.595047, acc.: 66.41%] [G loss: 0.697564]\n",
      "epoch:16 step:15761 [D loss: 0.496095, acc.: 74.22%] [G loss: 1.000993]\n",
      "epoch:16 step:15762 [D loss: 0.508533, acc.: 76.56%] [G loss: 0.637567]\n",
      "epoch:16 step:15763 [D loss: 0.496447, acc.: 78.91%] [G loss: 0.897345]\n",
      "epoch:16 step:15764 [D loss: 0.545992, acc.: 66.41%] [G loss: 0.576073]\n",
      "epoch:16 step:15765 [D loss: 0.497036, acc.: 76.56%] [G loss: 0.708480]\n",
      "epoch:16 step:15766 [D loss: 0.546363, acc.: 75.00%] [G loss: 0.643335]\n",
      "epoch:16 step:15767 [D loss: 0.536355, acc.: 71.09%] [G loss: 0.695531]\n",
      "epoch:16 step:15768 [D loss: 0.569629, acc.: 69.53%] [G loss: 0.671055]\n",
      "epoch:16 step:15769 [D loss: 0.562255, acc.: 68.75%] [G loss: 0.542670]\n",
      "epoch:16 step:15770 [D loss: 0.535811, acc.: 71.88%] [G loss: 0.622806]\n",
      "epoch:16 step:15771 [D loss: 0.549110, acc.: 71.88%] [G loss: 0.654458]\n",
      "epoch:16 step:15772 [D loss: 0.529751, acc.: 65.62%] [G loss: 0.749696]\n",
      "epoch:16 step:15773 [D loss: 0.541611, acc.: 70.31%] [G loss: 0.683638]\n",
      "epoch:16 step:15774 [D loss: 0.507321, acc.: 70.31%] [G loss: 0.852417]\n",
      "epoch:16 step:15775 [D loss: 0.585134, acc.: 70.31%] [G loss: 0.579569]\n",
      "epoch:16 step:15776 [D loss: 0.596414, acc.: 68.75%] [G loss: 0.543489]\n",
      "epoch:16 step:15777 [D loss: 0.534396, acc.: 70.31%] [G loss: 0.495327]\n",
      "epoch:16 step:15778 [D loss: 0.553298, acc.: 67.97%] [G loss: 0.661773]\n",
      "epoch:16 step:15779 [D loss: 0.559921, acc.: 70.31%] [G loss: 0.738997]\n",
      "epoch:16 step:15780 [D loss: 0.626577, acc.: 65.62%] [G loss: 0.580306]\n",
      "epoch:16 step:15781 [D loss: 0.512084, acc.: 71.88%] [G loss: 0.544509]\n",
      "epoch:16 step:15782 [D loss: 0.552226, acc.: 70.31%] [G loss: 0.578014]\n",
      "epoch:16 step:15783 [D loss: 0.568364, acc.: 71.09%] [G loss: 0.553616]\n",
      "epoch:16 step:15784 [D loss: 0.461367, acc.: 76.56%] [G loss: 0.722569]\n",
      "epoch:16 step:15785 [D loss: 0.623766, acc.: 64.84%] [G loss: 0.530926]\n",
      "epoch:16 step:15786 [D loss: 0.645724, acc.: 58.59%] [G loss: 0.457744]\n",
      "epoch:16 step:15787 [D loss: 0.583615, acc.: 62.50%] [G loss: 0.595764]\n",
      "epoch:16 step:15788 [D loss: 0.498389, acc.: 75.78%] [G loss: 0.554508]\n",
      "epoch:16 step:15789 [D loss: 0.571284, acc.: 69.53%] [G loss: 0.635551]\n",
      "epoch:16 step:15790 [D loss: 0.540519, acc.: 72.66%] [G loss: 0.587384]\n",
      "epoch:16 step:15791 [D loss: 0.597635, acc.: 68.75%] [G loss: 0.540424]\n",
      "epoch:16 step:15792 [D loss: 0.588406, acc.: 66.41%] [G loss: 0.616865]\n",
      "epoch:16 step:15793 [D loss: 0.493078, acc.: 75.78%] [G loss: 0.696204]\n",
      "epoch:16 step:15794 [D loss: 0.443181, acc.: 79.69%] [G loss: 0.844885]\n",
      "epoch:16 step:15795 [D loss: 0.496173, acc.: 76.56%] [G loss: 0.697092]\n",
      "epoch:16 step:15796 [D loss: 0.616402, acc.: 63.28%] [G loss: 0.544989]\n",
      "epoch:16 step:15797 [D loss: 0.574160, acc.: 67.97%] [G loss: 0.562973]\n",
      "epoch:16 step:15798 [D loss: 0.546483, acc.: 67.19%] [G loss: 0.547152]\n",
      "epoch:16 step:15799 [D loss: 0.566459, acc.: 64.84%] [G loss: 0.571339]\n",
      "epoch:16 step:15800 [D loss: 0.579664, acc.: 65.62%] [G loss: 0.583206]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.644748\n",
      "FID: 44.811672\n",
      "0 = 12.921423409748062\n",
      "1 = 0.09353613301131782\n",
      "2 = 0.8812999725341797\n",
      "3 = 0.8235999941825867\n",
      "4 = 0.9390000104904175\n",
      "5 = 0.9310422539710999\n",
      "6 = 0.8235999941825867\n",
      "7 = 7.9922920241356055\n",
      "8 = 0.13836851653061227\n",
      "9 = 0.6972000002861023\n",
      "10 = 0.6855999827384949\n",
      "11 = 0.7088000178337097\n",
      "12 = 0.7018836736679077\n",
      "13 = 0.6855999827384949\n",
      "14 = 6.644771575927734\n",
      "15 = 7.071249961853027\n",
      "16 = 0.38217946887016296\n",
      "17 = 6.644747734069824\n",
      "18 = 44.81167221069336\n",
      "epoch:16 step:15801 [D loss: 0.558701, acc.: 69.53%] [G loss: 0.552564]\n",
      "epoch:16 step:15802 [D loss: 0.500748, acc.: 75.78%] [G loss: 0.583108]\n",
      "epoch:16 step:15803 [D loss: 0.550916, acc.: 71.09%] [G loss: 0.622378]\n",
      "epoch:16 step:15804 [D loss: 0.601283, acc.: 64.84%] [G loss: 0.501207]\n",
      "epoch:16 step:15805 [D loss: 0.569569, acc.: 64.84%] [G loss: 0.508162]\n",
      "epoch:16 step:15806 [D loss: 0.490495, acc.: 78.12%] [G loss: 0.911879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15807 [D loss: 0.529202, acc.: 75.00%] [G loss: 0.676543]\n",
      "epoch:16 step:15808 [D loss: 0.602175, acc.: 67.19%] [G loss: 0.603307]\n",
      "epoch:16 step:15809 [D loss: 0.572882, acc.: 70.31%] [G loss: 0.670538]\n",
      "epoch:16 step:15810 [D loss: 0.611912, acc.: 61.72%] [G loss: 0.492048]\n",
      "epoch:16 step:15811 [D loss: 0.591000, acc.: 67.19%] [G loss: 0.520533]\n",
      "epoch:16 step:15812 [D loss: 0.661978, acc.: 64.84%] [G loss: 0.505457]\n",
      "epoch:16 step:15813 [D loss: 0.546675, acc.: 70.31%] [G loss: 0.604539]\n",
      "epoch:16 step:15814 [D loss: 0.548950, acc.: 65.62%] [G loss: 0.539563]\n",
      "epoch:16 step:15815 [D loss: 0.499049, acc.: 75.78%] [G loss: 0.667271]\n",
      "epoch:16 step:15816 [D loss: 0.589608, acc.: 67.19%] [G loss: 0.606211]\n",
      "epoch:16 step:15817 [D loss: 0.510669, acc.: 73.44%] [G loss: 0.652125]\n",
      "epoch:16 step:15818 [D loss: 0.531534, acc.: 68.75%] [G loss: 0.563669]\n",
      "epoch:16 step:15819 [D loss: 0.576175, acc.: 69.53%] [G loss: 0.604595]\n",
      "epoch:16 step:15820 [D loss: 0.654981, acc.: 61.72%] [G loss: 0.480345]\n",
      "epoch:16 step:15821 [D loss: 0.530510, acc.: 70.31%] [G loss: 0.722789]\n",
      "epoch:16 step:15822 [D loss: 0.509265, acc.: 74.22%] [G loss: 0.612328]\n",
      "epoch:16 step:15823 [D loss: 0.570880, acc.: 70.31%] [G loss: 0.554491]\n",
      "epoch:16 step:15824 [D loss: 0.608933, acc.: 69.53%] [G loss: 0.656498]\n",
      "epoch:16 step:15825 [D loss: 0.528979, acc.: 71.88%] [G loss: 0.500954]\n",
      "epoch:16 step:15826 [D loss: 0.521841, acc.: 75.00%] [G loss: 0.578820]\n",
      "epoch:16 step:15827 [D loss: 0.521414, acc.: 75.78%] [G loss: 0.536366]\n",
      "epoch:16 step:15828 [D loss: 0.559953, acc.: 67.19%] [G loss: 0.533366]\n",
      "epoch:16 step:15829 [D loss: 0.505405, acc.: 76.56%] [G loss: 0.538309]\n",
      "epoch:16 step:15830 [D loss: 0.531802, acc.: 71.09%] [G loss: 0.666697]\n",
      "epoch:16 step:15831 [D loss: 0.621191, acc.: 65.62%] [G loss: 0.499865]\n",
      "epoch:16 step:15832 [D loss: 0.603835, acc.: 67.19%] [G loss: 0.507337]\n",
      "epoch:16 step:15833 [D loss: 0.545511, acc.: 67.97%] [G loss: 0.456781]\n",
      "epoch:16 step:15834 [D loss: 0.513319, acc.: 71.88%] [G loss: 0.615019]\n",
      "epoch:16 step:15835 [D loss: 0.514909, acc.: 73.44%] [G loss: 0.721502]\n",
      "epoch:16 step:15836 [D loss: 0.528916, acc.: 71.88%] [G loss: 0.671048]\n",
      "epoch:16 step:15837 [D loss: 0.545375, acc.: 67.97%] [G loss: 0.627509]\n",
      "epoch:16 step:15838 [D loss: 0.585023, acc.: 64.06%] [G loss: 0.491372]\n",
      "epoch:16 step:15839 [D loss: 0.608359, acc.: 64.06%] [G loss: 0.422225]\n",
      "epoch:16 step:15840 [D loss: 0.585125, acc.: 64.84%] [G loss: 0.465972]\n",
      "epoch:16 step:15841 [D loss: 0.578742, acc.: 64.06%] [G loss: 0.425825]\n",
      "epoch:16 step:15842 [D loss: 0.566973, acc.: 64.84%] [G loss: 0.521056]\n",
      "epoch:16 step:15843 [D loss: 0.579616, acc.: 65.62%] [G loss: 0.596341]\n",
      "epoch:16 step:15844 [D loss: 0.548639, acc.: 71.88%] [G loss: 0.531848]\n",
      "epoch:16 step:15845 [D loss: 0.519849, acc.: 73.44%] [G loss: 0.608585]\n",
      "epoch:16 step:15846 [D loss: 0.506056, acc.: 71.09%] [G loss: 0.691426]\n",
      "epoch:16 step:15847 [D loss: 0.590753, acc.: 67.19%] [G loss: 0.727729]\n",
      "epoch:16 step:15848 [D loss: 0.632741, acc.: 64.06%] [G loss: 0.624680]\n",
      "epoch:16 step:15849 [D loss: 0.474764, acc.: 76.56%] [G loss: 0.854646]\n",
      "epoch:16 step:15850 [D loss: 0.614208, acc.: 63.28%] [G loss: 0.528347]\n",
      "epoch:16 step:15851 [D loss: 0.540981, acc.: 71.88%] [G loss: 0.466338]\n",
      "epoch:16 step:15852 [D loss: 0.482587, acc.: 71.09%] [G loss: 0.784368]\n",
      "epoch:16 step:15853 [D loss: 0.591077, acc.: 65.62%] [G loss: 0.559549]\n",
      "epoch:16 step:15854 [D loss: 0.637732, acc.: 62.50%] [G loss: 0.466494]\n",
      "epoch:16 step:15855 [D loss: 0.610337, acc.: 64.84%] [G loss: 0.386987]\n",
      "epoch:16 step:15856 [D loss: 0.542759, acc.: 71.09%] [G loss: 0.529102]\n",
      "epoch:16 step:15857 [D loss: 0.565467, acc.: 67.97%] [G loss: 0.448894]\n",
      "epoch:16 step:15858 [D loss: 0.563550, acc.: 64.84%] [G loss: 0.566309]\n",
      "epoch:16 step:15859 [D loss: 0.646414, acc.: 57.81%] [G loss: 0.462606]\n",
      "epoch:16 step:15860 [D loss: 0.532286, acc.: 74.22%] [G loss: 0.557674]\n",
      "epoch:16 step:15861 [D loss: 0.562683, acc.: 71.09%] [G loss: 0.559720]\n",
      "epoch:16 step:15862 [D loss: 0.499995, acc.: 73.44%] [G loss: 0.556633]\n",
      "epoch:16 step:15863 [D loss: 0.465491, acc.: 75.78%] [G loss: 0.724404]\n",
      "epoch:16 step:15864 [D loss: 0.538278, acc.: 69.53%] [G loss: 0.798241]\n",
      "epoch:16 step:15865 [D loss: 0.613081, acc.: 65.62%] [G loss: 0.615297]\n",
      "epoch:16 step:15866 [D loss: 0.542710, acc.: 68.75%] [G loss: 0.536450]\n",
      "epoch:16 step:15867 [D loss: 0.489750, acc.: 75.00%] [G loss: 0.545306]\n",
      "epoch:16 step:15868 [D loss: 0.584351, acc.: 70.31%] [G loss: 0.610518]\n",
      "epoch:16 step:15869 [D loss: 0.634384, acc.: 60.94%] [G loss: 0.561772]\n",
      "epoch:16 step:15870 [D loss: 0.535315, acc.: 76.56%] [G loss: 0.706736]\n",
      "epoch:16 step:15871 [D loss: 0.600608, acc.: 65.62%] [G loss: 0.468547]\n",
      "epoch:16 step:15872 [D loss: 0.665820, acc.: 60.16%] [G loss: 0.434415]\n",
      "epoch:16 step:15873 [D loss: 0.550548, acc.: 72.66%] [G loss: 0.511304]\n",
      "epoch:16 step:15874 [D loss: 0.540460, acc.: 67.19%] [G loss: 0.546521]\n",
      "epoch:16 step:15875 [D loss: 0.619429, acc.: 64.84%] [G loss: 0.421688]\n",
      "epoch:16 step:15876 [D loss: 0.520561, acc.: 73.44%] [G loss: 0.521362]\n",
      "epoch:16 step:15877 [D loss: 0.538582, acc.: 72.66%] [G loss: 0.545647]\n",
      "epoch:16 step:15878 [D loss: 0.556754, acc.: 69.53%] [G loss: 0.612101]\n",
      "epoch:16 step:15879 [D loss: 0.545174, acc.: 68.75%] [G loss: 0.633059]\n",
      "epoch:16 step:15880 [D loss: 0.540866, acc.: 68.75%] [G loss: 0.689618]\n",
      "epoch:16 step:15881 [D loss: 0.591292, acc.: 64.06%] [G loss: 0.605274]\n",
      "epoch:16 step:15882 [D loss: 0.474399, acc.: 77.34%] [G loss: 0.661893]\n",
      "epoch:16 step:15883 [D loss: 0.608454, acc.: 63.28%] [G loss: 0.493100]\n",
      "epoch:16 step:15884 [D loss: 0.604407, acc.: 65.62%] [G loss: 0.563287]\n",
      "epoch:16 step:15885 [D loss: 0.586528, acc.: 64.06%] [G loss: 0.589647]\n",
      "epoch:16 step:15886 [D loss: 0.481538, acc.: 78.12%] [G loss: 0.619257]\n",
      "epoch:16 step:15887 [D loss: 0.518105, acc.: 71.88%] [G loss: 0.788054]\n",
      "epoch:16 step:15888 [D loss: 0.510108, acc.: 76.56%] [G loss: 0.773845]\n",
      "epoch:16 step:15889 [D loss: 0.551425, acc.: 71.09%] [G loss: 0.614333]\n",
      "epoch:16 step:15890 [D loss: 0.453295, acc.: 81.25%] [G loss: 0.727550]\n",
      "epoch:16 step:15891 [D loss: 0.490716, acc.: 73.44%] [G loss: 0.768244]\n",
      "epoch:16 step:15892 [D loss: 0.541536, acc.: 71.88%] [G loss: 0.815495]\n",
      "epoch:16 step:15893 [D loss: 0.551425, acc.: 69.53%] [G loss: 0.645458]\n",
      "epoch:16 step:15894 [D loss: 0.583726, acc.: 64.84%] [G loss: 0.573865]\n",
      "epoch:16 step:15895 [D loss: 0.521257, acc.: 71.09%] [G loss: 0.668559]\n",
      "epoch:16 step:15896 [D loss: 0.583380, acc.: 71.88%] [G loss: 0.647293]\n",
      "epoch:16 step:15897 [D loss: 0.580116, acc.: 74.22%] [G loss: 0.639037]\n",
      "epoch:16 step:15898 [D loss: 0.489589, acc.: 78.12%] [G loss: 0.602921]\n",
      "epoch:16 step:15899 [D loss: 0.554668, acc.: 67.97%] [G loss: 0.579564]\n",
      "epoch:16 step:15900 [D loss: 0.506613, acc.: 75.78%] [G loss: 0.533559]\n",
      "epoch:16 step:15901 [D loss: 0.598828, acc.: 63.28%] [G loss: 0.673800]\n",
      "epoch:16 step:15902 [D loss: 0.551674, acc.: 71.88%] [G loss: 0.708476]\n",
      "epoch:16 step:15903 [D loss: 0.543893, acc.: 71.09%] [G loss: 0.744213]\n",
      "epoch:16 step:15904 [D loss: 0.490515, acc.: 75.00%] [G loss: 0.705690]\n",
      "epoch:16 step:15905 [D loss: 0.573374, acc.: 68.75%] [G loss: 0.751002]\n",
      "epoch:16 step:15906 [D loss: 0.558365, acc.: 69.53%] [G loss: 0.716219]\n",
      "epoch:16 step:15907 [D loss: 0.645139, acc.: 67.19%] [G loss: 0.387231]\n",
      "epoch:16 step:15908 [D loss: 0.559574, acc.: 71.88%] [G loss: 0.635225]\n",
      "epoch:16 step:15909 [D loss: 0.591503, acc.: 67.97%] [G loss: 0.666334]\n",
      "epoch:16 step:15910 [D loss: 0.481944, acc.: 81.25%] [G loss: 0.648903]\n",
      "epoch:16 step:15911 [D loss: 0.465436, acc.: 78.12%] [G loss: 0.823364]\n",
      "epoch:16 step:15912 [D loss: 0.714260, acc.: 56.25%] [G loss: 0.537425]\n",
      "epoch:16 step:15913 [D loss: 0.499510, acc.: 79.69%] [G loss: 0.649690]\n",
      "epoch:16 step:15914 [D loss: 0.566324, acc.: 67.19%] [G loss: 0.629753]\n",
      "epoch:16 step:15915 [D loss: 0.439455, acc.: 75.00%] [G loss: 0.706182]\n",
      "epoch:16 step:15916 [D loss: 0.437667, acc.: 76.56%] [G loss: 0.781068]\n",
      "epoch:16 step:15917 [D loss: 0.431858, acc.: 83.59%] [G loss: 0.870068]\n",
      "epoch:16 step:15918 [D loss: 0.463916, acc.: 77.34%] [G loss: 0.927184]\n",
      "epoch:16 step:15919 [D loss: 0.556244, acc.: 71.88%] [G loss: 0.949778]\n",
      "epoch:16 step:15920 [D loss: 0.658511, acc.: 66.41%] [G loss: 1.003713]\n",
      "epoch:16 step:15921 [D loss: 0.466035, acc.: 78.91%] [G loss: 1.207952]\n",
      "epoch:16 step:15922 [D loss: 0.451933, acc.: 76.56%] [G loss: 0.969872]\n",
      "epoch:16 step:15923 [D loss: 0.580930, acc.: 71.88%] [G loss: 1.153316]\n",
      "epoch:16 step:15924 [D loss: 0.639027, acc.: 60.94%] [G loss: 0.824891]\n",
      "epoch:16 step:15925 [D loss: 0.564239, acc.: 70.31%] [G loss: 0.812813]\n",
      "epoch:16 step:15926 [D loss: 0.513629, acc.: 69.53%] [G loss: 0.767440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15927 [D loss: 0.482305, acc.: 76.56%] [G loss: 1.016028]\n",
      "epoch:16 step:15928 [D loss: 0.354569, acc.: 85.94%] [G loss: 1.120109]\n",
      "epoch:16 step:15929 [D loss: 0.432050, acc.: 78.91%] [G loss: 1.386136]\n",
      "epoch:17 step:15930 [D loss: 0.636110, acc.: 64.06%] [G loss: 0.950777]\n",
      "epoch:17 step:15931 [D loss: 0.494960, acc.: 75.78%] [G loss: 1.112724]\n",
      "epoch:17 step:15932 [D loss: 0.574310, acc.: 69.53%] [G loss: 0.944532]\n",
      "epoch:17 step:15933 [D loss: 0.528363, acc.: 73.44%] [G loss: 0.871006]\n",
      "epoch:17 step:15934 [D loss: 0.554306, acc.: 74.22%] [G loss: 0.714533]\n",
      "epoch:17 step:15935 [D loss: 0.606232, acc.: 67.97%] [G loss: 0.727688]\n",
      "epoch:17 step:15936 [D loss: 0.450797, acc.: 79.69%] [G loss: 0.938450]\n",
      "epoch:17 step:15937 [D loss: 0.465122, acc.: 80.47%] [G loss: 0.875202]\n",
      "epoch:17 step:15938 [D loss: 0.454601, acc.: 80.47%] [G loss: 0.791364]\n",
      "epoch:17 step:15939 [D loss: 0.507964, acc.: 72.66%] [G loss: 0.855931]\n",
      "epoch:17 step:15940 [D loss: 0.504550, acc.: 71.88%] [G loss: 0.896766]\n",
      "epoch:17 step:15941 [D loss: 0.575727, acc.: 66.41%] [G loss: 0.831746]\n",
      "epoch:17 step:15942 [D loss: 0.564588, acc.: 70.31%] [G loss: 0.619670]\n",
      "epoch:17 step:15943 [D loss: 0.516439, acc.: 73.44%] [G loss: 0.701500]\n",
      "epoch:17 step:15944 [D loss: 0.476216, acc.: 80.47%] [G loss: 0.710435]\n",
      "epoch:17 step:15945 [D loss: 0.487086, acc.: 71.88%] [G loss: 0.747072]\n",
      "epoch:17 step:15946 [D loss: 0.530863, acc.: 71.88%] [G loss: 0.811785]\n",
      "epoch:17 step:15947 [D loss: 0.545003, acc.: 67.97%] [G loss: 0.521521]\n",
      "epoch:17 step:15948 [D loss: 0.560971, acc.: 75.78%] [G loss: 0.753455]\n",
      "epoch:17 step:15949 [D loss: 0.570642, acc.: 71.09%] [G loss: 0.628536]\n",
      "epoch:17 step:15950 [D loss: 0.577420, acc.: 66.41%] [G loss: 0.606463]\n",
      "epoch:17 step:15951 [D loss: 0.477236, acc.: 71.88%] [G loss: 0.759456]\n",
      "epoch:17 step:15952 [D loss: 0.558603, acc.: 68.75%] [G loss: 0.784578]\n",
      "epoch:17 step:15953 [D loss: 0.492995, acc.: 70.31%] [G loss: 0.774155]\n",
      "epoch:17 step:15954 [D loss: 0.534585, acc.: 71.09%] [G loss: 0.782392]\n",
      "epoch:17 step:15955 [D loss: 0.588980, acc.: 65.62%] [G loss: 0.662995]\n",
      "epoch:17 step:15956 [D loss: 0.494470, acc.: 75.00%] [G loss: 0.583948]\n",
      "epoch:17 step:15957 [D loss: 0.573932, acc.: 67.97%] [G loss: 0.618918]\n",
      "epoch:17 step:15958 [D loss: 0.471046, acc.: 77.34%] [G loss: 0.800671]\n",
      "epoch:17 step:15959 [D loss: 0.590194, acc.: 71.09%] [G loss: 0.615444]\n",
      "epoch:17 step:15960 [D loss: 0.650258, acc.: 61.72%] [G loss: 0.526000]\n",
      "epoch:17 step:15961 [D loss: 0.576404, acc.: 69.53%] [G loss: 0.579873]\n",
      "epoch:17 step:15962 [D loss: 0.565946, acc.: 65.62%] [G loss: 0.555618]\n",
      "epoch:17 step:15963 [D loss: 0.533572, acc.: 72.66%] [G loss: 0.557170]\n",
      "epoch:17 step:15964 [D loss: 0.556127, acc.: 74.22%] [G loss: 0.623431]\n",
      "epoch:17 step:15965 [D loss: 0.536588, acc.: 70.31%] [G loss: 0.699598]\n",
      "epoch:17 step:15966 [D loss: 0.487769, acc.: 78.91%] [G loss: 0.545970]\n",
      "epoch:17 step:15967 [D loss: 0.567632, acc.: 69.53%] [G loss: 0.551002]\n",
      "epoch:17 step:15968 [D loss: 0.533331, acc.: 70.31%] [G loss: 0.620830]\n",
      "epoch:17 step:15969 [D loss: 0.528847, acc.: 72.66%] [G loss: 0.575490]\n",
      "epoch:17 step:15970 [D loss: 0.558391, acc.: 70.31%] [G loss: 0.698617]\n",
      "epoch:17 step:15971 [D loss: 0.535858, acc.: 69.53%] [G loss: 0.635741]\n",
      "epoch:17 step:15972 [D loss: 0.540233, acc.: 72.66%] [G loss: 0.556796]\n",
      "epoch:17 step:15973 [D loss: 0.609440, acc.: 61.72%] [G loss: 0.529467]\n",
      "epoch:17 step:15974 [D loss: 0.509968, acc.: 70.31%] [G loss: 0.592267]\n",
      "epoch:17 step:15975 [D loss: 0.519977, acc.: 70.31%] [G loss: 0.659217]\n",
      "epoch:17 step:15976 [D loss: 0.553432, acc.: 69.53%] [G loss: 0.657383]\n",
      "epoch:17 step:15977 [D loss: 0.493555, acc.: 77.34%] [G loss: 0.607229]\n",
      "epoch:17 step:15978 [D loss: 0.459385, acc.: 76.56%] [G loss: 0.786930]\n",
      "epoch:17 step:15979 [D loss: 0.523232, acc.: 72.66%] [G loss: 0.627203]\n",
      "epoch:17 step:15980 [D loss: 0.640492, acc.: 58.59%] [G loss: 0.574835]\n",
      "epoch:17 step:15981 [D loss: 0.670208, acc.: 68.75%] [G loss: 0.655109]\n",
      "epoch:17 step:15982 [D loss: 0.541334, acc.: 71.09%] [G loss: 0.614350]\n",
      "epoch:17 step:15983 [D loss: 0.485769, acc.: 77.34%] [G loss: 0.824093]\n",
      "epoch:17 step:15984 [D loss: 0.497965, acc.: 75.00%] [G loss: 0.663762]\n",
      "epoch:17 step:15985 [D loss: 0.502078, acc.: 75.00%] [G loss: 0.546692]\n",
      "epoch:17 step:15986 [D loss: 0.524478, acc.: 69.53%] [G loss: 0.450339]\n",
      "epoch:17 step:15987 [D loss: 0.562401, acc.: 68.75%] [G loss: 0.641366]\n",
      "epoch:17 step:15988 [D loss: 0.485202, acc.: 78.12%] [G loss: 0.703605]\n",
      "epoch:17 step:15989 [D loss: 0.584656, acc.: 68.75%] [G loss: 0.686235]\n",
      "epoch:17 step:15990 [D loss: 0.579184, acc.: 67.97%] [G loss: 0.558666]\n",
      "epoch:17 step:15991 [D loss: 0.569689, acc.: 74.22%] [G loss: 0.650358]\n",
      "epoch:17 step:15992 [D loss: 0.577761, acc.: 66.41%] [G loss: 0.510545]\n",
      "epoch:17 step:15993 [D loss: 0.569606, acc.: 68.75%] [G loss: 0.661008]\n",
      "epoch:17 step:15994 [D loss: 0.548580, acc.: 73.44%] [G loss: 0.589892]\n",
      "epoch:17 step:15995 [D loss: 0.541416, acc.: 68.75%] [G loss: 0.602520]\n",
      "epoch:17 step:15996 [D loss: 0.496716, acc.: 74.22%] [G loss: 0.570966]\n",
      "epoch:17 step:15997 [D loss: 0.535439, acc.: 71.09%] [G loss: 0.579950]\n",
      "epoch:17 step:15998 [D loss: 0.514845, acc.: 74.22%] [G loss: 0.613413]\n",
      "epoch:17 step:15999 [D loss: 0.542241, acc.: 69.53%] [G loss: 0.533427]\n",
      "epoch:17 step:16000 [D loss: 0.564007, acc.: 71.09%] [G loss: 0.569426]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.524089\n",
      "FID: 51.685989\n",
      "0 = 12.934758032512624\n",
      "1 = 0.08598880970031367\n",
      "2 = 0.8752999901771545\n",
      "3 = 0.8155999779701233\n",
      "4 = 0.9350000023841858\n",
      "5 = 0.9261866807937622\n",
      "6 = 0.8155999779701233\n",
      "7 = 8.433600942969314\n",
      "8 = 0.1581533848311345\n",
      "9 = 0.720300018787384\n",
      "10 = 0.7110000252723694\n",
      "11 = 0.7296000123023987\n",
      "12 = 0.7244752645492554\n",
      "13 = 0.7110000252723694\n",
      "14 = 6.524112224578857\n",
      "15 = 6.860581398010254\n",
      "16 = 0.39589715003967285\n",
      "17 = 6.5240888595581055\n",
      "18 = 51.68598937988281\n",
      "epoch:17 step:16001 [D loss: 0.525870, acc.: 73.44%] [G loss: 0.615840]\n",
      "epoch:17 step:16002 [D loss: 0.547754, acc.: 68.75%] [G loss: 0.641812]\n",
      "epoch:17 step:16003 [D loss: 0.475488, acc.: 77.34%] [G loss: 0.597567]\n",
      "epoch:17 step:16004 [D loss: 0.530496, acc.: 73.44%] [G loss: 0.768145]\n",
      "epoch:17 step:16005 [D loss: 0.505438, acc.: 71.09%] [G loss: 0.657686]\n",
      "epoch:17 step:16006 [D loss: 0.512825, acc.: 74.22%] [G loss: 0.678641]\n",
      "epoch:17 step:16007 [D loss: 0.629950, acc.: 64.84%] [G loss: 0.529557]\n",
      "epoch:17 step:16008 [D loss: 0.562556, acc.: 61.72%] [G loss: 0.575087]\n",
      "epoch:17 step:16009 [D loss: 0.506899, acc.: 76.56%] [G loss: 0.600192]\n",
      "epoch:17 step:16010 [D loss: 0.495604, acc.: 75.00%] [G loss: 0.655733]\n",
      "epoch:17 step:16011 [D loss: 0.531572, acc.: 70.31%] [G loss: 0.779482]\n",
      "epoch:17 step:16012 [D loss: 0.499431, acc.: 75.78%] [G loss: 1.036662]\n",
      "epoch:17 step:16013 [D loss: 0.597470, acc.: 69.53%] [G loss: 0.747139]\n",
      "epoch:17 step:16014 [D loss: 0.527667, acc.: 73.44%] [G loss: 0.609553]\n",
      "epoch:17 step:16015 [D loss: 0.521301, acc.: 75.00%] [G loss: 0.535491]\n",
      "epoch:17 step:16016 [D loss: 0.595446, acc.: 67.19%] [G loss: 0.633767]\n",
      "epoch:17 step:16017 [D loss: 0.472016, acc.: 79.69%] [G loss: 0.637417]\n",
      "epoch:17 step:16018 [D loss: 0.524691, acc.: 71.88%] [G loss: 0.771752]\n",
      "epoch:17 step:16019 [D loss: 0.540172, acc.: 72.66%] [G loss: 0.665797]\n",
      "epoch:17 step:16020 [D loss: 0.584023, acc.: 67.97%] [G loss: 0.654474]\n",
      "epoch:17 step:16021 [D loss: 0.487524, acc.: 78.12%] [G loss: 0.787092]\n",
      "epoch:17 step:16022 [D loss: 0.515245, acc.: 71.88%] [G loss: 0.739975]\n",
      "epoch:17 step:16023 [D loss: 0.504613, acc.: 75.78%] [G loss: 0.695877]\n",
      "epoch:17 step:16024 [D loss: 0.495932, acc.: 74.22%] [G loss: 0.673009]\n",
      "epoch:17 step:16025 [D loss: 0.510191, acc.: 73.44%] [G loss: 0.665509]\n",
      "epoch:17 step:16026 [D loss: 0.513195, acc.: 74.22%] [G loss: 0.707606]\n",
      "epoch:17 step:16027 [D loss: 0.559802, acc.: 69.53%] [G loss: 0.761265]\n",
      "epoch:17 step:16028 [D loss: 0.533326, acc.: 74.22%] [G loss: 0.697196]\n",
      "epoch:17 step:16029 [D loss: 0.473350, acc.: 75.00%] [G loss: 0.766940]\n",
      "epoch:17 step:16030 [D loss: 0.563684, acc.: 65.62%] [G loss: 0.665283]\n",
      "epoch:17 step:16031 [D loss: 0.601670, acc.: 64.06%] [G loss: 0.677173]\n",
      "epoch:17 step:16032 [D loss: 0.544778, acc.: 72.66%] [G loss: 0.483933]\n",
      "epoch:17 step:16033 [D loss: 0.520947, acc.: 71.88%] [G loss: 0.675203]\n",
      "epoch:17 step:16034 [D loss: 0.585563, acc.: 62.50%] [G loss: 0.574226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16035 [D loss: 0.527106, acc.: 67.97%] [G loss: 0.731063]\n",
      "epoch:17 step:16036 [D loss: 0.580527, acc.: 66.41%] [G loss: 0.702635]\n",
      "epoch:17 step:16037 [D loss: 0.615846, acc.: 64.84%] [G loss: 0.604302]\n",
      "epoch:17 step:16038 [D loss: 0.587954, acc.: 67.97%] [G loss: 0.564443]\n",
      "epoch:17 step:16039 [D loss: 0.518517, acc.: 74.22%] [G loss: 0.598914]\n",
      "epoch:17 step:16040 [D loss: 0.546096, acc.: 62.50%] [G loss: 0.601510]\n",
      "epoch:17 step:16041 [D loss: 0.532249, acc.: 75.78%] [G loss: 0.527975]\n",
      "epoch:17 step:16042 [D loss: 0.584218, acc.: 64.84%] [G loss: 0.447205]\n",
      "epoch:17 step:16043 [D loss: 0.541271, acc.: 75.00%] [G loss: 0.597775]\n",
      "epoch:17 step:16044 [D loss: 0.566611, acc.: 67.19%] [G loss: 0.658717]\n",
      "epoch:17 step:16045 [D loss: 0.513761, acc.: 72.66%] [G loss: 0.595453]\n",
      "epoch:17 step:16046 [D loss: 0.550848, acc.: 68.75%] [G loss: 0.636684]\n",
      "epoch:17 step:16047 [D loss: 0.534877, acc.: 71.88%] [G loss: 0.695220]\n",
      "epoch:17 step:16048 [D loss: 0.468770, acc.: 77.34%] [G loss: 0.760940]\n",
      "epoch:17 step:16049 [D loss: 0.546235, acc.: 74.22%] [G loss: 0.810251]\n",
      "epoch:17 step:16050 [D loss: 0.505554, acc.: 74.22%] [G loss: 0.715942]\n",
      "epoch:17 step:16051 [D loss: 0.531268, acc.: 72.66%] [G loss: 0.712409]\n",
      "epoch:17 step:16052 [D loss: 0.531075, acc.: 71.88%] [G loss: 0.698510]\n",
      "epoch:17 step:16053 [D loss: 0.535009, acc.: 71.88%] [G loss: 0.794229]\n",
      "epoch:17 step:16054 [D loss: 0.559591, acc.: 66.41%] [G loss: 0.672211]\n",
      "epoch:17 step:16055 [D loss: 0.504968, acc.: 71.09%] [G loss: 0.620878]\n",
      "epoch:17 step:16056 [D loss: 0.492740, acc.: 74.22%] [G loss: 0.552616]\n",
      "epoch:17 step:16057 [D loss: 0.568025, acc.: 67.19%] [G loss: 0.634923]\n",
      "epoch:17 step:16058 [D loss: 0.549740, acc.: 74.22%] [G loss: 0.562595]\n",
      "epoch:17 step:16059 [D loss: 0.535479, acc.: 74.22%] [G loss: 0.640318]\n",
      "epoch:17 step:16060 [D loss: 0.538903, acc.: 68.75%] [G loss: 0.799446]\n",
      "epoch:17 step:16061 [D loss: 0.565182, acc.: 69.53%] [G loss: 0.704463]\n",
      "epoch:17 step:16062 [D loss: 0.538287, acc.: 73.44%] [G loss: 0.654060]\n",
      "epoch:17 step:16063 [D loss: 0.575406, acc.: 69.53%] [G loss: 0.649492]\n",
      "epoch:17 step:16064 [D loss: 0.514492, acc.: 75.78%] [G loss: 0.821643]\n",
      "epoch:17 step:16065 [D loss: 0.559160, acc.: 65.62%] [G loss: 0.780732]\n",
      "epoch:17 step:16066 [D loss: 0.616676, acc.: 64.06%] [G loss: 0.630743]\n",
      "epoch:17 step:16067 [D loss: 0.549917, acc.: 71.88%] [G loss: 0.719786]\n",
      "epoch:17 step:16068 [D loss: 0.518340, acc.: 71.88%] [G loss: 0.677720]\n",
      "epoch:17 step:16069 [D loss: 0.629668, acc.: 64.06%] [G loss: 0.676132]\n",
      "epoch:17 step:16070 [D loss: 0.539129, acc.: 71.09%] [G loss: 0.591901]\n",
      "epoch:17 step:16071 [D loss: 0.590005, acc.: 68.75%] [G loss: 0.512868]\n",
      "epoch:17 step:16072 [D loss: 0.601662, acc.: 68.75%] [G loss: 0.642602]\n",
      "epoch:17 step:16073 [D loss: 0.521584, acc.: 74.22%] [G loss: 0.689124]\n",
      "epoch:17 step:16074 [D loss: 0.522136, acc.: 72.66%] [G loss: 0.703478]\n",
      "epoch:17 step:16075 [D loss: 0.466305, acc.: 80.47%] [G loss: 0.651041]\n",
      "epoch:17 step:16076 [D loss: 0.641986, acc.: 69.53%] [G loss: 0.521673]\n",
      "epoch:17 step:16077 [D loss: 0.562836, acc.: 67.97%] [G loss: 0.521657]\n",
      "epoch:17 step:16078 [D loss: 0.540222, acc.: 74.22%] [G loss: 0.610967]\n",
      "epoch:17 step:16079 [D loss: 0.650485, acc.: 62.50%] [G loss: 0.519268]\n",
      "epoch:17 step:16080 [D loss: 0.580624, acc.: 67.97%] [G loss: 0.599326]\n",
      "epoch:17 step:16081 [D loss: 0.532088, acc.: 75.00%] [G loss: 0.701429]\n",
      "epoch:17 step:16082 [D loss: 0.597981, acc.: 62.50%] [G loss: 0.586500]\n",
      "epoch:17 step:16083 [D loss: 0.553957, acc.: 69.53%] [G loss: 0.556665]\n",
      "epoch:17 step:16084 [D loss: 0.478436, acc.: 76.56%] [G loss: 0.639488]\n",
      "epoch:17 step:16085 [D loss: 0.524968, acc.: 72.66%] [G loss: 0.651730]\n",
      "epoch:17 step:16086 [D loss: 0.545645, acc.: 70.31%] [G loss: 0.734665]\n",
      "epoch:17 step:16087 [D loss: 0.640565, acc.: 62.50%] [G loss: 0.512224]\n",
      "epoch:17 step:16088 [D loss: 0.528461, acc.: 70.31%] [G loss: 0.683432]\n",
      "epoch:17 step:16089 [D loss: 0.555767, acc.: 68.75%] [G loss: 0.712499]\n",
      "epoch:17 step:16090 [D loss: 0.516644, acc.: 75.78%] [G loss: 0.939690]\n",
      "epoch:17 step:16091 [D loss: 0.469624, acc.: 77.34%] [G loss: 0.898561]\n",
      "epoch:17 step:16092 [D loss: 0.568773, acc.: 68.75%] [G loss: 0.700043]\n",
      "epoch:17 step:16093 [D loss: 0.627776, acc.: 64.84%] [G loss: 0.556262]\n",
      "epoch:17 step:16094 [D loss: 0.483384, acc.: 71.09%] [G loss: 0.661017]\n",
      "epoch:17 step:16095 [D loss: 0.580625, acc.: 64.84%] [G loss: 0.755965]\n",
      "epoch:17 step:16096 [D loss: 0.532754, acc.: 72.66%] [G loss: 0.673934]\n",
      "epoch:17 step:16097 [D loss: 0.535177, acc.: 72.66%] [G loss: 0.516507]\n",
      "epoch:17 step:16098 [D loss: 0.626368, acc.: 57.81%] [G loss: 0.514136]\n",
      "epoch:17 step:16099 [D loss: 0.558311, acc.: 67.19%] [G loss: 0.491936]\n",
      "epoch:17 step:16100 [D loss: 0.499626, acc.: 76.56%] [G loss: 0.568065]\n",
      "epoch:17 step:16101 [D loss: 0.561559, acc.: 66.41%] [G loss: 0.556920]\n",
      "epoch:17 step:16102 [D loss: 0.515268, acc.: 72.66%] [G loss: 0.759027]\n",
      "epoch:17 step:16103 [D loss: 0.576109, acc.: 67.19%] [G loss: 0.602949]\n",
      "epoch:17 step:16104 [D loss: 0.605809, acc.: 60.94%] [G loss: 0.495718]\n",
      "epoch:17 step:16105 [D loss: 0.525526, acc.: 71.09%] [G loss: 0.652313]\n",
      "epoch:17 step:16106 [D loss: 0.513473, acc.: 71.88%] [G loss: 0.615110]\n",
      "epoch:17 step:16107 [D loss: 0.586710, acc.: 63.28%] [G loss: 0.459163]\n",
      "epoch:17 step:16108 [D loss: 0.534346, acc.: 69.53%] [G loss: 0.538769]\n",
      "epoch:17 step:16109 [D loss: 0.623320, acc.: 65.62%] [G loss: 0.574587]\n",
      "epoch:17 step:16110 [D loss: 0.539658, acc.: 65.62%] [G loss: 0.539397]\n",
      "epoch:17 step:16111 [D loss: 0.481418, acc.: 73.44%] [G loss: 0.718658]\n",
      "epoch:17 step:16112 [D loss: 0.614529, acc.: 65.62%] [G loss: 0.664828]\n",
      "epoch:17 step:16113 [D loss: 0.501965, acc.: 75.00%] [G loss: 0.534395]\n",
      "epoch:17 step:16114 [D loss: 0.542848, acc.: 71.88%] [G loss: 0.693359]\n",
      "epoch:17 step:16115 [D loss: 0.562154, acc.: 75.00%] [G loss: 0.609054]\n",
      "epoch:17 step:16116 [D loss: 0.672703, acc.: 59.38%] [G loss: 0.539654]\n",
      "epoch:17 step:16117 [D loss: 0.489260, acc.: 78.12%] [G loss: 0.639550]\n",
      "epoch:17 step:16118 [D loss: 0.567587, acc.: 69.53%] [G loss: 0.647403]\n",
      "epoch:17 step:16119 [D loss: 0.506653, acc.: 75.78%] [G loss: 0.590794]\n",
      "epoch:17 step:16120 [D loss: 0.500063, acc.: 75.78%] [G loss: 0.706078]\n",
      "epoch:17 step:16121 [D loss: 0.480321, acc.: 71.09%] [G loss: 0.669684]\n",
      "epoch:17 step:16122 [D loss: 0.601829, acc.: 71.88%] [G loss: 0.700169]\n",
      "epoch:17 step:16123 [D loss: 0.488320, acc.: 78.12%] [G loss: 0.877952]\n",
      "epoch:17 step:16124 [D loss: 0.601406, acc.: 65.62%] [G loss: 0.706793]\n",
      "epoch:17 step:16125 [D loss: 0.510040, acc.: 71.09%] [G loss: 0.670474]\n",
      "epoch:17 step:16126 [D loss: 0.480310, acc.: 77.34%] [G loss: 0.778605]\n",
      "epoch:17 step:16127 [D loss: 0.481710, acc.: 75.00%] [G loss: 0.627424]\n",
      "epoch:17 step:16128 [D loss: 0.521968, acc.: 70.31%] [G loss: 0.783249]\n",
      "epoch:17 step:16129 [D loss: 0.659062, acc.: 66.41%] [G loss: 0.582892]\n",
      "epoch:17 step:16130 [D loss: 0.570338, acc.: 67.19%] [G loss: 0.600353]\n",
      "epoch:17 step:16131 [D loss: 0.536047, acc.: 71.09%] [G loss: 0.670120]\n",
      "epoch:17 step:16132 [D loss: 0.597149, acc.: 68.75%] [G loss: 0.603048]\n",
      "epoch:17 step:16133 [D loss: 0.573944, acc.: 68.75%] [G loss: 0.531041]\n",
      "epoch:17 step:16134 [D loss: 0.503834, acc.: 75.78%] [G loss: 0.691057]\n",
      "epoch:17 step:16135 [D loss: 0.529874, acc.: 73.44%] [G loss: 0.703888]\n",
      "epoch:17 step:16136 [D loss: 0.443446, acc.: 79.69%] [G loss: 0.784621]\n",
      "epoch:17 step:16137 [D loss: 0.475845, acc.: 76.56%] [G loss: 0.675320]\n",
      "epoch:17 step:16138 [D loss: 0.529953, acc.: 75.78%] [G loss: 0.649573]\n",
      "epoch:17 step:16139 [D loss: 0.627300, acc.: 62.50%] [G loss: 0.682866]\n",
      "epoch:17 step:16140 [D loss: 0.657978, acc.: 62.50%] [G loss: 0.558672]\n",
      "epoch:17 step:16141 [D loss: 0.602570, acc.: 69.53%] [G loss: 0.489962]\n",
      "epoch:17 step:16142 [D loss: 0.485444, acc.: 75.78%] [G loss: 0.581315]\n",
      "epoch:17 step:16143 [D loss: 0.627696, acc.: 64.84%] [G loss: 0.502576]\n",
      "epoch:17 step:16144 [D loss: 0.543131, acc.: 68.75%] [G loss: 0.570374]\n",
      "epoch:17 step:16145 [D loss: 0.547092, acc.: 72.66%] [G loss: 0.565747]\n",
      "epoch:17 step:16146 [D loss: 0.539132, acc.: 74.22%] [G loss: 0.506279]\n",
      "epoch:17 step:16147 [D loss: 0.516947, acc.: 72.66%] [G loss: 0.680452]\n",
      "epoch:17 step:16148 [D loss: 0.503051, acc.: 78.91%] [G loss: 0.757035]\n",
      "epoch:17 step:16149 [D loss: 0.590873, acc.: 67.19%] [G loss: 0.604119]\n",
      "epoch:17 step:16150 [D loss: 0.513835, acc.: 71.09%] [G loss: 0.883807]\n",
      "epoch:17 step:16151 [D loss: 0.498166, acc.: 75.78%] [G loss: 0.638938]\n",
      "epoch:17 step:16152 [D loss: 0.499843, acc.: 79.69%] [G loss: 0.798078]\n",
      "epoch:17 step:16153 [D loss: 0.590901, acc.: 67.97%] [G loss: 0.720720]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16154 [D loss: 0.568317, acc.: 64.84%] [G loss: 0.633147]\n",
      "epoch:17 step:16155 [D loss: 0.578221, acc.: 64.06%] [G loss: 0.556799]\n",
      "epoch:17 step:16156 [D loss: 0.510252, acc.: 73.44%] [G loss: 0.505686]\n",
      "epoch:17 step:16157 [D loss: 0.560747, acc.: 69.53%] [G loss: 0.636777]\n",
      "epoch:17 step:16158 [D loss: 0.535357, acc.: 73.44%] [G loss: 0.575859]\n",
      "epoch:17 step:16159 [D loss: 0.578722, acc.: 67.97%] [G loss: 0.600649]\n",
      "epoch:17 step:16160 [D loss: 0.476283, acc.: 75.00%] [G loss: 0.801845]\n",
      "epoch:17 step:16161 [D loss: 0.472274, acc.: 73.44%] [G loss: 0.744230]\n",
      "epoch:17 step:16162 [D loss: 0.516350, acc.: 71.09%] [G loss: 0.850843]\n",
      "epoch:17 step:16163 [D loss: 0.577092, acc.: 68.75%] [G loss: 0.558836]\n",
      "epoch:17 step:16164 [D loss: 0.549038, acc.: 67.19%] [G loss: 0.571252]\n",
      "epoch:17 step:16165 [D loss: 0.586165, acc.: 67.97%] [G loss: 0.467878]\n",
      "epoch:17 step:16166 [D loss: 0.557318, acc.: 71.09%] [G loss: 0.480693]\n",
      "epoch:17 step:16167 [D loss: 0.616041, acc.: 67.97%] [G loss: 0.601950]\n",
      "epoch:17 step:16168 [D loss: 0.500908, acc.: 76.56%] [G loss: 0.579639]\n",
      "epoch:17 step:16169 [D loss: 0.592585, acc.: 67.97%] [G loss: 0.626563]\n",
      "epoch:17 step:16170 [D loss: 0.488774, acc.: 76.56%] [G loss: 0.736147]\n",
      "epoch:17 step:16171 [D loss: 0.519290, acc.: 71.09%] [G loss: 0.675913]\n",
      "epoch:17 step:16172 [D loss: 0.564207, acc.: 66.41%] [G loss: 0.620929]\n",
      "epoch:17 step:16173 [D loss: 0.512948, acc.: 72.66%] [G loss: 0.608792]\n",
      "epoch:17 step:16174 [D loss: 0.530705, acc.: 74.22%] [G loss: 0.598928]\n",
      "epoch:17 step:16175 [D loss: 0.602879, acc.: 64.06%] [G loss: 0.579572]\n",
      "epoch:17 step:16176 [D loss: 0.508892, acc.: 76.56%] [G loss: 0.770154]\n",
      "epoch:17 step:16177 [D loss: 0.496647, acc.: 75.00%] [G loss: 0.804986]\n",
      "epoch:17 step:16178 [D loss: 0.572712, acc.: 69.53%] [G loss: 0.742250]\n",
      "epoch:17 step:16179 [D loss: 0.630644, acc.: 64.06%] [G loss: 0.691222]\n",
      "epoch:17 step:16180 [D loss: 0.628317, acc.: 71.88%] [G loss: 0.614942]\n",
      "epoch:17 step:16181 [D loss: 0.603898, acc.: 66.41%] [G loss: 0.568819]\n",
      "epoch:17 step:16182 [D loss: 0.582319, acc.: 67.19%] [G loss: 0.551814]\n",
      "epoch:17 step:16183 [D loss: 0.512583, acc.: 71.09%] [G loss: 0.729765]\n",
      "epoch:17 step:16184 [D loss: 0.524423, acc.: 75.00%] [G loss: 0.618755]\n",
      "epoch:17 step:16185 [D loss: 0.567378, acc.: 67.19%] [G loss: 0.616285]\n",
      "epoch:17 step:16186 [D loss: 0.595323, acc.: 67.19%] [G loss: 0.661659]\n",
      "epoch:17 step:16187 [D loss: 0.583443, acc.: 67.97%] [G loss: 0.559125]\n",
      "epoch:17 step:16188 [D loss: 0.546748, acc.: 66.41%] [G loss: 0.660046]\n",
      "epoch:17 step:16189 [D loss: 0.592032, acc.: 60.94%] [G loss: 0.596948]\n",
      "epoch:17 step:16190 [D loss: 0.566312, acc.: 71.09%] [G loss: 0.470464]\n",
      "epoch:17 step:16191 [D loss: 0.530420, acc.: 73.44%] [G loss: 0.657990]\n",
      "epoch:17 step:16192 [D loss: 0.571403, acc.: 70.31%] [G loss: 0.499485]\n",
      "epoch:17 step:16193 [D loss: 0.519212, acc.: 76.56%] [G loss: 0.611829]\n",
      "epoch:17 step:16194 [D loss: 0.529503, acc.: 72.66%] [G loss: 0.623145]\n",
      "epoch:17 step:16195 [D loss: 0.569465, acc.: 68.75%] [G loss: 0.539729]\n",
      "epoch:17 step:16196 [D loss: 0.584410, acc.: 66.41%] [G loss: 0.468161]\n",
      "epoch:17 step:16197 [D loss: 0.537100, acc.: 70.31%] [G loss: 0.519120]\n",
      "epoch:17 step:16198 [D loss: 0.531929, acc.: 71.88%] [G loss: 0.567700]\n",
      "epoch:17 step:16199 [D loss: 0.497428, acc.: 71.88%] [G loss: 0.746665]\n",
      "epoch:17 step:16200 [D loss: 0.473023, acc.: 76.56%] [G loss: 0.853491]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.568257\n",
      "FID: 46.131954\n",
      "0 = 12.912698717784874\n",
      "1 = 0.08455305738374842\n",
      "2 = 0.8689000010490417\n",
      "3 = 0.8263999819755554\n",
      "4 = 0.9114000201225281\n",
      "5 = 0.9031693935394287\n",
      "6 = 0.8263999819755554\n",
      "7 = 7.963168388962761\n",
      "8 = 0.1395771404809413\n",
      "9 = 0.7081000208854675\n",
      "10 = 0.7021999955177307\n",
      "11 = 0.7139999866485596\n",
      "12 = 0.7105848789215088\n",
      "13 = 0.7021999955177307\n",
      "14 = 6.568284511566162\n",
      "15 = 7.022562503814697\n",
      "16 = 0.3824147880077362\n",
      "17 = 6.568256855010986\n",
      "18 = 46.131954193115234\n",
      "epoch:17 step:16201 [D loss: 0.528215, acc.: 70.31%] [G loss: 0.680623]\n",
      "epoch:17 step:16202 [D loss: 0.558818, acc.: 67.97%] [G loss: 0.663789]\n",
      "epoch:17 step:16203 [D loss: 0.516966, acc.: 76.56%] [G loss: 0.672620]\n",
      "epoch:17 step:16204 [D loss: 0.588183, acc.: 66.41%] [G loss: 0.583006]\n",
      "epoch:17 step:16205 [D loss: 0.440335, acc.: 78.12%] [G loss: 0.768328]\n",
      "epoch:17 step:16206 [D loss: 0.690573, acc.: 59.38%] [G loss: 0.540944]\n",
      "epoch:17 step:16207 [D loss: 0.596180, acc.: 64.06%] [G loss: 0.433603]\n",
      "epoch:17 step:16208 [D loss: 0.567344, acc.: 64.06%] [G loss: 0.517227]\n",
      "epoch:17 step:16209 [D loss: 0.537537, acc.: 70.31%] [G loss: 0.437679]\n",
      "epoch:17 step:16210 [D loss: 0.580842, acc.: 68.75%] [G loss: 0.564145]\n",
      "epoch:17 step:16211 [D loss: 0.537543, acc.: 73.44%] [G loss: 0.551188]\n",
      "epoch:17 step:16212 [D loss: 0.514086, acc.: 76.56%] [G loss: 0.588171]\n",
      "epoch:17 step:16213 [D loss: 0.554763, acc.: 73.44%] [G loss: 0.655655]\n",
      "epoch:17 step:16214 [D loss: 0.549678, acc.: 69.53%] [G loss: 0.559632]\n",
      "epoch:17 step:16215 [D loss: 0.488512, acc.: 75.78%] [G loss: 0.740175]\n",
      "epoch:17 step:16216 [D loss: 0.580819, acc.: 69.53%] [G loss: 0.731995]\n",
      "epoch:17 step:16217 [D loss: 0.582902, acc.: 66.41%] [G loss: 0.665348]\n",
      "epoch:17 step:16218 [D loss: 0.547078, acc.: 72.66%] [G loss: 0.701647]\n",
      "epoch:17 step:16219 [D loss: 0.554437, acc.: 70.31%] [G loss: 0.815277]\n",
      "epoch:17 step:16220 [D loss: 0.623910, acc.: 59.38%] [G loss: 0.595950]\n",
      "epoch:17 step:16221 [D loss: 0.602709, acc.: 64.06%] [G loss: 0.535124]\n",
      "epoch:17 step:16222 [D loss: 0.538677, acc.: 75.00%] [G loss: 0.617222]\n",
      "epoch:17 step:16223 [D loss: 0.573243, acc.: 66.41%] [G loss: 0.472025]\n",
      "epoch:17 step:16224 [D loss: 0.509277, acc.: 68.75%] [G loss: 0.587797]\n",
      "epoch:17 step:16225 [D loss: 0.456652, acc.: 76.56%] [G loss: 0.594526]\n",
      "epoch:17 step:16226 [D loss: 0.528757, acc.: 73.44%] [G loss: 0.620816]\n",
      "epoch:17 step:16227 [D loss: 0.484284, acc.: 78.91%] [G loss: 0.578489]\n",
      "epoch:17 step:16228 [D loss: 0.484357, acc.: 75.00%] [G loss: 0.730377]\n",
      "epoch:17 step:16229 [D loss: 0.510140, acc.: 71.88%] [G loss: 0.743121]\n",
      "epoch:17 step:16230 [D loss: 0.669112, acc.: 65.62%] [G loss: 0.642758]\n",
      "epoch:17 step:16231 [D loss: 0.522400, acc.: 71.09%] [G loss: 0.619703]\n",
      "epoch:17 step:16232 [D loss: 0.568778, acc.: 72.66%] [G loss: 0.569286]\n",
      "epoch:17 step:16233 [D loss: 0.445830, acc.: 85.94%] [G loss: 0.661409]\n",
      "epoch:17 step:16234 [D loss: 0.558991, acc.: 69.53%] [G loss: 0.765905]\n",
      "epoch:17 step:16235 [D loss: 0.527803, acc.: 73.44%] [G loss: 0.764403]\n",
      "epoch:17 step:16236 [D loss: 0.450947, acc.: 78.12%] [G loss: 0.881246]\n",
      "epoch:17 step:16237 [D loss: 0.603824, acc.: 64.06%] [G loss: 0.653499]\n",
      "epoch:17 step:16238 [D loss: 0.528887, acc.: 69.53%] [G loss: 0.668560]\n",
      "epoch:17 step:16239 [D loss: 0.563528, acc.: 66.41%] [G loss: 0.612591]\n",
      "epoch:17 step:16240 [D loss: 0.503945, acc.: 67.97%] [G loss: 0.666384]\n",
      "epoch:17 step:16241 [D loss: 0.456608, acc.: 78.91%] [G loss: 0.878225]\n",
      "epoch:17 step:16242 [D loss: 0.508571, acc.: 75.78%] [G loss: 0.956192]\n",
      "epoch:17 step:16243 [D loss: 0.492535, acc.: 79.69%] [G loss: 0.923667]\n",
      "epoch:17 step:16244 [D loss: 0.485309, acc.: 76.56%] [G loss: 1.006534]\n",
      "epoch:17 step:16245 [D loss: 0.708863, acc.: 67.19%] [G loss: 0.652280]\n",
      "epoch:17 step:16246 [D loss: 0.583105, acc.: 65.62%] [G loss: 0.552572]\n",
      "epoch:17 step:16247 [D loss: 0.546693, acc.: 68.75%] [G loss: 0.668233]\n",
      "epoch:17 step:16248 [D loss: 0.515055, acc.: 72.66%] [G loss: 0.666144]\n",
      "epoch:17 step:16249 [D loss: 0.545541, acc.: 74.22%] [G loss: 0.688150]\n",
      "epoch:17 step:16250 [D loss: 0.501364, acc.: 78.12%] [G loss: 0.580774]\n",
      "epoch:17 step:16251 [D loss: 0.553810, acc.: 71.09%] [G loss: 0.528055]\n",
      "epoch:17 step:16252 [D loss: 0.578019, acc.: 67.97%] [G loss: 0.628364]\n",
      "epoch:17 step:16253 [D loss: 0.575582, acc.: 66.41%] [G loss: 0.641781]\n",
      "epoch:17 step:16254 [D loss: 0.604584, acc.: 64.06%] [G loss: 0.482069]\n",
      "epoch:17 step:16255 [D loss: 0.473463, acc.: 80.47%] [G loss: 0.588879]\n",
      "epoch:17 step:16256 [D loss: 0.483234, acc.: 73.44%] [G loss: 0.773724]\n",
      "epoch:17 step:16257 [D loss: 0.501780, acc.: 76.56%] [G loss: 0.890196]\n",
      "epoch:17 step:16258 [D loss: 0.531093, acc.: 75.00%] [G loss: 0.739087]\n",
      "epoch:17 step:16259 [D loss: 0.600028, acc.: 65.62%] [G loss: 0.653713]\n",
      "epoch:17 step:16260 [D loss: 0.506314, acc.: 74.22%] [G loss: 0.648816]\n",
      "epoch:17 step:16261 [D loss: 0.516769, acc.: 75.00%] [G loss: 0.574960]\n",
      "epoch:17 step:16262 [D loss: 0.504066, acc.: 69.53%] [G loss: 0.476837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16263 [D loss: 0.529750, acc.: 71.88%] [G loss: 0.582485]\n",
      "epoch:17 step:16264 [D loss: 0.526808, acc.: 72.66%] [G loss: 0.546957]\n",
      "epoch:17 step:16265 [D loss: 0.532374, acc.: 71.88%] [G loss: 0.521293]\n",
      "epoch:17 step:16266 [D loss: 0.515958, acc.: 74.22%] [G loss: 0.633316]\n",
      "epoch:17 step:16267 [D loss: 0.515792, acc.: 75.00%] [G loss: 0.685079]\n",
      "epoch:17 step:16268 [D loss: 0.559879, acc.: 73.44%] [G loss: 0.585038]\n",
      "epoch:17 step:16269 [D loss: 0.531628, acc.: 73.44%] [G loss: 0.601411]\n",
      "epoch:17 step:16270 [D loss: 0.621968, acc.: 68.75%] [G loss: 0.704232]\n",
      "epoch:17 step:16271 [D loss: 0.641776, acc.: 64.06%] [G loss: 0.565962]\n",
      "epoch:17 step:16272 [D loss: 0.502282, acc.: 72.66%] [G loss: 0.641006]\n",
      "epoch:17 step:16273 [D loss: 0.456599, acc.: 81.25%] [G loss: 0.795241]\n",
      "epoch:17 step:16274 [D loss: 0.517928, acc.: 72.66%] [G loss: 0.833728]\n",
      "epoch:17 step:16275 [D loss: 0.495576, acc.: 75.78%] [G loss: 0.681364]\n",
      "epoch:17 step:16276 [D loss: 0.460977, acc.: 78.12%] [G loss: 0.854989]\n",
      "epoch:17 step:16277 [D loss: 0.589558, acc.: 66.41%] [G loss: 0.693737]\n",
      "epoch:17 step:16278 [D loss: 0.721610, acc.: 53.91%] [G loss: 0.539472]\n",
      "epoch:17 step:16279 [D loss: 0.507339, acc.: 75.00%] [G loss: 0.606403]\n",
      "epoch:17 step:16280 [D loss: 0.527528, acc.: 73.44%] [G loss: 0.687397]\n",
      "epoch:17 step:16281 [D loss: 0.568060, acc.: 67.97%] [G loss: 0.611956]\n",
      "epoch:17 step:16282 [D loss: 0.564721, acc.: 69.53%] [G loss: 0.651316]\n",
      "epoch:17 step:16283 [D loss: 0.357658, acc.: 88.28%] [G loss: 0.676779]\n",
      "epoch:17 step:16284 [D loss: 0.551332, acc.: 73.44%] [G loss: 0.603197]\n",
      "epoch:17 step:16285 [D loss: 0.552414, acc.: 69.53%] [G loss: 0.585338]\n",
      "epoch:17 step:16286 [D loss: 0.432582, acc.: 79.69%] [G loss: 0.836340]\n",
      "epoch:17 step:16287 [D loss: 0.476096, acc.: 75.00%] [G loss: 0.796755]\n",
      "epoch:17 step:16288 [D loss: 0.450857, acc.: 78.91%] [G loss: 0.909408]\n",
      "epoch:17 step:16289 [D loss: 0.529422, acc.: 72.66%] [G loss: 0.624779]\n",
      "epoch:17 step:16290 [D loss: 0.509100, acc.: 75.78%] [G loss: 0.818697]\n",
      "epoch:17 step:16291 [D loss: 0.586336, acc.: 67.97%] [G loss: 0.783803]\n",
      "epoch:17 step:16292 [D loss: 0.571239, acc.: 66.41%] [G loss: 0.531987]\n",
      "epoch:17 step:16293 [D loss: 0.575374, acc.: 66.41%] [G loss: 0.590582]\n",
      "epoch:17 step:16294 [D loss: 0.544974, acc.: 71.88%] [G loss: 0.517612]\n",
      "epoch:17 step:16295 [D loss: 0.521088, acc.: 73.44%] [G loss: 0.628392]\n",
      "epoch:17 step:16296 [D loss: 0.556388, acc.: 71.09%] [G loss: 0.534276]\n",
      "epoch:17 step:16297 [D loss: 0.561777, acc.: 67.19%] [G loss: 0.542986]\n",
      "epoch:17 step:16298 [D loss: 0.536432, acc.: 67.19%] [G loss: 0.690737]\n",
      "epoch:17 step:16299 [D loss: 0.613806, acc.: 67.97%] [G loss: 0.596902]\n",
      "epoch:17 step:16300 [D loss: 0.569942, acc.: 67.97%] [G loss: 0.504778]\n",
      "epoch:17 step:16301 [D loss: 0.517303, acc.: 71.88%] [G loss: 0.684761]\n",
      "epoch:17 step:16302 [D loss: 0.557806, acc.: 71.09%] [G loss: 0.580871]\n",
      "epoch:17 step:16303 [D loss: 0.490314, acc.: 76.56%] [G loss: 0.652930]\n",
      "epoch:17 step:16304 [D loss: 0.532714, acc.: 75.00%] [G loss: 0.604136]\n",
      "epoch:17 step:16305 [D loss: 0.684438, acc.: 60.16%] [G loss: 0.417267]\n",
      "epoch:17 step:16306 [D loss: 0.591304, acc.: 67.19%] [G loss: 0.421907]\n",
      "epoch:17 step:16307 [D loss: 0.545723, acc.: 74.22%] [G loss: 0.600939]\n",
      "epoch:17 step:16308 [D loss: 0.567190, acc.: 71.09%] [G loss: 0.601431]\n",
      "epoch:17 step:16309 [D loss: 0.644285, acc.: 61.72%] [G loss: 0.391785]\n",
      "epoch:17 step:16310 [D loss: 0.456670, acc.: 81.25%] [G loss: 0.579442]\n",
      "epoch:17 step:16311 [D loss: 0.496487, acc.: 76.56%] [G loss: 0.752372]\n",
      "epoch:17 step:16312 [D loss: 0.601748, acc.: 68.75%] [G loss: 0.634388]\n",
      "epoch:17 step:16313 [D loss: 0.558356, acc.: 64.06%] [G loss: 0.553219]\n",
      "epoch:17 step:16314 [D loss: 0.510533, acc.: 73.44%] [G loss: 0.588510]\n",
      "epoch:17 step:16315 [D loss: 0.590424, acc.: 74.22%] [G loss: 0.598379]\n",
      "epoch:17 step:16316 [D loss: 0.540763, acc.: 71.09%] [G loss: 0.566333]\n",
      "epoch:17 step:16317 [D loss: 0.530723, acc.: 69.53%] [G loss: 0.515544]\n",
      "epoch:17 step:16318 [D loss: 0.529398, acc.: 71.88%] [G loss: 0.445361]\n",
      "epoch:17 step:16319 [D loss: 0.587199, acc.: 67.97%] [G loss: 0.487235]\n",
      "epoch:17 step:16320 [D loss: 0.559142, acc.: 70.31%] [G loss: 0.582361]\n",
      "epoch:17 step:16321 [D loss: 0.474481, acc.: 75.00%] [G loss: 0.607790]\n",
      "epoch:17 step:16322 [D loss: 0.592285, acc.: 65.62%] [G loss: 0.708176]\n",
      "epoch:17 step:16323 [D loss: 0.551003, acc.: 67.19%] [G loss: 0.572987]\n",
      "epoch:17 step:16324 [D loss: 0.590646, acc.: 64.84%] [G loss: 0.580575]\n",
      "epoch:17 step:16325 [D loss: 0.531812, acc.: 67.19%] [G loss: 0.694298]\n",
      "epoch:17 step:16326 [D loss: 0.571496, acc.: 67.19%] [G loss: 0.667249]\n",
      "epoch:17 step:16327 [D loss: 0.471030, acc.: 78.91%] [G loss: 0.765428]\n",
      "epoch:17 step:16328 [D loss: 0.499651, acc.: 75.78%] [G loss: 0.721967]\n",
      "epoch:17 step:16329 [D loss: 0.709673, acc.: 52.34%] [G loss: 0.485624]\n",
      "epoch:17 step:16330 [D loss: 0.665281, acc.: 54.69%] [G loss: 0.699710]\n",
      "epoch:17 step:16331 [D loss: 0.522531, acc.: 78.12%] [G loss: 0.538831]\n",
      "epoch:17 step:16332 [D loss: 0.557523, acc.: 74.22%] [G loss: 0.685822]\n",
      "epoch:17 step:16333 [D loss: 0.568383, acc.: 67.19%] [G loss: 0.800891]\n",
      "epoch:17 step:16334 [D loss: 0.612200, acc.: 60.16%] [G loss: 0.560368]\n",
      "epoch:17 step:16335 [D loss: 0.508585, acc.: 71.88%] [G loss: 0.681248]\n",
      "epoch:17 step:16336 [D loss: 0.575507, acc.: 67.19%] [G loss: 0.662980]\n",
      "epoch:17 step:16337 [D loss: 0.574039, acc.: 67.19%] [G loss: 0.553187]\n",
      "epoch:17 step:16338 [D loss: 0.555003, acc.: 70.31%] [G loss: 0.619137]\n",
      "epoch:17 step:16339 [D loss: 0.623914, acc.: 61.72%] [G loss: 0.591417]\n",
      "epoch:17 step:16340 [D loss: 0.620615, acc.: 63.28%] [G loss: 0.675041]\n",
      "epoch:17 step:16341 [D loss: 0.650476, acc.: 61.72%] [G loss: 0.470327]\n",
      "epoch:17 step:16342 [D loss: 0.540347, acc.: 71.09%] [G loss: 0.593741]\n",
      "epoch:17 step:16343 [D loss: 0.529667, acc.: 75.00%] [G loss: 0.601003]\n",
      "epoch:17 step:16344 [D loss: 0.541067, acc.: 70.31%] [G loss: 0.847331]\n",
      "epoch:17 step:16345 [D loss: 0.507298, acc.: 68.75%] [G loss: 0.692777]\n",
      "epoch:17 step:16346 [D loss: 0.544412, acc.: 68.75%] [G loss: 0.680799]\n",
      "epoch:17 step:16347 [D loss: 0.606112, acc.: 63.28%] [G loss: 0.597095]\n",
      "epoch:17 step:16348 [D loss: 0.550499, acc.: 69.53%] [G loss: 0.566925]\n",
      "epoch:17 step:16349 [D loss: 0.627122, acc.: 63.28%] [G loss: 0.670122]\n",
      "epoch:17 step:16350 [D loss: 0.556571, acc.: 69.53%] [G loss: 0.570538]\n",
      "epoch:17 step:16351 [D loss: 0.618335, acc.: 65.62%] [G loss: 0.574526]\n",
      "epoch:17 step:16352 [D loss: 0.586639, acc.: 64.06%] [G loss: 0.519303]\n",
      "epoch:17 step:16353 [D loss: 0.607412, acc.: 60.94%] [G loss: 0.507171]\n",
      "epoch:17 step:16354 [D loss: 0.538185, acc.: 72.66%] [G loss: 0.627805]\n",
      "epoch:17 step:16355 [D loss: 0.531759, acc.: 67.97%] [G loss: 0.649854]\n",
      "epoch:17 step:16356 [D loss: 0.466157, acc.: 77.34%] [G loss: 0.660174]\n",
      "epoch:17 step:16357 [D loss: 0.542055, acc.: 70.31%] [G loss: 0.659292]\n",
      "epoch:17 step:16358 [D loss: 0.477592, acc.: 77.34%] [G loss: 0.631912]\n",
      "epoch:17 step:16359 [D loss: 0.497342, acc.: 75.78%] [G loss: 0.708649]\n",
      "epoch:17 step:16360 [D loss: 0.563546, acc.: 67.97%] [G loss: 0.579602]\n",
      "epoch:17 step:16361 [D loss: 0.522459, acc.: 73.44%] [G loss: 0.634699]\n",
      "epoch:17 step:16362 [D loss: 0.617373, acc.: 63.28%] [G loss: 0.513263]\n",
      "epoch:17 step:16363 [D loss: 0.558373, acc.: 70.31%] [G loss: 0.596672]\n",
      "epoch:17 step:16364 [D loss: 0.537964, acc.: 72.66%] [G loss: 0.738784]\n",
      "epoch:17 step:16365 [D loss: 0.517679, acc.: 70.31%] [G loss: 0.756712]\n",
      "epoch:17 step:16366 [D loss: 0.639892, acc.: 63.28%] [G loss: 0.566913]\n",
      "epoch:17 step:16367 [D loss: 0.564520, acc.: 67.97%] [G loss: 0.367366]\n",
      "epoch:17 step:16368 [D loss: 0.517867, acc.: 76.56%] [G loss: 0.511784]\n",
      "epoch:17 step:16369 [D loss: 0.483327, acc.: 79.69%] [G loss: 0.753804]\n",
      "epoch:17 step:16370 [D loss: 0.600019, acc.: 68.75%] [G loss: 0.522208]\n",
      "epoch:17 step:16371 [D loss: 0.505680, acc.: 73.44%] [G loss: 0.874048]\n",
      "epoch:17 step:16372 [D loss: 0.578810, acc.: 64.84%] [G loss: 0.690194]\n",
      "epoch:17 step:16373 [D loss: 0.562724, acc.: 71.09%] [G loss: 0.631862]\n",
      "epoch:17 step:16374 [D loss: 0.589440, acc.: 64.84%] [G loss: 0.780371]\n",
      "epoch:17 step:16375 [D loss: 0.513241, acc.: 76.56%] [G loss: 0.718416]\n",
      "epoch:17 step:16376 [D loss: 0.585002, acc.: 67.19%] [G loss: 0.642923]\n",
      "epoch:17 step:16377 [D loss: 0.562905, acc.: 69.53%] [G loss: 0.544540]\n",
      "epoch:17 step:16378 [D loss: 0.483542, acc.: 77.34%] [G loss: 0.725261]\n",
      "epoch:17 step:16379 [D loss: 0.541262, acc.: 70.31%] [G loss: 0.640149]\n",
      "epoch:17 step:16380 [D loss: 0.482051, acc.: 77.34%] [G loss: 0.857882]\n",
      "epoch:17 step:16381 [D loss: 0.494277, acc.: 76.56%] [G loss: 0.675880]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16382 [D loss: 0.513965, acc.: 72.66%] [G loss: 0.603788]\n",
      "epoch:17 step:16383 [D loss: 0.570775, acc.: 72.66%] [G loss: 0.625741]\n",
      "epoch:17 step:16384 [D loss: 0.544526, acc.: 75.00%] [G loss: 0.724459]\n",
      "epoch:17 step:16385 [D loss: 0.630636, acc.: 65.62%] [G loss: 0.512439]\n",
      "epoch:17 step:16386 [D loss: 0.528427, acc.: 74.22%] [G loss: 0.634333]\n",
      "epoch:17 step:16387 [D loss: 0.618111, acc.: 60.16%] [G loss: 0.528312]\n",
      "epoch:17 step:16388 [D loss: 0.539033, acc.: 72.66%] [G loss: 0.693891]\n",
      "epoch:17 step:16389 [D loss: 0.482823, acc.: 77.34%] [G loss: 0.756988]\n",
      "epoch:17 step:16390 [D loss: 0.530902, acc.: 71.09%] [G loss: 0.759627]\n",
      "epoch:17 step:16391 [D loss: 0.569575, acc.: 68.75%] [G loss: 0.661175]\n",
      "epoch:17 step:16392 [D loss: 0.568112, acc.: 63.28%] [G loss: 0.681184]\n",
      "epoch:17 step:16393 [D loss: 0.534700, acc.: 75.78%] [G loss: 0.624898]\n",
      "epoch:17 step:16394 [D loss: 0.608016, acc.: 68.75%] [G loss: 0.587442]\n",
      "epoch:17 step:16395 [D loss: 0.520853, acc.: 74.22%] [G loss: 0.619609]\n",
      "epoch:17 step:16396 [D loss: 0.603501, acc.: 62.50%] [G loss: 0.609652]\n",
      "epoch:17 step:16397 [D loss: 0.546308, acc.: 75.00%] [G loss: 0.701040]\n",
      "epoch:17 step:16398 [D loss: 0.506257, acc.: 71.09%] [G loss: 0.742206]\n",
      "epoch:17 step:16399 [D loss: 0.478052, acc.: 78.91%] [G loss: 0.926493]\n",
      "epoch:17 step:16400 [D loss: 0.475955, acc.: 78.12%] [G loss: 0.853908]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.747876\n",
      "FID: 42.400543\n",
      "0 = 12.885754362392442\n",
      "1 = 0.08768082055055959\n",
      "2 = 0.8840000033378601\n",
      "3 = 0.8366000056266785\n",
      "4 = 0.9314000010490417\n",
      "5 = 0.9242156147956848\n",
      "6 = 0.8366000056266785\n",
      "7 = 7.758527591800668\n",
      "8 = 0.1267905735182275\n",
      "9 = 0.7138000130653381\n",
      "10 = 0.7070000171661377\n",
      "11 = 0.7206000089645386\n",
      "12 = 0.7167477607727051\n",
      "13 = 0.7070000171661377\n",
      "14 = 6.747901916503906\n",
      "15 = 7.142736911773682\n",
      "16 = 0.3697851002216339\n",
      "17 = 6.747875690460205\n",
      "18 = 42.400543212890625\n",
      "epoch:17 step:16401 [D loss: 0.489050, acc.: 78.12%] [G loss: 1.082263]\n",
      "epoch:17 step:16402 [D loss: 0.633809, acc.: 62.50%] [G loss: 0.677120]\n",
      "epoch:17 step:16403 [D loss: 0.588558, acc.: 64.84%] [G loss: 0.710574]\n",
      "epoch:17 step:16404 [D loss: 0.496623, acc.: 76.56%] [G loss: 0.732822]\n",
      "epoch:17 step:16405 [D loss: 0.564362, acc.: 71.09%] [G loss: 0.739015]\n",
      "epoch:17 step:16406 [D loss: 0.717026, acc.: 53.12%] [G loss: 0.440963]\n",
      "epoch:17 step:16407 [D loss: 0.595116, acc.: 65.62%] [G loss: 0.512328]\n",
      "epoch:17 step:16408 [D loss: 0.496788, acc.: 75.78%] [G loss: 0.612040]\n",
      "epoch:17 step:16409 [D loss: 0.615716, acc.: 67.97%] [G loss: 0.605212]\n",
      "epoch:17 step:16410 [D loss: 0.455458, acc.: 83.59%] [G loss: 0.650023]\n",
      "epoch:17 step:16411 [D loss: 0.620061, acc.: 64.84%] [G loss: 0.489240]\n",
      "epoch:17 step:16412 [D loss: 0.538461, acc.: 67.19%] [G loss: 0.645897]\n",
      "epoch:17 step:16413 [D loss: 0.511588, acc.: 74.22%] [G loss: 0.490455]\n",
      "epoch:17 step:16414 [D loss: 0.537081, acc.: 70.31%] [G loss: 0.604292]\n",
      "epoch:17 step:16415 [D loss: 0.601561, acc.: 64.84%] [G loss: 0.591877]\n",
      "epoch:17 step:16416 [D loss: 0.562854, acc.: 71.88%] [G loss: 0.508798]\n",
      "epoch:17 step:16417 [D loss: 0.501799, acc.: 72.66%] [G loss: 0.594474]\n",
      "epoch:17 step:16418 [D loss: 0.527777, acc.: 72.66%] [G loss: 0.656101]\n",
      "epoch:17 step:16419 [D loss: 0.509366, acc.: 77.34%] [G loss: 0.562583]\n",
      "epoch:17 step:16420 [D loss: 0.502926, acc.: 71.09%] [G loss: 0.697680]\n",
      "epoch:17 step:16421 [D loss: 0.598548, acc.: 64.84%] [G loss: 0.528236]\n",
      "epoch:17 step:16422 [D loss: 0.573362, acc.: 65.62%] [G loss: 0.542007]\n",
      "epoch:17 step:16423 [D loss: 0.580799, acc.: 69.53%] [G loss: 0.594989]\n",
      "epoch:17 step:16424 [D loss: 0.501032, acc.: 75.00%] [G loss: 0.640644]\n",
      "epoch:17 step:16425 [D loss: 0.550218, acc.: 72.66%] [G loss: 0.722696]\n",
      "epoch:17 step:16426 [D loss: 0.563287, acc.: 69.53%] [G loss: 0.638796]\n",
      "epoch:17 step:16427 [D loss: 0.531144, acc.: 72.66%] [G loss: 0.593265]\n",
      "epoch:17 step:16428 [D loss: 0.469807, acc.: 78.91%] [G loss: 0.754402]\n",
      "epoch:17 step:16429 [D loss: 0.585493, acc.: 69.53%] [G loss: 0.739505]\n",
      "epoch:17 step:16430 [D loss: 0.635460, acc.: 65.62%] [G loss: 0.556754]\n",
      "epoch:17 step:16431 [D loss: 0.632806, acc.: 59.38%] [G loss: 0.346691]\n",
      "epoch:17 step:16432 [D loss: 0.554962, acc.: 67.19%] [G loss: 0.521643]\n",
      "epoch:17 step:16433 [D loss: 0.499320, acc.: 74.22%] [G loss: 0.719580]\n",
      "epoch:17 step:16434 [D loss: 0.519118, acc.: 74.22%] [G loss: 0.674541]\n",
      "epoch:17 step:16435 [D loss: 0.509304, acc.: 71.88%] [G loss: 0.583301]\n",
      "epoch:17 step:16436 [D loss: 0.513099, acc.: 75.78%] [G loss: 0.684514]\n",
      "epoch:17 step:16437 [D loss: 0.465523, acc.: 78.91%] [G loss: 0.757239]\n",
      "epoch:17 step:16438 [D loss: 0.574532, acc.: 71.88%] [G loss: 0.724958]\n",
      "epoch:17 step:16439 [D loss: 0.631511, acc.: 60.94%] [G loss: 0.647193]\n",
      "epoch:17 step:16440 [D loss: 0.646621, acc.: 57.81%] [G loss: 0.546173]\n",
      "epoch:17 step:16441 [D loss: 0.603194, acc.: 66.41%] [G loss: 0.504716]\n",
      "epoch:17 step:16442 [D loss: 0.491256, acc.: 78.91%] [G loss: 0.555816]\n",
      "epoch:17 step:16443 [D loss: 0.520907, acc.: 71.09%] [G loss: 0.687233]\n",
      "epoch:17 step:16444 [D loss: 0.555757, acc.: 67.97%] [G loss: 0.563172]\n",
      "epoch:17 step:16445 [D loss: 0.470144, acc.: 78.12%] [G loss: 0.839326]\n",
      "epoch:17 step:16446 [D loss: 0.478160, acc.: 76.56%] [G loss: 0.660240]\n",
      "epoch:17 step:16447 [D loss: 0.543044, acc.: 71.88%] [G loss: 0.562764]\n",
      "epoch:17 step:16448 [D loss: 0.510325, acc.: 75.78%] [G loss: 0.723184]\n",
      "epoch:17 step:16449 [D loss: 0.471856, acc.: 75.00%] [G loss: 0.821415]\n",
      "epoch:17 step:16450 [D loss: 0.563423, acc.: 69.53%] [G loss: 0.702735]\n",
      "epoch:17 step:16451 [D loss: 0.512512, acc.: 75.78%] [G loss: 0.752195]\n",
      "epoch:17 step:16452 [D loss: 0.494525, acc.: 74.22%] [G loss: 0.762793]\n",
      "epoch:17 step:16453 [D loss: 0.539217, acc.: 70.31%] [G loss: 0.642205]\n",
      "epoch:17 step:16454 [D loss: 0.632355, acc.: 68.75%] [G loss: 0.634548]\n",
      "epoch:17 step:16455 [D loss: 0.561964, acc.: 69.53%] [G loss: 0.496437]\n",
      "epoch:17 step:16456 [D loss: 0.555309, acc.: 75.00%] [G loss: 0.593130]\n",
      "epoch:17 step:16457 [D loss: 0.654329, acc.: 55.47%] [G loss: 0.545014]\n",
      "epoch:17 step:16458 [D loss: 0.557376, acc.: 72.66%] [G loss: 0.452881]\n",
      "epoch:17 step:16459 [D loss: 0.555586, acc.: 68.75%] [G loss: 0.583818]\n",
      "epoch:17 step:16460 [D loss: 0.594317, acc.: 65.62%] [G loss: 0.697132]\n",
      "epoch:17 step:16461 [D loss: 0.546503, acc.: 67.19%] [G loss: 0.582136]\n",
      "epoch:17 step:16462 [D loss: 0.585480, acc.: 66.41%] [G loss: 0.627466]\n",
      "epoch:17 step:16463 [D loss: 0.533912, acc.: 72.66%] [G loss: 0.559336]\n",
      "epoch:17 step:16464 [D loss: 0.666466, acc.: 62.50%] [G loss: 0.476242]\n",
      "epoch:17 step:16465 [D loss: 0.532174, acc.: 69.53%] [G loss: 0.484095]\n",
      "epoch:17 step:16466 [D loss: 0.595540, acc.: 69.53%] [G loss: 0.515601]\n",
      "epoch:17 step:16467 [D loss: 0.600410, acc.: 67.97%] [G loss: 0.465024]\n",
      "epoch:17 step:16468 [D loss: 0.574086, acc.: 65.62%] [G loss: 0.579539]\n",
      "epoch:17 step:16469 [D loss: 0.615204, acc.: 61.72%] [G loss: 0.512941]\n",
      "epoch:17 step:16470 [D loss: 0.548001, acc.: 68.75%] [G loss: 0.757153]\n",
      "epoch:17 step:16471 [D loss: 0.650594, acc.: 64.84%] [G loss: 0.538389]\n",
      "epoch:17 step:16472 [D loss: 0.514393, acc.: 74.22%] [G loss: 0.585853]\n",
      "epoch:17 step:16473 [D loss: 0.565079, acc.: 67.97%] [G loss: 0.568822]\n",
      "epoch:17 step:16474 [D loss: 0.521977, acc.: 75.78%] [G loss: 0.655551]\n",
      "epoch:17 step:16475 [D loss: 0.519862, acc.: 74.22%] [G loss: 0.718393]\n",
      "epoch:17 step:16476 [D loss: 0.543346, acc.: 70.31%] [G loss: 0.796463]\n",
      "epoch:17 step:16477 [D loss: 0.487638, acc.: 75.00%] [G loss: 0.832469]\n",
      "epoch:17 step:16478 [D loss: 0.543261, acc.: 74.22%] [G loss: 0.726669]\n",
      "epoch:17 step:16479 [D loss: 0.517015, acc.: 73.44%] [G loss: 0.646252]\n",
      "epoch:17 step:16480 [D loss: 0.505814, acc.: 73.44%] [G loss: 0.602778]\n",
      "epoch:17 step:16481 [D loss: 0.515128, acc.: 75.78%] [G loss: 0.605036]\n",
      "epoch:17 step:16482 [D loss: 0.630860, acc.: 66.41%] [G loss: 0.574185]\n",
      "epoch:17 step:16483 [D loss: 0.472074, acc.: 78.12%] [G loss: 0.703532]\n",
      "epoch:17 step:16484 [D loss: 0.520858, acc.: 75.00%] [G loss: 0.753140]\n",
      "epoch:17 step:16485 [D loss: 0.584889, acc.: 65.62%] [G loss: 0.708372]\n",
      "epoch:17 step:16486 [D loss: 0.517498, acc.: 78.12%] [G loss: 0.754220]\n",
      "epoch:17 step:16487 [D loss: 0.512398, acc.: 75.78%] [G loss: 0.558820]\n",
      "epoch:17 step:16488 [D loss: 0.621235, acc.: 60.16%] [G loss: 0.530121]\n",
      "epoch:17 step:16489 [D loss: 0.505137, acc.: 74.22%] [G loss: 0.571383]\n",
      "epoch:17 step:16490 [D loss: 0.532214, acc.: 67.97%] [G loss: 0.570124]\n",
      "epoch:17 step:16491 [D loss: 0.545360, acc.: 74.22%] [G loss: 0.610130]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16492 [D loss: 0.529752, acc.: 68.75%] [G loss: 0.656123]\n",
      "epoch:17 step:16493 [D loss: 0.474303, acc.: 78.12%] [G loss: 0.912973]\n",
      "epoch:17 step:16494 [D loss: 0.580273, acc.: 67.19%] [G loss: 0.685404]\n",
      "epoch:17 step:16495 [D loss: 0.705560, acc.: 58.59%] [G loss: 0.638730]\n",
      "epoch:17 step:16496 [D loss: 0.466118, acc.: 80.47%] [G loss: 0.681135]\n",
      "epoch:17 step:16497 [D loss: 0.510688, acc.: 71.09%] [G loss: 0.674643]\n",
      "epoch:17 step:16498 [D loss: 0.542051, acc.: 71.09%] [G loss: 0.596711]\n",
      "epoch:17 step:16499 [D loss: 0.525813, acc.: 73.44%] [G loss: 0.586287]\n",
      "epoch:17 step:16500 [D loss: 0.563130, acc.: 68.75%] [G loss: 0.700090]\n",
      "epoch:17 step:16501 [D loss: 0.520523, acc.: 74.22%] [G loss: 0.633707]\n",
      "epoch:17 step:16502 [D loss: 0.539621, acc.: 71.09%] [G loss: 0.682464]\n",
      "epoch:17 step:16503 [D loss: 0.496661, acc.: 75.78%] [G loss: 0.719694]\n",
      "epoch:17 step:16504 [D loss: 0.469014, acc.: 77.34%] [G loss: 0.738784]\n",
      "epoch:17 step:16505 [D loss: 0.751982, acc.: 56.25%] [G loss: 0.634262]\n",
      "epoch:17 step:16506 [D loss: 0.542762, acc.: 71.09%] [G loss: 0.721895]\n",
      "epoch:17 step:16507 [D loss: 0.545118, acc.: 70.31%] [G loss: 0.552145]\n",
      "epoch:17 step:16508 [D loss: 0.523997, acc.: 72.66%] [G loss: 0.625870]\n",
      "epoch:17 step:16509 [D loss: 0.564003, acc.: 68.75%] [G loss: 0.517477]\n",
      "epoch:17 step:16510 [D loss: 0.525946, acc.: 74.22%] [G loss: 0.680401]\n",
      "epoch:17 step:16511 [D loss: 0.453956, acc.: 78.91%] [G loss: 0.705741]\n",
      "epoch:17 step:16512 [D loss: 0.526860, acc.: 74.22%] [G loss: 0.777056]\n",
      "epoch:17 step:16513 [D loss: 0.686702, acc.: 60.16%] [G loss: 0.604842]\n",
      "epoch:17 step:16514 [D loss: 0.540914, acc.: 73.44%] [G loss: 0.569189]\n",
      "epoch:17 step:16515 [D loss: 0.595189, acc.: 67.19%] [G loss: 0.539816]\n",
      "epoch:17 step:16516 [D loss: 0.542571, acc.: 69.53%] [G loss: 0.668732]\n",
      "epoch:17 step:16517 [D loss: 0.515704, acc.: 71.09%] [G loss: 0.633782]\n",
      "epoch:17 step:16518 [D loss: 0.512267, acc.: 75.00%] [G loss: 0.756662]\n",
      "epoch:17 step:16519 [D loss: 0.570256, acc.: 71.09%] [G loss: 0.724854]\n",
      "epoch:17 step:16520 [D loss: 0.595989, acc.: 69.53%] [G loss: 0.514926]\n",
      "epoch:17 step:16521 [D loss: 0.464238, acc.: 75.78%] [G loss: 0.629899]\n",
      "epoch:17 step:16522 [D loss: 0.539632, acc.: 71.09%] [G loss: 0.693033]\n",
      "epoch:17 step:16523 [D loss: 0.589911, acc.: 71.88%] [G loss: 0.542959]\n",
      "epoch:17 step:16524 [D loss: 0.569602, acc.: 67.97%] [G loss: 0.584310]\n",
      "epoch:17 step:16525 [D loss: 0.516963, acc.: 74.22%] [G loss: 0.617743]\n",
      "epoch:17 step:16526 [D loss: 0.510704, acc.: 72.66%] [G loss: 0.635006]\n",
      "epoch:17 step:16527 [D loss: 0.487207, acc.: 75.00%] [G loss: 0.771514]\n",
      "epoch:17 step:16528 [D loss: 0.561616, acc.: 67.19%] [G loss: 0.737912]\n",
      "epoch:17 step:16529 [D loss: 0.625360, acc.: 64.84%] [G loss: 0.615348]\n",
      "epoch:17 step:16530 [D loss: 0.554958, acc.: 69.53%] [G loss: 0.607058]\n",
      "epoch:17 step:16531 [D loss: 0.555483, acc.: 69.53%] [G loss: 0.658744]\n",
      "epoch:17 step:16532 [D loss: 0.486032, acc.: 77.34%] [G loss: 0.744692]\n",
      "epoch:17 step:16533 [D loss: 0.566524, acc.: 65.62%] [G loss: 0.677243]\n",
      "epoch:17 step:16534 [D loss: 0.510438, acc.: 73.44%] [G loss: 0.723936]\n",
      "epoch:17 step:16535 [D loss: 0.587550, acc.: 65.62%] [G loss: 0.664024]\n",
      "epoch:17 step:16536 [D loss: 0.548945, acc.: 66.41%] [G loss: 0.548834]\n",
      "epoch:17 step:16537 [D loss: 0.535396, acc.: 68.75%] [G loss: 0.562553]\n",
      "epoch:17 step:16538 [D loss: 0.537604, acc.: 64.06%] [G loss: 0.415643]\n",
      "epoch:17 step:16539 [D loss: 0.549917, acc.: 67.97%] [G loss: 0.410894]\n",
      "epoch:17 step:16540 [D loss: 0.554823, acc.: 71.09%] [G loss: 0.513068]\n",
      "epoch:17 step:16541 [D loss: 0.573327, acc.: 68.75%] [G loss: 0.499642]\n",
      "epoch:17 step:16542 [D loss: 0.497470, acc.: 76.56%] [G loss: 0.640425]\n",
      "epoch:17 step:16543 [D loss: 0.618895, acc.: 64.06%] [G loss: 0.616537]\n",
      "epoch:17 step:16544 [D loss: 0.541456, acc.: 67.19%] [G loss: 0.704219]\n",
      "epoch:17 step:16545 [D loss: 0.566864, acc.: 71.88%] [G loss: 0.723307]\n",
      "epoch:17 step:16546 [D loss: 0.495337, acc.: 79.69%] [G loss: 0.585981]\n",
      "epoch:17 step:16547 [D loss: 0.557491, acc.: 70.31%] [G loss: 0.681181]\n",
      "epoch:17 step:16548 [D loss: 0.538134, acc.: 70.31%] [G loss: 0.745981]\n",
      "epoch:17 step:16549 [D loss: 0.549094, acc.: 70.31%] [G loss: 0.673840]\n",
      "epoch:17 step:16550 [D loss: 0.581174, acc.: 68.75%] [G loss: 0.563433]\n",
      "epoch:17 step:16551 [D loss: 0.599644, acc.: 60.94%] [G loss: 0.595916]\n",
      "epoch:17 step:16552 [D loss: 0.511936, acc.: 75.00%] [G loss: 0.761296]\n",
      "epoch:17 step:16553 [D loss: 0.508078, acc.: 77.34%] [G loss: 0.698251]\n",
      "epoch:17 step:16554 [D loss: 0.602776, acc.: 65.62%] [G loss: 0.624812]\n",
      "epoch:17 step:16555 [D loss: 0.575901, acc.: 66.41%] [G loss: 0.557274]\n",
      "epoch:17 step:16556 [D loss: 0.529934, acc.: 71.88%] [G loss: 0.584374]\n",
      "epoch:17 step:16557 [D loss: 0.589259, acc.: 67.97%] [G loss: 0.565825]\n",
      "epoch:17 step:16558 [D loss: 0.535145, acc.: 71.88%] [G loss: 0.485656]\n",
      "epoch:17 step:16559 [D loss: 0.556424, acc.: 71.88%] [G loss: 0.597518]\n",
      "epoch:17 step:16560 [D loss: 0.498151, acc.: 78.12%] [G loss: 0.683881]\n",
      "epoch:17 step:16561 [D loss: 0.531519, acc.: 73.44%] [G loss: 0.593480]\n",
      "epoch:17 step:16562 [D loss: 0.478697, acc.: 77.34%] [G loss: 0.761891]\n",
      "epoch:17 step:16563 [D loss: 0.485257, acc.: 75.78%] [G loss: 0.788128]\n",
      "epoch:17 step:16564 [D loss: 0.465798, acc.: 75.78%] [G loss: 0.681961]\n",
      "epoch:17 step:16565 [D loss: 0.564007, acc.: 70.31%] [G loss: 0.614214]\n",
      "epoch:17 step:16566 [D loss: 0.530748, acc.: 76.56%] [G loss: 0.506158]\n",
      "epoch:17 step:16567 [D loss: 0.529001, acc.: 73.44%] [G loss: 0.635466]\n",
      "epoch:17 step:16568 [D loss: 0.520718, acc.: 70.31%] [G loss: 0.611846]\n",
      "epoch:17 step:16569 [D loss: 0.597465, acc.: 64.06%] [G loss: 0.627715]\n",
      "epoch:17 step:16570 [D loss: 0.544391, acc.: 68.75%] [G loss: 0.577194]\n",
      "epoch:17 step:16571 [D loss: 0.506851, acc.: 75.78%] [G loss: 0.821566]\n",
      "epoch:17 step:16572 [D loss: 0.604191, acc.: 64.84%] [G loss: 0.829599]\n",
      "epoch:17 step:16573 [D loss: 0.569399, acc.: 65.62%] [G loss: 0.635585]\n",
      "epoch:17 step:16574 [D loss: 0.515320, acc.: 71.88%] [G loss: 0.747181]\n",
      "epoch:17 step:16575 [D loss: 0.494152, acc.: 75.78%] [G loss: 0.729133]\n",
      "epoch:17 step:16576 [D loss: 0.506457, acc.: 76.56%] [G loss: 0.657416]\n",
      "epoch:17 step:16577 [D loss: 0.413756, acc.: 82.03%] [G loss: 0.932554]\n",
      "epoch:17 step:16578 [D loss: 0.522846, acc.: 72.66%] [G loss: 0.750941]\n",
      "epoch:17 step:16579 [D loss: 0.487601, acc.: 75.78%] [G loss: 0.882336]\n",
      "epoch:17 step:16580 [D loss: 0.504182, acc.: 73.44%] [G loss: 0.744106]\n",
      "epoch:17 step:16581 [D loss: 0.675264, acc.: 60.16%] [G loss: 0.704997]\n",
      "epoch:17 step:16582 [D loss: 0.592792, acc.: 62.50%] [G loss: 0.542976]\n",
      "epoch:17 step:16583 [D loss: 0.449255, acc.: 80.47%] [G loss: 0.715258]\n",
      "epoch:17 step:16584 [D loss: 0.551551, acc.: 71.88%] [G loss: 0.597379]\n",
      "epoch:17 step:16585 [D loss: 0.533877, acc.: 75.00%] [G loss: 0.704846]\n",
      "epoch:17 step:16586 [D loss: 0.527844, acc.: 73.44%] [G loss: 0.718573]\n",
      "epoch:17 step:16587 [D loss: 0.560848, acc.: 68.75%] [G loss: 0.679593]\n",
      "epoch:17 step:16588 [D loss: 0.522318, acc.: 75.78%] [G loss: 0.721055]\n",
      "epoch:17 step:16589 [D loss: 0.575408, acc.: 72.66%] [G loss: 0.719320]\n",
      "epoch:17 step:16590 [D loss: 0.517342, acc.: 73.44%] [G loss: 0.675500]\n",
      "epoch:17 step:16591 [D loss: 0.549675, acc.: 68.75%] [G loss: 0.695804]\n",
      "epoch:17 step:16592 [D loss: 0.598454, acc.: 65.62%] [G loss: 0.541886]\n",
      "epoch:17 step:16593 [D loss: 0.545387, acc.: 68.75%] [G loss: 0.656806]\n",
      "epoch:17 step:16594 [D loss: 0.520547, acc.: 71.09%] [G loss: 0.609734]\n",
      "epoch:17 step:16595 [D loss: 0.584341, acc.: 63.28%] [G loss: 0.519287]\n",
      "epoch:17 step:16596 [D loss: 0.550028, acc.: 70.31%] [G loss: 0.617845]\n",
      "epoch:17 step:16597 [D loss: 0.541110, acc.: 73.44%] [G loss: 0.593338]\n",
      "epoch:17 step:16598 [D loss: 0.555059, acc.: 71.09%] [G loss: 0.550331]\n",
      "epoch:17 step:16599 [D loss: 0.528343, acc.: 67.97%] [G loss: 0.596847]\n",
      "epoch:17 step:16600 [D loss: 0.576682, acc.: 63.28%] [G loss: 0.579953]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.742056\n",
      "FID: 41.315331\n",
      "0 = 12.929329778099062\n",
      "1 = 0.08248123996020874\n",
      "2 = 0.8736000061035156\n",
      "3 = 0.8208000063896179\n",
      "4 = 0.9264000058174133\n",
      "5 = 0.9177101850509644\n",
      "6 = 0.8208000063896179\n",
      "7 = 7.766299438333504\n",
      "8 = 0.13259151249529894\n",
      "9 = 0.7121999859809875\n",
      "10 = 0.6980000138282776\n",
      "11 = 0.7264000177383423\n",
      "12 = 0.718402624130249\n",
      "13 = 0.6980000138282776\n",
      "14 = 6.742084503173828\n",
      "15 = 7.2543134689331055\n",
      "16 = 0.3621692359447479\n",
      "17 = 6.742055892944336\n",
      "18 = 41.315330505371094\n",
      "epoch:17 step:16601 [D loss: 0.581720, acc.: 67.97%] [G loss: 0.606147]\n",
      "epoch:17 step:16602 [D loss: 0.599570, acc.: 64.06%] [G loss: 0.590022]\n",
      "epoch:17 step:16603 [D loss: 0.591411, acc.: 65.62%] [G loss: 0.709859]\n",
      "epoch:17 step:16604 [D loss: 0.638923, acc.: 63.28%] [G loss: 0.515290]\n",
      "epoch:17 step:16605 [D loss: 0.522796, acc.: 75.00%] [G loss: 0.617789]\n",
      "epoch:17 step:16606 [D loss: 0.491401, acc.: 77.34%] [G loss: 0.672430]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16607 [D loss: 0.603435, acc.: 67.97%] [G loss: 0.549682]\n",
      "epoch:17 step:16608 [D loss: 0.528772, acc.: 72.66%] [G loss: 0.564856]\n",
      "epoch:17 step:16609 [D loss: 0.566023, acc.: 70.31%] [G loss: 0.468439]\n",
      "epoch:17 step:16610 [D loss: 0.495777, acc.: 76.56%] [G loss: 0.535707]\n",
      "epoch:17 step:16611 [D loss: 0.513423, acc.: 74.22%] [G loss: 0.603788]\n",
      "epoch:17 step:16612 [D loss: 0.574335, acc.: 71.09%] [G loss: 0.552350]\n",
      "epoch:17 step:16613 [D loss: 0.560902, acc.: 63.28%] [G loss: 0.712788]\n",
      "epoch:17 step:16614 [D loss: 0.566017, acc.: 72.66%] [G loss: 0.452536]\n",
      "epoch:17 step:16615 [D loss: 0.581127, acc.: 67.19%] [G loss: 0.530302]\n",
      "epoch:17 step:16616 [D loss: 0.553187, acc.: 67.19%] [G loss: 0.553922]\n",
      "epoch:17 step:16617 [D loss: 0.533442, acc.: 74.22%] [G loss: 0.643207]\n",
      "epoch:17 step:16618 [D loss: 0.541192, acc.: 67.19%] [G loss: 0.615153]\n",
      "epoch:17 step:16619 [D loss: 0.511515, acc.: 74.22%] [G loss: 0.686179]\n",
      "epoch:17 step:16620 [D loss: 0.566418, acc.: 68.75%] [G loss: 0.618594]\n",
      "epoch:17 step:16621 [D loss: 0.564672, acc.: 68.75%] [G loss: 0.514468]\n",
      "epoch:17 step:16622 [D loss: 0.472167, acc.: 76.56%] [G loss: 0.769423]\n",
      "epoch:17 step:16623 [D loss: 0.487188, acc.: 81.25%] [G loss: 0.637092]\n",
      "epoch:17 step:16624 [D loss: 0.556616, acc.: 65.62%] [G loss: 0.623971]\n",
      "epoch:17 step:16625 [D loss: 0.638083, acc.: 58.59%] [G loss: 0.521893]\n",
      "epoch:17 step:16626 [D loss: 0.551830, acc.: 66.41%] [G loss: 0.580731]\n",
      "epoch:17 step:16627 [D loss: 0.537127, acc.: 72.66%] [G loss: 0.534119]\n",
      "epoch:17 step:16628 [D loss: 0.499946, acc.: 76.56%] [G loss: 0.578938]\n",
      "epoch:17 step:16629 [D loss: 0.571655, acc.: 68.75%] [G loss: 0.751875]\n",
      "epoch:17 step:16630 [D loss: 0.511231, acc.: 75.00%] [G loss: 0.762510]\n",
      "epoch:17 step:16631 [D loss: 0.602243, acc.: 65.62%] [G loss: 0.558591]\n",
      "epoch:17 step:16632 [D loss: 0.588854, acc.: 65.62%] [G loss: 0.618560]\n",
      "epoch:17 step:16633 [D loss: 0.559684, acc.: 69.53%] [G loss: 0.624120]\n",
      "epoch:17 step:16634 [D loss: 0.516900, acc.: 71.88%] [G loss: 0.646309]\n",
      "epoch:17 step:16635 [D loss: 0.586651, acc.: 66.41%] [G loss: 0.509630]\n",
      "epoch:17 step:16636 [D loss: 0.508673, acc.: 75.78%] [G loss: 0.607850]\n",
      "epoch:17 step:16637 [D loss: 0.480739, acc.: 73.44%] [G loss: 0.916346]\n",
      "epoch:17 step:16638 [D loss: 0.514981, acc.: 71.09%] [G loss: 0.785597]\n",
      "epoch:17 step:16639 [D loss: 0.594688, acc.: 68.75%] [G loss: 0.661682]\n",
      "epoch:17 step:16640 [D loss: 0.588050, acc.: 64.06%] [G loss: 0.614941]\n",
      "epoch:17 step:16641 [D loss: 0.592166, acc.: 66.41%] [G loss: 0.563430]\n",
      "epoch:17 step:16642 [D loss: 0.600409, acc.: 65.62%] [G loss: 0.821014]\n",
      "epoch:17 step:16643 [D loss: 0.534048, acc.: 71.09%] [G loss: 0.654848]\n",
      "epoch:17 step:16644 [D loss: 0.546571, acc.: 74.22%] [G loss: 0.571120]\n",
      "epoch:17 step:16645 [D loss: 0.615353, acc.: 64.84%] [G loss: 0.518844]\n",
      "epoch:17 step:16646 [D loss: 0.573640, acc.: 71.88%] [G loss: 0.503003]\n",
      "epoch:17 step:16647 [D loss: 0.580807, acc.: 67.97%] [G loss: 0.537003]\n",
      "epoch:17 step:16648 [D loss: 0.489350, acc.: 77.34%] [G loss: 0.694904]\n",
      "epoch:17 step:16649 [D loss: 0.587850, acc.: 73.44%] [G loss: 0.633320]\n",
      "epoch:17 step:16650 [D loss: 0.589168, acc.: 67.19%] [G loss: 0.389220]\n",
      "epoch:17 step:16651 [D loss: 0.553540, acc.: 68.75%] [G loss: 0.582384]\n",
      "epoch:17 step:16652 [D loss: 0.597004, acc.: 61.72%] [G loss: 0.485933]\n",
      "epoch:17 step:16653 [D loss: 0.504242, acc.: 75.78%] [G loss: 0.676965]\n",
      "epoch:17 step:16654 [D loss: 0.500188, acc.: 69.53%] [G loss: 0.728492]\n",
      "epoch:17 step:16655 [D loss: 0.492196, acc.: 74.22%] [G loss: 0.689437]\n",
      "epoch:17 step:16656 [D loss: 0.551872, acc.: 70.31%] [G loss: 0.564822]\n",
      "epoch:17 step:16657 [D loss: 0.530316, acc.: 73.44%] [G loss: 0.548409]\n",
      "epoch:17 step:16658 [D loss: 0.544601, acc.: 67.97%] [G loss: 0.536165]\n",
      "epoch:17 step:16659 [D loss: 0.511680, acc.: 71.88%] [G loss: 0.503099]\n",
      "epoch:17 step:16660 [D loss: 0.603856, acc.: 66.41%] [G loss: 0.507175]\n",
      "epoch:17 step:16661 [D loss: 0.521893, acc.: 68.75%] [G loss: 0.561972]\n",
      "epoch:17 step:16662 [D loss: 0.580035, acc.: 70.31%] [G loss: 0.567494]\n",
      "epoch:17 step:16663 [D loss: 0.498098, acc.: 75.00%] [G loss: 0.699715]\n",
      "epoch:17 step:16664 [D loss: 0.522947, acc.: 71.88%] [G loss: 0.582555]\n",
      "epoch:17 step:16665 [D loss: 0.484176, acc.: 72.66%] [G loss: 0.661228]\n",
      "epoch:17 step:16666 [D loss: 0.534948, acc.: 71.88%] [G loss: 0.648413]\n",
      "epoch:17 step:16667 [D loss: 0.571608, acc.: 64.06%] [G loss: 0.515181]\n",
      "epoch:17 step:16668 [D loss: 0.589547, acc.: 69.53%] [G loss: 0.673036]\n",
      "epoch:17 step:16669 [D loss: 0.645675, acc.: 56.25%] [G loss: 0.462313]\n",
      "epoch:17 step:16670 [D loss: 0.557422, acc.: 66.41%] [G loss: 0.458812]\n",
      "epoch:17 step:16671 [D loss: 0.527602, acc.: 71.09%] [G loss: 0.568670]\n",
      "epoch:17 step:16672 [D loss: 0.529896, acc.: 69.53%] [G loss: 0.569084]\n",
      "epoch:17 step:16673 [D loss: 0.496051, acc.: 75.00%] [G loss: 0.585024]\n",
      "epoch:17 step:16674 [D loss: 0.583249, acc.: 66.41%] [G loss: 0.566786]\n",
      "epoch:17 step:16675 [D loss: 0.484584, acc.: 78.12%] [G loss: 0.613872]\n",
      "epoch:17 step:16676 [D loss: 0.477830, acc.: 75.78%] [G loss: 0.701526]\n",
      "epoch:17 step:16677 [D loss: 0.549517, acc.: 71.88%] [G loss: 0.678883]\n",
      "epoch:17 step:16678 [D loss: 0.565894, acc.: 67.19%] [G loss: 0.617119]\n",
      "epoch:17 step:16679 [D loss: 0.495068, acc.: 75.00%] [G loss: 0.663223]\n",
      "epoch:17 step:16680 [D loss: 0.482661, acc.: 76.56%] [G loss: 0.684202]\n",
      "epoch:17 step:16681 [D loss: 0.604172, acc.: 64.84%] [G loss: 0.655289]\n",
      "epoch:17 step:16682 [D loss: 0.505623, acc.: 77.34%] [G loss: 0.597052]\n",
      "epoch:17 step:16683 [D loss: 0.561366, acc.: 65.62%] [G loss: 0.704430]\n",
      "epoch:17 step:16684 [D loss: 0.509440, acc.: 71.88%] [G loss: 0.762895]\n",
      "epoch:17 step:16685 [D loss: 0.533336, acc.: 73.44%] [G loss: 0.702719]\n",
      "epoch:17 step:16686 [D loss: 0.597097, acc.: 66.41%] [G loss: 0.687995]\n",
      "epoch:17 step:16687 [D loss: 0.567213, acc.: 72.66%] [G loss: 0.559079]\n",
      "epoch:17 step:16688 [D loss: 0.564239, acc.: 67.97%] [G loss: 0.475571]\n",
      "epoch:17 step:16689 [D loss: 0.525050, acc.: 71.88%] [G loss: 0.571927]\n",
      "epoch:17 step:16690 [D loss: 0.561664, acc.: 65.62%] [G loss: 0.499428]\n",
      "epoch:17 step:16691 [D loss: 0.563442, acc.: 64.06%] [G loss: 0.627701]\n",
      "epoch:17 step:16692 [D loss: 0.530061, acc.: 69.53%] [G loss: 0.725312]\n",
      "epoch:17 step:16693 [D loss: 0.556873, acc.: 68.75%] [G loss: 0.832633]\n",
      "epoch:17 step:16694 [D loss: 0.596866, acc.: 67.97%] [G loss: 0.566623]\n",
      "epoch:17 step:16695 [D loss: 0.658930, acc.: 58.59%] [G loss: 0.526402]\n",
      "epoch:17 step:16696 [D loss: 0.504512, acc.: 74.22%] [G loss: 0.628200]\n",
      "epoch:17 step:16697 [D loss: 0.548332, acc.: 72.66%] [G loss: 0.708323]\n",
      "epoch:17 step:16698 [D loss: 0.507718, acc.: 73.44%] [G loss: 0.584159]\n",
      "epoch:17 step:16699 [D loss: 0.545081, acc.: 69.53%] [G loss: 0.701578]\n",
      "epoch:17 step:16700 [D loss: 0.521286, acc.: 71.09%] [G loss: 0.595111]\n",
      "epoch:17 step:16701 [D loss: 0.554990, acc.: 69.53%] [G loss: 0.607539]\n",
      "epoch:17 step:16702 [D loss: 0.546067, acc.: 71.09%] [G loss: 0.549935]\n",
      "epoch:17 step:16703 [D loss: 0.541479, acc.: 72.66%] [G loss: 0.560933]\n",
      "epoch:17 step:16704 [D loss: 0.534756, acc.: 74.22%] [G loss: 0.636103]\n",
      "epoch:17 step:16705 [D loss: 0.596167, acc.: 66.41%] [G loss: 0.776214]\n",
      "epoch:17 step:16706 [D loss: 0.578307, acc.: 67.97%] [G loss: 0.551610]\n",
      "epoch:17 step:16707 [D loss: 0.573055, acc.: 67.97%] [G loss: 0.561109]\n",
      "epoch:17 step:16708 [D loss: 0.569207, acc.: 71.09%] [G loss: 0.693067]\n",
      "epoch:17 step:16709 [D loss: 0.548840, acc.: 70.31%] [G loss: 0.719444]\n",
      "epoch:17 step:16710 [D loss: 0.507236, acc.: 75.78%] [G loss: 0.943049]\n",
      "epoch:17 step:16711 [D loss: 0.489243, acc.: 71.09%] [G loss: 0.821651]\n",
      "epoch:17 step:16712 [D loss: 0.594616, acc.: 68.75%] [G loss: 0.889858]\n",
      "epoch:17 step:16713 [D loss: 0.659002, acc.: 66.41%] [G loss: 0.549237]\n",
      "epoch:17 step:16714 [D loss: 0.541660, acc.: 74.22%] [G loss: 0.592978]\n",
      "epoch:17 step:16715 [D loss: 0.561574, acc.: 67.97%] [G loss: 0.683560]\n",
      "epoch:17 step:16716 [D loss: 0.586115, acc.: 65.62%] [G loss: 0.478185]\n",
      "epoch:17 step:16717 [D loss: 0.651920, acc.: 57.03%] [G loss: 0.474509]\n",
      "epoch:17 step:16718 [D loss: 0.498035, acc.: 73.44%] [G loss: 0.605040]\n",
      "epoch:17 step:16719 [D loss: 0.537406, acc.: 68.75%] [G loss: 0.566442]\n",
      "epoch:17 step:16720 [D loss: 0.530412, acc.: 73.44%] [G loss: 0.575451]\n",
      "epoch:17 step:16721 [D loss: 0.502524, acc.: 71.09%] [G loss: 0.671862]\n",
      "epoch:17 step:16722 [D loss: 0.563929, acc.: 69.53%] [G loss: 0.612488]\n",
      "epoch:17 step:16723 [D loss: 0.637433, acc.: 60.94%] [G loss: 0.521638]\n",
      "epoch:17 step:16724 [D loss: 0.550632, acc.: 65.62%] [G loss: 0.581257]\n",
      "epoch:17 step:16725 [D loss: 0.491198, acc.: 74.22%] [G loss: 0.699404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16726 [D loss: 0.561679, acc.: 65.62%] [G loss: 0.529640]\n",
      "epoch:17 step:16727 [D loss: 0.539903, acc.: 68.75%] [G loss: 0.813840]\n",
      "epoch:17 step:16728 [D loss: 0.607256, acc.: 66.41%] [G loss: 0.654188]\n",
      "epoch:17 step:16729 [D loss: 0.581759, acc.: 66.41%] [G loss: 0.617619]\n",
      "epoch:17 step:16730 [D loss: 0.518713, acc.: 75.00%] [G loss: 0.740527]\n",
      "epoch:17 step:16731 [D loss: 0.515043, acc.: 75.00%] [G loss: 0.675576]\n",
      "epoch:17 step:16732 [D loss: 0.485500, acc.: 78.12%] [G loss: 0.619581]\n",
      "epoch:17 step:16733 [D loss: 0.599041, acc.: 67.19%] [G loss: 0.715306]\n",
      "epoch:17 step:16734 [D loss: 0.546277, acc.: 72.66%] [G loss: 0.600923]\n",
      "epoch:17 step:16735 [D loss: 0.585049, acc.: 64.06%] [G loss: 0.470782]\n",
      "epoch:17 step:16736 [D loss: 0.531815, acc.: 68.75%] [G loss: 0.609028]\n",
      "epoch:17 step:16737 [D loss: 0.552005, acc.: 71.09%] [G loss: 0.564246]\n",
      "epoch:17 step:16738 [D loss: 0.548721, acc.: 67.97%] [G loss: 0.595259]\n",
      "epoch:17 step:16739 [D loss: 0.556893, acc.: 67.19%] [G loss: 0.659328]\n",
      "epoch:17 step:16740 [D loss: 0.534285, acc.: 75.00%] [G loss: 0.541809]\n",
      "epoch:17 step:16741 [D loss: 0.640541, acc.: 61.72%] [G loss: 0.412889]\n",
      "epoch:17 step:16742 [D loss: 0.544056, acc.: 69.53%] [G loss: 0.530087]\n",
      "epoch:17 step:16743 [D loss: 0.540680, acc.: 73.44%] [G loss: 0.649824]\n",
      "epoch:17 step:16744 [D loss: 0.498392, acc.: 75.78%] [G loss: 0.941275]\n",
      "epoch:17 step:16745 [D loss: 0.588022, acc.: 70.31%] [G loss: 0.765938]\n",
      "epoch:17 step:16746 [D loss: 0.601540, acc.: 67.19%] [G loss: 0.533387]\n",
      "epoch:17 step:16747 [D loss: 0.582587, acc.: 67.97%] [G loss: 0.824541]\n",
      "epoch:17 step:16748 [D loss: 0.547449, acc.: 71.09%] [G loss: 0.643284]\n",
      "epoch:17 step:16749 [D loss: 0.662317, acc.: 60.16%] [G loss: 0.479941]\n",
      "epoch:17 step:16750 [D loss: 0.553751, acc.: 67.97%] [G loss: 0.539605]\n",
      "epoch:17 step:16751 [D loss: 0.569546, acc.: 67.19%] [G loss: 0.473693]\n",
      "epoch:17 step:16752 [D loss: 0.463372, acc.: 75.78%] [G loss: 0.521412]\n",
      "epoch:17 step:16753 [D loss: 0.565004, acc.: 71.09%] [G loss: 0.487379]\n",
      "epoch:17 step:16754 [D loss: 0.550795, acc.: 70.31%] [G loss: 0.530322]\n",
      "epoch:17 step:16755 [D loss: 0.592188, acc.: 66.41%] [G loss: 0.672849]\n",
      "epoch:17 step:16756 [D loss: 0.601478, acc.: 65.62%] [G loss: 0.492551]\n",
      "epoch:17 step:16757 [D loss: 0.666424, acc.: 63.28%] [G loss: 0.541447]\n",
      "epoch:17 step:16758 [D loss: 0.576447, acc.: 67.19%] [G loss: 0.659817]\n",
      "epoch:17 step:16759 [D loss: 0.565468, acc.: 69.53%] [G loss: 0.586741]\n",
      "epoch:17 step:16760 [D loss: 0.563295, acc.: 69.53%] [G loss: 0.581268]\n",
      "epoch:17 step:16761 [D loss: 0.512744, acc.: 75.00%] [G loss: 0.636919]\n",
      "epoch:17 step:16762 [D loss: 0.539839, acc.: 68.75%] [G loss: 0.498723]\n",
      "epoch:17 step:16763 [D loss: 0.560012, acc.: 67.19%] [G loss: 0.449570]\n",
      "epoch:17 step:16764 [D loss: 0.528203, acc.: 71.09%] [G loss: 0.540719]\n",
      "epoch:17 step:16765 [D loss: 0.524651, acc.: 72.66%] [G loss: 0.564449]\n",
      "epoch:17 step:16766 [D loss: 0.517999, acc.: 77.34%] [G loss: 0.595161]\n",
      "epoch:17 step:16767 [D loss: 0.553009, acc.: 71.09%] [G loss: 0.490410]\n",
      "epoch:17 step:16768 [D loss: 0.577464, acc.: 67.19%] [G loss: 0.600804]\n",
      "epoch:17 step:16769 [D loss: 0.599676, acc.: 66.41%] [G loss: 0.513458]\n",
      "epoch:17 step:16770 [D loss: 0.525312, acc.: 68.75%] [G loss: 0.510845]\n",
      "epoch:17 step:16771 [D loss: 0.538395, acc.: 69.53%] [G loss: 0.583758]\n",
      "epoch:17 step:16772 [D loss: 0.508740, acc.: 72.66%] [G loss: 0.720867]\n",
      "epoch:17 step:16773 [D loss: 0.516807, acc.: 72.66%] [G loss: 0.677677]\n",
      "epoch:17 step:16774 [D loss: 0.600879, acc.: 63.28%] [G loss: 0.715165]\n",
      "epoch:17 step:16775 [D loss: 0.630537, acc.: 60.94%] [G loss: 0.491903]\n",
      "epoch:17 step:16776 [D loss: 0.594430, acc.: 64.06%] [G loss: 0.511029]\n",
      "epoch:17 step:16777 [D loss: 0.543500, acc.: 71.09%] [G loss: 0.310107]\n",
      "epoch:17 step:16778 [D loss: 0.536650, acc.: 71.09%] [G loss: 0.445618]\n",
      "epoch:17 step:16779 [D loss: 0.592336, acc.: 66.41%] [G loss: 0.557055]\n",
      "epoch:17 step:16780 [D loss: 0.612945, acc.: 66.41%] [G loss: 0.497963]\n",
      "epoch:17 step:16781 [D loss: 0.549884, acc.: 71.09%] [G loss: 0.572904]\n",
      "epoch:17 step:16782 [D loss: 0.599396, acc.: 65.62%] [G loss: 0.585499]\n",
      "epoch:17 step:16783 [D loss: 0.555591, acc.: 64.84%] [G loss: 0.651663]\n",
      "epoch:17 step:16784 [D loss: 0.577846, acc.: 68.75%] [G loss: 0.527204]\n",
      "epoch:17 step:16785 [D loss: 0.558797, acc.: 71.09%] [G loss: 0.520572]\n",
      "epoch:17 step:16786 [D loss: 0.460935, acc.: 79.69%] [G loss: 0.679323]\n",
      "epoch:17 step:16787 [D loss: 0.647460, acc.: 66.41%] [G loss: 0.640804]\n",
      "epoch:17 step:16788 [D loss: 0.594272, acc.: 66.41%] [G loss: 0.596991]\n",
      "epoch:17 step:16789 [D loss: 0.447706, acc.: 78.12%] [G loss: 0.920230]\n",
      "epoch:17 step:16790 [D loss: 0.628454, acc.: 62.50%] [G loss: 0.657032]\n",
      "epoch:17 step:16791 [D loss: 0.572683, acc.: 66.41%] [G loss: 0.591553]\n",
      "epoch:17 step:16792 [D loss: 0.551885, acc.: 63.28%] [G loss: 0.606450]\n",
      "epoch:17 step:16793 [D loss: 0.557783, acc.: 70.31%] [G loss: 0.488037]\n",
      "epoch:17 step:16794 [D loss: 0.557604, acc.: 68.75%] [G loss: 0.418830]\n",
      "epoch:17 step:16795 [D loss: 0.608653, acc.: 64.84%] [G loss: 0.455414]\n",
      "epoch:17 step:16796 [D loss: 0.718354, acc.: 54.69%] [G loss: 0.526379]\n",
      "epoch:17 step:16797 [D loss: 0.527693, acc.: 74.22%] [G loss: 0.488500]\n",
      "epoch:17 step:16798 [D loss: 0.558590, acc.: 70.31%] [G loss: 0.530726]\n",
      "epoch:17 step:16799 [D loss: 0.464335, acc.: 76.56%] [G loss: 0.796659]\n",
      "epoch:17 step:16800 [D loss: 0.520820, acc.: 71.88%] [G loss: 0.652369]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.961205\n",
      "FID: 45.383488\n",
      "0 = 12.959954812717475\n",
      "1 = 0.09382943920323647\n",
      "2 = 0.8705999851226807\n",
      "3 = 0.8205999732017517\n",
      "4 = 0.9205999970436096\n",
      "5 = 0.9117777943611145\n",
      "6 = 0.8205999732017517\n",
      "7 = 8.075306348299968\n",
      "8 = 0.14394279488130757\n",
      "9 = 0.698199987411499\n",
      "10 = 0.6898000240325928\n",
      "11 = 0.70660001039505\n",
      "12 = 0.7015866637229919\n",
      "13 = 0.6898000240325928\n",
      "14 = 6.961236953735352\n",
      "15 = 7.467090606689453\n",
      "16 = 0.3509093225002289\n",
      "17 = 6.96120548248291\n",
      "18 = 45.383487701416016\n",
      "epoch:17 step:16801 [D loss: 0.552095, acc.: 67.97%] [G loss: 0.668185]\n",
      "epoch:17 step:16802 [D loss: 0.595501, acc.: 66.41%] [G loss: 0.527431]\n",
      "epoch:17 step:16803 [D loss: 0.586515, acc.: 66.41%] [G loss: 0.560839]\n",
      "epoch:17 step:16804 [D loss: 0.485730, acc.: 76.56%] [G loss: 0.565802]\n",
      "epoch:17 step:16805 [D loss: 0.599730, acc.: 66.41%] [G loss: 0.620749]\n",
      "epoch:17 step:16806 [D loss: 0.612019, acc.: 63.28%] [G loss: 0.541969]\n",
      "epoch:17 step:16807 [D loss: 0.565233, acc.: 69.53%] [G loss: 0.526013]\n",
      "epoch:17 step:16808 [D loss: 0.539629, acc.: 71.88%] [G loss: 0.440392]\n",
      "epoch:17 step:16809 [D loss: 0.649152, acc.: 62.50%] [G loss: 0.404832]\n",
      "epoch:17 step:16810 [D loss: 0.560988, acc.: 68.75%] [G loss: 0.538339]\n",
      "epoch:17 step:16811 [D loss: 0.567766, acc.: 69.53%] [G loss: 0.690745]\n",
      "epoch:17 step:16812 [D loss: 0.587690, acc.: 72.66%] [G loss: 0.446374]\n",
      "epoch:17 step:16813 [D loss: 0.503908, acc.: 76.56%] [G loss: 0.694561]\n",
      "epoch:17 step:16814 [D loss: 0.598697, acc.: 67.97%] [G loss: 0.594720]\n",
      "epoch:17 step:16815 [D loss: 0.552507, acc.: 68.75%] [G loss: 0.726436]\n",
      "epoch:17 step:16816 [D loss: 0.505499, acc.: 71.88%] [G loss: 0.697838]\n",
      "epoch:17 step:16817 [D loss: 0.577765, acc.: 67.19%] [G loss: 0.633587]\n",
      "epoch:17 step:16818 [D loss: 0.574525, acc.: 66.41%] [G loss: 0.582578]\n",
      "epoch:17 step:16819 [D loss: 0.488245, acc.: 74.22%] [G loss: 0.624261]\n",
      "epoch:17 step:16820 [D loss: 0.665360, acc.: 57.03%] [G loss: 0.478238]\n",
      "epoch:17 step:16821 [D loss: 0.596736, acc.: 64.06%] [G loss: 0.521675]\n",
      "epoch:17 step:16822 [D loss: 0.539175, acc.: 75.78%] [G loss: 0.626502]\n",
      "epoch:17 step:16823 [D loss: 0.489013, acc.: 74.22%] [G loss: 0.563554]\n",
      "epoch:17 step:16824 [D loss: 0.520288, acc.: 76.56%] [G loss: 0.727561]\n",
      "epoch:17 step:16825 [D loss: 0.504453, acc.: 71.09%] [G loss: 0.816931]\n",
      "epoch:17 step:16826 [D loss: 0.527614, acc.: 71.88%] [G loss: 0.801202]\n",
      "epoch:17 step:16827 [D loss: 0.502528, acc.: 77.34%] [G loss: 0.736144]\n",
      "epoch:17 step:16828 [D loss: 0.475306, acc.: 78.91%] [G loss: 0.792337]\n",
      "epoch:17 step:16829 [D loss: 0.525074, acc.: 72.66%] [G loss: 0.619706]\n",
      "epoch:17 step:16830 [D loss: 0.507924, acc.: 78.12%] [G loss: 0.656133]\n",
      "epoch:17 step:16831 [D loss: 0.570059, acc.: 73.44%] [G loss: 0.539717]\n",
      "epoch:17 step:16832 [D loss: 0.587924, acc.: 67.97%] [G loss: 0.647046]\n",
      "epoch:17 step:16833 [D loss: 0.552753, acc.: 72.66%] [G loss: 0.636811]\n",
      "epoch:17 step:16834 [D loss: 0.622088, acc.: 64.06%] [G loss: 0.507387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16835 [D loss: 0.509781, acc.: 76.56%] [G loss: 0.621682]\n",
      "epoch:17 step:16836 [D loss: 0.571319, acc.: 67.97%] [G loss: 0.609256]\n",
      "epoch:17 step:16837 [D loss: 0.549922, acc.: 73.44%] [G loss: 0.619050]\n",
      "epoch:17 step:16838 [D loss: 0.515774, acc.: 71.09%] [G loss: 0.759212]\n",
      "epoch:17 step:16839 [D loss: 0.527288, acc.: 70.31%] [G loss: 0.741957]\n",
      "epoch:17 step:16840 [D loss: 0.520426, acc.: 71.09%] [G loss: 0.761121]\n",
      "epoch:17 step:16841 [D loss: 0.470168, acc.: 78.12%] [G loss: 0.764842]\n",
      "epoch:17 step:16842 [D loss: 0.600717, acc.: 67.97%] [G loss: 0.709840]\n",
      "epoch:17 step:16843 [D loss: 0.524100, acc.: 70.31%] [G loss: 0.652359]\n",
      "epoch:17 step:16844 [D loss: 0.656254, acc.: 59.38%] [G loss: 0.728084]\n",
      "epoch:17 step:16845 [D loss: 0.513758, acc.: 74.22%] [G loss: 0.690340]\n",
      "epoch:17 step:16846 [D loss: 0.566095, acc.: 67.19%] [G loss: 0.615541]\n",
      "epoch:17 step:16847 [D loss: 0.463918, acc.: 81.25%] [G loss: 0.685963]\n",
      "epoch:17 step:16848 [D loss: 0.487105, acc.: 76.56%] [G loss: 0.771037]\n",
      "epoch:17 step:16849 [D loss: 0.676821, acc.: 64.84%] [G loss: 0.626979]\n",
      "epoch:17 step:16850 [D loss: 0.535730, acc.: 71.88%] [G loss: 0.603099]\n",
      "epoch:17 step:16851 [D loss: 0.578917, acc.: 64.06%] [G loss: 0.397732]\n",
      "epoch:17 step:16852 [D loss: 0.429305, acc.: 76.56%] [G loss: 0.827078]\n",
      "epoch:17 step:16853 [D loss: 0.511916, acc.: 77.34%] [G loss: 0.812175]\n",
      "epoch:17 step:16854 [D loss: 0.403365, acc.: 81.25%] [G loss: 0.896173]\n",
      "epoch:17 step:16855 [D loss: 0.453326, acc.: 78.91%] [G loss: 0.892348]\n",
      "epoch:17 step:16856 [D loss: 0.494597, acc.: 73.44%] [G loss: 0.936806]\n",
      "epoch:17 step:16857 [D loss: 0.729456, acc.: 64.06%] [G loss: 1.212733]\n",
      "epoch:17 step:16858 [D loss: 0.529520, acc.: 70.31%] [G loss: 1.381640]\n",
      "epoch:17 step:16859 [D loss: 0.521651, acc.: 71.09%] [G loss: 1.013596]\n",
      "epoch:17 step:16860 [D loss: 0.597993, acc.: 67.97%] [G loss: 1.053707]\n",
      "epoch:17 step:16861 [D loss: 0.691317, acc.: 61.72%] [G loss: 0.764111]\n",
      "epoch:17 step:16862 [D loss: 0.526928, acc.: 75.00%] [G loss: 0.868731]\n",
      "epoch:17 step:16863 [D loss: 0.536005, acc.: 64.06%] [G loss: 0.999098]\n",
      "epoch:17 step:16864 [D loss: 0.486512, acc.: 75.78%] [G loss: 1.029316]\n",
      "epoch:17 step:16865 [D loss: 0.395305, acc.: 81.25%] [G loss: 1.077707]\n",
      "epoch:17 step:16866 [D loss: 0.476366, acc.: 77.34%] [G loss: 0.958093]\n",
      "epoch:18 step:16867 [D loss: 0.591265, acc.: 71.88%] [G loss: 1.091020]\n",
      "epoch:18 step:16868 [D loss: 0.467038, acc.: 76.56%] [G loss: 1.052274]\n",
      "epoch:18 step:16869 [D loss: 0.541481, acc.: 71.09%] [G loss: 0.835757]\n",
      "epoch:18 step:16870 [D loss: 0.507724, acc.: 75.00%] [G loss: 0.695464]\n",
      "epoch:18 step:16871 [D loss: 0.534852, acc.: 70.31%] [G loss: 0.709503]\n",
      "epoch:18 step:16872 [D loss: 0.579932, acc.: 67.97%] [G loss: 0.917591]\n",
      "epoch:18 step:16873 [D loss: 0.495898, acc.: 75.00%] [G loss: 0.938792]\n",
      "epoch:18 step:16874 [D loss: 0.521994, acc.: 72.66%] [G loss: 0.744671]\n",
      "epoch:18 step:16875 [D loss: 0.504807, acc.: 75.78%] [G loss: 0.666401]\n",
      "epoch:18 step:16876 [D loss: 0.540158, acc.: 74.22%] [G loss: 0.556904]\n",
      "epoch:18 step:16877 [D loss: 0.493380, acc.: 76.56%] [G loss: 0.811201]\n",
      "epoch:18 step:16878 [D loss: 0.570314, acc.: 70.31%] [G loss: 0.743131]\n",
      "epoch:18 step:16879 [D loss: 0.505100, acc.: 74.22%] [G loss: 0.623612]\n",
      "epoch:18 step:16880 [D loss: 0.509660, acc.: 72.66%] [G loss: 0.707779]\n",
      "epoch:18 step:16881 [D loss: 0.455600, acc.: 75.00%] [G loss: 0.762062]\n",
      "epoch:18 step:16882 [D loss: 0.468057, acc.: 78.12%] [G loss: 0.829544]\n",
      "epoch:18 step:16883 [D loss: 0.604883, acc.: 67.19%] [G loss: 0.606332]\n",
      "epoch:18 step:16884 [D loss: 0.557136, acc.: 71.09%] [G loss: 0.616454]\n",
      "epoch:18 step:16885 [D loss: 0.574490, acc.: 73.44%] [G loss: 0.431796]\n",
      "epoch:18 step:16886 [D loss: 0.696675, acc.: 58.59%] [G loss: 0.655114]\n",
      "epoch:18 step:16887 [D loss: 0.585420, acc.: 67.19%] [G loss: 0.645666]\n",
      "epoch:18 step:16888 [D loss: 0.456630, acc.: 82.03%] [G loss: 0.667772]\n",
      "epoch:18 step:16889 [D loss: 0.588592, acc.: 70.31%] [G loss: 0.572083]\n",
      "epoch:18 step:16890 [D loss: 0.481803, acc.: 75.78%] [G loss: 0.736813]\n",
      "epoch:18 step:16891 [D loss: 0.517691, acc.: 75.78%] [G loss: 0.667973]\n",
      "epoch:18 step:16892 [D loss: 0.595799, acc.: 67.19%] [G loss: 0.654388]\n",
      "epoch:18 step:16893 [D loss: 0.487461, acc.: 70.31%] [G loss: 0.647347]\n",
      "epoch:18 step:16894 [D loss: 0.614062, acc.: 67.19%] [G loss: 0.537795]\n",
      "epoch:18 step:16895 [D loss: 0.566210, acc.: 67.19%] [G loss: 0.551741]\n",
      "epoch:18 step:16896 [D loss: 0.520136, acc.: 75.00%] [G loss: 0.734203]\n",
      "epoch:18 step:16897 [D loss: 0.658999, acc.: 60.16%] [G loss: 0.466590]\n",
      "epoch:18 step:16898 [D loss: 0.577961, acc.: 69.53%] [G loss: 0.588062]\n",
      "epoch:18 step:16899 [D loss: 0.516225, acc.: 71.09%] [G loss: 0.678861]\n",
      "epoch:18 step:16900 [D loss: 0.563231, acc.: 63.28%] [G loss: 0.689461]\n",
      "epoch:18 step:16901 [D loss: 0.540451, acc.: 69.53%] [G loss: 0.659833]\n",
      "epoch:18 step:16902 [D loss: 0.531023, acc.: 71.09%] [G loss: 0.686605]\n",
      "epoch:18 step:16903 [D loss: 0.499608, acc.: 78.12%] [G loss: 0.610707]\n",
      "epoch:18 step:16904 [D loss: 0.583090, acc.: 66.41%] [G loss: 0.664616]\n",
      "epoch:18 step:16905 [D loss: 0.531532, acc.: 73.44%] [G loss: 0.737715]\n",
      "epoch:18 step:16906 [D loss: 0.428753, acc.: 81.25%] [G loss: 0.688641]\n",
      "epoch:18 step:16907 [D loss: 0.537215, acc.: 71.88%] [G loss: 0.669281]\n",
      "epoch:18 step:16908 [D loss: 0.557598, acc.: 70.31%] [G loss: 0.699148]\n",
      "epoch:18 step:16909 [D loss: 0.539226, acc.: 75.00%] [G loss: 0.497295]\n",
      "epoch:18 step:16910 [D loss: 0.532727, acc.: 70.31%] [G loss: 0.532245]\n",
      "epoch:18 step:16911 [D loss: 0.508487, acc.: 77.34%] [G loss: 0.693936]\n",
      "epoch:18 step:16912 [D loss: 0.514370, acc.: 70.31%] [G loss: 0.701542]\n",
      "epoch:18 step:16913 [D loss: 0.585801, acc.: 68.75%] [G loss: 0.611897]\n",
      "epoch:18 step:16914 [D loss: 0.514488, acc.: 75.00%] [G loss: 0.755530]\n",
      "epoch:18 step:16915 [D loss: 0.468144, acc.: 77.34%] [G loss: 0.682986]\n",
      "epoch:18 step:16916 [D loss: 0.526076, acc.: 67.19%] [G loss: 0.825775]\n",
      "epoch:18 step:16917 [D loss: 0.645413, acc.: 61.72%] [G loss: 0.631090]\n",
      "epoch:18 step:16918 [D loss: 0.583580, acc.: 66.41%] [G loss: 0.615634]\n",
      "epoch:18 step:16919 [D loss: 0.578189, acc.: 64.06%] [G loss: 0.619163]\n",
      "epoch:18 step:16920 [D loss: 0.516615, acc.: 74.22%] [G loss: 0.653377]\n",
      "epoch:18 step:16921 [D loss: 0.592178, acc.: 62.50%] [G loss: 0.751730]\n",
      "epoch:18 step:16922 [D loss: 0.506503, acc.: 76.56%] [G loss: 0.704960]\n",
      "epoch:18 step:16923 [D loss: 0.523668, acc.: 68.75%] [G loss: 0.858863]\n",
      "epoch:18 step:16924 [D loss: 0.565901, acc.: 67.97%] [G loss: 0.715157]\n",
      "epoch:18 step:16925 [D loss: 0.521556, acc.: 73.44%] [G loss: 0.816649]\n",
      "epoch:18 step:16926 [D loss: 0.619201, acc.: 64.06%] [G loss: 0.598360]\n",
      "epoch:18 step:16927 [D loss: 0.548430, acc.: 67.19%] [G loss: 0.553724]\n",
      "epoch:18 step:16928 [D loss: 0.588617, acc.: 67.19%] [G loss: 0.550419]\n",
      "epoch:18 step:16929 [D loss: 0.574973, acc.: 65.62%] [G loss: 0.558894]\n",
      "epoch:18 step:16930 [D loss: 0.567161, acc.: 69.53%] [G loss: 0.500551]\n",
      "epoch:18 step:16931 [D loss: 0.512645, acc.: 75.00%] [G loss: 0.659892]\n",
      "epoch:18 step:16932 [D loss: 0.574712, acc.: 66.41%] [G loss: 0.647407]\n",
      "epoch:18 step:16933 [D loss: 0.539383, acc.: 70.31%] [G loss: 0.612598]\n",
      "epoch:18 step:16934 [D loss: 0.529342, acc.: 70.31%] [G loss: 0.570712]\n",
      "epoch:18 step:16935 [D loss: 0.560445, acc.: 72.66%] [G loss: 0.704275]\n",
      "epoch:18 step:16936 [D loss: 0.539108, acc.: 71.09%] [G loss: 0.690901]\n",
      "epoch:18 step:16937 [D loss: 0.578696, acc.: 68.75%] [G loss: 0.673311]\n",
      "epoch:18 step:16938 [D loss: 0.527569, acc.: 75.00%] [G loss: 0.558439]\n",
      "epoch:18 step:16939 [D loss: 0.574705, acc.: 67.19%] [G loss: 0.591671]\n",
      "epoch:18 step:16940 [D loss: 0.517144, acc.: 73.44%] [G loss: 0.633633]\n",
      "epoch:18 step:16941 [D loss: 0.529431, acc.: 72.66%] [G loss: 0.680932]\n",
      "epoch:18 step:16942 [D loss: 0.528365, acc.: 69.53%] [G loss: 0.699750]\n",
      "epoch:18 step:16943 [D loss: 0.459446, acc.: 73.44%] [G loss: 0.700170]\n",
      "epoch:18 step:16944 [D loss: 0.579713, acc.: 68.75%] [G loss: 0.705120]\n",
      "epoch:18 step:16945 [D loss: 0.605156, acc.: 62.50%] [G loss: 0.532211]\n",
      "epoch:18 step:16946 [D loss: 0.508233, acc.: 78.12%] [G loss: 0.591336]\n",
      "epoch:18 step:16947 [D loss: 0.544774, acc.: 67.97%] [G loss: 0.765602]\n",
      "epoch:18 step:16948 [D loss: 0.498536, acc.: 76.56%] [G loss: 0.675378]\n",
      "epoch:18 step:16949 [D loss: 0.475977, acc.: 77.34%] [G loss: 0.723480]\n",
      "epoch:18 step:16950 [D loss: 0.525344, acc.: 66.41%] [G loss: 0.647959]\n",
      "epoch:18 step:16951 [D loss: 0.576769, acc.: 68.75%] [G loss: 0.705578]\n",
      "epoch:18 step:16952 [D loss: 0.543565, acc.: 71.88%] [G loss: 0.648293]\n",
      "epoch:18 step:16953 [D loss: 0.525058, acc.: 76.56%] [G loss: 0.662391]\n",
      "epoch:18 step:16954 [D loss: 0.583215, acc.: 69.53%] [G loss: 0.673124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:16955 [D loss: 0.568380, acc.: 70.31%] [G loss: 0.877819]\n",
      "epoch:18 step:16956 [D loss: 0.535111, acc.: 72.66%] [G loss: 0.779625]\n",
      "epoch:18 step:16957 [D loss: 0.564500, acc.: 69.53%] [G loss: 0.556846]\n",
      "epoch:18 step:16958 [D loss: 0.455618, acc.: 80.47%] [G loss: 0.633359]\n",
      "epoch:18 step:16959 [D loss: 0.493660, acc.: 75.78%] [G loss: 0.549111]\n",
      "epoch:18 step:16960 [D loss: 0.541023, acc.: 70.31%] [G loss: 0.633557]\n",
      "epoch:18 step:16961 [D loss: 0.524049, acc.: 71.09%] [G loss: 0.734460]\n",
      "epoch:18 step:16962 [D loss: 0.502600, acc.: 76.56%] [G loss: 0.700416]\n",
      "epoch:18 step:16963 [D loss: 0.550151, acc.: 67.97%] [G loss: 0.786697]\n",
      "epoch:18 step:16964 [D loss: 0.569431, acc.: 67.97%] [G loss: 0.636649]\n",
      "epoch:18 step:16965 [D loss: 0.525793, acc.: 76.56%] [G loss: 0.734187]\n",
      "epoch:18 step:16966 [D loss: 0.439944, acc.: 79.69%] [G loss: 0.815010]\n",
      "epoch:18 step:16967 [D loss: 0.512845, acc.: 76.56%] [G loss: 0.808976]\n",
      "epoch:18 step:16968 [D loss: 0.614450, acc.: 60.94%] [G loss: 0.581948]\n",
      "epoch:18 step:16969 [D loss: 0.538361, acc.: 71.09%] [G loss: 0.702629]\n",
      "epoch:18 step:16970 [D loss: 0.548291, acc.: 66.41%] [G loss: 0.831237]\n",
      "epoch:18 step:16971 [D loss: 0.543545, acc.: 72.66%] [G loss: 0.580495]\n",
      "epoch:18 step:16972 [D loss: 0.553652, acc.: 69.53%] [G loss: 0.699797]\n",
      "epoch:18 step:16973 [D loss: 0.539508, acc.: 75.00%] [G loss: 0.817661]\n",
      "epoch:18 step:16974 [D loss: 0.688154, acc.: 54.69%] [G loss: 0.549130]\n",
      "epoch:18 step:16975 [D loss: 0.612633, acc.: 69.53%] [G loss: 0.573406]\n",
      "epoch:18 step:16976 [D loss: 0.549269, acc.: 75.78%] [G loss: 0.630914]\n",
      "epoch:18 step:16977 [D loss: 0.512461, acc.: 70.31%] [G loss: 0.863160]\n",
      "epoch:18 step:16978 [D loss: 0.549456, acc.: 69.53%] [G loss: 0.891200]\n",
      "epoch:18 step:16979 [D loss: 0.550988, acc.: 69.53%] [G loss: 0.616541]\n",
      "epoch:18 step:16980 [D loss: 0.533571, acc.: 72.66%] [G loss: 0.689839]\n",
      "epoch:18 step:16981 [D loss: 0.514744, acc.: 73.44%] [G loss: 0.773286]\n",
      "epoch:18 step:16982 [D loss: 0.541644, acc.: 75.00%] [G loss: 0.837508]\n",
      "epoch:18 step:16983 [D loss: 0.545434, acc.: 69.53%] [G loss: 0.865237]\n",
      "epoch:18 step:16984 [D loss: 0.523172, acc.: 72.66%] [G loss: 0.862465]\n",
      "epoch:18 step:16985 [D loss: 0.462122, acc.: 79.69%] [G loss: 0.975584]\n",
      "epoch:18 step:16986 [D loss: 0.573179, acc.: 70.31%] [G loss: 0.761688]\n",
      "epoch:18 step:16987 [D loss: 0.543980, acc.: 73.44%] [G loss: 0.717583]\n",
      "epoch:18 step:16988 [D loss: 0.515015, acc.: 78.12%] [G loss: 0.660411]\n",
      "epoch:18 step:16989 [D loss: 0.543771, acc.: 71.09%] [G loss: 0.754044]\n",
      "epoch:18 step:16990 [D loss: 0.578446, acc.: 71.88%] [G loss: 0.604325]\n",
      "epoch:18 step:16991 [D loss: 0.601465, acc.: 64.06%] [G loss: 0.589696]\n",
      "epoch:18 step:16992 [D loss: 0.545768, acc.: 69.53%] [G loss: 0.566481]\n",
      "epoch:18 step:16993 [D loss: 0.506731, acc.: 69.53%] [G loss: 0.823706]\n",
      "epoch:18 step:16994 [D loss: 0.521422, acc.: 69.53%] [G loss: 0.673538]\n",
      "epoch:18 step:16995 [D loss: 0.560724, acc.: 68.75%] [G loss: 0.598089]\n",
      "epoch:18 step:16996 [D loss: 0.534093, acc.: 72.66%] [G loss: 0.559032]\n",
      "epoch:18 step:16997 [D loss: 0.511199, acc.: 73.44%] [G loss: 0.678473]\n",
      "epoch:18 step:16998 [D loss: 0.586648, acc.: 64.84%] [G loss: 0.604341]\n",
      "epoch:18 step:16999 [D loss: 0.550776, acc.: 65.62%] [G loss: 0.611173]\n",
      "epoch:18 step:17000 [D loss: 0.556961, acc.: 71.09%] [G loss: 0.660317]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.661344\n",
      "FID: 45.214069\n",
      "0 = 13.029898016643479\n",
      "1 = 0.0956526642718958\n",
      "2 = 0.8805000185966492\n",
      "3 = 0.829200029373169\n",
      "4 = 0.9318000078201294\n",
      "5 = 0.9240026473999023\n",
      "6 = 0.829200029373169\n",
      "7 = 8.043007097816435\n",
      "8 = 0.14195932377920487\n",
      "9 = 0.7190999984741211\n",
      "10 = 0.7075999975204468\n",
      "11 = 0.7305999994277954\n",
      "12 = 0.7242579460144043\n",
      "13 = 0.7075999975204468\n",
      "14 = 6.6613688468933105\n",
      "15 = 7.229171276092529\n",
      "16 = 0.36800944805145264\n",
      "17 = 6.661343574523926\n",
      "18 = 45.21406936645508\n",
      "epoch:18 step:17001 [D loss: 0.511380, acc.: 72.66%] [G loss: 0.874337]\n",
      "epoch:18 step:17002 [D loss: 0.540272, acc.: 71.09%] [G loss: 0.707991]\n",
      "epoch:18 step:17003 [D loss: 0.646670, acc.: 64.06%] [G loss: 0.772605]\n",
      "epoch:18 step:17004 [D loss: 0.585539, acc.: 66.41%] [G loss: 0.598199]\n",
      "epoch:18 step:17005 [D loss: 0.491749, acc.: 81.25%] [G loss: 0.634978]\n",
      "epoch:18 step:17006 [D loss: 0.577834, acc.: 65.62%] [G loss: 0.612271]\n",
      "epoch:18 step:17007 [D loss: 0.540280, acc.: 72.66%] [G loss: 0.606048]\n",
      "epoch:18 step:17008 [D loss: 0.595532, acc.: 62.50%] [G loss: 0.645706]\n",
      "epoch:18 step:17009 [D loss: 0.569318, acc.: 68.75%] [G loss: 0.611839]\n",
      "epoch:18 step:17010 [D loss: 0.515682, acc.: 75.00%] [G loss: 0.757030]\n",
      "epoch:18 step:17011 [D loss: 0.564807, acc.: 69.53%] [G loss: 0.746831]\n",
      "epoch:18 step:17012 [D loss: 0.531902, acc.: 74.22%] [G loss: 0.770744]\n",
      "epoch:18 step:17013 [D loss: 0.604555, acc.: 69.53%] [G loss: 0.633312]\n",
      "epoch:18 step:17014 [D loss: 0.559245, acc.: 68.75%] [G loss: 0.686391]\n",
      "epoch:18 step:17015 [D loss: 0.491919, acc.: 75.00%] [G loss: 0.600874]\n",
      "epoch:18 step:17016 [D loss: 0.590676, acc.: 67.19%] [G loss: 0.669032]\n",
      "epoch:18 step:17017 [D loss: 0.515202, acc.: 75.00%] [G loss: 0.512797]\n",
      "epoch:18 step:17018 [D loss: 0.550772, acc.: 71.88%] [G loss: 0.623480]\n",
      "epoch:18 step:17019 [D loss: 0.586674, acc.: 65.62%] [G loss: 0.473034]\n",
      "epoch:18 step:17020 [D loss: 0.545143, acc.: 72.66%] [G loss: 0.524354]\n",
      "epoch:18 step:17021 [D loss: 0.493393, acc.: 73.44%] [G loss: 0.632764]\n",
      "epoch:18 step:17022 [D loss: 0.490475, acc.: 71.88%] [G loss: 0.726639]\n",
      "epoch:18 step:17023 [D loss: 0.570487, acc.: 67.19%] [G loss: 0.669106]\n",
      "epoch:18 step:17024 [D loss: 0.617907, acc.: 64.06%] [G loss: 0.514689]\n",
      "epoch:18 step:17025 [D loss: 0.517535, acc.: 73.44%] [G loss: 0.645096]\n",
      "epoch:18 step:17026 [D loss: 0.577411, acc.: 68.75%] [G loss: 0.759888]\n",
      "epoch:18 step:17027 [D loss: 0.520585, acc.: 77.34%] [G loss: 0.813016]\n",
      "epoch:18 step:17028 [D loss: 0.484045, acc.: 71.09%] [G loss: 0.955075]\n",
      "epoch:18 step:17029 [D loss: 0.550056, acc.: 68.75%] [G loss: 0.719224]\n",
      "epoch:18 step:17030 [D loss: 0.543397, acc.: 71.88%] [G loss: 0.743113]\n",
      "epoch:18 step:17031 [D loss: 0.525519, acc.: 73.44%] [G loss: 0.677046]\n",
      "epoch:18 step:17032 [D loss: 0.598083, acc.: 60.16%] [G loss: 0.580765]\n",
      "epoch:18 step:17033 [D loss: 0.585193, acc.: 67.19%] [G loss: 0.515140]\n",
      "epoch:18 step:17034 [D loss: 0.532918, acc.: 71.88%] [G loss: 0.556092]\n",
      "epoch:18 step:17035 [D loss: 0.582061, acc.: 62.50%] [G loss: 0.647005]\n",
      "epoch:18 step:17036 [D loss: 0.578918, acc.: 62.50%] [G loss: 0.483511]\n",
      "epoch:18 step:17037 [D loss: 0.538035, acc.: 71.09%] [G loss: 0.574620]\n",
      "epoch:18 step:17038 [D loss: 0.526143, acc.: 71.09%] [G loss: 0.642677]\n",
      "epoch:18 step:17039 [D loss: 0.531155, acc.: 76.56%] [G loss: 0.763138]\n",
      "epoch:18 step:17040 [D loss: 0.598948, acc.: 61.72%] [G loss: 0.525672]\n",
      "epoch:18 step:17041 [D loss: 0.578230, acc.: 63.28%] [G loss: 0.621924]\n",
      "epoch:18 step:17042 [D loss: 0.579359, acc.: 67.19%] [G loss: 0.473359]\n",
      "epoch:18 step:17043 [D loss: 0.545870, acc.: 72.66%] [G loss: 0.434055]\n",
      "epoch:18 step:17044 [D loss: 0.541725, acc.: 68.75%] [G loss: 0.588107]\n",
      "epoch:18 step:17045 [D loss: 0.508100, acc.: 71.88%] [G loss: 0.532437]\n",
      "epoch:18 step:17046 [D loss: 0.637057, acc.: 62.50%] [G loss: 0.488942]\n",
      "epoch:18 step:17047 [D loss: 0.537785, acc.: 72.66%] [G loss: 0.589772]\n",
      "epoch:18 step:17048 [D loss: 0.516143, acc.: 72.66%] [G loss: 0.524982]\n",
      "epoch:18 step:17049 [D loss: 0.624434, acc.: 68.75%] [G loss: 0.565658]\n",
      "epoch:18 step:17050 [D loss: 0.534960, acc.: 71.88%] [G loss: 0.608238]\n",
      "epoch:18 step:17051 [D loss: 0.573859, acc.: 64.06%] [G loss: 0.690578]\n",
      "epoch:18 step:17052 [D loss: 0.652407, acc.: 64.84%] [G loss: 0.642226]\n",
      "epoch:18 step:17053 [D loss: 0.601529, acc.: 64.06%] [G loss: 0.609700]\n",
      "epoch:18 step:17054 [D loss: 0.555270, acc.: 68.75%] [G loss: 0.563915]\n",
      "epoch:18 step:17055 [D loss: 0.617715, acc.: 64.06%] [G loss: 0.518487]\n",
      "epoch:18 step:17056 [D loss: 0.477750, acc.: 79.69%] [G loss: 0.540753]\n",
      "epoch:18 step:17057 [D loss: 0.497388, acc.: 73.44%] [G loss: 0.539399]\n",
      "epoch:18 step:17058 [D loss: 0.496169, acc.: 75.78%] [G loss: 0.566868]\n",
      "epoch:18 step:17059 [D loss: 0.511534, acc.: 72.66%] [G loss: 0.646754]\n",
      "epoch:18 step:17060 [D loss: 0.516107, acc.: 75.00%] [G loss: 0.831132]\n",
      "epoch:18 step:17061 [D loss: 0.549507, acc.: 71.09%] [G loss: 0.717640]\n",
      "epoch:18 step:17062 [D loss: 0.569208, acc.: 70.31%] [G loss: 0.716088]\n",
      "epoch:18 step:17063 [D loss: 0.569173, acc.: 67.19%] [G loss: 0.576710]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17064 [D loss: 0.440152, acc.: 78.91%] [G loss: 0.839197]\n",
      "epoch:18 step:17065 [D loss: 0.522639, acc.: 70.31%] [G loss: 0.842687]\n",
      "epoch:18 step:17066 [D loss: 0.622138, acc.: 65.62%] [G loss: 0.635960]\n",
      "epoch:18 step:17067 [D loss: 0.568814, acc.: 69.53%] [G loss: 0.613664]\n",
      "epoch:18 step:17068 [D loss: 0.529417, acc.: 71.88%] [G loss: 0.666324]\n",
      "epoch:18 step:17069 [D loss: 0.681421, acc.: 58.59%] [G loss: 0.610231]\n",
      "epoch:18 step:17070 [D loss: 0.539545, acc.: 67.97%] [G loss: 0.687909]\n",
      "epoch:18 step:17071 [D loss: 0.485874, acc.: 75.00%] [G loss: 0.719647]\n",
      "epoch:18 step:17072 [D loss: 0.513277, acc.: 75.00%] [G loss: 0.786327]\n",
      "epoch:18 step:17073 [D loss: 0.456397, acc.: 79.69%] [G loss: 0.718739]\n",
      "epoch:18 step:17074 [D loss: 0.473558, acc.: 77.34%] [G loss: 0.762938]\n",
      "epoch:18 step:17075 [D loss: 0.495572, acc.: 72.66%] [G loss: 0.845406]\n",
      "epoch:18 step:17076 [D loss: 0.598753, acc.: 66.41%] [G loss: 0.533161]\n",
      "epoch:18 step:17077 [D loss: 0.590735, acc.: 67.19%] [G loss: 0.460682]\n",
      "epoch:18 step:17078 [D loss: 0.585522, acc.: 68.75%] [G loss: 0.380487]\n",
      "epoch:18 step:17079 [D loss: 0.504102, acc.: 74.22%] [G loss: 0.452452]\n",
      "epoch:18 step:17080 [D loss: 0.633913, acc.: 64.06%] [G loss: 0.472719]\n",
      "epoch:18 step:17081 [D loss: 0.614217, acc.: 61.72%] [G loss: 0.495885]\n",
      "epoch:18 step:17082 [D loss: 0.538001, acc.: 68.75%] [G loss: 0.569152]\n",
      "epoch:18 step:17083 [D loss: 0.539402, acc.: 71.09%] [G loss: 0.642324]\n",
      "epoch:18 step:17084 [D loss: 0.443864, acc.: 81.25%] [G loss: 0.639872]\n",
      "epoch:18 step:17085 [D loss: 0.552295, acc.: 75.78%] [G loss: 0.788082]\n",
      "epoch:18 step:17086 [D loss: 0.691444, acc.: 63.28%] [G loss: 0.566252]\n",
      "epoch:18 step:17087 [D loss: 0.529917, acc.: 72.66%] [G loss: 0.614001]\n",
      "epoch:18 step:17088 [D loss: 0.498027, acc.: 78.12%] [G loss: 0.902299]\n",
      "epoch:18 step:17089 [D loss: 0.500281, acc.: 78.91%] [G loss: 0.694815]\n",
      "epoch:18 step:17090 [D loss: 0.553116, acc.: 72.66%] [G loss: 0.571346]\n",
      "epoch:18 step:17091 [D loss: 0.563094, acc.: 67.97%] [G loss: 0.661874]\n",
      "epoch:18 step:17092 [D loss: 0.602387, acc.: 62.50%] [G loss: 0.507750]\n",
      "epoch:18 step:17093 [D loss: 0.561153, acc.: 68.75%] [G loss: 0.508380]\n",
      "epoch:18 step:17094 [D loss: 0.645941, acc.: 58.59%] [G loss: 0.559969]\n",
      "epoch:18 step:17095 [D loss: 0.543578, acc.: 76.56%] [G loss: 0.456679]\n",
      "epoch:18 step:17096 [D loss: 0.558436, acc.: 70.31%] [G loss: 0.624094]\n",
      "epoch:18 step:17097 [D loss: 0.434957, acc.: 80.47%] [G loss: 0.870003]\n",
      "epoch:18 step:17098 [D loss: 0.497706, acc.: 75.78%] [G loss: 0.838292]\n",
      "epoch:18 step:17099 [D loss: 0.521688, acc.: 70.31%] [G loss: 0.754245]\n",
      "epoch:18 step:17100 [D loss: 0.554538, acc.: 70.31%] [G loss: 0.635425]\n",
      "epoch:18 step:17101 [D loss: 0.578406, acc.: 62.50%] [G loss: 0.605428]\n",
      "epoch:18 step:17102 [D loss: 0.608523, acc.: 62.50%] [G loss: 0.551679]\n",
      "epoch:18 step:17103 [D loss: 0.536644, acc.: 67.97%] [G loss: 0.632676]\n",
      "epoch:18 step:17104 [D loss: 0.562554, acc.: 69.53%] [G loss: 0.613745]\n",
      "epoch:18 step:17105 [D loss: 0.509761, acc.: 68.75%] [G loss: 0.445205]\n",
      "epoch:18 step:17106 [D loss: 0.602009, acc.: 66.41%] [G loss: 0.510426]\n",
      "epoch:18 step:17107 [D loss: 0.526318, acc.: 75.78%] [G loss: 0.541901]\n",
      "epoch:18 step:17108 [D loss: 0.510577, acc.: 75.00%] [G loss: 0.641417]\n",
      "epoch:18 step:17109 [D loss: 0.545007, acc.: 71.09%] [G loss: 0.555807]\n",
      "epoch:18 step:17110 [D loss: 0.467910, acc.: 73.44%] [G loss: 0.797991]\n",
      "epoch:18 step:17111 [D loss: 0.502295, acc.: 71.88%] [G loss: 0.764726]\n",
      "epoch:18 step:17112 [D loss: 0.546911, acc.: 71.88%] [G loss: 0.697905]\n",
      "epoch:18 step:17113 [D loss: 0.544008, acc.: 68.75%] [G loss: 0.775512]\n",
      "epoch:18 step:17114 [D loss: 0.449253, acc.: 75.00%] [G loss: 0.775998]\n",
      "epoch:18 step:17115 [D loss: 0.620800, acc.: 68.75%] [G loss: 0.942148]\n",
      "epoch:18 step:17116 [D loss: 0.641545, acc.: 63.28%] [G loss: 0.797809]\n",
      "epoch:18 step:17117 [D loss: 0.659738, acc.: 61.72%] [G loss: 0.554326]\n",
      "epoch:18 step:17118 [D loss: 0.567382, acc.: 69.53%] [G loss: 0.718567]\n",
      "epoch:18 step:17119 [D loss: 0.570007, acc.: 71.09%] [G loss: 0.623247]\n",
      "epoch:18 step:17120 [D loss: 0.543979, acc.: 71.09%] [G loss: 0.629143]\n",
      "epoch:18 step:17121 [D loss: 0.586487, acc.: 64.06%] [G loss: 0.523893]\n",
      "epoch:18 step:17122 [D loss: 0.516607, acc.: 71.09%] [G loss: 0.603023]\n",
      "epoch:18 step:17123 [D loss: 0.592321, acc.: 65.62%] [G loss: 0.648724]\n",
      "epoch:18 step:17124 [D loss: 0.514546, acc.: 70.31%] [G loss: 0.784604]\n",
      "epoch:18 step:17125 [D loss: 0.556469, acc.: 65.62%] [G loss: 0.706956]\n",
      "epoch:18 step:17126 [D loss: 0.597772, acc.: 60.94%] [G loss: 0.545264]\n",
      "epoch:18 step:17127 [D loss: 0.543321, acc.: 71.09%] [G loss: 0.665138]\n",
      "epoch:18 step:17128 [D loss: 0.551323, acc.: 69.53%] [G loss: 0.589072]\n",
      "epoch:18 step:17129 [D loss: 0.602039, acc.: 73.44%] [G loss: 0.565118]\n",
      "epoch:18 step:17130 [D loss: 0.573983, acc.: 60.94%] [G loss: 0.627360]\n",
      "epoch:18 step:17131 [D loss: 0.508358, acc.: 73.44%] [G loss: 0.715338]\n",
      "epoch:18 step:17132 [D loss: 0.601861, acc.: 67.19%] [G loss: 0.584344]\n",
      "epoch:18 step:17133 [D loss: 0.546845, acc.: 70.31%] [G loss: 0.616894]\n",
      "epoch:18 step:17134 [D loss: 0.619099, acc.: 64.06%] [G loss: 0.565068]\n",
      "epoch:18 step:17135 [D loss: 0.527853, acc.: 75.00%] [G loss: 0.529207]\n",
      "epoch:18 step:17136 [D loss: 0.465889, acc.: 79.69%] [G loss: 0.680142]\n",
      "epoch:18 step:17137 [D loss: 0.507322, acc.: 68.75%] [G loss: 0.626895]\n",
      "epoch:18 step:17138 [D loss: 0.556555, acc.: 71.88%] [G loss: 0.855819]\n",
      "epoch:18 step:17139 [D loss: 0.537915, acc.: 64.06%] [G loss: 0.641941]\n",
      "epoch:18 step:17140 [D loss: 0.515173, acc.: 78.12%] [G loss: 0.636053]\n",
      "epoch:18 step:17141 [D loss: 0.556955, acc.: 69.53%] [G loss: 0.600044]\n",
      "epoch:18 step:17142 [D loss: 0.446585, acc.: 80.47%] [G loss: 0.612298]\n",
      "epoch:18 step:17143 [D loss: 0.671804, acc.: 62.50%] [G loss: 0.565043]\n",
      "epoch:18 step:17144 [D loss: 0.659560, acc.: 59.38%] [G loss: 0.438902]\n",
      "epoch:18 step:17145 [D loss: 0.649345, acc.: 57.81%] [G loss: 0.438893]\n",
      "epoch:18 step:17146 [D loss: 0.498388, acc.: 73.44%] [G loss: 0.603210]\n",
      "epoch:18 step:17147 [D loss: 0.584629, acc.: 62.50%] [G loss: 0.524839]\n",
      "epoch:18 step:17148 [D loss: 0.577070, acc.: 66.41%] [G loss: 0.532119]\n",
      "epoch:18 step:17149 [D loss: 0.493065, acc.: 78.12%] [G loss: 0.681119]\n",
      "epoch:18 step:17150 [D loss: 0.500214, acc.: 76.56%] [G loss: 0.592250]\n",
      "epoch:18 step:17151 [D loss: 0.566710, acc.: 67.19%] [G loss: 0.666551]\n",
      "epoch:18 step:17152 [D loss: 0.505843, acc.: 68.75%] [G loss: 0.597115]\n",
      "epoch:18 step:17153 [D loss: 0.642631, acc.: 66.41%] [G loss: 0.543586]\n",
      "epoch:18 step:17154 [D loss: 0.603396, acc.: 67.19%] [G loss: 0.930532]\n",
      "epoch:18 step:17155 [D loss: 0.540265, acc.: 68.75%] [G loss: 0.766532]\n",
      "epoch:18 step:17156 [D loss: 0.567681, acc.: 69.53%] [G loss: 0.645707]\n",
      "epoch:18 step:17157 [D loss: 0.607264, acc.: 70.31%] [G loss: 0.540374]\n",
      "epoch:18 step:17158 [D loss: 0.531250, acc.: 71.88%] [G loss: 0.498558]\n",
      "epoch:18 step:17159 [D loss: 0.579033, acc.: 64.84%] [G loss: 0.482240]\n",
      "epoch:18 step:17160 [D loss: 0.608696, acc.: 57.03%] [G loss: 0.419708]\n",
      "epoch:18 step:17161 [D loss: 0.543534, acc.: 71.09%] [G loss: 0.599422]\n",
      "epoch:18 step:17162 [D loss: 0.459198, acc.: 78.12%] [G loss: 0.733252]\n",
      "epoch:18 step:17163 [D loss: 0.614579, acc.: 66.41%] [G loss: 0.609475]\n",
      "epoch:18 step:17164 [D loss: 0.486480, acc.: 81.25%] [G loss: 0.514755]\n",
      "epoch:18 step:17165 [D loss: 0.494859, acc.: 75.78%] [G loss: 0.781194]\n",
      "epoch:18 step:17166 [D loss: 0.472118, acc.: 77.34%] [G loss: 0.791845]\n",
      "epoch:18 step:17167 [D loss: 0.646868, acc.: 64.06%] [G loss: 0.628620]\n",
      "epoch:18 step:17168 [D loss: 0.528476, acc.: 72.66%] [G loss: 0.596254]\n",
      "epoch:18 step:17169 [D loss: 0.558198, acc.: 71.88%] [G loss: 0.610323]\n",
      "epoch:18 step:17170 [D loss: 0.520550, acc.: 72.66%] [G loss: 0.542536]\n",
      "epoch:18 step:17171 [D loss: 0.517997, acc.: 74.22%] [G loss: 0.768484]\n",
      "epoch:18 step:17172 [D loss: 0.537248, acc.: 67.19%] [G loss: 0.712481]\n",
      "epoch:18 step:17173 [D loss: 0.537549, acc.: 73.44%] [G loss: 0.798287]\n",
      "epoch:18 step:17174 [D loss: 0.586940, acc.: 67.97%] [G loss: 0.579671]\n",
      "epoch:18 step:17175 [D loss: 0.529395, acc.: 68.75%] [G loss: 0.586680]\n",
      "epoch:18 step:17176 [D loss: 0.611511, acc.: 64.06%] [G loss: 0.495989]\n",
      "epoch:18 step:17177 [D loss: 0.536950, acc.: 69.53%] [G loss: 0.593299]\n",
      "epoch:18 step:17178 [D loss: 0.500274, acc.: 73.44%] [G loss: 0.762843]\n",
      "epoch:18 step:17179 [D loss: 0.470854, acc.: 75.78%] [G loss: 1.076218]\n",
      "epoch:18 step:17180 [D loss: 0.481178, acc.: 78.91%] [G loss: 0.896919]\n",
      "epoch:18 step:17181 [D loss: 0.427777, acc.: 85.94%] [G loss: 1.117947]\n",
      "epoch:18 step:17182 [D loss: 0.682023, acc.: 64.84%] [G loss: 0.713282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17183 [D loss: 0.605845, acc.: 64.84%] [G loss: 0.487523]\n",
      "epoch:18 step:17184 [D loss: 0.568420, acc.: 65.62%] [G loss: 0.492379]\n",
      "epoch:18 step:17185 [D loss: 0.519661, acc.: 72.66%] [G loss: 0.574602]\n",
      "epoch:18 step:17186 [D loss: 0.550587, acc.: 73.44%] [G loss: 0.587045]\n",
      "epoch:18 step:17187 [D loss: 0.520967, acc.: 72.66%] [G loss: 0.505833]\n",
      "epoch:18 step:17188 [D loss: 0.611752, acc.: 64.84%] [G loss: 0.495485]\n",
      "epoch:18 step:17189 [D loss: 0.622477, acc.: 64.06%] [G loss: 0.572665]\n",
      "epoch:18 step:17190 [D loss: 0.556334, acc.: 71.88%] [G loss: 0.523155]\n",
      "epoch:18 step:17191 [D loss: 0.549034, acc.: 69.53%] [G loss: 0.647748]\n",
      "epoch:18 step:17192 [D loss: 0.484105, acc.: 74.22%] [G loss: 0.708259]\n",
      "epoch:18 step:17193 [D loss: 0.546673, acc.: 66.41%] [G loss: 0.669735]\n",
      "epoch:18 step:17194 [D loss: 0.497509, acc.: 76.56%] [G loss: 0.873921]\n",
      "epoch:18 step:17195 [D loss: 0.469712, acc.: 75.78%] [G loss: 0.724128]\n",
      "epoch:18 step:17196 [D loss: 0.561055, acc.: 69.53%] [G loss: 0.567967]\n",
      "epoch:18 step:17197 [D loss: 0.577991, acc.: 65.62%] [G loss: 0.492647]\n",
      "epoch:18 step:17198 [D loss: 0.540453, acc.: 70.31%] [G loss: 0.453749]\n",
      "epoch:18 step:17199 [D loss: 0.485687, acc.: 76.56%] [G loss: 0.565934]\n",
      "epoch:18 step:17200 [D loss: 0.520605, acc.: 75.00%] [G loss: 0.635982]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.743118\n",
      "FID: 44.624252\n",
      "0 = 12.880968943977338\n",
      "1 = 0.09153548014494822\n",
      "2 = 0.8619999885559082\n",
      "3 = 0.8065999746322632\n",
      "4 = 0.9174000024795532\n",
      "5 = 0.9071075320243835\n",
      "6 = 0.8065999746322632\n",
      "7 = 8.018224870836729\n",
      "8 = 0.14190599010639462\n",
      "9 = 0.6966999769210815\n",
      "10 = 0.6912000179290771\n",
      "11 = 0.7021999955177307\n",
      "12 = 0.6988877654075623\n",
      "13 = 0.6912000179290771\n",
      "14 = 6.743147373199463\n",
      "15 = 7.118908882141113\n",
      "16 = 0.3769032657146454\n",
      "17 = 6.7431182861328125\n",
      "18 = 44.62425231933594\n",
      "epoch:18 step:17201 [D loss: 0.539819, acc.: 70.31%] [G loss: 0.622667]\n",
      "epoch:18 step:17202 [D loss: 0.482827, acc.: 77.34%] [G loss: 0.918069]\n",
      "epoch:18 step:17203 [D loss: 0.495863, acc.: 78.12%] [G loss: 0.620558]\n",
      "epoch:18 step:17204 [D loss: 0.529661, acc.: 69.53%] [G loss: 0.744055]\n",
      "epoch:18 step:17205 [D loss: 0.614217, acc.: 64.84%] [G loss: 0.562228]\n",
      "epoch:18 step:17206 [D loss: 0.522994, acc.: 71.88%] [G loss: 0.634886]\n",
      "epoch:18 step:17207 [D loss: 0.652751, acc.: 66.41%] [G loss: 0.693850]\n",
      "epoch:18 step:17208 [D loss: 0.684669, acc.: 61.72%] [G loss: 0.821072]\n",
      "epoch:18 step:17209 [D loss: 0.486159, acc.: 77.34%] [G loss: 0.829057]\n",
      "epoch:18 step:17210 [D loss: 0.457429, acc.: 78.12%] [G loss: 0.880007]\n",
      "epoch:18 step:17211 [D loss: 0.624835, acc.: 59.38%] [G loss: 0.770151]\n",
      "epoch:18 step:17212 [D loss: 0.527755, acc.: 63.28%] [G loss: 0.855201]\n",
      "epoch:18 step:17213 [D loss: 0.424238, acc.: 79.69%] [G loss: 0.928469]\n",
      "epoch:18 step:17214 [D loss: 0.655652, acc.: 64.84%] [G loss: 0.814666]\n",
      "epoch:18 step:17215 [D loss: 0.738274, acc.: 55.47%] [G loss: 0.468433]\n",
      "epoch:18 step:17216 [D loss: 0.488000, acc.: 76.56%] [G loss: 0.605251]\n",
      "epoch:18 step:17217 [D loss: 0.530481, acc.: 71.09%] [G loss: 0.671446]\n",
      "epoch:18 step:17218 [D loss: 0.515194, acc.: 73.44%] [G loss: 0.792349]\n",
      "epoch:18 step:17219 [D loss: 0.573556, acc.: 65.62%] [G loss: 0.640808]\n",
      "epoch:18 step:17220 [D loss: 0.423257, acc.: 79.69%] [G loss: 0.784857]\n",
      "epoch:18 step:17221 [D loss: 0.560870, acc.: 71.09%] [G loss: 0.852855]\n",
      "epoch:18 step:17222 [D loss: 0.520346, acc.: 74.22%] [G loss: 0.832307]\n",
      "epoch:18 step:17223 [D loss: 0.451367, acc.: 78.12%] [G loss: 0.754489]\n",
      "epoch:18 step:17224 [D loss: 0.461733, acc.: 81.25%] [G loss: 0.768599]\n",
      "epoch:18 step:17225 [D loss: 0.411403, acc.: 82.03%] [G loss: 0.715945]\n",
      "epoch:18 step:17226 [D loss: 0.504011, acc.: 75.00%] [G loss: 0.796572]\n",
      "epoch:18 step:17227 [D loss: 0.492383, acc.: 75.78%] [G loss: 0.839883]\n",
      "epoch:18 step:17228 [D loss: 0.603780, acc.: 67.19%] [G loss: 0.650153]\n",
      "epoch:18 step:17229 [D loss: 0.579117, acc.: 67.97%] [G loss: 0.635670]\n",
      "epoch:18 step:17230 [D loss: 0.558310, acc.: 65.62%] [G loss: 0.580266]\n",
      "epoch:18 step:17231 [D loss: 0.568258, acc.: 67.19%] [G loss: 0.574892]\n",
      "epoch:18 step:17232 [D loss: 0.542993, acc.: 70.31%] [G loss: 0.699257]\n",
      "epoch:18 step:17233 [D loss: 0.570995, acc.: 65.62%] [G loss: 0.662863]\n",
      "epoch:18 step:17234 [D loss: 0.544504, acc.: 72.66%] [G loss: 0.562958]\n",
      "epoch:18 step:17235 [D loss: 0.468883, acc.: 75.78%] [G loss: 0.758950]\n",
      "epoch:18 step:17236 [D loss: 0.542159, acc.: 74.22%] [G loss: 0.661857]\n",
      "epoch:18 step:17237 [D loss: 0.536874, acc.: 67.97%] [G loss: 0.706596]\n",
      "epoch:18 step:17238 [D loss: 0.511022, acc.: 74.22%] [G loss: 0.582318]\n",
      "epoch:18 step:17239 [D loss: 0.558170, acc.: 68.75%] [G loss: 0.777087]\n",
      "epoch:18 step:17240 [D loss: 0.492828, acc.: 77.34%] [G loss: 0.816754]\n",
      "epoch:18 step:17241 [D loss: 0.613059, acc.: 64.84%] [G loss: 0.671589]\n",
      "epoch:18 step:17242 [D loss: 0.662415, acc.: 59.38%] [G loss: 0.542113]\n",
      "epoch:18 step:17243 [D loss: 0.591239, acc.: 65.62%] [G loss: 0.467317]\n",
      "epoch:18 step:17244 [D loss: 0.556933, acc.: 72.66%] [G loss: 0.486752]\n",
      "epoch:18 step:17245 [D loss: 0.577163, acc.: 70.31%] [G loss: 0.647281]\n",
      "epoch:18 step:17246 [D loss: 0.567734, acc.: 73.44%] [G loss: 0.629668]\n",
      "epoch:18 step:17247 [D loss: 0.494146, acc.: 75.00%] [G loss: 0.646703]\n",
      "epoch:18 step:17248 [D loss: 0.543537, acc.: 70.31%] [G loss: 0.743575]\n",
      "epoch:18 step:17249 [D loss: 0.610405, acc.: 64.06%] [G loss: 0.578716]\n",
      "epoch:18 step:17250 [D loss: 0.513436, acc.: 74.22%] [G loss: 0.659101]\n",
      "epoch:18 step:17251 [D loss: 0.515359, acc.: 71.88%] [G loss: 0.562516]\n",
      "epoch:18 step:17252 [D loss: 0.613303, acc.: 67.97%] [G loss: 0.549935]\n",
      "epoch:18 step:17253 [D loss: 0.523739, acc.: 73.44%] [G loss: 0.607266]\n",
      "epoch:18 step:17254 [D loss: 0.593820, acc.: 66.41%] [G loss: 0.593789]\n",
      "epoch:18 step:17255 [D loss: 0.475830, acc.: 76.56%] [G loss: 0.691360]\n",
      "epoch:18 step:17256 [D loss: 0.678649, acc.: 58.59%] [G loss: 0.564831]\n",
      "epoch:18 step:17257 [D loss: 0.526439, acc.: 68.75%] [G loss: 0.525319]\n",
      "epoch:18 step:17258 [D loss: 0.498768, acc.: 75.78%] [G loss: 0.665327]\n",
      "epoch:18 step:17259 [D loss: 0.584644, acc.: 66.41%] [G loss: 0.589056]\n",
      "epoch:18 step:17260 [D loss: 0.564769, acc.: 71.09%] [G loss: 0.582448]\n",
      "epoch:18 step:17261 [D loss: 0.562789, acc.: 67.97%] [G loss: 0.479247]\n",
      "epoch:18 step:17262 [D loss: 0.572300, acc.: 66.41%] [G loss: 0.533046]\n",
      "epoch:18 step:17263 [D loss: 0.591112, acc.: 63.28%] [G loss: 0.601750]\n",
      "epoch:18 step:17264 [D loss: 0.476768, acc.: 74.22%] [G loss: 0.734618]\n",
      "epoch:18 step:17265 [D loss: 0.537178, acc.: 75.00%] [G loss: 0.696214]\n",
      "epoch:18 step:17266 [D loss: 0.599352, acc.: 67.19%] [G loss: 0.634994]\n",
      "epoch:18 step:17267 [D loss: 0.669134, acc.: 53.12%] [G loss: 0.451288]\n",
      "epoch:18 step:17268 [D loss: 0.503267, acc.: 72.66%] [G loss: 0.610490]\n",
      "epoch:18 step:17269 [D loss: 0.483756, acc.: 74.22%] [G loss: 0.707140]\n",
      "epoch:18 step:17270 [D loss: 0.612338, acc.: 60.16%] [G loss: 0.503050]\n",
      "epoch:18 step:17271 [D loss: 0.565951, acc.: 62.50%] [G loss: 0.616426]\n",
      "epoch:18 step:17272 [D loss: 0.457021, acc.: 76.56%] [G loss: 0.794279]\n",
      "epoch:18 step:17273 [D loss: 0.575729, acc.: 67.19%] [G loss: 0.648707]\n",
      "epoch:18 step:17274 [D loss: 0.608275, acc.: 68.75%] [G loss: 0.638444]\n",
      "epoch:18 step:17275 [D loss: 0.545644, acc.: 68.75%] [G loss: 0.657296]\n",
      "epoch:18 step:17276 [D loss: 0.589602, acc.: 65.62%] [G loss: 0.615351]\n",
      "epoch:18 step:17277 [D loss: 0.581843, acc.: 67.19%] [G loss: 0.563985]\n",
      "epoch:18 step:17278 [D loss: 0.630088, acc.: 65.62%] [G loss: 0.504059]\n",
      "epoch:18 step:17279 [D loss: 0.547344, acc.: 71.88%] [G loss: 0.485220]\n",
      "epoch:18 step:17280 [D loss: 0.598541, acc.: 64.84%] [G loss: 0.645537]\n",
      "epoch:18 step:17281 [D loss: 0.523894, acc.: 75.00%] [G loss: 0.698750]\n",
      "epoch:18 step:17282 [D loss: 0.503270, acc.: 73.44%] [G loss: 0.795967]\n",
      "epoch:18 step:17283 [D loss: 0.564684, acc.: 71.88%] [G loss: 0.716586]\n",
      "epoch:18 step:17284 [D loss: 0.647790, acc.: 60.94%] [G loss: 0.522229]\n",
      "epoch:18 step:17285 [D loss: 0.594741, acc.: 60.94%] [G loss: 0.474405]\n",
      "epoch:18 step:17286 [D loss: 0.592296, acc.: 62.50%] [G loss: 0.815451]\n",
      "epoch:18 step:17287 [D loss: 0.644251, acc.: 58.59%] [G loss: 0.456333]\n",
      "epoch:18 step:17288 [D loss: 0.578474, acc.: 69.53%] [G loss: 0.524717]\n",
      "epoch:18 step:17289 [D loss: 0.589640, acc.: 68.75%] [G loss: 0.534368]\n",
      "epoch:18 step:17290 [D loss: 0.620826, acc.: 60.16%] [G loss: 0.568563]\n",
      "epoch:18 step:17291 [D loss: 0.536585, acc.: 69.53%] [G loss: 0.755783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17292 [D loss: 0.520053, acc.: 73.44%] [G loss: 0.609433]\n",
      "epoch:18 step:17293 [D loss: 0.505815, acc.: 75.78%] [G loss: 0.812829]\n",
      "epoch:18 step:17294 [D loss: 0.532998, acc.: 70.31%] [G loss: 0.718138]\n",
      "epoch:18 step:17295 [D loss: 0.441429, acc.: 80.47%] [G loss: 0.697195]\n",
      "epoch:18 step:17296 [D loss: 0.505107, acc.: 78.12%] [G loss: 0.763614]\n",
      "epoch:18 step:17297 [D loss: 0.549155, acc.: 67.19%] [G loss: 0.828398]\n",
      "epoch:18 step:17298 [D loss: 0.553334, acc.: 67.97%] [G loss: 0.725611]\n",
      "epoch:18 step:17299 [D loss: 0.550444, acc.: 70.31%] [G loss: 0.633368]\n",
      "epoch:18 step:17300 [D loss: 0.525616, acc.: 74.22%] [G loss: 0.570990]\n",
      "epoch:18 step:17301 [D loss: 0.547712, acc.: 66.41%] [G loss: 0.801019]\n",
      "epoch:18 step:17302 [D loss: 0.529418, acc.: 75.78%] [G loss: 0.802984]\n",
      "epoch:18 step:17303 [D loss: 0.676948, acc.: 60.94%] [G loss: 0.582969]\n",
      "epoch:18 step:17304 [D loss: 0.563276, acc.: 65.62%] [G loss: 0.664654]\n",
      "epoch:18 step:17305 [D loss: 0.531840, acc.: 72.66%] [G loss: 0.706834]\n",
      "epoch:18 step:17306 [D loss: 0.485183, acc.: 76.56%] [G loss: 0.634805]\n",
      "epoch:18 step:17307 [D loss: 0.550448, acc.: 70.31%] [G loss: 0.684200]\n",
      "epoch:18 step:17308 [D loss: 0.593497, acc.: 64.84%] [G loss: 0.612904]\n",
      "epoch:18 step:17309 [D loss: 0.526951, acc.: 73.44%] [G loss: 0.676416]\n",
      "epoch:18 step:17310 [D loss: 0.519726, acc.: 73.44%] [G loss: 0.622657]\n",
      "epoch:18 step:17311 [D loss: 0.566449, acc.: 64.06%] [G loss: 0.836495]\n",
      "epoch:18 step:17312 [D loss: 0.546929, acc.: 73.44%] [G loss: 0.760231]\n",
      "epoch:18 step:17313 [D loss: 0.535614, acc.: 71.09%] [G loss: 0.460285]\n",
      "epoch:18 step:17314 [D loss: 0.538464, acc.: 78.12%] [G loss: 0.674087]\n",
      "epoch:18 step:17315 [D loss: 0.495394, acc.: 77.34%] [G loss: 0.717631]\n",
      "epoch:18 step:17316 [D loss: 0.474631, acc.: 79.69%] [G loss: 0.709232]\n",
      "epoch:18 step:17317 [D loss: 0.424269, acc.: 77.34%] [G loss: 0.941073]\n",
      "epoch:18 step:17318 [D loss: 0.511263, acc.: 72.66%] [G loss: 0.791600]\n",
      "epoch:18 step:17319 [D loss: 0.512602, acc.: 71.88%] [G loss: 0.691408]\n",
      "epoch:18 step:17320 [D loss: 0.569962, acc.: 72.66%] [G loss: 0.727099]\n",
      "epoch:18 step:17321 [D loss: 0.572046, acc.: 68.75%] [G loss: 0.651476]\n",
      "epoch:18 step:17322 [D loss: 0.661162, acc.: 60.16%] [G loss: 0.511681]\n",
      "epoch:18 step:17323 [D loss: 0.523862, acc.: 75.00%] [G loss: 0.699704]\n",
      "epoch:18 step:17324 [D loss: 0.631180, acc.: 63.28%] [G loss: 0.556572]\n",
      "epoch:18 step:17325 [D loss: 0.585768, acc.: 67.19%] [G loss: 0.541771]\n",
      "epoch:18 step:17326 [D loss: 0.484784, acc.: 75.78%] [G loss: 0.745261]\n",
      "epoch:18 step:17327 [D loss: 0.508442, acc.: 75.00%] [G loss: 0.710413]\n",
      "epoch:18 step:17328 [D loss: 0.529505, acc.: 69.53%] [G loss: 0.557261]\n",
      "epoch:18 step:17329 [D loss: 0.523454, acc.: 71.88%] [G loss: 0.700078]\n",
      "epoch:18 step:17330 [D loss: 0.539705, acc.: 72.66%] [G loss: 0.628376]\n",
      "epoch:18 step:17331 [D loss: 0.639470, acc.: 64.06%] [G loss: 0.627327]\n",
      "epoch:18 step:17332 [D loss: 0.589287, acc.: 64.06%] [G loss: 0.526265]\n",
      "epoch:18 step:17333 [D loss: 0.535463, acc.: 69.53%] [G loss: 0.556943]\n",
      "epoch:18 step:17334 [D loss: 0.552305, acc.: 69.53%] [G loss: 0.806944]\n",
      "epoch:18 step:17335 [D loss: 0.548317, acc.: 71.09%] [G loss: 0.681235]\n",
      "epoch:18 step:17336 [D loss: 0.522853, acc.: 73.44%] [G loss: 0.668390]\n",
      "epoch:18 step:17337 [D loss: 0.423628, acc.: 82.03%] [G loss: 0.835137]\n",
      "epoch:18 step:17338 [D loss: 0.475394, acc.: 76.56%] [G loss: 0.837686]\n",
      "epoch:18 step:17339 [D loss: 0.628140, acc.: 61.72%] [G loss: 0.741200]\n",
      "epoch:18 step:17340 [D loss: 0.509588, acc.: 72.66%] [G loss: 0.786505]\n",
      "epoch:18 step:17341 [D loss: 0.476728, acc.: 78.12%] [G loss: 0.912482]\n",
      "epoch:18 step:17342 [D loss: 0.572059, acc.: 71.88%] [G loss: 0.687428]\n",
      "epoch:18 step:17343 [D loss: 0.678755, acc.: 62.50%] [G loss: 0.498137]\n",
      "epoch:18 step:17344 [D loss: 0.586012, acc.: 66.41%] [G loss: 0.382525]\n",
      "epoch:18 step:17345 [D loss: 0.542397, acc.: 67.97%] [G loss: 0.589539]\n",
      "epoch:18 step:17346 [D loss: 0.578709, acc.: 68.75%] [G loss: 0.561243]\n",
      "epoch:18 step:17347 [D loss: 0.492947, acc.: 78.12%] [G loss: 0.578858]\n",
      "epoch:18 step:17348 [D loss: 0.634004, acc.: 66.41%] [G loss: 0.499430]\n",
      "epoch:18 step:17349 [D loss: 0.581406, acc.: 64.06%] [G loss: 0.630740]\n",
      "epoch:18 step:17350 [D loss: 0.528507, acc.: 70.31%] [G loss: 0.691424]\n",
      "epoch:18 step:17351 [D loss: 0.542049, acc.: 74.22%] [G loss: 0.601549]\n",
      "epoch:18 step:17352 [D loss: 0.558475, acc.: 70.31%] [G loss: 0.629905]\n",
      "epoch:18 step:17353 [D loss: 0.563833, acc.: 67.19%] [G loss: 0.539301]\n",
      "epoch:18 step:17354 [D loss: 0.560555, acc.: 66.41%] [G loss: 0.766420]\n",
      "epoch:18 step:17355 [D loss: 0.540721, acc.: 71.88%] [G loss: 0.608864]\n",
      "epoch:18 step:17356 [D loss: 0.561863, acc.: 72.66%] [G loss: 0.483011]\n",
      "epoch:18 step:17357 [D loss: 0.543400, acc.: 71.88%] [G loss: 0.687970]\n",
      "epoch:18 step:17358 [D loss: 0.576130, acc.: 67.97%] [G loss: 0.644600]\n",
      "epoch:18 step:17359 [D loss: 0.539479, acc.: 71.88%] [G loss: 0.662145]\n",
      "epoch:18 step:17360 [D loss: 0.600041, acc.: 66.41%] [G loss: 0.566337]\n",
      "epoch:18 step:17361 [D loss: 0.469343, acc.: 76.56%] [G loss: 0.645221]\n",
      "epoch:18 step:17362 [D loss: 0.545974, acc.: 75.78%] [G loss: 0.736179]\n",
      "epoch:18 step:17363 [D loss: 0.581324, acc.: 67.97%] [G loss: 0.631721]\n",
      "epoch:18 step:17364 [D loss: 0.556970, acc.: 68.75%] [G loss: 0.786725]\n",
      "epoch:18 step:17365 [D loss: 0.465790, acc.: 82.03%] [G loss: 0.824302]\n",
      "epoch:18 step:17366 [D loss: 0.523642, acc.: 76.56%] [G loss: 0.676840]\n",
      "epoch:18 step:17367 [D loss: 0.633065, acc.: 64.06%] [G loss: 0.534314]\n",
      "epoch:18 step:17368 [D loss: 0.607293, acc.: 63.28%] [G loss: 0.405170]\n",
      "epoch:18 step:17369 [D loss: 0.494814, acc.: 75.00%] [G loss: 0.614119]\n",
      "epoch:18 step:17370 [D loss: 0.500745, acc.: 76.56%] [G loss: 0.675406]\n",
      "epoch:18 step:17371 [D loss: 0.478740, acc.: 78.12%] [G loss: 0.583392]\n",
      "epoch:18 step:17372 [D loss: 0.497636, acc.: 72.66%] [G loss: 0.685731]\n",
      "epoch:18 step:17373 [D loss: 0.535841, acc.: 73.44%] [G loss: 0.759538]\n",
      "epoch:18 step:17374 [D loss: 0.387886, acc.: 85.94%] [G loss: 0.844573]\n",
      "epoch:18 step:17375 [D loss: 0.550143, acc.: 68.75%] [G loss: 0.733022]\n",
      "epoch:18 step:17376 [D loss: 0.630813, acc.: 65.62%] [G loss: 0.638686]\n",
      "epoch:18 step:17377 [D loss: 0.676332, acc.: 57.81%] [G loss: 0.554899]\n",
      "epoch:18 step:17378 [D loss: 0.633395, acc.: 64.06%] [G loss: 0.460887]\n",
      "epoch:18 step:17379 [D loss: 0.517394, acc.: 71.88%] [G loss: 0.514310]\n",
      "epoch:18 step:17380 [D loss: 0.466045, acc.: 78.91%] [G loss: 0.718856]\n",
      "epoch:18 step:17381 [D loss: 0.553804, acc.: 71.09%] [G loss: 0.621694]\n",
      "epoch:18 step:17382 [D loss: 0.480035, acc.: 80.47%] [G loss: 0.919500]\n",
      "epoch:18 step:17383 [D loss: 0.521308, acc.: 72.66%] [G loss: 0.653755]\n",
      "epoch:18 step:17384 [D loss: 0.523925, acc.: 75.00%] [G loss: 0.754833]\n",
      "epoch:18 step:17385 [D loss: 0.502868, acc.: 76.56%] [G loss: 0.576830]\n",
      "epoch:18 step:17386 [D loss: 0.486870, acc.: 75.78%] [G loss: 0.892380]\n",
      "epoch:18 step:17387 [D loss: 0.467220, acc.: 79.69%] [G loss: 0.821838]\n",
      "epoch:18 step:17388 [D loss: 0.479248, acc.: 77.34%] [G loss: 0.710133]\n",
      "epoch:18 step:17389 [D loss: 0.472294, acc.: 75.00%] [G loss: 0.765618]\n",
      "epoch:18 step:17390 [D loss: 0.564964, acc.: 67.97%] [G loss: 0.769782]\n",
      "epoch:18 step:17391 [D loss: 0.618478, acc.: 62.50%] [G loss: 0.589318]\n",
      "epoch:18 step:17392 [D loss: 0.523231, acc.: 71.09%] [G loss: 0.613353]\n",
      "epoch:18 step:17393 [D loss: 0.608470, acc.: 62.50%] [G loss: 0.616477]\n",
      "epoch:18 step:17394 [D loss: 0.696107, acc.: 53.91%] [G loss: 0.432724]\n",
      "epoch:18 step:17395 [D loss: 0.597081, acc.: 67.97%] [G loss: 0.638879]\n",
      "epoch:18 step:17396 [D loss: 0.582331, acc.: 67.97%] [G loss: 0.611402]\n",
      "epoch:18 step:17397 [D loss: 0.572543, acc.: 67.19%] [G loss: 0.785612]\n",
      "epoch:18 step:17398 [D loss: 0.527614, acc.: 70.31%] [G loss: 0.689350]\n",
      "epoch:18 step:17399 [D loss: 0.520033, acc.: 71.09%] [G loss: 0.717834]\n",
      "epoch:18 step:17400 [D loss: 0.524634, acc.: 75.00%] [G loss: 0.751685]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.624909\n",
      "FID: 46.745441\n",
      "0 = 12.990507306003561\n",
      "1 = 0.09301661693647083\n",
      "2 = 0.8756999969482422\n",
      "3 = 0.8276000022888184\n",
      "4 = 0.923799991607666\n",
      "5 = 0.9156892895698547\n",
      "6 = 0.8276000022888184\n",
      "7 = 8.165711078286183\n",
      "8 = 0.14769963025851968\n",
      "9 = 0.7031999826431274\n",
      "10 = 0.6949999928474426\n",
      "11 = 0.7113999724388123\n",
      "12 = 0.7065880298614502\n",
      "13 = 0.6949999928474426\n",
      "14 = 6.624936580657959\n",
      "15 = 7.0462517738342285\n",
      "16 = 0.38390839099884033\n",
      "17 = 6.624909400939941\n",
      "18 = 46.74544143676758\n",
      "epoch:18 step:17401 [D loss: 0.635422, acc.: 67.19%] [G loss: 0.710271]\n",
      "epoch:18 step:17402 [D loss: 0.522503, acc.: 70.31%] [G loss: 0.480094]\n",
      "epoch:18 step:17403 [D loss: 0.547236, acc.: 72.66%] [G loss: 0.576420]\n",
      "epoch:18 step:17404 [D loss: 0.539847, acc.: 76.56%] [G loss: 0.639712]\n",
      "epoch:18 step:17405 [D loss: 0.553783, acc.: 67.97%] [G loss: 0.543262]\n",
      "epoch:18 step:17406 [D loss: 0.565524, acc.: 67.19%] [G loss: 0.716853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17407 [D loss: 0.527346, acc.: 74.22%] [G loss: 0.650058]\n",
      "epoch:18 step:17408 [D loss: 0.654736, acc.: 60.94%] [G loss: 0.478089]\n",
      "epoch:18 step:17409 [D loss: 0.536691, acc.: 71.88%] [G loss: 0.694316]\n",
      "epoch:18 step:17410 [D loss: 0.533048, acc.: 71.88%] [G loss: 0.671607]\n",
      "epoch:18 step:17411 [D loss: 0.501696, acc.: 72.66%] [G loss: 0.689742]\n",
      "epoch:18 step:17412 [D loss: 0.511719, acc.: 74.22%] [G loss: 0.890142]\n",
      "epoch:18 step:17413 [D loss: 0.587289, acc.: 70.31%] [G loss: 0.759145]\n",
      "epoch:18 step:17414 [D loss: 0.537971, acc.: 69.53%] [G loss: 0.637323]\n",
      "epoch:18 step:17415 [D loss: 0.517762, acc.: 72.66%] [G loss: 0.591345]\n",
      "epoch:18 step:17416 [D loss: 0.529116, acc.: 75.78%] [G loss: 0.672409]\n",
      "epoch:18 step:17417 [D loss: 0.501302, acc.: 75.00%] [G loss: 0.541742]\n",
      "epoch:18 step:17418 [D loss: 0.456032, acc.: 79.69%] [G loss: 0.758733]\n",
      "epoch:18 step:17419 [D loss: 0.576564, acc.: 67.97%] [G loss: 0.614616]\n",
      "epoch:18 step:17420 [D loss: 0.444511, acc.: 79.69%] [G loss: 0.651800]\n",
      "epoch:18 step:17421 [D loss: 0.473456, acc.: 77.34%] [G loss: 0.772265]\n",
      "epoch:18 step:17422 [D loss: 0.546791, acc.: 72.66%] [G loss: 0.767923]\n",
      "epoch:18 step:17423 [D loss: 0.454487, acc.: 76.56%] [G loss: 0.737857]\n",
      "epoch:18 step:17424 [D loss: 0.506849, acc.: 73.44%] [G loss: 0.805502]\n",
      "epoch:18 step:17425 [D loss: 0.592124, acc.: 64.06%] [G loss: 0.712673]\n",
      "epoch:18 step:17426 [D loss: 0.542539, acc.: 71.09%] [G loss: 0.609146]\n",
      "epoch:18 step:17427 [D loss: 0.592859, acc.: 63.28%] [G loss: 0.585013]\n",
      "epoch:18 step:17428 [D loss: 0.625252, acc.: 64.84%] [G loss: 0.462656]\n",
      "epoch:18 step:17429 [D loss: 0.588804, acc.: 64.06%] [G loss: 0.579637]\n",
      "epoch:18 step:17430 [D loss: 0.493930, acc.: 78.12%] [G loss: 0.665994]\n",
      "epoch:18 step:17431 [D loss: 0.582276, acc.: 71.09%] [G loss: 0.707819]\n",
      "epoch:18 step:17432 [D loss: 0.640941, acc.: 66.41%] [G loss: 0.717941]\n",
      "epoch:18 step:17433 [D loss: 0.480174, acc.: 77.34%] [G loss: 0.663626]\n",
      "epoch:18 step:17434 [D loss: 0.529562, acc.: 70.31%] [G loss: 0.741241]\n",
      "epoch:18 step:17435 [D loss: 0.587085, acc.: 71.09%] [G loss: 0.551395]\n",
      "epoch:18 step:17436 [D loss: 0.557503, acc.: 71.09%] [G loss: 0.475137]\n",
      "epoch:18 step:17437 [D loss: 0.566309, acc.: 69.53%] [G loss: 0.617978]\n",
      "epoch:18 step:17438 [D loss: 0.571789, acc.: 69.53%] [G loss: 0.576016]\n",
      "epoch:18 step:17439 [D loss: 0.542523, acc.: 67.97%] [G loss: 0.759721]\n",
      "epoch:18 step:17440 [D loss: 0.483180, acc.: 72.66%] [G loss: 0.712232]\n",
      "epoch:18 step:17441 [D loss: 0.573139, acc.: 70.31%] [G loss: 0.691576]\n",
      "epoch:18 step:17442 [D loss: 0.637261, acc.: 67.19%] [G loss: 0.637167]\n",
      "epoch:18 step:17443 [D loss: 0.531805, acc.: 75.00%] [G loss: 0.549386]\n",
      "epoch:18 step:17444 [D loss: 0.575010, acc.: 64.06%] [G loss: 0.540327]\n",
      "epoch:18 step:17445 [D loss: 0.589603, acc.: 67.97%] [G loss: 0.686021]\n",
      "epoch:18 step:17446 [D loss: 0.541471, acc.: 70.31%] [G loss: 0.706949]\n",
      "epoch:18 step:17447 [D loss: 0.544911, acc.: 71.09%] [G loss: 0.563491]\n",
      "epoch:18 step:17448 [D loss: 0.482001, acc.: 75.78%] [G loss: 0.736663]\n",
      "epoch:18 step:17449 [D loss: 0.536244, acc.: 71.09%] [G loss: 0.709517]\n",
      "epoch:18 step:17450 [D loss: 0.615093, acc.: 60.16%] [G loss: 0.632459]\n",
      "epoch:18 step:17451 [D loss: 0.571340, acc.: 67.19%] [G loss: 0.583615]\n",
      "epoch:18 step:17452 [D loss: 0.591375, acc.: 67.97%] [G loss: 0.556064]\n",
      "epoch:18 step:17453 [D loss: 0.543230, acc.: 68.75%] [G loss: 0.464118]\n",
      "epoch:18 step:17454 [D loss: 0.529766, acc.: 72.66%] [G loss: 0.557799]\n",
      "epoch:18 step:17455 [D loss: 0.502035, acc.: 74.22%] [G loss: 1.029407]\n",
      "epoch:18 step:17456 [D loss: 0.492204, acc.: 73.44%] [G loss: 0.749467]\n",
      "epoch:18 step:17457 [D loss: 0.584627, acc.: 67.97%] [G loss: 0.657747]\n",
      "epoch:18 step:17458 [D loss: 0.515373, acc.: 71.09%] [G loss: 0.634935]\n",
      "epoch:18 step:17459 [D loss: 0.529160, acc.: 75.00%] [G loss: 0.588697]\n",
      "epoch:18 step:17460 [D loss: 0.569165, acc.: 72.66%] [G loss: 0.655118]\n",
      "epoch:18 step:17461 [D loss: 0.540234, acc.: 71.88%] [G loss: 0.531823]\n",
      "epoch:18 step:17462 [D loss: 0.546231, acc.: 71.09%] [G loss: 0.644062]\n",
      "epoch:18 step:17463 [D loss: 0.522875, acc.: 75.78%] [G loss: 0.574731]\n",
      "epoch:18 step:17464 [D loss: 0.526636, acc.: 75.78%] [G loss: 0.702155]\n",
      "epoch:18 step:17465 [D loss: 0.548396, acc.: 71.09%] [G loss: 0.721278]\n",
      "epoch:18 step:17466 [D loss: 0.618360, acc.: 63.28%] [G loss: 0.541051]\n",
      "epoch:18 step:17467 [D loss: 0.516437, acc.: 71.88%] [G loss: 0.583958]\n",
      "epoch:18 step:17468 [D loss: 0.495959, acc.: 71.88%] [G loss: 0.621682]\n",
      "epoch:18 step:17469 [D loss: 0.498058, acc.: 75.00%] [G loss: 0.595360]\n",
      "epoch:18 step:17470 [D loss: 0.601291, acc.: 67.19%] [G loss: 0.539198]\n",
      "epoch:18 step:17471 [D loss: 0.472089, acc.: 77.34%] [G loss: 0.759759]\n",
      "epoch:18 step:17472 [D loss: 0.588359, acc.: 68.75%] [G loss: 0.645881]\n",
      "epoch:18 step:17473 [D loss: 0.573993, acc.: 66.41%] [G loss: 0.556050]\n",
      "epoch:18 step:17474 [D loss: 0.535720, acc.: 68.75%] [G loss: 0.693832]\n",
      "epoch:18 step:17475 [D loss: 0.516937, acc.: 73.44%] [G loss: 0.539059]\n",
      "epoch:18 step:17476 [D loss: 0.594641, acc.: 63.28%] [G loss: 0.583656]\n",
      "epoch:18 step:17477 [D loss: 0.516325, acc.: 73.44%] [G loss: 0.446968]\n",
      "epoch:18 step:17478 [D loss: 0.544615, acc.: 73.44%] [G loss: 0.515672]\n",
      "epoch:18 step:17479 [D loss: 0.488696, acc.: 75.00%] [G loss: 0.589654]\n",
      "epoch:18 step:17480 [D loss: 0.590166, acc.: 62.50%] [G loss: 0.701307]\n",
      "epoch:18 step:17481 [D loss: 0.601838, acc.: 66.41%] [G loss: 0.681791]\n",
      "epoch:18 step:17482 [D loss: 0.593044, acc.: 64.84%] [G loss: 0.661269]\n",
      "epoch:18 step:17483 [D loss: 0.559208, acc.: 71.88%] [G loss: 0.756013]\n",
      "epoch:18 step:17484 [D loss: 0.565211, acc.: 67.19%] [G loss: 0.616880]\n",
      "epoch:18 step:17485 [D loss: 0.540974, acc.: 66.41%] [G loss: 0.713341]\n",
      "epoch:18 step:17486 [D loss: 0.523709, acc.: 73.44%] [G loss: 0.897319]\n",
      "epoch:18 step:17487 [D loss: 0.575946, acc.: 67.19%] [G loss: 0.765749]\n",
      "epoch:18 step:17488 [D loss: 0.569645, acc.: 68.75%] [G loss: 0.635484]\n",
      "epoch:18 step:17489 [D loss: 0.474313, acc.: 76.56%] [G loss: 0.783747]\n",
      "epoch:18 step:17490 [D loss: 0.534724, acc.: 72.66%] [G loss: 0.712796]\n",
      "epoch:18 step:17491 [D loss: 0.589667, acc.: 66.41%] [G loss: 0.483102]\n",
      "epoch:18 step:17492 [D loss: 0.548912, acc.: 69.53%] [G loss: 0.604716]\n",
      "epoch:18 step:17493 [D loss: 0.533067, acc.: 67.97%] [G loss: 0.475390]\n",
      "epoch:18 step:17494 [D loss: 0.572394, acc.: 68.75%] [G loss: 0.602667]\n",
      "epoch:18 step:17495 [D loss: 0.559619, acc.: 66.41%] [G loss: 0.557147]\n",
      "epoch:18 step:17496 [D loss: 0.531454, acc.: 73.44%] [G loss: 0.614800]\n",
      "epoch:18 step:17497 [D loss: 0.526486, acc.: 71.88%] [G loss: 0.664856]\n",
      "epoch:18 step:17498 [D loss: 0.475193, acc.: 77.34%] [G loss: 0.580656]\n",
      "epoch:18 step:17499 [D loss: 0.494644, acc.: 79.69%] [G loss: 0.681375]\n",
      "epoch:18 step:17500 [D loss: 0.479145, acc.: 74.22%] [G loss: 0.909002]\n",
      "epoch:18 step:17501 [D loss: 0.473182, acc.: 75.00%] [G loss: 0.893656]\n",
      "epoch:18 step:17502 [D loss: 0.592884, acc.: 70.31%] [G loss: 0.672289]\n",
      "epoch:18 step:17503 [D loss: 0.542890, acc.: 73.44%] [G loss: 0.586168]\n",
      "epoch:18 step:17504 [D loss: 0.525502, acc.: 73.44%] [G loss: 0.559487]\n",
      "epoch:18 step:17505 [D loss: 0.522454, acc.: 68.75%] [G loss: 0.600397]\n",
      "epoch:18 step:17506 [D loss: 0.578506, acc.: 59.38%] [G loss: 0.844956]\n",
      "epoch:18 step:17507 [D loss: 0.518529, acc.: 72.66%] [G loss: 0.702153]\n",
      "epoch:18 step:17508 [D loss: 0.486778, acc.: 77.34%] [G loss: 0.841345]\n",
      "epoch:18 step:17509 [D loss: 0.544246, acc.: 70.31%] [G loss: 0.823583]\n",
      "epoch:18 step:17510 [D loss: 0.535761, acc.: 68.75%] [G loss: 0.655906]\n",
      "epoch:18 step:17511 [D loss: 0.538427, acc.: 72.66%] [G loss: 0.528782]\n",
      "epoch:18 step:17512 [D loss: 0.553578, acc.: 68.75%] [G loss: 0.622309]\n",
      "epoch:18 step:17513 [D loss: 0.436167, acc.: 81.25%] [G loss: 0.722978]\n",
      "epoch:18 step:17514 [D loss: 0.423826, acc.: 80.47%] [G loss: 1.085020]\n",
      "epoch:18 step:17515 [D loss: 0.477946, acc.: 78.12%] [G loss: 0.913539]\n",
      "epoch:18 step:17516 [D loss: 0.506581, acc.: 72.66%] [G loss: 1.047449]\n",
      "epoch:18 step:17517 [D loss: 0.526875, acc.: 76.56%] [G loss: 0.820632]\n",
      "epoch:18 step:17518 [D loss: 0.673427, acc.: 60.16%] [G loss: 0.561666]\n",
      "epoch:18 step:17519 [D loss: 0.608907, acc.: 64.84%] [G loss: 0.613001]\n",
      "epoch:18 step:17520 [D loss: 0.484946, acc.: 75.78%] [G loss: 0.619081]\n",
      "epoch:18 step:17521 [D loss: 0.596861, acc.: 64.84%] [G loss: 0.658918]\n",
      "epoch:18 step:17522 [D loss: 0.539039, acc.: 76.56%] [G loss: 0.674471]\n",
      "epoch:18 step:17523 [D loss: 0.552002, acc.: 68.75%] [G loss: 0.688436]\n",
      "epoch:18 step:17524 [D loss: 0.546262, acc.: 73.44%] [G loss: 0.793000]\n",
      "epoch:18 step:17525 [D loss: 0.501542, acc.: 74.22%] [G loss: 0.713357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17526 [D loss: 0.488832, acc.: 78.12%] [G loss: 0.746497]\n",
      "epoch:18 step:17527 [D loss: 0.532737, acc.: 70.31%] [G loss: 0.756649]\n",
      "epoch:18 step:17528 [D loss: 0.531162, acc.: 71.88%] [G loss: 0.738395]\n",
      "epoch:18 step:17529 [D loss: 0.585270, acc.: 70.31%] [G loss: 0.623982]\n",
      "epoch:18 step:17530 [D loss: 0.555264, acc.: 67.97%] [G loss: 0.689022]\n",
      "epoch:18 step:17531 [D loss: 0.557866, acc.: 67.97%] [G loss: 0.669062]\n",
      "epoch:18 step:17532 [D loss: 0.462183, acc.: 77.34%] [G loss: 0.802997]\n",
      "epoch:18 step:17533 [D loss: 0.627463, acc.: 64.06%] [G loss: 0.569894]\n",
      "epoch:18 step:17534 [D loss: 0.567612, acc.: 66.41%] [G loss: 0.547714]\n",
      "epoch:18 step:17535 [D loss: 0.553243, acc.: 71.09%] [G loss: 0.481603]\n",
      "epoch:18 step:17536 [D loss: 0.529932, acc.: 65.62%] [G loss: 0.582577]\n",
      "epoch:18 step:17537 [D loss: 0.570987, acc.: 65.62%] [G loss: 0.723614]\n",
      "epoch:18 step:17538 [D loss: 0.576531, acc.: 71.09%] [G loss: 0.682726]\n",
      "epoch:18 step:17539 [D loss: 0.661480, acc.: 65.62%] [G loss: 0.626087]\n",
      "epoch:18 step:17540 [D loss: 0.526896, acc.: 71.09%] [G loss: 0.575206]\n",
      "epoch:18 step:17541 [D loss: 0.593913, acc.: 67.19%] [G loss: 0.609868]\n",
      "epoch:18 step:17542 [D loss: 0.534452, acc.: 72.66%] [G loss: 0.695157]\n",
      "epoch:18 step:17543 [D loss: 0.486764, acc.: 75.78%] [G loss: 0.863995]\n",
      "epoch:18 step:17544 [D loss: 0.575995, acc.: 65.62%] [G loss: 0.564933]\n",
      "epoch:18 step:17545 [D loss: 0.517568, acc.: 72.66%] [G loss: 0.632615]\n",
      "epoch:18 step:17546 [D loss: 0.529633, acc.: 75.00%] [G loss: 0.544067]\n",
      "epoch:18 step:17547 [D loss: 0.482155, acc.: 75.78%] [G loss: 0.738221]\n",
      "epoch:18 step:17548 [D loss: 0.460524, acc.: 78.91%] [G loss: 0.739100]\n",
      "epoch:18 step:17549 [D loss: 0.537397, acc.: 70.31%] [G loss: 0.696776]\n",
      "epoch:18 step:17550 [D loss: 0.587846, acc.: 64.06%] [G loss: 0.537966]\n",
      "epoch:18 step:17551 [D loss: 0.492195, acc.: 76.56%] [G loss: 0.593727]\n",
      "epoch:18 step:17552 [D loss: 0.576745, acc.: 68.75%] [G loss: 0.534736]\n",
      "epoch:18 step:17553 [D loss: 0.581123, acc.: 62.50%] [G loss: 0.506357]\n",
      "epoch:18 step:17554 [D loss: 0.536123, acc.: 71.09%] [G loss: 0.502811]\n",
      "epoch:18 step:17555 [D loss: 0.558072, acc.: 69.53%] [G loss: 0.551936]\n",
      "epoch:18 step:17556 [D loss: 0.512862, acc.: 74.22%] [G loss: 0.615506]\n",
      "epoch:18 step:17557 [D loss: 0.533631, acc.: 71.88%] [G loss: 0.762084]\n",
      "epoch:18 step:17558 [D loss: 0.531851, acc.: 71.09%] [G loss: 0.760893]\n",
      "epoch:18 step:17559 [D loss: 0.509772, acc.: 75.78%] [G loss: 0.875579]\n",
      "epoch:18 step:17560 [D loss: 0.521996, acc.: 71.88%] [G loss: 0.755206]\n",
      "epoch:18 step:17561 [D loss: 0.530484, acc.: 71.88%] [G loss: 0.626487]\n",
      "epoch:18 step:17562 [D loss: 0.689706, acc.: 54.69%] [G loss: 0.491041]\n",
      "epoch:18 step:17563 [D loss: 0.573557, acc.: 66.41%] [G loss: 0.508355]\n",
      "epoch:18 step:17564 [D loss: 0.557236, acc.: 68.75%] [G loss: 0.646962]\n",
      "epoch:18 step:17565 [D loss: 0.499761, acc.: 77.34%] [G loss: 0.529257]\n",
      "epoch:18 step:17566 [D loss: 0.558137, acc.: 65.62%] [G loss: 0.874087]\n",
      "epoch:18 step:17567 [D loss: 0.501867, acc.: 76.56%] [G loss: 0.830186]\n",
      "epoch:18 step:17568 [D loss: 0.575936, acc.: 70.31%] [G loss: 0.807893]\n",
      "epoch:18 step:17569 [D loss: 0.624600, acc.: 60.16%] [G loss: 0.629949]\n",
      "epoch:18 step:17570 [D loss: 0.559815, acc.: 70.31%] [G loss: 0.609139]\n",
      "epoch:18 step:17571 [D loss: 0.526484, acc.: 71.88%] [G loss: 0.592230]\n",
      "epoch:18 step:17572 [D loss: 0.585146, acc.: 71.09%] [G loss: 0.709391]\n",
      "epoch:18 step:17573 [D loss: 0.539931, acc.: 67.97%] [G loss: 0.620035]\n",
      "epoch:18 step:17574 [D loss: 0.457373, acc.: 81.25%] [G loss: 0.787019]\n",
      "epoch:18 step:17575 [D loss: 0.523713, acc.: 67.97%] [G loss: 0.734637]\n",
      "epoch:18 step:17576 [D loss: 0.524112, acc.: 73.44%] [G loss: 0.694566]\n",
      "epoch:18 step:17577 [D loss: 0.597777, acc.: 61.72%] [G loss: 0.683485]\n",
      "epoch:18 step:17578 [D loss: 0.561649, acc.: 65.62%] [G loss: 0.572559]\n",
      "epoch:18 step:17579 [D loss: 0.571461, acc.: 66.41%] [G loss: 0.723003]\n",
      "epoch:18 step:17580 [D loss: 0.548953, acc.: 64.84%] [G loss: 0.704511]\n",
      "epoch:18 step:17581 [D loss: 0.543324, acc.: 71.88%] [G loss: 0.582662]\n",
      "epoch:18 step:17582 [D loss: 0.644791, acc.: 64.06%] [G loss: 0.619994]\n",
      "epoch:18 step:17583 [D loss: 0.567213, acc.: 71.09%] [G loss: 0.571294]\n",
      "epoch:18 step:17584 [D loss: 0.573096, acc.: 67.19%] [G loss: 0.581218]\n",
      "epoch:18 step:17585 [D loss: 0.508184, acc.: 78.12%] [G loss: 0.661918]\n",
      "epoch:18 step:17586 [D loss: 0.566522, acc.: 71.09%] [G loss: 0.578421]\n",
      "epoch:18 step:17587 [D loss: 0.594937, acc.: 69.53%] [G loss: 0.686983]\n",
      "epoch:18 step:17588 [D loss: 0.592672, acc.: 63.28%] [G loss: 0.511389]\n",
      "epoch:18 step:17589 [D loss: 0.529564, acc.: 71.09%] [G loss: 0.546668]\n",
      "epoch:18 step:17590 [D loss: 0.528567, acc.: 73.44%] [G loss: 0.620562]\n",
      "epoch:18 step:17591 [D loss: 0.500593, acc.: 75.78%] [G loss: 0.601624]\n",
      "epoch:18 step:17592 [D loss: 0.538129, acc.: 75.78%] [G loss: 0.577562]\n",
      "epoch:18 step:17593 [D loss: 0.552064, acc.: 67.19%] [G loss: 0.566074]\n",
      "epoch:18 step:17594 [D loss: 0.545685, acc.: 71.09%] [G loss: 0.572280]\n",
      "epoch:18 step:17595 [D loss: 0.550776, acc.: 68.75%] [G loss: 0.684253]\n",
      "epoch:18 step:17596 [D loss: 0.527189, acc.: 71.88%] [G loss: 0.695692]\n",
      "epoch:18 step:17597 [D loss: 0.565570, acc.: 70.31%] [G loss: 0.617124]\n",
      "epoch:18 step:17598 [D loss: 0.581271, acc.: 68.75%] [G loss: 0.511497]\n",
      "epoch:18 step:17599 [D loss: 0.521389, acc.: 75.00%] [G loss: 0.556469]\n",
      "epoch:18 step:17600 [D loss: 0.552488, acc.: 67.97%] [G loss: 0.541089]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.637159\n",
      "FID: 47.068207\n",
      "0 = 12.910227550220458\n",
      "1 = 0.08929608148604398\n",
      "2 = 0.8712000250816345\n",
      "3 = 0.8198000192642212\n",
      "4 = 0.9225999712944031\n",
      "5 = 0.9137316346168518\n",
      "6 = 0.8198000192642212\n",
      "7 = 8.102843280792234\n",
      "8 = 0.14608547574851652\n",
      "9 = 0.7102000117301941\n",
      "10 = 0.699400007724762\n",
      "11 = 0.7210000157356262\n",
      "12 = 0.7148405313491821\n",
      "13 = 0.699400007724762\n",
      "14 = 6.637185573577881\n",
      "15 = 7.024372577667236\n",
      "16 = 0.38350486755371094\n",
      "17 = 6.63715934753418\n",
      "18 = 47.068206787109375\n",
      "epoch:18 step:17601 [D loss: 0.549958, acc.: 67.97%] [G loss: 0.537672]\n",
      "epoch:18 step:17602 [D loss: 0.529561, acc.: 71.09%] [G loss: 0.691686]\n",
      "epoch:18 step:17603 [D loss: 0.532207, acc.: 70.31%] [G loss: 0.629611]\n",
      "epoch:18 step:17604 [D loss: 0.566489, acc.: 67.97%] [G loss: 0.589924]\n",
      "epoch:18 step:17605 [D loss: 0.613264, acc.: 64.84%] [G loss: 0.457623]\n",
      "epoch:18 step:17606 [D loss: 0.679680, acc.: 51.56%] [G loss: 0.466610]\n",
      "epoch:18 step:17607 [D loss: 0.532242, acc.: 67.97%] [G loss: 0.475037]\n",
      "epoch:18 step:17608 [D loss: 0.531518, acc.: 77.34%] [G loss: 0.674069]\n",
      "epoch:18 step:17609 [D loss: 0.481997, acc.: 76.56%] [G loss: 0.929387]\n",
      "epoch:18 step:17610 [D loss: 0.547448, acc.: 70.31%] [G loss: 0.716659]\n",
      "epoch:18 step:17611 [D loss: 0.639291, acc.: 62.50%] [G loss: 0.584263]\n",
      "epoch:18 step:17612 [D loss: 0.446469, acc.: 80.47%] [G loss: 0.688001]\n",
      "epoch:18 step:17613 [D loss: 0.455424, acc.: 77.34%] [G loss: 0.816572]\n",
      "epoch:18 step:17614 [D loss: 0.576745, acc.: 65.62%] [G loss: 0.833746]\n",
      "epoch:18 step:17615 [D loss: 0.521358, acc.: 75.00%] [G loss: 0.795946]\n",
      "epoch:18 step:17616 [D loss: 0.590065, acc.: 67.19%] [G loss: 0.678427]\n",
      "epoch:18 step:17617 [D loss: 0.519128, acc.: 66.41%] [G loss: 0.665099]\n",
      "epoch:18 step:17618 [D loss: 0.578234, acc.: 66.41%] [G loss: 0.651837]\n",
      "epoch:18 step:17619 [D loss: 0.520762, acc.: 73.44%] [G loss: 0.673339]\n",
      "epoch:18 step:17620 [D loss: 0.577202, acc.: 69.53%] [G loss: 0.667283]\n",
      "epoch:18 step:17621 [D loss: 0.511281, acc.: 70.31%] [G loss: 0.717672]\n",
      "epoch:18 step:17622 [D loss: 0.528109, acc.: 73.44%] [G loss: 0.774661]\n",
      "epoch:18 step:17623 [D loss: 0.551221, acc.: 66.41%] [G loss: 0.491139]\n",
      "epoch:18 step:17624 [D loss: 0.569621, acc.: 67.97%] [G loss: 0.535981]\n",
      "epoch:18 step:17625 [D loss: 0.533124, acc.: 73.44%] [G loss: 0.584614]\n",
      "epoch:18 step:17626 [D loss: 0.525996, acc.: 72.66%] [G loss: 0.576081]\n",
      "epoch:18 step:17627 [D loss: 0.527426, acc.: 69.53%] [G loss: 0.652390]\n",
      "epoch:18 step:17628 [D loss: 0.564997, acc.: 67.97%] [G loss: 0.599201]\n",
      "epoch:18 step:17629 [D loss: 0.534194, acc.: 71.09%] [G loss: 0.597246]\n",
      "epoch:18 step:17630 [D loss: 0.530918, acc.: 71.88%] [G loss: 0.532126]\n",
      "epoch:18 step:17631 [D loss: 0.588858, acc.: 64.84%] [G loss: 0.655067]\n",
      "epoch:18 step:17632 [D loss: 0.679362, acc.: 59.38%] [G loss: 0.452697]\n",
      "epoch:18 step:17633 [D loss: 0.537602, acc.: 71.88%] [G loss: 0.530962]\n",
      "epoch:18 step:17634 [D loss: 0.586566, acc.: 67.19%] [G loss: 0.538842]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17635 [D loss: 0.503470, acc.: 75.00%] [G loss: 0.697549]\n",
      "epoch:18 step:17636 [D loss: 0.509894, acc.: 77.34%] [G loss: 0.756521]\n",
      "epoch:18 step:17637 [D loss: 0.495563, acc.: 72.66%] [G loss: 0.724306]\n",
      "epoch:18 step:17638 [D loss: 0.536256, acc.: 72.66%] [G loss: 0.630393]\n",
      "epoch:18 step:17639 [D loss: 0.557512, acc.: 68.75%] [G loss: 0.583536]\n",
      "epoch:18 step:17640 [D loss: 0.542618, acc.: 72.66%] [G loss: 0.704405]\n",
      "epoch:18 step:17641 [D loss: 0.527417, acc.: 71.88%] [G loss: 0.662896]\n",
      "epoch:18 step:17642 [D loss: 0.531376, acc.: 75.00%] [G loss: 0.723121]\n",
      "epoch:18 step:17643 [D loss: 0.545297, acc.: 68.75%] [G loss: 0.790027]\n",
      "epoch:18 step:17644 [D loss: 0.574607, acc.: 64.06%] [G loss: 0.544497]\n",
      "epoch:18 step:17645 [D loss: 0.525750, acc.: 70.31%] [G loss: 0.508580]\n",
      "epoch:18 step:17646 [D loss: 0.540682, acc.: 70.31%] [G loss: 0.644331]\n",
      "epoch:18 step:17647 [D loss: 0.471620, acc.: 73.44%] [G loss: 1.121971]\n",
      "epoch:18 step:17648 [D loss: 0.491403, acc.: 71.09%] [G loss: 1.053416]\n",
      "epoch:18 step:17649 [D loss: 0.581573, acc.: 67.97%] [G loss: 0.798031]\n",
      "epoch:18 step:17650 [D loss: 0.649706, acc.: 64.84%] [G loss: 0.689657]\n",
      "epoch:18 step:17651 [D loss: 0.577865, acc.: 66.41%] [G loss: 0.463018]\n",
      "epoch:18 step:17652 [D loss: 0.523578, acc.: 75.00%] [G loss: 0.640328]\n",
      "epoch:18 step:17653 [D loss: 0.611462, acc.: 65.62%] [G loss: 0.641569]\n",
      "epoch:18 step:17654 [D loss: 0.644143, acc.: 63.28%] [G loss: 0.555679]\n",
      "epoch:18 step:17655 [D loss: 0.502517, acc.: 77.34%] [G loss: 0.777400]\n",
      "epoch:18 step:17656 [D loss: 0.582926, acc.: 67.19%] [G loss: 0.729028]\n",
      "epoch:18 step:17657 [D loss: 0.547379, acc.: 67.97%] [G loss: 0.637073]\n",
      "epoch:18 step:17658 [D loss: 0.487239, acc.: 72.66%] [G loss: 0.720208]\n",
      "epoch:18 step:17659 [D loss: 0.592622, acc.: 64.06%] [G loss: 0.559276]\n",
      "epoch:18 step:17660 [D loss: 0.638811, acc.: 64.06%] [G loss: 0.557484]\n",
      "epoch:18 step:17661 [D loss: 0.554943, acc.: 70.31%] [G loss: 0.577793]\n",
      "epoch:18 step:17662 [D loss: 0.513006, acc.: 75.78%] [G loss: 0.911976]\n",
      "epoch:18 step:17663 [D loss: 0.534782, acc.: 68.75%] [G loss: 0.873601]\n",
      "epoch:18 step:17664 [D loss: 0.598959, acc.: 67.19%] [G loss: 0.718492]\n",
      "epoch:18 step:17665 [D loss: 0.565117, acc.: 71.88%] [G loss: 0.630071]\n",
      "epoch:18 step:17666 [D loss: 0.589791, acc.: 66.41%] [G loss: 0.734293]\n",
      "epoch:18 step:17667 [D loss: 0.487880, acc.: 78.12%] [G loss: 0.703860]\n",
      "epoch:18 step:17668 [D loss: 0.483363, acc.: 75.00%] [G loss: 0.873270]\n",
      "epoch:18 step:17669 [D loss: 0.451277, acc.: 82.03%] [G loss: 0.991989]\n",
      "epoch:18 step:17670 [D loss: 0.652218, acc.: 58.59%] [G loss: 0.538010]\n",
      "epoch:18 step:17671 [D loss: 0.559617, acc.: 67.19%] [G loss: 0.523689]\n",
      "epoch:18 step:17672 [D loss: 0.573134, acc.: 64.84%] [G loss: 0.532722]\n",
      "epoch:18 step:17673 [D loss: 0.526615, acc.: 71.88%] [G loss: 0.538170]\n",
      "epoch:18 step:17674 [D loss: 0.592893, acc.: 66.41%] [G loss: 0.666499]\n",
      "epoch:18 step:17675 [D loss: 0.535170, acc.: 73.44%] [G loss: 0.619828]\n",
      "epoch:18 step:17676 [D loss: 0.514059, acc.: 71.88%] [G loss: 0.727568]\n",
      "epoch:18 step:17677 [D loss: 0.590082, acc.: 64.84%] [G loss: 0.600743]\n",
      "epoch:18 step:17678 [D loss: 0.604067, acc.: 66.41%] [G loss: 0.464816]\n",
      "epoch:18 step:17679 [D loss: 0.543500, acc.: 64.84%] [G loss: 0.531815]\n",
      "epoch:18 step:17680 [D loss: 0.477710, acc.: 78.91%] [G loss: 0.588100]\n",
      "epoch:18 step:17681 [D loss: 0.490744, acc.: 76.56%] [G loss: 0.846202]\n",
      "epoch:18 step:17682 [D loss: 0.588773, acc.: 69.53%] [G loss: 0.762816]\n",
      "epoch:18 step:17683 [D loss: 0.650975, acc.: 63.28%] [G loss: 0.726773]\n",
      "epoch:18 step:17684 [D loss: 0.558736, acc.: 69.53%] [G loss: 0.754390]\n",
      "epoch:18 step:17685 [D loss: 0.507809, acc.: 71.09%] [G loss: 0.685202]\n",
      "epoch:18 step:17686 [D loss: 0.620114, acc.: 67.97%] [G loss: 0.527794]\n",
      "epoch:18 step:17687 [D loss: 0.548740, acc.: 70.31%] [G loss: 0.522316]\n",
      "epoch:18 step:17688 [D loss: 0.560271, acc.: 68.75%] [G loss: 0.469163]\n",
      "epoch:18 step:17689 [D loss: 0.502116, acc.: 71.88%] [G loss: 0.557450]\n",
      "epoch:18 step:17690 [D loss: 0.563892, acc.: 70.31%] [G loss: 0.594643]\n",
      "epoch:18 step:17691 [D loss: 0.524366, acc.: 71.88%] [G loss: 0.760005]\n",
      "epoch:18 step:17692 [D loss: 0.520408, acc.: 71.09%] [G loss: 0.731927]\n",
      "epoch:18 step:17693 [D loss: 0.607216, acc.: 65.62%] [G loss: 0.611636]\n",
      "epoch:18 step:17694 [D loss: 0.669773, acc.: 59.38%] [G loss: 0.518831]\n",
      "epoch:18 step:17695 [D loss: 0.534991, acc.: 71.88%] [G loss: 0.558463]\n",
      "epoch:18 step:17696 [D loss: 0.564812, acc.: 67.97%] [G loss: 0.623805]\n",
      "epoch:18 step:17697 [D loss: 0.534353, acc.: 70.31%] [G loss: 0.630230]\n",
      "epoch:18 step:17698 [D loss: 0.488362, acc.: 74.22%] [G loss: 0.657746]\n",
      "epoch:18 step:17699 [D loss: 0.547813, acc.: 73.44%] [G loss: 0.485160]\n",
      "epoch:18 step:17700 [D loss: 0.516964, acc.: 71.88%] [G loss: 0.645720]\n",
      "epoch:18 step:17701 [D loss: 0.511253, acc.: 70.31%] [G loss: 0.609416]\n",
      "epoch:18 step:17702 [D loss: 0.551368, acc.: 70.31%] [G loss: 0.492586]\n",
      "epoch:18 step:17703 [D loss: 0.533077, acc.: 73.44%] [G loss: 0.507341]\n",
      "epoch:18 step:17704 [D loss: 0.538819, acc.: 72.66%] [G loss: 0.592616]\n",
      "epoch:18 step:17705 [D loss: 0.561457, acc.: 69.53%] [G loss: 0.468848]\n",
      "epoch:18 step:17706 [D loss: 0.566068, acc.: 72.66%] [G loss: 0.528658]\n",
      "epoch:18 step:17707 [D loss: 0.552598, acc.: 69.53%] [G loss: 0.495649]\n",
      "epoch:18 step:17708 [D loss: 0.500151, acc.: 76.56%] [G loss: 0.651788]\n",
      "epoch:18 step:17709 [D loss: 0.467738, acc.: 75.78%] [G loss: 0.631994]\n",
      "epoch:18 step:17710 [D loss: 0.555193, acc.: 69.53%] [G loss: 0.580568]\n",
      "epoch:18 step:17711 [D loss: 0.566820, acc.: 67.97%] [G loss: 0.600004]\n",
      "epoch:18 step:17712 [D loss: 0.582403, acc.: 67.97%] [G loss: 0.490872]\n",
      "epoch:18 step:17713 [D loss: 0.589504, acc.: 66.41%] [G loss: 0.518341]\n",
      "epoch:18 step:17714 [D loss: 0.567706, acc.: 67.19%] [G loss: 0.456148]\n",
      "epoch:18 step:17715 [D loss: 0.558673, acc.: 71.88%] [G loss: 0.433845]\n",
      "epoch:18 step:17716 [D loss: 0.584294, acc.: 65.62%] [G loss: 0.400658]\n",
      "epoch:18 step:17717 [D loss: 0.579258, acc.: 66.41%] [G loss: 0.567998]\n",
      "epoch:18 step:17718 [D loss: 0.556715, acc.: 67.97%] [G loss: 0.495576]\n",
      "epoch:18 step:17719 [D loss: 0.593574, acc.: 65.62%] [G loss: 0.471046]\n",
      "epoch:18 step:17720 [D loss: 0.577852, acc.: 64.06%] [G loss: 0.552647]\n",
      "epoch:18 step:17721 [D loss: 0.584536, acc.: 64.84%] [G loss: 0.636963]\n",
      "epoch:18 step:17722 [D loss: 0.573879, acc.: 69.53%] [G loss: 0.599766]\n",
      "epoch:18 step:17723 [D loss: 0.500057, acc.: 76.56%] [G loss: 0.599321]\n",
      "epoch:18 step:17724 [D loss: 0.632890, acc.: 62.50%] [G loss: 0.692519]\n",
      "epoch:18 step:17725 [D loss: 0.553200, acc.: 72.66%] [G loss: 0.631083]\n",
      "epoch:18 step:17726 [D loss: 0.502174, acc.: 75.00%] [G loss: 0.654977]\n",
      "epoch:18 step:17727 [D loss: 0.637909, acc.: 60.16%] [G loss: 0.552397]\n",
      "epoch:18 step:17728 [D loss: 0.564364, acc.: 65.62%] [G loss: 0.517194]\n",
      "epoch:18 step:17729 [D loss: 0.586029, acc.: 67.19%] [G loss: 0.552099]\n",
      "epoch:18 step:17730 [D loss: 0.553939, acc.: 71.09%] [G loss: 0.589400]\n",
      "epoch:18 step:17731 [D loss: 0.628603, acc.: 59.38%] [G loss: 0.421026]\n",
      "epoch:18 step:17732 [D loss: 0.544599, acc.: 65.62%] [G loss: 0.595772]\n",
      "epoch:18 step:17733 [D loss: 0.646365, acc.: 63.28%] [G loss: 0.534023]\n",
      "epoch:18 step:17734 [D loss: 0.527178, acc.: 75.78%] [G loss: 0.487559]\n",
      "epoch:18 step:17735 [D loss: 0.581925, acc.: 65.62%] [G loss: 0.471825]\n",
      "epoch:18 step:17736 [D loss: 0.499542, acc.: 73.44%] [G loss: 0.505361]\n",
      "epoch:18 step:17737 [D loss: 0.492234, acc.: 73.44%] [G loss: 0.697766]\n",
      "epoch:18 step:17738 [D loss: 0.526440, acc.: 71.09%] [G loss: 0.672064]\n",
      "epoch:18 step:17739 [D loss: 0.584435, acc.: 69.53%] [G loss: 0.641740]\n",
      "epoch:18 step:17740 [D loss: 0.530016, acc.: 73.44%] [G loss: 0.555188]\n",
      "epoch:18 step:17741 [D loss: 0.550290, acc.: 71.88%] [G loss: 0.526601]\n",
      "epoch:18 step:17742 [D loss: 0.559518, acc.: 69.53%] [G loss: 0.628257]\n",
      "epoch:18 step:17743 [D loss: 0.596667, acc.: 67.19%] [G loss: 0.539214]\n",
      "epoch:18 step:17744 [D loss: 0.553728, acc.: 72.66%] [G loss: 0.395563]\n",
      "epoch:18 step:17745 [D loss: 0.600184, acc.: 63.28%] [G loss: 0.468384]\n",
      "epoch:18 step:17746 [D loss: 0.650382, acc.: 62.50%] [G loss: 0.498308]\n",
      "epoch:18 step:17747 [D loss: 0.556682, acc.: 68.75%] [G loss: 0.482302]\n",
      "epoch:18 step:17748 [D loss: 0.532097, acc.: 74.22%] [G loss: 0.483108]\n",
      "epoch:18 step:17749 [D loss: 0.629430, acc.: 64.84%] [G loss: 0.460451]\n",
      "epoch:18 step:17750 [D loss: 0.521153, acc.: 75.00%] [G loss: 0.627843]\n",
      "epoch:18 step:17751 [D loss: 0.570911, acc.: 63.28%] [G loss: 0.614258]\n",
      "epoch:18 step:17752 [D loss: 0.562135, acc.: 67.19%] [G loss: 0.593761]\n",
      "epoch:18 step:17753 [D loss: 0.564311, acc.: 70.31%] [G loss: 0.904826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17754 [D loss: 0.562764, acc.: 65.62%] [G loss: 0.695472]\n",
      "epoch:18 step:17755 [D loss: 0.550628, acc.: 70.31%] [G loss: 0.611867]\n",
      "epoch:18 step:17756 [D loss: 0.486454, acc.: 75.78%] [G loss: 0.730040]\n",
      "epoch:18 step:17757 [D loss: 0.621840, acc.: 64.06%] [G loss: 0.602177]\n",
      "epoch:18 step:17758 [D loss: 0.611520, acc.: 64.84%] [G loss: 0.475621]\n",
      "epoch:18 step:17759 [D loss: 0.526363, acc.: 72.66%] [G loss: 0.659957]\n",
      "epoch:18 step:17760 [D loss: 0.485543, acc.: 71.88%] [G loss: 0.622939]\n",
      "epoch:18 step:17761 [D loss: 0.524001, acc.: 72.66%] [G loss: 0.737586]\n",
      "epoch:18 step:17762 [D loss: 0.497726, acc.: 78.91%] [G loss: 1.013716]\n",
      "epoch:18 step:17763 [D loss: 0.517037, acc.: 71.09%] [G loss: 0.710538]\n",
      "epoch:18 step:17764 [D loss: 0.501777, acc.: 75.78%] [G loss: 0.653971]\n",
      "epoch:18 step:17765 [D loss: 0.462256, acc.: 78.12%] [G loss: 0.813634]\n",
      "epoch:18 step:17766 [D loss: 0.593020, acc.: 64.84%] [G loss: 0.827600]\n",
      "epoch:18 step:17767 [D loss: 0.530699, acc.: 71.09%] [G loss: 0.786284]\n",
      "epoch:18 step:17768 [D loss: 0.562525, acc.: 70.31%] [G loss: 0.541777]\n",
      "epoch:18 step:17769 [D loss: 0.540711, acc.: 75.00%] [G loss: 0.529001]\n",
      "epoch:18 step:17770 [D loss: 0.552928, acc.: 67.97%] [G loss: 0.550487]\n",
      "epoch:18 step:17771 [D loss: 0.581467, acc.: 67.97%] [G loss: 0.739154]\n",
      "epoch:18 step:17772 [D loss: 0.473270, acc.: 76.56%] [G loss: 0.669901]\n",
      "epoch:18 step:17773 [D loss: 0.558011, acc.: 65.62%] [G loss: 0.664170]\n",
      "epoch:18 step:17774 [D loss: 0.531241, acc.: 71.09%] [G loss: 0.666442]\n",
      "epoch:18 step:17775 [D loss: 0.511982, acc.: 72.66%] [G loss: 0.697355]\n",
      "epoch:18 step:17776 [D loss: 0.536344, acc.: 74.22%] [G loss: 0.521170]\n",
      "epoch:18 step:17777 [D loss: 0.525165, acc.: 68.75%] [G loss: 0.516427]\n",
      "epoch:18 step:17778 [D loss: 0.440797, acc.: 81.25%] [G loss: 0.738518]\n",
      "epoch:18 step:17779 [D loss: 0.590956, acc.: 71.88%] [G loss: 0.787979]\n",
      "epoch:18 step:17780 [D loss: 0.493215, acc.: 75.00%] [G loss: 0.855134]\n",
      "epoch:18 step:17781 [D loss: 0.670998, acc.: 59.38%] [G loss: 0.715373]\n",
      "epoch:18 step:17782 [D loss: 0.567825, acc.: 67.19%] [G loss: 0.610297]\n",
      "epoch:18 step:17783 [D loss: 0.593700, acc.: 66.41%] [G loss: 0.670347]\n",
      "epoch:18 step:17784 [D loss: 0.495597, acc.: 80.47%] [G loss: 0.670743]\n",
      "epoch:18 step:17785 [D loss: 0.469742, acc.: 76.56%] [G loss: 0.987407]\n",
      "epoch:18 step:17786 [D loss: 0.703848, acc.: 63.28%] [G loss: 0.623895]\n",
      "epoch:18 step:17787 [D loss: 0.486193, acc.: 77.34%] [G loss: 0.589852]\n",
      "epoch:18 step:17788 [D loss: 0.559363, acc.: 69.53%] [G loss: 0.647809]\n",
      "epoch:18 step:17789 [D loss: 0.432140, acc.: 79.69%] [G loss: 0.761944]\n",
      "epoch:18 step:17790 [D loss: 0.478262, acc.: 76.56%] [G loss: 0.801121]\n",
      "epoch:18 step:17791 [D loss: 0.442294, acc.: 75.78%] [G loss: 0.876497]\n",
      "epoch:18 step:17792 [D loss: 0.404240, acc.: 82.03%] [G loss: 1.139834]\n",
      "epoch:18 step:17793 [D loss: 0.543290, acc.: 67.19%] [G loss: 1.115007]\n",
      "epoch:18 step:17794 [D loss: 0.682165, acc.: 63.28%] [G loss: 1.108807]\n",
      "epoch:18 step:17795 [D loss: 0.528579, acc.: 78.91%] [G loss: 1.220450]\n",
      "epoch:18 step:17796 [D loss: 0.441146, acc.: 74.22%] [G loss: 1.110621]\n",
      "epoch:18 step:17797 [D loss: 0.541172, acc.: 71.09%] [G loss: 0.799674]\n",
      "epoch:18 step:17798 [D loss: 0.564194, acc.: 71.09%] [G loss: 0.645086]\n",
      "epoch:18 step:17799 [D loss: 0.493312, acc.: 74.22%] [G loss: 0.807634]\n",
      "epoch:18 step:17800 [D loss: 0.553769, acc.: 64.84%] [G loss: 0.635449]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.726616\n",
      "FID: 46.995777\n",
      "0 = 13.109254739379887\n",
      "1 = 0.0945483231692849\n",
      "2 = 0.8808000087738037\n",
      "3 = 0.8442000150680542\n",
      "4 = 0.9174000024795532\n",
      "5 = 0.9108761548995972\n",
      "6 = 0.8442000150680542\n",
      "7 = 8.35030974593164\n",
      "8 = 0.14170843892297771\n",
      "9 = 0.7075999975204468\n",
      "10 = 0.7092000246047974\n",
      "11 = 0.7059999704360962\n",
      "12 = 0.7069377899169922\n",
      "13 = 0.7092000246047974\n",
      "14 = 6.726640701293945\n",
      "15 = 6.8782477378845215\n",
      "16 = 0.38906094431877136\n",
      "17 = 6.726616382598877\n",
      "18 = 46.99577713012695\n",
      "epoch:18 step:17801 [D loss: 0.521421, acc.: 72.66%] [G loss: 0.748522]\n",
      "epoch:18 step:17802 [D loss: 0.358679, acc.: 85.16%] [G loss: 1.148908]\n",
      "epoch:18 step:17803 [D loss: 0.446732, acc.: 80.47%] [G loss: 1.246209]\n",
      "epoch:19 step:17804 [D loss: 0.581961, acc.: 68.75%] [G loss: 1.090226]\n",
      "epoch:19 step:17805 [D loss: 0.472854, acc.: 77.34%] [G loss: 1.041070]\n",
      "epoch:19 step:17806 [D loss: 0.597446, acc.: 70.31%] [G loss: 0.851329]\n",
      "epoch:19 step:17807 [D loss: 0.516251, acc.: 75.78%] [G loss: 0.815504]\n",
      "epoch:19 step:17808 [D loss: 0.533696, acc.: 70.31%] [G loss: 0.871280]\n",
      "epoch:19 step:17809 [D loss: 0.564271, acc.: 71.09%] [G loss: 0.944281]\n",
      "epoch:19 step:17810 [D loss: 0.500456, acc.: 77.34%] [G loss: 0.896830]\n",
      "epoch:19 step:17811 [D loss: 0.532577, acc.: 75.00%] [G loss: 0.776685]\n",
      "epoch:19 step:17812 [D loss: 0.496787, acc.: 71.88%] [G loss: 0.699614]\n",
      "epoch:19 step:17813 [D loss: 0.559095, acc.: 75.00%] [G loss: 0.619333]\n",
      "epoch:19 step:17814 [D loss: 0.495088, acc.: 78.91%] [G loss: 0.741289]\n",
      "epoch:19 step:17815 [D loss: 0.605375, acc.: 69.53%] [G loss: 0.735505]\n",
      "epoch:19 step:17816 [D loss: 0.522733, acc.: 71.09%] [G loss: 0.625727]\n",
      "epoch:19 step:17817 [D loss: 0.525698, acc.: 69.53%] [G loss: 0.579522]\n",
      "epoch:19 step:17818 [D loss: 0.457454, acc.: 75.78%] [G loss: 0.714639]\n",
      "epoch:19 step:17819 [D loss: 0.508736, acc.: 72.66%] [G loss: 0.794891]\n",
      "epoch:19 step:17820 [D loss: 0.524542, acc.: 76.56%] [G loss: 0.749821]\n",
      "epoch:19 step:17821 [D loss: 0.605089, acc.: 68.75%] [G loss: 0.746575]\n",
      "epoch:19 step:17822 [D loss: 0.577413, acc.: 70.31%] [G loss: 0.673592]\n",
      "epoch:19 step:17823 [D loss: 0.637666, acc.: 64.84%] [G loss: 0.860050]\n",
      "epoch:19 step:17824 [D loss: 0.551896, acc.: 70.31%] [G loss: 0.832632]\n",
      "epoch:19 step:17825 [D loss: 0.446432, acc.: 81.25%] [G loss: 1.012026]\n",
      "epoch:19 step:17826 [D loss: 0.569234, acc.: 67.19%] [G loss: 0.553599]\n",
      "epoch:19 step:17827 [D loss: 0.586850, acc.: 69.53%] [G loss: 0.607670]\n",
      "epoch:19 step:17828 [D loss: 0.491170, acc.: 75.78%] [G loss: 0.704548]\n",
      "epoch:19 step:17829 [D loss: 0.573306, acc.: 71.09%] [G loss: 0.508000]\n",
      "epoch:19 step:17830 [D loss: 0.464519, acc.: 77.34%] [G loss: 0.581414]\n",
      "epoch:19 step:17831 [D loss: 0.579043, acc.: 64.06%] [G loss: 0.531574]\n",
      "epoch:19 step:17832 [D loss: 0.474441, acc.: 78.91%] [G loss: 0.625539]\n",
      "epoch:19 step:17833 [D loss: 0.550532, acc.: 66.41%] [G loss: 0.788532]\n",
      "epoch:19 step:17834 [D loss: 0.633906, acc.: 63.28%] [G loss: 0.590357]\n",
      "epoch:19 step:17835 [D loss: 0.566893, acc.: 67.97%] [G loss: 0.623874]\n",
      "epoch:19 step:17836 [D loss: 0.542307, acc.: 71.09%] [G loss: 0.707082]\n",
      "epoch:19 step:17837 [D loss: 0.530442, acc.: 69.53%] [G loss: 0.499631]\n",
      "epoch:19 step:17838 [D loss: 0.588420, acc.: 71.09%] [G loss: 0.615040]\n",
      "epoch:19 step:17839 [D loss: 0.533717, acc.: 70.31%] [G loss: 0.850182]\n",
      "epoch:19 step:17840 [D loss: 0.507283, acc.: 75.78%] [G loss: 0.726030]\n",
      "epoch:19 step:17841 [D loss: 0.659205, acc.: 62.50%] [G loss: 0.593630]\n",
      "epoch:19 step:17842 [D loss: 0.545725, acc.: 69.53%] [G loss: 0.668050]\n",
      "epoch:19 step:17843 [D loss: 0.442786, acc.: 78.12%] [G loss: 0.824021]\n",
      "epoch:19 step:17844 [D loss: 0.529719, acc.: 74.22%] [G loss: 0.711393]\n",
      "epoch:19 step:17845 [D loss: 0.557250, acc.: 70.31%] [G loss: 0.667046]\n",
      "epoch:19 step:17846 [D loss: 0.552342, acc.: 74.22%] [G loss: 0.626632]\n",
      "epoch:19 step:17847 [D loss: 0.555051, acc.: 69.53%] [G loss: 0.608534]\n",
      "epoch:19 step:17848 [D loss: 0.498847, acc.: 78.91%] [G loss: 0.801849]\n",
      "epoch:19 step:17849 [D loss: 0.523753, acc.: 73.44%] [G loss: 0.710121]\n",
      "epoch:19 step:17850 [D loss: 0.525074, acc.: 72.66%] [G loss: 0.680536]\n",
      "epoch:19 step:17851 [D loss: 0.540528, acc.: 71.09%] [G loss: 0.745853]\n",
      "epoch:19 step:17852 [D loss: 0.484062, acc.: 72.66%] [G loss: 0.632329]\n",
      "epoch:19 step:17853 [D loss: 0.574175, acc.: 64.84%] [G loss: 0.723746]\n",
      "epoch:19 step:17854 [D loss: 0.680651, acc.: 60.94%] [G loss: 0.366806]\n",
      "epoch:19 step:17855 [D loss: 0.597012, acc.: 64.84%] [G loss: 0.611337]\n",
      "epoch:19 step:17856 [D loss: 0.480398, acc.: 75.78%] [G loss: 0.700326]\n",
      "epoch:19 step:17857 [D loss: 0.477949, acc.: 77.34%] [G loss: 0.637593]\n",
      "epoch:19 step:17858 [D loss: 0.578525, acc.: 66.41%] [G loss: 0.733174]\n",
      "epoch:19 step:17859 [D loss: 0.483452, acc.: 75.00%] [G loss: 0.743820]\n",
      "epoch:19 step:17860 [D loss: 0.543154, acc.: 67.97%] [G loss: 0.731644]\n",
      "epoch:19 step:17861 [D loss: 0.571977, acc.: 64.84%] [G loss: 0.715717]\n",
      "epoch:19 step:17862 [D loss: 0.482901, acc.: 74.22%] [G loss: 0.788752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:17863 [D loss: 0.590131, acc.: 66.41%] [G loss: 0.816483]\n",
      "epoch:19 step:17864 [D loss: 0.533766, acc.: 70.31%] [G loss: 0.695865]\n",
      "epoch:19 step:17865 [D loss: 0.553204, acc.: 71.09%] [G loss: 0.513933]\n",
      "epoch:19 step:17866 [D loss: 0.590779, acc.: 68.75%] [G loss: 0.554119]\n",
      "epoch:19 step:17867 [D loss: 0.515181, acc.: 75.00%] [G loss: 0.565427]\n",
      "epoch:19 step:17868 [D loss: 0.511874, acc.: 75.00%] [G loss: 0.672220]\n",
      "epoch:19 step:17869 [D loss: 0.573451, acc.: 64.84%] [G loss: 0.688230]\n",
      "epoch:19 step:17870 [D loss: 0.541680, acc.: 70.31%] [G loss: 0.623532]\n",
      "epoch:19 step:17871 [D loss: 0.534163, acc.: 65.62%] [G loss: 0.628684]\n",
      "epoch:19 step:17872 [D loss: 0.528904, acc.: 72.66%] [G loss: 0.651578]\n",
      "epoch:19 step:17873 [D loss: 0.553791, acc.: 70.31%] [G loss: 0.648686]\n",
      "epoch:19 step:17874 [D loss: 0.563254, acc.: 66.41%] [G loss: 0.733565]\n",
      "epoch:19 step:17875 [D loss: 0.547089, acc.: 71.09%] [G loss: 0.670218]\n",
      "epoch:19 step:17876 [D loss: 0.572482, acc.: 66.41%] [G loss: 0.659262]\n",
      "epoch:19 step:17877 [D loss: 0.455808, acc.: 82.03%] [G loss: 0.627491]\n",
      "epoch:19 step:17878 [D loss: 0.559344, acc.: 68.75%] [G loss: 0.637666]\n",
      "epoch:19 step:17879 [D loss: 0.516617, acc.: 74.22%] [G loss: 0.815774]\n",
      "epoch:19 step:17880 [D loss: 0.508516, acc.: 75.00%] [G loss: 0.769505]\n",
      "epoch:19 step:17881 [D loss: 0.601786, acc.: 68.75%] [G loss: 0.536108]\n",
      "epoch:19 step:17882 [D loss: 0.566477, acc.: 69.53%] [G loss: 0.576836]\n",
      "epoch:19 step:17883 [D loss: 0.494895, acc.: 76.56%] [G loss: 0.608649]\n",
      "epoch:19 step:17884 [D loss: 0.546131, acc.: 71.88%] [G loss: 0.695340]\n",
      "epoch:19 step:17885 [D loss: 0.521851, acc.: 68.75%] [G loss: 0.548774]\n",
      "epoch:19 step:17886 [D loss: 0.467688, acc.: 76.56%] [G loss: 0.733630]\n",
      "epoch:19 step:17887 [D loss: 0.546716, acc.: 72.66%] [G loss: 0.670913]\n",
      "epoch:19 step:17888 [D loss: 0.568657, acc.: 65.62%] [G loss: 0.688865]\n",
      "epoch:19 step:17889 [D loss: 0.542666, acc.: 75.78%] [G loss: 0.550271]\n",
      "epoch:19 step:17890 [D loss: 0.513216, acc.: 75.78%] [G loss: 0.729542]\n",
      "epoch:19 step:17891 [D loss: 0.531368, acc.: 71.09%] [G loss: 0.676718]\n",
      "epoch:19 step:17892 [D loss: 0.611836, acc.: 70.31%] [G loss: 0.877401]\n",
      "epoch:19 step:17893 [D loss: 0.533553, acc.: 76.56%] [G loss: 0.687903]\n",
      "epoch:19 step:17894 [D loss: 0.552881, acc.: 70.31%] [G loss: 0.639044]\n",
      "epoch:19 step:17895 [D loss: 0.470287, acc.: 77.34%] [G loss: 0.716839]\n",
      "epoch:19 step:17896 [D loss: 0.496872, acc.: 76.56%] [G loss: 0.819881]\n",
      "epoch:19 step:17897 [D loss: 0.563079, acc.: 65.62%] [G loss: 0.823669]\n",
      "epoch:19 step:17898 [D loss: 0.548019, acc.: 71.88%] [G loss: 0.690570]\n",
      "epoch:19 step:17899 [D loss: 0.542856, acc.: 69.53%] [G loss: 0.699897]\n",
      "epoch:19 step:17900 [D loss: 0.518869, acc.: 72.66%] [G loss: 0.712625]\n",
      "epoch:19 step:17901 [D loss: 0.571392, acc.: 68.75%] [G loss: 0.535979]\n",
      "epoch:19 step:17902 [D loss: 0.530130, acc.: 73.44%] [G loss: 0.743573]\n",
      "epoch:19 step:17903 [D loss: 0.447374, acc.: 77.34%] [G loss: 0.763488]\n",
      "epoch:19 step:17904 [D loss: 0.524999, acc.: 72.66%] [G loss: 0.716378]\n",
      "epoch:19 step:17905 [D loss: 0.626426, acc.: 64.06%] [G loss: 0.611848]\n",
      "epoch:19 step:17906 [D loss: 0.558856, acc.: 64.84%] [G loss: 0.518334]\n",
      "epoch:19 step:17907 [D loss: 0.507935, acc.: 71.09%] [G loss: 0.579155]\n",
      "epoch:19 step:17908 [D loss: 0.608179, acc.: 60.94%] [G loss: 0.646439]\n",
      "epoch:19 step:17909 [D loss: 0.534600, acc.: 68.75%] [G loss: 0.630089]\n",
      "epoch:19 step:17910 [D loss: 0.520497, acc.: 75.00%] [G loss: 0.642078]\n",
      "epoch:19 step:17911 [D loss: 0.740036, acc.: 57.03%] [G loss: 0.604453]\n",
      "epoch:19 step:17912 [D loss: 0.597252, acc.: 65.62%] [G loss: 0.613974]\n",
      "epoch:19 step:17913 [D loss: 0.551740, acc.: 71.09%] [G loss: 0.614028]\n",
      "epoch:19 step:17914 [D loss: 0.507149, acc.: 73.44%] [G loss: 0.669235]\n",
      "epoch:19 step:17915 [D loss: 0.504389, acc.: 75.78%] [G loss: 0.572550]\n",
      "epoch:19 step:17916 [D loss: 0.523860, acc.: 75.00%] [G loss: 0.569395]\n",
      "epoch:19 step:17917 [D loss: 0.520460, acc.: 75.00%] [G loss: 0.571845]\n",
      "epoch:19 step:17918 [D loss: 0.557085, acc.: 71.09%] [G loss: 0.690912]\n",
      "epoch:19 step:17919 [D loss: 0.480405, acc.: 78.12%] [G loss: 0.840151]\n",
      "epoch:19 step:17920 [D loss: 0.560223, acc.: 69.53%] [G loss: 0.804358]\n",
      "epoch:19 step:17921 [D loss: 0.537606, acc.: 71.09%] [G loss: 0.714637]\n",
      "epoch:19 step:17922 [D loss: 0.484587, acc.: 78.91%] [G loss: 0.938698]\n",
      "epoch:19 step:17923 [D loss: 0.516012, acc.: 75.78%] [G loss: 0.771306]\n",
      "epoch:19 step:17924 [D loss: 0.516069, acc.: 71.09%] [G loss: 0.668396]\n",
      "epoch:19 step:17925 [D loss: 0.469563, acc.: 82.81%] [G loss: 0.852385]\n",
      "epoch:19 step:17926 [D loss: 0.576228, acc.: 70.31%] [G loss: 0.828650]\n",
      "epoch:19 step:17927 [D loss: 0.563589, acc.: 71.09%] [G loss: 0.849601]\n",
      "epoch:19 step:17928 [D loss: 0.588317, acc.: 66.41%] [G loss: 0.638207]\n",
      "epoch:19 step:17929 [D loss: 0.498079, acc.: 71.09%] [G loss: 0.760497]\n",
      "epoch:19 step:17930 [D loss: 0.507270, acc.: 71.09%] [G loss: 0.637427]\n",
      "epoch:19 step:17931 [D loss: 0.508207, acc.: 71.09%] [G loss: 0.771579]\n",
      "epoch:19 step:17932 [D loss: 0.589533, acc.: 66.41%] [G loss: 0.609032]\n",
      "epoch:19 step:17933 [D loss: 0.507507, acc.: 75.00%] [G loss: 0.634250]\n",
      "epoch:19 step:17934 [D loss: 0.525738, acc.: 71.09%] [G loss: 0.636832]\n",
      "epoch:19 step:17935 [D loss: 0.579037, acc.: 71.09%] [G loss: 0.620833]\n",
      "epoch:19 step:17936 [D loss: 0.650411, acc.: 62.50%] [G loss: 0.786071]\n",
      "epoch:19 step:17937 [D loss: 0.509365, acc.: 71.88%] [G loss: 0.781839]\n",
      "epoch:19 step:17938 [D loss: 0.505391, acc.: 76.56%] [G loss: 0.662555]\n",
      "epoch:19 step:17939 [D loss: 0.552295, acc.: 72.66%] [G loss: 0.750954]\n",
      "epoch:19 step:17940 [D loss: 0.613927, acc.: 70.31%] [G loss: 0.631539]\n",
      "epoch:19 step:17941 [D loss: 0.635794, acc.: 67.97%] [G loss: 0.527856]\n",
      "epoch:19 step:17942 [D loss: 0.533429, acc.: 76.56%] [G loss: 0.552846]\n",
      "epoch:19 step:17943 [D loss: 0.553643, acc.: 70.31%] [G loss: 0.641497]\n",
      "epoch:19 step:17944 [D loss: 0.539880, acc.: 67.19%] [G loss: 0.631883]\n",
      "epoch:19 step:17945 [D loss: 0.536687, acc.: 70.31%] [G loss: 0.657717]\n",
      "epoch:19 step:17946 [D loss: 0.561971, acc.: 71.09%] [G loss: 0.592715]\n",
      "epoch:19 step:17947 [D loss: 0.494161, acc.: 75.78%] [G loss: 0.608334]\n",
      "epoch:19 step:17948 [D loss: 0.609158, acc.: 64.06%] [G loss: 0.674876]\n",
      "epoch:19 step:17949 [D loss: 0.533213, acc.: 74.22%] [G loss: 0.661283]\n",
      "epoch:19 step:17950 [D loss: 0.629549, acc.: 68.75%] [G loss: 0.606309]\n",
      "epoch:19 step:17951 [D loss: 0.625334, acc.: 64.84%] [G loss: 0.552169]\n",
      "epoch:19 step:17952 [D loss: 0.489773, acc.: 77.34%] [G loss: 0.500990]\n",
      "epoch:19 step:17953 [D loss: 0.572100, acc.: 65.62%] [G loss: 0.634057]\n",
      "epoch:19 step:17954 [D loss: 0.582176, acc.: 66.41%] [G loss: 0.560645]\n",
      "epoch:19 step:17955 [D loss: 0.504762, acc.: 76.56%] [G loss: 0.694394]\n",
      "epoch:19 step:17956 [D loss: 0.570470, acc.: 70.31%] [G loss: 0.616809]\n",
      "epoch:19 step:17957 [D loss: 0.537220, acc.: 71.88%] [G loss: 0.574066]\n",
      "epoch:19 step:17958 [D loss: 0.505640, acc.: 70.31%] [G loss: 0.656188]\n",
      "epoch:19 step:17959 [D loss: 0.483659, acc.: 77.34%] [G loss: 0.699986]\n",
      "epoch:19 step:17960 [D loss: 0.563869, acc.: 72.66%] [G loss: 0.810899]\n",
      "epoch:19 step:17961 [D loss: 0.571499, acc.: 73.44%] [G loss: 0.619988]\n",
      "epoch:19 step:17962 [D loss: 0.526562, acc.: 68.75%] [G loss: 0.711160]\n",
      "epoch:19 step:17963 [D loss: 0.588562, acc.: 67.19%] [G loss: 0.766850]\n",
      "epoch:19 step:17964 [D loss: 0.537305, acc.: 75.00%] [G loss: 0.784008]\n",
      "epoch:19 step:17965 [D loss: 0.515886, acc.: 69.53%] [G loss: 0.869078]\n",
      "epoch:19 step:17966 [D loss: 0.567525, acc.: 69.53%] [G loss: 0.785317]\n",
      "epoch:19 step:17967 [D loss: 0.578082, acc.: 64.84%] [G loss: 0.569797]\n",
      "epoch:19 step:17968 [D loss: 0.472658, acc.: 75.00%] [G loss: 0.666251]\n",
      "epoch:19 step:17969 [D loss: 0.595681, acc.: 64.84%] [G loss: 0.439756]\n",
      "epoch:19 step:17970 [D loss: 0.521326, acc.: 70.31%] [G loss: 0.536445]\n",
      "epoch:19 step:17971 [D loss: 0.544429, acc.: 67.19%] [G loss: 0.455084]\n",
      "epoch:19 step:17972 [D loss: 0.539105, acc.: 68.75%] [G loss: 0.718347]\n",
      "epoch:19 step:17973 [D loss: 0.567540, acc.: 65.62%] [G loss: 0.656675]\n",
      "epoch:19 step:17974 [D loss: 0.510006, acc.: 71.88%] [G loss: 0.633998]\n",
      "epoch:19 step:17975 [D loss: 0.507936, acc.: 74.22%] [G loss: 0.687193]\n",
      "epoch:19 step:17976 [D loss: 0.495737, acc.: 72.66%] [G loss: 0.768920]\n",
      "epoch:19 step:17977 [D loss: 0.624358, acc.: 60.94%] [G loss: 0.866630]\n",
      "epoch:19 step:17978 [D loss: 0.617363, acc.: 60.16%] [G loss: 0.700042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:17979 [D loss: 0.540231, acc.: 71.88%] [G loss: 0.704206]\n",
      "epoch:19 step:17980 [D loss: 0.570797, acc.: 73.44%] [G loss: 0.583093]\n",
      "epoch:19 step:17981 [D loss: 0.614788, acc.: 61.72%] [G loss: 0.426480]\n",
      "epoch:19 step:17982 [D loss: 0.553121, acc.: 69.53%] [G loss: 0.616516]\n",
      "epoch:19 step:17983 [D loss: 0.592182, acc.: 68.75%] [G loss: 0.408583]\n",
      "epoch:19 step:17984 [D loss: 0.580479, acc.: 68.75%] [G loss: 0.614039]\n",
      "epoch:19 step:17985 [D loss: 0.520985, acc.: 76.56%] [G loss: 0.643589]\n",
      "epoch:19 step:17986 [D loss: 0.623844, acc.: 65.62%] [G loss: 0.628199]\n",
      "epoch:19 step:17987 [D loss: 0.505720, acc.: 72.66%] [G loss: 0.678639]\n",
      "epoch:19 step:17988 [D loss: 0.546811, acc.: 69.53%] [G loss: 0.697601]\n",
      "epoch:19 step:17989 [D loss: 0.540042, acc.: 73.44%] [G loss: 0.676719]\n",
      "epoch:19 step:17990 [D loss: 0.597618, acc.: 63.28%] [G loss: 0.576486]\n",
      "epoch:19 step:17991 [D loss: 0.531621, acc.: 72.66%] [G loss: 0.486210]\n",
      "epoch:19 step:17992 [D loss: 0.561939, acc.: 70.31%] [G loss: 0.547985]\n",
      "epoch:19 step:17993 [D loss: 0.522544, acc.: 68.75%] [G loss: 0.602840]\n",
      "epoch:19 step:17994 [D loss: 0.503422, acc.: 76.56%] [G loss: 0.632945]\n",
      "epoch:19 step:17995 [D loss: 0.507409, acc.: 76.56%] [G loss: 0.733722]\n",
      "epoch:19 step:17996 [D loss: 0.540947, acc.: 71.09%] [G loss: 0.606583]\n",
      "epoch:19 step:17997 [D loss: 0.459259, acc.: 80.47%] [G loss: 0.629265]\n",
      "epoch:19 step:17998 [D loss: 0.573742, acc.: 66.41%] [G loss: 0.780729]\n",
      "epoch:19 step:17999 [D loss: 0.547333, acc.: 67.97%] [G loss: 0.663844]\n",
      "epoch:19 step:18000 [D loss: 0.522799, acc.: 71.88%] [G loss: 0.526404]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.583385\n",
      "FID: 51.437130\n",
      "0 = 13.007993397235875\n",
      "1 = 0.10032189536140058\n",
      "2 = 0.8691999912261963\n",
      "3 = 0.8230000138282776\n",
      "4 = 0.9154000282287598\n",
      "5 = 0.9067871570587158\n",
      "6 = 0.8230000138282776\n",
      "7 = 8.468512675917133\n",
      "8 = 0.15580227601882612\n",
      "9 = 0.7170000076293945\n",
      "10 = 0.7044000029563904\n",
      "11 = 0.7296000123023987\n",
      "12 = 0.7226097583770752\n",
      "13 = 0.7044000029563904\n",
      "14 = 6.583414554595947\n",
      "15 = 7.018918037414551\n",
      "16 = 0.3865627348423004\n",
      "17 = 6.583385467529297\n",
      "18 = 51.437129974365234\n",
      "epoch:19 step:18001 [D loss: 0.482501, acc.: 77.34%] [G loss: 0.853425]\n",
      "epoch:19 step:18002 [D loss: 0.494478, acc.: 74.22%] [G loss: 0.806332]\n",
      "epoch:19 step:18003 [D loss: 0.613238, acc.: 69.53%] [G loss: 0.651835]\n",
      "epoch:19 step:18004 [D loss: 0.618013, acc.: 64.06%] [G loss: 0.618234]\n",
      "epoch:19 step:18005 [D loss: 0.545792, acc.: 67.97%] [G loss: 0.743025]\n",
      "epoch:19 step:18006 [D loss: 0.600145, acc.: 65.62%] [G loss: 0.706267]\n",
      "epoch:19 step:18007 [D loss: 0.584038, acc.: 66.41%] [G loss: 0.662587]\n",
      "epoch:19 step:18008 [D loss: 0.516854, acc.: 72.66%] [G loss: 0.851338]\n",
      "epoch:19 step:18009 [D loss: 0.571719, acc.: 68.75%] [G loss: 0.686902]\n",
      "epoch:19 step:18010 [D loss: 0.530133, acc.: 74.22%] [G loss: 0.734687]\n",
      "epoch:19 step:18011 [D loss: 0.435208, acc.: 82.81%] [G loss: 0.907582]\n",
      "epoch:19 step:18012 [D loss: 0.545194, acc.: 71.88%] [G loss: 0.766549]\n",
      "epoch:19 step:18013 [D loss: 0.687483, acc.: 57.81%] [G loss: 0.665678]\n",
      "epoch:19 step:18014 [D loss: 0.578314, acc.: 65.62%] [G loss: 0.596739]\n",
      "epoch:19 step:18015 [D loss: 0.570173, acc.: 69.53%] [G loss: 0.521474]\n",
      "epoch:19 step:18016 [D loss: 0.540138, acc.: 69.53%] [G loss: 0.606714]\n",
      "epoch:19 step:18017 [D loss: 0.643087, acc.: 61.72%] [G loss: 0.585401]\n",
      "epoch:19 step:18018 [D loss: 0.631566, acc.: 58.59%] [G loss: 0.544762]\n",
      "epoch:19 step:18019 [D loss: 0.529330, acc.: 72.66%] [G loss: 0.565958]\n",
      "epoch:19 step:18020 [D loss: 0.526811, acc.: 71.88%] [G loss: 0.656582]\n",
      "epoch:19 step:18021 [D loss: 0.502127, acc.: 78.91%] [G loss: 0.628830]\n",
      "epoch:19 step:18022 [D loss: 0.420581, acc.: 82.81%] [G loss: 0.842780]\n",
      "epoch:19 step:18023 [D loss: 0.694010, acc.: 61.72%] [G loss: 0.639272]\n",
      "epoch:19 step:18024 [D loss: 0.567756, acc.: 67.19%] [G loss: 0.689991]\n",
      "epoch:19 step:18025 [D loss: 0.512900, acc.: 73.44%] [G loss: 0.849636]\n",
      "epoch:19 step:18026 [D loss: 0.566349, acc.: 71.09%] [G loss: 0.795597]\n",
      "epoch:19 step:18027 [D loss: 0.595742, acc.: 70.31%] [G loss: 0.648777]\n",
      "epoch:19 step:18028 [D loss: 0.571128, acc.: 68.75%] [G loss: 0.776741]\n",
      "epoch:19 step:18029 [D loss: 0.565060, acc.: 63.28%] [G loss: 0.672006]\n",
      "epoch:19 step:18030 [D loss: 0.507364, acc.: 75.00%] [G loss: 0.608823]\n",
      "epoch:19 step:18031 [D loss: 0.595022, acc.: 67.19%] [G loss: 0.538619]\n",
      "epoch:19 step:18032 [D loss: 0.494476, acc.: 78.91%] [G loss: 0.529074]\n",
      "epoch:19 step:18033 [D loss: 0.500939, acc.: 74.22%] [G loss: 0.704279]\n",
      "epoch:19 step:18034 [D loss: 0.523526, acc.: 67.97%] [G loss: 0.771893]\n",
      "epoch:19 step:18035 [D loss: 0.509757, acc.: 78.91%] [G loss: 1.215879]\n",
      "epoch:19 step:18036 [D loss: 0.511895, acc.: 72.66%] [G loss: 0.779770]\n",
      "epoch:19 step:18037 [D loss: 0.645901, acc.: 68.75%] [G loss: 0.697465]\n",
      "epoch:19 step:18038 [D loss: 0.586752, acc.: 64.84%] [G loss: 0.617099]\n",
      "epoch:19 step:18039 [D loss: 0.556291, acc.: 71.09%] [G loss: 0.614133]\n",
      "epoch:19 step:18040 [D loss: 0.505205, acc.: 73.44%] [G loss: 0.651310]\n",
      "epoch:19 step:18041 [D loss: 0.616127, acc.: 63.28%] [G loss: 0.575123]\n",
      "epoch:19 step:18042 [D loss: 0.549291, acc.: 66.41%] [G loss: 0.501026]\n",
      "epoch:19 step:18043 [D loss: 0.536911, acc.: 75.00%] [G loss: 0.566647]\n",
      "epoch:19 step:18044 [D loss: 0.515546, acc.: 76.56%] [G loss: 0.673704]\n",
      "epoch:19 step:18045 [D loss: 0.525934, acc.: 72.66%] [G loss: 0.683109]\n",
      "epoch:19 step:18046 [D loss: 0.531147, acc.: 70.31%] [G loss: 0.592379]\n",
      "epoch:19 step:18047 [D loss: 0.473826, acc.: 75.78%] [G loss: 0.704471]\n",
      "epoch:19 step:18048 [D loss: 0.503736, acc.: 71.09%] [G loss: 0.806276]\n",
      "epoch:19 step:18049 [D loss: 0.494534, acc.: 73.44%] [G loss: 0.638304]\n",
      "epoch:19 step:18050 [D loss: 0.577820, acc.: 61.72%] [G loss: 0.706008]\n",
      "epoch:19 step:18051 [D loss: 0.511938, acc.: 70.31%] [G loss: 0.791449]\n",
      "epoch:19 step:18052 [D loss: 0.576380, acc.: 72.66%] [G loss: 0.894289]\n",
      "epoch:19 step:18053 [D loss: 0.627998, acc.: 64.84%] [G loss: 0.619909]\n",
      "epoch:19 step:18054 [D loss: 0.666332, acc.: 62.50%] [G loss: 0.691840]\n",
      "epoch:19 step:18055 [D loss: 0.571067, acc.: 70.31%] [G loss: 0.506580]\n",
      "epoch:19 step:18056 [D loss: 0.574737, acc.: 67.97%] [G loss: 0.560153]\n",
      "epoch:19 step:18057 [D loss: 0.475228, acc.: 76.56%] [G loss: 0.843072]\n",
      "epoch:19 step:18058 [D loss: 0.554987, acc.: 67.97%] [G loss: 0.574891]\n",
      "epoch:19 step:18059 [D loss: 0.585420, acc.: 64.06%] [G loss: 0.572923]\n",
      "epoch:19 step:18060 [D loss: 0.576091, acc.: 71.09%] [G loss: 0.404566]\n",
      "epoch:19 step:18061 [D loss: 0.541422, acc.: 72.66%] [G loss: 0.725706]\n",
      "epoch:19 step:18062 [D loss: 0.535819, acc.: 73.44%] [G loss: 0.640099]\n",
      "epoch:19 step:18063 [D loss: 0.575874, acc.: 63.28%] [G loss: 0.670854]\n",
      "epoch:19 step:18064 [D loss: 0.519575, acc.: 72.66%] [G loss: 0.572014]\n",
      "epoch:19 step:18065 [D loss: 0.539001, acc.: 69.53%] [G loss: 0.678612]\n",
      "epoch:19 step:18066 [D loss: 0.590891, acc.: 71.88%] [G loss: 0.636782]\n",
      "epoch:19 step:18067 [D loss: 0.556635, acc.: 68.75%] [G loss: 0.534887]\n",
      "epoch:19 step:18068 [D loss: 0.518098, acc.: 69.53%] [G loss: 0.511435]\n",
      "epoch:19 step:18069 [D loss: 0.579745, acc.: 70.31%] [G loss: 0.589732]\n",
      "epoch:19 step:18070 [D loss: 0.529493, acc.: 73.44%] [G loss: 0.689452]\n",
      "epoch:19 step:18071 [D loss: 0.569228, acc.: 64.84%] [G loss: 0.514998]\n",
      "epoch:19 step:18072 [D loss: 0.477436, acc.: 79.69%] [G loss: 0.620623]\n",
      "epoch:19 step:18073 [D loss: 0.532559, acc.: 73.44%] [G loss: 0.709847]\n",
      "epoch:19 step:18074 [D loss: 0.519532, acc.: 71.09%] [G loss: 0.749726]\n",
      "epoch:19 step:18075 [D loss: 0.541599, acc.: 71.88%] [G loss: 0.766735]\n",
      "epoch:19 step:18076 [D loss: 0.508708, acc.: 73.44%] [G loss: 0.857988]\n",
      "epoch:19 step:18077 [D loss: 0.480817, acc.: 78.91%] [G loss: 0.644708]\n",
      "epoch:19 step:18078 [D loss: 0.550898, acc.: 69.53%] [G loss: 0.748568]\n",
      "epoch:19 step:18079 [D loss: 0.472355, acc.: 79.69%] [G loss: 0.849168]\n",
      "epoch:19 step:18080 [D loss: 0.666198, acc.: 66.41%] [G loss: 0.538649]\n",
      "epoch:19 step:18081 [D loss: 0.637810, acc.: 62.50%] [G loss: 0.544401]\n",
      "epoch:19 step:18082 [D loss: 0.564934, acc.: 65.62%] [G loss: 0.728328]\n",
      "epoch:19 step:18083 [D loss: 0.546530, acc.: 71.88%] [G loss: 0.639593]\n",
      "epoch:19 step:18084 [D loss: 0.611446, acc.: 65.62%] [G loss: 0.605876]\n",
      "epoch:19 step:18085 [D loss: 0.597741, acc.: 67.97%] [G loss: 0.667574]\n",
      "epoch:19 step:18086 [D loss: 0.500939, acc.: 75.78%] [G loss: 0.755634]\n",
      "epoch:19 step:18087 [D loss: 0.535842, acc.: 66.41%] [G loss: 0.602482]\n",
      "epoch:19 step:18088 [D loss: 0.508053, acc.: 72.66%] [G loss: 0.759720]\n",
      "epoch:19 step:18089 [D loss: 0.539865, acc.: 69.53%] [G loss: 0.641884]\n",
      "epoch:19 step:18090 [D loss: 0.588583, acc.: 67.97%] [G loss: 0.671766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18091 [D loss: 0.577084, acc.: 64.84%] [G loss: 0.583299]\n",
      "epoch:19 step:18092 [D loss: 0.496964, acc.: 75.00%] [G loss: 0.662152]\n",
      "epoch:19 step:18093 [D loss: 0.611420, acc.: 67.97%] [G loss: 0.666599]\n",
      "epoch:19 step:18094 [D loss: 0.618312, acc.: 65.62%] [G loss: 0.623869]\n",
      "epoch:19 step:18095 [D loss: 0.514829, acc.: 72.66%] [G loss: 0.547124]\n",
      "epoch:19 step:18096 [D loss: 0.555482, acc.: 69.53%] [G loss: 0.606616]\n",
      "epoch:19 step:18097 [D loss: 0.594690, acc.: 65.62%] [G loss: 0.548080]\n",
      "epoch:19 step:18098 [D loss: 0.566316, acc.: 64.06%] [G loss: 0.509090]\n",
      "epoch:19 step:18099 [D loss: 0.434581, acc.: 78.91%] [G loss: 0.608575]\n",
      "epoch:19 step:18100 [D loss: 0.521716, acc.: 73.44%] [G loss: 0.583543]\n",
      "epoch:19 step:18101 [D loss: 0.499937, acc.: 76.56%] [G loss: 0.629489]\n",
      "epoch:19 step:18102 [D loss: 0.485289, acc.: 75.78%] [G loss: 0.904757]\n",
      "epoch:19 step:18103 [D loss: 0.517491, acc.: 76.56%] [G loss: 0.610734]\n",
      "epoch:19 step:18104 [D loss: 0.658856, acc.: 63.28%] [G loss: 0.648248]\n",
      "epoch:19 step:18105 [D loss: 0.526735, acc.: 70.31%] [G loss: 0.644238]\n",
      "epoch:19 step:18106 [D loss: 0.529714, acc.: 72.66%] [G loss: 0.755422]\n",
      "epoch:19 step:18107 [D loss: 0.464818, acc.: 76.56%] [G loss: 0.584099]\n",
      "epoch:19 step:18108 [D loss: 0.540013, acc.: 71.09%] [G loss: 0.655478]\n",
      "epoch:19 step:18109 [D loss: 0.530862, acc.: 72.66%] [G loss: 0.722040]\n",
      "epoch:19 step:18110 [D loss: 0.522596, acc.: 71.88%] [G loss: 0.835208]\n",
      "epoch:19 step:18111 [D loss: 0.531128, acc.: 70.31%] [G loss: 0.634973]\n",
      "epoch:19 step:18112 [D loss: 0.495716, acc.: 75.78%] [G loss: 0.516878]\n",
      "epoch:19 step:18113 [D loss: 0.543786, acc.: 72.66%] [G loss: 0.568904]\n",
      "epoch:19 step:18114 [D loss: 0.485718, acc.: 71.88%] [G loss: 0.686467]\n",
      "epoch:19 step:18115 [D loss: 0.504267, acc.: 76.56%] [G loss: 0.838808]\n",
      "epoch:19 step:18116 [D loss: 0.508344, acc.: 74.22%] [G loss: 0.888671]\n",
      "epoch:19 step:18117 [D loss: 0.403307, acc.: 83.59%] [G loss: 1.053726]\n",
      "epoch:19 step:18118 [D loss: 0.424222, acc.: 83.59%] [G loss: 1.276113]\n",
      "epoch:19 step:18119 [D loss: 0.650116, acc.: 64.84%] [G loss: 0.683099]\n",
      "epoch:19 step:18120 [D loss: 0.587247, acc.: 66.41%] [G loss: 0.539856]\n",
      "epoch:19 step:18121 [D loss: 0.594614, acc.: 64.84%] [G loss: 0.727644]\n",
      "epoch:19 step:18122 [D loss: 0.518079, acc.: 75.78%] [G loss: 0.634660]\n",
      "epoch:19 step:18123 [D loss: 0.573103, acc.: 71.09%] [G loss: 0.572096]\n",
      "epoch:19 step:18124 [D loss: 0.524934, acc.: 77.34%] [G loss: 0.804713]\n",
      "epoch:19 step:18125 [D loss: 0.590600, acc.: 69.53%] [G loss: 0.695962]\n",
      "epoch:19 step:18126 [D loss: 0.623882, acc.: 64.84%] [G loss: 0.756857]\n",
      "epoch:19 step:18127 [D loss: 0.549646, acc.: 68.75%] [G loss: 0.567697]\n",
      "epoch:19 step:18128 [D loss: 0.571388, acc.: 67.19%] [G loss: 0.631435]\n",
      "epoch:19 step:18129 [D loss: 0.528073, acc.: 71.09%] [G loss: 0.613766]\n",
      "epoch:19 step:18130 [D loss: 0.531750, acc.: 72.66%] [G loss: 0.731553]\n",
      "epoch:19 step:18131 [D loss: 0.471577, acc.: 78.91%] [G loss: 0.828771]\n",
      "epoch:19 step:18132 [D loss: 0.502831, acc.: 73.44%] [G loss: 0.692893]\n",
      "epoch:19 step:18133 [D loss: 0.568050, acc.: 69.53%] [G loss: 0.537766]\n",
      "epoch:19 step:18134 [D loss: 0.573424, acc.: 67.97%] [G loss: 0.591477]\n",
      "epoch:19 step:18135 [D loss: 0.562531, acc.: 64.84%] [G loss: 0.454857]\n",
      "epoch:19 step:18136 [D loss: 0.465781, acc.: 75.78%] [G loss: 0.619239]\n",
      "epoch:19 step:18137 [D loss: 0.478944, acc.: 80.47%] [G loss: 0.740137]\n",
      "epoch:19 step:18138 [D loss: 0.558509, acc.: 68.75%] [G loss: 0.737527]\n",
      "epoch:19 step:18139 [D loss: 0.520241, acc.: 73.44%] [G loss: 0.794673]\n",
      "epoch:19 step:18140 [D loss: 0.517570, acc.: 75.00%] [G loss: 0.684587]\n",
      "epoch:19 step:18141 [D loss: 0.555511, acc.: 70.31%] [G loss: 0.644926]\n",
      "epoch:19 step:18142 [D loss: 0.532255, acc.: 77.34%] [G loss: 0.611458]\n",
      "epoch:19 step:18143 [D loss: 0.487705, acc.: 75.78%] [G loss: 0.652145]\n",
      "epoch:19 step:18144 [D loss: 0.613678, acc.: 67.19%] [G loss: 0.817605]\n",
      "epoch:19 step:18145 [D loss: 0.626784, acc.: 63.28%] [G loss: 0.627750]\n",
      "epoch:19 step:18146 [D loss: 0.524008, acc.: 73.44%] [G loss: 0.593951]\n",
      "epoch:19 step:18147 [D loss: 0.465053, acc.: 78.91%] [G loss: 0.802737]\n",
      "epoch:19 step:18148 [D loss: 0.573668, acc.: 66.41%] [G loss: 0.818101]\n",
      "epoch:19 step:18149 [D loss: 0.522610, acc.: 70.31%] [G loss: 0.991836]\n",
      "epoch:19 step:18150 [D loss: 0.439681, acc.: 79.69%] [G loss: 1.179132]\n",
      "epoch:19 step:18151 [D loss: 0.631293, acc.: 64.06%] [G loss: 0.870124]\n",
      "epoch:19 step:18152 [D loss: 0.724310, acc.: 57.81%] [G loss: 0.491799]\n",
      "epoch:19 step:18153 [D loss: 0.532125, acc.: 68.75%] [G loss: 0.449373]\n",
      "epoch:19 step:18154 [D loss: 0.530440, acc.: 74.22%] [G loss: 0.681855]\n",
      "epoch:19 step:18155 [D loss: 0.541496, acc.: 66.41%] [G loss: 0.809044]\n",
      "epoch:19 step:18156 [D loss: 0.562624, acc.: 67.97%] [G loss: 0.664592]\n",
      "epoch:19 step:18157 [D loss: 0.427277, acc.: 76.56%] [G loss: 0.857266]\n",
      "epoch:19 step:18158 [D loss: 0.559535, acc.: 71.88%] [G loss: 0.760000]\n",
      "epoch:19 step:18159 [D loss: 0.519331, acc.: 74.22%] [G loss: 0.788561]\n",
      "epoch:19 step:18160 [D loss: 0.482386, acc.: 72.66%] [G loss: 0.673824]\n",
      "epoch:19 step:18161 [D loss: 0.487629, acc.: 78.91%] [G loss: 0.789626]\n",
      "epoch:19 step:18162 [D loss: 0.430654, acc.: 79.69%] [G loss: 0.857658]\n",
      "epoch:19 step:18163 [D loss: 0.507452, acc.: 71.88%] [G loss: 0.804837]\n",
      "epoch:19 step:18164 [D loss: 0.507907, acc.: 75.78%] [G loss: 0.856689]\n",
      "epoch:19 step:18165 [D loss: 0.597575, acc.: 69.53%] [G loss: 0.715877]\n",
      "epoch:19 step:18166 [D loss: 0.597836, acc.: 65.62%] [G loss: 0.659702]\n",
      "epoch:19 step:18167 [D loss: 0.541166, acc.: 71.09%] [G loss: 0.662688]\n",
      "epoch:19 step:18168 [D loss: 0.560157, acc.: 68.75%] [G loss: 0.535834]\n",
      "epoch:19 step:18169 [D loss: 0.549574, acc.: 67.97%] [G loss: 0.615430]\n",
      "epoch:19 step:18170 [D loss: 0.596437, acc.: 64.84%] [G loss: 0.548745]\n",
      "epoch:19 step:18171 [D loss: 0.539242, acc.: 71.88%] [G loss: 0.657938]\n",
      "epoch:19 step:18172 [D loss: 0.522061, acc.: 71.09%] [G loss: 0.611931]\n",
      "epoch:19 step:18173 [D loss: 0.539512, acc.: 71.09%] [G loss: 0.669785]\n",
      "epoch:19 step:18174 [D loss: 0.494296, acc.: 77.34%] [G loss: 0.726315]\n",
      "epoch:19 step:18175 [D loss: 0.521179, acc.: 71.09%] [G loss: 0.596829]\n",
      "epoch:19 step:18176 [D loss: 0.528542, acc.: 72.66%] [G loss: 0.630149]\n",
      "epoch:19 step:18177 [D loss: 0.458818, acc.: 80.47%] [G loss: 0.711481]\n",
      "epoch:19 step:18178 [D loss: 0.539311, acc.: 72.66%] [G loss: 0.897464]\n",
      "epoch:19 step:18179 [D loss: 0.661192, acc.: 61.72%] [G loss: 0.497550]\n",
      "epoch:19 step:18180 [D loss: 0.564636, acc.: 68.75%] [G loss: 0.550176]\n",
      "epoch:19 step:18181 [D loss: 0.578125, acc.: 64.84%] [G loss: 0.606410]\n",
      "epoch:19 step:18182 [D loss: 0.572127, acc.: 68.75%] [G loss: 0.622557]\n",
      "epoch:19 step:18183 [D loss: 0.624635, acc.: 62.50%] [G loss: 0.549227]\n",
      "epoch:19 step:18184 [D loss: 0.444599, acc.: 78.12%] [G loss: 0.692416]\n",
      "epoch:19 step:18185 [D loss: 0.553448, acc.: 71.88%] [G loss: 0.784669]\n",
      "epoch:19 step:18186 [D loss: 0.520167, acc.: 73.44%] [G loss: 0.683585]\n",
      "epoch:19 step:18187 [D loss: 0.563895, acc.: 71.09%] [G loss: 0.680402]\n",
      "epoch:19 step:18188 [D loss: 0.442508, acc.: 81.25%] [G loss: 0.656613]\n",
      "epoch:19 step:18189 [D loss: 0.589496, acc.: 67.19%] [G loss: 0.577758]\n",
      "epoch:19 step:18190 [D loss: 0.555554, acc.: 72.66%] [G loss: 0.557180]\n",
      "epoch:19 step:18191 [D loss: 0.526644, acc.: 74.22%] [G loss: 0.637210]\n",
      "epoch:19 step:18192 [D loss: 0.546575, acc.: 76.56%] [G loss: 0.686582]\n",
      "epoch:19 step:18193 [D loss: 0.573225, acc.: 64.84%] [G loss: 0.569478]\n",
      "epoch:19 step:18194 [D loss: 0.542031, acc.: 68.75%] [G loss: 0.559134]\n",
      "epoch:19 step:18195 [D loss: 0.505967, acc.: 74.22%] [G loss: 0.594780]\n",
      "epoch:19 step:18196 [D loss: 0.549346, acc.: 71.09%] [G loss: 0.637278]\n",
      "epoch:19 step:18197 [D loss: 0.564696, acc.: 67.19%] [G loss: 0.535186]\n",
      "epoch:19 step:18198 [D loss: 0.544062, acc.: 69.53%] [G loss: 0.613573]\n",
      "epoch:19 step:18199 [D loss: 0.559798, acc.: 68.75%] [G loss: 0.653600]\n",
      "epoch:19 step:18200 [D loss: 0.587788, acc.: 61.72%] [G loss: 0.744248]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.642352\n",
      "FID: 53.206604\n",
      "0 = 12.94502755527497\n",
      "1 = 0.09790252150502057\n",
      "2 = 0.8733999729156494\n",
      "3 = 0.8302000164985657\n",
      "4 = 0.9165999889373779\n",
      "5 = 0.9087128043174744\n",
      "6 = 0.8302000164985657\n",
      "7 = 8.614160685873035\n",
      "8 = 0.1608595872445024\n",
      "9 = 0.7013000249862671\n",
      "10 = 0.6972000002861023\n",
      "11 = 0.7053999900817871\n",
      "12 = 0.7029643058776855\n",
      "13 = 0.6972000002861023\n",
      "14 = 6.642375946044922\n",
      "15 = 7.219158172607422\n",
      "16 = 0.37739577889442444\n",
      "17 = 6.6423516273498535\n",
      "18 = 53.20660400390625\n",
      "epoch:19 step:18201 [D loss: 0.484934, acc.: 75.78%] [G loss: 0.548142]\n",
      "epoch:19 step:18202 [D loss: 0.507153, acc.: 74.22%] [G loss: 0.874081]\n",
      "epoch:19 step:18203 [D loss: 0.629380, acc.: 59.38%] [G loss: 0.569222]\n",
      "epoch:19 step:18204 [D loss: 0.658443, acc.: 53.12%] [G loss: 0.371924]\n",
      "epoch:19 step:18205 [D loss: 0.513182, acc.: 73.44%] [G loss: 0.463005]\n",
      "epoch:19 step:18206 [D loss: 0.492343, acc.: 72.66%] [G loss: 0.638151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18207 [D loss: 0.616943, acc.: 62.50%] [G loss: 0.477778]\n",
      "epoch:19 step:18208 [D loss: 0.542163, acc.: 69.53%] [G loss: 0.743936]\n",
      "epoch:19 step:18209 [D loss: 0.490148, acc.: 71.09%] [G loss: 0.726927]\n",
      "epoch:19 step:18210 [D loss: 0.607190, acc.: 66.41%] [G loss: 0.558230]\n",
      "epoch:19 step:18211 [D loss: 0.585437, acc.: 66.41%] [G loss: 0.684129]\n",
      "epoch:19 step:18212 [D loss: 0.534475, acc.: 71.09%] [G loss: 0.570375]\n",
      "epoch:19 step:18213 [D loss: 0.607682, acc.: 62.50%] [G loss: 0.600729]\n",
      "epoch:19 step:18214 [D loss: 0.563071, acc.: 68.75%] [G loss: 0.592229]\n",
      "epoch:19 step:18215 [D loss: 0.615127, acc.: 64.84%] [G loss: 0.528851]\n",
      "epoch:19 step:18216 [D loss: 0.529570, acc.: 72.66%] [G loss: 0.527015]\n",
      "epoch:19 step:18217 [D loss: 0.542727, acc.: 77.34%] [G loss: 0.704999]\n",
      "epoch:19 step:18218 [D loss: 0.589901, acc.: 68.75%] [G loss: 0.601316]\n",
      "epoch:19 step:18219 [D loss: 0.510333, acc.: 78.91%] [G loss: 0.792511]\n",
      "epoch:19 step:18220 [D loss: 0.530398, acc.: 75.78%] [G loss: 0.591983]\n",
      "epoch:19 step:18221 [D loss: 0.632944, acc.: 60.94%] [G loss: 0.655447]\n",
      "epoch:19 step:18222 [D loss: 0.566739, acc.: 70.31%] [G loss: 0.610776]\n",
      "epoch:19 step:18223 [D loss: 0.615573, acc.: 66.41%] [G loss: 0.639146]\n",
      "epoch:19 step:18224 [D loss: 0.610045, acc.: 69.53%] [G loss: 0.587117]\n",
      "epoch:19 step:18225 [D loss: 0.595581, acc.: 66.41%] [G loss: 0.618065]\n",
      "epoch:19 step:18226 [D loss: 0.581589, acc.: 67.97%] [G loss: 0.511104]\n",
      "epoch:19 step:18227 [D loss: 0.567003, acc.: 67.19%] [G loss: 0.558250]\n",
      "epoch:19 step:18228 [D loss: 0.504997, acc.: 71.09%] [G loss: 0.732751]\n",
      "epoch:19 step:18229 [D loss: 0.474078, acc.: 78.91%] [G loss: 0.569455]\n",
      "epoch:19 step:18230 [D loss: 0.474677, acc.: 78.12%] [G loss: 0.821983]\n",
      "epoch:19 step:18231 [D loss: 0.591226, acc.: 64.06%] [G loss: 0.788922]\n",
      "epoch:19 step:18232 [D loss: 0.453771, acc.: 79.69%] [G loss: 0.764486]\n",
      "epoch:19 step:18233 [D loss: 0.521305, acc.: 71.88%] [G loss: 0.759686]\n",
      "epoch:19 step:18234 [D loss: 0.518333, acc.: 74.22%] [G loss: 0.815639]\n",
      "epoch:19 step:18235 [D loss: 0.580619, acc.: 69.53%] [G loss: 0.701801]\n",
      "epoch:19 step:18236 [D loss: 0.558632, acc.: 71.88%] [G loss: 0.614439]\n",
      "epoch:19 step:18237 [D loss: 0.543919, acc.: 67.97%] [G loss: 0.641720]\n",
      "epoch:19 step:18238 [D loss: 0.538676, acc.: 70.31%] [G loss: 0.630119]\n",
      "epoch:19 step:18239 [D loss: 0.450608, acc.: 76.56%] [G loss: 0.793074]\n",
      "epoch:19 step:18240 [D loss: 0.671832, acc.: 64.84%] [G loss: 0.562478]\n",
      "epoch:19 step:18241 [D loss: 0.572610, acc.: 65.62%] [G loss: 0.577074]\n",
      "epoch:19 step:18242 [D loss: 0.502895, acc.: 75.00%] [G loss: 0.658929]\n",
      "epoch:19 step:18243 [D loss: 0.489929, acc.: 74.22%] [G loss: 0.759607]\n",
      "epoch:19 step:18244 [D loss: 0.495125, acc.: 77.34%] [G loss: 0.796420]\n",
      "epoch:19 step:18245 [D loss: 0.567205, acc.: 69.53%] [G loss: 0.584516]\n",
      "epoch:19 step:18246 [D loss: 0.546934, acc.: 71.09%] [G loss: 0.741099]\n",
      "epoch:19 step:18247 [D loss: 0.565691, acc.: 68.75%] [G loss: 0.635160]\n",
      "epoch:19 step:18248 [D loss: 0.563974, acc.: 66.41%] [G loss: 0.792712]\n",
      "epoch:19 step:18249 [D loss: 0.545441, acc.: 72.66%] [G loss: 0.810540]\n",
      "epoch:19 step:18250 [D loss: 0.526940, acc.: 71.88%] [G loss: 0.807840]\n",
      "epoch:19 step:18251 [D loss: 0.472014, acc.: 77.34%] [G loss: 0.805650]\n",
      "epoch:19 step:18252 [D loss: 0.496222, acc.: 77.34%] [G loss: 0.795765]\n",
      "epoch:19 step:18253 [D loss: 0.499837, acc.: 78.91%] [G loss: 0.817455]\n",
      "epoch:19 step:18254 [D loss: 0.383710, acc.: 85.16%] [G loss: 0.792244]\n",
      "epoch:19 step:18255 [D loss: 0.490299, acc.: 72.66%] [G loss: 0.973839]\n",
      "epoch:19 step:18256 [D loss: 0.490352, acc.: 74.22%] [G loss: 0.826498]\n",
      "epoch:19 step:18257 [D loss: 0.533442, acc.: 76.56%] [G loss: 0.668548]\n",
      "epoch:19 step:18258 [D loss: 0.595892, acc.: 70.31%] [G loss: 0.551910]\n",
      "epoch:19 step:18259 [D loss: 0.607301, acc.: 64.84%] [G loss: 0.570221]\n",
      "epoch:19 step:18260 [D loss: 0.507348, acc.: 72.66%] [G loss: 0.692029]\n",
      "epoch:19 step:18261 [D loss: 0.616359, acc.: 61.72%] [G loss: 0.511989]\n",
      "epoch:19 step:18262 [D loss: 0.543748, acc.: 69.53%] [G loss: 0.605097]\n",
      "epoch:19 step:18263 [D loss: 0.559828, acc.: 73.44%] [G loss: 0.632731]\n",
      "epoch:19 step:18264 [D loss: 0.527633, acc.: 67.19%] [G loss: 0.661044]\n",
      "epoch:19 step:18265 [D loss: 0.539694, acc.: 67.97%] [G loss: 0.794656]\n",
      "epoch:19 step:18266 [D loss: 0.566744, acc.: 67.19%] [G loss: 0.514116]\n",
      "epoch:19 step:18267 [D loss: 0.504000, acc.: 73.44%] [G loss: 0.545833]\n",
      "epoch:19 step:18268 [D loss: 0.559923, acc.: 68.75%] [G loss: 0.643530]\n",
      "epoch:19 step:18269 [D loss: 0.611218, acc.: 65.62%] [G loss: 0.638459]\n",
      "epoch:19 step:18270 [D loss: 0.514015, acc.: 75.00%] [G loss: 0.633018]\n",
      "epoch:19 step:18271 [D loss: 0.549917, acc.: 69.53%] [G loss: 0.590159]\n",
      "epoch:19 step:18272 [D loss: 0.523455, acc.: 71.88%] [G loss: 0.692420]\n",
      "epoch:19 step:18273 [D loss: 0.536719, acc.: 71.09%] [G loss: 0.680226]\n",
      "epoch:19 step:18274 [D loss: 0.470485, acc.: 75.00%] [G loss: 0.811398]\n",
      "epoch:19 step:18275 [D loss: 0.396243, acc.: 85.16%] [G loss: 0.916268]\n",
      "epoch:19 step:18276 [D loss: 0.689238, acc.: 57.81%] [G loss: 0.698324]\n",
      "epoch:19 step:18277 [D loss: 0.541409, acc.: 69.53%] [G loss: 0.660438]\n",
      "epoch:19 step:18278 [D loss: 0.449088, acc.: 82.81%] [G loss: 0.863101]\n",
      "epoch:19 step:18279 [D loss: 0.601507, acc.: 71.09%] [G loss: 0.796360]\n",
      "epoch:19 step:18280 [D loss: 0.693803, acc.: 60.16%] [G loss: 0.546082]\n",
      "epoch:19 step:18281 [D loss: 0.584378, acc.: 70.31%] [G loss: 0.527356]\n",
      "epoch:19 step:18282 [D loss: 0.502389, acc.: 75.00%] [G loss: 0.549148]\n",
      "epoch:19 step:18283 [D loss: 0.581987, acc.: 71.09%] [G loss: 0.502724]\n",
      "epoch:19 step:18284 [D loss: 0.495875, acc.: 79.69%] [G loss: 0.596769]\n",
      "epoch:19 step:18285 [D loss: 0.685385, acc.: 59.38%] [G loss: 0.485254]\n",
      "epoch:19 step:18286 [D loss: 0.588105, acc.: 65.62%] [G loss: 0.688330]\n",
      "epoch:19 step:18287 [D loss: 0.491403, acc.: 75.00%] [G loss: 0.702281]\n",
      "epoch:19 step:18288 [D loss: 0.546677, acc.: 71.09%] [G loss: 0.616324]\n",
      "epoch:19 step:18289 [D loss: 0.572469, acc.: 69.53%] [G loss: 0.554148]\n",
      "epoch:19 step:18290 [D loss: 0.585144, acc.: 65.62%] [G loss: 0.604662]\n",
      "epoch:19 step:18291 [D loss: 0.477310, acc.: 76.56%] [G loss: 0.737041]\n",
      "epoch:19 step:18292 [D loss: 0.561713, acc.: 70.31%] [G loss: 0.639148]\n",
      "epoch:19 step:18293 [D loss: 0.574955, acc.: 72.66%] [G loss: 0.573353]\n",
      "epoch:19 step:18294 [D loss: 0.526857, acc.: 75.78%] [G loss: 0.647737]\n",
      "epoch:19 step:18295 [D loss: 0.572189, acc.: 70.31%] [G loss: 0.676980]\n",
      "epoch:19 step:18296 [D loss: 0.584937, acc.: 67.97%] [G loss: 0.585754]\n",
      "epoch:19 step:18297 [D loss: 0.560899, acc.: 70.31%] [G loss: 0.649490]\n",
      "epoch:19 step:18298 [D loss: 0.520468, acc.: 73.44%] [G loss: 0.650198]\n",
      "epoch:19 step:18299 [D loss: 0.564887, acc.: 70.31%] [G loss: 0.680184]\n",
      "epoch:19 step:18300 [D loss: 0.579052, acc.: 71.09%] [G loss: 0.643727]\n",
      "epoch:19 step:18301 [D loss: 0.565937, acc.: 71.88%] [G loss: 0.748935]\n",
      "epoch:19 step:18302 [D loss: 0.482736, acc.: 82.03%] [G loss: 0.756183]\n",
      "epoch:19 step:18303 [D loss: 0.548179, acc.: 73.44%] [G loss: 0.657316]\n",
      "epoch:19 step:18304 [D loss: 0.620769, acc.: 65.62%] [G loss: 0.556418]\n",
      "epoch:19 step:18305 [D loss: 0.573860, acc.: 64.84%] [G loss: 0.525553]\n",
      "epoch:19 step:18306 [D loss: 0.585258, acc.: 65.62%] [G loss: 0.566421]\n",
      "epoch:19 step:18307 [D loss: 0.494633, acc.: 75.00%] [G loss: 0.718700]\n",
      "epoch:19 step:18308 [D loss: 0.516150, acc.: 73.44%] [G loss: 0.725471]\n",
      "epoch:19 step:18309 [D loss: 0.580327, acc.: 67.19%] [G loss: 0.549662]\n",
      "epoch:19 step:18310 [D loss: 0.511187, acc.: 74.22%] [G loss: 0.698835]\n",
      "epoch:19 step:18311 [D loss: 0.520348, acc.: 75.00%] [G loss: 0.736029]\n",
      "epoch:19 step:18312 [D loss: 0.500375, acc.: 77.34%] [G loss: 0.849391]\n",
      "epoch:19 step:18313 [D loss: 0.613149, acc.: 63.28%] [G loss: 0.627201]\n",
      "epoch:19 step:18314 [D loss: 0.661488, acc.: 59.38%] [G loss: 0.456423]\n",
      "epoch:19 step:18315 [D loss: 0.587958, acc.: 69.53%] [G loss: 0.517395]\n",
      "epoch:19 step:18316 [D loss: 0.520533, acc.: 71.09%] [G loss: 0.536486]\n",
      "epoch:19 step:18317 [D loss: 0.488491, acc.: 79.69%] [G loss: 0.594668]\n",
      "epoch:19 step:18318 [D loss: 0.544344, acc.: 72.66%] [G loss: 0.598151]\n",
      "epoch:19 step:18319 [D loss: 0.487124, acc.: 72.66%] [G loss: 0.723411]\n",
      "epoch:19 step:18320 [D loss: 0.517607, acc.: 75.00%] [G loss: 0.642306]\n",
      "epoch:19 step:18321 [D loss: 0.532330, acc.: 69.53%] [G loss: 0.630694]\n",
      "epoch:19 step:18322 [D loss: 0.481777, acc.: 76.56%] [G loss: 0.764894]\n",
      "epoch:19 step:18323 [D loss: 0.493813, acc.: 72.66%] [G loss: 0.631797]\n",
      "epoch:19 step:18324 [D loss: 0.530584, acc.: 75.00%] [G loss: 0.724984]\n",
      "epoch:19 step:18325 [D loss: 0.548307, acc.: 74.22%] [G loss: 0.689122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18326 [D loss: 0.520974, acc.: 73.44%] [G loss: 0.817566]\n",
      "epoch:19 step:18327 [D loss: 0.555187, acc.: 74.22%] [G loss: 0.605208]\n",
      "epoch:19 step:18328 [D loss: 0.614619, acc.: 64.06%] [G loss: 0.561205]\n",
      "epoch:19 step:18329 [D loss: 0.518290, acc.: 71.09%] [G loss: 0.654747]\n",
      "epoch:19 step:18330 [D loss: 0.561938, acc.: 71.09%] [G loss: 0.612758]\n",
      "epoch:19 step:18331 [D loss: 0.673952, acc.: 60.94%] [G loss: 0.528183]\n",
      "epoch:19 step:18332 [D loss: 0.565387, acc.: 69.53%] [G loss: 0.480842]\n",
      "epoch:19 step:18333 [D loss: 0.535942, acc.: 71.09%] [G loss: 0.729395]\n",
      "epoch:19 step:18334 [D loss: 0.597041, acc.: 67.97%] [G loss: 0.561041]\n",
      "epoch:19 step:18335 [D loss: 0.590672, acc.: 63.28%] [G loss: 0.579026]\n",
      "epoch:19 step:18336 [D loss: 0.539824, acc.: 70.31%] [G loss: 0.826547]\n",
      "epoch:19 step:18337 [D loss: 0.521672, acc.: 72.66%] [G loss: 0.640774]\n",
      "epoch:19 step:18338 [D loss: 0.646837, acc.: 62.50%] [G loss: 0.603794]\n",
      "epoch:19 step:18339 [D loss: 0.493104, acc.: 73.44%] [G loss: 0.564136]\n",
      "epoch:19 step:18340 [D loss: 0.570603, acc.: 67.19%] [G loss: 0.485835]\n",
      "epoch:19 step:18341 [D loss: 0.540509, acc.: 75.00%] [G loss: 0.473725]\n",
      "epoch:19 step:18342 [D loss: 0.574067, acc.: 67.97%] [G loss: 0.616897]\n",
      "epoch:19 step:18343 [D loss: 0.551311, acc.: 69.53%] [G loss: 0.542734]\n",
      "epoch:19 step:18344 [D loss: 0.533820, acc.: 75.78%] [G loss: 0.555139]\n",
      "epoch:19 step:18345 [D loss: 0.602957, acc.: 70.31%] [G loss: 0.605861]\n",
      "epoch:19 step:18346 [D loss: 0.524260, acc.: 73.44%] [G loss: 0.557158]\n",
      "epoch:19 step:18347 [D loss: 0.572063, acc.: 72.66%] [G loss: 0.589288]\n",
      "epoch:19 step:18348 [D loss: 0.591672, acc.: 70.31%] [G loss: 0.511377]\n",
      "epoch:19 step:18349 [D loss: 0.524262, acc.: 75.00%] [G loss: 0.715853]\n",
      "epoch:19 step:18350 [D loss: 0.552725, acc.: 72.66%] [G loss: 0.659960]\n",
      "epoch:19 step:18351 [D loss: 0.474705, acc.: 76.56%] [G loss: 0.874038]\n",
      "epoch:19 step:18352 [D loss: 0.537333, acc.: 68.75%] [G loss: 0.682865]\n",
      "epoch:19 step:18353 [D loss: 0.590894, acc.: 67.97%] [G loss: 0.608834]\n",
      "epoch:19 step:18354 [D loss: 0.513573, acc.: 75.00%] [G loss: 0.619813]\n",
      "epoch:19 step:18355 [D loss: 0.469799, acc.: 77.34%] [G loss: 0.570560]\n",
      "epoch:19 step:18356 [D loss: 0.576282, acc.: 67.19%] [G loss: 0.701339]\n",
      "epoch:19 step:18357 [D loss: 0.444831, acc.: 80.47%] [G loss: 0.686200]\n",
      "epoch:19 step:18358 [D loss: 0.474476, acc.: 74.22%] [G loss: 0.832659]\n",
      "epoch:19 step:18359 [D loss: 0.535600, acc.: 71.88%] [G loss: 0.776391]\n",
      "epoch:19 step:18360 [D loss: 0.473993, acc.: 75.00%] [G loss: 0.753757]\n",
      "epoch:19 step:18361 [D loss: 0.485939, acc.: 78.12%] [G loss: 0.822120]\n",
      "epoch:19 step:18362 [D loss: 0.590086, acc.: 67.97%] [G loss: 0.659752]\n",
      "epoch:19 step:18363 [D loss: 0.533916, acc.: 71.09%] [G loss: 0.688064]\n",
      "epoch:19 step:18364 [D loss: 0.578866, acc.: 71.88%] [G loss: 0.711698]\n",
      "epoch:19 step:18365 [D loss: 0.540190, acc.: 69.53%] [G loss: 0.669797]\n",
      "epoch:19 step:18366 [D loss: 0.593319, acc.: 64.06%] [G loss: 0.645864]\n",
      "epoch:19 step:18367 [D loss: 0.474178, acc.: 74.22%] [G loss: 0.814662]\n",
      "epoch:19 step:18368 [D loss: 0.608900, acc.: 65.62%] [G loss: 0.863289]\n",
      "epoch:19 step:18369 [D loss: 0.650809, acc.: 62.50%] [G loss: 0.630740]\n",
      "epoch:19 step:18370 [D loss: 0.486417, acc.: 76.56%] [G loss: 0.803773]\n",
      "epoch:19 step:18371 [D loss: 0.522212, acc.: 73.44%] [G loss: 0.764129]\n",
      "epoch:19 step:18372 [D loss: 0.583188, acc.: 66.41%] [G loss: 0.677154]\n",
      "epoch:19 step:18373 [D loss: 0.544270, acc.: 74.22%] [G loss: 0.618209]\n",
      "epoch:19 step:18374 [D loss: 0.547828, acc.: 68.75%] [G loss: 0.651172]\n",
      "epoch:19 step:18375 [D loss: 0.608140, acc.: 64.84%] [G loss: 0.684345]\n",
      "epoch:19 step:18376 [D loss: 0.493094, acc.: 76.56%] [G loss: 0.647659]\n",
      "epoch:19 step:18377 [D loss: 0.499419, acc.: 74.22%] [G loss: 0.741629]\n",
      "epoch:19 step:18378 [D loss: 0.513571, acc.: 77.34%] [G loss: 0.987775]\n",
      "epoch:19 step:18379 [D loss: 0.619672, acc.: 64.06%] [G loss: 0.713959]\n",
      "epoch:19 step:18380 [D loss: 0.515581, acc.: 74.22%] [G loss: 0.619099]\n",
      "epoch:19 step:18381 [D loss: 0.533763, acc.: 73.44%] [G loss: 0.553137]\n",
      "epoch:19 step:18382 [D loss: 0.537138, acc.: 71.09%] [G loss: 0.725870]\n",
      "epoch:19 step:18383 [D loss: 0.550105, acc.: 72.66%] [G loss: 0.710272]\n",
      "epoch:19 step:18384 [D loss: 0.569161, acc.: 69.53%] [G loss: 0.650440]\n",
      "epoch:19 step:18385 [D loss: 0.447929, acc.: 82.03%] [G loss: 0.759228]\n",
      "epoch:19 step:18386 [D loss: 0.605852, acc.: 64.84%] [G loss: 0.724242]\n",
      "epoch:19 step:18387 [D loss: 0.669587, acc.: 57.03%] [G loss: 0.726529]\n",
      "epoch:19 step:18388 [D loss: 0.566035, acc.: 68.75%] [G loss: 0.598217]\n",
      "epoch:19 step:18389 [D loss: 0.629945, acc.: 60.94%] [G loss: 0.571360]\n",
      "epoch:19 step:18390 [D loss: 0.548642, acc.: 68.75%] [G loss: 0.607075]\n",
      "epoch:19 step:18391 [D loss: 0.532166, acc.: 71.09%] [G loss: 0.613141]\n",
      "epoch:19 step:18392 [D loss: 0.494388, acc.: 75.78%] [G loss: 0.735502]\n",
      "epoch:19 step:18393 [D loss: 0.576299, acc.: 66.41%] [G loss: 0.674017]\n",
      "epoch:19 step:18394 [D loss: 0.584131, acc.: 70.31%] [G loss: 0.695082]\n",
      "epoch:19 step:18395 [D loss: 0.535582, acc.: 75.00%] [G loss: 0.479771]\n",
      "epoch:19 step:18396 [D loss: 0.510160, acc.: 71.09%] [G loss: 0.635414]\n",
      "epoch:19 step:18397 [D loss: 0.576007, acc.: 67.19%] [G loss: 0.548462]\n",
      "epoch:19 step:18398 [D loss: 0.576440, acc.: 64.06%] [G loss: 0.580146]\n",
      "epoch:19 step:18399 [D loss: 0.481745, acc.: 78.91%] [G loss: 0.694484]\n",
      "epoch:19 step:18400 [D loss: 0.569027, acc.: 67.19%] [G loss: 0.766252]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.496746\n",
      "FID: 54.833023\n",
      "0 = 12.993463525104527\n",
      "1 = 0.09313828853549329\n",
      "2 = 0.8762999773025513\n",
      "3 = 0.8281999826431274\n",
      "4 = 0.9243999719619751\n",
      "5 = 0.916353166103363\n",
      "6 = 0.8281999826431274\n",
      "7 = 8.708697311758986\n",
      "8 = 0.16282148105796326\n",
      "9 = 0.7121000289916992\n",
      "10 = 0.7057999968528748\n",
      "11 = 0.7184000015258789\n",
      "12 = 0.7148065567016602\n",
      "13 = 0.7057999968528748\n",
      "14 = 6.496776103973389\n",
      "15 = 6.698073387145996\n",
      "16 = 0.41138043999671936\n",
      "17 = 6.496746063232422\n",
      "18 = 54.83302307128906\n",
      "epoch:19 step:18401 [D loss: 0.502294, acc.: 75.78%] [G loss: 0.916203]\n",
      "epoch:19 step:18402 [D loss: 0.585514, acc.: 71.09%] [G loss: 0.735254]\n",
      "epoch:19 step:18403 [D loss: 0.539948, acc.: 71.88%] [G loss: 0.800294]\n",
      "epoch:19 step:18404 [D loss: 0.496597, acc.: 73.44%] [G loss: 0.836611]\n",
      "epoch:19 step:18405 [D loss: 0.503402, acc.: 70.31%] [G loss: 0.690538]\n",
      "epoch:19 step:18406 [D loss: 0.516970, acc.: 67.97%] [G loss: 0.716427]\n",
      "epoch:19 step:18407 [D loss: 0.558350, acc.: 71.09%] [G loss: 0.756013]\n",
      "epoch:19 step:18408 [D loss: 0.452539, acc.: 75.00%] [G loss: 0.765017]\n",
      "epoch:19 step:18409 [D loss: 0.669851, acc.: 63.28%] [G loss: 0.608923]\n",
      "epoch:19 step:18410 [D loss: 0.567827, acc.: 71.88%] [G loss: 0.548687]\n",
      "epoch:19 step:18411 [D loss: 0.576672, acc.: 64.06%] [G loss: 0.637948]\n",
      "epoch:19 step:18412 [D loss: 0.522728, acc.: 70.31%] [G loss: 0.507594]\n",
      "epoch:19 step:18413 [D loss: 0.553159, acc.: 67.97%] [G loss: 0.559261]\n",
      "epoch:19 step:18414 [D loss: 0.487409, acc.: 74.22%] [G loss: 0.595469]\n",
      "epoch:19 step:18415 [D loss: 0.548283, acc.: 70.31%] [G loss: 0.647176]\n",
      "epoch:19 step:18416 [D loss: 0.439585, acc.: 80.47%] [G loss: 0.670397]\n",
      "epoch:19 step:18417 [D loss: 0.557190, acc.: 68.75%] [G loss: 0.610567]\n",
      "epoch:19 step:18418 [D loss: 0.575053, acc.: 64.84%] [G loss: 0.752011]\n",
      "epoch:19 step:18419 [D loss: 0.569865, acc.: 71.88%] [G loss: 0.818894]\n",
      "epoch:19 step:18420 [D loss: 0.490654, acc.: 80.47%] [G loss: 0.729914]\n",
      "epoch:19 step:18421 [D loss: 0.563401, acc.: 71.88%] [G loss: 0.853333]\n",
      "epoch:19 step:18422 [D loss: 0.510064, acc.: 71.88%] [G loss: 0.803468]\n",
      "epoch:19 step:18423 [D loss: 0.544605, acc.: 69.53%] [G loss: 0.739279]\n",
      "epoch:19 step:18424 [D loss: 0.524400, acc.: 71.09%] [G loss: 0.664576]\n",
      "epoch:19 step:18425 [D loss: 0.554207, acc.: 68.75%] [G loss: 0.635145]\n",
      "epoch:19 step:18426 [D loss: 0.500716, acc.: 75.78%] [G loss: 0.752841]\n",
      "epoch:19 step:18427 [D loss: 0.442150, acc.: 80.47%] [G loss: 0.807593]\n",
      "epoch:19 step:18428 [D loss: 0.572601, acc.: 66.41%] [G loss: 0.626351]\n",
      "epoch:19 step:18429 [D loss: 0.540092, acc.: 73.44%] [G loss: 0.553006]\n",
      "epoch:19 step:18430 [D loss: 0.573895, acc.: 66.41%] [G loss: 0.681546]\n",
      "epoch:19 step:18431 [D loss: 0.587923, acc.: 68.75%] [G loss: 0.594962]\n",
      "epoch:19 step:18432 [D loss: 0.529825, acc.: 70.31%] [G loss: 0.588714]\n",
      "epoch:19 step:18433 [D loss: 0.472323, acc.: 75.78%] [G loss: 0.635196]\n",
      "epoch:19 step:18434 [D loss: 0.474449, acc.: 79.69%] [G loss: 0.589792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18435 [D loss: 0.503785, acc.: 75.00%] [G loss: 0.776743]\n",
      "epoch:19 step:18436 [D loss: 0.535237, acc.: 76.56%] [G loss: 0.648347]\n",
      "epoch:19 step:18437 [D loss: 0.488500, acc.: 78.12%] [G loss: 0.867872]\n",
      "epoch:19 step:18438 [D loss: 0.475362, acc.: 73.44%] [G loss: 0.862334]\n",
      "epoch:19 step:18439 [D loss: 0.600828, acc.: 67.97%] [G loss: 0.607163]\n",
      "epoch:19 step:18440 [D loss: 0.543309, acc.: 69.53%] [G loss: 0.598612]\n",
      "epoch:19 step:18441 [D loss: 0.506750, acc.: 77.34%] [G loss: 0.660825]\n",
      "epoch:19 step:18442 [D loss: 0.529491, acc.: 71.88%] [G loss: 0.575940]\n",
      "epoch:19 step:18443 [D loss: 0.591087, acc.: 66.41%] [G loss: 0.571586]\n",
      "epoch:19 step:18444 [D loss: 0.531381, acc.: 69.53%] [G loss: 0.569104]\n",
      "epoch:19 step:18445 [D loss: 0.478268, acc.: 75.00%] [G loss: 1.021477]\n",
      "epoch:19 step:18446 [D loss: 0.550730, acc.: 69.53%] [G loss: 0.705773]\n",
      "epoch:19 step:18447 [D loss: 0.541248, acc.: 67.19%] [G loss: 0.610526]\n",
      "epoch:19 step:18448 [D loss: 0.521689, acc.: 70.31%] [G loss: 0.497571]\n",
      "epoch:19 step:18449 [D loss: 0.511527, acc.: 71.88%] [G loss: 0.644286]\n",
      "epoch:19 step:18450 [D loss: 0.472200, acc.: 80.47%] [G loss: 0.756330]\n",
      "epoch:19 step:18451 [D loss: 0.402094, acc.: 88.28%] [G loss: 0.876113]\n",
      "epoch:19 step:18452 [D loss: 0.486332, acc.: 76.56%] [G loss: 0.957258]\n",
      "epoch:19 step:18453 [D loss: 0.529759, acc.: 72.66%] [G loss: 0.730616]\n",
      "epoch:19 step:18454 [D loss: 0.517485, acc.: 74.22%] [G loss: 0.687101]\n",
      "epoch:19 step:18455 [D loss: 0.609766, acc.: 66.41%] [G loss: 0.635167]\n",
      "epoch:19 step:18456 [D loss: 0.611915, acc.: 68.75%] [G loss: 0.564644]\n",
      "epoch:19 step:18457 [D loss: 0.517937, acc.: 71.88%] [G loss: 0.692590]\n",
      "epoch:19 step:18458 [D loss: 0.544896, acc.: 72.66%] [G loss: 0.630133]\n",
      "epoch:19 step:18459 [D loss: 0.562928, acc.: 68.75%] [G loss: 0.746927]\n",
      "epoch:19 step:18460 [D loss: 0.496440, acc.: 75.00%] [G loss: 0.653256]\n",
      "epoch:19 step:18461 [D loss: 0.547916, acc.: 70.31%] [G loss: 0.593056]\n",
      "epoch:19 step:18462 [D loss: 0.522673, acc.: 71.88%] [G loss: 0.583597]\n",
      "epoch:19 step:18463 [D loss: 0.534597, acc.: 73.44%] [G loss: 0.681442]\n",
      "epoch:19 step:18464 [D loss: 0.506859, acc.: 72.66%] [G loss: 0.542465]\n",
      "epoch:19 step:18465 [D loss: 0.499350, acc.: 72.66%] [G loss: 0.720755]\n",
      "epoch:19 step:18466 [D loss: 0.554070, acc.: 67.19%] [G loss: 0.591686]\n",
      "epoch:19 step:18467 [D loss: 0.542930, acc.: 71.88%] [G loss: 0.505983]\n",
      "epoch:19 step:18468 [D loss: 0.532757, acc.: 70.31%] [G loss: 0.632914]\n",
      "epoch:19 step:18469 [D loss: 0.495718, acc.: 73.44%] [G loss: 0.633977]\n",
      "epoch:19 step:18470 [D loss: 0.619326, acc.: 66.41%] [G loss: 0.585425]\n",
      "epoch:19 step:18471 [D loss: 0.528951, acc.: 71.88%] [G loss: 0.744915]\n",
      "epoch:19 step:18472 [D loss: 0.516087, acc.: 74.22%] [G loss: 0.550441]\n",
      "epoch:19 step:18473 [D loss: 0.561088, acc.: 66.41%] [G loss: 0.639217]\n",
      "epoch:19 step:18474 [D loss: 0.559685, acc.: 71.09%] [G loss: 0.648553]\n",
      "epoch:19 step:18475 [D loss: 0.577883, acc.: 71.09%] [G loss: 0.672183]\n",
      "epoch:19 step:18476 [D loss: 0.594148, acc.: 67.97%] [G loss: 0.546370]\n",
      "epoch:19 step:18477 [D loss: 0.601815, acc.: 63.28%] [G loss: 0.715918]\n",
      "epoch:19 step:18478 [D loss: 0.603086, acc.: 64.84%] [G loss: 0.691582]\n",
      "epoch:19 step:18479 [D loss: 0.521139, acc.: 64.06%] [G loss: 0.709961]\n",
      "epoch:19 step:18480 [D loss: 0.504837, acc.: 72.66%] [G loss: 0.693790]\n",
      "epoch:19 step:18481 [D loss: 0.566816, acc.: 68.75%] [G loss: 0.566478]\n",
      "epoch:19 step:18482 [D loss: 0.531343, acc.: 71.88%] [G loss: 0.627630]\n",
      "epoch:19 step:18483 [D loss: 0.544769, acc.: 71.09%] [G loss: 0.605292]\n",
      "epoch:19 step:18484 [D loss: 0.499808, acc.: 73.44%] [G loss: 0.804945]\n",
      "epoch:19 step:18485 [D loss: 0.588212, acc.: 71.09%] [G loss: 0.546397]\n",
      "epoch:19 step:18486 [D loss: 0.574432, acc.: 70.31%] [G loss: 0.659673]\n",
      "epoch:19 step:18487 [D loss: 0.566381, acc.: 68.75%] [G loss: 0.676831]\n",
      "epoch:19 step:18488 [D loss: 0.529913, acc.: 71.09%] [G loss: 0.769352]\n",
      "epoch:19 step:18489 [D loss: 0.554963, acc.: 65.62%] [G loss: 0.483879]\n",
      "epoch:19 step:18490 [D loss: 0.580109, acc.: 65.62%] [G loss: 0.563936]\n",
      "epoch:19 step:18491 [D loss: 0.525948, acc.: 75.00%] [G loss: 0.567566]\n",
      "epoch:19 step:18492 [D loss: 0.531065, acc.: 74.22%] [G loss: 0.679597]\n",
      "epoch:19 step:18493 [D loss: 0.528656, acc.: 69.53%] [G loss: 0.463842]\n",
      "epoch:19 step:18494 [D loss: 0.480023, acc.: 78.12%] [G loss: 0.784134]\n",
      "epoch:19 step:18495 [D loss: 0.532934, acc.: 75.00%] [G loss: 0.632206]\n",
      "epoch:19 step:18496 [D loss: 0.464816, acc.: 78.12%] [G loss: 0.725938]\n",
      "epoch:19 step:18497 [D loss: 0.535436, acc.: 68.75%] [G loss: 0.692624]\n",
      "epoch:19 step:18498 [D loss: 0.528832, acc.: 71.88%] [G loss: 0.787884]\n",
      "epoch:19 step:18499 [D loss: 0.633018, acc.: 59.38%] [G loss: 0.537236]\n",
      "epoch:19 step:18500 [D loss: 0.562173, acc.: 61.72%] [G loss: 0.567639]\n",
      "epoch:19 step:18501 [D loss: 0.577736, acc.: 64.84%] [G loss: 0.569158]\n",
      "epoch:19 step:18502 [D loss: 0.526005, acc.: 71.09%] [G loss: 0.625472]\n",
      "epoch:19 step:18503 [D loss: 0.553035, acc.: 67.19%] [G loss: 0.578301]\n",
      "epoch:19 step:18504 [D loss: 0.519557, acc.: 76.56%] [G loss: 0.895683]\n",
      "epoch:19 step:18505 [D loss: 0.572521, acc.: 64.84%] [G loss: 0.733584]\n",
      "epoch:19 step:18506 [D loss: 0.579143, acc.: 68.75%] [G loss: 0.546614]\n",
      "epoch:19 step:18507 [D loss: 0.607683, acc.: 67.19%] [G loss: 0.479084]\n",
      "epoch:19 step:18508 [D loss: 0.546594, acc.: 72.66%] [G loss: 0.529353]\n",
      "epoch:19 step:18509 [D loss: 0.550389, acc.: 70.31%] [G loss: 0.549849]\n",
      "epoch:19 step:18510 [D loss: 0.522689, acc.: 74.22%] [G loss: 0.803571]\n",
      "epoch:19 step:18511 [D loss: 0.467938, acc.: 79.69%] [G loss: 0.754032]\n",
      "epoch:19 step:18512 [D loss: 0.548287, acc.: 67.97%] [G loss: 0.687467]\n",
      "epoch:19 step:18513 [D loss: 0.601749, acc.: 66.41%] [G loss: 0.633134]\n",
      "epoch:19 step:18514 [D loss: 0.565526, acc.: 64.84%] [G loss: 0.487349]\n",
      "epoch:19 step:18515 [D loss: 0.546919, acc.: 70.31%] [G loss: 0.527057]\n",
      "epoch:19 step:18516 [D loss: 0.603570, acc.: 60.16%] [G loss: 0.567246]\n",
      "epoch:19 step:18517 [D loss: 0.511547, acc.: 71.88%] [G loss: 0.679884]\n",
      "epoch:19 step:18518 [D loss: 0.536279, acc.: 70.31%] [G loss: 0.620316]\n",
      "epoch:19 step:18519 [D loss: 0.631190, acc.: 60.94%] [G loss: 0.539078]\n",
      "epoch:19 step:18520 [D loss: 0.581107, acc.: 71.09%] [G loss: 0.515544]\n",
      "epoch:19 step:18521 [D loss: 0.584726, acc.: 64.06%] [G loss: 0.656978]\n",
      "epoch:19 step:18522 [D loss: 0.488803, acc.: 78.91%] [G loss: 0.565515]\n",
      "epoch:19 step:18523 [D loss: 0.602596, acc.: 65.62%] [G loss: 0.669719]\n",
      "epoch:19 step:18524 [D loss: 0.569568, acc.: 67.97%] [G loss: 0.581796]\n",
      "epoch:19 step:18525 [D loss: 0.553553, acc.: 69.53%] [G loss: 0.616929]\n",
      "epoch:19 step:18526 [D loss: 0.547785, acc.: 67.97%] [G loss: 0.535791]\n",
      "epoch:19 step:18527 [D loss: 0.463016, acc.: 75.78%] [G loss: 0.675439]\n",
      "epoch:19 step:18528 [D loss: 0.534699, acc.: 71.88%] [G loss: 0.658471]\n",
      "epoch:19 step:18529 [D loss: 0.502705, acc.: 75.78%] [G loss: 0.767204]\n",
      "epoch:19 step:18530 [D loss: 0.588829, acc.: 67.97%] [G loss: 0.740186]\n",
      "epoch:19 step:18531 [D loss: 0.554086, acc.: 69.53%] [G loss: 0.685529]\n",
      "epoch:19 step:18532 [D loss: 0.600821, acc.: 63.28%] [G loss: 0.676129]\n",
      "epoch:19 step:18533 [D loss: 0.550572, acc.: 66.41%] [G loss: 0.550723]\n",
      "epoch:19 step:18534 [D loss: 0.549711, acc.: 71.88%] [G loss: 0.668199]\n",
      "epoch:19 step:18535 [D loss: 0.542222, acc.: 68.75%] [G loss: 0.660158]\n",
      "epoch:19 step:18536 [D loss: 0.511010, acc.: 75.78%] [G loss: 0.592011]\n",
      "epoch:19 step:18537 [D loss: 0.531320, acc.: 70.31%] [G loss: 0.599044]\n",
      "epoch:19 step:18538 [D loss: 0.585186, acc.: 67.97%] [G loss: 0.564834]\n",
      "epoch:19 step:18539 [D loss: 0.520621, acc.: 70.31%] [G loss: 0.537813]\n",
      "epoch:19 step:18540 [D loss: 0.520951, acc.: 71.09%] [G loss: 0.758615]\n",
      "epoch:19 step:18541 [D loss: 0.536001, acc.: 69.53%] [G loss: 0.559735]\n",
      "epoch:19 step:18542 [D loss: 0.593192, acc.: 66.41%] [G loss: 0.702171]\n",
      "epoch:19 step:18543 [D loss: 0.635042, acc.: 62.50%] [G loss: 0.470753]\n",
      "epoch:19 step:18544 [D loss: 0.524720, acc.: 72.66%] [G loss: 0.601860]\n",
      "epoch:19 step:18545 [D loss: 0.532352, acc.: 71.09%] [G loss: 0.558890]\n",
      "epoch:19 step:18546 [D loss: 0.528259, acc.: 71.09%] [G loss: 0.673740]\n",
      "epoch:19 step:18547 [D loss: 0.564430, acc.: 72.66%] [G loss: 0.604777]\n",
      "epoch:19 step:18548 [D loss: 0.588239, acc.: 67.19%] [G loss: 0.526818]\n",
      "epoch:19 step:18549 [D loss: 0.442344, acc.: 80.47%] [G loss: 0.551932]\n",
      "epoch:19 step:18550 [D loss: 0.458349, acc.: 75.78%] [G loss: 0.763571]\n",
      "epoch:19 step:18551 [D loss: 0.522364, acc.: 72.66%] [G loss: 0.736725]\n",
      "epoch:19 step:18552 [D loss: 0.529244, acc.: 73.44%] [G loss: 0.749236]\n",
      "epoch:19 step:18553 [D loss: 0.518400, acc.: 75.78%] [G loss: 0.681669]\n",
      "epoch:19 step:18554 [D loss: 0.519444, acc.: 75.00%] [G loss: 0.736701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18555 [D loss: 0.612097, acc.: 64.06%] [G loss: 0.685787]\n",
      "epoch:19 step:18556 [D loss: 0.483685, acc.: 77.34%] [G loss: 0.542140]\n",
      "epoch:19 step:18557 [D loss: 0.540183, acc.: 65.62%] [G loss: 0.795515]\n",
      "epoch:19 step:18558 [D loss: 0.593088, acc.: 65.62%] [G loss: 0.526861]\n",
      "epoch:19 step:18559 [D loss: 0.565120, acc.: 64.84%] [G loss: 0.572526]\n",
      "epoch:19 step:18560 [D loss: 0.546589, acc.: 68.75%] [G loss: 0.699337]\n",
      "epoch:19 step:18561 [D loss: 0.590770, acc.: 68.75%] [G loss: 0.444316]\n",
      "epoch:19 step:18562 [D loss: 0.551210, acc.: 70.31%] [G loss: 0.543710]\n",
      "epoch:19 step:18563 [D loss: 0.546549, acc.: 67.97%] [G loss: 0.574926]\n",
      "epoch:19 step:18564 [D loss: 0.603749, acc.: 62.50%] [G loss: 0.582282]\n",
      "epoch:19 step:18565 [D loss: 0.574068, acc.: 71.09%] [G loss: 0.664383]\n",
      "epoch:19 step:18566 [D loss: 0.551547, acc.: 66.41%] [G loss: 0.652261]\n",
      "epoch:19 step:18567 [D loss: 0.592281, acc.: 67.19%] [G loss: 0.711623]\n",
      "epoch:19 step:18568 [D loss: 0.633150, acc.: 65.62%] [G loss: 0.540380]\n",
      "epoch:19 step:18569 [D loss: 0.650897, acc.: 61.72%] [G loss: 0.470313]\n",
      "epoch:19 step:18570 [D loss: 0.539990, acc.: 74.22%] [G loss: 0.630224]\n",
      "epoch:19 step:18571 [D loss: 0.565996, acc.: 70.31%] [G loss: 0.781099]\n",
      "epoch:19 step:18572 [D loss: 0.520142, acc.: 72.66%] [G loss: 0.695541]\n",
      "epoch:19 step:18573 [D loss: 0.490597, acc.: 75.78%] [G loss: 0.882197]\n",
      "epoch:19 step:18574 [D loss: 0.658316, acc.: 70.31%] [G loss: 0.873264]\n",
      "epoch:19 step:18575 [D loss: 0.550511, acc.: 74.22%] [G loss: 0.873473]\n",
      "epoch:19 step:18576 [D loss: 0.537740, acc.: 71.88%] [G loss: 0.645060]\n",
      "epoch:19 step:18577 [D loss: 0.550140, acc.: 68.75%] [G loss: 0.692631]\n",
      "epoch:19 step:18578 [D loss: 0.497604, acc.: 74.22%] [G loss: 0.640803]\n",
      "epoch:19 step:18579 [D loss: 0.599126, acc.: 67.97%] [G loss: 0.672736]\n",
      "epoch:19 step:18580 [D loss: 0.584173, acc.: 64.06%] [G loss: 0.633518]\n",
      "epoch:19 step:18581 [D loss: 0.554121, acc.: 70.31%] [G loss: 0.525388]\n",
      "epoch:19 step:18582 [D loss: 0.532821, acc.: 71.09%] [G loss: 0.583211]\n",
      "epoch:19 step:18583 [D loss: 0.555664, acc.: 69.53%] [G loss: 0.601536]\n",
      "epoch:19 step:18584 [D loss: 0.549196, acc.: 72.66%] [G loss: 0.623771]\n",
      "epoch:19 step:18585 [D loss: 0.498473, acc.: 71.09%] [G loss: 0.770188]\n",
      "epoch:19 step:18586 [D loss: 0.525631, acc.: 74.22%] [G loss: 0.741565]\n",
      "epoch:19 step:18587 [D loss: 0.623418, acc.: 63.28%] [G loss: 0.590313]\n",
      "epoch:19 step:18588 [D loss: 0.537439, acc.: 67.97%] [G loss: 0.498805]\n",
      "epoch:19 step:18589 [D loss: 0.513127, acc.: 72.66%] [G loss: 0.720698]\n",
      "epoch:19 step:18590 [D loss: 0.569245, acc.: 71.09%] [G loss: 0.593123]\n",
      "epoch:19 step:18591 [D loss: 0.601537, acc.: 67.97%] [G loss: 0.405006]\n",
      "epoch:19 step:18592 [D loss: 0.558077, acc.: 72.66%] [G loss: 0.572139]\n",
      "epoch:19 step:18593 [D loss: 0.545196, acc.: 67.19%] [G loss: 0.573561]\n",
      "epoch:19 step:18594 [D loss: 0.554808, acc.: 71.88%] [G loss: 0.696712]\n",
      "epoch:19 step:18595 [D loss: 0.524490, acc.: 71.09%] [G loss: 0.810677]\n",
      "epoch:19 step:18596 [D loss: 0.592495, acc.: 64.84%] [G loss: 0.654896]\n",
      "epoch:19 step:18597 [D loss: 0.671834, acc.: 53.91%] [G loss: 0.525528]\n",
      "epoch:19 step:18598 [D loss: 0.526725, acc.: 71.09%] [G loss: 0.711162]\n",
      "epoch:19 step:18599 [D loss: 0.550173, acc.: 69.53%] [G loss: 0.793713]\n",
      "epoch:19 step:18600 [D loss: 0.605384, acc.: 62.50%] [G loss: 0.621433]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.631207\n",
      "FID: 47.070648\n",
      "0 = 12.70959304943079\n",
      "1 = 0.08400211590418062\n",
      "2 = 0.8608999848365784\n",
      "3 = 0.8022000193595886\n",
      "4 = 0.9196000099182129\n",
      "5 = 0.9089055061340332\n",
      "6 = 0.8022000193595886\n",
      "7 = 8.078962315869347\n",
      "8 = 0.14710845250080015\n",
      "9 = 0.7128000259399414\n",
      "10 = 0.6980000138282776\n",
      "11 = 0.7275999784469604\n",
      "12 = 0.7192910313606262\n",
      "13 = 0.6980000138282776\n",
      "14 = 6.631234645843506\n",
      "15 = 7.304384231567383\n",
      "16 = 0.36862713098526\n",
      "17 = 6.631207466125488\n",
      "18 = 47.070648193359375\n",
      "epoch:19 step:18601 [D loss: 0.618458, acc.: 64.84%] [G loss: 0.716283]\n",
      "epoch:19 step:18602 [D loss: 0.635938, acc.: 61.72%] [G loss: 0.633238]\n",
      "epoch:19 step:18603 [D loss: 0.573918, acc.: 69.53%] [G loss: 0.616417]\n",
      "epoch:19 step:18604 [D loss: 0.481467, acc.: 77.34%] [G loss: 0.713476]\n",
      "epoch:19 step:18605 [D loss: 0.456411, acc.: 78.91%] [G loss: 0.807635]\n",
      "epoch:19 step:18606 [D loss: 0.494279, acc.: 72.66%] [G loss: 0.910794]\n",
      "epoch:19 step:18607 [D loss: 0.609801, acc.: 64.84%] [G loss: 0.634245]\n",
      "epoch:19 step:18608 [D loss: 0.595728, acc.: 67.19%] [G loss: 0.550623]\n",
      "epoch:19 step:18609 [D loss: 0.612167, acc.: 59.38%] [G loss: 0.492961]\n",
      "epoch:19 step:18610 [D loss: 0.563872, acc.: 70.31%] [G loss: 0.540030]\n",
      "epoch:19 step:18611 [D loss: 0.556793, acc.: 67.19%] [G loss: 0.589736]\n",
      "epoch:19 step:18612 [D loss: 0.536864, acc.: 71.88%] [G loss: 0.576654]\n",
      "epoch:19 step:18613 [D loss: 0.530258, acc.: 68.75%] [G loss: 0.474189]\n",
      "epoch:19 step:18614 [D loss: 0.592605, acc.: 64.06%] [G loss: 0.544660]\n",
      "epoch:19 step:18615 [D loss: 0.629468, acc.: 64.84%] [G loss: 0.538359]\n",
      "epoch:19 step:18616 [D loss: 0.523219, acc.: 74.22%] [G loss: 0.758074]\n",
      "epoch:19 step:18617 [D loss: 0.471321, acc.: 78.12%] [G loss: 0.569779]\n",
      "epoch:19 step:18618 [D loss: 0.468040, acc.: 79.69%] [G loss: 0.744473]\n",
      "epoch:19 step:18619 [D loss: 0.561976, acc.: 70.31%] [G loss: 0.623985]\n",
      "epoch:19 step:18620 [D loss: 0.611691, acc.: 65.62%] [G loss: 0.654640]\n",
      "epoch:19 step:18621 [D loss: 0.520267, acc.: 72.66%] [G loss: 0.589011]\n",
      "epoch:19 step:18622 [D loss: 0.529857, acc.: 72.66%] [G loss: 0.657766]\n",
      "epoch:19 step:18623 [D loss: 0.658019, acc.: 59.38%] [G loss: 0.481738]\n",
      "epoch:19 step:18624 [D loss: 0.538028, acc.: 71.88%] [G loss: 0.541431]\n",
      "epoch:19 step:18625 [D loss: 0.557919, acc.: 67.19%] [G loss: 0.568273]\n",
      "epoch:19 step:18626 [D loss: 0.432506, acc.: 79.69%] [G loss: 0.723990]\n",
      "epoch:19 step:18627 [D loss: 0.570369, acc.: 67.19%] [G loss: 0.603000]\n",
      "epoch:19 step:18628 [D loss: 0.525274, acc.: 71.09%] [G loss: 0.564511]\n",
      "epoch:19 step:18629 [D loss: 0.519418, acc.: 74.22%] [G loss: 0.626046]\n",
      "epoch:19 step:18630 [D loss: 0.621850, acc.: 63.28%] [G loss: 0.471305]\n",
      "epoch:19 step:18631 [D loss: 0.669373, acc.: 56.25%] [G loss: 0.523357]\n",
      "epoch:19 step:18632 [D loss: 0.574967, acc.: 67.97%] [G loss: 0.614348]\n",
      "epoch:19 step:18633 [D loss: 0.559750, acc.: 66.41%] [G loss: 0.677644]\n",
      "epoch:19 step:18634 [D loss: 0.532040, acc.: 71.88%] [G loss: 0.643845]\n",
      "epoch:19 step:18635 [D loss: 0.578644, acc.: 66.41%] [G loss: 0.573435]\n",
      "epoch:19 step:18636 [D loss: 0.510095, acc.: 74.22%] [G loss: 0.657671]\n",
      "epoch:19 step:18637 [D loss: 0.510196, acc.: 71.88%] [G loss: 0.614396]\n",
      "epoch:19 step:18638 [D loss: 0.543538, acc.: 65.62%] [G loss: 0.521292]\n",
      "epoch:19 step:18639 [D loss: 0.580759, acc.: 67.97%] [G loss: 0.578707]\n",
      "epoch:19 step:18640 [D loss: 0.516157, acc.: 74.22%] [G loss: 0.586073]\n",
      "epoch:19 step:18641 [D loss: 0.550064, acc.: 69.53%] [G loss: 0.595537]\n",
      "epoch:19 step:18642 [D loss: 0.592757, acc.: 64.84%] [G loss: 0.505960]\n",
      "epoch:19 step:18643 [D loss: 0.573001, acc.: 64.84%] [G loss: 0.511638]\n",
      "epoch:19 step:18644 [D loss: 0.557709, acc.: 64.84%] [G loss: 0.553575]\n",
      "epoch:19 step:18645 [D loss: 0.517119, acc.: 74.22%] [G loss: 0.755606]\n",
      "epoch:19 step:18646 [D loss: 0.510842, acc.: 73.44%] [G loss: 0.661539]\n",
      "epoch:19 step:18647 [D loss: 0.533800, acc.: 68.75%] [G loss: 0.651460]\n",
      "epoch:19 step:18648 [D loss: 0.553335, acc.: 66.41%] [G loss: 0.595177]\n",
      "epoch:19 step:18649 [D loss: 0.572948, acc.: 66.41%] [G loss: 0.495858]\n",
      "epoch:19 step:18650 [D loss: 0.599521, acc.: 62.50%] [G loss: 0.478941]\n",
      "epoch:19 step:18651 [D loss: 0.556688, acc.: 67.97%] [G loss: 0.459989]\n",
      "epoch:19 step:18652 [D loss: 0.582601, acc.: 66.41%] [G loss: 0.517661]\n",
      "epoch:19 step:18653 [D loss: 0.560535, acc.: 66.41%] [G loss: 0.435804]\n",
      "epoch:19 step:18654 [D loss: 0.580865, acc.: 70.31%] [G loss: 0.626737]\n",
      "epoch:19 step:18655 [D loss: 0.511966, acc.: 75.00%] [G loss: 0.634083]\n",
      "epoch:19 step:18656 [D loss: 0.530857, acc.: 70.31%] [G loss: 0.750875]\n",
      "epoch:19 step:18657 [D loss: 0.551592, acc.: 65.62%] [G loss: 0.695349]\n",
      "epoch:19 step:18658 [D loss: 0.541861, acc.: 72.66%] [G loss: 0.710175]\n",
      "epoch:19 step:18659 [D loss: 0.660279, acc.: 61.72%] [G loss: 0.565938]\n",
      "epoch:19 step:18660 [D loss: 0.449416, acc.: 79.69%] [G loss: 0.610444]\n",
      "epoch:19 step:18661 [D loss: 0.626103, acc.: 65.62%] [G loss: 0.652614]\n",
      "epoch:19 step:18662 [D loss: 0.529759, acc.: 72.66%] [G loss: 0.621578]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18663 [D loss: 0.472028, acc.: 74.22%] [G loss: 0.716174]\n",
      "epoch:19 step:18664 [D loss: 0.616988, acc.: 68.75%] [G loss: 0.662297]\n",
      "epoch:19 step:18665 [D loss: 0.588644, acc.: 65.62%] [G loss: 0.570577]\n",
      "epoch:19 step:18666 [D loss: 0.665071, acc.: 60.94%] [G loss: 0.612499]\n",
      "epoch:19 step:18667 [D loss: 0.533416, acc.: 72.66%] [G loss: 0.480463]\n",
      "epoch:19 step:18668 [D loss: 0.584086, acc.: 61.72%] [G loss: 0.488260]\n",
      "epoch:19 step:18669 [D loss: 0.542261, acc.: 67.97%] [G loss: 0.506691]\n",
      "epoch:19 step:18670 [D loss: 0.604821, acc.: 65.62%] [G loss: 0.444252]\n",
      "epoch:19 step:18671 [D loss: 0.542967, acc.: 71.88%] [G loss: 0.455716]\n",
      "epoch:19 step:18672 [D loss: 0.571012, acc.: 67.19%] [G loss: 0.540100]\n",
      "epoch:19 step:18673 [D loss: 0.493223, acc.: 75.00%] [G loss: 0.664718]\n",
      "epoch:19 step:18674 [D loss: 0.489549, acc.: 74.22%] [G loss: 0.685140]\n",
      "epoch:19 step:18675 [D loss: 0.492708, acc.: 76.56%] [G loss: 0.622049]\n",
      "epoch:19 step:18676 [D loss: 0.537965, acc.: 71.09%] [G loss: 0.672779]\n",
      "epoch:19 step:18677 [D loss: 0.562341, acc.: 68.75%] [G loss: 0.683532]\n",
      "epoch:19 step:18678 [D loss: 0.516565, acc.: 76.56%] [G loss: 0.780114]\n",
      "epoch:19 step:18679 [D loss: 0.567003, acc.: 67.19%] [G loss: 0.698920]\n",
      "epoch:19 step:18680 [D loss: 0.592640, acc.: 68.75%] [G loss: 0.416638]\n",
      "epoch:19 step:18681 [D loss: 0.599701, acc.: 66.41%] [G loss: 0.499562]\n",
      "epoch:19 step:18682 [D loss: 0.553081, acc.: 65.62%] [G loss: 0.492604]\n",
      "epoch:19 step:18683 [D loss: 0.643075, acc.: 61.72%] [G loss: 0.470019]\n",
      "epoch:19 step:18684 [D loss: 0.574619, acc.: 67.97%] [G loss: 0.482144]\n",
      "epoch:19 step:18685 [D loss: 0.555958, acc.: 67.19%] [G loss: 0.467192]\n",
      "epoch:19 step:18686 [D loss: 0.604561, acc.: 64.84%] [G loss: 0.497898]\n",
      "epoch:19 step:18687 [D loss: 0.526773, acc.: 75.00%] [G loss: 0.622910]\n",
      "epoch:19 step:18688 [D loss: 0.489689, acc.: 76.56%] [G loss: 0.586726]\n",
      "epoch:19 step:18689 [D loss: 0.537234, acc.: 73.44%] [G loss: 0.805503]\n",
      "epoch:19 step:18690 [D loss: 0.548480, acc.: 71.09%] [G loss: 0.743852]\n",
      "epoch:19 step:18691 [D loss: 0.555088, acc.: 71.88%] [G loss: 0.697444]\n",
      "epoch:19 step:18692 [D loss: 0.535954, acc.: 70.31%] [G loss: 0.620359]\n",
      "epoch:19 step:18693 [D loss: 0.508983, acc.: 70.31%] [G loss: 0.623663]\n",
      "epoch:19 step:18694 [D loss: 0.561791, acc.: 67.19%] [G loss: 0.704590]\n",
      "epoch:19 step:18695 [D loss: 0.634442, acc.: 60.16%] [G loss: 0.522333]\n",
      "epoch:19 step:18696 [D loss: 0.572965, acc.: 70.31%] [G loss: 0.600332]\n",
      "epoch:19 step:18697 [D loss: 0.484673, acc.: 74.22%] [G loss: 0.604946]\n",
      "epoch:19 step:18698 [D loss: 0.540745, acc.: 72.66%] [G loss: 0.710755]\n",
      "epoch:19 step:18699 [D loss: 0.497261, acc.: 77.34%] [G loss: 0.803802]\n",
      "epoch:19 step:18700 [D loss: 0.517740, acc.: 74.22%] [G loss: 0.791816]\n",
      "epoch:19 step:18701 [D loss: 0.473522, acc.: 74.22%] [G loss: 0.783854]\n",
      "epoch:19 step:18702 [D loss: 0.445100, acc.: 81.25%] [G loss: 0.756835]\n",
      "epoch:19 step:18703 [D loss: 0.515152, acc.: 76.56%] [G loss: 0.788864]\n",
      "epoch:19 step:18704 [D loss: 0.575854, acc.: 70.31%] [G loss: 0.727547]\n",
      "epoch:19 step:18705 [D loss: 0.559435, acc.: 69.53%] [G loss: 0.614060]\n",
      "epoch:19 step:18706 [D loss: 0.533339, acc.: 75.78%] [G loss: 0.591238]\n",
      "epoch:19 step:18707 [D loss: 0.575884, acc.: 71.09%] [G loss: 0.612434]\n",
      "epoch:19 step:18708 [D loss: 0.594509, acc.: 65.62%] [G loss: 0.636567]\n",
      "epoch:19 step:18709 [D loss: 0.505765, acc.: 78.12%] [G loss: 0.645169]\n",
      "epoch:19 step:18710 [D loss: 0.531850, acc.: 73.44%] [G loss: 0.616754]\n",
      "epoch:19 step:18711 [D loss: 0.587080, acc.: 69.53%] [G loss: 0.612407]\n",
      "epoch:19 step:18712 [D loss: 0.534257, acc.: 75.00%] [G loss: 0.618200]\n",
      "epoch:19 step:18713 [D loss: 0.544104, acc.: 75.78%] [G loss: 0.708572]\n",
      "epoch:19 step:18714 [D loss: 0.569083, acc.: 68.75%] [G loss: 0.682932]\n",
      "epoch:19 step:18715 [D loss: 0.437286, acc.: 82.81%] [G loss: 0.760237]\n",
      "epoch:19 step:18716 [D loss: 0.529893, acc.: 77.34%] [G loss: 0.908934]\n",
      "epoch:19 step:18717 [D loss: 0.469824, acc.: 77.34%] [G loss: 0.791560]\n",
      "epoch:19 step:18718 [D loss: 0.632098, acc.: 63.28%] [G loss: 0.586385]\n",
      "epoch:19 step:18719 [D loss: 0.548154, acc.: 71.09%] [G loss: 0.740244]\n",
      "epoch:19 step:18720 [D loss: 0.549384, acc.: 70.31%] [G loss: 0.711857]\n",
      "epoch:19 step:18721 [D loss: 0.492029, acc.: 74.22%] [G loss: 0.573228]\n",
      "epoch:19 step:18722 [D loss: 0.435823, acc.: 82.81%] [G loss: 0.886492]\n",
      "epoch:19 step:18723 [D loss: 0.708585, acc.: 59.38%] [G loss: 0.615673]\n",
      "epoch:19 step:18724 [D loss: 0.565037, acc.: 68.75%] [G loss: 0.722398]\n",
      "epoch:19 step:18725 [D loss: 0.556543, acc.: 68.75%] [G loss: 0.557935]\n",
      "epoch:19 step:18726 [D loss: 0.447974, acc.: 74.22%] [G loss: 0.857144]\n",
      "epoch:19 step:18727 [D loss: 0.421035, acc.: 81.25%] [G loss: 0.810951]\n",
      "epoch:19 step:18728 [D loss: 0.437000, acc.: 76.56%] [G loss: 0.892166]\n",
      "epoch:19 step:18729 [D loss: 0.475212, acc.: 77.34%] [G loss: 0.989374]\n",
      "epoch:19 step:18730 [D loss: 0.476572, acc.: 78.91%] [G loss: 1.104480]\n",
      "epoch:19 step:18731 [D loss: 0.665674, acc.: 69.53%] [G loss: 1.028808]\n",
      "epoch:19 step:18732 [D loss: 0.582296, acc.: 71.09%] [G loss: 0.920148]\n",
      "epoch:19 step:18733 [D loss: 0.458420, acc.: 81.25%] [G loss: 1.035957]\n",
      "epoch:19 step:18734 [D loss: 0.560359, acc.: 67.19%] [G loss: 0.950608]\n",
      "epoch:19 step:18735 [D loss: 0.598736, acc.: 65.62%] [G loss: 0.793373]\n",
      "epoch:19 step:18736 [D loss: 0.504124, acc.: 74.22%] [G loss: 0.972921]\n",
      "epoch:19 step:18737 [D loss: 0.501647, acc.: 71.88%] [G loss: 0.854276]\n",
      "epoch:19 step:18738 [D loss: 0.457753, acc.: 75.78%] [G loss: 1.045480]\n",
      "epoch:19 step:18739 [D loss: 0.413518, acc.: 84.38%] [G loss: 1.114972]\n",
      "epoch:19 step:18740 [D loss: 0.385444, acc.: 85.94%] [G loss: 1.473291]\n",
      "epoch:20 step:18741 [D loss: 0.587799, acc.: 70.31%] [G loss: 1.113348]\n",
      "epoch:20 step:18742 [D loss: 0.461363, acc.: 78.91%] [G loss: 1.032698]\n",
      "epoch:20 step:18743 [D loss: 0.568337, acc.: 69.53%] [G loss: 0.767888]\n",
      "epoch:20 step:18744 [D loss: 0.503277, acc.: 71.88%] [G loss: 0.783512]\n",
      "epoch:20 step:18745 [D loss: 0.515028, acc.: 74.22%] [G loss: 0.724131]\n",
      "epoch:20 step:18746 [D loss: 0.590343, acc.: 68.75%] [G loss: 0.740601]\n",
      "epoch:20 step:18747 [D loss: 0.530147, acc.: 70.31%] [G loss: 0.865006]\n",
      "epoch:20 step:18748 [D loss: 0.499099, acc.: 75.00%] [G loss: 0.627361]\n",
      "epoch:20 step:18749 [D loss: 0.470423, acc.: 75.78%] [G loss: 0.896408]\n",
      "epoch:20 step:18750 [D loss: 0.475030, acc.: 77.34%] [G loss: 0.909535]\n",
      "epoch:20 step:18751 [D loss: 0.463206, acc.: 77.34%] [G loss: 0.955527]\n",
      "epoch:20 step:18752 [D loss: 0.576434, acc.: 70.31%] [G loss: 0.657916]\n",
      "epoch:20 step:18753 [D loss: 0.573146, acc.: 66.41%] [G loss: 0.623674]\n",
      "epoch:20 step:18754 [D loss: 0.523770, acc.: 71.88%] [G loss: 0.871225]\n",
      "epoch:20 step:18755 [D loss: 0.461240, acc.: 79.69%] [G loss: 0.771767]\n",
      "epoch:20 step:18756 [D loss: 0.523558, acc.: 71.09%] [G loss: 0.785941]\n",
      "epoch:20 step:18757 [D loss: 0.557562, acc.: 74.22%] [G loss: 0.649075]\n",
      "epoch:20 step:18758 [D loss: 0.560852, acc.: 67.97%] [G loss: 0.685737]\n",
      "epoch:20 step:18759 [D loss: 0.527889, acc.: 75.78%] [G loss: 0.676624]\n",
      "epoch:20 step:18760 [D loss: 0.630590, acc.: 63.28%] [G loss: 0.578328]\n",
      "epoch:20 step:18761 [D loss: 0.549930, acc.: 68.75%] [G loss: 0.601056]\n",
      "epoch:20 step:18762 [D loss: 0.475675, acc.: 76.56%] [G loss: 0.860382]\n",
      "epoch:20 step:18763 [D loss: 0.567269, acc.: 66.41%] [G loss: 0.597238]\n",
      "epoch:20 step:18764 [D loss: 0.571143, acc.: 68.75%] [G loss: 0.519876]\n",
      "epoch:20 step:18765 [D loss: 0.511216, acc.: 72.66%] [G loss: 0.702551]\n",
      "epoch:20 step:18766 [D loss: 0.569435, acc.: 67.19%] [G loss: 0.587642]\n",
      "epoch:20 step:18767 [D loss: 0.495101, acc.: 71.09%] [G loss: 0.690078]\n",
      "epoch:20 step:18768 [D loss: 0.547011, acc.: 71.88%] [G loss: 0.524899]\n",
      "epoch:20 step:18769 [D loss: 0.509625, acc.: 71.88%] [G loss: 0.691457]\n",
      "epoch:20 step:18770 [D loss: 0.537225, acc.: 73.44%] [G loss: 0.571923]\n",
      "epoch:20 step:18771 [D loss: 0.603385, acc.: 61.72%] [G loss: 0.583340]\n",
      "epoch:20 step:18772 [D loss: 0.591082, acc.: 66.41%] [G loss: 0.741269]\n",
      "epoch:20 step:18773 [D loss: 0.563593, acc.: 65.62%] [G loss: 0.716044]\n",
      "epoch:20 step:18774 [D loss: 0.524735, acc.: 68.75%] [G loss: 0.738479]\n",
      "epoch:20 step:18775 [D loss: 0.541103, acc.: 72.66%] [G loss: 0.506902]\n",
      "epoch:20 step:18776 [D loss: 0.500722, acc.: 77.34%] [G loss: 0.637482]\n",
      "epoch:20 step:18777 [D loss: 0.510098, acc.: 75.00%] [G loss: 0.634635]\n",
      "epoch:20 step:18778 [D loss: 0.543078, acc.: 71.09%] [G loss: 0.569508]\n",
      "epoch:20 step:18779 [D loss: 0.541682, acc.: 71.09%] [G loss: 0.707298]\n",
      "epoch:20 step:18780 [D loss: 0.513158, acc.: 75.78%] [G loss: 0.811909]\n",
      "epoch:20 step:18781 [D loss: 0.538597, acc.: 71.88%] [G loss: 0.723362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:18782 [D loss: 0.631225, acc.: 60.16%] [G loss: 0.545154]\n",
      "epoch:20 step:18783 [D loss: 0.537582, acc.: 79.69%] [G loss: 0.620190]\n",
      "epoch:20 step:18784 [D loss: 0.593598, acc.: 64.06%] [G loss: 0.571251]\n",
      "epoch:20 step:18785 [D loss: 0.519938, acc.: 72.66%] [G loss: 0.620635]\n",
      "epoch:20 step:18786 [D loss: 0.480903, acc.: 75.78%] [G loss: 0.848844]\n",
      "epoch:20 step:18787 [D loss: 0.532357, acc.: 69.53%] [G loss: 0.756373]\n",
      "epoch:20 step:18788 [D loss: 0.506725, acc.: 72.66%] [G loss: 0.800972]\n",
      "epoch:20 step:18789 [D loss: 0.525447, acc.: 75.00%] [G loss: 0.715033]\n",
      "epoch:20 step:18790 [D loss: 0.589326, acc.: 67.97%] [G loss: 0.660928]\n",
      "epoch:20 step:18791 [D loss: 0.619014, acc.: 62.50%] [G loss: 0.520802]\n",
      "epoch:20 step:18792 [D loss: 0.583768, acc.: 67.19%] [G loss: 0.628861]\n",
      "epoch:20 step:18793 [D loss: 0.508165, acc.: 75.78%] [G loss: 0.744298]\n",
      "epoch:20 step:18794 [D loss: 0.448804, acc.: 79.69%] [G loss: 0.865791]\n",
      "epoch:20 step:18795 [D loss: 0.592036, acc.: 67.97%] [G loss: 0.574550]\n",
      "epoch:20 step:18796 [D loss: 0.522826, acc.: 75.00%] [G loss: 0.713820]\n",
      "epoch:20 step:18797 [D loss: 0.538981, acc.: 75.00%] [G loss: 0.709736]\n",
      "epoch:20 step:18798 [D loss: 0.565923, acc.: 66.41%] [G loss: 0.769840]\n",
      "epoch:20 step:18799 [D loss: 0.522983, acc.: 76.56%] [G loss: 0.587139]\n",
      "epoch:20 step:18800 [D loss: 0.560287, acc.: 70.31%] [G loss: 0.694618]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.588662\n",
      "FID: 50.653347\n",
      "0 = 13.078825091457395\n",
      "1 = 0.09570437060732769\n",
      "2 = 0.8689000010490417\n",
      "3 = 0.823199987411499\n",
      "4 = 0.9146000146865845\n",
      "5 = 0.9060092568397522\n",
      "6 = 0.823199987411499\n",
      "7 = 8.507890900754948\n",
      "8 = 0.15711279123197186\n",
      "9 = 0.713699996471405\n",
      "10 = 0.7038000226020813\n",
      "11 = 0.7235999703407288\n",
      "12 = 0.7180167436599731\n",
      "13 = 0.7038000226020813\n",
      "14 = 6.588683605194092\n",
      "15 = 6.699606895446777\n",
      "16 = 0.40178975462913513\n",
      "17 = 6.5886616706848145\n",
      "18 = 50.65334701538086\n",
      "epoch:20 step:18801 [D loss: 0.558260, acc.: 64.06%] [G loss: 0.640775]\n",
      "epoch:20 step:18802 [D loss: 0.568080, acc.: 69.53%] [G loss: 0.616429]\n",
      "epoch:20 step:18803 [D loss: 0.545937, acc.: 75.00%] [G loss: 0.515349]\n",
      "epoch:20 step:18804 [D loss: 0.562818, acc.: 64.06%] [G loss: 0.502296]\n",
      "epoch:20 step:18805 [D loss: 0.562245, acc.: 71.09%] [G loss: 0.521894]\n",
      "epoch:20 step:18806 [D loss: 0.541191, acc.: 71.88%] [G loss: 0.553560]\n",
      "epoch:20 step:18807 [D loss: 0.508219, acc.: 75.00%] [G loss: 0.678576]\n",
      "epoch:20 step:18808 [D loss: 0.590789, acc.: 64.06%] [G loss: 0.600166]\n",
      "epoch:20 step:18809 [D loss: 0.490826, acc.: 72.66%] [G loss: 0.713123]\n",
      "epoch:20 step:18810 [D loss: 0.554798, acc.: 72.66%] [G loss: 0.800065]\n",
      "epoch:20 step:18811 [D loss: 0.529854, acc.: 73.44%] [G loss: 0.896126]\n",
      "epoch:20 step:18812 [D loss: 0.582892, acc.: 65.62%] [G loss: 0.594477]\n",
      "epoch:20 step:18813 [D loss: 0.574711, acc.: 65.62%] [G loss: 0.775573]\n",
      "epoch:20 step:18814 [D loss: 0.543603, acc.: 73.44%] [G loss: 0.713095]\n",
      "epoch:20 step:18815 [D loss: 0.558088, acc.: 71.09%] [G loss: 0.681119]\n",
      "epoch:20 step:18816 [D loss: 0.523612, acc.: 70.31%] [G loss: 0.840376]\n",
      "epoch:20 step:18817 [D loss: 0.426464, acc.: 82.03%] [G loss: 0.824821]\n",
      "epoch:20 step:18818 [D loss: 0.634155, acc.: 68.75%] [G loss: 0.559937]\n",
      "epoch:20 step:18819 [D loss: 0.602312, acc.: 64.84%] [G loss: 0.547068]\n",
      "epoch:20 step:18820 [D loss: 0.507236, acc.: 71.88%] [G loss: 0.608144]\n",
      "epoch:20 step:18821 [D loss: 0.561091, acc.: 71.09%] [G loss: 0.638526]\n",
      "epoch:20 step:18822 [D loss: 0.495617, acc.: 74.22%] [G loss: 0.541669]\n",
      "epoch:20 step:18823 [D loss: 0.467095, acc.: 75.00%] [G loss: 0.754870]\n",
      "epoch:20 step:18824 [D loss: 0.543871, acc.: 69.53%] [G loss: 0.662973]\n",
      "epoch:20 step:18825 [D loss: 0.580889, acc.: 67.97%] [G loss: 0.751121]\n",
      "epoch:20 step:18826 [D loss: 0.526756, acc.: 69.53%] [G loss: 0.548259]\n",
      "epoch:20 step:18827 [D loss: 0.514087, acc.: 71.88%] [G loss: 0.852458]\n",
      "epoch:20 step:18828 [D loss: 0.517898, acc.: 75.78%] [G loss: 0.723792]\n",
      "epoch:20 step:18829 [D loss: 0.509209, acc.: 69.53%] [G loss: 0.687679]\n",
      "epoch:20 step:18830 [D loss: 0.542967, acc.: 67.97%] [G loss: 0.718189]\n",
      "epoch:20 step:18831 [D loss: 0.558129, acc.: 73.44%] [G loss: 0.566896]\n",
      "epoch:20 step:18832 [D loss: 0.489132, acc.: 73.44%] [G loss: 0.767422]\n",
      "epoch:20 step:18833 [D loss: 0.539986, acc.: 75.00%] [G loss: 0.815218]\n",
      "epoch:20 step:18834 [D loss: 0.484611, acc.: 75.78%] [G loss: 0.880729]\n",
      "epoch:20 step:18835 [D loss: 0.570470, acc.: 67.97%] [G loss: 0.732658]\n",
      "epoch:20 step:18836 [D loss: 0.564998, acc.: 66.41%] [G loss: 0.606209]\n",
      "epoch:20 step:18837 [D loss: 0.535424, acc.: 71.88%] [G loss: 0.722096]\n",
      "epoch:20 step:18838 [D loss: 0.538576, acc.: 75.00%] [G loss: 0.682299]\n",
      "epoch:20 step:18839 [D loss: 0.569744, acc.: 67.19%] [G loss: 0.610350]\n",
      "epoch:20 step:18840 [D loss: 0.478240, acc.: 76.56%] [G loss: 0.776525]\n",
      "epoch:20 step:18841 [D loss: 0.476233, acc.: 77.34%] [G loss: 0.886189]\n",
      "epoch:20 step:18842 [D loss: 0.644453, acc.: 64.84%] [G loss: 0.487800]\n",
      "epoch:20 step:18843 [D loss: 0.607690, acc.: 64.84%] [G loss: 0.596473]\n",
      "epoch:20 step:18844 [D loss: 0.525877, acc.: 68.75%] [G loss: 0.598237]\n",
      "epoch:20 step:18845 [D loss: 0.618554, acc.: 62.50%] [G loss: 0.651475]\n",
      "epoch:20 step:18846 [D loss: 0.498106, acc.: 74.22%] [G loss: 0.702425]\n",
      "epoch:20 step:18847 [D loss: 0.615145, acc.: 62.50%] [G loss: 0.649523]\n",
      "epoch:20 step:18848 [D loss: 0.603306, acc.: 63.28%] [G loss: 0.613290]\n",
      "epoch:20 step:18849 [D loss: 0.593670, acc.: 67.19%] [G loss: 0.456304]\n",
      "epoch:20 step:18850 [D loss: 0.521908, acc.: 74.22%] [G loss: 0.557769]\n",
      "epoch:20 step:18851 [D loss: 0.525835, acc.: 68.75%] [G loss: 0.577993]\n",
      "epoch:20 step:18852 [D loss: 0.552546, acc.: 72.66%] [G loss: 0.623599]\n",
      "epoch:20 step:18853 [D loss: 0.586009, acc.: 66.41%] [G loss: 0.569640]\n",
      "epoch:20 step:18854 [D loss: 0.531672, acc.: 71.88%] [G loss: 0.777705]\n",
      "epoch:20 step:18855 [D loss: 0.484965, acc.: 79.69%] [G loss: 0.740919]\n",
      "epoch:20 step:18856 [D loss: 0.547708, acc.: 66.41%] [G loss: 0.723257]\n",
      "epoch:20 step:18857 [D loss: 0.587428, acc.: 61.72%] [G loss: 0.585293]\n",
      "epoch:20 step:18858 [D loss: 0.560221, acc.: 69.53%] [G loss: 0.796687]\n",
      "epoch:20 step:18859 [D loss: 0.474109, acc.: 78.12%] [G loss: 1.022058]\n",
      "epoch:20 step:18860 [D loss: 0.583058, acc.: 69.53%] [G loss: 0.701768]\n",
      "epoch:20 step:18861 [D loss: 0.539926, acc.: 70.31%] [G loss: 0.653635]\n",
      "epoch:20 step:18862 [D loss: 0.524461, acc.: 77.34%] [G loss: 0.655302]\n",
      "epoch:20 step:18863 [D loss: 0.503726, acc.: 72.66%] [G loss: 0.841966]\n",
      "epoch:20 step:18864 [D loss: 0.567340, acc.: 68.75%] [G loss: 0.737496]\n",
      "epoch:20 step:18865 [D loss: 0.563831, acc.: 71.88%] [G loss: 0.643102]\n",
      "epoch:20 step:18866 [D loss: 0.502181, acc.: 71.09%] [G loss: 0.566650]\n",
      "epoch:20 step:18867 [D loss: 0.487101, acc.: 78.12%] [G loss: 0.591300]\n",
      "epoch:20 step:18868 [D loss: 0.522282, acc.: 68.75%] [G loss: 0.590652]\n",
      "epoch:20 step:18869 [D loss: 0.613279, acc.: 64.84%] [G loss: 0.724147]\n",
      "epoch:20 step:18870 [D loss: 0.514213, acc.: 71.09%] [G loss: 0.705643]\n",
      "epoch:20 step:18871 [D loss: 0.501086, acc.: 72.66%] [G loss: 0.676351]\n",
      "epoch:20 step:18872 [D loss: 0.583367, acc.: 69.53%] [G loss: 0.749220]\n",
      "epoch:20 step:18873 [D loss: 0.516750, acc.: 70.31%] [G loss: 0.657106]\n",
      "epoch:20 step:18874 [D loss: 0.521763, acc.: 72.66%] [G loss: 0.736951]\n",
      "epoch:20 step:18875 [D loss: 0.534216, acc.: 75.00%] [G loss: 0.627340]\n",
      "epoch:20 step:18876 [D loss: 0.528867, acc.: 72.66%] [G loss: 0.835747]\n",
      "epoch:20 step:18877 [D loss: 0.645403, acc.: 65.62%] [G loss: 0.635796]\n",
      "epoch:20 step:18878 [D loss: 0.555948, acc.: 72.66%] [G loss: 0.602771]\n",
      "epoch:20 step:18879 [D loss: 0.525455, acc.: 74.22%] [G loss: 0.682296]\n",
      "epoch:20 step:18880 [D loss: 0.541323, acc.: 67.97%] [G loss: 0.623261]\n",
      "epoch:20 step:18881 [D loss: 0.547170, acc.: 64.84%] [G loss: 0.505355]\n",
      "epoch:20 step:18882 [D loss: 0.533333, acc.: 69.53%] [G loss: 0.603065]\n",
      "epoch:20 step:18883 [D loss: 0.582339, acc.: 66.41%] [G loss: 0.527372]\n",
      "epoch:20 step:18884 [D loss: 0.521463, acc.: 75.00%] [G loss: 0.569636]\n",
      "epoch:20 step:18885 [D loss: 0.525004, acc.: 70.31%] [G loss: 0.482250]\n",
      "epoch:20 step:18886 [D loss: 0.550343, acc.: 71.09%] [G loss: 0.625148]\n",
      "epoch:20 step:18887 [D loss: 0.629404, acc.: 62.50%] [G loss: 0.568186]\n",
      "epoch:20 step:18888 [D loss: 0.552919, acc.: 71.88%] [G loss: 0.593126]\n",
      "epoch:20 step:18889 [D loss: 0.495161, acc.: 74.22%] [G loss: 0.579708]\n",
      "epoch:20 step:18890 [D loss: 0.624293, acc.: 65.62%] [G loss: 0.635031]\n",
      "epoch:20 step:18891 [D loss: 0.564011, acc.: 67.19%] [G loss: 0.619491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:18892 [D loss: 0.494827, acc.: 76.56%] [G loss: 0.627791]\n",
      "epoch:20 step:18893 [D loss: 0.632385, acc.: 60.94%] [G loss: 0.476087]\n",
      "epoch:20 step:18894 [D loss: 0.547656, acc.: 67.97%] [G loss: 0.596309]\n",
      "epoch:20 step:18895 [D loss: 0.480727, acc.: 72.66%] [G loss: 0.791174]\n",
      "epoch:20 step:18896 [D loss: 0.494486, acc.: 75.00%] [G loss: 0.756249]\n",
      "epoch:20 step:18897 [D loss: 0.559126, acc.: 66.41%] [G loss: 0.700458]\n",
      "epoch:20 step:18898 [D loss: 0.586534, acc.: 64.84%] [G loss: 0.576198]\n",
      "epoch:20 step:18899 [D loss: 0.541984, acc.: 71.88%] [G loss: 0.644402]\n",
      "epoch:20 step:18900 [D loss: 0.608128, acc.: 67.19%] [G loss: 0.650012]\n",
      "epoch:20 step:18901 [D loss: 0.503742, acc.: 75.00%] [G loss: 0.796460]\n",
      "epoch:20 step:18902 [D loss: 0.532473, acc.: 74.22%] [G loss: 0.884520]\n",
      "epoch:20 step:18903 [D loss: 0.594502, acc.: 68.75%] [G loss: 0.608825]\n",
      "epoch:20 step:18904 [D loss: 0.591421, acc.: 61.72%] [G loss: 0.657437]\n",
      "epoch:20 step:18905 [D loss: 0.508721, acc.: 76.56%] [G loss: 0.680236]\n",
      "epoch:20 step:18906 [D loss: 0.559301, acc.: 67.19%] [G loss: 0.601397]\n",
      "epoch:20 step:18907 [D loss: 0.535967, acc.: 71.88%] [G loss: 0.560591]\n",
      "epoch:20 step:18908 [D loss: 0.536693, acc.: 72.66%] [G loss: 0.762683]\n",
      "epoch:20 step:18909 [D loss: 0.566085, acc.: 70.31%] [G loss: 0.704966]\n",
      "epoch:20 step:18910 [D loss: 0.567328, acc.: 66.41%] [G loss: 0.458688]\n",
      "epoch:20 step:18911 [D loss: 0.542629, acc.: 70.31%] [G loss: 0.676829]\n",
      "epoch:20 step:18912 [D loss: 0.486309, acc.: 75.78%] [G loss: 0.669996]\n",
      "epoch:20 step:18913 [D loss: 0.516166, acc.: 75.00%] [G loss: 0.787396]\n",
      "epoch:20 step:18914 [D loss: 0.566076, acc.: 67.97%] [G loss: 0.547405]\n",
      "epoch:20 step:18915 [D loss: 0.564584, acc.: 68.75%] [G loss: 0.573445]\n",
      "epoch:20 step:18916 [D loss: 0.503666, acc.: 77.34%] [G loss: 0.557374]\n",
      "epoch:20 step:18917 [D loss: 0.567414, acc.: 67.19%] [G loss: 0.627294]\n",
      "epoch:20 step:18918 [D loss: 0.589787, acc.: 65.62%] [G loss: 0.660894]\n",
      "epoch:20 step:18919 [D loss: 0.519570, acc.: 75.78%] [G loss: 0.666337]\n",
      "epoch:20 step:18920 [D loss: 0.592507, acc.: 67.19%] [G loss: 0.536168]\n",
      "epoch:20 step:18921 [D loss: 0.580930, acc.: 65.62%] [G loss: 0.635220]\n",
      "epoch:20 step:18922 [D loss: 0.521450, acc.: 72.66%] [G loss: 0.714246]\n",
      "epoch:20 step:18923 [D loss: 0.600196, acc.: 66.41%] [G loss: 0.763541]\n",
      "epoch:20 step:18924 [D loss: 0.562312, acc.: 70.31%] [G loss: 0.767704]\n",
      "epoch:20 step:18925 [D loss: 0.541091, acc.: 71.88%] [G loss: 0.596286]\n",
      "epoch:20 step:18926 [D loss: 0.569927, acc.: 71.88%] [G loss: 0.677970]\n",
      "epoch:20 step:18927 [D loss: 0.621896, acc.: 61.72%] [G loss: 0.624793]\n",
      "epoch:20 step:18928 [D loss: 0.516014, acc.: 74.22%] [G loss: 0.564227]\n",
      "epoch:20 step:18929 [D loss: 0.593482, acc.: 67.97%] [G loss: 0.504773]\n",
      "epoch:20 step:18930 [D loss: 0.494202, acc.: 77.34%] [G loss: 0.904060]\n",
      "epoch:20 step:18931 [D loss: 0.508831, acc.: 71.88%] [G loss: 0.600466]\n",
      "epoch:20 step:18932 [D loss: 0.507845, acc.: 71.88%] [G loss: 0.672271]\n",
      "epoch:20 step:18933 [D loss: 0.551308, acc.: 71.09%] [G loss: 0.666944]\n",
      "epoch:20 step:18934 [D loss: 0.446465, acc.: 77.34%] [G loss: 0.668297]\n",
      "epoch:20 step:18935 [D loss: 0.558750, acc.: 67.97%] [G loss: 0.603546]\n",
      "epoch:20 step:18936 [D loss: 0.484060, acc.: 72.66%] [G loss: 0.727875]\n",
      "epoch:20 step:18937 [D loss: 0.594513, acc.: 72.66%] [G loss: 0.648830]\n",
      "epoch:20 step:18938 [D loss: 0.485314, acc.: 75.78%] [G loss: 0.764639]\n",
      "epoch:20 step:18939 [D loss: 0.469189, acc.: 75.00%] [G loss: 0.784796]\n",
      "epoch:20 step:18940 [D loss: 0.614024, acc.: 64.06%] [G loss: 0.780867]\n",
      "epoch:20 step:18941 [D loss: 0.589460, acc.: 68.75%] [G loss: 0.697631]\n",
      "epoch:20 step:18942 [D loss: 0.501802, acc.: 73.44%] [G loss: 0.735027]\n",
      "epoch:20 step:18943 [D loss: 0.589816, acc.: 67.19%] [G loss: 0.668284]\n",
      "epoch:20 step:18944 [D loss: 0.590258, acc.: 66.41%] [G loss: 0.772921]\n",
      "epoch:20 step:18945 [D loss: 0.528576, acc.: 71.88%] [G loss: 1.036095]\n",
      "epoch:20 step:18946 [D loss: 0.523179, acc.: 75.78%] [G loss: 0.814779]\n",
      "epoch:20 step:18947 [D loss: 0.460159, acc.: 80.47%] [G loss: 0.761994]\n",
      "epoch:20 step:18948 [D loss: 0.428255, acc.: 83.59%] [G loss: 0.720100]\n",
      "epoch:20 step:18949 [D loss: 0.524092, acc.: 75.00%] [G loss: 0.740136]\n",
      "epoch:20 step:18950 [D loss: 0.638950, acc.: 69.53%] [G loss: 0.593483]\n",
      "epoch:20 step:18951 [D loss: 0.632544, acc.: 62.50%] [G loss: 0.469293]\n",
      "epoch:20 step:18952 [D loss: 0.501702, acc.: 75.78%] [G loss: 0.636456]\n",
      "epoch:20 step:18953 [D loss: 0.562842, acc.: 67.19%] [G loss: 0.666796]\n",
      "epoch:20 step:18954 [D loss: 0.659056, acc.: 60.16%] [G loss: 0.600733]\n",
      "epoch:20 step:18955 [D loss: 0.542473, acc.: 71.88%] [G loss: 0.632152]\n",
      "epoch:20 step:18956 [D loss: 0.545772, acc.: 72.66%] [G loss: 0.721051]\n",
      "epoch:20 step:18957 [D loss: 0.523404, acc.: 70.31%] [G loss: 0.615943]\n",
      "epoch:20 step:18958 [D loss: 0.474520, acc.: 75.00%] [G loss: 0.723183]\n",
      "epoch:20 step:18959 [D loss: 0.478412, acc.: 74.22%] [G loss: 0.878941]\n",
      "epoch:20 step:18960 [D loss: 0.693209, acc.: 62.50%] [G loss: 0.666087]\n",
      "epoch:20 step:18961 [D loss: 0.569122, acc.: 65.62%] [G loss: 0.756377]\n",
      "epoch:20 step:18962 [D loss: 0.504357, acc.: 76.56%] [G loss: 0.709249]\n",
      "epoch:20 step:18963 [D loss: 0.557562, acc.: 74.22%] [G loss: 0.842328]\n",
      "epoch:20 step:18964 [D loss: 0.617370, acc.: 63.28%] [G loss: 0.670251]\n",
      "epoch:20 step:18965 [D loss: 0.562853, acc.: 70.31%] [G loss: 0.752280]\n",
      "epoch:20 step:18966 [D loss: 0.579799, acc.: 68.75%] [G loss: 0.615324]\n",
      "epoch:20 step:18967 [D loss: 0.533722, acc.: 70.31%] [G loss: 0.570879]\n",
      "epoch:20 step:18968 [D loss: 0.582725, acc.: 63.28%] [G loss: 0.617293]\n",
      "epoch:20 step:18969 [D loss: 0.526358, acc.: 77.34%] [G loss: 0.486474]\n",
      "epoch:20 step:18970 [D loss: 0.574590, acc.: 71.88%] [G loss: 0.641175]\n",
      "epoch:20 step:18971 [D loss: 0.599073, acc.: 64.06%] [G loss: 0.658872]\n",
      "epoch:20 step:18972 [D loss: 0.450493, acc.: 83.59%] [G loss: 0.900303]\n",
      "epoch:20 step:18973 [D loss: 0.531670, acc.: 72.66%] [G loss: 0.921731]\n",
      "epoch:20 step:18974 [D loss: 0.593222, acc.: 68.75%] [G loss: 0.771234]\n",
      "epoch:20 step:18975 [D loss: 0.601800, acc.: 60.16%] [G loss: 0.516023]\n",
      "epoch:20 step:18976 [D loss: 0.549876, acc.: 67.19%] [G loss: 0.544682]\n",
      "epoch:20 step:18977 [D loss: 0.532286, acc.: 74.22%] [G loss: 0.649574]\n",
      "epoch:20 step:18978 [D loss: 0.589008, acc.: 64.06%] [G loss: 0.712816]\n",
      "epoch:20 step:18979 [D loss: 0.520786, acc.: 69.53%] [G loss: 0.622793]\n",
      "epoch:20 step:18980 [D loss: 0.567504, acc.: 71.88%] [G loss: 0.625094]\n",
      "epoch:20 step:18981 [D loss: 0.545824, acc.: 73.44%] [G loss: 0.563327]\n",
      "epoch:20 step:18982 [D loss: 0.545842, acc.: 65.62%] [G loss: 0.601389]\n",
      "epoch:20 step:18983 [D loss: 0.541966, acc.: 73.44%] [G loss: 0.600750]\n",
      "epoch:20 step:18984 [D loss: 0.497234, acc.: 74.22%] [G loss: 0.746539]\n",
      "epoch:20 step:18985 [D loss: 0.557140, acc.: 73.44%] [G loss: 0.690263]\n",
      "epoch:20 step:18986 [D loss: 0.562595, acc.: 67.19%] [G loss: 0.624151]\n",
      "epoch:20 step:18987 [D loss: 0.517132, acc.: 74.22%] [G loss: 0.573043]\n",
      "epoch:20 step:18988 [D loss: 0.503214, acc.: 72.66%] [G loss: 0.609313]\n",
      "epoch:20 step:18989 [D loss: 0.556108, acc.: 69.53%] [G loss: 0.714074]\n",
      "epoch:20 step:18990 [D loss: 0.652710, acc.: 67.19%] [G loss: 0.686641]\n",
      "epoch:20 step:18991 [D loss: 0.588107, acc.: 68.75%] [G loss: 0.547944]\n",
      "epoch:20 step:18992 [D loss: 0.555099, acc.: 72.66%] [G loss: 0.684396]\n",
      "epoch:20 step:18993 [D loss: 0.528930, acc.: 72.66%] [G loss: 0.597508]\n",
      "epoch:20 step:18994 [D loss: 0.460526, acc.: 78.12%] [G loss: 0.632043]\n",
      "epoch:20 step:18995 [D loss: 0.569742, acc.: 67.19%] [G loss: 0.695391]\n",
      "epoch:20 step:18996 [D loss: 0.527656, acc.: 72.66%] [G loss: 0.724488]\n",
      "epoch:20 step:18997 [D loss: 0.553914, acc.: 64.84%] [G loss: 0.595577]\n",
      "epoch:20 step:18998 [D loss: 0.575882, acc.: 69.53%] [G loss: 0.588076]\n",
      "epoch:20 step:18999 [D loss: 0.538912, acc.: 66.41%] [G loss: 0.612166]\n",
      "epoch:20 step:19000 [D loss: 0.586875, acc.: 66.41%] [G loss: 0.723964]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.567566\n",
      "FID: 46.691261\n",
      "0 = 13.05456100320816\n",
      "1 = 0.09700589105079943\n",
      "2 = 0.8784000277519226\n",
      "3 = 0.8370000123977661\n",
      "4 = 0.9197999835014343\n",
      "5 = 0.912559986114502\n",
      "6 = 0.8370000123977661\n",
      "7 = 8.177840111863636\n",
      "8 = 0.14379571933980392\n",
      "9 = 0.7098000049591064\n",
      "10 = 0.6990000009536743\n",
      "11 = 0.7206000089645386\n",
      "12 = 0.7144317030906677\n",
      "13 = 0.6990000009536743\n",
      "14 = 6.5675883293151855\n",
      "15 = 7.057477951049805\n",
      "16 = 0.3854093551635742\n",
      "17 = 6.56756591796875\n",
      "18 = 46.691261291503906\n",
      "epoch:20 step:19001 [D loss: 0.543010, acc.: 71.09%] [G loss: 0.716854]\n",
      "epoch:20 step:19002 [D loss: 0.510385, acc.: 74.22%] [G loss: 0.594386]\n",
      "epoch:20 step:19003 [D loss: 0.643384, acc.: 65.62%] [G loss: 0.506486]\n",
      "epoch:20 step:19004 [D loss: 0.522689, acc.: 68.75%] [G loss: 0.768708]\n",
      "epoch:20 step:19005 [D loss: 0.522917, acc.: 71.88%] [G loss: 0.682470]\n",
      "epoch:20 step:19006 [D loss: 0.613729, acc.: 63.28%] [G loss: 0.654733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19007 [D loss: 0.521615, acc.: 72.66%] [G loss: 0.603997]\n",
      "epoch:20 step:19008 [D loss: 0.560459, acc.: 65.62%] [G loss: 0.577899]\n",
      "epoch:20 step:19009 [D loss: 0.529269, acc.: 71.09%] [G loss: 0.617327]\n",
      "epoch:20 step:19010 [D loss: 0.493572, acc.: 77.34%] [G loss: 0.654059]\n",
      "epoch:20 step:19011 [D loss: 0.507955, acc.: 76.56%] [G loss: 0.741587]\n",
      "epoch:20 step:19012 [D loss: 0.526448, acc.: 69.53%] [G loss: 0.670351]\n",
      "epoch:20 step:19013 [D loss: 0.526297, acc.: 73.44%] [G loss: 0.742318]\n",
      "epoch:20 step:19014 [D loss: 0.518702, acc.: 75.78%] [G loss: 0.585919]\n",
      "epoch:20 step:19015 [D loss: 0.615178, acc.: 69.53%] [G loss: 0.623032]\n",
      "epoch:20 step:19016 [D loss: 0.471324, acc.: 77.34%] [G loss: 0.817508]\n",
      "epoch:20 step:19017 [D loss: 0.683990, acc.: 64.84%] [G loss: 0.644159]\n",
      "epoch:20 step:19018 [D loss: 0.635827, acc.: 64.06%] [G loss: 0.541382]\n",
      "epoch:20 step:19019 [D loss: 0.541790, acc.: 69.53%] [G loss: 0.646540]\n",
      "epoch:20 step:19020 [D loss: 0.525180, acc.: 70.31%] [G loss: 0.577878]\n",
      "epoch:20 step:19021 [D loss: 0.618970, acc.: 61.72%] [G loss: 0.604739]\n",
      "epoch:20 step:19022 [D loss: 0.528000, acc.: 71.88%] [G loss: 0.617522]\n",
      "epoch:20 step:19023 [D loss: 0.521069, acc.: 78.12%] [G loss: 0.562385]\n",
      "epoch:20 step:19024 [D loss: 0.513997, acc.: 72.66%] [G loss: 0.483389]\n",
      "epoch:20 step:19025 [D loss: 0.570697, acc.: 71.88%] [G loss: 0.558602]\n",
      "epoch:20 step:19026 [D loss: 0.453265, acc.: 80.47%] [G loss: 0.839830]\n",
      "epoch:20 step:19027 [D loss: 0.577889, acc.: 68.75%] [G loss: 0.794826]\n",
      "epoch:20 step:19028 [D loss: 0.586701, acc.: 63.28%] [G loss: 0.641810]\n",
      "epoch:20 step:19029 [D loss: 0.558107, acc.: 71.09%] [G loss: 0.668978]\n",
      "epoch:20 step:19030 [D loss: 0.524811, acc.: 72.66%] [G loss: 0.640933]\n",
      "epoch:20 step:19031 [D loss: 0.616517, acc.: 64.84%] [G loss: 0.570559]\n",
      "epoch:20 step:19032 [D loss: 0.506271, acc.: 72.66%] [G loss: 0.625593]\n",
      "epoch:20 step:19033 [D loss: 0.553980, acc.: 67.19%] [G loss: 0.555039]\n",
      "epoch:20 step:19034 [D loss: 0.581179, acc.: 67.97%] [G loss: 0.527968]\n",
      "epoch:20 step:19035 [D loss: 0.500325, acc.: 75.00%] [G loss: 0.608133]\n",
      "epoch:20 step:19036 [D loss: 0.489414, acc.: 72.66%] [G loss: 0.750148]\n",
      "epoch:20 step:19037 [D loss: 0.554313, acc.: 67.19%] [G loss: 0.699774]\n",
      "epoch:20 step:19038 [D loss: 0.464184, acc.: 82.03%] [G loss: 0.800043]\n",
      "epoch:20 step:19039 [D loss: 0.512812, acc.: 71.09%] [G loss: 0.734688]\n",
      "epoch:20 step:19040 [D loss: 0.450557, acc.: 75.78%] [G loss: 0.905478]\n",
      "epoch:20 step:19041 [D loss: 0.693871, acc.: 59.38%] [G loss: 0.526345]\n",
      "epoch:20 step:19042 [D loss: 0.494645, acc.: 74.22%] [G loss: 0.785145]\n",
      "epoch:20 step:19043 [D loss: 0.590710, acc.: 68.75%] [G loss: 0.691959]\n",
      "epoch:20 step:19044 [D loss: 0.487397, acc.: 76.56%] [G loss: 0.697924]\n",
      "epoch:20 step:19045 [D loss: 0.511685, acc.: 73.44%] [G loss: 0.740328]\n",
      "epoch:20 step:19046 [D loss: 0.503531, acc.: 74.22%] [G loss: 0.647358]\n",
      "epoch:20 step:19047 [D loss: 0.560413, acc.: 71.09%] [G loss: 0.742089]\n",
      "epoch:20 step:19048 [D loss: 0.551247, acc.: 67.97%] [G loss: 0.707368]\n",
      "epoch:20 step:19049 [D loss: 0.501578, acc.: 71.88%] [G loss: 0.644426]\n",
      "epoch:20 step:19050 [D loss: 0.559454, acc.: 73.44%] [G loss: 0.582578]\n",
      "epoch:20 step:19051 [D loss: 0.500723, acc.: 68.75%] [G loss: 0.830811]\n",
      "epoch:20 step:19052 [D loss: 0.432789, acc.: 81.25%] [G loss: 0.767247]\n",
      "epoch:20 step:19053 [D loss: 0.523308, acc.: 75.78%] [G loss: 0.982296]\n",
      "epoch:20 step:19054 [D loss: 0.428180, acc.: 78.91%] [G loss: 0.884735]\n",
      "epoch:20 step:19055 [D loss: 0.496749, acc.: 72.66%] [G loss: 1.009944]\n",
      "epoch:20 step:19056 [D loss: 0.678885, acc.: 62.50%] [G loss: 0.684729]\n",
      "epoch:20 step:19057 [D loss: 0.559207, acc.: 69.53%] [G loss: 0.564033]\n",
      "epoch:20 step:19058 [D loss: 0.500646, acc.: 72.66%] [G loss: 0.561971]\n",
      "epoch:20 step:19059 [D loss: 0.532150, acc.: 67.19%] [G loss: 0.641005]\n",
      "epoch:20 step:19060 [D loss: 0.559219, acc.: 71.88%] [G loss: 0.622293]\n",
      "epoch:20 step:19061 [D loss: 0.514602, acc.: 71.09%] [G loss: 0.650206]\n",
      "epoch:20 step:19062 [D loss: 0.538882, acc.: 70.31%] [G loss: 0.614932]\n",
      "epoch:20 step:19063 [D loss: 0.639996, acc.: 66.41%] [G loss: 0.567257]\n",
      "epoch:20 step:19064 [D loss: 0.524702, acc.: 71.88%] [G loss: 0.684111]\n",
      "epoch:20 step:19065 [D loss: 0.508651, acc.: 75.00%] [G loss: 0.682609]\n",
      "epoch:20 step:19066 [D loss: 0.504400, acc.: 75.00%] [G loss: 0.715413]\n",
      "epoch:20 step:19067 [D loss: 0.478992, acc.: 82.03%] [G loss: 0.610595]\n",
      "epoch:20 step:19068 [D loss: 0.448627, acc.: 83.59%] [G loss: 0.844310]\n",
      "epoch:20 step:19069 [D loss: 0.543724, acc.: 74.22%] [G loss: 0.669849]\n",
      "epoch:20 step:19070 [D loss: 0.560906, acc.: 70.31%] [G loss: 0.683476]\n",
      "epoch:20 step:19071 [D loss: 0.555800, acc.: 71.09%] [G loss: 0.505723]\n",
      "epoch:20 step:19072 [D loss: 0.513767, acc.: 71.88%] [G loss: 0.567237]\n",
      "epoch:20 step:19073 [D loss: 0.504498, acc.: 76.56%] [G loss: 0.676975]\n",
      "epoch:20 step:19074 [D loss: 0.488287, acc.: 78.12%] [G loss: 0.697608]\n",
      "epoch:20 step:19075 [D loss: 0.526056, acc.: 71.09%] [G loss: 0.705306]\n",
      "epoch:20 step:19076 [D loss: 0.475457, acc.: 78.91%] [G loss: 0.732818]\n",
      "epoch:20 step:19077 [D loss: 0.497368, acc.: 78.12%] [G loss: 0.806537]\n",
      "epoch:20 step:19078 [D loss: 0.539022, acc.: 71.09%] [G loss: 0.759113]\n",
      "epoch:20 step:19079 [D loss: 0.515630, acc.: 75.00%] [G loss: 0.804180]\n",
      "epoch:20 step:19080 [D loss: 0.531433, acc.: 68.75%] [G loss: 0.726070]\n",
      "epoch:20 step:19081 [D loss: 0.644738, acc.: 67.19%] [G loss: 0.797906]\n",
      "epoch:20 step:19082 [D loss: 0.642195, acc.: 60.94%] [G loss: 0.586318]\n",
      "epoch:20 step:19083 [D loss: 0.482402, acc.: 76.56%] [G loss: 0.906498]\n",
      "epoch:20 step:19084 [D loss: 0.468348, acc.: 77.34%] [G loss: 0.835452]\n",
      "epoch:20 step:19085 [D loss: 0.506021, acc.: 75.00%] [G loss: 0.788643]\n",
      "epoch:20 step:19086 [D loss: 0.557238, acc.: 70.31%] [G loss: 0.610665]\n",
      "epoch:20 step:19087 [D loss: 0.446681, acc.: 82.03%] [G loss: 0.932490]\n",
      "epoch:20 step:19088 [D loss: 0.596522, acc.: 68.75%] [G loss: 0.676468]\n",
      "epoch:20 step:19089 [D loss: 0.730996, acc.: 57.81%] [G loss: 0.519756]\n",
      "epoch:20 step:19090 [D loss: 0.505643, acc.: 74.22%] [G loss: 0.601775]\n",
      "epoch:20 step:19091 [D loss: 0.533697, acc.: 72.66%] [G loss: 0.648248]\n",
      "epoch:20 step:19092 [D loss: 0.603913, acc.: 64.06%] [G loss: 0.644452]\n",
      "epoch:20 step:19093 [D loss: 0.531475, acc.: 74.22%] [G loss: 0.706950]\n",
      "epoch:20 step:19094 [D loss: 0.422469, acc.: 82.03%] [G loss: 0.912554]\n",
      "epoch:20 step:19095 [D loss: 0.527864, acc.: 68.75%] [G loss: 0.840432]\n",
      "epoch:20 step:19096 [D loss: 0.506273, acc.: 75.78%] [G loss: 0.682032]\n",
      "epoch:20 step:19097 [D loss: 0.416257, acc.: 80.47%] [G loss: 0.840074]\n",
      "epoch:20 step:19098 [D loss: 0.528131, acc.: 70.31%] [G loss: 0.666583]\n",
      "epoch:20 step:19099 [D loss: 0.418487, acc.: 84.38%] [G loss: 0.776492]\n",
      "epoch:20 step:19100 [D loss: 0.512722, acc.: 75.78%] [G loss: 0.698119]\n",
      "epoch:20 step:19101 [D loss: 0.515751, acc.: 76.56%] [G loss: 0.654995]\n",
      "epoch:20 step:19102 [D loss: 0.558669, acc.: 72.66%] [G loss: 0.881864]\n",
      "epoch:20 step:19103 [D loss: 0.554235, acc.: 64.84%] [G loss: 0.855706]\n",
      "epoch:20 step:19104 [D loss: 0.565040, acc.: 64.84%] [G loss: 0.682327]\n",
      "epoch:20 step:19105 [D loss: 0.561911, acc.: 70.31%] [G loss: 0.779888]\n",
      "epoch:20 step:19106 [D loss: 0.554390, acc.: 74.22%] [G loss: 0.703614]\n",
      "epoch:20 step:19107 [D loss: 0.603783, acc.: 64.06%] [G loss: 0.536220]\n",
      "epoch:20 step:19108 [D loss: 0.559632, acc.: 67.97%] [G loss: 0.601129]\n",
      "epoch:20 step:19109 [D loss: 0.515515, acc.: 69.53%] [G loss: 0.633117]\n",
      "epoch:20 step:19110 [D loss: 0.531647, acc.: 73.44%] [G loss: 0.672020]\n",
      "epoch:20 step:19111 [D loss: 0.501682, acc.: 75.00%] [G loss: 0.525160]\n",
      "epoch:20 step:19112 [D loss: 0.570753, acc.: 71.09%] [G loss: 0.534621]\n",
      "epoch:20 step:19113 [D loss: 0.498885, acc.: 71.88%] [G loss: 0.684771]\n",
      "epoch:20 step:19114 [D loss: 0.475204, acc.: 74.22%] [G loss: 0.738891]\n",
      "epoch:20 step:19115 [D loss: 0.621684, acc.: 63.28%] [G loss: 0.697476]\n",
      "epoch:20 step:19116 [D loss: 0.630365, acc.: 64.06%] [G loss: 0.512727]\n",
      "epoch:20 step:19117 [D loss: 0.579060, acc.: 67.97%] [G loss: 0.605455]\n",
      "epoch:20 step:19118 [D loss: 0.523291, acc.: 71.88%] [G loss: 0.653956]\n",
      "epoch:20 step:19119 [D loss: 0.573636, acc.: 65.62%] [G loss: 0.550428]\n",
      "epoch:20 step:19120 [D loss: 0.582715, acc.: 67.97%] [G loss: 0.541635]\n",
      "epoch:20 step:19121 [D loss: 0.465881, acc.: 75.00%] [G loss: 0.656524]\n",
      "epoch:20 step:19122 [D loss: 0.497479, acc.: 76.56%] [G loss: 0.763232]\n",
      "epoch:20 step:19123 [D loss: 0.578172, acc.: 71.09%] [G loss: 0.657498]\n",
      "epoch:20 step:19124 [D loss: 0.532925, acc.: 67.19%] [G loss: 0.610634]\n",
      "epoch:20 step:19125 [D loss: 0.503510, acc.: 72.66%] [G loss: 0.684907]\n",
      "epoch:20 step:19126 [D loss: 0.580159, acc.: 68.75%] [G loss: 0.526439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19127 [D loss: 0.544700, acc.: 69.53%] [G loss: 0.528214]\n",
      "epoch:20 step:19128 [D loss: 0.517434, acc.: 75.78%] [G loss: 0.562005]\n",
      "epoch:20 step:19129 [D loss: 0.563572, acc.: 70.31%] [G loss: 0.574241]\n",
      "epoch:20 step:19130 [D loss: 0.571273, acc.: 67.97%] [G loss: 0.599102]\n",
      "epoch:20 step:19131 [D loss: 0.538285, acc.: 72.66%] [G loss: 0.714717]\n",
      "epoch:20 step:19132 [D loss: 0.457941, acc.: 78.12%] [G loss: 0.571095]\n",
      "epoch:20 step:19133 [D loss: 0.572864, acc.: 65.62%] [G loss: 0.553735]\n",
      "epoch:20 step:19134 [D loss: 0.584184, acc.: 64.06%] [G loss: 0.506988]\n",
      "epoch:20 step:19135 [D loss: 0.554286, acc.: 67.97%] [G loss: 0.627676]\n",
      "epoch:20 step:19136 [D loss: 0.546956, acc.: 68.75%] [G loss: 0.623134]\n",
      "epoch:20 step:19137 [D loss: 0.601507, acc.: 63.28%] [G loss: 0.665668]\n",
      "epoch:20 step:19138 [D loss: 0.489123, acc.: 71.88%] [G loss: 0.899155]\n",
      "epoch:20 step:19139 [D loss: 0.529063, acc.: 69.53%] [G loss: 0.946441]\n",
      "epoch:20 step:19140 [D loss: 0.688553, acc.: 53.91%] [G loss: 0.540585]\n",
      "epoch:20 step:19141 [D loss: 0.603570, acc.: 63.28%] [G loss: 0.452229]\n",
      "epoch:20 step:19142 [D loss: 0.571245, acc.: 66.41%] [G loss: 0.611597]\n",
      "epoch:20 step:19143 [D loss: 0.514982, acc.: 71.88%] [G loss: 0.625747]\n",
      "epoch:20 step:19144 [D loss: 0.618557, acc.: 59.38%] [G loss: 0.619612]\n",
      "epoch:20 step:19145 [D loss: 0.566743, acc.: 67.97%] [G loss: 0.544693]\n",
      "epoch:20 step:19146 [D loss: 0.496385, acc.: 75.00%] [G loss: 0.792583]\n",
      "epoch:20 step:19147 [D loss: 0.640458, acc.: 62.50%] [G loss: 0.631794]\n",
      "epoch:20 step:19148 [D loss: 0.543401, acc.: 67.97%] [G loss: 0.651053]\n",
      "epoch:20 step:19149 [D loss: 0.542110, acc.: 72.66%] [G loss: 0.613117]\n",
      "epoch:20 step:19150 [D loss: 0.572436, acc.: 71.09%] [G loss: 0.596282]\n",
      "epoch:20 step:19151 [D loss: 0.550857, acc.: 69.53%] [G loss: 0.549812]\n",
      "epoch:20 step:19152 [D loss: 0.659063, acc.: 60.94%] [G loss: 0.413671]\n",
      "epoch:20 step:19153 [D loss: 0.533072, acc.: 66.41%] [G loss: 0.650001]\n",
      "epoch:20 step:19154 [D loss: 0.521283, acc.: 71.09%] [G loss: 0.615342]\n",
      "epoch:20 step:19155 [D loss: 0.557329, acc.: 71.88%] [G loss: 0.585112]\n",
      "epoch:20 step:19156 [D loss: 0.551168, acc.: 71.09%] [G loss: 0.650056]\n",
      "epoch:20 step:19157 [D loss: 0.593641, acc.: 71.09%] [G loss: 0.766634]\n",
      "epoch:20 step:19158 [D loss: 0.651384, acc.: 63.28%] [G loss: 0.550376]\n",
      "epoch:20 step:19159 [D loss: 0.557702, acc.: 66.41%] [G loss: 0.683305]\n",
      "epoch:20 step:19160 [D loss: 0.547804, acc.: 68.75%] [G loss: 0.654965]\n",
      "epoch:20 step:19161 [D loss: 0.626499, acc.: 55.47%] [G loss: 0.473924]\n",
      "epoch:20 step:19162 [D loss: 0.636097, acc.: 65.62%] [G loss: 0.455784]\n",
      "epoch:20 step:19163 [D loss: 0.586857, acc.: 67.19%] [G loss: 0.465804]\n",
      "epoch:20 step:19164 [D loss: 0.590588, acc.: 67.19%] [G loss: 0.535619]\n",
      "epoch:20 step:19165 [D loss: 0.489501, acc.: 79.69%] [G loss: 0.668093]\n",
      "epoch:20 step:19166 [D loss: 0.478675, acc.: 74.22%] [G loss: 0.837601]\n",
      "epoch:20 step:19167 [D loss: 0.482196, acc.: 80.47%] [G loss: 0.675283]\n",
      "epoch:20 step:19168 [D loss: 0.532236, acc.: 68.75%] [G loss: 0.753927]\n",
      "epoch:20 step:19169 [D loss: 0.452241, acc.: 76.56%] [G loss: 0.705769]\n",
      "epoch:20 step:19170 [D loss: 0.501566, acc.: 73.44%] [G loss: 0.771495]\n",
      "epoch:20 step:19171 [D loss: 0.590388, acc.: 70.31%] [G loss: 0.809088]\n",
      "epoch:20 step:19172 [D loss: 0.532424, acc.: 70.31%] [G loss: 0.700456]\n",
      "epoch:20 step:19173 [D loss: 0.554821, acc.: 69.53%] [G loss: 0.627412]\n",
      "epoch:20 step:19174 [D loss: 0.522975, acc.: 77.34%] [G loss: 0.690688]\n",
      "epoch:20 step:19175 [D loss: 0.608793, acc.: 66.41%] [G loss: 0.656652]\n",
      "epoch:20 step:19176 [D loss: 0.450654, acc.: 75.78%] [G loss: 0.858654]\n",
      "epoch:20 step:19177 [D loss: 0.664646, acc.: 63.28%] [G loss: 0.634123]\n",
      "epoch:20 step:19178 [D loss: 0.584691, acc.: 64.84%] [G loss: 0.596237]\n",
      "epoch:20 step:19179 [D loss: 0.529369, acc.: 71.09%] [G loss: 0.755631]\n",
      "epoch:20 step:19180 [D loss: 0.488986, acc.: 76.56%] [G loss: 0.755431]\n",
      "epoch:20 step:19181 [D loss: 0.590607, acc.: 65.62%] [G loss: 0.772122]\n",
      "epoch:20 step:19182 [D loss: 0.515585, acc.: 71.09%] [G loss: 0.730689]\n",
      "epoch:20 step:19183 [D loss: 0.597282, acc.: 64.84%] [G loss: 0.618865]\n",
      "epoch:20 step:19184 [D loss: 0.527819, acc.: 71.09%] [G loss: 0.821744]\n",
      "epoch:20 step:19185 [D loss: 0.567408, acc.: 66.41%] [G loss: 0.733960]\n",
      "epoch:20 step:19186 [D loss: 0.519183, acc.: 73.44%] [G loss: 0.873225]\n",
      "epoch:20 step:19187 [D loss: 0.516088, acc.: 71.09%] [G loss: 0.769704]\n",
      "epoch:20 step:19188 [D loss: 0.567483, acc.: 71.09%] [G loss: 0.772940]\n",
      "epoch:20 step:19189 [D loss: 0.467827, acc.: 78.12%] [G loss: 0.850676]\n",
      "epoch:20 step:19190 [D loss: 0.549101, acc.: 71.88%] [G loss: 0.800839]\n",
      "epoch:20 step:19191 [D loss: 0.435225, acc.: 84.38%] [G loss: 0.913355]\n",
      "epoch:20 step:19192 [D loss: 0.484648, acc.: 76.56%] [G loss: 0.932729]\n",
      "epoch:20 step:19193 [D loss: 0.498207, acc.: 73.44%] [G loss: 0.770826]\n",
      "epoch:20 step:19194 [D loss: 0.543776, acc.: 75.00%] [G loss: 0.668784]\n",
      "epoch:20 step:19195 [D loss: 0.527971, acc.: 75.00%] [G loss: 0.681238]\n",
      "epoch:20 step:19196 [D loss: 0.629707, acc.: 63.28%] [G loss: 0.644372]\n",
      "epoch:20 step:19197 [D loss: 0.476840, acc.: 77.34%] [G loss: 0.671380]\n",
      "epoch:20 step:19198 [D loss: 0.663107, acc.: 60.94%] [G loss: 0.724563]\n",
      "epoch:20 step:19199 [D loss: 0.528259, acc.: 69.53%] [G loss: 0.744521]\n",
      "epoch:20 step:19200 [D loss: 0.500295, acc.: 73.44%] [G loss: 0.713420]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.676797\n",
      "FID: 44.399643\n",
      "0 = 13.070515189743004\n",
      "1 = 0.10122860529408255\n",
      "2 = 0.8652999997138977\n",
      "3 = 0.8212000131607056\n",
      "4 = 0.9093999862670898\n",
      "5 = 0.9006360769271851\n",
      "6 = 0.8212000131607056\n",
      "7 = 8.049933822107317\n",
      "8 = 0.1364839990760242\n",
      "9 = 0.7070000171661377\n",
      "10 = 0.703000009059906\n",
      "11 = 0.7110000252723694\n",
      "12 = 0.7086693644523621\n",
      "13 = 0.703000009059906\n",
      "14 = 6.676827907562256\n",
      "15 = 7.041593074798584\n",
      "16 = 0.3843597173690796\n",
      "17 = 6.676797389984131\n",
      "18 = 44.39964294433594\n",
      "epoch:20 step:19201 [D loss: 0.563084, acc.: 69.53%] [G loss: 0.693302]\n",
      "epoch:20 step:19202 [D loss: 0.557156, acc.: 70.31%] [G loss: 0.607571]\n",
      "epoch:20 step:19203 [D loss: 0.536553, acc.: 69.53%] [G loss: 0.576649]\n",
      "epoch:20 step:19204 [D loss: 0.520491, acc.: 70.31%] [G loss: 0.642089]\n",
      "epoch:20 step:19205 [D loss: 0.720192, acc.: 54.69%] [G loss: 0.555924]\n",
      "epoch:20 step:19206 [D loss: 0.566803, acc.: 68.75%] [G loss: 0.667341]\n",
      "epoch:20 step:19207 [D loss: 0.555720, acc.: 66.41%] [G loss: 0.589492]\n",
      "epoch:20 step:19208 [D loss: 0.531913, acc.: 71.09%] [G loss: 0.672929]\n",
      "epoch:20 step:19209 [D loss: 0.558076, acc.: 71.88%] [G loss: 0.578557]\n",
      "epoch:20 step:19210 [D loss: 0.520662, acc.: 76.56%] [G loss: 0.637179]\n",
      "epoch:20 step:19211 [D loss: 0.451002, acc.: 82.03%] [G loss: 0.687885]\n",
      "epoch:20 step:19212 [D loss: 0.417341, acc.: 82.03%] [G loss: 1.111590]\n",
      "epoch:20 step:19213 [D loss: 0.636931, acc.: 62.50%] [G loss: 0.860261]\n",
      "epoch:20 step:19214 [D loss: 0.545891, acc.: 67.19%] [G loss: 0.675207]\n",
      "epoch:20 step:19215 [D loss: 0.484530, acc.: 75.00%] [G loss: 0.772789]\n",
      "epoch:20 step:19216 [D loss: 0.573322, acc.: 68.75%] [G loss: 0.554429]\n",
      "epoch:20 step:19217 [D loss: 0.657844, acc.: 62.50%] [G loss: 0.576382]\n",
      "epoch:20 step:19218 [D loss: 0.536003, acc.: 75.78%] [G loss: 0.472554]\n",
      "epoch:20 step:19219 [D loss: 0.468070, acc.: 80.47%] [G loss: 0.580556]\n",
      "epoch:20 step:19220 [D loss: 0.602686, acc.: 67.97%] [G loss: 0.641356]\n",
      "epoch:20 step:19221 [D loss: 0.530115, acc.: 81.25%] [G loss: 0.983372]\n",
      "epoch:20 step:19222 [D loss: 0.647384, acc.: 63.28%] [G loss: 0.572139]\n",
      "epoch:20 step:19223 [D loss: 0.522481, acc.: 71.09%] [G loss: 0.690575]\n",
      "epoch:20 step:19224 [D loss: 0.476593, acc.: 78.91%] [G loss: 0.648939]\n",
      "epoch:20 step:19225 [D loss: 0.557104, acc.: 69.53%] [G loss: 0.591861]\n",
      "epoch:20 step:19226 [D loss: 0.568212, acc.: 65.62%] [G loss: 0.693446]\n",
      "epoch:20 step:19227 [D loss: 0.562143, acc.: 69.53%] [G loss: 0.513206]\n",
      "epoch:20 step:19228 [D loss: 0.491170, acc.: 75.00%] [G loss: 0.620142]\n",
      "epoch:20 step:19229 [D loss: 0.525475, acc.: 71.88%] [G loss: 0.561869]\n",
      "epoch:20 step:19230 [D loss: 0.548372, acc.: 72.66%] [G loss: 0.702296]\n",
      "epoch:20 step:19231 [D loss: 0.559636, acc.: 71.09%] [G loss: 0.735556]\n",
      "epoch:20 step:19232 [D loss: 0.590270, acc.: 68.75%] [G loss: 0.741447]\n",
      "epoch:20 step:19233 [D loss: 0.550217, acc.: 74.22%] [G loss: 0.640855]\n",
      "epoch:20 step:19234 [D loss: 0.593274, acc.: 66.41%] [G loss: 0.662295]\n",
      "epoch:20 step:19235 [D loss: 0.498161, acc.: 77.34%] [G loss: 0.684409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19236 [D loss: 0.538762, acc.: 69.53%] [G loss: 0.682978]\n",
      "epoch:20 step:19237 [D loss: 0.580790, acc.: 67.97%] [G loss: 0.711390]\n",
      "epoch:20 step:19238 [D loss: 0.573545, acc.: 67.97%] [G loss: 0.534379]\n",
      "epoch:20 step:19239 [D loss: 0.476166, acc.: 79.69%] [G loss: 0.910154]\n",
      "epoch:20 step:19240 [D loss: 0.589122, acc.: 69.53%] [G loss: 0.697579]\n",
      "epoch:20 step:19241 [D loss: 0.622249, acc.: 63.28%] [G loss: 0.572731]\n",
      "epoch:20 step:19242 [D loss: 0.622564, acc.: 61.72%] [G loss: 0.503828]\n",
      "epoch:20 step:19243 [D loss: 0.511457, acc.: 75.78%] [G loss: 0.716319]\n",
      "epoch:20 step:19244 [D loss: 0.499006, acc.: 75.78%] [G loss: 0.641008]\n",
      "epoch:20 step:19245 [D loss: 0.503988, acc.: 75.78%] [G loss: 0.694643]\n",
      "epoch:20 step:19246 [D loss: 0.545447, acc.: 71.88%] [G loss: 0.829052]\n",
      "epoch:20 step:19247 [D loss: 0.588963, acc.: 70.31%] [G loss: 0.754439]\n",
      "epoch:20 step:19248 [D loss: 0.441074, acc.: 81.25%] [G loss: 1.032844]\n",
      "epoch:20 step:19249 [D loss: 0.561097, acc.: 75.00%] [G loss: 0.850765]\n",
      "epoch:20 step:19250 [D loss: 0.668357, acc.: 61.72%] [G loss: 0.569381]\n",
      "epoch:20 step:19251 [D loss: 0.652899, acc.: 60.16%] [G loss: 0.498095]\n",
      "epoch:20 step:19252 [D loss: 0.609392, acc.: 65.62%] [G loss: 0.432224]\n",
      "epoch:20 step:19253 [D loss: 0.570618, acc.: 66.41%] [G loss: 0.412948]\n",
      "epoch:20 step:19254 [D loss: 0.503541, acc.: 72.66%] [G loss: 0.610181]\n",
      "epoch:20 step:19255 [D loss: 0.533488, acc.: 71.09%] [G loss: 0.721936]\n",
      "epoch:20 step:19256 [D loss: 0.476688, acc.: 82.03%] [G loss: 0.774526]\n",
      "epoch:20 step:19257 [D loss: 0.534323, acc.: 74.22%] [G loss: 0.664496]\n",
      "epoch:20 step:19258 [D loss: 0.559834, acc.: 66.41%] [G loss: 0.688457]\n",
      "epoch:20 step:19259 [D loss: 0.453314, acc.: 79.69%] [G loss: 0.772185]\n",
      "epoch:20 step:19260 [D loss: 0.498572, acc.: 69.53%] [G loss: 0.622119]\n",
      "epoch:20 step:19261 [D loss: 0.483144, acc.: 80.47%] [G loss: 0.761690]\n",
      "epoch:20 step:19262 [D loss: 0.498355, acc.: 76.56%] [G loss: 0.830008]\n",
      "epoch:20 step:19263 [D loss: 0.499211, acc.: 73.44%] [G loss: 0.812606]\n",
      "epoch:20 step:19264 [D loss: 0.547069, acc.: 67.97%] [G loss: 0.732216]\n",
      "epoch:20 step:19265 [D loss: 0.607431, acc.: 69.53%] [G loss: 0.532765]\n",
      "epoch:20 step:19266 [D loss: 0.547575, acc.: 70.31%] [G loss: 0.407550]\n",
      "epoch:20 step:19267 [D loss: 0.536490, acc.: 69.53%] [G loss: 0.582314]\n",
      "epoch:20 step:19268 [D loss: 0.677534, acc.: 58.59%] [G loss: 0.519412]\n",
      "epoch:20 step:19269 [D loss: 0.585071, acc.: 67.19%] [G loss: 0.582839]\n",
      "epoch:20 step:19270 [D loss: 0.541626, acc.: 69.53%] [G loss: 0.582271]\n",
      "epoch:20 step:19271 [D loss: 0.630904, acc.: 64.84%] [G loss: 0.744497]\n",
      "epoch:20 step:19272 [D loss: 0.551998, acc.: 70.31%] [G loss: 0.624579]\n",
      "epoch:20 step:19273 [D loss: 0.559280, acc.: 70.31%] [G loss: 0.735387]\n",
      "epoch:20 step:19274 [D loss: 0.524607, acc.: 68.75%] [G loss: 0.669372]\n",
      "epoch:20 step:19275 [D loss: 0.593219, acc.: 68.75%] [G loss: 0.621306]\n",
      "epoch:20 step:19276 [D loss: 0.551052, acc.: 67.97%] [G loss: 0.509966]\n",
      "epoch:20 step:19277 [D loss: 0.534495, acc.: 72.66%] [G loss: 0.657964]\n",
      "epoch:20 step:19278 [D loss: 0.547723, acc.: 71.88%] [G loss: 0.623432]\n",
      "epoch:20 step:19279 [D loss: 0.601939, acc.: 60.16%] [G loss: 0.498218]\n",
      "epoch:20 step:19280 [D loss: 0.563940, acc.: 67.97%] [G loss: 0.554819]\n",
      "epoch:20 step:19281 [D loss: 0.511141, acc.: 73.44%] [G loss: 0.734206]\n",
      "epoch:20 step:19282 [D loss: 0.590270, acc.: 71.88%] [G loss: 0.532513]\n",
      "epoch:20 step:19283 [D loss: 0.505361, acc.: 70.31%] [G loss: 0.772146]\n",
      "epoch:20 step:19284 [D loss: 0.571736, acc.: 67.97%] [G loss: 0.549100]\n",
      "epoch:20 step:19285 [D loss: 0.521464, acc.: 72.66%] [G loss: 0.698736]\n",
      "epoch:20 step:19286 [D loss: 0.528555, acc.: 74.22%] [G loss: 0.706384]\n",
      "epoch:20 step:19287 [D loss: 0.525573, acc.: 73.44%] [G loss: 0.708637]\n",
      "epoch:20 step:19288 [D loss: 0.542236, acc.: 70.31%] [G loss: 0.663646]\n",
      "epoch:20 step:19289 [D loss: 0.515216, acc.: 75.00%] [G loss: 0.620415]\n",
      "epoch:20 step:19290 [D loss: 0.539828, acc.: 72.66%] [G loss: 0.560402]\n",
      "epoch:20 step:19291 [D loss: 0.519676, acc.: 75.00%] [G loss: 0.577996]\n",
      "epoch:20 step:19292 [D loss: 0.533690, acc.: 76.56%] [G loss: 0.806738]\n",
      "epoch:20 step:19293 [D loss: 0.581883, acc.: 67.19%] [G loss: 0.521164]\n",
      "epoch:20 step:19294 [D loss: 0.482518, acc.: 75.00%] [G loss: 0.711190]\n",
      "epoch:20 step:19295 [D loss: 0.513138, acc.: 74.22%] [G loss: 0.684739]\n",
      "epoch:20 step:19296 [D loss: 0.583696, acc.: 62.50%] [G loss: 0.720011]\n",
      "epoch:20 step:19297 [D loss: 0.482668, acc.: 78.91%] [G loss: 0.797832]\n",
      "epoch:20 step:19298 [D loss: 0.555569, acc.: 67.19%] [G loss: 0.725352]\n",
      "epoch:20 step:19299 [D loss: 0.589870, acc.: 66.41%] [G loss: 0.822672]\n",
      "epoch:20 step:19300 [D loss: 0.562003, acc.: 68.75%] [G loss: 0.627342]\n",
      "epoch:20 step:19301 [D loss: 0.603141, acc.: 63.28%] [G loss: 0.630986]\n",
      "epoch:20 step:19302 [D loss: 0.508528, acc.: 75.78%] [G loss: 0.631155]\n",
      "epoch:20 step:19303 [D loss: 0.564468, acc.: 67.19%] [G loss: 0.624270]\n",
      "epoch:20 step:19304 [D loss: 0.524800, acc.: 73.44%] [G loss: 0.736689]\n",
      "epoch:20 step:19305 [D loss: 0.538622, acc.: 71.88%] [G loss: 0.619292]\n",
      "epoch:20 step:19306 [D loss: 0.666743, acc.: 64.06%] [G loss: 0.620014]\n",
      "epoch:20 step:19307 [D loss: 0.552460, acc.: 68.75%] [G loss: 0.594984]\n",
      "epoch:20 step:19308 [D loss: 0.509457, acc.: 73.44%] [G loss: 0.628123]\n",
      "epoch:20 step:19309 [D loss: 0.533705, acc.: 70.31%] [G loss: 0.631253]\n",
      "epoch:20 step:19310 [D loss: 0.486416, acc.: 77.34%] [G loss: 0.685769]\n",
      "epoch:20 step:19311 [D loss: 0.552076, acc.: 71.09%] [G loss: 0.648452]\n",
      "epoch:20 step:19312 [D loss: 0.576223, acc.: 65.62%] [G loss: 0.579606]\n",
      "epoch:20 step:19313 [D loss: 0.566556, acc.: 65.62%] [G loss: 0.585646]\n",
      "epoch:20 step:19314 [D loss: 0.486528, acc.: 76.56%] [G loss: 0.966496]\n",
      "epoch:20 step:19315 [D loss: 0.503086, acc.: 76.56%] [G loss: 0.775269]\n",
      "epoch:20 step:19316 [D loss: 0.676590, acc.: 57.81%] [G loss: 0.659261]\n",
      "epoch:20 step:19317 [D loss: 0.497412, acc.: 78.12%] [G loss: 0.751307]\n",
      "epoch:20 step:19318 [D loss: 0.540724, acc.: 71.88%] [G loss: 0.626244]\n",
      "epoch:20 step:19319 [D loss: 0.532675, acc.: 72.66%] [G loss: 0.713013]\n",
      "epoch:20 step:19320 [D loss: 0.628918, acc.: 61.72%] [G loss: 0.719460]\n",
      "epoch:20 step:19321 [D loss: 0.564759, acc.: 71.88%] [G loss: 0.626571]\n",
      "epoch:20 step:19322 [D loss: 0.489794, acc.: 75.78%] [G loss: 0.724094]\n",
      "epoch:20 step:19323 [D loss: 0.578422, acc.: 70.31%] [G loss: 0.739206]\n",
      "epoch:20 step:19324 [D loss: 0.636623, acc.: 64.06%] [G loss: 0.524551]\n",
      "epoch:20 step:19325 [D loss: 0.533136, acc.: 68.75%] [G loss: 0.537658]\n",
      "epoch:20 step:19326 [D loss: 0.572497, acc.: 71.88%] [G loss: 0.514304]\n",
      "epoch:20 step:19327 [D loss: 0.565695, acc.: 71.88%] [G loss: 0.554840]\n",
      "epoch:20 step:19328 [D loss: 0.595711, acc.: 71.09%] [G loss: 0.669264]\n",
      "epoch:20 step:19329 [D loss: 0.513410, acc.: 71.88%] [G loss: 0.594146]\n",
      "epoch:20 step:19330 [D loss: 0.569290, acc.: 66.41%] [G loss: 0.602747]\n",
      "epoch:20 step:19331 [D loss: 0.615254, acc.: 65.62%] [G loss: 0.672065]\n",
      "epoch:20 step:19332 [D loss: 0.515341, acc.: 72.66%] [G loss: 0.604143]\n",
      "epoch:20 step:19333 [D loss: 0.485135, acc.: 78.12%] [G loss: 0.752088]\n",
      "epoch:20 step:19334 [D loss: 0.599888, acc.: 67.97%] [G loss: 0.510349]\n",
      "epoch:20 step:19335 [D loss: 0.583676, acc.: 67.19%] [G loss: 0.661887]\n",
      "epoch:20 step:19336 [D loss: 0.481427, acc.: 76.56%] [G loss: 0.662215]\n",
      "epoch:20 step:19337 [D loss: 0.541036, acc.: 72.66%] [G loss: 0.559811]\n",
      "epoch:20 step:19338 [D loss: 0.542151, acc.: 72.66%] [G loss: 0.728523]\n",
      "epoch:20 step:19339 [D loss: 0.509995, acc.: 75.00%] [G loss: 0.718347]\n",
      "epoch:20 step:19340 [D loss: 0.622855, acc.: 64.06%] [G loss: 0.716908]\n",
      "epoch:20 step:19341 [D loss: 0.553025, acc.: 71.88%] [G loss: 0.758541]\n",
      "epoch:20 step:19342 [D loss: 0.556284, acc.: 71.09%] [G loss: 0.686332]\n",
      "epoch:20 step:19343 [D loss: 0.510594, acc.: 76.56%] [G loss: 0.658743]\n",
      "epoch:20 step:19344 [D loss: 0.540319, acc.: 71.88%] [G loss: 0.856027]\n",
      "epoch:20 step:19345 [D loss: 0.455020, acc.: 80.47%] [G loss: 0.691330]\n",
      "epoch:20 step:19346 [D loss: 0.584285, acc.: 69.53%] [G loss: 0.613734]\n",
      "epoch:20 step:19347 [D loss: 0.589090, acc.: 66.41%] [G loss: 0.509953]\n",
      "epoch:20 step:19348 [D loss: 0.574010, acc.: 68.75%] [G loss: 0.556781]\n",
      "epoch:20 step:19349 [D loss: 0.564042, acc.: 72.66%] [G loss: 0.487787]\n",
      "epoch:20 step:19350 [D loss: 0.591596, acc.: 65.62%] [G loss: 0.629784]\n",
      "epoch:20 step:19351 [D loss: 0.522288, acc.: 71.09%] [G loss: 0.454703]\n",
      "epoch:20 step:19352 [D loss: 0.557901, acc.: 64.84%] [G loss: 0.561348]\n",
      "epoch:20 step:19353 [D loss: 0.468953, acc.: 77.34%] [G loss: 0.529848]\n",
      "epoch:20 step:19354 [D loss: 0.628473, acc.: 57.81%] [G loss: 0.539342]\n",
      "epoch:20 step:19355 [D loss: 0.545796, acc.: 72.66%] [G loss: 0.775636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19356 [D loss: 0.536236, acc.: 74.22%] [G loss: 0.625057]\n",
      "epoch:20 step:19357 [D loss: 0.547520, acc.: 71.88%] [G loss: 0.779838]\n",
      "epoch:20 step:19358 [D loss: 0.603621, acc.: 64.84%] [G loss: 0.682287]\n",
      "epoch:20 step:19359 [D loss: 0.527918, acc.: 73.44%] [G loss: 0.845617]\n",
      "epoch:20 step:19360 [D loss: 0.481687, acc.: 72.66%] [G loss: 0.638965]\n",
      "epoch:20 step:19361 [D loss: 0.637534, acc.: 65.62%] [G loss: 0.698698]\n",
      "epoch:20 step:19362 [D loss: 0.617876, acc.: 69.53%] [G loss: 0.690548]\n",
      "epoch:20 step:19363 [D loss: 0.470400, acc.: 76.56%] [G loss: 0.584422]\n",
      "epoch:20 step:19364 [D loss: 0.476228, acc.: 78.12%] [G loss: 0.727361]\n",
      "epoch:20 step:19365 [D loss: 0.561745, acc.: 66.41%] [G loss: 0.679387]\n",
      "epoch:20 step:19366 [D loss: 0.526055, acc.: 73.44%] [G loss: 0.602260]\n",
      "epoch:20 step:19367 [D loss: 0.565150, acc.: 63.28%] [G loss: 0.559563]\n",
      "epoch:20 step:19368 [D loss: 0.596043, acc.: 66.41%] [G loss: 0.688528]\n",
      "epoch:20 step:19369 [D loss: 0.556722, acc.: 69.53%] [G loss: 0.667650]\n",
      "epoch:20 step:19370 [D loss: 0.551495, acc.: 68.75%] [G loss: 0.592514]\n",
      "epoch:20 step:19371 [D loss: 0.536796, acc.: 72.66%] [G loss: 0.574098]\n",
      "epoch:20 step:19372 [D loss: 0.502692, acc.: 73.44%] [G loss: 0.854594]\n",
      "epoch:20 step:19373 [D loss: 0.485111, acc.: 78.91%] [G loss: 0.782615]\n",
      "epoch:20 step:19374 [D loss: 0.526850, acc.: 71.88%] [G loss: 0.751452]\n",
      "epoch:20 step:19375 [D loss: 0.463307, acc.: 75.78%] [G loss: 0.845047]\n",
      "epoch:20 step:19376 [D loss: 0.576026, acc.: 67.97%] [G loss: 0.606441]\n",
      "epoch:20 step:19377 [D loss: 0.568053, acc.: 71.09%] [G loss: 0.636461]\n",
      "epoch:20 step:19378 [D loss: 0.519731, acc.: 68.75%] [G loss: 0.703236]\n",
      "epoch:20 step:19379 [D loss: 0.510784, acc.: 70.31%] [G loss: 0.665607]\n",
      "epoch:20 step:19380 [D loss: 0.590077, acc.: 67.19%] [G loss: 1.024916]\n",
      "epoch:20 step:19381 [D loss: 0.519316, acc.: 71.88%] [G loss: 0.745438]\n",
      "epoch:20 step:19382 [D loss: 0.532915, acc.: 74.22%] [G loss: 0.883136]\n",
      "epoch:20 step:19383 [D loss: 0.565873, acc.: 67.97%] [G loss: 0.846648]\n",
      "epoch:20 step:19384 [D loss: 0.540118, acc.: 70.31%] [G loss: 0.657517]\n",
      "epoch:20 step:19385 [D loss: 0.534143, acc.: 70.31%] [G loss: 0.589623]\n",
      "epoch:20 step:19386 [D loss: 0.495207, acc.: 76.56%] [G loss: 0.613115]\n",
      "epoch:20 step:19387 [D loss: 0.477670, acc.: 75.00%] [G loss: 0.781854]\n",
      "epoch:20 step:19388 [D loss: 0.404720, acc.: 81.25%] [G loss: 0.996787]\n",
      "epoch:20 step:19389 [D loss: 0.597813, acc.: 69.53%] [G loss: 0.759016]\n",
      "epoch:20 step:19390 [D loss: 0.484101, acc.: 75.78%] [G loss: 1.063830]\n",
      "epoch:20 step:19391 [D loss: 0.553131, acc.: 71.09%] [G loss: 0.775881]\n",
      "epoch:20 step:19392 [D loss: 0.651203, acc.: 65.62%] [G loss: 0.559636]\n",
      "epoch:20 step:19393 [D loss: 0.617637, acc.: 62.50%] [G loss: 0.544493]\n",
      "epoch:20 step:19394 [D loss: 0.505907, acc.: 68.75%] [G loss: 0.758777]\n",
      "epoch:20 step:19395 [D loss: 0.544397, acc.: 68.75%] [G loss: 0.552908]\n",
      "epoch:20 step:19396 [D loss: 0.546616, acc.: 69.53%] [G loss: 0.643577]\n",
      "epoch:20 step:19397 [D loss: 0.518557, acc.: 72.66%] [G loss: 0.607178]\n",
      "epoch:20 step:19398 [D loss: 0.596303, acc.: 64.84%] [G loss: 0.645797]\n",
      "epoch:20 step:19399 [D loss: 0.505683, acc.: 75.78%] [G loss: 0.672540]\n",
      "epoch:20 step:19400 [D loss: 0.600184, acc.: 63.28%] [G loss: 0.532048]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.577851\n",
      "FID: 52.079395\n",
      "0 = 12.832111825656884\n",
      "1 = 0.08864893679597151\n",
      "2 = 0.8672000169754028\n",
      "3 = 0.8144000172615051\n",
      "4 = 0.9200000166893005\n",
      "5 = 0.9105545878410339\n",
      "6 = 0.8144000172615051\n",
      "7 = 8.404589256334317\n",
      "8 = 0.1613631018899621\n",
      "9 = 0.7008000016212463\n",
      "10 = 0.6837999820709229\n",
      "11 = 0.7178000211715698\n",
      "12 = 0.7078675031661987\n",
      "13 = 0.6837999820709229\n",
      "14 = 6.577877044677734\n",
      "15 = 7.088438510894775\n",
      "16 = 0.383395791053772\n",
      "17 = 6.577851295471191\n",
      "18 = 52.07939529418945\n",
      "epoch:20 step:19401 [D loss: 0.486732, acc.: 74.22%] [G loss: 0.712318]\n",
      "epoch:20 step:19402 [D loss: 0.588059, acc.: 71.88%] [G loss: 0.721034]\n",
      "epoch:20 step:19403 [D loss: 0.545190, acc.: 70.31%] [G loss: 0.690779]\n",
      "epoch:20 step:19404 [D loss: 0.535657, acc.: 72.66%] [G loss: 0.649040]\n",
      "epoch:20 step:19405 [D loss: 0.606850, acc.: 58.59%] [G loss: 0.679120]\n",
      "epoch:20 step:19406 [D loss: 0.584161, acc.: 71.88%] [G loss: 0.739359]\n",
      "epoch:20 step:19407 [D loss: 0.587199, acc.: 67.97%] [G loss: 0.679942]\n",
      "epoch:20 step:19408 [D loss: 0.573720, acc.: 70.31%] [G loss: 0.612557]\n",
      "epoch:20 step:19409 [D loss: 0.546647, acc.: 67.97%] [G loss: 0.566602]\n",
      "epoch:20 step:19410 [D loss: 0.568592, acc.: 64.06%] [G loss: 0.498121]\n",
      "epoch:20 step:19411 [D loss: 0.608423, acc.: 63.28%] [G loss: 0.552353]\n",
      "epoch:20 step:19412 [D loss: 0.540928, acc.: 72.66%] [G loss: 0.598908]\n",
      "epoch:20 step:19413 [D loss: 0.530569, acc.: 68.75%] [G loss: 0.520346]\n",
      "epoch:20 step:19414 [D loss: 0.520049, acc.: 70.31%] [G loss: 0.644763]\n",
      "epoch:20 step:19415 [D loss: 0.596366, acc.: 70.31%] [G loss: 0.705066]\n",
      "epoch:20 step:19416 [D loss: 0.517471, acc.: 74.22%] [G loss: 0.638592]\n",
      "epoch:20 step:19417 [D loss: 0.542976, acc.: 71.88%] [G loss: 0.614327]\n",
      "epoch:20 step:19418 [D loss: 0.593567, acc.: 67.19%] [G loss: 0.787490]\n",
      "epoch:20 step:19419 [D loss: 0.560097, acc.: 68.75%] [G loss: 0.602978]\n",
      "epoch:20 step:19420 [D loss: 0.538075, acc.: 74.22%] [G loss: 0.606019]\n",
      "epoch:20 step:19421 [D loss: 0.518549, acc.: 73.44%] [G loss: 0.682196]\n",
      "epoch:20 step:19422 [D loss: 0.522852, acc.: 74.22%] [G loss: 0.674504]\n",
      "epoch:20 step:19423 [D loss: 0.541741, acc.: 74.22%] [G loss: 0.601401]\n",
      "epoch:20 step:19424 [D loss: 0.604245, acc.: 67.19%] [G loss: 0.485464]\n",
      "epoch:20 step:19425 [D loss: 0.528760, acc.: 71.88%] [G loss: 0.652055]\n",
      "epoch:20 step:19426 [D loss: 0.600434, acc.: 64.84%] [G loss: 0.602680]\n",
      "epoch:20 step:19427 [D loss: 0.559124, acc.: 65.62%] [G loss: 0.577161]\n",
      "epoch:20 step:19428 [D loss: 0.530440, acc.: 67.19%] [G loss: 0.633923]\n",
      "epoch:20 step:19429 [D loss: 0.552049, acc.: 69.53%] [G loss: 0.676150]\n",
      "epoch:20 step:19430 [D loss: 0.499784, acc.: 78.91%] [G loss: 0.699095]\n",
      "epoch:20 step:19431 [D loss: 0.492896, acc.: 79.69%] [G loss: 0.675678]\n",
      "epoch:20 step:19432 [D loss: 0.538652, acc.: 72.66%] [G loss: 0.726726]\n",
      "epoch:20 step:19433 [D loss: 0.473397, acc.: 79.69%] [G loss: 0.663223]\n",
      "epoch:20 step:19434 [D loss: 0.490181, acc.: 75.00%] [G loss: 0.733631]\n",
      "epoch:20 step:19435 [D loss: 0.528158, acc.: 75.00%] [G loss: 0.665409]\n",
      "epoch:20 step:19436 [D loss: 0.652759, acc.: 59.38%] [G loss: 0.545579]\n",
      "epoch:20 step:19437 [D loss: 0.542552, acc.: 67.97%] [G loss: 0.537699]\n",
      "epoch:20 step:19438 [D loss: 0.575558, acc.: 64.06%] [G loss: 0.500639]\n",
      "epoch:20 step:19439 [D loss: 0.527476, acc.: 69.53%] [G loss: 0.543664]\n",
      "epoch:20 step:19440 [D loss: 0.581106, acc.: 64.84%] [G loss: 0.547234]\n",
      "epoch:20 step:19441 [D loss: 0.524738, acc.: 71.09%] [G loss: 0.800250]\n",
      "epoch:20 step:19442 [D loss: 0.583220, acc.: 65.62%] [G loss: 0.657964]\n",
      "epoch:20 step:19443 [D loss: 0.623733, acc.: 59.38%] [G loss: 0.743836]\n",
      "epoch:20 step:19444 [D loss: 0.613566, acc.: 64.84%] [G loss: 0.558498]\n",
      "epoch:20 step:19445 [D loss: 0.531182, acc.: 71.88%] [G loss: 0.668866]\n",
      "epoch:20 step:19446 [D loss: 0.555855, acc.: 69.53%] [G loss: 0.657947]\n",
      "epoch:20 step:19447 [D loss: 0.521642, acc.: 72.66%] [G loss: 0.523829]\n",
      "epoch:20 step:19448 [D loss: 0.505212, acc.: 75.78%] [G loss: 0.745184]\n",
      "epoch:20 step:19449 [D loss: 0.555606, acc.: 65.62%] [G loss: 0.632653]\n",
      "epoch:20 step:19450 [D loss: 0.608776, acc.: 64.06%] [G loss: 0.596862]\n",
      "epoch:20 step:19451 [D loss: 0.572056, acc.: 66.41%] [G loss: 0.803910]\n",
      "epoch:20 step:19452 [D loss: 0.563475, acc.: 67.19%] [G loss: 0.645518]\n",
      "epoch:20 step:19453 [D loss: 0.612329, acc.: 64.06%] [G loss: 0.642204]\n",
      "epoch:20 step:19454 [D loss: 0.558337, acc.: 68.75%] [G loss: 0.766744]\n",
      "epoch:20 step:19455 [D loss: 0.524543, acc.: 74.22%] [G loss: 0.552424]\n",
      "epoch:20 step:19456 [D loss: 0.629798, acc.: 62.50%] [G loss: 0.514114]\n",
      "epoch:20 step:19457 [D loss: 0.551986, acc.: 71.09%] [G loss: 0.546049]\n",
      "epoch:20 step:19458 [D loss: 0.583055, acc.: 65.62%] [G loss: 0.598137]\n",
      "epoch:20 step:19459 [D loss: 0.441641, acc.: 81.25%] [G loss: 0.795011]\n",
      "epoch:20 step:19460 [D loss: 0.595628, acc.: 69.53%] [G loss: 0.587123]\n",
      "epoch:20 step:19461 [D loss: 0.553171, acc.: 67.19%] [G loss: 0.612700]\n",
      "epoch:20 step:19462 [D loss: 0.560750, acc.: 71.09%] [G loss: 0.687361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19463 [D loss: 0.575063, acc.: 64.06%] [G loss: 0.512115]\n",
      "epoch:20 step:19464 [D loss: 0.510146, acc.: 75.78%] [G loss: 0.568352]\n",
      "epoch:20 step:19465 [D loss: 0.572304, acc.: 73.44%] [G loss: 0.720132]\n",
      "epoch:20 step:19466 [D loss: 0.549220, acc.: 76.56%] [G loss: 0.565235]\n",
      "epoch:20 step:19467 [D loss: 0.537118, acc.: 71.09%] [G loss: 0.731996]\n",
      "epoch:20 step:19468 [D loss: 0.548040, acc.: 71.88%] [G loss: 0.720166]\n",
      "epoch:20 step:19469 [D loss: 0.577884, acc.: 66.41%] [G loss: 0.511444]\n",
      "epoch:20 step:19470 [D loss: 0.481954, acc.: 78.91%] [G loss: 0.770799]\n",
      "epoch:20 step:19471 [D loss: 0.646067, acc.: 57.03%] [G loss: 0.561498]\n",
      "epoch:20 step:19472 [D loss: 0.577926, acc.: 63.28%] [G loss: 0.719749]\n",
      "epoch:20 step:19473 [D loss: 0.498709, acc.: 79.69%] [G loss: 0.701138]\n",
      "epoch:20 step:19474 [D loss: 0.578072, acc.: 64.84%] [G loss: 0.534759]\n",
      "epoch:20 step:19475 [D loss: 0.548883, acc.: 67.19%] [G loss: 0.674031]\n",
      "epoch:20 step:19476 [D loss: 0.489982, acc.: 69.53%] [G loss: 0.724235]\n",
      "epoch:20 step:19477 [D loss: 0.566448, acc.: 69.53%] [G loss: 0.689479]\n",
      "epoch:20 step:19478 [D loss: 0.571416, acc.: 64.84%] [G loss: 0.676942]\n",
      "epoch:20 step:19479 [D loss: 0.601452, acc.: 65.62%] [G loss: 0.528041]\n",
      "epoch:20 step:19480 [D loss: 0.633530, acc.: 64.84%] [G loss: 0.463133]\n",
      "epoch:20 step:19481 [D loss: 0.553759, acc.: 70.31%] [G loss: 0.504184]\n",
      "epoch:20 step:19482 [D loss: 0.553951, acc.: 68.75%] [G loss: 0.668338]\n",
      "epoch:20 step:19483 [D loss: 0.509104, acc.: 75.00%] [G loss: 0.602529]\n",
      "epoch:20 step:19484 [D loss: 0.519343, acc.: 75.78%] [G loss: 0.719853]\n",
      "epoch:20 step:19485 [D loss: 0.561044, acc.: 68.75%] [G loss: 0.778235]\n",
      "epoch:20 step:19486 [D loss: 0.462602, acc.: 81.25%] [G loss: 0.714974]\n",
      "epoch:20 step:19487 [D loss: 0.489262, acc.: 74.22%] [G loss: 0.733758]\n",
      "epoch:20 step:19488 [D loss: 0.519343, acc.: 75.00%] [G loss: 0.588524]\n",
      "epoch:20 step:19489 [D loss: 0.496716, acc.: 74.22%] [G loss: 0.685823]\n",
      "epoch:20 step:19490 [D loss: 0.545533, acc.: 69.53%] [G loss: 0.883459]\n",
      "epoch:20 step:19491 [D loss: 0.500893, acc.: 71.88%] [G loss: 0.791194]\n",
      "epoch:20 step:19492 [D loss: 0.594086, acc.: 67.19%] [G loss: 0.668578]\n",
      "epoch:20 step:19493 [D loss: 0.526602, acc.: 72.66%] [G loss: 0.703904]\n",
      "epoch:20 step:19494 [D loss: 0.572479, acc.: 66.41%] [G loss: 0.700813]\n",
      "epoch:20 step:19495 [D loss: 0.522542, acc.: 71.88%] [G loss: 0.621416]\n",
      "epoch:20 step:19496 [D loss: 0.538224, acc.: 71.09%] [G loss: 0.604195]\n",
      "epoch:20 step:19497 [D loss: 0.539096, acc.: 73.44%] [G loss: 0.546667]\n",
      "epoch:20 step:19498 [D loss: 0.494392, acc.: 78.12%] [G loss: 0.750611]\n",
      "epoch:20 step:19499 [D loss: 0.558363, acc.: 69.53%] [G loss: 0.546091]\n",
      "epoch:20 step:19500 [D loss: 0.510785, acc.: 71.88%] [G loss: 0.714656]\n",
      "epoch:20 step:19501 [D loss: 0.549772, acc.: 72.66%] [G loss: 0.522733]\n",
      "epoch:20 step:19502 [D loss: 0.635704, acc.: 62.50%] [G loss: 0.486716]\n",
      "epoch:20 step:19503 [D loss: 0.532370, acc.: 70.31%] [G loss: 0.497123]\n",
      "epoch:20 step:19504 [D loss: 0.599636, acc.: 64.84%] [G loss: 0.508696]\n",
      "epoch:20 step:19505 [D loss: 0.559698, acc.: 73.44%] [G loss: 0.545201]\n",
      "epoch:20 step:19506 [D loss: 0.627452, acc.: 64.84%] [G loss: 0.653648]\n",
      "epoch:20 step:19507 [D loss: 0.482713, acc.: 77.34%] [G loss: 0.695974]\n",
      "epoch:20 step:19508 [D loss: 0.566917, acc.: 69.53%] [G loss: 0.869368]\n",
      "epoch:20 step:19509 [D loss: 0.490170, acc.: 78.12%] [G loss: 0.737332]\n",
      "epoch:20 step:19510 [D loss: 0.484136, acc.: 74.22%] [G loss: 0.857443]\n",
      "epoch:20 step:19511 [D loss: 0.516014, acc.: 75.00%] [G loss: 0.759358]\n",
      "epoch:20 step:19512 [D loss: 0.550775, acc.: 67.97%] [G loss: 0.611690]\n",
      "epoch:20 step:19513 [D loss: 0.517135, acc.: 74.22%] [G loss: 0.663584]\n",
      "epoch:20 step:19514 [D loss: 0.491000, acc.: 73.44%] [G loss: 0.633882]\n",
      "epoch:20 step:19515 [D loss: 0.533471, acc.: 71.09%] [G loss: 0.629654]\n",
      "epoch:20 step:19516 [D loss: 0.562657, acc.: 74.22%] [G loss: 0.713774]\n",
      "epoch:20 step:19517 [D loss: 0.589823, acc.: 63.28%] [G loss: 0.595258]\n",
      "epoch:20 step:19518 [D loss: 0.551675, acc.: 67.97%] [G loss: 0.609850]\n",
      "epoch:20 step:19519 [D loss: 0.539944, acc.: 74.22%] [G loss: 0.635181]\n",
      "epoch:20 step:19520 [D loss: 0.498945, acc.: 71.88%] [G loss: 0.486298]\n",
      "epoch:20 step:19521 [D loss: 0.515937, acc.: 75.78%] [G loss: 0.623420]\n",
      "epoch:20 step:19522 [D loss: 0.550890, acc.: 68.75%] [G loss: 0.636974]\n",
      "epoch:20 step:19523 [D loss: 0.591906, acc.: 71.88%] [G loss: 0.742077]\n",
      "epoch:20 step:19524 [D loss: 0.572374, acc.: 65.62%] [G loss: 0.779664]\n",
      "epoch:20 step:19525 [D loss: 0.536803, acc.: 70.31%] [G loss: 0.754171]\n",
      "epoch:20 step:19526 [D loss: 0.544730, acc.: 65.62%] [G loss: 0.601717]\n",
      "epoch:20 step:19527 [D loss: 0.602637, acc.: 65.62%] [G loss: 0.548270]\n",
      "epoch:20 step:19528 [D loss: 0.601153, acc.: 66.41%] [G loss: 0.575167]\n",
      "epoch:20 step:19529 [D loss: 0.564367, acc.: 65.62%] [G loss: 0.620633]\n",
      "epoch:20 step:19530 [D loss: 0.551811, acc.: 69.53%] [G loss: 0.609102]\n",
      "epoch:20 step:19531 [D loss: 0.575354, acc.: 71.09%] [G loss: 0.506220]\n",
      "epoch:20 step:19532 [D loss: 0.445883, acc.: 76.56%] [G loss: 0.858606]\n",
      "epoch:20 step:19533 [D loss: 0.625130, acc.: 68.75%] [G loss: 0.623587]\n",
      "epoch:20 step:19534 [D loss: 0.656546, acc.: 61.72%] [G loss: 0.551323]\n",
      "epoch:20 step:19535 [D loss: 0.543750, acc.: 74.22%] [G loss: 0.575822]\n",
      "epoch:20 step:19536 [D loss: 0.470540, acc.: 75.78%] [G loss: 0.918600]\n",
      "epoch:20 step:19537 [D loss: 0.557797, acc.: 69.53%] [G loss: 0.608632]\n",
      "epoch:20 step:19538 [D loss: 0.528929, acc.: 69.53%] [G loss: 0.793684]\n",
      "epoch:20 step:19539 [D loss: 0.591989, acc.: 70.31%] [G loss: 0.520021]\n",
      "epoch:20 step:19540 [D loss: 0.580740, acc.: 64.84%] [G loss: 0.706092]\n",
      "epoch:20 step:19541 [D loss: 0.518664, acc.: 74.22%] [G loss: 0.678806]\n",
      "epoch:20 step:19542 [D loss: 0.450849, acc.: 80.47%] [G loss: 0.855115]\n",
      "epoch:20 step:19543 [D loss: 0.586763, acc.: 71.09%] [G loss: 0.864545]\n",
      "epoch:20 step:19544 [D loss: 0.618400, acc.: 64.06%] [G loss: 0.553552]\n",
      "epoch:20 step:19545 [D loss: 0.564814, acc.: 69.53%] [G loss: 0.546563]\n",
      "epoch:20 step:19546 [D loss: 0.574955, acc.: 64.84%] [G loss: 0.702895]\n",
      "epoch:20 step:19547 [D loss: 0.533699, acc.: 67.19%] [G loss: 0.685383]\n",
      "epoch:20 step:19548 [D loss: 0.578196, acc.: 70.31%] [G loss: 0.787255]\n",
      "epoch:20 step:19549 [D loss: 0.523524, acc.: 71.88%] [G loss: 0.653903]\n",
      "epoch:20 step:19550 [D loss: 0.537612, acc.: 71.09%] [G loss: 0.511368]\n",
      "epoch:20 step:19551 [D loss: 0.568995, acc.: 64.84%] [G loss: 0.506374]\n",
      "epoch:20 step:19552 [D loss: 0.623910, acc.: 63.28%] [G loss: 0.461668]\n",
      "epoch:20 step:19553 [D loss: 0.569870, acc.: 64.84%] [G loss: 0.436873]\n",
      "epoch:20 step:19554 [D loss: 0.463430, acc.: 73.44%] [G loss: 0.729133]\n",
      "epoch:20 step:19555 [D loss: 0.467029, acc.: 79.69%] [G loss: 0.874159]\n",
      "epoch:20 step:19556 [D loss: 0.518959, acc.: 71.09%] [G loss: 0.832718]\n",
      "epoch:20 step:19557 [D loss: 0.676184, acc.: 63.28%] [G loss: 0.681115]\n",
      "epoch:20 step:19558 [D loss: 0.590652, acc.: 65.62%] [G loss: 0.614274]\n",
      "epoch:20 step:19559 [D loss: 0.489892, acc.: 75.78%] [G loss: 0.624814]\n",
      "epoch:20 step:19560 [D loss: 0.587857, acc.: 65.62%] [G loss: 0.596964]\n",
      "epoch:20 step:19561 [D loss: 0.536074, acc.: 72.66%] [G loss: 0.718386]\n",
      "epoch:20 step:19562 [D loss: 0.538879, acc.: 67.97%] [G loss: 0.645465]\n",
      "epoch:20 step:19563 [D loss: 0.490813, acc.: 72.66%] [G loss: 0.664797]\n",
      "epoch:20 step:19564 [D loss: 0.564933, acc.: 68.75%] [G loss: 0.691467]\n",
      "epoch:20 step:19565 [D loss: 0.524269, acc.: 75.00%] [G loss: 0.612030]\n",
      "epoch:20 step:19566 [D loss: 0.542139, acc.: 71.09%] [G loss: 0.553119]\n",
      "epoch:20 step:19567 [D loss: 0.622680, acc.: 64.06%] [G loss: 0.494136]\n",
      "epoch:20 step:19568 [D loss: 0.616114, acc.: 58.59%] [G loss: 0.612994]\n",
      "epoch:20 step:19569 [D loss: 0.551620, acc.: 68.75%] [G loss: 0.621075]\n",
      "epoch:20 step:19570 [D loss: 0.588318, acc.: 64.84%] [G loss: 0.600109]\n",
      "epoch:20 step:19571 [D loss: 0.576772, acc.: 64.84%] [G loss: 0.681800]\n",
      "epoch:20 step:19572 [D loss: 0.531744, acc.: 71.09%] [G loss: 0.650523]\n",
      "epoch:20 step:19573 [D loss: 0.501595, acc.: 76.56%] [G loss: 0.657539]\n",
      "epoch:20 step:19574 [D loss: 0.496775, acc.: 78.12%] [G loss: 0.560909]\n",
      "epoch:20 step:19575 [D loss: 0.500976, acc.: 72.66%] [G loss: 0.524221]\n",
      "epoch:20 step:19576 [D loss: 0.548382, acc.: 69.53%] [G loss: 0.570964]\n",
      "epoch:20 step:19577 [D loss: 0.517305, acc.: 75.00%] [G loss: 0.485377]\n",
      "epoch:20 step:19578 [D loss: 0.542485, acc.: 69.53%] [G loss: 0.572844]\n",
      "epoch:20 step:19579 [D loss: 0.521986, acc.: 71.09%] [G loss: 0.604081]\n",
      "epoch:20 step:19580 [D loss: 0.584868, acc.: 67.19%] [G loss: 0.538145]\n",
      "epoch:20 step:19581 [D loss: 0.522088, acc.: 68.75%] [G loss: 0.552539]\n",
      "epoch:20 step:19582 [D loss: 0.493960, acc.: 75.78%] [G loss: 0.659320]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19583 [D loss: 0.554170, acc.: 68.75%] [G loss: 0.572414]\n",
      "epoch:20 step:19584 [D loss: 0.538772, acc.: 71.88%] [G loss: 0.709345]\n",
      "epoch:20 step:19585 [D loss: 0.597994, acc.: 67.19%] [G loss: 0.664865]\n",
      "epoch:20 step:19586 [D loss: 0.627337, acc.: 63.28%] [G loss: 0.546911]\n",
      "epoch:20 step:19587 [D loss: 0.585169, acc.: 70.31%] [G loss: 0.569435]\n",
      "epoch:20 step:19588 [D loss: 0.565382, acc.: 64.84%] [G loss: 0.471014]\n",
      "epoch:20 step:19589 [D loss: 0.572404, acc.: 64.06%] [G loss: 0.475700]\n",
      "epoch:20 step:19590 [D loss: 0.589009, acc.: 67.97%] [G loss: 0.517745]\n",
      "epoch:20 step:19591 [D loss: 0.580123, acc.: 65.62%] [G loss: 0.688206]\n",
      "epoch:20 step:19592 [D loss: 0.600151, acc.: 67.97%] [G loss: 0.637672]\n",
      "epoch:20 step:19593 [D loss: 0.530625, acc.: 72.66%] [G loss: 0.647048]\n",
      "epoch:20 step:19594 [D loss: 0.527693, acc.: 71.09%] [G loss: 0.518661]\n",
      "epoch:20 step:19595 [D loss: 0.608495, acc.: 60.94%] [G loss: 0.571174]\n",
      "epoch:20 step:19596 [D loss: 0.582779, acc.: 65.62%] [G loss: 0.578185]\n",
      "epoch:20 step:19597 [D loss: 0.483608, acc.: 78.12%] [G loss: 0.508049]\n",
      "epoch:20 step:19598 [D loss: 0.629852, acc.: 67.97%] [G loss: 0.596816]\n",
      "epoch:20 step:19599 [D loss: 0.521470, acc.: 71.09%] [G loss: 0.567839]\n",
      "epoch:20 step:19600 [D loss: 0.452231, acc.: 79.69%] [G loss: 0.649490]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.796192\n",
      "FID: 45.483013\n",
      "0 = 12.750176969623576\n",
      "1 = 0.08586117215831271\n",
      "2 = 0.8586999773979187\n",
      "3 = 0.803600013256073\n",
      "4 = 0.9138000011444092\n",
      "5 = 0.9031242728233337\n",
      "6 = 0.803600013256073\n",
      "7 = 8.047891773772237\n",
      "8 = 0.1434932239806784\n",
      "9 = 0.691100001335144\n",
      "10 = 0.6808000206947327\n",
      "11 = 0.7013999819755554\n",
      "12 = 0.6951194405555725\n",
      "13 = 0.6808000206947327\n",
      "14 = 6.796217441558838\n",
      "15 = 7.014798641204834\n",
      "16 = 0.3784785568714142\n",
      "17 = 6.796192169189453\n",
      "18 = 45.48301315307617\n",
      "epoch:20 step:19601 [D loss: 0.595298, acc.: 60.16%] [G loss: 0.498513]\n",
      "epoch:20 step:19602 [D loss: 0.534215, acc.: 70.31%] [G loss: 0.562527]\n",
      "epoch:20 step:19603 [D loss: 0.564100, acc.: 65.62%] [G loss: 0.475011]\n",
      "epoch:20 step:19604 [D loss: 0.523620, acc.: 69.53%] [G loss: 0.485990]\n",
      "epoch:20 step:19605 [D loss: 0.592269, acc.: 67.19%] [G loss: 0.558787]\n",
      "epoch:20 step:19606 [D loss: 0.565112, acc.: 72.66%] [G loss: 0.523164]\n",
      "epoch:20 step:19607 [D loss: 0.702883, acc.: 54.69%] [G loss: 0.487384]\n",
      "epoch:20 step:19608 [D loss: 0.531101, acc.: 71.88%] [G loss: 0.464627]\n",
      "epoch:20 step:19609 [D loss: 0.572135, acc.: 71.09%] [G loss: 0.488915]\n",
      "epoch:20 step:19610 [D loss: 0.502419, acc.: 75.78%] [G loss: 0.587763]\n",
      "epoch:20 step:19611 [D loss: 0.489799, acc.: 76.56%] [G loss: 0.605315]\n",
      "epoch:20 step:19612 [D loss: 0.545298, acc.: 67.97%] [G loss: 0.692893]\n",
      "epoch:20 step:19613 [D loss: 0.608958, acc.: 73.44%] [G loss: 0.632186]\n",
      "epoch:20 step:19614 [D loss: 0.557850, acc.: 70.31%] [G loss: 0.576472]\n",
      "epoch:20 step:19615 [D loss: 0.569076, acc.: 67.97%] [G loss: 0.609756]\n",
      "epoch:20 step:19616 [D loss: 0.561346, acc.: 67.97%] [G loss: 0.507855]\n",
      "epoch:20 step:19617 [D loss: 0.595488, acc.: 61.72%] [G loss: 0.429467]\n",
      "epoch:20 step:19618 [D loss: 0.554680, acc.: 68.75%] [G loss: 0.462704]\n",
      "epoch:20 step:19619 [D loss: 0.575316, acc.: 70.31%] [G loss: 0.591011]\n",
      "epoch:20 step:19620 [D loss: 0.634400, acc.: 60.94%] [G loss: 0.452189]\n",
      "epoch:20 step:19621 [D loss: 0.565656, acc.: 67.97%] [G loss: 0.412039]\n",
      "epoch:20 step:19622 [D loss: 0.562239, acc.: 68.75%] [G loss: 0.449689]\n",
      "epoch:20 step:19623 [D loss: 0.632161, acc.: 62.50%] [G loss: 0.464838]\n",
      "epoch:20 step:19624 [D loss: 0.526634, acc.: 75.00%] [G loss: 0.544159]\n",
      "epoch:20 step:19625 [D loss: 0.501773, acc.: 71.88%] [G loss: 0.731302]\n",
      "epoch:20 step:19626 [D loss: 0.549063, acc.: 72.66%] [G loss: 0.702909]\n",
      "epoch:20 step:19627 [D loss: 0.518833, acc.: 72.66%] [G loss: 0.624572]\n",
      "epoch:20 step:19628 [D loss: 0.538007, acc.: 68.75%] [G loss: 0.628029]\n",
      "epoch:20 step:19629 [D loss: 0.534680, acc.: 66.41%] [G loss: 0.645263]\n",
      "epoch:20 step:19630 [D loss: 0.454668, acc.: 82.81%] [G loss: 0.715539]\n",
      "epoch:20 step:19631 [D loss: 0.547905, acc.: 67.97%] [G loss: 0.658607]\n",
      "epoch:20 step:19632 [D loss: 0.720361, acc.: 52.34%] [G loss: 0.439567]\n",
      "epoch:20 step:19633 [D loss: 0.548563, acc.: 69.53%] [G loss: 0.560282]\n",
      "epoch:20 step:19634 [D loss: 0.419989, acc.: 80.47%] [G loss: 0.888444]\n",
      "epoch:20 step:19635 [D loss: 0.540355, acc.: 70.31%] [G loss: 0.682530]\n",
      "epoch:20 step:19636 [D loss: 0.487107, acc.: 78.12%] [G loss: 0.746412]\n",
      "epoch:20 step:19637 [D loss: 0.523665, acc.: 71.88%] [G loss: 0.603671]\n",
      "epoch:20 step:19638 [D loss: 0.467446, acc.: 78.12%] [G loss: 0.697493]\n",
      "epoch:20 step:19639 [D loss: 0.515798, acc.: 76.56%] [G loss: 0.739388]\n",
      "epoch:20 step:19640 [D loss: 0.496011, acc.: 76.56%] [G loss: 0.806375]\n",
      "epoch:20 step:19641 [D loss: 0.540281, acc.: 75.00%] [G loss: 0.643158]\n",
      "epoch:20 step:19642 [D loss: 0.538421, acc.: 72.66%] [G loss: 0.590662]\n",
      "epoch:20 step:19643 [D loss: 0.530802, acc.: 76.56%] [G loss: 0.678504]\n",
      "epoch:20 step:19644 [D loss: 0.602008, acc.: 70.31%] [G loss: 0.585896]\n",
      "epoch:20 step:19645 [D loss: 0.545634, acc.: 71.88%] [G loss: 0.603280]\n",
      "epoch:20 step:19646 [D loss: 0.509657, acc.: 73.44%] [G loss: 0.761133]\n",
      "epoch:20 step:19647 [D loss: 0.572085, acc.: 65.62%] [G loss: 0.723149]\n",
      "epoch:20 step:19648 [D loss: 0.527365, acc.: 73.44%] [G loss: 0.677734]\n",
      "epoch:20 step:19649 [D loss: 0.534481, acc.: 71.09%] [G loss: 0.670375]\n",
      "epoch:20 step:19650 [D loss: 0.529019, acc.: 73.44%] [G loss: 0.643848]\n",
      "epoch:20 step:19651 [D loss: 0.534774, acc.: 70.31%] [G loss: 0.780994]\n",
      "epoch:20 step:19652 [D loss: 0.468372, acc.: 76.56%] [G loss: 0.736375]\n",
      "epoch:20 step:19653 [D loss: 0.558292, acc.: 70.31%] [G loss: 0.977905]\n",
      "epoch:20 step:19654 [D loss: 0.541894, acc.: 72.66%] [G loss: 0.741951]\n",
      "epoch:20 step:19655 [D loss: 0.694064, acc.: 57.81%] [G loss: 0.639441]\n",
      "epoch:20 step:19656 [D loss: 0.533512, acc.: 74.22%] [G loss: 0.683199]\n",
      "epoch:20 step:19657 [D loss: 0.635827, acc.: 55.47%] [G loss: 0.600542]\n",
      "epoch:20 step:19658 [D loss: 0.493717, acc.: 75.78%] [G loss: 0.736443]\n",
      "epoch:20 step:19659 [D loss: 0.431417, acc.: 80.47%] [G loss: 0.931701]\n",
      "epoch:20 step:19660 [D loss: 0.695517, acc.: 57.81%] [G loss: 0.650021]\n",
      "epoch:20 step:19661 [D loss: 0.523393, acc.: 70.31%] [G loss: 0.753252]\n",
      "epoch:20 step:19662 [D loss: 0.527261, acc.: 72.66%] [G loss: 0.578389]\n",
      "epoch:20 step:19663 [D loss: 0.471934, acc.: 71.88%] [G loss: 0.603365]\n",
      "epoch:20 step:19664 [D loss: 0.478190, acc.: 75.00%] [G loss: 0.791574]\n",
      "epoch:20 step:19665 [D loss: 0.439834, acc.: 80.47%] [G loss: 0.920074]\n",
      "epoch:20 step:19666 [D loss: 0.443670, acc.: 79.69%] [G loss: 1.181575]\n",
      "epoch:20 step:19667 [D loss: 0.510417, acc.: 72.66%] [G loss: 1.190014]\n",
      "epoch:20 step:19668 [D loss: 0.703261, acc.: 65.62%] [G loss: 1.261750]\n",
      "epoch:20 step:19669 [D loss: 0.604066, acc.: 72.66%] [G loss: 1.124093]\n",
      "epoch:20 step:19670 [D loss: 0.430838, acc.: 76.56%] [G loss: 1.246611]\n",
      "epoch:20 step:19671 [D loss: 0.579329, acc.: 67.19%] [G loss: 0.902500]\n",
      "epoch:20 step:19672 [D loss: 0.656732, acc.: 64.06%] [G loss: 0.723831]\n",
      "epoch:20 step:19673 [D loss: 0.525350, acc.: 74.22%] [G loss: 0.836882]\n",
      "epoch:20 step:19674 [D loss: 0.535894, acc.: 68.75%] [G loss: 0.868387]\n",
      "epoch:20 step:19675 [D loss: 0.504477, acc.: 75.00%] [G loss: 0.839310]\n",
      "epoch:20 step:19676 [D loss: 0.436910, acc.: 77.34%] [G loss: 1.020813]\n",
      "epoch:20 step:19677 [D loss: 0.449280, acc.: 81.25%] [G loss: 1.402646]\n",
      "epoch:21 step:19678 [D loss: 0.614248, acc.: 69.53%] [G loss: 1.321059]\n",
      "epoch:21 step:19679 [D loss: 0.552074, acc.: 71.88%] [G loss: 0.997172]\n",
      "epoch:21 step:19680 [D loss: 0.540889, acc.: 73.44%] [G loss: 1.004158]\n",
      "epoch:21 step:19681 [D loss: 0.469179, acc.: 78.12%] [G loss: 0.863808]\n",
      "epoch:21 step:19682 [D loss: 0.618890, acc.: 68.75%] [G loss: 0.758024]\n",
      "epoch:21 step:19683 [D loss: 0.613623, acc.: 71.09%] [G loss: 0.682613]\n",
      "epoch:21 step:19684 [D loss: 0.528381, acc.: 73.44%] [G loss: 0.779857]\n",
      "epoch:21 step:19685 [D loss: 0.499976, acc.: 74.22%] [G loss: 0.916226]\n",
      "epoch:21 step:19686 [D loss: 0.514780, acc.: 72.66%] [G loss: 0.924200]\n",
      "epoch:21 step:19687 [D loss: 0.608871, acc.: 71.09%] [G loss: 0.872431]\n",
      "epoch:21 step:19688 [D loss: 0.508188, acc.: 75.78%] [G loss: 0.746185]\n",
      "epoch:21 step:19689 [D loss: 0.561795, acc.: 67.97%] [G loss: 0.647487]\n",
      "epoch:21 step:19690 [D loss: 0.566426, acc.: 69.53%] [G loss: 0.625342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19691 [D loss: 0.551149, acc.: 72.66%] [G loss: 0.638792]\n",
      "epoch:21 step:19692 [D loss: 0.455026, acc.: 80.47%] [G loss: 0.783775]\n",
      "epoch:21 step:19693 [D loss: 0.468286, acc.: 75.00%] [G loss: 0.753479]\n",
      "epoch:21 step:19694 [D loss: 0.561397, acc.: 75.78%] [G loss: 0.732359]\n",
      "epoch:21 step:19695 [D loss: 0.526612, acc.: 73.44%] [G loss: 0.657858]\n",
      "epoch:21 step:19696 [D loss: 0.537215, acc.: 75.78%] [G loss: 0.765593]\n",
      "epoch:21 step:19697 [D loss: 0.681454, acc.: 60.16%] [G loss: 0.748588]\n",
      "epoch:21 step:19698 [D loss: 0.613300, acc.: 64.84%] [G loss: 0.790655]\n",
      "epoch:21 step:19699 [D loss: 0.516229, acc.: 73.44%] [G loss: 1.008303]\n",
      "epoch:21 step:19700 [D loss: 0.613895, acc.: 65.62%] [G loss: 0.757668]\n",
      "epoch:21 step:19701 [D loss: 0.515112, acc.: 74.22%] [G loss: 0.694611]\n",
      "epoch:21 step:19702 [D loss: 0.482586, acc.: 78.12%] [G loss: 0.669964]\n",
      "epoch:21 step:19703 [D loss: 0.579664, acc.: 64.06%] [G loss: 0.699863]\n",
      "epoch:21 step:19704 [D loss: 0.480639, acc.: 75.78%] [G loss: 0.649432]\n",
      "epoch:21 step:19705 [D loss: 0.592080, acc.: 66.41%] [G loss: 0.681264]\n",
      "epoch:21 step:19706 [D loss: 0.519096, acc.: 73.44%] [G loss: 0.568776]\n",
      "epoch:21 step:19707 [D loss: 0.516694, acc.: 74.22%] [G loss: 0.613817]\n",
      "epoch:21 step:19708 [D loss: 0.653568, acc.: 59.38%] [G loss: 0.499488]\n",
      "epoch:21 step:19709 [D loss: 0.523430, acc.: 75.00%] [G loss: 0.656163]\n",
      "epoch:21 step:19710 [D loss: 0.593223, acc.: 65.62%] [G loss: 0.511156]\n",
      "epoch:21 step:19711 [D loss: 0.550295, acc.: 67.19%] [G loss: 0.538260]\n",
      "epoch:21 step:19712 [D loss: 0.594932, acc.: 66.41%] [G loss: 0.553180]\n",
      "epoch:21 step:19713 [D loss: 0.509078, acc.: 75.78%] [G loss: 0.706075]\n",
      "epoch:21 step:19714 [D loss: 0.508696, acc.: 73.44%] [G loss: 0.755331]\n",
      "epoch:21 step:19715 [D loss: 0.579050, acc.: 71.09%] [G loss: 0.761856]\n",
      "epoch:21 step:19716 [D loss: 0.501918, acc.: 71.88%] [G loss: 0.750460]\n",
      "epoch:21 step:19717 [D loss: 0.500140, acc.: 74.22%] [G loss: 0.662168]\n",
      "epoch:21 step:19718 [D loss: 0.533027, acc.: 72.66%] [G loss: 0.718062]\n",
      "epoch:21 step:19719 [D loss: 0.504097, acc.: 75.00%] [G loss: 0.751477]\n",
      "epoch:21 step:19720 [D loss: 0.533884, acc.: 76.56%] [G loss: 0.596994]\n",
      "epoch:21 step:19721 [D loss: 0.561136, acc.: 72.66%] [G loss: 0.472134]\n",
      "epoch:21 step:19722 [D loss: 0.488527, acc.: 74.22%] [G loss: 0.825500]\n",
      "epoch:21 step:19723 [D loss: 0.518428, acc.: 72.66%] [G loss: 0.751363]\n",
      "epoch:21 step:19724 [D loss: 0.531768, acc.: 71.88%] [G loss: 0.645426]\n",
      "epoch:21 step:19725 [D loss: 0.537042, acc.: 70.31%] [G loss: 0.805887]\n",
      "epoch:21 step:19726 [D loss: 0.526281, acc.: 73.44%] [G loss: 0.816345]\n",
      "epoch:21 step:19727 [D loss: 0.613873, acc.: 58.59%] [G loss: 0.607526]\n",
      "epoch:21 step:19728 [D loss: 0.606081, acc.: 60.94%] [G loss: 0.500697]\n",
      "epoch:21 step:19729 [D loss: 0.640518, acc.: 61.72%] [G loss: 0.580864]\n",
      "epoch:21 step:19730 [D loss: 0.581028, acc.: 68.75%] [G loss: 0.650060]\n",
      "epoch:21 step:19731 [D loss: 0.518070, acc.: 71.09%] [G loss: 0.653836]\n",
      "epoch:21 step:19732 [D loss: 0.540785, acc.: 68.75%] [G loss: 0.626950]\n",
      "epoch:21 step:19733 [D loss: 0.513527, acc.: 78.12%] [G loss: 0.688246]\n",
      "epoch:21 step:19734 [D loss: 0.550432, acc.: 71.88%] [G loss: 0.645674]\n",
      "epoch:21 step:19735 [D loss: 0.583818, acc.: 65.62%] [G loss: 0.656087]\n",
      "epoch:21 step:19736 [D loss: 0.495327, acc.: 76.56%] [G loss: 0.818933]\n",
      "epoch:21 step:19737 [D loss: 0.590437, acc.: 67.19%] [G loss: 0.644551]\n",
      "epoch:21 step:19738 [D loss: 0.538490, acc.: 72.66%] [G loss: 0.500101]\n",
      "epoch:21 step:19739 [D loss: 0.567854, acc.: 72.66%] [G loss: 0.579979]\n",
      "epoch:21 step:19740 [D loss: 0.558403, acc.: 66.41%] [G loss: 0.583168]\n",
      "epoch:21 step:19741 [D loss: 0.573828, acc.: 68.75%] [G loss: 0.516451]\n",
      "epoch:21 step:19742 [D loss: 0.522411, acc.: 74.22%] [G loss: 0.595246]\n",
      "epoch:21 step:19743 [D loss: 0.553418, acc.: 71.09%] [G loss: 0.596400]\n",
      "epoch:21 step:19744 [D loss: 0.510281, acc.: 75.00%] [G loss: 0.684181]\n",
      "epoch:21 step:19745 [D loss: 0.573263, acc.: 70.31%] [G loss: 0.473808]\n",
      "epoch:21 step:19746 [D loss: 0.522110, acc.: 76.56%] [G loss: 0.544990]\n",
      "epoch:21 step:19747 [D loss: 0.495977, acc.: 73.44%] [G loss: 0.771543]\n",
      "epoch:21 step:19748 [D loss: 0.555272, acc.: 68.75%] [G loss: 0.567685]\n",
      "epoch:21 step:19749 [D loss: 0.528117, acc.: 71.09%] [G loss: 0.640083]\n",
      "epoch:21 step:19750 [D loss: 0.558000, acc.: 69.53%] [G loss: 0.609430]\n",
      "epoch:21 step:19751 [D loss: 0.476411, acc.: 79.69%] [G loss: 0.612018]\n",
      "epoch:21 step:19752 [D loss: 0.494392, acc.: 76.56%] [G loss: 0.972999]\n",
      "epoch:21 step:19753 [D loss: 0.564609, acc.: 67.97%] [G loss: 0.676966]\n",
      "epoch:21 step:19754 [D loss: 0.491772, acc.: 76.56%] [G loss: 0.801726]\n",
      "epoch:21 step:19755 [D loss: 0.629466, acc.: 64.84%] [G loss: 0.765509]\n",
      "epoch:21 step:19756 [D loss: 0.562821, acc.: 67.19%] [G loss: 0.668860]\n",
      "epoch:21 step:19757 [D loss: 0.477920, acc.: 77.34%] [G loss: 0.609529]\n",
      "epoch:21 step:19758 [D loss: 0.531434, acc.: 70.31%] [G loss: 0.673254]\n",
      "epoch:21 step:19759 [D loss: 0.519765, acc.: 70.31%] [G loss: 0.602278]\n",
      "epoch:21 step:19760 [D loss: 0.476229, acc.: 74.22%] [G loss: 0.812128]\n",
      "epoch:21 step:19761 [D loss: 0.549920, acc.: 64.06%] [G loss: 0.958363]\n",
      "epoch:21 step:19762 [D loss: 0.547468, acc.: 71.88%] [G loss: 0.723121]\n",
      "epoch:21 step:19763 [D loss: 0.564897, acc.: 71.88%] [G loss: 0.698167]\n",
      "epoch:21 step:19764 [D loss: 0.526705, acc.: 72.66%] [G loss: 0.624620]\n",
      "epoch:21 step:19765 [D loss: 0.471993, acc.: 78.12%] [G loss: 0.750882]\n",
      "epoch:21 step:19766 [D loss: 0.547551, acc.: 67.19%] [G loss: 0.619422]\n",
      "epoch:21 step:19767 [D loss: 0.506937, acc.: 73.44%] [G loss: 0.689718]\n",
      "epoch:21 step:19768 [D loss: 0.538473, acc.: 75.00%] [G loss: 0.629310]\n",
      "epoch:21 step:19769 [D loss: 0.493398, acc.: 75.78%] [G loss: 0.752264]\n",
      "epoch:21 step:19770 [D loss: 0.539333, acc.: 71.88%] [G loss: 0.635923]\n",
      "epoch:21 step:19771 [D loss: 0.515925, acc.: 71.09%] [G loss: 0.669801]\n",
      "epoch:21 step:19772 [D loss: 0.535803, acc.: 67.97%] [G loss: 0.747112]\n",
      "epoch:21 step:19773 [D loss: 0.503341, acc.: 71.09%] [G loss: 0.677371]\n",
      "epoch:21 step:19774 [D loss: 0.542162, acc.: 72.66%] [G loss: 0.736817]\n",
      "epoch:21 step:19775 [D loss: 0.608213, acc.: 63.28%] [G loss: 0.628383]\n",
      "epoch:21 step:19776 [D loss: 0.545483, acc.: 74.22%] [G loss: 0.752344]\n",
      "epoch:21 step:19777 [D loss: 0.459923, acc.: 78.12%] [G loss: 0.771581]\n",
      "epoch:21 step:19778 [D loss: 0.487360, acc.: 75.78%] [G loss: 0.992777]\n",
      "epoch:21 step:19779 [D loss: 0.633584, acc.: 64.84%] [G loss: 0.694002]\n",
      "epoch:21 step:19780 [D loss: 0.540938, acc.: 69.53%] [G loss: 0.692249]\n",
      "epoch:21 step:19781 [D loss: 0.535578, acc.: 68.75%] [G loss: 0.701055]\n",
      "epoch:21 step:19782 [D loss: 0.576274, acc.: 64.84%] [G loss: 0.647804]\n",
      "epoch:21 step:19783 [D loss: 0.547525, acc.: 70.31%] [G loss: 0.694025]\n",
      "epoch:21 step:19784 [D loss: 0.592004, acc.: 68.75%] [G loss: 0.638144]\n",
      "epoch:21 step:19785 [D loss: 0.635316, acc.: 61.72%] [G loss: 0.745163]\n",
      "epoch:21 step:19786 [D loss: 0.572407, acc.: 68.75%] [G loss: 0.652150]\n",
      "epoch:21 step:19787 [D loss: 0.552831, acc.: 71.88%] [G loss: 0.736702]\n",
      "epoch:21 step:19788 [D loss: 0.533402, acc.: 68.75%] [G loss: 0.609623]\n",
      "epoch:21 step:19789 [D loss: 0.530505, acc.: 75.00%] [G loss: 0.675484]\n",
      "epoch:21 step:19790 [D loss: 0.551054, acc.: 72.66%] [G loss: 0.605394]\n",
      "epoch:21 step:19791 [D loss: 0.585157, acc.: 70.31%] [G loss: 0.573191]\n",
      "epoch:21 step:19792 [D loss: 0.526195, acc.: 73.44%] [G loss: 0.663767]\n",
      "epoch:21 step:19793 [D loss: 0.568586, acc.: 66.41%] [G loss: 0.745266]\n",
      "epoch:21 step:19794 [D loss: 0.616188, acc.: 60.16%] [G loss: 0.706019]\n",
      "epoch:21 step:19795 [D loss: 0.550901, acc.: 67.19%] [G loss: 0.827941]\n",
      "epoch:21 step:19796 [D loss: 0.451184, acc.: 78.12%] [G loss: 0.995711]\n",
      "epoch:21 step:19797 [D loss: 0.524877, acc.: 75.78%] [G loss: 0.720975]\n",
      "epoch:21 step:19798 [D loss: 0.515603, acc.: 74.22%] [G loss: 0.632172]\n",
      "epoch:21 step:19799 [D loss: 0.497269, acc.: 71.09%] [G loss: 0.725194]\n",
      "epoch:21 step:19800 [D loss: 0.468885, acc.: 81.25%] [G loss: 0.719560]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.691275\n",
      "FID: 43.280312\n",
      "0 = 12.831757437419858\n",
      "1 = 0.08897242759765313\n",
      "2 = 0.8673999905586243\n",
      "3 = 0.8127999901771545\n",
      "4 = 0.921999990940094\n",
      "5 = 0.9124382734298706\n",
      "6 = 0.8127999901771545\n",
      "7 = 8.048005493831617\n",
      "8 = 0.13861684647600783\n",
      "9 = 0.6978999972343445\n",
      "10 = 0.6858000159263611\n",
      "11 = 0.7099999785423279\n",
      "12 = 0.7028079628944397\n",
      "13 = 0.6858000159263611\n",
      "14 = 6.6913018226623535\n",
      "15 = 7.089292049407959\n",
      "16 = 0.3780196011066437\n",
      "17 = 6.691274642944336\n",
      "18 = 43.280311584472656\n",
      "epoch:21 step:19801 [D loss: 0.564306, acc.: 67.97%] [G loss: 0.723154]\n",
      "epoch:21 step:19802 [D loss: 0.599992, acc.: 68.75%] [G loss: 0.525967]\n",
      "epoch:21 step:19803 [D loss: 0.486536, acc.: 75.00%] [G loss: 0.660350]\n",
      "epoch:21 step:19804 [D loss: 0.485610, acc.: 75.78%] [G loss: 0.622245]\n",
      "epoch:21 step:19805 [D loss: 0.496975, acc.: 78.12%] [G loss: 0.629685]\n",
      "epoch:21 step:19806 [D loss: 0.534255, acc.: 71.09%] [G loss: 0.768874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19807 [D loss: 0.577711, acc.: 67.97%] [G loss: 0.623149]\n",
      "epoch:21 step:19808 [D loss: 0.477353, acc.: 76.56%] [G loss: 0.729243]\n",
      "epoch:21 step:19809 [D loss: 0.569949, acc.: 68.75%] [G loss: 0.739890]\n",
      "epoch:21 step:19810 [D loss: 0.576228, acc.: 64.84%] [G loss: 0.607120]\n",
      "epoch:21 step:19811 [D loss: 0.536564, acc.: 71.88%] [G loss: 0.740869]\n",
      "epoch:21 step:19812 [D loss: 0.469806, acc.: 81.25%] [G loss: 0.802580]\n",
      "epoch:21 step:19813 [D loss: 0.552583, acc.: 73.44%] [G loss: 0.754981]\n",
      "epoch:21 step:19814 [D loss: 0.619285, acc.: 64.06%] [G loss: 0.789189]\n",
      "epoch:21 step:19815 [D loss: 0.649492, acc.: 65.62%] [G loss: 0.679621]\n",
      "epoch:21 step:19816 [D loss: 0.551658, acc.: 73.44%] [G loss: 0.705889]\n",
      "epoch:21 step:19817 [D loss: 0.552528, acc.: 71.09%] [G loss: 0.519547]\n",
      "epoch:21 step:19818 [D loss: 0.587734, acc.: 61.72%] [G loss: 0.641335]\n",
      "epoch:21 step:19819 [D loss: 0.588985, acc.: 60.16%] [G loss: 0.523064]\n",
      "epoch:21 step:19820 [D loss: 0.621374, acc.: 65.62%] [G loss: 0.487618]\n",
      "epoch:21 step:19821 [D loss: 0.508264, acc.: 75.78%] [G loss: 0.618060]\n",
      "epoch:21 step:19822 [D loss: 0.588495, acc.: 63.28%] [G loss: 0.667823]\n",
      "epoch:21 step:19823 [D loss: 0.490731, acc.: 78.91%] [G loss: 0.664684]\n",
      "epoch:21 step:19824 [D loss: 0.634203, acc.: 61.72%] [G loss: 0.478970]\n",
      "epoch:21 step:19825 [D loss: 0.548115, acc.: 74.22%] [G loss: 0.450847]\n",
      "epoch:21 step:19826 [D loss: 0.468308, acc.: 78.12%] [G loss: 0.723849]\n",
      "epoch:21 step:19827 [D loss: 0.593765, acc.: 64.06%] [G loss: 0.490389]\n",
      "epoch:21 step:19828 [D loss: 0.610056, acc.: 67.97%] [G loss: 0.602494]\n",
      "epoch:21 step:19829 [D loss: 0.511375, acc.: 67.97%] [G loss: 0.584989]\n",
      "epoch:21 step:19830 [D loss: 0.608962, acc.: 63.28%] [G loss: 0.685287]\n",
      "epoch:21 step:19831 [D loss: 0.519897, acc.: 68.75%] [G loss: 0.794456]\n",
      "epoch:21 step:19832 [D loss: 0.450815, acc.: 81.25%] [G loss: 0.654857]\n",
      "epoch:21 step:19833 [D loss: 0.526020, acc.: 71.88%] [G loss: 0.794747]\n",
      "epoch:21 step:19834 [D loss: 0.557472, acc.: 70.31%] [G loss: 0.741636]\n",
      "epoch:21 step:19835 [D loss: 0.598322, acc.: 65.62%] [G loss: 0.553349]\n",
      "epoch:21 step:19836 [D loss: 0.510918, acc.: 73.44%] [G loss: 0.540464]\n",
      "epoch:21 step:19837 [D loss: 0.602982, acc.: 67.19%] [G loss: 0.780546]\n",
      "epoch:21 step:19838 [D loss: 0.560954, acc.: 71.88%] [G loss: 0.875859]\n",
      "epoch:21 step:19839 [D loss: 0.475958, acc.: 79.69%] [G loss: 0.937587]\n",
      "epoch:21 step:19840 [D loss: 0.577959, acc.: 64.84%] [G loss: 0.796454]\n",
      "epoch:21 step:19841 [D loss: 0.557179, acc.: 67.19%] [G loss: 0.616546]\n",
      "epoch:21 step:19842 [D loss: 0.487812, acc.: 77.34%] [G loss: 0.595837]\n",
      "epoch:21 step:19843 [D loss: 0.565003, acc.: 67.97%] [G loss: 0.732830]\n",
      "epoch:21 step:19844 [D loss: 0.573488, acc.: 65.62%] [G loss: 0.543945]\n",
      "epoch:21 step:19845 [D loss: 0.526259, acc.: 71.88%] [G loss: 0.573690]\n",
      "epoch:21 step:19846 [D loss: 0.591278, acc.: 63.28%] [G loss: 0.508963]\n",
      "epoch:21 step:19847 [D loss: 0.528501, acc.: 73.44%] [G loss: 0.527510]\n",
      "epoch:21 step:19848 [D loss: 0.557578, acc.: 72.66%] [G loss: 0.427411]\n",
      "epoch:21 step:19849 [D loss: 0.469907, acc.: 77.34%] [G loss: 0.690329]\n",
      "epoch:21 step:19850 [D loss: 0.488644, acc.: 75.78%] [G loss: 0.602350]\n",
      "epoch:21 step:19851 [D loss: 0.580924, acc.: 67.19%] [G loss: 0.505603]\n",
      "epoch:21 step:19852 [D loss: 0.602149, acc.: 64.84%] [G loss: 0.456594]\n",
      "epoch:21 step:19853 [D loss: 0.469031, acc.: 76.56%] [G loss: 0.843320]\n",
      "epoch:21 step:19854 [D loss: 0.535097, acc.: 72.66%] [G loss: 0.571331]\n",
      "epoch:21 step:19855 [D loss: 0.560893, acc.: 70.31%] [G loss: 0.604214]\n",
      "epoch:21 step:19856 [D loss: 0.549990, acc.: 74.22%] [G loss: 0.611594]\n",
      "epoch:21 step:19857 [D loss: 0.650274, acc.: 65.62%] [G loss: 0.511338]\n",
      "epoch:21 step:19858 [D loss: 0.585955, acc.: 69.53%] [G loss: 0.494377]\n",
      "epoch:21 step:19859 [D loss: 0.597175, acc.: 71.09%] [G loss: 0.654252]\n",
      "epoch:21 step:19860 [D loss: 0.554716, acc.: 68.75%] [G loss: 0.696123]\n",
      "epoch:21 step:19861 [D loss: 0.518725, acc.: 70.31%] [G loss: 0.669510]\n",
      "epoch:21 step:19862 [D loss: 0.582490, acc.: 68.75%] [G loss: 0.698232]\n",
      "epoch:21 step:19863 [D loss: 0.526934, acc.: 72.66%] [G loss: 0.782829]\n",
      "epoch:21 step:19864 [D loss: 0.646521, acc.: 64.84%] [G loss: 0.615014]\n",
      "epoch:21 step:19865 [D loss: 0.515048, acc.: 71.88%] [G loss: 0.576217]\n",
      "epoch:21 step:19866 [D loss: 0.565366, acc.: 73.44%] [G loss: 0.457084]\n",
      "epoch:21 step:19867 [D loss: 0.476417, acc.: 78.12%] [G loss: 0.562777]\n",
      "epoch:21 step:19868 [D loss: 0.473463, acc.: 80.47%] [G loss: 0.716360]\n",
      "epoch:21 step:19869 [D loss: 0.542406, acc.: 71.88%] [G loss: 0.568428]\n",
      "epoch:21 step:19870 [D loss: 0.524993, acc.: 71.88%] [G loss: 0.588466]\n",
      "epoch:21 step:19871 [D loss: 0.507553, acc.: 77.34%] [G loss: 0.606900]\n",
      "epoch:21 step:19872 [D loss: 0.593953, acc.: 64.84%] [G loss: 0.661794]\n",
      "epoch:21 step:19873 [D loss: 0.540347, acc.: 71.09%] [G loss: 0.718269]\n",
      "epoch:21 step:19874 [D loss: 0.523102, acc.: 73.44%] [G loss: 0.713531]\n",
      "epoch:21 step:19875 [D loss: 0.451759, acc.: 79.69%] [G loss: 0.783248]\n",
      "epoch:21 step:19876 [D loss: 0.522924, acc.: 70.31%] [G loss: 0.883492]\n",
      "epoch:21 step:19877 [D loss: 0.580462, acc.: 69.53%] [G loss: 0.791825]\n",
      "epoch:21 step:19878 [D loss: 0.574105, acc.: 68.75%] [G loss: 0.685550]\n",
      "epoch:21 step:19879 [D loss: 0.521660, acc.: 71.88%] [G loss: 0.704822]\n",
      "epoch:21 step:19880 [D loss: 0.636108, acc.: 64.84%] [G loss: 0.540503]\n",
      "epoch:21 step:19881 [D loss: 0.539671, acc.: 72.66%] [G loss: 0.648933]\n",
      "epoch:21 step:19882 [D loss: 0.520084, acc.: 70.31%] [G loss: 0.811923]\n",
      "epoch:21 step:19883 [D loss: 0.469555, acc.: 76.56%] [G loss: 0.858398]\n",
      "epoch:21 step:19884 [D loss: 0.473712, acc.: 73.44%] [G loss: 0.835013]\n",
      "epoch:21 step:19885 [D loss: 0.458287, acc.: 77.34%] [G loss: 0.807711]\n",
      "epoch:21 step:19886 [D loss: 0.497708, acc.: 70.31%] [G loss: 0.992382]\n",
      "epoch:21 step:19887 [D loss: 0.667586, acc.: 61.72%] [G loss: 0.545628]\n",
      "epoch:21 step:19888 [D loss: 0.618408, acc.: 62.50%] [G loss: 0.464210]\n",
      "epoch:21 step:19889 [D loss: 0.571412, acc.: 70.31%] [G loss: 0.493741]\n",
      "epoch:21 step:19890 [D loss: 0.559162, acc.: 64.84%] [G loss: 0.572351]\n",
      "epoch:21 step:19891 [D loss: 0.665765, acc.: 64.06%] [G loss: 0.595702]\n",
      "epoch:21 step:19892 [D loss: 0.573599, acc.: 65.62%] [G loss: 0.634602]\n",
      "epoch:21 step:19893 [D loss: 0.573753, acc.: 68.75%] [G loss: 0.596102]\n",
      "epoch:21 step:19894 [D loss: 0.554535, acc.: 74.22%] [G loss: 0.546719]\n",
      "epoch:21 step:19895 [D loss: 0.527698, acc.: 74.22%] [G loss: 0.603496]\n",
      "epoch:21 step:19896 [D loss: 0.451179, acc.: 78.91%] [G loss: 0.670736]\n",
      "epoch:21 step:19897 [D loss: 0.610778, acc.: 66.41%] [G loss: 0.777619]\n",
      "epoch:21 step:19898 [D loss: 0.528319, acc.: 72.66%] [G loss: 0.665669]\n",
      "epoch:21 step:19899 [D loss: 0.475930, acc.: 75.78%] [G loss: 0.657606]\n",
      "epoch:21 step:19900 [D loss: 0.507221, acc.: 75.00%] [G loss: 0.700540]\n",
      "epoch:21 step:19901 [D loss: 0.577108, acc.: 71.88%] [G loss: 0.609120]\n",
      "epoch:21 step:19902 [D loss: 0.535625, acc.: 71.09%] [G loss: 0.750641]\n",
      "epoch:21 step:19903 [D loss: 0.642624, acc.: 60.16%] [G loss: 0.601322]\n",
      "epoch:21 step:19904 [D loss: 0.576012, acc.: 68.75%] [G loss: 0.599179]\n",
      "epoch:21 step:19905 [D loss: 0.573838, acc.: 67.97%] [G loss: 0.553608]\n",
      "epoch:21 step:19906 [D loss: 0.561376, acc.: 67.19%] [G loss: 0.553207]\n",
      "epoch:21 step:19907 [D loss: 0.524845, acc.: 75.78%] [G loss: 0.757774]\n",
      "epoch:21 step:19908 [D loss: 0.443812, acc.: 77.34%] [G loss: 1.061655]\n",
      "epoch:21 step:19909 [D loss: 0.520586, acc.: 73.44%] [G loss: 1.048233]\n",
      "epoch:21 step:19910 [D loss: 0.530284, acc.: 71.88%] [G loss: 0.939370]\n",
      "epoch:21 step:19911 [D loss: 0.594369, acc.: 69.53%] [G loss: 0.707078]\n",
      "epoch:21 step:19912 [D loss: 0.602683, acc.: 65.62%] [G loss: 0.641325]\n",
      "epoch:21 step:19913 [D loss: 0.548420, acc.: 72.66%] [G loss: 0.574838]\n",
      "epoch:21 step:19914 [D loss: 0.525225, acc.: 75.78%] [G loss: 0.641146]\n",
      "epoch:21 step:19915 [D loss: 0.583142, acc.: 67.19%] [G loss: 0.649675]\n",
      "epoch:21 step:19916 [D loss: 0.525682, acc.: 67.97%] [G loss: 0.746302]\n",
      "epoch:21 step:19917 [D loss: 0.551101, acc.: 69.53%] [G loss: 0.761008]\n",
      "epoch:21 step:19918 [D loss: 0.508855, acc.: 71.09%] [G loss: 0.797345]\n",
      "epoch:21 step:19919 [D loss: 0.588274, acc.: 64.84%] [G loss: 0.629553]\n",
      "epoch:21 step:19920 [D loss: 0.555836, acc.: 65.62%] [G loss: 0.653972]\n",
      "epoch:21 step:19921 [D loss: 0.493308, acc.: 71.88%] [G loss: 0.609548]\n",
      "epoch:21 step:19922 [D loss: 0.498526, acc.: 75.00%] [G loss: 0.673897]\n",
      "epoch:21 step:19923 [D loss: 0.517398, acc.: 69.53%] [G loss: 0.805145]\n",
      "epoch:21 step:19924 [D loss: 0.491260, acc.: 75.00%] [G loss: 0.776667]\n",
      "epoch:21 step:19925 [D loss: 0.497632, acc.: 73.44%] [G loss: 0.843403]\n",
      "epoch:21 step:19926 [D loss: 0.604735, acc.: 67.19%] [G loss: 0.827714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19927 [D loss: 0.587277, acc.: 66.41%] [G loss: 0.690096]\n",
      "epoch:21 step:19928 [D loss: 0.637852, acc.: 66.41%] [G loss: 0.672191]\n",
      "epoch:21 step:19929 [D loss: 0.557891, acc.: 67.97%] [G loss: 0.652261]\n",
      "epoch:21 step:19930 [D loss: 0.603004, acc.: 67.19%] [G loss: 0.514488]\n",
      "epoch:21 step:19931 [D loss: 0.495164, acc.: 73.44%] [G loss: 0.669214]\n",
      "epoch:21 step:19932 [D loss: 0.568590, acc.: 67.97%] [G loss: 0.641952]\n",
      "epoch:21 step:19933 [D loss: 0.576820, acc.: 65.62%] [G loss: 0.653067]\n",
      "epoch:21 step:19934 [D loss: 0.559103, acc.: 71.88%] [G loss: 0.609448]\n",
      "epoch:21 step:19935 [D loss: 0.563248, acc.: 70.31%] [G loss: 0.501564]\n",
      "epoch:21 step:19936 [D loss: 0.529163, acc.: 69.53%] [G loss: 0.666385]\n",
      "epoch:21 step:19937 [D loss: 0.574259, acc.: 64.84%] [G loss: 0.508628]\n",
      "epoch:21 step:19938 [D loss: 0.586068, acc.: 67.19%] [G loss: 0.613133]\n",
      "epoch:21 step:19939 [D loss: 0.529761, acc.: 75.00%] [G loss: 0.612671]\n",
      "epoch:21 step:19940 [D loss: 0.592119, acc.: 69.53%] [G loss: 0.567681]\n",
      "epoch:21 step:19941 [D loss: 0.526106, acc.: 73.44%] [G loss: 0.745879]\n",
      "epoch:21 step:19942 [D loss: 0.467347, acc.: 79.69%] [G loss: 0.737206]\n",
      "epoch:21 step:19943 [D loss: 0.578418, acc.: 68.75%] [G loss: 0.563853]\n",
      "epoch:21 step:19944 [D loss: 0.601252, acc.: 68.75%] [G loss: 0.464397]\n",
      "epoch:21 step:19945 [D loss: 0.534600, acc.: 70.31%] [G loss: 0.588558]\n",
      "epoch:21 step:19946 [D loss: 0.572094, acc.: 68.75%] [G loss: 0.651548]\n",
      "epoch:21 step:19947 [D loss: 0.482202, acc.: 78.12%] [G loss: 0.600577]\n",
      "epoch:21 step:19948 [D loss: 0.497979, acc.: 75.00%] [G loss: 0.690638]\n",
      "epoch:21 step:19949 [D loss: 0.544158, acc.: 70.31%] [G loss: 0.738059]\n",
      "epoch:21 step:19950 [D loss: 0.519365, acc.: 71.09%] [G loss: 0.576347]\n",
      "epoch:21 step:19951 [D loss: 0.512174, acc.: 75.00%] [G loss: 0.610212]\n",
      "epoch:21 step:19952 [D loss: 0.594109, acc.: 71.09%] [G loss: 0.641731]\n",
      "epoch:21 step:19953 [D loss: 0.461611, acc.: 82.03%] [G loss: 0.802213]\n",
      "epoch:21 step:19954 [D loss: 0.638396, acc.: 65.62%] [G loss: 0.561674]\n",
      "epoch:21 step:19955 [D loss: 0.693116, acc.: 58.59%] [G loss: 0.451112]\n",
      "epoch:21 step:19956 [D loss: 0.533274, acc.: 69.53%] [G loss: 0.675371]\n",
      "epoch:21 step:19957 [D loss: 0.580532, acc.: 64.84%] [G loss: 0.602159]\n",
      "epoch:21 step:19958 [D loss: 0.630659, acc.: 61.72%] [G loss: 0.458447]\n",
      "epoch:21 step:19959 [D loss: 0.585952, acc.: 70.31%] [G loss: 0.582716]\n",
      "epoch:21 step:19960 [D loss: 0.552866, acc.: 67.97%] [G loss: 0.561895]\n",
      "epoch:21 step:19961 [D loss: 0.577354, acc.: 64.06%] [G loss: 0.480126]\n",
      "epoch:21 step:19962 [D loss: 0.558618, acc.: 65.62%] [G loss: 0.594471]\n",
      "epoch:21 step:19963 [D loss: 0.448633, acc.: 78.91%] [G loss: 0.701982]\n",
      "epoch:21 step:19964 [D loss: 0.559195, acc.: 71.09%] [G loss: 0.668990]\n",
      "epoch:21 step:19965 [D loss: 0.624281, acc.: 58.59%] [G loss: 0.620396]\n",
      "epoch:21 step:19966 [D loss: 0.522388, acc.: 75.00%] [G loss: 0.945122]\n",
      "epoch:21 step:19967 [D loss: 0.563546, acc.: 64.06%] [G loss: 0.682459]\n",
      "epoch:21 step:19968 [D loss: 0.564605, acc.: 67.19%] [G loss: 0.635809]\n",
      "epoch:21 step:19969 [D loss: 0.482140, acc.: 79.69%] [G loss: 0.683299]\n",
      "epoch:21 step:19970 [D loss: 0.620904, acc.: 59.38%] [G loss: 0.560753]\n",
      "epoch:21 step:19971 [D loss: 0.597548, acc.: 64.06%] [G loss: 0.488577]\n",
      "epoch:21 step:19972 [D loss: 0.578241, acc.: 67.97%] [G loss: 0.602768]\n",
      "epoch:21 step:19973 [D loss: 0.467839, acc.: 78.91%] [G loss: 0.710812]\n",
      "epoch:21 step:19974 [D loss: 0.554676, acc.: 71.09%] [G loss: 0.717393]\n",
      "epoch:21 step:19975 [D loss: 0.494006, acc.: 77.34%] [G loss: 0.662184]\n",
      "epoch:21 step:19976 [D loss: 0.489275, acc.: 75.78%] [G loss: 0.698114]\n",
      "epoch:21 step:19977 [D loss: 0.542500, acc.: 73.44%] [G loss: 0.799554]\n",
      "epoch:21 step:19978 [D loss: 0.632523, acc.: 62.50%] [G loss: 0.620785]\n",
      "epoch:21 step:19979 [D loss: 0.524999, acc.: 71.09%] [G loss: 0.480651]\n",
      "epoch:21 step:19980 [D loss: 0.579419, acc.: 69.53%] [G loss: 0.569817]\n",
      "epoch:21 step:19981 [D loss: 0.514585, acc.: 75.00%] [G loss: 0.659813]\n",
      "epoch:21 step:19982 [D loss: 0.586363, acc.: 67.19%] [G loss: 0.631278]\n",
      "epoch:21 step:19983 [D loss: 0.527296, acc.: 73.44%] [G loss: 0.638992]\n",
      "epoch:21 step:19984 [D loss: 0.529646, acc.: 70.31%] [G loss: 0.790925]\n",
      "epoch:21 step:19985 [D loss: 0.603507, acc.: 64.84%] [G loss: 0.564057]\n",
      "epoch:21 step:19986 [D loss: 0.524741, acc.: 67.97%] [G loss: 0.597140]\n",
      "epoch:21 step:19987 [D loss: 0.532980, acc.: 74.22%] [G loss: 0.546533]\n",
      "epoch:21 step:19988 [D loss: 0.496026, acc.: 74.22%] [G loss: 0.601406]\n",
      "epoch:21 step:19989 [D loss: 0.511585, acc.: 80.47%] [G loss: 0.776133]\n",
      "epoch:21 step:19990 [D loss: 0.515647, acc.: 71.09%] [G loss: 1.036776]\n",
      "epoch:21 step:19991 [D loss: 0.490275, acc.: 75.78%] [G loss: 0.956422]\n",
      "epoch:21 step:19992 [D loss: 0.459565, acc.: 82.03%] [G loss: 0.905815]\n",
      "epoch:21 step:19993 [D loss: 0.636583, acc.: 64.84%] [G loss: 0.671459]\n",
      "epoch:21 step:19994 [D loss: 0.575248, acc.: 67.19%] [G loss: 0.510870]\n",
      "epoch:21 step:19995 [D loss: 0.574875, acc.: 67.97%] [G loss: 0.514928]\n",
      "epoch:21 step:19996 [D loss: 0.529979, acc.: 72.66%] [G loss: 0.525222]\n",
      "epoch:21 step:19997 [D loss: 0.559422, acc.: 69.53%] [G loss: 0.647823]\n",
      "epoch:21 step:19998 [D loss: 0.500006, acc.: 71.88%] [G loss: 0.771776]\n",
      "epoch:21 step:19999 [D loss: 0.556411, acc.: 69.53%] [G loss: 0.697950]\n",
      "epoch:21 step:20000 [D loss: 0.585646, acc.: 65.62%] [G loss: 0.645154]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.598815\n",
      "FID: 50.617317\n",
      "0 = 12.835675673770941\n",
      "1 = 0.09502748493336667\n",
      "2 = 0.8578000068664551\n",
      "3 = 0.8059999942779541\n",
      "4 = 0.909600019454956\n",
      "5 = 0.8991521596908569\n",
      "6 = 0.8059999942779541\n",
      "7 = 8.29327669572829\n",
      "8 = 0.15424278373469702\n",
      "9 = 0.7039999961853027\n",
      "10 = 0.6966000199317932\n",
      "11 = 0.7113999724388123\n",
      "12 = 0.7070645689964294\n",
      "13 = 0.6966000199317932\n",
      "14 = 6.598845481872559\n",
      "15 = 7.01115608215332\n",
      "16 = 0.38953259587287903\n",
      "17 = 6.598815441131592\n",
      "18 = 50.61731719970703\n",
      "epoch:21 step:20001 [D loss: 0.573206, acc.: 68.75%] [G loss: 0.624792]\n",
      "epoch:21 step:20002 [D loss: 0.568534, acc.: 67.97%] [G loss: 0.619513]\n",
      "epoch:21 step:20003 [D loss: 0.462478, acc.: 79.69%] [G loss: 0.743689]\n",
      "epoch:21 step:20004 [D loss: 0.486826, acc.: 75.78%] [G loss: 0.743710]\n",
      "epoch:21 step:20005 [D loss: 0.498244, acc.: 76.56%] [G loss: 0.686002]\n",
      "epoch:21 step:20006 [D loss: 0.482910, acc.: 75.00%] [G loss: 0.737147]\n",
      "epoch:21 step:20007 [D loss: 0.630998, acc.: 65.62%] [G loss: 0.589605]\n",
      "epoch:21 step:20008 [D loss: 0.571087, acc.: 67.19%] [G loss: 0.538010]\n",
      "epoch:21 step:20009 [D loss: 0.529276, acc.: 71.88%] [G loss: 0.553588]\n",
      "epoch:21 step:20010 [D loss: 0.524050, acc.: 71.09%] [G loss: 0.488214]\n",
      "epoch:21 step:20011 [D loss: 0.490213, acc.: 78.91%] [G loss: 0.695463]\n",
      "epoch:21 step:20012 [D loss: 0.505655, acc.: 71.88%] [G loss: 0.608700]\n",
      "epoch:21 step:20013 [D loss: 0.466074, acc.: 78.12%] [G loss: 0.887494]\n",
      "epoch:21 step:20014 [D loss: 0.514979, acc.: 71.88%] [G loss: 0.762589]\n",
      "epoch:21 step:20015 [D loss: 0.569270, acc.: 74.22%] [G loss: 0.640809]\n",
      "epoch:21 step:20016 [D loss: 0.543470, acc.: 70.31%] [G loss: 0.727651]\n",
      "epoch:21 step:20017 [D loss: 0.509355, acc.: 75.00%] [G loss: 0.757091]\n",
      "epoch:21 step:20018 [D loss: 0.580982, acc.: 70.31%] [G loss: 0.692707]\n",
      "epoch:21 step:20019 [D loss: 0.632943, acc.: 57.03%] [G loss: 0.545187]\n",
      "epoch:21 step:20020 [D loss: 0.442424, acc.: 82.03%] [G loss: 0.689957]\n",
      "epoch:21 step:20021 [D loss: 0.454139, acc.: 79.69%] [G loss: 0.793208]\n",
      "epoch:21 step:20022 [D loss: 0.568995, acc.: 69.53%] [G loss: 0.751588]\n",
      "epoch:21 step:20023 [D loss: 0.503164, acc.: 73.44%] [G loss: 0.766842]\n",
      "epoch:21 step:20024 [D loss: 0.483680, acc.: 74.22%] [G loss: 0.833488]\n",
      "epoch:21 step:20025 [D loss: 0.589498, acc.: 70.31%] [G loss: 0.583852]\n",
      "epoch:21 step:20026 [D loss: 0.671264, acc.: 60.94%] [G loss: 0.513726]\n",
      "epoch:21 step:20027 [D loss: 0.458192, acc.: 78.91%] [G loss: 0.673760]\n",
      "epoch:21 step:20028 [D loss: 0.487619, acc.: 77.34%] [G loss: 0.563610]\n",
      "epoch:21 step:20029 [D loss: 0.516114, acc.: 70.31%] [G loss: 0.756689]\n",
      "epoch:21 step:20030 [D loss: 0.578487, acc.: 67.97%] [G loss: 0.935689]\n",
      "epoch:21 step:20031 [D loss: 0.394812, acc.: 84.38%] [G loss: 0.816530]\n",
      "epoch:21 step:20032 [D loss: 0.557248, acc.: 69.53%] [G loss: 0.782683]\n",
      "epoch:21 step:20033 [D loss: 0.562272, acc.: 70.31%] [G loss: 0.787463]\n",
      "epoch:21 step:20034 [D loss: 0.492943, acc.: 73.44%] [G loss: 0.765954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20035 [D loss: 0.483734, acc.: 75.00%] [G loss: 0.917877]\n",
      "epoch:21 step:20036 [D loss: 0.459893, acc.: 75.00%] [G loss: 0.885729]\n",
      "epoch:21 step:20037 [D loss: 0.496726, acc.: 76.56%] [G loss: 0.628774]\n",
      "epoch:21 step:20038 [D loss: 0.482143, acc.: 78.12%] [G loss: 0.896021]\n",
      "epoch:21 step:20039 [D loss: 0.542703, acc.: 71.09%] [G loss: 0.858076]\n",
      "epoch:21 step:20040 [D loss: 0.654607, acc.: 63.28%] [G loss: 0.586743]\n",
      "epoch:21 step:20041 [D loss: 0.473275, acc.: 78.91%] [G loss: 0.691429]\n",
      "epoch:21 step:20042 [D loss: 0.572737, acc.: 68.75%] [G loss: 0.599564]\n",
      "epoch:21 step:20043 [D loss: 0.548119, acc.: 71.88%] [G loss: 0.600782]\n",
      "epoch:21 step:20044 [D loss: 0.583608, acc.: 65.62%] [G loss: 0.552423]\n",
      "epoch:21 step:20045 [D loss: 0.555599, acc.: 64.84%] [G loss: 0.558311]\n",
      "epoch:21 step:20046 [D loss: 0.489463, acc.: 74.22%] [G loss: 0.645366]\n",
      "epoch:21 step:20047 [D loss: 0.555464, acc.: 72.66%] [G loss: 0.619884]\n",
      "epoch:21 step:20048 [D loss: 0.501187, acc.: 72.66%] [G loss: 0.819359]\n",
      "epoch:21 step:20049 [D loss: 0.526911, acc.: 70.31%] [G loss: 0.923468]\n",
      "epoch:21 step:20050 [D loss: 0.553982, acc.: 72.66%] [G loss: 0.820078]\n",
      "epoch:21 step:20051 [D loss: 0.470909, acc.: 74.22%] [G loss: 0.795463]\n",
      "epoch:21 step:20052 [D loss: 0.589989, acc.: 70.31%] [G loss: 0.535796]\n",
      "epoch:21 step:20053 [D loss: 0.717421, acc.: 55.47%] [G loss: 0.509820]\n",
      "epoch:21 step:20054 [D loss: 0.602095, acc.: 59.38%] [G loss: 0.489174]\n",
      "epoch:21 step:20055 [D loss: 0.606087, acc.: 66.41%] [G loss: 0.626081]\n",
      "epoch:21 step:20056 [D loss: 0.621423, acc.: 62.50%] [G loss: 0.470197]\n",
      "epoch:21 step:20057 [D loss: 0.562869, acc.: 68.75%] [G loss: 0.479105]\n",
      "epoch:21 step:20058 [D loss: 0.500403, acc.: 77.34%] [G loss: 0.552920]\n",
      "epoch:21 step:20059 [D loss: 0.528612, acc.: 74.22%] [G loss: 0.638634]\n",
      "epoch:21 step:20060 [D loss: 0.573802, acc.: 70.31%] [G loss: 0.600461]\n",
      "epoch:21 step:20061 [D loss: 0.541737, acc.: 71.88%] [G loss: 0.638347]\n",
      "epoch:21 step:20062 [D loss: 0.457709, acc.: 78.91%] [G loss: 0.687633]\n",
      "epoch:21 step:20063 [D loss: 0.583238, acc.: 72.66%] [G loss: 0.565995]\n",
      "epoch:21 step:20064 [D loss: 0.532303, acc.: 71.88%] [G loss: 0.567546]\n",
      "epoch:21 step:20065 [D loss: 0.520136, acc.: 75.78%] [G loss: 0.561341]\n",
      "epoch:21 step:20066 [D loss: 0.500435, acc.: 70.31%] [G loss: 0.750639]\n",
      "epoch:21 step:20067 [D loss: 0.621287, acc.: 64.84%] [G loss: 0.572982]\n",
      "epoch:21 step:20068 [D loss: 0.476839, acc.: 77.34%] [G loss: 0.609329]\n",
      "epoch:21 step:20069 [D loss: 0.512922, acc.: 74.22%] [G loss: 0.668009]\n",
      "epoch:21 step:20070 [D loss: 0.560097, acc.: 70.31%] [G loss: 0.622906]\n",
      "epoch:21 step:20071 [D loss: 0.545665, acc.: 70.31%] [G loss: 0.433293]\n",
      "epoch:21 step:20072 [D loss: 0.550312, acc.: 69.53%] [G loss: 0.525134]\n",
      "epoch:21 step:20073 [D loss: 0.535011, acc.: 71.88%] [G loss: 0.625132]\n",
      "epoch:21 step:20074 [D loss: 0.603537, acc.: 61.72%] [G loss: 0.620379]\n",
      "epoch:21 step:20075 [D loss: 0.495486, acc.: 78.12%] [G loss: 0.773408]\n",
      "epoch:21 step:20076 [D loss: 0.512463, acc.: 77.34%] [G loss: 0.687684]\n",
      "epoch:21 step:20077 [D loss: 0.615463, acc.: 61.72%] [G loss: 0.587636]\n",
      "epoch:21 step:20078 [D loss: 0.601311, acc.: 63.28%] [G loss: 0.488516]\n",
      "epoch:21 step:20079 [D loss: 0.554382, acc.: 68.75%] [G loss: 0.701676]\n",
      "epoch:21 step:20080 [D loss: 0.531400, acc.: 71.09%] [G loss: 0.689197]\n",
      "epoch:21 step:20081 [D loss: 0.589068, acc.: 63.28%] [G loss: 0.599864]\n",
      "epoch:21 step:20082 [D loss: 0.529783, acc.: 75.00%] [G loss: 0.570915]\n",
      "epoch:21 step:20083 [D loss: 0.497818, acc.: 71.88%] [G loss: 0.839948]\n",
      "epoch:21 step:20084 [D loss: 0.534750, acc.: 69.53%] [G loss: 0.724910]\n",
      "epoch:21 step:20085 [D loss: 0.610899, acc.: 64.84%] [G loss: 0.771659]\n",
      "epoch:21 step:20086 [D loss: 0.518719, acc.: 70.31%] [G loss: 0.825657]\n",
      "epoch:21 step:20087 [D loss: 0.556076, acc.: 69.53%] [G loss: 0.644900]\n",
      "epoch:21 step:20088 [D loss: 0.595376, acc.: 66.41%] [G loss: 0.639311]\n",
      "epoch:21 step:20089 [D loss: 0.632832, acc.: 61.72%] [G loss: 0.535956]\n",
      "epoch:21 step:20090 [D loss: 0.539581, acc.: 74.22%] [G loss: 0.573696]\n",
      "epoch:21 step:20091 [D loss: 0.489754, acc.: 72.66%] [G loss: 0.697585]\n",
      "epoch:21 step:20092 [D loss: 0.599741, acc.: 64.84%] [G loss: 0.566346]\n",
      "epoch:21 step:20093 [D loss: 0.488929, acc.: 78.12%] [G loss: 0.780538]\n",
      "epoch:21 step:20094 [D loss: 0.506229, acc.: 75.00%] [G loss: 0.864735]\n",
      "epoch:21 step:20095 [D loss: 0.624751, acc.: 67.19%] [G loss: 0.722651]\n",
      "epoch:21 step:20096 [D loss: 0.549922, acc.: 70.31%] [G loss: 0.676473]\n",
      "epoch:21 step:20097 [D loss: 0.624653, acc.: 62.50%] [G loss: 0.594464]\n",
      "epoch:21 step:20098 [D loss: 0.669940, acc.: 54.69%] [G loss: 0.557659]\n",
      "epoch:21 step:20099 [D loss: 0.587545, acc.: 70.31%] [G loss: 0.531859]\n",
      "epoch:21 step:20100 [D loss: 0.555054, acc.: 68.75%] [G loss: 0.574620]\n",
      "epoch:21 step:20101 [D loss: 0.591757, acc.: 67.97%] [G loss: 0.437395]\n",
      "epoch:21 step:20102 [D loss: 0.490457, acc.: 74.22%] [G loss: 0.601904]\n",
      "epoch:21 step:20103 [D loss: 0.522800, acc.: 73.44%] [G loss: 0.756321]\n",
      "epoch:21 step:20104 [D loss: 0.424453, acc.: 82.03%] [G loss: 0.770311]\n",
      "epoch:21 step:20105 [D loss: 0.525982, acc.: 69.53%] [G loss: 0.731723]\n",
      "epoch:21 step:20106 [D loss: 0.478377, acc.: 78.12%] [G loss: 1.052904]\n",
      "epoch:21 step:20107 [D loss: 0.534863, acc.: 75.78%] [G loss: 0.862939]\n",
      "epoch:21 step:20108 [D loss: 0.576421, acc.: 64.84%] [G loss: 0.877807]\n",
      "epoch:21 step:20109 [D loss: 0.579598, acc.: 66.41%] [G loss: 0.699733]\n",
      "epoch:21 step:20110 [D loss: 0.545959, acc.: 71.88%] [G loss: 0.680587]\n",
      "epoch:21 step:20111 [D loss: 0.575451, acc.: 68.75%] [G loss: 0.679953]\n",
      "epoch:21 step:20112 [D loss: 0.543566, acc.: 66.41%] [G loss: 0.808957]\n",
      "epoch:21 step:20113 [D loss: 0.518867, acc.: 70.31%] [G loss: 0.691931]\n",
      "epoch:21 step:20114 [D loss: 0.670910, acc.: 63.28%] [G loss: 0.656539]\n",
      "epoch:21 step:20115 [D loss: 0.576084, acc.: 62.50%] [G loss: 0.625364]\n",
      "epoch:21 step:20116 [D loss: 0.544801, acc.: 74.22%] [G loss: 0.632870]\n",
      "epoch:21 step:20117 [D loss: 0.485056, acc.: 75.00%] [G loss: 0.639521]\n",
      "epoch:21 step:20118 [D loss: 0.556759, acc.: 71.88%] [G loss: 0.855576]\n",
      "epoch:21 step:20119 [D loss: 0.536952, acc.: 69.53%] [G loss: 0.752139]\n",
      "epoch:21 step:20120 [D loss: 0.577254, acc.: 71.88%] [G loss: 0.616641]\n",
      "epoch:21 step:20121 [D loss: 0.499710, acc.: 76.56%] [G loss: 0.871820]\n",
      "epoch:21 step:20122 [D loss: 0.563719, acc.: 63.28%] [G loss: 0.771695]\n",
      "epoch:21 step:20123 [D loss: 0.530487, acc.: 75.78%] [G loss: 0.919548]\n",
      "epoch:21 step:20124 [D loss: 0.503181, acc.: 74.22%] [G loss: 0.797597]\n",
      "epoch:21 step:20125 [D loss: 0.535962, acc.: 74.22%] [G loss: 0.738024]\n",
      "epoch:21 step:20126 [D loss: 0.499635, acc.: 75.00%] [G loss: 0.634018]\n",
      "epoch:21 step:20127 [D loss: 0.565122, acc.: 71.09%] [G loss: 0.694975]\n",
      "epoch:21 step:20128 [D loss: 0.378410, acc.: 85.16%] [G loss: 0.816938]\n",
      "epoch:21 step:20129 [D loss: 0.463586, acc.: 78.12%] [G loss: 1.051376]\n",
      "epoch:21 step:20130 [D loss: 0.494231, acc.: 73.44%] [G loss: 0.895090]\n",
      "epoch:21 step:20131 [D loss: 0.572150, acc.: 67.97%] [G loss: 0.579225]\n",
      "epoch:21 step:20132 [D loss: 0.621246, acc.: 67.19%] [G loss: 0.721265]\n",
      "epoch:21 step:20133 [D loss: 0.687826, acc.: 54.69%] [G loss: 0.545266]\n",
      "epoch:21 step:20134 [D loss: 0.465586, acc.: 82.03%] [G loss: 0.644558]\n",
      "epoch:21 step:20135 [D loss: 0.628265, acc.: 68.75%] [G loss: 0.647177]\n",
      "epoch:21 step:20136 [D loss: 0.522668, acc.: 71.09%] [G loss: 0.577354]\n",
      "epoch:21 step:20137 [D loss: 0.539720, acc.: 66.41%] [G loss: 0.696066]\n",
      "epoch:21 step:20138 [D loss: 0.501562, acc.: 73.44%] [G loss: 0.783551]\n",
      "epoch:21 step:20139 [D loss: 0.551979, acc.: 68.75%] [G loss: 0.602946]\n",
      "epoch:21 step:20140 [D loss: 0.556084, acc.: 74.22%] [G loss: 0.559274]\n",
      "epoch:21 step:20141 [D loss: 0.479489, acc.: 76.56%] [G loss: 0.674971]\n",
      "epoch:21 step:20142 [D loss: 0.608422, acc.: 66.41%] [G loss: 0.556115]\n",
      "epoch:21 step:20143 [D loss: 0.562071, acc.: 67.97%] [G loss: 0.630312]\n",
      "epoch:21 step:20144 [D loss: 0.592756, acc.: 67.19%] [G loss: 0.656779]\n",
      "epoch:21 step:20145 [D loss: 0.569650, acc.: 71.09%] [G loss: 0.676065]\n",
      "epoch:21 step:20146 [D loss: 0.565059, acc.: 65.62%] [G loss: 0.723777]\n",
      "epoch:21 step:20147 [D loss: 0.554348, acc.: 68.75%] [G loss: 0.616483]\n",
      "epoch:21 step:20148 [D loss: 0.441337, acc.: 82.03%] [G loss: 0.855868]\n",
      "epoch:21 step:20149 [D loss: 0.498156, acc.: 78.12%] [G loss: 0.880684]\n",
      "epoch:21 step:20150 [D loss: 0.669308, acc.: 60.94%] [G loss: 0.770367]\n",
      "epoch:21 step:20151 [D loss: 0.615739, acc.: 63.28%] [G loss: 0.658029]\n",
      "epoch:21 step:20152 [D loss: 0.489156, acc.: 75.00%] [G loss: 0.711998]\n",
      "epoch:21 step:20153 [D loss: 0.532265, acc.: 72.66%] [G loss: 0.691489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20154 [D loss: 0.611367, acc.: 63.28%] [G loss: 0.636597]\n",
      "epoch:21 step:20155 [D loss: 0.565853, acc.: 66.41%] [G loss: 0.452128]\n",
      "epoch:21 step:20156 [D loss: 0.550460, acc.: 71.09%] [G loss: 0.607736]\n",
      "epoch:21 step:20157 [D loss: 0.637335, acc.: 68.75%] [G loss: 0.534176]\n",
      "epoch:21 step:20158 [D loss: 0.467327, acc.: 80.47%] [G loss: 0.599641]\n",
      "epoch:21 step:20159 [D loss: 0.678682, acc.: 60.94%] [G loss: 0.615744]\n",
      "epoch:21 step:20160 [D loss: 0.505182, acc.: 74.22%] [G loss: 0.592287]\n",
      "epoch:21 step:20161 [D loss: 0.531082, acc.: 71.88%] [G loss: 0.492697]\n",
      "epoch:21 step:20162 [D loss: 0.490678, acc.: 75.00%] [G loss: 0.625469]\n",
      "epoch:21 step:20163 [D loss: 0.577982, acc.: 70.31%] [G loss: 0.543632]\n",
      "epoch:21 step:20164 [D loss: 0.544679, acc.: 69.53%] [G loss: 0.565907]\n",
      "epoch:21 step:20165 [D loss: 0.495093, acc.: 74.22%] [G loss: 0.652911]\n",
      "epoch:21 step:20166 [D loss: 0.577898, acc.: 66.41%] [G loss: 0.658186]\n",
      "epoch:21 step:20167 [D loss: 0.539172, acc.: 75.78%] [G loss: 0.728129]\n",
      "epoch:21 step:20168 [D loss: 0.522714, acc.: 73.44%] [G loss: 0.755053]\n",
      "epoch:21 step:20169 [D loss: 0.600718, acc.: 70.31%] [G loss: 0.593931]\n",
      "epoch:21 step:20170 [D loss: 0.603389, acc.: 65.62%] [G loss: 0.537265]\n",
      "epoch:21 step:20171 [D loss: 0.603471, acc.: 67.97%] [G loss: 0.580266]\n",
      "epoch:21 step:20172 [D loss: 0.492794, acc.: 75.78%] [G loss: 0.575471]\n",
      "epoch:21 step:20173 [D loss: 0.524216, acc.: 76.56%] [G loss: 0.760450]\n",
      "epoch:21 step:20174 [D loss: 0.559658, acc.: 70.31%] [G loss: 0.732117]\n",
      "epoch:21 step:20175 [D loss: 0.541022, acc.: 75.78%] [G loss: 0.699974]\n",
      "epoch:21 step:20176 [D loss: 0.508574, acc.: 74.22%] [G loss: 0.697851]\n",
      "epoch:21 step:20177 [D loss: 0.639750, acc.: 65.62%] [G loss: 0.487863]\n",
      "epoch:21 step:20178 [D loss: 0.670978, acc.: 60.16%] [G loss: 0.745126]\n",
      "epoch:21 step:20179 [D loss: 0.606652, acc.: 64.06%] [G loss: 0.701435]\n",
      "epoch:21 step:20180 [D loss: 0.490485, acc.: 75.00%] [G loss: 0.585936]\n",
      "epoch:21 step:20181 [D loss: 0.496349, acc.: 73.44%] [G loss: 0.733179]\n",
      "epoch:21 step:20182 [D loss: 0.488549, acc.: 78.12%] [G loss: 0.737066]\n",
      "epoch:21 step:20183 [D loss: 0.511881, acc.: 73.44%] [G loss: 0.629360]\n",
      "epoch:21 step:20184 [D loss: 0.534631, acc.: 72.66%] [G loss: 0.788970]\n",
      "epoch:21 step:20185 [D loss: 0.421844, acc.: 79.69%] [G loss: 0.880345]\n",
      "epoch:21 step:20186 [D loss: 0.513801, acc.: 73.44%] [G loss: 0.910098]\n",
      "epoch:21 step:20187 [D loss: 0.635439, acc.: 60.94%] [G loss: 0.691821]\n",
      "epoch:21 step:20188 [D loss: 0.700437, acc.: 54.69%] [G loss: 0.544367]\n",
      "epoch:21 step:20189 [D loss: 0.663825, acc.: 57.81%] [G loss: 0.451636]\n",
      "epoch:21 step:20190 [D loss: 0.529076, acc.: 73.44%] [G loss: 0.512208]\n",
      "epoch:21 step:20191 [D loss: 0.494867, acc.: 73.44%] [G loss: 0.692743]\n",
      "epoch:21 step:20192 [D loss: 0.565309, acc.: 67.97%] [G loss: 0.572452]\n",
      "epoch:21 step:20193 [D loss: 0.507368, acc.: 73.44%] [G loss: 0.792519]\n",
      "epoch:21 step:20194 [D loss: 0.483075, acc.: 76.56%] [G loss: 0.784545]\n",
      "epoch:21 step:20195 [D loss: 0.568938, acc.: 71.09%] [G loss: 0.692068]\n",
      "epoch:21 step:20196 [D loss: 0.520757, acc.: 72.66%] [G loss: 0.905314]\n",
      "epoch:21 step:20197 [D loss: 0.456227, acc.: 79.69%] [G loss: 0.783892]\n",
      "epoch:21 step:20198 [D loss: 0.500230, acc.: 72.66%] [G loss: 0.801385]\n",
      "epoch:21 step:20199 [D loss: 0.555000, acc.: 72.66%] [G loss: 0.882344]\n",
      "epoch:21 step:20200 [D loss: 0.458100, acc.: 78.91%] [G loss: 0.684023]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.793193\n",
      "FID: 42.093227\n",
      "0 = 12.919682163429265\n",
      "1 = 0.08966710513361069\n",
      "2 = 0.8639000058174133\n",
      "3 = 0.8199999928474426\n",
      "4 = 0.907800018787384\n",
      "5 = 0.8989256620407104\n",
      "6 = 0.8199999928474426\n",
      "7 = 8.004133750975145\n",
      "8 = 0.13554620660182826\n",
      "9 = 0.6988999843597412\n",
      "10 = 0.6953999996185303\n",
      "11 = 0.7024000287055969\n",
      "12 = 0.7003021240234375\n",
      "13 = 0.6953999996185303\n",
      "14 = 6.793222427368164\n",
      "15 = 7.1894917488098145\n",
      "16 = 0.37011685967445374\n",
      "17 = 6.793193340301514\n",
      "18 = 42.09322738647461\n",
      "epoch:21 step:20201 [D loss: 0.558464, acc.: 65.62%] [G loss: 0.620992]\n",
      "epoch:21 step:20202 [D loss: 0.591563, acc.: 70.31%] [G loss: 0.518533]\n",
      "epoch:21 step:20203 [D loss: 0.527853, acc.: 67.19%] [G loss: 0.567576]\n",
      "epoch:21 step:20204 [D loss: 0.548744, acc.: 71.09%] [G loss: 0.595325]\n",
      "epoch:21 step:20205 [D loss: 0.702783, acc.: 56.25%] [G loss: 0.555934]\n",
      "epoch:21 step:20206 [D loss: 0.591367, acc.: 61.72%] [G loss: 0.627403]\n",
      "epoch:21 step:20207 [D loss: 0.555603, acc.: 69.53%] [G loss: 0.636121]\n",
      "epoch:21 step:20208 [D loss: 0.598322, acc.: 68.75%] [G loss: 0.585151]\n",
      "epoch:21 step:20209 [D loss: 0.585053, acc.: 64.06%] [G loss: 0.583092]\n",
      "epoch:21 step:20210 [D loss: 0.526382, acc.: 67.97%] [G loss: 0.622131]\n",
      "epoch:21 step:20211 [D loss: 0.456458, acc.: 78.91%] [G loss: 0.717209]\n",
      "epoch:21 step:20212 [D loss: 0.651501, acc.: 67.19%] [G loss: 0.503638]\n",
      "epoch:21 step:20213 [D loss: 0.540538, acc.: 66.41%] [G loss: 0.586800]\n",
      "epoch:21 step:20214 [D loss: 0.545550, acc.: 71.88%] [G loss: 0.558462]\n",
      "epoch:21 step:20215 [D loss: 0.581396, acc.: 68.75%] [G loss: 0.525656]\n",
      "epoch:21 step:20216 [D loss: 0.545058, acc.: 70.31%] [G loss: 0.554642]\n",
      "epoch:21 step:20217 [D loss: 0.600266, acc.: 64.06%] [G loss: 0.605993]\n",
      "epoch:21 step:20218 [D loss: 0.534074, acc.: 73.44%] [G loss: 0.471075]\n",
      "epoch:21 step:20219 [D loss: 0.620926, acc.: 67.19%] [G loss: 0.511069]\n",
      "epoch:21 step:20220 [D loss: 0.550839, acc.: 73.44%] [G loss: 0.600928]\n",
      "epoch:21 step:20221 [D loss: 0.533180, acc.: 71.88%] [G loss: 0.516804]\n",
      "epoch:21 step:20222 [D loss: 0.578360, acc.: 74.22%] [G loss: 0.559872]\n",
      "epoch:21 step:20223 [D loss: 0.520869, acc.: 73.44%] [G loss: 0.665020]\n",
      "epoch:21 step:20224 [D loss: 0.545490, acc.: 70.31%] [G loss: 0.646501]\n",
      "epoch:21 step:20225 [D loss: 0.543922, acc.: 72.66%] [G loss: 0.811173]\n",
      "epoch:21 step:20226 [D loss: 0.545706, acc.: 70.31%] [G loss: 0.576946]\n",
      "epoch:21 step:20227 [D loss: 0.522443, acc.: 75.00%] [G loss: 0.659234]\n",
      "epoch:21 step:20228 [D loss: 0.524826, acc.: 75.78%] [G loss: 0.670362]\n",
      "epoch:21 step:20229 [D loss: 0.484658, acc.: 78.12%] [G loss: 0.606499]\n",
      "epoch:21 step:20230 [D loss: 0.562792, acc.: 69.53%] [G loss: 0.699748]\n",
      "epoch:21 step:20231 [D loss: 0.455727, acc.: 78.12%] [G loss: 0.626229]\n",
      "epoch:21 step:20232 [D loss: 0.532115, acc.: 73.44%] [G loss: 0.663923]\n",
      "epoch:21 step:20233 [D loss: 0.532981, acc.: 74.22%] [G loss: 0.738283]\n",
      "epoch:21 step:20234 [D loss: 0.507575, acc.: 71.88%] [G loss: 0.782762]\n",
      "epoch:21 step:20235 [D loss: 0.462818, acc.: 79.69%] [G loss: 0.697123]\n",
      "epoch:21 step:20236 [D loss: 0.600302, acc.: 67.19%] [G loss: 0.608543]\n",
      "epoch:21 step:20237 [D loss: 0.534509, acc.: 72.66%] [G loss: 0.500910]\n",
      "epoch:21 step:20238 [D loss: 0.563558, acc.: 70.31%] [G loss: 0.515837]\n",
      "epoch:21 step:20239 [D loss: 0.560632, acc.: 67.97%] [G loss: 0.654840]\n",
      "epoch:21 step:20240 [D loss: 0.591068, acc.: 67.19%] [G loss: 0.664009]\n",
      "epoch:21 step:20241 [D loss: 0.496670, acc.: 75.00%] [G loss: 0.652125]\n",
      "epoch:21 step:20242 [D loss: 0.592754, acc.: 72.66%] [G loss: 0.583041]\n",
      "epoch:21 step:20243 [D loss: 0.673860, acc.: 63.28%] [G loss: 0.636079]\n",
      "epoch:21 step:20244 [D loss: 0.468672, acc.: 81.25%] [G loss: 0.802276]\n",
      "epoch:21 step:20245 [D loss: 0.517142, acc.: 71.88%] [G loss: 0.636564]\n",
      "epoch:21 step:20246 [D loss: 0.567474, acc.: 68.75%] [G loss: 0.537803]\n",
      "epoch:21 step:20247 [D loss: 0.554060, acc.: 70.31%] [G loss: 0.540521]\n",
      "epoch:21 step:20248 [D loss: 0.502804, acc.: 78.12%] [G loss: 0.556441]\n",
      "epoch:21 step:20249 [D loss: 0.567988, acc.: 70.31%] [G loss: 0.613785]\n",
      "epoch:21 step:20250 [D loss: 0.506851, acc.: 73.44%] [G loss: 0.629163]\n",
      "epoch:21 step:20251 [D loss: 0.420769, acc.: 80.47%] [G loss: 0.721692]\n",
      "epoch:21 step:20252 [D loss: 0.462400, acc.: 76.56%] [G loss: 0.772413]\n",
      "epoch:21 step:20253 [D loss: 0.689241, acc.: 57.81%] [G loss: 0.669217]\n",
      "epoch:21 step:20254 [D loss: 0.582244, acc.: 67.19%] [G loss: 0.737055]\n",
      "epoch:21 step:20255 [D loss: 0.564211, acc.: 67.97%] [G loss: 0.688271]\n",
      "epoch:21 step:20256 [D loss: 0.578964, acc.: 68.75%] [G loss: 0.576206]\n",
      "epoch:21 step:20257 [D loss: 0.574610, acc.: 67.97%] [G loss: 0.617708]\n",
      "epoch:21 step:20258 [D loss: 0.506600, acc.: 72.66%] [G loss: 0.794818]\n",
      "epoch:21 step:20259 [D loss: 0.470832, acc.: 75.78%] [G loss: 0.805911]\n",
      "epoch:21 step:20260 [D loss: 0.588963, acc.: 66.41%] [G loss: 0.776239]\n",
      "epoch:21 step:20261 [D loss: 0.617527, acc.: 60.94%] [G loss: 0.646241]\n",
      "epoch:21 step:20262 [D loss: 0.569492, acc.: 68.75%] [G loss: 0.582130]\n",
      "epoch:21 step:20263 [D loss: 0.595523, acc.: 67.19%] [G loss: 0.498009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20264 [D loss: 0.534863, acc.: 73.44%] [G loss: 0.604269]\n",
      "epoch:21 step:20265 [D loss: 0.522973, acc.: 67.97%] [G loss: 0.549290]\n",
      "epoch:21 step:20266 [D loss: 0.527007, acc.: 68.75%] [G loss: 0.630849]\n",
      "epoch:21 step:20267 [D loss: 0.575556, acc.: 69.53%] [G loss: 0.723869]\n",
      "epoch:21 step:20268 [D loss: 0.586360, acc.: 72.66%] [G loss: 0.503518]\n",
      "epoch:21 step:20269 [D loss: 0.485065, acc.: 77.34%] [G loss: 0.665983]\n",
      "epoch:21 step:20270 [D loss: 0.512730, acc.: 73.44%] [G loss: 0.598610]\n",
      "epoch:21 step:20271 [D loss: 0.546839, acc.: 74.22%] [G loss: 0.597160]\n",
      "epoch:21 step:20272 [D loss: 0.580757, acc.: 66.41%] [G loss: 0.556047]\n",
      "epoch:21 step:20273 [D loss: 0.554188, acc.: 68.75%] [G loss: 0.649224]\n",
      "epoch:21 step:20274 [D loss: 0.537379, acc.: 71.88%] [G loss: 0.631492]\n",
      "epoch:21 step:20275 [D loss: 0.504193, acc.: 74.22%] [G loss: 0.642791]\n",
      "epoch:21 step:20276 [D loss: 0.517372, acc.: 71.88%] [G loss: 0.786301]\n",
      "epoch:21 step:20277 [D loss: 0.591216, acc.: 68.75%] [G loss: 0.590795]\n",
      "epoch:21 step:20278 [D loss: 0.493218, acc.: 70.31%] [G loss: 0.820531]\n",
      "epoch:21 step:20279 [D loss: 0.524245, acc.: 67.97%] [G loss: 0.549619]\n",
      "epoch:21 step:20280 [D loss: 0.495166, acc.: 75.78%] [G loss: 0.598728]\n",
      "epoch:21 step:20281 [D loss: 0.471841, acc.: 78.12%] [G loss: 0.818321]\n",
      "epoch:21 step:20282 [D loss: 0.446543, acc.: 79.69%] [G loss: 0.720193]\n",
      "epoch:21 step:20283 [D loss: 0.597670, acc.: 64.84%] [G loss: 0.459841]\n",
      "epoch:21 step:20284 [D loss: 0.544580, acc.: 67.19%] [G loss: 0.647162]\n",
      "epoch:21 step:20285 [D loss: 0.553518, acc.: 71.09%] [G loss: 0.583526]\n",
      "epoch:21 step:20286 [D loss: 0.532297, acc.: 70.31%] [G loss: 0.632644]\n",
      "epoch:21 step:20287 [D loss: 0.546672, acc.: 67.97%] [G loss: 0.513759]\n",
      "epoch:21 step:20288 [D loss: 0.529386, acc.: 72.66%] [G loss: 0.540060]\n",
      "epoch:21 step:20289 [D loss: 0.558634, acc.: 71.09%] [G loss: 0.510171]\n",
      "epoch:21 step:20290 [D loss: 0.497081, acc.: 73.44%] [G loss: 0.620046]\n",
      "epoch:21 step:20291 [D loss: 0.539233, acc.: 68.75%] [G loss: 0.744980]\n",
      "epoch:21 step:20292 [D loss: 0.614451, acc.: 60.16%] [G loss: 0.613145]\n",
      "epoch:21 step:20293 [D loss: 0.529566, acc.: 71.88%] [G loss: 0.602244]\n",
      "epoch:21 step:20294 [D loss: 0.515163, acc.: 73.44%] [G loss: 0.919866]\n",
      "epoch:21 step:20295 [D loss: 0.617115, acc.: 67.97%] [G loss: 0.655977]\n",
      "epoch:21 step:20296 [D loss: 0.510069, acc.: 73.44%] [G loss: 0.661418]\n",
      "epoch:21 step:20297 [D loss: 0.535080, acc.: 72.66%] [G loss: 0.833123]\n",
      "epoch:21 step:20298 [D loss: 0.573552, acc.: 67.19%] [G loss: 0.769346]\n",
      "epoch:21 step:20299 [D loss: 0.562785, acc.: 77.34%] [G loss: 0.541211]\n",
      "epoch:21 step:20300 [D loss: 0.485329, acc.: 77.34%] [G loss: 0.633544]\n",
      "epoch:21 step:20301 [D loss: 0.457457, acc.: 79.69%] [G loss: 0.696055]\n",
      "epoch:21 step:20302 [D loss: 0.559991, acc.: 64.84%] [G loss: 0.655865]\n",
      "epoch:21 step:20303 [D loss: 0.527358, acc.: 70.31%] [G loss: 0.623644]\n",
      "epoch:21 step:20304 [D loss: 0.481948, acc.: 73.44%] [G loss: 0.622812]\n",
      "epoch:21 step:20305 [D loss: 0.561994, acc.: 71.09%] [G loss: 0.747644]\n",
      "epoch:21 step:20306 [D loss: 0.597591, acc.: 66.41%] [G loss: 0.587985]\n",
      "epoch:21 step:20307 [D loss: 0.534963, acc.: 71.09%] [G loss: 0.789940]\n",
      "epoch:21 step:20308 [D loss: 0.470826, acc.: 79.69%] [G loss: 0.776969]\n",
      "epoch:21 step:20309 [D loss: 0.485362, acc.: 78.12%] [G loss: 0.762279]\n",
      "epoch:21 step:20310 [D loss: 0.486763, acc.: 75.78%] [G loss: 0.824421]\n",
      "epoch:21 step:20311 [D loss: 0.524287, acc.: 71.09%] [G loss: 0.816072]\n",
      "epoch:21 step:20312 [D loss: 0.539360, acc.: 72.66%] [G loss: 0.925494]\n",
      "epoch:21 step:20313 [D loss: 0.577431, acc.: 67.97%] [G loss: 0.716732]\n",
      "epoch:21 step:20314 [D loss: 0.556023, acc.: 69.53%] [G loss: 0.588804]\n",
      "epoch:21 step:20315 [D loss: 0.547951, acc.: 67.97%] [G loss: 0.564107]\n",
      "epoch:21 step:20316 [D loss: 0.496051, acc.: 71.88%] [G loss: 0.546561]\n",
      "epoch:21 step:20317 [D loss: 0.602464, acc.: 66.41%] [G loss: 0.623121]\n",
      "epoch:21 step:20318 [D loss: 0.486911, acc.: 76.56%] [G loss: 0.788721]\n",
      "epoch:21 step:20319 [D loss: 0.531037, acc.: 71.09%] [G loss: 0.843342]\n",
      "epoch:21 step:20320 [D loss: 0.571915, acc.: 68.75%] [G loss: 0.831733]\n",
      "epoch:21 step:20321 [D loss: 0.565628, acc.: 65.62%] [G loss: 0.573928]\n",
      "epoch:21 step:20322 [D loss: 0.544255, acc.: 71.88%] [G loss: 0.505149]\n",
      "epoch:21 step:20323 [D loss: 0.475148, acc.: 79.69%] [G loss: 0.571386]\n",
      "epoch:21 step:20324 [D loss: 0.410571, acc.: 85.94%] [G loss: 0.781215]\n",
      "epoch:21 step:20325 [D loss: 0.435874, acc.: 77.34%] [G loss: 0.866873]\n",
      "epoch:21 step:20326 [D loss: 0.468390, acc.: 75.00%] [G loss: 0.893736]\n",
      "epoch:21 step:20327 [D loss: 0.518126, acc.: 75.00%] [G loss: 0.749733]\n",
      "epoch:21 step:20328 [D loss: 0.512097, acc.: 72.66%] [G loss: 0.856760]\n",
      "epoch:21 step:20329 [D loss: 0.611212, acc.: 64.84%] [G loss: 0.649718]\n",
      "epoch:21 step:20330 [D loss: 0.510937, acc.: 76.56%] [G loss: 0.620365]\n",
      "epoch:21 step:20331 [D loss: 0.488885, acc.: 74.22%] [G loss: 0.636418]\n",
      "epoch:21 step:20332 [D loss: 0.588646, acc.: 70.31%] [G loss: 0.676484]\n",
      "epoch:21 step:20333 [D loss: 0.553037, acc.: 73.44%] [G loss: 0.722372]\n",
      "epoch:21 step:20334 [D loss: 0.497424, acc.: 73.44%] [G loss: 0.654041]\n",
      "epoch:21 step:20335 [D loss: 0.521739, acc.: 73.44%] [G loss: 0.723188]\n",
      "epoch:21 step:20336 [D loss: 0.526321, acc.: 71.09%] [G loss: 0.701958]\n",
      "epoch:21 step:20337 [D loss: 0.534474, acc.: 74.22%] [G loss: 0.777921]\n",
      "epoch:21 step:20338 [D loss: 0.465605, acc.: 75.00%] [G loss: 0.706814]\n",
      "epoch:21 step:20339 [D loss: 0.505624, acc.: 74.22%] [G loss: 0.694620]\n",
      "epoch:21 step:20340 [D loss: 0.582652, acc.: 66.41%] [G loss: 0.618220]\n",
      "epoch:21 step:20341 [D loss: 0.565091, acc.: 65.62%] [G loss: 0.694629]\n",
      "epoch:21 step:20342 [D loss: 0.599508, acc.: 64.84%] [G loss: 0.565486]\n",
      "epoch:21 step:20343 [D loss: 0.527463, acc.: 72.66%] [G loss: 0.571837]\n",
      "epoch:21 step:20344 [D loss: 0.525598, acc.: 71.88%] [G loss: 0.687585]\n",
      "epoch:21 step:20345 [D loss: 0.585486, acc.: 69.53%] [G loss: 0.565996]\n",
      "epoch:21 step:20346 [D loss: 0.503494, acc.: 74.22%] [G loss: 0.749887]\n",
      "epoch:21 step:20347 [D loss: 0.510365, acc.: 71.88%] [G loss: 0.672381]\n",
      "epoch:21 step:20348 [D loss: 0.537962, acc.: 72.66%] [G loss: 0.682049]\n",
      "epoch:21 step:20349 [D loss: 0.528361, acc.: 71.09%] [G loss: 0.741782]\n",
      "epoch:21 step:20350 [D loss: 0.600109, acc.: 68.75%] [G loss: 0.590478]\n",
      "epoch:21 step:20351 [D loss: 0.605326, acc.: 63.28%] [G loss: 0.565497]\n",
      "epoch:21 step:20352 [D loss: 0.575609, acc.: 68.75%] [G loss: 0.658080]\n",
      "epoch:21 step:20353 [D loss: 0.520233, acc.: 71.88%] [G loss: 0.781487]\n",
      "epoch:21 step:20354 [D loss: 0.482024, acc.: 78.91%] [G loss: 0.674893]\n",
      "epoch:21 step:20355 [D loss: 0.560471, acc.: 71.88%] [G loss: 0.525891]\n",
      "epoch:21 step:20356 [D loss: 0.529949, acc.: 69.53%] [G loss: 0.618304]\n",
      "epoch:21 step:20357 [D loss: 0.532101, acc.: 72.66%] [G loss: 0.549245]\n",
      "epoch:21 step:20358 [D loss: 0.465794, acc.: 78.12%] [G loss: 0.593549]\n",
      "epoch:21 step:20359 [D loss: 0.516755, acc.: 71.88%] [G loss: 0.616960]\n",
      "epoch:21 step:20360 [D loss: 0.560718, acc.: 71.09%] [G loss: 0.630683]\n",
      "epoch:21 step:20361 [D loss: 0.617383, acc.: 63.28%] [G loss: 0.548421]\n",
      "epoch:21 step:20362 [D loss: 0.487329, acc.: 79.69%] [G loss: 0.715558]\n",
      "epoch:21 step:20363 [D loss: 0.582686, acc.: 67.19%] [G loss: 0.684125]\n",
      "epoch:21 step:20364 [D loss: 0.591857, acc.: 67.19%] [G loss: 0.529648]\n",
      "epoch:21 step:20365 [D loss: 0.557078, acc.: 67.19%] [G loss: 0.625697]\n",
      "epoch:21 step:20366 [D loss: 0.541429, acc.: 70.31%] [G loss: 0.663837]\n",
      "epoch:21 step:20367 [D loss: 0.482429, acc.: 77.34%] [G loss: 0.631169]\n",
      "epoch:21 step:20368 [D loss: 0.533099, acc.: 75.00%] [G loss: 0.614989]\n",
      "epoch:21 step:20369 [D loss: 0.547714, acc.: 72.66%] [G loss: 0.725679]\n",
      "epoch:21 step:20370 [D loss: 0.475953, acc.: 78.91%] [G loss: 0.721119]\n",
      "epoch:21 step:20371 [D loss: 0.505723, acc.: 78.12%] [G loss: 0.789337]\n",
      "epoch:21 step:20372 [D loss: 0.537181, acc.: 74.22%] [G loss: 0.629772]\n",
      "epoch:21 step:20373 [D loss: 0.629438, acc.: 59.38%] [G loss: 0.502014]\n",
      "epoch:21 step:20374 [D loss: 0.544789, acc.: 67.19%] [G loss: 0.434108]\n",
      "epoch:21 step:20375 [D loss: 0.687510, acc.: 54.69%] [G loss: 0.557363]\n",
      "epoch:21 step:20376 [D loss: 0.507951, acc.: 75.00%] [G loss: 0.802099]\n",
      "epoch:21 step:20377 [D loss: 0.521657, acc.: 71.88%] [G loss: 0.910517]\n",
      "epoch:21 step:20378 [D loss: 0.551682, acc.: 71.09%] [G loss: 0.863701]\n",
      "epoch:21 step:20379 [D loss: 0.651515, acc.: 62.50%] [G loss: 0.588715]\n",
      "epoch:21 step:20380 [D loss: 0.601132, acc.: 66.41%] [G loss: 0.597869]\n",
      "epoch:21 step:20381 [D loss: 0.614773, acc.: 62.50%] [G loss: 0.689037]\n",
      "epoch:21 step:20382 [D loss: 0.551707, acc.: 74.22%] [G loss: 0.546972]\n",
      "epoch:21 step:20383 [D loss: 0.531842, acc.: 73.44%] [G loss: 0.682195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20384 [D loss: 0.487698, acc.: 79.69%] [G loss: 0.795014]\n",
      "epoch:21 step:20385 [D loss: 0.453115, acc.: 79.69%] [G loss: 0.752346]\n",
      "epoch:21 step:20386 [D loss: 0.499860, acc.: 71.88%] [G loss: 0.720368]\n",
      "epoch:21 step:20387 [D loss: 0.574439, acc.: 67.19%] [G loss: 0.603089]\n",
      "epoch:21 step:20388 [D loss: 0.561578, acc.: 64.06%] [G loss: 0.645367]\n",
      "epoch:21 step:20389 [D loss: 0.577077, acc.: 69.53%] [G loss: 0.605504]\n",
      "epoch:21 step:20390 [D loss: 0.568915, acc.: 67.97%] [G loss: 0.588836]\n",
      "epoch:21 step:20391 [D loss: 0.597522, acc.: 68.75%] [G loss: 0.800370]\n",
      "epoch:21 step:20392 [D loss: 0.529927, acc.: 71.09%] [G loss: 0.702507]\n",
      "epoch:21 step:20393 [D loss: 0.621296, acc.: 63.28%] [G loss: 0.577520]\n",
      "epoch:21 step:20394 [D loss: 0.552335, acc.: 73.44%] [G loss: 0.531695]\n",
      "epoch:21 step:20395 [D loss: 0.554700, acc.: 70.31%] [G loss: 0.646944]\n",
      "epoch:21 step:20396 [D loss: 0.481286, acc.: 75.00%] [G loss: 0.641305]\n",
      "epoch:21 step:20397 [D loss: 0.547962, acc.: 68.75%] [G loss: 0.701504]\n",
      "epoch:21 step:20398 [D loss: 0.585186, acc.: 65.62%] [G loss: 0.535636]\n",
      "epoch:21 step:20399 [D loss: 0.618376, acc.: 61.72%] [G loss: 0.538174]\n",
      "epoch:21 step:20400 [D loss: 0.587050, acc.: 69.53%] [G loss: 0.591032]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.618131\n",
      "FID: 47.354080\n",
      "0 = 12.722250199699385\n",
      "1 = 0.08321507327831498\n",
      "2 = 0.8633000254631042\n",
      "3 = 0.8029999732971191\n",
      "4 = 0.9236000180244446\n",
      "5 = 0.9131225943565369\n",
      "6 = 0.8029999732971191\n",
      "7 = 8.178591517543788\n",
      "8 = 0.14840697928414331\n",
      "9 = 0.7095000147819519\n",
      "10 = 0.6904000043869019\n",
      "11 = 0.728600025177002\n",
      "12 = 0.7178207635879517\n",
      "13 = 0.6904000043869019\n",
      "14 = 6.6181535720825195\n",
      "15 = 7.314847469329834\n",
      "16 = 0.36998793482780457\n",
      "17 = 6.618131160736084\n",
      "18 = 47.35408020019531\n",
      "epoch:21 step:20401 [D loss: 0.517894, acc.: 74.22%] [G loss: 0.584957]\n",
      "epoch:21 step:20402 [D loss: 0.510641, acc.: 72.66%] [G loss: 0.720954]\n",
      "epoch:21 step:20403 [D loss: 0.509844, acc.: 77.34%] [G loss: 0.643589]\n",
      "epoch:21 step:20404 [D loss: 0.513053, acc.: 74.22%] [G loss: 0.694526]\n",
      "epoch:21 step:20405 [D loss: 0.506739, acc.: 74.22%] [G loss: 0.778558]\n",
      "epoch:21 step:20406 [D loss: 0.605224, acc.: 64.06%] [G loss: 0.531950]\n",
      "epoch:21 step:20407 [D loss: 0.497187, acc.: 71.88%] [G loss: 0.545792]\n",
      "epoch:21 step:20408 [D loss: 0.599121, acc.: 66.41%] [G loss: 0.570670]\n",
      "epoch:21 step:20409 [D loss: 0.585794, acc.: 66.41%] [G loss: 0.612022]\n",
      "epoch:21 step:20410 [D loss: 0.565386, acc.: 73.44%] [G loss: 0.599625]\n",
      "epoch:21 step:20411 [D loss: 0.529033, acc.: 71.09%] [G loss: 0.591248]\n",
      "epoch:21 step:20412 [D loss: 0.569183, acc.: 64.84%] [G loss: 0.610431]\n",
      "epoch:21 step:20413 [D loss: 0.501094, acc.: 71.09%] [G loss: 0.775024]\n",
      "epoch:21 step:20414 [D loss: 0.544244, acc.: 67.19%] [G loss: 0.551160]\n",
      "epoch:21 step:20415 [D loss: 0.600426, acc.: 61.72%] [G loss: 0.532157]\n",
      "epoch:21 step:20416 [D loss: 0.554238, acc.: 72.66%] [G loss: 0.597286]\n",
      "epoch:21 step:20417 [D loss: 0.640341, acc.: 64.06%] [G loss: 0.532311]\n",
      "epoch:21 step:20418 [D loss: 0.522326, acc.: 73.44%] [G loss: 0.599168]\n",
      "epoch:21 step:20419 [D loss: 0.464422, acc.: 78.91%] [G loss: 0.649947]\n",
      "epoch:21 step:20420 [D loss: 0.505409, acc.: 75.78%] [G loss: 0.866271]\n",
      "epoch:21 step:20421 [D loss: 0.625990, acc.: 70.31%] [G loss: 0.592458]\n",
      "epoch:21 step:20422 [D loss: 0.572070, acc.: 66.41%] [G loss: 0.650923]\n",
      "epoch:21 step:20423 [D loss: 0.513017, acc.: 74.22%] [G loss: 0.815869]\n",
      "epoch:21 step:20424 [D loss: 0.434488, acc.: 81.25%] [G loss: 0.741311]\n",
      "epoch:21 step:20425 [D loss: 0.571999, acc.: 69.53%] [G loss: 0.740083]\n",
      "epoch:21 step:20426 [D loss: 0.530528, acc.: 75.00%] [G loss: 0.873618]\n",
      "epoch:21 step:20427 [D loss: 0.517811, acc.: 71.88%] [G loss: 0.712023]\n",
      "epoch:21 step:20428 [D loss: 0.492198, acc.: 75.00%] [G loss: 0.779888]\n",
      "epoch:21 step:20429 [D loss: 0.540811, acc.: 69.53%] [G loss: 0.782321]\n",
      "epoch:21 step:20430 [D loss: 0.532080, acc.: 78.12%] [G loss: 0.770367]\n",
      "epoch:21 step:20431 [D loss: 0.560069, acc.: 72.66%] [G loss: 0.732361]\n",
      "epoch:21 step:20432 [D loss: 0.542020, acc.: 74.22%] [G loss: 0.725670]\n",
      "epoch:21 step:20433 [D loss: 0.585558, acc.: 67.97%] [G loss: 0.708881]\n",
      "epoch:21 step:20434 [D loss: 0.557909, acc.: 65.62%] [G loss: 0.740858]\n",
      "epoch:21 step:20435 [D loss: 0.520188, acc.: 76.56%] [G loss: 0.699573]\n",
      "epoch:21 step:20436 [D loss: 0.586311, acc.: 61.72%] [G loss: 0.522826]\n",
      "epoch:21 step:20437 [D loss: 0.530565, acc.: 71.88%] [G loss: 0.580774]\n",
      "epoch:21 step:20438 [D loss: 0.579545, acc.: 66.41%] [G loss: 0.436780]\n",
      "epoch:21 step:20439 [D loss: 0.566754, acc.: 69.53%] [G loss: 0.502933]\n",
      "epoch:21 step:20440 [D loss: 0.536654, acc.: 75.78%] [G loss: 0.636018]\n",
      "epoch:21 step:20441 [D loss: 0.556189, acc.: 68.75%] [G loss: 0.551605]\n",
      "epoch:21 step:20442 [D loss: 0.588293, acc.: 63.28%] [G loss: 0.600542]\n",
      "epoch:21 step:20443 [D loss: 0.664634, acc.: 59.38%] [G loss: 0.465193]\n",
      "epoch:21 step:20444 [D loss: 0.544192, acc.: 72.66%] [G loss: 0.525596]\n",
      "epoch:21 step:20445 [D loss: 0.561725, acc.: 72.66%] [G loss: 0.648064]\n",
      "epoch:21 step:20446 [D loss: 0.418036, acc.: 81.25%] [G loss: 0.762179]\n",
      "epoch:21 step:20447 [D loss: 0.613317, acc.: 63.28%] [G loss: 0.837624]\n",
      "epoch:21 step:20448 [D loss: 0.524899, acc.: 73.44%] [G loss: 0.785455]\n",
      "epoch:21 step:20449 [D loss: 0.551799, acc.: 71.88%] [G loss: 0.730629]\n",
      "epoch:21 step:20450 [D loss: 0.567459, acc.: 68.75%] [G loss: 0.852153]\n",
      "epoch:21 step:20451 [D loss: 0.574275, acc.: 69.53%] [G loss: 0.739178]\n",
      "epoch:21 step:20452 [D loss: 0.535572, acc.: 74.22%] [G loss: 0.808860]\n",
      "epoch:21 step:20453 [D loss: 0.544959, acc.: 68.75%] [G loss: 0.597497]\n",
      "epoch:21 step:20454 [D loss: 0.542679, acc.: 73.44%] [G loss: 0.645768]\n",
      "epoch:21 step:20455 [D loss: 0.542371, acc.: 71.88%] [G loss: 0.620119]\n",
      "epoch:21 step:20456 [D loss: 0.562478, acc.: 68.75%] [G loss: 0.634260]\n",
      "epoch:21 step:20457 [D loss: 0.539455, acc.: 70.31%] [G loss: 0.549622]\n",
      "epoch:21 step:20458 [D loss: 0.568779, acc.: 69.53%] [G loss: 0.797864]\n",
      "epoch:21 step:20459 [D loss: 0.512196, acc.: 72.66%] [G loss: 0.901945]\n",
      "epoch:21 step:20460 [D loss: 0.561970, acc.: 67.97%] [G loss: 0.691875]\n",
      "epoch:21 step:20461 [D loss: 0.623292, acc.: 60.16%] [G loss: 0.724460]\n",
      "epoch:21 step:20462 [D loss: 0.548658, acc.: 67.97%] [G loss: 0.678191]\n",
      "epoch:21 step:20463 [D loss: 0.552949, acc.: 70.31%] [G loss: 0.560461]\n",
      "epoch:21 step:20464 [D loss: 0.638255, acc.: 64.84%] [G loss: 0.546865]\n",
      "epoch:21 step:20465 [D loss: 0.643461, acc.: 60.16%] [G loss: 0.487280]\n",
      "epoch:21 step:20466 [D loss: 0.524932, acc.: 67.97%] [G loss: 0.613633]\n",
      "epoch:21 step:20467 [D loss: 0.554179, acc.: 66.41%] [G loss: 0.588588]\n",
      "epoch:21 step:20468 [D loss: 0.556595, acc.: 62.50%] [G loss: 0.634275]\n",
      "epoch:21 step:20469 [D loss: 0.444576, acc.: 75.00%] [G loss: 0.741285]\n",
      "epoch:21 step:20470 [D loss: 0.587793, acc.: 64.84%] [G loss: 0.728937]\n",
      "epoch:21 step:20471 [D loss: 0.708466, acc.: 55.47%] [G loss: 0.557687]\n",
      "epoch:21 step:20472 [D loss: 0.536650, acc.: 69.53%] [G loss: 0.679021]\n",
      "epoch:21 step:20473 [D loss: 0.494293, acc.: 70.31%] [G loss: 0.659314]\n",
      "epoch:21 step:20474 [D loss: 0.529194, acc.: 70.31%] [G loss: 0.590712]\n",
      "epoch:21 step:20475 [D loss: 0.545681, acc.: 74.22%] [G loss: 0.594494]\n",
      "epoch:21 step:20476 [D loss: 0.592337, acc.: 64.06%] [G loss: 0.820446]\n",
      "epoch:21 step:20477 [D loss: 0.622245, acc.: 61.72%] [G loss: 0.577069]\n",
      "epoch:21 step:20478 [D loss: 0.538724, acc.: 71.88%] [G loss: 0.827540]\n",
      "epoch:21 step:20479 [D loss: 0.498938, acc.: 77.34%] [G loss: 1.064675]\n",
      "epoch:21 step:20480 [D loss: 0.569838, acc.: 71.88%] [G loss: 0.705726]\n",
      "epoch:21 step:20481 [D loss: 0.585731, acc.: 69.53%] [G loss: 0.590606]\n",
      "epoch:21 step:20482 [D loss: 0.583578, acc.: 64.84%] [G loss: 0.483742]\n",
      "epoch:21 step:20483 [D loss: 0.596057, acc.: 64.06%] [G loss: 0.601067]\n",
      "epoch:21 step:20484 [D loss: 0.524146, acc.: 74.22%] [G loss: 0.606976]\n",
      "epoch:21 step:20485 [D loss: 0.556908, acc.: 71.88%] [G loss: 0.666918]\n",
      "epoch:21 step:20486 [D loss: 0.548495, acc.: 71.88%] [G loss: 0.577083]\n",
      "epoch:21 step:20487 [D loss: 0.524298, acc.: 69.53%] [G loss: 0.629851]\n",
      "epoch:21 step:20488 [D loss: 0.602530, acc.: 67.19%] [G loss: 0.675560]\n",
      "epoch:21 step:20489 [D loss: 0.659418, acc.: 60.94%] [G loss: 0.597236]\n",
      "epoch:21 step:20490 [D loss: 0.530802, acc.: 76.56%] [G loss: 0.551928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20491 [D loss: 0.485333, acc.: 75.78%] [G loss: 0.745964]\n",
      "epoch:21 step:20492 [D loss: 0.441603, acc.: 75.78%] [G loss: 0.893173]\n",
      "epoch:21 step:20493 [D loss: 0.579133, acc.: 64.84%] [G loss: 0.729370]\n",
      "epoch:21 step:20494 [D loss: 0.587214, acc.: 70.31%] [G loss: 0.639234]\n",
      "epoch:21 step:20495 [D loss: 0.627292, acc.: 62.50%] [G loss: 0.506547]\n",
      "epoch:21 step:20496 [D loss: 0.551700, acc.: 72.66%] [G loss: 0.586989]\n",
      "epoch:21 step:20497 [D loss: 0.616508, acc.: 69.53%] [G loss: 0.617451]\n",
      "epoch:21 step:20498 [D loss: 0.578767, acc.: 65.62%] [G loss: 0.462139]\n",
      "epoch:21 step:20499 [D loss: 0.540616, acc.: 68.75%] [G loss: 0.727997]\n",
      "epoch:21 step:20500 [D loss: 0.435566, acc.: 77.34%] [G loss: 0.625427]\n",
      "epoch:21 step:20501 [D loss: 0.580752, acc.: 69.53%] [G loss: 0.613634]\n",
      "epoch:21 step:20502 [D loss: 0.513157, acc.: 74.22%] [G loss: 0.635250]\n",
      "epoch:21 step:20503 [D loss: 0.553757, acc.: 71.88%] [G loss: 0.578600]\n",
      "epoch:21 step:20504 [D loss: 0.592217, acc.: 67.97%] [G loss: 0.552281]\n",
      "epoch:21 step:20505 [D loss: 0.604492, acc.: 64.84%] [G loss: 0.589945]\n",
      "epoch:21 step:20506 [D loss: 0.526650, acc.: 71.09%] [G loss: 0.651944]\n",
      "epoch:21 step:20507 [D loss: 0.599462, acc.: 63.28%] [G loss: 0.643642]\n",
      "epoch:21 step:20508 [D loss: 0.546309, acc.: 70.31%] [G loss: 0.561382]\n",
      "epoch:21 step:20509 [D loss: 0.600400, acc.: 65.62%] [G loss: 0.524441]\n",
      "epoch:21 step:20510 [D loss: 0.544086, acc.: 67.97%] [G loss: 0.748334]\n",
      "epoch:21 step:20511 [D loss: 0.504638, acc.: 72.66%] [G loss: 0.550177]\n",
      "epoch:21 step:20512 [D loss: 0.543322, acc.: 71.88%] [G loss: 0.523399]\n",
      "epoch:21 step:20513 [D loss: 0.527212, acc.: 71.88%] [G loss: 0.559040]\n",
      "epoch:21 step:20514 [D loss: 0.501416, acc.: 78.91%] [G loss: 0.537007]\n",
      "epoch:21 step:20515 [D loss: 0.533625, acc.: 77.34%] [G loss: 0.652954]\n",
      "epoch:21 step:20516 [D loss: 0.578265, acc.: 65.62%] [G loss: 0.575379]\n",
      "epoch:21 step:20517 [D loss: 0.553867, acc.: 71.09%] [G loss: 0.547513]\n",
      "epoch:21 step:20518 [D loss: 0.526871, acc.: 67.97%] [G loss: 0.596802]\n",
      "epoch:21 step:20519 [D loss: 0.544705, acc.: 71.88%] [G loss: 0.564369]\n",
      "epoch:21 step:20520 [D loss: 0.511476, acc.: 71.09%] [G loss: 0.619264]\n",
      "epoch:21 step:20521 [D loss: 0.560026, acc.: 70.31%] [G loss: 0.846284]\n",
      "epoch:21 step:20522 [D loss: 0.552800, acc.: 66.41%] [G loss: 0.707697]\n",
      "epoch:21 step:20523 [D loss: 0.565531, acc.: 72.66%] [G loss: 0.540711]\n",
      "epoch:21 step:20524 [D loss: 0.626403, acc.: 60.94%] [G loss: 0.464713]\n",
      "epoch:21 step:20525 [D loss: 0.565556, acc.: 66.41%] [G loss: 0.487728]\n",
      "epoch:21 step:20526 [D loss: 0.554139, acc.: 65.62%] [G loss: 0.532572]\n",
      "epoch:21 step:20527 [D loss: 0.548567, acc.: 66.41%] [G loss: 0.494616]\n",
      "epoch:21 step:20528 [D loss: 0.626080, acc.: 62.50%] [G loss: 0.468736]\n",
      "epoch:21 step:20529 [D loss: 0.563033, acc.: 70.31%] [G loss: 0.652323]\n",
      "epoch:21 step:20530 [D loss: 0.560981, acc.: 68.75%] [G loss: 0.582299]\n",
      "epoch:21 step:20531 [D loss: 0.536167, acc.: 70.31%] [G loss: 0.617026]\n",
      "epoch:21 step:20532 [D loss: 0.527646, acc.: 70.31%] [G loss: 0.636709]\n",
      "epoch:21 step:20533 [D loss: 0.624135, acc.: 64.84%] [G loss: 0.594287]\n",
      "epoch:21 step:20534 [D loss: 0.458427, acc.: 80.47%] [G loss: 0.676635]\n",
      "epoch:21 step:20535 [D loss: 0.624132, acc.: 61.72%] [G loss: 0.556246]\n",
      "epoch:21 step:20536 [D loss: 0.518351, acc.: 76.56%] [G loss: 0.732513]\n",
      "epoch:21 step:20537 [D loss: 0.479302, acc.: 71.09%] [G loss: 0.795427]\n",
      "epoch:21 step:20538 [D loss: 0.626781, acc.: 61.72%] [G loss: 0.608422]\n",
      "epoch:21 step:20539 [D loss: 0.575918, acc.: 60.94%] [G loss: 0.738769]\n",
      "epoch:21 step:20540 [D loss: 0.533539, acc.: 71.88%] [G loss: 0.561010]\n",
      "epoch:21 step:20541 [D loss: 0.533375, acc.: 66.41%] [G loss: 0.622945]\n",
      "epoch:21 step:20542 [D loss: 0.601365, acc.: 63.28%] [G loss: 0.459027]\n",
      "epoch:21 step:20543 [D loss: 0.576940, acc.: 65.62%] [G loss: 0.563185]\n",
      "epoch:21 step:20544 [D loss: 0.687486, acc.: 57.81%] [G loss: 0.492522]\n",
      "epoch:21 step:20545 [D loss: 0.531663, acc.: 73.44%] [G loss: 0.482330]\n",
      "epoch:21 step:20546 [D loss: 0.585940, acc.: 64.84%] [G loss: 0.455726]\n",
      "epoch:21 step:20547 [D loss: 0.497204, acc.: 71.88%] [G loss: 0.592766]\n",
      "epoch:21 step:20548 [D loss: 0.455416, acc.: 80.47%] [G loss: 0.719598]\n",
      "epoch:21 step:20549 [D loss: 0.553241, acc.: 68.75%] [G loss: 0.616580]\n",
      "epoch:21 step:20550 [D loss: 0.551844, acc.: 72.66%] [G loss: 0.533043]\n",
      "epoch:21 step:20551 [D loss: 0.528403, acc.: 71.09%] [G loss: 0.500536]\n",
      "epoch:21 step:20552 [D loss: 0.503043, acc.: 72.66%] [G loss: 0.576137]\n",
      "epoch:21 step:20553 [D loss: 0.583452, acc.: 66.41%] [G loss: 0.688680]\n",
      "epoch:21 step:20554 [D loss: 0.570441, acc.: 70.31%] [G loss: 0.505095]\n",
      "epoch:21 step:20555 [D loss: 0.552881, acc.: 65.62%] [G loss: 0.666844]\n",
      "epoch:21 step:20556 [D loss: 0.539453, acc.: 70.31%] [G loss: 0.587484]\n",
      "epoch:21 step:20557 [D loss: 0.648669, acc.: 62.50%] [G loss: 0.532870]\n",
      "epoch:21 step:20558 [D loss: 0.526796, acc.: 70.31%] [G loss: 0.614427]\n",
      "epoch:21 step:20559 [D loss: 0.582247, acc.: 63.28%] [G loss: 0.506826]\n",
      "epoch:21 step:20560 [D loss: 0.602334, acc.: 64.06%] [G loss: 0.667242]\n",
      "epoch:21 step:20561 [D loss: 0.497090, acc.: 74.22%] [G loss: 0.791285]\n",
      "epoch:21 step:20562 [D loss: 0.538432, acc.: 71.88%] [G loss: 0.669211]\n",
      "epoch:21 step:20563 [D loss: 0.509962, acc.: 71.09%] [G loss: 0.673349]\n",
      "epoch:21 step:20564 [D loss: 0.537030, acc.: 71.09%] [G loss: 0.814948]\n",
      "epoch:21 step:20565 [D loss: 0.527523, acc.: 71.09%] [G loss: 0.781920]\n",
      "epoch:21 step:20566 [D loss: 0.509867, acc.: 69.53%] [G loss: 0.667842]\n",
      "epoch:21 step:20567 [D loss: 0.540767, acc.: 73.44%] [G loss: 0.708857]\n",
      "epoch:21 step:20568 [D loss: 0.630349, acc.: 60.16%] [G loss: 0.556012]\n",
      "epoch:21 step:20569 [D loss: 0.638965, acc.: 64.84%] [G loss: 0.622131]\n",
      "epoch:21 step:20570 [D loss: 0.527115, acc.: 72.66%] [G loss: 0.693972]\n",
      "epoch:21 step:20571 [D loss: 0.514101, acc.: 71.09%] [G loss: 0.633631]\n",
      "epoch:21 step:20572 [D loss: 0.517941, acc.: 72.66%] [G loss: 0.792620]\n",
      "epoch:21 step:20573 [D loss: 0.483642, acc.: 79.69%] [G loss: 0.684970]\n",
      "epoch:21 step:20574 [D loss: 0.507089, acc.: 74.22%] [G loss: 0.731447]\n",
      "epoch:21 step:20575 [D loss: 0.477669, acc.: 75.00%] [G loss: 0.876923]\n",
      "epoch:21 step:20576 [D loss: 0.477382, acc.: 76.56%] [G loss: 0.819319]\n",
      "epoch:21 step:20577 [D loss: 0.484866, acc.: 78.12%] [G loss: 0.892751]\n",
      "epoch:21 step:20578 [D loss: 0.540534, acc.: 74.22%] [G loss: 0.759994]\n",
      "epoch:21 step:20579 [D loss: 0.524296, acc.: 74.22%] [G loss: 0.727727]\n",
      "epoch:21 step:20580 [D loss: 0.521515, acc.: 71.88%] [G loss: 0.672992]\n",
      "epoch:21 step:20581 [D loss: 0.607505, acc.: 63.28%] [G loss: 0.651750]\n",
      "epoch:21 step:20582 [D loss: 0.573636, acc.: 67.97%] [G loss: 0.613906]\n",
      "epoch:21 step:20583 [D loss: 0.520551, acc.: 69.53%] [G loss: 0.880362]\n",
      "epoch:21 step:20584 [D loss: 0.568140, acc.: 68.75%] [G loss: 0.694869]\n",
      "epoch:21 step:20585 [D loss: 0.587302, acc.: 72.66%] [G loss: 0.646482]\n",
      "epoch:21 step:20586 [D loss: 0.560460, acc.: 64.06%] [G loss: 0.536195]\n",
      "epoch:21 step:20587 [D loss: 0.569407, acc.: 72.66%] [G loss: 0.619700]\n",
      "epoch:21 step:20588 [D loss: 0.492996, acc.: 73.44%] [G loss: 0.675802]\n",
      "epoch:21 step:20589 [D loss: 0.482133, acc.: 75.00%] [G loss: 0.866593]\n",
      "epoch:21 step:20590 [D loss: 0.538123, acc.: 75.00%] [G loss: 0.902956]\n",
      "epoch:21 step:20591 [D loss: 0.522905, acc.: 67.19%] [G loss: 0.954773]\n",
      "epoch:21 step:20592 [D loss: 0.722221, acc.: 57.81%] [G loss: 0.680977]\n",
      "epoch:21 step:20593 [D loss: 0.467393, acc.: 78.12%] [G loss: 0.821068]\n",
      "epoch:21 step:20594 [D loss: 0.635430, acc.: 55.47%] [G loss: 0.750834]\n",
      "epoch:21 step:20595 [D loss: 0.503009, acc.: 75.00%] [G loss: 0.602886]\n",
      "epoch:21 step:20596 [D loss: 0.453805, acc.: 80.47%] [G loss: 0.876322]\n",
      "epoch:21 step:20597 [D loss: 0.692328, acc.: 60.94%] [G loss: 0.743494]\n",
      "epoch:21 step:20598 [D loss: 0.541131, acc.: 71.09%] [G loss: 0.649475]\n",
      "epoch:21 step:20599 [D loss: 0.547743, acc.: 69.53%] [G loss: 0.642691]\n",
      "epoch:21 step:20600 [D loss: 0.481481, acc.: 75.00%] [G loss: 0.806902]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.796039\n",
      "FID: 43.395039\n",
      "0 = 12.844164000511176\n",
      "1 = 0.08588477164343013\n",
      "2 = 0.8679999709129333\n",
      "3 = 0.8198000192642212\n",
      "4 = 0.9161999821662903\n",
      "5 = 0.9072598218917847\n",
      "6 = 0.8198000192642212\n",
      "7 = 8.082545802402496\n",
      "8 = 0.135068690873468\n",
      "9 = 0.7139000296592712\n",
      "10 = 0.6958000063896179\n",
      "11 = 0.7319999933242798\n",
      "12 = 0.7219340205192566\n",
      "13 = 0.6958000063896179\n",
      "14 = 6.796063423156738\n",
      "15 = 7.259354114532471\n",
      "16 = 0.3687993586063385\n",
      "17 = 6.79603910446167\n",
      "18 = 43.39503860473633\n",
      "epoch:21 step:20601 [D loss: 0.434222, acc.: 80.47%] [G loss: 0.859867]\n",
      "epoch:21 step:20602 [D loss: 0.390818, acc.: 82.81%] [G loss: 1.071933]\n",
      "epoch:21 step:20603 [D loss: 0.422995, acc.: 84.38%] [G loss: 1.278297]\n",
      "epoch:21 step:20604 [D loss: 0.461266, acc.: 81.25%] [G loss: 1.459609]\n",
      "epoch:21 step:20605 [D loss: 0.622340, acc.: 71.09%] [G loss: 1.258373]\n",
      "epoch:21 step:20606 [D loss: 0.531577, acc.: 73.44%] [G loss: 1.267539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20607 [D loss: 0.512165, acc.: 72.66%] [G loss: 1.510898]\n",
      "epoch:21 step:20608 [D loss: 0.535771, acc.: 74.22%] [G loss: 0.984384]\n",
      "epoch:21 step:20609 [D loss: 0.670944, acc.: 62.50%] [G loss: 0.788461]\n",
      "epoch:21 step:20610 [D loss: 0.488405, acc.: 76.56%] [G loss: 0.713746]\n",
      "epoch:21 step:20611 [D loss: 0.439477, acc.: 77.34%] [G loss: 0.959622]\n",
      "epoch:21 step:20612 [D loss: 0.462824, acc.: 75.78%] [G loss: 1.098298]\n",
      "epoch:21 step:20613 [D loss: 0.344623, acc.: 85.94%] [G loss: 1.046766]\n",
      "epoch:21 step:20614 [D loss: 0.371224, acc.: 88.28%] [G loss: 1.550340]\n",
      "epoch:22 step:20615 [D loss: 0.536544, acc.: 75.00%] [G loss: 1.351082]\n",
      "epoch:22 step:20616 [D loss: 0.552361, acc.: 71.88%] [G loss: 1.217103]\n",
      "epoch:22 step:20617 [D loss: 0.601423, acc.: 68.75%] [G loss: 0.965264]\n",
      "epoch:22 step:20618 [D loss: 0.516641, acc.: 74.22%] [G loss: 0.909715]\n",
      "epoch:22 step:20619 [D loss: 0.536217, acc.: 71.88%] [G loss: 0.817002]\n",
      "epoch:22 step:20620 [D loss: 0.548239, acc.: 72.66%] [G loss: 0.912380]\n",
      "epoch:22 step:20621 [D loss: 0.502152, acc.: 77.34%] [G loss: 0.889962]\n",
      "epoch:22 step:20622 [D loss: 0.467709, acc.: 77.34%] [G loss: 0.773886]\n",
      "epoch:22 step:20623 [D loss: 0.477864, acc.: 76.56%] [G loss: 0.974747]\n",
      "epoch:22 step:20624 [D loss: 0.493223, acc.: 77.34%] [G loss: 0.784390]\n",
      "epoch:22 step:20625 [D loss: 0.476363, acc.: 75.78%] [G loss: 0.650682]\n",
      "epoch:22 step:20626 [D loss: 0.586589, acc.: 69.53%] [G loss: 0.715422]\n",
      "epoch:22 step:20627 [D loss: 0.493303, acc.: 75.00%] [G loss: 0.759233]\n",
      "epoch:22 step:20628 [D loss: 0.531360, acc.: 72.66%] [G loss: 0.675864]\n",
      "epoch:22 step:20629 [D loss: 0.441539, acc.: 79.69%] [G loss: 0.865600]\n",
      "epoch:22 step:20630 [D loss: 0.465721, acc.: 78.12%] [G loss: 0.821122]\n",
      "epoch:22 step:20631 [D loss: 0.529001, acc.: 74.22%] [G loss: 0.759688]\n",
      "epoch:22 step:20632 [D loss: 0.582678, acc.: 71.09%] [G loss: 0.687063]\n",
      "epoch:22 step:20633 [D loss: 0.583699, acc.: 68.75%] [G loss: 0.755210]\n",
      "epoch:22 step:20634 [D loss: 0.642934, acc.: 66.41%] [G loss: 0.763149]\n",
      "epoch:22 step:20635 [D loss: 0.545992, acc.: 69.53%] [G loss: 0.743615]\n",
      "epoch:22 step:20636 [D loss: 0.434170, acc.: 75.78%] [G loss: 0.974831]\n",
      "epoch:22 step:20637 [D loss: 0.611794, acc.: 70.31%] [G loss: 0.702749]\n",
      "epoch:22 step:20638 [D loss: 0.455812, acc.: 77.34%] [G loss: 0.622235]\n",
      "epoch:22 step:20639 [D loss: 0.516668, acc.: 72.66%] [G loss: 0.666736]\n",
      "epoch:22 step:20640 [D loss: 0.612722, acc.: 64.84%] [G loss: 0.665986]\n",
      "epoch:22 step:20641 [D loss: 0.503941, acc.: 71.88%] [G loss: 0.665356]\n",
      "epoch:22 step:20642 [D loss: 0.538555, acc.: 70.31%] [G loss: 0.694574]\n",
      "epoch:22 step:20643 [D loss: 0.517146, acc.: 69.53%] [G loss: 0.591544]\n",
      "epoch:22 step:20644 [D loss: 0.566126, acc.: 66.41%] [G loss: 0.637023]\n",
      "epoch:22 step:20645 [D loss: 0.646936, acc.: 62.50%] [G loss: 0.672162]\n",
      "epoch:22 step:20646 [D loss: 0.547884, acc.: 71.88%] [G loss: 0.642419]\n",
      "epoch:22 step:20647 [D loss: 0.562573, acc.: 69.53%] [G loss: 0.676368]\n",
      "epoch:22 step:20648 [D loss: 0.569712, acc.: 67.19%] [G loss: 0.652313]\n",
      "epoch:22 step:20649 [D loss: 0.570911, acc.: 67.19%] [G loss: 0.702390]\n",
      "epoch:22 step:20650 [D loss: 0.528119, acc.: 73.44%] [G loss: 0.921067]\n",
      "epoch:22 step:20651 [D loss: 0.504744, acc.: 75.00%] [G loss: 0.815725]\n",
      "epoch:22 step:20652 [D loss: 0.601555, acc.: 69.53%] [G loss: 0.597373]\n",
      "epoch:22 step:20653 [D loss: 0.545181, acc.: 71.09%] [G loss: 0.532934]\n",
      "epoch:22 step:20654 [D loss: 0.457083, acc.: 78.12%] [G loss: 0.762916]\n",
      "epoch:22 step:20655 [D loss: 0.626290, acc.: 63.28%] [G loss: 0.769634]\n",
      "epoch:22 step:20656 [D loss: 0.532657, acc.: 73.44%] [G loss: 0.725341]\n",
      "epoch:22 step:20657 [D loss: 0.506105, acc.: 73.44%] [G loss: 0.588596]\n",
      "epoch:22 step:20658 [D loss: 0.588819, acc.: 71.88%] [G loss: 0.537581]\n",
      "epoch:22 step:20659 [D loss: 0.497477, acc.: 70.31%] [G loss: 0.722860]\n",
      "epoch:22 step:20660 [D loss: 0.494815, acc.: 78.12%] [G loss: 0.692736]\n",
      "epoch:22 step:20661 [D loss: 0.556624, acc.: 68.75%] [G loss: 0.604498]\n",
      "epoch:22 step:20662 [D loss: 0.511441, acc.: 74.22%] [G loss: 0.767098]\n",
      "epoch:22 step:20663 [D loss: 0.452875, acc.: 78.91%] [G loss: 0.955376]\n",
      "epoch:22 step:20664 [D loss: 0.534678, acc.: 66.41%] [G loss: 0.755373]\n",
      "epoch:22 step:20665 [D loss: 0.671139, acc.: 61.72%] [G loss: 0.521794]\n",
      "epoch:22 step:20666 [D loss: 0.620271, acc.: 66.41%] [G loss: 0.513789]\n",
      "epoch:22 step:20667 [D loss: 0.502555, acc.: 75.78%] [G loss: 0.693109]\n",
      "epoch:22 step:20668 [D loss: 0.486237, acc.: 75.00%] [G loss: 0.724160]\n",
      "epoch:22 step:20669 [D loss: 0.566889, acc.: 70.31%] [G loss: 0.743546]\n",
      "epoch:22 step:20670 [D loss: 0.511676, acc.: 73.44%] [G loss: 0.839615]\n",
      "epoch:22 step:20671 [D loss: 0.542154, acc.: 69.53%] [G loss: 0.716670]\n",
      "epoch:22 step:20672 [D loss: 0.549749, acc.: 73.44%] [G loss: 0.739016]\n",
      "epoch:22 step:20673 [D loss: 0.512723, acc.: 77.34%] [G loss: 0.650748]\n",
      "epoch:22 step:20674 [D loss: 0.594644, acc.: 67.19%] [G loss: 0.645583]\n",
      "epoch:22 step:20675 [D loss: 0.543373, acc.: 69.53%] [G loss: 0.569578]\n",
      "epoch:22 step:20676 [D loss: 0.581245, acc.: 68.75%] [G loss: 0.518541]\n",
      "epoch:22 step:20677 [D loss: 0.623435, acc.: 67.97%] [G loss: 0.588884]\n",
      "epoch:22 step:20678 [D loss: 0.562544, acc.: 68.75%] [G loss: 0.633865]\n",
      "epoch:22 step:20679 [D loss: 0.555779, acc.: 70.31%] [G loss: 0.620545]\n",
      "epoch:22 step:20680 [D loss: 0.531078, acc.: 71.09%] [G loss: 0.858178]\n",
      "epoch:22 step:20681 [D loss: 0.557902, acc.: 69.53%] [G loss: 0.637185]\n",
      "epoch:22 step:20682 [D loss: 0.545859, acc.: 71.88%] [G loss: 0.589641]\n",
      "epoch:22 step:20683 [D loss: 0.511787, acc.: 76.56%] [G loss: 0.589147]\n",
      "epoch:22 step:20684 [D loss: 0.499559, acc.: 74.22%] [G loss: 0.795824]\n",
      "epoch:22 step:20685 [D loss: 0.524191, acc.: 73.44%] [G loss: 0.801917]\n",
      "epoch:22 step:20686 [D loss: 0.543966, acc.: 73.44%] [G loss: 0.675546]\n",
      "epoch:22 step:20687 [D loss: 0.537105, acc.: 71.09%] [G loss: 0.677702]\n",
      "epoch:22 step:20688 [D loss: 0.463236, acc.: 79.69%] [G loss: 0.720013]\n",
      "epoch:22 step:20689 [D loss: 0.493042, acc.: 78.12%] [G loss: 0.779174]\n",
      "epoch:22 step:20690 [D loss: 0.535925, acc.: 72.66%] [G loss: 0.854012]\n",
      "epoch:22 step:20691 [D loss: 0.439920, acc.: 80.47%] [G loss: 0.874181]\n",
      "epoch:22 step:20692 [D loss: 0.660820, acc.: 59.38%] [G loss: 0.526476]\n",
      "epoch:22 step:20693 [D loss: 0.536055, acc.: 69.53%] [G loss: 0.567506]\n",
      "epoch:22 step:20694 [D loss: 0.511682, acc.: 72.66%] [G loss: 0.712318]\n",
      "epoch:22 step:20695 [D loss: 0.534021, acc.: 71.88%] [G loss: 0.694483]\n",
      "epoch:22 step:20696 [D loss: 0.543737, acc.: 66.41%] [G loss: 0.787211]\n",
      "epoch:22 step:20697 [D loss: 0.478004, acc.: 76.56%] [G loss: 0.714327]\n",
      "epoch:22 step:20698 [D loss: 0.476611, acc.: 75.00%] [G loss: 0.635845]\n",
      "epoch:22 step:20699 [D loss: 0.575858, acc.: 67.97%] [G loss: 0.775897]\n",
      "epoch:22 step:20700 [D loss: 0.581428, acc.: 67.97%] [G loss: 0.766416]\n",
      "epoch:22 step:20701 [D loss: 0.475895, acc.: 73.44%] [G loss: 0.782781]\n",
      "epoch:22 step:20702 [D loss: 0.523232, acc.: 75.00%] [G loss: 0.846039]\n",
      "epoch:22 step:20703 [D loss: 0.520094, acc.: 70.31%] [G loss: 0.571303]\n",
      "epoch:22 step:20704 [D loss: 0.508785, acc.: 72.66%] [G loss: 0.600753]\n",
      "epoch:22 step:20705 [D loss: 0.538860, acc.: 68.75%] [G loss: 0.743299]\n",
      "epoch:22 step:20706 [D loss: 0.461894, acc.: 76.56%] [G loss: 0.773303]\n",
      "epoch:22 step:20707 [D loss: 0.482986, acc.: 75.00%] [G loss: 0.765675]\n",
      "epoch:22 step:20708 [D loss: 0.486152, acc.: 72.66%] [G loss: 0.825538]\n",
      "epoch:22 step:20709 [D loss: 0.532415, acc.: 72.66%] [G loss: 0.890905]\n",
      "epoch:22 step:20710 [D loss: 0.530082, acc.: 71.88%] [G loss: 0.922016]\n",
      "epoch:22 step:20711 [D loss: 0.513334, acc.: 72.66%] [G loss: 0.776103]\n",
      "epoch:22 step:20712 [D loss: 0.570006, acc.: 67.97%] [G loss: 0.706165]\n",
      "epoch:22 step:20713 [D loss: 0.509913, acc.: 76.56%] [G loss: 0.822792]\n",
      "epoch:22 step:20714 [D loss: 0.462802, acc.: 78.91%] [G loss: 0.895949]\n",
      "epoch:22 step:20715 [D loss: 0.536590, acc.: 69.53%] [G loss: 0.862297]\n",
      "epoch:22 step:20716 [D loss: 0.651191, acc.: 60.94%] [G loss: 0.538829]\n",
      "epoch:22 step:20717 [D loss: 0.560262, acc.: 67.19%] [G loss: 0.596692]\n",
      "epoch:22 step:20718 [D loss: 0.570211, acc.: 68.75%] [G loss: 0.626093]\n",
      "epoch:22 step:20719 [D loss: 0.620293, acc.: 62.50%] [G loss: 0.475569]\n",
      "epoch:22 step:20720 [D loss: 0.572573, acc.: 66.41%] [G loss: 0.460435]\n",
      "epoch:22 step:20721 [D loss: 0.574062, acc.: 71.09%] [G loss: 0.613632]\n",
      "epoch:22 step:20722 [D loss: 0.648082, acc.: 61.72%] [G loss: 0.641281]\n",
      "epoch:22 step:20723 [D loss: 0.559627, acc.: 64.84%] [G loss: 0.694954]\n",
      "epoch:22 step:20724 [D loss: 0.546787, acc.: 69.53%] [G loss: 0.674284]\n",
      "epoch:22 step:20725 [D loss: 0.528929, acc.: 73.44%] [G loss: 0.739547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20726 [D loss: 0.493506, acc.: 75.00%] [G loss: 0.746527]\n",
      "epoch:22 step:20727 [D loss: 0.522866, acc.: 69.53%] [G loss: 0.723589]\n",
      "epoch:22 step:20728 [D loss: 0.491017, acc.: 75.78%] [G loss: 0.640161]\n",
      "epoch:22 step:20729 [D loss: 0.520113, acc.: 74.22%] [G loss: 0.716543]\n",
      "epoch:22 step:20730 [D loss: 0.511139, acc.: 74.22%] [G loss: 0.816845]\n",
      "epoch:22 step:20731 [D loss: 0.514676, acc.: 71.88%] [G loss: 0.721452]\n",
      "epoch:22 step:20732 [D loss: 0.535963, acc.: 71.09%] [G loss: 1.026013]\n",
      "epoch:22 step:20733 [D loss: 0.482855, acc.: 78.12%] [G loss: 1.055261]\n",
      "epoch:22 step:20734 [D loss: 0.489988, acc.: 75.00%] [G loss: 0.840098]\n",
      "epoch:22 step:20735 [D loss: 0.513606, acc.: 71.88%] [G loss: 0.842249]\n",
      "epoch:22 step:20736 [D loss: 0.512407, acc.: 73.44%] [G loss: 0.766220]\n",
      "epoch:22 step:20737 [D loss: 0.555179, acc.: 68.75%] [G loss: 0.699841]\n",
      "epoch:22 step:20738 [D loss: 0.579804, acc.: 68.75%] [G loss: 0.751402]\n",
      "epoch:22 step:20739 [D loss: 0.550117, acc.: 71.09%] [G loss: 0.731451]\n",
      "epoch:22 step:20740 [D loss: 0.530384, acc.: 71.09%] [G loss: 0.831211]\n",
      "epoch:22 step:20741 [D loss: 0.477676, acc.: 75.78%] [G loss: 0.660592]\n",
      "epoch:22 step:20742 [D loss: 0.533666, acc.: 69.53%] [G loss: 0.727785]\n",
      "epoch:22 step:20743 [D loss: 0.612648, acc.: 67.19%] [G loss: 0.688171]\n",
      "epoch:22 step:20744 [D loss: 0.546757, acc.: 73.44%] [G loss: 0.733143]\n",
      "epoch:22 step:20745 [D loss: 0.482077, acc.: 71.09%] [G loss: 0.673883]\n",
      "epoch:22 step:20746 [D loss: 0.555680, acc.: 68.75%] [G loss: 0.759146]\n",
      "epoch:22 step:20747 [D loss: 0.571961, acc.: 69.53%] [G loss: 0.874937]\n",
      "epoch:22 step:20748 [D loss: 0.517541, acc.: 71.09%] [G loss: 0.619056]\n",
      "epoch:22 step:20749 [D loss: 0.536395, acc.: 75.78%] [G loss: 0.833609]\n",
      "epoch:22 step:20750 [D loss: 0.524615, acc.: 70.31%] [G loss: 0.684352]\n",
      "epoch:22 step:20751 [D loss: 0.671496, acc.: 64.06%] [G loss: 0.598075]\n",
      "epoch:22 step:20752 [D loss: 0.591768, acc.: 67.19%] [G loss: 0.625957]\n",
      "epoch:22 step:20753 [D loss: 0.550425, acc.: 68.75%] [G loss: 0.585659]\n",
      "epoch:22 step:20754 [D loss: 0.570984, acc.: 67.19%] [G loss: 0.559543]\n",
      "epoch:22 step:20755 [D loss: 0.575083, acc.: 67.97%] [G loss: 0.618999]\n",
      "epoch:22 step:20756 [D loss: 0.526428, acc.: 69.53%] [G loss: 0.587658]\n",
      "epoch:22 step:20757 [D loss: 0.573142, acc.: 65.62%] [G loss: 0.761467]\n",
      "epoch:22 step:20758 [D loss: 0.543638, acc.: 74.22%] [G loss: 0.591714]\n",
      "epoch:22 step:20759 [D loss: 0.572095, acc.: 69.53%] [G loss: 0.745889]\n",
      "epoch:22 step:20760 [D loss: 0.486799, acc.: 78.12%] [G loss: 0.819093]\n",
      "epoch:22 step:20761 [D loss: 0.590590, acc.: 66.41%] [G loss: 0.730104]\n",
      "epoch:22 step:20762 [D loss: 0.535942, acc.: 71.88%] [G loss: 0.575677]\n",
      "epoch:22 step:20763 [D loss: 0.530145, acc.: 72.66%] [G loss: 0.650079]\n",
      "epoch:22 step:20764 [D loss: 0.537907, acc.: 71.09%] [G loss: 0.545503]\n",
      "epoch:22 step:20765 [D loss: 0.550728, acc.: 68.75%] [G loss: 0.632198]\n",
      "epoch:22 step:20766 [D loss: 0.555494, acc.: 67.97%] [G loss: 0.564555]\n",
      "epoch:22 step:20767 [D loss: 0.604872, acc.: 66.41%] [G loss: 0.851543]\n",
      "epoch:22 step:20768 [D loss: 0.529463, acc.: 71.88%] [G loss: 0.647098]\n",
      "epoch:22 step:20769 [D loss: 0.481301, acc.: 74.22%] [G loss: 0.782083]\n",
      "epoch:22 step:20770 [D loss: 0.477849, acc.: 75.00%] [G loss: 0.737579]\n",
      "epoch:22 step:20771 [D loss: 0.546115, acc.: 70.31%] [G loss: 0.808195]\n",
      "epoch:22 step:20772 [D loss: 0.601606, acc.: 68.75%] [G loss: 0.623238]\n",
      "epoch:22 step:20773 [D loss: 0.470040, acc.: 78.12%] [G loss: 0.668564]\n",
      "epoch:22 step:20774 [D loss: 0.571445, acc.: 69.53%] [G loss: 0.627237]\n",
      "epoch:22 step:20775 [D loss: 0.598537, acc.: 71.09%] [G loss: 0.740970]\n",
      "epoch:22 step:20776 [D loss: 0.451249, acc.: 77.34%] [G loss: 0.821315]\n",
      "epoch:22 step:20777 [D loss: 0.563773, acc.: 68.75%] [G loss: 0.773427]\n",
      "epoch:22 step:20778 [D loss: 0.603106, acc.: 69.53%] [G loss: 0.632691]\n",
      "epoch:22 step:20779 [D loss: 0.532413, acc.: 74.22%] [G loss: 0.800282]\n",
      "epoch:22 step:20780 [D loss: 0.530567, acc.: 69.53%] [G loss: 0.674843]\n",
      "epoch:22 step:20781 [D loss: 0.602884, acc.: 64.06%] [G loss: 0.790493]\n",
      "epoch:22 step:20782 [D loss: 0.574917, acc.: 69.53%] [G loss: 0.556966]\n",
      "epoch:22 step:20783 [D loss: 0.570487, acc.: 68.75%] [G loss: 0.626433]\n",
      "epoch:22 step:20784 [D loss: 0.546109, acc.: 70.31%] [G loss: 0.519890]\n",
      "epoch:22 step:20785 [D loss: 0.568093, acc.: 71.09%] [G loss: 0.792337]\n",
      "epoch:22 step:20786 [D loss: 0.540849, acc.: 68.75%] [G loss: 0.651056]\n",
      "epoch:22 step:20787 [D loss: 0.490075, acc.: 74.22%] [G loss: 0.833092]\n",
      "epoch:22 step:20788 [D loss: 0.611197, acc.: 64.06%] [G loss: 0.772802]\n",
      "epoch:22 step:20789 [D loss: 0.611267, acc.: 60.16%] [G loss: 0.662165]\n",
      "epoch:22 step:20790 [D loss: 0.492994, acc.: 74.22%] [G loss: 0.576078]\n",
      "epoch:22 step:20791 [D loss: 0.533179, acc.: 71.88%] [G loss: 0.511244]\n",
      "epoch:22 step:20792 [D loss: 0.503085, acc.: 74.22%] [G loss: 0.586051]\n",
      "epoch:22 step:20793 [D loss: 0.538550, acc.: 69.53%] [G loss: 0.521151]\n",
      "epoch:22 step:20794 [D loss: 0.611430, acc.: 65.62%] [G loss: 0.571658]\n",
      "epoch:22 step:20795 [D loss: 0.571642, acc.: 67.19%] [G loss: 0.638556]\n",
      "epoch:22 step:20796 [D loss: 0.455361, acc.: 76.56%] [G loss: 0.694996]\n",
      "epoch:22 step:20797 [D loss: 0.632517, acc.: 64.06%] [G loss: 0.856489]\n",
      "epoch:22 step:20798 [D loss: 0.562113, acc.: 67.19%] [G loss: 0.633008]\n",
      "epoch:22 step:20799 [D loss: 0.579528, acc.: 67.19%] [G loss: 0.734280]\n",
      "epoch:22 step:20800 [D loss: 0.564806, acc.: 70.31%] [G loss: 0.739181]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.771451\n",
      "FID: 45.941982\n",
      "0 = 12.858288849449186\n",
      "1 = 0.09445245154103633\n",
      "2 = 0.8579999804496765\n",
      "3 = 0.8082000017166138\n",
      "4 = 0.907800018787384\n",
      "5 = 0.8976010680198669\n",
      "6 = 0.8082000017166138\n",
      "7 = 8.17356118669508\n",
      "8 = 0.1415834612622206\n",
      "9 = 0.6976000070571899\n",
      "10 = 0.682200014591217\n",
      "11 = 0.7129999995231628\n",
      "12 = 0.7038794755935669\n",
      "13 = 0.682200014591217\n",
      "14 = 6.771474838256836\n",
      "15 = 7.2962565422058105\n",
      "16 = 0.37001997232437134\n",
      "17 = 6.771450519561768\n",
      "18 = 45.94198226928711\n",
      "epoch:22 step:20801 [D loss: 0.605612, acc.: 64.84%] [G loss: 0.679004]\n",
      "epoch:22 step:20802 [D loss: 0.487345, acc.: 78.12%] [G loss: 0.728557]\n",
      "epoch:22 step:20803 [D loss: 0.592475, acc.: 67.97%] [G loss: 0.559181]\n",
      "epoch:22 step:20804 [D loss: 0.505330, acc.: 76.56%] [G loss: 0.690132]\n",
      "epoch:22 step:20805 [D loss: 0.463245, acc.: 78.12%] [G loss: 0.747213]\n",
      "epoch:22 step:20806 [D loss: 0.493778, acc.: 75.78%] [G loss: 0.716165]\n",
      "epoch:22 step:20807 [D loss: 0.551495, acc.: 70.31%] [G loss: 0.635766]\n",
      "epoch:22 step:20808 [D loss: 0.491365, acc.: 78.12%] [G loss: 0.803691]\n",
      "epoch:22 step:20809 [D loss: 0.611884, acc.: 66.41%] [G loss: 0.681679]\n",
      "epoch:22 step:20810 [D loss: 0.550222, acc.: 67.97%] [G loss: 0.752481]\n",
      "epoch:22 step:20811 [D loss: 0.527867, acc.: 75.00%] [G loss: 0.843163]\n",
      "epoch:22 step:20812 [D loss: 0.465639, acc.: 78.91%] [G loss: 0.727484]\n",
      "epoch:22 step:20813 [D loss: 0.540381, acc.: 72.66%] [G loss: 0.833716]\n",
      "epoch:22 step:20814 [D loss: 0.663032, acc.: 59.38%] [G loss: 0.645724]\n",
      "epoch:22 step:20815 [D loss: 0.548417, acc.: 69.53%] [G loss: 0.731085]\n",
      "epoch:22 step:20816 [D loss: 0.515450, acc.: 71.88%] [G loss: 0.646024]\n",
      "epoch:22 step:20817 [D loss: 0.609282, acc.: 63.28%] [G loss: 0.609029]\n",
      "epoch:22 step:20818 [D loss: 0.556327, acc.: 70.31%] [G loss: 0.676467]\n",
      "epoch:22 step:20819 [D loss: 0.497154, acc.: 75.00%] [G loss: 0.782336]\n",
      "epoch:22 step:20820 [D loss: 0.505899, acc.: 75.78%] [G loss: 0.808739]\n",
      "epoch:22 step:20821 [D loss: 0.466511, acc.: 76.56%] [G loss: 0.780481]\n",
      "epoch:22 step:20822 [D loss: 0.431268, acc.: 79.69%] [G loss: 0.728870]\n",
      "epoch:22 step:20823 [D loss: 0.499876, acc.: 76.56%] [G loss: 0.710277]\n",
      "epoch:22 step:20824 [D loss: 0.688420, acc.: 63.28%] [G loss: 0.515969]\n",
      "epoch:22 step:20825 [D loss: 0.560042, acc.: 67.19%] [G loss: 0.617959]\n",
      "epoch:22 step:20826 [D loss: 0.546226, acc.: 69.53%] [G loss: 0.622278]\n",
      "epoch:22 step:20827 [D loss: 0.536729, acc.: 68.75%] [G loss: 0.715688]\n",
      "epoch:22 step:20828 [D loss: 0.632028, acc.: 61.72%] [G loss: 0.609846]\n",
      "epoch:22 step:20829 [D loss: 0.582597, acc.: 63.28%] [G loss: 0.606081]\n",
      "epoch:22 step:20830 [D loss: 0.503830, acc.: 72.66%] [G loss: 0.716587]\n",
      "epoch:22 step:20831 [D loss: 0.514624, acc.: 75.00%] [G loss: 0.575601]\n",
      "epoch:22 step:20832 [D loss: 0.451020, acc.: 75.00%] [G loss: 0.795727]\n",
      "epoch:22 step:20833 [D loss: 0.483110, acc.: 77.34%] [G loss: 0.891251]\n",
      "epoch:22 step:20834 [D loss: 0.646403, acc.: 66.41%] [G loss: 0.784947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20835 [D loss: 0.570314, acc.: 71.09%] [G loss: 0.645462]\n",
      "epoch:22 step:20836 [D loss: 0.450199, acc.: 80.47%] [G loss: 0.824383]\n",
      "epoch:22 step:20837 [D loss: 0.545876, acc.: 71.09%] [G loss: 0.772658]\n",
      "epoch:22 step:20838 [D loss: 0.585135, acc.: 65.62%] [G loss: 0.651397]\n",
      "epoch:22 step:20839 [D loss: 0.513593, acc.: 75.00%] [G loss: 0.639968]\n",
      "epoch:22 step:20840 [D loss: 0.546342, acc.: 67.19%] [G loss: 0.600036]\n",
      "epoch:22 step:20841 [D loss: 0.597986, acc.: 70.31%] [G loss: 0.507797]\n",
      "epoch:22 step:20842 [D loss: 0.622191, acc.: 67.97%] [G loss: 0.495309]\n",
      "epoch:22 step:20843 [D loss: 0.525263, acc.: 76.56%] [G loss: 0.709504]\n",
      "epoch:22 step:20844 [D loss: 0.525753, acc.: 78.12%] [G loss: 0.647954]\n",
      "epoch:22 step:20845 [D loss: 0.437837, acc.: 78.91%] [G loss: 0.724805]\n",
      "epoch:22 step:20846 [D loss: 0.511963, acc.: 77.34%] [G loss: 0.999703]\n",
      "epoch:22 step:20847 [D loss: 0.543866, acc.: 70.31%] [G loss: 0.839544]\n",
      "epoch:22 step:20848 [D loss: 0.589149, acc.: 73.44%] [G loss: 0.538585]\n",
      "epoch:22 step:20849 [D loss: 0.598506, acc.: 62.50%] [G loss: 0.661926]\n",
      "epoch:22 step:20850 [D loss: 0.554638, acc.: 67.97%] [G loss: 0.492190]\n",
      "epoch:22 step:20851 [D loss: 0.514432, acc.: 75.78%] [G loss: 0.664411]\n",
      "epoch:22 step:20852 [D loss: 0.536118, acc.: 71.88%] [G loss: 0.617774]\n",
      "epoch:22 step:20853 [D loss: 0.525443, acc.: 68.75%] [G loss: 0.518862]\n",
      "epoch:22 step:20854 [D loss: 0.581172, acc.: 64.84%] [G loss: 0.578414]\n",
      "epoch:22 step:20855 [D loss: 0.550806, acc.: 75.00%] [G loss: 0.757341]\n",
      "epoch:22 step:20856 [D loss: 0.532600, acc.: 69.53%] [G loss: 0.617974]\n",
      "epoch:22 step:20857 [D loss: 0.544251, acc.: 69.53%] [G loss: 0.709928]\n",
      "epoch:22 step:20858 [D loss: 0.490902, acc.: 75.00%] [G loss: 0.780087]\n",
      "epoch:22 step:20859 [D loss: 0.540053, acc.: 69.53%] [G loss: 0.701867]\n",
      "epoch:22 step:20860 [D loss: 0.578109, acc.: 67.19%] [G loss: 0.690943]\n",
      "epoch:22 step:20861 [D loss: 0.477514, acc.: 72.66%] [G loss: 0.819728]\n",
      "epoch:22 step:20862 [D loss: 0.492839, acc.: 72.66%] [G loss: 0.824101]\n",
      "epoch:22 step:20863 [D loss: 0.615474, acc.: 71.88%] [G loss: 0.733249]\n",
      "epoch:22 step:20864 [D loss: 0.618137, acc.: 63.28%] [G loss: 0.637050]\n",
      "epoch:22 step:20865 [D loss: 0.638405, acc.: 62.50%] [G loss: 0.540760]\n",
      "epoch:22 step:20866 [D loss: 0.576516, acc.: 70.31%] [G loss: 0.723711]\n",
      "epoch:22 step:20867 [D loss: 0.561277, acc.: 71.88%] [G loss: 0.604044]\n",
      "epoch:22 step:20868 [D loss: 0.462426, acc.: 79.69%] [G loss: 0.683494]\n",
      "epoch:22 step:20869 [D loss: 0.587602, acc.: 66.41%] [G loss: 0.610311]\n",
      "epoch:22 step:20870 [D loss: 0.562948, acc.: 62.50%] [G loss: 0.568610]\n",
      "epoch:22 step:20871 [D loss: 0.587524, acc.: 58.59%] [G loss: 0.561202]\n",
      "epoch:22 step:20872 [D loss: 0.511067, acc.: 72.66%] [G loss: 0.527772]\n",
      "epoch:22 step:20873 [D loss: 0.586147, acc.: 66.41%] [G loss: 0.582628]\n",
      "epoch:22 step:20874 [D loss: 0.594698, acc.: 59.38%] [G loss: 0.622225]\n",
      "epoch:22 step:20875 [D loss: 0.529396, acc.: 75.00%] [G loss: 0.656480]\n",
      "epoch:22 step:20876 [D loss: 0.551489, acc.: 69.53%] [G loss: 0.687557]\n",
      "epoch:22 step:20877 [D loss: 0.558054, acc.: 72.66%] [G loss: 0.702947]\n",
      "epoch:22 step:20878 [D loss: 0.535365, acc.: 69.53%] [G loss: 0.774107]\n",
      "epoch:22 step:20879 [D loss: 0.521468, acc.: 72.66%] [G loss: 0.621159]\n",
      "epoch:22 step:20880 [D loss: 0.553635, acc.: 71.88%] [G loss: 0.664141]\n",
      "epoch:22 step:20881 [D loss: 0.524558, acc.: 73.44%] [G loss: 0.721424]\n",
      "epoch:22 step:20882 [D loss: 0.546988, acc.: 71.88%] [G loss: 0.604040]\n",
      "epoch:22 step:20883 [D loss: 0.554750, acc.: 75.00%] [G loss: 0.606075]\n",
      "epoch:22 step:20884 [D loss: 0.489903, acc.: 74.22%] [G loss: 0.869945]\n",
      "epoch:22 step:20885 [D loss: 0.429253, acc.: 81.25%] [G loss: 0.840560]\n",
      "epoch:22 step:20886 [D loss: 0.583863, acc.: 62.50%] [G loss: 0.617898]\n",
      "epoch:22 step:20887 [D loss: 0.523044, acc.: 67.19%] [G loss: 0.636143]\n",
      "epoch:22 step:20888 [D loss: 0.511168, acc.: 75.00%] [G loss: 0.757375]\n",
      "epoch:22 step:20889 [D loss: 0.587849, acc.: 65.62%] [G loss: 0.606387]\n",
      "epoch:22 step:20890 [D loss: 0.436166, acc.: 82.03%] [G loss: 0.909094]\n",
      "epoch:22 step:20891 [D loss: 0.692029, acc.: 61.72%] [G loss: 0.680768]\n",
      "epoch:22 step:20892 [D loss: 0.624314, acc.: 60.16%] [G loss: 0.532821]\n",
      "epoch:22 step:20893 [D loss: 0.599551, acc.: 63.28%] [G loss: 0.533153]\n",
      "epoch:22 step:20894 [D loss: 0.524846, acc.: 71.88%] [G loss: 0.620085]\n",
      "epoch:22 step:20895 [D loss: 0.619164, acc.: 64.84%] [G loss: 0.446479]\n",
      "epoch:22 step:20896 [D loss: 0.545327, acc.: 73.44%] [G loss: 0.426905]\n",
      "epoch:22 step:20897 [D loss: 0.538595, acc.: 67.19%] [G loss: 0.550751]\n",
      "epoch:22 step:20898 [D loss: 0.504056, acc.: 75.00%] [G loss: 0.702082]\n",
      "epoch:22 step:20899 [D loss: 0.531928, acc.: 74.22%] [G loss: 0.702779]\n",
      "epoch:22 step:20900 [D loss: 0.450604, acc.: 81.25%] [G loss: 0.825498]\n",
      "epoch:22 step:20901 [D loss: 0.589556, acc.: 64.84%] [G loss: 0.568573]\n",
      "epoch:22 step:20902 [D loss: 0.578832, acc.: 64.84%] [G loss: 0.680644]\n",
      "epoch:22 step:20903 [D loss: 0.544810, acc.: 68.75%] [G loss: 0.691794]\n",
      "epoch:22 step:20904 [D loss: 0.545248, acc.: 73.44%] [G loss: 0.579535]\n",
      "epoch:22 step:20905 [D loss: 0.621139, acc.: 68.75%] [G loss: 0.539574]\n",
      "epoch:22 step:20906 [D loss: 0.541687, acc.: 68.75%] [G loss: 0.589437]\n",
      "epoch:22 step:20907 [D loss: 0.593463, acc.: 65.62%] [G loss: 0.731980]\n",
      "epoch:22 step:20908 [D loss: 0.588228, acc.: 64.06%] [G loss: 0.496474]\n",
      "epoch:22 step:20909 [D loss: 0.551584, acc.: 69.53%] [G loss: 0.459885]\n",
      "epoch:22 step:20910 [D loss: 0.465149, acc.: 75.00%] [G loss: 0.645482]\n",
      "epoch:22 step:20911 [D loss: 0.540566, acc.: 69.53%] [G loss: 0.584108]\n",
      "epoch:22 step:20912 [D loss: 0.479071, acc.: 78.91%] [G loss: 0.685783]\n",
      "epoch:22 step:20913 [D loss: 0.463025, acc.: 75.78%] [G loss: 0.842943]\n",
      "epoch:22 step:20914 [D loss: 0.495453, acc.: 75.78%] [G loss: 0.831513]\n",
      "epoch:22 step:20915 [D loss: 0.673543, acc.: 60.94%] [G loss: 0.660920]\n",
      "epoch:22 step:20916 [D loss: 0.498438, acc.: 74.22%] [G loss: 0.799133]\n",
      "epoch:22 step:20917 [D loss: 0.584380, acc.: 63.28%] [G loss: 0.633717]\n",
      "epoch:22 step:20918 [D loss: 0.485073, acc.: 77.34%] [G loss: 0.567814]\n",
      "epoch:22 step:20919 [D loss: 0.557010, acc.: 64.84%] [G loss: 0.641244]\n",
      "epoch:22 step:20920 [D loss: 0.503442, acc.: 76.56%] [G loss: 0.774316]\n",
      "epoch:22 step:20921 [D loss: 0.494320, acc.: 78.91%] [G loss: 0.798002]\n",
      "epoch:22 step:20922 [D loss: 0.575483, acc.: 63.28%] [G loss: 0.621864]\n",
      "epoch:22 step:20923 [D loss: 0.532257, acc.: 64.84%] [G loss: 0.725343]\n",
      "epoch:22 step:20924 [D loss: 0.555003, acc.: 69.53%] [G loss: 0.685481]\n",
      "epoch:22 step:20925 [D loss: 0.505468, acc.: 71.88%] [G loss: 0.781412]\n",
      "epoch:22 step:20926 [D loss: 0.532808, acc.: 73.44%] [G loss: 0.881203]\n",
      "epoch:22 step:20927 [D loss: 0.490980, acc.: 75.78%] [G loss: 0.876744]\n",
      "epoch:22 step:20928 [D loss: 0.425382, acc.: 78.91%] [G loss: 1.017426]\n",
      "epoch:22 step:20929 [D loss: 0.453253, acc.: 77.34%] [G loss: 1.097724]\n",
      "epoch:22 step:20930 [D loss: 0.693575, acc.: 62.50%] [G loss: 0.787358]\n",
      "epoch:22 step:20931 [D loss: 0.592928, acc.: 64.84%] [G loss: 0.683980]\n",
      "epoch:22 step:20932 [D loss: 0.525095, acc.: 70.31%] [G loss: 0.610903]\n",
      "epoch:22 step:20933 [D loss: 0.552597, acc.: 69.53%] [G loss: 0.577223]\n",
      "epoch:22 step:20934 [D loss: 0.561839, acc.: 69.53%] [G loss: 0.679742]\n",
      "epoch:22 step:20935 [D loss: 0.481671, acc.: 78.12%] [G loss: 0.692910]\n",
      "epoch:22 step:20936 [D loss: 0.540560, acc.: 68.75%] [G loss: 0.662192]\n",
      "epoch:22 step:20937 [D loss: 0.592594, acc.: 66.41%] [G loss: 0.545190]\n",
      "epoch:22 step:20938 [D loss: 0.542523, acc.: 75.00%] [G loss: 0.511533]\n",
      "epoch:22 step:20939 [D loss: 0.549205, acc.: 68.75%] [G loss: 0.677986]\n",
      "epoch:22 step:20940 [D loss: 0.457725, acc.: 75.78%] [G loss: 0.631188]\n",
      "epoch:22 step:20941 [D loss: 0.503538, acc.: 75.00%] [G loss: 0.689621]\n",
      "epoch:22 step:20942 [D loss: 0.471733, acc.: 73.44%] [G loss: 0.730721]\n",
      "epoch:22 step:20943 [D loss: 0.513309, acc.: 75.00%] [G loss: 0.682874]\n",
      "epoch:22 step:20944 [D loss: 0.541403, acc.: 70.31%] [G loss: 0.650984]\n",
      "epoch:22 step:20945 [D loss: 0.564821, acc.: 67.97%] [G loss: 0.560844]\n",
      "epoch:22 step:20946 [D loss: 0.491243, acc.: 75.78%] [G loss: 0.645412]\n",
      "epoch:22 step:20947 [D loss: 0.472496, acc.: 76.56%] [G loss: 0.735845]\n",
      "epoch:22 step:20948 [D loss: 0.461578, acc.: 76.56%] [G loss: 0.809180]\n",
      "epoch:22 step:20949 [D loss: 0.514637, acc.: 71.09%] [G loss: 0.939689]\n",
      "epoch:22 step:20950 [D loss: 0.487631, acc.: 75.78%] [G loss: 0.836593]\n",
      "epoch:22 step:20951 [D loss: 0.527145, acc.: 75.00%] [G loss: 0.852394]\n",
      "epoch:22 step:20952 [D loss: 0.549433, acc.: 70.31%] [G loss: 0.682597]\n",
      "epoch:22 step:20953 [D loss: 0.542813, acc.: 68.75%] [G loss: 0.662107]\n",
      "epoch:22 step:20954 [D loss: 0.481874, acc.: 76.56%] [G loss: 0.673181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20955 [D loss: 0.590019, acc.: 71.09%] [G loss: 0.653044]\n",
      "epoch:22 step:20956 [D loss: 0.673103, acc.: 58.59%] [G loss: 0.512579]\n",
      "epoch:22 step:20957 [D loss: 0.520005, acc.: 75.00%] [G loss: 0.792616]\n",
      "epoch:22 step:20958 [D loss: 0.450808, acc.: 77.34%] [G loss: 0.863595]\n",
      "epoch:22 step:20959 [D loss: 0.590844, acc.: 64.84%] [G loss: 0.767668]\n",
      "epoch:22 step:20960 [D loss: 0.538330, acc.: 68.75%] [G loss: 0.904384]\n",
      "epoch:22 step:20961 [D loss: 0.423795, acc.: 82.03%] [G loss: 1.036557]\n",
      "epoch:22 step:20962 [D loss: 0.617618, acc.: 63.28%] [G loss: 0.786846]\n",
      "epoch:22 step:20963 [D loss: 0.716228, acc.: 54.69%] [G loss: 0.578556]\n",
      "epoch:22 step:20964 [D loss: 0.495822, acc.: 73.44%] [G loss: 0.546919]\n",
      "epoch:22 step:20965 [D loss: 0.522990, acc.: 71.88%] [G loss: 0.820703]\n",
      "epoch:22 step:20966 [D loss: 0.632406, acc.: 67.19%] [G loss: 0.829469]\n",
      "epoch:22 step:20967 [D loss: 0.577170, acc.: 67.97%] [G loss: 0.681339]\n",
      "epoch:22 step:20968 [D loss: 0.419218, acc.: 79.69%] [G loss: 0.861084]\n",
      "epoch:22 step:20969 [D loss: 0.556785, acc.: 68.75%] [G loss: 0.641995]\n",
      "epoch:22 step:20970 [D loss: 0.548705, acc.: 75.00%] [G loss: 0.673401]\n",
      "epoch:22 step:20971 [D loss: 0.462183, acc.: 78.12%] [G loss: 0.767070]\n",
      "epoch:22 step:20972 [D loss: 0.454378, acc.: 77.34%] [G loss: 0.827922]\n",
      "epoch:22 step:20973 [D loss: 0.546841, acc.: 71.88%] [G loss: 0.777056]\n",
      "epoch:22 step:20974 [D loss: 0.528524, acc.: 72.66%] [G loss: 0.701010]\n",
      "epoch:22 step:20975 [D loss: 0.495456, acc.: 76.56%] [G loss: 0.841366]\n",
      "epoch:22 step:20976 [D loss: 0.551536, acc.: 71.09%] [G loss: 0.657702]\n",
      "epoch:22 step:20977 [D loss: 0.601598, acc.: 64.06%] [G loss: 0.714722]\n",
      "epoch:22 step:20978 [D loss: 0.536251, acc.: 69.53%] [G loss: 0.717619]\n",
      "epoch:22 step:20979 [D loss: 0.582762, acc.: 68.75%] [G loss: 0.576531]\n",
      "epoch:22 step:20980 [D loss: 0.558493, acc.: 71.09%] [G loss: 0.675779]\n",
      "epoch:22 step:20981 [D loss: 0.545174, acc.: 69.53%] [G loss: 0.738811]\n",
      "epoch:22 step:20982 [D loss: 0.519070, acc.: 67.97%] [G loss: 0.664263]\n",
      "epoch:22 step:20983 [D loss: 0.474210, acc.: 78.12%] [G loss: 0.655543]\n",
      "epoch:22 step:20984 [D loss: 0.541677, acc.: 73.44%] [G loss: 0.634262]\n",
      "epoch:22 step:20985 [D loss: 0.476600, acc.: 71.88%] [G loss: 0.751516]\n",
      "epoch:22 step:20986 [D loss: 0.572433, acc.: 69.53%] [G loss: 0.775701]\n",
      "epoch:22 step:20987 [D loss: 0.514840, acc.: 73.44%] [G loss: 0.823196]\n",
      "epoch:22 step:20988 [D loss: 0.414842, acc.: 82.03%] [G loss: 0.845415]\n",
      "epoch:22 step:20989 [D loss: 0.518880, acc.: 74.22%] [G loss: 0.837106]\n",
      "epoch:22 step:20990 [D loss: 0.738375, acc.: 48.44%] [G loss: 0.465418]\n",
      "epoch:22 step:20991 [D loss: 0.536017, acc.: 69.53%] [G loss: 0.593621]\n",
      "epoch:22 step:20992 [D loss: 0.585009, acc.: 68.75%] [G loss: 0.721649]\n",
      "epoch:22 step:20993 [D loss: 0.515684, acc.: 75.78%] [G loss: 0.550464]\n",
      "epoch:22 step:20994 [D loss: 0.579184, acc.: 68.75%] [G loss: 0.494494]\n",
      "epoch:22 step:20995 [D loss: 0.464976, acc.: 76.56%] [G loss: 0.664591]\n",
      "epoch:22 step:20996 [D loss: 0.494041, acc.: 72.66%] [G loss: 0.624739]\n",
      "epoch:22 step:20997 [D loss: 0.532282, acc.: 67.97%] [G loss: 0.702014]\n",
      "epoch:22 step:20998 [D loss: 0.519288, acc.: 71.09%] [G loss: 0.954343]\n",
      "epoch:22 step:20999 [D loss: 0.486944, acc.: 75.00%] [G loss: 0.834228]\n",
      "epoch:22 step:21000 [D loss: 0.620270, acc.: 68.75%] [G loss: 0.603797]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.682087\n",
      "FID: 48.306988\n",
      "0 = 12.933559307384508\n",
      "1 = 0.09364503706670269\n",
      "2 = 0.8647000193595886\n",
      "3 = 0.8205999732017517\n",
      "4 = 0.9088000059127808\n",
      "5 = 0.8999780416488647\n",
      "6 = 0.8205999732017517\n",
      "7 = 8.28380283898116\n",
      "8 = 0.1521852108511022\n",
      "9 = 0.7056999802589417\n",
      "10 = 0.6962000131607056\n",
      "11 = 0.7152000069618225\n",
      "12 = 0.7096840143203735\n",
      "13 = 0.6962000131607056\n",
      "14 = 6.6821112632751465\n",
      "15 = 6.971405029296875\n",
      "16 = 0.3864741325378418\n",
      "17 = 6.682087421417236\n",
      "18 = 48.30698776245117\n",
      "epoch:22 step:21001 [D loss: 0.551902, acc.: 70.31%] [G loss: 0.669599]\n",
      "epoch:22 step:21002 [D loss: 0.524311, acc.: 75.00%] [G loss: 0.716941]\n",
      "epoch:22 step:21003 [D loss: 0.538905, acc.: 68.75%] [G loss: 0.627714]\n",
      "epoch:22 step:21004 [D loss: 0.598650, acc.: 68.75%] [G loss: 0.647164]\n",
      "epoch:22 step:21005 [D loss: 0.518667, acc.: 70.31%] [G loss: 0.655951]\n",
      "epoch:22 step:21006 [D loss: 0.485495, acc.: 75.78%] [G loss: 0.566726]\n",
      "epoch:22 step:21007 [D loss: 0.590135, acc.: 65.62%] [G loss: 0.496988]\n",
      "epoch:22 step:21008 [D loss: 0.612004, acc.: 69.53%] [G loss: 0.531999]\n",
      "epoch:22 step:21009 [D loss: 0.576633, acc.: 69.53%] [G loss: 0.628154]\n",
      "epoch:22 step:21010 [D loss: 0.586446, acc.: 65.62%] [G loss: 0.594219]\n",
      "epoch:22 step:21011 [D loss: 0.569856, acc.: 66.41%] [G loss: 0.725667]\n",
      "epoch:22 step:21012 [D loss: 0.473482, acc.: 75.00%] [G loss: 0.794432]\n",
      "epoch:22 step:21013 [D loss: 0.518275, acc.: 77.34%] [G loss: 0.757084]\n",
      "epoch:22 step:21014 [D loss: 0.583571, acc.: 60.16%] [G loss: 0.672479]\n",
      "epoch:22 step:21015 [D loss: 0.619524, acc.: 62.50%] [G loss: 0.490633]\n",
      "epoch:22 step:21016 [D loss: 0.552406, acc.: 68.75%] [G loss: 0.627170]\n",
      "epoch:22 step:21017 [D loss: 0.493508, acc.: 76.56%] [G loss: 0.671242]\n",
      "epoch:22 step:21018 [D loss: 0.575320, acc.: 65.62%] [G loss: 0.709289]\n",
      "epoch:22 step:21019 [D loss: 0.543007, acc.: 70.31%] [G loss: 0.558087]\n",
      "epoch:22 step:21020 [D loss: 0.446081, acc.: 77.34%] [G loss: 0.736597]\n",
      "epoch:22 step:21021 [D loss: 0.571944, acc.: 64.84%] [G loss: 0.689464]\n",
      "epoch:22 step:21022 [D loss: 0.557886, acc.: 71.09%] [G loss: 0.688242]\n",
      "epoch:22 step:21023 [D loss: 0.564563, acc.: 67.97%] [G loss: 0.707840]\n",
      "epoch:22 step:21024 [D loss: 0.627095, acc.: 65.62%] [G loss: 0.662155]\n",
      "epoch:22 step:21025 [D loss: 0.551940, acc.: 70.31%] [G loss: 0.574006]\n",
      "epoch:22 step:21026 [D loss: 0.630548, acc.: 53.12%] [G loss: 0.564186]\n",
      "epoch:22 step:21027 [D loss: 0.544964, acc.: 69.53%] [G loss: 0.526699]\n",
      "epoch:22 step:21028 [D loss: 0.521923, acc.: 73.44%] [G loss: 0.689379]\n",
      "epoch:22 step:21029 [D loss: 0.571297, acc.: 68.75%] [G loss: 0.575326]\n",
      "epoch:22 step:21030 [D loss: 0.547101, acc.: 70.31%] [G loss: 0.791259]\n",
      "epoch:22 step:21031 [D loss: 0.550393, acc.: 71.09%] [G loss: 0.673276]\n",
      "epoch:22 step:21032 [D loss: 0.631384, acc.: 57.03%] [G loss: 0.598232]\n",
      "epoch:22 step:21033 [D loss: 0.569286, acc.: 66.41%] [G loss: 0.646816]\n",
      "epoch:22 step:21034 [D loss: 0.639281, acc.: 60.16%] [G loss: 0.814508]\n",
      "epoch:22 step:21035 [D loss: 0.590978, acc.: 68.75%] [G loss: 0.631355]\n",
      "epoch:22 step:21036 [D loss: 0.591506, acc.: 67.97%] [G loss: 0.548735]\n",
      "epoch:22 step:21037 [D loss: 0.534516, acc.: 74.22%] [G loss: 0.637791]\n",
      "epoch:22 step:21038 [D loss: 0.570454, acc.: 59.38%] [G loss: 0.648552]\n",
      "epoch:22 step:21039 [D loss: 0.554984, acc.: 69.53%] [G loss: 0.619306]\n",
      "epoch:22 step:21040 [D loss: 0.487015, acc.: 75.78%] [G loss: 0.686937]\n",
      "epoch:22 step:21041 [D loss: 0.461190, acc.: 77.34%] [G loss: 0.670226]\n",
      "epoch:22 step:21042 [D loss: 0.509295, acc.: 74.22%] [G loss: 0.783862]\n",
      "epoch:22 step:21043 [D loss: 0.471224, acc.: 77.34%] [G loss: 0.855080]\n",
      "epoch:22 step:21044 [D loss: 0.465995, acc.: 75.78%] [G loss: 0.868397]\n",
      "epoch:22 step:21045 [D loss: 0.524112, acc.: 75.78%] [G loss: 0.762120]\n",
      "epoch:22 step:21046 [D loss: 0.581732, acc.: 69.53%] [G loss: 0.812747]\n",
      "epoch:22 step:21047 [D loss: 0.606763, acc.: 65.62%] [G loss: 0.610400]\n",
      "epoch:22 step:21048 [D loss: 0.543556, acc.: 75.00%] [G loss: 0.607613]\n",
      "epoch:22 step:21049 [D loss: 0.576771, acc.: 66.41%] [G loss: 0.613014]\n",
      "epoch:22 step:21050 [D loss: 0.476340, acc.: 75.78%] [G loss: 0.598422]\n",
      "epoch:22 step:21051 [D loss: 0.703572, acc.: 57.81%] [G loss: 0.455129]\n",
      "epoch:22 step:21052 [D loss: 0.532307, acc.: 69.53%] [G loss: 0.669420]\n",
      "epoch:22 step:21053 [D loss: 0.510451, acc.: 75.00%] [G loss: 0.742126]\n",
      "epoch:22 step:21054 [D loss: 0.501487, acc.: 75.00%] [G loss: 0.736720]\n",
      "epoch:22 step:21055 [D loss: 0.558248, acc.: 66.41%] [G loss: 0.798177]\n",
      "epoch:22 step:21056 [D loss: 0.564967, acc.: 70.31%] [G loss: 0.626993]\n",
      "epoch:22 step:21057 [D loss: 0.570468, acc.: 67.97%] [G loss: 0.615023]\n",
      "epoch:22 step:21058 [D loss: 0.494478, acc.: 77.34%] [G loss: 0.936900]\n",
      "epoch:22 step:21059 [D loss: 0.548867, acc.: 72.66%] [G loss: 0.740701]\n",
      "epoch:22 step:21060 [D loss: 0.507104, acc.: 75.00%] [G loss: 0.863868]\n",
      "epoch:22 step:21061 [D loss: 0.585892, acc.: 69.53%] [G loss: 0.711575]\n",
      "epoch:22 step:21062 [D loss: 0.567011, acc.: 69.53%] [G loss: 0.709729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21063 [D loss: 0.525465, acc.: 75.00%] [G loss: 0.775155]\n",
      "epoch:22 step:21064 [D loss: 0.528655, acc.: 72.66%] [G loss: 0.882510]\n",
      "epoch:22 step:21065 [D loss: 0.397586, acc.: 80.47%] [G loss: 0.917684]\n",
      "epoch:22 step:21066 [D loss: 0.524829, acc.: 71.88%] [G loss: 0.873721]\n",
      "epoch:22 step:21067 [D loss: 0.491753, acc.: 72.66%] [G loss: 0.866155]\n",
      "epoch:22 step:21068 [D loss: 0.525215, acc.: 75.78%] [G loss: 0.609194]\n",
      "epoch:22 step:21069 [D loss: 0.530059, acc.: 71.09%] [G loss: 0.731503]\n",
      "epoch:22 step:21070 [D loss: 0.646300, acc.: 62.50%] [G loss: 0.572214]\n",
      "epoch:22 step:21071 [D loss: 0.504426, acc.: 75.78%] [G loss: 0.800858]\n",
      "epoch:22 step:21072 [D loss: 0.640329, acc.: 63.28%] [G loss: 0.681630]\n",
      "epoch:22 step:21073 [D loss: 0.562494, acc.: 69.53%] [G loss: 0.604541]\n",
      "epoch:22 step:21074 [D loss: 0.540980, acc.: 71.88%] [G loss: 0.650620]\n",
      "epoch:22 step:21075 [D loss: 0.544579, acc.: 71.09%] [G loss: 0.686903]\n",
      "epoch:22 step:21076 [D loss: 0.565913, acc.: 67.97%] [G loss: 0.666644]\n",
      "epoch:22 step:21077 [D loss: 0.534940, acc.: 71.09%] [G loss: 0.666112]\n",
      "epoch:22 step:21078 [D loss: 0.536122, acc.: 71.88%] [G loss: 0.566997]\n",
      "epoch:22 step:21079 [D loss: 0.624489, acc.: 63.28%] [G loss: 0.631727]\n",
      "epoch:22 step:21080 [D loss: 0.568249, acc.: 67.97%] [G loss: 0.558011]\n",
      "epoch:22 step:21081 [D loss: 0.577986, acc.: 64.06%] [G loss: 0.663890]\n",
      "epoch:22 step:21082 [D loss: 0.510436, acc.: 72.66%] [G loss: 0.684142]\n",
      "epoch:22 step:21083 [D loss: 0.532566, acc.: 70.31%] [G loss: 0.914747]\n",
      "epoch:22 step:21084 [D loss: 0.498933, acc.: 74.22%] [G loss: 0.697276]\n",
      "epoch:22 step:21085 [D loss: 0.483723, acc.: 77.34%] [G loss: 0.773700]\n",
      "epoch:22 step:21086 [D loss: 0.417897, acc.: 80.47%] [G loss: 0.976243]\n",
      "epoch:22 step:21087 [D loss: 0.665925, acc.: 62.50%] [G loss: 0.759056]\n",
      "epoch:22 step:21088 [D loss: 0.549986, acc.: 70.31%] [G loss: 0.826994]\n",
      "epoch:22 step:21089 [D loss: 0.512135, acc.: 74.22%] [G loss: 0.865037]\n",
      "epoch:22 step:21090 [D loss: 0.579319, acc.: 69.53%] [G loss: 0.972128]\n",
      "epoch:22 step:21091 [D loss: 0.718770, acc.: 54.69%] [G loss: 0.503593]\n",
      "epoch:22 step:21092 [D loss: 0.590611, acc.: 64.84%] [G loss: 0.345511]\n",
      "epoch:22 step:21093 [D loss: 0.528491, acc.: 71.09%] [G loss: 0.577026]\n",
      "epoch:22 step:21094 [D loss: 0.583540, acc.: 70.31%] [G loss: 0.538616]\n",
      "epoch:22 step:21095 [D loss: 0.450058, acc.: 83.59%] [G loss: 0.670894]\n",
      "epoch:22 step:21096 [D loss: 0.618204, acc.: 66.41%] [G loss: 0.512951]\n",
      "epoch:22 step:21097 [D loss: 0.515807, acc.: 76.56%] [G loss: 0.585663]\n",
      "epoch:22 step:21098 [D loss: 0.461140, acc.: 78.12%] [G loss: 0.734828]\n",
      "epoch:22 step:21099 [D loss: 0.516322, acc.: 72.66%] [G loss: 0.710725]\n",
      "epoch:22 step:21100 [D loss: 0.546600, acc.: 69.53%] [G loss: 0.812182]\n",
      "epoch:22 step:21101 [D loss: 0.602190, acc.: 61.72%] [G loss: 0.677899]\n",
      "epoch:22 step:21102 [D loss: 0.513960, acc.: 72.66%] [G loss: 0.520820]\n",
      "epoch:22 step:21103 [D loss: 0.488042, acc.: 74.22%] [G loss: 0.663206]\n",
      "epoch:22 step:21104 [D loss: 0.598738, acc.: 71.09%] [G loss: 0.691930]\n",
      "epoch:22 step:21105 [D loss: 0.564002, acc.: 72.66%] [G loss: 0.734529]\n",
      "epoch:22 step:21106 [D loss: 0.608321, acc.: 66.41%] [G loss: 0.581499]\n",
      "epoch:22 step:21107 [D loss: 0.549546, acc.: 71.09%] [G loss: 0.512419]\n",
      "epoch:22 step:21108 [D loss: 0.581275, acc.: 65.62%] [G loss: 0.537267]\n",
      "epoch:22 step:21109 [D loss: 0.476625, acc.: 78.12%] [G loss: 0.716330]\n",
      "epoch:22 step:21110 [D loss: 0.517464, acc.: 72.66%] [G loss: 0.724581]\n",
      "epoch:22 step:21111 [D loss: 0.587414, acc.: 64.84%] [G loss: 0.721019]\n",
      "epoch:22 step:21112 [D loss: 0.536233, acc.: 70.31%] [G loss: 0.655433]\n",
      "epoch:22 step:21113 [D loss: 0.446578, acc.: 78.12%] [G loss: 0.828553]\n",
      "epoch:22 step:21114 [D loss: 0.562456, acc.: 69.53%] [G loss: 0.553545]\n",
      "epoch:22 step:21115 [D loss: 0.609538, acc.: 66.41%] [G loss: 0.639610]\n",
      "epoch:22 step:21116 [D loss: 0.613137, acc.: 67.19%] [G loss: 0.586612]\n",
      "epoch:22 step:21117 [D loss: 0.495688, acc.: 75.00%] [G loss: 0.530760]\n",
      "epoch:22 step:21118 [D loss: 0.489153, acc.: 73.44%] [G loss: 0.710020]\n",
      "epoch:22 step:21119 [D loss: 0.477507, acc.: 78.91%] [G loss: 0.796239]\n",
      "epoch:22 step:21120 [D loss: 0.542166, acc.: 68.75%] [G loss: 0.687816]\n",
      "epoch:22 step:21121 [D loss: 0.563830, acc.: 70.31%] [G loss: 0.657538]\n",
      "epoch:22 step:21122 [D loss: 0.456344, acc.: 81.25%] [G loss: 0.886427]\n",
      "epoch:22 step:21123 [D loss: 0.487675, acc.: 76.56%] [G loss: 0.758040]\n",
      "epoch:22 step:21124 [D loss: 0.675576, acc.: 57.81%] [G loss: 0.590706]\n",
      "epoch:22 step:21125 [D loss: 0.688272, acc.: 54.69%] [G loss: 0.508340]\n",
      "epoch:22 step:21126 [D loss: 0.626913, acc.: 62.50%] [G loss: 0.576035]\n",
      "epoch:22 step:21127 [D loss: 0.500022, acc.: 75.78%] [G loss: 0.706693]\n",
      "epoch:22 step:21128 [D loss: 0.510625, acc.: 71.09%] [G loss: 0.667403]\n",
      "epoch:22 step:21129 [D loss: 0.573666, acc.: 64.06%] [G loss: 0.820137]\n",
      "epoch:22 step:21130 [D loss: 0.509585, acc.: 72.66%] [G loss: 0.877596]\n",
      "epoch:22 step:21131 [D loss: 0.567990, acc.: 70.31%] [G loss: 0.577935]\n",
      "epoch:22 step:21132 [D loss: 0.533880, acc.: 74.22%] [G loss: 0.790391]\n",
      "epoch:22 step:21133 [D loss: 0.476002, acc.: 75.00%] [G loss: 0.804956]\n",
      "epoch:22 step:21134 [D loss: 0.469835, acc.: 75.78%] [G loss: 0.767036]\n",
      "epoch:22 step:21135 [D loss: 0.463531, acc.: 78.91%] [G loss: 0.702086]\n",
      "epoch:22 step:21136 [D loss: 0.497551, acc.: 78.12%] [G loss: 0.689296]\n",
      "epoch:22 step:21137 [D loss: 0.473039, acc.: 76.56%] [G loss: 0.770928]\n",
      "epoch:22 step:21138 [D loss: 0.517493, acc.: 73.44%] [G loss: 0.771775]\n",
      "epoch:22 step:21139 [D loss: 0.591863, acc.: 67.19%] [G loss: 0.677082]\n",
      "epoch:22 step:21140 [D loss: 0.532833, acc.: 70.31%] [G loss: 0.559160]\n",
      "epoch:22 step:21141 [D loss: 0.575737, acc.: 68.75%] [G loss: 0.597819]\n",
      "epoch:22 step:21142 [D loss: 0.665058, acc.: 54.69%] [G loss: 0.558603]\n",
      "epoch:22 step:21143 [D loss: 0.576516, acc.: 63.28%] [G loss: 0.679461]\n",
      "epoch:22 step:21144 [D loss: 0.568488, acc.: 67.97%] [G loss: 0.617890]\n",
      "epoch:22 step:21145 [D loss: 0.614430, acc.: 66.41%] [G loss: 0.705585]\n",
      "epoch:22 step:21146 [D loss: 0.551307, acc.: 71.09%] [G loss: 0.599536]\n",
      "epoch:22 step:21147 [D loss: 0.511664, acc.: 74.22%] [G loss: 0.701064]\n",
      "epoch:22 step:21148 [D loss: 0.516015, acc.: 72.66%] [G loss: 0.682025]\n",
      "epoch:22 step:21149 [D loss: 0.598354, acc.: 63.28%] [G loss: 0.718619]\n",
      "epoch:22 step:21150 [D loss: 0.504060, acc.: 71.88%] [G loss: 0.632512]\n",
      "epoch:22 step:21151 [D loss: 0.588677, acc.: 67.19%] [G loss: 0.599759]\n",
      "epoch:22 step:21152 [D loss: 0.602930, acc.: 69.53%] [G loss: 0.533531]\n",
      "epoch:22 step:21153 [D loss: 0.583651, acc.: 64.84%] [G loss: 0.544198]\n",
      "epoch:22 step:21154 [D loss: 0.570864, acc.: 65.62%] [G loss: 0.651238]\n",
      "epoch:22 step:21155 [D loss: 0.526211, acc.: 71.09%] [G loss: 0.690849]\n",
      "epoch:22 step:21156 [D loss: 0.584951, acc.: 64.84%] [G loss: 0.538544]\n",
      "epoch:22 step:21157 [D loss: 0.577114, acc.: 66.41%] [G loss: 0.510306]\n",
      "epoch:22 step:21158 [D loss: 0.604539, acc.: 67.19%] [G loss: 0.576522]\n",
      "epoch:22 step:21159 [D loss: 0.565211, acc.: 73.44%] [G loss: 0.718282]\n",
      "epoch:22 step:21160 [D loss: 0.531463, acc.: 74.22%] [G loss: 0.641617]\n",
      "epoch:22 step:21161 [D loss: 0.548396, acc.: 71.88%] [G loss: 0.660152]\n",
      "epoch:22 step:21162 [D loss: 0.509604, acc.: 73.44%] [G loss: 0.724222]\n",
      "epoch:22 step:21163 [D loss: 0.546844, acc.: 73.44%] [G loss: 0.663748]\n",
      "epoch:22 step:21164 [D loss: 0.554111, acc.: 71.88%] [G loss: 0.534018]\n",
      "epoch:22 step:21165 [D loss: 0.514119, acc.: 71.88%] [G loss: 0.629367]\n",
      "epoch:22 step:21166 [D loss: 0.485875, acc.: 75.78%] [G loss: 0.748964]\n",
      "epoch:22 step:21167 [D loss: 0.571903, acc.: 69.53%] [G loss: 0.695246]\n",
      "epoch:22 step:21168 [D loss: 0.456791, acc.: 75.00%] [G loss: 0.601969]\n",
      "epoch:22 step:21169 [D loss: 0.464803, acc.: 80.47%] [G loss: 0.873831]\n",
      "epoch:22 step:21170 [D loss: 0.558666, acc.: 71.88%] [G loss: 0.729300]\n",
      "epoch:22 step:21171 [D loss: 0.479180, acc.: 78.12%] [G loss: 0.811508]\n",
      "epoch:22 step:21172 [D loss: 0.512237, acc.: 78.12%] [G loss: 0.814816]\n",
      "epoch:22 step:21173 [D loss: 0.593669, acc.: 68.75%] [G loss: 0.734508]\n",
      "epoch:22 step:21174 [D loss: 0.551469, acc.: 71.88%] [G loss: 0.531453]\n",
      "epoch:22 step:21175 [D loss: 0.576636, acc.: 65.62%] [G loss: 0.678612]\n",
      "epoch:22 step:21176 [D loss: 0.617032, acc.: 62.50%] [G loss: 0.653739]\n",
      "epoch:22 step:21177 [D loss: 0.562882, acc.: 67.19%] [G loss: 0.621823]\n",
      "epoch:22 step:21178 [D loss: 0.485663, acc.: 75.78%] [G loss: 0.630163]\n",
      "epoch:22 step:21179 [D loss: 0.547044, acc.: 73.44%] [G loss: 0.715482]\n",
      "epoch:22 step:21180 [D loss: 0.664153, acc.: 59.38%] [G loss: 0.705882]\n",
      "epoch:22 step:21181 [D loss: 0.470851, acc.: 79.69%] [G loss: 0.605181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21182 [D loss: 0.508327, acc.: 72.66%] [G loss: 0.671432]\n",
      "epoch:22 step:21183 [D loss: 0.593372, acc.: 68.75%] [G loss: 0.655856]\n",
      "epoch:22 step:21184 [D loss: 0.588272, acc.: 71.09%] [G loss: 0.809110]\n",
      "epoch:22 step:21185 [D loss: 0.501666, acc.: 77.34%] [G loss: 0.725546]\n",
      "epoch:22 step:21186 [D loss: 0.602763, acc.: 62.50%] [G loss: 0.560452]\n",
      "epoch:22 step:21187 [D loss: 0.549749, acc.: 64.84%] [G loss: 0.763754]\n",
      "epoch:22 step:21188 [D loss: 0.486865, acc.: 75.78%] [G loss: 0.591744]\n",
      "epoch:22 step:21189 [D loss: 0.474246, acc.: 77.34%] [G loss: 0.817956]\n",
      "epoch:22 step:21190 [D loss: 0.615974, acc.: 69.53%] [G loss: 0.651511]\n",
      "epoch:22 step:21191 [D loss: 0.540457, acc.: 74.22%] [G loss: 0.664869]\n",
      "epoch:22 step:21192 [D loss: 0.523853, acc.: 70.31%] [G loss: 0.612863]\n",
      "epoch:22 step:21193 [D loss: 0.531414, acc.: 71.09%] [G loss: 0.643583]\n",
      "epoch:22 step:21194 [D loss: 0.564073, acc.: 70.31%] [G loss: 0.635403]\n",
      "epoch:22 step:21195 [D loss: 0.574645, acc.: 67.97%] [G loss: 0.628301]\n",
      "epoch:22 step:21196 [D loss: 0.430237, acc.: 84.38%] [G loss: 0.899489]\n",
      "epoch:22 step:21197 [D loss: 0.565876, acc.: 70.31%] [G loss: 0.886465]\n",
      "epoch:22 step:21198 [D loss: 0.581750, acc.: 67.19%] [G loss: 0.591548]\n",
      "epoch:22 step:21199 [D loss: 0.574426, acc.: 66.41%] [G loss: 0.582989]\n",
      "epoch:22 step:21200 [D loss: 0.528556, acc.: 70.31%] [G loss: 0.764087]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.720188\n",
      "FID: 46.746395\n",
      "0 = 12.773969326114686\n",
      "1 = 0.08728906547172867\n",
      "2 = 0.8521999716758728\n",
      "3 = 0.8014000058174133\n",
      "4 = 0.902999997138977\n",
      "5 = 0.8920302987098694\n",
      "6 = 0.8014000058174133\n",
      "7 = 8.1557033695221\n",
      "8 = 0.14710034878065095\n",
      "9 = 0.6899999976158142\n",
      "10 = 0.6894000172615051\n",
      "11 = 0.6905999779701233\n",
      "12 = 0.690228283405304\n",
      "13 = 0.6894000172615051\n",
      "14 = 6.720217227935791\n",
      "15 = 6.9216389656066895\n",
      "16 = 0.3924568295478821\n",
      "17 = 6.720187664031982\n",
      "18 = 46.746395111083984\n",
      "epoch:22 step:21201 [D loss: 0.595704, acc.: 68.75%] [G loss: 0.551551]\n",
      "epoch:22 step:21202 [D loss: 0.573430, acc.: 66.41%] [G loss: 0.577070]\n",
      "epoch:22 step:21203 [D loss: 0.569162, acc.: 63.28%] [G loss: 0.685545]\n",
      "epoch:22 step:21204 [D loss: 0.527274, acc.: 72.66%] [G loss: 0.541045]\n",
      "epoch:22 step:21205 [D loss: 0.565246, acc.: 71.88%] [G loss: 0.719895]\n",
      "epoch:22 step:21206 [D loss: 0.543713, acc.: 71.88%] [G loss: 0.703465]\n",
      "epoch:22 step:21207 [D loss: 0.494622, acc.: 75.78%] [G loss: 0.801045]\n",
      "epoch:22 step:21208 [D loss: 0.568394, acc.: 70.31%] [G loss: 0.531932]\n",
      "epoch:22 step:21209 [D loss: 0.544521, acc.: 69.53%] [G loss: 0.422708]\n",
      "epoch:22 step:21210 [D loss: 0.533879, acc.: 69.53%] [G loss: 0.511574]\n",
      "epoch:22 step:21211 [D loss: 0.588705, acc.: 67.97%] [G loss: 0.590405]\n",
      "epoch:22 step:21212 [D loss: 0.513292, acc.: 74.22%] [G loss: 0.732285]\n",
      "epoch:22 step:21213 [D loss: 0.491346, acc.: 74.22%] [G loss: 0.786063]\n",
      "epoch:22 step:21214 [D loss: 0.615464, acc.: 65.62%] [G loss: 0.638094]\n",
      "epoch:22 step:21215 [D loss: 0.575498, acc.: 69.53%] [G loss: 0.636141]\n",
      "epoch:22 step:21216 [D loss: 0.513434, acc.: 73.44%] [G loss: 0.587548]\n",
      "epoch:22 step:21217 [D loss: 0.520708, acc.: 76.56%] [G loss: 0.804538]\n",
      "epoch:22 step:21218 [D loss: 0.569786, acc.: 71.09%] [G loss: 0.929497]\n",
      "epoch:22 step:21219 [D loss: 0.427963, acc.: 79.69%] [G loss: 0.712232]\n",
      "epoch:22 step:21220 [D loss: 0.604562, acc.: 63.28%] [G loss: 0.571501]\n",
      "epoch:22 step:21221 [D loss: 0.539151, acc.: 69.53%] [G loss: 0.534260]\n",
      "epoch:22 step:21222 [D loss: 0.583924, acc.: 68.75%] [G loss: 0.528286]\n",
      "epoch:22 step:21223 [D loss: 0.474831, acc.: 78.91%] [G loss: 0.646907]\n",
      "epoch:22 step:21224 [D loss: 0.567033, acc.: 68.75%] [G loss: 0.447732]\n",
      "epoch:22 step:21225 [D loss: 0.525331, acc.: 72.66%] [G loss: 0.512818]\n",
      "epoch:22 step:21226 [D loss: 0.568998, acc.: 69.53%] [G loss: 0.582125]\n",
      "epoch:22 step:21227 [D loss: 0.498977, acc.: 72.66%] [G loss: 0.664904]\n",
      "epoch:22 step:21228 [D loss: 0.599119, acc.: 64.06%] [G loss: 0.587207]\n",
      "epoch:22 step:21229 [D loss: 0.593688, acc.: 69.53%] [G loss: 0.591010]\n",
      "epoch:22 step:21230 [D loss: 0.559111, acc.: 67.19%] [G loss: 0.767848]\n",
      "epoch:22 step:21231 [D loss: 0.533807, acc.: 69.53%] [G loss: 0.749498]\n",
      "epoch:22 step:21232 [D loss: 0.599402, acc.: 64.06%] [G loss: 0.940557]\n",
      "epoch:22 step:21233 [D loss: 0.551724, acc.: 67.19%] [G loss: 0.799455]\n",
      "epoch:22 step:21234 [D loss: 0.565950, acc.: 68.75%] [G loss: 0.790015]\n",
      "epoch:22 step:21235 [D loss: 0.573654, acc.: 66.41%] [G loss: 0.812941]\n",
      "epoch:22 step:21236 [D loss: 0.602411, acc.: 60.16%] [G loss: 0.545227]\n",
      "epoch:22 step:21237 [D loss: 0.480399, acc.: 73.44%] [G loss: 0.693637]\n",
      "epoch:22 step:21238 [D loss: 0.439254, acc.: 79.69%] [G loss: 0.877911]\n",
      "epoch:22 step:21239 [D loss: 0.563140, acc.: 68.75%] [G loss: 0.542019]\n",
      "epoch:22 step:21240 [D loss: 0.535388, acc.: 69.53%] [G loss: 0.608001]\n",
      "epoch:22 step:21241 [D loss: 0.549437, acc.: 66.41%] [G loss: 0.633113]\n",
      "epoch:22 step:21242 [D loss: 0.573871, acc.: 65.62%] [G loss: 0.593363]\n",
      "epoch:22 step:21243 [D loss: 0.500030, acc.: 71.88%] [G loss: 0.712539]\n",
      "epoch:22 step:21244 [D loss: 0.517306, acc.: 73.44%] [G loss: 0.574488]\n",
      "epoch:22 step:21245 [D loss: 0.512352, acc.: 75.78%] [G loss: 0.669113]\n",
      "epoch:22 step:21246 [D loss: 0.480423, acc.: 75.78%] [G loss: 0.732147]\n",
      "epoch:22 step:21247 [D loss: 0.446701, acc.: 82.81%] [G loss: 0.813196]\n",
      "epoch:22 step:21248 [D loss: 0.458485, acc.: 76.56%] [G loss: 0.749814]\n",
      "epoch:22 step:21249 [D loss: 0.472106, acc.: 76.56%] [G loss: 0.797658]\n",
      "epoch:22 step:21250 [D loss: 0.556516, acc.: 71.88%] [G loss: 0.709545]\n",
      "epoch:22 step:21251 [D loss: 0.548709, acc.: 72.66%] [G loss: 0.581299]\n",
      "epoch:22 step:21252 [D loss: 0.524904, acc.: 74.22%] [G loss: 0.673694]\n",
      "epoch:22 step:21253 [D loss: 0.491892, acc.: 74.22%] [G loss: 0.580998]\n",
      "epoch:22 step:21254 [D loss: 0.576115, acc.: 67.97%] [G loss: 0.582106]\n",
      "epoch:22 step:21255 [D loss: 0.548339, acc.: 67.19%] [G loss: 0.629205]\n",
      "epoch:22 step:21256 [D loss: 0.510339, acc.: 75.00%] [G loss: 0.704394]\n",
      "epoch:22 step:21257 [D loss: 0.502656, acc.: 70.31%] [G loss: 0.796511]\n",
      "epoch:22 step:21258 [D loss: 0.551342, acc.: 72.66%] [G loss: 0.642278]\n",
      "epoch:22 step:21259 [D loss: 0.520028, acc.: 71.09%] [G loss: 0.828836]\n",
      "epoch:22 step:21260 [D loss: 0.502450, acc.: 72.66%] [G loss: 0.588610]\n",
      "epoch:22 step:21261 [D loss: 0.420502, acc.: 83.59%] [G loss: 0.857134]\n",
      "epoch:22 step:21262 [D loss: 0.443110, acc.: 77.34%] [G loss: 0.933153]\n",
      "epoch:22 step:21263 [D loss: 0.450525, acc.: 78.91%] [G loss: 0.997646]\n",
      "epoch:22 step:21264 [D loss: 0.495156, acc.: 73.44%] [G loss: 0.989375]\n",
      "epoch:22 step:21265 [D loss: 0.562562, acc.: 68.75%] [G loss: 0.898330]\n",
      "epoch:22 step:21266 [D loss: 0.661508, acc.: 59.38%] [G loss: 0.716030]\n",
      "epoch:22 step:21267 [D loss: 0.582949, acc.: 69.53%] [G loss: 0.602312]\n",
      "epoch:22 step:21268 [D loss: 0.479693, acc.: 74.22%] [G loss: 0.974906]\n",
      "epoch:22 step:21269 [D loss: 0.515688, acc.: 78.12%] [G loss: 0.795377]\n",
      "epoch:22 step:21270 [D loss: 0.566517, acc.: 70.31%] [G loss: 0.654623]\n",
      "epoch:22 step:21271 [D loss: 0.511018, acc.: 72.66%] [G loss: 0.712647]\n",
      "epoch:22 step:21272 [D loss: 0.562039, acc.: 67.19%] [G loss: 0.706653]\n",
      "epoch:22 step:21273 [D loss: 0.536162, acc.: 71.88%] [G loss: 0.978816]\n",
      "epoch:22 step:21274 [D loss: 0.571222, acc.: 69.53%] [G loss: 0.901168]\n",
      "epoch:22 step:21275 [D loss: 0.505398, acc.: 75.00%] [G loss: 0.839347]\n",
      "epoch:22 step:21276 [D loss: 0.545427, acc.: 67.97%] [G loss: 0.672883]\n",
      "epoch:22 step:21277 [D loss: 0.543174, acc.: 72.66%] [G loss: 0.760427]\n",
      "epoch:22 step:21278 [D loss: 0.612825, acc.: 64.06%] [G loss: 0.722458]\n",
      "epoch:22 step:21279 [D loss: 0.609303, acc.: 70.31%] [G loss: 0.817119]\n",
      "epoch:22 step:21280 [D loss: 0.586550, acc.: 67.97%] [G loss: 0.699137]\n",
      "epoch:22 step:21281 [D loss: 0.631623, acc.: 62.50%] [G loss: 0.573280]\n",
      "epoch:22 step:21282 [D loss: 0.532364, acc.: 76.56%] [G loss: 0.520797]\n",
      "epoch:22 step:21283 [D loss: 0.509963, acc.: 73.44%] [G loss: 0.654804]\n",
      "epoch:22 step:21284 [D loss: 0.553555, acc.: 71.88%] [G loss: 0.581941]\n",
      "epoch:22 step:21285 [D loss: 0.496683, acc.: 75.00%] [G loss: 0.600155]\n",
      "epoch:22 step:21286 [D loss: 0.534936, acc.: 72.66%] [G loss: 0.652242]\n",
      "epoch:22 step:21287 [D loss: 0.568076, acc.: 67.19%] [G loss: 0.676905]\n",
      "epoch:22 step:21288 [D loss: 0.531413, acc.: 69.53%] [G loss: 0.667061]\n",
      "epoch:22 step:21289 [D loss: 0.610750, acc.: 67.97%] [G loss: 0.500673]\n",
      "epoch:22 step:21290 [D loss: 0.515797, acc.: 71.09%] [G loss: 0.565948]\n",
      "epoch:22 step:21291 [D loss: 0.510198, acc.: 71.09%] [G loss: 0.556780]\n",
      "epoch:22 step:21292 [D loss: 0.605879, acc.: 67.97%] [G loss: 0.638951]\n",
      "epoch:22 step:21293 [D loss: 0.527415, acc.: 69.53%] [G loss: 0.549458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21294 [D loss: 0.564871, acc.: 70.31%] [G loss: 0.590861]\n",
      "epoch:22 step:21295 [D loss: 0.449844, acc.: 78.12%] [G loss: 0.570527]\n",
      "epoch:22 step:21296 [D loss: 0.552197, acc.: 69.53%] [G loss: 0.617138]\n",
      "epoch:22 step:21297 [D loss: 0.532475, acc.: 69.53%] [G loss: 0.593935]\n",
      "epoch:22 step:21298 [D loss: 0.612884, acc.: 65.62%] [G loss: 0.449732]\n",
      "epoch:22 step:21299 [D loss: 0.518934, acc.: 75.78%] [G loss: 0.606917]\n",
      "epoch:22 step:21300 [D loss: 0.570408, acc.: 66.41%] [G loss: 0.631956]\n",
      "epoch:22 step:21301 [D loss: 0.553826, acc.: 66.41%] [G loss: 0.584754]\n",
      "epoch:22 step:21302 [D loss: 0.495378, acc.: 75.78%] [G loss: 0.615666]\n",
      "epoch:22 step:21303 [D loss: 0.535366, acc.: 75.78%] [G loss: 0.719257]\n",
      "epoch:22 step:21304 [D loss: 0.516579, acc.: 75.78%] [G loss: 0.704775]\n",
      "epoch:22 step:21305 [D loss: 0.538241, acc.: 71.09%] [G loss: 0.596813]\n",
      "epoch:22 step:21306 [D loss: 0.538405, acc.: 73.44%] [G loss: 0.560555]\n",
      "epoch:22 step:21307 [D loss: 0.464778, acc.: 75.78%] [G loss: 0.912250]\n",
      "epoch:22 step:21308 [D loss: 0.475389, acc.: 73.44%] [G loss: 0.815151]\n",
      "epoch:22 step:21309 [D loss: 0.537198, acc.: 75.00%] [G loss: 0.697892]\n",
      "epoch:22 step:21310 [D loss: 0.647631, acc.: 60.16%] [G loss: 0.669013]\n",
      "epoch:22 step:21311 [D loss: 0.575020, acc.: 66.41%] [G loss: 0.567676]\n",
      "epoch:22 step:21312 [D loss: 0.619320, acc.: 67.97%] [G loss: 0.568443]\n",
      "epoch:22 step:21313 [D loss: 0.560384, acc.: 71.09%] [G loss: 0.676788]\n",
      "epoch:22 step:21314 [D loss: 0.565481, acc.: 67.19%] [G loss: 0.849041]\n",
      "epoch:22 step:21315 [D loss: 0.470113, acc.: 76.56%] [G loss: 1.148157]\n",
      "epoch:22 step:21316 [D loss: 0.665820, acc.: 61.72%] [G loss: 0.669245]\n",
      "epoch:22 step:21317 [D loss: 0.608127, acc.: 64.84%] [G loss: 0.525236]\n",
      "epoch:22 step:21318 [D loss: 0.611751, acc.: 59.38%] [G loss: 0.484890]\n",
      "epoch:22 step:21319 [D loss: 0.532705, acc.: 72.66%] [G loss: 0.671823]\n",
      "epoch:22 step:21320 [D loss: 0.540820, acc.: 71.09%] [G loss: 0.594058]\n",
      "epoch:22 step:21321 [D loss: 0.478099, acc.: 74.22%] [G loss: 0.714438]\n",
      "epoch:22 step:21322 [D loss: 0.516423, acc.: 77.34%] [G loss: 0.639371]\n",
      "epoch:22 step:21323 [D loss: 0.478060, acc.: 75.00%] [G loss: 0.776158]\n",
      "epoch:22 step:21324 [D loss: 0.616969, acc.: 63.28%] [G loss: 0.716897]\n",
      "epoch:22 step:21325 [D loss: 0.562088, acc.: 71.09%] [G loss: 0.582101]\n",
      "epoch:22 step:21326 [D loss: 0.552776, acc.: 70.31%] [G loss: 0.786737]\n",
      "epoch:22 step:21327 [D loss: 0.611706, acc.: 64.06%] [G loss: 0.645680]\n",
      "epoch:22 step:21328 [D loss: 0.524383, acc.: 71.09%] [G loss: 0.583249]\n",
      "epoch:22 step:21329 [D loss: 0.547935, acc.: 68.75%] [G loss: 0.611231]\n",
      "epoch:22 step:21330 [D loss: 0.601587, acc.: 64.84%] [G loss: 0.584222]\n",
      "epoch:22 step:21331 [D loss: 0.525628, acc.: 75.00%] [G loss: 0.520887]\n",
      "epoch:22 step:21332 [D loss: 0.580452, acc.: 65.62%] [G loss: 0.628103]\n",
      "epoch:22 step:21333 [D loss: 0.480753, acc.: 75.00%] [G loss: 0.723210]\n",
      "epoch:22 step:21334 [D loss: 0.601843, acc.: 71.09%] [G loss: 0.676840]\n",
      "epoch:22 step:21335 [D loss: 0.620740, acc.: 67.19%] [G loss: 0.623857]\n",
      "epoch:22 step:21336 [D loss: 0.591876, acc.: 67.19%] [G loss: 0.631379]\n",
      "epoch:22 step:21337 [D loss: 0.587916, acc.: 68.75%] [G loss: 0.543932]\n",
      "epoch:22 step:21338 [D loss: 0.482987, acc.: 76.56%] [G loss: 0.695016]\n",
      "epoch:22 step:21339 [D loss: 0.467838, acc.: 78.12%] [G loss: 0.656546]\n",
      "epoch:22 step:21340 [D loss: 0.523835, acc.: 74.22%] [G loss: 0.687886]\n",
      "epoch:22 step:21341 [D loss: 0.567656, acc.: 67.19%] [G loss: 0.499288]\n",
      "epoch:22 step:21342 [D loss: 0.487478, acc.: 77.34%] [G loss: 0.711005]\n",
      "epoch:22 step:21343 [D loss: 0.577344, acc.: 65.62%] [G loss: 0.520452]\n",
      "epoch:22 step:21344 [D loss: 0.452515, acc.: 80.47%] [G loss: 0.710088]\n",
      "epoch:22 step:21345 [D loss: 0.594859, acc.: 64.06%] [G loss: 0.668426]\n",
      "epoch:22 step:21346 [D loss: 0.568695, acc.: 64.84%] [G loss: 0.576180]\n",
      "epoch:22 step:21347 [D loss: 0.526663, acc.: 69.53%] [G loss: 0.510569]\n",
      "epoch:22 step:21348 [D loss: 0.521202, acc.: 73.44%] [G loss: 0.577355]\n",
      "epoch:22 step:21349 [D loss: 0.571014, acc.: 71.09%] [G loss: 0.647843]\n",
      "epoch:22 step:21350 [D loss: 0.496102, acc.: 73.44%] [G loss: 0.683906]\n",
      "epoch:22 step:21351 [D loss: 0.508307, acc.: 73.44%] [G loss: 0.598979]\n",
      "epoch:22 step:21352 [D loss: 0.570512, acc.: 68.75%] [G loss: 0.622871]\n",
      "epoch:22 step:21353 [D loss: 0.649170, acc.: 64.06%] [G loss: 0.517241]\n",
      "epoch:22 step:21354 [D loss: 0.665685, acc.: 57.81%] [G loss: 0.498139]\n",
      "epoch:22 step:21355 [D loss: 0.506104, acc.: 74.22%] [G loss: 0.725130]\n",
      "epoch:22 step:21356 [D loss: 0.542986, acc.: 71.09%] [G loss: 0.547490]\n",
      "epoch:22 step:21357 [D loss: 0.515066, acc.: 72.66%] [G loss: 0.663070]\n",
      "epoch:22 step:21358 [D loss: 0.490085, acc.: 79.69%] [G loss: 0.744382]\n",
      "epoch:22 step:21359 [D loss: 0.569307, acc.: 64.84%] [G loss: 0.668029]\n",
      "epoch:22 step:21360 [D loss: 0.459386, acc.: 78.12%] [G loss: 0.710676]\n",
      "epoch:22 step:21361 [D loss: 0.420842, acc.: 81.25%] [G loss: 0.914022]\n",
      "epoch:22 step:21362 [D loss: 0.571369, acc.: 69.53%] [G loss: 0.668029]\n",
      "epoch:22 step:21363 [D loss: 0.499057, acc.: 75.78%] [G loss: 0.662014]\n",
      "epoch:22 step:21364 [D loss: 0.524604, acc.: 73.44%] [G loss: 0.727209]\n",
      "epoch:22 step:21365 [D loss: 0.490519, acc.: 74.22%] [G loss: 0.857699]\n",
      "epoch:22 step:21366 [D loss: 0.553128, acc.: 70.31%] [G loss: 0.705364]\n",
      "epoch:22 step:21367 [D loss: 0.556142, acc.: 68.75%] [G loss: 0.775187]\n",
      "epoch:22 step:21368 [D loss: 0.506805, acc.: 73.44%] [G loss: 0.707752]\n",
      "epoch:22 step:21369 [D loss: 0.528724, acc.: 69.53%] [G loss: 0.729688]\n",
      "epoch:22 step:21370 [D loss: 0.514623, acc.: 74.22%] [G loss: 0.824027]\n",
      "epoch:22 step:21371 [D loss: 0.558461, acc.: 67.97%] [G loss: 0.558784]\n",
      "epoch:22 step:21372 [D loss: 0.575539, acc.: 68.75%] [G loss: 0.794468]\n",
      "epoch:22 step:21373 [D loss: 0.600257, acc.: 67.97%] [G loss: 0.685929]\n",
      "epoch:22 step:21374 [D loss: 0.549934, acc.: 70.31%] [G loss: 0.582619]\n",
      "epoch:22 step:21375 [D loss: 0.568877, acc.: 69.53%] [G loss: 0.614339]\n",
      "epoch:22 step:21376 [D loss: 0.577919, acc.: 66.41%] [G loss: 0.571618]\n",
      "epoch:22 step:21377 [D loss: 0.573694, acc.: 71.88%] [G loss: 0.607513]\n",
      "epoch:22 step:21378 [D loss: 0.584094, acc.: 64.84%] [G loss: 0.555979]\n",
      "epoch:22 step:21379 [D loss: 0.602592, acc.: 67.97%] [G loss: 0.621098]\n",
      "epoch:22 step:21380 [D loss: 0.662309, acc.: 59.38%] [G loss: 0.472621]\n",
      "epoch:22 step:21381 [D loss: 0.504591, acc.: 78.91%] [G loss: 0.572264]\n",
      "epoch:22 step:21382 [D loss: 0.485670, acc.: 75.00%] [G loss: 0.785532]\n",
      "epoch:22 step:21383 [D loss: 0.490136, acc.: 75.78%] [G loss: 0.922961]\n",
      "epoch:22 step:21384 [D loss: 0.529030, acc.: 67.97%] [G loss: 0.964677]\n",
      "epoch:22 step:21385 [D loss: 0.593078, acc.: 67.19%] [G loss: 0.732598]\n",
      "epoch:22 step:21386 [D loss: 0.550330, acc.: 65.62%] [G loss: 0.693295]\n",
      "epoch:22 step:21387 [D loss: 0.562556, acc.: 71.09%] [G loss: 0.657043]\n",
      "epoch:22 step:21388 [D loss: 0.530157, acc.: 72.66%] [G loss: 0.630528]\n",
      "epoch:22 step:21389 [D loss: 0.520653, acc.: 74.22%] [G loss: 0.716355]\n",
      "epoch:22 step:21390 [D loss: 0.599320, acc.: 67.19%] [G loss: 0.732152]\n",
      "epoch:22 step:21391 [D loss: 0.560937, acc.: 68.75%] [G loss: 0.546188]\n",
      "epoch:22 step:21392 [D loss: 0.540279, acc.: 69.53%] [G loss: 0.673624]\n",
      "epoch:22 step:21393 [D loss: 0.567052, acc.: 69.53%] [G loss: 0.617375]\n",
      "epoch:22 step:21394 [D loss: 0.502124, acc.: 75.00%] [G loss: 0.765518]\n",
      "epoch:22 step:21395 [D loss: 0.504427, acc.: 75.78%] [G loss: 0.727866]\n",
      "epoch:22 step:21396 [D loss: 0.549362, acc.: 69.53%] [G loss: 0.886591]\n",
      "epoch:22 step:21397 [D loss: 0.544883, acc.: 71.09%] [G loss: 0.818718]\n",
      "epoch:22 step:21398 [D loss: 0.629574, acc.: 61.72%] [G loss: 0.479224]\n",
      "epoch:22 step:21399 [D loss: 0.519513, acc.: 69.53%] [G loss: 0.594822]\n",
      "epoch:22 step:21400 [D loss: 0.543096, acc.: 65.62%] [G loss: 0.682332]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.653027\n",
      "FID: 49.755993\n",
      "0 = 12.804311704444913\n",
      "1 = 0.0835082285133097\n",
      "2 = 0.8601999878883362\n",
      "3 = 0.8086000084877014\n",
      "4 = 0.9118000268936157\n",
      "5 = 0.9016503095626831\n",
      "6 = 0.8086000084877014\n",
      "7 = 8.331440546119206\n",
      "8 = 0.1518570387780574\n",
      "9 = 0.703499972820282\n",
      "10 = 0.6880000233650208\n",
      "11 = 0.718999981880188\n",
      "12 = 0.710010290145874\n",
      "13 = 0.6880000233650208\n",
      "14 = 6.653048515319824\n",
      "15 = 6.926130771636963\n",
      "16 = 0.3902175724506378\n",
      "17 = 6.653027057647705\n",
      "18 = 49.7559928894043\n",
      "epoch:22 step:21401 [D loss: 0.646151, acc.: 63.28%] [G loss: 0.509724]\n",
      "epoch:22 step:21402 [D loss: 0.658345, acc.: 57.81%] [G loss: 0.528231]\n",
      "epoch:22 step:21403 [D loss: 0.517166, acc.: 69.53%] [G loss: 0.618576]\n",
      "epoch:22 step:21404 [D loss: 0.527638, acc.: 73.44%] [G loss: 0.663298]\n",
      "epoch:22 step:21405 [D loss: 0.586741, acc.: 66.41%] [G loss: 0.576349]\n",
      "epoch:22 step:21406 [D loss: 0.483533, acc.: 76.56%] [G loss: 0.648976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21407 [D loss: 0.553929, acc.: 65.62%] [G loss: 0.579789]\n",
      "epoch:22 step:21408 [D loss: 0.656526, acc.: 58.59%] [G loss: 0.589204]\n",
      "epoch:22 step:21409 [D loss: 0.549744, acc.: 68.75%] [G loss: 0.715545]\n",
      "epoch:22 step:21410 [D loss: 0.567546, acc.: 65.62%] [G loss: 0.744463]\n",
      "epoch:22 step:21411 [D loss: 0.594084, acc.: 64.84%] [G loss: 0.743413]\n",
      "epoch:22 step:21412 [D loss: 0.517164, acc.: 71.88%] [G loss: 0.707162]\n",
      "epoch:22 step:21413 [D loss: 0.625994, acc.: 63.28%] [G loss: 0.484157]\n",
      "epoch:22 step:21414 [D loss: 0.519551, acc.: 72.66%] [G loss: 0.634219]\n",
      "epoch:22 step:21415 [D loss: 0.502879, acc.: 72.66%] [G loss: 0.709576]\n",
      "epoch:22 step:21416 [D loss: 0.512923, acc.: 74.22%] [G loss: 0.735937]\n",
      "epoch:22 step:21417 [D loss: 0.474706, acc.: 74.22%] [G loss: 0.842216]\n",
      "epoch:22 step:21418 [D loss: 0.588942, acc.: 63.28%] [G loss: 0.704460]\n",
      "epoch:22 step:21419 [D loss: 0.559225, acc.: 67.19%] [G loss: 0.545935]\n",
      "epoch:22 step:21420 [D loss: 0.553475, acc.: 66.41%] [G loss: 0.477013]\n",
      "epoch:22 step:21421 [D loss: 0.541653, acc.: 66.41%] [G loss: 0.470437]\n",
      "epoch:22 step:21422 [D loss: 0.531305, acc.: 71.88%] [G loss: 0.667063]\n",
      "epoch:22 step:21423 [D loss: 0.546144, acc.: 71.09%] [G loss: 0.630163]\n",
      "epoch:22 step:21424 [D loss: 0.559572, acc.: 68.75%] [G loss: 0.663251]\n",
      "epoch:22 step:21425 [D loss: 0.541933, acc.: 71.09%] [G loss: 0.646860]\n",
      "epoch:22 step:21426 [D loss: 0.649854, acc.: 62.50%] [G loss: 0.495981]\n",
      "epoch:22 step:21427 [D loss: 0.549081, acc.: 69.53%] [G loss: 0.504930]\n",
      "epoch:22 step:21428 [D loss: 0.475568, acc.: 78.91%] [G loss: 0.581574]\n",
      "epoch:22 step:21429 [D loss: 0.510222, acc.: 74.22%] [G loss: 0.892262]\n",
      "epoch:22 step:21430 [D loss: 0.554817, acc.: 70.31%] [G loss: 0.754681]\n",
      "epoch:22 step:21431 [D loss: 0.607467, acc.: 66.41%] [G loss: 0.721027]\n",
      "epoch:22 step:21432 [D loss: 0.600816, acc.: 67.19%] [G loss: 0.575824]\n",
      "epoch:22 step:21433 [D loss: 0.482707, acc.: 75.00%] [G loss: 0.774273]\n",
      "epoch:22 step:21434 [D loss: 0.633259, acc.: 67.19%] [G loss: 0.565510]\n",
      "epoch:22 step:21435 [D loss: 0.526593, acc.: 74.22%] [G loss: 0.511645]\n",
      "epoch:22 step:21436 [D loss: 0.572689, acc.: 64.84%] [G loss: 0.590072]\n",
      "epoch:22 step:21437 [D loss: 0.443767, acc.: 78.12%] [G loss: 0.766065]\n",
      "epoch:22 step:21438 [D loss: 0.530028, acc.: 70.31%] [G loss: 0.670947]\n",
      "epoch:22 step:21439 [D loss: 0.609942, acc.: 67.97%] [G loss: 0.562968]\n",
      "epoch:22 step:21440 [D loss: 0.569324, acc.: 71.09%] [G loss: 0.690042]\n",
      "epoch:22 step:21441 [D loss: 0.631800, acc.: 66.41%] [G loss: 0.584687]\n",
      "epoch:22 step:21442 [D loss: 0.633353, acc.: 62.50%] [G loss: 0.585586]\n",
      "epoch:22 step:21443 [D loss: 0.524374, acc.: 73.44%] [G loss: 0.563996]\n",
      "epoch:22 step:21444 [D loss: 0.566430, acc.: 68.75%] [G loss: 0.602287]\n",
      "epoch:22 step:21445 [D loss: 0.562486, acc.: 72.66%] [G loss: 0.694532]\n",
      "epoch:22 step:21446 [D loss: 0.591494, acc.: 68.75%] [G loss: 0.792766]\n",
      "epoch:22 step:21447 [D loss: 0.550175, acc.: 70.31%] [G loss: 0.705725]\n",
      "epoch:22 step:21448 [D loss: 0.556019, acc.: 69.53%] [G loss: 0.560956]\n",
      "epoch:22 step:21449 [D loss: 0.577196, acc.: 71.09%] [G loss: 0.577105]\n",
      "epoch:22 step:21450 [D loss: 0.527811, acc.: 65.62%] [G loss: 0.514951]\n",
      "epoch:22 step:21451 [D loss: 0.502723, acc.: 75.78%] [G loss: 0.482607]\n",
      "epoch:22 step:21452 [D loss: 0.514755, acc.: 75.00%] [G loss: 0.586607]\n",
      "epoch:22 step:21453 [D loss: 0.549639, acc.: 71.09%] [G loss: 0.550118]\n",
      "epoch:22 step:21454 [D loss: 0.598821, acc.: 64.84%] [G loss: 0.602534]\n",
      "epoch:22 step:21455 [D loss: 0.580973, acc.: 62.50%] [G loss: 0.554642]\n",
      "epoch:22 step:21456 [D loss: 0.527784, acc.: 74.22%] [G loss: 0.430628]\n",
      "epoch:22 step:21457 [D loss: 0.539601, acc.: 69.53%] [G loss: 0.681949]\n",
      "epoch:22 step:21458 [D loss: 0.598302, acc.: 67.97%] [G loss: 0.594696]\n",
      "epoch:22 step:21459 [D loss: 0.564346, acc.: 68.75%] [G loss: 0.659283]\n",
      "epoch:22 step:21460 [D loss: 0.574982, acc.: 66.41%] [G loss: 0.512169]\n",
      "epoch:22 step:21461 [D loss: 0.625490, acc.: 61.72%] [G loss: 0.503885]\n",
      "epoch:22 step:21462 [D loss: 0.533791, acc.: 67.97%] [G loss: 0.436933]\n",
      "epoch:22 step:21463 [D loss: 0.565033, acc.: 65.62%] [G loss: 0.425008]\n",
      "epoch:22 step:21464 [D loss: 0.607699, acc.: 60.16%] [G loss: 0.493749]\n",
      "epoch:22 step:21465 [D loss: 0.617131, acc.: 68.75%] [G loss: 0.554351]\n",
      "epoch:22 step:21466 [D loss: 0.565141, acc.: 71.88%] [G loss: 0.687145]\n",
      "epoch:22 step:21467 [D loss: 0.593691, acc.: 63.28%] [G loss: 0.570012]\n",
      "epoch:22 step:21468 [D loss: 0.551225, acc.: 64.06%] [G loss: 0.681294]\n",
      "epoch:22 step:21469 [D loss: 0.563205, acc.: 71.09%] [G loss: 0.630614]\n",
      "epoch:22 step:21470 [D loss: 0.589374, acc.: 61.72%] [G loss: 0.620967]\n",
      "epoch:22 step:21471 [D loss: 0.448505, acc.: 78.91%] [G loss: 0.506651]\n",
      "epoch:22 step:21472 [D loss: 0.561186, acc.: 71.09%] [G loss: 0.607066]\n",
      "epoch:22 step:21473 [D loss: 0.621813, acc.: 66.41%] [G loss: 0.643691]\n",
      "epoch:22 step:21474 [D loss: 0.462053, acc.: 79.69%] [G loss: 0.935811]\n",
      "epoch:22 step:21475 [D loss: 0.625200, acc.: 67.97%] [G loss: 0.652364]\n",
      "epoch:22 step:21476 [D loss: 0.607159, acc.: 64.84%] [G loss: 0.412208]\n",
      "epoch:22 step:21477 [D loss: 0.598290, acc.: 62.50%] [G loss: 0.507051]\n",
      "epoch:22 step:21478 [D loss: 0.483543, acc.: 75.00%] [G loss: 0.651321]\n",
      "epoch:22 step:21479 [D loss: 0.584458, acc.: 63.28%] [G loss: 0.525967]\n",
      "epoch:22 step:21480 [D loss: 0.520728, acc.: 72.66%] [G loss: 0.564757]\n",
      "epoch:22 step:21481 [D loss: 0.638416, acc.: 60.94%] [G loss: 0.419493]\n",
      "epoch:22 step:21482 [D loss: 0.514842, acc.: 73.44%] [G loss: 0.467855]\n",
      "epoch:22 step:21483 [D loss: 0.564086, acc.: 67.97%] [G loss: 0.694301]\n",
      "epoch:22 step:21484 [D loss: 0.463096, acc.: 80.47%] [G loss: 0.701169]\n",
      "epoch:22 step:21485 [D loss: 0.464510, acc.: 75.00%] [G loss: 0.804553]\n",
      "epoch:22 step:21486 [D loss: 0.560873, acc.: 65.62%] [G loss: 0.667356]\n",
      "epoch:22 step:21487 [D loss: 0.601606, acc.: 64.84%] [G loss: 0.698563]\n",
      "epoch:22 step:21488 [D loss: 0.543202, acc.: 69.53%] [G loss: 0.694469]\n",
      "epoch:22 step:21489 [D loss: 0.556992, acc.: 67.97%] [G loss: 0.656275]\n",
      "epoch:22 step:21490 [D loss: 0.583009, acc.: 65.62%] [G loss: 0.594153]\n",
      "epoch:22 step:21491 [D loss: 0.584314, acc.: 64.06%] [G loss: 0.608373]\n",
      "epoch:22 step:21492 [D loss: 0.556613, acc.: 71.88%] [G loss: 0.515225]\n",
      "epoch:22 step:21493 [D loss: 0.584334, acc.: 65.62%] [G loss: 0.524815]\n",
      "epoch:22 step:21494 [D loss: 0.662382, acc.: 57.03%] [G loss: 0.474565]\n",
      "epoch:22 step:21495 [D loss: 0.545726, acc.: 69.53%] [G loss: 0.558200]\n",
      "epoch:22 step:21496 [D loss: 0.578860, acc.: 73.44%] [G loss: 0.413364]\n",
      "epoch:22 step:21497 [D loss: 0.616848, acc.: 66.41%] [G loss: 0.604089]\n",
      "epoch:22 step:21498 [D loss: 0.521731, acc.: 75.00%] [G loss: 0.594741]\n",
      "epoch:22 step:21499 [D loss: 0.561431, acc.: 67.19%] [G loss: 0.623322]\n",
      "epoch:22 step:21500 [D loss: 0.539947, acc.: 72.66%] [G loss: 0.622290]\n",
      "epoch:22 step:21501 [D loss: 0.507038, acc.: 75.78%] [G loss: 0.723697]\n",
      "epoch:22 step:21502 [D loss: 0.527607, acc.: 72.66%] [G loss: 0.730659]\n",
      "epoch:22 step:21503 [D loss: 0.587264, acc.: 62.50%] [G loss: 0.623539]\n",
      "epoch:22 step:21504 [D loss: 0.530640, acc.: 71.88%] [G loss: 0.528591]\n",
      "epoch:22 step:21505 [D loss: 0.593256, acc.: 66.41%] [G loss: 0.572911]\n",
      "epoch:22 step:21506 [D loss: 0.651040, acc.: 60.94%] [G loss: 0.682878]\n",
      "epoch:22 step:21507 [D loss: 0.583849, acc.: 61.72%] [G loss: 0.457965]\n",
      "epoch:22 step:21508 [D loss: 0.445568, acc.: 78.12%] [G loss: 0.708740]\n",
      "epoch:22 step:21509 [D loss: 0.594618, acc.: 62.50%] [G loss: 0.656921]\n",
      "epoch:22 step:21510 [D loss: 0.492765, acc.: 75.00%] [G loss: 0.768513]\n",
      "epoch:22 step:21511 [D loss: 0.595523, acc.: 67.97%] [G loss: 0.748716]\n",
      "epoch:22 step:21512 [D loss: 0.463737, acc.: 77.34%] [G loss: 0.690820]\n",
      "epoch:22 step:21513 [D loss: 0.496926, acc.: 75.00%] [G loss: 0.692436]\n",
      "epoch:22 step:21514 [D loss: 0.546710, acc.: 72.66%] [G loss: 0.722128]\n",
      "epoch:22 step:21515 [D loss: 0.529637, acc.: 71.88%] [G loss: 0.711520]\n",
      "epoch:22 step:21516 [D loss: 0.533120, acc.: 70.31%] [G loss: 0.631839]\n",
      "epoch:22 step:21517 [D loss: 0.496959, acc.: 71.88%] [G loss: 0.701628]\n",
      "epoch:22 step:21518 [D loss: 0.634908, acc.: 63.28%] [G loss: 0.651493]\n",
      "epoch:22 step:21519 [D loss: 0.559295, acc.: 67.97%] [G loss: 0.718501]\n",
      "epoch:22 step:21520 [D loss: 0.527084, acc.: 75.00%] [G loss: 0.823534]\n",
      "epoch:22 step:21521 [D loss: 0.586938, acc.: 67.19%] [G loss: 0.711968]\n",
      "epoch:22 step:21522 [D loss: 0.560046, acc.: 67.19%] [G loss: 0.634078]\n",
      "epoch:22 step:21523 [D loss: 0.559981, acc.: 67.19%] [G loss: 0.698377]\n",
      "epoch:22 step:21524 [D loss: 0.601908, acc.: 58.59%] [G loss: 0.659275]\n",
      "epoch:22 step:21525 [D loss: 0.516108, acc.: 77.34%] [G loss: 0.542276]\n",
      "epoch:22 step:21526 [D loss: 0.453593, acc.: 78.91%] [G loss: 0.691400]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21527 [D loss: 0.574731, acc.: 70.31%] [G loss: 0.799682]\n",
      "epoch:22 step:21528 [D loss: 0.537019, acc.: 71.09%] [G loss: 0.762138]\n",
      "epoch:22 step:21529 [D loss: 0.713690, acc.: 57.81%] [G loss: 0.537064]\n",
      "epoch:22 step:21530 [D loss: 0.523124, acc.: 70.31%] [G loss: 0.660032]\n",
      "epoch:22 step:21531 [D loss: 0.635381, acc.: 57.81%] [G loss: 0.554306]\n",
      "epoch:22 step:21532 [D loss: 0.471926, acc.: 78.91%] [G loss: 0.641882]\n",
      "epoch:22 step:21533 [D loss: 0.437899, acc.: 82.81%] [G loss: 0.964136]\n",
      "epoch:22 step:21534 [D loss: 0.709724, acc.: 53.12%] [G loss: 0.707164]\n",
      "epoch:22 step:21535 [D loss: 0.504523, acc.: 72.66%] [G loss: 0.792420]\n",
      "epoch:22 step:21536 [D loss: 0.542763, acc.: 72.66%] [G loss: 0.728167]\n",
      "epoch:22 step:21537 [D loss: 0.470821, acc.: 78.91%] [G loss: 0.672013]\n",
      "epoch:22 step:21538 [D loss: 0.448876, acc.: 79.69%] [G loss: 0.911789]\n",
      "epoch:22 step:21539 [D loss: 0.457553, acc.: 74.22%] [G loss: 1.189258]\n",
      "epoch:22 step:21540 [D loss: 0.439554, acc.: 78.12%] [G loss: 1.091521]\n",
      "epoch:22 step:21541 [D loss: 0.598441, acc.: 67.19%] [G loss: 1.157176]\n",
      "epoch:22 step:21542 [D loss: 0.642228, acc.: 69.53%] [G loss: 1.239757]\n",
      "epoch:22 step:21543 [D loss: 0.451137, acc.: 79.69%] [G loss: 1.197929]\n",
      "epoch:22 step:21544 [D loss: 0.461053, acc.: 78.12%] [G loss: 1.291202]\n",
      "epoch:22 step:21545 [D loss: 0.537792, acc.: 71.09%] [G loss: 1.075144]\n",
      "epoch:22 step:21546 [D loss: 0.688807, acc.: 59.38%] [G loss: 1.004298]\n",
      "epoch:22 step:21547 [D loss: 0.510538, acc.: 74.22%] [G loss: 0.959484]\n",
      "epoch:22 step:21548 [D loss: 0.514800, acc.: 71.09%] [G loss: 0.976595]\n",
      "epoch:22 step:21549 [D loss: 0.430722, acc.: 80.47%] [G loss: 0.985272]\n",
      "epoch:22 step:21550 [D loss: 0.342892, acc.: 86.72%] [G loss: 1.351729]\n",
      "epoch:22 step:21551 [D loss: 0.411353, acc.: 81.25%] [G loss: 1.352198]\n",
      "epoch:23 step:21552 [D loss: 0.603951, acc.: 71.09%] [G loss: 0.989554]\n",
      "epoch:23 step:21553 [D loss: 0.494321, acc.: 73.44%] [G loss: 1.032952]\n",
      "epoch:23 step:21554 [D loss: 0.554532, acc.: 71.09%] [G loss: 0.878535]\n",
      "epoch:23 step:21555 [D loss: 0.515682, acc.: 71.88%] [G loss: 0.849039]\n",
      "epoch:23 step:21556 [D loss: 0.548068, acc.: 71.09%] [G loss: 0.798758]\n",
      "epoch:23 step:21557 [D loss: 0.605340, acc.: 69.53%] [G loss: 0.673122]\n",
      "epoch:23 step:21558 [D loss: 0.479756, acc.: 82.03%] [G loss: 0.722530]\n",
      "epoch:23 step:21559 [D loss: 0.541251, acc.: 71.09%] [G loss: 0.701485]\n",
      "epoch:23 step:21560 [D loss: 0.501967, acc.: 75.78%] [G loss: 1.017927]\n",
      "epoch:23 step:21561 [D loss: 0.482226, acc.: 76.56%] [G loss: 0.824119]\n",
      "epoch:23 step:21562 [D loss: 0.483870, acc.: 74.22%] [G loss: 0.790040]\n",
      "epoch:23 step:21563 [D loss: 0.594450, acc.: 69.53%] [G loss: 0.918050]\n",
      "epoch:23 step:21564 [D loss: 0.539674, acc.: 72.66%] [G loss: 0.843681]\n",
      "epoch:23 step:21565 [D loss: 0.546055, acc.: 69.53%] [G loss: 0.761721]\n",
      "epoch:23 step:21566 [D loss: 0.456789, acc.: 73.44%] [G loss: 0.819539]\n",
      "epoch:23 step:21567 [D loss: 0.477456, acc.: 78.12%] [G loss: 0.878865]\n",
      "epoch:23 step:21568 [D loss: 0.559957, acc.: 72.66%] [G loss: 0.749569]\n",
      "epoch:23 step:21569 [D loss: 0.580361, acc.: 67.19%] [G loss: 0.621358]\n",
      "epoch:23 step:21570 [D loss: 0.593533, acc.: 70.31%] [G loss: 0.622595]\n",
      "epoch:23 step:21571 [D loss: 0.649605, acc.: 57.81%] [G loss: 0.643884]\n",
      "epoch:23 step:21572 [D loss: 0.566778, acc.: 67.19%] [G loss: 0.863813]\n",
      "epoch:23 step:21573 [D loss: 0.442808, acc.: 79.69%] [G loss: 1.154032]\n",
      "epoch:23 step:21574 [D loss: 0.641391, acc.: 65.62%] [G loss: 0.551594]\n",
      "epoch:23 step:21575 [D loss: 0.453212, acc.: 75.00%] [G loss: 0.644075]\n",
      "epoch:23 step:21576 [D loss: 0.492602, acc.: 78.91%] [G loss: 0.725843]\n",
      "epoch:23 step:21577 [D loss: 0.592404, acc.: 67.19%] [G loss: 0.522924]\n",
      "epoch:23 step:21578 [D loss: 0.446119, acc.: 81.25%] [G loss: 0.685584]\n",
      "epoch:23 step:21579 [D loss: 0.568007, acc.: 68.75%] [G loss: 0.573560]\n",
      "epoch:23 step:21580 [D loss: 0.485778, acc.: 73.44%] [G loss: 0.757118]\n",
      "epoch:23 step:21581 [D loss: 0.540043, acc.: 69.53%] [G loss: 0.567134]\n",
      "epoch:23 step:21582 [D loss: 0.619323, acc.: 66.41%] [G loss: 0.608377]\n",
      "epoch:23 step:21583 [D loss: 0.545692, acc.: 73.44%] [G loss: 0.569394]\n",
      "epoch:23 step:21584 [D loss: 0.498737, acc.: 76.56%] [G loss: 0.677983]\n",
      "epoch:23 step:21585 [D loss: 0.523670, acc.: 71.88%] [G loss: 0.550845]\n",
      "epoch:23 step:21586 [D loss: 0.549494, acc.: 66.41%] [G loss: 0.713106]\n",
      "epoch:23 step:21587 [D loss: 0.519991, acc.: 71.88%] [G loss: 0.778610]\n",
      "epoch:23 step:21588 [D loss: 0.552140, acc.: 74.22%] [G loss: 0.686492]\n",
      "epoch:23 step:21589 [D loss: 0.593257, acc.: 67.19%] [G loss: 0.764982]\n",
      "epoch:23 step:21590 [D loss: 0.573691, acc.: 64.06%] [G loss: 0.562460]\n",
      "epoch:23 step:21591 [D loss: 0.487102, acc.: 75.00%] [G loss: 0.650339]\n",
      "epoch:23 step:21592 [D loss: 0.519708, acc.: 74.22%] [G loss: 0.783593]\n",
      "epoch:23 step:21593 [D loss: 0.614996, acc.: 64.84%] [G loss: 0.670373]\n",
      "epoch:23 step:21594 [D loss: 0.519555, acc.: 75.00%] [G loss: 0.770046]\n",
      "epoch:23 step:21595 [D loss: 0.559317, acc.: 71.88%] [G loss: 0.553004]\n",
      "epoch:23 step:21596 [D loss: 0.483364, acc.: 76.56%] [G loss: 0.706543]\n",
      "epoch:23 step:21597 [D loss: 0.548748, acc.: 72.66%] [G loss: 0.687449]\n",
      "epoch:23 step:21598 [D loss: 0.594431, acc.: 69.53%] [G loss: 0.618295]\n",
      "epoch:23 step:21599 [D loss: 0.506246, acc.: 75.78%] [G loss: 0.806543]\n",
      "epoch:23 step:21600 [D loss: 0.488215, acc.: 77.34%] [G loss: 0.878505]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.673547\n",
      "FID: 54.155563\n",
      "0 = 12.992067397689828\n",
      "1 = 0.09570250219025662\n",
      "2 = 0.8682000041007996\n",
      "3 = 0.8307999968528748\n",
      "4 = 0.9056000113487244\n",
      "5 = 0.8979679942131042\n",
      "6 = 0.8307999968528748\n",
      "7 = 8.622507541489606\n",
      "8 = 0.16231147393867001\n",
      "9 = 0.7014999985694885\n",
      "10 = 0.6998000144958496\n",
      "11 = 0.7031999826431274\n",
      "12 = 0.7021874189376831\n",
      "13 = 0.6998000144958496\n",
      "14 = 6.6735734939575195\n",
      "15 = 6.956032752990723\n",
      "16 = 0.39462876319885254\n",
      "17 = 6.67354679107666\n",
      "18 = 54.15556335449219\n",
      "epoch:23 step:21601 [D loss: 0.524423, acc.: 71.09%] [G loss: 0.783286]\n",
      "epoch:23 step:21602 [D loss: 0.648946, acc.: 59.38%] [G loss: 0.515107]\n",
      "epoch:23 step:21603 [D loss: 0.617034, acc.: 62.50%] [G loss: 0.464724]\n",
      "epoch:23 step:21604 [D loss: 0.537847, acc.: 71.09%] [G loss: 0.579356]\n",
      "epoch:23 step:21605 [D loss: 0.495641, acc.: 75.00%] [G loss: 0.727378]\n",
      "epoch:23 step:21606 [D loss: 0.559318, acc.: 71.88%] [G loss: 0.852409]\n",
      "epoch:23 step:21607 [D loss: 0.512044, acc.: 72.66%] [G loss: 0.675040]\n",
      "epoch:23 step:21608 [D loss: 0.521003, acc.: 68.75%] [G loss: 0.794816]\n",
      "epoch:23 step:21609 [D loss: 0.587597, acc.: 70.31%] [G loss: 0.635838]\n",
      "epoch:23 step:21610 [D loss: 0.473698, acc.: 77.34%] [G loss: 0.719094]\n",
      "epoch:23 step:21611 [D loss: 0.610505, acc.: 64.06%] [G loss: 0.706231]\n",
      "epoch:23 step:21612 [D loss: 0.547391, acc.: 65.62%] [G loss: 0.599571]\n",
      "epoch:23 step:21613 [D loss: 0.532550, acc.: 72.66%] [G loss: 0.559228]\n",
      "epoch:23 step:21614 [D loss: 0.572109, acc.: 67.97%] [G loss: 0.511210]\n",
      "epoch:23 step:21615 [D loss: 0.512256, acc.: 77.34%] [G loss: 0.590837]\n",
      "epoch:23 step:21616 [D loss: 0.565285, acc.: 67.97%] [G loss: 0.651048]\n",
      "epoch:23 step:21617 [D loss: 0.513958, acc.: 72.66%] [G loss: 0.616861]\n",
      "epoch:23 step:21618 [D loss: 0.528595, acc.: 75.00%] [G loss: 0.698125]\n",
      "epoch:23 step:21619 [D loss: 0.529287, acc.: 71.09%] [G loss: 0.600149]\n",
      "epoch:23 step:21620 [D loss: 0.512885, acc.: 76.56%] [G loss: 0.560131]\n",
      "epoch:23 step:21621 [D loss: 0.521675, acc.: 73.44%] [G loss: 0.681286]\n",
      "epoch:23 step:21622 [D loss: 0.591231, acc.: 67.19%] [G loss: 0.713305]\n",
      "epoch:23 step:21623 [D loss: 0.533468, acc.: 69.53%] [G loss: 0.684285]\n",
      "epoch:23 step:21624 [D loss: 0.578257, acc.: 67.19%] [G loss: 0.542001]\n",
      "epoch:23 step:21625 [D loss: 0.456950, acc.: 82.03%] [G loss: 0.782336]\n",
      "epoch:23 step:21626 [D loss: 0.520167, acc.: 71.88%] [G loss: 0.779996]\n",
      "epoch:23 step:21627 [D loss: 0.470598, acc.: 78.12%] [G loss: 0.765327]\n",
      "epoch:23 step:21628 [D loss: 0.486434, acc.: 80.47%] [G loss: 0.748156]\n",
      "epoch:23 step:21629 [D loss: 0.586983, acc.: 64.84%] [G loss: 0.792362]\n",
      "epoch:23 step:21630 [D loss: 0.556829, acc.: 71.09%] [G loss: 0.570876]\n",
      "epoch:23 step:21631 [D loss: 0.514678, acc.: 72.66%] [G loss: 0.688504]\n",
      "epoch:23 step:21632 [D loss: 0.547855, acc.: 69.53%] [G loss: 0.648061]\n",
      "epoch:23 step:21633 [D loss: 0.508763, acc.: 71.09%] [G loss: 0.642960]\n",
      "epoch:23 step:21634 [D loss: 0.453109, acc.: 78.12%] [G loss: 0.766852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21635 [D loss: 0.553337, acc.: 68.75%] [G loss: 0.669365]\n",
      "epoch:23 step:21636 [D loss: 0.563561, acc.: 70.31%] [G loss: 0.774675]\n",
      "epoch:23 step:21637 [D loss: 0.525047, acc.: 75.00%] [G loss: 0.629117]\n",
      "epoch:23 step:21638 [D loss: 0.520185, acc.: 72.66%] [G loss: 0.686203]\n",
      "epoch:23 step:21639 [D loss: 0.538405, acc.: 72.66%] [G loss: 0.758292]\n",
      "epoch:23 step:21640 [D loss: 0.502206, acc.: 77.34%] [G loss: 0.591242]\n",
      "epoch:23 step:21641 [D loss: 0.533170, acc.: 72.66%] [G loss: 0.730704]\n",
      "epoch:23 step:21642 [D loss: 0.559504, acc.: 73.44%] [G loss: 0.697329]\n",
      "epoch:23 step:21643 [D loss: 0.493333, acc.: 76.56%] [G loss: 0.747595]\n",
      "epoch:23 step:21644 [D loss: 0.522213, acc.: 71.09%] [G loss: 0.884810]\n",
      "epoch:23 step:21645 [D loss: 0.509341, acc.: 78.12%] [G loss: 1.113219]\n",
      "epoch:23 step:21646 [D loss: 0.509574, acc.: 69.53%] [G loss: 0.781583]\n",
      "epoch:23 step:21647 [D loss: 0.525104, acc.: 69.53%] [G loss: 0.674784]\n",
      "epoch:23 step:21648 [D loss: 0.517349, acc.: 71.88%] [G loss: 0.829044]\n",
      "epoch:23 step:21649 [D loss: 0.579692, acc.: 67.19%] [G loss: 0.623640]\n",
      "epoch:23 step:21650 [D loss: 0.574117, acc.: 66.41%] [G loss: 0.737725]\n",
      "epoch:23 step:21651 [D loss: 0.479954, acc.: 80.47%] [G loss: 0.881066]\n",
      "epoch:23 step:21652 [D loss: 0.513683, acc.: 72.66%] [G loss: 0.972467]\n",
      "epoch:23 step:21653 [D loss: 0.653914, acc.: 64.06%] [G loss: 0.488117]\n",
      "epoch:23 step:21654 [D loss: 0.552172, acc.: 70.31%] [G loss: 0.645512]\n",
      "epoch:23 step:21655 [D loss: 0.518899, acc.: 70.31%] [G loss: 0.538014]\n",
      "epoch:23 step:21656 [D loss: 0.597585, acc.: 62.50%] [G loss: 0.494607]\n",
      "epoch:23 step:21657 [D loss: 0.547765, acc.: 68.75%] [G loss: 0.688096]\n",
      "epoch:23 step:21658 [D loss: 0.550620, acc.: 71.88%] [G loss: 0.662051]\n",
      "epoch:23 step:21659 [D loss: 0.588205, acc.: 70.31%] [G loss: 0.558488]\n",
      "epoch:23 step:21660 [D loss: 0.585317, acc.: 65.62%] [G loss: 0.668718]\n",
      "epoch:23 step:21661 [D loss: 0.516273, acc.: 75.00%] [G loss: 0.714569]\n",
      "epoch:23 step:21662 [D loss: 0.538641, acc.: 67.97%] [G loss: 0.582408]\n",
      "epoch:23 step:21663 [D loss: 0.513040, acc.: 74.22%] [G loss: 0.801069]\n",
      "epoch:23 step:21664 [D loss: 0.521310, acc.: 75.78%] [G loss: 0.648764]\n",
      "epoch:23 step:21665 [D loss: 0.551158, acc.: 71.09%] [G loss: 0.819260]\n",
      "epoch:23 step:21666 [D loss: 0.499610, acc.: 80.47%] [G loss: 0.850081]\n",
      "epoch:23 step:21667 [D loss: 0.506979, acc.: 74.22%] [G loss: 0.826178]\n",
      "epoch:23 step:21668 [D loss: 0.616225, acc.: 66.41%] [G loss: 0.708417]\n",
      "epoch:23 step:21669 [D loss: 0.481607, acc.: 74.22%] [G loss: 1.152810]\n",
      "epoch:23 step:21670 [D loss: 0.469220, acc.: 79.69%] [G loss: 0.939058]\n",
      "epoch:23 step:21671 [D loss: 0.574882, acc.: 72.66%] [G loss: 0.835344]\n",
      "epoch:23 step:21672 [D loss: 0.526926, acc.: 71.09%] [G loss: 0.786856]\n",
      "epoch:23 step:21673 [D loss: 0.522953, acc.: 77.34%] [G loss: 0.682695]\n",
      "epoch:23 step:21674 [D loss: 0.501763, acc.: 75.78%] [G loss: 0.866658]\n",
      "epoch:23 step:21675 [D loss: 0.550006, acc.: 73.44%] [G loss: 0.895404]\n",
      "epoch:23 step:21676 [D loss: 0.583012, acc.: 67.19%] [G loss: 0.749442]\n",
      "epoch:23 step:21677 [D loss: 0.514931, acc.: 75.00%] [G loss: 0.686611]\n",
      "epoch:23 step:21678 [D loss: 0.509893, acc.: 71.88%] [G loss: 0.778954]\n",
      "epoch:23 step:21679 [D loss: 0.577110, acc.: 69.53%] [G loss: 0.695638]\n",
      "epoch:23 step:21680 [D loss: 0.527362, acc.: 74.22%] [G loss: 0.922935]\n",
      "epoch:23 step:21681 [D loss: 0.585737, acc.: 66.41%] [G loss: 0.643237]\n",
      "epoch:23 step:21682 [D loss: 0.478727, acc.: 75.78%] [G loss: 0.627408]\n",
      "epoch:23 step:21683 [D loss: 0.554862, acc.: 71.09%] [G loss: 0.766907]\n",
      "epoch:23 step:21684 [D loss: 0.566452, acc.: 67.97%] [G loss: 0.762359]\n",
      "epoch:23 step:21685 [D loss: 0.525611, acc.: 69.53%] [G loss: 0.718422]\n",
      "epoch:23 step:21686 [D loss: 0.552693, acc.: 70.31%] [G loss: 0.649747]\n",
      "epoch:23 step:21687 [D loss: 0.513742, acc.: 75.78%] [G loss: 0.875268]\n",
      "epoch:23 step:21688 [D loss: 0.574508, acc.: 70.31%] [G loss: 0.845055]\n",
      "epoch:23 step:21689 [D loss: 0.579192, acc.: 69.53%] [G loss: 0.655578]\n",
      "epoch:23 step:21690 [D loss: 0.496231, acc.: 77.34%] [G loss: 0.723102]\n",
      "epoch:23 step:21691 [D loss: 0.560338, acc.: 66.41%] [G loss: 0.753688]\n",
      "epoch:23 step:21692 [D loss: 0.547459, acc.: 66.41%] [G loss: 0.594305]\n",
      "epoch:23 step:21693 [D loss: 0.517518, acc.: 70.31%] [G loss: 0.733322]\n",
      "epoch:23 step:21694 [D loss: 0.554453, acc.: 69.53%] [G loss: 0.592975]\n",
      "epoch:23 step:21695 [D loss: 0.513382, acc.: 71.88%] [G loss: 0.769958]\n",
      "epoch:23 step:21696 [D loss: 0.555763, acc.: 69.53%] [G loss: 0.620570]\n",
      "epoch:23 step:21697 [D loss: 0.495809, acc.: 73.44%] [G loss: 0.631569]\n",
      "epoch:23 step:21698 [D loss: 0.625355, acc.: 65.62%] [G loss: 0.660636]\n",
      "epoch:23 step:21699 [D loss: 0.587954, acc.: 69.53%] [G loss: 0.706824]\n",
      "epoch:23 step:21700 [D loss: 0.516881, acc.: 76.56%] [G loss: 0.595773]\n",
      "epoch:23 step:21701 [D loss: 0.586799, acc.: 67.19%] [G loss: 0.680187]\n",
      "epoch:23 step:21702 [D loss: 0.558239, acc.: 71.88%] [G loss: 0.686427]\n",
      "epoch:23 step:21703 [D loss: 0.529636, acc.: 71.09%] [G loss: 0.751314]\n",
      "epoch:23 step:21704 [D loss: 0.620358, acc.: 68.75%] [G loss: 0.600039]\n",
      "epoch:23 step:21705 [D loss: 0.575783, acc.: 68.75%] [G loss: 0.681790]\n",
      "epoch:23 step:21706 [D loss: 0.494674, acc.: 74.22%] [G loss: 0.803844]\n",
      "epoch:23 step:21707 [D loss: 0.480957, acc.: 75.00%] [G loss: 0.726652]\n",
      "epoch:23 step:21708 [D loss: 0.556015, acc.: 68.75%] [G loss: 0.691322]\n",
      "epoch:23 step:21709 [D loss: 0.560250, acc.: 70.31%] [G loss: 0.549055]\n",
      "epoch:23 step:21710 [D loss: 0.496712, acc.: 76.56%] [G loss: 0.760002]\n",
      "epoch:23 step:21711 [D loss: 0.614131, acc.: 61.72%] [G loss: 0.716197]\n",
      "epoch:23 step:21712 [D loss: 0.551873, acc.: 71.88%] [G loss: 0.668778]\n",
      "epoch:23 step:21713 [D loss: 0.490828, acc.: 78.91%] [G loss: 0.917354]\n",
      "epoch:23 step:21714 [D loss: 0.604027, acc.: 67.19%] [G loss: 0.727430]\n",
      "epoch:23 step:21715 [D loss: 0.557107, acc.: 63.28%] [G loss: 0.727239]\n",
      "epoch:23 step:21716 [D loss: 0.501705, acc.: 74.22%] [G loss: 0.671220]\n",
      "epoch:23 step:21717 [D loss: 0.602070, acc.: 64.84%] [G loss: 0.528714]\n",
      "epoch:23 step:21718 [D loss: 0.521160, acc.: 75.78%] [G loss: 0.725691]\n",
      "epoch:23 step:21719 [D loss: 0.516762, acc.: 71.88%] [G loss: 0.716236]\n",
      "epoch:23 step:21720 [D loss: 0.585112, acc.: 67.97%] [G loss: 0.599356]\n",
      "epoch:23 step:21721 [D loss: 0.545088, acc.: 67.19%] [G loss: 0.629545]\n",
      "epoch:23 step:21722 [D loss: 0.584162, acc.: 66.41%] [G loss: 0.561590]\n",
      "epoch:23 step:21723 [D loss: 0.538558, acc.: 70.31%] [G loss: 0.523564]\n",
      "epoch:23 step:21724 [D loss: 0.513201, acc.: 75.00%] [G loss: 0.672778]\n",
      "epoch:23 step:21725 [D loss: 0.568305, acc.: 68.75%] [G loss: 0.808867]\n",
      "epoch:23 step:21726 [D loss: 0.574634, acc.: 67.19%] [G loss: 0.582209]\n",
      "epoch:23 step:21727 [D loss: 0.497923, acc.: 71.88%] [G loss: 0.574049]\n",
      "epoch:23 step:21728 [D loss: 0.493899, acc.: 74.22%] [G loss: 0.590434]\n",
      "epoch:23 step:21729 [D loss: 0.609864, acc.: 62.50%] [G loss: 0.545099]\n",
      "epoch:23 step:21730 [D loss: 0.535178, acc.: 74.22%] [G loss: 0.649374]\n",
      "epoch:23 step:21731 [D loss: 0.679475, acc.: 62.50%] [G loss: 0.555187]\n",
      "epoch:23 step:21732 [D loss: 0.598613, acc.: 63.28%] [G loss: 0.497245]\n",
      "epoch:23 step:21733 [D loss: 0.506172, acc.: 72.66%] [G loss: 0.690484]\n",
      "epoch:23 step:21734 [D loss: 0.549293, acc.: 67.97%] [G loss: 0.735974]\n",
      "epoch:23 step:21735 [D loss: 0.515864, acc.: 72.66%] [G loss: 0.660909]\n",
      "epoch:23 step:21736 [D loss: 0.587971, acc.: 71.88%] [G loss: 0.636827]\n",
      "epoch:23 step:21737 [D loss: 0.564043, acc.: 69.53%] [G loss: 0.643289]\n",
      "epoch:23 step:21738 [D loss: 0.620310, acc.: 61.72%] [G loss: 0.542444]\n",
      "epoch:23 step:21739 [D loss: 0.492592, acc.: 75.00%] [G loss: 0.587413]\n",
      "epoch:23 step:21740 [D loss: 0.590594, acc.: 72.66%] [G loss: 0.520919]\n",
      "epoch:23 step:21741 [D loss: 0.455933, acc.: 76.56%] [G loss: 0.722600]\n",
      "epoch:23 step:21742 [D loss: 0.489618, acc.: 71.88%] [G loss: 0.686778]\n",
      "epoch:23 step:21743 [D loss: 0.493244, acc.: 75.78%] [G loss: 0.782420]\n",
      "epoch:23 step:21744 [D loss: 0.530599, acc.: 73.44%] [G loss: 0.752856]\n",
      "epoch:23 step:21745 [D loss: 0.479471, acc.: 75.78%] [G loss: 0.816859]\n",
      "epoch:23 step:21746 [D loss: 0.620143, acc.: 65.62%] [G loss: 0.711823]\n",
      "epoch:23 step:21747 [D loss: 0.527411, acc.: 72.66%] [G loss: 0.742098]\n",
      "epoch:23 step:21748 [D loss: 0.512181, acc.: 72.66%] [G loss: 0.728270]\n",
      "epoch:23 step:21749 [D loss: 0.425949, acc.: 82.81%] [G loss: 0.735349]\n",
      "epoch:23 step:21750 [D loss: 0.505404, acc.: 73.44%] [G loss: 0.714320]\n",
      "epoch:23 step:21751 [D loss: 0.584311, acc.: 71.88%] [G loss: 0.707745]\n",
      "epoch:23 step:21752 [D loss: 0.559780, acc.: 67.97%] [G loss: 0.670958]\n",
      "epoch:23 step:21753 [D loss: 0.533286, acc.: 69.53%] [G loss: 0.778068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21754 [D loss: 0.620242, acc.: 68.75%] [G loss: 0.504155]\n",
      "epoch:23 step:21755 [D loss: 0.565995, acc.: 70.31%] [G loss: 0.738194]\n",
      "epoch:23 step:21756 [D loss: 0.476905, acc.: 76.56%] [G loss: 0.818920]\n",
      "epoch:23 step:21757 [D loss: 0.468502, acc.: 78.91%] [G loss: 0.799227]\n",
      "epoch:23 step:21758 [D loss: 0.483104, acc.: 74.22%] [G loss: 0.833456]\n",
      "epoch:23 step:21759 [D loss: 0.491503, acc.: 75.78%] [G loss: 0.769247]\n",
      "epoch:23 step:21760 [D loss: 0.487787, acc.: 72.66%] [G loss: 0.798044]\n",
      "epoch:23 step:21761 [D loss: 0.636953, acc.: 66.41%] [G loss: 0.675804]\n",
      "epoch:23 step:21762 [D loss: 0.623010, acc.: 60.94%] [G loss: 0.591907]\n",
      "epoch:23 step:21763 [D loss: 0.522407, acc.: 73.44%] [G loss: 0.603205]\n",
      "epoch:23 step:21764 [D loss: 0.540318, acc.: 66.41%] [G loss: 0.686433]\n",
      "epoch:23 step:21765 [D loss: 0.617190, acc.: 66.41%] [G loss: 0.626476]\n",
      "epoch:23 step:21766 [D loss: 0.581008, acc.: 67.19%] [G loss: 0.520411]\n",
      "epoch:23 step:21767 [D loss: 0.570807, acc.: 71.09%] [G loss: 0.497846]\n",
      "epoch:23 step:21768 [D loss: 0.484184, acc.: 78.12%] [G loss: 0.749565]\n",
      "epoch:23 step:21769 [D loss: 0.479977, acc.: 74.22%] [G loss: 0.861718]\n",
      "epoch:23 step:21770 [D loss: 0.476093, acc.: 78.12%] [G loss: 0.923799]\n",
      "epoch:23 step:21771 [D loss: 0.654506, acc.: 63.28%] [G loss: 0.780653]\n",
      "epoch:23 step:21772 [D loss: 0.505901, acc.: 73.44%] [G loss: 0.811841]\n",
      "epoch:23 step:21773 [D loss: 0.464481, acc.: 82.03%] [G loss: 0.807177]\n",
      "epoch:23 step:21774 [D loss: 0.527298, acc.: 67.97%] [G loss: 0.875596]\n",
      "epoch:23 step:21775 [D loss: 0.564966, acc.: 68.75%] [G loss: 0.829645]\n",
      "epoch:23 step:21776 [D loss: 0.551540, acc.: 68.75%] [G loss: 0.527594]\n",
      "epoch:23 step:21777 [D loss: 0.649134, acc.: 57.03%] [G loss: 0.806817]\n",
      "epoch:23 step:21778 [D loss: 0.556103, acc.: 67.97%] [G loss: 0.793458]\n",
      "epoch:23 step:21779 [D loss: 0.623975, acc.: 63.28%] [G loss: 0.591943]\n",
      "epoch:23 step:21780 [D loss: 0.520395, acc.: 76.56%] [G loss: 0.613316]\n",
      "epoch:23 step:21781 [D loss: 0.497135, acc.: 77.34%] [G loss: 0.736074]\n",
      "epoch:23 step:21782 [D loss: 0.515200, acc.: 75.00%] [G loss: 0.703092]\n",
      "epoch:23 step:21783 [D loss: 0.464712, acc.: 78.91%] [G loss: 0.998162]\n",
      "epoch:23 step:21784 [D loss: 0.560792, acc.: 73.44%] [G loss: 0.791917]\n",
      "epoch:23 step:21785 [D loss: 0.560158, acc.: 67.97%] [G loss: 0.773742]\n",
      "epoch:23 step:21786 [D loss: 0.546304, acc.: 74.22%] [G loss: 0.704680]\n",
      "epoch:23 step:21787 [D loss: 0.569639, acc.: 74.22%] [G loss: 0.489620]\n",
      "epoch:23 step:21788 [D loss: 0.578508, acc.: 64.84%] [G loss: 0.564404]\n",
      "epoch:23 step:21789 [D loss: 0.612920, acc.: 62.50%] [G loss: 0.651627]\n",
      "epoch:23 step:21790 [D loss: 0.507119, acc.: 71.09%] [G loss: 0.652788]\n",
      "epoch:23 step:21791 [D loss: 0.594584, acc.: 65.62%] [G loss: 0.528993]\n",
      "epoch:23 step:21792 [D loss: 0.486058, acc.: 78.12%] [G loss: 0.839784]\n",
      "epoch:23 step:21793 [D loss: 0.524231, acc.: 74.22%] [G loss: 0.829851]\n",
      "epoch:23 step:21794 [D loss: 0.515045, acc.: 71.09%] [G loss: 0.739046]\n",
      "epoch:23 step:21795 [D loss: 0.489204, acc.: 74.22%] [G loss: 0.824356]\n",
      "epoch:23 step:21796 [D loss: 0.540560, acc.: 74.22%] [G loss: 0.558211]\n",
      "epoch:23 step:21797 [D loss: 0.513574, acc.: 74.22%] [G loss: 0.697776]\n",
      "epoch:23 step:21798 [D loss: 0.494810, acc.: 71.88%] [G loss: 0.702459]\n",
      "epoch:23 step:21799 [D loss: 0.491138, acc.: 77.34%] [G loss: 0.766775]\n",
      "epoch:23 step:21800 [D loss: 0.541529, acc.: 75.00%] [G loss: 0.717417]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.587508\n",
      "FID: 50.293331\n",
      "0 = 12.929837958812682\n",
      "1 = 0.08501073320726632\n",
      "2 = 0.859499990940094\n",
      "3 = 0.8108000159263611\n",
      "4 = 0.9082000255584717\n",
      "5 = 0.8982937932014465\n",
      "6 = 0.8108000159263611\n",
      "7 = 8.355973050284375\n",
      "8 = 0.1521166551568994\n",
      "9 = 0.7020999789237976\n",
      "10 = 0.6905999779701233\n",
      "11 = 0.7135999798774719\n",
      "12 = 0.7068577408790588\n",
      "13 = 0.6905999779701233\n",
      "14 = 6.587535381317139\n",
      "15 = 6.798487186431885\n",
      "16 = 0.39798688888549805\n",
      "17 = 6.587507724761963\n",
      "18 = 50.293331146240234\n",
      "epoch:23 step:21801 [D loss: 0.599892, acc.: 64.06%] [G loss: 0.693055]\n",
      "epoch:23 step:21802 [D loss: 0.605860, acc.: 67.19%] [G loss: 0.585323]\n",
      "epoch:23 step:21803 [D loss: 0.563100, acc.: 69.53%] [G loss: 0.650114]\n",
      "epoch:23 step:21804 [D loss: 0.526757, acc.: 73.44%] [G loss: 0.696170]\n",
      "epoch:23 step:21805 [D loss: 0.493613, acc.: 71.09%] [G loss: 0.710348]\n",
      "epoch:23 step:21806 [D loss: 0.640409, acc.: 60.16%] [G loss: 0.566418]\n",
      "epoch:23 step:21807 [D loss: 0.528876, acc.: 73.44%] [G loss: 0.865205]\n",
      "epoch:23 step:21808 [D loss: 0.558438, acc.: 68.75%] [G loss: 0.960120]\n",
      "epoch:23 step:21809 [D loss: 0.540226, acc.: 71.09%] [G loss: 0.503215]\n",
      "epoch:23 step:21810 [D loss: 0.525532, acc.: 65.62%] [G loss: 0.653417]\n",
      "epoch:23 step:21811 [D loss: 0.576176, acc.: 64.84%] [G loss: 0.574088]\n",
      "epoch:23 step:21812 [D loss: 0.530365, acc.: 69.53%] [G loss: 0.628688]\n",
      "epoch:23 step:21813 [D loss: 0.518447, acc.: 72.66%] [G loss: 0.634542]\n",
      "epoch:23 step:21814 [D loss: 0.624814, acc.: 67.97%] [G loss: 0.520939]\n",
      "epoch:23 step:21815 [D loss: 0.506757, acc.: 70.31%] [G loss: 0.666090]\n",
      "epoch:23 step:21816 [D loss: 0.518223, acc.: 75.78%] [G loss: 0.725688]\n",
      "epoch:23 step:21817 [D loss: 0.563040, acc.: 64.06%] [G loss: 0.650577]\n",
      "epoch:23 step:21818 [D loss: 0.534806, acc.: 69.53%] [G loss: 0.641930]\n",
      "epoch:23 step:21819 [D loss: 0.502497, acc.: 73.44%] [G loss: 0.724722]\n",
      "epoch:23 step:21820 [D loss: 0.536147, acc.: 75.78%] [G loss: 0.709607]\n",
      "epoch:23 step:21821 [D loss: 0.506694, acc.: 74.22%] [G loss: 0.704047]\n",
      "epoch:23 step:21822 [D loss: 0.530783, acc.: 70.31%] [G loss: 0.656506]\n",
      "epoch:23 step:21823 [D loss: 0.515748, acc.: 71.09%] [G loss: 0.700653]\n",
      "epoch:23 step:21824 [D loss: 0.517038, acc.: 74.22%] [G loss: 0.630397]\n",
      "epoch:23 step:21825 [D loss: 0.513432, acc.: 71.88%] [G loss: 0.802409]\n",
      "epoch:23 step:21826 [D loss: 0.550434, acc.: 71.09%] [G loss: 0.593189]\n",
      "epoch:23 step:21827 [D loss: 0.421616, acc.: 85.94%] [G loss: 0.760438]\n",
      "epoch:23 step:21828 [D loss: 0.668220, acc.: 64.06%] [G loss: 0.626561]\n",
      "epoch:23 step:21829 [D loss: 0.636791, acc.: 65.62%] [G loss: 0.725973]\n",
      "epoch:23 step:21830 [D loss: 0.619174, acc.: 61.72%] [G loss: 0.595385]\n",
      "epoch:23 step:21831 [D loss: 0.490181, acc.: 74.22%] [G loss: 0.643428]\n",
      "epoch:23 step:21832 [D loss: 0.643121, acc.: 61.72%] [G loss: 0.515535]\n",
      "epoch:23 step:21833 [D loss: 0.574013, acc.: 67.19%] [G loss: 0.468152]\n",
      "epoch:23 step:21834 [D loss: 0.535016, acc.: 72.66%] [G loss: 0.629740]\n",
      "epoch:23 step:21835 [D loss: 0.524510, acc.: 71.88%] [G loss: 0.618969]\n",
      "epoch:23 step:21836 [D loss: 0.509881, acc.: 70.31%] [G loss: 0.580399]\n",
      "epoch:23 step:21837 [D loss: 0.483476, acc.: 75.00%] [G loss: 0.695885]\n",
      "epoch:23 step:21838 [D loss: 0.572095, acc.: 64.06%] [G loss: 0.601503]\n",
      "epoch:23 step:21839 [D loss: 0.544512, acc.: 71.09%] [G loss: 0.554498]\n",
      "epoch:23 step:21840 [D loss: 0.532378, acc.: 66.41%] [G loss: 0.732190]\n",
      "epoch:23 step:21841 [D loss: 0.519899, acc.: 73.44%] [G loss: 0.688978]\n",
      "epoch:23 step:21842 [D loss: 0.629841, acc.: 66.41%] [G loss: 0.556035]\n",
      "epoch:23 step:21843 [D loss: 0.508884, acc.: 71.88%] [G loss: 0.836829]\n",
      "epoch:23 step:21844 [D loss: 0.542653, acc.: 71.09%] [G loss: 0.706195]\n",
      "epoch:23 step:21845 [D loss: 0.586107, acc.: 66.41%] [G loss: 0.470939]\n",
      "epoch:23 step:21846 [D loss: 0.519179, acc.: 70.31%] [G loss: 0.692561]\n",
      "epoch:23 step:21847 [D loss: 0.439055, acc.: 79.69%] [G loss: 0.656145]\n",
      "epoch:23 step:21848 [D loss: 0.523122, acc.: 71.09%] [G loss: 0.643003]\n",
      "epoch:23 step:21849 [D loss: 0.452206, acc.: 77.34%] [G loss: 0.811205]\n",
      "epoch:23 step:21850 [D loss: 0.525139, acc.: 71.09%] [G loss: 0.805673]\n",
      "epoch:23 step:21851 [D loss: 0.465029, acc.: 76.56%] [G loss: 0.890215]\n",
      "epoch:23 step:21852 [D loss: 0.631308, acc.: 64.06%] [G loss: 0.543870]\n",
      "epoch:23 step:21853 [D loss: 0.522526, acc.: 73.44%] [G loss: 0.657870]\n",
      "epoch:23 step:21854 [D loss: 0.558434, acc.: 74.22%] [G loss: 0.760850]\n",
      "epoch:23 step:21855 [D loss: 0.501644, acc.: 70.31%] [G loss: 0.877127]\n",
      "epoch:23 step:21856 [D loss: 0.550612, acc.: 71.88%] [G loss: 0.681287]\n",
      "epoch:23 step:21857 [D loss: 0.490320, acc.: 73.44%] [G loss: 0.681013]\n",
      "epoch:23 step:21858 [D loss: 0.525159, acc.: 72.66%] [G loss: 0.733311]\n",
      "epoch:23 step:21859 [D loss: 0.577051, acc.: 64.84%] [G loss: 0.668742]\n",
      "epoch:23 step:21860 [D loss: 0.497156, acc.: 71.09%] [G loss: 0.808458]\n",
      "epoch:23 step:21861 [D loss: 0.530079, acc.: 71.88%] [G loss: 0.750396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21862 [D loss: 0.498605, acc.: 74.22%] [G loss: 0.782836]\n",
      "epoch:23 step:21863 [D loss: 0.536739, acc.: 71.88%] [G loss: 0.964355]\n",
      "epoch:23 step:21864 [D loss: 0.480496, acc.: 71.88%] [G loss: 1.016199]\n",
      "epoch:23 step:21865 [D loss: 0.424176, acc.: 81.25%] [G loss: 1.084341]\n",
      "epoch:23 step:21866 [D loss: 0.459481, acc.: 78.12%] [G loss: 0.835809]\n",
      "epoch:23 step:21867 [D loss: 0.738627, acc.: 64.84%] [G loss: 0.557159]\n",
      "epoch:23 step:21868 [D loss: 0.611101, acc.: 64.84%] [G loss: 0.514971]\n",
      "epoch:23 step:21869 [D loss: 0.557229, acc.: 67.19%] [G loss: 0.724045]\n",
      "epoch:23 step:21870 [D loss: 0.499215, acc.: 74.22%] [G loss: 0.770140]\n",
      "epoch:23 step:21871 [D loss: 0.506175, acc.: 73.44%] [G loss: 0.709510]\n",
      "epoch:23 step:21872 [D loss: 0.515625, acc.: 71.09%] [G loss: 0.667423]\n",
      "epoch:23 step:21873 [D loss: 0.547600, acc.: 67.97%] [G loss: 0.686110]\n",
      "epoch:23 step:21874 [D loss: 0.576449, acc.: 67.97%] [G loss: 0.559825]\n",
      "epoch:23 step:21875 [D loss: 0.547706, acc.: 69.53%] [G loss: 0.509089]\n",
      "epoch:23 step:21876 [D loss: 0.554481, acc.: 69.53%] [G loss: 0.700952]\n",
      "epoch:23 step:21877 [D loss: 0.487901, acc.: 73.44%] [G loss: 0.695834]\n",
      "epoch:23 step:21878 [D loss: 0.528221, acc.: 71.09%] [G loss: 0.848731]\n",
      "epoch:23 step:21879 [D loss: 0.471849, acc.: 75.78%] [G loss: 0.836444]\n",
      "epoch:23 step:21880 [D loss: 0.517665, acc.: 73.44%] [G loss: 0.703866]\n",
      "epoch:23 step:21881 [D loss: 0.528839, acc.: 73.44%] [G loss: 0.613173]\n",
      "epoch:23 step:21882 [D loss: 0.609889, acc.: 64.06%] [G loss: 0.531494]\n",
      "epoch:23 step:21883 [D loss: 0.523055, acc.: 73.44%] [G loss: 0.686481]\n",
      "epoch:23 step:21884 [D loss: 0.502582, acc.: 75.78%] [G loss: 0.743742]\n",
      "epoch:23 step:21885 [D loss: 0.490427, acc.: 74.22%] [G loss: 0.753311]\n",
      "epoch:23 step:21886 [D loss: 0.534244, acc.: 75.78%] [G loss: 0.816102]\n",
      "epoch:23 step:21887 [D loss: 0.509179, acc.: 70.31%] [G loss: 0.691573]\n",
      "epoch:23 step:21888 [D loss: 0.515155, acc.: 75.78%] [G loss: 0.832722]\n",
      "epoch:23 step:21889 [D loss: 0.540231, acc.: 70.31%] [G loss: 0.794122]\n",
      "epoch:23 step:21890 [D loss: 0.560278, acc.: 69.53%] [G loss: 0.794594]\n",
      "epoch:23 step:21891 [D loss: 0.543813, acc.: 73.44%] [G loss: 0.888817]\n",
      "epoch:23 step:21892 [D loss: 0.603057, acc.: 73.44%] [G loss: 0.623282]\n",
      "epoch:23 step:21893 [D loss: 0.662096, acc.: 57.81%] [G loss: 0.657367]\n",
      "epoch:23 step:21894 [D loss: 0.537283, acc.: 72.66%] [G loss: 0.660978]\n",
      "epoch:23 step:21895 [D loss: 0.455502, acc.: 79.69%] [G loss: 0.909665]\n",
      "epoch:23 step:21896 [D loss: 0.591137, acc.: 64.84%] [G loss: 0.670099]\n",
      "epoch:23 step:21897 [D loss: 0.515660, acc.: 73.44%] [G loss: 0.915366]\n",
      "epoch:23 step:21898 [D loss: 0.393540, acc.: 78.91%] [G loss: 1.081480]\n",
      "epoch:23 step:21899 [D loss: 0.576348, acc.: 70.31%] [G loss: 0.865479]\n",
      "epoch:23 step:21900 [D loss: 0.684408, acc.: 59.38%] [G loss: 0.539384]\n",
      "epoch:23 step:21901 [D loss: 0.504247, acc.: 76.56%] [G loss: 0.427203]\n",
      "epoch:23 step:21902 [D loss: 0.544564, acc.: 70.31%] [G loss: 0.567198]\n",
      "epoch:23 step:21903 [D loss: 0.552469, acc.: 69.53%] [G loss: 0.780041]\n",
      "epoch:23 step:21904 [D loss: 0.557692, acc.: 67.97%] [G loss: 0.698939]\n",
      "epoch:23 step:21905 [D loss: 0.372865, acc.: 83.59%] [G loss: 0.889828]\n",
      "epoch:23 step:21906 [D loss: 0.515837, acc.: 71.88%] [G loss: 0.894728]\n",
      "epoch:23 step:21907 [D loss: 0.532680, acc.: 72.66%] [G loss: 0.752070]\n",
      "epoch:23 step:21908 [D loss: 0.447044, acc.: 80.47%] [G loss: 0.833766]\n",
      "epoch:23 step:21909 [D loss: 0.453249, acc.: 81.25%] [G loss: 0.814747]\n",
      "epoch:23 step:21910 [D loss: 0.456264, acc.: 73.44%] [G loss: 0.775888]\n",
      "epoch:23 step:21911 [D loss: 0.478595, acc.: 76.56%] [G loss: 0.853099]\n",
      "epoch:23 step:21912 [D loss: 0.494134, acc.: 75.00%] [G loss: 0.846746]\n",
      "epoch:23 step:21913 [D loss: 0.554136, acc.: 69.53%] [G loss: 0.777697]\n",
      "epoch:23 step:21914 [D loss: 0.602996, acc.: 60.16%] [G loss: 0.682577]\n",
      "epoch:23 step:21915 [D loss: 0.520689, acc.: 72.66%] [G loss: 0.656740]\n",
      "epoch:23 step:21916 [D loss: 0.553562, acc.: 69.53%] [G loss: 0.516771]\n",
      "epoch:23 step:21917 [D loss: 0.527521, acc.: 70.31%] [G loss: 0.651086]\n",
      "epoch:23 step:21918 [D loss: 0.550743, acc.: 71.09%] [G loss: 0.630534]\n",
      "epoch:23 step:21919 [D loss: 0.571421, acc.: 68.75%] [G loss: 0.600379]\n",
      "epoch:23 step:21920 [D loss: 0.456604, acc.: 76.56%] [G loss: 0.705304]\n",
      "epoch:23 step:21921 [D loss: 0.572278, acc.: 66.41%] [G loss: 0.731818]\n",
      "epoch:23 step:21922 [D loss: 0.460638, acc.: 75.00%] [G loss: 0.872812]\n",
      "epoch:23 step:21923 [D loss: 0.494096, acc.: 73.44%] [G loss: 0.766608]\n",
      "epoch:23 step:21924 [D loss: 0.511284, acc.: 72.66%] [G loss: 0.681371]\n",
      "epoch:23 step:21925 [D loss: 0.425831, acc.: 82.81%] [G loss: 0.627492]\n",
      "epoch:23 step:21926 [D loss: 0.541947, acc.: 69.53%] [G loss: 0.748316]\n",
      "epoch:23 step:21927 [D loss: 0.695835, acc.: 58.59%] [G loss: 0.620783]\n",
      "epoch:23 step:21928 [D loss: 0.573678, acc.: 65.62%] [G loss: 0.577339]\n",
      "epoch:23 step:21929 [D loss: 0.500494, acc.: 76.56%] [G loss: 0.573771]\n",
      "epoch:23 step:21930 [D loss: 0.548193, acc.: 67.97%] [G loss: 0.642940]\n",
      "epoch:23 step:21931 [D loss: 0.569422, acc.: 68.75%] [G loss: 0.478104]\n",
      "epoch:23 step:21932 [D loss: 0.470868, acc.: 76.56%] [G loss: 0.517400]\n",
      "epoch:23 step:21933 [D loss: 0.498468, acc.: 75.78%] [G loss: 0.847339]\n",
      "epoch:23 step:21934 [D loss: 0.526354, acc.: 74.22%] [G loss: 0.806101]\n",
      "epoch:23 step:21935 [D loss: 0.473419, acc.: 73.44%] [G loss: 0.750731]\n",
      "epoch:23 step:21936 [D loss: 0.495431, acc.: 76.56%] [G loss: 0.698359]\n",
      "epoch:23 step:21937 [D loss: 0.638266, acc.: 62.50%] [G loss: 0.537417]\n",
      "epoch:23 step:21938 [D loss: 0.551540, acc.: 67.19%] [G loss: 0.685359]\n",
      "epoch:23 step:21939 [D loss: 0.553765, acc.: 69.53%] [G loss: 0.582379]\n",
      "epoch:23 step:21940 [D loss: 0.527475, acc.: 70.31%] [G loss: 0.763899]\n",
      "epoch:23 step:21941 [D loss: 0.579274, acc.: 68.75%] [G loss: 0.628536]\n",
      "epoch:23 step:21942 [D loss: 0.526565, acc.: 71.09%] [G loss: 0.753604]\n",
      "epoch:23 step:21943 [D loss: 0.473005, acc.: 75.78%] [G loss: 0.648695]\n",
      "epoch:23 step:21944 [D loss: 0.538955, acc.: 71.09%] [G loss: 0.582721]\n",
      "epoch:23 step:21945 [D loss: 0.550314, acc.: 70.31%] [G loss: 0.711030]\n",
      "epoch:23 step:21946 [D loss: 0.491751, acc.: 74.22%] [G loss: 0.639535]\n",
      "epoch:23 step:21947 [D loss: 0.571092, acc.: 73.44%] [G loss: 0.788427]\n",
      "epoch:23 step:21948 [D loss: 0.540436, acc.: 68.75%] [G loss: 0.629202]\n",
      "epoch:23 step:21949 [D loss: 0.511834, acc.: 73.44%] [G loss: 0.774366]\n",
      "epoch:23 step:21950 [D loss: 0.537530, acc.: 67.97%] [G loss: 0.802135]\n",
      "epoch:23 step:21951 [D loss: 0.613666, acc.: 64.84%] [G loss: 0.696208]\n",
      "epoch:23 step:21952 [D loss: 0.673126, acc.: 55.47%] [G loss: 0.466086]\n",
      "epoch:23 step:21953 [D loss: 0.522894, acc.: 73.44%] [G loss: 0.687401]\n",
      "epoch:23 step:21954 [D loss: 0.545345, acc.: 71.88%] [G loss: 0.663807]\n",
      "epoch:23 step:21955 [D loss: 0.601784, acc.: 65.62%] [G loss: 0.796117]\n",
      "epoch:23 step:21956 [D loss: 0.553535, acc.: 64.84%] [G loss: 0.644666]\n",
      "epoch:23 step:21957 [D loss: 0.508425, acc.: 67.97%] [G loss: 0.756878]\n",
      "epoch:23 step:21958 [D loss: 0.537310, acc.: 71.88%] [G loss: 0.690687]\n",
      "epoch:23 step:21959 [D loss: 0.567509, acc.: 67.97%] [G loss: 0.635411]\n",
      "epoch:23 step:21960 [D loss: 0.552226, acc.: 72.66%] [G loss: 0.710486]\n",
      "epoch:23 step:21961 [D loss: 0.553613, acc.: 71.88%] [G loss: 0.531005]\n",
      "epoch:23 step:21962 [D loss: 0.561078, acc.: 65.62%] [G loss: 0.564618]\n",
      "epoch:23 step:21963 [D loss: 0.612597, acc.: 60.94%] [G loss: 0.428788]\n",
      "epoch:23 step:21964 [D loss: 0.496944, acc.: 75.00%] [G loss: 0.670377]\n",
      "epoch:23 step:21965 [D loss: 0.521644, acc.: 67.97%] [G loss: 0.771821]\n",
      "epoch:23 step:21966 [D loss: 0.518279, acc.: 71.09%] [G loss: 0.906339]\n",
      "epoch:23 step:21967 [D loss: 0.488602, acc.: 78.12%] [G loss: 0.928255]\n",
      "epoch:23 step:21968 [D loss: 0.527448, acc.: 73.44%] [G loss: 0.891927]\n",
      "epoch:23 step:21969 [D loss: 0.620708, acc.: 59.38%] [G loss: 0.626608]\n",
      "epoch:23 step:21970 [D loss: 0.569443, acc.: 63.28%] [G loss: 0.846811]\n",
      "epoch:23 step:21971 [D loss: 0.607276, acc.: 63.28%] [G loss: 0.603117]\n",
      "epoch:23 step:21972 [D loss: 0.606249, acc.: 64.06%] [G loss: 0.716618]\n",
      "epoch:23 step:21973 [D loss: 0.667998, acc.: 62.50%] [G loss: 0.663608]\n",
      "epoch:23 step:21974 [D loss: 0.583839, acc.: 63.28%] [G loss: 0.576820]\n",
      "epoch:23 step:21975 [D loss: 0.574675, acc.: 62.50%] [G loss: 0.542448]\n",
      "epoch:23 step:21976 [D loss: 0.513694, acc.: 73.44%] [G loss: 0.667937]\n",
      "epoch:23 step:21977 [D loss: 0.448275, acc.: 80.47%] [G loss: 0.816935]\n",
      "epoch:23 step:21978 [D loss: 0.465062, acc.: 77.34%] [G loss: 0.819936]\n",
      "epoch:23 step:21979 [D loss: 0.538193, acc.: 71.09%] [G loss: 0.783192]\n",
      "epoch:23 step:21980 [D loss: 0.456670, acc.: 76.56%] [G loss: 0.860303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21981 [D loss: 0.519526, acc.: 73.44%] [G loss: 0.868983]\n",
      "epoch:23 step:21982 [D loss: 0.572100, acc.: 65.62%] [G loss: 0.917085]\n",
      "epoch:23 step:21983 [D loss: 0.582385, acc.: 66.41%] [G loss: 0.822008]\n",
      "epoch:23 step:21984 [D loss: 0.529784, acc.: 75.00%] [G loss: 0.704311]\n",
      "epoch:23 step:21985 [D loss: 0.503830, acc.: 74.22%] [G loss: 0.637705]\n",
      "epoch:23 step:21986 [D loss: 0.476866, acc.: 74.22%] [G loss: 0.948680]\n",
      "epoch:23 step:21987 [D loss: 0.447691, acc.: 78.91%] [G loss: 0.962708]\n",
      "epoch:23 step:21988 [D loss: 0.711950, acc.: 61.72%] [G loss: 0.603119]\n",
      "epoch:23 step:21989 [D loss: 0.559642, acc.: 65.62%] [G loss: 0.603038]\n",
      "epoch:23 step:21990 [D loss: 0.497570, acc.: 75.78%] [G loss: 0.794054]\n",
      "epoch:23 step:21991 [D loss: 0.501958, acc.: 73.44%] [G loss: 0.858288]\n",
      "epoch:23 step:21992 [D loss: 0.542080, acc.: 72.66%] [G loss: 0.726455]\n",
      "epoch:23 step:21993 [D loss: 0.568627, acc.: 67.97%] [G loss: 0.721811]\n",
      "epoch:23 step:21994 [D loss: 0.492745, acc.: 74.22%] [G loss: 0.779665]\n",
      "epoch:23 step:21995 [D loss: 0.404448, acc.: 82.81%] [G loss: 0.874962]\n",
      "epoch:23 step:21996 [D loss: 0.545051, acc.: 70.31%] [G loss: 0.636075]\n",
      "epoch:23 step:21997 [D loss: 0.522983, acc.: 72.66%] [G loss: 0.664450]\n",
      "epoch:23 step:21998 [D loss: 0.571892, acc.: 69.53%] [G loss: 0.704537]\n",
      "epoch:23 step:21999 [D loss: 0.494000, acc.: 75.78%] [G loss: 0.680762]\n",
      "epoch:23 step:22000 [D loss: 0.459379, acc.: 75.00%] [G loss: 0.783171]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.794864\n",
      "FID: 50.735512\n",
      "0 = 12.87979722843168\n",
      "1 = 0.09375572187128611\n",
      "2 = 0.8536999821662903\n",
      "3 = 0.8040000200271606\n",
      "4 = 0.9034000039100647\n",
      "5 = 0.8927381634712219\n",
      "6 = 0.8040000200271606\n",
      "7 = 8.266322523593908\n",
      "8 = 0.1513935681396672\n",
      "9 = 0.6991999745368958\n",
      "10 = 0.6881999969482422\n",
      "11 = 0.7102000117301941\n",
      "12 = 0.7036809921264648\n",
      "13 = 0.6881999969482422\n",
      "14 = 6.794893264770508\n",
      "15 = 6.963621616363525\n",
      "16 = 0.3856573700904846\n",
      "17 = 6.794863700866699\n",
      "18 = 50.735511779785156\n",
      "epoch:23 step:22001 [D loss: 0.530357, acc.: 72.66%] [G loss: 0.685972]\n",
      "epoch:23 step:22002 [D loss: 0.429977, acc.: 77.34%] [G loss: 0.911595]\n",
      "epoch:23 step:22003 [D loss: 0.498114, acc.: 72.66%] [G loss: 0.967795]\n",
      "epoch:23 step:22004 [D loss: 0.480793, acc.: 75.78%] [G loss: 0.832216]\n",
      "epoch:23 step:22005 [D loss: 0.568179, acc.: 71.09%] [G loss: 0.793676]\n",
      "epoch:23 step:22006 [D loss: 0.558340, acc.: 70.31%] [G loss: 0.620851]\n",
      "epoch:23 step:22007 [D loss: 0.593891, acc.: 69.53%] [G loss: 0.613655]\n",
      "epoch:23 step:22008 [D loss: 0.502618, acc.: 74.22%] [G loss: 0.658001]\n",
      "epoch:23 step:22009 [D loss: 0.598778, acc.: 69.53%] [G loss: 0.846017]\n",
      "epoch:23 step:22010 [D loss: 0.570811, acc.: 68.75%] [G loss: 0.677251]\n",
      "epoch:23 step:22011 [D loss: 0.541598, acc.: 71.88%] [G loss: 0.639221]\n",
      "epoch:23 step:22012 [D loss: 0.518525, acc.: 71.09%] [G loss: 0.650773]\n",
      "epoch:23 step:22013 [D loss: 0.564742, acc.: 69.53%] [G loss: 0.522216]\n",
      "epoch:23 step:22014 [D loss: 0.585564, acc.: 67.97%] [G loss: 0.584859]\n",
      "epoch:23 step:22015 [D loss: 0.497087, acc.: 74.22%] [G loss: 0.757007]\n",
      "epoch:23 step:22016 [D loss: 0.566490, acc.: 67.97%] [G loss: 0.624106]\n",
      "epoch:23 step:22017 [D loss: 0.563611, acc.: 71.09%] [G loss: 0.582307]\n",
      "epoch:23 step:22018 [D loss: 0.524833, acc.: 71.09%] [G loss: 0.753005]\n",
      "epoch:23 step:22019 [D loss: 0.553249, acc.: 71.09%] [G loss: 0.588858]\n",
      "epoch:23 step:22020 [D loss: 0.511560, acc.: 72.66%] [G loss: 0.876591]\n",
      "epoch:23 step:22021 [D loss: 0.544170, acc.: 72.66%] [G loss: 0.741885]\n",
      "epoch:23 step:22022 [D loss: 0.447035, acc.: 80.47%] [G loss: 0.804844]\n",
      "epoch:23 step:22023 [D loss: 0.507726, acc.: 75.78%] [G loss: 1.131755]\n",
      "epoch:23 step:22024 [D loss: 0.637892, acc.: 61.72%] [G loss: 0.788775]\n",
      "epoch:23 step:22025 [D loss: 0.578945, acc.: 66.41%] [G loss: 0.833137]\n",
      "epoch:23 step:22026 [D loss: 0.512954, acc.: 75.00%] [G loss: 0.768127]\n",
      "epoch:23 step:22027 [D loss: 0.530506, acc.: 74.22%] [G loss: 0.734753]\n",
      "epoch:23 step:22028 [D loss: 0.591557, acc.: 69.53%] [G loss: 0.658588]\n",
      "epoch:23 step:22029 [D loss: 0.554021, acc.: 70.31%] [G loss: 0.530252]\n",
      "epoch:23 step:22030 [D loss: 0.530306, acc.: 71.09%] [G loss: 0.590097]\n",
      "epoch:23 step:22031 [D loss: 0.638470, acc.: 64.84%] [G loss: 0.698970]\n",
      "epoch:23 step:22032 [D loss: 0.511280, acc.: 81.25%] [G loss: 0.728353]\n",
      "epoch:23 step:22033 [D loss: 0.616116, acc.: 68.75%] [G loss: 0.698599]\n",
      "epoch:23 step:22034 [D loss: 0.495943, acc.: 78.12%] [G loss: 0.606821]\n",
      "epoch:23 step:22035 [D loss: 0.476500, acc.: 74.22%] [G loss: 0.839303]\n",
      "epoch:23 step:22036 [D loss: 0.524464, acc.: 75.00%] [G loss: 0.691233]\n",
      "epoch:23 step:22037 [D loss: 0.594017, acc.: 64.06%] [G loss: 0.713896]\n",
      "epoch:23 step:22038 [D loss: 0.548013, acc.: 68.75%] [G loss: 0.731386]\n",
      "epoch:23 step:22039 [D loss: 0.494756, acc.: 72.66%] [G loss: 0.745145]\n",
      "epoch:23 step:22040 [D loss: 0.530205, acc.: 72.66%] [G loss: 0.727322]\n",
      "epoch:23 step:22041 [D loss: 0.541201, acc.: 68.75%] [G loss: 0.663651]\n",
      "epoch:23 step:22042 [D loss: 0.504139, acc.: 75.00%] [G loss: 0.677321]\n",
      "epoch:23 step:22043 [D loss: 0.539572, acc.: 72.66%] [G loss: 0.617185]\n",
      "epoch:23 step:22044 [D loss: 0.524248, acc.: 74.22%] [G loss: 0.552328]\n",
      "epoch:23 step:22045 [D loss: 0.575735, acc.: 67.97%] [G loss: 0.574503]\n",
      "epoch:23 step:22046 [D loss: 0.469530, acc.: 76.56%] [G loss: 0.804441]\n",
      "epoch:23 step:22047 [D loss: 0.549356, acc.: 67.97%] [G loss: 0.667199]\n",
      "epoch:23 step:22048 [D loss: 0.649723, acc.: 58.59%] [G loss: 0.706914]\n",
      "epoch:23 step:22049 [D loss: 0.541744, acc.: 71.88%] [G loss: 0.681680]\n",
      "epoch:23 step:22050 [D loss: 0.440964, acc.: 81.25%] [G loss: 0.860549]\n",
      "epoch:23 step:22051 [D loss: 0.593385, acc.: 71.88%] [G loss: 0.677578]\n",
      "epoch:23 step:22052 [D loss: 0.594064, acc.: 65.62%] [G loss: 0.571419]\n",
      "epoch:23 step:22053 [D loss: 0.645164, acc.: 60.16%] [G loss: 0.465759]\n",
      "epoch:23 step:22054 [D loss: 0.481211, acc.: 78.12%] [G loss: 0.556681]\n",
      "epoch:23 step:22055 [D loss: 0.498874, acc.: 75.00%] [G loss: 0.656256]\n",
      "epoch:23 step:22056 [D loss: 0.433276, acc.: 80.47%] [G loss: 0.770986]\n",
      "epoch:23 step:22057 [D loss: 0.504559, acc.: 78.12%] [G loss: 0.727945]\n",
      "epoch:23 step:22058 [D loss: 0.583799, acc.: 67.97%] [G loss: 0.777962]\n",
      "epoch:23 step:22059 [D loss: 0.423877, acc.: 83.59%] [G loss: 0.755885]\n",
      "epoch:23 step:22060 [D loss: 0.521203, acc.: 75.00%] [G loss: 0.756777]\n",
      "epoch:23 step:22061 [D loss: 0.589564, acc.: 69.53%] [G loss: 0.735262]\n",
      "epoch:23 step:22062 [D loss: 0.632275, acc.: 63.28%] [G loss: 0.652851]\n",
      "epoch:23 step:22063 [D loss: 0.679003, acc.: 60.16%] [G loss: 0.582002]\n",
      "epoch:23 step:22064 [D loss: 0.490653, acc.: 74.22%] [G loss: 0.703454]\n",
      "epoch:23 step:22065 [D loss: 0.531078, acc.: 69.53%] [G loss: 0.595961]\n",
      "epoch:23 step:22066 [D loss: 0.523942, acc.: 73.44%] [G loss: 0.639727]\n",
      "epoch:23 step:22067 [D loss: 0.472117, acc.: 76.56%] [G loss: 0.749940]\n",
      "epoch:23 step:22068 [D loss: 0.499977, acc.: 77.34%] [G loss: 0.636821]\n",
      "epoch:23 step:22069 [D loss: 0.499933, acc.: 72.66%] [G loss: 0.693169]\n",
      "epoch:23 step:22070 [D loss: 0.489359, acc.: 74.22%] [G loss: 0.686295]\n",
      "epoch:23 step:22071 [D loss: 0.513542, acc.: 75.78%] [G loss: 0.668114]\n",
      "epoch:23 step:22072 [D loss: 0.454817, acc.: 79.69%] [G loss: 0.827488]\n",
      "epoch:23 step:22073 [D loss: 0.527805, acc.: 72.66%] [G loss: 0.837997]\n",
      "epoch:23 step:22074 [D loss: 0.483244, acc.: 80.47%] [G loss: 0.892239]\n",
      "epoch:23 step:22075 [D loss: 0.546472, acc.: 67.97%] [G loss: 0.650752]\n",
      "epoch:23 step:22076 [D loss: 0.597306, acc.: 64.84%] [G loss: 0.757840]\n",
      "epoch:23 step:22077 [D loss: 0.501029, acc.: 71.88%] [G loss: 0.791081]\n",
      "epoch:23 step:22078 [D loss: 0.585286, acc.: 67.19%] [G loss: 0.796473]\n",
      "epoch:23 step:22079 [D loss: 0.754148, acc.: 52.34%] [G loss: 0.569050]\n",
      "epoch:23 step:22080 [D loss: 0.592866, acc.: 62.50%] [G loss: 0.759506]\n",
      "epoch:23 step:22081 [D loss: 0.534445, acc.: 71.88%] [G loss: 0.596709]\n",
      "epoch:23 step:22082 [D loss: 0.604667, acc.: 64.06%] [G loss: 0.592298]\n",
      "epoch:23 step:22083 [D loss: 0.552461, acc.: 69.53%] [G loss: 0.683444]\n",
      "epoch:23 step:22084 [D loss: 0.486895, acc.: 76.56%] [G loss: 0.872358]\n",
      "epoch:23 step:22085 [D loss: 0.455961, acc.: 79.69%] [G loss: 0.847906]\n",
      "epoch:23 step:22086 [D loss: 0.653187, acc.: 61.72%] [G loss: 0.710318]\n",
      "epoch:23 step:22087 [D loss: 0.500252, acc.: 74.22%] [G loss: 0.605475]\n",
      "epoch:23 step:22088 [D loss: 0.571163, acc.: 69.53%] [G loss: 0.597639]\n",
      "epoch:23 step:22089 [D loss: 0.569442, acc.: 66.41%] [G loss: 0.688024]\n",
      "epoch:23 step:22090 [D loss: 0.556795, acc.: 71.09%] [G loss: 0.614833]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22091 [D loss: 0.566398, acc.: 70.31%] [G loss: 0.643269]\n",
      "epoch:23 step:22092 [D loss: 0.568819, acc.: 63.28%] [G loss: 0.643608]\n",
      "epoch:23 step:22093 [D loss: 0.610731, acc.: 72.66%] [G loss: 0.563544]\n",
      "epoch:23 step:22094 [D loss: 0.564463, acc.: 67.19%] [G loss: 0.673374]\n",
      "epoch:23 step:22095 [D loss: 0.549390, acc.: 74.22%] [G loss: 0.603424]\n",
      "epoch:23 step:22096 [D loss: 0.505879, acc.: 73.44%] [G loss: 0.684847]\n",
      "epoch:23 step:22097 [D loss: 0.522377, acc.: 73.44%] [G loss: 0.801665]\n",
      "epoch:23 step:22098 [D loss: 0.546323, acc.: 71.88%] [G loss: 0.875322]\n",
      "epoch:23 step:22099 [D loss: 0.528004, acc.: 75.00%] [G loss: 0.758213]\n",
      "epoch:23 step:22100 [D loss: 0.554514, acc.: 73.44%] [G loss: 0.759393]\n",
      "epoch:23 step:22101 [D loss: 0.548782, acc.: 71.09%] [G loss: 0.541890]\n",
      "epoch:23 step:22102 [D loss: 0.527035, acc.: 71.88%] [G loss: 0.632923]\n",
      "epoch:23 step:22103 [D loss: 0.442639, acc.: 79.69%] [G loss: 0.789445]\n",
      "epoch:23 step:22104 [D loss: 0.567253, acc.: 68.75%] [G loss: 0.764945]\n",
      "epoch:23 step:22105 [D loss: 0.437616, acc.: 80.47%] [G loss: 0.674622]\n",
      "epoch:23 step:22106 [D loss: 0.455251, acc.: 78.91%] [G loss: 0.669370]\n",
      "epoch:23 step:22107 [D loss: 0.559391, acc.: 69.53%] [G loss: 0.737481]\n",
      "epoch:23 step:22108 [D loss: 0.512895, acc.: 71.88%] [G loss: 0.722150]\n",
      "epoch:23 step:22109 [D loss: 0.474093, acc.: 78.12%] [G loss: 0.790161]\n",
      "epoch:23 step:22110 [D loss: 0.585561, acc.: 67.97%] [G loss: 0.521762]\n",
      "epoch:23 step:22111 [D loss: 0.583580, acc.: 67.97%] [G loss: 0.430995]\n",
      "epoch:23 step:22112 [D loss: 0.590915, acc.: 64.06%] [G loss: 0.577814]\n",
      "epoch:23 step:22113 [D loss: 0.533085, acc.: 68.75%] [G loss: 0.647237]\n",
      "epoch:23 step:22114 [D loss: 0.569900, acc.: 70.31%] [G loss: 0.727706]\n",
      "epoch:23 step:22115 [D loss: 0.484662, acc.: 76.56%] [G loss: 0.725231]\n",
      "epoch:23 step:22116 [D loss: 0.553824, acc.: 69.53%] [G loss: 0.780252]\n",
      "epoch:23 step:22117 [D loss: 0.702501, acc.: 62.50%] [G loss: 0.634448]\n",
      "epoch:23 step:22118 [D loss: 0.485306, acc.: 74.22%] [G loss: 0.836303]\n",
      "epoch:23 step:22119 [D loss: 0.574863, acc.: 70.31%] [G loss: 0.474606]\n",
      "epoch:23 step:22120 [D loss: 0.571726, acc.: 68.75%] [G loss: 0.590855]\n",
      "epoch:23 step:22121 [D loss: 0.521511, acc.: 73.44%] [G loss: 0.545027]\n",
      "epoch:23 step:22122 [D loss: 0.511883, acc.: 75.78%] [G loss: 0.670789]\n",
      "epoch:23 step:22123 [D loss: 0.519875, acc.: 74.22%] [G loss: 0.769096]\n",
      "epoch:23 step:22124 [D loss: 0.567828, acc.: 70.31%] [G loss: 0.567165]\n",
      "epoch:23 step:22125 [D loss: 0.492081, acc.: 73.44%] [G loss: 0.777192]\n",
      "epoch:23 step:22126 [D loss: 0.521239, acc.: 77.34%] [G loss: 0.802999]\n",
      "epoch:23 step:22127 [D loss: 0.619198, acc.: 65.62%] [G loss: 0.444017]\n",
      "epoch:23 step:22128 [D loss: 0.566211, acc.: 67.97%] [G loss: 0.530391]\n",
      "epoch:23 step:22129 [D loss: 0.486541, acc.: 74.22%] [G loss: 0.639874]\n",
      "epoch:23 step:22130 [D loss: 0.491238, acc.: 76.56%] [G loss: 0.579928]\n",
      "epoch:23 step:22131 [D loss: 0.610755, acc.: 61.72%] [G loss: 0.493974]\n",
      "epoch:23 step:22132 [D loss: 0.522682, acc.: 71.88%] [G loss: 0.779090]\n",
      "epoch:23 step:22133 [D loss: 0.512360, acc.: 75.78%] [G loss: 0.845540]\n",
      "epoch:23 step:22134 [D loss: 0.562415, acc.: 69.53%] [G loss: 0.722582]\n",
      "epoch:23 step:22135 [D loss: 0.673060, acc.: 64.06%] [G loss: 0.632411]\n",
      "epoch:23 step:22136 [D loss: 0.568245, acc.: 62.50%] [G loss: 0.841097]\n",
      "epoch:23 step:22137 [D loss: 0.559152, acc.: 67.97%] [G loss: 0.753669]\n",
      "epoch:23 step:22138 [D loss: 0.525354, acc.: 71.88%] [G loss: 0.608818]\n",
      "epoch:23 step:22139 [D loss: 0.554945, acc.: 65.62%] [G loss: 0.672190]\n",
      "epoch:23 step:22140 [D loss: 0.551510, acc.: 67.19%] [G loss: 0.690731]\n",
      "epoch:23 step:22141 [D loss: 0.537616, acc.: 71.88%] [G loss: 0.703419]\n",
      "epoch:23 step:22142 [D loss: 0.592308, acc.: 64.84%] [G loss: 0.614727]\n",
      "epoch:23 step:22143 [D loss: 0.484385, acc.: 76.56%] [G loss: 0.600282]\n",
      "epoch:23 step:22144 [D loss: 0.542275, acc.: 71.09%] [G loss: 0.584342]\n",
      "epoch:23 step:22145 [D loss: 0.565521, acc.: 68.75%] [G loss: 0.740405]\n",
      "epoch:23 step:22146 [D loss: 0.559636, acc.: 68.75%] [G loss: 0.540382]\n",
      "epoch:23 step:22147 [D loss: 0.485770, acc.: 71.09%] [G loss: 0.690028]\n",
      "epoch:23 step:22148 [D loss: 0.511300, acc.: 69.53%] [G loss: 0.745183]\n",
      "epoch:23 step:22149 [D loss: 0.540043, acc.: 75.78%] [G loss: 0.688003]\n",
      "epoch:23 step:22150 [D loss: 0.474568, acc.: 75.78%] [G loss: 0.737794]\n",
      "epoch:23 step:22151 [D loss: 0.623836, acc.: 65.62%] [G loss: 0.652539]\n",
      "epoch:23 step:22152 [D loss: 0.554448, acc.: 70.31%] [G loss: 0.651136]\n",
      "epoch:23 step:22153 [D loss: 0.527056, acc.: 70.31%] [G loss: 0.639832]\n",
      "epoch:23 step:22154 [D loss: 0.460493, acc.: 77.34%] [G loss: 0.742017]\n",
      "epoch:23 step:22155 [D loss: 0.547286, acc.: 73.44%] [G loss: 0.685490]\n",
      "epoch:23 step:22156 [D loss: 0.456660, acc.: 80.47%] [G loss: 0.708912]\n",
      "epoch:23 step:22157 [D loss: 0.555597, acc.: 69.53%] [G loss: 0.739096]\n",
      "epoch:23 step:22158 [D loss: 0.541065, acc.: 71.88%] [G loss: 0.600251]\n",
      "epoch:23 step:22159 [D loss: 0.529005, acc.: 74.22%] [G loss: 0.630124]\n",
      "epoch:23 step:22160 [D loss: 0.563314, acc.: 67.19%] [G loss: 0.385515]\n",
      "epoch:23 step:22161 [D loss: 0.544570, acc.: 65.62%] [G loss: 0.607163]\n",
      "epoch:23 step:22162 [D loss: 0.456174, acc.: 80.47%] [G loss: 0.636325]\n",
      "epoch:23 step:22163 [D loss: 0.552934, acc.: 65.62%] [G loss: 0.497990]\n",
      "epoch:23 step:22164 [D loss: 0.487801, acc.: 76.56%] [G loss: 0.604289]\n",
      "epoch:23 step:22165 [D loss: 0.576130, acc.: 64.06%] [G loss: 0.630944]\n",
      "epoch:23 step:22166 [D loss: 0.540692, acc.: 69.53%] [G loss: 0.598675]\n",
      "epoch:23 step:22167 [D loss: 0.578448, acc.: 69.53%] [G loss: 0.595031]\n",
      "epoch:23 step:22168 [D loss: 0.501793, acc.: 73.44%] [G loss: 0.697954]\n",
      "epoch:23 step:22169 [D loss: 0.552444, acc.: 71.09%] [G loss: 0.746052]\n",
      "epoch:23 step:22170 [D loss: 0.502817, acc.: 72.66%] [G loss: 0.580152]\n",
      "epoch:23 step:22171 [D loss: 0.493558, acc.: 76.56%] [G loss: 0.953776]\n",
      "epoch:23 step:22172 [D loss: 0.578478, acc.: 65.62%] [G loss: 0.820415]\n",
      "epoch:23 step:22173 [D loss: 0.546342, acc.: 70.31%] [G loss: 0.653695]\n",
      "epoch:23 step:22174 [D loss: 0.509522, acc.: 71.88%] [G loss: 0.667651]\n",
      "epoch:23 step:22175 [D loss: 0.466307, acc.: 75.00%] [G loss: 0.926577]\n",
      "epoch:23 step:22176 [D loss: 0.604254, acc.: 64.06%] [G loss: 0.683206]\n",
      "epoch:23 step:22177 [D loss: 0.582744, acc.: 64.06%] [G loss: 0.543475]\n",
      "epoch:23 step:22178 [D loss: 0.521818, acc.: 73.44%] [G loss: 0.559753]\n",
      "epoch:23 step:22179 [D loss: 0.560808, acc.: 70.31%] [G loss: 0.659106]\n",
      "epoch:23 step:22180 [D loss: 0.569014, acc.: 65.62%] [G loss: 0.696840]\n",
      "epoch:23 step:22181 [D loss: 0.571045, acc.: 69.53%] [G loss: 0.675804]\n",
      "epoch:23 step:22182 [D loss: 0.501775, acc.: 75.00%] [G loss: 0.766644]\n",
      "epoch:23 step:22183 [D loss: 0.518983, acc.: 73.44%] [G loss: 0.733353]\n",
      "epoch:23 step:22184 [D loss: 0.517185, acc.: 71.88%] [G loss: 0.732884]\n",
      "epoch:23 step:22185 [D loss: 0.489654, acc.: 74.22%] [G loss: 0.844950]\n",
      "epoch:23 step:22186 [D loss: 0.504416, acc.: 71.09%] [G loss: 0.948269]\n",
      "epoch:23 step:22187 [D loss: 0.614220, acc.: 66.41%] [G loss: 0.632194]\n",
      "epoch:23 step:22188 [D loss: 0.541249, acc.: 72.66%] [G loss: 0.693372]\n",
      "epoch:23 step:22189 [D loss: 0.525492, acc.: 70.31%] [G loss: 0.585482]\n",
      "epoch:23 step:22190 [D loss: 0.486599, acc.: 77.34%] [G loss: 0.784139]\n",
      "epoch:23 step:22191 [D loss: 0.638180, acc.: 62.50%] [G loss: 0.593209]\n",
      "epoch:23 step:22192 [D loss: 0.495147, acc.: 75.78%] [G loss: 0.860884]\n",
      "epoch:23 step:22193 [D loss: 0.503003, acc.: 71.88%] [G loss: 0.890689]\n",
      "epoch:23 step:22194 [D loss: 0.498448, acc.: 72.66%] [G loss: 0.753110]\n",
      "epoch:23 step:22195 [D loss: 0.580978, acc.: 66.41%] [G loss: 0.576305]\n",
      "epoch:23 step:22196 [D loss: 0.536421, acc.: 75.78%] [G loss: 0.667974]\n",
      "epoch:23 step:22197 [D loss: 0.464706, acc.: 78.12%] [G loss: 0.752324]\n",
      "epoch:23 step:22198 [D loss: 0.446468, acc.: 78.12%] [G loss: 0.967325]\n",
      "epoch:23 step:22199 [D loss: 0.425333, acc.: 82.81%] [G loss: 1.130827]\n",
      "epoch:23 step:22200 [D loss: 0.492634, acc.: 75.78%] [G loss: 0.918752]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.602100\n",
      "FID: 49.569557\n",
      "0 = 12.973005448436743\n",
      "1 = 0.08584103805874228\n",
      "2 = 0.8598999977111816\n",
      "3 = 0.8133999705314636\n",
      "4 = 0.9064000248908997\n",
      "5 = 0.8968026638031006\n",
      "6 = 0.8133999705314636\n",
      "7 = 8.396812465822709\n",
      "8 = 0.1485282219165384\n",
      "9 = 0.6991999745368958\n",
      "10 = 0.6966000199317932\n",
      "11 = 0.7017999887466431\n",
      "12 = 0.7002412676811218\n",
      "13 = 0.6966000199317932\n",
      "14 = 6.60212516784668\n",
      "15 = 6.560941696166992\n",
      "16 = 0.4091579020023346\n",
      "17 = 6.602099895477295\n",
      "18 = 49.569557189941406\n",
      "epoch:23 step:22201 [D loss: 0.518714, acc.: 71.88%] [G loss: 1.081481]\n",
      "epoch:23 step:22202 [D loss: 0.491921, acc.: 75.78%] [G loss: 0.899675]\n",
      "epoch:23 step:22203 [D loss: 0.670319, acc.: 57.03%] [G loss: 0.632182]\n",
      "epoch:23 step:22204 [D loss: 0.531377, acc.: 74.22%] [G loss: 0.579691]\n",
      "epoch:23 step:22205 [D loss: 0.444534, acc.: 78.91%] [G loss: 0.939696]\n",
      "epoch:23 step:22206 [D loss: 0.561701, acc.: 71.09%] [G loss: 0.703654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22207 [D loss: 0.535216, acc.: 72.66%] [G loss: 0.655072]\n",
      "epoch:23 step:22208 [D loss: 0.508679, acc.: 71.88%] [G loss: 0.560702]\n",
      "epoch:23 step:22209 [D loss: 0.565542, acc.: 67.97%] [G loss: 0.572068]\n",
      "epoch:23 step:22210 [D loss: 0.517510, acc.: 75.00%] [G loss: 0.692672]\n",
      "epoch:23 step:22211 [D loss: 0.476852, acc.: 75.78%] [G loss: 0.731272]\n",
      "epoch:23 step:22212 [D loss: 0.571219, acc.: 71.88%] [G loss: 0.764052]\n",
      "epoch:23 step:22213 [D loss: 0.515804, acc.: 76.56%] [G loss: 0.708031]\n",
      "epoch:23 step:22214 [D loss: 0.570888, acc.: 69.53%] [G loss: 0.590820]\n",
      "epoch:23 step:22215 [D loss: 0.600816, acc.: 67.19%] [G loss: 0.568648]\n",
      "epoch:23 step:22216 [D loss: 0.546353, acc.: 69.53%] [G loss: 0.719693]\n",
      "epoch:23 step:22217 [D loss: 0.567029, acc.: 66.41%] [G loss: 0.586364]\n",
      "epoch:23 step:22218 [D loss: 0.575970, acc.: 66.41%] [G loss: 0.561173]\n",
      "epoch:23 step:22219 [D loss: 0.517577, acc.: 73.44%] [G loss: 0.772153]\n",
      "epoch:23 step:22220 [D loss: 0.558256, acc.: 67.97%] [G loss: 0.597799]\n",
      "epoch:23 step:22221 [D loss: 0.530125, acc.: 68.75%] [G loss: 0.761257]\n",
      "epoch:23 step:22222 [D loss: 0.564224, acc.: 70.31%] [G loss: 0.638367]\n",
      "epoch:23 step:22223 [D loss: 0.519656, acc.: 75.78%] [G loss: 0.717156]\n",
      "epoch:23 step:22224 [D loss: 0.557417, acc.: 65.62%] [G loss: 0.563480]\n",
      "epoch:23 step:22225 [D loss: 0.555567, acc.: 67.97%] [G loss: 0.602798]\n",
      "epoch:23 step:22226 [D loss: 0.600891, acc.: 68.75%] [G loss: 0.611720]\n",
      "epoch:23 step:22227 [D loss: 0.489331, acc.: 75.78%] [G loss: 0.686591]\n",
      "epoch:23 step:22228 [D loss: 0.487117, acc.: 78.12%] [G loss: 0.696735]\n",
      "epoch:23 step:22229 [D loss: 0.591463, acc.: 67.19%] [G loss: 0.576075]\n",
      "epoch:23 step:22230 [D loss: 0.502700, acc.: 75.78%] [G loss: 0.780996]\n",
      "epoch:23 step:22231 [D loss: 0.562590, acc.: 70.31%] [G loss: 0.612076]\n",
      "epoch:23 step:22232 [D loss: 0.515923, acc.: 75.78%] [G loss: 0.570609]\n",
      "epoch:23 step:22233 [D loss: 0.501788, acc.: 73.44%] [G loss: 0.698359]\n",
      "epoch:23 step:22234 [D loss: 0.585550, acc.: 68.75%] [G loss: 0.731471]\n",
      "epoch:23 step:22235 [D loss: 0.632335, acc.: 65.62%] [G loss: 0.658258]\n",
      "epoch:23 step:22236 [D loss: 0.517744, acc.: 75.00%] [G loss: 0.609973]\n",
      "epoch:23 step:22237 [D loss: 0.592928, acc.: 67.19%] [G loss: 0.648721]\n",
      "epoch:23 step:22238 [D loss: 0.619403, acc.: 61.72%] [G loss: 0.591006]\n",
      "epoch:23 step:22239 [D loss: 0.503095, acc.: 74.22%] [G loss: 0.657337]\n",
      "epoch:23 step:22240 [D loss: 0.633813, acc.: 58.59%] [G loss: 0.616964]\n",
      "epoch:23 step:22241 [D loss: 0.461528, acc.: 78.91%] [G loss: 0.720993]\n",
      "epoch:23 step:22242 [D loss: 0.471315, acc.: 78.91%] [G loss: 0.683965]\n",
      "epoch:23 step:22243 [D loss: 0.536600, acc.: 71.88%] [G loss: 0.768685]\n",
      "epoch:23 step:22244 [D loss: 0.449150, acc.: 79.69%] [G loss: 0.665497]\n",
      "epoch:23 step:22245 [D loss: 0.520695, acc.: 74.22%] [G loss: 0.774934]\n",
      "epoch:23 step:22246 [D loss: 0.568228, acc.: 69.53%] [G loss: 0.798241]\n",
      "epoch:23 step:22247 [D loss: 0.650144, acc.: 58.59%] [G loss: 0.638776]\n",
      "epoch:23 step:22248 [D loss: 0.569801, acc.: 64.84%] [G loss: 0.690837]\n",
      "epoch:23 step:22249 [D loss: 0.633980, acc.: 60.94%] [G loss: 0.547200]\n",
      "epoch:23 step:22250 [D loss: 0.502162, acc.: 74.22%] [G loss: 0.606972]\n",
      "epoch:23 step:22251 [D loss: 0.532781, acc.: 71.09%] [G loss: 0.830398]\n",
      "epoch:23 step:22252 [D loss: 0.517837, acc.: 71.09%] [G loss: 0.965699]\n",
      "epoch:23 step:22253 [D loss: 0.618211, acc.: 64.84%] [G loss: 0.767179]\n",
      "epoch:23 step:22254 [D loss: 0.652688, acc.: 62.50%] [G loss: 0.568468]\n",
      "epoch:23 step:22255 [D loss: 0.615954, acc.: 64.06%] [G loss: 0.636040]\n",
      "epoch:23 step:22256 [D loss: 0.540535, acc.: 72.66%] [G loss: 0.729609]\n",
      "epoch:23 step:22257 [D loss: 0.533565, acc.: 72.66%] [G loss: 0.567946]\n",
      "epoch:23 step:22258 [D loss: 0.488311, acc.: 78.91%] [G loss: 0.811533]\n",
      "epoch:23 step:22259 [D loss: 0.531222, acc.: 71.88%] [G loss: 0.858923]\n",
      "epoch:23 step:22260 [D loss: 0.568829, acc.: 64.84%] [G loss: 0.772254]\n",
      "epoch:23 step:22261 [D loss: 0.568912, acc.: 64.06%] [G loss: 0.677727]\n",
      "epoch:23 step:22262 [D loss: 0.490087, acc.: 76.56%] [G loss: 0.776918]\n",
      "epoch:23 step:22263 [D loss: 0.509965, acc.: 72.66%] [G loss: 0.735852]\n",
      "epoch:23 step:22264 [D loss: 0.546979, acc.: 67.97%] [G loss: 0.726757]\n",
      "epoch:23 step:22265 [D loss: 0.505362, acc.: 73.44%] [G loss: 0.554529]\n",
      "epoch:23 step:22266 [D loss: 0.561293, acc.: 73.44%] [G loss: 0.586988]\n",
      "epoch:23 step:22267 [D loss: 0.640163, acc.: 65.62%] [G loss: 0.606080]\n",
      "epoch:23 step:22268 [D loss: 0.549213, acc.: 72.66%] [G loss: 0.739419]\n",
      "epoch:23 step:22269 [D loss: 0.586834, acc.: 64.06%] [G loss: 0.812294]\n",
      "epoch:23 step:22270 [D loss: 0.483103, acc.: 74.22%] [G loss: 0.867861]\n",
      "epoch:23 step:22271 [D loss: 0.604346, acc.: 70.31%] [G loss: 0.667625]\n",
      "epoch:23 step:22272 [D loss: 0.511262, acc.: 76.56%] [G loss: 0.727580]\n",
      "epoch:23 step:22273 [D loss: 0.532390, acc.: 75.78%] [G loss: 0.592945]\n",
      "epoch:23 step:22274 [D loss: 0.566597, acc.: 67.19%] [G loss: 0.602190]\n",
      "epoch:23 step:22275 [D loss: 0.569532, acc.: 68.75%] [G loss: 0.654864]\n",
      "epoch:23 step:22276 [D loss: 0.521798, acc.: 74.22%] [G loss: 0.619155]\n",
      "epoch:23 step:22277 [D loss: 0.511727, acc.: 75.00%] [G loss: 0.762814]\n",
      "epoch:23 step:22278 [D loss: 0.525461, acc.: 72.66%] [G loss: 0.597220]\n",
      "epoch:23 step:22279 [D loss: 0.530749, acc.: 75.78%] [G loss: 0.768157]\n",
      "epoch:23 step:22280 [D loss: 0.548680, acc.: 70.31%] [G loss: 0.738578]\n",
      "epoch:23 step:22281 [D loss: 0.510757, acc.: 78.12%] [G loss: 0.567536]\n",
      "epoch:23 step:22282 [D loss: 0.590427, acc.: 59.38%] [G loss: 0.549427]\n",
      "epoch:23 step:22283 [D loss: 0.592271, acc.: 64.06%] [G loss: 0.536911]\n",
      "epoch:23 step:22284 [D loss: 0.589073, acc.: 66.41%] [G loss: 0.659178]\n",
      "epoch:23 step:22285 [D loss: 0.487684, acc.: 74.22%] [G loss: 0.652125]\n",
      "epoch:23 step:22286 [D loss: 0.541378, acc.: 71.88%] [G loss: 0.542106]\n",
      "epoch:23 step:22287 [D loss: 0.491954, acc.: 75.78%] [G loss: 0.774180]\n",
      "epoch:23 step:22288 [D loss: 0.543558, acc.: 70.31%] [G loss: 0.614736]\n",
      "epoch:23 step:22289 [D loss: 0.513076, acc.: 75.00%] [G loss: 0.584999]\n",
      "epoch:23 step:22290 [D loss: 0.527871, acc.: 71.09%] [G loss: 0.674480]\n",
      "epoch:23 step:22291 [D loss: 0.632070, acc.: 60.94%] [G loss: 0.446528]\n",
      "epoch:23 step:22292 [D loss: 0.530411, acc.: 73.44%] [G loss: 0.521450]\n",
      "epoch:23 step:22293 [D loss: 0.521048, acc.: 73.44%] [G loss: 0.757777]\n",
      "epoch:23 step:22294 [D loss: 0.527683, acc.: 73.44%] [G loss: 0.619851]\n",
      "epoch:23 step:22295 [D loss: 0.526571, acc.: 72.66%] [G loss: 0.785115]\n",
      "epoch:23 step:22296 [D loss: 0.580611, acc.: 67.97%] [G loss: 0.525902]\n",
      "epoch:23 step:22297 [D loss: 0.499435, acc.: 71.88%] [G loss: 0.675639]\n",
      "epoch:23 step:22298 [D loss: 0.445749, acc.: 81.25%] [G loss: 0.866404]\n",
      "epoch:23 step:22299 [D loss: 0.548201, acc.: 71.09%] [G loss: 0.584243]\n",
      "epoch:23 step:22300 [D loss: 0.523833, acc.: 71.88%] [G loss: 0.801375]\n",
      "epoch:23 step:22301 [D loss: 0.515349, acc.: 77.34%] [G loss: 0.634987]\n",
      "epoch:23 step:22302 [D loss: 0.512550, acc.: 72.66%] [G loss: 0.670035]\n",
      "epoch:23 step:22303 [D loss: 0.586161, acc.: 64.84%] [G loss: 0.728945]\n",
      "epoch:23 step:22304 [D loss: 0.521761, acc.: 74.22%] [G loss: 0.871601]\n",
      "epoch:23 step:22305 [D loss: 0.513512, acc.: 75.00%] [G loss: 0.884785]\n",
      "epoch:23 step:22306 [D loss: 0.552812, acc.: 70.31%] [G loss: 0.486777]\n",
      "epoch:23 step:22307 [D loss: 0.501940, acc.: 72.66%] [G loss: 0.642862]\n",
      "epoch:23 step:22308 [D loss: 0.585076, acc.: 64.06%] [G loss: 0.529097]\n",
      "epoch:23 step:22309 [D loss: 0.530973, acc.: 78.91%] [G loss: 0.710425]\n",
      "epoch:23 step:22310 [D loss: 0.546709, acc.: 71.09%] [G loss: 0.761315]\n",
      "epoch:23 step:22311 [D loss: 0.547762, acc.: 70.31%] [G loss: 0.548067]\n",
      "epoch:23 step:22312 [D loss: 0.612860, acc.: 65.62%] [G loss: 0.617046]\n",
      "epoch:23 step:22313 [D loss: 0.537276, acc.: 67.19%] [G loss: 0.610850]\n",
      "epoch:23 step:22314 [D loss: 0.568178, acc.: 67.97%] [G loss: 0.666142]\n",
      "epoch:23 step:22315 [D loss: 0.547357, acc.: 75.78%] [G loss: 0.578649]\n",
      "epoch:23 step:22316 [D loss: 0.563137, acc.: 68.75%] [G loss: 0.617091]\n",
      "epoch:23 step:22317 [D loss: 0.625832, acc.: 61.72%] [G loss: 0.480728]\n",
      "epoch:23 step:22318 [D loss: 0.518807, acc.: 76.56%] [G loss: 0.562029]\n",
      "epoch:23 step:22319 [D loss: 0.494311, acc.: 73.44%] [G loss: 0.840708]\n",
      "epoch:23 step:22320 [D loss: 0.433629, acc.: 84.38%] [G loss: 0.875620]\n",
      "epoch:23 step:22321 [D loss: 0.519497, acc.: 70.31%] [G loss: 0.991938]\n",
      "epoch:23 step:22322 [D loss: 0.493787, acc.: 75.00%] [G loss: 0.815737]\n",
      "epoch:23 step:22323 [D loss: 0.615981, acc.: 65.62%] [G loss: 0.610872]\n",
      "epoch:23 step:22324 [D loss: 0.581864, acc.: 66.41%] [G loss: 0.835528]\n",
      "epoch:23 step:22325 [D loss: 0.534006, acc.: 72.66%] [G loss: 0.744742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22326 [D loss: 0.509121, acc.: 71.88%] [G loss: 0.764401]\n",
      "epoch:23 step:22327 [D loss: 0.588004, acc.: 69.53%] [G loss: 0.664632]\n",
      "epoch:23 step:22328 [D loss: 0.603340, acc.: 66.41%] [G loss: 0.740324]\n",
      "epoch:23 step:22329 [D loss: 0.575640, acc.: 64.06%] [G loss: 0.598579]\n",
      "epoch:23 step:22330 [D loss: 0.534888, acc.: 70.31%] [G loss: 0.589774]\n",
      "epoch:23 step:22331 [D loss: 0.501785, acc.: 76.56%] [G loss: 0.796837]\n",
      "epoch:23 step:22332 [D loss: 0.520710, acc.: 71.88%] [G loss: 0.680167]\n",
      "epoch:23 step:22333 [D loss: 0.541585, acc.: 67.19%] [G loss: 0.895058]\n",
      "epoch:23 step:22334 [D loss: 0.508077, acc.: 75.00%] [G loss: 1.012996]\n",
      "epoch:23 step:22335 [D loss: 0.602527, acc.: 71.09%] [G loss: 0.771711]\n",
      "epoch:23 step:22336 [D loss: 0.523662, acc.: 73.44%] [G loss: 0.552297]\n",
      "epoch:23 step:22337 [D loss: 0.509398, acc.: 75.78%] [G loss: 0.686664]\n",
      "epoch:23 step:22338 [D loss: 0.603692, acc.: 70.31%] [G loss: 0.642244]\n",
      "epoch:23 step:22339 [D loss: 0.609156, acc.: 67.19%] [G loss: 0.623294]\n",
      "epoch:23 step:22340 [D loss: 0.563853, acc.: 67.19%] [G loss: 0.615450]\n",
      "epoch:23 step:22341 [D loss: 0.531234, acc.: 64.84%] [G loss: 0.607173]\n",
      "epoch:23 step:22342 [D loss: 0.548739, acc.: 75.00%] [G loss: 0.647450]\n",
      "epoch:23 step:22343 [D loss: 0.478402, acc.: 75.00%] [G loss: 0.904453]\n",
      "epoch:23 step:22344 [D loss: 0.587714, acc.: 65.62%] [G loss: 0.691903]\n",
      "epoch:23 step:22345 [D loss: 0.667199, acc.: 60.16%] [G loss: 0.669869]\n",
      "epoch:23 step:22346 [D loss: 0.509231, acc.: 73.44%] [G loss: 0.716809]\n",
      "epoch:23 step:22347 [D loss: 0.520366, acc.: 74.22%] [G loss: 0.866610]\n",
      "epoch:23 step:22348 [D loss: 0.559698, acc.: 67.97%] [G loss: 0.780002]\n",
      "epoch:23 step:22349 [D loss: 0.592243, acc.: 62.50%] [G loss: 0.825564]\n",
      "epoch:23 step:22350 [D loss: 0.605017, acc.: 64.84%] [G loss: 0.714634]\n",
      "epoch:23 step:22351 [D loss: 0.607983, acc.: 62.50%] [G loss: 0.517772]\n",
      "epoch:23 step:22352 [D loss: 0.510136, acc.: 71.88%] [G loss: 0.712824]\n",
      "epoch:23 step:22353 [D loss: 0.464794, acc.: 75.78%] [G loss: 0.715835]\n",
      "epoch:23 step:22354 [D loss: 0.525943, acc.: 75.78%] [G loss: 0.693497]\n",
      "epoch:23 step:22355 [D loss: 0.574668, acc.: 66.41%] [G loss: 0.683660]\n",
      "epoch:23 step:22356 [D loss: 0.567974, acc.: 69.53%] [G loss: 0.545372]\n",
      "epoch:23 step:22357 [D loss: 0.600966, acc.: 63.28%] [G loss: 0.621008]\n",
      "epoch:23 step:22358 [D loss: 0.518690, acc.: 73.44%] [G loss: 0.649477]\n",
      "epoch:23 step:22359 [D loss: 0.591251, acc.: 67.19%] [G loss: 0.724432]\n",
      "epoch:23 step:22360 [D loss: 0.597480, acc.: 65.62%] [G loss: 0.500340]\n",
      "epoch:23 step:22361 [D loss: 0.537135, acc.: 73.44%] [G loss: 0.593425]\n",
      "epoch:23 step:22362 [D loss: 0.535662, acc.: 70.31%] [G loss: 0.602761]\n",
      "epoch:23 step:22363 [D loss: 0.609983, acc.: 64.84%] [G loss: 0.569576]\n",
      "epoch:23 step:22364 [D loss: 0.560814, acc.: 68.75%] [G loss: 0.615608]\n",
      "epoch:23 step:22365 [D loss: 0.454801, acc.: 80.47%] [G loss: 0.684342]\n",
      "epoch:23 step:22366 [D loss: 0.556150, acc.: 75.00%] [G loss: 1.031905]\n",
      "epoch:23 step:22367 [D loss: 0.542988, acc.: 70.31%] [G loss: 0.887321]\n",
      "epoch:23 step:22368 [D loss: 0.642068, acc.: 66.41%] [G loss: 0.674428]\n",
      "epoch:23 step:22369 [D loss: 0.569469, acc.: 67.19%] [G loss: 0.850351]\n",
      "epoch:23 step:22370 [D loss: 0.538520, acc.: 73.44%] [G loss: 0.516849]\n",
      "epoch:23 step:22371 [D loss: 0.630955, acc.: 66.41%] [G loss: 0.717218]\n",
      "epoch:23 step:22372 [D loss: 0.564768, acc.: 74.22%] [G loss: 0.488638]\n",
      "epoch:23 step:22373 [D loss: 0.584248, acc.: 64.06%] [G loss: 0.634303]\n",
      "epoch:23 step:22374 [D loss: 0.434028, acc.: 77.34%] [G loss: 0.843043]\n",
      "epoch:23 step:22375 [D loss: 0.557106, acc.: 68.75%] [G loss: 0.609402]\n",
      "epoch:23 step:22376 [D loss: 0.524216, acc.: 70.31%] [G loss: 0.690317]\n",
      "epoch:23 step:22377 [D loss: 0.562046, acc.: 70.31%] [G loss: 0.607091]\n",
      "epoch:23 step:22378 [D loss: 0.589199, acc.: 66.41%] [G loss: 0.683419]\n",
      "epoch:23 step:22379 [D loss: 0.660996, acc.: 61.72%] [G loss: 0.629933]\n",
      "epoch:23 step:22380 [D loss: 0.552144, acc.: 70.31%] [G loss: 0.672445]\n",
      "epoch:23 step:22381 [D loss: 0.562624, acc.: 71.88%] [G loss: 0.603344]\n",
      "epoch:23 step:22382 [D loss: 0.565155, acc.: 69.53%] [G loss: 0.694987]\n",
      "epoch:23 step:22383 [D loss: 0.510931, acc.: 75.78%] [G loss: 0.690288]\n",
      "epoch:23 step:22384 [D loss: 0.533637, acc.: 69.53%] [G loss: 0.682863]\n",
      "epoch:23 step:22385 [D loss: 0.555633, acc.: 70.31%] [G loss: 0.568723]\n",
      "epoch:23 step:22386 [D loss: 0.532646, acc.: 71.88%] [G loss: 0.593334]\n",
      "epoch:23 step:22387 [D loss: 0.506853, acc.: 78.12%] [G loss: 0.718710]\n",
      "epoch:23 step:22388 [D loss: 0.517372, acc.: 70.31%] [G loss: 0.825326]\n",
      "epoch:23 step:22389 [D loss: 0.517864, acc.: 78.91%] [G loss: 0.590060]\n",
      "epoch:23 step:22390 [D loss: 0.606282, acc.: 61.72%] [G loss: 0.554286]\n",
      "epoch:23 step:22391 [D loss: 0.565530, acc.: 67.19%] [G loss: 0.597183]\n",
      "epoch:23 step:22392 [D loss: 0.549449, acc.: 68.75%] [G loss: 0.497262]\n",
      "epoch:23 step:22393 [D loss: 0.546712, acc.: 66.41%] [G loss: 0.568964]\n",
      "epoch:23 step:22394 [D loss: 0.515310, acc.: 71.88%] [G loss: 0.722427]\n",
      "epoch:23 step:22395 [D loss: 0.548011, acc.: 73.44%] [G loss: 0.669652]\n",
      "epoch:23 step:22396 [D loss: 0.563045, acc.: 62.50%] [G loss: 0.777522]\n",
      "epoch:23 step:22397 [D loss: 0.573297, acc.: 64.84%] [G loss: 0.521372]\n",
      "epoch:23 step:22398 [D loss: 0.621901, acc.: 60.16%] [G loss: 0.446723]\n",
      "epoch:23 step:22399 [D loss: 0.642682, acc.: 57.81%] [G loss: 0.410297]\n",
      "epoch:23 step:22400 [D loss: 0.581644, acc.: 65.62%] [G loss: 0.561832]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.699183\n",
      "FID: 44.746574\n",
      "0 = 12.838383879184729\n",
      "1 = 0.08797129002320682\n",
      "2 = 0.8647000193595886\n",
      "3 = 0.8234000205993652\n",
      "4 = 0.906000018119812\n",
      "5 = 0.8975365161895752\n",
      "6 = 0.8234000205993652\n",
      "7 = 8.168047119331368\n",
      "8 = 0.14631704309866042\n",
      "9 = 0.6998999714851379\n",
      "10 = 0.6930000185966492\n",
      "11 = 0.7067999839782715\n",
      "12 = 0.702697217464447\n",
      "13 = 0.6930000185966492\n",
      "14 = 6.699213027954102\n",
      "15 = 7.127821922302246\n",
      "16 = 0.37825173139572144\n",
      "17 = 6.699182510375977\n",
      "18 = 44.74657440185547\n",
      "epoch:23 step:22401 [D loss: 0.563178, acc.: 69.53%] [G loss: 0.598198]\n",
      "epoch:23 step:22402 [D loss: 0.537400, acc.: 67.97%] [G loss: 0.630536]\n",
      "epoch:23 step:22403 [D loss: 0.529747, acc.: 73.44%] [G loss: 0.630061]\n",
      "epoch:23 step:22404 [D loss: 0.528384, acc.: 71.09%] [G loss: 0.696433]\n",
      "epoch:23 step:22405 [D loss: 0.538139, acc.: 67.19%] [G loss: 0.708536]\n",
      "epoch:23 step:22406 [D loss: 0.545587, acc.: 72.66%] [G loss: 0.728238]\n",
      "epoch:23 step:22407 [D loss: 0.605371, acc.: 64.06%] [G loss: 0.688643]\n",
      "epoch:23 step:22408 [D loss: 0.435839, acc.: 78.91%] [G loss: 0.828542]\n",
      "epoch:23 step:22409 [D loss: 0.607569, acc.: 69.53%] [G loss: 0.665999]\n",
      "epoch:23 step:22410 [D loss: 0.565000, acc.: 72.66%] [G loss: 0.684117]\n",
      "epoch:23 step:22411 [D loss: 0.480349, acc.: 80.47%] [G loss: 0.805666]\n",
      "epoch:23 step:22412 [D loss: 0.640331, acc.: 62.50%] [G loss: 0.621514]\n",
      "epoch:23 step:22413 [D loss: 0.574835, acc.: 70.31%] [G loss: 0.508829]\n",
      "epoch:23 step:22414 [D loss: 0.554938, acc.: 72.66%] [G loss: 0.481107]\n",
      "epoch:23 step:22415 [D loss: 0.527862, acc.: 69.53%] [G loss: 0.574093]\n",
      "epoch:23 step:22416 [D loss: 0.563921, acc.: 67.19%] [G loss: 0.537388]\n",
      "epoch:23 step:22417 [D loss: 0.564560, acc.: 67.19%] [G loss: 0.649714]\n",
      "epoch:23 step:22418 [D loss: 0.644736, acc.: 59.38%] [G loss: 0.574947]\n",
      "epoch:23 step:22419 [D loss: 0.520296, acc.: 67.19%] [G loss: 0.614340]\n",
      "epoch:23 step:22420 [D loss: 0.602068, acc.: 65.62%] [G loss: 0.560800]\n",
      "epoch:23 step:22421 [D loss: 0.513936, acc.: 74.22%] [G loss: 0.598450]\n",
      "epoch:23 step:22422 [D loss: 0.470769, acc.: 73.44%] [G loss: 0.806657]\n",
      "epoch:23 step:22423 [D loss: 0.556742, acc.: 64.84%] [G loss: 0.651137]\n",
      "epoch:23 step:22424 [D loss: 0.589125, acc.: 64.06%] [G loss: 0.684955]\n",
      "epoch:23 step:22425 [D loss: 0.576192, acc.: 66.41%] [G loss: 0.576291]\n",
      "epoch:23 step:22426 [D loss: 0.505521, acc.: 71.88%] [G loss: 0.720859]\n",
      "epoch:23 step:22427 [D loss: 0.579652, acc.: 69.53%] [G loss: 0.585745]\n",
      "epoch:23 step:22428 [D loss: 0.619869, acc.: 58.59%] [G loss: 0.495092]\n",
      "epoch:23 step:22429 [D loss: 0.595069, acc.: 64.06%] [G loss: 0.450263]\n",
      "epoch:23 step:22430 [D loss: 0.637633, acc.: 64.06%] [G loss: 0.507718]\n",
      "epoch:23 step:22431 [D loss: 0.651632, acc.: 63.28%] [G loss: 0.470096]\n",
      "epoch:23 step:22432 [D loss: 0.553459, acc.: 68.75%] [G loss: 0.529657]\n",
      "epoch:23 step:22433 [D loss: 0.590345, acc.: 64.06%] [G loss: 0.607261]\n",
      "epoch:23 step:22434 [D loss: 0.591163, acc.: 67.19%] [G loss: 0.524443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22435 [D loss: 0.478698, acc.: 78.91%] [G loss: 0.511673]\n",
      "epoch:23 step:22436 [D loss: 0.589287, acc.: 62.50%] [G loss: 0.736743]\n",
      "epoch:23 step:22437 [D loss: 0.519801, acc.: 69.53%] [G loss: 0.789277]\n",
      "epoch:23 step:22438 [D loss: 0.514339, acc.: 72.66%] [G loss: 0.724895]\n",
      "epoch:23 step:22439 [D loss: 0.539280, acc.: 66.41%] [G loss: 0.758718]\n",
      "epoch:23 step:22440 [D loss: 0.565880, acc.: 65.62%] [G loss: 0.797482]\n",
      "epoch:23 step:22441 [D loss: 0.462588, acc.: 78.12%] [G loss: 0.696234]\n",
      "epoch:23 step:22442 [D loss: 0.616171, acc.: 62.50%] [G loss: 0.567867]\n",
      "epoch:23 step:22443 [D loss: 0.586651, acc.: 67.97%] [G loss: 0.564184]\n",
      "epoch:23 step:22444 [D loss: 0.556359, acc.: 71.88%] [G loss: 0.475972]\n",
      "epoch:23 step:22445 [D loss: 0.461048, acc.: 78.12%] [G loss: 0.680392]\n",
      "epoch:23 step:22446 [D loss: 0.486218, acc.: 78.12%] [G loss: 0.662377]\n",
      "epoch:23 step:22447 [D loss: 0.522294, acc.: 72.66%] [G loss: 0.721206]\n",
      "epoch:23 step:22448 [D loss: 0.516298, acc.: 75.78%] [G loss: 0.819621]\n",
      "epoch:23 step:22449 [D loss: 0.491684, acc.: 78.12%] [G loss: 0.860728]\n",
      "epoch:23 step:22450 [D loss: 0.512348, acc.: 74.22%] [G loss: 0.842820]\n",
      "epoch:23 step:22451 [D loss: 0.568167, acc.: 73.44%] [G loss: 0.728125]\n",
      "epoch:23 step:22452 [D loss: 0.501343, acc.: 74.22%] [G loss: 0.766021]\n",
      "epoch:23 step:22453 [D loss: 0.559691, acc.: 67.19%] [G loss: 0.696771]\n",
      "epoch:23 step:22454 [D loss: 0.542529, acc.: 69.53%] [G loss: 0.646219]\n",
      "epoch:23 step:22455 [D loss: 0.584211, acc.: 64.84%] [G loss: 0.716002]\n",
      "epoch:23 step:22456 [D loss: 0.559301, acc.: 68.75%] [G loss: 0.791721]\n",
      "epoch:23 step:22457 [D loss: 0.521071, acc.: 75.78%] [G loss: 0.680130]\n",
      "epoch:23 step:22458 [D loss: 0.544157, acc.: 74.22%] [G loss: 0.697455]\n",
      "epoch:23 step:22459 [D loss: 0.513434, acc.: 72.66%] [G loss: 0.671892]\n",
      "epoch:23 step:22460 [D loss: 0.556508, acc.: 71.88%] [G loss: 0.583136]\n",
      "epoch:23 step:22461 [D loss: 0.593310, acc.: 62.50%] [G loss: 0.677512]\n",
      "epoch:23 step:22462 [D loss: 0.469041, acc.: 71.88%] [G loss: 0.777711]\n",
      "epoch:23 step:22463 [D loss: 0.492249, acc.: 74.22%] [G loss: 0.810340]\n",
      "epoch:23 step:22464 [D loss: 0.583479, acc.: 71.09%] [G loss: 0.632681]\n",
      "epoch:23 step:22465 [D loss: 0.518812, acc.: 67.97%] [G loss: 0.922920]\n",
      "epoch:23 step:22466 [D loss: 0.689582, acc.: 65.62%] [G loss: 0.682401]\n",
      "epoch:23 step:22467 [D loss: 0.544374, acc.: 73.44%] [G loss: 0.715301]\n",
      "epoch:23 step:22468 [D loss: 0.593406, acc.: 60.94%] [G loss: 0.720917]\n",
      "epoch:23 step:22469 [D loss: 0.473916, acc.: 78.12%] [G loss: 0.635382]\n",
      "epoch:23 step:22470 [D loss: 0.462254, acc.: 79.69%] [G loss: 1.034717]\n",
      "epoch:23 step:22471 [D loss: 0.655243, acc.: 66.41%] [G loss: 0.762957]\n",
      "epoch:23 step:22472 [D loss: 0.513377, acc.: 71.88%] [G loss: 0.823117]\n",
      "epoch:23 step:22473 [D loss: 0.524095, acc.: 71.09%] [G loss: 0.582454]\n",
      "epoch:23 step:22474 [D loss: 0.460960, acc.: 77.34%] [G loss: 0.776963]\n",
      "epoch:23 step:22475 [D loss: 0.474331, acc.: 78.12%] [G loss: 0.804026]\n",
      "epoch:23 step:22476 [D loss: 0.484864, acc.: 73.44%] [G loss: 1.076355]\n",
      "epoch:23 step:22477 [D loss: 0.450650, acc.: 79.69%] [G loss: 0.990986]\n",
      "epoch:23 step:22478 [D loss: 0.528024, acc.: 71.09%] [G loss: 1.023481]\n",
      "epoch:23 step:22479 [D loss: 0.586695, acc.: 74.22%] [G loss: 1.139009]\n",
      "epoch:23 step:22480 [D loss: 0.459959, acc.: 75.00%] [G loss: 1.144858]\n",
      "epoch:23 step:22481 [D loss: 0.516533, acc.: 74.22%] [G loss: 1.278043]\n",
      "epoch:23 step:22482 [D loss: 0.551599, acc.: 71.09%] [G loss: 0.739337]\n",
      "epoch:23 step:22483 [D loss: 0.609210, acc.: 67.97%] [G loss: 0.993413]\n",
      "epoch:23 step:22484 [D loss: 0.470084, acc.: 78.12%] [G loss: 0.909168]\n",
      "epoch:23 step:22485 [D loss: 0.509330, acc.: 71.88%] [G loss: 0.749436]\n",
      "epoch:23 step:22486 [D loss: 0.526080, acc.: 68.75%] [G loss: 0.767850]\n",
      "epoch:23 step:22487 [D loss: 0.387491, acc.: 83.59%] [G loss: 1.250690]\n",
      "epoch:23 step:22488 [D loss: 0.417475, acc.: 82.81%] [G loss: 1.217930]\n",
      "epoch:24 step:22489 [D loss: 0.562378, acc.: 67.97%] [G loss: 1.117133]\n",
      "epoch:24 step:22490 [D loss: 0.455580, acc.: 74.22%] [G loss: 1.088211]\n",
      "epoch:24 step:22491 [D loss: 0.570597, acc.: 68.75%] [G loss: 0.905459]\n",
      "epoch:24 step:22492 [D loss: 0.454382, acc.: 80.47%] [G loss: 0.902541]\n",
      "epoch:24 step:22493 [D loss: 0.602692, acc.: 64.84%] [G loss: 0.857609]\n",
      "epoch:24 step:22494 [D loss: 0.643550, acc.: 59.38%] [G loss: 0.761452]\n",
      "epoch:24 step:22495 [D loss: 0.521930, acc.: 79.69%] [G loss: 0.847987]\n",
      "epoch:24 step:22496 [D loss: 0.510321, acc.: 75.00%] [G loss: 0.807769]\n",
      "epoch:24 step:22497 [D loss: 0.507994, acc.: 76.56%] [G loss: 1.099974]\n",
      "epoch:24 step:22498 [D loss: 0.505427, acc.: 74.22%] [G loss: 0.962626]\n",
      "epoch:24 step:22499 [D loss: 0.501244, acc.: 76.56%] [G loss: 0.848571]\n",
      "epoch:24 step:22500 [D loss: 0.671256, acc.: 62.50%] [G loss: 0.627835]\n",
      "epoch:24 step:22501 [D loss: 0.559342, acc.: 74.22%] [G loss: 0.541476]\n",
      "epoch:24 step:22502 [D loss: 0.549554, acc.: 72.66%] [G loss: 0.822114]\n",
      "epoch:24 step:22503 [D loss: 0.500610, acc.: 74.22%] [G loss: 0.756430]\n",
      "epoch:24 step:22504 [D loss: 0.469422, acc.: 75.00%] [G loss: 0.980586]\n",
      "epoch:24 step:22505 [D loss: 0.590954, acc.: 69.53%] [G loss: 0.580806]\n",
      "epoch:24 step:22506 [D loss: 0.553940, acc.: 67.19%] [G loss: 0.686213]\n",
      "epoch:24 step:22507 [D loss: 0.619779, acc.: 62.50%] [G loss: 0.695922]\n",
      "epoch:24 step:22508 [D loss: 0.644462, acc.: 60.94%] [G loss: 0.667109]\n",
      "epoch:24 step:22509 [D loss: 0.574939, acc.: 65.62%] [G loss: 0.635956]\n",
      "epoch:24 step:22510 [D loss: 0.489468, acc.: 77.34%] [G loss: 0.840868]\n",
      "epoch:24 step:22511 [D loss: 0.531428, acc.: 71.09%] [G loss: 0.735436]\n",
      "epoch:24 step:22512 [D loss: 0.500230, acc.: 74.22%] [G loss: 0.707030]\n",
      "epoch:24 step:22513 [D loss: 0.516983, acc.: 75.78%] [G loss: 0.678965]\n",
      "epoch:24 step:22514 [D loss: 0.574894, acc.: 65.62%] [G loss: 0.605484]\n",
      "epoch:24 step:22515 [D loss: 0.495259, acc.: 74.22%] [G loss: 0.616314]\n",
      "epoch:24 step:22516 [D loss: 0.580708, acc.: 66.41%] [G loss: 0.513515]\n",
      "epoch:24 step:22517 [D loss: 0.518171, acc.: 75.78%] [G loss: 0.629293]\n",
      "epoch:24 step:22518 [D loss: 0.566188, acc.: 71.09%] [G loss: 0.633086]\n",
      "epoch:24 step:22519 [D loss: 0.613701, acc.: 64.84%] [G loss: 0.547559]\n",
      "epoch:24 step:22520 [D loss: 0.521308, acc.: 75.78%] [G loss: 0.694624]\n",
      "epoch:24 step:22521 [D loss: 0.548554, acc.: 70.31%] [G loss: 0.670602]\n",
      "epoch:24 step:22522 [D loss: 0.530011, acc.: 69.53%] [G loss: 0.686598]\n",
      "epoch:24 step:22523 [D loss: 0.612771, acc.: 67.19%] [G loss: 0.596672]\n",
      "epoch:24 step:22524 [D loss: 0.486481, acc.: 78.12%] [G loss: 0.804793]\n",
      "epoch:24 step:22525 [D loss: 0.521177, acc.: 74.22%] [G loss: 0.700904]\n",
      "epoch:24 step:22526 [D loss: 0.584227, acc.: 67.97%] [G loss: 0.658595]\n",
      "epoch:24 step:22527 [D loss: 0.566233, acc.: 67.19%] [G loss: 0.575175]\n",
      "epoch:24 step:22528 [D loss: 0.425783, acc.: 82.81%] [G loss: 0.760080]\n",
      "epoch:24 step:22529 [D loss: 0.530616, acc.: 69.53%] [G loss: 0.704272]\n",
      "epoch:24 step:22530 [D loss: 0.585280, acc.: 65.62%] [G loss: 0.642613]\n",
      "epoch:24 step:22531 [D loss: 0.526818, acc.: 74.22%] [G loss: 0.551827]\n",
      "epoch:24 step:22532 [D loss: 0.526316, acc.: 75.00%] [G loss: 0.695828]\n",
      "epoch:24 step:22533 [D loss: 0.524117, acc.: 72.66%] [G loss: 0.551388]\n",
      "epoch:24 step:22534 [D loss: 0.500311, acc.: 75.78%] [G loss: 0.673105]\n",
      "epoch:24 step:22535 [D loss: 0.539213, acc.: 69.53%] [G loss: 0.773784]\n",
      "epoch:24 step:22536 [D loss: 0.457174, acc.: 82.03%] [G loss: 0.891019]\n",
      "epoch:24 step:22537 [D loss: 0.500478, acc.: 76.56%] [G loss: 0.744647]\n",
      "epoch:24 step:22538 [D loss: 0.545500, acc.: 71.88%] [G loss: 0.675251]\n",
      "epoch:24 step:22539 [D loss: 0.603472, acc.: 67.19%] [G loss: 0.562296]\n",
      "epoch:24 step:22540 [D loss: 0.607605, acc.: 67.19%] [G loss: 0.600968]\n",
      "epoch:24 step:22541 [D loss: 0.545236, acc.: 67.19%] [G loss: 0.674866]\n",
      "epoch:24 step:22542 [D loss: 0.512731, acc.: 75.78%] [G loss: 0.754102]\n",
      "epoch:24 step:22543 [D loss: 0.532364, acc.: 74.22%] [G loss: 0.659788]\n",
      "epoch:24 step:22544 [D loss: 0.515115, acc.: 70.31%] [G loss: 0.698248]\n",
      "epoch:24 step:22545 [D loss: 0.491585, acc.: 70.31%] [G loss: 0.745177]\n",
      "epoch:24 step:22546 [D loss: 0.567353, acc.: 67.97%] [G loss: 0.701559]\n",
      "epoch:24 step:22547 [D loss: 0.509823, acc.: 75.78%] [G loss: 0.820208]\n",
      "epoch:24 step:22548 [D loss: 0.585645, acc.: 65.62%] [G loss: 0.641747]\n",
      "epoch:24 step:22549 [D loss: 0.541294, acc.: 69.53%] [G loss: 0.767889]\n",
      "epoch:24 step:22550 [D loss: 0.573118, acc.: 70.31%] [G loss: 0.632379]\n",
      "epoch:24 step:22551 [D loss: 0.591410, acc.: 64.84%] [G loss: 0.664509]\n",
      "epoch:24 step:22552 [D loss: 0.562760, acc.: 70.31%] [G loss: 0.761517]\n",
      "epoch:24 step:22553 [D loss: 0.532078, acc.: 73.44%] [G loss: 0.562974]\n",
      "epoch:24 step:22554 [D loss: 0.556928, acc.: 72.66%] [G loss: 0.686685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22555 [D loss: 0.524564, acc.: 72.66%] [G loss: 0.785337]\n",
      "epoch:24 step:22556 [D loss: 0.540589, acc.: 68.75%] [G loss: 0.678425]\n",
      "epoch:24 step:22557 [D loss: 0.473140, acc.: 78.91%] [G loss: 0.758955]\n",
      "epoch:24 step:22558 [D loss: 0.541551, acc.: 71.88%] [G loss: 0.718316]\n",
      "epoch:24 step:22559 [D loss: 0.557896, acc.: 71.88%] [G loss: 0.753020]\n",
      "epoch:24 step:22560 [D loss: 0.559582, acc.: 71.09%] [G loss: 0.677040]\n",
      "epoch:24 step:22561 [D loss: 0.593814, acc.: 64.06%] [G loss: 0.486281]\n",
      "epoch:24 step:22562 [D loss: 0.476576, acc.: 76.56%] [G loss: 0.773939]\n",
      "epoch:24 step:22563 [D loss: 0.523672, acc.: 75.78%] [G loss: 0.713439]\n",
      "epoch:24 step:22564 [D loss: 0.464162, acc.: 77.34%] [G loss: 0.760644]\n",
      "epoch:24 step:22565 [D loss: 0.463665, acc.: 75.00%] [G loss: 0.854949]\n",
      "epoch:24 step:22566 [D loss: 0.583163, acc.: 67.19%] [G loss: 0.674980]\n",
      "epoch:24 step:22567 [D loss: 0.560952, acc.: 70.31%] [G loss: 0.609382]\n",
      "epoch:24 step:22568 [D loss: 0.465217, acc.: 77.34%] [G loss: 0.618988]\n",
      "epoch:24 step:22569 [D loss: 0.577325, acc.: 67.97%] [G loss: 0.585334]\n",
      "epoch:24 step:22570 [D loss: 0.536626, acc.: 64.84%] [G loss: 0.862555]\n",
      "epoch:24 step:22571 [D loss: 0.488691, acc.: 79.69%] [G loss: 0.764946]\n",
      "epoch:24 step:22572 [D loss: 0.551788, acc.: 67.97%] [G loss: 0.662698]\n",
      "epoch:24 step:22573 [D loss: 0.563993, acc.: 71.88%] [G loss: 0.653825]\n",
      "epoch:24 step:22574 [D loss: 0.541698, acc.: 73.44%] [G loss: 0.769476]\n",
      "epoch:24 step:22575 [D loss: 0.516216, acc.: 72.66%] [G loss: 0.656432]\n",
      "epoch:24 step:22576 [D loss: 0.495491, acc.: 75.00%] [G loss: 0.556735]\n",
      "epoch:24 step:22577 [D loss: 0.547991, acc.: 71.09%] [G loss: 0.692507]\n",
      "epoch:24 step:22578 [D loss: 0.533279, acc.: 70.31%] [G loss: 0.718993]\n",
      "epoch:24 step:22579 [D loss: 0.544959, acc.: 70.31%] [G loss: 0.698605]\n",
      "epoch:24 step:22580 [D loss: 0.475675, acc.: 77.34%] [G loss: 0.665924]\n",
      "epoch:24 step:22581 [D loss: 0.521535, acc.: 71.09%] [G loss: 0.686318]\n",
      "epoch:24 step:22582 [D loss: 0.472174, acc.: 78.91%] [G loss: 0.885462]\n",
      "epoch:24 step:22583 [D loss: 0.503685, acc.: 72.66%] [G loss: 0.859589]\n",
      "epoch:24 step:22584 [D loss: 0.477682, acc.: 77.34%] [G loss: 0.659955]\n",
      "epoch:24 step:22585 [D loss: 0.523343, acc.: 72.66%] [G loss: 0.704330]\n",
      "epoch:24 step:22586 [D loss: 0.552423, acc.: 71.88%] [G loss: 0.815175]\n",
      "epoch:24 step:22587 [D loss: 0.504145, acc.: 75.00%] [G loss: 0.647110]\n",
      "epoch:24 step:22588 [D loss: 0.467985, acc.: 78.12%] [G loss: 0.945631]\n",
      "epoch:24 step:22589 [D loss: 0.480394, acc.: 75.78%] [G loss: 0.787878]\n",
      "epoch:24 step:22590 [D loss: 0.621436, acc.: 59.38%] [G loss: 0.698365]\n",
      "epoch:24 step:22591 [D loss: 0.550782, acc.: 69.53%] [G loss: 0.737768]\n",
      "epoch:24 step:22592 [D loss: 0.506301, acc.: 73.44%] [G loss: 0.694323]\n",
      "epoch:24 step:22593 [D loss: 0.621169, acc.: 63.28%] [G loss: 0.577004]\n",
      "epoch:24 step:22594 [D loss: 0.530136, acc.: 67.19%] [G loss: 0.596284]\n",
      "epoch:24 step:22595 [D loss: 0.555436, acc.: 68.75%] [G loss: 0.641748]\n",
      "epoch:24 step:22596 [D loss: 0.688882, acc.: 60.94%] [G loss: 0.611297]\n",
      "epoch:24 step:22597 [D loss: 0.512226, acc.: 72.66%] [G loss: 0.594659]\n",
      "epoch:24 step:22598 [D loss: 0.565239, acc.: 66.41%] [G loss: 0.782035]\n",
      "epoch:24 step:22599 [D loss: 0.521899, acc.: 68.75%] [G loss: 0.728191]\n",
      "epoch:24 step:22600 [D loss: 0.521936, acc.: 71.09%] [G loss: 0.782142]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.920461\n",
      "FID: 39.808578\n",
      "0 = 13.005439872360252\n",
      "1 = 0.10344362453987571\n",
      "2 = 0.8626999855041504\n",
      "3 = 0.8271999955177307\n",
      "4 = 0.8981999754905701\n",
      "5 = 0.8904197812080383\n",
      "6 = 0.8271999955177307\n",
      "7 = 7.87614857076407\n",
      "8 = 0.13537680210312422\n",
      "9 = 0.6869000196456909\n",
      "10 = 0.6836000084877014\n",
      "11 = 0.6901999711990356\n",
      "12 = 0.6881417632102966\n",
      "13 = 0.6836000084877014\n",
      "14 = 6.920485496520996\n",
      "15 = 7.339848518371582\n",
      "16 = 0.36135628819465637\n",
      "17 = 6.9204607009887695\n",
      "18 = 39.80857849121094\n",
      "epoch:24 step:22601 [D loss: 0.496602, acc.: 75.00%] [G loss: 0.648167]\n",
      "epoch:24 step:22602 [D loss: 0.588552, acc.: 67.19%] [G loss: 0.683978]\n",
      "epoch:24 step:22603 [D loss: 0.518076, acc.: 78.12%] [G loss: 0.742344]\n",
      "epoch:24 step:22604 [D loss: 0.502397, acc.: 72.66%] [G loss: 0.768004]\n",
      "epoch:24 step:22605 [D loss: 0.518999, acc.: 72.66%] [G loss: 0.761380]\n",
      "epoch:24 step:22606 [D loss: 0.574187, acc.: 69.53%] [G loss: 0.780698]\n",
      "epoch:24 step:22607 [D loss: 0.473268, acc.: 79.69%] [G loss: 0.956014]\n",
      "epoch:24 step:22608 [D loss: 0.628356, acc.: 64.84%] [G loss: 0.853840]\n",
      "epoch:24 step:22609 [D loss: 0.581227, acc.: 65.62%] [G loss: 0.877944]\n",
      "epoch:24 step:22610 [D loss: 0.532787, acc.: 71.88%] [G loss: 0.891820]\n",
      "epoch:24 step:22611 [D loss: 0.495258, acc.: 75.78%] [G loss: 0.709987]\n",
      "epoch:24 step:22612 [D loss: 0.589736, acc.: 67.97%] [G loss: 0.759022]\n",
      "epoch:24 step:22613 [D loss: 0.636324, acc.: 63.28%] [G loss: 0.788547]\n",
      "epoch:24 step:22614 [D loss: 0.505703, acc.: 73.44%] [G loss: 0.651701]\n",
      "epoch:24 step:22615 [D loss: 0.478541, acc.: 72.66%] [G loss: 0.682851]\n",
      "epoch:24 step:22616 [D loss: 0.511059, acc.: 75.00%] [G loss: 0.637734]\n",
      "epoch:24 step:22617 [D loss: 0.568445, acc.: 67.97%] [G loss: 0.673784]\n",
      "epoch:24 step:22618 [D loss: 0.517379, acc.: 73.44%] [G loss: 0.441116]\n",
      "epoch:24 step:22619 [D loss: 0.526975, acc.: 69.53%] [G loss: 0.619849]\n",
      "epoch:24 step:22620 [D loss: 0.595488, acc.: 67.97%] [G loss: 0.672102]\n",
      "epoch:24 step:22621 [D loss: 0.562774, acc.: 66.41%] [G loss: 0.903020]\n",
      "epoch:24 step:22622 [D loss: 0.469901, acc.: 75.00%] [G loss: 0.941447]\n",
      "epoch:24 step:22623 [D loss: 0.556261, acc.: 76.56%] [G loss: 0.642165]\n",
      "epoch:24 step:22624 [D loss: 0.492455, acc.: 75.00%] [G loss: 0.710434]\n",
      "epoch:24 step:22625 [D loss: 0.632331, acc.: 67.19%] [G loss: 0.691661]\n",
      "epoch:24 step:22626 [D loss: 0.568772, acc.: 67.19%] [G loss: 0.624031]\n",
      "epoch:24 step:22627 [D loss: 0.535167, acc.: 73.44%] [G loss: 0.650057]\n",
      "epoch:24 step:22628 [D loss: 0.589630, acc.: 68.75%] [G loss: 0.834289]\n",
      "epoch:24 step:22629 [D loss: 0.551621, acc.: 65.62%] [G loss: 0.663593]\n",
      "epoch:24 step:22630 [D loss: 0.535028, acc.: 67.97%] [G loss: 0.706852]\n",
      "epoch:24 step:22631 [D loss: 0.563633, acc.: 65.62%] [G loss: 0.626718]\n",
      "epoch:24 step:22632 [D loss: 0.494885, acc.: 75.00%] [G loss: 0.754756]\n",
      "epoch:24 step:22633 [D loss: 0.557292, acc.: 67.19%] [G loss: 0.750735]\n",
      "epoch:24 step:22634 [D loss: 0.494461, acc.: 77.34%] [G loss: 0.747205]\n",
      "epoch:24 step:22635 [D loss: 0.636843, acc.: 62.50%] [G loss: 0.635567]\n",
      "epoch:24 step:22636 [D loss: 0.540470, acc.: 66.41%] [G loss: 0.716177]\n",
      "epoch:24 step:22637 [D loss: 0.502426, acc.: 71.88%] [G loss: 0.622437]\n",
      "epoch:24 step:22638 [D loss: 0.580868, acc.: 63.28%] [G loss: 0.544706]\n",
      "epoch:24 step:22639 [D loss: 0.563497, acc.: 65.62%] [G loss: 0.755553]\n",
      "epoch:24 step:22640 [D loss: 0.515249, acc.: 75.00%] [G loss: 0.721010]\n",
      "epoch:24 step:22641 [D loss: 0.604174, acc.: 62.50%] [G loss: 0.689058]\n",
      "epoch:24 step:22642 [D loss: 0.575194, acc.: 67.19%] [G loss: 0.745010]\n",
      "epoch:24 step:22643 [D loss: 0.503753, acc.: 76.56%] [G loss: 0.700571]\n",
      "epoch:24 step:22644 [D loss: 0.485013, acc.: 77.34%] [G loss: 0.842358]\n",
      "epoch:24 step:22645 [D loss: 0.546984, acc.: 70.31%] [G loss: 0.811324]\n",
      "epoch:24 step:22646 [D loss: 0.566506, acc.: 67.19%] [G loss: 0.665676]\n",
      "epoch:24 step:22647 [D loss: 0.464741, acc.: 73.44%] [G loss: 0.869622]\n",
      "epoch:24 step:22648 [D loss: 0.622180, acc.: 65.62%] [G loss: 0.750367]\n",
      "epoch:24 step:22649 [D loss: 0.525927, acc.: 73.44%] [G loss: 0.850512]\n",
      "epoch:24 step:22650 [D loss: 0.476638, acc.: 75.00%] [G loss: 0.816406]\n",
      "epoch:24 step:22651 [D loss: 0.605395, acc.: 64.84%] [G loss: 0.780066]\n",
      "epoch:24 step:22652 [D loss: 0.607108, acc.: 63.28%] [G loss: 0.601580]\n",
      "epoch:24 step:22653 [D loss: 0.509208, acc.: 75.00%] [G loss: 0.664121]\n",
      "epoch:24 step:22654 [D loss: 0.610996, acc.: 64.06%] [G loss: 0.698833]\n",
      "epoch:24 step:22655 [D loss: 0.524819, acc.: 69.53%] [G loss: 0.576525]\n",
      "epoch:24 step:22656 [D loss: 0.514649, acc.: 73.44%] [G loss: 0.706959]\n",
      "epoch:24 step:22657 [D loss: 0.558846, acc.: 64.06%] [G loss: 0.593082]\n",
      "epoch:24 step:22658 [D loss: 0.529344, acc.: 70.31%] [G loss: 0.654543]\n",
      "epoch:24 step:22659 [D loss: 0.502298, acc.: 72.66%] [G loss: 0.626348]\n",
      "epoch:24 step:22660 [D loss: 0.496024, acc.: 74.22%] [G loss: 0.769119]\n",
      "epoch:24 step:22661 [D loss: 0.485333, acc.: 72.66%] [G loss: 0.868339]\n",
      "epoch:24 step:22662 [D loss: 0.557370, acc.: 70.31%] [G loss: 0.579719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22663 [D loss: 0.656361, acc.: 60.16%] [G loss: 0.378007]\n",
      "epoch:24 step:22664 [D loss: 0.538741, acc.: 71.09%] [G loss: 0.626260]\n",
      "epoch:24 step:22665 [D loss: 0.537089, acc.: 74.22%] [G loss: 0.658416]\n",
      "epoch:24 step:22666 [D loss: 0.568131, acc.: 64.06%] [G loss: 0.503628]\n",
      "epoch:24 step:22667 [D loss: 0.579936, acc.: 69.53%] [G loss: 0.632452]\n",
      "epoch:24 step:22668 [D loss: 0.559613, acc.: 68.75%] [G loss: 0.554312]\n",
      "epoch:24 step:22669 [D loss: 0.621544, acc.: 61.72%] [G loss: 0.453006]\n",
      "epoch:24 step:22670 [D loss: 0.490591, acc.: 75.00%] [G loss: 0.656403]\n",
      "epoch:24 step:22671 [D loss: 0.570536, acc.: 70.31%] [G loss: 0.692776]\n",
      "epoch:24 step:22672 [D loss: 0.486773, acc.: 72.66%] [G loss: 0.630595]\n",
      "epoch:24 step:22673 [D loss: 0.632043, acc.: 63.28%] [G loss: 0.918581]\n",
      "epoch:24 step:22674 [D loss: 0.523033, acc.: 71.88%] [G loss: 0.840762]\n",
      "epoch:24 step:22675 [D loss: 0.609181, acc.: 63.28%] [G loss: 0.553959]\n",
      "epoch:24 step:22676 [D loss: 0.523357, acc.: 71.88%] [G loss: 0.574245]\n",
      "epoch:24 step:22677 [D loss: 0.564219, acc.: 72.66%] [G loss: 0.583980]\n",
      "epoch:24 step:22678 [D loss: 0.476715, acc.: 78.12%] [G loss: 0.743715]\n",
      "epoch:24 step:22679 [D loss: 0.541614, acc.: 67.19%] [G loss: 0.721773]\n",
      "epoch:24 step:22680 [D loss: 0.522258, acc.: 71.09%] [G loss: 0.648534]\n",
      "epoch:24 step:22681 [D loss: 0.525520, acc.: 68.75%] [G loss: 0.697795]\n",
      "epoch:24 step:22682 [D loss: 0.471659, acc.: 78.91%] [G loss: 0.834776]\n",
      "epoch:24 step:22683 [D loss: 0.649934, acc.: 67.97%] [G loss: 0.775609]\n",
      "epoch:24 step:22684 [D loss: 0.534128, acc.: 71.88%] [G loss: 0.660102]\n",
      "epoch:24 step:22685 [D loss: 0.527667, acc.: 71.88%] [G loss: 0.762513]\n",
      "epoch:24 step:22686 [D loss: 0.448548, acc.: 79.69%] [G loss: 0.845619]\n",
      "epoch:24 step:22687 [D loss: 0.569059, acc.: 71.88%] [G loss: 0.765623]\n",
      "epoch:24 step:22688 [D loss: 0.603158, acc.: 66.41%] [G loss: 0.573877]\n",
      "epoch:24 step:22689 [D loss: 0.584231, acc.: 69.53%] [G loss: 0.656045]\n",
      "epoch:24 step:22690 [D loss: 0.478779, acc.: 74.22%] [G loss: 0.715816]\n",
      "epoch:24 step:22691 [D loss: 0.616586, acc.: 65.62%] [G loss: 0.582553]\n",
      "epoch:24 step:22692 [D loss: 0.540549, acc.: 70.31%] [G loss: 0.672665]\n",
      "epoch:24 step:22693 [D loss: 0.480075, acc.: 75.78%] [G loss: 0.923561]\n",
      "epoch:24 step:22694 [D loss: 0.511291, acc.: 73.44%] [G loss: 1.021327]\n",
      "epoch:24 step:22695 [D loss: 0.467308, acc.: 76.56%] [G loss: 0.819056]\n",
      "epoch:24 step:22696 [D loss: 0.431307, acc.: 79.69%] [G loss: 0.728907]\n",
      "epoch:24 step:22697 [D loss: 0.555224, acc.: 69.53%] [G loss: 0.937145]\n",
      "epoch:24 step:22698 [D loss: 0.631896, acc.: 66.41%] [G loss: 0.623283]\n",
      "epoch:24 step:22699 [D loss: 0.562956, acc.: 73.44%] [G loss: 0.708904]\n",
      "epoch:24 step:22700 [D loss: 0.524900, acc.: 76.56%] [G loss: 0.538246]\n",
      "epoch:24 step:22701 [D loss: 0.536055, acc.: 70.31%] [G loss: 0.650252]\n",
      "epoch:24 step:22702 [D loss: 0.658763, acc.: 59.38%] [G loss: 0.516904]\n",
      "epoch:24 step:22703 [D loss: 0.621630, acc.: 60.16%] [G loss: 0.491168]\n",
      "epoch:24 step:22704 [D loss: 0.488443, acc.: 75.00%] [G loss: 0.736914]\n",
      "epoch:24 step:22705 [D loss: 0.521923, acc.: 71.09%] [G loss: 0.554922]\n",
      "epoch:24 step:22706 [D loss: 0.537317, acc.: 70.31%] [G loss: 0.808624]\n",
      "epoch:24 step:22707 [D loss: 0.487489, acc.: 75.78%] [G loss: 0.792401]\n",
      "epoch:24 step:22708 [D loss: 0.655909, acc.: 65.62%] [G loss: 0.749610]\n",
      "epoch:24 step:22709 [D loss: 0.547051, acc.: 69.53%] [G loss: 0.643217]\n",
      "epoch:24 step:22710 [D loss: 0.484531, acc.: 76.56%] [G loss: 0.767915]\n",
      "epoch:24 step:22711 [D loss: 0.558203, acc.: 68.75%] [G loss: 0.757915]\n",
      "epoch:24 step:22712 [D loss: 0.537464, acc.: 71.09%] [G loss: 0.598041]\n",
      "epoch:24 step:22713 [D loss: 0.488410, acc.: 77.34%] [G loss: 0.601825]\n",
      "epoch:24 step:22714 [D loss: 0.565031, acc.: 64.84%] [G loss: 0.712530]\n",
      "epoch:24 step:22715 [D loss: 0.538774, acc.: 71.09%] [G loss: 0.732947]\n",
      "epoch:24 step:22716 [D loss: 0.580402, acc.: 67.97%] [G loss: 0.757223]\n",
      "epoch:24 step:22717 [D loss: 0.534268, acc.: 77.34%] [G loss: 0.616645]\n",
      "epoch:24 step:22718 [D loss: 0.482018, acc.: 76.56%] [G loss: 0.741537]\n",
      "epoch:24 step:22719 [D loss: 0.532192, acc.: 72.66%] [G loss: 1.046817]\n",
      "epoch:24 step:22720 [D loss: 0.455926, acc.: 78.91%] [G loss: 1.185261]\n",
      "epoch:24 step:22721 [D loss: 0.557796, acc.: 71.88%] [G loss: 0.792115]\n",
      "epoch:24 step:22722 [D loss: 0.584438, acc.: 71.09%] [G loss: 0.723026]\n",
      "epoch:24 step:22723 [D loss: 0.556829, acc.: 70.31%] [G loss: 0.594731]\n",
      "epoch:24 step:22724 [D loss: 0.580404, acc.: 66.41%] [G loss: 0.642274]\n",
      "epoch:24 step:22725 [D loss: 0.505253, acc.: 76.56%] [G loss: 0.631089]\n",
      "epoch:24 step:22726 [D loss: 0.580577, acc.: 62.50%] [G loss: 0.571284]\n",
      "epoch:24 step:22727 [D loss: 0.532322, acc.: 70.31%] [G loss: 0.595343]\n",
      "epoch:24 step:22728 [D loss: 0.545690, acc.: 68.75%] [G loss: 0.631459]\n",
      "epoch:24 step:22729 [D loss: 0.504410, acc.: 76.56%] [G loss: 0.701898]\n",
      "epoch:24 step:22730 [D loss: 0.499317, acc.: 75.78%] [G loss: 0.750632]\n",
      "epoch:24 step:22731 [D loss: 0.591978, acc.: 62.50%] [G loss: 0.755561]\n",
      "epoch:24 step:22732 [D loss: 0.465838, acc.: 78.12%] [G loss: 0.643558]\n",
      "epoch:24 step:22733 [D loss: 0.562743, acc.: 68.75%] [G loss: 0.653221]\n",
      "epoch:24 step:22734 [D loss: 0.512388, acc.: 73.44%] [G loss: 0.814350]\n",
      "epoch:24 step:22735 [D loss: 0.510456, acc.: 73.44%] [G loss: 0.765805]\n",
      "epoch:24 step:22736 [D loss: 0.482580, acc.: 71.88%] [G loss: 0.754078]\n",
      "epoch:24 step:22737 [D loss: 0.519526, acc.: 76.56%] [G loss: 0.779369]\n",
      "epoch:24 step:22738 [D loss: 0.614300, acc.: 64.06%] [G loss: 0.698515]\n",
      "epoch:24 step:22739 [D loss: 0.640871, acc.: 61.72%] [G loss: 0.587219]\n",
      "epoch:24 step:22740 [D loss: 0.540057, acc.: 69.53%] [G loss: 0.573062]\n",
      "epoch:24 step:22741 [D loss: 0.566818, acc.: 71.88%] [G loss: 0.618363]\n",
      "epoch:24 step:22742 [D loss: 0.504656, acc.: 71.09%] [G loss: 0.735277]\n",
      "epoch:24 step:22743 [D loss: 0.539079, acc.: 70.31%] [G loss: 0.673176]\n",
      "epoch:24 step:22744 [D loss: 0.581357, acc.: 67.97%] [G loss: 0.534615]\n",
      "epoch:24 step:22745 [D loss: 0.581185, acc.: 69.53%] [G loss: 0.926310]\n",
      "epoch:24 step:22746 [D loss: 0.589320, acc.: 62.50%] [G loss: 0.630841]\n",
      "epoch:24 step:22747 [D loss: 0.519114, acc.: 69.53%] [G loss: 0.743564]\n",
      "epoch:24 step:22748 [D loss: 0.595271, acc.: 65.62%] [G loss: 0.563218]\n",
      "epoch:24 step:22749 [D loss: 0.556067, acc.: 68.75%] [G loss: 0.645031]\n",
      "epoch:24 step:22750 [D loss: 0.534812, acc.: 70.31%] [G loss: 0.611035]\n",
      "epoch:24 step:22751 [D loss: 0.609560, acc.: 67.19%] [G loss: 0.682294]\n",
      "epoch:24 step:22752 [D loss: 0.501209, acc.: 74.22%] [G loss: 0.711855]\n",
      "epoch:24 step:22753 [D loss: 0.526697, acc.: 69.53%] [G loss: 0.595847]\n",
      "epoch:24 step:22754 [D loss: 0.611917, acc.: 64.84%] [G loss: 0.684160]\n",
      "epoch:24 step:22755 [D loss: 0.541667, acc.: 70.31%] [G loss: 0.648459]\n",
      "epoch:24 step:22756 [D loss: 0.583207, acc.: 65.62%] [G loss: 0.557919]\n",
      "epoch:24 step:22757 [D loss: 0.548393, acc.: 73.44%] [G loss: 0.728276]\n",
      "epoch:24 step:22758 [D loss: 0.489694, acc.: 77.34%] [G loss: 0.797605]\n",
      "epoch:24 step:22759 [D loss: 0.523964, acc.: 74.22%] [G loss: 0.920442]\n",
      "epoch:24 step:22760 [D loss: 0.564208, acc.: 69.53%] [G loss: 0.714900]\n",
      "epoch:24 step:22761 [D loss: 0.497012, acc.: 71.88%] [G loss: 0.689836]\n",
      "epoch:24 step:22762 [D loss: 0.522511, acc.: 72.66%] [G loss: 0.779026]\n",
      "epoch:24 step:22763 [D loss: 0.587125, acc.: 69.53%] [G loss: 0.590351]\n",
      "epoch:24 step:22764 [D loss: 0.474122, acc.: 79.69%] [G loss: 0.642034]\n",
      "epoch:24 step:22765 [D loss: 0.671699, acc.: 61.72%] [G loss: 0.603090]\n",
      "epoch:24 step:22766 [D loss: 0.704870, acc.: 57.81%] [G loss: 0.526158]\n",
      "epoch:24 step:22767 [D loss: 0.550861, acc.: 67.97%] [G loss: 0.501366]\n",
      "epoch:24 step:22768 [D loss: 0.607626, acc.: 67.19%] [G loss: 0.685944]\n",
      "epoch:24 step:22769 [D loss: 0.588390, acc.: 65.62%] [G loss: 0.670966]\n",
      "epoch:24 step:22770 [D loss: 0.605037, acc.: 70.31%] [G loss: 0.501665]\n",
      "epoch:24 step:22771 [D loss: 0.514646, acc.: 75.78%] [G loss: 0.591834]\n",
      "epoch:24 step:22772 [D loss: 0.567155, acc.: 67.97%] [G loss: 0.545662]\n",
      "epoch:24 step:22773 [D loss: 0.510898, acc.: 69.53%] [G loss: 0.724913]\n",
      "epoch:24 step:22774 [D loss: 0.508458, acc.: 70.31%] [G loss: 0.885811]\n",
      "epoch:24 step:22775 [D loss: 0.595942, acc.: 70.31%] [G loss: 0.599578]\n",
      "epoch:24 step:22776 [D loss: 0.557913, acc.: 64.84%] [G loss: 0.604717]\n",
      "epoch:24 step:22777 [D loss: 0.540745, acc.: 69.53%] [G loss: 0.714885]\n",
      "epoch:24 step:22778 [D loss: 0.574684, acc.: 65.62%] [G loss: 0.603893]\n",
      "epoch:24 step:22779 [D loss: 0.615842, acc.: 64.84%] [G loss: 0.589511]\n",
      "epoch:24 step:22780 [D loss: 0.530837, acc.: 71.09%] [G loss: 0.476861]\n",
      "epoch:24 step:22781 [D loss: 0.555156, acc.: 67.19%] [G loss: 0.608010]\n",
      "epoch:24 step:22782 [D loss: 0.546052, acc.: 71.09%] [G loss: 0.694398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22783 [D loss: 0.585522, acc.: 64.06%] [G loss: 0.593094]\n",
      "epoch:24 step:22784 [D loss: 0.432709, acc.: 79.69%] [G loss: 0.569645]\n",
      "epoch:24 step:22785 [D loss: 0.509099, acc.: 78.12%] [G loss: 0.595698]\n",
      "epoch:24 step:22786 [D loss: 0.431021, acc.: 86.72%] [G loss: 0.713432]\n",
      "epoch:24 step:22787 [D loss: 0.553453, acc.: 69.53%] [G loss: 0.798571]\n",
      "epoch:24 step:22788 [D loss: 0.502820, acc.: 73.44%] [G loss: 0.739513]\n",
      "epoch:24 step:22789 [D loss: 0.636042, acc.: 65.62%] [G loss: 0.765157]\n",
      "epoch:24 step:22790 [D loss: 0.522914, acc.: 72.66%] [G loss: 0.811418]\n",
      "epoch:24 step:22791 [D loss: 0.577849, acc.: 66.41%] [G loss: 0.745037]\n",
      "epoch:24 step:22792 [D loss: 0.475187, acc.: 75.78%] [G loss: 0.609951]\n",
      "epoch:24 step:22793 [D loss: 0.528123, acc.: 72.66%] [G loss: 0.575698]\n",
      "epoch:24 step:22794 [D loss: 0.542163, acc.: 70.31%] [G loss: 0.701721]\n",
      "epoch:24 step:22795 [D loss: 0.473670, acc.: 78.91%] [G loss: 0.837718]\n",
      "epoch:24 step:22796 [D loss: 0.614186, acc.: 64.06%] [G loss: 0.691649]\n",
      "epoch:24 step:22797 [D loss: 0.541138, acc.: 71.09%] [G loss: 0.613182]\n",
      "epoch:24 step:22798 [D loss: 0.545429, acc.: 70.31%] [G loss: 0.741152]\n",
      "epoch:24 step:22799 [D loss: 0.523955, acc.: 70.31%] [G loss: 0.684012]\n",
      "epoch:24 step:22800 [D loss: 0.502643, acc.: 75.00%] [G loss: 0.881458]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.654173\n",
      "FID: 52.281345\n",
      "0 = 12.902486517047842\n",
      "1 = 0.09286462418317654\n",
      "2 = 0.8510000109672546\n",
      "3 = 0.805400013923645\n",
      "4 = 0.8966000080108643\n",
      "5 = 0.8862236142158508\n",
      "6 = 0.805400013923645\n",
      "7 = 8.438557358086113\n",
      "8 = 0.15840870251302594\n",
      "9 = 0.6897000074386597\n",
      "10 = 0.6741999983787537\n",
      "11 = 0.7052000164985657\n",
      "12 = 0.6957688331604004\n",
      "13 = 0.6741999983787537\n",
      "14 = 6.654202938079834\n",
      "15 = 6.8857421875\n",
      "16 = 0.3973054885864258\n",
      "17 = 6.654172897338867\n",
      "18 = 52.28134536743164\n",
      "epoch:24 step:22801 [D loss: 0.457996, acc.: 78.91%] [G loss: 0.812688]\n",
      "epoch:24 step:22802 [D loss: 0.408832, acc.: 85.16%] [G loss: 0.948499]\n",
      "epoch:24 step:22803 [D loss: 0.392000, acc.: 87.50%] [G loss: 0.762796]\n",
      "epoch:24 step:22804 [D loss: 0.716537, acc.: 59.38%] [G loss: 0.697242]\n",
      "epoch:24 step:22805 [D loss: 0.550795, acc.: 72.66%] [G loss: 0.757173]\n",
      "epoch:24 step:22806 [D loss: 0.573484, acc.: 67.19%] [G loss: 0.665866]\n",
      "epoch:24 step:22807 [D loss: 0.513102, acc.: 72.66%] [G loss: 0.839494]\n",
      "epoch:24 step:22808 [D loss: 0.567555, acc.: 71.88%] [G loss: 0.621136]\n",
      "epoch:24 step:22809 [D loss: 0.512535, acc.: 71.09%] [G loss: 0.728248]\n",
      "epoch:24 step:22810 [D loss: 0.538218, acc.: 71.88%] [G loss: 0.663030]\n",
      "epoch:24 step:22811 [D loss: 0.624085, acc.: 61.72%] [G loss: 0.630911]\n",
      "epoch:24 step:22812 [D loss: 0.547651, acc.: 75.00%] [G loss: 0.668075]\n",
      "epoch:24 step:22813 [D loss: 0.498808, acc.: 75.78%] [G loss: 0.727329]\n",
      "epoch:24 step:22814 [D loss: 0.472144, acc.: 75.00%] [G loss: 0.672475]\n",
      "epoch:24 step:22815 [D loss: 0.539700, acc.: 72.66%] [G loss: 0.899860]\n",
      "epoch:24 step:22816 [D loss: 0.464895, acc.: 75.78%] [G loss: 0.940586]\n",
      "epoch:24 step:22817 [D loss: 0.561046, acc.: 71.88%] [G loss: 0.900858]\n",
      "epoch:24 step:22818 [D loss: 0.548468, acc.: 72.66%] [G loss: 0.767439]\n",
      "epoch:24 step:22819 [D loss: 0.605784, acc.: 64.06%] [G loss: 0.567335]\n",
      "epoch:24 step:22820 [D loss: 0.508671, acc.: 71.88%] [G loss: 0.567917]\n",
      "epoch:24 step:22821 [D loss: 0.494433, acc.: 73.44%] [G loss: 0.675598]\n",
      "epoch:24 step:22822 [D loss: 0.538557, acc.: 70.31%] [G loss: 0.769410]\n",
      "epoch:24 step:22823 [D loss: 0.540824, acc.: 68.75%] [G loss: 0.676923]\n",
      "epoch:24 step:22824 [D loss: 0.531529, acc.: 71.88%] [G loss: 0.651004]\n",
      "epoch:24 step:22825 [D loss: 0.558930, acc.: 70.31%] [G loss: 0.691118]\n",
      "epoch:24 step:22826 [D loss: 0.549925, acc.: 71.88%] [G loss: 0.679099]\n",
      "epoch:24 step:22827 [D loss: 0.565206, acc.: 72.66%] [G loss: 0.686458]\n",
      "epoch:24 step:22828 [D loss: 0.508535, acc.: 74.22%] [G loss: 0.710604]\n",
      "epoch:24 step:22829 [D loss: 0.549390, acc.: 75.00%] [G loss: 0.593707]\n",
      "epoch:24 step:22830 [D loss: 0.658371, acc.: 60.94%] [G loss: 0.528049]\n",
      "epoch:24 step:22831 [D loss: 0.502964, acc.: 75.00%] [G loss: 0.929893]\n",
      "epoch:24 step:22832 [D loss: 0.435472, acc.: 82.81%] [G loss: 0.975869]\n",
      "epoch:24 step:22833 [D loss: 0.579947, acc.: 71.09%] [G loss: 0.718228]\n",
      "epoch:24 step:22834 [D loss: 0.538131, acc.: 69.53%] [G loss: 0.760798]\n",
      "epoch:24 step:22835 [D loss: 0.415411, acc.: 82.03%] [G loss: 0.962574]\n",
      "epoch:24 step:22836 [D loss: 0.597003, acc.: 67.19%] [G loss: 0.829554]\n",
      "epoch:24 step:22837 [D loss: 0.685165, acc.: 61.72%] [G loss: 0.422285]\n",
      "epoch:24 step:22838 [D loss: 0.503445, acc.: 73.44%] [G loss: 0.581063]\n",
      "epoch:24 step:22839 [D loss: 0.537731, acc.: 71.09%] [G loss: 0.692129]\n",
      "epoch:24 step:22840 [D loss: 0.600515, acc.: 64.06%] [G loss: 0.635416]\n",
      "epoch:24 step:22841 [D loss: 0.563394, acc.: 68.75%] [G loss: 0.626541]\n",
      "epoch:24 step:22842 [D loss: 0.436536, acc.: 79.69%] [G loss: 0.948355]\n",
      "epoch:24 step:22843 [D loss: 0.495408, acc.: 80.47%] [G loss: 0.763417]\n",
      "epoch:24 step:22844 [D loss: 0.484744, acc.: 79.69%] [G loss: 0.715536]\n",
      "epoch:24 step:22845 [D loss: 0.475916, acc.: 73.44%] [G loss: 0.886258]\n",
      "epoch:24 step:22846 [D loss: 0.463427, acc.: 75.78%] [G loss: 0.940009]\n",
      "epoch:24 step:22847 [D loss: 0.431574, acc.: 82.03%] [G loss: 0.861676]\n",
      "epoch:24 step:22848 [D loss: 0.500642, acc.: 73.44%] [G loss: 0.851323]\n",
      "epoch:24 step:22849 [D loss: 0.522294, acc.: 68.75%] [G loss: 0.798331]\n",
      "epoch:24 step:22850 [D loss: 0.528416, acc.: 71.09%] [G loss: 0.886690]\n",
      "epoch:24 step:22851 [D loss: 0.638257, acc.: 64.84%] [G loss: 0.631926]\n",
      "epoch:24 step:22852 [D loss: 0.533650, acc.: 71.09%] [G loss: 0.641886]\n",
      "epoch:24 step:22853 [D loss: 0.542726, acc.: 71.88%] [G loss: 0.708261]\n",
      "epoch:24 step:22854 [D loss: 0.555653, acc.: 67.97%] [G loss: 0.814254]\n",
      "epoch:24 step:22855 [D loss: 0.561696, acc.: 68.75%] [G loss: 0.874997]\n",
      "epoch:24 step:22856 [D loss: 0.536886, acc.: 68.75%] [G loss: 0.748075]\n",
      "epoch:24 step:22857 [D loss: 0.547368, acc.: 68.75%] [G loss: 0.531557]\n",
      "epoch:24 step:22858 [D loss: 0.548897, acc.: 75.00%] [G loss: 0.738881]\n",
      "epoch:24 step:22859 [D loss: 0.471350, acc.: 78.91%] [G loss: 0.856817]\n",
      "epoch:24 step:22860 [D loss: 0.475683, acc.: 78.12%] [G loss: 0.587172]\n",
      "epoch:24 step:22861 [D loss: 0.526223, acc.: 67.97%] [G loss: 0.669755]\n",
      "epoch:24 step:22862 [D loss: 0.457288, acc.: 75.78%] [G loss: 0.770332]\n",
      "epoch:24 step:22863 [D loss: 0.528916, acc.: 71.88%] [G loss: 0.651875]\n",
      "epoch:24 step:22864 [D loss: 0.660621, acc.: 66.41%] [G loss: 0.504553]\n",
      "epoch:24 step:22865 [D loss: 0.566631, acc.: 65.62%] [G loss: 0.434614]\n",
      "epoch:24 step:22866 [D loss: 0.563171, acc.: 66.41%] [G loss: 0.553154]\n",
      "epoch:24 step:22867 [D loss: 0.585509, acc.: 68.75%] [G loss: 0.536607]\n",
      "epoch:24 step:22868 [D loss: 0.628926, acc.: 64.84%] [G loss: 0.545707]\n",
      "epoch:24 step:22869 [D loss: 0.462430, acc.: 81.25%] [G loss: 0.756513]\n",
      "epoch:24 step:22870 [D loss: 0.483122, acc.: 75.78%] [G loss: 0.653113]\n",
      "epoch:24 step:22871 [D loss: 0.565511, acc.: 67.97%] [G loss: 0.652809]\n",
      "epoch:24 step:22872 [D loss: 0.552714, acc.: 68.75%] [G loss: 0.662056]\n",
      "epoch:24 step:22873 [D loss: 0.464260, acc.: 80.47%] [G loss: 0.810659]\n",
      "epoch:24 step:22874 [D loss: 0.599339, acc.: 72.66%] [G loss: 0.733902]\n",
      "epoch:24 step:22875 [D loss: 0.586226, acc.: 66.41%] [G loss: 0.601587]\n",
      "epoch:24 step:22876 [D loss: 0.566037, acc.: 68.75%] [G loss: 0.575118]\n",
      "epoch:24 step:22877 [D loss: 0.551248, acc.: 67.97%] [G loss: 0.530881]\n",
      "epoch:24 step:22878 [D loss: 0.552478, acc.: 70.31%] [G loss: 0.604660]\n",
      "epoch:24 step:22879 [D loss: 0.524906, acc.: 72.66%] [G loss: 0.648159]\n",
      "epoch:24 step:22880 [D loss: 0.495857, acc.: 75.00%] [G loss: 0.651774]\n",
      "epoch:24 step:22881 [D loss: 0.582002, acc.: 69.53%] [G loss: 0.716040]\n",
      "epoch:24 step:22882 [D loss: 0.603678, acc.: 64.84%] [G loss: 0.498852]\n",
      "epoch:24 step:22883 [D loss: 0.500056, acc.: 73.44%] [G loss: 0.579044]\n",
      "epoch:24 step:22884 [D loss: 0.526996, acc.: 69.53%] [G loss: 0.690990]\n",
      "epoch:24 step:22885 [D loss: 0.627877, acc.: 62.50%] [G loss: 0.625307]\n",
      "epoch:24 step:22886 [D loss: 0.484861, acc.: 77.34%] [G loss: 0.675436]\n",
      "epoch:24 step:22887 [D loss: 0.490793, acc.: 74.22%] [G loss: 0.747372]\n",
      "epoch:24 step:22888 [D loss: 0.578291, acc.: 61.72%] [G loss: 0.693602]\n",
      "epoch:24 step:22889 [D loss: 0.674341, acc.: 50.78%] [G loss: 0.529431]\n",
      "epoch:24 step:22890 [D loss: 0.489129, acc.: 75.78%] [G loss: 0.662025]\n",
      "epoch:24 step:22891 [D loss: 0.486786, acc.: 76.56%] [G loss: 0.805029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22892 [D loss: 0.596978, acc.: 62.50%] [G loss: 0.574988]\n",
      "epoch:24 step:22893 [D loss: 0.528272, acc.: 70.31%] [G loss: 0.951154]\n",
      "epoch:24 step:22894 [D loss: 0.461646, acc.: 78.12%] [G loss: 0.825546]\n",
      "epoch:24 step:22895 [D loss: 0.631326, acc.: 57.03%] [G loss: 0.687189]\n",
      "epoch:24 step:22896 [D loss: 0.566725, acc.: 67.19%] [G loss: 0.782476]\n",
      "epoch:24 step:22897 [D loss: 0.556320, acc.: 69.53%] [G loss: 0.772537]\n",
      "epoch:24 step:22898 [D loss: 0.598125, acc.: 64.06%] [G loss: 0.595498]\n",
      "epoch:24 step:22899 [D loss: 0.578139, acc.: 68.75%] [G loss: 0.542906]\n",
      "epoch:24 step:22900 [D loss: 0.676083, acc.: 60.16%] [G loss: 0.449978]\n",
      "epoch:24 step:22901 [D loss: 0.564889, acc.: 71.09%] [G loss: 0.731305]\n",
      "epoch:24 step:22902 [D loss: 0.539664, acc.: 67.97%] [G loss: 0.706136]\n",
      "epoch:24 step:22903 [D loss: 0.509982, acc.: 71.88%] [G loss: 0.699927]\n",
      "epoch:24 step:22904 [D loss: 0.557263, acc.: 68.75%] [G loss: 0.551124]\n",
      "epoch:24 step:22905 [D loss: 0.553739, acc.: 69.53%] [G loss: 0.743291]\n",
      "epoch:24 step:22906 [D loss: 0.622202, acc.: 64.84%] [G loss: 0.649552]\n",
      "epoch:24 step:22907 [D loss: 0.519205, acc.: 71.09%] [G loss: 0.614862]\n",
      "epoch:24 step:22908 [D loss: 0.622174, acc.: 62.50%] [G loss: 0.609189]\n",
      "epoch:24 step:22909 [D loss: 0.581605, acc.: 68.75%] [G loss: 0.560522]\n",
      "epoch:24 step:22910 [D loss: 0.645546, acc.: 67.19%] [G loss: 0.557180]\n",
      "epoch:24 step:22911 [D loss: 0.533022, acc.: 72.66%] [G loss: 0.548435]\n",
      "epoch:24 step:22912 [D loss: 0.564327, acc.: 69.53%] [G loss: 0.570792]\n",
      "epoch:24 step:22913 [D loss: 0.501933, acc.: 75.78%] [G loss: 0.790121]\n",
      "epoch:24 step:22914 [D loss: 0.483820, acc.: 73.44%] [G loss: 0.760727]\n",
      "epoch:24 step:22915 [D loss: 0.473091, acc.: 78.91%] [G loss: 0.670480]\n",
      "epoch:24 step:22916 [D loss: 0.500805, acc.: 73.44%] [G loss: 0.807550]\n",
      "epoch:24 step:22917 [D loss: 0.546671, acc.: 68.75%] [G loss: 0.765326]\n",
      "epoch:24 step:22918 [D loss: 0.477758, acc.: 76.56%] [G loss: 0.825242]\n",
      "epoch:24 step:22919 [D loss: 0.526667, acc.: 71.09%] [G loss: 0.695863]\n",
      "epoch:24 step:22920 [D loss: 0.571647, acc.: 69.53%] [G loss: 0.690440]\n",
      "epoch:24 step:22921 [D loss: 0.538013, acc.: 66.41%] [G loss: 0.646293]\n",
      "epoch:24 step:22922 [D loss: 0.539538, acc.: 71.09%] [G loss: 0.886970]\n",
      "epoch:24 step:22923 [D loss: 0.453862, acc.: 80.47%] [G loss: 0.961499]\n",
      "epoch:24 step:22924 [D loss: 0.497180, acc.: 75.00%] [G loss: 0.835023]\n",
      "epoch:24 step:22925 [D loss: 0.683512, acc.: 60.16%] [G loss: 0.624818]\n",
      "epoch:24 step:22926 [D loss: 0.597401, acc.: 64.84%] [G loss: 0.387999]\n",
      "epoch:24 step:22927 [D loss: 0.496219, acc.: 77.34%] [G loss: 0.760493]\n",
      "epoch:24 step:22928 [D loss: 0.521129, acc.: 72.66%] [G loss: 0.746483]\n",
      "epoch:24 step:22929 [D loss: 0.551511, acc.: 69.53%] [G loss: 1.002091]\n",
      "epoch:24 step:22930 [D loss: 0.560913, acc.: 71.88%] [G loss: 1.040884]\n",
      "epoch:24 step:22931 [D loss: 0.565825, acc.: 71.88%] [G loss: 0.827190]\n",
      "epoch:24 step:22932 [D loss: 0.448214, acc.: 77.34%] [G loss: 0.856296]\n",
      "epoch:24 step:22933 [D loss: 0.591010, acc.: 66.41%] [G loss: 0.735886]\n",
      "epoch:24 step:22934 [D loss: 0.478528, acc.: 76.56%] [G loss: 0.782726]\n",
      "epoch:24 step:22935 [D loss: 0.520653, acc.: 70.31%] [G loss: 0.791431]\n",
      "epoch:24 step:22936 [D loss: 0.486345, acc.: 75.78%] [G loss: 0.823450]\n",
      "epoch:24 step:22937 [D loss: 0.511987, acc.: 74.22%] [G loss: 0.852579]\n",
      "epoch:24 step:22938 [D loss: 0.492986, acc.: 77.34%] [G loss: 0.928002]\n",
      "epoch:24 step:22939 [D loss: 0.440925, acc.: 82.03%] [G loss: 0.818119]\n",
      "epoch:24 step:22940 [D loss: 0.455796, acc.: 77.34%] [G loss: 0.931691]\n",
      "epoch:24 step:22941 [D loss: 0.510047, acc.: 72.66%] [G loss: 0.781111]\n",
      "epoch:24 step:22942 [D loss: 0.514717, acc.: 75.00%] [G loss: 0.757635]\n",
      "epoch:24 step:22943 [D loss: 0.576865, acc.: 68.75%] [G loss: 0.639293]\n",
      "epoch:24 step:22944 [D loss: 0.632212, acc.: 63.28%] [G loss: 0.775391]\n",
      "epoch:24 step:22945 [D loss: 0.502652, acc.: 75.00%] [G loss: 0.763454]\n",
      "epoch:24 step:22946 [D loss: 0.609376, acc.: 68.75%] [G loss: 0.806238]\n",
      "epoch:24 step:22947 [D loss: 0.562391, acc.: 66.41%] [G loss: 0.724443]\n",
      "epoch:24 step:22948 [D loss: 0.551555, acc.: 68.75%] [G loss: 0.725417]\n",
      "epoch:24 step:22949 [D loss: 0.520148, acc.: 73.44%] [G loss: 0.666575]\n",
      "epoch:24 step:22950 [D loss: 0.579923, acc.: 67.97%] [G loss: 0.686883]\n",
      "epoch:24 step:22951 [D loss: 0.532504, acc.: 71.88%] [G loss: 0.587231]\n",
      "epoch:24 step:22952 [D loss: 0.514407, acc.: 76.56%] [G loss: 0.609483]\n",
      "epoch:24 step:22953 [D loss: 0.603113, acc.: 67.97%] [G loss: 0.560856]\n",
      "epoch:24 step:22954 [D loss: 0.591741, acc.: 65.62%] [G loss: 0.538037]\n",
      "epoch:24 step:22955 [D loss: 0.526554, acc.: 71.88%] [G loss: 0.727179]\n",
      "epoch:24 step:22956 [D loss: 0.569802, acc.: 67.19%] [G loss: 0.761076]\n",
      "epoch:24 step:22957 [D loss: 0.539251, acc.: 70.31%] [G loss: 0.564910]\n",
      "epoch:24 step:22958 [D loss: 0.523473, acc.: 74.22%] [G loss: 0.863248]\n",
      "epoch:24 step:22959 [D loss: 0.480851, acc.: 81.25%] [G loss: 0.793034]\n",
      "epoch:24 step:22960 [D loss: 0.447965, acc.: 79.69%] [G loss: 0.777878]\n",
      "epoch:24 step:22961 [D loss: 0.627241, acc.: 64.84%] [G loss: 0.803762]\n",
      "epoch:24 step:22962 [D loss: 0.548529, acc.: 71.88%] [G loss: 0.704906]\n",
      "epoch:24 step:22963 [D loss: 0.490311, acc.: 76.56%] [G loss: 0.668197]\n",
      "epoch:24 step:22964 [D loss: 0.555007, acc.: 73.44%] [G loss: 0.644290]\n",
      "epoch:24 step:22965 [D loss: 0.616083, acc.: 67.97%] [G loss: 0.639066]\n",
      "epoch:24 step:22966 [D loss: 0.554937, acc.: 73.44%] [G loss: 0.466912]\n",
      "epoch:24 step:22967 [D loss: 0.482733, acc.: 75.00%] [G loss: 0.587811]\n",
      "epoch:24 step:22968 [D loss: 0.629320, acc.: 66.41%] [G loss: 0.620872]\n",
      "epoch:24 step:22969 [D loss: 0.504455, acc.: 76.56%] [G loss: 0.634379]\n",
      "epoch:24 step:22970 [D loss: 0.556700, acc.: 75.00%] [G loss: 0.625066]\n",
      "epoch:24 step:22971 [D loss: 0.528438, acc.: 73.44%] [G loss: 0.686214]\n",
      "epoch:24 step:22972 [D loss: 0.506035, acc.: 76.56%] [G loss: 0.548483]\n",
      "epoch:24 step:22973 [D loss: 0.543313, acc.: 75.78%] [G loss: 0.649013]\n",
      "epoch:24 step:22974 [D loss: 0.547553, acc.: 67.19%] [G loss: 0.633485]\n",
      "epoch:24 step:22975 [D loss: 0.609723, acc.: 60.94%] [G loss: 0.686677]\n",
      "epoch:24 step:22976 [D loss: 0.532592, acc.: 70.31%] [G loss: 0.604915]\n",
      "epoch:24 step:22977 [D loss: 0.531000, acc.: 72.66%] [G loss: 0.846931]\n",
      "epoch:24 step:22978 [D loss: 0.511528, acc.: 74.22%] [G loss: 0.725361]\n",
      "epoch:24 step:22979 [D loss: 0.527869, acc.: 72.66%] [G loss: 0.639415]\n",
      "epoch:24 step:22980 [D loss: 0.546564, acc.: 71.88%] [G loss: 0.681071]\n",
      "epoch:24 step:22981 [D loss: 0.548033, acc.: 68.75%] [G loss: 0.727132]\n",
      "epoch:24 step:22982 [D loss: 0.575644, acc.: 67.97%] [G loss: 0.699413]\n",
      "epoch:24 step:22983 [D loss: 0.466654, acc.: 77.34%] [G loss: 0.632746]\n",
      "epoch:24 step:22984 [D loss: 0.560183, acc.: 71.88%] [G loss: 0.602810]\n",
      "epoch:24 step:22985 [D loss: 0.609776, acc.: 66.41%] [G loss: 0.751389]\n",
      "epoch:24 step:22986 [D loss: 0.542076, acc.: 71.09%] [G loss: 0.670089]\n",
      "epoch:24 step:22987 [D loss: 0.480545, acc.: 76.56%] [G loss: 0.681060]\n",
      "epoch:24 step:22988 [D loss: 0.605393, acc.: 67.97%] [G loss: 0.597998]\n",
      "epoch:24 step:22989 [D loss: 0.655338, acc.: 66.41%] [G loss: 0.626587]\n",
      "epoch:24 step:22990 [D loss: 0.569443, acc.: 66.41%] [G loss: 0.544416]\n",
      "epoch:24 step:22991 [D loss: 0.523440, acc.: 72.66%] [G loss: 0.651539]\n",
      "epoch:24 step:22992 [D loss: 0.476382, acc.: 76.56%] [G loss: 0.769106]\n",
      "epoch:24 step:22993 [D loss: 0.480788, acc.: 72.66%] [G loss: 0.918877]\n",
      "epoch:24 step:22994 [D loss: 0.484593, acc.: 76.56%] [G loss: 0.843604]\n",
      "epoch:24 step:22995 [D loss: 0.582589, acc.: 69.53%] [G loss: 0.690020]\n",
      "epoch:24 step:22996 [D loss: 0.469477, acc.: 82.03%] [G loss: 0.909061]\n",
      "epoch:24 step:22997 [D loss: 0.502369, acc.: 72.66%] [G loss: 0.881657]\n",
      "epoch:24 step:22998 [D loss: 0.643327, acc.: 60.16%] [G loss: 0.754337]\n",
      "epoch:24 step:22999 [D loss: 0.664282, acc.: 61.72%] [G loss: 0.559924]\n",
      "epoch:24 step:23000 [D loss: 0.590528, acc.: 64.84%] [G loss: 0.454108]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.652708\n",
      "FID: 44.701202\n",
      "0 = 12.897474793720226\n",
      "1 = 0.08378186317228002\n",
      "2 = 0.8639000058174133\n",
      "3 = 0.8271999955177307\n",
      "4 = 0.900600016117096\n",
      "5 = 0.8927261233329773\n",
      "6 = 0.8271999955177307\n",
      "7 = 8.163255717039105\n",
      "8 = 0.14445411066984734\n",
      "9 = 0.7002999782562256\n",
      "10 = 0.6934000253677368\n",
      "11 = 0.7071999907493591\n",
      "12 = 0.7031028270721436\n",
      "13 = 0.6934000253677368\n",
      "14 = 6.652732849121094\n",
      "15 = 6.956197738647461\n",
      "16 = 0.38924047350883484\n",
      "17 = 6.652707576751709\n",
      "18 = 44.701202392578125\n",
      "epoch:24 step:23001 [D loss: 0.558228, acc.: 67.19%] [G loss: 0.514364]\n",
      "epoch:24 step:23002 [D loss: 0.540994, acc.: 71.88%] [G loss: 0.512946]\n",
      "epoch:24 step:23003 [D loss: 0.564456, acc.: 67.97%] [G loss: 0.624603]\n",
      "epoch:24 step:23004 [D loss: 0.470834, acc.: 77.34%] [G loss: 0.798852]\n",
      "epoch:24 step:23005 [D loss: 0.498221, acc.: 75.78%] [G loss: 0.865724]\n",
      "epoch:24 step:23006 [D loss: 0.555306, acc.: 70.31%] [G loss: 0.707077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23007 [D loss: 0.481773, acc.: 78.12%] [G loss: 0.786708]\n",
      "epoch:24 step:23008 [D loss: 0.481399, acc.: 75.00%] [G loss: 0.785735]\n",
      "epoch:24 step:23009 [D loss: 0.470385, acc.: 81.25%] [G loss: 0.831910]\n",
      "epoch:24 step:23010 [D loss: 0.513265, acc.: 72.66%] [G loss: 0.778686]\n",
      "epoch:24 step:23011 [D loss: 0.438190, acc.: 77.34%] [G loss: 0.849991]\n",
      "epoch:24 step:23012 [D loss: 0.564574, acc.: 68.75%] [G loss: 0.577637]\n",
      "epoch:24 step:23013 [D loss: 0.562692, acc.: 73.44%] [G loss: 0.674092]\n",
      "epoch:24 step:23014 [D loss: 0.519496, acc.: 70.31%] [G loss: 0.606566]\n",
      "epoch:24 step:23015 [D loss: 0.515658, acc.: 71.09%] [G loss: 0.721765]\n",
      "epoch:24 step:23016 [D loss: 0.662207, acc.: 58.59%] [G loss: 0.560160]\n",
      "epoch:24 step:23017 [D loss: 0.576132, acc.: 67.19%] [G loss: 0.502777]\n",
      "epoch:24 step:23018 [D loss: 0.520391, acc.: 71.09%] [G loss: 0.741202]\n",
      "epoch:24 step:23019 [D loss: 0.583397, acc.: 66.41%] [G loss: 0.700845]\n",
      "epoch:24 step:23020 [D loss: 0.565234, acc.: 68.75%] [G loss: 0.723683]\n",
      "epoch:24 step:23021 [D loss: 0.501490, acc.: 71.88%] [G loss: 0.768474]\n",
      "epoch:24 step:23022 [D loss: 0.540937, acc.: 70.31%] [G loss: 0.639938]\n",
      "epoch:24 step:23023 [D loss: 0.601025, acc.: 65.62%] [G loss: 0.655700]\n",
      "epoch:24 step:23024 [D loss: 0.494726, acc.: 75.00%] [G loss: 0.661016]\n",
      "epoch:24 step:23025 [D loss: 0.588550, acc.: 61.72%] [G loss: 0.679624]\n",
      "epoch:24 step:23026 [D loss: 0.568151, acc.: 73.44%] [G loss: 0.758479]\n",
      "epoch:24 step:23027 [D loss: 0.559502, acc.: 70.31%] [G loss: 0.667011]\n",
      "epoch:24 step:23028 [D loss: 0.591311, acc.: 67.97%] [G loss: 0.629579]\n",
      "epoch:24 step:23029 [D loss: 0.528930, acc.: 71.88%] [G loss: 0.644429]\n",
      "epoch:24 step:23030 [D loss: 0.582685, acc.: 66.41%] [G loss: 0.647217]\n",
      "epoch:24 step:23031 [D loss: 0.534903, acc.: 71.88%] [G loss: 0.653733]\n",
      "epoch:24 step:23032 [D loss: 0.539863, acc.: 71.09%] [G loss: 0.668088]\n",
      "epoch:24 step:23033 [D loss: 0.544662, acc.: 71.09%] [G loss: 0.759559]\n",
      "epoch:24 step:23034 [D loss: 0.523778, acc.: 71.88%] [G loss: 0.776843]\n",
      "epoch:24 step:23035 [D loss: 0.524847, acc.: 73.44%] [G loss: 0.702177]\n",
      "epoch:24 step:23036 [D loss: 0.568395, acc.: 68.75%] [G loss: 0.713339]\n",
      "epoch:24 step:23037 [D loss: 0.527409, acc.: 75.78%] [G loss: 0.684684]\n",
      "epoch:24 step:23038 [D loss: 0.562524, acc.: 69.53%] [G loss: 0.637992]\n",
      "epoch:24 step:23039 [D loss: 0.534709, acc.: 74.22%] [G loss: 0.692491]\n",
      "epoch:24 step:23040 [D loss: 0.483239, acc.: 74.22%] [G loss: 0.795584]\n",
      "epoch:24 step:23041 [D loss: 0.587926, acc.: 64.06%] [G loss: 0.729767]\n",
      "epoch:24 step:23042 [D loss: 0.460728, acc.: 77.34%] [G loss: 0.743786]\n",
      "epoch:24 step:23043 [D loss: 0.453901, acc.: 77.34%] [G loss: 0.731668]\n",
      "epoch:24 step:23044 [D loss: 0.529399, acc.: 76.56%] [G loss: 0.662137]\n",
      "epoch:24 step:23045 [D loss: 0.503865, acc.: 75.00%] [G loss: 0.626906]\n",
      "epoch:24 step:23046 [D loss: 0.500269, acc.: 73.44%] [G loss: 0.708277]\n",
      "epoch:24 step:23047 [D loss: 0.587208, acc.: 68.75%] [G loss: 0.689255]\n",
      "epoch:24 step:23048 [D loss: 0.542580, acc.: 71.88%] [G loss: 0.618718]\n",
      "epoch:24 step:23049 [D loss: 0.558466, acc.: 71.09%] [G loss: 0.724842]\n",
      "epoch:24 step:23050 [D loss: 0.553190, acc.: 67.97%] [G loss: 0.595500]\n",
      "epoch:24 step:23051 [D loss: 0.537624, acc.: 67.19%] [G loss: 0.669431]\n",
      "epoch:24 step:23052 [D loss: 0.473026, acc.: 77.34%] [G loss: 0.745629]\n",
      "epoch:24 step:23053 [D loss: 0.665082, acc.: 61.72%] [G loss: 0.691156]\n",
      "epoch:24 step:23054 [D loss: 0.725236, acc.: 60.16%] [G loss: 0.576953]\n",
      "epoch:24 step:23055 [D loss: 0.485381, acc.: 75.00%] [G loss: 0.686468]\n",
      "epoch:24 step:23056 [D loss: 0.524830, acc.: 72.66%] [G loss: 0.644988]\n",
      "epoch:24 step:23057 [D loss: 0.584495, acc.: 68.75%] [G loss: 0.597201]\n",
      "epoch:24 step:23058 [D loss: 0.538030, acc.: 72.66%] [G loss: 0.573868]\n",
      "epoch:24 step:23059 [D loss: 0.495233, acc.: 74.22%] [G loss: 0.795973]\n",
      "epoch:24 step:23060 [D loss: 0.518119, acc.: 74.22%] [G loss: 0.592092]\n",
      "epoch:24 step:23061 [D loss: 0.516794, acc.: 73.44%] [G loss: 0.738420]\n",
      "epoch:24 step:23062 [D loss: 0.442354, acc.: 78.91%] [G loss: 0.775340]\n",
      "epoch:24 step:23063 [D loss: 0.470395, acc.: 76.56%] [G loss: 0.671098]\n",
      "epoch:24 step:23064 [D loss: 0.700767, acc.: 55.47%] [G loss: 0.655237]\n",
      "epoch:24 step:23065 [D loss: 0.522730, acc.: 73.44%] [G loss: 0.740257]\n",
      "epoch:24 step:23066 [D loss: 0.532815, acc.: 70.31%] [G loss: 0.634663]\n",
      "epoch:24 step:23067 [D loss: 0.526442, acc.: 76.56%] [G loss: 0.767874]\n",
      "epoch:24 step:23068 [D loss: 0.545372, acc.: 70.31%] [G loss: 0.540272]\n",
      "epoch:24 step:23069 [D loss: 0.595532, acc.: 66.41%] [G loss: 0.760949]\n",
      "epoch:24 step:23070 [D loss: 0.486566, acc.: 78.12%] [G loss: 0.796518]\n",
      "epoch:24 step:23071 [D loss: 0.599435, acc.: 65.62%] [G loss: 0.736682]\n",
      "epoch:24 step:23072 [D loss: 0.616525, acc.: 65.62%] [G loss: 0.634566]\n",
      "epoch:24 step:23073 [D loss: 0.527920, acc.: 74.22%] [G loss: 0.617118]\n",
      "epoch:24 step:23074 [D loss: 0.525163, acc.: 67.97%] [G loss: 0.690934]\n",
      "epoch:24 step:23075 [D loss: 0.596854, acc.: 71.09%] [G loss: 0.749852]\n",
      "epoch:24 step:23076 [D loss: 0.526460, acc.: 76.56%] [G loss: 0.628158]\n",
      "epoch:24 step:23077 [D loss: 0.521264, acc.: 68.75%] [G loss: 0.735258]\n",
      "epoch:24 step:23078 [D loss: 0.591694, acc.: 63.28%] [G loss: 0.693906]\n",
      "epoch:24 step:23079 [D loss: 0.584001, acc.: 70.31%] [G loss: 0.752846]\n",
      "epoch:24 step:23080 [D loss: 0.541007, acc.: 70.31%] [G loss: 0.604253]\n",
      "epoch:24 step:23081 [D loss: 0.496232, acc.: 74.22%] [G loss: 0.678767]\n",
      "epoch:24 step:23082 [D loss: 0.608898, acc.: 65.62%] [G loss: 0.741172]\n",
      "epoch:24 step:23083 [D loss: 0.552739, acc.: 65.62%] [G loss: 0.661111]\n",
      "epoch:24 step:23084 [D loss: 0.568877, acc.: 65.62%] [G loss: 0.612916]\n",
      "epoch:24 step:23085 [D loss: 0.552004, acc.: 74.22%] [G loss: 0.646189]\n",
      "epoch:24 step:23086 [D loss: 0.504671, acc.: 73.44%] [G loss: 0.761123]\n",
      "epoch:24 step:23087 [D loss: 0.499602, acc.: 75.78%] [G loss: 0.727691]\n",
      "epoch:24 step:23088 [D loss: 0.611720, acc.: 69.53%] [G loss: 0.597278]\n",
      "epoch:24 step:23089 [D loss: 0.545945, acc.: 71.09%] [G loss: 0.620514]\n",
      "epoch:24 step:23090 [D loss: 0.478744, acc.: 73.44%] [G loss: 0.708734]\n",
      "epoch:24 step:23091 [D loss: 0.434912, acc.: 78.12%] [G loss: 0.762000]\n",
      "epoch:24 step:23092 [D loss: 0.544122, acc.: 73.44%] [G loss: 0.732771]\n",
      "epoch:24 step:23093 [D loss: 0.462271, acc.: 75.00%] [G loss: 0.741407]\n",
      "epoch:24 step:23094 [D loss: 0.540441, acc.: 67.97%] [G loss: 0.751221]\n",
      "epoch:24 step:23095 [D loss: 0.568707, acc.: 64.84%] [G loss: 0.545359]\n",
      "epoch:24 step:23096 [D loss: 0.551898, acc.: 67.19%] [G loss: 0.593347]\n",
      "epoch:24 step:23097 [D loss: 0.505884, acc.: 71.09%] [G loss: 0.671099]\n",
      "epoch:24 step:23098 [D loss: 0.594354, acc.: 64.84%] [G loss: 0.478606]\n",
      "epoch:24 step:23099 [D loss: 0.523261, acc.: 72.66%] [G loss: 0.555129]\n",
      "epoch:24 step:23100 [D loss: 0.569054, acc.: 66.41%] [G loss: 0.470417]\n",
      "epoch:24 step:23101 [D loss: 0.494190, acc.: 73.44%] [G loss: 0.609434]\n",
      "epoch:24 step:23102 [D loss: 0.582916, acc.: 63.28%] [G loss: 0.555393]\n",
      "epoch:24 step:23103 [D loss: 0.521856, acc.: 73.44%] [G loss: 0.660342]\n",
      "epoch:24 step:23104 [D loss: 0.539478, acc.: 67.97%] [G loss: 0.781862]\n",
      "epoch:24 step:23105 [D loss: 0.520650, acc.: 74.22%] [G loss: 0.844194]\n",
      "epoch:24 step:23106 [D loss: 0.558704, acc.: 67.97%] [G loss: 0.798818]\n",
      "epoch:24 step:23107 [D loss: 0.602111, acc.: 63.28%] [G loss: 0.843617]\n",
      "epoch:24 step:23108 [D loss: 0.472465, acc.: 76.56%] [G loss: 0.654635]\n",
      "epoch:24 step:23109 [D loss: 0.570492, acc.: 70.31%] [G loss: 0.601993]\n",
      "epoch:24 step:23110 [D loss: 0.532370, acc.: 72.66%] [G loss: 0.602926]\n",
      "epoch:24 step:23111 [D loss: 0.508979, acc.: 72.66%] [G loss: 0.660723]\n",
      "epoch:24 step:23112 [D loss: 0.482253, acc.: 77.34%] [G loss: 0.845921]\n",
      "epoch:24 step:23113 [D loss: 0.620305, acc.: 62.50%] [G loss: 0.742181]\n",
      "epoch:24 step:23114 [D loss: 0.524032, acc.: 71.09%] [G loss: 0.623388]\n",
      "epoch:24 step:23115 [D loss: 0.509262, acc.: 73.44%] [G loss: 0.550093]\n",
      "epoch:24 step:23116 [D loss: 0.556219, acc.: 66.41%] [G loss: 0.550365]\n",
      "epoch:24 step:23117 [D loss: 0.511124, acc.: 71.09%] [G loss: 0.726211]\n",
      "epoch:24 step:23118 [D loss: 0.541733, acc.: 67.19%] [G loss: 0.739860]\n",
      "epoch:24 step:23119 [D loss: 0.486466, acc.: 77.34%] [G loss: 0.858455]\n",
      "epoch:24 step:23120 [D loss: 0.486648, acc.: 75.00%] [G loss: 0.798672]\n",
      "epoch:24 step:23121 [D loss: 0.495206, acc.: 76.56%] [G loss: 0.731038]\n",
      "epoch:24 step:23122 [D loss: 0.461434, acc.: 80.47%] [G loss: 0.653178]\n",
      "epoch:24 step:23123 [D loss: 0.466636, acc.: 75.78%] [G loss: 0.808343]\n",
      "epoch:24 step:23124 [D loss: 0.569706, acc.: 68.75%] [G loss: 0.711449]\n",
      "epoch:24 step:23125 [D loss: 0.505571, acc.: 78.12%] [G loss: 0.544207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23126 [D loss: 0.531689, acc.: 69.53%] [G loss: 0.627224]\n",
      "epoch:24 step:23127 [D loss: 0.473427, acc.: 78.12%] [G loss: 0.726503]\n",
      "epoch:24 step:23128 [D loss: 0.583017, acc.: 67.19%] [G loss: 0.687784]\n",
      "epoch:24 step:23129 [D loss: 0.570686, acc.: 70.31%] [G loss: 0.723691]\n",
      "epoch:24 step:23130 [D loss: 0.505622, acc.: 72.66%] [G loss: 0.962113]\n",
      "epoch:24 step:23131 [D loss: 0.465862, acc.: 78.12%] [G loss: 0.954095]\n",
      "epoch:24 step:23132 [D loss: 0.564090, acc.: 64.06%] [G loss: 0.685826]\n",
      "epoch:24 step:23133 [D loss: 0.493649, acc.: 75.00%] [G loss: 0.806443]\n",
      "epoch:24 step:23134 [D loss: 0.504145, acc.: 74.22%] [G loss: 0.610346]\n",
      "epoch:24 step:23135 [D loss: 0.390268, acc.: 84.38%] [G loss: 0.934464]\n",
      "epoch:24 step:23136 [D loss: 0.370012, acc.: 82.81%] [G loss: 0.890828]\n",
      "epoch:24 step:23137 [D loss: 0.471954, acc.: 77.34%] [G loss: 0.932797]\n",
      "epoch:24 step:23138 [D loss: 0.458094, acc.: 75.78%] [G loss: 0.982266]\n",
      "epoch:24 step:23139 [D loss: 0.520792, acc.: 73.44%] [G loss: 0.923978]\n",
      "epoch:24 step:23140 [D loss: 0.667823, acc.: 65.62%] [G loss: 0.776418]\n",
      "epoch:24 step:23141 [D loss: 0.564018, acc.: 72.66%] [G loss: 0.654294]\n",
      "epoch:24 step:23142 [D loss: 0.489096, acc.: 71.88%] [G loss: 0.783973]\n",
      "epoch:24 step:23143 [D loss: 0.521515, acc.: 73.44%] [G loss: 0.671039]\n",
      "epoch:24 step:23144 [D loss: 0.563952, acc.: 71.09%] [G loss: 0.656686]\n",
      "epoch:24 step:23145 [D loss: 0.479586, acc.: 73.44%] [G loss: 0.860962]\n",
      "epoch:24 step:23146 [D loss: 0.561893, acc.: 67.97%] [G loss: 0.795724]\n",
      "epoch:24 step:23147 [D loss: 0.495639, acc.: 73.44%] [G loss: 0.768213]\n",
      "epoch:24 step:23148 [D loss: 0.479127, acc.: 79.69%] [G loss: 0.713097]\n",
      "epoch:24 step:23149 [D loss: 0.493344, acc.: 74.22%] [G loss: 0.914790]\n",
      "epoch:24 step:23150 [D loss: 0.610689, acc.: 71.09%] [G loss: 0.769216]\n",
      "epoch:24 step:23151 [D loss: 0.548025, acc.: 69.53%] [G loss: 0.888365]\n",
      "epoch:24 step:23152 [D loss: 0.540104, acc.: 69.53%] [G loss: 0.676948]\n",
      "epoch:24 step:23153 [D loss: 0.637596, acc.: 60.94%] [G loss: 0.700543]\n",
      "epoch:24 step:23154 [D loss: 0.570300, acc.: 69.53%] [G loss: 0.648336]\n",
      "epoch:24 step:23155 [D loss: 0.531657, acc.: 75.00%] [G loss: 0.657786]\n",
      "epoch:24 step:23156 [D loss: 0.518944, acc.: 78.91%] [G loss: 0.741738]\n",
      "epoch:24 step:23157 [D loss: 0.521962, acc.: 70.31%] [G loss: 0.788518]\n",
      "epoch:24 step:23158 [D loss: 0.541137, acc.: 67.19%] [G loss: 0.676023]\n",
      "epoch:24 step:23159 [D loss: 0.519154, acc.: 73.44%] [G loss: 0.667596]\n",
      "epoch:24 step:23160 [D loss: 0.545225, acc.: 70.31%] [G loss: 0.491547]\n",
      "epoch:24 step:23161 [D loss: 0.588300, acc.: 68.75%] [G loss: 0.830273]\n",
      "epoch:24 step:23162 [D loss: 0.556644, acc.: 68.75%] [G loss: 0.687114]\n",
      "epoch:24 step:23163 [D loss: 0.587082, acc.: 67.97%] [G loss: 0.443619]\n",
      "epoch:24 step:23164 [D loss: 0.491591, acc.: 75.78%] [G loss: 0.612212]\n",
      "epoch:24 step:23165 [D loss: 0.510390, acc.: 76.56%] [G loss: 0.686119]\n",
      "epoch:24 step:23166 [D loss: 0.547834, acc.: 71.09%] [G loss: 0.666163]\n",
      "epoch:24 step:23167 [D loss: 0.531226, acc.: 74.22%] [G loss: 0.626207]\n",
      "epoch:24 step:23168 [D loss: 0.527003, acc.: 75.78%] [G loss: 0.743368]\n",
      "epoch:24 step:23169 [D loss: 0.513740, acc.: 71.88%] [G loss: 0.678528]\n",
      "epoch:24 step:23170 [D loss: 0.561591, acc.: 72.66%] [G loss: 0.754389]\n",
      "epoch:24 step:23171 [D loss: 0.541636, acc.: 70.31%] [G loss: 0.712015]\n",
      "epoch:24 step:23172 [D loss: 0.590950, acc.: 67.97%] [G loss: 0.614236]\n",
      "epoch:24 step:23173 [D loss: 0.506775, acc.: 72.66%] [G loss: 0.623067]\n",
      "epoch:24 step:23174 [D loss: 0.589010, acc.: 67.19%] [G loss: 0.516243]\n",
      "epoch:24 step:23175 [D loss: 0.626467, acc.: 61.72%] [G loss: 0.506508]\n",
      "epoch:24 step:23176 [D loss: 0.526366, acc.: 71.88%] [G loss: 0.678763]\n",
      "epoch:24 step:23177 [D loss: 0.569492, acc.: 65.62%] [G loss: 0.585651]\n",
      "epoch:24 step:23178 [D loss: 0.510750, acc.: 71.09%] [G loss: 0.808774]\n",
      "epoch:24 step:23179 [D loss: 0.494419, acc.: 75.78%] [G loss: 0.766491]\n",
      "epoch:24 step:23180 [D loss: 0.549633, acc.: 73.44%] [G loss: 0.864958]\n",
      "epoch:24 step:23181 [D loss: 0.463706, acc.: 77.34%] [G loss: 0.861565]\n",
      "epoch:24 step:23182 [D loss: 0.517123, acc.: 71.09%] [G loss: 0.749443]\n",
      "epoch:24 step:23183 [D loss: 0.478129, acc.: 77.34%] [G loss: 0.760324]\n",
      "epoch:24 step:23184 [D loss: 0.710287, acc.: 50.78%] [G loss: 0.512159]\n",
      "epoch:24 step:23185 [D loss: 0.549430, acc.: 69.53%] [G loss: 0.509687]\n",
      "epoch:24 step:23186 [D loss: 0.606468, acc.: 67.19%] [G loss: 0.559497]\n",
      "epoch:24 step:23187 [D loss: 0.534831, acc.: 71.88%] [G loss: 0.559425]\n",
      "epoch:24 step:23188 [D loss: 0.572762, acc.: 63.28%] [G loss: 0.629125]\n",
      "epoch:24 step:23189 [D loss: 0.495757, acc.: 72.66%] [G loss: 0.692228]\n",
      "epoch:24 step:23190 [D loss: 0.605866, acc.: 64.84%] [G loss: 0.553479]\n",
      "epoch:24 step:23191 [D loss: 0.612680, acc.: 66.41%] [G loss: 0.685441]\n",
      "epoch:24 step:23192 [D loss: 0.611951, acc.: 66.41%] [G loss: 0.665022]\n",
      "epoch:24 step:23193 [D loss: 0.544455, acc.: 73.44%] [G loss: 0.612154]\n",
      "epoch:24 step:23194 [D loss: 0.532947, acc.: 71.09%] [G loss: 0.606582]\n",
      "epoch:24 step:23195 [D loss: 0.486385, acc.: 78.12%] [G loss: 0.761965]\n",
      "epoch:24 step:23196 [D loss: 0.503415, acc.: 78.91%] [G loss: 0.886712]\n",
      "epoch:24 step:23197 [D loss: 0.473598, acc.: 73.44%] [G loss: 0.628546]\n",
      "epoch:24 step:23198 [D loss: 0.662952, acc.: 60.94%] [G loss: 0.485713]\n",
      "epoch:24 step:23199 [D loss: 0.544136, acc.: 68.75%] [G loss: 0.701161]\n",
      "epoch:24 step:23200 [D loss: 0.556400, acc.: 68.75%] [G loss: 0.519827]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.506036\n",
      "FID: 55.010494\n",
      "0 = 12.986896788883177\n",
      "1 = 0.08742893602970155\n",
      "2 = 0.8611000180244446\n",
      "3 = 0.8212000131607056\n",
      "4 = 0.9010000228881836\n",
      "5 = 0.8924146890640259\n",
      "6 = 0.8212000131607056\n",
      "7 = 8.615102305912991\n",
      "8 = 0.16576842809452494\n",
      "9 = 0.704200029373169\n",
      "10 = 0.6966000199317932\n",
      "11 = 0.7117999792098999\n",
      "12 = 0.7073517441749573\n",
      "13 = 0.6966000199317932\n",
      "14 = 6.50606107711792\n",
      "15 = 6.620321750640869\n",
      "16 = 0.41166093945503235\n",
      "17 = 6.506035804748535\n",
      "18 = 55.010494232177734\n",
      "epoch:24 step:23201 [D loss: 0.600264, acc.: 60.94%] [G loss: 0.515606]\n",
      "epoch:24 step:23202 [D loss: 0.523810, acc.: 73.44%] [G loss: 0.672899]\n",
      "epoch:24 step:23203 [D loss: 0.555203, acc.: 71.88%] [G loss: 0.605448]\n",
      "epoch:24 step:23204 [D loss: 0.563276, acc.: 68.75%] [G loss: 0.584567]\n",
      "epoch:24 step:23205 [D loss: 0.588376, acc.: 67.97%] [G loss: 0.563923]\n",
      "epoch:24 step:23206 [D loss: 0.546979, acc.: 71.09%] [G loss: 0.527563]\n",
      "epoch:24 step:23207 [D loss: 0.479241, acc.: 76.56%] [G loss: 0.846433]\n",
      "epoch:24 step:23208 [D loss: 0.543682, acc.: 71.88%] [G loss: 0.799957]\n",
      "epoch:24 step:23209 [D loss: 0.568372, acc.: 68.75%] [G loss: 0.529330]\n",
      "epoch:24 step:23210 [D loss: 0.518001, acc.: 76.56%] [G loss: 0.619863]\n",
      "epoch:24 step:23211 [D loss: 0.570467, acc.: 67.97%] [G loss: 0.587881]\n",
      "epoch:24 step:23212 [D loss: 0.468983, acc.: 77.34%] [G loss: 0.685767]\n",
      "epoch:24 step:23213 [D loss: 0.499372, acc.: 76.56%] [G loss: 0.776880]\n",
      "epoch:24 step:23214 [D loss: 0.530421, acc.: 79.69%] [G loss: 0.769246]\n",
      "epoch:24 step:23215 [D loss: 0.588776, acc.: 64.84%] [G loss: 0.691562]\n",
      "epoch:24 step:23216 [D loss: 0.554270, acc.: 67.97%] [G loss: 0.668659]\n",
      "epoch:24 step:23217 [D loss: 0.582368, acc.: 69.53%] [G loss: 0.599790]\n",
      "epoch:24 step:23218 [D loss: 0.526994, acc.: 69.53%] [G loss: 0.576561]\n",
      "epoch:24 step:23219 [D loss: 0.562614, acc.: 64.84%] [G loss: 0.704559]\n",
      "epoch:24 step:23220 [D loss: 0.581694, acc.: 68.75%] [G loss: 0.753683]\n",
      "epoch:24 step:23221 [D loss: 0.510890, acc.: 74.22%] [G loss: 0.586658]\n",
      "epoch:24 step:23222 [D loss: 0.549470, acc.: 68.75%] [G loss: 0.494388]\n",
      "epoch:24 step:23223 [D loss: 0.525132, acc.: 70.31%] [G loss: 0.639685]\n",
      "epoch:24 step:23224 [D loss: 0.485991, acc.: 76.56%] [G loss: 0.745549]\n",
      "epoch:24 step:23225 [D loss: 0.536300, acc.: 71.09%] [G loss: 0.649216]\n",
      "epoch:24 step:23226 [D loss: 0.515099, acc.: 71.88%] [G loss: 0.701950]\n",
      "epoch:24 step:23227 [D loss: 0.554216, acc.: 70.31%] [G loss: 0.661608]\n",
      "epoch:24 step:23228 [D loss: 0.631833, acc.: 60.16%] [G loss: 0.442841]\n",
      "epoch:24 step:23229 [D loss: 0.548906, acc.: 72.66%] [G loss: 0.602864]\n",
      "epoch:24 step:23230 [D loss: 0.539738, acc.: 67.97%] [G loss: 0.564963]\n",
      "epoch:24 step:23231 [D loss: 0.548795, acc.: 71.09%] [G loss: 0.625431]\n",
      "epoch:24 step:23232 [D loss: 0.477513, acc.: 77.34%] [G loss: 0.821219]\n",
      "epoch:24 step:23233 [D loss: 0.557488, acc.: 66.41%] [G loss: 0.753205]\n",
      "epoch:24 step:23234 [D loss: 0.447271, acc.: 78.12%] [G loss: 0.603362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23235 [D loss: 0.459405, acc.: 79.69%] [G loss: 0.762167]\n",
      "epoch:24 step:23236 [D loss: 0.539364, acc.: 71.88%] [G loss: 0.720966]\n",
      "epoch:24 step:23237 [D loss: 0.521504, acc.: 70.31%] [G loss: 0.776895]\n",
      "epoch:24 step:23238 [D loss: 0.475033, acc.: 74.22%] [G loss: 0.805674]\n",
      "epoch:24 step:23239 [D loss: 0.528358, acc.: 71.09%] [G loss: 0.812950]\n",
      "epoch:24 step:23240 [D loss: 0.566326, acc.: 68.75%] [G loss: 0.748855]\n",
      "epoch:24 step:23241 [D loss: 0.528482, acc.: 75.00%] [G loss: 0.754210]\n",
      "epoch:24 step:23242 [D loss: 0.531735, acc.: 73.44%] [G loss: 0.637789]\n",
      "epoch:24 step:23243 [D loss: 0.506432, acc.: 73.44%] [G loss: 0.741958]\n",
      "epoch:24 step:23244 [D loss: 0.471074, acc.: 78.12%] [G loss: 0.699306]\n",
      "epoch:24 step:23245 [D loss: 0.584924, acc.: 63.28%] [G loss: 0.615297]\n",
      "epoch:24 step:23246 [D loss: 0.530174, acc.: 71.09%] [G loss: 0.607129]\n",
      "epoch:24 step:23247 [D loss: 0.557963, acc.: 73.44%] [G loss: 0.826789]\n",
      "epoch:24 step:23248 [D loss: 0.557281, acc.: 67.97%] [G loss: 0.801853]\n",
      "epoch:24 step:23249 [D loss: 0.565333, acc.: 66.41%] [G loss: 0.629470]\n",
      "epoch:24 step:23250 [D loss: 0.545644, acc.: 71.88%] [G loss: 0.732757]\n",
      "epoch:24 step:23251 [D loss: 0.538134, acc.: 70.31%] [G loss: 0.741148]\n",
      "epoch:24 step:23252 [D loss: 0.531968, acc.: 73.44%] [G loss: 0.615847]\n",
      "epoch:24 step:23253 [D loss: 0.571050, acc.: 68.75%] [G loss: 0.641162]\n",
      "epoch:24 step:23254 [D loss: 0.637766, acc.: 59.38%] [G loss: 0.681628]\n",
      "epoch:24 step:23255 [D loss: 0.563055, acc.: 69.53%] [G loss: 0.847058]\n",
      "epoch:24 step:23256 [D loss: 0.559078, acc.: 67.97%] [G loss: 0.862903]\n",
      "epoch:24 step:23257 [D loss: 0.440558, acc.: 82.03%] [G loss: 0.921025]\n",
      "epoch:24 step:23258 [D loss: 0.536930, acc.: 73.44%] [G loss: 0.759504]\n",
      "epoch:24 step:23259 [D loss: 0.506530, acc.: 74.22%] [G loss: 0.853198]\n",
      "epoch:24 step:23260 [D loss: 0.545390, acc.: 72.66%] [G loss: 0.816274]\n",
      "epoch:24 step:23261 [D loss: 0.530061, acc.: 72.66%] [G loss: 0.647548]\n",
      "epoch:24 step:23262 [D loss: 0.487762, acc.: 75.00%] [G loss: 0.798431]\n",
      "epoch:24 step:23263 [D loss: 0.524482, acc.: 73.44%] [G loss: 0.703108]\n",
      "epoch:24 step:23264 [D loss: 0.588893, acc.: 65.62%] [G loss: 0.563097]\n",
      "epoch:24 step:23265 [D loss: 0.530961, acc.: 70.31%] [G loss: 0.681795]\n",
      "epoch:24 step:23266 [D loss: 0.541343, acc.: 71.09%] [G loss: 0.638574]\n",
      "epoch:24 step:23267 [D loss: 0.535807, acc.: 69.53%] [G loss: 0.636887]\n",
      "epoch:24 step:23268 [D loss: 0.547975, acc.: 71.09%] [G loss: 0.731704]\n",
      "epoch:24 step:23269 [D loss: 0.460691, acc.: 81.25%] [G loss: 0.795307]\n",
      "epoch:24 step:23270 [D loss: 0.513217, acc.: 70.31%] [G loss: 0.931991]\n",
      "epoch:24 step:23271 [D loss: 0.534176, acc.: 72.66%] [G loss: 0.717193]\n",
      "epoch:24 step:23272 [D loss: 0.593106, acc.: 67.97%] [G loss: 0.671281]\n",
      "epoch:24 step:23273 [D loss: 0.529299, acc.: 68.75%] [G loss: 0.708754]\n",
      "epoch:24 step:23274 [D loss: 0.487862, acc.: 80.47%] [G loss: 0.740755]\n",
      "epoch:24 step:23275 [D loss: 0.550707, acc.: 74.22%] [G loss: 0.790217]\n",
      "epoch:24 step:23276 [D loss: 0.610985, acc.: 70.31%] [G loss: 0.546238]\n",
      "epoch:24 step:23277 [D loss: 0.570665, acc.: 67.19%] [G loss: 0.626064]\n",
      "epoch:24 step:23278 [D loss: 0.526651, acc.: 71.88%] [G loss: 0.878635]\n",
      "epoch:24 step:23279 [D loss: 0.576442, acc.: 70.31%] [G loss: 0.786015]\n",
      "epoch:24 step:23280 [D loss: 0.507754, acc.: 67.97%] [G loss: 0.518098]\n",
      "epoch:24 step:23281 [D loss: 0.640876, acc.: 59.38%] [G loss: 0.558888]\n",
      "epoch:24 step:23282 [D loss: 0.638239, acc.: 64.06%] [G loss: 0.599768]\n",
      "epoch:24 step:23283 [D loss: 0.532103, acc.: 70.31%] [G loss: 0.692512]\n",
      "epoch:24 step:23284 [D loss: 0.535127, acc.: 74.22%] [G loss: 0.642975]\n",
      "epoch:24 step:23285 [D loss: 0.481201, acc.: 77.34%] [G loss: 0.874746]\n",
      "epoch:24 step:23286 [D loss: 0.482950, acc.: 76.56%] [G loss: 0.808386]\n",
      "epoch:24 step:23287 [D loss: 0.555898, acc.: 69.53%] [G loss: 0.639106]\n",
      "epoch:24 step:23288 [D loss: 0.597706, acc.: 60.16%] [G loss: 0.633595]\n",
      "epoch:24 step:23289 [D loss: 0.524612, acc.: 74.22%] [G loss: 0.692608]\n",
      "epoch:24 step:23290 [D loss: 0.524533, acc.: 71.88%] [G loss: 0.851033]\n",
      "epoch:24 step:23291 [D loss: 0.498919, acc.: 80.47%] [G loss: 0.832196]\n",
      "epoch:24 step:23292 [D loss: 0.602772, acc.: 64.84%] [G loss: 0.655966]\n",
      "epoch:24 step:23293 [D loss: 0.513006, acc.: 75.78%] [G loss: 0.582850]\n",
      "epoch:24 step:23294 [D loss: 0.576641, acc.: 67.19%] [G loss: 0.583123]\n",
      "epoch:24 step:23295 [D loss: 0.510885, acc.: 72.66%] [G loss: 0.524006]\n",
      "epoch:24 step:23296 [D loss: 0.533361, acc.: 69.53%] [G loss: 0.652816]\n",
      "epoch:24 step:23297 [D loss: 0.602426, acc.: 65.62%] [G loss: 0.561347]\n",
      "epoch:24 step:23298 [D loss: 0.509068, acc.: 70.31%] [G loss: 0.645049]\n",
      "epoch:24 step:23299 [D loss: 0.574263, acc.: 67.19%] [G loss: 0.685602]\n",
      "epoch:24 step:23300 [D loss: 0.677402, acc.: 58.59%] [G loss: 0.483328]\n",
      "epoch:24 step:23301 [D loss: 0.536078, acc.: 71.09%] [G loss: 0.594806]\n",
      "epoch:24 step:23302 [D loss: 0.522612, acc.: 70.31%] [G loss: 0.668739]\n",
      "epoch:24 step:23303 [D loss: 0.502642, acc.: 76.56%] [G loss: 0.807782]\n",
      "epoch:24 step:23304 [D loss: 0.547603, acc.: 71.88%] [G loss: 0.616411]\n",
      "epoch:24 step:23305 [D loss: 0.590668, acc.: 69.53%] [G loss: 0.635968]\n",
      "epoch:24 step:23306 [D loss: 0.580807, acc.: 65.62%] [G loss: 0.716987]\n",
      "epoch:24 step:23307 [D loss: 0.511379, acc.: 73.44%] [G loss: 0.826629]\n",
      "epoch:24 step:23308 [D loss: 0.637052, acc.: 67.97%] [G loss: 0.612478]\n",
      "epoch:24 step:23309 [D loss: 0.557300, acc.: 68.75%] [G loss: 0.648031]\n",
      "epoch:24 step:23310 [D loss: 0.512065, acc.: 74.22%] [G loss: 0.616205]\n",
      "epoch:24 step:23311 [D loss: 0.483135, acc.: 72.66%] [G loss: 0.767118]\n",
      "epoch:24 step:23312 [D loss: 0.606717, acc.: 64.06%] [G loss: 0.639184]\n",
      "epoch:24 step:23313 [D loss: 0.480991, acc.: 74.22%] [G loss: 0.725699]\n",
      "epoch:24 step:23314 [D loss: 0.539786, acc.: 71.88%] [G loss: 0.712258]\n",
      "epoch:24 step:23315 [D loss: 0.587340, acc.: 67.97%] [G loss: 0.599860]\n",
      "epoch:24 step:23316 [D loss: 0.660334, acc.: 60.16%] [G loss: 0.627155]\n",
      "epoch:24 step:23317 [D loss: 0.532869, acc.: 71.09%] [G loss: 0.739108]\n",
      "epoch:24 step:23318 [D loss: 0.608869, acc.: 68.75%] [G loss: 0.756439]\n",
      "epoch:24 step:23319 [D loss: 0.589318, acc.: 67.97%] [G loss: 0.638526]\n",
      "epoch:24 step:23320 [D loss: 0.561145, acc.: 67.19%] [G loss: 0.777685]\n",
      "epoch:24 step:23321 [D loss: 0.506395, acc.: 75.00%] [G loss: 0.861995]\n",
      "epoch:24 step:23322 [D loss: 0.573648, acc.: 65.62%] [G loss: 0.690563]\n",
      "epoch:24 step:23323 [D loss: 0.513846, acc.: 71.88%] [G loss: 0.603840]\n",
      "epoch:24 step:23324 [D loss: 0.542379, acc.: 74.22%] [G loss: 0.598616]\n",
      "epoch:24 step:23325 [D loss: 0.505416, acc.: 71.88%] [G loss: 0.613901]\n",
      "epoch:24 step:23326 [D loss: 0.516328, acc.: 71.88%] [G loss: 0.632326]\n",
      "epoch:24 step:23327 [D loss: 0.593496, acc.: 66.41%] [G loss: 0.621770]\n",
      "epoch:24 step:23328 [D loss: 0.620823, acc.: 66.41%] [G loss: 0.587005]\n",
      "epoch:24 step:23329 [D loss: 0.508718, acc.: 70.31%] [G loss: 0.599201]\n",
      "epoch:24 step:23330 [D loss: 0.551009, acc.: 71.09%] [G loss: 0.532186]\n",
      "epoch:24 step:23331 [D loss: 0.506468, acc.: 74.22%] [G loss: 0.665091]\n",
      "epoch:24 step:23332 [D loss: 0.548996, acc.: 67.19%] [G loss: 0.785222]\n",
      "epoch:24 step:23333 [D loss: 0.580256, acc.: 68.75%] [G loss: 0.624580]\n",
      "epoch:24 step:23334 [D loss: 0.584769, acc.: 65.62%] [G loss: 0.658582]\n",
      "epoch:24 step:23335 [D loss: 0.664741, acc.: 57.81%] [G loss: 0.421868]\n",
      "epoch:24 step:23336 [D loss: 0.581769, acc.: 64.84%] [G loss: 0.514148]\n",
      "epoch:24 step:23337 [D loss: 0.594781, acc.: 65.62%] [G loss: 0.470823]\n",
      "epoch:24 step:23338 [D loss: 0.583172, acc.: 67.97%] [G loss: 0.521320]\n",
      "epoch:24 step:23339 [D loss: 0.563611, acc.: 68.75%] [G loss: 0.683717]\n",
      "epoch:24 step:23340 [D loss: 0.537099, acc.: 71.88%] [G loss: 0.704523]\n",
      "epoch:24 step:23341 [D loss: 0.596493, acc.: 67.97%] [G loss: 0.675141]\n",
      "epoch:24 step:23342 [D loss: 0.522279, acc.: 71.09%] [G loss: 0.776190]\n",
      "epoch:24 step:23343 [D loss: 0.546067, acc.: 68.75%] [G loss: 0.592697]\n",
      "epoch:24 step:23344 [D loss: 0.602171, acc.: 64.06%] [G loss: 0.633391]\n",
      "epoch:24 step:23345 [D loss: 0.445279, acc.: 79.69%] [G loss: 0.704031]\n",
      "epoch:24 step:23346 [D loss: 0.637783, acc.: 59.38%] [G loss: 0.635110]\n",
      "epoch:24 step:23347 [D loss: 0.566613, acc.: 66.41%] [G loss: 0.676596]\n",
      "epoch:24 step:23348 [D loss: 0.431099, acc.: 78.91%] [G loss: 0.681791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23349 [D loss: 0.590749, acc.: 67.19%] [G loss: 0.665451]\n",
      "epoch:24 step:23350 [D loss: 0.593742, acc.: 68.75%] [G loss: 0.445667]\n",
      "epoch:24 step:23351 [D loss: 0.548329, acc.: 66.41%] [G loss: 0.614111]\n",
      "epoch:24 step:23352 [D loss: 0.524680, acc.: 71.88%] [G loss: 0.692508]\n",
      "epoch:24 step:23353 [D loss: 0.532503, acc.: 71.88%] [G loss: 0.578663]\n",
      "epoch:24 step:23354 [D loss: 0.534606, acc.: 75.78%] [G loss: 0.543030]\n",
      "epoch:24 step:23355 [D loss: 0.664221, acc.: 57.81%] [G loss: 0.433497]\n",
      "epoch:24 step:23356 [D loss: 0.503419, acc.: 79.69%] [G loss: 0.558935]\n",
      "epoch:24 step:23357 [D loss: 0.569560, acc.: 67.97%] [G loss: 0.700509]\n",
      "epoch:24 step:23358 [D loss: 0.453573, acc.: 76.56%] [G loss: 0.814961]\n",
      "epoch:24 step:23359 [D loss: 0.516085, acc.: 72.66%] [G loss: 0.992575]\n",
      "epoch:24 step:23360 [D loss: 0.500733, acc.: 71.88%] [G loss: 0.732252]\n",
      "epoch:24 step:23361 [D loss: 0.591336, acc.: 67.97%] [G loss: 0.638917]\n",
      "epoch:24 step:23362 [D loss: 0.520649, acc.: 71.09%] [G loss: 0.667553]\n",
      "epoch:24 step:23363 [D loss: 0.525246, acc.: 69.53%] [G loss: 0.548320]\n",
      "epoch:24 step:23364 [D loss: 0.576983, acc.: 63.28%] [G loss: 0.690778]\n",
      "epoch:24 step:23365 [D loss: 0.598413, acc.: 64.84%] [G loss: 0.511295]\n",
      "epoch:24 step:23366 [D loss: 0.557536, acc.: 70.31%] [G loss: 0.506481]\n",
      "epoch:24 step:23367 [D loss: 0.570846, acc.: 65.62%] [G loss: 0.476663]\n",
      "epoch:24 step:23368 [D loss: 0.605565, acc.: 64.06%] [G loss: 0.547034]\n",
      "epoch:24 step:23369 [D loss: 0.532560, acc.: 69.53%] [G loss: 0.480054]\n",
      "epoch:24 step:23370 [D loss: 0.563684, acc.: 66.41%] [G loss: 0.506313]\n",
      "epoch:24 step:23371 [D loss: 0.671981, acc.: 59.38%] [G loss: 0.470584]\n",
      "epoch:24 step:23372 [D loss: 0.480319, acc.: 78.12%] [G loss: 0.659435]\n",
      "epoch:24 step:23373 [D loss: 0.528938, acc.: 73.44%] [G loss: 0.764716]\n",
      "epoch:24 step:23374 [D loss: 0.560447, acc.: 69.53%] [G loss: 0.941747]\n",
      "epoch:24 step:23375 [D loss: 0.571032, acc.: 72.66%] [G loss: 0.763377]\n",
      "epoch:24 step:23376 [D loss: 0.591589, acc.: 64.06%] [G loss: 0.688017]\n",
      "epoch:24 step:23377 [D loss: 0.548830, acc.: 72.66%] [G loss: 0.587247]\n",
      "epoch:24 step:23378 [D loss: 0.472840, acc.: 79.69%] [G loss: 0.657029]\n",
      "epoch:24 step:23379 [D loss: 0.568684, acc.: 72.66%] [G loss: 0.653540]\n",
      "epoch:24 step:23380 [D loss: 0.621647, acc.: 67.19%] [G loss: 0.584366]\n",
      "epoch:24 step:23381 [D loss: 0.554884, acc.: 73.44%] [G loss: 0.643035]\n",
      "epoch:24 step:23382 [D loss: 0.476832, acc.: 73.44%] [G loss: 0.609945]\n",
      "epoch:24 step:23383 [D loss: 0.503835, acc.: 73.44%] [G loss: 0.798474]\n",
      "epoch:24 step:23384 [D loss: 0.480765, acc.: 75.78%] [G loss: 0.732805]\n",
      "epoch:24 step:23385 [D loss: 0.514238, acc.: 69.53%] [G loss: 0.705991]\n",
      "epoch:24 step:23386 [D loss: 0.478418, acc.: 77.34%] [G loss: 0.722173]\n",
      "epoch:24 step:23387 [D loss: 0.463276, acc.: 79.69%] [G loss: 0.641812]\n",
      "epoch:24 step:23388 [D loss: 0.484054, acc.: 75.00%] [G loss: 0.915388]\n",
      "epoch:24 step:23389 [D loss: 0.520845, acc.: 72.66%] [G loss: 0.820418]\n",
      "epoch:24 step:23390 [D loss: 0.544201, acc.: 67.19%] [G loss: 0.915264]\n",
      "epoch:24 step:23391 [D loss: 0.564631, acc.: 67.97%] [G loss: 0.751692]\n",
      "epoch:24 step:23392 [D loss: 0.548148, acc.: 72.66%] [G loss: 0.759901]\n",
      "epoch:24 step:23393 [D loss: 0.575785, acc.: 71.88%] [G loss: 0.738791]\n",
      "epoch:24 step:23394 [D loss: 0.505261, acc.: 74.22%] [G loss: 0.766374]\n",
      "epoch:24 step:23395 [D loss: 0.594830, acc.: 65.62%] [G loss: 0.660796]\n",
      "epoch:24 step:23396 [D loss: 0.540720, acc.: 69.53%] [G loss: 0.794262]\n",
      "epoch:24 step:23397 [D loss: 0.509332, acc.: 73.44%] [G loss: 0.734157]\n",
      "epoch:24 step:23398 [D loss: 0.582764, acc.: 69.53%] [G loss: 0.591230]\n",
      "epoch:24 step:23399 [D loss: 0.462267, acc.: 78.12%] [G loss: 0.792272]\n",
      "epoch:24 step:23400 [D loss: 0.437988, acc.: 78.12%] [G loss: 1.122134]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.584438\n",
      "FID: 49.641960\n",
      "0 = 12.82093759307863\n",
      "1 = 0.08424302128263599\n",
      "2 = 0.8641999959945679\n",
      "3 = 0.8167999982833862\n",
      "4 = 0.9115999937057495\n",
      "5 = 0.9023420214653015\n",
      "6 = 0.8167999982833862\n",
      "7 = 8.252344510459904\n",
      "8 = 0.1531304806098279\n",
      "9 = 0.7127000093460083\n",
      "10 = 0.699400007724762\n",
      "11 = 0.7260000109672546\n",
      "12 = 0.7185124158859253\n",
      "13 = 0.699400007724762\n",
      "14 = 6.584464073181152\n",
      "15 = 6.7973527908325195\n",
      "16 = 0.3964226543903351\n",
      "17 = 6.584437847137451\n",
      "18 = 49.64196014404297\n",
      "epoch:24 step:23401 [D loss: 0.543354, acc.: 72.66%] [G loss: 1.012528]\n",
      "epoch:24 step:23402 [D loss: 0.489319, acc.: 75.00%] [G loss: 1.002558]\n",
      "epoch:24 step:23403 [D loss: 0.686820, acc.: 61.72%] [G loss: 0.872551]\n",
      "epoch:24 step:23404 [D loss: 0.524200, acc.: 72.66%] [G loss: 0.983033]\n",
      "epoch:24 step:23405 [D loss: 0.590900, acc.: 65.62%] [G loss: 0.591913]\n",
      "epoch:24 step:23406 [D loss: 0.470523, acc.: 75.78%] [G loss: 0.926197]\n",
      "epoch:24 step:23407 [D loss: 0.390326, acc.: 85.94%] [G loss: 1.144159]\n",
      "epoch:24 step:23408 [D loss: 0.724993, acc.: 62.50%] [G loss: 0.782261]\n",
      "epoch:24 step:23409 [D loss: 0.496263, acc.: 75.00%] [G loss: 0.766351]\n",
      "epoch:24 step:23410 [D loss: 0.518813, acc.: 72.66%] [G loss: 0.819653]\n",
      "epoch:24 step:23411 [D loss: 0.440849, acc.: 79.69%] [G loss: 0.805963]\n",
      "epoch:24 step:23412 [D loss: 0.404502, acc.: 82.03%] [G loss: 0.872256]\n",
      "epoch:24 step:23413 [D loss: 0.507471, acc.: 70.31%] [G loss: 1.241591]\n",
      "epoch:24 step:23414 [D loss: 0.428748, acc.: 77.34%] [G loss: 1.153966]\n",
      "epoch:24 step:23415 [D loss: 0.482381, acc.: 75.78%] [G loss: 1.193434]\n",
      "epoch:24 step:23416 [D loss: 0.684583, acc.: 67.97%] [G loss: 1.057060]\n",
      "epoch:24 step:23417 [D loss: 0.550071, acc.: 69.53%] [G loss: 1.429895]\n",
      "epoch:24 step:23418 [D loss: 0.450154, acc.: 78.12%] [G loss: 1.156407]\n",
      "epoch:24 step:23419 [D loss: 0.529605, acc.: 68.75%] [G loss: 1.084093]\n",
      "epoch:24 step:23420 [D loss: 0.615282, acc.: 63.28%] [G loss: 0.722155]\n",
      "epoch:24 step:23421 [D loss: 0.499867, acc.: 73.44%] [G loss: 0.659799]\n",
      "epoch:24 step:23422 [D loss: 0.579743, acc.: 60.16%] [G loss: 0.895991]\n",
      "epoch:24 step:23423 [D loss: 0.460080, acc.: 78.12%] [G loss: 1.041509]\n",
      "epoch:24 step:23424 [D loss: 0.332516, acc.: 89.84%] [G loss: 1.111667]\n",
      "epoch:24 step:23425 [D loss: 0.484498, acc.: 78.12%] [G loss: 1.467141]\n",
      "epoch:25 step:23426 [D loss: 0.499578, acc.: 77.34%] [G loss: 1.206956]\n",
      "epoch:25 step:23427 [D loss: 0.484401, acc.: 77.34%] [G loss: 1.288074]\n",
      "epoch:25 step:23428 [D loss: 0.559271, acc.: 72.66%] [G loss: 1.173956]\n",
      "epoch:25 step:23429 [D loss: 0.556146, acc.: 70.31%] [G loss: 0.785485]\n",
      "epoch:25 step:23430 [D loss: 0.527202, acc.: 75.00%] [G loss: 0.954401]\n",
      "epoch:25 step:23431 [D loss: 0.628347, acc.: 64.84%] [G loss: 0.924598]\n",
      "epoch:25 step:23432 [D loss: 0.542593, acc.: 74.22%] [G loss: 0.657696]\n",
      "epoch:25 step:23433 [D loss: 0.496218, acc.: 73.44%] [G loss: 0.800992]\n",
      "epoch:25 step:23434 [D loss: 0.493214, acc.: 71.88%] [G loss: 0.791215]\n",
      "epoch:25 step:23435 [D loss: 0.536758, acc.: 73.44%] [G loss: 0.625790]\n",
      "epoch:25 step:23436 [D loss: 0.450049, acc.: 78.91%] [G loss: 0.709428]\n",
      "epoch:25 step:23437 [D loss: 0.652043, acc.: 65.62%] [G loss: 0.774987]\n",
      "epoch:25 step:23438 [D loss: 0.533059, acc.: 71.09%] [G loss: 0.842203]\n",
      "epoch:25 step:23439 [D loss: 0.559261, acc.: 70.31%] [G loss: 0.657941]\n",
      "epoch:25 step:23440 [D loss: 0.463612, acc.: 74.22%] [G loss: 0.786622]\n",
      "epoch:25 step:23441 [D loss: 0.511430, acc.: 74.22%] [G loss: 0.646675]\n",
      "epoch:25 step:23442 [D loss: 0.541212, acc.: 71.09%] [G loss: 0.732840]\n",
      "epoch:25 step:23443 [D loss: 0.569996, acc.: 71.09%] [G loss: 0.589090]\n",
      "epoch:25 step:23444 [D loss: 0.543236, acc.: 71.09%] [G loss: 0.701864]\n",
      "epoch:25 step:23445 [D loss: 0.618839, acc.: 63.28%] [G loss: 0.726267]\n",
      "epoch:25 step:23446 [D loss: 0.557960, acc.: 67.97%] [G loss: 0.681649]\n",
      "epoch:25 step:23447 [D loss: 0.478287, acc.: 75.78%] [G loss: 0.938320]\n",
      "epoch:25 step:23448 [D loss: 0.585814, acc.: 64.84%] [G loss: 0.808918]\n",
      "epoch:25 step:23449 [D loss: 0.576582, acc.: 71.09%] [G loss: 0.760659]\n",
      "epoch:25 step:23450 [D loss: 0.505877, acc.: 73.44%] [G loss: 0.817531]\n",
      "epoch:25 step:23451 [D loss: 0.586834, acc.: 64.84%] [G loss: 0.780751]\n",
      "epoch:25 step:23452 [D loss: 0.445625, acc.: 75.00%] [G loss: 0.790527]\n",
      "epoch:25 step:23453 [D loss: 0.616754, acc.: 64.06%] [G loss: 0.660830]\n",
      "epoch:25 step:23454 [D loss: 0.482507, acc.: 75.00%] [G loss: 0.683823]\n",
      "epoch:25 step:23455 [D loss: 0.535616, acc.: 71.88%] [G loss: 0.555739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23456 [D loss: 0.632466, acc.: 64.84%] [G loss: 0.510054]\n",
      "epoch:25 step:23457 [D loss: 0.553921, acc.: 71.88%] [G loss: 0.666659]\n",
      "epoch:25 step:23458 [D loss: 0.507222, acc.: 72.66%] [G loss: 0.702221]\n",
      "epoch:25 step:23459 [D loss: 0.543385, acc.: 67.97%] [G loss: 0.734472]\n",
      "epoch:25 step:23460 [D loss: 0.560561, acc.: 70.31%] [G loss: 0.814101]\n",
      "epoch:25 step:23461 [D loss: 0.497050, acc.: 75.00%] [G loss: 0.678127]\n",
      "epoch:25 step:23462 [D loss: 0.535605, acc.: 70.31%] [G loss: 0.876969]\n",
      "epoch:25 step:23463 [D loss: 0.586929, acc.: 66.41%] [G loss: 0.795942]\n",
      "epoch:25 step:23464 [D loss: 0.552545, acc.: 73.44%] [G loss: 0.833761]\n",
      "epoch:25 step:23465 [D loss: 0.461246, acc.: 77.34%] [G loss: 0.745694]\n",
      "epoch:25 step:23466 [D loss: 0.518305, acc.: 74.22%] [G loss: 0.591529]\n",
      "epoch:25 step:23467 [D loss: 0.613459, acc.: 63.28%] [G loss: 0.619668]\n",
      "epoch:25 step:23468 [D loss: 0.496605, acc.: 75.00%] [G loss: 0.626233]\n",
      "epoch:25 step:23469 [D loss: 0.534184, acc.: 75.78%] [G loss: 0.704440]\n",
      "epoch:25 step:23470 [D loss: 0.476491, acc.: 75.00%] [G loss: 0.797775]\n",
      "epoch:25 step:23471 [D loss: 0.510257, acc.: 71.88%] [G loss: 0.918883]\n",
      "epoch:25 step:23472 [D loss: 0.537708, acc.: 71.88%] [G loss: 0.799903]\n",
      "epoch:25 step:23473 [D loss: 0.492562, acc.: 75.78%] [G loss: 0.909244]\n",
      "epoch:25 step:23474 [D loss: 0.447575, acc.: 80.47%] [G loss: 0.834914]\n",
      "epoch:25 step:23475 [D loss: 0.528526, acc.: 70.31%] [G loss: 0.676209]\n",
      "epoch:25 step:23476 [D loss: 0.576673, acc.: 64.06%] [G loss: 0.615487]\n",
      "epoch:25 step:23477 [D loss: 0.595015, acc.: 64.06%] [G loss: 0.484552]\n",
      "epoch:25 step:23478 [D loss: 0.483002, acc.: 75.00%] [G loss: 0.696606]\n",
      "epoch:25 step:23479 [D loss: 0.498079, acc.: 75.78%] [G loss: 0.839244]\n",
      "epoch:25 step:23480 [D loss: 0.544627, acc.: 71.88%] [G loss: 0.730527]\n",
      "epoch:25 step:23481 [D loss: 0.519867, acc.: 74.22%] [G loss: 0.823259]\n",
      "epoch:25 step:23482 [D loss: 0.463270, acc.: 74.22%] [G loss: 0.855647]\n",
      "epoch:25 step:23483 [D loss: 0.520051, acc.: 68.75%] [G loss: 0.728206]\n",
      "epoch:25 step:23484 [D loss: 0.461098, acc.: 80.47%] [G loss: 0.735546]\n",
      "epoch:25 step:23485 [D loss: 0.577538, acc.: 62.50%] [G loss: 0.775609]\n",
      "epoch:25 step:23486 [D loss: 0.522079, acc.: 66.41%] [G loss: 0.867648]\n",
      "epoch:25 step:23487 [D loss: 0.549488, acc.: 71.09%] [G loss: 0.596364]\n",
      "epoch:25 step:23488 [D loss: 0.581652, acc.: 65.62%] [G loss: 0.642372]\n",
      "epoch:25 step:23489 [D loss: 0.525606, acc.: 70.31%] [G loss: 0.605768]\n",
      "epoch:25 step:23490 [D loss: 0.513276, acc.: 75.78%] [G loss: 0.667501]\n",
      "epoch:25 step:23491 [D loss: 0.538028, acc.: 73.44%] [G loss: 0.708330]\n",
      "epoch:25 step:23492 [D loss: 0.531112, acc.: 69.53%] [G loss: 0.712684]\n",
      "epoch:25 step:23493 [D loss: 0.549937, acc.: 67.97%] [G loss: 0.685657]\n",
      "epoch:25 step:23494 [D loss: 0.522795, acc.: 71.88%] [G loss: 0.779886]\n",
      "epoch:25 step:23495 [D loss: 0.547838, acc.: 67.19%] [G loss: 0.713214]\n",
      "epoch:25 step:23496 [D loss: 0.516443, acc.: 72.66%] [G loss: 0.956222]\n",
      "epoch:25 step:23497 [D loss: 0.577330, acc.: 65.62%] [G loss: 0.612854]\n",
      "epoch:25 step:23498 [D loss: 0.523830, acc.: 73.44%] [G loss: 0.879278]\n",
      "epoch:25 step:23499 [D loss: 0.502162, acc.: 73.44%] [G loss: 0.848406]\n",
      "epoch:25 step:23500 [D loss: 0.514685, acc.: 72.66%] [G loss: 0.833430]\n",
      "epoch:25 step:23501 [D loss: 0.547862, acc.: 67.19%] [G loss: 0.906769]\n",
      "epoch:25 step:23502 [D loss: 0.494983, acc.: 74.22%] [G loss: 0.711627]\n",
      "epoch:25 step:23503 [D loss: 0.602434, acc.: 64.84%] [G loss: 0.633036]\n",
      "epoch:25 step:23504 [D loss: 0.574004, acc.: 69.53%] [G loss: 0.648212]\n",
      "epoch:25 step:23505 [D loss: 0.538553, acc.: 70.31%] [G loss: 0.607360]\n",
      "epoch:25 step:23506 [D loss: 0.520226, acc.: 70.31%] [G loss: 0.751610]\n",
      "epoch:25 step:23507 [D loss: 0.484125, acc.: 71.09%] [G loss: 0.705686]\n",
      "epoch:25 step:23508 [D loss: 0.440303, acc.: 79.69%] [G loss: 0.677359]\n",
      "epoch:25 step:23509 [D loss: 0.508876, acc.: 67.97%] [G loss: 0.801537]\n",
      "epoch:25 step:23510 [D loss: 0.552183, acc.: 71.88%] [G loss: 0.624739]\n",
      "epoch:25 step:23511 [D loss: 0.493211, acc.: 75.78%] [G loss: 0.642988]\n",
      "epoch:25 step:23512 [D loss: 0.514582, acc.: 74.22%] [G loss: 0.711114]\n",
      "epoch:25 step:23513 [D loss: 0.541709, acc.: 69.53%] [G loss: 0.665129]\n",
      "epoch:25 step:23514 [D loss: 0.489262, acc.: 75.00%] [G loss: 0.664433]\n",
      "epoch:25 step:23515 [D loss: 0.511574, acc.: 75.78%] [G loss: 0.718056]\n",
      "epoch:25 step:23516 [D loss: 0.553529, acc.: 71.88%] [G loss: 0.770619]\n",
      "epoch:25 step:23517 [D loss: 0.475792, acc.: 76.56%] [G loss: 0.818524]\n",
      "epoch:25 step:23518 [D loss: 0.550682, acc.: 71.88%] [G loss: 0.798773]\n",
      "epoch:25 step:23519 [D loss: 0.489030, acc.: 76.56%] [G loss: 0.828601]\n",
      "epoch:25 step:23520 [D loss: 0.568385, acc.: 66.41%] [G loss: 0.913614]\n",
      "epoch:25 step:23521 [D loss: 0.464765, acc.: 75.78%] [G loss: 0.892542]\n",
      "epoch:25 step:23522 [D loss: 0.543589, acc.: 68.75%] [G loss: 0.831745]\n",
      "epoch:25 step:23523 [D loss: 0.576172, acc.: 64.84%] [G loss: 0.889126]\n",
      "epoch:25 step:23524 [D loss: 0.515192, acc.: 77.34%] [G loss: 0.836555]\n",
      "epoch:25 step:23525 [D loss: 0.493575, acc.: 78.12%] [G loss: 0.989978]\n",
      "epoch:25 step:23526 [D loss: 0.559997, acc.: 69.53%] [G loss: 0.919365]\n",
      "epoch:25 step:23527 [D loss: 0.623219, acc.: 67.19%] [G loss: 0.615056]\n",
      "epoch:25 step:23528 [D loss: 0.542119, acc.: 68.75%] [G loss: 0.554542]\n",
      "epoch:25 step:23529 [D loss: 0.513607, acc.: 72.66%] [G loss: 0.585695]\n",
      "epoch:25 step:23530 [D loss: 0.634358, acc.: 63.28%] [G loss: 0.546872]\n",
      "epoch:25 step:23531 [D loss: 0.507146, acc.: 69.53%] [G loss: 0.588463]\n",
      "epoch:25 step:23532 [D loss: 0.552460, acc.: 74.22%] [G loss: 0.562185]\n",
      "epoch:25 step:23533 [D loss: 0.653496, acc.: 61.72%] [G loss: 0.667625]\n",
      "epoch:25 step:23534 [D loss: 0.543121, acc.: 70.31%] [G loss: 0.447328]\n",
      "epoch:25 step:23535 [D loss: 0.550955, acc.: 71.88%] [G loss: 0.616946]\n",
      "epoch:25 step:23536 [D loss: 0.489256, acc.: 75.78%] [G loss: 0.577297]\n",
      "epoch:25 step:23537 [D loss: 0.499227, acc.: 73.44%] [G loss: 0.711125]\n",
      "epoch:25 step:23538 [D loss: 0.550418, acc.: 75.00%] [G loss: 0.817829]\n",
      "epoch:25 step:23539 [D loss: 0.542480, acc.: 71.09%] [G loss: 0.682962]\n",
      "epoch:25 step:23540 [D loss: 0.467877, acc.: 80.47%] [G loss: 0.767288]\n",
      "epoch:25 step:23541 [D loss: 0.517063, acc.: 75.00%] [G loss: 0.664401]\n",
      "epoch:25 step:23542 [D loss: 0.549370, acc.: 65.62%] [G loss: 0.859901]\n",
      "epoch:25 step:23543 [D loss: 0.526583, acc.: 69.53%] [G loss: 0.811029]\n",
      "epoch:25 step:23544 [D loss: 0.471885, acc.: 80.47%] [G loss: 0.816697]\n",
      "epoch:25 step:23545 [D loss: 0.559085, acc.: 66.41%] [G loss: 0.835305]\n",
      "epoch:25 step:23546 [D loss: 0.555928, acc.: 71.09%] [G loss: 0.897423]\n",
      "epoch:25 step:23547 [D loss: 0.513478, acc.: 74.22%] [G loss: 0.835014]\n",
      "epoch:25 step:23548 [D loss: 0.509935, acc.: 75.78%] [G loss: 0.843686]\n",
      "epoch:25 step:23549 [D loss: 0.566329, acc.: 71.88%] [G loss: 0.798532]\n",
      "epoch:25 step:23550 [D loss: 0.583278, acc.: 66.41%] [G loss: 0.607967]\n",
      "epoch:25 step:23551 [D loss: 0.539014, acc.: 65.62%] [G loss: 0.746846]\n",
      "epoch:25 step:23552 [D loss: 0.500105, acc.: 74.22%] [G loss: 0.721400]\n",
      "epoch:25 step:23553 [D loss: 0.512191, acc.: 71.09%] [G loss: 0.515966]\n",
      "epoch:25 step:23554 [D loss: 0.541806, acc.: 71.88%] [G loss: 0.660147]\n",
      "epoch:25 step:23555 [D loss: 0.463793, acc.: 77.34%] [G loss: 0.691581]\n",
      "epoch:25 step:23556 [D loss: 0.496998, acc.: 70.31%] [G loss: 0.600938]\n",
      "epoch:25 step:23557 [D loss: 0.567112, acc.: 68.75%] [G loss: 0.659245]\n",
      "epoch:25 step:23558 [D loss: 0.550879, acc.: 70.31%] [G loss: 0.765626]\n",
      "epoch:25 step:23559 [D loss: 0.513821, acc.: 71.88%] [G loss: 0.809000]\n",
      "epoch:25 step:23560 [D loss: 0.564647, acc.: 70.31%] [G loss: 0.838699]\n",
      "epoch:25 step:23561 [D loss: 0.528940, acc.: 75.00%] [G loss: 0.723027]\n",
      "epoch:25 step:23562 [D loss: 0.596710, acc.: 68.75%] [G loss: 0.790348]\n",
      "epoch:25 step:23563 [D loss: 0.601730, acc.: 64.84%] [G loss: 0.570114]\n",
      "epoch:25 step:23564 [D loss: 0.495845, acc.: 78.91%] [G loss: 0.758693]\n",
      "epoch:25 step:23565 [D loss: 0.613669, acc.: 67.19%] [G loss: 0.647328]\n",
      "epoch:25 step:23566 [D loss: 0.543134, acc.: 68.75%] [G loss: 0.635174]\n",
      "epoch:25 step:23567 [D loss: 0.544327, acc.: 71.88%] [G loss: 0.609091]\n",
      "epoch:25 step:23568 [D loss: 0.561451, acc.: 67.97%] [G loss: 0.549755]\n",
      "epoch:25 step:23569 [D loss: 0.545522, acc.: 66.41%] [G loss: 0.603619]\n",
      "epoch:25 step:23570 [D loss: 0.568124, acc.: 67.19%] [G loss: 0.590885]\n",
      "epoch:25 step:23571 [D loss: 0.498421, acc.: 71.88%] [G loss: 0.668910]\n",
      "epoch:25 step:23572 [D loss: 0.615290, acc.: 66.41%] [G loss: 0.633235]\n",
      "epoch:25 step:23573 [D loss: 0.604226, acc.: 64.06%] [G loss: 0.683755]\n",
      "epoch:25 step:23574 [D loss: 0.527029, acc.: 69.53%] [G loss: 0.724278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23575 [D loss: 0.526674, acc.: 73.44%] [G loss: 0.721263]\n",
      "epoch:25 step:23576 [D loss: 0.559130, acc.: 69.53%] [G loss: 0.717611]\n",
      "epoch:25 step:23577 [D loss: 0.479502, acc.: 71.09%] [G loss: 0.717180]\n",
      "epoch:25 step:23578 [D loss: 0.594825, acc.: 67.97%] [G loss: 0.735355]\n",
      "epoch:25 step:23579 [D loss: 0.569260, acc.: 68.75%] [G loss: 0.702987]\n",
      "epoch:25 step:23580 [D loss: 0.447439, acc.: 78.12%] [G loss: 0.659127]\n",
      "epoch:25 step:23581 [D loss: 0.499128, acc.: 73.44%] [G loss: 0.759405]\n",
      "epoch:25 step:23582 [D loss: 0.562467, acc.: 69.53%] [G loss: 0.715948]\n",
      "epoch:25 step:23583 [D loss: 0.555039, acc.: 70.31%] [G loss: 0.575093]\n",
      "epoch:25 step:23584 [D loss: 0.509597, acc.: 71.88%] [G loss: 0.762141]\n",
      "epoch:25 step:23585 [D loss: 0.531057, acc.: 70.31%] [G loss: 0.726857]\n",
      "epoch:25 step:23586 [D loss: 0.517584, acc.: 70.31%] [G loss: 0.740839]\n",
      "epoch:25 step:23587 [D loss: 0.544357, acc.: 69.53%] [G loss: 0.853149]\n",
      "epoch:25 step:23588 [D loss: 0.528179, acc.: 73.44%] [G loss: 0.723920]\n",
      "epoch:25 step:23589 [D loss: 0.539153, acc.: 70.31%] [G loss: 0.762115]\n",
      "epoch:25 step:23590 [D loss: 0.505728, acc.: 73.44%] [G loss: 0.762907]\n",
      "epoch:25 step:23591 [D loss: 0.616621, acc.: 63.28%] [G loss: 0.618641]\n",
      "epoch:25 step:23592 [D loss: 0.578981, acc.: 64.06%] [G loss: 0.541048]\n",
      "epoch:25 step:23593 [D loss: 0.537253, acc.: 68.75%] [G loss: 0.652418]\n",
      "epoch:25 step:23594 [D loss: 0.582271, acc.: 67.97%] [G loss: 0.657995]\n",
      "epoch:25 step:23595 [D loss: 0.589851, acc.: 66.41%] [G loss: 0.519248]\n",
      "epoch:25 step:23596 [D loss: 0.513081, acc.: 77.34%] [G loss: 0.657682]\n",
      "epoch:25 step:23597 [D loss: 0.518845, acc.: 71.09%] [G loss: 0.709927]\n",
      "epoch:25 step:23598 [D loss: 0.518109, acc.: 75.00%] [G loss: 0.663118]\n",
      "epoch:25 step:23599 [D loss: 0.569802, acc.: 66.41%] [G loss: 0.715981]\n",
      "epoch:25 step:23600 [D loss: 0.600059, acc.: 64.06%] [G loss: 0.776644]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.719185\n",
      "FID: 49.231476\n",
      "0 = 12.887061182022082\n",
      "1 = 0.09615112080746505\n",
      "2 = 0.8496000170707703\n",
      "3 = 0.8047999739646912\n",
      "4 = 0.8944000005722046\n",
      "5 = 0.8840070366859436\n",
      "6 = 0.8047999739646912\n",
      "7 = 8.424316898620107\n",
      "8 = 0.15727280594206872\n",
      "9 = 0.6935999989509583\n",
      "10 = 0.6833999752998352\n",
      "11 = 0.7038000226020813\n",
      "12 = 0.6976316571235657\n",
      "13 = 0.6833999752998352\n",
      "14 = 6.71921443939209\n",
      "15 = 6.996787071228027\n",
      "16 = 0.3886471092700958\n",
      "17 = 6.719184875488281\n",
      "18 = 49.231475830078125\n",
      "epoch:25 step:23601 [D loss: 0.501836, acc.: 73.44%] [G loss: 0.771380]\n",
      "epoch:25 step:23602 [D loss: 0.503902, acc.: 76.56%] [G loss: 0.619320]\n",
      "epoch:25 step:23603 [D loss: 0.558016, acc.: 67.97%] [G loss: 0.556790]\n",
      "epoch:25 step:23604 [D loss: 0.523149, acc.: 75.00%] [G loss: 0.758071]\n",
      "epoch:25 step:23605 [D loss: 0.616007, acc.: 63.28%] [G loss: 0.643430]\n",
      "epoch:25 step:23606 [D loss: 0.538805, acc.: 75.00%] [G loss: 0.594859]\n",
      "epoch:25 step:23607 [D loss: 0.522071, acc.: 70.31%] [G loss: 0.714283]\n",
      "epoch:25 step:23608 [D loss: 0.589210, acc.: 65.62%] [G loss: 0.707785]\n",
      "epoch:25 step:23609 [D loss: 0.588352, acc.: 67.19%] [G loss: 0.657199]\n",
      "epoch:25 step:23610 [D loss: 0.534765, acc.: 68.75%] [G loss: 0.603973]\n",
      "epoch:25 step:23611 [D loss: 0.565738, acc.: 74.22%] [G loss: 0.743456]\n",
      "epoch:25 step:23612 [D loss: 0.589160, acc.: 61.72%] [G loss: 0.681178]\n",
      "epoch:25 step:23613 [D loss: 0.528451, acc.: 73.44%] [G loss: 0.644260]\n",
      "epoch:25 step:23614 [D loss: 0.571611, acc.: 70.31%] [G loss: 0.615698]\n",
      "epoch:25 step:23615 [D loss: 0.488546, acc.: 72.66%] [G loss: 0.685714]\n",
      "epoch:25 step:23616 [D loss: 0.506090, acc.: 74.22%] [G loss: 0.689021]\n",
      "epoch:25 step:23617 [D loss: 0.516946, acc.: 75.78%] [G loss: 0.671211]\n",
      "epoch:25 step:23618 [D loss: 0.552027, acc.: 68.75%] [G loss: 0.668113]\n",
      "epoch:25 step:23619 [D loss: 0.470161, acc.: 75.00%] [G loss: 0.802838]\n",
      "epoch:25 step:23620 [D loss: 0.590645, acc.: 66.41%] [G loss: 0.625224]\n",
      "epoch:25 step:23621 [D loss: 0.560538, acc.: 64.84%] [G loss: 0.644330]\n",
      "epoch:25 step:23622 [D loss: 0.495676, acc.: 75.78%] [G loss: 0.730961]\n",
      "epoch:25 step:23623 [D loss: 0.443766, acc.: 78.91%] [G loss: 0.771286]\n",
      "epoch:25 step:23624 [D loss: 0.486619, acc.: 73.44%] [G loss: 0.976725]\n",
      "epoch:25 step:23625 [D loss: 0.587815, acc.: 67.97%] [G loss: 0.706070]\n",
      "epoch:25 step:23626 [D loss: 0.569311, acc.: 70.31%] [G loss: 0.592677]\n",
      "epoch:25 step:23627 [D loss: 0.550097, acc.: 65.62%] [G loss: 0.660483]\n",
      "epoch:25 step:23628 [D loss: 0.651427, acc.: 64.84%] [G loss: 0.731061]\n",
      "epoch:25 step:23629 [D loss: 0.512686, acc.: 75.78%] [G loss: 0.899280]\n",
      "epoch:25 step:23630 [D loss: 0.476156, acc.: 78.12%] [G loss: 0.930956]\n",
      "epoch:25 step:23631 [D loss: 0.518560, acc.: 72.66%] [G loss: 0.842507]\n",
      "epoch:25 step:23632 [D loss: 0.461921, acc.: 77.34%] [G loss: 0.940930]\n",
      "epoch:25 step:23633 [D loss: 0.427461, acc.: 76.56%] [G loss: 0.980053]\n",
      "epoch:25 step:23634 [D loss: 0.499256, acc.: 73.44%] [G loss: 0.912131]\n",
      "epoch:25 step:23635 [D loss: 0.698315, acc.: 60.16%] [G loss: 0.506288]\n",
      "epoch:25 step:23636 [D loss: 0.588014, acc.: 67.19%] [G loss: 0.581232]\n",
      "epoch:25 step:23637 [D loss: 0.536299, acc.: 68.75%] [G loss: 0.563580]\n",
      "epoch:25 step:23638 [D loss: 0.555892, acc.: 67.97%] [G loss: 0.515398]\n",
      "epoch:25 step:23639 [D loss: 0.565943, acc.: 69.53%] [G loss: 0.699879]\n",
      "epoch:25 step:23640 [D loss: 0.596855, acc.: 65.62%] [G loss: 0.573539]\n",
      "epoch:25 step:23641 [D loss: 0.501120, acc.: 73.44%] [G loss: 0.575716]\n",
      "epoch:25 step:23642 [D loss: 0.553641, acc.: 67.97%] [G loss: 0.708618]\n",
      "epoch:25 step:23643 [D loss: 0.468371, acc.: 78.12%] [G loss: 0.832618]\n",
      "epoch:25 step:23644 [D loss: 0.414205, acc.: 85.16%] [G loss: 0.933009]\n",
      "epoch:25 step:23645 [D loss: 0.601607, acc.: 67.97%] [G loss: 0.735119]\n",
      "epoch:25 step:23646 [D loss: 0.591790, acc.: 64.06%] [G loss: 0.611867]\n",
      "epoch:25 step:23647 [D loss: 0.470454, acc.: 80.47%] [G loss: 1.042985]\n",
      "epoch:25 step:23648 [D loss: 0.552056, acc.: 71.09%] [G loss: 0.799941]\n",
      "epoch:25 step:23649 [D loss: 0.611974, acc.: 61.72%] [G loss: 0.666469]\n",
      "epoch:25 step:23650 [D loss: 0.456319, acc.: 80.47%] [G loss: 0.820855]\n",
      "epoch:25 step:23651 [D loss: 0.621411, acc.: 63.28%] [G loss: 0.636744]\n",
      "epoch:25 step:23652 [D loss: 0.504177, acc.: 74.22%] [G loss: 0.758999]\n",
      "epoch:25 step:23653 [D loss: 0.561905, acc.: 69.53%] [G loss: 0.632718]\n",
      "epoch:25 step:23654 [D loss: 0.568156, acc.: 71.88%] [G loss: 0.776628]\n",
      "epoch:25 step:23655 [D loss: 0.522138, acc.: 75.00%] [G loss: 0.784267]\n",
      "epoch:25 step:23656 [D loss: 0.464089, acc.: 75.78%] [G loss: 0.772941]\n",
      "epoch:25 step:23657 [D loss: 0.443571, acc.: 81.25%] [G loss: 1.022139]\n",
      "epoch:25 step:23658 [D loss: 0.504251, acc.: 71.09%] [G loss: 0.757129]\n",
      "epoch:25 step:23659 [D loss: 0.627755, acc.: 64.06%] [G loss: 0.795753]\n",
      "epoch:25 step:23660 [D loss: 0.553136, acc.: 67.97%] [G loss: 0.669451]\n",
      "epoch:25 step:23661 [D loss: 0.544555, acc.: 68.75%] [G loss: 0.679988]\n",
      "epoch:25 step:23662 [D loss: 0.521304, acc.: 72.66%] [G loss: 0.476449]\n",
      "epoch:25 step:23663 [D loss: 0.597434, acc.: 64.84%] [G loss: 0.629619]\n",
      "epoch:25 step:23664 [D loss: 0.524881, acc.: 67.97%] [G loss: 0.687650]\n",
      "epoch:25 step:23665 [D loss: 0.559475, acc.: 63.28%] [G loss: 0.713741]\n",
      "epoch:25 step:23666 [D loss: 0.530332, acc.: 71.09%] [G loss: 0.743994]\n",
      "epoch:25 step:23667 [D loss: 0.501024, acc.: 73.44%] [G loss: 0.666533]\n",
      "epoch:25 step:23668 [D loss: 0.552167, acc.: 66.41%] [G loss: 0.524373]\n",
      "epoch:25 step:23669 [D loss: 0.486028, acc.: 75.78%] [G loss: 0.807979]\n",
      "epoch:25 step:23670 [D loss: 0.564641, acc.: 65.62%] [G loss: 0.724072]\n",
      "epoch:25 step:23671 [D loss: 0.502661, acc.: 71.88%] [G loss: 0.714191]\n",
      "epoch:25 step:23672 [D loss: 0.468712, acc.: 77.34%] [G loss: 0.785631]\n",
      "epoch:25 step:23673 [D loss: 0.469521, acc.: 75.00%] [G loss: 0.861332]\n",
      "epoch:25 step:23674 [D loss: 0.557218, acc.: 70.31%] [G loss: 0.830392]\n",
      "epoch:25 step:23675 [D loss: 0.597613, acc.: 62.50%] [G loss: 0.679420]\n",
      "epoch:25 step:23676 [D loss: 0.680545, acc.: 59.38%] [G loss: 0.650698]\n",
      "epoch:25 step:23677 [D loss: 0.541059, acc.: 73.44%] [G loss: 0.657689]\n",
      "epoch:25 step:23678 [D loss: 0.535659, acc.: 70.31%] [G loss: 0.789317]\n",
      "epoch:25 step:23679 [D loss: 0.478273, acc.: 71.88%] [G loss: 0.828849]\n",
      "epoch:25 step:23680 [D loss: 0.553335, acc.: 67.97%] [G loss: 0.572129]\n",
      "epoch:25 step:23681 [D loss: 0.557091, acc.: 65.62%] [G loss: 0.640190]\n",
      "epoch:25 step:23682 [D loss: 0.544141, acc.: 67.97%] [G loss: 0.641249]\n",
      "epoch:25 step:23683 [D loss: 0.561189, acc.: 68.75%] [G loss: 0.472867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23684 [D loss: 0.544087, acc.: 69.53%] [G loss: 0.512367]\n",
      "epoch:25 step:23685 [D loss: 0.566696, acc.: 67.97%] [G loss: 0.609624]\n",
      "epoch:25 step:23686 [D loss: 0.552369, acc.: 70.31%] [G loss: 0.792560]\n",
      "epoch:25 step:23687 [D loss: 0.523867, acc.: 72.66%] [G loss: 0.605144]\n",
      "epoch:25 step:23688 [D loss: 0.551466, acc.: 73.44%] [G loss: 0.658341]\n",
      "epoch:25 step:23689 [D loss: 0.516924, acc.: 71.09%] [G loss: 0.660088]\n",
      "epoch:25 step:23690 [D loss: 0.530228, acc.: 72.66%] [G loss: 0.749523]\n",
      "epoch:25 step:23691 [D loss: 0.616038, acc.: 65.62%] [G loss: 0.727757]\n",
      "epoch:25 step:23692 [D loss: 0.576588, acc.: 67.19%] [G loss: 0.885734]\n",
      "epoch:25 step:23693 [D loss: 0.540877, acc.: 67.19%] [G loss: 0.676585]\n",
      "epoch:25 step:23694 [D loss: 0.551258, acc.: 74.22%] [G loss: 0.664854]\n",
      "epoch:25 step:23695 [D loss: 0.507927, acc.: 72.66%] [G loss: 0.643404]\n",
      "epoch:25 step:23696 [D loss: 0.473226, acc.: 75.78%] [G loss: 0.780797]\n",
      "epoch:25 step:23697 [D loss: 0.539349, acc.: 70.31%] [G loss: 0.720851]\n",
      "epoch:25 step:23698 [D loss: 0.524350, acc.: 72.66%] [G loss: 0.623147]\n",
      "epoch:25 step:23699 [D loss: 0.451753, acc.: 78.91%] [G loss: 0.707908]\n",
      "epoch:25 step:23700 [D loss: 0.539474, acc.: 69.53%] [G loss: 0.838753]\n",
      "epoch:25 step:23701 [D loss: 0.462347, acc.: 78.12%] [G loss: 0.906681]\n",
      "epoch:25 step:23702 [D loss: 0.637742, acc.: 64.06%] [G loss: 0.626644]\n",
      "epoch:25 step:23703 [D loss: 0.666480, acc.: 63.28%] [G loss: 0.610233]\n",
      "epoch:25 step:23704 [D loss: 0.545925, acc.: 70.31%] [G loss: 0.676907]\n",
      "epoch:25 step:23705 [D loss: 0.573437, acc.: 65.62%] [G loss: 0.569970]\n",
      "epoch:25 step:23706 [D loss: 0.573273, acc.: 69.53%] [G loss: 0.561610]\n",
      "epoch:25 step:23707 [D loss: 0.543721, acc.: 71.88%] [G loss: 0.589764]\n",
      "epoch:25 step:23708 [D loss: 0.493576, acc.: 77.34%] [G loss: 0.632242]\n",
      "epoch:25 step:23709 [D loss: 0.575372, acc.: 67.19%] [G loss: 0.572966]\n",
      "epoch:25 step:23710 [D loss: 0.576627, acc.: 69.53%] [G loss: 0.660474]\n",
      "epoch:25 step:23711 [D loss: 0.481801, acc.: 77.34%] [G loss: 0.542853]\n",
      "epoch:25 step:23712 [D loss: 0.588008, acc.: 66.41%] [G loss: 0.599631]\n",
      "epoch:25 step:23713 [D loss: 0.528200, acc.: 66.41%] [G loss: 0.647165]\n",
      "epoch:25 step:23714 [D loss: 0.538107, acc.: 70.31%] [G loss: 0.714598]\n",
      "epoch:25 step:23715 [D loss: 0.537687, acc.: 67.97%] [G loss: 0.729083]\n",
      "epoch:25 step:23716 [D loss: 0.622234, acc.: 65.62%] [G loss: 0.594130]\n",
      "epoch:25 step:23717 [D loss: 0.544965, acc.: 67.97%] [G loss: 0.685554]\n",
      "epoch:25 step:23718 [D loss: 0.606401, acc.: 64.84%] [G loss: 0.645604]\n",
      "epoch:25 step:23719 [D loss: 0.638343, acc.: 60.94%] [G loss: 0.511373]\n",
      "epoch:25 step:23720 [D loss: 0.525451, acc.: 74.22%] [G loss: 0.630017]\n",
      "epoch:25 step:23721 [D loss: 0.518944, acc.: 75.78%] [G loss: 0.660830]\n",
      "epoch:25 step:23722 [D loss: 0.543333, acc.: 70.31%] [G loss: 0.584827]\n",
      "epoch:25 step:23723 [D loss: 0.481201, acc.: 76.56%] [G loss: 0.606102]\n",
      "epoch:25 step:23724 [D loss: 0.475249, acc.: 77.34%] [G loss: 0.887607]\n",
      "epoch:25 step:23725 [D loss: 0.470587, acc.: 75.78%] [G loss: 0.666775]\n",
      "epoch:25 step:23726 [D loss: 0.643346, acc.: 64.84%] [G loss: 0.744835]\n",
      "epoch:25 step:23727 [D loss: 0.530565, acc.: 74.22%] [G loss: 0.770432]\n",
      "epoch:25 step:23728 [D loss: 0.507295, acc.: 76.56%] [G loss: 0.720609]\n",
      "epoch:25 step:23729 [D loss: 0.530624, acc.: 69.53%] [G loss: 0.632962]\n",
      "epoch:25 step:23730 [D loss: 0.533431, acc.: 70.31%] [G loss: 0.768070]\n",
      "epoch:25 step:23731 [D loss: 0.536676, acc.: 71.88%] [G loss: 0.820568]\n",
      "epoch:25 step:23732 [D loss: 0.522425, acc.: 73.44%] [G loss: 0.803262]\n",
      "epoch:25 step:23733 [D loss: 0.572732, acc.: 67.19%] [G loss: 0.651529]\n",
      "epoch:25 step:23734 [D loss: 0.538055, acc.: 67.97%] [G loss: 0.576143]\n",
      "epoch:25 step:23735 [D loss: 0.559677, acc.: 70.31%] [G loss: 0.690964]\n",
      "epoch:25 step:23736 [D loss: 0.501733, acc.: 71.09%] [G loss: 0.795688]\n",
      "epoch:25 step:23737 [D loss: 0.448675, acc.: 81.25%] [G loss: 0.778651]\n",
      "epoch:25 step:23738 [D loss: 0.538696, acc.: 69.53%] [G loss: 0.865722]\n",
      "epoch:25 step:23739 [D loss: 0.429748, acc.: 78.91%] [G loss: 1.251073]\n",
      "epoch:25 step:23740 [D loss: 0.470297, acc.: 79.69%] [G loss: 0.893091]\n",
      "epoch:25 step:23741 [D loss: 0.598021, acc.: 69.53%] [G loss: 0.777645]\n",
      "epoch:25 step:23742 [D loss: 0.556415, acc.: 74.22%] [G loss: 0.624155]\n",
      "epoch:25 step:23743 [D loss: 0.496473, acc.: 77.34%] [G loss: 0.619830]\n",
      "epoch:25 step:23744 [D loss: 0.561761, acc.: 64.06%] [G loss: 0.644477]\n",
      "epoch:25 step:23745 [D loss: 0.556378, acc.: 68.75%] [G loss: 0.730721]\n",
      "epoch:25 step:23746 [D loss: 0.528458, acc.: 71.09%] [G loss: 0.736025]\n",
      "epoch:25 step:23747 [D loss: 0.501016, acc.: 75.78%] [G loss: 0.639062]\n",
      "epoch:25 step:23748 [D loss: 0.603246, acc.: 68.75%] [G loss: 0.784330]\n",
      "epoch:25 step:23749 [D loss: 0.573092, acc.: 63.28%] [G loss: 0.450291]\n",
      "epoch:25 step:23750 [D loss: 0.555545, acc.: 68.75%] [G loss: 0.644425]\n",
      "epoch:25 step:23751 [D loss: 0.478157, acc.: 78.12%] [G loss: 0.750623]\n",
      "epoch:25 step:23752 [D loss: 0.504380, acc.: 75.00%] [G loss: 0.820762]\n",
      "epoch:25 step:23753 [D loss: 0.458656, acc.: 75.78%] [G loss: 0.881323]\n",
      "epoch:25 step:23754 [D loss: 0.528647, acc.: 71.09%] [G loss: 0.733408]\n",
      "epoch:25 step:23755 [D loss: 0.591207, acc.: 64.84%] [G loss: 0.614278]\n",
      "epoch:25 step:23756 [D loss: 0.555459, acc.: 71.09%] [G loss: 0.642443]\n",
      "epoch:25 step:23757 [D loss: 0.508681, acc.: 71.09%] [G loss: 0.533940]\n",
      "epoch:25 step:23758 [D loss: 0.510655, acc.: 75.78%] [G loss: 0.699022]\n",
      "epoch:25 step:23759 [D loss: 0.488123, acc.: 78.12%] [G loss: 0.802673]\n",
      "epoch:25 step:23760 [D loss: 0.523792, acc.: 70.31%] [G loss: 0.719514]\n",
      "epoch:25 step:23761 [D loss: 0.485200, acc.: 72.66%] [G loss: 0.783388]\n",
      "epoch:25 step:23762 [D loss: 0.532236, acc.: 70.31%] [G loss: 0.718039]\n",
      "epoch:25 step:23763 [D loss: 0.551222, acc.: 66.41%] [G loss: 0.760785]\n",
      "epoch:25 step:23764 [D loss: 0.568130, acc.: 65.62%] [G loss: 0.832882]\n",
      "epoch:25 step:23765 [D loss: 0.500916, acc.: 75.00%] [G loss: 0.796821]\n",
      "epoch:25 step:23766 [D loss: 0.616587, acc.: 67.19%] [G loss: 0.896074]\n",
      "epoch:25 step:23767 [D loss: 0.638796, acc.: 61.72%] [G loss: 0.688511]\n",
      "epoch:25 step:23768 [D loss: 0.484390, acc.: 75.00%] [G loss: 0.835419]\n",
      "epoch:25 step:23769 [D loss: 0.473839, acc.: 76.56%] [G loss: 0.891740]\n",
      "epoch:25 step:23770 [D loss: 0.583601, acc.: 66.41%] [G loss: 0.724495]\n",
      "epoch:25 step:23771 [D loss: 0.534660, acc.: 67.97%] [G loss: 0.822294]\n",
      "epoch:25 step:23772 [D loss: 0.501966, acc.: 74.22%] [G loss: 0.883207]\n",
      "epoch:25 step:23773 [D loss: 0.565775, acc.: 71.09%] [G loss: 0.907388]\n",
      "epoch:25 step:23774 [D loss: 0.722490, acc.: 55.47%] [G loss: 0.644402]\n",
      "epoch:25 step:23775 [D loss: 0.499803, acc.: 76.56%] [G loss: 0.645689]\n",
      "epoch:25 step:23776 [D loss: 0.532973, acc.: 67.97%] [G loss: 0.778698]\n",
      "epoch:25 step:23777 [D loss: 0.520793, acc.: 74.22%] [G loss: 0.691497]\n",
      "epoch:25 step:23778 [D loss: 0.530820, acc.: 71.88%] [G loss: 0.734983]\n",
      "epoch:25 step:23779 [D loss: 0.384194, acc.: 82.03%] [G loss: 0.879545]\n",
      "epoch:25 step:23780 [D loss: 0.533946, acc.: 72.66%] [G loss: 0.824250]\n",
      "epoch:25 step:23781 [D loss: 0.532759, acc.: 71.88%] [G loss: 0.912819]\n",
      "epoch:25 step:23782 [D loss: 0.420424, acc.: 83.59%] [G loss: 0.828165]\n",
      "epoch:25 step:23783 [D loss: 0.486321, acc.: 75.78%] [G loss: 0.920728]\n",
      "epoch:25 step:23784 [D loss: 0.477299, acc.: 76.56%] [G loss: 0.819668]\n",
      "epoch:25 step:23785 [D loss: 0.505804, acc.: 71.88%] [G loss: 0.901741]\n",
      "epoch:25 step:23786 [D loss: 0.533625, acc.: 70.31%] [G loss: 0.971171]\n",
      "epoch:25 step:23787 [D loss: 0.624040, acc.: 64.84%] [G loss: 0.857454]\n",
      "epoch:25 step:23788 [D loss: 0.575141, acc.: 64.84%] [G loss: 0.621849]\n",
      "epoch:25 step:23789 [D loss: 0.535752, acc.: 69.53%] [G loss: 0.796728]\n",
      "epoch:25 step:23790 [D loss: 0.584625, acc.: 68.75%] [G loss: 0.607440]\n",
      "epoch:25 step:23791 [D loss: 0.532888, acc.: 74.22%] [G loss: 0.704715]\n",
      "epoch:25 step:23792 [D loss: 0.603384, acc.: 60.16%] [G loss: 0.651206]\n",
      "epoch:25 step:23793 [D loss: 0.542164, acc.: 68.75%] [G loss: 0.711377]\n",
      "epoch:25 step:23794 [D loss: 0.521604, acc.: 70.31%] [G loss: 0.724752]\n",
      "epoch:25 step:23795 [D loss: 0.591819, acc.: 65.62%] [G loss: 0.695439]\n",
      "epoch:25 step:23796 [D loss: 0.532503, acc.: 73.44%] [G loss: 0.676254]\n",
      "epoch:25 step:23797 [D loss: 0.531137, acc.: 71.88%] [G loss: 0.767072]\n",
      "epoch:25 step:23798 [D loss: 0.511494, acc.: 78.91%] [G loss: 0.742866]\n",
      "epoch:25 step:23799 [D loss: 0.408587, acc.: 83.59%] [G loss: 0.830557]\n",
      "epoch:25 step:23800 [D loss: 0.563713, acc.: 71.09%] [G loss: 0.737489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.748375\n",
      "FID: 48.508949\n",
      "0 = 12.924059398746513\n",
      "1 = 0.08868053242772643\n",
      "2 = 0.8543000221252441\n",
      "3 = 0.8091999888420105\n",
      "4 = 0.899399995803833\n",
      "5 = 0.8894262313842773\n",
      "6 = 0.8091999888420105\n",
      "7 = 8.212974381971376\n",
      "8 = 0.1506147819630476\n",
      "9 = 0.7002000212669373\n",
      "10 = 0.6913999915122986\n",
      "11 = 0.7089999914169312\n",
      "12 = 0.7037866711616516\n",
      "13 = 0.6913999915122986\n",
      "14 = 6.748401641845703\n",
      "15 = 6.941415309906006\n",
      "16 = 0.38778796792030334\n",
      "17 = 6.748375415802002\n",
      "18 = 48.508949279785156\n",
      "epoch:25 step:23801 [D loss: 0.687313, acc.: 59.38%] [G loss: 0.432871]\n",
      "epoch:25 step:23802 [D loss: 0.563476, acc.: 65.62%] [G loss: 0.635200]\n",
      "epoch:25 step:23803 [D loss: 0.507373, acc.: 74.22%] [G loss: 0.627207]\n",
      "epoch:25 step:23804 [D loss: 0.587555, acc.: 64.84%] [G loss: 0.550804]\n",
      "epoch:25 step:23805 [D loss: 0.561975, acc.: 71.09%] [G loss: 0.659235]\n",
      "epoch:25 step:23806 [D loss: 0.454096, acc.: 78.12%] [G loss: 0.755505]\n",
      "epoch:25 step:23807 [D loss: 0.532225, acc.: 75.78%] [G loss: 0.813341]\n",
      "epoch:25 step:23808 [D loss: 0.523224, acc.: 68.75%] [G loss: 0.790259]\n",
      "epoch:25 step:23809 [D loss: 0.526990, acc.: 71.09%] [G loss: 0.699938]\n",
      "epoch:25 step:23810 [D loss: 0.479081, acc.: 76.56%] [G loss: 0.704305]\n",
      "epoch:25 step:23811 [D loss: 0.591617, acc.: 66.41%] [G loss: 0.631167]\n",
      "epoch:25 step:23812 [D loss: 0.567601, acc.: 69.53%] [G loss: 0.555410]\n",
      "epoch:25 step:23813 [D loss: 0.551860, acc.: 69.53%] [G loss: 0.735228]\n",
      "epoch:25 step:23814 [D loss: 0.509598, acc.: 75.00%] [G loss: 0.725071]\n",
      "epoch:25 step:23815 [D loss: 0.613951, acc.: 63.28%] [G loss: 0.517499]\n",
      "epoch:25 step:23816 [D loss: 0.528126, acc.: 75.00%] [G loss: 0.659349]\n",
      "epoch:25 step:23817 [D loss: 0.465630, acc.: 77.34%] [G loss: 0.693228]\n",
      "epoch:25 step:23818 [D loss: 0.534498, acc.: 70.31%] [G loss: 0.577677]\n",
      "epoch:25 step:23819 [D loss: 0.604324, acc.: 63.28%] [G loss: 0.581399]\n",
      "epoch:25 step:23820 [D loss: 0.543725, acc.: 70.31%] [G loss: 0.753156]\n",
      "epoch:25 step:23821 [D loss: 0.544310, acc.: 70.31%] [G loss: 0.767580]\n",
      "epoch:25 step:23822 [D loss: 0.586442, acc.: 64.84%] [G loss: 0.785339]\n",
      "epoch:25 step:23823 [D loss: 0.476208, acc.: 81.25%] [G loss: 0.875667]\n",
      "epoch:25 step:23824 [D loss: 0.573640, acc.: 70.31%] [G loss: 0.944622]\n",
      "epoch:25 step:23825 [D loss: 0.640639, acc.: 61.72%] [G loss: 0.815198]\n",
      "epoch:25 step:23826 [D loss: 0.668638, acc.: 60.94%] [G loss: 0.449572]\n",
      "epoch:25 step:23827 [D loss: 0.520797, acc.: 73.44%] [G loss: 0.712604]\n",
      "epoch:25 step:23828 [D loss: 0.528729, acc.: 70.31%] [G loss: 0.739646]\n",
      "epoch:25 step:23829 [D loss: 0.623463, acc.: 61.72%] [G loss: 0.747552]\n",
      "epoch:25 step:23830 [D loss: 0.557224, acc.: 63.28%] [G loss: 0.532062]\n",
      "epoch:25 step:23831 [D loss: 0.452096, acc.: 77.34%] [G loss: 0.949326]\n",
      "epoch:25 step:23832 [D loss: 0.574913, acc.: 65.62%] [G loss: 0.680860]\n",
      "epoch:25 step:23833 [D loss: 0.534454, acc.: 71.09%] [G loss: 0.763408]\n",
      "epoch:25 step:23834 [D loss: 0.581459, acc.: 64.84%] [G loss: 0.674032]\n",
      "epoch:25 step:23835 [D loss: 0.551528, acc.: 66.41%] [G loss: 0.634923]\n",
      "epoch:25 step:23836 [D loss: 0.592611, acc.: 62.50%] [G loss: 0.721071]\n",
      "epoch:25 step:23837 [D loss: 0.534137, acc.: 69.53%] [G loss: 0.535055]\n",
      "epoch:25 step:23838 [D loss: 0.577965, acc.: 62.50%] [G loss: 0.468124]\n",
      "epoch:25 step:23839 [D loss: 0.493317, acc.: 73.44%] [G loss: 0.724141]\n",
      "epoch:25 step:23840 [D loss: 0.536201, acc.: 71.88%] [G loss: 0.798487]\n",
      "epoch:25 step:23841 [D loss: 0.472206, acc.: 79.69%] [G loss: 0.876191]\n",
      "epoch:25 step:23842 [D loss: 0.550840, acc.: 71.88%] [G loss: 0.921121]\n",
      "epoch:25 step:23843 [D loss: 0.607459, acc.: 66.41%] [G loss: 0.734464]\n",
      "epoch:25 step:23844 [D loss: 0.557484, acc.: 71.88%] [G loss: 0.731269]\n",
      "epoch:25 step:23845 [D loss: 0.630209, acc.: 57.03%] [G loss: 0.724056]\n",
      "epoch:25 step:23846 [D loss: 0.567972, acc.: 69.53%] [G loss: 0.790063]\n",
      "epoch:25 step:23847 [D loss: 0.566512, acc.: 69.53%] [G loss: 0.682118]\n",
      "epoch:25 step:23848 [D loss: 0.568866, acc.: 67.97%] [G loss: 0.576818]\n",
      "epoch:25 step:23849 [D loss: 0.579141, acc.: 66.41%] [G loss: 0.713706]\n",
      "epoch:25 step:23850 [D loss: 0.525700, acc.: 78.12%] [G loss: 0.842215]\n",
      "epoch:25 step:23851 [D loss: 0.441835, acc.: 76.56%] [G loss: 0.842250]\n",
      "epoch:25 step:23852 [D loss: 0.539076, acc.: 72.66%] [G loss: 0.895854]\n",
      "epoch:25 step:23853 [D loss: 0.547175, acc.: 71.09%] [G loss: 0.791702]\n",
      "epoch:25 step:23854 [D loss: 0.501347, acc.: 75.78%] [G loss: 0.864332]\n",
      "epoch:25 step:23855 [D loss: 0.544405, acc.: 67.97%] [G loss: 0.890870]\n",
      "epoch:25 step:23856 [D loss: 0.529963, acc.: 67.19%] [G loss: 0.871015]\n",
      "epoch:25 step:23857 [D loss: 0.560245, acc.: 67.19%] [G loss: 0.899158]\n",
      "epoch:25 step:23858 [D loss: 0.551018, acc.: 70.31%] [G loss: 0.685393]\n",
      "epoch:25 step:23859 [D loss: 0.523891, acc.: 74.22%] [G loss: 0.778678]\n",
      "epoch:25 step:23860 [D loss: 0.509793, acc.: 74.22%] [G loss: 0.645630]\n",
      "epoch:25 step:23861 [D loss: 0.451804, acc.: 73.44%] [G loss: 0.797933]\n",
      "epoch:25 step:23862 [D loss: 0.637194, acc.: 70.31%] [G loss: 0.708403]\n",
      "epoch:25 step:23863 [D loss: 0.584738, acc.: 67.19%] [G loss: 0.683635]\n",
      "epoch:25 step:23864 [D loss: 0.532791, acc.: 72.66%] [G loss: 0.595228]\n",
      "epoch:25 step:23865 [D loss: 0.504104, acc.: 76.56%] [G loss: 0.760583]\n",
      "epoch:25 step:23866 [D loss: 0.519385, acc.: 72.66%] [G loss: 0.803619]\n",
      "epoch:25 step:23867 [D loss: 0.550172, acc.: 68.75%] [G loss: 0.770995]\n",
      "epoch:25 step:23868 [D loss: 0.555524, acc.: 67.97%] [G loss: 0.727913]\n",
      "epoch:25 step:23869 [D loss: 0.509011, acc.: 76.56%] [G loss: 0.813004]\n",
      "epoch:25 step:23870 [D loss: 0.568837, acc.: 67.19%] [G loss: 0.858434]\n",
      "epoch:25 step:23871 [D loss: 0.511086, acc.: 72.66%] [G loss: 0.799810]\n",
      "epoch:25 step:23872 [D loss: 0.549845, acc.: 66.41%] [G loss: 0.605862]\n",
      "epoch:25 step:23873 [D loss: 0.480306, acc.: 80.47%] [G loss: 0.835068]\n",
      "epoch:25 step:23874 [D loss: 0.481050, acc.: 77.34%] [G loss: 0.747398]\n",
      "epoch:25 step:23875 [D loss: 0.481798, acc.: 75.78%] [G loss: 0.741038]\n",
      "epoch:25 step:23876 [D loss: 0.449740, acc.: 79.69%] [G loss: 0.969665]\n",
      "epoch:25 step:23877 [D loss: 0.474918, acc.: 75.00%] [G loss: 1.127120]\n",
      "epoch:25 step:23878 [D loss: 0.526572, acc.: 74.22%] [G loss: 0.927875]\n",
      "epoch:25 step:23879 [D loss: 0.538305, acc.: 72.66%] [G loss: 0.723238]\n",
      "epoch:25 step:23880 [D loss: 0.569176, acc.: 68.75%] [G loss: 0.765884]\n",
      "epoch:25 step:23881 [D loss: 0.672494, acc.: 56.25%] [G loss: 0.593288]\n",
      "epoch:25 step:23882 [D loss: 0.443933, acc.: 82.03%] [G loss: 0.692728]\n",
      "epoch:25 step:23883 [D loss: 0.659760, acc.: 62.50%] [G loss: 0.704974]\n",
      "epoch:25 step:23884 [D loss: 0.539544, acc.: 67.97%] [G loss: 0.708509]\n",
      "epoch:25 step:23885 [D loss: 0.484304, acc.: 78.12%] [G loss: 0.841166]\n",
      "epoch:25 step:23886 [D loss: 0.556347, acc.: 67.97%] [G loss: 0.733698]\n",
      "epoch:25 step:23887 [D loss: 0.555418, acc.: 71.09%] [G loss: 0.630671]\n",
      "epoch:25 step:23888 [D loss: 0.586444, acc.: 60.16%] [G loss: 0.585250]\n",
      "epoch:25 step:23889 [D loss: 0.492983, acc.: 77.34%] [G loss: 0.692319]\n",
      "epoch:25 step:23890 [D loss: 0.592401, acc.: 66.41%] [G loss: 0.632598]\n",
      "epoch:25 step:23891 [D loss: 0.591666, acc.: 68.75%] [G loss: 0.606982]\n",
      "epoch:25 step:23892 [D loss: 0.536119, acc.: 71.09%] [G loss: 0.582192]\n",
      "epoch:25 step:23893 [D loss: 0.524066, acc.: 73.44%] [G loss: 0.731402]\n",
      "epoch:25 step:23894 [D loss: 0.578352, acc.: 67.97%] [G loss: 0.743584]\n",
      "epoch:25 step:23895 [D loss: 0.556881, acc.: 71.09%] [G loss: 0.721469]\n",
      "epoch:25 step:23896 [D loss: 0.436916, acc.: 82.03%] [G loss: 1.028888]\n",
      "epoch:25 step:23897 [D loss: 0.443692, acc.: 78.91%] [G loss: 0.991158]\n",
      "epoch:25 step:23898 [D loss: 0.648885, acc.: 61.72%] [G loss: 0.751143]\n",
      "epoch:25 step:23899 [D loss: 0.546011, acc.: 70.31%] [G loss: 0.694340]\n",
      "epoch:25 step:23900 [D loss: 0.454841, acc.: 82.03%] [G loss: 0.921811]\n",
      "epoch:25 step:23901 [D loss: 0.562916, acc.: 69.53%] [G loss: 0.888455]\n",
      "epoch:25 step:23902 [D loss: 0.630273, acc.: 67.19%] [G loss: 0.784614]\n",
      "epoch:25 step:23903 [D loss: 0.595376, acc.: 69.53%] [G loss: 0.502646]\n",
      "epoch:25 step:23904 [D loss: 0.578019, acc.: 69.53%] [G loss: 0.653005]\n",
      "epoch:25 step:23905 [D loss: 0.562140, acc.: 71.09%] [G loss: 0.749584]\n",
      "epoch:25 step:23906 [D loss: 0.490770, acc.: 79.69%] [G loss: 0.738330]\n",
      "epoch:25 step:23907 [D loss: 0.614186, acc.: 70.31%] [G loss: 0.646483]\n",
      "epoch:25 step:23908 [D loss: 0.552016, acc.: 67.19%] [G loss: 0.711106]\n",
      "epoch:25 step:23909 [D loss: 0.503945, acc.: 75.78%] [G loss: 0.719338]\n",
      "epoch:25 step:23910 [D loss: 0.489542, acc.: 78.12%] [G loss: 0.611684]\n",
      "epoch:25 step:23911 [D loss: 0.485428, acc.: 78.12%] [G loss: 0.761258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23912 [D loss: 0.564029, acc.: 65.62%] [G loss: 0.634863]\n",
      "epoch:25 step:23913 [D loss: 0.489593, acc.: 73.44%] [G loss: 0.568866]\n",
      "epoch:25 step:23914 [D loss: 0.531620, acc.: 71.09%] [G loss: 0.731864]\n",
      "epoch:25 step:23915 [D loss: 0.609881, acc.: 68.75%] [G loss: 0.740967]\n",
      "epoch:25 step:23916 [D loss: 0.518722, acc.: 76.56%] [G loss: 0.638173]\n",
      "epoch:25 step:23917 [D loss: 0.531314, acc.: 72.66%] [G loss: 0.641971]\n",
      "epoch:25 step:23918 [D loss: 0.524607, acc.: 73.44%] [G loss: 0.686064]\n",
      "epoch:25 step:23919 [D loss: 0.573632, acc.: 69.53%] [G loss: 0.698075]\n",
      "epoch:25 step:23920 [D loss: 0.500904, acc.: 73.44%] [G loss: 0.689127]\n",
      "epoch:25 step:23921 [D loss: 0.542611, acc.: 72.66%] [G loss: 0.638353]\n",
      "epoch:25 step:23922 [D loss: 0.577816, acc.: 70.31%] [G loss: 0.763132]\n",
      "epoch:25 step:23923 [D loss: 0.520825, acc.: 75.78%] [G loss: 0.648791]\n",
      "epoch:25 step:23924 [D loss: 0.461833, acc.: 78.91%] [G loss: 0.852561]\n",
      "epoch:25 step:23925 [D loss: 0.518563, acc.: 71.88%] [G loss: 0.785308]\n",
      "epoch:25 step:23926 [D loss: 0.624630, acc.: 64.06%] [G loss: 0.649892]\n",
      "epoch:25 step:23927 [D loss: 0.590627, acc.: 63.28%] [G loss: 0.647303]\n",
      "epoch:25 step:23928 [D loss: 0.557044, acc.: 73.44%] [G loss: 0.621942]\n",
      "epoch:25 step:23929 [D loss: 0.481938, acc.: 80.47%] [G loss: 0.674177]\n",
      "epoch:25 step:23930 [D loss: 0.528010, acc.: 72.66%] [G loss: 0.835293]\n",
      "epoch:25 step:23931 [D loss: 0.534420, acc.: 70.31%] [G loss: 0.822020]\n",
      "epoch:25 step:23932 [D loss: 0.553477, acc.: 71.88%] [G loss: 0.733426]\n",
      "epoch:25 step:23933 [D loss: 0.425836, acc.: 82.81%] [G loss: 0.931399]\n",
      "epoch:25 step:23934 [D loss: 0.544676, acc.: 75.78%] [G loss: 0.854549]\n",
      "epoch:25 step:23935 [D loss: 0.629540, acc.: 60.94%] [G loss: 0.698667]\n",
      "epoch:25 step:23936 [D loss: 0.659854, acc.: 58.59%] [G loss: 0.612734]\n",
      "epoch:25 step:23937 [D loss: 0.615218, acc.: 59.38%] [G loss: 0.503227]\n",
      "epoch:25 step:23938 [D loss: 0.496437, acc.: 74.22%] [G loss: 0.682943]\n",
      "epoch:25 step:23939 [D loss: 0.487031, acc.: 78.12%] [G loss: 0.718210]\n",
      "epoch:25 step:23940 [D loss: 0.545343, acc.: 67.19%] [G loss: 0.732039]\n",
      "epoch:25 step:23941 [D loss: 0.511181, acc.: 75.78%] [G loss: 0.811946]\n",
      "epoch:25 step:23942 [D loss: 0.496530, acc.: 75.78%] [G loss: 0.711308]\n",
      "epoch:25 step:23943 [D loss: 0.515095, acc.: 71.88%] [G loss: 0.844046]\n",
      "epoch:25 step:23944 [D loss: 0.497018, acc.: 74.22%] [G loss: 0.829634]\n",
      "epoch:25 step:23945 [D loss: 0.488613, acc.: 76.56%] [G loss: 0.743886]\n",
      "epoch:25 step:23946 [D loss: 0.413510, acc.: 83.59%] [G loss: 0.990537]\n",
      "epoch:25 step:23947 [D loss: 0.513143, acc.: 76.56%] [G loss: 0.846327]\n",
      "epoch:25 step:23948 [D loss: 0.477050, acc.: 70.31%] [G loss: 0.737890]\n",
      "epoch:25 step:23949 [D loss: 0.574394, acc.: 64.84%] [G loss: 0.842793]\n",
      "epoch:25 step:23950 [D loss: 0.584431, acc.: 71.09%] [G loss: 0.643596]\n",
      "epoch:25 step:23951 [D loss: 0.493627, acc.: 75.00%] [G loss: 0.761912]\n",
      "epoch:25 step:23952 [D loss: 0.588737, acc.: 68.75%] [G loss: 0.572685]\n",
      "epoch:25 step:23953 [D loss: 0.695800, acc.: 57.81%] [G loss: 0.576222]\n",
      "epoch:25 step:23954 [D loss: 0.647693, acc.: 60.94%] [G loss: 0.573696]\n",
      "epoch:25 step:23955 [D loss: 0.542083, acc.: 69.53%] [G loss: 0.956248]\n",
      "epoch:25 step:23956 [D loss: 0.575300, acc.: 65.62%] [G loss: 0.748048]\n",
      "epoch:25 step:23957 [D loss: 0.604156, acc.: 66.41%] [G loss: 0.797431]\n",
      "epoch:25 step:23958 [D loss: 0.506032, acc.: 71.88%] [G loss: 0.682960]\n",
      "epoch:25 step:23959 [D loss: 0.506349, acc.: 73.44%] [G loss: 0.821805]\n",
      "epoch:25 step:23960 [D loss: 0.605503, acc.: 67.19%] [G loss: 0.766635]\n",
      "epoch:25 step:23961 [D loss: 0.502259, acc.: 74.22%] [G loss: 0.618369]\n",
      "epoch:25 step:23962 [D loss: 0.601324, acc.: 64.84%] [G loss: 0.470226]\n",
      "epoch:25 step:23963 [D loss: 0.550463, acc.: 75.00%] [G loss: 0.584141]\n",
      "epoch:25 step:23964 [D loss: 0.548055, acc.: 67.97%] [G loss: 0.646729]\n",
      "epoch:25 step:23965 [D loss: 0.571272, acc.: 69.53%] [G loss: 0.540033]\n",
      "epoch:25 step:23966 [D loss: 0.529778, acc.: 66.41%] [G loss: 0.745311]\n",
      "epoch:25 step:23967 [D loss: 0.614910, acc.: 64.84%] [G loss: 0.449675]\n",
      "epoch:25 step:23968 [D loss: 0.590113, acc.: 64.84%] [G loss: 0.644349]\n",
      "epoch:25 step:23969 [D loss: 0.488009, acc.: 75.78%] [G loss: 0.738389]\n",
      "epoch:25 step:23970 [D loss: 0.537868, acc.: 71.09%] [G loss: 0.594340]\n",
      "epoch:25 step:23971 [D loss: 0.529427, acc.: 72.66%] [G loss: 0.750422]\n",
      "epoch:25 step:23972 [D loss: 0.587107, acc.: 68.75%] [G loss: 0.620969]\n",
      "epoch:25 step:23973 [D loss: 0.545725, acc.: 70.31%] [G loss: 0.566525]\n",
      "epoch:25 step:23974 [D loss: 0.553716, acc.: 71.09%] [G loss: 0.551054]\n",
      "epoch:25 step:23975 [D loss: 0.546626, acc.: 70.31%] [G loss: 0.666008]\n",
      "epoch:25 step:23976 [D loss: 0.474116, acc.: 75.00%] [G loss: 0.702631]\n",
      "epoch:25 step:23977 [D loss: 0.468826, acc.: 75.78%] [G loss: 0.653636]\n",
      "epoch:25 step:23978 [D loss: 0.584705, acc.: 70.31%] [G loss: 0.586335]\n",
      "epoch:25 step:23979 [D loss: 0.464102, acc.: 75.00%] [G loss: 0.791149]\n",
      "epoch:25 step:23980 [D loss: 0.497153, acc.: 75.78%] [G loss: 0.744429]\n",
      "epoch:25 step:23981 [D loss: 0.574603, acc.: 68.75%] [G loss: 0.670360]\n",
      "epoch:25 step:23982 [D loss: 0.471372, acc.: 76.56%] [G loss: 0.752799]\n",
      "epoch:25 step:23983 [D loss: 0.524879, acc.: 77.34%] [G loss: 0.943084]\n",
      "epoch:25 step:23984 [D loss: 0.608758, acc.: 61.72%] [G loss: 0.726588]\n",
      "epoch:25 step:23985 [D loss: 0.522102, acc.: 71.09%] [G loss: 0.740208]\n",
      "epoch:25 step:23986 [D loss: 0.564699, acc.: 68.75%] [G loss: 0.466761]\n",
      "epoch:25 step:23987 [D loss: 0.507948, acc.: 71.88%] [G loss: 0.596283]\n",
      "epoch:25 step:23988 [D loss: 0.553371, acc.: 62.50%] [G loss: 0.681568]\n",
      "epoch:25 step:23989 [D loss: 0.473423, acc.: 75.78%] [G loss: 0.941996]\n",
      "epoch:25 step:23990 [D loss: 0.557947, acc.: 72.66%] [G loss: 0.909193]\n",
      "epoch:25 step:23991 [D loss: 0.684937, acc.: 56.25%] [G loss: 0.675705]\n",
      "epoch:25 step:23992 [D loss: 0.501203, acc.: 71.88%] [G loss: 0.758588]\n",
      "epoch:25 step:23993 [D loss: 0.546110, acc.: 75.00%] [G loss: 0.789845]\n",
      "epoch:25 step:23994 [D loss: 0.545915, acc.: 71.88%] [G loss: 0.820653]\n",
      "epoch:25 step:23995 [D loss: 0.504937, acc.: 72.66%] [G loss: 0.755831]\n",
      "epoch:25 step:23996 [D loss: 0.517982, acc.: 73.44%] [G loss: 0.788036]\n",
      "epoch:25 step:23997 [D loss: 0.578896, acc.: 65.62%] [G loss: 0.724535]\n",
      "epoch:25 step:23998 [D loss: 0.557326, acc.: 67.19%] [G loss: 0.755294]\n",
      "epoch:25 step:23999 [D loss: 0.498895, acc.: 74.22%] [G loss: 0.645193]\n",
      "epoch:25 step:24000 [D loss: 0.494257, acc.: 75.78%] [G loss: 0.877043]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.696756\n",
      "FID: 44.027431\n",
      "0 = 12.817113826656344\n",
      "1 = 0.08276691261630849\n",
      "2 = 0.8575000166893005\n",
      "3 = 0.8116000294685364\n",
      "4 = 0.9034000039100647\n",
      "5 = 0.8936357498168945\n",
      "6 = 0.8116000294685364\n",
      "7 = 8.045970181870446\n",
      "8 = 0.14093280589878077\n",
      "9 = 0.6869999766349792\n",
      "10 = 0.6772000193595886\n",
      "11 = 0.6967999935150146\n",
      "12 = 0.6907384991645813\n",
      "13 = 0.6772000193595886\n",
      "14 = 6.696787357330322\n",
      "15 = 6.927402019500732\n",
      "16 = 0.3887457549571991\n",
      "17 = 6.696756362915039\n",
      "18 = 44.02743148803711\n",
      "epoch:25 step:24001 [D loss: 0.607083, acc.: 64.06%] [G loss: 0.601250]\n",
      "epoch:25 step:24002 [D loss: 0.560961, acc.: 75.00%] [G loss: 0.647879]\n",
      "epoch:25 step:24003 [D loss: 0.575862, acc.: 62.50%] [G loss: 0.685684]\n",
      "epoch:25 step:24004 [D loss: 0.536227, acc.: 73.44%] [G loss: 0.632824]\n",
      "epoch:25 step:24005 [D loss: 0.557592, acc.: 69.53%] [G loss: 0.731986]\n",
      "epoch:25 step:24006 [D loss: 0.564371, acc.: 71.88%] [G loss: 0.641004]\n",
      "epoch:25 step:24007 [D loss: 0.455621, acc.: 80.47%] [G loss: 1.032100]\n",
      "epoch:25 step:24008 [D loss: 0.533223, acc.: 70.31%] [G loss: 0.779745]\n",
      "epoch:25 step:24009 [D loss: 0.644105, acc.: 60.16%] [G loss: 0.695897]\n",
      "epoch:25 step:24010 [D loss: 0.503466, acc.: 75.78%] [G loss: 0.635725]\n",
      "epoch:25 step:24011 [D loss: 0.567734, acc.: 67.19%] [G loss: 0.719834]\n",
      "epoch:25 step:24012 [D loss: 0.571486, acc.: 67.19%] [G loss: 0.521037]\n",
      "epoch:25 step:24013 [D loss: 0.601885, acc.: 63.28%] [G loss: 0.622419]\n",
      "epoch:25 step:24014 [D loss: 0.496714, acc.: 69.53%] [G loss: 0.684377]\n",
      "epoch:25 step:24015 [D loss: 0.577753, acc.: 65.62%] [G loss: 0.785601]\n",
      "epoch:25 step:24016 [D loss: 0.521135, acc.: 73.44%] [G loss: 0.783039]\n",
      "epoch:25 step:24017 [D loss: 0.515895, acc.: 74.22%] [G loss: 0.802215]\n",
      "epoch:25 step:24018 [D loss: 0.542942, acc.: 70.31%] [G loss: 0.683862]\n",
      "epoch:25 step:24019 [D loss: 0.563281, acc.: 71.88%] [G loss: 0.690954]\n",
      "epoch:25 step:24020 [D loss: 0.552861, acc.: 74.22%] [G loss: 0.792455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24021 [D loss: 0.566931, acc.: 67.19%] [G loss: 0.617013]\n",
      "epoch:25 step:24022 [D loss: 0.537443, acc.: 71.09%] [G loss: 0.589484]\n",
      "epoch:25 step:24023 [D loss: 0.533942, acc.: 68.75%] [G loss: 0.637666]\n",
      "epoch:25 step:24024 [D loss: 0.463313, acc.: 78.12%] [G loss: 0.812460]\n",
      "epoch:25 step:24025 [D loss: 0.644885, acc.: 63.28%] [G loss: 0.671323]\n",
      "epoch:25 step:24026 [D loss: 0.537689, acc.: 69.53%] [G loss: 0.770017]\n",
      "epoch:25 step:24027 [D loss: 0.497505, acc.: 73.44%] [G loss: 0.785691]\n",
      "epoch:25 step:24028 [D loss: 0.456657, acc.: 77.34%] [G loss: 0.819345]\n",
      "epoch:25 step:24029 [D loss: 0.524942, acc.: 71.88%] [G loss: 0.721993]\n",
      "epoch:25 step:24030 [D loss: 0.433349, acc.: 80.47%] [G loss: 0.876528]\n",
      "epoch:25 step:24031 [D loss: 0.624545, acc.: 59.38%] [G loss: 0.585115]\n",
      "epoch:25 step:24032 [D loss: 0.538658, acc.: 71.88%] [G loss: 0.520608]\n",
      "epoch:25 step:24033 [D loss: 0.579276, acc.: 64.84%] [G loss: 0.569489]\n",
      "epoch:25 step:24034 [D loss: 0.529689, acc.: 70.31%] [G loss: 0.577185]\n",
      "epoch:25 step:24035 [D loss: 0.602373, acc.: 67.19%] [G loss: 0.560135]\n",
      "epoch:25 step:24036 [D loss: 0.536422, acc.: 67.97%] [G loss: 0.476810]\n",
      "epoch:25 step:24037 [D loss: 0.542287, acc.: 67.97%] [G loss: 0.530369]\n",
      "epoch:25 step:24038 [D loss: 0.493726, acc.: 77.34%] [G loss: 0.549975]\n",
      "epoch:25 step:24039 [D loss: 0.588711, acc.: 67.19%] [G loss: 0.664737]\n",
      "epoch:25 step:24040 [D loss: 0.557155, acc.: 65.62%] [G loss: 0.672126]\n",
      "epoch:25 step:24041 [D loss: 0.538867, acc.: 69.53%] [G loss: 0.801259]\n",
      "epoch:25 step:24042 [D loss: 0.480013, acc.: 80.47%] [G loss: 0.772391]\n",
      "epoch:25 step:24043 [D loss: 0.598943, acc.: 64.84%] [G loss: 0.768133]\n",
      "epoch:25 step:24044 [D loss: 0.528312, acc.: 73.44%] [G loss: 0.848842]\n",
      "epoch:25 step:24045 [D loss: 0.489646, acc.: 74.22%] [G loss: 0.678654]\n",
      "epoch:25 step:24046 [D loss: 0.593614, acc.: 69.53%] [G loss: 0.704506]\n",
      "epoch:25 step:24047 [D loss: 0.545440, acc.: 70.31%] [G loss: 0.611183]\n",
      "epoch:25 step:24048 [D loss: 0.410188, acc.: 82.81%] [G loss: 0.785732]\n",
      "epoch:25 step:24049 [D loss: 0.498710, acc.: 75.78%] [G loss: 0.702217]\n",
      "epoch:25 step:24050 [D loss: 0.623618, acc.: 63.28%] [G loss: 0.894401]\n",
      "epoch:25 step:24051 [D loss: 0.587974, acc.: 64.06%] [G loss: 0.710936]\n",
      "epoch:25 step:24052 [D loss: 0.516574, acc.: 72.66%] [G loss: 0.623282]\n",
      "epoch:25 step:24053 [D loss: 0.596749, acc.: 66.41%] [G loss: 0.637653]\n",
      "epoch:25 step:24054 [D loss: 0.557822, acc.: 61.72%] [G loss: 0.660606]\n",
      "epoch:25 step:24055 [D loss: 0.515072, acc.: 74.22%] [G loss: 0.837050]\n",
      "epoch:25 step:24056 [D loss: 0.549258, acc.: 67.19%] [G loss: 0.752526]\n",
      "epoch:25 step:24057 [D loss: 0.509255, acc.: 77.34%] [G loss: 0.864858]\n",
      "epoch:25 step:24058 [D loss: 0.501879, acc.: 72.66%] [G loss: 0.815744]\n",
      "epoch:25 step:24059 [D loss: 0.473740, acc.: 79.69%] [G loss: 0.751009]\n",
      "epoch:25 step:24060 [D loss: 0.466691, acc.: 73.44%] [G loss: 0.723267]\n",
      "epoch:25 step:24061 [D loss: 0.530903, acc.: 75.00%] [G loss: 0.663579]\n",
      "epoch:25 step:24062 [D loss: 0.547037, acc.: 73.44%] [G loss: 0.503202]\n",
      "epoch:25 step:24063 [D loss: 0.514091, acc.: 72.66%] [G loss: 0.637872]\n",
      "epoch:25 step:24064 [D loss: 0.541831, acc.: 67.19%] [G loss: 0.612792]\n",
      "epoch:25 step:24065 [D loss: 0.619466, acc.: 67.19%] [G loss: 0.665117]\n",
      "epoch:25 step:24066 [D loss: 0.470651, acc.: 78.91%] [G loss: 0.740908]\n",
      "epoch:25 step:24067 [D loss: 0.517384, acc.: 75.00%] [G loss: 0.876280]\n",
      "epoch:25 step:24068 [D loss: 0.520797, acc.: 71.88%] [G loss: 0.738589]\n",
      "epoch:25 step:24069 [D loss: 0.494881, acc.: 71.09%] [G loss: 0.830035]\n",
      "epoch:25 step:24070 [D loss: 0.553308, acc.: 66.41%] [G loss: 0.632824]\n",
      "epoch:25 step:24071 [D loss: 0.527768, acc.: 72.66%] [G loss: 0.685415]\n",
      "epoch:25 step:24072 [D loss: 0.375030, acc.: 85.94%] [G loss: 0.855867]\n",
      "epoch:25 step:24073 [D loss: 0.422371, acc.: 78.12%] [G loss: 1.036718]\n",
      "epoch:25 step:24074 [D loss: 0.518615, acc.: 72.66%] [G loss: 0.970127]\n",
      "epoch:25 step:24075 [D loss: 0.498949, acc.: 76.56%] [G loss: 0.805302]\n",
      "epoch:25 step:24076 [D loss: 0.497137, acc.: 71.09%] [G loss: 0.827055]\n",
      "epoch:25 step:24077 [D loss: 0.618009, acc.: 60.94%] [G loss: 0.626088]\n",
      "epoch:25 step:24078 [D loss: 0.584487, acc.: 67.19%] [G loss: 0.771049]\n",
      "epoch:25 step:24079 [D loss: 0.493388, acc.: 67.97%] [G loss: 0.902900]\n",
      "epoch:25 step:24080 [D loss: 0.569453, acc.: 71.88%] [G loss: 0.824230]\n",
      "epoch:25 step:24081 [D loss: 0.532717, acc.: 71.09%] [G loss: 0.676747]\n",
      "epoch:25 step:24082 [D loss: 0.532782, acc.: 70.31%] [G loss: 0.510653]\n",
      "epoch:25 step:24083 [D loss: 0.540934, acc.: 71.09%] [G loss: 0.664451]\n",
      "epoch:25 step:24084 [D loss: 0.531659, acc.: 69.53%] [G loss: 0.799362]\n",
      "epoch:25 step:24085 [D loss: 0.546928, acc.: 69.53%] [G loss: 0.809806]\n",
      "epoch:25 step:24086 [D loss: 0.452468, acc.: 78.91%] [G loss: 0.782968]\n",
      "epoch:25 step:24087 [D loss: 0.529864, acc.: 70.31%] [G loss: 0.860304]\n",
      "epoch:25 step:24088 [D loss: 0.525902, acc.: 71.09%] [G loss: 0.762693]\n",
      "epoch:25 step:24089 [D loss: 0.604749, acc.: 64.84%] [G loss: 0.712339]\n",
      "epoch:25 step:24090 [D loss: 0.575887, acc.: 67.97%] [G loss: 0.628191]\n",
      "epoch:25 step:24091 [D loss: 0.518127, acc.: 74.22%] [G loss: 0.670002]\n",
      "epoch:25 step:24092 [D loss: 0.519227, acc.: 72.66%] [G loss: 0.673802]\n",
      "epoch:25 step:24093 [D loss: 0.523302, acc.: 72.66%] [G loss: 0.749865]\n",
      "epoch:25 step:24094 [D loss: 0.522485, acc.: 70.31%] [G loss: 0.555206]\n",
      "epoch:25 step:24095 [D loss: 0.517598, acc.: 72.66%] [G loss: 0.833195]\n",
      "epoch:25 step:24096 [D loss: 0.607078, acc.: 65.62%] [G loss: 0.587618]\n",
      "epoch:25 step:24097 [D loss: 0.515523, acc.: 75.78%] [G loss: 0.840389]\n",
      "epoch:25 step:24098 [D loss: 0.585786, acc.: 73.44%] [G loss: 0.516580]\n",
      "epoch:25 step:24099 [D loss: 0.516855, acc.: 68.75%] [G loss: 0.632813]\n",
      "epoch:25 step:24100 [D loss: 0.612234, acc.: 64.84%] [G loss: 0.745386]\n",
      "epoch:25 step:24101 [D loss: 0.535520, acc.: 68.75%] [G loss: 0.635491]\n",
      "epoch:25 step:24102 [D loss: 0.504077, acc.: 76.56%] [G loss: 0.713994]\n",
      "epoch:25 step:24103 [D loss: 0.598020, acc.: 68.75%] [G loss: 0.702948]\n",
      "epoch:25 step:24104 [D loss: 0.473856, acc.: 75.00%] [G loss: 0.853767]\n",
      "epoch:25 step:24105 [D loss: 0.531145, acc.: 71.88%] [G loss: 0.697530]\n",
      "epoch:25 step:24106 [D loss: 0.424506, acc.: 82.81%] [G loss: 0.769869]\n",
      "epoch:25 step:24107 [D loss: 0.495307, acc.: 74.22%] [G loss: 0.645850]\n",
      "epoch:25 step:24108 [D loss: 0.529678, acc.: 71.09%] [G loss: 0.674919]\n",
      "epoch:25 step:24109 [D loss: 0.569200, acc.: 70.31%] [G loss: 0.609600]\n",
      "epoch:25 step:24110 [D loss: 0.548582, acc.: 67.97%] [G loss: 0.560942]\n",
      "epoch:25 step:24111 [D loss: 0.592336, acc.: 60.16%] [G loss: 0.623639]\n",
      "epoch:25 step:24112 [D loss: 0.610534, acc.: 64.84%] [G loss: 0.709792]\n",
      "epoch:25 step:24113 [D loss: 0.515295, acc.: 67.97%] [G loss: 0.682970]\n",
      "epoch:25 step:24114 [D loss: 0.576515, acc.: 66.41%] [G loss: 0.783705]\n",
      "epoch:25 step:24115 [D loss: 0.476138, acc.: 76.56%] [G loss: 0.918202]\n",
      "epoch:25 step:24116 [D loss: 0.495389, acc.: 74.22%] [G loss: 0.784624]\n",
      "epoch:25 step:24117 [D loss: 0.520715, acc.: 69.53%] [G loss: 0.746110]\n",
      "epoch:25 step:24118 [D loss: 0.493671, acc.: 75.00%] [G loss: 0.688948]\n",
      "epoch:25 step:24119 [D loss: 0.489584, acc.: 77.34%] [G loss: 0.728321]\n",
      "epoch:25 step:24120 [D loss: 0.543801, acc.: 71.09%] [G loss: 0.808020]\n",
      "epoch:25 step:24121 [D loss: 0.611966, acc.: 59.38%] [G loss: 0.586211]\n",
      "epoch:25 step:24122 [D loss: 0.562188, acc.: 65.62%] [G loss: 0.621728]\n",
      "epoch:25 step:24123 [D loss: 0.589916, acc.: 69.53%] [G loss: 0.509963]\n",
      "epoch:25 step:24124 [D loss: 0.501913, acc.: 74.22%] [G loss: 0.658685]\n",
      "epoch:25 step:24125 [D loss: 0.554447, acc.: 70.31%] [G loss: 0.758968]\n",
      "epoch:25 step:24126 [D loss: 0.539113, acc.: 71.88%] [G loss: 0.956752]\n",
      "epoch:25 step:24127 [D loss: 0.551164, acc.: 70.31%] [G loss: 0.693919]\n",
      "epoch:25 step:24128 [D loss: 0.626109, acc.: 64.06%] [G loss: 0.553585]\n",
      "epoch:25 step:24129 [D loss: 0.611618, acc.: 64.84%] [G loss: 0.653060]\n",
      "epoch:25 step:24130 [D loss: 0.516025, acc.: 74.22%] [G loss: 0.584979]\n",
      "epoch:25 step:24131 [D loss: 0.515462, acc.: 72.66%] [G loss: 0.552791]\n",
      "epoch:25 step:24132 [D loss: 0.494316, acc.: 75.00%] [G loss: 0.700147]\n",
      "epoch:25 step:24133 [D loss: 0.518850, acc.: 76.56%] [G loss: 0.636934]\n",
      "epoch:25 step:24134 [D loss: 0.535877, acc.: 68.75%] [G loss: 0.646337]\n",
      "epoch:25 step:24135 [D loss: 0.585312, acc.: 69.53%] [G loss: 0.647303]\n",
      "epoch:25 step:24136 [D loss: 0.551869, acc.: 70.31%] [G loss: 0.663104]\n",
      "epoch:25 step:24137 [D loss: 0.521430, acc.: 71.88%] [G loss: 0.738572]\n",
      "epoch:25 step:24138 [D loss: 0.584020, acc.: 65.62%] [G loss: 0.750610]\n",
      "epoch:25 step:24139 [D loss: 0.506798, acc.: 73.44%] [G loss: 0.775189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24140 [D loss: 0.539565, acc.: 75.00%] [G loss: 0.570777]\n",
      "epoch:25 step:24141 [D loss: 0.639380, acc.: 64.84%] [G loss: 0.483443]\n",
      "epoch:25 step:24142 [D loss: 0.559358, acc.: 73.44%] [G loss: 0.720860]\n",
      "epoch:25 step:24143 [D loss: 0.535316, acc.: 73.44%] [G loss: 0.767427]\n",
      "epoch:25 step:24144 [D loss: 0.494539, acc.: 76.56%] [G loss: 0.708877]\n",
      "epoch:25 step:24145 [D loss: 0.579877, acc.: 69.53%] [G loss: 0.664414]\n",
      "epoch:25 step:24146 [D loss: 0.568090, acc.: 69.53%] [G loss: 0.588373]\n",
      "epoch:25 step:24147 [D loss: 0.532277, acc.: 77.34%] [G loss: 0.493740]\n",
      "epoch:25 step:24148 [D loss: 0.584105, acc.: 64.84%] [G loss: 0.556708]\n",
      "epoch:25 step:24149 [D loss: 0.516746, acc.: 69.53%] [G loss: 0.644324]\n",
      "epoch:25 step:24150 [D loss: 0.519898, acc.: 72.66%] [G loss: 0.775811]\n",
      "epoch:25 step:24151 [D loss: 0.490443, acc.: 77.34%] [G loss: 0.781277]\n",
      "epoch:25 step:24152 [D loss: 0.565247, acc.: 70.31%] [G loss: 0.717073]\n",
      "epoch:25 step:24153 [D loss: 0.579086, acc.: 68.75%] [G loss: 0.636105]\n",
      "epoch:25 step:24154 [D loss: 0.543156, acc.: 69.53%] [G loss: 0.688262]\n",
      "epoch:25 step:24155 [D loss: 0.519892, acc.: 78.12%] [G loss: 0.651239]\n",
      "epoch:25 step:24156 [D loss: 0.601720, acc.: 64.84%] [G loss: 0.646742]\n",
      "epoch:25 step:24157 [D loss: 0.527510, acc.: 72.66%] [G loss: 0.698834]\n",
      "epoch:25 step:24158 [D loss: 0.517025, acc.: 75.00%] [G loss: 0.756710]\n",
      "epoch:25 step:24159 [D loss: 0.496260, acc.: 75.00%] [G loss: 0.700777]\n",
      "epoch:25 step:24160 [D loss: 0.510637, acc.: 71.09%] [G loss: 0.614529]\n",
      "epoch:25 step:24161 [D loss: 0.478111, acc.: 73.44%] [G loss: 0.641376]\n",
      "epoch:25 step:24162 [D loss: 0.555743, acc.: 69.53%] [G loss: 0.787117]\n",
      "epoch:25 step:24163 [D loss: 0.608615, acc.: 62.50%] [G loss: 0.498247]\n",
      "epoch:25 step:24164 [D loss: 0.607744, acc.: 61.72%] [G loss: 0.616338]\n",
      "epoch:25 step:24165 [D loss: 0.634827, acc.: 60.94%] [G loss: 0.568405]\n",
      "epoch:25 step:24166 [D loss: 0.559106, acc.: 67.97%] [G loss: 0.605719]\n",
      "epoch:25 step:24167 [D loss: 0.511713, acc.: 72.66%] [G loss: 0.672157]\n",
      "epoch:25 step:24168 [D loss: 0.520497, acc.: 73.44%] [G loss: 0.745253]\n",
      "epoch:25 step:24169 [D loss: 0.458333, acc.: 79.69%] [G loss: 0.760160]\n",
      "epoch:25 step:24170 [D loss: 0.574738, acc.: 67.97%] [G loss: 0.668527]\n",
      "epoch:25 step:24171 [D loss: 0.469650, acc.: 73.44%] [G loss: 0.741777]\n",
      "epoch:25 step:24172 [D loss: 0.459818, acc.: 73.44%] [G loss: 0.768764]\n",
      "epoch:25 step:24173 [D loss: 0.541815, acc.: 67.97%] [G loss: 0.705190]\n",
      "epoch:25 step:24174 [D loss: 0.485682, acc.: 77.34%] [G loss: 0.834354]\n",
      "epoch:25 step:24175 [D loss: 0.528100, acc.: 70.31%] [G loss: 0.893437]\n",
      "epoch:25 step:24176 [D loss: 0.528944, acc.: 68.75%] [G loss: 0.689242]\n",
      "epoch:25 step:24177 [D loss: 0.557532, acc.: 71.09%] [G loss: 0.722019]\n",
      "epoch:25 step:24178 [D loss: 0.535826, acc.: 72.66%] [G loss: 0.934649]\n",
      "epoch:25 step:24179 [D loss: 0.566230, acc.: 67.19%] [G loss: 0.678848]\n",
      "epoch:25 step:24180 [D loss: 0.531276, acc.: 72.66%] [G loss: 0.715968]\n",
      "epoch:25 step:24181 [D loss: 0.549796, acc.: 70.31%] [G loss: 0.699377]\n",
      "epoch:25 step:24182 [D loss: 0.561990, acc.: 64.84%] [G loss: 0.766317]\n",
      "epoch:25 step:24183 [D loss: 0.524631, acc.: 71.09%] [G loss: 0.619750]\n",
      "epoch:25 step:24184 [D loss: 0.559498, acc.: 71.09%] [G loss: 0.751786]\n",
      "epoch:25 step:24185 [D loss: 0.506712, acc.: 73.44%] [G loss: 0.751608]\n",
      "epoch:25 step:24186 [D loss: 0.534124, acc.: 71.09%] [G loss: 0.801110]\n",
      "epoch:25 step:24187 [D loss: 0.620726, acc.: 65.62%] [G loss: 0.624961]\n",
      "epoch:25 step:24188 [D loss: 0.496589, acc.: 74.22%] [G loss: 0.583225]\n",
      "epoch:25 step:24189 [D loss: 0.542036, acc.: 70.31%] [G loss: 0.627431]\n",
      "epoch:25 step:24190 [D loss: 0.567162, acc.: 68.75%] [G loss: 0.761540]\n",
      "epoch:25 step:24191 [D loss: 0.655073, acc.: 65.62%] [G loss: 0.437340]\n",
      "epoch:25 step:24192 [D loss: 0.464543, acc.: 80.47%] [G loss: 0.731714]\n",
      "epoch:25 step:24193 [D loss: 0.589901, acc.: 69.53%] [G loss: 0.728750]\n",
      "epoch:25 step:24194 [D loss: 0.451720, acc.: 78.91%] [G loss: 0.816876]\n",
      "epoch:25 step:24195 [D loss: 0.527890, acc.: 75.78%] [G loss: 1.034635]\n",
      "epoch:25 step:24196 [D loss: 0.479335, acc.: 77.34%] [G loss: 0.823905]\n",
      "epoch:25 step:24197 [D loss: 0.618499, acc.: 65.62%] [G loss: 0.761524]\n",
      "epoch:25 step:24198 [D loss: 0.569549, acc.: 69.53%] [G loss: 0.806666]\n",
      "epoch:25 step:24199 [D loss: 0.465309, acc.: 79.69%] [G loss: 0.969566]\n",
      "epoch:25 step:24200 [D loss: 0.532654, acc.: 71.88%] [G loss: 0.816207]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.755470\n",
      "FID: 48.107491\n",
      "0 = 12.975135429477708\n",
      "1 = 0.0960748421279837\n",
      "2 = 0.8503000140190125\n",
      "3 = 0.8162000179290771\n",
      "4 = 0.8844000101089478\n",
      "5 = 0.8759390711784363\n",
      "6 = 0.8162000179290771\n",
      "7 = 8.316062051749215\n",
      "8 = 0.14932228211542048\n",
      "9 = 0.6926000118255615\n",
      "10 = 0.6937999725341797\n",
      "11 = 0.6913999915122986\n",
      "12 = 0.6921388506889343\n",
      "13 = 0.6937999725341797\n",
      "14 = 6.755497455596924\n",
      "15 = 6.910281181335449\n",
      "16 = 0.3893055319786072\n",
      "17 = 6.755469799041748\n",
      "18 = 48.10749053955078\n",
      "epoch:25 step:24201 [D loss: 0.595188, acc.: 68.75%] [G loss: 0.722346]\n",
      "epoch:25 step:24202 [D loss: 0.577138, acc.: 69.53%] [G loss: 0.770075]\n",
      "epoch:25 step:24203 [D loss: 0.596353, acc.: 65.62%] [G loss: 0.684007]\n",
      "epoch:25 step:24204 [D loss: 0.539566, acc.: 68.75%] [G loss: 0.646611]\n",
      "epoch:25 step:24205 [D loss: 0.499319, acc.: 78.12%] [G loss: 0.713507]\n",
      "epoch:25 step:24206 [D loss: 0.508838, acc.: 75.00%] [G loss: 0.719196]\n",
      "epoch:25 step:24207 [D loss: 0.474491, acc.: 74.22%] [G loss: 0.790262]\n",
      "epoch:25 step:24208 [D loss: 0.501759, acc.: 72.66%] [G loss: 0.880254]\n",
      "epoch:25 step:24209 [D loss: 0.640093, acc.: 63.28%] [G loss: 0.693352]\n",
      "epoch:25 step:24210 [D loss: 0.541760, acc.: 68.75%] [G loss: 0.757770]\n",
      "epoch:25 step:24211 [D loss: 0.535575, acc.: 71.09%] [G loss: 0.782676]\n",
      "epoch:25 step:24212 [D loss: 0.586230, acc.: 68.75%] [G loss: 0.666565]\n",
      "epoch:25 step:24213 [D loss: 0.638496, acc.: 62.50%] [G loss: 0.619937]\n",
      "epoch:25 step:24214 [D loss: 0.516511, acc.: 72.66%] [G loss: 0.759153]\n",
      "epoch:25 step:24215 [D loss: 0.491355, acc.: 75.00%] [G loss: 0.754364]\n",
      "epoch:25 step:24216 [D loss: 0.599348, acc.: 70.31%] [G loss: 0.568746]\n",
      "epoch:25 step:24217 [D loss: 0.497680, acc.: 78.12%] [G loss: 0.713302]\n",
      "epoch:25 step:24218 [D loss: 0.550413, acc.: 70.31%] [G loss: 0.641490]\n",
      "epoch:25 step:24219 [D loss: 0.582651, acc.: 65.62%] [G loss: 0.614287]\n",
      "epoch:25 step:24220 [D loss: 0.486323, acc.: 76.56%] [G loss: 0.706900]\n",
      "epoch:25 step:24221 [D loss: 0.487554, acc.: 76.56%] [G loss: 0.576287]\n",
      "epoch:25 step:24222 [D loss: 0.584403, acc.: 63.28%] [G loss: 0.800780]\n",
      "epoch:25 step:24223 [D loss: 0.514359, acc.: 69.53%] [G loss: 0.674132]\n",
      "epoch:25 step:24224 [D loss: 0.593110, acc.: 67.97%] [G loss: 0.712324]\n",
      "epoch:25 step:24225 [D loss: 0.624055, acc.: 59.38%] [G loss: 0.692545]\n",
      "epoch:25 step:24226 [D loss: 0.522027, acc.: 71.88%] [G loss: 0.809498]\n",
      "epoch:25 step:24227 [D loss: 0.522357, acc.: 73.44%] [G loss: 0.651513]\n",
      "epoch:25 step:24228 [D loss: 0.502389, acc.: 73.44%] [G loss: 0.901458]\n",
      "epoch:25 step:24229 [D loss: 0.543786, acc.: 71.88%] [G loss: 0.591246]\n",
      "epoch:25 step:24230 [D loss: 0.552150, acc.: 67.97%] [G loss: 0.597420]\n",
      "epoch:25 step:24231 [D loss: 0.561363, acc.: 68.75%] [G loss: 0.687045]\n",
      "epoch:25 step:24232 [D loss: 0.526329, acc.: 65.62%] [G loss: 0.699461]\n",
      "epoch:25 step:24233 [D loss: 0.563693, acc.: 71.88%] [G loss: 0.643113]\n",
      "epoch:25 step:24234 [D loss: 0.578416, acc.: 69.53%] [G loss: 0.587703]\n",
      "epoch:25 step:24235 [D loss: 0.537117, acc.: 70.31%] [G loss: 0.620577]\n",
      "epoch:25 step:24236 [D loss: 0.535671, acc.: 73.44%] [G loss: 0.756043]\n",
      "epoch:25 step:24237 [D loss: 0.662345, acc.: 61.72%] [G loss: 0.453685]\n",
      "epoch:25 step:24238 [D loss: 0.569889, acc.: 65.62%] [G loss: 0.430462]\n",
      "epoch:25 step:24239 [D loss: 0.500615, acc.: 76.56%] [G loss: 0.721679]\n",
      "epoch:25 step:24240 [D loss: 0.508708, acc.: 78.12%] [G loss: 0.901904]\n",
      "epoch:25 step:24241 [D loss: 0.591649, acc.: 68.75%] [G loss: 0.825469]\n",
      "epoch:25 step:24242 [D loss: 0.620722, acc.: 67.19%] [G loss: 0.810744]\n",
      "epoch:25 step:24243 [D loss: 0.573142, acc.: 66.41%] [G loss: 0.734411]\n",
      "epoch:25 step:24244 [D loss: 0.504180, acc.: 75.78%] [G loss: 0.752201]\n",
      "epoch:25 step:24245 [D loss: 0.683445, acc.: 58.59%] [G loss: 0.621645]\n",
      "epoch:25 step:24246 [D loss: 0.578669, acc.: 65.62%] [G loss: 0.607837]\n",
      "epoch:25 step:24247 [D loss: 0.540313, acc.: 65.62%] [G loss: 0.554523]\n",
      "epoch:25 step:24248 [D loss: 0.430804, acc.: 82.81%] [G loss: 0.779082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24249 [D loss: 0.533190, acc.: 72.66%] [G loss: 0.823074]\n",
      "epoch:25 step:24250 [D loss: 0.503640, acc.: 78.12%] [G loss: 0.618934]\n",
      "epoch:25 step:24251 [D loss: 0.591788, acc.: 68.75%] [G loss: 0.606905]\n",
      "epoch:25 step:24252 [D loss: 0.600397, acc.: 66.41%] [G loss: 0.649007]\n",
      "epoch:25 step:24253 [D loss: 0.613922, acc.: 64.84%] [G loss: 0.619403]\n",
      "epoch:25 step:24254 [D loss: 0.548667, acc.: 68.75%] [G loss: 0.722326]\n",
      "epoch:25 step:24255 [D loss: 0.541470, acc.: 71.88%] [G loss: 0.573363]\n",
      "epoch:25 step:24256 [D loss: 0.548288, acc.: 73.44%] [G loss: 0.681001]\n",
      "epoch:25 step:24257 [D loss: 0.543210, acc.: 69.53%] [G loss: 0.655505]\n",
      "epoch:25 step:24258 [D loss: 0.489346, acc.: 76.56%] [G loss: 0.720589]\n",
      "epoch:25 step:24259 [D loss: 0.523062, acc.: 70.31%] [G loss: 0.513399]\n",
      "epoch:25 step:24260 [D loss: 0.526888, acc.: 74.22%] [G loss: 0.598393]\n",
      "epoch:25 step:24261 [D loss: 0.547698, acc.: 66.41%] [G loss: 0.666879]\n",
      "epoch:25 step:24262 [D loss: 0.538067, acc.: 71.09%] [G loss: 0.578253]\n",
      "epoch:25 step:24263 [D loss: 0.528822, acc.: 74.22%] [G loss: 0.573542]\n",
      "epoch:25 step:24264 [D loss: 0.570406, acc.: 65.62%] [G loss: 0.655532]\n",
      "epoch:25 step:24265 [D loss: 0.583467, acc.: 70.31%] [G loss: 0.595245]\n",
      "epoch:25 step:24266 [D loss: 0.534808, acc.: 71.09%] [G loss: 0.498883]\n",
      "epoch:25 step:24267 [D loss: 0.503642, acc.: 72.66%] [G loss: 0.584419]\n",
      "epoch:25 step:24268 [D loss: 0.532765, acc.: 73.44%] [G loss: 0.633971]\n",
      "epoch:25 step:24269 [D loss: 0.572979, acc.: 65.62%] [G loss: 0.817920]\n",
      "epoch:25 step:24270 [D loss: 0.524916, acc.: 71.88%] [G loss: 0.725267]\n",
      "epoch:25 step:24271 [D loss: 0.588770, acc.: 63.28%] [G loss: 0.579227]\n",
      "epoch:25 step:24272 [D loss: 0.592058, acc.: 64.84%] [G loss: 0.438260]\n",
      "epoch:25 step:24273 [D loss: 0.606196, acc.: 60.16%] [G loss: 0.443578]\n",
      "epoch:25 step:24274 [D loss: 0.582403, acc.: 68.75%] [G loss: 0.534331]\n",
      "epoch:25 step:24275 [D loss: 0.563283, acc.: 68.75%] [G loss: 0.613035]\n",
      "epoch:25 step:24276 [D loss: 0.596035, acc.: 62.50%] [G loss: 0.635624]\n",
      "epoch:25 step:24277 [D loss: 0.558068, acc.: 70.31%] [G loss: 0.637896]\n",
      "epoch:25 step:24278 [D loss: 0.535733, acc.: 69.53%] [G loss: 0.625263]\n",
      "epoch:25 step:24279 [D loss: 0.522829, acc.: 71.88%] [G loss: 0.658987]\n",
      "epoch:25 step:24280 [D loss: 0.549733, acc.: 73.44%] [G loss: 0.692788]\n",
      "epoch:25 step:24281 [D loss: 0.586578, acc.: 68.75%] [G loss: 0.785253]\n",
      "epoch:25 step:24282 [D loss: 0.454643, acc.: 79.69%] [G loss: 0.705875]\n",
      "epoch:25 step:24283 [D loss: 0.621127, acc.: 65.62%] [G loss: 0.738521]\n",
      "epoch:25 step:24284 [D loss: 0.574064, acc.: 68.75%] [G loss: 0.770576]\n",
      "epoch:25 step:24285 [D loss: 0.467441, acc.: 77.34%] [G loss: 0.952432]\n",
      "epoch:25 step:24286 [D loss: 0.639804, acc.: 56.25%] [G loss: 0.565263]\n",
      "epoch:25 step:24287 [D loss: 0.543787, acc.: 68.75%] [G loss: 0.595341]\n",
      "epoch:25 step:24288 [D loss: 0.566704, acc.: 72.66%] [G loss: 0.539018]\n",
      "epoch:25 step:24289 [D loss: 0.534207, acc.: 67.97%] [G loss: 0.529513]\n",
      "epoch:25 step:24290 [D loss: 0.586735, acc.: 67.19%] [G loss: 0.549546]\n",
      "epoch:25 step:24291 [D loss: 0.559677, acc.: 69.53%] [G loss: 0.662002]\n",
      "epoch:25 step:24292 [D loss: 0.663470, acc.: 58.59%] [G loss: 0.559328]\n",
      "epoch:25 step:24293 [D loss: 0.521680, acc.: 77.34%] [G loss: 0.554702]\n",
      "epoch:25 step:24294 [D loss: 0.558388, acc.: 68.75%] [G loss: 0.531339]\n",
      "epoch:25 step:24295 [D loss: 0.511331, acc.: 71.09%] [G loss: 0.659100]\n",
      "epoch:25 step:24296 [D loss: 0.515492, acc.: 74.22%] [G loss: 0.894098]\n",
      "epoch:25 step:24297 [D loss: 0.530371, acc.: 71.09%] [G loss: 0.749801]\n",
      "epoch:25 step:24298 [D loss: 0.634401, acc.: 67.19%] [G loss: 0.568178]\n",
      "epoch:25 step:24299 [D loss: 0.582048, acc.: 64.84%] [G loss: 0.662214]\n",
      "epoch:25 step:24300 [D loss: 0.506891, acc.: 71.09%] [G loss: 0.658986]\n",
      "epoch:25 step:24301 [D loss: 0.576942, acc.: 68.75%] [G loss: 0.597028]\n",
      "epoch:25 step:24302 [D loss: 0.600803, acc.: 64.06%] [G loss: 0.518389]\n",
      "epoch:25 step:24303 [D loss: 0.538962, acc.: 72.66%] [G loss: 0.587705]\n",
      "epoch:25 step:24304 [D loss: 0.607277, acc.: 62.50%] [G loss: 0.581854]\n",
      "epoch:25 step:24305 [D loss: 0.625867, acc.: 63.28%] [G loss: 0.490489]\n",
      "epoch:25 step:24306 [D loss: 0.571787, acc.: 71.09%] [G loss: 0.643396]\n",
      "epoch:25 step:24307 [D loss: 0.612764, acc.: 63.28%] [G loss: 0.479857]\n",
      "epoch:25 step:24308 [D loss: 0.595540, acc.: 62.50%] [G loss: 0.519245]\n",
      "epoch:25 step:24309 [D loss: 0.476860, acc.: 75.78%] [G loss: 0.718736]\n",
      "epoch:25 step:24310 [D loss: 0.502498, acc.: 73.44%] [G loss: 0.793680]\n",
      "epoch:25 step:24311 [D loss: 0.489363, acc.: 72.66%] [G loss: 0.601641]\n",
      "epoch:25 step:24312 [D loss: 0.601671, acc.: 65.62%] [G loss: 0.553773]\n",
      "epoch:25 step:24313 [D loss: 0.545192, acc.: 72.66%] [G loss: 0.696769]\n",
      "epoch:25 step:24314 [D loss: 0.543739, acc.: 69.53%] [G loss: 0.718044]\n",
      "epoch:25 step:24315 [D loss: 0.495721, acc.: 77.34%] [G loss: 0.737424]\n",
      "epoch:25 step:24316 [D loss: 0.565141, acc.: 69.53%] [G loss: 0.767907]\n",
      "epoch:25 step:24317 [D loss: 0.597099, acc.: 66.41%] [G loss: 0.547110]\n",
      "epoch:25 step:24318 [D loss: 0.510090, acc.: 76.56%] [G loss: 0.649534]\n",
      "epoch:25 step:24319 [D loss: 0.464027, acc.: 76.56%] [G loss: 0.685561]\n",
      "epoch:25 step:24320 [D loss: 0.529491, acc.: 68.75%] [G loss: 0.687634]\n",
      "epoch:25 step:24321 [D loss: 0.436015, acc.: 82.81%] [G loss: 0.883675]\n",
      "epoch:25 step:24322 [D loss: 0.520932, acc.: 74.22%] [G loss: 0.750760]\n",
      "epoch:25 step:24323 [D loss: 0.480652, acc.: 75.00%] [G loss: 0.850374]\n",
      "epoch:25 step:24324 [D loss: 0.464606, acc.: 80.47%] [G loss: 0.823180]\n",
      "epoch:25 step:24325 [D loss: 0.496562, acc.: 76.56%] [G loss: 1.001522]\n",
      "epoch:25 step:24326 [D loss: 0.549495, acc.: 71.88%] [G loss: 0.734271]\n",
      "epoch:25 step:24327 [D loss: 0.563006, acc.: 70.31%] [G loss: 0.621865]\n",
      "epoch:25 step:24328 [D loss: 0.579846, acc.: 67.97%] [G loss: 0.644413]\n",
      "epoch:25 step:24329 [D loss: 0.540694, acc.: 71.88%] [G loss: 0.805709]\n",
      "epoch:25 step:24330 [D loss: 0.571019, acc.: 67.97%] [G loss: 0.700924]\n",
      "epoch:25 step:24331 [D loss: 0.502279, acc.: 75.78%] [G loss: 0.702160]\n",
      "epoch:25 step:24332 [D loss: 0.546158, acc.: 68.75%] [G loss: 0.525111]\n",
      "epoch:25 step:24333 [D loss: 0.560263, acc.: 67.19%] [G loss: 0.682265]\n",
      "epoch:25 step:24334 [D loss: 0.483299, acc.: 75.00%] [G loss: 0.616856]\n",
      "epoch:25 step:24335 [D loss: 0.594336, acc.: 67.97%] [G loss: 0.802512]\n",
      "epoch:25 step:24336 [D loss: 0.498284, acc.: 74.22%] [G loss: 0.767891]\n",
      "epoch:25 step:24337 [D loss: 0.481183, acc.: 71.88%] [G loss: 1.010127]\n",
      "epoch:25 step:24338 [D loss: 0.554198, acc.: 71.88%] [G loss: 0.866247]\n",
      "epoch:25 step:24339 [D loss: 0.512709, acc.: 73.44%] [G loss: 0.840077]\n",
      "epoch:25 step:24340 [D loss: 0.702492, acc.: 62.50%] [G loss: 0.646877]\n",
      "epoch:25 step:24341 [D loss: 0.534092, acc.: 73.44%] [G loss: 0.846492]\n",
      "epoch:25 step:24342 [D loss: 0.651251, acc.: 64.06%] [G loss: 0.537787]\n",
      "epoch:25 step:24343 [D loss: 0.524823, acc.: 72.66%] [G loss: 0.727936]\n",
      "epoch:25 step:24344 [D loss: 0.426808, acc.: 84.38%] [G loss: 0.973595]\n",
      "epoch:25 step:24345 [D loss: 0.704794, acc.: 63.28%] [G loss: 0.880084]\n",
      "epoch:25 step:24346 [D loss: 0.530715, acc.: 68.75%] [G loss: 0.644765]\n",
      "epoch:25 step:24347 [D loss: 0.575884, acc.: 68.75%] [G loss: 0.859718]\n",
      "epoch:25 step:24348 [D loss: 0.463034, acc.: 75.78%] [G loss: 0.788734]\n",
      "epoch:25 step:24349 [D loss: 0.441704, acc.: 78.12%] [G loss: 0.920340]\n",
      "epoch:25 step:24350 [D loss: 0.490226, acc.: 75.00%] [G loss: 1.107145]\n",
      "epoch:25 step:24351 [D loss: 0.429851, acc.: 78.12%] [G loss: 1.153250]\n",
      "epoch:25 step:24352 [D loss: 0.513870, acc.: 68.75%] [G loss: 1.256777]\n",
      "epoch:25 step:24353 [D loss: 0.683232, acc.: 61.72%] [G loss: 1.013779]\n",
      "epoch:25 step:24354 [D loss: 0.507204, acc.: 72.66%] [G loss: 1.347159]\n",
      "epoch:25 step:24355 [D loss: 0.519962, acc.: 68.75%] [G loss: 1.365859]\n",
      "epoch:25 step:24356 [D loss: 0.526600, acc.: 73.44%] [G loss: 0.779586]\n",
      "epoch:25 step:24357 [D loss: 0.689395, acc.: 57.03%] [G loss: 1.055235]\n",
      "epoch:25 step:24358 [D loss: 0.551417, acc.: 71.88%] [G loss: 0.835375]\n",
      "epoch:25 step:24359 [D loss: 0.548574, acc.: 70.31%] [G loss: 0.812600]\n",
      "epoch:25 step:24360 [D loss: 0.466107, acc.: 74.22%] [G loss: 1.065710]\n",
      "epoch:25 step:24361 [D loss: 0.379930, acc.: 83.59%] [G loss: 1.338652]\n",
      "epoch:25 step:24362 [D loss: 0.385679, acc.: 82.03%] [G loss: 1.687266]\n",
      "epoch:26 step:24363 [D loss: 0.564510, acc.: 72.66%] [G loss: 1.328500]\n",
      "epoch:26 step:24364 [D loss: 0.515265, acc.: 73.44%] [G loss: 1.093955]\n",
      "epoch:26 step:24365 [D loss: 0.548751, acc.: 74.22%] [G loss: 0.980987]\n",
      "epoch:26 step:24366 [D loss: 0.559641, acc.: 68.75%] [G loss: 0.824449]\n",
      "epoch:26 step:24367 [D loss: 0.486500, acc.: 75.78%] [G loss: 0.762056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24368 [D loss: 0.608899, acc.: 67.97%] [G loss: 0.736653]\n",
      "epoch:26 step:24369 [D loss: 0.482191, acc.: 82.81%] [G loss: 0.756698]\n",
      "epoch:26 step:24370 [D loss: 0.563636, acc.: 70.31%] [G loss: 0.754492]\n",
      "epoch:26 step:24371 [D loss: 0.502610, acc.: 73.44%] [G loss: 1.011294]\n",
      "epoch:26 step:24372 [D loss: 0.508451, acc.: 76.56%] [G loss: 0.724773]\n",
      "epoch:26 step:24373 [D loss: 0.488653, acc.: 75.78%] [G loss: 0.896453]\n",
      "epoch:26 step:24374 [D loss: 0.650668, acc.: 64.06%] [G loss: 0.646668]\n",
      "epoch:26 step:24375 [D loss: 0.516083, acc.: 71.09%] [G loss: 0.825765]\n",
      "epoch:26 step:24376 [D loss: 0.543539, acc.: 69.53%] [G loss: 0.585099]\n",
      "epoch:26 step:24377 [D loss: 0.464288, acc.: 73.44%] [G loss: 0.588251]\n",
      "epoch:26 step:24378 [D loss: 0.465414, acc.: 78.12%] [G loss: 0.774766]\n",
      "epoch:26 step:24379 [D loss: 0.568104, acc.: 72.66%] [G loss: 0.712377]\n",
      "epoch:26 step:24380 [D loss: 0.574883, acc.: 65.62%] [G loss: 0.668291]\n",
      "epoch:26 step:24381 [D loss: 0.629714, acc.: 61.72%] [G loss: 0.658194]\n",
      "epoch:26 step:24382 [D loss: 0.597935, acc.: 68.75%] [G loss: 0.754390]\n",
      "epoch:26 step:24383 [D loss: 0.578663, acc.: 67.19%] [G loss: 0.678944]\n",
      "epoch:26 step:24384 [D loss: 0.477109, acc.: 77.34%] [G loss: 0.982494]\n",
      "epoch:26 step:24385 [D loss: 0.613474, acc.: 67.97%] [G loss: 0.699534]\n",
      "epoch:26 step:24386 [D loss: 0.506095, acc.: 71.88%] [G loss: 0.523971]\n",
      "epoch:26 step:24387 [D loss: 0.566961, acc.: 70.31%] [G loss: 0.557589]\n",
      "epoch:26 step:24388 [D loss: 0.573391, acc.: 67.19%] [G loss: 0.624561]\n",
      "epoch:26 step:24389 [D loss: 0.513630, acc.: 76.56%] [G loss: 0.723682]\n",
      "epoch:26 step:24390 [D loss: 0.592020, acc.: 65.62%] [G loss: 0.713966]\n",
      "epoch:26 step:24391 [D loss: 0.492785, acc.: 76.56%] [G loss: 0.638009]\n",
      "epoch:26 step:24392 [D loss: 0.540648, acc.: 72.66%] [G loss: 0.552375]\n",
      "epoch:26 step:24393 [D loss: 0.604308, acc.: 65.62%] [G loss: 0.743500]\n",
      "epoch:26 step:24394 [D loss: 0.546756, acc.: 67.97%] [G loss: 0.549450]\n",
      "epoch:26 step:24395 [D loss: 0.501862, acc.: 71.88%] [G loss: 0.629698]\n",
      "epoch:26 step:24396 [D loss: 0.537964, acc.: 67.19%] [G loss: 0.708917]\n",
      "epoch:26 step:24397 [D loss: 0.542188, acc.: 71.88%] [G loss: 0.570850]\n",
      "epoch:26 step:24398 [D loss: 0.477432, acc.: 77.34%] [G loss: 0.607491]\n",
      "epoch:26 step:24399 [D loss: 0.497211, acc.: 73.44%] [G loss: 0.759386]\n",
      "epoch:26 step:24400 [D loss: 0.573870, acc.: 70.31%] [G loss: 0.557360]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.837364\n",
      "FID: 45.444153\n",
      "0 = 12.994162964057917\n",
      "1 = 0.09454327971784736\n",
      "2 = 0.8605999946594238\n",
      "3 = 0.829200029373169\n",
      "4 = 0.8920000195503235\n",
      "5 = 0.8847631216049194\n",
      "6 = 0.829200029373169\n",
      "7 = 8.244245474529272\n",
      "8 = 0.14598670342397324\n",
      "9 = 0.6873000264167786\n",
      "10 = 0.6858000159263611\n",
      "11 = 0.6887999773025513\n",
      "12 = 0.6878635883331299\n",
      "13 = 0.6858000159263611\n",
      "14 = 6.837389945983887\n",
      "15 = 7.056293964385986\n",
      "16 = 0.38236895203590393\n",
      "17 = 6.837364196777344\n",
      "18 = 45.44415283203125\n",
      "epoch:26 step:24401 [D loss: 0.564620, acc.: 71.88%] [G loss: 0.731180]\n",
      "epoch:26 step:24402 [D loss: 0.460261, acc.: 78.12%] [G loss: 0.759081]\n",
      "epoch:26 step:24403 [D loss: 0.544994, acc.: 74.22%] [G loss: 0.755484]\n",
      "epoch:26 step:24404 [D loss: 0.519563, acc.: 71.88%] [G loss: 0.602174]\n",
      "epoch:26 step:24405 [D loss: 0.526508, acc.: 70.31%] [G loss: 0.674293]\n",
      "epoch:26 step:24406 [D loss: 0.601530, acc.: 64.84%] [G loss: 0.621041]\n",
      "epoch:26 step:24407 [D loss: 0.511203, acc.: 75.78%] [G loss: 0.658255]\n",
      "epoch:26 step:24408 [D loss: 0.525339, acc.: 75.00%] [G loss: 0.891003]\n",
      "epoch:26 step:24409 [D loss: 0.547933, acc.: 71.88%] [G loss: 0.801337]\n",
      "epoch:26 step:24410 [D loss: 0.467034, acc.: 78.91%] [G loss: 0.959933]\n",
      "epoch:26 step:24411 [D loss: 0.472818, acc.: 74.22%] [G loss: 0.895629]\n",
      "epoch:26 step:24412 [D loss: 0.528336, acc.: 69.53%] [G loss: 0.767107]\n",
      "epoch:26 step:24413 [D loss: 0.614096, acc.: 64.06%] [G loss: 0.495754]\n",
      "epoch:26 step:24414 [D loss: 0.559977, acc.: 69.53%] [G loss: 0.612629]\n",
      "epoch:26 step:24415 [D loss: 0.514718, acc.: 73.44%] [G loss: 0.801024]\n",
      "epoch:26 step:24416 [D loss: 0.486927, acc.: 73.44%] [G loss: 0.682766]\n",
      "epoch:26 step:24417 [D loss: 0.593589, acc.: 74.22%] [G loss: 0.893643]\n",
      "epoch:26 step:24418 [D loss: 0.516936, acc.: 73.44%] [G loss: 0.844562]\n",
      "epoch:26 step:24419 [D loss: 0.521106, acc.: 67.97%] [G loss: 0.840272]\n",
      "epoch:26 step:24420 [D loss: 0.590641, acc.: 67.97%] [G loss: 0.791558]\n",
      "epoch:26 step:24421 [D loss: 0.541369, acc.: 73.44%] [G loss: 0.769136]\n",
      "epoch:26 step:24422 [D loss: 0.582221, acc.: 66.41%] [G loss: 0.516232]\n",
      "epoch:26 step:24423 [D loss: 0.486106, acc.: 75.78%] [G loss: 0.686532]\n",
      "epoch:26 step:24424 [D loss: 0.612837, acc.: 66.41%] [G loss: 0.567351]\n",
      "epoch:26 step:24425 [D loss: 0.571392, acc.: 67.19%] [G loss: 0.658752]\n",
      "epoch:26 step:24426 [D loss: 0.545941, acc.: 74.22%] [G loss: 0.730329]\n",
      "epoch:26 step:24427 [D loss: 0.550494, acc.: 69.53%] [G loss: 0.639051]\n",
      "epoch:26 step:24428 [D loss: 0.615704, acc.: 63.28%] [G loss: 0.660893]\n",
      "epoch:26 step:24429 [D loss: 0.522415, acc.: 71.88%] [G loss: 0.641026]\n",
      "epoch:26 step:24430 [D loss: 0.551471, acc.: 69.53%] [G loss: 0.598417]\n",
      "epoch:26 step:24431 [D loss: 0.502201, acc.: 69.53%] [G loss: 0.625494]\n",
      "epoch:26 step:24432 [D loss: 0.548168, acc.: 68.75%] [G loss: 0.534911]\n",
      "epoch:26 step:24433 [D loss: 0.550651, acc.: 71.09%] [G loss: 0.779344]\n",
      "epoch:26 step:24434 [D loss: 0.507955, acc.: 78.91%] [G loss: 0.737079]\n",
      "epoch:26 step:24435 [D loss: 0.555959, acc.: 69.53%] [G loss: 0.657141]\n",
      "epoch:26 step:24436 [D loss: 0.455710, acc.: 81.25%] [G loss: 0.640471]\n",
      "epoch:26 step:24437 [D loss: 0.479647, acc.: 75.78%] [G loss: 0.868874]\n",
      "epoch:26 step:24438 [D loss: 0.500859, acc.: 76.56%] [G loss: 0.655348]\n",
      "epoch:26 step:24439 [D loss: 0.453663, acc.: 74.22%] [G loss: 0.756193]\n",
      "epoch:26 step:24440 [D loss: 0.628109, acc.: 64.06%] [G loss: 0.469562]\n",
      "epoch:26 step:24441 [D loss: 0.597000, acc.: 67.97%] [G loss: 0.595645]\n",
      "epoch:26 step:24442 [D loss: 0.477720, acc.: 77.34%] [G loss: 0.667391]\n",
      "epoch:26 step:24443 [D loss: 0.524014, acc.: 75.00%] [G loss: 0.648788]\n",
      "epoch:26 step:24444 [D loss: 0.523906, acc.: 71.88%] [G loss: 0.710488]\n",
      "epoch:26 step:24445 [D loss: 0.472482, acc.: 77.34%] [G loss: 0.997814]\n",
      "epoch:26 step:24446 [D loss: 0.517733, acc.: 74.22%] [G loss: 0.687921]\n",
      "epoch:26 step:24447 [D loss: 0.591305, acc.: 64.84%] [G loss: 0.758123]\n",
      "epoch:26 step:24448 [D loss: 0.570071, acc.: 65.62%] [G loss: 0.675967]\n",
      "epoch:26 step:24449 [D loss: 0.555155, acc.: 67.97%] [G loss: 0.592190]\n",
      "epoch:26 step:24450 [D loss: 0.515874, acc.: 75.78%] [G loss: 0.720824]\n",
      "epoch:26 step:24451 [D loss: 0.497302, acc.: 75.78%] [G loss: 0.807182]\n",
      "epoch:26 step:24452 [D loss: 0.491040, acc.: 71.09%] [G loss: 0.809328]\n",
      "epoch:26 step:24453 [D loss: 0.557218, acc.: 70.31%] [G loss: 0.839166]\n",
      "epoch:26 step:24454 [D loss: 0.475259, acc.: 78.12%] [G loss: 0.905115]\n",
      "epoch:26 step:24455 [D loss: 0.527372, acc.: 73.44%] [G loss: 1.115027]\n",
      "epoch:26 step:24456 [D loss: 0.482312, acc.: 78.12%] [G loss: 0.876673]\n",
      "epoch:26 step:24457 [D loss: 0.528963, acc.: 70.31%] [G loss: 0.809303]\n",
      "epoch:26 step:24458 [D loss: 0.558252, acc.: 74.22%] [G loss: 0.823120]\n",
      "epoch:26 step:24459 [D loss: 0.497876, acc.: 73.44%] [G loss: 0.802244]\n",
      "epoch:26 step:24460 [D loss: 0.575965, acc.: 69.53%] [G loss: 0.762931]\n",
      "epoch:26 step:24461 [D loss: 0.527576, acc.: 74.22%] [G loss: 0.913236]\n",
      "epoch:26 step:24462 [D loss: 0.470687, acc.: 76.56%] [G loss: 0.969260]\n",
      "epoch:26 step:24463 [D loss: 0.528658, acc.: 74.22%] [G loss: 0.811249]\n",
      "epoch:26 step:24464 [D loss: 0.635182, acc.: 60.16%] [G loss: 0.617803]\n",
      "epoch:26 step:24465 [D loss: 0.541185, acc.: 69.53%] [G loss: 0.640341]\n",
      "epoch:26 step:24466 [D loss: 0.522849, acc.: 69.53%] [G loss: 0.634729]\n",
      "epoch:26 step:24467 [D loss: 0.612248, acc.: 64.06%] [G loss: 0.486687]\n",
      "epoch:26 step:24468 [D loss: 0.508282, acc.: 75.00%] [G loss: 0.745677]\n",
      "epoch:26 step:24469 [D loss: 0.614352, acc.: 67.19%] [G loss: 0.773890]\n",
      "epoch:26 step:24470 [D loss: 0.608490, acc.: 64.84%] [G loss: 0.732628]\n",
      "epoch:26 step:24471 [D loss: 0.577963, acc.: 65.62%] [G loss: 0.605258]\n",
      "epoch:26 step:24472 [D loss: 0.536156, acc.: 71.88%] [G loss: 0.535016]\n",
      "epoch:26 step:24473 [D loss: 0.507404, acc.: 75.00%] [G loss: 0.550766]\n",
      "epoch:26 step:24474 [D loss: 0.508098, acc.: 74.22%] [G loss: 0.750896]\n",
      "epoch:26 step:24475 [D loss: 0.537180, acc.: 68.75%] [G loss: 0.625129]\n",
      "epoch:26 step:24476 [D loss: 0.515483, acc.: 75.00%] [G loss: 0.733052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24477 [D loss: 0.541946, acc.: 75.78%] [G loss: 0.778087]\n",
      "epoch:26 step:24478 [D loss: 0.487804, acc.: 73.44%] [G loss: 0.798454]\n",
      "epoch:26 step:24479 [D loss: 0.562673, acc.: 66.41%] [G loss: 0.796971]\n",
      "epoch:26 step:24480 [D loss: 0.492591, acc.: 75.00%] [G loss: 0.817795]\n",
      "epoch:26 step:24481 [D loss: 0.484291, acc.: 74.22%] [G loss: 0.749953]\n",
      "epoch:26 step:24482 [D loss: 0.576946, acc.: 71.09%] [G loss: 0.727819]\n",
      "epoch:26 step:24483 [D loss: 0.550188, acc.: 72.66%] [G loss: 0.697713]\n",
      "epoch:26 step:24484 [D loss: 0.492123, acc.: 78.91%] [G loss: 0.788049]\n",
      "epoch:26 step:24485 [D loss: 0.532478, acc.: 72.66%] [G loss: 0.880799]\n",
      "epoch:26 step:24486 [D loss: 0.549546, acc.: 67.97%] [G loss: 0.859816]\n",
      "epoch:26 step:24487 [D loss: 0.563212, acc.: 71.09%] [G loss: 0.773717]\n",
      "epoch:26 step:24488 [D loss: 0.534830, acc.: 72.66%] [G loss: 0.680258]\n",
      "epoch:26 step:24489 [D loss: 0.502352, acc.: 71.88%] [G loss: 0.721163]\n",
      "epoch:26 step:24490 [D loss: 0.518134, acc.: 71.88%] [G loss: 0.618275]\n",
      "epoch:26 step:24491 [D loss: 0.556700, acc.: 73.44%] [G loss: 0.877679]\n",
      "epoch:26 step:24492 [D loss: 0.545340, acc.: 66.41%] [G loss: 0.622639]\n",
      "epoch:26 step:24493 [D loss: 0.497841, acc.: 72.66%] [G loss: 0.662481]\n",
      "epoch:26 step:24494 [D loss: 0.594007, acc.: 68.75%] [G loss: 0.655963]\n",
      "epoch:26 step:24495 [D loss: 0.593960, acc.: 63.28%] [G loss: 0.855390]\n",
      "epoch:26 step:24496 [D loss: 0.525156, acc.: 72.66%] [G loss: 0.751314]\n",
      "epoch:26 step:24497 [D loss: 0.589787, acc.: 64.84%] [G loss: 0.617112]\n",
      "epoch:26 step:24498 [D loss: 0.489832, acc.: 79.69%] [G loss: 0.768990]\n",
      "epoch:26 step:24499 [D loss: 0.564754, acc.: 69.53%] [G loss: 0.864156]\n",
      "epoch:26 step:24500 [D loss: 0.555915, acc.: 70.31%] [G loss: 0.579165]\n",
      "epoch:26 step:24501 [D loss: 0.546830, acc.: 71.09%] [G loss: 0.686743]\n",
      "epoch:26 step:24502 [D loss: 0.544294, acc.: 68.75%] [G loss: 0.578913]\n",
      "epoch:26 step:24503 [D loss: 0.508045, acc.: 75.78%] [G loss: 0.746342]\n",
      "epoch:26 step:24504 [D loss: 0.503661, acc.: 71.09%] [G loss: 0.645642]\n",
      "epoch:26 step:24505 [D loss: 0.573010, acc.: 67.97%] [G loss: 0.605783]\n",
      "epoch:26 step:24506 [D loss: 0.537601, acc.: 71.09%] [G loss: 0.792532]\n",
      "epoch:26 step:24507 [D loss: 0.534586, acc.: 70.31%] [G loss: 0.588044]\n",
      "epoch:26 step:24508 [D loss: 0.532474, acc.: 73.44%] [G loss: 0.686932]\n",
      "epoch:26 step:24509 [D loss: 0.579002, acc.: 73.44%] [G loss: 0.700041]\n",
      "epoch:26 step:24510 [D loss: 0.587281, acc.: 68.75%] [G loss: 0.633609]\n",
      "epoch:26 step:24511 [D loss: 0.455764, acc.: 81.25%] [G loss: 0.576103]\n",
      "epoch:26 step:24512 [D loss: 0.620086, acc.: 64.84%] [G loss: 0.623072]\n",
      "epoch:26 step:24513 [D loss: 0.533565, acc.: 75.00%] [G loss: 0.628476]\n",
      "epoch:26 step:24514 [D loss: 0.496519, acc.: 78.12%] [G loss: 0.710603]\n",
      "epoch:26 step:24515 [D loss: 0.570518, acc.: 67.97%] [G loss: 0.714644]\n",
      "epoch:26 step:24516 [D loss: 0.562346, acc.: 67.19%] [G loss: 0.736692]\n",
      "epoch:26 step:24517 [D loss: 0.422418, acc.: 80.47%] [G loss: 0.856747]\n",
      "epoch:26 step:24518 [D loss: 0.495276, acc.: 72.66%] [G loss: 0.823882]\n",
      "epoch:26 step:24519 [D loss: 0.571411, acc.: 69.53%] [G loss: 0.740475]\n",
      "epoch:26 step:24520 [D loss: 0.565663, acc.: 68.75%] [G loss: 0.575090]\n",
      "epoch:26 step:24521 [D loss: 0.459175, acc.: 77.34%] [G loss: 0.792540]\n",
      "epoch:26 step:24522 [D loss: 0.574105, acc.: 65.62%] [G loss: 0.766994]\n",
      "epoch:26 step:24523 [D loss: 0.553445, acc.: 71.09%] [G loss: 0.851061]\n",
      "epoch:26 step:24524 [D loss: 0.477403, acc.: 77.34%] [G loss: 0.879347]\n",
      "epoch:26 step:24525 [D loss: 0.545864, acc.: 69.53%] [G loss: 0.844008]\n",
      "epoch:26 step:24526 [D loss: 0.563316, acc.: 67.19%] [G loss: 0.752414]\n",
      "epoch:26 step:24527 [D loss: 0.583848, acc.: 71.09%] [G loss: 0.735595]\n",
      "epoch:26 step:24528 [D loss: 0.590059, acc.: 64.06%] [G loss: 0.610426]\n",
      "epoch:26 step:24529 [D loss: 0.564671, acc.: 68.75%] [G loss: 0.690914]\n",
      "epoch:26 step:24530 [D loss: 0.535766, acc.: 74.22%] [G loss: 0.663954]\n",
      "epoch:26 step:24531 [D loss: 0.596840, acc.: 67.19%] [G loss: 0.630353]\n",
      "epoch:26 step:24532 [D loss: 0.537231, acc.: 68.75%] [G loss: 0.592121]\n",
      "epoch:26 step:24533 [D loss: 0.524814, acc.: 67.97%] [G loss: 0.664389]\n",
      "epoch:26 step:24534 [D loss: 0.511762, acc.: 73.44%] [G loss: 0.724698]\n",
      "epoch:26 step:24535 [D loss: 0.493668, acc.: 75.78%] [G loss: 0.805036]\n",
      "epoch:26 step:24536 [D loss: 0.590352, acc.: 71.09%] [G loss: 0.603579]\n",
      "epoch:26 step:24537 [D loss: 0.597473, acc.: 68.75%] [G loss: 0.706831]\n",
      "epoch:26 step:24538 [D loss: 0.555075, acc.: 71.09%] [G loss: 0.594480]\n",
      "epoch:26 step:24539 [D loss: 0.509652, acc.: 71.09%] [G loss: 0.599245]\n",
      "epoch:26 step:24540 [D loss: 0.561104, acc.: 72.66%] [G loss: 0.697486]\n",
      "epoch:26 step:24541 [D loss: 0.521868, acc.: 71.88%] [G loss: 0.645338]\n",
      "epoch:26 step:24542 [D loss: 0.601127, acc.: 66.41%] [G loss: 0.588301]\n",
      "epoch:26 step:24543 [D loss: 0.600318, acc.: 62.50%] [G loss: 0.597464]\n",
      "epoch:26 step:24544 [D loss: 0.515918, acc.: 74.22%] [G loss: 0.831663]\n",
      "epoch:26 step:24545 [D loss: 0.591551, acc.: 67.19%] [G loss: 0.674662]\n",
      "epoch:26 step:24546 [D loss: 0.517999, acc.: 71.09%] [G loss: 0.852476]\n",
      "epoch:26 step:24547 [D loss: 0.606964, acc.: 64.06%] [G loss: 0.632512]\n",
      "epoch:26 step:24548 [D loss: 0.492886, acc.: 71.88%] [G loss: 0.896726]\n",
      "epoch:26 step:24549 [D loss: 0.590604, acc.: 62.50%] [G loss: 0.590928]\n",
      "epoch:26 step:24550 [D loss: 0.567998, acc.: 67.97%] [G loss: 0.696081]\n",
      "epoch:26 step:24551 [D loss: 0.578737, acc.: 62.50%] [G loss: 0.565715]\n",
      "epoch:26 step:24552 [D loss: 0.473361, acc.: 77.34%] [G loss: 0.680955]\n",
      "epoch:26 step:24553 [D loss: 0.510112, acc.: 70.31%] [G loss: 0.756338]\n",
      "epoch:26 step:24554 [D loss: 0.507139, acc.: 75.78%] [G loss: 0.580531]\n",
      "epoch:26 step:24555 [D loss: 0.530537, acc.: 71.88%] [G loss: 0.730726]\n",
      "epoch:26 step:24556 [D loss: 0.427366, acc.: 80.47%] [G loss: 0.736538]\n",
      "epoch:26 step:24557 [D loss: 0.573206, acc.: 68.75%] [G loss: 0.782864]\n",
      "epoch:26 step:24558 [D loss: 0.558190, acc.: 65.62%] [G loss: 0.534676]\n",
      "epoch:26 step:24559 [D loss: 0.496872, acc.: 72.66%] [G loss: 0.727516]\n",
      "epoch:26 step:24560 [D loss: 0.448502, acc.: 79.69%] [G loss: 0.674142]\n",
      "epoch:26 step:24561 [D loss: 0.539305, acc.: 71.88%] [G loss: 0.887536]\n",
      "epoch:26 step:24562 [D loss: 0.611173, acc.: 67.97%] [G loss: 0.673730]\n",
      "epoch:26 step:24563 [D loss: 0.602373, acc.: 65.62%] [G loss: 0.733000]\n",
      "epoch:26 step:24564 [D loss: 0.522938, acc.: 72.66%] [G loss: 0.660242]\n",
      "epoch:26 step:24565 [D loss: 0.602147, acc.: 65.62%] [G loss: 0.722748]\n",
      "epoch:26 step:24566 [D loss: 0.548661, acc.: 73.44%] [G loss: 0.766550]\n",
      "epoch:26 step:24567 [D loss: 0.485315, acc.: 74.22%] [G loss: 0.853196]\n",
      "epoch:26 step:24568 [D loss: 0.481024, acc.: 74.22%] [G loss: 0.832689]\n",
      "epoch:26 step:24569 [D loss: 0.486818, acc.: 77.34%] [G loss: 0.807925]\n",
      "epoch:26 step:24570 [D loss: 0.418261, acc.: 84.38%] [G loss: 0.725501]\n",
      "epoch:26 step:24571 [D loss: 0.474997, acc.: 73.44%] [G loss: 0.900298]\n",
      "epoch:26 step:24572 [D loss: 0.643914, acc.: 63.28%] [G loss: 0.657068]\n",
      "epoch:26 step:24573 [D loss: 0.534943, acc.: 73.44%] [G loss: 0.708159]\n",
      "epoch:26 step:24574 [D loss: 0.544122, acc.: 74.22%] [G loss: 0.558261]\n",
      "epoch:26 step:24575 [D loss: 0.510710, acc.: 69.53%] [G loss: 0.623485]\n",
      "epoch:26 step:24576 [D loss: 0.628054, acc.: 64.84%] [G loss: 0.711932]\n",
      "epoch:26 step:24577 [D loss: 0.607197, acc.: 60.94%] [G loss: 0.547756]\n",
      "epoch:26 step:24578 [D loss: 0.532633, acc.: 75.00%] [G loss: 0.569838]\n",
      "epoch:26 step:24579 [D loss: 0.506567, acc.: 70.31%] [G loss: 0.732031]\n",
      "epoch:26 step:24580 [D loss: 0.494519, acc.: 76.56%] [G loss: 0.716855]\n",
      "epoch:26 step:24581 [D loss: 0.479065, acc.: 80.47%] [G loss: 0.787807]\n",
      "epoch:26 step:24582 [D loss: 0.635371, acc.: 67.19%] [G loss: 0.594705]\n",
      "epoch:26 step:24583 [D loss: 0.522208, acc.: 69.53%] [G loss: 0.685115]\n",
      "epoch:26 step:24584 [D loss: 0.461483, acc.: 78.91%] [G loss: 0.808878]\n",
      "epoch:26 step:24585 [D loss: 0.536738, acc.: 72.66%] [G loss: 0.799937]\n",
      "epoch:26 step:24586 [D loss: 0.559752, acc.: 71.09%] [G loss: 0.760299]\n",
      "epoch:26 step:24587 [D loss: 0.511998, acc.: 79.69%] [G loss: 0.814221]\n",
      "epoch:26 step:24588 [D loss: 0.580013, acc.: 70.31%] [G loss: 0.658499]\n",
      "epoch:26 step:24589 [D loss: 0.553666, acc.: 64.06%] [G loss: 0.788503]\n",
      "epoch:26 step:24590 [D loss: 0.657716, acc.: 60.16%] [G loss: 0.503602]\n",
      "epoch:26 step:24591 [D loss: 0.515094, acc.: 76.56%] [G loss: 0.602746]\n",
      "epoch:26 step:24592 [D loss: 0.517851, acc.: 74.22%] [G loss: 0.630169]\n",
      "epoch:26 step:24593 [D loss: 0.490517, acc.: 78.12%] [G loss: 0.804902]\n",
      "epoch:26 step:24594 [D loss: 0.467348, acc.: 80.47%] [G loss: 0.984955]\n",
      "epoch:26 step:24595 [D loss: 0.478745, acc.: 78.12%] [G loss: 0.679253]\n",
      "epoch:26 step:24596 [D loss: 0.555862, acc.: 68.75%] [G loss: 0.885395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24597 [D loss: 0.580127, acc.: 67.97%] [G loss: 0.539004]\n",
      "epoch:26 step:24598 [D loss: 0.525986, acc.: 75.00%] [G loss: 0.575064]\n",
      "epoch:26 step:24599 [D loss: 0.554226, acc.: 71.09%] [G loss: 0.580372]\n",
      "epoch:26 step:24600 [D loss: 0.576024, acc.: 67.97%] [G loss: 0.529462]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.522022\n",
      "FID: 52.817230\n",
      "0 = 13.00619473190307\n",
      "1 = 0.0918182997355146\n",
      "2 = 0.8651000261306763\n",
      "3 = 0.8276000022888184\n",
      "4 = 0.9025999903678894\n",
      "5 = 0.894702672958374\n",
      "6 = 0.8276000022888184\n",
      "7 = 8.495448376393332\n",
      "8 = 0.16177333334493285\n",
      "9 = 0.6988000273704529\n",
      "10 = 0.6863999962806702\n",
      "11 = 0.7111999988555908\n",
      "12 = 0.7038556337356567\n",
      "13 = 0.6863999962806702\n",
      "14 = 6.522047996520996\n",
      "15 = 6.725286483764648\n",
      "16 = 0.4075924754142761\n",
      "17 = 6.522022247314453\n",
      "18 = 52.817230224609375\n",
      "epoch:26 step:24601 [D loss: 0.482905, acc.: 74.22%] [G loss: 0.698207]\n",
      "epoch:26 step:24602 [D loss: 0.548819, acc.: 73.44%] [G loss: 0.644512]\n",
      "epoch:26 step:24603 [D loss: 0.488421, acc.: 79.69%] [G loss: 0.628855]\n",
      "epoch:26 step:24604 [D loss: 0.481152, acc.: 72.66%] [G loss: 0.732970]\n",
      "epoch:26 step:24605 [D loss: 0.539334, acc.: 71.09%] [G loss: 0.587674]\n",
      "epoch:26 step:24606 [D loss: 0.490975, acc.: 78.91%] [G loss: 0.603110]\n",
      "epoch:26 step:24607 [D loss: 0.561211, acc.: 66.41%] [G loss: 0.848220]\n",
      "epoch:26 step:24608 [D loss: 0.495831, acc.: 75.00%] [G loss: 0.859872]\n",
      "epoch:26 step:24609 [D loss: 0.520500, acc.: 77.34%] [G loss: 0.901598]\n",
      "epoch:26 step:24610 [D loss: 0.472197, acc.: 78.12%] [G loss: 0.888067]\n",
      "epoch:26 step:24611 [D loss: 0.584168, acc.: 70.31%] [G loss: 0.646282]\n",
      "epoch:26 step:24612 [D loss: 0.600863, acc.: 65.62%] [G loss: 0.693745]\n",
      "epoch:26 step:24613 [D loss: 0.684981, acc.: 61.72%] [G loss: 0.678529]\n",
      "epoch:26 step:24614 [D loss: 0.542470, acc.: 69.53%] [G loss: 0.706171]\n",
      "epoch:26 step:24615 [D loss: 0.548587, acc.: 70.31%] [G loss: 0.733109]\n",
      "epoch:26 step:24616 [D loss: 0.480711, acc.: 74.22%] [G loss: 0.784689]\n",
      "epoch:26 step:24617 [D loss: 0.595492, acc.: 67.97%] [G loss: 0.585268]\n",
      "epoch:26 step:24618 [D loss: 0.553197, acc.: 67.97%] [G loss: 0.590583]\n",
      "epoch:26 step:24619 [D loss: 0.565895, acc.: 68.75%] [G loss: 0.548569]\n",
      "epoch:26 step:24620 [D loss: 0.520639, acc.: 71.09%] [G loss: 0.587269]\n",
      "epoch:26 step:24621 [D loss: 0.503496, acc.: 75.78%] [G loss: 0.795774]\n",
      "epoch:26 step:24622 [D loss: 0.612655, acc.: 60.16%] [G loss: 0.537742]\n",
      "epoch:26 step:24623 [D loss: 0.498621, acc.: 74.22%] [G loss: 0.661577]\n",
      "epoch:26 step:24624 [D loss: 0.508214, acc.: 74.22%] [G loss: 0.654171]\n",
      "epoch:26 step:24625 [D loss: 0.585064, acc.: 67.97%] [G loss: 0.530169]\n",
      "epoch:26 step:24626 [D loss: 0.561587, acc.: 65.62%] [G loss: 0.684626]\n",
      "epoch:26 step:24627 [D loss: 0.466295, acc.: 76.56%] [G loss: 0.765974]\n",
      "epoch:26 step:24628 [D loss: 0.567357, acc.: 71.88%] [G loss: 0.622116]\n",
      "epoch:26 step:24629 [D loss: 0.607655, acc.: 66.41%] [G loss: 0.577024]\n",
      "epoch:26 step:24630 [D loss: 0.491532, acc.: 73.44%] [G loss: 0.629120]\n",
      "epoch:26 step:24631 [D loss: 0.512944, acc.: 72.66%] [G loss: 0.725639]\n",
      "epoch:26 step:24632 [D loss: 0.488603, acc.: 80.47%] [G loss: 0.660010]\n",
      "epoch:26 step:24633 [D loss: 0.509881, acc.: 70.31%] [G loss: 0.709098]\n",
      "epoch:26 step:24634 [D loss: 0.576600, acc.: 67.19%] [G loss: 0.677521]\n",
      "epoch:26 step:24635 [D loss: 0.484200, acc.: 74.22%] [G loss: 0.798573]\n",
      "epoch:26 step:24636 [D loss: 0.475313, acc.: 77.34%] [G loss: 0.883511]\n",
      "epoch:26 step:24637 [D loss: 0.592901, acc.: 69.53%] [G loss: 0.762744]\n",
      "epoch:26 step:24638 [D loss: 0.479330, acc.: 76.56%] [G loss: 0.636909]\n",
      "epoch:26 step:24639 [D loss: 0.633401, acc.: 64.84%] [G loss: 0.700481]\n",
      "epoch:26 step:24640 [D loss: 0.651699, acc.: 60.94%] [G loss: 0.567997]\n",
      "epoch:26 step:24641 [D loss: 0.507531, acc.: 71.09%] [G loss: 0.704696]\n",
      "epoch:26 step:24642 [D loss: 0.528603, acc.: 71.09%] [G loss: 0.797667]\n",
      "epoch:26 step:24643 [D loss: 0.603077, acc.: 64.84%] [G loss: 0.605916]\n",
      "epoch:26 step:24644 [D loss: 0.565144, acc.: 70.31%] [G loss: 0.679266]\n",
      "epoch:26 step:24645 [D loss: 0.483428, acc.: 75.00%] [G loss: 0.779717]\n",
      "epoch:26 step:24646 [D loss: 0.539293, acc.: 72.66%] [G loss: 0.544115]\n",
      "epoch:26 step:24647 [D loss: 0.532926, acc.: 71.09%] [G loss: 0.629209]\n",
      "epoch:26 step:24648 [D loss: 0.487487, acc.: 71.88%] [G loss: 0.685215]\n",
      "epoch:26 step:24649 [D loss: 0.590187, acc.: 63.28%] [G loss: 0.695523]\n",
      "epoch:26 step:24650 [D loss: 0.617762, acc.: 64.06%] [G loss: 0.654840]\n",
      "epoch:26 step:24651 [D loss: 0.499134, acc.: 74.22%] [G loss: 0.868567]\n",
      "epoch:26 step:24652 [D loss: 0.536330, acc.: 72.66%] [G loss: 0.674229]\n",
      "epoch:26 step:24653 [D loss: 0.613161, acc.: 68.75%] [G loss: 0.598860]\n",
      "epoch:26 step:24654 [D loss: 0.548406, acc.: 69.53%] [G loss: 0.649823]\n",
      "epoch:26 step:24655 [D loss: 0.532100, acc.: 71.88%] [G loss: 0.634073]\n",
      "epoch:26 step:24656 [D loss: 0.627391, acc.: 61.72%] [G loss: 0.609496]\n",
      "epoch:26 step:24657 [D loss: 0.543282, acc.: 68.75%] [G loss: 0.579866]\n",
      "epoch:26 step:24658 [D loss: 0.458658, acc.: 75.78%] [G loss: 0.708258]\n",
      "epoch:26 step:24659 [D loss: 0.523626, acc.: 74.22%] [G loss: 0.725367]\n",
      "epoch:26 step:24660 [D loss: 0.441642, acc.: 83.59%] [G loss: 0.807388]\n",
      "epoch:26 step:24661 [D loss: 0.506308, acc.: 75.78%] [G loss: 0.861529]\n",
      "epoch:26 step:24662 [D loss: 0.530253, acc.: 70.31%] [G loss: 0.886504]\n",
      "epoch:26 step:24663 [D loss: 0.639700, acc.: 64.84%] [G loss: 0.852013]\n",
      "epoch:26 step:24664 [D loss: 0.515695, acc.: 67.19%] [G loss: 0.643412]\n",
      "epoch:26 step:24665 [D loss: 0.578454, acc.: 67.19%] [G loss: 0.719960]\n",
      "epoch:26 step:24666 [D loss: 0.513368, acc.: 71.09%] [G loss: 0.659732]\n",
      "epoch:26 step:24667 [D loss: 0.533221, acc.: 71.88%] [G loss: 0.792440]\n",
      "epoch:26 step:24668 [D loss: 0.450360, acc.: 77.34%] [G loss: 0.862160]\n",
      "epoch:26 step:24669 [D loss: 0.468798, acc.: 76.56%] [G loss: 0.901274]\n",
      "epoch:26 step:24670 [D loss: 0.609203, acc.: 62.50%] [G loss: 0.739531]\n",
      "epoch:26 step:24671 [D loss: 0.511594, acc.: 69.53%] [G loss: 0.688034]\n",
      "epoch:26 step:24672 [D loss: 0.513137, acc.: 73.44%] [G loss: 0.823112]\n",
      "epoch:26 step:24673 [D loss: 0.485601, acc.: 74.22%] [G loss: 0.742105]\n",
      "epoch:26 step:24674 [D loss: 0.546145, acc.: 69.53%] [G loss: 0.671898]\n",
      "epoch:26 step:24675 [D loss: 0.514272, acc.: 75.00%] [G loss: 0.874265]\n",
      "epoch:26 step:24676 [D loss: 0.429157, acc.: 78.12%] [G loss: 0.958046]\n",
      "epoch:26 step:24677 [D loss: 0.438454, acc.: 81.25%] [G loss: 0.998877]\n",
      "epoch:26 step:24678 [D loss: 0.691945, acc.: 61.72%] [G loss: 0.805832]\n",
      "epoch:26 step:24679 [D loss: 0.592669, acc.: 68.75%] [G loss: 0.685486]\n",
      "epoch:26 step:24680 [D loss: 0.511226, acc.: 74.22%] [G loss: 0.635894]\n",
      "epoch:26 step:24681 [D loss: 0.543907, acc.: 66.41%] [G loss: 0.584419]\n",
      "epoch:26 step:24682 [D loss: 0.531268, acc.: 71.09%] [G loss: 0.625057]\n",
      "epoch:26 step:24683 [D loss: 0.468700, acc.: 78.12%] [G loss: 0.831383]\n",
      "epoch:26 step:24684 [D loss: 0.572212, acc.: 67.97%] [G loss: 0.631948]\n",
      "epoch:26 step:24685 [D loss: 0.634098, acc.: 64.06%] [G loss: 0.589274]\n",
      "epoch:26 step:24686 [D loss: 0.500747, acc.: 78.12%] [G loss: 0.682743]\n",
      "epoch:26 step:24687 [D loss: 0.565096, acc.: 70.31%] [G loss: 0.617826]\n",
      "epoch:26 step:24688 [D loss: 0.480632, acc.: 73.44%] [G loss: 0.636038]\n",
      "epoch:26 step:24689 [D loss: 0.525517, acc.: 67.97%] [G loss: 0.799638]\n",
      "epoch:26 step:24690 [D loss: 0.489823, acc.: 76.56%] [G loss: 0.787724]\n",
      "epoch:26 step:24691 [D loss: 0.507058, acc.: 71.88%] [G loss: 0.807622]\n",
      "epoch:26 step:24692 [D loss: 0.539293, acc.: 69.53%] [G loss: 0.806506]\n",
      "epoch:26 step:24693 [D loss: 0.570780, acc.: 66.41%] [G loss: 0.675341]\n",
      "epoch:26 step:24694 [D loss: 0.516029, acc.: 75.00%] [G loss: 0.706359]\n",
      "epoch:26 step:24695 [D loss: 0.481338, acc.: 75.78%] [G loss: 0.795954]\n",
      "epoch:26 step:24696 [D loss: 0.504177, acc.: 75.78%] [G loss: 0.685846]\n",
      "epoch:26 step:24697 [D loss: 0.562395, acc.: 69.53%] [G loss: 0.769076]\n",
      "epoch:26 step:24698 [D loss: 0.457166, acc.: 75.00%] [G loss: 0.733080]\n",
      "epoch:26 step:24699 [D loss: 0.548466, acc.: 74.22%] [G loss: 0.624592]\n",
      "epoch:26 step:24700 [D loss: 0.551802, acc.: 71.09%] [G loss: 0.737439]\n",
      "epoch:26 step:24701 [D loss: 0.543680, acc.: 71.88%] [G loss: 0.794180]\n",
      "epoch:26 step:24702 [D loss: 0.513658, acc.: 75.78%] [G loss: 0.772559]\n",
      "epoch:26 step:24703 [D loss: 0.572391, acc.: 74.22%] [G loss: 0.806888]\n",
      "epoch:26 step:24704 [D loss: 0.613885, acc.: 60.94%] [G loss: 0.576356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24705 [D loss: 0.498959, acc.: 76.56%] [G loss: 0.703417]\n",
      "epoch:26 step:24706 [D loss: 0.482349, acc.: 78.91%] [G loss: 0.792817]\n",
      "epoch:26 step:24707 [D loss: 0.559739, acc.: 67.97%] [G loss: 0.763995]\n",
      "epoch:26 step:24708 [D loss: 0.534332, acc.: 73.44%] [G loss: 0.727972]\n",
      "epoch:26 step:24709 [D loss: 0.456827, acc.: 78.12%] [G loss: 0.788016]\n",
      "epoch:26 step:24710 [D loss: 0.620258, acc.: 63.28%] [G loss: 0.751641]\n",
      "epoch:26 step:24711 [D loss: 0.685270, acc.: 60.94%] [G loss: 0.648031]\n",
      "epoch:26 step:24712 [D loss: 0.516517, acc.: 74.22%] [G loss: 0.637679]\n",
      "epoch:26 step:24713 [D loss: 0.535455, acc.: 71.88%] [G loss: 0.594211]\n",
      "epoch:26 step:24714 [D loss: 0.600699, acc.: 63.28%] [G loss: 0.706656]\n",
      "epoch:26 step:24715 [D loss: 0.559986, acc.: 69.53%] [G loss: 0.676160]\n",
      "epoch:26 step:24716 [D loss: 0.402817, acc.: 82.81%] [G loss: 0.847801]\n",
      "epoch:26 step:24717 [D loss: 0.578147, acc.: 69.53%] [G loss: 0.873110]\n",
      "epoch:26 step:24718 [D loss: 0.542811, acc.: 72.66%] [G loss: 0.790160]\n",
      "epoch:26 step:24719 [D loss: 0.485105, acc.: 78.91%] [G loss: 0.798442]\n",
      "epoch:26 step:24720 [D loss: 0.514593, acc.: 68.75%] [G loss: 0.841037]\n",
      "epoch:26 step:24721 [D loss: 0.456193, acc.: 76.56%] [G loss: 1.069928]\n",
      "epoch:26 step:24722 [D loss: 0.536355, acc.: 72.66%] [G loss: 0.971835]\n",
      "epoch:26 step:24723 [D loss: 0.510233, acc.: 76.56%] [G loss: 0.805579]\n",
      "epoch:26 step:24724 [D loss: 0.560807, acc.: 71.88%] [G loss: 0.880201]\n",
      "epoch:26 step:24725 [D loss: 0.583904, acc.: 67.19%] [G loss: 0.732184]\n",
      "epoch:26 step:24726 [D loss: 0.530694, acc.: 71.09%] [G loss: 0.591952]\n",
      "epoch:26 step:24727 [D loss: 0.518874, acc.: 70.31%] [G loss: 0.570818]\n",
      "epoch:26 step:24728 [D loss: 0.572119, acc.: 72.66%] [G loss: 0.829246]\n",
      "epoch:26 step:24729 [D loss: 0.583816, acc.: 67.19%] [G loss: 0.706076]\n",
      "epoch:26 step:24730 [D loss: 0.526977, acc.: 74.22%] [G loss: 0.694591]\n",
      "epoch:26 step:24731 [D loss: 0.485227, acc.: 77.34%] [G loss: 0.713796]\n",
      "epoch:26 step:24732 [D loss: 0.622536, acc.: 65.62%] [G loss: 0.814772]\n",
      "epoch:26 step:24733 [D loss: 0.495674, acc.: 72.66%] [G loss: 0.847517]\n",
      "epoch:26 step:24734 [D loss: 0.544818, acc.: 69.53%] [G loss: 0.751382]\n",
      "epoch:26 step:24735 [D loss: 0.598707, acc.: 63.28%] [G loss: 0.588826]\n",
      "epoch:26 step:24736 [D loss: 0.445981, acc.: 82.81%] [G loss: 0.807925]\n",
      "epoch:26 step:24737 [D loss: 0.577413, acc.: 71.09%] [G loss: 0.707497]\n",
      "epoch:26 step:24738 [D loss: 0.690423, acc.: 60.16%] [G loss: 0.523840]\n",
      "epoch:26 step:24739 [D loss: 0.580553, acc.: 70.31%] [G loss: 0.549785]\n",
      "epoch:26 step:24740 [D loss: 0.556325, acc.: 65.62%] [G loss: 0.649685]\n",
      "epoch:26 step:24741 [D loss: 0.542201, acc.: 65.62%] [G loss: 0.624280]\n",
      "epoch:26 step:24742 [D loss: 0.556333, acc.: 72.66%] [G loss: 0.629443]\n",
      "epoch:26 step:24743 [D loss: 0.440144, acc.: 78.12%] [G loss: 0.763234]\n",
      "epoch:26 step:24744 [D loss: 0.527179, acc.: 70.31%] [G loss: 0.706699]\n",
      "epoch:26 step:24745 [D loss: 0.520649, acc.: 72.66%] [G loss: 0.725706]\n",
      "epoch:26 step:24746 [D loss: 0.540017, acc.: 70.31%] [G loss: 0.547098]\n",
      "epoch:26 step:24747 [D loss: 0.480297, acc.: 74.22%] [G loss: 0.629955]\n",
      "epoch:26 step:24748 [D loss: 0.605155, acc.: 64.84%] [G loss: 0.716396]\n",
      "epoch:26 step:24749 [D loss: 0.571452, acc.: 71.88%] [G loss: 0.796365]\n",
      "epoch:26 step:24750 [D loss: 0.577103, acc.: 72.66%] [G loss: 0.654353]\n",
      "epoch:26 step:24751 [D loss: 0.509794, acc.: 71.88%] [G loss: 0.740073]\n",
      "epoch:26 step:24752 [D loss: 0.585964, acc.: 64.06%] [G loss: 0.682256]\n",
      "epoch:26 step:24753 [D loss: 0.519984, acc.: 70.31%] [G loss: 0.669320]\n",
      "epoch:26 step:24754 [D loss: 0.500911, acc.: 72.66%] [G loss: 0.735862]\n",
      "epoch:26 step:24755 [D loss: 0.584431, acc.: 67.97%] [G loss: 0.699214]\n",
      "epoch:26 step:24756 [D loss: 0.501214, acc.: 73.44%] [G loss: 0.627912]\n",
      "epoch:26 step:24757 [D loss: 0.561962, acc.: 73.44%] [G loss: 0.746920]\n",
      "epoch:26 step:24758 [D loss: 0.603548, acc.: 66.41%] [G loss: 0.863066]\n",
      "epoch:26 step:24759 [D loss: 0.574967, acc.: 67.19%] [G loss: 0.674540]\n",
      "epoch:26 step:24760 [D loss: 0.462308, acc.: 81.25%] [G loss: 0.755382]\n",
      "epoch:26 step:24761 [D loss: 0.489697, acc.: 75.78%] [G loss: 0.805207]\n",
      "epoch:26 step:24762 [D loss: 0.667866, acc.: 57.81%] [G loss: 0.623226]\n",
      "epoch:26 step:24763 [D loss: 0.595185, acc.: 69.53%] [G loss: 0.610139]\n",
      "epoch:26 step:24764 [D loss: 0.485271, acc.: 71.09%] [G loss: 0.705200]\n",
      "epoch:26 step:24765 [D loss: 0.526603, acc.: 75.00%] [G loss: 0.815783]\n",
      "epoch:26 step:24766 [D loss: 0.590306, acc.: 67.97%] [G loss: 0.865051]\n",
      "epoch:26 step:24767 [D loss: 0.555702, acc.: 68.75%] [G loss: 0.856574]\n",
      "epoch:26 step:24768 [D loss: 0.455030, acc.: 79.69%] [G loss: 0.796445]\n",
      "epoch:26 step:24769 [D loss: 0.600255, acc.: 67.97%] [G loss: 0.721123]\n",
      "epoch:26 step:24770 [D loss: 0.553339, acc.: 72.66%] [G loss: 0.756166]\n",
      "epoch:26 step:24771 [D loss: 0.555998, acc.: 69.53%] [G loss: 0.708168]\n",
      "epoch:26 step:24772 [D loss: 0.577623, acc.: 63.28%] [G loss: 0.721943]\n",
      "epoch:26 step:24773 [D loss: 0.646266, acc.: 57.81%] [G loss: 0.640330]\n",
      "epoch:26 step:24774 [D loss: 0.592025, acc.: 66.41%] [G loss: 0.507209]\n",
      "epoch:26 step:24775 [D loss: 0.560303, acc.: 70.31%] [G loss: 0.550715]\n",
      "epoch:26 step:24776 [D loss: 0.544973, acc.: 70.31%] [G loss: 0.599702]\n",
      "epoch:26 step:24777 [D loss: 0.502418, acc.: 73.44%] [G loss: 0.814256]\n",
      "epoch:26 step:24778 [D loss: 0.493365, acc.: 77.34%] [G loss: 0.823842]\n",
      "epoch:26 step:24779 [D loss: 0.522020, acc.: 74.22%] [G loss: 0.749893]\n",
      "epoch:26 step:24780 [D loss: 0.623585, acc.: 64.84%] [G loss: 0.651669]\n",
      "epoch:26 step:24781 [D loss: 0.547563, acc.: 70.31%] [G loss: 0.701883]\n",
      "epoch:26 step:24782 [D loss: 0.592827, acc.: 64.84%] [G loss: 0.844639]\n",
      "epoch:26 step:24783 [D loss: 0.582413, acc.: 73.44%] [G loss: 0.641860]\n",
      "epoch:26 step:24784 [D loss: 0.615410, acc.: 68.75%] [G loss: 0.731395]\n",
      "epoch:26 step:24785 [D loss: 0.571144, acc.: 67.97%] [G loss: 0.719790]\n",
      "epoch:26 step:24786 [D loss: 0.616930, acc.: 62.50%] [G loss: 0.619943]\n",
      "epoch:26 step:24787 [D loss: 0.544785, acc.: 69.53%] [G loss: 0.676276]\n",
      "epoch:26 step:24788 [D loss: 0.478656, acc.: 75.78%] [G loss: 0.651593]\n",
      "epoch:26 step:24789 [D loss: 0.451679, acc.: 78.12%] [G loss: 0.802674]\n",
      "epoch:26 step:24790 [D loss: 0.524536, acc.: 71.09%] [G loss: 0.828321]\n",
      "epoch:26 step:24791 [D loss: 0.485655, acc.: 72.66%] [G loss: 0.929621]\n",
      "epoch:26 step:24792 [D loss: 0.526834, acc.: 74.22%] [G loss: 0.721442]\n",
      "epoch:26 step:24793 [D loss: 0.546970, acc.: 67.19%] [G loss: 0.844258]\n",
      "epoch:26 step:24794 [D loss: 0.589116, acc.: 67.19%] [G loss: 0.670493]\n",
      "epoch:26 step:24795 [D loss: 0.540132, acc.: 75.00%] [G loss: 0.775087]\n",
      "epoch:26 step:24796 [D loss: 0.586014, acc.: 71.88%] [G loss: 0.501196]\n",
      "epoch:26 step:24797 [D loss: 0.565354, acc.: 62.50%] [G loss: 0.627130]\n",
      "epoch:26 step:24798 [D loss: 0.469337, acc.: 75.00%] [G loss: 0.923319]\n",
      "epoch:26 step:24799 [D loss: 0.670524, acc.: 57.81%] [G loss: 0.610695]\n",
      "epoch:26 step:24800 [D loss: 0.557578, acc.: 68.75%] [G loss: 0.579891]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.638289\n",
      "FID: 54.756046\n",
      "0 = 12.84465534734723\n",
      "1 = 0.0866647638209842\n",
      "2 = 0.847599983215332\n",
      "3 = 0.8011999726295471\n",
      "4 = 0.8939999938011169\n",
      "5 = 0.883156955242157\n",
      "6 = 0.8011999726295471\n",
      "7 = 8.552819117856016\n",
      "8 = 0.16054543450444853\n",
      "9 = 0.6866999864578247\n",
      "10 = 0.680400013923645\n",
      "11 = 0.6930000185966492\n",
      "12 = 0.6890824437141418\n",
      "13 = 0.680400013923645\n",
      "14 = 6.638314247131348\n",
      "15 = 6.708944797515869\n",
      "16 = 0.40570348501205444\n",
      "17 = 6.638289451599121\n",
      "18 = 54.756046295166016\n",
      "epoch:26 step:24801 [D loss: 0.527180, acc.: 71.88%] [G loss: 0.752187]\n",
      "epoch:26 step:24802 [D loss: 0.495053, acc.: 78.12%] [G loss: 0.937004]\n",
      "epoch:26 step:24803 [D loss: 0.533340, acc.: 69.53%] [G loss: 0.811092]\n",
      "epoch:26 step:24804 [D loss: 0.500493, acc.: 74.22%] [G loss: 0.973069]\n",
      "epoch:26 step:24805 [D loss: 0.554263, acc.: 68.75%] [G loss: 0.892506]\n",
      "epoch:26 step:24806 [D loss: 0.446906, acc.: 76.56%] [G loss: 0.857535]\n",
      "epoch:26 step:24807 [D loss: 0.567472, acc.: 64.06%] [G loss: 0.720318]\n",
      "epoch:26 step:24808 [D loss: 0.532382, acc.: 67.19%] [G loss: 0.537082]\n",
      "epoch:26 step:24809 [D loss: 0.556662, acc.: 66.41%] [G loss: 0.912540]\n",
      "epoch:26 step:24810 [D loss: 0.493378, acc.: 74.22%] [G loss: 0.793543]\n",
      "epoch:26 step:24811 [D loss: 0.432753, acc.: 82.81%] [G loss: 0.688864]\n",
      "epoch:26 step:24812 [D loss: 0.512490, acc.: 72.66%] [G loss: 0.830097]\n",
      "epoch:26 step:24813 [D loss: 0.413894, acc.: 84.38%] [G loss: 0.947146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24814 [D loss: 0.496944, acc.: 73.44%] [G loss: 1.109902]\n",
      "epoch:26 step:24815 [D loss: 0.509217, acc.: 75.00%] [G loss: 0.788760]\n",
      "epoch:26 step:24816 [D loss: 0.560203, acc.: 73.44%] [G loss: 0.687155]\n",
      "epoch:26 step:24817 [D loss: 0.576586, acc.: 67.97%] [G loss: 0.666382]\n",
      "epoch:26 step:24818 [D loss: 0.607262, acc.: 67.97%] [G loss: 0.612259]\n",
      "epoch:26 step:24819 [D loss: 0.423712, acc.: 79.69%] [G loss: 0.771967]\n",
      "epoch:26 step:24820 [D loss: 0.647378, acc.: 63.28%] [G loss: 0.638840]\n",
      "epoch:26 step:24821 [D loss: 0.566491, acc.: 64.84%] [G loss: 0.656497]\n",
      "epoch:26 step:24822 [D loss: 0.537178, acc.: 67.19%] [G loss: 0.630426]\n",
      "epoch:26 step:24823 [D loss: 0.505029, acc.: 75.78%] [G loss: 0.802115]\n",
      "epoch:26 step:24824 [D loss: 0.566074, acc.: 68.75%] [G loss: 0.755341]\n",
      "epoch:26 step:24825 [D loss: 0.571253, acc.: 65.62%] [G loss: 0.528101]\n",
      "epoch:26 step:24826 [D loss: 0.526482, acc.: 75.78%] [G loss: 0.651782]\n",
      "epoch:26 step:24827 [D loss: 0.554403, acc.: 69.53%] [G loss: 0.752878]\n",
      "epoch:26 step:24828 [D loss: 0.609377, acc.: 64.84%] [G loss: 0.769873]\n",
      "epoch:26 step:24829 [D loss: 0.512656, acc.: 72.66%] [G loss: 0.667236]\n",
      "epoch:26 step:24830 [D loss: 0.507867, acc.: 76.56%] [G loss: 0.700772]\n",
      "epoch:26 step:24831 [D loss: 0.511079, acc.: 71.09%] [G loss: 0.779697]\n",
      "epoch:26 step:24832 [D loss: 0.527083, acc.: 73.44%] [G loss: 0.736944]\n",
      "epoch:26 step:24833 [D loss: 0.497485, acc.: 74.22%] [G loss: 0.721459]\n",
      "epoch:26 step:24834 [D loss: 0.402653, acc.: 82.03%] [G loss: 0.919711]\n",
      "epoch:26 step:24835 [D loss: 0.626854, acc.: 63.28%] [G loss: 0.729165]\n",
      "epoch:26 step:24836 [D loss: 0.524433, acc.: 70.31%] [G loss: 0.799231]\n",
      "epoch:26 step:24837 [D loss: 0.487636, acc.: 75.78%] [G loss: 0.850634]\n",
      "epoch:26 step:24838 [D loss: 0.546122, acc.: 73.44%] [G loss: 0.905846]\n",
      "epoch:26 step:24839 [D loss: 0.649389, acc.: 63.28%] [G loss: 0.737846]\n",
      "epoch:26 step:24840 [D loss: 0.526336, acc.: 72.66%] [G loss: 0.462857]\n",
      "epoch:26 step:24841 [D loss: 0.521543, acc.: 72.66%] [G loss: 0.609438]\n",
      "epoch:26 step:24842 [D loss: 0.571739, acc.: 70.31%] [G loss: 0.750332]\n",
      "epoch:26 step:24843 [D loss: 0.489873, acc.: 78.91%] [G loss: 0.676454]\n",
      "epoch:26 step:24844 [D loss: 0.669602, acc.: 61.72%] [G loss: 0.687657]\n",
      "epoch:26 step:24845 [D loss: 0.536939, acc.: 73.44%] [G loss: 0.736925]\n",
      "epoch:26 step:24846 [D loss: 0.495444, acc.: 75.00%] [G loss: 0.761009]\n",
      "epoch:26 step:24847 [D loss: 0.537498, acc.: 74.22%] [G loss: 0.617053]\n",
      "epoch:26 step:24848 [D loss: 0.586294, acc.: 69.53%] [G loss: 0.596034]\n",
      "epoch:26 step:24849 [D loss: 0.557060, acc.: 69.53%] [G loss: 0.699697]\n",
      "epoch:26 step:24850 [D loss: 0.493097, acc.: 75.00%] [G loss: 0.764198]\n",
      "epoch:26 step:24851 [D loss: 0.557009, acc.: 70.31%] [G loss: 0.752656]\n",
      "epoch:26 step:24852 [D loss: 0.507933, acc.: 75.78%] [G loss: 0.697401]\n",
      "epoch:26 step:24853 [D loss: 0.527491, acc.: 75.78%] [G loss: 0.673360]\n",
      "epoch:26 step:24854 [D loss: 0.563869, acc.: 68.75%] [G loss: 0.663881]\n",
      "epoch:26 step:24855 [D loss: 0.497031, acc.: 77.34%] [G loss: 0.574832]\n",
      "epoch:26 step:24856 [D loss: 0.592744, acc.: 67.97%] [G loss: 0.750488]\n",
      "epoch:26 step:24857 [D loss: 0.482979, acc.: 75.00%] [G loss: 0.585477]\n",
      "epoch:26 step:24858 [D loss: 0.539212, acc.: 72.66%] [G loss: 0.800633]\n",
      "epoch:26 step:24859 [D loss: 0.549087, acc.: 71.09%] [G loss: 0.712525]\n",
      "epoch:26 step:24860 [D loss: 0.543946, acc.: 75.00%] [G loss: 0.801208]\n",
      "epoch:26 step:24861 [D loss: 0.523742, acc.: 71.88%] [G loss: 0.664813]\n",
      "epoch:26 step:24862 [D loss: 0.548088, acc.: 74.22%] [G loss: 0.635666]\n",
      "epoch:26 step:24863 [D loss: 0.683940, acc.: 60.94%] [G loss: 0.544324]\n",
      "epoch:26 step:24864 [D loss: 0.613289, acc.: 63.28%] [G loss: 0.487500]\n",
      "epoch:26 step:24865 [D loss: 0.525192, acc.: 72.66%] [G loss: 0.708560]\n",
      "epoch:26 step:24866 [D loss: 0.471259, acc.: 79.69%] [G loss: 0.713073]\n",
      "epoch:26 step:24867 [D loss: 0.476017, acc.: 76.56%] [G loss: 0.881888]\n",
      "epoch:26 step:24868 [D loss: 0.513206, acc.: 72.66%] [G loss: 0.814010]\n",
      "epoch:26 step:24869 [D loss: 0.541252, acc.: 72.66%] [G loss: 0.868432]\n",
      "epoch:26 step:24870 [D loss: 0.464725, acc.: 80.47%] [G loss: 0.994133]\n",
      "epoch:26 step:24871 [D loss: 0.453369, acc.: 75.78%] [G loss: 0.796771]\n",
      "epoch:26 step:24872 [D loss: 0.590228, acc.: 64.84%] [G loss: 0.655072]\n",
      "epoch:26 step:24873 [D loss: 0.667971, acc.: 62.50%] [G loss: 0.449863]\n",
      "epoch:26 step:24874 [D loss: 0.623189, acc.: 65.62%] [G loss: 0.507653]\n",
      "epoch:26 step:24875 [D loss: 0.543880, acc.: 69.53%] [G loss: 0.582056]\n",
      "epoch:26 step:24876 [D loss: 0.497821, acc.: 75.78%] [G loss: 0.668246]\n",
      "epoch:26 step:24877 [D loss: 0.566917, acc.: 68.75%] [G loss: 0.655769]\n",
      "epoch:26 step:24878 [D loss: 0.522646, acc.: 71.88%] [G loss: 0.911604]\n",
      "epoch:26 step:24879 [D loss: 0.467403, acc.: 78.91%] [G loss: 0.969814]\n",
      "epoch:26 step:24880 [D loss: 0.522484, acc.: 75.78%] [G loss: 0.751393]\n",
      "epoch:26 step:24881 [D loss: 0.524765, acc.: 72.66%] [G loss: 0.832549]\n",
      "epoch:26 step:24882 [D loss: 0.480906, acc.: 75.00%] [G loss: 0.785474]\n",
      "epoch:26 step:24883 [D loss: 0.445731, acc.: 80.47%] [G loss: 0.832615]\n",
      "epoch:26 step:24884 [D loss: 0.491360, acc.: 75.00%] [G loss: 0.867091]\n",
      "epoch:26 step:24885 [D loss: 0.461970, acc.: 78.91%] [G loss: 0.758537]\n",
      "epoch:26 step:24886 [D loss: 0.530732, acc.: 69.53%] [G loss: 0.706570]\n",
      "epoch:26 step:24887 [D loss: 0.691273, acc.: 62.50%] [G loss: 0.633474]\n",
      "epoch:26 step:24888 [D loss: 0.557492, acc.: 67.97%] [G loss: 0.717381]\n",
      "epoch:26 step:24889 [D loss: 0.569821, acc.: 67.97%] [G loss: 0.883468]\n",
      "epoch:26 step:24890 [D loss: 0.695955, acc.: 64.06%] [G loss: 0.704347]\n",
      "epoch:26 step:24891 [D loss: 0.538123, acc.: 67.97%] [G loss: 0.847974]\n",
      "epoch:26 step:24892 [D loss: 0.501869, acc.: 71.09%] [G loss: 0.824940]\n",
      "epoch:26 step:24893 [D loss: 0.608096, acc.: 62.50%] [G loss: 0.673745]\n",
      "epoch:26 step:24894 [D loss: 0.576453, acc.: 66.41%] [G loss: 0.737752]\n",
      "epoch:26 step:24895 [D loss: 0.495239, acc.: 74.22%] [G loss: 0.695777]\n",
      "epoch:26 step:24896 [D loss: 0.474540, acc.: 73.44%] [G loss: 0.798663]\n",
      "epoch:26 step:24897 [D loss: 0.599356, acc.: 70.31%] [G loss: 0.695776]\n",
      "epoch:26 step:24898 [D loss: 0.554305, acc.: 67.19%] [G loss: 0.738852]\n",
      "epoch:26 step:24899 [D loss: 0.598184, acc.: 65.62%] [G loss: 0.602702]\n",
      "epoch:26 step:24900 [D loss: 0.549090, acc.: 74.22%] [G loss: 0.549517]\n",
      "epoch:26 step:24901 [D loss: 0.574684, acc.: 67.19%] [G loss: 0.459503]\n",
      "epoch:26 step:24902 [D loss: 0.543776, acc.: 64.84%] [G loss: 0.639579]\n",
      "epoch:26 step:24903 [D loss: 0.527788, acc.: 67.97%] [G loss: 0.814870]\n",
      "epoch:26 step:24904 [D loss: 0.542401, acc.: 71.09%] [G loss: 0.721914]\n",
      "epoch:26 step:24905 [D loss: 0.527657, acc.: 76.56%] [G loss: 0.645876]\n",
      "epoch:26 step:24906 [D loss: 0.547714, acc.: 75.78%] [G loss: 0.633184]\n",
      "epoch:26 step:24907 [D loss: 0.537146, acc.: 70.31%] [G loss: 0.759862]\n",
      "epoch:26 step:24908 [D loss: 0.464485, acc.: 77.34%] [G loss: 0.848411]\n",
      "epoch:26 step:24909 [D loss: 0.562479, acc.: 65.62%] [G loss: 0.756595]\n",
      "epoch:26 step:24910 [D loss: 0.496066, acc.: 72.66%] [G loss: 0.660017]\n",
      "epoch:26 step:24911 [D loss: 0.511829, acc.: 75.00%] [G loss: 0.792887]\n",
      "epoch:26 step:24912 [D loss: 0.544073, acc.: 69.53%] [G loss: 0.646619]\n",
      "epoch:26 step:24913 [D loss: 0.547364, acc.: 71.88%] [G loss: 0.741262]\n",
      "epoch:26 step:24914 [D loss: 0.475554, acc.: 75.00%] [G loss: 0.579713]\n",
      "epoch:26 step:24915 [D loss: 0.560517, acc.: 71.09%] [G loss: 0.812917]\n",
      "epoch:26 step:24916 [D loss: 0.435483, acc.: 81.25%] [G loss: 0.655736]\n",
      "epoch:26 step:24917 [D loss: 0.464563, acc.: 75.00%] [G loss: 0.740010]\n",
      "epoch:26 step:24918 [D loss: 0.564120, acc.: 71.09%] [G loss: 0.860152]\n",
      "epoch:26 step:24919 [D loss: 0.467287, acc.: 76.56%] [G loss: 0.917926]\n",
      "epoch:26 step:24920 [D loss: 0.489102, acc.: 75.00%] [G loss: 0.749475]\n",
      "epoch:26 step:24921 [D loss: 0.583135, acc.: 69.53%] [G loss: 0.631412]\n",
      "epoch:26 step:24922 [D loss: 0.569555, acc.: 67.97%] [G loss: 0.744123]\n",
      "epoch:26 step:24923 [D loss: 0.566621, acc.: 67.19%] [G loss: 0.616082]\n",
      "epoch:26 step:24924 [D loss: 0.558884, acc.: 64.84%] [G loss: 0.570812]\n",
      "epoch:26 step:24925 [D loss: 0.586979, acc.: 67.19%] [G loss: 0.572327]\n",
      "epoch:26 step:24926 [D loss: 0.486070, acc.: 77.34%] [G loss: 0.714513]\n",
      "epoch:26 step:24927 [D loss: 0.617583, acc.: 64.06%] [G loss: 0.722536]\n",
      "epoch:26 step:24928 [D loss: 0.677222, acc.: 57.03%] [G loss: 0.575464]\n",
      "epoch:26 step:24929 [D loss: 0.507656, acc.: 75.00%] [G loss: 0.743381]\n",
      "epoch:26 step:24930 [D loss: 0.498150, acc.: 75.00%] [G loss: 0.873192]\n",
      "epoch:26 step:24931 [D loss: 0.613460, acc.: 66.41%] [G loss: 0.721308]\n",
      "epoch:26 step:24932 [D loss: 0.549951, acc.: 71.09%] [G loss: 0.830012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24933 [D loss: 0.536497, acc.: 72.66%] [G loss: 0.795529]\n",
      "epoch:26 step:24934 [D loss: 0.547261, acc.: 69.53%] [G loss: 0.724987]\n",
      "epoch:26 step:24935 [D loss: 0.564551, acc.: 67.97%] [G loss: 0.689573]\n",
      "epoch:26 step:24936 [D loss: 0.498658, acc.: 71.09%] [G loss: 0.844302]\n",
      "epoch:26 step:24937 [D loss: 0.457238, acc.: 82.81%] [G loss: 1.040202]\n",
      "epoch:26 step:24938 [D loss: 0.691733, acc.: 59.38%] [G loss: 0.656127]\n",
      "epoch:26 step:24939 [D loss: 0.497750, acc.: 77.34%] [G loss: 0.771481]\n",
      "epoch:26 step:24940 [D loss: 0.563003, acc.: 68.75%] [G loss: 0.584543]\n",
      "epoch:26 step:24941 [D loss: 0.492718, acc.: 75.00%] [G loss: 0.655373]\n",
      "epoch:26 step:24942 [D loss: 0.568420, acc.: 68.75%] [G loss: 0.645836]\n",
      "epoch:26 step:24943 [D loss: 0.510989, acc.: 74.22%] [G loss: 0.772905]\n",
      "epoch:26 step:24944 [D loss: 0.460386, acc.: 82.81%] [G loss: 0.869245]\n",
      "epoch:26 step:24945 [D loss: 0.500490, acc.: 72.66%] [G loss: 0.824720]\n",
      "epoch:26 step:24946 [D loss: 0.620373, acc.: 61.72%] [G loss: 0.684229]\n",
      "epoch:26 step:24947 [D loss: 0.608613, acc.: 67.19%] [G loss: 0.843273]\n",
      "epoch:26 step:24948 [D loss: 0.587404, acc.: 65.62%] [G loss: 0.682283]\n",
      "epoch:26 step:24949 [D loss: 0.560801, acc.: 67.19%] [G loss: 0.647168]\n",
      "epoch:26 step:24950 [D loss: 0.546796, acc.: 65.62%] [G loss: 0.697181]\n",
      "epoch:26 step:24951 [D loss: 0.495834, acc.: 75.78%] [G loss: 0.844821]\n",
      "epoch:26 step:24952 [D loss: 0.556725, acc.: 69.53%] [G loss: 0.872008]\n",
      "epoch:26 step:24953 [D loss: 0.635819, acc.: 65.62%] [G loss: 0.644531]\n",
      "epoch:26 step:24954 [D loss: 0.515128, acc.: 71.88%] [G loss: 0.693778]\n",
      "epoch:26 step:24955 [D loss: 0.504233, acc.: 74.22%] [G loss: 0.753307]\n",
      "epoch:26 step:24956 [D loss: 0.540358, acc.: 69.53%] [G loss: 0.671781]\n",
      "epoch:26 step:24957 [D loss: 0.549557, acc.: 65.62%] [G loss: 0.750291]\n",
      "epoch:26 step:24958 [D loss: 0.540894, acc.: 70.31%] [G loss: 0.634477]\n",
      "epoch:26 step:24959 [D loss: 0.530327, acc.: 73.44%] [G loss: 0.705662]\n",
      "epoch:26 step:24960 [D loss: 0.455589, acc.: 76.56%] [G loss: 0.788889]\n",
      "epoch:26 step:24961 [D loss: 0.528915, acc.: 71.09%] [G loss: 0.812130]\n",
      "epoch:26 step:24962 [D loss: 0.593304, acc.: 67.97%] [G loss: 0.709410]\n",
      "epoch:26 step:24963 [D loss: 0.535785, acc.: 67.19%] [G loss: 0.586975]\n",
      "epoch:26 step:24964 [D loss: 0.514441, acc.: 71.88%] [G loss: 0.768322]\n",
      "epoch:26 step:24965 [D loss: 0.443131, acc.: 78.91%] [G loss: 0.789570]\n",
      "epoch:26 step:24966 [D loss: 0.507781, acc.: 75.00%] [G loss: 0.687150]\n",
      "epoch:26 step:24967 [D loss: 0.448555, acc.: 77.34%] [G loss: 0.967053]\n",
      "epoch:26 step:24968 [D loss: 0.616270, acc.: 65.62%] [G loss: 0.705384]\n",
      "epoch:26 step:24969 [D loss: 0.545607, acc.: 68.75%] [G loss: 0.724404]\n",
      "epoch:26 step:24970 [D loss: 0.533199, acc.: 72.66%] [G loss: 0.619389]\n",
      "epoch:26 step:24971 [D loss: 0.562878, acc.: 64.84%] [G loss: 0.499353]\n",
      "epoch:26 step:24972 [D loss: 0.621623, acc.: 65.62%] [G loss: 0.575750]\n",
      "epoch:26 step:24973 [D loss: 0.511947, acc.: 75.00%] [G loss: 0.549123]\n",
      "epoch:26 step:24974 [D loss: 0.582511, acc.: 64.84%] [G loss: 0.547256]\n",
      "epoch:26 step:24975 [D loss: 0.520876, acc.: 71.09%] [G loss: 0.756945]\n",
      "epoch:26 step:24976 [D loss: 0.582243, acc.: 67.19%] [G loss: 0.612534]\n",
      "epoch:26 step:24977 [D loss: 0.521399, acc.: 70.31%] [G loss: 0.797477]\n",
      "epoch:26 step:24978 [D loss: 0.550977, acc.: 72.66%] [G loss: 0.812438]\n",
      "epoch:26 step:24979 [D loss: 0.531660, acc.: 68.75%] [G loss: 0.889299]\n",
      "epoch:26 step:24980 [D loss: 0.579813, acc.: 64.84%] [G loss: 0.700404]\n",
      "epoch:26 step:24981 [D loss: 0.501613, acc.: 74.22%] [G loss: 0.691299]\n",
      "epoch:26 step:24982 [D loss: 0.520451, acc.: 70.31%] [G loss: 0.681575]\n",
      "epoch:26 step:24983 [D loss: 0.580297, acc.: 67.97%] [G loss: 0.655674]\n",
      "epoch:26 step:24984 [D loss: 0.571869, acc.: 67.97%] [G loss: 0.570953]\n",
      "epoch:26 step:24985 [D loss: 0.510055, acc.: 73.44%] [G loss: 0.653610]\n",
      "epoch:26 step:24986 [D loss: 0.501832, acc.: 75.78%] [G loss: 0.785770]\n",
      "epoch:26 step:24987 [D loss: 0.577973, acc.: 62.50%] [G loss: 0.831001]\n",
      "epoch:26 step:24988 [D loss: 0.556535, acc.: 69.53%] [G loss: 0.544109]\n",
      "epoch:26 step:24989 [D loss: 0.531191, acc.: 68.75%] [G loss: 0.720935]\n",
      "epoch:26 step:24990 [D loss: 0.570939, acc.: 68.75%] [G loss: 0.776611]\n",
      "epoch:26 step:24991 [D loss: 0.562616, acc.: 67.19%] [G loss: 0.688265]\n",
      "epoch:26 step:24992 [D loss: 0.590552, acc.: 70.31%] [G loss: 0.626585]\n",
      "epoch:26 step:24993 [D loss: 0.500445, acc.: 75.78%] [G loss: 0.694694]\n",
      "epoch:26 step:24994 [D loss: 0.489545, acc.: 75.00%] [G loss: 0.842840]\n",
      "epoch:26 step:24995 [D loss: 0.501609, acc.: 77.34%] [G loss: 0.797347]\n",
      "epoch:26 step:24996 [D loss: 0.433712, acc.: 78.91%] [G loss: 0.925449]\n",
      "epoch:26 step:24997 [D loss: 0.476294, acc.: 76.56%] [G loss: 0.790532]\n",
      "epoch:26 step:24998 [D loss: 0.559905, acc.: 69.53%] [G loss: 0.684931]\n",
      "epoch:26 step:24999 [D loss: 0.570077, acc.: 62.50%] [G loss: 0.560108]\n",
      "epoch:26 step:25000 [D loss: 0.530214, acc.: 67.97%] [G loss: 0.631973]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.650919\n",
      "FID: 48.146942\n",
      "0 = 13.048068153572055\n",
      "1 = 0.10118148934771169\n",
      "2 = 0.8532999753952026\n",
      "3 = 0.8141999840736389\n",
      "4 = 0.8924000263214111\n",
      "5 = 0.8832718729972839\n",
      "6 = 0.8141999840736389\n",
      "7 = 8.315755690002417\n",
      "8 = 0.14818953834396398\n",
      "9 = 0.7044000029563904\n",
      "10 = 0.6977999806404114\n",
      "11 = 0.7110000252723694\n",
      "12 = 0.7071341872215271\n",
      "13 = 0.6977999806404114\n",
      "14 = 6.650940895080566\n",
      "15 = 6.892829418182373\n",
      "16 = 0.39583688974380493\n",
      "17 = 6.650919437408447\n",
      "18 = 48.146942138671875\n",
      "epoch:26 step:25001 [D loss: 0.522488, acc.: 74.22%] [G loss: 0.727859]\n",
      "epoch:26 step:25002 [D loss: 0.633501, acc.: 64.06%] [G loss: 0.735380]\n",
      "epoch:26 step:25003 [D loss: 0.528703, acc.: 72.66%] [G loss: 0.744746]\n",
      "epoch:26 step:25004 [D loss: 0.474370, acc.: 76.56%] [G loss: 0.779190]\n",
      "epoch:26 step:25005 [D loss: 0.520093, acc.: 72.66%] [G loss: 0.641093]\n",
      "epoch:26 step:25006 [D loss: 0.499146, acc.: 73.44%] [G loss: 0.713404]\n",
      "epoch:26 step:25007 [D loss: 0.492915, acc.: 71.88%] [G loss: 0.675428]\n",
      "epoch:26 step:25008 [D loss: 0.523632, acc.: 71.88%] [G loss: 0.759335]\n",
      "epoch:26 step:25009 [D loss: 0.401409, acc.: 81.25%] [G loss: 0.786092]\n",
      "epoch:26 step:25010 [D loss: 0.398132, acc.: 80.47%] [G loss: 0.958015]\n",
      "epoch:26 step:25011 [D loss: 0.493414, acc.: 75.78%] [G loss: 0.929989]\n",
      "epoch:26 step:25012 [D loss: 0.452984, acc.: 76.56%] [G loss: 0.890769]\n",
      "epoch:26 step:25013 [D loss: 0.542206, acc.: 67.97%] [G loss: 0.880918]\n",
      "epoch:26 step:25014 [D loss: 0.608067, acc.: 66.41%] [G loss: 0.646624]\n",
      "epoch:26 step:25015 [D loss: 0.557140, acc.: 72.66%] [G loss: 0.664412]\n",
      "epoch:26 step:25016 [D loss: 0.491780, acc.: 73.44%] [G loss: 0.777240]\n",
      "epoch:26 step:25017 [D loss: 0.576801, acc.: 70.31%] [G loss: 0.730062]\n",
      "epoch:26 step:25018 [D loss: 0.533362, acc.: 69.53%] [G loss: 0.676637]\n",
      "epoch:26 step:25019 [D loss: 0.515712, acc.: 71.09%] [G loss: 0.768566]\n",
      "epoch:26 step:25020 [D loss: 0.575983, acc.: 64.84%] [G loss: 0.678515]\n",
      "epoch:26 step:25021 [D loss: 0.521562, acc.: 71.88%] [G loss: 1.032643]\n",
      "epoch:26 step:25022 [D loss: 0.504206, acc.: 73.44%] [G loss: 0.826101]\n",
      "epoch:26 step:25023 [D loss: 0.470746, acc.: 73.44%] [G loss: 0.789477]\n",
      "epoch:26 step:25024 [D loss: 0.497746, acc.: 73.44%] [G loss: 0.744747]\n",
      "epoch:26 step:25025 [D loss: 0.628736, acc.: 60.16%] [G loss: 0.657724]\n",
      "epoch:26 step:25026 [D loss: 0.526452, acc.: 69.53%] [G loss: 0.707395]\n",
      "epoch:26 step:25027 [D loss: 0.641110, acc.: 61.72%] [G loss: 0.621923]\n",
      "epoch:26 step:25028 [D loss: 0.570599, acc.: 70.31%] [G loss: 0.750502]\n",
      "epoch:26 step:25029 [D loss: 0.535793, acc.: 68.75%] [G loss: 0.596163]\n",
      "epoch:26 step:25030 [D loss: 0.541360, acc.: 73.44%] [G loss: 0.661362]\n",
      "epoch:26 step:25031 [D loss: 0.539060, acc.: 74.22%] [G loss: 0.579068]\n",
      "epoch:26 step:25032 [D loss: 0.566501, acc.: 67.97%] [G loss: 0.614966]\n",
      "epoch:26 step:25033 [D loss: 0.534766, acc.: 68.75%] [G loss: 0.698257]\n",
      "epoch:26 step:25034 [D loss: 0.535549, acc.: 72.66%] [G loss: 0.602796]\n",
      "epoch:26 step:25035 [D loss: 0.618280, acc.: 60.94%] [G loss: 0.626971]\n",
      "epoch:26 step:25036 [D loss: 0.510679, acc.: 74.22%] [G loss: 0.685860]\n",
      "epoch:26 step:25037 [D loss: 0.575525, acc.: 65.62%] [G loss: 0.595926]\n",
      "epoch:26 step:25038 [D loss: 0.506354, acc.: 73.44%] [G loss: 0.699117]\n",
      "epoch:26 step:25039 [D loss: 0.477847, acc.: 77.34%] [G loss: 0.797068]\n",
      "epoch:26 step:25040 [D loss: 0.585408, acc.: 72.66%] [G loss: 0.717254]\n",
      "epoch:26 step:25041 [D loss: 0.494709, acc.: 75.78%] [G loss: 0.805278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25042 [D loss: 0.584449, acc.: 69.53%] [G loss: 0.558257]\n",
      "epoch:26 step:25043 [D loss: 0.470160, acc.: 77.34%] [G loss: 0.646387]\n",
      "epoch:26 step:25044 [D loss: 0.557874, acc.: 72.66%] [G loss: 0.581488]\n",
      "epoch:26 step:25045 [D loss: 0.559152, acc.: 66.41%] [G loss: 0.662328]\n",
      "epoch:26 step:25046 [D loss: 0.563714, acc.: 70.31%] [G loss: 0.659144]\n",
      "epoch:26 step:25047 [D loss: 0.551861, acc.: 67.19%] [G loss: 0.528123]\n",
      "epoch:26 step:25048 [D loss: 0.585278, acc.: 66.41%] [G loss: 0.620292]\n",
      "epoch:26 step:25049 [D loss: 0.545699, acc.: 72.66%] [G loss: 0.592389]\n",
      "epoch:26 step:25050 [D loss: 0.514005, acc.: 73.44%] [G loss: 0.555483]\n",
      "epoch:26 step:25051 [D loss: 0.561552, acc.: 67.19%] [G loss: 0.662317]\n",
      "epoch:26 step:25052 [D loss: 0.465545, acc.: 77.34%] [G loss: 0.672183]\n",
      "epoch:26 step:25053 [D loss: 0.484464, acc.: 75.00%] [G loss: 0.861566]\n",
      "epoch:26 step:25054 [D loss: 0.562150, acc.: 73.44%] [G loss: 0.572555]\n",
      "epoch:26 step:25055 [D loss: 0.467650, acc.: 78.12%] [G loss: 0.801268]\n",
      "epoch:26 step:25056 [D loss: 0.497496, acc.: 76.56%] [G loss: 0.947361]\n",
      "epoch:26 step:25057 [D loss: 0.513229, acc.: 75.78%] [G loss: 0.770059]\n",
      "epoch:26 step:25058 [D loss: 0.603575, acc.: 64.06%] [G loss: 0.564564]\n",
      "epoch:26 step:25059 [D loss: 0.577380, acc.: 62.50%] [G loss: 0.545324]\n",
      "epoch:26 step:25060 [D loss: 0.620060, acc.: 60.94%] [G loss: 0.570737]\n",
      "epoch:26 step:25061 [D loss: 0.551420, acc.: 71.88%] [G loss: 0.624203]\n",
      "epoch:26 step:25062 [D loss: 0.527396, acc.: 75.00%] [G loss: 0.869701]\n",
      "epoch:26 step:25063 [D loss: 0.526102, acc.: 69.53%] [G loss: 0.601038]\n",
      "epoch:26 step:25064 [D loss: 0.597288, acc.: 64.06%] [G loss: 0.716599]\n",
      "epoch:26 step:25065 [D loss: 0.650726, acc.: 63.28%] [G loss: 0.510268]\n",
      "epoch:26 step:25066 [D loss: 0.627765, acc.: 60.94%] [G loss: 0.521408]\n",
      "epoch:26 step:25067 [D loss: 0.497120, acc.: 74.22%] [G loss: 0.770985]\n",
      "epoch:26 step:25068 [D loss: 0.515783, acc.: 75.78%] [G loss: 0.720237]\n",
      "epoch:26 step:25069 [D loss: 0.494995, acc.: 74.22%] [G loss: 0.756883]\n",
      "epoch:26 step:25070 [D loss: 0.516730, acc.: 75.78%] [G loss: 0.951052]\n",
      "epoch:26 step:25071 [D loss: 0.513083, acc.: 71.09%] [G loss: 1.046610]\n",
      "epoch:26 step:25072 [D loss: 0.589950, acc.: 63.28%] [G loss: 0.544281]\n",
      "epoch:26 step:25073 [D loss: 0.532527, acc.: 66.41%] [G loss: 0.606285]\n",
      "epoch:26 step:25074 [D loss: 0.547076, acc.: 68.75%] [G loss: 0.794773]\n",
      "epoch:26 step:25075 [D loss: 0.609069, acc.: 62.50%] [G loss: 0.746624]\n",
      "epoch:26 step:25076 [D loss: 0.527378, acc.: 69.53%] [G loss: 0.697202]\n",
      "epoch:26 step:25077 [D loss: 0.520276, acc.: 73.44%] [G loss: 0.706398]\n",
      "epoch:26 step:25078 [D loss: 0.668534, acc.: 53.12%] [G loss: 0.550190]\n",
      "epoch:26 step:25079 [D loss: 0.573471, acc.: 67.19%] [G loss: 0.627550]\n",
      "epoch:26 step:25080 [D loss: 0.565918, acc.: 67.19%] [G loss: 0.670218]\n",
      "epoch:26 step:25081 [D loss: 0.500033, acc.: 71.88%] [G loss: 0.972438]\n",
      "epoch:26 step:25082 [D loss: 0.569351, acc.: 68.75%] [G loss: 0.817238]\n",
      "epoch:26 step:25083 [D loss: 0.605527, acc.: 67.19%] [G loss: 0.686026]\n",
      "epoch:26 step:25084 [D loss: 0.591538, acc.: 64.84%] [G loss: 0.795015]\n",
      "epoch:26 step:25085 [D loss: 0.567122, acc.: 63.28%] [G loss: 0.573757]\n",
      "epoch:26 step:25086 [D loss: 0.529436, acc.: 70.31%] [G loss: 0.644670]\n",
      "epoch:26 step:25087 [D loss: 0.477553, acc.: 75.00%] [G loss: 0.842015]\n",
      "epoch:26 step:25088 [D loss: 0.490192, acc.: 75.00%] [G loss: 0.821246]\n",
      "epoch:26 step:25089 [D loss: 0.511691, acc.: 71.09%] [G loss: 0.712924]\n",
      "epoch:26 step:25090 [D loss: 0.529310, acc.: 70.31%] [G loss: 0.571593]\n",
      "epoch:26 step:25091 [D loss: 0.542158, acc.: 69.53%] [G loss: 0.605253]\n",
      "epoch:26 step:25092 [D loss: 0.468199, acc.: 71.88%] [G loss: 0.786979]\n",
      "epoch:26 step:25093 [D loss: 0.630082, acc.: 60.94%] [G loss: 0.610412]\n",
      "epoch:26 step:25094 [D loss: 0.583279, acc.: 64.84%] [G loss: 0.637806]\n",
      "epoch:26 step:25095 [D loss: 0.479363, acc.: 78.12%] [G loss: 0.564156]\n",
      "epoch:26 step:25096 [D loss: 0.556516, acc.: 63.28%] [G loss: 0.659537]\n",
      "epoch:26 step:25097 [D loss: 0.543003, acc.: 70.31%] [G loss: 0.696330]\n",
      "epoch:26 step:25098 [D loss: 0.509607, acc.: 72.66%] [G loss: 0.688659]\n",
      "epoch:26 step:25099 [D loss: 0.522890, acc.: 71.88%] [G loss: 0.652655]\n",
      "epoch:26 step:25100 [D loss: 0.546782, acc.: 74.22%] [G loss: 0.750282]\n",
      "epoch:26 step:25101 [D loss: 0.565897, acc.: 69.53%] [G loss: 0.741235]\n",
      "epoch:26 step:25102 [D loss: 0.642031, acc.: 60.94%] [G loss: 0.521558]\n",
      "epoch:26 step:25103 [D loss: 0.557104, acc.: 71.88%] [G loss: 0.516708]\n",
      "epoch:26 step:25104 [D loss: 0.549706, acc.: 70.31%] [G loss: 0.696749]\n",
      "epoch:26 step:25105 [D loss: 0.517499, acc.: 71.09%] [G loss: 0.744274]\n",
      "epoch:26 step:25106 [D loss: 0.455056, acc.: 79.69%] [G loss: 1.018751]\n",
      "epoch:26 step:25107 [D loss: 0.583493, acc.: 68.75%] [G loss: 0.757116]\n",
      "epoch:26 step:25108 [D loss: 0.467410, acc.: 75.00%] [G loss: 0.813728]\n",
      "epoch:26 step:25109 [D loss: 0.504804, acc.: 73.44%] [G loss: 1.005793]\n",
      "epoch:26 step:25110 [D loss: 0.550042, acc.: 70.31%] [G loss: 0.823360]\n",
      "epoch:26 step:25111 [D loss: 0.594284, acc.: 66.41%] [G loss: 0.743280]\n",
      "epoch:26 step:25112 [D loss: 0.475620, acc.: 76.56%] [G loss: 0.727153]\n",
      "epoch:26 step:25113 [D loss: 0.473092, acc.: 77.34%] [G loss: 0.782314]\n",
      "epoch:26 step:25114 [D loss: 0.623588, acc.: 63.28%] [G loss: 0.588536]\n",
      "epoch:26 step:25115 [D loss: 0.517903, acc.: 74.22%] [G loss: 0.602079]\n",
      "epoch:26 step:25116 [D loss: 0.542904, acc.: 71.09%] [G loss: 0.762946]\n",
      "epoch:26 step:25117 [D loss: 0.566610, acc.: 69.53%] [G loss: 0.636582]\n",
      "epoch:26 step:25118 [D loss: 0.590559, acc.: 64.84%] [G loss: 0.688774]\n",
      "epoch:26 step:25119 [D loss: 0.505684, acc.: 74.22%] [G loss: 0.780794]\n",
      "epoch:26 step:25120 [D loss: 0.533025, acc.: 75.00%] [G loss: 0.618629]\n",
      "epoch:26 step:25121 [D loss: 0.532864, acc.: 67.97%] [G loss: 0.792507]\n",
      "epoch:26 step:25122 [D loss: 0.530877, acc.: 72.66%] [G loss: 0.547627]\n",
      "epoch:26 step:25123 [D loss: 0.555482, acc.: 67.97%] [G loss: 0.559913]\n",
      "epoch:26 step:25124 [D loss: 0.579418, acc.: 68.75%] [G loss: 0.573313]\n",
      "epoch:26 step:25125 [D loss: 0.569010, acc.: 67.97%] [G loss: 0.567362]\n",
      "epoch:26 step:25126 [D loss: 0.543674, acc.: 71.88%] [G loss: 0.584645]\n",
      "epoch:26 step:25127 [D loss: 0.641056, acc.: 67.97%] [G loss: 0.680489]\n",
      "epoch:26 step:25128 [D loss: 0.641809, acc.: 60.94%] [G loss: 0.522264]\n",
      "epoch:26 step:25129 [D loss: 0.495465, acc.: 76.56%] [G loss: 0.627529]\n",
      "epoch:26 step:25130 [D loss: 0.609007, acc.: 66.41%] [G loss: 0.808534]\n",
      "epoch:26 step:25131 [D loss: 0.482608, acc.: 76.56%] [G loss: 1.030644]\n",
      "epoch:26 step:25132 [D loss: 0.521893, acc.: 71.09%] [G loss: 1.010665]\n",
      "epoch:26 step:25133 [D loss: 0.566316, acc.: 70.31%] [G loss: 0.690058]\n",
      "epoch:26 step:25134 [D loss: 0.577127, acc.: 68.75%] [G loss: 0.737955]\n",
      "epoch:26 step:25135 [D loss: 0.588541, acc.: 64.84%] [G loss: 0.588099]\n",
      "epoch:26 step:25136 [D loss: 0.495500, acc.: 77.34%] [G loss: 0.564120]\n",
      "epoch:26 step:25137 [D loss: 0.483267, acc.: 72.66%] [G loss: 0.892372]\n",
      "epoch:26 step:25138 [D loss: 0.553388, acc.: 71.09%] [G loss: 0.641183]\n",
      "epoch:26 step:25139 [D loss: 0.516545, acc.: 74.22%] [G loss: 0.649244]\n",
      "epoch:26 step:25140 [D loss: 0.572619, acc.: 68.75%] [G loss: 0.653957]\n",
      "epoch:26 step:25141 [D loss: 0.611841, acc.: 65.62%] [G loss: 0.674185]\n",
      "epoch:26 step:25142 [D loss: 0.505005, acc.: 77.34%] [G loss: 0.675700]\n",
      "epoch:26 step:25143 [D loss: 0.539459, acc.: 71.88%] [G loss: 0.680984]\n",
      "epoch:26 step:25144 [D loss: 0.470630, acc.: 78.91%] [G loss: 0.731074]\n",
      "epoch:26 step:25145 [D loss: 0.487623, acc.: 77.34%] [G loss: 0.845628]\n",
      "epoch:26 step:25146 [D loss: 0.641356, acc.: 67.97%] [G loss: 0.700648]\n",
      "epoch:26 step:25147 [D loss: 0.577832, acc.: 67.97%] [G loss: 0.646989]\n",
      "epoch:26 step:25148 [D loss: 0.523977, acc.: 71.09%] [G loss: 0.804875]\n",
      "epoch:26 step:25149 [D loss: 0.599049, acc.: 65.62%] [G loss: 0.659442]\n",
      "epoch:26 step:25150 [D loss: 0.645097, acc.: 60.94%] [G loss: 0.643481]\n",
      "epoch:26 step:25151 [D loss: 0.499532, acc.: 77.34%] [G loss: 0.658273]\n",
      "epoch:26 step:25152 [D loss: 0.491450, acc.: 73.44%] [G loss: 0.604130]\n",
      "epoch:26 step:25153 [D loss: 0.612107, acc.: 68.75%] [G loss: 0.632509]\n",
      "epoch:26 step:25154 [D loss: 0.453508, acc.: 76.56%] [G loss: 0.646446]\n",
      "epoch:26 step:25155 [D loss: 0.606975, acc.: 64.06%] [G loss: 0.681701]\n",
      "epoch:26 step:25156 [D loss: 0.610706, acc.: 64.06%] [G loss: 0.633359]\n",
      "epoch:26 step:25157 [D loss: 0.488039, acc.: 74.22%] [G loss: 0.710850]\n",
      "epoch:26 step:25158 [D loss: 0.519834, acc.: 73.44%] [G loss: 0.785629]\n",
      "epoch:26 step:25159 [D loss: 0.520736, acc.: 67.97%] [G loss: 0.757236]\n",
      "epoch:26 step:25160 [D loss: 0.544153, acc.: 69.53%] [G loss: 0.491510]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25161 [D loss: 0.520057, acc.: 69.53%] [G loss: 0.637683]\n",
      "epoch:26 step:25162 [D loss: 0.536976, acc.: 67.97%] [G loss: 0.613719]\n",
      "epoch:26 step:25163 [D loss: 0.557647, acc.: 66.41%] [G loss: 0.749081]\n",
      "epoch:26 step:25164 [D loss: 0.501273, acc.: 74.22%] [G loss: 1.006921]\n",
      "epoch:26 step:25165 [D loss: 0.512869, acc.: 76.56%] [G loss: 0.951901]\n",
      "epoch:26 step:25166 [D loss: 0.602482, acc.: 66.41%] [G loss: 0.612075]\n",
      "epoch:26 step:25167 [D loss: 0.550580, acc.: 71.88%] [G loss: 0.650830]\n",
      "epoch:26 step:25168 [D loss: 0.581033, acc.: 67.97%] [G loss: 0.616764]\n",
      "epoch:26 step:25169 [D loss: 0.577542, acc.: 65.62%] [G loss: 0.481589]\n",
      "epoch:26 step:25170 [D loss: 0.521062, acc.: 73.44%] [G loss: 0.711935]\n",
      "epoch:26 step:25171 [D loss: 0.533420, acc.: 70.31%] [G loss: 0.531381]\n",
      "epoch:26 step:25172 [D loss: 0.513569, acc.: 70.31%] [G loss: 0.626853]\n",
      "epoch:26 step:25173 [D loss: 0.583235, acc.: 64.84%] [G loss: 0.718831]\n",
      "epoch:26 step:25174 [D loss: 0.649893, acc.: 64.84%] [G loss: 0.601837]\n",
      "epoch:26 step:25175 [D loss: 0.564696, acc.: 66.41%] [G loss: 0.576177]\n",
      "epoch:26 step:25176 [D loss: 0.528074, acc.: 70.31%] [G loss: 0.746539]\n",
      "epoch:26 step:25177 [D loss: 0.484783, acc.: 77.34%] [G loss: 0.968003]\n",
      "epoch:26 step:25178 [D loss: 0.556469, acc.: 71.09%] [G loss: 0.780091]\n",
      "epoch:26 step:25179 [D loss: 0.600519, acc.: 67.97%] [G loss: 0.637404]\n",
      "epoch:26 step:25180 [D loss: 0.562600, acc.: 73.44%] [G loss: 0.845740]\n",
      "epoch:26 step:25181 [D loss: 0.505721, acc.: 69.53%] [G loss: 0.666393]\n",
      "epoch:26 step:25182 [D loss: 0.630818, acc.: 67.19%] [G loss: 0.715711]\n",
      "epoch:26 step:25183 [D loss: 0.507269, acc.: 76.56%] [G loss: 0.698855]\n",
      "epoch:26 step:25184 [D loss: 0.535420, acc.: 71.88%] [G loss: 0.628982]\n",
      "epoch:26 step:25185 [D loss: 0.450908, acc.: 75.78%] [G loss: 0.589118]\n",
      "epoch:26 step:25186 [D loss: 0.612501, acc.: 73.44%] [G loss: 0.626651]\n",
      "epoch:26 step:25187 [D loss: 0.592968, acc.: 69.53%] [G loss: 0.624609]\n",
      "epoch:26 step:25188 [D loss: 0.558406, acc.: 65.62%] [G loss: 0.617354]\n",
      "epoch:26 step:25189 [D loss: 0.604439, acc.: 67.19%] [G loss: 0.603726]\n",
      "epoch:26 step:25190 [D loss: 0.630229, acc.: 62.50%] [G loss: 0.682375]\n",
      "epoch:26 step:25191 [D loss: 0.563645, acc.: 67.97%] [G loss: 0.573822]\n",
      "epoch:26 step:25192 [D loss: 0.573095, acc.: 66.41%] [G loss: 0.732167]\n",
      "epoch:26 step:25193 [D loss: 0.514253, acc.: 74.22%] [G loss: 0.547712]\n",
      "epoch:26 step:25194 [D loss: 0.522547, acc.: 71.88%] [G loss: 0.635367]\n",
      "epoch:26 step:25195 [D loss: 0.514610, acc.: 71.09%] [G loss: 0.601356]\n",
      "epoch:26 step:25196 [D loss: 0.508771, acc.: 72.66%] [G loss: 0.567105]\n",
      "epoch:26 step:25197 [D loss: 0.537876, acc.: 71.88%] [G loss: 0.650301]\n",
      "epoch:26 step:25198 [D loss: 0.500289, acc.: 74.22%] [G loss: 0.659355]\n",
      "epoch:26 step:25199 [D loss: 0.475126, acc.: 81.25%] [G loss: 0.821311]\n",
      "epoch:26 step:25200 [D loss: 0.542773, acc.: 70.31%] [G loss: 0.593996]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.703866\n",
      "FID: 49.768501\n",
      "0 = 12.825718737888298\n",
      "1 = 0.09181440470091551\n",
      "2 = 0.8432999849319458\n",
      "3 = 0.8100000023841858\n",
      "4 = 0.8766000270843506\n",
      "5 = 0.8677951693534851\n",
      "6 = 0.8100000023841858\n",
      "7 = 8.364616994786273\n",
      "8 = 0.15261777051194791\n",
      "9 = 0.6901999711990356\n",
      "10 = 0.6814000010490417\n",
      "11 = 0.6990000009536743\n",
      "12 = 0.6936075091362\n",
      "13 = 0.6814000010490417\n",
      "14 = 6.7038960456848145\n",
      "15 = 6.830436706542969\n",
      "16 = 0.39788731932640076\n",
      "17 = 6.703866481781006\n",
      "18 = 49.76850128173828\n",
      "epoch:26 step:25201 [D loss: 0.599822, acc.: 67.19%] [G loss: 0.450852]\n",
      "epoch:26 step:25202 [D loss: 0.556673, acc.: 75.00%] [G loss: 0.499327]\n",
      "epoch:26 step:25203 [D loss: 0.583462, acc.: 63.28%] [G loss: 0.617435]\n",
      "epoch:26 step:25204 [D loss: 0.517463, acc.: 75.00%] [G loss: 0.556309]\n",
      "epoch:26 step:25205 [D loss: 0.543853, acc.: 72.66%] [G loss: 0.678951]\n",
      "epoch:26 step:25206 [D loss: 0.545527, acc.: 71.88%] [G loss: 0.709652]\n",
      "epoch:26 step:25207 [D loss: 0.606201, acc.: 61.72%] [G loss: 0.600212]\n",
      "epoch:26 step:25208 [D loss: 0.562557, acc.: 67.97%] [G loss: 0.674787]\n",
      "epoch:26 step:25209 [D loss: 0.598036, acc.: 66.41%] [G loss: 0.497133]\n",
      "epoch:26 step:25210 [D loss: 0.584170, acc.: 62.50%] [G loss: 0.395347]\n",
      "epoch:26 step:25211 [D loss: 0.582926, acc.: 67.19%] [G loss: 0.599195]\n",
      "epoch:26 step:25212 [D loss: 0.562894, acc.: 70.31%] [G loss: 0.493071]\n",
      "epoch:26 step:25213 [D loss: 0.564468, acc.: 67.19%] [G loss: 0.597568]\n",
      "epoch:26 step:25214 [D loss: 0.560626, acc.: 70.31%] [G loss: 0.532791]\n",
      "epoch:26 step:25215 [D loss: 0.601799, acc.: 66.41%] [G loss: 0.844413]\n",
      "epoch:26 step:25216 [D loss: 0.514163, acc.: 70.31%] [G loss: 0.573047]\n",
      "epoch:26 step:25217 [D loss: 0.619707, acc.: 61.72%] [G loss: 0.655346]\n",
      "epoch:26 step:25218 [D loss: 0.627270, acc.: 67.19%] [G loss: 0.537736]\n",
      "epoch:26 step:25219 [D loss: 0.469511, acc.: 75.00%] [G loss: 0.836147]\n",
      "epoch:26 step:25220 [D loss: 0.634500, acc.: 64.84%] [G loss: 0.626943]\n",
      "epoch:26 step:25221 [D loss: 0.550571, acc.: 74.22%] [G loss: 0.708556]\n",
      "epoch:26 step:25222 [D loss: 0.441635, acc.: 79.69%] [G loss: 0.891676]\n",
      "epoch:26 step:25223 [D loss: 0.608833, acc.: 64.84%] [G loss: 0.584485]\n",
      "epoch:26 step:25224 [D loss: 0.577700, acc.: 67.19%] [G loss: 0.622911]\n",
      "epoch:26 step:25225 [D loss: 0.524861, acc.: 69.53%] [G loss: 0.598468]\n",
      "epoch:26 step:25226 [D loss: 0.514267, acc.: 71.09%] [G loss: 0.567060]\n",
      "epoch:26 step:25227 [D loss: 0.534982, acc.: 70.31%] [G loss: 0.569201]\n",
      "epoch:26 step:25228 [D loss: 0.538021, acc.: 71.09%] [G loss: 0.532115]\n",
      "epoch:26 step:25229 [D loss: 0.648193, acc.: 65.62%] [G loss: 0.456772]\n",
      "epoch:26 step:25230 [D loss: 0.479758, acc.: 79.69%] [G loss: 0.573831]\n",
      "epoch:26 step:25231 [D loss: 0.576268, acc.: 68.75%] [G loss: 0.479064]\n",
      "epoch:26 step:25232 [D loss: 0.437449, acc.: 78.91%] [G loss: 0.751236]\n",
      "epoch:26 step:25233 [D loss: 0.479055, acc.: 72.66%] [G loss: 0.655141]\n",
      "epoch:26 step:25234 [D loss: 0.522033, acc.: 69.53%] [G loss: 0.732755]\n",
      "epoch:26 step:25235 [D loss: 0.609985, acc.: 67.97%] [G loss: 0.444956]\n",
      "epoch:26 step:25236 [D loss: 0.557037, acc.: 72.66%] [G loss: 0.572256]\n",
      "epoch:26 step:25237 [D loss: 0.533828, acc.: 68.75%] [G loss: 0.726503]\n",
      "epoch:26 step:25238 [D loss: 0.563462, acc.: 75.00%] [G loss: 0.591539]\n",
      "epoch:26 step:25239 [D loss: 0.572197, acc.: 66.41%] [G loss: 0.696915]\n",
      "epoch:26 step:25240 [D loss: 0.532445, acc.: 68.75%] [G loss: 0.603280]\n",
      "epoch:26 step:25241 [D loss: 0.592501, acc.: 67.97%] [G loss: 0.497804]\n",
      "epoch:26 step:25242 [D loss: 0.648638, acc.: 64.06%] [G loss: 0.548471]\n",
      "epoch:26 step:25243 [D loss: 0.561486, acc.: 67.97%] [G loss: 0.581148]\n",
      "epoch:26 step:25244 [D loss: 0.570002, acc.: 71.09%] [G loss: 0.507387]\n",
      "epoch:26 step:25245 [D loss: 0.601383, acc.: 68.75%] [G loss: 0.515116]\n",
      "epoch:26 step:25246 [D loss: 0.485704, acc.: 75.78%] [G loss: 0.572835]\n",
      "epoch:26 step:25247 [D loss: 0.587245, acc.: 64.06%] [G loss: 0.708860]\n",
      "epoch:26 step:25248 [D loss: 0.543663, acc.: 73.44%] [G loss: 0.735289]\n",
      "epoch:26 step:25249 [D loss: 0.463348, acc.: 75.78%] [G loss: 0.752556]\n",
      "epoch:26 step:25250 [D loss: 0.584876, acc.: 64.06%] [G loss: 0.704508]\n",
      "epoch:26 step:25251 [D loss: 0.555551, acc.: 70.31%] [G loss: 0.752879]\n",
      "epoch:26 step:25252 [D loss: 0.459222, acc.: 78.91%] [G loss: 0.890261]\n",
      "epoch:26 step:25253 [D loss: 0.603534, acc.: 65.62%] [G loss: 0.544734]\n",
      "epoch:26 step:25254 [D loss: 0.631367, acc.: 61.72%] [G loss: 0.551881]\n",
      "epoch:26 step:25255 [D loss: 0.527814, acc.: 68.75%] [G loss: 0.699419]\n",
      "epoch:26 step:25256 [D loss: 0.429026, acc.: 82.81%] [G loss: 0.666856]\n",
      "epoch:26 step:25257 [D loss: 0.504162, acc.: 73.44%] [G loss: 0.693953]\n",
      "epoch:26 step:25258 [D loss: 0.464327, acc.: 77.34%] [G loss: 0.776777]\n",
      "epoch:26 step:25259 [D loss: 0.548980, acc.: 72.66%] [G loss: 0.587659]\n",
      "epoch:26 step:25260 [D loss: 0.504738, acc.: 77.34%] [G loss: 0.907619]\n",
      "epoch:26 step:25261 [D loss: 0.497621, acc.: 80.47%] [G loss: 0.765888]\n",
      "epoch:26 step:25262 [D loss: 0.507342, acc.: 75.00%] [G loss: 0.830703]\n",
      "epoch:26 step:25263 [D loss: 0.511003, acc.: 75.00%] [G loss: 0.724837]\n",
      "epoch:26 step:25264 [D loss: 0.542680, acc.: 74.22%] [G loss: 0.737045]\n",
      "epoch:26 step:25265 [D loss: 0.515741, acc.: 72.66%] [G loss: 0.729622]\n",
      "epoch:26 step:25266 [D loss: 0.644219, acc.: 64.06%] [G loss: 0.664034]\n",
      "epoch:26 step:25267 [D loss: 0.550711, acc.: 67.19%] [G loss: 0.955355]\n",
      "epoch:26 step:25268 [D loss: 0.505191, acc.: 77.34%] [G loss: 0.740355]\n",
      "epoch:26 step:25269 [D loss: 0.601272, acc.: 69.53%] [G loss: 0.623392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25270 [D loss: 0.538592, acc.: 71.88%] [G loss: 0.662545]\n",
      "epoch:26 step:25271 [D loss: 0.525655, acc.: 75.00%] [G loss: 0.828630]\n",
      "epoch:26 step:25272 [D loss: 0.553515, acc.: 69.53%] [G loss: 0.702293]\n",
      "epoch:26 step:25273 [D loss: 0.497415, acc.: 71.88%] [G loss: 0.786878]\n",
      "epoch:26 step:25274 [D loss: 0.467832, acc.: 73.44%] [G loss: 0.891438]\n",
      "epoch:26 step:25275 [D loss: 0.522547, acc.: 74.22%] [G loss: 0.858299]\n",
      "epoch:26 step:25276 [D loss: 0.449134, acc.: 78.12%] [G loss: 0.851233]\n",
      "epoch:26 step:25277 [D loss: 0.683039, acc.: 59.38%] [G loss: 0.692986]\n",
      "epoch:26 step:25278 [D loss: 0.550218, acc.: 75.00%] [G loss: 0.802969]\n",
      "epoch:26 step:25279 [D loss: 0.599281, acc.: 67.97%] [G loss: 0.551827]\n",
      "epoch:26 step:25280 [D loss: 0.499372, acc.: 75.78%] [G loss: 0.696234]\n",
      "epoch:26 step:25281 [D loss: 0.416270, acc.: 84.38%] [G loss: 0.713550]\n",
      "epoch:26 step:25282 [D loss: 0.733219, acc.: 60.16%] [G loss: 0.844091]\n",
      "epoch:26 step:25283 [D loss: 0.465130, acc.: 75.00%] [G loss: 0.909219]\n",
      "epoch:26 step:25284 [D loss: 0.544654, acc.: 69.53%] [G loss: 0.934624]\n",
      "epoch:26 step:25285 [D loss: 0.481894, acc.: 75.78%] [G loss: 0.812714]\n",
      "epoch:26 step:25286 [D loss: 0.492772, acc.: 75.00%] [G loss: 0.901826]\n",
      "epoch:26 step:25287 [D loss: 0.393174, acc.: 82.81%] [G loss: 1.135841]\n",
      "epoch:26 step:25288 [D loss: 0.406740, acc.: 80.47%] [G loss: 1.058685]\n",
      "epoch:26 step:25289 [D loss: 0.570722, acc.: 71.09%] [G loss: 1.116273]\n",
      "epoch:26 step:25290 [D loss: 0.717385, acc.: 58.59%] [G loss: 1.017034]\n",
      "epoch:26 step:25291 [D loss: 0.461742, acc.: 78.12%] [G loss: 1.228236]\n",
      "epoch:26 step:25292 [D loss: 0.453028, acc.: 76.56%] [G loss: 1.398164]\n",
      "epoch:26 step:25293 [D loss: 0.483380, acc.: 74.22%] [G loss: 1.145601]\n",
      "epoch:26 step:25294 [D loss: 0.587734, acc.: 66.41%] [G loss: 0.720873]\n",
      "epoch:26 step:25295 [D loss: 0.542058, acc.: 69.53%] [G loss: 0.847283]\n",
      "epoch:26 step:25296 [D loss: 0.507783, acc.: 71.88%] [G loss: 1.008424]\n",
      "epoch:26 step:25297 [D loss: 0.486597, acc.: 75.00%] [G loss: 1.217720]\n",
      "epoch:26 step:25298 [D loss: 0.378634, acc.: 83.59%] [G loss: 1.445675]\n",
      "epoch:26 step:25299 [D loss: 0.419365, acc.: 81.25%] [G loss: 1.466278]\n",
      "epoch:27 step:25300 [D loss: 0.550668, acc.: 73.44%] [G loss: 1.263952]\n",
      "epoch:27 step:25301 [D loss: 0.553859, acc.: 72.66%] [G loss: 1.139567]\n",
      "epoch:27 step:25302 [D loss: 0.585054, acc.: 67.97%] [G loss: 1.081040]\n",
      "epoch:27 step:25303 [D loss: 0.480968, acc.: 79.69%] [G loss: 0.981268]\n",
      "epoch:27 step:25304 [D loss: 0.563226, acc.: 70.31%] [G loss: 1.007207]\n",
      "epoch:27 step:25305 [D loss: 0.597774, acc.: 66.41%] [G loss: 0.876451]\n",
      "epoch:27 step:25306 [D loss: 0.459195, acc.: 78.91%] [G loss: 0.844371]\n",
      "epoch:27 step:25307 [D loss: 0.480330, acc.: 78.91%] [G loss: 0.826781]\n",
      "epoch:27 step:25308 [D loss: 0.468254, acc.: 78.12%] [G loss: 0.802087]\n",
      "epoch:27 step:25309 [D loss: 0.544884, acc.: 73.44%] [G loss: 0.789702]\n",
      "epoch:27 step:25310 [D loss: 0.493523, acc.: 78.91%] [G loss: 0.783301]\n",
      "epoch:27 step:25311 [D loss: 0.583950, acc.: 69.53%] [G loss: 0.805723]\n",
      "epoch:27 step:25312 [D loss: 0.569088, acc.: 71.09%] [G loss: 0.892351]\n",
      "epoch:27 step:25313 [D loss: 0.517484, acc.: 76.56%] [G loss: 0.856734]\n",
      "epoch:27 step:25314 [D loss: 0.462350, acc.: 75.78%] [G loss: 0.714562]\n",
      "epoch:27 step:25315 [D loss: 0.531907, acc.: 75.78%] [G loss: 0.945590]\n",
      "epoch:27 step:25316 [D loss: 0.511939, acc.: 78.12%] [G loss: 0.854749]\n",
      "epoch:27 step:25317 [D loss: 0.605354, acc.: 66.41%] [G loss: 0.662300]\n",
      "epoch:27 step:25318 [D loss: 0.588518, acc.: 65.62%] [G loss: 0.718515]\n",
      "epoch:27 step:25319 [D loss: 0.602966, acc.: 64.84%] [G loss: 0.809485]\n",
      "epoch:27 step:25320 [D loss: 0.547597, acc.: 71.09%] [G loss: 0.785097]\n",
      "epoch:27 step:25321 [D loss: 0.459243, acc.: 75.78%] [G loss: 1.176097]\n",
      "epoch:27 step:25322 [D loss: 0.619582, acc.: 65.62%] [G loss: 0.643068]\n",
      "epoch:27 step:25323 [D loss: 0.479618, acc.: 72.66%] [G loss: 0.689659]\n",
      "epoch:27 step:25324 [D loss: 0.511978, acc.: 74.22%] [G loss: 0.781598]\n",
      "epoch:27 step:25325 [D loss: 0.565918, acc.: 66.41%] [G loss: 0.690892]\n",
      "epoch:27 step:25326 [D loss: 0.498533, acc.: 71.88%] [G loss: 0.676696]\n",
      "epoch:27 step:25327 [D loss: 0.588778, acc.: 70.31%] [G loss: 0.669890]\n",
      "epoch:27 step:25328 [D loss: 0.527375, acc.: 72.66%] [G loss: 0.451888]\n",
      "epoch:27 step:25329 [D loss: 0.533520, acc.: 67.97%] [G loss: 0.698672]\n",
      "epoch:27 step:25330 [D loss: 0.577413, acc.: 65.62%] [G loss: 0.503923]\n",
      "epoch:27 step:25331 [D loss: 0.511578, acc.: 76.56%] [G loss: 0.590228]\n",
      "epoch:27 step:25332 [D loss: 0.557837, acc.: 67.97%] [G loss: 0.746503]\n",
      "epoch:27 step:25333 [D loss: 0.520805, acc.: 71.88%] [G loss: 0.664325]\n",
      "epoch:27 step:25334 [D loss: 0.557596, acc.: 64.84%] [G loss: 0.673209]\n",
      "epoch:27 step:25335 [D loss: 0.544451, acc.: 71.88%] [G loss: 0.740981]\n",
      "epoch:27 step:25336 [D loss: 0.470251, acc.: 78.91%] [G loss: 0.673822]\n",
      "epoch:27 step:25337 [D loss: 0.580104, acc.: 71.09%] [G loss: 0.631732]\n",
      "epoch:27 step:25338 [D loss: 0.564201, acc.: 65.62%] [G loss: 0.740492]\n",
      "epoch:27 step:25339 [D loss: 0.460686, acc.: 76.56%] [G loss: 0.833622]\n",
      "epoch:27 step:25340 [D loss: 0.524236, acc.: 69.53%] [G loss: 0.775257]\n",
      "epoch:27 step:25341 [D loss: 0.567488, acc.: 69.53%] [G loss: 0.620399]\n",
      "epoch:27 step:25342 [D loss: 0.512315, acc.: 75.78%] [G loss: 0.628518]\n",
      "epoch:27 step:25343 [D loss: 0.569006, acc.: 67.19%] [G loss: 0.630953]\n",
      "epoch:27 step:25344 [D loss: 0.460111, acc.: 77.34%] [G loss: 0.723413]\n",
      "epoch:27 step:25345 [D loss: 0.483979, acc.: 74.22%] [G loss: 0.790064]\n",
      "epoch:27 step:25346 [D loss: 0.582522, acc.: 65.62%] [G loss: 0.770067]\n",
      "epoch:27 step:25347 [D loss: 0.516778, acc.: 71.09%] [G loss: 0.797113]\n",
      "epoch:27 step:25348 [D loss: 0.500710, acc.: 73.44%] [G loss: 0.707245]\n",
      "epoch:27 step:25349 [D loss: 0.521683, acc.: 73.44%] [G loss: 0.597699]\n",
      "epoch:27 step:25350 [D loss: 0.576610, acc.: 72.66%] [G loss: 0.610814]\n",
      "epoch:27 step:25351 [D loss: 0.530178, acc.: 75.78%] [G loss: 0.686760]\n",
      "epoch:27 step:25352 [D loss: 0.548595, acc.: 71.09%] [G loss: 0.548804]\n",
      "epoch:27 step:25353 [D loss: 0.525115, acc.: 66.41%] [G loss: 0.812446]\n",
      "epoch:27 step:25354 [D loss: 0.546034, acc.: 69.53%] [G loss: 0.857220]\n",
      "epoch:27 step:25355 [D loss: 0.487648, acc.: 76.56%] [G loss: 0.814866]\n",
      "epoch:27 step:25356 [D loss: 0.490629, acc.: 72.66%] [G loss: 0.672135]\n",
      "epoch:27 step:25357 [D loss: 0.592876, acc.: 67.19%] [G loss: 0.654698]\n",
      "epoch:27 step:25358 [D loss: 0.501587, acc.: 75.00%] [G loss: 0.680188]\n",
      "epoch:27 step:25359 [D loss: 0.523201, acc.: 74.22%] [G loss: 0.800138]\n",
      "epoch:27 step:25360 [D loss: 0.564636, acc.: 69.53%] [G loss: 0.539514]\n",
      "epoch:27 step:25361 [D loss: 0.541455, acc.: 74.22%] [G loss: 0.671201]\n",
      "epoch:27 step:25362 [D loss: 0.588025, acc.: 65.62%] [G loss: 0.556040]\n",
      "epoch:27 step:25363 [D loss: 0.514574, acc.: 78.91%] [G loss: 0.723051]\n",
      "epoch:27 step:25364 [D loss: 0.562865, acc.: 72.66%] [G loss: 0.789328]\n",
      "epoch:27 step:25365 [D loss: 0.566182, acc.: 64.84%] [G loss: 0.752102]\n",
      "epoch:27 step:25366 [D loss: 0.532401, acc.: 73.44%] [G loss: 0.766283]\n",
      "epoch:27 step:25367 [D loss: 0.582699, acc.: 64.84%] [G loss: 0.587713]\n",
      "epoch:27 step:25368 [D loss: 0.503097, acc.: 77.34%] [G loss: 0.614352]\n",
      "epoch:27 step:25369 [D loss: 0.520414, acc.: 74.22%] [G loss: 0.797891]\n",
      "epoch:27 step:25370 [D loss: 0.533047, acc.: 67.97%] [G loss: 0.752668]\n",
      "epoch:27 step:25371 [D loss: 0.548577, acc.: 69.53%] [G loss: 0.668911]\n",
      "epoch:27 step:25372 [D loss: 0.563897, acc.: 67.97%] [G loss: 0.840906]\n",
      "epoch:27 step:25373 [D loss: 0.511570, acc.: 77.34%] [G loss: 0.632579]\n",
      "epoch:27 step:25374 [D loss: 0.527804, acc.: 75.78%] [G loss: 0.833467]\n",
      "epoch:27 step:25375 [D loss: 0.543222, acc.: 71.09%] [G loss: 0.806268]\n",
      "epoch:27 step:25376 [D loss: 0.495128, acc.: 71.88%] [G loss: 0.881948]\n",
      "epoch:27 step:25377 [D loss: 0.613428, acc.: 69.53%] [G loss: 0.654518]\n",
      "epoch:27 step:25378 [D loss: 0.593778, acc.: 62.50%] [G loss: 0.560595]\n",
      "epoch:27 step:25379 [D loss: 0.502358, acc.: 76.56%] [G loss: 0.639705]\n",
      "epoch:27 step:25380 [D loss: 0.552324, acc.: 68.75%] [G loss: 0.679628]\n",
      "epoch:27 step:25381 [D loss: 0.482953, acc.: 71.09%] [G loss: 0.676013]\n",
      "epoch:27 step:25382 [D loss: 0.443241, acc.: 80.47%] [G loss: 0.690149]\n",
      "epoch:27 step:25383 [D loss: 0.488351, acc.: 73.44%] [G loss: 0.796910]\n",
      "epoch:27 step:25384 [D loss: 0.573649, acc.: 67.97%] [G loss: 0.687040]\n",
      "epoch:27 step:25385 [D loss: 0.513193, acc.: 72.66%] [G loss: 0.698462]\n",
      "epoch:27 step:25386 [D loss: 0.514575, acc.: 73.44%] [G loss: 0.656026]\n",
      "epoch:27 step:25387 [D loss: 0.544896, acc.: 71.88%] [G loss: 0.668298]\n",
      "epoch:27 step:25388 [D loss: 0.534286, acc.: 68.75%] [G loss: 0.644301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25389 [D loss: 0.485878, acc.: 76.56%] [G loss: 0.750850]\n",
      "epoch:27 step:25390 [D loss: 0.594353, acc.: 64.84%] [G loss: 0.713873]\n",
      "epoch:27 step:25391 [D loss: 0.462680, acc.: 75.78%] [G loss: 0.789807]\n",
      "epoch:27 step:25392 [D loss: 0.514018, acc.: 70.31%] [G loss: 0.832064]\n",
      "epoch:27 step:25393 [D loss: 0.484273, acc.: 73.44%] [G loss: 0.911573]\n",
      "epoch:27 step:25394 [D loss: 0.580113, acc.: 67.19%] [G loss: 0.920465]\n",
      "epoch:27 step:25395 [D loss: 0.477855, acc.: 75.78%] [G loss: 1.073033]\n",
      "epoch:27 step:25396 [D loss: 0.504599, acc.: 73.44%] [G loss: 1.038323]\n",
      "epoch:27 step:25397 [D loss: 0.563258, acc.: 67.19%] [G loss: 0.792551]\n",
      "epoch:27 step:25398 [D loss: 0.538292, acc.: 74.22%] [G loss: 0.685584]\n",
      "epoch:27 step:25399 [D loss: 0.480743, acc.: 73.44%] [G loss: 0.796108]\n",
      "epoch:27 step:25400 [D loss: 0.521169, acc.: 75.78%] [G loss: 0.986700]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.776812\n",
      "FID: 51.272018\n",
      "0 = 12.951439632797229\n",
      "1 = 0.0955042426879504\n",
      "2 = 0.8564000129699707\n",
      "3 = 0.8101999759674072\n",
      "4 = 0.9025999903678894\n",
      "5 = 0.8926839828491211\n",
      "6 = 0.8101999759674072\n",
      "7 = 8.50091584331991\n",
      "8 = 0.1605413505550427\n",
      "9 = 0.6988000273704529\n",
      "10 = 0.6923999786376953\n",
      "11 = 0.7052000164985657\n",
      "12 = 0.7013776302337646\n",
      "13 = 0.6923999786376953\n",
      "14 = 6.776844024658203\n",
      "15 = 7.029510021209717\n",
      "16 = 0.38569244742393494\n",
      "17 = 6.776811599731445\n",
      "18 = 51.27201843261719\n",
      "epoch:27 step:25401 [D loss: 0.673420, acc.: 60.94%] [G loss: 0.803611]\n",
      "epoch:27 step:25402 [D loss: 0.501947, acc.: 78.12%] [G loss: 0.716682]\n",
      "epoch:27 step:25403 [D loss: 0.512111, acc.: 71.88%] [G loss: 0.700517]\n",
      "epoch:27 step:25404 [D loss: 0.637874, acc.: 56.25%] [G loss: 0.841338]\n",
      "epoch:27 step:25405 [D loss: 0.539314, acc.: 71.88%] [G loss: 0.831161]\n",
      "epoch:27 step:25406 [D loss: 0.535143, acc.: 68.75%] [G loss: 0.860836]\n",
      "epoch:27 step:25407 [D loss: 0.610927, acc.: 70.31%] [G loss: 0.918674]\n",
      "epoch:27 step:25408 [D loss: 0.569133, acc.: 67.19%] [G loss: 0.629430]\n",
      "epoch:27 step:25409 [D loss: 0.540479, acc.: 76.56%] [G loss: 0.673780]\n",
      "epoch:27 step:25410 [D loss: 0.512148, acc.: 71.09%] [G loss: 0.636009]\n",
      "epoch:27 step:25411 [D loss: 0.517091, acc.: 75.00%] [G loss: 0.742645]\n",
      "epoch:27 step:25412 [D loss: 0.541676, acc.: 74.22%] [G loss: 0.659245]\n",
      "epoch:27 step:25413 [D loss: 0.524454, acc.: 75.00%] [G loss: 0.746005]\n",
      "epoch:27 step:25414 [D loss: 0.508788, acc.: 75.78%] [G loss: 0.741616]\n",
      "epoch:27 step:25415 [D loss: 0.523693, acc.: 71.88%] [G loss: 0.766275]\n",
      "epoch:27 step:25416 [D loss: 0.483759, acc.: 73.44%] [G loss: 0.855515]\n",
      "epoch:27 step:25417 [D loss: 0.527051, acc.: 77.34%] [G loss: 0.833629]\n",
      "epoch:27 step:25418 [D loss: 0.459495, acc.: 80.47%] [G loss: 1.021603]\n",
      "epoch:27 step:25419 [D loss: 0.582938, acc.: 68.75%] [G loss: 0.802060]\n",
      "epoch:27 step:25420 [D loss: 0.568124, acc.: 69.53%] [G loss: 0.646387]\n",
      "epoch:27 step:25421 [D loss: 0.542375, acc.: 73.44%] [G loss: 0.909123]\n",
      "epoch:27 step:25422 [D loss: 0.584150, acc.: 71.09%] [G loss: 0.948640]\n",
      "epoch:27 step:25423 [D loss: 0.555796, acc.: 72.66%] [G loss: 0.837128]\n",
      "epoch:27 step:25424 [D loss: 0.600163, acc.: 65.62%] [G loss: 0.760134]\n",
      "epoch:27 step:25425 [D loss: 0.509882, acc.: 76.56%] [G loss: 0.685452]\n",
      "epoch:27 step:25426 [D loss: 0.500112, acc.: 75.78%] [G loss: 0.796119]\n",
      "epoch:27 step:25427 [D loss: 0.509047, acc.: 75.00%] [G loss: 0.842238]\n",
      "epoch:27 step:25428 [D loss: 0.548041, acc.: 67.97%] [G loss: 0.641303]\n",
      "epoch:27 step:25429 [D loss: 0.531096, acc.: 69.53%] [G loss: 0.663133]\n",
      "epoch:27 step:25430 [D loss: 0.492493, acc.: 75.00%] [G loss: 0.747773]\n",
      "epoch:27 step:25431 [D loss: 0.531001, acc.: 74.22%] [G loss: 0.671007]\n",
      "epoch:27 step:25432 [D loss: 0.578833, acc.: 64.06%] [G loss: 0.830758]\n",
      "epoch:27 step:25433 [D loss: 0.500383, acc.: 72.66%] [G loss: 0.723186]\n",
      "epoch:27 step:25434 [D loss: 0.522346, acc.: 72.66%] [G loss: 0.712636]\n",
      "epoch:27 step:25435 [D loss: 0.518497, acc.: 74.22%] [G loss: 0.724180]\n",
      "epoch:27 step:25436 [D loss: 0.600913, acc.: 69.53%] [G loss: 0.856791]\n",
      "epoch:27 step:25437 [D loss: 0.548446, acc.: 70.31%] [G loss: 0.564983]\n",
      "epoch:27 step:25438 [D loss: 0.501276, acc.: 78.91%] [G loss: 0.677803]\n",
      "epoch:27 step:25439 [D loss: 0.548482, acc.: 67.19%] [G loss: 0.629750]\n",
      "epoch:27 step:25440 [D loss: 0.562794, acc.: 65.62%] [G loss: 0.538350]\n",
      "epoch:27 step:25441 [D loss: 0.595568, acc.: 66.41%] [G loss: 0.628107]\n",
      "epoch:27 step:25442 [D loss: 0.574827, acc.: 63.28%] [G loss: 0.654049]\n",
      "epoch:27 step:25443 [D loss: 0.534151, acc.: 71.09%] [G loss: 0.494366]\n",
      "epoch:27 step:25444 [D loss: 0.516172, acc.: 70.31%] [G loss: 0.637741]\n",
      "epoch:27 step:25445 [D loss: 0.474625, acc.: 78.12%] [G loss: 0.753184]\n",
      "epoch:27 step:25446 [D loss: 0.601680, acc.: 67.19%] [G loss: 0.574457]\n",
      "epoch:27 step:25447 [D loss: 0.564370, acc.: 70.31%] [G loss: 0.524948]\n",
      "epoch:27 step:25448 [D loss: 0.518622, acc.: 71.09%] [G loss: 0.652845]\n",
      "epoch:27 step:25449 [D loss: 0.601551, acc.: 67.19%] [G loss: 0.552489]\n",
      "epoch:27 step:25450 [D loss: 0.541316, acc.: 69.53%] [G loss: 0.612195]\n",
      "epoch:27 step:25451 [D loss: 0.490931, acc.: 75.00%] [G loss: 0.677717]\n",
      "epoch:27 step:25452 [D loss: 0.564206, acc.: 66.41%] [G loss: 0.776934]\n",
      "epoch:27 step:25453 [D loss: 0.556332, acc.: 71.09%] [G loss: 0.599619]\n",
      "epoch:27 step:25454 [D loss: 0.529564, acc.: 67.97%] [G loss: 0.594190]\n",
      "epoch:27 step:25455 [D loss: 0.467838, acc.: 77.34%] [G loss: 0.802374]\n",
      "epoch:27 step:25456 [D loss: 0.595102, acc.: 64.84%] [G loss: 0.812663]\n",
      "epoch:27 step:25457 [D loss: 0.548866, acc.: 71.09%] [G loss: 1.036030]\n",
      "epoch:27 step:25458 [D loss: 0.485901, acc.: 73.44%] [G loss: 0.712981]\n",
      "epoch:27 step:25459 [D loss: 0.604893, acc.: 66.41%] [G loss: 0.718540]\n",
      "epoch:27 step:25460 [D loss: 0.509679, acc.: 72.66%] [G loss: 0.797040]\n",
      "epoch:27 step:25461 [D loss: 0.465654, acc.: 79.69%] [G loss: 1.007333]\n",
      "epoch:27 step:25462 [D loss: 0.575372, acc.: 71.88%] [G loss: 0.659310]\n",
      "epoch:27 step:25463 [D loss: 0.654832, acc.: 58.59%] [G loss: 0.581497]\n",
      "epoch:27 step:25464 [D loss: 0.525821, acc.: 72.66%] [G loss: 0.716188]\n",
      "epoch:27 step:25465 [D loss: 0.582109, acc.: 67.97%] [G loss: 0.552241]\n",
      "epoch:27 step:25466 [D loss: 0.508826, acc.: 71.88%] [G loss: 0.682153]\n",
      "epoch:27 step:25467 [D loss: 0.583196, acc.: 70.31%] [G loss: 0.641368]\n",
      "epoch:27 step:25468 [D loss: 0.542145, acc.: 72.66%] [G loss: 0.562767]\n",
      "epoch:27 step:25469 [D loss: 0.601586, acc.: 63.28%] [G loss: 0.460448]\n",
      "epoch:27 step:25470 [D loss: 0.502466, acc.: 76.56%] [G loss: 0.694142]\n",
      "epoch:27 step:25471 [D loss: 0.518861, acc.: 74.22%] [G loss: 0.663816]\n",
      "epoch:27 step:25472 [D loss: 0.472524, acc.: 75.78%] [G loss: 0.858981]\n",
      "epoch:27 step:25473 [D loss: 0.615825, acc.: 63.28%] [G loss: 0.649051]\n",
      "epoch:27 step:25474 [D loss: 0.611186, acc.: 61.72%] [G loss: 0.638613]\n",
      "epoch:27 step:25475 [D loss: 0.507542, acc.: 72.66%] [G loss: 0.670248]\n",
      "epoch:27 step:25476 [D loss: 0.541178, acc.: 68.75%] [G loss: 0.693446]\n",
      "epoch:27 step:25477 [D loss: 0.582376, acc.: 68.75%] [G loss: 0.599651]\n",
      "epoch:27 step:25478 [D loss: 0.536805, acc.: 71.88%] [G loss: 0.646933]\n",
      "epoch:27 step:25479 [D loss: 0.601295, acc.: 66.41%] [G loss: 0.654601]\n",
      "epoch:27 step:25480 [D loss: 0.619766, acc.: 65.62%] [G loss: 0.598565]\n",
      "epoch:27 step:25481 [D loss: 0.524370, acc.: 70.31%] [G loss: 0.728996]\n",
      "epoch:27 step:25482 [D loss: 0.567958, acc.: 67.19%] [G loss: 0.773959]\n",
      "epoch:27 step:25483 [D loss: 0.569324, acc.: 68.75%] [G loss: 0.792470]\n",
      "epoch:27 step:25484 [D loss: 0.595233, acc.: 67.97%] [G loss: 0.891407]\n",
      "epoch:27 step:25485 [D loss: 0.522389, acc.: 74.22%] [G loss: 0.697931]\n",
      "epoch:27 step:25486 [D loss: 0.644247, acc.: 64.06%] [G loss: 0.499203]\n",
      "epoch:27 step:25487 [D loss: 0.514896, acc.: 79.69%] [G loss: 0.664146]\n",
      "epoch:27 step:25488 [D loss: 0.573826, acc.: 68.75%] [G loss: 0.614987]\n",
      "epoch:27 step:25489 [D loss: 0.556506, acc.: 70.31%] [G loss: 0.582858]\n",
      "epoch:27 step:25490 [D loss: 0.493573, acc.: 72.66%] [G loss: 0.724282]\n",
      "epoch:27 step:25491 [D loss: 0.533811, acc.: 75.78%] [G loss: 0.718049]\n",
      "epoch:27 step:25492 [D loss: 0.504980, acc.: 74.22%] [G loss: 0.773483]\n",
      "epoch:27 step:25493 [D loss: 0.499812, acc.: 74.22%] [G loss: 0.907342]\n",
      "epoch:27 step:25494 [D loss: 0.566164, acc.: 65.62%] [G loss: 0.700650]\n",
      "epoch:27 step:25495 [D loss: 0.532708, acc.: 69.53%] [G loss: 0.816296]\n",
      "epoch:27 step:25496 [D loss: 0.541173, acc.: 69.53%] [G loss: 0.649371]\n",
      "epoch:27 step:25497 [D loss: 0.432142, acc.: 78.12%] [G loss: 0.800613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25498 [D loss: 0.586650, acc.: 67.97%] [G loss: 0.924895]\n",
      "epoch:27 step:25499 [D loss: 0.558272, acc.: 71.88%] [G loss: 0.864739]\n",
      "epoch:27 step:25500 [D loss: 0.574295, acc.: 67.97%] [G loss: 0.636763]\n",
      "epoch:27 step:25501 [D loss: 0.541244, acc.: 69.53%] [G loss: 0.632241]\n",
      "epoch:27 step:25502 [D loss: 0.560012, acc.: 69.53%] [G loss: 0.617156]\n",
      "epoch:27 step:25503 [D loss: 0.538031, acc.: 67.97%] [G loss: 0.753968]\n",
      "epoch:27 step:25504 [D loss: 0.456464, acc.: 78.91%] [G loss: 0.859912]\n",
      "epoch:27 step:25505 [D loss: 0.481680, acc.: 76.56%] [G loss: 1.134855]\n",
      "epoch:27 step:25506 [D loss: 0.461338, acc.: 80.47%] [G loss: 0.939700]\n",
      "epoch:27 step:25507 [D loss: 0.430782, acc.: 77.34%] [G loss: 1.131822]\n",
      "epoch:27 step:25508 [D loss: 0.481093, acc.: 77.34%] [G loss: 1.012541]\n",
      "epoch:27 step:25509 [D loss: 0.675658, acc.: 66.41%] [G loss: 0.631001]\n",
      "epoch:27 step:25510 [D loss: 0.631586, acc.: 64.06%] [G loss: 0.667845]\n",
      "epoch:27 step:25511 [D loss: 0.529641, acc.: 72.66%] [G loss: 0.590143]\n",
      "epoch:27 step:25512 [D loss: 0.498773, acc.: 70.31%] [G loss: 0.725726]\n",
      "epoch:27 step:25513 [D loss: 0.623968, acc.: 67.19%] [G loss: 0.678043]\n",
      "epoch:27 step:25514 [D loss: 0.604395, acc.: 67.97%] [G loss: 0.680025]\n",
      "epoch:27 step:25515 [D loss: 0.477821, acc.: 75.78%] [G loss: 0.724600]\n",
      "epoch:27 step:25516 [D loss: 0.558759, acc.: 68.75%] [G loss: 0.687329]\n",
      "epoch:27 step:25517 [D loss: 0.468713, acc.: 79.69%] [G loss: 0.612535]\n",
      "epoch:27 step:25518 [D loss: 0.491598, acc.: 71.88%] [G loss: 0.746749]\n",
      "epoch:27 step:25519 [D loss: 0.629682, acc.: 64.84%] [G loss: 0.775192]\n",
      "epoch:27 step:25520 [D loss: 0.549257, acc.: 71.09%] [G loss: 0.714972]\n",
      "epoch:27 step:25521 [D loss: 0.484931, acc.: 76.56%] [G loss: 0.805029]\n",
      "epoch:27 step:25522 [D loss: 0.542672, acc.: 70.31%] [G loss: 0.772677]\n",
      "epoch:27 step:25523 [D loss: 0.527414, acc.: 73.44%] [G loss: 0.857492]\n",
      "epoch:27 step:25524 [D loss: 0.477067, acc.: 79.69%] [G loss: 0.781765]\n",
      "epoch:27 step:25525 [D loss: 0.588668, acc.: 65.62%] [G loss: 0.616195]\n",
      "epoch:27 step:25526 [D loss: 0.519521, acc.: 73.44%] [G loss: 0.687990]\n",
      "epoch:27 step:25527 [D loss: 0.619056, acc.: 63.28%] [G loss: 0.617709]\n",
      "epoch:27 step:25528 [D loss: 0.516878, acc.: 76.56%] [G loss: 0.610276]\n",
      "epoch:27 step:25529 [D loss: 0.487602, acc.: 75.78%] [G loss: 0.854920]\n",
      "epoch:27 step:25530 [D loss: 0.483661, acc.: 75.78%] [G loss: 0.777309]\n",
      "epoch:27 step:25531 [D loss: 0.506009, acc.: 73.44%] [G loss: 1.021732]\n",
      "epoch:27 step:25532 [D loss: 0.543478, acc.: 70.31%] [G loss: 0.814267]\n",
      "epoch:27 step:25533 [D loss: 0.614488, acc.: 69.53%] [G loss: 0.697706]\n",
      "epoch:27 step:25534 [D loss: 0.593975, acc.: 67.19%] [G loss: 0.697911]\n",
      "epoch:27 step:25535 [D loss: 0.578161, acc.: 68.75%] [G loss: 0.532787]\n",
      "epoch:27 step:25536 [D loss: 0.498748, acc.: 73.44%] [G loss: 0.661401]\n",
      "epoch:27 step:25537 [D loss: 0.560633, acc.: 70.31%] [G loss: 0.537403]\n",
      "epoch:27 step:25538 [D loss: 0.542703, acc.: 67.97%] [G loss: 0.570832]\n",
      "epoch:27 step:25539 [D loss: 0.538552, acc.: 75.78%] [G loss: 0.670403]\n",
      "epoch:27 step:25540 [D loss: 0.549866, acc.: 69.53%] [G loss: 0.653736]\n",
      "epoch:27 step:25541 [D loss: 0.522272, acc.: 68.75%] [G loss: 0.717815]\n",
      "epoch:27 step:25542 [D loss: 0.506566, acc.: 71.88%] [G loss: 0.665019]\n",
      "epoch:27 step:25543 [D loss: 0.468933, acc.: 76.56%] [G loss: 0.646419]\n",
      "epoch:27 step:25544 [D loss: 0.521763, acc.: 73.44%] [G loss: 0.715398]\n",
      "epoch:27 step:25545 [D loss: 0.496273, acc.: 75.78%] [G loss: 0.689154]\n",
      "epoch:27 step:25546 [D loss: 0.475029, acc.: 72.66%] [G loss: 0.792796]\n",
      "epoch:27 step:25547 [D loss: 0.447364, acc.: 78.12%] [G loss: 0.890283]\n",
      "epoch:27 step:25548 [D loss: 0.532903, acc.: 73.44%] [G loss: 0.798950]\n",
      "epoch:27 step:25549 [D loss: 0.610536, acc.: 64.06%] [G loss: 0.851965]\n",
      "epoch:27 step:25550 [D loss: 0.658096, acc.: 65.62%] [G loss: 0.713766]\n",
      "epoch:27 step:25551 [D loss: 0.571209, acc.: 68.75%] [G loss: 0.747807]\n",
      "epoch:27 step:25552 [D loss: 0.600282, acc.: 62.50%] [G loss: 0.675020]\n",
      "epoch:27 step:25553 [D loss: 0.482868, acc.: 73.44%] [G loss: 0.713449]\n",
      "epoch:27 step:25554 [D loss: 0.515292, acc.: 71.09%] [G loss: 0.555921]\n",
      "epoch:27 step:25555 [D loss: 0.569204, acc.: 63.28%] [G loss: 0.467396]\n",
      "epoch:27 step:25556 [D loss: 0.582937, acc.: 62.50%] [G loss: 0.566446]\n",
      "epoch:27 step:25557 [D loss: 0.532776, acc.: 65.62%] [G loss: 0.624195]\n",
      "epoch:27 step:25558 [D loss: 0.531505, acc.: 67.97%] [G loss: 0.466180]\n",
      "epoch:27 step:25559 [D loss: 0.570656, acc.: 66.41%] [G loss: 0.622735]\n",
      "epoch:27 step:25560 [D loss: 0.540858, acc.: 72.66%] [G loss: 0.641786]\n",
      "epoch:27 step:25561 [D loss: 0.495844, acc.: 78.12%] [G loss: 0.636951]\n",
      "epoch:27 step:25562 [D loss: 0.599213, acc.: 65.62%] [G loss: 0.741458]\n",
      "epoch:27 step:25563 [D loss: 0.525898, acc.: 70.31%] [G loss: 0.598386]\n",
      "epoch:27 step:25564 [D loss: 0.534408, acc.: 70.31%] [G loss: 0.721478]\n",
      "epoch:27 step:25565 [D loss: 0.607764, acc.: 64.84%] [G loss: 0.808944]\n",
      "epoch:27 step:25566 [D loss: 0.522129, acc.: 75.78%] [G loss: 0.805215]\n",
      "epoch:27 step:25567 [D loss: 0.565449, acc.: 68.75%] [G loss: 0.600517]\n",
      "epoch:27 step:25568 [D loss: 0.534123, acc.: 73.44%] [G loss: 0.651454]\n",
      "epoch:27 step:25569 [D loss: 0.458369, acc.: 78.91%] [G loss: 0.550507]\n",
      "epoch:27 step:25570 [D loss: 0.513023, acc.: 72.66%] [G loss: 0.765616]\n",
      "epoch:27 step:25571 [D loss: 0.513306, acc.: 73.44%] [G loss: 0.625928]\n",
      "epoch:27 step:25572 [D loss: 0.520223, acc.: 71.88%] [G loss: 0.762981]\n",
      "epoch:27 step:25573 [D loss: 0.479535, acc.: 76.56%] [G loss: 0.910829]\n",
      "epoch:27 step:25574 [D loss: 0.553184, acc.: 73.44%] [G loss: 0.872277]\n",
      "epoch:27 step:25575 [D loss: 0.466717, acc.: 78.91%] [G loss: 0.787021]\n",
      "epoch:27 step:25576 [D loss: 0.735779, acc.: 57.81%] [G loss: 0.740558]\n",
      "epoch:27 step:25577 [D loss: 0.672418, acc.: 57.81%] [G loss: 0.393244]\n",
      "epoch:27 step:25578 [D loss: 0.543518, acc.: 69.53%] [G loss: 0.541523]\n",
      "epoch:27 step:25579 [D loss: 0.508423, acc.: 69.53%] [G loss: 0.704784]\n",
      "epoch:27 step:25580 [D loss: 0.596711, acc.: 66.41%] [G loss: 0.549844]\n",
      "epoch:27 step:25581 [D loss: 0.546259, acc.: 70.31%] [G loss: 0.522838]\n",
      "epoch:27 step:25582 [D loss: 0.515136, acc.: 74.22%] [G loss: 0.622961]\n",
      "epoch:27 step:25583 [D loss: 0.534624, acc.: 71.09%] [G loss: 0.759515]\n",
      "epoch:27 step:25584 [D loss: 0.551004, acc.: 71.09%] [G loss: 0.621276]\n",
      "epoch:27 step:25585 [D loss: 0.479258, acc.: 74.22%] [G loss: 0.919851]\n",
      "epoch:27 step:25586 [D loss: 0.595679, acc.: 65.62%] [G loss: 0.615712]\n",
      "epoch:27 step:25587 [D loss: 0.548306, acc.: 68.75%] [G loss: 0.752173]\n",
      "epoch:27 step:25588 [D loss: 0.568828, acc.: 65.62%] [G loss: 0.792826]\n",
      "epoch:27 step:25589 [D loss: 0.577476, acc.: 69.53%] [G loss: 0.685334]\n",
      "epoch:27 step:25590 [D loss: 0.597279, acc.: 66.41%] [G loss: 0.704655]\n",
      "epoch:27 step:25591 [D loss: 0.521030, acc.: 75.78%] [G loss: 0.795032]\n",
      "epoch:27 step:25592 [D loss: 0.570586, acc.: 66.41%] [G loss: 0.477990]\n",
      "epoch:27 step:25593 [D loss: 0.589346, acc.: 65.62%] [G loss: 0.583181]\n",
      "epoch:27 step:25594 [D loss: 0.509800, acc.: 75.78%] [G loss: 0.697417]\n",
      "epoch:27 step:25595 [D loss: 0.524900, acc.: 71.88%] [G loss: 0.591914]\n",
      "epoch:27 step:25596 [D loss: 0.542399, acc.: 71.09%] [G loss: 0.557424]\n",
      "epoch:27 step:25597 [D loss: 0.460150, acc.: 82.81%] [G loss: 0.677495]\n",
      "epoch:27 step:25598 [D loss: 0.500734, acc.: 76.56%] [G loss: 0.701895]\n",
      "epoch:27 step:25599 [D loss: 0.464584, acc.: 79.69%] [G loss: 0.845081]\n",
      "epoch:27 step:25600 [D loss: 0.607561, acc.: 67.97%] [G loss: 0.616854]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.729774\n",
      "FID: 50.396484\n",
      "0 = 12.928834489631658\n",
      "1 = 0.09254378957659329\n",
      "2 = 0.8561000227928162\n",
      "3 = 0.8149999976158142\n",
      "4 = 0.8971999883651733\n",
      "5 = 0.8879930377006531\n",
      "6 = 0.8149999976158142\n",
      "7 = 8.47356527115105\n",
      "8 = 0.15553233317710238\n",
      "9 = 0.6942999958992004\n",
      "10 = 0.6909999847412109\n",
      "11 = 0.6976000070571899\n",
      "12 = 0.6955909132957458\n",
      "13 = 0.6909999847412109\n",
      "14 = 6.729802131652832\n",
      "15 = 6.966597557067871\n",
      "16 = 0.39290595054626465\n",
      "17 = 6.72977352142334\n",
      "18 = 50.396484375\n",
      "epoch:27 step:25601 [D loss: 0.503052, acc.: 71.88%] [G loss: 0.777395]\n",
      "epoch:27 step:25602 [D loss: 0.557623, acc.: 71.09%] [G loss: 0.706511]\n",
      "epoch:27 step:25603 [D loss: 0.435729, acc.: 83.59%] [G loss: 0.909957]\n",
      "epoch:27 step:25604 [D loss: 0.511979, acc.: 72.66%] [G loss: 0.785967]\n",
      "epoch:27 step:25605 [D loss: 0.473493, acc.: 77.34%] [G loss: 0.735751]\n",
      "epoch:27 step:25606 [D loss: 0.504874, acc.: 74.22%] [G loss: 0.898631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25607 [D loss: 0.603631, acc.: 64.06%] [G loss: 0.832039]\n",
      "epoch:27 step:25608 [D loss: 0.496476, acc.: 71.88%] [G loss: 0.967635]\n",
      "epoch:27 step:25609 [D loss: 0.521328, acc.: 72.66%] [G loss: 0.946180]\n",
      "epoch:27 step:25610 [D loss: 0.480235, acc.: 73.44%] [G loss: 0.629710]\n",
      "epoch:27 step:25611 [D loss: 0.477043, acc.: 78.12%] [G loss: 0.858126]\n",
      "epoch:27 step:25612 [D loss: 0.529775, acc.: 71.09%] [G loss: 0.807347]\n",
      "epoch:27 step:25613 [D loss: 0.473028, acc.: 76.56%] [G loss: 0.953543]\n",
      "epoch:27 step:25614 [D loss: 0.457999, acc.: 79.69%] [G loss: 0.863594]\n",
      "epoch:27 step:25615 [D loss: 0.665312, acc.: 65.62%] [G loss: 0.627060]\n",
      "epoch:27 step:25616 [D loss: 0.552023, acc.: 72.66%] [G loss: 0.741304]\n",
      "epoch:27 step:25617 [D loss: 0.516784, acc.: 71.88%] [G loss: 0.708960]\n",
      "epoch:27 step:25618 [D loss: 0.569625, acc.: 67.97%] [G loss: 0.709798]\n",
      "epoch:27 step:25619 [D loss: 0.547301, acc.: 70.31%] [G loss: 0.666062]\n",
      "epoch:27 step:25620 [D loss: 0.539645, acc.: 72.66%] [G loss: 0.743496]\n",
      "epoch:27 step:25621 [D loss: 0.530316, acc.: 75.78%] [G loss: 0.697900]\n",
      "epoch:27 step:25622 [D loss: 0.576054, acc.: 68.75%] [G loss: 0.815587]\n",
      "epoch:27 step:25623 [D loss: 0.577557, acc.: 68.75%] [G loss: 0.600363]\n",
      "epoch:27 step:25624 [D loss: 0.529040, acc.: 75.00%] [G loss: 0.635987]\n",
      "epoch:27 step:25625 [D loss: 0.449566, acc.: 75.78%] [G loss: 0.840627]\n",
      "epoch:27 step:25626 [D loss: 0.557380, acc.: 71.88%] [G loss: 0.820533]\n",
      "epoch:27 step:25627 [D loss: 0.480323, acc.: 78.91%] [G loss: 0.779228]\n",
      "epoch:27 step:25628 [D loss: 0.544822, acc.: 72.66%] [G loss: 0.769357]\n",
      "epoch:27 step:25629 [D loss: 0.563104, acc.: 71.09%] [G loss: 0.684540]\n",
      "epoch:27 step:25630 [D loss: 0.588523, acc.: 75.00%] [G loss: 0.671376]\n",
      "epoch:27 step:25631 [D loss: 0.511395, acc.: 71.88%] [G loss: 0.499226]\n",
      "epoch:27 step:25632 [D loss: 0.473879, acc.: 78.91%] [G loss: 0.605317]\n",
      "epoch:27 step:25633 [D loss: 0.478141, acc.: 75.00%] [G loss: 0.663351]\n",
      "epoch:27 step:25634 [D loss: 0.491618, acc.: 74.22%] [G loss: 0.758369]\n",
      "epoch:27 step:25635 [D loss: 0.508903, acc.: 74.22%] [G loss: 0.720234]\n",
      "epoch:27 step:25636 [D loss: 0.500162, acc.: 78.91%] [G loss: 0.716325]\n",
      "epoch:27 step:25637 [D loss: 0.550276, acc.: 66.41%] [G loss: 0.879637]\n",
      "epoch:27 step:25638 [D loss: 0.575122, acc.: 69.53%] [G loss: 0.713171]\n",
      "epoch:27 step:25639 [D loss: 0.474350, acc.: 77.34%] [G loss: 0.841887]\n",
      "epoch:27 step:25640 [D loss: 0.671589, acc.: 64.84%] [G loss: 0.686330]\n",
      "epoch:27 step:25641 [D loss: 0.600010, acc.: 60.94%] [G loss: 0.650410]\n",
      "epoch:27 step:25642 [D loss: 0.475856, acc.: 76.56%] [G loss: 0.828321]\n",
      "epoch:27 step:25643 [D loss: 0.501007, acc.: 75.78%] [G loss: 0.766726]\n",
      "epoch:27 step:25644 [D loss: 0.569934, acc.: 67.97%] [G loss: 0.930442]\n",
      "epoch:27 step:25645 [D loss: 0.525913, acc.: 75.78%] [G loss: 0.925346]\n",
      "epoch:27 step:25646 [D loss: 0.415698, acc.: 83.59%] [G loss: 1.397820]\n",
      "epoch:27 step:25647 [D loss: 0.564446, acc.: 74.22%] [G loss: 0.760287]\n",
      "epoch:27 step:25648 [D loss: 0.707112, acc.: 61.72%] [G loss: 0.482149]\n",
      "epoch:27 step:25649 [D loss: 0.486093, acc.: 72.66%] [G loss: 0.600659]\n",
      "epoch:27 step:25650 [D loss: 0.549241, acc.: 70.31%] [G loss: 0.585947]\n",
      "epoch:27 step:25651 [D loss: 0.555286, acc.: 71.88%] [G loss: 0.603742]\n",
      "epoch:27 step:25652 [D loss: 0.533466, acc.: 70.31%] [G loss: 0.769987]\n",
      "epoch:27 step:25653 [D loss: 0.393200, acc.: 82.81%] [G loss: 0.882654]\n",
      "epoch:27 step:25654 [D loss: 0.542446, acc.: 74.22%] [G loss: 0.894090]\n",
      "epoch:27 step:25655 [D loss: 0.544415, acc.: 74.22%] [G loss: 0.876397]\n",
      "epoch:27 step:25656 [D loss: 0.419157, acc.: 78.91%] [G loss: 0.931458]\n",
      "epoch:27 step:25657 [D loss: 0.476133, acc.: 75.00%] [G loss: 0.952876]\n",
      "epoch:27 step:25658 [D loss: 0.520175, acc.: 71.88%] [G loss: 0.899553]\n",
      "epoch:27 step:25659 [D loss: 0.481127, acc.: 76.56%] [G loss: 0.777327]\n",
      "epoch:27 step:25660 [D loss: 0.516322, acc.: 74.22%] [G loss: 0.889417]\n",
      "epoch:27 step:25661 [D loss: 0.519653, acc.: 71.88%] [G loss: 0.711153]\n",
      "epoch:27 step:25662 [D loss: 0.572806, acc.: 66.41%] [G loss: 0.703227]\n",
      "epoch:27 step:25663 [D loss: 0.505383, acc.: 76.56%] [G loss: 0.618074]\n",
      "epoch:27 step:25664 [D loss: 0.521546, acc.: 75.78%] [G loss: 0.738039]\n",
      "epoch:27 step:25665 [D loss: 0.517062, acc.: 73.44%] [G loss: 0.794743]\n",
      "epoch:27 step:25666 [D loss: 0.595780, acc.: 66.41%] [G loss: 0.785256]\n",
      "epoch:27 step:25667 [D loss: 0.559357, acc.: 71.88%] [G loss: 0.698200]\n",
      "epoch:27 step:25668 [D loss: 0.553853, acc.: 71.88%] [G loss: 0.619068]\n",
      "epoch:27 step:25669 [D loss: 0.540108, acc.: 74.22%] [G loss: 0.753422]\n",
      "epoch:27 step:25670 [D loss: 0.514597, acc.: 74.22%] [G loss: 0.851913]\n",
      "epoch:27 step:25671 [D loss: 0.514674, acc.: 74.22%] [G loss: 0.782426]\n",
      "epoch:27 step:25672 [D loss: 0.491779, acc.: 76.56%] [G loss: 0.711242]\n",
      "epoch:27 step:25673 [D loss: 0.521367, acc.: 71.88%] [G loss: 0.819754]\n",
      "epoch:27 step:25674 [D loss: 0.544292, acc.: 70.31%] [G loss: 0.705513]\n",
      "epoch:27 step:25675 [D loss: 0.688493, acc.: 63.28%] [G loss: 0.570531]\n",
      "epoch:27 step:25676 [D loss: 0.579644, acc.: 71.88%] [G loss: 0.569530]\n",
      "epoch:27 step:25677 [D loss: 0.512475, acc.: 75.00%] [G loss: 0.495712]\n",
      "epoch:27 step:25678 [D loss: 0.545640, acc.: 70.31%] [G loss: 0.570753]\n",
      "epoch:27 step:25679 [D loss: 0.537476, acc.: 75.00%] [G loss: 0.621955]\n",
      "epoch:27 step:25680 [D loss: 0.443181, acc.: 80.47%] [G loss: 0.643420]\n",
      "epoch:27 step:25681 [D loss: 0.484279, acc.: 79.69%] [G loss: 0.630618]\n",
      "epoch:27 step:25682 [D loss: 0.519990, acc.: 67.97%] [G loss: 0.709368]\n",
      "epoch:27 step:25683 [D loss: 0.519763, acc.: 72.66%] [G loss: 0.797532]\n",
      "epoch:27 step:25684 [D loss: 0.457882, acc.: 78.91%] [G loss: 0.912023]\n",
      "epoch:27 step:25685 [D loss: 0.609936, acc.: 71.88%] [G loss: 0.710002]\n",
      "epoch:27 step:25686 [D loss: 0.527991, acc.: 70.31%] [G loss: 0.653572]\n",
      "epoch:27 step:25687 [D loss: 0.560179, acc.: 69.53%] [G loss: 0.529786]\n",
      "epoch:27 step:25688 [D loss: 0.542111, acc.: 67.19%] [G loss: 0.632880]\n",
      "epoch:27 step:25689 [D loss: 0.592550, acc.: 66.41%] [G loss: 0.663629]\n",
      "epoch:27 step:25690 [D loss: 0.537233, acc.: 71.88%] [G loss: 0.566464]\n",
      "epoch:27 step:25691 [D loss: 0.500242, acc.: 71.09%] [G loss: 0.678744]\n",
      "epoch:27 step:25692 [D loss: 0.605549, acc.: 64.06%] [G loss: 0.690219]\n",
      "epoch:27 step:25693 [D loss: 0.667594, acc.: 57.81%] [G loss: 0.481753]\n",
      "epoch:27 step:25694 [D loss: 0.578200, acc.: 70.31%] [G loss: 0.581156]\n",
      "epoch:27 step:25695 [D loss: 0.576885, acc.: 61.72%] [G loss: 0.793548]\n",
      "epoch:27 step:25696 [D loss: 0.493744, acc.: 76.56%] [G loss: 0.810412]\n",
      "epoch:27 step:25697 [D loss: 0.449473, acc.: 76.56%] [G loss: 0.940052]\n",
      "epoch:27 step:25698 [D loss: 0.490148, acc.: 77.34%] [G loss: 0.824379]\n",
      "epoch:27 step:25699 [D loss: 0.641965, acc.: 60.94%] [G loss: 0.745931]\n",
      "epoch:27 step:25700 [D loss: 0.641506, acc.: 56.25%] [G loss: 0.632616]\n",
      "epoch:27 step:25701 [D loss: 0.510705, acc.: 73.44%] [G loss: 0.748346]\n",
      "epoch:27 step:25702 [D loss: 0.508622, acc.: 67.97%] [G loss: 0.811436]\n",
      "epoch:27 step:25703 [D loss: 0.586870, acc.: 64.06%] [G loss: 0.689194]\n",
      "epoch:27 step:25704 [D loss: 0.552946, acc.: 70.31%] [G loss: 0.779402]\n",
      "epoch:27 step:25705 [D loss: 0.485304, acc.: 72.66%] [G loss: 0.951512]\n",
      "epoch:27 step:25706 [D loss: 0.570037, acc.: 67.97%] [G loss: 0.801202]\n",
      "epoch:27 step:25707 [D loss: 0.596968, acc.: 66.41%] [G loss: 0.824633]\n",
      "epoch:27 step:25708 [D loss: 0.538083, acc.: 73.44%] [G loss: 0.655447]\n",
      "epoch:27 step:25709 [D loss: 0.545061, acc.: 64.06%] [G loss: 0.733716]\n",
      "epoch:27 step:25710 [D loss: 0.564179, acc.: 67.19%] [G loss: 0.758580]\n",
      "epoch:27 step:25711 [D loss: 0.568870, acc.: 68.75%] [G loss: 0.552038]\n",
      "epoch:27 step:25712 [D loss: 0.537423, acc.: 68.75%] [G loss: 0.495986]\n",
      "epoch:27 step:25713 [D loss: 0.525269, acc.: 69.53%] [G loss: 0.695045]\n",
      "epoch:27 step:25714 [D loss: 0.512531, acc.: 75.00%] [G loss: 0.782279]\n",
      "epoch:27 step:25715 [D loss: 0.503340, acc.: 72.66%] [G loss: 0.741777]\n",
      "epoch:27 step:25716 [D loss: 0.509278, acc.: 74.22%] [G loss: 0.762287]\n",
      "epoch:27 step:25717 [D loss: 0.617506, acc.: 65.62%] [G loss: 0.802110]\n",
      "epoch:27 step:25718 [D loss: 0.550258, acc.: 71.09%] [G loss: 0.755316]\n",
      "epoch:27 step:25719 [D loss: 0.556911, acc.: 70.31%] [G loss: 0.728151]\n",
      "epoch:27 step:25720 [D loss: 0.618102, acc.: 63.28%] [G loss: 0.695922]\n",
      "epoch:27 step:25721 [D loss: 0.612548, acc.: 64.06%] [G loss: 0.533432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25722 [D loss: 0.584153, acc.: 64.84%] [G loss: 0.493631]\n",
      "epoch:27 step:25723 [D loss: 0.546042, acc.: 68.75%] [G loss: 0.663560]\n",
      "epoch:27 step:25724 [D loss: 0.527583, acc.: 69.53%] [G loss: 0.587144]\n",
      "epoch:27 step:25725 [D loss: 0.457169, acc.: 75.00%] [G loss: 0.825342]\n",
      "epoch:27 step:25726 [D loss: 0.450687, acc.: 80.47%] [G loss: 0.987983]\n",
      "epoch:27 step:25727 [D loss: 0.608079, acc.: 64.84%] [G loss: 0.703047]\n",
      "epoch:27 step:25728 [D loss: 0.449813, acc.: 82.03%] [G loss: 0.892160]\n",
      "epoch:27 step:25729 [D loss: 0.499481, acc.: 76.56%] [G loss: 0.902426]\n",
      "epoch:27 step:25730 [D loss: 0.531807, acc.: 68.75%] [G loss: 0.755165]\n",
      "epoch:27 step:25731 [D loss: 0.576050, acc.: 69.53%] [G loss: 0.752587]\n",
      "epoch:27 step:25732 [D loss: 0.552910, acc.: 67.19%] [G loss: 0.670301]\n",
      "epoch:27 step:25733 [D loss: 0.545502, acc.: 71.88%] [G loss: 0.847919]\n",
      "epoch:27 step:25734 [D loss: 0.495652, acc.: 74.22%] [G loss: 0.931803]\n",
      "epoch:27 step:25735 [D loss: 0.473688, acc.: 72.66%] [G loss: 0.830813]\n",
      "epoch:27 step:25736 [D loss: 0.684973, acc.: 59.38%] [G loss: 0.663179]\n",
      "epoch:27 step:25737 [D loss: 0.584666, acc.: 62.50%] [G loss: 0.609713]\n",
      "epoch:27 step:25738 [D loss: 0.494960, acc.: 75.78%] [G loss: 0.852939]\n",
      "epoch:27 step:25739 [D loss: 0.482836, acc.: 76.56%] [G loss: 0.824173]\n",
      "epoch:27 step:25740 [D loss: 0.542402, acc.: 70.31%] [G loss: 0.689019]\n",
      "epoch:27 step:25741 [D loss: 0.525125, acc.: 67.97%] [G loss: 0.828845]\n",
      "epoch:27 step:25742 [D loss: 0.488500, acc.: 74.22%] [G loss: 0.856935]\n",
      "epoch:27 step:25743 [D loss: 0.504631, acc.: 71.88%] [G loss: 0.677524]\n",
      "epoch:27 step:25744 [D loss: 0.563320, acc.: 68.75%] [G loss: 0.820040]\n",
      "epoch:27 step:25745 [D loss: 0.482964, acc.: 75.00%] [G loss: 0.987429]\n",
      "epoch:27 step:25746 [D loss: 0.537528, acc.: 69.53%] [G loss: 0.723733]\n",
      "epoch:27 step:25747 [D loss: 0.497713, acc.: 77.34%] [G loss: 0.779488]\n",
      "epoch:27 step:25748 [D loss: 0.507795, acc.: 71.09%] [G loss: 0.856806]\n",
      "epoch:27 step:25749 [D loss: 0.559354, acc.: 71.09%] [G loss: 0.679586]\n",
      "epoch:27 step:25750 [D loss: 0.462933, acc.: 79.69%] [G loss: 0.844927]\n",
      "epoch:27 step:25751 [D loss: 0.470806, acc.: 76.56%] [G loss: 0.849110]\n",
      "epoch:27 step:25752 [D loss: 0.455932, acc.: 82.03%] [G loss: 0.717361]\n",
      "epoch:27 step:25753 [D loss: 0.583767, acc.: 67.19%] [G loss: 0.826213]\n",
      "epoch:27 step:25754 [D loss: 0.565566, acc.: 65.62%] [G loss: 0.680731]\n",
      "epoch:27 step:25755 [D loss: 0.593247, acc.: 64.06%] [G loss: 0.761200]\n",
      "epoch:27 step:25756 [D loss: 0.471634, acc.: 77.34%] [G loss: 0.783311]\n",
      "epoch:27 step:25757 [D loss: 0.598189, acc.: 74.22%] [G loss: 0.645444]\n",
      "epoch:27 step:25758 [D loss: 0.545801, acc.: 64.84%] [G loss: 0.714179]\n",
      "epoch:27 step:25759 [D loss: 0.495581, acc.: 75.78%] [G loss: 0.763278]\n",
      "epoch:27 step:25760 [D loss: 0.506193, acc.: 72.66%] [G loss: 0.869083]\n",
      "epoch:27 step:25761 [D loss: 0.597434, acc.: 65.62%] [G loss: 0.596941]\n",
      "epoch:27 step:25762 [D loss: 0.596620, acc.: 64.06%] [G loss: 0.666068]\n",
      "epoch:27 step:25763 [D loss: 0.512468, acc.: 75.78%] [G loss: 0.547908]\n",
      "epoch:27 step:25764 [D loss: 0.611388, acc.: 63.28%] [G loss: 0.583207]\n",
      "epoch:27 step:25765 [D loss: 0.576421, acc.: 63.28%] [G loss: 0.567490]\n",
      "epoch:27 step:25766 [D loss: 0.542258, acc.: 69.53%] [G loss: 0.729247]\n",
      "epoch:27 step:25767 [D loss: 0.577493, acc.: 67.97%] [G loss: 0.747805]\n",
      "epoch:27 step:25768 [D loss: 0.503308, acc.: 71.09%] [G loss: 0.635983]\n",
      "epoch:27 step:25769 [D loss: 0.523764, acc.: 71.88%] [G loss: 0.686642]\n",
      "epoch:27 step:25770 [D loss: 0.437925, acc.: 82.81%] [G loss: 0.682793]\n",
      "epoch:27 step:25771 [D loss: 0.439500, acc.: 79.69%] [G loss: 0.837013]\n",
      "epoch:27 step:25772 [D loss: 0.689057, acc.: 57.81%] [G loss: 0.641567]\n",
      "epoch:27 step:25773 [D loss: 0.515479, acc.: 72.66%] [G loss: 0.797292]\n",
      "epoch:27 step:25774 [D loss: 0.475310, acc.: 77.34%] [G loss: 0.820545]\n",
      "epoch:27 step:25775 [D loss: 0.589806, acc.: 71.88%] [G loss: 0.982467]\n",
      "epoch:27 step:25776 [D loss: 0.640354, acc.: 65.62%] [G loss: 0.549355]\n",
      "epoch:27 step:25777 [D loss: 0.614049, acc.: 66.41%] [G loss: 0.665899]\n",
      "epoch:27 step:25778 [D loss: 0.495217, acc.: 75.78%] [G loss: 0.671023]\n",
      "epoch:27 step:25779 [D loss: 0.573726, acc.: 71.09%] [G loss: 0.671613]\n",
      "epoch:27 step:25780 [D loss: 0.486774, acc.: 82.81%] [G loss: 0.996880]\n",
      "epoch:27 step:25781 [D loss: 0.612064, acc.: 69.53%] [G loss: 0.684283]\n",
      "epoch:27 step:25782 [D loss: 0.571112, acc.: 70.31%] [G loss: 0.629140]\n",
      "epoch:27 step:25783 [D loss: 0.461760, acc.: 83.59%] [G loss: 0.775761]\n",
      "epoch:27 step:25784 [D loss: 0.547625, acc.: 75.78%] [G loss: 0.822738]\n",
      "epoch:27 step:25785 [D loss: 0.532846, acc.: 67.97%] [G loss: 0.756126]\n",
      "epoch:27 step:25786 [D loss: 0.575848, acc.: 67.19%] [G loss: 0.618139]\n",
      "epoch:27 step:25787 [D loss: 0.478742, acc.: 75.78%] [G loss: 0.777336]\n",
      "epoch:27 step:25788 [D loss: 0.496081, acc.: 77.34%] [G loss: 0.930819]\n",
      "epoch:27 step:25789 [D loss: 0.535964, acc.: 71.09%] [G loss: 0.832869]\n",
      "epoch:27 step:25790 [D loss: 0.521577, acc.: 74.22%] [G loss: 0.846715]\n",
      "epoch:27 step:25791 [D loss: 0.583535, acc.: 69.53%] [G loss: 0.729040]\n",
      "epoch:27 step:25792 [D loss: 0.552244, acc.: 70.31%] [G loss: 0.575695]\n",
      "epoch:27 step:25793 [D loss: 0.599094, acc.: 69.53%] [G loss: 0.691906]\n",
      "epoch:27 step:25794 [D loss: 0.489931, acc.: 78.91%] [G loss: 0.817533]\n",
      "epoch:27 step:25795 [D loss: 0.519716, acc.: 73.44%] [G loss: 0.744338]\n",
      "epoch:27 step:25796 [D loss: 0.561265, acc.: 67.97%] [G loss: 0.649792]\n",
      "epoch:27 step:25797 [D loss: 0.532439, acc.: 70.31%] [G loss: 0.713687]\n",
      "epoch:27 step:25798 [D loss: 0.534486, acc.: 74.22%] [G loss: 0.796680]\n",
      "epoch:27 step:25799 [D loss: 0.523582, acc.: 78.12%] [G loss: 0.868815]\n",
      "epoch:27 step:25800 [D loss: 0.569087, acc.: 69.53%] [G loss: 0.754622]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.671196\n",
      "FID: 44.176964\n",
      "0 = 12.782850851345056\n",
      "1 = 0.07973724009639986\n",
      "2 = 0.8539000153541565\n",
      "3 = 0.8037999868392944\n",
      "4 = 0.9039999842643738\n",
      "5 = 0.8933096528053284\n",
      "6 = 0.8037999868392944\n",
      "7 = 8.009907536315914\n",
      "8 = 0.13904832060457134\n",
      "9 = 0.6973000168800354\n",
      "10 = 0.6808000206947327\n",
      "11 = 0.7138000130653381\n",
      "12 = 0.7040330767631531\n",
      "13 = 0.6808000206947327\n",
      "14 = 6.6712188720703125\n",
      "15 = 6.8376288414001465\n",
      "16 = 0.3963012993335724\n",
      "17 = 6.6711955070495605\n",
      "18 = 44.176963806152344\n",
      "epoch:27 step:25801 [D loss: 0.661868, acc.: 60.94%] [G loss: 0.539446]\n",
      "epoch:27 step:25802 [D loss: 0.482731, acc.: 77.34%] [G loss: 0.571981]\n",
      "epoch:27 step:25803 [D loss: 0.491853, acc.: 77.34%] [G loss: 0.768317]\n",
      "epoch:27 step:25804 [D loss: 0.514108, acc.: 75.78%] [G loss: 0.738078]\n",
      "epoch:27 step:25805 [D loss: 0.457547, acc.: 79.69%] [G loss: 0.852564]\n",
      "epoch:27 step:25806 [D loss: 0.454800, acc.: 78.12%] [G loss: 0.929330]\n",
      "epoch:27 step:25807 [D loss: 0.428006, acc.: 82.03%] [G loss: 0.974668]\n",
      "epoch:27 step:25808 [D loss: 0.511778, acc.: 75.78%] [G loss: 0.975925]\n",
      "epoch:27 step:25809 [D loss: 0.639523, acc.: 59.38%] [G loss: 0.818922]\n",
      "epoch:27 step:25810 [D loss: 0.641700, acc.: 61.72%] [G loss: 0.716014]\n",
      "epoch:27 step:25811 [D loss: 0.614527, acc.: 64.84%] [G loss: 0.665052]\n",
      "epoch:27 step:25812 [D loss: 0.509879, acc.: 77.34%] [G loss: 0.645080]\n",
      "epoch:27 step:25813 [D loss: 0.549490, acc.: 67.97%] [G loss: 0.711179]\n",
      "epoch:27 step:25814 [D loss: 0.536970, acc.: 70.31%] [G loss: 0.742814]\n",
      "epoch:27 step:25815 [D loss: 0.474442, acc.: 78.12%] [G loss: 0.797163]\n",
      "epoch:27 step:25816 [D loss: 0.492527, acc.: 76.56%] [G loss: 0.931980]\n",
      "epoch:27 step:25817 [D loss: 0.542190, acc.: 71.09%] [G loss: 0.727375]\n",
      "epoch:27 step:25818 [D loss: 0.552776, acc.: 75.78%] [G loss: 1.041103]\n",
      "epoch:27 step:25819 [D loss: 0.514604, acc.: 78.91%] [G loss: 0.807697]\n",
      "epoch:27 step:25820 [D loss: 0.473060, acc.: 79.69%] [G loss: 0.837874]\n",
      "epoch:27 step:25821 [D loss: 0.489094, acc.: 77.34%] [G loss: 0.804451]\n",
      "epoch:27 step:25822 [D loss: 0.489504, acc.: 75.78%] [G loss: 0.842281]\n",
      "epoch:27 step:25823 [D loss: 0.521360, acc.: 72.66%] [G loss: 0.911020]\n",
      "epoch:27 step:25824 [D loss: 0.642455, acc.: 62.50%] [G loss: 0.619870]\n",
      "epoch:27 step:25825 [D loss: 0.548493, acc.: 70.31%] [G loss: 0.716165]\n",
      "epoch:27 step:25826 [D loss: 0.584322, acc.: 69.53%] [G loss: 0.702236]\n",
      "epoch:27 step:25827 [D loss: 0.698534, acc.: 56.25%] [G loss: 0.524585]\n",
      "epoch:27 step:25828 [D loss: 0.599625, acc.: 66.41%] [G loss: 0.745475]\n",
      "epoch:27 step:25829 [D loss: 0.556923, acc.: 67.97%] [G loss: 0.715844]\n",
      "epoch:27 step:25830 [D loss: 0.539757, acc.: 72.66%] [G loss: 0.699782]\n",
      "epoch:27 step:25831 [D loss: 0.562820, acc.: 69.53%] [G loss: 0.792291]\n",
      "epoch:27 step:25832 [D loss: 0.434629, acc.: 78.91%] [G loss: 0.728817]\n",
      "epoch:27 step:25833 [D loss: 0.471440, acc.: 75.78%] [G loss: 0.792910]\n",
      "epoch:27 step:25834 [D loss: 0.695456, acc.: 54.69%] [G loss: 0.627371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25835 [D loss: 0.467776, acc.: 77.34%] [G loss: 0.631807]\n",
      "epoch:27 step:25836 [D loss: 0.551180, acc.: 73.44%] [G loss: 0.748857]\n",
      "epoch:27 step:25837 [D loss: 0.545309, acc.: 70.31%] [G loss: 0.662483]\n",
      "epoch:27 step:25838 [D loss: 0.543413, acc.: 69.53%] [G loss: 0.663934]\n",
      "epoch:27 step:25839 [D loss: 0.542102, acc.: 71.88%] [G loss: 0.589491]\n",
      "epoch:27 step:25840 [D loss: 0.536830, acc.: 67.19%] [G loss: 0.761916]\n",
      "epoch:27 step:25841 [D loss: 0.588819, acc.: 68.75%] [G loss: 0.668115]\n",
      "epoch:27 step:25842 [D loss: 0.524718, acc.: 71.88%] [G loss: 0.707627]\n",
      "epoch:27 step:25843 [D loss: 0.527810, acc.: 71.09%] [G loss: 0.752474]\n",
      "epoch:27 step:25844 [D loss: 0.519697, acc.: 72.66%] [G loss: 0.558959]\n",
      "epoch:27 step:25845 [D loss: 0.520333, acc.: 76.56%] [G loss: 0.777704]\n",
      "epoch:27 step:25846 [D loss: 0.559005, acc.: 67.19%] [G loss: 0.771706]\n",
      "epoch:27 step:25847 [D loss: 0.544418, acc.: 71.09%] [G loss: 0.764690]\n",
      "epoch:27 step:25848 [D loss: 0.493598, acc.: 75.00%] [G loss: 0.732080]\n",
      "epoch:27 step:25849 [D loss: 0.565189, acc.: 65.62%] [G loss: 0.443300]\n",
      "epoch:27 step:25850 [D loss: 0.505182, acc.: 73.44%] [G loss: 0.784399]\n",
      "epoch:27 step:25851 [D loss: 0.517974, acc.: 71.09%] [G loss: 0.748427]\n",
      "epoch:27 step:25852 [D loss: 0.587931, acc.: 66.41%] [G loss: 0.821757]\n",
      "epoch:27 step:25853 [D loss: 0.470866, acc.: 75.00%] [G loss: 0.754738]\n",
      "epoch:27 step:25854 [D loss: 0.537844, acc.: 74.22%] [G loss: 0.751275]\n",
      "epoch:27 step:25855 [D loss: 0.569619, acc.: 67.97%] [G loss: 0.728817]\n",
      "epoch:27 step:25856 [D loss: 0.479148, acc.: 75.00%] [G loss: 0.544360]\n",
      "epoch:27 step:25857 [D loss: 0.511823, acc.: 74.22%] [G loss: 0.757502]\n",
      "epoch:27 step:25858 [D loss: 0.541179, acc.: 75.00%] [G loss: 0.835440]\n",
      "epoch:27 step:25859 [D loss: 0.546384, acc.: 70.31%] [G loss: 0.765630]\n",
      "epoch:27 step:25860 [D loss: 0.581238, acc.: 64.84%] [G loss: 0.760798]\n",
      "epoch:27 step:25861 [D loss: 0.576586, acc.: 66.41%] [G loss: 0.697877]\n",
      "epoch:27 step:25862 [D loss: 0.572048, acc.: 65.62%] [G loss: 0.745840]\n",
      "epoch:27 step:25863 [D loss: 0.446064, acc.: 78.12%] [G loss: 0.909866]\n",
      "epoch:27 step:25864 [D loss: 0.617221, acc.: 67.19%] [G loss: 0.936160]\n",
      "epoch:27 step:25865 [D loss: 0.747037, acc.: 57.81%] [G loss: 0.549607]\n",
      "epoch:27 step:25866 [D loss: 0.519687, acc.: 69.53%] [G loss: 0.869581]\n",
      "epoch:27 step:25867 [D loss: 0.604630, acc.: 62.50%] [G loss: 0.558695]\n",
      "epoch:27 step:25868 [D loss: 0.579232, acc.: 65.62%] [G loss: 0.598224]\n",
      "epoch:27 step:25869 [D loss: 0.490539, acc.: 77.34%] [G loss: 0.605150]\n",
      "epoch:27 step:25870 [D loss: 0.512421, acc.: 73.44%] [G loss: 0.714797]\n",
      "epoch:27 step:25871 [D loss: 0.562691, acc.: 66.41%] [G loss: 0.703964]\n",
      "epoch:27 step:25872 [D loss: 0.513165, acc.: 71.88%] [G loss: 0.708406]\n",
      "epoch:27 step:25873 [D loss: 0.494384, acc.: 72.66%] [G loss: 0.830638]\n",
      "epoch:27 step:25874 [D loss: 0.506047, acc.: 75.78%] [G loss: 0.762995]\n",
      "epoch:27 step:25875 [D loss: 0.631199, acc.: 64.06%] [G loss: 0.738977]\n",
      "epoch:27 step:25876 [D loss: 0.525110, acc.: 73.44%] [G loss: 0.600038]\n",
      "epoch:27 step:25877 [D loss: 0.561165, acc.: 71.09%] [G loss: 0.684544]\n",
      "epoch:27 step:25878 [D loss: 0.542290, acc.: 68.75%] [G loss: 0.824362]\n",
      "epoch:27 step:25879 [D loss: 0.536254, acc.: 74.22%] [G loss: 0.586248]\n",
      "epoch:27 step:25880 [D loss: 0.550147, acc.: 71.88%] [G loss: 0.633220]\n",
      "epoch:27 step:25881 [D loss: 0.497491, acc.: 76.56%] [G loss: 0.947500]\n",
      "epoch:27 step:25882 [D loss: 0.570233, acc.: 71.88%] [G loss: 0.912122]\n",
      "epoch:27 step:25883 [D loss: 0.597024, acc.: 66.41%] [G loss: 0.591752]\n",
      "epoch:27 step:25884 [D loss: 0.606334, acc.: 64.06%] [G loss: 0.564152]\n",
      "epoch:27 step:25885 [D loss: 0.550139, acc.: 73.44%] [G loss: 0.672097]\n",
      "epoch:27 step:25886 [D loss: 0.515935, acc.: 75.00%] [G loss: 0.513675]\n",
      "epoch:27 step:25887 [D loss: 0.520197, acc.: 71.09%] [G loss: 0.696357]\n",
      "epoch:27 step:25888 [D loss: 0.464968, acc.: 74.22%] [G loss: 0.694809]\n",
      "epoch:27 step:25889 [D loss: 0.512695, acc.: 74.22%] [G loss: 0.749757]\n",
      "epoch:27 step:25890 [D loss: 0.579808, acc.: 66.41%] [G loss: 0.494130]\n",
      "epoch:27 step:25891 [D loss: 0.483812, acc.: 76.56%] [G loss: 0.718340]\n",
      "epoch:27 step:25892 [D loss: 0.503684, acc.: 75.00%] [G loss: 0.743932]\n",
      "epoch:27 step:25893 [D loss: 0.545704, acc.: 71.09%] [G loss: 0.711492]\n",
      "epoch:27 step:25894 [D loss: 0.533709, acc.: 69.53%] [G loss: 0.687679]\n",
      "epoch:27 step:25895 [D loss: 0.522238, acc.: 67.97%] [G loss: 0.702920]\n",
      "epoch:27 step:25896 [D loss: 0.543085, acc.: 69.53%] [G loss: 0.530573]\n",
      "epoch:27 step:25897 [D loss: 0.506911, acc.: 75.78%] [G loss: 0.660889]\n",
      "epoch:27 step:25898 [D loss: 0.548261, acc.: 73.44%] [G loss: 0.749084]\n",
      "epoch:27 step:25899 [D loss: 0.583799, acc.: 67.97%] [G loss: 0.680319]\n",
      "epoch:27 step:25900 [D loss: 0.552251, acc.: 70.31%] [G loss: 0.765762]\n",
      "epoch:27 step:25901 [D loss: 0.471654, acc.: 74.22%] [G loss: 0.666685]\n",
      "epoch:27 step:25902 [D loss: 0.524476, acc.: 74.22%] [G loss: 0.628403]\n",
      "epoch:27 step:25903 [D loss: 0.541198, acc.: 68.75%] [G loss: 0.883630]\n",
      "epoch:27 step:25904 [D loss: 0.499146, acc.: 78.12%] [G loss: 0.823519]\n",
      "epoch:27 step:25905 [D loss: 0.605351, acc.: 68.75%] [G loss: 0.843171]\n",
      "epoch:27 step:25906 [D loss: 0.498812, acc.: 75.00%] [G loss: 0.723924]\n",
      "epoch:27 step:25907 [D loss: 0.592722, acc.: 63.28%] [G loss: 0.726157]\n",
      "epoch:27 step:25908 [D loss: 0.510019, acc.: 68.75%] [G loss: 0.581727]\n",
      "epoch:27 step:25909 [D loss: 0.577825, acc.: 64.84%] [G loss: 0.844545]\n",
      "epoch:27 step:25910 [D loss: 0.522611, acc.: 75.00%] [G loss: 0.593886]\n",
      "epoch:27 step:25911 [D loss: 0.548800, acc.: 67.19%] [G loss: 0.552632]\n",
      "epoch:27 step:25912 [D loss: 0.473986, acc.: 75.00%] [G loss: 0.617913]\n",
      "epoch:27 step:25913 [D loss: 0.562619, acc.: 67.19%] [G loss: 0.619335]\n",
      "epoch:27 step:25914 [D loss: 0.560027, acc.: 69.53%] [G loss: 0.701397]\n",
      "epoch:27 step:25915 [D loss: 0.543209, acc.: 73.44%] [G loss: 0.700338]\n",
      "epoch:27 step:25916 [D loss: 0.498212, acc.: 73.44%] [G loss: 0.727873]\n",
      "epoch:27 step:25917 [D loss: 0.567631, acc.: 64.06%] [G loss: 0.883400]\n",
      "epoch:27 step:25918 [D loss: 0.509300, acc.: 76.56%] [G loss: 0.816397]\n",
      "epoch:27 step:25919 [D loss: 0.553414, acc.: 71.88%] [G loss: 0.812792]\n",
      "epoch:27 step:25920 [D loss: 0.525269, acc.: 74.22%] [G loss: 0.699750]\n",
      "epoch:27 step:25921 [D loss: 0.519398, acc.: 73.44%] [G loss: 0.734073]\n",
      "epoch:27 step:25922 [D loss: 0.462596, acc.: 77.34%] [G loss: 0.725056]\n",
      "epoch:27 step:25923 [D loss: 0.488190, acc.: 75.78%] [G loss: 0.902630]\n",
      "epoch:27 step:25924 [D loss: 0.542137, acc.: 70.31%] [G loss: 0.830181]\n",
      "epoch:27 step:25925 [D loss: 0.564265, acc.: 69.53%] [G loss: 0.712316]\n",
      "epoch:27 step:25926 [D loss: 0.554630, acc.: 66.41%] [G loss: 0.698977]\n",
      "epoch:27 step:25927 [D loss: 0.540832, acc.: 73.44%] [G loss: 0.692297]\n",
      "epoch:27 step:25928 [D loss: 0.567995, acc.: 65.62%] [G loss: 0.652155]\n",
      "epoch:27 step:25929 [D loss: 0.504720, acc.: 76.56%] [G loss: 0.784256]\n",
      "epoch:27 step:25930 [D loss: 0.549781, acc.: 69.53%] [G loss: 0.771783]\n",
      "epoch:27 step:25931 [D loss: 0.510148, acc.: 74.22%] [G loss: 0.880182]\n",
      "epoch:27 step:25932 [D loss: 0.528061, acc.: 73.44%] [G loss: 0.940750]\n",
      "epoch:27 step:25933 [D loss: 0.453618, acc.: 82.03%] [G loss: 0.837785]\n",
      "epoch:27 step:25934 [D loss: 0.499370, acc.: 74.22%] [G loss: 0.985179]\n",
      "epoch:27 step:25935 [D loss: 0.588470, acc.: 66.41%] [G loss: 0.589210]\n",
      "epoch:27 step:25936 [D loss: 0.520273, acc.: 75.78%] [G loss: 0.544058]\n",
      "epoch:27 step:25937 [D loss: 0.585560, acc.: 63.28%] [G loss: 0.686924]\n",
      "epoch:27 step:25938 [D loss: 0.484118, acc.: 74.22%] [G loss: 0.753469]\n",
      "epoch:27 step:25939 [D loss: 0.612746, acc.: 63.28%] [G loss: 0.778838]\n",
      "epoch:27 step:25940 [D loss: 0.488062, acc.: 78.12%] [G loss: 0.781024]\n",
      "epoch:27 step:25941 [D loss: 0.479216, acc.: 77.34%] [G loss: 0.705930]\n",
      "epoch:27 step:25942 [D loss: 0.528850, acc.: 70.31%] [G loss: 0.933347]\n",
      "epoch:27 step:25943 [D loss: 0.552406, acc.: 72.66%] [G loss: 0.759954]\n",
      "epoch:27 step:25944 [D loss: 0.521737, acc.: 71.88%] [G loss: 0.565359]\n",
      "epoch:27 step:25945 [D loss: 0.507786, acc.: 75.78%] [G loss: 0.569228]\n",
      "epoch:27 step:25946 [D loss: 0.425033, acc.: 84.38%] [G loss: 0.825943]\n",
      "epoch:27 step:25947 [D loss: 0.423081, acc.: 83.59%] [G loss: 1.181482]\n",
      "epoch:27 step:25948 [D loss: 0.541529, acc.: 75.00%] [G loss: 0.947718]\n",
      "epoch:27 step:25949 [D loss: 0.520395, acc.: 73.44%] [G loss: 1.014789]\n",
      "epoch:27 step:25950 [D loss: 0.517817, acc.: 71.09%] [G loss: 0.782074]\n",
      "epoch:27 step:25951 [D loss: 0.653038, acc.: 61.72%] [G loss: 0.598357]\n",
      "epoch:27 step:25952 [D loss: 0.554019, acc.: 73.44%] [G loss: 0.619848]\n",
      "epoch:27 step:25953 [D loss: 0.476710, acc.: 75.00%] [G loss: 0.867342]\n",
      "epoch:27 step:25954 [D loss: 0.579810, acc.: 68.75%] [G loss: 0.758515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25955 [D loss: 0.518860, acc.: 71.09%] [G loss: 0.702113]\n",
      "epoch:27 step:25956 [D loss: 0.534370, acc.: 71.88%] [G loss: 0.637714]\n",
      "epoch:27 step:25957 [D loss: 0.571671, acc.: 67.19%] [G loss: 0.684115]\n",
      "epoch:27 step:25958 [D loss: 0.538141, acc.: 71.09%] [G loss: 0.849823]\n",
      "epoch:27 step:25959 [D loss: 0.509504, acc.: 73.44%] [G loss: 0.797957]\n",
      "epoch:27 step:25960 [D loss: 0.496128, acc.: 75.78%] [G loss: 0.776496]\n",
      "epoch:27 step:25961 [D loss: 0.520759, acc.: 71.88%] [G loss: 0.749931]\n",
      "epoch:27 step:25962 [D loss: 0.569315, acc.: 64.06%] [G loss: 0.707460]\n",
      "epoch:27 step:25963 [D loss: 0.553558, acc.: 70.31%] [G loss: 0.671967]\n",
      "epoch:27 step:25964 [D loss: 0.515709, acc.: 73.44%] [G loss: 0.795318]\n",
      "epoch:27 step:25965 [D loss: 0.497782, acc.: 75.00%] [G loss: 0.709450]\n",
      "epoch:27 step:25966 [D loss: 0.541357, acc.: 70.31%] [G loss: 0.863027]\n",
      "epoch:27 step:25967 [D loss: 0.556114, acc.: 71.88%] [G loss: 0.736897]\n",
      "epoch:27 step:25968 [D loss: 0.525498, acc.: 73.44%] [G loss: 0.687652]\n",
      "epoch:27 step:25969 [D loss: 0.513393, acc.: 75.00%] [G loss: 0.524625]\n",
      "epoch:27 step:25970 [D loss: 0.500182, acc.: 77.34%] [G loss: 0.577339]\n",
      "epoch:27 step:25971 [D loss: 0.520732, acc.: 75.00%] [G loss: 0.760475]\n",
      "epoch:27 step:25972 [D loss: 0.609219, acc.: 66.41%] [G loss: 0.645057]\n",
      "epoch:27 step:25973 [D loss: 0.523073, acc.: 72.66%] [G loss: 0.788421]\n",
      "epoch:27 step:25974 [D loss: 0.597536, acc.: 66.41%] [G loss: 0.762902]\n",
      "epoch:27 step:25975 [D loss: 0.535611, acc.: 67.97%] [G loss: 0.852183]\n",
      "epoch:27 step:25976 [D loss: 0.528868, acc.: 72.66%] [G loss: 0.617420]\n",
      "epoch:27 step:25977 [D loss: 0.589342, acc.: 65.62%] [G loss: 0.571592]\n",
      "epoch:27 step:25978 [D loss: 0.479120, acc.: 75.78%] [G loss: 0.798812]\n",
      "epoch:27 step:25979 [D loss: 0.533410, acc.: 73.44%] [G loss: 0.732694]\n",
      "epoch:27 step:25980 [D loss: 0.448090, acc.: 81.25%] [G loss: 0.806632]\n",
      "epoch:27 step:25981 [D loss: 0.514735, acc.: 75.00%] [G loss: 0.768113]\n",
      "epoch:27 step:25982 [D loss: 0.578126, acc.: 66.41%] [G loss: 0.755973]\n",
      "epoch:27 step:25983 [D loss: 0.635779, acc.: 59.38%] [G loss: 0.559996]\n",
      "epoch:27 step:25984 [D loss: 0.548814, acc.: 69.53%] [G loss: 0.632232]\n",
      "epoch:27 step:25985 [D loss: 0.589732, acc.: 65.62%] [G loss: 0.603486]\n",
      "epoch:27 step:25986 [D loss: 0.555828, acc.: 67.19%] [G loss: 0.591574]\n",
      "epoch:27 step:25987 [D loss: 0.472772, acc.: 76.56%] [G loss: 0.853958]\n",
      "epoch:27 step:25988 [D loss: 0.591030, acc.: 61.72%] [G loss: 0.636593]\n",
      "epoch:27 step:25989 [D loss: 0.467734, acc.: 75.78%] [G loss: 0.757690]\n",
      "epoch:27 step:25990 [D loss: 0.515125, acc.: 75.78%] [G loss: 0.720921]\n",
      "epoch:27 step:25991 [D loss: 0.543561, acc.: 70.31%] [G loss: 0.790909]\n",
      "epoch:27 step:25992 [D loss: 0.470949, acc.: 79.69%] [G loss: 0.921257]\n",
      "epoch:27 step:25993 [D loss: 0.473530, acc.: 75.78%] [G loss: 1.000909]\n",
      "epoch:27 step:25994 [D loss: 0.570902, acc.: 71.88%] [G loss: 0.767329]\n",
      "epoch:27 step:25995 [D loss: 0.657474, acc.: 59.38%] [G loss: 0.526606]\n",
      "epoch:27 step:25996 [D loss: 0.565348, acc.: 67.97%] [G loss: 0.542339]\n",
      "epoch:27 step:25997 [D loss: 0.596310, acc.: 68.75%] [G loss: 0.536099]\n",
      "epoch:27 step:25998 [D loss: 0.555470, acc.: 63.28%] [G loss: 0.819482]\n",
      "epoch:27 step:25999 [D loss: 0.563151, acc.: 67.19%] [G loss: 0.580706]\n",
      "epoch:27 step:26000 [D loss: 0.485105, acc.: 75.00%] [G loss: 0.769531]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.668646\n",
      "FID: 52.674023\n",
      "0 = 12.979750876808195\n",
      "1 = 0.09596434773590114\n",
      "2 = 0.845300018787384\n",
      "3 = 0.8069999814033508\n",
      "4 = 0.8835999965667725\n",
      "5 = 0.873944103717804\n",
      "6 = 0.8069999814033508\n",
      "7 = 8.49973657178882\n",
      "8 = 0.16050904656699633\n",
      "9 = 0.6819999814033508\n",
      "10 = 0.6761999726295471\n",
      "11 = 0.6877999901771545\n",
      "12 = 0.6841359734535217\n",
      "13 = 0.6761999726295471\n",
      "14 = 6.668671131134033\n",
      "15 = 6.690677165985107\n",
      "16 = 0.4053560197353363\n",
      "17 = 6.668646335601807\n",
      "18 = 52.67402267456055\n",
      "epoch:27 step:26001 [D loss: 0.585621, acc.: 64.06%] [G loss: 0.649985]\n",
      "epoch:27 step:26002 [D loss: 0.647429, acc.: 63.28%] [G loss: 0.584583]\n",
      "epoch:27 step:26003 [D loss: 0.564104, acc.: 68.75%] [G loss: 0.734032]\n",
      "epoch:27 step:26004 [D loss: 0.478683, acc.: 77.34%] [G loss: 0.686931]\n",
      "epoch:27 step:26005 [D loss: 0.597164, acc.: 67.19%] [G loss: 0.539387]\n",
      "epoch:27 step:26006 [D loss: 0.528474, acc.: 68.75%] [G loss: 0.744330]\n",
      "epoch:27 step:26007 [D loss: 0.490025, acc.: 75.00%] [G loss: 0.784746]\n",
      "epoch:27 step:26008 [D loss: 0.526137, acc.: 64.84%] [G loss: 0.840797]\n",
      "epoch:27 step:26009 [D loss: 0.587345, acc.: 66.41%] [G loss: 0.687040]\n",
      "epoch:27 step:26010 [D loss: 0.550305, acc.: 73.44%] [G loss: 0.718557]\n",
      "epoch:27 step:26011 [D loss: 0.515916, acc.: 72.66%] [G loss: 0.778672]\n",
      "epoch:27 step:26012 [D loss: 0.621200, acc.: 60.94%] [G loss: 0.699758]\n",
      "epoch:27 step:26013 [D loss: 0.585923, acc.: 64.06%] [G loss: 0.695529]\n",
      "epoch:27 step:26014 [D loss: 0.513803, acc.: 74.22%] [G loss: 0.620280]\n",
      "epoch:27 step:26015 [D loss: 0.610567, acc.: 67.19%] [G loss: 0.596054]\n",
      "epoch:27 step:26016 [D loss: 0.560267, acc.: 71.09%] [G loss: 0.634951]\n",
      "epoch:27 step:26017 [D loss: 0.547541, acc.: 71.09%] [G loss: 0.628740]\n",
      "epoch:27 step:26018 [D loss: 0.467311, acc.: 76.56%] [G loss: 0.868506]\n",
      "epoch:27 step:26019 [D loss: 0.588697, acc.: 70.31%] [G loss: 0.699199]\n",
      "epoch:27 step:26020 [D loss: 0.575632, acc.: 71.09%] [G loss: 0.605706]\n",
      "epoch:27 step:26021 [D loss: 0.570015, acc.: 67.97%] [G loss: 0.686788]\n",
      "epoch:27 step:26022 [D loss: 0.600219, acc.: 67.97%] [G loss: 0.655564]\n",
      "epoch:27 step:26023 [D loss: 0.490709, acc.: 75.78%] [G loss: 0.666274]\n",
      "epoch:27 step:26024 [D loss: 0.488650, acc.: 75.78%] [G loss: 0.807424]\n",
      "epoch:27 step:26025 [D loss: 0.466445, acc.: 77.34%] [G loss: 0.666292]\n",
      "epoch:27 step:26026 [D loss: 0.558711, acc.: 69.53%] [G loss: 0.744579]\n",
      "epoch:27 step:26027 [D loss: 0.565343, acc.: 71.09%] [G loss: 0.443440]\n",
      "epoch:27 step:26028 [D loss: 0.544062, acc.: 73.44%] [G loss: 0.646891]\n",
      "epoch:27 step:26029 [D loss: 0.510631, acc.: 71.09%] [G loss: 0.723791]\n",
      "epoch:27 step:26030 [D loss: 0.557498, acc.: 68.75%] [G loss: 0.669454]\n",
      "epoch:27 step:26031 [D loss: 0.549274, acc.: 67.19%] [G loss: 0.681229]\n",
      "epoch:27 step:26032 [D loss: 0.514571, acc.: 78.91%] [G loss: 0.619378]\n",
      "epoch:27 step:26033 [D loss: 0.533734, acc.: 73.44%] [G loss: 0.729280]\n",
      "epoch:27 step:26034 [D loss: 0.545219, acc.: 67.97%] [G loss: 0.611029]\n",
      "epoch:27 step:26035 [D loss: 0.473993, acc.: 75.00%] [G loss: 0.671055]\n",
      "epoch:27 step:26036 [D loss: 0.540818, acc.: 68.75%] [G loss: 0.724911]\n",
      "epoch:27 step:26037 [D loss: 0.568451, acc.: 67.19%] [G loss: 0.528960]\n",
      "epoch:27 step:26038 [D loss: 0.558026, acc.: 68.75%] [G loss: 0.719298]\n",
      "epoch:27 step:26039 [D loss: 0.679829, acc.: 58.59%] [G loss: 0.468419]\n",
      "epoch:27 step:26040 [D loss: 0.500684, acc.: 72.66%] [G loss: 0.491701]\n",
      "epoch:27 step:26041 [D loss: 0.522305, acc.: 75.00%] [G loss: 0.734414]\n",
      "epoch:27 step:26042 [D loss: 0.483685, acc.: 74.22%] [G loss: 0.799963]\n",
      "epoch:27 step:26043 [D loss: 0.492406, acc.: 75.78%] [G loss: 0.638504]\n",
      "epoch:27 step:26044 [D loss: 0.531661, acc.: 67.97%] [G loss: 0.751192]\n",
      "epoch:27 step:26045 [D loss: 0.505530, acc.: 73.44%] [G loss: 0.910980]\n",
      "epoch:27 step:26046 [D loss: 0.425483, acc.: 82.81%] [G loss: 0.892851]\n",
      "epoch:27 step:26047 [D loss: 0.520793, acc.: 72.66%] [G loss: 0.887364]\n",
      "epoch:27 step:26048 [D loss: 0.500762, acc.: 71.88%] [G loss: 0.783147]\n",
      "epoch:27 step:26049 [D loss: 0.509119, acc.: 75.00%] [G loss: 0.629146]\n",
      "epoch:27 step:26050 [D loss: 0.513383, acc.: 72.66%] [G loss: 0.799170]\n",
      "epoch:27 step:26051 [D loss: 0.631239, acc.: 64.06%] [G loss: 0.784218]\n",
      "epoch:27 step:26052 [D loss: 0.505537, acc.: 75.78%] [G loss: 0.732677]\n",
      "epoch:27 step:26053 [D loss: 0.568412, acc.: 69.53%] [G loss: 0.632261]\n",
      "epoch:27 step:26054 [D loss: 0.502058, acc.: 70.31%] [G loss: 0.695535]\n",
      "epoch:27 step:26055 [D loss: 0.550901, acc.: 67.97%] [G loss: 0.730468]\n",
      "epoch:27 step:26056 [D loss: 0.622264, acc.: 64.06%] [G loss: 0.691891]\n",
      "epoch:27 step:26057 [D loss: 0.535984, acc.: 75.00%] [G loss: 0.678139]\n",
      "epoch:27 step:26058 [D loss: 0.569919, acc.: 67.97%] [G loss: 0.743084]\n",
      "epoch:27 step:26059 [D loss: 0.531836, acc.: 71.88%] [G loss: 0.655017]\n",
      "epoch:27 step:26060 [D loss: 0.524423, acc.: 75.00%] [G loss: 0.677684]\n",
      "epoch:27 step:26061 [D loss: 0.601967, acc.: 65.62%] [G loss: 0.526848]\n",
      "epoch:27 step:26062 [D loss: 0.509569, acc.: 72.66%] [G loss: 0.641292]\n",
      "epoch:27 step:26063 [D loss: 0.488678, acc.: 73.44%] [G loss: 0.623449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:26064 [D loss: 0.543412, acc.: 69.53%] [G loss: 0.657336]\n",
      "epoch:27 step:26065 [D loss: 0.627979, acc.: 62.50%] [G loss: 0.634110]\n",
      "epoch:27 step:26066 [D loss: 0.570141, acc.: 67.97%] [G loss: 0.706589]\n",
      "epoch:27 step:26067 [D loss: 0.583283, acc.: 69.53%] [G loss: 0.916405]\n",
      "epoch:27 step:26068 [D loss: 0.549141, acc.: 69.53%] [G loss: 0.920932]\n",
      "epoch:27 step:26069 [D loss: 0.526523, acc.: 72.66%] [G loss: 0.850649]\n",
      "epoch:27 step:26070 [D loss: 0.528591, acc.: 68.75%] [G loss: 0.787895]\n",
      "epoch:27 step:26071 [D loss: 0.573428, acc.: 66.41%] [G loss: 0.761671]\n",
      "epoch:27 step:26072 [D loss: 0.567201, acc.: 65.62%] [G loss: 0.636765]\n",
      "epoch:27 step:26073 [D loss: 0.507414, acc.: 75.78%] [G loss: 0.679428]\n",
      "epoch:27 step:26074 [D loss: 0.486413, acc.: 75.78%] [G loss: 0.751792]\n",
      "epoch:27 step:26075 [D loss: 0.530832, acc.: 74.22%] [G loss: 0.747459]\n",
      "epoch:27 step:26076 [D loss: 0.529370, acc.: 71.09%] [G loss: 0.704322]\n",
      "epoch:27 step:26077 [D loss: 0.561456, acc.: 68.75%] [G loss: 0.729323]\n",
      "epoch:27 step:26078 [D loss: 0.520747, acc.: 73.44%] [G loss: 0.528385]\n",
      "epoch:27 step:26079 [D loss: 0.526091, acc.: 71.88%] [G loss: 0.686772]\n",
      "epoch:27 step:26080 [D loss: 0.504066, acc.: 75.00%] [G loss: 0.773600]\n",
      "epoch:27 step:26081 [D loss: 0.496991, acc.: 71.88%] [G loss: 0.922125]\n",
      "epoch:27 step:26082 [D loss: 0.572587, acc.: 68.75%] [G loss: 0.948710]\n",
      "epoch:27 step:26083 [D loss: 0.591216, acc.: 67.97%] [G loss: 0.714441]\n",
      "epoch:27 step:26084 [D loss: 0.502292, acc.: 71.88%] [G loss: 0.584497]\n",
      "epoch:27 step:26085 [D loss: 0.525920, acc.: 70.31%] [G loss: 0.596298]\n",
      "epoch:27 step:26086 [D loss: 0.618465, acc.: 64.84%] [G loss: 0.627743]\n",
      "epoch:27 step:26087 [D loss: 0.651651, acc.: 59.38%] [G loss: 0.528348]\n",
      "epoch:27 step:26088 [D loss: 0.512848, acc.: 73.44%] [G loss: 0.908214]\n",
      "epoch:27 step:26089 [D loss: 0.533444, acc.: 70.31%] [G loss: 0.756611]\n",
      "epoch:27 step:26090 [D loss: 0.540606, acc.: 70.31%] [G loss: 0.744421]\n",
      "epoch:27 step:26091 [D loss: 0.488176, acc.: 72.66%] [G loss: 0.632908]\n",
      "epoch:27 step:26092 [D loss: 0.588194, acc.: 68.75%] [G loss: 0.804226]\n",
      "epoch:27 step:26093 [D loss: 0.682504, acc.: 55.47%] [G loss: 0.685394]\n",
      "epoch:27 step:26094 [D loss: 0.552470, acc.: 65.62%] [G loss: 0.946851]\n",
      "epoch:27 step:26095 [D loss: 0.532490, acc.: 71.09%] [G loss: 0.769054]\n",
      "epoch:27 step:26096 [D loss: 0.489221, acc.: 75.78%] [G loss: 0.736011]\n",
      "epoch:27 step:26097 [D loss: 0.504665, acc.: 71.09%] [G loss: 0.713310]\n",
      "epoch:27 step:26098 [D loss: 0.544410, acc.: 69.53%] [G loss: 0.712705]\n",
      "epoch:27 step:26099 [D loss: 0.602019, acc.: 62.50%] [G loss: 0.675463]\n",
      "epoch:27 step:26100 [D loss: 0.505111, acc.: 71.09%] [G loss: 0.751864]\n",
      "epoch:27 step:26101 [D loss: 0.543057, acc.: 70.31%] [G loss: 0.770030]\n",
      "epoch:27 step:26102 [D loss: 0.538364, acc.: 74.22%] [G loss: 0.846792]\n",
      "epoch:27 step:26103 [D loss: 0.544543, acc.: 68.75%] [G loss: 0.751924]\n",
      "epoch:27 step:26104 [D loss: 0.562250, acc.: 69.53%] [G loss: 0.596078]\n",
      "epoch:27 step:26105 [D loss: 0.583111, acc.: 62.50%] [G loss: 0.602865]\n",
      "epoch:27 step:26106 [D loss: 0.552970, acc.: 67.97%] [G loss: 0.668759]\n",
      "epoch:27 step:26107 [D loss: 0.550785, acc.: 72.66%] [G loss: 0.687961]\n",
      "epoch:27 step:26108 [D loss: 0.534356, acc.: 69.53%] [G loss: 0.600389]\n",
      "epoch:27 step:26109 [D loss: 0.562154, acc.: 66.41%] [G loss: 0.594015]\n",
      "epoch:27 step:26110 [D loss: 0.559234, acc.: 69.53%] [G loss: 0.553029]\n",
      "epoch:27 step:26111 [D loss: 0.651850, acc.: 63.28%] [G loss: 0.550652]\n",
      "epoch:27 step:26112 [D loss: 0.554493, acc.: 64.84%] [G loss: 0.485241]\n",
      "epoch:27 step:26113 [D loss: 0.502486, acc.: 73.44%] [G loss: 0.604170]\n",
      "epoch:27 step:26114 [D loss: 0.456163, acc.: 78.91%] [G loss: 0.804701]\n",
      "epoch:27 step:26115 [D loss: 0.576176, acc.: 71.88%] [G loss: 0.855658]\n",
      "epoch:27 step:26116 [D loss: 0.629754, acc.: 64.06%] [G loss: 0.703182]\n",
      "epoch:27 step:26117 [D loss: 0.585355, acc.: 62.50%] [G loss: 0.711946]\n",
      "epoch:27 step:26118 [D loss: 0.518429, acc.: 71.88%] [G loss: 0.573166]\n",
      "epoch:27 step:26119 [D loss: 0.594878, acc.: 66.41%] [G loss: 0.494131]\n",
      "epoch:27 step:26120 [D loss: 0.489167, acc.: 75.00%] [G loss: 0.675583]\n",
      "epoch:27 step:26121 [D loss: 0.598366, acc.: 68.75%] [G loss: 0.666447]\n",
      "epoch:27 step:26122 [D loss: 0.477026, acc.: 77.34%] [G loss: 0.735012]\n",
      "epoch:27 step:26123 [D loss: 0.543300, acc.: 78.12%] [G loss: 0.602938]\n",
      "epoch:27 step:26124 [D loss: 0.536983, acc.: 72.66%] [G loss: 0.688386]\n",
      "epoch:27 step:26125 [D loss: 0.497009, acc.: 73.44%] [G loss: 0.718645]\n",
      "epoch:27 step:26126 [D loss: 0.620827, acc.: 64.06%] [G loss: 0.645922]\n",
      "epoch:27 step:26127 [D loss: 0.620939, acc.: 61.72%] [G loss: 0.658246]\n",
      "epoch:27 step:26128 [D loss: 0.511853, acc.: 77.34%] [G loss: 0.867591]\n",
      "epoch:27 step:26129 [D loss: 0.576817, acc.: 69.53%] [G loss: 0.767069]\n",
      "epoch:27 step:26130 [D loss: 0.563180, acc.: 68.75%] [G loss: 0.680394]\n",
      "epoch:27 step:26131 [D loss: 0.522559, acc.: 71.09%] [G loss: 0.623832]\n",
      "epoch:27 step:26132 [D loss: 0.552304, acc.: 69.53%] [G loss: 0.744029]\n",
      "epoch:27 step:26133 [D loss: 0.531412, acc.: 69.53%] [G loss: 0.545506]\n",
      "epoch:27 step:26134 [D loss: 0.525838, acc.: 71.88%] [G loss: 0.738104]\n",
      "epoch:27 step:26135 [D loss: 0.534684, acc.: 70.31%] [G loss: 0.627506]\n",
      "epoch:27 step:26136 [D loss: 0.492370, acc.: 74.22%] [G loss: 0.643252]\n",
      "epoch:27 step:26137 [D loss: 0.508973, acc.: 73.44%] [G loss: 0.608036]\n",
      "epoch:27 step:26138 [D loss: 0.559497, acc.: 67.97%] [G loss: 0.548678]\n",
      "epoch:27 step:26139 [D loss: 0.610397, acc.: 70.31%] [G loss: 0.509873]\n",
      "epoch:27 step:26140 [D loss: 0.536273, acc.: 70.31%] [G loss: 0.679212]\n",
      "epoch:27 step:26141 [D loss: 0.529689, acc.: 68.75%] [G loss: 0.715882]\n",
      "epoch:27 step:26142 [D loss: 0.509463, acc.: 72.66%] [G loss: 0.629708]\n",
      "epoch:27 step:26143 [D loss: 0.571123, acc.: 71.09%] [G loss: 0.735994]\n",
      "epoch:27 step:26144 [D loss: 0.568428, acc.: 67.97%] [G loss: 0.634366]\n",
      "epoch:27 step:26145 [D loss: 0.590491, acc.: 64.06%] [G loss: 0.608567]\n",
      "epoch:27 step:26146 [D loss: 0.584402, acc.: 61.72%] [G loss: 0.560538]\n",
      "epoch:27 step:26147 [D loss: 0.557134, acc.: 64.84%] [G loss: 0.451340]\n",
      "epoch:27 step:26148 [D loss: 0.573879, acc.: 66.41%] [G loss: 0.602984]\n",
      "epoch:27 step:26149 [D loss: 0.508595, acc.: 75.78%] [G loss: 0.645661]\n",
      "epoch:27 step:26150 [D loss: 0.597202, acc.: 65.62%] [G loss: 0.462524]\n",
      "epoch:27 step:26151 [D loss: 0.531765, acc.: 71.88%] [G loss: 0.581461]\n",
      "epoch:27 step:26152 [D loss: 0.618626, acc.: 66.41%] [G loss: 0.889382]\n",
      "epoch:27 step:26153 [D loss: 0.519893, acc.: 71.09%] [G loss: 0.582340]\n",
      "epoch:27 step:26154 [D loss: 0.567487, acc.: 67.97%] [G loss: 0.605982]\n",
      "epoch:27 step:26155 [D loss: 0.573941, acc.: 67.19%] [G loss: 0.631247]\n",
      "epoch:27 step:26156 [D loss: 0.453623, acc.: 78.91%] [G loss: 0.582861]\n",
      "epoch:27 step:26157 [D loss: 0.573100, acc.: 68.75%] [G loss: 0.709792]\n",
      "epoch:27 step:26158 [D loss: 0.562505, acc.: 69.53%] [G loss: 0.713095]\n",
      "epoch:27 step:26159 [D loss: 0.528308, acc.: 76.56%] [G loss: 0.800481]\n",
      "epoch:27 step:26160 [D loss: 0.581879, acc.: 66.41%] [G loss: 0.779032]\n",
      "epoch:27 step:26161 [D loss: 0.535499, acc.: 71.09%] [G loss: 0.604981]\n",
      "epoch:27 step:26162 [D loss: 0.519245, acc.: 72.66%] [G loss: 0.619197]\n",
      "epoch:27 step:26163 [D loss: 0.534531, acc.: 70.31%] [G loss: 0.554785]\n",
      "epoch:27 step:26164 [D loss: 0.549984, acc.: 68.75%] [G loss: 0.536033]\n",
      "epoch:27 step:26165 [D loss: 0.533634, acc.: 72.66%] [G loss: 0.641218]\n",
      "epoch:27 step:26166 [D loss: 0.667666, acc.: 60.16%] [G loss: 0.433397]\n",
      "epoch:27 step:26167 [D loss: 0.513923, acc.: 74.22%] [G loss: 0.557187]\n",
      "epoch:27 step:26168 [D loss: 0.545415, acc.: 62.50%] [G loss: 0.501699]\n",
      "epoch:27 step:26169 [D loss: 0.445374, acc.: 78.12%] [G loss: 0.655615]\n",
      "epoch:27 step:26170 [D loss: 0.495398, acc.: 74.22%] [G loss: 0.817523]\n",
      "epoch:27 step:26171 [D loss: 0.558481, acc.: 66.41%] [G loss: 0.650687]\n",
      "epoch:27 step:26172 [D loss: 0.601323, acc.: 64.84%] [G loss: 0.674450]\n",
      "epoch:27 step:26173 [D loss: 0.582958, acc.: 64.84%] [G loss: 0.570373]\n",
      "epoch:27 step:26174 [D loss: 0.538775, acc.: 70.31%] [G loss: 0.621758]\n",
      "epoch:27 step:26175 [D loss: 0.550517, acc.: 67.97%] [G loss: 0.864223]\n",
      "epoch:27 step:26176 [D loss: 0.623917, acc.: 60.94%] [G loss: 0.574800]\n",
      "epoch:27 step:26177 [D loss: 0.551501, acc.: 69.53%] [G loss: 0.537465]\n",
      "epoch:27 step:26178 [D loss: 0.585951, acc.: 67.97%] [G loss: 0.576448]\n",
      "epoch:27 step:26179 [D loss: 0.620132, acc.: 65.62%] [G loss: 0.576983]\n",
      "epoch:27 step:26180 [D loss: 0.540702, acc.: 68.75%] [G loss: 0.498710]\n",
      "epoch:27 step:26181 [D loss: 0.532171, acc.: 71.09%] [G loss: 0.652079]\n",
      "epoch:27 step:26182 [D loss: 0.565042, acc.: 68.75%] [G loss: 0.618737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:26183 [D loss: 0.497628, acc.: 75.78%] [G loss: 0.707617]\n",
      "epoch:27 step:26184 [D loss: 0.563954, acc.: 71.09%] [G loss: 0.668391]\n",
      "epoch:27 step:26185 [D loss: 0.540415, acc.: 67.97%] [G loss: 0.763171]\n",
      "epoch:27 step:26186 [D loss: 0.493648, acc.: 76.56%] [G loss: 0.835301]\n",
      "epoch:27 step:26187 [D loss: 0.571333, acc.: 67.19%] [G loss: 0.559122]\n",
      "epoch:27 step:26188 [D loss: 0.601110, acc.: 65.62%] [G loss: 0.730899]\n",
      "epoch:27 step:26189 [D loss: 0.460961, acc.: 80.47%] [G loss: 0.733727]\n",
      "epoch:27 step:26190 [D loss: 0.546991, acc.: 68.75%] [G loss: 0.696876]\n",
      "epoch:27 step:26191 [D loss: 0.627795, acc.: 54.69%] [G loss: 0.518467]\n",
      "epoch:27 step:26192 [D loss: 0.582973, acc.: 71.88%] [G loss: 0.692464]\n",
      "epoch:27 step:26193 [D loss: 0.422441, acc.: 82.81%] [G loss: 0.768632]\n",
      "epoch:27 step:26194 [D loss: 0.532844, acc.: 67.97%] [G loss: 0.517587]\n",
      "epoch:27 step:26195 [D loss: 0.511452, acc.: 75.00%] [G loss: 0.676011]\n",
      "epoch:27 step:26196 [D loss: 0.497325, acc.: 74.22%] [G loss: 0.986149]\n",
      "epoch:27 step:26197 [D loss: 0.486375, acc.: 78.12%] [G loss: 0.662826]\n",
      "epoch:27 step:26198 [D loss: 0.504935, acc.: 73.44%] [G loss: 0.764670]\n",
      "epoch:27 step:26199 [D loss: 0.538317, acc.: 71.88%] [G loss: 0.774135]\n",
      "epoch:27 step:26200 [D loss: 0.532835, acc.: 71.09%] [G loss: 0.859197]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.729394\n",
      "FID: 52.592018\n",
      "0 = 12.742252311515779\n",
      "1 = 0.08788452127243265\n",
      "2 = 0.8364999890327454\n",
      "3 = 0.7893999814987183\n",
      "4 = 0.8835999965667725\n",
      "5 = 0.8714948296546936\n",
      "6 = 0.7893999814987183\n",
      "7 = 8.554512913751577\n",
      "8 = 0.16241132811137704\n",
      "9 = 0.6991000175476074\n",
      "10 = 0.6895999908447266\n",
      "11 = 0.7085999846458435\n",
      "12 = 0.7029561400413513\n",
      "13 = 0.6895999908447266\n",
      "14 = 6.729428291320801\n",
      "15 = 7.036840438842773\n",
      "16 = 0.38963180780410767\n",
      "17 = 6.72939395904541\n",
      "18 = 52.592018127441406\n",
      "epoch:27 step:26201 [D loss: 0.553994, acc.: 75.00%] [G loss: 0.612818]\n",
      "epoch:27 step:26202 [D loss: 0.529357, acc.: 72.66%] [G loss: 0.659990]\n",
      "epoch:27 step:26203 [D loss: 0.558292, acc.: 70.31%] [G loss: 0.605851]\n",
      "epoch:27 step:26204 [D loss: 0.580648, acc.: 68.75%] [G loss: 0.593531]\n",
      "epoch:27 step:26205 [D loss: 0.599517, acc.: 69.53%] [G loss: 0.709660]\n",
      "epoch:27 step:26206 [D loss: 0.543460, acc.: 75.78%] [G loss: 0.727722]\n",
      "epoch:27 step:26207 [D loss: 0.593619, acc.: 71.88%] [G loss: 0.619232]\n",
      "epoch:27 step:26208 [D loss: 0.552612, acc.: 70.31%] [G loss: 0.681791]\n",
      "epoch:27 step:26209 [D loss: 0.562555, acc.: 68.75%] [G loss: 0.778888]\n",
      "epoch:27 step:26210 [D loss: 0.472310, acc.: 75.00%] [G loss: 0.674410]\n",
      "epoch:27 step:26211 [D loss: 0.501661, acc.: 70.31%] [G loss: 0.887249]\n",
      "epoch:27 step:26212 [D loss: 0.602283, acc.: 69.53%] [G loss: 0.835764]\n",
      "epoch:27 step:26213 [D loss: 0.517071, acc.: 74.22%] [G loss: 0.711270]\n",
      "epoch:27 step:26214 [D loss: 0.658492, acc.: 62.50%] [G loss: 0.791671]\n",
      "epoch:27 step:26215 [D loss: 0.554617, acc.: 70.31%] [G loss: 0.708466]\n",
      "epoch:27 step:26216 [D loss: 0.595628, acc.: 64.84%] [G loss: 0.629834]\n",
      "epoch:27 step:26217 [D loss: 0.492405, acc.: 71.88%] [G loss: 0.613058]\n",
      "epoch:27 step:26218 [D loss: 0.473647, acc.: 82.03%] [G loss: 0.841232]\n",
      "epoch:27 step:26219 [D loss: 0.740784, acc.: 61.72%] [G loss: 0.636225]\n",
      "epoch:27 step:26220 [D loss: 0.469626, acc.: 79.69%] [G loss: 0.832331]\n",
      "epoch:27 step:26221 [D loss: 0.551624, acc.: 69.53%] [G loss: 0.653369]\n",
      "epoch:27 step:26222 [D loss: 0.489662, acc.: 73.44%] [G loss: 0.812769]\n",
      "epoch:27 step:26223 [D loss: 0.495708, acc.: 71.88%] [G loss: 0.862953]\n",
      "epoch:27 step:26224 [D loss: 0.405083, acc.: 83.59%] [G loss: 1.024680]\n",
      "epoch:27 step:26225 [D loss: 0.432202, acc.: 82.81%] [G loss: 1.101469]\n",
      "epoch:27 step:26226 [D loss: 0.488645, acc.: 73.44%] [G loss: 1.279606]\n",
      "epoch:27 step:26227 [D loss: 0.650093, acc.: 69.53%] [G loss: 1.060441]\n",
      "epoch:27 step:26228 [D loss: 0.547107, acc.: 68.75%] [G loss: 1.350489]\n",
      "epoch:27 step:26229 [D loss: 0.423228, acc.: 80.47%] [G loss: 1.218413]\n",
      "epoch:27 step:26230 [D loss: 0.600797, acc.: 70.31%] [G loss: 0.928867]\n",
      "epoch:27 step:26231 [D loss: 0.597633, acc.: 65.62%] [G loss: 0.814471]\n",
      "epoch:27 step:26232 [D loss: 0.476936, acc.: 75.78%] [G loss: 0.784056]\n",
      "epoch:27 step:26233 [D loss: 0.579134, acc.: 68.75%] [G loss: 0.757225]\n",
      "epoch:27 step:26234 [D loss: 0.483259, acc.: 71.09%] [G loss: 1.037981]\n",
      "epoch:27 step:26235 [D loss: 0.416552, acc.: 83.59%] [G loss: 1.167081]\n",
      "epoch:27 step:26236 [D loss: 0.420578, acc.: 80.47%] [G loss: 1.368870]\n",
      "epoch:28 step:26237 [D loss: 0.539466, acc.: 72.66%] [G loss: 1.029826]\n",
      "epoch:28 step:26238 [D loss: 0.459914, acc.: 79.69%] [G loss: 1.038093]\n",
      "epoch:28 step:26239 [D loss: 0.572907, acc.: 69.53%] [G loss: 0.957047]\n",
      "epoch:28 step:26240 [D loss: 0.520343, acc.: 71.09%] [G loss: 0.999334]\n",
      "epoch:28 step:26241 [D loss: 0.533184, acc.: 70.31%] [G loss: 0.951437]\n",
      "epoch:28 step:26242 [D loss: 0.647670, acc.: 66.41%] [G loss: 0.825661]\n",
      "epoch:28 step:26243 [D loss: 0.486213, acc.: 78.12%] [G loss: 0.736744]\n",
      "epoch:28 step:26244 [D loss: 0.526279, acc.: 74.22%] [G loss: 0.912280]\n",
      "epoch:28 step:26245 [D loss: 0.518828, acc.: 73.44%] [G loss: 0.894948]\n",
      "epoch:28 step:26246 [D loss: 0.557290, acc.: 71.88%] [G loss: 0.910214]\n",
      "epoch:28 step:26247 [D loss: 0.454198, acc.: 78.12%] [G loss: 1.038355]\n",
      "epoch:28 step:26248 [D loss: 0.578338, acc.: 68.75%] [G loss: 0.964789]\n",
      "epoch:28 step:26249 [D loss: 0.542952, acc.: 71.88%] [G loss: 0.643022]\n",
      "epoch:28 step:26250 [D loss: 0.528065, acc.: 71.88%] [G loss: 0.872112]\n",
      "epoch:28 step:26251 [D loss: 0.403306, acc.: 77.34%] [G loss: 0.958680]\n",
      "epoch:28 step:26252 [D loss: 0.496957, acc.: 74.22%] [G loss: 0.690124]\n",
      "epoch:28 step:26253 [D loss: 0.495951, acc.: 78.12%] [G loss: 0.741253]\n",
      "epoch:28 step:26254 [D loss: 0.537882, acc.: 72.66%] [G loss: 0.680170]\n",
      "epoch:28 step:26255 [D loss: 0.532335, acc.: 73.44%] [G loss: 0.901664]\n",
      "epoch:28 step:26256 [D loss: 0.677418, acc.: 62.50%] [G loss: 0.873725]\n",
      "epoch:28 step:26257 [D loss: 0.626914, acc.: 63.28%] [G loss: 0.826403]\n",
      "epoch:28 step:26258 [D loss: 0.460316, acc.: 76.56%] [G loss: 0.950693]\n",
      "epoch:28 step:26259 [D loss: 0.580921, acc.: 68.75%] [G loss: 0.735803]\n",
      "epoch:28 step:26260 [D loss: 0.488471, acc.: 78.12%] [G loss: 0.749653]\n",
      "epoch:28 step:26261 [D loss: 0.547336, acc.: 72.66%] [G loss: 0.700352]\n",
      "epoch:28 step:26262 [D loss: 0.620742, acc.: 65.62%] [G loss: 0.689769]\n",
      "epoch:28 step:26263 [D loss: 0.434928, acc.: 79.69%] [G loss: 0.704073]\n",
      "epoch:28 step:26264 [D loss: 0.607447, acc.: 66.41%] [G loss: 0.676964]\n",
      "epoch:28 step:26265 [D loss: 0.479053, acc.: 75.78%] [G loss: 0.829295]\n",
      "epoch:28 step:26266 [D loss: 0.566860, acc.: 67.97%] [G loss: 0.691794]\n",
      "epoch:28 step:26267 [D loss: 0.640820, acc.: 67.19%] [G loss: 0.541396]\n",
      "epoch:28 step:26268 [D loss: 0.574218, acc.: 67.97%] [G loss: 0.733459]\n",
      "epoch:28 step:26269 [D loss: 0.514704, acc.: 72.66%] [G loss: 0.717292]\n",
      "epoch:28 step:26270 [D loss: 0.574911, acc.: 62.50%] [G loss: 0.494742]\n",
      "epoch:28 step:26271 [D loss: 0.571155, acc.: 67.97%] [G loss: 0.664051]\n",
      "epoch:28 step:26272 [D loss: 0.492779, acc.: 72.66%] [G loss: 0.875160]\n",
      "epoch:28 step:26273 [D loss: 0.486099, acc.: 74.22%] [G loss: 0.698913]\n",
      "epoch:28 step:26274 [D loss: 0.538475, acc.: 73.44%] [G loss: 0.776327]\n",
      "epoch:28 step:26275 [D loss: 0.520431, acc.: 71.88%] [G loss: 0.676385]\n",
      "epoch:28 step:26276 [D loss: 0.459763, acc.: 77.34%] [G loss: 0.710425]\n",
      "epoch:28 step:26277 [D loss: 0.552186, acc.: 71.88%] [G loss: 0.643079]\n",
      "epoch:28 step:26278 [D loss: 0.617387, acc.: 59.38%] [G loss: 0.726669]\n",
      "epoch:28 step:26279 [D loss: 0.485625, acc.: 81.25%] [G loss: 0.706752]\n",
      "epoch:28 step:26280 [D loss: 0.561223, acc.: 67.19%] [G loss: 0.619576]\n",
      "epoch:28 step:26281 [D loss: 0.488019, acc.: 76.56%] [G loss: 0.642154]\n",
      "epoch:28 step:26282 [D loss: 0.482294, acc.: 75.00%] [G loss: 0.730780]\n",
      "epoch:28 step:26283 [D loss: 0.509696, acc.: 77.34%] [G loss: 0.817837]\n",
      "epoch:28 step:26284 [D loss: 0.498207, acc.: 72.66%] [G loss: 0.947795]\n",
      "epoch:28 step:26285 [D loss: 0.482880, acc.: 75.00%] [G loss: 0.871163]\n",
      "epoch:28 step:26286 [D loss: 0.510173, acc.: 71.09%] [G loss: 0.795324]\n",
      "epoch:28 step:26287 [D loss: 0.641146, acc.: 64.06%] [G loss: 0.584876]\n",
      "epoch:28 step:26288 [D loss: 0.581849, acc.: 71.09%] [G loss: 0.724508]\n",
      "epoch:28 step:26289 [D loss: 0.578892, acc.: 70.31%] [G loss: 0.676758]\n",
      "epoch:28 step:26290 [D loss: 0.500150, acc.: 77.34%] [G loss: 0.855278]\n",
      "epoch:28 step:26291 [D loss: 0.559755, acc.: 71.09%] [G loss: 0.825100]\n",
      "epoch:28 step:26292 [D loss: 0.495255, acc.: 76.56%] [G loss: 0.818140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26293 [D loss: 0.523525, acc.: 73.44%] [G loss: 0.632487]\n",
      "epoch:28 step:26294 [D loss: 0.563861, acc.: 70.31%] [G loss: 0.691564]\n",
      "epoch:28 step:26295 [D loss: 0.532975, acc.: 71.09%] [G loss: 0.639993]\n",
      "epoch:28 step:26296 [D loss: 0.577696, acc.: 67.97%] [G loss: 0.599420]\n",
      "epoch:28 step:26297 [D loss: 0.496713, acc.: 72.66%] [G loss: 0.617022]\n",
      "epoch:28 step:26298 [D loss: 0.554594, acc.: 71.09%] [G loss: 0.643157]\n",
      "epoch:28 step:26299 [D loss: 0.543956, acc.: 74.22%] [G loss: 0.500011]\n",
      "epoch:28 step:26300 [D loss: 0.566296, acc.: 69.53%] [G loss: 0.747435]\n",
      "epoch:28 step:26301 [D loss: 0.538239, acc.: 71.88%] [G loss: 0.563002]\n",
      "epoch:28 step:26302 [D loss: 0.525955, acc.: 71.88%] [G loss: 0.629473]\n",
      "epoch:28 step:26303 [D loss: 0.560983, acc.: 68.75%] [G loss: 0.589443]\n",
      "epoch:28 step:26304 [D loss: 0.549082, acc.: 71.09%] [G loss: 0.694310]\n",
      "epoch:28 step:26305 [D loss: 0.591135, acc.: 63.28%] [G loss: 0.623417]\n",
      "epoch:28 step:26306 [D loss: 0.467795, acc.: 79.69%] [G loss: 0.923314]\n",
      "epoch:28 step:26307 [D loss: 0.510267, acc.: 71.09%] [G loss: 0.793750]\n",
      "epoch:28 step:26308 [D loss: 0.541609, acc.: 75.78%] [G loss: 0.631866]\n",
      "epoch:28 step:26309 [D loss: 0.615161, acc.: 64.06%] [G loss: 0.572674]\n",
      "epoch:28 step:26310 [D loss: 0.453081, acc.: 76.56%] [G loss: 0.775613]\n",
      "epoch:28 step:26311 [D loss: 0.484488, acc.: 78.12%] [G loss: 0.913345]\n",
      "epoch:28 step:26312 [D loss: 0.520004, acc.: 74.22%] [G loss: 0.818042]\n",
      "epoch:28 step:26313 [D loss: 0.481658, acc.: 74.22%] [G loss: 0.912969]\n",
      "epoch:28 step:26314 [D loss: 0.629593, acc.: 66.41%] [G loss: 0.646789]\n",
      "epoch:28 step:26315 [D loss: 0.566903, acc.: 68.75%] [G loss: 0.600646]\n",
      "epoch:28 step:26316 [D loss: 0.494484, acc.: 73.44%] [G loss: 0.677189]\n",
      "epoch:28 step:26317 [D loss: 0.491279, acc.: 76.56%] [G loss: 0.752456]\n",
      "epoch:28 step:26318 [D loss: 0.521448, acc.: 71.09%] [G loss: 0.669361]\n",
      "epoch:28 step:26319 [D loss: 0.453281, acc.: 81.25%] [G loss: 0.859838]\n",
      "epoch:28 step:26320 [D loss: 0.512770, acc.: 74.22%] [G loss: 0.784315]\n",
      "epoch:28 step:26321 [D loss: 0.576039, acc.: 67.97%] [G loss: 0.545466]\n",
      "epoch:28 step:26322 [D loss: 0.526065, acc.: 72.66%] [G loss: 0.772174]\n",
      "epoch:28 step:26323 [D loss: 0.540942, acc.: 71.09%] [G loss: 0.775795]\n",
      "epoch:28 step:26324 [D loss: 0.556150, acc.: 74.22%] [G loss: 0.737766]\n",
      "epoch:28 step:26325 [D loss: 0.543308, acc.: 69.53%] [G loss: 0.704787]\n",
      "epoch:28 step:26326 [D loss: 0.544985, acc.: 68.75%] [G loss: 0.715131]\n",
      "epoch:28 step:26327 [D loss: 0.562417, acc.: 65.62%] [G loss: 0.808309]\n",
      "epoch:28 step:26328 [D loss: 0.497476, acc.: 78.91%] [G loss: 0.656210]\n",
      "epoch:28 step:26329 [D loss: 0.498857, acc.: 76.56%] [G loss: 0.909179]\n",
      "epoch:28 step:26330 [D loss: 0.511624, acc.: 73.44%] [G loss: 0.957773]\n",
      "epoch:28 step:26331 [D loss: 0.548568, acc.: 67.97%] [G loss: 0.648109]\n",
      "epoch:28 step:26332 [D loss: 0.485813, acc.: 76.56%] [G loss: 0.903907]\n",
      "epoch:28 step:26333 [D loss: 0.525632, acc.: 70.31%] [G loss: 0.896252]\n",
      "epoch:28 step:26334 [D loss: 0.560696, acc.: 67.19%] [G loss: 0.846943]\n",
      "epoch:28 step:26335 [D loss: 0.536676, acc.: 73.44%] [G loss: 0.838270]\n",
      "epoch:28 step:26336 [D loss: 0.454610, acc.: 78.12%] [G loss: 1.128324]\n",
      "epoch:28 step:26337 [D loss: 0.461122, acc.: 79.69%] [G loss: 0.898538]\n",
      "epoch:28 step:26338 [D loss: 0.653132, acc.: 64.06%] [G loss: 0.718256]\n",
      "epoch:28 step:26339 [D loss: 0.515153, acc.: 72.66%] [G loss: 0.618018]\n",
      "epoch:28 step:26340 [D loss: 0.575207, acc.: 67.19%] [G loss: 0.562125]\n",
      "epoch:28 step:26341 [D loss: 0.600461, acc.: 64.06%] [G loss: 0.775665]\n",
      "epoch:28 step:26342 [D loss: 0.518531, acc.: 69.53%] [G loss: 0.539335]\n",
      "epoch:28 step:26343 [D loss: 0.565781, acc.: 70.31%] [G loss: 0.650917]\n",
      "epoch:28 step:26344 [D loss: 0.641697, acc.: 62.50%] [G loss: 0.658415]\n",
      "epoch:28 step:26345 [D loss: 0.559793, acc.: 66.41%] [G loss: 0.657697]\n",
      "epoch:28 step:26346 [D loss: 0.567791, acc.: 67.97%] [G loss: 0.544076]\n",
      "epoch:28 step:26347 [D loss: 0.481965, acc.: 75.78%] [G loss: 0.739715]\n",
      "epoch:28 step:26348 [D loss: 0.519511, acc.: 71.09%] [G loss: 0.525735]\n",
      "epoch:28 step:26349 [D loss: 0.490540, acc.: 78.12%] [G loss: 0.879639]\n",
      "epoch:28 step:26350 [D loss: 0.553121, acc.: 71.09%] [G loss: 0.653289]\n",
      "epoch:28 step:26351 [D loss: 0.521136, acc.: 78.91%] [G loss: 0.910854]\n",
      "epoch:28 step:26352 [D loss: 0.478247, acc.: 77.34%] [G loss: 0.787700]\n",
      "epoch:28 step:26353 [D loss: 0.550635, acc.: 67.97%] [G loss: 0.671288]\n",
      "epoch:28 step:26354 [D loss: 0.554443, acc.: 72.66%] [G loss: 0.628478]\n",
      "epoch:28 step:26355 [D loss: 0.440720, acc.: 78.91%] [G loss: 0.828229]\n",
      "epoch:28 step:26356 [D loss: 0.564932, acc.: 70.31%] [G loss: 0.856838]\n",
      "epoch:28 step:26357 [D loss: 0.526295, acc.: 71.09%] [G loss: 0.818209]\n",
      "epoch:28 step:26358 [D loss: 0.490243, acc.: 75.78%] [G loss: 0.809370]\n",
      "epoch:28 step:26359 [D loss: 0.490238, acc.: 76.56%] [G loss: 0.983171]\n",
      "epoch:28 step:26360 [D loss: 0.579813, acc.: 67.19%] [G loss: 0.722655]\n",
      "epoch:28 step:26361 [D loss: 0.522956, acc.: 72.66%] [G loss: 0.735896]\n",
      "epoch:28 step:26362 [D loss: 0.490599, acc.: 74.22%] [G loss: 0.815088]\n",
      "epoch:28 step:26363 [D loss: 0.479498, acc.: 75.00%] [G loss: 0.716431]\n",
      "epoch:28 step:26364 [D loss: 0.545523, acc.: 70.31%] [G loss: 0.634104]\n",
      "epoch:28 step:26365 [D loss: 0.526345, acc.: 75.78%] [G loss: 0.665677]\n",
      "epoch:28 step:26366 [D loss: 0.483299, acc.: 77.34%] [G loss: 0.607385]\n",
      "epoch:28 step:26367 [D loss: 0.503356, acc.: 72.66%] [G loss: 0.959948]\n",
      "epoch:28 step:26368 [D loss: 0.496236, acc.: 74.22%] [G loss: 0.803671]\n",
      "epoch:28 step:26369 [D loss: 0.572555, acc.: 66.41%] [G loss: 0.970449]\n",
      "epoch:28 step:26370 [D loss: 0.572052, acc.: 66.41%] [G loss: 0.879765]\n",
      "epoch:28 step:26371 [D loss: 0.576141, acc.: 65.62%] [G loss: 0.988914]\n",
      "epoch:28 step:26372 [D loss: 0.532869, acc.: 70.31%] [G loss: 0.755565]\n",
      "epoch:28 step:26373 [D loss: 0.619188, acc.: 64.84%] [G loss: 0.613849]\n",
      "epoch:28 step:26374 [D loss: 0.574275, acc.: 69.53%] [G loss: 0.637953]\n",
      "epoch:28 step:26375 [D loss: 0.495773, acc.: 74.22%] [G loss: 0.679342]\n",
      "epoch:28 step:26376 [D loss: 0.533426, acc.: 68.75%] [G loss: 0.686780]\n",
      "epoch:28 step:26377 [D loss: 0.530056, acc.: 69.53%] [G loss: 0.611559]\n",
      "epoch:28 step:26378 [D loss: 0.540407, acc.: 69.53%] [G loss: 0.588797]\n",
      "epoch:28 step:26379 [D loss: 0.629218, acc.: 62.50%] [G loss: 0.616968]\n",
      "epoch:28 step:26380 [D loss: 0.556797, acc.: 68.75%] [G loss: 0.651831]\n",
      "epoch:28 step:26381 [D loss: 0.554059, acc.: 66.41%] [G loss: 0.603040]\n",
      "epoch:28 step:26382 [D loss: 0.485182, acc.: 76.56%] [G loss: 0.812155]\n",
      "epoch:28 step:26383 [D loss: 0.623133, acc.: 62.50%] [G loss: 0.680216]\n",
      "epoch:28 step:26384 [D loss: 0.607995, acc.: 60.94%] [G loss: 0.496734]\n",
      "epoch:28 step:26385 [D loss: 0.498791, acc.: 72.66%] [G loss: 0.771846]\n",
      "epoch:28 step:26386 [D loss: 0.574025, acc.: 73.44%] [G loss: 0.549785]\n",
      "epoch:28 step:26387 [D loss: 0.526847, acc.: 71.88%] [G loss: 0.510444]\n",
      "epoch:28 step:26388 [D loss: 0.476805, acc.: 78.12%] [G loss: 0.571231]\n",
      "epoch:28 step:26389 [D loss: 0.544616, acc.: 71.09%] [G loss: 0.837289]\n",
      "epoch:28 step:26390 [D loss: 0.534690, acc.: 72.66%] [G loss: 0.699033]\n",
      "epoch:28 step:26391 [D loss: 0.456430, acc.: 78.12%] [G loss: 0.761042]\n",
      "epoch:28 step:26392 [D loss: 0.511367, acc.: 75.00%] [G loss: 0.689041]\n",
      "epoch:28 step:26393 [D loss: 0.608723, acc.: 64.06%] [G loss: 0.688879]\n",
      "epoch:28 step:26394 [D loss: 0.587223, acc.: 69.53%] [G loss: 0.748078]\n",
      "epoch:28 step:26395 [D loss: 0.507843, acc.: 73.44%] [G loss: 0.807571]\n",
      "epoch:28 step:26396 [D loss: 0.612498, acc.: 61.72%] [G loss: 0.888777]\n",
      "epoch:28 step:26397 [D loss: 0.515269, acc.: 73.44%] [G loss: 0.824487]\n",
      "epoch:28 step:26398 [D loss: 0.495072, acc.: 73.44%] [G loss: 0.804101]\n",
      "epoch:28 step:26399 [D loss: 0.584729, acc.: 65.62%] [G loss: 0.829088]\n",
      "epoch:28 step:26400 [D loss: 0.542986, acc.: 73.44%] [G loss: 0.730199]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.822584\n",
      "FID: 44.530468\n",
      "0 = 13.05423820180899\n",
      "1 = 0.10208173218593787\n",
      "2 = 0.8504999876022339\n",
      "3 = 0.8217999935150146\n",
      "4 = 0.8791999816894531\n",
      "5 = 0.8718438148498535\n",
      "6 = 0.8217999935150146\n",
      "7 = 8.106432855081541\n",
      "8 = 0.14033563300005586\n",
      "9 = 0.6888999938964844\n",
      "10 = 0.6819999814033508\n",
      "11 = 0.6958000063896179\n",
      "12 = 0.6915432810783386\n",
      "13 = 0.6819999814033508\n",
      "14 = 6.822614669799805\n",
      "15 = 6.9496026039123535\n",
      "16 = 0.38993147015571594\n",
      "17 = 6.8225836753845215\n",
      "18 = 44.53046798706055\n",
      "epoch:28 step:26401 [D loss: 0.534045, acc.: 68.75%] [G loss: 0.698770]\n",
      "epoch:28 step:26402 [D loss: 0.586584, acc.: 68.75%] [G loss: 0.704527]\n",
      "epoch:28 step:26403 [D loss: 0.545184, acc.: 73.44%] [G loss: 0.722321]\n",
      "epoch:28 step:26404 [D loss: 0.516707, acc.: 73.44%] [G loss: 0.644047]\n",
      "epoch:28 step:26405 [D loss: 0.580249, acc.: 67.19%] [G loss: 0.683922]\n",
      "epoch:28 step:26406 [D loss: 0.565883, acc.: 64.84%] [G loss: 0.528707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26407 [D loss: 0.560745, acc.: 67.97%] [G loss: 0.505424]\n",
      "epoch:28 step:26408 [D loss: 0.495935, acc.: 72.66%] [G loss: 0.603939]\n",
      "epoch:28 step:26409 [D loss: 0.513436, acc.: 71.09%] [G loss: 0.831555]\n",
      "epoch:28 step:26410 [D loss: 0.582715, acc.: 64.06%] [G loss: 0.522006]\n",
      "epoch:28 step:26411 [D loss: 0.580726, acc.: 64.84%] [G loss: 0.631596]\n",
      "epoch:28 step:26412 [D loss: 0.530057, acc.: 71.88%] [G loss: 0.604194]\n",
      "epoch:28 step:26413 [D loss: 0.546538, acc.: 74.22%] [G loss: 0.680868]\n",
      "epoch:28 step:26414 [D loss: 0.568528, acc.: 67.97%] [G loss: 0.497932]\n",
      "epoch:28 step:26415 [D loss: 0.495784, acc.: 71.09%] [G loss: 0.634358]\n",
      "epoch:28 step:26416 [D loss: 0.663960, acc.: 58.59%] [G loss: 0.580516]\n",
      "epoch:28 step:26417 [D loss: 0.586534, acc.: 64.84%] [G loss: 0.531497]\n",
      "epoch:28 step:26418 [D loss: 0.537983, acc.: 71.09%] [G loss: 0.833642]\n",
      "epoch:28 step:26419 [D loss: 0.560225, acc.: 69.53%] [G loss: 0.708642]\n",
      "epoch:28 step:26420 [D loss: 0.537184, acc.: 71.88%] [G loss: 0.986345]\n",
      "epoch:28 step:26421 [D loss: 0.544602, acc.: 68.75%] [G loss: 0.692943]\n",
      "epoch:28 step:26422 [D loss: 0.558513, acc.: 73.44%] [G loss: 0.885035]\n",
      "epoch:28 step:26423 [D loss: 0.582955, acc.: 64.06%] [G loss: 0.591249]\n",
      "epoch:28 step:26424 [D loss: 0.489259, acc.: 75.00%] [G loss: 0.680718]\n",
      "epoch:28 step:26425 [D loss: 0.589722, acc.: 66.41%] [G loss: 0.517896]\n",
      "epoch:28 step:26426 [D loss: 0.453430, acc.: 81.25%] [G loss: 0.743162]\n",
      "epoch:28 step:26427 [D loss: 0.513866, acc.: 74.22%] [G loss: 0.627122]\n",
      "epoch:28 step:26428 [D loss: 0.468796, acc.: 79.69%] [G loss: 0.682407]\n",
      "epoch:28 step:26429 [D loss: 0.598455, acc.: 65.62%] [G loss: 0.677684]\n",
      "epoch:28 step:26430 [D loss: 0.431455, acc.: 78.12%] [G loss: 0.808837]\n",
      "epoch:28 step:26431 [D loss: 0.609599, acc.: 68.75%] [G loss: 0.796503]\n",
      "epoch:28 step:26432 [D loss: 0.523210, acc.: 68.75%] [G loss: 0.614250]\n",
      "epoch:28 step:26433 [D loss: 0.512493, acc.: 69.53%] [G loss: 0.812017]\n",
      "epoch:28 step:26434 [D loss: 0.493659, acc.: 75.78%] [G loss: 0.883518]\n",
      "epoch:28 step:26435 [D loss: 0.518961, acc.: 72.66%] [G loss: 0.715292]\n",
      "epoch:28 step:26436 [D loss: 0.616239, acc.: 64.06%] [G loss: 0.662467]\n",
      "epoch:28 step:26437 [D loss: 0.583569, acc.: 66.41%] [G loss: 0.680103]\n",
      "epoch:28 step:26438 [D loss: 0.488267, acc.: 77.34%] [G loss: 0.780317]\n",
      "epoch:28 step:26439 [D loss: 0.563348, acc.: 71.09%] [G loss: 0.706509]\n",
      "epoch:28 step:26440 [D loss: 0.572504, acc.: 70.31%] [G loss: 0.736284]\n",
      "epoch:28 step:26441 [D loss: 0.480456, acc.: 77.34%] [G loss: 0.837376]\n",
      "epoch:28 step:26442 [D loss: 0.470033, acc.: 78.91%] [G loss: 0.902528]\n",
      "epoch:28 step:26443 [D loss: 0.495841, acc.: 75.00%] [G loss: 1.108702]\n",
      "epoch:28 step:26444 [D loss: 0.436826, acc.: 80.47%] [G loss: 0.888375]\n",
      "epoch:28 step:26445 [D loss: 0.493073, acc.: 72.66%] [G loss: 0.825141]\n",
      "epoch:28 step:26446 [D loss: 0.634588, acc.: 66.41%] [G loss: 0.614542]\n",
      "epoch:28 step:26447 [D loss: 0.601879, acc.: 70.31%] [G loss: 0.646973]\n",
      "epoch:28 step:26448 [D loss: 0.547948, acc.: 71.09%] [G loss: 0.632288]\n",
      "epoch:28 step:26449 [D loss: 0.503936, acc.: 73.44%] [G loss: 0.833158]\n",
      "epoch:28 step:26450 [D loss: 0.680254, acc.: 58.59%] [G loss: 0.597164]\n",
      "epoch:28 step:26451 [D loss: 0.587274, acc.: 63.28%] [G loss: 0.506618]\n",
      "epoch:28 step:26452 [D loss: 0.517540, acc.: 73.44%] [G loss: 0.780463]\n",
      "epoch:28 step:26453 [D loss: 0.579430, acc.: 66.41%] [G loss: 0.769654]\n",
      "epoch:28 step:26454 [D loss: 0.511273, acc.: 75.78%] [G loss: 0.822309]\n",
      "epoch:28 step:26455 [D loss: 0.429938, acc.: 77.34%] [G loss: 0.941344]\n",
      "epoch:28 step:26456 [D loss: 0.671140, acc.: 65.62%] [G loss: 0.818384]\n",
      "epoch:28 step:26457 [D loss: 0.550217, acc.: 71.09%] [G loss: 0.701715]\n",
      "epoch:28 step:26458 [D loss: 0.513266, acc.: 69.53%] [G loss: 0.687925]\n",
      "epoch:28 step:26459 [D loss: 0.542589, acc.: 71.09%] [G loss: 0.691579]\n",
      "epoch:28 step:26460 [D loss: 0.552096, acc.: 68.75%] [G loss: 0.720152]\n",
      "epoch:28 step:26461 [D loss: 0.500046, acc.: 79.69%] [G loss: 0.765272]\n",
      "epoch:28 step:26462 [D loss: 0.619813, acc.: 65.62%] [G loss: 0.886187]\n",
      "epoch:28 step:26463 [D loss: 0.534724, acc.: 70.31%] [G loss: 0.728189]\n",
      "epoch:28 step:26464 [D loss: 0.618033, acc.: 64.84%] [G loss: 0.703623]\n",
      "epoch:28 step:26465 [D loss: 0.560817, acc.: 73.44%] [G loss: 0.659323]\n",
      "epoch:28 step:26466 [D loss: 0.553674, acc.: 73.44%] [G loss: 0.749885]\n",
      "epoch:28 step:26467 [D loss: 0.473733, acc.: 75.78%] [G loss: 0.920832]\n",
      "epoch:28 step:26468 [D loss: 0.484615, acc.: 74.22%] [G loss: 1.129756]\n",
      "epoch:28 step:26469 [D loss: 0.506200, acc.: 74.22%] [G loss: 0.899776]\n",
      "epoch:28 step:26470 [D loss: 0.630360, acc.: 64.06%] [G loss: 0.641555]\n",
      "epoch:28 step:26471 [D loss: 0.600226, acc.: 66.41%] [G loss: 0.628622]\n",
      "epoch:28 step:26472 [D loss: 0.588012, acc.: 68.75%] [G loss: 0.581307]\n",
      "epoch:28 step:26473 [D loss: 0.544498, acc.: 67.97%] [G loss: 0.486115]\n",
      "epoch:28 step:26474 [D loss: 0.577590, acc.: 70.31%] [G loss: 0.539600]\n",
      "epoch:28 step:26475 [D loss: 0.516959, acc.: 67.97%] [G loss: 0.633671]\n",
      "epoch:28 step:26476 [D loss: 0.564444, acc.: 67.19%] [G loss: 0.693140]\n",
      "epoch:28 step:26477 [D loss: 0.524932, acc.: 73.44%] [G loss: 0.652805]\n",
      "epoch:28 step:26478 [D loss: 0.491522, acc.: 75.78%] [G loss: 0.755297]\n",
      "epoch:28 step:26479 [D loss: 0.486227, acc.: 77.34%] [G loss: 0.685336]\n",
      "epoch:28 step:26480 [D loss: 0.497897, acc.: 73.44%] [G loss: 0.725888]\n",
      "epoch:28 step:26481 [D loss: 0.527389, acc.: 71.09%] [G loss: 0.733904]\n",
      "epoch:28 step:26482 [D loss: 0.483258, acc.: 75.78%] [G loss: 0.738959]\n",
      "epoch:28 step:26483 [D loss: 0.493702, acc.: 73.44%] [G loss: 0.700087]\n",
      "epoch:28 step:26484 [D loss: 0.464072, acc.: 73.44%] [G loss: 0.943642]\n",
      "epoch:28 step:26485 [D loss: 0.608177, acc.: 68.75%] [G loss: 0.795595]\n",
      "epoch:28 step:26486 [D loss: 0.567896, acc.: 67.97%] [G loss: 0.640829]\n",
      "epoch:28 step:26487 [D loss: 0.607483, acc.: 60.16%] [G loss: 0.673211]\n",
      "epoch:28 step:26488 [D loss: 0.505315, acc.: 72.66%] [G loss: 0.756644]\n",
      "epoch:28 step:26489 [D loss: 0.594421, acc.: 65.62%] [G loss: 0.565162]\n",
      "epoch:28 step:26490 [D loss: 0.480872, acc.: 74.22%] [G loss: 0.677953]\n",
      "epoch:28 step:26491 [D loss: 0.528374, acc.: 71.88%] [G loss: 0.548922]\n",
      "epoch:28 step:26492 [D loss: 0.535461, acc.: 70.31%] [G loss: 0.649179]\n",
      "epoch:28 step:26493 [D loss: 0.627904, acc.: 61.72%] [G loss: 0.503934]\n",
      "epoch:28 step:26494 [D loss: 0.560089, acc.: 65.62%] [G loss: 0.597635]\n",
      "epoch:28 step:26495 [D loss: 0.521860, acc.: 71.09%] [G loss: 0.578797]\n",
      "epoch:28 step:26496 [D loss: 0.587689, acc.: 61.72%] [G loss: 0.727653]\n",
      "epoch:28 step:26497 [D loss: 0.526798, acc.: 74.22%] [G loss: 0.676557]\n",
      "epoch:28 step:26498 [D loss: 0.524631, acc.: 70.31%] [G loss: 0.834107]\n",
      "epoch:28 step:26499 [D loss: 0.569643, acc.: 68.75%] [G loss: 0.593343]\n",
      "epoch:28 step:26500 [D loss: 0.523555, acc.: 70.31%] [G loss: 0.621905]\n",
      "epoch:28 step:26501 [D loss: 0.477103, acc.: 77.34%] [G loss: 0.750307]\n",
      "epoch:28 step:26502 [D loss: 0.635434, acc.: 64.06%] [G loss: 0.565255]\n",
      "epoch:28 step:26503 [D loss: 0.518336, acc.: 72.66%] [G loss: 0.692421]\n",
      "epoch:28 step:26504 [D loss: 0.537221, acc.: 68.75%] [G loss: 0.666763]\n",
      "epoch:28 step:26505 [D loss: 0.557194, acc.: 71.88%] [G loss: 0.598150]\n",
      "epoch:28 step:26506 [D loss: 0.458778, acc.: 77.34%] [G loss: 0.770606]\n",
      "epoch:28 step:26507 [D loss: 0.519880, acc.: 75.78%] [G loss: 0.761867]\n",
      "epoch:28 step:26508 [D loss: 0.534042, acc.: 73.44%] [G loss: 0.665386]\n",
      "epoch:28 step:26509 [D loss: 0.514280, acc.: 71.09%] [G loss: 0.778137]\n",
      "epoch:28 step:26510 [D loss: 0.464853, acc.: 77.34%] [G loss: 0.764532]\n",
      "epoch:28 step:26511 [D loss: 0.535964, acc.: 69.53%] [G loss: 0.792090]\n",
      "epoch:28 step:26512 [D loss: 0.459090, acc.: 82.81%] [G loss: 0.637532]\n",
      "epoch:28 step:26513 [D loss: 0.613484, acc.: 66.41%] [G loss: 0.679677]\n",
      "epoch:28 step:26514 [D loss: 0.686104, acc.: 64.06%] [G loss: 0.529077]\n",
      "epoch:28 step:26515 [D loss: 0.572226, acc.: 66.41%] [G loss: 0.672103]\n",
      "epoch:28 step:26516 [D loss: 0.588810, acc.: 61.72%] [G loss: 0.564156]\n",
      "epoch:28 step:26517 [D loss: 0.568577, acc.: 71.09%] [G loss: 0.698936]\n",
      "epoch:28 step:26518 [D loss: 0.581141, acc.: 67.97%] [G loss: 0.709543]\n",
      "epoch:28 step:26519 [D loss: 0.504319, acc.: 74.22%] [G loss: 0.614516]\n",
      "epoch:28 step:26520 [D loss: 0.551714, acc.: 66.41%] [G loss: 0.581553]\n",
      "epoch:28 step:26521 [D loss: 0.591321, acc.: 66.41%] [G loss: 0.490385]\n",
      "epoch:28 step:26522 [D loss: 0.511808, acc.: 73.44%] [G loss: 0.908884]\n",
      "epoch:28 step:26523 [D loss: 0.600023, acc.: 67.19%] [G loss: 0.773324]\n",
      "epoch:28 step:26524 [D loss: 0.558549, acc.: 64.84%] [G loss: 0.645253]\n",
      "epoch:28 step:26525 [D loss: 0.489683, acc.: 75.00%] [G loss: 0.623981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26526 [D loss: 0.583871, acc.: 70.31%] [G loss: 0.595185]\n",
      "epoch:28 step:26527 [D loss: 0.584721, acc.: 67.19%] [G loss: 0.560652]\n",
      "epoch:28 step:26528 [D loss: 0.508733, acc.: 78.12%] [G loss: 0.560916]\n",
      "epoch:28 step:26529 [D loss: 0.558181, acc.: 68.75%] [G loss: 0.645199]\n",
      "epoch:28 step:26530 [D loss: 0.584450, acc.: 64.84%] [G loss: 0.578240]\n",
      "epoch:28 step:26531 [D loss: 0.525459, acc.: 71.09%] [G loss: 0.645181]\n",
      "epoch:28 step:26532 [D loss: 0.427267, acc.: 78.12%] [G loss: 0.764799]\n",
      "epoch:28 step:26533 [D loss: 0.581873, acc.: 71.09%] [G loss: 0.499831]\n",
      "epoch:28 step:26534 [D loss: 0.455685, acc.: 79.69%] [G loss: 0.894567]\n",
      "epoch:28 step:26535 [D loss: 0.470077, acc.: 78.12%] [G loss: 0.752734]\n",
      "epoch:28 step:26536 [D loss: 0.519728, acc.: 74.22%] [G loss: 0.852300]\n",
      "epoch:28 step:26537 [D loss: 0.677347, acc.: 63.28%] [G loss: 0.726193]\n",
      "epoch:28 step:26538 [D loss: 0.511581, acc.: 73.44%] [G loss: 0.743946]\n",
      "epoch:28 step:26539 [D loss: 0.507773, acc.: 75.00%] [G loss: 0.709025]\n",
      "epoch:28 step:26540 [D loss: 0.471878, acc.: 77.34%] [G loss: 0.710571]\n",
      "epoch:28 step:26541 [D loss: 0.555083, acc.: 68.75%] [G loss: 0.757986]\n",
      "epoch:28 step:26542 [D loss: 0.500254, acc.: 76.56%] [G loss: 0.921105]\n",
      "epoch:28 step:26543 [D loss: 0.484735, acc.: 78.91%] [G loss: 0.798295]\n",
      "epoch:28 step:26544 [D loss: 0.627892, acc.: 67.19%] [G loss: 0.770123]\n",
      "epoch:28 step:26545 [D loss: 0.585160, acc.: 65.62%] [G loss: 0.709679]\n",
      "epoch:28 step:26546 [D loss: 0.557455, acc.: 67.19%] [G loss: 0.680246]\n",
      "epoch:28 step:26547 [D loss: 0.495129, acc.: 76.56%] [G loss: 0.785251]\n",
      "epoch:28 step:26548 [D loss: 0.475617, acc.: 82.81%] [G loss: 0.867556]\n",
      "epoch:28 step:26549 [D loss: 0.542630, acc.: 71.88%] [G loss: 0.749225]\n",
      "epoch:28 step:26550 [D loss: 0.426956, acc.: 82.03%] [G loss: 0.838335]\n",
      "epoch:28 step:26551 [D loss: 0.441431, acc.: 79.69%] [G loss: 1.204890]\n",
      "epoch:28 step:26552 [D loss: 0.700963, acc.: 65.62%] [G loss: 0.792178]\n",
      "epoch:28 step:26553 [D loss: 0.547322, acc.: 67.19%] [G loss: 0.735613]\n",
      "epoch:28 step:26554 [D loss: 0.462158, acc.: 77.34%] [G loss: 0.717309]\n",
      "epoch:28 step:26555 [D loss: 0.532568, acc.: 70.31%] [G loss: 0.720903]\n",
      "epoch:28 step:26556 [D loss: 0.528516, acc.: 72.66%] [G loss: 0.721028]\n",
      "epoch:28 step:26557 [D loss: 0.491170, acc.: 78.12%] [G loss: 0.681293]\n",
      "epoch:28 step:26558 [D loss: 0.591820, acc.: 68.75%] [G loss: 0.691354]\n",
      "epoch:28 step:26559 [D loss: 0.640389, acc.: 65.62%] [G loss: 0.622476]\n",
      "epoch:28 step:26560 [D loss: 0.530155, acc.: 72.66%] [G loss: 0.555986]\n",
      "epoch:28 step:26561 [D loss: 0.514940, acc.: 73.44%] [G loss: 0.822008]\n",
      "epoch:28 step:26562 [D loss: 0.445670, acc.: 78.12%] [G loss: 0.776421]\n",
      "epoch:28 step:26563 [D loss: 0.514163, acc.: 73.44%] [G loss: 0.887481]\n",
      "epoch:28 step:26564 [D loss: 0.516424, acc.: 75.78%] [G loss: 0.806590]\n",
      "epoch:28 step:26565 [D loss: 0.473408, acc.: 78.91%] [G loss: 0.876701]\n",
      "epoch:28 step:26566 [D loss: 0.603237, acc.: 66.41%] [G loss: 0.538952]\n",
      "epoch:28 step:26567 [D loss: 0.527070, acc.: 73.44%] [G loss: 0.546900]\n",
      "epoch:28 step:26568 [D loss: 0.571953, acc.: 68.75%] [G loss: 0.598672]\n",
      "epoch:28 step:26569 [D loss: 0.493729, acc.: 76.56%] [G loss: 0.502947]\n",
      "epoch:28 step:26570 [D loss: 0.516846, acc.: 75.78%] [G loss: 0.827572]\n",
      "epoch:28 step:26571 [D loss: 0.513491, acc.: 75.00%] [G loss: 0.744144]\n",
      "epoch:28 step:26572 [D loss: 0.502636, acc.: 73.44%] [G loss: 0.754953]\n",
      "epoch:28 step:26573 [D loss: 0.515726, acc.: 77.34%] [G loss: 0.875545]\n",
      "epoch:28 step:26574 [D loss: 0.564255, acc.: 68.75%] [G loss: 0.914455]\n",
      "epoch:28 step:26575 [D loss: 0.541379, acc.: 70.31%] [G loss: 0.747382]\n",
      "epoch:28 step:26576 [D loss: 0.474998, acc.: 76.56%] [G loss: 0.965650]\n",
      "epoch:28 step:26577 [D loss: 0.609585, acc.: 70.31%] [G loss: 0.820383]\n",
      "epoch:28 step:26578 [D loss: 0.652723, acc.: 62.50%] [G loss: 0.542560]\n",
      "epoch:28 step:26579 [D loss: 0.541502, acc.: 72.66%] [G loss: 0.808096]\n",
      "epoch:28 step:26580 [D loss: 0.460901, acc.: 79.69%] [G loss: 0.716773]\n",
      "epoch:28 step:26581 [D loss: 0.632749, acc.: 60.16%] [G loss: 0.850628]\n",
      "epoch:28 step:26582 [D loss: 0.522425, acc.: 70.31%] [G loss: 0.816694]\n",
      "epoch:28 step:26583 [D loss: 0.426231, acc.: 82.03%] [G loss: 0.874652]\n",
      "epoch:28 step:26584 [D loss: 0.583291, acc.: 68.75%] [G loss: 0.753379]\n",
      "epoch:28 step:26585 [D loss: 0.750250, acc.: 55.47%] [G loss: 0.460943]\n",
      "epoch:28 step:26586 [D loss: 0.490056, acc.: 76.56%] [G loss: 0.671314]\n",
      "epoch:28 step:26587 [D loss: 0.503610, acc.: 75.78%] [G loss: 0.686913]\n",
      "epoch:28 step:26588 [D loss: 0.536715, acc.: 70.31%] [G loss: 0.807237]\n",
      "epoch:28 step:26589 [D loss: 0.562392, acc.: 72.66%] [G loss: 0.676514]\n",
      "epoch:28 step:26590 [D loss: 0.443885, acc.: 83.59%] [G loss: 0.837541]\n",
      "epoch:28 step:26591 [D loss: 0.490376, acc.: 74.22%] [G loss: 0.787606]\n",
      "epoch:28 step:26592 [D loss: 0.527129, acc.: 75.78%] [G loss: 0.719186]\n",
      "epoch:28 step:26593 [D loss: 0.433268, acc.: 82.03%] [G loss: 0.917697]\n",
      "epoch:28 step:26594 [D loss: 0.429567, acc.: 80.47%] [G loss: 0.889688]\n",
      "epoch:28 step:26595 [D loss: 0.483347, acc.: 75.00%] [G loss: 0.997410]\n",
      "epoch:28 step:26596 [D loss: 0.453673, acc.: 78.91%] [G loss: 0.911223]\n",
      "epoch:28 step:26597 [D loss: 0.540544, acc.: 69.53%] [G loss: 0.712730]\n",
      "epoch:28 step:26598 [D loss: 0.532784, acc.: 74.22%] [G loss: 0.884718]\n",
      "epoch:28 step:26599 [D loss: 0.569369, acc.: 68.75%] [G loss: 0.740018]\n",
      "epoch:28 step:26600 [D loss: 0.565014, acc.: 68.75%] [G loss: 0.648924]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.806127\n",
      "FID: 48.914326\n",
      "0 = 12.806255288982369\n",
      "1 = 0.09420412067058616\n",
      "2 = 0.8411999940872192\n",
      "3 = 0.79339998960495\n",
      "4 = 0.8889999985694885\n",
      "5 = 0.8772667050361633\n",
      "6 = 0.79339998960495\n",
      "7 = 8.337724536120902\n",
      "8 = 0.14965481777624995\n",
      "9 = 0.6988999843597412\n",
      "10 = 0.7002000212669373\n",
      "11 = 0.6976000070571899\n",
      "12 = 0.6983842253684998\n",
      "13 = 0.7002000212669373\n",
      "14 = 6.80615758895874\n",
      "15 = 6.976519584655762\n",
      "16 = 0.3903595507144928\n",
      "17 = 6.806126594543457\n",
      "18 = 48.91432571411133\n",
      "epoch:28 step:26601 [D loss: 0.632212, acc.: 61.72%] [G loss: 0.663996]\n",
      "epoch:28 step:26602 [D loss: 0.557468, acc.: 70.31%] [G loss: 0.726337]\n",
      "epoch:28 step:26603 [D loss: 0.580512, acc.: 69.53%] [G loss: 0.756006]\n",
      "epoch:28 step:26604 [D loss: 0.533747, acc.: 70.31%] [G loss: 0.745817]\n",
      "epoch:28 step:26605 [D loss: 0.512026, acc.: 75.00%] [G loss: 0.886963]\n",
      "epoch:28 step:26606 [D loss: 0.510298, acc.: 75.78%] [G loss: 0.638290]\n",
      "epoch:28 step:26607 [D loss: 0.525302, acc.: 71.09%] [G loss: 0.771342]\n",
      "epoch:28 step:26608 [D loss: 0.486659, acc.: 74.22%] [G loss: 0.742213]\n",
      "epoch:28 step:26609 [D loss: 0.541079, acc.: 71.88%] [G loss: 0.657002]\n",
      "epoch:28 step:26610 [D loss: 0.460878, acc.: 75.00%] [G loss: 0.772426]\n",
      "epoch:28 step:26611 [D loss: 0.536271, acc.: 68.75%] [G loss: 0.829420]\n",
      "epoch:28 step:26612 [D loss: 0.720523, acc.: 59.38%] [G loss: 0.561845]\n",
      "epoch:28 step:26613 [D loss: 0.532542, acc.: 67.97%] [G loss: 0.533804]\n",
      "epoch:28 step:26614 [D loss: 0.559600, acc.: 65.62%] [G loss: 0.563164]\n",
      "epoch:28 step:26615 [D loss: 0.542562, acc.: 73.44%] [G loss: 0.648888]\n",
      "epoch:28 step:26616 [D loss: 0.567986, acc.: 71.09%] [G loss: 0.563235]\n",
      "epoch:28 step:26617 [D loss: 0.427109, acc.: 78.91%] [G loss: 0.790854]\n",
      "epoch:28 step:26618 [D loss: 0.504254, acc.: 76.56%] [G loss: 0.644075]\n",
      "epoch:28 step:26619 [D loss: 0.535686, acc.: 72.66%] [G loss: 0.755359]\n",
      "epoch:28 step:26620 [D loss: 0.575605, acc.: 70.31%] [G loss: 0.643415]\n",
      "epoch:28 step:26621 [D loss: 0.442028, acc.: 77.34%] [G loss: 0.787571]\n",
      "epoch:28 step:26622 [D loss: 0.581084, acc.: 67.97%] [G loss: 0.589810]\n",
      "epoch:28 step:26623 [D loss: 0.581088, acc.: 68.75%] [G loss: 0.614512]\n",
      "epoch:28 step:26624 [D loss: 0.590345, acc.: 71.09%] [G loss: 0.640498]\n",
      "epoch:28 step:26625 [D loss: 0.530378, acc.: 74.22%] [G loss: 0.552972]\n",
      "epoch:28 step:26626 [D loss: 0.592683, acc.: 66.41%] [G loss: 0.644461]\n",
      "epoch:28 step:26627 [D loss: 0.537013, acc.: 72.66%] [G loss: 0.580726]\n",
      "epoch:28 step:26628 [D loss: 0.484501, acc.: 71.09%] [G loss: 0.710948]\n",
      "epoch:28 step:26629 [D loss: 0.592361, acc.: 60.94%] [G loss: 0.700815]\n",
      "epoch:28 step:26630 [D loss: 0.551342, acc.: 68.75%] [G loss: 0.824960]\n",
      "epoch:28 step:26631 [D loss: 0.602105, acc.: 67.19%] [G loss: 0.586972]\n",
      "epoch:28 step:26632 [D loss: 0.527654, acc.: 68.75%] [G loss: 0.735728]\n",
      "epoch:28 step:26633 [D loss: 0.546637, acc.: 67.97%] [G loss: 0.714805]\n",
      "epoch:28 step:26634 [D loss: 0.467601, acc.: 75.78%] [G loss: 0.815232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26635 [D loss: 0.478410, acc.: 75.78%] [G loss: 0.955783]\n",
      "epoch:28 step:26636 [D loss: 0.699892, acc.: 53.91%] [G loss: 0.619026]\n",
      "epoch:28 step:26637 [D loss: 0.655546, acc.: 60.16%] [G loss: 0.549875]\n",
      "epoch:28 step:26638 [D loss: 0.502923, acc.: 71.09%] [G loss: 0.770089]\n",
      "epoch:28 step:26639 [D loss: 0.486497, acc.: 72.66%] [G loss: 0.799764]\n",
      "epoch:28 step:26640 [D loss: 0.574067, acc.: 66.41%] [G loss: 0.679262]\n",
      "epoch:28 step:26641 [D loss: 0.550314, acc.: 65.62%] [G loss: 0.808955]\n",
      "epoch:28 step:26642 [D loss: 0.514608, acc.: 72.66%] [G loss: 0.984999]\n",
      "epoch:28 step:26643 [D loss: 0.641437, acc.: 66.41%] [G loss: 0.728522]\n",
      "epoch:28 step:26644 [D loss: 0.591877, acc.: 62.50%] [G loss: 0.961122]\n",
      "epoch:28 step:26645 [D loss: 0.571998, acc.: 66.41%] [G loss: 0.762399]\n",
      "epoch:28 step:26646 [D loss: 0.556538, acc.: 69.53%] [G loss: 0.698942]\n",
      "epoch:28 step:26647 [D loss: 0.612380, acc.: 63.28%] [G loss: 0.626271]\n",
      "epoch:28 step:26648 [D loss: 0.602053, acc.: 61.72%] [G loss: 0.565692]\n",
      "epoch:28 step:26649 [D loss: 0.495407, acc.: 71.09%] [G loss: 0.561924]\n",
      "epoch:28 step:26650 [D loss: 0.517544, acc.: 72.66%] [G loss: 0.752372]\n",
      "epoch:28 step:26651 [D loss: 0.556685, acc.: 68.75%] [G loss: 0.649262]\n",
      "epoch:28 step:26652 [D loss: 0.486006, acc.: 75.78%] [G loss: 0.782637]\n",
      "epoch:28 step:26653 [D loss: 0.556504, acc.: 71.88%] [G loss: 0.878057]\n",
      "epoch:28 step:26654 [D loss: 0.575156, acc.: 70.31%] [G loss: 0.743605]\n",
      "epoch:28 step:26655 [D loss: 0.558491, acc.: 67.97%] [G loss: 0.846892]\n",
      "epoch:28 step:26656 [D loss: 0.652767, acc.: 54.69%] [G loss: 0.696598]\n",
      "epoch:28 step:26657 [D loss: 0.578030, acc.: 71.09%] [G loss: 0.675637]\n",
      "epoch:28 step:26658 [D loss: 0.618596, acc.: 66.41%] [G loss: 0.644471]\n",
      "epoch:28 step:26659 [D loss: 0.562540, acc.: 67.19%] [G loss: 0.608327]\n",
      "epoch:28 step:26660 [D loss: 0.597681, acc.: 65.62%] [G loss: 0.506699]\n",
      "epoch:28 step:26661 [D loss: 0.513013, acc.: 75.00%] [G loss: 0.834532]\n",
      "epoch:28 step:26662 [D loss: 0.529793, acc.: 71.88%] [G loss: 0.747134]\n",
      "epoch:28 step:26663 [D loss: 0.466220, acc.: 74.22%] [G loss: 0.805299]\n",
      "epoch:28 step:26664 [D loss: 0.511411, acc.: 71.09%] [G loss: 0.747596]\n",
      "epoch:28 step:26665 [D loss: 0.491473, acc.: 72.66%] [G loss: 0.855701]\n",
      "epoch:28 step:26666 [D loss: 0.515472, acc.: 75.78%] [G loss: 0.723806]\n",
      "epoch:28 step:26667 [D loss: 0.509762, acc.: 70.31%] [G loss: 0.818835]\n",
      "epoch:28 step:26668 [D loss: 0.583358, acc.: 67.97%] [G loss: 0.827250]\n",
      "epoch:28 step:26669 [D loss: 0.535185, acc.: 71.88%] [G loss: 0.818572]\n",
      "epoch:28 step:26670 [D loss: 0.532459, acc.: 72.66%] [G loss: 0.725701]\n",
      "epoch:28 step:26671 [D loss: 0.512380, acc.: 73.44%] [G loss: 0.751140]\n",
      "epoch:28 step:26672 [D loss: 0.527980, acc.: 74.22%] [G loss: 0.931264]\n",
      "epoch:28 step:26673 [D loss: 0.629958, acc.: 67.97%] [G loss: 0.628412]\n",
      "epoch:28 step:26674 [D loss: 0.559057, acc.: 68.75%] [G loss: 0.456775]\n",
      "epoch:28 step:26675 [D loss: 0.516472, acc.: 76.56%] [G loss: 0.786576]\n",
      "epoch:28 step:26676 [D loss: 0.484539, acc.: 73.44%] [G loss: 0.654255]\n",
      "epoch:28 step:26677 [D loss: 0.546240, acc.: 71.09%] [G loss: 0.917988]\n",
      "epoch:28 step:26678 [D loss: 0.541716, acc.: 70.31%] [G loss: 1.030181]\n",
      "epoch:28 step:26679 [D loss: 0.571038, acc.: 70.31%] [G loss: 0.846380]\n",
      "epoch:28 step:26680 [D loss: 0.443265, acc.: 78.12%] [G loss: 0.776624]\n",
      "epoch:28 step:26681 [D loss: 0.582058, acc.: 66.41%] [G loss: 0.814273]\n",
      "epoch:28 step:26682 [D loss: 0.509834, acc.: 76.56%] [G loss: 0.737980]\n",
      "epoch:28 step:26683 [D loss: 0.564417, acc.: 65.62%] [G loss: 0.784699]\n",
      "epoch:28 step:26684 [D loss: 0.535359, acc.: 69.53%] [G loss: 0.852408]\n",
      "epoch:28 step:26685 [D loss: 0.520444, acc.: 75.00%] [G loss: 0.913873]\n",
      "epoch:28 step:26686 [D loss: 0.447228, acc.: 78.12%] [G loss: 1.132792]\n",
      "epoch:28 step:26687 [D loss: 0.394363, acc.: 84.38%] [G loss: 1.037020]\n",
      "epoch:28 step:26688 [D loss: 0.461739, acc.: 79.69%] [G loss: 0.912915]\n",
      "epoch:28 step:26689 [D loss: 0.512296, acc.: 74.22%] [G loss: 0.627988]\n",
      "epoch:28 step:26690 [D loss: 0.559106, acc.: 71.09%] [G loss: 0.580971]\n",
      "epoch:28 step:26691 [D loss: 0.596939, acc.: 67.97%] [G loss: 0.731364]\n",
      "epoch:28 step:26692 [D loss: 0.595845, acc.: 67.97%] [G loss: 0.686186]\n",
      "epoch:28 step:26693 [D loss: 0.451965, acc.: 78.12%] [G loss: 0.735489]\n",
      "epoch:28 step:26694 [D loss: 0.643148, acc.: 66.41%] [G loss: 0.832936]\n",
      "epoch:28 step:26695 [D loss: 0.554453, acc.: 67.19%] [G loss: 0.732045]\n",
      "epoch:28 step:26696 [D loss: 0.491402, acc.: 74.22%] [G loss: 0.907534]\n",
      "epoch:28 step:26697 [D loss: 0.471106, acc.: 74.22%] [G loss: 0.688833]\n",
      "epoch:28 step:26698 [D loss: 0.594452, acc.: 64.06%] [G loss: 0.669597]\n",
      "epoch:28 step:26699 [D loss: 0.550396, acc.: 69.53%] [G loss: 0.667355]\n",
      "epoch:28 step:26700 [D loss: 0.511024, acc.: 75.78%] [G loss: 0.689629]\n",
      "epoch:28 step:26701 [D loss: 0.568542, acc.: 68.75%] [G loss: 0.580320]\n",
      "epoch:28 step:26702 [D loss: 0.587934, acc.: 67.97%] [G loss: 0.567085]\n",
      "epoch:28 step:26703 [D loss: 0.561655, acc.: 71.09%] [G loss: 0.599035]\n",
      "epoch:28 step:26704 [D loss: 0.524265, acc.: 71.09%] [G loss: 0.755274]\n",
      "epoch:28 step:26705 [D loss: 0.552093, acc.: 69.53%] [G loss: 0.754740]\n",
      "epoch:28 step:26706 [D loss: 0.528893, acc.: 73.44%] [G loss: 0.876083]\n",
      "epoch:28 step:26707 [D loss: 0.507740, acc.: 71.88%] [G loss: 0.907289]\n",
      "epoch:28 step:26708 [D loss: 0.419865, acc.: 82.03%] [G loss: 0.817142]\n",
      "epoch:28 step:26709 [D loss: 0.633016, acc.: 58.59%] [G loss: 0.591201]\n",
      "epoch:28 step:26710 [D loss: 0.493910, acc.: 78.91%] [G loss: 0.867823]\n",
      "epoch:28 step:26711 [D loss: 0.451513, acc.: 78.12%] [G loss: 0.896114]\n",
      "epoch:28 step:26712 [D loss: 0.517628, acc.: 77.34%] [G loss: 0.960989]\n",
      "epoch:28 step:26713 [D loss: 0.638594, acc.: 64.84%] [G loss: 0.639624]\n",
      "epoch:28 step:26714 [D loss: 0.604319, acc.: 67.97%] [G loss: 0.617741]\n",
      "epoch:28 step:26715 [D loss: 0.540871, acc.: 72.66%] [G loss: 0.841369]\n",
      "epoch:28 step:26716 [D loss: 0.560001, acc.: 70.31%] [G loss: 0.693051]\n",
      "epoch:28 step:26717 [D loss: 0.469781, acc.: 76.56%] [G loss: 0.742734]\n",
      "epoch:28 step:26718 [D loss: 0.660624, acc.: 63.28%] [G loss: 0.620506]\n",
      "epoch:28 step:26719 [D loss: 0.489577, acc.: 77.34%] [G loss: 0.701806]\n",
      "epoch:28 step:26720 [D loss: 0.504148, acc.: 74.22%] [G loss: 0.765479]\n",
      "epoch:28 step:26721 [D loss: 0.508289, acc.: 75.78%] [G loss: 0.862287]\n",
      "epoch:28 step:26722 [D loss: 0.558744, acc.: 67.97%] [G loss: 0.630959]\n",
      "epoch:28 step:26723 [D loss: 0.569121, acc.: 67.97%] [G loss: 0.605749]\n",
      "epoch:28 step:26724 [D loss: 0.503469, acc.: 72.66%] [G loss: 0.783471]\n",
      "epoch:28 step:26725 [D loss: 0.467058, acc.: 77.34%] [G loss: 0.797935]\n",
      "epoch:28 step:26726 [D loss: 0.548355, acc.: 71.88%] [G loss: 0.812253]\n",
      "epoch:28 step:26727 [D loss: 0.522621, acc.: 72.66%] [G loss: 0.742960]\n",
      "epoch:28 step:26728 [D loss: 0.553811, acc.: 67.97%] [G loss: 0.599056]\n",
      "epoch:28 step:26729 [D loss: 0.514817, acc.: 77.34%] [G loss: 0.754001]\n",
      "epoch:28 step:26730 [D loss: 0.520722, acc.: 75.78%] [G loss: 0.635301]\n",
      "epoch:28 step:26731 [D loss: 0.547472, acc.: 71.09%] [G loss: 0.702797]\n",
      "epoch:28 step:26732 [D loss: 0.500148, acc.: 78.91%] [G loss: 0.787456]\n",
      "epoch:28 step:26733 [D loss: 0.614879, acc.: 66.41%] [G loss: 0.685631]\n",
      "epoch:28 step:26734 [D loss: 0.546607, acc.: 75.78%] [G loss: 0.695383]\n",
      "epoch:28 step:26735 [D loss: 0.477035, acc.: 75.00%] [G loss: 0.989891]\n",
      "epoch:28 step:26736 [D loss: 0.529676, acc.: 74.22%] [G loss: 0.749991]\n",
      "epoch:28 step:26737 [D loss: 0.605878, acc.: 68.75%] [G loss: 0.576065]\n",
      "epoch:28 step:26738 [D loss: 0.599629, acc.: 67.97%] [G loss: 0.400888]\n",
      "epoch:28 step:26739 [D loss: 0.496150, acc.: 75.78%] [G loss: 0.630383]\n",
      "epoch:28 step:26740 [D loss: 0.523073, acc.: 74.22%] [G loss: 0.604214]\n",
      "epoch:28 step:26741 [D loss: 0.459949, acc.: 80.47%] [G loss: 0.850313]\n",
      "epoch:28 step:26742 [D loss: 0.490034, acc.: 75.78%] [G loss: 0.774124]\n",
      "epoch:28 step:26743 [D loss: 0.541043, acc.: 74.22%] [G loss: 0.774235]\n",
      "epoch:28 step:26744 [D loss: 0.443334, acc.: 78.12%] [G loss: 1.017170]\n",
      "epoch:28 step:26745 [D loss: 0.581450, acc.: 71.88%] [G loss: 1.217398]\n",
      "epoch:28 step:26746 [D loss: 0.571766, acc.: 71.88%] [G loss: 0.871646]\n",
      "epoch:28 step:26747 [D loss: 0.675436, acc.: 65.62%] [G loss: 0.707862]\n",
      "epoch:28 step:26748 [D loss: 0.627238, acc.: 59.38%] [G loss: 0.544796]\n",
      "epoch:28 step:26749 [D loss: 0.471291, acc.: 76.56%] [G loss: 0.678136]\n",
      "epoch:28 step:26750 [D loss: 0.538933, acc.: 68.75%] [G loss: 0.686707]\n",
      "epoch:28 step:26751 [D loss: 0.554483, acc.: 71.09%] [G loss: 0.877903]\n",
      "epoch:28 step:26752 [D loss: 0.498490, acc.: 76.56%] [G loss: 0.718072]\n",
      "epoch:28 step:26753 [D loss: 0.466109, acc.: 74.22%] [G loss: 0.789489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26754 [D loss: 0.562169, acc.: 64.84%] [G loss: 0.762083]\n",
      "epoch:28 step:26755 [D loss: 0.486082, acc.: 74.22%] [G loss: 0.744159]\n",
      "epoch:28 step:26756 [D loss: 0.460054, acc.: 75.78%] [G loss: 0.873968]\n",
      "epoch:28 step:26757 [D loss: 0.477805, acc.: 76.56%] [G loss: 0.820651]\n",
      "epoch:28 step:26758 [D loss: 0.519184, acc.: 77.34%] [G loss: 0.829194]\n",
      "epoch:28 step:26759 [D loss: 0.450452, acc.: 75.78%] [G loss: 1.079736]\n",
      "epoch:28 step:26760 [D loss: 0.588142, acc.: 71.09%] [G loss: 0.817670]\n",
      "epoch:28 step:26761 [D loss: 0.593893, acc.: 67.97%] [G loss: 0.727281]\n",
      "epoch:28 step:26762 [D loss: 0.520072, acc.: 69.53%] [G loss: 0.517111]\n",
      "epoch:28 step:26763 [D loss: 0.559770, acc.: 69.53%] [G loss: 0.651591]\n",
      "epoch:28 step:26764 [D loss: 0.690189, acc.: 60.94%] [G loss: 0.604052]\n",
      "epoch:28 step:26765 [D loss: 0.579021, acc.: 69.53%] [G loss: 0.768760]\n",
      "epoch:28 step:26766 [D loss: 0.521240, acc.: 75.78%] [G loss: 0.949760]\n",
      "epoch:28 step:26767 [D loss: 0.610774, acc.: 67.19%] [G loss: 0.614745]\n",
      "epoch:28 step:26768 [D loss: 0.538571, acc.: 71.88%] [G loss: 0.564244]\n",
      "epoch:28 step:26769 [D loss: 0.531370, acc.: 70.31%] [G loss: 0.693401]\n",
      "epoch:28 step:26770 [D loss: 0.524720, acc.: 71.09%] [G loss: 0.723518]\n",
      "epoch:28 step:26771 [D loss: 0.593776, acc.: 64.84%] [G loss: 0.717510]\n",
      "epoch:28 step:26772 [D loss: 0.499625, acc.: 75.00%] [G loss: 0.656633]\n",
      "epoch:28 step:26773 [D loss: 0.569725, acc.: 71.88%] [G loss: 0.715407]\n",
      "epoch:28 step:26774 [D loss: 0.549660, acc.: 70.31%] [G loss: 0.569442]\n",
      "epoch:28 step:26775 [D loss: 0.558310, acc.: 70.31%] [G loss: 0.562260]\n",
      "epoch:28 step:26776 [D loss: 0.573052, acc.: 71.88%] [G loss: 0.760206]\n",
      "epoch:28 step:26777 [D loss: 0.500425, acc.: 75.78%] [G loss: 0.704658]\n",
      "epoch:28 step:26778 [D loss: 0.588863, acc.: 68.75%] [G loss: 0.672239]\n",
      "epoch:28 step:26779 [D loss: 0.527441, acc.: 71.09%] [G loss: 0.711290]\n",
      "epoch:28 step:26780 [D loss: 0.606143, acc.: 71.09%] [G loss: 0.762787]\n",
      "epoch:28 step:26781 [D loss: 0.519529, acc.: 73.44%] [G loss: 0.644386]\n",
      "epoch:28 step:26782 [D loss: 0.454008, acc.: 79.69%] [G loss: 0.862121]\n",
      "epoch:28 step:26783 [D loss: 0.594700, acc.: 68.75%] [G loss: 1.028659]\n",
      "epoch:28 step:26784 [D loss: 0.589920, acc.: 69.53%] [G loss: 0.655174]\n",
      "epoch:28 step:26785 [D loss: 0.509662, acc.: 71.88%] [G loss: 0.639184]\n",
      "epoch:28 step:26786 [D loss: 0.543856, acc.: 68.75%] [G loss: 0.755093]\n",
      "epoch:28 step:26787 [D loss: 0.477175, acc.: 78.91%] [G loss: 0.795957]\n",
      "epoch:28 step:26788 [D loss: 0.513206, acc.: 73.44%] [G loss: 0.793743]\n",
      "epoch:28 step:26789 [D loss: 0.612321, acc.: 64.84%] [G loss: 0.700466]\n",
      "epoch:28 step:26790 [D loss: 0.450605, acc.: 80.47%] [G loss: 0.748497]\n",
      "epoch:28 step:26791 [D loss: 0.455429, acc.: 79.69%] [G loss: 0.766373]\n",
      "epoch:28 step:26792 [D loss: 0.520303, acc.: 75.78%] [G loss: 0.783329]\n",
      "epoch:28 step:26793 [D loss: 0.509131, acc.: 73.44%] [G loss: 0.849249]\n",
      "epoch:28 step:26794 [D loss: 0.466085, acc.: 78.12%] [G loss: 0.733997]\n",
      "epoch:28 step:26795 [D loss: 0.638861, acc.: 61.72%] [G loss: 0.704444]\n",
      "epoch:28 step:26796 [D loss: 0.528205, acc.: 71.09%] [G loss: 0.611669]\n",
      "epoch:28 step:26797 [D loss: 0.571927, acc.: 73.44%] [G loss: 0.655517]\n",
      "epoch:28 step:26798 [D loss: 0.555245, acc.: 69.53%] [G loss: 0.822198]\n",
      "epoch:28 step:26799 [D loss: 0.557854, acc.: 69.53%] [G loss: 0.692661]\n",
      "epoch:28 step:26800 [D loss: 0.451251, acc.: 79.69%] [G loss: 0.873679]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.721105\n",
      "FID: 50.930672\n",
      "0 = 12.859390055942567\n",
      "1 = 0.09308259548986525\n",
      "2 = 0.8442999720573425\n",
      "3 = 0.8033999800682068\n",
      "4 = 0.885200023651123\n",
      "5 = 0.8749727606773376\n",
      "6 = 0.8033999800682068\n",
      "7 = 8.377444709908948\n",
      "8 = 0.1516303942696504\n",
      "9 = 0.6883000135421753\n",
      "10 = 0.6782000064849854\n",
      "11 = 0.6984000205993652\n",
      "12 = 0.6921820640563965\n",
      "13 = 0.6782000064849854\n",
      "14 = 6.721132278442383\n",
      "15 = 6.827091217041016\n",
      "16 = 0.4007824957370758\n",
      "17 = 6.721105098724365\n",
      "18 = 50.93067169189453\n",
      "epoch:28 step:26801 [D loss: 0.551653, acc.: 69.53%] [G loss: 0.879730]\n",
      "epoch:28 step:26802 [D loss: 0.662301, acc.: 64.84%] [G loss: 0.778675]\n",
      "epoch:28 step:26803 [D loss: 0.469842, acc.: 75.00%] [G loss: 0.778726]\n",
      "epoch:28 step:26804 [D loss: 0.516267, acc.: 75.78%] [G loss: 0.863969]\n",
      "epoch:28 step:26805 [D loss: 0.534162, acc.: 72.66%] [G loss: 0.693754]\n",
      "epoch:28 step:26806 [D loss: 0.521613, acc.: 76.56%] [G loss: 0.649176]\n",
      "epoch:28 step:26807 [D loss: 0.538812, acc.: 68.75%] [G loss: 0.640965]\n",
      "epoch:28 step:26808 [D loss: 0.519685, acc.: 70.31%] [G loss: 0.823995]\n",
      "epoch:28 step:26809 [D loss: 0.526222, acc.: 74.22%] [G loss: 0.697909]\n",
      "epoch:28 step:26810 [D loss: 0.537915, acc.: 68.75%] [G loss: 0.671942]\n",
      "epoch:28 step:26811 [D loss: 0.455571, acc.: 82.81%] [G loss: 1.253616]\n",
      "epoch:28 step:26812 [D loss: 0.603932, acc.: 65.62%] [G loss: 0.751409]\n",
      "epoch:28 step:26813 [D loss: 0.531690, acc.: 71.88%] [G loss: 0.727479]\n",
      "epoch:28 step:26814 [D loss: 0.540282, acc.: 71.09%] [G loss: 0.593277]\n",
      "epoch:28 step:26815 [D loss: 0.515270, acc.: 72.66%] [G loss: 0.766096]\n",
      "epoch:28 step:26816 [D loss: 0.547062, acc.: 72.66%] [G loss: 0.747425]\n",
      "epoch:28 step:26817 [D loss: 0.543879, acc.: 70.31%] [G loss: 0.664377]\n",
      "epoch:28 step:26818 [D loss: 0.433515, acc.: 78.91%] [G loss: 0.906983]\n",
      "epoch:28 step:26819 [D loss: 0.572418, acc.: 70.31%] [G loss: 0.886729]\n",
      "epoch:28 step:26820 [D loss: 0.641391, acc.: 65.62%] [G loss: 0.661620]\n",
      "epoch:28 step:26821 [D loss: 0.514020, acc.: 76.56%] [G loss: 0.750648]\n",
      "epoch:28 step:26822 [D loss: 0.532872, acc.: 76.56%] [G loss: 0.671317]\n",
      "epoch:28 step:26823 [D loss: 0.568679, acc.: 71.09%] [G loss: 0.714649]\n",
      "epoch:28 step:26824 [D loss: 0.588538, acc.: 64.84%] [G loss: 0.580733]\n",
      "epoch:28 step:26825 [D loss: 0.540453, acc.: 65.62%] [G loss: 0.741719]\n",
      "epoch:28 step:26826 [D loss: 0.535984, acc.: 71.88%] [G loss: 0.668537]\n",
      "epoch:28 step:26827 [D loss: 0.624291, acc.: 65.62%] [G loss: 0.641102]\n",
      "epoch:28 step:26828 [D loss: 0.490919, acc.: 76.56%] [G loss: 0.740656]\n",
      "epoch:28 step:26829 [D loss: 0.478302, acc.: 77.34%] [G loss: 0.846689]\n",
      "epoch:28 step:26830 [D loss: 0.554919, acc.: 73.44%] [G loss: 0.750013]\n",
      "epoch:28 step:26831 [D loss: 0.598366, acc.: 64.06%] [G loss: 0.572676]\n",
      "epoch:28 step:26832 [D loss: 0.500254, acc.: 73.44%] [G loss: 0.600060]\n",
      "epoch:28 step:26833 [D loss: 0.543867, acc.: 70.31%] [G loss: 0.877126]\n",
      "epoch:28 step:26834 [D loss: 0.571505, acc.: 67.97%] [G loss: 0.646638]\n",
      "epoch:28 step:26835 [D loss: 0.516892, acc.: 74.22%] [G loss: 0.708524]\n",
      "epoch:28 step:26836 [D loss: 0.549625, acc.: 71.09%] [G loss: 0.599371]\n",
      "epoch:28 step:26837 [D loss: 0.437903, acc.: 78.91%] [G loss: 0.756178]\n",
      "epoch:28 step:26838 [D loss: 0.538524, acc.: 68.75%] [G loss: 0.781489]\n",
      "epoch:28 step:26839 [D loss: 0.506688, acc.: 72.66%] [G loss: 0.613150]\n",
      "epoch:28 step:26840 [D loss: 0.533799, acc.: 73.44%] [G loss: 0.834688]\n",
      "epoch:28 step:26841 [D loss: 0.444660, acc.: 80.47%] [G loss: 0.792882]\n",
      "epoch:28 step:26842 [D loss: 0.597183, acc.: 69.53%] [G loss: 0.768585]\n",
      "epoch:28 step:26843 [D loss: 0.525559, acc.: 71.88%] [G loss: 0.700642]\n",
      "epoch:28 step:26844 [D loss: 0.548810, acc.: 67.97%] [G loss: 0.614158]\n",
      "epoch:28 step:26845 [D loss: 0.508785, acc.: 74.22%] [G loss: 0.525353]\n",
      "epoch:28 step:26846 [D loss: 0.577244, acc.: 65.62%] [G loss: 0.613364]\n",
      "epoch:28 step:26847 [D loss: 0.586659, acc.: 65.62%] [G loss: 0.476699]\n",
      "epoch:28 step:26848 [D loss: 0.544916, acc.: 67.19%] [G loss: 0.678498]\n",
      "epoch:28 step:26849 [D loss: 0.493254, acc.: 75.78%] [G loss: 0.610219]\n",
      "epoch:28 step:26850 [D loss: 0.595627, acc.: 68.75%] [G loss: 0.617955]\n",
      "epoch:28 step:26851 [D loss: 0.557420, acc.: 69.53%] [G loss: 0.849831]\n",
      "epoch:28 step:26852 [D loss: 0.564582, acc.: 72.66%] [G loss: 0.788976]\n",
      "epoch:28 step:26853 [D loss: 0.515904, acc.: 75.78%] [G loss: 0.925165]\n",
      "epoch:28 step:26854 [D loss: 0.597134, acc.: 68.75%] [G loss: 0.809348]\n",
      "epoch:28 step:26855 [D loss: 0.532652, acc.: 69.53%] [G loss: 0.771629]\n",
      "epoch:28 step:26856 [D loss: 0.527979, acc.: 67.97%] [G loss: 0.659843]\n",
      "epoch:28 step:26857 [D loss: 0.534806, acc.: 74.22%] [G loss: 0.817568]\n",
      "epoch:28 step:26858 [D loss: 0.575641, acc.: 67.19%] [G loss: 0.599430]\n",
      "epoch:28 step:26859 [D loss: 0.463057, acc.: 77.34%] [G loss: 0.715220]\n",
      "epoch:28 step:26860 [D loss: 0.488727, acc.: 75.78%] [G loss: 0.752506]\n",
      "epoch:28 step:26861 [D loss: 0.562279, acc.: 64.06%] [G loss: 0.663916]\n",
      "epoch:28 step:26862 [D loss: 0.522053, acc.: 72.66%] [G loss: 0.710641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26863 [D loss: 0.538556, acc.: 67.19%] [G loss: 0.753087]\n",
      "epoch:28 step:26864 [D loss: 0.538974, acc.: 72.66%] [G loss: 0.748427]\n",
      "epoch:28 step:26865 [D loss: 0.498733, acc.: 70.31%] [G loss: 0.784184]\n",
      "epoch:28 step:26866 [D loss: 0.469532, acc.: 79.69%] [G loss: 0.911604]\n",
      "epoch:28 step:26867 [D loss: 0.467065, acc.: 78.91%] [G loss: 0.865658]\n",
      "epoch:28 step:26868 [D loss: 0.480934, acc.: 77.34%] [G loss: 0.661815]\n",
      "epoch:28 step:26869 [D loss: 0.542494, acc.: 71.88%] [G loss: 0.681773]\n",
      "epoch:28 step:26870 [D loss: 0.457753, acc.: 81.25%] [G loss: 0.851131]\n",
      "epoch:28 step:26871 [D loss: 0.513155, acc.: 72.66%] [G loss: 0.742311]\n",
      "epoch:28 step:26872 [D loss: 0.588125, acc.: 67.19%] [G loss: 0.719671]\n",
      "epoch:28 step:26873 [D loss: 0.610009, acc.: 66.41%] [G loss: 0.610967]\n",
      "epoch:28 step:26874 [D loss: 0.501433, acc.: 75.00%] [G loss: 0.670031]\n",
      "epoch:28 step:26875 [D loss: 0.512056, acc.: 71.88%] [G loss: 0.740052]\n",
      "epoch:28 step:26876 [D loss: 0.586755, acc.: 64.84%] [G loss: 0.706236]\n",
      "epoch:28 step:26877 [D loss: 0.491340, acc.: 73.44%] [G loss: 0.806636]\n",
      "epoch:28 step:26878 [D loss: 0.506617, acc.: 71.88%] [G loss: 0.939101]\n",
      "epoch:28 step:26879 [D loss: 0.516776, acc.: 71.09%] [G loss: 0.836285]\n",
      "epoch:28 step:26880 [D loss: 0.531890, acc.: 68.75%] [G loss: 0.737525]\n",
      "epoch:28 step:26881 [D loss: 0.497693, acc.: 71.88%] [G loss: 0.803497]\n",
      "epoch:28 step:26882 [D loss: 0.548223, acc.: 71.09%] [G loss: 0.779504]\n",
      "epoch:28 step:26883 [D loss: 0.473930, acc.: 79.69%] [G loss: 0.854291]\n",
      "epoch:28 step:26884 [D loss: 0.444573, acc.: 78.12%] [G loss: 1.024764]\n",
      "epoch:28 step:26885 [D loss: 0.451993, acc.: 78.91%] [G loss: 0.909045]\n",
      "epoch:28 step:26886 [D loss: 0.513455, acc.: 75.78%] [G loss: 0.819074]\n",
      "epoch:28 step:26887 [D loss: 0.495097, acc.: 75.78%] [G loss: 0.992596]\n",
      "epoch:28 step:26888 [D loss: 0.566093, acc.: 67.97%] [G loss: 0.709080]\n",
      "epoch:28 step:26889 [D loss: 0.573076, acc.: 64.06%] [G loss: 0.738598]\n",
      "epoch:28 step:26890 [D loss: 0.432131, acc.: 79.69%] [G loss: 0.767679]\n",
      "epoch:28 step:26891 [D loss: 0.571223, acc.: 63.28%] [G loss: 0.659198]\n",
      "epoch:28 step:26892 [D loss: 0.558349, acc.: 70.31%] [G loss: 0.674900]\n",
      "epoch:28 step:26893 [D loss: 0.488133, acc.: 73.44%] [G loss: 0.770133]\n",
      "epoch:28 step:26894 [D loss: 0.552328, acc.: 72.66%] [G loss: 0.843258]\n",
      "epoch:28 step:26895 [D loss: 0.520731, acc.: 75.78%] [G loss: 0.789353]\n",
      "epoch:28 step:26896 [D loss: 0.502011, acc.: 72.66%] [G loss: 0.851661]\n",
      "epoch:28 step:26897 [D loss: 0.484290, acc.: 77.34%] [G loss: 0.948861]\n",
      "epoch:28 step:26898 [D loss: 0.540156, acc.: 71.88%] [G loss: 0.918439]\n",
      "epoch:28 step:26899 [D loss: 0.570481, acc.: 68.75%] [G loss: 0.757764]\n",
      "epoch:28 step:26900 [D loss: 0.587171, acc.: 64.06%] [G loss: 0.574799]\n",
      "epoch:28 step:26901 [D loss: 0.562965, acc.: 64.84%] [G loss: 0.652148]\n",
      "epoch:28 step:26902 [D loss: 0.500057, acc.: 74.22%] [G loss: 0.797559]\n",
      "epoch:28 step:26903 [D loss: 0.522555, acc.: 75.78%] [G loss: 0.781367]\n",
      "epoch:28 step:26904 [D loss: 0.478266, acc.: 78.12%] [G loss: 0.788706]\n",
      "epoch:28 step:26905 [D loss: 0.568940, acc.: 66.41%] [G loss: 0.551708]\n",
      "epoch:28 step:26906 [D loss: 0.572543, acc.: 66.41%] [G loss: 0.611145]\n",
      "epoch:28 step:26907 [D loss: 0.480919, acc.: 80.47%] [G loss: 0.767019]\n",
      "epoch:28 step:26908 [D loss: 0.526832, acc.: 75.00%] [G loss: 0.707142]\n",
      "epoch:28 step:26909 [D loss: 0.616881, acc.: 64.06%] [G loss: 0.660601]\n",
      "epoch:28 step:26910 [D loss: 0.508167, acc.: 77.34%] [G loss: 0.742021]\n",
      "epoch:28 step:26911 [D loss: 0.597747, acc.: 64.06%] [G loss: 0.548650]\n",
      "epoch:28 step:26912 [D loss: 0.522069, acc.: 70.31%] [G loss: 0.800686]\n",
      "epoch:28 step:26913 [D loss: 0.536214, acc.: 69.53%] [G loss: 0.710786]\n",
      "epoch:28 step:26914 [D loss: 0.545511, acc.: 72.66%] [G loss: 0.740601]\n",
      "epoch:28 step:26915 [D loss: 0.492296, acc.: 75.78%] [G loss: 0.605000]\n",
      "epoch:28 step:26916 [D loss: 0.558883, acc.: 69.53%] [G loss: 0.726451]\n",
      "epoch:28 step:26917 [D loss: 0.426372, acc.: 78.91%] [G loss: 0.973806]\n",
      "epoch:28 step:26918 [D loss: 0.490420, acc.: 71.88%] [G loss: 0.870571]\n",
      "epoch:28 step:26919 [D loss: 0.614312, acc.: 64.06%] [G loss: 0.619264]\n",
      "epoch:28 step:26920 [D loss: 0.591856, acc.: 63.28%] [G loss: 0.643763]\n",
      "epoch:28 step:26921 [D loss: 0.525374, acc.: 71.09%] [G loss: 0.743420]\n",
      "epoch:28 step:26922 [D loss: 0.575562, acc.: 64.84%] [G loss: 0.768948]\n",
      "epoch:28 step:26923 [D loss: 0.586386, acc.: 68.75%] [G loss: 0.550661]\n",
      "epoch:28 step:26924 [D loss: 0.554278, acc.: 68.75%] [G loss: 0.613195]\n",
      "epoch:28 step:26925 [D loss: 0.620872, acc.: 65.62%] [G loss: 0.658293]\n",
      "epoch:28 step:26926 [D loss: 0.476863, acc.: 77.34%] [G loss: 0.973698]\n",
      "epoch:28 step:26927 [D loss: 0.504364, acc.: 79.69%] [G loss: 1.039050]\n",
      "epoch:28 step:26928 [D loss: 0.589554, acc.: 67.97%] [G loss: 0.724158]\n",
      "epoch:28 step:26929 [D loss: 0.429403, acc.: 82.03%] [G loss: 0.744530]\n",
      "epoch:28 step:26930 [D loss: 0.521290, acc.: 75.00%] [G loss: 0.769062]\n",
      "epoch:28 step:26931 [D loss: 0.501431, acc.: 75.00%] [G loss: 0.852857]\n",
      "epoch:28 step:26932 [D loss: 0.681594, acc.: 53.91%] [G loss: 0.544691]\n",
      "epoch:28 step:26933 [D loss: 0.599974, acc.: 60.94%] [G loss: 0.652904]\n",
      "epoch:28 step:26934 [D loss: 0.607710, acc.: 63.28%] [G loss: 0.640633]\n",
      "epoch:28 step:26935 [D loss: 0.504618, acc.: 78.91%] [G loss: 0.647653]\n",
      "epoch:28 step:26936 [D loss: 0.576970, acc.: 67.19%] [G loss: 0.831426]\n",
      "epoch:28 step:26937 [D loss: 0.481821, acc.: 76.56%] [G loss: 0.830965]\n",
      "epoch:28 step:26938 [D loss: 0.604287, acc.: 64.84%] [G loss: 0.831550]\n",
      "epoch:28 step:26939 [D loss: 0.631700, acc.: 60.94%] [G loss: 0.530030]\n",
      "epoch:28 step:26940 [D loss: 0.597036, acc.: 63.28%] [G loss: 0.533249]\n",
      "epoch:28 step:26941 [D loss: 0.559108, acc.: 69.53%] [G loss: 0.730821]\n",
      "epoch:28 step:26942 [D loss: 0.486419, acc.: 74.22%] [G loss: 0.669267]\n",
      "epoch:28 step:26943 [D loss: 0.462405, acc.: 78.12%] [G loss: 0.743195]\n",
      "epoch:28 step:26944 [D loss: 0.474193, acc.: 77.34%] [G loss: 0.842582]\n",
      "epoch:28 step:26945 [D loss: 0.508285, acc.: 72.66%] [G loss: 0.903852]\n",
      "epoch:28 step:26946 [D loss: 0.575765, acc.: 70.31%] [G loss: 0.956401]\n",
      "epoch:28 step:26947 [D loss: 0.482576, acc.: 77.34%] [G loss: 0.915722]\n",
      "epoch:28 step:26948 [D loss: 0.553466, acc.: 71.88%] [G loss: 0.559486]\n",
      "epoch:28 step:26949 [D loss: 0.607768, acc.: 65.62%] [G loss: 0.667202]\n",
      "epoch:28 step:26950 [D loss: 0.568794, acc.: 71.88%] [G loss: 0.903045]\n",
      "epoch:28 step:26951 [D loss: 0.583072, acc.: 68.75%] [G loss: 0.772001]\n",
      "epoch:28 step:26952 [D loss: 0.616042, acc.: 64.06%] [G loss: 0.680452]\n",
      "epoch:28 step:26953 [D loss: 0.563506, acc.: 71.09%] [G loss: 0.838058]\n",
      "epoch:28 step:26954 [D loss: 0.556054, acc.: 67.97%] [G loss: 0.560407]\n",
      "epoch:28 step:26955 [D loss: 0.466491, acc.: 78.12%] [G loss: 0.695792]\n",
      "epoch:28 step:26956 [D loss: 0.583200, acc.: 67.19%] [G loss: 0.771070]\n",
      "epoch:28 step:26957 [D loss: 0.601822, acc.: 65.62%] [G loss: 0.563138]\n",
      "epoch:28 step:26958 [D loss: 0.574116, acc.: 70.31%] [G loss: 0.546658]\n",
      "epoch:28 step:26959 [D loss: 0.560719, acc.: 70.31%] [G loss: 0.715758]\n",
      "epoch:28 step:26960 [D loss: 0.500198, acc.: 75.00%] [G loss: 0.791976]\n",
      "epoch:28 step:26961 [D loss: 0.507797, acc.: 73.44%] [G loss: 0.730308]\n",
      "epoch:28 step:26962 [D loss: 0.523896, acc.: 74.22%] [G loss: 0.754618]\n",
      "epoch:28 step:26963 [D loss: 0.567576, acc.: 69.53%] [G loss: 0.742212]\n",
      "epoch:28 step:26964 [D loss: 0.560983, acc.: 71.09%] [G loss: 0.578026]\n",
      "epoch:28 step:26965 [D loss: 0.538787, acc.: 67.97%] [G loss: 0.678596]\n",
      "epoch:28 step:26966 [D loss: 0.522959, acc.: 73.44%] [G loss: 0.681704]\n",
      "epoch:28 step:26967 [D loss: 0.542121, acc.: 71.09%] [G loss: 0.678890]\n",
      "epoch:28 step:26968 [D loss: 0.576687, acc.: 67.97%] [G loss: 0.503689]\n",
      "epoch:28 step:26969 [D loss: 0.527625, acc.: 71.88%] [G loss: 0.554256]\n",
      "epoch:28 step:26970 [D loss: 0.547913, acc.: 67.97%] [G loss: 0.636120]\n",
      "epoch:28 step:26971 [D loss: 0.538740, acc.: 66.41%] [G loss: 0.621666]\n",
      "epoch:28 step:26972 [D loss: 0.438746, acc.: 78.12%] [G loss: 0.768859]\n",
      "epoch:28 step:26973 [D loss: 0.540667, acc.: 70.31%] [G loss: 0.629960]\n",
      "epoch:28 step:26974 [D loss: 0.532902, acc.: 67.19%] [G loss: 0.558285]\n",
      "epoch:28 step:26975 [D loss: 0.568928, acc.: 66.41%] [G loss: 0.632263]\n",
      "epoch:28 step:26976 [D loss: 0.629689, acc.: 58.59%] [G loss: 0.504083]\n",
      "epoch:28 step:26977 [D loss: 0.538139, acc.: 71.09%] [G loss: 0.640808]\n",
      "epoch:28 step:26978 [D loss: 0.526752, acc.: 68.75%] [G loss: 0.790888]\n",
      "epoch:28 step:26979 [D loss: 0.463068, acc.: 78.91%] [G loss: 0.927215]\n",
      "epoch:28 step:26980 [D loss: 0.500219, acc.: 77.34%] [G loss: 0.790974]\n",
      "epoch:28 step:26981 [D loss: 0.594634, acc.: 67.19%] [G loss: 0.619364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26982 [D loss: 0.447289, acc.: 79.69%] [G loss: 0.736643]\n",
      "epoch:28 step:26983 [D loss: 0.466943, acc.: 80.47%] [G loss: 0.706682]\n",
      "epoch:28 step:26984 [D loss: 0.528388, acc.: 69.53%] [G loss: 0.818382]\n",
      "epoch:28 step:26985 [D loss: 0.506951, acc.: 73.44%] [G loss: 0.867275]\n",
      "epoch:28 step:26986 [D loss: 0.527941, acc.: 71.09%] [G loss: 0.959765]\n",
      "epoch:28 step:26987 [D loss: 0.459728, acc.: 75.00%] [G loss: 0.853902]\n",
      "epoch:28 step:26988 [D loss: 0.566607, acc.: 68.75%] [G loss: 0.824233]\n",
      "epoch:28 step:26989 [D loss: 0.523314, acc.: 73.44%] [G loss: 0.677527]\n",
      "epoch:28 step:26990 [D loss: 0.551046, acc.: 70.31%] [G loss: 0.819616]\n",
      "epoch:28 step:26991 [D loss: 0.506440, acc.: 75.78%] [G loss: 0.712342]\n",
      "epoch:28 step:26992 [D loss: 0.541909, acc.: 67.97%] [G loss: 0.662386]\n",
      "epoch:28 step:26993 [D loss: 0.559991, acc.: 71.09%] [G loss: 0.640433]\n",
      "epoch:28 step:26994 [D loss: 0.554256, acc.: 69.53%] [G loss: 0.821179]\n",
      "epoch:28 step:26995 [D loss: 0.583100, acc.: 70.31%] [G loss: 0.772652]\n",
      "epoch:28 step:26996 [D loss: 0.501255, acc.: 75.00%] [G loss: 0.832630]\n",
      "epoch:28 step:26997 [D loss: 0.526437, acc.: 73.44%] [G loss: 0.615105]\n",
      "epoch:28 step:26998 [D loss: 0.559470, acc.: 68.75%] [G loss: 0.565442]\n",
      "epoch:28 step:26999 [D loss: 0.563932, acc.: 69.53%] [G loss: 0.616063]\n",
      "epoch:28 step:27000 [D loss: 0.535585, acc.: 72.66%] [G loss: 0.767405]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.652820\n",
      "FID: 55.288395\n",
      "0 = 12.910619966220834\n",
      "1 = 0.09522423331106518\n",
      "2 = 0.8458999991416931\n",
      "3 = 0.8027999997138977\n",
      "4 = 0.8889999985694885\n",
      "5 = 0.8785291910171509\n",
      "6 = 0.8027999997138977\n",
      "7 = 8.681538696801674\n",
      "8 = 0.16216047469756056\n",
      "9 = 0.6886000037193298\n",
      "10 = 0.6815999746322632\n",
      "11 = 0.6955999732017517\n",
      "12 = 0.6912778615951538\n",
      "13 = 0.6815999746322632\n",
      "14 = 6.6528472900390625\n",
      "15 = 6.440382480621338\n",
      "16 = 0.42058253288269043\n",
      "17 = 6.652819633483887\n",
      "18 = 55.288394927978516\n",
      "epoch:28 step:27001 [D loss: 0.589257, acc.: 68.75%] [G loss: 0.494154]\n",
      "epoch:28 step:27002 [D loss: 0.606231, acc.: 61.72%] [G loss: 0.530117]\n",
      "epoch:28 step:27003 [D loss: 0.501150, acc.: 74.22%] [G loss: 0.708823]\n",
      "epoch:28 step:27004 [D loss: 0.479656, acc.: 78.12%] [G loss: 0.823208]\n",
      "epoch:28 step:27005 [D loss: 0.515493, acc.: 72.66%] [G loss: 0.938689]\n",
      "epoch:28 step:27006 [D loss: 0.535801, acc.: 70.31%] [G loss: 1.025965]\n",
      "epoch:28 step:27007 [D loss: 0.551933, acc.: 70.31%] [G loss: 0.835746]\n",
      "epoch:28 step:27008 [D loss: 0.558728, acc.: 67.19%] [G loss: 0.734047]\n",
      "epoch:28 step:27009 [D loss: 0.559752, acc.: 72.66%] [G loss: 0.607570]\n",
      "epoch:28 step:27010 [D loss: 0.438521, acc.: 80.47%] [G loss: 0.794920]\n",
      "epoch:28 step:27011 [D loss: 0.561218, acc.: 71.88%] [G loss: 0.872267]\n",
      "epoch:28 step:27012 [D loss: 0.641584, acc.: 62.50%] [G loss: 0.795904]\n",
      "epoch:28 step:27013 [D loss: 0.542363, acc.: 71.88%] [G loss: 0.732357]\n",
      "epoch:28 step:27014 [D loss: 0.577403, acc.: 65.62%] [G loss: 0.679864]\n",
      "epoch:28 step:27015 [D loss: 0.533324, acc.: 71.88%] [G loss: 0.681148]\n",
      "epoch:28 step:27016 [D loss: 0.527521, acc.: 75.00%] [G loss: 0.749023]\n",
      "epoch:28 step:27017 [D loss: 0.526377, acc.: 75.00%] [G loss: 0.624845]\n",
      "epoch:28 step:27018 [D loss: 0.485777, acc.: 75.78%] [G loss: 0.904717]\n",
      "epoch:28 step:27019 [D loss: 0.555235, acc.: 71.09%] [G loss: 1.014230]\n",
      "epoch:28 step:27020 [D loss: 0.613547, acc.: 65.62%] [G loss: 0.845045]\n",
      "epoch:28 step:27021 [D loss: 0.508934, acc.: 73.44%] [G loss: 0.678128]\n",
      "epoch:28 step:27022 [D loss: 0.518294, acc.: 75.78%] [G loss: 0.608913]\n",
      "epoch:28 step:27023 [D loss: 0.580267, acc.: 69.53%] [G loss: 0.618178]\n",
      "epoch:28 step:27024 [D loss: 0.637090, acc.: 64.06%] [G loss: 0.489550]\n",
      "epoch:28 step:27025 [D loss: 0.484777, acc.: 78.12%] [G loss: 0.700062]\n",
      "epoch:28 step:27026 [D loss: 0.542152, acc.: 64.84%] [G loss: 0.672361]\n",
      "epoch:28 step:27027 [D loss: 0.568460, acc.: 68.75%] [G loss: 0.840122]\n",
      "epoch:28 step:27028 [D loss: 0.509531, acc.: 77.34%] [G loss: 0.834409]\n",
      "epoch:28 step:27029 [D loss: 0.563597, acc.: 71.88%] [G loss: 0.760981]\n",
      "epoch:28 step:27030 [D loss: 0.615391, acc.: 66.41%] [G loss: 0.512017]\n",
      "epoch:28 step:27031 [D loss: 0.578938, acc.: 69.53%] [G loss: 0.651685]\n",
      "epoch:28 step:27032 [D loss: 0.529853, acc.: 72.66%] [G loss: 0.837072]\n",
      "epoch:28 step:27033 [D loss: 0.524582, acc.: 68.75%] [G loss: 0.859490]\n",
      "epoch:28 step:27034 [D loss: 0.473946, acc.: 70.31%] [G loss: 0.905422]\n",
      "epoch:28 step:27035 [D loss: 0.569043, acc.: 71.88%] [G loss: 0.610803]\n",
      "epoch:28 step:27036 [D loss: 0.574884, acc.: 64.84%] [G loss: 0.816784]\n",
      "epoch:28 step:27037 [D loss: 0.494830, acc.: 75.00%] [G loss: 0.702293]\n",
      "epoch:28 step:27038 [D loss: 0.498818, acc.: 79.69%] [G loss: 0.951174]\n",
      "epoch:28 step:27039 [D loss: 0.505383, acc.: 75.00%] [G loss: 0.845955]\n",
      "epoch:28 step:27040 [D loss: 0.564039, acc.: 63.28%] [G loss: 0.819706]\n",
      "epoch:28 step:27041 [D loss: 0.543276, acc.: 69.53%] [G loss: 0.578706]\n",
      "epoch:28 step:27042 [D loss: 0.552679, acc.: 68.75%] [G loss: 0.595140]\n",
      "epoch:28 step:27043 [D loss: 0.509625, acc.: 71.09%] [G loss: 0.658260]\n",
      "epoch:28 step:27044 [D loss: 0.533055, acc.: 71.88%] [G loss: 0.592392]\n",
      "epoch:28 step:27045 [D loss: 0.542181, acc.: 71.88%] [G loss: 0.671068]\n",
      "epoch:28 step:27046 [D loss: 0.478535, acc.: 77.34%] [G loss: 0.666703]\n",
      "epoch:28 step:27047 [D loss: 0.555303, acc.: 71.88%] [G loss: 0.579084]\n",
      "epoch:28 step:27048 [D loss: 0.600553, acc.: 67.19%] [G loss: 0.509892]\n",
      "epoch:28 step:27049 [D loss: 0.543544, acc.: 65.62%] [G loss: 0.523221]\n",
      "epoch:28 step:27050 [D loss: 0.476308, acc.: 78.91%] [G loss: 0.613622]\n",
      "epoch:28 step:27051 [D loss: 0.446410, acc.: 76.56%] [G loss: 0.765628]\n",
      "epoch:28 step:27052 [D loss: 0.579142, acc.: 67.19%] [G loss: 0.887700]\n",
      "epoch:28 step:27053 [D loss: 0.624235, acc.: 64.84%] [G loss: 0.865433]\n",
      "epoch:28 step:27054 [D loss: 0.593119, acc.: 67.19%] [G loss: 0.661788]\n",
      "epoch:28 step:27055 [D loss: 0.522950, acc.: 73.44%] [G loss: 0.687866]\n",
      "epoch:28 step:27056 [D loss: 0.620868, acc.: 67.97%] [G loss: 0.677980]\n",
      "epoch:28 step:27057 [D loss: 0.547369, acc.: 70.31%] [G loss: 0.579603]\n",
      "epoch:28 step:27058 [D loss: 0.552737, acc.: 66.41%] [G loss: 0.483194]\n",
      "epoch:28 step:27059 [D loss: 0.492411, acc.: 78.12%] [G loss: 0.635984]\n",
      "epoch:28 step:27060 [D loss: 0.532193, acc.: 76.56%] [G loss: 0.684808]\n",
      "epoch:28 step:27061 [D loss: 0.517480, acc.: 76.56%] [G loss: 0.585652]\n",
      "epoch:28 step:27062 [D loss: 0.494122, acc.: 75.78%] [G loss: 0.649855]\n",
      "epoch:28 step:27063 [D loss: 0.609818, acc.: 67.19%] [G loss: 0.679788]\n",
      "epoch:28 step:27064 [D loss: 0.633764, acc.: 65.62%] [G loss: 0.598808]\n",
      "epoch:28 step:27065 [D loss: 0.556227, acc.: 67.97%] [G loss: 0.586607]\n",
      "epoch:28 step:27066 [D loss: 0.551590, acc.: 67.97%] [G loss: 0.708173]\n",
      "epoch:28 step:27067 [D loss: 0.546321, acc.: 67.97%] [G loss: 0.615930]\n",
      "epoch:28 step:27068 [D loss: 0.523299, acc.: 73.44%] [G loss: 0.644383]\n",
      "epoch:28 step:27069 [D loss: 0.555265, acc.: 72.66%] [G loss: 0.830092]\n",
      "epoch:28 step:27070 [D loss: 0.545889, acc.: 68.75%] [G loss: 0.744647]\n",
      "epoch:28 step:27071 [D loss: 0.521729, acc.: 72.66%] [G loss: 0.708514]\n",
      "epoch:28 step:27072 [D loss: 0.541778, acc.: 71.09%] [G loss: 0.581763]\n",
      "epoch:28 step:27073 [D loss: 0.544390, acc.: 69.53%] [G loss: 0.617462]\n",
      "epoch:28 step:27074 [D loss: 0.508987, acc.: 72.66%] [G loss: 0.708138]\n",
      "epoch:28 step:27075 [D loss: 0.552799, acc.: 69.53%] [G loss: 0.659909]\n",
      "epoch:28 step:27076 [D loss: 0.576858, acc.: 70.31%] [G loss: 0.583756]\n",
      "epoch:28 step:27077 [D loss: 0.548481, acc.: 67.19%] [G loss: 0.579467]\n",
      "epoch:28 step:27078 [D loss: 0.506309, acc.: 75.78%] [G loss: 0.664908]\n",
      "epoch:28 step:27079 [D loss: 0.502524, acc.: 76.56%] [G loss: 0.642881]\n",
      "epoch:28 step:27080 [D loss: 0.523568, acc.: 70.31%] [G loss: 0.725800]\n",
      "epoch:28 step:27081 [D loss: 0.555414, acc.: 67.97%] [G loss: 0.683269]\n",
      "epoch:28 step:27082 [D loss: 0.601139, acc.: 64.06%] [G loss: 0.503811]\n",
      "epoch:28 step:27083 [D loss: 0.623614, acc.: 60.16%] [G loss: 0.655630]\n",
      "epoch:28 step:27084 [D loss: 0.539027, acc.: 70.31%] [G loss: 0.512822]\n",
      "epoch:28 step:27085 [D loss: 0.520729, acc.: 71.88%] [G loss: 0.481667]\n",
      "epoch:28 step:27086 [D loss: 0.549435, acc.: 74.22%] [G loss: 0.490329]\n",
      "epoch:28 step:27087 [D loss: 0.575009, acc.: 68.75%] [G loss: 0.497831]\n",
      "epoch:28 step:27088 [D loss: 0.538908, acc.: 68.75%] [G loss: 0.543153]\n",
      "epoch:28 step:27089 [D loss: 0.582933, acc.: 68.75%] [G loss: 0.686080]\n",
      "epoch:28 step:27090 [D loss: 0.524984, acc.: 70.31%] [G loss: 0.708087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:27091 [D loss: 0.555037, acc.: 71.09%] [G loss: 0.665743]\n",
      "epoch:28 step:27092 [D loss: 0.589532, acc.: 66.41%] [G loss: 0.683826]\n",
      "epoch:28 step:27093 [D loss: 0.467482, acc.: 76.56%] [G loss: 0.686013]\n",
      "epoch:28 step:27094 [D loss: 0.541331, acc.: 73.44%] [G loss: 0.782479]\n",
      "epoch:28 step:27095 [D loss: 0.579055, acc.: 69.53%] [G loss: 0.719769]\n",
      "epoch:28 step:27096 [D loss: 0.444944, acc.: 77.34%] [G loss: 0.781801]\n",
      "epoch:28 step:27097 [D loss: 0.609763, acc.: 65.62%] [G loss: 0.626047]\n",
      "epoch:28 step:27098 [D loss: 0.616062, acc.: 64.06%] [G loss: 0.579071]\n",
      "epoch:28 step:27099 [D loss: 0.551751, acc.: 71.09%] [G loss: 0.494604]\n",
      "epoch:28 step:27100 [D loss: 0.550741, acc.: 67.97%] [G loss: 0.566501]\n",
      "epoch:28 step:27101 [D loss: 0.600550, acc.: 64.84%] [G loss: 0.589254]\n",
      "epoch:28 step:27102 [D loss: 0.531006, acc.: 67.19%] [G loss: 0.569964]\n",
      "epoch:28 step:27103 [D loss: 0.597263, acc.: 67.97%] [G loss: 0.558951]\n",
      "epoch:28 step:27104 [D loss: 0.542001, acc.: 71.88%] [G loss: 0.566170]\n",
      "epoch:28 step:27105 [D loss: 0.522009, acc.: 71.88%] [G loss: 0.544517]\n",
      "epoch:28 step:27106 [D loss: 0.470983, acc.: 76.56%] [G loss: 0.768971]\n",
      "epoch:28 step:27107 [D loss: 0.494138, acc.: 68.75%] [G loss: 0.769331]\n",
      "epoch:28 step:27108 [D loss: 0.540054, acc.: 71.09%] [G loss: 0.738237]\n",
      "epoch:28 step:27109 [D loss: 0.592242, acc.: 67.19%] [G loss: 0.609537]\n",
      "epoch:28 step:27110 [D loss: 0.562989, acc.: 72.66%] [G loss: 0.691913]\n",
      "epoch:28 step:27111 [D loss: 0.508969, acc.: 72.66%] [G loss: 0.673026]\n",
      "epoch:28 step:27112 [D loss: 0.566954, acc.: 68.75%] [G loss: 0.721195]\n",
      "epoch:28 step:27113 [D loss: 0.629851, acc.: 60.94%] [G loss: 0.528217]\n",
      "epoch:28 step:27114 [D loss: 0.522182, acc.: 72.66%] [G loss: 0.580158]\n",
      "epoch:28 step:27115 [D loss: 0.524828, acc.: 69.53%] [G loss: 0.473071]\n",
      "epoch:28 step:27116 [D loss: 0.662432, acc.: 57.81%] [G loss: 0.506356]\n",
      "epoch:28 step:27117 [D loss: 0.573012, acc.: 67.19%] [G loss: 0.445489]\n",
      "epoch:28 step:27118 [D loss: 0.561694, acc.: 67.97%] [G loss: 0.484430]\n",
      "epoch:28 step:27119 [D loss: 0.601886, acc.: 64.84%] [G loss: 0.548881]\n",
      "epoch:28 step:27120 [D loss: 0.505329, acc.: 75.00%] [G loss: 0.651503]\n",
      "epoch:28 step:27121 [D loss: 0.483910, acc.: 77.34%] [G loss: 0.716561]\n",
      "epoch:28 step:27122 [D loss: 0.615443, acc.: 68.75%] [G loss: 0.919563]\n",
      "epoch:28 step:27123 [D loss: 0.508307, acc.: 71.88%] [G loss: 0.935948]\n",
      "epoch:28 step:27124 [D loss: 0.598525, acc.: 62.50%] [G loss: 0.696183]\n",
      "epoch:28 step:27125 [D loss: 0.527460, acc.: 73.44%] [G loss: 0.570751]\n",
      "epoch:28 step:27126 [D loss: 0.470176, acc.: 79.69%] [G loss: 0.624279]\n",
      "epoch:28 step:27127 [D loss: 0.515773, acc.: 78.91%] [G loss: 0.697824]\n",
      "epoch:28 step:27128 [D loss: 0.594049, acc.: 68.75%] [G loss: 0.494366]\n",
      "epoch:28 step:27129 [D loss: 0.550970, acc.: 73.44%] [G loss: 0.591424]\n",
      "epoch:28 step:27130 [D loss: 0.499599, acc.: 75.00%] [G loss: 0.639106]\n",
      "epoch:28 step:27131 [D loss: 0.504520, acc.: 74.22%] [G loss: 0.722981]\n",
      "epoch:28 step:27132 [D loss: 0.460208, acc.: 82.03%] [G loss: 0.858974]\n",
      "epoch:28 step:27133 [D loss: 0.512016, acc.: 71.09%] [G loss: 0.814752]\n",
      "epoch:28 step:27134 [D loss: 0.466474, acc.: 75.00%] [G loss: 0.775825]\n",
      "epoch:28 step:27135 [D loss: 0.513515, acc.: 72.66%] [G loss: 0.770901]\n",
      "epoch:28 step:27136 [D loss: 0.487395, acc.: 75.78%] [G loss: 0.894369]\n",
      "epoch:28 step:27137 [D loss: 0.571171, acc.: 70.31%] [G loss: 0.682528]\n",
      "epoch:28 step:27138 [D loss: 0.516718, acc.: 73.44%] [G loss: 0.884805]\n",
      "epoch:28 step:27139 [D loss: 0.517947, acc.: 70.31%] [G loss: 0.679956]\n",
      "epoch:28 step:27140 [D loss: 0.512271, acc.: 75.78%] [G loss: 0.903083]\n",
      "epoch:28 step:27141 [D loss: 0.542345, acc.: 66.41%] [G loss: 0.778215]\n",
      "epoch:28 step:27142 [D loss: 0.478650, acc.: 77.34%] [G loss: 0.856036]\n",
      "epoch:28 step:27143 [D loss: 0.566843, acc.: 70.31%] [G loss: 0.723316]\n",
      "epoch:28 step:27144 [D loss: 0.510403, acc.: 78.12%] [G loss: 0.907133]\n",
      "epoch:28 step:27145 [D loss: 0.552818, acc.: 65.62%] [G loss: 0.782402]\n",
      "epoch:28 step:27146 [D loss: 0.528597, acc.: 73.44%] [G loss: 0.892261]\n",
      "epoch:28 step:27147 [D loss: 0.473652, acc.: 77.34%] [G loss: 0.682247]\n",
      "epoch:28 step:27148 [D loss: 0.524665, acc.: 70.31%] [G loss: 0.636025]\n",
      "epoch:28 step:27149 [D loss: 0.564278, acc.: 73.44%] [G loss: 0.813799]\n",
      "epoch:28 step:27150 [D loss: 0.519804, acc.: 69.53%] [G loss: 0.920410]\n",
      "epoch:28 step:27151 [D loss: 0.704948, acc.: 62.50%] [G loss: 0.671826]\n",
      "epoch:28 step:27152 [D loss: 0.538953, acc.: 70.31%] [G loss: 0.685109]\n",
      "epoch:28 step:27153 [D loss: 0.578452, acc.: 67.19%] [G loss: 0.859213]\n",
      "epoch:28 step:27154 [D loss: 0.440181, acc.: 78.91%] [G loss: 0.858935]\n",
      "epoch:28 step:27155 [D loss: 0.423004, acc.: 80.47%] [G loss: 1.071815]\n",
      "epoch:28 step:27156 [D loss: 0.727749, acc.: 53.91%] [G loss: 0.830271]\n",
      "epoch:28 step:27157 [D loss: 0.516445, acc.: 69.53%] [G loss: 1.186450]\n",
      "epoch:28 step:27158 [D loss: 0.541446, acc.: 70.31%] [G loss: 0.793907]\n",
      "epoch:28 step:27159 [D loss: 0.450291, acc.: 73.44%] [G loss: 1.047706]\n",
      "epoch:28 step:27160 [D loss: 0.455188, acc.: 78.12%] [G loss: 0.868421]\n",
      "epoch:28 step:27161 [D loss: 0.452283, acc.: 78.91%] [G loss: 1.095716]\n",
      "epoch:28 step:27162 [D loss: 0.400954, acc.: 81.25%] [G loss: 1.133646]\n",
      "epoch:28 step:27163 [D loss: 0.479685, acc.: 74.22%] [G loss: 1.320098]\n",
      "epoch:28 step:27164 [D loss: 0.704997, acc.: 63.28%] [G loss: 1.224082]\n",
      "epoch:28 step:27165 [D loss: 0.449115, acc.: 76.56%] [G loss: 1.342631]\n",
      "epoch:28 step:27166 [D loss: 0.490239, acc.: 75.78%] [G loss: 1.391042]\n",
      "epoch:28 step:27167 [D loss: 0.433533, acc.: 80.47%] [G loss: 1.110797]\n",
      "epoch:28 step:27168 [D loss: 0.644435, acc.: 62.50%] [G loss: 1.227862]\n",
      "epoch:28 step:27169 [D loss: 0.502579, acc.: 76.56%] [G loss: 0.943006]\n",
      "epoch:28 step:27170 [D loss: 0.629456, acc.: 67.97%] [G loss: 1.125875]\n",
      "epoch:28 step:27171 [D loss: 0.465601, acc.: 73.44%] [G loss: 1.177315]\n",
      "epoch:28 step:27172 [D loss: 0.378789, acc.: 83.59%] [G loss: 1.299676]\n",
      "epoch:28 step:27173 [D loss: 0.453553, acc.: 77.34%] [G loss: 1.646602]\n",
      "epoch:29 step:27174 [D loss: 0.593582, acc.: 71.09%] [G loss: 1.539481]\n",
      "epoch:29 step:27175 [D loss: 0.427129, acc.: 82.03%] [G loss: 1.188345]\n",
      "epoch:29 step:27176 [D loss: 0.507666, acc.: 74.22%] [G loss: 1.012072]\n",
      "epoch:29 step:27177 [D loss: 0.561761, acc.: 71.09%] [G loss: 0.752651]\n",
      "epoch:29 step:27178 [D loss: 0.537430, acc.: 74.22%] [G loss: 0.852721]\n",
      "epoch:29 step:27179 [D loss: 0.621223, acc.: 63.28%] [G loss: 0.883135]\n",
      "epoch:29 step:27180 [D loss: 0.478405, acc.: 81.25%] [G loss: 0.705213]\n",
      "epoch:29 step:27181 [D loss: 0.467985, acc.: 76.56%] [G loss: 0.914735]\n",
      "epoch:29 step:27182 [D loss: 0.503809, acc.: 78.12%] [G loss: 0.896631]\n",
      "epoch:29 step:27183 [D loss: 0.497933, acc.: 72.66%] [G loss: 0.839464]\n",
      "epoch:29 step:27184 [D loss: 0.439359, acc.: 79.69%] [G loss: 1.009301]\n",
      "epoch:29 step:27185 [D loss: 0.559310, acc.: 66.41%] [G loss: 0.779552]\n",
      "epoch:29 step:27186 [D loss: 0.497054, acc.: 76.56%] [G loss: 0.701122]\n",
      "epoch:29 step:27187 [D loss: 0.519763, acc.: 71.88%] [G loss: 0.739711]\n",
      "epoch:29 step:27188 [D loss: 0.409765, acc.: 81.25%] [G loss: 0.951313]\n",
      "epoch:29 step:27189 [D loss: 0.473801, acc.: 73.44%] [G loss: 0.753952]\n",
      "epoch:29 step:27190 [D loss: 0.575492, acc.: 70.31%] [G loss: 0.715743]\n",
      "epoch:29 step:27191 [D loss: 0.535612, acc.: 70.31%] [G loss: 0.612780]\n",
      "epoch:29 step:27192 [D loss: 0.539945, acc.: 72.66%] [G loss: 0.790819]\n",
      "epoch:29 step:27193 [D loss: 0.588623, acc.: 71.09%] [G loss: 0.937476]\n",
      "epoch:29 step:27194 [D loss: 0.577958, acc.: 70.31%] [G loss: 0.785019]\n",
      "epoch:29 step:27195 [D loss: 0.436606, acc.: 86.72%] [G loss: 0.932529]\n",
      "epoch:29 step:27196 [D loss: 0.614908, acc.: 67.19%] [G loss: 0.729994]\n",
      "epoch:29 step:27197 [D loss: 0.512392, acc.: 73.44%] [G loss: 0.754831]\n",
      "epoch:29 step:27198 [D loss: 0.481623, acc.: 75.00%] [G loss: 0.815835]\n",
      "epoch:29 step:27199 [D loss: 0.578084, acc.: 65.62%] [G loss: 0.700397]\n",
      "epoch:29 step:27200 [D loss: 0.472436, acc.: 75.78%] [G loss: 0.827491]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.773237\n",
      "FID: 48.032814\n",
      "0 = 12.991380220127128\n",
      "1 = 0.10069313633094659\n",
      "2 = 0.8695999979972839\n",
      "3 = 0.8266000151634216\n",
      "4 = 0.9125999808311462\n",
      "5 = 0.9043763875961304\n",
      "6 = 0.8266000151634216\n",
      "7 = 8.251686273837102\n",
      "8 = 0.15144356062187989\n",
      "9 = 0.6917999982833862\n",
      "10 = 0.6858000159263611\n",
      "11 = 0.6977999806404114\n",
      "12 = 0.6941295266151428\n",
      "13 = 0.6858000159263611\n",
      "14 = 6.77325963973999\n",
      "15 = 6.826666831970215\n",
      "16 = 0.39937469363212585\n",
      "17 = 6.7732367515563965\n",
      "18 = 48.032814025878906\n",
      "epoch:29 step:27201 [D loss: 0.549925, acc.: 71.88%] [G loss: 0.599343]\n",
      "epoch:29 step:27202 [D loss: 0.525256, acc.: 71.88%] [G loss: 0.591068]\n",
      "epoch:29 step:27203 [D loss: 0.559894, acc.: 67.19%] [G loss: 0.722500]\n",
      "epoch:29 step:27204 [D loss: 0.613596, acc.: 67.97%] [G loss: 0.613729]\n",
      "epoch:29 step:27205 [D loss: 0.580920, acc.: 67.97%] [G loss: 0.632801]\n",
      "epoch:29 step:27206 [D loss: 0.553574, acc.: 69.53%] [G loss: 0.717017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27207 [D loss: 0.558835, acc.: 69.53%] [G loss: 0.482811]\n",
      "epoch:29 step:27208 [D loss: 0.602197, acc.: 68.75%] [G loss: 0.565256]\n",
      "epoch:29 step:27209 [D loss: 0.485785, acc.: 75.00%] [G loss: 0.852629]\n",
      "epoch:29 step:27210 [D loss: 0.513421, acc.: 73.44%] [G loss: 0.629575]\n",
      "epoch:29 step:27211 [D loss: 0.552854, acc.: 71.09%] [G loss: 0.579182]\n",
      "epoch:29 step:27212 [D loss: 0.525325, acc.: 74.22%] [G loss: 0.610126]\n",
      "epoch:29 step:27213 [D loss: 0.461599, acc.: 79.69%] [G loss: 0.686184]\n",
      "epoch:29 step:27214 [D loss: 0.519541, acc.: 74.22%] [G loss: 0.744775]\n",
      "epoch:29 step:27215 [D loss: 0.609852, acc.: 67.19%] [G loss: 0.597791]\n",
      "epoch:29 step:27216 [D loss: 0.514716, acc.: 78.12%] [G loss: 0.701897]\n",
      "epoch:29 step:27217 [D loss: 0.537274, acc.: 73.44%] [G loss: 0.876850]\n",
      "epoch:29 step:27218 [D loss: 0.488038, acc.: 71.09%] [G loss: 0.729227]\n",
      "epoch:29 step:27219 [D loss: 0.460915, acc.: 78.12%] [G loss: 0.923652]\n",
      "epoch:29 step:27220 [D loss: 0.547018, acc.: 67.97%] [G loss: 0.632102]\n",
      "epoch:29 step:27221 [D loss: 0.501872, acc.: 73.44%] [G loss: 0.790031]\n",
      "epoch:29 step:27222 [D loss: 0.451760, acc.: 74.22%] [G loss: 0.810816]\n",
      "epoch:29 step:27223 [D loss: 0.504241, acc.: 75.78%] [G loss: 0.886084]\n",
      "epoch:29 step:27224 [D loss: 0.591475, acc.: 67.97%] [G loss: 0.624376]\n",
      "epoch:29 step:27225 [D loss: 0.580524, acc.: 67.97%] [G loss: 0.582583]\n",
      "epoch:29 step:27226 [D loss: 0.519456, acc.: 76.56%] [G loss: 0.667865]\n",
      "epoch:29 step:27227 [D loss: 0.480758, acc.: 72.66%] [G loss: 0.730916]\n",
      "epoch:29 step:27228 [D loss: 0.534876, acc.: 73.44%] [G loss: 0.694854]\n",
      "epoch:29 step:27229 [D loss: 0.499938, acc.: 73.44%] [G loss: 0.728284]\n",
      "epoch:29 step:27230 [D loss: 0.531110, acc.: 69.53%] [G loss: 0.761887]\n",
      "epoch:29 step:27231 [D loss: 0.527366, acc.: 67.97%] [G loss: 0.840342]\n",
      "epoch:29 step:27232 [D loss: 0.534724, acc.: 68.75%] [G loss: 0.892689]\n",
      "epoch:29 step:27233 [D loss: 0.561260, acc.: 68.75%] [G loss: 0.798929]\n",
      "epoch:29 step:27234 [D loss: 0.536522, acc.: 68.75%] [G loss: 0.730758]\n",
      "epoch:29 step:27235 [D loss: 0.531635, acc.: 74.22%] [G loss: 0.841290]\n",
      "epoch:29 step:27236 [D loss: 0.562845, acc.: 71.88%] [G loss: 0.776921]\n",
      "epoch:29 step:27237 [D loss: 0.541878, acc.: 71.09%] [G loss: 0.664292]\n",
      "epoch:29 step:27238 [D loss: 0.484224, acc.: 77.34%] [G loss: 0.827253]\n",
      "epoch:29 step:27239 [D loss: 0.512489, acc.: 73.44%] [G loss: 0.866112]\n",
      "epoch:29 step:27240 [D loss: 0.570618, acc.: 69.53%] [G loss: 0.568926]\n",
      "epoch:29 step:27241 [D loss: 0.522275, acc.: 69.53%] [G loss: 0.680952]\n",
      "epoch:29 step:27242 [D loss: 0.508797, acc.: 70.31%] [G loss: 0.727292]\n",
      "epoch:29 step:27243 [D loss: 0.488767, acc.: 75.78%] [G loss: 0.878878]\n",
      "epoch:29 step:27244 [D loss: 0.463969, acc.: 80.47%] [G loss: 0.621067]\n",
      "epoch:29 step:27245 [D loss: 0.518778, acc.: 71.88%] [G loss: 0.663136]\n",
      "epoch:29 step:27246 [D loss: 0.576756, acc.: 65.62%] [G loss: 0.498872]\n",
      "epoch:29 step:27247 [D loss: 0.489396, acc.: 77.34%] [G loss: 0.709018]\n",
      "epoch:29 step:27248 [D loss: 0.538385, acc.: 73.44%] [G loss: 0.948166]\n",
      "epoch:29 step:27249 [D loss: 0.489720, acc.: 71.88%] [G loss: 0.774620]\n",
      "epoch:29 step:27250 [D loss: 0.486387, acc.: 71.88%] [G loss: 1.003166]\n",
      "epoch:29 step:27251 [D loss: 0.588732, acc.: 69.53%] [G loss: 0.700514]\n",
      "epoch:29 step:27252 [D loss: 0.586537, acc.: 64.84%] [G loss: 0.756028]\n",
      "epoch:29 step:27253 [D loss: 0.460144, acc.: 78.12%] [G loss: 0.727155]\n",
      "epoch:29 step:27254 [D loss: 0.471300, acc.: 76.56%] [G loss: 0.703755]\n",
      "epoch:29 step:27255 [D loss: 0.503720, acc.: 68.75%] [G loss: 0.770470]\n",
      "epoch:29 step:27256 [D loss: 0.510404, acc.: 79.69%] [G loss: 0.727140]\n",
      "epoch:29 step:27257 [D loss: 0.498608, acc.: 74.22%] [G loss: 1.023359]\n",
      "epoch:29 step:27258 [D loss: 0.559689, acc.: 66.41%] [G loss: 0.964142]\n",
      "epoch:29 step:27259 [D loss: 0.566409, acc.: 70.31%] [G loss: 0.576929]\n",
      "epoch:29 step:27260 [D loss: 0.568283, acc.: 67.19%] [G loss: 0.648767]\n",
      "epoch:29 step:27261 [D loss: 0.493626, acc.: 78.12%] [G loss: 0.639600]\n",
      "epoch:29 step:27262 [D loss: 0.538778, acc.: 68.75%] [G loss: 0.677010]\n",
      "epoch:29 step:27263 [D loss: 0.510937, acc.: 73.44%] [G loss: 0.775625]\n",
      "epoch:29 step:27264 [D loss: 0.585162, acc.: 71.09%] [G loss: 0.874905]\n",
      "epoch:29 step:27265 [D loss: 0.453775, acc.: 81.25%] [G loss: 0.853039]\n",
      "epoch:29 step:27266 [D loss: 0.556727, acc.: 71.09%] [G loss: 0.984903]\n",
      "epoch:29 step:27267 [D loss: 0.490244, acc.: 71.88%] [G loss: 0.862410]\n",
      "epoch:29 step:27268 [D loss: 0.499505, acc.: 71.88%] [G loss: 0.934138]\n",
      "epoch:29 step:27269 [D loss: 0.490120, acc.: 75.00%] [G loss: 0.872258]\n",
      "epoch:29 step:27270 [D loss: 0.482379, acc.: 75.78%] [G loss: 0.949158]\n",
      "epoch:29 step:27271 [D loss: 0.548723, acc.: 70.31%] [G loss: 0.741992]\n",
      "epoch:29 step:27272 [D loss: 0.545108, acc.: 73.44%] [G loss: 0.756444]\n",
      "epoch:29 step:27273 [D loss: 0.433748, acc.: 78.12%] [G loss: 0.897685]\n",
      "epoch:29 step:27274 [D loss: 0.483486, acc.: 78.12%] [G loss: 0.976106]\n",
      "epoch:29 step:27275 [D loss: 0.629711, acc.: 60.16%] [G loss: 0.826817]\n",
      "epoch:29 step:27276 [D loss: 0.557551, acc.: 65.62%] [G loss: 0.530836]\n",
      "epoch:29 step:27277 [D loss: 0.579515, acc.: 67.97%] [G loss: 0.686565]\n",
      "epoch:29 step:27278 [D loss: 0.586605, acc.: 62.50%] [G loss: 0.663512]\n",
      "epoch:29 step:27279 [D loss: 0.503071, acc.: 75.78%] [G loss: 0.821154]\n",
      "epoch:29 step:27280 [D loss: 0.495135, acc.: 75.78%] [G loss: 0.890821]\n",
      "epoch:29 step:27281 [D loss: 0.636339, acc.: 61.72%] [G loss: 0.618730]\n",
      "epoch:29 step:27282 [D loss: 0.561638, acc.: 71.09%] [G loss: 0.692968]\n",
      "epoch:29 step:27283 [D loss: 0.567456, acc.: 67.97%] [G loss: 0.788424]\n",
      "epoch:29 step:27284 [D loss: 0.512075, acc.: 72.66%] [G loss: 0.858548]\n",
      "epoch:29 step:27285 [D loss: 0.537016, acc.: 75.78%] [G loss: 0.670676]\n",
      "epoch:29 step:27286 [D loss: 0.534406, acc.: 75.78%] [G loss: 0.746376]\n",
      "epoch:29 step:27287 [D loss: 0.527031, acc.: 74.22%] [G loss: 0.831824]\n",
      "epoch:29 step:27288 [D loss: 0.482660, acc.: 79.69%] [G loss: 0.776607]\n",
      "epoch:29 step:27289 [D loss: 0.513188, acc.: 72.66%] [G loss: 0.895813]\n",
      "epoch:29 step:27290 [D loss: 0.540532, acc.: 70.31%] [G loss: 0.999443]\n",
      "epoch:29 step:27291 [D loss: 0.529564, acc.: 74.22%] [G loss: 0.831237]\n",
      "epoch:29 step:27292 [D loss: 0.476070, acc.: 77.34%] [G loss: 0.799697]\n",
      "epoch:29 step:27293 [D loss: 0.540698, acc.: 71.88%] [G loss: 0.834790]\n",
      "epoch:29 step:27294 [D loss: 0.502117, acc.: 72.66%] [G loss: 0.821044]\n",
      "epoch:29 step:27295 [D loss: 0.548661, acc.: 72.66%] [G loss: 0.978906]\n",
      "epoch:29 step:27296 [D loss: 0.534758, acc.: 70.31%] [G loss: 0.761410]\n",
      "epoch:29 step:27297 [D loss: 0.547365, acc.: 71.88%] [G loss: 0.660278]\n",
      "epoch:29 step:27298 [D loss: 0.550858, acc.: 67.97%] [G loss: 0.620026]\n",
      "epoch:29 step:27299 [D loss: 0.507495, acc.: 76.56%] [G loss: 0.774204]\n",
      "epoch:29 step:27300 [D loss: 0.549247, acc.: 71.88%] [G loss: 0.816070]\n",
      "epoch:29 step:27301 [D loss: 0.514958, acc.: 71.88%] [G loss: 0.675698]\n",
      "epoch:29 step:27302 [D loss: 0.526593, acc.: 69.53%] [G loss: 0.726072]\n",
      "epoch:29 step:27303 [D loss: 0.496270, acc.: 72.66%] [G loss: 0.632485]\n",
      "epoch:29 step:27304 [D loss: 0.514427, acc.: 74.22%] [G loss: 0.766740]\n",
      "epoch:29 step:27305 [D loss: 0.580193, acc.: 73.44%] [G loss: 0.687378]\n",
      "epoch:29 step:27306 [D loss: 0.552130, acc.: 70.31%] [G loss: 0.787302]\n",
      "epoch:29 step:27307 [D loss: 0.484557, acc.: 73.44%] [G loss: 0.768995]\n",
      "epoch:29 step:27308 [D loss: 0.496932, acc.: 74.22%] [G loss: 0.664477]\n",
      "epoch:29 step:27309 [D loss: 0.472044, acc.: 79.69%] [G loss: 0.926993]\n",
      "epoch:29 step:27310 [D loss: 0.639701, acc.: 64.06%] [G loss: 0.658180]\n",
      "epoch:29 step:27311 [D loss: 0.559891, acc.: 69.53%] [G loss: 0.673941]\n",
      "epoch:29 step:27312 [D loss: 0.508136, acc.: 71.88%] [G loss: 0.713245]\n",
      "epoch:29 step:27313 [D loss: 0.565432, acc.: 71.09%] [G loss: 0.631398]\n",
      "epoch:29 step:27314 [D loss: 0.522658, acc.: 69.53%] [G loss: 0.777873]\n",
      "epoch:29 step:27315 [D loss: 0.529518, acc.: 73.44%] [G loss: 0.749545]\n",
      "epoch:29 step:27316 [D loss: 0.609308, acc.: 66.41%] [G loss: 0.688827]\n",
      "epoch:29 step:27317 [D loss: 0.569680, acc.: 70.31%] [G loss: 0.692508]\n",
      "epoch:29 step:27318 [D loss: 0.567541, acc.: 70.31%] [G loss: 0.822966]\n",
      "epoch:29 step:27319 [D loss: 0.483443, acc.: 79.69%] [G loss: 0.702540]\n",
      "epoch:29 step:27320 [D loss: 0.577103, acc.: 71.88%] [G loss: 0.806474]\n",
      "epoch:29 step:27321 [D loss: 0.594032, acc.: 66.41%] [G loss: 0.501208]\n",
      "epoch:29 step:27322 [D loss: 0.506744, acc.: 77.34%] [G loss: 0.634508]\n",
      "epoch:29 step:27323 [D loss: 0.611602, acc.: 65.62%] [G loss: 0.527792]\n",
      "epoch:29 step:27324 [D loss: 0.534388, acc.: 74.22%] [G loss: 0.553104]\n",
      "epoch:29 step:27325 [D loss: 0.501801, acc.: 72.66%] [G loss: 0.767722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27326 [D loss: 0.624184, acc.: 65.62%] [G loss: 0.814522]\n",
      "epoch:29 step:27327 [D loss: 0.581094, acc.: 65.62%] [G loss: 0.703806]\n",
      "epoch:29 step:27328 [D loss: 0.450840, acc.: 78.12%] [G loss: 0.714358]\n",
      "epoch:29 step:27329 [D loss: 0.485600, acc.: 75.00%] [G loss: 0.669342]\n",
      "epoch:29 step:27330 [D loss: 0.568830, acc.: 66.41%] [G loss: 0.583427]\n",
      "epoch:29 step:27331 [D loss: 0.540080, acc.: 70.31%] [G loss: 0.629069]\n",
      "epoch:29 step:27332 [D loss: 0.467360, acc.: 78.12%] [G loss: 0.670777]\n",
      "epoch:29 step:27333 [D loss: 0.624761, acc.: 67.19%] [G loss: 0.684596]\n",
      "epoch:29 step:27334 [D loss: 0.542950, acc.: 69.53%] [G loss: 0.751449]\n",
      "epoch:29 step:27335 [D loss: 0.480631, acc.: 75.00%] [G loss: 0.826788]\n",
      "epoch:29 step:27336 [D loss: 0.526008, acc.: 70.31%] [G loss: 0.852518]\n",
      "epoch:29 step:27337 [D loss: 0.606559, acc.: 67.19%] [G loss: 0.745929]\n",
      "epoch:29 step:27338 [D loss: 0.499023, acc.: 70.31%] [G loss: 0.787970]\n",
      "epoch:29 step:27339 [D loss: 0.589984, acc.: 66.41%] [G loss: 0.653692]\n",
      "epoch:29 step:27340 [D loss: 0.545701, acc.: 69.53%] [G loss: 0.571475]\n",
      "epoch:29 step:27341 [D loss: 0.534776, acc.: 71.88%] [G loss: 0.728970]\n",
      "epoch:29 step:27342 [D loss: 0.530396, acc.: 68.75%] [G loss: 0.594858]\n",
      "epoch:29 step:27343 [D loss: 0.548818, acc.: 67.97%] [G loss: 0.743611]\n",
      "epoch:29 step:27344 [D loss: 0.524157, acc.: 68.75%] [G loss: 0.697465]\n",
      "epoch:29 step:27345 [D loss: 0.524858, acc.: 67.97%] [G loss: 0.637630]\n",
      "epoch:29 step:27346 [D loss: 0.513276, acc.: 75.00%] [G loss: 0.741112]\n",
      "epoch:29 step:27347 [D loss: 0.579854, acc.: 70.31%] [G loss: 0.674208]\n",
      "epoch:29 step:27348 [D loss: 0.581383, acc.: 67.97%] [G loss: 0.569790]\n",
      "epoch:29 step:27349 [D loss: 0.570513, acc.: 70.31%] [G loss: 0.621354]\n",
      "epoch:29 step:27350 [D loss: 0.508612, acc.: 69.53%] [G loss: 0.588611]\n",
      "epoch:29 step:27351 [D loss: 0.584767, acc.: 63.28%] [G loss: 0.605062]\n",
      "epoch:29 step:27352 [D loss: 0.508762, acc.: 71.09%] [G loss: 0.754350]\n",
      "epoch:29 step:27353 [D loss: 0.602361, acc.: 63.28%] [G loss: 0.606400]\n",
      "epoch:29 step:27354 [D loss: 0.610965, acc.: 65.62%] [G loss: 0.545960]\n",
      "epoch:29 step:27355 [D loss: 0.474367, acc.: 78.91%] [G loss: 0.677770]\n",
      "epoch:29 step:27356 [D loss: 0.649855, acc.: 66.41%] [G loss: 0.717701]\n",
      "epoch:29 step:27357 [D loss: 0.477783, acc.: 78.91%] [G loss: 0.713703]\n",
      "epoch:29 step:27358 [D loss: 0.608442, acc.: 63.28%] [G loss: 0.731906]\n",
      "epoch:29 step:27359 [D loss: 0.509200, acc.: 72.66%] [G loss: 0.776067]\n",
      "epoch:29 step:27360 [D loss: 0.640798, acc.: 62.50%] [G loss: 0.553975]\n",
      "epoch:29 step:27361 [D loss: 0.538459, acc.: 70.31%] [G loss: 0.514359]\n",
      "epoch:29 step:27362 [D loss: 0.548398, acc.: 71.88%] [G loss: 0.797997]\n",
      "epoch:29 step:27363 [D loss: 0.493818, acc.: 79.69%] [G loss: 0.655821]\n",
      "epoch:29 step:27364 [D loss: 0.548007, acc.: 69.53%] [G loss: 0.858628]\n",
      "epoch:29 step:27365 [D loss: 0.550529, acc.: 74.22%] [G loss: 0.646690]\n",
      "epoch:29 step:27366 [D loss: 0.532181, acc.: 72.66%] [G loss: 0.716617]\n",
      "epoch:29 step:27367 [D loss: 0.468686, acc.: 76.56%] [G loss: 0.790265]\n",
      "epoch:29 step:27368 [D loss: 0.543669, acc.: 72.66%] [G loss: 0.997819]\n",
      "epoch:29 step:27369 [D loss: 0.546803, acc.: 69.53%] [G loss: 0.787770]\n",
      "epoch:29 step:27370 [D loss: 0.545920, acc.: 73.44%] [G loss: 0.774804]\n",
      "epoch:29 step:27371 [D loss: 0.415898, acc.: 84.38%] [G loss: 0.828445]\n",
      "epoch:29 step:27372 [D loss: 0.519784, acc.: 70.31%] [G loss: 0.776847]\n",
      "epoch:29 step:27373 [D loss: 0.583119, acc.: 65.62%] [G loss: 0.634038]\n",
      "epoch:29 step:27374 [D loss: 0.604943, acc.: 67.97%] [G loss: 0.648304]\n",
      "epoch:29 step:27375 [D loss: 0.503739, acc.: 71.09%] [G loss: 0.925576]\n",
      "epoch:29 step:27376 [D loss: 0.580499, acc.: 67.19%] [G loss: 0.733353]\n",
      "epoch:29 step:27377 [D loss: 0.598793, acc.: 64.06%] [G loss: 0.798532]\n",
      "epoch:29 step:27378 [D loss: 0.486362, acc.: 76.56%] [G loss: 0.725866]\n",
      "epoch:29 step:27379 [D loss: 0.516203, acc.: 71.88%] [G loss: 0.836425]\n",
      "epoch:29 step:27380 [D loss: 0.511558, acc.: 71.09%] [G loss: 0.980712]\n",
      "epoch:29 step:27381 [D loss: 0.424714, acc.: 79.69%] [G loss: 0.995814]\n",
      "epoch:29 step:27382 [D loss: 0.490137, acc.: 77.34%] [G loss: 0.930027]\n",
      "epoch:29 step:27383 [D loss: 0.655274, acc.: 62.50%] [G loss: 0.782721]\n",
      "epoch:29 step:27384 [D loss: 0.541200, acc.: 73.44%] [G loss: 0.794502]\n",
      "epoch:29 step:27385 [D loss: 0.528795, acc.: 75.78%] [G loss: 0.748535]\n",
      "epoch:29 step:27386 [D loss: 0.511378, acc.: 74.22%] [G loss: 0.664735]\n",
      "epoch:29 step:27387 [D loss: 0.636649, acc.: 63.28%] [G loss: 0.561900]\n",
      "epoch:29 step:27388 [D loss: 0.543424, acc.: 70.31%] [G loss: 0.641573]\n",
      "epoch:29 step:27389 [D loss: 0.541537, acc.: 71.09%] [G loss: 0.564560]\n",
      "epoch:29 step:27390 [D loss: 0.542219, acc.: 68.75%] [G loss: 0.741935]\n",
      "epoch:29 step:27391 [D loss: 0.453534, acc.: 82.03%] [G loss: 0.901744]\n",
      "epoch:29 step:27392 [D loss: 0.428912, acc.: 82.81%] [G loss: 0.929296]\n",
      "epoch:29 step:27393 [D loss: 0.698746, acc.: 60.16%] [G loss: 0.745594]\n",
      "epoch:29 step:27394 [D loss: 0.546465, acc.: 67.97%] [G loss: 0.676619]\n",
      "epoch:29 step:27395 [D loss: 0.543534, acc.: 69.53%] [G loss: 0.729085]\n",
      "epoch:29 step:27396 [D loss: 0.628798, acc.: 63.28%] [G loss: 0.824142]\n",
      "epoch:29 step:27397 [D loss: 0.511590, acc.: 73.44%] [G loss: 0.785228]\n",
      "epoch:29 step:27398 [D loss: 0.506091, acc.: 77.34%] [G loss: 0.662655]\n",
      "epoch:29 step:27399 [D loss: 0.558547, acc.: 67.19%] [G loss: 0.698809]\n",
      "epoch:29 step:27400 [D loss: 0.567835, acc.: 65.62%] [G loss: 0.706111]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.700068\n",
      "FID: 45.001884\n",
      "0 = 13.006595987701417\n",
      "1 = 0.10041750536894195\n",
      "2 = 0.8476999998092651\n",
      "3 = 0.8069999814033508\n",
      "4 = 0.8884000182151794\n",
      "5 = 0.8785107731819153\n",
      "6 = 0.8069999814033508\n",
      "7 = 8.130254838275897\n",
      "8 = 0.14503514883286966\n",
      "9 = 0.6879000067710876\n",
      "10 = 0.6779999732971191\n",
      "11 = 0.6977999806404114\n",
      "12 = 0.6916955709457397\n",
      "13 = 0.6779999732971191\n",
      "14 = 6.700092315673828\n",
      "15 = 6.830770492553711\n",
      "16 = 0.3992096185684204\n",
      "17 = 6.700067520141602\n",
      "18 = 45.00188446044922\n",
      "epoch:29 step:27401 [D loss: 0.577343, acc.: 71.09%] [G loss: 0.490495]\n",
      "epoch:29 step:27402 [D loss: 0.501962, acc.: 74.22%] [G loss: 0.728962]\n",
      "epoch:29 step:27403 [D loss: 0.580461, acc.: 64.06%] [G loss: 0.674557]\n",
      "epoch:29 step:27404 [D loss: 0.398313, acc.: 85.16%] [G loss: 0.937014]\n",
      "epoch:29 step:27405 [D loss: 0.506678, acc.: 78.12%] [G loss: 0.992507]\n",
      "epoch:29 step:27406 [D loss: 0.603093, acc.: 67.19%] [G loss: 0.858341]\n",
      "epoch:29 step:27407 [D loss: 0.540526, acc.: 68.75%] [G loss: 0.808178]\n",
      "epoch:29 step:27408 [D loss: 0.596311, acc.: 67.19%] [G loss: 0.661279]\n",
      "epoch:29 step:27409 [D loss: 0.515534, acc.: 74.22%] [G loss: 0.752944]\n",
      "epoch:29 step:27410 [D loss: 0.511671, acc.: 72.66%] [G loss: 0.600075]\n",
      "epoch:29 step:27411 [D loss: 0.561231, acc.: 68.75%] [G loss: 0.653515]\n",
      "epoch:29 step:27412 [D loss: 0.487277, acc.: 72.66%] [G loss: 0.776664]\n",
      "epoch:29 step:27413 [D loss: 0.587485, acc.: 65.62%] [G loss: 0.658114]\n",
      "epoch:29 step:27414 [D loss: 0.498129, acc.: 74.22%] [G loss: 0.803010]\n",
      "epoch:29 step:27415 [D loss: 0.513564, acc.: 68.75%] [G loss: 0.872508]\n",
      "epoch:29 step:27416 [D loss: 0.490631, acc.: 78.12%] [G loss: 0.835807]\n",
      "epoch:29 step:27417 [D loss: 0.483313, acc.: 70.31%] [G loss: 0.958842]\n",
      "epoch:29 step:27418 [D loss: 0.537267, acc.: 68.75%] [G loss: 0.984565]\n",
      "epoch:29 step:27419 [D loss: 0.525405, acc.: 70.31%] [G loss: 0.710196]\n",
      "epoch:29 step:27420 [D loss: 0.509645, acc.: 75.00%] [G loss: 0.811245]\n",
      "epoch:29 step:27421 [D loss: 0.456089, acc.: 77.34%] [G loss: 0.829756]\n",
      "epoch:29 step:27422 [D loss: 0.554685, acc.: 66.41%] [G loss: 0.884048]\n",
      "epoch:29 step:27423 [D loss: 0.635845, acc.: 62.50%] [G loss: 0.785192]\n",
      "epoch:29 step:27424 [D loss: 0.646444, acc.: 64.06%] [G loss: 0.878299]\n",
      "epoch:29 step:27425 [D loss: 0.502539, acc.: 75.78%] [G loss: 0.728875]\n",
      "epoch:29 step:27426 [D loss: 0.574073, acc.: 65.62%] [G loss: 0.682614]\n",
      "epoch:29 step:27427 [D loss: 0.526758, acc.: 67.97%] [G loss: 0.655877]\n",
      "epoch:29 step:27428 [D loss: 0.523407, acc.: 72.66%] [G loss: 0.733386]\n",
      "epoch:29 step:27429 [D loss: 0.536944, acc.: 67.97%] [G loss: 0.850199]\n",
      "epoch:29 step:27430 [D loss: 0.608253, acc.: 59.38%] [G loss: 0.705322]\n",
      "epoch:29 step:27431 [D loss: 0.551923, acc.: 67.97%] [G loss: 0.630574]\n",
      "epoch:29 step:27432 [D loss: 0.556418, acc.: 67.97%] [G loss: 0.571507]\n",
      "epoch:29 step:27433 [D loss: 0.588178, acc.: 67.97%] [G loss: 0.620396]\n",
      "epoch:29 step:27434 [D loss: 0.481917, acc.: 75.78%] [G loss: 0.822267]\n",
      "epoch:29 step:27435 [D loss: 0.535408, acc.: 74.22%] [G loss: 0.662636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27436 [D loss: 0.575443, acc.: 70.31%] [G loss: 0.608799]\n",
      "epoch:29 step:27437 [D loss: 0.548105, acc.: 70.31%] [G loss: 0.606262]\n",
      "epoch:29 step:27438 [D loss: 0.508518, acc.: 71.09%] [G loss: 0.845419]\n",
      "epoch:29 step:27439 [D loss: 0.539722, acc.: 69.53%] [G loss: 0.639926]\n",
      "epoch:29 step:27440 [D loss: 0.542532, acc.: 71.88%] [G loss: 0.613617]\n",
      "epoch:29 step:27441 [D loss: 0.547093, acc.: 67.97%] [G loss: 0.709178]\n",
      "epoch:29 step:27442 [D loss: 0.575037, acc.: 72.66%] [G loss: 0.641895]\n",
      "epoch:29 step:27443 [D loss: 0.489082, acc.: 76.56%] [G loss: 0.795907]\n",
      "epoch:29 step:27444 [D loss: 0.520223, acc.: 70.31%] [G loss: 0.699564]\n",
      "epoch:29 step:27445 [D loss: 0.552350, acc.: 70.31%] [G loss: 0.671080]\n",
      "epoch:29 step:27446 [D loss: 0.550589, acc.: 67.97%] [G loss: 0.732680]\n",
      "epoch:29 step:27447 [D loss: 0.438968, acc.: 78.12%] [G loss: 0.850521]\n",
      "epoch:29 step:27448 [D loss: 0.541426, acc.: 71.09%] [G loss: 0.811693]\n",
      "epoch:29 step:27449 [D loss: 0.403599, acc.: 82.03%] [G loss: 0.843888]\n",
      "epoch:29 step:27450 [D loss: 0.691997, acc.: 59.38%] [G loss: 0.560515]\n",
      "epoch:29 step:27451 [D loss: 0.635231, acc.: 64.84%] [G loss: 0.626252]\n",
      "epoch:29 step:27452 [D loss: 0.616724, acc.: 60.16%] [G loss: 0.664101]\n",
      "epoch:29 step:27453 [D loss: 0.568116, acc.: 64.84%] [G loss: 0.877148]\n",
      "epoch:29 step:27454 [D loss: 0.613047, acc.: 67.97%] [G loss: 0.816251]\n",
      "epoch:29 step:27455 [D loss: 0.582408, acc.: 71.88%] [G loss: 0.586706]\n",
      "epoch:29 step:27456 [D loss: 0.524158, acc.: 75.00%] [G loss: 0.713019]\n",
      "epoch:29 step:27457 [D loss: 0.522512, acc.: 71.09%] [G loss: 0.522165]\n",
      "epoch:29 step:27458 [D loss: 0.511362, acc.: 75.00%] [G loss: 0.549401]\n",
      "epoch:29 step:27459 [D loss: 0.446949, acc.: 77.34%] [G loss: 0.763508]\n",
      "epoch:29 step:27460 [D loss: 0.587897, acc.: 64.84%] [G loss: 0.638556]\n",
      "epoch:29 step:27461 [D loss: 0.559371, acc.: 71.88%] [G loss: 0.837159]\n",
      "epoch:29 step:27462 [D loss: 0.560855, acc.: 67.19%] [G loss: 0.748122]\n",
      "epoch:29 step:27463 [D loss: 0.587161, acc.: 67.19%] [G loss: 0.780722]\n",
      "epoch:29 step:27464 [D loss: 0.603810, acc.: 67.97%] [G loss: 0.511775]\n",
      "epoch:29 step:27465 [D loss: 0.542826, acc.: 72.66%] [G loss: 0.562988]\n",
      "epoch:29 step:27466 [D loss: 0.552044, acc.: 68.75%] [G loss: 0.617853]\n",
      "epoch:29 step:27467 [D loss: 0.555150, acc.: 67.97%] [G loss: 0.568667]\n",
      "epoch:29 step:27468 [D loss: 0.537095, acc.: 69.53%] [G loss: 0.658763]\n",
      "epoch:29 step:27469 [D loss: 0.472423, acc.: 78.12%] [G loss: 0.741776]\n",
      "epoch:29 step:27470 [D loss: 0.499975, acc.: 72.66%] [G loss: 0.648508]\n",
      "epoch:29 step:27471 [D loss: 0.480312, acc.: 78.12%] [G loss: 0.684975]\n",
      "epoch:29 step:27472 [D loss: 0.524151, acc.: 75.00%] [G loss: 0.805397]\n",
      "epoch:29 step:27473 [D loss: 0.498074, acc.: 75.00%] [G loss: 0.959190]\n",
      "epoch:29 step:27474 [D loss: 0.614560, acc.: 66.41%] [G loss: 0.837603]\n",
      "epoch:29 step:27475 [D loss: 0.521615, acc.: 68.75%] [G loss: 0.569534]\n",
      "epoch:29 step:27476 [D loss: 0.572669, acc.: 65.62%] [G loss: 0.695484]\n",
      "epoch:29 step:27477 [D loss: 0.477795, acc.: 74.22%] [G loss: 0.790923]\n",
      "epoch:29 step:27478 [D loss: 0.504747, acc.: 75.00%] [G loss: 0.683654]\n",
      "epoch:29 step:27479 [D loss: 0.515731, acc.: 71.88%] [G loss: 0.867916]\n",
      "epoch:29 step:27480 [D loss: 0.470657, acc.: 79.69%] [G loss: 0.895321]\n",
      "epoch:29 step:27481 [D loss: 0.543536, acc.: 70.31%] [G loss: 0.721691]\n",
      "epoch:29 step:27482 [D loss: 0.566952, acc.: 67.19%] [G loss: 0.834583]\n",
      "epoch:29 step:27483 [D loss: 0.577679, acc.: 66.41%] [G loss: 0.834847]\n",
      "epoch:29 step:27484 [D loss: 0.497043, acc.: 74.22%] [G loss: 0.719295]\n",
      "epoch:29 step:27485 [D loss: 0.472822, acc.: 79.69%] [G loss: 0.908890]\n",
      "epoch:29 step:27486 [D loss: 0.558192, acc.: 74.22%] [G loss: 0.968066]\n",
      "epoch:29 step:27487 [D loss: 0.481121, acc.: 73.44%] [G loss: 0.977613]\n",
      "epoch:29 step:27488 [D loss: 0.461531, acc.: 77.34%] [G loss: 0.872474]\n",
      "epoch:29 step:27489 [D loss: 0.663979, acc.: 65.62%] [G loss: 0.884607]\n",
      "epoch:29 step:27490 [D loss: 0.570309, acc.: 70.31%] [G loss: 0.665662]\n",
      "epoch:29 step:27491 [D loss: 0.495546, acc.: 75.00%] [G loss: 0.589193]\n",
      "epoch:29 step:27492 [D loss: 0.522503, acc.: 77.34%] [G loss: 0.533831]\n",
      "epoch:29 step:27493 [D loss: 0.515717, acc.: 73.44%] [G loss: 0.630395]\n",
      "epoch:29 step:27494 [D loss: 0.523899, acc.: 72.66%] [G loss: 0.627946]\n",
      "epoch:29 step:27495 [D loss: 0.555285, acc.: 69.53%] [G loss: 0.781518]\n",
      "epoch:29 step:27496 [D loss: 0.646893, acc.: 64.06%] [G loss: 0.577985]\n",
      "epoch:29 step:27497 [D loss: 0.530923, acc.: 71.88%] [G loss: 0.570644]\n",
      "epoch:29 step:27498 [D loss: 0.519767, acc.: 71.88%] [G loss: 0.768059]\n",
      "epoch:29 step:27499 [D loss: 0.476283, acc.: 77.34%] [G loss: 0.803179]\n",
      "epoch:29 step:27500 [D loss: 0.532841, acc.: 72.66%] [G loss: 0.749351]\n",
      "epoch:29 step:27501 [D loss: 0.451070, acc.: 79.69%] [G loss: 0.857940]\n",
      "epoch:29 step:27502 [D loss: 0.562312, acc.: 66.41%] [G loss: 0.781955]\n",
      "epoch:29 step:27503 [D loss: 0.586184, acc.: 68.75%] [G loss: 0.665870]\n",
      "epoch:29 step:27504 [D loss: 0.547340, acc.: 71.09%] [G loss: 0.737493]\n",
      "epoch:29 step:27505 [D loss: 0.535077, acc.: 71.09%] [G loss: 0.762575]\n",
      "epoch:29 step:27506 [D loss: 0.518632, acc.: 73.44%] [G loss: 0.721052]\n",
      "epoch:29 step:27507 [D loss: 0.486993, acc.: 78.91%] [G loss: 0.730811]\n",
      "epoch:29 step:27508 [D loss: 0.540222, acc.: 70.31%] [G loss: 0.799307]\n",
      "epoch:29 step:27509 [D loss: 0.507850, acc.: 75.00%] [G loss: 0.774955]\n",
      "epoch:29 step:27510 [D loss: 0.514627, acc.: 76.56%] [G loss: 0.844933]\n",
      "epoch:29 step:27511 [D loss: 0.610186, acc.: 64.84%] [G loss: 0.683670]\n",
      "epoch:29 step:27512 [D loss: 0.568021, acc.: 64.84%] [G loss: 0.669847]\n",
      "epoch:29 step:27513 [D loss: 0.451358, acc.: 78.12%] [G loss: 0.819602]\n",
      "epoch:29 step:27514 [D loss: 0.614797, acc.: 65.62%] [G loss: 0.622585]\n",
      "epoch:29 step:27515 [D loss: 0.573484, acc.: 67.19%] [G loss: 0.914648]\n",
      "epoch:29 step:27516 [D loss: 0.508790, acc.: 73.44%] [G loss: 0.788481]\n",
      "epoch:29 step:27517 [D loss: 0.438959, acc.: 80.47%] [G loss: 0.841041]\n",
      "epoch:29 step:27518 [D loss: 0.540228, acc.: 67.19%] [G loss: 0.820153]\n",
      "epoch:29 step:27519 [D loss: 0.505962, acc.: 72.66%] [G loss: 0.946827]\n",
      "epoch:29 step:27520 [D loss: 0.420128, acc.: 78.12%] [G loss: 0.879643]\n",
      "epoch:29 step:27521 [D loss: 0.589653, acc.: 67.19%] [G loss: 0.758115]\n",
      "epoch:29 step:27522 [D loss: 0.725494, acc.: 57.03%] [G loss: 0.535422]\n",
      "epoch:29 step:27523 [D loss: 0.520143, acc.: 75.00%] [G loss: 0.596173]\n",
      "epoch:29 step:27524 [D loss: 0.500499, acc.: 75.78%] [G loss: 0.657211]\n",
      "epoch:29 step:27525 [D loss: 0.503622, acc.: 71.09%] [G loss: 0.880501]\n",
      "epoch:29 step:27526 [D loss: 0.547875, acc.: 69.53%] [G loss: 0.774567]\n",
      "epoch:29 step:27527 [D loss: 0.478849, acc.: 75.00%] [G loss: 0.817029]\n",
      "epoch:29 step:27528 [D loss: 0.505094, acc.: 75.78%] [G loss: 0.872209]\n",
      "epoch:29 step:27529 [D loss: 0.516473, acc.: 75.78%] [G loss: 0.774054]\n",
      "epoch:29 step:27530 [D loss: 0.415873, acc.: 82.81%] [G loss: 0.827244]\n",
      "epoch:29 step:27531 [D loss: 0.451001, acc.: 76.56%] [G loss: 0.996693]\n",
      "epoch:29 step:27532 [D loss: 0.544918, acc.: 70.31%] [G loss: 1.057888]\n",
      "epoch:29 step:27533 [D loss: 0.486975, acc.: 75.78%] [G loss: 0.950084]\n",
      "epoch:29 step:27534 [D loss: 0.476932, acc.: 77.34%] [G loss: 0.964574]\n",
      "epoch:29 step:27535 [D loss: 0.512090, acc.: 76.56%] [G loss: 0.938269]\n",
      "epoch:29 step:27536 [D loss: 0.592778, acc.: 66.41%] [G loss: 0.719529]\n",
      "epoch:29 step:27537 [D loss: 0.519427, acc.: 73.44%] [G loss: 0.701641]\n",
      "epoch:29 step:27538 [D loss: 0.521991, acc.: 69.53%] [G loss: 0.606619]\n",
      "epoch:29 step:27539 [D loss: 0.575793, acc.: 73.44%] [G loss: 0.751252]\n",
      "epoch:29 step:27540 [D loss: 0.572426, acc.: 67.97%] [G loss: 0.846948]\n",
      "epoch:29 step:27541 [D loss: 0.554553, acc.: 69.53%] [G loss: 0.686689]\n",
      "epoch:29 step:27542 [D loss: 0.509866, acc.: 73.44%] [G loss: 0.688470]\n",
      "epoch:29 step:27543 [D loss: 0.499173, acc.: 77.34%] [G loss: 0.906022]\n",
      "epoch:29 step:27544 [D loss: 0.457837, acc.: 76.56%] [G loss: 0.866259]\n",
      "epoch:29 step:27545 [D loss: 0.528750, acc.: 75.78%] [G loss: 0.723005]\n",
      "epoch:29 step:27546 [D loss: 0.518853, acc.: 75.78%] [G loss: 0.643830]\n",
      "epoch:29 step:27547 [D loss: 0.469972, acc.: 74.22%] [G loss: 0.743755]\n",
      "epoch:29 step:27548 [D loss: 0.528464, acc.: 71.88%] [G loss: 0.795922]\n",
      "epoch:29 step:27549 [D loss: 0.639292, acc.: 62.50%] [G loss: 0.541810]\n",
      "epoch:29 step:27550 [D loss: 0.536467, acc.: 66.41%] [G loss: 0.563007]\n",
      "epoch:29 step:27551 [D loss: 0.575312, acc.: 67.97%] [G loss: 0.621065]\n",
      "epoch:29 step:27552 [D loss: 0.544266, acc.: 67.97%] [G loss: 0.775934]\n",
      "epoch:29 step:27553 [D loss: 0.569547, acc.: 66.41%] [G loss: 0.707013]\n",
      "epoch:29 step:27554 [D loss: 0.438675, acc.: 79.69%] [G loss: 0.674023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27555 [D loss: 0.516070, acc.: 74.22%] [G loss: 0.758353]\n",
      "epoch:29 step:27556 [D loss: 0.561975, acc.: 68.75%] [G loss: 0.649536]\n",
      "epoch:29 step:27557 [D loss: 0.536517, acc.: 71.88%] [G loss: 0.685707]\n",
      "epoch:29 step:27558 [D loss: 0.441444, acc.: 78.91%] [G loss: 0.811958]\n",
      "epoch:29 step:27559 [D loss: 0.575694, acc.: 67.97%] [G loss: 0.727868]\n",
      "epoch:29 step:27560 [D loss: 0.566465, acc.: 64.84%] [G loss: 0.903018]\n",
      "epoch:29 step:27561 [D loss: 0.560881, acc.: 72.66%] [G loss: 0.560243]\n",
      "epoch:29 step:27562 [D loss: 0.496908, acc.: 79.69%] [G loss: 0.660933]\n",
      "epoch:29 step:27563 [D loss: 0.622313, acc.: 60.94%] [G loss: 0.617737]\n",
      "epoch:29 step:27564 [D loss: 0.589955, acc.: 60.94%] [G loss: 0.550222]\n",
      "epoch:29 step:27565 [D loss: 0.479885, acc.: 78.91%] [G loss: 0.798224]\n",
      "epoch:29 step:27566 [D loss: 0.563691, acc.: 69.53%] [G loss: 0.600710]\n",
      "epoch:29 step:27567 [D loss: 0.526562, acc.: 71.88%] [G loss: 0.712363]\n",
      "epoch:29 step:27568 [D loss: 0.527328, acc.: 71.09%] [G loss: 0.597857]\n",
      "epoch:29 step:27569 [D loss: 0.544847, acc.: 68.75%] [G loss: 0.648973]\n",
      "epoch:29 step:27570 [D loss: 0.548846, acc.: 68.75%] [G loss: 0.669062]\n",
      "epoch:29 step:27571 [D loss: 0.490853, acc.: 76.56%] [G loss: 0.747304]\n",
      "epoch:29 step:27572 [D loss: 0.562778, acc.: 72.66%] [G loss: 0.822356]\n",
      "epoch:29 step:27573 [D loss: 0.629547, acc.: 64.84%] [G loss: 0.686170]\n",
      "epoch:29 step:27574 [D loss: 0.650510, acc.: 60.94%] [G loss: 0.568345]\n",
      "epoch:29 step:27575 [D loss: 0.517846, acc.: 72.66%] [G loss: 0.822376]\n",
      "epoch:29 step:27576 [D loss: 0.509318, acc.: 71.88%] [G loss: 0.883353]\n",
      "epoch:29 step:27577 [D loss: 0.557683, acc.: 72.66%] [G loss: 0.803593]\n",
      "epoch:29 step:27578 [D loss: 0.535370, acc.: 68.75%] [G loss: 0.716940]\n",
      "epoch:29 step:27579 [D loss: 0.470020, acc.: 74.22%] [G loss: 0.987883]\n",
      "epoch:29 step:27580 [D loss: 0.557184, acc.: 67.19%] [G loss: 0.632160]\n",
      "epoch:29 step:27581 [D loss: 0.616791, acc.: 64.06%] [G loss: 0.743188]\n",
      "epoch:29 step:27582 [D loss: 0.559480, acc.: 68.75%] [G loss: 0.774457]\n",
      "epoch:29 step:27583 [D loss: 0.582679, acc.: 67.97%] [G loss: 0.614571]\n",
      "epoch:29 step:27584 [D loss: 0.545066, acc.: 67.97%] [G loss: 0.763661]\n",
      "epoch:29 step:27585 [D loss: 0.608948, acc.: 63.28%] [G loss: 0.648093]\n",
      "epoch:29 step:27586 [D loss: 0.527991, acc.: 70.31%] [G loss: 0.678146]\n",
      "epoch:29 step:27587 [D loss: 0.506771, acc.: 73.44%] [G loss: 0.910224]\n",
      "epoch:29 step:27588 [D loss: 0.489367, acc.: 75.78%] [G loss: 0.927184]\n",
      "epoch:29 step:27589 [D loss: 0.485277, acc.: 75.00%] [G loss: 1.051953]\n",
      "epoch:29 step:27590 [D loss: 0.532198, acc.: 74.22%] [G loss: 0.750824]\n",
      "epoch:29 step:27591 [D loss: 0.649236, acc.: 65.62%] [G loss: 0.704106]\n",
      "epoch:29 step:27592 [D loss: 0.557617, acc.: 71.09%] [G loss: 0.840562]\n",
      "epoch:29 step:27593 [D loss: 0.571975, acc.: 67.19%] [G loss: 0.801198]\n",
      "epoch:29 step:27594 [D loss: 0.611008, acc.: 66.41%] [G loss: 0.494941]\n",
      "epoch:29 step:27595 [D loss: 0.589906, acc.: 64.84%] [G loss: 0.556386]\n",
      "epoch:29 step:27596 [D loss: 0.594077, acc.: 64.84%] [G loss: 0.504734]\n",
      "epoch:29 step:27597 [D loss: 0.547304, acc.: 67.97%] [G loss: 0.636505]\n",
      "epoch:29 step:27598 [D loss: 0.552685, acc.: 72.66%] [G loss: 0.668808]\n",
      "epoch:29 step:27599 [D loss: 0.445114, acc.: 77.34%] [G loss: 0.815136]\n",
      "epoch:29 step:27600 [D loss: 0.502656, acc.: 74.22%] [G loss: 0.751519]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.627431\n",
      "FID: 52.800259\n",
      "0 = 12.855828098011047\n",
      "1 = 0.09459748272923807\n",
      "2 = 0.8518999814987183\n",
      "3 = 0.8080000281333923\n",
      "4 = 0.895799994468689\n",
      "5 = 0.8857706785202026\n",
      "6 = 0.8080000281333923\n",
      "7 = 8.539811582756045\n",
      "8 = 0.15915909148293214\n",
      "9 = 0.6840999722480774\n",
      "10 = 0.6736000180244446\n",
      "11 = 0.694599986076355\n",
      "12 = 0.6880490183830261\n",
      "13 = 0.6736000180244446\n",
      "14 = 6.627463340759277\n",
      "15 = 6.646902561187744\n",
      "16 = 0.412341833114624\n",
      "17 = 6.6274309158325195\n",
      "18 = 52.80025863647461\n",
      "epoch:29 step:27601 [D loss: 0.535932, acc.: 71.09%] [G loss: 0.665816]\n",
      "epoch:29 step:27602 [D loss: 0.426299, acc.: 79.69%] [G loss: 0.932213]\n",
      "epoch:29 step:27603 [D loss: 0.525433, acc.: 72.66%] [G loss: 0.819787]\n",
      "epoch:29 step:27604 [D loss: 0.528515, acc.: 69.53%] [G loss: 0.731369]\n",
      "epoch:29 step:27605 [D loss: 0.515150, acc.: 75.78%] [G loss: 0.819412]\n",
      "epoch:29 step:27606 [D loss: 0.604254, acc.: 65.62%] [G loss: 0.672741]\n",
      "epoch:29 step:27607 [D loss: 0.552583, acc.: 68.75%] [G loss: 0.665014]\n",
      "epoch:29 step:27608 [D loss: 0.492584, acc.: 75.78%] [G loss: 0.854048]\n",
      "epoch:29 step:27609 [D loss: 0.491810, acc.: 77.34%] [G loss: 0.819682]\n",
      "epoch:29 step:27610 [D loss: 0.665377, acc.: 65.62%] [G loss: 0.771507]\n",
      "epoch:29 step:27611 [D loss: 0.574320, acc.: 68.75%] [G loss: 0.805300]\n",
      "epoch:29 step:27612 [D loss: 0.524535, acc.: 64.84%] [G loss: 1.050119]\n",
      "epoch:29 step:27613 [D loss: 0.474037, acc.: 78.91%] [G loss: 0.738208]\n",
      "epoch:29 step:27614 [D loss: 0.484087, acc.: 77.34%] [G loss: 0.792967]\n",
      "epoch:29 step:27615 [D loss: 0.524710, acc.: 71.88%] [G loss: 0.857414]\n",
      "epoch:29 step:27616 [D loss: 0.538057, acc.: 69.53%] [G loss: 0.698568]\n",
      "epoch:29 step:27617 [D loss: 0.467981, acc.: 77.34%] [G loss: 1.024531]\n",
      "epoch:29 step:27618 [D loss: 0.559425, acc.: 67.19%] [G loss: 0.721822]\n",
      "epoch:29 step:27619 [D loss: 0.553558, acc.: 70.31%] [G loss: 0.840219]\n",
      "epoch:29 step:27620 [D loss: 0.526941, acc.: 68.75%] [G loss: 0.831196]\n",
      "epoch:29 step:27621 [D loss: 0.525725, acc.: 77.34%] [G loss: 0.937090]\n",
      "epoch:29 step:27622 [D loss: 0.479311, acc.: 77.34%] [G loss: 1.044346]\n",
      "epoch:29 step:27623 [D loss: 0.513796, acc.: 73.44%] [G loss: 0.683701]\n",
      "epoch:29 step:27624 [D loss: 0.429560, acc.: 81.25%] [G loss: 0.843447]\n",
      "epoch:29 step:27625 [D loss: 0.428083, acc.: 76.56%] [G loss: 0.870514]\n",
      "epoch:29 step:27626 [D loss: 0.479651, acc.: 76.56%] [G loss: 0.836910]\n",
      "epoch:29 step:27627 [D loss: 0.503923, acc.: 75.78%] [G loss: 0.729696]\n",
      "epoch:29 step:27628 [D loss: 0.530457, acc.: 74.22%] [G loss: 0.744859]\n",
      "epoch:29 step:27629 [D loss: 0.665956, acc.: 66.41%] [G loss: 0.598890]\n",
      "epoch:29 step:27630 [D loss: 0.433940, acc.: 80.47%] [G loss: 0.633546]\n",
      "epoch:29 step:27631 [D loss: 0.607876, acc.: 67.19%] [G loss: 0.644081]\n",
      "epoch:29 step:27632 [D loss: 0.510967, acc.: 77.34%] [G loss: 0.846757]\n",
      "epoch:29 step:27633 [D loss: 0.506744, acc.: 70.31%] [G loss: 0.974565]\n",
      "epoch:29 step:27634 [D loss: 0.604533, acc.: 65.62%] [G loss: 0.745489]\n",
      "epoch:29 step:27635 [D loss: 0.542523, acc.: 69.53%] [G loss: 0.711092]\n",
      "epoch:29 step:27636 [D loss: 0.552572, acc.: 70.31%] [G loss: 0.734136]\n",
      "epoch:29 step:27637 [D loss: 0.520842, acc.: 75.00%] [G loss: 0.684383]\n",
      "epoch:29 step:27638 [D loss: 0.559238, acc.: 75.00%] [G loss: 0.522382]\n",
      "epoch:29 step:27639 [D loss: 0.589547, acc.: 68.75%] [G loss: 0.474412]\n",
      "epoch:29 step:27640 [D loss: 0.527343, acc.: 68.75%] [G loss: 0.562336]\n",
      "epoch:29 step:27641 [D loss: 0.524361, acc.: 73.44%] [G loss: 0.530555]\n",
      "epoch:29 step:27642 [D loss: 0.502106, acc.: 75.78%] [G loss: 0.693424]\n",
      "epoch:29 step:27643 [D loss: 0.493665, acc.: 75.78%] [G loss: 0.780444]\n",
      "epoch:29 step:27644 [D loss: 0.428664, acc.: 80.47%] [G loss: 0.953094]\n",
      "epoch:29 step:27645 [D loss: 0.400763, acc.: 85.16%] [G loss: 1.040328]\n",
      "epoch:29 step:27646 [D loss: 0.655337, acc.: 60.94%] [G loss: 0.803807]\n",
      "epoch:29 step:27647 [D loss: 0.530704, acc.: 73.44%] [G loss: 0.785484]\n",
      "epoch:29 step:27648 [D loss: 0.511421, acc.: 74.22%] [G loss: 0.813228]\n",
      "epoch:29 step:27649 [D loss: 0.491309, acc.: 75.78%] [G loss: 0.864123]\n",
      "epoch:29 step:27650 [D loss: 0.608639, acc.: 67.97%] [G loss: 0.627311]\n",
      "epoch:29 step:27651 [D loss: 0.593624, acc.: 65.62%] [G loss: 0.631915]\n",
      "epoch:29 step:27652 [D loss: 0.480505, acc.: 76.56%] [G loss: 0.582597]\n",
      "epoch:29 step:27653 [D loss: 0.585450, acc.: 67.97%] [G loss: 0.619818]\n",
      "epoch:29 step:27654 [D loss: 0.495726, acc.: 78.91%] [G loss: 0.767647]\n",
      "epoch:29 step:27655 [D loss: 0.579377, acc.: 69.53%] [G loss: 0.667529]\n",
      "epoch:29 step:27656 [D loss: 0.510936, acc.: 72.66%] [G loss: 0.747614]\n",
      "epoch:29 step:27657 [D loss: 0.496060, acc.: 75.78%] [G loss: 0.737293]\n",
      "epoch:29 step:27658 [D loss: 0.579270, acc.: 70.31%] [G loss: 0.628777]\n",
      "epoch:29 step:27659 [D loss: 0.538171, acc.: 66.41%] [G loss: 0.778979]\n",
      "epoch:29 step:27660 [D loss: 0.559286, acc.: 69.53%] [G loss: 0.589760]\n",
      "epoch:29 step:27661 [D loss: 0.509769, acc.: 75.78%] [G loss: 0.685008]\n",
      "epoch:29 step:27662 [D loss: 0.463837, acc.: 76.56%] [G loss: 0.700126]\n",
      "epoch:29 step:27663 [D loss: 0.547589, acc.: 70.31%] [G loss: 0.554306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27664 [D loss: 0.511859, acc.: 73.44%] [G loss: 0.749061]\n",
      "epoch:29 step:27665 [D loss: 0.606689, acc.: 65.62%] [G loss: 0.591515]\n",
      "epoch:29 step:27666 [D loss: 0.510684, acc.: 76.56%] [G loss: 0.708492]\n",
      "epoch:29 step:27667 [D loss: 0.585782, acc.: 66.41%] [G loss: 0.574621]\n",
      "epoch:29 step:27668 [D loss: 0.487093, acc.: 78.12%] [G loss: 0.802252]\n",
      "epoch:29 step:27669 [D loss: 0.506414, acc.: 75.78%] [G loss: 0.702934]\n",
      "epoch:29 step:27670 [D loss: 0.622813, acc.: 65.62%] [G loss: 0.571254]\n",
      "epoch:29 step:27671 [D loss: 0.524819, acc.: 76.56%] [G loss: 0.820066]\n",
      "epoch:29 step:27672 [D loss: 0.435969, acc.: 82.81%] [G loss: 0.993614]\n",
      "epoch:29 step:27673 [D loss: 0.521836, acc.: 76.56%] [G loss: 0.916900]\n",
      "epoch:29 step:27674 [D loss: 0.634260, acc.: 64.84%] [G loss: 0.666073]\n",
      "epoch:29 step:27675 [D loss: 0.673946, acc.: 60.16%] [G loss: 0.559450]\n",
      "epoch:29 step:27676 [D loss: 0.504280, acc.: 73.44%] [G loss: 0.662902]\n",
      "epoch:29 step:27677 [D loss: 0.469293, acc.: 78.91%] [G loss: 0.657392]\n",
      "epoch:29 step:27678 [D loss: 0.473284, acc.: 78.91%] [G loss: 0.961111]\n",
      "epoch:29 step:27679 [D loss: 0.520169, acc.: 70.31%] [G loss: 0.695990]\n",
      "epoch:29 step:27680 [D loss: 0.615001, acc.: 64.06%] [G loss: 0.748647]\n",
      "epoch:29 step:27681 [D loss: 0.440563, acc.: 80.47%] [G loss: 0.950044]\n",
      "epoch:29 step:27682 [D loss: 0.484479, acc.: 75.78%] [G loss: 0.975146]\n",
      "epoch:29 step:27683 [D loss: 0.618754, acc.: 66.41%] [G loss: 0.674321]\n",
      "epoch:29 step:27684 [D loss: 0.657712, acc.: 59.38%] [G loss: 0.594302]\n",
      "epoch:29 step:27685 [D loss: 0.645958, acc.: 64.84%] [G loss: 0.618071]\n",
      "epoch:29 step:27686 [D loss: 0.488780, acc.: 76.56%] [G loss: 0.521054]\n",
      "epoch:29 step:27687 [D loss: 0.476220, acc.: 73.44%] [G loss: 0.745787]\n",
      "epoch:29 step:27688 [D loss: 0.546155, acc.: 70.31%] [G loss: 0.610409]\n",
      "epoch:29 step:27689 [D loss: 0.507970, acc.: 73.44%] [G loss: 0.801822]\n",
      "epoch:29 step:27690 [D loss: 0.490892, acc.: 75.00%] [G loss: 0.796281]\n",
      "epoch:29 step:27691 [D loss: 0.507218, acc.: 71.09%] [G loss: 0.822565]\n",
      "epoch:29 step:27692 [D loss: 0.482296, acc.: 76.56%] [G loss: 0.771607]\n",
      "epoch:29 step:27693 [D loss: 0.477236, acc.: 73.44%] [G loss: 0.965256]\n",
      "epoch:29 step:27694 [D loss: 0.491794, acc.: 78.91%] [G loss: 0.875138]\n",
      "epoch:29 step:27695 [D loss: 0.484573, acc.: 73.44%] [G loss: 0.747801]\n",
      "epoch:29 step:27696 [D loss: 0.497128, acc.: 71.88%] [G loss: 0.725036]\n",
      "epoch:29 step:27697 [D loss: 0.544408, acc.: 69.53%] [G loss: 0.690572]\n",
      "epoch:29 step:27698 [D loss: 0.575554, acc.: 67.19%] [G loss: 0.653544]\n",
      "epoch:29 step:27699 [D loss: 0.535892, acc.: 71.88%] [G loss: 0.777723]\n",
      "epoch:29 step:27700 [D loss: 0.523707, acc.: 69.53%] [G loss: 0.695097]\n",
      "epoch:29 step:27701 [D loss: 0.663620, acc.: 58.59%] [G loss: 0.561996]\n",
      "epoch:29 step:27702 [D loss: 0.552582, acc.: 69.53%] [G loss: 0.518745]\n",
      "epoch:29 step:27703 [D loss: 0.570812, acc.: 64.06%] [G loss: 0.809084]\n",
      "epoch:29 step:27704 [D loss: 0.564730, acc.: 67.97%] [G loss: 0.738678]\n",
      "epoch:29 step:27705 [D loss: 0.540138, acc.: 70.31%] [G loss: 0.681157]\n",
      "epoch:29 step:27706 [D loss: 0.470312, acc.: 78.91%] [G loss: 0.736054]\n",
      "epoch:29 step:27707 [D loss: 0.502990, acc.: 73.44%] [G loss: 0.917378]\n",
      "epoch:29 step:27708 [D loss: 0.674935, acc.: 64.06%] [G loss: 0.610203]\n",
      "epoch:29 step:27709 [D loss: 0.473853, acc.: 76.56%] [G loss: 0.653757]\n",
      "epoch:29 step:27710 [D loss: 0.577069, acc.: 66.41%] [G loss: 0.690920]\n",
      "epoch:29 step:27711 [D loss: 0.526783, acc.: 72.66%] [G loss: 0.573182]\n",
      "epoch:29 step:27712 [D loss: 0.547175, acc.: 66.41%] [G loss: 0.683153]\n",
      "epoch:29 step:27713 [D loss: 0.585104, acc.: 67.97%] [G loss: 0.795614]\n",
      "epoch:29 step:27714 [D loss: 0.539074, acc.: 68.75%] [G loss: 0.858624]\n",
      "epoch:29 step:27715 [D loss: 0.576395, acc.: 67.19%] [G loss: 0.669148]\n",
      "epoch:29 step:27716 [D loss: 0.558774, acc.: 67.97%] [G loss: 0.472708]\n",
      "epoch:29 step:27717 [D loss: 0.504772, acc.: 77.34%] [G loss: 0.583525]\n",
      "epoch:29 step:27718 [D loss: 0.514542, acc.: 74.22%] [G loss: 0.626539]\n",
      "epoch:29 step:27719 [D loss: 0.505115, acc.: 72.66%] [G loss: 0.720175]\n",
      "epoch:29 step:27720 [D loss: 0.538904, acc.: 71.88%] [G loss: 0.800418]\n",
      "epoch:29 step:27721 [D loss: 0.549201, acc.: 71.88%] [G loss: 0.740371]\n",
      "epoch:29 step:27722 [D loss: 0.549610, acc.: 75.78%] [G loss: 0.780605]\n",
      "epoch:29 step:27723 [D loss: 0.528011, acc.: 75.78%] [G loss: 0.665381]\n",
      "epoch:29 step:27724 [D loss: 0.527132, acc.: 76.56%] [G loss: 0.687011]\n",
      "epoch:29 step:27725 [D loss: 0.534613, acc.: 72.66%] [G loss: 0.828485]\n",
      "epoch:29 step:27726 [D loss: 0.572385, acc.: 67.19%] [G loss: 0.633910]\n",
      "epoch:29 step:27727 [D loss: 0.430882, acc.: 79.69%] [G loss: 0.833914]\n",
      "epoch:29 step:27728 [D loss: 0.429158, acc.: 78.12%] [G loss: 0.994231]\n",
      "epoch:29 step:27729 [D loss: 0.548748, acc.: 71.09%] [G loss: 0.691625]\n",
      "epoch:29 step:27730 [D loss: 0.503823, acc.: 73.44%] [G loss: 0.806655]\n",
      "epoch:29 step:27731 [D loss: 0.464954, acc.: 80.47%] [G loss: 0.907200]\n",
      "epoch:29 step:27732 [D loss: 0.598392, acc.: 67.97%] [G loss: 0.955684]\n",
      "epoch:29 step:27733 [D loss: 0.500231, acc.: 76.56%] [G loss: 0.732970]\n",
      "epoch:29 step:27734 [D loss: 0.579986, acc.: 63.28%] [G loss: 0.567466]\n",
      "epoch:29 step:27735 [D loss: 0.549796, acc.: 73.44%] [G loss: 0.757487]\n",
      "epoch:29 step:27736 [D loss: 0.527617, acc.: 69.53%] [G loss: 0.521209]\n",
      "epoch:29 step:27737 [D loss: 0.526023, acc.: 72.66%] [G loss: 0.851390]\n",
      "epoch:29 step:27738 [D loss: 0.627831, acc.: 66.41%] [G loss: 0.785315]\n",
      "epoch:29 step:27739 [D loss: 0.673639, acc.: 64.06%] [G loss: 0.600740]\n",
      "epoch:29 step:27740 [D loss: 0.469621, acc.: 72.66%] [G loss: 0.930328]\n",
      "epoch:29 step:27741 [D loss: 0.514577, acc.: 74.22%] [G loss: 0.679694]\n",
      "epoch:29 step:27742 [D loss: 0.541610, acc.: 71.09%] [G loss: 0.733984]\n",
      "epoch:29 step:27743 [D loss: 0.506930, acc.: 74.22%] [G loss: 0.725776]\n",
      "epoch:29 step:27744 [D loss: 0.526128, acc.: 74.22%] [G loss: 0.795015]\n",
      "epoch:29 step:27745 [D loss: 0.558711, acc.: 72.66%] [G loss: 0.625412]\n",
      "epoch:29 step:27746 [D loss: 0.518020, acc.: 77.34%] [G loss: 0.776819]\n",
      "epoch:29 step:27747 [D loss: 0.487066, acc.: 77.34%] [G loss: 0.865498]\n",
      "epoch:29 step:27748 [D loss: 0.494881, acc.: 70.31%] [G loss: 0.891021]\n",
      "epoch:29 step:27749 [D loss: 0.650761, acc.: 61.72%] [G loss: 0.798896]\n",
      "epoch:29 step:27750 [D loss: 0.588703, acc.: 65.62%] [G loss: 0.600179]\n",
      "epoch:29 step:27751 [D loss: 0.532944, acc.: 71.88%] [G loss: 0.881182]\n",
      "epoch:29 step:27752 [D loss: 0.514771, acc.: 75.00%] [G loss: 0.586252]\n",
      "epoch:29 step:27753 [D loss: 0.531812, acc.: 70.31%] [G loss: 0.624777]\n",
      "epoch:29 step:27754 [D loss: 0.554655, acc.: 69.53%] [G loss: 0.713981]\n",
      "epoch:29 step:27755 [D loss: 0.445193, acc.: 78.91%] [G loss: 0.986447]\n",
      "epoch:29 step:27756 [D loss: 0.536732, acc.: 73.44%] [G loss: 1.059906]\n",
      "epoch:29 step:27757 [D loss: 0.573749, acc.: 67.19%] [G loss: 0.876048]\n",
      "epoch:29 step:27758 [D loss: 0.535438, acc.: 67.19%] [G loss: 0.816091]\n",
      "epoch:29 step:27759 [D loss: 0.577744, acc.: 66.41%] [G loss: 0.606312]\n",
      "epoch:29 step:27760 [D loss: 0.551156, acc.: 68.75%] [G loss: 0.653320]\n",
      "epoch:29 step:27761 [D loss: 0.524137, acc.: 73.44%] [G loss: 0.645231]\n",
      "epoch:29 step:27762 [D loss: 0.508771, acc.: 71.09%] [G loss: 0.675563]\n",
      "epoch:29 step:27763 [D loss: 0.513152, acc.: 75.00%] [G loss: 0.856685]\n",
      "epoch:29 step:27764 [D loss: 0.614612, acc.: 66.41%] [G loss: 0.504429]\n",
      "epoch:29 step:27765 [D loss: 0.517598, acc.: 71.88%] [G loss: 0.618630]\n",
      "epoch:29 step:27766 [D loss: 0.480664, acc.: 78.91%] [G loss: 0.650072]\n",
      "epoch:29 step:27767 [D loss: 0.540717, acc.: 70.31%] [G loss: 0.638508]\n",
      "epoch:29 step:27768 [D loss: 0.525671, acc.: 73.44%] [G loss: 0.694173]\n",
      "epoch:29 step:27769 [D loss: 0.513633, acc.: 67.19%] [G loss: 0.789214]\n",
      "epoch:29 step:27770 [D loss: 0.512005, acc.: 74.22%] [G loss: 0.841478]\n",
      "epoch:29 step:27771 [D loss: 0.517345, acc.: 75.00%] [G loss: 0.699356]\n",
      "epoch:29 step:27772 [D loss: 0.497577, acc.: 75.00%] [G loss: 0.831412]\n",
      "epoch:29 step:27773 [D loss: 0.570540, acc.: 69.53%] [G loss: 0.746839]\n",
      "epoch:29 step:27774 [D loss: 0.633342, acc.: 67.97%] [G loss: 0.608864]\n",
      "epoch:29 step:27775 [D loss: 0.511865, acc.: 76.56%] [G loss: 0.788772]\n",
      "epoch:29 step:27776 [D loss: 0.508860, acc.: 72.66%] [G loss: 0.737864]\n",
      "epoch:29 step:27777 [D loss: 0.489345, acc.: 75.78%] [G loss: 0.806070]\n",
      "epoch:29 step:27778 [D loss: 0.445226, acc.: 77.34%] [G loss: 0.802357]\n",
      "epoch:29 step:27779 [D loss: 0.569707, acc.: 70.31%] [G loss: 0.745848]\n",
      "epoch:29 step:27780 [D loss: 0.544323, acc.: 70.31%] [G loss: 0.884665]\n",
      "epoch:29 step:27781 [D loss: 0.536490, acc.: 72.66%] [G loss: 0.811901]\n",
      "epoch:29 step:27782 [D loss: 0.481994, acc.: 74.22%] [G loss: 0.568802]\n",
      "epoch:29 step:27783 [D loss: 0.555213, acc.: 71.09%] [G loss: 0.417682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27784 [D loss: 0.521076, acc.: 71.88%] [G loss: 0.582647]\n",
      "epoch:29 step:27785 [D loss: 0.551839, acc.: 69.53%] [G loss: 0.586400]\n",
      "epoch:29 step:27786 [D loss: 0.462265, acc.: 73.44%] [G loss: 0.725420]\n",
      "epoch:29 step:27787 [D loss: 0.557061, acc.: 68.75%] [G loss: 0.671602]\n",
      "epoch:29 step:27788 [D loss: 0.507858, acc.: 72.66%] [G loss: 0.818577]\n",
      "epoch:29 step:27789 [D loss: 0.541739, acc.: 67.97%] [G loss: 0.694025]\n",
      "epoch:29 step:27790 [D loss: 0.550446, acc.: 72.66%] [G loss: 0.770726]\n",
      "epoch:29 step:27791 [D loss: 0.562557, acc.: 72.66%] [G loss: 0.747409]\n",
      "epoch:29 step:27792 [D loss: 0.530052, acc.: 66.41%] [G loss: 0.741035]\n",
      "epoch:29 step:27793 [D loss: 0.492938, acc.: 73.44%] [G loss: 0.848553]\n",
      "epoch:29 step:27794 [D loss: 0.516610, acc.: 71.09%] [G loss: 0.810171]\n",
      "epoch:29 step:27795 [D loss: 0.541610, acc.: 70.31%] [G loss: 0.829842]\n",
      "epoch:29 step:27796 [D loss: 0.464996, acc.: 82.03%] [G loss: 0.767588]\n",
      "epoch:29 step:27797 [D loss: 0.475378, acc.: 77.34%] [G loss: 0.836467]\n",
      "epoch:29 step:27798 [D loss: 0.564601, acc.: 67.19%] [G loss: 0.825628]\n",
      "epoch:29 step:27799 [D loss: 0.520757, acc.: 71.88%] [G loss: 0.861235]\n",
      "epoch:29 step:27800 [D loss: 0.575691, acc.: 68.75%] [G loss: 0.786223]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.669224\n",
      "FID: 52.148224\n",
      "0 = 13.049212102317835\n",
      "1 = 0.10106360338277542\n",
      "2 = 0.8540999889373779\n",
      "3 = 0.8222000002861023\n",
      "4 = 0.8859999775886536\n",
      "5 = 0.8782311677932739\n",
      "6 = 0.8222000002861023\n",
      "7 = 8.471425818657874\n",
      "8 = 0.15811606943596654\n",
      "9 = 0.6837000250816345\n",
      "10 = 0.6826000213623047\n",
      "11 = 0.6848000288009644\n",
      "12 = 0.6841050386428833\n",
      "13 = 0.6826000213623047\n",
      "14 = 6.669248580932617\n",
      "15 = 6.696261405944824\n",
      "16 = 0.4106893837451935\n",
      "17 = 6.669223785400391\n",
      "18 = 52.148223876953125\n",
      "epoch:29 step:27801 [D loss: 0.621777, acc.: 63.28%] [G loss: 0.592539]\n",
      "epoch:29 step:27802 [D loss: 0.516042, acc.: 71.09%] [G loss: 0.784311]\n",
      "epoch:29 step:27803 [D loss: 0.542732, acc.: 69.53%] [G loss: 0.904176]\n",
      "epoch:29 step:27804 [D loss: 0.509552, acc.: 75.00%] [G loss: 0.937549]\n",
      "epoch:29 step:27805 [D loss: 0.562957, acc.: 70.31%] [G loss: 0.812996]\n",
      "epoch:29 step:27806 [D loss: 0.510620, acc.: 72.66%] [G loss: 0.939069]\n",
      "epoch:29 step:27807 [D loss: 0.509056, acc.: 74.22%] [G loss: 0.751318]\n",
      "epoch:29 step:27808 [D loss: 0.516685, acc.: 71.88%] [G loss: 0.890005]\n",
      "epoch:29 step:27809 [D loss: 0.586667, acc.: 71.09%] [G loss: 0.755991]\n",
      "epoch:29 step:27810 [D loss: 0.554329, acc.: 71.09%] [G loss: 0.590083]\n",
      "epoch:29 step:27811 [D loss: 0.502107, acc.: 74.22%] [G loss: 0.739770]\n",
      "epoch:29 step:27812 [D loss: 0.481467, acc.: 73.44%] [G loss: 0.981019]\n",
      "epoch:29 step:27813 [D loss: 0.584465, acc.: 64.84%] [G loss: 0.682701]\n",
      "epoch:29 step:27814 [D loss: 0.508482, acc.: 75.00%] [G loss: 0.888196]\n",
      "epoch:29 step:27815 [D loss: 0.490811, acc.: 72.66%] [G loss: 0.716334]\n",
      "epoch:29 step:27816 [D loss: 0.530185, acc.: 67.97%] [G loss: 0.771184]\n",
      "epoch:29 step:27817 [D loss: 0.517481, acc.: 70.31%] [G loss: 0.778824]\n",
      "epoch:29 step:27818 [D loss: 0.517957, acc.: 74.22%] [G loss: 0.692821]\n",
      "epoch:29 step:27819 [D loss: 0.476169, acc.: 78.12%] [G loss: 0.725399]\n",
      "epoch:29 step:27820 [D loss: 0.454810, acc.: 80.47%] [G loss: 0.879642]\n",
      "epoch:29 step:27821 [D loss: 0.438023, acc.: 83.59%] [G loss: 1.012771]\n",
      "epoch:29 step:27822 [D loss: 0.541245, acc.: 72.66%] [G loss: 1.033332]\n",
      "epoch:29 step:27823 [D loss: 0.444850, acc.: 78.91%] [G loss: 0.878633]\n",
      "epoch:29 step:27824 [D loss: 0.511947, acc.: 73.44%] [G loss: 1.008777]\n",
      "epoch:29 step:27825 [D loss: 0.649630, acc.: 64.84%] [G loss: 0.672442]\n",
      "epoch:29 step:27826 [D loss: 0.554886, acc.: 73.44%] [G loss: 0.973417]\n",
      "epoch:29 step:27827 [D loss: 0.455446, acc.: 75.00%] [G loss: 0.872502]\n",
      "epoch:29 step:27828 [D loss: 0.550469, acc.: 71.09%] [G loss: 0.805203]\n",
      "epoch:29 step:27829 [D loss: 0.529376, acc.: 71.09%] [G loss: 0.746317]\n",
      "epoch:29 step:27830 [D loss: 0.532574, acc.: 71.09%] [G loss: 0.885874]\n",
      "epoch:29 step:27831 [D loss: 0.564319, acc.: 68.75%] [G loss: 0.900220]\n",
      "epoch:29 step:27832 [D loss: 0.524583, acc.: 72.66%] [G loss: 1.046819]\n",
      "epoch:29 step:27833 [D loss: 0.529158, acc.: 71.88%] [G loss: 1.000992]\n",
      "epoch:29 step:27834 [D loss: 0.506416, acc.: 78.91%] [G loss: 0.795809]\n",
      "epoch:29 step:27835 [D loss: 0.569562, acc.: 68.75%] [G loss: 0.692929]\n",
      "epoch:29 step:27836 [D loss: 0.595538, acc.: 67.97%] [G loss: 0.638299]\n",
      "epoch:29 step:27837 [D loss: 0.544395, acc.: 70.31%] [G loss: 0.867009]\n",
      "epoch:29 step:27838 [D loss: 0.575051, acc.: 64.84%] [G loss: 0.765177]\n",
      "epoch:29 step:27839 [D loss: 0.575141, acc.: 67.97%] [G loss: 0.539908]\n",
      "epoch:29 step:27840 [D loss: 0.498531, acc.: 77.34%] [G loss: 0.879031]\n",
      "epoch:29 step:27841 [D loss: 0.526612, acc.: 73.44%] [G loss: 0.818228]\n",
      "epoch:29 step:27842 [D loss: 0.531653, acc.: 73.44%] [G loss: 0.689240]\n",
      "epoch:29 step:27843 [D loss: 0.549547, acc.: 70.31%] [G loss: 0.779039]\n",
      "epoch:29 step:27844 [D loss: 0.571757, acc.: 69.53%] [G loss: 0.664600]\n",
      "epoch:29 step:27845 [D loss: 0.511065, acc.: 77.34%] [G loss: 0.775787]\n",
      "epoch:29 step:27846 [D loss: 0.553177, acc.: 70.31%] [G loss: 0.669036]\n",
      "epoch:29 step:27847 [D loss: 0.570814, acc.: 67.97%] [G loss: 0.681656]\n",
      "epoch:29 step:27848 [D loss: 0.572936, acc.: 66.41%] [G loss: 0.768293]\n",
      "epoch:29 step:27849 [D loss: 0.489388, acc.: 76.56%] [G loss: 0.592637]\n",
      "epoch:29 step:27850 [D loss: 0.481721, acc.: 74.22%] [G loss: 0.754732]\n",
      "epoch:29 step:27851 [D loss: 0.574049, acc.: 70.31%] [G loss: 0.734584]\n",
      "epoch:29 step:27852 [D loss: 0.509730, acc.: 75.78%] [G loss: 0.640949]\n",
      "epoch:29 step:27853 [D loss: 0.535363, acc.: 75.78%] [G loss: 0.746455]\n",
      "epoch:29 step:27854 [D loss: 0.428786, acc.: 82.03%] [G loss: 0.802231]\n",
      "epoch:29 step:27855 [D loss: 0.562552, acc.: 70.31%] [G loss: 0.625135]\n",
      "epoch:29 step:27856 [D loss: 0.513707, acc.: 75.78%] [G loss: 0.548403]\n",
      "epoch:29 step:27857 [D loss: 0.545860, acc.: 71.09%] [G loss: 0.600774]\n",
      "epoch:29 step:27858 [D loss: 0.548224, acc.: 70.31%] [G loss: 0.553876]\n",
      "epoch:29 step:27859 [D loss: 0.552620, acc.: 68.75%] [G loss: 0.662634]\n",
      "epoch:29 step:27860 [D loss: 0.548477, acc.: 64.84%] [G loss: 0.584383]\n",
      "epoch:29 step:27861 [D loss: 0.474781, acc.: 74.22%] [G loss: 0.754613]\n",
      "epoch:29 step:27862 [D loss: 0.580635, acc.: 68.75%] [G loss: 0.843180]\n",
      "epoch:29 step:27863 [D loss: 0.484430, acc.: 75.00%] [G loss: 0.825146]\n",
      "epoch:29 step:27864 [D loss: 0.529638, acc.: 73.44%] [G loss: 0.802935]\n",
      "epoch:29 step:27865 [D loss: 0.545817, acc.: 73.44%] [G loss: 0.774938]\n",
      "epoch:29 step:27866 [D loss: 0.492673, acc.: 76.56%] [G loss: 0.846941]\n",
      "epoch:29 step:27867 [D loss: 0.500833, acc.: 72.66%] [G loss: 0.766838]\n",
      "epoch:29 step:27868 [D loss: 0.526150, acc.: 72.66%] [G loss: 0.759743]\n",
      "epoch:29 step:27869 [D loss: 0.684526, acc.: 59.38%] [G loss: 0.488606]\n",
      "epoch:29 step:27870 [D loss: 0.551930, acc.: 70.31%] [G loss: 0.684151]\n",
      "epoch:29 step:27871 [D loss: 0.567649, acc.: 67.19%] [G loss: 0.561764]\n",
      "epoch:29 step:27872 [D loss: 0.547452, acc.: 71.88%] [G loss: 0.647521]\n",
      "epoch:29 step:27873 [D loss: 0.549852, acc.: 73.44%] [G loss: 0.795917]\n",
      "epoch:29 step:27874 [D loss: 0.501464, acc.: 76.56%] [G loss: 0.933953]\n",
      "epoch:29 step:27875 [D loss: 0.581778, acc.: 70.31%] [G loss: 0.814935]\n",
      "epoch:29 step:27876 [D loss: 0.680147, acc.: 57.03%] [G loss: 0.734861]\n",
      "epoch:29 step:27877 [D loss: 0.582240, acc.: 68.75%] [G loss: 0.682039]\n",
      "epoch:29 step:27878 [D loss: 0.480323, acc.: 77.34%] [G loss: 0.754524]\n",
      "epoch:29 step:27879 [D loss: 0.505529, acc.: 75.00%] [G loss: 0.630595]\n",
      "epoch:29 step:27880 [D loss: 0.502428, acc.: 71.88%] [G loss: 0.768168]\n",
      "epoch:29 step:27881 [D loss: 0.482679, acc.: 75.78%] [G loss: 0.756407]\n",
      "epoch:29 step:27882 [D loss: 0.577502, acc.: 61.72%] [G loss: 0.625440]\n",
      "epoch:29 step:27883 [D loss: 0.530691, acc.: 71.09%] [G loss: 0.694353]\n",
      "epoch:29 step:27884 [D loss: 0.527840, acc.: 67.97%] [G loss: 0.774688]\n",
      "epoch:29 step:27885 [D loss: 0.519117, acc.: 72.66%] [G loss: 0.809161]\n",
      "epoch:29 step:27886 [D loss: 0.585160, acc.: 63.28%] [G loss: 0.907220]\n",
      "epoch:29 step:27887 [D loss: 0.494355, acc.: 75.00%] [G loss: 0.651842]\n",
      "epoch:29 step:27888 [D loss: 0.541538, acc.: 72.66%] [G loss: 0.777292]\n",
      "epoch:29 step:27889 [D loss: 0.568614, acc.: 67.19%] [G loss: 0.750517]\n",
      "epoch:29 step:27890 [D loss: 0.561991, acc.: 70.31%] [G loss: 0.722913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27891 [D loss: 0.557460, acc.: 67.97%] [G loss: 0.674397]\n",
      "epoch:29 step:27892 [D loss: 0.466248, acc.: 75.78%] [G loss: 0.792189]\n",
      "epoch:29 step:27893 [D loss: 0.605151, acc.: 71.09%] [G loss: 0.695493]\n",
      "epoch:29 step:27894 [D loss: 0.543757, acc.: 68.75%] [G loss: 0.643199]\n",
      "epoch:29 step:27895 [D loss: 0.539034, acc.: 71.88%] [G loss: 0.577513]\n",
      "epoch:29 step:27896 [D loss: 0.571698, acc.: 67.97%] [G loss: 0.575531]\n",
      "epoch:29 step:27897 [D loss: 0.510163, acc.: 73.44%] [G loss: 0.864354]\n",
      "epoch:29 step:27898 [D loss: 0.448110, acc.: 79.69%] [G loss: 0.707789]\n",
      "epoch:29 step:27899 [D loss: 0.509399, acc.: 74.22%] [G loss: 0.771020]\n",
      "epoch:29 step:27900 [D loss: 0.521286, acc.: 73.44%] [G loss: 0.780774]\n",
      "epoch:29 step:27901 [D loss: 0.537573, acc.: 72.66%] [G loss: 0.620326]\n",
      "epoch:29 step:27902 [D loss: 0.523588, acc.: 67.97%] [G loss: 0.761965]\n",
      "epoch:29 step:27903 [D loss: 0.496446, acc.: 78.12%] [G loss: 0.624112]\n",
      "epoch:29 step:27904 [D loss: 0.622820, acc.: 64.06%] [G loss: 0.592057]\n",
      "epoch:29 step:27905 [D loss: 0.528465, acc.: 71.09%] [G loss: 0.677421]\n",
      "epoch:29 step:27906 [D loss: 0.539836, acc.: 74.22%] [G loss: 0.683020]\n",
      "epoch:29 step:27907 [D loss: 0.528960, acc.: 71.09%] [G loss: 0.568755]\n",
      "epoch:29 step:27908 [D loss: 0.524624, acc.: 70.31%] [G loss: 0.767033]\n",
      "epoch:29 step:27909 [D loss: 0.495815, acc.: 73.44%] [G loss: 0.770415]\n",
      "epoch:29 step:27910 [D loss: 0.517195, acc.: 71.09%] [G loss: 0.734585]\n",
      "epoch:29 step:27911 [D loss: 0.527430, acc.: 74.22%] [G loss: 0.629845]\n",
      "epoch:29 step:27912 [D loss: 0.557944, acc.: 71.88%] [G loss: 0.839718]\n",
      "epoch:29 step:27913 [D loss: 0.602586, acc.: 65.62%] [G loss: 0.549082]\n",
      "epoch:29 step:27914 [D loss: 0.483780, acc.: 72.66%] [G loss: 0.656848]\n",
      "epoch:29 step:27915 [D loss: 0.519664, acc.: 71.88%] [G loss: 0.767612]\n",
      "epoch:29 step:27916 [D loss: 0.556250, acc.: 69.53%] [G loss: 0.743300]\n",
      "epoch:29 step:27917 [D loss: 0.493506, acc.: 75.78%] [G loss: 0.754058]\n",
      "epoch:29 step:27918 [D loss: 0.555370, acc.: 64.84%] [G loss: 0.900188]\n",
      "epoch:29 step:27919 [D loss: 0.490621, acc.: 73.44%] [G loss: 0.803794]\n",
      "epoch:29 step:27920 [D loss: 0.416871, acc.: 80.47%] [G loss: 0.912024]\n",
      "epoch:29 step:27921 [D loss: 0.563252, acc.: 68.75%] [G loss: 0.777770]\n",
      "epoch:29 step:27922 [D loss: 0.533617, acc.: 69.53%] [G loss: 0.817760]\n",
      "epoch:29 step:27923 [D loss: 0.492133, acc.: 75.00%] [G loss: 0.898990]\n",
      "epoch:29 step:27924 [D loss: 0.499390, acc.: 71.09%] [G loss: 0.912461]\n",
      "epoch:29 step:27925 [D loss: 0.588821, acc.: 67.97%] [G loss: 0.958845]\n",
      "epoch:29 step:27926 [D loss: 0.553855, acc.: 71.09%] [G loss: 0.835197]\n",
      "epoch:29 step:27927 [D loss: 0.580830, acc.: 67.19%] [G loss: 0.678837]\n",
      "epoch:29 step:27928 [D loss: 0.517999, acc.: 70.31%] [G loss: 0.616170]\n",
      "epoch:29 step:27929 [D loss: 0.510124, acc.: 75.00%] [G loss: 0.652641]\n",
      "epoch:29 step:27930 [D loss: 0.595258, acc.: 68.75%] [G loss: 0.692291]\n",
      "epoch:29 step:27931 [D loss: 0.579746, acc.: 65.62%] [G loss: 0.641830]\n",
      "epoch:29 step:27932 [D loss: 0.557326, acc.: 69.53%] [G loss: 0.716993]\n",
      "epoch:29 step:27933 [D loss: 0.559671, acc.: 69.53%] [G loss: 0.680509]\n",
      "epoch:29 step:27934 [D loss: 0.572174, acc.: 67.19%] [G loss: 0.664265]\n",
      "epoch:29 step:27935 [D loss: 0.553551, acc.: 71.88%] [G loss: 0.678246]\n",
      "epoch:29 step:27936 [D loss: 0.537675, acc.: 71.88%] [G loss: 0.711711]\n",
      "epoch:29 step:27937 [D loss: 0.580420, acc.: 67.19%] [G loss: 0.648199]\n",
      "epoch:29 step:27938 [D loss: 0.573996, acc.: 67.97%] [G loss: 0.598777]\n",
      "epoch:29 step:27939 [D loss: 0.618820, acc.: 66.41%] [G loss: 0.483155]\n",
      "epoch:29 step:27940 [D loss: 0.540935, acc.: 68.75%] [G loss: 0.664976]\n",
      "epoch:29 step:27941 [D loss: 0.498805, acc.: 75.00%] [G loss: 0.762543]\n",
      "epoch:29 step:27942 [D loss: 0.443780, acc.: 81.25%] [G loss: 0.826052]\n",
      "epoch:29 step:27943 [D loss: 0.536078, acc.: 75.78%] [G loss: 0.925976]\n",
      "epoch:29 step:27944 [D loss: 0.505991, acc.: 70.31%] [G loss: 0.869912]\n",
      "epoch:29 step:27945 [D loss: 0.566602, acc.: 69.53%] [G loss: 0.751488]\n",
      "epoch:29 step:27946 [D loss: 0.584173, acc.: 69.53%] [G loss: 0.716511]\n",
      "epoch:29 step:27947 [D loss: 0.458093, acc.: 78.12%] [G loss: 0.710835]\n",
      "epoch:29 step:27948 [D loss: 0.527102, acc.: 74.22%] [G loss: 0.731681]\n",
      "epoch:29 step:27949 [D loss: 0.574451, acc.: 71.09%] [G loss: 0.698278]\n",
      "epoch:29 step:27950 [D loss: 0.596601, acc.: 67.97%] [G loss: 0.782961]\n",
      "epoch:29 step:27951 [D loss: 0.554160, acc.: 72.66%] [G loss: 0.628926]\n",
      "epoch:29 step:27952 [D loss: 0.542521, acc.: 71.09%] [G loss: 0.513947]\n",
      "epoch:29 step:27953 [D loss: 0.529073, acc.: 76.56%] [G loss: 0.709732]\n",
      "epoch:29 step:27954 [D loss: 0.489184, acc.: 72.66%] [G loss: 0.984763]\n",
      "epoch:29 step:27955 [D loss: 0.515255, acc.: 71.09%] [G loss: 0.802120]\n",
      "epoch:29 step:27956 [D loss: 0.529469, acc.: 73.44%] [G loss: 0.783133]\n",
      "epoch:29 step:27957 [D loss: 0.605286, acc.: 67.19%] [G loss: 0.639297]\n",
      "epoch:29 step:27958 [D loss: 0.517558, acc.: 71.88%] [G loss: 0.636466]\n",
      "epoch:29 step:27959 [D loss: 0.509181, acc.: 72.66%] [G loss: 0.671861]\n",
      "epoch:29 step:27960 [D loss: 0.560390, acc.: 68.75%] [G loss: 0.652555]\n",
      "epoch:29 step:27961 [D loss: 0.625233, acc.: 60.16%] [G loss: 0.548936]\n",
      "epoch:29 step:27962 [D loss: 0.537446, acc.: 67.19%] [G loss: 0.453053]\n",
      "epoch:29 step:27963 [D loss: 0.503123, acc.: 75.78%] [G loss: 0.729935]\n",
      "epoch:29 step:27964 [D loss: 0.477704, acc.: 76.56%] [G loss: 0.695691]\n",
      "epoch:29 step:27965 [D loss: 0.494576, acc.: 73.44%] [G loss: 0.671519]\n",
      "epoch:29 step:27966 [D loss: 0.577313, acc.: 68.75%] [G loss: 0.679229]\n",
      "epoch:29 step:27967 [D loss: 0.674744, acc.: 60.16%] [G loss: 0.913984]\n",
      "epoch:29 step:27968 [D loss: 0.536561, acc.: 71.09%] [G loss: 0.663093]\n",
      "epoch:29 step:27969 [D loss: 0.573892, acc.: 67.19%] [G loss: 0.742500]\n",
      "epoch:29 step:27970 [D loss: 0.494367, acc.: 73.44%] [G loss: 0.990805]\n",
      "epoch:29 step:27971 [D loss: 0.575233, acc.: 68.75%] [G loss: 0.769939]\n",
      "epoch:29 step:27972 [D loss: 0.604930, acc.: 64.06%] [G loss: 0.717078]\n",
      "epoch:29 step:27973 [D loss: 0.554942, acc.: 71.88%] [G loss: 0.670046]\n",
      "epoch:29 step:27974 [D loss: 0.482214, acc.: 75.00%] [G loss: 0.867974]\n",
      "epoch:29 step:27975 [D loss: 0.467694, acc.: 78.12%] [G loss: 0.910975]\n",
      "epoch:29 step:27976 [D loss: 0.499113, acc.: 77.34%] [G loss: 0.799561]\n",
      "epoch:29 step:27977 [D loss: 0.551376, acc.: 68.75%] [G loss: 0.592088]\n",
      "epoch:29 step:27978 [D loss: 0.552148, acc.: 74.22%] [G loss: 0.508366]\n",
      "epoch:29 step:27979 [D loss: 0.572334, acc.: 63.28%] [G loss: 0.575237]\n",
      "epoch:29 step:27980 [D loss: 0.529684, acc.: 73.44%] [G loss: 0.748516]\n",
      "epoch:29 step:27981 [D loss: 0.585881, acc.: 67.19%] [G loss: 0.542918]\n",
      "epoch:29 step:27982 [D loss: 0.542894, acc.: 71.09%] [G loss: 0.701604]\n",
      "epoch:29 step:27983 [D loss: 0.492098, acc.: 73.44%] [G loss: 0.888010]\n",
      "epoch:29 step:27984 [D loss: 0.529520, acc.: 73.44%] [G loss: 0.895250]\n",
      "epoch:29 step:27985 [D loss: 0.569001, acc.: 70.31%] [G loss: 0.640842]\n",
      "epoch:29 step:27986 [D loss: 0.569881, acc.: 68.75%] [G loss: 0.622887]\n",
      "epoch:29 step:27987 [D loss: 0.482264, acc.: 76.56%] [G loss: 0.823209]\n",
      "epoch:29 step:27988 [D loss: 0.500410, acc.: 76.56%] [G loss: 0.734431]\n",
      "epoch:29 step:27989 [D loss: 0.503772, acc.: 73.44%] [G loss: 0.934950]\n",
      "epoch:29 step:27990 [D loss: 0.596552, acc.: 71.09%] [G loss: 0.777324]\n",
      "epoch:29 step:27991 [D loss: 0.523171, acc.: 68.75%] [G loss: 0.797278]\n",
      "epoch:29 step:27992 [D loss: 0.497718, acc.: 72.66%] [G loss: 0.661150]\n",
      "epoch:29 step:27993 [D loss: 0.620360, acc.: 67.19%] [G loss: 0.608548]\n",
      "epoch:29 step:27994 [D loss: 0.520686, acc.: 74.22%] [G loss: 0.616896]\n",
      "epoch:29 step:27995 [D loss: 0.573564, acc.: 65.62%] [G loss: 0.582646]\n",
      "epoch:29 step:27996 [D loss: 0.425380, acc.: 82.03%] [G loss: 0.729519]\n",
      "epoch:29 step:27997 [D loss: 0.553019, acc.: 71.88%] [G loss: 0.792747]\n",
      "epoch:29 step:27998 [D loss: 0.521294, acc.: 70.31%] [G loss: 0.575600]\n",
      "epoch:29 step:27999 [D loss: 0.528175, acc.: 73.44%] [G loss: 0.655288]\n",
      "epoch:29 step:28000 [D loss: 0.575173, acc.: 66.41%] [G loss: 0.757574]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.719343\n",
      "FID: 46.877968\n",
      "0 = 12.822484905147556\n",
      "1 = 0.08970631578169672\n",
      "2 = 0.8413000106811523\n",
      "3 = 0.79339998960495\n",
      "4 = 0.88919997215271\n",
      "5 = 0.8774607181549072\n",
      "6 = 0.79339998960495\n",
      "7 = 8.168755415272715\n",
      "8 = 0.1450117218278173\n",
      "9 = 0.6912000179290771\n",
      "10 = 0.6814000010490417\n",
      "11 = 0.7009999752044678\n",
      "12 = 0.695022463798523\n",
      "13 = 0.6814000010490417\n",
      "14 = 6.719366550445557\n",
      "15 = 6.814765930175781\n",
      "16 = 0.39625173807144165\n",
      "17 = 6.7193427085876465\n",
      "18 = 46.877967834472656\n",
      "epoch:29 step:28001 [D loss: 0.605764, acc.: 63.28%] [G loss: 0.593646]\n",
      "epoch:29 step:28002 [D loss: 0.525218, acc.: 71.88%] [G loss: 0.628826]\n",
      "epoch:29 step:28003 [D loss: 0.522878, acc.: 74.22%] [G loss: 0.889521]\n",
      "epoch:29 step:28004 [D loss: 0.568992, acc.: 68.75%] [G loss: 0.659393]\n",
      "epoch:29 step:28005 [D loss: 0.519388, acc.: 74.22%] [G loss: 0.611722]\n",
      "epoch:29 step:28006 [D loss: 0.485268, acc.: 72.66%] [G loss: 0.767584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:28007 [D loss: 0.524847, acc.: 68.75%] [G loss: 0.631726]\n",
      "epoch:29 step:28008 [D loss: 0.522789, acc.: 75.78%] [G loss: 0.662400]\n",
      "epoch:29 step:28009 [D loss: 0.520273, acc.: 72.66%] [G loss: 0.674185]\n",
      "epoch:29 step:28010 [D loss: 0.531150, acc.: 73.44%] [G loss: 0.585066]\n",
      "epoch:29 step:28011 [D loss: 0.501744, acc.: 75.78%] [G loss: 0.638376]\n",
      "epoch:29 step:28012 [D loss: 0.625028, acc.: 67.97%] [G loss: 0.678459]\n",
      "epoch:29 step:28013 [D loss: 0.575207, acc.: 71.09%] [G loss: 0.630153]\n",
      "epoch:29 step:28014 [D loss: 0.523399, acc.: 70.31%] [G loss: 0.665760]\n",
      "epoch:29 step:28015 [D loss: 0.543336, acc.: 67.97%] [G loss: 0.536012]\n",
      "epoch:29 step:28016 [D loss: 0.509592, acc.: 69.53%] [G loss: 0.632883]\n",
      "epoch:29 step:28017 [D loss: 0.554910, acc.: 68.75%] [G loss: 0.692469]\n",
      "epoch:29 step:28018 [D loss: 0.567882, acc.: 68.75%] [G loss: 0.840761]\n",
      "epoch:29 step:28019 [D loss: 0.598691, acc.: 63.28%] [G loss: 0.551630]\n",
      "epoch:29 step:28020 [D loss: 0.599285, acc.: 62.50%] [G loss: 0.487920]\n",
      "epoch:29 step:28021 [D loss: 0.557716, acc.: 70.31%] [G loss: 0.508072]\n",
      "epoch:29 step:28022 [D loss: 0.567815, acc.: 63.28%] [G loss: 0.420856]\n",
      "epoch:29 step:28023 [D loss: 0.568480, acc.: 67.97%] [G loss: 0.611840]\n",
      "epoch:29 step:28024 [D loss: 0.595908, acc.: 68.75%] [G loss: 0.532497]\n",
      "epoch:29 step:28025 [D loss: 0.532926, acc.: 72.66%] [G loss: 0.624563]\n",
      "epoch:29 step:28026 [D loss: 0.585893, acc.: 68.75%] [G loss: 0.665793]\n",
      "epoch:29 step:28027 [D loss: 0.572905, acc.: 64.06%] [G loss: 0.584836]\n",
      "epoch:29 step:28028 [D loss: 0.584941, acc.: 67.19%] [G loss: 0.547787]\n",
      "epoch:29 step:28029 [D loss: 0.590490, acc.: 66.41%] [G loss: 0.653929]\n",
      "epoch:29 step:28030 [D loss: 0.448454, acc.: 77.34%] [G loss: 0.657866]\n",
      "epoch:29 step:28031 [D loss: 0.588715, acc.: 67.19%] [G loss: 0.646819]\n",
      "epoch:29 step:28032 [D loss: 0.521546, acc.: 69.53%] [G loss: 0.784551]\n",
      "epoch:29 step:28033 [D loss: 0.463034, acc.: 78.12%] [G loss: 0.834676]\n",
      "epoch:29 step:28034 [D loss: 0.598231, acc.: 67.97%] [G loss: 0.793713]\n",
      "epoch:29 step:28035 [D loss: 0.553019, acc.: 71.88%] [G loss: 0.607415]\n",
      "epoch:29 step:28036 [D loss: 0.565003, acc.: 69.53%] [G loss: 0.489755]\n",
      "epoch:29 step:28037 [D loss: 0.549474, acc.: 74.22%] [G loss: 0.689940]\n",
      "epoch:29 step:28038 [D loss: 0.570529, acc.: 70.31%] [G loss: 0.666721]\n",
      "epoch:29 step:28039 [D loss: 0.560327, acc.: 69.53%] [G loss: 0.745027]\n",
      "epoch:29 step:28040 [D loss: 0.656863, acc.: 61.72%] [G loss: 0.605767]\n",
      "epoch:29 step:28041 [D loss: 0.587037, acc.: 67.97%] [G loss: 0.480704]\n",
      "epoch:29 step:28042 [D loss: 0.589240, acc.: 68.75%] [G loss: 0.520335]\n",
      "epoch:29 step:28043 [D loss: 0.490164, acc.: 70.31%] [G loss: 0.728599]\n",
      "epoch:29 step:28044 [D loss: 0.478828, acc.: 78.91%] [G loss: 0.721469]\n",
      "epoch:29 step:28045 [D loss: 0.542800, acc.: 67.97%] [G loss: 0.787018]\n",
      "epoch:29 step:28046 [D loss: 0.597469, acc.: 65.62%] [G loss: 0.694366]\n",
      "epoch:29 step:28047 [D loss: 0.509295, acc.: 73.44%] [G loss: 0.644575]\n",
      "epoch:29 step:28048 [D loss: 0.540192, acc.: 67.19%] [G loss: 0.607906]\n",
      "epoch:29 step:28049 [D loss: 0.551668, acc.: 70.31%] [G loss: 0.595489]\n",
      "epoch:29 step:28050 [D loss: 0.603812, acc.: 61.72%] [G loss: 0.459476]\n",
      "epoch:29 step:28051 [D loss: 0.562737, acc.: 71.88%] [G loss: 0.552303]\n",
      "epoch:29 step:28052 [D loss: 0.542410, acc.: 75.00%] [G loss: 0.581840]\n",
      "epoch:29 step:28053 [D loss: 0.737980, acc.: 55.47%] [G loss: 0.481704]\n",
      "epoch:29 step:28054 [D loss: 0.592385, acc.: 64.06%] [G loss: 0.514950]\n",
      "epoch:29 step:28055 [D loss: 0.561852, acc.: 67.97%] [G loss: 0.687004]\n",
      "epoch:29 step:28056 [D loss: 0.591907, acc.: 66.41%] [G loss: 0.528931]\n",
      "epoch:29 step:28057 [D loss: 0.499286, acc.: 74.22%] [G loss: 0.575082]\n",
      "epoch:29 step:28058 [D loss: 0.510838, acc.: 72.66%] [G loss: 0.844310]\n",
      "epoch:29 step:28059 [D loss: 0.524193, acc.: 73.44%] [G loss: 0.801780]\n",
      "epoch:29 step:28060 [D loss: 0.592670, acc.: 65.62%] [G loss: 0.952383]\n",
      "epoch:29 step:28061 [D loss: 0.609015, acc.: 63.28%] [G loss: 0.782649]\n",
      "epoch:29 step:28062 [D loss: 0.552887, acc.: 72.66%] [G loss: 0.672532]\n",
      "epoch:29 step:28063 [D loss: 0.483243, acc.: 77.34%] [G loss: 0.799117]\n",
      "epoch:29 step:28064 [D loss: 0.569346, acc.: 67.19%] [G loss: 0.768318]\n",
      "epoch:29 step:28065 [D loss: 0.611441, acc.: 67.97%] [G loss: 0.662259]\n",
      "epoch:29 step:28066 [D loss: 0.526935, acc.: 69.53%] [G loss: 0.631705]\n",
      "epoch:29 step:28067 [D loss: 0.481188, acc.: 77.34%] [G loss: 0.761611]\n",
      "epoch:29 step:28068 [D loss: 0.479565, acc.: 76.56%] [G loss: 0.818555]\n",
      "epoch:29 step:28069 [D loss: 0.454173, acc.: 83.59%] [G loss: 0.756266]\n",
      "epoch:29 step:28070 [D loss: 0.543702, acc.: 70.31%] [G loss: 0.878166]\n",
      "epoch:29 step:28071 [D loss: 0.430106, acc.: 81.25%] [G loss: 0.948551]\n",
      "epoch:29 step:28072 [D loss: 0.484489, acc.: 75.78%] [G loss: 0.957164]\n",
      "epoch:29 step:28073 [D loss: 0.519487, acc.: 71.88%] [G loss: 0.714071]\n",
      "epoch:29 step:28074 [D loss: 0.510393, acc.: 72.66%] [G loss: 0.907701]\n",
      "epoch:29 step:28075 [D loss: 0.561084, acc.: 71.09%] [G loss: 0.634189]\n",
      "epoch:29 step:28076 [D loss: 0.506439, acc.: 77.34%] [G loss: 0.738252]\n",
      "epoch:29 step:28077 [D loss: 0.592280, acc.: 67.19%] [G loss: 0.840283]\n",
      "epoch:29 step:28078 [D loss: 0.535587, acc.: 71.88%] [G loss: 0.857492]\n",
      "epoch:29 step:28079 [D loss: 0.545723, acc.: 75.00%] [G loss: 0.684225]\n",
      "epoch:29 step:28080 [D loss: 0.532862, acc.: 71.88%] [G loss: 0.852114]\n",
      "epoch:29 step:28081 [D loss: 0.554928, acc.: 75.00%] [G loss: 0.805551]\n",
      "epoch:29 step:28082 [D loss: 0.545565, acc.: 74.22%] [G loss: 0.666032]\n",
      "epoch:29 step:28083 [D loss: 0.510726, acc.: 76.56%] [G loss: 0.665464]\n",
      "epoch:29 step:28084 [D loss: 0.486139, acc.: 74.22%] [G loss: 0.718593]\n",
      "epoch:29 step:28085 [D loss: 0.453695, acc.: 78.12%] [G loss: 0.737727]\n",
      "epoch:29 step:28086 [D loss: 0.538118, acc.: 74.22%] [G loss: 0.791695]\n",
      "epoch:29 step:28087 [D loss: 0.458388, acc.: 79.69%] [G loss: 0.947078]\n",
      "epoch:29 step:28088 [D loss: 0.588871, acc.: 68.75%] [G loss: 0.695628]\n",
      "epoch:29 step:28089 [D loss: 0.524877, acc.: 77.34%] [G loss: 0.781039]\n",
      "epoch:29 step:28090 [D loss: 0.619976, acc.: 60.16%] [G loss: 0.776366]\n",
      "epoch:29 step:28091 [D loss: 0.494897, acc.: 75.78%] [G loss: 0.895001]\n",
      "epoch:29 step:28092 [D loss: 0.435627, acc.: 84.38%] [G loss: 0.843070]\n",
      "epoch:29 step:28093 [D loss: 0.702701, acc.: 61.72%] [G loss: 0.787130]\n",
      "epoch:29 step:28094 [D loss: 0.559282, acc.: 71.09%] [G loss: 0.772214]\n",
      "epoch:29 step:28095 [D loss: 0.557282, acc.: 73.44%] [G loss: 0.720459]\n",
      "epoch:29 step:28096 [D loss: 0.456490, acc.: 77.34%] [G loss: 0.937818]\n",
      "epoch:29 step:28097 [D loss: 0.482435, acc.: 73.44%] [G loss: 0.905227]\n",
      "epoch:29 step:28098 [D loss: 0.406504, acc.: 83.59%] [G loss: 0.971348]\n",
      "epoch:29 step:28099 [D loss: 0.384239, acc.: 84.38%] [G loss: 0.966812]\n",
      "epoch:29 step:28100 [D loss: 0.462967, acc.: 73.44%] [G loss: 1.237033]\n",
      "epoch:29 step:28101 [D loss: 0.757888, acc.: 60.94%] [G loss: 1.270404]\n",
      "epoch:29 step:28102 [D loss: 0.502401, acc.: 74.22%] [G loss: 1.248245]\n",
      "epoch:29 step:28103 [D loss: 0.462452, acc.: 75.00%] [G loss: 1.526735]\n",
      "epoch:29 step:28104 [D loss: 0.519938, acc.: 73.44%] [G loss: 1.199647]\n",
      "epoch:29 step:28105 [D loss: 0.589324, acc.: 66.41%] [G loss: 0.870934]\n",
      "epoch:29 step:28106 [D loss: 0.482372, acc.: 78.12%] [G loss: 0.928977]\n",
      "epoch:29 step:28107 [D loss: 0.569777, acc.: 65.62%] [G loss: 1.158849]\n",
      "epoch:29 step:28108 [D loss: 0.481782, acc.: 71.09%] [G loss: 1.141657]\n",
      "epoch:29 step:28109 [D loss: 0.389930, acc.: 79.69%] [G loss: 1.440560]\n",
      "epoch:29 step:28110 [D loss: 0.410714, acc.: 81.25%] [G loss: 1.253414]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8XNWZ8PHfM11t1CXLKpaMDe4Nm2aDjRPAEAgleTeUhJANgSyw2c2mLCTZQEiyZN9N3t1llyRLEkLIQoClhzihm17cO25yk2Rbvbcp5/3j3hmNmiVbskbl+X4+/nh07p3RuZb8zJnnnPscMcaglFJqYnDEuwNKKaVGjgZ9pZSaQDToK6XUBKJBXymlJhAN+kopNYFo0FdKqQlEg75SSk0gGvSVUmoC0aCvlFITiCveHegpKyvLFBcXx7sbSik1pqxfv77aGJM90HmjLugXFxezbt26eHdDKaXGFBE5OJjzNL2jlFITiAZ9pZSaQDToK6XUBKJBXymlJhAN+kopNYEMGPRF5CERqRSRbf0cFxG5X0T2isgWEVkUc+yLIrLH/vPF4ey4UkqpEzeYkf7DwKrjHL8UmG7/uQX4BYCIZAB3A2cDZwF3i0j6UDqrlFJqaAZcp2+MeUtEio9zypXAI8bad/EDEUkTkTxgBfCKMaYWQERewXrz+MNQO92X1s4gv1yz71S89JClJ3kozkoiO9mLCPh9bgozEuPdLaXUBDQcN2flA4djvi6z2/pr70VEbsH6lEBRUdFJdaKtM8R/vrH3pJ57KvW3BfHUrCQump3LbSumkZrgHtlOKaUmrFFxR64x5kHgQYDFixef1E7tmcle9t/3qWHt13AwxlDd3Mn+6hbqWjsxBo42tPHax5X8+u39vLTtKL/8wpnMmOSPd1eVUhPAcAT9cqAw5usCu60cK8UT275mGL7fmCIiZKd4yU7xdmu/aWkJaw/UcvujG7jqgXf5yTXzuGphnx+EAGgPhPC6HIjIqe6yUmocG44lmy8AN9qreM4BGowxR4CXgItFJN2ewL3YblO2JcUZvPi1ZcwrSOPvn9jE95/fRmcw3Ou8339wkNl3v8Tsu1/iU/e/zXt7q+PQW6XUeDCYJZt/AN4HzhCRMhH5soh8VUS+ap+yGigF9gK/Am4DsCdwfwistf/cG5nUVV1yUnw8evPZ3LyshEfeP8iSH7/K3z2+kdc/PoYxhrUHavnBC9tZUpzOtUuKaO0M8aWH1/Lm7qp4d10pNQaJ6W+mMU4WL15sJmqVzXf2VPPsxnLe2FVJbUsn8wvTqKhvI9nr4vk7luL3ualt6eTzv/6QvZXN/OqLi1l++oCVVJVSE4CIrDfGLB7wPA36o08wFOaZDeX8x2t7qG/t5JnblnLGpJTo8frWTq598APK69t44Y5llGQlxbG3SqnRQIP+ONAZDNPSESQ9ydPr2OHaVq74r3fITvbyzG3n0R4I43E5dPmnUhOUBv0J4J091dz40IcYrPsBJqf6ePsfV+J06AofpSaawQb9UbFOX52cZdOz+NWNi1l7oI6m9gCPfniIDYfqWFKcEe+uKaVGKa2yOcZ9YmYud146gzsvnYHbKbyy41ivc7aU1dPSEYxD75RSo40G/XEixefmvNOyeGn7UWJTdrUtnVzz8/f44Ys74tg7pdRooUF/HLl4di4Ha1rZU9kcbXt3bzXBsOGZDeVUNrbHsXdKqdFAg/448smZuQC8vP1otO2dPdUkepwEw2F+8+7+eHVNKTVKaNAfR3L9PhYUpvGyndc3xvD2niqWn57NZXPzeOyDQzS2B+LcS6VUPGnQH2cunp3LlrIGdh9rorS6hYqGdpZNz+Kry0+jqSPIYx8eincXlVJxpEF/nLl2SRF+n4sfvriDt+36PBdMz2ZOfipLp2XyyHsHCIZ6F3VTSk0MGvTHmYwkD3/3ydN5e081//1WKVMyE6O7dH3x3GIqGtp5dWfvZZ1KqYlBg/449IVzpjA1K4kjDe0sm5YVbf/EzFzy0xL43XsH49g7pVQ8adAfhzwuB9+7fCYAn5iZE213OoTPnzOF90tr2HW0KV7dU0rFkQb9cWrljFzevXMlF56R0639c0sK8bgc/GLN3j43bFFKjW8a9Mex/LSEXtsrZiR5uG5JIc9tqmDZv7zOr94qZbQV3VNKnToa9Cegu6+YzW9vWsK0nGR+vHonL23XiV2lJgoN+hOQwyFcOCOHR/76LM7ITeHHq3fQHgjFu1tKqRGgQX8Cczkd3H3FLA7XtvHrt0vj3R2l1AjQoD/BnTcti1WzJ/HAG/s4pgXZlBr3BhX0RWSViOwSkb0icmcfx6eIyGsiskVE1ohIQcyx/ysi20Vkp4jcLz1nFlXcfeeymXSGwvz3mzraV2q8GzDoi4gTeAC4FJgFXCcis3qc9lPgEWPMPOBe4D77uecBS4F5wBxgCbB82HqvhkVRZiJXLpjMYx8dpKa5I97dUUqdQoMZ6Z8F7DXGlBpjOoHHgSt7nDMLeN1+/EbMcQP4AA/gBdyALhUZhW5bcRodwTC/ffdAvLuilDqFBhP084HDMV+X2W2xNgPX2I+vBlJEJNMY8z7Wm8AR+89LxpidQ+uyOhWm5aSwavYkfvf+AS2/rNQ4NlwTud8ElovIRqz0TTkQEpFpwEygAOuNYqWInN/zySJyi4isE5F1VVVVw9QldaJuv3AaTe1Bfv1235utdAbDbD5czys7jukNXUqNUa5BnFMOFMZ8XWC3RRljKrBH+iKSDHzGGFMvIl8BPjDGNNvH/gycC7zd4/kPAg8CLF68WKNJnMzJT+VTc/N48K19XHdWIXmpCQAEQmF+9vJufvvufjrs0g0v3LGUeQVp8eyuUuokDGakvxaYLiIlIuIBrgVeiD1BRLJEJPJadwEP2Y8PYX0CcImIG+tTgKZ3RrE7L51B2MC//mUXAAdrWvir/36fX765j0vnTOK7l1mF3A7UtMazm0qpkzTgSN8YExSRO4CXACfwkDFmu4jcC6wzxrwArADuExEDvAXcbj/9KWAlsBVrUvcvxpg/Dv9lqOFSmJHIzctK+PmafdS3BVizq5Ikj4sHrl/Ep+bl0dIR5Merd3K4VoO+UmPRYNI7GGNWA6t7tH0/5vFTWAG+5/NCwK1D7KMaYbddOI2nN5Sx9kAtX7lgKl86r4RJqT4AkrwuMpM8lNW1xbmXSqmTMaigryaWZK+Ll/9+OW6XkOjp/StSkJ5AWZ2O9JUai7QMg+pTaqK7z4APUJCRqCN9pcYoDfrqhBWkJ1Be10Y4rAutlBprNOirE1aYnkhnKExlk1Wy4fpffcDv3jsQ304ppQZFg746YYUZiQAcrmvlSEMb7+2r4ZkNZXHulVJqMDToqxNWkG7dtFVW18qGg/UAbClvoKG1e/mG9kCI1z/WUktKjSYa9NUJy0+zgv7h2jY2HKoDwBh4v7S623m/f/8gf/3wOjba5yil4k+DvjphPreTnBQvZXWtrD9Yx4LCNJI8Tt7Z2z3o/3FLBQBv7NJ6SkqNFhr01UkpzEhkX1UL2ysaOLskg3OmZvLu3pro8YM1LWwpawDgzV2V8eqmUqoHDfrqpBSkJ7DhUB2BkGFhUTpLp2Wxv7oletPWi1uOAHDtkkK2lDfo5ixKjRIa9NVJKUxPJFJdedGUNJZNzwLgPXu0/+KWIywsSuP6s4swBt7aoykepUYDDfrqpERW8BRmJJCT4mN6TjLZKV4e/fAgz2woY+eRRi6fN5k5k1PJSvawRvP6So0KGvTVSYms1V9UlA6AiPC1ldPYU9nMPzy5GRH41Nw8HA7hgunZvLW7ipDewatU3GnBNXVSirOSAFhSnBFt+8K5xXzmzAJe2XGMUNhEK3MuPyObZzaWs7msPvomoZSKDx3pq5OSn5bAc7cv5XNLCru1J3pcXLkgn2sWFUTbzp+eDcB7PZZ0xtp5pJG/e3wjgVD41HRYKQVo0FdDsKAwDbdz4F+hjCQPMyal8H5pTb/nPLepnOc3VXCgumU4u6iU6kGDvhoR556WyboDdXQEQ30e31Zurekvqx+eks17K5vZXtEwLK+l1HiiQV+NiHOnZtIRDLPpUH2vY8YYtpU3Agxbnf77Vu/k209tGZbXUmo80aCvRsTZJZmI0GeK53BtGw1tVrG28mEK+tUtnRxpaB+W11JqPNGgr0ZEaqKb2ZP9vLevd9DfUm6N/p0OGbZtGBvbAtS2dNIZ1IlhpWJp0Fcj5rzTsth0qJ72QPe8/tbyBtxOYVFRGuXDlNOPfHKo1vIPSnUzqKAvIqtEZJeI7BWRO/s4PkVEXhORLSKyRkQKYo4VicjLIrJTRHaISPHwdV+NJedOzaQzFGb9we6llreVN3DGpBRKspKGJadvjIkG/WONmuJRKtaAQV9EnMADwKXALOA6EZnV47SfAo8YY+YB9wL3xRx7BPhXY8xM4CxASy5OUEtKMnA6hA9i8vqRSdy5+akUpCdS1dTR65PAiWrpDEXv/o1s6aiUsgxmpH8WsNcYU2qM6QQeB67scc4s4HX78RuR4/abg8sY8wqAMabZGDM8SVs15iR7Xcye7GftgdpoW2QSd05+anRzloohpnjqWzujjzXoK9XdYIJ+PnA45usyuy3WZuAa+/HVQIqIZAKnA/Ui8oyIbBSRf7U/OagJavGUDDYdro9OsG611+fPy0+L2YZxaEE/ktoBqNT0jlLdDNdE7jeB5SKyEVgOlAMhrNo+59vHlwBTgZt6PllEbhGRdSKyrqpKqzGOZ0uK02kPhKM3Tm0pq8ftFE6flEy+HfSHOpnbPejrSF+pWIMJ+uVAbIGVArstyhhTYYy5xhizEPiu3VaP9algk50aCgLPAYt6fgNjzIPGmMXGmMXZ2dkneSlqLDiz2Cq4tu5AHcYY/rL9KIunZOB1OZnk9w3Lss1GO+h7nA4qm3Skr1SswQT9tcB0ESkREQ9wLfBC7AkikiUikde6C3go5rlpIhKJ5CuBHUPvthqrclJ8FGcmsvZALRsO1XOwppWrF1nZQpfTwSS/b8g3aEVG+lOzkzSnr1QPAwZ9e4R+B/ASsBN40hizXUTuFZFP26etAHaJyG4gF/ix/dwQVmrnNRHZCgjwq2G/CjWmLC7OYN3BOp7dWIbX5eDSOZOixwrSE4Ytpz89N4Vjmt5RqptB1dM3xqwGVvdo+37M46eAp/p57ivAvCH0UY0zS4rTeWp9GU+uK+OS2ZNI8bmjx/LTE3h/Xw3GGF7YXEGy18WSkgz8MecMpKEtgNMhlGQm8uKWCoKhMK5BVANVaiLQTVTUiFtsb7zSGQxz9cLJ3Y4VpCdytLGcf3t1D/e/tgcAh8BXLpjKXZfOHNTrN7QFSE1wk+P3YQzUtHSS6/cN70UoNUbp8EeNuKlZSWQkechM8kQ3WIkoSEvAGLj/tT1cvTCfx75yNpfNzeO/3yzllR3HBvX6DW1BK+ineAFdwaNULB3pqxEnInzrkjNIcDt7bcJSkGEt2zx/ehb/8pl5eFwOzpySTmlVC//49BbmF55PToo1au8Mhnl2YxmfWVTQLX3T0BbAb4/0wSrFMJfUEbo6pUY3HemruLjurCKuWtjzHj84qziDn1wzl198/kw8LuvX0+tycv91C2jpCPK9Z7dFz/3T1gr+8emt/GX70W6vEU3vREb6uoJHqSgN+mpUcTkdXHtWEcne7h9Cp+WkcNPSYl77uJKmdmt1zgf7rHIOr+/sXs6p0Q762dGgr2v1lYrQoK/GjOWnZxMKGz4otYL9B/utwm1v7KqMFliDyEjfhdvpIDPJoyN9pWJo0FdjxplT0klwO3lnTxUV9W0crGllUVEada0BNh6yyjVHyiqnJlhLPLNTvFp/R6kYGvTVmOF1OTl7agZv76nmQ3uU/61LZuByCK99bKV4ImWVI0E/1+8b8ZF+OGwwxgx8olJxoEFfjSnLpmVRWt3CMxvKSU1wc3ZJBkuKM6J5/UhZ5UjQz0nxjuiSzY5giCU/fpU/bjkyYt9TqROhQV+NKZF1/W/vqeaskgwcDuETM3PYdayJw7Wt0RIMsSP9quYOgqGR2Su3sS1ITUsn2+2S0UqNNhr01Zhyem5ydCnmOVMzAVg5IweANburokHfbwf9KZmJhMKGw8OwDeNgtHQEAV0mqkYvDfpqTBERlk3PAuCcqVY5h5KsJHJSvKw7UBstqxwZ6U/NTgZgf3XziPSvpTMS9HXyWI1OGvTVmHPjucVcd1YhMyf5AeuNYHFxOusP1vVK70zNSgKgtKplRPrW2mnt76ulH9RopWUY1JizoDCNBYVp3doWFaWzeutRdh+zRvSRoJ+e5CEt0c3+6pEJ+preUaOdjvTVuBCp3PnGx5U4HdLtjt6SrKQ+g/6ftx6htGp40z6RkX5DW4D2QGhYX1up4aBBX40Ls/L8eF0OSqtb8PtciEj0WF9Bf29lM7c9toEf/HF4N3KLjPQBqnS0r0YhDfpqXPC4HMy3Uz6R1E7E1KwkjjS009rZFZAffGsfxsBb9t29YN3NGxji0s7YoK+TuWo00qCvxo0zp1ibrvcM+iVZ1gqeA9XWhutHGtp4dmM5n5yZgzHw1PoyAL773DYu+fe3htSHls6ulI5O5qrRSCdy1bix2A76/l5B31rBs7+6hVmT/fz67f2EDdx9xWxaO0M8ue4ws/L8PPbhIcC6q9brcp5UH2I/TehkrhqNdKSvxo1FRVbQT0v0dGsvzkoErLX6dS2d/OGjQ3x6/mQKMxL5q8WFlNW18bd/2IjDngYYygi9pSNEsteF0yGa3lGjkgZ9NW6kJ3m4fF5e9KatiESPi7xUH6VVLTz4diltgRB/s+I0AFbNmUSKz0UgFOYbF58BDG2E3toZJMXnIivZo+kdNSoNKr0jIquA/wCcwK+NMT/pcXwK8BCQDdQCnzfGlMUc9wM7gOeMMXcMU9+V6uW/rl/UZ/vU7CQ2Ha7n6PZ2Lp83mdNzUwDwuZ386Ko5hI3hjFw///rSriGVYm7pDJHocZLocWl6R41KAwZ9EXECDwAXAWXAWhF5wRgTu9btp8AjxpjfichK4D7gCzHHfwgMbYZMqSEoyUri3b01OAT+7hPTux27coG1bWN1sxWkjw0l6HcESfK6yE72UtGg6R01+gwmvXMWsNcYU2qM6QQeB67scc4s4HX78Ruxx0XkTCAXeHno3VXq5ERW8Fy5IJ9pOcl9npOR6MHlkKGldzqskX6O30uV5vTVKDSYoJ8PHI75usxui7UZuMZ+fDWQIiKZIuIAfgZ8c6gdVWoozi7JoCgjsdcoP5bDIWSneDk2lIncziBJHhfZKT5qWjpHrKSzUoM1XBO53wSWi8hGYDlQDoSA24DVsfn9vojILSKyTkTWVVVVDVOXlOoyJz+Vt759IcX28s3+5Ph9Q1p109oZIsnrIifFizFQ3dx50q+l1KkwmInccqAw5usCuy3KGFOBPdIXkWTgM8aYehE5FzhfRG4DkgGPiDQbY+7s8fwHgQcBFi9erPvMqbjJSfFyqKb1pJ9v5fSd0Zr/lU3tTEr1DVf3lBqywQT9tcB0ESnBCvbXAtfHniAiWUCtMSYM3IW1kgdjzA0x59wELO4Z8JUaTXL9XtYeqO33eEtHEKdD8Ln7vnmrpSNIosdFjt8K9LpsU402A6Z3jDFB4A7gJWAn8KQxZruI3Csin7ZPWwHsEpHdWJO2Pz5F/VXqlMpN8VHfGqAj2HeFzJt/t47vPLu1z2PhsKE1ECLJ4yTXHxnpa9BXo8ug1ukbY1YDq3u0fT/m8VPAUwO8xsPAwyfcQ6VGUE4kWDd2UJiR2O2YMYZt5Q00tgf6fG57MIQxkOh1kZXsRUSLrqnRR+/IVSpGNC3Txwi9tqWTpo4gR/tZf9/SYX06SPI4cTsdZCR6hrTmX6lTQYO+UjGiE7B9BOuDtdYEb01LZ58bpESKrSXZG7jMLUjl1Z2V/aaKlIoHDfpKxcg9zkj/YE3XRix9jeAjI/1EjxX0b142laqmDp7bWN7rXKXiRYO+UjEid+Uea2ynqT3APz61JZrOORizlPNIHymeluhI31rZs3RaJrPy/Pzq7f2Ew7oSWY0OGvSVihF7V+5T68t4Yt1hVm89AlhB32nXXz7S0NbruZFdsyIjfRHhlgumsreymTd2VY7QFSh1fBr0leohJ8VLZVM7j39kVR/ZXFYPWOmdOfmpQNdIf19VMz98cYe1XNPeNSsy0gf41Lw8Jqf6+O27B0bwCpTqnwZ9pXrI8ftYd6COXceaSHA72Xw4EvRbmTkphdQEdzTl8/T6Mn7zzn7K6tqiI/0kT9dKaLfTwUWzctl0uB5jNMWj4k+DvlI95Pq9tAWsapl/vayYAzWtHK5tpaalkymZSeSl+qiot4L+x0ebACivb4sZ6Xe//WVabgrNHUGO2pO/obDhjY8r9U1AxYUGfaV6yEmxVvB8ev5klp6WBcAft1QAMCUzkUmpPo42Wjn9XXbQr6hvozma0+9eomFatlXKeW9lMwAvbz/Klx5ey5rdWlxQjTwN+kr1UGTfiXvdWUXMKUhFBF7Y1BX081ITOFLfTkNbgPJ6K/hX1LfR2mnV5fG6uv+3itTv33PMCvqbyxoAeHXHsRG5HqViDaoMg1ITyafm5TE9N5nZk61J29Oyk6NpnEh6p6alk6128AaoaGjD63KS6HEiIt1eLyvZQ2qCm71VVtDfVm4977WdlfzoKtPrfKVOJR3pK9WD2+mIBnyA+QVpgBW8k73WJusAb+62lmHmpHgpr2+n1d5ApScRYVpOMnsrm636PRUNpCW6OdrYzvaKxhG4IqW6aNBXagALiqygH0n75KUmAPD6x5WkJrhZVJRORX2btSm6t++Sy9Oyk9lX2UxZXRv1rQH+emkJIvDqTk3xqJGlQV+pASywR/pTMq1dt/LSrJH+vqoWZkxKIT89wQr6HUGSvX1nTKfnJlPT0snbe6oBWH56NouK0jXoqxGnQV+pAZwxKYXsFC/zC6yUT17MTlgzJqUwOS2B1s4QFfVtvVbuRJxmT+Y+t6kcl0M4Y1IKn5iZw7byxn6rdip1KmjQV2oAHpeDt799ITeeWwxYZRZSE9wAzMjzk2+P/PdXt/SZ04euZZsf7a9lem4KPreTlTNyAHhnb/UpvgKluujqHaUGoef2iHmpPhraAsyYlBKtxxMIGRL7Se/kpyWQ4HbSFggxZ7If6JojqGnW3bXUyNGRvlInYVKqDxE4PddK70Qk9ZPecTiEqdnWnMBcO02U4Hbicki3nbjaOkPRcg5KnQoa9JU6CYunpHN2SQZJXheZSR489g1ZPUswxIrcpBVZDioi+BPcNLR1Bf1vP72Fm3+3rtdzmzuCXPfgB+w51jScl6EmIA36Sp2EO1ZO5/FbzgWs4J1vj/b7G+kDLChMI8XnYlaeP9qWmuCmsa1rZH+opoW1B2p77cy180gj75fW8OH+2uG8DDUBadBXahhMtidz+8vpA3zhnCm89a0LSYh5Y/D7XN3SO/VtAYJhw9byhm7PrbDLPdQ0dw5nt9UENKigLyKrRGSXiOwVkTv7OD5FRF4TkS0iskZECuz2BSLyvohst499brgvQKnRYHLqwCN9l9NBepKnW5s/wU1jTHqnvtV6vP5gXbfzyursoN+ik75qaAYM+iLiBB4ALgVmAdeJyKwep/0UeMQYMw+4F7jPbm8FbjTGzAZWAf8uImnD1XmlRovIZG5iP0s2++P3deX0w2ETHfVv6BH0y3Wkr4bJYEb6ZwF7jTGlxphO4HHgyh7nzAJetx+/ETlujNltjNljP64AKoHs4ei4UqNJNKffTxmG/vgT3DS2Wzn9pvYgxoBDYMOhum719iPpnSpd3qmGaDBBPx84HPN1md0WazNwjf34aiBFRDJjTxCRswAPsO/kuqrU6FWQYQV9v33T1mD5E1zR9E59mzWKX1iUTnVzJ4dquzZiL4+kdzToqyEaroncbwLLRWQjsBwoB6LLD0QkD/g98CVjTLjnk0XkFhFZJyLrqqp0Ywk19pxTksnPb1jE2SWZA58cw+9z0xEM0x4IRfP5kTt1NxyyUjzGmK6J3BZN76ihGUzQLwcKY74usNuijDEVxphrjDELge/abfUAIuIH/gR81xjzQV/fwBjzoDFmsTFmcXa2Zn/U2ONwCJfNzYvenTtYkXIOje0B6u0R/+Ip6aR4XdHJ3Ia2AC2dIVIT3NS3BgiEeo2blBq0wQT9tcB0ESkREQ9wLfBC7AkikiUikde6C3jIbvcAz2JN8j41fN1WanyIpIMa24LUt1qj+MxkDwuK0lh/0NqQPbJyZ559J2+djvbVEAwY9I0xQeAO4CVgJ/CkMWa7iNwrIp+2T1sB7BKR3UAu8GO7/a+AC4CbRGST/WfBcF+EUmOV32et9mloC0Rz+6kJHs6cks6uo400tgeiqZ25+VbQr9YVPGoIBrW+zBizGljdo+37MY+fAnqN5I0x/wP8zxD7qNS45Y9N77RGgr6bs0oyCBtYd6A2ulxznl3Xv/okJnO3lTeQ5HVRkpU0TD1XY5XekatUHEVz+m1WTj/R48TjcrCoKB2P08GHpbWU17XhczuYnmvV7oncoLW1rCG63+5A/ubR9Xz/+W2n5iLUmKKllZWKI78vJui3Bkiz3wR8bifzC1P5oLSG/PQEJqclkJXsBbpu0Pr201sor2vlT187n0K7THNf6ls7OVzbZt8HoBuxT3Q60lcqjlLsnH5je5CGtk5SE7vKNJwzNZNtFY3sOtpEfloCfp8Lj9NBdXMngVCYfZXNNLYHueOxDXQG+1/Rs8PefL2+NRBNFamJS4O+UnHkczvxuhw0tgVoaOsa6QOcXZJJKGzYV9VCQXoCIkJmsoea5g4O1rTQGQpz6ZxJbC5r4F/+8nG/32O7HfQBtpU39nuemhg06CsVZ6l2Tf361gBpiV1Bf9GUNFz2uv9IQbfMZA81LZ3sOtoMwO0XTuPz5xTx0Lv72VvZd6397RUNZCV7cDpk0HMAg/W/6w4P+2uqU0uDvlJxZtXfsSZyY4N+oscVXZufn24H/SQv1c0d7D7WhIi1Mcs/XHQGiW4n//bqnj5ff3tFI/ML0piek8y2iu4Buqa5g7/9w8bjrv0/WNPC4x9J+saJAAAgAElEQVQd6tVujOF7z23j/tf6/r5qdNKgr1Sc+X0uGtoCNLQGetXuOWeqVdYhUsXTSu90svtYE8WZSfjcTjKSPHxpaQl/2nIkmr+PaOsMsa+qmdmT/czJT2VbeUO3Qm6v7DjGHzdXHHdzlt+9d5A7n9naaxvH+tYAHcEwH+6vJRw2/TxbjTYa9JWKs9QEN8caO+gMhUlL6F5v/4r5k1lYlMZMe7et7GRrpL/rWBOn20s4Ab5y/lRSfC7+7dXd3Z6/82gjYQOzJqcyZ7Kf6uZOKpu61vlH6vscb4J3X5WVSjrS0N6t/ViT9XVDW4CPj+o2jmOFBn2l4syf4OawXVEzNr0DMDPPz7O3LY2u589M9tARDLO/uoXTc1Oi56Umurl52VRe2XGMvZXN0fbIJO7syf7ohuxby7pSPBsORUo9dFX0vP+1Pfxl25Ho15Ggf7Rn0G/sevP4oLTmRC9bxYkGfaXiLFJpE+i2eqcvmUnWWn1j6Bb0Aa47qxCHwPObuuoh7qhoIDXBTUF6AjPz/IgQzevXt3ZG3yAipZvDYcMv1uzjd+8dBKz0UORTwJGG7p8GjtlvAokepwb9MUSDvlJx5k/oukcyNXGAoJ/clf45Y1L3oJ/j97F0WhbPbiyP5u23VzQyK8+PiJDocXFadnJ02ebGw9YoP8XnihZ1O9bUTlsgxLYKK/dfWt1MZAqg90jf+vriWbma1x9DNOgrFWepMaP71AFG+pG7ct1OoTizdx2dqxbkU1bXxvqDdRxpaOPjo03MnuyPHl9QmMaH+2toag+w8WAdDoGLZuVGR/P7q1oAaxevQ7Wt7LO/BqjoEfSPNraTnujmgtOzNa8/hmjQVyrOIqUYANISPcc5syvol2Ql4XH1/u97yZxJ+NwOnlpfxh2PbcTtEK4/uyh6/KbzimlqD/Lbdw+w/lAdMyb5OT03hYa2AE3tAUqru4L81vIG9lU24xCYnpPM0Z7pncYOcv0+zrZXGGmKZ2zQoK9UnMUu0xwop5+RZL0p9MznRyR7XVw8axKPrz3M+oN1/OQz85ia3bXKZ05+KhfNyuVXb5ey6VA9i6akRff3La9v40B1C16XA7dT2FbeyL6qZgozEpmSmdRr9U5lUzu5fh/5aQkUZSRq0B8jNOgrFWeRlI7bKSR6jr+xusfl4Ir5k7l8Xl6/51yzyNrC+sZzp3DF/Mm9jv/9J6fT1B6kpTPEmVPSKbBv/Cqva2N/dQslWUmcnpvC9ooG9lY2c1p2MnmpPo429kjvNLST67c+ecwtSGXXMU3vjAVaZVOpOIukd1ITPIOqgPmf1y087vEVZ+Tw7G3nMcfedKWn2ZNTuWR2Li9tP8aionQS7DeaMjvoz8hLIcXr5qUdR2nrDHH+9CzSEj3UtwZo6wyR4HESDIWpbrbSOwCnZSXx561H6AiG8LqO/8al4ktH+krFWWT1Ts81+kOxsCgdt7P//973XjmHf/nMXKZkJpGd7MXrcnCwppVDta0UZyYxpyA1esdtZKQPXcs2q5s7CRuiQX9qdjJhA4dqWvv9nmp00KCvVJx1jfSHL+gPJNfv43NLrAleESE/LYEPSmsIhg0lWUnMiVnxMy0nmUl20I8s24ws1+wK+tZKotjVPqrLscZ2/ri5It7dADToKxV3kZr6A03inkr56QnsOGKt35+ancTMPD9Ou8LnadnJ0SqfkcncSH5/kh30I9sw7q/WoN+XJ9Ye5m//sJG2zlC8u6I5faXizeV0kOx1DXhj1qkUmcwFKMlKxud2Mj0nmcqmDtKTPNG8fyS9Uxkd6VsTuSk+N9kpXkqrmlG9Ndib3je1B6L/lvGiQV+pUeDLy0pYUJQWt+8fWbbp97lIt998bji7KFqczed2kp7ojo70jzV24HQImfZ9AwBTs5K6rfMfKX/acoRl07Li+qY5kOZ2q0JpU0eQnDj3ZVDpHRFZJSK7RGSviNzZx/EpIvKaiGwRkTUiUhBz7Isissf+88Xh7LxS48XXLzqdC8+IXzgoSLf22C3JTo6uIPrCucV84+IzoudMSk2I5vSPNraTneyNpoDAmsw93kj/zd1V0U8Iw6WsrpXbH9vA0xvKhvV1h1tTR2SkHxzgzFNvwKAvIk7gAeBSYBZwnYjM6nHaT4FHjDHzgHuB++znZgB3A2cDZwF3i0j68HVfKTUcIpu0TM3qXdohIi/VFzPS71qjH3FadhJ1rYE+N2RpD4T48sNr+cGLO4ax11BqTxzXtHQMcGZ8RYJ981gI+ljBeq8xptQY0wk8DlzZ45xZwOv24zdijl8CvGKMqTXG1AGvAKuG3m2l1HCK5PT7qucTYQV9uzBbY3t05U5EZAVPaXXv0f7BmlaCYcPL249S0zx8AToycVx7nJ2/RoNI0G9qD8S5J4ML+vnA4Zivy+y2WJuBa+zHVwMpIpI5yOcqpeIsLzWBf756LtedVXicc3zUtQZoD4SidXdilWRZ5R5Kq1pYs6uST/xsDfWtnXab9UYQCJlhTcWMlaDf3NGV04+34Vqy+U1guYhsBJYD5cCg1yaJyC0isk5E1lVVVQ1Tl5RSJ+L6s4vI6RHIY02yl23e/fx2GtoCvdI7hekJuJ3C9opGvvPMVvZVtUTLN0cmeGdP9vOHjw5327JxKCKvW9cS/xH08URG+GMip48VwGPf/gvstihjTIUx5hpjzELgu3Zb/WCea5/7oDFmsTFmcXZ29gleglJqJJxdksGCwjT+uMW6yahn0TeX00FRRiK//+BgtAxzZM/efVXNTPL7+PKyEvZXt/DnbUd5aftRPhxikbYD1WMjp988inL6g1myuRaYLiIlWAH7WuD62BNEJAuoNcaEgbuAh+xDLwH/HDN5e7F9XCk1xhRmJPLc7UsxxtDYHuzzDuKp2cnsq2rh2iWFvL2nmp32DV+lVS1MzU7isrl53PPCdm57dANg7bq16fsXdysTbYxhx5FGTsu27hfoT0cwFN3msa519I70Q2FDi31T1pjI6RtjgsAdWAF8J/CkMWa7iNwrIp+2T1sB7BKR3UAu8GP7ubXAD7HeONYC99ptSqkxSkT6LRmxqCidXL+Xf1w1g5l5fnYeabR24KpqZmp2Ej63k/+4diHfvWwm37rkDFo7Q2y0N2cHeGt3FVf//D0+df87/Oad/cftx+HaVsIGCjMSqGvtJDRKd+5qjsnjN4+CnP6gbs4yxqwGVvdo+37M46eAp/p57kN0jfyVUuPYV5dP5cvLSvC4HMzKS+H1j49RXt9GY3uQqfZE74UzcrhwRg4NrQF+9vIu3t1Xw9lTM1l7oJYbH/qI/LQEspI9rD9Yd9zvtb/aGuUvnpLB4dpyGtoC0f0GRpPY0f1YyekrpdSgiEg0VTMzz0/YwF+2HQW6lnRGpCa6mZufynt7qwH433WHSfI4efnrF7D89By2lNUfd8J3v700dJF9J3PtKM3rx47ux9PqHaWU6mZmnlWp809bjwBW4baelk7LYtPhemqaO/jz1qOsmpNHktfF/MJUqps7e+3LG2t/dQsZSZ7oUtHaUbqCJzK6d8gYyekrpdTJKMpIJMnjZOOhejwuB5PTEnqds2xaFsGw4Z9Xf0xTRzC669dcewOYrWX1/b5+aZW1y1d6kjW/cCJr9Vf9+1v8+u3SE7mckxZZsZPr942K1Tsa9JVSp4TDIcywR/slmUnd6vRELJqSjtfl4OkNZeT6vZxjb7I+M8+PyyFsLmvo9/UP1FhBPzPJul+gZ9CvbGznjY8rez2vrqWTj482sfFQ/28ow6nRHt3npfo0p6+UGt9m5llr+Xvm8yN8bidLijMAuHJBfvSNwed2MiMvhS09RvrGGFo6grR0BDnW2NFtpF/X2j3o3/fnj7n5kXUEQ+Fu7ftrrLX9h+uOv8tXz/mErz+xiZ+9vOu4z+lLJKefl5YwKlbvaNBXSp0ykbx+f0Ef4PzpWQBcvbB7hZa5+WlsKWvoFnwfX3uY2Xe/xIqfrgGszVu8LifJXhc1zV1Bv60zxMvbjxIKm15r+PfbRdrK6tr67VNlYzsL7n2FdQesFeaN7QFe2FzBR/tPfMV5ZHQ/OdVHc0cw7ktLNegrpU6ZOZOt3Pz0nJR+z/niecU8ccs50TeIiPkFqTS1BzkQs+/um7uqyEr2cFZxBguL0lhcbN33mZ7k7jbSf/3jyugNUT3TPrH1elr6GXkfqGmloS3A0xusAgLv76shFDbUn8RNYM3tQZwOISfFKnHR0hnf0b4GfaXUKTOvIJUHv3Aml83N6/ccn9vJ2XYuv/tzraWYkRSPMYb1h+pYNi2LB25YxLO3LY0G0oxEDzUxwf2FzV3VXnqWaIjd0rG/0X5klc0rO44RDhve2m3VBKttPfHCbk3tAZK9rui2mPHO62vQV0qdMiLCxbMndSuzMFjTc5PxuhxsPmxN5pbVtVHV1MGZU3pvyZGR5InW8W9sD/DGriqWTbPSRrFpH+ha6gnWXb19iQTm6uYONh6u5+091r0E9a2dJ1wsrqkjaAd9a+4h3it4NOgrpUYlt9PB4uJ03thViTGGDXa5hkV9BP30JE80jfPStqN0BsPcdF4x0D29Y4xhf3VL9A2hrJ/J3Nj19L96q5RDta0UZiQQCJkTnoxtag+S4nORHB3px3etvgZ9pdSodfm8yeyvbmFbeSMbDtaR6HFyRm7v+YHMmKC/eusRCjMSWHFGNiJ0S/sca+ygLRBiSXE6PreDw/2kdxrt0fhZxRn8Zbt1R/Gn508GOOG8frMd9KPpnTiv4NGgr5QatS6dMwm3U3hhcznrD9WxoDANl7N32EpP8tAWCNHQFuCD0lpWnpGDy+kgLcHdrTxDZFevkqxkCtITjzPSD+J2ClfMt+YiCjMSWFhofcLouTR0IE0dAVJ8blK8mtNXSqnjSkv0sPz0bJ7bVMHOI0195vPBGukDvLrjGG2BEMumW/tyZMR8AgA4YBdpK8lOojA9gcO1/U/kpvjcXDRrEgDnT88+qTt/wRrpj6ac/qCqbCqlVLxcMX8yr+607qxdVNR30E9PtIL+85srcDqEc6ZaN3xlJnmpjpnI3V/djNflIM/voyA9sd9KnpE8/KRUHw/dtJg5k1OjufwTTe9oTl8ppU7ARbNySbA3U1loV9TsKTPZCvrv7q1mUVFadFSdmdx9pL+/uoXizCQcDqEwI4HG9iANbb2DsDXSt4L0yhm55Ph90TeWEx3pN3UESfa5SPI4EYl/TX0N+kqpUS3R4+KqhZNZWJRGWmLf9fIjATkUNiyb1rXlas/0Tmm1Va8HoCA9Eeh7BU9Te5AUb/eNYvwJbhxCdLP3wegIhugMhvH73IgIyV4XTe1BgqEw97ywvds9AyNFg75SatT70VVzefLWc/s9Him6BrDMLutgtXuiu2oFQ2EO1bRSYpeEKLSDfl95/UhKJpbTYe0YdiJbM0by98n2JK7f56apPcj2ikYefu8Az24oG/RrDRfN6SulRj2nQ3DSu0pnRIrPhdMhJHqczC9IjbZnJHkwxhqdN7UHCYYNJZl20M+wSj33PdIPRFNEsdITPSd0V24klRN5A7FG+gG2lFs3nG23N44fSRr0lVJjnsMhZCd7mV+Y2m1JZ0ZyV9nlSA2fabnWpiupCW6Sva4+SzH0NdIHa2noiaR3mnqM9FN8Lpo7gmyzS0Zvq+i/dPSpokFfKTUu/OLzi8j1+7q1ZdlLOaubO9l9rAmAaTlW0BcRCtITeGLtYZ7bVM7puSk8eeu5hMOG5s4g/j42f09PdFNe3/9uXj1Fgn7kU0Oyz6oGGhnpH2vsoLKpPVpDaCRoTl8pNS4sLErvtTtXRnLXipu9lc3kpfrwx6RtvnL+VFbOzKEoI5G1B2oJhMI0dwYxBvx9jPTTEk90pG/l/yOfGlJ8bqqbO9h9rInF9j0HI53iGVTQF5FVIrJLRPaKyJ19HC8SkTdEZKOIbBGRy+x2t4j8TkS2ishOEblruC9AKaX6EymsVttiBdrIKD/iM2cW8MD1i/irxYUYY705dI3Oewf9DHtieLD6yukfaWgnFDZ8bkkhADtGW9AXESfwAHApMAu4TkRm9Tjte8CTxpiFwLXAz+32/wN4jTFzgTOBW0WkeHi6rpRSxxdZylnVbI30T++jbg9AdoqV+69q6ogZnfdO76QlumkPhGmza/UPpGdOP/bTw9JpWUzJTGRb+cjm9Qcz0j8L2GuMKTXGdAKPA1f2OMcAkR0QUoGKmPYkEXEBCUAnMPLT1UqpCcntdJCa4Gbz4Xo6gmGm9xjpR0SCfmVT+3FH+pE3kcGO9iMj/eSYkT5AVrKHvFQfsyf7R3wydzBBPx84HPN1md0W6x7g8yJSBqwG/tZufwpoAY4Ah4CfGmNOfL8xpZQ6SZnJnui2h9P7G+knD26kf6J35Ta2B/C4HHhdTvs1raA/Jz8VEWH25FQO17bRcBI7cp2s4ZrIvQ542BhTAFwG/F5EHFifEkLAZKAE+IaITO35ZBG5RUTWici6qqqqYeqSUkpZN2hFtk7smdOP6J7eOd5I33ojGGz9neb2YLeUTrL9RjIv37qXYI799/YjIzfaH0zQLwcKY74usNtifRl4EsAY8z7gA7KA64G/GGMCxphK4F1gcc9vYIx50Biz2BizODs7u+dhpZQ6aZHJ3El+H6l9LMMEa8tGv89FVVNHtJZ+f+v0YfDpnfrWQDSlE/uakWA/e7KVFd/QT+G3U2EwQX8tMF1ESkTEgzVR+0KPcw4BnwAQkZlYQb/Kbl9ptycB5wAfD0/XlVJqYBl2iYbpuX2P8iOyU7xUNnXQaBdg8x8nvTOYoN/SEeTN3VXdKoOed1omt604jQtOtwa3WcleZuX5+enLu7nt0fXsrWwe3EUNwYBB3xgTBO4AXgJ2Yq3S2S4i94rIp+3TvgF8RUQ2A38AbjLWRpIPAMkish3rzeO3xpgtp+JClFKqL5Fa+9Nz+s7nR2SneKPpHbdT8Paxr2+and6pa+md3mnrDLHiX9/gDx8dAuC5TeU0dwS54Zwp0XNSfG6+vWoGPrtqKMDjt57D11ZOY82uKm57dP0J78F7ogZ1R64xZjXWBG1s2/djHu8AlvbxvGasZZtKKRUXkbLLA430c1J8bC6rj9bdEeld68ftdJDidfU50v/oQC0Halr54Ys7OH96Fr9//yCz8vws6qccdITf5+YfLj6DG88r5mhDe5/fdzjpHblKqXEtMknb3xr92PMiI/2+8vkR6f3coPXOnio8dt2fL/12LR8fbeIL504ZdBDPSvZGc/2nkgZ9pdS49smZufy/v5o/4Ig7O8VLa2eIo43txw/6iX2XV357TzWLi9P5xsVnsKeymRSviysXTB5y/4ebBn2l1Ljmczu5ZlHBgCPuyFr90qrmXhuoxEpL9FDXY51+ZVM7Hx9tYtn0LG46r5hPzMjh1uVTSfSMvpqWcqonDU7U4sWLzbp16+LdDaWULRAIUFZWRnv74KtLjkXtgVB0P90Et4PMZG+f59W2dNIRCDEp1Rd9I2ntDFLbEiAnxYunjwngUyEvL4+0tK5PLyKy3hjTa0l8T6PvbUgpNaqUlZWRkpJCcXHxKZ9kjKe2QIg9dvnl9EQPhRmJfZ7X1B5gf3ULWX4fOXYp58O1rSS0B5iZ5x+Rf6O2tjbKy8u7Bf3B0vSOUuq42tvbyczMHNcBH8Dt6Lo+p+N4u3S5SU1wU9nUQWcwjDGG5o4gSV7XiP0b+Xw+AoGTK92gI32l1IDGe8AHK9ALgsHgOE7QB8hL9dHU3szh2lZcTiEQCpPi6zsddCoM5eehI32llMIKpC6nFUydImzatInf/OY3fZ77ja//PTl+Ly2dQdo6Q2QkeUhN8Azp+z/88MOsX7++z2P33HMPr7766pBeP0JH+kqpCSUcDuNw9D3edTmEQAicDliwYAELFizo87z//M//xBhDRqKn2568Q3HTTTcNy+sMREf6SqlRo6KiggsvvJBly5Zx2223EQ6Hufnmm1m+fDmXXnopAO+++y5Lly5lxYoVPPHEE71eY82aNVx88cVceumlrFy5ktraWg4cOMCFF17IZz/7WR5++GE+/PBDVqxYwdKlS/ntb38bfd3rrriYL/+fy3n+6adYs2YN3/ve96itrWXFihVceOGFfO1rXwNg2bJliAjbt21l6dKlnHPOOfzP//wPYAXvr371qyxbtowf/OAHfV7nTTfdxJe//GUuuOACvv99q7hBZDT/4osv8q1vfYtwOMyqVas4dOjQsP4b60hfKTVoP/jj9iFv7zdrsp+7r5jd57GsrCxeeeUVXC4Xn//85/nZz35GTk4Ov/71rwmHwwDcddddPP/882RlZUXbejLG8Oc//5knnniCBx98kGuvvZbKykpeffVVnE4nl1xyCS+88AIpKSlcdNFF3HDDDdx111089OgTkOBnSkYCGz96H4CNGzeyYsUK7rnnnl51cf7pn/6JRx99lPz8fJYtW8bnPvc5AC655BJ++ctfcvbZZ3P33Xf32ceVK1fym9/8hssvv5zy8q7CxZdffjlPPvkkt9xyC1dccQVFRUUn9g88AB3pK6VGjZqaGj772c+yYsUK3nnnHVpbWznvvPMAoikZYwxZWVnd2npauHAhYKVo9u7dC8D8+fNxOq1CZ5s3b+bTn/40F154IUePHqWqqgpjDDk5OQC4XV0F0S644ALC4TA33HBDdDQfUVdXR3FxMW63m5KSEiorKwGYM2cOAAkJ3Tdq76uPc+fOZf/+/d2O3XrrrTz55JPcfPPNx/33Ohk60ldKDVp/I/Th8thjj3HVVVdx0003ccMNNzB//nw++OADLr/88mguXkSoqakhMzOz3/z85s2bo3+fdtppQPc3iIULF/LUU0+RlJREIBDA7bYKrDXW14IzEWJG9KFQiHvvvRew3kS+8IUvRI+lpaVx4MAB8vPzKS0tjb5pDGZ1zebNm5k1axbbtm3j9ttvj7aHw2F++MMfcvfdd/OTn/yk308KJ0uDvlJq1Fi5ciU33ngjzz33HAB+v58jR45wwQUXkJyczOrVq7nvvvu44oor8Hq9fPWrX42mVGK53W5WrVpFe3s7Tz/9NE1NTd2O/+AHP+CKK66wJmMzMnj66ae57777+OK1n8XhcvO3t/8NkyZNAuCjjz7iO9/5DoFAgE9+8pPdXufee+/l+uuvJxQKcfvtt+N291++oac333yTn//85yxfvpyCgoJo+/3338/VV1/Nrbfeymc/+1m2b98+6NccDC3DoJQ6rp07dzJz5sx4d2PQ1qxZw6uvvsqPfvSjeHelXzfddBPf+973mDZt2km/Rs+fi5ZhUEqNew0NDVx55ZXd2r7+9a/HqTd9e+KJJ/jFL34R/frcc8+NY290pK+UGsBYG+lPFCc70tfVO0qpAY22weFEN5SfhwZ9pdRx+Xw+ampqNPCPIu3t7Sc0aRxLc/pKqeMqKCigrKyMqqqqeHdFxcjLyzup52nQV0odV+TGIzU+aHpHKaUmEA36Sik1gYy6JZsiUgUcPMGnZQHVp6A7o4Fe29g0Xq9tvF4XjP1rm2KMyR7opFEX9E+GiKwbzPrUsUivbWwar9c2Xq8Lxve1xdL0jlJKTSAa9JVSagIZL0H/wXh34BTSaxubxuu1jdfrgvF9bVHjIqevlFJqcMbLSF8ppdQgjPmgLyKrRGSXiOwVkTvj3Z/BEJEDIrJVRDaJyDq7LUNEXhGRPfbf6Xa7iMj99vVtEZFFMa/zRfv8PSLyxThdy0MiUiki22Lahu1aRORM+99qr/3cgbckOrXXdo+IlNs/u00iclnMsbvsfu4SkUti2vv8HRWREhH50G5/QkQ8I3hthSLyhojsEJHtIvJ3dvuY/tkd57rGxc9tWBhjxuwfwAnsA6YCHmAzMCve/RpEvw8AWT3a/i9wp/34TuBf7MeXAX8GBDgH+NBuzwBK7b/T7cfpcbiWC4BFwLZTcS3AR/a5Yj/30jhf2z3AN/s4d5b9++cFSuzfS+fxfkeBJ4Fr7ce/BP5mBK8tD1hkP04BdtvXMKZ/dse5rnHxcxuOP2N9pH8WsNcYU2qM6QQeB64c4Dmj1ZXA7+zHvwOuiml/xFg+ANJEJA+4BHjFGFNrjKkDXgFWjXSnjTFvAbU9moflWuxjfmPMB8b6H/ZIzGudcv1cW3+uBB43xnQYY/YDe7F+P/v8HbVHvSuBp+znx/47nXLGmCPGmA324yZgJ5DPGP/ZHee6+jOmfm7DYawH/XzgcMzXZRz/BzxaGOBlEVkvIrfYbbnGmCP246NArv24v2sczdc+XNeSbz/u2R5vd9gpjoci6Q9O/NoygXpjTLBH+4gTkWJgIfAh4+hn1+O6YJz93E7WWA/6Y9UyY8wi4FLgdhG5IPagPTIaF8uqxtO12H4BnAYsAI4AP4tvd4ZGRJKBp4G/N8Y0xh4byz+7Pq5rXP3chmKsB/1yoDDm6wK7bVQzxpTbf1cCz2J9lDxmfyTG/rvSPr2/axzN1z5c11JuP+7ZHjfGmGPGmJAxJgz8CutnByd+bTVYKRJXj/YRIyJurMD4qDHmGbt5zP/s+rqu8fRzG6qxHvTXAtPt2XQPcC3wQpz7dFwikiQiKZHHwMXANqx+R1Y+fBF43n78AnCjvXriHKDB/vj9EnCxiKTbH1UvtttGg2G5FvtYo4icY+dSb4x5rbiIBETb1Vg/O7Cu7VoR8YpICTAdayKzz99RexT9BvBZ+/mx/06nnP3v+RtgpzHm/8UcGtM/u/6ua7z83IZFvGeSh/oHa1XBbqyZ9u/Guz+D6O9UrJUAm4HtkT5j5QpfA/YArwIZdrsAD9jXtxVYHPNaf4018bQX+FKcrucPWB+XA1j5zS8P57UAi7H+g+4D/gv7hsI4Xtvv7b5vwQoYeTHnf9fu5y5iVqr09ztq/y58ZF/z/wLeEby2ZVipmy3AJvvPZWP9Z3ec6xoXP7fh+KN35Cql1AQy1nJOx8kAAAA5SURBVNM7SimlToAGfaWUmkA06Cul1ASiQV8ppSYQDfpKKTWBaNBXSqkJRIO+UkpNIBr0lVJqAvn/s+cAx/KpVGgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8VNX9//HXyWSykn3fwxKWsO+giIiIiAtWbd21VqXfurTWrfpzqbV++622Vmu1tla01n2rSgVEKJsg+xIIgUASIPtO1sk6c35/zCQkkIQhTDLM5PN8PHgwc+fOzOdkkndOzj33XKW1RgghhHvxcHYBQgghHE/CXQgh3JCEuxBCuCEJdyGEcEMS7kII4YYk3IUQwg1JuAshhBuScBdCCDck4S6EEG7I01lvHB4erpOTk5319kII4ZJ27txZrrWOON1+Tgv35ORkduzY4ay3F0IIl6SUOmbPfjIsI4QQbkjCXQgh3JCEuxBCuCEJdyGEcEMS7kII4YZOG+5KqbeUUqVKqfRuHldKqVeUUllKqb1KqUmOL1MIIcSZsKfn/k9gQQ+PXwak2P4tBl4/+7KEEEKcjdOGu9Z6A1DZwy6LgH9pqy1AsFIqxlEFCjEQpOVVsf1oTz9mQpwZR4y5xwF5He7n27adQim1WCm1Qym1o6yszAFvLYR7+NXne3nk0zRnl+E0rWaLs0twO/16QFVr/YbWeorWekpExGnPnhViQCisauBgcS1HK0xUmZqdXU6/O1RSS+qvV7I797izS3Erjgj3AiChw/142zYhhB3WZZ74KzYtv9qJlTjHN+nFNLdaWHuw1NmluBVHhPtS4DbbrJkZQLXWusgBryvEgLAus5SIAG+Uso69DzTrMq2hvv2oY3rurWYLJTWNDnktV3bahcOUUh8Cc4BwpVQ+8GvACKC1/huwHFgIZAEm4I6+KlYId9PUamZTVjk/mBTHlpzKARfuVaZm9uRV4e3pwe684zS3WvDy7H2fU2vNfR/sZlNWOTuemoe3p8FhtX6TXszqAyVMGxzKhcMjiAr0cdhr94XThrvW+sbTPK6Bex1WkRADyI6jx6lvNjNneCQNzRbWHypFa41Sql/ev8Vs4cFP0rh1RhLTBod2eiy9oJoV6UU8PH8ESikams08/GkaQyMH8cPJ8SSE+p3Re73z/VF8vQz8aMqJUdwNh8uxaLj9vGTe2JBDemE1kxJDet2ef3yXwzf7iwE4VFzH2PigHvfXWtPUasHH2PMvgbxKEw9+sofmVguf7czH29ODT/9nJuPig3tda1+TM1SFcILaxhZMza2sPViKl8GD84aFMSEhiPK6Zgqr+29IYcfR4/wnrZBHPkujscXc6bHX1mbx2trs9uMA32YUs2xfEa/89zCz/7CWV9cctvt9mlrN/GFlJi98cxCzRbdvX5dZSoifkTtnDQZg+xH7p4NWN7Rg7Vtabc2p4PlvMpmabP3lsK+g5+MXZovmnvd3MfW51Szb2/1IssWiefjTNDyUYt0jc1j281mE+Xtx3we7qWls6bTfrUu28uhnaVg6tNFZJNyF6GclNY1M/d/VpD69kjc3HmH6kFD8vDwZn2DtBablVbHmYAkL//wdeZWmU56fW2Ei//ip23tjXWYpHgqOVZj4+/qc9u2m5lbW2sbCv9iVD8BXewqJDfLhu0cvYsHoaF5cdYhttjBuNVs6Bd3JtuZUUtfUSnldc/t8fotFsz6zjNm2IY7B4f52jbun5VVxz/s7mfDst9zz/i7qm1rZklPBne/sICnUjyU/nkqQr5F9BT0Pcf1u+QFWpBcT7G/k3g928eSX+2jpYkrmW5uOsPVIJU9fmUp8iB+jY4P4y00TKahq4PHP97X/glm2r4jvDpfzyY58nl95sNv3zSis6fQLrq9IuAtxErNFs2xvEcfr+2Za4n/SCmlssXD/3GHcNWswD80fAcDI6EC8DB4s3VPILz7aQ0ZRDS9+m9npucXVjVz12kZuXbLNIb3DtZmlzBwaxuXjYvjruixyK6y/NNZnltHYYiE+xJf/7C2itKaRDYfKuHJCLAmhfvzxh+NJDPXjlx/vYVVGCQv+/B3n/d+abo8ZrD5Qgq/RgLenB9+kW4dN0gurqahvZs4I67Toqckh7DhW2WO7Nh4uZ9Frm/jucDlXjotl5f5irnp1I7e/tY3oIB8+uHsGgT5GxsYF9dhzf/O7HJZsPMId5yez5qE5LJ49hPe25PLTd3fS0HziL5hPtufxv8sPMG9UFD+cHN++fXJSKA/PH8GyfUW8uiaLVrOFP606xIioAG6ensjf1+fw/tbO19TQWvOvzUe56tWNvLXxSM8fjANIuAvRgdaap75K594PdnHRi+t4d8sxCqoaKK1tdNiJNl/tKWRcfBAPzR/Bk1ekMsHWY/fy9CA1NpBv9hfjoRTXTIzjq7RCMgprgBPDA1WmFo6U17OmF1MHs8vq+PPqw7SaLeQfN3GopI6LRkTy1OWpGDwUT3xp7YmuSC8mxM/Ik5enUlnfzK8+30urRXP1BOv5if7enrx8/QSKaxq5+187aGo1E+Rr5Pa3t5FZXMvOY5W8tjaLiromtNaszijhgpRw5oyIYEV6ERaL5m/rs/EyeDA7pS3cQ6kytZBVVtdt/a+sOUxskA/fPzaXV26cyNt3TKOstomR0QF88tOZRAdZD3KOiQsis7iWptbOQ02tZgu/+c9+nlt2gAWjo3ny8lSMBg/+38JR/O4HY1mbWcotS7byzvdHeeGbgzz6+V5mDQvnLzdOPOU4yE9nD+GaiXG8uOoQd/9rB0fK63n40hH85qrRXDQigqe/2t/+14+puZWHP93L01/t58LhEfxoagJ9zWmX2RPiXPTqmiw+2JrLjdMSOVJex1NfpvOU7bGoQG9um5nMTdMSCfH3Ou1r7cmrory2iXmpUe3bssvq2FdQzZOXj+ryORMTg0nLr+LlGyYwKSGE1QdK+MPKg/zlpkm8tfEIG7PK+e2i0fx1XTZLNh5hXmoUlfXWoY75qVHtAWSxaDw8OoeRxaJ58JM00vKq8DF64O9t/fGfMyKS6CAfHr9sJE99tZ+3Nx1lzcFSrhgXw9yRkYT4GVmbWcaIqABGxQR2qDWEP/5wHEXVjfzk/MGU1jRx3d++Z8GfN9A2FL4lp4JfLRhJYXUjD1wyHG9PD1buL+H/Vhxg+b5iHrl0BGGDvAFruAPcumQrQb5Gbp6exO3nJbe/385jlWw7UsnTV6QS4GME4MLhEWx6bC6+RgOehhN91bFxQbSYNZnFtYyLD2ZzdgWbs8tZf7ictLwqfnL+YJ64fBSGDl+jm6YnEujrycOfprHzmHV46JLUKF69aWKXs248PBTPXzeOmsYWVh8oZWJiMPNGRaKU4tWbJvGjv2/mvvd38eQVqby+LpvcShMPzEvh53NTTvls+oKEu+i1X322lynJIfxwytn3QrTW7CuopsrUgsFDMTkppNsZDMfrm/lgWy63zUxq/yE/2Wc783l70xH+ecc0IgK87aphxb4iXlx1iGsmxvG7H4wBYGNWOUVVjTS2mlmVUcIfVmby5nc5/PmGicwe3vks601Z5YyNDyLQVtOvl+7nQGENax+ZQ1ywLwBL9xSiFFw5PrbLGu6fm8LlY2OYYgu6ey4axu9XHGTsMyvR2ho2t8xIor7ZzO9XHGR1Rgm/W36AnPJ63r1zGhekRFDd0ML8l9YzMjqQ3187lpgg63v/e3cBaXlVxAX78qdVhxgeFUBCqC9DI/wBuGVGEt9mlPDs1xkALBgTjZenB1eMi+XdLcdYNPHUmn8w8cRQRWKYH+/fNZ03vzvCzKFhlNc18dyyAxQc341SMHdkJN6eHngZPPjHd0dIjQlk8ewh7c9PCvPj5xencLS8nv2F1fzx20yumxzf/kvo9XXZBPsZuWFa5++3rr4HxtlmyewrqKa6oYVbl2zD4KFIiRzEC9eO67bnfMW4WC5JjaK+yUxzq4WoQO8eZy4ZDR68etMkXlp9iGsmxrfv6+/tyVs/nsrVr23i8X/vIynMj48Xz2D6kLBuX8vRJNxFr9Q3tfLxjjz+s7eQWSnh7QHSW+9uOcbTX+1vvz8qJpA3b5/SHoptLBbNLz/Zw7rMMkpqGnl20ZhTXsti0fz5v4fIq2zg/g938d6d0zv16rpyvL6Zp75KZ0xcIL+/dlz7D+kFKScC/LaZyWQU1vDLj/dw+9vbeOTSEdwzZxhgnTZ485tbuXPWYJ66IpXS2sb28edXVh/m+evGobVmaVohM4eEdTtHOtTfi1D/E1MSf3xeMqU1TYT4GRkdF8isYREopbhxaiKv/Pcwd/1rBwE+noT4GXl701EuSIngvS3HKKlporqhgvkvbeCeOcOYOzKS5785yPiEYF6/eRKXvrSBfQXV3DYzqb2tSileuG4c81/aAMB5Q8Nt7U4ivbCaayfFn1rwSVKiAnj+unGA9Rf27twqlu0rYnJSCOG2Hvrs4eGszSzjhevGYezwuSilePCS4YC1l37t65v5YncBt8xIIrO4ltUHSnlgXgp+XqePrfgQX4J8jezNq+b9LbnEh/iy8oHZ7b8oeuLtaTij+fE+RgOPX3bqX2JRgT68f9d01hws5ebpSfh6OW7OvT1kzF30SmZJLQCmZjPPLTtwVq+VXlDNc18f4MLhEXz+s5n8+YYJ5FeaWPTqJvbmdz5At2TjEdbZhgje3XKMfV2crr/+cBl5lQ0sHBvNlpxK/nDSQcmuPLfsAFWmFl64dnyPJ9Gkxgbyxb3ncfnYGF74JrN9TPUf31lnmvwnrRCzRbPuoHVJgfOHhfHZrnyyy+p4b2suR8rrWTSh6157V3yMBp6+MpX7L05h7sio9tqC/Iz8+LxkogK9+WjxDG6bmcyag6UcKKrhrY1HuHB4BCsfmM3YuCCe/+Ygl768gbLaJp65MpXYYF9+ddlIAOaNiur0fjFBvrx52xRevn5C+3ulRAXwxT3nn/FJO0opfnfNWCYkBHPjtMT27b++cjTv3TmdMXHdz0GflBjC2Lgg/vn9URqazTz06R4CvD25fWay3e89Ni6IL3YXkFFUwyOXjrAr2B1tSMQg7rpgSL8HO0i4i146WGQN9+smx7NsbxGbssp79Tp1Ta3c/+FuQvyNvHT9BCYnhbJoQhxf3HseXgbFo5/tbZ9qti+/2hpUo6P45H9mEubvzZNf7jtlWtl7m48RPsibl6+f2D5zIb2HmRNrM0v5fFc+P71wCKmxgd3u18bPy5M//WgCyWF+PPd1Bscq6vl6bxEpkYMorW1iS04Fqw+UEBvkw8vXT8Tb04PrXv+ep75MZ+aQsG6HZM7UI5eO4PvHLmZ0bBA3z0jEaFDc+c/tVNQ387M5Q0kK8+eDu2ew/pE5/GrBSH579Rgm2k4Qunl6Iit+cQEXpISf8rrTh4Rx8Umh31tBvka+vPd8rusw0yQh1I+ZQ3senlBK8ePzkskqreNHf9/M/sIaXr5hgl3HOtqMiQui2WxhTFwgV45zzNfclUi4i17JLK5hkLcnz109hqQwP174pvt5vT351+ajHCmv55UbJhLa4Qd3WGQA91+cwsHi2va5z79bfoBgPy9euHY8Qb5GnrpiFGn51Xy4Lbf9efnHTazJLOWGqQl4eXrw6IKR+Bg9Ou3T0dacCu55bxcjogK4f26K3XV7eXrwxOWpZJfVc+uSbQD8/dbJDPL25OPteXx3uJy5oyKJCPDmfy4cSn2TmScWjuL9u6bbNaxgD6VU+wHByAAfrhwXS2F1I5MSg5ne4WzTpDB/fjZnKLfOSOr03FExgf12JmxvXDE+hvBBXuwrqOahS4af8S+caYNDUAoev2xUvxzAPNdIuIteOVBcy4joAHyMBm6ZnkRafjVHyuvP+HW+TitiYmJwlweaFk2IJdDHk3c2H2VLTgWbcyq4Z85QgvysB9CuGh/LzCFh/GFlJuV1TQD8c9NRFHDjdOswQJCvkYVjY1i6pxBTc2un1992pJIfv72duBBf3r1r2mlPQT/ZvFGRzBoWTm6liSvHxTAkYhALxkSzNK2QhhYzF4+0htH9c4ex++lLuHv2kD4NmTsvGIy3pwe/mDf8nA5te3l7GvjNVWO496Kh3HvRsDN+/kUjIvn+sbmcP+zUv04GAgl30aPGFvMpQxpaW6eYjYgOAKw9LKXg67TCbl/nUEntKeF6pLyejKIaLh/b9YW7/Lw8uX5qAt+kF/PbrzOIDPDmpuknxm6VUvz26tGYmlv5/YqDvLflGG9uPMI1k+I7HYi9YWoitU2tLN9X3KkNj36WZjvxZTqRAWe+CJRSimeuSmVUTGB7+LTNA/c1GtqHHpRS/TLeOzo2iH3PXMqFw93nWgmXj4vhkUtH9uqXlVLqrA/0uzIJd9GjP67M5Iq/bOR/l2W0n8RTXNNIdUMLo2zhHhPky9TkUJamFaK1pqHZzMr9xezKPc6hklrueX8n81/awI3/2Epd04mAX77Pup7Hwm7CHeDWGclYtGZ/YQ0/mzP0lN71sMgA7r5gCJ/tzOfJL9OZNyqS3/1gbKd9piaHMCTcn4+3nxia2ZJTydEKEz+/eFivgr3j+6/4xQWkRFm/FjOHhhET5MOFwyPO+C8BRzibFRWFe5GpkKJbFovm671FhPp78Y/vjpBRVMObt03lYLH1YOqI6BMHH68cH8tTX6azv7CGP36b2ekCFL5GAzdOS+CTHfnc/c4O3r5jKj5GA1/vtU6Riw3uvneVGObHJaOi2FdQ3WnGRUf3z01h5f5iksP8ee3mSacEnFKK66cm8H8rDnK4pJaUqAA+3p5LoI8nl41x7OV+DR6Kz392Hv4OGlcXorfkO1B0a3fecYprGnnp+vG0tGoe/XwvSzbmYPCwhmfbsAzAwjHRPLN0P3f8cztltU08sXAUg8P9Kapu4JLUaKKDfJg2OJRffpzG9W9s4ZbpiRwoquHpK1JPW8dL10/ocVlWXy8DKx+Y3eNc9msnx/OXNVk89Gkab942heXpxdw4NaFPetc9/bISor9IuItuLdtbjJenB/NGRRHgY2T1gRL+tj6HSUkhxAb5EOR74szAsEHenD8snA2HyvjZnKHc3eHMwzY/mBiPwcOD577O4JHP9gI9D8m08ff2xP80J5me7iSl8EHe/OlH41n87k6uef17mlstXD+1678EhHAHEu6iSxaLZvm+ImanRLSf3v3oghHMf2kDGw6VMXdk5CnPeWLhKOYMj+DHHdYDOdlV42O5ZFQU7205RqtFty/01B/mj47m/rnD+MuaLMbFB9k1p10IVyXh7iKKqxtPu86FI7SdMNQ2JPOry0a0PzYsMoDrpybw4ba8TkMybUZEB3S5/WS+XoYue/b94YF51tPbT14XRgh3I+HuAvIqTcz54zpe/OF4rp4Y12fvk1FYw30f7KKwugFfo6F9SKajB+YNZ9uRyvZlWl2NwUO1r58uhDuTcHcBu/OqMFs0X+wucGi45x838draLML8vQn09eSlVYcJ8jVy07QkCqpMTEgIOWXFvahAH/770ByH1SCE6BsS7i5gf6H1JKJNWeVUmZoJ9jv9+hpaaz7ansfU5BCGRZ46VFJW28StS7ZRUNWA2aIxWzSTk0J4/ZZJZzXvWwhxbpBwdwEZhTUEeHtS29TKtxklna4e353NORU8/u99RAf6sPS+84nssKJflamZ29/aRlF1Ax/ePZ3RsUHkVpoYHO7faQlWIYTrkp/kc5zWmozCGhaMiSY+xLf9rM7TeXVNFmH+XtQ0tnD3uztpbDHTarbw8fZcLn5xPYdLa/nbLZOZnBSKj9HA8KgACXYh3Ij03M9xpbVNVNQ3Mzo2kFB/L5ZsPEJuhYlVB0qIDPBuXz52zcES3t50lCcuH0V9UyvfZ1fw5OWjSAj146fv7mTMr1fSalsad0pSCL9ZNJrRsd2vpy2EcG0S7ueQL3cXYGo2d1ocq228PTU2iImJIfx9Qw5z/riWtiXMj5TXMzo2kP95byctZs01f/2e+BBfQv29uGl6In5envztlsnszjuOv5cnw6MGcenoaLdYNVAI0T0J93OExaL53fIDmJrNXDs5rv0yXxmFNQCMiglgkLcnl42JxtfLwO0zk3ln81H+tOoQSlkvCPynH43n0c/2siu3ikcuHdG+bviCMdEsGBPtrKYJIZxAwt2JjpbXkxDqh8FDsSv3OKW11jXJt+RUti/bur+whqQwv/Ypia/fMrn9+S/Gjyc+xI99+VW8fP1EgvyMfLh4BmsPljJ3pGOupCOEcE1yBM1J0guqmfviOl5bmwXAivRivAwe+HkZWLn/xLrjGUU1jO7mNPm2Cwq/fce09gtYeHsaWDAmRpZ+FWKAkwRwkhdWZmLR8I8NOVSZmvkmvZgLUsKZMyKCVRklWCyamsYWjlWYSI2RNVCEEGdGwt0JtuRUsOFQGT+cHE9tUyu//HgPBVUNLBgTzaWjoymrbWJ3XhU7j1mvHSqzWoQQZ0rG3PuZ1poXvjlIdKAPv716DKYWM8v2FuHpobgkNQqlFJ4eipdXH2JPbhVRgd5MSgpxdtlCCBcjPfd+tjm7gl25Vfz84hR8jAYeuDgFpayXZwv28yLI18jMoWF8d7icmGAf/n3P+Z3WTRdCCHtIz72frUgvxtdo4JpJ1gXAUqICePXGSQyJ8G/f576LhpEQ6sevFoyUYBdC9IqEez/SWrMqo4TZw8M7Xd7t8nGdr0Y0fUgY04eE9Xd5Qgg3IsMy/WhfQTXFNY1ckionFAkh+paEex8zWzSNLWYAVmWU4KHg4i4uUSeEEI4k4d7HnvoqnYv+uI7ssjq+3V/C1ORQQvxPvx67EEKcDQn3PnawqIai6kauff17MktqmT9ahmSEEH1Pwr2PFVY1Mn1wKD62hcDmp8qaL0KIvmdXuCulFiilMpVSWUqpx7p4PFEptVYptVsptVcptdDxpbqeFrOFktpGpg8J44t7z+PtO6aSEOrn7LKEEAPAaadCKqUMwGvAJUA+sF0ptVRrndFhtyeBT7TWryulUoHlQHIf1OtSiqsb0Rrign2ICfIlJsjX2SUJIQYIe3ru04AsrXWO1roZ+AhYdNI+Gmhb3SoIKHRcia6rsKoBgNhgCXUhRP+yJ9zjgLwO9/Nt2zp6BrhFKZWPtdd+f1cvpJRarJTaoZTaUVZW1otyXUthtYS7EMI5HHVA9Ubgn1rreGAh8K5S6pTX1lq/obWeorWeEhER4aC3PncVVjUCECvDMUKIfmZPuBcACR3ux9u2dXQn8AmA1noz4AOEO6JAV1ZQ1UCovxe+XobT7yyEEA5kT7hvB1KUUoOVUl7ADcDSk/bJBS4GUEqNwhru7j/uchqFVQ3EBvs4uwwhxAB02nDXWrcC9wErgQNYZ8XsV0o9q5S6yrbbQ8DdSqk04EPgx1pr3VdFn6ssFs3j/97H3vwqAIqqGmWGjBDCKexaFVJrvRzrgdKO257ucDsDON+xpbmevOMmPtyWS6vZwh9+GExhVQMzh8rqjkKI/idnqDpQdlkdAN9nV1Dd0EJtU6sMywghnELC3YGyS+sB64HULTkVgEyDFEI4h4S7A2WX1WE0KAA+3ZEPSLgLIZxDwt2BssvqmJgQQlSgN2szSwGIk3AXQjiBhLsDZZXWMTRyEOcPDcds0RgNiohB3s4uSwgxAEm4O0hlfTPHTS0MjfDnvGHW87eig3zw8FBOrkwIMRDJBbLPQIvZgqeHQqlTA7ttpszQyEGMjA4AZNkBIYTzSM/9DPz03Z0sePk7citMpzyWXWoN92ERg4gJ8mV8QjCjY4P6u0QhhACk535GduUep8rUwtV/3cTjl43EaPAgMsCb84aFk11Wh7enR/vsmE9/OhNPGZIRQjiJhLudqhtaqDK1cNP0RLZkV/DIZ3vbH1ty+xSyy+oZEjEIgy3QvTzljyIhhPNIuNspr9I6FHPBsHCeviKVrNI6fIwGfvbeTn69dD8Wi2ZSUoiTqxRCCCvpXtqpLdwTQv3wMRoYExfEsMhB/PbqMeQfb6CwupGhEYOcXKUQQlhJuNsp1xbuiWGdL3A9Y0gY10yyXphqaKSEuxDi3CDDMnbKrTQR7Gck0Md4ymNPXp6Kv5cnF6a4/9WlhBCuQcLdTrmVJhJD/bp8LNTfi99ePaafKxJCiO7JsIyd8ipNJHQT7kIIca6RcLeD2aIpqGrotucuhBDnGgl3OxTXNNJi1hLuQgiXIeFuh7blBiTchRCuQsLdDu1z3EMk3IUQrkHC3Q65lSYMHooYuR6qEMJFSLjbIbfSRGywD0aDfLmEEK5B0soOPc1xF0KIc5GEew+O1zfz7f5issvqJNyFEC5FzlDtRl6liYtfXE+z2YK3pwcXyNICQggXIuHejW1HKmk2W3j95knMHRWJt6fB2SUJIYTdJNy7kZZfhb+Xgfmjo9svwCGEEK5Cxty7kZZXxdj4IAl2IYRLknDvQlOrmQNFtYyPD3Z2KUII0SsS7l04WFRLs9nC+AQJdyGEa5Jw78Le/CoACXchhMuScO/Cnrxqwgd5ERskyw0IIVyThHsX0vKrGB8fjFJyMFUI4Zok3E9S29hCdlkd4+RgqhDChUm4n2TNwVK0hvEJQc4uRQghek1OYrIprWnkiS/TWZVRQnyIL5OTQpxdkhBC9JqEu83r67NZl1nKw/OH85NZg/Hzki+NEMJ1SYLZlNQ0khjqx31zU5xdihBCnDUZc7cpr20mbJC3s8sQQgiHsCvclVILlFKZSqkspdRj3ezzI6VUhlJqv1LqA8eW2ffK65uIkHAXQriJ0w7LKKUMwGvAJUA+sF0ptVRrndFhnxTgceB8rfVxpVRkXxXcVyrqmgkb5OXsMoQQwiHs6blPA7K01jla62bgI2DRSfvcDbymtT4OoLUudWyZfau51UJ1Qwvh0nMXQrgJe8I9DsjrcD/ftq2j4cBwpdQmpdQWpdQCRxXYHyrrmwGk5y6EcBuOmi3jCaQAc4B4YINSaqzWuqrjTkqpxcBigMTERAe99dkrr2sCIMxfeu5CCPdgT8+9AEjocD/etq2jfGCp1rpFa30EOIQ17DvRWr+htZ6itZ4SEXHuXJO0LdwjAqTnLoRwD/aE+3YgRSk1WCnlBdwALD1pny9gxx7nAAAPGElEQVSx9tpRSoVjHabJcWCdfaqizjYsIz13IYSbOG24a61bgfuAlcAB4BOt9X6l1LNKqatsu60EKpRSGcBa4BGtdUVfFe1obT338AAJdyGEe7BrzF1rvRxYftK2pzvc1sCDtn8up6K+GW9PD/y9DM4uRQghHGLAnqF6qKSWalMLYO25hw/ylvXbhRBuY0CGu9aa617/npdWHwKgvK6ZcJkGKYRwIwMy3MvqmqhpbG2/VmpFXZOsKyOEcCsDMtzzKk0AHCyuxWLRVEjPXQjhZgZkuOfawt3UbOZYpYmKeum5CyHcy8AM94qG9ttbcypoMWtZV0YI4VYGZrhXmgj198JDwXdZ5QAyLCOEcCsD8kpMeZUmhkUMoqK+iU22cJezU4UQ7mTA9twTQv0YFRNIlW2ue7isKyOEcCMDLtwbW8wU266XOiomsH279NyFEO5kwA3L5B+3HkxNCvMjyNcIgFIQ6i89dyGE+xhw4d42xz0h1I+4YF8AQv28MHjI0gNCCPcx4MK9bY57Yqgf4YO8CPEzyjRIIYTbGZDh7ms0ED7IC6UU5w0Nl167EMLtDMhwTwz1a18B8pUbJyLRLoRwNwMu3PNs0yDbSK9dCOGOBtRUSK11e89dCCHc2YAK95KaJkzNZhJCfZ1dihBC9KkBFe6f7cwD4Pxh4U6uRAgh+taACffGFjP//P4YFw6PYHhUgLPLEUKIPjVgwn3pnkLK65q4+4Ihzi5FCCH63IAId601b27MYWR0AOcPC3N2OUII0ecGRLh/n13BoZI6Fs8e0j6/XQgh3NmACPcV6UX4eRlYODbG2aUIIUS/cPtw11qzOqOU2SkR+BgNzi5HCCH6hduHe3pBDcU1jcxLjXJ2KUII0W/cPtxXHSjBQ8HckZHOLkUIIfqNW4Z7aW0jxyrqAViVUcKUpFC5GIcQYkBxy4XDnvwinf8eLOWW6YkcKKrh/y0c6eyShBCiX7llz/1YhQkfTw/e2XwMgHmjZLxdCDGwuGXPvai6gWsnxzM7JYKssjqGRAxydklCCNGv3C7cTc2t1DS2EhXow7zUKOYhvXYhxMDjdsMyxdWNAMQE+Ti5EiGEcB63DfdoCXchxADmduFe1N5zlwtyCCEGLrcL9+IaW889UHruQoiBy+3Cvai6gWA/I75eso6MEGLgcrtwL65ulF67EGLAc79wr2mUg6lCiAHPrnBXSi1QSmUqpbKUUo/1sN+1SimtlJriuBLPTHF1o0yDFEIMeKcNd6WUAXgNuAxIBW5USqV2sV8A8Atgq6OLtFdTq5nyumaiA2WmjBBiYLOn5z4NyNJa52itm4GPgEVd7Pdb4Hmg0YH1nZHSmiZATmASQgh7wj0OyOtwP9+2rZ1SahKQoLVe5sDazliRnMAkhBCAAw6oKqU8gD8BD9mx72Kl1A6l1I6ysrKzfetTtM1xl567EGKgsyfcC4CEDvfjbdvaBABjgHVKqaPADGBpVwdVtdZvaK2naK2nRERE9L7qbhRXNwAQJeEuhBjg7An37UCKUmqwUsoLuAFY2vag1rpaax2utU7WWicDW4CrtNY7+qTiHhRVN+LvZSDA2+0WuxRCiDNy2nDXWrcC9wErgQPAJ1rr/UqpZ5VSV/V1gWeiuNo6x10p5exShBDCqezq4mqtlwPLT9r2dDf7zjn7snqnqLpRFgwTQgjc7AzVouoGmSkjhBC4UbjXNbVSUtPE4HB/Z5cihBBO5zbhnlNWB8BQuV6qEEK4T7hn28J9WKT03IUQwm3CPaesHoOHIjFUwl0IIdwm3LPL6kgK9cPL022aJIQQveY2SZhdWs8QGW8XQgjATcLdbNEcKa9nqIy3CyEE4Cbhnn/cRLPZIjNlhBDCxi3CPbt9GqT03IUQAtwl3EvrARgSLj13IYQAdwn3sjrC/L0I8fdydilCCHFOcJtwl/F2IYQ4wU3CXWbKCCFERy4f7tWmFirrm2XBMCGE6MDlw728vgmAyABZ6lcIIdq4fLhXmZoBCPYzOrkSIYQ4d7h8uB+vbwEgxE9mygghRBvXD3dbz13CXQghTnD5cK8yWXvuwf4yLCOEEG1cPtyPm5rx9FAEeNt1rW8hhBgQ3CDcWwj2M6KUcnYpQghxznD5cK8yNRMs4+1CCNGJy4f7cVMzITINUgghOnH9cK9vkZ67EEKcxPXDXXruQghxCpcOd601VaYWmeMuhBAncelwNzWbaTZbZB13IYQ4iUuH+4mzU2VYRgghOnLpcG8/O1WGZYQQohOXDndZV0YIIbrm4uHetiKkDMsIIURHLh3uJ9Zyl567EEJ05NLh3raWu1yoQwghOnPtcDc1E+DtidHg0s0QQgiHc+lUrDI1yzruQgjRBZcO9+NydqoQQnTJpcNdlvsVQoiuufTli46bWkgO93d2GUIIF1FVVUVRUZGzy7Cbj48P8fHxGI1nPvzs2uFe3yzDMkIIu5WXl5OcnIyvr6+zSzktrTUVFRXk5+czePDgM36+XcMySqkFSqlMpVSWUuqxLh5/UCmVoZTaq5T6r1Iq6YwrOUMtZgu1Ta0yDVIIYbeWlhZ8fHycXYZdlFKEhYXR2NjYq+efNtyVUgbgNeAyIBW4USmVetJuu4EpWutxwGfAC72q5gxUtZ+dKj13IYT9XOl6y2dTqz0992lAltY6R2vdDHwELOq4g9Z6rdbaZLu7BYjvdUV2ajs7VZb7FUKIU9kT7nFAXof7+bZt3bkTWNHVA0qpxUqpHUqpHWVlZfZX2YXyOlnuVwhxbjp69Chr1qwB4K677mLWrFkUFBR02mfPnj0sWbKkz2pw6AFVpdQtwBTgwq4e11q/AbwBMGXKFH027/XelmP4Gg2Migk8m5cRQgiHawv3uXPnkpmZycaNG0/ZZ8KECUyYMKHParAn3AuAhA73423bOlFKzQOeAC7UWjc5pryu7TxWybJ9Rfzi4hTCB3n35VsJIdzQb/6zn4zCmrN6jdTYQH595eguH3vjjTfYtGkTmzdvZu/evVxxxRV8/fXXnfZZt24dq1ev5sEHH+Saa65BKcXYsWN55ZVXzqquNvaE+3YgRSk1GGuo3wDc1HEHpdRE4O/AAq11qUMq64bWmueWHSAywJufXjikL99KCCF6ZfHixQwZMoTnnnuOWbNmnRLsHe3evZs5c+bwzDPPoPVZDWh0ctpw11q3KqXuA1YCBuAtrfV+pdSzwA6t9VLgD8Ag4FPb0d1crfVVDquyg2X7itidW8UL147Dz8ulp+kLIZykux63M8yePZv169dz8803s2DBAm699VaHvK5d6ai1Xg4sP2nb0x1uz3NINXbw9/bkktQorp3c5xNyhBCiV4xGI2az2a59zWYzzz77LGAdh3dUuLvc2jIXjYjkH7dNweDhOnNVhRADy5gxY9i0aRPXX3/9affdtm0bs2bNYvr06cyb57h+soxrCCGEgwUFBbFhw4Ye95kzZw5z5swB6HI2zdmScBdCiD5WXV3NokWdzv3kq6++IigoqM/eU8JdCDGgaK37fQmCoKAg1q1bd8bPO5vZMy435i6EEL1lNBp7vRBXf2tbFbK3C51Jz10IMWCEh4dz9OhRZ5dht7b13HtDwl0IMWAEBwcTHBzs7DL6hQzLCCGEG5JwF0IIN6QcuZbBGb2xUmXAsTN8WjhQ3gflnAukba7HXdsF0rZzWZLWOuJ0Ozkt3HtDKbVDaz3F2XX0BWmb63HXdoG0zR3IsIwQQrghCXchhHBDrhbubzi7gD4kbXM97toukLa5PJcacxdCCGEfV+u5CyGEsIPLhLtSaoFSKlMplaWUeszZ9dhDKXVUKbVPKbVHKbXDti1UKbVKKXXY9n+IbbtSSr1ia99epdSkDq9zu23/w0qp253UlreUUqVKqfQO2xzWFqXUZNvXKsv23H5b2ambtj2jlCqwfXZ7lFILOzz2uK3OTKXUpR22d/k9qpQarJTaatv+sVLKq5/alaCUWquUylBK7VdK/cK23eU/tx7a5vKfm8Norc/5f1gv75cNDAG8gDQg1dl12VH3USD8pG0vAI/Zbj8GPG+7vRBYAShgBrDVtj0UyLH9H2K7HeKEtswGJgHpfdEWYJttX2V77mVObtszwMNd7Jtq+/7zBgbbvi8NPX2PAp8AN9hu/w34WT+1KwaYZLsdAByy1e/yn1sPbXP5z81R/1yl5z4NyNJa52itm4GPgEWnec65ahHwju32O8DVHbb/S1ttAYKVUjHApcAqrXWl1vo4sApY0N9Fa603AJUnbXZIW2yPBWqtt2jrT9K/OrxWn+umbd1ZBHyktW7SWh8BsrB+f3b5PWrryc4FPrM9v+PXqU9prYu01rtst2uBA0AcbvC59dC27rjM5+YorhLucUBeh/v59PxBnis08K1SaqdSarFtW5TWush2uxiIst3uro3nctsd1ZY42+2Ttzvbfbbhibfahi4487aFAVVa69aTtvcrpVQyMBHYipt9bie1DdzoczsbrhLurmqW1noScBlwr1JqdscHbb0dt5iu5E5tsXkdGApMAIqAF51bTu8ppQYBnwMPaK1rOj7m6p9bF21zm8/tbLlKuBcACR3ux9u2ndO01gW2/0uBL7D+CVhi+3MW2/+ltt27a+O53HZHtaXAdvvk7U6jtS7RWpu11hbgH1g/OzjztlVgHd7wPGl7v1BKGbGG3/ta63/bNrvF59ZV29zlc3MEVwn37UCK7ei1F3ADsNTJNfVIKeWvlApouw3MB9Kx1t022+B24Cvb7aXAbbYZCzOAatufziuB+UqpENufmPNt284FDmmL7bEapdQM21jnbR1eyynaws/mB1g/O7C27QallLdSajCQgvWgYpffo7ae8VrgOtvzO36d+roNClgCHNBa/6nDQy7/uXXXNnf43BzG2Ud07f2H9Uj+IaxHtp9wdj121DsE65H3NGB/W81Yx/L+CxwGVgOhtu0KeM3Wvn3AlA6v9ROsB4CygDuc1J4Psf6Z24J1/PFOR7YFmIL1BzEbeBXbCXZObNu7ttr3Yg2GmA77P2GrM5MOs0O6+x61fS9ss7X5U8C7n9o1C+uQy15gj+3fQnf43Hpom8t/bo76J2eoCiGEG3KVYRkhhBBnQMJdCCHckIS7EEK4IQl3IYRwQxLuQgjhhiTchRDCDUm4CyGEG5JwF0IIN/T/AdVEykwekTnsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8VNXdx/HPb2aykH0lIRsJ+75GFgFF3JAqWLeCtViX8tSqT2trW7Wt0lZbbW2farVaXKp1w11UVBQB2ZewLyEQQla2kIQkkD05zx8ziVnJAAmTmfzerxcvJnfuzPzuLN8599wz54oxBqWUUp7F4uoClFJKdTwNd6WU8kAa7kop5YE03JVSygNpuCullAfScFdKKQ+k4a6UUh5Iw10ppTyQhrtSSnkgm6seOCIiwiQmJrrq4ZVSyi1t3rz5uDEmsr31XBbuiYmJpKSkuOrhlVLKLYlIljPrabeMUkp5IA13pZTyQBruSinlgTTclVLKA2m4K6WUB2o33EXkZRE5JiK72rheRORpEUkXkR0iMqbjy1RKKXUmnGm5vwJMP831VwH9Hf/mAc+de1lKKaXORbvhboxZCRSeZpVZwH+N3XogRER6dVSBzWUeP8UTX+ylrk5PD6iUUm3piD73WCCn0d+5jmUtiMg8EUkRkZT8/PyzerAv9xzhuRUH+MOne9DzvyqlVOvO6y9UjTELgAUAycnJZ5XMP5rSh6Mllby0+iBBvjZ+fsXADq1RKaU8QUe03POA+EZ/xzmWdQoR4bffGcxNyXE8vSydvUdKOuuhlFLKbXVEuH8MzHWMmpkAFBtjDnfA/bZJRLhhrP375HhpVWc+lFJKuaV2u2VE5C1gKhAhIrnAI4AXgDHmeeAzYAaQDpQBt3VWsY35etm/lyqqa8/HwymllFtpN9yNMXPaud4Ad3dYRU7y9bICUFGj4a6UUs257S9UfW2OcK+uc3ElSinV9bhvuGu3jFJKtcltw92nvltGw10ppVpw23Cvb7lX1mi3jFJKNee24e5ttSCiLXellGqN24a7iOBrs2q4K6VUK9w23MHeNaOjZZRSqiU3D3dtuSulVGvcP9z1gKpSSrXg1uHuY7Noy10ppVrh1uGu3TJKKdU6Nw93C5V6QFUppVpw83C36sRhSinVCvcOdx3nrpRSrXLvcNdx7kop1So3D3dtuSulVGs03JVSygO5dbj7eFn0R0xKKdUKtw53X5uVqpo66uqMq0tRSqkuxb3D3XHCDp3TXSmlmnLzcNdT7SmlVGvcPNwdp9rTHzIppVQTbh7u9S137ZZRSqnG3DvcbXqSbKWUao17h7uXhrtSSrXGrcPdR7tllFKqVW4d7npAVSmlWufe4e7oc6/UbhmllGrCvcNdu2WUUqpVbh7uekBVKaVa4xHhXq7hrpRSTbh5uGu3jFJKtca9w11/xKSUUq1y63C3WARvq0WHQiqlVDNuHe5g/yFTpXbLKKVUE24f7nqqPaWUasmpcBeR6SKSJiLpIvJAK9cniMhyEdkqIjtEZEbHl9o6Xy+LhrtSSjXTbriLiBV4FrgKGALMEZEhzVb7LfCOMWY0MBv4V0cX2hZfm1VHyyilVDPOtNzHAenGmAxjTBWwEJjVbB0DBDkuBwOHOq7E0/P1suoBVaWUasbmxDqxQE6jv3OB8c3WmQ98KSL3Av7AZR1SnRO0W0YppVrqqAOqc4BXjDFxwAzgNRFpcd8iMk9EUkQkJT8/v0Me2H5AVbtllFKqMWfCPQ+Ib/R3nGNZY3cA7wAYY9YBvkBE8zsyxiwwxiQbY5IjIyPPruJmfGw6WkYppZpzJtw3Af1FJElEvLEfMP242TrZwKUAIjIYe7h3TNO8Hb5eFiprtOWulFKNtRvuxpga4B5gCZCKfVTMbhH5g4jMdKz2C+BHIrIdeAv4oTHGdFbRjek4d6WUasmZA6oYYz4DPmu27OFGl/cAkzq2NOfoAVWllGrJ/X+hquPclVKqBfcPd8c49/PUC6SUUm7BA8LdgjFQVautd6WUqucB4V4/p7uGu1JK1XP7cPdxhHulHlRVSqkGbh/uvjY91Z5SSjXn/uFe3y2jk4cppVQDzwl37ZZRSqkGHhDu2i2jlFLNeUC4a8tdKaWac/9wt2m4K6VUc+4f7vXdMjozpFJKNfCAcNeWu1JKNecx4V5epeGulFL13D7cw/y98bIKh4srXF2KUkp1GW4f7laLEBfqR3bhKVeXopRSXYbbhztAQpgfWQVlri5DKaW6DI8I997hfmQXlOmc7kop5eAh4e5PaWUNRWXVri5FKaW6BM8I9zA/ALIKtN9dKaXAU8I93B7u2YXa766UUuAh4R7f0HLXcFdKKfCQcPf1shId5KvhrpRSDh4R7gAJ4X7a566UUg4eE+69w/zI0j53pZQCPCncw/3IL62krKrG1aUopZTLeUy4J4T7AzpiRimlwIPCvbeOmFFKqQaeE+71Y9013JVSynPCPcTPmyBfG1k6O6RSSnlOuIN9jhntllFKKQ8L94RwPz2gqpRSeFi4J4b7kVdUTk2tnixbKdW9eVS49w7zp6bOcOiEnnJPKdW9eVS4JzhGzOhBVaVUd+dR4V4/HFIPqiqlujuPCveoQF+8bRY9qKqU6vacCncRmS4iaSKSLiIPtLHOTSKyR0R2i8ibHVumcywWcZwsW7tllFLdm629FUTECjwLXA7kAptE5GNjzJ5G6/QHHgQmGWOKRKRnZxXcnt5hftoto5Tq9pxpuY8D0o0xGcaYKmAhMKvZOj8CnjXGFAEYY451bJnOqx/rboxxVQlKKeVyzoR7LJDT6O9cx7LGBgADRGSNiKwXkekdVeCZ6h3mR1lVLfknK11VglJKuVxHHVC1Af2BqcAc4AURCWm+kojME5EUEUnJz8/voIduqnf91L/aNaOU6sacCfc8IL7R33GOZY3lAh8bY6qNMQeBfdjDvgljzAJjTLIxJjkyMvJsaz6tBB0OqZRSToX7JqC/iCSJiDcwG/i42TofYW+1IyIR2LtpMjqwTqfFhfZABD3lnlKqW2s33I0xNcA9wBIgFXjHGLNbRP4gIjMdqy0BCkRkD7Ac+KUxpqCzij4dH5uVmOAeZOtwSKVUN9buUEgAY8xnwGfNlj3c6LIBfu7453IJerJspVQ351G/UK2XGOGnB1SVUt2aR4Z7Qpg/BaeqOFlZ4+pSlFLKJTwy3L+dQEz73ZVS3ZNHhntCmJ4sWynVvXlkuDe03PWgqlKqm/LIcA/09SLM31t/yKSU6rY8MtzB3jWTrWdkUkp1Ux4b7r3DdepfpVT35bnhHubHoRPlVNXUuboUpZQ67zw23BPC/akzkHei3NWlKKXUeeex4a5j3ZVS3Znnhnv9WHcdDqmU6oY8NtwjA33o4WUl83gZB/JP8qv3tlNWpdMRKKW6B48NdxGhd7gfu/KKueOVTbyTksuGg4WuLksppc4Ljw13sI9135hZ2HBQdc+hEhdXpJRS54dHh3tShP18qo9eO4yEMD92Hyp2cUVKKXV+OHWyDnd126QkRieEMn1YNCvS8tmVpy13pVT34NEt9+hgX6YPiwZgaEwQ2YVllFRUu7gqpZTqfB4d7o0NjQ0GtN9dKdU9dJ9wjwkCYLeGu1KqG+g24d4z0JfIQB89qKqU6ha6TbiDvfW+Ww+qKqW6gW4V7sNigknPP0lFda2rS1FKqU7VrcJ9aEwQtXWGtCOlri5FKaU6VTcLd/uIGT2oqpTydN0q3OPDehDoa2OXHlRVSnm4bhXuImI/qKotd6WUh+tW4Q72rpm9h0uoqdXT7ymlPFc3DPcgKmvqyDiuZ2hSSnmubhju9oOqu/K0310p5bm6Xbj3jfTHx2bRfnellEfrduFus1oY1CtIpyFQSnm0bhfuYO9333OoBGOMq0tRSqlO0W3DvaSihtyicleXopRSnaJbhvswPaiqlPJw3TLcB0YHYrUIn+44TG2dds0opTxPtwx3Xy8rd1/Sj8U7D3PX65t1lkillMdxKtxFZLqIpIlIuog8cJr1rhcRIyLJHVdi5/j55QOYf80Qvko9yoMf7HR1OUop1aFs7a0gIlbgWeByIBfYJCIfG2P2NFsvEPgpsKEzCu0MP5yUxM68EpbtPYoxBhFxdUlKKdUhnGm5jwPSjTEZxpgqYCEwq5X1/gg8AVR0YH2dbnRCCEVl1TpyRinlUZwJ91ggp9HfuY5lDURkDBBvjFncgbWdFyPjQgDYnnvCxZUopVTHOecDqiJiAf4O/MKJdeeJSIqIpOTn55/rQ3eIgdGBeFst7MjVYZFKKc/hTLjnAfGN/o5zLKsXCAwDVohIJjAB+Li1g6rGmAXGmGRjTHJkZOTZV92BvG0WBscEsT1HW+5KKc/hTLhvAvqLSJKIeAOzgY/rrzTGFBtjIowxicaYRGA9MNMYk9IpFXeCkXHB7Mor1jHvSimP0W64G2NqgHuAJUAq8I4xZreI/EFEZnZ2gefDiLgQTlXVkpF/0tWlKKVUh2h3KCSAMeYz4LNmyx5uY92p517W+TUq3j4dwbacE/SPCnRxNUopde665S9Um+sTEUCAj00PqiqlPIaGO2CxCMNig9ihwyGVUh5Cw91hTEIouw+VUFxe7epSlFLqnGm4O1w6OIqaOsOKtGOuLkUppc6ZhrvD6PgQegb6sGT3EVeXopRS50zD3cFiES4fEsWKtHydAlgp5fY03Bu5cmg0ZVW1rN5/3NWlKKXUOdFwb2RCn3ACfW3aNaOUcnsa7o142yxcOqgnS1OPUlNb5+pylFLqrGm4N3P5kGiKyqrZqhOJKaXcmIZ7M5P7R2C1CCv3dY0piZVS6mxouDcT3MOL0fEhfKPhrpRyYxrurbh4QCQ784opOFnp6lKUUuqsaLi34qIBkRgDq9N1SKRSyj1puLdieGwwYf7efJOmXTNKKfek4d4Ki0WY0j+ClfuPU6dnZ1JKuSEN9zZcPCCS4ycr2XO4xNWlKKXUGdNwb8PUgT3xtlp4b3Ouq0tRSqkzpuHehjB/b2YMj+b9zbmcqqxxdTlKKXVGNNxP4wcTEymtrOGjbXmuLkUppc6IhvtpjEkIYWhMEK+ty8IYPbCqlHIfGu6nISLMndibvUdK2ZRZ5OpylFLKaRru7Zg5MhY/byuLdxxydSlKKeU0Dfd29PC2MjohhJQsbbkrpdyHhrsTxvYOI/VwCSd11IxSyk1ouDshuXcodQa2Zesc70op96Dh7oTRCSFYBFKyCttcR0fTKKW6Eg13JwT6ejEwOoiUNkbMbMkuYswfvyLtSOl5rkwppVqn4e6k5N6hbM0uoqa2jk2Zhby5IRtjDLV1ht9+uIuismpW7ddZJJVSXYPN1QW4i+TEUF5bn8WibYd4eNEuTlXVklNURnSQL3sOl+BlFbbpeVeVUl2EhruTxvYOBeD+97YTGeDDlUOjeW7FAWwWYXK/CIJ62Nieq+GulOoatFvGSbEhPYgO8sXbauGFuck8eeNI5oyLx9tmYf7MoYyKDyGnsLzh1Hwfbc1jU2bbB2CVUqozacvdSSLC49cPx8dmZWR8CAB/vm4ED189lB7e1oZQ3557gjEJofzqvR1c2C+cV24b58qylVLdlIb7GZg6sGeLZT28rQAMiw3GIrAtp5hDJyqoqq3jQP7J812iUkoBGu4dxt/HxoCoQLbnnKCkohqA3KJyKqpr8fWyurg61ZgxBhFxdRlKdSrtc+9AI+NC2HCwgK3ZJxgRF4wxcPD4KVeXpRopr6pl+j9W8fTX+11dilKdSsO9A41KCKGiug6LwM8vHwCgXTNdzKvrMkk7Wsozy9LJLSpzdTlKdRqnwl1EpotImoiki8gDrVz/cxHZIyI7RORrEend8aV2fSPj7AdaJ/ePZEKfcEQg/ZiGe1dRXFbNv5anMyYhBAT+/tU+V5ekVKdpN9xFxAo8C1wFDAHmiMiQZqttBZKNMSOA94C/dHSh7mBAVABXDYvmJ1P74utlJS60BwfytVumq/j3ygOUVNTwx2uHcduFiXy4NY+9R0pcXZbyABXVtV1ufilnWu7jgHRjTIYxpgpYCMxqvIIxZrkxpn4fdz0Q17Flugeb1cJzt4xlQp9wAPpGBnDgLFru72/O5Ybn1lJX17XeLO6suKya/6zJZObIGIbGBHPX1L4E+tj425ee03rfmVvML9/dztGSCleXAkBZ1blPkf3aukx+9d72c7qPYyUVPLZ4D4dOlJ9zPa3JKSzjgkeX8vr6rE65/7PlTLjHAjmN/s51LGvLHcDn51KUp+gbGUDG8ZNnHNIfbz9ESlYRuw4Vd1Jl3c/q9OOUV9dy64WJAIT4eXPrhYksTT1KTqF7971XVNfyxBd7ufZfa3h3cy5vbsh2dUmkZBYy6g9fnXPgvb4+m3dSctl48Ox+ELhqfz4znl7FC6sO8q8V6edUS1seW5xKaWUNb23MaX/l86hDD6iKyC1AMvDXNq6fJyIpIpKSn+/5k2z1jQygorqOQ8XOtxhq6wxbHGd9WpHm+c/R+bI6/TiBPjZGxgU3LLt5fAIWkQ5vcX2zL79Fd09n7bJvzipkxtOreG7FAW4YE8fohBAW7zzc7u3q6gzL9x7rlL3D2jrDw4t2U1VTx2OLU8kqcL5r8s0N2Q1ftgUnK0k7ap9p9ZnlZx7MGzIKmPvyRkL9vJncL4JFWw9RXlXr9DbMWbCeSY8vY9rfVvDiqoxW11u5L58vdh+hf88A9hwuYd/RpjPDfrAllxHzl/CDlzbw8uqD1NTWnfF2nC1nwj0PiG/0d5xjWRMichnwG2CmMaaytTsyxiwwxiQbY5IjIyPPpl630jfSH+CM+t3TjpRSWlmDRewh4SxjTMOvZNuTW1TGa+uzulwf4bn4+5dpfH6aUFuTfpwJfcOxWb99y/cK7sGVQ6NYuCnH6Q99vfRjJ3lhZQa1zcJx96Fi7nhlEw9+sLNh2evrs7jgsa8pLqs+o8dozxsbsrjh+XVUVtfx39vH8cQNI/ju6FjSj51sCJlTlTVU1bQMlMU7D3PbK5tYtL3FR5nKmtozfj4aW7gpmz2HS/jNjMHYrMIv39tBeVUtew6VcOw0XUaHi8t56MOdPOUYprrB0Vq/fEgUK/fls/0MJ+Z7e1MOgT42Prp7EvdM60dpZQ2fNXqP1NYZXlufxQPv7+DL3UeorPl2m7flnGBdRgF9HJ/h51YcaPFaV1TXMv+T3SSG+/HK7eOwWoSPtjZ9Pt/ckI23zcqR4gr+8Oke3knJPaNtOBfOhPsmoL+IJImINzAb+LjxCiIyGvg39mA/1vFluqe+PQMAzqjfvf6EINeOjmVrdhEnyqqcut1Lqw9ywWNLWbb36GnXq6iu5c5XU/jdR7tIPXz+559/e1M2H28/s5ONL9yYzSOLdrV5/ZHiCp5els6ji1NbbRllF5SRXVjG5H4RLa6bOzGR4vJqPjmDmnbmFnPj82t57LNU/rPmYMPyqpo67n93BzV1hq3ZJ8guKMMYw6trMzl+spLX1mc6/RjtOVZSwZ8Wp3Jh33CW3HcRFw2wN5amD4tGBBbvOExxWTVX/N9Kvv/i+hbB9KnjhO+tdSX84p3tzH5h/VnVdaKsiieXpDE+KYw7pyTxu6uHsPFgIUMe+YIZT6/iwseXcd/b29h/tOV7b3uOvRtyiSNo12cU4Odt5S/XjyDI18bfvtpHRv5Jp1q/5VW1LNl9hBnDe+HvY2N8UhhJEf4s3GTvstqSXcTMZ1bzu492sWjbIea9tpkpTyznWKn9y2dp6lFsFuGZm8fwi8sHUnCqipRGc0XlFJZxw/Nrycg/xSMzhxIb0sO+d7DtUMPe0JHiClKyirh1Ym++vO8iRsaH8OzydKrPU+u93XA3xtQA9wBLgFTgHWPMbhH5g4jMdKz2VyAAeFdEtonIx23cXbcS7u9NiJ8Xu/KKeXFVBg+8v6PdHzVtyiwiOsiX749PoM7YuxPac7KyhmeXp1Nn4GcLt5Fd0HYf8qOL97D3SCki8NWe038RdLSiU1U8vGg38z/e3aSV1J4XVx/k1XVZbU7EtmT3EQDyTpSzNLXlNtU/h5P7twz38UlhDIwK5Kmv9/Piqgwy2vldwtbsIm5+YT1+3jYm9gnnyS/TGrodnl2eTurhEh65xj6Y7JMdh9iRW8z+YycJ9LHxnzWZVFSffYu4sce/2Et1reGxa4cT4PPtD817BvoyLjGMxTsP8+CHOzhUXM6mzCJeWZvZsM7JyhqWp+UT3MOLjQcLm2zzibIqluw+wvacE04diyitqKa43L5HUl1bxz1vbqW0oob5M4ciItw4No5fXjmQe6f15+k5o5k7MZElu49w3XNrKTrVtOGywzGramlFDSv3HWfdgQKSE8MI9ffmfy7uy8p9+Uz72zcMecTezfHiqowmexjGmIa90a/3HuVUVS0zR8UA9rmhvndBPJsyi7j7zS1c96+1HD9ZyTM3j2bH/CtY8IOxHCut5K0N9i+7pXuOMr5PGME9vJg6MBJvm4UvHO+znbnFXPPMarIKynhxbjKXOKYluXZ0DHknytmcbe9W/XyXfS/hquG9EBF+emk/8k6U8+GWlntLnUFctWuenJxsUlJSXPLY59P1z61ls6MP3dtqAYG7Lu7LHVOSCPL1ori8mrc2ZnNBYihjEkKZ+OdlJCeG8tTs0Yz541dcPiSKJ28cedrHeGbZfp78ch/P3Dya33y4i5iQHrx/10T8vJvOLvHZzsP85I0tzLuoD5uziqisqeXTe6c0WWftgeN8sv0QqYdLGRoTxGPfHX7G27x6/3EW7zzEI9cMbTL1wvPfHODxz/cC8OzNY/jOiF7t3tehE+Vc+PgyACb3i+D1O8e3WGf2gnXkl1ZSUV1HfFgPFs6b2OT6u9/YwpbsItY+MK3VaQfWpB/n4UW7OJB/ChH41ZWD+PHFfVpd96bn15FbVMb7P7kQgCv+vpK+PQPw97GyJr2AWaNieGr2aG54bi0lFdWMTwrnnZQcnrl5DD/6bwp/nDWUH0xMbHe7m6upreNoaSW1tYbMglPMfXkjP764Lw9cNajFuv9dl8nDi3YD8Ovpg9iUWci6AwUs+dlFJIT7sWhbHj9duI1/fX8M9761lTsnJ/HgjMGAvRvhoQ/tXUqPXDOE2yYlUVtn2HiwkPFJYVgs3z4nmcdP8f0XN1BSXs19lw8g7Ugpb6fk8NcbRnBjcnyLuuqlHSll+lMr+dGUPjzkeFyAW17cwPGTlRwtqWBITBBr0gv49fRB3DW1L8aYhi/KPYdKWLU/n/3HTnJTchx/uWEkxhh+unAb2YVlvHLbBfzqvR1szz3B2gcuxeqoOetwPmkZ2VhECPC1Eehrw9LoNT5+spLqWkNEgDdHSyoJ6eFFgK/9M1TguC4qyIf80kpqDUQGeDfp5qszhiPFFfjYLIQH2NerM4aoIN+GdY6VVFBnICrIx6kpMHx9fYmLi8PLy6thmYhsNsYkt3dbnVumk10/Jo4wf2/mXdSH3mF+/HFxKk99vZ8XVmVw6WB7X2JxeTWhfl68eGsyR0oquCAxDKtFmNI/gm/25Z92LpTi8moWrMzgssE9uXpEDP7eNm5/dRM/fHkTL/0wmUBf+5ui6FQVv/toFyPjgvnllQN5afVBHv98L4dOlBMT0sN+X2XV3P7KJrysFiIDfHhjQza3XpjIgKhACk9V8dLqDCb2iWBi3/CGD0xjdXWGfy5L5x9f78MYuGJINJcMsrdqausMr63LYlxSGHlF5SzclO1UuK90HHe4KTmOd1Jy2ZRZyAWJYQ3XHz9ZycaDhdxzST/8fWz8+fO9pB4uYXCvoIaa1hw4zmWDo9p8Dif1i+DrX0wlt6iMxz/fyxNf7CXtSAm/nzWM4B7ffqhyCsvYmFnIL68cSK9g+3P20HcG8+AHO4kK8uHX0wfxQ8donFmjYvjdot1kFZQxfVg0lw3uyaj4EBasymDOuIQmoXA6m7OKePCDHRzIP9Wka6VnoA/3TOvX6m2mD4vm95/sYUKfMP7noj5cOzqGK/6+kvve2cYrt13ApzsOEx3ky/Sh0Vw6qCfvb8nlF1cMxNtmYdG2PPpE+mMRYWnqUW6blMSLqzL48+d7+d3VQ7hjchJgD+hbXtpAbZ1hZHwIf/h0DwD3XNLvtMEOMDA6kOtGx/HK2kxum5RIr+AejvA+wXdG2Fvab220d59M6GN/rUWEkfEh9hlZx9rv59FP9/DSmoPMnZhIVkFZQ3ff91/cwP6jJ5k7sXeT92n1qWLGDhuIv1+PVud7KimvJrPgFP4+NqJCahgYHYiPzb5e4akqcovKCPLzpiaoioQwP0L8vFvcR2RpBUeKKwjy96E6sJKoIN8m4R5TXk1WwSl6hfoR6t/y9o0ZYygoKCA3N5ekpKTTrtsanX6gk908PoEX5iZzQWIYPYN8+eec0Xx672RmDO/F0j1HGZMQwvO3jKGypo7bX7HvySQn2k8McvGASPJLK9mV9+3Ii7q6b3c9swpOcd/b2yipqOE+x3QHlwzqyVOzR7M5u6ihJQTw2GepFJdX8/j1I/CyWrhscBRAk26MdzfnUFFdx9vzJvLeXRfiY7PwnzWZgL0759nlB7jlpQ1MenwZu1sZpvno4lT+b+k+Zo6MwcsqrD9Y0HDd16lHyTtRzu2TErkxOY7V6ced2u1fuT+f6CBf5s8cSkSAN/9Y2nRc+pe7j1JnYPqwXnzvgnh8vSy82qgLYvehEk6UVbfa395cXKgf/5wzml9eOZBF2w9x4Z+/5k+fpTYc96g/WDbLsasPMPuCeD69dzKrfjWNu6b2bZgldMbwXlgtQmVNHTeOjUdE+MnUvuQUlvNyo376eh9syeXOVzfx87e38fjne1my+whvbMhizoL1VFTXcdfFffnzdcN58saR/Om7w3n3xxObdMc01jPQl/fvupDnbxmLxSL0Cu7B49ePYFvOCW7693q+SctnxvBeWCzC7HHxHD9ZxZsbsjh0opwNBwu5dlQslw2OYkNGIcdKKliwMgMReHJJGtkFZezIPcH3FqzDIvD2vAm8dsc4nr9lLL+ePqhh2o32/Oyy/hhjGub4ySwoo6SihpFxwVzj+NL397YyLDa4zfu499L+hPl589uPdjH/k90Mjw3mhbnJ7DvxwZoqAAAPHklEQVRaSlVtXUOXTL3q6mrCgvzbnMgv0NeGt9XCqcoafL2sDcEOEORrQxAKy6oI8LE1+dJvLDLAh1A/bwpO2T93zdcL8rUREeCDr3f7kwmKCOHh4VRUnN3vFrTl7gLDYoN58saRTbpbTlbWcv+72wnwsTEo2t7qvHRwFBax9ykPjwumqqaOK/7vGwpOVhEf5sf+Y6XYLBYemjGIoTHffghmjoyhh5eVu9/YwiV/XcHMUTG8tzmXn0zt29Ci7dczgD4R/ny15yhzJyZS5xg5kNw7lCEx9nW+OzqWD7bkctngnnywJY87JicxJiGUhz7cyb+WH+DZ749peMwt2UX8Z+1Bvj8+gUevHUZeUTkbMr7tI391XSYxwb5cNjiK4XEhPPX1ft7dnHvaMKiprWP1/uNMHxaNn7eN/7moL499lsq2nBOMcsyp//muw/QO92Nwr0BEhFkjY/l4+yF+d/UQ/H1sfLrjEDbHXpAzRIS7L+nHJQN78vw3B3hxVQZbs4t480cT+GBrHuOTwogL9WuyfmsBFB7gwyUDI9l7pJSJfe0/art8SBRXDo3iySX7uGhAZMPrnHq4hF+9t4PIQB8sIhwrraC61v4FPqlfOM/ePKbVVuLp1D8/9b4zohcBvjbuen0zVbV1DXtNFw/oyaR+4cz/ZA8fNPryOn6ykue/OcDdb26h4FQV/5wzmoc+2Mldb2wmq6CMED8v3rxzAgnh9udi+rDoM6ovPsyP74/vzWvrs7hzSh925tobCyPiQhgYHUjPQB+GxQbjdZo9nOAeXtx/5UAe/GAnVovwnx9ewDBHwG88WMjwVl6X03WFiAhh/t4cKakgyLdpNNqsFvx9rJyqrCUmpEeb9yMixIb2oLq2DmNo8UUiIg17ys44l9lLNdy7iOvHxLLnUAlWCw27kmH+3oxPCueL3Ue4/8qBLNt7lMyCMmYMj+ZUZS3JiaHcc0k/ejba7at3+ZAoFv/vZJ74Io03NmTTO9yP/720f4t1Xl5zkOMnK9mVV0xWQVmTsL1tUhILN+Xw49c3ExXkw88vH4C/j43NWUW8tj6TgpOVhAf4UFVTx4Pv7yQ6yJcHZwxGRBjfJ4znv8ngVGUN+aWVrEkv4P4rBmCzWogN6cFF/SN5dW0mFw+IYGzvsOblA7A9t5iSipqGkSBzxifw9DJ7l9azN48hp7CMdQcKuGNKUsOH4KYL4ng7JYfFOw9z3ehYPtiaxyWDehIe4HNGr8eQmCCenjOaSwf35KcLt/Gj/6Zw8PgpfnxxH6fv4283jaKyurbh9RQR/vTd4Vz5j1X8bOE2Prp7El5WCw98sJPgHl589r9TCPX3pqK6ll15xRwrreTyIVGnDbgzcfGASN798UTWHShgtCP87aE4jt98uJN3N+cyKj6E3uH+xIX6EebvzabMIib2CeeakTGUVtTw0Ic76RPpzxt3jm/omjpb90zrxzspOfz9y330DPLB18vCgKgArBbhrXkT2twzaeym5HiW7z3G2N6hDV+yUwf2bPXcC84I8/emrKq21S6TmJAeVNXUtTuFt0WEpAj/s3r8jqTh3kWICA9f03zKHrhqeDQPL9pN+rFS3t6UQ3SQL0/PHu1Un23/qEBevDWZHbknCPXzbvGmvGZkDC+syuCyv39DuL83EQE+XDXs237wgdGBTO4Xwer04zw0YzD+jg/bnHHxvLzmIO9vyWXeRX3514p00o6W8tKtyQ0fyPFJ4Ty7/ACbs4rYcLAAi8ANY7/ti50/cyi3/WcjcxZs4E/XDeeGsfYZK4wxrDtQgJfNwtLUo1iEhi6VAB+bvZtrZQY5hWX85qNd+NgszG10gHJMQih9Iv15NyWHiABv8ksruXHs2c+GMWtULCmZRby2Pgsfm4Wrhrd/nKBecA8vaLZbHh7gwxPXD+eOV1O4+K/LGRUfwvacEzw1e1RDoPh6WUlObP0L71wNjQluspcH4G2z8JcbRjCpXwT9HMN3rRZh2qCevLc5t6FRMGdcPCF+XkzoE05YO/3FzogI8OHOKX14+uv9RAX5MCwmuOF93TcywKn7sFqEBXPbPbboNJvVQmIbwezrZXXq3AyZmZlkZGQwbdo07rzzTvbu3cvbb79NbOy3P+zftm0bmzdv5o477mhyu/nz5/PKK6+c83aAhnuXd8UQe7i/sjaTb/bl85Op/Zw+GFdvRFxIq8uHxQbzyb2Tefzzvazaf5yfXtofb1vT+/7NdwazbO8xZo78tv+yf1Qgyb1DWbgxhzB/H/6xdD/fHR3LpY5+fLCfUNxqEdYeKODDrblMHdiT6OBv9zCSIvz56O5J3P3mFu5/dzvf7Mvnvsv685cv0hqGnIG9e6Fxl8RtFybx0qqD3PlqCmlHS/m9Y4xxPRHhpuR4Hv98L5VL9xPu791wUPds/fbqwWQWnKJ/z0CCfFvvaz0Tlw6O4tXbx/HS6oMs2X2UqQMjmzy/riAiXDu66awi907rx+iEkCYHNWecwZebM340JYnX1mVytKSS7wx37XPQUTIzM1m2bBnTpk0jLS2N1atXt1hn1KhRjBo1qlPr0HDv4qKDfRmdEMLr6+2jB25qZyTCmRoaE8xrd4xn39FS+rTSYhncK6ihn76x2eMSuP/d7dz/7nYm94vg8eubDpn097ExPDaY19dncbKyht/PbFl3iJ83r942jue/OcBTX+/nk+2H8LIKD1w1iP49A9iVV8KkfuFNbhMd7MvMkTF8sDWPMQkh3DKh5ezS142O5a9L0tiRW8ztk5LOuVvDx2bltTvGd+gvei8eEMnFAyI5XFxOSA/vLnlmqN7h/vQO79zuhUBfL+6+pB+PLk5lZHzbB0870u8/2c2eQ+c2G+iQmCAeuWZoq9ctWLCANWvWsG7dOnbs2MHVV1/Np59+2mSdFStWsHTpUh599FEefvhhli9fzpAhLffcz4WGuxuYPjSardknuLBveMMBrI42ICrwjNb/zvBe/PmzVOJCe/DvH4xtMrKg3vg+YWzLOUG4vzfT2mg926wW7pnWn8uGRPGf1Zn8YGLvhr7TxnsCjd09rR+Hiyv447VDWx2S2TPIl6kDIvl67zFuTO64CUo7I4DPtd/aE8ydmEiAj+2MD8p2VfPmzaNPnz48+uijTJ48uUWwN3b48GE2btzIqlWrePPNN/nyyy87rA4NdzcwY3gv/rF0f8MY6q6gh7eVL++7iEBfrxZdOfUmJIXz728yuG5MbJvr1BsUHcQTN4xw6rH7Rgbw1rwJp13nV9MHcWG/iFb3OlTX4m2zMHtcwnl7vLZa3K6QlZXFiBH29/3YsWM13Lub+DA/dsy/osNGTXSU9kagXNgvnDsnJ3HnFOdHmHSUgdGBDIw+s70RpTqCl5cXtbXOTTPRu3dvdu60/yJ469atHVpH10oL1aauFuzO8LFZ+e3VQ5r8Qk8pTzds2DDWrFnD9773vXbX7dWrF2PHjmXKlCmtHng9Fzq3jFKqW0hNTWXw4MHtr9jFNK9b55ZRSqkuoLi4mFmzmpyZlEWLFhEc3LmjgzTclVLdxukm4esswcHBrFix4qxuey49K+7XkauUUmfBy8vrrCfhcoX6WSF9fc/umJW23JVS3UJERASZmZmuLuOM1M/nfjY03JVS3UJISAghIa1PxeGJtFtGKaU8kIa7Ukp5IJeNcxeRfCDrDG8WAbR/xmj3pNvmfjx1u0C3rSvrbYyJbG8ll4X72RCRFGcG77sj3Tb346nbBbptnkC7ZZRSygNpuCullAdyt3Bf4OoCOpFum/vx1O0C3Ta351Z97koppZzjbi13pZRSTnCbcBeR6SKSJiLpIvKAq+txhohkishOEdkmIimOZWEi8pWI7Hf8H+pYLiLytGP7dojImEb3c6tj/f0icquLtuVlETkmIrsaLeuwbRGRsY7nKt1x2/M2u1Mb2zZfRPIcr902EZnR6LoHHXWmiciVjZa3+h4VkSQR2eBY/raIfHvG787drngRWS4ie0Rkt4j81LHc7V+302yb279uHcYY0+X/AVbgANAH8Aa2A0NcXZcTdWcCEc2W/QV4wHH5AeAJx+UZwOeAABOADY7lYUCG4/9Qx+VQF2zLRcAYYFdnbAuw0bGuOG57lYu3bT5wfyvrDnG8/3yAJMf70nq69yjwDjDbcfl54K7ztF29gDGOy4HAPkf9bv+6nWbb3P5166h/7tJyHwekG2MyjDFVwEJgVju36apmAa86Lr8KXNto+X+N3XogRER6AVcCXxljCo0xRcBXwPTzXbQxZiVQ2Gxxh2yL47ogY8x6Y/8k/bfRfXW6NratLbOAhcaYSmPMQSAd+/uz1feooyU7DXjPcfvGz1OnMsYcNsZscVwuBVKBWDzgdTvNtrXFbV63juIu4R4L5DT6O5fTv5BdhQG+FJHNIjLPsSzKGHPYcfkIEOW43NY2duVt76htiXVcbr7c1e5xdE+8XN91wZlvWzhwwhhT02z5eSUiicBoYAMe9ro12zbwoNftXLhLuLurycaYMcBVwN0iclHjKx2tHY8YruRJ2+LwHNAXGAUcBv7m2nLOnogEAO8DPzPGlDS+zt1ft1a2zWNet3PlLuGeB8Q3+jvOsaxLM8bkOf4/BnyIfRfwqGN3Fsf/xxyrt7WNXXnbO2pb8hyXmy93GWPMUWNMrTGmDngB+2sHZ75tBdi7N2zNlp8XIuKFPfzeMMZ84FjsEa9ba9vmKa9bR3CXcN8E9HccvfYGZgMfu7im0xIRfxEJrL8MXAHswl53/WiDW4FFjssfA3MdIxYmAMWOXeclwBUiEurYxbzCsawr6JBtcVxXIiITHH2dcxvdl0vUh5/Dd7G/dmDfttki4iMiSUB/7AcVW32POlrGy4EbHLdv/Dx19jYI8BKQaoz5e6Or3P51a2vbPOF16zCuPqLr7D/sR/L3YT+y/RtX1+NEvX2wH3nfDuyurxl7X97XwH5gKRDmWC7As47t2wkkN7qv27EfAEoHbnPR9ryFfTe3Gnv/4x0duS1AMvYP4gHgGRw/sHPhtr3mqH0H9mDo1Wj93zjqTKPR6JC23qOO98JGxza/C/icp+2ajL3LZQewzfFvhie8bqfZNrd/3Trqn/5CVSmlPJC7dMsopZQ6AxruSinlgTTclVLKA2m4K6WUB9JwV0opD6ThrpRSHkjDXSmlPJCGu1JKeaD/B0pDYhbpuusWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "class BGAN():\n",
    "    \"\"\"Reference: https://wiseodd.github.io/techblog/2017/03/07/boundary-seeking-gan/\"\"\"\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "        self.x = []\n",
    "        self.y = np.zeros((31, 1), dtype=np.int)\n",
    "        self.y = list(self.y)\n",
    "        for i in range(31):\n",
    "            self.y[i] = []\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss=self.boundary_loss, optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def boundary_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Boundary seeking loss.\n",
    "        Reference: https://wiseodd.github.io/techblog/2017/03/07/boundary-seeking-gan/\n",
    "        \"\"\"\n",
    "        return 0.5 * K.mean((K.log(y_pred) - K.log(1 - y_pred))**2)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (X_test, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "        # X_test = X_test / 127.5 - 1.\n",
    "        X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            # idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                # progress_bar.update(index)\n",
    "\n",
    "                # get a batch of real images\n",
    "                image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch(image_batch, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"epoch:%d step:%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch,global_step, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                sampleSize = 5000\n",
    "                # If at save interval => save generated image samples\n",
    "                if global_step % sample_interval == 0:\n",
    "                    s = self.metrics(global_step, X_test, sampleSize)\n",
    "        for i in range(len(s)):\n",
    "            self.y[i] = [float(j) / max(self.y[i]) for j in self.y[i]]#对值进行归一化处理\n",
    "\n",
    "        for i in range(len(s)):\n",
    "            font1={'size':8}\n",
    "\n",
    "            plt.plot(self.x, self.y[i], label=labels_name[i])\n",
    "            plt.legend(loc='lower right',prop=font1)\n",
    "            plt.savefig('saved_models_bgan/{}.png'.format(labels_name[i]))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "    def metrics(self, epoch, X_test, sampleSize):\n",
    "        self.x.append(epoch)\n",
    "        r, c = 10, sampleSize // 10\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "#         sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise])\n",
    "        x_dataset = MyDataset(X_test[:sampleSize])\n",
    "        # print(x_dataset[0].shape)\n",
    "        x_real_loader = Data.DataLoader(dataset=x_dataset, batch_size=sampleSize, shuffle=True)\n",
    "        x_fake_dataset = MyDataset(gen_imgs)\n",
    "        x_fake_loader = Data.DataLoader(dataset=x_fake_dataset, batch_size=sampleSize, shuffle=True)\n",
    "        s = compute_score_raw(x_real_loader, x_fake_loader, 256, '/real/', './fake', conv_model='tfgan',\n",
    "                              workers=int(1))\n",
    "        real_images = tf.convert_to_tensor(X_test)  # real images\n",
    "#         # MNIST_CLASSIFIER_FROZEN_GRAPH = '.\\classify_mnist_graph_def.pb'\n",
    "        gen_imgs = np.array(gen_imgs)\n",
    "        eval_images = tf.convert_to_tensor(gen_imgs)\n",
    "        eval_score = utils.mnist_score(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH)  # IS score\n",
    "        frechet_distance = utils.mnist_frechet_distance(real_images, eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH)\n",
    "        mnist_score, f_distance = sess.run([eval_score, frechet_distance])\n",
    "#         print(mnist_score)\n",
    "#         print(f_distance)\n",
    "# #         s[14]=mnist_score\n",
    "# #         s[16]=f_distance\n",
    "        s[17] = mnist_score\n",
    "        s[18] = f_distance\n",
    "        print('IS socre: %f' % mnist_score)\n",
    "        print('FID: %f' % f_distance)\n",
    "\n",
    "        for i in range(len(s)):\n",
    "            print(i, \"=\", s[i])\n",
    "        for i in range(len(s)):\n",
    "            self.y[i].append(s[i])\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('epoch:' + str(epoch))\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('%.8f' % (i) for i in s)\n",
    "        f.writelines('\\n')\n",
    "        return s\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    bgan = BGAN()\n",
    "    bgan.train(epochs=30, batch_size=64, sample_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
