{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "from __future__ import print_function, division\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "import util\n",
    "import utils\n",
    "import tensorflow.contrib.gan as tfgan\n",
    "num_images_to_eval = 500\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, imgs, transform=None):\n",
    "        # super().__init__()\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = torch.from_numpy(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import ot\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.models as models\n",
    "\n",
    "from scipy import linalg\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def giveName(iter):  # 7 digit name.\n",
    "    ans = str(iter)\n",
    "    return ans.zfill(7)\n",
    "\n",
    "def make_dataset(dataset, dataroot, imageSize):\n",
    "    \"\"\"\n",
    "    :param dataset: must be in 'cifar10 | lsun | imagenet | folder | lfw | fake'\n",
    "    :return: pytorch dataset for DataLoader to utilize\n",
    "    \"\"\"\n",
    "    if dataset in ['imagenet', 'folder', 'lfw']:\n",
    "        print(os.getcwd() + dataroot)  # 函数的作用是用于返回当前工作目录\n",
    "        # folder dataset\n",
    "        # dataset = dset.ImageFolder(root=dataroot,\n",
    "        dataset = dset.ImageFolder(root=os.getcwd() + dataroot,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.Resize(imageSize),\n",
    "                                       # transforms.CenterCrop(imageSize),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(\n",
    "                                           (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                   ]))\n",
    "    elif dataset == 'lsun':\n",
    "        dataset = dset.LSUN(db_path=dataroot, classes=['bedroom_train'],\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.Resize(imageSize),\n",
    "                                transforms.CenterCrop(imageSize),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(\n",
    "                                    (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                            ]))\n",
    "    elif dataset == 'cifar10':\n",
    "        dataset = dset.CIFAR10(root=dataroot, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Resize(imageSize),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize(\n",
    "                                       (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                               ]))\n",
    "    elif dataset == 'celeba':\n",
    "        dataset = dset.ImageFolder(root=dataroot,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.CenterCrop(138),\n",
    "                                       transforms.Resize(imageSize),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(\n",
    "                                           (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                   ]))\n",
    "    else:\n",
    "        raise Exception('--dataset must be in cifar10 | lsun | imagenet | folder | lfw | fake')\n",
    "    assert dataset\n",
    "    return dataset\n",
    "\n",
    "MNIST_CLASSIFIER_FROZEN_GRAPH = './classify_mnist_graph_def.pb'\n",
    "INPUT_TENSOR = 'inputs:0'\n",
    "OUTPUT_TENSOR = 'logits:0'\n",
    "# CONV_TENSOR = 'fc3/Relu:0'\n",
    "CONV_TENSOR = 'fc4/BiasAdd:0'\n",
    "class ConvNetFeatureSaver(object):\n",
    "    def __init__(self, model='cnn', workers=4, batchSize=64):\n",
    "        '''\n",
    "        model: inception_v3, vgg13, vgg16, vgg19, resnet18, resnet34,\n",
    "               resnet50, resnet101, or resnet152\n",
    "        '''\n",
    "        self.model = model\n",
    "        self.batch_size = batchSize\n",
    "        self.workers = workers\n",
    "        if self.model.find('tfgan') >= 0:\n",
    "            print('tfgan')\n",
    "\n",
    "        elif self.model.find('vgg') >= 0:\n",
    "            self.vgg = getattr(models, model)(pretrained=True).cuda().eval()\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                     (0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "        elif self.model.find('resnet') >= 0:\n",
    "            resnet = getattr(models, model)(pretrained=True)\n",
    "            resnet.cuda().eval()\n",
    "            resnet_feature = nn.Sequential(resnet.conv1, resnet.bn1,\n",
    "                                           resnet.relu,\n",
    "                                           resnet.maxpool, resnet.layer1,\n",
    "                                           resnet.layer2, resnet.layer3,\n",
    "                                           resnet.layer4).cuda().eval()\n",
    "            self.resnet = resnet\n",
    "            self.resnet_feature = resnet_feature\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                     (0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "        elif self.model == 'inception' or self.model == 'inception_v3':\n",
    "            inception = models.inception_v3(\n",
    "                pretrained=True, transform_input=False).cuda().eval()\n",
    "            inception_feature = nn.Sequential(inception.Conv2d_1a_3x3,\n",
    "                                              inception.Conv2d_2a_3x3,\n",
    "                                              inception.Conv2d_2b_3x3,\n",
    "                                              nn.MaxPool2d(3, 2),\n",
    "                                              inception.Conv2d_3b_1x1,\n",
    "                                              inception.Conv2d_4a_3x3,\n",
    "                                              nn.MaxPool2d(3, 2),\n",
    "                                              inception.Mixed_5b,\n",
    "                                              inception.Mixed_5c,\n",
    "                                              inception.Mixed_5d,\n",
    "                                              inception.Mixed_6a,\n",
    "                                              inception.Mixed_6b,\n",
    "                                              inception.Mixed_6c,\n",
    "                                              inception.Mixed_6d,\n",
    "                                              inception.Mixed_7a,\n",
    "                                              inception.Mixed_7b,\n",
    "                                              inception.Mixed_7c,\n",
    "                                              ).cuda().eval()\n",
    "            self.inception = inception\n",
    "            self.inception_feature = inception_feature\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.Resize(299),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def save(self, imgFolder, dataloader, save2disk=False):\n",
    "        feature_pixl, feature_conv, feature_smax, feature_logit = [], [], [], []\n",
    "\n",
    "        for img in dataloader:\n",
    "            with torch.no_grad():\n",
    "                input = img.cuda()\n",
    "                if self.model == 'tfgan':\n",
    "                    gen_imgs = np.array(img)\n",
    "                    eval_images = tf.convert_to_tensor(gen_imgs)\n",
    "                    flogit = util.mnist_logits(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH, INPUT_TENSOR, OUTPUT_TENSOR)\n",
    "                    fconv = util.mnist_logits(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH, INPUT_TENSOR, CONV_TENSOR)\n",
    "                    flogit,fconv=tf.Session().run([flogit,fconv])\n",
    "\n",
    "                    flogit=torch.from_numpy(flogit)\n",
    "                    fconv=torch.from_numpy(fconv)\n",
    "                elif self.model == 'vgg' or self.model == 'vgg16':\n",
    "                    print(self.vgg.features(input).shape)\n",
    "                    fconv = self.vgg.features(input).view(input.size(0), -1)  # 相当于reshape\n",
    "                    flogit = self.vgg.classifier(fconv)\n",
    "                    # flogit = self.vgg.logitifier(fconv)\n",
    "                elif self.model.find('resnet') >= 0:\n",
    "                    fconv = self.resnet_feature(\n",
    "                        input).mean(3).mean(2).squeeze()\n",
    "                    flogit = self.resnet.fc(fconv)\n",
    "                elif self.model == 'inception' or self.model == 'inception_v3':\n",
    "                    fconv = self.inception_feature(\n",
    "                        input).mean(3).mean(2).squeeze()\n",
    "                    flogit = self.inception.fc(fconv)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                fsmax = F.softmax(flogit)\n",
    "                '''\n",
    "                总共有四个空间：1.feature_pixl 2.feature_conv 3.feature_logit 4.feature_smax\n",
    "                '''\n",
    "                feature_pixl.append(img)\n",
    "                feature_conv.append(fconv.data.cpu())\n",
    "                feature_logit.append(flogit.data.cpu())\n",
    "                feature_smax.append(fsmax.data.cpu())\n",
    "\n",
    "        feature_pixl = torch.cat(feature_pixl, 0).to('cpu')\n",
    "        feature_conv = torch.cat(feature_conv, 0).to('cpu')\n",
    "        feature_logit = torch.cat(feature_logit, 0).to('cpu')\n",
    "        feature_smax = torch.cat(feature_smax, 0).to('cpu')\n",
    "\n",
    "        return feature_pixl, feature_conv, feature_logit, feature_smax\n",
    "\n",
    "    # return feature_pixl, feature_conv, feature_logit, feature_smax\n",
    "\n",
    "\n",
    "def distance(X, Y, sqrt):\n",
    "    nX = X.size(0)\n",
    "    nY = Y.size(0)\n",
    "    X = X.view(nX, -1)\n",
    "    X2 = (X * X).sum(1).resize_(nX, 1)\n",
    "    Y = Y.view(nY, -1)\n",
    "    Y2 = (Y * Y).sum(1).resize_(nY, 1)\n",
    "\n",
    "    M = torch.zeros(nX, nY)\n",
    "    M.copy_(X2.expand(nX, nY) + Y2.expand(nY, nX).transpose(0, 1) -\n",
    "            2 * torch.mm(X, Y.transpose(0, 1)))\n",
    "\n",
    "    del X, X2, Y, Y2\n",
    "\n",
    "    if sqrt:\n",
    "        M = ((M + M.abs()) / 2).sqrt()\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "def wasserstein(M, sqrt):\n",
    "    if sqrt:\n",
    "        M = M.abs().sqrt()\n",
    "    emd = ot.emd2([], [], M.numpy())\n",
    "\n",
    "    return emd\n",
    "\n",
    "\n",
    "class Score_knn:\n",
    "    acc = 0\n",
    "    acc_real = 0\n",
    "    acc_fake = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    ft = 0\n",
    "\n",
    "\n",
    "def knn(Mxx, Mxy, Myy, k, sqrt):\n",
    "    n0 = Mxx.size(0)\n",
    "    n1 = Myy.size(0)\n",
    "    label = torch.cat((torch.ones(n0), torch.zeros(n1)))\n",
    "    M = torch.cat((torch.cat((Mxx, Mxy), 1), torch.cat(\n",
    "        (Mxy.transpose(0, 1), Myy), 1)), 0)\n",
    "    if sqrt:\n",
    "        M = M.abs().sqrt()\n",
    "    INFINITY = float('inf')\n",
    "    val, idx = (M + torch.diag(INFINITY * torch.ones(n0 + n1))\n",
    "                ).topk(k, 0, False)\n",
    "\n",
    "    count = torch.zeros(n0 + n1)\n",
    "    for i in range(0, k):\n",
    "        count = count + label.index_select(0, idx[i])\n",
    "    pred = torch.ge(count, (float(k) / 2) * torch.ones(n0 + n1)).float()\n",
    "\n",
    "    s = Score_knn()\n",
    "    s.tp = (pred * label).sum()\n",
    "    s.fp = (pred * (1 - label)).sum()\n",
    "    s.fn = ((1 - pred) * label).sum()\n",
    "    s.tn = ((1 - pred) * (1 - label)).sum()\n",
    "    s.precision = s.tp / (s.tp + s.fp + 1e-10)\n",
    "    s.recall = s.tp / (s.tp + s.fn + 1e-10)\n",
    "    s.acc_t = s.tp / (s.tp + s.fn)\n",
    "    s.acc_f = s.tn / (s.tn + s.fp)\n",
    "    s.acc = torch.eq(label, pred).float().mean()\n",
    "    s.k = k\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def mmd(Mxx, Mxy, Myy, sigma):\n",
    "    scale = Mxx.mean()\n",
    "    Mxx = torch.exp(-Mxx / (scale * 2 * sigma * sigma))\n",
    "    Mxy = torch.exp(-Mxy / (scale * 2 * sigma * sigma))\n",
    "    Myy = torch.exp(-Myy / (scale * 2 * sigma * sigma))\n",
    "    mmd = math.sqrt(Mxx.mean() + Myy.mean() - 2 * Mxy.mean())\n",
    "\n",
    "    return mmd\n",
    "\n",
    "\n",
    "def entropy_score(X, Y, epsilons):\n",
    "    Mxy = distance(X, Y, False)\n",
    "    scores = []\n",
    "    for epsilon in epsilons:\n",
    "        scores.append(ent(Mxy.t(), epsilon))\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def ent(M, epsilon):\n",
    "    n0 = M.size(0)\n",
    "    n1 = M.size(1)\n",
    "    neighbors = M.lt(epsilon).float()\n",
    "    sums = neighbors.sum(0).repeat(n0, 1)\n",
    "    sums[sums.eq(0)] = 1\n",
    "    neighbors = neighbors.div(sums)\n",
    "    probs = neighbors.sum(1) / n1\n",
    "    rem = 1 - probs.sum()\n",
    "    if rem < 0:\n",
    "        rem = 0\n",
    "    probs = torch.cat((probs, rem * torch.ones(1)), 0)\n",
    "    e = {}\n",
    "    e['probs'] = probs\n",
    "    probs = probs[probs.gt(0)]\n",
    "    e['ent'] = -probs.mul(probs.log()).sum()\n",
    "\n",
    "    return e\n",
    "\n",
    "\n",
    "eps = 1e-20\n",
    "\n",
    "\n",
    "def inception_score(X):\n",
    "    kl = X * ((X + eps).log() - (X.mean(0) + eps).log().expand_as(X))\n",
    "    score = np.exp(kl.sum(1).mean())\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def mode_score(X, Y):\n",
    "    kl1 = X * ((X + eps).log() - (X.mean(0) + eps).log().expand_as(X))\n",
    "    kl2 = X.mean(0) * ((X.mean(0) + eps).log() - (Y.mean(0) + eps).log())\n",
    "    score = np.exp(kl1.sum(1).mean() - kl2.sum())\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def fid(X, Y):\n",
    "    m = X.mean(0)\n",
    "    m_w = Y.mean(0)\n",
    "    X_np = X.numpy()\n",
    "    Y_np = Y.numpy()\n",
    "\n",
    "    C = np.cov(X_np.transpose())\n",
    "    C_w = np.cov(Y_np.transpose())\n",
    "    C_C_w_sqrt = linalg.sqrtm(C.dot(C_w), True).real\n",
    "\n",
    "    score = m.dot(m) + m_w.dot(m_w) - 2 * m_w.dot(m) + \\\n",
    "            np.trace(C + C_w - 2 * C_C_w_sqrt)\n",
    "#     return np.exp(score)\n",
    "    return np.sqrt(score)\n",
    "\n",
    "\n",
    "class Score:\n",
    "    emd = 0\n",
    "    mmd = 0\n",
    "    knn = None\n",
    "\n",
    "\n",
    "def compute_score(real, fake, k=1, sigma=1, sqrt=True):\n",
    "    Mxx = distance(real, real, False)\n",
    "    Mxy = distance(real, fake, False)\n",
    "    Myy = distance(fake, fake, False)\n",
    "\n",
    "    s = Score()\n",
    "    s.emd = wasserstein(Mxy, sqrt)\n",
    "    s.mmd = mmd(Mxx, Mxy, Myy, sigma)\n",
    "    s.knn = knn(Mxx, Mxy, Myy, k, sqrt)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "'''\n",
    "参数说明：\n",
    "dataset:真实数据集的path\n",
    "imageSize:图片的大小\n",
    "dataroot_real:真实数据所在的path\n",
    "batchSize\n",
    "saveFolder_r:真实数据的保存位置\n",
    "conv_model:卷积模型\n",
    "'''\n",
    "\n",
    "\n",
    "def compute_score_raw(real_dataloader, fake_dataloader, batchSize, saveFolder_r, saveFolder_f, conv_model='resnet34',\n",
    "                      workers=4):\n",
    "    convnet_feature_saver = ConvNetFeatureSaver(model=conv_model,\n",
    "                                                batchSize=batchSize, workers=workers)\n",
    "    print(saveFolder_r)\n",
    "    print(saveFolder_f)\n",
    "    feature_r = convnet_feature_saver.save(saveFolder_r, real_dataloader, False)\n",
    "    feature_f = convnet_feature_saver.save(saveFolder_f, fake_dataloader, False)\n",
    "\n",
    "    # 4 feature spaces and 7 scores + incep + modescore + fid\n",
    "    score = np.zeros(2 * 7 + 5)\n",
    "    for i in range(0, 2):\n",
    "        print('compute score in space: ' + str(i))\n",
    "        Mxx = distance(feature_r[i], feature_r[i], False)\n",
    "        Mxy = distance(feature_r[i], feature_f[i], False)\n",
    "        Myy = distance(feature_f[i], feature_f[i], False)\n",
    "\n",
    "        score[i * 7] = wasserstein(Mxy, True)\n",
    "        score[i * 7 + 1] = mmd(Mxx, Mxy, Myy, 1)\n",
    "        tmp = knn(Mxx, Mxy, Myy, 1, False)\n",
    "        score[(i * 7 + 2):(i * 7 + 7)] = \\\n",
    "            tmp.acc, tmp.acc_t, tmp.acc_f, tmp.precision, tmp.recall\n",
    "\n",
    "\n",
    "    score[14] = inception_score(feature_f[3])\n",
    "    score[15] = mode_score(feature_r[3], feature_f[3])\n",
    "    score[16] = fid(feature_r[3], feature_f[3])\n",
    "\n",
    "    return score\n",
    "labels_name=['w_pixl','mmd_pixl','acc_pixl','acc_t_pixl','acc_f_pixl','acc_precision_pixl','acc_recall_pixl',\n",
    "             'w_conv','mmd_conv','acc_conv','acc_t_conv','acc_f_conv','acc_precision_conv','acc_recall_conv',\n",
    "             'is','mode_score','fid' ,'tf_is','tf_fid']\n",
    "if not os.path.isdir('saved_models_{}'.format('bigan')):\n",
    "    os.mkdir('saved_models_{}'.format('bigan'))\n",
    "f = open('saved_models_{}/log_collapse1.txt'.format('bigan'), mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 512)               51712     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 784)               402192    \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 720,656\n",
      "Trainable params: 718,608\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_12 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 719,972\n",
      "Trainable params: 717,924\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "epoch:0 step:1 [D loss: 0.912495, acc: 39.06%] [G loss: 4.530239]\n",
      "epoch:0 step:2 [D loss: 0.439272, acc: 73.44%] [G loss: 5.196466]\n",
      "epoch:0 step:3 [D loss: 0.169101, acc: 96.09%] [G loss: 6.083126]\n",
      "epoch:0 step:4 [D loss: 0.134595, acc: 97.66%] [G loss: 7.240883]\n",
      "epoch:0 step:5 [D loss: 0.112393, acc: 98.44%] [G loss: 8.924417]\n",
      "epoch:0 step:6 [D loss: 0.049609, acc: 100.00%] [G loss: 9.484959]\n",
      "epoch:0 step:7 [D loss: 0.034904, acc: 100.00%] [G loss: 10.111000]\n",
      "epoch:0 step:8 [D loss: 0.020286, acc: 100.00%] [G loss: 10.137009]\n",
      "epoch:0 step:9 [D loss: 0.020149, acc: 100.00%] [G loss: 10.336619]\n",
      "epoch:0 step:10 [D loss: 0.016917, acc: 100.00%] [G loss: 10.608325]\n",
      "epoch:0 step:11 [D loss: 0.015796, acc: 100.00%] [G loss: 11.070005]\n",
      "epoch:0 step:12 [D loss: 0.015855, acc: 100.00%] [G loss: 11.012123]\n",
      "epoch:0 step:13 [D loss: 0.014795, acc: 100.00%] [G loss: 11.998514]\n",
      "epoch:0 step:14 [D loss: 0.012099, acc: 100.00%] [G loss: 11.935541]\n",
      "epoch:0 step:15 [D loss: 0.012373, acc: 100.00%] [G loss: 12.919386]\n",
      "epoch:0 step:16 [D loss: 0.011541, acc: 100.00%] [G loss: 13.202497]\n",
      "epoch:0 step:17 [D loss: 0.019915, acc: 99.22%] [G loss: 14.676004]\n",
      "epoch:0 step:18 [D loss: 0.006330, acc: 100.00%] [G loss: 14.654833]\n",
      "epoch:0 step:19 [D loss: 0.009393, acc: 100.00%] [G loss: 14.875551]\n",
      "epoch:0 step:20 [D loss: 0.009673, acc: 100.00%] [G loss: 14.597576]\n",
      "epoch:0 step:21 [D loss: 0.005879, acc: 100.00%] [G loss: 15.214924]\n",
      "epoch:0 step:22 [D loss: 0.044977, acc: 99.22%] [G loss: 15.393982]\n",
      "epoch:0 step:23 [D loss: 0.037023, acc: 99.22%] [G loss: 17.924423]\n",
      "epoch:0 step:24 [D loss: 0.010017, acc: 100.00%] [G loss: 18.952854]\n",
      "epoch:0 step:25 [D loss: 0.011510, acc: 100.00%] [G loss: 18.635338]\n",
      "epoch:0 step:26 [D loss: 0.007722, acc: 100.00%] [G loss: 18.222084]\n",
      "epoch:0 step:27 [D loss: 0.014769, acc: 99.22%] [G loss: 18.885490]\n",
      "epoch:0 step:28 [D loss: 0.007405, acc: 100.00%] [G loss: 19.551842]\n",
      "epoch:0 step:29 [D loss: 0.007061, acc: 100.00%] [G loss: 18.946972]\n",
      "epoch:0 step:30 [D loss: 0.004301, acc: 100.00%] [G loss: 19.419695]\n",
      "epoch:0 step:31 [D loss: 0.013591, acc: 99.22%] [G loss: 19.901157]\n",
      "epoch:0 step:32 [D loss: 0.006923, acc: 100.00%] [G loss: 19.760506]\n",
      "epoch:0 step:33 [D loss: 0.005243, acc: 100.00%] [G loss: 19.340349]\n",
      "epoch:0 step:34 [D loss: 0.060419, acc: 98.44%] [G loss: 20.595348]\n",
      "epoch:0 step:35 [D loss: 0.009827, acc: 100.00%] [G loss: 20.823477]\n",
      "epoch:0 step:36 [D loss: 0.010226, acc: 100.00%] [G loss: 21.320595]\n",
      "epoch:0 step:37 [D loss: 0.008345, acc: 100.00%] [G loss: 20.784666]\n",
      "epoch:0 step:38 [D loss: 0.020527, acc: 99.22%] [G loss: 21.873123]\n",
      "epoch:0 step:39 [D loss: 0.007557, acc: 100.00%] [G loss: 21.452740]\n",
      "epoch:0 step:40 [D loss: 0.064388, acc: 99.22%] [G loss: 21.707764]\n",
      "epoch:0 step:41 [D loss: 0.006028, acc: 100.00%] [G loss: 21.807396]\n",
      "epoch:0 step:42 [D loss: 0.005527, acc: 100.00%] [G loss: 21.475777]\n",
      "epoch:0 step:43 [D loss: 0.020330, acc: 99.22%] [G loss: 22.352177]\n",
      "epoch:0 step:44 [D loss: 0.018853, acc: 99.22%] [G loss: 22.450298]\n",
      "epoch:0 step:45 [D loss: 0.012645, acc: 99.22%] [G loss: 22.606699]\n",
      "epoch:0 step:46 [D loss: 0.006411, acc: 100.00%] [G loss: 21.984158]\n",
      "epoch:0 step:47 [D loss: 0.002959, acc: 100.00%] [G loss: 22.745626]\n",
      "epoch:0 step:48 [D loss: 0.005206, acc: 100.00%] [G loss: 22.338793]\n",
      "epoch:0 step:49 [D loss: 0.212319, acc: 96.88%] [G loss: 21.490831]\n",
      "epoch:0 step:50 [D loss: 0.051241, acc: 98.44%] [G loss: 21.390942]\n",
      "epoch:0 step:51 [D loss: 0.024361, acc: 100.00%] [G loss: 22.451113]\n",
      "epoch:0 step:52 [D loss: 0.013648, acc: 100.00%] [G loss: 22.799833]\n",
      "epoch:0 step:53 [D loss: 0.028756, acc: 99.22%] [G loss: 23.534111]\n",
      "epoch:0 step:54 [D loss: 0.014022, acc: 99.22%] [G loss: 23.891632]\n",
      "epoch:0 step:55 [D loss: 0.005608, acc: 100.00%] [G loss: 23.717674]\n",
      "epoch:0 step:56 [D loss: 0.047974, acc: 98.44%] [G loss: 22.534775]\n",
      "epoch:0 step:57 [D loss: 0.043998, acc: 97.66%] [G loss: 22.527037]\n",
      "epoch:0 step:58 [D loss: 0.008000, acc: 100.00%] [G loss: 22.493847]\n",
      "epoch:0 step:59 [D loss: 0.208171, acc: 92.97%] [G loss: 22.058435]\n",
      "epoch:0 step:60 [D loss: 0.058842, acc: 97.66%] [G loss: 22.684855]\n",
      "epoch:0 step:61 [D loss: 0.079504, acc: 96.09%] [G loss: 23.400299]\n",
      "epoch:0 step:62 [D loss: 0.091769, acc: 97.66%] [G loss: 24.024353]\n",
      "epoch:0 step:63 [D loss: 0.074564, acc: 97.66%] [G loss: 24.966341]\n",
      "epoch:0 step:64 [D loss: 0.204652, acc: 92.19%] [G loss: 25.076092]\n",
      "epoch:0 step:65 [D loss: 0.120458, acc: 96.09%] [G loss: 25.092758]\n",
      "epoch:0 step:66 [D loss: 0.092841, acc: 96.88%] [G loss: 24.138386]\n",
      "epoch:0 step:67 [D loss: 0.190160, acc: 93.75%] [G loss: 23.937111]\n",
      "epoch:0 step:68 [D loss: 0.404050, acc: 92.97%] [G loss: 23.409863]\n",
      "epoch:0 step:69 [D loss: 0.057330, acc: 97.66%] [G loss: 21.079931]\n",
      "epoch:0 step:70 [D loss: 0.388617, acc: 89.06%] [G loss: 24.416084]\n",
      "epoch:0 step:71 [D loss: 0.161723, acc: 96.88%] [G loss: 19.475824]\n",
      "epoch:0 step:72 [D loss: 0.116673, acc: 95.31%] [G loss: 21.778347]\n",
      "epoch:0 step:73 [D loss: 0.356699, acc: 89.06%] [G loss: 19.004740]\n",
      "epoch:0 step:74 [D loss: 0.554278, acc: 81.25%] [G loss: 20.026913]\n",
      "epoch:0 step:75 [D loss: 0.469733, acc: 86.72%] [G loss: 20.268337]\n",
      "epoch:0 step:76 [D loss: 0.510932, acc: 85.16%] [G loss: 22.642796]\n",
      "epoch:0 step:77 [D loss: 0.429177, acc: 87.50%] [G loss: 15.620347]\n",
      "epoch:0 step:78 [D loss: 2.701833, acc: 53.12%] [G loss: 23.453190]\n",
      "epoch:0 step:79 [D loss: 1.342892, acc: 79.69%] [G loss: 23.008245]\n",
      "epoch:0 step:80 [D loss: 0.826874, acc: 85.16%] [G loss: 20.023773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:81 [D loss: 0.275081, acc: 90.62%] [G loss: 16.809078]\n",
      "epoch:0 step:82 [D loss: 0.301127, acc: 85.94%] [G loss: 16.358898]\n",
      "epoch:0 step:83 [D loss: 0.237588, acc: 90.62%] [G loss: 14.323850]\n",
      "epoch:0 step:84 [D loss: 0.397076, acc: 82.81%] [G loss: 15.956106]\n",
      "epoch:0 step:85 [D loss: 0.331105, acc: 85.16%] [G loss: 15.321983]\n",
      "epoch:0 step:86 [D loss: 0.210009, acc: 92.19%] [G loss: 13.669735]\n",
      "epoch:0 step:87 [D loss: 0.150067, acc: 92.19%] [G loss: 10.764244]\n",
      "epoch:0 step:88 [D loss: 0.344277, acc: 82.81%] [G loss: 13.079170]\n",
      "epoch:0 step:89 [D loss: 0.253635, acc: 85.94%] [G loss: 12.241133]\n",
      "epoch:0 step:90 [D loss: 0.340201, acc: 82.81%] [G loss: 11.231940]\n",
      "epoch:0 step:91 [D loss: 0.121303, acc: 93.75%] [G loss: 11.044257]\n",
      "epoch:0 step:92 [D loss: 0.351838, acc: 82.03%] [G loss: 13.177404]\n",
      "epoch:0 step:93 [D loss: 0.188943, acc: 91.41%] [G loss: 11.728052]\n",
      "epoch:0 step:94 [D loss: 0.720653, acc: 73.44%] [G loss: 12.393776]\n",
      "epoch:0 step:95 [D loss: 0.332880, acc: 82.03%] [G loss: 9.263541]\n",
      "epoch:0 step:96 [D loss: 0.241663, acc: 85.16%] [G loss: 9.119934]\n",
      "epoch:0 step:97 [D loss: 0.268059, acc: 86.72%] [G loss: 10.534442]\n",
      "epoch:0 step:98 [D loss: 0.249436, acc: 85.16%] [G loss: 9.675942]\n",
      "epoch:0 step:99 [D loss: 0.183147, acc: 91.41%] [G loss: 8.834884]\n",
      "epoch:0 step:100 [D loss: 0.598221, acc: 71.88%] [G loss: 11.881092]\n",
      "epoch:0 step:101 [D loss: 0.228886, acc: 88.28%] [G loss: 9.424318]\n",
      "epoch:0 step:102 [D loss: 0.412414, acc: 80.47%] [G loss: 10.033590]\n",
      "epoch:0 step:103 [D loss: 0.227922, acc: 87.50%] [G loss: 10.338184]\n",
      "epoch:0 step:104 [D loss: 0.267793, acc: 85.94%] [G loss: 11.115385]\n",
      "epoch:0 step:105 [D loss: 0.274558, acc: 85.94%] [G loss: 9.254892]\n",
      "epoch:0 step:106 [D loss: 0.378871, acc: 77.34%] [G loss: 10.335392]\n",
      "epoch:0 step:107 [D loss: 0.246742, acc: 88.28%] [G loss: 10.060391]\n",
      "epoch:0 step:108 [D loss: 0.561119, acc: 73.44%] [G loss: 11.533909]\n",
      "epoch:0 step:109 [D loss: 0.173070, acc: 92.19%] [G loss: 10.234495]\n",
      "epoch:0 step:110 [D loss: 0.315176, acc: 82.81%] [G loss: 8.794405]\n",
      "epoch:0 step:111 [D loss: 0.568134, acc: 74.22%] [G loss: 10.211532]\n",
      "epoch:0 step:112 [D loss: 0.284248, acc: 85.16%] [G loss: 9.253839]\n",
      "epoch:0 step:113 [D loss: 0.289100, acc: 83.59%] [G loss: 8.471275]\n",
      "epoch:0 step:114 [D loss: 0.438052, acc: 79.69%] [G loss: 9.149747]\n",
      "epoch:0 step:115 [D loss: 0.205275, acc: 90.62%] [G loss: 8.633429]\n",
      "epoch:0 step:116 [D loss: 0.266367, acc: 86.72%] [G loss: 9.447246]\n",
      "epoch:0 step:117 [D loss: 0.298013, acc: 82.81%] [G loss: 9.556812]\n",
      "epoch:0 step:118 [D loss: 0.247677, acc: 90.62%] [G loss: 9.321249]\n",
      "epoch:0 step:119 [D loss: 0.271122, acc: 86.72%] [G loss: 9.110993]\n",
      "epoch:0 step:120 [D loss: 0.385405, acc: 78.12%] [G loss: 11.596175]\n",
      "epoch:0 step:121 [D loss: 0.236176, acc: 84.38%] [G loss: 10.454191]\n",
      "epoch:0 step:122 [D loss: 0.185481, acc: 92.97%] [G loss: 8.704130]\n",
      "epoch:0 step:123 [D loss: 0.501043, acc: 77.34%] [G loss: 10.894862]\n",
      "epoch:0 step:124 [D loss: 0.219385, acc: 88.28%] [G loss: 10.142297]\n",
      "epoch:0 step:125 [D loss: 0.270557, acc: 86.72%] [G loss: 8.590091]\n",
      "epoch:0 step:126 [D loss: 0.233316, acc: 89.84%] [G loss: 9.338961]\n",
      "epoch:0 step:127 [D loss: 0.266713, acc: 86.72%] [G loss: 8.852354]\n",
      "epoch:0 step:128 [D loss: 0.228181, acc: 87.50%] [G loss: 8.827730]\n",
      "epoch:0 step:129 [D loss: 0.190899, acc: 93.75%] [G loss: 9.138035]\n",
      "epoch:0 step:130 [D loss: 0.214798, acc: 86.72%] [G loss: 10.099451]\n",
      "epoch:0 step:131 [D loss: 0.159735, acc: 89.84%] [G loss: 9.594612]\n",
      "epoch:0 step:132 [D loss: 0.237620, acc: 89.84%] [G loss: 9.107027]\n",
      "epoch:0 step:133 [D loss: 0.390028, acc: 81.25%] [G loss: 10.141905]\n",
      "epoch:0 step:134 [D loss: 0.188161, acc: 92.19%] [G loss: 8.539055]\n",
      "epoch:0 step:135 [D loss: 0.210031, acc: 89.06%] [G loss: 8.753741]\n",
      "epoch:0 step:136 [D loss: 0.306645, acc: 84.38%] [G loss: 9.787333]\n",
      "epoch:0 step:137 [D loss: 0.152711, acc: 92.97%] [G loss: 8.631076]\n",
      "epoch:0 step:138 [D loss: 0.221889, acc: 91.41%] [G loss: 8.792694]\n",
      "epoch:0 step:139 [D loss: 0.195003, acc: 89.84%] [G loss: 9.193642]\n",
      "epoch:0 step:140 [D loss: 0.193237, acc: 90.62%] [G loss: 9.794666]\n",
      "epoch:0 step:141 [D loss: 0.161225, acc: 91.41%] [G loss: 8.418739]\n",
      "epoch:0 step:142 [D loss: 0.188497, acc: 91.41%] [G loss: 8.924082]\n",
      "epoch:0 step:143 [D loss: 0.178917, acc: 92.19%] [G loss: 8.871023]\n",
      "epoch:0 step:144 [D loss: 0.161245, acc: 95.31%] [G loss: 8.548115]\n",
      "epoch:0 step:145 [D loss: 0.158741, acc: 96.09%] [G loss: 9.432979]\n",
      "epoch:0 step:146 [D loss: 0.494586, acc: 83.59%] [G loss: 10.552501]\n",
      "epoch:0 step:147 [D loss: 0.231920, acc: 89.84%] [G loss: 9.104134]\n",
      "epoch:0 step:148 [D loss: 0.239477, acc: 86.72%] [G loss: 8.955077]\n",
      "epoch:0 step:149 [D loss: 0.140153, acc: 95.31%] [G loss: 8.054265]\n",
      "epoch:0 step:150 [D loss: 0.120835, acc: 96.88%] [G loss: 8.863186]\n",
      "epoch:0 step:151 [D loss: 0.131905, acc: 92.19%] [G loss: 9.512240]\n",
      "epoch:0 step:152 [D loss: 0.153468, acc: 93.75%] [G loss: 9.381648]\n",
      "epoch:0 step:153 [D loss: 0.188094, acc: 92.19%] [G loss: 9.659122]\n",
      "epoch:0 step:154 [D loss: 0.128030, acc: 92.97%] [G loss: 9.238521]\n",
      "epoch:0 step:155 [D loss: 0.087878, acc: 97.66%] [G loss: 9.041173]\n",
      "epoch:0 step:156 [D loss: 0.186405, acc: 90.62%] [G loss: 9.420528]\n",
      "epoch:0 step:157 [D loss: 0.186821, acc: 90.62%] [G loss: 8.674845]\n",
      "epoch:0 step:158 [D loss: 0.156591, acc: 92.19%] [G loss: 9.020913]\n",
      "epoch:0 step:159 [D loss: 0.293404, acc: 86.72%] [G loss: 10.570734]\n",
      "epoch:0 step:160 [D loss: 0.401253, acc: 84.38%] [G loss: 10.491728]\n",
      "epoch:0 step:161 [D loss: 0.241817, acc: 91.41%] [G loss: 8.434206]\n",
      "epoch:0 step:162 [D loss: 0.316092, acc: 82.03%] [G loss: 9.200194]\n",
      "epoch:0 step:163 [D loss: 0.117900, acc: 94.53%] [G loss: 7.670903]\n",
      "epoch:0 step:164 [D loss: 0.167920, acc: 94.53%] [G loss: 8.615413]\n",
      "epoch:0 step:165 [D loss: 0.123953, acc: 94.53%] [G loss: 7.916833]\n",
      "epoch:0 step:166 [D loss: 0.093300, acc: 97.66%] [G loss: 7.610947]\n",
      "epoch:0 step:167 [D loss: 0.190492, acc: 90.62%] [G loss: 8.047490]\n",
      "epoch:0 step:168 [D loss: 0.152369, acc: 93.75%] [G loss: 7.647343]\n",
      "epoch:0 step:169 [D loss: 0.227956, acc: 91.41%] [G loss: 7.888627]\n",
      "epoch:0 step:170 [D loss: 0.386816, acc: 84.38%] [G loss: 9.846658]\n",
      "epoch:0 step:171 [D loss: 0.190469, acc: 88.28%] [G loss: 8.694434]\n",
      "epoch:0 step:172 [D loss: 0.213501, acc: 89.06%] [G loss: 7.537879]\n",
      "epoch:0 step:173 [D loss: 0.298526, acc: 83.59%] [G loss: 8.662985]\n",
      "epoch:0 step:174 [D loss: 0.152022, acc: 95.31%] [G loss: 7.585101]\n",
      "epoch:0 step:175 [D loss: 0.194220, acc: 92.97%] [G loss: 6.614108]\n",
      "epoch:0 step:176 [D loss: 0.294135, acc: 85.94%] [G loss: 8.273109]\n",
      "epoch:0 step:177 [D loss: 0.202767, acc: 87.50%] [G loss: 8.380901]\n",
      "epoch:0 step:178 [D loss: 0.112527, acc: 96.88%] [G loss: 7.068259]\n",
      "epoch:0 step:179 [D loss: 0.183907, acc: 92.97%] [G loss: 7.556221]\n",
      "epoch:0 step:180 [D loss: 0.231184, acc: 92.97%] [G loss: 7.085826]\n",
      "epoch:0 step:181 [D loss: 0.166524, acc: 93.75%] [G loss: 7.100756]\n",
      "epoch:0 step:182 [D loss: 0.215104, acc: 91.41%] [G loss: 8.088289]\n",
      "epoch:0 step:183 [D loss: 0.214266, acc: 92.19%] [G loss: 6.980982]\n",
      "epoch:0 step:184 [D loss: 0.336422, acc: 83.59%] [G loss: 8.578098]\n",
      "epoch:0 step:185 [D loss: 0.223168, acc: 86.72%] [G loss: 6.962720]\n",
      "epoch:0 step:186 [D loss: 0.445302, acc: 78.91%] [G loss: 8.293616]\n",
      "epoch:0 step:187 [D loss: 0.175942, acc: 90.62%] [G loss: 6.078642]\n",
      "epoch:0 step:188 [D loss: 0.566062, acc: 70.31%] [G loss: 7.321882]\n",
      "epoch:0 step:189 [D loss: 0.143364, acc: 93.75%] [G loss: 6.610036]\n",
      "epoch:0 step:190 [D loss: 0.323452, acc: 85.94%] [G loss: 6.508660]\n",
      "epoch:0 step:191 [D loss: 0.185698, acc: 95.31%] [G loss: 5.504508]\n",
      "epoch:0 step:192 [D loss: 0.265403, acc: 89.06%] [G loss: 6.693792]\n",
      "epoch:0 step:193 [D loss: 0.206377, acc: 91.41%] [G loss: 6.338882]\n",
      "epoch:0 step:194 [D loss: 0.255125, acc: 88.28%] [G loss: 6.504058]\n",
      "epoch:0 step:195 [D loss: 0.321420, acc: 82.81%] [G loss: 6.773744]\n",
      "epoch:0 step:196 [D loss: 0.208600, acc: 91.41%] [G loss: 5.807372]\n",
      "epoch:0 step:197 [D loss: 0.203573, acc: 92.97%] [G loss: 5.858023]\n",
      "epoch:0 step:198 [D loss: 0.455089, acc: 77.34%] [G loss: 6.462632]\n",
      "epoch:0 step:199 [D loss: 0.241045, acc: 89.06%] [G loss: 5.785374]\n",
      "epoch:0 step:200 [D loss: 0.277662, acc: 89.84%] [G loss: 5.781464]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:208: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 1.197217\n",
      "FID: 281.459656\n",
      "0 = 23.77975600051876\n",
      "1 = 0.5558518023856381\n",
      "2 = 1.0\n",
      "3 = 1.0\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 1.0\n",
      "7 = 16.271036064863235\n",
      "8 = 0.2638933170237083\n",
      "9 = 0.9986000061035156\n",
      "10 = 0.9972000122070312\n",
      "11 = 1.0\n",
      "12 = 1.0\n",
      "13 = 0.9972000122070312\n",
      "14 = 1.1972161531448364\n",
      "15 = 6.150457382202148\n",
      "16 = 0.8523467779159546\n",
      "17 = 1.1972168684005737\n",
      "18 = 281.45965576171875\n",
      "epoch:0 step:201 [D loss: 0.127832, acc: 98.44%] [G loss: 5.498044]\n",
      "epoch:0 step:202 [D loss: 0.566950, acc: 73.44%] [G loss: 5.699416]\n",
      "epoch:0 step:203 [D loss: 0.218560, acc: 91.41%] [G loss: 5.686241]\n",
      "epoch:0 step:204 [D loss: 0.220127, acc: 92.19%] [G loss: 4.997640]\n",
      "epoch:0 step:205 [D loss: 0.383802, acc: 83.59%] [G loss: 6.185755]\n",
      "epoch:0 step:206 [D loss: 0.235125, acc: 89.84%] [G loss: 5.536955]\n",
      "epoch:0 step:207 [D loss: 0.259150, acc: 91.41%] [G loss: 4.722014]\n",
      "epoch:0 step:208 [D loss: 0.331674, acc: 85.94%] [G loss: 6.284761]\n",
      "epoch:0 step:209 [D loss: 0.249996, acc: 89.84%] [G loss: 5.071922]\n",
      "epoch:0 step:210 [D loss: 0.460821, acc: 80.47%] [G loss: 5.584142]\n",
      "epoch:0 step:211 [D loss: 0.222902, acc: 91.41%] [G loss: 4.611708]\n",
      "epoch:0 step:212 [D loss: 0.282996, acc: 89.84%] [G loss: 4.918389]\n",
      "epoch:0 step:213 [D loss: 0.195002, acc: 91.41%] [G loss: 4.243126]\n",
      "epoch:0 step:214 [D loss: 0.380141, acc: 86.72%] [G loss: 5.263564]\n",
      "epoch:0 step:215 [D loss: 0.157419, acc: 94.53%] [G loss: 4.248158]\n",
      "epoch:0 step:216 [D loss: 0.422703, acc: 78.12%] [G loss: 5.080349]\n",
      "epoch:0 step:217 [D loss: 0.231278, acc: 91.41%] [G loss: 4.621187]\n",
      "epoch:0 step:218 [D loss: 0.257800, acc: 92.19%] [G loss: 5.324829]\n",
      "epoch:0 step:219 [D loss: 0.461992, acc: 76.56%] [G loss: 6.028549]\n",
      "epoch:0 step:220 [D loss: 0.581753, acc: 72.66%] [G loss: 4.796672]\n",
      "epoch:0 step:221 [D loss: 0.140195, acc: 96.09%] [G loss: 4.314862]\n",
      "epoch:0 step:222 [D loss: 0.595638, acc: 74.22%] [G loss: 5.832808]\n",
      "epoch:0 step:223 [D loss: 0.209348, acc: 91.41%] [G loss: 4.132602]\n",
      "epoch:0 step:224 [D loss: 0.193465, acc: 94.53%] [G loss: 4.545640]\n",
      "epoch:0 step:225 [D loss: 0.391705, acc: 82.81%] [G loss: 4.559865]\n",
      "epoch:0 step:226 [D loss: 0.298015, acc: 88.28%] [G loss: 3.740041]\n",
      "epoch:0 step:227 [D loss: 0.280502, acc: 87.50%] [G loss: 4.062807]\n",
      "epoch:0 step:228 [D loss: 0.494072, acc: 76.56%] [G loss: 4.982564]\n",
      "epoch:0 step:229 [D loss: 0.262992, acc: 87.50%] [G loss: 4.238431]\n",
      "epoch:0 step:230 [D loss: 0.626928, acc: 70.31%] [G loss: 4.240435]\n",
      "epoch:0 step:231 [D loss: 0.311683, acc: 85.94%] [G loss: 4.300263]\n",
      "epoch:0 step:232 [D loss: 0.388686, acc: 81.25%] [G loss: 4.020000]\n",
      "epoch:0 step:233 [D loss: 0.734358, acc: 61.72%] [G loss: 5.429449]\n",
      "epoch:0 step:234 [D loss: 0.256994, acc: 89.06%] [G loss: 4.176187]\n",
      "epoch:0 step:235 [D loss: 0.575874, acc: 75.00%] [G loss: 5.249095]\n",
      "epoch:0 step:236 [D loss: 0.267445, acc: 88.28%] [G loss: 4.533985]\n",
      "epoch:0 step:237 [D loss: 0.527406, acc: 78.91%] [G loss: 4.844983]\n",
      "epoch:0 step:238 [D loss: 0.237599, acc: 92.97%] [G loss: 4.807310]\n",
      "epoch:0 step:239 [D loss: 0.326888, acc: 87.50%] [G loss: 4.729004]\n",
      "epoch:0 step:240 [D loss: 0.257886, acc: 91.41%] [G loss: 4.169470]\n",
      "epoch:0 step:241 [D loss: 0.481939, acc: 84.38%] [G loss: 5.534553]\n",
      "epoch:0 step:242 [D loss: 0.305786, acc: 84.38%] [G loss: 4.186841]\n",
      "epoch:0 step:243 [D loss: 0.431079, acc: 78.12%] [G loss: 5.457615]\n",
      "epoch:0 step:244 [D loss: 0.288883, acc: 84.38%] [G loss: 4.602690]\n",
      "epoch:0 step:245 [D loss: 0.361364, acc: 86.72%] [G loss: 4.242231]\n",
      "epoch:0 step:246 [D loss: 0.333097, acc: 85.94%] [G loss: 4.635921]\n",
      "epoch:0 step:247 [D loss: 0.330190, acc: 85.16%] [G loss: 4.687617]\n",
      "epoch:0 step:248 [D loss: 0.288790, acc: 89.06%] [G loss: 4.307317]\n",
      "epoch:0 step:249 [D loss: 0.555365, acc: 68.75%] [G loss: 5.492458]\n",
      "epoch:0 step:250 [D loss: 0.174817, acc: 94.53%] [G loss: 4.174225]\n",
      "epoch:0 step:251 [D loss: 0.355670, acc: 85.16%] [G loss: 4.971240]\n",
      "epoch:0 step:252 [D loss: 0.677381, acc: 66.41%] [G loss: 5.553175]\n",
      "epoch:0 step:253 [D loss: 0.206338, acc: 94.53%] [G loss: 4.715425]\n",
      "epoch:0 step:254 [D loss: 0.422805, acc: 81.25%] [G loss: 5.763265]\n",
      "epoch:0 step:255 [D loss: 0.181724, acc: 94.53%] [G loss: 4.893459]\n",
      "epoch:0 step:256 [D loss: 0.601068, acc: 66.41%] [G loss: 7.233041]\n",
      "epoch:0 step:257 [D loss: 0.294662, acc: 82.81%] [G loss: 5.035075]\n",
      "epoch:0 step:258 [D loss: 0.287531, acc: 89.06%] [G loss: 4.882568]\n",
      "epoch:0 step:259 [D loss: 0.400065, acc: 79.69%] [G loss: 5.720631]\n",
      "epoch:0 step:260 [D loss: 0.269876, acc: 90.62%] [G loss: 5.158138]\n",
      "epoch:0 step:261 [D loss: 0.226017, acc: 94.53%] [G loss: 5.155935]\n",
      "epoch:0 step:262 [D loss: 0.271120, acc: 89.84%] [G loss: 5.199314]\n",
      "epoch:0 step:263 [D loss: 0.355976, acc: 83.59%] [G loss: 4.536366]\n",
      "epoch:0 step:264 [D loss: 0.382366, acc: 85.94%] [G loss: 5.161056]\n",
      "epoch:0 step:265 [D loss: 0.368832, acc: 87.50%] [G loss: 5.572274]\n",
      "epoch:0 step:266 [D loss: 0.293436, acc: 90.62%] [G loss: 5.650166]\n",
      "epoch:0 step:267 [D loss: 0.484546, acc: 75.00%] [G loss: 6.624451]\n",
      "epoch:0 step:268 [D loss: 0.277653, acc: 85.16%] [G loss: 4.256467]\n",
      "epoch:0 step:269 [D loss: 0.467340, acc: 75.00%] [G loss: 5.216640]\n",
      "epoch:0 step:270 [D loss: 0.231785, acc: 90.62%] [G loss: 4.474104]\n",
      "epoch:0 step:271 [D loss: 0.418144, acc: 78.91%] [G loss: 4.417583]\n",
      "epoch:0 step:272 [D loss: 0.281585, acc: 85.94%] [G loss: 4.449265]\n",
      "epoch:0 step:273 [D loss: 0.424440, acc: 80.47%] [G loss: 5.219392]\n",
      "epoch:0 step:274 [D loss: 0.341508, acc: 83.59%] [G loss: 4.237761]\n",
      "epoch:0 step:275 [D loss: 0.535812, acc: 71.88%] [G loss: 5.443643]\n",
      "epoch:0 step:276 [D loss: 0.281826, acc: 90.62%] [G loss: 4.127338]\n",
      "epoch:0 step:277 [D loss: 0.402958, acc: 84.38%] [G loss: 5.049996]\n",
      "epoch:0 step:278 [D loss: 0.244175, acc: 92.97%] [G loss: 5.342497]\n",
      "epoch:0 step:279 [D loss: 0.478526, acc: 75.00%] [G loss: 5.509289]\n",
      "epoch:0 step:280 [D loss: 0.378545, acc: 81.25%] [G loss: 4.381702]\n",
      "epoch:0 step:281 [D loss: 0.592508, acc: 75.00%] [G loss: 6.143735]\n",
      "epoch:0 step:282 [D loss: 0.247560, acc: 89.84%] [G loss: 4.372576]\n",
      "epoch:0 step:283 [D loss: 0.445035, acc: 80.47%] [G loss: 4.362956]\n",
      "epoch:0 step:284 [D loss: 0.248020, acc: 94.53%] [G loss: 4.190609]\n",
      "epoch:0 step:285 [D loss: 0.375916, acc: 87.50%] [G loss: 4.682262]\n",
      "epoch:0 step:286 [D loss: 0.269729, acc: 91.41%] [G loss: 4.512236]\n",
      "epoch:0 step:287 [D loss: 0.306025, acc: 92.19%] [G loss: 5.336968]\n",
      "epoch:0 step:288 [D loss: 0.270320, acc: 89.84%] [G loss: 4.463839]\n",
      "epoch:0 step:289 [D loss: 0.363923, acc: 89.06%] [G loss: 5.039886]\n",
      "epoch:0 step:290 [D loss: 0.312809, acc: 85.16%] [G loss: 3.976025]\n",
      "epoch:0 step:291 [D loss: 0.520087, acc: 69.53%] [G loss: 4.746134]\n",
      "epoch:0 step:292 [D loss: 0.301369, acc: 87.50%] [G loss: 3.935052]\n",
      "epoch:0 step:293 [D loss: 0.541320, acc: 75.00%] [G loss: 4.458989]\n",
      "epoch:0 step:294 [D loss: 0.345399, acc: 82.81%] [G loss: 4.065678]\n",
      "epoch:0 step:295 [D loss: 0.344267, acc: 90.62%] [G loss: 4.221362]\n",
      "epoch:0 step:296 [D loss: 0.288768, acc: 89.84%] [G loss: 4.593309]\n",
      "epoch:0 step:297 [D loss: 0.364116, acc: 82.81%] [G loss: 4.161883]\n",
      "epoch:0 step:298 [D loss: 0.303079, acc: 92.19%] [G loss: 4.347525]\n",
      "epoch:0 step:299 [D loss: 0.301821, acc: 89.84%] [G loss: 4.194936]\n",
      "epoch:0 step:300 [D loss: 0.419118, acc: 83.59%] [G loss: 4.731538]\n",
      "epoch:0 step:301 [D loss: 0.440521, acc: 76.56%] [G loss: 4.787547]\n",
      "epoch:0 step:302 [D loss: 0.378043, acc: 87.50%] [G loss: 4.333867]\n",
      "epoch:0 step:303 [D loss: 0.332535, acc: 88.28%] [G loss: 4.452267]\n",
      "epoch:0 step:304 [D loss: 0.271261, acc: 92.19%] [G loss: 4.530410]\n",
      "epoch:0 step:305 [D loss: 0.197355, acc: 96.09%] [G loss: 5.048537]\n",
      "epoch:0 step:306 [D loss: 0.447402, acc: 80.47%] [G loss: 6.352313]\n",
      "epoch:0 step:307 [D loss: 0.208323, acc: 92.97%] [G loss: 4.848814]\n",
      "epoch:0 step:308 [D loss: 0.459438, acc: 75.78%] [G loss: 5.155718]\n",
      "epoch:0 step:309 [D loss: 0.212040, acc: 93.75%] [G loss: 4.405548]\n",
      "epoch:0 step:310 [D loss: 0.406733, acc: 82.81%] [G loss: 4.599377]\n",
      "epoch:0 step:311 [D loss: 0.235673, acc: 92.19%] [G loss: 4.438180]\n",
      "epoch:0 step:312 [D loss: 0.339936, acc: 91.41%] [G loss: 4.254565]\n",
      "epoch:0 step:313 [D loss: 0.226091, acc: 92.97%] [G loss: 3.958670]\n",
      "epoch:0 step:314 [D loss: 0.411391, acc: 80.47%] [G loss: 4.509353]\n",
      "epoch:0 step:315 [D loss: 0.277881, acc: 88.28%] [G loss: 4.311208]\n",
      "epoch:0 step:316 [D loss: 0.884737, acc: 53.12%] [G loss: 4.866809]\n",
      "epoch:0 step:317 [D loss: 0.316872, acc: 88.28%] [G loss: 3.359761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:318 [D loss: 0.505825, acc: 74.22%] [G loss: 4.193446]\n",
      "epoch:0 step:319 [D loss: 0.345364, acc: 85.16%] [G loss: 3.989229]\n",
      "epoch:0 step:320 [D loss: 0.299628, acc: 92.97%] [G loss: 4.015049]\n",
      "epoch:0 step:321 [D loss: 0.437760, acc: 79.69%] [G loss: 4.857816]\n",
      "epoch:0 step:322 [D loss: 0.328084, acc: 86.72%] [G loss: 4.117125]\n",
      "epoch:0 step:323 [D loss: 0.333710, acc: 87.50%] [G loss: 4.371173]\n",
      "epoch:0 step:324 [D loss: 0.394066, acc: 81.25%] [G loss: 4.326088]\n",
      "epoch:0 step:325 [D loss: 0.356602, acc: 84.38%] [G loss: 4.444832]\n",
      "epoch:0 step:326 [D loss: 0.343740, acc: 85.94%] [G loss: 4.717916]\n",
      "epoch:0 step:327 [D loss: 0.384439, acc: 82.81%] [G loss: 4.605781]\n",
      "epoch:0 step:328 [D loss: 0.257529, acc: 95.31%] [G loss: 4.486116]\n",
      "epoch:0 step:329 [D loss: 0.414187, acc: 82.81%] [G loss: 4.842558]\n",
      "epoch:0 step:330 [D loss: 0.336596, acc: 85.16%] [G loss: 4.145716]\n",
      "epoch:0 step:331 [D loss: 0.523020, acc: 71.09%] [G loss: 4.811646]\n",
      "epoch:0 step:332 [D loss: 0.299809, acc: 89.06%] [G loss: 4.452684]\n",
      "epoch:0 step:333 [D loss: 0.418276, acc: 82.03%] [G loss: 4.768478]\n",
      "epoch:0 step:334 [D loss: 0.283833, acc: 91.41%] [G loss: 4.167904]\n",
      "epoch:0 step:335 [D loss: 0.463230, acc: 80.47%] [G loss: 4.840430]\n",
      "epoch:0 step:336 [D loss: 0.225774, acc: 95.31%] [G loss: 4.182656]\n",
      "epoch:0 step:337 [D loss: 0.478087, acc: 78.91%] [G loss: 5.248737]\n",
      "epoch:0 step:338 [D loss: 0.226569, acc: 96.88%] [G loss: 4.265474]\n",
      "epoch:0 step:339 [D loss: 0.379418, acc: 84.38%] [G loss: 4.628054]\n",
      "epoch:0 step:340 [D loss: 0.312653, acc: 87.50%] [G loss: 4.637462]\n",
      "epoch:0 step:341 [D loss: 0.608378, acc: 66.41%] [G loss: 4.653612]\n",
      "epoch:0 step:342 [D loss: 0.284128, acc: 92.19%] [G loss: 4.124795]\n",
      "epoch:0 step:343 [D loss: 0.462618, acc: 77.34%] [G loss: 5.173882]\n",
      "epoch:0 step:344 [D loss: 0.280905, acc: 89.84%] [G loss: 4.462000]\n",
      "epoch:0 step:345 [D loss: 0.487891, acc: 75.00%] [G loss: 4.310187]\n",
      "epoch:0 step:346 [D loss: 0.255166, acc: 92.97%] [G loss: 4.579715]\n",
      "epoch:0 step:347 [D loss: 0.325307, acc: 89.84%] [G loss: 4.507504]\n",
      "epoch:0 step:348 [D loss: 0.669211, acc: 68.75%] [G loss: 5.218888]\n",
      "epoch:0 step:349 [D loss: 0.367356, acc: 82.81%] [G loss: 3.822092]\n",
      "epoch:0 step:350 [D loss: 0.452026, acc: 77.34%] [G loss: 4.211917]\n",
      "epoch:0 step:351 [D loss: 0.299585, acc: 91.41%] [G loss: 4.376603]\n",
      "epoch:0 step:352 [D loss: 0.401892, acc: 78.91%] [G loss: 4.007853]\n",
      "epoch:0 step:353 [D loss: 0.469979, acc: 80.47%] [G loss: 4.415148]\n",
      "epoch:0 step:354 [D loss: 0.298513, acc: 91.41%] [G loss: 4.550197]\n",
      "epoch:0 step:355 [D loss: 0.448479, acc: 80.47%] [G loss: 4.367749]\n",
      "epoch:0 step:356 [D loss: 0.372174, acc: 84.38%] [G loss: 4.510564]\n",
      "epoch:0 step:357 [D loss: 0.404635, acc: 86.72%] [G loss: 4.136971]\n",
      "epoch:0 step:358 [D loss: 0.271802, acc: 91.41%] [G loss: 4.215734]\n",
      "epoch:0 step:359 [D loss: 0.374699, acc: 81.25%] [G loss: 4.449374]\n",
      "epoch:0 step:360 [D loss: 0.273047, acc: 92.97%] [G loss: 4.567374]\n",
      "epoch:0 step:361 [D loss: 0.416097, acc: 78.91%] [G loss: 4.551360]\n",
      "epoch:0 step:362 [D loss: 0.303318, acc: 95.31%] [G loss: 4.279515]\n",
      "epoch:0 step:363 [D loss: 0.354801, acc: 82.81%] [G loss: 4.370939]\n",
      "epoch:0 step:364 [D loss: 0.426495, acc: 83.59%] [G loss: 4.209902]\n",
      "epoch:0 step:365 [D loss: 0.457031, acc: 81.25%] [G loss: 3.817771]\n",
      "epoch:0 step:366 [D loss: 0.382369, acc: 87.50%] [G loss: 3.703798]\n",
      "epoch:0 step:367 [D loss: 0.493493, acc: 78.12%] [G loss: 4.289012]\n",
      "epoch:0 step:368 [D loss: 0.327458, acc: 86.72%] [G loss: 4.062205]\n",
      "epoch:0 step:369 [D loss: 0.397899, acc: 83.59%] [G loss: 3.708929]\n",
      "epoch:0 step:370 [D loss: 0.293591, acc: 92.19%] [G loss: 4.049085]\n",
      "epoch:0 step:371 [D loss: 0.412892, acc: 78.91%] [G loss: 3.994691]\n",
      "epoch:0 step:372 [D loss: 0.367831, acc: 86.72%] [G loss: 4.300284]\n",
      "epoch:0 step:373 [D loss: 0.483695, acc: 78.91%] [G loss: 4.120212]\n",
      "epoch:0 step:374 [D loss: 0.319955, acc: 92.97%] [G loss: 4.260796]\n",
      "epoch:0 step:375 [D loss: 0.353895, acc: 86.72%] [G loss: 4.577582]\n",
      "epoch:0 step:376 [D loss: 0.334852, acc: 86.72%] [G loss: 4.355073]\n",
      "epoch:0 step:377 [D loss: 0.289145, acc: 90.62%] [G loss: 4.637545]\n",
      "epoch:0 step:378 [D loss: 0.384493, acc: 85.16%] [G loss: 4.986959]\n",
      "epoch:0 step:379 [D loss: 0.299540, acc: 85.94%] [G loss: 4.451318]\n",
      "epoch:0 step:380 [D loss: 0.357382, acc: 86.72%] [G loss: 4.996419]\n",
      "epoch:0 step:381 [D loss: 0.333248, acc: 89.84%] [G loss: 4.474269]\n",
      "epoch:0 step:382 [D loss: 0.405369, acc: 82.03%] [G loss: 4.208043]\n",
      "epoch:0 step:383 [D loss: 0.516388, acc: 78.12%] [G loss: 4.632574]\n",
      "epoch:0 step:384 [D loss: 0.253378, acc: 93.75%] [G loss: 4.126883]\n",
      "epoch:0 step:385 [D loss: 0.479745, acc: 81.25%] [G loss: 4.389338]\n",
      "epoch:0 step:386 [D loss: 0.372214, acc: 84.38%] [G loss: 4.317858]\n",
      "epoch:0 step:387 [D loss: 0.376207, acc: 84.38%] [G loss: 4.442447]\n",
      "epoch:0 step:388 [D loss: 0.247745, acc: 96.09%] [G loss: 4.780644]\n",
      "epoch:0 step:389 [D loss: 0.427959, acc: 81.25%] [G loss: 4.423622]\n",
      "epoch:0 step:390 [D loss: 0.292981, acc: 91.41%] [G loss: 4.195386]\n",
      "epoch:0 step:391 [D loss: 0.478309, acc: 77.34%] [G loss: 4.365586]\n",
      "epoch:0 step:392 [D loss: 0.387563, acc: 79.69%] [G loss: 4.162521]\n",
      "epoch:0 step:393 [D loss: 0.466386, acc: 77.34%] [G loss: 4.200547]\n",
      "epoch:0 step:394 [D loss: 0.425094, acc: 82.81%] [G loss: 4.303619]\n",
      "epoch:0 step:395 [D loss: 0.368359, acc: 85.16%] [G loss: 4.413146]\n",
      "epoch:0 step:396 [D loss: 0.497513, acc: 84.38%] [G loss: 4.968584]\n",
      "epoch:0 step:397 [D loss: 0.266290, acc: 89.06%] [G loss: 4.939793]\n",
      "epoch:0 step:398 [D loss: 0.386431, acc: 85.94%] [G loss: 5.085539]\n",
      "epoch:0 step:399 [D loss: 0.217945, acc: 92.97%] [G loss: 4.797814]\n",
      "epoch:0 step:400 [D loss: 0.446656, acc: 79.69%] [G loss: 3.959214]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 1.571254\n",
      "FID: 242.039566\n",
      "0 = 18.61954537563325\n",
      "1 = 0.38096233834582\n",
      "2 = 1.0\n",
      "3 = 1.0\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 1.0\n",
      "7 = 15.250575467443433\n",
      "8 = 0.25314286722301915\n",
      "9 = 0.996399998664856\n",
      "10 = 0.9927999973297119\n",
      "11 = 1.0\n",
      "12 = 1.0\n",
      "13 = 0.9927999973297119\n",
      "14 = 1.5712544918060303\n",
      "15 = 6.400903224945068\n",
      "16 = 0.7321042418479919\n",
      "17 = 1.5712538957595825\n",
      "18 = 242.03956604003906\n",
      "epoch:0 step:401 [D loss: 0.284085, acc: 88.28%] [G loss: 4.360022]\n",
      "epoch:0 step:402 [D loss: 0.329101, acc: 85.94%] [G loss: 4.334204]\n",
      "epoch:0 step:403 [D loss: 0.339282, acc: 86.72%] [G loss: 4.438328]\n",
      "epoch:0 step:404 [D loss: 0.434236, acc: 74.22%] [G loss: 4.506068]\n",
      "epoch:0 step:405 [D loss: 0.372505, acc: 85.94%] [G loss: 4.137559]\n",
      "epoch:0 step:406 [D loss: 0.512162, acc: 71.88%] [G loss: 4.935168]\n",
      "epoch:0 step:407 [D loss: 0.437186, acc: 78.91%] [G loss: 4.190319]\n",
      "epoch:0 step:408 [D loss: 0.366948, acc: 89.06%] [G loss: 4.117353]\n",
      "epoch:0 step:409 [D loss: 0.414167, acc: 84.38%] [G loss: 4.725716]\n",
      "epoch:0 step:410 [D loss: 0.341231, acc: 87.50%] [G loss: 4.171576]\n",
      "epoch:0 step:411 [D loss: 0.432042, acc: 77.34%] [G loss: 4.321645]\n",
      "epoch:0 step:412 [D loss: 0.356823, acc: 83.59%] [G loss: 3.746916]\n",
      "epoch:0 step:413 [D loss: 0.409135, acc: 82.81%] [G loss: 4.810789]\n",
      "epoch:0 step:414 [D loss: 0.330822, acc: 90.62%] [G loss: 4.633635]\n",
      "epoch:0 step:415 [D loss: 0.306347, acc: 89.06%] [G loss: 4.578965]\n",
      "epoch:0 step:416 [D loss: 0.306090, acc: 85.94%] [G loss: 4.400419]\n",
      "epoch:0 step:417 [D loss: 0.357320, acc: 84.38%] [G loss: 4.176933]\n",
      "epoch:0 step:418 [D loss: 0.542718, acc: 76.56%] [G loss: 4.266422]\n",
      "epoch:0 step:419 [D loss: 0.406836, acc: 85.94%] [G loss: 4.084199]\n",
      "epoch:0 step:420 [D loss: 0.386332, acc: 82.81%] [G loss: 4.733468]\n",
      "epoch:0 step:421 [D loss: 0.334854, acc: 90.62%] [G loss: 4.280732]\n",
      "epoch:0 step:422 [D loss: 0.401002, acc: 79.69%] [G loss: 4.014947]\n",
      "epoch:0 step:423 [D loss: 0.326585, acc: 87.50%] [G loss: 4.049495]\n",
      "epoch:0 step:424 [D loss: 0.530657, acc: 73.44%] [G loss: 4.269105]\n",
      "epoch:0 step:425 [D loss: 0.310552, acc: 89.84%] [G loss: 3.942457]\n",
      "epoch:0 step:426 [D loss: 0.405296, acc: 85.94%] [G loss: 4.376278]\n",
      "epoch:0 step:427 [D loss: 0.297599, acc: 88.28%] [G loss: 4.032382]\n",
      "epoch:0 step:428 [D loss: 0.503916, acc: 72.66%] [G loss: 5.661008]\n",
      "epoch:0 step:429 [D loss: 0.244599, acc: 92.19%] [G loss: 4.149662]\n",
      "epoch:0 step:430 [D loss: 0.407264, acc: 84.38%] [G loss: 4.082455]\n",
      "epoch:0 step:431 [D loss: 0.342048, acc: 86.72%] [G loss: 4.226775]\n",
      "epoch:0 step:432 [D loss: 0.375473, acc: 85.94%] [G loss: 3.878607]\n",
      "epoch:0 step:433 [D loss: 0.463131, acc: 78.12%] [G loss: 4.184756]\n",
      "epoch:0 step:434 [D loss: 0.445594, acc: 81.25%] [G loss: 3.922102]\n",
      "epoch:0 step:435 [D loss: 0.366348, acc: 85.94%] [G loss: 4.020252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:436 [D loss: 0.467300, acc: 75.78%] [G loss: 4.111886]\n",
      "epoch:0 step:437 [D loss: 0.444603, acc: 78.12%] [G loss: 4.315371]\n",
      "epoch:0 step:438 [D loss: 0.364427, acc: 86.72%] [G loss: 4.778986]\n",
      "epoch:0 step:439 [D loss: 0.446214, acc: 80.47%] [G loss: 4.700822]\n",
      "epoch:0 step:440 [D loss: 0.409822, acc: 84.38%] [G loss: 4.287948]\n",
      "epoch:0 step:441 [D loss: 0.461667, acc: 79.69%] [G loss: 4.536352]\n",
      "epoch:0 step:442 [D loss: 0.424791, acc: 81.25%] [G loss: 4.385532]\n",
      "epoch:0 step:443 [D loss: 0.366223, acc: 86.72%] [G loss: 4.503416]\n",
      "epoch:0 step:444 [D loss: 0.290842, acc: 89.84%] [G loss: 5.029062]\n",
      "epoch:0 step:445 [D loss: 0.442096, acc: 76.56%] [G loss: 4.475016]\n",
      "epoch:0 step:446 [D loss: 0.508428, acc: 76.56%] [G loss: 4.459172]\n",
      "epoch:0 step:447 [D loss: 0.335857, acc: 87.50%] [G loss: 4.863635]\n",
      "epoch:0 step:448 [D loss: 0.466094, acc: 78.91%] [G loss: 4.468387]\n",
      "epoch:0 step:449 [D loss: 0.337003, acc: 84.38%] [G loss: 4.363524]\n",
      "epoch:0 step:450 [D loss: 0.253178, acc: 92.97%] [G loss: 4.516683]\n",
      "epoch:0 step:451 [D loss: 0.480344, acc: 78.91%] [G loss: 4.339935]\n",
      "epoch:0 step:452 [D loss: 0.332451, acc: 89.06%] [G loss: 4.374437]\n",
      "epoch:0 step:453 [D loss: 0.437095, acc: 78.12%] [G loss: 3.900627]\n",
      "epoch:0 step:454 [D loss: 0.532484, acc: 73.44%] [G loss: 4.736455]\n",
      "epoch:0 step:455 [D loss: 0.424429, acc: 83.59%] [G loss: 4.225230]\n",
      "epoch:0 step:456 [D loss: 0.398556, acc: 82.81%] [G loss: 3.937440]\n",
      "epoch:0 step:457 [D loss: 0.467006, acc: 80.47%] [G loss: 4.233072]\n",
      "epoch:0 step:458 [D loss: 0.444956, acc: 83.59%] [G loss: 4.327431]\n",
      "epoch:0 step:459 [D loss: 0.378237, acc: 83.59%] [G loss: 4.497176]\n",
      "epoch:0 step:460 [D loss: 0.400102, acc: 77.34%] [G loss: 4.859897]\n",
      "epoch:0 step:461 [D loss: 0.387605, acc: 82.81%] [G loss: 4.302709]\n",
      "epoch:0 step:462 [D loss: 0.404606, acc: 85.16%] [G loss: 3.787714]\n",
      "epoch:0 step:463 [D loss: 0.396942, acc: 82.03%] [G loss: 4.077518]\n",
      "epoch:0 step:464 [D loss: 0.480577, acc: 73.44%] [G loss: 4.537873]\n",
      "epoch:0 step:465 [D loss: 0.374762, acc: 86.72%] [G loss: 4.212763]\n",
      "epoch:0 step:466 [D loss: 0.428411, acc: 80.47%] [G loss: 4.260019]\n",
      "epoch:0 step:467 [D loss: 0.570315, acc: 72.66%] [G loss: 4.048573]\n",
      "epoch:0 step:468 [D loss: 0.426501, acc: 82.81%] [G loss: 4.358578]\n",
      "epoch:0 step:469 [D loss: 0.478523, acc: 77.34%] [G loss: 4.071946]\n",
      "epoch:0 step:470 [D loss: 0.425569, acc: 79.69%] [G loss: 4.366977]\n",
      "epoch:0 step:471 [D loss: 0.331986, acc: 89.06%] [G loss: 4.824620]\n",
      "epoch:0 step:472 [D loss: 0.492842, acc: 77.34%] [G loss: 4.211067]\n",
      "epoch:0 step:473 [D loss: 0.593395, acc: 67.19%] [G loss: 3.785375]\n",
      "epoch:0 step:474 [D loss: 0.311664, acc: 88.28%] [G loss: 3.669217]\n",
      "epoch:0 step:475 [D loss: 0.353415, acc: 84.38%] [G loss: 4.111710]\n",
      "epoch:0 step:476 [D loss: 0.610388, acc: 71.09%] [G loss: 4.019162]\n",
      "epoch:0 step:477 [D loss: 0.480590, acc: 77.34%] [G loss: 4.283053]\n",
      "epoch:0 step:478 [D loss: 0.489214, acc: 78.91%] [G loss: 4.185216]\n",
      "epoch:0 step:479 [D loss: 0.391180, acc: 84.38%] [G loss: 4.047253]\n",
      "epoch:0 step:480 [D loss: 0.452527, acc: 78.91%] [G loss: 4.402523]\n",
      "epoch:0 step:481 [D loss: 0.436841, acc: 80.47%] [G loss: 4.438774]\n",
      "epoch:0 step:482 [D loss: 0.595749, acc: 64.84%] [G loss: 3.640939]\n",
      "epoch:0 step:483 [D loss: 0.379366, acc: 85.16%] [G loss: 3.384131]\n",
      "epoch:0 step:484 [D loss: 0.368054, acc: 86.72%] [G loss: 3.507308]\n",
      "epoch:0 step:485 [D loss: 0.492088, acc: 74.22%] [G loss: 3.680907]\n",
      "epoch:0 step:486 [D loss: 0.468172, acc: 80.47%] [G loss: 3.717083]\n",
      "epoch:0 step:487 [D loss: 0.407172, acc: 84.38%] [G loss: 3.666613]\n",
      "epoch:0 step:488 [D loss: 0.409040, acc: 84.38%] [G loss: 4.228027]\n",
      "epoch:0 step:489 [D loss: 0.529217, acc: 71.88%] [G loss: 3.854421]\n",
      "epoch:0 step:490 [D loss: 0.424949, acc: 82.03%] [G loss: 4.020036]\n",
      "epoch:0 step:491 [D loss: 0.504312, acc: 76.56%] [G loss: 3.994005]\n",
      "epoch:0 step:492 [D loss: 0.350655, acc: 87.50%] [G loss: 3.584515]\n",
      "epoch:0 step:493 [D loss: 0.493249, acc: 78.12%] [G loss: 3.770029]\n",
      "epoch:0 step:494 [D loss: 0.540373, acc: 72.66%] [G loss: 3.531175]\n",
      "epoch:0 step:495 [D loss: 0.433102, acc: 82.03%] [G loss: 3.817043]\n",
      "epoch:0 step:496 [D loss: 0.553240, acc: 70.31%] [G loss: 3.651534]\n",
      "epoch:0 step:497 [D loss: 0.355188, acc: 86.72%] [G loss: 3.674371]\n",
      "epoch:0 step:498 [D loss: 0.385023, acc: 85.94%] [G loss: 3.588990]\n",
      "epoch:0 step:499 [D loss: 0.383926, acc: 82.03%] [G loss: 4.187740]\n",
      "epoch:0 step:500 [D loss: 0.483954, acc: 75.00%] [G loss: 3.558148]\n",
      "epoch:0 step:501 [D loss: 0.465706, acc: 76.56%] [G loss: 3.683460]\n",
      "epoch:0 step:502 [D loss: 0.494010, acc: 79.69%] [G loss: 3.750209]\n",
      "epoch:0 step:503 [D loss: 0.397024, acc: 82.81%] [G loss: 3.833952]\n",
      "epoch:0 step:504 [D loss: 0.323874, acc: 86.72%] [G loss: 3.994437]\n",
      "epoch:0 step:505 [D loss: 0.433849, acc: 77.34%] [G loss: 3.788233]\n",
      "epoch:0 step:506 [D loss: 0.417745, acc: 82.03%] [G loss: 3.939571]\n",
      "epoch:0 step:507 [D loss: 0.561295, acc: 69.53%] [G loss: 4.254439]\n",
      "epoch:0 step:508 [D loss: 0.466508, acc: 75.78%] [G loss: 3.884648]\n",
      "epoch:0 step:509 [D loss: 0.525440, acc: 75.78%] [G loss: 3.881061]\n",
      "epoch:0 step:510 [D loss: 0.491491, acc: 74.22%] [G loss: 3.658375]\n",
      "epoch:0 step:511 [D loss: 0.448934, acc: 78.12%] [G loss: 3.498497]\n",
      "epoch:0 step:512 [D loss: 0.503892, acc: 71.88%] [G loss: 3.455914]\n",
      "epoch:0 step:513 [D loss: 0.604481, acc: 68.75%] [G loss: 3.432564]\n",
      "epoch:0 step:514 [D loss: 0.499745, acc: 75.00%] [G loss: 3.818813]\n",
      "epoch:0 step:515 [D loss: 0.423024, acc: 86.72%] [G loss: 3.673161]\n",
      "epoch:0 step:516 [D loss: 0.480693, acc: 77.34%] [G loss: 3.343398]\n",
      "epoch:0 step:517 [D loss: 0.805001, acc: 57.03%] [G loss: 3.520540]\n",
      "epoch:0 step:518 [D loss: 0.420469, acc: 78.91%] [G loss: 3.446697]\n",
      "epoch:0 step:519 [D loss: 0.496683, acc: 75.78%] [G loss: 3.700261]\n",
      "epoch:0 step:520 [D loss: 0.465802, acc: 80.47%] [G loss: 3.422639]\n",
      "epoch:0 step:521 [D loss: 0.471703, acc: 78.12%] [G loss: 3.547791]\n",
      "epoch:0 step:522 [D loss: 0.606033, acc: 74.22%] [G loss: 3.140941]\n",
      "epoch:0 step:523 [D loss: 0.482191, acc: 82.03%] [G loss: 3.556759]\n",
      "epoch:0 step:524 [D loss: 0.561043, acc: 72.66%] [G loss: 3.429124]\n",
      "epoch:0 step:525 [D loss: 0.494724, acc: 75.78%] [G loss: 3.246400]\n",
      "epoch:0 step:526 [D loss: 0.476656, acc: 77.34%] [G loss: 3.041695]\n",
      "epoch:0 step:527 [D loss: 0.770597, acc: 57.81%] [G loss: 3.384189]\n",
      "epoch:0 step:528 [D loss: 0.453246, acc: 82.03%] [G loss: 3.412964]\n",
      "epoch:0 step:529 [D loss: 0.444636, acc: 82.81%] [G loss: 3.494178]\n",
      "epoch:0 step:530 [D loss: 0.509509, acc: 76.56%] [G loss: 3.953904]\n",
      "epoch:0 step:531 [D loss: 0.570841, acc: 62.50%] [G loss: 3.218373]\n",
      "epoch:0 step:532 [D loss: 0.521033, acc: 72.66%] [G loss: 3.344749]\n",
      "epoch:0 step:533 [D loss: 0.552664, acc: 70.31%] [G loss: 3.483542]\n",
      "epoch:0 step:534 [D loss: 0.460242, acc: 79.69%] [G loss: 3.473012]\n",
      "epoch:0 step:535 [D loss: 0.490519, acc: 78.12%] [G loss: 3.365024]\n",
      "epoch:0 step:536 [D loss: 0.404519, acc: 85.16%] [G loss: 3.112954]\n",
      "epoch:0 step:537 [D loss: 0.479946, acc: 75.00%] [G loss: 3.312155]\n",
      "epoch:0 step:538 [D loss: 0.499593, acc: 74.22%] [G loss: 3.547347]\n",
      "epoch:0 step:539 [D loss: 0.541508, acc: 69.53%] [G loss: 2.981281]\n",
      "epoch:0 step:540 [D loss: 0.555806, acc: 70.31%] [G loss: 3.308083]\n",
      "epoch:0 step:541 [D loss: 0.544091, acc: 68.75%] [G loss: 3.279922]\n",
      "epoch:0 step:542 [D loss: 0.576536, acc: 70.31%] [G loss: 3.022048]\n",
      "epoch:0 step:543 [D loss: 0.567316, acc: 71.09%] [G loss: 3.365364]\n",
      "epoch:0 step:544 [D loss: 0.482279, acc: 78.91%] [G loss: 3.418018]\n",
      "epoch:0 step:545 [D loss: 0.495155, acc: 78.12%] [G loss: 3.599020]\n",
      "epoch:0 step:546 [D loss: 0.443607, acc: 78.91%] [G loss: 3.178357]\n",
      "epoch:0 step:547 [D loss: 0.568043, acc: 74.22%] [G loss: 3.582459]\n",
      "epoch:0 step:548 [D loss: 0.465992, acc: 80.47%] [G loss: 3.361317]\n",
      "epoch:0 step:549 [D loss: 0.424521, acc: 82.03%] [G loss: 3.023335]\n",
      "epoch:0 step:550 [D loss: 0.521936, acc: 77.34%] [G loss: 3.314922]\n",
      "epoch:0 step:551 [D loss: 0.551900, acc: 70.31%] [G loss: 3.342988]\n",
      "epoch:0 step:552 [D loss: 0.438039, acc: 78.91%] [G loss: 3.228512]\n",
      "epoch:0 step:553 [D loss: 0.655242, acc: 63.28%] [G loss: 3.209688]\n",
      "epoch:0 step:554 [D loss: 0.459314, acc: 82.03%] [G loss: 3.353935]\n",
      "epoch:0 step:555 [D loss: 0.445058, acc: 78.91%] [G loss: 3.398804]\n",
      "epoch:0 step:556 [D loss: 0.500863, acc: 78.91%] [G loss: 3.419874]\n",
      "epoch:0 step:557 [D loss: 0.502227, acc: 75.00%] [G loss: 3.201116]\n",
      "epoch:0 step:558 [D loss: 0.394111, acc: 85.16%] [G loss: 3.678709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:559 [D loss: 0.724382, acc: 57.81%] [G loss: 3.326401]\n",
      "epoch:0 step:560 [D loss: 0.505713, acc: 78.12%] [G loss: 3.127384]\n",
      "epoch:0 step:561 [D loss: 0.460916, acc: 82.03%] [G loss: 2.985786]\n",
      "epoch:0 step:562 [D loss: 0.618985, acc: 60.94%] [G loss: 3.149721]\n",
      "epoch:0 step:563 [D loss: 0.531130, acc: 75.00%] [G loss: 3.096847]\n",
      "epoch:0 step:564 [D loss: 0.413045, acc: 80.47%] [G loss: 3.394971]\n",
      "epoch:0 step:565 [D loss: 0.669342, acc: 66.41%] [G loss: 3.317565]\n",
      "epoch:0 step:566 [D loss: 0.590069, acc: 67.19%] [G loss: 3.373470]\n",
      "epoch:0 step:567 [D loss: 0.356954, acc: 88.28%] [G loss: 3.238988]\n",
      "epoch:0 step:568 [D loss: 0.651898, acc: 61.72%] [G loss: 3.545393]\n",
      "epoch:0 step:569 [D loss: 0.607079, acc: 64.84%] [G loss: 2.977820]\n",
      "epoch:0 step:570 [D loss: 0.512604, acc: 75.78%] [G loss: 2.788816]\n",
      "epoch:0 step:571 [D loss: 0.477201, acc: 76.56%] [G loss: 3.012520]\n",
      "epoch:0 step:572 [D loss: 0.588840, acc: 66.41%] [G loss: 2.967237]\n",
      "epoch:0 step:573 [D loss: 0.411843, acc: 82.81%] [G loss: 3.425305]\n",
      "epoch:0 step:574 [D loss: 0.472605, acc: 77.34%] [G loss: 3.302147]\n",
      "epoch:0 step:575 [D loss: 0.414775, acc: 83.59%] [G loss: 3.520656]\n",
      "epoch:0 step:576 [D loss: 0.563747, acc: 74.22%] [G loss: 2.982869]\n",
      "epoch:0 step:577 [D loss: 0.542584, acc: 77.34%] [G loss: 2.982919]\n",
      "epoch:0 step:578 [D loss: 0.530220, acc: 73.44%] [G loss: 2.846118]\n",
      "epoch:0 step:579 [D loss: 0.479441, acc: 78.12%] [G loss: 3.230880]\n",
      "epoch:0 step:580 [D loss: 0.528041, acc: 69.53%] [G loss: 3.135534]\n",
      "epoch:0 step:581 [D loss: 0.398521, acc: 78.12%] [G loss: 3.536608]\n",
      "epoch:0 step:582 [D loss: 0.472747, acc: 82.81%] [G loss: 3.748509]\n",
      "epoch:0 step:583 [D loss: 0.452443, acc: 82.03%] [G loss: 3.344705]\n",
      "epoch:0 step:584 [D loss: 0.546019, acc: 68.75%] [G loss: 3.020858]\n",
      "epoch:0 step:585 [D loss: 0.558671, acc: 71.09%] [G loss: 3.104878]\n",
      "epoch:0 step:586 [D loss: 0.530908, acc: 75.78%] [G loss: 3.014469]\n",
      "epoch:0 step:587 [D loss: 0.522428, acc: 78.12%] [G loss: 3.095790]\n",
      "epoch:0 step:588 [D loss: 0.508801, acc: 70.31%] [G loss: 3.429712]\n",
      "epoch:0 step:589 [D loss: 0.450530, acc: 79.69%] [G loss: 3.522154]\n",
      "epoch:0 step:590 [D loss: 0.682592, acc: 65.62%] [G loss: 3.146171]\n",
      "epoch:0 step:591 [D loss: 0.540495, acc: 74.22%] [G loss: 2.980716]\n",
      "epoch:0 step:592 [D loss: 0.581307, acc: 64.84%] [G loss: 3.109802]\n",
      "epoch:0 step:593 [D loss: 0.564680, acc: 73.44%] [G loss: 2.976653]\n",
      "epoch:0 step:594 [D loss: 0.488747, acc: 78.91%] [G loss: 3.024399]\n",
      "epoch:0 step:595 [D loss: 0.536307, acc: 71.88%] [G loss: 3.114724]\n",
      "epoch:0 step:596 [D loss: 0.588942, acc: 69.53%] [G loss: 2.987071]\n",
      "epoch:0 step:597 [D loss: 0.482187, acc: 71.88%] [G loss: 3.120641]\n",
      "epoch:0 step:598 [D loss: 0.460812, acc: 80.47%] [G loss: 3.105040]\n",
      "epoch:0 step:599 [D loss: 0.523624, acc: 74.22%] [G loss: 3.090490]\n",
      "epoch:0 step:600 [D loss: 0.574457, acc: 67.97%] [G loss: 3.045535]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 2.906403\n",
      "FID: 154.963257\n",
      "0 = 15.002722301292337\n",
      "1 = 0.24337722443444268\n",
      "2 = 0.9961000084877014\n",
      "3 = 0.9922000169754028\n",
      "4 = 1.0\n",
      "5 = 1.0\n",
      "6 = 0.9922000169754028\n",
      "7 = 12.914557813787459\n",
      "8 = 0.2241269445382826\n",
      "9 = 0.9807999730110168\n",
      "10 = 0.9649999737739563\n",
      "11 = 0.9965999722480774\n",
      "12 = 0.9964890480041504\n",
      "13 = 0.9649999737739563\n",
      "14 = 2.906405448913574\n",
      "15 = 7.590076446533203\n",
      "16 = 0.4826127886772156\n",
      "17 = 2.906402587890625\n",
      "18 = 154.9632568359375\n",
      "epoch:0 step:601 [D loss: 0.544936, acc: 75.78%] [G loss: 3.061544]\n",
      "epoch:0 step:602 [D loss: 0.533214, acc: 75.00%] [G loss: 3.116810]\n",
      "epoch:0 step:603 [D loss: 0.691094, acc: 57.03%] [G loss: 2.741992]\n",
      "epoch:0 step:604 [D loss: 0.501654, acc: 75.78%] [G loss: 3.043413]\n",
      "epoch:0 step:605 [D loss: 0.502501, acc: 74.22%] [G loss: 3.002053]\n",
      "epoch:0 step:606 [D loss: 0.540255, acc: 75.00%] [G loss: 3.046847]\n",
      "epoch:0 step:607 [D loss: 0.551478, acc: 72.66%] [G loss: 2.913223]\n",
      "epoch:0 step:608 [D loss: 0.492525, acc: 75.78%] [G loss: 3.122760]\n",
      "epoch:0 step:609 [D loss: 0.562715, acc: 65.62%] [G loss: 3.303449]\n",
      "epoch:0 step:610 [D loss: 0.508742, acc: 74.22%] [G loss: 3.066028]\n",
      "epoch:0 step:611 [D loss: 0.507396, acc: 75.78%] [G loss: 3.097117]\n",
      "epoch:0 step:612 [D loss: 0.570573, acc: 68.75%] [G loss: 3.222623]\n",
      "epoch:0 step:613 [D loss: 0.498394, acc: 75.00%] [G loss: 3.154089]\n",
      "epoch:0 step:614 [D loss: 0.523911, acc: 74.22%] [G loss: 3.004298]\n",
      "epoch:0 step:615 [D loss: 0.540077, acc: 70.31%] [G loss: 2.971515]\n",
      "epoch:0 step:616 [D loss: 0.458140, acc: 76.56%] [G loss: 3.218074]\n",
      "epoch:0 step:617 [D loss: 0.473758, acc: 75.78%] [G loss: 3.105068]\n",
      "epoch:0 step:618 [D loss: 0.413959, acc: 85.94%] [G loss: 3.274376]\n",
      "epoch:0 step:619 [D loss: 0.441241, acc: 79.69%] [G loss: 3.219713]\n",
      "epoch:0 step:620 [D loss: 0.443933, acc: 78.12%] [G loss: 3.174312]\n",
      "epoch:0 step:621 [D loss: 0.737082, acc: 53.91%] [G loss: 2.889011]\n",
      "epoch:0 step:622 [D loss: 0.483234, acc: 79.69%] [G loss: 2.981322]\n",
      "epoch:0 step:623 [D loss: 0.506383, acc: 72.66%] [G loss: 3.170482]\n",
      "epoch:0 step:624 [D loss: 0.530254, acc: 75.78%] [G loss: 3.201380]\n",
      "epoch:0 step:625 [D loss: 0.652558, acc: 63.28%] [G loss: 3.068891]\n",
      "epoch:0 step:626 [D loss: 0.524771, acc: 73.44%] [G loss: 2.801369]\n",
      "epoch:0 step:627 [D loss: 0.570443, acc: 72.66%] [G loss: 3.223446]\n",
      "epoch:0 step:628 [D loss: 0.478363, acc: 75.00%] [G loss: 2.980313]\n",
      "epoch:0 step:629 [D loss: 0.704882, acc: 61.72%] [G loss: 3.067254]\n",
      "epoch:0 step:630 [D loss: 0.522501, acc: 72.66%] [G loss: 3.050228]\n",
      "epoch:0 step:631 [D loss: 0.454084, acc: 82.03%] [G loss: 3.107045]\n",
      "epoch:0 step:632 [D loss: 0.549792, acc: 74.22%] [G loss: 2.963788]\n",
      "epoch:0 step:633 [D loss: 0.447022, acc: 78.91%] [G loss: 3.381745]\n",
      "epoch:0 step:634 [D loss: 0.507485, acc: 73.44%] [G loss: 3.198118]\n",
      "epoch:0 step:635 [D loss: 0.486915, acc: 80.47%] [G loss: 3.047625]\n",
      "epoch:0 step:636 [D loss: 0.556900, acc: 71.09%] [G loss: 3.195331]\n",
      "epoch:0 step:637 [D loss: 0.532794, acc: 75.78%] [G loss: 3.167063]\n",
      "epoch:0 step:638 [D loss: 0.537940, acc: 75.00%] [G loss: 3.231947]\n",
      "epoch:0 step:639 [D loss: 0.584030, acc: 67.19%] [G loss: 2.844583]\n",
      "epoch:0 step:640 [D loss: 0.533263, acc: 71.09%] [G loss: 3.042362]\n",
      "epoch:0 step:641 [D loss: 0.439561, acc: 79.69%] [G loss: 2.994615]\n",
      "epoch:0 step:642 [D loss: 0.432992, acc: 82.03%] [G loss: 3.240288]\n",
      "epoch:0 step:643 [D loss: 0.628203, acc: 68.75%] [G loss: 3.201748]\n",
      "epoch:0 step:644 [D loss: 0.592528, acc: 67.97%] [G loss: 3.015506]\n",
      "epoch:0 step:645 [D loss: 0.769628, acc: 56.25%] [G loss: 2.964021]\n",
      "epoch:0 step:646 [D loss: 0.492461, acc: 77.34%] [G loss: 2.992384]\n",
      "epoch:0 step:647 [D loss: 0.541128, acc: 77.34%] [G loss: 2.894384]\n",
      "epoch:0 step:648 [D loss: 0.529420, acc: 74.22%] [G loss: 3.407415]\n",
      "epoch:0 step:649 [D loss: 0.505677, acc: 75.00%] [G loss: 3.135273]\n",
      "epoch:0 step:650 [D loss: 0.508974, acc: 73.44%] [G loss: 3.191841]\n",
      "epoch:0 step:651 [D loss: 0.497770, acc: 75.78%] [G loss: 3.191336]\n",
      "epoch:0 step:652 [D loss: 0.572048, acc: 69.53%] [G loss: 2.914582]\n",
      "epoch:0 step:653 [D loss: 0.576409, acc: 65.62%] [G loss: 3.429824]\n",
      "epoch:0 step:654 [D loss: 0.440759, acc: 83.59%] [G loss: 2.957657]\n",
      "epoch:0 step:655 [D loss: 0.628446, acc: 67.19%] [G loss: 2.826997]\n",
      "epoch:0 step:656 [D loss: 0.510361, acc: 73.44%] [G loss: 3.130958]\n",
      "epoch:0 step:657 [D loss: 0.496103, acc: 75.00%] [G loss: 3.053960]\n",
      "epoch:0 step:658 [D loss: 0.526074, acc: 75.78%] [G loss: 2.786554]\n",
      "epoch:0 step:659 [D loss: 0.443686, acc: 78.91%] [G loss: 3.449389]\n",
      "epoch:0 step:660 [D loss: 0.498937, acc: 75.00%] [G loss: 3.442264]\n",
      "epoch:0 step:661 [D loss: 0.433847, acc: 76.56%] [G loss: 3.180341]\n",
      "epoch:0 step:662 [D loss: 0.587450, acc: 65.62%] [G loss: 2.950383]\n",
      "epoch:0 step:663 [D loss: 0.523641, acc: 72.66%] [G loss: 2.882599]\n",
      "epoch:0 step:664 [D loss: 0.489018, acc: 77.34%] [G loss: 3.240628]\n",
      "epoch:0 step:665 [D loss: 0.438573, acc: 79.69%] [G loss: 3.346342]\n",
      "epoch:0 step:666 [D loss: 0.561842, acc: 75.00%] [G loss: 3.587656]\n",
      "epoch:0 step:667 [D loss: 0.509144, acc: 71.09%] [G loss: 3.275402]\n",
      "epoch:0 step:668 [D loss: 0.572200, acc: 67.19%] [G loss: 3.106675]\n",
      "epoch:0 step:669 [D loss: 0.531009, acc: 69.53%] [G loss: 3.109424]\n",
      "epoch:0 step:670 [D loss: 0.550587, acc: 71.88%] [G loss: 3.033144]\n",
      "epoch:0 step:671 [D loss: 0.577634, acc: 65.62%] [G loss: 2.834183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:672 [D loss: 0.602066, acc: 67.19%] [G loss: 3.036819]\n",
      "epoch:0 step:673 [D loss: 0.580684, acc: 68.75%] [G loss: 2.979302]\n",
      "epoch:0 step:674 [D loss: 0.546807, acc: 67.97%] [G loss: 3.116378]\n",
      "epoch:0 step:675 [D loss: 0.504546, acc: 73.44%] [G loss: 3.425170]\n",
      "epoch:0 step:676 [D loss: 0.626136, acc: 66.41%] [G loss: 2.831759]\n",
      "epoch:0 step:677 [D loss: 0.565100, acc: 68.75%] [G loss: 3.018270]\n",
      "epoch:0 step:678 [D loss: 0.555428, acc: 72.66%] [G loss: 3.117193]\n",
      "epoch:0 step:679 [D loss: 0.580530, acc: 68.75%] [G loss: 3.093078]\n",
      "epoch:0 step:680 [D loss: 0.418523, acc: 81.25%] [G loss: 2.891939]\n",
      "epoch:0 step:681 [D loss: 0.559949, acc: 75.00%] [G loss: 3.010657]\n",
      "epoch:0 step:682 [D loss: 0.615754, acc: 65.62%] [G loss: 2.950319]\n",
      "epoch:0 step:683 [D loss: 0.468720, acc: 78.12%] [G loss: 3.012445]\n",
      "epoch:0 step:684 [D loss: 0.409742, acc: 84.38%] [G loss: 3.071218]\n",
      "epoch:0 step:685 [D loss: 0.564070, acc: 74.22%] [G loss: 2.916350]\n",
      "epoch:0 step:686 [D loss: 0.562902, acc: 69.53%] [G loss: 2.967710]\n",
      "epoch:0 step:687 [D loss: 0.539619, acc: 73.44%] [G loss: 3.113076]\n",
      "epoch:0 step:688 [D loss: 0.573631, acc: 71.09%] [G loss: 3.214835]\n",
      "epoch:0 step:689 [D loss: 0.498424, acc: 81.25%] [G loss: 3.049721]\n",
      "epoch:0 step:690 [D loss: 0.486166, acc: 74.22%] [G loss: 3.153431]\n",
      "epoch:0 step:691 [D loss: 0.610679, acc: 65.62%] [G loss: 2.884464]\n",
      "epoch:0 step:692 [D loss: 0.464443, acc: 78.91%] [G loss: 2.949382]\n",
      "epoch:0 step:693 [D loss: 0.681087, acc: 62.50%] [G loss: 3.526671]\n",
      "epoch:0 step:694 [D loss: 0.515242, acc: 71.88%] [G loss: 3.097156]\n",
      "epoch:0 step:695 [D loss: 0.515109, acc: 71.09%] [G loss: 2.754752]\n",
      "epoch:0 step:696 [D loss: 0.507285, acc: 76.56%] [G loss: 3.190432]\n",
      "epoch:0 step:697 [D loss: 0.534733, acc: 72.66%] [G loss: 3.009274]\n",
      "epoch:0 step:698 [D loss: 0.647358, acc: 59.38%] [G loss: 2.934349]\n",
      "epoch:0 step:699 [D loss: 0.533319, acc: 72.66%] [G loss: 2.827285]\n",
      "epoch:0 step:700 [D loss: 0.432137, acc: 79.69%] [G loss: 3.057171]\n",
      "epoch:0 step:701 [D loss: 0.528564, acc: 70.31%] [G loss: 3.066806]\n",
      "epoch:0 step:702 [D loss: 0.583266, acc: 67.97%] [G loss: 3.108825]\n",
      "epoch:0 step:703 [D loss: 0.528589, acc: 71.88%] [G loss: 3.063135]\n",
      "epoch:0 step:704 [D loss: 0.550807, acc: 70.31%] [G loss: 3.053646]\n",
      "epoch:0 step:705 [D loss: 0.494946, acc: 78.91%] [G loss: 3.016391]\n",
      "epoch:0 step:706 [D loss: 0.557054, acc: 73.44%] [G loss: 3.303561]\n",
      "epoch:0 step:707 [D loss: 0.388201, acc: 82.03%] [G loss: 3.521748]\n",
      "epoch:0 step:708 [D loss: 0.519614, acc: 75.78%] [G loss: 3.729210]\n",
      "epoch:0 step:709 [D loss: 0.403815, acc: 83.59%] [G loss: 3.077713]\n",
      "epoch:0 step:710 [D loss: 0.763948, acc: 57.81%] [G loss: 2.949020]\n",
      "epoch:0 step:711 [D loss: 0.639182, acc: 64.84%] [G loss: 2.800188]\n",
      "epoch:0 step:712 [D loss: 0.552460, acc: 69.53%] [G loss: 2.927749]\n",
      "epoch:0 step:713 [D loss: 0.532085, acc: 73.44%] [G loss: 3.199246]\n",
      "epoch:0 step:714 [D loss: 0.687728, acc: 60.16%] [G loss: 2.851647]\n",
      "epoch:0 step:715 [D loss: 0.530932, acc: 76.56%] [G loss: 2.579986]\n",
      "epoch:0 step:716 [D loss: 0.545593, acc: 68.75%] [G loss: 2.735287]\n",
      "epoch:0 step:717 [D loss: 0.531084, acc: 75.78%] [G loss: 2.729618]\n",
      "epoch:0 step:718 [D loss: 0.669810, acc: 57.03%] [G loss: 2.658907]\n",
      "epoch:0 step:719 [D loss: 0.576229, acc: 68.75%] [G loss: 2.780138]\n",
      "epoch:0 step:720 [D loss: 0.614797, acc: 71.09%] [G loss: 2.725462]\n",
      "epoch:0 step:721 [D loss: 0.490061, acc: 83.59%] [G loss: 2.968568]\n",
      "epoch:0 step:722 [D loss: 0.620408, acc: 71.09%] [G loss: 2.927422]\n",
      "epoch:0 step:723 [D loss: 0.520861, acc: 74.22%] [G loss: 2.787095]\n",
      "epoch:0 step:724 [D loss: 0.561013, acc: 71.09%] [G loss: 2.813249]\n",
      "epoch:0 step:725 [D loss: 0.615605, acc: 65.62%] [G loss: 2.547771]\n",
      "epoch:0 step:726 [D loss: 0.591980, acc: 65.62%] [G loss: 2.890769]\n",
      "epoch:0 step:727 [D loss: 0.591277, acc: 64.06%] [G loss: 2.696620]\n",
      "epoch:0 step:728 [D loss: 0.573726, acc: 71.88%] [G loss: 2.955081]\n",
      "epoch:0 step:729 [D loss: 0.618318, acc: 71.88%] [G loss: 2.994666]\n",
      "epoch:0 step:730 [D loss: 0.584389, acc: 68.75%] [G loss: 2.899849]\n",
      "epoch:0 step:731 [D loss: 0.552657, acc: 77.34%] [G loss: 2.869505]\n",
      "epoch:0 step:732 [D loss: 0.536577, acc: 69.53%] [G loss: 3.071740]\n",
      "epoch:0 step:733 [D loss: 0.452632, acc: 80.47%] [G loss: 2.913379]\n",
      "epoch:0 step:734 [D loss: 0.679748, acc: 63.28%] [G loss: 2.742465]\n",
      "epoch:0 step:735 [D loss: 0.515477, acc: 76.56%] [G loss: 2.784064]\n",
      "epoch:0 step:736 [D loss: 0.445475, acc: 82.81%] [G loss: 2.786416]\n",
      "epoch:0 step:737 [D loss: 0.554440, acc: 75.78%] [G loss: 2.743817]\n",
      "epoch:0 step:738 [D loss: 0.613003, acc: 65.62%] [G loss: 2.698656]\n",
      "epoch:0 step:739 [D loss: 0.549148, acc: 70.31%] [G loss: 2.830334]\n",
      "epoch:0 step:740 [D loss: 0.621234, acc: 60.94%] [G loss: 2.798350]\n",
      "epoch:0 step:741 [D loss: 0.493259, acc: 75.00%] [G loss: 3.147200]\n",
      "epoch:0 step:742 [D loss: 0.578734, acc: 67.97%] [G loss: 2.923895]\n",
      "epoch:0 step:743 [D loss: 0.540177, acc: 73.44%] [G loss: 2.864116]\n",
      "epoch:0 step:744 [D loss: 0.506703, acc: 78.91%] [G loss: 2.934335]\n",
      "epoch:0 step:745 [D loss: 0.575542, acc: 73.44%] [G loss: 2.921917]\n",
      "epoch:0 step:746 [D loss: 0.472279, acc: 82.03%] [G loss: 3.089811]\n",
      "epoch:0 step:747 [D loss: 0.560277, acc: 68.75%] [G loss: 2.902507]\n",
      "epoch:0 step:748 [D loss: 0.606577, acc: 67.19%] [G loss: 2.749235]\n",
      "epoch:0 step:749 [D loss: 0.532185, acc: 75.00%] [G loss: 2.654908]\n",
      "epoch:0 step:750 [D loss: 0.561067, acc: 71.09%] [G loss: 2.854092]\n",
      "epoch:0 step:751 [D loss: 0.542796, acc: 67.97%] [G loss: 3.000596]\n",
      "epoch:0 step:752 [D loss: 0.532654, acc: 74.22%] [G loss: 3.025450]\n",
      "epoch:0 step:753 [D loss: 0.617740, acc: 70.31%] [G loss: 2.795774]\n",
      "epoch:0 step:754 [D loss: 0.461006, acc: 75.78%] [G loss: 2.987020]\n",
      "epoch:0 step:755 [D loss: 0.624783, acc: 67.97%] [G loss: 2.930704]\n",
      "epoch:0 step:756 [D loss: 0.507331, acc: 74.22%] [G loss: 2.913263]\n",
      "epoch:0 step:757 [D loss: 0.570842, acc: 70.31%] [G loss: 3.135155]\n",
      "epoch:0 step:758 [D loss: 0.589376, acc: 75.78%] [G loss: 2.738840]\n",
      "epoch:0 step:759 [D loss: 0.590032, acc: 69.53%] [G loss: 2.886006]\n",
      "epoch:0 step:760 [D loss: 0.467703, acc: 78.91%] [G loss: 2.862309]\n",
      "epoch:0 step:761 [D loss: 0.629441, acc: 60.94%] [G loss: 2.689022]\n",
      "epoch:0 step:762 [D loss: 0.464990, acc: 81.25%] [G loss: 2.848151]\n",
      "epoch:0 step:763 [D loss: 0.525415, acc: 74.22%] [G loss: 2.999640]\n",
      "epoch:0 step:764 [D loss: 0.632641, acc: 68.75%] [G loss: 2.813844]\n",
      "epoch:0 step:765 [D loss: 0.712525, acc: 57.03%] [G loss: 2.719852]\n",
      "epoch:0 step:766 [D loss: 0.538425, acc: 79.69%] [G loss: 2.666700]\n",
      "epoch:0 step:767 [D loss: 0.561823, acc: 71.09%] [G loss: 2.945047]\n",
      "epoch:0 step:768 [D loss: 0.542678, acc: 75.78%] [G loss: 2.993287]\n",
      "epoch:0 step:769 [D loss: 0.523177, acc: 71.88%] [G loss: 3.150704]\n",
      "epoch:0 step:770 [D loss: 0.588737, acc: 69.53%] [G loss: 2.838767]\n",
      "epoch:0 step:771 [D loss: 0.559050, acc: 71.88%] [G loss: 2.865710]\n",
      "epoch:0 step:772 [D loss: 0.519354, acc: 76.56%] [G loss: 2.577311]\n",
      "epoch:0 step:773 [D loss: 0.566184, acc: 67.97%] [G loss: 2.753289]\n",
      "epoch:0 step:774 [D loss: 0.571203, acc: 68.75%] [G loss: 3.071712]\n",
      "epoch:0 step:775 [D loss: 0.504419, acc: 77.34%] [G loss: 3.255587]\n",
      "epoch:0 step:776 [D loss: 0.546933, acc: 71.88%] [G loss: 2.705123]\n",
      "epoch:0 step:777 [D loss: 0.552110, acc: 73.44%] [G loss: 2.679504]\n",
      "epoch:0 step:778 [D loss: 0.493826, acc: 78.91%] [G loss: 2.909180]\n",
      "epoch:0 step:779 [D loss: 0.632368, acc: 65.62%] [G loss: 2.989903]\n",
      "epoch:0 step:780 [D loss: 0.534640, acc: 72.66%] [G loss: 2.886083]\n",
      "epoch:0 step:781 [D loss: 0.517615, acc: 75.00%] [G loss: 3.161539]\n",
      "epoch:0 step:782 [D loss: 0.581402, acc: 75.00%] [G loss: 2.931063]\n",
      "epoch:0 step:783 [D loss: 0.564758, acc: 72.66%] [G loss: 3.198502]\n",
      "epoch:0 step:784 [D loss: 0.698956, acc: 59.38%] [G loss: 2.756375]\n",
      "epoch:0 step:785 [D loss: 0.612161, acc: 63.28%] [G loss: 2.740508]\n",
      "epoch:0 step:786 [D loss: 0.516118, acc: 74.22%] [G loss: 3.029140]\n",
      "epoch:0 step:787 [D loss: 0.573915, acc: 71.09%] [G loss: 2.724740]\n",
      "epoch:0 step:788 [D loss: 0.611953, acc: 68.75%] [G loss: 2.663331]\n",
      "epoch:0 step:789 [D loss: 0.482554, acc: 77.34%] [G loss: 3.200882]\n",
      "epoch:0 step:790 [D loss: 0.518516, acc: 71.09%] [G loss: 2.877906]\n",
      "epoch:0 step:791 [D loss: 0.577265, acc: 71.88%] [G loss: 3.124095]\n",
      "epoch:0 step:792 [D loss: 0.502773, acc: 73.44%] [G loss: 3.145257]\n",
      "epoch:0 step:793 [D loss: 0.539421, acc: 75.78%] [G loss: 3.137957]\n",
      "epoch:0 step:794 [D loss: 0.673750, acc: 63.28%] [G loss: 2.781415]\n",
      "epoch:0 step:795 [D loss: 0.530769, acc: 69.53%] [G loss: 3.114805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:796 [D loss: 0.603191, acc: 69.53%] [G loss: 2.953145]\n",
      "epoch:0 step:797 [D loss: 0.633595, acc: 61.72%] [G loss: 2.713647]\n",
      "epoch:0 step:798 [D loss: 0.533682, acc: 70.31%] [G loss: 2.888678]\n",
      "epoch:0 step:799 [D loss: 0.531324, acc: 77.34%] [G loss: 3.021362]\n",
      "epoch:0 step:800 [D loss: 0.498649, acc: 74.22%] [G loss: 3.084333]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 3.347806\n",
      "FID: 134.118866\n",
      "0 = 13.788170396804798\n",
      "1 = 0.17427844748211094\n",
      "2 = 0.982200026512146\n",
      "3 = 0.9648000001907349\n",
      "4 = 0.9995999932289124\n",
      "5 = 0.9995855689048767\n",
      "6 = 0.9648000001907349\n",
      "7 = 12.47203977563381\n",
      "8 = 0.2141855039612081\n",
      "9 = 0.9711999893188477\n",
      "10 = 0.9545999765396118\n",
      "11 = 0.9878000020980835\n",
      "12 = 0.9873810410499573\n",
      "13 = 0.9545999765396118\n",
      "14 = 3.3478076457977295\n",
      "15 = 8.132227897644043\n",
      "16 = 0.42066970467567444\n",
      "17 = 3.347805976867676\n",
      "18 = 134.11886596679688\n",
      "epoch:0 step:801 [D loss: 0.487299, acc: 78.91%] [G loss: 3.348352]\n",
      "epoch:0 step:802 [D loss: 0.512913, acc: 74.22%] [G loss: 3.344046]\n",
      "epoch:0 step:803 [D loss: 0.586883, acc: 71.09%] [G loss: 3.046343]\n",
      "epoch:0 step:804 [D loss: 0.628344, acc: 71.09%] [G loss: 2.648806]\n",
      "epoch:0 step:805 [D loss: 0.564654, acc: 68.75%] [G loss: 2.866097]\n",
      "epoch:0 step:806 [D loss: 0.585132, acc: 69.53%] [G loss: 2.888887]\n",
      "epoch:0 step:807 [D loss: 0.608617, acc: 70.31%] [G loss: 3.061500]\n",
      "epoch:0 step:808 [D loss: 0.548000, acc: 70.31%] [G loss: 2.891140]\n",
      "epoch:0 step:809 [D loss: 0.658911, acc: 59.38%] [G loss: 2.652418]\n",
      "epoch:0 step:810 [D loss: 0.564937, acc: 71.09%] [G loss: 2.820902]\n",
      "epoch:0 step:811 [D loss: 0.656490, acc: 58.59%] [G loss: 2.879646]\n",
      "epoch:0 step:812 [D loss: 0.581815, acc: 67.97%] [G loss: 2.743054]\n",
      "epoch:0 step:813 [D loss: 0.588804, acc: 67.97%] [G loss: 2.838837]\n",
      "epoch:0 step:814 [D loss: 0.438018, acc: 81.25%] [G loss: 2.817405]\n",
      "epoch:0 step:815 [D loss: 0.546311, acc: 72.66%] [G loss: 3.205062]\n",
      "epoch:0 step:816 [D loss: 0.438914, acc: 82.03%] [G loss: 2.902815]\n",
      "epoch:0 step:817 [D loss: 0.551012, acc: 72.66%] [G loss: 2.881744]\n",
      "epoch:0 step:818 [D loss: 0.549417, acc: 70.31%] [G loss: 3.030954]\n",
      "epoch:0 step:819 [D loss: 0.550310, acc: 71.09%] [G loss: 2.892147]\n",
      "epoch:0 step:820 [D loss: 0.631954, acc: 66.41%] [G loss: 2.913560]\n",
      "epoch:0 step:821 [D loss: 0.560352, acc: 70.31%] [G loss: 2.882107]\n",
      "epoch:0 step:822 [D loss: 0.494106, acc: 76.56%] [G loss: 2.925453]\n",
      "epoch:0 step:823 [D loss: 0.493183, acc: 75.00%] [G loss: 2.999621]\n",
      "epoch:0 step:824 [D loss: 0.647336, acc: 67.97%] [G loss: 3.070490]\n",
      "epoch:0 step:825 [D loss: 0.535781, acc: 72.66%] [G loss: 3.110552]\n",
      "epoch:0 step:826 [D loss: 0.546738, acc: 71.09%] [G loss: 2.797230]\n",
      "epoch:0 step:827 [D loss: 0.607776, acc: 65.62%] [G loss: 2.931235]\n",
      "epoch:0 step:828 [D loss: 0.575386, acc: 71.09%] [G loss: 2.906811]\n",
      "epoch:0 step:829 [D loss: 0.486813, acc: 75.78%] [G loss: 3.225757]\n",
      "epoch:0 step:830 [D loss: 0.460055, acc: 79.69%] [G loss: 3.039210]\n",
      "epoch:0 step:831 [D loss: 0.559569, acc: 69.53%] [G loss: 3.118109]\n",
      "epoch:0 step:832 [D loss: 0.490297, acc: 71.09%] [G loss: 3.044715]\n",
      "epoch:0 step:833 [D loss: 0.591218, acc: 73.44%] [G loss: 3.230847]\n",
      "epoch:0 step:834 [D loss: 0.591473, acc: 71.09%] [G loss: 2.633389]\n",
      "epoch:0 step:835 [D loss: 0.499555, acc: 74.22%] [G loss: 2.969216]\n",
      "epoch:0 step:836 [D loss: 0.536620, acc: 69.53%] [G loss: 2.827846]\n",
      "epoch:0 step:837 [D loss: 0.549438, acc: 71.09%] [G loss: 2.857455]\n",
      "epoch:0 step:838 [D loss: 0.500800, acc: 73.44%] [G loss: 3.001450]\n",
      "epoch:0 step:839 [D loss: 0.670490, acc: 64.06%] [G loss: 2.764025]\n",
      "epoch:0 step:840 [D loss: 0.587109, acc: 67.97%] [G loss: 2.747747]\n",
      "epoch:0 step:841 [D loss: 0.559220, acc: 70.31%] [G loss: 2.831284]\n",
      "epoch:0 step:842 [D loss: 0.505029, acc: 76.56%] [G loss: 2.811719]\n",
      "epoch:0 step:843 [D loss: 0.610836, acc: 68.75%] [G loss: 2.926955]\n",
      "epoch:0 step:844 [D loss: 0.511007, acc: 70.31%] [G loss: 3.053572]\n",
      "epoch:0 step:845 [D loss: 0.613857, acc: 69.53%] [G loss: 3.025508]\n",
      "epoch:0 step:846 [D loss: 0.604870, acc: 68.75%] [G loss: 2.844958]\n",
      "epoch:0 step:847 [D loss: 0.513828, acc: 74.22%] [G loss: 3.090298]\n",
      "epoch:0 step:848 [D loss: 0.620173, acc: 64.06%] [G loss: 2.884342]\n",
      "epoch:0 step:849 [D loss: 0.589339, acc: 64.84%] [G loss: 2.866894]\n",
      "epoch:0 step:850 [D loss: 0.575913, acc: 73.44%] [G loss: 2.706977]\n",
      "epoch:0 step:851 [D loss: 0.557281, acc: 76.56%] [G loss: 2.792176]\n",
      "epoch:0 step:852 [D loss: 0.547046, acc: 71.09%] [G loss: 2.962214]\n",
      "epoch:0 step:853 [D loss: 0.480215, acc: 78.91%] [G loss: 3.041459]\n",
      "epoch:0 step:854 [D loss: 0.502468, acc: 75.78%] [G loss: 2.829847]\n",
      "epoch:0 step:855 [D loss: 0.559454, acc: 74.22%] [G loss: 2.863621]\n",
      "epoch:0 step:856 [D loss: 0.559856, acc: 68.75%] [G loss: 2.935505]\n",
      "epoch:0 step:857 [D loss: 0.564005, acc: 72.66%] [G loss: 2.893908]\n",
      "epoch:0 step:858 [D loss: 0.704284, acc: 58.59%] [G loss: 2.946998]\n",
      "epoch:0 step:859 [D loss: 0.514179, acc: 73.44%] [G loss: 2.772634]\n",
      "epoch:0 step:860 [D loss: 0.521360, acc: 73.44%] [G loss: 3.248247]\n",
      "epoch:0 step:861 [D loss: 0.633211, acc: 60.16%] [G loss: 2.818222]\n",
      "epoch:0 step:862 [D loss: 0.570249, acc: 67.97%] [G loss: 3.044617]\n",
      "epoch:0 step:863 [D loss: 0.470880, acc: 78.91%] [G loss: 2.842498]\n",
      "epoch:0 step:864 [D loss: 0.607414, acc: 63.28%] [G loss: 2.832325]\n",
      "epoch:0 step:865 [D loss: 0.545585, acc: 75.00%] [G loss: 2.736961]\n",
      "epoch:0 step:866 [D loss: 0.572110, acc: 66.41%] [G loss: 2.854854]\n",
      "epoch:0 step:867 [D loss: 0.535529, acc: 72.66%] [G loss: 2.897321]\n",
      "epoch:0 step:868 [D loss: 0.643319, acc: 61.72%] [G loss: 2.748211]\n",
      "epoch:0 step:869 [D loss: 0.489226, acc: 75.78%] [G loss: 3.170964]\n",
      "epoch:0 step:870 [D loss: 0.507414, acc: 71.88%] [G loss: 3.061818]\n",
      "epoch:0 step:871 [D loss: 0.640722, acc: 60.94%] [G loss: 2.895731]\n",
      "epoch:0 step:872 [D loss: 0.487927, acc: 75.00%] [G loss: 2.683969]\n",
      "epoch:0 step:873 [D loss: 0.535226, acc: 71.88%] [G loss: 2.917540]\n",
      "epoch:0 step:874 [D loss: 0.624687, acc: 67.19%] [G loss: 2.911527]\n",
      "epoch:0 step:875 [D loss: 0.516885, acc: 76.56%] [G loss: 2.880623]\n",
      "epoch:0 step:876 [D loss: 0.656987, acc: 64.06%] [G loss: 2.872008]\n",
      "epoch:0 step:877 [D loss: 0.545562, acc: 71.88%] [G loss: 2.921846]\n",
      "epoch:0 step:878 [D loss: 0.599718, acc: 66.41%] [G loss: 2.793896]\n",
      "epoch:0 step:879 [D loss: 0.555175, acc: 74.22%] [G loss: 2.815907]\n",
      "epoch:0 step:880 [D loss: 0.537692, acc: 77.34%] [G loss: 2.666420]\n",
      "epoch:0 step:881 [D loss: 0.592555, acc: 67.97%] [G loss: 2.837450]\n",
      "epoch:0 step:882 [D loss: 0.529476, acc: 71.09%] [G loss: 2.790555]\n",
      "epoch:0 step:883 [D loss: 0.633854, acc: 60.16%] [G loss: 2.684792]\n",
      "epoch:0 step:884 [D loss: 0.504334, acc: 71.09%] [G loss: 2.824332]\n",
      "epoch:0 step:885 [D loss: 0.579010, acc: 68.75%] [G loss: 3.155963]\n",
      "epoch:0 step:886 [D loss: 0.510999, acc: 75.00%] [G loss: 3.063766]\n",
      "epoch:0 step:887 [D loss: 0.560927, acc: 71.88%] [G loss: 3.031350]\n",
      "epoch:0 step:888 [D loss: 0.438633, acc: 82.03%] [G loss: 2.966379]\n",
      "epoch:0 step:889 [D loss: 0.559510, acc: 71.09%] [G loss: 3.343817]\n",
      "epoch:0 step:890 [D loss: 0.539471, acc: 74.22%] [G loss: 3.231787]\n",
      "epoch:0 step:891 [D loss: 0.703099, acc: 53.91%] [G loss: 2.947342]\n",
      "epoch:0 step:892 [D loss: 0.717192, acc: 59.38%] [G loss: 2.673086]\n",
      "epoch:0 step:893 [D loss: 0.655481, acc: 58.59%] [G loss: 2.677644]\n",
      "epoch:0 step:894 [D loss: 0.436202, acc: 82.03%] [G loss: 2.908184]\n",
      "epoch:0 step:895 [D loss: 0.592522, acc: 73.44%] [G loss: 2.688807]\n",
      "epoch:0 step:896 [D loss: 0.606671, acc: 69.53%] [G loss: 2.896569]\n",
      "epoch:0 step:897 [D loss: 0.606687, acc: 69.53%] [G loss: 3.194465]\n",
      "epoch:0 step:898 [D loss: 0.561152, acc: 74.22%] [G loss: 2.653559]\n",
      "epoch:0 step:899 [D loss: 0.504667, acc: 75.00%] [G loss: 3.145079]\n",
      "epoch:0 step:900 [D loss: 0.642353, acc: 60.94%] [G loss: 2.766897]\n",
      "epoch:0 step:901 [D loss: 0.568132, acc: 70.31%] [G loss: 2.825830]\n",
      "epoch:0 step:902 [D loss: 0.526644, acc: 78.12%] [G loss: 2.868620]\n",
      "epoch:0 step:903 [D loss: 0.573899, acc: 71.88%] [G loss: 2.831769]\n",
      "epoch:0 step:904 [D loss: 0.500818, acc: 76.56%] [G loss: 2.734126]\n",
      "epoch:0 step:905 [D loss: 0.613642, acc: 66.41%] [G loss: 2.915538]\n",
      "epoch:0 step:906 [D loss: 0.526565, acc: 75.00%] [G loss: 2.844985]\n",
      "epoch:0 step:907 [D loss: 0.541830, acc: 75.00%] [G loss: 2.749105]\n",
      "epoch:0 step:908 [D loss: 0.535914, acc: 76.56%] [G loss: 2.634876]\n",
      "epoch:0 step:909 [D loss: 0.629629, acc: 64.84%] [G loss: 2.841604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:910 [D loss: 0.551796, acc: 71.88%] [G loss: 3.108085]\n",
      "epoch:0 step:911 [D loss: 0.568760, acc: 71.09%] [G loss: 2.978312]\n",
      "epoch:0 step:912 [D loss: 0.565461, acc: 71.88%] [G loss: 2.961184]\n",
      "epoch:0 step:913 [D loss: 0.551063, acc: 65.62%] [G loss: 2.954966]\n",
      "epoch:0 step:914 [D loss: 0.592131, acc: 69.53%] [G loss: 3.322700]\n",
      "epoch:0 step:915 [D loss: 0.712201, acc: 57.03%] [G loss: 2.969509]\n",
      "epoch:0 step:916 [D loss: 0.569035, acc: 67.19%] [G loss: 2.999711]\n",
      "epoch:0 step:917 [D loss: 0.580866, acc: 72.66%] [G loss: 2.917868]\n",
      "epoch:0 step:918 [D loss: 0.589841, acc: 75.78%] [G loss: 2.902074]\n",
      "epoch:0 step:919 [D loss: 0.596040, acc: 65.62%] [G loss: 2.727462]\n",
      "epoch:0 step:920 [D loss: 0.826360, acc: 47.66%] [G loss: 2.609909]\n",
      "epoch:0 step:921 [D loss: 0.542407, acc: 65.62%] [G loss: 3.067050]\n",
      "epoch:0 step:922 [D loss: 0.655531, acc: 63.28%] [G loss: 2.663516]\n",
      "epoch:0 step:923 [D loss: 0.486251, acc: 76.56%] [G loss: 2.770030]\n",
      "epoch:0 step:924 [D loss: 0.446665, acc: 78.91%] [G loss: 2.942422]\n",
      "epoch:0 step:925 [D loss: 0.462531, acc: 79.69%] [G loss: 3.066242]\n",
      "epoch:0 step:926 [D loss: 0.451432, acc: 75.78%] [G loss: 2.955291]\n",
      "epoch:0 step:927 [D loss: 0.481862, acc: 77.34%] [G loss: 3.491778]\n",
      "epoch:0 step:928 [D loss: 0.621244, acc: 67.97%] [G loss: 3.532171]\n",
      "epoch:0 step:929 [D loss: 0.541610, acc: 71.09%] [G loss: 3.987870]\n",
      "epoch:0 step:930 [D loss: 0.589741, acc: 69.53%] [G loss: 3.195159]\n",
      "epoch:0 step:931 [D loss: 0.815640, acc: 54.69%] [G loss: 2.764183]\n",
      "epoch:0 step:932 [D loss: 0.638863, acc: 61.72%] [G loss: 3.027375]\n",
      "epoch:0 step:933 [D loss: 0.565215, acc: 73.44%] [G loss: 2.974983]\n",
      "epoch:0 step:934 [D loss: 0.540116, acc: 67.19%] [G loss: 2.982553]\n",
      "epoch:0 step:935 [D loss: 0.465024, acc: 78.12%] [G loss: 3.327909]\n",
      "epoch:0 step:936 [D loss: 0.484226, acc: 76.56%] [G loss: 3.243396]\n",
      "epoch:0 step:937 [D loss: 0.665433, acc: 64.84%] [G loss: 2.960999]\n",
      "epoch:1 step:938 [D loss: 0.558406, acc: 66.41%] [G loss: 2.647730]\n",
      "epoch:1 step:939 [D loss: 0.578460, acc: 67.97%] [G loss: 2.724934]\n",
      "epoch:1 step:940 [D loss: 0.564625, acc: 74.22%] [G loss: 2.794194]\n",
      "epoch:1 step:941 [D loss: 0.509592, acc: 70.31%] [G loss: 2.776159]\n",
      "epoch:1 step:942 [D loss: 0.545803, acc: 71.88%] [G loss: 2.785997]\n",
      "epoch:1 step:943 [D loss: 0.503479, acc: 75.00%] [G loss: 3.007415]\n",
      "epoch:1 step:944 [D loss: 0.523331, acc: 76.56%] [G loss: 3.025057]\n",
      "epoch:1 step:945 [D loss: 0.568152, acc: 69.53%] [G loss: 3.054227]\n",
      "epoch:1 step:946 [D loss: 0.604467, acc: 66.41%] [G loss: 2.756416]\n",
      "epoch:1 step:947 [D loss: 0.453707, acc: 80.47%] [G loss: 3.041960]\n",
      "epoch:1 step:948 [D loss: 0.530353, acc: 72.66%] [G loss: 2.990050]\n",
      "epoch:1 step:949 [D loss: 0.559156, acc: 68.75%] [G loss: 2.669621]\n",
      "epoch:1 step:950 [D loss: 0.622907, acc: 67.97%] [G loss: 2.901485]\n",
      "epoch:1 step:951 [D loss: 0.568533, acc: 68.75%] [G loss: 2.917160]\n",
      "epoch:1 step:952 [D loss: 0.507297, acc: 75.78%] [G loss: 3.130496]\n",
      "epoch:1 step:953 [D loss: 0.525842, acc: 75.00%] [G loss: 2.855615]\n",
      "epoch:1 step:954 [D loss: 0.626077, acc: 61.72%] [G loss: 2.766091]\n",
      "epoch:1 step:955 [D loss: 0.626529, acc: 64.06%] [G loss: 2.447366]\n",
      "epoch:1 step:956 [D loss: 0.484256, acc: 79.69%] [G loss: 2.844395]\n",
      "epoch:1 step:957 [D loss: 0.710413, acc: 54.69%] [G loss: 2.842819]\n",
      "epoch:1 step:958 [D loss: 0.526025, acc: 74.22%] [G loss: 3.081910]\n",
      "epoch:1 step:959 [D loss: 0.488336, acc: 79.69%] [G loss: 3.228716]\n",
      "epoch:1 step:960 [D loss: 0.692401, acc: 58.59%] [G loss: 3.037943]\n",
      "epoch:1 step:961 [D loss: 0.546323, acc: 71.09%] [G loss: 2.874136]\n",
      "epoch:1 step:962 [D loss: 0.535021, acc: 76.56%] [G loss: 2.782007]\n",
      "epoch:1 step:963 [D loss: 0.697242, acc: 61.72%] [G loss: 2.659640]\n",
      "epoch:1 step:964 [D loss: 0.652078, acc: 61.72%] [G loss: 2.574862]\n",
      "epoch:1 step:965 [D loss: 0.544669, acc: 71.09%] [G loss: 2.769957]\n",
      "epoch:1 step:966 [D loss: 0.543350, acc: 78.12%] [G loss: 2.670100]\n",
      "epoch:1 step:967 [D loss: 0.512406, acc: 78.91%] [G loss: 2.785839]\n",
      "epoch:1 step:968 [D loss: 0.615407, acc: 69.53%] [G loss: 2.519552]\n",
      "epoch:1 step:969 [D loss: 0.519011, acc: 75.78%] [G loss: 3.068598]\n",
      "epoch:1 step:970 [D loss: 0.571082, acc: 67.97%] [G loss: 3.034586]\n",
      "epoch:1 step:971 [D loss: 0.559562, acc: 66.41%] [G loss: 2.707336]\n",
      "epoch:1 step:972 [D loss: 0.488951, acc: 76.56%] [G loss: 3.075659]\n",
      "epoch:1 step:973 [D loss: 0.499335, acc: 78.91%] [G loss: 3.214381]\n",
      "epoch:1 step:974 [D loss: 0.531023, acc: 76.56%] [G loss: 2.972386]\n",
      "epoch:1 step:975 [D loss: 0.566881, acc: 69.53%] [G loss: 2.906927]\n",
      "epoch:1 step:976 [D loss: 0.570382, acc: 71.88%] [G loss: 3.169419]\n",
      "epoch:1 step:977 [D loss: 0.448905, acc: 77.34%] [G loss: 3.402866]\n",
      "epoch:1 step:978 [D loss: 0.591005, acc: 68.75%] [G loss: 2.644168]\n",
      "epoch:1 step:979 [D loss: 0.587851, acc: 70.31%] [G loss: 2.832114]\n",
      "epoch:1 step:980 [D loss: 0.575903, acc: 67.97%] [G loss: 2.636418]\n",
      "epoch:1 step:981 [D loss: 0.554338, acc: 73.44%] [G loss: 2.903398]\n",
      "epoch:1 step:982 [D loss: 0.656589, acc: 62.50%] [G loss: 3.076738]\n",
      "epoch:1 step:983 [D loss: 0.600441, acc: 68.75%] [G loss: 2.807153]\n",
      "epoch:1 step:984 [D loss: 0.559300, acc: 69.53%] [G loss: 2.749207]\n",
      "epoch:1 step:985 [D loss: 0.614065, acc: 63.28%] [G loss: 2.783381]\n",
      "epoch:1 step:986 [D loss: 0.516865, acc: 74.22%] [G loss: 2.827779]\n",
      "epoch:1 step:987 [D loss: 0.557103, acc: 73.44%] [G loss: 2.946846]\n",
      "epoch:1 step:988 [D loss: 0.649154, acc: 66.41%] [G loss: 2.782717]\n",
      "epoch:1 step:989 [D loss: 0.587585, acc: 71.88%] [G loss: 2.853713]\n",
      "epoch:1 step:990 [D loss: 0.550464, acc: 70.31%] [G loss: 3.146518]\n",
      "epoch:1 step:991 [D loss: 0.522348, acc: 73.44%] [G loss: 2.918863]\n",
      "epoch:1 step:992 [D loss: 0.460217, acc: 82.03%] [G loss: 3.228392]\n",
      "epoch:1 step:993 [D loss: 0.610358, acc: 65.62%] [G loss: 2.975629]\n",
      "epoch:1 step:994 [D loss: 0.543217, acc: 74.22%] [G loss: 3.065501]\n",
      "epoch:1 step:995 [D loss: 0.617864, acc: 64.84%] [G loss: 2.887714]\n",
      "epoch:1 step:996 [D loss: 0.571275, acc: 71.09%] [G loss: 2.749736]\n",
      "epoch:1 step:997 [D loss: 0.558743, acc: 72.66%] [G loss: 3.032145]\n",
      "epoch:1 step:998 [D loss: 0.616396, acc: 64.84%] [G loss: 2.824671]\n",
      "epoch:1 step:999 [D loss: 0.606196, acc: 67.19%] [G loss: 2.961348]\n",
      "epoch:1 step:1000 [D loss: 0.528105, acc: 73.44%] [G loss: 2.795901]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 3.889347\n",
      "FID: 105.809448\n",
      "0 = 13.696209089756044\n",
      "1 = 0.15637049797035898\n",
      "2 = 0.9740999937057495\n",
      "3 = 0.9490000009536743\n",
      "4 = 0.9991999864578247\n",
      "5 = 0.999157726764679\n",
      "6 = 0.9490000009536743\n",
      "7 = 11.563471931839015\n",
      "8 = 0.19209663289905626\n",
      "9 = 0.9495000243186951\n",
      "10 = 0.9336000084877014\n",
      "11 = 0.965399980545044\n",
      "12 = 0.9642635583877563\n",
      "13 = 0.9336000084877014\n",
      "14 = 3.889350414276123\n",
      "15 = 8.445358276367188\n",
      "16 = 0.3616727292537689\n",
      "17 = 3.8893465995788574\n",
      "18 = 105.8094482421875\n",
      "epoch:1 step:1001 [D loss: 0.608815, acc: 65.62%] [G loss: 2.972356]\n",
      "epoch:1 step:1002 [D loss: 0.562601, acc: 69.53%] [G loss: 2.861777]\n",
      "epoch:1 step:1003 [D loss: 0.584213, acc: 67.19%] [G loss: 2.967312]\n",
      "epoch:1 step:1004 [D loss: 0.545659, acc: 67.97%] [G loss: 3.165740]\n",
      "epoch:1 step:1005 [D loss: 0.555717, acc: 72.66%] [G loss: 3.099208]\n",
      "epoch:1 step:1006 [D loss: 0.627609, acc: 64.06%] [G loss: 2.716969]\n",
      "epoch:1 step:1007 [D loss: 0.530853, acc: 76.56%] [G loss: 2.793846]\n",
      "epoch:1 step:1008 [D loss: 0.605374, acc: 66.41%] [G loss: 2.866525]\n",
      "epoch:1 step:1009 [D loss: 0.440282, acc: 81.25%] [G loss: 3.143220]\n",
      "epoch:1 step:1010 [D loss: 0.550383, acc: 75.78%] [G loss: 3.003339]\n",
      "epoch:1 step:1011 [D loss: 0.458299, acc: 79.69%] [G loss: 3.016521]\n",
      "epoch:1 step:1012 [D loss: 0.442163, acc: 82.03%] [G loss: 3.630850]\n",
      "epoch:1 step:1013 [D loss: 0.558357, acc: 73.44%] [G loss: 2.879545]\n",
      "epoch:1 step:1014 [D loss: 0.477477, acc: 73.44%] [G loss: 3.346094]\n",
      "epoch:1 step:1015 [D loss: 0.614710, acc: 67.97%] [G loss: 2.789145]\n",
      "epoch:1 step:1016 [D loss: 0.718959, acc: 64.06%] [G loss: 2.804794]\n",
      "epoch:1 step:1017 [D loss: 0.580485, acc: 70.31%] [G loss: 2.976927]\n",
      "epoch:1 step:1018 [D loss: 0.590266, acc: 64.06%] [G loss: 2.761271]\n",
      "epoch:1 step:1019 [D loss: 0.488528, acc: 78.12%] [G loss: 2.740727]\n",
      "epoch:1 step:1020 [D loss: 0.540233, acc: 69.53%] [G loss: 2.742017]\n",
      "epoch:1 step:1021 [D loss: 0.571852, acc: 70.31%] [G loss: 3.162002]\n",
      "epoch:1 step:1022 [D loss: 0.551551, acc: 71.09%] [G loss: 2.796652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1023 [D loss: 0.554235, acc: 71.88%] [G loss: 2.986850]\n",
      "epoch:1 step:1024 [D loss: 0.542584, acc: 70.31%] [G loss: 3.009085]\n",
      "epoch:1 step:1025 [D loss: 0.567624, acc: 66.41%] [G loss: 3.012719]\n",
      "epoch:1 step:1026 [D loss: 0.518512, acc: 73.44%] [G loss: 3.173171]\n",
      "epoch:1 step:1027 [D loss: 0.558541, acc: 68.75%] [G loss: 3.177888]\n",
      "epoch:1 step:1028 [D loss: 0.502921, acc: 73.44%] [G loss: 2.835358]\n",
      "epoch:1 step:1029 [D loss: 0.602820, acc: 65.62%] [G loss: 2.898655]\n",
      "epoch:1 step:1030 [D loss: 0.536955, acc: 75.78%] [G loss: 2.829905]\n",
      "epoch:1 step:1031 [D loss: 0.550380, acc: 73.44%] [G loss: 2.978732]\n",
      "epoch:1 step:1032 [D loss: 0.597738, acc: 69.53%] [G loss: 3.173989]\n",
      "epoch:1 step:1033 [D loss: 0.492440, acc: 71.88%] [G loss: 3.297852]\n",
      "epoch:1 step:1034 [D loss: 0.514737, acc: 77.34%] [G loss: 3.232071]\n",
      "epoch:1 step:1035 [D loss: 0.479505, acc: 78.12%] [G loss: 2.973196]\n",
      "epoch:1 step:1036 [D loss: 0.677029, acc: 59.38%] [G loss: 3.004763]\n",
      "epoch:1 step:1037 [D loss: 0.538937, acc: 78.12%] [G loss: 3.333204]\n",
      "epoch:1 step:1038 [D loss: 0.577365, acc: 71.88%] [G loss: 3.036020]\n",
      "epoch:1 step:1039 [D loss: 0.717478, acc: 54.69%] [G loss: 2.925625]\n",
      "epoch:1 step:1040 [D loss: 0.496919, acc: 75.78%] [G loss: 3.183443]\n",
      "epoch:1 step:1041 [D loss: 0.539369, acc: 74.22%] [G loss: 2.855800]\n",
      "epoch:1 step:1042 [D loss: 0.652593, acc: 67.97%] [G loss: 2.879196]\n",
      "epoch:1 step:1043 [D loss: 0.624672, acc: 63.28%] [G loss: 2.509489]\n",
      "epoch:1 step:1044 [D loss: 0.685218, acc: 63.28%] [G loss: 2.568925]\n",
      "epoch:1 step:1045 [D loss: 0.614915, acc: 67.97%] [G loss: 2.853482]\n",
      "epoch:1 step:1046 [D loss: 0.520914, acc: 76.56%] [G loss: 2.974453]\n",
      "epoch:1 step:1047 [D loss: 0.503050, acc: 71.88%] [G loss: 2.735845]\n",
      "epoch:1 step:1048 [D loss: 0.537144, acc: 73.44%] [G loss: 2.565538]\n",
      "epoch:1 step:1049 [D loss: 0.617991, acc: 64.84%] [G loss: 2.989686]\n",
      "epoch:1 step:1050 [D loss: 0.687595, acc: 64.06%] [G loss: 2.420937]\n",
      "epoch:1 step:1051 [D loss: 0.552845, acc: 71.09%] [G loss: 2.674173]\n",
      "epoch:1 step:1052 [D loss: 0.584189, acc: 71.09%] [G loss: 2.794909]\n",
      "epoch:1 step:1053 [D loss: 0.544970, acc: 72.66%] [G loss: 2.610453]\n",
      "epoch:1 step:1054 [D loss: 0.625958, acc: 64.06%] [G loss: 3.146784]\n",
      "epoch:1 step:1055 [D loss: 0.610349, acc: 65.62%] [G loss: 2.960490]\n",
      "epoch:1 step:1056 [D loss: 0.574600, acc: 70.31%] [G loss: 3.258350]\n",
      "epoch:1 step:1057 [D loss: 0.675970, acc: 68.75%] [G loss: 2.631278]\n",
      "epoch:1 step:1058 [D loss: 0.606406, acc: 67.97%] [G loss: 2.820760]\n",
      "epoch:1 step:1059 [D loss: 0.605143, acc: 71.88%] [G loss: 2.789431]\n",
      "epoch:1 step:1060 [D loss: 0.578016, acc: 66.41%] [G loss: 2.860945]\n",
      "epoch:1 step:1061 [D loss: 0.704161, acc: 57.81%] [G loss: 2.869430]\n",
      "epoch:1 step:1062 [D loss: 0.601210, acc: 70.31%] [G loss: 2.668823]\n",
      "epoch:1 step:1063 [D loss: 0.519568, acc: 71.88%] [G loss: 2.972215]\n",
      "epoch:1 step:1064 [D loss: 0.585589, acc: 75.78%] [G loss: 2.957922]\n",
      "epoch:1 step:1065 [D loss: 0.575413, acc: 66.41%] [G loss: 2.632555]\n",
      "epoch:1 step:1066 [D loss: 0.654350, acc: 64.06%] [G loss: 2.836631]\n",
      "epoch:1 step:1067 [D loss: 0.526954, acc: 71.88%] [G loss: 3.046162]\n",
      "epoch:1 step:1068 [D loss: 0.618468, acc: 64.84%] [G loss: 2.808397]\n",
      "epoch:1 step:1069 [D loss: 0.588946, acc: 67.97%] [G loss: 2.558407]\n",
      "epoch:1 step:1070 [D loss: 0.567913, acc: 71.88%] [G loss: 2.869862]\n",
      "epoch:1 step:1071 [D loss: 0.505967, acc: 75.78%] [G loss: 3.213855]\n",
      "epoch:1 step:1072 [D loss: 0.544513, acc: 72.66%] [G loss: 2.940901]\n",
      "epoch:1 step:1073 [D loss: 0.579476, acc: 67.97%] [G loss: 2.880996]\n",
      "epoch:1 step:1074 [D loss: 0.583221, acc: 65.62%] [G loss: 2.564041]\n",
      "epoch:1 step:1075 [D loss: 0.597889, acc: 69.53%] [G loss: 2.833034]\n",
      "epoch:1 step:1076 [D loss: 0.562801, acc: 73.44%] [G loss: 2.899597]\n",
      "epoch:1 step:1077 [D loss: 0.575447, acc: 72.66%] [G loss: 3.092829]\n",
      "epoch:1 step:1078 [D loss: 0.487015, acc: 77.34%] [G loss: 3.067281]\n",
      "epoch:1 step:1079 [D loss: 0.542348, acc: 70.31%] [G loss: 3.004305]\n",
      "epoch:1 step:1080 [D loss: 0.616830, acc: 67.97%] [G loss: 2.863857]\n",
      "epoch:1 step:1081 [D loss: 0.571272, acc: 66.41%] [G loss: 2.833416]\n",
      "epoch:1 step:1082 [D loss: 0.519612, acc: 70.31%] [G loss: 2.619147]\n",
      "epoch:1 step:1083 [D loss: 0.676029, acc: 54.69%] [G loss: 2.821118]\n",
      "epoch:1 step:1084 [D loss: 0.649545, acc: 64.06%] [G loss: 2.698761]\n",
      "epoch:1 step:1085 [D loss: 0.521257, acc: 74.22%] [G loss: 2.922065]\n",
      "epoch:1 step:1086 [D loss: 0.511877, acc: 74.22%] [G loss: 3.054235]\n",
      "epoch:1 step:1087 [D loss: 0.642616, acc: 64.06%] [G loss: 2.990745]\n",
      "epoch:1 step:1088 [D loss: 0.541293, acc: 71.88%] [G loss: 2.814195]\n",
      "epoch:1 step:1089 [D loss: 0.520515, acc: 71.09%] [G loss: 2.973143]\n",
      "epoch:1 step:1090 [D loss: 0.585112, acc: 71.88%] [G loss: 2.846759]\n",
      "epoch:1 step:1091 [D loss: 0.548633, acc: 68.75%] [G loss: 2.938800]\n",
      "epoch:1 step:1092 [D loss: 0.618282, acc: 68.75%] [G loss: 3.006870]\n",
      "epoch:1 step:1093 [D loss: 0.570082, acc: 68.75%] [G loss: 2.563855]\n",
      "epoch:1 step:1094 [D loss: 0.640943, acc: 61.72%] [G loss: 2.815316]\n",
      "epoch:1 step:1095 [D loss: 0.649079, acc: 60.16%] [G loss: 2.785278]\n",
      "epoch:1 step:1096 [D loss: 0.571016, acc: 72.66%] [G loss: 2.767837]\n",
      "epoch:1 step:1097 [D loss: 0.638493, acc: 67.97%] [G loss: 3.176494]\n",
      "epoch:1 step:1098 [D loss: 0.491441, acc: 75.78%] [G loss: 3.172792]\n",
      "epoch:1 step:1099 [D loss: 0.426222, acc: 82.81%] [G loss: 3.291482]\n",
      "epoch:1 step:1100 [D loss: 0.468114, acc: 77.34%] [G loss: 3.076044]\n",
      "epoch:1 step:1101 [D loss: 0.504899, acc: 76.56%] [G loss: 3.102218]\n",
      "epoch:1 step:1102 [D loss: 0.654227, acc: 63.28%] [G loss: 2.781994]\n",
      "epoch:1 step:1103 [D loss: 0.513703, acc: 78.12%] [G loss: 2.978485]\n",
      "epoch:1 step:1104 [D loss: 0.596539, acc: 71.88%] [G loss: 2.707555]\n",
      "epoch:1 step:1105 [D loss: 0.542587, acc: 71.09%] [G loss: 2.792990]\n",
      "epoch:1 step:1106 [D loss: 0.548927, acc: 71.88%] [G loss: 2.754640]\n",
      "epoch:1 step:1107 [D loss: 0.519419, acc: 76.56%] [G loss: 2.997914]\n",
      "epoch:1 step:1108 [D loss: 0.609745, acc: 69.53%] [G loss: 2.997213]\n",
      "epoch:1 step:1109 [D loss: 0.513638, acc: 71.88%] [G loss: 2.711933]\n",
      "epoch:1 step:1110 [D loss: 0.529220, acc: 75.78%] [G loss: 3.173619]\n",
      "epoch:1 step:1111 [D loss: 0.629129, acc: 67.19%] [G loss: 2.850198]\n",
      "epoch:1 step:1112 [D loss: 0.511504, acc: 75.78%] [G loss: 2.785337]\n",
      "epoch:1 step:1113 [D loss: 0.484182, acc: 76.56%] [G loss: 2.912588]\n",
      "epoch:1 step:1114 [D loss: 0.639143, acc: 62.50%] [G loss: 2.943221]\n",
      "epoch:1 step:1115 [D loss: 0.620754, acc: 65.62%] [G loss: 2.921936]\n",
      "epoch:1 step:1116 [D loss: 0.595679, acc: 68.75%] [G loss: 2.867660]\n",
      "epoch:1 step:1117 [D loss: 0.667189, acc: 68.75%] [G loss: 2.661253]\n",
      "epoch:1 step:1118 [D loss: 0.624754, acc: 64.06%] [G loss: 2.621543]\n",
      "epoch:1 step:1119 [D loss: 0.675568, acc: 60.16%] [G loss: 2.917354]\n",
      "epoch:1 step:1120 [D loss: 0.654919, acc: 58.59%] [G loss: 2.706771]\n",
      "epoch:1 step:1121 [D loss: 0.531184, acc: 72.66%] [G loss: 2.910638]\n",
      "epoch:1 step:1122 [D loss: 0.612675, acc: 67.19%] [G loss: 2.647381]\n",
      "epoch:1 step:1123 [D loss: 0.553259, acc: 67.19%] [G loss: 2.931857]\n",
      "epoch:1 step:1124 [D loss: 0.642689, acc: 64.06%] [G loss: 2.518778]\n",
      "epoch:1 step:1125 [D loss: 0.509996, acc: 77.34%] [G loss: 2.964892]\n",
      "epoch:1 step:1126 [D loss: 0.581021, acc: 68.75%] [G loss: 3.032442]\n",
      "epoch:1 step:1127 [D loss: 0.563738, acc: 71.09%] [G loss: 3.060432]\n",
      "epoch:1 step:1128 [D loss: 0.567956, acc: 71.88%] [G loss: 2.660348]\n",
      "epoch:1 step:1129 [D loss: 0.492247, acc: 75.78%] [G loss: 2.733377]\n",
      "epoch:1 step:1130 [D loss: 0.589550, acc: 71.09%] [G loss: 2.957651]\n",
      "epoch:1 step:1131 [D loss: 0.458638, acc: 75.78%] [G loss: 2.963243]\n",
      "epoch:1 step:1132 [D loss: 0.564915, acc: 69.53%] [G loss: 2.949996]\n",
      "epoch:1 step:1133 [D loss: 0.597522, acc: 66.41%] [G loss: 2.835362]\n",
      "epoch:1 step:1134 [D loss: 0.523329, acc: 76.56%] [G loss: 3.437622]\n",
      "epoch:1 step:1135 [D loss: 0.510380, acc: 74.22%] [G loss: 3.079690]\n",
      "epoch:1 step:1136 [D loss: 0.585646, acc: 73.44%] [G loss: 2.977041]\n",
      "epoch:1 step:1137 [D loss: 0.623975, acc: 67.19%] [G loss: 3.030205]\n",
      "epoch:1 step:1138 [D loss: 0.648051, acc: 67.97%] [G loss: 2.724052]\n",
      "epoch:1 step:1139 [D loss: 0.505686, acc: 74.22%] [G loss: 2.620024]\n",
      "epoch:1 step:1140 [D loss: 0.530249, acc: 71.88%] [G loss: 3.316072]\n",
      "epoch:1 step:1141 [D loss: 0.510290, acc: 73.44%] [G loss: 3.002227]\n",
      "epoch:1 step:1142 [D loss: 0.585450, acc: 69.53%] [G loss: 3.408261]\n",
      "epoch:1 step:1143 [D loss: 0.518262, acc: 71.09%] [G loss: 2.959122]\n",
      "epoch:1 step:1144 [D loss: 0.551059, acc: 71.88%] [G loss: 3.210119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1145 [D loss: 0.611410, acc: 61.72%] [G loss: 3.125952]\n",
      "epoch:1 step:1146 [D loss: 0.445426, acc: 80.47%] [G loss: 3.005168]\n",
      "epoch:1 step:1147 [D loss: 0.582532, acc: 68.75%] [G loss: 2.936311]\n",
      "epoch:1 step:1148 [D loss: 0.518018, acc: 68.75%] [G loss: 2.528213]\n",
      "epoch:1 step:1149 [D loss: 0.537652, acc: 72.66%] [G loss: 2.937292]\n",
      "epoch:1 step:1150 [D loss: 0.534358, acc: 67.97%] [G loss: 2.710599]\n",
      "epoch:1 step:1151 [D loss: 0.648349, acc: 63.28%] [G loss: 2.733827]\n",
      "epoch:1 step:1152 [D loss: 0.669032, acc: 62.50%] [G loss: 2.709476]\n",
      "epoch:1 step:1153 [D loss: 0.603507, acc: 70.31%] [G loss: 2.475507]\n",
      "epoch:1 step:1154 [D loss: 0.486760, acc: 75.00%] [G loss: 2.699408]\n",
      "epoch:1 step:1155 [D loss: 0.597661, acc: 65.62%] [G loss: 2.887951]\n",
      "epoch:1 step:1156 [D loss: 0.575343, acc: 70.31%] [G loss: 2.703337]\n",
      "epoch:1 step:1157 [D loss: 0.567840, acc: 71.09%] [G loss: 2.878839]\n",
      "epoch:1 step:1158 [D loss: 0.490141, acc: 75.00%] [G loss: 3.381304]\n",
      "epoch:1 step:1159 [D loss: 0.505713, acc: 73.44%] [G loss: 3.161664]\n",
      "epoch:1 step:1160 [D loss: 0.575391, acc: 67.97%] [G loss: 2.973953]\n",
      "epoch:1 step:1161 [D loss: 0.525483, acc: 75.78%] [G loss: 3.054219]\n",
      "epoch:1 step:1162 [D loss: 0.616595, acc: 66.41%] [G loss: 2.848021]\n",
      "epoch:1 step:1163 [D loss: 0.489472, acc: 77.34%] [G loss: 2.491092]\n",
      "epoch:1 step:1164 [D loss: 0.640328, acc: 64.84%] [G loss: 2.427784]\n",
      "epoch:1 step:1165 [D loss: 0.637567, acc: 64.06%] [G loss: 2.560340]\n",
      "epoch:1 step:1166 [D loss: 0.599063, acc: 64.06%] [G loss: 2.584141]\n",
      "epoch:1 step:1167 [D loss: 0.601431, acc: 65.62%] [G loss: 2.981610]\n",
      "epoch:1 step:1168 [D loss: 0.497579, acc: 78.12%] [G loss: 3.364390]\n",
      "epoch:1 step:1169 [D loss: 0.431377, acc: 79.69%] [G loss: 3.403562]\n",
      "epoch:1 step:1170 [D loss: 0.549473, acc: 71.88%] [G loss: 2.773636]\n",
      "epoch:1 step:1171 [D loss: 0.660313, acc: 62.50%] [G loss: 2.790398]\n",
      "epoch:1 step:1172 [D loss: 0.530242, acc: 72.66%] [G loss: 2.598182]\n",
      "epoch:1 step:1173 [D loss: 0.523972, acc: 72.66%] [G loss: 2.619766]\n",
      "epoch:1 step:1174 [D loss: 0.634543, acc: 66.41%] [G loss: 2.651141]\n",
      "epoch:1 step:1175 [D loss: 0.672742, acc: 63.28%] [G loss: 2.598308]\n",
      "epoch:1 step:1176 [D loss: 0.644185, acc: 65.62%] [G loss: 2.569271]\n",
      "epoch:1 step:1177 [D loss: 0.642756, acc: 64.06%] [G loss: 2.549615]\n",
      "epoch:1 step:1178 [D loss: 0.594718, acc: 66.41%] [G loss: 2.590173]\n",
      "epoch:1 step:1179 [D loss: 0.689038, acc: 64.84%] [G loss: 2.560886]\n",
      "epoch:1 step:1180 [D loss: 0.611413, acc: 67.97%] [G loss: 2.795829]\n",
      "epoch:1 step:1181 [D loss: 0.695500, acc: 57.03%] [G loss: 2.699430]\n",
      "epoch:1 step:1182 [D loss: 0.553799, acc: 72.66%] [G loss: 2.565661]\n",
      "epoch:1 step:1183 [D loss: 0.629826, acc: 65.62%] [G loss: 2.696293]\n",
      "epoch:1 step:1184 [D loss: 0.583355, acc: 71.88%] [G loss: 2.698861]\n",
      "epoch:1 step:1185 [D loss: 0.596542, acc: 64.84%] [G loss: 2.498325]\n",
      "epoch:1 step:1186 [D loss: 0.625045, acc: 61.72%] [G loss: 2.606577]\n",
      "epoch:1 step:1187 [D loss: 0.599052, acc: 71.09%] [G loss: 2.601404]\n",
      "epoch:1 step:1188 [D loss: 0.568354, acc: 69.53%] [G loss: 2.529993]\n",
      "epoch:1 step:1189 [D loss: 0.632130, acc: 67.19%] [G loss: 2.497876]\n",
      "epoch:1 step:1190 [D loss: 0.584273, acc: 68.75%] [G loss: 2.710753]\n",
      "epoch:1 step:1191 [D loss: 0.535211, acc: 74.22%] [G loss: 2.652936]\n",
      "epoch:1 step:1192 [D loss: 0.545038, acc: 75.00%] [G loss: 2.818902]\n",
      "epoch:1 step:1193 [D loss: 0.598961, acc: 68.75%] [G loss: 2.852223]\n",
      "epoch:1 step:1194 [D loss: 0.573439, acc: 70.31%] [G loss: 2.873790]\n",
      "epoch:1 step:1195 [D loss: 0.553893, acc: 75.00%] [G loss: 2.976276]\n",
      "epoch:1 step:1196 [D loss: 0.522725, acc: 71.88%] [G loss: 2.883997]\n",
      "epoch:1 step:1197 [D loss: 0.679351, acc: 61.72%] [G loss: 2.676622]\n",
      "epoch:1 step:1198 [D loss: 0.672931, acc: 63.28%] [G loss: 2.510077]\n",
      "epoch:1 step:1199 [D loss: 0.613061, acc: 65.62%] [G loss: 2.664239]\n",
      "epoch:1 step:1200 [D loss: 0.801045, acc: 50.78%] [G loss: 2.554128]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 3.919264\n",
      "FID: 100.329231\n",
      "0 = 13.565091510963452\n",
      "1 = 0.13445079351483954\n",
      "2 = 0.9700999855995178\n",
      "3 = 0.942799985408783\n",
      "4 = 0.9973999857902527\n",
      "5 = 0.9972498416900635\n",
      "6 = 0.942799985408783\n",
      "7 = 11.503784455585478\n",
      "8 = 0.1888395776459751\n",
      "9 = 0.9470999836921692\n",
      "10 = 0.930400013923645\n",
      "11 = 0.9638000130653381\n",
      "12 = 0.9625491499900818\n",
      "13 = 0.930400013923645\n",
      "14 = 3.919267177581787\n",
      "15 = 8.379583358764648\n",
      "16 = 0.3687540590763092\n",
      "17 = 3.9192638397216797\n",
      "18 = 100.32923126220703\n",
      "epoch:1 step:1201 [D loss: 0.581005, acc: 70.31%] [G loss: 2.693441]\n",
      "epoch:1 step:1202 [D loss: 0.604763, acc: 70.31%] [G loss: 2.645659]\n",
      "epoch:1 step:1203 [D loss: 0.593176, acc: 67.97%] [G loss: 2.646476]\n",
      "epoch:1 step:1204 [D loss: 0.612247, acc: 67.97%] [G loss: 2.831540]\n",
      "epoch:1 step:1205 [D loss: 0.621928, acc: 64.84%] [G loss: 2.656410]\n",
      "epoch:1 step:1206 [D loss: 0.627164, acc: 63.28%] [G loss: 2.513002]\n",
      "epoch:1 step:1207 [D loss: 0.618372, acc: 65.62%] [G loss: 2.593097]\n",
      "epoch:1 step:1208 [D loss: 0.495554, acc: 73.44%] [G loss: 2.612151]\n",
      "epoch:1 step:1209 [D loss: 0.633849, acc: 63.28%] [G loss: 2.553631]\n",
      "epoch:1 step:1210 [D loss: 0.576025, acc: 69.53%] [G loss: 2.671230]\n",
      "epoch:1 step:1211 [D loss: 0.576944, acc: 67.97%] [G loss: 2.658814]\n",
      "epoch:1 step:1212 [D loss: 0.672917, acc: 61.72%] [G loss: 2.498768]\n",
      "epoch:1 step:1213 [D loss: 0.584484, acc: 69.53%] [G loss: 2.417164]\n",
      "epoch:1 step:1214 [D loss: 0.632336, acc: 62.50%] [G loss: 2.707640]\n",
      "epoch:1 step:1215 [D loss: 0.647143, acc: 59.38%] [G loss: 2.538125]\n",
      "epoch:1 step:1216 [D loss: 0.587434, acc: 65.62%] [G loss: 2.789587]\n",
      "epoch:1 step:1217 [D loss: 0.595854, acc: 67.19%] [G loss: 2.662009]\n",
      "epoch:1 step:1218 [D loss: 0.563549, acc: 72.66%] [G loss: 2.724428]\n",
      "epoch:1 step:1219 [D loss: 0.618021, acc: 64.84%] [G loss: 2.475761]\n",
      "epoch:1 step:1220 [D loss: 0.593348, acc: 64.84%] [G loss: 2.393137]\n",
      "epoch:1 step:1221 [D loss: 0.577699, acc: 70.31%] [G loss: 2.729558]\n",
      "epoch:1 step:1222 [D loss: 0.638187, acc: 60.94%] [G loss: 2.633721]\n",
      "epoch:1 step:1223 [D loss: 0.582824, acc: 70.31%] [G loss: 2.595850]\n",
      "epoch:1 step:1224 [D loss: 0.588458, acc: 67.97%] [G loss: 2.619480]\n",
      "epoch:1 step:1225 [D loss: 0.574773, acc: 65.62%] [G loss: 2.697221]\n",
      "epoch:1 step:1226 [D loss: 0.611157, acc: 64.84%] [G loss: 2.515299]\n",
      "epoch:1 step:1227 [D loss: 0.647410, acc: 64.06%] [G loss: 2.627348]\n",
      "epoch:1 step:1228 [D loss: 0.716264, acc: 54.69%] [G loss: 2.506768]\n",
      "epoch:1 step:1229 [D loss: 0.691492, acc: 57.03%] [G loss: 2.526253]\n",
      "epoch:1 step:1230 [D loss: 0.574067, acc: 69.53%] [G loss: 2.768481]\n",
      "epoch:1 step:1231 [D loss: 0.608676, acc: 67.19%] [G loss: 2.557418]\n",
      "epoch:1 step:1232 [D loss: 0.598009, acc: 67.19%] [G loss: 2.544221]\n",
      "epoch:1 step:1233 [D loss: 0.602605, acc: 65.62%] [G loss: 2.646224]\n",
      "epoch:1 step:1234 [D loss: 0.579016, acc: 71.88%] [G loss: 2.542682]\n",
      "epoch:1 step:1235 [D loss: 0.524395, acc: 74.22%] [G loss: 2.667089]\n",
      "epoch:1 step:1236 [D loss: 0.596937, acc: 64.84%] [G loss: 2.759908]\n",
      "epoch:1 step:1237 [D loss: 0.617336, acc: 65.62%] [G loss: 2.554744]\n",
      "epoch:1 step:1238 [D loss: 0.612939, acc: 64.84%] [G loss: 2.518910]\n",
      "epoch:1 step:1239 [D loss: 0.651559, acc: 60.16%] [G loss: 2.383650]\n",
      "epoch:1 step:1240 [D loss: 0.599178, acc: 67.19%] [G loss: 2.731372]\n",
      "epoch:1 step:1241 [D loss: 0.587755, acc: 68.75%] [G loss: 2.813998]\n",
      "epoch:1 step:1242 [D loss: 0.567593, acc: 71.88%] [G loss: 2.675917]\n",
      "epoch:1 step:1243 [D loss: 0.545773, acc: 68.75%] [G loss: 2.769198]\n",
      "epoch:1 step:1244 [D loss: 0.585601, acc: 66.41%] [G loss: 2.788038]\n",
      "epoch:1 step:1245 [D loss: 0.579256, acc: 67.97%] [G loss: 2.715787]\n",
      "epoch:1 step:1246 [D loss: 0.592715, acc: 70.31%] [G loss: 2.900663]\n",
      "epoch:1 step:1247 [D loss: 0.533310, acc: 76.56%] [G loss: 2.933098]\n",
      "epoch:1 step:1248 [D loss: 0.534320, acc: 69.53%] [G loss: 3.003870]\n",
      "epoch:1 step:1249 [D loss: 0.529912, acc: 75.00%] [G loss: 3.202780]\n",
      "epoch:1 step:1250 [D loss: 0.555580, acc: 67.97%] [G loss: 3.143320]\n",
      "epoch:1 step:1251 [D loss: 0.502820, acc: 76.56%] [G loss: 3.229986]\n",
      "epoch:1 step:1252 [D loss: 0.484524, acc: 75.78%] [G loss: 3.062472]\n",
      "epoch:1 step:1253 [D loss: 0.686991, acc: 62.50%] [G loss: 2.638622]\n",
      "epoch:1 step:1254 [D loss: 0.569169, acc: 71.88%] [G loss: 2.513895]\n",
      "epoch:1 step:1255 [D loss: 0.593997, acc: 67.19%] [G loss: 2.759349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1256 [D loss: 0.501373, acc: 77.34%] [G loss: 2.459445]\n",
      "epoch:1 step:1257 [D loss: 0.589949, acc: 66.41%] [G loss: 2.683157]\n",
      "epoch:1 step:1258 [D loss: 0.530690, acc: 74.22%] [G loss: 2.669424]\n",
      "epoch:1 step:1259 [D loss: 0.584226, acc: 70.31%] [G loss: 2.836909]\n",
      "epoch:1 step:1260 [D loss: 0.544898, acc: 72.66%] [G loss: 2.602577]\n",
      "epoch:1 step:1261 [D loss: 0.505382, acc: 69.53%] [G loss: 2.671946]\n",
      "epoch:1 step:1262 [D loss: 0.555225, acc: 67.97%] [G loss: 2.779319]\n",
      "epoch:1 step:1263 [D loss: 0.646835, acc: 64.84%] [G loss: 2.520350]\n",
      "epoch:1 step:1264 [D loss: 0.695738, acc: 65.62%] [G loss: 2.433465]\n",
      "epoch:1 step:1265 [D loss: 0.549226, acc: 75.78%] [G loss: 2.626476]\n",
      "epoch:1 step:1266 [D loss: 0.655990, acc: 60.94%] [G loss: 2.624719]\n",
      "epoch:1 step:1267 [D loss: 0.643735, acc: 62.50%] [G loss: 2.584458]\n",
      "epoch:1 step:1268 [D loss: 0.552721, acc: 68.75%] [G loss: 2.684985]\n",
      "epoch:1 step:1269 [D loss: 0.553196, acc: 69.53%] [G loss: 2.593955]\n",
      "epoch:1 step:1270 [D loss: 0.588281, acc: 64.06%] [G loss: 2.870947]\n",
      "epoch:1 step:1271 [D loss: 0.565773, acc: 65.62%] [G loss: 2.572522]\n",
      "epoch:1 step:1272 [D loss: 0.487190, acc: 78.12%] [G loss: 3.119198]\n",
      "epoch:1 step:1273 [D loss: 0.540148, acc: 78.91%] [G loss: 2.924861]\n",
      "epoch:1 step:1274 [D loss: 0.506498, acc: 71.88%] [G loss: 2.898759]\n",
      "epoch:1 step:1275 [D loss: 0.671334, acc: 58.59%] [G loss: 2.660968]\n",
      "epoch:1 step:1276 [D loss: 0.607522, acc: 64.06%] [G loss: 2.495646]\n",
      "epoch:1 step:1277 [D loss: 0.639298, acc: 69.53%] [G loss: 2.488236]\n",
      "epoch:1 step:1278 [D loss: 0.627244, acc: 66.41%] [G loss: 2.802777]\n",
      "epoch:1 step:1279 [D loss: 0.574725, acc: 69.53%] [G loss: 2.787148]\n",
      "epoch:1 step:1280 [D loss: 0.539610, acc: 69.53%] [G loss: 2.970664]\n",
      "epoch:1 step:1281 [D loss: 0.518483, acc: 73.44%] [G loss: 2.874357]\n",
      "epoch:1 step:1282 [D loss: 0.601068, acc: 71.88%] [G loss: 2.994030]\n",
      "epoch:1 step:1283 [D loss: 0.580445, acc: 72.66%] [G loss: 2.695240]\n",
      "epoch:1 step:1284 [D loss: 0.453969, acc: 81.25%] [G loss: 2.700173]\n",
      "epoch:1 step:1285 [D loss: 0.711403, acc: 58.59%] [G loss: 2.567963]\n",
      "epoch:1 step:1286 [D loss: 0.614923, acc: 64.06%] [G loss: 2.626217]\n",
      "epoch:1 step:1287 [D loss: 0.537786, acc: 70.31%] [G loss: 2.703398]\n",
      "epoch:1 step:1288 [D loss: 0.657659, acc: 66.41%] [G loss: 2.726784]\n",
      "epoch:1 step:1289 [D loss: 0.573174, acc: 68.75%] [G loss: 2.753651]\n",
      "epoch:1 step:1290 [D loss: 0.502761, acc: 75.00%] [G loss: 2.657816]\n",
      "epoch:1 step:1291 [D loss: 0.547335, acc: 71.88%] [G loss: 2.735243]\n",
      "epoch:1 step:1292 [D loss: 0.536974, acc: 78.12%] [G loss: 2.953697]\n",
      "epoch:1 step:1293 [D loss: 0.642019, acc: 64.06%] [G loss: 2.619681]\n",
      "epoch:1 step:1294 [D loss: 0.663232, acc: 64.84%] [G loss: 2.784257]\n",
      "epoch:1 step:1295 [D loss: 0.504288, acc: 69.53%] [G loss: 3.155700]\n",
      "epoch:1 step:1296 [D loss: 0.520074, acc: 72.66%] [G loss: 3.060580]\n",
      "epoch:1 step:1297 [D loss: 0.556462, acc: 70.31%] [G loss: 2.975920]\n",
      "epoch:1 step:1298 [D loss: 0.515715, acc: 75.00%] [G loss: 2.890842]\n",
      "epoch:1 step:1299 [D loss: 0.617376, acc: 57.03%] [G loss: 2.540565]\n",
      "epoch:1 step:1300 [D loss: 0.585795, acc: 72.66%] [G loss: 2.776720]\n",
      "epoch:1 step:1301 [D loss: 0.485297, acc: 78.12%] [G loss: 3.115934]\n",
      "epoch:1 step:1302 [D loss: 0.583942, acc: 73.44%] [G loss: 2.895423]\n",
      "epoch:1 step:1303 [D loss: 0.559182, acc: 73.44%] [G loss: 2.898258]\n",
      "epoch:1 step:1304 [D loss: 0.547162, acc: 71.88%] [G loss: 2.866078]\n",
      "epoch:1 step:1305 [D loss: 0.564955, acc: 67.97%] [G loss: 2.606415]\n",
      "epoch:1 step:1306 [D loss: 0.565696, acc: 69.53%] [G loss: 2.808981]\n",
      "epoch:1 step:1307 [D loss: 0.618863, acc: 67.19%] [G loss: 2.746213]\n",
      "epoch:1 step:1308 [D loss: 0.568749, acc: 75.00%] [G loss: 2.554653]\n",
      "epoch:1 step:1309 [D loss: 0.630110, acc: 70.31%] [G loss: 2.586926]\n",
      "epoch:1 step:1310 [D loss: 0.655400, acc: 60.94%] [G loss: 2.731156]\n",
      "epoch:1 step:1311 [D loss: 0.515621, acc: 73.44%] [G loss: 2.797687]\n",
      "epoch:1 step:1312 [D loss: 0.666628, acc: 60.16%] [G loss: 2.569516]\n",
      "epoch:1 step:1313 [D loss: 0.616239, acc: 63.28%] [G loss: 2.389478]\n",
      "epoch:1 step:1314 [D loss: 0.678435, acc: 60.94%] [G loss: 2.723387]\n",
      "epoch:1 step:1315 [D loss: 0.511777, acc: 75.00%] [G loss: 2.750024]\n",
      "epoch:1 step:1316 [D loss: 0.657723, acc: 63.28%] [G loss: 2.708552]\n",
      "epoch:1 step:1317 [D loss: 0.576949, acc: 70.31%] [G loss: 2.684477]\n",
      "epoch:1 step:1318 [D loss: 0.530073, acc: 78.12%] [G loss: 2.777930]\n",
      "epoch:1 step:1319 [D loss: 0.478202, acc: 78.91%] [G loss: 2.846439]\n",
      "epoch:1 step:1320 [D loss: 0.569787, acc: 67.97%] [G loss: 2.823453]\n",
      "epoch:1 step:1321 [D loss: 0.552146, acc: 71.09%] [G loss: 2.674216]\n",
      "epoch:1 step:1322 [D loss: 0.560026, acc: 73.44%] [G loss: 2.692059]\n",
      "epoch:1 step:1323 [D loss: 0.562208, acc: 67.19%] [G loss: 2.565027]\n",
      "epoch:1 step:1324 [D loss: 0.615526, acc: 65.62%] [G loss: 2.783967]\n",
      "epoch:1 step:1325 [D loss: 0.535836, acc: 74.22%] [G loss: 2.797494]\n",
      "epoch:1 step:1326 [D loss: 0.596692, acc: 71.88%] [G loss: 2.588761]\n",
      "epoch:1 step:1327 [D loss: 0.620238, acc: 67.97%] [G loss: 2.709399]\n",
      "epoch:1 step:1328 [D loss: 0.717617, acc: 53.91%] [G loss: 2.517647]\n",
      "epoch:1 step:1329 [D loss: 0.585450, acc: 67.97%] [G loss: 2.908246]\n",
      "epoch:1 step:1330 [D loss: 0.670409, acc: 64.06%] [G loss: 2.521439]\n",
      "epoch:1 step:1331 [D loss: 0.562384, acc: 69.53%] [G loss: 2.587424]\n",
      "epoch:1 step:1332 [D loss: 0.618555, acc: 61.72%] [G loss: 2.594223]\n",
      "epoch:1 step:1333 [D loss: 0.625230, acc: 68.75%] [G loss: 2.677397]\n",
      "epoch:1 step:1334 [D loss: 0.568279, acc: 70.31%] [G loss: 3.149740]\n",
      "epoch:1 step:1335 [D loss: 0.509115, acc: 75.78%] [G loss: 2.958783]\n",
      "epoch:1 step:1336 [D loss: 0.484919, acc: 72.66%] [G loss: 2.881894]\n",
      "epoch:1 step:1337 [D loss: 0.618300, acc: 65.62%] [G loss: 2.736753]\n",
      "epoch:1 step:1338 [D loss: 0.629574, acc: 64.06%] [G loss: 2.841787]\n",
      "epoch:1 step:1339 [D loss: 0.504457, acc: 78.12%] [G loss: 2.854233]\n",
      "epoch:1 step:1340 [D loss: 0.541007, acc: 70.31%] [G loss: 2.845970]\n",
      "epoch:1 step:1341 [D loss: 0.635330, acc: 62.50%] [G loss: 2.597088]\n",
      "epoch:1 step:1342 [D loss: 0.553031, acc: 67.19%] [G loss: 2.641984]\n",
      "epoch:1 step:1343 [D loss: 0.591617, acc: 67.19%] [G loss: 2.766390]\n",
      "epoch:1 step:1344 [D loss: 0.550402, acc: 73.44%] [G loss: 2.583403]\n",
      "epoch:1 step:1345 [D loss: 0.544655, acc: 72.66%] [G loss: 2.685258]\n",
      "epoch:1 step:1346 [D loss: 0.615403, acc: 60.16%] [G loss: 2.772103]\n",
      "epoch:1 step:1347 [D loss: 0.608758, acc: 70.31%] [G loss: 2.708212]\n",
      "epoch:1 step:1348 [D loss: 0.638166, acc: 64.06%] [G loss: 2.786908]\n",
      "epoch:1 step:1349 [D loss: 0.651358, acc: 61.72%] [G loss: 2.633486]\n",
      "epoch:1 step:1350 [D loss: 0.580442, acc: 67.97%] [G loss: 2.700725]\n",
      "epoch:1 step:1351 [D loss: 0.695429, acc: 57.03%] [G loss: 2.431440]\n",
      "epoch:1 step:1352 [D loss: 0.642011, acc: 69.53%] [G loss: 2.639438]\n",
      "epoch:1 step:1353 [D loss: 0.546735, acc: 73.44%] [G loss: 2.723117]\n",
      "epoch:1 step:1354 [D loss: 0.642258, acc: 62.50%] [G loss: 2.859711]\n",
      "epoch:1 step:1355 [D loss: 0.566756, acc: 68.75%] [G loss: 2.789574]\n",
      "epoch:1 step:1356 [D loss: 0.535438, acc: 75.00%] [G loss: 2.721321]\n",
      "epoch:1 step:1357 [D loss: 0.591959, acc: 67.19%] [G loss: 2.725831]\n",
      "epoch:1 step:1358 [D loss: 0.600163, acc: 67.19%] [G loss: 2.746628]\n",
      "epoch:1 step:1359 [D loss: 0.524725, acc: 71.88%] [G loss: 2.725046]\n",
      "epoch:1 step:1360 [D loss: 0.517193, acc: 82.03%] [G loss: 2.583746]\n",
      "epoch:1 step:1361 [D loss: 0.540172, acc: 71.09%] [G loss: 2.787658]\n",
      "epoch:1 step:1362 [D loss: 0.577069, acc: 70.31%] [G loss: 2.994508]\n",
      "epoch:1 step:1363 [D loss: 0.513282, acc: 73.44%] [G loss: 3.258656]\n",
      "epoch:1 step:1364 [D loss: 0.577359, acc: 67.19%] [G loss: 3.140839]\n",
      "epoch:1 step:1365 [D loss: 0.590892, acc: 68.75%] [G loss: 2.908798]\n",
      "epoch:1 step:1366 [D loss: 0.561511, acc: 72.66%] [G loss: 2.900505]\n",
      "epoch:1 step:1367 [D loss: 0.526322, acc: 76.56%] [G loss: 2.807790]\n",
      "epoch:1 step:1368 [D loss: 0.659705, acc: 64.84%] [G loss: 2.735885]\n",
      "epoch:1 step:1369 [D loss: 0.690875, acc: 60.94%] [G loss: 2.500053]\n",
      "epoch:1 step:1370 [D loss: 0.636945, acc: 62.50%] [G loss: 2.713974]\n",
      "epoch:1 step:1371 [D loss: 0.570729, acc: 71.09%] [G loss: 2.740242]\n",
      "epoch:1 step:1372 [D loss: 0.505581, acc: 79.69%] [G loss: 2.733828]\n",
      "epoch:1 step:1373 [D loss: 0.603365, acc: 69.53%] [G loss: 2.786118]\n",
      "epoch:1 step:1374 [D loss: 0.618039, acc: 66.41%] [G loss: 2.354227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1375 [D loss: 0.519441, acc: 72.66%] [G loss: 2.607289]\n",
      "epoch:1 step:1376 [D loss: 0.560358, acc: 71.88%] [G loss: 2.668476]\n",
      "epoch:1 step:1377 [D loss: 0.603029, acc: 64.06%] [G loss: 2.789389]\n",
      "epoch:1 step:1378 [D loss: 0.546085, acc: 73.44%] [G loss: 2.670971]\n",
      "epoch:1 step:1379 [D loss: 0.597262, acc: 70.31%] [G loss: 2.805202]\n",
      "epoch:1 step:1380 [D loss: 0.608541, acc: 68.75%] [G loss: 2.617368]\n",
      "epoch:1 step:1381 [D loss: 0.501773, acc: 75.78%] [G loss: 3.061756]\n",
      "epoch:1 step:1382 [D loss: 0.608172, acc: 66.41%] [G loss: 2.761619]\n",
      "epoch:1 step:1383 [D loss: 0.568926, acc: 72.66%] [G loss: 2.764050]\n",
      "epoch:1 step:1384 [D loss: 0.475479, acc: 77.34%] [G loss: 3.193751]\n",
      "epoch:1 step:1385 [D loss: 0.649643, acc: 65.62%] [G loss: 2.762894]\n",
      "epoch:1 step:1386 [D loss: 0.648858, acc: 62.50%] [G loss: 2.605658]\n",
      "epoch:1 step:1387 [D loss: 0.635575, acc: 66.41%] [G loss: 2.720482]\n",
      "epoch:1 step:1388 [D loss: 0.641429, acc: 65.62%] [G loss: 2.804813]\n",
      "epoch:1 step:1389 [D loss: 0.537495, acc: 75.00%] [G loss: 2.799301]\n",
      "epoch:1 step:1390 [D loss: 0.612205, acc: 69.53%] [G loss: 2.524610]\n",
      "epoch:1 step:1391 [D loss: 0.612025, acc: 66.41%] [G loss: 2.450077]\n",
      "epoch:1 step:1392 [D loss: 0.576018, acc: 67.97%] [G loss: 2.541091]\n",
      "epoch:1 step:1393 [D loss: 0.615830, acc: 66.41%] [G loss: 2.444222]\n",
      "epoch:1 step:1394 [D loss: 0.620246, acc: 67.97%] [G loss: 2.607870]\n",
      "epoch:1 step:1395 [D loss: 0.570832, acc: 71.88%] [G loss: 2.767837]\n",
      "epoch:1 step:1396 [D loss: 0.608269, acc: 64.84%] [G loss: 2.569665]\n",
      "epoch:1 step:1397 [D loss: 0.606816, acc: 67.19%] [G loss: 2.763274]\n",
      "epoch:1 step:1398 [D loss: 0.525898, acc: 75.78%] [G loss: 3.001145]\n",
      "epoch:1 step:1399 [D loss: 0.612125, acc: 71.88%] [G loss: 2.630600]\n",
      "epoch:1 step:1400 [D loss: 0.643288, acc: 60.94%] [G loss: 2.456063]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.154058\n",
      "FID: 90.550613\n",
      "0 = 13.435105851650246\n",
      "1 = 0.11949629927136146\n",
      "2 = 0.964900016784668\n",
      "3 = 0.9345999956130981\n",
      "4 = 0.995199978351593\n",
      "5 = 0.9948903322219849\n",
      "6 = 0.9345999956130981\n",
      "7 = 11.117504952216134\n",
      "8 = 0.18381957105974828\n",
      "9 = 0.9318000078201294\n",
      "10 = 0.9205999970436096\n",
      "11 = 0.9430000185966492\n",
      "12 = 0.9416939616203308\n",
      "13 = 0.9205999970436096\n",
      "14 = 4.15406608581543\n",
      "15 = 8.22253704071045\n",
      "16 = 0.3637545108795166\n",
      "17 = 4.154058456420898\n",
      "18 = 90.55061340332031\n",
      "epoch:1 step:1401 [D loss: 0.538623, acc: 71.09%] [G loss: 2.421565]\n",
      "epoch:1 step:1402 [D loss: 0.679129, acc: 61.72%] [G loss: 2.459361]\n",
      "epoch:1 step:1403 [D loss: 0.575261, acc: 70.31%] [G loss: 2.546901]\n",
      "epoch:1 step:1404 [D loss: 0.655956, acc: 63.28%] [G loss: 2.490265]\n",
      "epoch:1 step:1405 [D loss: 0.651737, acc: 65.62%] [G loss: 2.497447]\n",
      "epoch:1 step:1406 [D loss: 0.650216, acc: 64.06%] [G loss: 2.737299]\n",
      "epoch:1 step:1407 [D loss: 0.585540, acc: 66.41%] [G loss: 2.595718]\n",
      "epoch:1 step:1408 [D loss: 0.549351, acc: 67.97%] [G loss: 3.089478]\n",
      "epoch:1 step:1409 [D loss: 0.556171, acc: 73.44%] [G loss: 2.694142]\n",
      "epoch:1 step:1410 [D loss: 0.607285, acc: 69.53%] [G loss: 2.454994]\n",
      "epoch:1 step:1411 [D loss: 0.571829, acc: 68.75%] [G loss: 2.588709]\n",
      "epoch:1 step:1412 [D loss: 0.523344, acc: 74.22%] [G loss: 3.042371]\n",
      "epoch:1 step:1413 [D loss: 0.655147, acc: 64.84%] [G loss: 2.700490]\n",
      "epoch:1 step:1414 [D loss: 0.653524, acc: 61.72%] [G loss: 2.497208]\n",
      "epoch:1 step:1415 [D loss: 0.678498, acc: 59.38%] [G loss: 2.522438]\n",
      "epoch:1 step:1416 [D loss: 0.639409, acc: 65.62%] [G loss: 2.537050]\n",
      "epoch:1 step:1417 [D loss: 0.583762, acc: 67.97%] [G loss: 2.355633]\n",
      "epoch:1 step:1418 [D loss: 0.596121, acc: 67.19%] [G loss: 2.603529]\n",
      "epoch:1 step:1419 [D loss: 0.581413, acc: 70.31%] [G loss: 2.687503]\n",
      "epoch:1 step:1420 [D loss: 0.591155, acc: 69.53%] [G loss: 2.577462]\n",
      "epoch:1 step:1421 [D loss: 0.561698, acc: 67.19%] [G loss: 2.902641]\n",
      "epoch:1 step:1422 [D loss: 0.568257, acc: 68.75%] [G loss: 2.680088]\n",
      "epoch:1 step:1423 [D loss: 0.605980, acc: 65.62%] [G loss: 2.626832]\n",
      "epoch:1 step:1424 [D loss: 0.561318, acc: 75.78%] [G loss: 2.632823]\n",
      "epoch:1 step:1425 [D loss: 0.531678, acc: 76.56%] [G loss: 3.070554]\n",
      "epoch:1 step:1426 [D loss: 0.626180, acc: 67.97%] [G loss: 2.656138]\n",
      "epoch:1 step:1427 [D loss: 0.682165, acc: 58.59%] [G loss: 2.519728]\n",
      "epoch:1 step:1428 [D loss: 0.588687, acc: 68.75%] [G loss: 2.594172]\n",
      "epoch:1 step:1429 [D loss: 0.644701, acc: 62.50%] [G loss: 2.780076]\n",
      "epoch:1 step:1430 [D loss: 0.687259, acc: 55.47%] [G loss: 2.567780]\n",
      "epoch:1 step:1431 [D loss: 0.594358, acc: 72.66%] [G loss: 2.726387]\n",
      "epoch:1 step:1432 [D loss: 0.622159, acc: 66.41%] [G loss: 2.564621]\n",
      "epoch:1 step:1433 [D loss: 0.609765, acc: 68.75%] [G loss: 2.428242]\n",
      "epoch:1 step:1434 [D loss: 0.549996, acc: 71.09%] [G loss: 2.856740]\n",
      "epoch:1 step:1435 [D loss: 0.584833, acc: 69.53%] [G loss: 2.911019]\n",
      "epoch:1 step:1436 [D loss: 0.475217, acc: 83.59%] [G loss: 2.804035]\n",
      "epoch:1 step:1437 [D loss: 0.744073, acc: 57.81%] [G loss: 2.514413]\n",
      "epoch:1 step:1438 [D loss: 0.652814, acc: 60.94%] [G loss: 2.611459]\n",
      "epoch:1 step:1439 [D loss: 0.642677, acc: 65.62%] [G loss: 2.530634]\n",
      "epoch:1 step:1440 [D loss: 0.562596, acc: 75.00%] [G loss: 2.724525]\n",
      "epoch:1 step:1441 [D loss: 0.574624, acc: 73.44%] [G loss: 2.749221]\n",
      "epoch:1 step:1442 [D loss: 0.652190, acc: 66.41%] [G loss: 2.251218]\n",
      "epoch:1 step:1443 [D loss: 0.591707, acc: 70.31%] [G loss: 2.667799]\n",
      "epoch:1 step:1444 [D loss: 0.563063, acc: 69.53%] [G loss: 2.668677]\n",
      "epoch:1 step:1445 [D loss: 0.493850, acc: 77.34%] [G loss: 3.125648]\n",
      "epoch:1 step:1446 [D loss: 0.565966, acc: 71.88%] [G loss: 2.915425]\n",
      "epoch:1 step:1447 [D loss: 0.605692, acc: 67.97%] [G loss: 2.575539]\n",
      "epoch:1 step:1448 [D loss: 0.629182, acc: 63.28%] [G loss: 2.466778]\n",
      "epoch:1 step:1449 [D loss: 0.608501, acc: 67.97%] [G loss: 2.572123]\n",
      "epoch:1 step:1450 [D loss: 0.544530, acc: 71.88%] [G loss: 2.652014]\n",
      "epoch:1 step:1451 [D loss: 0.522073, acc: 76.56%] [G loss: 2.476741]\n",
      "epoch:1 step:1452 [D loss: 0.533788, acc: 73.44%] [G loss: 2.809355]\n",
      "epoch:1 step:1453 [D loss: 0.563411, acc: 71.88%] [G loss: 2.663063]\n",
      "epoch:1 step:1454 [D loss: 0.710095, acc: 62.50%] [G loss: 2.622693]\n",
      "epoch:1 step:1455 [D loss: 0.580203, acc: 69.53%] [G loss: 2.547088]\n",
      "epoch:1 step:1456 [D loss: 0.575844, acc: 67.97%] [G loss: 2.647869]\n",
      "epoch:1 step:1457 [D loss: 0.629095, acc: 64.06%] [G loss: 2.471573]\n",
      "epoch:1 step:1458 [D loss: 0.580671, acc: 71.88%] [G loss: 2.554299]\n",
      "epoch:1 step:1459 [D loss: 0.570036, acc: 75.00%] [G loss: 2.799598]\n",
      "epoch:1 step:1460 [D loss: 0.531034, acc: 76.56%] [G loss: 2.657245]\n",
      "epoch:1 step:1461 [D loss: 0.605364, acc: 67.19%] [G loss: 2.498190]\n",
      "epoch:1 step:1462 [D loss: 0.606175, acc: 69.53%] [G loss: 2.461886]\n",
      "epoch:1 step:1463 [D loss: 0.589432, acc: 71.09%] [G loss: 2.532979]\n",
      "epoch:1 step:1464 [D loss: 0.654767, acc: 64.84%] [G loss: 2.527150]\n",
      "epoch:1 step:1465 [D loss: 0.612851, acc: 72.66%] [G loss: 2.340241]\n",
      "epoch:1 step:1466 [D loss: 0.587325, acc: 70.31%] [G loss: 2.648833]\n",
      "epoch:1 step:1467 [D loss: 0.561385, acc: 68.75%] [G loss: 2.481313]\n",
      "epoch:1 step:1468 [D loss: 0.622379, acc: 61.72%] [G loss: 2.418303]\n",
      "epoch:1 step:1469 [D loss: 0.591603, acc: 67.19%] [G loss: 2.541958]\n",
      "epoch:1 step:1470 [D loss: 0.665593, acc: 57.81%] [G loss: 2.478162]\n",
      "epoch:1 step:1471 [D loss: 0.586960, acc: 68.75%] [G loss: 2.733512]\n",
      "epoch:1 step:1472 [D loss: 0.600732, acc: 66.41%] [G loss: 2.267231]\n",
      "epoch:1 step:1473 [D loss: 0.576297, acc: 68.75%] [G loss: 3.035191]\n",
      "epoch:1 step:1474 [D loss: 0.656118, acc: 64.06%] [G loss: 2.703276]\n",
      "epoch:1 step:1475 [D loss: 0.566530, acc: 75.00%] [G loss: 2.666359]\n",
      "epoch:1 step:1476 [D loss: 0.578811, acc: 64.84%] [G loss: 2.457216]\n",
      "epoch:1 step:1477 [D loss: 0.569265, acc: 70.31%] [G loss: 2.792034]\n",
      "epoch:1 step:1478 [D loss: 0.487125, acc: 79.69%] [G loss: 2.823887]\n",
      "epoch:1 step:1479 [D loss: 0.594175, acc: 67.19%] [G loss: 2.554963]\n",
      "epoch:1 step:1480 [D loss: 0.616070, acc: 66.41%] [G loss: 2.477267]\n",
      "epoch:1 step:1481 [D loss: 0.534331, acc: 74.22%] [G loss: 2.858016]\n",
      "epoch:1 step:1482 [D loss: 0.599307, acc: 67.97%] [G loss: 2.565656]\n",
      "epoch:1 step:1483 [D loss: 0.548616, acc: 73.44%] [G loss: 2.804245]\n",
      "epoch:1 step:1484 [D loss: 0.568469, acc: 72.66%] [G loss: 2.875168]\n",
      "epoch:1 step:1485 [D loss: 0.589107, acc: 71.09%] [G loss: 2.830377]\n",
      "epoch:1 step:1486 [D loss: 0.546553, acc: 71.09%] [G loss: 2.843459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1487 [D loss: 0.575247, acc: 67.97%] [G loss: 2.674036]\n",
      "epoch:1 step:1488 [D loss: 0.655744, acc: 57.03%] [G loss: 2.612148]\n",
      "epoch:1 step:1489 [D loss: 0.584414, acc: 71.88%] [G loss: 2.708230]\n",
      "epoch:1 step:1490 [D loss: 0.522153, acc: 74.22%] [G loss: 2.776170]\n",
      "epoch:1 step:1491 [D loss: 0.519956, acc: 71.88%] [G loss: 3.112981]\n",
      "epoch:1 step:1492 [D loss: 0.469130, acc: 75.78%] [G loss: 2.862240]\n",
      "epoch:1 step:1493 [D loss: 0.495811, acc: 75.00%] [G loss: 2.930060]\n",
      "epoch:1 step:1494 [D loss: 0.460849, acc: 80.47%] [G loss: 2.841333]\n",
      "epoch:1 step:1495 [D loss: 0.512665, acc: 71.88%] [G loss: 3.366887]\n",
      "epoch:1 step:1496 [D loss: 0.605047, acc: 65.62%] [G loss: 2.790430]\n",
      "epoch:1 step:1497 [D loss: 0.561895, acc: 69.53%] [G loss: 2.634494]\n",
      "epoch:1 step:1498 [D loss: 0.505098, acc: 74.22%] [G loss: 2.687657]\n",
      "epoch:1 step:1499 [D loss: 0.593228, acc: 67.97%] [G loss: 2.654158]\n",
      "epoch:1 step:1500 [D loss: 0.635577, acc: 61.72%] [G loss: 2.488928]\n",
      "epoch:1 step:1501 [D loss: 0.571283, acc: 69.53%] [G loss: 2.738320]\n",
      "epoch:1 step:1502 [D loss: 0.543549, acc: 72.66%] [G loss: 3.018526]\n",
      "epoch:1 step:1503 [D loss: 0.606612, acc: 68.75%] [G loss: 2.802919]\n",
      "epoch:1 step:1504 [D loss: 0.563565, acc: 71.88%] [G loss: 3.099520]\n",
      "epoch:1 step:1505 [D loss: 0.565346, acc: 66.41%] [G loss: 2.680059]\n",
      "epoch:1 step:1506 [D loss: 0.573081, acc: 71.09%] [G loss: 2.528259]\n",
      "epoch:1 step:1507 [D loss: 0.574906, acc: 69.53%] [G loss: 2.705677]\n",
      "epoch:1 step:1508 [D loss: 0.555452, acc: 71.88%] [G loss: 2.933059]\n",
      "epoch:1 step:1509 [D loss: 0.540070, acc: 71.88%] [G loss: 2.861837]\n",
      "epoch:1 step:1510 [D loss: 0.531247, acc: 75.00%] [G loss: 3.014097]\n",
      "epoch:1 step:1511 [D loss: 0.562123, acc: 66.41%] [G loss: 2.878609]\n",
      "epoch:1 step:1512 [D loss: 0.529536, acc: 70.31%] [G loss: 3.181527]\n",
      "epoch:1 step:1513 [D loss: 0.524088, acc: 71.09%] [G loss: 2.675139]\n",
      "epoch:1 step:1514 [D loss: 0.638173, acc: 68.75%] [G loss: 2.807692]\n",
      "epoch:1 step:1515 [D loss: 0.575785, acc: 70.31%] [G loss: 2.876903]\n",
      "epoch:1 step:1516 [D loss: 0.565980, acc: 69.53%] [G loss: 2.916260]\n",
      "epoch:1 step:1517 [D loss: 0.564444, acc: 75.00%] [G loss: 2.978938]\n",
      "epoch:1 step:1518 [D loss: 0.554933, acc: 75.78%] [G loss: 2.707293]\n",
      "epoch:1 step:1519 [D loss: 0.468382, acc: 80.47%] [G loss: 3.100367]\n",
      "epoch:1 step:1520 [D loss: 0.562456, acc: 68.75%] [G loss: 3.032987]\n",
      "epoch:1 step:1521 [D loss: 0.641231, acc: 65.62%] [G loss: 2.604730]\n",
      "epoch:1 step:1522 [D loss: 0.629221, acc: 64.84%] [G loss: 2.832718]\n",
      "epoch:1 step:1523 [D loss: 0.552923, acc: 74.22%] [G loss: 2.710773]\n",
      "epoch:1 step:1524 [D loss: 0.556084, acc: 67.97%] [G loss: 2.804440]\n",
      "epoch:1 step:1525 [D loss: 0.525110, acc: 77.34%] [G loss: 2.899614]\n",
      "epoch:1 step:1526 [D loss: 0.583552, acc: 71.09%] [G loss: 2.912713]\n",
      "epoch:1 step:1527 [D loss: 0.589006, acc: 68.75%] [G loss: 2.674352]\n",
      "epoch:1 step:1528 [D loss: 0.607691, acc: 66.41%] [G loss: 2.593522]\n",
      "epoch:1 step:1529 [D loss: 0.534744, acc: 71.88%] [G loss: 3.028498]\n",
      "epoch:1 step:1530 [D loss: 0.686807, acc: 60.94%] [G loss: 2.552219]\n",
      "epoch:1 step:1531 [D loss: 0.593442, acc: 71.88%] [G loss: 2.624865]\n",
      "epoch:1 step:1532 [D loss: 0.543410, acc: 71.09%] [G loss: 2.739277]\n",
      "epoch:1 step:1533 [D loss: 0.595003, acc: 68.75%] [G loss: 2.808532]\n",
      "epoch:1 step:1534 [D loss: 0.501285, acc: 77.34%] [G loss: 3.089821]\n",
      "epoch:1 step:1535 [D loss: 0.511837, acc: 76.56%] [G loss: 2.757346]\n",
      "epoch:1 step:1536 [D loss: 0.545851, acc: 67.97%] [G loss: 2.498016]\n",
      "epoch:1 step:1537 [D loss: 0.615975, acc: 69.53%] [G loss: 2.832757]\n",
      "epoch:1 step:1538 [D loss: 0.601704, acc: 65.62%] [G loss: 2.979498]\n",
      "epoch:1 step:1539 [D loss: 0.563167, acc: 67.97%] [G loss: 2.466283]\n",
      "epoch:1 step:1540 [D loss: 0.537907, acc: 73.44%] [G loss: 3.373160]\n",
      "epoch:1 step:1541 [D loss: 0.585508, acc: 65.62%] [G loss: 2.735456]\n",
      "epoch:1 step:1542 [D loss: 0.574136, acc: 67.19%] [G loss: 2.839230]\n",
      "epoch:1 step:1543 [D loss: 0.519227, acc: 79.69%] [G loss: 2.796639]\n",
      "epoch:1 step:1544 [D loss: 0.567565, acc: 69.53%] [G loss: 2.775145]\n",
      "epoch:1 step:1545 [D loss: 0.546366, acc: 75.00%] [G loss: 2.776804]\n",
      "epoch:1 step:1546 [D loss: 0.519557, acc: 74.22%] [G loss: 2.910972]\n",
      "epoch:1 step:1547 [D loss: 0.515371, acc: 71.88%] [G loss: 2.865732]\n",
      "epoch:1 step:1548 [D loss: 0.594821, acc: 64.84%] [G loss: 3.092702]\n",
      "epoch:1 step:1549 [D loss: 0.512810, acc: 78.12%] [G loss: 3.041737]\n",
      "epoch:1 step:1550 [D loss: 0.476395, acc: 80.47%] [G loss: 2.936743]\n",
      "epoch:1 step:1551 [D loss: 0.637897, acc: 67.19%] [G loss: 2.968111]\n",
      "epoch:1 step:1552 [D loss: 0.637748, acc: 65.62%] [G loss: 2.663423]\n",
      "epoch:1 step:1553 [D loss: 0.576486, acc: 67.19%] [G loss: 2.711859]\n",
      "epoch:1 step:1554 [D loss: 0.514188, acc: 72.66%] [G loss: 2.570530]\n",
      "epoch:1 step:1555 [D loss: 0.504934, acc: 75.78%] [G loss: 2.820331]\n",
      "epoch:1 step:1556 [D loss: 0.478965, acc: 76.56%] [G loss: 3.055031]\n",
      "epoch:1 step:1557 [D loss: 0.537078, acc: 71.88%] [G loss: 2.713392]\n",
      "epoch:1 step:1558 [D loss: 0.689803, acc: 58.59%] [G loss: 2.741433]\n",
      "epoch:1 step:1559 [D loss: 0.660340, acc: 64.84%] [G loss: 2.474357]\n",
      "epoch:1 step:1560 [D loss: 0.650880, acc: 64.06%] [G loss: 2.981882]\n",
      "epoch:1 step:1561 [D loss: 0.561580, acc: 71.09%] [G loss: 2.615624]\n",
      "epoch:1 step:1562 [D loss: 0.543663, acc: 73.44%] [G loss: 3.042379]\n",
      "epoch:1 step:1563 [D loss: 0.476610, acc: 78.12%] [G loss: 2.868311]\n",
      "epoch:1 step:1564 [D loss: 0.448510, acc: 82.81%] [G loss: 3.072767]\n",
      "epoch:1 step:1565 [D loss: 0.550597, acc: 67.19%] [G loss: 2.925694]\n",
      "epoch:1 step:1566 [D loss: 0.576368, acc: 71.09%] [G loss: 3.181079]\n",
      "epoch:1 step:1567 [D loss: 0.594709, acc: 64.84%] [G loss: 2.641431]\n",
      "epoch:1 step:1568 [D loss: 0.502098, acc: 75.00%] [G loss: 2.981570]\n",
      "epoch:1 step:1569 [D loss: 0.512823, acc: 75.78%] [G loss: 2.938547]\n",
      "epoch:1 step:1570 [D loss: 0.618652, acc: 70.31%] [G loss: 3.302850]\n",
      "epoch:1 step:1571 [D loss: 0.490563, acc: 79.69%] [G loss: 3.098478]\n",
      "epoch:1 step:1572 [D loss: 0.453363, acc: 82.03%] [G loss: 3.173627]\n",
      "epoch:1 step:1573 [D loss: 0.626427, acc: 64.06%] [G loss: 2.784609]\n",
      "epoch:1 step:1574 [D loss: 0.548715, acc: 76.56%] [G loss: 2.664701]\n",
      "epoch:1 step:1575 [D loss: 0.537190, acc: 70.31%] [G loss: 3.257916]\n",
      "epoch:1 step:1576 [D loss: 0.475356, acc: 78.91%] [G loss: 3.278905]\n",
      "epoch:1 step:1577 [D loss: 0.476404, acc: 79.69%] [G loss: 2.903064]\n",
      "epoch:1 step:1578 [D loss: 0.477023, acc: 76.56%] [G loss: 3.287768]\n",
      "epoch:1 step:1579 [D loss: 0.539440, acc: 71.88%] [G loss: 3.110645]\n",
      "epoch:1 step:1580 [D loss: 0.463274, acc: 80.47%] [G loss: 3.046910]\n",
      "epoch:1 step:1581 [D loss: 0.548986, acc: 70.31%] [G loss: 3.247589]\n",
      "epoch:1 step:1582 [D loss: 0.589175, acc: 65.62%] [G loss: 2.811115]\n",
      "epoch:1 step:1583 [D loss: 0.608281, acc: 66.41%] [G loss: 2.700246]\n",
      "epoch:1 step:1584 [D loss: 0.514977, acc: 75.78%] [G loss: 2.909252]\n",
      "epoch:1 step:1585 [D loss: 0.462636, acc: 75.78%] [G loss: 3.365948]\n",
      "epoch:1 step:1586 [D loss: 0.560658, acc: 70.31%] [G loss: 3.151404]\n",
      "epoch:1 step:1587 [D loss: 0.507302, acc: 74.22%] [G loss: 3.228118]\n",
      "epoch:1 step:1588 [D loss: 0.522308, acc: 76.56%] [G loss: 3.036222]\n",
      "epoch:1 step:1589 [D loss: 0.661435, acc: 63.28%] [G loss: 2.511744]\n",
      "epoch:1 step:1590 [D loss: 0.648711, acc: 64.84%] [G loss: 2.637475]\n",
      "epoch:1 step:1591 [D loss: 0.485591, acc: 75.78%] [G loss: 3.001213]\n",
      "epoch:1 step:1592 [D loss: 0.635829, acc: 65.62%] [G loss: 2.719500]\n",
      "epoch:1 step:1593 [D loss: 0.595298, acc: 66.41%] [G loss: 2.886228]\n",
      "epoch:1 step:1594 [D loss: 0.499601, acc: 72.66%] [G loss: 2.799067]\n",
      "epoch:1 step:1595 [D loss: 0.568542, acc: 69.53%] [G loss: 2.863936]\n",
      "epoch:1 step:1596 [D loss: 0.493934, acc: 78.12%] [G loss: 2.931862]\n",
      "epoch:1 step:1597 [D loss: 0.482081, acc: 78.91%] [G loss: 3.148951]\n",
      "epoch:1 step:1598 [D loss: 0.512586, acc: 75.78%] [G loss: 2.976689]\n",
      "epoch:1 step:1599 [D loss: 0.580728, acc: 67.97%] [G loss: 2.798754]\n",
      "epoch:1 step:1600 [D loss: 0.454500, acc: 78.12%] [G loss: 3.261316]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.284789\n",
      "FID: 85.340576\n",
      "0 = 13.490490371608727\n",
      "1 = 0.11556648629607884\n",
      "2 = 0.9611999988555908\n",
      "3 = 0.928600013256073\n",
      "4 = 0.9937999844551086\n",
      "5 = 0.9933675527572632\n",
      "6 = 0.928600013256073\n",
      "7 = 10.968738705873488\n",
      "8 = 0.17929897597999714\n",
      "9 = 0.9283000230789185\n",
      "10 = 0.9114000201225281\n",
      "11 = 0.9452000260353088\n",
      "12 = 0.943282961845398\n",
      "13 = 0.9114000201225281\n",
      "14 = 4.284797668457031\n",
      "15 = 8.363628387451172\n",
      "16 = 0.34746235609054565\n",
      "17 = 4.284789085388184\n",
      "18 = 85.340576171875\n",
      "epoch:1 step:1601 [D loss: 0.431862, acc: 82.81%] [G loss: 3.189713]\n",
      "epoch:1 step:1602 [D loss: 0.476669, acc: 76.56%] [G loss: 3.071623]\n",
      "epoch:1 step:1603 [D loss: 0.521082, acc: 75.00%] [G loss: 3.241609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1604 [D loss: 0.652402, acc: 64.06%] [G loss: 3.050171]\n",
      "epoch:1 step:1605 [D loss: 0.579242, acc: 72.66%] [G loss: 2.722520]\n",
      "epoch:1 step:1606 [D loss: 0.514770, acc: 74.22%] [G loss: 3.178152]\n",
      "epoch:1 step:1607 [D loss: 0.491018, acc: 78.91%] [G loss: 3.330190]\n",
      "epoch:1 step:1608 [D loss: 0.596060, acc: 67.19%] [G loss: 2.846478]\n",
      "epoch:1 step:1609 [D loss: 0.517413, acc: 72.66%] [G loss: 2.990998]\n",
      "epoch:1 step:1610 [D loss: 0.663953, acc: 64.06%] [G loss: 2.923226]\n",
      "epoch:1 step:1611 [D loss: 0.542234, acc: 71.88%] [G loss: 3.137920]\n",
      "epoch:1 step:1612 [D loss: 0.711529, acc: 64.06%] [G loss: 3.009792]\n",
      "epoch:1 step:1613 [D loss: 0.452775, acc: 78.91%] [G loss: 3.056839]\n",
      "epoch:1 step:1614 [D loss: 0.484585, acc: 78.12%] [G loss: 2.736417]\n",
      "epoch:1 step:1615 [D loss: 0.571337, acc: 68.75%] [G loss: 2.934827]\n",
      "epoch:1 step:1616 [D loss: 0.528215, acc: 75.78%] [G loss: 2.694945]\n",
      "epoch:1 step:1617 [D loss: 0.462693, acc: 78.12%] [G loss: 3.273533]\n",
      "epoch:1 step:1618 [D loss: 0.542821, acc: 75.00%] [G loss: 3.004756]\n",
      "epoch:1 step:1619 [D loss: 0.452972, acc: 80.47%] [G loss: 3.053439]\n",
      "epoch:1 step:1620 [D loss: 0.673455, acc: 61.72%] [G loss: 3.008647]\n",
      "epoch:1 step:1621 [D loss: 0.516869, acc: 78.12%] [G loss: 3.324124]\n",
      "epoch:1 step:1622 [D loss: 0.545115, acc: 71.88%] [G loss: 3.176649]\n",
      "epoch:1 step:1623 [D loss: 0.641599, acc: 64.06%] [G loss: 2.901379]\n",
      "epoch:1 step:1624 [D loss: 0.541387, acc: 69.53%] [G loss: 3.060452]\n",
      "epoch:1 step:1625 [D loss: 0.643357, acc: 67.19%] [G loss: 3.029382]\n",
      "epoch:1 step:1626 [D loss: 0.528652, acc: 75.78%] [G loss: 2.932103]\n",
      "epoch:1 step:1627 [D loss: 0.600541, acc: 70.31%] [G loss: 2.603875]\n",
      "epoch:1 step:1628 [D loss: 0.614114, acc: 69.53%] [G loss: 2.853020]\n",
      "epoch:1 step:1629 [D loss: 0.570918, acc: 70.31%] [G loss: 2.914773]\n",
      "epoch:1 step:1630 [D loss: 0.498626, acc: 78.12%] [G loss: 3.249713]\n",
      "epoch:1 step:1631 [D loss: 0.547442, acc: 74.22%] [G loss: 3.266358]\n",
      "epoch:1 step:1632 [D loss: 0.528428, acc: 73.44%] [G loss: 2.958376]\n",
      "epoch:1 step:1633 [D loss: 0.547706, acc: 71.88%] [G loss: 3.041904]\n",
      "epoch:1 step:1634 [D loss: 0.475741, acc: 74.22%] [G loss: 3.195687]\n",
      "epoch:1 step:1635 [D loss: 0.513315, acc: 71.09%] [G loss: 2.987617]\n",
      "epoch:1 step:1636 [D loss: 0.426858, acc: 84.38%] [G loss: 3.095745]\n",
      "epoch:1 step:1637 [D loss: 0.495480, acc: 77.34%] [G loss: 3.270453]\n",
      "epoch:1 step:1638 [D loss: 0.560443, acc: 72.66%] [G loss: 2.955979]\n",
      "epoch:1 step:1639 [D loss: 0.589989, acc: 68.75%] [G loss: 3.263162]\n",
      "epoch:1 step:1640 [D loss: 0.539843, acc: 72.66%] [G loss: 3.034311]\n",
      "epoch:1 step:1641 [D loss: 0.557371, acc: 73.44%] [G loss: 3.002692]\n",
      "epoch:1 step:1642 [D loss: 0.527102, acc: 71.88%] [G loss: 3.092318]\n",
      "epoch:1 step:1643 [D loss: 0.523673, acc: 75.00%] [G loss: 3.305562]\n",
      "epoch:1 step:1644 [D loss: 0.546353, acc: 68.75%] [G loss: 3.746276]\n",
      "epoch:1 step:1645 [D loss: 0.491058, acc: 74.22%] [G loss: 3.249477]\n",
      "epoch:1 step:1646 [D loss: 0.428890, acc: 83.59%] [G loss: 3.541462]\n",
      "epoch:1 step:1647 [D loss: 0.810690, acc: 54.69%] [G loss: 2.822933]\n",
      "epoch:1 step:1648 [D loss: 0.552309, acc: 69.53%] [G loss: 2.862237]\n",
      "epoch:1 step:1649 [D loss: 0.499463, acc: 74.22%] [G loss: 3.078420]\n",
      "epoch:1 step:1650 [D loss: 0.516278, acc: 71.09%] [G loss: 3.252966]\n",
      "epoch:1 step:1651 [D loss: 0.573792, acc: 71.88%] [G loss: 3.142726]\n",
      "epoch:1 step:1652 [D loss: 0.520176, acc: 75.78%] [G loss: 2.827157]\n",
      "epoch:1 step:1653 [D loss: 0.601820, acc: 66.41%] [G loss: 2.612257]\n",
      "epoch:1 step:1654 [D loss: 0.479236, acc: 79.69%] [G loss: 2.877589]\n",
      "epoch:1 step:1655 [D loss: 0.532384, acc: 74.22%] [G loss: 2.812457]\n",
      "epoch:1 step:1656 [D loss: 0.605079, acc: 64.84%] [G loss: 3.278664]\n",
      "epoch:1 step:1657 [D loss: 0.641451, acc: 64.06%] [G loss: 2.891604]\n",
      "epoch:1 step:1658 [D loss: 0.592298, acc: 66.41%] [G loss: 2.805767]\n",
      "epoch:1 step:1659 [D loss: 0.499948, acc: 76.56%] [G loss: 3.021113]\n",
      "epoch:1 step:1660 [D loss: 0.589219, acc: 66.41%] [G loss: 2.843944]\n",
      "epoch:1 step:1661 [D loss: 0.550914, acc: 71.88%] [G loss: 3.339479]\n",
      "epoch:1 step:1662 [D loss: 0.627421, acc: 64.84%] [G loss: 2.748556]\n",
      "epoch:1 step:1663 [D loss: 0.501161, acc: 80.47%] [G loss: 2.905935]\n",
      "epoch:1 step:1664 [D loss: 0.494008, acc: 78.12%] [G loss: 3.056247]\n",
      "epoch:1 step:1665 [D loss: 0.470087, acc: 79.69%] [G loss: 3.490704]\n",
      "epoch:1 step:1666 [D loss: 0.516206, acc: 74.22%] [G loss: 3.178816]\n",
      "epoch:1 step:1667 [D loss: 0.525395, acc: 73.44%] [G loss: 2.999964]\n",
      "epoch:1 step:1668 [D loss: 0.530179, acc: 71.88%] [G loss: 3.340037]\n",
      "epoch:1 step:1669 [D loss: 0.452175, acc: 78.91%] [G loss: 3.422015]\n",
      "epoch:1 step:1670 [D loss: 0.580057, acc: 68.75%] [G loss: 3.350981]\n",
      "epoch:1 step:1671 [D loss: 0.573687, acc: 75.00%] [G loss: 3.476592]\n",
      "epoch:1 step:1672 [D loss: 0.671148, acc: 64.06%] [G loss: 3.020285]\n",
      "epoch:1 step:1673 [D loss: 0.449110, acc: 77.34%] [G loss: 3.079215]\n",
      "epoch:1 step:1674 [D loss: 0.596343, acc: 69.53%] [G loss: 3.124128]\n",
      "epoch:1 step:1675 [D loss: 0.465178, acc: 80.47%] [G loss: 3.213729]\n",
      "epoch:1 step:1676 [D loss: 0.586090, acc: 70.31%] [G loss: 3.302940]\n",
      "epoch:1 step:1677 [D loss: 0.577195, acc: 67.97%] [G loss: 3.037546]\n",
      "epoch:1 step:1678 [D loss: 0.537625, acc: 75.78%] [G loss: 2.971606]\n",
      "epoch:1 step:1679 [D loss: 0.563082, acc: 73.44%] [G loss: 2.669542]\n",
      "epoch:1 step:1680 [D loss: 0.560152, acc: 72.66%] [G loss: 2.953309]\n",
      "epoch:1 step:1681 [D loss: 0.566222, acc: 71.88%] [G loss: 3.092799]\n",
      "epoch:1 step:1682 [D loss: 0.528464, acc: 71.09%] [G loss: 3.200042]\n",
      "epoch:1 step:1683 [D loss: 0.491916, acc: 73.44%] [G loss: 3.216327]\n",
      "epoch:1 step:1684 [D loss: 0.526184, acc: 75.00%] [G loss: 3.253514]\n",
      "epoch:1 step:1685 [D loss: 0.565887, acc: 71.88%] [G loss: 2.995628]\n",
      "epoch:1 step:1686 [D loss: 0.501032, acc: 72.66%] [G loss: 3.202760]\n",
      "epoch:1 step:1687 [D loss: 0.503357, acc: 75.78%] [G loss: 3.252595]\n",
      "epoch:1 step:1688 [D loss: 0.542127, acc: 71.88%] [G loss: 3.698150]\n",
      "epoch:1 step:1689 [D loss: 0.469733, acc: 79.69%] [G loss: 3.612448]\n",
      "epoch:1 step:1690 [D loss: 0.469465, acc: 82.03%] [G loss: 3.673095]\n",
      "epoch:1 step:1691 [D loss: 0.435251, acc: 81.25%] [G loss: 3.562102]\n",
      "epoch:1 step:1692 [D loss: 0.531980, acc: 68.75%] [G loss: 3.405729]\n",
      "epoch:1 step:1693 [D loss: 0.462030, acc: 79.69%] [G loss: 3.140056]\n",
      "epoch:1 step:1694 [D loss: 0.496052, acc: 74.22%] [G loss: 3.534814]\n",
      "epoch:1 step:1695 [D loss: 0.492744, acc: 77.34%] [G loss: 3.230148]\n",
      "epoch:1 step:1696 [D loss: 0.581558, acc: 69.53%] [G loss: 3.553089]\n",
      "epoch:1 step:1697 [D loss: 0.551535, acc: 66.41%] [G loss: 3.119849]\n",
      "epoch:1 step:1698 [D loss: 0.494399, acc: 75.00%] [G loss: 3.203175]\n",
      "epoch:1 step:1699 [D loss: 0.595573, acc: 67.19%] [G loss: 2.984192]\n",
      "epoch:1 step:1700 [D loss: 0.466261, acc: 80.47%] [G loss: 3.145632]\n",
      "epoch:1 step:1701 [D loss: 0.501878, acc: 75.00%] [G loss: 2.882073]\n",
      "epoch:1 step:1702 [D loss: 0.726301, acc: 63.28%] [G loss: 3.175016]\n",
      "epoch:1 step:1703 [D loss: 0.596445, acc: 70.31%] [G loss: 2.567020]\n",
      "epoch:1 step:1704 [D loss: 0.568108, acc: 69.53%] [G loss: 2.806016]\n",
      "epoch:1 step:1705 [D loss: 0.559754, acc: 71.09%] [G loss: 3.021881]\n",
      "epoch:1 step:1706 [D loss: 0.623218, acc: 67.97%] [G loss: 3.275481]\n",
      "epoch:1 step:1707 [D loss: 0.552427, acc: 73.44%] [G loss: 3.122545]\n",
      "epoch:1 step:1708 [D loss: 0.586809, acc: 66.41%] [G loss: 2.981985]\n",
      "epoch:1 step:1709 [D loss: 0.525739, acc: 76.56%] [G loss: 2.971471]\n",
      "epoch:1 step:1710 [D loss: 0.638610, acc: 65.62%] [G loss: 3.038417]\n",
      "epoch:1 step:1711 [D loss: 0.506338, acc: 75.00%] [G loss: 3.425554]\n",
      "epoch:1 step:1712 [D loss: 0.533302, acc: 70.31%] [G loss: 3.258529]\n",
      "epoch:1 step:1713 [D loss: 0.518247, acc: 75.00%] [G loss: 3.339598]\n",
      "epoch:1 step:1714 [D loss: 0.681401, acc: 63.28%] [G loss: 2.777356]\n",
      "epoch:1 step:1715 [D loss: 0.616915, acc: 71.09%] [G loss: 2.861424]\n",
      "epoch:1 step:1716 [D loss: 0.585935, acc: 69.53%] [G loss: 3.024279]\n",
      "epoch:1 step:1717 [D loss: 0.632063, acc: 67.19%] [G loss: 3.068842]\n",
      "epoch:1 step:1718 [D loss: 0.492919, acc: 78.12%] [G loss: 3.202218]\n",
      "epoch:1 step:1719 [D loss: 0.494648, acc: 74.22%] [G loss: 3.062523]\n",
      "epoch:1 step:1720 [D loss: 0.624683, acc: 71.09%] [G loss: 3.023215]\n",
      "epoch:1 step:1721 [D loss: 0.578898, acc: 70.31%] [G loss: 3.424301]\n",
      "epoch:1 step:1722 [D loss: 0.485049, acc: 81.25%] [G loss: 3.198206]\n",
      "epoch:1 step:1723 [D loss: 0.561397, acc: 69.53%] [G loss: 3.314408]\n",
      "epoch:1 step:1724 [D loss: 0.690704, acc: 60.16%] [G loss: 2.966057]\n",
      "epoch:1 step:1725 [D loss: 0.707667, acc: 64.06%] [G loss: 2.519436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1726 [D loss: 0.502542, acc: 73.44%] [G loss: 2.809722]\n",
      "epoch:1 step:1727 [D loss: 0.561590, acc: 70.31%] [G loss: 2.883826]\n",
      "epoch:1 step:1728 [D loss: 0.534791, acc: 77.34%] [G loss: 3.354167]\n",
      "epoch:1 step:1729 [D loss: 0.484063, acc: 76.56%] [G loss: 3.358667]\n",
      "epoch:1 step:1730 [D loss: 0.525492, acc: 75.78%] [G loss: 3.210698]\n",
      "epoch:1 step:1731 [D loss: 0.710666, acc: 64.06%] [G loss: 3.008837]\n",
      "epoch:1 step:1732 [D loss: 0.464946, acc: 77.34%] [G loss: 3.210728]\n",
      "epoch:1 step:1733 [D loss: 0.571252, acc: 70.31%] [G loss: 3.258928]\n",
      "epoch:1 step:1734 [D loss: 0.564100, acc: 71.88%] [G loss: 2.722440]\n",
      "epoch:1 step:1735 [D loss: 0.486535, acc: 74.22%] [G loss: 3.458970]\n",
      "epoch:1 step:1736 [D loss: 0.588955, acc: 70.31%] [G loss: 3.075376]\n",
      "epoch:1 step:1737 [D loss: 0.708294, acc: 59.38%] [G loss: 2.919305]\n",
      "epoch:1 step:1738 [D loss: 0.517390, acc: 73.44%] [G loss: 3.087796]\n",
      "epoch:1 step:1739 [D loss: 0.529690, acc: 67.97%] [G loss: 2.906337]\n",
      "epoch:1 step:1740 [D loss: 0.484107, acc: 77.34%] [G loss: 3.285712]\n",
      "epoch:1 step:1741 [D loss: 0.562952, acc: 67.97%] [G loss: 3.118232]\n",
      "epoch:1 step:1742 [D loss: 0.514155, acc: 74.22%] [G loss: 3.268294]\n",
      "epoch:1 step:1743 [D loss: 0.586362, acc: 71.88%] [G loss: 3.307803]\n",
      "epoch:1 step:1744 [D loss: 0.519696, acc: 74.22%] [G loss: 3.158343]\n",
      "epoch:1 step:1745 [D loss: 0.628224, acc: 65.62%] [G loss: 2.756048]\n",
      "epoch:1 step:1746 [D loss: 0.526334, acc: 68.75%] [G loss: 2.880395]\n",
      "epoch:1 step:1747 [D loss: 0.595964, acc: 67.97%] [G loss: 2.549446]\n",
      "epoch:1 step:1748 [D loss: 0.653591, acc: 62.50%] [G loss: 2.940900]\n",
      "epoch:1 step:1749 [D loss: 0.533697, acc: 67.97%] [G loss: 2.948755]\n",
      "epoch:1 step:1750 [D loss: 0.578998, acc: 68.75%] [G loss: 2.920617]\n",
      "epoch:1 step:1751 [D loss: 0.574127, acc: 72.66%] [G loss: 2.894272]\n",
      "epoch:1 step:1752 [D loss: 0.562941, acc: 73.44%] [G loss: 2.978483]\n",
      "epoch:1 step:1753 [D loss: 0.598959, acc: 68.75%] [G loss: 2.826337]\n",
      "epoch:1 step:1754 [D loss: 0.641936, acc: 68.75%] [G loss: 3.044059]\n",
      "epoch:1 step:1755 [D loss: 0.586603, acc: 68.75%] [G loss: 3.014105]\n",
      "epoch:1 step:1756 [D loss: 0.540586, acc: 68.75%] [G loss: 3.116106]\n",
      "epoch:1 step:1757 [D loss: 0.454543, acc: 78.12%] [G loss: 2.650487]\n",
      "epoch:1 step:1758 [D loss: 0.603752, acc: 66.41%] [G loss: 2.952841]\n",
      "epoch:1 step:1759 [D loss: 0.525546, acc: 70.31%] [G loss: 2.797998]\n",
      "epoch:1 step:1760 [D loss: 0.529584, acc: 70.31%] [G loss: 3.115898]\n",
      "epoch:1 step:1761 [D loss: 0.573649, acc: 64.84%] [G loss: 2.404447]\n",
      "epoch:1 step:1762 [D loss: 0.498008, acc: 78.12%] [G loss: 2.891798]\n",
      "epoch:1 step:1763 [D loss: 0.629533, acc: 71.09%] [G loss: 2.780724]\n",
      "epoch:1 step:1764 [D loss: 0.610870, acc: 66.41%] [G loss: 2.808200]\n",
      "epoch:1 step:1765 [D loss: 0.698766, acc: 54.69%] [G loss: 2.394992]\n",
      "epoch:1 step:1766 [D loss: 0.515087, acc: 77.34%] [G loss: 2.928056]\n",
      "epoch:1 step:1767 [D loss: 0.648679, acc: 66.41%] [G loss: 2.753164]\n",
      "epoch:1 step:1768 [D loss: 0.537238, acc: 75.00%] [G loss: 2.733629]\n",
      "epoch:1 step:1769 [D loss: 0.622557, acc: 64.84%] [G loss: 2.483185]\n",
      "epoch:1 step:1770 [D loss: 0.500400, acc: 82.81%] [G loss: 2.849270]\n",
      "epoch:1 step:1771 [D loss: 0.509068, acc: 75.78%] [G loss: 2.849149]\n",
      "epoch:1 step:1772 [D loss: 0.524237, acc: 75.78%] [G loss: 3.170230]\n",
      "epoch:1 step:1773 [D loss: 0.666696, acc: 63.28%] [G loss: 2.894699]\n",
      "epoch:1 step:1774 [D loss: 0.537566, acc: 71.88%] [G loss: 3.137785]\n",
      "epoch:1 step:1775 [D loss: 0.607804, acc: 73.44%] [G loss: 2.826424]\n",
      "epoch:1 step:1776 [D loss: 0.569962, acc: 68.75%] [G loss: 2.937611]\n",
      "epoch:1 step:1777 [D loss: 0.552297, acc: 72.66%] [G loss: 2.699900]\n",
      "epoch:1 step:1778 [D loss: 0.515815, acc: 72.66%] [G loss: 3.172017]\n",
      "epoch:1 step:1779 [D loss: 0.525738, acc: 77.34%] [G loss: 3.161839]\n",
      "epoch:1 step:1780 [D loss: 0.579888, acc: 66.41%] [G loss: 3.318587]\n",
      "epoch:1 step:1781 [D loss: 0.519577, acc: 78.91%] [G loss: 2.670677]\n",
      "epoch:1 step:1782 [D loss: 0.536924, acc: 71.09%] [G loss: 3.024823]\n",
      "epoch:1 step:1783 [D loss: 0.560425, acc: 70.31%] [G loss: 3.179692]\n",
      "epoch:1 step:1784 [D loss: 0.556700, acc: 71.09%] [G loss: 2.474496]\n",
      "epoch:1 step:1785 [D loss: 0.509905, acc: 71.88%] [G loss: 3.263771]\n",
      "epoch:1 step:1786 [D loss: 0.553527, acc: 71.09%] [G loss: 3.092061]\n",
      "epoch:1 step:1787 [D loss: 0.515455, acc: 73.44%] [G loss: 3.057212]\n",
      "epoch:1 step:1788 [D loss: 0.612226, acc: 66.41%] [G loss: 2.877333]\n",
      "epoch:1 step:1789 [D loss: 0.525397, acc: 77.34%] [G loss: 3.186360]\n",
      "epoch:1 step:1790 [D loss: 0.528950, acc: 75.78%] [G loss: 3.418821]\n",
      "epoch:1 step:1791 [D loss: 0.544971, acc: 72.66%] [G loss: 3.344578]\n",
      "epoch:1 step:1792 [D loss: 0.579024, acc: 65.62%] [G loss: 3.114138]\n",
      "epoch:1 step:1793 [D loss: 0.467701, acc: 77.34%] [G loss: 3.350188]\n",
      "epoch:1 step:1794 [D loss: 0.604539, acc: 68.75%] [G loss: 3.040258]\n",
      "epoch:1 step:1795 [D loss: 0.649051, acc: 60.94%] [G loss: 2.598729]\n",
      "epoch:1 step:1796 [D loss: 0.634605, acc: 65.62%] [G loss: 2.506994]\n",
      "epoch:1 step:1797 [D loss: 0.536784, acc: 75.00%] [G loss: 3.366915]\n",
      "epoch:1 step:1798 [D loss: 0.641021, acc: 61.72%] [G loss: 2.686320]\n",
      "epoch:1 step:1799 [D loss: 0.554780, acc: 70.31%] [G loss: 2.645412]\n",
      "epoch:1 step:1800 [D loss: 0.645127, acc: 63.28%] [G loss: 2.692420]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.365234\n",
      "FID: 82.076202\n",
      "0 = 13.612226752758023\n",
      "1 = 0.1071552087839469\n",
      "2 = 0.9666000008583069\n",
      "3 = 0.9458000063896179\n",
      "4 = 0.9873999953269958\n",
      "5 = 0.9868530631065369\n",
      "6 = 0.9458000063896179\n",
      "7 = 10.783747243857391\n",
      "8 = 0.17462251003622797\n",
      "9 = 0.9150000214576721\n",
      "10 = 0.9097999930381775\n",
      "11 = 0.920199990272522\n",
      "12 = 0.9193613529205322\n",
      "13 = 0.9097999930381775\n",
      "14 = 4.365242958068848\n",
      "15 = 8.152483940124512\n",
      "16 = 0.34973037242889404\n",
      "17 = 4.365234375\n",
      "18 = 82.07620239257812\n",
      "epoch:1 step:1801 [D loss: 0.589479, acc: 67.19%] [G loss: 2.575489]\n",
      "epoch:1 step:1802 [D loss: 0.632077, acc: 64.06%] [G loss: 2.944035]\n",
      "epoch:1 step:1803 [D loss: 0.561042, acc: 67.19%] [G loss: 2.744254]\n",
      "epoch:1 step:1804 [D loss: 0.632762, acc: 58.59%] [G loss: 2.967492]\n",
      "epoch:1 step:1805 [D loss: 0.609924, acc: 65.62%] [G loss: 3.008144]\n",
      "epoch:1 step:1806 [D loss: 0.514952, acc: 75.00%] [G loss: 2.912889]\n",
      "epoch:1 step:1807 [D loss: 0.541377, acc: 71.88%] [G loss: 2.953267]\n",
      "epoch:1 step:1808 [D loss: 0.483107, acc: 77.34%] [G loss: 3.112799]\n",
      "epoch:1 step:1809 [D loss: 0.464491, acc: 74.22%] [G loss: 3.044563]\n",
      "epoch:1 step:1810 [D loss: 0.577465, acc: 65.62%] [G loss: 2.758344]\n",
      "epoch:1 step:1811 [D loss: 0.541872, acc: 68.75%] [G loss: 3.286802]\n",
      "epoch:1 step:1812 [D loss: 0.526192, acc: 67.97%] [G loss: 3.155996]\n",
      "epoch:1 step:1813 [D loss: 0.563237, acc: 75.00%] [G loss: 3.096327]\n",
      "epoch:1 step:1814 [D loss: 0.541745, acc: 73.44%] [G loss: 2.988498]\n",
      "epoch:1 step:1815 [D loss: 0.598247, acc: 71.09%] [G loss: 2.722401]\n",
      "epoch:1 step:1816 [D loss: 0.600086, acc: 70.31%] [G loss: 2.745217]\n",
      "epoch:1 step:1817 [D loss: 0.541810, acc: 75.00%] [G loss: 3.154400]\n",
      "epoch:1 step:1818 [D loss: 0.557191, acc: 71.88%] [G loss: 2.735953]\n",
      "epoch:1 step:1819 [D loss: 0.613891, acc: 68.75%] [G loss: 2.831228]\n",
      "epoch:1 step:1820 [D loss: 0.646046, acc: 64.06%] [G loss: 2.871307]\n",
      "epoch:1 step:1821 [D loss: 0.553793, acc: 69.53%] [G loss: 3.001917]\n",
      "epoch:1 step:1822 [D loss: 0.537751, acc: 75.00%] [G loss: 2.981963]\n",
      "epoch:1 step:1823 [D loss: 0.482187, acc: 77.34%] [G loss: 3.086876]\n",
      "epoch:1 step:1824 [D loss: 0.462447, acc: 75.00%] [G loss: 3.276349]\n",
      "epoch:1 step:1825 [D loss: 0.569282, acc: 71.88%] [G loss: 3.191288]\n",
      "epoch:1 step:1826 [D loss: 0.502105, acc: 78.12%] [G loss: 2.984419]\n",
      "epoch:1 step:1827 [D loss: 0.541562, acc: 72.66%] [G loss: 2.807933]\n",
      "epoch:1 step:1828 [D loss: 0.579581, acc: 71.88%] [G loss: 3.107296]\n",
      "epoch:1 step:1829 [D loss: 0.713104, acc: 58.59%] [G loss: 2.588080]\n",
      "epoch:1 step:1830 [D loss: 0.653919, acc: 62.50%] [G loss: 2.542220]\n",
      "epoch:1 step:1831 [D loss: 0.618147, acc: 71.88%] [G loss: 2.861625]\n",
      "epoch:1 step:1832 [D loss: 0.554106, acc: 68.75%] [G loss: 2.976216]\n",
      "epoch:1 step:1833 [D loss: 0.621497, acc: 67.19%] [G loss: 3.130441]\n",
      "epoch:1 step:1834 [D loss: 0.558568, acc: 65.62%] [G loss: 2.946021]\n",
      "epoch:1 step:1835 [D loss: 0.551813, acc: 65.62%] [G loss: 3.064905]\n",
      "epoch:1 step:1836 [D loss: 0.529727, acc: 77.34%] [G loss: 2.707443]\n",
      "epoch:1 step:1837 [D loss: 0.538826, acc: 73.44%] [G loss: 2.933765]\n",
      "epoch:1 step:1838 [D loss: 0.558753, acc: 71.88%] [G loss: 3.113976]\n",
      "epoch:1 step:1839 [D loss: 0.612444, acc: 65.62%] [G loss: 2.965941]\n",
      "epoch:1 step:1840 [D loss: 0.681845, acc: 62.50%] [G loss: 2.686280]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1841 [D loss: 0.594329, acc: 69.53%] [G loss: 3.078562]\n",
      "epoch:1 step:1842 [D loss: 0.717397, acc: 61.72%] [G loss: 2.869273]\n",
      "epoch:1 step:1843 [D loss: 0.624782, acc: 61.72%] [G loss: 2.954110]\n",
      "epoch:1 step:1844 [D loss: 0.501143, acc: 75.00%] [G loss: 2.972458]\n",
      "epoch:1 step:1845 [D loss: 0.624790, acc: 67.97%] [G loss: 2.901386]\n",
      "epoch:1 step:1846 [D loss: 0.567287, acc: 70.31%] [G loss: 2.921416]\n",
      "epoch:1 step:1847 [D loss: 0.515057, acc: 74.22%] [G loss: 3.543261]\n",
      "epoch:1 step:1848 [D loss: 0.542944, acc: 75.78%] [G loss: 3.088971]\n",
      "epoch:1 step:1849 [D loss: 0.640473, acc: 65.62%] [G loss: 3.249455]\n",
      "epoch:1 step:1850 [D loss: 0.552785, acc: 74.22%] [G loss: 2.991007]\n",
      "epoch:1 step:1851 [D loss: 0.557277, acc: 69.53%] [G loss: 3.324178]\n",
      "epoch:1 step:1852 [D loss: 0.567380, acc: 66.41%] [G loss: 2.880857]\n",
      "epoch:1 step:1853 [D loss: 0.556728, acc: 70.31%] [G loss: 2.866504]\n",
      "epoch:1 step:1854 [D loss: 0.624191, acc: 64.06%] [G loss: 3.181468]\n",
      "epoch:1 step:1855 [D loss: 0.594591, acc: 65.62%] [G loss: 3.024402]\n",
      "epoch:1 step:1856 [D loss: 0.599640, acc: 60.94%] [G loss: 3.037464]\n",
      "epoch:1 step:1857 [D loss: 0.676936, acc: 64.06%] [G loss: 2.952284]\n",
      "epoch:1 step:1858 [D loss: 0.496323, acc: 73.44%] [G loss: 3.176746]\n",
      "epoch:1 step:1859 [D loss: 0.561276, acc: 73.44%] [G loss: 2.760449]\n",
      "epoch:1 step:1860 [D loss: 0.558120, acc: 74.22%] [G loss: 3.118975]\n",
      "epoch:1 step:1861 [D loss: 0.418752, acc: 82.81%] [G loss: 3.198709]\n",
      "epoch:1 step:1862 [D loss: 0.411927, acc: 84.38%] [G loss: 3.396924]\n",
      "epoch:1 step:1863 [D loss: 0.586991, acc: 68.75%] [G loss: 3.496027]\n",
      "epoch:1 step:1864 [D loss: 0.570807, acc: 71.88%] [G loss: 3.367086]\n",
      "epoch:1 step:1865 [D loss: 0.871694, acc: 57.81%] [G loss: 3.162640]\n",
      "epoch:1 step:1866 [D loss: 0.650242, acc: 65.62%] [G loss: 3.196940]\n",
      "epoch:1 step:1867 [D loss: 0.459573, acc: 77.34%] [G loss: 2.861971]\n",
      "epoch:1 step:1868 [D loss: 0.529193, acc: 75.00%] [G loss: 3.008799]\n",
      "epoch:1 step:1869 [D loss: 0.608242, acc: 70.31%] [G loss: 2.734231]\n",
      "epoch:1 step:1870 [D loss: 0.543068, acc: 72.66%] [G loss: 2.931328]\n",
      "epoch:1 step:1871 [D loss: 0.513106, acc: 78.12%] [G loss: 3.061441]\n",
      "epoch:1 step:1872 [D loss: 0.455550, acc: 75.78%] [G loss: 3.303167]\n",
      "epoch:1 step:1873 [D loss: 0.404159, acc: 82.81%] [G loss: 3.804787]\n",
      "epoch:1 step:1874 [D loss: 0.621930, acc: 66.41%] [G loss: 3.416965]\n",
      "epoch:2 step:1875 [D loss: 0.538289, acc: 73.44%] [G loss: 3.151996]\n",
      "epoch:2 step:1876 [D loss: 0.523545, acc: 75.00%] [G loss: 3.187967]\n",
      "epoch:2 step:1877 [D loss: 0.712795, acc: 60.16%] [G loss: 2.559378]\n",
      "epoch:2 step:1878 [D loss: 0.485129, acc: 74.22%] [G loss: 2.952365]\n",
      "epoch:2 step:1879 [D loss: 0.619080, acc: 69.53%] [G loss: 2.835565]\n",
      "epoch:2 step:1880 [D loss: 0.581734, acc: 69.53%] [G loss: 3.020284]\n",
      "epoch:2 step:1881 [D loss: 0.531845, acc: 73.44%] [G loss: 2.876666]\n",
      "epoch:2 step:1882 [D loss: 0.608990, acc: 67.97%] [G loss: 2.755728]\n",
      "epoch:2 step:1883 [D loss: 0.579406, acc: 73.44%] [G loss: 2.870667]\n",
      "epoch:2 step:1884 [D loss: 0.584000, acc: 72.66%] [G loss: 2.806684]\n",
      "epoch:2 step:1885 [D loss: 0.534951, acc: 72.66%] [G loss: 2.863592]\n",
      "epoch:2 step:1886 [D loss: 0.525473, acc: 70.31%] [G loss: 3.036005]\n",
      "epoch:2 step:1887 [D loss: 0.628592, acc: 60.16%] [G loss: 2.593850]\n",
      "epoch:2 step:1888 [D loss: 0.569662, acc: 69.53%] [G loss: 2.715158]\n",
      "epoch:2 step:1889 [D loss: 0.548200, acc: 70.31%] [G loss: 2.877781]\n",
      "epoch:2 step:1890 [D loss: 0.580756, acc: 71.88%] [G loss: 2.626959]\n",
      "epoch:2 step:1891 [D loss: 0.636982, acc: 67.97%] [G loss: 2.981316]\n",
      "epoch:2 step:1892 [D loss: 0.620377, acc: 68.75%] [G loss: 2.608108]\n",
      "epoch:2 step:1893 [D loss: 0.660404, acc: 60.16%] [G loss: 2.752641]\n",
      "epoch:2 step:1894 [D loss: 0.676738, acc: 58.59%] [G loss: 2.725811]\n",
      "epoch:2 step:1895 [D loss: 0.649693, acc: 60.94%] [G loss: 2.695237]\n",
      "epoch:2 step:1896 [D loss: 0.471174, acc: 82.81%] [G loss: 2.723377]\n",
      "epoch:2 step:1897 [D loss: 0.517962, acc: 76.56%] [G loss: 2.807280]\n",
      "epoch:2 step:1898 [D loss: 0.585627, acc: 67.97%] [G loss: 2.851680]\n",
      "epoch:2 step:1899 [D loss: 0.562894, acc: 70.31%] [G loss: 2.912715]\n",
      "epoch:2 step:1900 [D loss: 0.654682, acc: 60.16%] [G loss: 2.450723]\n",
      "epoch:2 step:1901 [D loss: 0.522744, acc: 73.44%] [G loss: 3.030254]\n",
      "epoch:2 step:1902 [D loss: 0.538916, acc: 73.44%] [G loss: 2.697859]\n",
      "epoch:2 step:1903 [D loss: 0.543795, acc: 71.88%] [G loss: 2.988615]\n",
      "epoch:2 step:1904 [D loss: 0.676651, acc: 67.97%] [G loss: 2.897045]\n",
      "epoch:2 step:1905 [D loss: 0.603368, acc: 67.19%] [G loss: 2.715663]\n",
      "epoch:2 step:1906 [D loss: 0.543369, acc: 69.53%] [G loss: 3.035093]\n",
      "epoch:2 step:1907 [D loss: 0.598493, acc: 69.53%] [G loss: 3.000654]\n",
      "epoch:2 step:1908 [D loss: 0.614382, acc: 65.62%] [G loss: 3.194536]\n",
      "epoch:2 step:1909 [D loss: 0.518492, acc: 77.34%] [G loss: 3.310221]\n",
      "epoch:2 step:1910 [D loss: 0.544129, acc: 75.00%] [G loss: 3.002000]\n",
      "epoch:2 step:1911 [D loss: 0.615719, acc: 66.41%] [G loss: 2.866136]\n",
      "epoch:2 step:1912 [D loss: 0.630884, acc: 67.19%] [G loss: 3.098360]\n",
      "epoch:2 step:1913 [D loss: 0.584436, acc: 66.41%] [G loss: 3.062488]\n",
      "epoch:2 step:1914 [D loss: 0.531851, acc: 77.34%] [G loss: 2.884486]\n",
      "epoch:2 step:1915 [D loss: 0.579091, acc: 70.31%] [G loss: 2.749682]\n",
      "epoch:2 step:1916 [D loss: 0.542749, acc: 72.66%] [G loss: 2.928729]\n",
      "epoch:2 step:1917 [D loss: 0.568388, acc: 72.66%] [G loss: 2.902549]\n",
      "epoch:2 step:1918 [D loss: 0.582188, acc: 71.09%] [G loss: 2.780596]\n",
      "epoch:2 step:1919 [D loss: 0.581877, acc: 66.41%] [G loss: 2.857553]\n",
      "epoch:2 step:1920 [D loss: 0.563075, acc: 72.66%] [G loss: 2.732080]\n",
      "epoch:2 step:1921 [D loss: 0.559312, acc: 72.66%] [G loss: 2.979255]\n",
      "epoch:2 step:1922 [D loss: 0.556636, acc: 72.66%] [G loss: 2.582668]\n",
      "epoch:2 step:1923 [D loss: 0.567554, acc: 70.31%] [G loss: 2.901502]\n",
      "epoch:2 step:1924 [D loss: 0.574796, acc: 67.19%] [G loss: 2.639840]\n",
      "epoch:2 step:1925 [D loss: 0.615024, acc: 63.28%] [G loss: 2.655990]\n",
      "epoch:2 step:1926 [D loss: 0.577610, acc: 71.88%] [G loss: 2.432137]\n",
      "epoch:2 step:1927 [D loss: 0.605333, acc: 63.28%] [G loss: 2.877560]\n",
      "epoch:2 step:1928 [D loss: 0.450264, acc: 75.78%] [G loss: 3.075445]\n",
      "epoch:2 step:1929 [D loss: 0.496012, acc: 78.12%] [G loss: 3.281628]\n",
      "epoch:2 step:1930 [D loss: 0.706169, acc: 64.84%] [G loss: 2.716887]\n",
      "epoch:2 step:1931 [D loss: 0.500135, acc: 74.22%] [G loss: 2.754090]\n",
      "epoch:2 step:1932 [D loss: 0.585885, acc: 73.44%] [G loss: 3.063282]\n",
      "epoch:2 step:1933 [D loss: 0.443234, acc: 79.69%] [G loss: 3.220182]\n",
      "epoch:2 step:1934 [D loss: 0.519318, acc: 70.31%] [G loss: 2.952065]\n",
      "epoch:2 step:1935 [D loss: 0.538886, acc: 70.31%] [G loss: 2.996633]\n",
      "epoch:2 step:1936 [D loss: 0.629507, acc: 69.53%] [G loss: 2.835699]\n",
      "epoch:2 step:1937 [D loss: 0.541077, acc: 70.31%] [G loss: 2.976499]\n",
      "epoch:2 step:1938 [D loss: 0.602203, acc: 66.41%] [G loss: 2.832039]\n",
      "epoch:2 step:1939 [D loss: 0.555900, acc: 70.31%] [G loss: 2.897656]\n",
      "epoch:2 step:1940 [D loss: 0.632083, acc: 66.41%] [G loss: 3.047367]\n",
      "epoch:2 step:1941 [D loss: 0.542093, acc: 71.09%] [G loss: 2.803008]\n",
      "epoch:2 step:1942 [D loss: 0.579408, acc: 69.53%] [G loss: 2.590166]\n",
      "epoch:2 step:1943 [D loss: 0.531375, acc: 75.00%] [G loss: 2.480179]\n",
      "epoch:2 step:1944 [D loss: 0.531963, acc: 68.75%] [G loss: 2.919464]\n",
      "epoch:2 step:1945 [D loss: 0.594015, acc: 70.31%] [G loss: 3.553263]\n",
      "epoch:2 step:1946 [D loss: 0.519832, acc: 75.78%] [G loss: 3.119804]\n",
      "epoch:2 step:1947 [D loss: 0.557994, acc: 71.09%] [G loss: 3.106705]\n",
      "epoch:2 step:1948 [D loss: 0.577972, acc: 69.53%] [G loss: 3.060909]\n",
      "epoch:2 step:1949 [D loss: 0.452953, acc: 78.12%] [G loss: 3.277213]\n",
      "epoch:2 step:1950 [D loss: 0.511179, acc: 75.00%] [G loss: 3.378334]\n",
      "epoch:2 step:1951 [D loss: 0.459968, acc: 78.91%] [G loss: 3.710058]\n",
      "epoch:2 step:1952 [D loss: 0.590575, acc: 71.09%] [G loss: 3.110178]\n",
      "epoch:2 step:1953 [D loss: 0.610890, acc: 60.94%] [G loss: 2.871679]\n",
      "epoch:2 step:1954 [D loss: 0.660758, acc: 65.62%] [G loss: 2.758723]\n",
      "epoch:2 step:1955 [D loss: 0.590875, acc: 68.75%] [G loss: 2.901493]\n",
      "epoch:2 step:1956 [D loss: 0.529060, acc: 71.09%] [G loss: 3.045597]\n",
      "epoch:2 step:1957 [D loss: 0.533965, acc: 74.22%] [G loss: 2.800882]\n",
      "epoch:2 step:1958 [D loss: 0.558163, acc: 71.88%] [G loss: 3.082408]\n",
      "epoch:2 step:1959 [D loss: 0.482560, acc: 77.34%] [G loss: 2.757431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1960 [D loss: 0.543129, acc: 71.88%] [G loss: 2.949776]\n",
      "epoch:2 step:1961 [D loss: 0.531932, acc: 71.09%] [G loss: 3.077263]\n",
      "epoch:2 step:1962 [D loss: 0.515259, acc: 75.00%] [G loss: 2.878377]\n",
      "epoch:2 step:1963 [D loss: 0.533361, acc: 75.78%] [G loss: 3.393515]\n",
      "epoch:2 step:1964 [D loss: 0.538432, acc: 71.09%] [G loss: 3.106753]\n",
      "epoch:2 step:1965 [D loss: 0.553114, acc: 75.00%] [G loss: 2.921366]\n",
      "epoch:2 step:1966 [D loss: 0.595309, acc: 69.53%] [G loss: 3.070889]\n",
      "epoch:2 step:1967 [D loss: 0.498504, acc: 77.34%] [G loss: 3.123123]\n",
      "epoch:2 step:1968 [D loss: 0.557203, acc: 69.53%] [G loss: 2.913391]\n",
      "epoch:2 step:1969 [D loss: 0.437523, acc: 79.69%] [G loss: 3.176246]\n",
      "epoch:2 step:1970 [D loss: 0.514562, acc: 71.09%] [G loss: 3.148773]\n",
      "epoch:2 step:1971 [D loss: 0.521089, acc: 75.00%] [G loss: 3.356282]\n",
      "epoch:2 step:1972 [D loss: 0.574970, acc: 73.44%] [G loss: 2.986948]\n",
      "epoch:2 step:1973 [D loss: 0.492789, acc: 82.81%] [G loss: 3.131331]\n",
      "epoch:2 step:1974 [D loss: 0.491693, acc: 77.34%] [G loss: 2.884980]\n",
      "epoch:2 step:1975 [D loss: 0.595058, acc: 67.97%] [G loss: 3.149491]\n",
      "epoch:2 step:1976 [D loss: 0.622427, acc: 64.06%] [G loss: 2.799309]\n",
      "epoch:2 step:1977 [D loss: 0.522243, acc: 80.47%] [G loss: 3.155108]\n",
      "epoch:2 step:1978 [D loss: 0.524792, acc: 75.78%] [G loss: 3.263553]\n",
      "epoch:2 step:1979 [D loss: 0.584893, acc: 68.75%] [G loss: 2.908486]\n",
      "epoch:2 step:1980 [D loss: 0.548446, acc: 73.44%] [G loss: 2.820022]\n",
      "epoch:2 step:1981 [D loss: 0.718697, acc: 64.06%] [G loss: 2.438201]\n",
      "epoch:2 step:1982 [D loss: 0.651410, acc: 66.41%] [G loss: 2.787594]\n",
      "epoch:2 step:1983 [D loss: 0.703805, acc: 64.84%] [G loss: 2.884686]\n",
      "epoch:2 step:1984 [D loss: 0.543938, acc: 75.00%] [G loss: 2.828059]\n",
      "epoch:2 step:1985 [D loss: 0.651233, acc: 67.19%] [G loss: 2.646761]\n",
      "epoch:2 step:1986 [D loss: 0.621490, acc: 62.50%] [G loss: 2.692693]\n",
      "epoch:2 step:1987 [D loss: 0.610184, acc: 60.94%] [G loss: 2.650769]\n",
      "epoch:2 step:1988 [D loss: 0.596648, acc: 68.75%] [G loss: 2.727800]\n",
      "epoch:2 step:1989 [D loss: 0.566458, acc: 68.75%] [G loss: 2.878504]\n",
      "epoch:2 step:1990 [D loss: 0.603935, acc: 68.75%] [G loss: 2.932781]\n",
      "epoch:2 step:1991 [D loss: 0.636613, acc: 64.84%] [G loss: 3.231618]\n",
      "epoch:2 step:1992 [D loss: 0.505499, acc: 75.00%] [G loss: 3.077207]\n",
      "epoch:2 step:1993 [D loss: 0.482740, acc: 78.91%] [G loss: 3.263108]\n",
      "epoch:2 step:1994 [D loss: 0.530572, acc: 75.00%] [G loss: 2.800045]\n",
      "epoch:2 step:1995 [D loss: 0.635675, acc: 67.97%] [G loss: 2.893108]\n",
      "epoch:2 step:1996 [D loss: 0.606039, acc: 70.31%] [G loss: 2.762892]\n",
      "epoch:2 step:1997 [D loss: 0.619343, acc: 64.84%] [G loss: 2.922424]\n",
      "epoch:2 step:1998 [D loss: 0.707715, acc: 60.16%] [G loss: 2.773257]\n",
      "epoch:2 step:1999 [D loss: 0.516322, acc: 76.56%] [G loss: 2.837646]\n",
      "epoch:2 step:2000 [D loss: 0.580136, acc: 67.97%] [G loss: 2.844206]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.505740\n",
      "FID: 78.440514\n",
      "0 = 13.61354628810885\n",
      "1 = 0.10667017712379627\n",
      "2 = 0.9634000062942505\n",
      "3 = 0.9440000057220459\n",
      "4 = 0.9828000068664551\n",
      "5 = 0.9821056723594666\n",
      "6 = 0.9440000057220459\n",
      "7 = 10.678794850635521\n",
      "8 = 0.17345946576661098\n",
      "9 = 0.9110000133514404\n",
      "10 = 0.9070000052452087\n",
      "11 = 0.9150000214576721\n",
      "12 = 0.9143145084381104\n",
      "13 = 0.9070000052452087\n",
      "14 = 4.505749702453613\n",
      "15 = 8.177907943725586\n",
      "16 = 0.3384687006473541\n",
      "17 = 4.505740165710449\n",
      "18 = 78.44051361083984\n",
      "epoch:2 step:2001 [D loss: 0.446213, acc: 79.69%] [G loss: 3.066324]\n",
      "epoch:2 step:2002 [D loss: 0.501692, acc: 77.34%] [G loss: 3.260446]\n",
      "epoch:2 step:2003 [D loss: 0.650408, acc: 62.50%] [G loss: 2.771214]\n",
      "epoch:2 step:2004 [D loss: 0.484976, acc: 78.91%] [G loss: 2.986784]\n",
      "epoch:2 step:2005 [D loss: 0.532269, acc: 77.34%] [G loss: 2.877197]\n",
      "epoch:2 step:2006 [D loss: 0.536424, acc: 71.88%] [G loss: 2.933031]\n",
      "epoch:2 step:2007 [D loss: 0.596993, acc: 69.53%] [G loss: 2.944480]\n",
      "epoch:2 step:2008 [D loss: 0.644336, acc: 67.97%] [G loss: 3.208066]\n",
      "epoch:2 step:2009 [D loss: 0.503145, acc: 75.78%] [G loss: 2.998612]\n",
      "epoch:2 step:2010 [D loss: 0.537637, acc: 76.56%] [G loss: 3.047302]\n",
      "epoch:2 step:2011 [D loss: 0.687786, acc: 62.50%] [G loss: 2.937238]\n",
      "epoch:2 step:2012 [D loss: 0.696001, acc: 61.72%] [G loss: 2.590971]\n",
      "epoch:2 step:2013 [D loss: 0.583001, acc: 66.41%] [G loss: 2.691130]\n",
      "epoch:2 step:2014 [D loss: 0.667341, acc: 63.28%] [G loss: 2.753409]\n",
      "epoch:2 step:2015 [D loss: 0.507883, acc: 79.69%] [G loss: 2.949296]\n",
      "epoch:2 step:2016 [D loss: 0.553481, acc: 66.41%] [G loss: 3.060747]\n",
      "epoch:2 step:2017 [D loss: 0.618166, acc: 65.62%] [G loss: 2.494993]\n",
      "epoch:2 step:2018 [D loss: 0.565795, acc: 68.75%] [G loss: 2.908087]\n",
      "epoch:2 step:2019 [D loss: 0.547826, acc: 71.88%] [G loss: 2.775671]\n",
      "epoch:2 step:2020 [D loss: 0.616624, acc: 65.62%] [G loss: 2.984829]\n",
      "epoch:2 step:2021 [D loss: 0.611014, acc: 65.62%] [G loss: 2.908685]\n",
      "epoch:2 step:2022 [D loss: 0.550203, acc: 71.09%] [G loss: 2.727887]\n",
      "epoch:2 step:2023 [D loss: 0.553592, acc: 73.44%] [G loss: 3.210575]\n",
      "epoch:2 step:2024 [D loss: 0.523697, acc: 75.78%] [G loss: 3.178155]\n",
      "epoch:2 step:2025 [D loss: 0.540666, acc: 69.53%] [G loss: 3.392330]\n",
      "epoch:2 step:2026 [D loss: 0.475389, acc: 78.12%] [G loss: 3.491022]\n",
      "epoch:2 step:2027 [D loss: 0.616758, acc: 64.84%] [G loss: 2.734050]\n",
      "epoch:2 step:2028 [D loss: 0.589188, acc: 67.97%] [G loss: 2.864635]\n",
      "epoch:2 step:2029 [D loss: 0.464601, acc: 80.47%] [G loss: 3.124525]\n",
      "epoch:2 step:2030 [D loss: 0.629266, acc: 70.31%] [G loss: 2.657397]\n",
      "epoch:2 step:2031 [D loss: 0.512485, acc: 71.88%] [G loss: 2.621828]\n",
      "epoch:2 step:2032 [D loss: 0.591556, acc: 67.97%] [G loss: 2.379591]\n",
      "epoch:2 step:2033 [D loss: 0.625578, acc: 64.84%] [G loss: 2.721228]\n",
      "epoch:2 step:2034 [D loss: 0.572889, acc: 68.75%] [G loss: 3.058904]\n",
      "epoch:2 step:2035 [D loss: 0.593217, acc: 68.75%] [G loss: 2.842329]\n",
      "epoch:2 step:2036 [D loss: 0.555429, acc: 72.66%] [G loss: 3.605778]\n",
      "epoch:2 step:2037 [D loss: 0.452181, acc: 79.69%] [G loss: 3.470637]\n",
      "epoch:2 step:2038 [D loss: 0.565568, acc: 70.31%] [G loss: 2.830275]\n",
      "epoch:2 step:2039 [D loss: 0.636583, acc: 65.62%] [G loss: 2.980249]\n",
      "epoch:2 step:2040 [D loss: 0.525621, acc: 72.66%] [G loss: 2.853578]\n",
      "epoch:2 step:2041 [D loss: 0.560926, acc: 71.88%] [G loss: 2.800157]\n",
      "epoch:2 step:2042 [D loss: 0.569675, acc: 67.19%] [G loss: 2.783593]\n",
      "epoch:2 step:2043 [D loss: 0.691741, acc: 61.72%] [G loss: 2.750347]\n",
      "epoch:2 step:2044 [D loss: 0.595486, acc: 72.66%] [G loss: 2.739378]\n",
      "epoch:2 step:2045 [D loss: 0.597327, acc: 67.19%] [G loss: 3.016308]\n",
      "epoch:2 step:2046 [D loss: 0.496352, acc: 76.56%] [G loss: 3.023558]\n",
      "epoch:2 step:2047 [D loss: 0.526182, acc: 67.97%] [G loss: 2.977136]\n",
      "epoch:2 step:2048 [D loss: 0.511964, acc: 69.53%] [G loss: 2.929243]\n",
      "epoch:2 step:2049 [D loss: 0.611534, acc: 61.72%] [G loss: 2.991261]\n",
      "epoch:2 step:2050 [D loss: 0.605299, acc: 68.75%] [G loss: 2.709243]\n",
      "epoch:2 step:2051 [D loss: 0.555251, acc: 71.88%] [G loss: 2.829713]\n",
      "epoch:2 step:2052 [D loss: 0.469768, acc: 78.12%] [G loss: 2.771432]\n",
      "epoch:2 step:2053 [D loss: 0.627352, acc: 62.50%] [G loss: 2.742785]\n",
      "epoch:2 step:2054 [D loss: 0.506904, acc: 81.25%] [G loss: 2.798223]\n",
      "epoch:2 step:2055 [D loss: 0.591773, acc: 70.31%] [G loss: 2.894585]\n",
      "epoch:2 step:2056 [D loss: 0.542732, acc: 71.88%] [G loss: 2.553452]\n",
      "epoch:2 step:2057 [D loss: 0.537770, acc: 71.88%] [G loss: 2.855766]\n",
      "epoch:2 step:2058 [D loss: 0.598615, acc: 70.31%] [G loss: 2.873577]\n",
      "epoch:2 step:2059 [D loss: 0.539922, acc: 71.88%] [G loss: 2.726253]\n",
      "epoch:2 step:2060 [D loss: 0.578506, acc: 67.19%] [G loss: 2.900017]\n",
      "epoch:2 step:2061 [D loss: 0.586337, acc: 64.84%] [G loss: 2.745440]\n",
      "epoch:2 step:2062 [D loss: 0.539982, acc: 73.44%] [G loss: 2.667534]\n",
      "epoch:2 step:2063 [D loss: 0.620250, acc: 65.62%] [G loss: 2.450472]\n",
      "epoch:2 step:2064 [D loss: 0.570299, acc: 70.31%] [G loss: 2.934344]\n",
      "epoch:2 step:2065 [D loss: 0.526049, acc: 69.53%] [G loss: 2.843938]\n",
      "epoch:2 step:2066 [D loss: 0.550565, acc: 73.44%] [G loss: 2.948133]\n",
      "epoch:2 step:2067 [D loss: 0.548801, acc: 77.34%] [G loss: 2.743774]\n",
      "epoch:2 step:2068 [D loss: 0.515524, acc: 70.31%] [G loss: 3.068089]\n",
      "epoch:2 step:2069 [D loss: 0.505048, acc: 74.22%] [G loss: 2.998339]\n",
      "epoch:2 step:2070 [D loss: 0.562575, acc: 67.97%] [G loss: 2.997786]\n",
      "epoch:2 step:2071 [D loss: 0.515938, acc: 74.22%] [G loss: 3.078622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2072 [D loss: 0.526702, acc: 71.88%] [G loss: 3.020725]\n",
      "epoch:2 step:2073 [D loss: 0.570394, acc: 65.62%] [G loss: 3.062103]\n",
      "epoch:2 step:2074 [D loss: 0.633875, acc: 67.19%] [G loss: 2.952423]\n",
      "epoch:2 step:2075 [D loss: 0.640111, acc: 62.50%] [G loss: 2.700474]\n",
      "epoch:2 step:2076 [D loss: 0.553891, acc: 68.75%] [G loss: 2.858780]\n",
      "epoch:2 step:2077 [D loss: 0.621350, acc: 65.62%] [G loss: 3.026084]\n",
      "epoch:2 step:2078 [D loss: 0.595631, acc: 66.41%] [G loss: 3.084196]\n",
      "epoch:2 step:2079 [D loss: 0.618509, acc: 71.88%] [G loss: 2.765606]\n",
      "epoch:2 step:2080 [D loss: 0.613030, acc: 67.97%] [G loss: 2.980147]\n",
      "epoch:2 step:2081 [D loss: 0.595012, acc: 67.97%] [G loss: 3.093533]\n",
      "epoch:2 step:2082 [D loss: 0.566214, acc: 73.44%] [G loss: 3.167964]\n",
      "epoch:2 step:2083 [D loss: 0.529829, acc: 71.88%] [G loss: 3.054661]\n",
      "epoch:2 step:2084 [D loss: 0.606181, acc: 65.62%] [G loss: 2.744863]\n",
      "epoch:2 step:2085 [D loss: 0.593958, acc: 68.75%] [G loss: 2.762320]\n",
      "epoch:2 step:2086 [D loss: 0.494951, acc: 73.44%] [G loss: 2.893193]\n",
      "epoch:2 step:2087 [D loss: 0.582178, acc: 71.09%] [G loss: 3.264846]\n",
      "epoch:2 step:2088 [D loss: 0.636498, acc: 63.28%] [G loss: 2.421101]\n",
      "epoch:2 step:2089 [D loss: 0.748443, acc: 53.91%] [G loss: 2.552651]\n",
      "epoch:2 step:2090 [D loss: 0.598219, acc: 71.09%] [G loss: 2.622005]\n",
      "epoch:2 step:2091 [D loss: 0.486035, acc: 78.12%] [G loss: 3.002181]\n",
      "epoch:2 step:2092 [D loss: 0.619933, acc: 67.97%] [G loss: 2.918260]\n",
      "epoch:2 step:2093 [D loss: 0.542530, acc: 75.78%] [G loss: 2.980741]\n",
      "epoch:2 step:2094 [D loss: 0.589893, acc: 68.75%] [G loss: 2.931882]\n",
      "epoch:2 step:2095 [D loss: 0.534747, acc: 70.31%] [G loss: 3.036957]\n",
      "epoch:2 step:2096 [D loss: 0.680441, acc: 59.38%] [G loss: 3.066110]\n",
      "epoch:2 step:2097 [D loss: 0.540891, acc: 71.09%] [G loss: 3.175573]\n",
      "epoch:2 step:2098 [D loss: 0.628266, acc: 64.06%] [G loss: 2.685653]\n",
      "epoch:2 step:2099 [D loss: 0.570952, acc: 72.66%] [G loss: 2.643854]\n",
      "epoch:2 step:2100 [D loss: 0.554146, acc: 71.88%] [G loss: 2.826418]\n",
      "epoch:2 step:2101 [D loss: 0.581687, acc: 69.53%] [G loss: 2.759576]\n",
      "epoch:2 step:2102 [D loss: 0.568736, acc: 72.66%] [G loss: 2.798091]\n",
      "epoch:2 step:2103 [D loss: 0.542077, acc: 71.09%] [G loss: 2.942695]\n",
      "epoch:2 step:2104 [D loss: 0.526461, acc: 70.31%] [G loss: 3.349158]\n",
      "epoch:2 step:2105 [D loss: 0.578410, acc: 73.44%] [G loss: 3.359834]\n",
      "epoch:2 step:2106 [D loss: 0.443219, acc: 77.34%] [G loss: 3.387994]\n",
      "epoch:2 step:2107 [D loss: 0.635643, acc: 70.31%] [G loss: 2.990513]\n",
      "epoch:2 step:2108 [D loss: 0.504524, acc: 74.22%] [G loss: 3.152948]\n",
      "epoch:2 step:2109 [D loss: 0.550841, acc: 75.78%] [G loss: 2.885657]\n",
      "epoch:2 step:2110 [D loss: 0.521476, acc: 74.22%] [G loss: 2.947412]\n",
      "epoch:2 step:2111 [D loss: 0.576825, acc: 72.66%] [G loss: 2.830467]\n",
      "epoch:2 step:2112 [D loss: 0.556040, acc: 72.66%] [G loss: 3.099663]\n",
      "epoch:2 step:2113 [D loss: 0.661078, acc: 62.50%] [G loss: 2.835965]\n",
      "epoch:2 step:2114 [D loss: 0.660426, acc: 66.41%] [G loss: 2.595652]\n",
      "epoch:2 step:2115 [D loss: 0.567797, acc: 68.75%] [G loss: 2.866545]\n",
      "epoch:2 step:2116 [D loss: 0.564569, acc: 70.31%] [G loss: 2.634817]\n",
      "epoch:2 step:2117 [D loss: 0.658035, acc: 60.16%] [G loss: 2.842379]\n",
      "epoch:2 step:2118 [D loss: 0.526626, acc: 66.41%] [G loss: 2.895322]\n",
      "epoch:2 step:2119 [D loss: 0.685456, acc: 56.25%] [G loss: 2.860925]\n",
      "epoch:2 step:2120 [D loss: 0.624153, acc: 67.97%] [G loss: 2.628042]\n",
      "epoch:2 step:2121 [D loss: 0.598749, acc: 71.09%] [G loss: 2.536052]\n",
      "epoch:2 step:2122 [D loss: 0.603640, acc: 64.06%] [G loss: 2.712176]\n",
      "epoch:2 step:2123 [D loss: 0.582394, acc: 65.62%] [G loss: 2.905337]\n",
      "epoch:2 step:2124 [D loss: 0.634742, acc: 67.19%] [G loss: 2.858306]\n",
      "epoch:2 step:2125 [D loss: 0.589282, acc: 67.97%] [G loss: 2.525527]\n",
      "epoch:2 step:2126 [D loss: 0.609467, acc: 64.06%] [G loss: 2.674138]\n",
      "epoch:2 step:2127 [D loss: 0.512895, acc: 78.12%] [G loss: 2.467838]\n",
      "epoch:2 step:2128 [D loss: 0.529891, acc: 72.66%] [G loss: 2.952309]\n",
      "epoch:2 step:2129 [D loss: 0.572385, acc: 67.97%] [G loss: 2.725851]\n",
      "epoch:2 step:2130 [D loss: 0.527319, acc: 78.12%] [G loss: 3.295882]\n",
      "epoch:2 step:2131 [D loss: 0.541380, acc: 70.31%] [G loss: 2.817654]\n",
      "epoch:2 step:2132 [D loss: 0.491225, acc: 75.00%] [G loss: 3.143082]\n",
      "epoch:2 step:2133 [D loss: 0.481777, acc: 75.00%] [G loss: 3.066411]\n",
      "epoch:2 step:2134 [D loss: 0.627081, acc: 66.41%] [G loss: 2.905208]\n",
      "epoch:2 step:2135 [D loss: 0.472838, acc: 78.91%] [G loss: 3.576365]\n",
      "epoch:2 step:2136 [D loss: 0.547150, acc: 70.31%] [G loss: 3.491979]\n",
      "epoch:2 step:2137 [D loss: 0.678908, acc: 60.94%] [G loss: 2.634414]\n",
      "epoch:2 step:2138 [D loss: 0.587438, acc: 65.62%] [G loss: 2.802376]\n",
      "epoch:2 step:2139 [D loss: 0.608217, acc: 69.53%] [G loss: 2.780559]\n",
      "epoch:2 step:2140 [D loss: 0.519925, acc: 75.00%] [G loss: 2.825391]\n",
      "epoch:2 step:2141 [D loss: 0.548547, acc: 72.66%] [G loss: 2.829801]\n",
      "epoch:2 step:2142 [D loss: 0.640596, acc: 61.72%] [G loss: 2.863065]\n",
      "epoch:2 step:2143 [D loss: 0.578567, acc: 71.88%] [G loss: 2.796770]\n",
      "epoch:2 step:2144 [D loss: 0.561312, acc: 70.31%] [G loss: 2.872440]\n",
      "epoch:2 step:2145 [D loss: 0.587172, acc: 67.19%] [G loss: 2.968045]\n",
      "epoch:2 step:2146 [D loss: 0.562014, acc: 73.44%] [G loss: 3.001135]\n",
      "epoch:2 step:2147 [D loss: 0.580820, acc: 67.19%] [G loss: 3.111233]\n",
      "epoch:2 step:2148 [D loss: 0.602587, acc: 60.16%] [G loss: 3.073707]\n",
      "epoch:2 step:2149 [D loss: 0.677285, acc: 58.59%] [G loss: 2.610676]\n",
      "epoch:2 step:2150 [D loss: 0.603019, acc: 70.31%] [G loss: 2.629858]\n",
      "epoch:2 step:2151 [D loss: 0.705246, acc: 57.03%] [G loss: 2.788719]\n",
      "epoch:2 step:2152 [D loss: 0.596629, acc: 67.97%] [G loss: 2.842547]\n",
      "epoch:2 step:2153 [D loss: 0.566622, acc: 73.44%] [G loss: 3.051531]\n",
      "epoch:2 step:2154 [D loss: 0.486111, acc: 76.56%] [G loss: 2.610623]\n",
      "epoch:2 step:2155 [D loss: 0.646053, acc: 65.62%] [G loss: 2.737325]\n",
      "epoch:2 step:2156 [D loss: 0.634475, acc: 60.16%] [G loss: 2.565443]\n",
      "epoch:2 step:2157 [D loss: 0.585846, acc: 69.53%] [G loss: 2.675195]\n",
      "epoch:2 step:2158 [D loss: 0.558307, acc: 71.88%] [G loss: 2.868611]\n",
      "epoch:2 step:2159 [D loss: 0.496098, acc: 76.56%] [G loss: 2.836098]\n",
      "epoch:2 step:2160 [D loss: 0.497921, acc: 75.00%] [G loss: 2.956808]\n",
      "epoch:2 step:2161 [D loss: 0.562062, acc: 71.88%] [G loss: 2.796636]\n",
      "epoch:2 step:2162 [D loss: 0.602741, acc: 64.06%] [G loss: 2.709332]\n",
      "epoch:2 step:2163 [D loss: 0.570722, acc: 69.53%] [G loss: 2.926206]\n",
      "epoch:2 step:2164 [D loss: 0.530001, acc: 71.88%] [G loss: 2.819867]\n",
      "epoch:2 step:2165 [D loss: 0.591052, acc: 63.28%] [G loss: 2.903423]\n",
      "epoch:2 step:2166 [D loss: 0.552148, acc: 75.78%] [G loss: 2.859107]\n",
      "epoch:2 step:2167 [D loss: 0.610785, acc: 64.06%] [G loss: 3.066666]\n",
      "epoch:2 step:2168 [D loss: 0.559398, acc: 75.00%] [G loss: 2.926311]\n",
      "epoch:2 step:2169 [D loss: 0.664791, acc: 67.19%] [G loss: 2.773241]\n",
      "epoch:2 step:2170 [D loss: 0.466430, acc: 76.56%] [G loss: 3.184734]\n",
      "epoch:2 step:2171 [D loss: 0.497900, acc: 76.56%] [G loss: 2.607768]\n",
      "epoch:2 step:2172 [D loss: 0.581339, acc: 72.66%] [G loss: 2.927350]\n",
      "epoch:2 step:2173 [D loss: 0.490100, acc: 78.12%] [G loss: 2.987474]\n",
      "epoch:2 step:2174 [D loss: 0.497684, acc: 77.34%] [G loss: 3.136413]\n",
      "epoch:2 step:2175 [D loss: 0.661229, acc: 60.16%] [G loss: 2.823745]\n",
      "epoch:2 step:2176 [D loss: 0.562276, acc: 67.19%] [G loss: 2.790641]\n",
      "epoch:2 step:2177 [D loss: 0.541660, acc: 77.34%] [G loss: 2.787681]\n",
      "epoch:2 step:2178 [D loss: 0.499707, acc: 76.56%] [G loss: 3.175202]\n",
      "epoch:2 step:2179 [D loss: 0.558353, acc: 71.09%] [G loss: 3.025115]\n",
      "epoch:2 step:2180 [D loss: 0.637229, acc: 64.84%] [G loss: 3.072078]\n",
      "epoch:2 step:2181 [D loss: 0.560977, acc: 71.09%] [G loss: 3.070457]\n",
      "epoch:2 step:2182 [D loss: 0.561870, acc: 74.22%] [G loss: 3.120411]\n",
      "epoch:2 step:2183 [D loss: 0.463581, acc: 78.91%] [G loss: 3.177339]\n",
      "epoch:2 step:2184 [D loss: 0.556448, acc: 74.22%] [G loss: 2.751022]\n",
      "epoch:2 step:2185 [D loss: 0.531554, acc: 77.34%] [G loss: 2.991548]\n",
      "epoch:2 step:2186 [D loss: 0.454843, acc: 78.12%] [G loss: 3.391560]\n",
      "epoch:2 step:2187 [D loss: 0.577866, acc: 71.88%] [G loss: 3.446760]\n",
      "epoch:2 step:2188 [D loss: 0.403082, acc: 82.03%] [G loss: 3.255531]\n",
      "epoch:2 step:2189 [D loss: 0.468169, acc: 78.91%] [G loss: 3.821773]\n",
      "epoch:2 step:2190 [D loss: 0.702230, acc: 62.50%] [G loss: 2.724228]\n",
      "epoch:2 step:2191 [D loss: 0.555037, acc: 69.53%] [G loss: 2.742269]\n",
      "epoch:2 step:2192 [D loss: 0.648787, acc: 66.41%] [G loss: 2.595668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2193 [D loss: 0.475150, acc: 79.69%] [G loss: 2.932658]\n",
      "epoch:2 step:2194 [D loss: 0.545840, acc: 71.88%] [G loss: 2.660580]\n",
      "epoch:2 step:2195 [D loss: 0.497837, acc: 75.00%] [G loss: 3.166227]\n",
      "epoch:2 step:2196 [D loss: 0.535778, acc: 70.31%] [G loss: 3.133524]\n",
      "epoch:2 step:2197 [D loss: 0.543625, acc: 71.09%] [G loss: 3.100717]\n",
      "epoch:2 step:2198 [D loss: 0.514465, acc: 73.44%] [G loss: 2.894209]\n",
      "epoch:2 step:2199 [D loss: 0.676243, acc: 64.84%] [G loss: 2.862134]\n",
      "epoch:2 step:2200 [D loss: 0.584024, acc: 67.19%] [G loss: 2.835101]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.536582\n",
      "FID: 74.794647\n",
      "0 = 13.601444177818292\n",
      "1 = 0.09975168870151377\n",
      "2 = 0.9599999785423279\n",
      "3 = 0.9441999793052673\n",
      "4 = 0.9757999777793884\n",
      "5 = 0.975010335445404\n",
      "6 = 0.9441999793052673\n",
      "7 = 10.541721079921725\n",
      "8 = 0.16721654621825735\n",
      "9 = 0.9061999917030334\n",
      "10 = 0.8989999890327454\n",
      "11 = 0.9133999943733215\n",
      "12 = 0.9121347665786743\n",
      "13 = 0.8989999890327454\n",
      "14 = 4.536593437194824\n",
      "15 = 8.100523948669434\n",
      "16 = 0.33960574865341187\n",
      "17 = 4.536581993103027\n",
      "18 = 74.79464721679688\n",
      "epoch:2 step:2201 [D loss: 0.646401, acc: 66.41%] [G loss: 2.762038]\n",
      "epoch:2 step:2202 [D loss: 0.575484, acc: 70.31%] [G loss: 2.785778]\n",
      "epoch:2 step:2203 [D loss: 0.586457, acc: 69.53%] [G loss: 2.907608]\n",
      "epoch:2 step:2204 [D loss: 0.599169, acc: 64.84%] [G loss: 2.685212]\n",
      "epoch:2 step:2205 [D loss: 0.517536, acc: 68.75%] [G loss: 2.892105]\n",
      "epoch:2 step:2206 [D loss: 0.479018, acc: 77.34%] [G loss: 2.859299]\n",
      "epoch:2 step:2207 [D loss: 0.473892, acc: 74.22%] [G loss: 3.372538]\n",
      "epoch:2 step:2208 [D loss: 0.588958, acc: 71.09%] [G loss: 2.979022]\n",
      "epoch:2 step:2209 [D loss: 0.463009, acc: 75.78%] [G loss: 3.506328]\n",
      "epoch:2 step:2210 [D loss: 0.530907, acc: 71.88%] [G loss: 2.977715]\n",
      "epoch:2 step:2211 [D loss: 0.504329, acc: 73.44%] [G loss: 3.565531]\n",
      "epoch:2 step:2212 [D loss: 0.651863, acc: 57.03%] [G loss: 3.003320]\n",
      "epoch:2 step:2213 [D loss: 0.533863, acc: 77.34%] [G loss: 2.684192]\n",
      "epoch:2 step:2214 [D loss: 0.576945, acc: 77.34%] [G loss: 3.074554]\n",
      "epoch:2 step:2215 [D loss: 0.566956, acc: 67.19%] [G loss: 3.469021]\n",
      "epoch:2 step:2216 [D loss: 0.623171, acc: 66.41%] [G loss: 3.288129]\n",
      "epoch:2 step:2217 [D loss: 0.434391, acc: 85.16%] [G loss: 3.449082]\n",
      "epoch:2 step:2218 [D loss: 0.531360, acc: 75.00%] [G loss: 3.594432]\n",
      "epoch:2 step:2219 [D loss: 0.528901, acc: 73.44%] [G loss: 3.118122]\n",
      "epoch:2 step:2220 [D loss: 0.493334, acc: 75.78%] [G loss: 3.410606]\n",
      "epoch:2 step:2221 [D loss: 0.535127, acc: 71.88%] [G loss: 3.517590]\n",
      "epoch:2 step:2222 [D loss: 0.709121, acc: 64.06%] [G loss: 2.953697]\n",
      "epoch:2 step:2223 [D loss: 0.649683, acc: 63.28%] [G loss: 2.697483]\n",
      "epoch:2 step:2224 [D loss: 0.492675, acc: 74.22%] [G loss: 2.995122]\n",
      "epoch:2 step:2225 [D loss: 0.465875, acc: 76.56%] [G loss: 3.106863]\n",
      "epoch:2 step:2226 [D loss: 0.661568, acc: 62.50%] [G loss: 2.928386]\n",
      "epoch:2 step:2227 [D loss: 0.556319, acc: 66.41%] [G loss: 2.772441]\n",
      "epoch:2 step:2228 [D loss: 0.497333, acc: 75.00%] [G loss: 3.070754]\n",
      "epoch:2 step:2229 [D loss: 0.587916, acc: 64.84%] [G loss: 3.295469]\n",
      "epoch:2 step:2230 [D loss: 0.590679, acc: 69.53%] [G loss: 2.881477]\n",
      "epoch:2 step:2231 [D loss: 0.498354, acc: 72.66%] [G loss: 3.011414]\n",
      "epoch:2 step:2232 [D loss: 0.626375, acc: 65.62%] [G loss: 3.067671]\n",
      "epoch:2 step:2233 [D loss: 0.626278, acc: 69.53%] [G loss: 2.901050]\n",
      "epoch:2 step:2234 [D loss: 0.590600, acc: 71.88%] [G loss: 3.107588]\n",
      "epoch:2 step:2235 [D loss: 0.623970, acc: 60.94%] [G loss: 2.772860]\n",
      "epoch:2 step:2236 [D loss: 0.529413, acc: 69.53%] [G loss: 2.626316]\n",
      "epoch:2 step:2237 [D loss: 0.488135, acc: 75.78%] [G loss: 2.877649]\n",
      "epoch:2 step:2238 [D loss: 0.518543, acc: 75.00%] [G loss: 2.872591]\n",
      "epoch:2 step:2239 [D loss: 0.517012, acc: 75.00%] [G loss: 2.908128]\n",
      "epoch:2 step:2240 [D loss: 0.522892, acc: 74.22%] [G loss: 3.566057]\n",
      "epoch:2 step:2241 [D loss: 0.546992, acc: 71.88%] [G loss: 3.370661]\n",
      "epoch:2 step:2242 [D loss: 0.519072, acc: 75.78%] [G loss: 2.969438]\n",
      "epoch:2 step:2243 [D loss: 0.709414, acc: 63.28%] [G loss: 2.601695]\n",
      "epoch:2 step:2244 [D loss: 0.604178, acc: 67.19%] [G loss: 2.444606]\n",
      "epoch:2 step:2245 [D loss: 0.582044, acc: 69.53%] [G loss: 2.618572]\n",
      "epoch:2 step:2246 [D loss: 0.536729, acc: 70.31%] [G loss: 2.795856]\n",
      "epoch:2 step:2247 [D loss: 0.698432, acc: 60.16%] [G loss: 2.575199]\n",
      "epoch:2 step:2248 [D loss: 0.553977, acc: 69.53%] [G loss: 3.012689]\n",
      "epoch:2 step:2249 [D loss: 0.602786, acc: 64.84%] [G loss: 2.953226]\n",
      "epoch:2 step:2250 [D loss: 0.627730, acc: 68.75%] [G loss: 2.718084]\n",
      "epoch:2 step:2251 [D loss: 0.552868, acc: 69.53%] [G loss: 2.810697]\n",
      "epoch:2 step:2252 [D loss: 0.599688, acc: 68.75%] [G loss: 3.065874]\n",
      "epoch:2 step:2253 [D loss: 0.586395, acc: 68.75%] [G loss: 2.646173]\n",
      "epoch:2 step:2254 [D loss: 0.632418, acc: 66.41%] [G loss: 2.660815]\n",
      "epoch:2 step:2255 [D loss: 0.458272, acc: 81.25%] [G loss: 3.271056]\n",
      "epoch:2 step:2256 [D loss: 0.686651, acc: 64.06%] [G loss: 2.701708]\n",
      "epoch:2 step:2257 [D loss: 0.659243, acc: 64.84%] [G loss: 2.736991]\n",
      "epoch:2 step:2258 [D loss: 0.528349, acc: 78.12%] [G loss: 2.969184]\n",
      "epoch:2 step:2259 [D loss: 0.565482, acc: 71.88%] [G loss: 3.069153]\n",
      "epoch:2 step:2260 [D loss: 0.589805, acc: 68.75%] [G loss: 2.695649]\n",
      "epoch:2 step:2261 [D loss: 0.619948, acc: 66.41%] [G loss: 2.776519]\n",
      "epoch:2 step:2262 [D loss: 0.569776, acc: 64.84%] [G loss: 2.899328]\n",
      "epoch:2 step:2263 [D loss: 0.682378, acc: 64.06%] [G loss: 2.510184]\n",
      "epoch:2 step:2264 [D loss: 0.631793, acc: 63.28%] [G loss: 2.894865]\n",
      "epoch:2 step:2265 [D loss: 0.575778, acc: 70.31%] [G loss: 2.806540]\n",
      "epoch:2 step:2266 [D loss: 0.475704, acc: 78.91%] [G loss: 3.128861]\n",
      "epoch:2 step:2267 [D loss: 0.583563, acc: 70.31%] [G loss: 2.880507]\n",
      "epoch:2 step:2268 [D loss: 0.526262, acc: 74.22%] [G loss: 2.863513]\n",
      "epoch:2 step:2269 [D loss: 0.653223, acc: 60.16%] [G loss: 3.153733]\n",
      "epoch:2 step:2270 [D loss: 0.598718, acc: 69.53%] [G loss: 3.113458]\n",
      "epoch:2 step:2271 [D loss: 0.451870, acc: 77.34%] [G loss: 3.333392]\n",
      "epoch:2 step:2272 [D loss: 0.544977, acc: 72.66%] [G loss: 3.505739]\n",
      "epoch:2 step:2273 [D loss: 0.546201, acc: 71.88%] [G loss: 3.348917]\n",
      "epoch:2 step:2274 [D loss: 0.622060, acc: 67.19%] [G loss: 2.856733]\n",
      "epoch:2 step:2275 [D loss: 0.552063, acc: 68.75%] [G loss: 2.850356]\n",
      "epoch:2 step:2276 [D loss: 0.500853, acc: 74.22%] [G loss: 3.022345]\n",
      "epoch:2 step:2277 [D loss: 0.523330, acc: 75.78%] [G loss: 2.890301]\n",
      "epoch:2 step:2278 [D loss: 0.732292, acc: 53.91%] [G loss: 2.823315]\n",
      "epoch:2 step:2279 [D loss: 0.595200, acc: 70.31%] [G loss: 2.860533]\n",
      "epoch:2 step:2280 [D loss: 0.599926, acc: 67.97%] [G loss: 3.047603]\n",
      "epoch:2 step:2281 [D loss: 0.606778, acc: 62.50%] [G loss: 2.921033]\n",
      "epoch:2 step:2282 [D loss: 0.515372, acc: 73.44%] [G loss: 3.197945]\n",
      "epoch:2 step:2283 [D loss: 0.547844, acc: 68.75%] [G loss: 3.090574]\n",
      "epoch:2 step:2284 [D loss: 0.584762, acc: 72.66%] [G loss: 2.889150]\n",
      "epoch:2 step:2285 [D loss: 0.613639, acc: 62.50%] [G loss: 2.890821]\n",
      "epoch:2 step:2286 [D loss: 0.546141, acc: 73.44%] [G loss: 2.676408]\n",
      "epoch:2 step:2287 [D loss: 0.576055, acc: 67.19%] [G loss: 3.028090]\n",
      "epoch:2 step:2288 [D loss: 0.609660, acc: 72.66%] [G loss: 2.849106]\n",
      "epoch:2 step:2289 [D loss: 0.563564, acc: 73.44%] [G loss: 2.950797]\n",
      "epoch:2 step:2290 [D loss: 0.477302, acc: 76.56%] [G loss: 2.860395]\n",
      "epoch:2 step:2291 [D loss: 0.649216, acc: 68.75%] [G loss: 2.775319]\n",
      "epoch:2 step:2292 [D loss: 0.711234, acc: 63.28%] [G loss: 2.650077]\n",
      "epoch:2 step:2293 [D loss: 0.519918, acc: 77.34%] [G loss: 2.983832]\n",
      "epoch:2 step:2294 [D loss: 0.554824, acc: 64.06%] [G loss: 3.081227]\n",
      "epoch:2 step:2295 [D loss: 0.722513, acc: 59.38%] [G loss: 2.682676]\n",
      "epoch:2 step:2296 [D loss: 0.482771, acc: 77.34%] [G loss: 3.098980]\n",
      "epoch:2 step:2297 [D loss: 0.571791, acc: 70.31%] [G loss: 2.876501]\n",
      "epoch:2 step:2298 [D loss: 0.572568, acc: 75.78%] [G loss: 2.788739]\n",
      "epoch:2 step:2299 [D loss: 0.520174, acc: 75.00%] [G loss: 3.049201]\n",
      "epoch:2 step:2300 [D loss: 0.556525, acc: 71.88%] [G loss: 3.298715]\n",
      "epoch:2 step:2301 [D loss: 0.516987, acc: 78.12%] [G loss: 3.426127]\n",
      "epoch:2 step:2302 [D loss: 0.551010, acc: 73.44%] [G loss: 3.279735]\n",
      "epoch:2 step:2303 [D loss: 0.429125, acc: 81.25%] [G loss: 3.197741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2304 [D loss: 0.555647, acc: 73.44%] [G loss: 3.145073]\n",
      "epoch:2 step:2305 [D loss: 0.497344, acc: 73.44%] [G loss: 3.367209]\n",
      "epoch:2 step:2306 [D loss: 0.672272, acc: 66.41%] [G loss: 2.778352]\n",
      "epoch:2 step:2307 [D loss: 0.622346, acc: 67.97%] [G loss: 2.746195]\n",
      "epoch:2 step:2308 [D loss: 0.546295, acc: 72.66%] [G loss: 3.243357]\n",
      "epoch:2 step:2309 [D loss: 0.647593, acc: 67.19%] [G loss: 2.872216]\n",
      "epoch:2 step:2310 [D loss: 0.649060, acc: 65.62%] [G loss: 2.779010]\n",
      "epoch:2 step:2311 [D loss: 0.696121, acc: 61.72%] [G loss: 2.686706]\n",
      "epoch:2 step:2312 [D loss: 0.666139, acc: 67.19%] [G loss: 2.906794]\n",
      "epoch:2 step:2313 [D loss: 0.555701, acc: 71.88%] [G loss: 2.897988]\n",
      "epoch:2 step:2314 [D loss: 0.484455, acc: 81.25%] [G loss: 2.698018]\n",
      "epoch:2 step:2315 [D loss: 0.615045, acc: 66.41%] [G loss: 2.723638]\n",
      "epoch:2 step:2316 [D loss: 0.622620, acc: 61.72%] [G loss: 2.754056]\n",
      "epoch:2 step:2317 [D loss: 0.582962, acc: 74.22%] [G loss: 2.604203]\n",
      "epoch:2 step:2318 [D loss: 0.526172, acc: 76.56%] [G loss: 2.961413]\n",
      "epoch:2 step:2319 [D loss: 0.586086, acc: 71.09%] [G loss: 2.983316]\n",
      "epoch:2 step:2320 [D loss: 0.571871, acc: 67.97%] [G loss: 2.809032]\n",
      "epoch:2 step:2321 [D loss: 0.548577, acc: 75.00%] [G loss: 3.087779]\n",
      "epoch:2 step:2322 [D loss: 0.637705, acc: 60.94%] [G loss: 3.109643]\n",
      "epoch:2 step:2323 [D loss: 0.610552, acc: 66.41%] [G loss: 2.633453]\n",
      "epoch:2 step:2324 [D loss: 0.543570, acc: 72.66%] [G loss: 2.708225]\n",
      "epoch:2 step:2325 [D loss: 0.530953, acc: 75.00%] [G loss: 3.140846]\n",
      "epoch:2 step:2326 [D loss: 0.503229, acc: 73.44%] [G loss: 3.100067]\n",
      "epoch:2 step:2327 [D loss: 0.557917, acc: 67.19%] [G loss: 2.975353]\n",
      "epoch:2 step:2328 [D loss: 0.607044, acc: 66.41%] [G loss: 2.488641]\n",
      "epoch:2 step:2329 [D loss: 0.598319, acc: 69.53%] [G loss: 3.059826]\n",
      "epoch:2 step:2330 [D loss: 0.562613, acc: 69.53%] [G loss: 2.817841]\n",
      "epoch:2 step:2331 [D loss: 0.564408, acc: 74.22%] [G loss: 2.569011]\n",
      "epoch:2 step:2332 [D loss: 0.627550, acc: 65.62%] [G loss: 2.874584]\n",
      "epoch:2 step:2333 [D loss: 0.511293, acc: 70.31%] [G loss: 2.949103]\n",
      "epoch:2 step:2334 [D loss: 0.561643, acc: 71.88%] [G loss: 3.430758]\n",
      "epoch:2 step:2335 [D loss: 0.529558, acc: 76.56%] [G loss: 3.027019]\n",
      "epoch:2 step:2336 [D loss: 0.553916, acc: 73.44%] [G loss: 3.158745]\n",
      "epoch:2 step:2337 [D loss: 0.585466, acc: 70.31%] [G loss: 2.740290]\n",
      "epoch:2 step:2338 [D loss: 0.565383, acc: 72.66%] [G loss: 2.907497]\n",
      "epoch:2 step:2339 [D loss: 0.703403, acc: 61.72%] [G loss: 2.286645]\n",
      "epoch:2 step:2340 [D loss: 0.560837, acc: 64.84%] [G loss: 2.930033]\n",
      "epoch:2 step:2341 [D loss: 0.610582, acc: 68.75%] [G loss: 2.500893]\n",
      "epoch:2 step:2342 [D loss: 0.599200, acc: 62.50%] [G loss: 2.328612]\n",
      "epoch:2 step:2343 [D loss: 0.585073, acc: 71.88%] [G loss: 2.523781]\n",
      "epoch:2 step:2344 [D loss: 0.574121, acc: 72.66%] [G loss: 2.869478]\n",
      "epoch:2 step:2345 [D loss: 0.523017, acc: 75.78%] [G loss: 2.766245]\n",
      "epoch:2 step:2346 [D loss: 0.566448, acc: 73.44%] [G loss: 2.948041]\n",
      "epoch:2 step:2347 [D loss: 0.665635, acc: 57.81%] [G loss: 2.879899]\n",
      "epoch:2 step:2348 [D loss: 0.535427, acc: 75.00%] [G loss: 3.335939]\n",
      "epoch:2 step:2349 [D loss: 0.563510, acc: 69.53%] [G loss: 3.249519]\n",
      "epoch:2 step:2350 [D loss: 0.603919, acc: 65.62%] [G loss: 2.841439]\n",
      "epoch:2 step:2351 [D loss: 0.645071, acc: 63.28%] [G loss: 2.554286]\n",
      "epoch:2 step:2352 [D loss: 0.659481, acc: 60.16%] [G loss: 2.765649]\n",
      "epoch:2 step:2353 [D loss: 0.640348, acc: 63.28%] [G loss: 2.713992]\n",
      "epoch:2 step:2354 [D loss: 0.610766, acc: 64.06%] [G loss: 3.023718]\n",
      "epoch:2 step:2355 [D loss: 0.526202, acc: 71.88%] [G loss: 2.947158]\n",
      "epoch:2 step:2356 [D loss: 0.628474, acc: 67.19%] [G loss: 2.770900]\n",
      "epoch:2 step:2357 [D loss: 0.512516, acc: 74.22%] [G loss: 2.710608]\n",
      "epoch:2 step:2358 [D loss: 0.561273, acc: 72.66%] [G loss: 2.957296]\n",
      "epoch:2 step:2359 [D loss: 0.479133, acc: 79.69%] [G loss: 2.784692]\n",
      "epoch:2 step:2360 [D loss: 0.558790, acc: 71.88%] [G loss: 2.907212]\n",
      "epoch:2 step:2361 [D loss: 0.545042, acc: 73.44%] [G loss: 3.082003]\n",
      "epoch:2 step:2362 [D loss: 0.501962, acc: 77.34%] [G loss: 3.001435]\n",
      "epoch:2 step:2363 [D loss: 0.578512, acc: 68.75%] [G loss: 2.836912]\n",
      "epoch:2 step:2364 [D loss: 0.610062, acc: 68.75%] [G loss: 2.783320]\n",
      "epoch:2 step:2365 [D loss: 0.591659, acc: 64.84%] [G loss: 3.116960]\n",
      "epoch:2 step:2366 [D loss: 0.534166, acc: 75.00%] [G loss: 2.811861]\n",
      "epoch:2 step:2367 [D loss: 0.610854, acc: 64.06%] [G loss: 2.802991]\n",
      "epoch:2 step:2368 [D loss: 0.489079, acc: 76.56%] [G loss: 3.472104]\n",
      "epoch:2 step:2369 [D loss: 0.570406, acc: 70.31%] [G loss: 2.785850]\n",
      "epoch:2 step:2370 [D loss: 0.537301, acc: 71.88%] [G loss: 2.868353]\n",
      "epoch:2 step:2371 [D loss: 0.501128, acc: 76.56%] [G loss: 3.125489]\n",
      "epoch:2 step:2372 [D loss: 0.457583, acc: 78.12%] [G loss: 3.484707]\n",
      "epoch:2 step:2373 [D loss: 0.502621, acc: 72.66%] [G loss: 3.387039]\n",
      "epoch:2 step:2374 [D loss: 0.728099, acc: 60.94%] [G loss: 2.608487]\n",
      "epoch:2 step:2375 [D loss: 0.693885, acc: 60.94%] [G loss: 2.700238]\n",
      "epoch:2 step:2376 [D loss: 0.596935, acc: 67.97%] [G loss: 2.748875]\n",
      "epoch:2 step:2377 [D loss: 0.553316, acc: 72.66%] [G loss: 2.799469]\n",
      "epoch:2 step:2378 [D loss: 0.546442, acc: 75.78%] [G loss: 2.973351]\n",
      "epoch:2 step:2379 [D loss: 0.586613, acc: 68.75%] [G loss: 2.616243]\n",
      "epoch:2 step:2380 [D loss: 0.549278, acc: 74.22%] [G loss: 2.848912]\n",
      "epoch:2 step:2381 [D loss: 0.632247, acc: 64.06%] [G loss: 3.041416]\n",
      "epoch:2 step:2382 [D loss: 0.478196, acc: 76.56%] [G loss: 3.043150]\n",
      "epoch:2 step:2383 [D loss: 0.570084, acc: 69.53%] [G loss: 2.729538]\n",
      "epoch:2 step:2384 [D loss: 0.636852, acc: 63.28%] [G loss: 2.766228]\n",
      "epoch:2 step:2385 [D loss: 0.584177, acc: 70.31%] [G loss: 2.884002]\n",
      "epoch:2 step:2386 [D loss: 0.584424, acc: 67.19%] [G loss: 2.813513]\n",
      "epoch:2 step:2387 [D loss: 0.502165, acc: 74.22%] [G loss: 3.057963]\n",
      "epoch:2 step:2388 [D loss: 0.501069, acc: 77.34%] [G loss: 3.248431]\n",
      "epoch:2 step:2389 [D loss: 0.589268, acc: 68.75%] [G loss: 2.961525]\n",
      "epoch:2 step:2390 [D loss: 0.558907, acc: 71.09%] [G loss: 3.084888]\n",
      "epoch:2 step:2391 [D loss: 0.614219, acc: 69.53%] [G loss: 2.658107]\n",
      "epoch:2 step:2392 [D loss: 0.519980, acc: 71.88%] [G loss: 2.918425]\n",
      "epoch:2 step:2393 [D loss: 0.500803, acc: 72.66%] [G loss: 3.021797]\n",
      "epoch:2 step:2394 [D loss: 0.551633, acc: 71.88%] [G loss: 2.990544]\n",
      "epoch:2 step:2395 [D loss: 0.540065, acc: 69.53%] [G loss: 3.090944]\n",
      "epoch:2 step:2396 [D loss: 0.558552, acc: 69.53%] [G loss: 3.145012]\n",
      "epoch:2 step:2397 [D loss: 0.583968, acc: 67.97%] [G loss: 3.238982]\n",
      "epoch:2 step:2398 [D loss: 0.539773, acc: 71.88%] [G loss: 3.087639]\n",
      "epoch:2 step:2399 [D loss: 0.584083, acc: 68.75%] [G loss: 2.659482]\n",
      "epoch:2 step:2400 [D loss: 0.529617, acc: 73.44%] [G loss: 3.142923]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.589466\n",
      "FID: 72.372841\n",
      "0 = 13.389664096546158\n",
      "1 = 0.08864154044730435\n",
      "2 = 0.9509000182151794\n",
      "3 = 0.9318000078201294\n",
      "4 = 0.9700000286102295\n",
      "5 = 0.9688084721565247\n",
      "6 = 0.9318000078201294\n",
      "7 = 10.39818722991944\n",
      "8 = 0.16721618976604224\n",
      "9 = 0.8939999938011169\n",
      "10 = 0.8898000121116638\n",
      "11 = 0.8981999754905701\n",
      "12 = 0.8973376154899597\n",
      "13 = 0.8898000121116638\n",
      "14 = 4.589478969573975\n",
      "15 = 8.310229301452637\n",
      "16 = 0.32821890711784363\n",
      "17 = 4.589466094970703\n",
      "18 = 72.37284088134766\n",
      "epoch:2 step:2401 [D loss: 0.511188, acc: 73.44%] [G loss: 2.836812]\n",
      "epoch:2 step:2402 [D loss: 0.533705, acc: 71.88%] [G loss: 2.958008]\n",
      "epoch:2 step:2403 [D loss: 0.555883, acc: 68.75%] [G loss: 2.943051]\n",
      "epoch:2 step:2404 [D loss: 0.514526, acc: 79.69%] [G loss: 3.230888]\n",
      "epoch:2 step:2405 [D loss: 0.694250, acc: 61.72%] [G loss: 2.892530]\n",
      "epoch:2 step:2406 [D loss: 0.548908, acc: 74.22%] [G loss: 3.092931]\n",
      "epoch:2 step:2407 [D loss: 0.580881, acc: 67.19%] [G loss: 3.300766]\n",
      "epoch:2 step:2408 [D loss: 0.545494, acc: 72.66%] [G loss: 2.923768]\n",
      "epoch:2 step:2409 [D loss: 0.532587, acc: 74.22%] [G loss: 2.791395]\n",
      "epoch:2 step:2410 [D loss: 0.542388, acc: 71.09%] [G loss: 3.070338]\n",
      "epoch:2 step:2411 [D loss: 0.612547, acc: 71.88%] [G loss: 2.993782]\n",
      "epoch:2 step:2412 [D loss: 0.613855, acc: 65.62%] [G loss: 2.867312]\n",
      "epoch:2 step:2413 [D loss: 0.584953, acc: 71.09%] [G loss: 2.806314]\n",
      "epoch:2 step:2414 [D loss: 0.542231, acc: 69.53%] [G loss: 3.052411]\n",
      "epoch:2 step:2415 [D loss: 0.635096, acc: 64.06%] [G loss: 2.777275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2416 [D loss: 0.494306, acc: 76.56%] [G loss: 2.953766]\n",
      "epoch:2 step:2417 [D loss: 0.537818, acc: 73.44%] [G loss: 2.800750]\n",
      "epoch:2 step:2418 [D loss: 0.580395, acc: 70.31%] [G loss: 2.849051]\n",
      "epoch:2 step:2419 [D loss: 0.590378, acc: 64.06%] [G loss: 2.779446]\n",
      "epoch:2 step:2420 [D loss: 0.507944, acc: 75.78%] [G loss: 2.732007]\n",
      "epoch:2 step:2421 [D loss: 0.506198, acc: 71.88%] [G loss: 3.360089]\n",
      "epoch:2 step:2422 [D loss: 0.603930, acc: 66.41%] [G loss: 2.691269]\n",
      "epoch:2 step:2423 [D loss: 0.612416, acc: 65.62%] [G loss: 2.922835]\n",
      "epoch:2 step:2424 [D loss: 0.485156, acc: 75.78%] [G loss: 2.932928]\n",
      "epoch:2 step:2425 [D loss: 0.545400, acc: 73.44%] [G loss: 2.859818]\n",
      "epoch:2 step:2426 [D loss: 0.581329, acc: 71.88%] [G loss: 2.929628]\n",
      "epoch:2 step:2427 [D loss: 0.561282, acc: 70.31%] [G loss: 2.978825]\n",
      "epoch:2 step:2428 [D loss: 0.578981, acc: 71.88%] [G loss: 3.130703]\n",
      "epoch:2 step:2429 [D loss: 0.505949, acc: 77.34%] [G loss: 3.091110]\n",
      "epoch:2 step:2430 [D loss: 0.502603, acc: 72.66%] [G loss: 3.274599]\n",
      "epoch:2 step:2431 [D loss: 0.497939, acc: 78.91%] [G loss: 3.340403]\n",
      "epoch:2 step:2432 [D loss: 0.558413, acc: 73.44%] [G loss: 3.183626]\n",
      "epoch:2 step:2433 [D loss: 0.557183, acc: 75.78%] [G loss: 3.129576]\n",
      "epoch:2 step:2434 [D loss: 0.580495, acc: 67.97%] [G loss: 3.047333]\n",
      "epoch:2 step:2435 [D loss: 0.462221, acc: 79.69%] [G loss: 3.057960]\n",
      "epoch:2 step:2436 [D loss: 0.616076, acc: 67.97%] [G loss: 3.048952]\n",
      "epoch:2 step:2437 [D loss: 0.553066, acc: 71.09%] [G loss: 2.682067]\n",
      "epoch:2 step:2438 [D loss: 0.644647, acc: 59.38%] [G loss: 2.694487]\n",
      "epoch:2 step:2439 [D loss: 0.524337, acc: 74.22%] [G loss: 2.909522]\n",
      "epoch:2 step:2440 [D loss: 0.616625, acc: 65.62%] [G loss: 3.395096]\n",
      "epoch:2 step:2441 [D loss: 0.510121, acc: 78.91%] [G loss: 3.405824]\n",
      "epoch:2 step:2442 [D loss: 0.578226, acc: 75.78%] [G loss: 3.367067]\n",
      "epoch:2 step:2443 [D loss: 0.564144, acc: 70.31%] [G loss: 2.669407]\n",
      "epoch:2 step:2444 [D loss: 0.558380, acc: 71.09%] [G loss: 3.217092]\n",
      "epoch:2 step:2445 [D loss: 0.592499, acc: 69.53%] [G loss: 2.882071]\n",
      "epoch:2 step:2446 [D loss: 0.549232, acc: 69.53%] [G loss: 2.977092]\n",
      "epoch:2 step:2447 [D loss: 0.654533, acc: 69.53%] [G loss: 2.777867]\n",
      "epoch:2 step:2448 [D loss: 0.464535, acc: 79.69%] [G loss: 2.827088]\n",
      "epoch:2 step:2449 [D loss: 0.477380, acc: 77.34%] [G loss: 2.960281]\n",
      "epoch:2 step:2450 [D loss: 0.607736, acc: 69.53%] [G loss: 2.986650]\n",
      "epoch:2 step:2451 [D loss: 0.555358, acc: 68.75%] [G loss: 3.027217]\n",
      "epoch:2 step:2452 [D loss: 0.534781, acc: 73.44%] [G loss: 2.799678]\n",
      "epoch:2 step:2453 [D loss: 0.573156, acc: 66.41%] [G loss: 2.816127]\n",
      "epoch:2 step:2454 [D loss: 0.613598, acc: 65.62%] [G loss: 3.145746]\n",
      "epoch:2 step:2455 [D loss: 0.524892, acc: 75.00%] [G loss: 3.042482]\n",
      "epoch:2 step:2456 [D loss: 0.577319, acc: 73.44%] [G loss: 3.063671]\n",
      "epoch:2 step:2457 [D loss: 0.579782, acc: 71.09%] [G loss: 2.860064]\n",
      "epoch:2 step:2458 [D loss: 0.633948, acc: 62.50%] [G loss: 2.651083]\n",
      "epoch:2 step:2459 [D loss: 0.618119, acc: 64.84%] [G loss: 2.707457]\n",
      "epoch:2 step:2460 [D loss: 0.574329, acc: 67.97%] [G loss: 2.613626]\n",
      "epoch:2 step:2461 [D loss: 0.554658, acc: 71.09%] [G loss: 3.063926]\n",
      "epoch:2 step:2462 [D loss: 0.588422, acc: 72.66%] [G loss: 2.860711]\n",
      "epoch:2 step:2463 [D loss: 0.505683, acc: 73.44%] [G loss: 3.261639]\n",
      "epoch:2 step:2464 [D loss: 0.583370, acc: 69.53%] [G loss: 3.379336]\n",
      "epoch:2 step:2465 [D loss: 0.565793, acc: 74.22%] [G loss: 3.147285]\n",
      "epoch:2 step:2466 [D loss: 0.447742, acc: 85.16%] [G loss: 3.376904]\n",
      "epoch:2 step:2467 [D loss: 0.500260, acc: 77.34%] [G loss: 3.010963]\n",
      "epoch:2 step:2468 [D loss: 0.560561, acc: 69.53%] [G loss: 3.079615]\n",
      "epoch:2 step:2469 [D loss: 0.562124, acc: 71.09%] [G loss: 2.872941]\n",
      "epoch:2 step:2470 [D loss: 0.502490, acc: 71.88%] [G loss: 3.202426]\n",
      "epoch:2 step:2471 [D loss: 0.514636, acc: 76.56%] [G loss: 2.996093]\n",
      "epoch:2 step:2472 [D loss: 0.486372, acc: 75.78%] [G loss: 3.008164]\n",
      "epoch:2 step:2473 [D loss: 0.497783, acc: 74.22%] [G loss: 3.269189]\n",
      "epoch:2 step:2474 [D loss: 0.605775, acc: 67.97%] [G loss: 3.025013]\n",
      "epoch:2 step:2475 [D loss: 0.637983, acc: 64.84%] [G loss: 3.011067]\n",
      "epoch:2 step:2476 [D loss: 0.532195, acc: 69.53%] [G loss: 3.380342]\n",
      "epoch:2 step:2477 [D loss: 0.540536, acc: 72.66%] [G loss: 3.220047]\n",
      "epoch:2 step:2478 [D loss: 0.539180, acc: 71.88%] [G loss: 2.843574]\n",
      "epoch:2 step:2479 [D loss: 0.503654, acc: 75.78%] [G loss: 3.252841]\n",
      "epoch:2 step:2480 [D loss: 0.434937, acc: 79.69%] [G loss: 3.296354]\n",
      "epoch:2 step:2481 [D loss: 0.576362, acc: 68.75%] [G loss: 2.941225]\n",
      "epoch:2 step:2482 [D loss: 0.470311, acc: 82.03%] [G loss: 2.932488]\n",
      "epoch:2 step:2483 [D loss: 0.475893, acc: 79.69%] [G loss: 3.299353]\n",
      "epoch:2 step:2484 [D loss: 0.501140, acc: 77.34%] [G loss: 3.068781]\n",
      "epoch:2 step:2485 [D loss: 0.474732, acc: 78.12%] [G loss: 3.265441]\n",
      "epoch:2 step:2486 [D loss: 0.518574, acc: 74.22%] [G loss: 3.030822]\n",
      "epoch:2 step:2487 [D loss: 0.536276, acc: 69.53%] [G loss: 2.996022]\n",
      "epoch:2 step:2488 [D loss: 0.590685, acc: 71.09%] [G loss: 3.166071]\n",
      "epoch:2 step:2489 [D loss: 0.602984, acc: 68.75%] [G loss: 2.785270]\n",
      "epoch:2 step:2490 [D loss: 0.522227, acc: 75.78%] [G loss: 2.895927]\n",
      "epoch:2 step:2491 [D loss: 0.616649, acc: 67.97%] [G loss: 2.862868]\n",
      "epoch:2 step:2492 [D loss: 0.526753, acc: 77.34%] [G loss: 2.853227]\n",
      "epoch:2 step:2493 [D loss: 0.508062, acc: 73.44%] [G loss: 3.048166]\n",
      "epoch:2 step:2494 [D loss: 0.525941, acc: 72.66%] [G loss: 2.886517]\n",
      "epoch:2 step:2495 [D loss: 0.605029, acc: 72.66%] [G loss: 2.988145]\n",
      "epoch:2 step:2496 [D loss: 0.679766, acc: 59.38%] [G loss: 2.790473]\n",
      "epoch:2 step:2497 [D loss: 0.557720, acc: 71.88%] [G loss: 3.028785]\n",
      "epoch:2 step:2498 [D loss: 0.497786, acc: 78.91%] [G loss: 3.204448]\n",
      "epoch:2 step:2499 [D loss: 0.626310, acc: 67.97%] [G loss: 2.933548]\n",
      "epoch:2 step:2500 [D loss: 0.491147, acc: 78.12%] [G loss: 3.234976]\n",
      "epoch:2 step:2501 [D loss: 0.610274, acc: 64.84%] [G loss: 2.936856]\n",
      "epoch:2 step:2502 [D loss: 0.521888, acc: 73.44%] [G loss: 3.204760]\n",
      "epoch:2 step:2503 [D loss: 0.540847, acc: 71.88%] [G loss: 2.799931]\n",
      "epoch:2 step:2504 [D loss: 0.621343, acc: 72.66%] [G loss: 2.845790]\n",
      "epoch:2 step:2505 [D loss: 0.549728, acc: 73.44%] [G loss: 2.789398]\n",
      "epoch:2 step:2506 [D loss: 0.483137, acc: 75.78%] [G loss: 3.125983]\n",
      "epoch:2 step:2507 [D loss: 0.495857, acc: 80.47%] [G loss: 2.979938]\n",
      "epoch:2 step:2508 [D loss: 0.583256, acc: 67.19%] [G loss: 3.317200]\n",
      "epoch:2 step:2509 [D loss: 0.440608, acc: 80.47%] [G loss: 3.411630]\n",
      "epoch:2 step:2510 [D loss: 0.588688, acc: 70.31%] [G loss: 2.774575]\n",
      "epoch:2 step:2511 [D loss: 0.464330, acc: 78.12%] [G loss: 3.093109]\n",
      "epoch:2 step:2512 [D loss: 0.554623, acc: 74.22%] [G loss: 3.101480]\n",
      "epoch:2 step:2513 [D loss: 0.477129, acc: 82.03%] [G loss: 3.382939]\n",
      "epoch:2 step:2514 [D loss: 0.585462, acc: 69.53%] [G loss: 3.439857]\n",
      "epoch:2 step:2515 [D loss: 0.436301, acc: 82.03%] [G loss: 3.593448]\n",
      "epoch:2 step:2516 [D loss: 0.434967, acc: 78.91%] [G loss: 3.701725]\n",
      "epoch:2 step:2517 [D loss: 0.624628, acc: 65.62%] [G loss: 2.925264]\n",
      "epoch:2 step:2518 [D loss: 0.587791, acc: 65.62%] [G loss: 2.959330]\n",
      "epoch:2 step:2519 [D loss: 0.590143, acc: 63.28%] [G loss: 2.970516]\n",
      "epoch:2 step:2520 [D loss: 0.552181, acc: 72.66%] [G loss: 3.137193]\n",
      "epoch:2 step:2521 [D loss: 0.597080, acc: 66.41%] [G loss: 3.202806]\n",
      "epoch:2 step:2522 [D loss: 0.513435, acc: 77.34%] [G loss: 3.436225]\n",
      "epoch:2 step:2523 [D loss: 0.585232, acc: 70.31%] [G loss: 3.381446]\n",
      "epoch:2 step:2524 [D loss: 0.547413, acc: 78.12%] [G loss: 3.602431]\n",
      "epoch:2 step:2525 [D loss: 0.493354, acc: 75.00%] [G loss: 3.439183]\n",
      "epoch:2 step:2526 [D loss: 0.571744, acc: 69.53%] [G loss: 2.834998]\n",
      "epoch:2 step:2527 [D loss: 0.578047, acc: 67.19%] [G loss: 3.145902]\n",
      "epoch:2 step:2528 [D loss: 0.533607, acc: 73.44%] [G loss: 3.062238]\n",
      "epoch:2 step:2529 [D loss: 0.511038, acc: 75.78%] [G loss: 2.887392]\n",
      "epoch:2 step:2530 [D loss: 0.586767, acc: 64.06%] [G loss: 2.874610]\n",
      "epoch:2 step:2531 [D loss: 0.493459, acc: 74.22%] [G loss: 2.988127]\n",
      "epoch:2 step:2532 [D loss: 0.578989, acc: 68.75%] [G loss: 2.660888]\n",
      "epoch:2 step:2533 [D loss: 0.535674, acc: 69.53%] [G loss: 2.863632]\n",
      "epoch:2 step:2534 [D loss: 0.515889, acc: 74.22%] [G loss: 3.139970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2535 [D loss: 0.513181, acc: 77.34%] [G loss: 3.129395]\n",
      "epoch:2 step:2536 [D loss: 0.704535, acc: 61.72%] [G loss: 3.117321]\n",
      "epoch:2 step:2537 [D loss: 0.539337, acc: 71.88%] [G loss: 3.307408]\n",
      "epoch:2 step:2538 [D loss: 0.616900, acc: 67.19%] [G loss: 3.257777]\n",
      "epoch:2 step:2539 [D loss: 0.562932, acc: 73.44%] [G loss: 3.251560]\n",
      "epoch:2 step:2540 [D loss: 0.528675, acc: 71.09%] [G loss: 2.804698]\n",
      "epoch:2 step:2541 [D loss: 0.605411, acc: 67.97%] [G loss: 2.926157]\n",
      "epoch:2 step:2542 [D loss: 0.709610, acc: 61.72%] [G loss: 2.903834]\n",
      "epoch:2 step:2543 [D loss: 0.517369, acc: 75.00%] [G loss: 2.840780]\n",
      "epoch:2 step:2544 [D loss: 0.546705, acc: 75.78%] [G loss: 2.954718]\n",
      "epoch:2 step:2545 [D loss: 0.497020, acc: 75.00%] [G loss: 3.283347]\n",
      "epoch:2 step:2546 [D loss: 0.657946, acc: 55.47%] [G loss: 2.802246]\n",
      "epoch:2 step:2547 [D loss: 0.622206, acc: 64.84%] [G loss: 3.098537]\n",
      "epoch:2 step:2548 [D loss: 0.554590, acc: 67.19%] [G loss: 3.051385]\n",
      "epoch:2 step:2549 [D loss: 0.543691, acc: 71.88%] [G loss: 3.128476]\n",
      "epoch:2 step:2550 [D loss: 0.625077, acc: 60.94%] [G loss: 3.175176]\n",
      "epoch:2 step:2551 [D loss: 0.493187, acc: 78.12%] [G loss: 3.198114]\n",
      "epoch:2 step:2552 [D loss: 0.624260, acc: 66.41%] [G loss: 3.028944]\n",
      "epoch:2 step:2553 [D loss: 0.551682, acc: 75.78%] [G loss: 3.129921]\n",
      "epoch:2 step:2554 [D loss: 0.504715, acc: 75.78%] [G loss: 2.930983]\n",
      "epoch:2 step:2555 [D loss: 0.506542, acc: 74.22%] [G loss: 2.971126]\n",
      "epoch:2 step:2556 [D loss: 0.704777, acc: 63.28%] [G loss: 2.569198]\n",
      "epoch:2 step:2557 [D loss: 0.554940, acc: 67.97%] [G loss: 2.821578]\n",
      "epoch:2 step:2558 [D loss: 0.560572, acc: 71.09%] [G loss: 2.934673]\n",
      "epoch:2 step:2559 [D loss: 0.530401, acc: 71.88%] [G loss: 2.834683]\n",
      "epoch:2 step:2560 [D loss: 0.552918, acc: 73.44%] [G loss: 2.784763]\n",
      "epoch:2 step:2561 [D loss: 0.562329, acc: 71.09%] [G loss: 2.602847]\n",
      "epoch:2 step:2562 [D loss: 0.609469, acc: 69.53%] [G loss: 2.564660]\n",
      "epoch:2 step:2563 [D loss: 0.523554, acc: 74.22%] [G loss: 2.781577]\n",
      "epoch:2 step:2564 [D loss: 0.654677, acc: 64.84%] [G loss: 2.851202]\n",
      "epoch:2 step:2565 [D loss: 0.499433, acc: 77.34%] [G loss: 3.116204]\n",
      "epoch:2 step:2566 [D loss: 0.600077, acc: 66.41%] [G loss: 2.691999]\n",
      "epoch:2 step:2567 [D loss: 0.508811, acc: 73.44%] [G loss: 2.981030]\n",
      "epoch:2 step:2568 [D loss: 0.498153, acc: 78.91%] [G loss: 3.078581]\n",
      "epoch:2 step:2569 [D loss: 0.559528, acc: 72.66%] [G loss: 3.121571]\n",
      "epoch:2 step:2570 [D loss: 0.626117, acc: 66.41%] [G loss: 3.111385]\n",
      "epoch:2 step:2571 [D loss: 0.526545, acc: 71.09%] [G loss: 3.548765]\n",
      "epoch:2 step:2572 [D loss: 0.598630, acc: 70.31%] [G loss: 2.842786]\n",
      "epoch:2 step:2573 [D loss: 0.499339, acc: 76.56%] [G loss: 3.264315]\n",
      "epoch:2 step:2574 [D loss: 0.485532, acc: 77.34%] [G loss: 3.355903]\n",
      "epoch:2 step:2575 [D loss: 0.513546, acc: 75.78%] [G loss: 3.314396]\n",
      "epoch:2 step:2576 [D loss: 0.629586, acc: 70.31%] [G loss: 2.921411]\n",
      "epoch:2 step:2577 [D loss: 0.593494, acc: 70.31%] [G loss: 2.727947]\n",
      "epoch:2 step:2578 [D loss: 0.577092, acc: 66.41%] [G loss: 2.911073]\n",
      "epoch:2 step:2579 [D loss: 0.575981, acc: 75.00%] [G loss: 3.037004]\n",
      "epoch:2 step:2580 [D loss: 0.565213, acc: 68.75%] [G loss: 3.044055]\n",
      "epoch:2 step:2581 [D loss: 0.429501, acc: 84.38%] [G loss: 3.620029]\n",
      "epoch:2 step:2582 [D loss: 0.494004, acc: 77.34%] [G loss: 3.252048]\n",
      "epoch:2 step:2583 [D loss: 0.508036, acc: 78.91%] [G loss: 3.327232]\n",
      "epoch:2 step:2584 [D loss: 0.714820, acc: 58.59%] [G loss: 2.896458]\n",
      "epoch:2 step:2585 [D loss: 0.549561, acc: 71.88%] [G loss: 2.852210]\n",
      "epoch:2 step:2586 [D loss: 0.534117, acc: 71.88%] [G loss: 3.169522]\n",
      "epoch:2 step:2587 [D loss: 0.507787, acc: 73.44%] [G loss: 3.020651]\n",
      "epoch:2 step:2588 [D loss: 0.490847, acc: 75.00%] [G loss: 3.485458]\n",
      "epoch:2 step:2589 [D loss: 0.531292, acc: 74.22%] [G loss: 3.126497]\n",
      "epoch:2 step:2590 [D loss: 0.674495, acc: 60.94%] [G loss: 2.748447]\n",
      "epoch:2 step:2591 [D loss: 0.611376, acc: 67.19%] [G loss: 2.815395]\n",
      "epoch:2 step:2592 [D loss: 0.555670, acc: 68.75%] [G loss: 3.079729]\n",
      "epoch:2 step:2593 [D loss: 0.627284, acc: 64.84%] [G loss: 2.786605]\n",
      "epoch:2 step:2594 [D loss: 0.630867, acc: 66.41%] [G loss: 2.916245]\n",
      "epoch:2 step:2595 [D loss: 0.599647, acc: 68.75%] [G loss: 2.768612]\n",
      "epoch:2 step:2596 [D loss: 0.550317, acc: 71.88%] [G loss: 3.126209]\n",
      "epoch:2 step:2597 [D loss: 0.638522, acc: 66.41%] [G loss: 2.923857]\n",
      "epoch:2 step:2598 [D loss: 0.518500, acc: 71.09%] [G loss: 2.968658]\n",
      "epoch:2 step:2599 [D loss: 0.617997, acc: 70.31%] [G loss: 2.861330]\n",
      "epoch:2 step:2600 [D loss: 0.581504, acc: 71.09%] [G loss: 2.853221]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.649808\n",
      "FID: 71.389130\n",
      "0 = 13.564648309516857\n",
      "1 = 0.1031140870764673\n",
      "2 = 0.9520999789237976\n",
      "3 = 0.9362000226974487\n",
      "4 = 0.9679999947547913\n",
      "5 = 0.9669489860534668\n",
      "6 = 0.9362000226974487\n",
      "7 = 10.320167581820469\n",
      "8 = 0.16948740871327672\n",
      "9 = 0.8952000141143799\n",
      "10 = 0.8921999931335449\n",
      "11 = 0.8981999754905701\n",
      "12 = 0.8975855112075806\n",
      "13 = 0.8921999931335449\n",
      "14 = 4.649816036224365\n",
      "15 = 7.930846691131592\n",
      "16 = 0.3501092195510864\n",
      "17 = 4.649807929992676\n",
      "18 = 71.38912963867188\n",
      "epoch:2 step:2601 [D loss: 0.537993, acc: 77.34%] [G loss: 2.780662]\n",
      "epoch:2 step:2602 [D loss: 0.498667, acc: 74.22%] [G loss: 3.374285]\n",
      "epoch:2 step:2603 [D loss: 0.562790, acc: 75.78%] [G loss: 3.052622]\n",
      "epoch:2 step:2604 [D loss: 0.526347, acc: 75.00%] [G loss: 3.014849]\n",
      "epoch:2 step:2605 [D loss: 0.642847, acc: 64.84%] [G loss: 3.080549]\n",
      "epoch:2 step:2606 [D loss: 0.467753, acc: 76.56%] [G loss: 3.161092]\n",
      "epoch:2 step:2607 [D loss: 0.484177, acc: 78.12%] [G loss: 3.334948]\n",
      "epoch:2 step:2608 [D loss: 0.470308, acc: 77.34%] [G loss: 3.520380]\n",
      "epoch:2 step:2609 [D loss: 0.540517, acc: 76.56%] [G loss: 3.219854]\n",
      "epoch:2 step:2610 [D loss: 0.550026, acc: 73.44%] [G loss: 3.233273]\n",
      "epoch:2 step:2611 [D loss: 0.517074, acc: 75.78%] [G loss: 2.950616]\n",
      "epoch:2 step:2612 [D loss: 0.544146, acc: 72.66%] [G loss: 2.939205]\n",
      "epoch:2 step:2613 [D loss: 0.619933, acc: 64.84%] [G loss: 2.847258]\n",
      "epoch:2 step:2614 [D loss: 0.606970, acc: 65.62%] [G loss: 2.970996]\n",
      "epoch:2 step:2615 [D loss: 0.548477, acc: 71.88%] [G loss: 3.112578]\n",
      "epoch:2 step:2616 [D loss: 0.539559, acc: 70.31%] [G loss: 3.115757]\n",
      "epoch:2 step:2617 [D loss: 0.620690, acc: 63.28%] [G loss: 3.031763]\n",
      "epoch:2 step:2618 [D loss: 0.564434, acc: 75.78%] [G loss: 3.280780]\n",
      "epoch:2 step:2619 [D loss: 0.497867, acc: 75.00%] [G loss: 3.473677]\n",
      "epoch:2 step:2620 [D loss: 0.523267, acc: 71.88%] [G loss: 3.659965]\n",
      "epoch:2 step:2621 [D loss: 0.434931, acc: 82.03%] [G loss: 3.826336]\n",
      "epoch:2 step:2622 [D loss: 0.515417, acc: 73.44%] [G loss: 3.330902]\n",
      "epoch:2 step:2623 [D loss: 0.608109, acc: 70.31%] [G loss: 3.026485]\n",
      "epoch:2 step:2624 [D loss: 0.526809, acc: 75.00%] [G loss: 3.225116]\n",
      "epoch:2 step:2625 [D loss: 0.633418, acc: 71.09%] [G loss: 2.811849]\n",
      "epoch:2 step:2626 [D loss: 0.644733, acc: 65.62%] [G loss: 3.107295]\n",
      "epoch:2 step:2627 [D loss: 0.628452, acc: 62.50%] [G loss: 3.185867]\n",
      "epoch:2 step:2628 [D loss: 0.491782, acc: 76.56%] [G loss: 3.191085]\n",
      "epoch:2 step:2629 [D loss: 0.470434, acc: 79.69%] [G loss: 3.472038]\n",
      "epoch:2 step:2630 [D loss: 0.524780, acc: 78.12%] [G loss: 3.470636]\n",
      "epoch:2 step:2631 [D loss: 0.522352, acc: 77.34%] [G loss: 3.198641]\n",
      "epoch:2 step:2632 [D loss: 0.596012, acc: 67.19%] [G loss: 3.158764]\n",
      "epoch:2 step:2633 [D loss: 0.622621, acc: 65.62%] [G loss: 3.353526]\n",
      "epoch:2 step:2634 [D loss: 0.535816, acc: 74.22%] [G loss: 3.231564]\n",
      "epoch:2 step:2635 [D loss: 0.587437, acc: 71.09%] [G loss: 2.989215]\n",
      "epoch:2 step:2636 [D loss: 0.516292, acc: 74.22%] [G loss: 2.723391]\n",
      "epoch:2 step:2637 [D loss: 0.567143, acc: 75.78%] [G loss: 2.746517]\n",
      "epoch:2 step:2638 [D loss: 0.653875, acc: 64.06%] [G loss: 3.139445]\n",
      "epoch:2 step:2639 [D loss: 0.635539, acc: 68.75%] [G loss: 2.723027]\n",
      "epoch:2 step:2640 [D loss: 0.713966, acc: 57.03%] [G loss: 2.590084]\n",
      "epoch:2 step:2641 [D loss: 0.575765, acc: 71.09%] [G loss: 2.678956]\n",
      "epoch:2 step:2642 [D loss: 0.691831, acc: 62.50%] [G loss: 2.575324]\n",
      "epoch:2 step:2643 [D loss: 0.548669, acc: 71.09%] [G loss: 2.849408]\n",
      "epoch:2 step:2644 [D loss: 0.607645, acc: 67.19%] [G loss: 3.012349]\n",
      "epoch:2 step:2645 [D loss: 0.601738, acc: 67.19%] [G loss: 2.867059]\n",
      "epoch:2 step:2646 [D loss: 0.576250, acc: 69.53%] [G loss: 2.970609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2647 [D loss: 0.543047, acc: 73.44%] [G loss: 3.020105]\n",
      "epoch:2 step:2648 [D loss: 0.589796, acc: 70.31%] [G loss: 2.970703]\n",
      "epoch:2 step:2649 [D loss: 0.584220, acc: 68.75%] [G loss: 2.946634]\n",
      "epoch:2 step:2650 [D loss: 0.602126, acc: 69.53%] [G loss: 2.707590]\n",
      "epoch:2 step:2651 [D loss: 0.556637, acc: 73.44%] [G loss: 3.013251]\n",
      "epoch:2 step:2652 [D loss: 0.503365, acc: 76.56%] [G loss: 2.809896]\n",
      "epoch:2 step:2653 [D loss: 0.595002, acc: 65.62%] [G loss: 2.929211]\n",
      "epoch:2 step:2654 [D loss: 0.546650, acc: 67.19%] [G loss: 3.083137]\n",
      "epoch:2 step:2655 [D loss: 0.521472, acc: 78.91%] [G loss: 3.383944]\n",
      "epoch:2 step:2656 [D loss: 0.559182, acc: 69.53%] [G loss: 3.056154]\n",
      "epoch:2 step:2657 [D loss: 0.604701, acc: 67.19%] [G loss: 2.965087]\n",
      "epoch:2 step:2658 [D loss: 0.536666, acc: 75.00%] [G loss: 2.809954]\n",
      "epoch:2 step:2659 [D loss: 0.540103, acc: 74.22%] [G loss: 2.926195]\n",
      "epoch:2 step:2660 [D loss: 0.492418, acc: 75.00%] [G loss: 3.344265]\n",
      "epoch:2 step:2661 [D loss: 0.589820, acc: 69.53%] [G loss: 3.245378]\n",
      "epoch:2 step:2662 [D loss: 0.627339, acc: 66.41%] [G loss: 2.699765]\n",
      "epoch:2 step:2663 [D loss: 0.599082, acc: 64.06%] [G loss: 2.806765]\n",
      "epoch:2 step:2664 [D loss: 0.538630, acc: 72.66%] [G loss: 2.766586]\n",
      "epoch:2 step:2665 [D loss: 0.463175, acc: 78.91%] [G loss: 3.103978]\n",
      "epoch:2 step:2666 [D loss: 0.461046, acc: 77.34%] [G loss: 3.267037]\n",
      "epoch:2 step:2667 [D loss: 0.628118, acc: 68.75%] [G loss: 2.837629]\n",
      "epoch:2 step:2668 [D loss: 0.567750, acc: 66.41%] [G loss: 3.092340]\n",
      "epoch:2 step:2669 [D loss: 0.531837, acc: 69.53%] [G loss: 3.471480]\n",
      "epoch:2 step:2670 [D loss: 0.626046, acc: 61.72%] [G loss: 3.135695]\n",
      "epoch:2 step:2671 [D loss: 0.604501, acc: 70.31%] [G loss: 3.001557]\n",
      "epoch:2 step:2672 [D loss: 0.593345, acc: 72.66%] [G loss: 2.784457]\n",
      "epoch:2 step:2673 [D loss: 0.553735, acc: 71.09%] [G loss: 2.423769]\n",
      "epoch:2 step:2674 [D loss: 0.524979, acc: 71.09%] [G loss: 2.734139]\n",
      "epoch:2 step:2675 [D loss: 0.613384, acc: 68.75%] [G loss: 2.742732]\n",
      "epoch:2 step:2676 [D loss: 0.619059, acc: 64.84%] [G loss: 2.655807]\n",
      "epoch:2 step:2677 [D loss: 0.450555, acc: 76.56%] [G loss: 3.602588]\n",
      "epoch:2 step:2678 [D loss: 0.513436, acc: 74.22%] [G loss: 3.122430]\n",
      "epoch:2 step:2679 [D loss: 0.495239, acc: 76.56%] [G loss: 3.463729]\n",
      "epoch:2 step:2680 [D loss: 0.453833, acc: 78.91%] [G loss: 3.725437]\n",
      "epoch:2 step:2681 [D loss: 0.579011, acc: 72.66%] [G loss: 3.246767]\n",
      "epoch:2 step:2682 [D loss: 0.626636, acc: 64.06%] [G loss: 3.013778]\n",
      "epoch:2 step:2683 [D loss: 0.584833, acc: 69.53%] [G loss: 2.792137]\n",
      "epoch:2 step:2684 [D loss: 0.490435, acc: 75.00%] [G loss: 3.070916]\n",
      "epoch:2 step:2685 [D loss: 0.612232, acc: 71.09%] [G loss: 2.856667]\n",
      "epoch:2 step:2686 [D loss: 0.602240, acc: 65.62%] [G loss: 2.687392]\n",
      "epoch:2 step:2687 [D loss: 0.626020, acc: 66.41%] [G loss: 3.132920]\n",
      "epoch:2 step:2688 [D loss: 0.596922, acc: 69.53%] [G loss: 3.032925]\n",
      "epoch:2 step:2689 [D loss: 0.612887, acc: 64.84%] [G loss: 3.020509]\n",
      "epoch:2 step:2690 [D loss: 0.554962, acc: 73.44%] [G loss: 3.020735]\n",
      "epoch:2 step:2691 [D loss: 0.566882, acc: 71.09%] [G loss: 2.846253]\n",
      "epoch:2 step:2692 [D loss: 0.616488, acc: 64.84%] [G loss: 3.004889]\n",
      "epoch:2 step:2693 [D loss: 0.496358, acc: 75.78%] [G loss: 3.044824]\n",
      "epoch:2 step:2694 [D loss: 0.547179, acc: 71.09%] [G loss: 2.951477]\n",
      "epoch:2 step:2695 [D loss: 0.445250, acc: 80.47%] [G loss: 2.994605]\n",
      "epoch:2 step:2696 [D loss: 0.518629, acc: 73.44%] [G loss: 3.225997]\n",
      "epoch:2 step:2697 [D loss: 0.434108, acc: 77.34%] [G loss: 3.389559]\n",
      "epoch:2 step:2698 [D loss: 0.659325, acc: 63.28%] [G loss: 2.925522]\n",
      "epoch:2 step:2699 [D loss: 0.569360, acc: 64.06%] [G loss: 3.093977]\n",
      "epoch:2 step:2700 [D loss: 0.621395, acc: 68.75%] [G loss: 2.895249]\n",
      "epoch:2 step:2701 [D loss: 0.676887, acc: 62.50%] [G loss: 2.461196]\n",
      "epoch:2 step:2702 [D loss: 0.644160, acc: 68.75%] [G loss: 2.385033]\n",
      "epoch:2 step:2703 [D loss: 0.563172, acc: 68.75%] [G loss: 2.793707]\n",
      "epoch:2 step:2704 [D loss: 0.553668, acc: 72.66%] [G loss: 2.819792]\n",
      "epoch:2 step:2705 [D loss: 0.556241, acc: 70.31%] [G loss: 3.023084]\n",
      "epoch:2 step:2706 [D loss: 0.547527, acc: 72.66%] [G loss: 2.723655]\n",
      "epoch:2 step:2707 [D loss: 0.572044, acc: 70.31%] [G loss: 3.426341]\n",
      "epoch:2 step:2708 [D loss: 0.507319, acc: 75.78%] [G loss: 2.883300]\n",
      "epoch:2 step:2709 [D loss: 0.571124, acc: 67.19%] [G loss: 2.964020]\n",
      "epoch:2 step:2710 [D loss: 0.561934, acc: 73.44%] [G loss: 3.012380]\n",
      "epoch:2 step:2711 [D loss: 0.502301, acc: 76.56%] [G loss: 3.338986]\n",
      "epoch:2 step:2712 [D loss: 0.541335, acc: 75.00%] [G loss: 2.976453]\n",
      "epoch:2 step:2713 [D loss: 0.530527, acc: 74.22%] [G loss: 2.834090]\n",
      "epoch:2 step:2714 [D loss: 0.580316, acc: 73.44%] [G loss: 2.883826]\n",
      "epoch:2 step:2715 [D loss: 0.599244, acc: 69.53%] [G loss: 2.954536]\n",
      "epoch:2 step:2716 [D loss: 0.452045, acc: 80.47%] [G loss: 2.961427]\n",
      "epoch:2 step:2717 [D loss: 0.534258, acc: 74.22%] [G loss: 2.815656]\n",
      "epoch:2 step:2718 [D loss: 0.570652, acc: 68.75%] [G loss: 3.027169]\n",
      "epoch:2 step:2719 [D loss: 0.574607, acc: 68.75%] [G loss: 2.917711]\n",
      "epoch:2 step:2720 [D loss: 0.515958, acc: 72.66%] [G loss: 2.627825]\n",
      "epoch:2 step:2721 [D loss: 0.477709, acc: 74.22%] [G loss: 2.538384]\n",
      "epoch:2 step:2722 [D loss: 0.481598, acc: 79.69%] [G loss: 3.208383]\n",
      "epoch:2 step:2723 [D loss: 0.650550, acc: 66.41%] [G loss: 2.683262]\n",
      "epoch:2 step:2724 [D loss: 0.588433, acc: 67.19%] [G loss: 3.154469]\n",
      "epoch:2 step:2725 [D loss: 0.587155, acc: 65.62%] [G loss: 2.752039]\n",
      "epoch:2 step:2726 [D loss: 0.676675, acc: 67.97%] [G loss: 3.087366]\n",
      "epoch:2 step:2727 [D loss: 0.540977, acc: 72.66%] [G loss: 3.013005]\n",
      "epoch:2 step:2728 [D loss: 0.468695, acc: 78.12%] [G loss: 3.310938]\n",
      "epoch:2 step:2729 [D loss: 0.622655, acc: 64.84%] [G loss: 3.290395]\n",
      "epoch:2 step:2730 [D loss: 0.544044, acc: 71.88%] [G loss: 2.842896]\n",
      "epoch:2 step:2731 [D loss: 0.512549, acc: 75.78%] [G loss: 3.117242]\n",
      "epoch:2 step:2732 [D loss: 0.591168, acc: 67.97%] [G loss: 2.984659]\n",
      "epoch:2 step:2733 [D loss: 0.542929, acc: 71.88%] [G loss: 2.754680]\n",
      "epoch:2 step:2734 [D loss: 0.579751, acc: 65.62%] [G loss: 3.185577]\n",
      "epoch:2 step:2735 [D loss: 0.632953, acc: 71.88%] [G loss: 2.704646]\n",
      "epoch:2 step:2736 [D loss: 0.459262, acc: 82.03%] [G loss: 2.765719]\n",
      "epoch:2 step:2737 [D loss: 0.496454, acc: 76.56%] [G loss: 2.852415]\n",
      "epoch:2 step:2738 [D loss: 0.704774, acc: 64.06%] [G loss: 2.924837]\n",
      "epoch:2 step:2739 [D loss: 0.591213, acc: 69.53%] [G loss: 2.920853]\n",
      "epoch:2 step:2740 [D loss: 0.507564, acc: 74.22%] [G loss: 3.138537]\n",
      "epoch:2 step:2741 [D loss: 0.613598, acc: 69.53%] [G loss: 2.764200]\n",
      "epoch:2 step:2742 [D loss: 0.552702, acc: 71.88%] [G loss: 2.666759]\n",
      "epoch:2 step:2743 [D loss: 0.640343, acc: 66.41%] [G loss: 2.727016]\n",
      "epoch:2 step:2744 [D loss: 0.582076, acc: 72.66%] [G loss: 3.038943]\n",
      "epoch:2 step:2745 [D loss: 0.511046, acc: 77.34%] [G loss: 3.053225]\n",
      "epoch:2 step:2746 [D loss: 0.550558, acc: 71.09%] [G loss: 2.852502]\n",
      "epoch:2 step:2747 [D loss: 0.571096, acc: 67.97%] [G loss: 2.830924]\n",
      "epoch:2 step:2748 [D loss: 0.541389, acc: 75.78%] [G loss: 2.946205]\n",
      "epoch:2 step:2749 [D loss: 0.488532, acc: 75.78%] [G loss: 3.377111]\n",
      "epoch:2 step:2750 [D loss: 0.582037, acc: 73.44%] [G loss: 2.819824]\n",
      "epoch:2 step:2751 [D loss: 0.603290, acc: 70.31%] [G loss: 2.818147]\n",
      "epoch:2 step:2752 [D loss: 0.542092, acc: 71.88%] [G loss: 2.614105]\n",
      "epoch:2 step:2753 [D loss: 0.564562, acc: 71.09%] [G loss: 2.805511]\n",
      "epoch:2 step:2754 [D loss: 0.589214, acc: 70.31%] [G loss: 2.607093]\n",
      "epoch:2 step:2755 [D loss: 0.583805, acc: 66.41%] [G loss: 2.912824]\n",
      "epoch:2 step:2756 [D loss: 0.594192, acc: 67.97%] [G loss: 2.989862]\n",
      "epoch:2 step:2757 [D loss: 0.566615, acc: 71.09%] [G loss: 2.659333]\n",
      "epoch:2 step:2758 [D loss: 0.518379, acc: 74.22%] [G loss: 3.307599]\n",
      "epoch:2 step:2759 [D loss: 0.528709, acc: 71.09%] [G loss: 3.195956]\n",
      "epoch:2 step:2760 [D loss: 0.490531, acc: 76.56%] [G loss: 3.555954]\n",
      "epoch:2 step:2761 [D loss: 0.566312, acc: 75.00%] [G loss: 3.250757]\n",
      "epoch:2 step:2762 [D loss: 0.472619, acc: 81.25%] [G loss: 3.587694]\n",
      "epoch:2 step:2763 [D loss: 0.528379, acc: 77.34%] [G loss: 3.218671]\n",
      "epoch:2 step:2764 [D loss: 0.474817, acc: 81.25%] [G loss: 3.830839]\n",
      "epoch:2 step:2765 [D loss: 0.603255, acc: 67.19%] [G loss: 2.770885]\n",
      "epoch:2 step:2766 [D loss: 0.656252, acc: 60.94%] [G loss: 2.603537]\n",
      "epoch:2 step:2767 [D loss: 0.506204, acc: 75.78%] [G loss: 2.871702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2768 [D loss: 0.583605, acc: 70.31%] [G loss: 3.173724]\n",
      "epoch:2 step:2769 [D loss: 0.538266, acc: 73.44%] [G loss: 3.134044]\n",
      "epoch:2 step:2770 [D loss: 0.548319, acc: 68.75%] [G loss: 3.128629]\n",
      "epoch:2 step:2771 [D loss: 0.480690, acc: 77.34%] [G loss: 2.963921]\n",
      "epoch:2 step:2772 [D loss: 0.502671, acc: 74.22%] [G loss: 3.326057]\n",
      "epoch:2 step:2773 [D loss: 0.580543, acc: 69.53%] [G loss: 3.072287]\n",
      "epoch:2 step:2774 [D loss: 0.496094, acc: 78.91%] [G loss: 3.167793]\n",
      "epoch:2 step:2775 [D loss: 0.610048, acc: 73.44%] [G loss: 3.200245]\n",
      "epoch:2 step:2776 [D loss: 0.611796, acc: 65.62%] [G loss: 2.629988]\n",
      "epoch:2 step:2777 [D loss: 0.635254, acc: 69.53%] [G loss: 2.979266]\n",
      "epoch:2 step:2778 [D loss: 0.586962, acc: 67.19%] [G loss: 3.279933]\n",
      "epoch:2 step:2779 [D loss: 0.515828, acc: 71.09%] [G loss: 3.262073]\n",
      "epoch:2 step:2780 [D loss: 0.450519, acc: 80.47%] [G loss: 3.470397]\n",
      "epoch:2 step:2781 [D loss: 0.587804, acc: 67.97%] [G loss: 3.128439]\n",
      "epoch:2 step:2782 [D loss: 0.575832, acc: 64.84%] [G loss: 3.076001]\n",
      "epoch:2 step:2783 [D loss: 0.478965, acc: 79.69%] [G loss: 2.953353]\n",
      "epoch:2 step:2784 [D loss: 0.451569, acc: 80.47%] [G loss: 3.460468]\n",
      "epoch:2 step:2785 [D loss: 0.530266, acc: 75.78%] [G loss: 3.318033]\n",
      "epoch:2 step:2786 [D loss: 0.438918, acc: 77.34%] [G loss: 3.488566]\n",
      "epoch:2 step:2787 [D loss: 0.533300, acc: 72.66%] [G loss: 3.341834]\n",
      "epoch:2 step:2788 [D loss: 0.553851, acc: 74.22%] [G loss: 3.256830]\n",
      "epoch:2 step:2789 [D loss: 0.609456, acc: 69.53%] [G loss: 2.918626]\n",
      "epoch:2 step:2790 [D loss: 0.605912, acc: 62.50%] [G loss: 3.054011]\n",
      "epoch:2 step:2791 [D loss: 0.547739, acc: 70.31%] [G loss: 3.501530]\n",
      "epoch:2 step:2792 [D loss: 0.494644, acc: 72.66%] [G loss: 3.291978]\n",
      "epoch:2 step:2793 [D loss: 0.535337, acc: 69.53%] [G loss: 3.496223]\n",
      "epoch:2 step:2794 [D loss: 0.661693, acc: 66.41%] [G loss: 3.312558]\n",
      "epoch:2 step:2795 [D loss: 0.575260, acc: 68.75%] [G loss: 3.339775]\n",
      "epoch:2 step:2796 [D loss: 0.511097, acc: 75.00%] [G loss: 3.346219]\n",
      "epoch:2 step:2797 [D loss: 0.386699, acc: 84.38%] [G loss: 3.586522]\n",
      "epoch:2 step:2798 [D loss: 0.411788, acc: 85.16%] [G loss: 3.839740]\n",
      "epoch:2 step:2799 [D loss: 0.395064, acc: 84.38%] [G loss: 3.902298]\n",
      "epoch:2 step:2800 [D loss: 0.474887, acc: 78.12%] [G loss: 3.371917]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.606813\n",
      "FID: 72.855751\n",
      "0 = 13.450149398422253\n",
      "1 = 0.09075991355195852\n",
      "2 = 0.9502000212669373\n",
      "3 = 0.9358000159263611\n",
      "4 = 0.9646000266075134\n",
      "5 = 0.9635502696037292\n",
      "6 = 0.9358000159263611\n",
      "7 = 10.406778977870903\n",
      "8 = 0.17019771242408663\n",
      "9 = 0.8909000158309937\n",
      "10 = 0.8885999917984009\n",
      "11 = 0.8931999802589417\n",
      "12 = 0.8927064538002014\n",
      "13 = 0.8885999917984009\n",
      "14 = 4.606823444366455\n",
      "15 = 7.960797309875488\n",
      "16 = 0.3513309955596924\n",
      "17 = 4.606813430786133\n",
      "18 = 72.85575103759766\n",
      "epoch:2 step:2801 [D loss: 0.590112, acc: 75.00%] [G loss: 3.426178]\n",
      "epoch:2 step:2802 [D loss: 0.784259, acc: 60.16%] [G loss: 3.283643]\n",
      "epoch:2 step:2803 [D loss: 0.533156, acc: 76.56%] [G loss: 3.829318]\n",
      "epoch:2 step:2804 [D loss: 0.413734, acc: 83.59%] [G loss: 3.653117]\n",
      "epoch:2 step:2805 [D loss: 0.534331, acc: 75.78%] [G loss: 3.458929]\n",
      "epoch:2 step:2806 [D loss: 0.526244, acc: 71.09%] [G loss: 3.120133]\n",
      "epoch:2 step:2807 [D loss: 0.434772, acc: 78.12%] [G loss: 3.331677]\n",
      "epoch:2 step:2808 [D loss: 0.593146, acc: 68.75%] [G loss: 3.711769]\n",
      "epoch:2 step:2809 [D loss: 0.565222, acc: 72.66%] [G loss: 3.154089]\n",
      "epoch:2 step:2810 [D loss: 0.393152, acc: 83.59%] [G loss: 4.082576]\n",
      "epoch:2 step:2811 [D loss: 0.507297, acc: 73.44%] [G loss: 3.821819]\n",
      "epoch:3 step:2812 [D loss: 0.628142, acc: 71.09%] [G loss: 2.955813]\n",
      "epoch:3 step:2813 [D loss: 0.437989, acc: 82.81%] [G loss: 3.472870]\n",
      "epoch:3 step:2814 [D loss: 0.586094, acc: 71.09%] [G loss: 3.024524]\n",
      "epoch:3 step:2815 [D loss: 0.503614, acc: 76.56%] [G loss: 3.320193]\n",
      "epoch:3 step:2816 [D loss: 0.663461, acc: 67.19%] [G loss: 3.237013]\n",
      "epoch:3 step:2817 [D loss: 0.522832, acc: 75.78%] [G loss: 3.332675]\n",
      "epoch:3 step:2818 [D loss: 0.535891, acc: 71.09%] [G loss: 3.074082]\n",
      "epoch:3 step:2819 [D loss: 0.580495, acc: 67.19%] [G loss: 2.962942]\n",
      "epoch:3 step:2820 [D loss: 0.600498, acc: 74.22%] [G loss: 2.867566]\n",
      "epoch:3 step:2821 [D loss: 0.627960, acc: 66.41%] [G loss: 2.747617]\n",
      "epoch:3 step:2822 [D loss: 0.525941, acc: 74.22%] [G loss: 3.291472]\n",
      "epoch:3 step:2823 [D loss: 0.549044, acc: 72.66%] [G loss: 3.078977]\n",
      "epoch:3 step:2824 [D loss: 0.556731, acc: 71.88%] [G loss: 2.752218]\n",
      "epoch:3 step:2825 [D loss: 0.561192, acc: 71.09%] [G loss: 3.240125]\n",
      "epoch:3 step:2826 [D loss: 0.537421, acc: 75.00%] [G loss: 3.079289]\n",
      "epoch:3 step:2827 [D loss: 0.465636, acc: 77.34%] [G loss: 3.178230]\n",
      "epoch:3 step:2828 [D loss: 0.620367, acc: 66.41%] [G loss: 2.678488]\n",
      "epoch:3 step:2829 [D loss: 0.637961, acc: 71.09%] [G loss: 2.643714]\n",
      "epoch:3 step:2830 [D loss: 0.602214, acc: 71.88%] [G loss: 2.764049]\n",
      "epoch:3 step:2831 [D loss: 0.718167, acc: 53.91%] [G loss: 2.410196]\n",
      "epoch:3 step:2832 [D loss: 0.604914, acc: 67.97%] [G loss: 2.801688]\n",
      "epoch:3 step:2833 [D loss: 0.577719, acc: 69.53%] [G loss: 2.775311]\n",
      "epoch:3 step:2834 [D loss: 0.485806, acc: 75.78%] [G loss: 3.015911]\n",
      "epoch:3 step:2835 [D loss: 0.541617, acc: 72.66%] [G loss: 3.382118]\n",
      "epoch:3 step:2836 [D loss: 0.511714, acc: 72.66%] [G loss: 3.049805]\n",
      "epoch:3 step:2837 [D loss: 0.594495, acc: 67.97%] [G loss: 3.084423]\n",
      "epoch:3 step:2838 [D loss: 0.496582, acc: 78.91%] [G loss: 2.956738]\n",
      "epoch:3 step:2839 [D loss: 0.592820, acc: 67.19%] [G loss: 2.820301]\n",
      "epoch:3 step:2840 [D loss: 0.472817, acc: 78.91%] [G loss: 2.989121]\n",
      "epoch:3 step:2841 [D loss: 0.622164, acc: 71.09%] [G loss: 3.041542]\n",
      "epoch:3 step:2842 [D loss: 0.530771, acc: 75.78%] [G loss: 3.071463]\n",
      "epoch:3 step:2843 [D loss: 0.545272, acc: 71.88%] [G loss: 2.734138]\n",
      "epoch:3 step:2844 [D loss: 0.514242, acc: 75.78%] [G loss: 3.156815]\n",
      "epoch:3 step:2845 [D loss: 0.500974, acc: 73.44%] [G loss: 3.092536]\n",
      "epoch:3 step:2846 [D loss: 0.492049, acc: 75.78%] [G loss: 3.098445]\n",
      "epoch:3 step:2847 [D loss: 0.472339, acc: 79.69%] [G loss: 3.257577]\n",
      "epoch:3 step:2848 [D loss: 0.564307, acc: 70.31%] [G loss: 3.350202]\n",
      "epoch:3 step:2849 [D loss: 0.673092, acc: 60.94%] [G loss: 3.190716]\n",
      "epoch:3 step:2850 [D loss: 0.489997, acc: 74.22%] [G loss: 3.007472]\n",
      "epoch:3 step:2851 [D loss: 0.467221, acc: 78.12%] [G loss: 3.000692]\n",
      "epoch:3 step:2852 [D loss: 0.695981, acc: 65.62%] [G loss: 2.595722]\n",
      "epoch:3 step:2853 [D loss: 0.548443, acc: 75.00%] [G loss: 3.453388]\n",
      "epoch:3 step:2854 [D loss: 0.588428, acc: 71.09%] [G loss: 3.375027]\n",
      "epoch:3 step:2855 [D loss: 0.619094, acc: 68.75%] [G loss: 3.015292]\n",
      "epoch:3 step:2856 [D loss: 0.598433, acc: 69.53%] [G loss: 2.916800]\n",
      "epoch:3 step:2857 [D loss: 0.682775, acc: 62.50%] [G loss: 2.879288]\n",
      "epoch:3 step:2858 [D loss: 0.514793, acc: 77.34%] [G loss: 2.698365]\n",
      "epoch:3 step:2859 [D loss: 0.548993, acc: 71.88%] [G loss: 2.780735]\n",
      "epoch:3 step:2860 [D loss: 0.607540, acc: 68.75%] [G loss: 2.785508]\n",
      "epoch:3 step:2861 [D loss: 0.567847, acc: 72.66%] [G loss: 2.597274]\n",
      "epoch:3 step:2862 [D loss: 0.541492, acc: 73.44%] [G loss: 2.789258]\n",
      "epoch:3 step:2863 [D loss: 0.591524, acc: 70.31%] [G loss: 2.587516]\n",
      "epoch:3 step:2864 [D loss: 0.494131, acc: 76.56%] [G loss: 3.028181]\n",
      "epoch:3 step:2865 [D loss: 0.485577, acc: 79.69%] [G loss: 3.439779]\n",
      "epoch:3 step:2866 [D loss: 0.513580, acc: 74.22%] [G loss: 3.153995]\n",
      "epoch:3 step:2867 [D loss: 0.598325, acc: 68.75%] [G loss: 2.917899]\n",
      "epoch:3 step:2868 [D loss: 0.484480, acc: 79.69%] [G loss: 3.032051]\n",
      "epoch:3 step:2869 [D loss: 0.480770, acc: 74.22%] [G loss: 3.187526]\n",
      "epoch:3 step:2870 [D loss: 0.627948, acc: 65.62%] [G loss: 3.302978]\n",
      "epoch:3 step:2871 [D loss: 0.584250, acc: 68.75%] [G loss: 3.397175]\n",
      "epoch:3 step:2872 [D loss: 0.471888, acc: 75.00%] [G loss: 3.353567]\n",
      "epoch:3 step:2873 [D loss: 0.593599, acc: 63.28%] [G loss: 3.196008]\n",
      "epoch:3 step:2874 [D loss: 0.502991, acc: 75.78%] [G loss: 2.976591]\n",
      "epoch:3 step:2875 [D loss: 0.609058, acc: 69.53%] [G loss: 2.981719]\n",
      "epoch:3 step:2876 [D loss: 0.549816, acc: 74.22%] [G loss: 3.163490]\n",
      "epoch:3 step:2877 [D loss: 0.518596, acc: 74.22%] [G loss: 3.028148]\n",
      "epoch:3 step:2878 [D loss: 0.605579, acc: 66.41%] [G loss: 3.149832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2879 [D loss: 0.561393, acc: 71.88%] [G loss: 2.784261]\n",
      "epoch:3 step:2880 [D loss: 0.572844, acc: 67.19%] [G loss: 2.856009]\n",
      "epoch:3 step:2881 [D loss: 0.539603, acc: 75.00%] [G loss: 3.331061]\n",
      "epoch:3 step:2882 [D loss: 0.529239, acc: 74.22%] [G loss: 3.203471]\n",
      "epoch:3 step:2883 [D loss: 0.442593, acc: 78.91%] [G loss: 3.182117]\n",
      "epoch:3 step:2884 [D loss: 0.610398, acc: 68.75%] [G loss: 3.126935]\n",
      "epoch:3 step:2885 [D loss: 0.492272, acc: 82.81%] [G loss: 3.304442]\n",
      "epoch:3 step:2886 [D loss: 0.523290, acc: 72.66%] [G loss: 3.632846]\n",
      "epoch:3 step:2887 [D loss: 0.442707, acc: 75.78%] [G loss: 3.280417]\n",
      "epoch:3 step:2888 [D loss: 0.472999, acc: 75.78%] [G loss: 3.604123]\n",
      "epoch:3 step:2889 [D loss: 0.571615, acc: 71.88%] [G loss: 3.312927]\n",
      "epoch:3 step:2890 [D loss: 0.586776, acc: 69.53%] [G loss: 3.074021]\n",
      "epoch:3 step:2891 [D loss: 0.530509, acc: 71.09%] [G loss: 2.760815]\n",
      "epoch:3 step:2892 [D loss: 0.510652, acc: 76.56%] [G loss: 2.931492]\n",
      "epoch:3 step:2893 [D loss: 0.490399, acc: 78.91%] [G loss: 3.466717]\n",
      "epoch:3 step:2894 [D loss: 0.557726, acc: 78.12%] [G loss: 3.320597]\n",
      "epoch:3 step:2895 [D loss: 0.608242, acc: 66.41%] [G loss: 2.734868]\n",
      "epoch:3 step:2896 [D loss: 0.557742, acc: 71.09%] [G loss: 2.987241]\n",
      "epoch:3 step:2897 [D loss: 0.510207, acc: 74.22%] [G loss: 3.111707]\n",
      "epoch:3 step:2898 [D loss: 0.582750, acc: 68.75%] [G loss: 3.219420]\n",
      "epoch:3 step:2899 [D loss: 0.572697, acc: 69.53%] [G loss: 3.196782]\n",
      "epoch:3 step:2900 [D loss: 0.556980, acc: 69.53%] [G loss: 3.085770]\n",
      "epoch:3 step:2901 [D loss: 0.554367, acc: 71.09%] [G loss: 2.730986]\n",
      "epoch:3 step:2902 [D loss: 0.568776, acc: 73.44%] [G loss: 2.716338]\n",
      "epoch:3 step:2903 [D loss: 0.648538, acc: 67.97%] [G loss: 2.893340]\n",
      "epoch:3 step:2904 [D loss: 0.514453, acc: 76.56%] [G loss: 2.911218]\n",
      "epoch:3 step:2905 [D loss: 0.564644, acc: 73.44%] [G loss: 2.941997]\n",
      "epoch:3 step:2906 [D loss: 0.649801, acc: 63.28%] [G loss: 3.051308]\n",
      "epoch:3 step:2907 [D loss: 0.496444, acc: 73.44%] [G loss: 3.469372]\n",
      "epoch:3 step:2908 [D loss: 0.505317, acc: 77.34%] [G loss: 2.837887]\n",
      "epoch:3 step:2909 [D loss: 0.563747, acc: 71.09%] [G loss: 2.935174]\n",
      "epoch:3 step:2910 [D loss: 0.481225, acc: 77.34%] [G loss: 3.090825]\n",
      "epoch:3 step:2911 [D loss: 0.589789, acc: 65.62%] [G loss: 3.199077]\n",
      "epoch:3 step:2912 [D loss: 0.527161, acc: 77.34%] [G loss: 3.139444]\n",
      "epoch:3 step:2913 [D loss: 0.642996, acc: 60.16%] [G loss: 2.837031]\n",
      "epoch:3 step:2914 [D loss: 0.483623, acc: 78.12%] [G loss: 3.085401]\n",
      "epoch:3 step:2915 [D loss: 0.556376, acc: 69.53%] [G loss: 2.995107]\n",
      "epoch:3 step:2916 [D loss: 0.582694, acc: 72.66%] [G loss: 2.974509]\n",
      "epoch:3 step:2917 [D loss: 0.580844, acc: 70.31%] [G loss: 3.128416]\n",
      "epoch:3 step:2918 [D loss: 0.688550, acc: 58.59%] [G loss: 2.701443]\n",
      "epoch:3 step:2919 [D loss: 0.569772, acc: 75.00%] [G loss: 2.993367]\n",
      "epoch:3 step:2920 [D loss: 0.557062, acc: 75.78%] [G loss: 2.935742]\n",
      "epoch:3 step:2921 [D loss: 0.511712, acc: 74.22%] [G loss: 3.057923]\n",
      "epoch:3 step:2922 [D loss: 0.615067, acc: 63.28%] [G loss: 2.978420]\n",
      "epoch:3 step:2923 [D loss: 0.536327, acc: 73.44%] [G loss: 2.946911]\n",
      "epoch:3 step:2924 [D loss: 0.610879, acc: 67.97%] [G loss: 3.244581]\n",
      "epoch:3 step:2925 [D loss: 0.601959, acc: 72.66%] [G loss: 2.637916]\n",
      "epoch:3 step:2926 [D loss: 0.568250, acc: 68.75%] [G loss: 2.583686]\n",
      "epoch:3 step:2927 [D loss: 0.672304, acc: 60.94%] [G loss: 2.975853]\n",
      "epoch:3 step:2928 [D loss: 0.559813, acc: 71.09%] [G loss: 2.962907]\n",
      "epoch:3 step:2929 [D loss: 0.538894, acc: 72.66%] [G loss: 3.366828]\n",
      "epoch:3 step:2930 [D loss: 0.544205, acc: 73.44%] [G loss: 3.226748]\n",
      "epoch:3 step:2931 [D loss: 0.643136, acc: 69.53%] [G loss: 2.838340]\n",
      "epoch:3 step:2932 [D loss: 0.594321, acc: 70.31%] [G loss: 2.814166]\n",
      "epoch:3 step:2933 [D loss: 0.584709, acc: 72.66%] [G loss: 3.218225]\n",
      "epoch:3 step:2934 [D loss: 0.632169, acc: 64.84%] [G loss: 3.078578]\n",
      "epoch:3 step:2935 [D loss: 0.603032, acc: 73.44%] [G loss: 2.897513]\n",
      "epoch:3 step:2936 [D loss: 0.614325, acc: 64.84%] [G loss: 2.641855]\n",
      "epoch:3 step:2937 [D loss: 0.585806, acc: 64.84%] [G loss: 2.984961]\n",
      "epoch:3 step:2938 [D loss: 0.521489, acc: 75.78%] [G loss: 2.945499]\n",
      "epoch:3 step:2939 [D loss: 0.482530, acc: 76.56%] [G loss: 3.062207]\n",
      "epoch:3 step:2940 [D loss: 0.637772, acc: 59.38%] [G loss: 2.713477]\n",
      "epoch:3 step:2941 [D loss: 0.602720, acc: 67.97%] [G loss: 2.784650]\n",
      "epoch:3 step:2942 [D loss: 0.488322, acc: 79.69%] [G loss: 3.206139]\n",
      "epoch:3 step:2943 [D loss: 0.518278, acc: 71.09%] [G loss: 2.901752]\n",
      "epoch:3 step:2944 [D loss: 0.604248, acc: 70.31%] [G loss: 2.680133]\n",
      "epoch:3 step:2945 [D loss: 0.615363, acc: 67.97%] [G loss: 3.276286]\n",
      "epoch:3 step:2946 [D loss: 0.618026, acc: 63.28%] [G loss: 3.100528]\n",
      "epoch:3 step:2947 [D loss: 0.590960, acc: 68.75%] [G loss: 2.727200]\n",
      "epoch:3 step:2948 [D loss: 0.595730, acc: 69.53%] [G loss: 2.821138]\n",
      "epoch:3 step:2949 [D loss: 0.547190, acc: 73.44%] [G loss: 3.224216]\n",
      "epoch:3 step:2950 [D loss: 0.596528, acc: 67.97%] [G loss: 2.814728]\n",
      "epoch:3 step:2951 [D loss: 0.664621, acc: 67.97%] [G loss: 2.939260]\n",
      "epoch:3 step:2952 [D loss: 0.440688, acc: 82.03%] [G loss: 2.858621]\n",
      "epoch:3 step:2953 [D loss: 0.500803, acc: 70.31%] [G loss: 3.158791]\n",
      "epoch:3 step:2954 [D loss: 0.500501, acc: 72.66%] [G loss: 3.037622]\n",
      "epoch:3 step:2955 [D loss: 0.506534, acc: 81.25%] [G loss: 3.454638]\n",
      "epoch:3 step:2956 [D loss: 0.532089, acc: 74.22%] [G loss: 3.169180]\n",
      "epoch:3 step:2957 [D loss: 0.483895, acc: 80.47%] [G loss: 3.238198]\n",
      "epoch:3 step:2958 [D loss: 0.586982, acc: 65.62%] [G loss: 3.043739]\n",
      "epoch:3 step:2959 [D loss: 0.545343, acc: 72.66%] [G loss: 2.527450]\n",
      "epoch:3 step:2960 [D loss: 0.493811, acc: 77.34%] [G loss: 3.346248]\n",
      "epoch:3 step:2961 [D loss: 0.556280, acc: 67.19%] [G loss: 3.089437]\n",
      "epoch:3 step:2962 [D loss: 0.533840, acc: 70.31%] [G loss: 3.461062]\n",
      "epoch:3 step:2963 [D loss: 0.466928, acc: 78.91%] [G loss: 3.588623]\n",
      "epoch:3 step:2964 [D loss: 0.574857, acc: 70.31%] [G loss: 2.842192]\n",
      "epoch:3 step:2965 [D loss: 0.485579, acc: 76.56%] [G loss: 3.383421]\n",
      "epoch:3 step:2966 [D loss: 0.489881, acc: 73.44%] [G loss: 3.551403]\n",
      "epoch:3 step:2967 [D loss: 0.554666, acc: 71.09%] [G loss: 3.297013]\n",
      "epoch:3 step:2968 [D loss: 0.515015, acc: 75.00%] [G loss: 3.183306]\n",
      "epoch:3 step:2969 [D loss: 0.542413, acc: 71.09%] [G loss: 3.243996]\n",
      "epoch:3 step:2970 [D loss: 0.539853, acc: 73.44%] [G loss: 3.304559]\n",
      "epoch:3 step:2971 [D loss: 0.641882, acc: 67.19%] [G loss: 2.742383]\n",
      "epoch:3 step:2972 [D loss: 0.558948, acc: 70.31%] [G loss: 3.126222]\n",
      "epoch:3 step:2973 [D loss: 0.545171, acc: 73.44%] [G loss: 3.314340]\n",
      "epoch:3 step:2974 [D loss: 0.524202, acc: 75.78%] [G loss: 3.042653]\n",
      "epoch:3 step:2975 [D loss: 0.545427, acc: 72.66%] [G loss: 2.957017]\n",
      "epoch:3 step:2976 [D loss: 0.611852, acc: 66.41%] [G loss: 2.889131]\n",
      "epoch:3 step:2977 [D loss: 0.556695, acc: 75.00%] [G loss: 2.978010]\n",
      "epoch:3 step:2978 [D loss: 0.511853, acc: 72.66%] [G loss: 2.988407]\n",
      "epoch:3 step:2979 [D loss: 0.465869, acc: 78.91%] [G loss: 2.897129]\n",
      "epoch:3 step:2980 [D loss: 0.551310, acc: 70.31%] [G loss: 2.767876]\n",
      "epoch:3 step:2981 [D loss: 0.528074, acc: 75.78%] [G loss: 3.079473]\n",
      "epoch:3 step:2982 [D loss: 0.577495, acc: 71.88%] [G loss: 3.287042]\n",
      "epoch:3 step:2983 [D loss: 0.451184, acc: 78.91%] [G loss: 3.141332]\n",
      "epoch:3 step:2984 [D loss: 0.586015, acc: 67.97%] [G loss: 3.040240]\n",
      "epoch:3 step:2985 [D loss: 0.430356, acc: 80.47%] [G loss: 3.250593]\n",
      "epoch:3 step:2986 [D loss: 0.506852, acc: 76.56%] [G loss: 2.956567]\n",
      "epoch:3 step:2987 [D loss: 0.436393, acc: 77.34%] [G loss: 3.402330]\n",
      "epoch:3 step:2988 [D loss: 0.634856, acc: 74.22%] [G loss: 3.662975]\n",
      "epoch:3 step:2989 [D loss: 0.477952, acc: 74.22%] [G loss: 3.139956]\n",
      "epoch:3 step:2990 [D loss: 0.517876, acc: 73.44%] [G loss: 3.415157]\n",
      "epoch:3 step:2991 [D loss: 0.546124, acc: 72.66%] [G loss: 2.864349]\n",
      "epoch:3 step:2992 [D loss: 0.586002, acc: 69.53%] [G loss: 3.130728]\n",
      "epoch:3 step:2993 [D loss: 0.634968, acc: 69.53%] [G loss: 2.798781]\n",
      "epoch:3 step:2994 [D loss: 0.574831, acc: 70.31%] [G loss: 3.388284]\n",
      "epoch:3 step:2995 [D loss: 0.616787, acc: 68.75%] [G loss: 3.077648]\n",
      "epoch:3 step:2996 [D loss: 0.578227, acc: 69.53%] [G loss: 2.929193]\n",
      "epoch:3 step:2997 [D loss: 0.515156, acc: 75.00%] [G loss: 3.026083]\n",
      "epoch:3 step:2998 [D loss: 0.631704, acc: 61.72%] [G loss: 2.738693]\n",
      "epoch:3 step:2999 [D loss: 0.531321, acc: 71.88%] [G loss: 3.057794]\n",
      "epoch:3 step:3000 [D loss: 0.464321, acc: 82.03%] [G loss: 3.062018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.942091\n",
      "FID: 64.076828\n",
      "0 = 13.648829196167025\n",
      "1 = 0.10030822934956105\n",
      "2 = 0.9513999819755554\n",
      "3 = 0.9405999779701233\n",
      "4 = 0.9621999859809875\n",
      "5 = 0.9613655209541321\n",
      "6 = 0.9405999779701233\n",
      "7 = 9.999131205511095\n",
      "8 = 0.15814584707716453\n",
      "9 = 0.8837000131607056\n",
      "10 = 0.8799999952316284\n",
      "11 = 0.8873999714851379\n",
      "12 = 0.8865605592727661\n",
      "13 = 0.8799999952316284\n",
      "14 = 4.942105770111084\n",
      "15 = 8.329146385192871\n",
      "16 = 0.30699074268341064\n",
      "17 = 4.94209098815918\n",
      "18 = 64.07682800292969\n",
      "epoch:3 step:3001 [D loss: 0.503022, acc: 76.56%] [G loss: 3.305350]\n",
      "epoch:3 step:3002 [D loss: 0.506679, acc: 80.47%] [G loss: 3.214539]\n",
      "epoch:3 step:3003 [D loss: 0.545528, acc: 75.00%] [G loss: 3.162266]\n",
      "epoch:3 step:3004 [D loss: 0.447165, acc: 75.00%] [G loss: 3.018538]\n",
      "epoch:3 step:3005 [D loss: 0.495576, acc: 75.00%] [G loss: 3.311808]\n",
      "epoch:3 step:3006 [D loss: 0.509203, acc: 70.31%] [G loss: 3.464097]\n",
      "epoch:3 step:3007 [D loss: 0.517064, acc: 73.44%] [G loss: 2.873448]\n",
      "epoch:3 step:3008 [D loss: 0.561069, acc: 73.44%] [G loss: 3.237216]\n",
      "epoch:3 step:3009 [D loss: 0.523257, acc: 76.56%] [G loss: 3.093895]\n",
      "epoch:3 step:3010 [D loss: 0.534618, acc: 69.53%] [G loss: 2.836246]\n",
      "epoch:3 step:3011 [D loss: 0.549416, acc: 76.56%] [G loss: 3.395555]\n",
      "epoch:3 step:3012 [D loss: 0.501988, acc: 73.44%] [G loss: 3.272244]\n",
      "epoch:3 step:3013 [D loss: 0.500367, acc: 76.56%] [G loss: 3.083651]\n",
      "epoch:3 step:3014 [D loss: 0.649901, acc: 64.06%] [G loss: 2.997513]\n",
      "epoch:3 step:3015 [D loss: 0.483117, acc: 74.22%] [G loss: 3.340279]\n",
      "epoch:3 step:3016 [D loss: 0.588429, acc: 68.75%] [G loss: 3.192439]\n",
      "epoch:3 step:3017 [D loss: 0.571158, acc: 73.44%] [G loss: 3.332401]\n",
      "epoch:3 step:3018 [D loss: 0.546075, acc: 75.00%] [G loss: 3.485458]\n",
      "epoch:3 step:3019 [D loss: 0.508291, acc: 73.44%] [G loss: 3.261932]\n",
      "epoch:3 step:3020 [D loss: 0.492113, acc: 78.91%] [G loss: 3.334577]\n",
      "epoch:3 step:3021 [D loss: 0.647162, acc: 65.62%] [G loss: 2.928262]\n",
      "epoch:3 step:3022 [D loss: 0.524131, acc: 73.44%] [G loss: 3.038265]\n",
      "epoch:3 step:3023 [D loss: 0.570332, acc: 75.00%] [G loss: 3.008683]\n",
      "epoch:3 step:3024 [D loss: 0.477482, acc: 74.22%] [G loss: 3.367069]\n",
      "epoch:3 step:3025 [D loss: 0.687619, acc: 61.72%] [G loss: 2.442044]\n",
      "epoch:3 step:3026 [D loss: 0.740676, acc: 64.06%] [G loss: 2.715382]\n",
      "epoch:3 step:3027 [D loss: 0.527799, acc: 74.22%] [G loss: 2.717145]\n",
      "epoch:3 step:3028 [D loss: 0.571782, acc: 67.19%] [G loss: 2.887682]\n",
      "epoch:3 step:3029 [D loss: 0.516381, acc: 75.00%] [G loss: 2.992759]\n",
      "epoch:3 step:3030 [D loss: 0.585204, acc: 71.09%] [G loss: 2.870809]\n",
      "epoch:3 step:3031 [D loss: 0.598238, acc: 63.28%] [G loss: 3.258756]\n",
      "epoch:3 step:3032 [D loss: 0.473375, acc: 75.78%] [G loss: 3.040761]\n",
      "epoch:3 step:3033 [D loss: 0.635282, acc: 64.06%] [G loss: 2.993194]\n",
      "epoch:3 step:3034 [D loss: 0.482466, acc: 75.00%] [G loss: 3.060773]\n",
      "epoch:3 step:3035 [D loss: 0.569099, acc: 71.09%] [G loss: 3.076182]\n",
      "epoch:3 step:3036 [D loss: 0.610750, acc: 69.53%] [G loss: 3.131118]\n",
      "epoch:3 step:3037 [D loss: 0.528710, acc: 76.56%] [G loss: 2.936243]\n",
      "epoch:3 step:3038 [D loss: 0.575506, acc: 71.88%] [G loss: 2.933736]\n",
      "epoch:3 step:3039 [D loss: 0.529437, acc: 75.00%] [G loss: 3.070713]\n",
      "epoch:3 step:3040 [D loss: 0.477575, acc: 72.66%] [G loss: 3.301797]\n",
      "epoch:3 step:3041 [D loss: 0.567179, acc: 67.97%] [G loss: 3.500716]\n",
      "epoch:3 step:3042 [D loss: 0.476485, acc: 77.34%] [G loss: 3.705931]\n",
      "epoch:3 step:3043 [D loss: 0.496528, acc: 76.56%] [G loss: 3.595094]\n",
      "epoch:3 step:3044 [D loss: 0.517856, acc: 76.56%] [G loss: 3.187803]\n",
      "epoch:3 step:3045 [D loss: 0.569521, acc: 67.97%] [G loss: 2.951390]\n",
      "epoch:3 step:3046 [D loss: 0.402822, acc: 85.16%] [G loss: 3.259177]\n",
      "epoch:3 step:3047 [D loss: 0.561318, acc: 68.75%] [G loss: 3.001316]\n",
      "epoch:3 step:3048 [D loss: 0.653042, acc: 63.28%] [G loss: 3.070650]\n",
      "epoch:3 step:3049 [D loss: 0.557406, acc: 74.22%] [G loss: 2.961002]\n",
      "epoch:3 step:3050 [D loss: 0.558692, acc: 70.31%] [G loss: 2.847036]\n",
      "epoch:3 step:3051 [D loss: 0.416825, acc: 83.59%] [G loss: 3.107110]\n",
      "epoch:3 step:3052 [D loss: 0.567503, acc: 65.62%] [G loss: 3.048767]\n",
      "epoch:3 step:3053 [D loss: 0.520295, acc: 77.34%] [G loss: 3.055301]\n",
      "epoch:3 step:3054 [D loss: 0.616796, acc: 70.31%] [G loss: 3.084134]\n",
      "epoch:3 step:3055 [D loss: 0.618489, acc: 66.41%] [G loss: 3.040113]\n",
      "epoch:3 step:3056 [D loss: 0.552990, acc: 71.88%] [G loss: 2.978112]\n",
      "epoch:3 step:3057 [D loss: 0.596377, acc: 67.97%] [G loss: 2.664747]\n",
      "epoch:3 step:3058 [D loss: 0.616011, acc: 63.28%] [G loss: 2.507758]\n",
      "epoch:3 step:3059 [D loss: 0.504403, acc: 75.78%] [G loss: 2.940619]\n",
      "epoch:3 step:3060 [D loss: 0.540409, acc: 72.66%] [G loss: 2.706242]\n",
      "epoch:3 step:3061 [D loss: 0.576397, acc: 68.75%] [G loss: 2.822979]\n",
      "epoch:3 step:3062 [D loss: 0.649328, acc: 67.19%] [G loss: 2.847134]\n",
      "epoch:3 step:3063 [D loss: 0.649419, acc: 70.31%] [G loss: 2.823487]\n",
      "epoch:3 step:3064 [D loss: 0.558532, acc: 74.22%] [G loss: 2.797909]\n",
      "epoch:3 step:3065 [D loss: 0.488535, acc: 78.91%] [G loss: 3.048168]\n",
      "epoch:3 step:3066 [D loss: 0.481434, acc: 75.00%] [G loss: 2.775689]\n",
      "epoch:3 step:3067 [D loss: 0.499075, acc: 79.69%] [G loss: 3.180698]\n",
      "epoch:3 step:3068 [D loss: 0.529069, acc: 71.88%] [G loss: 3.111425]\n",
      "epoch:3 step:3069 [D loss: 0.481980, acc: 75.78%] [G loss: 3.583769]\n",
      "epoch:3 step:3070 [D loss: 0.594184, acc: 73.44%] [G loss: 4.003391]\n",
      "epoch:3 step:3071 [D loss: 0.495453, acc: 81.25%] [G loss: 3.102760]\n",
      "epoch:3 step:3072 [D loss: 0.557380, acc: 74.22%] [G loss: 3.806043]\n",
      "epoch:3 step:3073 [D loss: 0.580325, acc: 67.19%] [G loss: 3.324163]\n",
      "epoch:3 step:3074 [D loss: 0.665116, acc: 61.72%] [G loss: 2.635942]\n",
      "epoch:3 step:3075 [D loss: 0.527117, acc: 75.78%] [G loss: 3.038349]\n",
      "epoch:3 step:3076 [D loss: 0.545452, acc: 75.78%] [G loss: 3.238386]\n",
      "epoch:3 step:3077 [D loss: 0.556659, acc: 68.75%] [G loss: 2.829783]\n",
      "epoch:3 step:3078 [D loss: 0.564813, acc: 67.97%] [G loss: 3.012527]\n",
      "epoch:3 step:3079 [D loss: 0.542562, acc: 72.66%] [G loss: 2.795652]\n",
      "epoch:3 step:3080 [D loss: 0.573425, acc: 71.09%] [G loss: 2.731679]\n",
      "epoch:3 step:3081 [D loss: 0.686571, acc: 62.50%] [G loss: 2.832267]\n",
      "epoch:3 step:3082 [D loss: 0.454087, acc: 79.69%] [G loss: 3.011845]\n",
      "epoch:3 step:3083 [D loss: 0.553621, acc: 71.09%] [G loss: 2.665930]\n",
      "epoch:3 step:3084 [D loss: 0.519708, acc: 78.12%] [G loss: 2.782787]\n",
      "epoch:3 step:3085 [D loss: 0.606387, acc: 71.88%] [G loss: 2.705153]\n",
      "epoch:3 step:3086 [D loss: 0.627297, acc: 66.41%] [G loss: 2.904037]\n",
      "epoch:3 step:3087 [D loss: 0.692286, acc: 63.28%] [G loss: 2.671404]\n",
      "epoch:3 step:3088 [D loss: 0.632682, acc: 69.53%] [G loss: 2.774609]\n",
      "epoch:3 step:3089 [D loss: 0.505655, acc: 75.78%] [G loss: 3.129459]\n",
      "epoch:3 step:3090 [D loss: 0.491868, acc: 75.00%] [G loss: 3.451605]\n",
      "epoch:3 step:3091 [D loss: 0.515892, acc: 72.66%] [G loss: 3.394032]\n",
      "epoch:3 step:3092 [D loss: 0.543767, acc: 73.44%] [G loss: 3.035196]\n",
      "epoch:3 step:3093 [D loss: 0.501181, acc: 77.34%] [G loss: 3.029049]\n",
      "epoch:3 step:3094 [D loss: 0.471116, acc: 76.56%] [G loss: 3.206433]\n",
      "epoch:3 step:3095 [D loss: 0.485894, acc: 76.56%] [G loss: 3.224943]\n",
      "epoch:3 step:3096 [D loss: 0.489643, acc: 79.69%] [G loss: 3.279745]\n",
      "epoch:3 step:3097 [D loss: 0.434521, acc: 80.47%] [G loss: 3.489363]\n",
      "epoch:3 step:3098 [D loss: 0.637657, acc: 67.19%] [G loss: 3.143005]\n",
      "epoch:3 step:3099 [D loss: 0.742561, acc: 53.12%] [G loss: 2.991020]\n",
      "epoch:3 step:3100 [D loss: 0.576771, acc: 70.31%] [G loss: 3.014400]\n",
      "epoch:3 step:3101 [D loss: 0.552868, acc: 75.78%] [G loss: 3.195107]\n",
      "epoch:3 step:3102 [D loss: 0.512590, acc: 76.56%] [G loss: 2.918882]\n",
      "epoch:3 step:3103 [D loss: 0.607119, acc: 67.97%] [G loss: 3.007845]\n",
      "epoch:3 step:3104 [D loss: 0.511290, acc: 74.22%] [G loss: 3.123946]\n",
      "epoch:3 step:3105 [D loss: 0.521911, acc: 76.56%] [G loss: 2.992018]\n",
      "epoch:3 step:3106 [D loss: 0.549839, acc: 71.88%] [G loss: 3.296436]\n",
      "epoch:3 step:3107 [D loss: 0.634325, acc: 68.75%] [G loss: 2.902683]\n",
      "epoch:3 step:3108 [D loss: 0.547970, acc: 72.66%] [G loss: 2.802701]\n",
      "epoch:3 step:3109 [D loss: 0.519212, acc: 72.66%] [G loss: 3.358819]\n",
      "epoch:3 step:3110 [D loss: 0.541858, acc: 68.75%] [G loss: 2.993203]\n",
      "epoch:3 step:3111 [D loss: 0.522513, acc: 72.66%] [G loss: 3.495233]\n",
      "epoch:3 step:3112 [D loss: 0.486967, acc: 75.00%] [G loss: 2.618496]\n",
      "epoch:3 step:3113 [D loss: 0.549175, acc: 69.53%] [G loss: 3.101056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3114 [D loss: 0.593105, acc: 67.19%] [G loss: 3.000912]\n",
      "epoch:3 step:3115 [D loss: 0.499906, acc: 78.91%] [G loss: 3.584453]\n",
      "epoch:3 step:3116 [D loss: 0.528331, acc: 71.88%] [G loss: 3.187305]\n",
      "epoch:3 step:3117 [D loss: 0.601136, acc: 68.75%] [G loss: 3.131760]\n",
      "epoch:3 step:3118 [D loss: 0.509836, acc: 72.66%] [G loss: 3.481136]\n",
      "epoch:3 step:3119 [D loss: 0.510881, acc: 72.66%] [G loss: 3.278248]\n",
      "epoch:3 step:3120 [D loss: 0.465797, acc: 79.69%] [G loss: 3.351622]\n",
      "epoch:3 step:3121 [D loss: 0.521303, acc: 71.88%] [G loss: 3.220172]\n",
      "epoch:3 step:3122 [D loss: 0.524451, acc: 75.00%] [G loss: 3.518307]\n",
      "epoch:3 step:3123 [D loss: 0.477284, acc: 71.88%] [G loss: 3.907189]\n",
      "epoch:3 step:3124 [D loss: 0.442745, acc: 79.69%] [G loss: 3.654965]\n",
      "epoch:3 step:3125 [D loss: 0.396840, acc: 84.38%] [G loss: 4.266102]\n",
      "epoch:3 step:3126 [D loss: 0.407258, acc: 81.25%] [G loss: 3.441717]\n",
      "epoch:3 step:3127 [D loss: 0.597877, acc: 72.66%] [G loss: 3.162219]\n",
      "epoch:3 step:3128 [D loss: 0.634102, acc: 64.84%] [G loss: 3.005677]\n",
      "epoch:3 step:3129 [D loss: 0.486769, acc: 76.56%] [G loss: 2.897937]\n",
      "epoch:3 step:3130 [D loss: 0.543414, acc: 75.78%] [G loss: 3.038996]\n",
      "epoch:3 step:3131 [D loss: 0.439815, acc: 82.81%] [G loss: 3.385187]\n",
      "epoch:3 step:3132 [D loss: 0.616764, acc: 64.06%] [G loss: 3.253195]\n",
      "epoch:3 step:3133 [D loss: 0.521559, acc: 77.34%] [G loss: 3.312605]\n",
      "epoch:3 step:3134 [D loss: 0.597522, acc: 69.53%] [G loss: 3.069129]\n",
      "epoch:3 step:3135 [D loss: 0.511874, acc: 73.44%] [G loss: 3.025145]\n",
      "epoch:3 step:3136 [D loss: 0.584607, acc: 72.66%] [G loss: 3.135063]\n",
      "epoch:3 step:3137 [D loss: 0.558223, acc: 67.97%] [G loss: 2.903517]\n",
      "epoch:3 step:3138 [D loss: 0.537897, acc: 74.22%] [G loss: 2.891416]\n",
      "epoch:3 step:3139 [D loss: 0.617708, acc: 67.97%] [G loss: 3.048295]\n",
      "epoch:3 step:3140 [D loss: 0.558955, acc: 74.22%] [G loss: 3.020388]\n",
      "epoch:3 step:3141 [D loss: 0.473211, acc: 77.34%] [G loss: 3.245034]\n",
      "epoch:3 step:3142 [D loss: 0.516124, acc: 75.00%] [G loss: 3.199920]\n",
      "epoch:3 step:3143 [D loss: 0.533772, acc: 75.78%] [G loss: 3.257205]\n",
      "epoch:3 step:3144 [D loss: 0.508376, acc: 72.66%] [G loss: 3.505107]\n",
      "epoch:3 step:3145 [D loss: 0.496024, acc: 78.91%] [G loss: 3.316395]\n",
      "epoch:3 step:3146 [D loss: 0.546461, acc: 71.88%] [G loss: 4.008946]\n",
      "epoch:3 step:3147 [D loss: 0.548149, acc: 70.31%] [G loss: 3.166349]\n",
      "epoch:3 step:3148 [D loss: 0.491912, acc: 78.91%] [G loss: 3.378972]\n",
      "epoch:3 step:3149 [D loss: 0.577329, acc: 69.53%] [G loss: 3.394787]\n",
      "epoch:3 step:3150 [D loss: 0.534193, acc: 72.66%] [G loss: 3.124976]\n",
      "epoch:3 step:3151 [D loss: 0.600726, acc: 67.19%] [G loss: 3.391672]\n",
      "epoch:3 step:3152 [D loss: 0.705759, acc: 58.59%] [G loss: 2.752255]\n",
      "epoch:3 step:3153 [D loss: 0.665631, acc: 65.62%] [G loss: 2.873689]\n",
      "epoch:3 step:3154 [D loss: 0.518067, acc: 78.12%] [G loss: 3.479089]\n",
      "epoch:3 step:3155 [D loss: 0.461961, acc: 78.12%] [G loss: 3.338137]\n",
      "epoch:3 step:3156 [D loss: 0.572409, acc: 71.09%] [G loss: 3.206762]\n",
      "epoch:3 step:3157 [D loss: 0.451499, acc: 78.12%] [G loss: 4.054188]\n",
      "epoch:3 step:3158 [D loss: 0.525124, acc: 70.31%] [G loss: 3.536026]\n",
      "epoch:3 step:3159 [D loss: 0.608787, acc: 67.19%] [G loss: 3.409365]\n",
      "epoch:3 step:3160 [D loss: 0.680924, acc: 64.06%] [G loss: 2.477599]\n",
      "epoch:3 step:3161 [D loss: 0.532037, acc: 71.09%] [G loss: 3.090904]\n",
      "epoch:3 step:3162 [D loss: 0.481964, acc: 80.47%] [G loss: 2.916042]\n",
      "epoch:3 step:3163 [D loss: 0.642116, acc: 60.16%] [G loss: 2.924213]\n",
      "epoch:3 step:3164 [D loss: 0.570398, acc: 72.66%] [G loss: 2.953032]\n",
      "epoch:3 step:3165 [D loss: 0.521898, acc: 75.00%] [G loss: 3.094439]\n",
      "epoch:3 step:3166 [D loss: 0.526328, acc: 78.12%] [G loss: 3.138626]\n",
      "epoch:3 step:3167 [D loss: 0.517404, acc: 75.00%] [G loss: 2.826087]\n",
      "epoch:3 step:3168 [D loss: 0.669278, acc: 69.53%] [G loss: 3.116390]\n",
      "epoch:3 step:3169 [D loss: 0.539069, acc: 71.88%] [G loss: 2.901690]\n",
      "epoch:3 step:3170 [D loss: 0.448986, acc: 82.03%] [G loss: 3.170447]\n",
      "epoch:3 step:3171 [D loss: 0.570578, acc: 67.19%] [G loss: 3.019376]\n",
      "epoch:3 step:3172 [D loss: 0.530475, acc: 73.44%] [G loss: 2.933125]\n",
      "epoch:3 step:3173 [D loss: 0.601385, acc: 64.84%] [G loss: 3.016596]\n",
      "epoch:3 step:3174 [D loss: 0.466887, acc: 79.69%] [G loss: 2.913492]\n",
      "epoch:3 step:3175 [D loss: 0.577275, acc: 75.00%] [G loss: 3.084089]\n",
      "epoch:3 step:3176 [D loss: 0.562333, acc: 72.66%] [G loss: 3.177110]\n",
      "epoch:3 step:3177 [D loss: 0.471763, acc: 79.69%] [G loss: 3.558734]\n",
      "epoch:3 step:3178 [D loss: 0.525085, acc: 71.88%] [G loss: 3.373287]\n",
      "epoch:3 step:3179 [D loss: 0.463094, acc: 78.91%] [G loss: 3.488790]\n",
      "epoch:3 step:3180 [D loss: 0.583805, acc: 67.97%] [G loss: 3.115216]\n",
      "epoch:3 step:3181 [D loss: 0.666270, acc: 63.28%] [G loss: 3.020230]\n",
      "epoch:3 step:3182 [D loss: 0.561308, acc: 74.22%] [G loss: 3.216912]\n",
      "epoch:3 step:3183 [D loss: 0.567079, acc: 71.09%] [G loss: 3.173811]\n",
      "epoch:3 step:3184 [D loss: 0.583086, acc: 70.31%] [G loss: 3.066529]\n",
      "epoch:3 step:3185 [D loss: 0.565244, acc: 66.41%] [G loss: 3.458848]\n",
      "epoch:3 step:3186 [D loss: 0.655563, acc: 65.62%] [G loss: 2.786182]\n",
      "epoch:3 step:3187 [D loss: 0.690665, acc: 65.62%] [G loss: 2.727196]\n",
      "epoch:3 step:3188 [D loss: 0.667494, acc: 65.62%] [G loss: 2.825616]\n",
      "epoch:3 step:3189 [D loss: 0.626452, acc: 67.19%] [G loss: 2.722419]\n",
      "epoch:3 step:3190 [D loss: 0.561021, acc: 66.41%] [G loss: 2.812066]\n",
      "epoch:3 step:3191 [D loss: 0.569931, acc: 70.31%] [G loss: 2.942080]\n",
      "epoch:3 step:3192 [D loss: 0.457394, acc: 79.69%] [G loss: 3.748538]\n",
      "epoch:3 step:3193 [D loss: 0.556626, acc: 76.56%] [G loss: 2.920804]\n",
      "epoch:3 step:3194 [D loss: 0.599104, acc: 69.53%] [G loss: 3.139058]\n",
      "epoch:3 step:3195 [D loss: 0.598113, acc: 66.41%] [G loss: 2.719507]\n",
      "epoch:3 step:3196 [D loss: 0.590797, acc: 65.62%] [G loss: 2.992775]\n",
      "epoch:3 step:3197 [D loss: 0.606847, acc: 67.19%] [G loss: 2.822386]\n",
      "epoch:3 step:3198 [D loss: 0.548509, acc: 73.44%] [G loss: 3.049998]\n",
      "epoch:3 step:3199 [D loss: 0.520738, acc: 70.31%] [G loss: 2.868732]\n",
      "epoch:3 step:3200 [D loss: 0.567326, acc: 72.66%] [G loss: 2.923210]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.749820\n",
      "FID: 68.461586\n",
      "0 = 13.508087466907527\n",
      "1 = 0.08613286786066815\n",
      "2 = 0.9501000046730042\n",
      "3 = 0.9422000050544739\n",
      "4 = 0.9580000042915344\n",
      "5 = 0.9573257565498352\n",
      "6 = 0.9422000050544739\n",
      "7 = 10.185401658844977\n",
      "8 = 0.164813186198525\n",
      "9 = 0.8776000142097473\n",
      "10 = 0.8755999803543091\n",
      "11 = 0.8795999884605408\n",
      "12 = 0.8791164755821228\n",
      "13 = 0.8755999803543091\n",
      "14 = 4.749831676483154\n",
      "15 = 8.055481910705566\n",
      "16 = 0.3388120234012604\n",
      "17 = 4.749819755554199\n",
      "18 = 68.46158599853516\n",
      "epoch:3 step:3201 [D loss: 0.567892, acc: 67.97%] [G loss: 3.391781]\n",
      "epoch:3 step:3202 [D loss: 0.606644, acc: 65.62%] [G loss: 3.067433]\n",
      "epoch:3 step:3203 [D loss: 0.479747, acc: 79.69%] [G loss: 3.166049]\n",
      "epoch:3 step:3204 [D loss: 0.515599, acc: 75.00%] [G loss: 2.833249]\n",
      "epoch:3 step:3205 [D loss: 0.562104, acc: 72.66%] [G loss: 2.937357]\n",
      "epoch:3 step:3206 [D loss: 0.498688, acc: 74.22%] [G loss: 3.234539]\n",
      "epoch:3 step:3207 [D loss: 0.624382, acc: 66.41%] [G loss: 2.972548]\n",
      "epoch:3 step:3208 [D loss: 0.572391, acc: 75.78%] [G loss: 3.091492]\n",
      "epoch:3 step:3209 [D loss: 0.469000, acc: 75.00%] [G loss: 3.604611]\n",
      "epoch:3 step:3210 [D loss: 0.479857, acc: 76.56%] [G loss: 3.890326]\n",
      "epoch:3 step:3211 [D loss: 0.614937, acc: 68.75%] [G loss: 3.304011]\n",
      "epoch:3 step:3212 [D loss: 0.538382, acc: 75.00%] [G loss: 3.354636]\n",
      "epoch:3 step:3213 [D loss: 0.516418, acc: 77.34%] [G loss: 3.158551]\n",
      "epoch:3 step:3214 [D loss: 0.531095, acc: 71.88%] [G loss: 3.099325]\n",
      "epoch:3 step:3215 [D loss: 0.540225, acc: 77.34%] [G loss: 3.073400]\n",
      "epoch:3 step:3216 [D loss: 0.545133, acc: 71.88%] [G loss: 3.183439]\n",
      "epoch:3 step:3217 [D loss: 0.548646, acc: 74.22%] [G loss: 3.737049]\n",
      "epoch:3 step:3218 [D loss: 0.590150, acc: 64.84%] [G loss: 3.204140]\n",
      "epoch:3 step:3219 [D loss: 0.517036, acc: 73.44%] [G loss: 3.324743]\n",
      "epoch:3 step:3220 [D loss: 0.557125, acc: 70.31%] [G loss: 3.279950]\n",
      "epoch:3 step:3221 [D loss: 0.732620, acc: 60.16%] [G loss: 2.972755]\n",
      "epoch:3 step:3222 [D loss: 0.596879, acc: 71.09%] [G loss: 2.945592]\n",
      "epoch:3 step:3223 [D loss: 0.620664, acc: 66.41%] [G loss: 2.680655]\n",
      "epoch:3 step:3224 [D loss: 0.529958, acc: 74.22%] [G loss: 3.133964]\n",
      "epoch:3 step:3225 [D loss: 0.568868, acc: 75.00%] [G loss: 3.341960]\n",
      "epoch:3 step:3226 [D loss: 0.607886, acc: 69.53%] [G loss: 3.009261]\n",
      "epoch:3 step:3227 [D loss: 0.615591, acc: 64.06%] [G loss: 3.004293]\n",
      "epoch:3 step:3228 [D loss: 0.534753, acc: 71.88%] [G loss: 2.817226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3229 [D loss: 0.509236, acc: 75.00%] [G loss: 3.101589]\n",
      "epoch:3 step:3230 [D loss: 0.561078, acc: 68.75%] [G loss: 3.126696]\n",
      "epoch:3 step:3231 [D loss: 0.563029, acc: 70.31%] [G loss: 3.115396]\n",
      "epoch:3 step:3232 [D loss: 0.587122, acc: 69.53%] [G loss: 2.835227]\n",
      "epoch:3 step:3233 [D loss: 0.461694, acc: 74.22%] [G loss: 3.210532]\n",
      "epoch:3 step:3234 [D loss: 0.565885, acc: 72.66%] [G loss: 3.311589]\n",
      "epoch:3 step:3235 [D loss: 0.574354, acc: 69.53%] [G loss: 3.267916]\n",
      "epoch:3 step:3236 [D loss: 0.571310, acc: 71.09%] [G loss: 3.310981]\n",
      "epoch:3 step:3237 [D loss: 0.617820, acc: 67.97%] [G loss: 3.483075]\n",
      "epoch:3 step:3238 [D loss: 0.468391, acc: 78.91%] [G loss: 3.651835]\n",
      "epoch:3 step:3239 [D loss: 0.467014, acc: 80.47%] [G loss: 3.639569]\n",
      "epoch:3 step:3240 [D loss: 0.459123, acc: 78.91%] [G loss: 3.491241]\n",
      "epoch:3 step:3241 [D loss: 0.543609, acc: 73.44%] [G loss: 3.616227]\n",
      "epoch:3 step:3242 [D loss: 0.611170, acc: 72.66%] [G loss: 3.251051]\n",
      "epoch:3 step:3243 [D loss: 0.592486, acc: 67.97%] [G loss: 2.964103]\n",
      "epoch:3 step:3244 [D loss: 0.516002, acc: 72.66%] [G loss: 2.924636]\n",
      "epoch:3 step:3245 [D loss: 0.524583, acc: 74.22%] [G loss: 2.985424]\n",
      "epoch:3 step:3246 [D loss: 0.640706, acc: 64.06%] [G loss: 2.807994]\n",
      "epoch:3 step:3247 [D loss: 0.576033, acc: 71.09%] [G loss: 2.754742]\n",
      "epoch:3 step:3248 [D loss: 0.544401, acc: 70.31%] [G loss: 2.988362]\n",
      "epoch:3 step:3249 [D loss: 0.470454, acc: 81.25%] [G loss: 2.969718]\n",
      "epoch:3 step:3250 [D loss: 0.595767, acc: 67.97%] [G loss: 3.192441]\n",
      "epoch:3 step:3251 [D loss: 0.504983, acc: 77.34%] [G loss: 3.572588]\n",
      "epoch:3 step:3252 [D loss: 0.640997, acc: 66.41%] [G loss: 3.017175]\n",
      "epoch:3 step:3253 [D loss: 0.562443, acc: 74.22%] [G loss: 2.896696]\n",
      "epoch:3 step:3254 [D loss: 0.615640, acc: 61.72%] [G loss: 2.954906]\n",
      "epoch:3 step:3255 [D loss: 0.625613, acc: 67.97%] [G loss: 3.061838]\n",
      "epoch:3 step:3256 [D loss: 0.561416, acc: 73.44%] [G loss: 3.023658]\n",
      "epoch:3 step:3257 [D loss: 0.490229, acc: 76.56%] [G loss: 3.380761]\n",
      "epoch:3 step:3258 [D loss: 0.487795, acc: 77.34%] [G loss: 3.386699]\n",
      "epoch:3 step:3259 [D loss: 0.589145, acc: 71.09%] [G loss: 2.841309]\n",
      "epoch:3 step:3260 [D loss: 0.653242, acc: 67.97%] [G loss: 2.613112]\n",
      "epoch:3 step:3261 [D loss: 0.597321, acc: 67.97%] [G loss: 2.891542]\n",
      "epoch:3 step:3262 [D loss: 0.520841, acc: 75.78%] [G loss: 3.310844]\n",
      "epoch:3 step:3263 [D loss: 0.554347, acc: 71.88%] [G loss: 3.152779]\n",
      "epoch:3 step:3264 [D loss: 0.556312, acc: 71.09%] [G loss: 2.927492]\n",
      "epoch:3 step:3265 [D loss: 0.539886, acc: 72.66%] [G loss: 2.692464]\n",
      "epoch:3 step:3266 [D loss: 0.586220, acc: 71.88%] [G loss: 3.400224]\n",
      "epoch:3 step:3267 [D loss: 0.659166, acc: 63.28%] [G loss: 2.574923]\n",
      "epoch:3 step:3268 [D loss: 0.595096, acc: 70.31%] [G loss: 2.563496]\n",
      "epoch:3 step:3269 [D loss: 0.563991, acc: 68.75%] [G loss: 3.175338]\n",
      "epoch:3 step:3270 [D loss: 0.531872, acc: 75.00%] [G loss: 2.736244]\n",
      "epoch:3 step:3271 [D loss: 0.609169, acc: 64.84%] [G loss: 2.825258]\n",
      "epoch:3 step:3272 [D loss: 0.583468, acc: 68.75%] [G loss: 2.974852]\n",
      "epoch:3 step:3273 [D loss: 0.516657, acc: 71.88%] [G loss: 2.947632]\n",
      "epoch:3 step:3274 [D loss: 0.588046, acc: 71.09%] [G loss: 2.783240]\n",
      "epoch:3 step:3275 [D loss: 0.508226, acc: 74.22%] [G loss: 2.974662]\n",
      "epoch:3 step:3276 [D loss: 0.615056, acc: 67.19%] [G loss: 2.523844]\n",
      "epoch:3 step:3277 [D loss: 0.553535, acc: 72.66%] [G loss: 2.819694]\n",
      "epoch:3 step:3278 [D loss: 0.554194, acc: 71.88%] [G loss: 2.865891]\n",
      "epoch:3 step:3279 [D loss: 0.550191, acc: 69.53%] [G loss: 2.821805]\n",
      "epoch:3 step:3280 [D loss: 0.590447, acc: 67.97%] [G loss: 2.955804]\n",
      "epoch:3 step:3281 [D loss: 0.519599, acc: 76.56%] [G loss: 2.965751]\n",
      "epoch:3 step:3282 [D loss: 0.519532, acc: 74.22%] [G loss: 3.302667]\n",
      "epoch:3 step:3283 [D loss: 0.486679, acc: 79.69%] [G loss: 3.501883]\n",
      "epoch:3 step:3284 [D loss: 0.618201, acc: 68.75%] [G loss: 3.143782]\n",
      "epoch:3 step:3285 [D loss: 0.577854, acc: 71.09%] [G loss: 3.205029]\n",
      "epoch:3 step:3286 [D loss: 0.485056, acc: 77.34%] [G loss: 3.163998]\n",
      "epoch:3 step:3287 [D loss: 0.636016, acc: 66.41%] [G loss: 3.099862]\n",
      "epoch:3 step:3288 [D loss: 0.630592, acc: 59.38%] [G loss: 2.413044]\n",
      "epoch:3 step:3289 [D loss: 0.633924, acc: 67.97%] [G loss: 2.625829]\n",
      "epoch:3 step:3290 [D loss: 0.523175, acc: 74.22%] [G loss: 2.772284]\n",
      "epoch:3 step:3291 [D loss: 0.573468, acc: 70.31%] [G loss: 2.618344]\n",
      "epoch:3 step:3292 [D loss: 0.566801, acc: 72.66%] [G loss: 2.922807]\n",
      "epoch:3 step:3293 [D loss: 0.570210, acc: 71.09%] [G loss: 2.798095]\n",
      "epoch:3 step:3294 [D loss: 0.590678, acc: 68.75%] [G loss: 2.889748]\n",
      "epoch:3 step:3295 [D loss: 0.500385, acc: 78.12%] [G loss: 3.227070]\n",
      "epoch:3 step:3296 [D loss: 0.562051, acc: 74.22%] [G loss: 2.949937]\n",
      "epoch:3 step:3297 [D loss: 0.644772, acc: 67.19%] [G loss: 2.893208]\n",
      "epoch:3 step:3298 [D loss: 0.498253, acc: 74.22%] [G loss: 3.239442]\n",
      "epoch:3 step:3299 [D loss: 0.566477, acc: 71.09%] [G loss: 2.772920]\n",
      "epoch:3 step:3300 [D loss: 0.674576, acc: 56.25%] [G loss: 2.635883]\n",
      "epoch:3 step:3301 [D loss: 0.595409, acc: 65.62%] [G loss: 2.822556]\n",
      "epoch:3 step:3302 [D loss: 0.568124, acc: 71.88%] [G loss: 2.659451]\n",
      "epoch:3 step:3303 [D loss: 0.557529, acc: 72.66%] [G loss: 2.918042]\n",
      "epoch:3 step:3304 [D loss: 0.601024, acc: 67.19%] [G loss: 2.716003]\n",
      "epoch:3 step:3305 [D loss: 0.489913, acc: 78.91%] [G loss: 2.819454]\n",
      "epoch:3 step:3306 [D loss: 0.598638, acc: 70.31%] [G loss: 2.946463]\n",
      "epoch:3 step:3307 [D loss: 0.554720, acc: 71.09%] [G loss: 2.901464]\n",
      "epoch:3 step:3308 [D loss: 0.455344, acc: 78.91%] [G loss: 3.570865]\n",
      "epoch:3 step:3309 [D loss: 0.430642, acc: 80.47%] [G loss: 3.884675]\n",
      "epoch:3 step:3310 [D loss: 0.481011, acc: 78.91%] [G loss: 4.171462]\n",
      "epoch:3 step:3311 [D loss: 0.739353, acc: 60.94%] [G loss: 2.685055]\n",
      "epoch:3 step:3312 [D loss: 0.629824, acc: 64.06%] [G loss: 2.819712]\n",
      "epoch:3 step:3313 [D loss: 0.583636, acc: 67.97%] [G loss: 2.581987]\n",
      "epoch:3 step:3314 [D loss: 0.511768, acc: 75.78%] [G loss: 2.919386]\n",
      "epoch:3 step:3315 [D loss: 0.525454, acc: 71.09%] [G loss: 3.096658]\n",
      "epoch:3 step:3316 [D loss: 0.608216, acc: 69.53%] [G loss: 2.673801]\n",
      "epoch:3 step:3317 [D loss: 0.553003, acc: 72.66%] [G loss: 2.928006]\n",
      "epoch:3 step:3318 [D loss: 0.666180, acc: 67.97%] [G loss: 2.680229]\n",
      "epoch:3 step:3319 [D loss: 0.515821, acc: 77.34%] [G loss: 3.006530]\n",
      "epoch:3 step:3320 [D loss: 0.632094, acc: 69.53%] [G loss: 3.294877]\n",
      "epoch:3 step:3321 [D loss: 0.568102, acc: 71.88%] [G loss: 2.682386]\n",
      "epoch:3 step:3322 [D loss: 0.576737, acc: 68.75%] [G loss: 3.109430]\n",
      "epoch:3 step:3323 [D loss: 0.505946, acc: 73.44%] [G loss: 3.200547]\n",
      "epoch:3 step:3324 [D loss: 0.466284, acc: 82.81%] [G loss: 3.348070]\n",
      "epoch:3 step:3325 [D loss: 0.502969, acc: 75.00%] [G loss: 3.763206]\n",
      "epoch:3 step:3326 [D loss: 0.514898, acc: 74.22%] [G loss: 3.115032]\n",
      "epoch:3 step:3327 [D loss: 0.465670, acc: 76.56%] [G loss: 3.161526]\n",
      "epoch:3 step:3328 [D loss: 0.702989, acc: 57.03%] [G loss: 2.930471]\n",
      "epoch:3 step:3329 [D loss: 0.525446, acc: 78.12%] [G loss: 3.167521]\n",
      "epoch:3 step:3330 [D loss: 0.520422, acc: 72.66%] [G loss: 3.425763]\n",
      "epoch:3 step:3331 [D loss: 0.491506, acc: 75.00%] [G loss: 2.721486]\n",
      "epoch:3 step:3332 [D loss: 0.565614, acc: 69.53%] [G loss: 2.825628]\n",
      "epoch:3 step:3333 [D loss: 0.578202, acc: 71.09%] [G loss: 3.031827]\n",
      "epoch:3 step:3334 [D loss: 0.499794, acc: 74.22%] [G loss: 3.608500]\n",
      "epoch:3 step:3335 [D loss: 0.555480, acc: 70.31%] [G loss: 3.065108]\n",
      "epoch:3 step:3336 [D loss: 0.562681, acc: 72.66%] [G loss: 3.128704]\n",
      "epoch:3 step:3337 [D loss: 0.547874, acc: 71.88%] [G loss: 3.091846]\n",
      "epoch:3 step:3338 [D loss: 0.512253, acc: 75.78%] [G loss: 3.079601]\n",
      "epoch:3 step:3339 [D loss: 0.570357, acc: 71.88%] [G loss: 3.076042]\n",
      "epoch:3 step:3340 [D loss: 0.658208, acc: 65.62%] [G loss: 2.888956]\n",
      "epoch:3 step:3341 [D loss: 0.550936, acc: 72.66%] [G loss: 2.781406]\n",
      "epoch:3 step:3342 [D loss: 0.711821, acc: 64.84%] [G loss: 2.568284]\n",
      "epoch:3 step:3343 [D loss: 0.626343, acc: 69.53%] [G loss: 3.046436]\n",
      "epoch:3 step:3344 [D loss: 0.592035, acc: 67.97%] [G loss: 3.008104]\n",
      "epoch:3 step:3345 [D loss: 0.448685, acc: 80.47%] [G loss: 3.312298]\n",
      "epoch:3 step:3346 [D loss: 0.515920, acc: 75.78%] [G loss: 3.315208]\n",
      "epoch:3 step:3347 [D loss: 0.539369, acc: 70.31%] [G loss: 3.329481]\n",
      "epoch:3 step:3348 [D loss: 0.579395, acc: 75.00%] [G loss: 3.060507]\n",
      "epoch:3 step:3349 [D loss: 0.628713, acc: 65.62%] [G loss: 3.255830]\n",
      "epoch:3 step:3350 [D loss: 0.595064, acc: 66.41%] [G loss: 2.799417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3351 [D loss: 0.544219, acc: 67.19%] [G loss: 2.928195]\n",
      "epoch:3 step:3352 [D loss: 0.588605, acc: 67.97%] [G loss: 3.118902]\n",
      "epoch:3 step:3353 [D loss: 0.647592, acc: 65.62%] [G loss: 2.881189]\n",
      "epoch:3 step:3354 [D loss: 0.577446, acc: 72.66%] [G loss: 2.793333]\n",
      "epoch:3 step:3355 [D loss: 0.563605, acc: 70.31%] [G loss: 2.881956]\n",
      "epoch:3 step:3356 [D loss: 0.569654, acc: 72.66%] [G loss: 2.733572]\n",
      "epoch:3 step:3357 [D loss: 0.480286, acc: 82.03%] [G loss: 3.273897]\n",
      "epoch:3 step:3358 [D loss: 0.506400, acc: 76.56%] [G loss: 3.408140]\n",
      "epoch:3 step:3359 [D loss: 0.510971, acc: 76.56%] [G loss: 2.919878]\n",
      "epoch:3 step:3360 [D loss: 0.520084, acc: 71.88%] [G loss: 2.989266]\n",
      "epoch:3 step:3361 [D loss: 0.559550, acc: 71.88%] [G loss: 2.802019]\n",
      "epoch:3 step:3362 [D loss: 0.517488, acc: 78.12%] [G loss: 3.205160]\n",
      "epoch:3 step:3363 [D loss: 0.528470, acc: 75.78%] [G loss: 3.160105]\n",
      "epoch:3 step:3364 [D loss: 0.487091, acc: 77.34%] [G loss: 2.768482]\n",
      "epoch:3 step:3365 [D loss: 0.502036, acc: 77.34%] [G loss: 3.254741]\n",
      "epoch:3 step:3366 [D loss: 0.594111, acc: 68.75%] [G loss: 2.988786]\n",
      "epoch:3 step:3367 [D loss: 0.440677, acc: 79.69%] [G loss: 3.399996]\n",
      "epoch:3 step:3368 [D loss: 0.515188, acc: 72.66%] [G loss: 3.274261]\n",
      "epoch:3 step:3369 [D loss: 0.439584, acc: 76.56%] [G loss: 3.089419]\n",
      "epoch:3 step:3370 [D loss: 0.592435, acc: 70.31%] [G loss: 3.268027]\n",
      "epoch:3 step:3371 [D loss: 0.623111, acc: 64.84%] [G loss: 2.936906]\n",
      "epoch:3 step:3372 [D loss: 0.534250, acc: 74.22%] [G loss: 3.059968]\n",
      "epoch:3 step:3373 [D loss: 0.611327, acc: 68.75%] [G loss: 2.814696]\n",
      "epoch:3 step:3374 [D loss: 0.582846, acc: 71.09%] [G loss: 2.793042]\n",
      "epoch:3 step:3375 [D loss: 0.597565, acc: 67.97%] [G loss: 3.363230]\n",
      "epoch:3 step:3376 [D loss: 0.475211, acc: 76.56%] [G loss: 3.550481]\n",
      "epoch:3 step:3377 [D loss: 0.606970, acc: 63.28%] [G loss: 3.010638]\n",
      "epoch:3 step:3378 [D loss: 0.442453, acc: 82.03%] [G loss: 3.282158]\n",
      "epoch:3 step:3379 [D loss: 0.495353, acc: 74.22%] [G loss: 2.953166]\n",
      "epoch:3 step:3380 [D loss: 0.582572, acc: 67.97%] [G loss: 3.120855]\n",
      "epoch:3 step:3381 [D loss: 0.496405, acc: 74.22%] [G loss: 3.148361]\n",
      "epoch:3 step:3382 [D loss: 0.588760, acc: 72.66%] [G loss: 3.092262]\n",
      "epoch:3 step:3383 [D loss: 0.605065, acc: 66.41%] [G loss: 2.794348]\n",
      "epoch:3 step:3384 [D loss: 0.646528, acc: 62.50%] [G loss: 2.645333]\n",
      "epoch:3 step:3385 [D loss: 0.517330, acc: 78.12%] [G loss: 2.923036]\n",
      "epoch:3 step:3386 [D loss: 0.483557, acc: 78.12%] [G loss: 3.146883]\n",
      "epoch:3 step:3387 [D loss: 0.574540, acc: 70.31%] [G loss: 2.796655]\n",
      "epoch:3 step:3388 [D loss: 0.548928, acc: 74.22%] [G loss: 3.302641]\n",
      "epoch:3 step:3389 [D loss: 0.535077, acc: 75.78%] [G loss: 2.628957]\n",
      "epoch:3 step:3390 [D loss: 0.546533, acc: 67.19%] [G loss: 2.847360]\n",
      "epoch:3 step:3391 [D loss: 0.691965, acc: 63.28%] [G loss: 2.852489]\n",
      "epoch:3 step:3392 [D loss: 0.563337, acc: 72.66%] [G loss: 3.102689]\n",
      "epoch:3 step:3393 [D loss: 0.484236, acc: 75.78%] [G loss: 3.072710]\n",
      "epoch:3 step:3394 [D loss: 0.622693, acc: 69.53%] [G loss: 3.164868]\n",
      "epoch:3 step:3395 [D loss: 0.540901, acc: 71.09%] [G loss: 2.792512]\n",
      "epoch:3 step:3396 [D loss: 0.615351, acc: 61.72%] [G loss: 2.809308]\n",
      "epoch:3 step:3397 [D loss: 0.598181, acc: 68.75%] [G loss: 3.085891]\n",
      "epoch:3 step:3398 [D loss: 0.625020, acc: 69.53%] [G loss: 2.866961]\n",
      "epoch:3 step:3399 [D loss: 0.533162, acc: 78.91%] [G loss: 3.100414]\n",
      "epoch:3 step:3400 [D loss: 0.509384, acc: 71.09%] [G loss: 3.940140]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 4.793049\n",
      "FID: 67.677185\n",
      "0 = 13.466382624816916\n",
      "1 = 0.09011798965765117\n",
      "2 = 0.9433000087738037\n",
      "3 = 0.9394000172615051\n",
      "4 = 0.9472000002861023\n",
      "5 = 0.9467849135398865\n",
      "6 = 0.9394000172615051\n",
      "7 = 10.152332009244011\n",
      "8 = 0.1647372222483553\n",
      "9 = 0.8758999705314636\n",
      "10 = 0.8712000250816345\n",
      "11 = 0.8805999755859375\n",
      "12 = 0.8794670104980469\n",
      "13 = 0.8712000250816345\n",
      "14 = 4.793060779571533\n",
      "15 = 8.042869567871094\n",
      "16 = 0.3444576859474182\n",
      "17 = 4.793049335479736\n",
      "18 = 67.67718505859375\n",
      "epoch:3 step:3401 [D loss: 0.582005, acc: 71.88%] [G loss: 2.999414]\n",
      "epoch:3 step:3402 [D loss: 0.656298, acc: 62.50%] [G loss: 2.663209]\n",
      "epoch:3 step:3403 [D loss: 0.463867, acc: 78.91%] [G loss: 3.365023]\n",
      "epoch:3 step:3404 [D loss: 0.603788, acc: 66.41%] [G loss: 2.857770]\n",
      "epoch:3 step:3405 [D loss: 0.596131, acc: 67.97%] [G loss: 2.770085]\n",
      "epoch:3 step:3406 [D loss: 0.540546, acc: 69.53%] [G loss: 3.025041]\n",
      "epoch:3 step:3407 [D loss: 0.612495, acc: 64.84%] [G loss: 2.883592]\n",
      "epoch:3 step:3408 [D loss: 0.465094, acc: 79.69%] [G loss: 2.956861]\n",
      "epoch:3 step:3409 [D loss: 0.545481, acc: 75.78%] [G loss: 3.297166]\n",
      "epoch:3 step:3410 [D loss: 0.584901, acc: 67.97%] [G loss: 3.336027]\n",
      "epoch:3 step:3411 [D loss: 0.656467, acc: 65.62%] [G loss: 2.790232]\n",
      "epoch:3 step:3412 [D loss: 0.615229, acc: 67.19%] [G loss: 2.816749]\n",
      "epoch:3 step:3413 [D loss: 0.482730, acc: 77.34%] [G loss: 3.301465]\n",
      "epoch:3 step:3414 [D loss: 0.576780, acc: 65.62%] [G loss: 3.648722]\n",
      "epoch:3 step:3415 [D loss: 0.504092, acc: 71.09%] [G loss: 2.961534]\n",
      "epoch:3 step:3416 [D loss: 0.541081, acc: 71.09%] [G loss: 3.325835]\n",
      "epoch:3 step:3417 [D loss: 0.500300, acc: 75.00%] [G loss: 3.132444]\n",
      "epoch:3 step:3418 [D loss: 0.572162, acc: 67.97%] [G loss: 2.892901]\n",
      "epoch:3 step:3419 [D loss: 0.586673, acc: 67.19%] [G loss: 3.122841]\n",
      "epoch:3 step:3420 [D loss: 0.549731, acc: 75.00%] [G loss: 3.075875]\n",
      "epoch:3 step:3421 [D loss: 0.529858, acc: 78.12%] [G loss: 3.105510]\n",
      "epoch:3 step:3422 [D loss: 0.481271, acc: 78.91%] [G loss: 3.264410]\n",
      "epoch:3 step:3423 [D loss: 0.494357, acc: 74.22%] [G loss: 3.397880]\n",
      "epoch:3 step:3424 [D loss: 0.595364, acc: 67.97%] [G loss: 3.218091]\n",
      "epoch:3 step:3425 [D loss: 0.534672, acc: 71.88%] [G loss: 3.172736]\n",
      "epoch:3 step:3426 [D loss: 0.694545, acc: 58.59%] [G loss: 2.670994]\n",
      "epoch:3 step:3427 [D loss: 0.598484, acc: 66.41%] [G loss: 3.060096]\n",
      "epoch:3 step:3428 [D loss: 0.565213, acc: 67.97%] [G loss: 2.588315]\n",
      "epoch:3 step:3429 [D loss: 0.449843, acc: 80.47%] [G loss: 2.908021]\n",
      "epoch:3 step:3430 [D loss: 0.530903, acc: 70.31%] [G loss: 2.997477]\n",
      "epoch:3 step:3431 [D loss: 0.488722, acc: 80.47%] [G loss: 3.607224]\n",
      "epoch:3 step:3432 [D loss: 0.534023, acc: 75.00%] [G loss: 2.955253]\n",
      "epoch:3 step:3433 [D loss: 0.726498, acc: 58.59%] [G loss: 2.550803]\n",
      "epoch:3 step:3434 [D loss: 0.638372, acc: 65.62%] [G loss: 3.293893]\n",
      "epoch:3 step:3435 [D loss: 0.549146, acc: 71.88%] [G loss: 2.858680]\n",
      "epoch:3 step:3436 [D loss: 0.590391, acc: 69.53%] [G loss: 2.881327]\n",
      "epoch:3 step:3437 [D loss: 0.605887, acc: 65.62%] [G loss: 2.977584]\n",
      "epoch:3 step:3438 [D loss: 0.515481, acc: 75.78%] [G loss: 2.901459]\n",
      "epoch:3 step:3439 [D loss: 0.577211, acc: 72.66%] [G loss: 3.124593]\n",
      "epoch:3 step:3440 [D loss: 0.591413, acc: 68.75%] [G loss: 2.895373]\n",
      "epoch:3 step:3441 [D loss: 0.553218, acc: 72.66%] [G loss: 3.073935]\n",
      "epoch:3 step:3442 [D loss: 0.483508, acc: 79.69%] [G loss: 3.035427]\n",
      "epoch:3 step:3443 [D loss: 0.587064, acc: 69.53%] [G loss: 3.117877]\n",
      "epoch:3 step:3444 [D loss: 0.474214, acc: 79.69%] [G loss: 2.968429]\n",
      "epoch:3 step:3445 [D loss: 0.531257, acc: 70.31%] [G loss: 3.002889]\n",
      "epoch:3 step:3446 [D loss: 0.634319, acc: 64.84%] [G loss: 3.109277]\n",
      "epoch:3 step:3447 [D loss: 0.612161, acc: 67.19%] [G loss: 2.777280]\n",
      "epoch:3 step:3448 [D loss: 0.517541, acc: 73.44%] [G loss: 2.943080]\n",
      "epoch:3 step:3449 [D loss: 0.508837, acc: 74.22%] [G loss: 3.102270]\n",
      "epoch:3 step:3450 [D loss: 0.480824, acc: 78.91%] [G loss: 3.353768]\n",
      "epoch:3 step:3451 [D loss: 0.586201, acc: 69.53%] [G loss: 3.177568]\n",
      "epoch:3 step:3452 [D loss: 0.456529, acc: 75.78%] [G loss: 3.634743]\n",
      "epoch:3 step:3453 [D loss: 0.519707, acc: 78.12%] [G loss: 3.339875]\n",
      "epoch:3 step:3454 [D loss: 0.596049, acc: 67.97%] [G loss: 3.165769]\n",
      "epoch:3 step:3455 [D loss: 0.614890, acc: 67.19%] [G loss: 2.764454]\n",
      "epoch:3 step:3456 [D loss: 0.636709, acc: 65.62%] [G loss: 2.776228]\n",
      "epoch:3 step:3457 [D loss: 0.549096, acc: 74.22%] [G loss: 2.908428]\n",
      "epoch:3 step:3458 [D loss: 0.520672, acc: 74.22%] [G loss: 3.516963]\n",
      "epoch:3 step:3459 [D loss: 0.544847, acc: 71.09%] [G loss: 3.271147]\n",
      "epoch:3 step:3460 [D loss: 0.525602, acc: 75.00%] [G loss: 3.713774]\n",
      "epoch:3 step:3461 [D loss: 0.527716, acc: 70.31%] [G loss: 3.201056]\n",
      "epoch:3 step:3462 [D loss: 0.594964, acc: 68.75%] [G loss: 3.181041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3463 [D loss: 0.655144, acc: 65.62%] [G loss: 3.002399]\n",
      "epoch:3 step:3464 [D loss: 0.546712, acc: 72.66%] [G loss: 3.111483]\n",
      "epoch:3 step:3465 [D loss: 0.566445, acc: 70.31%] [G loss: 3.294220]\n",
      "epoch:3 step:3466 [D loss: 0.666938, acc: 65.62%] [G loss: 2.870689]\n",
      "epoch:3 step:3467 [D loss: 0.536309, acc: 73.44%] [G loss: 3.039289]\n",
      "epoch:3 step:3468 [D loss: 0.508283, acc: 78.12%] [G loss: 2.929208]\n",
      "epoch:3 step:3469 [D loss: 0.538263, acc: 70.31%] [G loss: 2.863914]\n",
      "epoch:3 step:3470 [D loss: 0.469712, acc: 75.78%] [G loss: 3.156341]\n",
      "epoch:3 step:3471 [D loss: 0.503967, acc: 75.78%] [G loss: 3.164083]\n",
      "epoch:3 step:3472 [D loss: 0.510226, acc: 76.56%] [G loss: 3.019818]\n",
      "epoch:3 step:3473 [D loss: 0.578695, acc: 67.97%] [G loss: 3.476786]\n",
      "epoch:3 step:3474 [D loss: 0.526088, acc: 74.22%] [G loss: 3.528003]\n",
      "epoch:3 step:3475 [D loss: 0.621803, acc: 69.53%] [G loss: 3.392129]\n",
      "epoch:3 step:3476 [D loss: 0.532428, acc: 71.09%] [G loss: 3.178887]\n",
      "epoch:3 step:3477 [D loss: 0.673068, acc: 57.81%] [G loss: 2.910659]\n",
      "epoch:3 step:3478 [D loss: 0.547996, acc: 68.75%] [G loss: 2.520832]\n",
      "epoch:3 step:3479 [D loss: 0.662588, acc: 66.41%] [G loss: 2.652299]\n",
      "epoch:3 step:3480 [D loss: 0.600145, acc: 69.53%] [G loss: 2.823531]\n",
      "epoch:3 step:3481 [D loss: 0.512973, acc: 73.44%] [G loss: 2.738258]\n",
      "epoch:3 step:3482 [D loss: 0.567243, acc: 65.62%] [G loss: 2.668264]\n",
      "epoch:3 step:3483 [D loss: 0.740871, acc: 56.25%] [G loss: 2.904037]\n",
      "epoch:3 step:3484 [D loss: 0.564101, acc: 72.66%] [G loss: 2.767754]\n",
      "epoch:3 step:3485 [D loss: 0.577369, acc: 67.97%] [G loss: 2.373725]\n",
      "epoch:3 step:3486 [D loss: 0.496895, acc: 77.34%] [G loss: 3.222469]\n",
      "epoch:3 step:3487 [D loss: 0.511144, acc: 77.34%] [G loss: 3.043473]\n",
      "epoch:3 step:3488 [D loss: 0.497291, acc: 77.34%] [G loss: 3.349105]\n",
      "epoch:3 step:3489 [D loss: 0.437246, acc: 81.25%] [G loss: 2.962852]\n",
      "epoch:3 step:3490 [D loss: 0.618778, acc: 67.97%] [G loss: 3.035181]\n",
      "epoch:3 step:3491 [D loss: 0.489742, acc: 75.00%] [G loss: 3.083707]\n",
      "epoch:3 step:3492 [D loss: 0.483045, acc: 77.34%] [G loss: 3.121749]\n",
      "epoch:3 step:3493 [D loss: 0.588597, acc: 75.00%] [G loss: 2.851005]\n",
      "epoch:3 step:3494 [D loss: 0.609046, acc: 69.53%] [G loss: 2.981776]\n",
      "epoch:3 step:3495 [D loss: 0.518942, acc: 74.22%] [G loss: 2.981295]\n",
      "epoch:3 step:3496 [D loss: 0.608480, acc: 70.31%] [G loss: 2.878168]\n",
      "epoch:3 step:3497 [D loss: 0.567395, acc: 71.88%] [G loss: 3.018443]\n",
      "epoch:3 step:3498 [D loss: 0.560657, acc: 72.66%] [G loss: 2.894239]\n",
      "epoch:3 step:3499 [D loss: 0.626977, acc: 70.31%] [G loss: 2.749682]\n",
      "epoch:3 step:3500 [D loss: 0.594429, acc: 65.62%] [G loss: 2.939237]\n",
      "epoch:3 step:3501 [D loss: 0.608354, acc: 65.62%] [G loss: 3.188278]\n",
      "epoch:3 step:3502 [D loss: 0.540641, acc: 68.75%] [G loss: 3.157223]\n",
      "epoch:3 step:3503 [D loss: 0.515212, acc: 74.22%] [G loss: 2.904857]\n",
      "epoch:3 step:3504 [D loss: 0.532532, acc: 72.66%] [G loss: 2.792722]\n",
      "epoch:3 step:3505 [D loss: 0.503252, acc: 77.34%] [G loss: 3.341215]\n",
      "epoch:3 step:3506 [D loss: 0.575344, acc: 69.53%] [G loss: 3.188917]\n",
      "epoch:3 step:3507 [D loss: 0.607767, acc: 64.06%] [G loss: 2.997397]\n",
      "epoch:3 step:3508 [D loss: 0.535617, acc: 76.56%] [G loss: 2.975963]\n",
      "epoch:3 step:3509 [D loss: 0.546071, acc: 71.88%] [G loss: 2.988077]\n",
      "epoch:3 step:3510 [D loss: 0.592528, acc: 65.62%] [G loss: 3.104166]\n",
      "epoch:3 step:3511 [D loss: 0.595796, acc: 66.41%] [G loss: 3.248524]\n",
      "epoch:3 step:3512 [D loss: 0.528196, acc: 72.66%] [G loss: 2.921242]\n",
      "epoch:3 step:3513 [D loss: 0.569784, acc: 71.09%] [G loss: 3.006471]\n",
      "epoch:3 step:3514 [D loss: 0.533085, acc: 75.78%] [G loss: 2.799574]\n",
      "epoch:3 step:3515 [D loss: 0.612680, acc: 68.75%] [G loss: 2.923543]\n",
      "epoch:3 step:3516 [D loss: 0.483119, acc: 74.22%] [G loss: 3.355786]\n",
      "epoch:3 step:3517 [D loss: 0.544964, acc: 71.88%] [G loss: 3.409411]\n",
      "epoch:3 step:3518 [D loss: 0.411605, acc: 81.25%] [G loss: 3.383544]\n",
      "epoch:3 step:3519 [D loss: 0.469257, acc: 81.25%] [G loss: 3.496218]\n",
      "epoch:3 step:3520 [D loss: 0.438183, acc: 78.91%] [G loss: 3.785058]\n",
      "epoch:3 step:3521 [D loss: 0.671655, acc: 68.75%] [G loss: 3.115347]\n",
      "epoch:3 step:3522 [D loss: 0.599915, acc: 71.88%] [G loss: 3.048852]\n",
      "epoch:3 step:3523 [D loss: 0.493121, acc: 75.00%] [G loss: 3.289735]\n",
      "epoch:3 step:3524 [D loss: 0.596275, acc: 67.97%] [G loss: 3.141763]\n",
      "epoch:3 step:3525 [D loss: 0.574788, acc: 71.09%] [G loss: 3.426813]\n",
      "epoch:3 step:3526 [D loss: 0.542667, acc: 73.44%] [G loss: 3.000701]\n",
      "epoch:3 step:3527 [D loss: 0.636092, acc: 64.06%] [G loss: 2.973080]\n",
      "epoch:3 step:3528 [D loss: 0.601336, acc: 65.62%] [G loss: 2.870611]\n",
      "epoch:3 step:3529 [D loss: 0.602211, acc: 68.75%] [G loss: 2.900715]\n",
      "epoch:3 step:3530 [D loss: 0.472915, acc: 75.00%] [G loss: 2.859574]\n",
      "epoch:3 step:3531 [D loss: 0.567200, acc: 67.19%] [G loss: 2.765124]\n",
      "epoch:3 step:3532 [D loss: 0.552698, acc: 71.09%] [G loss: 3.020107]\n",
      "epoch:3 step:3533 [D loss: 0.574630, acc: 70.31%] [G loss: 3.012767]\n",
      "epoch:3 step:3534 [D loss: 0.561014, acc: 71.09%] [G loss: 2.819022]\n",
      "epoch:3 step:3535 [D loss: 0.574178, acc: 65.62%] [G loss: 3.563250]\n",
      "epoch:3 step:3536 [D loss: 0.566466, acc: 67.97%] [G loss: 3.193518]\n",
      "epoch:3 step:3537 [D loss: 0.638665, acc: 54.69%] [G loss: 3.118436]\n",
      "epoch:3 step:3538 [D loss: 0.542013, acc: 68.75%] [G loss: 2.860089]\n",
      "epoch:3 step:3539 [D loss: 0.560250, acc: 69.53%] [G loss: 3.144620]\n",
      "epoch:3 step:3540 [D loss: 0.598997, acc: 69.53%] [G loss: 2.964463]\n",
      "epoch:3 step:3541 [D loss: 0.480096, acc: 80.47%] [G loss: 2.788784]\n",
      "epoch:3 step:3542 [D loss: 0.623002, acc: 67.97%] [G loss: 2.954988]\n",
      "epoch:3 step:3543 [D loss: 0.500442, acc: 77.34%] [G loss: 3.084093]\n",
      "epoch:3 step:3544 [D loss: 0.473056, acc: 78.12%] [G loss: 2.962555]\n",
      "epoch:3 step:3545 [D loss: 0.570802, acc: 66.41%] [G loss: 3.132733]\n",
      "epoch:3 step:3546 [D loss: 0.653781, acc: 64.06%] [G loss: 2.769802]\n",
      "epoch:3 step:3547 [D loss: 0.505784, acc: 71.09%] [G loss: 3.000273]\n",
      "epoch:3 step:3548 [D loss: 0.503971, acc: 74.22%] [G loss: 2.879156]\n",
      "epoch:3 step:3549 [D loss: 0.569945, acc: 71.88%] [G loss: 3.047422]\n",
      "epoch:3 step:3550 [D loss: 0.610529, acc: 69.53%] [G loss: 3.077081]\n",
      "epoch:3 step:3551 [D loss: 0.597251, acc: 69.53%] [G loss: 2.878292]\n",
      "epoch:3 step:3552 [D loss: 0.580912, acc: 69.53%] [G loss: 3.260358]\n",
      "epoch:3 step:3553 [D loss: 0.549924, acc: 69.53%] [G loss: 2.872365]\n",
      "epoch:3 step:3554 [D loss: 0.643061, acc: 66.41%] [G loss: 3.273063]\n",
      "epoch:3 step:3555 [D loss: 0.541459, acc: 70.31%] [G loss: 3.160971]\n",
      "epoch:3 step:3556 [D loss: 0.595457, acc: 70.31%] [G loss: 2.658130]\n",
      "epoch:3 step:3557 [D loss: 0.475473, acc: 80.47%] [G loss: 3.263774]\n",
      "epoch:3 step:3558 [D loss: 0.485166, acc: 74.22%] [G loss: 3.767320]\n",
      "epoch:3 step:3559 [D loss: 0.589678, acc: 63.28%] [G loss: 3.045167]\n",
      "epoch:3 step:3560 [D loss: 0.585156, acc: 67.97%] [G loss: 3.092340]\n",
      "epoch:3 step:3561 [D loss: 0.569455, acc: 74.22%] [G loss: 2.643972]\n",
      "epoch:3 step:3562 [D loss: 0.604138, acc: 66.41%] [G loss: 2.563673]\n",
      "epoch:3 step:3563 [D loss: 0.515506, acc: 74.22%] [G loss: 3.185088]\n",
      "epoch:3 step:3564 [D loss: 0.503036, acc: 72.66%] [G loss: 2.793221]\n",
      "epoch:3 step:3565 [D loss: 0.543720, acc: 75.00%] [G loss: 3.092764]\n",
      "epoch:3 step:3566 [D loss: 0.489886, acc: 78.12%] [G loss: 3.512412]\n",
      "epoch:3 step:3567 [D loss: 0.564852, acc: 77.34%] [G loss: 2.806482]\n",
      "epoch:3 step:3568 [D loss: 0.541335, acc: 75.00%] [G loss: 3.079932]\n",
      "epoch:3 step:3569 [D loss: 0.540181, acc: 73.44%] [G loss: 3.197081]\n",
      "epoch:3 step:3570 [D loss: 0.538307, acc: 69.53%] [G loss: 2.921730]\n",
      "epoch:3 step:3571 [D loss: 0.551687, acc: 72.66%] [G loss: 2.830706]\n",
      "epoch:3 step:3572 [D loss: 0.606780, acc: 68.75%] [G loss: 3.181105]\n",
      "epoch:3 step:3573 [D loss: 0.599732, acc: 67.19%] [G loss: 2.382431]\n",
      "epoch:3 step:3574 [D loss: 0.648954, acc: 66.41%] [G loss: 3.019495]\n",
      "epoch:3 step:3575 [D loss: 0.606009, acc: 63.28%] [G loss: 2.832966]\n",
      "epoch:3 step:3576 [D loss: 0.638260, acc: 63.28%] [G loss: 2.650333]\n",
      "epoch:3 step:3577 [D loss: 0.735280, acc: 57.03%] [G loss: 2.398391]\n",
      "epoch:3 step:3578 [D loss: 0.535270, acc: 76.56%] [G loss: 2.760711]\n",
      "epoch:3 step:3579 [D loss: 0.649788, acc: 59.38%] [G loss: 2.574002]\n",
      "epoch:3 step:3580 [D loss: 0.541977, acc: 70.31%] [G loss: 3.091084]\n",
      "epoch:3 step:3581 [D loss: 0.592895, acc: 66.41%] [G loss: 2.620555]\n",
      "epoch:3 step:3582 [D loss: 0.549566, acc: 67.19%] [G loss: 2.976730]\n",
      "epoch:3 step:3583 [D loss: 0.508417, acc: 78.12%] [G loss: 3.066336]\n",
      "epoch:3 step:3584 [D loss: 0.607211, acc: 70.31%] [G loss: 2.638530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3585 [D loss: 0.528935, acc: 71.88%] [G loss: 2.803808]\n",
      "epoch:3 step:3586 [D loss: 0.601599, acc: 67.19%] [G loss: 2.815254]\n",
      "epoch:3 step:3587 [D loss: 0.588877, acc: 68.75%] [G loss: 3.010816]\n",
      "epoch:3 step:3588 [D loss: 0.643617, acc: 65.62%] [G loss: 2.844962]\n",
      "epoch:3 step:3589 [D loss: 0.563313, acc: 74.22%] [G loss: 2.875956]\n",
      "epoch:3 step:3590 [D loss: 0.577357, acc: 67.97%] [G loss: 2.813430]\n",
      "epoch:3 step:3591 [D loss: 0.679610, acc: 64.06%] [G loss: 3.046680]\n",
      "epoch:3 step:3592 [D loss: 0.498492, acc: 75.78%] [G loss: 3.232024]\n",
      "epoch:3 step:3593 [D loss: 0.626440, acc: 67.19%] [G loss: 2.986762]\n",
      "epoch:3 step:3594 [D loss: 0.588203, acc: 73.44%] [G loss: 2.951372]\n",
      "epoch:3 step:3595 [D loss: 0.590217, acc: 65.62%] [G loss: 2.935064]\n",
      "epoch:3 step:3596 [D loss: 0.616818, acc: 64.84%] [G loss: 2.577004]\n",
      "epoch:3 step:3597 [D loss: 0.573464, acc: 66.41%] [G loss: 2.769725]\n",
      "epoch:3 step:3598 [D loss: 0.627269, acc: 67.19%] [G loss: 2.558458]\n",
      "epoch:3 step:3599 [D loss: 0.702019, acc: 58.59%] [G loss: 2.414123]\n",
      "epoch:3 step:3600 [D loss: 0.587516, acc: 66.41%] [G loss: 2.567407]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.007446\n",
      "FID: 61.662445\n",
      "0 = 13.397195959472667\n",
      "1 = 0.08779223551035571\n",
      "2 = 0.9415000081062317\n",
      "3 = 0.9332000017166138\n",
      "4 = 0.9498000144958496\n",
      "5 = 0.9489526152610779\n",
      "6 = 0.9332000017166138\n",
      "7 = 9.849303851127607\n",
      "8 = 0.15441310705675348\n",
      "9 = 0.8733000159263611\n",
      "10 = 0.8687999844551086\n",
      "11 = 0.8777999877929688\n",
      "12 = 0.8766902089118958\n",
      "13 = 0.8687999844551086\n",
      "14 = 5.007457256317139\n",
      "15 = 8.522851943969727\n",
      "16 = 0.29211363196372986\n",
      "17 = 5.007445812225342\n",
      "18 = 61.662445068359375\n",
      "epoch:3 step:3601 [D loss: 0.553510, acc: 73.44%] [G loss: 2.507965]\n",
      "epoch:3 step:3602 [D loss: 0.553652, acc: 71.88%] [G loss: 3.291560]\n",
      "epoch:3 step:3603 [D loss: 0.488439, acc: 74.22%] [G loss: 3.162235]\n",
      "epoch:3 step:3604 [D loss: 0.496794, acc: 75.78%] [G loss: 2.828719]\n",
      "epoch:3 step:3605 [D loss: 0.607992, acc: 67.19%] [G loss: 2.705345]\n",
      "epoch:3 step:3606 [D loss: 0.535590, acc: 71.88%] [G loss: 3.408173]\n",
      "epoch:3 step:3607 [D loss: 0.663028, acc: 58.59%] [G loss: 3.326176]\n",
      "epoch:3 step:3608 [D loss: 0.589491, acc: 72.66%] [G loss: 2.692203]\n",
      "epoch:3 step:3609 [D loss: 0.543530, acc: 68.75%] [G loss: 3.046962]\n",
      "epoch:3 step:3610 [D loss: 0.540252, acc: 79.69%] [G loss: 2.717151]\n",
      "epoch:3 step:3611 [D loss: 0.584951, acc: 68.75%] [G loss: 3.114717]\n",
      "epoch:3 step:3612 [D loss: 0.510970, acc: 71.88%] [G loss: 3.066622]\n",
      "epoch:3 step:3613 [D loss: 0.497877, acc: 72.66%] [G loss: 3.101191]\n",
      "epoch:3 step:3614 [D loss: 0.505930, acc: 77.34%] [G loss: 2.963846]\n",
      "epoch:3 step:3615 [D loss: 0.441303, acc: 82.03%] [G loss: 3.544380]\n",
      "epoch:3 step:3616 [D loss: 0.502375, acc: 75.78%] [G loss: 3.320925]\n",
      "epoch:3 step:3617 [D loss: 0.478631, acc: 77.34%] [G loss: 3.665351]\n",
      "epoch:3 step:3618 [D loss: 0.464008, acc: 77.34%] [G loss: 3.443008]\n",
      "epoch:3 step:3619 [D loss: 0.669003, acc: 60.94%] [G loss: 2.735929]\n",
      "epoch:3 step:3620 [D loss: 0.591762, acc: 72.66%] [G loss: 3.047433]\n",
      "epoch:3 step:3621 [D loss: 0.541371, acc: 71.09%] [G loss: 2.867153]\n",
      "epoch:3 step:3622 [D loss: 0.604454, acc: 68.75%] [G loss: 2.704082]\n",
      "epoch:3 step:3623 [D loss: 0.523782, acc: 72.66%] [G loss: 2.967665]\n",
      "epoch:3 step:3624 [D loss: 0.560571, acc: 74.22%] [G loss: 3.088445]\n",
      "epoch:3 step:3625 [D loss: 0.574723, acc: 68.75%] [G loss: 2.902975]\n",
      "epoch:3 step:3626 [D loss: 0.589269, acc: 71.88%] [G loss: 3.089870]\n",
      "epoch:3 step:3627 [D loss: 0.506066, acc: 75.00%] [G loss: 2.869286]\n",
      "epoch:3 step:3628 [D loss: 0.569947, acc: 67.97%] [G loss: 3.297258]\n",
      "epoch:3 step:3629 [D loss: 0.628846, acc: 71.09%] [G loss: 2.873452]\n",
      "epoch:3 step:3630 [D loss: 0.620571, acc: 67.19%] [G loss: 3.030106]\n",
      "epoch:3 step:3631 [D loss: 0.619837, acc: 68.75%] [G loss: 2.808693]\n",
      "epoch:3 step:3632 [D loss: 0.531497, acc: 75.00%] [G loss: 2.574445]\n",
      "epoch:3 step:3633 [D loss: 0.551787, acc: 73.44%] [G loss: 3.360079]\n",
      "epoch:3 step:3634 [D loss: 0.510949, acc: 78.91%] [G loss: 3.095261]\n",
      "epoch:3 step:3635 [D loss: 0.584123, acc: 70.31%] [G loss: 2.713619]\n",
      "epoch:3 step:3636 [D loss: 0.554288, acc: 74.22%] [G loss: 3.489426]\n",
      "epoch:3 step:3637 [D loss: 0.606184, acc: 64.06%] [G loss: 2.761806]\n",
      "epoch:3 step:3638 [D loss: 0.691679, acc: 60.94%] [G loss: 2.938581]\n",
      "epoch:3 step:3639 [D loss: 0.697882, acc: 60.94%] [G loss: 2.707267]\n",
      "epoch:3 step:3640 [D loss: 0.577549, acc: 71.09%] [G loss: 2.578158]\n",
      "epoch:3 step:3641 [D loss: 0.654717, acc: 66.41%] [G loss: 3.014272]\n",
      "epoch:3 step:3642 [D loss: 0.561128, acc: 75.78%] [G loss: 3.386797]\n",
      "epoch:3 step:3643 [D loss: 0.474961, acc: 74.22%] [G loss: 2.853116]\n",
      "epoch:3 step:3644 [D loss: 0.567210, acc: 75.00%] [G loss: 2.974279]\n",
      "epoch:3 step:3645 [D loss: 0.515055, acc: 75.78%] [G loss: 2.781084]\n",
      "epoch:3 step:3646 [D loss: 0.619211, acc: 70.31%] [G loss: 2.752455]\n",
      "epoch:3 step:3647 [D loss: 0.543769, acc: 72.66%] [G loss: 2.935857]\n",
      "epoch:3 step:3648 [D loss: 0.561423, acc: 74.22%] [G loss: 2.655931]\n",
      "epoch:3 step:3649 [D loss: 0.544906, acc: 78.91%] [G loss: 2.912509]\n",
      "epoch:3 step:3650 [D loss: 0.546786, acc: 77.34%] [G loss: 2.790278]\n",
      "epoch:3 step:3651 [D loss: 0.571556, acc: 71.88%] [G loss: 2.857480]\n",
      "epoch:3 step:3652 [D loss: 0.572864, acc: 71.09%] [G loss: 2.997958]\n",
      "epoch:3 step:3653 [D loss: 0.477588, acc: 78.91%] [G loss: 3.156909]\n",
      "epoch:3 step:3654 [D loss: 0.563101, acc: 72.66%] [G loss: 3.083074]\n",
      "epoch:3 step:3655 [D loss: 0.621691, acc: 64.06%] [G loss: 2.701744]\n",
      "epoch:3 step:3656 [D loss: 0.640824, acc: 63.28%] [G loss: 2.703292]\n",
      "epoch:3 step:3657 [D loss: 0.561870, acc: 72.66%] [G loss: 2.745539]\n",
      "epoch:3 step:3658 [D loss: 0.523756, acc: 69.53%] [G loss: 3.108076]\n",
      "epoch:3 step:3659 [D loss: 0.544591, acc: 74.22%] [G loss: 3.299446]\n",
      "epoch:3 step:3660 [D loss: 0.610112, acc: 66.41%] [G loss: 2.823501]\n",
      "epoch:3 step:3661 [D loss: 0.566213, acc: 73.44%] [G loss: 3.552811]\n",
      "epoch:3 step:3662 [D loss: 0.506168, acc: 78.12%] [G loss: 2.407418]\n",
      "epoch:3 step:3663 [D loss: 0.542284, acc: 74.22%] [G loss: 3.068857]\n",
      "epoch:3 step:3664 [D loss: 0.489861, acc: 78.91%] [G loss: 3.249693]\n",
      "epoch:3 step:3665 [D loss: 0.513112, acc: 75.78%] [G loss: 3.036756]\n",
      "epoch:3 step:3666 [D loss: 0.640945, acc: 63.28%] [G loss: 2.790213]\n",
      "epoch:3 step:3667 [D loss: 0.623517, acc: 64.84%] [G loss: 2.898750]\n",
      "epoch:3 step:3668 [D loss: 0.563200, acc: 68.75%] [G loss: 3.069286]\n",
      "epoch:3 step:3669 [D loss: 0.652406, acc: 62.50%] [G loss: 2.685063]\n",
      "epoch:3 step:3670 [D loss: 0.574033, acc: 72.66%] [G loss: 2.883509]\n",
      "epoch:3 step:3671 [D loss: 0.538238, acc: 71.09%] [G loss: 2.866725]\n",
      "epoch:3 step:3672 [D loss: 0.613295, acc: 71.09%] [G loss: 2.940529]\n",
      "epoch:3 step:3673 [D loss: 0.550020, acc: 74.22%] [G loss: 2.872118]\n",
      "epoch:3 step:3674 [D loss: 0.587824, acc: 71.88%] [G loss: 2.779355]\n",
      "epoch:3 step:3675 [D loss: 0.611425, acc: 68.75%] [G loss: 2.502820]\n",
      "epoch:3 step:3676 [D loss: 0.571723, acc: 69.53%] [G loss: 2.995127]\n",
      "epoch:3 step:3677 [D loss: 0.535736, acc: 72.66%] [G loss: 2.884791]\n",
      "epoch:3 step:3678 [D loss: 0.594286, acc: 67.97%] [G loss: 2.596117]\n",
      "epoch:3 step:3679 [D loss: 0.609137, acc: 71.88%] [G loss: 2.780863]\n",
      "epoch:3 step:3680 [D loss: 0.559081, acc: 75.00%] [G loss: 2.967042]\n",
      "epoch:3 step:3681 [D loss: 0.579616, acc: 69.53%] [G loss: 2.857273]\n",
      "epoch:3 step:3682 [D loss: 0.591820, acc: 68.75%] [G loss: 3.165513]\n",
      "epoch:3 step:3683 [D loss: 0.561453, acc: 70.31%] [G loss: 2.902210]\n",
      "epoch:3 step:3684 [D loss: 0.601981, acc: 67.97%] [G loss: 2.815996]\n",
      "epoch:3 step:3685 [D loss: 0.554157, acc: 74.22%] [G loss: 2.972388]\n",
      "epoch:3 step:3686 [D loss: 0.486121, acc: 78.91%] [G loss: 3.160431]\n",
      "epoch:3 step:3687 [D loss: 0.526221, acc: 72.66%] [G loss: 2.896882]\n",
      "epoch:3 step:3688 [D loss: 0.567743, acc: 72.66%] [G loss: 3.138900]\n",
      "epoch:3 step:3689 [D loss: 0.611721, acc: 67.19%] [G loss: 2.642901]\n",
      "epoch:3 step:3690 [D loss: 0.533527, acc: 74.22%] [G loss: 2.766254]\n",
      "epoch:3 step:3691 [D loss: 0.714906, acc: 61.72%] [G loss: 2.791384]\n",
      "epoch:3 step:3692 [D loss: 0.551829, acc: 78.12%] [G loss: 2.589608]\n",
      "epoch:3 step:3693 [D loss: 0.633656, acc: 69.53%] [G loss: 2.772952]\n",
      "epoch:3 step:3694 [D loss: 0.533777, acc: 73.44%] [G loss: 3.030093]\n",
      "epoch:3 step:3695 [D loss: 0.536457, acc: 70.31%] [G loss: 3.191644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3696 [D loss: 0.526009, acc: 73.44%] [G loss: 3.049640]\n",
      "epoch:3 step:3697 [D loss: 0.500648, acc: 78.91%] [G loss: 3.489239]\n",
      "epoch:3 step:3698 [D loss: 0.531489, acc: 71.09%] [G loss: 3.395136]\n",
      "epoch:3 step:3699 [D loss: 0.534721, acc: 78.91%] [G loss: 3.377837]\n",
      "epoch:3 step:3700 [D loss: 0.532969, acc: 79.69%] [G loss: 3.294196]\n",
      "epoch:3 step:3701 [D loss: 0.387613, acc: 85.16%] [G loss: 3.278583]\n",
      "epoch:3 step:3702 [D loss: 0.696137, acc: 62.50%] [G loss: 2.457229]\n",
      "epoch:3 step:3703 [D loss: 0.573215, acc: 71.88%] [G loss: 2.637032]\n",
      "epoch:3 step:3704 [D loss: 0.635223, acc: 69.53%] [G loss: 2.604221]\n",
      "epoch:3 step:3705 [D loss: 0.535706, acc: 70.31%] [G loss: 3.005023]\n",
      "epoch:3 step:3706 [D loss: 0.590317, acc: 68.75%] [G loss: 2.519077]\n",
      "epoch:3 step:3707 [D loss: 0.554519, acc: 71.88%] [G loss: 2.956300]\n",
      "epoch:3 step:3708 [D loss: 0.553382, acc: 72.66%] [G loss: 2.837891]\n",
      "epoch:3 step:3709 [D loss: 0.514522, acc: 74.22%] [G loss: 3.249190]\n",
      "epoch:3 step:3710 [D loss: 0.575496, acc: 71.88%] [G loss: 2.889336]\n",
      "epoch:3 step:3711 [D loss: 0.617962, acc: 67.19%] [G loss: 2.695767]\n",
      "epoch:3 step:3712 [D loss: 0.447605, acc: 81.25%] [G loss: 3.157217]\n",
      "epoch:3 step:3713 [D loss: 0.609424, acc: 66.41%] [G loss: 2.764318]\n",
      "epoch:3 step:3714 [D loss: 0.555501, acc: 76.56%] [G loss: 3.098001]\n",
      "epoch:3 step:3715 [D loss: 0.535686, acc: 71.09%] [G loss: 3.107329]\n",
      "epoch:3 step:3716 [D loss: 0.588816, acc: 68.75%] [G loss: 3.048585]\n",
      "epoch:3 step:3717 [D loss: 0.598347, acc: 72.66%] [G loss: 3.208384]\n",
      "epoch:3 step:3718 [D loss: 0.529985, acc: 72.66%] [G loss: 3.169070]\n",
      "epoch:3 step:3719 [D loss: 0.597482, acc: 69.53%] [G loss: 2.880805]\n",
      "epoch:3 step:3720 [D loss: 0.513144, acc: 72.66%] [G loss: 3.206414]\n",
      "epoch:3 step:3721 [D loss: 0.485008, acc: 78.91%] [G loss: 3.514642]\n",
      "epoch:3 step:3722 [D loss: 0.527550, acc: 70.31%] [G loss: 3.274014]\n",
      "epoch:3 step:3723 [D loss: 0.460778, acc: 77.34%] [G loss: 3.579008]\n",
      "epoch:3 step:3724 [D loss: 0.625590, acc: 71.09%] [G loss: 2.765779]\n",
      "epoch:3 step:3725 [D loss: 0.550579, acc: 72.66%] [G loss: 2.764017]\n",
      "epoch:3 step:3726 [D loss: 0.647267, acc: 61.72%] [G loss: 2.754034]\n",
      "epoch:3 step:3727 [D loss: 0.560034, acc: 67.97%] [G loss: 2.720367]\n",
      "epoch:3 step:3728 [D loss: 0.759375, acc: 64.84%] [G loss: 2.545035]\n",
      "epoch:3 step:3729 [D loss: 0.650927, acc: 67.19%] [G loss: 2.635025]\n",
      "epoch:3 step:3730 [D loss: 0.611697, acc: 67.97%] [G loss: 3.054260]\n",
      "epoch:3 step:3731 [D loss: 0.711910, acc: 59.38%] [G loss: 3.203115]\n",
      "epoch:3 step:3732 [D loss: 0.553563, acc: 75.00%] [G loss: 2.966642]\n",
      "epoch:3 step:3733 [D loss: 0.608589, acc: 71.88%] [G loss: 2.656623]\n",
      "epoch:3 step:3734 [D loss: 0.499682, acc: 77.34%] [G loss: 3.320582]\n",
      "epoch:3 step:3735 [D loss: 0.516490, acc: 77.34%] [G loss: 3.132160]\n",
      "epoch:3 step:3736 [D loss: 0.505993, acc: 75.00%] [G loss: 3.008697]\n",
      "epoch:3 step:3737 [D loss: 0.460829, acc: 77.34%] [G loss: 3.246248]\n",
      "epoch:3 step:3738 [D loss: 0.561676, acc: 71.09%] [G loss: 3.210272]\n",
      "epoch:3 step:3739 [D loss: 0.934868, acc: 51.56%] [G loss: 2.763486]\n",
      "epoch:3 step:3740 [D loss: 0.593466, acc: 70.31%] [G loss: 3.252199]\n",
      "epoch:3 step:3741 [D loss: 0.515825, acc: 75.78%] [G loss: 3.083889]\n",
      "epoch:3 step:3742 [D loss: 0.509858, acc: 75.78%] [G loss: 3.130293]\n",
      "epoch:3 step:3743 [D loss: 0.598222, acc: 70.31%] [G loss: 3.183223]\n",
      "epoch:3 step:3744 [D loss: 0.619754, acc: 65.62%] [G loss: 3.026890]\n",
      "epoch:3 step:3745 [D loss: 0.573487, acc: 71.09%] [G loss: 3.003904]\n",
      "epoch:3 step:3746 [D loss: 0.583214, acc: 72.66%] [G loss: 3.008993]\n",
      "epoch:3 step:3747 [D loss: 0.443663, acc: 81.25%] [G loss: 3.427286]\n",
      "epoch:3 step:3748 [D loss: 0.489533, acc: 81.25%] [G loss: 3.466806]\n",
      "epoch:4 step:3749 [D loss: 0.657828, acc: 66.41%] [G loss: 3.029440]\n",
      "epoch:4 step:3750 [D loss: 0.631849, acc: 71.09%] [G loss: 3.174268]\n",
      "epoch:4 step:3751 [D loss: 0.620845, acc: 62.50%] [G loss: 2.686978]\n",
      "epoch:4 step:3752 [D loss: 0.654713, acc: 64.84%] [G loss: 3.002825]\n",
      "epoch:4 step:3753 [D loss: 0.544829, acc: 71.88%] [G loss: 2.794127]\n",
      "epoch:4 step:3754 [D loss: 0.648641, acc: 64.84%] [G loss: 2.840784]\n",
      "epoch:4 step:3755 [D loss: 0.568936, acc: 73.44%] [G loss: 2.691812]\n",
      "epoch:4 step:3756 [D loss: 0.546325, acc: 69.53%] [G loss: 2.733603]\n",
      "epoch:4 step:3757 [D loss: 0.536281, acc: 72.66%] [G loss: 2.883700]\n",
      "epoch:4 step:3758 [D loss: 0.630191, acc: 67.19%] [G loss: 2.968330]\n",
      "epoch:4 step:3759 [D loss: 0.641334, acc: 66.41%] [G loss: 3.005353]\n",
      "epoch:4 step:3760 [D loss: 0.546236, acc: 69.53%] [G loss: 3.200539]\n",
      "epoch:4 step:3761 [D loss: 0.575969, acc: 71.88%] [G loss: 2.949150]\n",
      "epoch:4 step:3762 [D loss: 0.616062, acc: 63.28%] [G loss: 2.703717]\n",
      "epoch:4 step:3763 [D loss: 0.501793, acc: 78.12%] [G loss: 2.812707]\n",
      "epoch:4 step:3764 [D loss: 0.499820, acc: 78.12%] [G loss: 2.924139]\n",
      "epoch:4 step:3765 [D loss: 0.616766, acc: 68.75%] [G loss: 2.659654]\n",
      "epoch:4 step:3766 [D loss: 0.630416, acc: 62.50%] [G loss: 2.600608]\n",
      "epoch:4 step:3767 [D loss: 0.589188, acc: 71.09%] [G loss: 2.665060]\n",
      "epoch:4 step:3768 [D loss: 0.678397, acc: 60.16%] [G loss: 2.535542]\n",
      "epoch:4 step:3769 [D loss: 0.570908, acc: 71.88%] [G loss: 2.507873]\n",
      "epoch:4 step:3770 [D loss: 0.533573, acc: 72.66%] [G loss: 2.693494]\n",
      "epoch:4 step:3771 [D loss: 0.675829, acc: 67.97%] [G loss: 2.974944]\n",
      "epoch:4 step:3772 [D loss: 0.522733, acc: 76.56%] [G loss: 2.869538]\n",
      "epoch:4 step:3773 [D loss: 0.512429, acc: 80.47%] [G loss: 2.766799]\n",
      "epoch:4 step:3774 [D loss: 0.646780, acc: 66.41%] [G loss: 2.469332]\n",
      "epoch:4 step:3775 [D loss: 0.573430, acc: 73.44%] [G loss: 2.765914]\n",
      "epoch:4 step:3776 [D loss: 0.519900, acc: 73.44%] [G loss: 2.703716]\n",
      "epoch:4 step:3777 [D loss: 0.576522, acc: 70.31%] [G loss: 2.735668]\n",
      "epoch:4 step:3778 [D loss: 0.558342, acc: 73.44%] [G loss: 2.593175]\n",
      "epoch:4 step:3779 [D loss: 0.588590, acc: 68.75%] [G loss: 2.777478]\n",
      "epoch:4 step:3780 [D loss: 0.554926, acc: 73.44%] [G loss: 2.725659]\n",
      "epoch:4 step:3781 [D loss: 0.514212, acc: 72.66%] [G loss: 3.034712]\n",
      "epoch:4 step:3782 [D loss: 0.574105, acc: 67.97%] [G loss: 2.631784]\n",
      "epoch:4 step:3783 [D loss: 0.440044, acc: 82.03%] [G loss: 2.907589]\n",
      "epoch:4 step:3784 [D loss: 0.548809, acc: 73.44%] [G loss: 2.946263]\n",
      "epoch:4 step:3785 [D loss: 0.491721, acc: 75.78%] [G loss: 3.089343]\n",
      "epoch:4 step:3786 [D loss: 0.592436, acc: 69.53%] [G loss: 3.126457]\n",
      "epoch:4 step:3787 [D loss: 0.518655, acc: 73.44%] [G loss: 3.272587]\n",
      "epoch:4 step:3788 [D loss: 0.541523, acc: 71.88%] [G loss: 3.141890]\n",
      "epoch:4 step:3789 [D loss: 0.544939, acc: 70.31%] [G loss: 2.729461]\n",
      "epoch:4 step:3790 [D loss: 0.562622, acc: 75.00%] [G loss: 2.775012]\n",
      "epoch:4 step:3791 [D loss: 0.604054, acc: 64.84%] [G loss: 3.108039]\n",
      "epoch:4 step:3792 [D loss: 0.618932, acc: 66.41%] [G loss: 2.485915]\n",
      "epoch:4 step:3793 [D loss: 0.636774, acc: 60.16%] [G loss: 2.618885]\n",
      "epoch:4 step:3794 [D loss: 0.623730, acc: 65.62%] [G loss: 2.508159]\n",
      "epoch:4 step:3795 [D loss: 0.542130, acc: 74.22%] [G loss: 3.081145]\n",
      "epoch:4 step:3796 [D loss: 0.652436, acc: 67.97%] [G loss: 2.637373]\n",
      "epoch:4 step:3797 [D loss: 0.580013, acc: 69.53%] [G loss: 2.762941]\n",
      "epoch:4 step:3798 [D loss: 0.518387, acc: 76.56%] [G loss: 2.692255]\n",
      "epoch:4 step:3799 [D loss: 0.666345, acc: 63.28%] [G loss: 2.426396]\n",
      "epoch:4 step:3800 [D loss: 0.587492, acc: 67.19%] [G loss: 2.482864]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.073127\n",
      "FID: 61.286968\n",
      "0 = 13.429775022602081\n",
      "1 = 0.09276958256793603\n",
      "2 = 0.9380000233650208\n",
      "3 = 0.9354000091552734\n",
      "4 = 0.9405999779701233\n",
      "5 = 0.9402894973754883\n",
      "6 = 0.9354000091552734\n",
      "7 = 9.79738650584224\n",
      "8 = 0.15982206881821875\n",
      "9 = 0.8711000084877014\n",
      "10 = 0.871999979019165\n",
      "11 = 0.870199978351593\n",
      "12 = 0.8704332113265991\n",
      "13 = 0.871999979019165\n",
      "14 = 5.0731401443481445\n",
      "15 = 8.378464698791504\n",
      "16 = 0.31095564365386963\n",
      "17 = 5.073127269744873\n",
      "18 = 61.28696823120117\n",
      "epoch:4 step:3801 [D loss: 0.553834, acc: 71.88%] [G loss: 2.692186]\n",
      "epoch:4 step:3802 [D loss: 0.571762, acc: 70.31%] [G loss: 2.942311]\n",
      "epoch:4 step:3803 [D loss: 0.522057, acc: 70.31%] [G loss: 2.937269]\n",
      "epoch:4 step:3804 [D loss: 0.548484, acc: 74.22%] [G loss: 2.619766]\n",
      "epoch:4 step:3805 [D loss: 0.527633, acc: 75.78%] [G loss: 3.070024]\n",
      "epoch:4 step:3806 [D loss: 0.615198, acc: 67.19%] [G loss: 2.810899]\n",
      "epoch:4 step:3807 [D loss: 0.547106, acc: 71.88%] [G loss: 3.116414]\n",
      "epoch:4 step:3808 [D loss: 0.508110, acc: 78.12%] [G loss: 3.065807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3809 [D loss: 0.497915, acc: 76.56%] [G loss: 3.119817]\n",
      "epoch:4 step:3810 [D loss: 0.573704, acc: 69.53%] [G loss: 2.854369]\n",
      "epoch:4 step:3811 [D loss: 0.570425, acc: 71.88%] [G loss: 2.684805]\n",
      "epoch:4 step:3812 [D loss: 0.656046, acc: 64.06%] [G loss: 2.641794]\n",
      "epoch:4 step:3813 [D loss: 0.581287, acc: 65.62%] [G loss: 2.631379]\n",
      "epoch:4 step:3814 [D loss: 0.636579, acc: 64.84%] [G loss: 2.540101]\n",
      "epoch:4 step:3815 [D loss: 0.572345, acc: 67.19%] [G loss: 2.837169]\n",
      "epoch:4 step:3816 [D loss: 0.627170, acc: 64.06%] [G loss: 2.733941]\n",
      "epoch:4 step:3817 [D loss: 0.570705, acc: 65.62%] [G loss: 2.860713]\n",
      "epoch:4 step:3818 [D loss: 0.490128, acc: 78.12%] [G loss: 2.957780]\n",
      "epoch:4 step:3819 [D loss: 0.579813, acc: 66.41%] [G loss: 2.883416]\n",
      "epoch:4 step:3820 [D loss: 0.504335, acc: 78.12%] [G loss: 3.175401]\n",
      "epoch:4 step:3821 [D loss: 0.556104, acc: 72.66%] [G loss: 2.822624]\n",
      "epoch:4 step:3822 [D loss: 0.579705, acc: 66.41%] [G loss: 2.536487]\n",
      "epoch:4 step:3823 [D loss: 0.552988, acc: 71.88%] [G loss: 3.168696]\n",
      "epoch:4 step:3824 [D loss: 0.542164, acc: 73.44%] [G loss: 3.331938]\n",
      "epoch:4 step:3825 [D loss: 0.376119, acc: 81.25%] [G loss: 3.712558]\n",
      "epoch:4 step:3826 [D loss: 0.703848, acc: 59.38%] [G loss: 3.016585]\n",
      "epoch:4 step:3827 [D loss: 0.615701, acc: 71.09%] [G loss: 2.497183]\n",
      "epoch:4 step:3828 [D loss: 0.626483, acc: 64.84%] [G loss: 2.851679]\n",
      "epoch:4 step:3829 [D loss: 0.506413, acc: 71.09%] [G loss: 2.902637]\n",
      "epoch:4 step:3830 [D loss: 0.482031, acc: 76.56%] [G loss: 3.053065]\n",
      "epoch:4 step:3831 [D loss: 0.555102, acc: 72.66%] [G loss: 2.879556]\n",
      "epoch:4 step:3832 [D loss: 0.610189, acc: 65.62%] [G loss: 2.562418]\n",
      "epoch:4 step:3833 [D loss: 0.554757, acc: 69.53%] [G loss: 2.821907]\n",
      "epoch:4 step:3834 [D loss: 0.603331, acc: 67.19%] [G loss: 2.644615]\n",
      "epoch:4 step:3835 [D loss: 0.605894, acc: 68.75%] [G loss: 2.884724]\n",
      "epoch:4 step:3836 [D loss: 0.608411, acc: 64.84%] [G loss: 2.672572]\n",
      "epoch:4 step:3837 [D loss: 0.512485, acc: 73.44%] [G loss: 2.960048]\n",
      "epoch:4 step:3838 [D loss: 0.561314, acc: 69.53%] [G loss: 2.880563]\n",
      "epoch:4 step:3839 [D loss: 0.528958, acc: 73.44%] [G loss: 2.750387]\n",
      "epoch:4 step:3840 [D loss: 0.491701, acc: 80.47%] [G loss: 3.013435]\n",
      "epoch:4 step:3841 [D loss: 0.496363, acc: 76.56%] [G loss: 3.058124]\n",
      "epoch:4 step:3842 [D loss: 0.591785, acc: 72.66%] [G loss: 2.844734]\n",
      "epoch:4 step:3843 [D loss: 0.681455, acc: 62.50%] [G loss: 2.835918]\n",
      "epoch:4 step:3844 [D loss: 0.528459, acc: 72.66%] [G loss: 3.112970]\n",
      "epoch:4 step:3845 [D loss: 0.587645, acc: 71.88%] [G loss: 2.716676]\n",
      "epoch:4 step:3846 [D loss: 0.485775, acc: 78.91%] [G loss: 2.954000]\n",
      "epoch:4 step:3847 [D loss: 0.575275, acc: 68.75%] [G loss: 2.791253]\n",
      "epoch:4 step:3848 [D loss: 0.564357, acc: 75.78%] [G loss: 3.076805]\n",
      "epoch:4 step:3849 [D loss: 0.506679, acc: 75.78%] [G loss: 2.965095]\n",
      "epoch:4 step:3850 [D loss: 0.623675, acc: 64.84%] [G loss: 2.751245]\n",
      "epoch:4 step:3851 [D loss: 0.513556, acc: 77.34%] [G loss: 3.523923]\n",
      "epoch:4 step:3852 [D loss: 0.673317, acc: 64.06%] [G loss: 2.826956]\n",
      "epoch:4 step:3853 [D loss: 0.537983, acc: 69.53%] [G loss: 2.789354]\n",
      "epoch:4 step:3854 [D loss: 0.627102, acc: 69.53%] [G loss: 2.555325]\n",
      "epoch:4 step:3855 [D loss: 0.671042, acc: 57.03%] [G loss: 2.330518]\n",
      "epoch:4 step:3856 [D loss: 0.591189, acc: 67.97%] [G loss: 2.756763]\n",
      "epoch:4 step:3857 [D loss: 0.654955, acc: 64.06%] [G loss: 2.599200]\n",
      "epoch:4 step:3858 [D loss: 0.585744, acc: 71.09%] [G loss: 2.473205]\n",
      "epoch:4 step:3859 [D loss: 0.542822, acc: 73.44%] [G loss: 2.890188]\n",
      "epoch:4 step:3860 [D loss: 0.574884, acc: 70.31%] [G loss: 2.947305]\n",
      "epoch:4 step:3861 [D loss: 0.672382, acc: 60.16%] [G loss: 2.526184]\n",
      "epoch:4 step:3862 [D loss: 0.642637, acc: 64.84%] [G loss: 2.350010]\n",
      "epoch:4 step:3863 [D loss: 0.578287, acc: 70.31%] [G loss: 2.729833]\n",
      "epoch:4 step:3864 [D loss: 0.624802, acc: 64.06%] [G loss: 2.238915]\n",
      "epoch:4 step:3865 [D loss: 0.603794, acc: 67.97%] [G loss: 2.627192]\n",
      "epoch:4 step:3866 [D loss: 0.585375, acc: 67.97%] [G loss: 2.806469]\n",
      "epoch:4 step:3867 [D loss: 0.500372, acc: 81.25%] [G loss: 2.950694]\n",
      "epoch:4 step:3868 [D loss: 0.610288, acc: 73.44%] [G loss: 2.970770]\n",
      "epoch:4 step:3869 [D loss: 0.569923, acc: 71.88%] [G loss: 2.458538]\n",
      "epoch:4 step:3870 [D loss: 0.629812, acc: 61.72%] [G loss: 2.823151]\n",
      "epoch:4 step:3871 [D loss: 0.517531, acc: 73.44%] [G loss: 2.708725]\n",
      "epoch:4 step:3872 [D loss: 0.683764, acc: 66.41%] [G loss: 2.495112]\n",
      "epoch:4 step:3873 [D loss: 0.579641, acc: 72.66%] [G loss: 2.640087]\n",
      "epoch:4 step:3874 [D loss: 0.675019, acc: 61.72%] [G loss: 2.661572]\n",
      "epoch:4 step:3875 [D loss: 0.652299, acc: 64.06%] [G loss: 2.408221]\n",
      "epoch:4 step:3876 [D loss: 0.592487, acc: 64.84%] [G loss: 2.862802]\n",
      "epoch:4 step:3877 [D loss: 0.700228, acc: 60.16%] [G loss: 2.484404]\n",
      "epoch:4 step:3878 [D loss: 0.583744, acc: 71.88%] [G loss: 2.575708]\n",
      "epoch:4 step:3879 [D loss: 0.501689, acc: 75.00%] [G loss: 2.841996]\n",
      "epoch:4 step:3880 [D loss: 0.611287, acc: 66.41%] [G loss: 2.639975]\n",
      "epoch:4 step:3881 [D loss: 0.608023, acc: 66.41%] [G loss: 2.603802]\n",
      "epoch:4 step:3882 [D loss: 0.601985, acc: 65.62%] [G loss: 2.595719]\n",
      "epoch:4 step:3883 [D loss: 0.576053, acc: 73.44%] [G loss: 2.819879]\n",
      "epoch:4 step:3884 [D loss: 0.556084, acc: 73.44%] [G loss: 2.665905]\n",
      "epoch:4 step:3885 [D loss: 0.607247, acc: 64.84%] [G loss: 2.509727]\n",
      "epoch:4 step:3886 [D loss: 0.568357, acc: 71.88%] [G loss: 2.593499]\n",
      "epoch:4 step:3887 [D loss: 0.657868, acc: 62.50%] [G loss: 2.613525]\n",
      "epoch:4 step:3888 [D loss: 0.607998, acc: 67.19%] [G loss: 2.573928]\n",
      "epoch:4 step:3889 [D loss: 0.522882, acc: 77.34%] [G loss: 2.783859]\n",
      "epoch:4 step:3890 [D loss: 0.521521, acc: 72.66%] [G loss: 2.682636]\n",
      "epoch:4 step:3891 [D loss: 0.576483, acc: 66.41%] [G loss: 2.709468]\n",
      "epoch:4 step:3892 [D loss: 0.517879, acc: 78.12%] [G loss: 2.976783]\n",
      "epoch:4 step:3893 [D loss: 0.526875, acc: 74.22%] [G loss: 3.069426]\n",
      "epoch:4 step:3894 [D loss: 0.624708, acc: 65.62%] [G loss: 2.819016]\n",
      "epoch:4 step:3895 [D loss: 0.607722, acc: 68.75%] [G loss: 2.803700]\n",
      "epoch:4 step:3896 [D loss: 0.660831, acc: 67.19%] [G loss: 2.600990]\n",
      "epoch:4 step:3897 [D loss: 0.558230, acc: 65.62%] [G loss: 3.154330]\n",
      "epoch:4 step:3898 [D loss: 0.696509, acc: 64.06%] [G loss: 3.089309]\n",
      "epoch:4 step:3899 [D loss: 0.586392, acc: 67.97%] [G loss: 3.212664]\n",
      "epoch:4 step:3900 [D loss: 0.545590, acc: 72.66%] [G loss: 3.120909]\n",
      "epoch:4 step:3901 [D loss: 0.603092, acc: 71.88%] [G loss: 2.514122]\n",
      "epoch:4 step:3902 [D loss: 0.604429, acc: 73.44%] [G loss: 2.829429]\n",
      "epoch:4 step:3903 [D loss: 0.519833, acc: 75.78%] [G loss: 2.620599]\n",
      "epoch:4 step:3904 [D loss: 0.574224, acc: 71.88%] [G loss: 2.736843]\n",
      "epoch:4 step:3905 [D loss: 0.471908, acc: 79.69%] [G loss: 2.841280]\n",
      "epoch:4 step:3906 [D loss: 0.602023, acc: 70.31%] [G loss: 2.494686]\n",
      "epoch:4 step:3907 [D loss: 0.572298, acc: 69.53%] [G loss: 3.088709]\n",
      "epoch:4 step:3908 [D loss: 0.763751, acc: 48.44%] [G loss: 2.410953]\n",
      "epoch:4 step:3909 [D loss: 0.627041, acc: 63.28%] [G loss: 2.567223]\n",
      "epoch:4 step:3910 [D loss: 0.525985, acc: 75.00%] [G loss: 3.046791]\n",
      "epoch:4 step:3911 [D loss: 0.539983, acc: 75.78%] [G loss: 2.581047]\n",
      "epoch:4 step:3912 [D loss: 0.523604, acc: 72.66%] [G loss: 2.640489]\n",
      "epoch:4 step:3913 [D loss: 0.557335, acc: 74.22%] [G loss: 2.836168]\n",
      "epoch:4 step:3914 [D loss: 0.613662, acc: 69.53%] [G loss: 2.866545]\n",
      "epoch:4 step:3915 [D loss: 0.601716, acc: 64.06%] [G loss: 2.883229]\n",
      "epoch:4 step:3916 [D loss: 0.528185, acc: 75.00%] [G loss: 2.918241]\n",
      "epoch:4 step:3917 [D loss: 0.664357, acc: 66.41%] [G loss: 2.787372]\n",
      "epoch:4 step:3918 [D loss: 0.615787, acc: 64.06%] [G loss: 2.726634]\n",
      "epoch:4 step:3919 [D loss: 0.586776, acc: 72.66%] [G loss: 2.558308]\n",
      "epoch:4 step:3920 [D loss: 0.585444, acc: 71.09%] [G loss: 2.970117]\n",
      "epoch:4 step:3921 [D loss: 0.500575, acc: 76.56%] [G loss: 2.812420]\n",
      "epoch:4 step:3922 [D loss: 0.516025, acc: 76.56%] [G loss: 2.963669]\n",
      "epoch:4 step:3923 [D loss: 0.641710, acc: 65.62%] [G loss: 2.889608]\n",
      "epoch:4 step:3924 [D loss: 0.593141, acc: 67.19%] [G loss: 3.001550]\n",
      "epoch:4 step:3925 [D loss: 0.585551, acc: 72.66%] [G loss: 2.975085]\n",
      "epoch:4 step:3926 [D loss: 0.594692, acc: 75.78%] [G loss: 3.064146]\n",
      "epoch:4 step:3927 [D loss: 0.531093, acc: 71.88%] [G loss: 2.809237]\n",
      "epoch:4 step:3928 [D loss: 0.687051, acc: 64.06%] [G loss: 2.726861]\n",
      "epoch:4 step:3929 [D loss: 0.565728, acc: 69.53%] [G loss: 2.664323]\n",
      "epoch:4 step:3930 [D loss: 0.679614, acc: 60.16%] [G loss: 2.439801]\n",
      "epoch:4 step:3931 [D loss: 0.604546, acc: 67.19%] [G loss: 2.540929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3932 [D loss: 0.598682, acc: 67.19%] [G loss: 2.519499]\n",
      "epoch:4 step:3933 [D loss: 0.722321, acc: 60.16%] [G loss: 2.367917]\n",
      "epoch:4 step:3934 [D loss: 0.614353, acc: 65.62%] [G loss: 2.375828]\n",
      "epoch:4 step:3935 [D loss: 0.620336, acc: 68.75%] [G loss: 2.470896]\n",
      "epoch:4 step:3936 [D loss: 0.604315, acc: 63.28%] [G loss: 2.726454]\n",
      "epoch:4 step:3937 [D loss: 0.630121, acc: 68.75%] [G loss: 2.561222]\n",
      "epoch:4 step:3938 [D loss: 0.552227, acc: 73.44%] [G loss: 2.654482]\n",
      "epoch:4 step:3939 [D loss: 0.549752, acc: 69.53%] [G loss: 2.792698]\n",
      "epoch:4 step:3940 [D loss: 0.545810, acc: 71.88%] [G loss: 2.651428]\n",
      "epoch:4 step:3941 [D loss: 0.632369, acc: 68.75%] [G loss: 2.561475]\n",
      "epoch:4 step:3942 [D loss: 0.518581, acc: 78.12%] [G loss: 3.109301]\n",
      "epoch:4 step:3943 [D loss: 0.605439, acc: 64.06%] [G loss: 2.943164]\n",
      "epoch:4 step:3944 [D loss: 0.641589, acc: 63.28%] [G loss: 2.472484]\n",
      "epoch:4 step:3945 [D loss: 0.653053, acc: 61.72%] [G loss: 2.596262]\n",
      "epoch:4 step:3946 [D loss: 0.565686, acc: 74.22%] [G loss: 2.754372]\n",
      "epoch:4 step:3947 [D loss: 0.607242, acc: 63.28%] [G loss: 2.697383]\n",
      "epoch:4 step:3948 [D loss: 0.644828, acc: 59.38%] [G loss: 2.717718]\n",
      "epoch:4 step:3949 [D loss: 0.644755, acc: 65.62%] [G loss: 2.564485]\n",
      "epoch:4 step:3950 [D loss: 0.565328, acc: 70.31%] [G loss: 2.438903]\n",
      "epoch:4 step:3951 [D loss: 0.589277, acc: 69.53%] [G loss: 2.611995]\n",
      "epoch:4 step:3952 [D loss: 0.613189, acc: 67.97%] [G loss: 2.456574]\n",
      "epoch:4 step:3953 [D loss: 0.503893, acc: 77.34%] [G loss: 2.513408]\n",
      "epoch:4 step:3954 [D loss: 0.586520, acc: 67.97%] [G loss: 2.678872]\n",
      "epoch:4 step:3955 [D loss: 0.487306, acc: 75.78%] [G loss: 3.007837]\n",
      "epoch:4 step:3956 [D loss: 0.468914, acc: 78.12%] [G loss: 3.006193]\n",
      "epoch:4 step:3957 [D loss: 0.606617, acc: 71.09%] [G loss: 2.860852]\n",
      "epoch:4 step:3958 [D loss: 0.610166, acc: 64.84%] [G loss: 2.700350]\n",
      "epoch:4 step:3959 [D loss: 0.740037, acc: 61.72%] [G loss: 2.513276]\n",
      "epoch:4 step:3960 [D loss: 0.593171, acc: 68.75%] [G loss: 2.888441]\n",
      "epoch:4 step:3961 [D loss: 0.563146, acc: 71.09%] [G loss: 2.647894]\n",
      "epoch:4 step:3962 [D loss: 0.682918, acc: 62.50%] [G loss: 2.625877]\n",
      "epoch:4 step:3963 [D loss: 0.700832, acc: 60.94%] [G loss: 2.753687]\n",
      "epoch:4 step:3964 [D loss: 0.705387, acc: 63.28%] [G loss: 2.480108]\n",
      "epoch:4 step:3965 [D loss: 0.578283, acc: 69.53%] [G loss: 2.631904]\n",
      "epoch:4 step:3966 [D loss: 0.522337, acc: 73.44%] [G loss: 2.520190]\n",
      "epoch:4 step:3967 [D loss: 0.548089, acc: 74.22%] [G loss: 2.667322]\n",
      "epoch:4 step:3968 [D loss: 0.654705, acc: 65.62%] [G loss: 2.615805]\n",
      "epoch:4 step:3969 [D loss: 0.650396, acc: 60.94%] [G loss: 2.794195]\n",
      "epoch:4 step:3970 [D loss: 0.615802, acc: 65.62%] [G loss: 2.724399]\n",
      "epoch:4 step:3971 [D loss: 0.494516, acc: 78.12%] [G loss: 2.942359]\n",
      "epoch:4 step:3972 [D loss: 0.651615, acc: 66.41%] [G loss: 2.627967]\n",
      "epoch:4 step:3973 [D loss: 0.666389, acc: 61.72%] [G loss: 2.437981]\n",
      "epoch:4 step:3974 [D loss: 0.563128, acc: 68.75%] [G loss: 2.635339]\n",
      "epoch:4 step:3975 [D loss: 0.686755, acc: 58.59%] [G loss: 2.374862]\n",
      "epoch:4 step:3976 [D loss: 0.585919, acc: 67.97%] [G loss: 2.449880]\n",
      "epoch:4 step:3977 [D loss: 0.562507, acc: 72.66%] [G loss: 2.688569]\n",
      "epoch:4 step:3978 [D loss: 0.537995, acc: 71.09%] [G loss: 3.150130]\n",
      "epoch:4 step:3979 [D loss: 0.511155, acc: 72.66%] [G loss: 2.853444]\n",
      "epoch:4 step:3980 [D loss: 0.479212, acc: 78.12%] [G loss: 3.480303]\n",
      "epoch:4 step:3981 [D loss: 0.531636, acc: 72.66%] [G loss: 3.008437]\n",
      "epoch:4 step:3982 [D loss: 0.641049, acc: 67.97%] [G loss: 2.631698]\n",
      "epoch:4 step:3983 [D loss: 0.630230, acc: 60.16%] [G loss: 2.658632]\n",
      "epoch:4 step:3984 [D loss: 0.572653, acc: 74.22%] [G loss: 2.420386]\n",
      "epoch:4 step:3985 [D loss: 0.566746, acc: 71.88%] [G loss: 2.712027]\n",
      "epoch:4 step:3986 [D loss: 0.582235, acc: 71.88%] [G loss: 2.561975]\n",
      "epoch:4 step:3987 [D loss: 0.606185, acc: 72.66%] [G loss: 2.559981]\n",
      "epoch:4 step:3988 [D loss: 0.680189, acc: 64.06%] [G loss: 2.696836]\n",
      "epoch:4 step:3989 [D loss: 0.570512, acc: 74.22%] [G loss: 2.381024]\n",
      "epoch:4 step:3990 [D loss: 0.522683, acc: 76.56%] [G loss: 2.394347]\n",
      "epoch:4 step:3991 [D loss: 0.581353, acc: 67.19%] [G loss: 2.657238]\n",
      "epoch:4 step:3992 [D loss: 0.600527, acc: 72.66%] [G loss: 2.651691]\n",
      "epoch:4 step:3993 [D loss: 0.590026, acc: 69.53%] [G loss: 2.641260]\n",
      "epoch:4 step:3994 [D loss: 0.607199, acc: 67.19%] [G loss: 2.375380]\n",
      "epoch:4 step:3995 [D loss: 0.630005, acc: 66.41%] [G loss: 2.655251]\n",
      "epoch:4 step:3996 [D loss: 0.503933, acc: 74.22%] [G loss: 2.398503]\n",
      "epoch:4 step:3997 [D loss: 0.695989, acc: 60.16%] [G loss: 2.426256]\n",
      "epoch:4 step:3998 [D loss: 0.565730, acc: 71.09%] [G loss: 2.428674]\n",
      "epoch:4 step:3999 [D loss: 0.613132, acc: 66.41%] [G loss: 2.531188]\n",
      "epoch:4 step:4000 [D loss: 0.603058, acc: 64.84%] [G loss: 2.419994]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.239450\n",
      "FID: 56.275932\n",
      "0 = 13.428660338878629\n",
      "1 = 0.09424397347607993\n",
      "2 = 0.9347000122070312\n",
      "3 = 0.9300000071525574\n",
      "4 = 0.9394000172615051\n",
      "5 = 0.9388249516487122\n",
      "6 = 0.9300000071525574\n",
      "7 = 9.539975894737266\n",
      "8 = 0.1500315156253751\n",
      "9 = 0.8603000044822693\n",
      "10 = 0.8593999743461609\n",
      "11 = 0.8611999750137329\n",
      "12 = 0.860949695110321\n",
      "13 = 0.8593999743461609\n",
      "14 = 5.23946475982666\n",
      "15 = 8.43215560913086\n",
      "16 = 0.294341117143631\n",
      "17 = 5.239450454711914\n",
      "18 = 56.27593231201172\n",
      "epoch:4 step:4001 [D loss: 0.520010, acc: 71.09%] [G loss: 2.314866]\n",
      "epoch:4 step:4002 [D loss: 0.472848, acc: 75.78%] [G loss: 2.850429]\n",
      "epoch:4 step:4003 [D loss: 0.547987, acc: 67.19%] [G loss: 2.783061]\n",
      "epoch:4 step:4004 [D loss: 0.549925, acc: 69.53%] [G loss: 2.428948]\n",
      "epoch:4 step:4005 [D loss: 0.586604, acc: 68.75%] [G loss: 2.709724]\n",
      "epoch:4 step:4006 [D loss: 0.517773, acc: 71.88%] [G loss: 2.794308]\n",
      "epoch:4 step:4007 [D loss: 0.577186, acc: 71.09%] [G loss: 2.978213]\n",
      "epoch:4 step:4008 [D loss: 0.500898, acc: 78.12%] [G loss: 2.928016]\n",
      "epoch:4 step:4009 [D loss: 0.563460, acc: 72.66%] [G loss: 3.370946]\n",
      "epoch:4 step:4010 [D loss: 0.603399, acc: 65.62%] [G loss: 2.814058]\n",
      "epoch:4 step:4011 [D loss: 0.663023, acc: 62.50%] [G loss: 2.603489]\n",
      "epoch:4 step:4012 [D loss: 0.647765, acc: 68.75%] [G loss: 2.637341]\n",
      "epoch:4 step:4013 [D loss: 0.525433, acc: 75.78%] [G loss: 2.646148]\n",
      "epoch:4 step:4014 [D loss: 0.604945, acc: 71.09%] [G loss: 2.933567]\n",
      "epoch:4 step:4015 [D loss: 0.609800, acc: 67.19%] [G loss: 2.801800]\n",
      "epoch:4 step:4016 [D loss: 0.563127, acc: 71.09%] [G loss: 2.532392]\n",
      "epoch:4 step:4017 [D loss: 0.598569, acc: 69.53%] [G loss: 2.808155]\n",
      "epoch:4 step:4018 [D loss: 0.648124, acc: 64.06%] [G loss: 2.674453]\n",
      "epoch:4 step:4019 [D loss: 0.585795, acc: 73.44%] [G loss: 2.731933]\n",
      "epoch:4 step:4020 [D loss: 0.570528, acc: 67.97%] [G loss: 2.393607]\n",
      "epoch:4 step:4021 [D loss: 0.528321, acc: 71.88%] [G loss: 2.651823]\n",
      "epoch:4 step:4022 [D loss: 0.630252, acc: 61.72%] [G loss: 2.784445]\n",
      "epoch:4 step:4023 [D loss: 0.587154, acc: 65.62%] [G loss: 2.314467]\n",
      "epoch:4 step:4024 [D loss: 0.573848, acc: 71.09%] [G loss: 2.811055]\n",
      "epoch:4 step:4025 [D loss: 0.624646, acc: 67.19%] [G loss: 2.754761]\n",
      "epoch:4 step:4026 [D loss: 0.618134, acc: 72.66%] [G loss: 2.508841]\n",
      "epoch:4 step:4027 [D loss: 0.610445, acc: 67.97%] [G loss: 3.086536]\n",
      "epoch:4 step:4028 [D loss: 0.556827, acc: 71.88%] [G loss: 2.678695]\n",
      "epoch:4 step:4029 [D loss: 0.667679, acc: 60.16%] [G loss: 2.645848]\n",
      "epoch:4 step:4030 [D loss: 0.594768, acc: 66.41%] [G loss: 2.545719]\n",
      "epoch:4 step:4031 [D loss: 0.561934, acc: 69.53%] [G loss: 2.643556]\n",
      "epoch:4 step:4032 [D loss: 0.574050, acc: 69.53%] [G loss: 2.856369]\n",
      "epoch:4 step:4033 [D loss: 0.561579, acc: 68.75%] [G loss: 2.685716]\n",
      "epoch:4 step:4034 [D loss: 0.608963, acc: 64.06%] [G loss: 2.879142]\n",
      "epoch:4 step:4035 [D loss: 0.548488, acc: 73.44%] [G loss: 2.795136]\n",
      "epoch:4 step:4036 [D loss: 0.677447, acc: 60.16%] [G loss: 2.705347]\n",
      "epoch:4 step:4037 [D loss: 0.641739, acc: 65.62%] [G loss: 2.808067]\n",
      "epoch:4 step:4038 [D loss: 0.583318, acc: 70.31%] [G loss: 2.668820]\n",
      "epoch:4 step:4039 [D loss: 0.577102, acc: 71.88%] [G loss: 2.440676]\n",
      "epoch:4 step:4040 [D loss: 0.616047, acc: 69.53%] [G loss: 2.758401]\n",
      "epoch:4 step:4041 [D loss: 0.519139, acc: 74.22%] [G loss: 2.788698]\n",
      "epoch:4 step:4042 [D loss: 0.517777, acc: 74.22%] [G loss: 2.778456]\n",
      "epoch:4 step:4043 [D loss: 0.580069, acc: 74.22%] [G loss: 2.619946]\n",
      "epoch:4 step:4044 [D loss: 0.588442, acc: 69.53%] [G loss: 2.823849]\n",
      "epoch:4 step:4045 [D loss: 0.532093, acc: 71.09%] [G loss: 2.483041]\n",
      "epoch:4 step:4046 [D loss: 0.574321, acc: 71.09%] [G loss: 2.759703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4047 [D loss: 0.587065, acc: 73.44%] [G loss: 2.799235]\n",
      "epoch:4 step:4048 [D loss: 0.565206, acc: 72.66%] [G loss: 2.932505]\n",
      "epoch:4 step:4049 [D loss: 0.660214, acc: 60.16%] [G loss: 2.506343]\n",
      "epoch:4 step:4050 [D loss: 0.575788, acc: 71.09%] [G loss: 2.525479]\n",
      "epoch:4 step:4051 [D loss: 0.572419, acc: 64.06%] [G loss: 2.629795]\n",
      "epoch:4 step:4052 [D loss: 0.578844, acc: 73.44%] [G loss: 3.036465]\n",
      "epoch:4 step:4053 [D loss: 0.546608, acc: 71.09%] [G loss: 2.662078]\n",
      "epoch:4 step:4054 [D loss: 0.592240, acc: 68.75%] [G loss: 3.240062]\n",
      "epoch:4 step:4055 [D loss: 0.586904, acc: 70.31%] [G loss: 2.861968]\n",
      "epoch:4 step:4056 [D loss: 0.620364, acc: 71.88%] [G loss: 2.607218]\n",
      "epoch:4 step:4057 [D loss: 0.518415, acc: 73.44%] [G loss: 2.798983]\n",
      "epoch:4 step:4058 [D loss: 0.626712, acc: 65.62%] [G loss: 2.705595]\n",
      "epoch:4 step:4059 [D loss: 0.587311, acc: 73.44%] [G loss: 2.750911]\n",
      "epoch:4 step:4060 [D loss: 0.505311, acc: 75.78%] [G loss: 3.025556]\n",
      "epoch:4 step:4061 [D loss: 0.441832, acc: 78.12%] [G loss: 3.263499]\n",
      "epoch:4 step:4062 [D loss: 0.412914, acc: 82.03%] [G loss: 3.241713]\n",
      "epoch:4 step:4063 [D loss: 0.445165, acc: 79.69%] [G loss: 3.497693]\n",
      "epoch:4 step:4064 [D loss: 0.695392, acc: 62.50%] [G loss: 2.338950]\n",
      "epoch:4 step:4065 [D loss: 0.589481, acc: 66.41%] [G loss: 2.664858]\n",
      "epoch:4 step:4066 [D loss: 0.548391, acc: 75.00%] [G loss: 2.934963]\n",
      "epoch:4 step:4067 [D loss: 0.583399, acc: 71.09%] [G loss: 2.642999]\n",
      "epoch:4 step:4068 [D loss: 0.514384, acc: 77.34%] [G loss: 2.761097]\n",
      "epoch:4 step:4069 [D loss: 0.547936, acc: 72.66%] [G loss: 3.029159]\n",
      "epoch:4 step:4070 [D loss: 0.577186, acc: 67.19%] [G loss: 2.567939]\n",
      "epoch:4 step:4071 [D loss: 0.592651, acc: 67.97%] [G loss: 2.741673]\n",
      "epoch:4 step:4072 [D loss: 0.586773, acc: 68.75%] [G loss: 2.868526]\n",
      "epoch:4 step:4073 [D loss: 0.554704, acc: 72.66%] [G loss: 2.462328]\n",
      "epoch:4 step:4074 [D loss: 0.599387, acc: 70.31%] [G loss: 2.504474]\n",
      "epoch:4 step:4075 [D loss: 0.642929, acc: 70.31%] [G loss: 2.528785]\n",
      "epoch:4 step:4076 [D loss: 0.575467, acc: 71.88%] [G loss: 2.562701]\n",
      "epoch:4 step:4077 [D loss: 0.542675, acc: 75.78%] [G loss: 2.753443]\n",
      "epoch:4 step:4078 [D loss: 0.602025, acc: 72.66%] [G loss: 2.649207]\n",
      "epoch:4 step:4079 [D loss: 0.579614, acc: 72.66%] [G loss: 2.708560]\n",
      "epoch:4 step:4080 [D loss: 0.571155, acc: 73.44%] [G loss: 2.837704]\n",
      "epoch:4 step:4081 [D loss: 0.567321, acc: 73.44%] [G loss: 2.812659]\n",
      "epoch:4 step:4082 [D loss: 0.644345, acc: 64.84%] [G loss: 2.838000]\n",
      "epoch:4 step:4083 [D loss: 0.466683, acc: 78.12%] [G loss: 3.118337]\n",
      "epoch:4 step:4084 [D loss: 0.591405, acc: 65.62%] [G loss: 3.112390]\n",
      "epoch:4 step:4085 [D loss: 0.605069, acc: 67.97%] [G loss: 3.274613]\n",
      "epoch:4 step:4086 [D loss: 0.626264, acc: 65.62%] [G loss: 2.616009]\n",
      "epoch:4 step:4087 [D loss: 0.666563, acc: 66.41%] [G loss: 2.784455]\n",
      "epoch:4 step:4088 [D loss: 0.574690, acc: 72.66%] [G loss: 2.729841]\n",
      "epoch:4 step:4089 [D loss: 0.638674, acc: 67.97%] [G loss: 2.703015]\n",
      "epoch:4 step:4090 [D loss: 0.651736, acc: 67.19%] [G loss: 2.538364]\n",
      "epoch:4 step:4091 [D loss: 0.574202, acc: 71.88%] [G loss: 2.832664]\n",
      "epoch:4 step:4092 [D loss: 0.512926, acc: 75.78%] [G loss: 2.924776]\n",
      "epoch:4 step:4093 [D loss: 0.613301, acc: 65.62%] [G loss: 2.914120]\n",
      "epoch:4 step:4094 [D loss: 0.448571, acc: 80.47%] [G loss: 3.298779]\n",
      "epoch:4 step:4095 [D loss: 0.528745, acc: 71.88%] [G loss: 3.377603]\n",
      "epoch:4 step:4096 [D loss: 0.714646, acc: 58.59%] [G loss: 2.787107]\n",
      "epoch:4 step:4097 [D loss: 0.695727, acc: 62.50%] [G loss: 2.431728]\n",
      "epoch:4 step:4098 [D loss: 0.506478, acc: 73.44%] [G loss: 2.788752]\n",
      "epoch:4 step:4099 [D loss: 0.509789, acc: 77.34%] [G loss: 2.720435]\n",
      "epoch:4 step:4100 [D loss: 0.648539, acc: 67.19%] [G loss: 2.507010]\n",
      "epoch:4 step:4101 [D loss: 0.530243, acc: 76.56%] [G loss: 2.396776]\n",
      "epoch:4 step:4102 [D loss: 0.572037, acc: 71.88%] [G loss: 3.029859]\n",
      "epoch:4 step:4103 [D loss: 0.550134, acc: 69.53%] [G loss: 2.645655]\n",
      "epoch:4 step:4104 [D loss: 0.521046, acc: 77.34%] [G loss: 2.803371]\n",
      "epoch:4 step:4105 [D loss: 0.609589, acc: 70.31%] [G loss: 2.705865]\n",
      "epoch:4 step:4106 [D loss: 0.558079, acc: 71.09%] [G loss: 3.061374]\n",
      "epoch:4 step:4107 [D loss: 0.569690, acc: 69.53%] [G loss: 3.062599]\n",
      "epoch:4 step:4108 [D loss: 0.572861, acc: 71.09%] [G loss: 2.837748]\n",
      "epoch:4 step:4109 [D loss: 0.618655, acc: 67.97%] [G loss: 2.414455]\n",
      "epoch:4 step:4110 [D loss: 0.804201, acc: 52.34%] [G loss: 2.376301]\n",
      "epoch:4 step:4111 [D loss: 0.558028, acc: 71.88%] [G loss: 2.801088]\n",
      "epoch:4 step:4112 [D loss: 0.507415, acc: 78.12%] [G loss: 2.640503]\n",
      "epoch:4 step:4113 [D loss: 0.532063, acc: 75.78%] [G loss: 3.051332]\n",
      "epoch:4 step:4114 [D loss: 0.573636, acc: 71.09%] [G loss: 3.178433]\n",
      "epoch:4 step:4115 [D loss: 0.522534, acc: 75.00%] [G loss: 2.598877]\n",
      "epoch:4 step:4116 [D loss: 0.513377, acc: 73.44%] [G loss: 2.784895]\n",
      "epoch:4 step:4117 [D loss: 0.622935, acc: 70.31%] [G loss: 2.392088]\n",
      "epoch:4 step:4118 [D loss: 0.565070, acc: 71.09%] [G loss: 2.743309]\n",
      "epoch:4 step:4119 [D loss: 0.513995, acc: 76.56%] [G loss: 2.961539]\n",
      "epoch:4 step:4120 [D loss: 0.592987, acc: 66.41%] [G loss: 2.587402]\n",
      "epoch:4 step:4121 [D loss: 0.662387, acc: 60.16%] [G loss: 2.531925]\n",
      "epoch:4 step:4122 [D loss: 0.526939, acc: 72.66%] [G loss: 2.964086]\n",
      "epoch:4 step:4123 [D loss: 0.635949, acc: 68.75%] [G loss: 2.602186]\n",
      "epoch:4 step:4124 [D loss: 0.659193, acc: 62.50%] [G loss: 2.365366]\n",
      "epoch:4 step:4125 [D loss: 0.708336, acc: 60.94%] [G loss: 2.379356]\n",
      "epoch:4 step:4126 [D loss: 0.611775, acc: 68.75%] [G loss: 2.636582]\n",
      "epoch:4 step:4127 [D loss: 0.619777, acc: 67.19%] [G loss: 2.470340]\n",
      "epoch:4 step:4128 [D loss: 0.561624, acc: 75.00%] [G loss: 2.654226]\n",
      "epoch:4 step:4129 [D loss: 0.535732, acc: 75.00%] [G loss: 3.166108]\n",
      "epoch:4 step:4130 [D loss: 0.581140, acc: 70.31%] [G loss: 2.579515]\n",
      "epoch:4 step:4131 [D loss: 0.531285, acc: 69.53%] [G loss: 2.428500]\n",
      "epoch:4 step:4132 [D loss: 0.513032, acc: 79.69%] [G loss: 2.982041]\n",
      "epoch:4 step:4133 [D loss: 0.512481, acc: 75.00%] [G loss: 2.730611]\n",
      "epoch:4 step:4134 [D loss: 0.582850, acc: 68.75%] [G loss: 2.724970]\n",
      "epoch:4 step:4135 [D loss: 0.605811, acc: 66.41%] [G loss: 2.729764]\n",
      "epoch:4 step:4136 [D loss: 0.596923, acc: 68.75%] [G loss: 2.678515]\n",
      "epoch:4 step:4137 [D loss: 0.614233, acc: 67.19%] [G loss: 2.413297]\n",
      "epoch:4 step:4138 [D loss: 0.577103, acc: 70.31%] [G loss: 2.586684]\n",
      "epoch:4 step:4139 [D loss: 0.617761, acc: 63.28%] [G loss: 2.441031]\n",
      "epoch:4 step:4140 [D loss: 0.488042, acc: 79.69%] [G loss: 3.191338]\n",
      "epoch:4 step:4141 [D loss: 0.575068, acc: 68.75%] [G loss: 2.764113]\n",
      "epoch:4 step:4142 [D loss: 0.566976, acc: 70.31%] [G loss: 2.756669]\n",
      "epoch:4 step:4143 [D loss: 0.450787, acc: 80.47%] [G loss: 3.069286]\n",
      "epoch:4 step:4144 [D loss: 0.692114, acc: 59.38%] [G loss: 2.655705]\n",
      "epoch:4 step:4145 [D loss: 0.562400, acc: 72.66%] [G loss: 3.044644]\n",
      "epoch:4 step:4146 [D loss: 0.615497, acc: 67.97%] [G loss: 2.740988]\n",
      "epoch:4 step:4147 [D loss: 0.583438, acc: 70.31%] [G loss: 3.050881]\n",
      "epoch:4 step:4148 [D loss: 0.730554, acc: 61.72%] [G loss: 2.641456]\n",
      "epoch:4 step:4149 [D loss: 0.589544, acc: 70.31%] [G loss: 2.547747]\n",
      "epoch:4 step:4150 [D loss: 0.605301, acc: 64.06%] [G loss: 3.074304]\n",
      "epoch:4 step:4151 [D loss: 0.657548, acc: 65.62%] [G loss: 2.696110]\n",
      "epoch:4 step:4152 [D loss: 0.634658, acc: 62.50%] [G loss: 2.818464]\n",
      "epoch:4 step:4153 [D loss: 0.536843, acc: 70.31%] [G loss: 3.006061]\n",
      "epoch:4 step:4154 [D loss: 0.548640, acc: 73.44%] [G loss: 2.733681]\n",
      "epoch:4 step:4155 [D loss: 0.539008, acc: 71.09%] [G loss: 2.610045]\n",
      "epoch:4 step:4156 [D loss: 0.644844, acc: 65.62%] [G loss: 2.621871]\n",
      "epoch:4 step:4157 [D loss: 0.588741, acc: 71.88%] [G loss: 2.684072]\n",
      "epoch:4 step:4158 [D loss: 0.549708, acc: 70.31%] [G loss: 2.797860]\n",
      "epoch:4 step:4159 [D loss: 0.537730, acc: 75.00%] [G loss: 2.651507]\n",
      "epoch:4 step:4160 [D loss: 0.580392, acc: 75.78%] [G loss: 2.509511]\n",
      "epoch:4 step:4161 [D loss: 0.609554, acc: 73.44%] [G loss: 2.873044]\n",
      "epoch:4 step:4162 [D loss: 0.608860, acc: 69.53%] [G loss: 2.675317]\n",
      "epoch:4 step:4163 [D loss: 0.565926, acc: 71.88%] [G loss: 2.873089]\n",
      "epoch:4 step:4164 [D loss: 0.634124, acc: 69.53%] [G loss: 2.646314]\n",
      "epoch:4 step:4165 [D loss: 0.672780, acc: 66.41%] [G loss: 2.504748]\n",
      "epoch:4 step:4166 [D loss: 0.520298, acc: 73.44%] [G loss: 2.761990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4167 [D loss: 0.550246, acc: 76.56%] [G loss: 2.840054]\n",
      "epoch:4 step:4168 [D loss: 0.636601, acc: 62.50%] [G loss: 2.546826]\n",
      "epoch:4 step:4169 [D loss: 0.688013, acc: 63.28%] [G loss: 2.461063]\n",
      "epoch:4 step:4170 [D loss: 0.526160, acc: 73.44%] [G loss: 2.744255]\n",
      "epoch:4 step:4171 [D loss: 0.655301, acc: 62.50%] [G loss: 2.648261]\n",
      "epoch:4 step:4172 [D loss: 0.552886, acc: 73.44%] [G loss: 2.872514]\n",
      "epoch:4 step:4173 [D loss: 0.575561, acc: 74.22%] [G loss: 2.791400]\n",
      "epoch:4 step:4174 [D loss: 0.522372, acc: 71.09%] [G loss: 2.743918]\n",
      "epoch:4 step:4175 [D loss: 0.574625, acc: 69.53%] [G loss: 3.089860]\n",
      "epoch:4 step:4176 [D loss: 0.560496, acc: 75.78%] [G loss: 3.037262]\n",
      "epoch:4 step:4177 [D loss: 0.474155, acc: 81.25%] [G loss: 3.173233]\n",
      "epoch:4 step:4178 [D loss: 0.522067, acc: 75.00%] [G loss: 3.150710]\n",
      "epoch:4 step:4179 [D loss: 0.555122, acc: 72.66%] [G loss: 3.156130]\n",
      "epoch:4 step:4180 [D loss: 0.592344, acc: 74.22%] [G loss: 2.856914]\n",
      "epoch:4 step:4181 [D loss: 0.506134, acc: 78.91%] [G loss: 2.997055]\n",
      "epoch:4 step:4182 [D loss: 0.513307, acc: 72.66%] [G loss: 2.629623]\n",
      "epoch:4 step:4183 [D loss: 0.622012, acc: 69.53%] [G loss: 2.748880]\n",
      "epoch:4 step:4184 [D loss: 0.489569, acc: 76.56%] [G loss: 2.960970]\n",
      "epoch:4 step:4185 [D loss: 0.640543, acc: 71.09%] [G loss: 2.668990]\n",
      "epoch:4 step:4186 [D loss: 0.508311, acc: 74.22%] [G loss: 2.817180]\n",
      "epoch:4 step:4187 [D loss: 0.629717, acc: 64.84%] [G loss: 2.646431]\n",
      "epoch:4 step:4188 [D loss: 0.559118, acc: 72.66%] [G loss: 2.947223]\n",
      "epoch:4 step:4189 [D loss: 0.632653, acc: 62.50%] [G loss: 2.844160]\n",
      "epoch:4 step:4190 [D loss: 0.664447, acc: 61.72%] [G loss: 2.263899]\n",
      "epoch:4 step:4191 [D loss: 0.648895, acc: 64.06%] [G loss: 2.525589]\n",
      "epoch:4 step:4192 [D loss: 0.635938, acc: 64.84%] [G loss: 2.328458]\n",
      "epoch:4 step:4193 [D loss: 0.548129, acc: 75.00%] [G loss: 2.518214]\n",
      "epoch:4 step:4194 [D loss: 0.610373, acc: 67.19%] [G loss: 2.739840]\n",
      "epoch:4 step:4195 [D loss: 0.555982, acc: 75.00%] [G loss: 2.496295]\n",
      "epoch:4 step:4196 [D loss: 0.598317, acc: 66.41%] [G loss: 2.503438]\n",
      "epoch:4 step:4197 [D loss: 0.608951, acc: 64.06%] [G loss: 2.703472]\n",
      "epoch:4 step:4198 [D loss: 0.558081, acc: 72.66%] [G loss: 2.592203]\n",
      "epoch:4 step:4199 [D loss: 0.566211, acc: 70.31%] [G loss: 2.828948]\n",
      "epoch:4 step:4200 [D loss: 0.492358, acc: 78.12%] [G loss: 3.026840]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.386107\n",
      "FID: 51.825069\n",
      "0 = 13.18204054222108\n",
      "1 = 0.07916182118360762\n",
      "2 = 0.9319999814033508\n",
      "3 = 0.9333999752998352\n",
      "4 = 0.9305999875068665\n",
      "5 = 0.9307937622070312\n",
      "6 = 0.9333999752998352\n",
      "7 = 9.29796634452342\n",
      "8 = 0.1418471738440321\n",
      "9 = 0.8511000275611877\n",
      "10 = 0.8525999784469604\n",
      "11 = 0.8496000170707703\n",
      "12 = 0.8500498533248901\n",
      "13 = 0.8525999784469604\n",
      "14 = 5.38612174987793\n",
      "15 = 8.806057929992676\n",
      "16 = 0.26612597703933716\n",
      "17 = 5.386106967926025\n",
      "18 = 51.825069427490234\n",
      "epoch:4 step:4201 [D loss: 0.557111, acc: 70.31%] [G loss: 2.945625]\n",
      "epoch:4 step:4202 [D loss: 0.615706, acc: 68.75%] [G loss: 2.702909]\n",
      "epoch:4 step:4203 [D loss: 0.562684, acc: 71.09%] [G loss: 2.671288]\n",
      "epoch:4 step:4204 [D loss: 0.613184, acc: 65.62%] [G loss: 2.635556]\n",
      "epoch:4 step:4205 [D loss: 0.568919, acc: 65.62%] [G loss: 2.663300]\n",
      "epoch:4 step:4206 [D loss: 0.700995, acc: 57.03%] [G loss: 2.489658]\n",
      "epoch:4 step:4207 [D loss: 0.569303, acc: 70.31%] [G loss: 2.307843]\n",
      "epoch:4 step:4208 [D loss: 0.534833, acc: 74.22%] [G loss: 2.626713]\n",
      "epoch:4 step:4209 [D loss: 0.657856, acc: 64.84%] [G loss: 2.638815]\n",
      "epoch:4 step:4210 [D loss: 0.593549, acc: 72.66%] [G loss: 2.613410]\n",
      "epoch:4 step:4211 [D loss: 0.689496, acc: 59.38%] [G loss: 2.521283]\n",
      "epoch:4 step:4212 [D loss: 0.573823, acc: 68.75%] [G loss: 2.676376]\n",
      "epoch:4 step:4213 [D loss: 0.590584, acc: 71.09%] [G loss: 2.385094]\n",
      "epoch:4 step:4214 [D loss: 0.633496, acc: 69.53%] [G loss: 2.627095]\n",
      "epoch:4 step:4215 [D loss: 0.551085, acc: 71.09%] [G loss: 2.420666]\n",
      "epoch:4 step:4216 [D loss: 0.660679, acc: 64.06%] [G loss: 2.680428]\n",
      "epoch:4 step:4217 [D loss: 0.605055, acc: 62.50%] [G loss: 2.462643]\n",
      "epoch:4 step:4218 [D loss: 0.501424, acc: 78.12%] [G loss: 2.603938]\n",
      "epoch:4 step:4219 [D loss: 0.629751, acc: 66.41%] [G loss: 3.016512]\n",
      "epoch:4 step:4220 [D loss: 0.522704, acc: 72.66%] [G loss: 2.747691]\n",
      "epoch:4 step:4221 [D loss: 0.657588, acc: 64.06%] [G loss: 2.613850]\n",
      "epoch:4 step:4222 [D loss: 0.621841, acc: 71.88%] [G loss: 2.671978]\n",
      "epoch:4 step:4223 [D loss: 0.535958, acc: 75.00%] [G loss: 2.755047]\n",
      "epoch:4 step:4224 [D loss: 0.516606, acc: 75.00%] [G loss: 2.755834]\n",
      "epoch:4 step:4225 [D loss: 0.651830, acc: 61.72%] [G loss: 2.137253]\n",
      "epoch:4 step:4226 [D loss: 0.668324, acc: 59.38%] [G loss: 2.412650]\n",
      "epoch:4 step:4227 [D loss: 0.572556, acc: 69.53%] [G loss: 2.556108]\n",
      "epoch:4 step:4228 [D loss: 0.612879, acc: 69.53%] [G loss: 2.683589]\n",
      "epoch:4 step:4229 [D loss: 0.635831, acc: 67.19%] [G loss: 2.522808]\n",
      "epoch:4 step:4230 [D loss: 0.647050, acc: 67.97%] [G loss: 2.489099]\n",
      "epoch:4 step:4231 [D loss: 0.608644, acc: 68.75%] [G loss: 2.674579]\n",
      "epoch:4 step:4232 [D loss: 0.654740, acc: 64.84%] [G loss: 2.882308]\n",
      "epoch:4 step:4233 [D loss: 0.602749, acc: 64.06%] [G loss: 2.874503]\n",
      "epoch:4 step:4234 [D loss: 0.601829, acc: 67.97%] [G loss: 2.523890]\n",
      "epoch:4 step:4235 [D loss: 0.559450, acc: 70.31%] [G loss: 2.748708]\n",
      "epoch:4 step:4236 [D loss: 0.557963, acc: 74.22%] [G loss: 2.571650]\n",
      "epoch:4 step:4237 [D loss: 0.665141, acc: 63.28%] [G loss: 2.638153]\n",
      "epoch:4 step:4238 [D loss: 0.595754, acc: 67.97%] [G loss: 2.387933]\n",
      "epoch:4 step:4239 [D loss: 0.567407, acc: 68.75%] [G loss: 2.664701]\n",
      "epoch:4 step:4240 [D loss: 0.521574, acc: 75.78%] [G loss: 2.760782]\n",
      "epoch:4 step:4241 [D loss: 0.580634, acc: 71.09%] [G loss: 2.577899]\n",
      "epoch:4 step:4242 [D loss: 0.564688, acc: 75.00%] [G loss: 2.645068]\n",
      "epoch:4 step:4243 [D loss: 0.582633, acc: 67.97%] [G loss: 2.547475]\n",
      "epoch:4 step:4244 [D loss: 0.598336, acc: 65.62%] [G loss: 2.669008]\n",
      "epoch:4 step:4245 [D loss: 0.548396, acc: 69.53%] [G loss: 2.925372]\n",
      "epoch:4 step:4246 [D loss: 0.534621, acc: 73.44%] [G loss: 3.296034]\n",
      "epoch:4 step:4247 [D loss: 0.511808, acc: 77.34%] [G loss: 3.067411]\n",
      "epoch:4 step:4248 [D loss: 0.642468, acc: 66.41%] [G loss: 2.685083]\n",
      "epoch:4 step:4249 [D loss: 0.595198, acc: 67.19%] [G loss: 2.454320]\n",
      "epoch:4 step:4250 [D loss: 0.676135, acc: 59.38%] [G loss: 2.317056]\n",
      "epoch:4 step:4251 [D loss: 0.570702, acc: 71.09%] [G loss: 2.711372]\n",
      "epoch:4 step:4252 [D loss: 0.564283, acc: 68.75%] [G loss: 2.764094]\n",
      "epoch:4 step:4253 [D loss: 0.549636, acc: 72.66%] [G loss: 2.346049]\n",
      "epoch:4 step:4254 [D loss: 0.639053, acc: 68.75%] [G loss: 2.735016]\n",
      "epoch:4 step:4255 [D loss: 0.667233, acc: 60.94%] [G loss: 2.362775]\n",
      "epoch:4 step:4256 [D loss: 0.555686, acc: 71.09%] [G loss: 2.759041]\n",
      "epoch:4 step:4257 [D loss: 0.563621, acc: 71.09%] [G loss: 2.585419]\n",
      "epoch:4 step:4258 [D loss: 0.644367, acc: 63.28%] [G loss: 2.578991]\n",
      "epoch:4 step:4259 [D loss: 0.580806, acc: 72.66%] [G loss: 2.599753]\n",
      "epoch:4 step:4260 [D loss: 0.601146, acc: 69.53%] [G loss: 2.729442]\n",
      "epoch:4 step:4261 [D loss: 0.575865, acc: 66.41%] [G loss: 2.754445]\n",
      "epoch:4 step:4262 [D loss: 0.512716, acc: 74.22%] [G loss: 2.757591]\n",
      "epoch:4 step:4263 [D loss: 0.517382, acc: 76.56%] [G loss: 2.676996]\n",
      "epoch:4 step:4264 [D loss: 0.589555, acc: 67.97%] [G loss: 2.638924]\n",
      "epoch:4 step:4265 [D loss: 0.646280, acc: 67.19%] [G loss: 2.664530]\n",
      "epoch:4 step:4266 [D loss: 0.544228, acc: 74.22%] [G loss: 2.585059]\n",
      "epoch:4 step:4267 [D loss: 0.631530, acc: 64.84%] [G loss: 2.840611]\n",
      "epoch:4 step:4268 [D loss: 0.591958, acc: 65.62%] [G loss: 2.722801]\n",
      "epoch:4 step:4269 [D loss: 0.509508, acc: 78.12%] [G loss: 2.948985]\n",
      "epoch:4 step:4270 [D loss: 0.675726, acc: 66.41%] [G loss: 2.953511]\n",
      "epoch:4 step:4271 [D loss: 0.552160, acc: 71.09%] [G loss: 2.945499]\n",
      "epoch:4 step:4272 [D loss: 0.573413, acc: 74.22%] [G loss: 2.684572]\n",
      "epoch:4 step:4273 [D loss: 0.621360, acc: 69.53%] [G loss: 2.721597]\n",
      "epoch:4 step:4274 [D loss: 0.566957, acc: 69.53%] [G loss: 2.827227]\n",
      "epoch:4 step:4275 [D loss: 0.603771, acc: 63.28%] [G loss: 2.711961]\n",
      "epoch:4 step:4276 [D loss: 0.576051, acc: 71.09%] [G loss: 2.616986]\n",
      "epoch:4 step:4277 [D loss: 0.650079, acc: 68.75%] [G loss: 2.615986]\n",
      "epoch:4 step:4278 [D loss: 0.599142, acc: 64.84%] [G loss: 2.833532]\n",
      "epoch:4 step:4279 [D loss: 0.654352, acc: 67.19%] [G loss: 2.373956]\n",
      "epoch:4 step:4280 [D loss: 0.622483, acc: 67.19%] [G loss: 2.514206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4281 [D loss: 0.565286, acc: 71.88%] [G loss: 2.742599]\n",
      "epoch:4 step:4282 [D loss: 0.550208, acc: 72.66%] [G loss: 2.858449]\n",
      "epoch:4 step:4283 [D loss: 0.609441, acc: 67.19%] [G loss: 2.687435]\n",
      "epoch:4 step:4284 [D loss: 0.558263, acc: 73.44%] [G loss: 2.608825]\n",
      "epoch:4 step:4285 [D loss: 0.636113, acc: 63.28%] [G loss: 2.758824]\n",
      "epoch:4 step:4286 [D loss: 0.526840, acc: 69.53%] [G loss: 2.495440]\n",
      "epoch:4 step:4287 [D loss: 0.629187, acc: 65.62%] [G loss: 2.623538]\n",
      "epoch:4 step:4288 [D loss: 0.608878, acc: 68.75%] [G loss: 2.520708]\n",
      "epoch:4 step:4289 [D loss: 0.616357, acc: 67.19%] [G loss: 2.356395]\n",
      "epoch:4 step:4290 [D loss: 0.711403, acc: 57.81%] [G loss: 2.380941]\n",
      "epoch:4 step:4291 [D loss: 0.661848, acc: 59.38%] [G loss: 2.337131]\n",
      "epoch:4 step:4292 [D loss: 0.581493, acc: 69.53%] [G loss: 2.804077]\n",
      "epoch:4 step:4293 [D loss: 0.580412, acc: 68.75%] [G loss: 2.410537]\n",
      "epoch:4 step:4294 [D loss: 0.533261, acc: 75.78%] [G loss: 2.670559]\n",
      "epoch:4 step:4295 [D loss: 0.539455, acc: 69.53%] [G loss: 2.655814]\n",
      "epoch:4 step:4296 [D loss: 0.569933, acc: 70.31%] [G loss: 2.754343]\n",
      "epoch:4 step:4297 [D loss: 0.612481, acc: 63.28%] [G loss: 2.598586]\n",
      "epoch:4 step:4298 [D loss: 0.620940, acc: 72.66%] [G loss: 2.697352]\n",
      "epoch:4 step:4299 [D loss: 0.623177, acc: 66.41%] [G loss: 2.709394]\n",
      "epoch:4 step:4300 [D loss: 0.501043, acc: 77.34%] [G loss: 2.735630]\n",
      "epoch:4 step:4301 [D loss: 0.618202, acc: 67.97%] [G loss: 2.727219]\n",
      "epoch:4 step:4302 [D loss: 0.521464, acc: 74.22%] [G loss: 2.983114]\n",
      "epoch:4 step:4303 [D loss: 0.567580, acc: 68.75%] [G loss: 2.762249]\n",
      "epoch:4 step:4304 [D loss: 0.507214, acc: 75.00%] [G loss: 2.945290]\n",
      "epoch:4 step:4305 [D loss: 0.523201, acc: 76.56%] [G loss: 2.702267]\n",
      "epoch:4 step:4306 [D loss: 0.602941, acc: 70.31%] [G loss: 2.721108]\n",
      "epoch:4 step:4307 [D loss: 0.643466, acc: 64.06%] [G loss: 2.676356]\n",
      "epoch:4 step:4308 [D loss: 0.682223, acc: 62.50%] [G loss: 2.454710]\n",
      "epoch:4 step:4309 [D loss: 0.620148, acc: 64.84%] [G loss: 2.923837]\n",
      "epoch:4 step:4310 [D loss: 0.714355, acc: 60.16%] [G loss: 2.543972]\n",
      "epoch:4 step:4311 [D loss: 0.696845, acc: 59.38%] [G loss: 2.550920]\n",
      "epoch:4 step:4312 [D loss: 0.587171, acc: 69.53%] [G loss: 2.695151]\n",
      "epoch:4 step:4313 [D loss: 0.583614, acc: 72.66%] [G loss: 2.436076]\n",
      "epoch:4 step:4314 [D loss: 0.658329, acc: 64.84%] [G loss: 2.553460]\n",
      "epoch:4 step:4315 [D loss: 0.574655, acc: 68.75%] [G loss: 2.607670]\n",
      "epoch:4 step:4316 [D loss: 0.599260, acc: 67.19%] [G loss: 2.616537]\n",
      "epoch:4 step:4317 [D loss: 0.611057, acc: 62.50%] [G loss: 2.442732]\n",
      "epoch:4 step:4318 [D loss: 0.536260, acc: 75.00%] [G loss: 2.574367]\n",
      "epoch:4 step:4319 [D loss: 0.602201, acc: 68.75%] [G loss: 2.763608]\n",
      "epoch:4 step:4320 [D loss: 0.613221, acc: 63.28%] [G loss: 2.503835]\n",
      "epoch:4 step:4321 [D loss: 0.695300, acc: 54.69%] [G loss: 2.159341]\n",
      "epoch:4 step:4322 [D loss: 0.581290, acc: 72.66%] [G loss: 2.643975]\n",
      "epoch:4 step:4323 [D loss: 0.510405, acc: 78.91%] [G loss: 2.592389]\n",
      "epoch:4 step:4324 [D loss: 0.600910, acc: 69.53%] [G loss: 2.400327]\n",
      "epoch:4 step:4325 [D loss: 0.601736, acc: 66.41%] [G loss: 2.738415]\n",
      "epoch:4 step:4326 [D loss: 0.615898, acc: 65.62%] [G loss: 2.506371]\n",
      "epoch:4 step:4327 [D loss: 0.549137, acc: 71.88%] [G loss: 2.489388]\n",
      "epoch:4 step:4328 [D loss: 0.727009, acc: 54.69%] [G loss: 2.376151]\n",
      "epoch:4 step:4329 [D loss: 0.562408, acc: 75.00%] [G loss: 2.512082]\n",
      "epoch:4 step:4330 [D loss: 0.553257, acc: 72.66%] [G loss: 2.584972]\n",
      "epoch:4 step:4331 [D loss: 0.572030, acc: 67.97%] [G loss: 2.496940]\n",
      "epoch:4 step:4332 [D loss: 0.574539, acc: 71.09%] [G loss: 2.760704]\n",
      "epoch:4 step:4333 [D loss: 0.570145, acc: 72.66%] [G loss: 2.630664]\n",
      "epoch:4 step:4334 [D loss: 0.575105, acc: 69.53%] [G loss: 2.962985]\n",
      "epoch:4 step:4335 [D loss: 0.684205, acc: 64.84%] [G loss: 2.534790]\n",
      "epoch:4 step:4336 [D loss: 0.562839, acc: 67.19%] [G loss: 2.521603]\n",
      "epoch:4 step:4337 [D loss: 0.520917, acc: 71.88%] [G loss: 3.137352]\n",
      "epoch:4 step:4338 [D loss: 0.654791, acc: 64.06%] [G loss: 2.544977]\n",
      "epoch:4 step:4339 [D loss: 0.536719, acc: 74.22%] [G loss: 2.573772]\n",
      "epoch:4 step:4340 [D loss: 0.557066, acc: 71.09%] [G loss: 2.846132]\n",
      "epoch:4 step:4341 [D loss: 0.581085, acc: 72.66%] [G loss: 2.706074]\n",
      "epoch:4 step:4342 [D loss: 0.588191, acc: 69.53%] [G loss: 2.630040]\n",
      "epoch:4 step:4343 [D loss: 0.507073, acc: 76.56%] [G loss: 2.861892]\n",
      "epoch:4 step:4344 [D loss: 0.646188, acc: 64.84%] [G loss: 2.641000]\n",
      "epoch:4 step:4345 [D loss: 0.553356, acc: 76.56%] [G loss: 2.443103]\n",
      "epoch:4 step:4346 [D loss: 0.534802, acc: 69.53%] [G loss: 2.778799]\n",
      "epoch:4 step:4347 [D loss: 0.523519, acc: 75.78%] [G loss: 2.563076]\n",
      "epoch:4 step:4348 [D loss: 0.608001, acc: 67.97%] [G loss: 2.622257]\n",
      "epoch:4 step:4349 [D loss: 0.567688, acc: 72.66%] [G loss: 2.736890]\n",
      "epoch:4 step:4350 [D loss: 0.539988, acc: 74.22%] [G loss: 2.509852]\n",
      "epoch:4 step:4351 [D loss: 0.567840, acc: 71.88%] [G loss: 3.092163]\n",
      "epoch:4 step:4352 [D loss: 0.669916, acc: 56.25%] [G loss: 2.776598]\n",
      "epoch:4 step:4353 [D loss: 0.623325, acc: 69.53%] [G loss: 2.471599]\n",
      "epoch:4 step:4354 [D loss: 0.638783, acc: 67.97%] [G loss: 2.538241]\n",
      "epoch:4 step:4355 [D loss: 0.605088, acc: 66.41%] [G loss: 2.665521]\n",
      "epoch:4 step:4356 [D loss: 0.587265, acc: 67.19%] [G loss: 2.451335]\n",
      "epoch:4 step:4357 [D loss: 0.529985, acc: 71.09%] [G loss: 2.763244]\n",
      "epoch:4 step:4358 [D loss: 0.570169, acc: 71.88%] [G loss: 2.544826]\n",
      "epoch:4 step:4359 [D loss: 0.614647, acc: 63.28%] [G loss: 2.542970]\n",
      "epoch:4 step:4360 [D loss: 0.570601, acc: 70.31%] [G loss: 2.959392]\n",
      "epoch:4 step:4361 [D loss: 0.590306, acc: 70.31%] [G loss: 2.696186]\n",
      "epoch:4 step:4362 [D loss: 0.598291, acc: 67.97%] [G loss: 2.720901]\n",
      "epoch:4 step:4363 [D loss: 0.642456, acc: 61.72%] [G loss: 2.162371]\n",
      "epoch:4 step:4364 [D loss: 0.593542, acc: 69.53%] [G loss: 2.406627]\n",
      "epoch:4 step:4365 [D loss: 0.644499, acc: 60.94%] [G loss: 2.355360]\n",
      "epoch:4 step:4366 [D loss: 0.566193, acc: 68.75%] [G loss: 2.663122]\n",
      "epoch:4 step:4367 [D loss: 0.594820, acc: 68.75%] [G loss: 2.487727]\n",
      "epoch:4 step:4368 [D loss: 0.569699, acc: 67.97%] [G loss: 2.622708]\n",
      "epoch:4 step:4369 [D loss: 0.579603, acc: 67.19%] [G loss: 2.489504]\n",
      "epoch:4 step:4370 [D loss: 0.685447, acc: 59.38%] [G loss: 2.114031]\n",
      "epoch:4 step:4371 [D loss: 0.587067, acc: 71.09%] [G loss: 2.503873]\n",
      "epoch:4 step:4372 [D loss: 0.530561, acc: 77.34%] [G loss: 2.744865]\n",
      "epoch:4 step:4373 [D loss: 0.561670, acc: 71.88%] [G loss: 2.362422]\n",
      "epoch:4 step:4374 [D loss: 0.538192, acc: 75.78%] [G loss: 2.550754]\n",
      "epoch:4 step:4375 [D loss: 0.625491, acc: 64.84%] [G loss: 2.425833]\n",
      "epoch:4 step:4376 [D loss: 0.611484, acc: 71.09%] [G loss: 2.339942]\n",
      "epoch:4 step:4377 [D loss: 0.612582, acc: 64.84%] [G loss: 2.571985]\n",
      "epoch:4 step:4378 [D loss: 0.561966, acc: 72.66%] [G loss: 2.648030]\n",
      "epoch:4 step:4379 [D loss: 0.506434, acc: 75.78%] [G loss: 2.826037]\n",
      "epoch:4 step:4380 [D loss: 0.564518, acc: 69.53%] [G loss: 2.930097]\n",
      "epoch:4 step:4381 [D loss: 0.651687, acc: 69.53%] [G loss: 2.669153]\n",
      "epoch:4 step:4382 [D loss: 0.626104, acc: 68.75%] [G loss: 2.736921]\n",
      "epoch:4 step:4383 [D loss: 0.564390, acc: 73.44%] [G loss: 2.688413]\n",
      "epoch:4 step:4384 [D loss: 0.674435, acc: 60.94%] [G loss: 2.512368]\n",
      "epoch:4 step:4385 [D loss: 0.507064, acc: 79.69%] [G loss: 2.947370]\n",
      "epoch:4 step:4386 [D loss: 0.604493, acc: 63.28%] [G loss: 2.661696]\n",
      "epoch:4 step:4387 [D loss: 0.570833, acc: 71.88%] [G loss: 2.976339]\n",
      "epoch:4 step:4388 [D loss: 0.560831, acc: 74.22%] [G loss: 2.773997]\n",
      "epoch:4 step:4389 [D loss: 0.556017, acc: 74.22%] [G loss: 2.897844]\n",
      "epoch:4 step:4390 [D loss: 0.560473, acc: 78.91%] [G loss: 3.188696]\n",
      "epoch:4 step:4391 [D loss: 0.583541, acc: 70.31%] [G loss: 2.714066]\n",
      "epoch:4 step:4392 [D loss: 0.646384, acc: 65.62%] [G loss: 2.812841]\n",
      "epoch:4 step:4393 [D loss: 0.662559, acc: 62.50%] [G loss: 2.696569]\n",
      "epoch:4 step:4394 [D loss: 0.603763, acc: 67.19%] [G loss: 2.548913]\n",
      "epoch:4 step:4395 [D loss: 0.506142, acc: 75.00%] [G loss: 2.979695]\n",
      "epoch:4 step:4396 [D loss: 0.450232, acc: 82.03%] [G loss: 3.085468]\n",
      "epoch:4 step:4397 [D loss: 0.533439, acc: 71.88%] [G loss: 3.103599]\n",
      "epoch:4 step:4398 [D loss: 0.522552, acc: 75.78%] [G loss: 2.864972]\n",
      "epoch:4 step:4399 [D loss: 0.569665, acc: 66.41%] [G loss: 2.838580]\n",
      "epoch:4 step:4400 [D loss: 0.612313, acc: 68.75%] [G loss: 2.489932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.422679\n",
      "FID: 52.678387\n",
      "0 = 13.299305605792977\n",
      "1 = 0.0896916968662951\n",
      "2 = 0.9312000274658203\n",
      "3 = 0.932200014591217\n",
      "4 = 0.9301999807357788\n",
      "5 = 0.9303393363952637\n",
      "6 = 0.932200014591217\n",
      "7 = 9.39764976365568\n",
      "8 = 0.14119773370366734\n",
      "9 = 0.8567000031471252\n",
      "10 = 0.8560000061988831\n",
      "11 = 0.8574000000953674\n",
      "12 = 0.8572000861167908\n",
      "13 = 0.8560000061988831\n",
      "14 = 5.42269229888916\n",
      "15 = 8.866142272949219\n",
      "16 = 0.25369808077812195\n",
      "17 = 5.422679424285889\n",
      "18 = 52.67838668823242\n",
      "epoch:4 step:4401 [D loss: 0.601845, acc: 68.75%] [G loss: 2.591605]\n",
      "epoch:4 step:4402 [D loss: 0.524736, acc: 78.12%] [G loss: 2.795904]\n",
      "epoch:4 step:4403 [D loss: 0.634701, acc: 67.97%] [G loss: 2.488604]\n",
      "epoch:4 step:4404 [D loss: 0.581978, acc: 67.97%] [G loss: 2.446877]\n",
      "epoch:4 step:4405 [D loss: 0.565111, acc: 71.88%] [G loss: 2.518055]\n",
      "epoch:4 step:4406 [D loss: 0.592606, acc: 66.41%] [G loss: 2.490445]\n",
      "epoch:4 step:4407 [D loss: 0.585059, acc: 73.44%] [G loss: 2.615323]\n",
      "epoch:4 step:4408 [D loss: 0.591219, acc: 71.09%] [G loss: 2.821336]\n",
      "epoch:4 step:4409 [D loss: 0.532174, acc: 78.12%] [G loss: 2.667370]\n",
      "epoch:4 step:4410 [D loss: 0.613242, acc: 65.62%] [G loss: 2.973363]\n",
      "epoch:4 step:4411 [D loss: 0.650813, acc: 62.50%] [G loss: 3.043835]\n",
      "epoch:4 step:4412 [D loss: 0.488447, acc: 77.34%] [G loss: 2.850682]\n",
      "epoch:4 step:4413 [D loss: 0.668231, acc: 62.50%] [G loss: 2.787637]\n",
      "epoch:4 step:4414 [D loss: 0.568823, acc: 74.22%] [G loss: 2.541224]\n",
      "epoch:4 step:4415 [D loss: 0.600730, acc: 69.53%] [G loss: 2.400277]\n",
      "epoch:4 step:4416 [D loss: 0.580170, acc: 68.75%] [G loss: 2.929267]\n",
      "epoch:4 step:4417 [D loss: 0.612878, acc: 67.97%] [G loss: 2.478128]\n",
      "epoch:4 step:4418 [D loss: 0.595383, acc: 67.97%] [G loss: 2.407887]\n",
      "epoch:4 step:4419 [D loss: 0.583450, acc: 64.84%] [G loss: 3.048472]\n",
      "epoch:4 step:4420 [D loss: 0.586403, acc: 64.84%] [G loss: 2.443291]\n",
      "epoch:4 step:4421 [D loss: 0.595321, acc: 66.41%] [G loss: 2.522067]\n",
      "epoch:4 step:4422 [D loss: 0.553031, acc: 72.66%] [G loss: 2.549554]\n",
      "epoch:4 step:4423 [D loss: 0.657184, acc: 65.62%] [G loss: 2.598397]\n",
      "epoch:4 step:4424 [D loss: 0.635549, acc: 59.38%] [G loss: 2.493572]\n",
      "epoch:4 step:4425 [D loss: 0.555108, acc: 71.09%] [G loss: 2.828775]\n",
      "epoch:4 step:4426 [D loss: 0.461252, acc: 81.25%] [G loss: 2.839915]\n",
      "epoch:4 step:4427 [D loss: 0.574330, acc: 67.97%] [G loss: 3.113691]\n",
      "epoch:4 step:4428 [D loss: 0.515998, acc: 75.78%] [G loss: 2.940809]\n",
      "epoch:4 step:4429 [D loss: 0.508478, acc: 75.00%] [G loss: 2.793989]\n",
      "epoch:4 step:4430 [D loss: 0.555101, acc: 68.75%] [G loss: 2.566252]\n",
      "epoch:4 step:4431 [D loss: 0.557334, acc: 70.31%] [G loss: 2.482584]\n",
      "epoch:4 step:4432 [D loss: 0.533514, acc: 74.22%] [G loss: 2.709250]\n",
      "epoch:4 step:4433 [D loss: 0.647229, acc: 67.97%] [G loss: 2.777552]\n",
      "epoch:4 step:4434 [D loss: 0.522824, acc: 78.12%] [G loss: 2.395181]\n",
      "epoch:4 step:4435 [D loss: 0.600913, acc: 69.53%] [G loss: 2.860413]\n",
      "epoch:4 step:4436 [D loss: 0.572503, acc: 67.97%] [G loss: 2.493103]\n",
      "epoch:4 step:4437 [D loss: 0.635708, acc: 64.06%] [G loss: 2.789240]\n",
      "epoch:4 step:4438 [D loss: 0.585081, acc: 73.44%] [G loss: 2.873016]\n",
      "epoch:4 step:4439 [D loss: 0.509840, acc: 73.44%] [G loss: 2.798771]\n",
      "epoch:4 step:4440 [D loss: 0.539533, acc: 71.09%] [G loss: 2.822888]\n",
      "epoch:4 step:4441 [D loss: 0.539623, acc: 69.53%] [G loss: 2.679868]\n",
      "epoch:4 step:4442 [D loss: 0.588386, acc: 73.44%] [G loss: 3.296425]\n",
      "epoch:4 step:4443 [D loss: 0.488846, acc: 74.22%] [G loss: 2.954718]\n",
      "epoch:4 step:4444 [D loss: 0.656731, acc: 64.06%] [G loss: 2.600762]\n",
      "epoch:4 step:4445 [D loss: 0.560896, acc: 72.66%] [G loss: 2.846264]\n",
      "epoch:4 step:4446 [D loss: 0.495552, acc: 77.34%] [G loss: 2.655265]\n",
      "epoch:4 step:4447 [D loss: 0.545860, acc: 67.19%] [G loss: 3.084349]\n",
      "epoch:4 step:4448 [D loss: 0.548159, acc: 71.88%] [G loss: 2.753045]\n",
      "epoch:4 step:4449 [D loss: 0.594055, acc: 71.09%] [G loss: 2.755347]\n",
      "epoch:4 step:4450 [D loss: 0.591014, acc: 69.53%] [G loss: 2.541314]\n",
      "epoch:4 step:4451 [D loss: 0.576597, acc: 68.75%] [G loss: 2.869299]\n",
      "epoch:4 step:4452 [D loss: 0.591354, acc: 67.19%] [G loss: 2.705604]\n",
      "epoch:4 step:4453 [D loss: 0.491173, acc: 81.25%] [G loss: 2.693405]\n",
      "epoch:4 step:4454 [D loss: 0.530565, acc: 73.44%] [G loss: 3.057004]\n",
      "epoch:4 step:4455 [D loss: 0.507014, acc: 76.56%] [G loss: 3.331600]\n",
      "epoch:4 step:4456 [D loss: 0.519770, acc: 75.00%] [G loss: 2.939837]\n",
      "epoch:4 step:4457 [D loss: 0.523202, acc: 75.00%] [G loss: 2.899044]\n",
      "epoch:4 step:4458 [D loss: 0.642307, acc: 61.72%] [G loss: 2.656706]\n",
      "epoch:4 step:4459 [D loss: 0.606474, acc: 64.84%] [G loss: 2.527017]\n",
      "epoch:4 step:4460 [D loss: 0.577618, acc: 70.31%] [G loss: 2.846762]\n",
      "epoch:4 step:4461 [D loss: 0.610161, acc: 70.31%] [G loss: 2.547393]\n",
      "epoch:4 step:4462 [D loss: 0.588369, acc: 72.66%] [G loss: 2.660034]\n",
      "epoch:4 step:4463 [D loss: 0.543835, acc: 73.44%] [G loss: 2.764561]\n",
      "epoch:4 step:4464 [D loss: 0.616972, acc: 66.41%] [G loss: 2.691135]\n",
      "epoch:4 step:4465 [D loss: 0.619544, acc: 64.06%] [G loss: 2.446598]\n",
      "epoch:4 step:4466 [D loss: 0.560449, acc: 71.88%] [G loss: 2.646205]\n",
      "epoch:4 step:4467 [D loss: 0.518796, acc: 75.00%] [G loss: 2.805924]\n",
      "epoch:4 step:4468 [D loss: 0.566651, acc: 65.62%] [G loss: 2.699228]\n",
      "epoch:4 step:4469 [D loss: 0.645837, acc: 67.19%] [G loss: 2.711332]\n",
      "epoch:4 step:4470 [D loss: 0.625138, acc: 64.06%] [G loss: 2.835104]\n",
      "epoch:4 step:4471 [D loss: 0.642575, acc: 63.28%] [G loss: 2.636643]\n",
      "epoch:4 step:4472 [D loss: 0.587812, acc: 67.19%] [G loss: 2.562209]\n",
      "epoch:4 step:4473 [D loss: 0.508769, acc: 77.34%] [G loss: 3.005902]\n",
      "epoch:4 step:4474 [D loss: 0.627734, acc: 67.97%] [G loss: 2.661380]\n",
      "epoch:4 step:4475 [D loss: 0.590818, acc: 71.88%] [G loss: 2.734380]\n",
      "epoch:4 step:4476 [D loss: 0.584303, acc: 67.97%] [G loss: 2.771326]\n",
      "epoch:4 step:4477 [D loss: 0.536936, acc: 74.22%] [G loss: 2.571781]\n",
      "epoch:4 step:4478 [D loss: 0.649314, acc: 64.84%] [G loss: 2.564883]\n",
      "epoch:4 step:4479 [D loss: 0.593888, acc: 67.19%] [G loss: 2.211686]\n",
      "epoch:4 step:4480 [D loss: 0.558942, acc: 69.53%] [G loss: 2.753067]\n",
      "epoch:4 step:4481 [D loss: 0.573747, acc: 71.88%] [G loss: 2.780093]\n",
      "epoch:4 step:4482 [D loss: 0.537879, acc: 71.88%] [G loss: 2.548916]\n",
      "epoch:4 step:4483 [D loss: 0.619590, acc: 66.41%] [G loss: 2.384041]\n",
      "epoch:4 step:4484 [D loss: 0.496693, acc: 77.34%] [G loss: 2.762491]\n",
      "epoch:4 step:4485 [D loss: 0.534205, acc: 70.31%] [G loss: 2.698507]\n",
      "epoch:4 step:4486 [D loss: 0.558177, acc: 73.44%] [G loss: 2.729602]\n",
      "epoch:4 step:4487 [D loss: 0.574408, acc: 67.97%] [G loss: 2.321717]\n",
      "epoch:4 step:4488 [D loss: 0.601930, acc: 69.53%] [G loss: 2.513033]\n",
      "epoch:4 step:4489 [D loss: 0.615643, acc: 66.41%] [G loss: 2.725847]\n",
      "epoch:4 step:4490 [D loss: 0.557145, acc: 72.66%] [G loss: 2.779781]\n",
      "epoch:4 step:4491 [D loss: 0.547469, acc: 67.97%] [G loss: 2.711156]\n",
      "epoch:4 step:4492 [D loss: 0.539844, acc: 73.44%] [G loss: 2.848479]\n",
      "epoch:4 step:4493 [D loss: 0.592751, acc: 69.53%] [G loss: 2.916058]\n",
      "epoch:4 step:4494 [D loss: 0.613377, acc: 71.09%] [G loss: 2.758906]\n",
      "epoch:4 step:4495 [D loss: 0.589797, acc: 71.88%] [G loss: 2.812662]\n",
      "epoch:4 step:4496 [D loss: 0.593502, acc: 64.06%] [G loss: 2.613160]\n",
      "epoch:4 step:4497 [D loss: 0.586074, acc: 72.66%] [G loss: 2.678152]\n",
      "epoch:4 step:4498 [D loss: 0.567462, acc: 71.09%] [G loss: 2.833055]\n",
      "epoch:4 step:4499 [D loss: 0.659706, acc: 63.28%] [G loss: 2.510013]\n",
      "epoch:4 step:4500 [D loss: 0.618437, acc: 62.50%] [G loss: 2.394095]\n",
      "epoch:4 step:4501 [D loss: 0.571886, acc: 72.66%] [G loss: 2.790940]\n",
      "epoch:4 step:4502 [D loss: 0.593508, acc: 67.19%] [G loss: 2.630367]\n",
      "epoch:4 step:4503 [D loss: 0.588915, acc: 72.66%] [G loss: 2.796994]\n",
      "epoch:4 step:4504 [D loss: 0.657573, acc: 64.06%] [G loss: 2.513008]\n",
      "epoch:4 step:4505 [D loss: 0.493521, acc: 71.88%] [G loss: 2.745337]\n",
      "epoch:4 step:4506 [D loss: 0.547036, acc: 72.66%] [G loss: 2.529743]\n",
      "epoch:4 step:4507 [D loss: 0.564820, acc: 69.53%] [G loss: 2.649021]\n",
      "epoch:4 step:4508 [D loss: 0.591742, acc: 67.19%] [G loss: 2.710293]\n",
      "epoch:4 step:4509 [D loss: 0.529042, acc: 76.56%] [G loss: 2.618569]\n",
      "epoch:4 step:4510 [D loss: 0.634003, acc: 64.84%] [G loss: 2.493752]\n",
      "epoch:4 step:4511 [D loss: 0.561422, acc: 71.09%] [G loss: 2.578462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4512 [D loss: 0.656534, acc: 62.50%] [G loss: 2.515298]\n",
      "epoch:4 step:4513 [D loss: 0.685333, acc: 65.62%] [G loss: 2.452811]\n",
      "epoch:4 step:4514 [D loss: 0.699297, acc: 60.16%] [G loss: 2.316347]\n",
      "epoch:4 step:4515 [D loss: 0.587699, acc: 66.41%] [G loss: 2.629438]\n",
      "epoch:4 step:4516 [D loss: 0.566733, acc: 67.97%] [G loss: 2.526935]\n",
      "epoch:4 step:4517 [D loss: 0.573914, acc: 66.41%] [G loss: 2.625283]\n",
      "epoch:4 step:4518 [D loss: 0.611230, acc: 67.19%] [G loss: 2.543717]\n",
      "epoch:4 step:4519 [D loss: 0.659524, acc: 65.62%] [G loss: 2.451237]\n",
      "epoch:4 step:4520 [D loss: 0.547640, acc: 71.09%] [G loss: 2.726872]\n",
      "epoch:4 step:4521 [D loss: 0.625468, acc: 63.28%] [G loss: 2.603862]\n",
      "epoch:4 step:4522 [D loss: 0.613253, acc: 70.31%] [G loss: 3.124722]\n",
      "epoch:4 step:4523 [D loss: 0.584363, acc: 64.06%] [G loss: 3.115738]\n",
      "epoch:4 step:4524 [D loss: 0.589018, acc: 68.75%] [G loss: 2.789169]\n",
      "epoch:4 step:4525 [D loss: 0.544867, acc: 73.44%] [G loss: 2.789325]\n",
      "epoch:4 step:4526 [D loss: 0.624309, acc: 62.50%] [G loss: 2.482718]\n",
      "epoch:4 step:4527 [D loss: 0.556949, acc: 70.31%] [G loss: 2.669627]\n",
      "epoch:4 step:4528 [D loss: 0.541100, acc: 73.44%] [G loss: 2.888715]\n",
      "epoch:4 step:4529 [D loss: 0.525466, acc: 69.53%] [G loss: 2.901757]\n",
      "epoch:4 step:4530 [D loss: 0.534262, acc: 74.22%] [G loss: 2.919244]\n",
      "epoch:4 step:4531 [D loss: 0.565133, acc: 77.34%] [G loss: 2.901037]\n",
      "epoch:4 step:4532 [D loss: 0.573526, acc: 67.97%] [G loss: 2.790990]\n",
      "epoch:4 step:4533 [D loss: 0.637170, acc: 63.28%] [G loss: 2.683242]\n",
      "epoch:4 step:4534 [D loss: 0.503498, acc: 78.12%] [G loss: 2.974450]\n",
      "epoch:4 step:4535 [D loss: 0.690693, acc: 65.62%] [G loss: 2.438014]\n",
      "epoch:4 step:4536 [D loss: 0.728904, acc: 54.69%] [G loss: 2.404180]\n",
      "epoch:4 step:4537 [D loss: 0.645415, acc: 67.19%] [G loss: 2.499231]\n",
      "epoch:4 step:4538 [D loss: 0.615660, acc: 68.75%] [G loss: 2.561314]\n",
      "epoch:4 step:4539 [D loss: 0.561841, acc: 69.53%] [G loss: 2.781020]\n",
      "epoch:4 step:4540 [D loss: 0.518966, acc: 75.00%] [G loss: 3.326242]\n",
      "epoch:4 step:4541 [D loss: 0.647862, acc: 64.84%] [G loss: 2.410741]\n",
      "epoch:4 step:4542 [D loss: 0.662720, acc: 61.72%] [G loss: 2.280670]\n",
      "epoch:4 step:4543 [D loss: 0.602431, acc: 68.75%] [G loss: 2.712595]\n",
      "epoch:4 step:4544 [D loss: 0.604633, acc: 67.19%] [G loss: 2.723485]\n",
      "epoch:4 step:4545 [D loss: 0.593299, acc: 64.06%] [G loss: 2.305221]\n",
      "epoch:4 step:4546 [D loss: 0.555135, acc: 73.44%] [G loss: 2.757438]\n",
      "epoch:4 step:4547 [D loss: 0.598507, acc: 68.75%] [G loss: 2.627451]\n",
      "epoch:4 step:4548 [D loss: 0.657346, acc: 64.06%] [G loss: 2.633390]\n",
      "epoch:4 step:4549 [D loss: 0.562756, acc: 67.97%] [G loss: 2.517679]\n",
      "epoch:4 step:4550 [D loss: 0.545454, acc: 68.75%] [G loss: 2.466930]\n",
      "epoch:4 step:4551 [D loss: 0.642717, acc: 61.72%] [G loss: 2.941260]\n",
      "epoch:4 step:4552 [D loss: 0.561424, acc: 72.66%] [G loss: 2.969968]\n",
      "epoch:4 step:4553 [D loss: 0.501859, acc: 77.34%] [G loss: 3.011369]\n",
      "epoch:4 step:4554 [D loss: 0.568961, acc: 69.53%] [G loss: 3.100888]\n",
      "epoch:4 step:4555 [D loss: 0.598220, acc: 71.09%] [G loss: 2.796106]\n",
      "epoch:4 step:4556 [D loss: 0.555551, acc: 70.31%] [G loss: 2.563393]\n",
      "epoch:4 step:4557 [D loss: 0.621465, acc: 71.09%] [G loss: 2.961249]\n",
      "epoch:4 step:4558 [D loss: 0.587909, acc: 75.00%] [G loss: 2.532258]\n",
      "epoch:4 step:4559 [D loss: 0.570304, acc: 69.53%] [G loss: 2.697429]\n",
      "epoch:4 step:4560 [D loss: 0.574334, acc: 70.31%] [G loss: 2.312857]\n",
      "epoch:4 step:4561 [D loss: 0.593984, acc: 70.31%] [G loss: 2.608403]\n",
      "epoch:4 step:4562 [D loss: 0.584202, acc: 66.41%] [G loss: 2.588504]\n",
      "epoch:4 step:4563 [D loss: 0.613288, acc: 64.84%] [G loss: 2.743642]\n",
      "epoch:4 step:4564 [D loss: 0.531225, acc: 75.78%] [G loss: 2.747527]\n",
      "epoch:4 step:4565 [D loss: 0.603776, acc: 66.41%] [G loss: 2.671628]\n",
      "epoch:4 step:4566 [D loss: 0.571696, acc: 66.41%] [G loss: 2.531906]\n",
      "epoch:4 step:4567 [D loss: 0.638283, acc: 70.31%] [G loss: 2.696903]\n",
      "epoch:4 step:4568 [D loss: 0.542304, acc: 75.78%] [G loss: 2.725837]\n",
      "epoch:4 step:4569 [D loss: 0.514990, acc: 76.56%] [G loss: 2.794034]\n",
      "epoch:4 step:4570 [D loss: 0.563261, acc: 71.88%] [G loss: 2.908519]\n",
      "epoch:4 step:4571 [D loss: 0.542378, acc: 75.78%] [G loss: 3.015810]\n",
      "epoch:4 step:4572 [D loss: 0.613469, acc: 67.19%] [G loss: 2.666544]\n",
      "epoch:4 step:4573 [D loss: 0.572462, acc: 70.31%] [G loss: 2.802411]\n",
      "epoch:4 step:4574 [D loss: 0.705355, acc: 64.06%] [G loss: 2.370429]\n",
      "epoch:4 step:4575 [D loss: 0.707803, acc: 57.81%] [G loss: 2.169416]\n",
      "epoch:4 step:4576 [D loss: 0.582248, acc: 71.09%] [G loss: 2.348415]\n",
      "epoch:4 step:4577 [D loss: 0.628371, acc: 65.62%] [G loss: 2.699418]\n",
      "epoch:4 step:4578 [D loss: 0.591900, acc: 65.62%] [G loss: 2.489969]\n",
      "epoch:4 step:4579 [D loss: 0.595253, acc: 68.75%] [G loss: 3.181677]\n",
      "epoch:4 step:4580 [D loss: 0.655763, acc: 60.94%] [G loss: 2.646067]\n",
      "epoch:4 step:4581 [D loss: 0.490124, acc: 77.34%] [G loss: 2.640479]\n",
      "epoch:4 step:4582 [D loss: 0.560425, acc: 71.09%] [G loss: 2.789520]\n",
      "epoch:4 step:4583 [D loss: 0.648761, acc: 66.41%] [G loss: 2.607136]\n",
      "epoch:4 step:4584 [D loss: 0.607639, acc: 67.97%] [G loss: 2.466114]\n",
      "epoch:4 step:4585 [D loss: 0.517448, acc: 75.78%] [G loss: 2.900709]\n",
      "epoch:4 step:4586 [D loss: 0.632079, acc: 64.84%] [G loss: 2.654151]\n",
      "epoch:4 step:4587 [D loss: 0.651933, acc: 64.84%] [G loss: 2.461425]\n",
      "epoch:4 step:4588 [D loss: 0.693446, acc: 58.59%] [G loss: 2.390746]\n",
      "epoch:4 step:4589 [D loss: 0.578464, acc: 66.41%] [G loss: 2.528916]\n",
      "epoch:4 step:4590 [D loss: 0.579741, acc: 67.97%] [G loss: 2.647478]\n",
      "epoch:4 step:4591 [D loss: 0.630721, acc: 65.62%] [G loss: 2.636132]\n",
      "epoch:4 step:4592 [D loss: 0.660427, acc: 61.72%] [G loss: 2.437002]\n",
      "epoch:4 step:4593 [D loss: 0.613864, acc: 67.97%] [G loss: 2.201087]\n",
      "epoch:4 step:4594 [D loss: 0.624866, acc: 64.84%] [G loss: 2.472178]\n",
      "epoch:4 step:4595 [D loss: 0.552836, acc: 70.31%] [G loss: 2.703569]\n",
      "epoch:4 step:4596 [D loss: 0.590377, acc: 67.97%] [G loss: 2.682624]\n",
      "epoch:4 step:4597 [D loss: 0.599639, acc: 67.97%] [G loss: 2.329677]\n",
      "epoch:4 step:4598 [D loss: 0.613731, acc: 66.41%] [G loss: 2.561501]\n",
      "epoch:4 step:4599 [D loss: 0.663965, acc: 62.50%] [G loss: 2.306736]\n",
      "epoch:4 step:4600 [D loss: 0.502393, acc: 73.44%] [G loss: 2.796082]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.373926\n",
      "FID: 53.343212\n",
      "0 = 13.268954673290269\n",
      "1 = 0.08027810039110782\n",
      "2 = 0.9380000233650208\n",
      "3 = 0.9368000030517578\n",
      "4 = 0.9391999840736389\n",
      "5 = 0.9390537142753601\n",
      "6 = 0.9368000030517578\n",
      "7 = 9.378796558976187\n",
      "8 = 0.14714450864354467\n",
      "9 = 0.8508999943733215\n",
      "10 = 0.8557999730110168\n",
      "11 = 0.8460000157356262\n",
      "12 = 0.8474945425987244\n",
      "13 = 0.8557999730110168\n",
      "14 = 5.373941898345947\n",
      "15 = 8.661683082580566\n",
      "16 = 0.27433720231056213\n",
      "17 = 5.373925685882568\n",
      "18 = 53.34321212768555\n",
      "epoch:4 step:4601 [D loss: 0.569636, acc: 70.31%] [G loss: 2.572026]\n",
      "epoch:4 step:4602 [D loss: 0.548231, acc: 69.53%] [G loss: 2.734850]\n",
      "epoch:4 step:4603 [D loss: 0.598499, acc: 67.97%] [G loss: 2.373399]\n",
      "epoch:4 step:4604 [D loss: 0.599126, acc: 67.19%] [G loss: 2.501509]\n",
      "epoch:4 step:4605 [D loss: 0.613521, acc: 70.31%] [G loss: 2.563782]\n",
      "epoch:4 step:4606 [D loss: 0.751311, acc: 57.03%] [G loss: 2.336051]\n",
      "epoch:4 step:4607 [D loss: 0.651100, acc: 66.41%] [G loss: 2.692713]\n",
      "epoch:4 step:4608 [D loss: 0.531575, acc: 74.22%] [G loss: 2.636664]\n",
      "epoch:4 step:4609 [D loss: 0.615418, acc: 62.50%] [G loss: 2.431388]\n",
      "epoch:4 step:4610 [D loss: 0.556099, acc: 73.44%] [G loss: 2.436239]\n",
      "epoch:4 step:4611 [D loss: 0.605707, acc: 66.41%] [G loss: 2.356984]\n",
      "epoch:4 step:4612 [D loss: 0.569486, acc: 66.41%] [G loss: 2.519812]\n",
      "epoch:4 step:4613 [D loss: 0.633331, acc: 70.31%] [G loss: 2.472666]\n",
      "epoch:4 step:4614 [D loss: 0.590013, acc: 74.22%] [G loss: 2.651351]\n",
      "epoch:4 step:4615 [D loss: 0.652328, acc: 65.62%] [G loss: 2.357517]\n",
      "epoch:4 step:4616 [D loss: 0.630327, acc: 67.97%] [G loss: 2.364712]\n",
      "epoch:4 step:4617 [D loss: 0.610768, acc: 67.97%] [G loss: 2.486894]\n",
      "epoch:4 step:4618 [D loss: 0.563899, acc: 73.44%] [G loss: 2.348389]\n",
      "epoch:4 step:4619 [D loss: 0.614806, acc: 69.53%] [G loss: 2.792155]\n",
      "epoch:4 step:4620 [D loss: 0.581417, acc: 71.09%] [G loss: 2.488079]\n",
      "epoch:4 step:4621 [D loss: 0.658474, acc: 63.28%] [G loss: 2.437918]\n",
      "epoch:4 step:4622 [D loss: 0.610071, acc: 67.19%] [G loss: 2.424559]\n",
      "epoch:4 step:4623 [D loss: 0.567633, acc: 73.44%] [G loss: 2.614785]\n",
      "epoch:4 step:4624 [D loss: 0.571522, acc: 67.19%] [G loss: 2.508312]\n",
      "epoch:4 step:4625 [D loss: 0.531855, acc: 73.44%] [G loss: 2.564643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4626 [D loss: 0.649943, acc: 63.28%] [G loss: 2.354015]\n",
      "epoch:4 step:4627 [D loss: 0.535334, acc: 75.78%] [G loss: 2.523493]\n",
      "epoch:4 step:4628 [D loss: 0.644792, acc: 60.16%] [G loss: 2.503366]\n",
      "epoch:4 step:4629 [D loss: 0.502119, acc: 78.12%] [G loss: 2.704496]\n",
      "epoch:4 step:4630 [D loss: 0.618077, acc: 67.19%] [G loss: 2.681613]\n",
      "epoch:4 step:4631 [D loss: 0.618366, acc: 71.88%] [G loss: 2.537370]\n",
      "epoch:4 step:4632 [D loss: 0.535209, acc: 77.34%] [G loss: 3.124855]\n",
      "epoch:4 step:4633 [D loss: 0.517272, acc: 72.66%] [G loss: 2.945307]\n",
      "epoch:4 step:4634 [D loss: 0.518529, acc: 71.09%] [G loss: 3.106733]\n",
      "epoch:4 step:4635 [D loss: 0.604825, acc: 64.06%] [G loss: 3.050407]\n",
      "epoch:4 step:4636 [D loss: 0.572217, acc: 72.66%] [G loss: 2.923923]\n",
      "epoch:4 step:4637 [D loss: 0.568745, acc: 71.88%] [G loss: 2.809310]\n",
      "epoch:4 step:4638 [D loss: 0.560228, acc: 70.31%] [G loss: 3.071939]\n",
      "epoch:4 step:4639 [D loss: 0.605186, acc: 65.62%] [G loss: 2.628596]\n",
      "epoch:4 step:4640 [D loss: 0.618972, acc: 65.62%] [G loss: 2.531373]\n",
      "epoch:4 step:4641 [D loss: 0.582597, acc: 67.19%] [G loss: 2.650661]\n",
      "epoch:4 step:4642 [D loss: 0.495445, acc: 76.56%] [G loss: 2.883457]\n",
      "epoch:4 step:4643 [D loss: 0.551294, acc: 69.53%] [G loss: 2.901002]\n",
      "epoch:4 step:4644 [D loss: 0.601975, acc: 67.19%] [G loss: 2.527338]\n",
      "epoch:4 step:4645 [D loss: 0.548647, acc: 73.44%] [G loss: 2.720831]\n",
      "epoch:4 step:4646 [D loss: 0.545814, acc: 73.44%] [G loss: 2.940150]\n",
      "epoch:4 step:4647 [D loss: 0.568968, acc: 67.97%] [G loss: 2.680466]\n",
      "epoch:4 step:4648 [D loss: 0.631573, acc: 67.97%] [G loss: 2.521193]\n",
      "epoch:4 step:4649 [D loss: 0.596098, acc: 70.31%] [G loss: 2.658798]\n",
      "epoch:4 step:4650 [D loss: 0.599592, acc: 67.19%] [G loss: 2.762922]\n",
      "epoch:4 step:4651 [D loss: 0.606364, acc: 70.31%] [G loss: 2.634224]\n",
      "epoch:4 step:4652 [D loss: 0.438125, acc: 80.47%] [G loss: 3.232879]\n",
      "epoch:4 step:4653 [D loss: 0.484815, acc: 76.56%] [G loss: 3.019274]\n",
      "epoch:4 step:4654 [D loss: 0.528186, acc: 73.44%] [G loss: 3.145240]\n",
      "epoch:4 step:4655 [D loss: 0.638136, acc: 64.84%] [G loss: 2.614278]\n",
      "epoch:4 step:4656 [D loss: 0.554237, acc: 70.31%] [G loss: 2.549432]\n",
      "epoch:4 step:4657 [D loss: 0.452314, acc: 78.12%] [G loss: 3.147088]\n",
      "epoch:4 step:4658 [D loss: 0.541538, acc: 77.34%] [G loss: 2.843092]\n",
      "epoch:4 step:4659 [D loss: 0.534568, acc: 75.00%] [G loss: 3.061317]\n",
      "epoch:4 step:4660 [D loss: 0.452706, acc: 78.91%] [G loss: 3.385296]\n",
      "epoch:4 step:4661 [D loss: 0.594267, acc: 67.19%] [G loss: 2.588861]\n",
      "epoch:4 step:4662 [D loss: 0.556538, acc: 74.22%] [G loss: 2.865702]\n",
      "epoch:4 step:4663 [D loss: 0.582541, acc: 65.62%] [G loss: 2.722644]\n",
      "epoch:4 step:4664 [D loss: 0.609148, acc: 64.84%] [G loss: 2.624968]\n",
      "epoch:4 step:4665 [D loss: 0.596803, acc: 65.62%] [G loss: 2.682611]\n",
      "epoch:4 step:4666 [D loss: 0.514171, acc: 71.88%] [G loss: 3.256083]\n",
      "epoch:4 step:4667 [D loss: 0.524170, acc: 74.22%] [G loss: 3.143802]\n",
      "epoch:4 step:4668 [D loss: 0.580803, acc: 71.88%] [G loss: 3.079232]\n",
      "epoch:4 step:4669 [D loss: 0.475126, acc: 75.00%] [G loss: 3.362823]\n",
      "epoch:4 step:4670 [D loss: 0.584689, acc: 71.88%] [G loss: 2.861072]\n",
      "epoch:4 step:4671 [D loss: 0.479361, acc: 78.12%] [G loss: 3.427465]\n",
      "epoch:4 step:4672 [D loss: 0.515038, acc: 71.88%] [G loss: 3.476752]\n",
      "epoch:4 step:4673 [D loss: 0.553110, acc: 72.66%] [G loss: 3.233516]\n",
      "epoch:4 step:4674 [D loss: 0.470592, acc: 76.56%] [G loss: 3.299213]\n",
      "epoch:4 step:4675 [D loss: 0.618083, acc: 67.19%] [G loss: 3.282912]\n",
      "epoch:4 step:4676 [D loss: 0.908917, acc: 50.00%] [G loss: 2.528151]\n",
      "epoch:4 step:4677 [D loss: 0.612647, acc: 68.75%] [G loss: 2.905735]\n",
      "epoch:4 step:4678 [D loss: 0.539709, acc: 78.91%] [G loss: 2.582534]\n",
      "epoch:4 step:4679 [D loss: 0.594675, acc: 68.75%] [G loss: 2.492659]\n",
      "epoch:4 step:4680 [D loss: 0.561652, acc: 70.31%] [G loss: 2.749170]\n",
      "epoch:4 step:4681 [D loss: 0.529926, acc: 75.78%] [G loss: 2.778734]\n",
      "epoch:4 step:4682 [D loss: 0.616890, acc: 67.19%] [G loss: 2.673893]\n",
      "epoch:4 step:4683 [D loss: 0.541085, acc: 78.91%] [G loss: 2.746031]\n",
      "epoch:4 step:4684 [D loss: 0.587751, acc: 71.88%] [G loss: 2.989880]\n",
      "epoch:4 step:4685 [D loss: 0.560632, acc: 70.31%] [G loss: 3.422528]\n",
      "epoch:5 step:4686 [D loss: 0.586301, acc: 67.97%] [G loss: 2.497260]\n",
      "epoch:5 step:4687 [D loss: 0.592701, acc: 66.41%] [G loss: 2.816159]\n",
      "epoch:5 step:4688 [D loss: 0.588916, acc: 65.62%] [G loss: 2.517033]\n",
      "epoch:5 step:4689 [D loss: 0.536691, acc: 75.00%] [G loss: 2.689904]\n",
      "epoch:5 step:4690 [D loss: 0.580603, acc: 68.75%] [G loss: 2.433596]\n",
      "epoch:5 step:4691 [D loss: 0.504767, acc: 78.12%] [G loss: 2.842190]\n",
      "epoch:5 step:4692 [D loss: 0.540002, acc: 70.31%] [G loss: 2.892156]\n",
      "epoch:5 step:4693 [D loss: 0.571639, acc: 68.75%] [G loss: 2.656869]\n",
      "epoch:5 step:4694 [D loss: 0.642310, acc: 65.62%] [G loss: 2.525873]\n",
      "epoch:5 step:4695 [D loss: 0.615336, acc: 65.62%] [G loss: 2.824655]\n",
      "epoch:5 step:4696 [D loss: 0.563413, acc: 73.44%] [G loss: 2.604177]\n",
      "epoch:5 step:4697 [D loss: 0.597562, acc: 73.44%] [G loss: 2.763501]\n",
      "epoch:5 step:4698 [D loss: 0.602596, acc: 67.19%] [G loss: 2.876283]\n",
      "epoch:5 step:4699 [D loss: 0.569226, acc: 71.09%] [G loss: 2.595068]\n",
      "epoch:5 step:4700 [D loss: 0.559519, acc: 70.31%] [G loss: 2.831441]\n",
      "epoch:5 step:4701 [D loss: 0.576165, acc: 73.44%] [G loss: 2.780265]\n",
      "epoch:5 step:4702 [D loss: 0.698348, acc: 58.59%] [G loss: 2.497455]\n",
      "epoch:5 step:4703 [D loss: 0.657815, acc: 59.38%] [G loss: 2.380305]\n",
      "epoch:5 step:4704 [D loss: 0.650643, acc: 57.03%] [G loss: 2.265604]\n",
      "epoch:5 step:4705 [D loss: 0.712105, acc: 56.25%] [G loss: 2.063399]\n",
      "epoch:5 step:4706 [D loss: 0.628409, acc: 66.41%] [G loss: 2.267485]\n",
      "epoch:5 step:4707 [D loss: 0.688099, acc: 64.06%] [G loss: 2.163422]\n",
      "epoch:5 step:4708 [D loss: 0.521356, acc: 71.88%] [G loss: 2.590023]\n",
      "epoch:5 step:4709 [D loss: 0.589140, acc: 68.75%] [G loss: 2.508890]\n",
      "epoch:5 step:4710 [D loss: 0.538313, acc: 73.44%] [G loss: 2.820320]\n",
      "epoch:5 step:4711 [D loss: 0.576564, acc: 73.44%] [G loss: 2.261922]\n",
      "epoch:5 step:4712 [D loss: 0.604163, acc: 68.75%] [G loss: 2.768372]\n",
      "epoch:5 step:4713 [D loss: 0.637256, acc: 67.19%] [G loss: 2.567462]\n",
      "epoch:5 step:4714 [D loss: 0.514112, acc: 76.56%] [G loss: 2.607703]\n",
      "epoch:5 step:4715 [D loss: 0.663747, acc: 61.72%] [G loss: 2.761996]\n",
      "epoch:5 step:4716 [D loss: 0.621875, acc: 67.19%] [G loss: 2.531797]\n",
      "epoch:5 step:4717 [D loss: 0.730807, acc: 59.38%] [G loss: 2.348062]\n",
      "epoch:5 step:4718 [D loss: 0.476990, acc: 81.25%] [G loss: 2.601710]\n",
      "epoch:5 step:4719 [D loss: 0.565357, acc: 67.19%] [G loss: 2.762027]\n",
      "epoch:5 step:4720 [D loss: 0.546080, acc: 72.66%] [G loss: 2.407123]\n",
      "epoch:5 step:4721 [D loss: 0.570278, acc: 70.31%] [G loss: 3.123391]\n",
      "epoch:5 step:4722 [D loss: 0.598893, acc: 64.84%] [G loss: 2.573987]\n",
      "epoch:5 step:4723 [D loss: 0.651073, acc: 64.84%] [G loss: 2.723391]\n",
      "epoch:5 step:4724 [D loss: 0.556969, acc: 76.56%] [G loss: 2.713998]\n",
      "epoch:5 step:4725 [D loss: 0.530165, acc: 72.66%] [G loss: 2.894222]\n",
      "epoch:5 step:4726 [D loss: 0.628007, acc: 67.97%] [G loss: 2.569022]\n",
      "epoch:5 step:4727 [D loss: 0.484756, acc: 79.69%] [G loss: 2.896035]\n",
      "epoch:5 step:4728 [D loss: 0.713459, acc: 62.50%] [G loss: 2.619143]\n",
      "epoch:5 step:4729 [D loss: 0.629155, acc: 64.84%] [G loss: 2.331715]\n",
      "epoch:5 step:4730 [D loss: 0.540065, acc: 71.88%] [G loss: 2.445778]\n",
      "epoch:5 step:4731 [D loss: 0.569757, acc: 71.09%] [G loss: 2.702324]\n",
      "epoch:5 step:4732 [D loss: 0.612859, acc: 69.53%] [G loss: 2.777800]\n",
      "epoch:5 step:4733 [D loss: 0.563086, acc: 71.09%] [G loss: 2.712425]\n",
      "epoch:5 step:4734 [D loss: 0.589713, acc: 71.09%] [G loss: 2.848563]\n",
      "epoch:5 step:4735 [D loss: 0.660454, acc: 67.97%] [G loss: 2.471918]\n",
      "epoch:5 step:4736 [D loss: 0.627406, acc: 67.97%] [G loss: 2.659945]\n",
      "epoch:5 step:4737 [D loss: 0.671623, acc: 63.28%] [G loss: 2.578228]\n",
      "epoch:5 step:4738 [D loss: 0.563996, acc: 73.44%] [G loss: 2.290586]\n",
      "epoch:5 step:4739 [D loss: 0.581887, acc: 68.75%] [G loss: 2.539582]\n",
      "epoch:5 step:4740 [D loss: 0.555203, acc: 73.44%] [G loss: 2.827619]\n",
      "epoch:5 step:4741 [D loss: 0.569404, acc: 67.97%] [G loss: 2.663341]\n",
      "epoch:5 step:4742 [D loss: 0.528504, acc: 72.66%] [G loss: 2.940635]\n",
      "epoch:5 step:4743 [D loss: 0.594200, acc: 68.75%] [G loss: 2.676062]\n",
      "epoch:5 step:4744 [D loss: 0.561881, acc: 71.88%] [G loss: 2.629710]\n",
      "epoch:5 step:4745 [D loss: 0.545631, acc: 71.88%] [G loss: 3.001914]\n",
      "epoch:5 step:4746 [D loss: 0.564277, acc: 68.75%] [G loss: 2.939606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4747 [D loss: 0.509472, acc: 78.12%] [G loss: 2.653916]\n",
      "epoch:5 step:4748 [D loss: 0.633162, acc: 63.28%] [G loss: 2.413052]\n",
      "epoch:5 step:4749 [D loss: 0.541072, acc: 69.53%] [G loss: 2.736940]\n",
      "epoch:5 step:4750 [D loss: 0.508336, acc: 76.56%] [G loss: 2.740588]\n",
      "epoch:5 step:4751 [D loss: 0.560581, acc: 69.53%] [G loss: 2.565154]\n",
      "epoch:5 step:4752 [D loss: 0.555065, acc: 77.34%] [G loss: 2.706772]\n",
      "epoch:5 step:4753 [D loss: 0.614937, acc: 62.50%] [G loss: 2.690720]\n",
      "epoch:5 step:4754 [D loss: 0.599593, acc: 64.84%] [G loss: 2.609316]\n",
      "epoch:5 step:4755 [D loss: 0.600172, acc: 64.84%] [G loss: 2.623175]\n",
      "epoch:5 step:4756 [D loss: 0.603880, acc: 68.75%] [G loss: 2.829556]\n",
      "epoch:5 step:4757 [D loss: 0.604974, acc: 69.53%] [G loss: 2.598330]\n",
      "epoch:5 step:4758 [D loss: 0.584455, acc: 69.53%] [G loss: 2.547590]\n",
      "epoch:5 step:4759 [D loss: 0.561195, acc: 73.44%] [G loss: 2.816011]\n",
      "epoch:5 step:4760 [D loss: 0.544780, acc: 75.00%] [G loss: 2.954626]\n",
      "epoch:5 step:4761 [D loss: 0.549411, acc: 69.53%] [G loss: 2.858009]\n",
      "epoch:5 step:4762 [D loss: 0.566268, acc: 73.44%] [G loss: 3.163144]\n",
      "epoch:5 step:4763 [D loss: 0.648967, acc: 64.06%] [G loss: 2.857341]\n",
      "epoch:5 step:4764 [D loss: 0.600703, acc: 67.97%] [G loss: 2.962714]\n",
      "epoch:5 step:4765 [D loss: 0.655623, acc: 66.41%] [G loss: 2.508056]\n",
      "epoch:5 step:4766 [D loss: 0.707145, acc: 57.03%] [G loss: 2.398075]\n",
      "epoch:5 step:4767 [D loss: 0.574396, acc: 68.75%] [G loss: 2.578762]\n",
      "epoch:5 step:4768 [D loss: 0.607225, acc: 67.97%] [G loss: 2.751218]\n",
      "epoch:5 step:4769 [D loss: 0.623862, acc: 64.84%] [G loss: 2.343066]\n",
      "epoch:5 step:4770 [D loss: 0.575244, acc: 67.19%] [G loss: 2.432155]\n",
      "epoch:5 step:4771 [D loss: 0.605496, acc: 67.97%] [G loss: 2.641554]\n",
      "epoch:5 step:4772 [D loss: 0.581623, acc: 65.62%] [G loss: 2.541045]\n",
      "epoch:5 step:4773 [D loss: 0.608224, acc: 67.97%] [G loss: 2.683656]\n",
      "epoch:5 step:4774 [D loss: 0.506787, acc: 79.69%] [G loss: 2.581661]\n",
      "epoch:5 step:4775 [D loss: 0.590951, acc: 68.75%] [G loss: 2.527249]\n",
      "epoch:5 step:4776 [D loss: 0.596789, acc: 69.53%] [G loss: 2.677394]\n",
      "epoch:5 step:4777 [D loss: 0.626043, acc: 64.06%] [G loss: 2.892456]\n",
      "epoch:5 step:4778 [D loss: 0.550960, acc: 71.09%] [G loss: 2.603985]\n",
      "epoch:5 step:4779 [D loss: 0.694427, acc: 60.94%] [G loss: 2.643907]\n",
      "epoch:5 step:4780 [D loss: 0.624607, acc: 64.06%] [G loss: 2.270547]\n",
      "epoch:5 step:4781 [D loss: 0.556348, acc: 71.88%] [G loss: 2.718985]\n",
      "epoch:5 step:4782 [D loss: 0.586994, acc: 69.53%] [G loss: 2.761719]\n",
      "epoch:5 step:4783 [D loss: 0.565815, acc: 69.53%] [G loss: 2.491214]\n",
      "epoch:5 step:4784 [D loss: 0.571784, acc: 67.19%] [G loss: 2.287820]\n",
      "epoch:5 step:4785 [D loss: 0.601133, acc: 67.19%] [G loss: 2.430899]\n",
      "epoch:5 step:4786 [D loss: 0.549457, acc: 71.88%] [G loss: 2.512568]\n",
      "epoch:5 step:4787 [D loss: 0.598596, acc: 64.84%] [G loss: 2.543131]\n",
      "epoch:5 step:4788 [D loss: 0.499153, acc: 76.56%] [G loss: 2.785763]\n",
      "epoch:5 step:4789 [D loss: 0.628870, acc: 67.97%] [G loss: 2.655841]\n",
      "epoch:5 step:4790 [D loss: 0.552721, acc: 70.31%] [G loss: 2.878319]\n",
      "epoch:5 step:4791 [D loss: 0.693523, acc: 66.41%] [G loss: 2.484691]\n",
      "epoch:5 step:4792 [D loss: 0.591448, acc: 68.75%] [G loss: 2.410029]\n",
      "epoch:5 step:4793 [D loss: 0.619405, acc: 66.41%] [G loss: 2.628739]\n",
      "epoch:5 step:4794 [D loss: 0.666671, acc: 61.72%] [G loss: 2.449386]\n",
      "epoch:5 step:4795 [D loss: 0.576923, acc: 76.56%] [G loss: 2.496412]\n",
      "epoch:5 step:4796 [D loss: 0.654070, acc: 62.50%] [G loss: 2.346266]\n",
      "epoch:5 step:4797 [D loss: 0.562294, acc: 67.97%] [G loss: 2.496153]\n",
      "epoch:5 step:4798 [D loss: 0.605156, acc: 67.97%] [G loss: 2.396549]\n",
      "epoch:5 step:4799 [D loss: 0.639103, acc: 61.72%] [G loss: 2.574489]\n",
      "epoch:5 step:4800 [D loss: 0.583758, acc: 69.53%] [G loss: 2.378833]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.602541\n",
      "FID: 48.009823\n",
      "0 = 13.347293403911598\n",
      "1 = 0.09495281453071672\n",
      "2 = 0.9376999735832214\n",
      "3 = 0.9377999901771545\n",
      "4 = 0.9376000165939331\n",
      "5 = 0.9376124739646912\n",
      "6 = 0.9377999901771545\n",
      "7 = 9.041202200913425\n",
      "8 = 0.13620552737073607\n",
      "9 = 0.843500018119812\n",
      "10 = 0.8384000062942505\n",
      "11 = 0.8485999703407288\n",
      "12 = 0.8470398187637329\n",
      "13 = 0.8384000062942505\n",
      "14 = 5.602560043334961\n",
      "15 = 8.8292818069458\n",
      "16 = 0.24873578548431396\n",
      "17 = 5.602541446685791\n",
      "18 = 48.009822845458984\n",
      "epoch:5 step:4801 [D loss: 0.565180, acc: 71.88%] [G loss: 2.478891]\n",
      "epoch:5 step:4802 [D loss: 0.567798, acc: 69.53%] [G loss: 2.861341]\n",
      "epoch:5 step:4803 [D loss: 0.624650, acc: 64.06%] [G loss: 2.579201]\n",
      "epoch:5 step:4804 [D loss: 0.488047, acc: 75.78%] [G loss: 3.073654]\n",
      "epoch:5 step:4805 [D loss: 0.663224, acc: 59.38%] [G loss: 2.614786]\n",
      "epoch:5 step:4806 [D loss: 0.621981, acc: 64.84%] [G loss: 2.482613]\n",
      "epoch:5 step:4807 [D loss: 0.633916, acc: 64.84%] [G loss: 2.818528]\n",
      "epoch:5 step:4808 [D loss: 0.575534, acc: 68.75%] [G loss: 2.784484]\n",
      "epoch:5 step:4809 [D loss: 0.586521, acc: 72.66%] [G loss: 2.469582]\n",
      "epoch:5 step:4810 [D loss: 0.595090, acc: 65.62%] [G loss: 2.338598]\n",
      "epoch:5 step:4811 [D loss: 0.622062, acc: 65.62%] [G loss: 2.526547]\n",
      "epoch:5 step:4812 [D loss: 0.586541, acc: 69.53%] [G loss: 2.441065]\n",
      "epoch:5 step:4813 [D loss: 0.579065, acc: 73.44%] [G loss: 2.575350]\n",
      "epoch:5 step:4814 [D loss: 0.680743, acc: 59.38%] [G loss: 2.366273]\n",
      "epoch:5 step:4815 [D loss: 0.518500, acc: 72.66%] [G loss: 2.902321]\n",
      "epoch:5 step:4816 [D loss: 0.512329, acc: 73.44%] [G loss: 2.459110]\n",
      "epoch:5 step:4817 [D loss: 0.597994, acc: 70.31%] [G loss: 2.433468]\n",
      "epoch:5 step:4818 [D loss: 0.625224, acc: 61.72%] [G loss: 2.482121]\n",
      "epoch:5 step:4819 [D loss: 0.585584, acc: 71.09%] [G loss: 2.432693]\n",
      "epoch:5 step:4820 [D loss: 0.631902, acc: 64.84%] [G loss: 2.723891]\n",
      "epoch:5 step:4821 [D loss: 0.527342, acc: 71.88%] [G loss: 2.657284]\n",
      "epoch:5 step:4822 [D loss: 0.657069, acc: 64.84%] [G loss: 2.431835]\n",
      "epoch:5 step:4823 [D loss: 0.622323, acc: 64.06%] [G loss: 2.466536]\n",
      "epoch:5 step:4824 [D loss: 0.604496, acc: 65.62%] [G loss: 2.634170]\n",
      "epoch:5 step:4825 [D loss: 0.698017, acc: 60.16%] [G loss: 2.361155]\n",
      "epoch:5 step:4826 [D loss: 0.611571, acc: 66.41%] [G loss: 2.652772]\n",
      "epoch:5 step:4827 [D loss: 0.532660, acc: 78.12%] [G loss: 2.817808]\n",
      "epoch:5 step:4828 [D loss: 0.621288, acc: 63.28%] [G loss: 2.627544]\n",
      "epoch:5 step:4829 [D loss: 0.480131, acc: 76.56%] [G loss: 3.035056]\n",
      "epoch:5 step:4830 [D loss: 0.550316, acc: 75.78%] [G loss: 2.667246]\n",
      "epoch:5 step:4831 [D loss: 0.608955, acc: 64.06%] [G loss: 2.519570]\n",
      "epoch:5 step:4832 [D loss: 0.529625, acc: 73.44%] [G loss: 2.552062]\n",
      "epoch:5 step:4833 [D loss: 0.562543, acc: 73.44%] [G loss: 2.418139]\n",
      "epoch:5 step:4834 [D loss: 0.583868, acc: 67.97%] [G loss: 2.988921]\n",
      "epoch:5 step:4835 [D loss: 0.574562, acc: 68.75%] [G loss: 2.964560]\n",
      "epoch:5 step:4836 [D loss: 0.541367, acc: 69.53%] [G loss: 2.956305]\n",
      "epoch:5 step:4837 [D loss: 0.612145, acc: 67.97%] [G loss: 2.645425]\n",
      "epoch:5 step:4838 [D loss: 0.615762, acc: 69.53%] [G loss: 2.523323]\n",
      "epoch:5 step:4839 [D loss: 0.499073, acc: 78.91%] [G loss: 2.685886]\n",
      "epoch:5 step:4840 [D loss: 0.532084, acc: 74.22%] [G loss: 2.903811]\n",
      "epoch:5 step:4841 [D loss: 0.559196, acc: 71.88%] [G loss: 2.638803]\n",
      "epoch:5 step:4842 [D loss: 0.524500, acc: 76.56%] [G loss: 2.407039]\n",
      "epoch:5 step:4843 [D loss: 0.567602, acc: 67.97%] [G loss: 2.622063]\n",
      "epoch:5 step:4844 [D loss: 0.579936, acc: 70.31%] [G loss: 2.877362]\n",
      "epoch:5 step:4845 [D loss: 0.659171, acc: 62.50%] [G loss: 2.344349]\n",
      "epoch:5 step:4846 [D loss: 0.581601, acc: 66.41%] [G loss: 2.864564]\n",
      "epoch:5 step:4847 [D loss: 0.505933, acc: 76.56%] [G loss: 3.108072]\n",
      "epoch:5 step:4848 [D loss: 0.572921, acc: 69.53%] [G loss: 2.713083]\n",
      "epoch:5 step:4849 [D loss: 0.591172, acc: 76.56%] [G loss: 2.600415]\n",
      "epoch:5 step:4850 [D loss: 0.561748, acc: 69.53%] [G loss: 2.525258]\n",
      "epoch:5 step:4851 [D loss: 0.525686, acc: 71.09%] [G loss: 2.461749]\n",
      "epoch:5 step:4852 [D loss: 0.689969, acc: 57.03%] [G loss: 2.770247]\n",
      "epoch:5 step:4853 [D loss: 0.646143, acc: 64.84%] [G loss: 2.590662]\n",
      "epoch:5 step:4854 [D loss: 0.532608, acc: 71.88%] [G loss: 2.501589]\n",
      "epoch:5 step:4855 [D loss: 0.636614, acc: 66.41%] [G loss: 2.399982]\n",
      "epoch:5 step:4856 [D loss: 0.563682, acc: 71.88%] [G loss: 2.706589]\n",
      "epoch:5 step:4857 [D loss: 0.585936, acc: 69.53%] [G loss: 2.952424]\n",
      "epoch:5 step:4858 [D loss: 0.586695, acc: 65.62%] [G loss: 2.456283]\n",
      "epoch:5 step:4859 [D loss: 0.553908, acc: 71.09%] [G loss: 2.769376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4860 [D loss: 0.619036, acc: 69.53%] [G loss: 2.774142]\n",
      "epoch:5 step:4861 [D loss: 0.516382, acc: 74.22%] [G loss: 2.642454]\n",
      "epoch:5 step:4862 [D loss: 0.652011, acc: 62.50%] [G loss: 2.335647]\n",
      "epoch:5 step:4863 [D loss: 0.530150, acc: 71.88%] [G loss: 2.918794]\n",
      "epoch:5 step:4864 [D loss: 0.621118, acc: 64.84%] [G loss: 2.559357]\n",
      "epoch:5 step:4865 [D loss: 0.621040, acc: 67.97%] [G loss: 2.511268]\n",
      "epoch:5 step:4866 [D loss: 0.700292, acc: 61.72%] [G loss: 2.636371]\n",
      "epoch:5 step:4867 [D loss: 0.687331, acc: 60.16%] [G loss: 2.597289]\n",
      "epoch:5 step:4868 [D loss: 0.672890, acc: 60.16%] [G loss: 2.645004]\n",
      "epoch:5 step:4869 [D loss: 0.562087, acc: 68.75%] [G loss: 2.361034]\n",
      "epoch:5 step:4870 [D loss: 0.607082, acc: 61.72%] [G loss: 2.546324]\n",
      "epoch:5 step:4871 [D loss: 0.607031, acc: 68.75%] [G loss: 2.607757]\n",
      "epoch:5 step:4872 [D loss: 0.632823, acc: 64.84%] [G loss: 2.411687]\n",
      "epoch:5 step:4873 [D loss: 0.484340, acc: 78.91%] [G loss: 2.848097]\n",
      "epoch:5 step:4874 [D loss: 0.632818, acc: 66.41%] [G loss: 2.579004]\n",
      "epoch:5 step:4875 [D loss: 0.542583, acc: 66.41%] [G loss: 2.614209]\n",
      "epoch:5 step:4876 [D loss: 0.550171, acc: 72.66%] [G loss: 2.596491]\n",
      "epoch:5 step:4877 [D loss: 0.557777, acc: 69.53%] [G loss: 2.868056]\n",
      "epoch:5 step:4878 [D loss: 0.624621, acc: 64.06%] [G loss: 2.751272]\n",
      "epoch:5 step:4879 [D loss: 0.511506, acc: 75.78%] [G loss: 2.841628]\n",
      "epoch:5 step:4880 [D loss: 0.574892, acc: 73.44%] [G loss: 2.553021]\n",
      "epoch:5 step:4881 [D loss: 0.642995, acc: 64.84%] [G loss: 2.375350]\n",
      "epoch:5 step:4882 [D loss: 0.588706, acc: 66.41%] [G loss: 2.527965]\n",
      "epoch:5 step:4883 [D loss: 0.573937, acc: 69.53%] [G loss: 2.702970]\n",
      "epoch:5 step:4884 [D loss: 0.572946, acc: 68.75%] [G loss: 2.619361]\n",
      "epoch:5 step:4885 [D loss: 0.587919, acc: 67.19%] [G loss: 2.233619]\n",
      "epoch:5 step:4886 [D loss: 0.570572, acc: 71.09%] [G loss: 2.510849]\n",
      "epoch:5 step:4887 [D loss: 0.574823, acc: 71.88%] [G loss: 2.727358]\n",
      "epoch:5 step:4888 [D loss: 0.630297, acc: 66.41%] [G loss: 2.256061]\n",
      "epoch:5 step:4889 [D loss: 0.620550, acc: 68.75%] [G loss: 2.872681]\n",
      "epoch:5 step:4890 [D loss: 0.594688, acc: 69.53%] [G loss: 2.656872]\n",
      "epoch:5 step:4891 [D loss: 0.512813, acc: 72.66%] [G loss: 2.964485]\n",
      "epoch:5 step:4892 [D loss: 0.565881, acc: 71.88%] [G loss: 2.935561]\n",
      "epoch:5 step:4893 [D loss: 0.546142, acc: 72.66%] [G loss: 2.923328]\n",
      "epoch:5 step:4894 [D loss: 0.514025, acc: 77.34%] [G loss: 2.757650]\n",
      "epoch:5 step:4895 [D loss: 0.612308, acc: 67.97%] [G loss: 2.565034]\n",
      "epoch:5 step:4896 [D loss: 0.666137, acc: 64.06%] [G loss: 2.511106]\n",
      "epoch:5 step:4897 [D loss: 0.620910, acc: 65.62%] [G loss: 2.583486]\n",
      "epoch:5 step:4898 [D loss: 0.644333, acc: 62.50%] [G loss: 2.479635]\n",
      "epoch:5 step:4899 [D loss: 0.663791, acc: 57.03%] [G loss: 2.487133]\n",
      "epoch:5 step:4900 [D loss: 0.639820, acc: 61.72%] [G loss: 2.542121]\n",
      "epoch:5 step:4901 [D loss: 0.627595, acc: 63.28%] [G loss: 2.640559]\n",
      "epoch:5 step:4902 [D loss: 0.562867, acc: 70.31%] [G loss: 2.870124]\n",
      "epoch:5 step:4903 [D loss: 0.498939, acc: 77.34%] [G loss: 2.984774]\n",
      "epoch:5 step:4904 [D loss: 0.506750, acc: 71.88%] [G loss: 2.952601]\n",
      "epoch:5 step:4905 [D loss: 0.734280, acc: 57.03%] [G loss: 2.581793]\n",
      "epoch:5 step:4906 [D loss: 0.600767, acc: 68.75%] [G loss: 3.224722]\n",
      "epoch:5 step:4907 [D loss: 0.611676, acc: 68.75%] [G loss: 2.785802]\n",
      "epoch:5 step:4908 [D loss: 0.605561, acc: 67.19%] [G loss: 3.083060]\n",
      "epoch:5 step:4909 [D loss: 0.574085, acc: 70.31%] [G loss: 2.514637]\n",
      "epoch:5 step:4910 [D loss: 0.697484, acc: 64.06%] [G loss: 2.602356]\n",
      "epoch:5 step:4911 [D loss: 0.563334, acc: 73.44%] [G loss: 2.490075]\n",
      "epoch:5 step:4912 [D loss: 0.592877, acc: 67.97%] [G loss: 2.274193]\n",
      "epoch:5 step:4913 [D loss: 0.600077, acc: 67.97%] [G loss: 2.265670]\n",
      "epoch:5 step:4914 [D loss: 0.571666, acc: 66.41%] [G loss: 2.817160]\n",
      "epoch:5 step:4915 [D loss: 0.502883, acc: 71.09%] [G loss: 3.347966]\n",
      "epoch:5 step:4916 [D loss: 0.527536, acc: 75.00%] [G loss: 3.131638]\n",
      "epoch:5 step:4917 [D loss: 0.501865, acc: 77.34%] [G loss: 3.270717]\n",
      "epoch:5 step:4918 [D loss: 0.616543, acc: 69.53%] [G loss: 2.795990]\n",
      "epoch:5 step:4919 [D loss: 0.645086, acc: 64.06%] [G loss: 2.495493]\n",
      "epoch:5 step:4920 [D loss: 0.572036, acc: 66.41%] [G loss: 2.426309]\n",
      "epoch:5 step:4921 [D loss: 0.532359, acc: 75.78%] [G loss: 2.537319]\n",
      "epoch:5 step:4922 [D loss: 0.662487, acc: 68.75%] [G loss: 2.467290]\n",
      "epoch:5 step:4923 [D loss: 0.563929, acc: 75.78%] [G loss: 2.455640]\n",
      "epoch:5 step:4924 [D loss: 0.542380, acc: 74.22%] [G loss: 2.472267]\n",
      "epoch:5 step:4925 [D loss: 0.584528, acc: 67.97%] [G loss: 2.757539]\n",
      "epoch:5 step:4926 [D loss: 0.582711, acc: 71.09%] [G loss: 2.554134]\n",
      "epoch:5 step:4927 [D loss: 0.513348, acc: 76.56%] [G loss: 2.785356]\n",
      "epoch:5 step:4928 [D loss: 0.684958, acc: 62.50%] [G loss: 2.637139]\n",
      "epoch:5 step:4929 [D loss: 0.567521, acc: 74.22%] [G loss: 2.403564]\n",
      "epoch:5 step:4930 [D loss: 0.576281, acc: 70.31%] [G loss: 2.351622]\n",
      "epoch:5 step:4931 [D loss: 0.626752, acc: 60.16%] [G loss: 2.479984]\n",
      "epoch:5 step:4932 [D loss: 0.649505, acc: 67.19%] [G loss: 2.404174]\n",
      "epoch:5 step:4933 [D loss: 0.577688, acc: 71.09%] [G loss: 2.534485]\n",
      "epoch:5 step:4934 [D loss: 0.716106, acc: 60.16%] [G loss: 2.517206]\n",
      "epoch:5 step:4935 [D loss: 0.729642, acc: 56.25%] [G loss: 2.385040]\n",
      "epoch:5 step:4936 [D loss: 0.621153, acc: 62.50%] [G loss: 2.326585]\n",
      "epoch:5 step:4937 [D loss: 0.629454, acc: 63.28%] [G loss: 2.243723]\n",
      "epoch:5 step:4938 [D loss: 0.553654, acc: 73.44%] [G loss: 2.320713]\n",
      "epoch:5 step:4939 [D loss: 0.677948, acc: 58.59%] [G loss: 2.550952]\n",
      "epoch:5 step:4940 [D loss: 0.594296, acc: 70.31%] [G loss: 2.597003]\n",
      "epoch:5 step:4941 [D loss: 0.572090, acc: 67.19%] [G loss: 2.577675]\n",
      "epoch:5 step:4942 [D loss: 0.601823, acc: 71.09%] [G loss: 2.429399]\n",
      "epoch:5 step:4943 [D loss: 0.620403, acc: 67.19%] [G loss: 2.725863]\n",
      "epoch:5 step:4944 [D loss: 0.538142, acc: 75.78%] [G loss: 2.600530]\n",
      "epoch:5 step:4945 [D loss: 0.537146, acc: 75.00%] [G loss: 2.695662]\n",
      "epoch:5 step:4946 [D loss: 0.569452, acc: 67.97%] [G loss: 2.718851]\n",
      "epoch:5 step:4947 [D loss: 0.554254, acc: 71.09%] [G loss: 2.713808]\n",
      "epoch:5 step:4948 [D loss: 0.615288, acc: 67.97%] [G loss: 2.423645]\n",
      "epoch:5 step:4949 [D loss: 0.552697, acc: 71.88%] [G loss: 2.890280]\n",
      "epoch:5 step:4950 [D loss: 0.562540, acc: 69.53%] [G loss: 2.740021]\n",
      "epoch:5 step:4951 [D loss: 0.591632, acc: 65.62%] [G loss: 2.793680]\n",
      "epoch:5 step:4952 [D loss: 0.538643, acc: 74.22%] [G loss: 2.623570]\n",
      "epoch:5 step:4953 [D loss: 0.576389, acc: 70.31%] [G loss: 2.426694]\n",
      "epoch:5 step:4954 [D loss: 0.601404, acc: 65.62%] [G loss: 2.735585]\n",
      "epoch:5 step:4955 [D loss: 0.560386, acc: 71.88%] [G loss: 2.822827]\n",
      "epoch:5 step:4956 [D loss: 0.546590, acc: 72.66%] [G loss: 2.734830]\n",
      "epoch:5 step:4957 [D loss: 0.625737, acc: 65.62%] [G loss: 2.718141]\n",
      "epoch:5 step:4958 [D loss: 0.609388, acc: 67.19%] [G loss: 2.436431]\n",
      "epoch:5 step:4959 [D loss: 0.559640, acc: 68.75%] [G loss: 2.488061]\n",
      "epoch:5 step:4960 [D loss: 0.613729, acc: 64.84%] [G loss: 2.556774]\n",
      "epoch:5 step:4961 [D loss: 0.638159, acc: 70.31%] [G loss: 2.396458]\n",
      "epoch:5 step:4962 [D loss: 0.621816, acc: 57.81%] [G loss: 2.185682]\n",
      "epoch:5 step:4963 [D loss: 0.611612, acc: 68.75%] [G loss: 2.557104]\n",
      "epoch:5 step:4964 [D loss: 0.564803, acc: 70.31%] [G loss: 2.559787]\n",
      "epoch:5 step:4965 [D loss: 0.614598, acc: 66.41%] [G loss: 2.807006]\n",
      "epoch:5 step:4966 [D loss: 0.671438, acc: 61.72%] [G loss: 2.332765]\n",
      "epoch:5 step:4967 [D loss: 0.624359, acc: 68.75%] [G loss: 2.431948]\n",
      "epoch:5 step:4968 [D loss: 0.530930, acc: 72.66%] [G loss: 2.378407]\n",
      "epoch:5 step:4969 [D loss: 0.497485, acc: 75.00%] [G loss: 2.591704]\n",
      "epoch:5 step:4970 [D loss: 0.610645, acc: 69.53%] [G loss: 2.737242]\n",
      "epoch:5 step:4971 [D loss: 0.600651, acc: 68.75%] [G loss: 2.923308]\n",
      "epoch:5 step:4972 [D loss: 0.627529, acc: 70.31%] [G loss: 2.660657]\n",
      "epoch:5 step:4973 [D loss: 0.621130, acc: 63.28%] [G loss: 2.138318]\n",
      "epoch:5 step:4974 [D loss: 0.586601, acc: 71.09%] [G loss: 2.514096]\n",
      "epoch:5 step:4975 [D loss: 0.585036, acc: 67.97%] [G loss: 2.370330]\n",
      "epoch:5 step:4976 [D loss: 0.555616, acc: 71.09%] [G loss: 2.486100]\n",
      "epoch:5 step:4977 [D loss: 0.628180, acc: 61.72%] [G loss: 2.495495]\n",
      "epoch:5 step:4978 [D loss: 0.557746, acc: 74.22%] [G loss: 2.791401]\n",
      "epoch:5 step:4979 [D loss: 0.597643, acc: 67.19%] [G loss: 2.484161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4980 [D loss: 0.542802, acc: 67.97%] [G loss: 2.574430]\n",
      "epoch:5 step:4981 [D loss: 0.523467, acc: 75.00%] [G loss: 2.673813]\n",
      "epoch:5 step:4982 [D loss: 0.577129, acc: 69.53%] [G loss: 2.672835]\n",
      "epoch:5 step:4983 [D loss: 0.588185, acc: 75.00%] [G loss: 2.797153]\n",
      "epoch:5 step:4984 [D loss: 0.632235, acc: 67.97%] [G loss: 2.660869]\n",
      "epoch:5 step:4985 [D loss: 0.491789, acc: 75.78%] [G loss: 2.514364]\n",
      "epoch:5 step:4986 [D loss: 0.694354, acc: 60.94%] [G loss: 2.294251]\n",
      "epoch:5 step:4987 [D loss: 0.610317, acc: 65.62%] [G loss: 2.386709]\n",
      "epoch:5 step:4988 [D loss: 0.590679, acc: 65.62%] [G loss: 2.328831]\n",
      "epoch:5 step:4989 [D loss: 0.566903, acc: 71.09%] [G loss: 2.819771]\n",
      "epoch:5 step:4990 [D loss: 0.538092, acc: 76.56%] [G loss: 2.793787]\n",
      "epoch:5 step:4991 [D loss: 0.692982, acc: 62.50%] [G loss: 2.411241]\n",
      "epoch:5 step:4992 [D loss: 0.530206, acc: 73.44%] [G loss: 2.479918]\n",
      "epoch:5 step:4993 [D loss: 0.621775, acc: 65.62%] [G loss: 2.550080]\n",
      "epoch:5 step:4994 [D loss: 0.600401, acc: 63.28%] [G loss: 2.672741]\n",
      "epoch:5 step:4995 [D loss: 0.519724, acc: 73.44%] [G loss: 2.644281]\n",
      "epoch:5 step:4996 [D loss: 0.618400, acc: 67.97%] [G loss: 2.679981]\n",
      "epoch:5 step:4997 [D loss: 0.470390, acc: 77.34%] [G loss: 3.353182]\n",
      "epoch:5 step:4998 [D loss: 0.502592, acc: 75.78%] [G loss: 3.253857]\n",
      "epoch:5 step:4999 [D loss: 0.493236, acc: 78.91%] [G loss: 3.472251]\n",
      "epoch:5 step:5000 [D loss: 0.453635, acc: 81.25%] [G loss: 3.324926]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.577183\n",
      "FID: 48.352657\n",
      "0 = 13.150504543972056\n",
      "1 = 0.0917267226957016\n",
      "2 = 0.9340000152587891\n",
      "3 = 0.9309999942779541\n",
      "4 = 0.9369999766349792\n",
      "5 = 0.9366196990013123\n",
      "6 = 0.9309999942779541\n",
      "7 = 9.050814889311793\n",
      "8 = 0.13762863384442148\n",
      "9 = 0.8400999903678894\n",
      "10 = 0.8407999873161316\n",
      "11 = 0.8393999934196472\n",
      "12 = 0.8396245241165161\n",
      "13 = 0.8407999873161316\n",
      "14 = 5.5771989822387695\n",
      "15 = 8.798288345336914\n",
      "16 = 0.26261597871780396\n",
      "17 = 5.577182769775391\n",
      "18 = 48.352657318115234\n",
      "epoch:5 step:5001 [D loss: 0.748535, acc: 55.47%] [G loss: 2.331500]\n",
      "epoch:5 step:5002 [D loss: 0.591289, acc: 65.62%] [G loss: 2.610228]\n",
      "epoch:5 step:5003 [D loss: 0.610601, acc: 71.09%] [G loss: 2.741599]\n",
      "epoch:5 step:5004 [D loss: 0.505381, acc: 81.25%] [G loss: 2.599843]\n",
      "epoch:5 step:5005 [D loss: 0.576756, acc: 62.50%] [G loss: 2.706116]\n",
      "epoch:5 step:5006 [D loss: 0.520233, acc: 71.88%] [G loss: 3.007914]\n",
      "epoch:5 step:5007 [D loss: 0.584490, acc: 69.53%] [G loss: 2.757668]\n",
      "epoch:5 step:5008 [D loss: 0.676669, acc: 60.94%] [G loss: 2.494861]\n",
      "epoch:5 step:5009 [D loss: 0.591365, acc: 66.41%] [G loss: 2.357500]\n",
      "epoch:5 step:5010 [D loss: 0.567698, acc: 70.31%] [G loss: 2.505495]\n",
      "epoch:5 step:5011 [D loss: 0.537035, acc: 71.09%] [G loss: 2.591254]\n",
      "epoch:5 step:5012 [D loss: 0.613700, acc: 61.72%] [G loss: 2.597527]\n",
      "epoch:5 step:5013 [D loss: 0.638090, acc: 70.31%] [G loss: 2.410480]\n",
      "epoch:5 step:5014 [D loss: 0.664483, acc: 63.28%] [G loss: 2.546422]\n",
      "epoch:5 step:5015 [D loss: 0.549166, acc: 72.66%] [G loss: 2.633812]\n",
      "epoch:5 step:5016 [D loss: 0.592950, acc: 64.84%] [G loss: 2.440818]\n",
      "epoch:5 step:5017 [D loss: 0.491805, acc: 76.56%] [G loss: 3.012214]\n",
      "epoch:5 step:5018 [D loss: 0.509723, acc: 73.44%] [G loss: 2.911957]\n",
      "epoch:5 step:5019 [D loss: 0.553733, acc: 70.31%] [G loss: 2.793650]\n",
      "epoch:5 step:5020 [D loss: 0.605896, acc: 73.44%] [G loss: 2.953888]\n",
      "epoch:5 step:5021 [D loss: 0.570726, acc: 70.31%] [G loss: 2.658531]\n",
      "epoch:5 step:5022 [D loss: 0.523190, acc: 71.09%] [G loss: 2.987635]\n",
      "epoch:5 step:5023 [D loss: 0.574656, acc: 67.19%] [G loss: 2.882209]\n",
      "epoch:5 step:5024 [D loss: 0.552535, acc: 73.44%] [G loss: 2.775191]\n",
      "epoch:5 step:5025 [D loss: 0.559662, acc: 67.97%] [G loss: 3.092554]\n",
      "epoch:5 step:5026 [D loss: 0.617027, acc: 65.62%] [G loss: 2.478441]\n",
      "epoch:5 step:5027 [D loss: 0.667762, acc: 60.94%] [G loss: 2.656196]\n",
      "epoch:5 step:5028 [D loss: 0.658398, acc: 62.50%] [G loss: 2.782980]\n",
      "epoch:5 step:5029 [D loss: 0.590230, acc: 65.62%] [G loss: 2.950060]\n",
      "epoch:5 step:5030 [D loss: 0.526805, acc: 79.69%] [G loss: 2.912611]\n",
      "epoch:5 step:5031 [D loss: 0.491342, acc: 72.66%] [G loss: 3.231006]\n",
      "epoch:5 step:5032 [D loss: 0.504117, acc: 75.78%] [G loss: 3.190339]\n",
      "epoch:5 step:5033 [D loss: 0.679689, acc: 64.06%] [G loss: 2.403117]\n",
      "epoch:5 step:5034 [D loss: 0.659451, acc: 60.16%] [G loss: 2.431447]\n",
      "epoch:5 step:5035 [D loss: 0.583946, acc: 68.75%] [G loss: 2.609504]\n",
      "epoch:5 step:5036 [D loss: 0.563100, acc: 74.22%] [G loss: 2.461440]\n",
      "epoch:5 step:5037 [D loss: 0.653204, acc: 64.84%] [G loss: 2.654066]\n",
      "epoch:5 step:5038 [D loss: 0.609280, acc: 67.19%] [G loss: 2.577130]\n",
      "epoch:5 step:5039 [D loss: 0.523690, acc: 74.22%] [G loss: 2.699403]\n",
      "epoch:5 step:5040 [D loss: 0.602743, acc: 69.53%] [G loss: 2.663998]\n",
      "epoch:5 step:5041 [D loss: 0.644762, acc: 65.62%] [G loss: 2.605204]\n",
      "epoch:5 step:5042 [D loss: 0.593856, acc: 67.97%] [G loss: 2.802014]\n",
      "epoch:5 step:5043 [D loss: 0.546986, acc: 69.53%] [G loss: 2.667526]\n",
      "epoch:5 step:5044 [D loss: 0.564381, acc: 74.22%] [G loss: 2.624282]\n",
      "epoch:5 step:5045 [D loss: 0.594194, acc: 70.31%] [G loss: 2.519032]\n",
      "epoch:5 step:5046 [D loss: 0.543573, acc: 75.00%] [G loss: 2.499857]\n",
      "epoch:5 step:5047 [D loss: 0.715959, acc: 59.38%] [G loss: 2.487764]\n",
      "epoch:5 step:5048 [D loss: 0.652680, acc: 64.84%] [G loss: 2.665240]\n",
      "epoch:5 step:5049 [D loss: 0.527666, acc: 75.78%] [G loss: 2.574389]\n",
      "epoch:5 step:5050 [D loss: 0.608123, acc: 66.41%] [G loss: 2.925872]\n",
      "epoch:5 step:5051 [D loss: 0.588991, acc: 71.09%] [G loss: 2.883450]\n",
      "epoch:5 step:5052 [D loss: 0.611602, acc: 67.19%] [G loss: 2.321653]\n",
      "epoch:5 step:5053 [D loss: 0.534119, acc: 70.31%] [G loss: 2.635144]\n",
      "epoch:5 step:5054 [D loss: 0.590719, acc: 70.31%] [G loss: 2.506015]\n",
      "epoch:5 step:5055 [D loss: 0.623376, acc: 66.41%] [G loss: 2.498477]\n",
      "epoch:5 step:5056 [D loss: 0.578033, acc: 70.31%] [G loss: 2.680632]\n",
      "epoch:5 step:5057 [D loss: 0.598231, acc: 65.62%] [G loss: 2.570326]\n",
      "epoch:5 step:5058 [D loss: 0.646481, acc: 65.62%] [G loss: 2.598620]\n",
      "epoch:5 step:5059 [D loss: 0.550454, acc: 75.78%] [G loss: 2.503758]\n",
      "epoch:5 step:5060 [D loss: 0.571530, acc: 72.66%] [G loss: 2.514101]\n",
      "epoch:5 step:5061 [D loss: 0.678570, acc: 60.16%] [G loss: 2.113070]\n",
      "epoch:5 step:5062 [D loss: 0.649207, acc: 65.62%] [G loss: 2.092801]\n",
      "epoch:5 step:5063 [D loss: 0.594390, acc: 65.62%] [G loss: 2.370526]\n",
      "epoch:5 step:5064 [D loss: 0.583428, acc: 73.44%] [G loss: 2.336751]\n",
      "epoch:5 step:5065 [D loss: 0.676854, acc: 64.06%] [G loss: 2.413993]\n",
      "epoch:5 step:5066 [D loss: 0.550733, acc: 74.22%] [G loss: 2.721930]\n",
      "epoch:5 step:5067 [D loss: 0.567564, acc: 70.31%] [G loss: 2.383493]\n",
      "epoch:5 step:5068 [D loss: 0.636771, acc: 63.28%] [G loss: 2.420533]\n",
      "epoch:5 step:5069 [D loss: 0.647295, acc: 65.62%] [G loss: 2.280336]\n",
      "epoch:5 step:5070 [D loss: 0.556022, acc: 67.19%] [G loss: 2.504692]\n",
      "epoch:5 step:5071 [D loss: 0.618016, acc: 63.28%] [G loss: 2.333349]\n",
      "epoch:5 step:5072 [D loss: 0.682818, acc: 62.50%] [G loss: 2.085210]\n",
      "epoch:5 step:5073 [D loss: 0.556970, acc: 71.09%] [G loss: 2.171865]\n",
      "epoch:5 step:5074 [D loss: 0.662141, acc: 62.50%] [G loss: 2.237416]\n",
      "epoch:5 step:5075 [D loss: 0.559298, acc: 67.97%] [G loss: 2.878669]\n",
      "epoch:5 step:5076 [D loss: 0.578240, acc: 68.75%] [G loss: 2.475591]\n",
      "epoch:5 step:5077 [D loss: 0.558600, acc: 74.22%] [G loss: 2.653336]\n",
      "epoch:5 step:5078 [D loss: 0.597382, acc: 64.84%] [G loss: 2.468004]\n",
      "epoch:5 step:5079 [D loss: 0.607244, acc: 66.41%] [G loss: 2.621994]\n",
      "epoch:5 step:5080 [D loss: 0.574161, acc: 69.53%] [G loss: 2.563725]\n",
      "epoch:5 step:5081 [D loss: 0.690381, acc: 60.94%] [G loss: 2.497017]\n",
      "epoch:5 step:5082 [D loss: 0.597611, acc: 67.19%] [G loss: 2.595705]\n",
      "epoch:5 step:5083 [D loss: 0.545346, acc: 71.88%] [G loss: 2.426073]\n",
      "epoch:5 step:5084 [D loss: 0.615016, acc: 63.28%] [G loss: 2.682766]\n",
      "epoch:5 step:5085 [D loss: 0.667212, acc: 60.16%] [G loss: 2.385331]\n",
      "epoch:5 step:5086 [D loss: 0.612015, acc: 64.84%] [G loss: 2.415076]\n",
      "epoch:5 step:5087 [D loss: 0.550190, acc: 75.78%] [G loss: 2.575268]\n",
      "epoch:5 step:5088 [D loss: 0.594189, acc: 64.84%] [G loss: 2.466286]\n",
      "epoch:5 step:5089 [D loss: 0.562009, acc: 70.31%] [G loss: 2.518213]\n",
      "epoch:5 step:5090 [D loss: 0.592043, acc: 75.78%] [G loss: 3.059117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5091 [D loss: 0.568398, acc: 71.09%] [G loss: 2.775756]\n",
      "epoch:5 step:5092 [D loss: 0.588854, acc: 68.75%] [G loss: 2.854955]\n",
      "epoch:5 step:5093 [D loss: 0.610356, acc: 66.41%] [G loss: 2.691662]\n",
      "epoch:5 step:5094 [D loss: 0.577678, acc: 67.97%] [G loss: 2.475264]\n",
      "epoch:5 step:5095 [D loss: 0.590756, acc: 67.19%] [G loss: 2.345480]\n",
      "epoch:5 step:5096 [D loss: 0.598241, acc: 65.62%] [G loss: 2.714069]\n",
      "epoch:5 step:5097 [D loss: 0.661007, acc: 62.50%] [G loss: 2.562759]\n",
      "epoch:5 step:5098 [D loss: 0.569010, acc: 65.62%] [G loss: 2.780294]\n",
      "epoch:5 step:5099 [D loss: 0.622499, acc: 71.09%] [G loss: 2.708252]\n",
      "epoch:5 step:5100 [D loss: 0.660360, acc: 58.59%] [G loss: 2.343921]\n",
      "epoch:5 step:5101 [D loss: 0.573489, acc: 67.19%] [G loss: 2.362617]\n",
      "epoch:5 step:5102 [D loss: 0.635969, acc: 64.84%] [G loss: 2.521834]\n",
      "epoch:5 step:5103 [D loss: 0.667746, acc: 58.59%] [G loss: 2.478144]\n",
      "epoch:5 step:5104 [D loss: 0.659721, acc: 64.06%] [G loss: 2.463813]\n",
      "epoch:5 step:5105 [D loss: 0.642170, acc: 67.97%] [G loss: 2.593078]\n",
      "epoch:5 step:5106 [D loss: 0.598434, acc: 70.31%] [G loss: 2.303783]\n",
      "epoch:5 step:5107 [D loss: 0.643699, acc: 58.59%] [G loss: 2.125789]\n",
      "epoch:5 step:5108 [D loss: 0.670496, acc: 58.59%] [G loss: 2.287587]\n",
      "epoch:5 step:5109 [D loss: 0.546312, acc: 74.22%] [G loss: 2.308273]\n",
      "epoch:5 step:5110 [D loss: 0.669726, acc: 63.28%] [G loss: 2.462621]\n",
      "epoch:5 step:5111 [D loss: 0.555174, acc: 73.44%] [G loss: 2.395438]\n",
      "epoch:5 step:5112 [D loss: 0.536924, acc: 77.34%] [G loss: 2.987258]\n",
      "epoch:5 step:5113 [D loss: 0.478468, acc: 81.25%] [G loss: 2.976086]\n",
      "epoch:5 step:5114 [D loss: 0.452950, acc: 77.34%] [G loss: 3.224897]\n",
      "epoch:5 step:5115 [D loss: 0.614047, acc: 67.97%] [G loss: 3.026893]\n",
      "epoch:5 step:5116 [D loss: 0.571317, acc: 75.00%] [G loss: 2.595202]\n",
      "epoch:5 step:5117 [D loss: 0.666847, acc: 60.16%] [G loss: 2.501658]\n",
      "epoch:5 step:5118 [D loss: 0.610161, acc: 66.41%] [G loss: 2.556828]\n",
      "epoch:5 step:5119 [D loss: 0.604069, acc: 71.09%] [G loss: 2.580689]\n",
      "epoch:5 step:5120 [D loss: 0.600105, acc: 63.28%] [G loss: 2.474977]\n",
      "epoch:5 step:5121 [D loss: 0.629976, acc: 68.75%] [G loss: 2.403252]\n",
      "epoch:5 step:5122 [D loss: 0.652234, acc: 59.38%] [G loss: 2.340910]\n",
      "epoch:5 step:5123 [D loss: 0.628540, acc: 64.06%] [G loss: 2.651175]\n",
      "epoch:5 step:5124 [D loss: 0.576298, acc: 69.53%] [G loss: 2.562042]\n",
      "epoch:5 step:5125 [D loss: 0.642566, acc: 67.19%] [G loss: 2.526984]\n",
      "epoch:5 step:5126 [D loss: 0.642080, acc: 66.41%] [G loss: 2.317740]\n",
      "epoch:5 step:5127 [D loss: 0.653185, acc: 63.28%] [G loss: 2.255223]\n",
      "epoch:5 step:5128 [D loss: 0.517986, acc: 75.78%] [G loss: 2.595168]\n",
      "epoch:5 step:5129 [D loss: 0.602335, acc: 67.97%] [G loss: 2.335192]\n",
      "epoch:5 step:5130 [D loss: 0.590506, acc: 66.41%] [G loss: 2.433193]\n",
      "epoch:5 step:5131 [D loss: 0.582860, acc: 72.66%] [G loss: 2.409374]\n",
      "epoch:5 step:5132 [D loss: 0.629131, acc: 66.41%] [G loss: 2.634918]\n",
      "epoch:5 step:5133 [D loss: 0.761830, acc: 60.16%] [G loss: 2.201297]\n",
      "epoch:5 step:5134 [D loss: 0.570239, acc: 74.22%] [G loss: 2.600813]\n",
      "epoch:5 step:5135 [D loss: 0.586613, acc: 69.53%] [G loss: 2.525630]\n",
      "epoch:5 step:5136 [D loss: 0.530540, acc: 70.31%] [G loss: 2.877731]\n",
      "epoch:5 step:5137 [D loss: 0.550228, acc: 66.41%] [G loss: 2.636515]\n",
      "epoch:5 step:5138 [D loss: 0.532044, acc: 73.44%] [G loss: 2.782449]\n",
      "epoch:5 step:5139 [D loss: 0.537658, acc: 67.97%] [G loss: 2.397237]\n",
      "epoch:5 step:5140 [D loss: 0.555276, acc: 71.09%] [G loss: 2.754590]\n",
      "epoch:5 step:5141 [D loss: 0.615513, acc: 67.19%] [G loss: 2.534289]\n",
      "epoch:5 step:5142 [D loss: 0.606316, acc: 64.84%] [G loss: 2.752927]\n",
      "epoch:5 step:5143 [D loss: 0.698666, acc: 60.16%] [G loss: 2.397576]\n",
      "epoch:5 step:5144 [D loss: 0.658768, acc: 60.16%] [G loss: 2.336656]\n",
      "epoch:5 step:5145 [D loss: 0.644606, acc: 62.50%] [G loss: 2.479991]\n",
      "epoch:5 step:5146 [D loss: 0.698164, acc: 59.38%] [G loss: 2.446596]\n",
      "epoch:5 step:5147 [D loss: 0.566978, acc: 67.97%] [G loss: 2.551929]\n",
      "epoch:5 step:5148 [D loss: 0.636972, acc: 62.50%] [G loss: 2.250973]\n",
      "epoch:5 step:5149 [D loss: 0.577869, acc: 69.53%] [G loss: 2.688629]\n",
      "epoch:5 step:5150 [D loss: 0.691180, acc: 57.81%] [G loss: 2.313697]\n",
      "epoch:5 step:5151 [D loss: 0.608833, acc: 67.97%] [G loss: 2.582966]\n",
      "epoch:5 step:5152 [D loss: 0.621284, acc: 67.97%] [G loss: 2.383910]\n",
      "epoch:5 step:5153 [D loss: 0.568420, acc: 65.62%] [G loss: 2.471383]\n",
      "epoch:5 step:5154 [D loss: 0.620469, acc: 64.06%] [G loss: 2.451769]\n",
      "epoch:5 step:5155 [D loss: 0.521007, acc: 74.22%] [G loss: 2.658334]\n",
      "epoch:5 step:5156 [D loss: 0.622669, acc: 66.41%] [G loss: 2.814445]\n",
      "epoch:5 step:5157 [D loss: 0.533536, acc: 72.66%] [G loss: 2.620119]\n",
      "epoch:5 step:5158 [D loss: 0.656343, acc: 62.50%] [G loss: 2.596818]\n",
      "epoch:5 step:5159 [D loss: 0.610191, acc: 71.88%] [G loss: 2.476182]\n",
      "epoch:5 step:5160 [D loss: 0.683176, acc: 60.94%] [G loss: 2.540144]\n",
      "epoch:5 step:5161 [D loss: 0.599522, acc: 67.19%] [G loss: 2.682632]\n",
      "epoch:5 step:5162 [D loss: 0.707091, acc: 57.03%] [G loss: 2.157470]\n",
      "epoch:5 step:5163 [D loss: 0.660122, acc: 60.16%] [G loss: 2.196264]\n",
      "epoch:5 step:5164 [D loss: 0.599601, acc: 69.53%] [G loss: 2.347776]\n",
      "epoch:5 step:5165 [D loss: 0.661152, acc: 60.16%] [G loss: 2.207032]\n",
      "epoch:5 step:5166 [D loss: 0.567398, acc: 72.66%] [G loss: 2.573328]\n",
      "epoch:5 step:5167 [D loss: 0.705393, acc: 60.16%] [G loss: 2.203448]\n",
      "epoch:5 step:5168 [D loss: 0.602066, acc: 68.75%] [G loss: 2.455436]\n",
      "epoch:5 step:5169 [D loss: 0.632199, acc: 67.97%] [G loss: 2.340966]\n",
      "epoch:5 step:5170 [D loss: 0.557123, acc: 69.53%] [G loss: 2.633096]\n",
      "epoch:5 step:5171 [D loss: 0.640825, acc: 64.06%] [G loss: 2.287694]\n",
      "epoch:5 step:5172 [D loss: 0.652298, acc: 60.16%] [G loss: 2.386517]\n",
      "epoch:5 step:5173 [D loss: 0.629432, acc: 64.84%] [G loss: 2.607758]\n",
      "epoch:5 step:5174 [D loss: 0.601777, acc: 67.19%] [G loss: 2.597322]\n",
      "epoch:5 step:5175 [D loss: 0.664303, acc: 67.19%] [G loss: 2.427479]\n",
      "epoch:5 step:5176 [D loss: 0.578584, acc: 64.84%] [G loss: 2.485183]\n",
      "epoch:5 step:5177 [D loss: 0.615585, acc: 63.28%] [G loss: 2.119841]\n",
      "epoch:5 step:5178 [D loss: 0.624061, acc: 63.28%] [G loss: 2.277624]\n",
      "epoch:5 step:5179 [D loss: 0.564032, acc: 75.00%] [G loss: 2.640193]\n",
      "epoch:5 step:5180 [D loss: 0.534425, acc: 75.00%] [G loss: 2.598276]\n",
      "epoch:5 step:5181 [D loss: 0.603731, acc: 66.41%] [G loss: 2.510216]\n",
      "epoch:5 step:5182 [D loss: 0.548926, acc: 75.78%] [G loss: 2.892699]\n",
      "epoch:5 step:5183 [D loss: 0.529272, acc: 72.66%] [G loss: 3.177285]\n",
      "epoch:5 step:5184 [D loss: 0.539947, acc: 70.31%] [G loss: 2.970430]\n",
      "epoch:5 step:5185 [D loss: 0.791304, acc: 54.69%] [G loss: 1.965496]\n",
      "epoch:5 step:5186 [D loss: 0.696971, acc: 58.59%] [G loss: 1.957123]\n",
      "epoch:5 step:5187 [D loss: 0.644031, acc: 64.06%] [G loss: 2.106721]\n",
      "epoch:5 step:5188 [D loss: 0.604954, acc: 67.97%] [G loss: 2.285010]\n",
      "epoch:5 step:5189 [D loss: 0.558792, acc: 70.31%] [G loss: 2.498939]\n",
      "epoch:5 step:5190 [D loss: 0.549354, acc: 75.00%] [G loss: 2.560778]\n",
      "epoch:5 step:5191 [D loss: 0.606014, acc: 64.84%] [G loss: 2.273313]\n",
      "epoch:5 step:5192 [D loss: 0.620370, acc: 64.84%] [G loss: 2.384744]\n",
      "epoch:5 step:5193 [D loss: 0.576701, acc: 68.75%] [G loss: 2.820444]\n",
      "epoch:5 step:5194 [D loss: 0.566826, acc: 73.44%] [G loss: 2.620702]\n",
      "epoch:5 step:5195 [D loss: 0.608689, acc: 63.28%] [G loss: 2.614286]\n",
      "epoch:5 step:5196 [D loss: 0.669253, acc: 59.38%] [G loss: 2.539763]\n",
      "epoch:5 step:5197 [D loss: 0.547048, acc: 71.09%] [G loss: 2.577772]\n",
      "epoch:5 step:5198 [D loss: 0.580573, acc: 75.78%] [G loss: 2.486199]\n",
      "epoch:5 step:5199 [D loss: 0.629965, acc: 64.84%] [G loss: 2.618389]\n",
      "epoch:5 step:5200 [D loss: 0.598119, acc: 71.88%] [G loss: 2.627249]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.645963\n",
      "FID: 46.749203\n",
      "0 = 13.17771878976821\n",
      "1 = 0.09206982877504717\n",
      "2 = 0.9265000224113464\n",
      "3 = 0.9265999794006348\n",
      "4 = 0.9264000058174133\n",
      "5 = 0.9264147281646729\n",
      "6 = 0.9265999794006348\n",
      "7 = 8.942563392400718\n",
      "8 = 0.13474306664119506\n",
      "9 = 0.8345999717712402\n",
      "10 = 0.8392000198364258\n",
      "11 = 0.8299999833106995\n",
      "12 = 0.8315497636795044\n",
      "13 = 0.8392000198364258\n",
      "14 = 5.645982265472412\n",
      "15 = 8.991511344909668\n",
      "16 = 0.22681057453155518\n",
      "17 = 5.645963191986084\n",
      "18 = 46.749202728271484\n",
      "epoch:5 step:5201 [D loss: 0.524804, acc: 73.44%] [G loss: 2.814498]\n",
      "epoch:5 step:5202 [D loss: 0.602075, acc: 67.19%] [G loss: 2.522429]\n",
      "epoch:5 step:5203 [D loss: 0.583130, acc: 69.53%] [G loss: 2.547051]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5204 [D loss: 0.551226, acc: 71.09%] [G loss: 2.783033]\n",
      "epoch:5 step:5205 [D loss: 0.536747, acc: 72.66%] [G loss: 2.556458]\n",
      "epoch:5 step:5206 [D loss: 0.545083, acc: 71.09%] [G loss: 2.783668]\n",
      "epoch:5 step:5207 [D loss: 0.593170, acc: 71.09%] [G loss: 2.606997]\n",
      "epoch:5 step:5208 [D loss: 0.511798, acc: 75.00%] [G loss: 3.084998]\n",
      "epoch:5 step:5209 [D loss: 0.551576, acc: 69.53%] [G loss: 2.746592]\n",
      "epoch:5 step:5210 [D loss: 0.643077, acc: 63.28%] [G loss: 2.808620]\n",
      "epoch:5 step:5211 [D loss: 0.534996, acc: 69.53%] [G loss: 2.836975]\n",
      "epoch:5 step:5212 [D loss: 0.601259, acc: 66.41%] [G loss: 2.608178]\n",
      "epoch:5 step:5213 [D loss: 0.674644, acc: 61.72%] [G loss: 2.381377]\n",
      "epoch:5 step:5214 [D loss: 0.686532, acc: 65.62%] [G loss: 2.080976]\n",
      "epoch:5 step:5215 [D loss: 0.578932, acc: 70.31%] [G loss: 2.383722]\n",
      "epoch:5 step:5216 [D loss: 0.633448, acc: 64.06%] [G loss: 2.356241]\n",
      "epoch:5 step:5217 [D loss: 0.546016, acc: 75.00%] [G loss: 2.646834]\n",
      "epoch:5 step:5218 [D loss: 0.631777, acc: 66.41%] [G loss: 2.531328]\n",
      "epoch:5 step:5219 [D loss: 0.548360, acc: 69.53%] [G loss: 2.803304]\n",
      "epoch:5 step:5220 [D loss: 0.640374, acc: 66.41%] [G loss: 2.283735]\n",
      "epoch:5 step:5221 [D loss: 0.626324, acc: 68.75%] [G loss: 2.599106]\n",
      "epoch:5 step:5222 [D loss: 0.673987, acc: 62.50%] [G loss: 2.356242]\n",
      "epoch:5 step:5223 [D loss: 0.667980, acc: 60.16%] [G loss: 2.019732]\n",
      "epoch:5 step:5224 [D loss: 0.598628, acc: 68.75%] [G loss: 2.432000]\n",
      "epoch:5 step:5225 [D loss: 0.614031, acc: 69.53%] [G loss: 2.321053]\n",
      "epoch:5 step:5226 [D loss: 0.545968, acc: 71.88%] [G loss: 2.664430]\n",
      "epoch:5 step:5227 [D loss: 0.647146, acc: 60.94%] [G loss: 2.344285]\n",
      "epoch:5 step:5228 [D loss: 0.626838, acc: 66.41%] [G loss: 2.389894]\n",
      "epoch:5 step:5229 [D loss: 0.563429, acc: 69.53%] [G loss: 2.461330]\n",
      "epoch:5 step:5230 [D loss: 0.593762, acc: 69.53%] [G loss: 2.496878]\n",
      "epoch:5 step:5231 [D loss: 0.599983, acc: 71.88%] [G loss: 2.598314]\n",
      "epoch:5 step:5232 [D loss: 0.552455, acc: 70.31%] [G loss: 2.512239]\n",
      "epoch:5 step:5233 [D loss: 0.518073, acc: 73.44%] [G loss: 2.626231]\n",
      "epoch:5 step:5234 [D loss: 0.574429, acc: 69.53%] [G loss: 2.553272]\n",
      "epoch:5 step:5235 [D loss: 0.566824, acc: 67.97%] [G loss: 2.586852]\n",
      "epoch:5 step:5236 [D loss: 0.664707, acc: 64.84%] [G loss: 2.583762]\n",
      "epoch:5 step:5237 [D loss: 0.529533, acc: 75.78%] [G loss: 2.690726]\n",
      "epoch:5 step:5238 [D loss: 0.682219, acc: 60.94%] [G loss: 2.413765]\n",
      "epoch:5 step:5239 [D loss: 0.504253, acc: 72.66%] [G loss: 2.980630]\n",
      "epoch:5 step:5240 [D loss: 0.673873, acc: 64.06%] [G loss: 2.431465]\n",
      "epoch:5 step:5241 [D loss: 0.564366, acc: 67.97%] [G loss: 2.807714]\n",
      "epoch:5 step:5242 [D loss: 0.596409, acc: 67.97%] [G loss: 2.686614]\n",
      "epoch:5 step:5243 [D loss: 0.607363, acc: 68.75%] [G loss: 2.463523]\n",
      "epoch:5 step:5244 [D loss: 0.660674, acc: 64.06%] [G loss: 2.272421]\n",
      "epoch:5 step:5245 [D loss: 0.602889, acc: 67.19%] [G loss: 2.099102]\n",
      "epoch:5 step:5246 [D loss: 0.617978, acc: 67.97%] [G loss: 2.364685]\n",
      "epoch:5 step:5247 [D loss: 0.629090, acc: 61.72%] [G loss: 2.562956]\n",
      "epoch:5 step:5248 [D loss: 0.718766, acc: 56.25%] [G loss: 2.293211]\n",
      "epoch:5 step:5249 [D loss: 0.561057, acc: 67.97%] [G loss: 2.673170]\n",
      "epoch:5 step:5250 [D loss: 0.677139, acc: 63.28%] [G loss: 2.318495]\n",
      "epoch:5 step:5251 [D loss: 0.684175, acc: 64.84%] [G loss: 2.195387]\n",
      "epoch:5 step:5252 [D loss: 0.607861, acc: 66.41%] [G loss: 2.590142]\n",
      "epoch:5 step:5253 [D loss: 0.570492, acc: 69.53%] [G loss: 2.550398]\n",
      "epoch:5 step:5254 [D loss: 0.620135, acc: 67.19%] [G loss: 2.405990]\n",
      "epoch:5 step:5255 [D loss: 0.595345, acc: 66.41%] [G loss: 2.265255]\n",
      "epoch:5 step:5256 [D loss: 0.578113, acc: 73.44%] [G loss: 2.507602]\n",
      "epoch:5 step:5257 [D loss: 0.561866, acc: 75.78%] [G loss: 2.200725]\n",
      "epoch:5 step:5258 [D loss: 0.639728, acc: 62.50%] [G loss: 2.267621]\n",
      "epoch:5 step:5259 [D loss: 0.609782, acc: 69.53%] [G loss: 2.549364]\n",
      "epoch:5 step:5260 [D loss: 0.571152, acc: 74.22%] [G loss: 2.373484]\n",
      "epoch:5 step:5261 [D loss: 0.609480, acc: 69.53%] [G loss: 2.213369]\n",
      "epoch:5 step:5262 [D loss: 0.603888, acc: 68.75%] [G loss: 2.580625]\n",
      "epoch:5 step:5263 [D loss: 0.676930, acc: 62.50%] [G loss: 2.334268]\n",
      "epoch:5 step:5264 [D loss: 0.627648, acc: 65.62%] [G loss: 2.149427]\n",
      "epoch:5 step:5265 [D loss: 0.671355, acc: 59.38%] [G loss: 2.194417]\n",
      "epoch:5 step:5266 [D loss: 0.554708, acc: 72.66%] [G loss: 2.427409]\n",
      "epoch:5 step:5267 [D loss: 0.568586, acc: 73.44%] [G loss: 2.421954]\n",
      "epoch:5 step:5268 [D loss: 0.571303, acc: 71.88%] [G loss: 2.415319]\n",
      "epoch:5 step:5269 [D loss: 0.667114, acc: 62.50%] [G loss: 2.643993]\n",
      "epoch:5 step:5270 [D loss: 0.597428, acc: 67.97%] [G loss: 2.277900]\n",
      "epoch:5 step:5271 [D loss: 0.604203, acc: 66.41%] [G loss: 2.369396]\n",
      "epoch:5 step:5272 [D loss: 0.625418, acc: 64.84%] [G loss: 2.396305]\n",
      "epoch:5 step:5273 [D loss: 0.594826, acc: 67.97%] [G loss: 2.710040]\n",
      "epoch:5 step:5274 [D loss: 0.621707, acc: 67.97%] [G loss: 2.806978]\n",
      "epoch:5 step:5275 [D loss: 0.678406, acc: 58.59%] [G loss: 2.480432]\n",
      "epoch:5 step:5276 [D loss: 0.618701, acc: 70.31%] [G loss: 2.177199]\n",
      "epoch:5 step:5277 [D loss: 0.590148, acc: 69.53%] [G loss: 2.427511]\n",
      "epoch:5 step:5278 [D loss: 0.710234, acc: 57.03%] [G loss: 2.133062]\n",
      "epoch:5 step:5279 [D loss: 0.635027, acc: 63.28%] [G loss: 2.443457]\n",
      "epoch:5 step:5280 [D loss: 0.569651, acc: 68.75%] [G loss: 2.314981]\n",
      "epoch:5 step:5281 [D loss: 0.703697, acc: 59.38%] [G loss: 2.246505]\n",
      "epoch:5 step:5282 [D loss: 0.670149, acc: 64.06%] [G loss: 2.369201]\n",
      "epoch:5 step:5283 [D loss: 0.528885, acc: 72.66%] [G loss: 2.535543]\n",
      "epoch:5 step:5284 [D loss: 0.609225, acc: 64.84%] [G loss: 2.715750]\n",
      "epoch:5 step:5285 [D loss: 0.647367, acc: 64.06%] [G loss: 2.611165]\n",
      "epoch:5 step:5286 [D loss: 0.598015, acc: 66.41%] [G loss: 2.794213]\n",
      "epoch:5 step:5287 [D loss: 0.652229, acc: 67.19%] [G loss: 2.596573]\n",
      "epoch:5 step:5288 [D loss: 0.559489, acc: 74.22%] [G loss: 2.777007]\n",
      "epoch:5 step:5289 [D loss: 0.508732, acc: 77.34%] [G loss: 2.441902]\n",
      "epoch:5 step:5290 [D loss: 0.574151, acc: 67.19%] [G loss: 2.559494]\n",
      "epoch:5 step:5291 [D loss: 0.557438, acc: 70.31%] [G loss: 2.396724]\n",
      "epoch:5 step:5292 [D loss: 0.644509, acc: 62.50%] [G loss: 2.491205]\n",
      "epoch:5 step:5293 [D loss: 0.561839, acc: 68.75%] [G loss: 2.443059]\n",
      "epoch:5 step:5294 [D loss: 0.512706, acc: 75.00%] [G loss: 2.591634]\n",
      "epoch:5 step:5295 [D loss: 0.558862, acc: 71.88%] [G loss: 2.446305]\n",
      "epoch:5 step:5296 [D loss: 0.635148, acc: 60.16%] [G loss: 2.363672]\n",
      "epoch:5 step:5297 [D loss: 0.523264, acc: 78.12%] [G loss: 2.547900]\n",
      "epoch:5 step:5298 [D loss: 0.567208, acc: 69.53%] [G loss: 2.594036]\n",
      "epoch:5 step:5299 [D loss: 0.607071, acc: 70.31%] [G loss: 2.403061]\n",
      "epoch:5 step:5300 [D loss: 0.710867, acc: 62.50%] [G loss: 2.128681]\n",
      "epoch:5 step:5301 [D loss: 0.547602, acc: 74.22%] [G loss: 2.538320]\n",
      "epoch:5 step:5302 [D loss: 0.605317, acc: 70.31%] [G loss: 2.128505]\n",
      "epoch:5 step:5303 [D loss: 0.598990, acc: 67.97%] [G loss: 2.153725]\n",
      "epoch:5 step:5304 [D loss: 0.632231, acc: 64.06%] [G loss: 2.400707]\n",
      "epoch:5 step:5305 [D loss: 0.582466, acc: 68.75%] [G loss: 2.451599]\n",
      "epoch:5 step:5306 [D loss: 0.566706, acc: 70.31%] [G loss: 2.359617]\n",
      "epoch:5 step:5307 [D loss: 0.636671, acc: 62.50%] [G loss: 2.117535]\n",
      "epoch:5 step:5308 [D loss: 0.557446, acc: 75.00%] [G loss: 2.500418]\n",
      "epoch:5 step:5309 [D loss: 0.590067, acc: 70.31%] [G loss: 2.624112]\n",
      "epoch:5 step:5310 [D loss: 0.671207, acc: 60.16%] [G loss: 2.298797]\n",
      "epoch:5 step:5311 [D loss: 0.631417, acc: 65.62%] [G loss: 2.271383]\n",
      "epoch:5 step:5312 [D loss: 0.594048, acc: 65.62%] [G loss: 2.480339]\n",
      "epoch:5 step:5313 [D loss: 0.607473, acc: 65.62%] [G loss: 2.338115]\n",
      "epoch:5 step:5314 [D loss: 0.582207, acc: 67.97%] [G loss: 2.327298]\n",
      "epoch:5 step:5315 [D loss: 0.629428, acc: 63.28%] [G loss: 2.706283]\n",
      "epoch:5 step:5316 [D loss: 0.591079, acc: 71.09%] [G loss: 2.542523]\n",
      "epoch:5 step:5317 [D loss: 0.503114, acc: 71.88%] [G loss: 2.543441]\n",
      "epoch:5 step:5318 [D loss: 0.617819, acc: 71.09%] [G loss: 2.311672]\n",
      "epoch:5 step:5319 [D loss: 0.545522, acc: 74.22%] [G loss: 2.787867]\n",
      "epoch:5 step:5320 [D loss: 0.602546, acc: 66.41%] [G loss: 2.692570]\n",
      "epoch:5 step:5321 [D loss: 0.595821, acc: 67.97%] [G loss: 2.335021]\n",
      "epoch:5 step:5322 [D loss: 0.621354, acc: 62.50%] [G loss: 2.450158]\n",
      "epoch:5 step:5323 [D loss: 0.559514, acc: 70.31%] [G loss: 2.497629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5324 [D loss: 0.534424, acc: 71.88%] [G loss: 2.520456]\n",
      "epoch:5 step:5325 [D loss: 0.570432, acc: 69.53%] [G loss: 2.675006]\n",
      "epoch:5 step:5326 [D loss: 0.549580, acc: 66.41%] [G loss: 3.207171]\n",
      "epoch:5 step:5327 [D loss: 0.480092, acc: 75.78%] [G loss: 2.826959]\n",
      "epoch:5 step:5328 [D loss: 0.603363, acc: 69.53%] [G loss: 2.674979]\n",
      "epoch:5 step:5329 [D loss: 0.565325, acc: 73.44%] [G loss: 2.791444]\n",
      "epoch:5 step:5330 [D loss: 0.559156, acc: 73.44%] [G loss: 2.909755]\n",
      "epoch:5 step:5331 [D loss: 0.517737, acc: 72.66%] [G loss: 2.873879]\n",
      "epoch:5 step:5332 [D loss: 0.555158, acc: 71.09%] [G loss: 3.298488]\n",
      "epoch:5 step:5333 [D loss: 0.504647, acc: 79.69%] [G loss: 3.400473]\n",
      "epoch:5 step:5334 [D loss: 0.536538, acc: 74.22%] [G loss: 2.799836]\n",
      "epoch:5 step:5335 [D loss: 0.574839, acc: 71.88%] [G loss: 2.975268]\n",
      "epoch:5 step:5336 [D loss: 0.574076, acc: 67.19%] [G loss: 2.773672]\n",
      "epoch:5 step:5337 [D loss: 0.645555, acc: 63.28%] [G loss: 2.473532]\n",
      "epoch:5 step:5338 [D loss: 0.642372, acc: 67.97%] [G loss: 2.685050]\n",
      "epoch:5 step:5339 [D loss: 0.568082, acc: 72.66%] [G loss: 2.723764]\n",
      "epoch:5 step:5340 [D loss: 0.617409, acc: 63.28%] [G loss: 2.406830]\n",
      "epoch:5 step:5341 [D loss: 0.578384, acc: 69.53%] [G loss: 2.703752]\n",
      "epoch:5 step:5342 [D loss: 0.632877, acc: 60.94%] [G loss: 2.432806]\n",
      "epoch:5 step:5343 [D loss: 0.627928, acc: 64.06%] [G loss: 2.375793]\n",
      "epoch:5 step:5344 [D loss: 0.603366, acc: 67.97%] [G loss: 2.340314]\n",
      "epoch:5 step:5345 [D loss: 0.576720, acc: 69.53%] [G loss: 2.636407]\n",
      "epoch:5 step:5346 [D loss: 0.552959, acc: 75.00%] [G loss: 2.553386]\n",
      "epoch:5 step:5347 [D loss: 0.631213, acc: 64.84%] [G loss: 2.736672]\n",
      "epoch:5 step:5348 [D loss: 0.542270, acc: 73.44%] [G loss: 2.750038]\n",
      "epoch:5 step:5349 [D loss: 0.558675, acc: 60.94%] [G loss: 2.477563]\n",
      "epoch:5 step:5350 [D loss: 0.601727, acc: 67.19%] [G loss: 2.760361]\n",
      "epoch:5 step:5351 [D loss: 0.573150, acc: 74.22%] [G loss: 2.954088]\n",
      "epoch:5 step:5352 [D loss: 0.609975, acc: 69.53%] [G loss: 2.348970]\n",
      "epoch:5 step:5353 [D loss: 0.598385, acc: 67.19%] [G loss: 2.511812]\n",
      "epoch:5 step:5354 [D loss: 0.579031, acc: 65.62%] [G loss: 2.334259]\n",
      "epoch:5 step:5355 [D loss: 0.707125, acc: 57.03%] [G loss: 2.309170]\n",
      "epoch:5 step:5356 [D loss: 0.633990, acc: 67.19%] [G loss: 2.274076]\n",
      "epoch:5 step:5357 [D loss: 0.599879, acc: 67.97%] [G loss: 2.541071]\n",
      "epoch:5 step:5358 [D loss: 0.622758, acc: 67.97%] [G loss: 2.396081]\n",
      "epoch:5 step:5359 [D loss: 0.605938, acc: 63.28%] [G loss: 2.539509]\n",
      "epoch:5 step:5360 [D loss: 0.539433, acc: 72.66%] [G loss: 2.546974]\n",
      "epoch:5 step:5361 [D loss: 0.598507, acc: 68.75%] [G loss: 2.178389]\n",
      "epoch:5 step:5362 [D loss: 0.512511, acc: 71.09%] [G loss: 2.581700]\n",
      "epoch:5 step:5363 [D loss: 0.573445, acc: 67.97%] [G loss: 2.681047]\n",
      "epoch:5 step:5364 [D loss: 0.619748, acc: 70.31%] [G loss: 2.604648]\n",
      "epoch:5 step:5365 [D loss: 0.523943, acc: 74.22%] [G loss: 2.440938]\n",
      "epoch:5 step:5366 [D loss: 0.582599, acc: 74.22%] [G loss: 2.876117]\n",
      "epoch:5 step:5367 [D loss: 0.566912, acc: 73.44%] [G loss: 2.407290]\n",
      "epoch:5 step:5368 [D loss: 0.602586, acc: 66.41%] [G loss: 2.590321]\n",
      "epoch:5 step:5369 [D loss: 0.672352, acc: 56.25%] [G loss: 2.422893]\n",
      "epoch:5 step:5370 [D loss: 0.556339, acc: 73.44%] [G loss: 2.258027]\n",
      "epoch:5 step:5371 [D loss: 0.608145, acc: 67.97%] [G loss: 2.350245]\n",
      "epoch:5 step:5372 [D loss: 0.636909, acc: 63.28%] [G loss: 2.363201]\n",
      "epoch:5 step:5373 [D loss: 0.627447, acc: 67.97%] [G loss: 2.269032]\n",
      "epoch:5 step:5374 [D loss: 0.652403, acc: 65.62%] [G loss: 2.364596]\n",
      "epoch:5 step:5375 [D loss: 0.625115, acc: 66.41%] [G loss: 2.649390]\n",
      "epoch:5 step:5376 [D loss: 0.548770, acc: 73.44%] [G loss: 2.572906]\n",
      "epoch:5 step:5377 [D loss: 0.581210, acc: 63.28%] [G loss: 2.551467]\n",
      "epoch:5 step:5378 [D loss: 0.567302, acc: 66.41%] [G loss: 2.699435]\n",
      "epoch:5 step:5379 [D loss: 0.616456, acc: 65.62%] [G loss: 2.881627]\n",
      "epoch:5 step:5380 [D loss: 0.598464, acc: 66.41%] [G loss: 2.589839]\n",
      "epoch:5 step:5381 [D loss: 0.612796, acc: 70.31%] [G loss: 2.598352]\n",
      "epoch:5 step:5382 [D loss: 0.589918, acc: 66.41%] [G loss: 2.691852]\n",
      "epoch:5 step:5383 [D loss: 0.642530, acc: 59.38%] [G loss: 2.505166]\n",
      "epoch:5 step:5384 [D loss: 0.585520, acc: 72.66%] [G loss: 2.847785]\n",
      "epoch:5 step:5385 [D loss: 0.634133, acc: 66.41%] [G loss: 2.454434]\n",
      "epoch:5 step:5386 [D loss: 0.565319, acc: 67.97%] [G loss: 2.472468]\n",
      "epoch:5 step:5387 [D loss: 0.635775, acc: 62.50%] [G loss: 2.367983]\n",
      "epoch:5 step:5388 [D loss: 0.639876, acc: 61.72%] [G loss: 2.356848]\n",
      "epoch:5 step:5389 [D loss: 0.577914, acc: 68.75%] [G loss: 2.470537]\n",
      "epoch:5 step:5390 [D loss: 0.566176, acc: 70.31%] [G loss: 2.466997]\n",
      "epoch:5 step:5391 [D loss: 0.524768, acc: 75.78%] [G loss: 2.772422]\n",
      "epoch:5 step:5392 [D loss: 0.582509, acc: 68.75%] [G loss: 3.168408]\n",
      "epoch:5 step:5393 [D loss: 0.560573, acc: 73.44%] [G loss: 2.735553]\n",
      "epoch:5 step:5394 [D loss: 0.505840, acc: 74.22%] [G loss: 2.898764]\n",
      "epoch:5 step:5395 [D loss: 0.665367, acc: 55.47%] [G loss: 2.269915]\n",
      "epoch:5 step:5396 [D loss: 0.693967, acc: 64.84%] [G loss: 2.367701]\n",
      "epoch:5 step:5397 [D loss: 0.572618, acc: 64.06%] [G loss: 2.622824]\n",
      "epoch:5 step:5398 [D loss: 0.637877, acc: 68.75%] [G loss: 2.356652]\n",
      "epoch:5 step:5399 [D loss: 0.590201, acc: 71.88%] [G loss: 2.400078]\n",
      "epoch:5 step:5400 [D loss: 0.640211, acc: 64.84%] [G loss: 2.345456]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.704382\n",
      "FID: 46.555351\n",
      "0 = 13.131127378177649\n",
      "1 = 0.08584798138510857\n",
      "2 = 0.9298999905586243\n",
      "3 = 0.9314000010490417\n",
      "4 = 0.9283999800682068\n",
      "5 = 0.9286141395568848\n",
      "6 = 0.9314000010490417\n",
      "7 = 8.938119187736504\n",
      "8 = 0.13562445715167137\n",
      "9 = 0.8331000208854675\n",
      "10 = 0.8312000036239624\n",
      "11 = 0.8349999785423279\n",
      "12 = 0.8343706130981445\n",
      "13 = 0.8312000036239624\n",
      "14 = 5.704399108886719\n",
      "15 = 9.019569396972656\n",
      "16 = 0.22272217273712158\n",
      "17 = 5.704381942749023\n",
      "18 = 46.55535125732422\n",
      "epoch:5 step:5401 [D loss: 0.683206, acc: 64.06%] [G loss: 2.248113]\n",
      "epoch:5 step:5402 [D loss: 0.635819, acc: 62.50%] [G loss: 2.317580]\n",
      "epoch:5 step:5403 [D loss: 0.584459, acc: 69.53%] [G loss: 2.587616]\n",
      "epoch:5 step:5404 [D loss: 0.581189, acc: 67.19%] [G loss: 2.933738]\n",
      "epoch:5 step:5405 [D loss: 0.592053, acc: 67.97%] [G loss: 2.682696]\n",
      "epoch:5 step:5406 [D loss: 0.636270, acc: 64.06%] [G loss: 2.559487]\n",
      "epoch:5 step:5407 [D loss: 0.594460, acc: 74.22%] [G loss: 2.423666]\n",
      "epoch:5 step:5408 [D loss: 0.687647, acc: 58.59%] [G loss: 2.274921]\n",
      "epoch:5 step:5409 [D loss: 0.594315, acc: 67.97%] [G loss: 2.590453]\n",
      "epoch:5 step:5410 [D loss: 0.582193, acc: 67.97%] [G loss: 2.792251]\n",
      "epoch:5 step:5411 [D loss: 0.603009, acc: 68.75%] [G loss: 2.499639]\n",
      "epoch:5 step:5412 [D loss: 0.646478, acc: 62.50%] [G loss: 2.564729]\n",
      "epoch:5 step:5413 [D loss: 0.649005, acc: 67.19%] [G loss: 2.769961]\n",
      "epoch:5 step:5414 [D loss: 0.571130, acc: 68.75%] [G loss: 2.428032]\n",
      "epoch:5 step:5415 [D loss: 0.600481, acc: 70.31%] [G loss: 2.555188]\n",
      "epoch:5 step:5416 [D loss: 0.595512, acc: 67.19%] [G loss: 2.470619]\n",
      "epoch:5 step:5417 [D loss: 0.548105, acc: 72.66%] [G loss: 2.766689]\n",
      "epoch:5 step:5418 [D loss: 0.591482, acc: 66.41%] [G loss: 2.744525]\n",
      "epoch:5 step:5419 [D loss: 0.579175, acc: 69.53%] [G loss: 2.644138]\n",
      "epoch:5 step:5420 [D loss: 0.579566, acc: 67.97%] [G loss: 2.380384]\n",
      "epoch:5 step:5421 [D loss: 0.625225, acc: 70.31%] [G loss: 2.402448]\n",
      "epoch:5 step:5422 [D loss: 0.580429, acc: 69.53%] [G loss: 2.383101]\n",
      "epoch:5 step:5423 [D loss: 0.707421, acc: 59.38%] [G loss: 2.275301]\n",
      "epoch:5 step:5424 [D loss: 0.691926, acc: 60.16%] [G loss: 2.404834]\n",
      "epoch:5 step:5425 [D loss: 0.664490, acc: 63.28%] [G loss: 2.396133]\n",
      "epoch:5 step:5426 [D loss: 0.616526, acc: 66.41%] [G loss: 2.337465]\n",
      "epoch:5 step:5427 [D loss: 0.676564, acc: 67.97%] [G loss: 2.182516]\n",
      "epoch:5 step:5428 [D loss: 0.569051, acc: 71.09%] [G loss: 2.408076]\n",
      "epoch:5 step:5429 [D loss: 0.627708, acc: 65.62%] [G loss: 2.221655]\n",
      "epoch:5 step:5430 [D loss: 0.605170, acc: 69.53%] [G loss: 2.297453]\n",
      "epoch:5 step:5431 [D loss: 0.582506, acc: 68.75%] [G loss: 2.529500]\n",
      "epoch:5 step:5432 [D loss: 0.591887, acc: 72.66%] [G loss: 2.810941]\n",
      "epoch:5 step:5433 [D loss: 0.558583, acc: 75.78%] [G loss: 2.261720]\n",
      "epoch:5 step:5434 [D loss: 0.605253, acc: 65.62%] [G loss: 2.190829]\n",
      "epoch:5 step:5435 [D loss: 0.617150, acc: 64.84%] [G loss: 2.591966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5436 [D loss: 0.589186, acc: 64.06%] [G loss: 2.394218]\n",
      "epoch:5 step:5437 [D loss: 0.654549, acc: 62.50%] [G loss: 2.221754]\n",
      "epoch:5 step:5438 [D loss: 0.645950, acc: 63.28%] [G loss: 2.382985]\n",
      "epoch:5 step:5439 [D loss: 0.553992, acc: 71.88%] [G loss: 2.425339]\n",
      "epoch:5 step:5440 [D loss: 0.602276, acc: 63.28%] [G loss: 2.539604]\n",
      "epoch:5 step:5441 [D loss: 0.666605, acc: 59.38%] [G loss: 2.316868]\n",
      "epoch:5 step:5442 [D loss: 0.570860, acc: 67.97%] [G loss: 2.455051]\n",
      "epoch:5 step:5443 [D loss: 0.581217, acc: 69.53%] [G loss: 2.378389]\n",
      "epoch:5 step:5444 [D loss: 0.676642, acc: 65.62%] [G loss: 2.390974]\n",
      "epoch:5 step:5445 [D loss: 0.598643, acc: 66.41%] [G loss: 2.264828]\n",
      "epoch:5 step:5446 [D loss: 0.585832, acc: 69.53%] [G loss: 2.596905]\n",
      "epoch:5 step:5447 [D loss: 0.641567, acc: 64.84%] [G loss: 2.324369]\n",
      "epoch:5 step:5448 [D loss: 0.612349, acc: 66.41%] [G loss: 2.311093]\n",
      "epoch:5 step:5449 [D loss: 0.651422, acc: 64.06%] [G loss: 2.396935]\n",
      "epoch:5 step:5450 [D loss: 0.706843, acc: 60.16%] [G loss: 2.265653]\n",
      "epoch:5 step:5451 [D loss: 0.636790, acc: 63.28%] [G loss: 2.103985]\n",
      "epoch:5 step:5452 [D loss: 0.726034, acc: 57.81%] [G loss: 2.374076]\n",
      "epoch:5 step:5453 [D loss: 0.658882, acc: 63.28%] [G loss: 2.235787]\n",
      "epoch:5 step:5454 [D loss: 0.532241, acc: 74.22%] [G loss: 2.402520]\n",
      "epoch:5 step:5455 [D loss: 0.681822, acc: 65.62%] [G loss: 2.249233]\n",
      "epoch:5 step:5456 [D loss: 0.597191, acc: 67.97%] [G loss: 2.230383]\n",
      "epoch:5 step:5457 [D loss: 0.655187, acc: 67.19%] [G loss: 2.273595]\n",
      "epoch:5 step:5458 [D loss: 0.643798, acc: 62.50%] [G loss: 2.245881]\n",
      "epoch:5 step:5459 [D loss: 0.606170, acc: 66.41%] [G loss: 2.464114]\n",
      "epoch:5 step:5460 [D loss: 0.540093, acc: 75.78%] [G loss: 2.668135]\n",
      "epoch:5 step:5461 [D loss: 0.582234, acc: 68.75%] [G loss: 2.509715]\n",
      "epoch:5 step:5462 [D loss: 0.588611, acc: 69.53%] [G loss: 2.451495]\n",
      "epoch:5 step:5463 [D loss: 0.648980, acc: 62.50%] [G loss: 2.440346]\n",
      "epoch:5 step:5464 [D loss: 0.580297, acc: 68.75%] [G loss: 2.622990]\n",
      "epoch:5 step:5465 [D loss: 0.631535, acc: 67.19%] [G loss: 2.771574]\n",
      "epoch:5 step:5466 [D loss: 0.511920, acc: 75.78%] [G loss: 2.573666]\n",
      "epoch:5 step:5467 [D loss: 0.597939, acc: 67.97%] [G loss: 2.716442]\n",
      "epoch:5 step:5468 [D loss: 0.586206, acc: 67.19%] [G loss: 2.908901]\n",
      "epoch:5 step:5469 [D loss: 0.693632, acc: 57.03%] [G loss: 2.280396]\n",
      "epoch:5 step:5470 [D loss: 0.640379, acc: 71.09%] [G loss: 2.500519]\n",
      "epoch:5 step:5471 [D loss: 0.519362, acc: 75.78%] [G loss: 2.714879]\n",
      "epoch:5 step:5472 [D loss: 0.689343, acc: 62.50%] [G loss: 2.255510]\n",
      "epoch:5 step:5473 [D loss: 0.650152, acc: 61.72%] [G loss: 2.091130]\n",
      "epoch:5 step:5474 [D loss: 0.629574, acc: 66.41%] [G loss: 2.143390]\n",
      "epoch:5 step:5475 [D loss: 0.518790, acc: 80.47%] [G loss: 2.483151]\n",
      "epoch:5 step:5476 [D loss: 0.557535, acc: 70.31%] [G loss: 2.693754]\n",
      "epoch:5 step:5477 [D loss: 0.520751, acc: 75.78%] [G loss: 2.942636]\n",
      "epoch:5 step:5478 [D loss: 0.594392, acc: 67.19%] [G loss: 2.489297]\n",
      "epoch:5 step:5479 [D loss: 0.632266, acc: 66.41%] [G loss: 2.336267]\n",
      "epoch:5 step:5480 [D loss: 0.581091, acc: 68.75%] [G loss: 2.592912]\n",
      "epoch:5 step:5481 [D loss: 0.573716, acc: 71.09%] [G loss: 2.509540]\n",
      "epoch:5 step:5482 [D loss: 0.594395, acc: 74.22%] [G loss: 2.369139]\n",
      "epoch:5 step:5483 [D loss: 0.637016, acc: 69.53%] [G loss: 2.447719]\n",
      "epoch:5 step:5484 [D loss: 0.623348, acc: 62.50%] [G loss: 2.299059]\n",
      "epoch:5 step:5485 [D loss: 0.616371, acc: 69.53%] [G loss: 2.239718]\n",
      "epoch:5 step:5486 [D loss: 0.636875, acc: 65.62%] [G loss: 2.371677]\n",
      "epoch:5 step:5487 [D loss: 0.603873, acc: 64.06%] [G loss: 2.386484]\n",
      "epoch:5 step:5488 [D loss: 0.499376, acc: 79.69%] [G loss: 2.397442]\n",
      "epoch:5 step:5489 [D loss: 0.544812, acc: 76.56%] [G loss: 2.535654]\n",
      "epoch:5 step:5490 [D loss: 0.582659, acc: 67.19%] [G loss: 2.938949]\n",
      "epoch:5 step:5491 [D loss: 0.529191, acc: 76.56%] [G loss: 3.028975]\n",
      "epoch:5 step:5492 [D loss: 0.603853, acc: 65.62%] [G loss: 2.646462]\n",
      "epoch:5 step:5493 [D loss: 0.588813, acc: 62.50%] [G loss: 2.711116]\n",
      "epoch:5 step:5494 [D loss: 0.524361, acc: 71.88%] [G loss: 2.948225]\n",
      "epoch:5 step:5495 [D loss: 0.609461, acc: 60.94%] [G loss: 2.637894]\n",
      "epoch:5 step:5496 [D loss: 0.569848, acc: 71.09%] [G loss: 2.521060]\n",
      "epoch:5 step:5497 [D loss: 0.638557, acc: 67.97%] [G loss: 2.182217]\n",
      "epoch:5 step:5498 [D loss: 0.605011, acc: 70.31%] [G loss: 2.590183]\n",
      "epoch:5 step:5499 [D loss: 0.531563, acc: 78.12%] [G loss: 2.602748]\n",
      "epoch:5 step:5500 [D loss: 0.483083, acc: 82.03%] [G loss: 3.054494]\n",
      "epoch:5 step:5501 [D loss: 0.597919, acc: 69.53%] [G loss: 2.853655]\n",
      "epoch:5 step:5502 [D loss: 0.600690, acc: 71.88%] [G loss: 2.528032]\n",
      "epoch:5 step:5503 [D loss: 0.666864, acc: 64.84%] [G loss: 2.395077]\n",
      "epoch:5 step:5504 [D loss: 0.603375, acc: 68.75%] [G loss: 2.448143]\n",
      "epoch:5 step:5505 [D loss: 0.680304, acc: 60.94%] [G loss: 2.284987]\n",
      "epoch:5 step:5506 [D loss: 0.693501, acc: 61.72%] [G loss: 2.467280]\n",
      "epoch:5 step:5507 [D loss: 0.558734, acc: 70.31%] [G loss: 2.569307]\n",
      "epoch:5 step:5508 [D loss: 0.532327, acc: 75.00%] [G loss: 2.328211]\n",
      "epoch:5 step:5509 [D loss: 0.682064, acc: 62.50%] [G loss: 2.230274]\n",
      "epoch:5 step:5510 [D loss: 0.565509, acc: 71.88%] [G loss: 2.544350]\n",
      "epoch:5 step:5511 [D loss: 0.653283, acc: 60.94%] [G loss: 2.508346]\n",
      "epoch:5 step:5512 [D loss: 0.605061, acc: 67.97%] [G loss: 2.121624]\n",
      "epoch:5 step:5513 [D loss: 0.681996, acc: 60.16%] [G loss: 2.247073]\n",
      "epoch:5 step:5514 [D loss: 0.585537, acc: 67.19%] [G loss: 2.242254]\n",
      "epoch:5 step:5515 [D loss: 0.604875, acc: 66.41%] [G loss: 2.584697]\n",
      "epoch:5 step:5516 [D loss: 0.520197, acc: 76.56%] [G loss: 2.582774]\n",
      "epoch:5 step:5517 [D loss: 0.647245, acc: 64.06%] [G loss: 2.396137]\n",
      "epoch:5 step:5518 [D loss: 0.638721, acc: 67.19%] [G loss: 2.467211]\n",
      "epoch:5 step:5519 [D loss: 0.531794, acc: 74.22%] [G loss: 2.553814]\n",
      "epoch:5 step:5520 [D loss: 0.582623, acc: 70.31%] [G loss: 2.386409]\n",
      "epoch:5 step:5521 [D loss: 0.660963, acc: 62.50%] [G loss: 2.344167]\n",
      "epoch:5 step:5522 [D loss: 0.635674, acc: 66.41%] [G loss: 2.430695]\n",
      "epoch:5 step:5523 [D loss: 0.582585, acc: 67.19%] [G loss: 2.440793]\n",
      "epoch:5 step:5524 [D loss: 0.651919, acc: 64.06%] [G loss: 2.588351]\n",
      "epoch:5 step:5525 [D loss: 0.592995, acc: 73.44%] [G loss: 2.509294]\n",
      "epoch:5 step:5526 [D loss: 0.545672, acc: 75.00%] [G loss: 2.622375]\n",
      "epoch:5 step:5527 [D loss: 0.559234, acc: 75.00%] [G loss: 2.545877]\n",
      "epoch:5 step:5528 [D loss: 0.615428, acc: 66.41%] [G loss: 2.585221]\n",
      "epoch:5 step:5529 [D loss: 0.633348, acc: 67.97%] [G loss: 2.613271]\n",
      "epoch:5 step:5530 [D loss: 0.597381, acc: 67.19%] [G loss: 2.487660]\n",
      "epoch:5 step:5531 [D loss: 0.560088, acc: 67.97%] [G loss: 2.067904]\n",
      "epoch:5 step:5532 [D loss: 0.603397, acc: 69.53%] [G loss: 2.406507]\n",
      "epoch:5 step:5533 [D loss: 0.623896, acc: 67.19%] [G loss: 2.462155]\n",
      "epoch:5 step:5534 [D loss: 0.609186, acc: 65.62%] [G loss: 2.349488]\n",
      "epoch:5 step:5535 [D loss: 0.649054, acc: 60.94%] [G loss: 2.779934]\n",
      "epoch:5 step:5536 [D loss: 0.692886, acc: 57.03%] [G loss: 2.173553]\n",
      "epoch:5 step:5537 [D loss: 0.636177, acc: 67.97%] [G loss: 2.432923]\n",
      "epoch:5 step:5538 [D loss: 0.596366, acc: 68.75%] [G loss: 2.582461]\n",
      "epoch:5 step:5539 [D loss: 0.566153, acc: 69.53%] [G loss: 2.453056]\n",
      "epoch:5 step:5540 [D loss: 0.631832, acc: 69.53%] [G loss: 2.318823]\n",
      "epoch:5 step:5541 [D loss: 0.675241, acc: 66.41%] [G loss: 2.475862]\n",
      "epoch:5 step:5542 [D loss: 0.609639, acc: 70.31%] [G loss: 2.233156]\n",
      "epoch:5 step:5543 [D loss: 0.640815, acc: 63.28%] [G loss: 2.197526]\n",
      "epoch:5 step:5544 [D loss: 0.601965, acc: 62.50%] [G loss: 2.409813]\n",
      "epoch:5 step:5545 [D loss: 0.592824, acc: 72.66%] [G loss: 2.604774]\n",
      "epoch:5 step:5546 [D loss: 0.682343, acc: 56.25%] [G loss: 2.300767]\n",
      "epoch:5 step:5547 [D loss: 0.583623, acc: 66.41%] [G loss: 2.230114]\n",
      "epoch:5 step:5548 [D loss: 0.623962, acc: 63.28%] [G loss: 2.205157]\n",
      "epoch:5 step:5549 [D loss: 0.584994, acc: 67.97%] [G loss: 2.585428]\n",
      "epoch:5 step:5550 [D loss: 0.581886, acc: 68.75%] [G loss: 2.377514]\n",
      "epoch:5 step:5551 [D loss: 0.642672, acc: 62.50%] [G loss: 2.228367]\n",
      "epoch:5 step:5552 [D loss: 0.647486, acc: 64.06%] [G loss: 2.202209]\n",
      "epoch:5 step:5553 [D loss: 0.613095, acc: 67.97%] [G loss: 2.363220]\n",
      "epoch:5 step:5554 [D loss: 0.617703, acc: 64.84%] [G loss: 2.464210]\n",
      "epoch:5 step:5555 [D loss: 0.645293, acc: 63.28%] [G loss: 2.341754]\n",
      "epoch:5 step:5556 [D loss: 0.569506, acc: 75.78%] [G loss: 2.383534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5557 [D loss: 0.540552, acc: 67.97%] [G loss: 2.310622]\n",
      "epoch:5 step:5558 [D loss: 0.597605, acc: 67.97%] [G loss: 2.530799]\n",
      "epoch:5 step:5559 [D loss: 0.619956, acc: 67.97%] [G loss: 2.318197]\n",
      "epoch:5 step:5560 [D loss: 0.565717, acc: 69.53%] [G loss: 2.501066]\n",
      "epoch:5 step:5561 [D loss: 0.614904, acc: 66.41%] [G loss: 2.369495]\n",
      "epoch:5 step:5562 [D loss: 0.615629, acc: 66.41%] [G loss: 2.488461]\n",
      "epoch:5 step:5563 [D loss: 0.612688, acc: 60.16%] [G loss: 1.977917]\n",
      "epoch:5 step:5564 [D loss: 0.617289, acc: 68.75%] [G loss: 2.375553]\n",
      "epoch:5 step:5565 [D loss: 0.624119, acc: 64.06%] [G loss: 2.242028]\n",
      "epoch:5 step:5566 [D loss: 0.616073, acc: 64.06%] [G loss: 2.284073]\n",
      "epoch:5 step:5567 [D loss: 0.680332, acc: 60.16%] [G loss: 2.171832]\n",
      "epoch:5 step:5568 [D loss: 0.668798, acc: 64.06%] [G loss: 2.424179]\n",
      "epoch:5 step:5569 [D loss: 0.572101, acc: 73.44%] [G loss: 2.416739]\n",
      "epoch:5 step:5570 [D loss: 0.589520, acc: 67.19%] [G loss: 2.554328]\n",
      "epoch:5 step:5571 [D loss: 0.610877, acc: 67.19%] [G loss: 2.667696]\n",
      "epoch:5 step:5572 [D loss: 0.602907, acc: 64.84%] [G loss: 2.321745]\n",
      "epoch:5 step:5573 [D loss: 0.638991, acc: 68.75%] [G loss: 2.396095]\n",
      "epoch:5 step:5574 [D loss: 0.612021, acc: 66.41%] [G loss: 2.486455]\n",
      "epoch:5 step:5575 [D loss: 0.585615, acc: 68.75%] [G loss: 2.610705]\n",
      "epoch:5 step:5576 [D loss: 0.603755, acc: 68.75%] [G loss: 2.444302]\n",
      "epoch:5 step:5577 [D loss: 0.697831, acc: 60.16%] [G loss: 1.982019]\n",
      "epoch:5 step:5578 [D loss: 0.640706, acc: 67.97%] [G loss: 2.283087]\n",
      "epoch:5 step:5579 [D loss: 0.592609, acc: 63.28%] [G loss: 2.333674]\n",
      "epoch:5 step:5580 [D loss: 0.609392, acc: 65.62%] [G loss: 2.303749]\n",
      "epoch:5 step:5581 [D loss: 0.624970, acc: 60.16%] [G loss: 2.243382]\n",
      "epoch:5 step:5582 [D loss: 0.600340, acc: 72.66%] [G loss: 2.423279]\n",
      "epoch:5 step:5583 [D loss: 0.594033, acc: 71.88%] [G loss: 2.464092]\n",
      "epoch:5 step:5584 [D loss: 0.581637, acc: 68.75%] [G loss: 2.488773]\n",
      "epoch:5 step:5585 [D loss: 0.633139, acc: 64.84%] [G loss: 2.503150]\n",
      "epoch:5 step:5586 [D loss: 0.612633, acc: 62.50%] [G loss: 2.603133]\n",
      "epoch:5 step:5587 [D loss: 0.614117, acc: 64.84%] [G loss: 2.207598]\n",
      "epoch:5 step:5588 [D loss: 0.569659, acc: 74.22%] [G loss: 2.491452]\n",
      "epoch:5 step:5589 [D loss: 0.638598, acc: 65.62%] [G loss: 2.357571]\n",
      "epoch:5 step:5590 [D loss: 0.706766, acc: 59.38%] [G loss: 2.526601]\n",
      "epoch:5 step:5591 [D loss: 0.551503, acc: 71.09%] [G loss: 2.405998]\n",
      "epoch:5 step:5592 [D loss: 0.674269, acc: 64.84%] [G loss: 2.429060]\n",
      "epoch:5 step:5593 [D loss: 0.600712, acc: 67.97%] [G loss: 2.582572]\n",
      "epoch:5 step:5594 [D loss: 0.589398, acc: 70.31%] [G loss: 2.543739]\n",
      "epoch:5 step:5595 [D loss: 0.614725, acc: 67.97%] [G loss: 2.418080]\n",
      "epoch:5 step:5596 [D loss: 0.531491, acc: 75.78%] [G loss: 2.611971]\n",
      "epoch:5 step:5597 [D loss: 0.512303, acc: 72.66%] [G loss: 2.973545]\n",
      "epoch:5 step:5598 [D loss: 0.700038, acc: 60.16%] [G loss: 2.285638]\n",
      "epoch:5 step:5599 [D loss: 0.592255, acc: 62.50%] [G loss: 2.305357]\n",
      "epoch:5 step:5600 [D loss: 0.582589, acc: 73.44%] [G loss: 2.475629]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.842497\n",
      "FID: 44.016808\n",
      "0 = 13.194990025138857\n",
      "1 = 0.09232841971184613\n",
      "2 = 0.9241999983787537\n",
      "3 = 0.925599992275238\n",
      "4 = 0.9228000044822693\n",
      "5 = 0.9230155348777771\n",
      "6 = 0.925599992275238\n",
      "7 = 8.734991673111905\n",
      "8 = 0.1342635823914273\n",
      "9 = 0.8263000249862671\n",
      "10 = 0.8342000246047974\n",
      "11 = 0.8184000253677368\n",
      "12 = 0.8212246298789978\n",
      "13 = 0.8342000246047974\n",
      "14 = 5.842519283294678\n",
      "15 = 8.989583969116211\n",
      "16 = 0.2242404669523239\n",
      "17 = 5.8424973487854\n",
      "18 = 44.016807556152344\n",
      "epoch:5 step:5601 [D loss: 0.591146, acc: 69.53%] [G loss: 2.221664]\n",
      "epoch:5 step:5602 [D loss: 0.583208, acc: 68.75%] [G loss: 2.330978]\n",
      "epoch:5 step:5603 [D loss: 0.539924, acc: 77.34%] [G loss: 2.712238]\n",
      "epoch:5 step:5604 [D loss: 0.534928, acc: 73.44%] [G loss: 2.665492]\n",
      "epoch:5 step:5605 [D loss: 0.638340, acc: 67.19%] [G loss: 2.760381]\n",
      "epoch:5 step:5606 [D loss: 0.585820, acc: 67.97%] [G loss: 2.717727]\n",
      "epoch:5 step:5607 [D loss: 0.687555, acc: 59.38%] [G loss: 2.363886]\n",
      "epoch:5 step:5608 [D loss: 0.539752, acc: 72.66%] [G loss: 2.899192]\n",
      "epoch:5 step:5609 [D loss: 0.559637, acc: 70.31%] [G loss: 2.663464]\n",
      "epoch:5 step:5610 [D loss: 0.623282, acc: 67.19%] [G loss: 2.847595]\n",
      "epoch:5 step:5611 [D loss: 0.566111, acc: 75.78%] [G loss: 2.987447]\n",
      "epoch:5 step:5612 [D loss: 0.564183, acc: 71.09%] [G loss: 2.856059]\n",
      "epoch:5 step:5613 [D loss: 0.855010, acc: 54.69%] [G loss: 2.400560]\n",
      "epoch:5 step:5614 [D loss: 0.583248, acc: 69.53%] [G loss: 2.782201]\n",
      "epoch:5 step:5615 [D loss: 0.578570, acc: 67.19%] [G loss: 2.567667]\n",
      "epoch:5 step:5616 [D loss: 0.535625, acc: 72.66%] [G loss: 2.490927]\n",
      "epoch:5 step:5617 [D loss: 0.605665, acc: 74.22%] [G loss: 2.392693]\n",
      "epoch:5 step:5618 [D loss: 0.575401, acc: 73.44%] [G loss: 2.368110]\n",
      "epoch:5 step:5619 [D loss: 0.609424, acc: 67.97%] [G loss: 2.572353]\n",
      "epoch:5 step:5620 [D loss: 0.621074, acc: 68.75%] [G loss: 2.383126]\n",
      "epoch:5 step:5621 [D loss: 0.499158, acc: 76.56%] [G loss: 2.784051]\n",
      "epoch:5 step:5622 [D loss: 0.478084, acc: 80.47%] [G loss: 3.522874]\n",
      "epoch:6 step:5623 [D loss: 0.680611, acc: 64.06%] [G loss: 2.626675]\n",
      "epoch:6 step:5624 [D loss: 0.660511, acc: 67.19%] [G loss: 2.279451]\n",
      "epoch:6 step:5625 [D loss: 0.638883, acc: 62.50%] [G loss: 2.397410]\n",
      "epoch:6 step:5626 [D loss: 0.556855, acc: 70.31%] [G loss: 2.626488]\n",
      "epoch:6 step:5627 [D loss: 0.653079, acc: 59.38%] [G loss: 2.133603]\n",
      "epoch:6 step:5628 [D loss: 0.576135, acc: 66.41%] [G loss: 2.466968]\n",
      "epoch:6 step:5629 [D loss: 0.581771, acc: 71.09%] [G loss: 2.516137]\n",
      "epoch:6 step:5630 [D loss: 0.605683, acc: 67.19%] [G loss: 2.704755]\n",
      "epoch:6 step:5631 [D loss: 0.629769, acc: 64.06%] [G loss: 2.623856]\n",
      "epoch:6 step:5632 [D loss: 0.583405, acc: 72.66%] [G loss: 2.590839]\n",
      "epoch:6 step:5633 [D loss: 0.595238, acc: 64.84%] [G loss: 2.550650]\n",
      "epoch:6 step:5634 [D loss: 0.570078, acc: 65.62%] [G loss: 2.897151]\n",
      "epoch:6 step:5635 [D loss: 0.610629, acc: 64.06%] [G loss: 2.444995]\n",
      "epoch:6 step:5636 [D loss: 0.573143, acc: 68.75%] [G loss: 2.413640]\n",
      "epoch:6 step:5637 [D loss: 0.562738, acc: 78.12%] [G loss: 2.823690]\n",
      "epoch:6 step:5638 [D loss: 0.533056, acc: 68.75%] [G loss: 2.699877]\n",
      "epoch:6 step:5639 [D loss: 0.755032, acc: 57.03%] [G loss: 2.389818]\n",
      "epoch:6 step:5640 [D loss: 0.618633, acc: 63.28%] [G loss: 2.296225]\n",
      "epoch:6 step:5641 [D loss: 0.639841, acc: 60.94%] [G loss: 2.263505]\n",
      "epoch:6 step:5642 [D loss: 0.700559, acc: 58.59%] [G loss: 2.027054]\n",
      "epoch:6 step:5643 [D loss: 0.679734, acc: 57.03%] [G loss: 2.217210]\n",
      "epoch:6 step:5644 [D loss: 0.652507, acc: 60.94%] [G loss: 2.282597]\n",
      "epoch:6 step:5645 [D loss: 0.593685, acc: 67.97%] [G loss: 2.504666]\n",
      "epoch:6 step:5646 [D loss: 0.576139, acc: 71.09%] [G loss: 2.559714]\n",
      "epoch:6 step:5647 [D loss: 0.588419, acc: 70.31%] [G loss: 2.690447]\n",
      "epoch:6 step:5648 [D loss: 0.597514, acc: 73.44%] [G loss: 2.300532]\n",
      "epoch:6 step:5649 [D loss: 0.556632, acc: 71.88%] [G loss: 2.445254]\n",
      "epoch:6 step:5650 [D loss: 0.681369, acc: 58.59%] [G loss: 2.343408]\n",
      "epoch:6 step:5651 [D loss: 0.578750, acc: 70.31%] [G loss: 2.224948]\n",
      "epoch:6 step:5652 [D loss: 0.593661, acc: 69.53%] [G loss: 2.161433]\n",
      "epoch:6 step:5653 [D loss: 0.669991, acc: 59.38%] [G loss: 2.278514]\n",
      "epoch:6 step:5654 [D loss: 0.559924, acc: 72.66%] [G loss: 2.215102]\n",
      "epoch:6 step:5655 [D loss: 0.583197, acc: 67.19%] [G loss: 2.354452]\n",
      "epoch:6 step:5656 [D loss: 0.572926, acc: 74.22%] [G loss: 2.422186]\n",
      "epoch:6 step:5657 [D loss: 0.531039, acc: 75.00%] [G loss: 2.164538]\n",
      "epoch:6 step:5658 [D loss: 0.575876, acc: 66.41%] [G loss: 2.530104]\n",
      "epoch:6 step:5659 [D loss: 0.589545, acc: 71.88%] [G loss: 2.760329]\n",
      "epoch:6 step:5660 [D loss: 0.629367, acc: 67.97%] [G loss: 2.728468]\n",
      "epoch:6 step:5661 [D loss: 0.579613, acc: 68.75%] [G loss: 2.535687]\n",
      "epoch:6 step:5662 [D loss: 0.495827, acc: 74.22%] [G loss: 3.036135]\n",
      "epoch:6 step:5663 [D loss: 0.603301, acc: 71.09%] [G loss: 2.313570]\n",
      "epoch:6 step:5664 [D loss: 0.678343, acc: 61.72%] [G loss: 2.651562]\n",
      "epoch:6 step:5665 [D loss: 0.594130, acc: 65.62%] [G loss: 2.617149]\n",
      "epoch:6 step:5666 [D loss: 0.655890, acc: 64.06%] [G loss: 2.419350]\n",
      "epoch:6 step:5667 [D loss: 0.540426, acc: 71.09%] [G loss: 2.470099]\n",
      "epoch:6 step:5668 [D loss: 0.614827, acc: 66.41%] [G loss: 2.148627]\n",
      "epoch:6 step:5669 [D loss: 0.588810, acc: 74.22%] [G loss: 2.343598]\n",
      "epoch:6 step:5670 [D loss: 0.657947, acc: 64.06%] [G loss: 2.367560]\n",
      "epoch:6 step:5671 [D loss: 0.668543, acc: 57.03%] [G loss: 2.398373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5672 [D loss: 0.565845, acc: 69.53%] [G loss: 2.515019]\n",
      "epoch:6 step:5673 [D loss: 0.639328, acc: 65.62%] [G loss: 2.370265]\n",
      "epoch:6 step:5674 [D loss: 0.591088, acc: 69.53%] [G loss: 2.614113]\n",
      "epoch:6 step:5675 [D loss: 0.642369, acc: 68.75%] [G loss: 2.664355]\n",
      "epoch:6 step:5676 [D loss: 0.613224, acc: 67.19%] [G loss: 2.606568]\n",
      "epoch:6 step:5677 [D loss: 0.571561, acc: 68.75%] [G loss: 2.449757]\n",
      "epoch:6 step:5678 [D loss: 0.584519, acc: 75.00%] [G loss: 2.501110]\n",
      "epoch:6 step:5679 [D loss: 0.648900, acc: 64.06%] [G loss: 2.554836]\n",
      "epoch:6 step:5680 [D loss: 0.624037, acc: 67.19%] [G loss: 2.230607]\n",
      "epoch:6 step:5681 [D loss: 0.600856, acc: 67.97%] [G loss: 2.432199]\n",
      "epoch:6 step:5682 [D loss: 0.593705, acc: 69.53%] [G loss: 2.422411]\n",
      "epoch:6 step:5683 [D loss: 0.608817, acc: 67.97%] [G loss: 2.668993]\n",
      "epoch:6 step:5684 [D loss: 0.548389, acc: 68.75%] [G loss: 2.743218]\n",
      "epoch:6 step:5685 [D loss: 0.613434, acc: 64.84%] [G loss: 2.383794]\n",
      "epoch:6 step:5686 [D loss: 0.587013, acc: 66.41%] [G loss: 2.418797]\n",
      "epoch:6 step:5687 [D loss: 0.680832, acc: 64.06%] [G loss: 2.432995]\n",
      "epoch:6 step:5688 [D loss: 0.598132, acc: 64.06%] [G loss: 2.329660]\n",
      "epoch:6 step:5689 [D loss: 0.481560, acc: 78.12%] [G loss: 2.720479]\n",
      "epoch:6 step:5690 [D loss: 0.594692, acc: 67.19%] [G loss: 2.538037]\n",
      "epoch:6 step:5691 [D loss: 0.570764, acc: 71.09%] [G loss: 2.532085]\n",
      "epoch:6 step:5692 [D loss: 0.582424, acc: 68.75%] [G loss: 2.563196]\n",
      "epoch:6 step:5693 [D loss: 0.580584, acc: 67.19%] [G loss: 2.681447]\n",
      "epoch:6 step:5694 [D loss: 0.642646, acc: 57.81%] [G loss: 2.369329]\n",
      "epoch:6 step:5695 [D loss: 0.646513, acc: 65.62%] [G loss: 2.092210]\n",
      "epoch:6 step:5696 [D loss: 0.490476, acc: 75.78%] [G loss: 2.477173]\n",
      "epoch:6 step:5697 [D loss: 0.494649, acc: 76.56%] [G loss: 2.823539]\n",
      "epoch:6 step:5698 [D loss: 0.565946, acc: 70.31%] [G loss: 2.984859]\n",
      "epoch:6 step:5699 [D loss: 0.541069, acc: 70.31%] [G loss: 2.698435]\n",
      "epoch:6 step:5700 [D loss: 0.642865, acc: 68.75%] [G loss: 2.535895]\n",
      "epoch:6 step:5701 [D loss: 0.643430, acc: 64.84%] [G loss: 2.378664]\n",
      "epoch:6 step:5702 [D loss: 0.636936, acc: 62.50%] [G loss: 2.273153]\n",
      "epoch:6 step:5703 [D loss: 0.592691, acc: 68.75%] [G loss: 2.559556]\n",
      "epoch:6 step:5704 [D loss: 0.638243, acc: 64.06%] [G loss: 2.495657]\n",
      "epoch:6 step:5705 [D loss: 0.612226, acc: 67.19%] [G loss: 2.725318]\n",
      "epoch:6 step:5706 [D loss: 0.599953, acc: 60.94%] [G loss: 2.331188]\n",
      "epoch:6 step:5707 [D loss: 0.591107, acc: 65.62%] [G loss: 2.362236]\n",
      "epoch:6 step:5708 [D loss: 0.628517, acc: 64.06%] [G loss: 2.256877]\n",
      "epoch:6 step:5709 [D loss: 0.632555, acc: 60.16%] [G loss: 2.180669]\n",
      "epoch:6 step:5710 [D loss: 0.596743, acc: 71.88%] [G loss: 2.409660]\n",
      "epoch:6 step:5711 [D loss: 0.550906, acc: 72.66%] [G loss: 2.513509]\n",
      "epoch:6 step:5712 [D loss: 0.603835, acc: 65.62%] [G loss: 2.514147]\n",
      "epoch:6 step:5713 [D loss: 0.639046, acc: 60.16%] [G loss: 2.327427]\n",
      "epoch:6 step:5714 [D loss: 0.615731, acc: 62.50%] [G loss: 2.540147]\n",
      "epoch:6 step:5715 [D loss: 0.547778, acc: 75.00%] [G loss: 2.598906]\n",
      "epoch:6 step:5716 [D loss: 0.608386, acc: 67.97%] [G loss: 2.487405]\n",
      "epoch:6 step:5717 [D loss: 0.580439, acc: 72.66%] [G loss: 2.383481]\n",
      "epoch:6 step:5718 [D loss: 0.581700, acc: 67.97%] [G loss: 2.431295]\n",
      "epoch:6 step:5719 [D loss: 0.596071, acc: 61.72%] [G loss: 2.688571]\n",
      "epoch:6 step:5720 [D loss: 0.664480, acc: 60.16%] [G loss: 2.255987]\n",
      "epoch:6 step:5721 [D loss: 0.612734, acc: 67.97%] [G loss: 2.189492]\n",
      "epoch:6 step:5722 [D loss: 0.581468, acc: 72.66%] [G loss: 2.258174]\n",
      "epoch:6 step:5723 [D loss: 0.552857, acc: 76.56%] [G loss: 2.233765]\n",
      "epoch:6 step:5724 [D loss: 0.629828, acc: 66.41%] [G loss: 2.521317]\n",
      "epoch:6 step:5725 [D loss: 0.478021, acc: 81.25%] [G loss: 2.484511]\n",
      "epoch:6 step:5726 [D loss: 0.569779, acc: 69.53%] [G loss: 2.308631]\n",
      "epoch:6 step:5727 [D loss: 0.631669, acc: 67.19%] [G loss: 2.576333]\n",
      "epoch:6 step:5728 [D loss: 0.561758, acc: 71.09%] [G loss: 2.391648]\n",
      "epoch:6 step:5729 [D loss: 0.591696, acc: 67.97%] [G loss: 2.361636]\n",
      "epoch:6 step:5730 [D loss: 0.652772, acc: 67.97%] [G loss: 2.348900]\n",
      "epoch:6 step:5731 [D loss: 0.643753, acc: 62.50%] [G loss: 2.444998]\n",
      "epoch:6 step:5732 [D loss: 0.640637, acc: 61.72%] [G loss: 2.257521]\n",
      "epoch:6 step:5733 [D loss: 0.537126, acc: 74.22%] [G loss: 2.430401]\n",
      "epoch:6 step:5734 [D loss: 0.580157, acc: 70.31%] [G loss: 2.596262]\n",
      "epoch:6 step:5735 [D loss: 0.600356, acc: 68.75%] [G loss: 2.552294]\n",
      "epoch:6 step:5736 [D loss: 0.650269, acc: 61.72%] [G loss: 2.380973]\n",
      "epoch:6 step:5737 [D loss: 0.562491, acc: 68.75%] [G loss: 2.672711]\n",
      "epoch:6 step:5738 [D loss: 0.626779, acc: 66.41%] [G loss: 2.519782]\n",
      "epoch:6 step:5739 [D loss: 0.571629, acc: 71.09%] [G loss: 2.744444]\n",
      "epoch:6 step:5740 [D loss: 0.665618, acc: 63.28%] [G loss: 2.563403]\n",
      "epoch:6 step:5741 [D loss: 0.554804, acc: 69.53%] [G loss: 2.715424]\n",
      "epoch:6 step:5742 [D loss: 0.704474, acc: 60.94%] [G loss: 2.598738]\n",
      "epoch:6 step:5743 [D loss: 0.646525, acc: 62.50%] [G loss: 2.355710]\n",
      "epoch:6 step:5744 [D loss: 0.622742, acc: 65.62%] [G loss: 2.807281]\n",
      "epoch:6 step:5745 [D loss: 0.674909, acc: 58.59%] [G loss: 2.255759]\n",
      "epoch:6 step:5746 [D loss: 0.710399, acc: 55.47%] [G loss: 2.059610]\n",
      "epoch:6 step:5747 [D loss: 0.691563, acc: 60.94%] [G loss: 2.190002]\n",
      "epoch:6 step:5748 [D loss: 0.604838, acc: 71.09%] [G loss: 2.400465]\n",
      "epoch:6 step:5749 [D loss: 0.646127, acc: 63.28%] [G loss: 2.286369]\n",
      "epoch:6 step:5750 [D loss: 0.634185, acc: 64.06%] [G loss: 2.314573]\n",
      "epoch:6 step:5751 [D loss: 0.663259, acc: 60.94%] [G loss: 2.233592]\n",
      "epoch:6 step:5752 [D loss: 0.617264, acc: 62.50%] [G loss: 2.524919]\n",
      "epoch:6 step:5753 [D loss: 0.574938, acc: 75.00%] [G loss: 2.507582]\n",
      "epoch:6 step:5754 [D loss: 0.597919, acc: 60.16%] [G loss: 2.612227]\n",
      "epoch:6 step:5755 [D loss: 0.677961, acc: 55.47%] [G loss: 2.147091]\n",
      "epoch:6 step:5756 [D loss: 0.617626, acc: 66.41%] [G loss: 2.117512]\n",
      "epoch:6 step:5757 [D loss: 0.577438, acc: 68.75%] [G loss: 2.224350]\n",
      "epoch:6 step:5758 [D loss: 0.576119, acc: 71.88%] [G loss: 2.346857]\n",
      "epoch:6 step:5759 [D loss: 0.646550, acc: 64.84%] [G loss: 2.333685]\n",
      "epoch:6 step:5760 [D loss: 0.551947, acc: 71.09%] [G loss: 2.471178]\n",
      "epoch:6 step:5761 [D loss: 0.592111, acc: 72.66%] [G loss: 2.629694]\n",
      "epoch:6 step:5762 [D loss: 0.648268, acc: 69.53%] [G loss: 2.302010]\n",
      "epoch:6 step:5763 [D loss: 0.658540, acc: 62.50%] [G loss: 2.066849]\n",
      "epoch:6 step:5764 [D loss: 0.587934, acc: 75.00%] [G loss: 2.251748]\n",
      "epoch:6 step:5765 [D loss: 0.638547, acc: 65.62%] [G loss: 2.215847]\n",
      "epoch:6 step:5766 [D loss: 0.566603, acc: 71.09%] [G loss: 2.621118]\n",
      "epoch:6 step:5767 [D loss: 0.582677, acc: 73.44%] [G loss: 2.271317]\n",
      "epoch:6 step:5768 [D loss: 0.670328, acc: 60.16%] [G loss: 2.351978]\n",
      "epoch:6 step:5769 [D loss: 0.650299, acc: 61.72%] [G loss: 2.315081]\n",
      "epoch:6 step:5770 [D loss: 0.686887, acc: 58.59%] [G loss: 2.317985]\n",
      "epoch:6 step:5771 [D loss: 0.541594, acc: 71.88%] [G loss: 2.512949]\n",
      "epoch:6 step:5772 [D loss: 0.586593, acc: 68.75%] [G loss: 2.295135]\n",
      "epoch:6 step:5773 [D loss: 0.543154, acc: 75.78%] [G loss: 3.090678]\n",
      "epoch:6 step:5774 [D loss: 0.553540, acc: 68.75%] [G loss: 2.599187]\n",
      "epoch:6 step:5775 [D loss: 0.671021, acc: 59.38%] [G loss: 2.250577]\n",
      "epoch:6 step:5776 [D loss: 0.608161, acc: 72.66%] [G loss: 2.521965]\n",
      "epoch:6 step:5777 [D loss: 0.539540, acc: 74.22%] [G loss: 2.476061]\n",
      "epoch:6 step:5778 [D loss: 0.572425, acc: 72.66%] [G loss: 2.552779]\n",
      "epoch:6 step:5779 [D loss: 0.623433, acc: 64.06%] [G loss: 2.141936]\n",
      "epoch:6 step:5780 [D loss: 0.592861, acc: 67.19%] [G loss: 2.552556]\n",
      "epoch:6 step:5781 [D loss: 0.554530, acc: 74.22%] [G loss: 2.451407]\n",
      "epoch:6 step:5782 [D loss: 0.676110, acc: 63.28%] [G loss: 2.254766]\n",
      "epoch:6 step:5783 [D loss: 0.620086, acc: 69.53%] [G loss: 2.368221]\n",
      "epoch:6 step:5784 [D loss: 0.644718, acc: 65.62%] [G loss: 2.457463]\n",
      "epoch:6 step:5785 [D loss: 0.613582, acc: 67.19%] [G loss: 2.603107]\n",
      "epoch:6 step:5786 [D loss: 0.582288, acc: 74.22%] [G loss: 2.322049]\n",
      "epoch:6 step:5787 [D loss: 0.650964, acc: 66.41%] [G loss: 2.531104]\n",
      "epoch:6 step:5788 [D loss: 0.651118, acc: 64.06%] [G loss: 2.518694]\n",
      "epoch:6 step:5789 [D loss: 0.646071, acc: 63.28%] [G loss: 2.534817]\n",
      "epoch:6 step:5790 [D loss: 0.631860, acc: 68.75%] [G loss: 2.371706]\n",
      "epoch:6 step:5791 [D loss: 0.528546, acc: 75.00%] [G loss: 2.211331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5792 [D loss: 0.635361, acc: 63.28%] [G loss: 2.186975]\n",
      "epoch:6 step:5793 [D loss: 0.692818, acc: 56.25%] [G loss: 2.401709]\n",
      "epoch:6 step:5794 [D loss: 0.702751, acc: 56.25%] [G loss: 2.331919]\n",
      "epoch:6 step:5795 [D loss: 0.648777, acc: 64.84%] [G loss: 2.109273]\n",
      "epoch:6 step:5796 [D loss: 0.542334, acc: 71.09%] [G loss: 2.448296]\n",
      "epoch:6 step:5797 [D loss: 0.570120, acc: 70.31%] [G loss: 2.219597]\n",
      "epoch:6 step:5798 [D loss: 0.518891, acc: 78.12%] [G loss: 2.361167]\n",
      "epoch:6 step:5799 [D loss: 0.623525, acc: 65.62%] [G loss: 2.498621]\n",
      "epoch:6 step:5800 [D loss: 0.626054, acc: 62.50%] [G loss: 2.457352]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.837904\n",
      "FID: 42.440662\n",
      "0 = 13.198352806663538\n",
      "1 = 0.09400904153397543\n",
      "2 = 0.9223999977111816\n",
      "3 = 0.920799970626831\n",
      "4 = 0.9240000247955322\n",
      "5 = 0.9237560033798218\n",
      "6 = 0.920799970626831\n",
      "7 = 8.708064251613655\n",
      "8 = 0.12822427392710517\n",
      "9 = 0.8305000066757202\n",
      "10 = 0.8352000117301941\n",
      "11 = 0.8258000016212463\n",
      "12 = 0.8274222016334534\n",
      "13 = 0.8352000117301941\n",
      "14 = 5.837924957275391\n",
      "15 = 9.112348556518555\n",
      "16 = 0.20617976784706116\n",
      "17 = 5.837904453277588\n",
      "18 = 42.4406623840332\n",
      "epoch:6 step:5801 [D loss: 0.703061, acc: 55.47%] [G loss: 2.148046]\n",
      "epoch:6 step:5802 [D loss: 0.614464, acc: 70.31%] [G loss: 2.428913]\n",
      "epoch:6 step:5803 [D loss: 0.656750, acc: 64.06%] [G loss: 2.314332]\n",
      "epoch:6 step:5804 [D loss: 0.674177, acc: 65.62%] [G loss: 2.033890]\n",
      "epoch:6 step:5805 [D loss: 0.648245, acc: 57.03%] [G loss: 2.256256]\n",
      "epoch:6 step:5806 [D loss: 0.632026, acc: 62.50%] [G loss: 2.323055]\n",
      "epoch:6 step:5807 [D loss: 0.643935, acc: 62.50%] [G loss: 2.454624]\n",
      "epoch:6 step:5808 [D loss: 0.567819, acc: 65.62%] [G loss: 2.355834]\n",
      "epoch:6 step:5809 [D loss: 0.646478, acc: 62.50%] [G loss: 2.296776]\n",
      "epoch:6 step:5810 [D loss: 0.582165, acc: 72.66%] [G loss: 2.131633]\n",
      "epoch:6 step:5811 [D loss: 0.642139, acc: 61.72%] [G loss: 2.133205]\n",
      "epoch:6 step:5812 [D loss: 0.669860, acc: 60.94%] [G loss: 2.350444]\n",
      "epoch:6 step:5813 [D loss: 0.576966, acc: 67.19%] [G loss: 2.474827]\n",
      "epoch:6 step:5814 [D loss: 0.672115, acc: 60.16%] [G loss: 2.326508]\n",
      "epoch:6 step:5815 [D loss: 0.608374, acc: 65.62%] [G loss: 2.336139]\n",
      "epoch:6 step:5816 [D loss: 0.537489, acc: 71.09%] [G loss: 2.605432]\n",
      "epoch:6 step:5817 [D loss: 0.631851, acc: 61.72%] [G loss: 2.494216]\n",
      "epoch:6 step:5818 [D loss: 0.609067, acc: 68.75%] [G loss: 2.296474]\n",
      "epoch:6 step:5819 [D loss: 0.548852, acc: 75.78%] [G loss: 2.406347]\n",
      "epoch:6 step:5820 [D loss: 0.595876, acc: 66.41%] [G loss: 2.542226]\n",
      "epoch:6 step:5821 [D loss: 0.596862, acc: 67.19%] [G loss: 2.298630]\n",
      "epoch:6 step:5822 [D loss: 0.585883, acc: 70.31%] [G loss: 2.160744]\n",
      "epoch:6 step:5823 [D loss: 0.573541, acc: 72.66%] [G loss: 2.436281]\n",
      "epoch:6 step:5824 [D loss: 0.663095, acc: 58.59%] [G loss: 2.351525]\n",
      "epoch:6 step:5825 [D loss: 0.692375, acc: 62.50%] [G loss: 2.246146]\n",
      "epoch:6 step:5826 [D loss: 0.608280, acc: 65.62%] [G loss: 2.380656]\n",
      "epoch:6 step:5827 [D loss: 0.590239, acc: 69.53%] [G loss: 2.286601]\n",
      "epoch:6 step:5828 [D loss: 0.616380, acc: 67.97%] [G loss: 2.573704]\n",
      "epoch:6 step:5829 [D loss: 0.559950, acc: 67.97%] [G loss: 2.748146]\n",
      "epoch:6 step:5830 [D loss: 0.537509, acc: 69.53%] [G loss: 2.614409]\n",
      "epoch:6 step:5831 [D loss: 0.570175, acc: 67.97%] [G loss: 2.486734]\n",
      "epoch:6 step:5832 [D loss: 0.647527, acc: 61.72%] [G loss: 2.288929]\n",
      "epoch:6 step:5833 [D loss: 0.654543, acc: 60.94%] [G loss: 2.070410]\n",
      "epoch:6 step:5834 [D loss: 0.622238, acc: 64.84%] [G loss: 2.315484]\n",
      "epoch:6 step:5835 [D loss: 0.685830, acc: 61.72%] [G loss: 2.279120]\n",
      "epoch:6 step:5836 [D loss: 0.692331, acc: 60.94%] [G loss: 2.061637]\n",
      "epoch:6 step:5837 [D loss: 0.627696, acc: 62.50%] [G loss: 2.224070]\n",
      "epoch:6 step:5838 [D loss: 0.575382, acc: 70.31%] [G loss: 2.517861]\n",
      "epoch:6 step:5839 [D loss: 0.513270, acc: 78.12%] [G loss: 2.571017]\n",
      "epoch:6 step:5840 [D loss: 0.606050, acc: 67.97%] [G loss: 2.314060]\n",
      "epoch:6 step:5841 [D loss: 0.540269, acc: 72.66%] [G loss: 2.523476]\n",
      "epoch:6 step:5842 [D loss: 0.726732, acc: 59.38%] [G loss: 2.256582]\n",
      "epoch:6 step:5843 [D loss: 0.617519, acc: 61.72%] [G loss: 2.310647]\n",
      "epoch:6 step:5844 [D loss: 0.663134, acc: 64.84%] [G loss: 2.451241]\n",
      "epoch:6 step:5845 [D loss: 0.660111, acc: 64.06%] [G loss: 2.201761]\n",
      "epoch:6 step:5846 [D loss: 0.618565, acc: 64.06%] [G loss: 2.129240]\n",
      "epoch:6 step:5847 [D loss: 0.630950, acc: 64.84%] [G loss: 2.065118]\n",
      "epoch:6 step:5848 [D loss: 0.561180, acc: 71.09%] [G loss: 2.337079]\n",
      "epoch:6 step:5849 [D loss: 0.619261, acc: 63.28%] [G loss: 2.125300]\n",
      "epoch:6 step:5850 [D loss: 0.599741, acc: 66.41%] [G loss: 2.287753]\n",
      "epoch:6 step:5851 [D loss: 0.573449, acc: 70.31%] [G loss: 2.646184]\n",
      "epoch:6 step:5852 [D loss: 0.497456, acc: 74.22%] [G loss: 2.997720]\n",
      "epoch:6 step:5853 [D loss: 0.506849, acc: 75.78%] [G loss: 3.291825]\n",
      "epoch:6 step:5854 [D loss: 0.540014, acc: 78.91%] [G loss: 3.161241]\n",
      "epoch:6 step:5855 [D loss: 0.580263, acc: 67.19%] [G loss: 2.655250]\n",
      "epoch:6 step:5856 [D loss: 0.699041, acc: 63.28%] [G loss: 2.386182]\n",
      "epoch:6 step:5857 [D loss: 0.596476, acc: 67.97%] [G loss: 2.308003]\n",
      "epoch:6 step:5858 [D loss: 0.606266, acc: 66.41%] [G loss: 2.384455]\n",
      "epoch:6 step:5859 [D loss: 0.653745, acc: 59.38%] [G loss: 2.271360]\n",
      "epoch:6 step:5860 [D loss: 0.615262, acc: 70.31%] [G loss: 2.303032]\n",
      "epoch:6 step:5861 [D loss: 0.625446, acc: 64.84%] [G loss: 2.361780]\n",
      "epoch:6 step:5862 [D loss: 0.557534, acc: 73.44%] [G loss: 2.241165]\n",
      "epoch:6 step:5863 [D loss: 0.641207, acc: 64.06%] [G loss: 2.377046]\n",
      "epoch:6 step:5864 [D loss: 0.603207, acc: 65.62%] [G loss: 2.226564]\n",
      "epoch:6 step:5865 [D loss: 0.614516, acc: 65.62%] [G loss: 2.073492]\n",
      "epoch:6 step:5866 [D loss: 0.592745, acc: 67.19%] [G loss: 2.338641]\n",
      "epoch:6 step:5867 [D loss: 0.541944, acc: 68.75%] [G loss: 2.616226]\n",
      "epoch:6 step:5868 [D loss: 0.701678, acc: 58.59%] [G loss: 2.303872]\n",
      "epoch:6 step:5869 [D loss: 0.611911, acc: 62.50%] [G loss: 2.401672]\n",
      "epoch:6 step:5870 [D loss: 0.632232, acc: 60.94%] [G loss: 2.473209]\n",
      "epoch:6 step:5871 [D loss: 0.675981, acc: 62.50%] [G loss: 2.152183]\n",
      "epoch:6 step:5872 [D loss: 0.668883, acc: 60.16%] [G loss: 1.963599]\n",
      "epoch:6 step:5873 [D loss: 0.690210, acc: 58.59%] [G loss: 2.039390]\n",
      "epoch:6 step:5874 [D loss: 0.691998, acc: 53.91%] [G loss: 1.958952]\n",
      "epoch:6 step:5875 [D loss: 0.557702, acc: 74.22%] [G loss: 2.211364]\n",
      "epoch:6 step:5876 [D loss: 0.637487, acc: 63.28%] [G loss: 2.309697]\n",
      "epoch:6 step:5877 [D loss: 0.620472, acc: 66.41%] [G loss: 2.237685]\n",
      "epoch:6 step:5878 [D loss: 0.640743, acc: 67.97%] [G loss: 2.310194]\n",
      "epoch:6 step:5879 [D loss: 0.594941, acc: 71.09%] [G loss: 1.994981]\n",
      "epoch:6 step:5880 [D loss: 0.540931, acc: 72.66%] [G loss: 2.313125]\n",
      "epoch:6 step:5881 [D loss: 0.568260, acc: 76.56%] [G loss: 2.571529]\n",
      "epoch:6 step:5882 [D loss: 0.641317, acc: 58.59%] [G loss: 2.506216]\n",
      "epoch:6 step:5883 [D loss: 0.657060, acc: 67.19%] [G loss: 2.216498]\n",
      "epoch:6 step:5884 [D loss: 0.552897, acc: 72.66%] [G loss: 2.511836]\n",
      "epoch:6 step:5885 [D loss: 0.679467, acc: 64.06%] [G loss: 2.217446]\n",
      "epoch:6 step:5886 [D loss: 0.636544, acc: 63.28%] [G loss: 2.499182]\n",
      "epoch:6 step:5887 [D loss: 0.631326, acc: 64.84%] [G loss: 2.251471]\n",
      "epoch:6 step:5888 [D loss: 0.658575, acc: 64.06%] [G loss: 2.171330]\n",
      "epoch:6 step:5889 [D loss: 0.553189, acc: 74.22%] [G loss: 2.389990]\n",
      "epoch:6 step:5890 [D loss: 0.623973, acc: 64.06%] [G loss: 2.274377]\n",
      "epoch:6 step:5891 [D loss: 0.527827, acc: 74.22%] [G loss: 2.331205]\n",
      "epoch:6 step:5892 [D loss: 0.533560, acc: 71.88%] [G loss: 2.424948]\n",
      "epoch:6 step:5893 [D loss: 0.499807, acc: 78.91%] [G loss: 2.378184]\n",
      "epoch:6 step:5894 [D loss: 0.555061, acc: 73.44%] [G loss: 2.544084]\n",
      "epoch:6 step:5895 [D loss: 0.580396, acc: 67.97%] [G loss: 2.769788]\n",
      "epoch:6 step:5896 [D loss: 0.674103, acc: 70.31%] [G loss: 2.453262]\n",
      "epoch:6 step:5897 [D loss: 0.647154, acc: 60.94%] [G loss: 2.153588]\n",
      "epoch:6 step:5898 [D loss: 0.658946, acc: 62.50%] [G loss: 2.347826]\n",
      "epoch:6 step:5899 [D loss: 0.600694, acc: 63.28%] [G loss: 2.282931]\n",
      "epoch:6 step:5900 [D loss: 0.639614, acc: 65.62%] [G loss: 2.361134]\n",
      "epoch:6 step:5901 [D loss: 0.606700, acc: 70.31%] [G loss: 2.605769]\n",
      "epoch:6 step:5902 [D loss: 0.661438, acc: 64.06%] [G loss: 2.552048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5903 [D loss: 0.663288, acc: 66.41%] [G loss: 2.140575]\n",
      "epoch:6 step:5904 [D loss: 0.534743, acc: 72.66%] [G loss: 2.343005]\n",
      "epoch:6 step:5905 [D loss: 0.607463, acc: 64.06%] [G loss: 2.479945]\n",
      "epoch:6 step:5906 [D loss: 0.566416, acc: 68.75%] [G loss: 2.586979]\n",
      "epoch:6 step:5907 [D loss: 0.559542, acc: 69.53%] [G loss: 2.443041]\n",
      "epoch:6 step:5908 [D loss: 0.511458, acc: 77.34%] [G loss: 2.654981]\n",
      "epoch:6 step:5909 [D loss: 0.596773, acc: 66.41%] [G loss: 2.226354]\n",
      "epoch:6 step:5910 [D loss: 0.682445, acc: 64.06%] [G loss: 2.251683]\n",
      "epoch:6 step:5911 [D loss: 0.572558, acc: 74.22%] [G loss: 2.554907]\n",
      "epoch:6 step:5912 [D loss: 0.627687, acc: 59.38%] [G loss: 2.553766]\n",
      "epoch:6 step:5913 [D loss: 0.530561, acc: 70.31%] [G loss: 2.659523]\n",
      "epoch:6 step:5914 [D loss: 0.645221, acc: 60.94%] [G loss: 2.448786]\n",
      "epoch:6 step:5915 [D loss: 0.569167, acc: 67.97%] [G loss: 2.523090]\n",
      "epoch:6 step:5916 [D loss: 0.574763, acc: 68.75%] [G loss: 2.260334]\n",
      "epoch:6 step:5917 [D loss: 0.659258, acc: 65.62%] [G loss: 2.544483]\n",
      "epoch:6 step:5918 [D loss: 0.530995, acc: 75.78%] [G loss: 2.553620]\n",
      "epoch:6 step:5919 [D loss: 0.586796, acc: 66.41%] [G loss: 2.573360]\n",
      "epoch:6 step:5920 [D loss: 0.603328, acc: 66.41%] [G loss: 2.712485]\n",
      "epoch:6 step:5921 [D loss: 0.603120, acc: 61.72%] [G loss: 2.457572]\n",
      "epoch:6 step:5922 [D loss: 0.544557, acc: 71.88%] [G loss: 2.650230]\n",
      "epoch:6 step:5923 [D loss: 0.662950, acc: 58.59%] [G loss: 2.164991]\n",
      "epoch:6 step:5924 [D loss: 0.637528, acc: 58.59%] [G loss: 2.220029]\n",
      "epoch:6 step:5925 [D loss: 0.593380, acc: 65.62%] [G loss: 2.373865]\n",
      "epoch:6 step:5926 [D loss: 0.559320, acc: 75.78%] [G loss: 2.413878]\n",
      "epoch:6 step:5927 [D loss: 0.604605, acc: 71.09%] [G loss: 2.612743]\n",
      "epoch:6 step:5928 [D loss: 0.629899, acc: 67.19%] [G loss: 2.563671]\n",
      "epoch:6 step:5929 [D loss: 0.536060, acc: 71.88%] [G loss: 2.294372]\n",
      "epoch:6 step:5930 [D loss: 0.626337, acc: 64.06%] [G loss: 2.227984]\n",
      "epoch:6 step:5931 [D loss: 0.676419, acc: 64.06%] [G loss: 2.565083]\n",
      "epoch:6 step:5932 [D loss: 0.566061, acc: 70.31%] [G loss: 2.281699]\n",
      "epoch:6 step:5933 [D loss: 0.601205, acc: 64.06%] [G loss: 2.279658]\n",
      "epoch:6 step:5934 [D loss: 0.507962, acc: 72.66%] [G loss: 3.252813]\n",
      "epoch:6 step:5935 [D loss: 0.588752, acc: 74.22%] [G loss: 3.011492]\n",
      "epoch:6 step:5936 [D loss: 0.493272, acc: 79.69%] [G loss: 2.978553]\n",
      "epoch:6 step:5937 [D loss: 0.539845, acc: 70.31%] [G loss: 2.966471]\n",
      "epoch:6 step:5938 [D loss: 0.670603, acc: 68.75%] [G loss: 2.317252]\n",
      "epoch:6 step:5939 [D loss: 0.634766, acc: 62.50%] [G loss: 2.334959]\n",
      "epoch:6 step:5940 [D loss: 0.642872, acc: 64.84%] [G loss: 2.556280]\n",
      "epoch:6 step:5941 [D loss: 0.642876, acc: 63.28%] [G loss: 2.419104]\n",
      "epoch:6 step:5942 [D loss: 0.526330, acc: 73.44%] [G loss: 2.471077]\n",
      "epoch:6 step:5943 [D loss: 0.560393, acc: 71.88%] [G loss: 2.694152]\n",
      "epoch:6 step:5944 [D loss: 0.585943, acc: 69.53%] [G loss: 2.476670]\n",
      "epoch:6 step:5945 [D loss: 0.706681, acc: 57.03%] [G loss: 2.263707]\n",
      "epoch:6 step:5946 [D loss: 0.605226, acc: 67.97%] [G loss: 2.505753]\n",
      "epoch:6 step:5947 [D loss: 0.688033, acc: 57.81%] [G loss: 2.516166]\n",
      "epoch:6 step:5948 [D loss: 0.566720, acc: 70.31%] [G loss: 2.255209]\n",
      "epoch:6 step:5949 [D loss: 0.618000, acc: 63.28%] [G loss: 2.355120]\n",
      "epoch:6 step:5950 [D loss: 0.696815, acc: 57.03%] [G loss: 2.146291]\n",
      "epoch:6 step:5951 [D loss: 0.555574, acc: 73.44%] [G loss: 2.300073]\n",
      "epoch:6 step:5952 [D loss: 0.598732, acc: 75.00%] [G loss: 2.586509]\n",
      "epoch:6 step:5953 [D loss: 0.610642, acc: 67.19%] [G loss: 2.629364]\n",
      "epoch:6 step:5954 [D loss: 0.582525, acc: 67.97%] [G loss: 2.505697]\n",
      "epoch:6 step:5955 [D loss: 0.538215, acc: 72.66%] [G loss: 2.752073]\n",
      "epoch:6 step:5956 [D loss: 0.593189, acc: 66.41%] [G loss: 2.510017]\n",
      "epoch:6 step:5957 [D loss: 0.678133, acc: 63.28%] [G loss: 2.970296]\n",
      "epoch:6 step:5958 [D loss: 0.607786, acc: 63.28%] [G loss: 2.666837]\n",
      "epoch:6 step:5959 [D loss: 0.581052, acc: 75.78%] [G loss: 2.493370]\n",
      "epoch:6 step:5960 [D loss: 0.634915, acc: 66.41%] [G loss: 2.614755]\n",
      "epoch:6 step:5961 [D loss: 0.566314, acc: 69.53%] [G loss: 2.315971]\n",
      "epoch:6 step:5962 [D loss: 0.577235, acc: 69.53%] [G loss: 2.569145]\n",
      "epoch:6 step:5963 [D loss: 0.707886, acc: 53.12%] [G loss: 2.114534]\n",
      "epoch:6 step:5964 [D loss: 0.660438, acc: 63.28%] [G loss: 2.208611]\n",
      "epoch:6 step:5965 [D loss: 0.662076, acc: 68.75%] [G loss: 2.235484]\n",
      "epoch:6 step:5966 [D loss: 0.584646, acc: 68.75%] [G loss: 2.711021]\n",
      "epoch:6 step:5967 [D loss: 0.604590, acc: 65.62%] [G loss: 2.750377]\n",
      "epoch:6 step:5968 [D loss: 0.532995, acc: 71.09%] [G loss: 2.960734]\n",
      "epoch:6 step:5969 [D loss: 0.520415, acc: 75.00%] [G loss: 2.990563]\n",
      "epoch:6 step:5970 [D loss: 0.681795, acc: 62.50%] [G loss: 2.147932]\n",
      "epoch:6 step:5971 [D loss: 0.672725, acc: 63.28%] [G loss: 1.952868]\n",
      "epoch:6 step:5972 [D loss: 0.553614, acc: 71.88%] [G loss: 2.274293]\n",
      "epoch:6 step:5973 [D loss: 0.607550, acc: 67.19%] [G loss: 2.273919]\n",
      "epoch:6 step:5974 [D loss: 0.640629, acc: 64.06%] [G loss: 2.316211]\n",
      "epoch:6 step:5975 [D loss: 0.568249, acc: 68.75%] [G loss: 2.751448]\n",
      "epoch:6 step:5976 [D loss: 0.567889, acc: 72.66%] [G loss: 2.472625]\n",
      "epoch:6 step:5977 [D loss: 0.626394, acc: 69.53%] [G loss: 2.235940]\n",
      "epoch:6 step:5978 [D loss: 0.650482, acc: 64.84%] [G loss: 2.266644]\n",
      "epoch:6 step:5979 [D loss: 0.561193, acc: 71.09%] [G loss: 2.344048]\n",
      "epoch:6 step:5980 [D loss: 0.543944, acc: 68.75%] [G loss: 2.204161]\n",
      "epoch:6 step:5981 [D loss: 0.580774, acc: 74.22%] [G loss: 2.430982]\n",
      "epoch:6 step:5982 [D loss: 0.648256, acc: 64.84%] [G loss: 2.286986]\n",
      "epoch:6 step:5983 [D loss: 0.692813, acc: 60.94%] [G loss: 2.171157]\n",
      "epoch:6 step:5984 [D loss: 0.612584, acc: 66.41%] [G loss: 2.468070]\n",
      "epoch:6 step:5985 [D loss: 0.579263, acc: 68.75%] [G loss: 2.409588]\n",
      "epoch:6 step:5986 [D loss: 0.543301, acc: 73.44%] [G loss: 2.374847]\n",
      "epoch:6 step:5987 [D loss: 0.619974, acc: 65.62%] [G loss: 2.539010]\n",
      "epoch:6 step:5988 [D loss: 0.517640, acc: 76.56%] [G loss: 2.366479]\n",
      "epoch:6 step:5989 [D loss: 0.613447, acc: 65.62%] [G loss: 2.572623]\n",
      "epoch:6 step:5990 [D loss: 0.581138, acc: 71.88%] [G loss: 2.470524]\n",
      "epoch:6 step:5991 [D loss: 0.577480, acc: 70.31%] [G loss: 2.470872]\n",
      "epoch:6 step:5992 [D loss: 0.548657, acc: 73.44%] [G loss: 2.324971]\n",
      "epoch:6 step:5993 [D loss: 0.597070, acc: 65.62%] [G loss: 2.321063]\n",
      "epoch:6 step:5994 [D loss: 0.546973, acc: 75.00%] [G loss: 2.409954]\n",
      "epoch:6 step:5995 [D loss: 0.650202, acc: 63.28%] [G loss: 2.117374]\n",
      "epoch:6 step:5996 [D loss: 0.526046, acc: 74.22%] [G loss: 2.598290]\n",
      "epoch:6 step:5997 [D loss: 0.629319, acc: 68.75%] [G loss: 2.254297]\n",
      "epoch:6 step:5998 [D loss: 0.715783, acc: 58.59%] [G loss: 2.076936]\n",
      "epoch:6 step:5999 [D loss: 0.689739, acc: 65.62%] [G loss: 2.068601]\n",
      "epoch:6 step:6000 [D loss: 0.589929, acc: 69.53%] [G loss: 2.152905]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.070318\n",
      "FID: 36.687214\n",
      "0 = 13.153216233158133\n",
      "1 = 0.0938281687054306\n",
      "2 = 0.9232000112533569\n",
      "3 = 0.9273999929428101\n",
      "4 = 0.9190000295639038\n",
      "5 = 0.9196747541427612\n",
      "6 = 0.9273999929428101\n",
      "7 = 8.35769856214524\n",
      "8 = 0.11904453091930295\n",
      "9 = 0.8144000172615051\n",
      "10 = 0.8271999955177307\n",
      "11 = 0.8015999794006348\n",
      "12 = 0.8065522909164429\n",
      "13 = 0.8271999955177307\n",
      "14 = 6.070339202880859\n",
      "15 = 9.254326820373535\n",
      "16 = 0.18486995995044708\n",
      "17 = 6.070318222045898\n",
      "18 = 36.68721389770508\n",
      "epoch:6 step:6001 [D loss: 0.635361, acc: 62.50%] [G loss: 2.310491]\n",
      "epoch:6 step:6002 [D loss: 0.646171, acc: 63.28%] [G loss: 2.502360]\n",
      "epoch:6 step:6003 [D loss: 0.524847, acc: 80.47%] [G loss: 2.520318]\n",
      "epoch:6 step:6004 [D loss: 0.587023, acc: 70.31%] [G loss: 2.561286]\n",
      "epoch:6 step:6005 [D loss: 0.632468, acc: 67.19%] [G loss: 2.270631]\n",
      "epoch:6 step:6006 [D loss: 0.606512, acc: 72.66%] [G loss: 2.287665]\n",
      "epoch:6 step:6007 [D loss: 0.651411, acc: 66.41%] [G loss: 2.458818]\n",
      "epoch:6 step:6008 [D loss: 0.611683, acc: 66.41%] [G loss: 2.193320]\n",
      "epoch:6 step:6009 [D loss: 0.661947, acc: 60.94%] [G loss: 2.189733]\n",
      "epoch:6 step:6010 [D loss: 0.591961, acc: 65.62%] [G loss: 2.252996]\n",
      "epoch:6 step:6011 [D loss: 0.612052, acc: 64.06%] [G loss: 2.435960]\n",
      "epoch:6 step:6012 [D loss: 0.565376, acc: 69.53%] [G loss: 2.364063]\n",
      "epoch:6 step:6013 [D loss: 0.637875, acc: 59.38%] [G loss: 2.411854]\n",
      "epoch:6 step:6014 [D loss: 0.629816, acc: 62.50%] [G loss: 2.383376]\n",
      "epoch:6 step:6015 [D loss: 0.611475, acc: 63.28%] [G loss: 2.435512]\n",
      "epoch:6 step:6016 [D loss: 0.642006, acc: 60.94%] [G loss: 2.543509]\n",
      "epoch:6 step:6017 [D loss: 0.604276, acc: 70.31%] [G loss: 2.324068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6018 [D loss: 0.657275, acc: 60.16%] [G loss: 2.274595]\n",
      "epoch:6 step:6019 [D loss: 0.668433, acc: 59.38%] [G loss: 2.277179]\n",
      "epoch:6 step:6020 [D loss: 0.590744, acc: 70.31%] [G loss: 2.287020]\n",
      "epoch:6 step:6021 [D loss: 0.572964, acc: 69.53%] [G loss: 2.743375]\n",
      "epoch:6 step:6022 [D loss: 0.667546, acc: 66.41%] [G loss: 2.300034]\n",
      "epoch:6 step:6023 [D loss: 0.616776, acc: 64.84%] [G loss: 2.277857]\n",
      "epoch:6 step:6024 [D loss: 0.562308, acc: 73.44%] [G loss: 2.496265]\n",
      "epoch:6 step:6025 [D loss: 0.589990, acc: 72.66%] [G loss: 2.407217]\n",
      "epoch:6 step:6026 [D loss: 0.657310, acc: 63.28%] [G loss: 2.344505]\n",
      "epoch:6 step:6027 [D loss: 0.542828, acc: 71.88%] [G loss: 2.658662]\n",
      "epoch:6 step:6028 [D loss: 0.576778, acc: 72.66%] [G loss: 2.512234]\n",
      "epoch:6 step:6029 [D loss: 0.649481, acc: 62.50%] [G loss: 2.516766]\n",
      "epoch:6 step:6030 [D loss: 0.622732, acc: 67.19%] [G loss: 2.290884]\n",
      "epoch:6 step:6031 [D loss: 0.553364, acc: 72.66%] [G loss: 2.635183]\n",
      "epoch:6 step:6032 [D loss: 0.650859, acc: 61.72%] [G loss: 2.185085]\n",
      "epoch:6 step:6033 [D loss: 0.563202, acc: 71.09%] [G loss: 2.542514]\n",
      "epoch:6 step:6034 [D loss: 0.588882, acc: 70.31%] [G loss: 2.335631]\n",
      "epoch:6 step:6035 [D loss: 0.624756, acc: 65.62%] [G loss: 2.303577]\n",
      "epoch:6 step:6036 [D loss: 0.663301, acc: 63.28%] [G loss: 2.325364]\n",
      "epoch:6 step:6037 [D loss: 0.740748, acc: 63.28%] [G loss: 2.278179]\n",
      "epoch:6 step:6038 [D loss: 0.572366, acc: 66.41%] [G loss: 2.562243]\n",
      "epoch:6 step:6039 [D loss: 0.701406, acc: 60.94%] [G loss: 2.139606]\n",
      "epoch:6 step:6040 [D loss: 0.583794, acc: 64.06%] [G loss: 2.319545]\n",
      "epoch:6 step:6041 [D loss: 0.743043, acc: 57.81%] [G loss: 2.366324]\n",
      "epoch:6 step:6042 [D loss: 0.611932, acc: 66.41%] [G loss: 2.164143]\n",
      "epoch:6 step:6043 [D loss: 0.679629, acc: 62.50%] [G loss: 2.272189]\n",
      "epoch:6 step:6044 [D loss: 0.650290, acc: 60.16%] [G loss: 2.210142]\n",
      "epoch:6 step:6045 [D loss: 0.583616, acc: 70.31%] [G loss: 2.254836]\n",
      "epoch:6 step:6046 [D loss: 0.599401, acc: 69.53%] [G loss: 2.372229]\n",
      "epoch:6 step:6047 [D loss: 0.554583, acc: 70.31%] [G loss: 2.550855]\n",
      "epoch:6 step:6048 [D loss: 0.672688, acc: 60.94%] [G loss: 2.748678]\n",
      "epoch:6 step:6049 [D loss: 0.569273, acc: 69.53%] [G loss: 2.767613]\n",
      "epoch:6 step:6050 [D loss: 0.543163, acc: 70.31%] [G loss: 2.828263]\n",
      "epoch:6 step:6051 [D loss: 0.651496, acc: 61.72%] [G loss: 2.648182]\n",
      "epoch:6 step:6052 [D loss: 0.560055, acc: 68.75%] [G loss: 2.875800]\n",
      "epoch:6 step:6053 [D loss: 0.645125, acc: 65.62%] [G loss: 2.320628]\n",
      "epoch:6 step:6054 [D loss: 0.695068, acc: 55.47%] [G loss: 2.250745]\n",
      "epoch:6 step:6055 [D loss: 0.620728, acc: 62.50%] [G loss: 2.398924]\n",
      "epoch:6 step:6056 [D loss: 0.572739, acc: 71.09%] [G loss: 2.647508]\n",
      "epoch:6 step:6057 [D loss: 0.616237, acc: 69.53%] [G loss: 2.270617]\n",
      "epoch:6 step:6058 [D loss: 0.577364, acc: 69.53%] [G loss: 2.153363]\n",
      "epoch:6 step:6059 [D loss: 0.694591, acc: 54.69%] [G loss: 2.081695]\n",
      "epoch:6 step:6060 [D loss: 0.653258, acc: 60.16%] [G loss: 2.196496]\n",
      "epoch:6 step:6061 [D loss: 0.610275, acc: 66.41%] [G loss: 2.192622]\n",
      "epoch:6 step:6062 [D loss: 0.627639, acc: 67.19%] [G loss: 2.385034]\n",
      "epoch:6 step:6063 [D loss: 0.636627, acc: 63.28%] [G loss: 2.193377]\n",
      "epoch:6 step:6064 [D loss: 0.646130, acc: 68.75%] [G loss: 2.245886]\n",
      "epoch:6 step:6065 [D loss: 0.572634, acc: 70.31%] [G loss: 2.171990]\n",
      "epoch:6 step:6066 [D loss: 0.614041, acc: 67.97%] [G loss: 2.221482]\n",
      "epoch:6 step:6067 [D loss: 0.635054, acc: 71.09%] [G loss: 2.197708]\n",
      "epoch:6 step:6068 [D loss: 0.644420, acc: 60.94%] [G loss: 2.176219]\n",
      "epoch:6 step:6069 [D loss: 0.619524, acc: 71.88%] [G loss: 2.310337]\n",
      "epoch:6 step:6070 [D loss: 0.722742, acc: 56.25%] [G loss: 2.107357]\n",
      "epoch:6 step:6071 [D loss: 0.590922, acc: 67.97%] [G loss: 2.093191]\n",
      "epoch:6 step:6072 [D loss: 0.585626, acc: 75.00%] [G loss: 2.209561]\n",
      "epoch:6 step:6073 [D loss: 0.532365, acc: 71.88%] [G loss: 2.568785]\n",
      "epoch:6 step:6074 [D loss: 0.661962, acc: 61.72%] [G loss: 2.246104]\n",
      "epoch:6 step:6075 [D loss: 0.638911, acc: 64.06%] [G loss: 2.347519]\n",
      "epoch:6 step:6076 [D loss: 0.651987, acc: 65.62%] [G loss: 2.174176]\n",
      "epoch:6 step:6077 [D loss: 0.637806, acc: 64.06%] [G loss: 2.021160]\n",
      "epoch:6 step:6078 [D loss: 0.681371, acc: 60.94%] [G loss: 2.089381]\n",
      "epoch:6 step:6079 [D loss: 0.550579, acc: 71.09%] [G loss: 2.161524]\n",
      "epoch:6 step:6080 [D loss: 0.622581, acc: 65.62%] [G loss: 2.129558]\n",
      "epoch:6 step:6081 [D loss: 0.606481, acc: 77.34%] [G loss: 2.198263]\n",
      "epoch:6 step:6082 [D loss: 0.603465, acc: 66.41%] [G loss: 2.336040]\n",
      "epoch:6 step:6083 [D loss: 0.585965, acc: 71.88%] [G loss: 2.202762]\n",
      "epoch:6 step:6084 [D loss: 0.600941, acc: 70.31%] [G loss: 2.415963]\n",
      "epoch:6 step:6085 [D loss: 0.580374, acc: 69.53%] [G loss: 2.188317]\n",
      "epoch:6 step:6086 [D loss: 0.560536, acc: 74.22%] [G loss: 2.588598]\n",
      "epoch:6 step:6087 [D loss: 0.602019, acc: 71.09%] [G loss: 2.201909]\n",
      "epoch:6 step:6088 [D loss: 0.559942, acc: 73.44%] [G loss: 2.409606]\n",
      "epoch:6 step:6089 [D loss: 0.619567, acc: 69.53%] [G loss: 2.381606]\n",
      "epoch:6 step:6090 [D loss: 0.582389, acc: 74.22%] [G loss: 2.322513]\n",
      "epoch:6 step:6091 [D loss: 0.601376, acc: 69.53%] [G loss: 2.599250]\n",
      "epoch:6 step:6092 [D loss: 0.540099, acc: 71.09%] [G loss: 2.479535]\n",
      "epoch:6 step:6093 [D loss: 0.660399, acc: 63.28%] [G loss: 2.797571]\n",
      "epoch:6 step:6094 [D loss: 0.571754, acc: 70.31%] [G loss: 2.579483]\n",
      "epoch:6 step:6095 [D loss: 0.654756, acc: 60.16%] [G loss: 2.384077]\n",
      "epoch:6 step:6096 [D loss: 0.579575, acc: 68.75%] [G loss: 2.685524]\n",
      "epoch:6 step:6097 [D loss: 0.565379, acc: 68.75%] [G loss: 2.371528]\n",
      "epoch:6 step:6098 [D loss: 0.659199, acc: 62.50%] [G loss: 2.349730]\n",
      "epoch:6 step:6099 [D loss: 0.705819, acc: 57.81%] [G loss: 2.002845]\n",
      "epoch:6 step:6100 [D loss: 0.723772, acc: 57.81%] [G loss: 2.110052]\n",
      "epoch:6 step:6101 [D loss: 0.539764, acc: 73.44%] [G loss: 2.454329]\n",
      "epoch:6 step:6102 [D loss: 0.668225, acc: 61.72%] [G loss: 2.070036]\n",
      "epoch:6 step:6103 [D loss: 0.619690, acc: 66.41%] [G loss: 2.198438]\n",
      "epoch:6 step:6104 [D loss: 0.632874, acc: 64.84%] [G loss: 2.164620]\n",
      "epoch:6 step:6105 [D loss: 0.669563, acc: 61.72%] [G loss: 2.143633]\n",
      "epoch:6 step:6106 [D loss: 0.617638, acc: 63.28%] [G loss: 2.393350]\n",
      "epoch:6 step:6107 [D loss: 0.619127, acc: 63.28%] [G loss: 2.374134]\n",
      "epoch:6 step:6108 [D loss: 0.607017, acc: 71.09%] [G loss: 2.356934]\n",
      "epoch:6 step:6109 [D loss: 0.622957, acc: 61.72%] [G loss: 2.255706]\n",
      "epoch:6 step:6110 [D loss: 0.621647, acc: 65.62%] [G loss: 2.331032]\n",
      "epoch:6 step:6111 [D loss: 0.646346, acc: 61.72%] [G loss: 2.292957]\n",
      "epoch:6 step:6112 [D loss: 0.585966, acc: 66.41%] [G loss: 2.151384]\n",
      "epoch:6 step:6113 [D loss: 0.626472, acc: 59.38%] [G loss: 2.250611]\n",
      "epoch:6 step:6114 [D loss: 0.632169, acc: 60.16%] [G loss: 2.191122]\n",
      "epoch:6 step:6115 [D loss: 0.629687, acc: 64.06%] [G loss: 2.210693]\n",
      "epoch:6 step:6116 [D loss: 0.547219, acc: 69.53%] [G loss: 2.370909]\n",
      "epoch:6 step:6117 [D loss: 0.565723, acc: 75.00%] [G loss: 2.172580]\n",
      "epoch:6 step:6118 [D loss: 0.601060, acc: 65.62%] [G loss: 2.369257]\n",
      "epoch:6 step:6119 [D loss: 0.611895, acc: 67.19%] [G loss: 2.419233]\n",
      "epoch:6 step:6120 [D loss: 0.545258, acc: 70.31%] [G loss: 2.527459]\n",
      "epoch:6 step:6121 [D loss: 0.503755, acc: 78.91%] [G loss: 2.798707]\n",
      "epoch:6 step:6122 [D loss: 0.668526, acc: 56.25%] [G loss: 2.247144]\n",
      "epoch:6 step:6123 [D loss: 0.594596, acc: 67.97%] [G loss: 2.284628]\n",
      "epoch:6 step:6124 [D loss: 0.644149, acc: 60.94%] [G loss: 2.095489]\n",
      "epoch:6 step:6125 [D loss: 0.632264, acc: 64.84%] [G loss: 2.428388]\n",
      "epoch:6 step:6126 [D loss: 0.588342, acc: 63.28%] [G loss: 2.686709]\n",
      "epoch:6 step:6127 [D loss: 0.620896, acc: 63.28%] [G loss: 2.218340]\n",
      "epoch:6 step:6128 [D loss: 0.649256, acc: 67.97%] [G loss: 2.092206]\n",
      "epoch:6 step:6129 [D loss: 0.592640, acc: 65.62%] [G loss: 2.173343]\n",
      "epoch:6 step:6130 [D loss: 0.521707, acc: 75.78%] [G loss: 2.465752]\n",
      "epoch:6 step:6131 [D loss: 0.678562, acc: 60.94%] [G loss: 2.365912]\n",
      "epoch:6 step:6132 [D loss: 0.669312, acc: 62.50%] [G loss: 2.258630]\n",
      "epoch:6 step:6133 [D loss: 0.647923, acc: 62.50%] [G loss: 2.091506]\n",
      "epoch:6 step:6134 [D loss: 0.597639, acc: 70.31%] [G loss: 2.306617]\n",
      "epoch:6 step:6135 [D loss: 0.649005, acc: 60.94%] [G loss: 2.324984]\n",
      "epoch:6 step:6136 [D loss: 0.572961, acc: 67.19%] [G loss: 2.465273]\n",
      "epoch:6 step:6137 [D loss: 0.588841, acc: 73.44%] [G loss: 2.264043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6138 [D loss: 0.564397, acc: 71.88%] [G loss: 2.580587]\n",
      "epoch:6 step:6139 [D loss: 0.638434, acc: 64.06%] [G loss: 2.339899]\n",
      "epoch:6 step:6140 [D loss: 0.659874, acc: 63.28%] [G loss: 2.217829]\n",
      "epoch:6 step:6141 [D loss: 0.596110, acc: 67.97%] [G loss: 2.191905]\n",
      "epoch:6 step:6142 [D loss: 0.624702, acc: 65.62%] [G loss: 2.200282]\n",
      "epoch:6 step:6143 [D loss: 0.558405, acc: 71.88%] [G loss: 2.425693]\n",
      "epoch:6 step:6144 [D loss: 0.620665, acc: 63.28%] [G loss: 2.439668]\n",
      "epoch:6 step:6145 [D loss: 0.542069, acc: 71.88%] [G loss: 2.503046]\n",
      "epoch:6 step:6146 [D loss: 0.572683, acc: 69.53%] [G loss: 2.329450]\n",
      "epoch:6 step:6147 [D loss: 0.621445, acc: 65.62%] [G loss: 2.401667]\n",
      "epoch:6 step:6148 [D loss: 0.696258, acc: 59.38%] [G loss: 2.357402]\n",
      "epoch:6 step:6149 [D loss: 0.625646, acc: 69.53%] [G loss: 2.395787]\n",
      "epoch:6 step:6150 [D loss: 0.622715, acc: 68.75%] [G loss: 2.205485]\n",
      "epoch:6 step:6151 [D loss: 0.653997, acc: 64.06%] [G loss: 2.216841]\n",
      "epoch:6 step:6152 [D loss: 0.592133, acc: 67.19%] [G loss: 2.483063]\n",
      "epoch:6 step:6153 [D loss: 0.699487, acc: 60.94%] [G loss: 2.189116]\n",
      "epoch:6 step:6154 [D loss: 0.604149, acc: 69.53%] [G loss: 2.229017]\n",
      "epoch:6 step:6155 [D loss: 0.608875, acc: 67.97%] [G loss: 2.371293]\n",
      "epoch:6 step:6156 [D loss: 0.570148, acc: 71.88%] [G loss: 2.556991]\n",
      "epoch:6 step:6157 [D loss: 0.550918, acc: 72.66%] [G loss: 2.228428]\n",
      "epoch:6 step:6158 [D loss: 0.583615, acc: 69.53%] [G loss: 2.404767]\n",
      "epoch:6 step:6159 [D loss: 0.611586, acc: 67.97%] [G loss: 2.494416]\n",
      "epoch:6 step:6160 [D loss: 0.586311, acc: 68.75%] [G loss: 2.240913]\n",
      "epoch:6 step:6161 [D loss: 0.636563, acc: 64.06%] [G loss: 2.350111]\n",
      "epoch:6 step:6162 [D loss: 0.679318, acc: 59.38%] [G loss: 2.430580]\n",
      "epoch:6 step:6163 [D loss: 0.616931, acc: 60.94%] [G loss: 2.258632]\n",
      "epoch:6 step:6164 [D loss: 0.638633, acc: 63.28%] [G loss: 2.238114]\n",
      "epoch:6 step:6165 [D loss: 0.591649, acc: 67.19%] [G loss: 2.353919]\n",
      "epoch:6 step:6166 [D loss: 0.619149, acc: 68.75%] [G loss: 2.359947]\n",
      "epoch:6 step:6167 [D loss: 0.625959, acc: 65.62%] [G loss: 2.172890]\n",
      "epoch:6 step:6168 [D loss: 0.605877, acc: 72.66%] [G loss: 2.292470]\n",
      "epoch:6 step:6169 [D loss: 0.608500, acc: 67.97%] [G loss: 2.514341]\n",
      "epoch:6 step:6170 [D loss: 0.629323, acc: 65.62%] [G loss: 2.367839]\n",
      "epoch:6 step:6171 [D loss: 0.600769, acc: 66.41%] [G loss: 2.420015]\n",
      "epoch:6 step:6172 [D loss: 0.590208, acc: 67.97%] [G loss: 2.690043]\n",
      "epoch:6 step:6173 [D loss: 0.600944, acc: 68.75%] [G loss: 2.466156]\n",
      "epoch:6 step:6174 [D loss: 0.575447, acc: 72.66%] [G loss: 2.240307]\n",
      "epoch:6 step:6175 [D loss: 0.627468, acc: 64.84%] [G loss: 2.334319]\n",
      "epoch:6 step:6176 [D loss: 0.516108, acc: 78.12%] [G loss: 2.621235]\n",
      "epoch:6 step:6177 [D loss: 0.507158, acc: 77.34%] [G loss: 2.429461]\n",
      "epoch:6 step:6178 [D loss: 0.594304, acc: 68.75%] [G loss: 2.490175]\n",
      "epoch:6 step:6179 [D loss: 0.576374, acc: 72.66%] [G loss: 2.449116]\n",
      "epoch:6 step:6180 [D loss: 0.510916, acc: 75.78%] [G loss: 2.629571]\n",
      "epoch:6 step:6181 [D loss: 0.649898, acc: 63.28%] [G loss: 2.320498]\n",
      "epoch:6 step:6182 [D loss: 0.623384, acc: 68.75%] [G loss: 2.269845]\n",
      "epoch:6 step:6183 [D loss: 0.592940, acc: 68.75%] [G loss: 2.354149]\n",
      "epoch:6 step:6184 [D loss: 0.668540, acc: 63.28%] [G loss: 2.294659]\n",
      "epoch:6 step:6185 [D loss: 0.668411, acc: 60.94%] [G loss: 2.212821]\n",
      "epoch:6 step:6186 [D loss: 0.549551, acc: 75.78%] [G loss: 2.451680]\n",
      "epoch:6 step:6187 [D loss: 0.599163, acc: 67.97%] [G loss: 2.210259]\n",
      "epoch:6 step:6188 [D loss: 0.740223, acc: 51.56%] [G loss: 2.211267]\n",
      "epoch:6 step:6189 [D loss: 0.611071, acc: 67.97%] [G loss: 2.327259]\n",
      "epoch:6 step:6190 [D loss: 0.560618, acc: 73.44%] [G loss: 2.462140]\n",
      "epoch:6 step:6191 [D loss: 0.631418, acc: 61.72%] [G loss: 2.455427]\n",
      "epoch:6 step:6192 [D loss: 0.649382, acc: 70.31%] [G loss: 2.366595]\n",
      "epoch:6 step:6193 [D loss: 0.623915, acc: 66.41%] [G loss: 2.128932]\n",
      "epoch:6 step:6194 [D loss: 0.649114, acc: 62.50%] [G loss: 2.117485]\n",
      "epoch:6 step:6195 [D loss: 0.557652, acc: 72.66%] [G loss: 2.297354]\n",
      "epoch:6 step:6196 [D loss: 0.639462, acc: 64.84%] [G loss: 2.286484]\n",
      "epoch:6 step:6197 [D loss: 0.601402, acc: 69.53%] [G loss: 2.241911]\n",
      "epoch:6 step:6198 [D loss: 0.649428, acc: 63.28%] [G loss: 2.013338]\n",
      "epoch:6 step:6199 [D loss: 0.629361, acc: 58.59%] [G loss: 2.081492]\n",
      "epoch:6 step:6200 [D loss: 0.604737, acc: 71.09%] [G loss: 2.327193]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 5.954514\n",
      "FID: 39.209774\n",
      "0 = 12.91206377229694\n",
      "1 = 0.08383378122663834\n",
      "2 = 0.9203000068664551\n",
      "3 = 0.9164000153541565\n",
      "4 = 0.9241999983787537\n",
      "5 = 0.9236041307449341\n",
      "6 = 0.9164000153541565\n",
      "7 = 8.426249152612646\n",
      "8 = 0.12626532055648507\n",
      "9 = 0.8093000054359436\n",
      "10 = 0.8100000023841858\n",
      "11 = 0.8086000084877014\n",
      "12 = 0.8088675737380981\n",
      "13 = 0.8100000023841858\n",
      "14 = 5.954536437988281\n",
      "15 = 9.135781288146973\n",
      "16 = 0.20744521915912628\n",
      "17 = 5.954514026641846\n",
      "18 = 39.209774017333984\n",
      "epoch:6 step:6201 [D loss: 0.658233, acc: 62.50%] [G loss: 2.277638]\n",
      "epoch:6 step:6202 [D loss: 0.580206, acc: 67.97%] [G loss: 2.068634]\n",
      "epoch:6 step:6203 [D loss: 0.597057, acc: 71.88%] [G loss: 2.344421]\n",
      "epoch:6 step:6204 [D loss: 0.559267, acc: 71.88%] [G loss: 2.453949]\n",
      "epoch:6 step:6205 [D loss: 0.613016, acc: 66.41%] [G loss: 2.200301]\n",
      "epoch:6 step:6206 [D loss: 0.658905, acc: 59.38%] [G loss: 2.428134]\n",
      "epoch:6 step:6207 [D loss: 0.629385, acc: 63.28%] [G loss: 2.237672]\n",
      "epoch:6 step:6208 [D loss: 0.620680, acc: 64.06%] [G loss: 2.194511]\n",
      "epoch:6 step:6209 [D loss: 0.627629, acc: 66.41%] [G loss: 2.485312]\n",
      "epoch:6 step:6210 [D loss: 0.628952, acc: 61.72%] [G loss: 2.442051]\n",
      "epoch:6 step:6211 [D loss: 0.597010, acc: 71.88%] [G loss: 2.296304]\n",
      "epoch:6 step:6212 [D loss: 0.689678, acc: 59.38%] [G loss: 2.190968]\n",
      "epoch:6 step:6213 [D loss: 0.613744, acc: 69.53%] [G loss: 2.132580]\n",
      "epoch:6 step:6214 [D loss: 0.604874, acc: 71.88%] [G loss: 2.368392]\n",
      "epoch:6 step:6215 [D loss: 0.633239, acc: 64.84%] [G loss: 1.962977]\n",
      "epoch:6 step:6216 [D loss: 0.637073, acc: 60.16%] [G loss: 2.229263]\n",
      "epoch:6 step:6217 [D loss: 0.599988, acc: 68.75%] [G loss: 2.175706]\n",
      "epoch:6 step:6218 [D loss: 0.666375, acc: 60.16%] [G loss: 2.122861]\n",
      "epoch:6 step:6219 [D loss: 0.618192, acc: 63.28%] [G loss: 2.057589]\n",
      "epoch:6 step:6220 [D loss: 0.580490, acc: 68.75%] [G loss: 2.331859]\n",
      "epoch:6 step:6221 [D loss: 0.670601, acc: 60.94%] [G loss: 2.326576]\n",
      "epoch:6 step:6222 [D loss: 0.630351, acc: 63.28%] [G loss: 2.233157]\n",
      "epoch:6 step:6223 [D loss: 0.614999, acc: 64.06%] [G loss: 2.312330]\n",
      "epoch:6 step:6224 [D loss: 0.581934, acc: 69.53%] [G loss: 2.374792]\n",
      "epoch:6 step:6225 [D loss: 0.579658, acc: 67.19%] [G loss: 2.561506]\n",
      "epoch:6 step:6226 [D loss: 0.594553, acc: 71.09%] [G loss: 2.047940]\n",
      "epoch:6 step:6227 [D loss: 0.591211, acc: 68.75%] [G loss: 2.432014]\n",
      "epoch:6 step:6228 [D loss: 0.637370, acc: 65.62%] [G loss: 2.240896]\n",
      "epoch:6 step:6229 [D loss: 0.626490, acc: 64.84%] [G loss: 2.375338]\n",
      "epoch:6 step:6230 [D loss: 0.615913, acc: 63.28%] [G loss: 2.118626]\n",
      "epoch:6 step:6231 [D loss: 0.587003, acc: 67.19%] [G loss: 2.322593]\n",
      "epoch:6 step:6232 [D loss: 0.565059, acc: 68.75%] [G loss: 2.149587]\n",
      "epoch:6 step:6233 [D loss: 0.618662, acc: 63.28%] [G loss: 2.170821]\n",
      "epoch:6 step:6234 [D loss: 0.668190, acc: 62.50%] [G loss: 2.453716]\n",
      "epoch:6 step:6235 [D loss: 0.555168, acc: 69.53%] [G loss: 2.547336]\n",
      "epoch:6 step:6236 [D loss: 0.634432, acc: 65.62%] [G loss: 2.062335]\n",
      "epoch:6 step:6237 [D loss: 0.657716, acc: 60.16%] [G loss: 1.955891]\n",
      "epoch:6 step:6238 [D loss: 0.666757, acc: 64.06%] [G loss: 2.052028]\n",
      "epoch:6 step:6239 [D loss: 0.575561, acc: 73.44%] [G loss: 2.332002]\n",
      "epoch:6 step:6240 [D loss: 0.620351, acc: 67.97%] [G loss: 2.023455]\n",
      "epoch:6 step:6241 [D loss: 0.592201, acc: 66.41%] [G loss: 2.124557]\n",
      "epoch:6 step:6242 [D loss: 0.589908, acc: 73.44%] [G loss: 2.238227]\n",
      "epoch:6 step:6243 [D loss: 0.674229, acc: 64.06%] [G loss: 2.233695]\n",
      "epoch:6 step:6244 [D loss: 0.699608, acc: 56.25%] [G loss: 2.083195]\n",
      "epoch:6 step:6245 [D loss: 0.618842, acc: 63.28%] [G loss: 2.315299]\n",
      "epoch:6 step:6246 [D loss: 0.587978, acc: 71.09%] [G loss: 2.535863]\n",
      "epoch:6 step:6247 [D loss: 0.597673, acc: 67.19%] [G loss: 2.164506]\n",
      "epoch:6 step:6248 [D loss: 0.652026, acc: 62.50%] [G loss: 2.308582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6249 [D loss: 0.622553, acc: 67.19%] [G loss: 2.179410]\n",
      "epoch:6 step:6250 [D loss: 0.603570, acc: 65.62%] [G loss: 2.315223]\n",
      "epoch:6 step:6251 [D loss: 0.623425, acc: 68.75%] [G loss: 2.402541]\n",
      "epoch:6 step:6252 [D loss: 0.582203, acc: 67.97%] [G loss: 2.357514]\n",
      "epoch:6 step:6253 [D loss: 0.585470, acc: 74.22%] [G loss: 2.462322]\n",
      "epoch:6 step:6254 [D loss: 0.566282, acc: 66.41%] [G loss: 2.373961]\n",
      "epoch:6 step:6255 [D loss: 0.542549, acc: 75.00%] [G loss: 2.449442]\n",
      "epoch:6 step:6256 [D loss: 0.582042, acc: 67.19%] [G loss: 2.598284]\n",
      "epoch:6 step:6257 [D loss: 0.703702, acc: 57.81%] [G loss: 2.452691]\n",
      "epoch:6 step:6258 [D loss: 0.533580, acc: 75.00%] [G loss: 2.343317]\n",
      "epoch:6 step:6259 [D loss: 0.595317, acc: 71.09%] [G loss: 2.562387]\n",
      "epoch:6 step:6260 [D loss: 0.547966, acc: 73.44%] [G loss: 2.376307]\n",
      "epoch:6 step:6261 [D loss: 0.609102, acc: 66.41%] [G loss: 2.561065]\n",
      "epoch:6 step:6262 [D loss: 0.630850, acc: 63.28%] [G loss: 2.374341]\n",
      "epoch:6 step:6263 [D loss: 0.627502, acc: 63.28%] [G loss: 2.640941]\n",
      "epoch:6 step:6264 [D loss: 0.483517, acc: 77.34%] [G loss: 2.657555]\n",
      "epoch:6 step:6265 [D loss: 0.612201, acc: 67.97%] [G loss: 2.629287]\n",
      "epoch:6 step:6266 [D loss: 0.632980, acc: 64.84%] [G loss: 2.473540]\n",
      "epoch:6 step:6267 [D loss: 0.564656, acc: 67.97%] [G loss: 2.392467]\n",
      "epoch:6 step:6268 [D loss: 0.666660, acc: 59.38%] [G loss: 2.357002]\n",
      "epoch:6 step:6269 [D loss: 0.591093, acc: 69.53%] [G loss: 2.650633]\n",
      "epoch:6 step:6270 [D loss: 0.573062, acc: 75.00%] [G loss: 2.839763]\n",
      "epoch:6 step:6271 [D loss: 0.500603, acc: 75.78%] [G loss: 2.552679]\n",
      "epoch:6 step:6272 [D loss: 0.589318, acc: 65.62%] [G loss: 2.715788]\n",
      "epoch:6 step:6273 [D loss: 0.648600, acc: 62.50%] [G loss: 2.535893]\n",
      "epoch:6 step:6274 [D loss: 0.597361, acc: 63.28%] [G loss: 2.194927]\n",
      "epoch:6 step:6275 [D loss: 0.550007, acc: 71.09%] [G loss: 2.392088]\n",
      "epoch:6 step:6276 [D loss: 0.533046, acc: 71.09%] [G loss: 2.583732]\n",
      "epoch:6 step:6277 [D loss: 0.638054, acc: 63.28%] [G loss: 2.635515]\n",
      "epoch:6 step:6278 [D loss: 0.608581, acc: 67.19%] [G loss: 2.162337]\n",
      "epoch:6 step:6279 [D loss: 0.687716, acc: 60.16%] [G loss: 2.196015]\n",
      "epoch:6 step:6280 [D loss: 0.655467, acc: 61.72%] [G loss: 2.066420]\n",
      "epoch:6 step:6281 [D loss: 0.633727, acc: 65.62%] [G loss: 2.222166]\n",
      "epoch:6 step:6282 [D loss: 0.602848, acc: 75.00%] [G loss: 2.198166]\n",
      "epoch:6 step:6283 [D loss: 0.627663, acc: 64.84%] [G loss: 2.148296]\n",
      "epoch:6 step:6284 [D loss: 0.612274, acc: 68.75%] [G loss: 2.436919]\n",
      "epoch:6 step:6285 [D loss: 0.535718, acc: 71.09%] [G loss: 2.419927]\n",
      "epoch:6 step:6286 [D loss: 0.622191, acc: 58.59%] [G loss: 2.583968]\n",
      "epoch:6 step:6287 [D loss: 0.576888, acc: 70.31%] [G loss: 2.495826]\n",
      "epoch:6 step:6288 [D loss: 0.747565, acc: 56.25%] [G loss: 2.096337]\n",
      "epoch:6 step:6289 [D loss: 0.664023, acc: 61.72%] [G loss: 2.035580]\n",
      "epoch:6 step:6290 [D loss: 0.650084, acc: 66.41%] [G loss: 2.224285]\n",
      "epoch:6 step:6291 [D loss: 0.721691, acc: 53.12%] [G loss: 2.093304]\n",
      "epoch:6 step:6292 [D loss: 0.658964, acc: 60.94%] [G loss: 2.115093]\n",
      "epoch:6 step:6293 [D loss: 0.614288, acc: 65.62%] [G loss: 2.292106]\n",
      "epoch:6 step:6294 [D loss: 0.681180, acc: 63.28%] [G loss: 2.068788]\n",
      "epoch:6 step:6295 [D loss: 0.616125, acc: 65.62%] [G loss: 2.140286]\n",
      "epoch:6 step:6296 [D loss: 0.605035, acc: 71.09%] [G loss: 2.254748]\n",
      "epoch:6 step:6297 [D loss: 0.625784, acc: 65.62%] [G loss: 2.264512]\n",
      "epoch:6 step:6298 [D loss: 0.633009, acc: 61.72%] [G loss: 2.241714]\n",
      "epoch:6 step:6299 [D loss: 0.525059, acc: 75.00%] [G loss: 2.556040]\n",
      "epoch:6 step:6300 [D loss: 0.683793, acc: 60.94%] [G loss: 2.360397]\n",
      "epoch:6 step:6301 [D loss: 0.605728, acc: 66.41%] [G loss: 2.289999]\n",
      "epoch:6 step:6302 [D loss: 0.587095, acc: 64.06%] [G loss: 2.326819]\n",
      "epoch:6 step:6303 [D loss: 0.588632, acc: 69.53%] [G loss: 2.487160]\n",
      "epoch:6 step:6304 [D loss: 0.669243, acc: 66.41%] [G loss: 2.402343]\n",
      "epoch:6 step:6305 [D loss: 0.583321, acc: 67.19%] [G loss: 2.300220]\n",
      "epoch:6 step:6306 [D loss: 0.650887, acc: 60.16%] [G loss: 2.384485]\n",
      "epoch:6 step:6307 [D loss: 0.656545, acc: 60.94%] [G loss: 2.258916]\n",
      "epoch:6 step:6308 [D loss: 0.592548, acc: 71.09%] [G loss: 2.070712]\n",
      "epoch:6 step:6309 [D loss: 0.630887, acc: 63.28%] [G loss: 2.248235]\n",
      "epoch:6 step:6310 [D loss: 0.613889, acc: 67.19%] [G loss: 2.126714]\n",
      "epoch:6 step:6311 [D loss: 0.640528, acc: 66.41%] [G loss: 2.368405]\n",
      "epoch:6 step:6312 [D loss: 0.648368, acc: 67.97%] [G loss: 2.510273]\n",
      "epoch:6 step:6313 [D loss: 0.596687, acc: 66.41%] [G loss: 2.290291]\n",
      "epoch:6 step:6314 [D loss: 0.588234, acc: 66.41%] [G loss: 2.372015]\n",
      "epoch:6 step:6315 [D loss: 0.566275, acc: 67.19%] [G loss: 2.506582]\n",
      "epoch:6 step:6316 [D loss: 0.586255, acc: 68.75%] [G loss: 2.717838]\n",
      "epoch:6 step:6317 [D loss: 0.609056, acc: 67.97%] [G loss: 2.386672]\n",
      "epoch:6 step:6318 [D loss: 0.632992, acc: 61.72%] [G loss: 2.190526]\n",
      "epoch:6 step:6319 [D loss: 0.611361, acc: 67.19%] [G loss: 2.259308]\n",
      "epoch:6 step:6320 [D loss: 0.657746, acc: 59.38%] [G loss: 2.280765]\n",
      "epoch:6 step:6321 [D loss: 0.530329, acc: 76.56%] [G loss: 2.345419]\n",
      "epoch:6 step:6322 [D loss: 0.551175, acc: 70.31%] [G loss: 2.417647]\n",
      "epoch:6 step:6323 [D loss: 0.606189, acc: 67.97%] [G loss: 2.337033]\n",
      "epoch:6 step:6324 [D loss: 0.609498, acc: 66.41%] [G loss: 2.234262]\n",
      "epoch:6 step:6325 [D loss: 0.660920, acc: 62.50%] [G loss: 2.431870]\n",
      "epoch:6 step:6326 [D loss: 0.636822, acc: 57.81%] [G loss: 2.357798]\n",
      "epoch:6 step:6327 [D loss: 0.632457, acc: 64.84%] [G loss: 2.394832]\n",
      "epoch:6 step:6328 [D loss: 0.639518, acc: 61.72%] [G loss: 2.312199]\n",
      "epoch:6 step:6329 [D loss: 0.505908, acc: 79.69%] [G loss: 2.705032]\n",
      "epoch:6 step:6330 [D loss: 0.526743, acc: 75.00%] [G loss: 2.616160]\n",
      "epoch:6 step:6331 [D loss: 0.549374, acc: 71.09%] [G loss: 2.771143]\n",
      "epoch:6 step:6332 [D loss: 0.550417, acc: 74.22%] [G loss: 2.276640]\n",
      "epoch:6 step:6333 [D loss: 0.630180, acc: 64.84%] [G loss: 2.460770]\n",
      "epoch:6 step:6334 [D loss: 0.627671, acc: 62.50%] [G loss: 2.477315]\n",
      "epoch:6 step:6335 [D loss: 0.682730, acc: 62.50%] [G loss: 2.151230]\n",
      "epoch:6 step:6336 [D loss: 0.629457, acc: 69.53%] [G loss: 2.204955]\n",
      "epoch:6 step:6337 [D loss: 0.660224, acc: 65.62%] [G loss: 2.223343]\n",
      "epoch:6 step:6338 [D loss: 0.575076, acc: 71.09%] [G loss: 2.290930]\n",
      "epoch:6 step:6339 [D loss: 0.653366, acc: 62.50%] [G loss: 2.264813]\n",
      "epoch:6 step:6340 [D loss: 0.544984, acc: 74.22%] [G loss: 2.308623]\n",
      "epoch:6 step:6341 [D loss: 0.646167, acc: 60.94%] [G loss: 2.363054]\n",
      "epoch:6 step:6342 [D loss: 0.693538, acc: 57.81%] [G loss: 2.144599]\n",
      "epoch:6 step:6343 [D loss: 0.585627, acc: 69.53%] [G loss: 2.413199]\n",
      "epoch:6 step:6344 [D loss: 0.646263, acc: 67.97%] [G loss: 2.177531]\n",
      "epoch:6 step:6345 [D loss: 0.714669, acc: 55.47%] [G loss: 2.013634]\n",
      "epoch:6 step:6346 [D loss: 0.578737, acc: 67.97%] [G loss: 2.352087]\n",
      "epoch:6 step:6347 [D loss: 0.537320, acc: 75.78%] [G loss: 2.156420]\n",
      "epoch:6 step:6348 [D loss: 0.652699, acc: 60.94%] [G loss: 2.162645]\n",
      "epoch:6 step:6349 [D loss: 0.615123, acc: 71.09%] [G loss: 2.361196]\n",
      "epoch:6 step:6350 [D loss: 0.644112, acc: 68.75%] [G loss: 2.290808]\n",
      "epoch:6 step:6351 [D loss: 0.537258, acc: 74.22%] [G loss: 2.349376]\n",
      "epoch:6 step:6352 [D loss: 0.648168, acc: 64.06%] [G loss: 2.421760]\n",
      "epoch:6 step:6353 [D loss: 0.647754, acc: 65.62%] [G loss: 2.389007]\n",
      "epoch:6 step:6354 [D loss: 0.554593, acc: 71.88%] [G loss: 2.581093]\n",
      "epoch:6 step:6355 [D loss: 0.602836, acc: 67.19%] [G loss: 2.505829]\n",
      "epoch:6 step:6356 [D loss: 0.653559, acc: 62.50%] [G loss: 2.201342]\n",
      "epoch:6 step:6357 [D loss: 0.579365, acc: 73.44%] [G loss: 2.246058]\n",
      "epoch:6 step:6358 [D loss: 0.546965, acc: 73.44%] [G loss: 2.422971]\n",
      "epoch:6 step:6359 [D loss: 0.598466, acc: 69.53%] [G loss: 2.359096]\n",
      "epoch:6 step:6360 [D loss: 0.637215, acc: 67.19%] [G loss: 2.073671]\n",
      "epoch:6 step:6361 [D loss: 0.653412, acc: 67.19%] [G loss: 2.116936]\n",
      "epoch:6 step:6362 [D loss: 0.668988, acc: 62.50%] [G loss: 2.273799]\n",
      "epoch:6 step:6363 [D loss: 0.679728, acc: 60.16%] [G loss: 1.958057]\n",
      "epoch:6 step:6364 [D loss: 0.672582, acc: 60.16%] [G loss: 2.559441]\n",
      "epoch:6 step:6365 [D loss: 0.642388, acc: 67.19%] [G loss: 2.489447]\n",
      "epoch:6 step:6366 [D loss: 0.679899, acc: 61.72%] [G loss: 2.144593]\n",
      "epoch:6 step:6367 [D loss: 0.612238, acc: 63.28%] [G loss: 2.538567]\n",
      "epoch:6 step:6368 [D loss: 0.554716, acc: 75.00%] [G loss: 2.179990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6369 [D loss: 0.624480, acc: 55.47%] [G loss: 2.256298]\n",
      "epoch:6 step:6370 [D loss: 0.659315, acc: 58.59%] [G loss: 2.214101]\n",
      "epoch:6 step:6371 [D loss: 0.577265, acc: 71.88%] [G loss: 2.309222]\n",
      "epoch:6 step:6372 [D loss: 0.660680, acc: 60.94%] [G loss: 2.390904]\n",
      "epoch:6 step:6373 [D loss: 0.614517, acc: 65.62%] [G loss: 2.263877]\n",
      "epoch:6 step:6374 [D loss: 0.672855, acc: 60.94%] [G loss: 2.339877]\n",
      "epoch:6 step:6375 [D loss: 0.608650, acc: 64.06%] [G loss: 2.188329]\n",
      "epoch:6 step:6376 [D loss: 0.560191, acc: 68.75%] [G loss: 2.310553]\n",
      "epoch:6 step:6377 [D loss: 0.591321, acc: 66.41%] [G loss: 2.422408]\n",
      "epoch:6 step:6378 [D loss: 0.625303, acc: 61.72%] [G loss: 2.300181]\n",
      "epoch:6 step:6379 [D loss: 0.539043, acc: 72.66%] [G loss: 2.563618]\n",
      "epoch:6 step:6380 [D loss: 0.598327, acc: 70.31%] [G loss: 2.288385]\n",
      "epoch:6 step:6381 [D loss: 0.640416, acc: 60.94%] [G loss: 2.288290]\n",
      "epoch:6 step:6382 [D loss: 0.596866, acc: 64.84%] [G loss: 2.372602]\n",
      "epoch:6 step:6383 [D loss: 0.592685, acc: 67.97%] [G loss: 2.235458]\n",
      "epoch:6 step:6384 [D loss: 0.666387, acc: 63.28%] [G loss: 2.030089]\n",
      "epoch:6 step:6385 [D loss: 0.548559, acc: 74.22%] [G loss: 2.479071]\n",
      "epoch:6 step:6386 [D loss: 0.614954, acc: 64.84%] [G loss: 2.554612]\n",
      "epoch:6 step:6387 [D loss: 0.731774, acc: 57.81%] [G loss: 2.073510]\n",
      "epoch:6 step:6388 [D loss: 0.707494, acc: 57.03%] [G loss: 2.229911]\n",
      "epoch:6 step:6389 [D loss: 0.593547, acc: 70.31%] [G loss: 2.294067]\n",
      "epoch:6 step:6390 [D loss: 0.625016, acc: 60.94%] [G loss: 2.451097]\n",
      "epoch:6 step:6391 [D loss: 0.606057, acc: 61.72%] [G loss: 2.286160]\n",
      "epoch:6 step:6392 [D loss: 0.620225, acc: 64.84%] [G loss: 2.195312]\n",
      "epoch:6 step:6393 [D loss: 0.658538, acc: 63.28%] [G loss: 2.354429]\n",
      "epoch:6 step:6394 [D loss: 0.622761, acc: 64.84%] [G loss: 2.130100]\n",
      "epoch:6 step:6395 [D loss: 0.563467, acc: 71.88%] [G loss: 2.405867]\n",
      "epoch:6 step:6396 [D loss: 0.599638, acc: 68.75%] [G loss: 2.379663]\n",
      "epoch:6 step:6397 [D loss: 0.558308, acc: 74.22%] [G loss: 2.575931]\n",
      "epoch:6 step:6398 [D loss: 0.653344, acc: 67.97%] [G loss: 2.282771]\n",
      "epoch:6 step:6399 [D loss: 0.645812, acc: 69.53%] [G loss: 2.341575]\n",
      "epoch:6 step:6400 [D loss: 0.657236, acc: 67.19%] [G loss: 2.019401]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.191779\n",
      "FID: 35.891647\n",
      "0 = 13.046406700134261\n",
      "1 = 0.08577087895525971\n",
      "2 = 0.9067999720573425\n",
      "3 = 0.9133999943733215\n",
      "4 = 0.9002000093460083\n",
      "5 = 0.9015002250671387\n",
      "6 = 0.9133999943733215\n",
      "7 = 8.294417501461528\n",
      "8 = 0.12002386173788364\n",
      "9 = 0.8048999905586243\n",
      "10 = 0.8119999766349792\n",
      "11 = 0.7978000044822693\n",
      "12 = 0.800631046295166\n",
      "13 = 0.8119999766349792\n",
      "14 = 6.191801071166992\n",
      "15 = 9.272500991821289\n",
      "16 = 0.18125391006469727\n",
      "17 = 6.191778659820557\n",
      "18 = 35.89164733886719\n",
      "epoch:6 step:6401 [D loss: 0.533482, acc: 75.00%] [G loss: 2.553494]\n",
      "epoch:6 step:6402 [D loss: 0.620694, acc: 66.41%] [G loss: 2.562844]\n",
      "epoch:6 step:6403 [D loss: 0.523052, acc: 77.34%] [G loss: 2.312450]\n",
      "epoch:6 step:6404 [D loss: 0.592741, acc: 70.31%] [G loss: 2.512497]\n",
      "epoch:6 step:6405 [D loss: 0.636839, acc: 64.06%] [G loss: 2.559980]\n",
      "epoch:6 step:6406 [D loss: 0.621688, acc: 67.19%] [G loss: 2.370364]\n",
      "epoch:6 step:6407 [D loss: 0.636372, acc: 60.16%] [G loss: 2.359613]\n",
      "epoch:6 step:6408 [D loss: 0.513902, acc: 79.69%] [G loss: 2.557602]\n",
      "epoch:6 step:6409 [D loss: 0.627579, acc: 64.06%] [G loss: 2.209798]\n",
      "epoch:6 step:6410 [D loss: 0.625378, acc: 67.19%] [G loss: 2.205776]\n",
      "epoch:6 step:6411 [D loss: 0.691715, acc: 63.28%] [G loss: 2.292410]\n",
      "epoch:6 step:6412 [D loss: 0.584876, acc: 71.88%] [G loss: 2.228656]\n",
      "epoch:6 step:6413 [D loss: 0.614810, acc: 68.75%] [G loss: 2.072524]\n",
      "epoch:6 step:6414 [D loss: 0.525920, acc: 73.44%] [G loss: 2.995318]\n",
      "epoch:6 step:6415 [D loss: 0.583107, acc: 68.75%] [G loss: 2.407382]\n",
      "epoch:6 step:6416 [D loss: 0.709863, acc: 57.81%] [G loss: 2.301393]\n",
      "epoch:6 step:6417 [D loss: 0.565643, acc: 73.44%] [G loss: 2.368026]\n",
      "epoch:6 step:6418 [D loss: 0.587036, acc: 69.53%] [G loss: 2.436058]\n",
      "epoch:6 step:6419 [D loss: 0.566999, acc: 71.09%] [G loss: 2.310364]\n",
      "epoch:6 step:6420 [D loss: 0.580861, acc: 69.53%] [G loss: 2.427984]\n",
      "epoch:6 step:6421 [D loss: 0.623082, acc: 63.28%] [G loss: 2.394453]\n",
      "epoch:6 step:6422 [D loss: 0.683840, acc: 64.06%] [G loss: 2.183790]\n",
      "epoch:6 step:6423 [D loss: 0.636898, acc: 61.72%] [G loss: 2.287772]\n",
      "epoch:6 step:6424 [D loss: 0.602596, acc: 67.97%] [G loss: 2.166397]\n",
      "epoch:6 step:6425 [D loss: 0.581159, acc: 70.31%] [G loss: 2.454947]\n",
      "epoch:6 step:6426 [D loss: 0.613582, acc: 70.31%] [G loss: 2.428164]\n",
      "epoch:6 step:6427 [D loss: 0.545668, acc: 71.09%] [G loss: 2.563113]\n",
      "epoch:6 step:6428 [D loss: 0.549412, acc: 73.44%] [G loss: 2.674036]\n",
      "epoch:6 step:6429 [D loss: 0.599329, acc: 64.84%] [G loss: 2.182540]\n",
      "epoch:6 step:6430 [D loss: 0.575278, acc: 75.78%] [G loss: 2.308166]\n",
      "epoch:6 step:6431 [D loss: 0.626451, acc: 64.06%] [G loss: 2.492076]\n",
      "epoch:6 step:6432 [D loss: 0.620467, acc: 67.19%] [G loss: 2.263552]\n",
      "epoch:6 step:6433 [D loss: 0.646832, acc: 67.19%] [G loss: 2.294136]\n",
      "epoch:6 step:6434 [D loss: 0.689446, acc: 64.84%] [G loss: 2.161537]\n",
      "epoch:6 step:6435 [D loss: 0.661485, acc: 58.59%] [G loss: 2.156116]\n",
      "epoch:6 step:6436 [D loss: 0.665345, acc: 59.38%] [G loss: 2.256013]\n",
      "epoch:6 step:6437 [D loss: 0.631270, acc: 64.06%] [G loss: 2.465834]\n",
      "epoch:6 step:6438 [D loss: 0.663891, acc: 60.94%] [G loss: 2.492711]\n",
      "epoch:6 step:6439 [D loss: 0.656040, acc: 60.94%] [G loss: 2.191037]\n",
      "epoch:6 step:6440 [D loss: 0.615501, acc: 67.97%] [G loss: 1.971148]\n",
      "epoch:6 step:6441 [D loss: 0.656984, acc: 60.16%] [G loss: 2.269755]\n",
      "epoch:6 step:6442 [D loss: 0.681914, acc: 59.38%] [G loss: 2.101519]\n",
      "epoch:6 step:6443 [D loss: 0.668178, acc: 66.41%] [G loss: 2.197202]\n",
      "epoch:6 step:6444 [D loss: 0.616715, acc: 67.19%] [G loss: 2.303624]\n",
      "epoch:6 step:6445 [D loss: 0.612633, acc: 64.84%] [G loss: 2.298141]\n",
      "epoch:6 step:6446 [D loss: 0.662881, acc: 61.72%] [G loss: 2.103136]\n",
      "epoch:6 step:6447 [D loss: 0.555205, acc: 71.88%] [G loss: 2.321725]\n",
      "epoch:6 step:6448 [D loss: 0.629188, acc: 63.28%] [G loss: 2.146248]\n",
      "epoch:6 step:6449 [D loss: 0.692299, acc: 60.94%] [G loss: 2.209625]\n",
      "epoch:6 step:6450 [D loss: 0.611956, acc: 62.50%] [G loss: 1.963380]\n",
      "epoch:6 step:6451 [D loss: 0.627774, acc: 60.16%] [G loss: 2.107321]\n",
      "epoch:6 step:6452 [D loss: 0.662970, acc: 60.16%] [G loss: 2.247469]\n",
      "epoch:6 step:6453 [D loss: 0.573818, acc: 74.22%] [G loss: 2.404752]\n",
      "epoch:6 step:6454 [D loss: 0.592358, acc: 64.84%] [G loss: 2.385752]\n",
      "epoch:6 step:6455 [D loss: 0.611044, acc: 71.09%] [G loss: 2.075776]\n",
      "epoch:6 step:6456 [D loss: 0.640423, acc: 69.53%] [G loss: 2.173896]\n",
      "epoch:6 step:6457 [D loss: 0.658976, acc: 63.28%] [G loss: 2.127223]\n",
      "epoch:6 step:6458 [D loss: 0.624061, acc: 67.97%] [G loss: 2.126993]\n",
      "epoch:6 step:6459 [D loss: 0.575754, acc: 67.19%] [G loss: 2.313123]\n",
      "epoch:6 step:6460 [D loss: 0.608488, acc: 65.62%] [G loss: 2.180650]\n",
      "epoch:6 step:6461 [D loss: 0.583750, acc: 75.00%] [G loss: 2.367989]\n",
      "epoch:6 step:6462 [D loss: 0.620648, acc: 63.28%] [G loss: 2.216106]\n",
      "epoch:6 step:6463 [D loss: 0.613711, acc: 67.97%] [G loss: 2.293110]\n",
      "epoch:6 step:6464 [D loss: 0.573109, acc: 75.78%] [G loss: 2.392893]\n",
      "epoch:6 step:6465 [D loss: 0.567577, acc: 71.88%] [G loss: 2.379490]\n",
      "epoch:6 step:6466 [D loss: 0.604689, acc: 67.97%] [G loss: 2.271800]\n",
      "epoch:6 step:6467 [D loss: 0.609062, acc: 62.50%] [G loss: 2.371119]\n",
      "epoch:6 step:6468 [D loss: 0.567019, acc: 75.00%] [G loss: 2.251343]\n",
      "epoch:6 step:6469 [D loss: 0.614805, acc: 70.31%] [G loss: 2.222381]\n",
      "epoch:6 step:6470 [D loss: 0.595243, acc: 67.19%] [G loss: 2.318692]\n",
      "epoch:6 step:6471 [D loss: 0.610892, acc: 69.53%] [G loss: 2.547589]\n",
      "epoch:6 step:6472 [D loss: 0.646029, acc: 65.62%] [G loss: 2.409726]\n",
      "epoch:6 step:6473 [D loss: 0.589403, acc: 67.97%] [G loss: 2.209492]\n",
      "epoch:6 step:6474 [D loss: 0.589144, acc: 63.28%] [G loss: 2.524354]\n",
      "epoch:6 step:6475 [D loss: 0.636451, acc: 63.28%] [G loss: 2.412302]\n",
      "epoch:6 step:6476 [D loss: 0.557148, acc: 75.00%] [G loss: 2.131727]\n",
      "epoch:6 step:6477 [D loss: 0.678795, acc: 67.19%] [G loss: 2.196822]\n",
      "epoch:6 step:6478 [D loss: 0.672260, acc: 66.41%] [G loss: 2.059415]\n",
      "epoch:6 step:6479 [D loss: 0.571521, acc: 71.88%] [G loss: 2.260401]\n",
      "epoch:6 step:6480 [D loss: 0.723744, acc: 60.16%] [G loss: 2.319453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6481 [D loss: 0.670957, acc: 64.06%] [G loss: 2.281958]\n",
      "epoch:6 step:6482 [D loss: 0.639685, acc: 63.28%] [G loss: 2.456146]\n",
      "epoch:6 step:6483 [D loss: 0.638365, acc: 59.38%] [G loss: 2.074976]\n",
      "epoch:6 step:6484 [D loss: 0.615967, acc: 71.09%] [G loss: 2.161405]\n",
      "epoch:6 step:6485 [D loss: 0.614436, acc: 67.19%] [G loss: 2.223492]\n",
      "epoch:6 step:6486 [D loss: 0.611901, acc: 67.97%] [G loss: 2.204336]\n",
      "epoch:6 step:6487 [D loss: 0.648115, acc: 60.16%] [G loss: 2.117106]\n",
      "epoch:6 step:6488 [D loss: 0.647956, acc: 65.62%] [G loss: 2.242353]\n",
      "epoch:6 step:6489 [D loss: 0.643693, acc: 60.16%] [G loss: 2.011741]\n",
      "epoch:6 step:6490 [D loss: 0.612210, acc: 65.62%] [G loss: 2.321332]\n",
      "epoch:6 step:6491 [D loss: 0.660916, acc: 64.84%] [G loss: 1.969609]\n",
      "epoch:6 step:6492 [D loss: 0.631179, acc: 61.72%] [G loss: 2.172890]\n",
      "epoch:6 step:6493 [D loss: 0.539852, acc: 71.88%] [G loss: 2.292075]\n",
      "epoch:6 step:6494 [D loss: 0.620069, acc: 65.62%] [G loss: 2.215241]\n",
      "epoch:6 step:6495 [D loss: 0.601067, acc: 66.41%] [G loss: 2.032679]\n",
      "epoch:6 step:6496 [D loss: 0.651519, acc: 57.03%] [G loss: 2.198808]\n",
      "epoch:6 step:6497 [D loss: 0.569688, acc: 73.44%] [G loss: 2.525613]\n",
      "epoch:6 step:6498 [D loss: 0.555964, acc: 74.22%] [G loss: 2.281040]\n",
      "epoch:6 step:6499 [D loss: 0.628000, acc: 60.16%] [G loss: 2.369280]\n",
      "epoch:6 step:6500 [D loss: 0.614498, acc: 64.84%] [G loss: 2.291617]\n",
      "epoch:6 step:6501 [D loss: 0.634901, acc: 63.28%] [G loss: 2.302665]\n",
      "epoch:6 step:6502 [D loss: 0.629184, acc: 64.06%] [G loss: 2.198391]\n",
      "epoch:6 step:6503 [D loss: 0.633971, acc: 63.28%] [G loss: 2.164544]\n",
      "epoch:6 step:6504 [D loss: 0.661752, acc: 65.62%] [G loss: 2.101075]\n",
      "epoch:6 step:6505 [D loss: 0.638647, acc: 66.41%] [G loss: 2.306373]\n",
      "epoch:6 step:6506 [D loss: 0.494843, acc: 74.22%] [G loss: 2.368623]\n",
      "epoch:6 step:6507 [D loss: 0.587010, acc: 63.28%] [G loss: 2.551307]\n",
      "epoch:6 step:6508 [D loss: 0.536092, acc: 71.88%] [G loss: 2.805789]\n",
      "epoch:6 step:6509 [D loss: 0.569158, acc: 70.31%] [G loss: 2.360708]\n",
      "epoch:6 step:6510 [D loss: 0.575686, acc: 71.09%] [G loss: 2.621967]\n",
      "epoch:6 step:6511 [D loss: 0.585637, acc: 67.19%] [G loss: 2.522607]\n",
      "epoch:6 step:6512 [D loss: 0.603593, acc: 61.72%] [G loss: 2.640816]\n",
      "epoch:6 step:6513 [D loss: 0.683748, acc: 60.16%] [G loss: 2.170795]\n",
      "epoch:6 step:6514 [D loss: 0.772194, acc: 53.91%] [G loss: 2.009100]\n",
      "epoch:6 step:6515 [D loss: 0.618898, acc: 68.75%] [G loss: 2.562156]\n",
      "epoch:6 step:6516 [D loss: 0.556024, acc: 74.22%] [G loss: 2.569146]\n",
      "epoch:6 step:6517 [D loss: 0.652186, acc: 64.84%] [G loss: 2.392907]\n",
      "epoch:6 step:6518 [D loss: 0.570434, acc: 69.53%] [G loss: 2.276974]\n",
      "epoch:6 step:6519 [D loss: 0.648840, acc: 64.06%] [G loss: 2.439409]\n",
      "epoch:6 step:6520 [D loss: 0.608078, acc: 64.84%] [G loss: 2.182884]\n",
      "epoch:6 step:6521 [D loss: 0.586737, acc: 71.09%] [G loss: 2.869634]\n",
      "epoch:6 step:6522 [D loss: 0.595805, acc: 64.84%] [G loss: 2.223535]\n",
      "epoch:6 step:6523 [D loss: 0.617404, acc: 65.62%] [G loss: 2.400339]\n",
      "epoch:6 step:6524 [D loss: 0.622812, acc: 67.19%] [G loss: 2.421878]\n",
      "epoch:6 step:6525 [D loss: 0.612188, acc: 66.41%] [G loss: 2.098900]\n",
      "epoch:6 step:6526 [D loss: 0.622829, acc: 65.62%] [G loss: 2.149131]\n",
      "epoch:6 step:6527 [D loss: 0.706055, acc: 58.59%] [G loss: 2.442216]\n",
      "epoch:6 step:6528 [D loss: 0.666295, acc: 61.72%] [G loss: 2.242574]\n",
      "epoch:6 step:6529 [D loss: 0.618443, acc: 65.62%] [G loss: 2.278364]\n",
      "epoch:6 step:6530 [D loss: 0.602534, acc: 70.31%] [G loss: 2.143059]\n",
      "epoch:6 step:6531 [D loss: 0.565367, acc: 75.78%] [G loss: 2.468273]\n",
      "epoch:6 step:6532 [D loss: 0.650648, acc: 60.16%] [G loss: 2.153651]\n",
      "epoch:6 step:6533 [D loss: 0.552052, acc: 65.62%] [G loss: 2.279676]\n",
      "epoch:6 step:6534 [D loss: 0.566120, acc: 71.09%] [G loss: 2.949817]\n",
      "epoch:6 step:6535 [D loss: 0.626984, acc: 61.72%] [G loss: 2.357244]\n",
      "epoch:6 step:6536 [D loss: 0.644513, acc: 64.06%] [G loss: 2.345303]\n",
      "epoch:6 step:6537 [D loss: 0.613600, acc: 63.28%] [G loss: 2.468651]\n",
      "epoch:6 step:6538 [D loss: 0.642719, acc: 61.72%] [G loss: 2.143233]\n",
      "epoch:6 step:6539 [D loss: 0.669030, acc: 59.38%] [G loss: 2.280184]\n",
      "epoch:6 step:6540 [D loss: 0.526789, acc: 76.56%] [G loss: 2.567905]\n",
      "epoch:6 step:6541 [D loss: 0.602612, acc: 71.09%] [G loss: 2.483137]\n",
      "epoch:6 step:6542 [D loss: 0.673088, acc: 64.84%] [G loss: 2.207888]\n",
      "epoch:6 step:6543 [D loss: 0.647400, acc: 65.62%] [G loss: 2.300431]\n",
      "epoch:6 step:6544 [D loss: 0.592143, acc: 67.19%] [G loss: 2.290742]\n",
      "epoch:6 step:6545 [D loss: 0.530777, acc: 78.12%] [G loss: 2.671140]\n",
      "epoch:6 step:6546 [D loss: 0.562048, acc: 70.31%] [G loss: 2.500106]\n",
      "epoch:6 step:6547 [D loss: 0.484750, acc: 82.81%] [G loss: 2.850789]\n",
      "epoch:6 step:6548 [D loss: 0.560842, acc: 66.41%] [G loss: 2.535776]\n",
      "epoch:6 step:6549 [D loss: 0.584069, acc: 72.66%] [G loss: 3.143257]\n",
      "epoch:6 step:6550 [D loss: 0.814126, acc: 61.72%] [G loss: 2.371018]\n",
      "epoch:6 step:6551 [D loss: 0.655291, acc: 64.06%] [G loss: 2.673167]\n",
      "epoch:6 step:6552 [D loss: 0.549970, acc: 75.78%] [G loss: 2.349893]\n",
      "epoch:6 step:6553 [D loss: 0.593343, acc: 71.09%] [G loss: 2.294088]\n",
      "epoch:6 step:6554 [D loss: 0.666558, acc: 62.50%] [G loss: 2.351061]\n",
      "epoch:6 step:6555 [D loss: 0.597980, acc: 71.88%] [G loss: 2.501144]\n",
      "epoch:6 step:6556 [D loss: 0.605502, acc: 66.41%] [G loss: 2.395838]\n",
      "epoch:6 step:6557 [D loss: 0.618632, acc: 65.62%] [G loss: 2.125002]\n",
      "epoch:6 step:6558 [D loss: 0.569093, acc: 68.75%] [G loss: 2.571144]\n",
      "epoch:6 step:6559 [D loss: 0.521606, acc: 75.00%] [G loss: 3.321765]\n",
      "epoch:7 step:6560 [D loss: 0.626879, acc: 57.81%] [G loss: 2.352070]\n",
      "epoch:7 step:6561 [D loss: 0.606901, acc: 67.19%] [G loss: 2.416837]\n",
      "epoch:7 step:6562 [D loss: 0.600596, acc: 63.28%] [G loss: 2.459946]\n",
      "epoch:7 step:6563 [D loss: 0.634755, acc: 64.06%] [G loss: 2.349370]\n",
      "epoch:7 step:6564 [D loss: 0.649252, acc: 57.81%] [G loss: 2.264454]\n",
      "epoch:7 step:6565 [D loss: 0.591488, acc: 67.19%] [G loss: 2.375659]\n",
      "epoch:7 step:6566 [D loss: 0.609569, acc: 70.31%] [G loss: 2.242492]\n",
      "epoch:7 step:6567 [D loss: 0.553388, acc: 69.53%] [G loss: 2.287200]\n",
      "epoch:7 step:6568 [D loss: 0.617771, acc: 65.62%] [G loss: 2.460687]\n",
      "epoch:7 step:6569 [D loss: 0.543973, acc: 71.88%] [G loss: 2.592338]\n",
      "epoch:7 step:6570 [D loss: 0.640876, acc: 64.84%] [G loss: 2.230742]\n",
      "epoch:7 step:6571 [D loss: 0.617219, acc: 65.62%] [G loss: 2.374102]\n",
      "epoch:7 step:6572 [D loss: 0.594162, acc: 65.62%] [G loss: 2.582082]\n",
      "epoch:7 step:6573 [D loss: 0.561375, acc: 74.22%] [G loss: 2.392961]\n",
      "epoch:7 step:6574 [D loss: 0.648317, acc: 66.41%] [G loss: 2.578246]\n",
      "epoch:7 step:6575 [D loss: 0.601084, acc: 63.28%] [G loss: 2.403019]\n",
      "epoch:7 step:6576 [D loss: 0.605226, acc: 65.62%] [G loss: 2.324614]\n",
      "epoch:7 step:6577 [D loss: 0.688333, acc: 60.94%] [G loss: 2.057730]\n",
      "epoch:7 step:6578 [D loss: 0.632890, acc: 65.62%] [G loss: 2.064021]\n",
      "epoch:7 step:6579 [D loss: 0.631391, acc: 65.62%] [G loss: 2.065816]\n",
      "epoch:7 step:6580 [D loss: 0.579894, acc: 67.97%] [G loss: 2.243703]\n",
      "epoch:7 step:6581 [D loss: 0.602132, acc: 70.31%] [G loss: 2.305561]\n",
      "epoch:7 step:6582 [D loss: 0.587257, acc: 67.19%] [G loss: 2.197713]\n",
      "epoch:7 step:6583 [D loss: 0.599152, acc: 70.31%] [G loss: 2.473995]\n",
      "epoch:7 step:6584 [D loss: 0.545099, acc: 74.22%] [G loss: 2.387081]\n",
      "epoch:7 step:6585 [D loss: 0.597863, acc: 68.75%] [G loss: 2.121242]\n",
      "epoch:7 step:6586 [D loss: 0.592903, acc: 69.53%] [G loss: 2.440467]\n",
      "epoch:7 step:6587 [D loss: 0.583690, acc: 66.41%] [G loss: 2.440621]\n",
      "epoch:7 step:6588 [D loss: 0.597970, acc: 67.97%] [G loss: 2.575637]\n",
      "epoch:7 step:6589 [D loss: 0.607127, acc: 62.50%] [G loss: 2.321910]\n",
      "epoch:7 step:6590 [D loss: 0.606173, acc: 65.62%] [G loss: 2.146258]\n",
      "epoch:7 step:6591 [D loss: 0.642327, acc: 64.06%] [G loss: 2.098708]\n",
      "epoch:7 step:6592 [D loss: 0.622985, acc: 65.62%] [G loss: 2.329473]\n",
      "epoch:7 step:6593 [D loss: 0.552514, acc: 70.31%] [G loss: 2.426062]\n",
      "epoch:7 step:6594 [D loss: 0.617673, acc: 67.97%] [G loss: 2.336337]\n",
      "epoch:7 step:6595 [D loss: 0.566050, acc: 74.22%] [G loss: 2.671192]\n",
      "epoch:7 step:6596 [D loss: 0.613187, acc: 67.19%] [G loss: 2.484163]\n",
      "epoch:7 step:6597 [D loss: 0.572995, acc: 68.75%] [G loss: 2.570421]\n",
      "epoch:7 step:6598 [D loss: 0.547198, acc: 72.66%] [G loss: 2.533875]\n",
      "epoch:7 step:6599 [D loss: 0.569014, acc: 67.97%] [G loss: 2.539528]\n",
      "epoch:7 step:6600 [D loss: 0.643554, acc: 65.62%] [G loss: 2.189961]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.234924\n",
      "FID: 32.961727\n",
      "0 = 13.019263269424446\n",
      "1 = 0.08100466735956147\n",
      "2 = 0.9236999750137329\n",
      "3 = 0.9290000200271606\n",
      "4 = 0.91839998960495\n",
      "5 = 0.9192559123039246\n",
      "6 = 0.9290000200271606\n",
      "7 = 8.09251796704532\n",
      "8 = 0.11361225045733181\n",
      "9 = 0.7943000197410583\n",
      "10 = 0.8008000254631042\n",
      "11 = 0.7878000140190125\n",
      "12 = 0.7905231714248657\n",
      "13 = 0.8008000254631042\n",
      "14 = 6.2349443435668945\n",
      "15 = 9.345274925231934\n",
      "16 = 0.16538582742214203\n",
      "17 = 6.234923839569092\n",
      "18 = 32.961727142333984\n",
      "epoch:7 step:6601 [D loss: 0.624093, acc: 66.41%] [G loss: 2.293472]\n",
      "epoch:7 step:6602 [D loss: 0.603368, acc: 67.19%] [G loss: 2.206380]\n",
      "epoch:7 step:6603 [D loss: 0.653087, acc: 63.28%] [G loss: 2.049173]\n",
      "epoch:7 step:6604 [D loss: 0.600527, acc: 71.09%] [G loss: 2.159847]\n",
      "epoch:7 step:6605 [D loss: 0.660154, acc: 61.72%] [G loss: 2.255031]\n",
      "epoch:7 step:6606 [D loss: 0.605242, acc: 71.88%] [G loss: 2.262381]\n",
      "epoch:7 step:6607 [D loss: 0.563296, acc: 65.62%] [G loss: 2.475375]\n",
      "epoch:7 step:6608 [D loss: 0.642669, acc: 64.06%] [G loss: 2.367898]\n",
      "epoch:7 step:6609 [D loss: 0.589433, acc: 68.75%] [G loss: 2.359264]\n",
      "epoch:7 step:6610 [D loss: 0.559515, acc: 75.00%] [G loss: 2.340940]\n",
      "epoch:7 step:6611 [D loss: 0.595165, acc: 69.53%] [G loss: 2.458305]\n",
      "epoch:7 step:6612 [D loss: 0.615044, acc: 67.19%] [G loss: 2.329026]\n",
      "epoch:7 step:6613 [D loss: 0.534028, acc: 73.44%] [G loss: 2.272926]\n",
      "epoch:7 step:6614 [D loss: 0.580911, acc: 69.53%] [G loss: 2.579624]\n",
      "epoch:7 step:6615 [D loss: 0.709194, acc: 58.59%] [G loss: 2.308678]\n",
      "epoch:7 step:6616 [D loss: 0.623372, acc: 64.06%] [G loss: 2.328007]\n",
      "epoch:7 step:6617 [D loss: 0.651029, acc: 65.62%] [G loss: 2.272381]\n",
      "epoch:7 step:6618 [D loss: 0.666355, acc: 58.59%] [G loss: 2.187657]\n",
      "epoch:7 step:6619 [D loss: 0.532917, acc: 75.78%] [G loss: 2.365156]\n",
      "epoch:7 step:6620 [D loss: 0.609061, acc: 64.06%] [G loss: 2.260380]\n",
      "epoch:7 step:6621 [D loss: 0.625575, acc: 66.41%] [G loss: 2.480388]\n",
      "epoch:7 step:6622 [D loss: 0.531550, acc: 75.00%] [G loss: 2.281851]\n",
      "epoch:7 step:6623 [D loss: 0.683600, acc: 64.06%] [G loss: 2.268390]\n",
      "epoch:7 step:6624 [D loss: 0.665744, acc: 61.72%] [G loss: 2.131790]\n",
      "epoch:7 step:6625 [D loss: 0.585277, acc: 69.53%] [G loss: 2.305019]\n",
      "epoch:7 step:6626 [D loss: 0.607846, acc: 70.31%] [G loss: 2.287243]\n",
      "epoch:7 step:6627 [D loss: 0.554115, acc: 72.66%] [G loss: 2.339843]\n",
      "epoch:7 step:6628 [D loss: 0.622699, acc: 66.41%] [G loss: 2.262156]\n",
      "epoch:7 step:6629 [D loss: 0.586288, acc: 69.53%] [G loss: 2.414617]\n",
      "epoch:7 step:6630 [D loss: 0.663802, acc: 60.94%] [G loss: 2.339877]\n",
      "epoch:7 step:6631 [D loss: 0.590248, acc: 67.97%] [G loss: 2.404813]\n",
      "epoch:7 step:6632 [D loss: 0.649527, acc: 62.50%] [G loss: 2.209586]\n",
      "epoch:7 step:6633 [D loss: 0.549717, acc: 70.31%] [G loss: 2.584493]\n",
      "epoch:7 step:6634 [D loss: 0.601824, acc: 69.53%] [G loss: 2.637598]\n",
      "epoch:7 step:6635 [D loss: 0.554892, acc: 70.31%] [G loss: 2.569388]\n",
      "epoch:7 step:6636 [D loss: 0.533553, acc: 75.00%] [G loss: 2.736102]\n",
      "epoch:7 step:6637 [D loss: 0.653398, acc: 64.84%] [G loss: 2.147706]\n",
      "epoch:7 step:6638 [D loss: 0.602297, acc: 65.62%] [G loss: 2.233412]\n",
      "epoch:7 step:6639 [D loss: 0.735842, acc: 50.00%] [G loss: 2.197984]\n",
      "epoch:7 step:6640 [D loss: 0.605367, acc: 66.41%] [G loss: 2.193578]\n",
      "epoch:7 step:6641 [D loss: 0.620378, acc: 62.50%] [G loss: 2.220807]\n",
      "epoch:7 step:6642 [D loss: 0.564743, acc: 71.88%] [G loss: 2.472864]\n",
      "epoch:7 step:6643 [D loss: 0.607677, acc: 65.62%] [G loss: 2.528267]\n",
      "epoch:7 step:6644 [D loss: 0.595534, acc: 68.75%] [G loss: 2.263401]\n",
      "epoch:7 step:6645 [D loss: 0.624513, acc: 62.50%] [G loss: 2.284709]\n",
      "epoch:7 step:6646 [D loss: 0.620175, acc: 67.97%] [G loss: 2.266321]\n",
      "epoch:7 step:6647 [D loss: 0.653369, acc: 64.84%] [G loss: 2.215719]\n",
      "epoch:7 step:6648 [D loss: 0.682165, acc: 64.06%] [G loss: 2.404000]\n",
      "epoch:7 step:6649 [D loss: 0.669252, acc: 63.28%] [G loss: 2.340256]\n",
      "epoch:7 step:6650 [D loss: 0.618498, acc: 68.75%] [G loss: 2.280429]\n",
      "epoch:7 step:6651 [D loss: 0.587907, acc: 72.66%] [G loss: 2.365167]\n",
      "epoch:7 step:6652 [D loss: 0.607166, acc: 69.53%] [G loss: 2.450019]\n",
      "epoch:7 step:6653 [D loss: 0.595286, acc: 70.31%] [G loss: 2.388880]\n",
      "epoch:7 step:6654 [D loss: 0.652766, acc: 65.62%] [G loss: 2.188374]\n",
      "epoch:7 step:6655 [D loss: 0.533270, acc: 75.00%] [G loss: 2.275019]\n",
      "epoch:7 step:6656 [D loss: 0.632447, acc: 59.38%] [G loss: 2.433740]\n",
      "epoch:7 step:6657 [D loss: 0.611549, acc: 66.41%] [G loss: 2.193746]\n",
      "epoch:7 step:6658 [D loss: 0.591320, acc: 70.31%] [G loss: 2.295395]\n",
      "epoch:7 step:6659 [D loss: 0.598481, acc: 68.75%] [G loss: 2.584850]\n",
      "epoch:7 step:6660 [D loss: 0.554141, acc: 73.44%] [G loss: 2.246302]\n",
      "epoch:7 step:6661 [D loss: 0.595862, acc: 64.84%] [G loss: 2.383664]\n",
      "epoch:7 step:6662 [D loss: 0.587702, acc: 68.75%] [G loss: 2.501347]\n",
      "epoch:7 step:6663 [D loss: 0.629779, acc: 64.06%] [G loss: 2.340829]\n",
      "epoch:7 step:6664 [D loss: 0.615158, acc: 69.53%] [G loss: 2.409098]\n",
      "epoch:7 step:6665 [D loss: 0.640374, acc: 61.72%] [G loss: 2.265434]\n",
      "epoch:7 step:6666 [D loss: 0.631415, acc: 67.19%] [G loss: 2.138604]\n",
      "epoch:7 step:6667 [D loss: 0.697861, acc: 60.16%] [G loss: 2.071063]\n",
      "epoch:7 step:6668 [D loss: 0.654440, acc: 65.62%] [G loss: 2.145256]\n",
      "epoch:7 step:6669 [D loss: 0.608461, acc: 70.31%] [G loss: 2.296637]\n",
      "epoch:7 step:6670 [D loss: 0.629699, acc: 64.84%] [G loss: 2.389597]\n",
      "epoch:7 step:6671 [D loss: 0.582207, acc: 68.75%] [G loss: 2.602526]\n",
      "epoch:7 step:6672 [D loss: 0.604193, acc: 63.28%] [G loss: 2.524085]\n",
      "epoch:7 step:6673 [D loss: 0.670689, acc: 64.84%] [G loss: 2.399260]\n",
      "epoch:7 step:6674 [D loss: 0.612528, acc: 64.84%] [G loss: 2.199685]\n",
      "epoch:7 step:6675 [D loss: 0.624377, acc: 64.84%] [G loss: 2.228143]\n",
      "epoch:7 step:6676 [D loss: 0.581365, acc: 70.31%] [G loss: 2.305296]\n",
      "epoch:7 step:6677 [D loss: 0.693777, acc: 61.72%] [G loss: 2.202722]\n",
      "epoch:7 step:6678 [D loss: 0.536851, acc: 73.44%] [G loss: 2.622821]\n",
      "epoch:7 step:6679 [D loss: 0.633352, acc: 65.62%] [G loss: 2.314133]\n",
      "epoch:7 step:6680 [D loss: 0.643242, acc: 63.28%] [G loss: 2.298716]\n",
      "epoch:7 step:6681 [D loss: 0.617653, acc: 66.41%] [G loss: 2.462234]\n",
      "epoch:7 step:6682 [D loss: 0.688009, acc: 59.38%] [G loss: 2.387052]\n",
      "epoch:7 step:6683 [D loss: 0.586385, acc: 67.97%] [G loss: 2.051477]\n",
      "epoch:7 step:6684 [D loss: 0.724042, acc: 55.47%] [G loss: 2.059086]\n",
      "epoch:7 step:6685 [D loss: 0.639673, acc: 64.84%] [G loss: 2.156078]\n",
      "epoch:7 step:6686 [D loss: 0.618335, acc: 67.97%] [G loss: 2.257407]\n",
      "epoch:7 step:6687 [D loss: 0.626770, acc: 70.31%] [G loss: 2.190289]\n",
      "epoch:7 step:6688 [D loss: 0.709738, acc: 54.69%] [G loss: 2.116458]\n",
      "epoch:7 step:6689 [D loss: 0.648774, acc: 67.19%] [G loss: 2.510927]\n",
      "epoch:7 step:6690 [D loss: 0.630761, acc: 67.19%] [G loss: 2.152973]\n",
      "epoch:7 step:6691 [D loss: 0.630132, acc: 67.97%] [G loss: 2.177187]\n",
      "epoch:7 step:6692 [D loss: 0.668932, acc: 58.59%] [G loss: 1.999814]\n",
      "epoch:7 step:6693 [D loss: 0.622466, acc: 67.97%] [G loss: 2.066463]\n",
      "epoch:7 step:6694 [D loss: 0.657010, acc: 55.47%] [G loss: 2.089465]\n",
      "epoch:7 step:6695 [D loss: 0.586284, acc: 69.53%] [G loss: 2.079820]\n",
      "epoch:7 step:6696 [D loss: 0.568328, acc: 68.75%] [G loss: 2.314795]\n",
      "epoch:7 step:6697 [D loss: 0.661195, acc: 65.62%] [G loss: 2.073380]\n",
      "epoch:7 step:6698 [D loss: 0.593512, acc: 70.31%] [G loss: 2.319604]\n",
      "epoch:7 step:6699 [D loss: 0.599951, acc: 67.19%] [G loss: 2.136617]\n",
      "epoch:7 step:6700 [D loss: 0.657507, acc: 62.50%] [G loss: 2.115303]\n",
      "epoch:7 step:6701 [D loss: 0.654047, acc: 64.06%] [G loss: 2.132261]\n",
      "epoch:7 step:6702 [D loss: 0.567688, acc: 73.44%] [G loss: 2.326982]\n",
      "epoch:7 step:6703 [D loss: 0.564156, acc: 71.09%] [G loss: 2.515433]\n",
      "epoch:7 step:6704 [D loss: 0.606257, acc: 69.53%] [G loss: 2.203488]\n",
      "epoch:7 step:6705 [D loss: 0.607302, acc: 63.28%] [G loss: 2.256411]\n",
      "epoch:7 step:6706 [D loss: 0.671194, acc: 60.16%] [G loss: 1.996101]\n",
      "epoch:7 step:6707 [D loss: 0.628487, acc: 62.50%] [G loss: 2.165920]\n",
      "epoch:7 step:6708 [D loss: 0.598876, acc: 62.50%] [G loss: 2.352009]\n",
      "epoch:7 step:6709 [D loss: 0.612643, acc: 65.62%] [G loss: 2.213705]\n",
      "epoch:7 step:6710 [D loss: 0.598935, acc: 71.88%] [G loss: 2.689290]\n",
      "epoch:7 step:6711 [D loss: 0.608198, acc: 64.06%] [G loss: 2.484035]\n",
      "epoch:7 step:6712 [D loss: 0.672867, acc: 58.59%] [G loss: 2.148803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6713 [D loss: 0.593433, acc: 74.22%] [G loss: 2.387141]\n",
      "epoch:7 step:6714 [D loss: 0.580178, acc: 69.53%] [G loss: 2.413524]\n",
      "epoch:7 step:6715 [D loss: 0.671632, acc: 62.50%] [G loss: 2.545276]\n",
      "epoch:7 step:6716 [D loss: 0.569339, acc: 67.97%] [G loss: 2.178398]\n",
      "epoch:7 step:6717 [D loss: 0.613849, acc: 69.53%] [G loss: 2.108805]\n",
      "epoch:7 step:6718 [D loss: 0.688091, acc: 67.19%] [G loss: 2.162702]\n",
      "epoch:7 step:6719 [D loss: 0.599061, acc: 67.97%] [G loss: 1.945343]\n",
      "epoch:7 step:6720 [D loss: 0.612956, acc: 65.62%] [G loss: 2.249886]\n",
      "epoch:7 step:6721 [D loss: 0.578036, acc: 66.41%] [G loss: 2.385778]\n",
      "epoch:7 step:6722 [D loss: 0.625482, acc: 64.06%] [G loss: 2.205509]\n",
      "epoch:7 step:6723 [D loss: 0.639713, acc: 67.97%] [G loss: 2.071130]\n",
      "epoch:7 step:6724 [D loss: 0.647093, acc: 68.75%] [G loss: 2.126710]\n",
      "epoch:7 step:6725 [D loss: 0.599409, acc: 64.06%] [G loss: 2.344883]\n",
      "epoch:7 step:6726 [D loss: 0.619562, acc: 67.97%] [G loss: 2.271192]\n",
      "epoch:7 step:6727 [D loss: 0.575201, acc: 68.75%] [G loss: 2.268395]\n",
      "epoch:7 step:6728 [D loss: 0.685329, acc: 57.81%] [G loss: 2.140997]\n",
      "epoch:7 step:6729 [D loss: 0.672176, acc: 60.16%] [G loss: 2.174047]\n",
      "epoch:7 step:6730 [D loss: 0.705566, acc: 60.94%] [G loss: 2.159756]\n",
      "epoch:7 step:6731 [D loss: 0.646772, acc: 63.28%] [G loss: 2.137662]\n",
      "epoch:7 step:6732 [D loss: 0.610581, acc: 65.62%] [G loss: 2.107284]\n",
      "epoch:7 step:6733 [D loss: 0.571953, acc: 69.53%] [G loss: 2.211436]\n",
      "epoch:7 step:6734 [D loss: 0.614030, acc: 65.62%] [G loss: 2.188555]\n",
      "epoch:7 step:6735 [D loss: 0.626741, acc: 60.94%] [G loss: 2.113644]\n",
      "epoch:7 step:6736 [D loss: 0.634978, acc: 66.41%] [G loss: 1.955638]\n",
      "epoch:7 step:6737 [D loss: 0.627797, acc: 65.62%] [G loss: 2.221838]\n",
      "epoch:7 step:6738 [D loss: 0.594640, acc: 70.31%] [G loss: 2.251701]\n",
      "epoch:7 step:6739 [D loss: 0.633078, acc: 63.28%] [G loss: 2.313522]\n",
      "epoch:7 step:6740 [D loss: 0.661300, acc: 68.75%] [G loss: 2.173573]\n",
      "epoch:7 step:6741 [D loss: 0.639522, acc: 67.19%] [G loss: 2.222337]\n",
      "epoch:7 step:6742 [D loss: 0.618887, acc: 66.41%] [G loss: 2.263094]\n",
      "epoch:7 step:6743 [D loss: 0.593182, acc: 68.75%] [G loss: 2.170442]\n",
      "epoch:7 step:6744 [D loss: 0.626299, acc: 65.62%] [G loss: 2.292746]\n",
      "epoch:7 step:6745 [D loss: 0.596897, acc: 67.19%] [G loss: 2.181678]\n",
      "epoch:7 step:6746 [D loss: 0.671670, acc: 60.94%] [G loss: 2.096489]\n",
      "epoch:7 step:6747 [D loss: 0.620808, acc: 71.09%] [G loss: 2.290201]\n",
      "epoch:7 step:6748 [D loss: 0.689558, acc: 59.38%] [G loss: 1.993239]\n",
      "epoch:7 step:6749 [D loss: 0.615101, acc: 65.62%] [G loss: 2.220612]\n",
      "epoch:7 step:6750 [D loss: 0.580217, acc: 67.97%] [G loss: 2.200293]\n",
      "epoch:7 step:6751 [D loss: 0.690135, acc: 57.03%] [G loss: 2.238074]\n",
      "epoch:7 step:6752 [D loss: 0.587908, acc: 67.97%] [G loss: 2.202629]\n",
      "epoch:7 step:6753 [D loss: 0.651530, acc: 57.81%] [G loss: 2.588709]\n",
      "epoch:7 step:6754 [D loss: 0.630597, acc: 64.84%] [G loss: 2.242610]\n",
      "epoch:7 step:6755 [D loss: 0.652056, acc: 61.72%] [G loss: 2.207683]\n",
      "epoch:7 step:6756 [D loss: 0.619452, acc: 69.53%] [G loss: 2.489156]\n",
      "epoch:7 step:6757 [D loss: 0.553696, acc: 69.53%] [G loss: 2.440381]\n",
      "epoch:7 step:6758 [D loss: 0.601373, acc: 64.06%] [G loss: 2.128913]\n",
      "epoch:7 step:6759 [D loss: 0.681310, acc: 64.84%] [G loss: 2.017379]\n",
      "epoch:7 step:6760 [D loss: 0.650635, acc: 57.03%] [G loss: 2.134892]\n",
      "epoch:7 step:6761 [D loss: 0.566740, acc: 73.44%] [G loss: 2.140915]\n",
      "epoch:7 step:6762 [D loss: 0.687268, acc: 58.59%] [G loss: 1.973956]\n",
      "epoch:7 step:6763 [D loss: 0.632777, acc: 64.84%] [G loss: 2.067947]\n",
      "epoch:7 step:6764 [D loss: 0.644453, acc: 60.94%] [G loss: 2.251997]\n",
      "epoch:7 step:6765 [D loss: 0.606158, acc: 65.62%] [G loss: 2.407959]\n",
      "epoch:7 step:6766 [D loss: 0.577496, acc: 74.22%] [G loss: 2.548944]\n",
      "epoch:7 step:6767 [D loss: 0.540532, acc: 80.47%] [G loss: 2.412348]\n",
      "epoch:7 step:6768 [D loss: 0.558939, acc: 71.88%] [G loss: 2.456160]\n",
      "epoch:7 step:6769 [D loss: 0.613747, acc: 62.50%] [G loss: 2.172713]\n",
      "epoch:7 step:6770 [D loss: 0.631903, acc: 64.06%] [G loss: 2.028057]\n",
      "epoch:7 step:6771 [D loss: 0.607976, acc: 70.31%] [G loss: 2.250459]\n",
      "epoch:7 step:6772 [D loss: 0.657270, acc: 64.06%] [G loss: 2.161992]\n",
      "epoch:7 step:6773 [D loss: 0.657809, acc: 66.41%] [G loss: 1.935710]\n",
      "epoch:7 step:6774 [D loss: 0.682577, acc: 57.03%] [G loss: 2.138875]\n",
      "epoch:7 step:6775 [D loss: 0.655380, acc: 64.84%] [G loss: 2.207583]\n",
      "epoch:7 step:6776 [D loss: 0.630595, acc: 65.62%] [G loss: 2.259736]\n",
      "epoch:7 step:6777 [D loss: 0.555215, acc: 71.09%] [G loss: 2.470587]\n",
      "epoch:7 step:6778 [D loss: 0.526953, acc: 72.66%] [G loss: 2.499204]\n",
      "epoch:7 step:6779 [D loss: 0.631330, acc: 67.97%] [G loss: 2.248764]\n",
      "epoch:7 step:6780 [D loss: 0.607433, acc: 64.84%] [G loss: 2.408880]\n",
      "epoch:7 step:6781 [D loss: 0.624314, acc: 67.19%] [G loss: 2.472322]\n",
      "epoch:7 step:6782 [D loss: 0.553045, acc: 67.97%] [G loss: 2.383862]\n",
      "epoch:7 step:6783 [D loss: 0.612849, acc: 67.19%] [G loss: 2.217443]\n",
      "epoch:7 step:6784 [D loss: 0.681367, acc: 61.72%] [G loss: 2.189980]\n",
      "epoch:7 step:6785 [D loss: 0.646709, acc: 67.19%] [G loss: 2.217913]\n",
      "epoch:7 step:6786 [D loss: 0.630027, acc: 64.06%] [G loss: 2.085726]\n",
      "epoch:7 step:6787 [D loss: 0.625475, acc: 67.19%] [G loss: 2.175960]\n",
      "epoch:7 step:6788 [D loss: 0.595354, acc: 66.41%] [G loss: 2.259567]\n",
      "epoch:7 step:6789 [D loss: 0.539899, acc: 70.31%] [G loss: 2.724434]\n",
      "epoch:7 step:6790 [D loss: 0.541328, acc: 69.53%] [G loss: 2.791229]\n",
      "epoch:7 step:6791 [D loss: 0.505452, acc: 73.44%] [G loss: 2.769346]\n",
      "epoch:7 step:6792 [D loss: 0.616518, acc: 65.62%] [G loss: 2.301773]\n",
      "epoch:7 step:6793 [D loss: 0.625512, acc: 64.06%] [G loss: 2.316593]\n",
      "epoch:7 step:6794 [D loss: 0.650183, acc: 64.06%] [G loss: 2.276358]\n",
      "epoch:7 step:6795 [D loss: 0.582478, acc: 68.75%] [G loss: 2.661044]\n",
      "epoch:7 step:6796 [D loss: 0.613576, acc: 64.06%] [G loss: 2.327407]\n",
      "epoch:7 step:6797 [D loss: 0.565267, acc: 67.19%] [G loss: 2.231516]\n",
      "epoch:7 step:6798 [D loss: 0.548186, acc: 73.44%] [G loss: 2.306732]\n",
      "epoch:7 step:6799 [D loss: 0.606729, acc: 68.75%] [G loss: 2.446878]\n",
      "epoch:7 step:6800 [D loss: 0.539160, acc: 75.78%] [G loss: 2.254368]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.104991\n",
      "FID: 32.918640\n",
      "0 = 13.202464070796989\n",
      "1 = 0.09899593776498976\n",
      "2 = 0.9136999845504761\n",
      "3 = 0.9264000058174133\n",
      "4 = 0.9010000228881836\n",
      "5 = 0.9034523367881775\n",
      "6 = 0.9264000058174133\n",
      "7 = 8.135813121116163\n",
      "8 = 0.11319492602619682\n",
      "9 = 0.8046000003814697\n",
      "10 = 0.8149999976158142\n",
      "11 = 0.7942000031471252\n",
      "12 = 0.7983934283256531\n",
      "13 = 0.8149999976158142\n",
      "14 = 6.105016231536865\n",
      "15 = 9.256509780883789\n",
      "16 = 0.18812350928783417\n",
      "17 = 6.104991436004639\n",
      "18 = 32.91864013671875\n",
      "epoch:7 step:6801 [D loss: 0.597639, acc: 65.62%] [G loss: 2.192467]\n",
      "epoch:7 step:6802 [D loss: 0.614783, acc: 62.50%] [G loss: 2.358190]\n",
      "epoch:7 step:6803 [D loss: 0.630533, acc: 66.41%] [G loss: 2.361862]\n",
      "epoch:7 step:6804 [D loss: 0.620435, acc: 70.31%] [G loss: 2.325299]\n",
      "epoch:7 step:6805 [D loss: 0.694756, acc: 56.25%] [G loss: 2.159329]\n",
      "epoch:7 step:6806 [D loss: 0.618020, acc: 69.53%] [G loss: 2.308972]\n",
      "epoch:7 step:6807 [D loss: 0.564610, acc: 74.22%] [G loss: 2.397777]\n",
      "epoch:7 step:6808 [D loss: 0.627177, acc: 61.72%] [G loss: 2.191182]\n",
      "epoch:7 step:6809 [D loss: 0.670609, acc: 60.16%] [G loss: 2.148218]\n",
      "epoch:7 step:6810 [D loss: 0.685450, acc: 60.16%] [G loss: 2.193872]\n",
      "epoch:7 step:6811 [D loss: 0.661695, acc: 54.69%] [G loss: 2.011590]\n",
      "epoch:7 step:6812 [D loss: 0.619636, acc: 63.28%] [G loss: 2.429730]\n",
      "epoch:7 step:6813 [D loss: 0.632417, acc: 65.62%] [G loss: 2.270564]\n",
      "epoch:7 step:6814 [D loss: 0.594923, acc: 72.66%] [G loss: 2.046096]\n",
      "epoch:7 step:6815 [D loss: 0.586298, acc: 68.75%] [G loss: 2.095600]\n",
      "epoch:7 step:6816 [D loss: 0.611617, acc: 60.16%] [G loss: 2.192199]\n",
      "epoch:7 step:6817 [D loss: 0.680641, acc: 64.06%] [G loss: 2.150540]\n",
      "epoch:7 step:6818 [D loss: 0.607574, acc: 67.19%] [G loss: 2.227552]\n",
      "epoch:7 step:6819 [D loss: 0.642153, acc: 67.97%] [G loss: 2.358847]\n",
      "epoch:7 step:6820 [D loss: 0.605076, acc: 65.62%] [G loss: 2.180604]\n",
      "epoch:7 step:6821 [D loss: 0.573428, acc: 73.44%] [G loss: 2.498233]\n",
      "epoch:7 step:6822 [D loss: 0.717657, acc: 60.16%] [G loss: 2.054106]\n",
      "epoch:7 step:6823 [D loss: 0.584136, acc: 73.44%] [G loss: 2.543304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6824 [D loss: 0.689703, acc: 63.28%] [G loss: 1.991529]\n",
      "epoch:7 step:6825 [D loss: 0.614207, acc: 62.50%] [G loss: 2.381052]\n",
      "epoch:7 step:6826 [D loss: 0.639321, acc: 65.62%] [G loss: 2.183334]\n",
      "epoch:7 step:6827 [D loss: 0.672149, acc: 64.06%] [G loss: 2.046146]\n",
      "epoch:7 step:6828 [D loss: 0.632290, acc: 64.84%] [G loss: 2.248421]\n",
      "epoch:7 step:6829 [D loss: 0.618854, acc: 63.28%] [G loss: 2.285858]\n",
      "epoch:7 step:6830 [D loss: 0.626015, acc: 66.41%] [G loss: 2.193538]\n",
      "epoch:7 step:6831 [D loss: 0.566042, acc: 71.88%] [G loss: 2.055846]\n",
      "epoch:7 step:6832 [D loss: 0.621900, acc: 70.31%] [G loss: 2.214255]\n",
      "epoch:7 step:6833 [D loss: 0.589723, acc: 66.41%] [G loss: 2.294095]\n",
      "epoch:7 step:6834 [D loss: 0.676763, acc: 60.94%] [G loss: 2.309736]\n",
      "epoch:7 step:6835 [D loss: 0.592072, acc: 68.75%] [G loss: 2.275114]\n",
      "epoch:7 step:6836 [D loss: 0.638908, acc: 59.38%] [G loss: 2.000242]\n",
      "epoch:7 step:6837 [D loss: 0.635518, acc: 65.62%] [G loss: 2.164711]\n",
      "epoch:7 step:6838 [D loss: 0.581793, acc: 67.97%] [G loss: 2.281589]\n",
      "epoch:7 step:6839 [D loss: 0.620116, acc: 65.62%] [G loss: 2.476248]\n",
      "epoch:7 step:6840 [D loss: 0.624109, acc: 65.62%] [G loss: 2.051641]\n",
      "epoch:7 step:6841 [D loss: 0.617551, acc: 63.28%] [G loss: 2.034424]\n",
      "epoch:7 step:6842 [D loss: 0.598618, acc: 67.97%] [G loss: 2.432568]\n",
      "epoch:7 step:6843 [D loss: 0.606896, acc: 67.97%] [G loss: 2.333317]\n",
      "epoch:7 step:6844 [D loss: 0.612860, acc: 72.66%] [G loss: 2.197511]\n",
      "epoch:7 step:6845 [D loss: 0.580811, acc: 73.44%] [G loss: 2.556961]\n",
      "epoch:7 step:6846 [D loss: 0.630941, acc: 66.41%] [G loss: 2.267493]\n",
      "epoch:7 step:6847 [D loss: 0.575723, acc: 68.75%] [G loss: 2.320672]\n",
      "epoch:7 step:6848 [D loss: 0.640736, acc: 61.72%] [G loss: 2.257937]\n",
      "epoch:7 step:6849 [D loss: 0.636257, acc: 67.97%] [G loss: 2.356081]\n",
      "epoch:7 step:6850 [D loss: 0.679596, acc: 64.06%] [G loss: 2.328408]\n",
      "epoch:7 step:6851 [D loss: 0.662775, acc: 60.94%] [G loss: 1.979522]\n",
      "epoch:7 step:6852 [D loss: 0.573926, acc: 67.19%] [G loss: 2.098653]\n",
      "epoch:7 step:6853 [D loss: 0.565421, acc: 69.53%] [G loss: 2.480873]\n",
      "epoch:7 step:6854 [D loss: 0.593906, acc: 69.53%] [G loss: 2.302041]\n",
      "epoch:7 step:6855 [D loss: 0.594195, acc: 67.97%] [G loss: 2.400546]\n",
      "epoch:7 step:6856 [D loss: 0.616747, acc: 67.97%] [G loss: 2.112648]\n",
      "epoch:7 step:6857 [D loss: 0.593436, acc: 66.41%] [G loss: 2.362659]\n",
      "epoch:7 step:6858 [D loss: 0.601216, acc: 72.66%] [G loss: 2.297744]\n",
      "epoch:7 step:6859 [D loss: 0.549610, acc: 71.88%] [G loss: 2.461448]\n",
      "epoch:7 step:6860 [D loss: 0.628787, acc: 65.62%] [G loss: 1.979830]\n",
      "epoch:7 step:6861 [D loss: 0.608365, acc: 69.53%] [G loss: 2.112866]\n",
      "epoch:7 step:6862 [D loss: 0.562475, acc: 72.66%] [G loss: 2.355566]\n",
      "epoch:7 step:6863 [D loss: 0.683493, acc: 61.72%] [G loss: 2.220234]\n",
      "epoch:7 step:6864 [D loss: 0.620888, acc: 64.06%] [G loss: 2.385046]\n",
      "epoch:7 step:6865 [D loss: 0.614921, acc: 66.41%] [G loss: 2.371072]\n",
      "epoch:7 step:6866 [D loss: 0.574848, acc: 67.97%] [G loss: 2.269579]\n",
      "epoch:7 step:6867 [D loss: 0.723454, acc: 57.81%] [G loss: 2.259259]\n",
      "epoch:7 step:6868 [D loss: 0.628886, acc: 70.31%] [G loss: 2.096942]\n",
      "epoch:7 step:6869 [D loss: 0.647982, acc: 67.19%] [G loss: 2.205184]\n",
      "epoch:7 step:6870 [D loss: 0.582728, acc: 67.97%] [G loss: 2.288066]\n",
      "epoch:7 step:6871 [D loss: 0.512588, acc: 75.78%] [G loss: 2.817489]\n",
      "epoch:7 step:6872 [D loss: 0.533667, acc: 72.66%] [G loss: 2.666158]\n",
      "epoch:7 step:6873 [D loss: 0.541030, acc: 72.66%] [G loss: 2.918452]\n",
      "epoch:7 step:6874 [D loss: 0.553034, acc: 75.00%] [G loss: 2.754408]\n",
      "epoch:7 step:6875 [D loss: 0.664551, acc: 57.81%] [G loss: 2.153440]\n",
      "epoch:7 step:6876 [D loss: 0.669370, acc: 58.59%] [G loss: 2.084106]\n",
      "epoch:7 step:6877 [D loss: 0.712838, acc: 56.25%] [G loss: 2.078981]\n",
      "epoch:7 step:6878 [D loss: 0.588753, acc: 73.44%] [G loss: 2.251876]\n",
      "epoch:7 step:6879 [D loss: 0.623367, acc: 65.62%] [G loss: 2.276942]\n",
      "epoch:7 step:6880 [D loss: 0.596428, acc: 68.75%] [G loss: 2.364223]\n",
      "epoch:7 step:6881 [D loss: 0.582972, acc: 65.62%] [G loss: 2.176150]\n",
      "epoch:7 step:6882 [D loss: 0.677551, acc: 59.38%] [G loss: 2.089521]\n",
      "epoch:7 step:6883 [D loss: 0.587302, acc: 71.88%] [G loss: 2.143008]\n",
      "epoch:7 step:6884 [D loss: 0.593204, acc: 68.75%] [G loss: 2.356393]\n",
      "epoch:7 step:6885 [D loss: 0.633039, acc: 60.94%] [G loss: 2.054228]\n",
      "epoch:7 step:6886 [D loss: 0.588775, acc: 73.44%] [G loss: 2.186418]\n",
      "epoch:7 step:6887 [D loss: 0.648329, acc: 64.84%] [G loss: 2.220804]\n",
      "epoch:7 step:6888 [D loss: 0.541385, acc: 72.66%] [G loss: 2.302707]\n",
      "epoch:7 step:6889 [D loss: 0.563683, acc: 68.75%] [G loss: 2.299686]\n",
      "epoch:7 step:6890 [D loss: 0.563624, acc: 68.75%] [G loss: 2.272985]\n",
      "epoch:7 step:6891 [D loss: 0.641139, acc: 67.19%] [G loss: 2.396118]\n",
      "epoch:7 step:6892 [D loss: 0.603246, acc: 66.41%] [G loss: 2.491134]\n",
      "epoch:7 step:6893 [D loss: 0.669399, acc: 60.16%] [G loss: 2.197450]\n",
      "epoch:7 step:6894 [D loss: 0.563013, acc: 69.53%] [G loss: 2.319396]\n",
      "epoch:7 step:6895 [D loss: 0.605381, acc: 67.97%] [G loss: 2.317940]\n",
      "epoch:7 step:6896 [D loss: 0.578511, acc: 69.53%] [G loss: 2.394125]\n",
      "epoch:7 step:6897 [D loss: 0.557267, acc: 78.12%] [G loss: 2.405931]\n",
      "epoch:7 step:6898 [D loss: 0.557974, acc: 75.78%] [G loss: 2.453323]\n",
      "epoch:7 step:6899 [D loss: 0.641755, acc: 65.62%] [G loss: 2.262494]\n",
      "epoch:7 step:6900 [D loss: 0.659999, acc: 60.94%] [G loss: 2.151836]\n",
      "epoch:7 step:6901 [D loss: 0.682375, acc: 62.50%] [G loss: 2.122165]\n",
      "epoch:7 step:6902 [D loss: 0.670322, acc: 60.94%] [G loss: 2.265171]\n",
      "epoch:7 step:6903 [D loss: 0.625547, acc: 65.62%] [G loss: 2.206913]\n",
      "epoch:7 step:6904 [D loss: 0.595874, acc: 67.19%] [G loss: 2.623467]\n",
      "epoch:7 step:6905 [D loss: 0.440387, acc: 82.03%] [G loss: 2.770019]\n",
      "epoch:7 step:6906 [D loss: 0.624937, acc: 69.53%] [G loss: 2.788583]\n",
      "epoch:7 step:6907 [D loss: 0.730567, acc: 54.69%] [G loss: 2.173064]\n",
      "epoch:7 step:6908 [D loss: 0.778211, acc: 53.91%] [G loss: 1.929828]\n",
      "epoch:7 step:6909 [D loss: 0.577662, acc: 70.31%] [G loss: 2.071829]\n",
      "epoch:7 step:6910 [D loss: 0.616439, acc: 62.50%] [G loss: 2.103220]\n",
      "epoch:7 step:6911 [D loss: 0.669745, acc: 64.06%] [G loss: 2.206741]\n",
      "epoch:7 step:6912 [D loss: 0.565761, acc: 66.41%] [G loss: 2.390734]\n",
      "epoch:7 step:6913 [D loss: 0.589234, acc: 66.41%] [G loss: 2.552539]\n",
      "epoch:7 step:6914 [D loss: 0.602914, acc: 66.41%] [G loss: 2.153461]\n",
      "epoch:7 step:6915 [D loss: 0.687209, acc: 62.50%] [G loss: 1.879980]\n",
      "epoch:7 step:6916 [D loss: 0.594214, acc: 67.19%] [G loss: 2.357749]\n",
      "epoch:7 step:6917 [D loss: 0.529068, acc: 79.69%] [G loss: 2.502038]\n",
      "epoch:7 step:6918 [D loss: 0.649745, acc: 67.19%] [G loss: 2.369401]\n",
      "epoch:7 step:6919 [D loss: 0.621572, acc: 65.62%] [G loss: 2.160763]\n",
      "epoch:7 step:6920 [D loss: 0.610802, acc: 63.28%] [G loss: 2.190683]\n",
      "epoch:7 step:6921 [D loss: 0.596407, acc: 66.41%] [G loss: 2.088739]\n",
      "epoch:7 step:6922 [D loss: 0.587665, acc: 72.66%] [G loss: 2.329840]\n",
      "epoch:7 step:6923 [D loss: 0.577501, acc: 71.09%] [G loss: 2.224677]\n",
      "epoch:7 step:6924 [D loss: 0.538401, acc: 69.53%] [G loss: 2.316994]\n",
      "epoch:7 step:6925 [D loss: 0.574383, acc: 71.09%] [G loss: 2.420554]\n",
      "epoch:7 step:6926 [D loss: 0.551546, acc: 73.44%] [G loss: 2.340765]\n",
      "epoch:7 step:6927 [D loss: 0.639794, acc: 63.28%] [G loss: 2.068666]\n",
      "epoch:7 step:6928 [D loss: 0.708632, acc: 62.50%] [G loss: 2.106772]\n",
      "epoch:7 step:6929 [D loss: 0.629702, acc: 62.50%] [G loss: 2.303950]\n",
      "epoch:7 step:6930 [D loss: 0.623996, acc: 65.62%] [G loss: 2.350973]\n",
      "epoch:7 step:6931 [D loss: 0.615933, acc: 67.19%] [G loss: 2.506793]\n",
      "epoch:7 step:6932 [D loss: 0.622841, acc: 67.19%] [G loss: 2.163200]\n",
      "epoch:7 step:6933 [D loss: 0.588862, acc: 69.53%] [G loss: 2.205178]\n",
      "epoch:7 step:6934 [D loss: 0.646163, acc: 61.72%] [G loss: 2.289727]\n",
      "epoch:7 step:6935 [D loss: 0.618200, acc: 65.62%] [G loss: 2.043482]\n",
      "epoch:7 step:6936 [D loss: 0.663236, acc: 61.72%] [G loss: 2.042583]\n",
      "epoch:7 step:6937 [D loss: 0.654570, acc: 67.19%] [G loss: 2.086204]\n",
      "epoch:7 step:6938 [D loss: 0.646550, acc: 62.50%] [G loss: 2.087153]\n",
      "epoch:7 step:6939 [D loss: 0.581446, acc: 69.53%] [G loss: 2.484642]\n",
      "epoch:7 step:6940 [D loss: 0.607339, acc: 70.31%] [G loss: 2.529029]\n",
      "epoch:7 step:6941 [D loss: 0.640177, acc: 66.41%] [G loss: 2.142159]\n",
      "epoch:7 step:6942 [D loss: 0.650015, acc: 65.62%] [G loss: 2.178740]\n",
      "epoch:7 step:6943 [D loss: 0.598675, acc: 61.72%] [G loss: 2.234214]\n",
      "epoch:7 step:6944 [D loss: 0.576371, acc: 71.88%] [G loss: 2.414713]\n",
      "epoch:7 step:6945 [D loss: 0.585302, acc: 67.19%] [G loss: 2.228213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6946 [D loss: 0.739661, acc: 60.94%] [G loss: 2.023619]\n",
      "epoch:7 step:6947 [D loss: 0.637474, acc: 64.84%] [G loss: 2.247838]\n",
      "epoch:7 step:6948 [D loss: 0.627138, acc: 68.75%] [G loss: 2.139889]\n",
      "epoch:7 step:6949 [D loss: 0.593894, acc: 67.97%] [G loss: 2.293185]\n",
      "epoch:7 step:6950 [D loss: 0.727098, acc: 53.91%] [G loss: 2.076186]\n",
      "epoch:7 step:6951 [D loss: 0.621408, acc: 66.41%] [G loss: 2.232551]\n",
      "epoch:7 step:6952 [D loss: 0.577319, acc: 68.75%] [G loss: 2.551953]\n",
      "epoch:7 step:6953 [D loss: 0.653815, acc: 60.94%] [G loss: 2.173656]\n",
      "epoch:7 step:6954 [D loss: 0.616563, acc: 65.62%] [G loss: 2.184749]\n",
      "epoch:7 step:6955 [D loss: 0.657935, acc: 62.50%] [G loss: 2.038864]\n",
      "epoch:7 step:6956 [D loss: 0.693106, acc: 56.25%] [G loss: 1.927290]\n",
      "epoch:7 step:6957 [D loss: 0.647982, acc: 62.50%] [G loss: 1.978869]\n",
      "epoch:7 step:6958 [D loss: 0.645917, acc: 67.19%] [G loss: 2.207290]\n",
      "epoch:7 step:6959 [D loss: 0.641207, acc: 62.50%] [G loss: 2.138805]\n",
      "epoch:7 step:6960 [D loss: 0.663865, acc: 64.06%] [G loss: 2.148067]\n",
      "epoch:7 step:6961 [D loss: 0.613656, acc: 70.31%] [G loss: 2.284624]\n",
      "epoch:7 step:6962 [D loss: 0.584248, acc: 70.31%] [G loss: 2.399596]\n",
      "epoch:7 step:6963 [D loss: 0.609470, acc: 67.19%] [G loss: 2.251667]\n",
      "epoch:7 step:6964 [D loss: 0.565344, acc: 76.56%] [G loss: 2.434173]\n",
      "epoch:7 step:6965 [D loss: 0.604543, acc: 70.31%] [G loss: 2.340276]\n",
      "epoch:7 step:6966 [D loss: 0.627767, acc: 62.50%] [G loss: 2.205114]\n",
      "epoch:7 step:6967 [D loss: 0.618215, acc: 67.19%] [G loss: 2.190525]\n",
      "epoch:7 step:6968 [D loss: 0.617736, acc: 67.19%] [G loss: 2.315605]\n",
      "epoch:7 step:6969 [D loss: 0.596550, acc: 70.31%] [G loss: 2.409691]\n",
      "epoch:7 step:6970 [D loss: 0.618445, acc: 68.75%] [G loss: 2.407302]\n",
      "epoch:7 step:6971 [D loss: 0.620055, acc: 65.62%] [G loss: 2.218022]\n",
      "epoch:7 step:6972 [D loss: 0.615688, acc: 64.84%] [G loss: 2.260342]\n",
      "epoch:7 step:6973 [D loss: 0.629274, acc: 66.41%] [G loss: 2.118643]\n",
      "epoch:7 step:6974 [D loss: 0.553191, acc: 70.31%] [G loss: 2.344836]\n",
      "epoch:7 step:6975 [D loss: 0.591263, acc: 67.19%] [G loss: 2.207844]\n",
      "epoch:7 step:6976 [D loss: 0.697765, acc: 61.72%] [G loss: 2.355428]\n",
      "epoch:7 step:6977 [D loss: 0.547996, acc: 74.22%] [G loss: 2.180009]\n",
      "epoch:7 step:6978 [D loss: 0.595472, acc: 70.31%] [G loss: 2.495020]\n",
      "epoch:7 step:6979 [D loss: 0.598176, acc: 71.88%] [G loss: 2.044015]\n",
      "epoch:7 step:6980 [D loss: 0.621270, acc: 64.06%] [G loss: 2.130836]\n",
      "epoch:7 step:6981 [D loss: 0.645536, acc: 61.72%] [G loss: 2.141632]\n",
      "epoch:7 step:6982 [D loss: 0.608103, acc: 65.62%] [G loss: 2.075398]\n",
      "epoch:7 step:6983 [D loss: 0.612437, acc: 61.72%] [G loss: 2.353732]\n",
      "epoch:7 step:6984 [D loss: 0.557099, acc: 67.97%] [G loss: 2.292443]\n",
      "epoch:7 step:6985 [D loss: 0.567777, acc: 65.62%] [G loss: 2.361427]\n",
      "epoch:7 step:6986 [D loss: 0.609728, acc: 64.84%] [G loss: 2.490339]\n",
      "epoch:7 step:6987 [D loss: 0.446378, acc: 81.25%] [G loss: 2.640687]\n",
      "epoch:7 step:6988 [D loss: 0.552958, acc: 71.09%] [G loss: 2.846187]\n",
      "epoch:7 step:6989 [D loss: 0.556765, acc: 70.31%] [G loss: 2.671579]\n",
      "epoch:7 step:6990 [D loss: 0.556824, acc: 75.00%] [G loss: 2.312993]\n",
      "epoch:7 step:6991 [D loss: 0.656540, acc: 64.84%] [G loss: 2.060756]\n",
      "epoch:7 step:6992 [D loss: 0.652040, acc: 60.94%] [G loss: 2.099866]\n",
      "epoch:7 step:6993 [D loss: 0.591271, acc: 69.53%] [G loss: 2.550122]\n",
      "epoch:7 step:6994 [D loss: 0.596685, acc: 71.88%] [G loss: 2.091117]\n",
      "epoch:7 step:6995 [D loss: 0.670502, acc: 65.62%] [G loss: 2.313182]\n",
      "epoch:7 step:6996 [D loss: 0.595981, acc: 62.50%] [G loss: 2.144896]\n",
      "epoch:7 step:6997 [D loss: 0.635329, acc: 63.28%] [G loss: 2.078132]\n",
      "epoch:7 step:6998 [D loss: 0.654794, acc: 60.94%] [G loss: 2.082212]\n",
      "epoch:7 step:6999 [D loss: 0.646030, acc: 60.16%] [G loss: 2.287132]\n",
      "epoch:7 step:7000 [D loss: 0.644510, acc: 66.41%] [G loss: 2.193727]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.131803\n",
      "FID: 34.332382\n",
      "0 = 12.873631535434692\n",
      "1 = 0.07836495741853304\n",
      "2 = 0.9067000150680542\n",
      "3 = 0.9120000004768372\n",
      "4 = 0.9014000296592712\n",
      "5 = 0.9024341702461243\n",
      "6 = 0.9120000004768372\n",
      "7 = 8.181376508975042\n",
      "8 = 0.1160389886315345\n",
      "9 = 0.7975999712944031\n",
      "10 = 0.8026000261306763\n",
      "11 = 0.7925999760627747\n",
      "12 = 0.7946534752845764\n",
      "13 = 0.8026000261306763\n",
      "14 = 6.131825923919678\n",
      "15 = 9.144779205322266\n",
      "16 = 0.19523419439792633\n",
      "17 = 6.131803035736084\n",
      "18 = 34.33238220214844\n",
      "epoch:7 step:7001 [D loss: 0.682196, acc: 66.41%] [G loss: 2.102931]\n",
      "epoch:7 step:7002 [D loss: 0.544257, acc: 75.00%] [G loss: 2.393640]\n",
      "epoch:7 step:7003 [D loss: 0.678725, acc: 61.72%] [G loss: 2.085375]\n",
      "epoch:7 step:7004 [D loss: 0.608263, acc: 68.75%] [G loss: 2.188347]\n",
      "epoch:7 step:7005 [D loss: 0.642476, acc: 60.94%] [G loss: 2.164512]\n",
      "epoch:7 step:7006 [D loss: 0.592308, acc: 66.41%] [G loss: 2.257716]\n",
      "epoch:7 step:7007 [D loss: 0.657808, acc: 61.72%] [G loss: 1.943188]\n",
      "epoch:7 step:7008 [D loss: 0.650735, acc: 60.94%] [G loss: 2.108062]\n",
      "epoch:7 step:7009 [D loss: 0.624734, acc: 62.50%] [G loss: 2.096755]\n",
      "epoch:7 step:7010 [D loss: 0.594138, acc: 68.75%] [G loss: 2.539332]\n",
      "epoch:7 step:7011 [D loss: 0.604671, acc: 67.19%] [G loss: 2.118083]\n",
      "epoch:7 step:7012 [D loss: 0.610747, acc: 67.19%] [G loss: 2.319061]\n",
      "epoch:7 step:7013 [D loss: 0.614447, acc: 64.84%] [G loss: 2.146208]\n",
      "epoch:7 step:7014 [D loss: 0.584894, acc: 67.97%] [G loss: 2.318241]\n",
      "epoch:7 step:7015 [D loss: 0.587812, acc: 66.41%] [G loss: 2.290392]\n",
      "epoch:7 step:7016 [D loss: 0.584870, acc: 71.88%] [G loss: 2.121242]\n",
      "epoch:7 step:7017 [D loss: 0.741990, acc: 51.56%] [G loss: 2.005486]\n",
      "epoch:7 step:7018 [D loss: 0.634578, acc: 63.28%] [G loss: 2.079784]\n",
      "epoch:7 step:7019 [D loss: 0.644220, acc: 63.28%] [G loss: 2.000425]\n",
      "epoch:7 step:7020 [D loss: 0.618661, acc: 61.72%] [G loss: 2.085694]\n",
      "epoch:7 step:7021 [D loss: 0.630769, acc: 67.19%] [G loss: 2.227077]\n",
      "epoch:7 step:7022 [D loss: 0.605032, acc: 66.41%] [G loss: 2.074827]\n",
      "epoch:7 step:7023 [D loss: 0.588981, acc: 69.53%] [G loss: 2.095128]\n",
      "epoch:7 step:7024 [D loss: 0.642605, acc: 67.19%] [G loss: 2.277213]\n",
      "epoch:7 step:7025 [D loss: 0.648795, acc: 64.06%] [G loss: 2.162318]\n",
      "epoch:7 step:7026 [D loss: 0.615441, acc: 71.09%] [G loss: 2.148765]\n",
      "epoch:7 step:7027 [D loss: 0.606278, acc: 66.41%] [G loss: 2.181823]\n",
      "epoch:7 step:7028 [D loss: 0.588207, acc: 65.62%] [G loss: 2.195566]\n",
      "epoch:7 step:7029 [D loss: 0.564306, acc: 68.75%] [G loss: 2.696064]\n",
      "epoch:7 step:7030 [D loss: 0.558802, acc: 72.66%] [G loss: 2.863987]\n",
      "epoch:7 step:7031 [D loss: 0.613259, acc: 69.53%] [G loss: 2.719245]\n",
      "epoch:7 step:7032 [D loss: 0.730181, acc: 56.25%] [G loss: 2.119965]\n",
      "epoch:7 step:7033 [D loss: 0.619361, acc: 65.62%] [G loss: 2.229845]\n",
      "epoch:7 step:7034 [D loss: 0.617771, acc: 67.97%] [G loss: 2.490324]\n",
      "epoch:7 step:7035 [D loss: 0.632162, acc: 67.19%] [G loss: 2.249363]\n",
      "epoch:7 step:7036 [D loss: 0.707618, acc: 57.03%] [G loss: 2.086435]\n",
      "epoch:7 step:7037 [D loss: 0.643005, acc: 60.94%] [G loss: 2.033654]\n",
      "epoch:7 step:7038 [D loss: 0.637123, acc: 64.84%] [G loss: 2.082802]\n",
      "epoch:7 step:7039 [D loss: 0.581737, acc: 71.88%] [G loss: 2.319304]\n",
      "epoch:7 step:7040 [D loss: 0.575647, acc: 72.66%] [G loss: 2.469724]\n",
      "epoch:7 step:7041 [D loss: 0.719165, acc: 53.91%] [G loss: 2.087459]\n",
      "epoch:7 step:7042 [D loss: 0.723829, acc: 54.69%] [G loss: 2.305346]\n",
      "epoch:7 step:7043 [D loss: 0.652718, acc: 66.41%] [G loss: 2.447232]\n",
      "epoch:7 step:7044 [D loss: 0.642070, acc: 64.84%] [G loss: 2.445446]\n",
      "epoch:7 step:7045 [D loss: 0.687801, acc: 60.16%] [G loss: 1.936384]\n",
      "epoch:7 step:7046 [D loss: 0.577144, acc: 68.75%] [G loss: 2.279784]\n",
      "epoch:7 step:7047 [D loss: 0.539247, acc: 73.44%] [G loss: 2.357492]\n",
      "epoch:7 step:7048 [D loss: 0.615305, acc: 63.28%] [G loss: 2.067845]\n",
      "epoch:7 step:7049 [D loss: 0.615006, acc: 68.75%] [G loss: 2.134355]\n",
      "epoch:7 step:7050 [D loss: 0.589659, acc: 71.09%] [G loss: 2.275352]\n",
      "epoch:7 step:7051 [D loss: 0.577710, acc: 69.53%] [G loss: 2.127745]\n",
      "epoch:7 step:7052 [D loss: 0.605807, acc: 69.53%] [G loss: 2.280076]\n",
      "epoch:7 step:7053 [D loss: 0.633911, acc: 65.62%] [G loss: 2.337431]\n",
      "epoch:7 step:7054 [D loss: 0.610271, acc: 69.53%] [G loss: 2.644877]\n",
      "epoch:7 step:7055 [D loss: 0.594856, acc: 67.19%] [G loss: 2.171171]\n",
      "epoch:7 step:7056 [D loss: 0.587925, acc: 71.09%] [G loss: 2.467621]\n",
      "epoch:7 step:7057 [D loss: 0.589665, acc: 66.41%] [G loss: 2.524610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7058 [D loss: 0.543831, acc: 75.00%] [G loss: 2.391969]\n",
      "epoch:7 step:7059 [D loss: 0.738336, acc: 54.69%] [G loss: 1.931446]\n",
      "epoch:7 step:7060 [D loss: 0.692659, acc: 59.38%] [G loss: 1.968234]\n",
      "epoch:7 step:7061 [D loss: 0.647168, acc: 62.50%] [G loss: 1.885921]\n",
      "epoch:7 step:7062 [D loss: 0.615515, acc: 67.97%] [G loss: 2.527861]\n",
      "epoch:7 step:7063 [D loss: 0.582442, acc: 65.62%] [G loss: 2.562651]\n",
      "epoch:7 step:7064 [D loss: 0.570971, acc: 71.88%] [G loss: 2.220324]\n",
      "epoch:7 step:7065 [D loss: 0.672582, acc: 57.03%] [G loss: 2.232687]\n",
      "epoch:7 step:7066 [D loss: 0.650327, acc: 59.38%] [G loss: 2.229096]\n",
      "epoch:7 step:7067 [D loss: 0.598474, acc: 67.97%] [G loss: 2.728112]\n",
      "epoch:7 step:7068 [D loss: 0.589134, acc: 66.41%] [G loss: 2.173692]\n",
      "epoch:7 step:7069 [D loss: 0.657779, acc: 64.84%] [G loss: 2.051254]\n",
      "epoch:7 step:7070 [D loss: 0.693407, acc: 53.12%] [G loss: 1.996324]\n",
      "epoch:7 step:7071 [D loss: 0.604711, acc: 66.41%] [G loss: 2.082047]\n",
      "epoch:7 step:7072 [D loss: 0.642171, acc: 68.75%] [G loss: 2.173176]\n",
      "epoch:7 step:7073 [D loss: 0.628536, acc: 66.41%] [G loss: 2.265099]\n",
      "epoch:7 step:7074 [D loss: 0.612266, acc: 67.19%] [G loss: 2.288060]\n",
      "epoch:7 step:7075 [D loss: 0.593060, acc: 75.00%] [G loss: 2.374781]\n",
      "epoch:7 step:7076 [D loss: 0.641335, acc: 63.28%] [G loss: 2.203144]\n",
      "epoch:7 step:7077 [D loss: 0.568886, acc: 71.88%] [G loss: 2.101388]\n",
      "epoch:7 step:7078 [D loss: 0.568358, acc: 70.31%] [G loss: 2.207653]\n",
      "epoch:7 step:7079 [D loss: 0.576863, acc: 67.97%] [G loss: 2.453454]\n",
      "epoch:7 step:7080 [D loss: 0.565717, acc: 74.22%] [G loss: 2.449511]\n",
      "epoch:7 step:7081 [D loss: 0.627415, acc: 67.19%] [G loss: 2.207556]\n",
      "epoch:7 step:7082 [D loss: 0.591209, acc: 69.53%] [G loss: 2.384501]\n",
      "epoch:7 step:7083 [D loss: 0.586030, acc: 71.88%] [G loss: 2.288852]\n",
      "epoch:7 step:7084 [D loss: 0.642901, acc: 61.72%] [G loss: 2.228786]\n",
      "epoch:7 step:7085 [D loss: 0.641521, acc: 60.94%] [G loss: 2.209710]\n",
      "epoch:7 step:7086 [D loss: 0.612725, acc: 68.75%] [G loss: 2.144000]\n",
      "epoch:7 step:7087 [D loss: 0.666413, acc: 60.16%] [G loss: 2.066222]\n",
      "epoch:7 step:7088 [D loss: 0.587162, acc: 68.75%] [G loss: 2.033350]\n",
      "epoch:7 step:7089 [D loss: 0.646622, acc: 67.97%] [G loss: 2.045103]\n",
      "epoch:7 step:7090 [D loss: 0.586355, acc: 69.53%] [G loss: 2.270392]\n",
      "epoch:7 step:7091 [D loss: 0.593768, acc: 70.31%] [G loss: 2.259483]\n",
      "epoch:7 step:7092 [D loss: 0.645689, acc: 64.06%] [G loss: 2.229887]\n",
      "epoch:7 step:7093 [D loss: 0.574543, acc: 64.84%] [G loss: 2.289699]\n",
      "epoch:7 step:7094 [D loss: 0.624660, acc: 66.41%] [G loss: 1.990668]\n",
      "epoch:7 step:7095 [D loss: 0.640286, acc: 64.06%] [G loss: 2.310838]\n",
      "epoch:7 step:7096 [D loss: 0.647910, acc: 64.06%] [G loss: 2.112666]\n",
      "epoch:7 step:7097 [D loss: 0.618240, acc: 67.97%] [G loss: 2.081291]\n",
      "epoch:7 step:7098 [D loss: 0.634795, acc: 61.72%] [G loss: 2.020375]\n",
      "epoch:7 step:7099 [D loss: 0.600467, acc: 64.84%] [G loss: 2.129200]\n",
      "epoch:7 step:7100 [D loss: 0.700115, acc: 60.94%] [G loss: 2.243347]\n",
      "epoch:7 step:7101 [D loss: 0.651950, acc: 62.50%] [G loss: 1.982214]\n",
      "epoch:7 step:7102 [D loss: 0.646763, acc: 62.50%] [G loss: 2.032233]\n",
      "epoch:7 step:7103 [D loss: 0.681101, acc: 57.03%] [G loss: 2.072754]\n",
      "epoch:7 step:7104 [D loss: 0.589440, acc: 66.41%] [G loss: 2.242188]\n",
      "epoch:7 step:7105 [D loss: 0.643295, acc: 69.53%] [G loss: 2.269082]\n",
      "epoch:7 step:7106 [D loss: 0.516009, acc: 76.56%] [G loss: 2.320436]\n",
      "epoch:7 step:7107 [D loss: 0.574601, acc: 70.31%] [G loss: 2.288759]\n",
      "epoch:7 step:7108 [D loss: 0.592516, acc: 68.75%] [G loss: 2.665017]\n",
      "epoch:7 step:7109 [D loss: 0.572986, acc: 71.09%] [G loss: 2.447372]\n",
      "epoch:7 step:7110 [D loss: 0.622082, acc: 67.97%] [G loss: 2.502854]\n",
      "epoch:7 step:7111 [D loss: 0.628899, acc: 64.06%] [G loss: 2.192450]\n",
      "epoch:7 step:7112 [D loss: 0.621495, acc: 64.06%] [G loss: 2.077221]\n",
      "epoch:7 step:7113 [D loss: 0.567043, acc: 69.53%] [G loss: 2.600615]\n",
      "epoch:7 step:7114 [D loss: 0.578466, acc: 67.97%] [G loss: 2.265841]\n",
      "epoch:7 step:7115 [D loss: 0.595163, acc: 63.28%] [G loss: 2.282545]\n",
      "epoch:7 step:7116 [D loss: 0.606111, acc: 64.84%] [G loss: 2.248455]\n",
      "epoch:7 step:7117 [D loss: 0.626926, acc: 63.28%] [G loss: 2.235088]\n",
      "epoch:7 step:7118 [D loss: 0.595275, acc: 64.06%] [G loss: 2.266946]\n",
      "epoch:7 step:7119 [D loss: 0.591400, acc: 66.41%] [G loss: 2.113856]\n",
      "epoch:7 step:7120 [D loss: 0.608981, acc: 67.19%] [G loss: 2.227687]\n",
      "epoch:7 step:7121 [D loss: 0.643381, acc: 64.84%] [G loss: 2.093366]\n",
      "epoch:7 step:7122 [D loss: 0.595986, acc: 68.75%] [G loss: 2.192557]\n",
      "epoch:7 step:7123 [D loss: 0.571034, acc: 70.31%] [G loss: 2.511899]\n",
      "epoch:7 step:7124 [D loss: 0.633623, acc: 64.84%] [G loss: 2.155468]\n",
      "epoch:7 step:7125 [D loss: 0.666558, acc: 61.72%] [G loss: 2.133882]\n",
      "epoch:7 step:7126 [D loss: 0.602681, acc: 69.53%] [G loss: 2.154108]\n",
      "epoch:7 step:7127 [D loss: 0.557292, acc: 72.66%] [G loss: 2.260638]\n",
      "epoch:7 step:7128 [D loss: 0.677789, acc: 57.81%] [G loss: 2.230177]\n",
      "epoch:7 step:7129 [D loss: 0.527193, acc: 77.34%] [G loss: 2.196499]\n",
      "epoch:7 step:7130 [D loss: 0.670364, acc: 63.28%] [G loss: 2.147836]\n",
      "epoch:7 step:7131 [D loss: 0.638619, acc: 61.72%] [G loss: 1.962261]\n",
      "epoch:7 step:7132 [D loss: 0.695840, acc: 59.38%] [G loss: 2.019177]\n",
      "epoch:7 step:7133 [D loss: 0.654288, acc: 67.19%] [G loss: 2.185269]\n",
      "epoch:7 step:7134 [D loss: 0.614296, acc: 64.06%] [G loss: 2.294392]\n",
      "epoch:7 step:7135 [D loss: 0.690688, acc: 59.38%] [G loss: 1.986582]\n",
      "epoch:7 step:7136 [D loss: 0.580530, acc: 69.53%] [G loss: 2.015391]\n",
      "epoch:7 step:7137 [D loss: 0.605431, acc: 71.09%] [G loss: 2.083891]\n",
      "epoch:7 step:7138 [D loss: 0.588557, acc: 69.53%] [G loss: 2.340574]\n",
      "epoch:7 step:7139 [D loss: 0.658475, acc: 62.50%] [G loss: 2.086592]\n",
      "epoch:7 step:7140 [D loss: 0.628129, acc: 63.28%] [G loss: 2.075279]\n",
      "epoch:7 step:7141 [D loss: 0.615402, acc: 67.97%] [G loss: 2.468866]\n",
      "epoch:7 step:7142 [D loss: 0.648769, acc: 63.28%] [G loss: 2.055483]\n",
      "epoch:7 step:7143 [D loss: 0.591417, acc: 67.97%] [G loss: 2.212344]\n",
      "epoch:7 step:7144 [D loss: 0.662769, acc: 57.81%] [G loss: 2.094940]\n",
      "epoch:7 step:7145 [D loss: 0.620943, acc: 70.31%] [G loss: 2.156636]\n",
      "epoch:7 step:7146 [D loss: 0.581983, acc: 70.31%] [G loss: 2.086394]\n",
      "epoch:7 step:7147 [D loss: 0.623477, acc: 66.41%] [G loss: 2.250553]\n",
      "epoch:7 step:7148 [D loss: 0.623851, acc: 67.97%] [G loss: 2.384569]\n",
      "epoch:7 step:7149 [D loss: 0.612131, acc: 64.84%] [G loss: 2.277427]\n",
      "epoch:7 step:7150 [D loss: 0.592697, acc: 64.84%] [G loss: 2.218874]\n",
      "epoch:7 step:7151 [D loss: 0.597624, acc: 69.53%] [G loss: 2.236510]\n",
      "epoch:7 step:7152 [D loss: 0.597123, acc: 65.62%] [G loss: 2.179516]\n",
      "epoch:7 step:7153 [D loss: 0.688084, acc: 57.81%] [G loss: 2.098354]\n",
      "epoch:7 step:7154 [D loss: 0.568992, acc: 70.31%] [G loss: 2.184357]\n",
      "epoch:7 step:7155 [D loss: 0.614739, acc: 65.62%] [G loss: 2.102706]\n",
      "epoch:7 step:7156 [D loss: 0.613920, acc: 63.28%] [G loss: 2.169745]\n",
      "epoch:7 step:7157 [D loss: 0.625130, acc: 62.50%] [G loss: 2.273314]\n",
      "epoch:7 step:7158 [D loss: 0.586940, acc: 66.41%] [G loss: 1.991080]\n",
      "epoch:7 step:7159 [D loss: 0.633279, acc: 65.62%] [G loss: 2.162755]\n",
      "epoch:7 step:7160 [D loss: 0.584913, acc: 69.53%] [G loss: 2.189121]\n",
      "epoch:7 step:7161 [D loss: 0.608501, acc: 65.62%] [G loss: 2.292091]\n",
      "epoch:7 step:7162 [D loss: 0.579086, acc: 66.41%] [G loss: 2.228932]\n",
      "epoch:7 step:7163 [D loss: 0.604670, acc: 67.19%] [G loss: 2.071911]\n",
      "epoch:7 step:7164 [D loss: 0.596444, acc: 68.75%] [G loss: 2.301468]\n",
      "epoch:7 step:7165 [D loss: 0.603904, acc: 71.09%] [G loss: 2.381530]\n",
      "epoch:7 step:7166 [D loss: 0.666354, acc: 59.38%] [G loss: 2.031947]\n",
      "epoch:7 step:7167 [D loss: 0.605704, acc: 72.66%] [G loss: 2.154728]\n",
      "epoch:7 step:7168 [D loss: 0.497293, acc: 78.91%] [G loss: 2.453713]\n",
      "epoch:7 step:7169 [D loss: 0.647470, acc: 64.06%] [G loss: 2.132420]\n",
      "epoch:7 step:7170 [D loss: 0.621763, acc: 64.06%] [G loss: 2.140626]\n",
      "epoch:7 step:7171 [D loss: 0.630516, acc: 62.50%] [G loss: 2.324776]\n",
      "epoch:7 step:7172 [D loss: 0.542531, acc: 71.88%] [G loss: 2.456284]\n",
      "epoch:7 step:7173 [D loss: 0.657160, acc: 61.72%] [G loss: 2.005911]\n",
      "epoch:7 step:7174 [D loss: 0.655731, acc: 61.72%] [G loss: 2.108392]\n",
      "epoch:7 step:7175 [D loss: 0.673603, acc: 63.28%] [G loss: 1.989761]\n",
      "epoch:7 step:7176 [D loss: 0.609245, acc: 66.41%] [G loss: 2.004311]\n",
      "epoch:7 step:7177 [D loss: 0.564509, acc: 69.53%] [G loss: 2.120076]\n",
      "epoch:7 step:7178 [D loss: 0.618091, acc: 66.41%] [G loss: 2.192036]\n",
      "epoch:7 step:7179 [D loss: 0.607252, acc: 63.28%] [G loss: 2.346780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7180 [D loss: 0.650307, acc: 65.62%] [G loss: 2.102220]\n",
      "epoch:7 step:7181 [D loss: 0.642255, acc: 63.28%] [G loss: 2.046886]\n",
      "epoch:7 step:7182 [D loss: 0.616337, acc: 67.97%] [G loss: 2.280113]\n",
      "epoch:7 step:7183 [D loss: 0.609920, acc: 68.75%] [G loss: 2.146602]\n",
      "epoch:7 step:7184 [D loss: 0.561786, acc: 71.88%] [G loss: 2.092104]\n",
      "epoch:7 step:7185 [D loss: 0.580218, acc: 69.53%] [G loss: 1.997760]\n",
      "epoch:7 step:7186 [D loss: 0.593636, acc: 66.41%] [G loss: 2.234617]\n",
      "epoch:7 step:7187 [D loss: 0.629983, acc: 67.19%] [G loss: 2.120517]\n",
      "epoch:7 step:7188 [D loss: 0.624488, acc: 66.41%] [G loss: 2.325089]\n",
      "epoch:7 step:7189 [D loss: 0.628306, acc: 68.75%] [G loss: 2.245328]\n",
      "epoch:7 step:7190 [D loss: 0.612122, acc: 64.84%] [G loss: 2.414097]\n",
      "epoch:7 step:7191 [D loss: 0.588622, acc: 71.88%] [G loss: 2.502092]\n",
      "epoch:7 step:7192 [D loss: 0.659325, acc: 60.16%] [G loss: 2.282429]\n",
      "epoch:7 step:7193 [D loss: 0.533281, acc: 76.56%] [G loss: 2.440259]\n",
      "epoch:7 step:7194 [D loss: 0.641115, acc: 62.50%] [G loss: 2.279575]\n",
      "epoch:7 step:7195 [D loss: 0.698328, acc: 56.25%] [G loss: 2.214861]\n",
      "epoch:7 step:7196 [D loss: 0.595165, acc: 72.66%] [G loss: 2.526700]\n",
      "epoch:7 step:7197 [D loss: 0.607560, acc: 67.97%] [G loss: 2.260690]\n",
      "epoch:7 step:7198 [D loss: 0.697492, acc: 62.50%] [G loss: 2.200439]\n",
      "epoch:7 step:7199 [D loss: 0.613641, acc: 66.41%] [G loss: 2.278760]\n",
      "epoch:7 step:7200 [D loss: 0.586925, acc: 69.53%] [G loss: 2.379134]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.222164\n",
      "FID: 34.496105\n",
      "0 = 13.01709779920578\n",
      "1 = 0.08749163451395978\n",
      "2 = 0.913100004196167\n",
      "3 = 0.9175999760627747\n",
      "4 = 0.9085999727249146\n",
      "5 = 0.9094152450561523\n",
      "6 = 0.9175999760627747\n",
      "7 = 8.218252223551282\n",
      "8 = 0.1138580370934736\n",
      "9 = 0.8022000193595886\n",
      "10 = 0.8105999827384949\n",
      "11 = 0.7937999963760376\n",
      "12 = 0.7972069382667542\n",
      "13 = 0.8105999827384949\n",
      "14 = 6.222186088562012\n",
      "15 = 9.298205375671387\n",
      "16 = 0.17837455868721008\n",
      "17 = 6.222163677215576\n",
      "18 = 34.4961051940918\n",
      "epoch:7 step:7201 [D loss: 0.578483, acc: 68.75%] [G loss: 2.301001]\n",
      "epoch:7 step:7202 [D loss: 0.607552, acc: 63.28%] [G loss: 2.103341]\n",
      "epoch:7 step:7203 [D loss: 0.627338, acc: 60.16%] [G loss: 2.239177]\n",
      "epoch:7 step:7204 [D loss: 0.627109, acc: 64.06%] [G loss: 2.360019]\n",
      "epoch:7 step:7205 [D loss: 0.703784, acc: 50.00%] [G loss: 2.172717]\n",
      "epoch:7 step:7206 [D loss: 0.569342, acc: 76.56%] [G loss: 2.491087]\n",
      "epoch:7 step:7207 [D loss: 0.530860, acc: 75.78%] [G loss: 2.766892]\n",
      "epoch:7 step:7208 [D loss: 0.602175, acc: 68.75%] [G loss: 2.368390]\n",
      "epoch:7 step:7209 [D loss: 0.589074, acc: 69.53%] [G loss: 2.495850]\n",
      "epoch:7 step:7210 [D loss: 0.582060, acc: 71.09%] [G loss: 2.433281]\n",
      "epoch:7 step:7211 [D loss: 0.715558, acc: 57.03%] [G loss: 1.990474]\n",
      "epoch:7 step:7212 [D loss: 0.650643, acc: 64.84%] [G loss: 2.233816]\n",
      "epoch:7 step:7213 [D loss: 0.611893, acc: 66.41%] [G loss: 2.362683]\n",
      "epoch:7 step:7214 [D loss: 0.631510, acc: 60.16%] [G loss: 2.203200]\n",
      "epoch:7 step:7215 [D loss: 0.625461, acc: 68.75%] [G loss: 2.289946]\n",
      "epoch:7 step:7216 [D loss: 0.644911, acc: 62.50%] [G loss: 2.078013]\n",
      "epoch:7 step:7217 [D loss: 0.635606, acc: 65.62%] [G loss: 2.036457]\n",
      "epoch:7 step:7218 [D loss: 0.580100, acc: 67.19%] [G loss: 2.168029]\n",
      "epoch:7 step:7219 [D loss: 0.588152, acc: 64.84%] [G loss: 2.063392]\n",
      "epoch:7 step:7220 [D loss: 0.585600, acc: 71.88%] [G loss: 2.121370]\n",
      "epoch:7 step:7221 [D loss: 0.601594, acc: 67.19%] [G loss: 2.160860]\n",
      "epoch:7 step:7222 [D loss: 0.668551, acc: 60.94%] [G loss: 2.154021]\n",
      "epoch:7 step:7223 [D loss: 0.656305, acc: 58.59%] [G loss: 2.119467]\n",
      "epoch:7 step:7224 [D loss: 0.593160, acc: 70.31%] [G loss: 2.180628]\n",
      "epoch:7 step:7225 [D loss: 0.674109, acc: 58.59%] [G loss: 2.119005]\n",
      "epoch:7 step:7226 [D loss: 0.677929, acc: 61.72%] [G loss: 2.011127]\n",
      "epoch:7 step:7227 [D loss: 0.620893, acc: 65.62%] [G loss: 2.191108]\n",
      "epoch:7 step:7228 [D loss: 0.656485, acc: 64.84%] [G loss: 1.978340]\n",
      "epoch:7 step:7229 [D loss: 0.626036, acc: 62.50%] [G loss: 2.132870]\n",
      "epoch:7 step:7230 [D loss: 0.626380, acc: 64.84%] [G loss: 2.004635]\n",
      "epoch:7 step:7231 [D loss: 0.604524, acc: 58.59%] [G loss: 2.214376]\n",
      "epoch:7 step:7232 [D loss: 0.667117, acc: 64.06%] [G loss: 1.970050]\n",
      "epoch:7 step:7233 [D loss: 0.664048, acc: 62.50%] [G loss: 2.039784]\n",
      "epoch:7 step:7234 [D loss: 0.669398, acc: 64.84%] [G loss: 2.202227]\n",
      "epoch:7 step:7235 [D loss: 0.630336, acc: 64.84%] [G loss: 2.094463]\n",
      "epoch:7 step:7236 [D loss: 0.572049, acc: 71.09%] [G loss: 2.161233]\n",
      "epoch:7 step:7237 [D loss: 0.604858, acc: 65.62%] [G loss: 2.076158]\n",
      "epoch:7 step:7238 [D loss: 0.589532, acc: 72.66%] [G loss: 2.215486]\n",
      "epoch:7 step:7239 [D loss: 0.607098, acc: 71.09%] [G loss: 2.359460]\n",
      "epoch:7 step:7240 [D loss: 0.561127, acc: 74.22%] [G loss: 2.274340]\n",
      "epoch:7 step:7241 [D loss: 0.595505, acc: 69.53%] [G loss: 2.236524]\n",
      "epoch:7 step:7242 [D loss: 0.616558, acc: 67.19%] [G loss: 2.195687]\n",
      "epoch:7 step:7243 [D loss: 0.636263, acc: 63.28%] [G loss: 2.094228]\n",
      "epoch:7 step:7244 [D loss: 0.636122, acc: 67.19%] [G loss: 2.072331]\n",
      "epoch:7 step:7245 [D loss: 0.610453, acc: 60.94%] [G loss: 2.312382]\n",
      "epoch:7 step:7246 [D loss: 0.612846, acc: 67.19%] [G loss: 2.047525]\n",
      "epoch:7 step:7247 [D loss: 0.634005, acc: 63.28%] [G loss: 2.080651]\n",
      "epoch:7 step:7248 [D loss: 0.553205, acc: 70.31%] [G loss: 2.214461]\n",
      "epoch:7 step:7249 [D loss: 0.573958, acc: 69.53%] [G loss: 2.406662]\n",
      "epoch:7 step:7250 [D loss: 0.570847, acc: 64.84%] [G loss: 2.634724]\n",
      "epoch:7 step:7251 [D loss: 0.565825, acc: 74.22%] [G loss: 2.402049]\n",
      "epoch:7 step:7252 [D loss: 0.657226, acc: 65.62%] [G loss: 2.715971]\n",
      "epoch:7 step:7253 [D loss: 0.564613, acc: 73.44%] [G loss: 2.484294]\n",
      "epoch:7 step:7254 [D loss: 0.618067, acc: 68.75%] [G loss: 2.348803]\n",
      "epoch:7 step:7255 [D loss: 0.662482, acc: 53.91%] [G loss: 2.186467]\n",
      "epoch:7 step:7256 [D loss: 0.585201, acc: 67.97%] [G loss: 2.307847]\n",
      "epoch:7 step:7257 [D loss: 0.655509, acc: 61.72%] [G loss: 2.204613]\n",
      "epoch:7 step:7258 [D loss: 0.566839, acc: 71.88%] [G loss: 2.359810]\n",
      "epoch:7 step:7259 [D loss: 0.627737, acc: 67.97%] [G loss: 2.313939]\n",
      "epoch:7 step:7260 [D loss: 0.658667, acc: 63.28%] [G loss: 2.327497]\n",
      "epoch:7 step:7261 [D loss: 0.658721, acc: 63.28%] [G loss: 2.070304]\n",
      "epoch:7 step:7262 [D loss: 0.704507, acc: 57.03%] [G loss: 2.023375]\n",
      "epoch:7 step:7263 [D loss: 0.681970, acc: 59.38%] [G loss: 1.938726]\n",
      "epoch:7 step:7264 [D loss: 0.571953, acc: 69.53%] [G loss: 2.286507]\n",
      "epoch:7 step:7265 [D loss: 0.620035, acc: 62.50%] [G loss: 2.251301]\n",
      "epoch:7 step:7266 [D loss: 0.590120, acc: 67.97%] [G loss: 2.568230]\n",
      "epoch:7 step:7267 [D loss: 0.558690, acc: 72.66%] [G loss: 2.455300]\n",
      "epoch:7 step:7268 [D loss: 0.542675, acc: 73.44%] [G loss: 2.570040]\n",
      "epoch:7 step:7269 [D loss: 0.726698, acc: 57.81%] [G loss: 2.002489]\n",
      "epoch:7 step:7270 [D loss: 0.604449, acc: 69.53%] [G loss: 2.439281]\n",
      "epoch:7 step:7271 [D loss: 0.587391, acc: 67.19%] [G loss: 2.468851]\n",
      "epoch:7 step:7272 [D loss: 0.661918, acc: 64.06%] [G loss: 2.126595]\n",
      "epoch:7 step:7273 [D loss: 0.650800, acc: 62.50%] [G loss: 2.187171]\n",
      "epoch:7 step:7274 [D loss: 0.628050, acc: 67.97%] [G loss: 2.095632]\n",
      "epoch:7 step:7275 [D loss: 0.626657, acc: 63.28%] [G loss: 2.204530]\n",
      "epoch:7 step:7276 [D loss: 0.690846, acc: 56.25%] [G loss: 2.081313]\n",
      "epoch:7 step:7277 [D loss: 0.645456, acc: 66.41%] [G loss: 2.393978]\n",
      "epoch:7 step:7278 [D loss: 0.634035, acc: 69.53%] [G loss: 2.253444]\n",
      "epoch:7 step:7279 [D loss: 0.623831, acc: 59.38%] [G loss: 2.160111]\n",
      "epoch:7 step:7280 [D loss: 0.629808, acc: 65.62%] [G loss: 2.250403]\n",
      "epoch:7 step:7281 [D loss: 0.633493, acc: 66.41%] [G loss: 2.117177]\n",
      "epoch:7 step:7282 [D loss: 0.688374, acc: 60.16%] [G loss: 2.178994]\n",
      "epoch:7 step:7283 [D loss: 0.614653, acc: 70.31%] [G loss: 2.434529]\n",
      "epoch:7 step:7284 [D loss: 0.594749, acc: 64.84%] [G loss: 2.395413]\n",
      "epoch:7 step:7285 [D loss: 0.611592, acc: 61.72%] [G loss: 2.203550]\n",
      "epoch:7 step:7286 [D loss: 0.611837, acc: 64.84%] [G loss: 2.093554]\n",
      "epoch:7 step:7287 [D loss: 0.607127, acc: 65.62%] [G loss: 2.433114]\n",
      "epoch:7 step:7288 [D loss: 0.667739, acc: 58.59%] [G loss: 2.339093]\n",
      "epoch:7 step:7289 [D loss: 0.647073, acc: 63.28%] [G loss: 2.079458]\n",
      "epoch:7 step:7290 [D loss: 0.607317, acc: 70.31%] [G loss: 2.060055]\n",
      "epoch:7 step:7291 [D loss: 0.611533, acc: 63.28%] [G loss: 2.115417]\n",
      "epoch:7 step:7292 [D loss: 0.629732, acc: 63.28%] [G loss: 2.151008]\n",
      "epoch:7 step:7293 [D loss: 0.613450, acc: 65.62%] [G loss: 2.112939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7294 [D loss: 0.676297, acc: 60.16%] [G loss: 2.028538]\n",
      "epoch:7 step:7295 [D loss: 0.578361, acc: 71.09%] [G loss: 2.222821]\n",
      "epoch:7 step:7296 [D loss: 0.575806, acc: 70.31%] [G loss: 2.207295]\n",
      "epoch:7 step:7297 [D loss: 0.590104, acc: 74.22%] [G loss: 2.058735]\n",
      "epoch:7 step:7298 [D loss: 0.732864, acc: 53.91%] [G loss: 2.009298]\n",
      "epoch:7 step:7299 [D loss: 0.601574, acc: 64.84%] [G loss: 2.154559]\n",
      "epoch:7 step:7300 [D loss: 0.717057, acc: 57.03%] [G loss: 2.219900]\n",
      "epoch:7 step:7301 [D loss: 0.615660, acc: 65.62%] [G loss: 2.080049]\n",
      "epoch:7 step:7302 [D loss: 0.598845, acc: 72.66%] [G loss: 2.257371]\n",
      "epoch:7 step:7303 [D loss: 0.629730, acc: 63.28%] [G loss: 2.161274]\n",
      "epoch:7 step:7304 [D loss: 0.658786, acc: 63.28%] [G loss: 2.251302]\n",
      "epoch:7 step:7305 [D loss: 0.568607, acc: 70.31%] [G loss: 2.333630]\n",
      "epoch:7 step:7306 [D loss: 0.515851, acc: 72.66%] [G loss: 2.346111]\n",
      "epoch:7 step:7307 [D loss: 0.644125, acc: 68.75%] [G loss: 2.263692]\n",
      "epoch:7 step:7308 [D loss: 0.608734, acc: 67.19%] [G loss: 2.238707]\n",
      "epoch:7 step:7309 [D loss: 0.632690, acc: 64.84%] [G loss: 2.232429]\n",
      "epoch:7 step:7310 [D loss: 0.659786, acc: 59.38%] [G loss: 2.193985]\n",
      "epoch:7 step:7311 [D loss: 0.599119, acc: 66.41%] [G loss: 2.082666]\n",
      "epoch:7 step:7312 [D loss: 0.614490, acc: 68.75%] [G loss: 2.239608]\n",
      "epoch:7 step:7313 [D loss: 0.634996, acc: 63.28%] [G loss: 2.240158]\n",
      "epoch:7 step:7314 [D loss: 0.637487, acc: 60.94%] [G loss: 2.166518]\n",
      "epoch:7 step:7315 [D loss: 0.629748, acc: 65.62%] [G loss: 2.080266]\n",
      "epoch:7 step:7316 [D loss: 0.604609, acc: 67.97%] [G loss: 2.416573]\n",
      "epoch:7 step:7317 [D loss: 0.658587, acc: 63.28%] [G loss: 2.198100]\n",
      "epoch:7 step:7318 [D loss: 0.629528, acc: 63.28%] [G loss: 2.037553]\n",
      "epoch:7 step:7319 [D loss: 0.620706, acc: 61.72%] [G loss: 2.000951]\n",
      "epoch:7 step:7320 [D loss: 0.607436, acc: 60.16%] [G loss: 2.218978]\n",
      "epoch:7 step:7321 [D loss: 0.659964, acc: 57.81%] [G loss: 2.098146]\n",
      "epoch:7 step:7322 [D loss: 0.640119, acc: 57.81%] [G loss: 2.257513]\n",
      "epoch:7 step:7323 [D loss: 0.627298, acc: 64.84%] [G loss: 1.977137]\n",
      "epoch:7 step:7324 [D loss: 0.580972, acc: 68.75%] [G loss: 1.921448]\n",
      "epoch:7 step:7325 [D loss: 0.723799, acc: 54.69%] [G loss: 1.957150]\n",
      "epoch:7 step:7326 [D loss: 0.666937, acc: 66.41%] [G loss: 2.286810]\n",
      "epoch:7 step:7327 [D loss: 0.671613, acc: 62.50%] [G loss: 2.163339]\n",
      "epoch:7 step:7328 [D loss: 0.545249, acc: 72.66%] [G loss: 2.376323]\n",
      "epoch:7 step:7329 [D loss: 0.689229, acc: 55.47%] [G loss: 2.135775]\n",
      "epoch:7 step:7330 [D loss: 0.617563, acc: 60.94%] [G loss: 2.102876]\n",
      "epoch:7 step:7331 [D loss: 0.682551, acc: 62.50%] [G loss: 2.214981]\n",
      "epoch:7 step:7332 [D loss: 0.637069, acc: 64.06%] [G loss: 2.165552]\n",
      "epoch:7 step:7333 [D loss: 0.561074, acc: 71.09%] [G loss: 2.636533]\n",
      "epoch:7 step:7334 [D loss: 0.597088, acc: 68.75%] [G loss: 2.370616]\n",
      "epoch:7 step:7335 [D loss: 0.602311, acc: 68.75%] [G loss: 2.190679]\n",
      "epoch:7 step:7336 [D loss: 0.545364, acc: 73.44%] [G loss: 2.240757]\n",
      "epoch:7 step:7337 [D loss: 0.600884, acc: 68.75%] [G loss: 2.109622]\n",
      "epoch:7 step:7338 [D loss: 0.679013, acc: 63.28%] [G loss: 2.370459]\n",
      "epoch:7 step:7339 [D loss: 0.631450, acc: 67.19%] [G loss: 2.198673]\n",
      "epoch:7 step:7340 [D loss: 0.551700, acc: 71.88%] [G loss: 2.379047]\n",
      "epoch:7 step:7341 [D loss: 0.570158, acc: 71.88%] [G loss: 2.328003]\n",
      "epoch:7 step:7342 [D loss: 0.626906, acc: 60.94%] [G loss: 2.279043]\n",
      "epoch:7 step:7343 [D loss: 0.744249, acc: 58.59%] [G loss: 1.867531]\n",
      "epoch:7 step:7344 [D loss: 0.621109, acc: 66.41%] [G loss: 2.062931]\n",
      "epoch:7 step:7345 [D loss: 0.598922, acc: 67.19%] [G loss: 2.227382]\n",
      "epoch:7 step:7346 [D loss: 0.649614, acc: 58.59%] [G loss: 2.013049]\n",
      "epoch:7 step:7347 [D loss: 0.647539, acc: 59.38%] [G loss: 2.250415]\n",
      "epoch:7 step:7348 [D loss: 0.634800, acc: 63.28%] [G loss: 2.185136]\n",
      "epoch:7 step:7349 [D loss: 0.628030, acc: 61.72%] [G loss: 2.255137]\n",
      "epoch:7 step:7350 [D loss: 0.581174, acc: 70.31%] [G loss: 2.246678]\n",
      "epoch:7 step:7351 [D loss: 0.596452, acc: 67.19%] [G loss: 2.316617]\n",
      "epoch:7 step:7352 [D loss: 0.666822, acc: 57.03%] [G loss: 2.195523]\n",
      "epoch:7 step:7353 [D loss: 0.672245, acc: 60.16%] [G loss: 2.002825]\n",
      "epoch:7 step:7354 [D loss: 0.708069, acc: 53.91%] [G loss: 2.037438]\n",
      "epoch:7 step:7355 [D loss: 0.579548, acc: 67.97%] [G loss: 2.177330]\n",
      "epoch:7 step:7356 [D loss: 0.623686, acc: 70.31%] [G loss: 2.229528]\n",
      "epoch:7 step:7357 [D loss: 0.655898, acc: 64.06%] [G loss: 2.150862]\n",
      "epoch:7 step:7358 [D loss: 0.626706, acc: 63.28%] [G loss: 2.019457]\n",
      "epoch:7 step:7359 [D loss: 0.667327, acc: 59.38%] [G loss: 2.102791]\n",
      "epoch:7 step:7360 [D loss: 0.639329, acc: 63.28%] [G loss: 2.086590]\n",
      "epoch:7 step:7361 [D loss: 0.599574, acc: 63.28%] [G loss: 2.125206]\n",
      "epoch:7 step:7362 [D loss: 0.574507, acc: 69.53%] [G loss: 2.313969]\n",
      "epoch:7 step:7363 [D loss: 0.670453, acc: 54.69%] [G loss: 2.133166]\n",
      "epoch:7 step:7364 [D loss: 0.629774, acc: 67.19%] [G loss: 2.507394]\n",
      "epoch:7 step:7365 [D loss: 0.641350, acc: 66.41%] [G loss: 2.204730]\n",
      "epoch:7 step:7366 [D loss: 0.558148, acc: 70.31%] [G loss: 2.379811]\n",
      "epoch:7 step:7367 [D loss: 0.641886, acc: 69.53%] [G loss: 2.490917]\n",
      "epoch:7 step:7368 [D loss: 0.615122, acc: 70.31%] [G loss: 2.227774]\n",
      "epoch:7 step:7369 [D loss: 0.656051, acc: 64.06%] [G loss: 2.288560]\n",
      "epoch:7 step:7370 [D loss: 0.581368, acc: 72.66%] [G loss: 2.116424]\n",
      "epoch:7 step:7371 [D loss: 0.619681, acc: 71.09%] [G loss: 2.086426]\n",
      "epoch:7 step:7372 [D loss: 0.643915, acc: 64.84%] [G loss: 2.277692]\n",
      "epoch:7 step:7373 [D loss: 0.650607, acc: 60.94%] [G loss: 2.094949]\n",
      "epoch:7 step:7374 [D loss: 0.614439, acc: 66.41%] [G loss: 2.601291]\n",
      "epoch:7 step:7375 [D loss: 0.653325, acc: 62.50%] [G loss: 2.306402]\n",
      "epoch:7 step:7376 [D loss: 0.578403, acc: 71.88%] [G loss: 2.215363]\n",
      "epoch:7 step:7377 [D loss: 0.690059, acc: 61.72%] [G loss: 1.878391]\n",
      "epoch:7 step:7378 [D loss: 0.625195, acc: 67.19%] [G loss: 2.198692]\n",
      "epoch:7 step:7379 [D loss: 0.645029, acc: 63.28%] [G loss: 1.987786]\n",
      "epoch:7 step:7380 [D loss: 0.623390, acc: 69.53%] [G loss: 2.137920]\n",
      "epoch:7 step:7381 [D loss: 0.665971, acc: 59.38%] [G loss: 2.054492]\n",
      "epoch:7 step:7382 [D loss: 0.565355, acc: 70.31%] [G loss: 2.319430]\n",
      "epoch:7 step:7383 [D loss: 0.650002, acc: 57.81%] [G loss: 2.105437]\n",
      "epoch:7 step:7384 [D loss: 0.594150, acc: 68.75%] [G loss: 2.334832]\n",
      "epoch:7 step:7385 [D loss: 0.654683, acc: 61.72%] [G loss: 2.161695]\n",
      "epoch:7 step:7386 [D loss: 0.604568, acc: 67.97%] [G loss: 2.063168]\n",
      "epoch:7 step:7387 [D loss: 0.703424, acc: 54.69%] [G loss: 1.961946]\n",
      "epoch:7 step:7388 [D loss: 0.656148, acc: 64.06%] [G loss: 1.865133]\n",
      "epoch:7 step:7389 [D loss: 0.588856, acc: 69.53%] [G loss: 2.263537]\n",
      "epoch:7 step:7390 [D loss: 0.605535, acc: 67.19%] [G loss: 2.178491]\n",
      "epoch:7 step:7391 [D loss: 0.623605, acc: 64.06%] [G loss: 2.320629]\n",
      "epoch:7 step:7392 [D loss: 0.634159, acc: 64.06%] [G loss: 2.245229]\n",
      "epoch:7 step:7393 [D loss: 0.601497, acc: 66.41%] [G loss: 2.152150]\n",
      "epoch:7 step:7394 [D loss: 0.675109, acc: 61.72%] [G loss: 2.244772]\n",
      "epoch:7 step:7395 [D loss: 0.648524, acc: 60.94%] [G loss: 2.248686]\n",
      "epoch:7 step:7396 [D loss: 0.550721, acc: 71.88%] [G loss: 2.166699]\n",
      "epoch:7 step:7397 [D loss: 0.590133, acc: 67.19%] [G loss: 2.398023]\n",
      "epoch:7 step:7398 [D loss: 0.560534, acc: 74.22%] [G loss: 2.281132]\n",
      "epoch:7 step:7399 [D loss: 0.550670, acc: 71.88%] [G loss: 2.427421]\n",
      "epoch:7 step:7400 [D loss: 0.619326, acc: 66.41%] [G loss: 2.195686]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.371435\n",
      "FID: 29.757458\n",
      "0 = 12.860142484092712\n",
      "1 = 0.08141641447196746\n",
      "2 = 0.9071000218391418\n",
      "3 = 0.9214000105857849\n",
      "4 = 0.892799973487854\n",
      "5 = 0.8957806825637817\n",
      "6 = 0.9214000105857849\n",
      "7 = 7.858766412019704\n",
      "8 = 0.10774376931286853\n",
      "9 = 0.7825999855995178\n",
      "10 = 0.7871999740600586\n",
      "11 = 0.777999997138977\n",
      "12 = 0.7800237536430359\n",
      "13 = 0.7871999740600586\n",
      "14 = 6.371458053588867\n",
      "15 = 9.235893249511719\n",
      "16 = 0.17462415993213654\n",
      "17 = 6.371435165405273\n",
      "18 = 29.757457733154297\n",
      "epoch:7 step:7401 [D loss: 0.565948, acc: 73.44%] [G loss: 2.388991]\n",
      "epoch:7 step:7402 [D loss: 0.611054, acc: 67.19%] [G loss: 2.491727]\n",
      "epoch:7 step:7403 [D loss: 0.614525, acc: 67.97%] [G loss: 2.256634]\n",
      "epoch:7 step:7404 [D loss: 0.660151, acc: 65.62%] [G loss: 2.488199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7405 [D loss: 0.625664, acc: 61.72%] [G loss: 2.143784]\n",
      "epoch:7 step:7406 [D loss: 0.605996, acc: 65.62%] [G loss: 2.326241]\n",
      "epoch:7 step:7407 [D loss: 0.597788, acc: 63.28%] [G loss: 2.143497]\n",
      "epoch:7 step:7408 [D loss: 0.594726, acc: 65.62%] [G loss: 2.314996]\n",
      "epoch:7 step:7409 [D loss: 0.713715, acc: 63.28%] [G loss: 2.213038]\n",
      "epoch:7 step:7410 [D loss: 0.615836, acc: 68.75%] [G loss: 2.166213]\n",
      "epoch:7 step:7411 [D loss: 0.594379, acc: 68.75%] [G loss: 2.140108]\n",
      "epoch:7 step:7412 [D loss: 0.629272, acc: 61.72%] [G loss: 2.262722]\n",
      "epoch:7 step:7413 [D loss: 0.621178, acc: 68.75%] [G loss: 2.237703]\n",
      "epoch:7 step:7414 [D loss: 0.635432, acc: 63.28%] [G loss: 2.063984]\n",
      "epoch:7 step:7415 [D loss: 0.692115, acc: 55.47%] [G loss: 1.925410]\n",
      "epoch:7 step:7416 [D loss: 0.575930, acc: 71.88%] [G loss: 2.349992]\n",
      "epoch:7 step:7417 [D loss: 0.698878, acc: 54.69%] [G loss: 1.857520]\n",
      "epoch:7 step:7418 [D loss: 0.630605, acc: 63.28%] [G loss: 2.174503]\n",
      "epoch:7 step:7419 [D loss: 0.607443, acc: 67.97%] [G loss: 2.157032]\n",
      "epoch:7 step:7420 [D loss: 0.645115, acc: 64.06%] [G loss: 2.242241]\n",
      "epoch:7 step:7421 [D loss: 0.628534, acc: 66.41%] [G loss: 2.171584]\n",
      "epoch:7 step:7422 [D loss: 0.538959, acc: 71.09%] [G loss: 2.184460]\n",
      "epoch:7 step:7423 [D loss: 0.633702, acc: 60.16%] [G loss: 2.050375]\n",
      "epoch:7 step:7424 [D loss: 0.661298, acc: 60.16%] [G loss: 2.269296]\n",
      "epoch:7 step:7425 [D loss: 0.603720, acc: 64.84%] [G loss: 2.295204]\n",
      "epoch:7 step:7426 [D loss: 0.658172, acc: 60.16%] [G loss: 1.976616]\n",
      "epoch:7 step:7427 [D loss: 0.666463, acc: 60.94%] [G loss: 2.087510]\n",
      "epoch:7 step:7428 [D loss: 0.636458, acc: 67.19%] [G loss: 2.113371]\n",
      "epoch:7 step:7429 [D loss: 0.610104, acc: 69.53%] [G loss: 2.226541]\n",
      "epoch:7 step:7430 [D loss: 0.617151, acc: 64.84%] [G loss: 2.050663]\n",
      "epoch:7 step:7431 [D loss: 0.616458, acc: 67.19%] [G loss: 2.077630]\n",
      "epoch:7 step:7432 [D loss: 0.669037, acc: 66.41%] [G loss: 1.897510]\n",
      "epoch:7 step:7433 [D loss: 0.684726, acc: 54.69%] [G loss: 2.008143]\n",
      "epoch:7 step:7434 [D loss: 0.634108, acc: 64.06%] [G loss: 2.176380]\n",
      "epoch:7 step:7435 [D loss: 0.650091, acc: 61.72%] [G loss: 2.034481]\n",
      "epoch:7 step:7436 [D loss: 0.651899, acc: 61.72%] [G loss: 2.101657]\n",
      "epoch:7 step:7437 [D loss: 0.676730, acc: 56.25%] [G loss: 2.037348]\n",
      "epoch:7 step:7438 [D loss: 0.625535, acc: 64.06%] [G loss: 2.090668]\n",
      "epoch:7 step:7439 [D loss: 0.622738, acc: 66.41%] [G loss: 1.936416]\n",
      "epoch:7 step:7440 [D loss: 0.556495, acc: 71.88%] [G loss: 2.109562]\n",
      "epoch:7 step:7441 [D loss: 0.621336, acc: 64.84%] [G loss: 2.126320]\n",
      "epoch:7 step:7442 [D loss: 0.595034, acc: 69.53%] [G loss: 2.179121]\n",
      "epoch:7 step:7443 [D loss: 0.549474, acc: 73.44%] [G loss: 2.585562]\n",
      "epoch:7 step:7444 [D loss: 0.668896, acc: 67.19%] [G loss: 2.301885]\n",
      "epoch:7 step:7445 [D loss: 0.605053, acc: 60.94%] [G loss: 2.393448]\n",
      "epoch:7 step:7446 [D loss: 0.563535, acc: 76.56%] [G loss: 2.260118]\n",
      "epoch:7 step:7447 [D loss: 0.650166, acc: 64.06%] [G loss: 2.156809]\n",
      "epoch:7 step:7448 [D loss: 0.630048, acc: 61.72%] [G loss: 2.216432]\n",
      "epoch:7 step:7449 [D loss: 0.607935, acc: 62.50%] [G loss: 2.287693]\n",
      "epoch:7 step:7450 [D loss: 0.640699, acc: 61.72%] [G loss: 2.320354]\n",
      "epoch:7 step:7451 [D loss: 0.662919, acc: 60.94%] [G loss: 2.119144]\n",
      "epoch:7 step:7452 [D loss: 0.688869, acc: 60.16%] [G loss: 2.094607]\n",
      "epoch:7 step:7453 [D loss: 0.638122, acc: 68.75%] [G loss: 2.349183]\n",
      "epoch:7 step:7454 [D loss: 0.642105, acc: 65.62%] [G loss: 2.149716]\n",
      "epoch:7 step:7455 [D loss: 0.621860, acc: 62.50%] [G loss: 1.986689]\n",
      "epoch:7 step:7456 [D loss: 0.673068, acc: 62.50%] [G loss: 2.220115]\n",
      "epoch:7 step:7457 [D loss: 0.630583, acc: 71.88%] [G loss: 1.956442]\n",
      "epoch:7 step:7458 [D loss: 0.627674, acc: 70.31%] [G loss: 2.236883]\n",
      "epoch:7 step:7459 [D loss: 0.541711, acc: 78.12%] [G loss: 2.418313]\n",
      "epoch:7 step:7460 [D loss: 0.641373, acc: 62.50%] [G loss: 2.205333]\n",
      "epoch:7 step:7461 [D loss: 0.637030, acc: 67.19%] [G loss: 1.972151]\n",
      "epoch:7 step:7462 [D loss: 0.573305, acc: 67.97%] [G loss: 2.314745]\n",
      "epoch:7 step:7463 [D loss: 0.557496, acc: 71.88%] [G loss: 2.370713]\n",
      "epoch:7 step:7464 [D loss: 0.566711, acc: 71.09%] [G loss: 2.232148]\n",
      "epoch:7 step:7465 [D loss: 0.591818, acc: 71.09%] [G loss: 2.269717]\n",
      "epoch:7 step:7466 [D loss: 0.625667, acc: 61.72%] [G loss: 2.059509]\n",
      "epoch:7 step:7467 [D loss: 0.615602, acc: 69.53%] [G loss: 2.274798]\n",
      "epoch:7 step:7468 [D loss: 0.581655, acc: 70.31%] [G loss: 2.356810]\n",
      "epoch:7 step:7469 [D loss: 0.626929, acc: 63.28%] [G loss: 2.387896]\n",
      "epoch:7 step:7470 [D loss: 0.545739, acc: 72.66%] [G loss: 2.537502]\n",
      "epoch:7 step:7471 [D loss: 0.548496, acc: 77.34%] [G loss: 2.735237]\n",
      "epoch:7 step:7472 [D loss: 0.611036, acc: 66.41%] [G loss: 2.286323]\n",
      "epoch:7 step:7473 [D loss: 0.651589, acc: 62.50%] [G loss: 2.164371]\n",
      "epoch:7 step:7474 [D loss: 0.585782, acc: 65.62%] [G loss: 2.163367]\n",
      "epoch:7 step:7475 [D loss: 0.667031, acc: 57.81%] [G loss: 2.112298]\n",
      "epoch:7 step:7476 [D loss: 0.634836, acc: 65.62%] [G loss: 2.204499]\n",
      "epoch:7 step:7477 [D loss: 0.590751, acc: 67.97%] [G loss: 2.424731]\n",
      "epoch:7 step:7478 [D loss: 0.590068, acc: 70.31%] [G loss: 2.487302]\n",
      "epoch:7 step:7479 [D loss: 0.660951, acc: 62.50%] [G loss: 2.386432]\n",
      "epoch:7 step:7480 [D loss: 0.622465, acc: 61.72%] [G loss: 2.396509]\n",
      "epoch:7 step:7481 [D loss: 0.646587, acc: 60.94%] [G loss: 2.280803]\n",
      "epoch:7 step:7482 [D loss: 0.533623, acc: 78.12%] [G loss: 2.269164]\n",
      "epoch:7 step:7483 [D loss: 0.533921, acc: 76.56%] [G loss: 2.515370]\n",
      "epoch:7 step:7484 [D loss: 0.478164, acc: 82.03%] [G loss: 2.531383]\n",
      "epoch:7 step:7485 [D loss: 0.565636, acc: 68.75%] [G loss: 2.373149]\n",
      "epoch:7 step:7486 [D loss: 0.589622, acc: 65.62%] [G loss: 2.526800]\n",
      "epoch:7 step:7487 [D loss: 0.765854, acc: 59.38%] [G loss: 2.103812]\n",
      "epoch:7 step:7488 [D loss: 0.708695, acc: 59.38%] [G loss: 2.698300]\n",
      "epoch:7 step:7489 [D loss: 0.677967, acc: 64.84%] [G loss: 2.156598]\n",
      "epoch:7 step:7490 [D loss: 0.596148, acc: 68.75%] [G loss: 2.117640]\n",
      "epoch:7 step:7491 [D loss: 0.520753, acc: 76.56%] [G loss: 2.257037]\n",
      "epoch:7 step:7492 [D loss: 0.558614, acc: 71.88%] [G loss: 2.336581]\n",
      "epoch:7 step:7493 [D loss: 0.585906, acc: 67.97%] [G loss: 2.530513]\n",
      "epoch:7 step:7494 [D loss: 0.542246, acc: 77.34%] [G loss: 2.363153]\n",
      "epoch:7 step:7495 [D loss: 0.612939, acc: 66.41%] [G loss: 2.419194]\n",
      "epoch:7 step:7496 [D loss: 0.509278, acc: 78.12%] [G loss: 3.194756]\n",
      "epoch:8 step:7497 [D loss: 0.626557, acc: 67.19%] [G loss: 2.141303]\n",
      "epoch:8 step:7498 [D loss: 0.596891, acc: 71.88%] [G loss: 2.390537]\n",
      "epoch:8 step:7499 [D loss: 0.630851, acc: 60.16%] [G loss: 2.341601]\n",
      "epoch:8 step:7500 [D loss: 0.594856, acc: 62.50%] [G loss: 2.163979]\n",
      "epoch:8 step:7501 [D loss: 0.698309, acc: 59.38%] [G loss: 2.059062]\n",
      "epoch:8 step:7502 [D loss: 0.676101, acc: 60.16%] [G loss: 2.185360]\n",
      "epoch:8 step:7503 [D loss: 0.652732, acc: 60.94%] [G loss: 2.037779]\n",
      "epoch:8 step:7504 [D loss: 0.634176, acc: 60.94%] [G loss: 2.342465]\n",
      "epoch:8 step:7505 [D loss: 0.656366, acc: 64.06%] [G loss: 2.291919]\n",
      "epoch:8 step:7506 [D loss: 0.605612, acc: 67.19%] [G loss: 2.333706]\n",
      "epoch:8 step:7507 [D loss: 0.566083, acc: 66.41%] [G loss: 2.354521]\n",
      "epoch:8 step:7508 [D loss: 0.598518, acc: 67.97%] [G loss: 2.318162]\n",
      "epoch:8 step:7509 [D loss: 0.577969, acc: 74.22%] [G loss: 2.271235]\n",
      "epoch:8 step:7510 [D loss: 0.684690, acc: 60.16%] [G loss: 2.406672]\n",
      "epoch:8 step:7511 [D loss: 0.530170, acc: 73.44%] [G loss: 2.601362]\n",
      "epoch:8 step:7512 [D loss: 0.641867, acc: 65.62%] [G loss: 2.573384]\n",
      "epoch:8 step:7513 [D loss: 0.703811, acc: 57.81%] [G loss: 2.234703]\n",
      "epoch:8 step:7514 [D loss: 0.633686, acc: 64.06%] [G loss: 2.225958]\n",
      "epoch:8 step:7515 [D loss: 0.618955, acc: 68.75%] [G loss: 2.298881]\n",
      "epoch:8 step:7516 [D loss: 0.687832, acc: 57.03%] [G loss: 2.056278]\n",
      "epoch:8 step:7517 [D loss: 0.617484, acc: 65.62%] [G loss: 2.090550]\n",
      "epoch:8 step:7518 [D loss: 0.633695, acc: 67.19%] [G loss: 2.152851]\n",
      "epoch:8 step:7519 [D loss: 0.597759, acc: 67.97%] [G loss: 2.424129]\n",
      "epoch:8 step:7520 [D loss: 0.599507, acc: 71.88%] [G loss: 2.312539]\n",
      "epoch:8 step:7521 [D loss: 0.540007, acc: 74.22%] [G loss: 2.435280]\n",
      "epoch:8 step:7522 [D loss: 0.648940, acc: 64.06%] [G loss: 2.087550]\n",
      "epoch:8 step:7523 [D loss: 0.575823, acc: 72.66%] [G loss: 2.195243]\n",
      "epoch:8 step:7524 [D loss: 0.659351, acc: 64.06%] [G loss: 2.057133]\n",
      "epoch:8 step:7525 [D loss: 0.562132, acc: 71.09%] [G loss: 2.052759]\n",
      "epoch:8 step:7526 [D loss: 0.639076, acc: 62.50%] [G loss: 2.177984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7527 [D loss: 0.658167, acc: 64.06%] [G loss: 2.066873]\n",
      "epoch:8 step:7528 [D loss: 0.616054, acc: 71.88%] [G loss: 2.117250]\n",
      "epoch:8 step:7529 [D loss: 0.627907, acc: 62.50%] [G loss: 1.970030]\n",
      "epoch:8 step:7530 [D loss: 0.613075, acc: 66.41%] [G loss: 2.173320]\n",
      "epoch:8 step:7531 [D loss: 0.643893, acc: 60.94%] [G loss: 1.987347]\n",
      "epoch:8 step:7532 [D loss: 0.572417, acc: 75.00%] [G loss: 2.431870]\n",
      "epoch:8 step:7533 [D loss: 0.630743, acc: 62.50%] [G loss: 2.096883]\n",
      "epoch:8 step:7534 [D loss: 0.657388, acc: 63.28%] [G loss: 2.345377]\n",
      "epoch:8 step:7535 [D loss: 0.661339, acc: 59.38%] [G loss: 2.331801]\n",
      "epoch:8 step:7536 [D loss: 0.540166, acc: 71.09%] [G loss: 2.447835]\n",
      "epoch:8 step:7537 [D loss: 0.641658, acc: 61.72%] [G loss: 2.160630]\n",
      "epoch:8 step:7538 [D loss: 0.555553, acc: 73.44%] [G loss: 2.220072]\n",
      "epoch:8 step:7539 [D loss: 0.626226, acc: 64.06%] [G loss: 2.366878]\n",
      "epoch:8 step:7540 [D loss: 0.643019, acc: 60.16%] [G loss: 2.072728]\n",
      "epoch:8 step:7541 [D loss: 0.659583, acc: 61.72%] [G loss: 2.356514]\n",
      "epoch:8 step:7542 [D loss: 0.642777, acc: 67.97%] [G loss: 2.163633]\n",
      "epoch:8 step:7543 [D loss: 0.638509, acc: 64.06%] [G loss: 2.427779]\n",
      "epoch:8 step:7544 [D loss: 0.596847, acc: 61.72%] [G loss: 2.133588]\n",
      "epoch:8 step:7545 [D loss: 0.633395, acc: 60.16%] [G loss: 2.209821]\n",
      "epoch:8 step:7546 [D loss: 0.538943, acc: 73.44%] [G loss: 2.286885]\n",
      "epoch:8 step:7547 [D loss: 0.603242, acc: 71.09%] [G loss: 2.093834]\n",
      "epoch:8 step:7548 [D loss: 0.589862, acc: 67.97%] [G loss: 1.995585]\n",
      "epoch:8 step:7549 [D loss: 0.634289, acc: 64.84%] [G loss: 2.169083]\n",
      "epoch:8 step:7550 [D loss: 0.559652, acc: 72.66%] [G loss: 2.364218]\n",
      "epoch:8 step:7551 [D loss: 0.657768, acc: 61.72%] [G loss: 2.448184]\n",
      "epoch:8 step:7552 [D loss: 0.557826, acc: 70.31%] [G loss: 2.715597]\n",
      "epoch:8 step:7553 [D loss: 0.652082, acc: 64.84%] [G loss: 2.288243]\n",
      "epoch:8 step:7554 [D loss: 0.581990, acc: 67.19%] [G loss: 2.238699]\n",
      "epoch:8 step:7555 [D loss: 0.632343, acc: 66.41%] [G loss: 2.257522]\n",
      "epoch:8 step:7556 [D loss: 0.689111, acc: 61.72%] [G loss: 2.175791]\n",
      "epoch:8 step:7557 [D loss: 0.672924, acc: 60.94%] [G loss: 2.200470]\n",
      "epoch:8 step:7558 [D loss: 0.633358, acc: 64.06%] [G loss: 2.285544]\n",
      "epoch:8 step:7559 [D loss: 0.572950, acc: 69.53%] [G loss: 2.081711]\n",
      "epoch:8 step:7560 [D loss: 0.665898, acc: 61.72%] [G loss: 2.280734]\n",
      "epoch:8 step:7561 [D loss: 0.638102, acc: 64.06%] [G loss: 2.233325]\n",
      "epoch:8 step:7562 [D loss: 0.623684, acc: 66.41%] [G loss: 2.103052]\n",
      "epoch:8 step:7563 [D loss: 0.577282, acc: 72.66%] [G loss: 2.236986]\n",
      "epoch:8 step:7564 [D loss: 0.585295, acc: 70.31%] [G loss: 2.054961]\n",
      "epoch:8 step:7565 [D loss: 0.610662, acc: 68.75%] [G loss: 2.308354]\n",
      "epoch:8 step:7566 [D loss: 0.629166, acc: 65.62%] [G loss: 2.304014]\n",
      "epoch:8 step:7567 [D loss: 0.700317, acc: 62.50%] [G loss: 2.181878]\n",
      "epoch:8 step:7568 [D loss: 0.687449, acc: 57.03%] [G loss: 2.275549]\n",
      "epoch:8 step:7569 [D loss: 0.623546, acc: 67.19%] [G loss: 2.016379]\n",
      "epoch:8 step:7570 [D loss: 0.596865, acc: 67.97%] [G loss: 2.319490]\n",
      "epoch:8 step:7571 [D loss: 0.567827, acc: 67.97%] [G loss: 2.440705]\n",
      "epoch:8 step:7572 [D loss: 0.560271, acc: 68.75%] [G loss: 2.400529]\n",
      "epoch:8 step:7573 [D loss: 0.573819, acc: 70.31%] [G loss: 2.446319]\n",
      "epoch:8 step:7574 [D loss: 0.618791, acc: 65.62%] [G loss: 2.024801]\n",
      "epoch:8 step:7575 [D loss: 0.634861, acc: 64.84%] [G loss: 2.174813]\n",
      "epoch:8 step:7576 [D loss: 0.652990, acc: 60.16%] [G loss: 2.098844]\n",
      "epoch:8 step:7577 [D loss: 0.620196, acc: 68.75%] [G loss: 2.119338]\n",
      "epoch:8 step:7578 [D loss: 0.624345, acc: 70.31%] [G loss: 2.216413]\n",
      "epoch:8 step:7579 [D loss: 0.625462, acc: 68.75%] [G loss: 2.397633]\n",
      "epoch:8 step:7580 [D loss: 0.635937, acc: 60.94%] [G loss: 2.244223]\n",
      "epoch:8 step:7581 [D loss: 0.605596, acc: 65.62%] [G loss: 2.020099]\n",
      "epoch:8 step:7582 [D loss: 0.627871, acc: 59.38%] [G loss: 2.082473]\n",
      "epoch:8 step:7583 [D loss: 0.576708, acc: 71.88%] [G loss: 2.016924]\n",
      "epoch:8 step:7584 [D loss: 0.651438, acc: 62.50%] [G loss: 2.052734]\n",
      "epoch:8 step:7585 [D loss: 0.571064, acc: 73.44%] [G loss: 2.194731]\n",
      "epoch:8 step:7586 [D loss: 0.625232, acc: 64.84%] [G loss: 2.207597]\n",
      "epoch:8 step:7587 [D loss: 0.606978, acc: 72.66%] [G loss: 2.147030]\n",
      "epoch:8 step:7588 [D loss: 0.546179, acc: 73.44%] [G loss: 2.440346]\n",
      "epoch:8 step:7589 [D loss: 0.582137, acc: 71.09%] [G loss: 2.537272]\n",
      "epoch:8 step:7590 [D loss: 0.575520, acc: 68.75%] [G loss: 2.373971]\n",
      "epoch:8 step:7591 [D loss: 0.665783, acc: 55.47%] [G loss: 2.406152]\n",
      "epoch:8 step:7592 [D loss: 0.596344, acc: 63.28%] [G loss: 2.266779]\n",
      "epoch:8 step:7593 [D loss: 0.642890, acc: 63.28%] [G loss: 2.361824]\n",
      "epoch:8 step:7594 [D loss: 0.662263, acc: 57.81%] [G loss: 2.038692]\n",
      "epoch:8 step:7595 [D loss: 0.567020, acc: 71.88%] [G loss: 2.381969]\n",
      "epoch:8 step:7596 [D loss: 0.630936, acc: 63.28%] [G loss: 2.648534]\n",
      "epoch:8 step:7597 [D loss: 0.569700, acc: 69.53%] [G loss: 2.227285]\n",
      "epoch:8 step:7598 [D loss: 0.640743, acc: 66.41%] [G loss: 2.170159]\n",
      "epoch:8 step:7599 [D loss: 0.545015, acc: 70.31%] [G loss: 2.394705]\n",
      "epoch:8 step:7600 [D loss: 0.628671, acc: 61.72%] [G loss: 2.219099]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.381917\n",
      "FID: 30.589474\n",
      "0 = 13.118206376171106\n",
      "1 = 0.09239424459359466\n",
      "2 = 0.9121999740600586\n",
      "3 = 0.9174000024795532\n",
      "4 = 0.9070000052452087\n",
      "5 = 0.9079572558403015\n",
      "6 = 0.9174000024795532\n",
      "7 = 7.916360215687773\n",
      "8 = 0.11018778031822549\n",
      "9 = 0.7936999797821045\n",
      "10 = 0.8019999861717224\n",
      "11 = 0.7853999733924866\n",
      "12 = 0.7889041900634766\n",
      "13 = 0.8019999861717224\n",
      "14 = 6.381943702697754\n",
      "15 = 9.183740615844727\n",
      "16 = 0.18683435022830963\n",
      "17 = 6.3819169998168945\n",
      "18 = 30.589473724365234\n",
      "epoch:8 step:7601 [D loss: 0.659875, acc: 58.59%] [G loss: 2.123174]\n",
      "epoch:8 step:7602 [D loss: 0.664484, acc: 63.28%] [G loss: 2.156750]\n",
      "epoch:8 step:7603 [D loss: 0.614174, acc: 64.06%] [G loss: 2.317737]\n",
      "epoch:8 step:7604 [D loss: 0.628231, acc: 67.19%] [G loss: 1.868043]\n",
      "epoch:8 step:7605 [D loss: 0.675053, acc: 63.28%] [G loss: 2.130985]\n",
      "epoch:8 step:7606 [D loss: 0.651261, acc: 62.50%] [G loss: 2.192832]\n",
      "epoch:8 step:7607 [D loss: 0.548713, acc: 73.44%] [G loss: 2.229282]\n",
      "epoch:8 step:7608 [D loss: 0.613668, acc: 64.06%] [G loss: 2.224425]\n",
      "epoch:8 step:7609 [D loss: 0.605855, acc: 66.41%] [G loss: 2.124828]\n",
      "epoch:8 step:7610 [D loss: 0.711485, acc: 53.12%] [G loss: 1.997174]\n",
      "epoch:8 step:7611 [D loss: 0.589535, acc: 69.53%] [G loss: 2.238313]\n",
      "epoch:8 step:7612 [D loss: 0.574578, acc: 64.84%] [G loss: 2.284115]\n",
      "epoch:8 step:7613 [D loss: 0.581078, acc: 67.19%] [G loss: 2.641180]\n",
      "epoch:8 step:7614 [D loss: 0.572127, acc: 70.31%] [G loss: 2.255422]\n",
      "epoch:8 step:7615 [D loss: 0.612373, acc: 68.75%] [G loss: 2.400107]\n",
      "epoch:8 step:7616 [D loss: 0.595072, acc: 67.19%] [G loss: 2.416364]\n",
      "epoch:8 step:7617 [D loss: 0.667583, acc: 57.03%] [G loss: 2.258016]\n",
      "epoch:8 step:7618 [D loss: 0.639145, acc: 64.06%] [G loss: 2.356538]\n",
      "epoch:8 step:7619 [D loss: 0.585049, acc: 68.75%] [G loss: 2.305011]\n",
      "epoch:8 step:7620 [D loss: 0.701567, acc: 58.59%] [G loss: 2.315693]\n",
      "epoch:8 step:7621 [D loss: 0.653904, acc: 61.72%] [G loss: 1.972321]\n",
      "epoch:8 step:7622 [D loss: 0.593045, acc: 69.53%] [G loss: 2.224598]\n",
      "epoch:8 step:7623 [D loss: 0.641663, acc: 64.06%] [G loss: 2.149945]\n",
      "epoch:8 step:7624 [D loss: 0.609116, acc: 67.97%] [G loss: 2.164472]\n",
      "epoch:8 step:7625 [D loss: 0.641675, acc: 59.38%] [G loss: 2.004960]\n",
      "epoch:8 step:7626 [D loss: 0.658229, acc: 64.84%] [G loss: 2.115422]\n",
      "epoch:8 step:7627 [D loss: 0.668213, acc: 55.47%] [G loss: 2.019520]\n",
      "epoch:8 step:7628 [D loss: 0.645479, acc: 64.84%] [G loss: 2.354733]\n",
      "epoch:8 step:7629 [D loss: 0.636525, acc: 63.28%] [G loss: 2.172747]\n",
      "epoch:8 step:7630 [D loss: 0.625431, acc: 63.28%] [G loss: 2.205374]\n",
      "epoch:8 step:7631 [D loss: 0.652966, acc: 69.53%] [G loss: 2.066164]\n",
      "epoch:8 step:7632 [D loss: 0.623488, acc: 67.19%] [G loss: 2.022807]\n",
      "epoch:8 step:7633 [D loss: 0.642889, acc: 60.94%] [G loss: 2.048311]\n",
      "epoch:8 step:7634 [D loss: 0.605246, acc: 67.19%] [G loss: 2.127554]\n",
      "epoch:8 step:7635 [D loss: 0.646142, acc: 62.50%] [G loss: 2.127398]\n",
      "epoch:8 step:7636 [D loss: 0.683855, acc: 55.47%] [G loss: 2.080859]\n",
      "epoch:8 step:7637 [D loss: 0.629560, acc: 61.72%] [G loss: 2.143083]\n",
      "epoch:8 step:7638 [D loss: 0.617092, acc: 64.06%] [G loss: 2.149192]\n",
      "epoch:8 step:7639 [D loss: 0.643861, acc: 64.84%] [G loss: 2.067954]\n",
      "epoch:8 step:7640 [D loss: 0.605713, acc: 70.31%] [G loss: 2.228078]\n",
      "epoch:8 step:7641 [D loss: 0.603634, acc: 67.19%] [G loss: 2.138722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7642 [D loss: 0.680290, acc: 63.28%] [G loss: 2.178102]\n",
      "epoch:8 step:7643 [D loss: 0.614223, acc: 64.06%] [G loss: 2.290127]\n",
      "epoch:8 step:7644 [D loss: 0.661452, acc: 59.38%] [G loss: 2.027070]\n",
      "epoch:8 step:7645 [D loss: 0.545829, acc: 74.22%] [G loss: 2.265179]\n",
      "epoch:8 step:7646 [D loss: 0.603491, acc: 67.19%] [G loss: 2.177119]\n",
      "epoch:8 step:7647 [D loss: 0.552230, acc: 73.44%] [G loss: 2.524289]\n",
      "epoch:8 step:7648 [D loss: 0.623562, acc: 65.62%] [G loss: 2.565675]\n",
      "epoch:8 step:7649 [D loss: 0.598638, acc: 68.75%] [G loss: 2.118166]\n",
      "epoch:8 step:7650 [D loss: 0.572322, acc: 75.78%] [G loss: 2.287521]\n",
      "epoch:8 step:7651 [D loss: 0.596895, acc: 68.75%] [G loss: 2.312077]\n",
      "epoch:8 step:7652 [D loss: 0.598854, acc: 67.97%] [G loss: 2.455752]\n",
      "epoch:8 step:7653 [D loss: 0.621718, acc: 71.09%] [G loss: 2.323873]\n",
      "epoch:8 step:7654 [D loss: 0.608823, acc: 65.62%] [G loss: 2.032434]\n",
      "epoch:8 step:7655 [D loss: 0.599399, acc: 69.53%] [G loss: 2.224573]\n",
      "epoch:8 step:7656 [D loss: 0.674138, acc: 59.38%] [G loss: 1.975081]\n",
      "epoch:8 step:7657 [D loss: 0.584272, acc: 66.41%] [G loss: 2.093764]\n",
      "epoch:8 step:7658 [D loss: 0.604314, acc: 66.41%] [G loss: 2.249590]\n",
      "epoch:8 step:7659 [D loss: 0.657993, acc: 58.59%] [G loss: 2.291685]\n",
      "epoch:8 step:7660 [D loss: 0.673846, acc: 61.72%] [G loss: 2.010512]\n",
      "epoch:8 step:7661 [D loss: 0.632838, acc: 70.31%] [G loss: 2.234749]\n",
      "epoch:8 step:7662 [D loss: 0.604061, acc: 69.53%] [G loss: 2.250260]\n",
      "epoch:8 step:7663 [D loss: 0.597312, acc: 67.19%] [G loss: 2.109434]\n",
      "epoch:8 step:7664 [D loss: 0.569987, acc: 70.31%] [G loss: 2.129363]\n",
      "epoch:8 step:7665 [D loss: 0.581878, acc: 71.09%] [G loss: 2.324805]\n",
      "epoch:8 step:7666 [D loss: 0.629901, acc: 61.72%] [G loss: 2.002009]\n",
      "epoch:8 step:7667 [D loss: 0.601986, acc: 68.75%] [G loss: 2.328215]\n",
      "epoch:8 step:7668 [D loss: 0.697891, acc: 59.38%] [G loss: 2.119745]\n",
      "epoch:8 step:7669 [D loss: 0.602547, acc: 64.84%] [G loss: 2.221529]\n",
      "epoch:8 step:7670 [D loss: 0.617922, acc: 70.31%] [G loss: 2.283847]\n",
      "epoch:8 step:7671 [D loss: 0.670050, acc: 60.16%] [G loss: 1.995799]\n",
      "epoch:8 step:7672 [D loss: 0.612842, acc: 65.62%] [G loss: 2.512536]\n",
      "epoch:8 step:7673 [D loss: 0.631650, acc: 64.06%] [G loss: 2.216197]\n",
      "epoch:8 step:7674 [D loss: 0.606785, acc: 64.06%] [G loss: 2.148535]\n",
      "epoch:8 step:7675 [D loss: 0.609771, acc: 67.97%] [G loss: 2.229281]\n",
      "epoch:8 step:7676 [D loss: 0.661111, acc: 60.94%] [G loss: 2.023414]\n",
      "epoch:8 step:7677 [D loss: 0.550291, acc: 73.44%] [G loss: 2.349657]\n",
      "epoch:8 step:7678 [D loss: 0.662921, acc: 58.59%] [G loss: 2.183479]\n",
      "epoch:8 step:7679 [D loss: 0.661136, acc: 64.06%] [G loss: 2.345281]\n",
      "epoch:8 step:7680 [D loss: 0.625345, acc: 64.06%] [G loss: 2.217694]\n",
      "epoch:8 step:7681 [D loss: 0.641820, acc: 63.28%] [G loss: 2.201633]\n",
      "epoch:8 step:7682 [D loss: 0.661696, acc: 62.50%] [G loss: 2.111361]\n",
      "epoch:8 step:7683 [D loss: 0.619902, acc: 64.84%] [G loss: 2.005929]\n",
      "epoch:8 step:7684 [D loss: 0.623696, acc: 64.06%] [G loss: 2.292338]\n",
      "epoch:8 step:7685 [D loss: 0.656818, acc: 62.50%] [G loss: 2.055642]\n",
      "epoch:8 step:7686 [D loss: 0.636538, acc: 61.72%] [G loss: 2.305509]\n",
      "epoch:8 step:7687 [D loss: 0.638979, acc: 64.06%] [G loss: 2.218163]\n",
      "epoch:8 step:7688 [D loss: 0.609227, acc: 68.75%] [G loss: 2.353652]\n",
      "epoch:8 step:7689 [D loss: 0.655685, acc: 63.28%] [G loss: 2.388165]\n",
      "epoch:8 step:7690 [D loss: 0.560262, acc: 75.78%] [G loss: 2.508593]\n",
      "epoch:8 step:7691 [D loss: 0.651016, acc: 60.94%] [G loss: 2.068977]\n",
      "epoch:8 step:7692 [D loss: 0.624472, acc: 66.41%] [G loss: 2.158703]\n",
      "epoch:8 step:7693 [D loss: 0.648680, acc: 64.06%] [G loss: 2.324188]\n",
      "epoch:8 step:7694 [D loss: 0.593334, acc: 64.06%] [G loss: 2.393565]\n",
      "epoch:8 step:7695 [D loss: 0.590651, acc: 67.19%] [G loss: 2.102099]\n",
      "epoch:8 step:7696 [D loss: 0.685904, acc: 57.81%] [G loss: 1.984003]\n",
      "epoch:8 step:7697 [D loss: 0.638237, acc: 67.97%] [G loss: 2.220098]\n",
      "epoch:8 step:7698 [D loss: 0.617152, acc: 63.28%] [G loss: 2.224820]\n",
      "epoch:8 step:7699 [D loss: 0.671803, acc: 63.28%] [G loss: 2.069408]\n",
      "epoch:8 step:7700 [D loss: 0.616656, acc: 67.97%] [G loss: 2.134073]\n",
      "epoch:8 step:7701 [D loss: 0.675504, acc: 58.59%] [G loss: 2.169497]\n",
      "epoch:8 step:7702 [D loss: 0.570575, acc: 71.88%] [G loss: 2.376665]\n",
      "epoch:8 step:7703 [D loss: 0.559199, acc: 68.75%] [G loss: 2.311204]\n",
      "epoch:8 step:7704 [D loss: 0.624975, acc: 67.19%] [G loss: 2.679462]\n",
      "epoch:8 step:7705 [D loss: 0.564838, acc: 67.19%] [G loss: 2.559407]\n",
      "epoch:8 step:7706 [D loss: 0.664524, acc: 67.19%] [G loss: 2.035121]\n",
      "epoch:8 step:7707 [D loss: 0.664486, acc: 60.94%] [G loss: 2.137004]\n",
      "epoch:8 step:7708 [D loss: 0.654832, acc: 59.38%] [G loss: 2.113579]\n",
      "epoch:8 step:7709 [D loss: 0.641226, acc: 67.97%] [G loss: 2.104244]\n",
      "epoch:8 step:7710 [D loss: 0.741569, acc: 53.91%] [G loss: 1.957405]\n",
      "epoch:8 step:7711 [D loss: 0.661680, acc: 58.59%] [G loss: 1.948866]\n",
      "epoch:8 step:7712 [D loss: 0.664088, acc: 60.94%] [G loss: 2.127921]\n",
      "epoch:8 step:7713 [D loss: 0.588368, acc: 67.19%] [G loss: 2.322035]\n",
      "epoch:8 step:7714 [D loss: 0.603346, acc: 71.88%] [G loss: 2.271330]\n",
      "epoch:8 step:7715 [D loss: 0.604313, acc: 69.53%] [G loss: 2.374558]\n",
      "epoch:8 step:7716 [D loss: 0.687182, acc: 58.59%] [G loss: 2.097429]\n",
      "epoch:8 step:7717 [D loss: 0.660707, acc: 56.25%] [G loss: 2.215364]\n",
      "epoch:8 step:7718 [D loss: 0.598547, acc: 65.62%] [G loss: 2.435101]\n",
      "epoch:8 step:7719 [D loss: 0.601656, acc: 68.75%] [G loss: 2.172147]\n",
      "epoch:8 step:7720 [D loss: 0.610454, acc: 65.62%] [G loss: 2.127721]\n",
      "epoch:8 step:7721 [D loss: 0.598251, acc: 67.97%] [G loss: 2.090903]\n",
      "epoch:8 step:7722 [D loss: 0.579786, acc: 71.88%] [G loss: 2.211691]\n",
      "epoch:8 step:7723 [D loss: 0.708700, acc: 53.12%] [G loss: 2.087833]\n",
      "epoch:8 step:7724 [D loss: 0.653034, acc: 60.94%] [G loss: 1.952129]\n",
      "epoch:8 step:7725 [D loss: 0.620667, acc: 64.84%] [G loss: 2.253633]\n",
      "epoch:8 step:7726 [D loss: 0.567792, acc: 68.75%] [G loss: 2.668543]\n",
      "epoch:8 step:7727 [D loss: 0.536964, acc: 69.53%] [G loss: 2.482228]\n",
      "epoch:8 step:7728 [D loss: 0.600540, acc: 71.09%] [G loss: 2.778534]\n",
      "epoch:8 step:7729 [D loss: 0.607090, acc: 70.31%] [G loss: 2.237506]\n",
      "epoch:8 step:7730 [D loss: 0.677397, acc: 67.19%] [G loss: 2.204728]\n",
      "epoch:8 step:7731 [D loss: 0.596598, acc: 62.50%] [G loss: 2.255517]\n",
      "epoch:8 step:7732 [D loss: 0.618313, acc: 68.75%] [G loss: 2.283121]\n",
      "epoch:8 step:7733 [D loss: 0.657055, acc: 65.62%] [G loss: 2.049484]\n",
      "epoch:8 step:7734 [D loss: 0.617653, acc: 67.19%] [G loss: 2.082918]\n",
      "epoch:8 step:7735 [D loss: 0.663832, acc: 60.16%] [G loss: 2.214359]\n",
      "epoch:8 step:7736 [D loss: 0.613265, acc: 67.19%] [G loss: 2.384407]\n",
      "epoch:8 step:7737 [D loss: 0.562364, acc: 68.75%] [G loss: 2.070280]\n",
      "epoch:8 step:7738 [D loss: 0.646217, acc: 63.28%] [G loss: 2.079314]\n",
      "epoch:8 step:7739 [D loss: 0.578879, acc: 72.66%] [G loss: 2.122822]\n",
      "epoch:8 step:7740 [D loss: 0.632556, acc: 64.84%] [G loss: 2.418239]\n",
      "epoch:8 step:7741 [D loss: 0.599362, acc: 60.94%] [G loss: 2.286548]\n",
      "epoch:8 step:7742 [D loss: 0.676837, acc: 60.94%] [G loss: 2.051958]\n",
      "epoch:8 step:7743 [D loss: 0.632293, acc: 67.19%] [G loss: 2.015873]\n",
      "epoch:8 step:7744 [D loss: 0.629373, acc: 67.97%] [G loss: 2.283098]\n",
      "epoch:8 step:7745 [D loss: 0.638991, acc: 64.84%] [G loss: 2.064000]\n",
      "epoch:8 step:7746 [D loss: 0.699960, acc: 60.16%] [G loss: 1.980525]\n",
      "epoch:8 step:7747 [D loss: 0.723991, acc: 53.91%] [G loss: 2.059482]\n",
      "epoch:8 step:7748 [D loss: 0.628279, acc: 62.50%] [G loss: 2.029309]\n",
      "epoch:8 step:7749 [D loss: 0.594391, acc: 67.19%] [G loss: 2.151153]\n",
      "epoch:8 step:7750 [D loss: 0.622556, acc: 66.41%] [G loss: 1.960099]\n",
      "epoch:8 step:7751 [D loss: 0.620836, acc: 66.41%] [G loss: 2.080576]\n",
      "epoch:8 step:7752 [D loss: 0.653336, acc: 60.16%] [G loss: 2.158364]\n",
      "epoch:8 step:7753 [D loss: 0.699946, acc: 67.97%] [G loss: 2.011747]\n",
      "epoch:8 step:7754 [D loss: 0.590133, acc: 66.41%] [G loss: 2.045229]\n",
      "epoch:8 step:7755 [D loss: 0.631452, acc: 60.16%] [G loss: 2.118813]\n",
      "epoch:8 step:7756 [D loss: 0.616003, acc: 67.19%] [G loss: 2.034201]\n",
      "epoch:8 step:7757 [D loss: 0.589730, acc: 65.62%] [G loss: 2.107951]\n",
      "epoch:8 step:7758 [D loss: 0.611431, acc: 68.75%] [G loss: 2.193340]\n",
      "epoch:8 step:7759 [D loss: 0.631364, acc: 65.62%] [G loss: 2.069009]\n",
      "epoch:8 step:7760 [D loss: 0.568641, acc: 71.09%] [G loss: 2.340432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7761 [D loss: 0.648501, acc: 67.19%] [G loss: 2.070654]\n",
      "epoch:8 step:7762 [D loss: 0.571108, acc: 75.00%] [G loss: 2.358914]\n",
      "epoch:8 step:7763 [D loss: 0.604937, acc: 64.06%] [G loss: 2.278713]\n",
      "epoch:8 step:7764 [D loss: 0.598449, acc: 61.72%] [G loss: 2.299402]\n",
      "epoch:8 step:7765 [D loss: 0.612800, acc: 66.41%] [G loss: 2.181370]\n",
      "epoch:8 step:7766 [D loss: 0.587756, acc: 71.09%] [G loss: 2.438932]\n",
      "epoch:8 step:7767 [D loss: 0.552196, acc: 68.75%] [G loss: 2.413557]\n",
      "epoch:8 step:7768 [D loss: 0.631245, acc: 64.84%] [G loss: 2.214080]\n",
      "epoch:8 step:7769 [D loss: 0.670236, acc: 64.06%] [G loss: 2.257082]\n",
      "epoch:8 step:7770 [D loss: 0.564289, acc: 75.00%] [G loss: 2.478545]\n",
      "epoch:8 step:7771 [D loss: 0.655647, acc: 60.94%] [G loss: 2.027436]\n",
      "epoch:8 step:7772 [D loss: 0.592157, acc: 67.19%] [G loss: 2.302222]\n",
      "epoch:8 step:7773 [D loss: 0.637123, acc: 67.97%] [G loss: 2.004140]\n",
      "epoch:8 step:7774 [D loss: 0.676335, acc: 63.28%] [G loss: 2.238614]\n",
      "epoch:8 step:7775 [D loss: 0.636211, acc: 64.06%] [G loss: 2.136176]\n",
      "epoch:8 step:7776 [D loss: 0.608303, acc: 66.41%] [G loss: 2.470308]\n",
      "epoch:8 step:7777 [D loss: 0.606088, acc: 66.41%] [G loss: 2.123981]\n",
      "epoch:8 step:7778 [D loss: 0.640490, acc: 64.06%] [G loss: 2.333132]\n",
      "epoch:8 step:7779 [D loss: 0.646523, acc: 67.97%] [G loss: 2.063575]\n",
      "epoch:8 step:7780 [D loss: 0.599623, acc: 64.06%] [G loss: 2.266110]\n",
      "epoch:8 step:7781 [D loss: 0.615686, acc: 66.41%] [G loss: 1.996650]\n",
      "epoch:8 step:7782 [D loss: 0.573215, acc: 73.44%] [G loss: 2.325100]\n",
      "epoch:8 step:7783 [D loss: 0.580498, acc: 71.09%] [G loss: 2.207130]\n",
      "epoch:8 step:7784 [D loss: 0.654338, acc: 60.16%] [G loss: 2.060370]\n",
      "epoch:8 step:7785 [D loss: 0.637888, acc: 64.06%] [G loss: 2.199402]\n",
      "epoch:8 step:7786 [D loss: 0.585166, acc: 67.19%] [G loss: 2.164506]\n",
      "epoch:8 step:7787 [D loss: 0.598349, acc: 66.41%] [G loss: 2.142998]\n",
      "epoch:8 step:7788 [D loss: 0.653083, acc: 59.38%] [G loss: 2.217355]\n",
      "epoch:8 step:7789 [D loss: 0.560009, acc: 72.66%] [G loss: 2.362937]\n",
      "epoch:8 step:7790 [D loss: 0.653394, acc: 60.94%] [G loss: 2.137359]\n",
      "epoch:8 step:7791 [D loss: 0.615926, acc: 69.53%] [G loss: 2.239552]\n",
      "epoch:8 step:7792 [D loss: 0.548781, acc: 72.66%] [G loss: 2.352862]\n",
      "epoch:8 step:7793 [D loss: 0.605312, acc: 66.41%] [G loss: 2.184100]\n",
      "epoch:8 step:7794 [D loss: 0.598281, acc: 66.41%] [G loss: 2.343208]\n",
      "epoch:8 step:7795 [D loss: 0.641663, acc: 64.06%] [G loss: 2.179492]\n",
      "epoch:8 step:7796 [D loss: 0.582683, acc: 65.62%] [G loss: 2.368659]\n",
      "epoch:8 step:7797 [D loss: 0.687606, acc: 57.81%] [G loss: 2.083525]\n",
      "epoch:8 step:7798 [D loss: 0.679373, acc: 64.06%] [G loss: 2.198138]\n",
      "epoch:8 step:7799 [D loss: 0.670696, acc: 65.62%] [G loss: 2.192924]\n",
      "epoch:8 step:7800 [D loss: 0.643883, acc: 64.06%] [G loss: 2.240260]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.170962\n",
      "FID: 35.751537\n",
      "0 = 13.004376989173906\n",
      "1 = 0.08222865013680032\n",
      "2 = 0.9138000011444092\n",
      "3 = 0.9205999970436096\n",
      "4 = 0.9070000052452087\n",
      "5 = 0.9082478284835815\n",
      "6 = 0.9205999970436096\n",
      "7 = 8.169693737721458\n",
      "8 = 0.11943892360220114\n",
      "9 = 0.7947999835014343\n",
      "10 = 0.7955999970436096\n",
      "11 = 0.7940000295639038\n",
      "12 = 0.794329047203064\n",
      "13 = 0.7955999970436096\n",
      "14 = 6.170987129211426\n",
      "15 = 9.108586311340332\n",
      "16 = 0.20325426757335663\n",
      "17 = 6.170961856842041\n",
      "18 = 35.75153732299805\n",
      "epoch:8 step:7801 [D loss: 0.608465, acc: 67.97%] [G loss: 2.478248]\n",
      "epoch:8 step:7802 [D loss: 0.604522, acc: 67.97%] [G loss: 2.076779]\n",
      "epoch:8 step:7803 [D loss: 0.541062, acc: 72.66%] [G loss: 2.216936]\n",
      "epoch:8 step:7804 [D loss: 0.625808, acc: 63.28%] [G loss: 2.286401]\n",
      "epoch:8 step:7805 [D loss: 0.608381, acc: 66.41%] [G loss: 2.217488]\n",
      "epoch:8 step:7806 [D loss: 0.578780, acc: 71.09%] [G loss: 2.096616]\n",
      "epoch:8 step:7807 [D loss: 0.599979, acc: 66.41%] [G loss: 2.181461]\n",
      "epoch:8 step:7808 [D loss: 0.539385, acc: 72.66%] [G loss: 3.014116]\n",
      "epoch:8 step:7809 [D loss: 0.526981, acc: 76.56%] [G loss: 2.494146]\n",
      "epoch:8 step:7810 [D loss: 0.525875, acc: 72.66%] [G loss: 3.067411]\n",
      "epoch:8 step:7811 [D loss: 0.504032, acc: 72.66%] [G loss: 2.987636]\n",
      "epoch:8 step:7812 [D loss: 0.703113, acc: 57.03%] [G loss: 1.967928]\n",
      "epoch:8 step:7813 [D loss: 0.669520, acc: 60.16%] [G loss: 2.084022]\n",
      "epoch:8 step:7814 [D loss: 0.559175, acc: 74.22%] [G loss: 2.311897]\n",
      "epoch:8 step:7815 [D loss: 0.628257, acc: 64.06%] [G loss: 2.345219]\n",
      "epoch:8 step:7816 [D loss: 0.618720, acc: 64.84%] [G loss: 2.374330]\n",
      "epoch:8 step:7817 [D loss: 0.573655, acc: 72.66%] [G loss: 2.268493]\n",
      "epoch:8 step:7818 [D loss: 0.571367, acc: 69.53%] [G loss: 2.285740]\n",
      "epoch:8 step:7819 [D loss: 0.679108, acc: 58.59%] [G loss: 2.017463]\n",
      "epoch:8 step:7820 [D loss: 0.638953, acc: 65.62%] [G loss: 2.246936]\n",
      "epoch:8 step:7821 [D loss: 0.570158, acc: 75.00%] [G loss: 2.264843]\n",
      "epoch:8 step:7822 [D loss: 0.646221, acc: 64.06%] [G loss: 1.968296]\n",
      "epoch:8 step:7823 [D loss: 0.706426, acc: 64.06%] [G loss: 2.291838]\n",
      "epoch:8 step:7824 [D loss: 0.617325, acc: 67.97%] [G loss: 2.394499]\n",
      "epoch:8 step:7825 [D loss: 0.606556, acc: 71.88%] [G loss: 2.284025]\n",
      "epoch:8 step:7826 [D loss: 0.679435, acc: 58.59%] [G loss: 2.267522]\n",
      "epoch:8 step:7827 [D loss: 0.597917, acc: 69.53%] [G loss: 2.219573]\n",
      "epoch:8 step:7828 [D loss: 0.518049, acc: 76.56%] [G loss: 2.366511]\n",
      "epoch:8 step:7829 [D loss: 0.594660, acc: 67.19%] [G loss: 2.574722]\n",
      "epoch:8 step:7830 [D loss: 0.659342, acc: 63.28%] [G loss: 2.271007]\n",
      "epoch:8 step:7831 [D loss: 0.596722, acc: 63.28%] [G loss: 2.459732]\n",
      "epoch:8 step:7832 [D loss: 0.576416, acc: 70.31%] [G loss: 2.720427]\n",
      "epoch:8 step:7833 [D loss: 0.568598, acc: 67.97%] [G loss: 2.268362]\n",
      "epoch:8 step:7834 [D loss: 0.637471, acc: 64.06%] [G loss: 2.245550]\n",
      "epoch:8 step:7835 [D loss: 0.630560, acc: 66.41%] [G loss: 2.138760]\n",
      "epoch:8 step:7836 [D loss: 0.560268, acc: 70.31%] [G loss: 2.347884]\n",
      "epoch:8 step:7837 [D loss: 0.652494, acc: 66.41%] [G loss: 2.125781]\n",
      "epoch:8 step:7838 [D loss: 0.617952, acc: 65.62%] [G loss: 2.043424]\n",
      "epoch:8 step:7839 [D loss: 0.642253, acc: 64.06%] [G loss: 2.262707]\n",
      "epoch:8 step:7840 [D loss: 0.576648, acc: 67.97%] [G loss: 2.160789]\n",
      "epoch:8 step:7841 [D loss: 0.508629, acc: 72.66%] [G loss: 2.650164]\n",
      "epoch:8 step:7842 [D loss: 0.542257, acc: 70.31%] [G loss: 2.735804]\n",
      "epoch:8 step:7843 [D loss: 0.526542, acc: 71.88%] [G loss: 2.921954]\n",
      "epoch:8 step:7844 [D loss: 0.655893, acc: 67.19%] [G loss: 2.126644]\n",
      "epoch:8 step:7845 [D loss: 0.669580, acc: 63.28%] [G loss: 1.925295]\n",
      "epoch:8 step:7846 [D loss: 0.564901, acc: 71.09%] [G loss: 2.346774]\n",
      "epoch:8 step:7847 [D loss: 0.614663, acc: 69.53%] [G loss: 2.125980]\n",
      "epoch:8 step:7848 [D loss: 0.666398, acc: 58.59%] [G loss: 2.315966]\n",
      "epoch:8 step:7849 [D loss: 0.613477, acc: 64.84%] [G loss: 2.365878]\n",
      "epoch:8 step:7850 [D loss: 0.620890, acc: 62.50%] [G loss: 2.343082]\n",
      "epoch:8 step:7851 [D loss: 0.624962, acc: 65.62%] [G loss: 2.172564]\n",
      "epoch:8 step:7852 [D loss: 0.658849, acc: 64.06%] [G loss: 2.092918]\n",
      "epoch:8 step:7853 [D loss: 0.551463, acc: 75.00%] [G loss: 2.420074]\n",
      "epoch:8 step:7854 [D loss: 0.578799, acc: 68.75%] [G loss: 2.404164]\n",
      "epoch:8 step:7855 [D loss: 0.510713, acc: 76.56%] [G loss: 2.352203]\n",
      "epoch:8 step:7856 [D loss: 0.558958, acc: 67.97%] [G loss: 2.357432]\n",
      "epoch:8 step:7857 [D loss: 0.675256, acc: 67.19%] [G loss: 2.279205]\n",
      "epoch:8 step:7858 [D loss: 0.575310, acc: 70.31%] [G loss: 2.360492]\n",
      "epoch:8 step:7859 [D loss: 0.611682, acc: 67.97%] [G loss: 2.125803]\n",
      "epoch:8 step:7860 [D loss: 0.574420, acc: 69.53%] [G loss: 2.292270]\n",
      "epoch:8 step:7861 [D loss: 0.629955, acc: 64.06%] [G loss: 2.295514]\n",
      "epoch:8 step:7862 [D loss: 0.594042, acc: 69.53%] [G loss: 2.346181]\n",
      "epoch:8 step:7863 [D loss: 0.562364, acc: 67.97%] [G loss: 2.333722]\n",
      "epoch:8 step:7864 [D loss: 0.602834, acc: 65.62%] [G loss: 2.304703]\n",
      "epoch:8 step:7865 [D loss: 0.704984, acc: 66.41%] [G loss: 2.463233]\n",
      "epoch:8 step:7866 [D loss: 0.657098, acc: 63.28%] [G loss: 2.153504]\n",
      "epoch:8 step:7867 [D loss: 0.597841, acc: 67.19%] [G loss: 2.388044]\n",
      "epoch:8 step:7868 [D loss: 0.572105, acc: 72.66%] [G loss: 2.330580]\n",
      "epoch:8 step:7869 [D loss: 0.661616, acc: 63.28%] [G loss: 2.247732]\n",
      "epoch:8 step:7870 [D loss: 0.545262, acc: 71.09%] [G loss: 2.431612]\n",
      "epoch:8 step:7871 [D loss: 0.715705, acc: 58.59%] [G loss: 2.122289]\n",
      "epoch:8 step:7872 [D loss: 0.629017, acc: 69.53%] [G loss: 2.022830]\n",
      "epoch:8 step:7873 [D loss: 0.697812, acc: 61.72%] [G loss: 1.939497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7874 [D loss: 0.618625, acc: 66.41%] [G loss: 2.126684]\n",
      "epoch:8 step:7875 [D loss: 0.612328, acc: 66.41%] [G loss: 2.114256]\n",
      "epoch:8 step:7876 [D loss: 0.618053, acc: 67.19%] [G loss: 2.239031]\n",
      "epoch:8 step:7877 [D loss: 0.609516, acc: 63.28%] [G loss: 2.195211]\n",
      "epoch:8 step:7878 [D loss: 0.599852, acc: 67.19%] [G loss: 2.273657]\n",
      "epoch:8 step:7879 [D loss: 0.633554, acc: 60.94%] [G loss: 2.017397]\n",
      "epoch:8 step:7880 [D loss: 0.697619, acc: 61.72%] [G loss: 2.191329]\n",
      "epoch:8 step:7881 [D loss: 0.595367, acc: 66.41%] [G loss: 2.108599]\n",
      "epoch:8 step:7882 [D loss: 0.676201, acc: 60.16%] [G loss: 1.926630]\n",
      "epoch:8 step:7883 [D loss: 0.677886, acc: 58.59%] [G loss: 1.843385]\n",
      "epoch:8 step:7884 [D loss: 0.644038, acc: 67.19%] [G loss: 1.810810]\n",
      "epoch:8 step:7885 [D loss: 0.641841, acc: 64.06%] [G loss: 2.034383]\n",
      "epoch:8 step:7886 [D loss: 0.620351, acc: 59.38%] [G loss: 2.060109]\n",
      "epoch:8 step:7887 [D loss: 0.660256, acc: 61.72%] [G loss: 2.180810]\n",
      "epoch:8 step:7888 [D loss: 0.617003, acc: 64.84%] [G loss: 2.245580]\n",
      "epoch:8 step:7889 [D loss: 0.636395, acc: 64.84%] [G loss: 2.126950]\n",
      "epoch:8 step:7890 [D loss: 0.663093, acc: 61.72%] [G loss: 2.138320]\n",
      "epoch:8 step:7891 [D loss: 0.525291, acc: 75.78%] [G loss: 2.198136]\n",
      "epoch:8 step:7892 [D loss: 0.667588, acc: 60.94%] [G loss: 2.046343]\n",
      "epoch:8 step:7893 [D loss: 0.751034, acc: 50.00%] [G loss: 2.207715]\n",
      "epoch:8 step:7894 [D loss: 0.586620, acc: 73.44%] [G loss: 2.070623]\n",
      "epoch:8 step:7895 [D loss: 0.570015, acc: 75.00%] [G loss: 2.225489]\n",
      "epoch:8 step:7896 [D loss: 0.669378, acc: 57.81%] [G loss: 2.042747]\n",
      "epoch:8 step:7897 [D loss: 0.612637, acc: 67.97%] [G loss: 2.228585]\n",
      "epoch:8 step:7898 [D loss: 0.617491, acc: 62.50%] [G loss: 2.180062]\n",
      "epoch:8 step:7899 [D loss: 0.595800, acc: 65.62%] [G loss: 2.120771]\n",
      "epoch:8 step:7900 [D loss: 0.613366, acc: 67.97%] [G loss: 2.228843]\n",
      "epoch:8 step:7901 [D loss: 0.550347, acc: 75.00%] [G loss: 2.331829]\n",
      "epoch:8 step:7902 [D loss: 0.590451, acc: 69.53%] [G loss: 2.295325]\n",
      "epoch:8 step:7903 [D loss: 0.615123, acc: 65.62%] [G loss: 2.303392]\n",
      "epoch:8 step:7904 [D loss: 0.651242, acc: 64.06%] [G loss: 2.250136]\n",
      "epoch:8 step:7905 [D loss: 0.651314, acc: 65.62%] [G loss: 2.243130]\n",
      "epoch:8 step:7906 [D loss: 0.640119, acc: 68.75%] [G loss: 2.337126]\n",
      "epoch:8 step:7907 [D loss: 0.643245, acc: 64.06%] [G loss: 2.181096]\n",
      "epoch:8 step:7908 [D loss: 0.678166, acc: 63.28%] [G loss: 2.172548]\n",
      "epoch:8 step:7909 [D loss: 0.657358, acc: 66.41%] [G loss: 2.014289]\n",
      "epoch:8 step:7910 [D loss: 0.623300, acc: 63.28%] [G loss: 2.112359]\n",
      "epoch:8 step:7911 [D loss: 0.625562, acc: 64.84%] [G loss: 2.316802]\n",
      "epoch:8 step:7912 [D loss: 0.639838, acc: 66.41%] [G loss: 2.110098]\n",
      "epoch:8 step:7913 [D loss: 0.629810, acc: 67.97%] [G loss: 2.329424]\n",
      "epoch:8 step:7914 [D loss: 0.680152, acc: 60.16%] [G loss: 2.073012]\n",
      "epoch:8 step:7915 [D loss: 0.724402, acc: 57.81%] [G loss: 2.080645]\n",
      "epoch:8 step:7916 [D loss: 0.644945, acc: 60.16%] [G loss: 2.075661]\n",
      "epoch:8 step:7917 [D loss: 0.687557, acc: 57.03%] [G loss: 2.022063]\n",
      "epoch:8 step:7918 [D loss: 0.685194, acc: 60.16%] [G loss: 2.018039]\n",
      "epoch:8 step:7919 [D loss: 0.623640, acc: 63.28%] [G loss: 2.040559]\n",
      "epoch:8 step:7920 [D loss: 0.665718, acc: 60.16%] [G loss: 2.159014]\n",
      "epoch:8 step:7921 [D loss: 0.587535, acc: 71.88%] [G loss: 2.166778]\n",
      "epoch:8 step:7922 [D loss: 0.606795, acc: 67.97%] [G loss: 2.288449]\n",
      "epoch:8 step:7923 [D loss: 0.566499, acc: 73.44%] [G loss: 2.908462]\n",
      "epoch:8 step:7924 [D loss: 0.531684, acc: 76.56%] [G loss: 2.865820]\n",
      "epoch:8 step:7925 [D loss: 0.539928, acc: 69.53%] [G loss: 2.480606]\n",
      "epoch:8 step:7926 [D loss: 0.601411, acc: 66.41%] [G loss: 2.396289]\n",
      "epoch:8 step:7927 [D loss: 0.609140, acc: 69.53%] [G loss: 2.317581]\n",
      "epoch:8 step:7928 [D loss: 0.654532, acc: 60.94%] [G loss: 2.082935]\n",
      "epoch:8 step:7929 [D loss: 0.657730, acc: 64.84%] [G loss: 2.074552]\n",
      "epoch:8 step:7930 [D loss: 0.578936, acc: 71.09%] [G loss: 2.330055]\n",
      "epoch:8 step:7931 [D loss: 0.670215, acc: 63.28%] [G loss: 2.207859]\n",
      "epoch:8 step:7932 [D loss: 0.623281, acc: 67.97%] [G loss: 2.297387]\n",
      "epoch:8 step:7933 [D loss: 0.681584, acc: 62.50%] [G loss: 1.905218]\n",
      "epoch:8 step:7934 [D loss: 0.714528, acc: 59.38%] [G loss: 1.919663]\n",
      "epoch:8 step:7935 [D loss: 0.606607, acc: 67.19%] [G loss: 2.089248]\n",
      "epoch:8 step:7936 [D loss: 0.661757, acc: 60.16%] [G loss: 1.928845]\n",
      "epoch:8 step:7937 [D loss: 0.662630, acc: 60.94%] [G loss: 1.933992]\n",
      "epoch:8 step:7938 [D loss: 0.717033, acc: 59.38%] [G loss: 2.040306]\n",
      "epoch:8 step:7939 [D loss: 0.646214, acc: 60.94%] [G loss: 1.978853]\n",
      "epoch:8 step:7940 [D loss: 0.639669, acc: 63.28%] [G loss: 1.914409]\n",
      "epoch:8 step:7941 [D loss: 0.625526, acc: 64.84%] [G loss: 1.981554]\n",
      "epoch:8 step:7942 [D loss: 0.665303, acc: 61.72%] [G loss: 1.933386]\n",
      "epoch:8 step:7943 [D loss: 0.684724, acc: 59.38%] [G loss: 2.032629]\n",
      "epoch:8 step:7944 [D loss: 0.642218, acc: 64.84%] [G loss: 2.093264]\n",
      "epoch:8 step:7945 [D loss: 0.602114, acc: 68.75%] [G loss: 2.105258]\n",
      "epoch:8 step:7946 [D loss: 0.602472, acc: 60.94%] [G loss: 2.036582]\n",
      "epoch:8 step:7947 [D loss: 0.598810, acc: 66.41%] [G loss: 2.195249]\n",
      "epoch:8 step:7948 [D loss: 0.679425, acc: 59.38%] [G loss: 1.982139]\n",
      "epoch:8 step:7949 [D loss: 0.611405, acc: 65.62%] [G loss: 2.125232]\n",
      "epoch:8 step:7950 [D loss: 0.665931, acc: 60.94%] [G loss: 2.126568]\n",
      "epoch:8 step:7951 [D loss: 0.605383, acc: 67.97%] [G loss: 2.065417]\n",
      "epoch:8 step:7952 [D loss: 0.650007, acc: 63.28%] [G loss: 2.138162]\n",
      "epoch:8 step:7953 [D loss: 0.581514, acc: 69.53%] [G loss: 2.171583]\n",
      "epoch:8 step:7954 [D loss: 0.641340, acc: 59.38%] [G loss: 2.008219]\n",
      "epoch:8 step:7955 [D loss: 0.625788, acc: 62.50%] [G loss: 2.151659]\n",
      "epoch:8 step:7956 [D loss: 0.652997, acc: 63.28%] [G loss: 2.197705]\n",
      "epoch:8 step:7957 [D loss: 0.655449, acc: 63.28%] [G loss: 2.054608]\n",
      "epoch:8 step:7958 [D loss: 0.609494, acc: 67.19%] [G loss: 2.136110]\n",
      "epoch:8 step:7959 [D loss: 0.657665, acc: 57.81%] [G loss: 1.999884]\n",
      "epoch:8 step:7960 [D loss: 0.603469, acc: 63.28%] [G loss: 2.117855]\n",
      "epoch:8 step:7961 [D loss: 0.672960, acc: 65.62%] [G loss: 2.164176]\n",
      "epoch:8 step:7962 [D loss: 0.582353, acc: 65.62%] [G loss: 2.206306]\n",
      "epoch:8 step:7963 [D loss: 0.658145, acc: 62.50%] [G loss: 2.158799]\n",
      "epoch:8 step:7964 [D loss: 0.588817, acc: 67.97%] [G loss: 2.384972]\n",
      "epoch:8 step:7965 [D loss: 0.640625, acc: 69.53%] [G loss: 2.289803]\n",
      "epoch:8 step:7966 [D loss: 0.640329, acc: 67.97%] [G loss: 2.285861]\n",
      "epoch:8 step:7967 [D loss: 0.599557, acc: 64.84%] [G loss: 2.432947]\n",
      "epoch:8 step:7968 [D loss: 0.576593, acc: 65.62%] [G loss: 2.514084]\n",
      "epoch:8 step:7969 [D loss: 0.656792, acc: 65.62%] [G loss: 2.192853]\n",
      "epoch:8 step:7970 [D loss: 0.613769, acc: 66.41%] [G loss: 2.297573]\n",
      "epoch:8 step:7971 [D loss: 0.665505, acc: 65.62%] [G loss: 2.351403]\n",
      "epoch:8 step:7972 [D loss: 0.617380, acc: 64.84%] [G loss: 2.216500]\n",
      "epoch:8 step:7973 [D loss: 0.693872, acc: 57.81%] [G loss: 1.861984]\n",
      "epoch:8 step:7974 [D loss: 0.687063, acc: 54.69%] [G loss: 2.072464]\n",
      "epoch:8 step:7975 [D loss: 0.610239, acc: 67.97%] [G loss: 2.220197]\n",
      "epoch:8 step:7976 [D loss: 0.644411, acc: 65.62%] [G loss: 2.223632]\n",
      "epoch:8 step:7977 [D loss: 0.619947, acc: 67.19%] [G loss: 2.340498]\n",
      "epoch:8 step:7978 [D loss: 0.679721, acc: 56.25%] [G loss: 2.094861]\n",
      "epoch:8 step:7979 [D loss: 0.624709, acc: 66.41%] [G loss: 2.153925]\n",
      "epoch:8 step:7980 [D loss: 0.569480, acc: 72.66%] [G loss: 2.336209]\n",
      "epoch:8 step:7981 [D loss: 0.633128, acc: 66.41%] [G loss: 2.207304]\n",
      "epoch:8 step:7982 [D loss: 0.637245, acc: 64.84%] [G loss: 2.122922]\n",
      "epoch:8 step:7983 [D loss: 0.657853, acc: 60.94%] [G loss: 1.902823]\n",
      "epoch:8 step:7984 [D loss: 0.618875, acc: 60.16%] [G loss: 2.357519]\n",
      "epoch:8 step:7985 [D loss: 0.606502, acc: 67.19%] [G loss: 2.134601]\n",
      "epoch:8 step:7986 [D loss: 0.642610, acc: 63.28%] [G loss: 2.247766]\n",
      "epoch:8 step:7987 [D loss: 0.620610, acc: 63.28%] [G loss: 2.275000]\n",
      "epoch:8 step:7988 [D loss: 0.580337, acc: 72.66%] [G loss: 2.054167]\n",
      "epoch:8 step:7989 [D loss: 0.590344, acc: 69.53%] [G loss: 2.135655]\n",
      "epoch:8 step:7990 [D loss: 0.549286, acc: 74.22%] [G loss: 2.252579]\n",
      "epoch:8 step:7991 [D loss: 0.595892, acc: 71.09%] [G loss: 2.460710]\n",
      "epoch:8 step:7992 [D loss: 0.604113, acc: 68.75%] [G loss: 2.291823]\n",
      "epoch:8 step:7993 [D loss: 0.514810, acc: 75.00%] [G loss: 2.445023]\n",
      "epoch:8 step:7994 [D loss: 0.601339, acc: 72.66%] [G loss: 2.614568]\n",
      "epoch:8 step:7995 [D loss: 0.614974, acc: 65.62%] [G loss: 2.397661]\n",
      "epoch:8 step:7996 [D loss: 0.705603, acc: 57.03%] [G loss: 2.039667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7997 [D loss: 0.757570, acc: 53.91%] [G loss: 1.995308]\n",
      "epoch:8 step:7998 [D loss: 0.697304, acc: 59.38%] [G loss: 1.933554]\n",
      "epoch:8 step:7999 [D loss: 0.591218, acc: 70.31%] [G loss: 1.998454]\n",
      "epoch:8 step:8000 [D loss: 0.573566, acc: 73.44%] [G loss: 2.142941]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.250772\n",
      "FID: 30.474615\n",
      "0 = 12.849303725624086\n",
      "1 = 0.08000589229818764\n",
      "2 = 0.9089000225067139\n",
      "3 = 0.920199990272522\n",
      "4 = 0.897599995136261\n",
      "5 = 0.899863064289093\n",
      "6 = 0.920199990272522\n",
      "7 = 7.847261047983157\n",
      "8 = 0.10762421019083791\n",
      "9 = 0.7875000238418579\n",
      "10 = 0.795199990272522\n",
      "11 = 0.7797999978065491\n",
      "12 = 0.7831396460533142\n",
      "13 = 0.795199990272522\n",
      "14 = 6.250796318054199\n",
      "15 = 9.246296882629395\n",
      "16 = 0.18662473559379578\n",
      "17 = 6.250771999359131\n",
      "18 = 30.4746150970459\n",
      "epoch:8 step:8001 [D loss: 0.642024, acc: 64.06%] [G loss: 2.203218]\n",
      "epoch:8 step:8002 [D loss: 0.664101, acc: 62.50%] [G loss: 1.869756]\n",
      "epoch:8 step:8003 [D loss: 0.637153, acc: 60.16%] [G loss: 2.142700]\n",
      "epoch:8 step:8004 [D loss: 0.544206, acc: 76.56%] [G loss: 2.250776]\n",
      "epoch:8 step:8005 [D loss: 0.601601, acc: 67.19%] [G loss: 2.160182]\n",
      "epoch:8 step:8006 [D loss: 0.691200, acc: 60.16%] [G loss: 2.106145]\n",
      "epoch:8 step:8007 [D loss: 0.700064, acc: 57.81%] [G loss: 2.054329]\n",
      "epoch:8 step:8008 [D loss: 0.621850, acc: 67.19%] [G loss: 1.982354]\n",
      "epoch:8 step:8009 [D loss: 0.586623, acc: 72.66%] [G loss: 2.119513]\n",
      "epoch:8 step:8010 [D loss: 0.602939, acc: 67.19%] [G loss: 2.021164]\n",
      "epoch:8 step:8011 [D loss: 0.601626, acc: 68.75%] [G loss: 2.258939]\n",
      "epoch:8 step:8012 [D loss: 0.590107, acc: 71.88%] [G loss: 2.315428]\n",
      "epoch:8 step:8013 [D loss: 0.642688, acc: 66.41%] [G loss: 2.169039]\n",
      "epoch:8 step:8014 [D loss: 0.626033, acc: 66.41%] [G loss: 2.194251]\n",
      "epoch:8 step:8015 [D loss: 0.643081, acc: 62.50%] [G loss: 2.201204]\n",
      "epoch:8 step:8016 [D loss: 0.598022, acc: 64.06%] [G loss: 2.142835]\n",
      "epoch:8 step:8017 [D loss: 0.599331, acc: 71.09%] [G loss: 2.310234]\n",
      "epoch:8 step:8018 [D loss: 0.591933, acc: 68.75%] [G loss: 2.361399]\n",
      "epoch:8 step:8019 [D loss: 0.595824, acc: 69.53%] [G loss: 2.239207]\n",
      "epoch:8 step:8020 [D loss: 0.596426, acc: 67.19%] [G loss: 2.302124]\n",
      "epoch:8 step:8021 [D loss: 0.656921, acc: 57.03%] [G loss: 2.218386]\n",
      "epoch:8 step:8022 [D loss: 0.591980, acc: 65.62%] [G loss: 2.101086]\n",
      "epoch:8 step:8023 [D loss: 0.588886, acc: 68.75%] [G loss: 2.097501]\n",
      "epoch:8 step:8024 [D loss: 0.649715, acc: 59.38%] [G loss: 2.057516]\n",
      "epoch:8 step:8025 [D loss: 0.624112, acc: 67.19%] [G loss: 1.967114]\n",
      "epoch:8 step:8026 [D loss: 0.616395, acc: 67.19%] [G loss: 2.175424]\n",
      "epoch:8 step:8027 [D loss: 0.650220, acc: 69.53%] [G loss: 2.120318]\n",
      "epoch:8 step:8028 [D loss: 0.590451, acc: 73.44%] [G loss: 2.362179]\n",
      "epoch:8 step:8029 [D loss: 0.647414, acc: 59.38%] [G loss: 1.983901]\n",
      "epoch:8 step:8030 [D loss: 0.523438, acc: 72.66%] [G loss: 2.347104]\n",
      "epoch:8 step:8031 [D loss: 0.645294, acc: 61.72%] [G loss: 2.218829]\n",
      "epoch:8 step:8032 [D loss: 0.627544, acc: 65.62%] [G loss: 2.287602]\n",
      "epoch:8 step:8033 [D loss: 0.624310, acc: 60.16%] [G loss: 2.056591]\n",
      "epoch:8 step:8034 [D loss: 0.691531, acc: 58.59%] [G loss: 2.056928]\n",
      "epoch:8 step:8035 [D loss: 0.642616, acc: 56.25%] [G loss: 2.120219]\n",
      "epoch:8 step:8036 [D loss: 0.679496, acc: 56.25%] [G loss: 2.035747]\n",
      "epoch:8 step:8037 [D loss: 0.596220, acc: 66.41%] [G loss: 2.003378]\n",
      "epoch:8 step:8038 [D loss: 0.641868, acc: 64.06%] [G loss: 2.283783]\n",
      "epoch:8 step:8039 [D loss: 0.581699, acc: 68.75%] [G loss: 1.939217]\n",
      "epoch:8 step:8040 [D loss: 0.630412, acc: 66.41%] [G loss: 2.153128]\n",
      "epoch:8 step:8041 [D loss: 0.591839, acc: 67.97%] [G loss: 2.341457]\n",
      "epoch:8 step:8042 [D loss: 0.690288, acc: 61.72%] [G loss: 2.133431]\n",
      "epoch:8 step:8043 [D loss: 0.630025, acc: 65.62%] [G loss: 2.294799]\n",
      "epoch:8 step:8044 [D loss: 0.635751, acc: 60.94%] [G loss: 2.189008]\n",
      "epoch:8 step:8045 [D loss: 0.578018, acc: 70.31%] [G loss: 2.123546]\n",
      "epoch:8 step:8046 [D loss: 0.610120, acc: 68.75%] [G loss: 2.124440]\n",
      "epoch:8 step:8047 [D loss: 0.609802, acc: 69.53%] [G loss: 2.426582]\n",
      "epoch:8 step:8048 [D loss: 0.580762, acc: 75.78%] [G loss: 2.209868]\n",
      "epoch:8 step:8049 [D loss: 0.581576, acc: 70.31%] [G loss: 2.205328]\n",
      "epoch:8 step:8050 [D loss: 0.503586, acc: 78.12%] [G loss: 2.555923]\n",
      "epoch:8 step:8051 [D loss: 0.583036, acc: 64.84%] [G loss: 2.571070]\n",
      "epoch:8 step:8052 [D loss: 0.588719, acc: 67.19%] [G loss: 2.672121]\n",
      "epoch:8 step:8053 [D loss: 0.600076, acc: 70.31%] [G loss: 2.374701]\n",
      "epoch:8 step:8054 [D loss: 0.618600, acc: 66.41%] [G loss: 2.449702]\n",
      "epoch:8 step:8055 [D loss: 0.659085, acc: 61.72%] [G loss: 2.166025]\n",
      "epoch:8 step:8056 [D loss: 0.647648, acc: 60.94%] [G loss: 2.204539]\n",
      "epoch:8 step:8057 [D loss: 0.653387, acc: 60.94%] [G loss: 2.056134]\n",
      "epoch:8 step:8058 [D loss: 0.637149, acc: 66.41%] [G loss: 2.096908]\n",
      "epoch:8 step:8059 [D loss: 0.661930, acc: 63.28%] [G loss: 2.076041]\n",
      "epoch:8 step:8060 [D loss: 0.566780, acc: 72.66%] [G loss: 2.245637]\n",
      "epoch:8 step:8061 [D loss: 0.649544, acc: 61.72%] [G loss: 2.190884]\n",
      "epoch:8 step:8062 [D loss: 0.738109, acc: 55.47%] [G loss: 2.093637]\n",
      "epoch:8 step:8063 [D loss: 0.606176, acc: 68.75%] [G loss: 2.206914]\n",
      "epoch:8 step:8064 [D loss: 0.635772, acc: 64.84%] [G loss: 2.319349]\n",
      "epoch:8 step:8065 [D loss: 0.622886, acc: 71.88%] [G loss: 2.110973]\n",
      "epoch:8 step:8066 [D loss: 0.665342, acc: 61.72%] [G loss: 1.883926]\n",
      "epoch:8 step:8067 [D loss: 0.641631, acc: 59.38%] [G loss: 2.040180]\n",
      "epoch:8 step:8068 [D loss: 0.675335, acc: 61.72%] [G loss: 1.924735]\n",
      "epoch:8 step:8069 [D loss: 0.643204, acc: 66.41%] [G loss: 2.075002]\n",
      "epoch:8 step:8070 [D loss: 0.672374, acc: 65.62%] [G loss: 2.147391]\n",
      "epoch:8 step:8071 [D loss: 0.588967, acc: 70.31%] [G loss: 2.164160]\n",
      "epoch:8 step:8072 [D loss: 0.634821, acc: 57.81%] [G loss: 1.934212]\n",
      "epoch:8 step:8073 [D loss: 0.676145, acc: 58.59%] [G loss: 1.863964]\n",
      "epoch:8 step:8074 [D loss: 0.637115, acc: 64.06%] [G loss: 2.112112]\n",
      "epoch:8 step:8075 [D loss: 0.665639, acc: 58.59%] [G loss: 2.005106]\n",
      "epoch:8 step:8076 [D loss: 0.708009, acc: 57.81%] [G loss: 1.989745]\n",
      "epoch:8 step:8077 [D loss: 0.627023, acc: 59.38%] [G loss: 1.952697]\n",
      "epoch:8 step:8078 [D loss: 0.632773, acc: 64.06%] [G loss: 2.208095]\n",
      "epoch:8 step:8079 [D loss: 0.678576, acc: 61.72%] [G loss: 1.981352]\n",
      "epoch:8 step:8080 [D loss: 0.643236, acc: 61.72%] [G loss: 2.042977]\n",
      "epoch:8 step:8081 [D loss: 0.610235, acc: 66.41%] [G loss: 1.880419]\n",
      "epoch:8 step:8082 [D loss: 0.627264, acc: 63.28%] [G loss: 2.069855]\n",
      "epoch:8 step:8083 [D loss: 0.652521, acc: 63.28%] [G loss: 2.268963]\n",
      "epoch:8 step:8084 [D loss: 0.543533, acc: 75.00%] [G loss: 2.077896]\n",
      "epoch:8 step:8085 [D loss: 0.682587, acc: 58.59%] [G loss: 2.011164]\n",
      "epoch:8 step:8086 [D loss: 0.657966, acc: 60.16%] [G loss: 2.190402]\n",
      "epoch:8 step:8087 [D loss: 0.571917, acc: 72.66%] [G loss: 2.281217]\n",
      "epoch:8 step:8088 [D loss: 0.567887, acc: 71.88%] [G loss: 2.072456]\n",
      "epoch:8 step:8089 [D loss: 0.651271, acc: 61.72%] [G loss: 2.221204]\n",
      "epoch:8 step:8090 [D loss: 0.679592, acc: 62.50%] [G loss: 2.056267]\n",
      "epoch:8 step:8091 [D loss: 0.552191, acc: 75.00%] [G loss: 2.191119]\n",
      "epoch:8 step:8092 [D loss: 0.661979, acc: 59.38%] [G loss: 1.926216]\n",
      "epoch:8 step:8093 [D loss: 0.642740, acc: 62.50%] [G loss: 2.152840]\n",
      "epoch:8 step:8094 [D loss: 0.583865, acc: 70.31%] [G loss: 2.213977]\n",
      "epoch:8 step:8095 [D loss: 0.690442, acc: 64.84%] [G loss: 2.154937]\n",
      "epoch:8 step:8096 [D loss: 0.668224, acc: 58.59%] [G loss: 2.042642]\n",
      "epoch:8 step:8097 [D loss: 0.686801, acc: 60.16%] [G loss: 2.206117]\n",
      "epoch:8 step:8098 [D loss: 0.618012, acc: 64.06%] [G loss: 2.121434]\n",
      "epoch:8 step:8099 [D loss: 0.599933, acc: 66.41%] [G loss: 2.125436]\n",
      "epoch:8 step:8100 [D loss: 0.643027, acc: 61.72%] [G loss: 2.146599]\n",
      "epoch:8 step:8101 [D loss: 0.695189, acc: 60.16%] [G loss: 2.239165]\n",
      "epoch:8 step:8102 [D loss: 0.588054, acc: 67.19%] [G loss: 2.073639]\n",
      "epoch:8 step:8103 [D loss: 0.658016, acc: 67.19%] [G loss: 2.095897]\n",
      "epoch:8 step:8104 [D loss: 0.573833, acc: 65.62%] [G loss: 2.214765]\n",
      "epoch:8 step:8105 [D loss: 0.585022, acc: 64.84%] [G loss: 2.298628]\n",
      "epoch:8 step:8106 [D loss: 0.607158, acc: 62.50%] [G loss: 2.239203]\n",
      "epoch:8 step:8107 [D loss: 0.636065, acc: 62.50%] [G loss: 1.971058]\n",
      "epoch:8 step:8108 [D loss: 0.628973, acc: 66.41%] [G loss: 2.102721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8109 [D loss: 0.588047, acc: 64.84%] [G loss: 2.357927]\n",
      "epoch:8 step:8110 [D loss: 0.638778, acc: 63.28%] [G loss: 1.912559]\n",
      "epoch:8 step:8111 [D loss: 0.650474, acc: 61.72%] [G loss: 1.874983]\n",
      "epoch:8 step:8112 [D loss: 0.616821, acc: 68.75%] [G loss: 2.112958]\n",
      "epoch:8 step:8113 [D loss: 0.620165, acc: 65.62%] [G loss: 2.334167]\n",
      "epoch:8 step:8114 [D loss: 0.580250, acc: 71.88%] [G loss: 2.166332]\n",
      "epoch:8 step:8115 [D loss: 0.636954, acc: 64.06%] [G loss: 2.090880]\n",
      "epoch:8 step:8116 [D loss: 0.635434, acc: 66.41%] [G loss: 2.088924]\n",
      "epoch:8 step:8117 [D loss: 0.611601, acc: 60.94%] [G loss: 2.120825]\n",
      "epoch:8 step:8118 [D loss: 0.663008, acc: 57.03%] [G loss: 2.218611]\n",
      "epoch:8 step:8119 [D loss: 0.552000, acc: 72.66%] [G loss: 2.245912]\n",
      "epoch:8 step:8120 [D loss: 0.535267, acc: 72.66%] [G loss: 2.404033]\n",
      "epoch:8 step:8121 [D loss: 0.664607, acc: 65.62%] [G loss: 2.149756]\n",
      "epoch:8 step:8122 [D loss: 0.648105, acc: 64.84%] [G loss: 2.117446]\n",
      "epoch:8 step:8123 [D loss: 0.611975, acc: 66.41%] [G loss: 2.153744]\n",
      "epoch:8 step:8124 [D loss: 0.645167, acc: 57.81%] [G loss: 2.086511]\n",
      "epoch:8 step:8125 [D loss: 0.570352, acc: 73.44%] [G loss: 2.248047]\n",
      "epoch:8 step:8126 [D loss: 0.538549, acc: 73.44%] [G loss: 2.310890]\n",
      "epoch:8 step:8127 [D loss: 0.597807, acc: 71.88%] [G loss: 2.320038]\n",
      "epoch:8 step:8128 [D loss: 0.517161, acc: 76.56%] [G loss: 2.272656]\n",
      "epoch:8 step:8129 [D loss: 0.644438, acc: 63.28%] [G loss: 2.463834]\n",
      "epoch:8 step:8130 [D loss: 0.579725, acc: 71.09%] [G loss: 2.214241]\n",
      "epoch:8 step:8131 [D loss: 0.603840, acc: 70.31%] [G loss: 2.433568]\n",
      "epoch:8 step:8132 [D loss: 0.638126, acc: 62.50%] [G loss: 2.335690]\n",
      "epoch:8 step:8133 [D loss: 0.586574, acc: 67.97%] [G loss: 2.216028]\n",
      "epoch:8 step:8134 [D loss: 0.618960, acc: 67.19%] [G loss: 2.133738]\n",
      "epoch:8 step:8135 [D loss: 0.549078, acc: 71.09%] [G loss: 2.338959]\n",
      "epoch:8 step:8136 [D loss: 0.580971, acc: 67.97%] [G loss: 2.412955]\n",
      "epoch:8 step:8137 [D loss: 0.662454, acc: 60.16%] [G loss: 2.328116]\n",
      "epoch:8 step:8138 [D loss: 0.533805, acc: 70.31%] [G loss: 2.496917]\n",
      "epoch:8 step:8139 [D loss: 0.594438, acc: 64.06%] [G loss: 2.111891]\n",
      "epoch:8 step:8140 [D loss: 0.692843, acc: 57.03%] [G loss: 2.454009]\n",
      "epoch:8 step:8141 [D loss: 0.613277, acc: 68.75%] [G loss: 2.259307]\n",
      "epoch:8 step:8142 [D loss: 0.574698, acc: 71.88%] [G loss: 2.268547]\n",
      "epoch:8 step:8143 [D loss: 0.545701, acc: 69.53%] [G loss: 2.748116]\n",
      "epoch:8 step:8144 [D loss: 0.574334, acc: 67.97%] [G loss: 2.804779]\n",
      "epoch:8 step:8145 [D loss: 0.535608, acc: 72.66%] [G loss: 2.791872]\n",
      "epoch:8 step:8146 [D loss: 0.575665, acc: 69.53%] [G loss: 2.588855]\n",
      "epoch:8 step:8147 [D loss: 0.651712, acc: 60.16%] [G loss: 2.325655]\n",
      "epoch:8 step:8148 [D loss: 0.667407, acc: 63.28%] [G loss: 2.034577]\n",
      "epoch:8 step:8149 [D loss: 0.643783, acc: 60.94%] [G loss: 2.229162]\n",
      "epoch:8 step:8150 [D loss: 0.584893, acc: 73.44%] [G loss: 2.247713]\n",
      "epoch:8 step:8151 [D loss: 0.633781, acc: 63.28%] [G loss: 2.380036]\n",
      "epoch:8 step:8152 [D loss: 0.630526, acc: 64.84%] [G loss: 2.153944]\n",
      "epoch:8 step:8153 [D loss: 0.687072, acc: 61.72%] [G loss: 2.135838]\n",
      "epoch:8 step:8154 [D loss: 0.661429, acc: 56.25%] [G loss: 1.952152]\n",
      "epoch:8 step:8155 [D loss: 0.645141, acc: 64.84%] [G loss: 2.149502]\n",
      "epoch:8 step:8156 [D loss: 0.568825, acc: 69.53%] [G loss: 2.212857]\n",
      "epoch:8 step:8157 [D loss: 0.602864, acc: 66.41%] [G loss: 2.301926]\n",
      "epoch:8 step:8158 [D loss: 0.650358, acc: 66.41%] [G loss: 2.145183]\n",
      "epoch:8 step:8159 [D loss: 0.660121, acc: 61.72%] [G loss: 2.335057]\n",
      "epoch:8 step:8160 [D loss: 0.597540, acc: 70.31%] [G loss: 2.152516]\n",
      "epoch:8 step:8161 [D loss: 0.630618, acc: 63.28%] [G loss: 2.048752]\n",
      "epoch:8 step:8162 [D loss: 0.680412, acc: 60.16%] [G loss: 2.122961]\n",
      "epoch:8 step:8163 [D loss: 0.638581, acc: 59.38%] [G loss: 1.970894]\n",
      "epoch:8 step:8164 [D loss: 0.630283, acc: 67.19%] [G loss: 2.329360]\n",
      "epoch:8 step:8165 [D loss: 0.654690, acc: 59.38%] [G loss: 1.833945]\n",
      "epoch:8 step:8166 [D loss: 0.618109, acc: 75.00%] [G loss: 2.238028]\n",
      "epoch:8 step:8167 [D loss: 0.694086, acc: 57.03%] [G loss: 2.147992]\n",
      "epoch:8 step:8168 [D loss: 0.613667, acc: 66.41%] [G loss: 2.071000]\n",
      "epoch:8 step:8169 [D loss: 0.598532, acc: 67.97%] [G loss: 2.205553]\n",
      "epoch:8 step:8170 [D loss: 0.671573, acc: 57.81%] [G loss: 2.107455]\n",
      "epoch:8 step:8171 [D loss: 0.629771, acc: 67.97%] [G loss: 2.231184]\n",
      "epoch:8 step:8172 [D loss: 0.683427, acc: 60.94%] [G loss: 1.917605]\n",
      "epoch:8 step:8173 [D loss: 0.577817, acc: 67.19%] [G loss: 2.066422]\n",
      "epoch:8 step:8174 [D loss: 0.651618, acc: 62.50%] [G loss: 2.092916]\n",
      "epoch:8 step:8175 [D loss: 0.578549, acc: 71.88%] [G loss: 2.320630]\n",
      "epoch:8 step:8176 [D loss: 0.553379, acc: 73.44%] [G loss: 2.119763]\n",
      "epoch:8 step:8177 [D loss: 0.648507, acc: 63.28%] [G loss: 2.072136]\n",
      "epoch:8 step:8178 [D loss: 0.610562, acc: 67.19%] [G loss: 2.378587]\n",
      "epoch:8 step:8179 [D loss: 0.669710, acc: 60.16%] [G loss: 2.090459]\n",
      "epoch:8 step:8180 [D loss: 0.640487, acc: 60.16%] [G loss: 2.145022]\n",
      "epoch:8 step:8181 [D loss: 0.606950, acc: 68.75%] [G loss: 2.168194]\n",
      "epoch:8 step:8182 [D loss: 0.615893, acc: 64.84%] [G loss: 2.215997]\n",
      "epoch:8 step:8183 [D loss: 0.592743, acc: 66.41%] [G loss: 2.113226]\n",
      "epoch:8 step:8184 [D loss: 0.627031, acc: 67.97%] [G loss: 2.074961]\n",
      "epoch:8 step:8185 [D loss: 0.682274, acc: 59.38%] [G loss: 2.236938]\n",
      "epoch:8 step:8186 [D loss: 0.579483, acc: 64.06%] [G loss: 2.367175]\n",
      "epoch:8 step:8187 [D loss: 0.613031, acc: 64.06%] [G loss: 2.462690]\n",
      "epoch:8 step:8188 [D loss: 0.581259, acc: 66.41%] [G loss: 2.363438]\n",
      "epoch:8 step:8189 [D loss: 0.661077, acc: 60.94%] [G loss: 2.311680]\n",
      "epoch:8 step:8190 [D loss: 0.568890, acc: 75.00%] [G loss: 2.454507]\n",
      "epoch:8 step:8191 [D loss: 0.591423, acc: 69.53%] [G loss: 2.396622]\n",
      "epoch:8 step:8192 [D loss: 0.627365, acc: 65.62%] [G loss: 2.300460]\n",
      "epoch:8 step:8193 [D loss: 0.612755, acc: 67.19%] [G loss: 2.127611]\n",
      "epoch:8 step:8194 [D loss: 0.605277, acc: 65.62%] [G loss: 2.014554]\n",
      "epoch:8 step:8195 [D loss: 0.590317, acc: 68.75%] [G loss: 2.461130]\n",
      "epoch:8 step:8196 [D loss: 0.639591, acc: 65.62%] [G loss: 2.068380]\n",
      "epoch:8 step:8197 [D loss: 0.615296, acc: 63.28%] [G loss: 2.205789]\n",
      "epoch:8 step:8198 [D loss: 0.684661, acc: 61.72%] [G loss: 1.908975]\n",
      "epoch:8 step:8199 [D loss: 0.712036, acc: 53.12%] [G loss: 1.910001]\n",
      "epoch:8 step:8200 [D loss: 0.631129, acc: 66.41%] [G loss: 2.202207]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.307202\n",
      "FID: 31.697378\n",
      "0 = 13.029034468030918\n",
      "1 = 0.09149770481587101\n",
      "2 = 0.9135000109672546\n",
      "3 = 0.9223999977111816\n",
      "4 = 0.9046000242233276\n",
      "5 = 0.9062684178352356\n",
      "6 = 0.9223999977111816\n",
      "7 = 7.907285485315328\n",
      "8 = 0.1117678874333586\n",
      "9 = 0.7858999967575073\n",
      "10 = 0.795199990272522\n",
      "11 = 0.7766000032424927\n",
      "12 = 0.7806793451309204\n",
      "13 = 0.795199990272522\n",
      "14 = 6.30722713470459\n",
      "15 = 9.254362106323242\n",
      "16 = 0.17506785690784454\n",
      "17 = 6.307202339172363\n",
      "18 = 31.697378158569336\n",
      "epoch:8 step:8201 [D loss: 0.660111, acc: 60.94%] [G loss: 2.093880]\n",
      "epoch:8 step:8202 [D loss: 0.537412, acc: 75.78%] [G loss: 2.363976]\n",
      "epoch:8 step:8203 [D loss: 0.539318, acc: 73.44%] [G loss: 2.489987]\n",
      "epoch:8 step:8204 [D loss: 0.622828, acc: 64.84%] [G loss: 2.199158]\n",
      "epoch:8 step:8205 [D loss: 0.588706, acc: 67.97%] [G loss: 2.260790]\n",
      "epoch:8 step:8206 [D loss: 0.622851, acc: 66.41%] [G loss: 2.073778]\n",
      "epoch:8 step:8207 [D loss: 0.683567, acc: 64.84%] [G loss: 2.192703]\n",
      "epoch:8 step:8208 [D loss: 0.618970, acc: 67.19%] [G loss: 2.463983]\n",
      "epoch:8 step:8209 [D loss: 0.622394, acc: 65.62%] [G loss: 2.366503]\n",
      "epoch:8 step:8210 [D loss: 0.607423, acc: 65.62%] [G loss: 2.297009]\n",
      "epoch:8 step:8211 [D loss: 0.639755, acc: 63.28%] [G loss: 2.027356]\n",
      "epoch:8 step:8212 [D loss: 0.596293, acc: 68.75%] [G loss: 2.109246]\n",
      "epoch:8 step:8213 [D loss: 0.653662, acc: 63.28%] [G loss: 1.963777]\n",
      "epoch:8 step:8214 [D loss: 0.613639, acc: 67.97%] [G loss: 2.118702]\n",
      "epoch:8 step:8215 [D loss: 0.686530, acc: 62.50%] [G loss: 2.217778]\n",
      "epoch:8 step:8216 [D loss: 0.567652, acc: 70.31%] [G loss: 2.273239]\n",
      "epoch:8 step:8217 [D loss: 0.612333, acc: 62.50%] [G loss: 2.326275]\n",
      "epoch:8 step:8218 [D loss: 0.677468, acc: 66.41%] [G loss: 2.227219]\n",
      "epoch:8 step:8219 [D loss: 0.648776, acc: 62.50%] [G loss: 2.009315]\n",
      "epoch:8 step:8220 [D loss: 0.635465, acc: 67.19%] [G loss: 2.173509]\n",
      "epoch:8 step:8221 [D loss: 0.613681, acc: 63.28%] [G loss: 2.378789]\n",
      "epoch:8 step:8222 [D loss: 0.638529, acc: 65.62%] [G loss: 2.187273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8223 [D loss: 0.636108, acc: 60.94%] [G loss: 2.082391]\n",
      "epoch:8 step:8224 [D loss: 0.605972, acc: 69.53%] [G loss: 2.277188]\n",
      "epoch:8 step:8225 [D loss: 0.597747, acc: 66.41%] [G loss: 2.072337]\n",
      "epoch:8 step:8226 [D loss: 0.625286, acc: 67.97%] [G loss: 2.157815]\n",
      "epoch:8 step:8227 [D loss: 0.633307, acc: 62.50%] [G loss: 2.071369]\n",
      "epoch:8 step:8228 [D loss: 0.633048, acc: 64.06%] [G loss: 2.316972]\n",
      "epoch:8 step:8229 [D loss: 0.607128, acc: 70.31%] [G loss: 2.361529]\n",
      "epoch:8 step:8230 [D loss: 0.682700, acc: 60.94%] [G loss: 1.931843]\n",
      "epoch:8 step:8231 [D loss: 0.589100, acc: 71.09%] [G loss: 2.312888]\n",
      "epoch:8 step:8232 [D loss: 0.545972, acc: 73.44%] [G loss: 2.378509]\n",
      "epoch:8 step:8233 [D loss: 0.646692, acc: 61.72%] [G loss: 2.346554]\n",
      "epoch:8 step:8234 [D loss: 0.652985, acc: 60.16%] [G loss: 2.185901]\n",
      "epoch:8 step:8235 [D loss: 0.625748, acc: 64.84%] [G loss: 1.978486]\n",
      "epoch:8 step:8236 [D loss: 0.630472, acc: 64.06%] [G loss: 2.051069]\n",
      "epoch:8 step:8237 [D loss: 0.653012, acc: 64.06%] [G loss: 1.991293]\n",
      "epoch:8 step:8238 [D loss: 0.655021, acc: 58.59%] [G loss: 2.299657]\n",
      "epoch:8 step:8239 [D loss: 0.672834, acc: 57.81%] [G loss: 2.215458]\n",
      "epoch:8 step:8240 [D loss: 0.593904, acc: 67.19%] [G loss: 2.187430]\n",
      "epoch:8 step:8241 [D loss: 0.584162, acc: 67.97%] [G loss: 2.191548]\n",
      "epoch:8 step:8242 [D loss: 0.619203, acc: 60.94%] [G loss: 2.132110]\n",
      "epoch:8 step:8243 [D loss: 0.594694, acc: 70.31%] [G loss: 2.143463]\n",
      "epoch:8 step:8244 [D loss: 0.651984, acc: 61.72%] [G loss: 2.025988]\n",
      "epoch:8 step:8245 [D loss: 0.666338, acc: 60.94%] [G loss: 1.934795]\n",
      "epoch:8 step:8246 [D loss: 0.571921, acc: 71.88%] [G loss: 2.110529]\n",
      "epoch:8 step:8247 [D loss: 0.649513, acc: 66.41%] [G loss: 2.051865]\n",
      "epoch:8 step:8248 [D loss: 0.632142, acc: 60.16%] [G loss: 2.090887]\n",
      "epoch:8 step:8249 [D loss: 0.639372, acc: 64.06%] [G loss: 1.982049]\n",
      "epoch:8 step:8250 [D loss: 0.560310, acc: 71.09%] [G loss: 2.263149]\n",
      "epoch:8 step:8251 [D loss: 0.605795, acc: 62.50%] [G loss: 2.370684]\n",
      "epoch:8 step:8252 [D loss: 0.602445, acc: 69.53%] [G loss: 2.325272]\n",
      "epoch:8 step:8253 [D loss: 0.637229, acc: 64.84%] [G loss: 2.407928]\n",
      "epoch:8 step:8254 [D loss: 0.694362, acc: 59.38%] [G loss: 1.856003]\n",
      "epoch:8 step:8255 [D loss: 0.690540, acc: 60.16%] [G loss: 1.927717]\n",
      "epoch:8 step:8256 [D loss: 0.618673, acc: 67.97%] [G loss: 1.999750]\n",
      "epoch:8 step:8257 [D loss: 0.619811, acc: 64.84%] [G loss: 2.083644]\n",
      "epoch:8 step:8258 [D loss: 0.690780, acc: 57.03%] [G loss: 1.992381]\n",
      "epoch:8 step:8259 [D loss: 0.590341, acc: 71.09%] [G loss: 2.129325]\n",
      "epoch:8 step:8260 [D loss: 0.626805, acc: 64.84%] [G loss: 2.138694]\n",
      "epoch:8 step:8261 [D loss: 0.754099, acc: 55.47%] [G loss: 1.825915]\n",
      "epoch:8 step:8262 [D loss: 0.642517, acc: 60.16%] [G loss: 1.919143]\n",
      "epoch:8 step:8263 [D loss: 0.625675, acc: 61.72%] [G loss: 2.247750]\n",
      "epoch:8 step:8264 [D loss: 0.627786, acc: 64.06%] [G loss: 2.039901]\n",
      "epoch:8 step:8265 [D loss: 0.593616, acc: 69.53%] [G loss: 2.264445]\n",
      "epoch:8 step:8266 [D loss: 0.658063, acc: 63.28%] [G loss: 2.012039]\n",
      "epoch:8 step:8267 [D loss: 0.589788, acc: 67.97%] [G loss: 1.977620]\n",
      "epoch:8 step:8268 [D loss: 0.630049, acc: 65.62%] [G loss: 2.044884]\n",
      "epoch:8 step:8269 [D loss: 0.638084, acc: 64.84%] [G loss: 1.872852]\n",
      "epoch:8 step:8270 [D loss: 0.657812, acc: 59.38%] [G loss: 2.235998]\n",
      "epoch:8 step:8271 [D loss: 0.567576, acc: 67.19%] [G loss: 2.190493]\n",
      "epoch:8 step:8272 [D loss: 0.724841, acc: 53.12%] [G loss: 2.007524]\n",
      "epoch:8 step:8273 [D loss: 0.627704, acc: 64.06%] [G loss: 2.159970]\n",
      "epoch:8 step:8274 [D loss: 0.665789, acc: 60.94%] [G loss: 2.051501]\n",
      "epoch:8 step:8275 [D loss: 0.626259, acc: 62.50%] [G loss: 2.198987]\n",
      "epoch:8 step:8276 [D loss: 0.604956, acc: 66.41%] [G loss: 2.295072]\n",
      "epoch:8 step:8277 [D loss: 0.524677, acc: 81.25%] [G loss: 2.440636]\n",
      "epoch:8 step:8278 [D loss: 0.605842, acc: 70.31%] [G loss: 2.305124]\n",
      "epoch:8 step:8279 [D loss: 0.611589, acc: 64.84%] [G loss: 2.401614]\n",
      "epoch:8 step:8280 [D loss: 0.659674, acc: 60.94%] [G loss: 2.075025]\n",
      "epoch:8 step:8281 [D loss: 0.567275, acc: 74.22%] [G loss: 2.197771]\n",
      "epoch:8 step:8282 [D loss: 0.565144, acc: 71.88%] [G loss: 2.479949]\n",
      "epoch:8 step:8283 [D loss: 0.627843, acc: 68.75%] [G loss: 2.177751]\n",
      "epoch:8 step:8284 [D loss: 0.641926, acc: 64.84%] [G loss: 2.079878]\n",
      "epoch:8 step:8285 [D loss: 0.688693, acc: 56.25%] [G loss: 2.290428]\n",
      "epoch:8 step:8286 [D loss: 0.593212, acc: 67.19%] [G loss: 2.221071]\n",
      "epoch:8 step:8287 [D loss: 0.660922, acc: 65.62%] [G loss: 2.310192]\n",
      "epoch:8 step:8288 [D loss: 0.570045, acc: 71.88%] [G loss: 2.377340]\n",
      "epoch:8 step:8289 [D loss: 0.686230, acc: 65.62%] [G loss: 2.058925]\n",
      "epoch:8 step:8290 [D loss: 0.728969, acc: 53.91%] [G loss: 1.998508]\n",
      "epoch:8 step:8291 [D loss: 0.756667, acc: 53.91%] [G loss: 2.036510]\n",
      "epoch:8 step:8292 [D loss: 0.547731, acc: 75.00%] [G loss: 2.193327]\n",
      "epoch:8 step:8293 [D loss: 0.640315, acc: 60.16%] [G loss: 2.097698]\n",
      "epoch:8 step:8294 [D loss: 0.589044, acc: 68.75%] [G loss: 2.128716]\n",
      "epoch:8 step:8295 [D loss: 0.641642, acc: 64.06%] [G loss: 2.155157]\n",
      "epoch:8 step:8296 [D loss: 0.647458, acc: 65.62%] [G loss: 1.999716]\n",
      "epoch:8 step:8297 [D loss: 0.648172, acc: 65.62%] [G loss: 1.959890]\n",
      "epoch:8 step:8298 [D loss: 0.599241, acc: 66.41%] [G loss: 2.067036]\n",
      "epoch:8 step:8299 [D loss: 0.628714, acc: 64.84%] [G loss: 2.172875]\n",
      "epoch:8 step:8300 [D loss: 0.650919, acc: 59.38%] [G loss: 2.167283]\n",
      "epoch:8 step:8301 [D loss: 0.555791, acc: 71.09%] [G loss: 2.320866]\n",
      "epoch:8 step:8302 [D loss: 0.562616, acc: 71.09%] [G loss: 2.221946]\n",
      "epoch:8 step:8303 [D loss: 0.640588, acc: 67.19%] [G loss: 2.201300]\n",
      "epoch:8 step:8304 [D loss: 0.626689, acc: 62.50%] [G loss: 1.971641]\n",
      "epoch:8 step:8305 [D loss: 0.706716, acc: 56.25%] [G loss: 2.089139]\n",
      "epoch:8 step:8306 [D loss: 0.634017, acc: 59.38%] [G loss: 2.185445]\n",
      "epoch:8 step:8307 [D loss: 0.595333, acc: 67.97%] [G loss: 2.100379]\n",
      "epoch:8 step:8308 [D loss: 0.609837, acc: 66.41%] [G loss: 2.067171]\n",
      "epoch:8 step:8309 [D loss: 0.558693, acc: 71.88%] [G loss: 2.208554]\n",
      "epoch:8 step:8310 [D loss: 0.658737, acc: 65.62%] [G loss: 2.125422]\n",
      "epoch:8 step:8311 [D loss: 0.577583, acc: 68.75%] [G loss: 2.495042]\n",
      "epoch:8 step:8312 [D loss: 0.588903, acc: 67.97%] [G loss: 2.395339]\n",
      "epoch:8 step:8313 [D loss: 0.666028, acc: 67.19%] [G loss: 2.152455]\n",
      "epoch:8 step:8314 [D loss: 0.656549, acc: 64.06%] [G loss: 2.136205]\n",
      "epoch:8 step:8315 [D loss: 0.642807, acc: 67.97%] [G loss: 2.216780]\n",
      "epoch:8 step:8316 [D loss: 0.707523, acc: 58.59%] [G loss: 1.995531]\n",
      "epoch:8 step:8317 [D loss: 0.606935, acc: 64.84%] [G loss: 2.133627]\n",
      "epoch:8 step:8318 [D loss: 0.612866, acc: 69.53%] [G loss: 2.206328]\n",
      "epoch:8 step:8319 [D loss: 0.618700, acc: 67.19%] [G loss: 2.245617]\n",
      "epoch:8 step:8320 [D loss: 0.636853, acc: 60.94%] [G loss: 1.976869]\n",
      "epoch:8 step:8321 [D loss: 0.610333, acc: 64.84%] [G loss: 2.305006]\n",
      "epoch:8 step:8322 [D loss: 0.652014, acc: 63.28%] [G loss: 2.004874]\n",
      "epoch:8 step:8323 [D loss: 0.738315, acc: 57.03%] [G loss: 2.013824]\n",
      "epoch:8 step:8324 [D loss: 0.753286, acc: 53.12%] [G loss: 2.090879]\n",
      "epoch:8 step:8325 [D loss: 0.645021, acc: 64.84%] [G loss: 1.974267]\n",
      "epoch:8 step:8326 [D loss: 0.657155, acc: 60.16%] [G loss: 2.003158]\n",
      "epoch:8 step:8327 [D loss: 0.616487, acc: 65.62%] [G loss: 2.255544]\n",
      "epoch:8 step:8328 [D loss: 0.628040, acc: 65.62%] [G loss: 2.003791]\n",
      "epoch:8 step:8329 [D loss: 0.541549, acc: 73.44%] [G loss: 2.278447]\n",
      "epoch:8 step:8330 [D loss: 0.624803, acc: 73.44%] [G loss: 2.106969]\n",
      "epoch:8 step:8331 [D loss: 0.658345, acc: 60.16%] [G loss: 1.972191]\n",
      "epoch:8 step:8332 [D loss: 0.545167, acc: 72.66%] [G loss: 2.025007]\n",
      "epoch:8 step:8333 [D loss: 0.589724, acc: 66.41%] [G loss: 2.209681]\n",
      "epoch:8 step:8334 [D loss: 0.579079, acc: 66.41%] [G loss: 2.200099]\n",
      "epoch:8 step:8335 [D loss: 0.659018, acc: 60.94%] [G loss: 2.256943]\n",
      "epoch:8 step:8336 [D loss: 0.644950, acc: 66.41%] [G loss: 2.099413]\n",
      "epoch:8 step:8337 [D loss: 0.559389, acc: 66.41%] [G loss: 2.251255]\n",
      "epoch:8 step:8338 [D loss: 0.634320, acc: 62.50%] [G loss: 2.129591]\n",
      "epoch:8 step:8339 [D loss: 0.634431, acc: 60.94%] [G loss: 2.147721]\n",
      "epoch:8 step:8340 [D loss: 0.639090, acc: 66.41%] [G loss: 1.989293]\n",
      "epoch:8 step:8341 [D loss: 0.588667, acc: 67.19%] [G loss: 2.069229]\n",
      "epoch:8 step:8342 [D loss: 0.644659, acc: 63.28%] [G loss: 2.293817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8343 [D loss: 0.607564, acc: 69.53%] [G loss: 2.282365]\n",
      "epoch:8 step:8344 [D loss: 0.626839, acc: 64.84%] [G loss: 2.202104]\n",
      "epoch:8 step:8345 [D loss: 0.600469, acc: 66.41%] [G loss: 2.302382]\n",
      "epoch:8 step:8346 [D loss: 0.655882, acc: 59.38%] [G loss: 2.254498]\n",
      "epoch:8 step:8347 [D loss: 0.670598, acc: 63.28%] [G loss: 2.085079]\n",
      "epoch:8 step:8348 [D loss: 0.582444, acc: 71.09%] [G loss: 2.163910]\n",
      "epoch:8 step:8349 [D loss: 0.570109, acc: 71.09%] [G loss: 2.255861]\n",
      "epoch:8 step:8350 [D loss: 0.686257, acc: 59.38%] [G loss: 2.138815]\n",
      "epoch:8 step:8351 [D loss: 0.608246, acc: 67.19%] [G loss: 1.935673]\n",
      "epoch:8 step:8352 [D loss: 0.706972, acc: 60.16%] [G loss: 2.005977]\n",
      "epoch:8 step:8353 [D loss: 0.620908, acc: 62.50%] [G loss: 2.171496]\n",
      "epoch:8 step:8354 [D loss: 0.729561, acc: 58.59%] [G loss: 2.107817]\n",
      "epoch:8 step:8355 [D loss: 0.676877, acc: 59.38%] [G loss: 2.062435]\n",
      "epoch:8 step:8356 [D loss: 0.613268, acc: 67.97%] [G loss: 2.195995]\n",
      "epoch:8 step:8357 [D loss: 0.641994, acc: 67.97%] [G loss: 1.953887]\n",
      "epoch:8 step:8358 [D loss: 0.648179, acc: 59.38%] [G loss: 2.244213]\n",
      "epoch:8 step:8359 [D loss: 0.593183, acc: 67.19%] [G loss: 2.045586]\n",
      "epoch:8 step:8360 [D loss: 0.640248, acc: 65.62%] [G loss: 2.190040]\n",
      "epoch:8 step:8361 [D loss: 0.623700, acc: 71.09%] [G loss: 1.967196]\n",
      "epoch:8 step:8362 [D loss: 0.616248, acc: 70.31%] [G loss: 2.062016]\n",
      "epoch:8 step:8363 [D loss: 0.659208, acc: 60.16%] [G loss: 1.899513]\n",
      "epoch:8 step:8364 [D loss: 0.677707, acc: 60.94%] [G loss: 2.226448]\n",
      "epoch:8 step:8365 [D loss: 0.717932, acc: 51.56%] [G loss: 1.926278]\n",
      "epoch:8 step:8366 [D loss: 0.665146, acc: 60.16%] [G loss: 2.157146]\n",
      "epoch:8 step:8367 [D loss: 0.601910, acc: 67.97%] [G loss: 2.070548]\n",
      "epoch:8 step:8368 [D loss: 0.649553, acc: 67.19%] [G loss: 2.105340]\n",
      "epoch:8 step:8369 [D loss: 0.627591, acc: 59.38%] [G loss: 2.020418]\n",
      "epoch:8 step:8370 [D loss: 0.651399, acc: 61.72%] [G loss: 1.991034]\n",
      "epoch:8 step:8371 [D loss: 0.627424, acc: 64.06%] [G loss: 2.180837]\n",
      "epoch:8 step:8372 [D loss: 0.601062, acc: 72.66%] [G loss: 2.216093]\n",
      "epoch:8 step:8373 [D loss: 0.621475, acc: 64.84%] [G loss: 2.139692]\n",
      "epoch:8 step:8374 [D loss: 0.630260, acc: 63.28%] [G loss: 2.008705]\n",
      "epoch:8 step:8375 [D loss: 0.615562, acc: 63.28%] [G loss: 1.947154]\n",
      "epoch:8 step:8376 [D loss: 0.648335, acc: 62.50%] [G loss: 1.989501]\n",
      "epoch:8 step:8377 [D loss: 0.582761, acc: 70.31%] [G loss: 1.917458]\n",
      "epoch:8 step:8378 [D loss: 0.643619, acc: 61.72%] [G loss: 1.869089]\n",
      "epoch:8 step:8379 [D loss: 0.661494, acc: 60.94%] [G loss: 2.344244]\n",
      "epoch:8 step:8380 [D loss: 0.563896, acc: 71.88%] [G loss: 2.366419]\n",
      "epoch:8 step:8381 [D loss: 0.588713, acc: 70.31%] [G loss: 2.025423]\n",
      "epoch:8 step:8382 [D loss: 0.609418, acc: 67.19%] [G loss: 2.351974]\n",
      "epoch:8 step:8383 [D loss: 0.615483, acc: 61.72%] [G loss: 2.278085]\n",
      "epoch:8 step:8384 [D loss: 0.521746, acc: 72.66%] [G loss: 2.168825]\n",
      "epoch:8 step:8385 [D loss: 0.617187, acc: 63.28%] [G loss: 2.191970]\n",
      "epoch:8 step:8386 [D loss: 0.626698, acc: 62.50%] [G loss: 2.314196]\n",
      "epoch:8 step:8387 [D loss: 0.623918, acc: 64.84%] [G loss: 2.077319]\n",
      "epoch:8 step:8388 [D loss: 0.738154, acc: 53.91%] [G loss: 1.961345]\n",
      "epoch:8 step:8389 [D loss: 0.623429, acc: 60.94%] [G loss: 2.210568]\n",
      "epoch:8 step:8390 [D loss: 0.607945, acc: 65.62%] [G loss: 2.107677]\n",
      "epoch:8 step:8391 [D loss: 0.633397, acc: 67.19%] [G loss: 2.298095]\n",
      "epoch:8 step:8392 [D loss: 0.644907, acc: 60.94%] [G loss: 2.058927]\n",
      "epoch:8 step:8393 [D loss: 0.566296, acc: 68.75%] [G loss: 2.222980]\n",
      "epoch:8 step:8394 [D loss: 0.674077, acc: 60.94%] [G loss: 2.193930]\n",
      "epoch:8 step:8395 [D loss: 0.593849, acc: 71.88%] [G loss: 2.387693]\n",
      "epoch:8 step:8396 [D loss: 0.611674, acc: 71.09%] [G loss: 2.277773]\n",
      "epoch:8 step:8397 [D loss: 0.694775, acc: 67.19%] [G loss: 2.172966]\n",
      "epoch:8 step:8398 [D loss: 0.643221, acc: 61.72%] [G loss: 1.918003]\n",
      "epoch:8 step:8399 [D loss: 0.668645, acc: 67.19%] [G loss: 2.321982]\n",
      "epoch:8 step:8400 [D loss: 0.595154, acc: 71.09%] [G loss: 2.201513]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.557808\n",
      "FID: 25.896500\n",
      "0 = 12.965278908729525\n",
      "1 = 0.08351393838005461\n",
      "2 = 0.9067000150680542\n",
      "3 = 0.9211999773979187\n",
      "4 = 0.8921999931335449\n",
      "5 = 0.8952381014823914\n",
      "6 = 0.9211999773979187\n",
      "7 = 7.528756152701401\n",
      "8 = 0.10217919552157632\n",
      "9 = 0.7702000141143799\n",
      "10 = 0.769599974155426\n",
      "11 = 0.770799994468689\n",
      "12 = 0.7705246210098267\n",
      "13 = 0.769599974155426\n",
      "14 = 6.557833671569824\n",
      "15 = 9.27282428741455\n",
      "16 = 0.16650272905826569\n",
      "17 = 6.5578083992004395\n",
      "18 = 25.896499633789062\n",
      "epoch:8 step:8401 [D loss: 0.647703, acc: 59.38%] [G loss: 2.299851]\n",
      "epoch:8 step:8402 [D loss: 0.603776, acc: 70.31%] [G loss: 2.110617]\n",
      "epoch:8 step:8403 [D loss: 0.607994, acc: 61.72%] [G loss: 2.230897]\n",
      "epoch:8 step:8404 [D loss: 0.619636, acc: 65.62%] [G loss: 2.087106]\n",
      "epoch:8 step:8405 [D loss: 0.585622, acc: 71.09%] [G loss: 2.235793]\n",
      "epoch:8 step:8406 [D loss: 0.642950, acc: 62.50%] [G loss: 2.097364]\n",
      "epoch:8 step:8407 [D loss: 0.608901, acc: 63.28%] [G loss: 2.259744]\n",
      "epoch:8 step:8408 [D loss: 0.563076, acc: 70.31%] [G loss: 2.717043]\n",
      "epoch:8 step:8409 [D loss: 0.622259, acc: 64.06%] [G loss: 2.158086]\n",
      "epoch:8 step:8410 [D loss: 0.611265, acc: 64.84%] [G loss: 2.090106]\n",
      "epoch:8 step:8411 [D loss: 0.611393, acc: 67.97%] [G loss: 2.115019]\n",
      "epoch:8 step:8412 [D loss: 0.604703, acc: 62.50%] [G loss: 2.181870]\n",
      "epoch:8 step:8413 [D loss: 0.580300, acc: 71.88%] [G loss: 1.974643]\n",
      "epoch:8 step:8414 [D loss: 0.524156, acc: 75.78%] [G loss: 2.305216]\n",
      "epoch:8 step:8415 [D loss: 0.565295, acc: 70.31%] [G loss: 2.457108]\n",
      "epoch:8 step:8416 [D loss: 0.755927, acc: 52.34%] [G loss: 2.231628]\n",
      "epoch:8 step:8417 [D loss: 0.569567, acc: 71.88%] [G loss: 2.267423]\n",
      "epoch:8 step:8418 [D loss: 0.625269, acc: 60.94%] [G loss: 2.164285]\n",
      "epoch:8 step:8419 [D loss: 0.541034, acc: 76.56%] [G loss: 2.447499]\n",
      "epoch:8 step:8420 [D loss: 0.531261, acc: 79.69%] [G loss: 2.625782]\n",
      "epoch:8 step:8421 [D loss: 0.607294, acc: 67.97%] [G loss: 2.538960]\n",
      "epoch:8 step:8422 [D loss: 0.646002, acc: 60.94%] [G loss: 2.594069]\n",
      "epoch:8 step:8423 [D loss: 0.640763, acc: 64.84%] [G loss: 2.442036]\n",
      "epoch:8 step:8424 [D loss: 0.780888, acc: 53.91%] [G loss: 2.077695]\n",
      "epoch:8 step:8425 [D loss: 0.709666, acc: 57.81%] [G loss: 2.144996]\n",
      "epoch:8 step:8426 [D loss: 0.659678, acc: 61.72%] [G loss: 2.117213]\n",
      "epoch:8 step:8427 [D loss: 0.610404, acc: 58.59%] [G loss: 2.030907]\n",
      "epoch:8 step:8428 [D loss: 0.577108, acc: 68.75%] [G loss: 2.238580]\n",
      "epoch:8 step:8429 [D loss: 0.519491, acc: 75.78%] [G loss: 2.314652]\n",
      "epoch:8 step:8430 [D loss: 0.703025, acc: 58.59%] [G loss: 2.160835]\n",
      "epoch:8 step:8431 [D loss: 0.686321, acc: 61.72%] [G loss: 2.063385]\n",
      "epoch:8 step:8432 [D loss: 0.533545, acc: 74.22%] [G loss: 2.386789]\n",
      "epoch:8 step:8433 [D loss: 0.529985, acc: 75.00%] [G loss: 2.659851]\n",
      "epoch:9 step:8434 [D loss: 0.645233, acc: 60.94%] [G loss: 2.148632]\n",
      "epoch:9 step:8435 [D loss: 0.582431, acc: 67.19%] [G loss: 2.359492]\n",
      "epoch:9 step:8436 [D loss: 0.629623, acc: 64.84%] [G loss: 2.231236]\n",
      "epoch:9 step:8437 [D loss: 0.647386, acc: 64.06%] [G loss: 2.077307]\n",
      "epoch:9 step:8438 [D loss: 0.642331, acc: 64.84%] [G loss: 2.141800]\n",
      "epoch:9 step:8439 [D loss: 0.629641, acc: 69.53%] [G loss: 2.231708]\n",
      "epoch:9 step:8440 [D loss: 0.611508, acc: 69.53%] [G loss: 2.303427]\n",
      "epoch:9 step:8441 [D loss: 0.647368, acc: 64.84%] [G loss: 2.244193]\n",
      "epoch:9 step:8442 [D loss: 0.672747, acc: 65.62%] [G loss: 2.275444]\n",
      "epoch:9 step:8443 [D loss: 0.635650, acc: 62.50%] [G loss: 2.155825]\n",
      "epoch:9 step:8444 [D loss: 0.569868, acc: 67.19%] [G loss: 2.332934]\n",
      "epoch:9 step:8445 [D loss: 0.574675, acc: 71.09%] [G loss: 2.342672]\n",
      "epoch:9 step:8446 [D loss: 0.598434, acc: 67.19%] [G loss: 2.324924]\n",
      "epoch:9 step:8447 [D loss: 0.586414, acc: 68.75%] [G loss: 2.101834]\n",
      "epoch:9 step:8448 [D loss: 0.678880, acc: 60.94%] [G loss: 2.467792]\n",
      "epoch:9 step:8449 [D loss: 0.597562, acc: 67.19%] [G loss: 2.395631]\n",
      "epoch:9 step:8450 [D loss: 0.696290, acc: 50.78%] [G loss: 2.007879]\n",
      "epoch:9 step:8451 [D loss: 0.665332, acc: 60.16%] [G loss: 2.031099]\n",
      "epoch:9 step:8452 [D loss: 0.663099, acc: 58.59%] [G loss: 1.972403]\n",
      "epoch:9 step:8453 [D loss: 0.636046, acc: 67.19%] [G loss: 1.796227]\n",
      "epoch:9 step:8454 [D loss: 0.595596, acc: 66.41%] [G loss: 2.087457]\n",
      "epoch:9 step:8455 [D loss: 0.618754, acc: 65.62%] [G loss: 2.207508]\n",
      "epoch:9 step:8456 [D loss: 0.545097, acc: 76.56%] [G loss: 2.238515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8457 [D loss: 0.653934, acc: 62.50%] [G loss: 2.165582]\n",
      "epoch:9 step:8458 [D loss: 0.627317, acc: 61.72%] [G loss: 2.246274]\n",
      "epoch:9 step:8459 [D loss: 0.628015, acc: 67.19%] [G loss: 2.067880]\n",
      "epoch:9 step:8460 [D loss: 0.640586, acc: 60.94%] [G loss: 2.095971]\n",
      "epoch:9 step:8461 [D loss: 0.655731, acc: 59.38%] [G loss: 2.003432]\n",
      "epoch:9 step:8462 [D loss: 0.601250, acc: 73.44%] [G loss: 1.973429]\n",
      "epoch:9 step:8463 [D loss: 0.616538, acc: 64.06%] [G loss: 1.925462]\n",
      "epoch:9 step:8464 [D loss: 0.687956, acc: 57.03%] [G loss: 2.020654]\n",
      "epoch:9 step:8465 [D loss: 0.653257, acc: 61.72%] [G loss: 1.863108]\n",
      "epoch:9 step:8466 [D loss: 0.655351, acc: 64.06%] [G loss: 2.040229]\n",
      "epoch:9 step:8467 [D loss: 0.648916, acc: 64.06%] [G loss: 1.915826]\n",
      "epoch:9 step:8468 [D loss: 0.601524, acc: 66.41%] [G loss: 1.950142]\n",
      "epoch:9 step:8469 [D loss: 0.617955, acc: 65.62%] [G loss: 2.307757]\n",
      "epoch:9 step:8470 [D loss: 0.624977, acc: 65.62%] [G loss: 2.056890]\n",
      "epoch:9 step:8471 [D loss: 0.640903, acc: 66.41%] [G loss: 2.224177]\n",
      "epoch:9 step:8472 [D loss: 0.589492, acc: 69.53%] [G loss: 2.185307]\n",
      "epoch:9 step:8473 [D loss: 0.623771, acc: 65.62%] [G loss: 2.578606]\n",
      "epoch:9 step:8474 [D loss: 0.622451, acc: 67.19%] [G loss: 2.087505]\n",
      "epoch:9 step:8475 [D loss: 0.564612, acc: 64.84%] [G loss: 2.132024]\n",
      "epoch:9 step:8476 [D loss: 0.592664, acc: 69.53%] [G loss: 2.238802]\n",
      "epoch:9 step:8477 [D loss: 0.672742, acc: 57.81%] [G loss: 2.166607]\n",
      "epoch:9 step:8478 [D loss: 0.643371, acc: 60.16%] [G loss: 2.172794]\n",
      "epoch:9 step:8479 [D loss: 0.586085, acc: 71.09%] [G loss: 2.093135]\n",
      "epoch:9 step:8480 [D loss: 0.604504, acc: 64.06%] [G loss: 2.171164]\n",
      "epoch:9 step:8481 [D loss: 0.661868, acc: 64.06%] [G loss: 2.131917]\n",
      "epoch:9 step:8482 [D loss: 0.660309, acc: 63.28%] [G loss: 2.282059]\n",
      "epoch:9 step:8483 [D loss: 0.601800, acc: 70.31%] [G loss: 2.202441]\n",
      "epoch:9 step:8484 [D loss: 0.637600, acc: 67.97%] [G loss: 2.137381]\n",
      "epoch:9 step:8485 [D loss: 0.662279, acc: 63.28%] [G loss: 2.074296]\n",
      "epoch:9 step:8486 [D loss: 0.623787, acc: 69.53%] [G loss: 2.261233]\n",
      "epoch:9 step:8487 [D loss: 0.580864, acc: 69.53%] [G loss: 2.343720]\n",
      "epoch:9 step:8488 [D loss: 0.586057, acc: 71.88%] [G loss: 2.338166]\n",
      "epoch:9 step:8489 [D loss: 0.568058, acc: 72.66%] [G loss: 2.300798]\n",
      "epoch:9 step:8490 [D loss: 0.645663, acc: 61.72%] [G loss: 2.210046]\n",
      "epoch:9 step:8491 [D loss: 0.609346, acc: 64.84%] [G loss: 2.031147]\n",
      "epoch:9 step:8492 [D loss: 0.757067, acc: 52.34%] [G loss: 2.083008]\n",
      "epoch:9 step:8493 [D loss: 0.626557, acc: 61.72%] [G loss: 2.042231]\n",
      "epoch:9 step:8494 [D loss: 0.593048, acc: 66.41%] [G loss: 2.096449]\n",
      "epoch:9 step:8495 [D loss: 0.724241, acc: 58.59%] [G loss: 2.189592]\n",
      "epoch:9 step:8496 [D loss: 0.647524, acc: 61.72%] [G loss: 2.228088]\n",
      "epoch:9 step:8497 [D loss: 0.623448, acc: 67.19%] [G loss: 2.294929]\n",
      "epoch:9 step:8498 [D loss: 0.619678, acc: 65.62%] [G loss: 2.007240]\n",
      "epoch:9 step:8499 [D loss: 0.617499, acc: 71.88%] [G loss: 1.990352]\n",
      "epoch:9 step:8500 [D loss: 0.619357, acc: 64.06%] [G loss: 2.234178]\n",
      "epoch:9 step:8501 [D loss: 0.639781, acc: 64.84%] [G loss: 2.300476]\n",
      "epoch:9 step:8502 [D loss: 0.566786, acc: 69.53%] [G loss: 2.301392]\n",
      "epoch:9 step:8503 [D loss: 0.595857, acc: 69.53%] [G loss: 2.217501]\n",
      "epoch:9 step:8504 [D loss: 0.651045, acc: 65.62%] [G loss: 2.131766]\n",
      "epoch:9 step:8505 [D loss: 0.663479, acc: 63.28%] [G loss: 2.125823]\n",
      "epoch:9 step:8506 [D loss: 0.672912, acc: 57.03%] [G loss: 2.032330]\n",
      "epoch:9 step:8507 [D loss: 0.583816, acc: 67.19%] [G loss: 2.207057]\n",
      "epoch:9 step:8508 [D loss: 0.615223, acc: 66.41%] [G loss: 2.560007]\n",
      "epoch:9 step:8509 [D loss: 0.618503, acc: 67.97%] [G loss: 2.393156]\n",
      "epoch:9 step:8510 [D loss: 0.606772, acc: 66.41%] [G loss: 2.452577]\n",
      "epoch:9 step:8511 [D loss: 0.659430, acc: 60.16%] [G loss: 1.954596]\n",
      "epoch:9 step:8512 [D loss: 0.626682, acc: 64.06%] [G loss: 1.919689]\n",
      "epoch:9 step:8513 [D loss: 0.728463, acc: 58.59%] [G loss: 2.039597]\n",
      "epoch:9 step:8514 [D loss: 0.693844, acc: 53.91%] [G loss: 1.974235]\n",
      "epoch:9 step:8515 [D loss: 0.613145, acc: 64.06%] [G loss: 2.184209]\n",
      "epoch:9 step:8516 [D loss: 0.602701, acc: 66.41%] [G loss: 2.284200]\n",
      "epoch:9 step:8517 [D loss: 0.603635, acc: 67.97%] [G loss: 2.010139]\n",
      "epoch:9 step:8518 [D loss: 0.622911, acc: 64.84%] [G loss: 1.928625]\n",
      "epoch:9 step:8519 [D loss: 0.699859, acc: 61.72%] [G loss: 1.930912]\n",
      "epoch:9 step:8520 [D loss: 0.600658, acc: 67.19%] [G loss: 1.874870]\n",
      "epoch:9 step:8521 [D loss: 0.655480, acc: 64.84%] [G loss: 2.009763]\n",
      "epoch:9 step:8522 [D loss: 0.596458, acc: 71.88%] [G loss: 2.032092]\n",
      "epoch:9 step:8523 [D loss: 0.618088, acc: 63.28%] [G loss: 2.146856]\n",
      "epoch:9 step:8524 [D loss: 0.649446, acc: 61.72%] [G loss: 1.951093]\n",
      "epoch:9 step:8525 [D loss: 0.567215, acc: 66.41%] [G loss: 2.334656]\n",
      "epoch:9 step:8526 [D loss: 0.556066, acc: 71.09%] [G loss: 2.351639]\n",
      "epoch:9 step:8527 [D loss: 0.622826, acc: 67.19%] [G loss: 2.001804]\n",
      "epoch:9 step:8528 [D loss: 0.657980, acc: 63.28%] [G loss: 1.976683]\n",
      "epoch:9 step:8529 [D loss: 0.605669, acc: 67.97%] [G loss: 2.115662]\n",
      "epoch:9 step:8530 [D loss: 0.588106, acc: 65.62%] [G loss: 2.199834]\n",
      "epoch:9 step:8531 [D loss: 0.639208, acc: 62.50%] [G loss: 1.970418]\n",
      "epoch:9 step:8532 [D loss: 0.615108, acc: 67.97%] [G loss: 2.133815]\n",
      "epoch:9 step:8533 [D loss: 0.624787, acc: 64.06%] [G loss: 2.225899]\n",
      "epoch:9 step:8534 [D loss: 0.674130, acc: 59.38%] [G loss: 2.012564]\n",
      "epoch:9 step:8535 [D loss: 0.669024, acc: 59.38%] [G loss: 1.952133]\n",
      "epoch:9 step:8536 [D loss: 0.553872, acc: 69.53%] [G loss: 2.323554]\n",
      "epoch:9 step:8537 [D loss: 0.640657, acc: 66.41%] [G loss: 2.068422]\n",
      "epoch:9 step:8538 [D loss: 0.654639, acc: 64.06%] [G loss: 2.083714]\n",
      "epoch:9 step:8539 [D loss: 0.633682, acc: 66.41%] [G loss: 2.133792]\n",
      "epoch:9 step:8540 [D loss: 0.601078, acc: 67.19%] [G loss: 1.963607]\n",
      "epoch:9 step:8541 [D loss: 0.648959, acc: 63.28%] [G loss: 1.993570]\n",
      "epoch:9 step:8542 [D loss: 0.642079, acc: 60.94%] [G loss: 2.081332]\n",
      "epoch:9 step:8543 [D loss: 0.633475, acc: 62.50%] [G loss: 2.165508]\n",
      "epoch:9 step:8544 [D loss: 0.654513, acc: 64.84%] [G loss: 2.137669]\n",
      "epoch:9 step:8545 [D loss: 0.624899, acc: 60.16%] [G loss: 2.176568]\n",
      "epoch:9 step:8546 [D loss: 0.669511, acc: 60.94%] [G loss: 2.182449]\n",
      "epoch:9 step:8547 [D loss: 0.579522, acc: 69.53%] [G loss: 2.286661]\n",
      "epoch:9 step:8548 [D loss: 0.626465, acc: 62.50%] [G loss: 2.251390]\n",
      "epoch:9 step:8549 [D loss: 0.566667, acc: 73.44%] [G loss: 2.337156]\n",
      "epoch:9 step:8550 [D loss: 0.614902, acc: 69.53%] [G loss: 2.227377]\n",
      "epoch:9 step:8551 [D loss: 0.604453, acc: 65.62%] [G loss: 2.263976]\n",
      "epoch:9 step:8552 [D loss: 0.646959, acc: 59.38%] [G loss: 2.423435]\n",
      "epoch:9 step:8553 [D loss: 0.642444, acc: 60.94%] [G loss: 2.279760]\n",
      "epoch:9 step:8554 [D loss: 0.667111, acc: 59.38%] [G loss: 2.177256]\n",
      "epoch:9 step:8555 [D loss: 0.644609, acc: 60.94%] [G loss: 2.241360]\n",
      "epoch:9 step:8556 [D loss: 0.611205, acc: 67.19%] [G loss: 2.030644]\n",
      "epoch:9 step:8557 [D loss: 0.627618, acc: 67.19%] [G loss: 2.201055]\n",
      "epoch:9 step:8558 [D loss: 0.637788, acc: 55.47%] [G loss: 1.839587]\n",
      "epoch:9 step:8559 [D loss: 0.630018, acc: 58.59%] [G loss: 2.250049]\n",
      "epoch:9 step:8560 [D loss: 0.623508, acc: 64.84%] [G loss: 2.032793]\n",
      "epoch:9 step:8561 [D loss: 0.643498, acc: 60.16%] [G loss: 2.100548]\n",
      "epoch:9 step:8562 [D loss: 0.661127, acc: 60.94%] [G loss: 2.005925]\n",
      "epoch:9 step:8563 [D loss: 0.578385, acc: 72.66%] [G loss: 2.072851]\n",
      "epoch:9 step:8564 [D loss: 0.630420, acc: 65.62%] [G loss: 2.280165]\n",
      "epoch:9 step:8565 [D loss: 0.610250, acc: 62.50%] [G loss: 2.184975]\n",
      "epoch:9 step:8566 [D loss: 0.751093, acc: 50.00%] [G loss: 1.885148]\n",
      "epoch:9 step:8567 [D loss: 0.627650, acc: 67.19%] [G loss: 2.023467]\n",
      "epoch:9 step:8568 [D loss: 0.630624, acc: 62.50%] [G loss: 2.041827]\n",
      "epoch:9 step:8569 [D loss: 0.693796, acc: 58.59%] [G loss: 1.985771]\n",
      "epoch:9 step:8570 [D loss: 0.683089, acc: 60.94%] [G loss: 2.054535]\n",
      "epoch:9 step:8571 [D loss: 0.633784, acc: 62.50%] [G loss: 2.136250]\n",
      "epoch:9 step:8572 [D loss: 0.642570, acc: 68.75%] [G loss: 2.154334]\n",
      "epoch:9 step:8573 [D loss: 0.687482, acc: 60.94%] [G loss: 1.797769]\n",
      "epoch:9 step:8574 [D loss: 0.624558, acc: 66.41%] [G loss: 1.999438]\n",
      "epoch:9 step:8575 [D loss: 0.556186, acc: 74.22%] [G loss: 1.941802]\n",
      "epoch:9 step:8576 [D loss: 0.631662, acc: 63.28%] [G loss: 2.007473]\n",
      "epoch:9 step:8577 [D loss: 0.609815, acc: 67.97%] [G loss: 2.216940]\n",
      "epoch:9 step:8578 [D loss: 0.594117, acc: 72.66%] [G loss: 2.033854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8579 [D loss: 0.611202, acc: 65.62%] [G loss: 2.104556]\n",
      "epoch:9 step:8580 [D loss: 0.650839, acc: 65.62%] [G loss: 2.157918]\n",
      "epoch:9 step:8581 [D loss: 0.633752, acc: 64.84%] [G loss: 1.856503]\n",
      "epoch:9 step:8582 [D loss: 0.618053, acc: 67.19%] [G loss: 2.127916]\n",
      "epoch:9 step:8583 [D loss: 0.594010, acc: 68.75%] [G loss: 1.963838]\n",
      "epoch:9 step:8584 [D loss: 0.597919, acc: 65.62%] [G loss: 2.347970]\n",
      "epoch:9 step:8585 [D loss: 0.647431, acc: 62.50%] [G loss: 2.079078]\n",
      "epoch:9 step:8586 [D loss: 0.628168, acc: 64.84%] [G loss: 2.084894]\n",
      "epoch:9 step:8587 [D loss: 0.636700, acc: 53.91%] [G loss: 2.160557]\n",
      "epoch:9 step:8588 [D loss: 0.629084, acc: 61.72%] [G loss: 2.124018]\n",
      "epoch:9 step:8589 [D loss: 0.610261, acc: 66.41%] [G loss: 2.153857]\n",
      "epoch:9 step:8590 [D loss: 0.647224, acc: 63.28%] [G loss: 2.002702]\n",
      "epoch:9 step:8591 [D loss: 0.658874, acc: 60.16%] [G loss: 2.117357]\n",
      "epoch:9 step:8592 [D loss: 0.618879, acc: 70.31%] [G loss: 2.089989]\n",
      "epoch:9 step:8593 [D loss: 0.654421, acc: 60.94%] [G loss: 1.891048]\n",
      "epoch:9 step:8594 [D loss: 0.655920, acc: 63.28%] [G loss: 2.270973]\n",
      "epoch:9 step:8595 [D loss: 0.583757, acc: 69.53%] [G loss: 2.168881]\n",
      "epoch:9 step:8596 [D loss: 0.602509, acc: 67.97%] [G loss: 2.175684]\n",
      "epoch:9 step:8597 [D loss: 0.684928, acc: 60.16%] [G loss: 2.183572]\n",
      "epoch:9 step:8598 [D loss: 0.640926, acc: 61.72%] [G loss: 2.208123]\n",
      "epoch:9 step:8599 [D loss: 0.590114, acc: 73.44%] [G loss: 2.171180]\n",
      "epoch:9 step:8600 [D loss: 0.656256, acc: 60.16%] [G loss: 1.941777]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.532369\n",
      "FID: 26.025101\n",
      "0 = 12.89161146993636\n",
      "1 = 0.08695725642806128\n",
      "2 = 0.9071000218391418\n",
      "3 = 0.9121999740600586\n",
      "4 = 0.9020000100135803\n",
      "5 = 0.9029895067214966\n",
      "6 = 0.9121999740600586\n",
      "7 = 7.59731669352055\n",
      "8 = 0.10160885266611622\n",
      "9 = 0.7788000106811523\n",
      "10 = 0.7843999862670898\n",
      "11 = 0.7731999754905701\n",
      "12 = 0.7757120132446289\n",
      "13 = 0.7843999862670898\n",
      "14 = 6.532393932342529\n",
      "15 = 9.295928955078125\n",
      "16 = 0.16777649521827698\n",
      "17 = 6.532369136810303\n",
      "18 = 26.025100708007812\n",
      "epoch:9 step:8601 [D loss: 0.581961, acc: 66.41%] [G loss: 2.041345]\n",
      "epoch:9 step:8602 [D loss: 0.674558, acc: 62.50%] [G loss: 2.113955]\n",
      "epoch:9 step:8603 [D loss: 0.704562, acc: 55.47%] [G loss: 2.087182]\n",
      "epoch:9 step:8604 [D loss: 0.673391, acc: 66.41%] [G loss: 2.207113]\n",
      "epoch:9 step:8605 [D loss: 0.556149, acc: 67.97%] [G loss: 2.209311]\n",
      "epoch:9 step:8606 [D loss: 0.646180, acc: 67.97%] [G loss: 2.041667]\n",
      "epoch:9 step:8607 [D loss: 0.695544, acc: 58.59%] [G loss: 2.034304]\n",
      "epoch:9 step:8608 [D loss: 0.588638, acc: 67.97%] [G loss: 2.094376]\n",
      "epoch:9 step:8609 [D loss: 0.662820, acc: 62.50%] [G loss: 1.790441]\n",
      "epoch:9 step:8610 [D loss: 0.606304, acc: 71.88%] [G loss: 2.153453]\n",
      "epoch:9 step:8611 [D loss: 0.656488, acc: 60.16%] [G loss: 1.870654]\n",
      "epoch:9 step:8612 [D loss: 0.653306, acc: 61.72%] [G loss: 1.973227]\n",
      "epoch:9 step:8613 [D loss: 0.629790, acc: 67.19%] [G loss: 2.040297]\n",
      "epoch:9 step:8614 [D loss: 0.671537, acc: 63.28%] [G loss: 2.074637]\n",
      "epoch:9 step:8615 [D loss: 0.613518, acc: 67.97%] [G loss: 2.097222]\n",
      "epoch:9 step:8616 [D loss: 0.685036, acc: 59.38%] [G loss: 1.893978]\n",
      "epoch:9 step:8617 [D loss: 0.598866, acc: 67.19%] [G loss: 2.037560]\n",
      "epoch:9 step:8618 [D loss: 0.672315, acc: 53.91%] [G loss: 2.140629]\n",
      "epoch:9 step:8619 [D loss: 0.640558, acc: 62.50%] [G loss: 2.245360]\n",
      "epoch:9 step:8620 [D loss: 0.669360, acc: 59.38%] [G loss: 2.028128]\n",
      "epoch:9 step:8621 [D loss: 0.571926, acc: 69.53%] [G loss: 2.191407]\n",
      "epoch:9 step:8622 [D loss: 0.606216, acc: 66.41%] [G loss: 2.004273]\n",
      "epoch:9 step:8623 [D loss: 0.594269, acc: 67.19%] [G loss: 2.011484]\n",
      "epoch:9 step:8624 [D loss: 0.573735, acc: 67.97%] [G loss: 2.028379]\n",
      "epoch:9 step:8625 [D loss: 0.648192, acc: 60.16%] [G loss: 2.062444]\n",
      "epoch:9 step:8626 [D loss: 0.611151, acc: 64.06%] [G loss: 1.981330]\n",
      "epoch:9 step:8627 [D loss: 0.563799, acc: 74.22%] [G loss: 2.243785]\n",
      "epoch:9 step:8628 [D loss: 0.624869, acc: 67.19%] [G loss: 2.244182]\n",
      "epoch:9 step:8629 [D loss: 0.687181, acc: 53.91%] [G loss: 2.052811]\n",
      "epoch:9 step:8630 [D loss: 0.593011, acc: 66.41%] [G loss: 2.137433]\n",
      "epoch:9 step:8631 [D loss: 0.583355, acc: 68.75%] [G loss: 2.155318]\n",
      "epoch:9 step:8632 [D loss: 0.688485, acc: 57.81%] [G loss: 1.981414]\n",
      "epoch:9 step:8633 [D loss: 0.629715, acc: 64.06%] [G loss: 1.936440]\n",
      "epoch:9 step:8634 [D loss: 0.608173, acc: 67.19%] [G loss: 2.008325]\n",
      "epoch:9 step:8635 [D loss: 0.610836, acc: 64.06%] [G loss: 2.296934]\n",
      "epoch:9 step:8636 [D loss: 0.601611, acc: 66.41%] [G loss: 2.080422]\n",
      "epoch:9 step:8637 [D loss: 0.607614, acc: 66.41%] [G loss: 2.097539]\n",
      "epoch:9 step:8638 [D loss: 0.573445, acc: 68.75%] [G loss: 2.094648]\n",
      "epoch:9 step:8639 [D loss: 0.625311, acc: 65.62%] [G loss: 2.194278]\n",
      "epoch:9 step:8640 [D loss: 0.532812, acc: 73.44%] [G loss: 2.576796]\n",
      "epoch:9 step:8641 [D loss: 0.520213, acc: 76.56%] [G loss: 2.493574]\n",
      "epoch:9 step:8642 [D loss: 0.558190, acc: 67.97%] [G loss: 2.708883]\n",
      "epoch:9 step:8643 [D loss: 0.631283, acc: 66.41%] [G loss: 2.223203]\n",
      "epoch:9 step:8644 [D loss: 0.641302, acc: 65.62%] [G loss: 2.017307]\n",
      "epoch:9 step:8645 [D loss: 0.732782, acc: 52.34%] [G loss: 2.100231]\n",
      "epoch:9 step:8646 [D loss: 0.662536, acc: 61.72%] [G loss: 1.919673]\n",
      "epoch:9 step:8647 [D loss: 0.659878, acc: 63.28%] [G loss: 1.966312]\n",
      "epoch:9 step:8648 [D loss: 0.676471, acc: 60.94%] [G loss: 2.079887]\n",
      "epoch:9 step:8649 [D loss: 0.623848, acc: 66.41%] [G loss: 2.123417]\n",
      "epoch:9 step:8650 [D loss: 0.612239, acc: 66.41%] [G loss: 2.249623]\n",
      "epoch:9 step:8651 [D loss: 0.603840, acc: 64.06%] [G loss: 2.185252]\n",
      "epoch:9 step:8652 [D loss: 0.589257, acc: 68.75%] [G loss: 2.328069]\n",
      "epoch:9 step:8653 [D loss: 0.689148, acc: 57.81%] [G loss: 1.896848]\n",
      "epoch:9 step:8654 [D loss: 0.574635, acc: 68.75%] [G loss: 2.311637]\n",
      "epoch:9 step:8655 [D loss: 0.643444, acc: 61.72%] [G loss: 2.184544]\n",
      "epoch:9 step:8656 [D loss: 0.628221, acc: 63.28%] [G loss: 2.238495]\n",
      "epoch:9 step:8657 [D loss: 0.709002, acc: 57.81%] [G loss: 2.118384]\n",
      "epoch:9 step:8658 [D loss: 0.668200, acc: 60.94%] [G loss: 1.980253]\n",
      "epoch:9 step:8659 [D loss: 0.678038, acc: 61.72%] [G loss: 1.976844]\n",
      "epoch:9 step:8660 [D loss: 0.630433, acc: 60.94%] [G loss: 1.988335]\n",
      "epoch:9 step:8661 [D loss: 0.669820, acc: 61.72%] [G loss: 1.892009]\n",
      "epoch:9 step:8662 [D loss: 0.660581, acc: 64.06%] [G loss: 2.088182]\n",
      "epoch:9 step:8663 [D loss: 0.579048, acc: 71.88%] [G loss: 2.304849]\n",
      "epoch:9 step:8664 [D loss: 0.549499, acc: 71.88%] [G loss: 2.361956]\n",
      "epoch:9 step:8665 [D loss: 0.579780, acc: 68.75%] [G loss: 2.515281]\n",
      "epoch:9 step:8666 [D loss: 0.688982, acc: 57.81%] [G loss: 1.996996]\n",
      "epoch:9 step:8667 [D loss: 0.645117, acc: 61.72%] [G loss: 1.984879]\n",
      "epoch:9 step:8668 [D loss: 0.602573, acc: 62.50%] [G loss: 2.137381]\n",
      "epoch:9 step:8669 [D loss: 0.622198, acc: 62.50%] [G loss: 1.971271]\n",
      "epoch:9 step:8670 [D loss: 0.541283, acc: 76.56%] [G loss: 2.154950]\n",
      "epoch:9 step:8671 [D loss: 0.581689, acc: 70.31%] [G loss: 2.054874]\n",
      "epoch:9 step:8672 [D loss: 0.638243, acc: 63.28%] [G loss: 2.018963]\n",
      "epoch:9 step:8673 [D loss: 0.637506, acc: 65.62%] [G loss: 2.141567]\n",
      "epoch:9 step:8674 [D loss: 0.638641, acc: 60.94%] [G loss: 2.128192]\n",
      "epoch:9 step:8675 [D loss: 0.625037, acc: 66.41%] [G loss: 2.308731]\n",
      "epoch:9 step:8676 [D loss: 0.605804, acc: 67.19%] [G loss: 2.141047]\n",
      "epoch:9 step:8677 [D loss: 0.663103, acc: 60.16%] [G loss: 2.010706]\n",
      "epoch:9 step:8678 [D loss: 0.525101, acc: 75.00%] [G loss: 2.246210]\n",
      "epoch:9 step:8679 [D loss: 0.587310, acc: 68.75%] [G loss: 2.151429]\n",
      "epoch:9 step:8680 [D loss: 0.715268, acc: 55.47%] [G loss: 2.233509]\n",
      "epoch:9 step:8681 [D loss: 0.666010, acc: 62.50%] [G loss: 2.445597]\n",
      "epoch:9 step:8682 [D loss: 0.693642, acc: 59.38%] [G loss: 2.047031]\n",
      "epoch:9 step:8683 [D loss: 0.666863, acc: 64.84%] [G loss: 1.989273]\n",
      "epoch:9 step:8684 [D loss: 0.631656, acc: 68.75%] [G loss: 1.916669]\n",
      "epoch:9 step:8685 [D loss: 0.754265, acc: 52.34%] [G loss: 1.996434]\n",
      "epoch:9 step:8686 [D loss: 0.598562, acc: 71.88%] [G loss: 2.073123]\n",
      "epoch:9 step:8687 [D loss: 0.638709, acc: 60.16%] [G loss: 1.960312]\n",
      "epoch:9 step:8688 [D loss: 0.646014, acc: 63.28%] [G loss: 1.990566]\n",
      "epoch:9 step:8689 [D loss: 0.641814, acc: 60.94%] [G loss: 1.918318]\n",
      "epoch:9 step:8690 [D loss: 0.611912, acc: 64.84%] [G loss: 2.152932]\n",
      "epoch:9 step:8691 [D loss: 0.627473, acc: 60.16%] [G loss: 2.226772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8692 [D loss: 0.640359, acc: 60.94%] [G loss: 2.156455]\n",
      "epoch:9 step:8693 [D loss: 0.581948, acc: 67.97%] [G loss: 2.106569]\n",
      "epoch:9 step:8694 [D loss: 0.641756, acc: 64.06%] [G loss: 2.115653]\n",
      "epoch:9 step:8695 [D loss: 0.578896, acc: 71.09%] [G loss: 2.369378]\n",
      "epoch:9 step:8696 [D loss: 0.552799, acc: 68.75%] [G loss: 2.342968]\n",
      "epoch:9 step:8697 [D loss: 0.568431, acc: 67.97%] [G loss: 2.362660]\n",
      "epoch:9 step:8698 [D loss: 0.673881, acc: 64.06%] [G loss: 2.072888]\n",
      "epoch:9 step:8699 [D loss: 0.599199, acc: 71.09%] [G loss: 2.129818]\n",
      "epoch:9 step:8700 [D loss: 0.596356, acc: 69.53%] [G loss: 2.086424]\n",
      "epoch:9 step:8701 [D loss: 0.609596, acc: 67.19%] [G loss: 2.067159]\n",
      "epoch:9 step:8702 [D loss: 0.594576, acc: 69.53%] [G loss: 2.165585]\n",
      "epoch:9 step:8703 [D loss: 0.589876, acc: 67.19%] [G loss: 2.123633]\n",
      "epoch:9 step:8704 [D loss: 0.615475, acc: 68.75%] [G loss: 2.277048]\n",
      "epoch:9 step:8705 [D loss: 0.565419, acc: 72.66%] [G loss: 2.506998]\n",
      "epoch:9 step:8706 [D loss: 0.655181, acc: 67.19%] [G loss: 2.429603]\n",
      "epoch:9 step:8707 [D loss: 0.628109, acc: 64.84%] [G loss: 2.184972]\n",
      "epoch:9 step:8708 [D loss: 0.670298, acc: 59.38%] [G loss: 2.139851]\n",
      "epoch:9 step:8709 [D loss: 0.557064, acc: 71.88%] [G loss: 2.312165]\n",
      "epoch:9 step:8710 [D loss: 0.651955, acc: 62.50%] [G loss: 2.085396]\n",
      "epoch:9 step:8711 [D loss: 0.609210, acc: 65.62%] [G loss: 2.084123]\n",
      "epoch:9 step:8712 [D loss: 0.639498, acc: 66.41%] [G loss: 2.123750]\n",
      "epoch:9 step:8713 [D loss: 0.601939, acc: 68.75%] [G loss: 2.222040]\n",
      "epoch:9 step:8714 [D loss: 0.637067, acc: 67.19%] [G loss: 2.151745]\n",
      "epoch:9 step:8715 [D loss: 0.625562, acc: 67.19%] [G loss: 2.029687]\n",
      "epoch:9 step:8716 [D loss: 0.653127, acc: 65.62%] [G loss: 2.215816]\n",
      "epoch:9 step:8717 [D loss: 0.640352, acc: 67.97%] [G loss: 2.157529]\n",
      "epoch:9 step:8718 [D loss: 0.604510, acc: 65.62%] [G loss: 2.132010]\n",
      "epoch:9 step:8719 [D loss: 0.638521, acc: 58.59%] [G loss: 2.423289]\n",
      "epoch:9 step:8720 [D loss: 0.593875, acc: 64.06%] [G loss: 2.231465]\n",
      "epoch:9 step:8721 [D loss: 0.682983, acc: 60.16%] [G loss: 2.132756]\n",
      "epoch:9 step:8722 [D loss: 0.632973, acc: 65.62%] [G loss: 2.215174]\n",
      "epoch:9 step:8723 [D loss: 0.606447, acc: 64.84%] [G loss: 2.125753]\n",
      "epoch:9 step:8724 [D loss: 0.627588, acc: 64.06%] [G loss: 2.064129]\n",
      "epoch:9 step:8725 [D loss: 0.639548, acc: 63.28%] [G loss: 2.011635]\n",
      "epoch:9 step:8726 [D loss: 0.593592, acc: 69.53%] [G loss: 2.160177]\n",
      "epoch:9 step:8727 [D loss: 0.626475, acc: 66.41%] [G loss: 2.254278]\n",
      "epoch:9 step:8728 [D loss: 0.630319, acc: 60.94%] [G loss: 2.175917]\n",
      "epoch:9 step:8729 [D loss: 0.657832, acc: 61.72%] [G loss: 2.060108]\n",
      "epoch:9 step:8730 [D loss: 0.636556, acc: 67.97%] [G loss: 2.228616]\n",
      "epoch:9 step:8731 [D loss: 0.644607, acc: 62.50%] [G loss: 2.224425]\n",
      "epoch:9 step:8732 [D loss: 0.659718, acc: 63.28%] [G loss: 2.190717]\n",
      "epoch:9 step:8733 [D loss: 0.598921, acc: 66.41%] [G loss: 1.999884]\n",
      "epoch:9 step:8734 [D loss: 0.617505, acc: 60.16%] [G loss: 2.133124]\n",
      "epoch:9 step:8735 [D loss: 0.586863, acc: 64.84%] [G loss: 2.301956]\n",
      "epoch:9 step:8736 [D loss: 0.649614, acc: 61.72%] [G loss: 2.147490]\n",
      "epoch:9 step:8737 [D loss: 0.630040, acc: 68.75%] [G loss: 2.022345]\n",
      "epoch:9 step:8738 [D loss: 0.610399, acc: 69.53%] [G loss: 2.218275]\n",
      "epoch:9 step:8739 [D loss: 0.667090, acc: 60.16%] [G loss: 2.042809]\n",
      "epoch:9 step:8740 [D loss: 0.652649, acc: 60.16%] [G loss: 1.984382]\n",
      "epoch:9 step:8741 [D loss: 0.687546, acc: 58.59%] [G loss: 2.046555]\n",
      "epoch:9 step:8742 [D loss: 0.650491, acc: 55.47%] [G loss: 2.200902]\n",
      "epoch:9 step:8743 [D loss: 0.609009, acc: 62.50%] [G loss: 2.022503]\n",
      "epoch:9 step:8744 [D loss: 0.662279, acc: 58.59%] [G loss: 2.107653]\n",
      "epoch:9 step:8745 [D loss: 0.534655, acc: 74.22%] [G loss: 2.564793]\n",
      "epoch:9 step:8746 [D loss: 0.591345, acc: 72.66%] [G loss: 2.326946]\n",
      "epoch:9 step:8747 [D loss: 0.579616, acc: 67.19%] [G loss: 2.706388]\n",
      "epoch:9 step:8748 [D loss: 0.531768, acc: 76.56%] [G loss: 2.356526]\n",
      "epoch:9 step:8749 [D loss: 0.695730, acc: 52.34%] [G loss: 1.942041]\n",
      "epoch:9 step:8750 [D loss: 0.720482, acc: 58.59%] [G loss: 2.019773]\n",
      "epoch:9 step:8751 [D loss: 0.590519, acc: 72.66%] [G loss: 2.047715]\n",
      "epoch:9 step:8752 [D loss: 0.650381, acc: 60.94%] [G loss: 1.980221]\n",
      "epoch:9 step:8753 [D loss: 0.594948, acc: 68.75%] [G loss: 2.096156]\n",
      "epoch:9 step:8754 [D loss: 0.645274, acc: 69.53%] [G loss: 2.339100]\n",
      "epoch:9 step:8755 [D loss: 0.607699, acc: 66.41%] [G loss: 2.114590]\n",
      "epoch:9 step:8756 [D loss: 0.708154, acc: 56.25%] [G loss: 1.990061]\n",
      "epoch:9 step:8757 [D loss: 0.653179, acc: 59.38%] [G loss: 1.857392]\n",
      "epoch:9 step:8758 [D loss: 0.582163, acc: 69.53%] [G loss: 2.044420]\n",
      "epoch:9 step:8759 [D loss: 0.649936, acc: 59.38%] [G loss: 2.008047]\n",
      "epoch:9 step:8760 [D loss: 0.603051, acc: 67.19%] [G loss: 1.968749]\n",
      "epoch:9 step:8761 [D loss: 0.675627, acc: 57.03%] [G loss: 2.276967]\n",
      "epoch:9 step:8762 [D loss: 0.580771, acc: 71.88%] [G loss: 2.188521]\n",
      "epoch:9 step:8763 [D loss: 0.685300, acc: 57.81%] [G loss: 2.003432]\n",
      "epoch:9 step:8764 [D loss: 0.585083, acc: 68.75%] [G loss: 2.153660]\n",
      "epoch:9 step:8765 [D loss: 0.538479, acc: 75.78%] [G loss: 2.390029]\n",
      "epoch:9 step:8766 [D loss: 0.605611, acc: 65.62%] [G loss: 2.372019]\n",
      "epoch:9 step:8767 [D loss: 0.657605, acc: 62.50%] [G loss: 2.106648]\n",
      "epoch:9 step:8768 [D loss: 0.629299, acc: 66.41%] [G loss: 2.201801]\n",
      "epoch:9 step:8769 [D loss: 0.593461, acc: 66.41%] [G loss: 2.253773]\n",
      "epoch:9 step:8770 [D loss: 0.641357, acc: 65.62%] [G loss: 2.276914]\n",
      "epoch:9 step:8771 [D loss: 0.590392, acc: 68.75%] [G loss: 2.229463]\n",
      "epoch:9 step:8772 [D loss: 0.588037, acc: 72.66%] [G loss: 2.197098]\n",
      "epoch:9 step:8773 [D loss: 0.635307, acc: 61.72%] [G loss: 2.143972]\n",
      "epoch:9 step:8774 [D loss: 0.751871, acc: 53.12%] [G loss: 2.003307]\n",
      "epoch:9 step:8775 [D loss: 0.639443, acc: 62.50%] [G loss: 1.990822]\n",
      "epoch:9 step:8776 [D loss: 0.640115, acc: 64.84%] [G loss: 2.105937]\n",
      "epoch:9 step:8777 [D loss: 0.714189, acc: 59.38%] [G loss: 2.259139]\n",
      "epoch:9 step:8778 [D loss: 0.583560, acc: 74.22%] [G loss: 2.351451]\n",
      "epoch:9 step:8779 [D loss: 0.545071, acc: 73.44%] [G loss: 2.381240]\n",
      "epoch:9 step:8780 [D loss: 0.550405, acc: 73.44%] [G loss: 2.458976]\n",
      "epoch:9 step:8781 [D loss: 0.666682, acc: 59.38%] [G loss: 2.026865]\n",
      "epoch:9 step:8782 [D loss: 0.651822, acc: 60.94%] [G loss: 1.962122]\n",
      "epoch:9 step:8783 [D loss: 0.594347, acc: 71.88%] [G loss: 2.100842]\n",
      "epoch:9 step:8784 [D loss: 0.656093, acc: 59.38%] [G loss: 2.065859]\n",
      "epoch:9 step:8785 [D loss: 0.661817, acc: 66.41%] [G loss: 1.901478]\n",
      "epoch:9 step:8786 [D loss: 0.597596, acc: 68.75%] [G loss: 2.229048]\n",
      "epoch:9 step:8787 [D loss: 0.622320, acc: 64.06%] [G loss: 2.193934]\n",
      "epoch:9 step:8788 [D loss: 0.634166, acc: 62.50%] [G loss: 1.974373]\n",
      "epoch:9 step:8789 [D loss: 0.671949, acc: 63.28%] [G loss: 2.019293]\n",
      "epoch:9 step:8790 [D loss: 0.621624, acc: 64.84%] [G loss: 2.135664]\n",
      "epoch:9 step:8791 [D loss: 0.586512, acc: 68.75%] [G loss: 2.317806]\n",
      "epoch:9 step:8792 [D loss: 0.551176, acc: 71.09%] [G loss: 2.315606]\n",
      "epoch:9 step:8793 [D loss: 0.581875, acc: 68.75%] [G loss: 2.178244]\n",
      "epoch:9 step:8794 [D loss: 0.628674, acc: 60.94%] [G loss: 2.149298]\n",
      "epoch:9 step:8795 [D loss: 0.612297, acc: 71.09%] [G loss: 2.177517]\n",
      "epoch:9 step:8796 [D loss: 0.661478, acc: 60.16%] [G loss: 2.266756]\n",
      "epoch:9 step:8797 [D loss: 0.595141, acc: 69.53%] [G loss: 2.266049]\n",
      "epoch:9 step:8798 [D loss: 0.594509, acc: 69.53%] [G loss: 2.229770]\n",
      "epoch:9 step:8799 [D loss: 0.661229, acc: 57.03%] [G loss: 2.171205]\n",
      "epoch:9 step:8800 [D loss: 0.578356, acc: 64.84%] [G loss: 2.253015]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.585249\n",
      "FID: 25.441319\n",
      "0 = 12.81412093691821\n",
      "1 = 0.0803018561408789\n",
      "2 = 0.9039000272750854\n",
      "3 = 0.9092000126838684\n",
      "4 = 0.8985999822616577\n",
      "5 = 0.8996635675430298\n",
      "6 = 0.9092000126838684\n",
      "7 = 7.49053883771894\n",
      "8 = 0.09788121061729231\n",
      "9 = 0.7789999842643738\n",
      "10 = 0.7868000268936157\n",
      "11 = 0.7712000012397766\n",
      "12 = 0.774714469909668\n",
      "13 = 0.7868000268936157\n",
      "14 = 6.585272312164307\n",
      "15 = 9.395598411560059\n",
      "16 = 0.14618416130542755\n",
      "17 = 6.585248947143555\n",
      "18 = 25.44131851196289\n",
      "epoch:9 step:8801 [D loss: 0.682213, acc: 56.25%] [G loss: 1.997102]\n",
      "epoch:9 step:8802 [D loss: 0.675302, acc: 57.03%] [G loss: 2.226855]\n",
      "epoch:9 step:8803 [D loss: 0.592000, acc: 73.44%] [G loss: 2.126739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8804 [D loss: 0.559289, acc: 67.97%] [G loss: 2.224191]\n",
      "epoch:9 step:8805 [D loss: 0.658193, acc: 58.59%] [G loss: 2.117226]\n",
      "epoch:9 step:8806 [D loss: 0.721123, acc: 57.03%] [G loss: 1.954470]\n",
      "epoch:9 step:8807 [D loss: 0.569098, acc: 72.66%] [G loss: 2.043757]\n",
      "epoch:9 step:8808 [D loss: 0.632279, acc: 66.41%] [G loss: 1.878195]\n",
      "epoch:9 step:8809 [D loss: 0.714559, acc: 60.16%] [G loss: 2.025896]\n",
      "epoch:9 step:8810 [D loss: 0.669098, acc: 56.25%] [G loss: 1.767009]\n",
      "epoch:9 step:8811 [D loss: 0.627794, acc: 65.62%] [G loss: 2.196028]\n",
      "epoch:9 step:8812 [D loss: 0.661384, acc: 64.84%] [G loss: 2.080688]\n",
      "epoch:9 step:8813 [D loss: 0.602379, acc: 71.09%] [G loss: 1.958629]\n",
      "epoch:9 step:8814 [D loss: 0.647397, acc: 60.94%] [G loss: 2.097818]\n",
      "epoch:9 step:8815 [D loss: 0.619239, acc: 63.28%] [G loss: 2.059299]\n",
      "epoch:9 step:8816 [D loss: 0.665671, acc: 59.38%] [G loss: 1.869253]\n",
      "epoch:9 step:8817 [D loss: 0.589133, acc: 67.97%] [G loss: 2.191621]\n",
      "epoch:9 step:8818 [D loss: 0.649754, acc: 64.84%] [G loss: 2.079582]\n",
      "epoch:9 step:8819 [D loss: 0.645093, acc: 62.50%] [G loss: 2.198973]\n",
      "epoch:9 step:8820 [D loss: 0.687091, acc: 57.03%] [G loss: 1.931503]\n",
      "epoch:9 step:8821 [D loss: 0.587014, acc: 67.19%] [G loss: 1.925908]\n",
      "epoch:9 step:8822 [D loss: 0.629953, acc: 63.28%] [G loss: 1.912851]\n",
      "epoch:9 step:8823 [D loss: 0.620504, acc: 60.94%] [G loss: 2.235679]\n",
      "epoch:9 step:8824 [D loss: 0.663953, acc: 60.16%] [G loss: 1.952900]\n",
      "epoch:9 step:8825 [D loss: 0.589604, acc: 71.88%] [G loss: 2.157417]\n",
      "epoch:9 step:8826 [D loss: 0.623778, acc: 64.84%] [G loss: 2.183952]\n",
      "epoch:9 step:8827 [D loss: 0.659160, acc: 64.84%] [G loss: 2.103641]\n",
      "epoch:9 step:8828 [D loss: 0.607069, acc: 66.41%] [G loss: 2.150349]\n",
      "epoch:9 step:8829 [D loss: 0.621374, acc: 66.41%] [G loss: 2.044282]\n",
      "epoch:9 step:8830 [D loss: 0.686579, acc: 57.03%] [G loss: 2.109861]\n",
      "epoch:9 step:8831 [D loss: 0.628368, acc: 64.84%] [G loss: 2.124421]\n",
      "epoch:9 step:8832 [D loss: 0.625318, acc: 63.28%] [G loss: 2.076254]\n",
      "epoch:9 step:8833 [D loss: 0.609307, acc: 65.62%] [G loss: 2.147323]\n",
      "epoch:9 step:8834 [D loss: 0.575714, acc: 67.97%] [G loss: 2.249828]\n",
      "epoch:9 step:8835 [D loss: 0.568307, acc: 70.31%] [G loss: 2.211463]\n",
      "epoch:9 step:8836 [D loss: 0.640802, acc: 62.50%] [G loss: 2.183474]\n",
      "epoch:9 step:8837 [D loss: 0.644620, acc: 59.38%] [G loss: 2.203778]\n",
      "epoch:9 step:8838 [D loss: 0.685274, acc: 64.06%] [G loss: 2.225599]\n",
      "epoch:9 step:8839 [D loss: 0.586964, acc: 70.31%] [G loss: 2.214607]\n",
      "epoch:9 step:8840 [D loss: 0.624365, acc: 64.06%] [G loss: 2.054748]\n",
      "epoch:9 step:8841 [D loss: 0.645371, acc: 63.28%] [G loss: 2.028722]\n",
      "epoch:9 step:8842 [D loss: 0.649337, acc: 60.94%] [G loss: 2.227613]\n",
      "epoch:9 step:8843 [D loss: 0.640119, acc: 64.06%] [G loss: 2.170785]\n",
      "epoch:9 step:8844 [D loss: 0.625941, acc: 67.19%] [G loss: 1.975026]\n",
      "epoch:9 step:8845 [D loss: 0.628982, acc: 58.59%] [G loss: 2.048725]\n",
      "epoch:9 step:8846 [D loss: 0.568608, acc: 68.75%] [G loss: 2.151303]\n",
      "epoch:9 step:8847 [D loss: 0.632825, acc: 65.62%] [G loss: 2.314820]\n",
      "epoch:9 step:8848 [D loss: 0.691740, acc: 57.81%] [G loss: 2.243120]\n",
      "epoch:9 step:8849 [D loss: 0.565781, acc: 70.31%] [G loss: 2.255254]\n",
      "epoch:9 step:8850 [D loss: 0.722481, acc: 58.59%] [G loss: 2.120417]\n",
      "epoch:9 step:8851 [D loss: 0.705336, acc: 56.25%] [G loss: 2.130108]\n",
      "epoch:9 step:8852 [D loss: 0.626262, acc: 64.84%] [G loss: 2.314705]\n",
      "epoch:9 step:8853 [D loss: 0.664928, acc: 62.50%] [G loss: 1.951705]\n",
      "epoch:9 step:8854 [D loss: 0.626727, acc: 64.84%] [G loss: 2.040019]\n",
      "epoch:9 step:8855 [D loss: 0.627064, acc: 62.50%] [G loss: 1.917460]\n",
      "epoch:9 step:8856 [D loss: 0.629524, acc: 63.28%] [G loss: 1.988426]\n",
      "epoch:9 step:8857 [D loss: 0.620808, acc: 65.62%] [G loss: 2.074711]\n",
      "epoch:9 step:8858 [D loss: 0.606890, acc: 67.19%] [G loss: 2.252666]\n",
      "epoch:9 step:8859 [D loss: 0.595259, acc: 62.50%] [G loss: 2.214115]\n",
      "epoch:9 step:8860 [D loss: 0.581733, acc: 70.31%] [G loss: 2.437500]\n",
      "epoch:9 step:8861 [D loss: 0.526800, acc: 72.66%] [G loss: 2.599911]\n",
      "epoch:9 step:8862 [D loss: 0.531938, acc: 73.44%] [G loss: 2.299933]\n",
      "epoch:9 step:8863 [D loss: 0.504350, acc: 76.56%] [G loss: 2.712780]\n",
      "epoch:9 step:8864 [D loss: 0.659927, acc: 63.28%] [G loss: 2.455302]\n",
      "epoch:9 step:8865 [D loss: 0.699627, acc: 54.69%] [G loss: 2.086691]\n",
      "epoch:9 step:8866 [D loss: 0.678220, acc: 60.94%] [G loss: 2.176872]\n",
      "epoch:9 step:8867 [D loss: 0.619092, acc: 66.41%] [G loss: 2.233146]\n",
      "epoch:9 step:8868 [D loss: 0.589176, acc: 66.41%] [G loss: 2.133729]\n",
      "epoch:9 step:8869 [D loss: 0.641137, acc: 64.06%] [G loss: 2.182428]\n",
      "epoch:9 step:8870 [D loss: 0.752933, acc: 53.12%] [G loss: 1.846657]\n",
      "epoch:9 step:8871 [D loss: 0.734201, acc: 54.69%] [G loss: 1.875754]\n",
      "epoch:9 step:8872 [D loss: 0.602652, acc: 67.19%] [G loss: 2.045426]\n",
      "epoch:9 step:8873 [D loss: 0.675049, acc: 55.47%] [G loss: 2.119610]\n",
      "epoch:9 step:8874 [D loss: 0.702627, acc: 56.25%] [G loss: 1.897312]\n",
      "epoch:9 step:8875 [D loss: 0.661037, acc: 60.94%] [G loss: 2.007073]\n",
      "epoch:9 step:8876 [D loss: 0.602971, acc: 66.41%] [G loss: 2.074626]\n",
      "epoch:9 step:8877 [D loss: 0.666953, acc: 57.03%] [G loss: 2.035848]\n",
      "epoch:9 step:8878 [D loss: 0.637143, acc: 62.50%] [G loss: 2.124017]\n",
      "epoch:9 step:8879 [D loss: 0.632233, acc: 67.97%] [G loss: 1.983192]\n",
      "epoch:9 step:8880 [D loss: 0.605735, acc: 69.53%] [G loss: 2.159244]\n",
      "epoch:9 step:8881 [D loss: 0.645657, acc: 62.50%] [G loss: 1.937076]\n",
      "epoch:9 step:8882 [D loss: 0.598791, acc: 66.41%] [G loss: 2.010394]\n",
      "epoch:9 step:8883 [D loss: 0.647448, acc: 59.38%] [G loss: 2.130298]\n",
      "epoch:9 step:8884 [D loss: 0.568924, acc: 72.66%] [G loss: 2.263270]\n",
      "epoch:9 step:8885 [D loss: 0.643430, acc: 62.50%] [G loss: 2.054465]\n",
      "epoch:9 step:8886 [D loss: 0.590183, acc: 71.88%] [G loss: 2.362745]\n",
      "epoch:9 step:8887 [D loss: 0.597674, acc: 64.06%] [G loss: 2.089966]\n",
      "epoch:9 step:8888 [D loss: 0.602200, acc: 69.53%] [G loss: 2.078015]\n",
      "epoch:9 step:8889 [D loss: 0.631006, acc: 64.06%] [G loss: 2.039497]\n",
      "epoch:9 step:8890 [D loss: 0.638429, acc: 67.19%] [G loss: 2.066376]\n",
      "epoch:9 step:8891 [D loss: 0.643446, acc: 64.84%] [G loss: 1.871594]\n",
      "epoch:9 step:8892 [D loss: 0.683897, acc: 58.59%] [G loss: 1.894277]\n",
      "epoch:9 step:8893 [D loss: 0.637205, acc: 62.50%] [G loss: 1.890458]\n",
      "epoch:9 step:8894 [D loss: 0.666546, acc: 59.38%] [G loss: 2.070401]\n",
      "epoch:9 step:8895 [D loss: 0.635192, acc: 65.62%] [G loss: 1.986851]\n",
      "epoch:9 step:8896 [D loss: 0.635755, acc: 65.62%] [G loss: 1.881989]\n",
      "epoch:9 step:8897 [D loss: 0.680098, acc: 54.69%] [G loss: 1.870229]\n",
      "epoch:9 step:8898 [D loss: 0.655062, acc: 64.84%] [G loss: 2.060312]\n",
      "epoch:9 step:8899 [D loss: 0.600641, acc: 69.53%] [G loss: 2.056002]\n",
      "epoch:9 step:8900 [D loss: 0.631175, acc: 60.16%] [G loss: 1.994028]\n",
      "epoch:9 step:8901 [D loss: 0.667964, acc: 57.03%] [G loss: 1.965457]\n",
      "epoch:9 step:8902 [D loss: 0.586482, acc: 66.41%] [G loss: 2.095880]\n",
      "epoch:9 step:8903 [D loss: 0.600055, acc: 67.97%] [G loss: 2.217749]\n",
      "epoch:9 step:8904 [D loss: 0.605832, acc: 60.94%] [G loss: 2.433933]\n",
      "epoch:9 step:8905 [D loss: 0.568035, acc: 72.66%] [G loss: 2.379212]\n",
      "epoch:9 step:8906 [D loss: 0.598139, acc: 66.41%] [G loss: 2.023854]\n",
      "epoch:9 step:8907 [D loss: 0.620966, acc: 70.31%] [G loss: 2.272797]\n",
      "epoch:9 step:8908 [D loss: 0.670134, acc: 62.50%] [G loss: 2.200755]\n",
      "epoch:9 step:8909 [D loss: 0.660947, acc: 63.28%] [G loss: 2.105055]\n",
      "epoch:9 step:8910 [D loss: 0.644916, acc: 67.19%] [G loss: 2.036034]\n",
      "epoch:9 step:8911 [D loss: 0.639410, acc: 61.72%] [G loss: 1.988252]\n",
      "epoch:9 step:8912 [D loss: 0.606484, acc: 67.97%] [G loss: 2.042690]\n",
      "epoch:9 step:8913 [D loss: 0.634285, acc: 64.84%] [G loss: 2.113968]\n",
      "epoch:9 step:8914 [D loss: 0.618906, acc: 69.53%] [G loss: 2.318499]\n",
      "epoch:9 step:8915 [D loss: 0.693527, acc: 61.72%] [G loss: 1.850767]\n",
      "epoch:9 step:8916 [D loss: 0.738152, acc: 56.25%] [G loss: 2.075453]\n",
      "epoch:9 step:8917 [D loss: 0.619314, acc: 64.06%] [G loss: 2.089465]\n",
      "epoch:9 step:8918 [D loss: 0.630970, acc: 65.62%] [G loss: 2.247809]\n",
      "epoch:9 step:8919 [D loss: 0.641475, acc: 63.28%] [G loss: 1.902434]\n",
      "epoch:9 step:8920 [D loss: 0.609859, acc: 66.41%] [G loss: 1.953312]\n",
      "epoch:9 step:8921 [D loss: 0.640399, acc: 65.62%] [G loss: 2.458491]\n",
      "epoch:9 step:8922 [D loss: 0.606946, acc: 67.97%] [G loss: 2.287570]\n",
      "epoch:9 step:8923 [D loss: 0.672865, acc: 57.03%] [G loss: 2.025848]\n",
      "epoch:9 step:8924 [D loss: 0.655975, acc: 64.06%] [G loss: 2.174526]\n",
      "epoch:9 step:8925 [D loss: 0.615151, acc: 68.75%] [G loss: 2.020994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8926 [D loss: 0.644912, acc: 65.62%] [G loss: 2.058056]\n",
      "epoch:9 step:8927 [D loss: 0.576978, acc: 71.09%] [G loss: 2.159406]\n",
      "epoch:9 step:8928 [D loss: 0.604592, acc: 66.41%] [G loss: 2.299989]\n",
      "epoch:9 step:8929 [D loss: 0.611121, acc: 68.75%] [G loss: 2.051093]\n",
      "epoch:9 step:8930 [D loss: 0.596797, acc: 67.19%] [G loss: 2.254306]\n",
      "epoch:9 step:8931 [D loss: 0.568562, acc: 70.31%] [G loss: 2.392468]\n",
      "epoch:9 step:8932 [D loss: 0.607916, acc: 66.41%] [G loss: 2.264518]\n",
      "epoch:9 step:8933 [D loss: 0.685494, acc: 57.03%] [G loss: 1.801401]\n",
      "epoch:9 step:8934 [D loss: 0.733068, acc: 56.25%] [G loss: 1.810870]\n",
      "epoch:9 step:8935 [D loss: 0.680535, acc: 58.59%] [G loss: 1.828792]\n",
      "epoch:9 step:8936 [D loss: 0.640401, acc: 63.28%] [G loss: 2.106706]\n",
      "epoch:9 step:8937 [D loss: 0.609356, acc: 64.84%] [G loss: 2.381149]\n",
      "epoch:9 step:8938 [D loss: 0.619851, acc: 61.72%] [G loss: 2.236351]\n",
      "epoch:9 step:8939 [D loss: 0.679126, acc: 59.38%] [G loss: 2.015493]\n",
      "epoch:9 step:8940 [D loss: 0.684164, acc: 60.94%] [G loss: 1.944068]\n",
      "epoch:9 step:8941 [D loss: 0.595745, acc: 63.28%] [G loss: 2.086370]\n",
      "epoch:9 step:8942 [D loss: 0.595350, acc: 69.53%] [G loss: 1.921394]\n",
      "epoch:9 step:8943 [D loss: 0.685889, acc: 59.38%] [G loss: 1.912353]\n",
      "epoch:9 step:8944 [D loss: 0.693856, acc: 57.03%] [G loss: 1.899379]\n",
      "epoch:9 step:8945 [D loss: 0.693856, acc: 57.81%] [G loss: 1.924631]\n",
      "epoch:9 step:8946 [D loss: 0.606616, acc: 68.75%] [G loss: 1.933157]\n",
      "epoch:9 step:8947 [D loss: 0.622987, acc: 65.62%] [G loss: 1.861690]\n",
      "epoch:9 step:8948 [D loss: 0.618631, acc: 71.88%] [G loss: 2.174338]\n",
      "epoch:9 step:8949 [D loss: 0.552922, acc: 72.66%] [G loss: 2.161200]\n",
      "epoch:9 step:8950 [D loss: 0.613604, acc: 64.84%] [G loss: 2.184123]\n",
      "epoch:9 step:8951 [D loss: 0.629960, acc: 60.16%] [G loss: 2.129857]\n",
      "epoch:9 step:8952 [D loss: 0.576712, acc: 74.22%] [G loss: 2.129570]\n",
      "epoch:9 step:8953 [D loss: 0.589016, acc: 71.09%] [G loss: 2.162932]\n",
      "epoch:9 step:8954 [D loss: 0.605374, acc: 71.09%] [G loss: 2.249451]\n",
      "epoch:9 step:8955 [D loss: 0.603301, acc: 67.19%] [G loss: 2.015604]\n",
      "epoch:9 step:8956 [D loss: 0.589495, acc: 65.62%] [G loss: 2.215681]\n",
      "epoch:9 step:8957 [D loss: 0.578051, acc: 70.31%] [G loss: 2.040019]\n",
      "epoch:9 step:8958 [D loss: 0.610039, acc: 66.41%] [G loss: 1.932658]\n",
      "epoch:9 step:8959 [D loss: 0.632483, acc: 64.84%] [G loss: 2.018630]\n",
      "epoch:9 step:8960 [D loss: 0.671414, acc: 60.16%] [G loss: 2.027815]\n",
      "epoch:9 step:8961 [D loss: 0.635494, acc: 64.06%] [G loss: 2.156410]\n",
      "epoch:9 step:8962 [D loss: 0.697539, acc: 54.69%] [G loss: 1.947209]\n",
      "epoch:9 step:8963 [D loss: 0.604237, acc: 62.50%] [G loss: 1.944717]\n",
      "epoch:9 step:8964 [D loss: 0.608342, acc: 65.62%] [G loss: 1.977560]\n",
      "epoch:9 step:8965 [D loss: 0.613417, acc: 64.84%] [G loss: 2.026222]\n",
      "epoch:9 step:8966 [D loss: 0.662147, acc: 62.50%] [G loss: 2.166717]\n",
      "epoch:9 step:8967 [D loss: 0.617930, acc: 71.09%] [G loss: 2.016637]\n",
      "epoch:9 step:8968 [D loss: 0.697130, acc: 53.12%] [G loss: 2.026964]\n",
      "epoch:9 step:8969 [D loss: 0.649790, acc: 60.16%] [G loss: 1.992268]\n",
      "epoch:9 step:8970 [D loss: 0.689310, acc: 54.69%] [G loss: 1.935350]\n",
      "epoch:9 step:8971 [D loss: 0.693718, acc: 54.69%] [G loss: 1.970423]\n",
      "epoch:9 step:8972 [D loss: 0.670416, acc: 60.16%] [G loss: 1.892026]\n",
      "epoch:9 step:8973 [D loss: 0.604701, acc: 65.62%] [G loss: 2.047680]\n",
      "epoch:9 step:8974 [D loss: 0.634890, acc: 68.75%] [G loss: 1.941943]\n",
      "epoch:9 step:8975 [D loss: 0.699557, acc: 54.69%] [G loss: 1.973494]\n",
      "epoch:9 step:8976 [D loss: 0.644142, acc: 58.59%] [G loss: 2.140922]\n",
      "epoch:9 step:8977 [D loss: 0.685951, acc: 60.94%] [G loss: 1.954408]\n",
      "epoch:9 step:8978 [D loss: 0.600779, acc: 67.97%] [G loss: 2.003519]\n",
      "epoch:9 step:8979 [D loss: 0.619396, acc: 62.50%] [G loss: 2.125348]\n",
      "epoch:9 step:8980 [D loss: 0.632479, acc: 62.50%] [G loss: 2.077373]\n",
      "epoch:9 step:8981 [D loss: 0.639853, acc: 66.41%] [G loss: 2.072948]\n",
      "epoch:9 step:8982 [D loss: 0.590750, acc: 67.97%] [G loss: 2.115021]\n",
      "epoch:9 step:8983 [D loss: 0.592974, acc: 65.62%] [G loss: 2.255141]\n",
      "epoch:9 step:8984 [D loss: 0.589570, acc: 64.06%] [G loss: 2.193110]\n",
      "epoch:9 step:8985 [D loss: 0.578492, acc: 71.09%] [G loss: 2.061913]\n",
      "epoch:9 step:8986 [D loss: 0.632731, acc: 68.75%] [G loss: 2.036927]\n",
      "epoch:9 step:8987 [D loss: 0.602331, acc: 70.31%] [G loss: 2.181965]\n",
      "epoch:9 step:8988 [D loss: 0.582413, acc: 71.88%] [G loss: 2.191041]\n",
      "epoch:9 step:8989 [D loss: 0.546463, acc: 75.78%] [G loss: 2.404858]\n",
      "epoch:9 step:8990 [D loss: 0.615576, acc: 67.97%] [G loss: 2.199376]\n",
      "epoch:9 step:8991 [D loss: 0.562344, acc: 71.88%] [G loss: 2.139731]\n",
      "epoch:9 step:8992 [D loss: 0.679589, acc: 57.81%] [G loss: 1.990933]\n",
      "epoch:9 step:8993 [D loss: 0.639405, acc: 66.41%] [G loss: 2.061863]\n",
      "epoch:9 step:8994 [D loss: 0.571649, acc: 73.44%] [G loss: 2.191563]\n",
      "epoch:9 step:8995 [D loss: 0.712304, acc: 57.81%] [G loss: 1.935042]\n",
      "epoch:9 step:8996 [D loss: 0.605347, acc: 67.19%] [G loss: 2.036315]\n",
      "epoch:9 step:8997 [D loss: 0.622237, acc: 65.62%] [G loss: 2.277266]\n",
      "epoch:9 step:8998 [D loss: 0.644778, acc: 60.16%] [G loss: 2.030453]\n",
      "epoch:9 step:8999 [D loss: 0.681140, acc: 57.03%] [G loss: 2.014377]\n",
      "epoch:9 step:9000 [D loss: 0.594887, acc: 66.41%] [G loss: 2.076652]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.552497\n",
      "FID: 26.974154\n",
      "0 = 12.887181094646452\n",
      "1 = 0.08522221521341339\n",
      "2 = 0.9036999940872192\n",
      "3 = 0.9218000173568726\n",
      "4 = 0.8855999708175659\n",
      "5 = 0.8895965814590454\n",
      "6 = 0.9218000173568726\n",
      "7 = 7.578961991369701\n",
      "8 = 0.10152199744523842\n",
      "9 = 0.7720000147819519\n",
      "10 = 0.7793999910354614\n",
      "11 = 0.7645999789237976\n",
      "12 = 0.7680330872535706\n",
      "13 = 0.7793999910354614\n",
      "14 = 6.552522659301758\n",
      "15 = 9.370891571044922\n",
      "16 = 0.15175800025463104\n",
      "17 = 6.552496910095215\n",
      "18 = 26.974153518676758\n",
      "epoch:9 step:9001 [D loss: 0.577659, acc: 71.88%] [G loss: 2.058070]\n",
      "epoch:9 step:9002 [D loss: 0.690655, acc: 61.72%] [G loss: 2.029813]\n",
      "epoch:9 step:9003 [D loss: 0.639744, acc: 62.50%] [G loss: 2.185713]\n",
      "epoch:9 step:9004 [D loss: 0.645500, acc: 58.59%] [G loss: 2.225335]\n",
      "epoch:9 step:9005 [D loss: 0.599105, acc: 71.09%] [G loss: 1.946179]\n",
      "epoch:9 step:9006 [D loss: 0.666991, acc: 62.50%] [G loss: 2.129173]\n",
      "epoch:9 step:9007 [D loss: 0.609157, acc: 67.97%] [G loss: 2.182135]\n",
      "epoch:9 step:9008 [D loss: 0.595834, acc: 68.75%] [G loss: 2.306377]\n",
      "epoch:9 step:9009 [D loss: 0.703381, acc: 53.91%] [G loss: 1.967235]\n",
      "epoch:9 step:9010 [D loss: 0.607360, acc: 67.97%] [G loss: 2.001256]\n",
      "epoch:9 step:9011 [D loss: 0.611805, acc: 66.41%] [G loss: 2.056304]\n",
      "epoch:9 step:9012 [D loss: 0.654638, acc: 64.06%] [G loss: 2.050407]\n",
      "epoch:9 step:9013 [D loss: 0.658509, acc: 61.72%] [G loss: 2.037706]\n",
      "epoch:9 step:9014 [D loss: 0.566016, acc: 70.31%] [G loss: 2.087297]\n",
      "epoch:9 step:9015 [D loss: 0.604580, acc: 68.75%] [G loss: 2.206222]\n",
      "epoch:9 step:9016 [D loss: 0.604199, acc: 67.97%] [G loss: 1.986302]\n",
      "epoch:9 step:9017 [D loss: 0.611461, acc: 64.84%] [G loss: 1.963650]\n",
      "epoch:9 step:9018 [D loss: 0.652436, acc: 67.97%] [G loss: 2.118155]\n",
      "epoch:9 step:9019 [D loss: 0.704235, acc: 62.50%] [G loss: 2.071252]\n",
      "epoch:9 step:9020 [D loss: 0.611452, acc: 64.84%] [G loss: 2.211149]\n",
      "epoch:9 step:9021 [D loss: 0.630632, acc: 61.72%] [G loss: 2.182472]\n",
      "epoch:9 step:9022 [D loss: 0.571147, acc: 75.78%] [G loss: 2.315003]\n",
      "epoch:9 step:9023 [D loss: 0.661824, acc: 60.16%] [G loss: 2.191203]\n",
      "epoch:9 step:9024 [D loss: 0.609767, acc: 69.53%] [G loss: 2.342691]\n",
      "epoch:9 step:9025 [D loss: 0.604242, acc: 62.50%] [G loss: 2.182396]\n",
      "epoch:9 step:9026 [D loss: 0.617787, acc: 66.41%] [G loss: 2.088438]\n",
      "epoch:9 step:9027 [D loss: 0.626699, acc: 65.62%] [G loss: 2.006053]\n",
      "epoch:9 step:9028 [D loss: 0.635591, acc: 67.97%] [G loss: 2.124840]\n",
      "epoch:9 step:9029 [D loss: 0.650170, acc: 60.94%] [G loss: 1.976541]\n",
      "epoch:9 step:9030 [D loss: 0.634321, acc: 61.72%] [G loss: 2.155043]\n",
      "epoch:9 step:9031 [D loss: 0.617939, acc: 63.28%] [G loss: 2.140670]\n",
      "epoch:9 step:9032 [D loss: 0.652622, acc: 57.03%] [G loss: 1.998641]\n",
      "epoch:9 step:9033 [D loss: 0.613845, acc: 63.28%] [G loss: 2.005288]\n",
      "epoch:9 step:9034 [D loss: 0.618513, acc: 69.53%] [G loss: 2.182593]\n",
      "epoch:9 step:9035 [D loss: 0.675362, acc: 58.59%] [G loss: 2.118046]\n",
      "epoch:9 step:9036 [D loss: 0.645100, acc: 63.28%] [G loss: 2.184448]\n",
      "epoch:9 step:9037 [D loss: 0.660217, acc: 60.94%] [G loss: 1.989554]\n",
      "epoch:9 step:9038 [D loss: 0.633598, acc: 65.62%] [G loss: 2.229358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9039 [D loss: 0.657048, acc: 58.59%] [G loss: 1.955200]\n",
      "epoch:9 step:9040 [D loss: 0.608273, acc: 67.97%] [G loss: 2.250764]\n",
      "epoch:9 step:9041 [D loss: 0.582723, acc: 64.06%] [G loss: 2.203248]\n",
      "epoch:9 step:9042 [D loss: 0.651400, acc: 61.72%] [G loss: 2.052802]\n",
      "epoch:9 step:9043 [D loss: 0.676249, acc: 57.81%] [G loss: 1.902671]\n",
      "epoch:9 step:9044 [D loss: 0.594325, acc: 67.97%] [G loss: 2.006121]\n",
      "epoch:9 step:9045 [D loss: 0.630089, acc: 61.72%] [G loss: 2.102779]\n",
      "epoch:9 step:9046 [D loss: 0.645687, acc: 60.94%] [G loss: 2.288370]\n",
      "epoch:9 step:9047 [D loss: 0.646269, acc: 66.41%] [G loss: 1.915841]\n",
      "epoch:9 step:9048 [D loss: 0.659638, acc: 56.25%] [G loss: 1.921652]\n",
      "epoch:9 step:9049 [D loss: 0.677679, acc: 56.25%] [G loss: 1.979677]\n",
      "epoch:9 step:9050 [D loss: 0.592445, acc: 67.97%] [G loss: 2.138834]\n",
      "epoch:9 step:9051 [D loss: 0.705630, acc: 61.72%] [G loss: 2.010347]\n",
      "epoch:9 step:9052 [D loss: 0.636324, acc: 67.97%] [G loss: 1.855888]\n",
      "epoch:9 step:9053 [D loss: 0.609770, acc: 64.06%] [G loss: 2.255817]\n",
      "epoch:9 step:9054 [D loss: 0.601135, acc: 65.62%] [G loss: 1.998823]\n",
      "epoch:9 step:9055 [D loss: 0.652631, acc: 60.16%] [G loss: 2.045714]\n",
      "epoch:9 step:9056 [D loss: 0.641555, acc: 67.97%] [G loss: 2.024221]\n",
      "epoch:9 step:9057 [D loss: 0.643335, acc: 65.62%] [G loss: 2.163396]\n",
      "epoch:9 step:9058 [D loss: 0.663527, acc: 62.50%] [G loss: 1.977274]\n",
      "epoch:9 step:9059 [D loss: 0.637259, acc: 63.28%] [G loss: 1.936042]\n",
      "epoch:9 step:9060 [D loss: 0.668851, acc: 57.03%] [G loss: 2.171480]\n",
      "epoch:9 step:9061 [D loss: 0.629197, acc: 67.97%] [G loss: 1.875142]\n",
      "epoch:9 step:9062 [D loss: 0.610560, acc: 67.19%] [G loss: 2.207845]\n",
      "epoch:9 step:9063 [D loss: 0.576591, acc: 67.19%] [G loss: 1.996779]\n",
      "epoch:9 step:9064 [D loss: 0.579657, acc: 71.09%] [G loss: 2.175616]\n",
      "epoch:9 step:9065 [D loss: 0.606055, acc: 63.28%] [G loss: 2.287085]\n",
      "epoch:9 step:9066 [D loss: 0.623506, acc: 64.84%] [G loss: 2.154097]\n",
      "epoch:9 step:9067 [D loss: 0.582048, acc: 71.09%] [G loss: 2.118038]\n",
      "epoch:9 step:9068 [D loss: 0.614742, acc: 67.97%] [G loss: 2.303669]\n",
      "epoch:9 step:9069 [D loss: 0.640709, acc: 62.50%] [G loss: 1.980214]\n",
      "epoch:9 step:9070 [D loss: 0.598230, acc: 62.50%] [G loss: 2.214366]\n",
      "epoch:9 step:9071 [D loss: 0.619192, acc: 64.06%] [G loss: 2.115673]\n",
      "epoch:9 step:9072 [D loss: 0.584903, acc: 65.62%] [G loss: 2.312240]\n",
      "epoch:9 step:9073 [D loss: 0.613648, acc: 67.19%] [G loss: 2.203672]\n",
      "epoch:9 step:9074 [D loss: 0.615560, acc: 63.28%] [G loss: 2.015540]\n",
      "epoch:9 step:9075 [D loss: 0.615925, acc: 65.62%] [G loss: 2.154598]\n",
      "epoch:9 step:9076 [D loss: 0.674004, acc: 60.94%] [G loss: 2.067308]\n",
      "epoch:9 step:9077 [D loss: 0.637532, acc: 60.94%] [G loss: 2.219745]\n",
      "epoch:9 step:9078 [D loss: 0.638287, acc: 62.50%] [G loss: 2.150880]\n",
      "epoch:9 step:9079 [D loss: 0.677691, acc: 59.38%] [G loss: 2.215787]\n",
      "epoch:9 step:9080 [D loss: 0.557874, acc: 73.44%] [G loss: 2.158743]\n",
      "epoch:9 step:9081 [D loss: 0.549038, acc: 71.09%] [G loss: 2.655364]\n",
      "epoch:9 step:9082 [D loss: 0.583661, acc: 71.09%] [G loss: 2.465882]\n",
      "epoch:9 step:9083 [D loss: 0.607611, acc: 67.97%] [G loss: 2.406895]\n",
      "epoch:9 step:9084 [D loss: 0.625178, acc: 62.50%] [G loss: 2.138200]\n",
      "epoch:9 step:9085 [D loss: 0.704700, acc: 58.59%] [G loss: 2.082348]\n",
      "epoch:9 step:9086 [D loss: 0.651467, acc: 60.16%] [G loss: 2.086593]\n",
      "epoch:9 step:9087 [D loss: 0.657494, acc: 62.50%] [G loss: 2.187064]\n",
      "epoch:9 step:9088 [D loss: 0.697438, acc: 60.16%] [G loss: 2.056381]\n",
      "epoch:9 step:9089 [D loss: 0.618619, acc: 64.84%] [G loss: 2.144935]\n",
      "epoch:9 step:9090 [D loss: 0.671799, acc: 62.50%] [G loss: 1.980202]\n",
      "epoch:9 step:9091 [D loss: 0.722716, acc: 58.59%] [G loss: 1.924930]\n",
      "epoch:9 step:9092 [D loss: 0.624891, acc: 67.97%] [G loss: 2.085750]\n",
      "epoch:9 step:9093 [D loss: 0.594447, acc: 71.88%] [G loss: 2.032801]\n",
      "epoch:9 step:9094 [D loss: 0.580613, acc: 76.56%] [G loss: 2.253935]\n",
      "epoch:9 step:9095 [D loss: 0.626659, acc: 64.84%] [G loss: 1.971066]\n",
      "epoch:9 step:9096 [D loss: 0.623278, acc: 63.28%] [G loss: 2.269177]\n",
      "epoch:9 step:9097 [D loss: 0.700169, acc: 57.81%] [G loss: 1.991811]\n",
      "epoch:9 step:9098 [D loss: 0.651773, acc: 67.19%] [G loss: 1.925265]\n",
      "epoch:9 step:9099 [D loss: 0.658124, acc: 58.59%] [G loss: 1.884025]\n",
      "epoch:9 step:9100 [D loss: 0.651413, acc: 64.84%] [G loss: 1.925311]\n",
      "epoch:9 step:9101 [D loss: 0.632554, acc: 64.06%] [G loss: 2.142043]\n",
      "epoch:9 step:9102 [D loss: 0.658526, acc: 60.94%] [G loss: 1.779876]\n",
      "epoch:9 step:9103 [D loss: 0.627844, acc: 67.97%] [G loss: 2.019542]\n",
      "epoch:9 step:9104 [D loss: 0.676437, acc: 59.38%] [G loss: 2.036501]\n",
      "epoch:9 step:9105 [D loss: 0.594638, acc: 71.88%] [G loss: 1.994952]\n",
      "epoch:9 step:9106 [D loss: 0.676407, acc: 59.38%] [G loss: 2.117083]\n",
      "epoch:9 step:9107 [D loss: 0.629273, acc: 67.97%] [G loss: 1.871437]\n",
      "epoch:9 step:9108 [D loss: 0.685418, acc: 62.50%] [G loss: 2.022593]\n",
      "epoch:9 step:9109 [D loss: 0.675486, acc: 55.47%] [G loss: 2.016806]\n",
      "epoch:9 step:9110 [D loss: 0.612109, acc: 68.75%] [G loss: 2.127776]\n",
      "epoch:9 step:9111 [D loss: 0.626573, acc: 64.06%] [G loss: 2.083920]\n",
      "epoch:9 step:9112 [D loss: 0.611207, acc: 62.50%] [G loss: 2.223490]\n",
      "epoch:9 step:9113 [D loss: 0.650174, acc: 59.38%] [G loss: 2.016222]\n",
      "epoch:9 step:9114 [D loss: 0.581037, acc: 71.88%] [G loss: 1.959998]\n",
      "epoch:9 step:9115 [D loss: 0.613973, acc: 67.19%] [G loss: 2.108032]\n",
      "epoch:9 step:9116 [D loss: 0.604508, acc: 68.75%] [G loss: 1.970378]\n",
      "epoch:9 step:9117 [D loss: 0.648765, acc: 65.62%] [G loss: 2.093533]\n",
      "epoch:9 step:9118 [D loss: 0.613745, acc: 66.41%] [G loss: 2.054770]\n",
      "epoch:9 step:9119 [D loss: 0.641586, acc: 65.62%] [G loss: 2.111015]\n",
      "epoch:9 step:9120 [D loss: 0.634889, acc: 66.41%] [G loss: 1.974058]\n",
      "epoch:9 step:9121 [D loss: 0.625830, acc: 64.84%] [G loss: 2.012951]\n",
      "epoch:9 step:9122 [D loss: 0.598012, acc: 67.19%] [G loss: 2.194807]\n",
      "epoch:9 step:9123 [D loss: 0.700089, acc: 55.47%] [G loss: 2.099613]\n",
      "epoch:9 step:9124 [D loss: 0.604603, acc: 66.41%] [G loss: 2.262657]\n",
      "epoch:9 step:9125 [D loss: 0.612909, acc: 62.50%] [G loss: 2.163153]\n",
      "epoch:9 step:9126 [D loss: 0.583050, acc: 69.53%] [G loss: 2.408611]\n",
      "epoch:9 step:9127 [D loss: 0.560879, acc: 73.44%] [G loss: 2.287143]\n",
      "epoch:9 step:9128 [D loss: 0.577306, acc: 70.31%] [G loss: 2.165378]\n",
      "epoch:9 step:9129 [D loss: 0.623130, acc: 63.28%] [G loss: 2.139531]\n",
      "epoch:9 step:9130 [D loss: 0.663836, acc: 63.28%] [G loss: 2.166343]\n",
      "epoch:9 step:9131 [D loss: 0.684786, acc: 59.38%] [G loss: 1.915573]\n",
      "epoch:9 step:9132 [D loss: 0.616279, acc: 65.62%] [G loss: 2.238197]\n",
      "epoch:9 step:9133 [D loss: 0.650787, acc: 59.38%] [G loss: 2.030300]\n",
      "epoch:9 step:9134 [D loss: 0.644842, acc: 64.06%] [G loss: 2.019076]\n",
      "epoch:9 step:9135 [D loss: 0.707081, acc: 60.16%] [G loss: 1.918552]\n",
      "epoch:9 step:9136 [D loss: 0.721589, acc: 52.34%] [G loss: 1.851901]\n",
      "epoch:9 step:9137 [D loss: 0.637774, acc: 62.50%] [G loss: 1.784119]\n",
      "epoch:9 step:9138 [D loss: 0.614099, acc: 73.44%] [G loss: 2.040054]\n",
      "epoch:9 step:9139 [D loss: 0.613619, acc: 67.19%] [G loss: 2.172416]\n",
      "epoch:9 step:9140 [D loss: 0.558878, acc: 71.88%] [G loss: 2.369384]\n",
      "epoch:9 step:9141 [D loss: 0.621488, acc: 71.88%] [G loss: 2.163653]\n",
      "epoch:9 step:9142 [D loss: 0.574025, acc: 71.88%] [G loss: 2.047593]\n",
      "epoch:9 step:9143 [D loss: 0.732190, acc: 50.78%] [G loss: 1.942912]\n",
      "epoch:9 step:9144 [D loss: 0.618269, acc: 61.72%] [G loss: 2.009371]\n",
      "epoch:9 step:9145 [D loss: 0.633107, acc: 63.28%] [G loss: 2.179699]\n",
      "epoch:9 step:9146 [D loss: 0.656430, acc: 64.84%] [G loss: 2.092406]\n",
      "epoch:9 step:9147 [D loss: 0.678196, acc: 60.94%] [G loss: 1.994877]\n",
      "epoch:9 step:9148 [D loss: 0.610268, acc: 64.84%] [G loss: 2.020219]\n",
      "epoch:9 step:9149 [D loss: 0.660171, acc: 62.50%] [G loss: 1.995586]\n",
      "epoch:9 step:9150 [D loss: 0.669242, acc: 60.16%] [G loss: 2.039539]\n",
      "epoch:9 step:9151 [D loss: 0.609627, acc: 63.28%] [G loss: 2.146835]\n",
      "epoch:9 step:9152 [D loss: 0.571350, acc: 74.22%] [G loss: 2.100049]\n",
      "epoch:9 step:9153 [D loss: 0.673391, acc: 60.16%] [G loss: 2.095074]\n",
      "epoch:9 step:9154 [D loss: 0.626503, acc: 64.06%] [G loss: 2.002056]\n",
      "epoch:9 step:9155 [D loss: 0.640012, acc: 64.06%] [G loss: 1.884354]\n",
      "epoch:9 step:9156 [D loss: 0.665122, acc: 60.94%] [G loss: 2.192003]\n",
      "epoch:9 step:9157 [D loss: 0.625753, acc: 70.31%] [G loss: 1.998564]\n",
      "epoch:9 step:9158 [D loss: 0.606135, acc: 63.28%] [G loss: 2.358090]\n",
      "epoch:9 step:9159 [D loss: 0.644312, acc: 64.84%] [G loss: 2.095129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9160 [D loss: 0.591647, acc: 73.44%] [G loss: 1.983369]\n",
      "epoch:9 step:9161 [D loss: 0.591089, acc: 72.66%] [G loss: 2.128536]\n",
      "epoch:9 step:9162 [D loss: 0.612144, acc: 60.94%] [G loss: 1.976529]\n",
      "epoch:9 step:9163 [D loss: 0.602116, acc: 67.97%] [G loss: 2.093995]\n",
      "epoch:9 step:9164 [D loss: 0.629249, acc: 60.16%] [G loss: 2.047699]\n",
      "epoch:9 step:9165 [D loss: 0.571369, acc: 70.31%] [G loss: 2.076676]\n",
      "epoch:9 step:9166 [D loss: 0.616805, acc: 67.97%] [G loss: 2.059907]\n",
      "epoch:9 step:9167 [D loss: 0.679193, acc: 60.94%] [G loss: 1.951849]\n",
      "epoch:9 step:9168 [D loss: 0.712341, acc: 58.59%] [G loss: 2.168484]\n",
      "epoch:9 step:9169 [D loss: 0.565506, acc: 73.44%] [G loss: 2.022112]\n",
      "epoch:9 step:9170 [D loss: 0.588129, acc: 68.75%] [G loss: 2.027915]\n",
      "epoch:9 step:9171 [D loss: 0.636188, acc: 63.28%] [G loss: 2.111123]\n",
      "epoch:9 step:9172 [D loss: 0.591874, acc: 68.75%] [G loss: 1.919883]\n",
      "epoch:9 step:9173 [D loss: 0.622136, acc: 67.19%] [G loss: 2.024746]\n",
      "epoch:9 step:9174 [D loss: 0.636593, acc: 71.09%] [G loss: 1.931356]\n",
      "epoch:9 step:9175 [D loss: 0.640919, acc: 60.16%] [G loss: 2.231267]\n",
      "epoch:9 step:9176 [D loss: 0.608508, acc: 64.84%] [G loss: 2.188103]\n",
      "epoch:9 step:9177 [D loss: 0.675235, acc: 65.62%] [G loss: 1.998901]\n",
      "epoch:9 step:9178 [D loss: 0.576895, acc: 71.09%] [G loss: 2.228774]\n",
      "epoch:9 step:9179 [D loss: 0.596353, acc: 67.19%] [G loss: 2.211746]\n",
      "epoch:9 step:9180 [D loss: 0.609875, acc: 61.72%] [G loss: 2.051979]\n",
      "epoch:9 step:9181 [D loss: 0.612809, acc: 63.28%] [G loss: 2.233023]\n",
      "epoch:9 step:9182 [D loss: 0.627576, acc: 64.84%] [G loss: 1.949899]\n",
      "epoch:9 step:9183 [D loss: 0.660431, acc: 57.81%] [G loss: 2.120402]\n",
      "epoch:9 step:9184 [D loss: 0.595461, acc: 69.53%] [G loss: 2.034828]\n",
      "epoch:9 step:9185 [D loss: 0.610699, acc: 65.62%] [G loss: 1.967167]\n",
      "epoch:9 step:9186 [D loss: 0.694825, acc: 57.81%] [G loss: 1.916136]\n",
      "epoch:9 step:9187 [D loss: 0.561401, acc: 75.00%] [G loss: 2.208808]\n",
      "epoch:9 step:9188 [D loss: 0.668460, acc: 57.03%] [G loss: 2.071799]\n",
      "epoch:9 step:9189 [D loss: 0.603680, acc: 63.28%] [G loss: 2.108327]\n",
      "epoch:9 step:9190 [D loss: 0.582965, acc: 67.19%] [G loss: 2.191522]\n",
      "epoch:9 step:9191 [D loss: 0.713114, acc: 59.38%] [G loss: 1.902024]\n",
      "epoch:9 step:9192 [D loss: 0.676042, acc: 62.50%] [G loss: 1.952765]\n",
      "epoch:9 step:9193 [D loss: 0.614986, acc: 68.75%] [G loss: 2.137699]\n",
      "epoch:9 step:9194 [D loss: 0.627094, acc: 63.28%] [G loss: 2.137805]\n",
      "epoch:9 step:9195 [D loss: 0.712957, acc: 53.91%] [G loss: 2.084578]\n",
      "epoch:9 step:9196 [D loss: 0.698642, acc: 55.47%] [G loss: 1.996124]\n",
      "epoch:9 step:9197 [D loss: 0.706861, acc: 59.38%] [G loss: 1.918451]\n",
      "epoch:9 step:9198 [D loss: 0.679651, acc: 60.16%] [G loss: 1.738180]\n",
      "epoch:9 step:9199 [D loss: 0.673158, acc: 57.03%] [G loss: 1.953450]\n",
      "epoch:9 step:9200 [D loss: 0.654639, acc: 58.59%] [G loss: 1.961811]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.632180\n",
      "FID: 25.200850\n",
      "0 = 12.850075065517446\n",
      "1 = 0.08598673017278949\n",
      "2 = 0.9014000296592712\n",
      "3 = 0.9065999984741211\n",
      "4 = 0.8962000012397766\n",
      "5 = 0.8972684144973755\n",
      "6 = 0.9065999984741211\n",
      "7 = 7.4784208129167755\n",
      "8 = 0.09828654021806399\n",
      "9 = 0.7684000134468079\n",
      "10 = 0.7720000147819519\n",
      "11 = 0.7648000121116638\n",
      "12 = 0.766481339931488\n",
      "13 = 0.7720000147819519\n",
      "14 = 6.632207870483398\n",
      "15 = 9.23961067199707\n",
      "16 = 0.16880878806114197\n",
      "17 = 6.632180213928223\n",
      "18 = 25.200849533081055\n",
      "epoch:9 step:9201 [D loss: 0.642368, acc: 60.16%] [G loss: 1.985814]\n",
      "epoch:9 step:9202 [D loss: 0.561781, acc: 74.22%] [G loss: 2.087121]\n",
      "epoch:9 step:9203 [D loss: 0.646610, acc: 61.72%] [G loss: 2.002434]\n",
      "epoch:9 step:9204 [D loss: 0.671894, acc: 66.41%] [G loss: 1.967664]\n",
      "epoch:9 step:9205 [D loss: 0.609175, acc: 68.75%] [G loss: 2.011587]\n",
      "epoch:9 step:9206 [D loss: 0.629522, acc: 65.62%] [G loss: 2.021503]\n",
      "epoch:9 step:9207 [D loss: 0.731096, acc: 60.16%] [G loss: 2.036093]\n",
      "epoch:9 step:9208 [D loss: 0.618208, acc: 69.53%] [G loss: 2.213030]\n",
      "epoch:9 step:9209 [D loss: 0.645061, acc: 61.72%] [G loss: 1.911271]\n",
      "epoch:9 step:9210 [D loss: 0.679988, acc: 59.38%] [G loss: 2.052780]\n",
      "epoch:9 step:9211 [D loss: 0.661206, acc: 60.94%] [G loss: 1.947362]\n",
      "epoch:9 step:9212 [D loss: 0.623446, acc: 64.84%] [G loss: 2.069781]\n",
      "epoch:9 step:9213 [D loss: 0.623146, acc: 63.28%] [G loss: 2.314191]\n",
      "epoch:9 step:9214 [D loss: 0.615381, acc: 64.06%] [G loss: 2.363167]\n",
      "epoch:9 step:9215 [D loss: 0.565493, acc: 71.88%] [G loss: 2.364061]\n",
      "epoch:9 step:9216 [D loss: 0.671810, acc: 60.94%] [G loss: 2.097825]\n",
      "epoch:9 step:9217 [D loss: 0.654863, acc: 60.16%] [G loss: 2.030923]\n",
      "epoch:9 step:9218 [D loss: 0.561180, acc: 74.22%] [G loss: 2.126976]\n",
      "epoch:9 step:9219 [D loss: 0.560876, acc: 73.44%] [G loss: 2.210028]\n",
      "epoch:9 step:9220 [D loss: 0.649361, acc: 60.16%] [G loss: 1.939003]\n",
      "epoch:9 step:9221 [D loss: 0.668176, acc: 60.94%] [G loss: 2.076509]\n",
      "epoch:9 step:9222 [D loss: 0.607206, acc: 70.31%] [G loss: 2.140867]\n",
      "epoch:9 step:9223 [D loss: 0.653353, acc: 67.97%] [G loss: 2.165234]\n",
      "epoch:9 step:9224 [D loss: 0.689070, acc: 61.72%] [G loss: 2.114511]\n",
      "epoch:9 step:9225 [D loss: 0.589079, acc: 67.97%] [G loss: 2.154688]\n",
      "epoch:9 step:9226 [D loss: 0.642841, acc: 67.19%] [G loss: 2.235798]\n",
      "epoch:9 step:9227 [D loss: 0.664684, acc: 60.16%] [G loss: 2.061046]\n",
      "epoch:9 step:9228 [D loss: 0.691478, acc: 52.34%] [G loss: 1.892497]\n",
      "epoch:9 step:9229 [D loss: 0.654775, acc: 60.94%] [G loss: 2.095891]\n",
      "epoch:9 step:9230 [D loss: 0.623794, acc: 66.41%] [G loss: 1.922909]\n",
      "epoch:9 step:9231 [D loss: 0.637053, acc: 67.19%] [G loss: 2.368412]\n",
      "epoch:9 step:9232 [D loss: 0.631740, acc: 66.41%] [G loss: 2.018515]\n",
      "epoch:9 step:9233 [D loss: 0.709263, acc: 57.81%] [G loss: 2.064205]\n",
      "epoch:9 step:9234 [D loss: 0.703580, acc: 53.91%] [G loss: 2.088295]\n",
      "epoch:9 step:9235 [D loss: 0.623622, acc: 66.41%] [G loss: 1.967728]\n",
      "epoch:9 step:9236 [D loss: 0.572745, acc: 75.78%] [G loss: 2.024614]\n",
      "epoch:9 step:9237 [D loss: 0.664390, acc: 61.72%] [G loss: 1.908633]\n",
      "epoch:9 step:9238 [D loss: 0.638442, acc: 58.59%] [G loss: 2.199815]\n",
      "epoch:9 step:9239 [D loss: 0.661430, acc: 58.59%] [G loss: 2.224548]\n",
      "epoch:9 step:9240 [D loss: 0.583572, acc: 70.31%] [G loss: 2.245370]\n",
      "epoch:9 step:9241 [D loss: 0.613074, acc: 68.75%] [G loss: 2.255178]\n",
      "epoch:9 step:9242 [D loss: 0.664012, acc: 57.03%] [G loss: 2.107418]\n",
      "epoch:9 step:9243 [D loss: 0.670911, acc: 63.28%] [G loss: 1.941615]\n",
      "epoch:9 step:9244 [D loss: 0.662672, acc: 62.50%] [G loss: 2.047098]\n",
      "epoch:9 step:9245 [D loss: 0.670424, acc: 58.59%] [G loss: 1.872143]\n",
      "epoch:9 step:9246 [D loss: 0.635519, acc: 65.62%] [G loss: 2.008999]\n",
      "epoch:9 step:9247 [D loss: 0.669776, acc: 63.28%] [G loss: 1.943816]\n",
      "epoch:9 step:9248 [D loss: 0.571794, acc: 68.75%] [G loss: 2.295222]\n",
      "epoch:9 step:9249 [D loss: 0.670469, acc: 60.16%] [G loss: 1.937282]\n",
      "epoch:9 step:9250 [D loss: 0.668461, acc: 59.38%] [G loss: 1.987288]\n",
      "epoch:9 step:9251 [D loss: 0.672669, acc: 60.94%] [G loss: 1.792601]\n",
      "epoch:9 step:9252 [D loss: 0.635794, acc: 59.38%] [G loss: 1.927171]\n",
      "epoch:9 step:9253 [D loss: 0.658068, acc: 59.38%] [G loss: 1.920143]\n",
      "epoch:9 step:9254 [D loss: 0.666375, acc: 64.84%] [G loss: 1.933253]\n",
      "epoch:9 step:9255 [D loss: 0.637160, acc: 64.84%] [G loss: 2.112574]\n",
      "epoch:9 step:9256 [D loss: 0.577130, acc: 71.09%] [G loss: 2.014183]\n",
      "epoch:9 step:9257 [D loss: 0.642518, acc: 66.41%] [G loss: 2.011270]\n",
      "epoch:9 step:9258 [D loss: 0.587764, acc: 68.75%] [G loss: 2.064900]\n",
      "epoch:9 step:9259 [D loss: 0.618512, acc: 67.19%] [G loss: 2.016784]\n",
      "epoch:9 step:9260 [D loss: 0.694026, acc: 53.91%] [G loss: 1.847951]\n",
      "epoch:9 step:9261 [D loss: 0.664691, acc: 59.38%] [G loss: 1.934611]\n",
      "epoch:9 step:9262 [D loss: 0.658835, acc: 57.81%] [G loss: 1.983133]\n",
      "epoch:9 step:9263 [D loss: 0.638233, acc: 62.50%] [G loss: 2.060040]\n",
      "epoch:9 step:9264 [D loss: 0.653856, acc: 56.25%] [G loss: 2.149287]\n",
      "epoch:9 step:9265 [D loss: 0.677573, acc: 60.94%] [G loss: 1.831540]\n",
      "epoch:9 step:9266 [D loss: 0.638291, acc: 61.72%] [G loss: 1.764154]\n",
      "epoch:9 step:9267 [D loss: 0.616687, acc: 71.09%] [G loss: 1.893016]\n",
      "epoch:9 step:9268 [D loss: 0.633931, acc: 64.84%] [G loss: 1.964090]\n",
      "epoch:9 step:9269 [D loss: 0.646771, acc: 60.16%] [G loss: 2.031307]\n",
      "epoch:9 step:9270 [D loss: 0.624431, acc: 64.84%] [G loss: 2.128611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9271 [D loss: 0.700811, acc: 57.03%] [G loss: 1.883840]\n",
      "epoch:9 step:9272 [D loss: 0.592716, acc: 72.66%] [G loss: 1.961714]\n",
      "epoch:9 step:9273 [D loss: 0.588686, acc: 72.66%] [G loss: 2.005677]\n",
      "epoch:9 step:9274 [D loss: 0.583451, acc: 71.09%] [G loss: 2.055683]\n",
      "epoch:9 step:9275 [D loss: 0.588206, acc: 66.41%] [G loss: 2.173861]\n",
      "epoch:9 step:9276 [D loss: 0.590377, acc: 67.19%] [G loss: 1.983654]\n",
      "epoch:9 step:9277 [D loss: 0.658064, acc: 59.38%] [G loss: 2.102405]\n",
      "epoch:9 step:9278 [D loss: 0.582321, acc: 71.09%] [G loss: 2.103225]\n",
      "epoch:9 step:9279 [D loss: 0.643223, acc: 61.72%] [G loss: 2.154661]\n",
      "epoch:9 step:9280 [D loss: 0.632049, acc: 63.28%] [G loss: 2.134205]\n",
      "epoch:9 step:9281 [D loss: 0.571147, acc: 73.44%] [G loss: 2.115302]\n",
      "epoch:9 step:9282 [D loss: 0.636174, acc: 61.72%] [G loss: 2.185076]\n",
      "epoch:9 step:9283 [D loss: 0.641541, acc: 67.97%] [G loss: 2.025799]\n",
      "epoch:9 step:9284 [D loss: 0.665658, acc: 61.72%] [G loss: 1.818226]\n",
      "epoch:9 step:9285 [D loss: 0.599579, acc: 71.09%] [G loss: 1.926462]\n",
      "epoch:9 step:9286 [D loss: 0.639426, acc: 65.62%] [G loss: 1.966582]\n",
      "epoch:9 step:9287 [D loss: 0.628750, acc: 64.06%] [G loss: 1.973772]\n",
      "epoch:9 step:9288 [D loss: 0.623234, acc: 66.41%] [G loss: 2.032750]\n",
      "epoch:9 step:9289 [D loss: 0.669503, acc: 58.59%] [G loss: 2.101530]\n",
      "epoch:9 step:9290 [D loss: 0.576187, acc: 67.19%] [G loss: 2.222519]\n",
      "epoch:9 step:9291 [D loss: 0.721769, acc: 53.12%] [G loss: 1.864795]\n",
      "epoch:9 step:9292 [D loss: 0.683119, acc: 63.28%] [G loss: 1.991123]\n",
      "epoch:9 step:9293 [D loss: 0.621699, acc: 70.31%] [G loss: 2.217014]\n",
      "epoch:9 step:9294 [D loss: 0.680666, acc: 60.16%] [G loss: 1.983946]\n",
      "epoch:9 step:9295 [D loss: 0.628406, acc: 62.50%] [G loss: 1.967631]\n",
      "epoch:9 step:9296 [D loss: 0.654096, acc: 64.06%] [G loss: 1.976393]\n",
      "epoch:9 step:9297 [D loss: 0.640448, acc: 62.50%] [G loss: 1.978687]\n",
      "epoch:9 step:9298 [D loss: 0.597670, acc: 71.88%] [G loss: 2.020906]\n",
      "epoch:9 step:9299 [D loss: 0.645741, acc: 58.59%] [G loss: 2.011557]\n",
      "epoch:9 step:9300 [D loss: 0.661711, acc: 61.72%] [G loss: 1.963989]\n",
      "epoch:9 step:9301 [D loss: 0.656071, acc: 60.16%] [G loss: 1.998520]\n",
      "epoch:9 step:9302 [D loss: 0.648079, acc: 62.50%] [G loss: 1.930156]\n",
      "epoch:9 step:9303 [D loss: 0.654455, acc: 60.16%] [G loss: 1.996580]\n",
      "epoch:9 step:9304 [D loss: 0.605804, acc: 67.19%] [G loss: 1.945215]\n",
      "epoch:9 step:9305 [D loss: 0.636401, acc: 60.16%] [G loss: 1.958181]\n",
      "epoch:9 step:9306 [D loss: 0.656938, acc: 55.47%] [G loss: 1.867341]\n",
      "epoch:9 step:9307 [D loss: 0.680349, acc: 53.91%] [G loss: 1.848543]\n",
      "epoch:9 step:9308 [D loss: 0.565559, acc: 72.66%] [G loss: 2.007862]\n",
      "epoch:9 step:9309 [D loss: 0.565093, acc: 69.53%] [G loss: 2.076138]\n",
      "epoch:9 step:9310 [D loss: 0.598500, acc: 65.62%] [G loss: 2.067039]\n",
      "epoch:9 step:9311 [D loss: 0.674809, acc: 57.81%] [G loss: 1.846906]\n",
      "epoch:9 step:9312 [D loss: 0.587066, acc: 67.19%] [G loss: 2.105809]\n",
      "epoch:9 step:9313 [D loss: 0.662508, acc: 62.50%] [G loss: 1.901748]\n",
      "epoch:9 step:9314 [D loss: 0.580696, acc: 71.09%] [G loss: 2.157674]\n",
      "epoch:9 step:9315 [D loss: 0.622584, acc: 67.19%] [G loss: 2.148881]\n",
      "epoch:9 step:9316 [D loss: 0.648370, acc: 60.94%] [G loss: 1.968753]\n",
      "epoch:9 step:9317 [D loss: 0.584609, acc: 62.50%] [G loss: 2.074446]\n",
      "epoch:9 step:9318 [D loss: 0.672435, acc: 59.38%] [G loss: 2.027208]\n",
      "epoch:9 step:9319 [D loss: 0.577940, acc: 73.44%] [G loss: 2.421927]\n",
      "epoch:9 step:9320 [D loss: 0.627163, acc: 64.84%] [G loss: 2.236780]\n",
      "epoch:9 step:9321 [D loss: 0.659523, acc: 56.25%] [G loss: 2.137784]\n",
      "epoch:9 step:9322 [D loss: 0.731241, acc: 56.25%] [G loss: 2.204848]\n",
      "epoch:9 step:9323 [D loss: 0.648630, acc: 64.84%] [G loss: 2.119016]\n",
      "epoch:9 step:9324 [D loss: 0.628402, acc: 68.75%] [G loss: 1.978896]\n",
      "epoch:9 step:9325 [D loss: 0.653654, acc: 64.84%] [G loss: 2.027562]\n",
      "epoch:9 step:9326 [D loss: 0.629750, acc: 65.62%] [G loss: 2.083765]\n",
      "epoch:9 step:9327 [D loss: 0.634426, acc: 67.97%] [G loss: 2.017187]\n",
      "epoch:9 step:9328 [D loss: 0.648054, acc: 58.59%] [G loss: 2.074998]\n",
      "epoch:9 step:9329 [D loss: 0.639958, acc: 65.62%] [G loss: 2.009563]\n",
      "epoch:9 step:9330 [D loss: 0.632141, acc: 63.28%] [G loss: 2.139087]\n",
      "epoch:9 step:9331 [D loss: 0.567651, acc: 71.09%] [G loss: 2.137743]\n",
      "epoch:9 step:9332 [D loss: 0.611255, acc: 64.84%] [G loss: 2.364184]\n",
      "epoch:9 step:9333 [D loss: 0.577526, acc: 71.88%] [G loss: 2.035576]\n",
      "epoch:9 step:9334 [D loss: 0.707367, acc: 58.59%] [G loss: 2.009613]\n",
      "epoch:9 step:9335 [D loss: 0.673723, acc: 60.16%] [G loss: 2.034100]\n",
      "epoch:9 step:9336 [D loss: 0.655459, acc: 59.38%] [G loss: 2.079735]\n",
      "epoch:9 step:9337 [D loss: 0.636290, acc: 62.50%] [G loss: 2.010713]\n",
      "epoch:9 step:9338 [D loss: 0.667164, acc: 64.06%] [G loss: 1.938973]\n",
      "epoch:9 step:9339 [D loss: 0.658831, acc: 60.94%] [G loss: 2.228102]\n",
      "epoch:9 step:9340 [D loss: 0.631489, acc: 64.84%] [G loss: 2.024988]\n",
      "epoch:9 step:9341 [D loss: 0.645972, acc: 68.75%] [G loss: 2.064212]\n",
      "epoch:9 step:9342 [D loss: 0.656621, acc: 61.72%] [G loss: 2.218236]\n",
      "epoch:9 step:9343 [D loss: 0.631440, acc: 60.94%] [G loss: 2.013799]\n",
      "epoch:9 step:9344 [D loss: 0.569281, acc: 71.88%] [G loss: 2.109504]\n",
      "epoch:9 step:9345 [D loss: 0.609603, acc: 70.31%] [G loss: 2.337798]\n",
      "epoch:9 step:9346 [D loss: 0.621002, acc: 62.50%] [G loss: 2.342901]\n",
      "epoch:9 step:9347 [D loss: 0.675469, acc: 60.94%] [G loss: 2.035746]\n",
      "epoch:9 step:9348 [D loss: 0.666371, acc: 62.50%] [G loss: 2.016614]\n",
      "epoch:9 step:9349 [D loss: 0.674881, acc: 60.16%] [G loss: 1.849175]\n",
      "epoch:9 step:9350 [D loss: 0.697321, acc: 58.59%] [G loss: 1.956095]\n",
      "epoch:9 step:9351 [D loss: 0.618218, acc: 68.75%] [G loss: 2.182217]\n",
      "epoch:9 step:9352 [D loss: 0.646926, acc: 64.06%] [G loss: 2.196850]\n",
      "epoch:9 step:9353 [D loss: 0.615661, acc: 62.50%] [G loss: 2.178445]\n",
      "epoch:9 step:9354 [D loss: 0.593658, acc: 67.97%] [G loss: 2.111979]\n",
      "epoch:9 step:9355 [D loss: 0.648128, acc: 60.16%] [G loss: 2.033583]\n",
      "epoch:9 step:9356 [D loss: 0.529167, acc: 71.09%] [G loss: 2.349360]\n",
      "epoch:9 step:9357 [D loss: 0.532748, acc: 78.12%] [G loss: 2.463253]\n",
      "epoch:9 step:9358 [D loss: 0.566845, acc: 74.22%] [G loss: 2.237430]\n",
      "epoch:9 step:9359 [D loss: 0.570424, acc: 69.53%] [G loss: 2.538491]\n",
      "epoch:9 step:9360 [D loss: 0.586614, acc: 67.19%] [G loss: 2.452752]\n",
      "epoch:9 step:9361 [D loss: 0.656972, acc: 59.38%] [G loss: 2.035962]\n",
      "epoch:9 step:9362 [D loss: 0.667187, acc: 57.81%] [G loss: 2.126676]\n",
      "epoch:9 step:9363 [D loss: 0.664894, acc: 60.94%] [G loss: 2.064909]\n",
      "epoch:9 step:9364 [D loss: 0.604319, acc: 67.97%] [G loss: 2.126224]\n",
      "epoch:9 step:9365 [D loss: 0.583160, acc: 69.53%] [G loss: 2.082262]\n",
      "epoch:9 step:9366 [D loss: 0.622684, acc: 66.41%] [G loss: 2.151413]\n",
      "epoch:9 step:9367 [D loss: 0.571211, acc: 72.66%] [G loss: 2.370100]\n",
      "epoch:9 step:9368 [D loss: 0.603529, acc: 69.53%] [G loss: 2.354900]\n",
      "epoch:9 step:9369 [D loss: 0.560509, acc: 76.56%] [G loss: 2.344519]\n",
      "epoch:9 step:9370 [D loss: 0.552790, acc: 72.66%] [G loss: 2.899260]\n",
      "epoch:10 step:9371 [D loss: 0.649495, acc: 65.62%] [G loss: 2.172249]\n",
      "epoch:10 step:9372 [D loss: 0.598563, acc: 65.62%] [G loss: 2.298554]\n",
      "epoch:10 step:9373 [D loss: 0.570460, acc: 75.00%] [G loss: 2.105484]\n",
      "epoch:10 step:9374 [D loss: 0.655961, acc: 64.06%] [G loss: 2.156263]\n",
      "epoch:10 step:9375 [D loss: 0.603869, acc: 71.09%] [G loss: 1.979162]\n",
      "epoch:10 step:9376 [D loss: 0.672829, acc: 53.91%] [G loss: 2.126116]\n",
      "epoch:10 step:9377 [D loss: 0.612872, acc: 64.06%] [G loss: 2.081300]\n",
      "epoch:10 step:9378 [D loss: 0.621776, acc: 68.75%] [G loss: 2.194999]\n",
      "epoch:10 step:9379 [D loss: 0.626486, acc: 67.97%] [G loss: 2.239338]\n",
      "epoch:10 step:9380 [D loss: 0.600942, acc: 67.19%] [G loss: 2.192176]\n",
      "epoch:10 step:9381 [D loss: 0.621130, acc: 64.84%] [G loss: 2.214620]\n",
      "epoch:10 step:9382 [D loss: 0.614421, acc: 68.75%] [G loss: 2.209192]\n",
      "epoch:10 step:9383 [D loss: 0.646772, acc: 62.50%] [G loss: 2.192160]\n",
      "epoch:10 step:9384 [D loss: 0.611882, acc: 66.41%] [G loss: 2.252478]\n",
      "epoch:10 step:9385 [D loss: 0.599665, acc: 71.09%] [G loss: 2.282145]\n",
      "epoch:10 step:9386 [D loss: 0.616706, acc: 67.19%] [G loss: 2.337480]\n",
      "epoch:10 step:9387 [D loss: 0.650845, acc: 58.59%] [G loss: 1.991565]\n",
      "epoch:10 step:9388 [D loss: 0.633035, acc: 60.94%] [G loss: 2.072688]\n",
      "epoch:10 step:9389 [D loss: 0.665106, acc: 60.16%] [G loss: 2.080847]\n",
      "epoch:10 step:9390 [D loss: 0.752926, acc: 56.25%] [G loss: 1.797422]\n",
      "epoch:10 step:9391 [D loss: 0.600617, acc: 64.06%] [G loss: 1.920784]\n",
      "epoch:10 step:9392 [D loss: 0.613126, acc: 68.75%] [G loss: 2.001434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9393 [D loss: 0.663312, acc: 67.19%] [G loss: 2.098760]\n",
      "epoch:10 step:9394 [D loss: 0.674284, acc: 57.03%] [G loss: 1.887659]\n",
      "epoch:10 step:9395 [D loss: 0.599442, acc: 72.66%] [G loss: 2.250409]\n",
      "epoch:10 step:9396 [D loss: 0.624432, acc: 64.06%] [G loss: 1.907074]\n",
      "epoch:10 step:9397 [D loss: 0.609218, acc: 65.62%] [G loss: 2.021679]\n",
      "epoch:10 step:9398 [D loss: 0.581066, acc: 68.75%] [G loss: 2.027746]\n",
      "epoch:10 step:9399 [D loss: 0.623992, acc: 66.41%] [G loss: 2.086704]\n",
      "epoch:10 step:9400 [D loss: 0.586375, acc: 72.66%] [G loss: 1.941153]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.619855\n",
      "FID: 25.959492\n",
      "0 = 12.899711280584327\n",
      "1 = 0.0815722019081593\n",
      "2 = 0.9042999744415283\n",
      "3 = 0.9211999773979187\n",
      "4 = 0.8873999714851379\n",
      "5 = 0.8910814523696899\n",
      "6 = 0.9211999773979187\n",
      "7 = 7.528349820554249\n",
      "8 = 0.1018707293217915\n",
      "9 = 0.7703999876976013\n",
      "10 = 0.777400016784668\n",
      "11 = 0.7634000182151794\n",
      "12 = 0.7666666507720947\n",
      "13 = 0.777400016784668\n",
      "14 = 6.619879245758057\n",
      "15 = 9.361302375793457\n",
      "16 = 0.15210749208927155\n",
      "17 = 6.6198554039001465\n",
      "18 = 25.959491729736328\n",
      "epoch:10 step:9401 [D loss: 0.707964, acc: 53.12%] [G loss: 1.845664]\n",
      "epoch:10 step:9402 [D loss: 0.639787, acc: 65.62%] [G loss: 2.043057]\n",
      "epoch:10 step:9403 [D loss: 0.606475, acc: 69.53%] [G loss: 1.885102]\n",
      "epoch:10 step:9404 [D loss: 0.666292, acc: 59.38%] [G loss: 1.894220]\n",
      "epoch:10 step:9405 [D loss: 0.592416, acc: 66.41%] [G loss: 1.929980]\n",
      "epoch:10 step:9406 [D loss: 0.639664, acc: 67.97%] [G loss: 2.004212]\n",
      "epoch:10 step:9407 [D loss: 0.688183, acc: 66.41%] [G loss: 2.144356]\n",
      "epoch:10 step:9408 [D loss: 0.661358, acc: 62.50%] [G loss: 2.215372]\n",
      "epoch:10 step:9409 [D loss: 0.555324, acc: 75.00%] [G loss: 2.022657]\n",
      "epoch:10 step:9410 [D loss: 0.606838, acc: 66.41%] [G loss: 2.318587]\n",
      "epoch:10 step:9411 [D loss: 0.637547, acc: 69.53%] [G loss: 2.036849]\n",
      "epoch:10 step:9412 [D loss: 0.564400, acc: 71.09%] [G loss: 1.947834]\n",
      "epoch:10 step:9413 [D loss: 0.563809, acc: 75.00%] [G loss: 2.070326]\n",
      "epoch:10 step:9414 [D loss: 0.605337, acc: 64.06%] [G loss: 1.878699]\n",
      "epoch:10 step:9415 [D loss: 0.610889, acc: 66.41%] [G loss: 2.114783]\n",
      "epoch:10 step:9416 [D loss: 0.637777, acc: 64.84%] [G loss: 2.019163]\n",
      "epoch:10 step:9417 [D loss: 0.630531, acc: 67.97%] [G loss: 2.207920]\n",
      "epoch:10 step:9418 [D loss: 0.623904, acc: 64.06%] [G loss: 2.149839]\n",
      "epoch:10 step:9419 [D loss: 0.650019, acc: 64.06%] [G loss: 2.044493]\n",
      "epoch:10 step:9420 [D loss: 0.617804, acc: 62.50%] [G loss: 2.140776]\n",
      "epoch:10 step:9421 [D loss: 0.655013, acc: 62.50%] [G loss: 1.864191]\n",
      "epoch:10 step:9422 [D loss: 0.628259, acc: 67.19%] [G loss: 1.997978]\n",
      "epoch:10 step:9423 [D loss: 0.635550, acc: 64.84%] [G loss: 2.024036]\n",
      "epoch:10 step:9424 [D loss: 0.642993, acc: 64.06%] [G loss: 2.034647]\n",
      "epoch:10 step:9425 [D loss: 0.555666, acc: 69.53%] [G loss: 2.241876]\n",
      "epoch:10 step:9426 [D loss: 0.593895, acc: 65.62%] [G loss: 2.071723]\n",
      "epoch:10 step:9427 [D loss: 0.641813, acc: 64.06%] [G loss: 1.882255]\n",
      "epoch:10 step:9428 [D loss: 0.623486, acc: 64.06%] [G loss: 2.080591]\n",
      "epoch:10 step:9429 [D loss: 0.610095, acc: 65.62%] [G loss: 1.972416]\n",
      "epoch:10 step:9430 [D loss: 0.611801, acc: 62.50%] [G loss: 2.011412]\n",
      "epoch:10 step:9431 [D loss: 0.587718, acc: 67.97%] [G loss: 2.074027]\n",
      "epoch:10 step:9432 [D loss: 0.628163, acc: 64.06%] [G loss: 2.084729]\n",
      "epoch:10 step:9433 [D loss: 0.667057, acc: 57.81%] [G loss: 2.152511]\n",
      "epoch:10 step:9434 [D loss: 0.625215, acc: 66.41%] [G loss: 2.026517]\n",
      "epoch:10 step:9435 [D loss: 0.678565, acc: 60.94%] [G loss: 2.021019]\n",
      "epoch:10 step:9436 [D loss: 0.629012, acc: 62.50%] [G loss: 1.812331]\n",
      "epoch:10 step:9437 [D loss: 0.668101, acc: 58.59%] [G loss: 2.001222]\n",
      "epoch:10 step:9438 [D loss: 0.618918, acc: 66.41%] [G loss: 2.063206]\n",
      "epoch:10 step:9439 [D loss: 0.681899, acc: 60.16%] [G loss: 1.995930]\n",
      "epoch:10 step:9440 [D loss: 0.641970, acc: 66.41%] [G loss: 2.142757]\n",
      "epoch:10 step:9441 [D loss: 0.633338, acc: 62.50%] [G loss: 2.011481]\n",
      "epoch:10 step:9442 [D loss: 0.693566, acc: 57.81%] [G loss: 1.994960]\n",
      "epoch:10 step:9443 [D loss: 0.630253, acc: 66.41%] [G loss: 1.951012]\n",
      "epoch:10 step:9444 [D loss: 0.597377, acc: 71.88%] [G loss: 2.052762]\n",
      "epoch:10 step:9445 [D loss: 0.580795, acc: 69.53%] [G loss: 2.394978]\n",
      "epoch:10 step:9446 [D loss: 0.637252, acc: 64.06%] [G loss: 2.007277]\n",
      "epoch:10 step:9447 [D loss: 0.587088, acc: 68.75%] [G loss: 2.324351]\n",
      "epoch:10 step:9448 [D loss: 0.634211, acc: 63.28%] [G loss: 1.945311]\n",
      "epoch:10 step:9449 [D loss: 0.681340, acc: 58.59%] [G loss: 2.021705]\n",
      "epoch:10 step:9450 [D loss: 0.675123, acc: 60.16%] [G loss: 1.909669]\n",
      "epoch:10 step:9451 [D loss: 0.664265, acc: 57.03%] [G loss: 1.856872]\n",
      "epoch:10 step:9452 [D loss: 0.607692, acc: 61.72%] [G loss: 2.005545]\n",
      "epoch:10 step:9453 [D loss: 0.568805, acc: 71.09%] [G loss: 2.100986]\n",
      "epoch:10 step:9454 [D loss: 0.626719, acc: 62.50%] [G loss: 2.107425]\n",
      "epoch:10 step:9455 [D loss: 0.660760, acc: 62.50%] [G loss: 1.807898]\n",
      "epoch:10 step:9456 [D loss: 0.680674, acc: 61.72%] [G loss: 1.937484]\n",
      "epoch:10 step:9457 [D loss: 0.586601, acc: 66.41%] [G loss: 2.084968]\n",
      "epoch:10 step:9458 [D loss: 0.616754, acc: 67.19%] [G loss: 2.029269]\n",
      "epoch:10 step:9459 [D loss: 0.651994, acc: 63.28%] [G loss: 2.065103]\n",
      "epoch:10 step:9460 [D loss: 0.646570, acc: 66.41%] [G loss: 1.962154]\n",
      "epoch:10 step:9461 [D loss: 0.646865, acc: 64.84%] [G loss: 2.041148]\n",
      "epoch:10 step:9462 [D loss: 0.628680, acc: 69.53%] [G loss: 2.034942]\n",
      "epoch:10 step:9463 [D loss: 0.614396, acc: 72.66%] [G loss: 2.047067]\n",
      "epoch:10 step:9464 [D loss: 0.637970, acc: 57.03%] [G loss: 2.257224]\n",
      "epoch:10 step:9465 [D loss: 0.659151, acc: 62.50%] [G loss: 2.025918]\n",
      "epoch:10 step:9466 [D loss: 0.685906, acc: 64.06%] [G loss: 2.016730]\n",
      "epoch:10 step:9467 [D loss: 0.595183, acc: 64.84%] [G loss: 2.049565]\n",
      "epoch:10 step:9468 [D loss: 0.659981, acc: 55.47%] [G loss: 1.924811]\n",
      "epoch:10 step:9469 [D loss: 0.599060, acc: 64.84%] [G loss: 1.932322]\n",
      "epoch:10 step:9470 [D loss: 0.591598, acc: 67.19%] [G loss: 2.225342]\n",
      "epoch:10 step:9471 [D loss: 0.636408, acc: 64.06%] [G loss: 2.018229]\n",
      "epoch:10 step:9472 [D loss: 0.646469, acc: 63.28%] [G loss: 1.842825]\n",
      "epoch:10 step:9473 [D loss: 0.599323, acc: 70.31%] [G loss: 2.055387]\n",
      "epoch:10 step:9474 [D loss: 0.698081, acc: 60.16%] [G loss: 1.739655]\n",
      "epoch:10 step:9475 [D loss: 0.678634, acc: 62.50%] [G loss: 2.000512]\n",
      "epoch:10 step:9476 [D loss: 0.675693, acc: 58.59%] [G loss: 2.270707]\n",
      "epoch:10 step:9477 [D loss: 0.661238, acc: 64.84%] [G loss: 2.083251]\n",
      "epoch:10 step:9478 [D loss: 0.730986, acc: 51.56%] [G loss: 1.857703]\n",
      "epoch:10 step:9479 [D loss: 0.654439, acc: 60.16%] [G loss: 1.808848]\n",
      "epoch:10 step:9480 [D loss: 0.652399, acc: 61.72%] [G loss: 1.971350]\n",
      "epoch:10 step:9481 [D loss: 0.604616, acc: 65.62%] [G loss: 2.198523]\n",
      "epoch:10 step:9482 [D loss: 0.672357, acc: 55.47%] [G loss: 2.048884]\n",
      "epoch:10 step:9483 [D loss: 0.623264, acc: 60.16%] [G loss: 1.900427]\n",
      "epoch:10 step:9484 [D loss: 0.639044, acc: 66.41%] [G loss: 2.108247]\n",
      "epoch:10 step:9485 [D loss: 0.619241, acc: 65.62%] [G loss: 2.090043]\n",
      "epoch:10 step:9486 [D loss: 0.580274, acc: 69.53%] [G loss: 2.224726]\n",
      "epoch:10 step:9487 [D loss: 0.658523, acc: 60.94%] [G loss: 2.231506]\n",
      "epoch:10 step:9488 [D loss: 0.639607, acc: 64.06%] [G loss: 2.109984]\n",
      "epoch:10 step:9489 [D loss: 0.641108, acc: 65.62%] [G loss: 2.208501]\n",
      "epoch:10 step:9490 [D loss: 0.674460, acc: 56.25%] [G loss: 2.011412]\n",
      "epoch:10 step:9491 [D loss: 0.620475, acc: 63.28%] [G loss: 2.064271]\n",
      "epoch:10 step:9492 [D loss: 0.655769, acc: 63.28%] [G loss: 2.161417]\n",
      "epoch:10 step:9493 [D loss: 0.653331, acc: 60.16%] [G loss: 2.235006]\n",
      "epoch:10 step:9494 [D loss: 0.674473, acc: 60.16%] [G loss: 2.054135]\n",
      "epoch:10 step:9495 [D loss: 0.651424, acc: 57.81%] [G loss: 1.773449]\n",
      "epoch:10 step:9496 [D loss: 0.622622, acc: 62.50%] [G loss: 2.093114]\n",
      "epoch:10 step:9497 [D loss: 0.624914, acc: 63.28%] [G loss: 1.997226]\n",
      "epoch:10 step:9498 [D loss: 0.652368, acc: 61.72%] [G loss: 1.979307]\n",
      "epoch:10 step:9499 [D loss: 0.704759, acc: 60.16%] [G loss: 1.990334]\n",
      "epoch:10 step:9500 [D loss: 0.604432, acc: 60.94%] [G loss: 2.113037]\n",
      "epoch:10 step:9501 [D loss: 0.655933, acc: 60.16%] [G loss: 2.037215]\n",
      "epoch:10 step:9502 [D loss: 0.629101, acc: 63.28%] [G loss: 2.064302]\n",
      "epoch:10 step:9503 [D loss: 0.698356, acc: 57.81%] [G loss: 1.841940]\n",
      "epoch:10 step:9504 [D loss: 0.677134, acc: 64.06%] [G loss: 1.914932]\n",
      "epoch:10 step:9505 [D loss: 0.681123, acc: 57.03%] [G loss: 1.905304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9506 [D loss: 0.626662, acc: 64.84%] [G loss: 1.881351]\n",
      "epoch:10 step:9507 [D loss: 0.634918, acc: 67.97%] [G loss: 2.115129]\n",
      "epoch:10 step:9508 [D loss: 0.631247, acc: 67.97%] [G loss: 1.966172]\n",
      "epoch:10 step:9509 [D loss: 0.680105, acc: 60.94%] [G loss: 2.036182]\n",
      "epoch:10 step:9510 [D loss: 0.632212, acc: 65.62%] [G loss: 2.007077]\n",
      "epoch:10 step:9511 [D loss: 0.638130, acc: 63.28%] [G loss: 1.847383]\n",
      "epoch:10 step:9512 [D loss: 0.641377, acc: 61.72%] [G loss: 1.937217]\n",
      "epoch:10 step:9513 [D loss: 0.618576, acc: 67.97%] [G loss: 1.994948]\n",
      "epoch:10 step:9514 [D loss: 0.618926, acc: 67.19%] [G loss: 2.148012]\n",
      "epoch:10 step:9515 [D loss: 0.618431, acc: 66.41%] [G loss: 2.069200]\n",
      "epoch:10 step:9516 [D loss: 0.698257, acc: 62.50%] [G loss: 1.956345]\n",
      "epoch:10 step:9517 [D loss: 0.616389, acc: 67.97%] [G loss: 2.030781]\n",
      "epoch:10 step:9518 [D loss: 0.729015, acc: 55.47%] [G loss: 1.901695]\n",
      "epoch:10 step:9519 [D loss: 0.640547, acc: 63.28%] [G loss: 1.938783]\n",
      "epoch:10 step:9520 [D loss: 0.672576, acc: 55.47%] [G loss: 1.940648]\n",
      "epoch:10 step:9521 [D loss: 0.617924, acc: 66.41%] [G loss: 2.193593]\n",
      "epoch:10 step:9522 [D loss: 0.667059, acc: 60.94%] [G loss: 2.085529]\n",
      "epoch:10 step:9523 [D loss: 0.709334, acc: 53.91%] [G loss: 1.931312]\n",
      "epoch:10 step:9524 [D loss: 0.636091, acc: 59.38%] [G loss: 2.041847]\n",
      "epoch:10 step:9525 [D loss: 0.621183, acc: 65.62%] [G loss: 1.964102]\n",
      "epoch:10 step:9526 [D loss: 0.553452, acc: 75.00%] [G loss: 2.144849]\n",
      "epoch:10 step:9527 [D loss: 0.736244, acc: 48.44%] [G loss: 2.028272]\n",
      "epoch:10 step:9528 [D loss: 0.633758, acc: 63.28%] [G loss: 2.046402]\n",
      "epoch:10 step:9529 [D loss: 0.661176, acc: 64.06%] [G loss: 1.994746]\n",
      "epoch:10 step:9530 [D loss: 0.627248, acc: 67.19%] [G loss: 1.895474]\n",
      "epoch:10 step:9531 [D loss: 0.669706, acc: 62.50%] [G loss: 2.005653]\n",
      "epoch:10 step:9532 [D loss: 0.602603, acc: 68.75%] [G loss: 2.022523]\n",
      "epoch:10 step:9533 [D loss: 0.653816, acc: 60.94%] [G loss: 1.939759]\n",
      "epoch:10 step:9534 [D loss: 0.656114, acc: 67.97%] [G loss: 1.851837]\n",
      "epoch:10 step:9535 [D loss: 0.616010, acc: 64.84%] [G loss: 1.931469]\n",
      "epoch:10 step:9536 [D loss: 0.625862, acc: 64.84%] [G loss: 1.972827]\n",
      "epoch:10 step:9537 [D loss: 0.642175, acc: 64.84%] [G loss: 1.912265]\n",
      "epoch:10 step:9538 [D loss: 0.572998, acc: 73.44%] [G loss: 2.092533]\n",
      "epoch:10 step:9539 [D loss: 0.671811, acc: 59.38%] [G loss: 1.781718]\n",
      "epoch:10 step:9540 [D loss: 0.703608, acc: 57.81%] [G loss: 1.920008]\n",
      "epoch:10 step:9541 [D loss: 0.624395, acc: 62.50%] [G loss: 1.871915]\n",
      "epoch:10 step:9542 [D loss: 0.621445, acc: 62.50%] [G loss: 2.046881]\n",
      "epoch:10 step:9543 [D loss: 0.653088, acc: 69.53%] [G loss: 2.015619]\n",
      "epoch:10 step:9544 [D loss: 0.712814, acc: 55.47%] [G loss: 1.923722]\n",
      "epoch:10 step:9545 [D loss: 0.647247, acc: 60.16%] [G loss: 2.010969]\n",
      "epoch:10 step:9546 [D loss: 0.612685, acc: 67.97%] [G loss: 2.022753]\n",
      "epoch:10 step:9547 [D loss: 0.610190, acc: 68.75%] [G loss: 1.921977]\n",
      "epoch:10 step:9548 [D loss: 0.659305, acc: 60.94%] [G loss: 1.963201]\n",
      "epoch:10 step:9549 [D loss: 0.638330, acc: 63.28%] [G loss: 1.878251]\n",
      "epoch:10 step:9550 [D loss: 0.659810, acc: 56.25%] [G loss: 1.975487]\n",
      "epoch:10 step:9551 [D loss: 0.658740, acc: 57.81%] [G loss: 1.927221]\n",
      "epoch:10 step:9552 [D loss: 0.685265, acc: 57.81%] [G loss: 1.849999]\n",
      "epoch:10 step:9553 [D loss: 0.629561, acc: 63.28%] [G loss: 1.808615]\n",
      "epoch:10 step:9554 [D loss: 0.608752, acc: 68.75%] [G loss: 1.848605]\n",
      "epoch:10 step:9555 [D loss: 0.619277, acc: 62.50%] [G loss: 1.970444]\n",
      "epoch:10 step:9556 [D loss: 0.611169, acc: 63.28%] [G loss: 1.855803]\n",
      "epoch:10 step:9557 [D loss: 0.641358, acc: 61.72%] [G loss: 1.963553]\n",
      "epoch:10 step:9558 [D loss: 0.661531, acc: 59.38%] [G loss: 1.974380]\n",
      "epoch:10 step:9559 [D loss: 0.637719, acc: 64.84%] [G loss: 2.003230]\n",
      "epoch:10 step:9560 [D loss: 0.623083, acc: 67.97%] [G loss: 1.990858]\n",
      "epoch:10 step:9561 [D loss: 0.612846, acc: 65.62%] [G loss: 2.125588]\n",
      "epoch:10 step:9562 [D loss: 0.565096, acc: 73.44%] [G loss: 2.264096]\n",
      "epoch:10 step:9563 [D loss: 0.659132, acc: 64.06%] [G loss: 2.167553]\n",
      "epoch:10 step:9564 [D loss: 0.553291, acc: 71.88%] [G loss: 2.275821]\n",
      "epoch:10 step:9565 [D loss: 0.683327, acc: 60.16%] [G loss: 2.028603]\n",
      "epoch:10 step:9566 [D loss: 0.617695, acc: 62.50%] [G loss: 1.917936]\n",
      "epoch:10 step:9567 [D loss: 0.610721, acc: 71.88%] [G loss: 2.062266]\n",
      "epoch:10 step:9568 [D loss: 0.628448, acc: 61.72%] [G loss: 1.974401]\n",
      "epoch:10 step:9569 [D loss: 0.668512, acc: 62.50%] [G loss: 1.986349]\n",
      "epoch:10 step:9570 [D loss: 0.713632, acc: 58.59%] [G loss: 1.975828]\n",
      "epoch:10 step:9571 [D loss: 0.615899, acc: 64.06%] [G loss: 2.028830]\n",
      "epoch:10 step:9572 [D loss: 0.651807, acc: 65.62%] [G loss: 2.162078]\n",
      "epoch:10 step:9573 [D loss: 0.637424, acc: 62.50%] [G loss: 1.987156]\n",
      "epoch:10 step:9574 [D loss: 0.656562, acc: 57.81%] [G loss: 1.927393]\n",
      "epoch:10 step:9575 [D loss: 0.673832, acc: 62.50%] [G loss: 1.920722]\n",
      "epoch:10 step:9576 [D loss: 0.551076, acc: 77.34%] [G loss: 2.292454]\n",
      "epoch:10 step:9577 [D loss: 0.576659, acc: 69.53%] [G loss: 2.407012]\n",
      "epoch:10 step:9578 [D loss: 0.543971, acc: 75.78%] [G loss: 2.398924]\n",
      "epoch:10 step:9579 [D loss: 0.564669, acc: 73.44%] [G loss: 2.498313]\n",
      "epoch:10 step:9580 [D loss: 0.643610, acc: 57.03%] [G loss: 1.964152]\n",
      "epoch:10 step:9581 [D loss: 0.673213, acc: 55.47%] [G loss: 1.989555]\n",
      "epoch:10 step:9582 [D loss: 0.679024, acc: 60.94%] [G loss: 1.812894]\n",
      "epoch:10 step:9583 [D loss: 0.710497, acc: 60.16%] [G loss: 1.933314]\n",
      "epoch:10 step:9584 [D loss: 0.698126, acc: 56.25%] [G loss: 1.977302]\n",
      "epoch:10 step:9585 [D loss: 0.622190, acc: 66.41%] [G loss: 1.850041]\n",
      "epoch:10 step:9586 [D loss: 0.668823, acc: 57.81%] [G loss: 1.998763]\n",
      "epoch:10 step:9587 [D loss: 0.594897, acc: 73.44%] [G loss: 1.897200]\n",
      "epoch:10 step:9588 [D loss: 0.632380, acc: 64.84%] [G loss: 2.321033]\n",
      "epoch:10 step:9589 [D loss: 0.619440, acc: 64.06%] [G loss: 2.134008]\n",
      "epoch:10 step:9590 [D loss: 0.669718, acc: 58.59%] [G loss: 1.923752]\n",
      "epoch:10 step:9591 [D loss: 0.626701, acc: 67.19%] [G loss: 2.195822]\n",
      "epoch:10 step:9592 [D loss: 0.616485, acc: 63.28%] [G loss: 2.113145]\n",
      "epoch:10 step:9593 [D loss: 0.616845, acc: 61.72%] [G loss: 1.991213]\n",
      "epoch:10 step:9594 [D loss: 0.665306, acc: 62.50%] [G loss: 1.999548]\n",
      "epoch:10 step:9595 [D loss: 0.643902, acc: 57.81%] [G loss: 1.966063]\n",
      "epoch:10 step:9596 [D loss: 0.652722, acc: 57.81%] [G loss: 1.904184]\n",
      "epoch:10 step:9597 [D loss: 0.661288, acc: 62.50%] [G loss: 1.955945]\n",
      "epoch:10 step:9598 [D loss: 0.646088, acc: 60.94%] [G loss: 1.880655]\n",
      "epoch:10 step:9599 [D loss: 0.630691, acc: 67.19%] [G loss: 2.028426]\n",
      "epoch:10 step:9600 [D loss: 0.596884, acc: 71.88%] [G loss: 2.164505]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.676711\n",
      "FID: 23.616226\n",
      "0 = 12.880682275486004\n",
      "1 = 0.09267058443345591\n",
      "2 = 0.9004999995231628\n",
      "3 = 0.9028000235557556\n",
      "4 = 0.8981999754905701\n",
      "5 = 0.8986661434173584\n",
      "6 = 0.9028000235557556\n",
      "7 = 7.395032497060332\n",
      "8 = 0.09560467073849793\n",
      "9 = 0.7663000226020813\n",
      "10 = 0.7688000202178955\n",
      "11 = 0.7638000249862671\n",
      "12 = 0.7649751305580139\n",
      "13 = 0.7688000202178955\n",
      "14 = 6.676734924316406\n",
      "15 = 9.374455451965332\n",
      "16 = 0.14662744104862213\n",
      "17 = 6.676711082458496\n",
      "18 = 23.616226196289062\n",
      "epoch:10 step:9601 [D loss: 0.613509, acc: 70.31%] [G loss: 2.295346]\n",
      "epoch:10 step:9602 [D loss: 0.541578, acc: 73.44%] [G loss: 2.301222]\n",
      "epoch:10 step:9603 [D loss: 0.682783, acc: 57.81%] [G loss: 1.992318]\n",
      "epoch:10 step:9604 [D loss: 0.637762, acc: 63.28%] [G loss: 2.022209]\n",
      "epoch:10 step:9605 [D loss: 0.655148, acc: 64.84%] [G loss: 2.078425]\n",
      "epoch:10 step:9606 [D loss: 0.567100, acc: 71.09%] [G loss: 2.120971]\n",
      "epoch:10 step:9607 [D loss: 0.586388, acc: 68.75%] [G loss: 1.982352]\n",
      "epoch:10 step:9608 [D loss: 0.540103, acc: 77.34%] [G loss: 2.049552]\n",
      "epoch:10 step:9609 [D loss: 0.663570, acc: 58.59%] [G loss: 2.040127]\n",
      "epoch:10 step:9610 [D loss: 0.639419, acc: 64.06%] [G loss: 2.075454]\n",
      "epoch:10 step:9611 [D loss: 0.604810, acc: 67.19%] [G loss: 2.211493]\n",
      "epoch:10 step:9612 [D loss: 0.658778, acc: 64.06%] [G loss: 2.155290]\n",
      "epoch:10 step:9613 [D loss: 0.637234, acc: 66.41%] [G loss: 2.126252]\n",
      "epoch:10 step:9614 [D loss: 0.631094, acc: 64.06%] [G loss: 2.119486]\n",
      "epoch:10 step:9615 [D loss: 0.624373, acc: 67.19%] [G loss: 2.010854]\n",
      "epoch:10 step:9616 [D loss: 0.581165, acc: 67.97%] [G loss: 2.088963]\n",
      "epoch:10 step:9617 [D loss: 0.600284, acc: 64.06%] [G loss: 2.247537]\n",
      "epoch:10 step:9618 [D loss: 0.613998, acc: 62.50%] [G loss: 2.110652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9619 [D loss: 0.745373, acc: 57.81%] [G loss: 2.135347]\n",
      "epoch:10 step:9620 [D loss: 0.712797, acc: 50.78%] [G loss: 1.799765]\n",
      "epoch:10 step:9621 [D loss: 0.669990, acc: 63.28%] [G loss: 1.856906]\n",
      "epoch:10 step:9622 [D loss: 0.640505, acc: 63.28%] [G loss: 1.958616]\n",
      "epoch:10 step:9623 [D loss: 0.658380, acc: 60.16%] [G loss: 2.024840]\n",
      "epoch:10 step:9624 [D loss: 0.621959, acc: 66.41%] [G loss: 1.991023]\n",
      "epoch:10 step:9625 [D loss: 0.683971, acc: 60.94%] [G loss: 1.878156]\n",
      "epoch:10 step:9626 [D loss: 0.613092, acc: 67.97%] [G loss: 2.150366]\n",
      "epoch:10 step:9627 [D loss: 0.686584, acc: 61.72%] [G loss: 1.988940]\n",
      "epoch:10 step:9628 [D loss: 0.645689, acc: 64.84%] [G loss: 1.914173]\n",
      "epoch:10 step:9629 [D loss: 0.610664, acc: 71.09%] [G loss: 1.979840]\n",
      "epoch:10 step:9630 [D loss: 0.613923, acc: 71.09%] [G loss: 2.120496]\n",
      "epoch:10 step:9631 [D loss: 0.610138, acc: 70.31%] [G loss: 2.159404]\n",
      "epoch:10 step:9632 [D loss: 0.677855, acc: 55.47%] [G loss: 2.221917]\n",
      "epoch:10 step:9633 [D loss: 0.746665, acc: 50.00%] [G loss: 2.006195]\n",
      "epoch:10 step:9634 [D loss: 0.615352, acc: 65.62%] [G loss: 2.150280]\n",
      "epoch:10 step:9635 [D loss: 0.662427, acc: 53.12%] [G loss: 1.898524]\n",
      "epoch:10 step:9636 [D loss: 0.703122, acc: 57.03%] [G loss: 1.871036]\n",
      "epoch:10 step:9637 [D loss: 0.670800, acc: 58.59%] [G loss: 1.983026]\n",
      "epoch:10 step:9638 [D loss: 0.619313, acc: 62.50%] [G loss: 1.838698]\n",
      "epoch:10 step:9639 [D loss: 0.657018, acc: 63.28%] [G loss: 1.990642]\n",
      "epoch:10 step:9640 [D loss: 0.633539, acc: 63.28%] [G loss: 2.195681]\n",
      "epoch:10 step:9641 [D loss: 0.574839, acc: 70.31%] [G loss: 2.089698]\n",
      "epoch:10 step:9642 [D loss: 0.608415, acc: 71.88%] [G loss: 2.155003]\n",
      "epoch:10 step:9643 [D loss: 0.568867, acc: 71.09%] [G loss: 2.101759]\n",
      "epoch:10 step:9644 [D loss: 0.574973, acc: 70.31%] [G loss: 2.145536]\n",
      "epoch:10 step:9645 [D loss: 0.651903, acc: 64.06%] [G loss: 1.979291]\n",
      "epoch:10 step:9646 [D loss: 0.617443, acc: 64.84%] [G loss: 2.117625]\n",
      "epoch:10 step:9647 [D loss: 0.637911, acc: 57.81%] [G loss: 1.949266]\n",
      "epoch:10 step:9648 [D loss: 0.666670, acc: 65.62%] [G loss: 1.975371]\n",
      "epoch:10 step:9649 [D loss: 0.640917, acc: 66.41%] [G loss: 1.814291]\n",
      "epoch:10 step:9650 [D loss: 0.676878, acc: 60.94%] [G loss: 2.110484]\n",
      "epoch:10 step:9651 [D loss: 0.705966, acc: 55.47%] [G loss: 1.966935]\n",
      "epoch:10 step:9652 [D loss: 0.665651, acc: 58.59%] [G loss: 1.929266]\n",
      "epoch:10 step:9653 [D loss: 0.625023, acc: 60.94%] [G loss: 2.017508]\n",
      "epoch:10 step:9654 [D loss: 0.605686, acc: 67.97%] [G loss: 1.947001]\n",
      "epoch:10 step:9655 [D loss: 0.647542, acc: 59.38%] [G loss: 2.052179]\n",
      "epoch:10 step:9656 [D loss: 0.633785, acc: 62.50%] [G loss: 1.993240]\n",
      "epoch:10 step:9657 [D loss: 0.632780, acc: 64.84%] [G loss: 2.087325]\n",
      "epoch:10 step:9658 [D loss: 0.684222, acc: 57.03%] [G loss: 1.890990]\n",
      "epoch:10 step:9659 [D loss: 0.614640, acc: 64.84%] [G loss: 2.130860]\n",
      "epoch:10 step:9660 [D loss: 0.711702, acc: 63.28%] [G loss: 1.978144]\n",
      "epoch:10 step:9661 [D loss: 0.657287, acc: 59.38%] [G loss: 2.047107]\n",
      "epoch:10 step:9662 [D loss: 0.670340, acc: 63.28%] [G loss: 1.972631]\n",
      "epoch:10 step:9663 [D loss: 0.613053, acc: 60.94%] [G loss: 1.992307]\n",
      "epoch:10 step:9664 [D loss: 0.610690, acc: 68.75%] [G loss: 1.878119]\n",
      "epoch:10 step:9665 [D loss: 0.629176, acc: 67.19%] [G loss: 2.058876]\n",
      "epoch:10 step:9666 [D loss: 0.567113, acc: 69.53%] [G loss: 2.076406]\n",
      "epoch:10 step:9667 [D loss: 0.617938, acc: 64.84%] [G loss: 1.959167]\n",
      "epoch:10 step:9668 [D loss: 0.588490, acc: 75.00%] [G loss: 2.082387]\n",
      "epoch:10 step:9669 [D loss: 0.577342, acc: 68.75%] [G loss: 2.164490]\n",
      "epoch:10 step:9670 [D loss: 0.623807, acc: 67.19%] [G loss: 2.106322]\n",
      "epoch:10 step:9671 [D loss: 0.630950, acc: 64.06%] [G loss: 1.982719]\n",
      "epoch:10 step:9672 [D loss: 0.622262, acc: 65.62%] [G loss: 1.855691]\n",
      "epoch:10 step:9673 [D loss: 0.614834, acc: 66.41%] [G loss: 2.084404]\n",
      "epoch:10 step:9674 [D loss: 0.613381, acc: 64.84%] [G loss: 1.968309]\n",
      "epoch:10 step:9675 [D loss: 0.654798, acc: 55.47%] [G loss: 1.910914]\n",
      "epoch:10 step:9676 [D loss: 0.601208, acc: 69.53%] [G loss: 2.099962]\n",
      "epoch:10 step:9677 [D loss: 0.649659, acc: 60.16%] [G loss: 1.924791]\n",
      "epoch:10 step:9678 [D loss: 0.623563, acc: 65.62%] [G loss: 1.953973]\n",
      "epoch:10 step:9679 [D loss: 0.634091, acc: 60.16%] [G loss: 2.047751]\n",
      "epoch:10 step:9680 [D loss: 0.631061, acc: 64.84%] [G loss: 2.057352]\n",
      "epoch:10 step:9681 [D loss: 0.668462, acc: 64.84%] [G loss: 2.030547]\n",
      "epoch:10 step:9682 [D loss: 0.600912, acc: 66.41%] [G loss: 2.474047]\n",
      "epoch:10 step:9683 [D loss: 0.647181, acc: 67.19%] [G loss: 2.258880]\n",
      "epoch:10 step:9684 [D loss: 0.560109, acc: 72.66%] [G loss: 2.301589]\n",
      "epoch:10 step:9685 [D loss: 0.604604, acc: 63.28%] [G loss: 2.235937]\n",
      "epoch:10 step:9686 [D loss: 0.704630, acc: 53.12%] [G loss: 1.769795]\n",
      "epoch:10 step:9687 [D loss: 0.707308, acc: 54.69%] [G loss: 1.869284]\n",
      "epoch:10 step:9688 [D loss: 0.613468, acc: 64.06%] [G loss: 1.907037]\n",
      "epoch:10 step:9689 [D loss: 0.749198, acc: 53.12%] [G loss: 2.018711]\n",
      "epoch:10 step:9690 [D loss: 0.680490, acc: 56.25%] [G loss: 1.982014]\n",
      "epoch:10 step:9691 [D loss: 0.594701, acc: 66.41%] [G loss: 2.077421]\n",
      "epoch:10 step:9692 [D loss: 0.634464, acc: 66.41%] [G loss: 2.037959]\n",
      "epoch:10 step:9693 [D loss: 0.634972, acc: 67.19%] [G loss: 1.879635]\n",
      "epoch:10 step:9694 [D loss: 0.651852, acc: 57.03%] [G loss: 1.892007]\n",
      "epoch:10 step:9695 [D loss: 0.616766, acc: 64.84%] [G loss: 1.918842]\n",
      "epoch:10 step:9696 [D loss: 0.579455, acc: 75.00%] [G loss: 2.022221]\n",
      "epoch:10 step:9697 [D loss: 0.646550, acc: 65.62%] [G loss: 1.981205]\n",
      "epoch:10 step:9698 [D loss: 0.603585, acc: 68.75%] [G loss: 2.066294]\n",
      "epoch:10 step:9699 [D loss: 0.630869, acc: 62.50%] [G loss: 2.127144]\n",
      "epoch:10 step:9700 [D loss: 0.604440, acc: 64.84%] [G loss: 2.089053]\n",
      "epoch:10 step:9701 [D loss: 0.602408, acc: 66.41%] [G loss: 2.047936]\n",
      "epoch:10 step:9702 [D loss: 0.587827, acc: 71.09%] [G loss: 1.980413]\n",
      "epoch:10 step:9703 [D loss: 0.637125, acc: 64.84%] [G loss: 1.993794]\n",
      "epoch:10 step:9704 [D loss: 0.640186, acc: 62.50%] [G loss: 2.071128]\n",
      "epoch:10 step:9705 [D loss: 0.605886, acc: 64.06%] [G loss: 2.041829]\n",
      "epoch:10 step:9706 [D loss: 0.616368, acc: 63.28%] [G loss: 2.011975]\n",
      "epoch:10 step:9707 [D loss: 0.647660, acc: 64.84%] [G loss: 2.098653]\n",
      "epoch:10 step:9708 [D loss: 0.575080, acc: 71.09%] [G loss: 2.210968]\n",
      "epoch:10 step:9709 [D loss: 0.593794, acc: 72.66%] [G loss: 2.171546]\n",
      "epoch:10 step:9710 [D loss: 0.686751, acc: 58.59%] [G loss: 2.034532]\n",
      "epoch:10 step:9711 [D loss: 0.664391, acc: 60.94%] [G loss: 1.953693]\n",
      "epoch:10 step:9712 [D loss: 0.663235, acc: 54.69%] [G loss: 1.859230]\n",
      "epoch:10 step:9713 [D loss: 0.633037, acc: 61.72%] [G loss: 1.984651]\n",
      "epoch:10 step:9714 [D loss: 0.654238, acc: 62.50%] [G loss: 1.885460]\n",
      "epoch:10 step:9715 [D loss: 0.581306, acc: 71.09%] [G loss: 2.279364]\n",
      "epoch:10 step:9716 [D loss: 0.607427, acc: 65.62%] [G loss: 2.441677]\n",
      "epoch:10 step:9717 [D loss: 0.601568, acc: 68.75%] [G loss: 2.467574]\n",
      "epoch:10 step:9718 [D loss: 0.629331, acc: 66.41%] [G loss: 1.973092]\n",
      "epoch:10 step:9719 [D loss: 0.739828, acc: 58.59%] [G loss: 1.832001]\n",
      "epoch:10 step:9720 [D loss: 0.651841, acc: 61.72%] [G loss: 1.906248]\n",
      "epoch:10 step:9721 [D loss: 0.658492, acc: 63.28%] [G loss: 1.860435]\n",
      "epoch:10 step:9722 [D loss: 0.658701, acc: 58.59%] [G loss: 1.936022]\n",
      "epoch:10 step:9723 [D loss: 0.616536, acc: 67.19%] [G loss: 2.090094]\n",
      "epoch:10 step:9724 [D loss: 0.607275, acc: 69.53%] [G loss: 2.289754]\n",
      "epoch:10 step:9725 [D loss: 0.661679, acc: 58.59%] [G loss: 1.923736]\n",
      "epoch:10 step:9726 [D loss: 0.686990, acc: 60.16%] [G loss: 1.880888]\n",
      "epoch:10 step:9727 [D loss: 0.588600, acc: 68.75%] [G loss: 2.215040]\n",
      "epoch:10 step:9728 [D loss: 0.567750, acc: 66.41%] [G loss: 2.133617]\n",
      "epoch:10 step:9729 [D loss: 0.716165, acc: 56.25%] [G loss: 2.146161]\n",
      "epoch:10 step:9730 [D loss: 0.628158, acc: 65.62%] [G loss: 1.873090]\n",
      "epoch:10 step:9731 [D loss: 0.647846, acc: 62.50%] [G loss: 1.888355]\n",
      "epoch:10 step:9732 [D loss: 0.652541, acc: 62.50%] [G loss: 1.897487]\n",
      "epoch:10 step:9733 [D loss: 0.625081, acc: 67.19%] [G loss: 2.056509]\n",
      "epoch:10 step:9734 [D loss: 0.561333, acc: 67.19%] [G loss: 2.138103]\n",
      "epoch:10 step:9735 [D loss: 0.642085, acc: 65.62%] [G loss: 2.000943]\n",
      "epoch:10 step:9736 [D loss: 0.581212, acc: 71.88%] [G loss: 2.190540]\n",
      "epoch:10 step:9737 [D loss: 0.622321, acc: 64.84%] [G loss: 1.953454]\n",
      "epoch:10 step:9738 [D loss: 0.603795, acc: 68.75%] [G loss: 2.054548]\n",
      "epoch:10 step:9739 [D loss: 0.628111, acc: 61.72%] [G loss: 2.019413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9740 [D loss: 0.666855, acc: 60.94%] [G loss: 2.144843]\n",
      "epoch:10 step:9741 [D loss: 0.615370, acc: 63.28%] [G loss: 2.128566]\n",
      "epoch:10 step:9742 [D loss: 0.622430, acc: 64.84%] [G loss: 2.163268]\n",
      "epoch:10 step:9743 [D loss: 0.637595, acc: 63.28%] [G loss: 1.868094]\n",
      "epoch:10 step:9744 [D loss: 0.626701, acc: 67.19%] [G loss: 2.142229]\n",
      "epoch:10 step:9745 [D loss: 0.651402, acc: 60.94%] [G loss: 1.946901]\n",
      "epoch:10 step:9746 [D loss: 0.629276, acc: 60.94%] [G loss: 1.927376]\n",
      "epoch:10 step:9747 [D loss: 0.750130, acc: 50.78%] [G loss: 1.856024]\n",
      "epoch:10 step:9748 [D loss: 0.620557, acc: 67.19%] [G loss: 1.969657]\n",
      "epoch:10 step:9749 [D loss: 0.613084, acc: 67.97%] [G loss: 2.016181]\n",
      "epoch:10 step:9750 [D loss: 0.623646, acc: 60.94%] [G loss: 2.109742]\n",
      "epoch:10 step:9751 [D loss: 0.702205, acc: 64.84%] [G loss: 2.129091]\n",
      "epoch:10 step:9752 [D loss: 0.592480, acc: 62.50%] [G loss: 2.095577]\n",
      "epoch:10 step:9753 [D loss: 0.648764, acc: 59.38%] [G loss: 2.141127]\n",
      "epoch:10 step:9754 [D loss: 0.630292, acc: 64.84%] [G loss: 2.197909]\n",
      "epoch:10 step:9755 [D loss: 0.599753, acc: 63.28%] [G loss: 2.071708]\n",
      "epoch:10 step:9756 [D loss: 0.672551, acc: 60.16%] [G loss: 1.853643]\n",
      "epoch:10 step:9757 [D loss: 0.662111, acc: 60.16%] [G loss: 2.016099]\n",
      "epoch:10 step:9758 [D loss: 0.622018, acc: 59.38%] [G loss: 2.045176]\n",
      "epoch:10 step:9759 [D loss: 0.632580, acc: 62.50%] [G loss: 2.041034]\n",
      "epoch:10 step:9760 [D loss: 0.627861, acc: 64.84%] [G loss: 2.056606]\n",
      "epoch:10 step:9761 [D loss: 0.644052, acc: 67.97%] [G loss: 1.921362]\n",
      "epoch:10 step:9762 [D loss: 0.622457, acc: 67.19%] [G loss: 2.000737]\n",
      "epoch:10 step:9763 [D loss: 0.633250, acc: 67.97%] [G loss: 1.948644]\n",
      "epoch:10 step:9764 [D loss: 0.619103, acc: 69.53%] [G loss: 2.040411]\n",
      "epoch:10 step:9765 [D loss: 0.681243, acc: 57.03%] [G loss: 2.038554]\n",
      "epoch:10 step:9766 [D loss: 0.639794, acc: 64.06%] [G loss: 1.791119]\n",
      "epoch:10 step:9767 [D loss: 0.682129, acc: 58.59%] [G loss: 2.079533]\n",
      "epoch:10 step:9768 [D loss: 0.661703, acc: 62.50%] [G loss: 1.887467]\n",
      "epoch:10 step:9769 [D loss: 0.609819, acc: 70.31%] [G loss: 2.213973]\n",
      "epoch:10 step:9770 [D loss: 0.679310, acc: 54.69%] [G loss: 2.111265]\n",
      "epoch:10 step:9771 [D loss: 0.622370, acc: 62.50%] [G loss: 1.972732]\n",
      "epoch:10 step:9772 [D loss: 0.576391, acc: 74.22%] [G loss: 2.033387]\n",
      "epoch:10 step:9773 [D loss: 0.675875, acc: 53.91%] [G loss: 1.967138]\n",
      "epoch:10 step:9774 [D loss: 0.655134, acc: 60.16%] [G loss: 1.914875]\n",
      "epoch:10 step:9775 [D loss: 0.600735, acc: 68.75%] [G loss: 2.307551]\n",
      "epoch:10 step:9776 [D loss: 0.635241, acc: 63.28%] [G loss: 2.251809]\n",
      "epoch:10 step:9777 [D loss: 0.586886, acc: 67.97%] [G loss: 2.185705]\n",
      "epoch:10 step:9778 [D loss: 0.602256, acc: 68.75%] [G loss: 2.013159]\n",
      "epoch:10 step:9779 [D loss: 0.587923, acc: 68.75%] [G loss: 2.002827]\n",
      "epoch:10 step:9780 [D loss: 0.643561, acc: 63.28%] [G loss: 2.070007]\n",
      "epoch:10 step:9781 [D loss: 0.662305, acc: 58.59%] [G loss: 2.033208]\n",
      "epoch:10 step:9782 [D loss: 0.640302, acc: 71.09%] [G loss: 2.035202]\n",
      "epoch:10 step:9783 [D loss: 0.584116, acc: 68.75%] [G loss: 2.217872]\n",
      "epoch:10 step:9784 [D loss: 0.636512, acc: 65.62%] [G loss: 2.008477]\n",
      "epoch:10 step:9785 [D loss: 0.692424, acc: 60.16%] [G loss: 2.116498]\n",
      "epoch:10 step:9786 [D loss: 0.588617, acc: 70.31%] [G loss: 2.047360]\n",
      "epoch:10 step:9787 [D loss: 0.639593, acc: 65.62%] [G loss: 2.160060]\n",
      "epoch:10 step:9788 [D loss: 0.638126, acc: 66.41%] [G loss: 1.908804]\n",
      "epoch:10 step:9789 [D loss: 0.639921, acc: 61.72%] [G loss: 2.067734]\n",
      "epoch:10 step:9790 [D loss: 0.680862, acc: 57.03%] [G loss: 1.998615]\n",
      "epoch:10 step:9791 [D loss: 0.600006, acc: 71.88%] [G loss: 2.054901]\n",
      "epoch:10 step:9792 [D loss: 0.665027, acc: 57.81%] [G loss: 1.862788]\n",
      "epoch:10 step:9793 [D loss: 0.671280, acc: 60.94%] [G loss: 1.956970]\n",
      "epoch:10 step:9794 [D loss: 0.685517, acc: 58.59%] [G loss: 1.932587]\n",
      "epoch:10 step:9795 [D loss: 0.695919, acc: 59.38%] [G loss: 1.874178]\n",
      "epoch:10 step:9796 [D loss: 0.630050, acc: 63.28%] [G loss: 2.125813]\n",
      "epoch:10 step:9797 [D loss: 0.649254, acc: 62.50%] [G loss: 2.144055]\n",
      "epoch:10 step:9798 [D loss: 0.593891, acc: 67.19%] [G loss: 2.122327]\n",
      "epoch:10 step:9799 [D loss: 0.568288, acc: 67.19%] [G loss: 2.354854]\n",
      "epoch:10 step:9800 [D loss: 0.539425, acc: 71.88%] [G loss: 2.195675]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.667288\n",
      "FID: 24.473522\n",
      "0 = 12.878325396347027\n",
      "1 = 0.08505279135813273\n",
      "2 = 0.9000999927520752\n",
      "3 = 0.9115999937057495\n",
      "4 = 0.8885999917984009\n",
      "5 = 0.8911045789718628\n",
      "6 = 0.9115999937057495\n",
      "7 = 7.468413484251516\n",
      "8 = 0.09827137811081281\n",
      "9 = 0.7594000101089478\n",
      "10 = 0.767799973487854\n",
      "11 = 0.7509999871253967\n",
      "12 = 0.7551140785217285\n",
      "13 = 0.767799973487854\n",
      "14 = 6.667313575744629\n",
      "15 = 9.286857604980469\n",
      "16 = 0.1606239229440689\n",
      "17 = 6.667288303375244\n",
      "18 = 24.473522186279297\n",
      "epoch:10 step:9801 [D loss: 0.594146, acc: 69.53%] [G loss: 2.191260]\n",
      "epoch:10 step:9802 [D loss: 0.682492, acc: 54.69%] [G loss: 1.989109]\n",
      "epoch:10 step:9803 [D loss: 0.649316, acc: 58.59%] [G loss: 2.113514]\n",
      "epoch:10 step:9804 [D loss: 0.631560, acc: 65.62%] [G loss: 2.049907]\n",
      "epoch:10 step:9805 [D loss: 0.662633, acc: 61.72%] [G loss: 2.005944]\n",
      "epoch:10 step:9806 [D loss: 0.637931, acc: 57.81%] [G loss: 2.084221]\n",
      "epoch:10 step:9807 [D loss: 0.673200, acc: 60.94%] [G loss: 1.877066]\n",
      "epoch:10 step:9808 [D loss: 0.663278, acc: 57.81%] [G loss: 1.843357]\n",
      "epoch:10 step:9809 [D loss: 0.643188, acc: 60.94%] [G loss: 2.029132]\n",
      "epoch:10 step:9810 [D loss: 0.643344, acc: 66.41%] [G loss: 2.034880]\n",
      "epoch:10 step:9811 [D loss: 0.610968, acc: 68.75%] [G loss: 2.018372]\n",
      "epoch:10 step:9812 [D loss: 0.666555, acc: 64.84%] [G loss: 1.978866]\n",
      "epoch:10 step:9813 [D loss: 0.664180, acc: 62.50%] [G loss: 1.953885]\n",
      "epoch:10 step:9814 [D loss: 0.649914, acc: 58.59%] [G loss: 2.021034]\n",
      "epoch:10 step:9815 [D loss: 0.631275, acc: 60.94%] [G loss: 1.880499]\n",
      "epoch:10 step:9816 [D loss: 0.602403, acc: 71.88%] [G loss: 1.981678]\n",
      "epoch:10 step:9817 [D loss: 0.610024, acc: 64.06%] [G loss: 1.991173]\n",
      "epoch:10 step:9818 [D loss: 0.639802, acc: 60.94%] [G loss: 1.880772]\n",
      "epoch:10 step:9819 [D loss: 0.710763, acc: 58.59%] [G loss: 1.901258]\n",
      "epoch:10 step:9820 [D loss: 0.629927, acc: 59.38%] [G loss: 1.859411]\n",
      "epoch:10 step:9821 [D loss: 0.648002, acc: 67.19%] [G loss: 2.126290]\n",
      "epoch:10 step:9822 [D loss: 0.633053, acc: 63.28%] [G loss: 1.944382]\n",
      "epoch:10 step:9823 [D loss: 0.578947, acc: 69.53%] [G loss: 2.237738]\n",
      "epoch:10 step:9824 [D loss: 0.620842, acc: 62.50%] [G loss: 1.889085]\n",
      "epoch:10 step:9825 [D loss: 0.676285, acc: 62.50%] [G loss: 2.087229]\n",
      "epoch:10 step:9826 [D loss: 0.646179, acc: 62.50%] [G loss: 2.158809]\n",
      "epoch:10 step:9827 [D loss: 0.609728, acc: 63.28%] [G loss: 1.878330]\n",
      "epoch:10 step:9828 [D loss: 0.694785, acc: 57.03%] [G loss: 1.893384]\n",
      "epoch:10 step:9829 [D loss: 0.655475, acc: 67.97%] [G loss: 1.858085]\n",
      "epoch:10 step:9830 [D loss: 0.676752, acc: 61.72%] [G loss: 1.768468]\n",
      "epoch:10 step:9831 [D loss: 0.662758, acc: 58.59%] [G loss: 1.854611]\n",
      "epoch:10 step:9832 [D loss: 0.603896, acc: 65.62%] [G loss: 1.873623]\n",
      "epoch:10 step:9833 [D loss: 0.648736, acc: 63.28%] [G loss: 1.883003]\n",
      "epoch:10 step:9834 [D loss: 0.669102, acc: 61.72%] [G loss: 1.925578]\n",
      "epoch:10 step:9835 [D loss: 0.654110, acc: 59.38%] [G loss: 1.944729]\n",
      "epoch:10 step:9836 [D loss: 0.664195, acc: 60.94%] [G loss: 1.978448]\n",
      "epoch:10 step:9837 [D loss: 0.673062, acc: 58.59%] [G loss: 1.931700]\n",
      "epoch:10 step:9838 [D loss: 0.627336, acc: 61.72%] [G loss: 2.008626]\n",
      "epoch:10 step:9839 [D loss: 0.665815, acc: 67.19%] [G loss: 2.056150]\n",
      "epoch:10 step:9840 [D loss: 0.550701, acc: 72.66%] [G loss: 2.173627]\n",
      "epoch:10 step:9841 [D loss: 0.528304, acc: 71.88%] [G loss: 2.437537]\n",
      "epoch:10 step:9842 [D loss: 0.587821, acc: 71.88%] [G loss: 2.249592]\n",
      "epoch:10 step:9843 [D loss: 0.685423, acc: 55.47%] [G loss: 2.000137]\n",
      "epoch:10 step:9844 [D loss: 0.612735, acc: 67.97%] [G loss: 1.956352]\n",
      "epoch:10 step:9845 [D loss: 0.644151, acc: 64.84%] [G loss: 2.209123]\n",
      "epoch:10 step:9846 [D loss: 0.656549, acc: 62.50%] [G loss: 1.925387]\n",
      "epoch:10 step:9847 [D loss: 0.715956, acc: 52.34%] [G loss: 1.870999]\n",
      "epoch:10 step:9848 [D loss: 0.677290, acc: 60.16%] [G loss: 1.895376]\n",
      "epoch:10 step:9849 [D loss: 0.660299, acc: 58.59%] [G loss: 2.001797]\n",
      "epoch:10 step:9850 [D loss: 0.614184, acc: 65.62%] [G loss: 2.099498]\n",
      "epoch:10 step:9851 [D loss: 0.625228, acc: 66.41%] [G loss: 2.092640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9852 [D loss: 0.717738, acc: 58.59%] [G loss: 1.861005]\n",
      "epoch:10 step:9853 [D loss: 0.640983, acc: 60.94%] [G loss: 1.848117]\n",
      "epoch:10 step:9854 [D loss: 0.622485, acc: 67.19%] [G loss: 1.988719]\n",
      "epoch:10 step:9855 [D loss: 0.650767, acc: 60.94%] [G loss: 1.840349]\n",
      "epoch:10 step:9856 [D loss: 0.729223, acc: 54.69%] [G loss: 1.886532]\n",
      "epoch:10 step:9857 [D loss: 0.631561, acc: 64.06%] [G loss: 1.929995]\n",
      "epoch:10 step:9858 [D loss: 0.630921, acc: 67.19%] [G loss: 2.050712]\n",
      "epoch:10 step:9859 [D loss: 0.658680, acc: 57.81%] [G loss: 1.819125]\n",
      "epoch:10 step:9860 [D loss: 0.677136, acc: 54.69%] [G loss: 1.887172]\n",
      "epoch:10 step:9861 [D loss: 0.631849, acc: 60.16%] [G loss: 1.933437]\n",
      "epoch:10 step:9862 [D loss: 0.586443, acc: 71.09%] [G loss: 1.995041]\n",
      "epoch:10 step:9863 [D loss: 0.675947, acc: 55.47%] [G loss: 1.863329]\n",
      "epoch:10 step:9864 [D loss: 0.602977, acc: 64.84%] [G loss: 2.012673]\n",
      "epoch:10 step:9865 [D loss: 0.600598, acc: 69.53%] [G loss: 2.272948]\n",
      "epoch:10 step:9866 [D loss: 0.681421, acc: 64.84%] [G loss: 2.050168]\n",
      "epoch:10 step:9867 [D loss: 0.611605, acc: 73.44%] [G loss: 2.190560]\n",
      "epoch:10 step:9868 [D loss: 0.614687, acc: 64.06%] [G loss: 2.120198]\n",
      "epoch:10 step:9869 [D loss: 0.588945, acc: 67.19%] [G loss: 2.238015]\n",
      "epoch:10 step:9870 [D loss: 0.657073, acc: 60.94%] [G loss: 1.862122]\n",
      "epoch:10 step:9871 [D loss: 0.678475, acc: 57.03%] [G loss: 1.736334]\n",
      "epoch:10 step:9872 [D loss: 0.686425, acc: 56.25%] [G loss: 1.823533]\n",
      "epoch:10 step:9873 [D loss: 0.643280, acc: 60.94%] [G loss: 2.105949]\n",
      "epoch:10 step:9874 [D loss: 0.604696, acc: 67.97%] [G loss: 2.218156]\n",
      "epoch:10 step:9875 [D loss: 0.628969, acc: 64.84%] [G loss: 1.920932]\n",
      "epoch:10 step:9876 [D loss: 0.686315, acc: 60.94%] [G loss: 1.884618]\n",
      "epoch:10 step:9877 [D loss: 0.658462, acc: 58.59%] [G loss: 1.911502]\n",
      "epoch:10 step:9878 [D loss: 0.585657, acc: 68.75%] [G loss: 1.986924]\n",
      "epoch:10 step:9879 [D loss: 0.604822, acc: 69.53%] [G loss: 2.105676]\n",
      "epoch:10 step:9880 [D loss: 0.666091, acc: 56.25%] [G loss: 1.916080]\n",
      "epoch:10 step:9881 [D loss: 0.691150, acc: 55.47%] [G loss: 1.901283]\n",
      "epoch:10 step:9882 [D loss: 0.636541, acc: 57.81%] [G loss: 2.075435]\n",
      "epoch:10 step:9883 [D loss: 0.610062, acc: 65.62%] [G loss: 1.828107]\n",
      "epoch:10 step:9884 [D loss: 0.616636, acc: 60.94%] [G loss: 1.983676]\n",
      "epoch:10 step:9885 [D loss: 0.664970, acc: 60.16%] [G loss: 2.057452]\n",
      "epoch:10 step:9886 [D loss: 0.597240, acc: 68.75%] [G loss: 2.082365]\n",
      "epoch:10 step:9887 [D loss: 0.599655, acc: 67.19%] [G loss: 2.042730]\n",
      "epoch:10 step:9888 [D loss: 0.665858, acc: 58.59%] [G loss: 2.048065]\n",
      "epoch:10 step:9889 [D loss: 0.638677, acc: 65.62%] [G loss: 2.099584]\n",
      "epoch:10 step:9890 [D loss: 0.587455, acc: 65.62%] [G loss: 2.051182]\n",
      "epoch:10 step:9891 [D loss: 0.618385, acc: 67.97%] [G loss: 2.236674]\n",
      "epoch:10 step:9892 [D loss: 0.617869, acc: 60.94%] [G loss: 2.184811]\n",
      "epoch:10 step:9893 [D loss: 0.634615, acc: 60.16%] [G loss: 2.093684]\n",
      "epoch:10 step:9894 [D loss: 0.602157, acc: 64.84%] [G loss: 2.129483]\n",
      "epoch:10 step:9895 [D loss: 0.651689, acc: 67.97%] [G loss: 1.979359]\n",
      "epoch:10 step:9896 [D loss: 0.663439, acc: 58.59%] [G loss: 1.794349]\n",
      "epoch:10 step:9897 [D loss: 0.675876, acc: 60.16%] [G loss: 2.005205]\n",
      "epoch:10 step:9898 [D loss: 0.664039, acc: 56.25%] [G loss: 2.010985]\n",
      "epoch:10 step:9899 [D loss: 0.636135, acc: 66.41%] [G loss: 1.920403]\n",
      "epoch:10 step:9900 [D loss: 0.605359, acc: 67.97%] [G loss: 1.993479]\n",
      "epoch:10 step:9901 [D loss: 0.573575, acc: 73.44%] [G loss: 1.985181]\n",
      "epoch:10 step:9902 [D loss: 0.617137, acc: 63.28%] [G loss: 2.020446]\n",
      "epoch:10 step:9903 [D loss: 0.563779, acc: 68.75%] [G loss: 2.015331]\n",
      "epoch:10 step:9904 [D loss: 0.575940, acc: 64.84%] [G loss: 2.147794]\n",
      "epoch:10 step:9905 [D loss: 0.632300, acc: 65.62%] [G loss: 2.002545]\n",
      "epoch:10 step:9906 [D loss: 0.595760, acc: 71.88%] [G loss: 2.152193]\n",
      "epoch:10 step:9907 [D loss: 0.637192, acc: 60.94%] [G loss: 1.955653]\n",
      "epoch:10 step:9908 [D loss: 0.638667, acc: 65.62%] [G loss: 1.940734]\n",
      "epoch:10 step:9909 [D loss: 0.684194, acc: 62.50%] [G loss: 1.957156]\n",
      "epoch:10 step:9910 [D loss: 0.687886, acc: 55.47%] [G loss: 1.964906]\n",
      "epoch:10 step:9911 [D loss: 0.662180, acc: 60.16%] [G loss: 2.026073]\n",
      "epoch:10 step:9912 [D loss: 0.657128, acc: 58.59%] [G loss: 1.943600]\n",
      "epoch:10 step:9913 [D loss: 0.698002, acc: 58.59%] [G loss: 2.043645]\n",
      "epoch:10 step:9914 [D loss: 0.602564, acc: 69.53%] [G loss: 2.063077]\n",
      "epoch:10 step:9915 [D loss: 0.629423, acc: 66.41%] [G loss: 1.940527]\n",
      "epoch:10 step:9916 [D loss: 0.632534, acc: 60.16%] [G loss: 1.965445]\n",
      "epoch:10 step:9917 [D loss: 0.596149, acc: 66.41%] [G loss: 1.842434]\n",
      "epoch:10 step:9918 [D loss: 0.610477, acc: 60.16%] [G loss: 2.068848]\n",
      "epoch:10 step:9919 [D loss: 0.573021, acc: 67.97%] [G loss: 2.208737]\n",
      "epoch:10 step:9920 [D loss: 0.643501, acc: 64.84%] [G loss: 2.189450]\n",
      "epoch:10 step:9921 [D loss: 0.613089, acc: 64.06%] [G loss: 2.480034]\n",
      "epoch:10 step:9922 [D loss: 0.538761, acc: 71.88%] [G loss: 2.203192]\n",
      "epoch:10 step:9923 [D loss: 0.618582, acc: 61.72%] [G loss: 1.959233]\n",
      "epoch:10 step:9924 [D loss: 0.551934, acc: 67.19%] [G loss: 2.301714]\n",
      "epoch:10 step:9925 [D loss: 0.548491, acc: 77.34%] [G loss: 2.243994]\n",
      "epoch:10 step:9926 [D loss: 0.658716, acc: 64.06%] [G loss: 2.151544]\n",
      "epoch:10 step:9927 [D loss: 0.624051, acc: 67.97%] [G loss: 2.042900]\n",
      "epoch:10 step:9928 [D loss: 0.599736, acc: 64.06%] [G loss: 2.133165]\n",
      "epoch:10 step:9929 [D loss: 0.637685, acc: 61.72%] [G loss: 2.042831]\n",
      "epoch:10 step:9930 [D loss: 0.659341, acc: 58.59%] [G loss: 1.974801]\n",
      "epoch:10 step:9931 [D loss: 0.651274, acc: 67.19%] [G loss: 1.955238]\n",
      "epoch:10 step:9932 [D loss: 0.686478, acc: 53.12%] [G loss: 1.876686]\n",
      "epoch:10 step:9933 [D loss: 0.615696, acc: 66.41%] [G loss: 2.002435]\n",
      "epoch:10 step:9934 [D loss: 0.632346, acc: 61.72%] [G loss: 2.240739]\n",
      "epoch:10 step:9935 [D loss: 0.747169, acc: 53.12%] [G loss: 1.901343]\n",
      "epoch:10 step:9936 [D loss: 0.629944, acc: 70.31%] [G loss: 1.890768]\n",
      "epoch:10 step:9937 [D loss: 0.646816, acc: 61.72%] [G loss: 1.955120]\n",
      "epoch:10 step:9938 [D loss: 0.599600, acc: 68.75%] [G loss: 2.102364]\n",
      "epoch:10 step:9939 [D loss: 0.610221, acc: 64.84%] [G loss: 2.199824]\n",
      "epoch:10 step:9940 [D loss: 0.677137, acc: 60.16%] [G loss: 1.926071]\n",
      "epoch:10 step:9941 [D loss: 0.636174, acc: 65.62%] [G loss: 1.952907]\n",
      "epoch:10 step:9942 [D loss: 0.632430, acc: 67.97%] [G loss: 1.925700]\n",
      "epoch:10 step:9943 [D loss: 0.641685, acc: 66.41%] [G loss: 1.926088]\n",
      "epoch:10 step:9944 [D loss: 0.655402, acc: 57.81%] [G loss: 2.087080]\n",
      "epoch:10 step:9945 [D loss: 0.632902, acc: 64.84%] [G loss: 2.077794]\n",
      "epoch:10 step:9946 [D loss: 0.693392, acc: 59.38%] [G loss: 1.888520]\n",
      "epoch:10 step:9947 [D loss: 0.681045, acc: 55.47%] [G loss: 1.924448]\n",
      "epoch:10 step:9948 [D loss: 0.633452, acc: 60.94%] [G loss: 1.972587]\n",
      "epoch:10 step:9949 [D loss: 0.643989, acc: 65.62%] [G loss: 1.898411]\n",
      "epoch:10 step:9950 [D loss: 0.667265, acc: 61.72%] [G loss: 1.993375]\n",
      "epoch:10 step:9951 [D loss: 0.654615, acc: 64.06%] [G loss: 1.801258]\n",
      "epoch:10 step:9952 [D loss: 0.655870, acc: 62.50%] [G loss: 2.090935]\n",
      "epoch:10 step:9953 [D loss: 0.667589, acc: 61.72%] [G loss: 1.848925]\n",
      "epoch:10 step:9954 [D loss: 0.613398, acc: 69.53%] [G loss: 1.955889]\n",
      "epoch:10 step:9955 [D loss: 0.691060, acc: 59.38%] [G loss: 2.005314]\n",
      "epoch:10 step:9956 [D loss: 0.674965, acc: 58.59%] [G loss: 1.977412]\n",
      "epoch:10 step:9957 [D loss: 0.636565, acc: 67.97%] [G loss: 2.011707]\n",
      "epoch:10 step:9958 [D loss: 0.574354, acc: 71.09%] [G loss: 2.189347]\n",
      "epoch:10 step:9959 [D loss: 0.595689, acc: 71.88%] [G loss: 2.042766]\n",
      "epoch:10 step:9960 [D loss: 0.670593, acc: 57.03%] [G loss: 2.017194]\n",
      "epoch:10 step:9961 [D loss: 0.601212, acc: 64.84%] [G loss: 2.077829]\n",
      "epoch:10 step:9962 [D loss: 0.645091, acc: 62.50%] [G loss: 2.154319]\n",
      "epoch:10 step:9963 [D loss: 0.687821, acc: 53.12%] [G loss: 2.031121]\n",
      "epoch:10 step:9964 [D loss: 0.637443, acc: 61.72%] [G loss: 2.178690]\n",
      "epoch:10 step:9965 [D loss: 0.646895, acc: 61.72%] [G loss: 2.099471]\n",
      "epoch:10 step:9966 [D loss: 0.676033, acc: 58.59%] [G loss: 1.921186]\n",
      "epoch:10 step:9967 [D loss: 0.646873, acc: 60.16%] [G loss: 1.915097]\n",
      "epoch:10 step:9968 [D loss: 0.582258, acc: 66.41%] [G loss: 2.133498]\n",
      "epoch:10 step:9969 [D loss: 0.664589, acc: 60.16%] [G loss: 1.894532]\n",
      "epoch:10 step:9970 [D loss: 0.675594, acc: 62.50%] [G loss: 2.115376]\n",
      "epoch:10 step:9971 [D loss: 0.643956, acc: 64.84%] [G loss: 1.917001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9972 [D loss: 0.596359, acc: 67.97%] [G loss: 1.996044]\n",
      "epoch:10 step:9973 [D loss: 0.609635, acc: 66.41%] [G loss: 2.066250]\n",
      "epoch:10 step:9974 [D loss: 0.647865, acc: 63.28%] [G loss: 1.971746]\n",
      "epoch:10 step:9975 [D loss: 0.639101, acc: 60.94%] [G loss: 2.258655]\n",
      "epoch:10 step:9976 [D loss: 0.628631, acc: 64.06%] [G loss: 2.061945]\n",
      "epoch:10 step:9977 [D loss: 0.631303, acc: 68.75%] [G loss: 2.162171]\n",
      "epoch:10 step:9978 [D loss: 0.689689, acc: 65.62%] [G loss: 2.137314]\n",
      "epoch:10 step:9979 [D loss: 0.595233, acc: 67.19%] [G loss: 2.084599]\n",
      "epoch:10 step:9980 [D loss: 0.624238, acc: 65.62%] [G loss: 1.949940]\n",
      "epoch:10 step:9981 [D loss: 0.677058, acc: 56.25%] [G loss: 1.915383]\n",
      "epoch:10 step:9982 [D loss: 0.640966, acc: 60.16%] [G loss: 1.938559]\n",
      "epoch:10 step:9983 [D loss: 0.574881, acc: 71.09%] [G loss: 1.971141]\n",
      "epoch:10 step:9984 [D loss: 0.678110, acc: 54.69%] [G loss: 2.000730]\n",
      "epoch:10 step:9985 [D loss: 0.691342, acc: 57.81%] [G loss: 1.863261]\n",
      "epoch:10 step:9986 [D loss: 0.644626, acc: 67.19%] [G loss: 1.935183]\n",
      "epoch:10 step:9987 [D loss: 0.646578, acc: 60.16%] [G loss: 1.946546]\n",
      "epoch:10 step:9988 [D loss: 0.649290, acc: 65.62%] [G loss: 2.206293]\n",
      "epoch:10 step:9989 [D loss: 0.661605, acc: 63.28%] [G loss: 1.943852]\n",
      "epoch:10 step:9990 [D loss: 0.620703, acc: 67.19%] [G loss: 2.088699]\n",
      "epoch:10 step:9991 [D loss: 0.657618, acc: 57.03%] [G loss: 1.823058]\n",
      "epoch:10 step:9992 [D loss: 0.604966, acc: 64.84%] [G loss: 2.113416]\n",
      "epoch:10 step:9993 [D loss: 0.593918, acc: 68.75%] [G loss: 1.902316]\n",
      "epoch:10 step:9994 [D loss: 0.553028, acc: 67.19%] [G loss: 2.217033]\n",
      "epoch:10 step:9995 [D loss: 0.676365, acc: 60.94%] [G loss: 1.973027]\n",
      "epoch:10 step:9996 [D loss: 0.736951, acc: 57.03%] [G loss: 1.938915]\n",
      "epoch:10 step:9997 [D loss: 0.657058, acc: 57.81%] [G loss: 1.865382]\n",
      "epoch:10 step:9998 [D loss: 0.705116, acc: 56.25%] [G loss: 1.722943]\n",
      "epoch:10 step:9999 [D loss: 0.647206, acc: 64.84%] [G loss: 2.083936]\n",
      "epoch:10 step:10000 [D loss: 0.598079, acc: 64.06%] [G loss: 2.122654]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.721273\n",
      "FID: 22.899466\n",
      "0 = 12.826191390275985\n",
      "1 = 0.08945147250452358\n",
      "2 = 0.902899980545044\n",
      "3 = 0.9039999842643738\n",
      "4 = 0.9017999768257141\n",
      "5 = 0.9020155668258667\n",
      "6 = 0.9039999842643738\n",
      "7 = 7.296921681058401\n",
      "8 = 0.09470516631748069\n",
      "9 = 0.756600022315979\n",
      "10 = 0.7648000121116638\n",
      "11 = 0.7483999729156494\n",
      "12 = 0.7524596452713013\n",
      "13 = 0.7648000121116638\n",
      "14 = 6.721304416656494\n",
      "15 = 9.340773582458496\n",
      "16 = 0.146172434091568\n",
      "17 = 6.721273422241211\n",
      "18 = 22.899465560913086\n",
      "epoch:10 step:10001 [D loss: 0.607682, acc: 68.75%] [G loss: 2.039176]\n",
      "epoch:10 step:10002 [D loss: 0.635472, acc: 61.72%] [G loss: 2.050170]\n",
      "epoch:10 step:10003 [D loss: 0.603052, acc: 67.97%] [G loss: 1.904696]\n",
      "epoch:10 step:10004 [D loss: 0.605074, acc: 69.53%] [G loss: 1.914297]\n",
      "epoch:10 step:10005 [D loss: 0.599789, acc: 63.28%] [G loss: 1.982276]\n",
      "epoch:10 step:10006 [D loss: 0.653629, acc: 64.84%] [G loss: 1.915657]\n",
      "epoch:10 step:10007 [D loss: 0.632769, acc: 62.50%] [G loss: 2.093302]\n",
      "epoch:10 step:10008 [D loss: 0.642013, acc: 67.19%] [G loss: 2.071864]\n",
      "epoch:10 step:10009 [D loss: 0.647346, acc: 60.16%] [G loss: 2.115174]\n",
      "epoch:10 step:10010 [D loss: 0.613556, acc: 61.72%] [G loss: 2.049514]\n",
      "epoch:10 step:10011 [D loss: 0.619433, acc: 67.97%] [G loss: 2.163533]\n",
      "epoch:10 step:10012 [D loss: 0.595790, acc: 71.09%] [G loss: 2.142141]\n",
      "epoch:10 step:10013 [D loss: 0.642693, acc: 64.84%] [G loss: 1.959529]\n",
      "epoch:10 step:10014 [D loss: 0.669284, acc: 59.38%] [G loss: 1.961082]\n",
      "epoch:10 step:10015 [D loss: 0.605289, acc: 67.19%] [G loss: 2.147011]\n",
      "epoch:10 step:10016 [D loss: 0.659765, acc: 61.72%] [G loss: 2.140987]\n",
      "epoch:10 step:10017 [D loss: 0.582667, acc: 67.19%] [G loss: 2.225447]\n",
      "epoch:10 step:10018 [D loss: 0.510936, acc: 75.78%] [G loss: 2.579194]\n",
      "epoch:10 step:10019 [D loss: 0.569508, acc: 73.44%] [G loss: 2.271349]\n",
      "epoch:10 step:10020 [D loss: 0.673415, acc: 63.28%] [G loss: 2.407200]\n",
      "epoch:10 step:10021 [D loss: 0.618622, acc: 65.62%] [G loss: 2.124161]\n",
      "epoch:10 step:10022 [D loss: 0.651169, acc: 64.84%] [G loss: 1.970593]\n",
      "epoch:10 step:10023 [D loss: 0.620101, acc: 63.28%] [G loss: 1.919146]\n",
      "epoch:10 step:10024 [D loss: 0.629506, acc: 67.97%] [G loss: 2.007135]\n",
      "epoch:10 step:10025 [D loss: 0.668411, acc: 57.81%] [G loss: 1.906580]\n",
      "epoch:10 step:10026 [D loss: 0.670414, acc: 64.84%] [G loss: 2.093189]\n",
      "epoch:10 step:10027 [D loss: 0.676095, acc: 58.59%] [G loss: 1.876418]\n",
      "epoch:10 step:10028 [D loss: 0.695172, acc: 56.25%] [G loss: 1.992804]\n",
      "epoch:10 step:10029 [D loss: 0.650454, acc: 63.28%] [G loss: 2.013497]\n",
      "epoch:10 step:10030 [D loss: 0.599107, acc: 66.41%] [G loss: 2.039630]\n",
      "epoch:10 step:10031 [D loss: 0.661075, acc: 59.38%] [G loss: 2.025779]\n",
      "epoch:10 step:10032 [D loss: 0.640611, acc: 60.94%] [G loss: 1.908920]\n",
      "epoch:10 step:10033 [D loss: 0.649936, acc: 64.06%] [G loss: 2.184390]\n",
      "epoch:10 step:10034 [D loss: 0.646943, acc: 60.16%] [G loss: 1.902681]\n",
      "epoch:10 step:10035 [D loss: 0.626137, acc: 68.75%] [G loss: 1.993356]\n",
      "epoch:10 step:10036 [D loss: 0.639610, acc: 66.41%] [G loss: 2.016424]\n",
      "epoch:10 step:10037 [D loss: 0.624769, acc: 64.06%] [G loss: 1.974452]\n",
      "epoch:10 step:10038 [D loss: 0.731853, acc: 49.22%] [G loss: 1.968220]\n",
      "epoch:10 step:10039 [D loss: 0.702221, acc: 59.38%] [G loss: 1.851214]\n",
      "epoch:10 step:10040 [D loss: 0.632080, acc: 67.97%] [G loss: 1.891745]\n",
      "epoch:10 step:10041 [D loss: 0.670304, acc: 63.28%] [G loss: 1.968809]\n",
      "epoch:10 step:10042 [D loss: 0.671537, acc: 57.03%] [G loss: 1.828625]\n",
      "epoch:10 step:10043 [D loss: 0.607954, acc: 64.84%] [G loss: 1.950526]\n",
      "epoch:10 step:10044 [D loss: 0.649918, acc: 64.06%] [G loss: 1.940377]\n",
      "epoch:10 step:10045 [D loss: 0.639439, acc: 65.62%] [G loss: 2.079921]\n",
      "epoch:10 step:10046 [D loss: 0.655847, acc: 62.50%] [G loss: 1.923954]\n",
      "epoch:10 step:10047 [D loss: 0.605091, acc: 67.19%] [G loss: 1.976688]\n",
      "epoch:10 step:10048 [D loss: 0.676131, acc: 59.38%] [G loss: 2.077861]\n",
      "epoch:10 step:10049 [D loss: 0.570489, acc: 69.53%] [G loss: 1.972198]\n",
      "epoch:10 step:10050 [D loss: 0.650089, acc: 62.50%] [G loss: 2.060325]\n",
      "epoch:10 step:10051 [D loss: 0.551517, acc: 71.88%] [G loss: 2.152062]\n",
      "epoch:10 step:10052 [D loss: 0.611410, acc: 59.38%] [G loss: 1.964095]\n",
      "epoch:10 step:10053 [D loss: 0.621936, acc: 64.06%] [G loss: 1.843584]\n",
      "epoch:10 step:10054 [D loss: 0.684090, acc: 55.47%] [G loss: 1.971190]\n",
      "epoch:10 step:10055 [D loss: 0.662212, acc: 61.72%] [G loss: 2.040563]\n",
      "epoch:10 step:10056 [D loss: 0.674457, acc: 60.16%] [G loss: 1.940551]\n",
      "epoch:10 step:10057 [D loss: 0.613315, acc: 65.62%] [G loss: 2.015830]\n",
      "epoch:10 step:10058 [D loss: 0.604326, acc: 67.97%] [G loss: 2.159517]\n",
      "epoch:10 step:10059 [D loss: 0.664720, acc: 60.16%] [G loss: 2.095352]\n",
      "epoch:10 step:10060 [D loss: 0.591706, acc: 67.19%] [G loss: 2.071997]\n",
      "epoch:10 step:10061 [D loss: 0.633692, acc: 64.06%] [G loss: 2.074049]\n",
      "epoch:10 step:10062 [D loss: 0.636178, acc: 64.06%] [G loss: 2.358898]\n",
      "epoch:10 step:10063 [D loss: 0.591283, acc: 66.41%] [G loss: 2.109191]\n",
      "epoch:10 step:10064 [D loss: 0.649609, acc: 60.94%] [G loss: 2.215817]\n",
      "epoch:10 step:10065 [D loss: 0.610438, acc: 67.19%] [G loss: 2.243965]\n",
      "epoch:10 step:10066 [D loss: 0.635549, acc: 57.81%] [G loss: 2.045074]\n",
      "epoch:10 step:10067 [D loss: 0.692304, acc: 54.69%] [G loss: 1.887164]\n",
      "epoch:10 step:10068 [D loss: 0.663566, acc: 54.69%] [G loss: 1.884127]\n",
      "epoch:10 step:10069 [D loss: 0.613326, acc: 67.19%] [G loss: 1.905672]\n",
      "epoch:10 step:10070 [D loss: 0.604947, acc: 67.97%] [G loss: 2.017522]\n",
      "epoch:10 step:10071 [D loss: 0.616506, acc: 66.41%] [G loss: 2.001476]\n",
      "epoch:10 step:10072 [D loss: 0.653683, acc: 56.25%] [G loss: 1.863846]\n",
      "epoch:10 step:10073 [D loss: 0.676370, acc: 62.50%] [G loss: 1.992714]\n",
      "epoch:10 step:10074 [D loss: 0.690944, acc: 58.59%] [G loss: 1.882444]\n",
      "epoch:10 step:10075 [D loss: 0.637787, acc: 64.84%] [G loss: 2.019304]\n",
      "epoch:10 step:10076 [D loss: 0.614741, acc: 64.06%] [G loss: 2.049604]\n",
      "epoch:10 step:10077 [D loss: 0.573355, acc: 69.53%] [G loss: 2.147760]\n",
      "epoch:10 step:10078 [D loss: 0.589997, acc: 70.31%] [G loss: 2.169155]\n",
      "epoch:10 step:10079 [D loss: 0.581619, acc: 67.19%] [G loss: 2.385605]\n",
      "epoch:10 step:10080 [D loss: 0.645419, acc: 63.28%] [G loss: 1.918025]\n",
      "epoch:10 step:10081 [D loss: 0.686272, acc: 56.25%] [G loss: 1.958728]\n",
      "epoch:10 step:10082 [D loss: 0.621948, acc: 67.19%] [G loss: 2.055633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10083 [D loss: 0.665553, acc: 60.16%] [G loss: 1.952567]\n",
      "epoch:10 step:10084 [D loss: 0.637731, acc: 60.94%] [G loss: 1.996360]\n",
      "epoch:10 step:10085 [D loss: 0.644452, acc: 59.38%] [G loss: 1.968765]\n",
      "epoch:10 step:10086 [D loss: 0.660378, acc: 61.72%] [G loss: 1.850243]\n",
      "epoch:10 step:10087 [D loss: 0.632596, acc: 64.06%] [G loss: 1.925332]\n",
      "epoch:10 step:10088 [D loss: 0.699023, acc: 61.72%] [G loss: 2.017404]\n",
      "epoch:10 step:10089 [D loss: 0.673988, acc: 57.03%] [G loss: 2.081377]\n",
      "epoch:10 step:10090 [D loss: 0.606628, acc: 67.97%] [G loss: 2.157388]\n",
      "epoch:10 step:10091 [D loss: 0.627885, acc: 71.09%] [G loss: 1.929483]\n",
      "epoch:10 step:10092 [D loss: 0.672435, acc: 57.81%] [G loss: 1.912187]\n",
      "epoch:10 step:10093 [D loss: 0.608281, acc: 66.41%] [G loss: 1.936293]\n",
      "epoch:10 step:10094 [D loss: 0.641224, acc: 59.38%] [G loss: 2.076286]\n",
      "epoch:10 step:10095 [D loss: 0.602694, acc: 67.97%] [G loss: 2.133874]\n",
      "epoch:10 step:10096 [D loss: 0.631561, acc: 67.97%] [G loss: 1.972962]\n",
      "epoch:10 step:10097 [D loss: 0.612717, acc: 68.75%] [G loss: 1.972115]\n",
      "epoch:10 step:10098 [D loss: 0.626762, acc: 64.84%] [G loss: 2.057065]\n",
      "epoch:10 step:10099 [D loss: 0.696175, acc: 58.59%] [G loss: 2.032563]\n",
      "epoch:10 step:10100 [D loss: 0.643889, acc: 66.41%] [G loss: 1.911679]\n",
      "epoch:10 step:10101 [D loss: 0.660723, acc: 63.28%] [G loss: 2.012411]\n",
      "epoch:10 step:10102 [D loss: 0.583426, acc: 67.97%] [G loss: 1.976478]\n",
      "epoch:10 step:10103 [D loss: 0.602549, acc: 68.75%] [G loss: 2.056647]\n",
      "epoch:10 step:10104 [D loss: 0.670555, acc: 66.41%] [G loss: 1.932891]\n",
      "epoch:10 step:10105 [D loss: 0.605293, acc: 67.19%] [G loss: 2.211579]\n",
      "epoch:10 step:10106 [D loss: 0.581279, acc: 71.09%] [G loss: 2.099989]\n",
      "epoch:10 step:10107 [D loss: 0.679949, acc: 61.72%] [G loss: 1.941725]\n",
      "epoch:10 step:10108 [D loss: 0.631553, acc: 62.50%] [G loss: 1.957215]\n",
      "epoch:10 step:10109 [D loss: 0.641438, acc: 63.28%] [G loss: 1.993148]\n",
      "epoch:10 step:10110 [D loss: 0.631313, acc: 64.84%] [G loss: 2.036528]\n",
      "epoch:10 step:10111 [D loss: 0.639941, acc: 62.50%] [G loss: 1.964707]\n",
      "epoch:10 step:10112 [D loss: 0.638717, acc: 62.50%] [G loss: 2.115775]\n",
      "epoch:10 step:10113 [D loss: 0.619765, acc: 64.84%] [G loss: 2.056360]\n",
      "epoch:10 step:10114 [D loss: 0.620827, acc: 71.88%] [G loss: 1.980310]\n",
      "epoch:10 step:10115 [D loss: 0.623483, acc: 63.28%] [G loss: 1.919907]\n",
      "epoch:10 step:10116 [D loss: 0.607489, acc: 65.62%] [G loss: 1.998248]\n",
      "epoch:10 step:10117 [D loss: 0.616620, acc: 68.75%] [G loss: 2.014795]\n",
      "epoch:10 step:10118 [D loss: 0.648520, acc: 67.19%] [G loss: 2.017077]\n",
      "epoch:10 step:10119 [D loss: 0.642182, acc: 67.19%] [G loss: 1.950013]\n",
      "epoch:10 step:10120 [D loss: 0.625160, acc: 64.06%] [G loss: 2.057835]\n",
      "epoch:10 step:10121 [D loss: 0.625826, acc: 63.28%] [G loss: 1.861934]\n",
      "epoch:10 step:10122 [D loss: 0.689140, acc: 60.16%] [G loss: 1.905722]\n",
      "epoch:10 step:10123 [D loss: 0.579390, acc: 71.88%] [G loss: 1.991416]\n",
      "epoch:10 step:10124 [D loss: 0.609273, acc: 66.41%] [G loss: 2.060287]\n",
      "epoch:10 step:10125 [D loss: 0.585974, acc: 66.41%] [G loss: 2.048968]\n",
      "epoch:10 step:10126 [D loss: 0.639995, acc: 65.62%] [G loss: 2.057263]\n",
      "epoch:10 step:10127 [D loss: 0.622376, acc: 71.09%] [G loss: 1.995763]\n",
      "epoch:10 step:10128 [D loss: 0.684408, acc: 58.59%] [G loss: 1.897645]\n",
      "epoch:10 step:10129 [D loss: 0.706749, acc: 53.12%] [G loss: 1.877797]\n",
      "epoch:10 step:10130 [D loss: 0.642509, acc: 61.72%] [G loss: 1.940684]\n",
      "epoch:10 step:10131 [D loss: 0.620641, acc: 62.50%] [G loss: 1.853079]\n",
      "epoch:10 step:10132 [D loss: 0.678480, acc: 60.16%] [G loss: 1.843328]\n",
      "epoch:10 step:10133 [D loss: 0.608978, acc: 62.50%] [G loss: 1.964081]\n",
      "epoch:10 step:10134 [D loss: 0.602614, acc: 67.97%] [G loss: 1.908623]\n",
      "epoch:10 step:10135 [D loss: 0.685793, acc: 59.38%] [G loss: 1.870915]\n",
      "epoch:10 step:10136 [D loss: 0.708181, acc: 57.03%] [G loss: 1.782736]\n",
      "epoch:10 step:10137 [D loss: 0.643322, acc: 61.72%] [G loss: 1.986738]\n",
      "epoch:10 step:10138 [D loss: 0.667756, acc: 61.72%] [G loss: 1.846040]\n",
      "epoch:10 step:10139 [D loss: 0.619784, acc: 69.53%] [G loss: 1.976211]\n",
      "epoch:10 step:10140 [D loss: 0.673389, acc: 57.03%] [G loss: 1.925665]\n",
      "epoch:10 step:10141 [D loss: 0.651960, acc: 64.06%] [G loss: 2.034123]\n",
      "epoch:10 step:10142 [D loss: 0.614732, acc: 65.62%] [G loss: 2.039526]\n",
      "epoch:10 step:10143 [D loss: 0.691097, acc: 58.59%] [G loss: 2.062314]\n",
      "epoch:10 step:10144 [D loss: 0.629435, acc: 62.50%] [G loss: 2.113190]\n",
      "epoch:10 step:10145 [D loss: 0.622031, acc: 58.59%] [G loss: 2.168056]\n",
      "epoch:10 step:10146 [D loss: 0.659041, acc: 56.25%] [G loss: 1.945217]\n",
      "epoch:10 step:10147 [D loss: 0.619896, acc: 65.62%] [G loss: 2.130643]\n",
      "epoch:10 step:10148 [D loss: 0.619623, acc: 62.50%] [G loss: 1.904038]\n",
      "epoch:10 step:10149 [D loss: 0.674169, acc: 60.16%] [G loss: 2.018902]\n",
      "epoch:10 step:10150 [D loss: 0.655870, acc: 60.16%] [G loss: 1.960555]\n",
      "epoch:10 step:10151 [D loss: 0.591137, acc: 67.97%] [G loss: 2.103092]\n",
      "epoch:10 step:10152 [D loss: 0.606271, acc: 62.50%] [G loss: 2.182182]\n",
      "epoch:10 step:10153 [D loss: 0.650077, acc: 61.72%] [G loss: 1.985703]\n",
      "epoch:10 step:10154 [D loss: 0.662975, acc: 60.94%] [G loss: 1.755207]\n",
      "epoch:10 step:10155 [D loss: 0.666284, acc: 57.03%] [G loss: 1.805023]\n",
      "epoch:10 step:10156 [D loss: 0.603687, acc: 65.62%] [G loss: 2.171741]\n",
      "epoch:10 step:10157 [D loss: 0.644170, acc: 64.06%] [G loss: 2.087135]\n",
      "epoch:10 step:10158 [D loss: 0.611380, acc: 64.84%] [G loss: 2.046010]\n",
      "epoch:10 step:10159 [D loss: 0.656362, acc: 66.41%] [G loss: 1.983772]\n",
      "epoch:10 step:10160 [D loss: 0.642443, acc: 62.50%] [G loss: 2.021855]\n",
      "epoch:10 step:10161 [D loss: 0.615410, acc: 64.06%] [G loss: 2.040199]\n",
      "epoch:10 step:10162 [D loss: 0.584291, acc: 69.53%] [G loss: 2.308939]\n",
      "epoch:10 step:10163 [D loss: 0.652310, acc: 64.06%] [G loss: 1.988163]\n",
      "epoch:10 step:10164 [D loss: 0.660743, acc: 61.72%] [G loss: 1.899540]\n",
      "epoch:10 step:10165 [D loss: 0.687349, acc: 58.59%] [G loss: 1.983605]\n",
      "epoch:10 step:10166 [D loss: 0.611104, acc: 60.94%] [G loss: 2.081425]\n",
      "epoch:10 step:10167 [D loss: 0.670003, acc: 58.59%] [G loss: 2.080410]\n",
      "epoch:10 step:10168 [D loss: 0.674678, acc: 57.81%] [G loss: 1.981033]\n",
      "epoch:10 step:10169 [D loss: 0.702778, acc: 58.59%] [G loss: 1.930446]\n",
      "epoch:10 step:10170 [D loss: 0.684181, acc: 57.03%] [G loss: 1.826873]\n",
      "epoch:10 step:10171 [D loss: 0.640404, acc: 67.97%] [G loss: 1.933904]\n",
      "epoch:10 step:10172 [D loss: 0.651087, acc: 60.94%] [G loss: 1.928764]\n",
      "epoch:10 step:10173 [D loss: 0.594221, acc: 66.41%] [G loss: 2.000766]\n",
      "epoch:10 step:10174 [D loss: 0.648333, acc: 62.50%] [G loss: 1.948633]\n",
      "epoch:10 step:10175 [D loss: 0.599423, acc: 68.75%] [G loss: 2.194201]\n",
      "epoch:10 step:10176 [D loss: 0.599373, acc: 67.97%] [G loss: 2.010308]\n",
      "epoch:10 step:10177 [D loss: 0.602263, acc: 64.06%] [G loss: 2.110033]\n",
      "epoch:10 step:10178 [D loss: 0.594853, acc: 66.41%] [G loss: 2.172309]\n",
      "epoch:10 step:10179 [D loss: 0.635228, acc: 61.72%] [G loss: 1.955750]\n",
      "epoch:10 step:10180 [D loss: 0.656679, acc: 67.19%] [G loss: 2.177767]\n",
      "epoch:10 step:10181 [D loss: 0.677529, acc: 64.06%] [G loss: 1.947134]\n",
      "epoch:10 step:10182 [D loss: 0.728976, acc: 55.47%] [G loss: 1.886470]\n",
      "epoch:10 step:10183 [D loss: 0.660223, acc: 60.94%] [G loss: 1.987023]\n",
      "epoch:10 step:10184 [D loss: 0.693563, acc: 58.59%] [G loss: 1.991378]\n",
      "epoch:10 step:10185 [D loss: 0.580040, acc: 63.28%] [G loss: 2.301745]\n",
      "epoch:10 step:10186 [D loss: 0.682241, acc: 63.28%] [G loss: 1.951993]\n",
      "epoch:10 step:10187 [D loss: 0.724349, acc: 61.72%] [G loss: 1.921864]\n",
      "epoch:10 step:10188 [D loss: 0.660893, acc: 62.50%] [G loss: 1.933587]\n",
      "epoch:10 step:10189 [D loss: 0.661284, acc: 60.94%] [G loss: 1.853536]\n",
      "epoch:10 step:10190 [D loss: 0.686486, acc: 57.81%] [G loss: 1.838095]\n",
      "epoch:10 step:10191 [D loss: 0.629008, acc: 63.28%] [G loss: 1.903472]\n",
      "epoch:10 step:10192 [D loss: 0.559194, acc: 72.66%] [G loss: 1.993793]\n",
      "epoch:10 step:10193 [D loss: 0.619528, acc: 67.19%] [G loss: 2.004080]\n",
      "epoch:10 step:10194 [D loss: 0.643320, acc: 65.62%] [G loss: 1.933456]\n",
      "epoch:10 step:10195 [D loss: 0.582695, acc: 72.66%] [G loss: 2.047061]\n",
      "epoch:10 step:10196 [D loss: 0.618960, acc: 66.41%] [G loss: 1.889651]\n",
      "epoch:10 step:10197 [D loss: 0.705325, acc: 58.59%] [G loss: 1.862113]\n",
      "epoch:10 step:10198 [D loss: 0.667283, acc: 57.81%] [G loss: 1.885978]\n",
      "epoch:10 step:10199 [D loss: 0.677009, acc: 61.72%] [G loss: 1.765116]\n",
      "epoch:10 step:10200 [D loss: 0.608392, acc: 64.84%] [G loss: 1.917155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.650978\n",
      "FID: 25.238810\n",
      "0 = 12.684909470176748\n",
      "1 = 0.07876931574248353\n",
      "2 = 0.8920000195503235\n",
      "3 = 0.8999999761581421\n",
      "4 = 0.8840000033378601\n",
      "5 = 0.8858267664909363\n",
      "6 = 0.8999999761581421\n",
      "7 = 7.465630829811081\n",
      "8 = 0.10044955294154985\n",
      "9 = 0.7581999897956848\n",
      "10 = 0.7680000066757202\n",
      "11 = 0.7483999729156494\n",
      "12 = 0.7532365918159485\n",
      "13 = 0.7680000066757202\n",
      "14 = 6.651001930236816\n",
      "15 = 9.279443740844727\n",
      "16 = 0.1632155179977417\n",
      "17 = 6.650977611541748\n",
      "18 = 25.23880958557129\n",
      "epoch:10 step:10201 [D loss: 0.670028, acc: 55.47%] [G loss: 2.044529]\n",
      "epoch:10 step:10202 [D loss: 0.592103, acc: 68.75%] [G loss: 1.953668]\n",
      "epoch:10 step:10203 [D loss: 0.644092, acc: 60.94%] [G loss: 2.077188]\n",
      "epoch:10 step:10204 [D loss: 0.634248, acc: 68.75%] [G loss: 2.009491]\n",
      "epoch:10 step:10205 [D loss: 0.647906, acc: 61.72%] [G loss: 1.982621]\n",
      "epoch:10 step:10206 [D loss: 0.662562, acc: 57.03%] [G loss: 1.920505]\n",
      "epoch:10 step:10207 [D loss: 0.541954, acc: 72.66%] [G loss: 2.023407]\n",
      "epoch:10 step:10208 [D loss: 0.672832, acc: 60.16%] [G loss: 1.895428]\n",
      "epoch:10 step:10209 [D loss: 0.657950, acc: 60.94%] [G loss: 1.968066]\n",
      "epoch:10 step:10210 [D loss: 0.604431, acc: 66.41%] [G loss: 1.975778]\n",
      "epoch:10 step:10211 [D loss: 0.587119, acc: 71.09%] [G loss: 2.108307]\n",
      "epoch:10 step:10212 [D loss: 0.660370, acc: 64.06%] [G loss: 1.996907]\n",
      "epoch:10 step:10213 [D loss: 0.682791, acc: 53.91%] [G loss: 1.879905]\n",
      "epoch:10 step:10214 [D loss: 0.673229, acc: 60.16%] [G loss: 2.280939]\n",
      "epoch:10 step:10215 [D loss: 0.675640, acc: 57.81%] [G loss: 2.047393]\n",
      "epoch:10 step:10216 [D loss: 0.630717, acc: 65.62%] [G loss: 2.082793]\n",
      "epoch:10 step:10217 [D loss: 0.594052, acc: 68.75%] [G loss: 2.089405]\n",
      "epoch:10 step:10218 [D loss: 0.678114, acc: 61.72%] [G loss: 2.076871]\n",
      "epoch:10 step:10219 [D loss: 0.597035, acc: 64.84%] [G loss: 2.139600]\n",
      "epoch:10 step:10220 [D loss: 0.676731, acc: 57.81%] [G loss: 2.240419]\n",
      "epoch:10 step:10221 [D loss: 0.626698, acc: 59.38%] [G loss: 1.983094]\n",
      "epoch:10 step:10222 [D loss: 0.680173, acc: 60.16%] [G loss: 2.084713]\n",
      "epoch:10 step:10223 [D loss: 0.613072, acc: 58.59%] [G loss: 1.905722]\n",
      "epoch:10 step:10224 [D loss: 0.681321, acc: 57.81%] [G loss: 1.847548]\n",
      "epoch:10 step:10225 [D loss: 0.599957, acc: 64.84%] [G loss: 2.091574]\n",
      "epoch:10 step:10226 [D loss: 0.697027, acc: 57.03%] [G loss: 1.904648]\n",
      "epoch:10 step:10227 [D loss: 0.618201, acc: 67.19%] [G loss: 2.082559]\n",
      "epoch:10 step:10228 [D loss: 0.713093, acc: 49.22%] [G loss: 1.856705]\n",
      "epoch:10 step:10229 [D loss: 0.715000, acc: 52.34%] [G loss: 1.897281]\n",
      "epoch:10 step:10230 [D loss: 0.650137, acc: 64.06%] [G loss: 1.846307]\n",
      "epoch:10 step:10231 [D loss: 0.622635, acc: 66.41%] [G loss: 2.033967]\n",
      "epoch:10 step:10232 [D loss: 0.620316, acc: 66.41%] [G loss: 1.820627]\n",
      "epoch:10 step:10233 [D loss: 0.614112, acc: 61.72%] [G loss: 2.027133]\n",
      "epoch:10 step:10234 [D loss: 0.669686, acc: 61.72%] [G loss: 2.004454]\n",
      "epoch:10 step:10235 [D loss: 0.651366, acc: 64.84%] [G loss: 1.909103]\n",
      "epoch:10 step:10236 [D loss: 0.621787, acc: 62.50%] [G loss: 2.004163]\n",
      "epoch:10 step:10237 [D loss: 0.626630, acc: 65.62%] [G loss: 1.923325]\n",
      "epoch:10 step:10238 [D loss: 0.659018, acc: 61.72%] [G loss: 1.819383]\n",
      "epoch:10 step:10239 [D loss: 0.686098, acc: 62.50%] [G loss: 1.836377]\n",
      "epoch:10 step:10240 [D loss: 0.645548, acc: 59.38%] [G loss: 1.936877]\n",
      "epoch:10 step:10241 [D loss: 0.594890, acc: 68.75%] [G loss: 2.042038]\n",
      "epoch:10 step:10242 [D loss: 0.642521, acc: 67.19%] [G loss: 1.969205]\n",
      "epoch:10 step:10243 [D loss: 0.692373, acc: 57.81%] [G loss: 1.812076]\n",
      "epoch:10 step:10244 [D loss: 0.665546, acc: 62.50%] [G loss: 1.817485]\n",
      "epoch:10 step:10245 [D loss: 0.611203, acc: 70.31%] [G loss: 2.024772]\n",
      "epoch:10 step:10246 [D loss: 0.616255, acc: 66.41%] [G loss: 1.958508]\n",
      "epoch:10 step:10247 [D loss: 0.633093, acc: 64.84%] [G loss: 2.108593]\n",
      "epoch:10 step:10248 [D loss: 0.647505, acc: 63.28%] [G loss: 1.773120]\n",
      "epoch:10 step:10249 [D loss: 0.655893, acc: 64.06%] [G loss: 1.833280]\n",
      "epoch:10 step:10250 [D loss: 0.584772, acc: 67.97%] [G loss: 1.938317]\n",
      "epoch:10 step:10251 [D loss: 0.680797, acc: 56.25%] [G loss: 1.957862]\n",
      "epoch:10 step:10252 [D loss: 0.601230, acc: 67.97%] [G loss: 1.955109]\n",
      "epoch:10 step:10253 [D loss: 0.600014, acc: 71.88%] [G loss: 2.001657]\n",
      "epoch:10 step:10254 [D loss: 0.640462, acc: 65.62%] [G loss: 2.209794]\n",
      "epoch:10 step:10255 [D loss: 0.590186, acc: 70.31%] [G loss: 2.004214]\n",
      "epoch:10 step:10256 [D loss: 0.583896, acc: 67.97%] [G loss: 2.323183]\n",
      "epoch:10 step:10257 [D loss: 0.637618, acc: 63.28%] [G loss: 1.947229]\n",
      "epoch:10 step:10258 [D loss: 0.577513, acc: 69.53%] [G loss: 2.278613]\n",
      "epoch:10 step:10259 [D loss: 0.628787, acc: 67.19%] [G loss: 2.129313]\n",
      "epoch:10 step:10260 [D loss: 0.566025, acc: 73.44%] [G loss: 2.101721]\n",
      "epoch:10 step:10261 [D loss: 0.623543, acc: 67.19%] [G loss: 1.944161]\n",
      "epoch:10 step:10262 [D loss: 0.755556, acc: 51.56%] [G loss: 1.942756]\n",
      "epoch:10 step:10263 [D loss: 0.644856, acc: 59.38%] [G loss: 1.969691]\n",
      "epoch:10 step:10264 [D loss: 0.604394, acc: 68.75%] [G loss: 2.063097]\n",
      "epoch:10 step:10265 [D loss: 0.622174, acc: 64.06%] [G loss: 2.068108]\n",
      "epoch:10 step:10266 [D loss: 0.665120, acc: 60.16%] [G loss: 1.917739]\n",
      "epoch:10 step:10267 [D loss: 0.579487, acc: 68.75%] [G loss: 2.124777]\n",
      "epoch:10 step:10268 [D loss: 0.642175, acc: 62.50%] [G loss: 1.983716]\n",
      "epoch:10 step:10269 [D loss: 0.564086, acc: 74.22%] [G loss: 2.171672]\n",
      "epoch:10 step:10270 [D loss: 0.626069, acc: 66.41%] [G loss: 2.093338]\n",
      "epoch:10 step:10271 [D loss: 0.635037, acc: 62.50%] [G loss: 1.982691]\n",
      "epoch:10 step:10272 [D loss: 0.646279, acc: 65.62%] [G loss: 1.963995]\n",
      "epoch:10 step:10273 [D loss: 0.600850, acc: 64.06%] [G loss: 2.095161]\n",
      "epoch:10 step:10274 [D loss: 0.607769, acc: 65.62%] [G loss: 2.072408]\n",
      "epoch:10 step:10275 [D loss: 0.682544, acc: 61.72%] [G loss: 2.297018]\n",
      "epoch:10 step:10276 [D loss: 0.626494, acc: 64.06%] [G loss: 2.156882]\n",
      "epoch:10 step:10277 [D loss: 0.570061, acc: 67.19%] [G loss: 2.415905]\n",
      "epoch:10 step:10278 [D loss: 0.638062, acc: 64.84%] [G loss: 2.002061]\n",
      "epoch:10 step:10279 [D loss: 0.597297, acc: 68.75%] [G loss: 2.366565]\n",
      "epoch:10 step:10280 [D loss: 0.626545, acc: 67.19%] [G loss: 2.205995]\n",
      "epoch:10 step:10281 [D loss: 0.624871, acc: 65.62%] [G loss: 2.023741]\n",
      "epoch:10 step:10282 [D loss: 0.606243, acc: 66.41%] [G loss: 2.325448]\n",
      "epoch:10 step:10283 [D loss: 0.625585, acc: 61.72%] [G loss: 2.255034]\n",
      "epoch:10 step:10284 [D loss: 0.661573, acc: 61.72%] [G loss: 2.060831]\n",
      "epoch:10 step:10285 [D loss: 0.684227, acc: 57.81%] [G loss: 1.913062]\n",
      "epoch:10 step:10286 [D loss: 0.628047, acc: 65.62%] [G loss: 1.938584]\n",
      "epoch:10 step:10287 [D loss: 0.663298, acc: 62.50%] [G loss: 2.131483]\n",
      "epoch:10 step:10288 [D loss: 0.614216, acc: 70.31%] [G loss: 2.065722]\n",
      "epoch:10 step:10289 [D loss: 0.557637, acc: 73.44%] [G loss: 2.312385]\n",
      "epoch:10 step:10290 [D loss: 0.716062, acc: 50.00%] [G loss: 2.170802]\n",
      "epoch:10 step:10291 [D loss: 0.568094, acc: 69.53%] [G loss: 2.243309]\n",
      "epoch:10 step:10292 [D loss: 0.639752, acc: 60.94%] [G loss: 2.029212]\n",
      "epoch:10 step:10293 [D loss: 0.615048, acc: 68.75%] [G loss: 2.300898]\n",
      "epoch:10 step:10294 [D loss: 0.593660, acc: 71.88%] [G loss: 2.157950]\n",
      "epoch:10 step:10295 [D loss: 0.639653, acc: 66.41%] [G loss: 2.312205]\n",
      "epoch:10 step:10296 [D loss: 0.565202, acc: 72.66%] [G loss: 2.452137]\n",
      "epoch:10 step:10297 [D loss: 0.609075, acc: 64.84%] [G loss: 2.337601]\n",
      "epoch:10 step:10298 [D loss: 0.723997, acc: 56.25%] [G loss: 1.887062]\n",
      "epoch:10 step:10299 [D loss: 0.716322, acc: 54.69%] [G loss: 2.068415]\n",
      "epoch:10 step:10300 [D loss: 0.688333, acc: 62.50%] [G loss: 2.064241]\n",
      "epoch:10 step:10301 [D loss: 0.620655, acc: 67.97%] [G loss: 2.145774]\n",
      "epoch:10 step:10302 [D loss: 0.553371, acc: 72.66%] [G loss: 2.005165]\n",
      "epoch:10 step:10303 [D loss: 0.585272, acc: 69.53%] [G loss: 2.159429]\n",
      "epoch:10 step:10304 [D loss: 0.591990, acc: 70.31%] [G loss: 2.328166]\n",
      "epoch:10 step:10305 [D loss: 0.676757, acc: 67.19%] [G loss: 1.953224]\n",
      "epoch:10 step:10306 [D loss: 0.513043, acc: 78.12%] [G loss: 2.266486]\n",
      "epoch:10 step:10307 [D loss: 0.612228, acc: 64.06%] [G loss: 2.701207]\n",
      "epoch:11 step:10308 [D loss: 0.649180, acc: 61.72%] [G loss: 2.154376]\n",
      "epoch:11 step:10309 [D loss: 0.632930, acc: 71.09%] [G loss: 2.233995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10310 [D loss: 0.627033, acc: 59.38%] [G loss: 2.140884]\n",
      "epoch:11 step:10311 [D loss: 0.633081, acc: 62.50%] [G loss: 2.106388]\n",
      "epoch:11 step:10312 [D loss: 0.654865, acc: 59.38%] [G loss: 1.999538]\n",
      "epoch:11 step:10313 [D loss: 0.600457, acc: 66.41%] [G loss: 2.081585]\n",
      "epoch:11 step:10314 [D loss: 0.640447, acc: 67.19%] [G loss: 2.034704]\n",
      "epoch:11 step:10315 [D loss: 0.654658, acc: 64.84%] [G loss: 1.983242]\n",
      "epoch:11 step:10316 [D loss: 0.629292, acc: 63.28%] [G loss: 2.066099]\n",
      "epoch:11 step:10317 [D loss: 0.596339, acc: 69.53%] [G loss: 2.239599]\n",
      "epoch:11 step:10318 [D loss: 0.616857, acc: 67.19%] [G loss: 2.027683]\n",
      "epoch:11 step:10319 [D loss: 0.606474, acc: 64.06%] [G loss: 2.086688]\n",
      "epoch:11 step:10320 [D loss: 0.682673, acc: 59.38%] [G loss: 2.064415]\n",
      "epoch:11 step:10321 [D loss: 0.580724, acc: 68.75%] [G loss: 2.169993]\n",
      "epoch:11 step:10322 [D loss: 0.558618, acc: 67.97%] [G loss: 2.251698]\n",
      "epoch:11 step:10323 [D loss: 0.666769, acc: 63.28%] [G loss: 2.264680]\n",
      "epoch:11 step:10324 [D loss: 0.626524, acc: 64.06%] [G loss: 2.134579]\n",
      "epoch:11 step:10325 [D loss: 0.601702, acc: 69.53%] [G loss: 1.976564]\n",
      "epoch:11 step:10326 [D loss: 0.641911, acc: 60.16%] [G loss: 2.098978]\n",
      "epoch:11 step:10327 [D loss: 0.661262, acc: 57.03%] [G loss: 1.942337]\n",
      "epoch:11 step:10328 [D loss: 0.620107, acc: 69.53%] [G loss: 1.926088]\n",
      "epoch:11 step:10329 [D loss: 0.609171, acc: 63.28%] [G loss: 2.173401]\n",
      "epoch:11 step:10330 [D loss: 0.621393, acc: 68.75%] [G loss: 2.273932]\n",
      "epoch:11 step:10331 [D loss: 0.675102, acc: 65.62%] [G loss: 2.049547]\n",
      "epoch:11 step:10332 [D loss: 0.560646, acc: 76.56%] [G loss: 2.256997]\n",
      "epoch:11 step:10333 [D loss: 0.613556, acc: 66.41%] [G loss: 1.913584]\n",
      "epoch:11 step:10334 [D loss: 0.613627, acc: 62.50%] [G loss: 2.174583]\n",
      "epoch:11 step:10335 [D loss: 0.628518, acc: 63.28%] [G loss: 2.008711]\n",
      "epoch:11 step:10336 [D loss: 0.579758, acc: 68.75%] [G loss: 2.071461]\n",
      "epoch:11 step:10337 [D loss: 0.694458, acc: 57.03%] [G loss: 2.012204]\n",
      "epoch:11 step:10338 [D loss: 0.634723, acc: 65.62%] [G loss: 1.932708]\n",
      "epoch:11 step:10339 [D loss: 0.662638, acc: 60.16%] [G loss: 1.919524]\n",
      "epoch:11 step:10340 [D loss: 0.624592, acc: 63.28%] [G loss: 2.036911]\n",
      "epoch:11 step:10341 [D loss: 0.685776, acc: 57.81%] [G loss: 1.923959]\n",
      "epoch:11 step:10342 [D loss: 0.616483, acc: 67.19%] [G loss: 1.925104]\n",
      "epoch:11 step:10343 [D loss: 0.627586, acc: 67.19%] [G loss: 2.087185]\n",
      "epoch:11 step:10344 [D loss: 0.572121, acc: 71.88%] [G loss: 2.189735]\n",
      "epoch:11 step:10345 [D loss: 0.638342, acc: 65.62%] [G loss: 2.009314]\n",
      "epoch:11 step:10346 [D loss: 0.552245, acc: 71.09%] [G loss: 1.994866]\n",
      "epoch:11 step:10347 [D loss: 0.584460, acc: 67.19%] [G loss: 2.155481]\n",
      "epoch:11 step:10348 [D loss: 0.604425, acc: 69.53%] [G loss: 2.091975]\n",
      "epoch:11 step:10349 [D loss: 0.594496, acc: 67.19%] [G loss: 2.059510]\n",
      "epoch:11 step:10350 [D loss: 0.641353, acc: 66.41%] [G loss: 1.952161]\n",
      "epoch:11 step:10351 [D loss: 0.643275, acc: 63.28%] [G loss: 2.096524]\n",
      "epoch:11 step:10352 [D loss: 0.647531, acc: 65.62%] [G loss: 1.976893]\n",
      "epoch:11 step:10353 [D loss: 0.616989, acc: 67.97%] [G loss: 1.977099]\n",
      "epoch:11 step:10354 [D loss: 0.615124, acc: 68.75%] [G loss: 2.004599]\n",
      "epoch:11 step:10355 [D loss: 0.612373, acc: 65.62%] [G loss: 2.055247]\n",
      "epoch:11 step:10356 [D loss: 0.589281, acc: 72.66%] [G loss: 2.034226]\n",
      "epoch:11 step:10357 [D loss: 0.625266, acc: 67.19%] [G loss: 2.037500]\n",
      "epoch:11 step:10358 [D loss: 0.640609, acc: 62.50%] [G loss: 2.048689]\n",
      "epoch:11 step:10359 [D loss: 0.638582, acc: 60.16%] [G loss: 2.083096]\n",
      "epoch:11 step:10360 [D loss: 0.575532, acc: 72.66%] [G loss: 2.000174]\n",
      "epoch:11 step:10361 [D loss: 0.638440, acc: 66.41%] [G loss: 2.215542]\n",
      "epoch:11 step:10362 [D loss: 0.611970, acc: 70.31%] [G loss: 2.088538]\n",
      "epoch:11 step:10363 [D loss: 0.611100, acc: 64.84%] [G loss: 2.138021]\n",
      "epoch:11 step:10364 [D loss: 0.677202, acc: 62.50%] [G loss: 2.365027]\n",
      "epoch:11 step:10365 [D loss: 0.658052, acc: 60.16%] [G loss: 1.985892]\n",
      "epoch:11 step:10366 [D loss: 0.680889, acc: 60.16%] [G loss: 1.992541]\n",
      "epoch:11 step:10367 [D loss: 0.661203, acc: 60.94%] [G loss: 2.098691]\n",
      "epoch:11 step:10368 [D loss: 0.600262, acc: 64.84%] [G loss: 2.141290]\n",
      "epoch:11 step:10369 [D loss: 0.631063, acc: 61.72%] [G loss: 2.087360]\n",
      "epoch:11 step:10370 [D loss: 0.669530, acc: 60.16%] [G loss: 2.033043]\n",
      "epoch:11 step:10371 [D loss: 0.590273, acc: 72.66%] [G loss: 2.215734]\n",
      "epoch:11 step:10372 [D loss: 0.637976, acc: 64.84%] [G loss: 1.989290]\n",
      "epoch:11 step:10373 [D loss: 0.647631, acc: 61.72%] [G loss: 1.947846]\n",
      "epoch:11 step:10374 [D loss: 0.575833, acc: 71.88%] [G loss: 1.949711]\n",
      "epoch:11 step:10375 [D loss: 0.597972, acc: 67.97%] [G loss: 2.111440]\n",
      "epoch:11 step:10376 [D loss: 0.594819, acc: 69.53%] [G loss: 2.068623]\n",
      "epoch:11 step:10377 [D loss: 0.596478, acc: 66.41%] [G loss: 2.217812]\n",
      "epoch:11 step:10378 [D loss: 0.689260, acc: 57.81%] [G loss: 1.947793]\n",
      "epoch:11 step:10379 [D loss: 0.597707, acc: 71.09%] [G loss: 2.042377]\n",
      "epoch:11 step:10380 [D loss: 0.705320, acc: 51.56%] [G loss: 1.936689]\n",
      "epoch:11 step:10381 [D loss: 0.667733, acc: 64.84%] [G loss: 2.059288]\n",
      "epoch:11 step:10382 [D loss: 0.609175, acc: 68.75%] [G loss: 1.984854]\n",
      "epoch:11 step:10383 [D loss: 0.556981, acc: 71.09%] [G loss: 2.267776]\n",
      "epoch:11 step:10384 [D loss: 0.653207, acc: 63.28%] [G loss: 2.175104]\n",
      "epoch:11 step:10385 [D loss: 0.709735, acc: 53.91%] [G loss: 1.897635]\n",
      "epoch:11 step:10386 [D loss: 0.620052, acc: 64.06%] [G loss: 1.869173]\n",
      "epoch:11 step:10387 [D loss: 0.623987, acc: 64.06%] [G loss: 1.988999]\n",
      "epoch:11 step:10388 [D loss: 0.653892, acc: 60.16%] [G loss: 1.868001]\n",
      "epoch:11 step:10389 [D loss: 0.647024, acc: 59.38%] [G loss: 2.016391]\n",
      "epoch:11 step:10390 [D loss: 0.649336, acc: 65.62%] [G loss: 2.033297]\n",
      "epoch:11 step:10391 [D loss: 0.632116, acc: 58.59%] [G loss: 2.044546]\n",
      "epoch:11 step:10392 [D loss: 0.598114, acc: 67.19%] [G loss: 1.971091]\n",
      "epoch:11 step:10393 [D loss: 0.656758, acc: 59.38%] [G loss: 1.939406]\n",
      "epoch:11 step:10394 [D loss: 0.625103, acc: 65.62%] [G loss: 2.024400]\n",
      "epoch:11 step:10395 [D loss: 0.640496, acc: 63.28%] [G loss: 1.874586]\n",
      "epoch:11 step:10396 [D loss: 0.601397, acc: 71.09%] [G loss: 2.047682]\n",
      "epoch:11 step:10397 [D loss: 0.601851, acc: 67.19%] [G loss: 1.832054]\n",
      "epoch:11 step:10398 [D loss: 0.650097, acc: 63.28%] [G loss: 1.969545]\n",
      "epoch:11 step:10399 [D loss: 0.609403, acc: 68.75%] [G loss: 2.101828]\n",
      "epoch:11 step:10400 [D loss: 0.583352, acc: 66.41%] [G loss: 2.039971]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.758160\n",
      "FID: 23.763519\n",
      "0 = 12.840036986970889\n",
      "1 = 0.08683996578178979\n",
      "2 = 0.8881999850273132\n",
      "3 = 0.9065999984741211\n",
      "4 = 0.8697999715805054\n",
      "5 = 0.8744212985038757\n",
      "6 = 0.9065999984741211\n",
      "7 = 7.326020028209661\n",
      "8 = 0.09787268496293043\n",
      "9 = 0.7638999819755554\n",
      "10 = 0.774399995803833\n",
      "11 = 0.7534000277519226\n",
      "12 = 0.7584720849990845\n",
      "13 = 0.774399995803833\n",
      "14 = 6.758184909820557\n",
      "15 = 9.285421371459961\n",
      "16 = 0.16056863963603973\n",
      "17 = 6.758159637451172\n",
      "18 = 23.763519287109375\n",
      "epoch:11 step:10401 [D loss: 0.607196, acc: 66.41%] [G loss: 2.060731]\n",
      "epoch:11 step:10402 [D loss: 0.676266, acc: 57.03%] [G loss: 1.875211]\n",
      "epoch:11 step:10403 [D loss: 0.613940, acc: 64.06%] [G loss: 1.909812]\n",
      "epoch:11 step:10404 [D loss: 0.634225, acc: 64.84%] [G loss: 2.050383]\n",
      "epoch:11 step:10405 [D loss: 0.634042, acc: 65.62%] [G loss: 1.951181]\n",
      "epoch:11 step:10406 [D loss: 0.650752, acc: 63.28%] [G loss: 2.008704]\n",
      "epoch:11 step:10407 [D loss: 0.631684, acc: 67.19%] [G loss: 2.116271]\n",
      "epoch:11 step:10408 [D loss: 0.631222, acc: 60.16%] [G loss: 2.047482]\n",
      "epoch:11 step:10409 [D loss: 0.670703, acc: 59.38%] [G loss: 2.154294]\n",
      "epoch:11 step:10410 [D loss: 0.621543, acc: 64.06%] [G loss: 2.043099]\n",
      "epoch:11 step:10411 [D loss: 0.661701, acc: 64.84%] [G loss: 1.919740]\n",
      "epoch:11 step:10412 [D loss: 0.671393, acc: 60.16%] [G loss: 1.853732]\n",
      "epoch:11 step:10413 [D loss: 0.590726, acc: 68.75%] [G loss: 2.077424]\n",
      "epoch:11 step:10414 [D loss: 0.627468, acc: 64.84%] [G loss: 1.979997]\n",
      "epoch:11 step:10415 [D loss: 0.741205, acc: 54.69%] [G loss: 1.898361]\n",
      "epoch:11 step:10416 [D loss: 0.654944, acc: 62.50%] [G loss: 1.853867]\n",
      "epoch:11 step:10417 [D loss: 0.697073, acc: 53.12%] [G loss: 1.833698]\n",
      "epoch:11 step:10418 [D loss: 0.595745, acc: 68.75%] [G loss: 1.947536]\n",
      "epoch:11 step:10419 [D loss: 0.625776, acc: 60.94%] [G loss: 2.117952]\n",
      "epoch:11 step:10420 [D loss: 0.641477, acc: 64.84%] [G loss: 2.065575]\n",
      "epoch:11 step:10421 [D loss: 0.590836, acc: 70.31%] [G loss: 2.239184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10422 [D loss: 0.602202, acc: 69.53%] [G loss: 2.128953]\n",
      "epoch:11 step:10423 [D loss: 0.634284, acc: 64.06%] [G loss: 2.073737]\n",
      "epoch:11 step:10424 [D loss: 0.615781, acc: 63.28%] [G loss: 2.269953]\n",
      "epoch:11 step:10425 [D loss: 0.589742, acc: 67.19%] [G loss: 1.978595]\n",
      "epoch:11 step:10426 [D loss: 0.586212, acc: 68.75%] [G loss: 2.466819]\n",
      "epoch:11 step:10427 [D loss: 0.687333, acc: 56.25%] [G loss: 2.067311]\n",
      "epoch:11 step:10428 [D loss: 0.700687, acc: 57.81%] [G loss: 2.117396]\n",
      "epoch:11 step:10429 [D loss: 0.632393, acc: 66.41%] [G loss: 2.181937]\n",
      "epoch:11 step:10430 [D loss: 0.678650, acc: 58.59%] [G loss: 2.026167]\n",
      "epoch:11 step:10431 [D loss: 0.649198, acc: 60.94%] [G loss: 2.133787]\n",
      "epoch:11 step:10432 [D loss: 0.621588, acc: 67.19%] [G loss: 1.791077]\n",
      "epoch:11 step:10433 [D loss: 0.619252, acc: 58.59%] [G loss: 2.093395]\n",
      "epoch:11 step:10434 [D loss: 0.675209, acc: 57.03%] [G loss: 1.867315]\n",
      "epoch:11 step:10435 [D loss: 0.605241, acc: 64.06%] [G loss: 1.946574]\n",
      "epoch:11 step:10436 [D loss: 0.648044, acc: 60.16%] [G loss: 1.842803]\n",
      "epoch:11 step:10437 [D loss: 0.625317, acc: 67.19%] [G loss: 2.282891]\n",
      "epoch:11 step:10438 [D loss: 0.678379, acc: 62.50%] [G loss: 2.168687]\n",
      "epoch:11 step:10439 [D loss: 0.622642, acc: 64.06%] [G loss: 1.995459]\n",
      "epoch:11 step:10440 [D loss: 0.649800, acc: 57.03%] [G loss: 1.883955]\n",
      "epoch:11 step:10441 [D loss: 0.695652, acc: 57.03%] [G loss: 1.833692]\n",
      "epoch:11 step:10442 [D loss: 0.634262, acc: 64.84%] [G loss: 1.969968]\n",
      "epoch:11 step:10443 [D loss: 0.644006, acc: 62.50%] [G loss: 1.914185]\n",
      "epoch:11 step:10444 [D loss: 0.704272, acc: 57.81%] [G loss: 1.841630]\n",
      "epoch:11 step:10445 [D loss: 0.606318, acc: 64.06%] [G loss: 1.933242]\n",
      "epoch:11 step:10446 [D loss: 0.680243, acc: 58.59%] [G loss: 1.902504]\n",
      "epoch:11 step:10447 [D loss: 0.599393, acc: 66.41%] [G loss: 2.061602]\n",
      "epoch:11 step:10448 [D loss: 0.641567, acc: 64.84%] [G loss: 1.941910]\n",
      "epoch:11 step:10449 [D loss: 0.681480, acc: 54.69%] [G loss: 1.938591]\n",
      "epoch:11 step:10450 [D loss: 0.657301, acc: 61.72%] [G loss: 1.757313]\n",
      "epoch:11 step:10451 [D loss: 0.606193, acc: 67.19%] [G loss: 2.014682]\n",
      "epoch:11 step:10452 [D loss: 0.676284, acc: 55.47%] [G loss: 1.968490]\n",
      "epoch:11 step:10453 [D loss: 0.611210, acc: 70.31%] [G loss: 1.996896]\n",
      "epoch:11 step:10454 [D loss: 0.697765, acc: 53.91%] [G loss: 1.882879]\n",
      "epoch:11 step:10455 [D loss: 0.672739, acc: 61.72%] [G loss: 1.813412]\n",
      "epoch:11 step:10456 [D loss: 0.559637, acc: 76.56%] [G loss: 2.052753]\n",
      "epoch:11 step:10457 [D loss: 0.616187, acc: 67.19%] [G loss: 1.979720]\n",
      "epoch:11 step:10458 [D loss: 0.621232, acc: 65.62%] [G loss: 2.297100]\n",
      "epoch:11 step:10459 [D loss: 0.631824, acc: 60.16%] [G loss: 2.095579]\n",
      "epoch:11 step:10460 [D loss: 0.710954, acc: 52.34%] [G loss: 1.947999]\n",
      "epoch:11 step:10461 [D loss: 0.618523, acc: 63.28%] [G loss: 2.213995]\n",
      "epoch:11 step:10462 [D loss: 0.641942, acc: 67.19%] [G loss: 2.020877]\n",
      "epoch:11 step:10463 [D loss: 0.606803, acc: 64.84%] [G loss: 2.080155]\n",
      "epoch:11 step:10464 [D loss: 0.632784, acc: 63.28%] [G loss: 2.131662]\n",
      "epoch:11 step:10465 [D loss: 0.663724, acc: 57.03%] [G loss: 2.066324]\n",
      "epoch:11 step:10466 [D loss: 0.609592, acc: 69.53%] [G loss: 2.134321]\n",
      "epoch:11 step:10467 [D loss: 0.666784, acc: 58.59%] [G loss: 1.977486]\n",
      "epoch:11 step:10468 [D loss: 0.651130, acc: 66.41%] [G loss: 2.044962]\n",
      "epoch:11 step:10469 [D loss: 0.576076, acc: 72.66%] [G loss: 2.117726]\n",
      "epoch:11 step:10470 [D loss: 0.610690, acc: 67.19%] [G loss: 2.052595]\n",
      "epoch:11 step:10471 [D loss: 0.575046, acc: 71.09%] [G loss: 1.951051]\n",
      "epoch:11 step:10472 [D loss: 0.639720, acc: 66.41%] [G loss: 2.248541]\n",
      "epoch:11 step:10473 [D loss: 0.596864, acc: 66.41%] [G loss: 2.017514]\n",
      "epoch:11 step:10474 [D loss: 0.612249, acc: 64.84%] [G loss: 2.004162]\n",
      "epoch:11 step:10475 [D loss: 0.605246, acc: 65.62%] [G loss: 2.002337]\n",
      "epoch:11 step:10476 [D loss: 0.678915, acc: 57.81%] [G loss: 2.269040]\n",
      "epoch:11 step:10477 [D loss: 0.735777, acc: 55.47%] [G loss: 1.883251]\n",
      "epoch:11 step:10478 [D loss: 0.607829, acc: 67.19%] [G loss: 2.121922]\n",
      "epoch:11 step:10479 [D loss: 0.683655, acc: 63.28%] [G loss: 1.962808]\n",
      "epoch:11 step:10480 [D loss: 0.666296, acc: 60.94%] [G loss: 1.867386]\n",
      "epoch:11 step:10481 [D loss: 0.655267, acc: 57.81%] [G loss: 1.935300]\n",
      "epoch:11 step:10482 [D loss: 0.709194, acc: 51.56%] [G loss: 1.772773]\n",
      "epoch:11 step:10483 [D loss: 0.611544, acc: 66.41%] [G loss: 1.847019]\n",
      "epoch:11 step:10484 [D loss: 0.704478, acc: 53.91%] [G loss: 1.759681]\n",
      "epoch:11 step:10485 [D loss: 0.678727, acc: 53.12%] [G loss: 1.774277]\n",
      "epoch:11 step:10486 [D loss: 0.665885, acc: 58.59%] [G loss: 1.760385]\n",
      "epoch:11 step:10487 [D loss: 0.675599, acc: 64.84%] [G loss: 1.833091]\n",
      "epoch:11 step:10488 [D loss: 0.616725, acc: 65.62%] [G loss: 1.954374]\n",
      "epoch:11 step:10489 [D loss: 0.674041, acc: 62.50%] [G loss: 1.841085]\n",
      "epoch:11 step:10490 [D loss: 0.636609, acc: 63.28%] [G loss: 1.874703]\n",
      "epoch:11 step:10491 [D loss: 0.644453, acc: 60.16%] [G loss: 1.953936]\n",
      "epoch:11 step:10492 [D loss: 0.672106, acc: 58.59%] [G loss: 1.886593]\n",
      "epoch:11 step:10493 [D loss: 0.699036, acc: 56.25%] [G loss: 1.862939]\n",
      "epoch:11 step:10494 [D loss: 0.683554, acc: 57.81%] [G loss: 2.001788]\n",
      "epoch:11 step:10495 [D loss: 0.673774, acc: 58.59%] [G loss: 1.960538]\n",
      "epoch:11 step:10496 [D loss: 0.703186, acc: 59.38%] [G loss: 1.860294]\n",
      "epoch:11 step:10497 [D loss: 0.617175, acc: 67.97%] [G loss: 1.823448]\n",
      "epoch:11 step:10498 [D loss: 0.685188, acc: 60.16%] [G loss: 1.908942]\n",
      "epoch:11 step:10499 [D loss: 0.624054, acc: 68.75%] [G loss: 2.138208]\n",
      "epoch:11 step:10500 [D loss: 0.618960, acc: 65.62%] [G loss: 2.054245]\n",
      "epoch:11 step:10501 [D loss: 0.650254, acc: 60.94%] [G loss: 2.249643]\n",
      "epoch:11 step:10502 [D loss: 0.616190, acc: 67.97%] [G loss: 1.962318]\n",
      "epoch:11 step:10503 [D loss: 0.687738, acc: 54.69%] [G loss: 1.842653]\n",
      "epoch:11 step:10504 [D loss: 0.627249, acc: 66.41%] [G loss: 1.905287]\n",
      "epoch:11 step:10505 [D loss: 0.634648, acc: 64.84%] [G loss: 2.022796]\n",
      "epoch:11 step:10506 [D loss: 0.648670, acc: 64.06%] [G loss: 2.093097]\n",
      "epoch:11 step:10507 [D loss: 0.642459, acc: 58.59%] [G loss: 1.861856]\n",
      "epoch:11 step:10508 [D loss: 0.690233, acc: 57.81%] [G loss: 1.926213]\n",
      "epoch:11 step:10509 [D loss: 0.644085, acc: 63.28%] [G loss: 1.812471]\n",
      "epoch:11 step:10510 [D loss: 0.636130, acc: 67.97%] [G loss: 2.000040]\n",
      "epoch:11 step:10511 [D loss: 0.663442, acc: 57.03%] [G loss: 1.875070]\n",
      "epoch:11 step:10512 [D loss: 0.652215, acc: 59.38%] [G loss: 1.791448]\n",
      "epoch:11 step:10513 [D loss: 0.633244, acc: 63.28%] [G loss: 2.134090]\n",
      "epoch:11 step:10514 [D loss: 0.567000, acc: 71.09%] [G loss: 2.355207]\n",
      "epoch:11 step:10515 [D loss: 0.595109, acc: 69.53%] [G loss: 2.312950]\n",
      "epoch:11 step:10516 [D loss: 0.624862, acc: 66.41%] [G loss: 2.323028]\n",
      "epoch:11 step:10517 [D loss: 0.653428, acc: 62.50%] [G loss: 1.938968]\n",
      "epoch:11 step:10518 [D loss: 0.647841, acc: 67.19%] [G loss: 1.822124]\n",
      "epoch:11 step:10519 [D loss: 0.654403, acc: 62.50%] [G loss: 1.799109]\n",
      "epoch:11 step:10520 [D loss: 0.660894, acc: 63.28%] [G loss: 1.856603]\n",
      "epoch:11 step:10521 [D loss: 0.667303, acc: 62.50%] [G loss: 1.926902]\n",
      "epoch:11 step:10522 [D loss: 0.643802, acc: 62.50%] [G loss: 1.845885]\n",
      "epoch:11 step:10523 [D loss: 0.599630, acc: 68.75%] [G loss: 2.104518]\n",
      "epoch:11 step:10524 [D loss: 0.602570, acc: 67.97%] [G loss: 2.187534]\n",
      "epoch:11 step:10525 [D loss: 0.551913, acc: 75.00%] [G loss: 2.294333]\n",
      "epoch:11 step:10526 [D loss: 0.595697, acc: 70.31%] [G loss: 2.287838]\n",
      "epoch:11 step:10527 [D loss: 0.734979, acc: 58.59%] [G loss: 1.829456]\n",
      "epoch:11 step:10528 [D loss: 0.623216, acc: 64.06%] [G loss: 2.034913]\n",
      "epoch:11 step:10529 [D loss: 0.603203, acc: 67.97%] [G loss: 2.085466]\n",
      "epoch:11 step:10530 [D loss: 0.639867, acc: 64.84%] [G loss: 2.190973]\n",
      "epoch:11 step:10531 [D loss: 0.663773, acc: 60.16%] [G loss: 1.982958]\n",
      "epoch:11 step:10532 [D loss: 0.669996, acc: 58.59%] [G loss: 2.106246]\n",
      "epoch:11 step:10533 [D loss: 0.666801, acc: 61.72%] [G loss: 1.942964]\n",
      "epoch:11 step:10534 [D loss: 0.637785, acc: 67.97%] [G loss: 1.899201]\n",
      "epoch:11 step:10535 [D loss: 0.624734, acc: 63.28%] [G loss: 1.921734]\n",
      "epoch:11 step:10536 [D loss: 0.584978, acc: 68.75%] [G loss: 2.150696]\n",
      "epoch:11 step:10537 [D loss: 0.583421, acc: 71.88%] [G loss: 2.258012]\n",
      "epoch:11 step:10538 [D loss: 0.554288, acc: 72.66%] [G loss: 2.245983]\n",
      "epoch:11 step:10539 [D loss: 0.583482, acc: 71.88%] [G loss: 2.493095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10540 [D loss: 0.612226, acc: 64.84%] [G loss: 2.220615]\n",
      "epoch:11 step:10541 [D loss: 0.613487, acc: 64.84%] [G loss: 2.186713]\n",
      "epoch:11 step:10542 [D loss: 0.620838, acc: 67.19%] [G loss: 2.082742]\n",
      "epoch:11 step:10543 [D loss: 0.584121, acc: 66.41%] [G loss: 2.115703]\n",
      "epoch:11 step:10544 [D loss: 0.618586, acc: 65.62%] [G loss: 2.113456]\n",
      "epoch:11 step:10545 [D loss: 0.575259, acc: 70.31%] [G loss: 2.117568]\n",
      "epoch:11 step:10546 [D loss: 0.712742, acc: 62.50%] [G loss: 2.065501]\n",
      "epoch:11 step:10547 [D loss: 0.608009, acc: 66.41%] [G loss: 2.013731]\n",
      "epoch:11 step:10548 [D loss: 0.687263, acc: 56.25%] [G loss: 1.987044]\n",
      "epoch:11 step:10549 [D loss: 0.603149, acc: 65.62%] [G loss: 2.147061]\n",
      "epoch:11 step:10550 [D loss: 0.583557, acc: 69.53%] [G loss: 2.024437]\n",
      "epoch:11 step:10551 [D loss: 0.633176, acc: 65.62%] [G loss: 2.134337]\n",
      "epoch:11 step:10552 [D loss: 0.628655, acc: 62.50%] [G loss: 2.080132]\n",
      "epoch:11 step:10553 [D loss: 0.661194, acc: 60.16%] [G loss: 1.986172]\n",
      "epoch:11 step:10554 [D loss: 0.651448, acc: 59.38%] [G loss: 2.020487]\n",
      "epoch:11 step:10555 [D loss: 0.602638, acc: 67.19%] [G loss: 2.020858]\n",
      "epoch:11 step:10556 [D loss: 0.634029, acc: 66.41%] [G loss: 1.974294]\n",
      "epoch:11 step:10557 [D loss: 0.753142, acc: 50.78%] [G loss: 1.751458]\n",
      "epoch:11 step:10558 [D loss: 0.658539, acc: 64.06%] [G loss: 1.941753]\n",
      "epoch:11 step:10559 [D loss: 0.671228, acc: 58.59%] [G loss: 1.871273]\n",
      "epoch:11 step:10560 [D loss: 0.617443, acc: 60.94%] [G loss: 1.926202]\n",
      "epoch:11 step:10561 [D loss: 0.651931, acc: 67.19%] [G loss: 1.986845]\n",
      "epoch:11 step:10562 [D loss: 0.599498, acc: 64.06%] [G loss: 1.967108]\n",
      "epoch:11 step:10563 [D loss: 0.640867, acc: 62.50%] [G loss: 1.977100]\n",
      "epoch:11 step:10564 [D loss: 0.686271, acc: 52.34%] [G loss: 1.908466]\n",
      "epoch:11 step:10565 [D loss: 0.611394, acc: 66.41%] [G loss: 1.967206]\n",
      "epoch:11 step:10566 [D loss: 0.612706, acc: 69.53%] [G loss: 2.014482]\n",
      "epoch:11 step:10567 [D loss: 0.646172, acc: 60.16%] [G loss: 1.907688]\n",
      "epoch:11 step:10568 [D loss: 0.623843, acc: 65.62%] [G loss: 2.161348]\n",
      "epoch:11 step:10569 [D loss: 0.620874, acc: 60.94%] [G loss: 2.153001]\n",
      "epoch:11 step:10570 [D loss: 0.593466, acc: 68.75%] [G loss: 2.191488]\n",
      "epoch:11 step:10571 [D loss: 0.595142, acc: 63.28%] [G loss: 2.286077]\n",
      "epoch:11 step:10572 [D loss: 0.697832, acc: 59.38%] [G loss: 1.935079]\n",
      "epoch:11 step:10573 [D loss: 0.600981, acc: 68.75%] [G loss: 1.967468]\n",
      "epoch:11 step:10574 [D loss: 0.679152, acc: 57.81%] [G loss: 1.759610]\n",
      "epoch:11 step:10575 [D loss: 0.695896, acc: 57.03%] [G loss: 1.936516]\n",
      "epoch:11 step:10576 [D loss: 0.649141, acc: 63.28%] [G loss: 1.952368]\n",
      "epoch:11 step:10577 [D loss: 0.610545, acc: 64.84%] [G loss: 2.273431]\n",
      "epoch:11 step:10578 [D loss: 0.617394, acc: 62.50%] [G loss: 2.077293]\n",
      "epoch:11 step:10579 [D loss: 0.592737, acc: 72.66%] [G loss: 2.204889]\n",
      "epoch:11 step:10580 [D loss: 0.593522, acc: 67.19%] [G loss: 2.149697]\n",
      "epoch:11 step:10581 [D loss: 0.614879, acc: 64.84%] [G loss: 2.112222]\n",
      "epoch:11 step:10582 [D loss: 0.627816, acc: 65.62%] [G loss: 2.073983]\n",
      "epoch:11 step:10583 [D loss: 0.671033, acc: 62.50%] [G loss: 2.163223]\n",
      "epoch:11 step:10584 [D loss: 0.617276, acc: 63.28%] [G loss: 1.917234]\n",
      "epoch:11 step:10585 [D loss: 0.659047, acc: 60.16%] [G loss: 1.856272]\n",
      "epoch:11 step:10586 [D loss: 0.680402, acc: 59.38%] [G loss: 1.936913]\n",
      "epoch:11 step:10587 [D loss: 0.618097, acc: 67.19%] [G loss: 2.096125]\n",
      "epoch:11 step:10588 [D loss: 0.623630, acc: 61.72%] [G loss: 1.822999]\n",
      "epoch:11 step:10589 [D loss: 0.672913, acc: 61.72%] [G loss: 1.921106]\n",
      "epoch:11 step:10590 [D loss: 0.646670, acc: 64.84%] [G loss: 2.097681]\n",
      "epoch:11 step:10591 [D loss: 0.614012, acc: 66.41%] [G loss: 2.194175]\n",
      "epoch:11 step:10592 [D loss: 0.673324, acc: 58.59%] [G loss: 1.961072]\n",
      "epoch:11 step:10593 [D loss: 0.691149, acc: 64.06%] [G loss: 1.931864]\n",
      "epoch:11 step:10594 [D loss: 0.594702, acc: 70.31%] [G loss: 2.032642]\n",
      "epoch:11 step:10595 [D loss: 0.609252, acc: 67.19%] [G loss: 1.875271]\n",
      "epoch:11 step:10596 [D loss: 0.636316, acc: 64.06%] [G loss: 1.857013]\n",
      "epoch:11 step:10597 [D loss: 0.634759, acc: 64.06%] [G loss: 1.922230]\n",
      "epoch:11 step:10598 [D loss: 0.634622, acc: 63.28%] [G loss: 1.962818]\n",
      "epoch:11 step:10599 [D loss: 0.632284, acc: 60.94%] [G loss: 1.916387]\n",
      "epoch:11 step:10600 [D loss: 0.604698, acc: 70.31%] [G loss: 2.063216]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.554259\n",
      "FID: 26.895294\n",
      "0 = 12.803968008422846\n",
      "1 = 0.08276907304002079\n",
      "2 = 0.902899980545044\n",
      "3 = 0.9083999991416931\n",
      "4 = 0.8974000215530396\n",
      "5 = 0.8985162973403931\n",
      "6 = 0.9083999991416931\n",
      "7 = 7.5414514825821\n",
      "8 = 0.10437918283452932\n",
      "9 = 0.7660999894142151\n",
      "10 = 0.767799973487854\n",
      "11 = 0.7644000053405762\n",
      "12 = 0.7651983499526978\n",
      "13 = 0.767799973487854\n",
      "14 = 6.554286956787109\n",
      "15 = 9.16092300415039\n",
      "16 = 0.1838444024324417\n",
      "17 = 6.554259300231934\n",
      "18 = 26.895294189453125\n",
      "epoch:11 step:10601 [D loss: 0.625332, acc: 64.06%] [G loss: 1.872896]\n",
      "epoch:11 step:10602 [D loss: 0.674010, acc: 59.38%] [G loss: 1.861918]\n",
      "epoch:11 step:10603 [D loss: 0.630880, acc: 65.62%] [G loss: 2.124313]\n",
      "epoch:11 step:10604 [D loss: 0.641276, acc: 67.19%] [G loss: 1.823005]\n",
      "epoch:11 step:10605 [D loss: 0.642582, acc: 61.72%] [G loss: 2.099082]\n",
      "epoch:11 step:10606 [D loss: 0.658503, acc: 58.59%] [G loss: 2.010757]\n",
      "epoch:11 step:10607 [D loss: 0.590150, acc: 67.19%] [G loss: 2.204506]\n",
      "epoch:11 step:10608 [D loss: 0.693847, acc: 55.47%] [G loss: 1.971560]\n",
      "epoch:11 step:10609 [D loss: 0.621084, acc: 66.41%] [G loss: 2.139116]\n",
      "epoch:11 step:10610 [D loss: 0.651447, acc: 57.81%] [G loss: 1.971556]\n",
      "epoch:11 step:10611 [D loss: 0.626481, acc: 67.19%] [G loss: 1.914212]\n",
      "epoch:11 step:10612 [D loss: 0.656641, acc: 64.06%] [G loss: 2.022815]\n",
      "epoch:11 step:10613 [D loss: 0.607097, acc: 64.84%] [G loss: 2.023125]\n",
      "epoch:11 step:10614 [D loss: 0.659958, acc: 61.72%] [G loss: 1.816270]\n",
      "epoch:11 step:10615 [D loss: 0.616840, acc: 64.84%] [G loss: 1.956652]\n",
      "epoch:11 step:10616 [D loss: 0.598405, acc: 71.88%] [G loss: 2.042866]\n",
      "epoch:11 step:10617 [D loss: 0.653395, acc: 64.84%] [G loss: 2.016532]\n",
      "epoch:11 step:10618 [D loss: 0.585989, acc: 69.53%] [G loss: 2.187686]\n",
      "epoch:11 step:10619 [D loss: 0.628910, acc: 64.06%] [G loss: 2.359663]\n",
      "epoch:11 step:10620 [D loss: 0.582271, acc: 68.75%] [G loss: 2.310434]\n",
      "epoch:11 step:10621 [D loss: 0.556063, acc: 70.31%] [G loss: 2.301597]\n",
      "epoch:11 step:10622 [D loss: 0.608958, acc: 64.06%] [G loss: 2.260779]\n",
      "epoch:11 step:10623 [D loss: 0.704803, acc: 54.69%] [G loss: 1.817658]\n",
      "epoch:11 step:10624 [D loss: 0.681188, acc: 53.91%] [G loss: 1.858026]\n",
      "epoch:11 step:10625 [D loss: 0.611978, acc: 63.28%] [G loss: 2.085620]\n",
      "epoch:11 step:10626 [D loss: 0.693229, acc: 56.25%] [G loss: 1.844166]\n",
      "epoch:11 step:10627 [D loss: 0.623596, acc: 62.50%] [G loss: 2.059093]\n",
      "epoch:11 step:10628 [D loss: 0.609855, acc: 68.75%] [G loss: 2.061310]\n",
      "epoch:11 step:10629 [D loss: 0.590379, acc: 64.84%] [G loss: 2.090996]\n",
      "epoch:11 step:10630 [D loss: 0.609420, acc: 72.66%] [G loss: 2.080923]\n",
      "epoch:11 step:10631 [D loss: 0.620869, acc: 63.28%] [G loss: 2.026688]\n",
      "epoch:11 step:10632 [D loss: 0.630090, acc: 61.72%] [G loss: 1.971027]\n",
      "epoch:11 step:10633 [D loss: 0.646231, acc: 65.62%] [G loss: 1.976396]\n",
      "epoch:11 step:10634 [D loss: 0.655534, acc: 64.06%] [G loss: 2.007472]\n",
      "epoch:11 step:10635 [D loss: 0.646093, acc: 64.06%] [G loss: 2.088977]\n",
      "epoch:11 step:10636 [D loss: 0.587764, acc: 67.97%] [G loss: 2.156817]\n",
      "epoch:11 step:10637 [D loss: 0.576251, acc: 71.88%] [G loss: 2.107492]\n",
      "epoch:11 step:10638 [D loss: 0.642522, acc: 60.94%] [G loss: 1.871761]\n",
      "epoch:11 step:10639 [D loss: 0.646120, acc: 65.62%] [G loss: 2.061969]\n",
      "epoch:11 step:10640 [D loss: 0.643716, acc: 64.06%] [G loss: 2.076445]\n",
      "epoch:11 step:10641 [D loss: 0.719393, acc: 54.69%] [G loss: 2.063104]\n",
      "epoch:11 step:10642 [D loss: 0.613414, acc: 63.28%] [G loss: 2.287311]\n",
      "epoch:11 step:10643 [D loss: 0.620019, acc: 67.97%] [G loss: 2.063630]\n",
      "epoch:11 step:10644 [D loss: 0.634220, acc: 67.97%] [G loss: 2.069765]\n",
      "epoch:11 step:10645 [D loss: 0.598938, acc: 67.19%] [G loss: 2.093662]\n",
      "epoch:11 step:10646 [D loss: 0.608164, acc: 64.84%] [G loss: 2.080139]\n",
      "epoch:11 step:10647 [D loss: 0.627008, acc: 69.53%] [G loss: 1.897732]\n",
      "epoch:11 step:10648 [D loss: 0.694090, acc: 58.59%] [G loss: 1.674944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10649 [D loss: 0.697969, acc: 53.91%] [G loss: 1.829596]\n",
      "epoch:11 step:10650 [D loss: 0.636694, acc: 62.50%] [G loss: 2.157428]\n",
      "epoch:11 step:10651 [D loss: 0.638864, acc: 64.06%] [G loss: 2.031908]\n",
      "epoch:11 step:10652 [D loss: 0.604375, acc: 68.75%] [G loss: 2.279541]\n",
      "epoch:11 step:10653 [D loss: 0.575574, acc: 67.97%] [G loss: 2.155170]\n",
      "epoch:11 step:10654 [D loss: 0.619167, acc: 64.06%] [G loss: 2.294278]\n",
      "epoch:11 step:10655 [D loss: 0.681208, acc: 62.50%] [G loss: 1.885571]\n",
      "epoch:11 step:10656 [D loss: 0.688375, acc: 56.25%] [G loss: 1.764333]\n",
      "epoch:11 step:10657 [D loss: 0.625718, acc: 64.84%] [G loss: 1.963688]\n",
      "epoch:11 step:10658 [D loss: 0.687349, acc: 57.81%] [G loss: 1.848238]\n",
      "epoch:11 step:10659 [D loss: 0.702254, acc: 56.25%] [G loss: 1.868232]\n",
      "epoch:11 step:10660 [D loss: 0.613314, acc: 64.06%] [G loss: 2.039055]\n",
      "epoch:11 step:10661 [D loss: 0.576094, acc: 71.09%] [G loss: 2.229337]\n",
      "epoch:11 step:10662 [D loss: 0.630273, acc: 64.84%] [G loss: 2.092980]\n",
      "epoch:11 step:10663 [D loss: 0.677724, acc: 60.94%] [G loss: 2.019713]\n",
      "epoch:11 step:10664 [D loss: 0.629254, acc: 62.50%] [G loss: 2.114124]\n",
      "epoch:11 step:10665 [D loss: 0.578221, acc: 67.19%] [G loss: 2.194111]\n",
      "epoch:11 step:10666 [D loss: 0.610106, acc: 66.41%] [G loss: 1.986713]\n",
      "epoch:11 step:10667 [D loss: 0.580157, acc: 67.97%] [G loss: 2.109028]\n",
      "epoch:11 step:10668 [D loss: 0.639233, acc: 66.41%] [G loss: 1.770667]\n",
      "epoch:11 step:10669 [D loss: 0.681211, acc: 57.81%] [G loss: 1.864429]\n",
      "epoch:11 step:10670 [D loss: 0.629834, acc: 64.84%] [G loss: 1.971353]\n",
      "epoch:11 step:10671 [D loss: 0.553920, acc: 76.56%] [G loss: 2.050273]\n",
      "epoch:11 step:10672 [D loss: 0.655708, acc: 62.50%] [G loss: 2.041092]\n",
      "epoch:11 step:10673 [D loss: 0.603764, acc: 66.41%] [G loss: 1.995848]\n",
      "epoch:11 step:10674 [D loss: 0.673271, acc: 58.59%] [G loss: 1.958180]\n",
      "epoch:11 step:10675 [D loss: 0.640755, acc: 57.03%] [G loss: 1.902413]\n",
      "epoch:11 step:10676 [D loss: 0.617747, acc: 65.62%] [G loss: 2.012132]\n",
      "epoch:11 step:10677 [D loss: 0.663049, acc: 64.06%] [G loss: 2.074666]\n",
      "epoch:11 step:10678 [D loss: 0.592986, acc: 67.97%] [G loss: 2.055907]\n",
      "epoch:11 step:10679 [D loss: 0.632285, acc: 64.84%] [G loss: 2.056183]\n",
      "epoch:11 step:10680 [D loss: 0.662354, acc: 59.38%] [G loss: 1.910280]\n",
      "epoch:11 step:10681 [D loss: 0.678695, acc: 60.16%] [G loss: 2.251215]\n",
      "epoch:11 step:10682 [D loss: 0.659799, acc: 60.94%] [G loss: 1.916301]\n",
      "epoch:11 step:10683 [D loss: 0.689651, acc: 62.50%] [G loss: 1.908253]\n",
      "epoch:11 step:10684 [D loss: 0.651521, acc: 60.16%] [G loss: 1.847485]\n",
      "epoch:11 step:10685 [D loss: 0.693028, acc: 62.50%] [G loss: 1.747875]\n",
      "epoch:11 step:10686 [D loss: 0.652321, acc: 62.50%] [G loss: 1.923823]\n",
      "epoch:11 step:10687 [D loss: 0.649285, acc: 64.06%] [G loss: 2.103079]\n",
      "epoch:11 step:10688 [D loss: 0.650992, acc: 62.50%] [G loss: 2.153324]\n",
      "epoch:11 step:10689 [D loss: 0.644393, acc: 62.50%] [G loss: 1.822677]\n",
      "epoch:11 step:10690 [D loss: 0.643043, acc: 62.50%] [G loss: 1.926556]\n",
      "epoch:11 step:10691 [D loss: 0.707626, acc: 60.16%] [G loss: 1.967237]\n",
      "epoch:11 step:10692 [D loss: 0.613630, acc: 66.41%] [G loss: 2.023835]\n",
      "epoch:11 step:10693 [D loss: 0.664926, acc: 60.94%] [G loss: 1.793736]\n",
      "epoch:11 step:10694 [D loss: 0.649362, acc: 64.06%] [G loss: 1.872119]\n",
      "epoch:11 step:10695 [D loss: 0.650388, acc: 59.38%] [G loss: 1.862750]\n",
      "epoch:11 step:10696 [D loss: 0.654874, acc: 59.38%] [G loss: 1.818560]\n",
      "epoch:11 step:10697 [D loss: 0.689946, acc: 60.16%] [G loss: 1.790965]\n",
      "epoch:11 step:10698 [D loss: 0.642231, acc: 69.53%] [G loss: 1.939127]\n",
      "epoch:11 step:10699 [D loss: 0.689393, acc: 58.59%] [G loss: 1.898786]\n",
      "epoch:11 step:10700 [D loss: 0.617189, acc: 66.41%] [G loss: 1.918846]\n",
      "epoch:11 step:10701 [D loss: 0.613783, acc: 65.62%] [G loss: 1.992988]\n",
      "epoch:11 step:10702 [D loss: 0.596246, acc: 65.62%] [G loss: 1.962111]\n",
      "epoch:11 step:10703 [D loss: 0.608502, acc: 67.97%] [G loss: 1.826744]\n",
      "epoch:11 step:10704 [D loss: 0.661771, acc: 64.06%] [G loss: 2.046793]\n",
      "epoch:11 step:10705 [D loss: 0.633211, acc: 70.31%] [G loss: 2.113057]\n",
      "epoch:11 step:10706 [D loss: 0.607932, acc: 68.75%] [G loss: 1.979747]\n",
      "epoch:11 step:10707 [D loss: 0.599605, acc: 64.84%] [G loss: 1.954758]\n",
      "epoch:11 step:10708 [D loss: 0.660403, acc: 60.94%] [G loss: 2.197411]\n",
      "epoch:11 step:10709 [D loss: 0.628219, acc: 68.75%] [G loss: 2.137121]\n",
      "epoch:11 step:10710 [D loss: 0.622819, acc: 66.41%] [G loss: 2.057946]\n",
      "epoch:11 step:10711 [D loss: 0.603823, acc: 68.75%] [G loss: 1.989595]\n",
      "epoch:11 step:10712 [D loss: 0.602578, acc: 67.19%] [G loss: 2.067958]\n",
      "epoch:11 step:10713 [D loss: 0.546062, acc: 72.66%] [G loss: 2.343585]\n",
      "epoch:11 step:10714 [D loss: 0.597972, acc: 65.62%] [G loss: 2.063981]\n",
      "epoch:11 step:10715 [D loss: 0.679216, acc: 58.59%] [G loss: 1.974520]\n",
      "epoch:11 step:10716 [D loss: 0.642554, acc: 62.50%] [G loss: 2.126369]\n",
      "epoch:11 step:10717 [D loss: 0.643807, acc: 66.41%] [G loss: 2.125593]\n",
      "epoch:11 step:10718 [D loss: 0.642904, acc: 65.62%] [G loss: 2.036225]\n",
      "epoch:11 step:10719 [D loss: 0.649886, acc: 65.62%] [G loss: 1.984974]\n",
      "epoch:11 step:10720 [D loss: 0.602190, acc: 68.75%] [G loss: 2.213288]\n",
      "epoch:11 step:10721 [D loss: 0.711058, acc: 58.59%] [G loss: 1.936780]\n",
      "epoch:11 step:10722 [D loss: 0.628304, acc: 67.19%] [G loss: 2.013952]\n",
      "epoch:11 step:10723 [D loss: 0.569221, acc: 71.88%] [G loss: 2.266410]\n",
      "epoch:11 step:10724 [D loss: 0.648843, acc: 61.72%] [G loss: 1.955326]\n",
      "epoch:11 step:10725 [D loss: 0.624522, acc: 65.62%] [G loss: 2.173350]\n",
      "epoch:11 step:10726 [D loss: 0.670086, acc: 53.91%] [G loss: 1.997908]\n",
      "epoch:11 step:10727 [D loss: 0.664882, acc: 61.72%] [G loss: 1.965151]\n",
      "epoch:11 step:10728 [D loss: 0.676533, acc: 59.38%] [G loss: 1.918507]\n",
      "epoch:11 step:10729 [D loss: 0.682501, acc: 56.25%] [G loss: 1.927089]\n",
      "epoch:11 step:10730 [D loss: 0.614493, acc: 64.84%] [G loss: 1.974359]\n",
      "epoch:11 step:10731 [D loss: 0.634873, acc: 63.28%] [G loss: 1.829687]\n",
      "epoch:11 step:10732 [D loss: 0.675680, acc: 60.16%] [G loss: 1.870741]\n",
      "epoch:11 step:10733 [D loss: 0.638911, acc: 64.06%] [G loss: 1.909551]\n",
      "epoch:11 step:10734 [D loss: 0.637505, acc: 65.62%] [G loss: 2.170452]\n",
      "epoch:11 step:10735 [D loss: 0.583994, acc: 68.75%] [G loss: 2.104211]\n",
      "epoch:11 step:10736 [D loss: 0.614193, acc: 68.75%] [G loss: 2.127840]\n",
      "epoch:11 step:10737 [D loss: 0.600163, acc: 70.31%] [G loss: 2.111046]\n",
      "epoch:11 step:10738 [D loss: 0.672380, acc: 57.81%] [G loss: 1.896971]\n",
      "epoch:11 step:10739 [D loss: 0.639959, acc: 63.28%] [G loss: 1.886848]\n",
      "epoch:11 step:10740 [D loss: 0.636470, acc: 67.97%] [G loss: 2.088661]\n",
      "epoch:11 step:10741 [D loss: 0.586110, acc: 71.09%] [G loss: 2.152399]\n",
      "epoch:11 step:10742 [D loss: 0.689961, acc: 57.81%] [G loss: 2.075147]\n",
      "epoch:11 step:10743 [D loss: 0.621156, acc: 67.97%] [G loss: 2.057136]\n",
      "epoch:11 step:10744 [D loss: 0.733281, acc: 55.47%] [G loss: 1.775843]\n",
      "epoch:11 step:10745 [D loss: 0.659705, acc: 60.16%] [G loss: 1.815474]\n",
      "epoch:11 step:10746 [D loss: 0.652219, acc: 63.28%] [G loss: 1.849499]\n",
      "epoch:11 step:10747 [D loss: 0.705355, acc: 51.56%] [G loss: 1.892500]\n",
      "epoch:11 step:10748 [D loss: 0.680914, acc: 59.38%] [G loss: 1.852173]\n",
      "epoch:11 step:10749 [D loss: 0.702276, acc: 55.47%] [G loss: 1.772530]\n",
      "epoch:11 step:10750 [D loss: 0.694143, acc: 57.03%] [G loss: 1.880148]\n",
      "epoch:11 step:10751 [D loss: 0.673678, acc: 52.34%] [G loss: 1.699704]\n",
      "epoch:11 step:10752 [D loss: 0.672842, acc: 56.25%] [G loss: 1.767115]\n",
      "epoch:11 step:10753 [D loss: 0.644567, acc: 66.41%] [G loss: 1.887398]\n",
      "epoch:11 step:10754 [D loss: 0.630163, acc: 59.38%] [G loss: 2.071028]\n",
      "epoch:11 step:10755 [D loss: 0.701237, acc: 55.47%] [G loss: 1.804677]\n",
      "epoch:11 step:10756 [D loss: 0.610090, acc: 71.09%] [G loss: 1.994373]\n",
      "epoch:11 step:10757 [D loss: 0.644400, acc: 60.94%] [G loss: 1.999455]\n",
      "epoch:11 step:10758 [D loss: 0.653363, acc: 61.72%] [G loss: 1.981813]\n",
      "epoch:11 step:10759 [D loss: 0.638081, acc: 65.62%] [G loss: 1.950562]\n",
      "epoch:11 step:10760 [D loss: 0.637919, acc: 62.50%] [G loss: 2.025327]\n",
      "epoch:11 step:10761 [D loss: 0.632602, acc: 64.84%] [G loss: 1.984197]\n",
      "epoch:11 step:10762 [D loss: 0.617842, acc: 60.94%] [G loss: 1.927461]\n",
      "epoch:11 step:10763 [D loss: 0.664927, acc: 60.16%] [G loss: 2.003541]\n",
      "epoch:11 step:10764 [D loss: 0.641506, acc: 60.94%] [G loss: 2.111018]\n",
      "epoch:11 step:10765 [D loss: 0.706647, acc: 57.03%] [G loss: 1.812953]\n",
      "epoch:11 step:10766 [D loss: 0.640925, acc: 60.94%] [G loss: 1.943896]\n",
      "epoch:11 step:10767 [D loss: 0.624889, acc: 63.28%] [G loss: 1.944350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10768 [D loss: 0.603476, acc: 64.06%] [G loss: 2.311375]\n",
      "epoch:11 step:10769 [D loss: 0.644880, acc: 64.84%] [G loss: 2.055987]\n",
      "epoch:11 step:10770 [D loss: 0.628376, acc: 67.19%] [G loss: 1.829387]\n",
      "epoch:11 step:10771 [D loss: 0.613948, acc: 69.53%] [G loss: 1.809634]\n",
      "epoch:11 step:10772 [D loss: 0.683348, acc: 59.38%] [G loss: 2.056503]\n",
      "epoch:11 step:10773 [D loss: 0.589633, acc: 72.66%] [G loss: 2.017705]\n",
      "epoch:11 step:10774 [D loss: 0.678052, acc: 57.81%] [G loss: 1.941679]\n",
      "epoch:11 step:10775 [D loss: 0.668082, acc: 57.81%] [G loss: 1.971688]\n",
      "epoch:11 step:10776 [D loss: 0.571181, acc: 74.22%] [G loss: 2.215309]\n",
      "epoch:11 step:10777 [D loss: 0.656567, acc: 63.28%] [G loss: 2.109109]\n",
      "epoch:11 step:10778 [D loss: 0.584845, acc: 66.41%] [G loss: 2.462990]\n",
      "epoch:11 step:10779 [D loss: 0.561733, acc: 67.97%] [G loss: 2.433856]\n",
      "epoch:11 step:10780 [D loss: 0.686711, acc: 55.47%] [G loss: 1.907987]\n",
      "epoch:11 step:10781 [D loss: 0.669744, acc: 64.84%] [G loss: 2.041226]\n",
      "epoch:11 step:10782 [D loss: 0.679521, acc: 57.03%] [G loss: 1.912285]\n",
      "epoch:11 step:10783 [D loss: 0.618683, acc: 71.88%] [G loss: 1.902742]\n",
      "epoch:11 step:10784 [D loss: 0.712148, acc: 53.12%] [G loss: 1.841187]\n",
      "epoch:11 step:10785 [D loss: 0.678232, acc: 57.03%] [G loss: 1.875664]\n",
      "epoch:11 step:10786 [D loss: 0.625997, acc: 62.50%] [G loss: 1.983435]\n",
      "epoch:11 step:10787 [D loss: 0.615869, acc: 63.28%] [G loss: 2.027794]\n",
      "epoch:11 step:10788 [D loss: 0.612205, acc: 67.97%] [G loss: 2.280138]\n",
      "epoch:11 step:10789 [D loss: 0.668814, acc: 62.50%] [G loss: 1.856206]\n",
      "epoch:11 step:10790 [D loss: 0.706690, acc: 57.81%] [G loss: 1.718793]\n",
      "epoch:11 step:10791 [D loss: 0.619725, acc: 64.84%] [G loss: 2.043315]\n",
      "epoch:11 step:10792 [D loss: 0.662042, acc: 57.81%] [G loss: 1.961839]\n",
      "epoch:11 step:10793 [D loss: 0.650672, acc: 64.06%] [G loss: 1.846022]\n",
      "epoch:11 step:10794 [D loss: 0.636192, acc: 60.94%] [G loss: 2.022192]\n",
      "epoch:11 step:10795 [D loss: 0.581103, acc: 71.09%] [G loss: 2.088618]\n",
      "epoch:11 step:10796 [D loss: 0.698972, acc: 60.16%] [G loss: 1.792656]\n",
      "epoch:11 step:10797 [D loss: 0.652031, acc: 65.62%] [G loss: 1.933866]\n",
      "epoch:11 step:10798 [D loss: 0.663860, acc: 64.06%] [G loss: 1.995167]\n",
      "epoch:11 step:10799 [D loss: 0.692784, acc: 59.38%] [G loss: 1.850092]\n",
      "epoch:11 step:10800 [D loss: 0.634106, acc: 62.50%] [G loss: 2.005158]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.802022\n",
      "FID: 21.778685\n",
      "0 = 12.71593334145543\n",
      "1 = 0.08309106578702097\n",
      "2 = 0.8991000056266785\n",
      "3 = 0.9196000099182129\n",
      "4 = 0.878600001335144\n",
      "5 = 0.8833813667297363\n",
      "6 = 0.9196000099182129\n",
      "7 = 7.128312905931474\n",
      "8 = 0.08973488223971864\n",
      "9 = 0.7559999823570251\n",
      "10 = 0.7653999924659729\n",
      "11 = 0.7465999722480774\n",
      "12 = 0.7512760162353516\n",
      "13 = 0.7653999924659729\n",
      "14 = 6.8020477294921875\n",
      "15 = 9.354211807250977\n",
      "16 = 0.1468316614627838\n",
      "17 = 6.802021503448486\n",
      "18 = 21.778684616088867\n",
      "epoch:11 step:10801 [D loss: 0.566675, acc: 67.97%] [G loss: 2.147961]\n",
      "epoch:11 step:10802 [D loss: 0.545831, acc: 73.44%] [G loss: 2.278795]\n",
      "epoch:11 step:10803 [D loss: 0.574717, acc: 70.31%] [G loss: 1.955423]\n",
      "epoch:11 step:10804 [D loss: 0.619435, acc: 67.97%] [G loss: 2.132752]\n",
      "epoch:11 step:10805 [D loss: 0.618731, acc: 65.62%] [G loss: 2.046239]\n",
      "epoch:11 step:10806 [D loss: 0.562084, acc: 72.66%] [G loss: 2.331169]\n",
      "epoch:11 step:10807 [D loss: 0.685005, acc: 57.03%] [G loss: 1.898408]\n",
      "epoch:11 step:10808 [D loss: 0.699980, acc: 57.03%] [G loss: 1.748312]\n",
      "epoch:11 step:10809 [D loss: 0.649379, acc: 63.28%] [G loss: 1.922909]\n",
      "epoch:11 step:10810 [D loss: 0.603905, acc: 68.75%] [G loss: 2.049821]\n",
      "epoch:11 step:10811 [D loss: 0.582344, acc: 68.75%] [G loss: 2.096727]\n",
      "epoch:11 step:10812 [D loss: 0.659736, acc: 60.16%] [G loss: 2.202428]\n",
      "epoch:11 step:10813 [D loss: 0.645051, acc: 62.50%] [G loss: 2.033564]\n",
      "epoch:11 step:10814 [D loss: 0.641114, acc: 68.75%] [G loss: 2.030993]\n",
      "epoch:11 step:10815 [D loss: 0.616613, acc: 66.41%] [G loss: 2.171974]\n",
      "epoch:11 step:10816 [D loss: 0.653487, acc: 63.28%] [G loss: 1.886910]\n",
      "epoch:11 step:10817 [D loss: 0.632882, acc: 65.62%] [G loss: 1.793630]\n",
      "epoch:11 step:10818 [D loss: 0.699029, acc: 57.81%] [G loss: 1.786503]\n",
      "epoch:11 step:10819 [D loss: 0.628972, acc: 62.50%] [G loss: 1.911175]\n",
      "epoch:11 step:10820 [D loss: 0.609723, acc: 63.28%] [G loss: 2.018464]\n",
      "epoch:11 step:10821 [D loss: 0.613536, acc: 66.41%] [G loss: 1.803522]\n",
      "epoch:11 step:10822 [D loss: 0.696454, acc: 50.78%] [G loss: 1.934573]\n",
      "epoch:11 step:10823 [D loss: 0.658899, acc: 60.94%] [G loss: 2.073745]\n",
      "epoch:11 step:10824 [D loss: 0.677569, acc: 62.50%] [G loss: 1.983307]\n",
      "epoch:11 step:10825 [D loss: 0.621458, acc: 61.72%] [G loss: 2.057253]\n",
      "epoch:11 step:10826 [D loss: 0.607290, acc: 64.84%] [G loss: 2.030451]\n",
      "epoch:11 step:10827 [D loss: 0.587780, acc: 67.97%] [G loss: 2.042830]\n",
      "epoch:11 step:10828 [D loss: 0.612508, acc: 67.97%] [G loss: 2.033051]\n",
      "epoch:11 step:10829 [D loss: 0.651593, acc: 63.28%] [G loss: 2.308077]\n",
      "epoch:11 step:10830 [D loss: 0.644348, acc: 64.06%] [G loss: 2.208353]\n",
      "epoch:11 step:10831 [D loss: 0.641082, acc: 62.50%] [G loss: 1.966379]\n",
      "epoch:11 step:10832 [D loss: 0.655515, acc: 65.62%] [G loss: 1.977503]\n",
      "epoch:11 step:10833 [D loss: 0.668489, acc: 53.91%] [G loss: 1.902227]\n",
      "epoch:11 step:10834 [D loss: 0.652653, acc: 64.06%] [G loss: 1.913475]\n",
      "epoch:11 step:10835 [D loss: 0.722462, acc: 57.81%] [G loss: 1.798456]\n",
      "epoch:11 step:10836 [D loss: 0.658252, acc: 54.69%] [G loss: 1.812419]\n",
      "epoch:11 step:10837 [D loss: 0.708564, acc: 53.12%] [G loss: 2.015119]\n",
      "epoch:11 step:10838 [D loss: 0.667949, acc: 60.94%] [G loss: 1.912639]\n",
      "epoch:11 step:10839 [D loss: 0.632921, acc: 66.41%] [G loss: 2.197680]\n",
      "epoch:11 step:10840 [D loss: 0.708349, acc: 53.12%] [G loss: 1.962394]\n",
      "epoch:11 step:10841 [D loss: 0.573132, acc: 71.09%] [G loss: 2.059724]\n",
      "epoch:11 step:10842 [D loss: 0.618748, acc: 67.19%] [G loss: 1.820858]\n",
      "epoch:11 step:10843 [D loss: 0.620337, acc: 67.97%] [G loss: 2.091088]\n",
      "epoch:11 step:10844 [D loss: 0.625827, acc: 64.84%] [G loss: 1.971750]\n",
      "epoch:11 step:10845 [D loss: 0.677308, acc: 59.38%] [G loss: 1.865600]\n",
      "epoch:11 step:10846 [D loss: 0.722605, acc: 56.25%] [G loss: 1.973177]\n",
      "epoch:11 step:10847 [D loss: 0.627969, acc: 61.72%] [G loss: 2.054047]\n",
      "epoch:11 step:10848 [D loss: 0.658590, acc: 61.72%] [G loss: 1.983176]\n",
      "epoch:11 step:10849 [D loss: 0.656752, acc: 60.16%] [G loss: 1.816243]\n",
      "epoch:11 step:10850 [D loss: 0.658217, acc: 57.03%] [G loss: 1.971464]\n",
      "epoch:11 step:10851 [D loss: 0.683963, acc: 52.34%] [G loss: 1.957316]\n",
      "epoch:11 step:10852 [D loss: 0.608520, acc: 64.84%] [G loss: 2.021885]\n",
      "epoch:11 step:10853 [D loss: 0.656529, acc: 60.16%] [G loss: 1.879384]\n",
      "epoch:11 step:10854 [D loss: 0.593189, acc: 67.19%] [G loss: 2.000932]\n",
      "epoch:11 step:10855 [D loss: 0.640619, acc: 62.50%] [G loss: 2.022307]\n",
      "epoch:11 step:10856 [D loss: 0.625926, acc: 62.50%] [G loss: 2.034510]\n",
      "epoch:11 step:10857 [D loss: 0.622266, acc: 65.62%] [G loss: 1.974610]\n",
      "epoch:11 step:10858 [D loss: 0.640564, acc: 67.97%] [G loss: 2.038702]\n",
      "epoch:11 step:10859 [D loss: 0.584135, acc: 75.78%] [G loss: 1.996425]\n",
      "epoch:11 step:10860 [D loss: 0.628304, acc: 62.50%] [G loss: 2.068274]\n",
      "epoch:11 step:10861 [D loss: 0.620482, acc: 58.59%] [G loss: 2.200548]\n",
      "epoch:11 step:10862 [D loss: 0.592934, acc: 71.88%] [G loss: 2.202482]\n",
      "epoch:11 step:10863 [D loss: 0.516058, acc: 79.69%] [G loss: 2.231669]\n",
      "epoch:11 step:10864 [D loss: 0.609365, acc: 65.62%] [G loss: 2.125972]\n",
      "epoch:11 step:10865 [D loss: 0.656798, acc: 64.06%] [G loss: 2.145821]\n",
      "epoch:11 step:10866 [D loss: 0.652555, acc: 60.94%] [G loss: 1.972309]\n",
      "epoch:11 step:10867 [D loss: 0.679258, acc: 60.94%] [G loss: 2.025214]\n",
      "epoch:11 step:10868 [D loss: 0.599619, acc: 71.88%] [G loss: 2.120145]\n",
      "epoch:11 step:10869 [D loss: 0.649462, acc: 66.41%] [G loss: 1.940528]\n",
      "epoch:11 step:10870 [D loss: 0.670237, acc: 63.28%] [G loss: 1.850824]\n",
      "epoch:11 step:10871 [D loss: 0.574511, acc: 73.44%] [G loss: 2.069092]\n",
      "epoch:11 step:10872 [D loss: 0.588134, acc: 71.88%] [G loss: 1.968049]\n",
      "epoch:11 step:10873 [D loss: 0.670465, acc: 62.50%] [G loss: 1.870919]\n",
      "epoch:11 step:10874 [D loss: 0.622624, acc: 55.47%] [G loss: 1.912590]\n",
      "epoch:11 step:10875 [D loss: 0.646414, acc: 64.06%] [G loss: 1.881663]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10876 [D loss: 0.651009, acc: 64.06%] [G loss: 1.826558]\n",
      "epoch:11 step:10877 [D loss: 0.615081, acc: 67.19%] [G loss: 1.857236]\n",
      "epoch:11 step:10878 [D loss: 0.605626, acc: 68.75%] [G loss: 1.887269]\n",
      "epoch:11 step:10879 [D loss: 0.717024, acc: 56.25%] [G loss: 1.950677]\n",
      "epoch:11 step:10880 [D loss: 0.675881, acc: 59.38%] [G loss: 1.843949]\n",
      "epoch:11 step:10881 [D loss: 0.589373, acc: 71.09%] [G loss: 2.015130]\n",
      "epoch:11 step:10882 [D loss: 0.637135, acc: 65.62%] [G loss: 2.042325]\n",
      "epoch:11 step:10883 [D loss: 0.672613, acc: 58.59%] [G loss: 1.831823]\n",
      "epoch:11 step:10884 [D loss: 0.692928, acc: 55.47%] [G loss: 1.948106]\n",
      "epoch:11 step:10885 [D loss: 0.736599, acc: 48.44%] [G loss: 1.829247]\n",
      "epoch:11 step:10886 [D loss: 0.706969, acc: 53.91%] [G loss: 1.814693]\n",
      "epoch:11 step:10887 [D loss: 0.633799, acc: 66.41%] [G loss: 1.953710]\n",
      "epoch:11 step:10888 [D loss: 0.675353, acc: 60.94%] [G loss: 1.894825]\n",
      "epoch:11 step:10889 [D loss: 0.632802, acc: 59.38%] [G loss: 2.009280]\n",
      "epoch:11 step:10890 [D loss: 0.628536, acc: 67.19%] [G loss: 1.936298]\n",
      "epoch:11 step:10891 [D loss: 0.638620, acc: 64.06%] [G loss: 1.888626]\n",
      "epoch:11 step:10892 [D loss: 0.703785, acc: 51.56%] [G loss: 1.961907]\n",
      "epoch:11 step:10893 [D loss: 0.627418, acc: 66.41%] [G loss: 1.815193]\n",
      "epoch:11 step:10894 [D loss: 0.637607, acc: 62.50%] [G loss: 2.025123]\n",
      "epoch:11 step:10895 [D loss: 0.670313, acc: 57.03%] [G loss: 2.032732]\n",
      "epoch:11 step:10896 [D loss: 0.606086, acc: 69.53%] [G loss: 1.905774]\n",
      "epoch:11 step:10897 [D loss: 0.655760, acc: 59.38%] [G loss: 1.905809]\n",
      "epoch:11 step:10898 [D loss: 0.613647, acc: 64.06%] [G loss: 1.997428]\n",
      "epoch:11 step:10899 [D loss: 0.576992, acc: 66.41%] [G loss: 2.051608]\n",
      "epoch:11 step:10900 [D loss: 0.632947, acc: 64.06%] [G loss: 2.086007]\n",
      "epoch:11 step:10901 [D loss: 0.628037, acc: 67.97%] [G loss: 1.822490]\n",
      "epoch:11 step:10902 [D loss: 0.634865, acc: 64.84%] [G loss: 1.911765]\n",
      "epoch:11 step:10903 [D loss: 0.630984, acc: 61.72%] [G loss: 1.905429]\n",
      "epoch:11 step:10904 [D loss: 0.673926, acc: 57.03%] [G loss: 1.863780]\n",
      "epoch:11 step:10905 [D loss: 0.629860, acc: 65.62%] [G loss: 1.919484]\n",
      "epoch:11 step:10906 [D loss: 0.702132, acc: 54.69%] [G loss: 1.904819]\n",
      "epoch:11 step:10907 [D loss: 0.673801, acc: 56.25%] [G loss: 1.853370]\n",
      "epoch:11 step:10908 [D loss: 0.618922, acc: 64.84%] [G loss: 1.878219]\n",
      "epoch:11 step:10909 [D loss: 0.655582, acc: 64.84%] [G loss: 1.972486]\n",
      "epoch:11 step:10910 [D loss: 0.591988, acc: 68.75%] [G loss: 1.969187]\n",
      "epoch:11 step:10911 [D loss: 0.652577, acc: 59.38%] [G loss: 1.913999]\n",
      "epoch:11 step:10912 [D loss: 0.613540, acc: 65.62%] [G loss: 2.010971]\n",
      "epoch:11 step:10913 [D loss: 0.665229, acc: 59.38%] [G loss: 2.022662]\n",
      "epoch:11 step:10914 [D loss: 0.679418, acc: 59.38%] [G loss: 1.897784]\n",
      "epoch:11 step:10915 [D loss: 0.620780, acc: 65.62%] [G loss: 2.044943]\n",
      "epoch:11 step:10916 [D loss: 0.606177, acc: 64.84%] [G loss: 2.064041]\n",
      "epoch:11 step:10917 [D loss: 0.614995, acc: 63.28%] [G loss: 2.114560]\n",
      "epoch:11 step:10918 [D loss: 0.695639, acc: 62.50%] [G loss: 1.806007]\n",
      "epoch:11 step:10919 [D loss: 0.664123, acc: 60.94%] [G loss: 1.890828]\n",
      "epoch:11 step:10920 [D loss: 0.623478, acc: 60.94%] [G loss: 2.009797]\n",
      "epoch:11 step:10921 [D loss: 0.649213, acc: 62.50%] [G loss: 1.826330]\n",
      "epoch:11 step:10922 [D loss: 0.714867, acc: 51.56%] [G loss: 1.732602]\n",
      "epoch:11 step:10923 [D loss: 0.644860, acc: 60.16%] [G loss: 1.953631]\n",
      "epoch:11 step:10924 [D loss: 0.676853, acc: 58.59%] [G loss: 1.890805]\n",
      "epoch:11 step:10925 [D loss: 0.625449, acc: 63.28%] [G loss: 1.907751]\n",
      "epoch:11 step:10926 [D loss: 0.741350, acc: 55.47%] [G loss: 1.807863]\n",
      "epoch:11 step:10927 [D loss: 0.645152, acc: 64.06%] [G loss: 2.056123]\n",
      "epoch:11 step:10928 [D loss: 0.604796, acc: 69.53%] [G loss: 1.891881]\n",
      "epoch:11 step:10929 [D loss: 0.631495, acc: 64.84%] [G loss: 2.017968]\n",
      "epoch:11 step:10930 [D loss: 0.597749, acc: 67.19%] [G loss: 1.881962]\n",
      "epoch:11 step:10931 [D loss: 0.601393, acc: 66.41%] [G loss: 2.145912]\n",
      "epoch:11 step:10932 [D loss: 0.683789, acc: 53.91%] [G loss: 1.831532]\n",
      "epoch:11 step:10933 [D loss: 0.642975, acc: 65.62%] [G loss: 2.048246]\n",
      "epoch:11 step:10934 [D loss: 0.587215, acc: 71.09%] [G loss: 1.919869]\n",
      "epoch:11 step:10935 [D loss: 0.664580, acc: 59.38%] [G loss: 1.940389]\n",
      "epoch:11 step:10936 [D loss: 0.610511, acc: 74.22%] [G loss: 1.911273]\n",
      "epoch:11 step:10937 [D loss: 0.643183, acc: 61.72%] [G loss: 1.921389]\n",
      "epoch:11 step:10938 [D loss: 0.630675, acc: 62.50%] [G loss: 2.111132]\n",
      "epoch:11 step:10939 [D loss: 0.607564, acc: 69.53%] [G loss: 1.997700]\n",
      "epoch:11 step:10940 [D loss: 0.626654, acc: 62.50%] [G loss: 2.055019]\n",
      "epoch:11 step:10941 [D loss: 0.616925, acc: 67.19%] [G loss: 2.087615]\n",
      "epoch:11 step:10942 [D loss: 0.567231, acc: 71.88%] [G loss: 2.249214]\n",
      "epoch:11 step:10943 [D loss: 0.631509, acc: 67.19%] [G loss: 1.982333]\n",
      "epoch:11 step:10944 [D loss: 0.654608, acc: 64.06%] [G loss: 2.061852]\n",
      "epoch:11 step:10945 [D loss: 0.620184, acc: 65.62%] [G loss: 2.088372]\n",
      "epoch:11 step:10946 [D loss: 0.654327, acc: 66.41%] [G loss: 2.155115]\n",
      "epoch:11 step:10947 [D loss: 0.649400, acc: 58.59%] [G loss: 2.237388]\n",
      "epoch:11 step:10948 [D loss: 0.642017, acc: 68.75%] [G loss: 2.183328]\n",
      "epoch:11 step:10949 [D loss: 0.616284, acc: 66.41%] [G loss: 2.187730]\n",
      "epoch:11 step:10950 [D loss: 0.627581, acc: 65.62%] [G loss: 1.904517]\n",
      "epoch:11 step:10951 [D loss: 0.617241, acc: 71.09%] [G loss: 1.965322]\n",
      "epoch:11 step:10952 [D loss: 0.609124, acc: 71.88%] [G loss: 2.197651]\n",
      "epoch:11 step:10953 [D loss: 0.586662, acc: 67.19%] [G loss: 2.201464]\n",
      "epoch:11 step:10954 [D loss: 0.560942, acc: 70.31%] [G loss: 2.214633]\n",
      "epoch:11 step:10955 [D loss: 0.615221, acc: 72.66%] [G loss: 2.473766]\n",
      "epoch:11 step:10956 [D loss: 0.578074, acc: 69.53%] [G loss: 2.329025]\n",
      "epoch:11 step:10957 [D loss: 0.587258, acc: 67.97%] [G loss: 2.257913]\n",
      "epoch:11 step:10958 [D loss: 0.560129, acc: 71.09%] [G loss: 2.067651]\n",
      "epoch:11 step:10959 [D loss: 0.721207, acc: 57.81%] [G loss: 1.923586]\n",
      "epoch:11 step:10960 [D loss: 0.656036, acc: 61.72%] [G loss: 1.891041]\n",
      "epoch:11 step:10961 [D loss: 0.681757, acc: 60.16%] [G loss: 2.208376]\n",
      "epoch:11 step:10962 [D loss: 0.640247, acc: 61.72%] [G loss: 2.010864]\n",
      "epoch:11 step:10963 [D loss: 0.668309, acc: 61.72%] [G loss: 2.090830]\n",
      "epoch:11 step:10964 [D loss: 0.644831, acc: 62.50%] [G loss: 1.839684]\n",
      "epoch:11 step:10965 [D loss: 0.627171, acc: 65.62%] [G loss: 1.875081]\n",
      "epoch:11 step:10966 [D loss: 0.669317, acc: 58.59%] [G loss: 1.891967]\n",
      "epoch:11 step:10967 [D loss: 0.656337, acc: 67.19%] [G loss: 1.881788]\n",
      "epoch:11 step:10968 [D loss: 0.629200, acc: 69.53%] [G loss: 2.026094]\n",
      "epoch:11 step:10969 [D loss: 0.606935, acc: 72.66%] [G loss: 2.067360]\n",
      "epoch:11 step:10970 [D loss: 0.687699, acc: 57.81%] [G loss: 1.977462]\n",
      "epoch:11 step:10971 [D loss: 0.660881, acc: 60.16%] [G loss: 1.846535]\n",
      "epoch:11 step:10972 [D loss: 0.626702, acc: 64.84%] [G loss: 1.842010]\n",
      "epoch:11 step:10973 [D loss: 0.642176, acc: 68.75%] [G loss: 1.924603]\n",
      "epoch:11 step:10974 [D loss: 0.679850, acc: 58.59%] [G loss: 1.704603]\n",
      "epoch:11 step:10975 [D loss: 0.672436, acc: 61.72%] [G loss: 2.019884]\n",
      "epoch:11 step:10976 [D loss: 0.674622, acc: 58.59%] [G loss: 1.822937]\n",
      "epoch:11 step:10977 [D loss: 0.658191, acc: 61.72%] [G loss: 1.924941]\n",
      "epoch:11 step:10978 [D loss: 0.607307, acc: 68.75%] [G loss: 1.958864]\n",
      "epoch:11 step:10979 [D loss: 0.675151, acc: 50.78%] [G loss: 1.891390]\n",
      "epoch:11 step:10980 [D loss: 0.676580, acc: 59.38%] [G loss: 1.913024]\n",
      "epoch:11 step:10981 [D loss: 0.634513, acc: 64.06%] [G loss: 1.973263]\n",
      "epoch:11 step:10982 [D loss: 0.618364, acc: 66.41%] [G loss: 1.888478]\n",
      "epoch:11 step:10983 [D loss: 0.655758, acc: 61.72%] [G loss: 1.941089]\n",
      "epoch:11 step:10984 [D loss: 0.604869, acc: 69.53%] [G loss: 2.106540]\n",
      "epoch:11 step:10985 [D loss: 0.662930, acc: 59.38%] [G loss: 1.873648]\n",
      "epoch:11 step:10986 [D loss: 0.572191, acc: 70.31%] [G loss: 2.112938]\n",
      "epoch:11 step:10987 [D loss: 0.603446, acc: 69.53%] [G loss: 2.057014]\n",
      "epoch:11 step:10988 [D loss: 0.607170, acc: 67.19%] [G loss: 2.126254]\n",
      "epoch:11 step:10989 [D loss: 0.656900, acc: 64.06%] [G loss: 1.808915]\n",
      "epoch:11 step:10990 [D loss: 0.607964, acc: 66.41%] [G loss: 2.165049]\n",
      "epoch:11 step:10991 [D loss: 0.623267, acc: 64.06%] [G loss: 1.903275]\n",
      "epoch:11 step:10992 [D loss: 0.642262, acc: 64.06%] [G loss: 1.901574]\n",
      "epoch:11 step:10993 [D loss: 0.660030, acc: 57.81%] [G loss: 2.076485]\n",
      "epoch:11 step:10994 [D loss: 0.661896, acc: 64.06%] [G loss: 1.946504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10995 [D loss: 0.669719, acc: 60.94%] [G loss: 1.928372]\n",
      "epoch:11 step:10996 [D loss: 0.629323, acc: 64.84%] [G loss: 1.927140]\n",
      "epoch:11 step:10997 [D loss: 0.680337, acc: 57.03%] [G loss: 1.976468]\n",
      "epoch:11 step:10998 [D loss: 0.641822, acc: 62.50%] [G loss: 2.011100]\n",
      "epoch:11 step:10999 [D loss: 0.597698, acc: 66.41%] [G loss: 2.086421]\n",
      "epoch:11 step:11000 [D loss: 0.594193, acc: 73.44%] [G loss: 2.103084]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.764251\n",
      "FID: 21.675665\n",
      "0 = 12.89359412107472\n",
      "1 = 0.08777254437754889\n",
      "2 = 0.8981999754905701\n",
      "3 = 0.9020000100135803\n",
      "4 = 0.8944000005722046\n",
      "5 = 0.8951964974403381\n",
      "6 = 0.9020000100135803\n",
      "7 = 7.243856570446516\n",
      "8 = 0.09063241862482835\n",
      "9 = 0.7587000131607056\n",
      "10 = 0.7656000256538391\n",
      "11 = 0.751800000667572\n",
      "12 = 0.7551785111427307\n",
      "13 = 0.7656000256538391\n",
      "14 = 6.764277458190918\n",
      "15 = 9.418317794799805\n",
      "16 = 0.1347610503435135\n",
      "17 = 6.764251232147217\n",
      "18 = 21.6756649017334\n",
      "epoch:11 step:11001 [D loss: 0.614007, acc: 64.84%] [G loss: 2.141481]\n",
      "epoch:11 step:11002 [D loss: 0.636076, acc: 62.50%] [G loss: 1.991827]\n",
      "epoch:11 step:11003 [D loss: 0.678743, acc: 60.16%] [G loss: 1.989128]\n",
      "epoch:11 step:11004 [D loss: 0.617853, acc: 64.84%] [G loss: 1.877445]\n",
      "epoch:11 step:11005 [D loss: 0.671711, acc: 57.03%] [G loss: 1.949074]\n",
      "epoch:11 step:11006 [D loss: 0.644222, acc: 67.97%] [G loss: 2.044871]\n",
      "epoch:11 step:11007 [D loss: 0.594559, acc: 68.75%] [G loss: 1.986548]\n",
      "epoch:11 step:11008 [D loss: 0.624300, acc: 63.28%] [G loss: 1.998090]\n",
      "epoch:11 step:11009 [D loss: 0.651247, acc: 61.72%] [G loss: 1.824391]\n",
      "epoch:11 step:11010 [D loss: 0.645620, acc: 64.84%] [G loss: 1.959459]\n",
      "epoch:11 step:11011 [D loss: 0.634459, acc: 58.59%] [G loss: 1.783328]\n",
      "epoch:11 step:11012 [D loss: 0.652111, acc: 61.72%] [G loss: 1.958284]\n",
      "epoch:11 step:11013 [D loss: 0.641880, acc: 61.72%] [G loss: 2.026543]\n",
      "epoch:11 step:11014 [D loss: 0.540742, acc: 72.66%] [G loss: 2.236249]\n",
      "epoch:11 step:11015 [D loss: 0.573743, acc: 69.53%] [G loss: 2.246560]\n",
      "epoch:11 step:11016 [D loss: 0.591742, acc: 71.09%] [G loss: 2.341629]\n",
      "epoch:11 step:11017 [D loss: 0.732144, acc: 51.56%] [G loss: 1.938918]\n",
      "epoch:11 step:11018 [D loss: 0.657145, acc: 58.59%] [G loss: 1.992526]\n",
      "epoch:11 step:11019 [D loss: 0.579953, acc: 71.88%] [G loss: 2.252367]\n",
      "epoch:11 step:11020 [D loss: 0.655114, acc: 58.59%] [G loss: 1.869948]\n",
      "epoch:11 step:11021 [D loss: 0.610870, acc: 62.50%] [G loss: 1.950527]\n",
      "epoch:11 step:11022 [D loss: 0.715514, acc: 50.78%] [G loss: 1.992908]\n",
      "epoch:11 step:11023 [D loss: 0.683911, acc: 58.59%] [G loss: 1.821289]\n",
      "epoch:11 step:11024 [D loss: 0.667845, acc: 61.72%] [G loss: 1.872063]\n",
      "epoch:11 step:11025 [D loss: 0.673161, acc: 60.94%] [G loss: 1.884292]\n",
      "epoch:11 step:11026 [D loss: 0.627149, acc: 62.50%] [G loss: 1.979158]\n",
      "epoch:11 step:11027 [D loss: 0.637156, acc: 63.28%] [G loss: 2.034721]\n",
      "epoch:11 step:11028 [D loss: 0.619302, acc: 64.06%] [G loss: 2.018916]\n",
      "epoch:11 step:11029 [D loss: 0.676501, acc: 57.03%] [G loss: 1.913777]\n",
      "epoch:11 step:11030 [D loss: 0.634128, acc: 62.50%] [G loss: 1.919568]\n",
      "epoch:11 step:11031 [D loss: 0.617058, acc: 69.53%] [G loss: 1.850371]\n",
      "epoch:11 step:11032 [D loss: 0.606105, acc: 65.62%] [G loss: 2.061656]\n",
      "epoch:11 step:11033 [D loss: 0.660618, acc: 59.38%] [G loss: 2.010532]\n",
      "epoch:11 step:11034 [D loss: 0.633232, acc: 64.84%] [G loss: 1.917727]\n",
      "epoch:11 step:11035 [D loss: 0.675873, acc: 62.50%] [G loss: 2.011726]\n",
      "epoch:11 step:11036 [D loss: 0.658761, acc: 64.84%] [G loss: 1.867980]\n",
      "epoch:11 step:11037 [D loss: 0.647635, acc: 64.06%] [G loss: 1.910517]\n",
      "epoch:11 step:11038 [D loss: 0.605867, acc: 69.53%] [G loss: 1.943700]\n",
      "epoch:11 step:11039 [D loss: 0.635491, acc: 64.84%] [G loss: 1.793380]\n",
      "epoch:11 step:11040 [D loss: 0.602728, acc: 65.62%] [G loss: 2.000199]\n",
      "epoch:11 step:11041 [D loss: 0.614959, acc: 67.97%] [G loss: 1.871142]\n",
      "epoch:11 step:11042 [D loss: 0.658220, acc: 59.38%] [G loss: 1.984282]\n",
      "epoch:11 step:11043 [D loss: 0.619485, acc: 67.97%] [G loss: 2.048176]\n",
      "epoch:11 step:11044 [D loss: 0.614193, acc: 65.62%] [G loss: 1.983306]\n",
      "epoch:11 step:11045 [D loss: 0.605170, acc: 67.19%] [G loss: 1.972318]\n",
      "epoch:11 step:11046 [D loss: 0.661520, acc: 61.72%] [G loss: 2.065791]\n",
      "epoch:11 step:11047 [D loss: 0.592917, acc: 67.97%] [G loss: 1.977329]\n",
      "epoch:11 step:11048 [D loss: 0.652265, acc: 60.16%] [G loss: 1.943710]\n",
      "epoch:11 step:11049 [D loss: 0.705579, acc: 50.78%] [G loss: 1.883726]\n",
      "epoch:11 step:11050 [D loss: 0.627873, acc: 64.84%] [G loss: 1.868693]\n",
      "epoch:11 step:11051 [D loss: 0.601961, acc: 72.66%] [G loss: 2.077110]\n",
      "epoch:11 step:11052 [D loss: 0.607457, acc: 65.62%] [G loss: 1.998851]\n",
      "epoch:11 step:11053 [D loss: 0.610664, acc: 67.97%] [G loss: 2.164356]\n",
      "epoch:11 step:11054 [D loss: 0.594065, acc: 68.75%] [G loss: 2.085560]\n",
      "epoch:11 step:11055 [D loss: 0.626576, acc: 60.16%] [G loss: 1.998298]\n",
      "epoch:11 step:11056 [D loss: 0.668915, acc: 61.72%] [G loss: 1.903536]\n",
      "epoch:11 step:11057 [D loss: 0.613771, acc: 68.75%] [G loss: 1.964632]\n",
      "epoch:11 step:11058 [D loss: 0.660065, acc: 67.19%] [G loss: 2.070212]\n",
      "epoch:11 step:11059 [D loss: 0.595539, acc: 67.97%] [G loss: 1.950488]\n",
      "epoch:11 step:11060 [D loss: 0.621127, acc: 68.75%] [G loss: 1.979008]\n",
      "epoch:11 step:11061 [D loss: 0.636643, acc: 63.28%] [G loss: 1.988203]\n",
      "epoch:11 step:11062 [D loss: 0.649713, acc: 60.16%] [G loss: 1.937515]\n",
      "epoch:11 step:11063 [D loss: 0.615224, acc: 67.19%] [G loss: 2.020542]\n",
      "epoch:11 step:11064 [D loss: 0.631199, acc: 60.16%] [G loss: 2.160655]\n",
      "epoch:11 step:11065 [D loss: 0.699603, acc: 56.25%] [G loss: 1.878422]\n",
      "epoch:11 step:11066 [D loss: 0.681065, acc: 58.59%] [G loss: 1.886634]\n",
      "epoch:11 step:11067 [D loss: 0.626456, acc: 66.41%] [G loss: 1.990421]\n",
      "epoch:11 step:11068 [D loss: 0.677450, acc: 56.25%] [G loss: 1.957976]\n",
      "epoch:11 step:11069 [D loss: 0.643862, acc: 62.50%] [G loss: 1.916102]\n",
      "epoch:11 step:11070 [D loss: 0.639806, acc: 67.97%] [G loss: 2.039808]\n",
      "epoch:11 step:11071 [D loss: 0.612967, acc: 67.19%] [G loss: 1.987576]\n",
      "epoch:11 step:11072 [D loss: 0.737655, acc: 56.25%] [G loss: 1.725462]\n",
      "epoch:11 step:11073 [D loss: 0.668652, acc: 62.50%] [G loss: 1.790762]\n",
      "epoch:11 step:11074 [D loss: 0.671793, acc: 57.81%] [G loss: 1.751893]\n",
      "epoch:11 step:11075 [D loss: 0.671801, acc: 60.94%] [G loss: 1.941901]\n",
      "epoch:11 step:11076 [D loss: 0.659975, acc: 65.62%] [G loss: 2.007350]\n",
      "epoch:11 step:11077 [D loss: 0.688366, acc: 61.72%] [G loss: 1.938033]\n",
      "epoch:11 step:11078 [D loss: 0.687807, acc: 57.03%] [G loss: 1.906249]\n",
      "epoch:11 step:11079 [D loss: 0.635571, acc: 63.28%] [G loss: 1.855034]\n",
      "epoch:11 step:11080 [D loss: 0.630647, acc: 69.53%] [G loss: 1.924655]\n",
      "epoch:11 step:11081 [D loss: 0.662499, acc: 60.16%] [G loss: 2.145349]\n",
      "epoch:11 step:11082 [D loss: 0.626391, acc: 64.84%] [G loss: 2.339669]\n",
      "epoch:11 step:11083 [D loss: 0.598981, acc: 67.19%] [G loss: 2.070825]\n",
      "epoch:11 step:11084 [D loss: 0.605850, acc: 62.50%] [G loss: 2.127908]\n",
      "epoch:11 step:11085 [D loss: 0.675340, acc: 58.59%] [G loss: 2.010764]\n",
      "epoch:11 step:11086 [D loss: 0.648012, acc: 66.41%] [G loss: 1.919775]\n",
      "epoch:11 step:11087 [D loss: 0.645511, acc: 57.03%] [G loss: 1.996305]\n",
      "epoch:11 step:11088 [D loss: 0.658753, acc: 67.19%] [G loss: 2.174543]\n",
      "epoch:11 step:11089 [D loss: 0.620630, acc: 65.62%] [G loss: 2.113829]\n",
      "epoch:11 step:11090 [D loss: 0.651483, acc: 63.28%] [G loss: 1.905646]\n",
      "epoch:11 step:11091 [D loss: 0.740073, acc: 52.34%] [G loss: 1.896196]\n",
      "epoch:11 step:11092 [D loss: 0.685871, acc: 57.03%] [G loss: 1.792104]\n",
      "epoch:11 step:11093 [D loss: 0.594297, acc: 67.97%] [G loss: 1.996081]\n",
      "epoch:11 step:11094 [D loss: 0.639870, acc: 64.06%] [G loss: 1.997530]\n",
      "epoch:11 step:11095 [D loss: 0.652716, acc: 63.28%] [G loss: 1.885326]\n",
      "epoch:11 step:11096 [D loss: 0.631821, acc: 60.16%] [G loss: 2.069857]\n",
      "epoch:11 step:11097 [D loss: 0.610119, acc: 67.97%] [G loss: 2.034807]\n",
      "epoch:11 step:11098 [D loss: 0.732407, acc: 58.59%] [G loss: 1.922946]\n",
      "epoch:11 step:11099 [D loss: 0.685985, acc: 60.16%] [G loss: 2.064067]\n",
      "epoch:11 step:11100 [D loss: 0.645446, acc: 64.06%] [G loss: 2.056041]\n",
      "epoch:11 step:11101 [D loss: 0.716089, acc: 55.47%] [G loss: 1.869150]\n",
      "epoch:11 step:11102 [D loss: 0.613520, acc: 67.97%] [G loss: 2.036191]\n",
      "epoch:11 step:11103 [D loss: 0.640671, acc: 57.81%] [G loss: 1.939861]\n",
      "epoch:11 step:11104 [D loss: 0.728591, acc: 51.56%] [G loss: 1.796465]\n",
      "epoch:11 step:11105 [D loss: 0.634013, acc: 60.94%] [G loss: 1.907844]\n",
      "epoch:11 step:11106 [D loss: 0.640373, acc: 61.72%] [G loss: 1.892091]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11107 [D loss: 0.678705, acc: 58.59%] [G loss: 1.788312]\n",
      "epoch:11 step:11108 [D loss: 0.682135, acc: 57.03%] [G loss: 1.794239]\n",
      "epoch:11 step:11109 [D loss: 0.599143, acc: 71.09%] [G loss: 1.919726]\n",
      "epoch:11 step:11110 [D loss: 0.639519, acc: 62.50%] [G loss: 1.974696]\n",
      "epoch:11 step:11111 [D loss: 0.643910, acc: 64.84%] [G loss: 1.921844]\n",
      "epoch:11 step:11112 [D loss: 0.573912, acc: 70.31%] [G loss: 2.065987]\n",
      "epoch:11 step:11113 [D loss: 0.636017, acc: 60.16%] [G loss: 2.045404]\n",
      "epoch:11 step:11114 [D loss: 0.580496, acc: 70.31%] [G loss: 2.075307]\n",
      "epoch:11 step:11115 [D loss: 0.654072, acc: 67.97%] [G loss: 1.974507]\n",
      "epoch:11 step:11116 [D loss: 0.628403, acc: 67.19%] [G loss: 2.004367]\n",
      "epoch:11 step:11117 [D loss: 0.634938, acc: 67.19%] [G loss: 1.950202]\n",
      "epoch:11 step:11118 [D loss: 0.609389, acc: 67.19%] [G loss: 1.879924]\n",
      "epoch:11 step:11119 [D loss: 0.701072, acc: 57.03%] [G loss: 1.824130]\n",
      "epoch:11 step:11120 [D loss: 0.647041, acc: 66.41%] [G loss: 1.892410]\n",
      "epoch:11 step:11121 [D loss: 0.605905, acc: 67.19%] [G loss: 1.997513]\n",
      "epoch:11 step:11122 [D loss: 0.600425, acc: 72.66%] [G loss: 2.303956]\n",
      "epoch:11 step:11123 [D loss: 0.618797, acc: 63.28%] [G loss: 2.082761]\n",
      "epoch:11 step:11124 [D loss: 0.658475, acc: 60.94%] [G loss: 1.936221]\n",
      "epoch:11 step:11125 [D loss: 0.712778, acc: 51.56%] [G loss: 1.925901]\n",
      "epoch:11 step:11126 [D loss: 0.705567, acc: 54.69%] [G loss: 1.891303]\n",
      "epoch:11 step:11127 [D loss: 0.721381, acc: 51.56%] [G loss: 1.871378]\n",
      "epoch:11 step:11128 [D loss: 0.647999, acc: 61.72%] [G loss: 1.887361]\n",
      "epoch:11 step:11129 [D loss: 0.623886, acc: 64.06%] [G loss: 1.982667]\n",
      "epoch:11 step:11130 [D loss: 0.622807, acc: 69.53%] [G loss: 2.019320]\n",
      "epoch:11 step:11131 [D loss: 0.613845, acc: 69.53%] [G loss: 1.949960]\n",
      "epoch:11 step:11132 [D loss: 0.617521, acc: 67.19%] [G loss: 1.948792]\n",
      "epoch:11 step:11133 [D loss: 0.619769, acc: 64.84%] [G loss: 1.981661]\n",
      "epoch:11 step:11134 [D loss: 0.669932, acc: 60.94%] [G loss: 1.879884]\n",
      "epoch:11 step:11135 [D loss: 0.686603, acc: 54.69%] [G loss: 1.780663]\n",
      "epoch:11 step:11136 [D loss: 0.698733, acc: 55.47%] [G loss: 1.801402]\n",
      "epoch:11 step:11137 [D loss: 0.661347, acc: 61.72%] [G loss: 1.853706]\n",
      "epoch:11 step:11138 [D loss: 0.650832, acc: 58.59%] [G loss: 2.019956]\n",
      "epoch:11 step:11139 [D loss: 0.636378, acc: 60.94%] [G loss: 2.031962]\n",
      "epoch:11 step:11140 [D loss: 0.611601, acc: 69.53%] [G loss: 1.903306]\n",
      "epoch:11 step:11141 [D loss: 0.639629, acc: 67.97%] [G loss: 1.932245]\n",
      "epoch:11 step:11142 [D loss: 0.693372, acc: 58.59%] [G loss: 1.944252]\n",
      "epoch:11 step:11143 [D loss: 0.630217, acc: 66.41%] [G loss: 1.854258]\n",
      "epoch:11 step:11144 [D loss: 0.588899, acc: 70.31%] [G loss: 2.092229]\n",
      "epoch:11 step:11145 [D loss: 0.646156, acc: 62.50%] [G loss: 1.936825]\n",
      "epoch:11 step:11146 [D loss: 0.664475, acc: 57.81%] [G loss: 1.964490]\n",
      "epoch:11 step:11147 [D loss: 0.608477, acc: 66.41%] [G loss: 1.958135]\n",
      "epoch:11 step:11148 [D loss: 0.610078, acc: 68.75%] [G loss: 1.881950]\n",
      "epoch:11 step:11149 [D loss: 0.639952, acc: 58.59%] [G loss: 1.955929]\n",
      "epoch:11 step:11150 [D loss: 0.610654, acc: 66.41%] [G loss: 2.059543]\n",
      "epoch:11 step:11151 [D loss: 0.616012, acc: 69.53%] [G loss: 2.144069]\n",
      "epoch:11 step:11152 [D loss: 0.595200, acc: 68.75%] [G loss: 1.997011]\n",
      "epoch:11 step:11153 [D loss: 0.603755, acc: 67.19%] [G loss: 1.956398]\n",
      "epoch:11 step:11154 [D loss: 0.629076, acc: 67.97%] [G loss: 1.899845]\n",
      "epoch:11 step:11155 [D loss: 0.666737, acc: 67.19%] [G loss: 2.077344]\n",
      "epoch:11 step:11156 [D loss: 0.603374, acc: 72.66%] [G loss: 2.144698]\n",
      "epoch:11 step:11157 [D loss: 0.668838, acc: 59.38%] [G loss: 1.871451]\n",
      "epoch:11 step:11158 [D loss: 0.649363, acc: 66.41%] [G loss: 2.008285]\n",
      "epoch:11 step:11159 [D loss: 0.622045, acc: 57.81%] [G loss: 1.932013]\n",
      "epoch:11 step:11160 [D loss: 0.615999, acc: 64.84%] [G loss: 2.004076]\n",
      "epoch:11 step:11161 [D loss: 0.633759, acc: 61.72%] [G loss: 2.022168]\n",
      "epoch:11 step:11162 [D loss: 0.664385, acc: 63.28%] [G loss: 1.813170]\n",
      "epoch:11 step:11163 [D loss: 0.663819, acc: 60.16%] [G loss: 1.833893]\n",
      "epoch:11 step:11164 [D loss: 0.642944, acc: 63.28%] [G loss: 2.112381]\n",
      "epoch:11 step:11165 [D loss: 0.667179, acc: 60.94%] [G loss: 1.852066]\n",
      "epoch:11 step:11166 [D loss: 0.668288, acc: 63.28%] [G loss: 1.996236]\n",
      "epoch:11 step:11167 [D loss: 0.619929, acc: 67.97%] [G loss: 1.991347]\n",
      "epoch:11 step:11168 [D loss: 0.665280, acc: 61.72%] [G loss: 2.004385]\n",
      "epoch:11 step:11169 [D loss: 0.655543, acc: 58.59%] [G loss: 1.767838]\n",
      "epoch:11 step:11170 [D loss: 0.660819, acc: 64.84%] [G loss: 2.034186]\n",
      "epoch:11 step:11171 [D loss: 0.647676, acc: 60.94%] [G loss: 1.983666]\n",
      "epoch:11 step:11172 [D loss: 0.663549, acc: 62.50%] [G loss: 1.841079]\n",
      "epoch:11 step:11173 [D loss: 0.633761, acc: 67.97%] [G loss: 1.908424]\n",
      "epoch:11 step:11174 [D loss: 0.635232, acc: 64.06%] [G loss: 1.866869]\n",
      "epoch:11 step:11175 [D loss: 0.672023, acc: 56.25%] [G loss: 1.907831]\n",
      "epoch:11 step:11176 [D loss: 0.672119, acc: 59.38%] [G loss: 1.912190]\n",
      "epoch:11 step:11177 [D loss: 0.655454, acc: 63.28%] [G loss: 1.935004]\n",
      "epoch:11 step:11178 [D loss: 0.624320, acc: 62.50%] [G loss: 2.065737]\n",
      "epoch:11 step:11179 [D loss: 0.601726, acc: 65.62%] [G loss: 1.892726]\n",
      "epoch:11 step:11180 [D loss: 0.648609, acc: 57.81%] [G loss: 1.902460]\n",
      "epoch:11 step:11181 [D loss: 0.669265, acc: 60.94%] [G loss: 1.870342]\n",
      "epoch:11 step:11182 [D loss: 0.618382, acc: 66.41%] [G loss: 2.062816]\n",
      "epoch:11 step:11183 [D loss: 0.623484, acc: 65.62%] [G loss: 1.936877]\n",
      "epoch:11 step:11184 [D loss: 0.629280, acc: 66.41%] [G loss: 1.984501]\n",
      "epoch:11 step:11185 [D loss: 0.675041, acc: 57.03%] [G loss: 1.866483]\n",
      "epoch:11 step:11186 [D loss: 0.638872, acc: 66.41%] [G loss: 1.874231]\n",
      "epoch:11 step:11187 [D loss: 0.581720, acc: 70.31%] [G loss: 1.955200]\n",
      "epoch:11 step:11188 [D loss: 0.626843, acc: 65.62%] [G loss: 2.066519]\n",
      "epoch:11 step:11189 [D loss: 0.583470, acc: 71.88%] [G loss: 2.114020]\n",
      "epoch:11 step:11190 [D loss: 0.667610, acc: 61.72%] [G loss: 1.869061]\n",
      "epoch:11 step:11191 [D loss: 0.612955, acc: 69.53%] [G loss: 2.026238]\n",
      "epoch:11 step:11192 [D loss: 0.662506, acc: 64.84%] [G loss: 1.868484]\n",
      "epoch:11 step:11193 [D loss: 0.623305, acc: 64.84%] [G loss: 2.122685]\n",
      "epoch:11 step:11194 [D loss: 0.660304, acc: 64.06%] [G loss: 1.960054]\n",
      "epoch:11 step:11195 [D loss: 0.672675, acc: 62.50%] [G loss: 2.007544]\n",
      "epoch:11 step:11196 [D loss: 0.628088, acc: 64.84%] [G loss: 2.006182]\n",
      "epoch:11 step:11197 [D loss: 0.607463, acc: 64.06%] [G loss: 2.067511]\n",
      "epoch:11 step:11198 [D loss: 0.671548, acc: 58.59%] [G loss: 2.042689]\n",
      "epoch:11 step:11199 [D loss: 0.660769, acc: 60.16%] [G loss: 1.888828]\n",
      "epoch:11 step:11200 [D loss: 0.626399, acc: 64.84%] [G loss: 1.964079]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.994750\n",
      "FID: 19.649899\n",
      "0 = 12.660883508062327\n",
      "1 = 0.07754453548458787\n",
      "2 = 0.8766000270843506\n",
      "3 = 0.8978000283241272\n",
      "4 = 0.855400025844574\n",
      "5 = 0.8612816333770752\n",
      "6 = 0.8978000283241272\n",
      "7 = 7.040947717165949\n",
      "8 = 0.08764342406304161\n",
      "9 = 0.746999979019165\n",
      "10 = 0.7599999904632568\n",
      "11 = 0.734000027179718\n",
      "12 = 0.7407407164573669\n",
      "13 = 0.7599999904632568\n",
      "14 = 6.994781970977783\n",
      "15 = 9.41692066192627\n",
      "16 = 0.12890787422657013\n",
      "17 = 6.994749546051025\n",
      "18 = 19.649898529052734\n",
      "epoch:11 step:11201 [D loss: 0.579957, acc: 74.22%] [G loss: 2.105801]\n",
      "epoch:11 step:11202 [D loss: 0.649906, acc: 62.50%] [G loss: 2.011364]\n",
      "epoch:11 step:11203 [D loss: 0.587256, acc: 67.97%] [G loss: 2.006641]\n",
      "epoch:11 step:11204 [D loss: 0.599325, acc: 66.41%] [G loss: 2.055242]\n",
      "epoch:11 step:11205 [D loss: 0.636338, acc: 60.16%] [G loss: 1.989187]\n",
      "epoch:11 step:11206 [D loss: 0.607063, acc: 66.41%] [G loss: 2.148471]\n",
      "epoch:11 step:11207 [D loss: 0.630932, acc: 67.19%] [G loss: 2.101193]\n",
      "epoch:11 step:11208 [D loss: 0.614487, acc: 68.75%] [G loss: 1.965668]\n",
      "epoch:11 step:11209 [D loss: 0.678376, acc: 57.81%] [G loss: 1.771510]\n",
      "epoch:11 step:11210 [D loss: 0.701848, acc: 58.59%] [G loss: 2.085188]\n",
      "epoch:11 step:11211 [D loss: 0.681388, acc: 61.72%] [G loss: 1.840486]\n",
      "epoch:11 step:11212 [D loss: 0.629610, acc: 67.19%] [G loss: 1.965374]\n",
      "epoch:11 step:11213 [D loss: 0.581477, acc: 71.88%] [G loss: 2.285686]\n",
      "epoch:11 step:11214 [D loss: 0.631170, acc: 64.06%] [G loss: 2.120609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11215 [D loss: 0.620692, acc: 66.41%] [G loss: 2.006364]\n",
      "epoch:11 step:11216 [D loss: 0.619099, acc: 67.19%] [G loss: 2.174601]\n",
      "epoch:11 step:11217 [D loss: 0.594934, acc: 68.75%] [G loss: 2.191339]\n",
      "epoch:11 step:11218 [D loss: 0.571042, acc: 72.66%] [G loss: 2.151700]\n",
      "epoch:11 step:11219 [D loss: 0.586873, acc: 70.31%] [G loss: 2.321067]\n",
      "epoch:11 step:11220 [D loss: 0.606378, acc: 62.50%] [G loss: 2.146167]\n",
      "epoch:11 step:11221 [D loss: 0.645227, acc: 60.16%] [G loss: 2.078622]\n",
      "epoch:11 step:11222 [D loss: 0.660632, acc: 60.94%] [G loss: 1.954986]\n",
      "epoch:11 step:11223 [D loss: 0.587845, acc: 71.09%] [G loss: 2.209518]\n",
      "epoch:11 step:11224 [D loss: 0.588538, acc: 68.75%] [G loss: 2.047282]\n",
      "epoch:11 step:11225 [D loss: 0.599563, acc: 69.53%] [G loss: 2.239367]\n",
      "epoch:11 step:11226 [D loss: 0.638188, acc: 60.16%] [G loss: 2.614679]\n",
      "epoch:11 step:11227 [D loss: 0.729620, acc: 51.56%] [G loss: 1.935269]\n",
      "epoch:11 step:11228 [D loss: 0.610277, acc: 65.62%] [G loss: 2.108935]\n",
      "epoch:11 step:11229 [D loss: 0.576144, acc: 67.97%] [G loss: 2.163382]\n",
      "epoch:11 step:11230 [D loss: 0.592010, acc: 69.53%] [G loss: 2.116620]\n",
      "epoch:11 step:11231 [D loss: 0.558010, acc: 78.91%] [G loss: 2.421135]\n",
      "epoch:11 step:11232 [D loss: 0.587667, acc: 64.06%] [G loss: 2.393780]\n",
      "epoch:11 step:11233 [D loss: 0.559674, acc: 68.75%] [G loss: 2.596681]\n",
      "epoch:11 step:11234 [D loss: 0.667882, acc: 60.16%] [G loss: 2.238910]\n",
      "epoch:11 step:11235 [D loss: 0.715688, acc: 60.94%] [G loss: 1.857735]\n",
      "epoch:11 step:11236 [D loss: 0.754184, acc: 51.56%] [G loss: 1.973866]\n",
      "epoch:11 step:11237 [D loss: 0.654723, acc: 61.72%] [G loss: 2.053200]\n",
      "epoch:11 step:11238 [D loss: 0.678855, acc: 62.50%] [G loss: 1.978775]\n",
      "epoch:11 step:11239 [D loss: 0.639579, acc: 64.06%] [G loss: 1.935188]\n",
      "epoch:11 step:11240 [D loss: 0.628232, acc: 64.84%] [G loss: 2.056728]\n",
      "epoch:11 step:11241 [D loss: 0.638505, acc: 64.06%] [G loss: 2.151542]\n",
      "epoch:11 step:11242 [D loss: 0.610811, acc: 68.75%] [G loss: 2.112432]\n",
      "epoch:11 step:11243 [D loss: 0.651250, acc: 67.19%] [G loss: 2.042921]\n",
      "epoch:11 step:11244 [D loss: 0.602422, acc: 71.88%] [G loss: 2.346659]\n",
      "epoch:12 step:11245 [D loss: 0.659417, acc: 57.81%] [G loss: 1.843360]\n",
      "epoch:12 step:11246 [D loss: 0.632226, acc: 64.84%] [G loss: 1.977732]\n",
      "epoch:12 step:11247 [D loss: 0.631685, acc: 68.75%] [G loss: 2.007314]\n",
      "epoch:12 step:11248 [D loss: 0.632761, acc: 66.41%] [G loss: 1.875061]\n",
      "epoch:12 step:11249 [D loss: 0.647256, acc: 60.16%] [G loss: 1.886920]\n",
      "epoch:12 step:11250 [D loss: 0.629778, acc: 60.16%] [G loss: 2.016616]\n",
      "epoch:12 step:11251 [D loss: 0.640953, acc: 60.16%] [G loss: 1.957824]\n",
      "epoch:12 step:11252 [D loss: 0.641250, acc: 64.06%] [G loss: 2.027815]\n",
      "epoch:12 step:11253 [D loss: 0.584614, acc: 62.50%] [G loss: 2.222387]\n",
      "epoch:12 step:11254 [D loss: 0.635803, acc: 67.97%] [G loss: 2.190722]\n",
      "epoch:12 step:11255 [D loss: 0.627199, acc: 67.97%] [G loss: 2.110769]\n",
      "epoch:12 step:11256 [D loss: 0.614426, acc: 67.19%] [G loss: 2.004357]\n",
      "epoch:12 step:11257 [D loss: 0.622199, acc: 67.97%] [G loss: 1.885668]\n",
      "epoch:12 step:11258 [D loss: 0.643565, acc: 65.62%] [G loss: 2.146520]\n",
      "epoch:12 step:11259 [D loss: 0.559472, acc: 66.41%] [G loss: 2.288630]\n",
      "epoch:12 step:11260 [D loss: 0.556549, acc: 67.97%] [G loss: 2.317322]\n",
      "epoch:12 step:11261 [D loss: 0.640736, acc: 66.41%] [G loss: 2.148597]\n",
      "epoch:12 step:11262 [D loss: 0.628173, acc: 65.62%] [G loss: 1.974658]\n",
      "epoch:12 step:11263 [D loss: 0.651757, acc: 60.94%] [G loss: 1.971381]\n",
      "epoch:12 step:11264 [D loss: 0.666351, acc: 61.72%] [G loss: 1.723904]\n",
      "epoch:12 step:11265 [D loss: 0.706476, acc: 57.81%] [G loss: 1.895256]\n",
      "epoch:12 step:11266 [D loss: 0.674865, acc: 60.16%] [G loss: 1.968604]\n",
      "epoch:12 step:11267 [D loss: 0.646800, acc: 62.50%] [G loss: 2.140218]\n",
      "epoch:12 step:11268 [D loss: 0.632820, acc: 65.62%] [G loss: 2.132413]\n",
      "epoch:12 step:11269 [D loss: 0.546684, acc: 75.00%] [G loss: 2.119654]\n",
      "epoch:12 step:11270 [D loss: 0.663451, acc: 61.72%] [G loss: 1.880124]\n",
      "epoch:12 step:11271 [D loss: 0.616914, acc: 64.06%] [G loss: 1.832984]\n",
      "epoch:12 step:11272 [D loss: 0.615161, acc: 63.28%] [G loss: 1.822004]\n",
      "epoch:12 step:11273 [D loss: 0.606304, acc: 67.97%] [G loss: 1.943778]\n",
      "epoch:12 step:11274 [D loss: 0.622583, acc: 67.97%] [G loss: 1.973819]\n",
      "epoch:12 step:11275 [D loss: 0.678146, acc: 61.72%] [G loss: 1.877117]\n",
      "epoch:12 step:11276 [D loss: 0.680411, acc: 59.38%] [G loss: 1.800262]\n",
      "epoch:12 step:11277 [D loss: 0.632944, acc: 67.19%] [G loss: 1.787006]\n",
      "epoch:12 step:11278 [D loss: 0.628398, acc: 61.72%] [G loss: 2.033343]\n",
      "epoch:12 step:11279 [D loss: 0.638014, acc: 60.16%] [G loss: 2.137389]\n",
      "epoch:12 step:11280 [D loss: 0.641760, acc: 59.38%] [G loss: 1.862416]\n",
      "epoch:12 step:11281 [D loss: 0.651761, acc: 64.84%] [G loss: 2.102484]\n",
      "epoch:12 step:11282 [D loss: 0.608101, acc: 66.41%] [G loss: 2.133603]\n",
      "epoch:12 step:11283 [D loss: 0.632830, acc: 63.28%] [G loss: 2.179552]\n",
      "epoch:12 step:11284 [D loss: 0.586386, acc: 67.97%] [G loss: 2.159969]\n",
      "epoch:12 step:11285 [D loss: 0.615090, acc: 63.28%] [G loss: 1.982592]\n",
      "epoch:12 step:11286 [D loss: 0.589014, acc: 64.84%] [G loss: 1.925393]\n",
      "epoch:12 step:11287 [D loss: 0.597291, acc: 70.31%] [G loss: 1.988616]\n",
      "epoch:12 step:11288 [D loss: 0.632625, acc: 63.28%] [G loss: 1.799769]\n",
      "epoch:12 step:11289 [D loss: 0.633449, acc: 62.50%] [G loss: 1.941957]\n",
      "epoch:12 step:11290 [D loss: 0.680132, acc: 56.25%] [G loss: 1.855724]\n",
      "epoch:12 step:11291 [D loss: 0.640066, acc: 64.84%] [G loss: 1.853543]\n",
      "epoch:12 step:11292 [D loss: 0.619396, acc: 69.53%] [G loss: 2.080997]\n",
      "epoch:12 step:11293 [D loss: 0.593696, acc: 65.62%] [G loss: 1.985249]\n",
      "epoch:12 step:11294 [D loss: 0.609875, acc: 67.19%] [G loss: 2.105649]\n",
      "epoch:12 step:11295 [D loss: 0.625152, acc: 64.84%] [G loss: 2.004039]\n",
      "epoch:12 step:11296 [D loss: 0.647204, acc: 61.72%] [G loss: 2.091444]\n",
      "epoch:12 step:11297 [D loss: 0.653387, acc: 67.19%] [G loss: 2.142089]\n",
      "epoch:12 step:11298 [D loss: 0.638065, acc: 64.84%] [G loss: 2.115751]\n",
      "epoch:12 step:11299 [D loss: 0.602937, acc: 69.53%] [G loss: 2.113822]\n",
      "epoch:12 step:11300 [D loss: 0.620387, acc: 63.28%] [G loss: 2.038623]\n",
      "epoch:12 step:11301 [D loss: 0.675243, acc: 57.81%] [G loss: 1.916425]\n",
      "epoch:12 step:11302 [D loss: 0.633689, acc: 57.03%] [G loss: 1.963519]\n",
      "epoch:12 step:11303 [D loss: 0.619388, acc: 68.75%] [G loss: 1.993136]\n",
      "epoch:12 step:11304 [D loss: 0.637389, acc: 60.94%] [G loss: 1.938227]\n",
      "epoch:12 step:11305 [D loss: 0.653144, acc: 64.84%] [G loss: 2.016459]\n",
      "epoch:12 step:11306 [D loss: 0.651697, acc: 67.97%] [G loss: 1.992059]\n",
      "epoch:12 step:11307 [D loss: 0.638221, acc: 60.94%] [G loss: 2.178847]\n",
      "epoch:12 step:11308 [D loss: 0.669929, acc: 63.28%] [G loss: 2.056828]\n",
      "epoch:12 step:11309 [D loss: 0.678981, acc: 60.94%] [G loss: 1.868702]\n",
      "epoch:12 step:11310 [D loss: 0.631054, acc: 64.06%] [G loss: 1.980618]\n",
      "epoch:12 step:11311 [D loss: 0.657616, acc: 62.50%] [G loss: 1.869838]\n",
      "epoch:12 step:11312 [D loss: 0.649414, acc: 65.62%] [G loss: 1.917511]\n",
      "epoch:12 step:11313 [D loss: 0.601217, acc: 66.41%] [G loss: 2.055232]\n",
      "epoch:12 step:11314 [D loss: 0.615114, acc: 69.53%] [G loss: 2.045883]\n",
      "epoch:12 step:11315 [D loss: 0.684370, acc: 60.94%] [G loss: 1.877185]\n",
      "epoch:12 step:11316 [D loss: 0.689857, acc: 57.81%] [G loss: 2.011680]\n",
      "epoch:12 step:11317 [D loss: 0.642352, acc: 62.50%] [G loss: 2.017002]\n",
      "epoch:12 step:11318 [D loss: 0.595568, acc: 65.62%] [G loss: 1.862650]\n",
      "epoch:12 step:11319 [D loss: 0.661048, acc: 60.16%] [G loss: 2.106477]\n",
      "epoch:12 step:11320 [D loss: 0.638732, acc: 60.94%] [G loss: 1.901419]\n",
      "epoch:12 step:11321 [D loss: 0.580248, acc: 71.09%] [G loss: 2.264350]\n",
      "epoch:12 step:11322 [D loss: 0.669915, acc: 61.72%] [G loss: 1.867052]\n",
      "epoch:12 step:11323 [D loss: 0.663075, acc: 56.25%] [G loss: 1.796310]\n",
      "epoch:12 step:11324 [D loss: 0.670049, acc: 57.03%] [G loss: 1.875578]\n",
      "epoch:12 step:11325 [D loss: 0.709875, acc: 52.34%] [G loss: 1.823630]\n",
      "epoch:12 step:11326 [D loss: 0.645284, acc: 60.94%] [G loss: 2.030228]\n",
      "epoch:12 step:11327 [D loss: 0.645416, acc: 67.97%] [G loss: 2.114837]\n",
      "epoch:12 step:11328 [D loss: 0.634294, acc: 60.16%] [G loss: 1.917765]\n",
      "epoch:12 step:11329 [D loss: 0.596930, acc: 65.62%] [G loss: 1.893137]\n",
      "epoch:12 step:11330 [D loss: 0.681888, acc: 62.50%] [G loss: 1.868337]\n",
      "epoch:12 step:11331 [D loss: 0.612738, acc: 67.19%] [G loss: 1.926219]\n",
      "epoch:12 step:11332 [D loss: 0.714477, acc: 58.59%] [G loss: 1.827825]\n",
      "epoch:12 step:11333 [D loss: 0.602370, acc: 64.84%] [G loss: 1.932778]\n",
      "epoch:12 step:11334 [D loss: 0.618373, acc: 65.62%] [G loss: 1.867974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11335 [D loss: 0.682893, acc: 60.16%] [G loss: 2.022596]\n",
      "epoch:12 step:11336 [D loss: 0.644130, acc: 60.94%] [G loss: 1.986538]\n",
      "epoch:12 step:11337 [D loss: 0.643976, acc: 61.72%] [G loss: 2.030427]\n",
      "epoch:12 step:11338 [D loss: 0.622988, acc: 64.06%] [G loss: 1.983209]\n",
      "epoch:12 step:11339 [D loss: 0.674178, acc: 60.94%] [G loss: 1.848278]\n",
      "epoch:12 step:11340 [D loss: 0.628132, acc: 65.62%] [G loss: 1.892901]\n",
      "epoch:12 step:11341 [D loss: 0.588409, acc: 68.75%] [G loss: 1.977026]\n",
      "epoch:12 step:11342 [D loss: 0.702040, acc: 58.59%] [G loss: 1.835881]\n",
      "epoch:12 step:11343 [D loss: 0.662494, acc: 56.25%] [G loss: 1.943846]\n",
      "epoch:12 step:11344 [D loss: 0.649944, acc: 66.41%] [G loss: 1.868297]\n",
      "epoch:12 step:11345 [D loss: 0.679193, acc: 64.06%] [G loss: 1.949985]\n",
      "epoch:12 step:11346 [D loss: 0.635986, acc: 55.47%] [G loss: 1.845734]\n",
      "epoch:12 step:11347 [D loss: 0.633752, acc: 60.94%] [G loss: 1.977257]\n",
      "epoch:12 step:11348 [D loss: 0.621688, acc: 63.28%] [G loss: 1.917467]\n",
      "epoch:12 step:11349 [D loss: 0.698735, acc: 58.59%] [G loss: 1.865111]\n",
      "epoch:12 step:11350 [D loss: 0.599304, acc: 69.53%] [G loss: 2.210597]\n",
      "epoch:12 step:11351 [D loss: 0.640965, acc: 63.28%] [G loss: 2.035877]\n",
      "epoch:12 step:11352 [D loss: 0.684564, acc: 55.47%] [G loss: 1.759668]\n",
      "epoch:12 step:11353 [D loss: 0.629113, acc: 64.84%] [G loss: 1.909950]\n",
      "epoch:12 step:11354 [D loss: 0.596392, acc: 61.72%] [G loss: 1.923596]\n",
      "epoch:12 step:11355 [D loss: 0.693215, acc: 60.94%] [G loss: 1.980821]\n",
      "epoch:12 step:11356 [D loss: 0.678405, acc: 57.81%] [G loss: 1.957783]\n",
      "epoch:12 step:11357 [D loss: 0.670660, acc: 62.50%] [G loss: 1.999568]\n",
      "epoch:12 step:11358 [D loss: 0.614680, acc: 65.62%] [G loss: 1.859808]\n",
      "epoch:12 step:11359 [D loss: 0.608860, acc: 66.41%] [G loss: 2.216437]\n",
      "epoch:12 step:11360 [D loss: 0.618952, acc: 64.84%] [G loss: 2.101338]\n",
      "epoch:12 step:11361 [D loss: 0.607240, acc: 67.19%] [G loss: 2.158218]\n",
      "epoch:12 step:11362 [D loss: 0.625713, acc: 66.41%] [G loss: 2.002553]\n",
      "epoch:12 step:11363 [D loss: 0.641685, acc: 63.28%] [G loss: 2.213016]\n",
      "epoch:12 step:11364 [D loss: 0.658769, acc: 62.50%] [G loss: 2.072209]\n",
      "epoch:12 step:11365 [D loss: 0.642909, acc: 64.06%] [G loss: 2.235228]\n",
      "epoch:12 step:11366 [D loss: 0.596556, acc: 65.62%] [G loss: 2.263899]\n",
      "epoch:12 step:11367 [D loss: 0.653432, acc: 60.16%] [G loss: 1.982852]\n",
      "epoch:12 step:11368 [D loss: 0.714822, acc: 54.69%] [G loss: 1.881600]\n",
      "epoch:12 step:11369 [D loss: 0.675982, acc: 59.38%] [G loss: 1.806772]\n",
      "epoch:12 step:11370 [D loss: 0.580890, acc: 70.31%] [G loss: 2.138095]\n",
      "epoch:12 step:11371 [D loss: 0.692285, acc: 57.81%] [G loss: 1.951715]\n",
      "epoch:12 step:11372 [D loss: 0.612074, acc: 66.41%] [G loss: 1.894561]\n",
      "epoch:12 step:11373 [D loss: 0.650513, acc: 58.59%] [G loss: 1.915469]\n",
      "epoch:12 step:11374 [D loss: 0.663087, acc: 59.38%] [G loss: 2.191914]\n",
      "epoch:12 step:11375 [D loss: 0.605468, acc: 66.41%] [G loss: 2.062199]\n",
      "epoch:12 step:11376 [D loss: 0.640409, acc: 61.72%] [G loss: 1.978892]\n",
      "epoch:12 step:11377 [D loss: 0.685851, acc: 53.12%] [G loss: 1.830970]\n",
      "epoch:12 step:11378 [D loss: 0.636264, acc: 60.16%] [G loss: 1.909772]\n",
      "epoch:12 step:11379 [D loss: 0.631007, acc: 65.62%] [G loss: 1.829265]\n",
      "epoch:12 step:11380 [D loss: 0.649242, acc: 63.28%] [G loss: 1.891234]\n",
      "epoch:12 step:11381 [D loss: 0.689634, acc: 57.03%] [G loss: 1.904065]\n",
      "epoch:12 step:11382 [D loss: 0.669362, acc: 66.41%] [G loss: 1.944914]\n",
      "epoch:12 step:11383 [D loss: 0.642849, acc: 62.50%] [G loss: 2.006986]\n",
      "epoch:12 step:11384 [D loss: 0.669016, acc: 58.59%] [G loss: 1.829898]\n",
      "epoch:12 step:11385 [D loss: 0.647786, acc: 62.50%] [G loss: 1.953687]\n",
      "epoch:12 step:11386 [D loss: 0.687476, acc: 59.38%] [G loss: 1.755306]\n",
      "epoch:12 step:11387 [D loss: 0.618104, acc: 64.06%] [G loss: 1.927844]\n",
      "epoch:12 step:11388 [D loss: 0.602368, acc: 67.97%] [G loss: 2.158889]\n",
      "epoch:12 step:11389 [D loss: 0.646728, acc: 60.94%] [G loss: 1.976870]\n",
      "epoch:12 step:11390 [D loss: 0.626035, acc: 63.28%] [G loss: 1.929181]\n",
      "epoch:12 step:11391 [D loss: 0.696157, acc: 53.12%] [G loss: 1.931830]\n",
      "epoch:12 step:11392 [D loss: 0.664861, acc: 60.16%] [G loss: 1.997020]\n",
      "epoch:12 step:11393 [D loss: 0.638344, acc: 61.72%] [G loss: 2.004509]\n",
      "epoch:12 step:11394 [D loss: 0.610498, acc: 67.97%] [G loss: 1.769198]\n",
      "epoch:12 step:11395 [D loss: 0.626957, acc: 68.75%] [G loss: 1.983957]\n",
      "epoch:12 step:11396 [D loss: 0.570999, acc: 74.22%] [G loss: 2.153152]\n",
      "epoch:12 step:11397 [D loss: 0.684064, acc: 59.38%] [G loss: 1.922565]\n",
      "epoch:12 step:11398 [D loss: 0.643306, acc: 60.94%] [G loss: 2.005608]\n",
      "epoch:12 step:11399 [D loss: 0.664384, acc: 59.38%] [G loss: 2.032361]\n",
      "epoch:12 step:11400 [D loss: 0.637954, acc: 64.06%] [G loss: 1.900563]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.763449\n",
      "FID: 22.917936\n",
      "0 = 12.903196538734447\n",
      "1 = 0.08821757222766705\n",
      "2 = 0.9050999879837036\n",
      "3 = 0.920199990272522\n",
      "4 = 0.8899999856948853\n",
      "5 = 0.8932245969772339\n",
      "6 = 0.920199990272522\n",
      "7 = 7.233244224286095\n",
      "8 = 0.09610399641287494\n",
      "9 = 0.7494000196456909\n",
      "10 = 0.7623999714851379\n",
      "11 = 0.7364000082015991\n",
      "12 = 0.7430799007415771\n",
      "13 = 0.7623999714851379\n",
      "14 = 6.7634758949279785\n",
      "15 = 9.334510803222656\n",
      "16 = 0.15340572595596313\n",
      "17 = 6.763449192047119\n",
      "18 = 22.917936325073242\n",
      "epoch:12 step:11401 [D loss: 0.612310, acc: 69.53%] [G loss: 1.955338]\n",
      "epoch:12 step:11402 [D loss: 0.631234, acc: 60.94%] [G loss: 1.880051]\n",
      "epoch:12 step:11403 [D loss: 0.586373, acc: 71.09%] [G loss: 1.856439]\n",
      "epoch:12 step:11404 [D loss: 0.661425, acc: 61.72%] [G loss: 1.913021]\n",
      "epoch:12 step:11405 [D loss: 0.675527, acc: 57.81%] [G loss: 1.941936]\n",
      "epoch:12 step:11406 [D loss: 0.691933, acc: 63.28%] [G loss: 1.833565]\n",
      "epoch:12 step:11407 [D loss: 0.632454, acc: 65.62%] [G loss: 1.873634]\n",
      "epoch:12 step:11408 [D loss: 0.692149, acc: 57.03%] [G loss: 1.769647]\n",
      "epoch:12 step:11409 [D loss: 0.588331, acc: 67.97%] [G loss: 1.946393]\n",
      "epoch:12 step:11410 [D loss: 0.644463, acc: 66.41%] [G loss: 1.874547]\n",
      "epoch:12 step:11411 [D loss: 0.597383, acc: 66.41%] [G loss: 1.944016]\n",
      "epoch:12 step:11412 [D loss: 0.586139, acc: 67.97%] [G loss: 1.990344]\n",
      "epoch:12 step:11413 [D loss: 0.714123, acc: 58.59%] [G loss: 1.779865]\n",
      "epoch:12 step:11414 [D loss: 0.640992, acc: 63.28%] [G loss: 1.890931]\n",
      "epoch:12 step:11415 [D loss: 0.620267, acc: 64.84%] [G loss: 1.969530]\n",
      "epoch:12 step:11416 [D loss: 0.688163, acc: 56.25%] [G loss: 1.796146]\n",
      "epoch:12 step:11417 [D loss: 0.647505, acc: 68.75%] [G loss: 1.812955]\n",
      "epoch:12 step:11418 [D loss: 0.657746, acc: 65.62%] [G loss: 1.887587]\n",
      "epoch:12 step:11419 [D loss: 0.625377, acc: 65.62%] [G loss: 1.866950]\n",
      "epoch:12 step:11420 [D loss: 0.725801, acc: 54.69%] [G loss: 1.872550]\n",
      "epoch:12 step:11421 [D loss: 0.678371, acc: 56.25%] [G loss: 1.946989]\n",
      "epoch:12 step:11422 [D loss: 0.626596, acc: 64.84%] [G loss: 1.914608]\n",
      "epoch:12 step:11423 [D loss: 0.644665, acc: 62.50%] [G loss: 1.785180]\n",
      "epoch:12 step:11424 [D loss: 0.675807, acc: 61.72%] [G loss: 1.933014]\n",
      "epoch:12 step:11425 [D loss: 0.593392, acc: 71.09%] [G loss: 1.915826]\n",
      "epoch:12 step:11426 [D loss: 0.704949, acc: 60.94%] [G loss: 1.927916]\n",
      "epoch:12 step:11427 [D loss: 0.687489, acc: 57.03%] [G loss: 1.819056]\n",
      "epoch:12 step:11428 [D loss: 0.653703, acc: 60.16%] [G loss: 1.912470]\n",
      "epoch:12 step:11429 [D loss: 0.643011, acc: 64.06%] [G loss: 2.081424]\n",
      "epoch:12 step:11430 [D loss: 0.675662, acc: 58.59%] [G loss: 1.846275]\n",
      "epoch:12 step:11431 [D loss: 0.674551, acc: 57.81%] [G loss: 1.981825]\n",
      "epoch:12 step:11432 [D loss: 0.625858, acc: 67.19%] [G loss: 1.974913]\n",
      "epoch:12 step:11433 [D loss: 0.684932, acc: 58.59%] [G loss: 1.897461]\n",
      "epoch:12 step:11434 [D loss: 0.683451, acc: 59.38%] [G loss: 1.751762]\n",
      "epoch:12 step:11435 [D loss: 0.664693, acc: 59.38%] [G loss: 1.830116]\n",
      "epoch:12 step:11436 [D loss: 0.660370, acc: 67.97%] [G loss: 1.898889]\n",
      "epoch:12 step:11437 [D loss: 0.646434, acc: 59.38%] [G loss: 2.077338]\n",
      "epoch:12 step:11438 [D loss: 0.567319, acc: 72.66%] [G loss: 2.161165]\n",
      "epoch:12 step:11439 [D loss: 0.587324, acc: 66.41%] [G loss: 2.017579]\n",
      "epoch:12 step:11440 [D loss: 0.679670, acc: 60.94%] [G loss: 1.816286]\n",
      "epoch:12 step:11441 [D loss: 0.590746, acc: 68.75%] [G loss: 2.101363]\n",
      "epoch:12 step:11442 [D loss: 0.638136, acc: 67.97%] [G loss: 2.092402]\n",
      "epoch:12 step:11443 [D loss: 0.625755, acc: 66.41%] [G loss: 2.061540]\n",
      "epoch:12 step:11444 [D loss: 0.650221, acc: 68.75%] [G loss: 2.025697]\n",
      "epoch:12 step:11445 [D loss: 0.617195, acc: 64.06%] [G loss: 2.167117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11446 [D loss: 0.635169, acc: 67.19%] [G loss: 1.971526]\n",
      "epoch:12 step:11447 [D loss: 0.602217, acc: 67.97%] [G loss: 1.974010]\n",
      "epoch:12 step:11448 [D loss: 0.662194, acc: 63.28%] [G loss: 1.915778]\n",
      "epoch:12 step:11449 [D loss: 0.627131, acc: 65.62%] [G loss: 1.953954]\n",
      "epoch:12 step:11450 [D loss: 0.560772, acc: 75.78%] [G loss: 2.057674]\n",
      "epoch:12 step:11451 [D loss: 0.563143, acc: 72.66%] [G loss: 2.325198]\n",
      "epoch:12 step:11452 [D loss: 0.603372, acc: 68.75%] [G loss: 2.217335]\n",
      "epoch:12 step:11453 [D loss: 0.561275, acc: 71.88%] [G loss: 2.261613]\n",
      "epoch:12 step:11454 [D loss: 0.616065, acc: 65.62%] [G loss: 2.036007]\n",
      "epoch:12 step:11455 [D loss: 0.718552, acc: 52.34%] [G loss: 1.830627]\n",
      "epoch:12 step:11456 [D loss: 0.611039, acc: 69.53%] [G loss: 1.950049]\n",
      "epoch:12 step:11457 [D loss: 0.663532, acc: 62.50%] [G loss: 1.847034]\n",
      "epoch:12 step:11458 [D loss: 0.674469, acc: 59.38%] [G loss: 1.731024]\n",
      "epoch:12 step:11459 [D loss: 0.641232, acc: 63.28%] [G loss: 1.927580]\n",
      "epoch:12 step:11460 [D loss: 0.631421, acc: 67.19%] [G loss: 1.989447]\n",
      "epoch:12 step:11461 [D loss: 0.624649, acc: 61.72%] [G loss: 2.178516]\n",
      "epoch:12 step:11462 [D loss: 0.617988, acc: 67.19%] [G loss: 2.065631]\n",
      "epoch:12 step:11463 [D loss: 0.622888, acc: 63.28%] [G loss: 2.171213]\n",
      "epoch:12 step:11464 [D loss: 0.738876, acc: 53.91%] [G loss: 1.771620]\n",
      "epoch:12 step:11465 [D loss: 0.638999, acc: 62.50%] [G loss: 2.129103]\n",
      "epoch:12 step:11466 [D loss: 0.647892, acc: 59.38%] [G loss: 1.936810]\n",
      "epoch:12 step:11467 [D loss: 0.664327, acc: 61.72%] [G loss: 1.989817]\n",
      "epoch:12 step:11468 [D loss: 0.644590, acc: 60.94%] [G loss: 2.015702]\n",
      "epoch:12 step:11469 [D loss: 0.659463, acc: 64.06%] [G loss: 1.943697]\n",
      "epoch:12 step:11470 [D loss: 0.665186, acc: 55.47%] [G loss: 1.805378]\n",
      "epoch:12 step:11471 [D loss: 0.666549, acc: 61.72%] [G loss: 1.893775]\n",
      "epoch:12 step:11472 [D loss: 0.665893, acc: 60.16%] [G loss: 1.895192]\n",
      "epoch:12 step:11473 [D loss: 0.558825, acc: 67.97%] [G loss: 2.173110]\n",
      "epoch:12 step:11474 [D loss: 0.601204, acc: 67.19%] [G loss: 2.215762]\n",
      "epoch:12 step:11475 [D loss: 0.605805, acc: 67.19%] [G loss: 2.222065]\n",
      "epoch:12 step:11476 [D loss: 0.554010, acc: 69.53%] [G loss: 2.458769]\n",
      "epoch:12 step:11477 [D loss: 0.718430, acc: 57.03%] [G loss: 2.017252]\n",
      "epoch:12 step:11478 [D loss: 0.634060, acc: 60.16%] [G loss: 1.811494]\n",
      "epoch:12 step:11479 [D loss: 0.641529, acc: 67.19%] [G loss: 1.968435]\n",
      "epoch:12 step:11480 [D loss: 0.689545, acc: 60.16%] [G loss: 2.096742]\n",
      "epoch:12 step:11481 [D loss: 0.616518, acc: 69.53%] [G loss: 1.937800]\n",
      "epoch:12 step:11482 [D loss: 0.599162, acc: 67.97%] [G loss: 2.206948]\n",
      "epoch:12 step:11483 [D loss: 0.648141, acc: 60.16%] [G loss: 2.016532]\n",
      "epoch:12 step:11484 [D loss: 0.652964, acc: 59.38%] [G loss: 1.939372]\n",
      "epoch:12 step:11485 [D loss: 0.648249, acc: 64.06%] [G loss: 2.029454]\n",
      "epoch:12 step:11486 [D loss: 0.606775, acc: 66.41%] [G loss: 2.111016]\n",
      "epoch:12 step:11487 [D loss: 0.634413, acc: 60.94%] [G loss: 2.086465]\n",
      "epoch:12 step:11488 [D loss: 0.670986, acc: 53.12%] [G loss: 2.128770]\n",
      "epoch:12 step:11489 [D loss: 0.618169, acc: 68.75%] [G loss: 2.015016]\n",
      "epoch:12 step:11490 [D loss: 0.668607, acc: 57.81%] [G loss: 2.031493]\n",
      "epoch:12 step:11491 [D loss: 0.636781, acc: 64.06%] [G loss: 2.137291]\n",
      "epoch:12 step:11492 [D loss: 0.581108, acc: 66.41%] [G loss: 2.067966]\n",
      "epoch:12 step:11493 [D loss: 0.691617, acc: 54.69%] [G loss: 1.925380]\n",
      "epoch:12 step:11494 [D loss: 0.700057, acc: 61.72%] [G loss: 1.733868]\n",
      "epoch:12 step:11495 [D loss: 0.687911, acc: 54.69%] [G loss: 1.848233]\n",
      "epoch:12 step:11496 [D loss: 0.681488, acc: 53.91%] [G loss: 1.786636]\n",
      "epoch:12 step:11497 [D loss: 0.617605, acc: 60.94%] [G loss: 1.960451]\n",
      "epoch:12 step:11498 [D loss: 0.624497, acc: 64.06%] [G loss: 1.875718]\n",
      "epoch:12 step:11499 [D loss: 0.636834, acc: 64.84%] [G loss: 1.950186]\n",
      "epoch:12 step:11500 [D loss: 0.618551, acc: 63.28%] [G loss: 2.014035]\n",
      "epoch:12 step:11501 [D loss: 0.615358, acc: 70.31%] [G loss: 1.989403]\n",
      "epoch:12 step:11502 [D loss: 0.674252, acc: 60.94%] [G loss: 2.042651]\n",
      "epoch:12 step:11503 [D loss: 0.621495, acc: 68.75%] [G loss: 2.055391]\n",
      "epoch:12 step:11504 [D loss: 0.703112, acc: 58.59%] [G loss: 1.925114]\n",
      "epoch:12 step:11505 [D loss: 0.659825, acc: 61.72%] [G loss: 1.972057]\n",
      "epoch:12 step:11506 [D loss: 0.623070, acc: 64.06%] [G loss: 2.035400]\n",
      "epoch:12 step:11507 [D loss: 0.635423, acc: 60.94%] [G loss: 1.979632]\n",
      "epoch:12 step:11508 [D loss: 0.640015, acc: 64.06%] [G loss: 1.984912]\n",
      "epoch:12 step:11509 [D loss: 0.721401, acc: 53.12%] [G loss: 1.907993]\n",
      "epoch:12 step:11510 [D loss: 0.651806, acc: 62.50%] [G loss: 1.854583]\n",
      "epoch:12 step:11511 [D loss: 0.665679, acc: 59.38%] [G loss: 1.942428]\n",
      "epoch:12 step:11512 [D loss: 0.639322, acc: 59.38%] [G loss: 1.830586]\n",
      "epoch:12 step:11513 [D loss: 0.623283, acc: 66.41%] [G loss: 2.044460]\n",
      "epoch:12 step:11514 [D loss: 0.621890, acc: 61.72%] [G loss: 2.151352]\n",
      "epoch:12 step:11515 [D loss: 0.637296, acc: 60.16%] [G loss: 1.964154]\n",
      "epoch:12 step:11516 [D loss: 0.592950, acc: 69.53%] [G loss: 2.109824]\n",
      "epoch:12 step:11517 [D loss: 0.630485, acc: 61.72%] [G loss: 1.994045]\n",
      "epoch:12 step:11518 [D loss: 0.609534, acc: 67.97%] [G loss: 2.039968]\n",
      "epoch:12 step:11519 [D loss: 0.623200, acc: 66.41%] [G loss: 2.198646]\n",
      "epoch:12 step:11520 [D loss: 0.626141, acc: 64.84%] [G loss: 2.165901]\n",
      "epoch:12 step:11521 [D loss: 0.713860, acc: 51.56%] [G loss: 1.847634]\n",
      "epoch:12 step:11522 [D loss: 0.661244, acc: 61.72%] [G loss: 1.870881]\n",
      "epoch:12 step:11523 [D loss: 0.668761, acc: 59.38%] [G loss: 2.019735]\n",
      "epoch:12 step:11524 [D loss: 0.688990, acc: 59.38%] [G loss: 1.960491]\n",
      "epoch:12 step:11525 [D loss: 0.641048, acc: 62.50%] [G loss: 1.979973]\n",
      "epoch:12 step:11526 [D loss: 0.686016, acc: 59.38%] [G loss: 1.957034]\n",
      "epoch:12 step:11527 [D loss: 0.656720, acc: 63.28%] [G loss: 1.836782]\n",
      "epoch:12 step:11528 [D loss: 0.633666, acc: 61.72%] [G loss: 2.084868]\n",
      "epoch:12 step:11529 [D loss: 0.614443, acc: 69.53%] [G loss: 1.911235]\n",
      "epoch:12 step:11530 [D loss: 0.603013, acc: 68.75%] [G loss: 2.103571]\n",
      "epoch:12 step:11531 [D loss: 0.648230, acc: 57.03%] [G loss: 1.873055]\n",
      "epoch:12 step:11532 [D loss: 0.621235, acc: 67.19%] [G loss: 1.896655]\n",
      "epoch:12 step:11533 [D loss: 0.612256, acc: 65.62%] [G loss: 1.942659]\n",
      "epoch:12 step:11534 [D loss: 0.592425, acc: 68.75%] [G loss: 2.052989]\n",
      "epoch:12 step:11535 [D loss: 0.609549, acc: 68.75%] [G loss: 2.147530]\n",
      "epoch:12 step:11536 [D loss: 0.655145, acc: 66.41%] [G loss: 1.845096]\n",
      "epoch:12 step:11537 [D loss: 0.627351, acc: 63.28%] [G loss: 2.048842]\n",
      "epoch:12 step:11538 [D loss: 0.654879, acc: 62.50%] [G loss: 1.976469]\n",
      "epoch:12 step:11539 [D loss: 0.680109, acc: 58.59%] [G loss: 1.988779]\n",
      "epoch:12 step:11540 [D loss: 0.633554, acc: 68.75%] [G loss: 1.968517]\n",
      "epoch:12 step:11541 [D loss: 0.675923, acc: 58.59%] [G loss: 2.085383]\n",
      "epoch:12 step:11542 [D loss: 0.683135, acc: 60.16%] [G loss: 1.992496]\n",
      "epoch:12 step:11543 [D loss: 0.618028, acc: 63.28%] [G loss: 2.163471]\n",
      "epoch:12 step:11544 [D loss: 0.631424, acc: 67.19%] [G loss: 1.991096]\n",
      "epoch:12 step:11545 [D loss: 0.632219, acc: 64.84%] [G loss: 1.858217]\n",
      "epoch:12 step:11546 [D loss: 0.631240, acc: 66.41%] [G loss: 1.896988]\n",
      "epoch:12 step:11547 [D loss: 0.665049, acc: 57.81%] [G loss: 1.886223]\n",
      "epoch:12 step:11548 [D loss: 0.671069, acc: 58.59%] [G loss: 1.788301]\n",
      "epoch:12 step:11549 [D loss: 0.702915, acc: 53.91%] [G loss: 1.803931]\n",
      "epoch:12 step:11550 [D loss: 0.669945, acc: 58.59%] [G loss: 1.782500]\n",
      "epoch:12 step:11551 [D loss: 0.622953, acc: 68.75%] [G loss: 1.797907]\n",
      "epoch:12 step:11552 [D loss: 0.667443, acc: 61.72%] [G loss: 1.826319]\n",
      "epoch:12 step:11553 [D loss: 0.668332, acc: 64.84%] [G loss: 1.850214]\n",
      "epoch:12 step:11554 [D loss: 0.619322, acc: 67.97%] [G loss: 1.822068]\n",
      "epoch:12 step:11555 [D loss: 0.650265, acc: 60.16%] [G loss: 1.776232]\n",
      "epoch:12 step:11556 [D loss: 0.571115, acc: 73.44%] [G loss: 2.241590]\n",
      "epoch:12 step:11557 [D loss: 0.602915, acc: 71.09%] [G loss: 2.210623]\n",
      "epoch:12 step:11558 [D loss: 0.598694, acc: 71.88%] [G loss: 2.280387]\n",
      "epoch:12 step:11559 [D loss: 0.555851, acc: 74.22%] [G loss: 2.336540]\n",
      "epoch:12 step:11560 [D loss: 0.717446, acc: 52.34%] [G loss: 1.955516]\n",
      "epoch:12 step:11561 [D loss: 0.657360, acc: 64.84%] [G loss: 1.925041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11562 [D loss: 0.646607, acc: 61.72%] [G loss: 2.085255]\n",
      "epoch:12 step:11563 [D loss: 0.550977, acc: 71.88%] [G loss: 1.867468]\n",
      "epoch:12 step:11564 [D loss: 0.658932, acc: 58.59%] [G loss: 2.039410]\n",
      "epoch:12 step:11565 [D loss: 0.594104, acc: 64.84%] [G loss: 2.058958]\n",
      "epoch:12 step:11566 [D loss: 0.616798, acc: 61.72%] [G loss: 1.965795]\n",
      "epoch:12 step:11567 [D loss: 0.666174, acc: 62.50%] [G loss: 1.807536]\n",
      "epoch:12 step:11568 [D loss: 0.617408, acc: 68.75%] [G loss: 1.888287]\n",
      "epoch:12 step:11569 [D loss: 0.601355, acc: 67.97%] [G loss: 2.039636]\n",
      "epoch:12 step:11570 [D loss: 0.644805, acc: 60.94%] [G loss: 1.948067]\n",
      "epoch:12 step:11571 [D loss: 0.571861, acc: 76.56%] [G loss: 1.965967]\n",
      "epoch:12 step:11572 [D loss: 0.694124, acc: 58.59%] [G loss: 1.953430]\n",
      "epoch:12 step:11573 [D loss: 0.679981, acc: 54.69%] [G loss: 1.954654]\n",
      "epoch:12 step:11574 [D loss: 0.664333, acc: 62.50%] [G loss: 1.916377]\n",
      "epoch:12 step:11575 [D loss: 0.597413, acc: 68.75%] [G loss: 1.983341]\n",
      "epoch:12 step:11576 [D loss: 0.604200, acc: 70.31%] [G loss: 1.978815]\n",
      "epoch:12 step:11577 [D loss: 0.646620, acc: 64.84%] [G loss: 1.919729]\n",
      "epoch:12 step:11578 [D loss: 0.619772, acc: 69.53%] [G loss: 1.920539]\n",
      "epoch:12 step:11579 [D loss: 0.605041, acc: 71.09%] [G loss: 2.088869]\n",
      "epoch:12 step:11580 [D loss: 0.636740, acc: 64.06%] [G loss: 2.026860]\n",
      "epoch:12 step:11581 [D loss: 0.583566, acc: 66.41%] [G loss: 2.062768]\n",
      "epoch:12 step:11582 [D loss: 0.725443, acc: 54.69%] [G loss: 1.935883]\n",
      "epoch:12 step:11583 [D loss: 0.599532, acc: 68.75%] [G loss: 2.070180]\n",
      "epoch:12 step:11584 [D loss: 0.598652, acc: 68.75%] [G loss: 2.097105]\n",
      "epoch:12 step:11585 [D loss: 0.656692, acc: 60.94%] [G loss: 2.024467]\n",
      "epoch:12 step:11586 [D loss: 0.684336, acc: 55.47%] [G loss: 1.931714]\n",
      "epoch:12 step:11587 [D loss: 0.657589, acc: 63.28%] [G loss: 1.998700]\n",
      "epoch:12 step:11588 [D loss: 0.648558, acc: 62.50%] [G loss: 2.021589]\n",
      "epoch:12 step:11589 [D loss: 0.554934, acc: 71.09%] [G loss: 2.192586]\n",
      "epoch:12 step:11590 [D loss: 0.592092, acc: 71.09%] [G loss: 2.203441]\n",
      "epoch:12 step:11591 [D loss: 0.597623, acc: 73.44%] [G loss: 2.276650]\n",
      "epoch:12 step:11592 [D loss: 0.802372, acc: 49.22%] [G loss: 1.945381]\n",
      "epoch:12 step:11593 [D loss: 0.702266, acc: 51.56%] [G loss: 1.783643]\n",
      "epoch:12 step:11594 [D loss: 0.617513, acc: 65.62%] [G loss: 1.922717]\n",
      "epoch:12 step:11595 [D loss: 0.646596, acc: 67.19%] [G loss: 1.792282]\n",
      "epoch:12 step:11596 [D loss: 0.707922, acc: 53.91%] [G loss: 1.782781]\n",
      "epoch:12 step:11597 [D loss: 0.690391, acc: 62.50%] [G loss: 1.973031]\n",
      "epoch:12 step:11598 [D loss: 0.541476, acc: 75.78%] [G loss: 1.947319]\n",
      "epoch:12 step:11599 [D loss: 0.661958, acc: 61.72%] [G loss: 1.755532]\n",
      "epoch:12 step:11600 [D loss: 0.675753, acc: 60.94%] [G loss: 1.859143]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.900712\n",
      "FID: 19.155930\n",
      "0 = 12.693701599693243\n",
      "1 = 0.08448747273419432\n",
      "2 = 0.886900007724762\n",
      "3 = 0.9002000093460083\n",
      "4 = 0.8736000061035156\n",
      "5 = 0.876875102519989\n",
      "6 = 0.9002000093460083\n",
      "7 = 6.988265942263601\n",
      "8 = 0.08318856698490457\n",
      "9 = 0.7459999918937683\n",
      "10 = 0.7623999714851379\n",
      "11 = 0.7296000123023987\n",
      "12 = 0.7381874322891235\n",
      "13 = 0.7623999714851379\n",
      "14 = 6.900743007659912\n",
      "15 = 9.43883228302002\n",
      "16 = 0.12841258943080902\n",
      "17 = 6.900712013244629\n",
      "18 = 19.155929565429688\n",
      "epoch:12 step:11601 [D loss: 0.665999, acc: 59.38%] [G loss: 1.920390]\n",
      "epoch:12 step:11602 [D loss: 0.638018, acc: 58.59%] [G loss: 2.003762]\n",
      "epoch:12 step:11603 [D loss: 0.619158, acc: 64.06%] [G loss: 1.930720]\n",
      "epoch:12 step:11604 [D loss: 0.631783, acc: 66.41%] [G loss: 2.050685]\n",
      "epoch:12 step:11605 [D loss: 0.587913, acc: 68.75%] [G loss: 1.843312]\n",
      "epoch:12 step:11606 [D loss: 0.670198, acc: 60.94%] [G loss: 1.828344]\n",
      "epoch:12 step:11607 [D loss: 0.579761, acc: 73.44%] [G loss: 1.964623]\n",
      "epoch:12 step:11608 [D loss: 0.642134, acc: 64.84%] [G loss: 2.030632]\n",
      "epoch:12 step:11609 [D loss: 0.675279, acc: 50.78%] [G loss: 1.848789]\n",
      "epoch:12 step:11610 [D loss: 0.635700, acc: 64.06%] [G loss: 2.144820]\n",
      "epoch:12 step:11611 [D loss: 0.668209, acc: 58.59%] [G loss: 2.142693]\n",
      "epoch:12 step:11612 [D loss: 0.601284, acc: 64.84%] [G loss: 1.977499]\n",
      "epoch:12 step:11613 [D loss: 0.651741, acc: 69.53%] [G loss: 2.060919]\n",
      "epoch:12 step:11614 [D loss: 0.630900, acc: 63.28%] [G loss: 2.163528]\n",
      "epoch:12 step:11615 [D loss: 0.603423, acc: 65.62%] [G loss: 2.226987]\n",
      "epoch:12 step:11616 [D loss: 0.621962, acc: 63.28%] [G loss: 2.054411]\n",
      "epoch:12 step:11617 [D loss: 0.753422, acc: 48.44%] [G loss: 1.866060]\n",
      "epoch:12 step:11618 [D loss: 0.623982, acc: 63.28%] [G loss: 2.036995]\n",
      "epoch:12 step:11619 [D loss: 0.691838, acc: 56.25%] [G loss: 1.778503]\n",
      "epoch:12 step:11620 [D loss: 0.725289, acc: 51.56%] [G loss: 1.857054]\n",
      "epoch:12 step:11621 [D loss: 0.766368, acc: 49.22%] [G loss: 1.739640]\n",
      "epoch:12 step:11622 [D loss: 0.617468, acc: 67.19%] [G loss: 1.781629]\n",
      "epoch:12 step:11623 [D loss: 0.663713, acc: 54.69%] [G loss: 1.837194]\n",
      "epoch:12 step:11624 [D loss: 0.562850, acc: 70.31%] [G loss: 2.028922]\n",
      "epoch:12 step:11625 [D loss: 0.605635, acc: 68.75%] [G loss: 1.943586]\n",
      "epoch:12 step:11626 [D loss: 0.610887, acc: 68.75%] [G loss: 1.892800]\n",
      "epoch:12 step:11627 [D loss: 0.657093, acc: 62.50%] [G loss: 1.895806]\n",
      "epoch:12 step:11628 [D loss: 0.606118, acc: 69.53%] [G loss: 2.000924]\n",
      "epoch:12 step:11629 [D loss: 0.611433, acc: 67.19%] [G loss: 1.974787]\n",
      "epoch:12 step:11630 [D loss: 0.674483, acc: 57.81%] [G loss: 1.833754]\n",
      "epoch:12 step:11631 [D loss: 0.654712, acc: 57.03%] [G loss: 1.764808]\n",
      "epoch:12 step:11632 [D loss: 0.645895, acc: 65.62%] [G loss: 1.896145]\n",
      "epoch:12 step:11633 [D loss: 0.665348, acc: 64.84%] [G loss: 1.877476]\n",
      "epoch:12 step:11634 [D loss: 0.655208, acc: 64.84%] [G loss: 1.839134]\n",
      "epoch:12 step:11635 [D loss: 0.626781, acc: 67.19%] [G loss: 1.937420]\n",
      "epoch:12 step:11636 [D loss: 0.566655, acc: 70.31%] [G loss: 1.953995]\n",
      "epoch:12 step:11637 [D loss: 0.646595, acc: 62.50%] [G loss: 1.830716]\n",
      "epoch:12 step:11638 [D loss: 0.645837, acc: 60.94%] [G loss: 2.008215]\n",
      "epoch:12 step:11639 [D loss: 0.632142, acc: 64.06%] [G loss: 1.967508]\n",
      "epoch:12 step:11640 [D loss: 0.697495, acc: 57.03%] [G loss: 1.764810]\n",
      "epoch:12 step:11641 [D loss: 0.701316, acc: 53.91%] [G loss: 1.839763]\n",
      "epoch:12 step:11642 [D loss: 0.642418, acc: 59.38%] [G loss: 1.879749]\n",
      "epoch:12 step:11643 [D loss: 0.615832, acc: 68.75%] [G loss: 1.881891]\n",
      "epoch:12 step:11644 [D loss: 0.645755, acc: 65.62%] [G loss: 1.911352]\n",
      "epoch:12 step:11645 [D loss: 0.639218, acc: 64.06%] [G loss: 2.010908]\n",
      "epoch:12 step:11646 [D loss: 0.581638, acc: 71.88%] [G loss: 2.031693]\n",
      "epoch:12 step:11647 [D loss: 0.631099, acc: 66.41%] [G loss: 1.950395]\n",
      "epoch:12 step:11648 [D loss: 0.638113, acc: 60.94%] [G loss: 1.925756]\n",
      "epoch:12 step:11649 [D loss: 0.630121, acc: 61.72%] [G loss: 2.117387]\n",
      "epoch:12 step:11650 [D loss: 0.620019, acc: 67.97%] [G loss: 2.122300]\n",
      "epoch:12 step:11651 [D loss: 0.639086, acc: 61.72%] [G loss: 1.827154]\n",
      "epoch:12 step:11652 [D loss: 0.640148, acc: 61.72%] [G loss: 1.925070]\n",
      "epoch:12 step:11653 [D loss: 0.622656, acc: 60.94%] [G loss: 2.085423]\n",
      "epoch:12 step:11654 [D loss: 0.740965, acc: 52.34%] [G loss: 1.863197]\n",
      "epoch:12 step:11655 [D loss: 0.635631, acc: 65.62%] [G loss: 2.009630]\n",
      "epoch:12 step:11656 [D loss: 0.652203, acc: 63.28%] [G loss: 1.958516]\n",
      "epoch:12 step:11657 [D loss: 0.625738, acc: 62.50%] [G loss: 2.005423]\n",
      "epoch:12 step:11658 [D loss: 0.608570, acc: 67.97%] [G loss: 2.006482]\n",
      "epoch:12 step:11659 [D loss: 0.575220, acc: 71.88%] [G loss: 2.162506]\n",
      "epoch:12 step:11660 [D loss: 0.583795, acc: 67.97%] [G loss: 2.184851]\n",
      "epoch:12 step:11661 [D loss: 0.652378, acc: 56.25%] [G loss: 1.892488]\n",
      "epoch:12 step:11662 [D loss: 0.675967, acc: 57.03%] [G loss: 1.955150]\n",
      "epoch:12 step:11663 [D loss: 0.681858, acc: 59.38%] [G loss: 1.881623]\n",
      "epoch:12 step:11664 [D loss: 0.681944, acc: 60.16%] [G loss: 1.870319]\n",
      "epoch:12 step:11665 [D loss: 0.689666, acc: 55.47%] [G loss: 1.892987]\n",
      "epoch:12 step:11666 [D loss: 0.638523, acc: 64.06%] [G loss: 1.916961]\n",
      "epoch:12 step:11667 [D loss: 0.657688, acc: 61.72%] [G loss: 1.919414]\n",
      "epoch:12 step:11668 [D loss: 0.686664, acc: 63.28%] [G loss: 1.841745]\n",
      "epoch:12 step:11669 [D loss: 0.665299, acc: 56.25%] [G loss: 1.956063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11670 [D loss: 0.604994, acc: 65.62%] [G loss: 1.883926]\n",
      "epoch:12 step:11671 [D loss: 0.633608, acc: 60.94%] [G loss: 2.000055]\n",
      "epoch:12 step:11672 [D loss: 0.607827, acc: 64.84%] [G loss: 2.126577]\n",
      "epoch:12 step:11673 [D loss: 0.581580, acc: 71.09%] [G loss: 2.179434]\n",
      "epoch:12 step:11674 [D loss: 0.537460, acc: 77.34%] [G loss: 2.095646]\n",
      "epoch:12 step:11675 [D loss: 0.677874, acc: 58.59%] [G loss: 1.988327]\n",
      "epoch:12 step:11676 [D loss: 0.695047, acc: 57.03%] [G loss: 2.058962]\n",
      "epoch:12 step:11677 [D loss: 0.637871, acc: 62.50%] [G loss: 1.933746]\n",
      "epoch:12 step:11678 [D loss: 0.612698, acc: 67.97%] [G loss: 2.047821]\n",
      "epoch:12 step:11679 [D loss: 0.657818, acc: 61.72%] [G loss: 1.902280]\n",
      "epoch:12 step:11680 [D loss: 0.620293, acc: 63.28%] [G loss: 1.963328]\n",
      "epoch:12 step:11681 [D loss: 0.666800, acc: 63.28%] [G loss: 1.859803]\n",
      "epoch:12 step:11682 [D loss: 0.680363, acc: 57.81%] [G loss: 1.872006]\n",
      "epoch:12 step:11683 [D loss: 0.611987, acc: 70.31%] [G loss: 1.866572]\n",
      "epoch:12 step:11684 [D loss: 0.640007, acc: 60.94%] [G loss: 1.864155]\n",
      "epoch:12 step:11685 [D loss: 0.729876, acc: 49.22%] [G loss: 1.918682]\n",
      "epoch:12 step:11686 [D loss: 0.640632, acc: 63.28%] [G loss: 1.814013]\n",
      "epoch:12 step:11687 [D loss: 0.650749, acc: 67.19%] [G loss: 1.885559]\n",
      "epoch:12 step:11688 [D loss: 0.618788, acc: 66.41%] [G loss: 1.986540]\n",
      "epoch:12 step:11689 [D loss: 0.641954, acc: 65.62%] [G loss: 1.943960]\n",
      "epoch:12 step:11690 [D loss: 0.616729, acc: 66.41%] [G loss: 1.882395]\n",
      "epoch:12 step:11691 [D loss: 0.636403, acc: 60.94%] [G loss: 1.907047]\n",
      "epoch:12 step:11692 [D loss: 0.717797, acc: 51.56%] [G loss: 1.799378]\n",
      "epoch:12 step:11693 [D loss: 0.601233, acc: 71.09%] [G loss: 2.046801]\n",
      "epoch:12 step:11694 [D loss: 0.649350, acc: 64.84%] [G loss: 1.893507]\n",
      "epoch:12 step:11695 [D loss: 0.643719, acc: 66.41%] [G loss: 1.991149]\n",
      "epoch:12 step:11696 [D loss: 0.660205, acc: 65.62%] [G loss: 1.906279]\n",
      "epoch:12 step:11697 [D loss: 0.609543, acc: 66.41%] [G loss: 1.995374]\n",
      "epoch:12 step:11698 [D loss: 0.660800, acc: 63.28%] [G loss: 1.902766]\n",
      "epoch:12 step:11699 [D loss: 0.688820, acc: 55.47%] [G loss: 1.881441]\n",
      "epoch:12 step:11700 [D loss: 0.591428, acc: 70.31%] [G loss: 2.013227]\n",
      "epoch:12 step:11701 [D loss: 0.605486, acc: 67.19%] [G loss: 2.092702]\n",
      "epoch:12 step:11702 [D loss: 0.654627, acc: 59.38%] [G loss: 1.819785]\n",
      "epoch:12 step:11703 [D loss: 0.676303, acc: 57.81%] [G loss: 1.899284]\n",
      "epoch:12 step:11704 [D loss: 0.723397, acc: 53.12%] [G loss: 1.842657]\n",
      "epoch:12 step:11705 [D loss: 0.646828, acc: 62.50%] [G loss: 1.866378]\n",
      "epoch:12 step:11706 [D loss: 0.684510, acc: 58.59%] [G loss: 1.968110]\n",
      "epoch:12 step:11707 [D loss: 0.629397, acc: 64.84%] [G loss: 1.992284]\n",
      "epoch:12 step:11708 [D loss: 0.632645, acc: 67.97%] [G loss: 1.904327]\n",
      "epoch:12 step:11709 [D loss: 0.647774, acc: 58.59%] [G loss: 2.140991]\n",
      "epoch:12 step:11710 [D loss: 0.648731, acc: 64.06%] [G loss: 2.026478]\n",
      "epoch:12 step:11711 [D loss: 0.659858, acc: 60.16%] [G loss: 1.923354]\n",
      "epoch:12 step:11712 [D loss: 0.647224, acc: 58.59%] [G loss: 1.943120]\n",
      "epoch:12 step:11713 [D loss: 0.567232, acc: 73.44%] [G loss: 2.091773]\n",
      "epoch:12 step:11714 [D loss: 0.579726, acc: 70.31%] [G loss: 2.117832]\n",
      "epoch:12 step:11715 [D loss: 0.576701, acc: 69.53%] [G loss: 2.548511]\n",
      "epoch:12 step:11716 [D loss: 0.583921, acc: 69.53%] [G loss: 2.517667]\n",
      "epoch:12 step:11717 [D loss: 0.690846, acc: 56.25%] [G loss: 1.805988]\n",
      "epoch:12 step:11718 [D loss: 0.598946, acc: 67.19%] [G loss: 1.979424]\n",
      "epoch:12 step:11719 [D loss: 0.683397, acc: 60.16%] [G loss: 1.944335]\n",
      "epoch:12 step:11720 [D loss: 0.614680, acc: 67.97%] [G loss: 2.036653]\n",
      "epoch:12 step:11721 [D loss: 0.689372, acc: 59.38%] [G loss: 1.782471]\n",
      "epoch:12 step:11722 [D loss: 0.703057, acc: 53.91%] [G loss: 1.849507]\n",
      "epoch:12 step:11723 [D loss: 0.629093, acc: 61.72%] [G loss: 2.119663]\n",
      "epoch:12 step:11724 [D loss: 0.686575, acc: 64.06%] [G loss: 1.976709]\n",
      "epoch:12 step:11725 [D loss: 0.564283, acc: 72.66%] [G loss: 2.133156]\n",
      "epoch:12 step:11726 [D loss: 0.620108, acc: 67.97%] [G loss: 1.874046]\n",
      "epoch:12 step:11727 [D loss: 0.664507, acc: 59.38%] [G loss: 1.801293]\n",
      "epoch:12 step:11728 [D loss: 0.585109, acc: 67.97%] [G loss: 2.107538]\n",
      "epoch:12 step:11729 [D loss: 0.647803, acc: 59.38%] [G loss: 1.957147]\n",
      "epoch:12 step:11730 [D loss: 0.645468, acc: 69.53%] [G loss: 1.966757]\n",
      "epoch:12 step:11731 [D loss: 0.636487, acc: 63.28%] [G loss: 1.967005]\n",
      "epoch:12 step:11732 [D loss: 0.595564, acc: 69.53%] [G loss: 2.149259]\n",
      "epoch:12 step:11733 [D loss: 0.617916, acc: 66.41%] [G loss: 2.000169]\n",
      "epoch:12 step:11734 [D loss: 0.599768, acc: 68.75%] [G loss: 1.888930]\n",
      "epoch:12 step:11735 [D loss: 0.657572, acc: 60.16%] [G loss: 1.905796]\n",
      "epoch:12 step:11736 [D loss: 0.645766, acc: 65.62%] [G loss: 1.912079]\n",
      "epoch:12 step:11737 [D loss: 0.610207, acc: 70.31%] [G loss: 1.863827]\n",
      "epoch:12 step:11738 [D loss: 0.630337, acc: 66.41%] [G loss: 2.238756]\n",
      "epoch:12 step:11739 [D loss: 0.592241, acc: 70.31%] [G loss: 2.174472]\n",
      "epoch:12 step:11740 [D loss: 0.638778, acc: 65.62%] [G loss: 1.875195]\n",
      "epoch:12 step:11741 [D loss: 0.656281, acc: 60.16%] [G loss: 2.084549]\n",
      "epoch:12 step:11742 [D loss: 0.601906, acc: 64.84%] [G loss: 2.165865]\n",
      "epoch:12 step:11743 [D loss: 0.586921, acc: 68.75%] [G loss: 2.020057]\n",
      "epoch:12 step:11744 [D loss: 0.723922, acc: 53.91%] [G loss: 1.769117]\n",
      "epoch:12 step:11745 [D loss: 0.725805, acc: 53.12%] [G loss: 1.729784]\n",
      "epoch:12 step:11746 [D loss: 0.668834, acc: 59.38%] [G loss: 1.904673]\n",
      "epoch:12 step:11747 [D loss: 0.679602, acc: 60.16%] [G loss: 1.883624]\n",
      "epoch:12 step:11748 [D loss: 0.613376, acc: 67.19%] [G loss: 1.984497]\n",
      "epoch:12 step:11749 [D loss: 0.581970, acc: 75.00%] [G loss: 1.963797]\n",
      "epoch:12 step:11750 [D loss: 0.678674, acc: 55.47%] [G loss: 1.870537]\n",
      "epoch:12 step:11751 [D loss: 0.689097, acc: 61.72%] [G loss: 1.873838]\n",
      "epoch:12 step:11752 [D loss: 0.596049, acc: 68.75%] [G loss: 1.960789]\n",
      "epoch:12 step:11753 [D loss: 0.649316, acc: 63.28%] [G loss: 2.004686]\n",
      "epoch:12 step:11754 [D loss: 0.652539, acc: 61.72%] [G loss: 2.017806]\n",
      "epoch:12 step:11755 [D loss: 0.673218, acc: 61.72%] [G loss: 1.792715]\n",
      "epoch:12 step:11756 [D loss: 0.637313, acc: 64.84%] [G loss: 1.979990]\n",
      "epoch:12 step:11757 [D loss: 0.581165, acc: 69.53%] [G loss: 2.184795]\n",
      "epoch:12 step:11758 [D loss: 0.588057, acc: 66.41%] [G loss: 1.964801]\n",
      "epoch:12 step:11759 [D loss: 0.604264, acc: 66.41%] [G loss: 2.153080]\n",
      "epoch:12 step:11760 [D loss: 0.639430, acc: 60.16%] [G loss: 2.012079]\n",
      "epoch:12 step:11761 [D loss: 0.666397, acc: 64.06%] [G loss: 1.982961]\n",
      "epoch:12 step:11762 [D loss: 0.601203, acc: 69.53%] [G loss: 1.913512]\n",
      "epoch:12 step:11763 [D loss: 0.623866, acc: 61.72%] [G loss: 1.956668]\n",
      "epoch:12 step:11764 [D loss: 0.586490, acc: 68.75%] [G loss: 2.055627]\n",
      "epoch:12 step:11765 [D loss: 0.606959, acc: 71.88%] [G loss: 2.100234]\n",
      "epoch:12 step:11766 [D loss: 0.599759, acc: 70.31%] [G loss: 2.279095]\n",
      "epoch:12 step:11767 [D loss: 0.592031, acc: 67.97%] [G loss: 2.167384]\n",
      "epoch:12 step:11768 [D loss: 0.587138, acc: 72.66%] [G loss: 2.027224]\n",
      "epoch:12 step:11769 [D loss: 0.664575, acc: 62.50%] [G loss: 2.235349]\n",
      "epoch:12 step:11770 [D loss: 0.574301, acc: 73.44%] [G loss: 2.055674]\n",
      "epoch:12 step:11771 [D loss: 0.634795, acc: 61.72%] [G loss: 1.906615]\n",
      "epoch:12 step:11772 [D loss: 0.691198, acc: 56.25%] [G loss: 1.781588]\n",
      "epoch:12 step:11773 [D loss: 0.650529, acc: 70.31%] [G loss: 1.745565]\n",
      "epoch:12 step:11774 [D loss: 0.658549, acc: 64.84%] [G loss: 1.937670]\n",
      "epoch:12 step:11775 [D loss: 0.619086, acc: 69.53%] [G loss: 2.029847]\n",
      "epoch:12 step:11776 [D loss: 0.571184, acc: 67.97%] [G loss: 2.019469]\n",
      "epoch:12 step:11777 [D loss: 0.584182, acc: 69.53%] [G loss: 2.019970]\n",
      "epoch:12 step:11778 [D loss: 0.661600, acc: 61.72%] [G loss: 2.084235]\n",
      "epoch:12 step:11779 [D loss: 0.618911, acc: 68.75%] [G loss: 2.110883]\n",
      "epoch:12 step:11780 [D loss: 0.612189, acc: 64.06%] [G loss: 1.966287]\n",
      "epoch:12 step:11781 [D loss: 0.684489, acc: 60.16%] [G loss: 1.846273]\n",
      "epoch:12 step:11782 [D loss: 0.643730, acc: 58.59%] [G loss: 1.827582]\n",
      "epoch:12 step:11783 [D loss: 0.636303, acc: 60.94%] [G loss: 1.978582]\n",
      "epoch:12 step:11784 [D loss: 0.716641, acc: 55.47%] [G loss: 1.975077]\n",
      "epoch:12 step:11785 [D loss: 0.670371, acc: 64.06%] [G loss: 1.906454]\n",
      "epoch:12 step:11786 [D loss: 0.684962, acc: 57.81%] [G loss: 1.774843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11787 [D loss: 0.667631, acc: 61.72%] [G loss: 1.986988]\n",
      "epoch:12 step:11788 [D loss: 0.620867, acc: 68.75%] [G loss: 1.918608]\n",
      "epoch:12 step:11789 [D loss: 0.674441, acc: 52.34%] [G loss: 1.921615]\n",
      "epoch:12 step:11790 [D loss: 0.664338, acc: 59.38%] [G loss: 1.855784]\n",
      "epoch:12 step:11791 [D loss: 0.609500, acc: 70.31%] [G loss: 2.003179]\n",
      "epoch:12 step:11792 [D loss: 0.623196, acc: 67.19%] [G loss: 1.969009]\n",
      "epoch:12 step:11793 [D loss: 0.600098, acc: 63.28%] [G loss: 2.042358]\n",
      "epoch:12 step:11794 [D loss: 0.632060, acc: 62.50%] [G loss: 1.944252]\n",
      "epoch:12 step:11795 [D loss: 0.595179, acc: 67.97%] [G loss: 2.040112]\n",
      "epoch:12 step:11796 [D loss: 0.616347, acc: 67.19%] [G loss: 1.807077]\n",
      "epoch:12 step:11797 [D loss: 0.702856, acc: 57.81%] [G loss: 1.857579]\n",
      "epoch:12 step:11798 [D loss: 0.628107, acc: 65.62%] [G loss: 2.138890]\n",
      "epoch:12 step:11799 [D loss: 0.635508, acc: 66.41%] [G loss: 1.993929]\n",
      "epoch:12 step:11800 [D loss: 0.563302, acc: 71.09%] [G loss: 2.000560]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.980192\n",
      "FID: 18.645176\n",
      "0 = 12.644961812257716\n",
      "1 = 0.08078435836507906\n",
      "2 = 0.8827999830245972\n",
      "3 = 0.8999999761581421\n",
      "4 = 0.8655999898910522\n",
      "5 = 0.8700696229934692\n",
      "6 = 0.8999999761581421\n",
      "7 = 6.888197868382923\n",
      "8 = 0.08254914062902961\n",
      "9 = 0.7383000254631042\n",
      "10 = 0.7444000244140625\n",
      "11 = 0.732200026512146\n",
      "12 = 0.7354277968406677\n",
      "13 = 0.7444000244140625\n",
      "14 = 6.980221748352051\n",
      "15 = 9.397448539733887\n",
      "16 = 0.13520054519176483\n",
      "17 = 6.980191707611084\n",
      "18 = 18.64517593383789\n",
      "epoch:12 step:11801 [D loss: 0.633855, acc: 64.84%] [G loss: 2.125630]\n",
      "epoch:12 step:11802 [D loss: 0.608922, acc: 68.75%] [G loss: 2.128456]\n",
      "epoch:12 step:11803 [D loss: 0.678890, acc: 57.81%] [G loss: 1.847286]\n",
      "epoch:12 step:11804 [D loss: 0.698418, acc: 54.69%] [G loss: 1.920287]\n",
      "epoch:12 step:11805 [D loss: 0.551405, acc: 78.12%] [G loss: 2.075606]\n",
      "epoch:12 step:11806 [D loss: 0.638238, acc: 58.59%] [G loss: 1.961742]\n",
      "epoch:12 step:11807 [D loss: 0.599415, acc: 71.88%] [G loss: 1.968637]\n",
      "epoch:12 step:11808 [D loss: 0.582945, acc: 67.97%] [G loss: 2.449858]\n",
      "epoch:12 step:11809 [D loss: 0.643471, acc: 61.72%] [G loss: 1.937684]\n",
      "epoch:12 step:11810 [D loss: 0.734278, acc: 52.34%] [G loss: 1.944113]\n",
      "epoch:12 step:11811 [D loss: 0.685748, acc: 60.16%] [G loss: 1.834316]\n",
      "epoch:12 step:11812 [D loss: 0.634030, acc: 63.28%] [G loss: 1.861271]\n",
      "epoch:12 step:11813 [D loss: 0.645973, acc: 61.72%] [G loss: 1.924952]\n",
      "epoch:12 step:11814 [D loss: 0.679341, acc: 56.25%] [G loss: 1.926849]\n",
      "epoch:12 step:11815 [D loss: 0.610422, acc: 67.19%] [G loss: 1.870496]\n",
      "epoch:12 step:11816 [D loss: 0.654244, acc: 58.59%] [G loss: 1.753124]\n",
      "epoch:12 step:11817 [D loss: 0.647806, acc: 62.50%] [G loss: 1.803625]\n",
      "epoch:12 step:11818 [D loss: 0.622777, acc: 70.31%] [G loss: 1.844627]\n",
      "epoch:12 step:11819 [D loss: 0.666320, acc: 60.94%] [G loss: 1.900487]\n",
      "epoch:12 step:11820 [D loss: 0.678618, acc: 60.16%] [G loss: 1.814829]\n",
      "epoch:12 step:11821 [D loss: 0.700553, acc: 57.81%] [G loss: 1.729159]\n",
      "epoch:12 step:11822 [D loss: 0.643727, acc: 63.28%] [G loss: 1.986485]\n",
      "epoch:12 step:11823 [D loss: 0.640860, acc: 60.94%] [G loss: 1.904201]\n",
      "epoch:12 step:11824 [D loss: 0.680471, acc: 60.94%] [G loss: 1.833122]\n",
      "epoch:12 step:11825 [D loss: 0.596192, acc: 67.19%] [G loss: 1.914487]\n",
      "epoch:12 step:11826 [D loss: 0.624833, acc: 63.28%] [G loss: 1.971052]\n",
      "epoch:12 step:11827 [D loss: 0.632470, acc: 64.06%] [G loss: 1.847513]\n",
      "epoch:12 step:11828 [D loss: 0.667325, acc: 59.38%] [G loss: 1.834502]\n",
      "epoch:12 step:11829 [D loss: 0.597830, acc: 66.41%] [G loss: 2.040324]\n",
      "epoch:12 step:11830 [D loss: 0.684505, acc: 56.25%] [G loss: 1.884307]\n",
      "epoch:12 step:11831 [D loss: 0.649931, acc: 63.28%] [G loss: 1.928978]\n",
      "epoch:12 step:11832 [D loss: 0.678240, acc: 62.50%] [G loss: 2.103235]\n",
      "epoch:12 step:11833 [D loss: 0.641724, acc: 60.16%] [G loss: 1.945253]\n",
      "epoch:12 step:11834 [D loss: 0.629251, acc: 65.62%] [G loss: 1.966522]\n",
      "epoch:12 step:11835 [D loss: 0.594931, acc: 67.97%] [G loss: 2.035913]\n",
      "epoch:12 step:11836 [D loss: 0.701780, acc: 58.59%] [G loss: 2.065150]\n",
      "epoch:12 step:11837 [D loss: 0.627440, acc: 62.50%] [G loss: 1.910473]\n",
      "epoch:12 step:11838 [D loss: 0.657317, acc: 63.28%] [G loss: 1.988694]\n",
      "epoch:12 step:11839 [D loss: 0.650075, acc: 61.72%] [G loss: 1.927149]\n",
      "epoch:12 step:11840 [D loss: 0.686243, acc: 56.25%] [G loss: 1.906520]\n",
      "epoch:12 step:11841 [D loss: 0.688339, acc: 55.47%] [G loss: 1.881843]\n",
      "epoch:12 step:11842 [D loss: 0.626956, acc: 65.62%] [G loss: 1.887257]\n",
      "epoch:12 step:11843 [D loss: 0.668661, acc: 60.16%] [G loss: 1.817865]\n",
      "epoch:12 step:11844 [D loss: 0.700121, acc: 54.69%] [G loss: 1.972135]\n",
      "epoch:12 step:11845 [D loss: 0.626655, acc: 64.06%] [G loss: 1.921069]\n",
      "epoch:12 step:11846 [D loss: 0.633574, acc: 59.38%] [G loss: 2.050292]\n",
      "epoch:12 step:11847 [D loss: 0.612960, acc: 67.19%] [G loss: 2.146201]\n",
      "epoch:12 step:11848 [D loss: 0.625446, acc: 64.84%] [G loss: 1.941279]\n",
      "epoch:12 step:11849 [D loss: 0.640932, acc: 64.06%] [G loss: 2.002637]\n",
      "epoch:12 step:11850 [D loss: 0.633244, acc: 61.72%] [G loss: 2.019998]\n",
      "epoch:12 step:11851 [D loss: 0.666303, acc: 61.72%] [G loss: 2.002605]\n",
      "epoch:12 step:11852 [D loss: 0.585747, acc: 71.09%] [G loss: 1.938229]\n",
      "epoch:12 step:11853 [D loss: 0.591106, acc: 71.09%] [G loss: 1.997917]\n",
      "epoch:12 step:11854 [D loss: 0.616000, acc: 65.62%] [G loss: 1.943197]\n",
      "epoch:12 step:11855 [D loss: 0.619039, acc: 68.75%] [G loss: 1.970741]\n",
      "epoch:12 step:11856 [D loss: 0.681935, acc: 62.50%] [G loss: 1.907699]\n",
      "epoch:12 step:11857 [D loss: 0.657333, acc: 61.72%] [G loss: 1.988164]\n",
      "epoch:12 step:11858 [D loss: 0.680133, acc: 53.91%] [G loss: 1.876613]\n",
      "epoch:12 step:11859 [D loss: 0.683332, acc: 56.25%] [G loss: 1.856100]\n",
      "epoch:12 step:11860 [D loss: 0.632961, acc: 65.62%] [G loss: 1.885431]\n",
      "epoch:12 step:11861 [D loss: 0.678501, acc: 57.81%] [G loss: 1.831107]\n",
      "epoch:12 step:11862 [D loss: 0.630088, acc: 62.50%] [G loss: 1.868844]\n",
      "epoch:12 step:11863 [D loss: 0.652441, acc: 59.38%] [G loss: 1.805555]\n",
      "epoch:12 step:11864 [D loss: 0.661482, acc: 64.84%] [G loss: 1.849326]\n",
      "epoch:12 step:11865 [D loss: 0.639559, acc: 64.06%] [G loss: 2.007087]\n",
      "epoch:12 step:11866 [D loss: 0.628724, acc: 60.94%] [G loss: 2.043145]\n",
      "epoch:12 step:11867 [D loss: 0.669165, acc: 59.38%] [G loss: 1.902405]\n",
      "epoch:12 step:11868 [D loss: 0.625073, acc: 69.53%] [G loss: 1.926624]\n",
      "epoch:12 step:11869 [D loss: 0.668503, acc: 59.38%] [G loss: 1.854613]\n",
      "epoch:12 step:11870 [D loss: 0.644964, acc: 62.50%] [G loss: 1.980455]\n",
      "epoch:12 step:11871 [D loss: 0.621637, acc: 68.75%] [G loss: 1.836754]\n",
      "epoch:12 step:11872 [D loss: 0.680672, acc: 64.06%] [G loss: 1.819738]\n",
      "epoch:12 step:11873 [D loss: 0.646777, acc: 59.38%] [G loss: 1.881625]\n",
      "epoch:12 step:11874 [D loss: 0.610485, acc: 67.97%] [G loss: 1.968536]\n",
      "epoch:12 step:11875 [D loss: 0.646174, acc: 61.72%] [G loss: 1.964813]\n",
      "epoch:12 step:11876 [D loss: 0.621433, acc: 60.94%] [G loss: 1.868275]\n",
      "epoch:12 step:11877 [D loss: 0.623073, acc: 65.62%] [G loss: 1.994252]\n",
      "epoch:12 step:11878 [D loss: 0.620631, acc: 69.53%] [G loss: 2.140812]\n",
      "epoch:12 step:11879 [D loss: 0.640176, acc: 62.50%] [G loss: 1.955688]\n",
      "epoch:12 step:11880 [D loss: 0.644026, acc: 63.28%] [G loss: 1.854210]\n",
      "epoch:12 step:11881 [D loss: 0.617742, acc: 62.50%] [G loss: 2.030706]\n",
      "epoch:12 step:11882 [D loss: 0.591886, acc: 65.62%] [G loss: 1.868843]\n",
      "epoch:12 step:11883 [D loss: 0.597464, acc: 63.28%] [G loss: 2.009196]\n",
      "epoch:12 step:11884 [D loss: 0.662320, acc: 61.72%] [G loss: 2.097377]\n",
      "epoch:12 step:11885 [D loss: 0.619265, acc: 65.62%] [G loss: 2.042106]\n",
      "epoch:12 step:11886 [D loss: 0.677913, acc: 57.03%] [G loss: 2.026840]\n",
      "epoch:12 step:11887 [D loss: 0.628616, acc: 62.50%] [G loss: 2.099951]\n",
      "epoch:12 step:11888 [D loss: 0.627321, acc: 65.62%] [G loss: 2.057724]\n",
      "epoch:12 step:11889 [D loss: 0.646267, acc: 57.03%] [G loss: 2.043971]\n",
      "epoch:12 step:11890 [D loss: 0.654477, acc: 64.06%] [G loss: 2.004486]\n",
      "epoch:12 step:11891 [D loss: 0.598695, acc: 68.75%] [G loss: 2.054731]\n",
      "epoch:12 step:11892 [D loss: 0.544323, acc: 72.66%] [G loss: 2.459677]\n",
      "epoch:12 step:11893 [D loss: 0.611089, acc: 67.97%] [G loss: 2.280111]\n",
      "epoch:12 step:11894 [D loss: 0.620488, acc: 68.75%] [G loss: 2.204009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11895 [D loss: 0.608625, acc: 64.06%] [G loss: 2.122050]\n",
      "epoch:12 step:11896 [D loss: 0.686793, acc: 60.16%] [G loss: 2.062468]\n",
      "epoch:12 step:11897 [D loss: 0.600522, acc: 63.28%] [G loss: 2.049089]\n",
      "epoch:12 step:11898 [D loss: 0.657274, acc: 56.25%] [G loss: 2.139924]\n",
      "epoch:12 step:11899 [D loss: 0.634875, acc: 64.84%] [G loss: 1.944339]\n",
      "epoch:12 step:11900 [D loss: 0.700251, acc: 57.03%] [G loss: 1.971910]\n",
      "epoch:12 step:11901 [D loss: 0.678648, acc: 60.94%] [G loss: 1.689571]\n",
      "epoch:12 step:11902 [D loss: 0.622853, acc: 62.50%] [G loss: 1.800932]\n",
      "epoch:12 step:11903 [D loss: 0.619106, acc: 64.84%] [G loss: 1.917207]\n",
      "epoch:12 step:11904 [D loss: 0.635492, acc: 65.62%] [G loss: 2.007172]\n",
      "epoch:12 step:11905 [D loss: 0.646328, acc: 62.50%] [G loss: 1.847455]\n",
      "epoch:12 step:11906 [D loss: 0.616672, acc: 62.50%] [G loss: 2.008600]\n",
      "epoch:12 step:11907 [D loss: 0.626647, acc: 63.28%] [G loss: 1.954675]\n",
      "epoch:12 step:11908 [D loss: 0.615230, acc: 64.06%] [G loss: 1.874667]\n",
      "epoch:12 step:11909 [D loss: 0.656279, acc: 58.59%] [G loss: 1.852877]\n",
      "epoch:12 step:11910 [D loss: 0.656919, acc: 63.28%] [G loss: 1.878163]\n",
      "epoch:12 step:11911 [D loss: 0.678937, acc: 52.34%] [G loss: 1.760332]\n",
      "epoch:12 step:11912 [D loss: 0.651558, acc: 62.50%] [G loss: 2.057194]\n",
      "epoch:12 step:11913 [D loss: 0.636422, acc: 62.50%] [G loss: 1.822201]\n",
      "epoch:12 step:11914 [D loss: 0.691777, acc: 60.16%] [G loss: 1.866635]\n",
      "epoch:12 step:11915 [D loss: 0.626908, acc: 65.62%] [G loss: 1.835448]\n",
      "epoch:12 step:11916 [D loss: 0.622113, acc: 67.97%] [G loss: 1.908373]\n",
      "epoch:12 step:11917 [D loss: 0.673769, acc: 61.72%] [G loss: 1.808900]\n",
      "epoch:12 step:11918 [D loss: 0.639517, acc: 63.28%] [G loss: 1.808259]\n",
      "epoch:12 step:11919 [D loss: 0.686923, acc: 53.12%] [G loss: 1.870011]\n",
      "epoch:12 step:11920 [D loss: 0.706480, acc: 53.91%] [G loss: 1.784774]\n",
      "epoch:12 step:11921 [D loss: 0.646477, acc: 60.16%] [G loss: 1.918005]\n",
      "epoch:12 step:11922 [D loss: 0.622607, acc: 66.41%] [G loss: 1.899581]\n",
      "epoch:12 step:11923 [D loss: 0.637032, acc: 62.50%] [G loss: 2.026228]\n",
      "epoch:12 step:11924 [D loss: 0.661665, acc: 61.72%] [G loss: 2.024770]\n",
      "epoch:12 step:11925 [D loss: 0.640745, acc: 64.06%] [G loss: 1.867590]\n",
      "epoch:12 step:11926 [D loss: 0.670863, acc: 63.28%] [G loss: 1.826319]\n",
      "epoch:12 step:11927 [D loss: 0.708048, acc: 56.25%] [G loss: 1.785094]\n",
      "epoch:12 step:11928 [D loss: 0.642289, acc: 57.03%] [G loss: 1.884482]\n",
      "epoch:12 step:11929 [D loss: 0.586318, acc: 74.22%] [G loss: 2.058220]\n",
      "epoch:12 step:11930 [D loss: 0.655543, acc: 57.03%] [G loss: 1.802126]\n",
      "epoch:12 step:11931 [D loss: 0.670471, acc: 58.59%] [G loss: 1.871625]\n",
      "epoch:12 step:11932 [D loss: 0.628510, acc: 63.28%] [G loss: 1.863128]\n",
      "epoch:12 step:11933 [D loss: 0.637594, acc: 62.50%] [G loss: 1.875418]\n",
      "epoch:12 step:11934 [D loss: 0.623960, acc: 71.09%] [G loss: 1.929988]\n",
      "epoch:12 step:11935 [D loss: 0.611474, acc: 65.62%] [G loss: 2.072873]\n",
      "epoch:12 step:11936 [D loss: 0.612088, acc: 66.41%] [G loss: 2.143286]\n",
      "epoch:12 step:11937 [D loss: 0.606332, acc: 71.09%] [G loss: 2.148100]\n",
      "epoch:12 step:11938 [D loss: 0.626616, acc: 67.19%] [G loss: 2.204003]\n",
      "epoch:12 step:11939 [D loss: 0.623373, acc: 64.84%] [G loss: 2.067685]\n",
      "epoch:12 step:11940 [D loss: 0.643800, acc: 55.47%] [G loss: 1.908523]\n",
      "epoch:12 step:11941 [D loss: 0.604635, acc: 64.84%] [G loss: 1.989568]\n",
      "epoch:12 step:11942 [D loss: 0.721834, acc: 52.34%] [G loss: 1.907198]\n",
      "epoch:12 step:11943 [D loss: 0.608918, acc: 64.06%] [G loss: 1.983120]\n",
      "epoch:12 step:11944 [D loss: 0.622554, acc: 71.09%] [G loss: 2.165413]\n",
      "epoch:12 step:11945 [D loss: 0.610649, acc: 67.97%] [G loss: 1.986124]\n",
      "epoch:12 step:11946 [D loss: 0.643227, acc: 66.41%] [G loss: 1.865864]\n",
      "epoch:12 step:11947 [D loss: 0.676377, acc: 57.81%] [G loss: 1.832158]\n",
      "epoch:12 step:11948 [D loss: 0.711681, acc: 53.91%] [G loss: 1.737737]\n",
      "epoch:12 step:11949 [D loss: 0.676484, acc: 55.47%] [G loss: 1.886732]\n",
      "epoch:12 step:11950 [D loss: 0.640566, acc: 60.16%] [G loss: 1.958792]\n",
      "epoch:12 step:11951 [D loss: 0.649052, acc: 62.50%] [G loss: 2.097886]\n",
      "epoch:12 step:11952 [D loss: 0.593831, acc: 67.97%] [G loss: 2.118483]\n",
      "epoch:12 step:11953 [D loss: 0.604887, acc: 69.53%] [G loss: 2.087667]\n",
      "epoch:12 step:11954 [D loss: 0.624502, acc: 67.97%] [G loss: 1.948753]\n",
      "epoch:12 step:11955 [D loss: 0.628442, acc: 67.97%] [G loss: 1.920778]\n",
      "epoch:12 step:11956 [D loss: 0.602688, acc: 66.41%] [G loss: 2.133756]\n",
      "epoch:12 step:11957 [D loss: 0.677055, acc: 56.25%] [G loss: 1.963403]\n",
      "epoch:12 step:11958 [D loss: 0.623396, acc: 60.94%] [G loss: 1.911749]\n",
      "epoch:12 step:11959 [D loss: 0.618489, acc: 67.97%] [G loss: 2.049980]\n",
      "epoch:12 step:11960 [D loss: 0.636721, acc: 67.19%] [G loss: 1.800437]\n",
      "epoch:12 step:11961 [D loss: 0.726251, acc: 60.94%] [G loss: 1.904105]\n",
      "epoch:12 step:11962 [D loss: 0.617529, acc: 71.09%] [G loss: 1.904749]\n",
      "epoch:12 step:11963 [D loss: 0.656538, acc: 62.50%] [G loss: 2.212016]\n",
      "epoch:12 step:11964 [D loss: 0.604873, acc: 67.97%] [G loss: 2.085050]\n",
      "epoch:12 step:11965 [D loss: 0.631985, acc: 63.28%] [G loss: 2.020865]\n",
      "epoch:12 step:11966 [D loss: 0.705767, acc: 59.38%] [G loss: 1.963691]\n",
      "epoch:12 step:11967 [D loss: 0.617490, acc: 64.06%] [G loss: 1.981086]\n",
      "epoch:12 step:11968 [D loss: 0.618290, acc: 67.97%] [G loss: 1.962870]\n",
      "epoch:12 step:11969 [D loss: 0.616317, acc: 64.06%] [G loss: 2.053221]\n",
      "epoch:12 step:11970 [D loss: 0.680887, acc: 61.72%] [G loss: 1.950826]\n",
      "epoch:12 step:11971 [D loss: 0.647548, acc: 61.72%] [G loss: 1.899947]\n",
      "epoch:12 step:11972 [D loss: 0.643907, acc: 68.75%] [G loss: 1.813024]\n",
      "epoch:12 step:11973 [D loss: 0.678825, acc: 57.03%] [G loss: 1.921367]\n",
      "epoch:12 step:11974 [D loss: 0.695564, acc: 55.47%] [G loss: 1.945855]\n",
      "epoch:12 step:11975 [D loss: 0.624903, acc: 64.84%] [G loss: 1.922872]\n",
      "epoch:12 step:11976 [D loss: 0.576575, acc: 67.97%] [G loss: 2.093535]\n",
      "epoch:12 step:11977 [D loss: 0.663846, acc: 61.72%] [G loss: 1.816455]\n",
      "epoch:12 step:11978 [D loss: 0.669547, acc: 61.72%] [G loss: 1.870826]\n",
      "epoch:12 step:11979 [D loss: 0.633935, acc: 63.28%] [G loss: 2.059391]\n",
      "epoch:12 step:11980 [D loss: 0.615949, acc: 71.88%] [G loss: 2.012030]\n",
      "epoch:12 step:11981 [D loss: 0.634173, acc: 63.28%] [G loss: 1.898024]\n",
      "epoch:12 step:11982 [D loss: 0.634049, acc: 62.50%] [G loss: 1.812379]\n",
      "epoch:12 step:11983 [D loss: 0.690186, acc: 60.16%] [G loss: 1.883263]\n",
      "epoch:12 step:11984 [D loss: 0.624174, acc: 64.84%] [G loss: 1.920945]\n",
      "epoch:12 step:11985 [D loss: 0.641784, acc: 63.28%] [G loss: 1.861588]\n",
      "epoch:12 step:11986 [D loss: 0.635821, acc: 62.50%] [G loss: 1.981341]\n",
      "epoch:12 step:11987 [D loss: 0.715308, acc: 50.00%] [G loss: 1.813863]\n",
      "epoch:12 step:11988 [D loss: 0.658313, acc: 63.28%] [G loss: 1.890139]\n",
      "epoch:12 step:11989 [D loss: 0.615398, acc: 64.06%] [G loss: 1.984958]\n",
      "epoch:12 step:11990 [D loss: 0.592789, acc: 62.50%] [G loss: 2.077376]\n",
      "epoch:12 step:11991 [D loss: 0.675920, acc: 60.16%] [G loss: 1.987689]\n",
      "epoch:12 step:11992 [D loss: 0.631948, acc: 67.19%] [G loss: 1.845840]\n",
      "epoch:12 step:11993 [D loss: 0.663080, acc: 55.47%] [G loss: 1.862717]\n",
      "epoch:12 step:11994 [D loss: 0.653242, acc: 65.62%] [G loss: 1.838473]\n",
      "epoch:12 step:11995 [D loss: 0.658945, acc: 59.38%] [G loss: 1.824241]\n",
      "epoch:12 step:11996 [D loss: 0.650158, acc: 64.84%] [G loss: 1.884327]\n",
      "epoch:12 step:11997 [D loss: 0.634188, acc: 70.31%] [G loss: 2.019725]\n",
      "epoch:12 step:11998 [D loss: 0.626667, acc: 67.97%] [G loss: 2.022592]\n",
      "epoch:12 step:11999 [D loss: 0.648172, acc: 65.62%] [G loss: 2.036250]\n",
      "epoch:12 step:12000 [D loss: 0.642178, acc: 60.94%] [G loss: 1.999912]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.910999\n",
      "FID: 21.058521\n",
      "0 = 12.608267283821075\n",
      "1 = 0.07376522230391101\n",
      "2 = 0.8907999992370605\n",
      "3 = 0.9056000113487244\n",
      "4 = 0.8759999871253967\n",
      "5 = 0.8795648813247681\n",
      "6 = 0.9056000113487244\n",
      "7 = 7.092273158442969\n",
      "8 = 0.08889870556295543\n",
      "9 = 0.7448999881744385\n",
      "10 = 0.7577999830245972\n",
      "11 = 0.7319999933242798\n",
      "12 = 0.7387405037879944\n",
      "13 = 0.7577999830245972\n",
      "14 = 6.911030292510986\n",
      "15 = 9.421468734741211\n",
      "16 = 0.1314941942691803\n",
      "17 = 6.910999298095703\n",
      "18 = 21.058521270751953\n",
      "epoch:12 step:12001 [D loss: 0.627629, acc: 62.50%] [G loss: 2.084900]\n",
      "epoch:12 step:12002 [D loss: 0.664768, acc: 58.59%] [G loss: 1.905685]\n",
      "epoch:12 step:12003 [D loss: 0.645201, acc: 61.72%] [G loss: 1.816724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:12004 [D loss: 0.703614, acc: 58.59%] [G loss: 1.896980]\n",
      "epoch:12 step:12005 [D loss: 0.668865, acc: 64.84%] [G loss: 1.840942]\n",
      "epoch:12 step:12006 [D loss: 0.674991, acc: 55.47%] [G loss: 1.820292]\n",
      "epoch:12 step:12007 [D loss: 0.640364, acc: 63.28%] [G loss: 1.854083]\n",
      "epoch:12 step:12008 [D loss: 0.644828, acc: 61.72%] [G loss: 1.863353]\n",
      "epoch:12 step:12009 [D loss: 0.687895, acc: 61.72%] [G loss: 1.790971]\n",
      "epoch:12 step:12010 [D loss: 0.681771, acc: 60.16%] [G loss: 1.755020]\n",
      "epoch:12 step:12011 [D loss: 0.639103, acc: 62.50%] [G loss: 2.038436]\n",
      "epoch:12 step:12012 [D loss: 0.618929, acc: 67.97%] [G loss: 1.880372]\n",
      "epoch:12 step:12013 [D loss: 0.625304, acc: 66.41%] [G loss: 2.104208]\n",
      "epoch:12 step:12014 [D loss: 0.682431, acc: 55.47%] [G loss: 1.832917]\n",
      "epoch:12 step:12015 [D loss: 0.649292, acc: 60.16%] [G loss: 1.887110]\n",
      "epoch:12 step:12016 [D loss: 0.705772, acc: 53.12%] [G loss: 1.839762]\n",
      "epoch:12 step:12017 [D loss: 0.705925, acc: 56.25%] [G loss: 1.853240]\n",
      "epoch:12 step:12018 [D loss: 0.599255, acc: 69.53%] [G loss: 2.086502]\n",
      "epoch:12 step:12019 [D loss: 0.566181, acc: 71.88%] [G loss: 2.374044]\n",
      "epoch:12 step:12020 [D loss: 0.627284, acc: 65.62%] [G loss: 2.035278]\n",
      "epoch:12 step:12021 [D loss: 0.689725, acc: 56.25%] [G loss: 1.989314]\n",
      "epoch:12 step:12022 [D loss: 0.652843, acc: 60.94%] [G loss: 1.915519]\n",
      "epoch:12 step:12023 [D loss: 0.648009, acc: 60.16%] [G loss: 1.898392]\n",
      "epoch:12 step:12024 [D loss: 0.647690, acc: 57.81%] [G loss: 2.140363]\n",
      "epoch:12 step:12025 [D loss: 0.593971, acc: 67.97%] [G loss: 2.179426]\n",
      "epoch:12 step:12026 [D loss: 0.620830, acc: 71.09%] [G loss: 2.106475]\n",
      "epoch:12 step:12027 [D loss: 0.655162, acc: 64.06%] [G loss: 1.932966]\n",
      "epoch:12 step:12028 [D loss: 0.669324, acc: 57.81%] [G loss: 1.869408]\n",
      "epoch:12 step:12029 [D loss: 0.689037, acc: 57.81%] [G loss: 1.845941]\n",
      "epoch:12 step:12030 [D loss: 0.609880, acc: 64.84%] [G loss: 2.194111]\n",
      "epoch:12 step:12031 [D loss: 0.662554, acc: 64.84%] [G loss: 1.849794]\n",
      "epoch:12 step:12032 [D loss: 0.620581, acc: 64.06%] [G loss: 1.931710]\n",
      "epoch:12 step:12033 [D loss: 0.666558, acc: 59.38%] [G loss: 2.070722]\n",
      "epoch:12 step:12034 [D loss: 0.590229, acc: 64.84%] [G loss: 2.020460]\n",
      "epoch:12 step:12035 [D loss: 0.624391, acc: 61.72%] [G loss: 1.905226]\n",
      "epoch:12 step:12036 [D loss: 0.540493, acc: 78.12%] [G loss: 2.217389]\n",
      "epoch:12 step:12037 [D loss: 0.616677, acc: 64.84%] [G loss: 2.065649]\n",
      "epoch:12 step:12038 [D loss: 0.720308, acc: 51.56%] [G loss: 1.747137]\n",
      "epoch:12 step:12039 [D loss: 0.673980, acc: 57.81%] [G loss: 1.912949]\n",
      "epoch:12 step:12040 [D loss: 0.674425, acc: 59.38%] [G loss: 1.948014]\n",
      "epoch:12 step:12041 [D loss: 0.654987, acc: 60.16%] [G loss: 1.920658]\n",
      "epoch:12 step:12042 [D loss: 0.628325, acc: 64.84%] [G loss: 2.017351]\n",
      "epoch:12 step:12043 [D loss: 0.596949, acc: 66.41%] [G loss: 1.966196]\n",
      "epoch:12 step:12044 [D loss: 0.751623, acc: 53.91%] [G loss: 1.777092]\n",
      "epoch:12 step:12045 [D loss: 0.677632, acc: 55.47%] [G loss: 1.776064]\n",
      "epoch:12 step:12046 [D loss: 0.684292, acc: 55.47%] [G loss: 2.023683]\n",
      "epoch:12 step:12047 [D loss: 0.663768, acc: 64.06%] [G loss: 1.846330]\n",
      "epoch:12 step:12048 [D loss: 0.652724, acc: 58.59%] [G loss: 2.114218]\n",
      "epoch:12 step:12049 [D loss: 0.640386, acc: 63.28%] [G loss: 2.067763]\n",
      "epoch:12 step:12050 [D loss: 0.628232, acc: 64.06%] [G loss: 2.013750]\n",
      "epoch:12 step:12051 [D loss: 0.586569, acc: 71.09%] [G loss: 2.078287]\n",
      "epoch:12 step:12052 [D loss: 0.656179, acc: 59.38%] [G loss: 1.952112]\n",
      "epoch:12 step:12053 [D loss: 0.662823, acc: 63.28%] [G loss: 1.990360]\n",
      "epoch:12 step:12054 [D loss: 0.637317, acc: 66.41%] [G loss: 2.018206]\n",
      "epoch:12 step:12055 [D loss: 0.643495, acc: 60.16%] [G loss: 1.914957]\n",
      "epoch:12 step:12056 [D loss: 0.674125, acc: 62.50%] [G loss: 1.771047]\n",
      "epoch:12 step:12057 [D loss: 0.612889, acc: 68.75%] [G loss: 1.991156]\n",
      "epoch:12 step:12058 [D loss: 0.669778, acc: 58.59%] [G loss: 1.844691]\n",
      "epoch:12 step:12059 [D loss: 0.563091, acc: 75.78%] [G loss: 2.379394]\n",
      "epoch:12 step:12060 [D loss: 0.631118, acc: 67.97%] [G loss: 2.012315]\n",
      "epoch:12 step:12061 [D loss: 0.602225, acc: 69.53%] [G loss: 1.886018]\n",
      "epoch:12 step:12062 [D loss: 0.661328, acc: 62.50%] [G loss: 1.772564]\n",
      "epoch:12 step:12063 [D loss: 0.646729, acc: 63.28%] [G loss: 1.829247]\n",
      "epoch:12 step:12064 [D loss: 0.666294, acc: 59.38%] [G loss: 1.893343]\n",
      "epoch:12 step:12065 [D loss: 0.673485, acc: 60.94%] [G loss: 1.859367]\n",
      "epoch:12 step:12066 [D loss: 0.606635, acc: 66.41%] [G loss: 2.141253]\n",
      "epoch:12 step:12067 [D loss: 0.606885, acc: 65.62%] [G loss: 2.043975]\n",
      "epoch:12 step:12068 [D loss: 0.685466, acc: 55.47%] [G loss: 1.822142]\n",
      "epoch:12 step:12069 [D loss: 0.572391, acc: 74.22%] [G loss: 2.174019]\n",
      "epoch:12 step:12070 [D loss: 0.654963, acc: 60.16%] [G loss: 1.949732]\n",
      "epoch:12 step:12071 [D loss: 0.693046, acc: 55.47%] [G loss: 1.967145]\n",
      "epoch:12 step:12072 [D loss: 0.718396, acc: 55.47%] [G loss: 1.931187]\n",
      "epoch:12 step:12073 [D loss: 0.779971, acc: 50.00%] [G loss: 1.829660]\n",
      "epoch:12 step:12074 [D loss: 0.653217, acc: 60.94%] [G loss: 1.784383]\n",
      "epoch:12 step:12075 [D loss: 0.635585, acc: 66.41%] [G loss: 1.996224]\n",
      "epoch:12 step:12076 [D loss: 0.707873, acc: 51.56%] [G loss: 1.836634]\n",
      "epoch:12 step:12077 [D loss: 0.576637, acc: 70.31%] [G loss: 1.941838]\n",
      "epoch:12 step:12078 [D loss: 0.601437, acc: 69.53%] [G loss: 1.954060]\n",
      "epoch:12 step:12079 [D loss: 0.621046, acc: 61.72%] [G loss: 1.761819]\n",
      "epoch:12 step:12080 [D loss: 0.614404, acc: 63.28%] [G loss: 1.951319]\n",
      "epoch:12 step:12081 [D loss: 0.594485, acc: 70.31%] [G loss: 1.960263]\n",
      "epoch:12 step:12082 [D loss: 0.634726, acc: 60.94%] [G loss: 2.035846]\n",
      "epoch:12 step:12083 [D loss: 0.639543, acc: 61.72%] [G loss: 1.939898]\n",
      "epoch:12 step:12084 [D loss: 0.604905, acc: 69.53%] [G loss: 1.959069]\n",
      "epoch:12 step:12085 [D loss: 0.644708, acc: 60.16%] [G loss: 1.935569]\n",
      "epoch:12 step:12086 [D loss: 0.570560, acc: 73.44%] [G loss: 2.039447]\n",
      "epoch:12 step:12087 [D loss: 0.611193, acc: 69.53%] [G loss: 1.967656]\n",
      "epoch:12 step:12088 [D loss: 0.645411, acc: 61.72%] [G loss: 2.176991]\n",
      "epoch:12 step:12089 [D loss: 0.644636, acc: 61.72%] [G loss: 1.993190]\n",
      "epoch:12 step:12090 [D loss: 0.636955, acc: 60.94%] [G loss: 1.826527]\n",
      "epoch:12 step:12091 [D loss: 0.695537, acc: 60.94%] [G loss: 1.900578]\n",
      "epoch:12 step:12092 [D loss: 0.639261, acc: 64.84%] [G loss: 2.001319]\n",
      "epoch:12 step:12093 [D loss: 0.642980, acc: 66.41%] [G loss: 2.138211]\n",
      "epoch:12 step:12094 [D loss: 0.700543, acc: 51.56%] [G loss: 1.993312]\n",
      "epoch:12 step:12095 [D loss: 0.629621, acc: 63.28%] [G loss: 1.946029]\n",
      "epoch:12 step:12096 [D loss: 0.626847, acc: 67.19%] [G loss: 2.082477]\n",
      "epoch:12 step:12097 [D loss: 0.639119, acc: 61.72%] [G loss: 1.998507]\n",
      "epoch:12 step:12098 [D loss: 0.703754, acc: 57.81%] [G loss: 1.860099]\n",
      "epoch:12 step:12099 [D loss: 0.685828, acc: 56.25%] [G loss: 1.815429]\n",
      "epoch:12 step:12100 [D loss: 0.652405, acc: 64.84%] [G loss: 1.842446]\n",
      "epoch:12 step:12101 [D loss: 0.667795, acc: 60.94%] [G loss: 1.901895]\n",
      "epoch:12 step:12102 [D loss: 0.683662, acc: 54.69%] [G loss: 1.724980]\n",
      "epoch:12 step:12103 [D loss: 0.687452, acc: 57.03%] [G loss: 1.772955]\n",
      "epoch:12 step:12104 [D loss: 0.598498, acc: 65.62%] [G loss: 1.938583]\n",
      "epoch:12 step:12105 [D loss: 0.618602, acc: 70.31%] [G loss: 1.955173]\n",
      "epoch:12 step:12106 [D loss: 0.637477, acc: 61.72%] [G loss: 1.743511]\n",
      "epoch:12 step:12107 [D loss: 0.613393, acc: 66.41%] [G loss: 1.865817]\n",
      "epoch:12 step:12108 [D loss: 0.667461, acc: 57.03%] [G loss: 1.806412]\n",
      "epoch:12 step:12109 [D loss: 0.641653, acc: 63.28%] [G loss: 1.945699]\n",
      "epoch:12 step:12110 [D loss: 0.625135, acc: 64.84%] [G loss: 1.958579]\n",
      "epoch:12 step:12111 [D loss: 0.677299, acc: 60.16%] [G loss: 1.918046]\n",
      "epoch:12 step:12112 [D loss: 0.664085, acc: 62.50%] [G loss: 1.847079]\n",
      "epoch:12 step:12113 [D loss: 0.676185, acc: 57.81%] [G loss: 1.935304]\n",
      "epoch:12 step:12114 [D loss: 0.647331, acc: 58.59%] [G loss: 1.886164]\n",
      "epoch:12 step:12115 [D loss: 0.591409, acc: 71.09%] [G loss: 1.868444]\n",
      "epoch:12 step:12116 [D loss: 0.656611, acc: 63.28%] [G loss: 1.939628]\n",
      "epoch:12 step:12117 [D loss: 0.667152, acc: 58.59%] [G loss: 1.861564]\n",
      "epoch:12 step:12118 [D loss: 0.680046, acc: 64.06%] [G loss: 1.937640]\n",
      "epoch:12 step:12119 [D loss: 0.561357, acc: 76.56%] [G loss: 2.103304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:12120 [D loss: 0.634589, acc: 65.62%] [G loss: 1.985037]\n",
      "epoch:12 step:12121 [D loss: 0.704614, acc: 58.59%] [G loss: 1.981300]\n",
      "epoch:12 step:12122 [D loss: 0.649306, acc: 58.59%] [G loss: 1.786638]\n",
      "epoch:12 step:12123 [D loss: 0.674750, acc: 58.59%] [G loss: 1.863014]\n",
      "epoch:12 step:12124 [D loss: 0.655367, acc: 57.03%] [G loss: 1.957673]\n",
      "epoch:12 step:12125 [D loss: 0.620996, acc: 60.16%] [G loss: 1.970991]\n",
      "epoch:12 step:12126 [D loss: 0.675204, acc: 58.59%] [G loss: 1.901875]\n",
      "epoch:12 step:12127 [D loss: 0.621549, acc: 66.41%] [G loss: 1.908611]\n",
      "epoch:12 step:12128 [D loss: 0.605531, acc: 68.75%] [G loss: 2.151920]\n",
      "epoch:12 step:12129 [D loss: 0.599598, acc: 70.31%] [G loss: 2.011616]\n",
      "epoch:12 step:12130 [D loss: 0.626515, acc: 65.62%] [G loss: 2.098238]\n",
      "epoch:12 step:12131 [D loss: 0.626368, acc: 67.97%] [G loss: 1.864100]\n",
      "epoch:12 step:12132 [D loss: 0.694704, acc: 57.81%] [G loss: 1.843686]\n",
      "epoch:12 step:12133 [D loss: 0.637186, acc: 63.28%] [G loss: 2.086973]\n",
      "epoch:12 step:12134 [D loss: 0.615457, acc: 71.09%] [G loss: 1.997017]\n",
      "epoch:12 step:12135 [D loss: 0.646204, acc: 61.72%] [G loss: 1.815854]\n",
      "epoch:12 step:12136 [D loss: 0.653190, acc: 63.28%] [G loss: 1.731878]\n",
      "epoch:12 step:12137 [D loss: 0.617189, acc: 65.62%] [G loss: 2.019522]\n",
      "epoch:12 step:12138 [D loss: 0.622172, acc: 66.41%] [G loss: 2.019722]\n",
      "epoch:12 step:12139 [D loss: 0.669499, acc: 62.50%] [G loss: 1.998874]\n",
      "epoch:12 step:12140 [D loss: 0.689315, acc: 59.38%] [G loss: 1.836098]\n",
      "epoch:12 step:12141 [D loss: 0.633404, acc: 61.72%] [G loss: 2.024562]\n",
      "epoch:12 step:12142 [D loss: 0.647935, acc: 58.59%] [G loss: 1.977645]\n",
      "epoch:12 step:12143 [D loss: 0.643416, acc: 58.59%] [G loss: 2.051575]\n",
      "epoch:12 step:12144 [D loss: 0.661544, acc: 55.47%] [G loss: 2.010320]\n",
      "epoch:12 step:12145 [D loss: 0.603140, acc: 68.75%] [G loss: 2.100724]\n",
      "epoch:12 step:12146 [D loss: 0.691783, acc: 57.03%] [G loss: 1.962990]\n",
      "epoch:12 step:12147 [D loss: 0.667104, acc: 54.69%] [G loss: 1.943540]\n",
      "epoch:12 step:12148 [D loss: 0.627642, acc: 63.28%] [G loss: 1.859744]\n",
      "epoch:12 step:12149 [D loss: 0.611747, acc: 71.09%] [G loss: 1.849843]\n",
      "epoch:12 step:12150 [D loss: 0.630068, acc: 60.16%] [G loss: 2.079515]\n",
      "epoch:12 step:12151 [D loss: 0.607019, acc: 69.53%] [G loss: 2.041444]\n",
      "epoch:12 step:12152 [D loss: 0.663464, acc: 62.50%] [G loss: 1.991595]\n",
      "epoch:12 step:12153 [D loss: 0.604508, acc: 67.97%] [G loss: 2.020740]\n",
      "epoch:12 step:12154 [D loss: 0.566560, acc: 70.31%] [G loss: 2.120458]\n",
      "epoch:12 step:12155 [D loss: 0.642841, acc: 63.28%] [G loss: 2.046431]\n",
      "epoch:12 step:12156 [D loss: 0.605448, acc: 68.75%] [G loss: 2.250191]\n",
      "epoch:12 step:12157 [D loss: 0.661572, acc: 59.38%] [G loss: 2.050868]\n",
      "epoch:12 step:12158 [D loss: 0.593057, acc: 64.84%] [G loss: 2.014357]\n",
      "epoch:12 step:12159 [D loss: 0.638389, acc: 66.41%] [G loss: 1.924931]\n",
      "epoch:12 step:12160 [D loss: 0.645457, acc: 64.84%] [G loss: 2.084250]\n",
      "epoch:12 step:12161 [D loss: 0.660617, acc: 64.84%] [G loss: 2.128838]\n",
      "epoch:12 step:12162 [D loss: 0.563277, acc: 72.66%] [G loss: 2.119633]\n",
      "epoch:12 step:12163 [D loss: 0.579384, acc: 70.31%] [G loss: 2.363156]\n",
      "epoch:12 step:12164 [D loss: 0.635332, acc: 62.50%] [G loss: 1.875269]\n",
      "epoch:12 step:12165 [D loss: 0.609384, acc: 65.62%] [G loss: 2.029453]\n",
      "epoch:12 step:12166 [D loss: 0.676297, acc: 55.47%] [G loss: 2.145607]\n",
      "epoch:12 step:12167 [D loss: 0.565641, acc: 72.66%] [G loss: 2.191182]\n",
      "epoch:12 step:12168 [D loss: 0.565215, acc: 75.00%] [G loss: 2.284651]\n",
      "epoch:12 step:12169 [D loss: 0.578206, acc: 74.22%] [G loss: 2.094994]\n",
      "epoch:12 step:12170 [D loss: 0.579572, acc: 71.88%] [G loss: 2.318685]\n",
      "epoch:12 step:12171 [D loss: 0.651680, acc: 63.28%] [G loss: 2.242432]\n",
      "epoch:12 step:12172 [D loss: 0.804815, acc: 55.47%] [G loss: 1.960257]\n",
      "epoch:12 step:12173 [D loss: 0.697494, acc: 53.91%] [G loss: 2.049326]\n",
      "epoch:12 step:12174 [D loss: 0.576584, acc: 67.19%] [G loss: 2.100830]\n",
      "epoch:12 step:12175 [D loss: 0.611467, acc: 67.19%] [G loss: 2.034109]\n",
      "epoch:12 step:12176 [D loss: 0.627583, acc: 66.41%] [G loss: 1.944039]\n",
      "epoch:12 step:12177 [D loss: 0.680757, acc: 54.69%] [G loss: 1.950494]\n",
      "epoch:12 step:12178 [D loss: 0.591547, acc: 67.97%] [G loss: 1.941483]\n",
      "epoch:12 step:12179 [D loss: 0.590756, acc: 71.09%] [G loss: 2.100890]\n",
      "epoch:12 step:12180 [D loss: 0.629444, acc: 61.72%] [G loss: 2.272568]\n",
      "epoch:12 step:12181 [D loss: 0.567196, acc: 76.56%] [G loss: 2.383135]\n",
      "epoch:13 step:12182 [D loss: 0.685151, acc: 60.94%] [G loss: 1.904059]\n",
      "epoch:13 step:12183 [D loss: 0.600744, acc: 66.41%] [G loss: 2.030913]\n",
      "epoch:13 step:12184 [D loss: 0.650467, acc: 62.50%] [G loss: 2.052355]\n",
      "epoch:13 step:12185 [D loss: 0.613548, acc: 67.97%] [G loss: 2.078331]\n",
      "epoch:13 step:12186 [D loss: 0.649243, acc: 60.94%] [G loss: 1.953043]\n",
      "epoch:13 step:12187 [D loss: 0.706131, acc: 55.47%] [G loss: 2.059173]\n",
      "epoch:13 step:12188 [D loss: 0.639653, acc: 61.72%] [G loss: 1.963351]\n",
      "epoch:13 step:12189 [D loss: 0.619543, acc: 68.75%] [G loss: 2.156845]\n",
      "epoch:13 step:12190 [D loss: 0.601095, acc: 73.44%] [G loss: 2.244182]\n",
      "epoch:13 step:12191 [D loss: 0.621229, acc: 65.62%] [G loss: 2.133991]\n",
      "epoch:13 step:12192 [D loss: 0.642322, acc: 61.72%] [G loss: 2.103849]\n",
      "epoch:13 step:12193 [D loss: 0.700136, acc: 58.59%] [G loss: 1.959584]\n",
      "epoch:13 step:12194 [D loss: 0.600573, acc: 68.75%] [G loss: 1.996959]\n",
      "epoch:13 step:12195 [D loss: 0.622973, acc: 66.41%] [G loss: 1.917949]\n",
      "epoch:13 step:12196 [D loss: 0.579141, acc: 67.97%] [G loss: 2.368502]\n",
      "epoch:13 step:12197 [D loss: 0.600551, acc: 70.31%] [G loss: 2.068568]\n",
      "epoch:13 step:12198 [D loss: 0.666943, acc: 64.06%] [G loss: 1.977585]\n",
      "epoch:13 step:12199 [D loss: 0.647223, acc: 65.62%] [G loss: 2.004416]\n",
      "epoch:13 step:12200 [D loss: 0.614127, acc: 61.72%] [G loss: 2.027679]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.995336\n",
      "FID: 18.826973\n",
      "0 = 12.94624943723679\n",
      "1 = 0.09429771644197898\n",
      "2 = 0.8930000066757202\n",
      "3 = 0.9125999808311462\n",
      "4 = 0.8733999729156494\n",
      "5 = 0.8781754970550537\n",
      "6 = 0.9125999808311462\n",
      "7 = 6.96955676771403\n",
      "8 = 0.0851823399325057\n",
      "9 = 0.7401999831199646\n",
      "10 = 0.7527999877929688\n",
      "11 = 0.7275999784469604\n",
      "12 = 0.7342957258224487\n",
      "13 = 0.7527999877929688\n",
      "14 = 6.995362281799316\n",
      "15 = 9.349547386169434\n",
      "16 = 0.14071084558963776\n",
      "17 = 6.995335578918457\n",
      "18 = 18.82697296142578\n",
      "epoch:13 step:12201 [D loss: 0.812553, acc: 41.41%] [G loss: 1.681215]\n",
      "epoch:13 step:12202 [D loss: 0.624685, acc: 65.62%] [G loss: 2.079319]\n",
      "epoch:13 step:12203 [D loss: 0.631008, acc: 62.50%] [G loss: 1.889662]\n",
      "epoch:13 step:12204 [D loss: 0.593267, acc: 64.06%] [G loss: 2.123427]\n",
      "epoch:13 step:12205 [D loss: 0.608210, acc: 69.53%] [G loss: 2.047215]\n",
      "epoch:13 step:12206 [D loss: 0.592230, acc: 65.62%] [G loss: 2.120021]\n",
      "epoch:13 step:12207 [D loss: 0.646319, acc: 59.38%] [G loss: 1.879288]\n",
      "epoch:13 step:12208 [D loss: 0.654492, acc: 60.16%] [G loss: 1.904629]\n",
      "epoch:13 step:12209 [D loss: 0.584020, acc: 67.19%] [G loss: 1.949323]\n",
      "epoch:13 step:12210 [D loss: 0.612559, acc: 71.88%] [G loss: 2.041470]\n",
      "epoch:13 step:12211 [D loss: 0.640792, acc: 60.16%] [G loss: 1.917611]\n",
      "epoch:13 step:12212 [D loss: 0.619246, acc: 64.06%] [G loss: 1.817425]\n",
      "epoch:13 step:12213 [D loss: 0.678165, acc: 56.25%] [G loss: 1.772575]\n",
      "epoch:13 step:12214 [D loss: 0.675688, acc: 57.81%] [G loss: 1.840931]\n",
      "epoch:13 step:12215 [D loss: 0.666622, acc: 64.84%] [G loss: 1.886290]\n",
      "epoch:13 step:12216 [D loss: 0.697296, acc: 52.34%] [G loss: 1.819382]\n",
      "epoch:13 step:12217 [D loss: 0.639720, acc: 59.38%] [G loss: 2.083910]\n",
      "epoch:13 step:12218 [D loss: 0.623731, acc: 61.72%] [G loss: 2.082026]\n",
      "epoch:13 step:12219 [D loss: 0.694142, acc: 53.12%] [G loss: 1.866793]\n",
      "epoch:13 step:12220 [D loss: 0.641445, acc: 64.84%] [G loss: 1.998127]\n",
      "epoch:13 step:12221 [D loss: 0.615789, acc: 65.62%] [G loss: 2.291142]\n",
      "epoch:13 step:12222 [D loss: 0.724562, acc: 55.47%] [G loss: 1.669397]\n",
      "epoch:13 step:12223 [D loss: 0.621334, acc: 61.72%] [G loss: 1.892988]\n",
      "epoch:13 step:12224 [D loss: 0.631856, acc: 63.28%] [G loss: 1.818096]\n",
      "epoch:13 step:12225 [D loss: 0.635030, acc: 64.06%] [G loss: 1.890324]\n",
      "epoch:13 step:12226 [D loss: 0.681013, acc: 57.03%] [G loss: 1.824521]\n",
      "epoch:13 step:12227 [D loss: 0.591397, acc: 68.75%] [G loss: 1.939526]\n",
      "epoch:13 step:12228 [D loss: 0.621230, acc: 63.28%] [G loss: 2.089393]\n",
      "epoch:13 step:12229 [D loss: 0.596733, acc: 67.19%] [G loss: 2.040007]\n",
      "epoch:13 step:12230 [D loss: 0.663987, acc: 57.81%] [G loss: 2.051342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12231 [D loss: 0.540926, acc: 73.44%] [G loss: 2.055969]\n",
      "epoch:13 step:12232 [D loss: 0.643847, acc: 61.72%] [G loss: 2.006975]\n",
      "epoch:13 step:12233 [D loss: 0.633487, acc: 67.19%] [G loss: 2.110160]\n",
      "epoch:13 step:12234 [D loss: 0.632170, acc: 64.06%] [G loss: 1.996312]\n",
      "epoch:13 step:12235 [D loss: 0.604625, acc: 66.41%] [G loss: 2.068666]\n",
      "epoch:13 step:12236 [D loss: 0.598660, acc: 70.31%] [G loss: 2.049269]\n",
      "epoch:13 step:12237 [D loss: 0.633723, acc: 63.28%] [G loss: 2.038662]\n",
      "epoch:13 step:12238 [D loss: 0.626596, acc: 65.62%] [G loss: 2.084127]\n",
      "epoch:13 step:12239 [D loss: 0.598128, acc: 66.41%] [G loss: 2.113804]\n",
      "epoch:13 step:12240 [D loss: 0.669779, acc: 61.72%] [G loss: 1.953092]\n",
      "epoch:13 step:12241 [D loss: 0.650021, acc: 61.72%] [G loss: 2.037621]\n",
      "epoch:13 step:12242 [D loss: 0.661183, acc: 58.59%] [G loss: 1.817307]\n",
      "epoch:13 step:12243 [D loss: 0.632882, acc: 63.28%] [G loss: 1.866900]\n",
      "epoch:13 step:12244 [D loss: 0.630203, acc: 64.06%] [G loss: 2.029088]\n",
      "epoch:13 step:12245 [D loss: 0.649332, acc: 60.94%] [G loss: 1.882751]\n",
      "epoch:13 step:12246 [D loss: 0.619910, acc: 71.09%] [G loss: 2.031313]\n",
      "epoch:13 step:12247 [D loss: 0.674292, acc: 56.25%] [G loss: 1.940897]\n",
      "epoch:13 step:12248 [D loss: 0.620214, acc: 69.53%] [G loss: 1.958035]\n",
      "epoch:13 step:12249 [D loss: 0.622445, acc: 71.09%] [G loss: 1.942185]\n",
      "epoch:13 step:12250 [D loss: 0.595139, acc: 66.41%] [G loss: 2.130354]\n",
      "epoch:13 step:12251 [D loss: 0.630992, acc: 61.72%] [G loss: 1.921410]\n",
      "epoch:13 step:12252 [D loss: 0.659133, acc: 63.28%] [G loss: 1.875496]\n",
      "epoch:13 step:12253 [D loss: 0.678043, acc: 62.50%] [G loss: 2.007947]\n",
      "epoch:13 step:12254 [D loss: 0.623388, acc: 64.84%] [G loss: 1.922788]\n",
      "epoch:13 step:12255 [D loss: 0.628499, acc: 67.19%] [G loss: 1.990861]\n",
      "epoch:13 step:12256 [D loss: 0.612795, acc: 66.41%] [G loss: 2.199725]\n",
      "epoch:13 step:12257 [D loss: 0.622395, acc: 61.72%] [G loss: 2.186418]\n",
      "epoch:13 step:12258 [D loss: 0.587239, acc: 73.44%] [G loss: 2.387430]\n",
      "epoch:13 step:12259 [D loss: 0.714855, acc: 52.34%] [G loss: 1.870341]\n",
      "epoch:13 step:12260 [D loss: 0.682783, acc: 58.59%] [G loss: 1.963189]\n",
      "epoch:13 step:12261 [D loss: 0.707722, acc: 58.59%] [G loss: 1.874355]\n",
      "epoch:13 step:12262 [D loss: 0.721126, acc: 54.69%] [G loss: 1.793649]\n",
      "epoch:13 step:12263 [D loss: 0.632328, acc: 65.62%] [G loss: 1.854672]\n",
      "epoch:13 step:12264 [D loss: 0.678219, acc: 52.34%] [G loss: 2.076478]\n",
      "epoch:13 step:12265 [D loss: 0.626271, acc: 66.41%] [G loss: 1.821769]\n",
      "epoch:13 step:12266 [D loss: 0.670663, acc: 58.59%] [G loss: 1.758914]\n",
      "epoch:13 step:12267 [D loss: 0.663082, acc: 54.69%] [G loss: 1.846912]\n",
      "epoch:13 step:12268 [D loss: 0.642874, acc: 57.81%] [G loss: 1.923520]\n",
      "epoch:13 step:12269 [D loss: 0.694902, acc: 57.81%] [G loss: 1.980823]\n",
      "epoch:13 step:12270 [D loss: 0.622835, acc: 63.28%] [G loss: 1.874255]\n",
      "epoch:13 step:12271 [D loss: 0.637348, acc: 60.16%] [G loss: 1.799362]\n",
      "epoch:13 step:12272 [D loss: 0.672407, acc: 57.81%] [G loss: 1.777143]\n",
      "epoch:13 step:12273 [D loss: 0.657786, acc: 61.72%] [G loss: 1.961385]\n",
      "epoch:13 step:12274 [D loss: 0.618106, acc: 65.62%] [G loss: 1.971280]\n",
      "epoch:13 step:12275 [D loss: 0.558066, acc: 72.66%] [G loss: 1.826905]\n",
      "epoch:13 step:12276 [D loss: 0.655852, acc: 62.50%] [G loss: 1.967714]\n",
      "epoch:13 step:12277 [D loss: 0.658366, acc: 59.38%] [G loss: 1.930056]\n",
      "epoch:13 step:12278 [D loss: 0.610551, acc: 67.97%] [G loss: 2.038362]\n",
      "epoch:13 step:12279 [D loss: 0.666909, acc: 59.38%] [G loss: 1.857963]\n",
      "epoch:13 step:12280 [D loss: 0.649841, acc: 58.59%] [G loss: 1.855739]\n",
      "epoch:13 step:12281 [D loss: 0.606973, acc: 66.41%] [G loss: 1.900405]\n",
      "epoch:13 step:12282 [D loss: 0.634419, acc: 64.06%] [G loss: 2.008997]\n",
      "epoch:13 step:12283 [D loss: 0.644225, acc: 64.06%] [G loss: 2.064779]\n",
      "epoch:13 step:12284 [D loss: 0.610265, acc: 66.41%] [G loss: 2.030874]\n",
      "epoch:13 step:12285 [D loss: 0.664592, acc: 59.38%] [G loss: 1.865423]\n",
      "epoch:13 step:12286 [D loss: 0.608348, acc: 62.50%] [G loss: 1.852656]\n",
      "epoch:13 step:12287 [D loss: 0.607550, acc: 68.75%] [G loss: 2.030521]\n",
      "epoch:13 step:12288 [D loss: 0.652983, acc: 60.16%] [G loss: 2.095407]\n",
      "epoch:13 step:12289 [D loss: 0.632371, acc: 61.72%] [G loss: 2.025259]\n",
      "epoch:13 step:12290 [D loss: 0.616054, acc: 67.19%] [G loss: 1.970562]\n",
      "epoch:13 step:12291 [D loss: 0.672697, acc: 63.28%] [G loss: 1.845802]\n",
      "epoch:13 step:12292 [D loss: 0.598130, acc: 67.19%] [G loss: 2.087904]\n",
      "epoch:13 step:12293 [D loss: 0.683978, acc: 57.81%] [G loss: 1.941889]\n",
      "epoch:13 step:12294 [D loss: 0.612992, acc: 68.75%] [G loss: 2.078807]\n",
      "epoch:13 step:12295 [D loss: 0.607728, acc: 71.09%] [G loss: 2.085193]\n",
      "epoch:13 step:12296 [D loss: 0.595638, acc: 70.31%] [G loss: 2.225321]\n",
      "epoch:13 step:12297 [D loss: 0.644124, acc: 56.25%] [G loss: 2.041290]\n",
      "epoch:13 step:12298 [D loss: 0.587740, acc: 68.75%] [G loss: 2.090827]\n",
      "epoch:13 step:12299 [D loss: 0.680936, acc: 63.28%] [G loss: 1.998604]\n",
      "epoch:13 step:12300 [D loss: 0.640689, acc: 61.72%] [G loss: 2.360859]\n",
      "epoch:13 step:12301 [D loss: 0.658463, acc: 60.16%] [G loss: 2.123324]\n",
      "epoch:13 step:12302 [D loss: 0.623421, acc: 64.84%] [G loss: 2.054632]\n",
      "epoch:13 step:12303 [D loss: 0.624409, acc: 64.84%] [G loss: 2.285970]\n",
      "epoch:13 step:12304 [D loss: 0.674116, acc: 54.69%] [G loss: 1.991330]\n",
      "epoch:13 step:12305 [D loss: 0.618411, acc: 64.84%] [G loss: 2.108974]\n",
      "epoch:13 step:12306 [D loss: 0.655934, acc: 62.50%] [G loss: 1.839140]\n",
      "epoch:13 step:12307 [D loss: 0.633937, acc: 65.62%] [G loss: 2.067372]\n",
      "epoch:13 step:12308 [D loss: 0.648257, acc: 62.50%] [G loss: 1.898866]\n",
      "epoch:13 step:12309 [D loss: 0.621008, acc: 64.84%] [G loss: 1.944888]\n",
      "epoch:13 step:12310 [D loss: 0.687476, acc: 53.91%] [G loss: 1.993057]\n",
      "epoch:13 step:12311 [D loss: 0.568299, acc: 73.44%] [G loss: 2.239886]\n",
      "epoch:13 step:12312 [D loss: 0.583704, acc: 71.88%] [G loss: 2.054437]\n",
      "epoch:13 step:12313 [D loss: 0.679472, acc: 61.72%] [G loss: 2.068143]\n",
      "epoch:13 step:12314 [D loss: 0.674028, acc: 58.59%] [G loss: 1.837745]\n",
      "epoch:13 step:12315 [D loss: 0.700356, acc: 54.69%] [G loss: 1.865915]\n",
      "epoch:13 step:12316 [D loss: 0.606429, acc: 67.97%] [G loss: 1.965778]\n",
      "epoch:13 step:12317 [D loss: 0.654845, acc: 58.59%] [G loss: 1.799375]\n",
      "epoch:13 step:12318 [D loss: 0.579736, acc: 67.97%] [G loss: 2.014484]\n",
      "epoch:13 step:12319 [D loss: 0.669515, acc: 62.50%] [G loss: 1.886910]\n",
      "epoch:13 step:12320 [D loss: 0.620247, acc: 66.41%] [G loss: 1.988934]\n",
      "epoch:13 step:12321 [D loss: 0.684070, acc: 60.94%] [G loss: 1.940980]\n",
      "epoch:13 step:12322 [D loss: 0.657808, acc: 60.16%] [G loss: 1.856029]\n",
      "epoch:13 step:12323 [D loss: 0.693618, acc: 53.91%] [G loss: 1.881375]\n",
      "epoch:13 step:12324 [D loss: 0.708836, acc: 56.25%] [G loss: 1.921420]\n",
      "epoch:13 step:12325 [D loss: 0.649974, acc: 60.16%] [G loss: 1.925857]\n",
      "epoch:13 step:12326 [D loss: 0.656773, acc: 62.50%] [G loss: 1.906717]\n",
      "epoch:13 step:12327 [D loss: 0.630090, acc: 64.84%] [G loss: 2.016162]\n",
      "epoch:13 step:12328 [D loss: 0.668135, acc: 59.38%] [G loss: 1.874235]\n",
      "epoch:13 step:12329 [D loss: 0.599754, acc: 70.31%] [G loss: 1.872960]\n",
      "epoch:13 step:12330 [D loss: 0.615077, acc: 67.19%] [G loss: 2.031336]\n",
      "epoch:13 step:12331 [D loss: 0.673541, acc: 57.03%] [G loss: 1.894064]\n",
      "epoch:13 step:12332 [D loss: 0.575371, acc: 75.00%] [G loss: 2.079345]\n",
      "epoch:13 step:12333 [D loss: 0.679440, acc: 54.69%] [G loss: 1.847486]\n",
      "epoch:13 step:12334 [D loss: 0.638326, acc: 64.84%] [G loss: 1.861312]\n",
      "epoch:13 step:12335 [D loss: 0.615087, acc: 65.62%] [G loss: 2.115939]\n",
      "epoch:13 step:12336 [D loss: 0.665733, acc: 55.47%] [G loss: 2.026998]\n",
      "epoch:13 step:12337 [D loss: 0.610307, acc: 65.62%] [G loss: 2.092448]\n",
      "epoch:13 step:12338 [D loss: 0.667255, acc: 57.03%] [G loss: 1.858075]\n",
      "epoch:13 step:12339 [D loss: 0.732988, acc: 49.22%] [G loss: 1.817906]\n",
      "epoch:13 step:12340 [D loss: 0.675207, acc: 63.28%] [G loss: 1.968338]\n",
      "epoch:13 step:12341 [D loss: 0.654290, acc: 64.84%] [G loss: 1.943520]\n",
      "epoch:13 step:12342 [D loss: 0.635865, acc: 61.72%] [G loss: 1.945206]\n",
      "epoch:13 step:12343 [D loss: 0.670951, acc: 59.38%] [G loss: 1.881239]\n",
      "epoch:13 step:12344 [D loss: 0.667150, acc: 55.47%] [G loss: 1.977640]\n",
      "epoch:13 step:12345 [D loss: 0.711040, acc: 53.91%] [G loss: 1.892229]\n",
      "epoch:13 step:12346 [D loss: 0.662155, acc: 60.16%] [G loss: 1.938255]\n",
      "epoch:13 step:12347 [D loss: 0.645288, acc: 67.97%] [G loss: 1.884609]\n",
      "epoch:13 step:12348 [D loss: 0.650462, acc: 60.94%] [G loss: 1.880964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12349 [D loss: 0.628696, acc: 65.62%] [G loss: 1.957364]\n",
      "epoch:13 step:12350 [D loss: 0.748144, acc: 57.81%] [G loss: 1.692286]\n",
      "epoch:13 step:12351 [D loss: 0.655244, acc: 60.94%] [G loss: 1.756415]\n",
      "epoch:13 step:12352 [D loss: 0.647545, acc: 62.50%] [G loss: 1.950611]\n",
      "epoch:13 step:12353 [D loss: 0.683953, acc: 57.03%] [G loss: 1.864583]\n",
      "epoch:13 step:12354 [D loss: 0.662309, acc: 60.16%] [G loss: 1.789017]\n",
      "epoch:13 step:12355 [D loss: 0.650644, acc: 61.72%] [G loss: 1.813235]\n",
      "epoch:13 step:12356 [D loss: 0.647736, acc: 60.94%] [G loss: 1.722011]\n",
      "epoch:13 step:12357 [D loss: 0.599354, acc: 67.97%] [G loss: 1.772096]\n",
      "epoch:13 step:12358 [D loss: 0.699497, acc: 53.91%] [G loss: 1.750146]\n",
      "epoch:13 step:12359 [D loss: 0.662935, acc: 58.59%] [G loss: 1.787831]\n",
      "epoch:13 step:12360 [D loss: 0.672241, acc: 60.16%] [G loss: 1.773651]\n",
      "epoch:13 step:12361 [D loss: 0.667233, acc: 56.25%] [G loss: 1.881172]\n",
      "epoch:13 step:12362 [D loss: 0.632739, acc: 63.28%] [G loss: 1.827648]\n",
      "epoch:13 step:12363 [D loss: 0.631372, acc: 71.09%] [G loss: 1.994768]\n",
      "epoch:13 step:12364 [D loss: 0.630020, acc: 66.41%] [G loss: 1.924956]\n",
      "epoch:13 step:12365 [D loss: 0.639202, acc: 62.50%] [G loss: 1.935126]\n",
      "epoch:13 step:12366 [D loss: 0.630223, acc: 67.19%] [G loss: 2.007198]\n",
      "epoch:13 step:12367 [D loss: 0.636545, acc: 66.41%] [G loss: 1.959229]\n",
      "epoch:13 step:12368 [D loss: 0.636167, acc: 67.97%] [G loss: 1.939980]\n",
      "epoch:13 step:12369 [D loss: 0.667369, acc: 62.50%] [G loss: 1.887164]\n",
      "epoch:13 step:12370 [D loss: 0.643918, acc: 63.28%] [G loss: 1.913844]\n",
      "epoch:13 step:12371 [D loss: 0.667848, acc: 56.25%] [G loss: 2.197394]\n",
      "epoch:13 step:12372 [D loss: 0.625347, acc: 67.19%] [G loss: 1.821974]\n",
      "epoch:13 step:12373 [D loss: 0.646480, acc: 63.28%] [G loss: 2.003674]\n",
      "epoch:13 step:12374 [D loss: 0.634958, acc: 60.94%] [G loss: 2.031840]\n",
      "epoch:13 step:12375 [D loss: 0.601040, acc: 67.19%] [G loss: 2.214528]\n",
      "epoch:13 step:12376 [D loss: 0.643416, acc: 67.19%] [G loss: 1.956194]\n",
      "epoch:13 step:12377 [D loss: 0.662399, acc: 62.50%] [G loss: 1.816327]\n",
      "epoch:13 step:12378 [D loss: 0.653453, acc: 59.38%] [G loss: 2.212410]\n",
      "epoch:13 step:12379 [D loss: 0.693905, acc: 60.94%] [G loss: 1.932517]\n",
      "epoch:13 step:12380 [D loss: 0.655656, acc: 60.94%] [G loss: 1.942267]\n",
      "epoch:13 step:12381 [D loss: 0.701187, acc: 53.12%] [G loss: 1.784019]\n",
      "epoch:13 step:12382 [D loss: 0.676083, acc: 58.59%] [G loss: 1.877694]\n",
      "epoch:13 step:12383 [D loss: 0.608292, acc: 64.84%] [G loss: 1.893694]\n",
      "epoch:13 step:12384 [D loss: 0.684681, acc: 57.03%] [G loss: 2.065844]\n",
      "epoch:13 step:12385 [D loss: 0.632370, acc: 65.62%] [G loss: 1.983023]\n",
      "epoch:13 step:12386 [D loss: 0.624919, acc: 66.41%] [G loss: 1.886125]\n",
      "epoch:13 step:12387 [D loss: 0.617444, acc: 62.50%] [G loss: 1.981408]\n",
      "epoch:13 step:12388 [D loss: 0.638441, acc: 62.50%] [G loss: 2.234278]\n",
      "epoch:13 step:12389 [D loss: 0.576044, acc: 70.31%] [G loss: 2.236364]\n",
      "epoch:13 step:12390 [D loss: 0.581134, acc: 70.31%] [G loss: 2.157392]\n",
      "epoch:13 step:12391 [D loss: 0.646825, acc: 63.28%] [G loss: 2.091711]\n",
      "epoch:13 step:12392 [D loss: 0.643879, acc: 66.41%] [G loss: 1.805350]\n",
      "epoch:13 step:12393 [D loss: 0.700195, acc: 60.94%] [G loss: 1.854089]\n",
      "epoch:13 step:12394 [D loss: 0.694577, acc: 57.81%] [G loss: 1.854141]\n",
      "epoch:13 step:12395 [D loss: 0.679694, acc: 55.47%] [G loss: 1.791252]\n",
      "epoch:13 step:12396 [D loss: 0.660224, acc: 58.59%] [G loss: 1.857777]\n",
      "epoch:13 step:12397 [D loss: 0.667327, acc: 63.28%] [G loss: 2.008778]\n",
      "epoch:13 step:12398 [D loss: 0.603190, acc: 71.09%] [G loss: 1.861878]\n",
      "epoch:13 step:12399 [D loss: 0.610466, acc: 64.84%] [G loss: 2.003208]\n",
      "epoch:13 step:12400 [D loss: 0.632873, acc: 64.84%] [G loss: 2.187976]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.044128\n",
      "FID: 18.669298\n",
      "0 = 12.874799672508246\n",
      "1 = 0.10047684466321861\n",
      "2 = 0.888700008392334\n",
      "3 = 0.902999997138977\n",
      "4 = 0.8744000196456909\n",
      "5 = 0.877892255783081\n",
      "6 = 0.902999997138977\n",
      "7 = 6.874120355391503\n",
      "8 = 0.0838209825016621\n",
      "9 = 0.7422999739646912\n",
      "10 = 0.753000020980835\n",
      "11 = 0.7315999865531921\n",
      "12 = 0.7372234463691711\n",
      "13 = 0.753000020980835\n",
      "14 = 7.044159889221191\n",
      "15 = 9.422825813293457\n",
      "16 = 0.12990032136440277\n",
      "17 = 7.04412841796875\n",
      "18 = 18.66929817199707\n",
      "epoch:13 step:12401 [D loss: 0.715792, acc: 50.00%] [G loss: 1.662726]\n",
      "epoch:13 step:12402 [D loss: 0.629125, acc: 67.19%] [G loss: 1.907507]\n",
      "epoch:13 step:12403 [D loss: 0.651844, acc: 62.50%] [G loss: 2.025918]\n",
      "epoch:13 step:12404 [D loss: 0.628571, acc: 64.06%] [G loss: 1.954818]\n",
      "epoch:13 step:12405 [D loss: 0.638472, acc: 63.28%] [G loss: 1.868321]\n",
      "epoch:13 step:12406 [D loss: 0.678320, acc: 59.38%] [G loss: 1.923450]\n",
      "epoch:13 step:12407 [D loss: 0.630541, acc: 64.06%] [G loss: 1.944373]\n",
      "epoch:13 step:12408 [D loss: 0.662666, acc: 57.03%] [G loss: 1.730396]\n",
      "epoch:13 step:12409 [D loss: 0.671031, acc: 60.94%] [G loss: 1.879842]\n",
      "epoch:13 step:12410 [D loss: 0.640860, acc: 64.06%] [G loss: 1.992127]\n",
      "epoch:13 step:12411 [D loss: 0.680523, acc: 57.03%] [G loss: 2.126766]\n",
      "epoch:13 step:12412 [D loss: 0.562753, acc: 68.75%] [G loss: 2.253188]\n",
      "epoch:13 step:12413 [D loss: 0.528806, acc: 74.22%] [G loss: 2.204252]\n",
      "epoch:13 step:12414 [D loss: 0.606578, acc: 64.84%] [G loss: 2.045934]\n",
      "epoch:13 step:12415 [D loss: 0.623607, acc: 64.06%] [G loss: 2.117303]\n",
      "epoch:13 step:12416 [D loss: 0.694966, acc: 55.47%] [G loss: 1.955745]\n",
      "epoch:13 step:12417 [D loss: 0.657256, acc: 63.28%] [G loss: 1.992662]\n",
      "epoch:13 step:12418 [D loss: 0.637975, acc: 68.75%] [G loss: 2.074489]\n",
      "epoch:13 step:12419 [D loss: 0.665980, acc: 56.25%] [G loss: 2.061422]\n",
      "epoch:13 step:12420 [D loss: 0.670139, acc: 58.59%] [G loss: 1.842289]\n",
      "epoch:13 step:12421 [D loss: 0.705913, acc: 53.91%] [G loss: 1.899889]\n",
      "epoch:13 step:12422 [D loss: 0.662597, acc: 59.38%] [G loss: 2.015071]\n",
      "epoch:13 step:12423 [D loss: 0.664986, acc: 62.50%] [G loss: 1.919266]\n",
      "epoch:13 step:12424 [D loss: 0.583490, acc: 75.00%] [G loss: 2.127717]\n",
      "epoch:13 step:12425 [D loss: 0.637166, acc: 67.19%] [G loss: 1.834630]\n",
      "epoch:13 step:12426 [D loss: 0.633627, acc: 58.59%] [G loss: 1.939568]\n",
      "epoch:13 step:12427 [D loss: 0.682486, acc: 58.59%] [G loss: 1.995640]\n",
      "epoch:13 step:12428 [D loss: 0.631935, acc: 62.50%] [G loss: 2.004800]\n",
      "epoch:13 step:12429 [D loss: 0.613492, acc: 67.97%] [G loss: 1.978167]\n",
      "epoch:13 step:12430 [D loss: 0.702177, acc: 56.25%] [G loss: 1.859585]\n",
      "epoch:13 step:12431 [D loss: 0.655955, acc: 60.16%] [G loss: 1.765443]\n",
      "epoch:13 step:12432 [D loss: 0.641666, acc: 60.94%] [G loss: 1.797705]\n",
      "epoch:13 step:12433 [D loss: 0.672582, acc: 57.81%] [G loss: 1.886194]\n",
      "epoch:13 step:12434 [D loss: 0.581414, acc: 70.31%] [G loss: 1.921031]\n",
      "epoch:13 step:12435 [D loss: 0.666591, acc: 60.16%] [G loss: 1.916369]\n",
      "epoch:13 step:12436 [D loss: 0.617651, acc: 70.31%] [G loss: 1.919257]\n",
      "epoch:13 step:12437 [D loss: 0.610508, acc: 66.41%] [G loss: 1.971140]\n",
      "epoch:13 step:12438 [D loss: 0.639444, acc: 60.16%] [G loss: 1.850450]\n",
      "epoch:13 step:12439 [D loss: 0.645534, acc: 57.81%] [G loss: 1.879986]\n",
      "epoch:13 step:12440 [D loss: 0.616814, acc: 60.94%] [G loss: 1.944488]\n",
      "epoch:13 step:12441 [D loss: 0.677561, acc: 63.28%] [G loss: 1.905685]\n",
      "epoch:13 step:12442 [D loss: 0.654297, acc: 64.84%] [G loss: 2.009481]\n",
      "epoch:13 step:12443 [D loss: 0.646343, acc: 61.72%] [G loss: 2.004260]\n",
      "epoch:13 step:12444 [D loss: 0.662105, acc: 66.41%] [G loss: 2.065752]\n",
      "epoch:13 step:12445 [D loss: 0.581932, acc: 64.84%] [G loss: 1.962928]\n",
      "epoch:13 step:12446 [D loss: 0.662919, acc: 59.38%] [G loss: 1.848901]\n",
      "epoch:13 step:12447 [D loss: 0.669280, acc: 57.03%] [G loss: 1.840001]\n",
      "epoch:13 step:12448 [D loss: 0.596155, acc: 65.62%] [G loss: 1.829967]\n",
      "epoch:13 step:12449 [D loss: 0.698050, acc: 53.12%] [G loss: 1.792092]\n",
      "epoch:13 step:12450 [D loss: 0.700337, acc: 55.47%] [G loss: 2.073258]\n",
      "epoch:13 step:12451 [D loss: 0.648858, acc: 60.94%] [G loss: 1.920601]\n",
      "epoch:13 step:12452 [D loss: 0.652783, acc: 60.16%] [G loss: 2.015442]\n",
      "epoch:13 step:12453 [D loss: 0.577064, acc: 66.41%] [G loss: 1.940436]\n",
      "epoch:13 step:12454 [D loss: 0.685408, acc: 56.25%] [G loss: 1.872761]\n",
      "epoch:13 step:12455 [D loss: 0.652228, acc: 67.19%] [G loss: 2.030812]\n",
      "epoch:13 step:12456 [D loss: 0.620141, acc: 67.19%] [G loss: 2.033783]\n",
      "epoch:13 step:12457 [D loss: 0.567366, acc: 69.53%] [G loss: 2.199306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12458 [D loss: 0.625200, acc: 64.06%] [G loss: 1.992971]\n",
      "epoch:13 step:12459 [D loss: 0.673967, acc: 54.69%] [G loss: 1.869613]\n",
      "epoch:13 step:12460 [D loss: 0.633071, acc: 66.41%] [G loss: 1.832139]\n",
      "epoch:13 step:12461 [D loss: 0.617689, acc: 70.31%] [G loss: 1.944562]\n",
      "epoch:13 step:12462 [D loss: 0.654727, acc: 62.50%] [G loss: 1.874477]\n",
      "epoch:13 step:12463 [D loss: 0.632081, acc: 63.28%] [G loss: 1.891411]\n",
      "epoch:13 step:12464 [D loss: 0.658179, acc: 53.91%] [G loss: 1.923056]\n",
      "epoch:13 step:12465 [D loss: 0.660279, acc: 60.94%] [G loss: 2.047287]\n",
      "epoch:13 step:12466 [D loss: 0.613614, acc: 67.19%] [G loss: 2.015776]\n",
      "epoch:13 step:12467 [D loss: 0.597974, acc: 68.75%] [G loss: 2.011768]\n",
      "epoch:13 step:12468 [D loss: 0.613331, acc: 68.75%] [G loss: 1.833929]\n",
      "epoch:13 step:12469 [D loss: 0.653428, acc: 64.84%] [G loss: 1.966799]\n",
      "epoch:13 step:12470 [D loss: 0.605600, acc: 69.53%] [G loss: 2.075303]\n",
      "epoch:13 step:12471 [D loss: 0.789108, acc: 46.88%] [G loss: 1.825007]\n",
      "epoch:13 step:12472 [D loss: 0.695926, acc: 58.59%] [G loss: 1.964164]\n",
      "epoch:13 step:12473 [D loss: 0.602873, acc: 64.84%] [G loss: 1.800647]\n",
      "epoch:13 step:12474 [D loss: 0.651422, acc: 64.06%] [G loss: 2.002332]\n",
      "epoch:13 step:12475 [D loss: 0.577294, acc: 72.66%] [G loss: 1.904941]\n",
      "epoch:13 step:12476 [D loss: 0.618072, acc: 60.94%] [G loss: 2.012900]\n",
      "epoch:13 step:12477 [D loss: 0.592608, acc: 65.62%] [G loss: 2.072461]\n",
      "epoch:13 step:12478 [D loss: 0.646760, acc: 60.94%] [G loss: 1.947801]\n",
      "epoch:13 step:12479 [D loss: 0.631083, acc: 66.41%] [G loss: 1.884906]\n",
      "epoch:13 step:12480 [D loss: 0.598087, acc: 70.31%] [G loss: 1.821103]\n",
      "epoch:13 step:12481 [D loss: 0.652952, acc: 65.62%] [G loss: 1.825359]\n",
      "epoch:13 step:12482 [D loss: 0.658200, acc: 59.38%] [G loss: 1.866103]\n",
      "epoch:13 step:12483 [D loss: 0.587902, acc: 74.22%] [G loss: 2.115256]\n",
      "epoch:13 step:12484 [D loss: 0.653952, acc: 64.06%] [G loss: 1.946354]\n",
      "epoch:13 step:12485 [D loss: 0.595419, acc: 65.62%] [G loss: 1.859895]\n",
      "epoch:13 step:12486 [D loss: 0.632346, acc: 66.41%] [G loss: 1.909365]\n",
      "epoch:13 step:12487 [D loss: 0.626735, acc: 60.94%] [G loss: 1.980296]\n",
      "epoch:13 step:12488 [D loss: 0.653205, acc: 57.03%] [G loss: 1.770194]\n",
      "epoch:13 step:12489 [D loss: 0.636395, acc: 69.53%] [G loss: 1.866602]\n",
      "epoch:13 step:12490 [D loss: 0.692130, acc: 56.25%] [G loss: 1.813403]\n",
      "epoch:13 step:12491 [D loss: 0.663535, acc: 58.59%] [G loss: 1.925831]\n",
      "epoch:13 step:12492 [D loss: 0.584459, acc: 70.31%] [G loss: 1.869357]\n",
      "epoch:13 step:12493 [D loss: 0.635251, acc: 64.84%] [G loss: 2.186176]\n",
      "epoch:13 step:12494 [D loss: 0.563585, acc: 75.78%] [G loss: 2.217244]\n",
      "epoch:13 step:12495 [D loss: 0.580491, acc: 71.88%] [G loss: 2.317389]\n",
      "epoch:13 step:12496 [D loss: 0.603599, acc: 67.19%] [G loss: 2.272044]\n",
      "epoch:13 step:12497 [D loss: 0.676873, acc: 57.81%] [G loss: 1.893076]\n",
      "epoch:13 step:12498 [D loss: 0.706093, acc: 57.81%] [G loss: 1.897990]\n",
      "epoch:13 step:12499 [D loss: 0.635935, acc: 62.50%] [G loss: 1.955893]\n",
      "epoch:13 step:12500 [D loss: 0.640582, acc: 61.72%] [G loss: 1.861882]\n",
      "epoch:13 step:12501 [D loss: 0.613759, acc: 69.53%] [G loss: 1.955275]\n",
      "epoch:13 step:12502 [D loss: 0.639260, acc: 62.50%] [G loss: 2.116868]\n",
      "epoch:13 step:12503 [D loss: 0.629951, acc: 67.19%] [G loss: 2.025221]\n",
      "epoch:13 step:12504 [D loss: 0.689517, acc: 54.69%] [G loss: 1.862998]\n",
      "epoch:13 step:12505 [D loss: 0.628484, acc: 57.03%] [G loss: 1.866547]\n",
      "epoch:13 step:12506 [D loss: 0.646341, acc: 64.06%] [G loss: 2.051123]\n",
      "epoch:13 step:12507 [D loss: 0.653017, acc: 62.50%] [G loss: 1.891086]\n",
      "epoch:13 step:12508 [D loss: 0.667963, acc: 62.50%] [G loss: 1.867931]\n",
      "epoch:13 step:12509 [D loss: 0.676601, acc: 58.59%] [G loss: 2.042659]\n",
      "epoch:13 step:12510 [D loss: 0.684568, acc: 58.59%] [G loss: 1.927955]\n",
      "epoch:13 step:12511 [D loss: 0.620923, acc: 67.19%] [G loss: 1.862278]\n",
      "epoch:13 step:12512 [D loss: 0.643030, acc: 64.06%] [G loss: 1.898018]\n",
      "epoch:13 step:12513 [D loss: 0.593364, acc: 65.62%] [G loss: 1.971284]\n",
      "epoch:13 step:12514 [D loss: 0.617892, acc: 67.19%] [G loss: 2.118295]\n",
      "epoch:13 step:12515 [D loss: 0.687318, acc: 60.94%] [G loss: 1.983168]\n",
      "epoch:13 step:12516 [D loss: 0.604252, acc: 68.75%] [G loss: 2.100692]\n",
      "epoch:13 step:12517 [D loss: 0.621460, acc: 67.97%] [G loss: 2.081466]\n",
      "epoch:13 step:12518 [D loss: 0.669942, acc: 60.94%] [G loss: 1.962894]\n",
      "epoch:13 step:12519 [D loss: 0.637127, acc: 63.28%] [G loss: 2.012454]\n",
      "epoch:13 step:12520 [D loss: 0.654131, acc: 63.28%] [G loss: 2.011531]\n",
      "epoch:13 step:12521 [D loss: 0.595258, acc: 68.75%] [G loss: 2.098639]\n",
      "epoch:13 step:12522 [D loss: 0.702348, acc: 55.47%] [G loss: 1.891396]\n",
      "epoch:13 step:12523 [D loss: 0.664984, acc: 60.94%] [G loss: 1.915634]\n",
      "epoch:13 step:12524 [D loss: 0.707601, acc: 62.50%] [G loss: 1.891721]\n",
      "epoch:13 step:12525 [D loss: 0.666008, acc: 59.38%] [G loss: 2.009740]\n",
      "epoch:13 step:12526 [D loss: 0.615232, acc: 68.75%] [G loss: 2.213403]\n",
      "epoch:13 step:12527 [D loss: 0.577356, acc: 71.88%] [G loss: 2.376188]\n",
      "epoch:13 step:12528 [D loss: 0.536405, acc: 74.22%] [G loss: 2.327601]\n",
      "epoch:13 step:12529 [D loss: 0.686008, acc: 56.25%] [G loss: 1.956702]\n",
      "epoch:13 step:12530 [D loss: 0.689328, acc: 55.47%] [G loss: 1.828938]\n",
      "epoch:13 step:12531 [D loss: 0.646174, acc: 58.59%] [G loss: 1.938350]\n",
      "epoch:13 step:12532 [D loss: 0.668377, acc: 63.28%] [G loss: 1.805843]\n",
      "epoch:13 step:12533 [D loss: 0.718595, acc: 53.12%] [G loss: 1.992403]\n",
      "epoch:13 step:12534 [D loss: 0.616671, acc: 63.28%] [G loss: 1.990965]\n",
      "epoch:13 step:12535 [D loss: 0.684300, acc: 54.69%] [G loss: 2.132199]\n",
      "epoch:13 step:12536 [D loss: 0.646575, acc: 62.50%] [G loss: 1.832945]\n",
      "epoch:13 step:12537 [D loss: 0.693486, acc: 57.81%] [G loss: 1.911298]\n",
      "epoch:13 step:12538 [D loss: 0.642132, acc: 61.72%] [G loss: 1.913239]\n",
      "epoch:13 step:12539 [D loss: 0.615266, acc: 65.62%] [G loss: 2.109974]\n",
      "epoch:13 step:12540 [D loss: 0.596032, acc: 69.53%] [G loss: 1.855138]\n",
      "epoch:13 step:12541 [D loss: 0.654272, acc: 58.59%] [G loss: 2.008220]\n",
      "epoch:13 step:12542 [D loss: 0.641185, acc: 61.72%] [G loss: 1.910056]\n",
      "epoch:13 step:12543 [D loss: 0.600965, acc: 69.53%] [G loss: 1.790476]\n",
      "epoch:13 step:12544 [D loss: 0.653042, acc: 67.97%] [G loss: 1.822581]\n",
      "epoch:13 step:12545 [D loss: 0.601189, acc: 75.78%] [G loss: 2.033213]\n",
      "epoch:13 step:12546 [D loss: 0.602051, acc: 69.53%] [G loss: 1.906962]\n",
      "epoch:13 step:12547 [D loss: 0.608426, acc: 64.84%] [G loss: 1.988812]\n",
      "epoch:13 step:12548 [D loss: 0.665673, acc: 64.84%] [G loss: 1.942923]\n",
      "epoch:13 step:12549 [D loss: 0.660703, acc: 62.50%] [G loss: 1.850151]\n",
      "epoch:13 step:12550 [D loss: 0.638367, acc: 68.75%] [G loss: 1.926158]\n",
      "epoch:13 step:12551 [D loss: 0.613627, acc: 67.97%] [G loss: 1.954524]\n",
      "epoch:13 step:12552 [D loss: 0.598289, acc: 67.97%] [G loss: 2.140147]\n",
      "epoch:13 step:12553 [D loss: 0.633132, acc: 60.94%] [G loss: 2.006949]\n",
      "epoch:13 step:12554 [D loss: 0.728449, acc: 53.91%] [G loss: 1.775812]\n",
      "epoch:13 step:12555 [D loss: 0.611570, acc: 64.84%] [G loss: 1.981008]\n",
      "epoch:13 step:12556 [D loss: 0.648144, acc: 62.50%] [G loss: 1.962638]\n",
      "epoch:13 step:12557 [D loss: 0.658258, acc: 63.28%] [G loss: 1.905895]\n",
      "epoch:13 step:12558 [D loss: 0.709762, acc: 55.47%] [G loss: 1.808005]\n",
      "epoch:13 step:12559 [D loss: 0.624668, acc: 64.84%] [G loss: 1.899626]\n",
      "epoch:13 step:12560 [D loss: 0.652160, acc: 61.72%] [G loss: 1.934168]\n",
      "epoch:13 step:12561 [D loss: 0.666936, acc: 59.38%] [G loss: 1.969707]\n",
      "epoch:13 step:12562 [D loss: 0.617309, acc: 73.44%] [G loss: 1.964977]\n",
      "epoch:13 step:12563 [D loss: 0.641281, acc: 60.16%] [G loss: 1.973316]\n",
      "epoch:13 step:12564 [D loss: 0.681414, acc: 57.81%] [G loss: 2.003829]\n",
      "epoch:13 step:12565 [D loss: 0.578382, acc: 72.66%] [G loss: 1.996702]\n",
      "epoch:13 step:12566 [D loss: 0.574210, acc: 70.31%] [G loss: 2.052876]\n",
      "epoch:13 step:12567 [D loss: 0.665822, acc: 62.50%] [G loss: 1.886204]\n",
      "epoch:13 step:12568 [D loss: 0.638835, acc: 65.62%] [G loss: 1.828948]\n",
      "epoch:13 step:12569 [D loss: 0.619228, acc: 68.75%] [G loss: 1.941616]\n",
      "epoch:13 step:12570 [D loss: 0.640026, acc: 65.62%] [G loss: 1.939084]\n",
      "epoch:13 step:12571 [D loss: 0.651266, acc: 61.72%] [G loss: 1.987491]\n",
      "epoch:13 step:12572 [D loss: 0.579555, acc: 71.09%] [G loss: 1.833207]\n",
      "epoch:13 step:12573 [D loss: 0.635169, acc: 63.28%] [G loss: 2.061477]\n",
      "epoch:13 step:12574 [D loss: 0.638179, acc: 60.94%] [G loss: 1.907570]\n",
      "epoch:13 step:12575 [D loss: 0.614154, acc: 61.72%] [G loss: 1.929049]\n",
      "epoch:13 step:12576 [D loss: 0.675839, acc: 60.94%] [G loss: 1.954246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12577 [D loss: 0.629049, acc: 60.94%] [G loss: 1.805110]\n",
      "epoch:13 step:12578 [D loss: 0.640991, acc: 58.59%] [G loss: 1.875094]\n",
      "epoch:13 step:12579 [D loss: 0.657812, acc: 59.38%] [G loss: 2.055232]\n",
      "epoch:13 step:12580 [D loss: 0.619909, acc: 68.75%] [G loss: 1.856291]\n",
      "epoch:13 step:12581 [D loss: 0.656980, acc: 65.62%] [G loss: 2.010761]\n",
      "epoch:13 step:12582 [D loss: 0.612179, acc: 67.97%] [G loss: 2.010692]\n",
      "epoch:13 step:12583 [D loss: 0.623578, acc: 66.41%] [G loss: 2.019615]\n",
      "epoch:13 step:12584 [D loss: 0.620489, acc: 63.28%] [G loss: 2.077528]\n",
      "epoch:13 step:12585 [D loss: 0.672955, acc: 57.03%] [G loss: 2.070019]\n",
      "epoch:13 step:12586 [D loss: 0.639732, acc: 65.62%] [G loss: 2.076138]\n",
      "epoch:13 step:12587 [D loss: 0.645276, acc: 64.84%] [G loss: 2.033681]\n",
      "epoch:13 step:12588 [D loss: 0.641597, acc: 65.62%] [G loss: 2.038110]\n",
      "epoch:13 step:12589 [D loss: 0.660331, acc: 60.16%] [G loss: 1.912357]\n",
      "epoch:13 step:12590 [D loss: 0.676589, acc: 58.59%] [G loss: 2.045066]\n",
      "epoch:13 step:12591 [D loss: 0.578893, acc: 74.22%] [G loss: 1.982851]\n",
      "epoch:13 step:12592 [D loss: 0.688058, acc: 60.94%] [G loss: 1.953845]\n",
      "epoch:13 step:12593 [D loss: 0.672638, acc: 60.94%] [G loss: 1.847189]\n",
      "epoch:13 step:12594 [D loss: 0.612983, acc: 66.41%] [G loss: 1.965462]\n",
      "epoch:13 step:12595 [D loss: 0.632461, acc: 64.06%] [G loss: 2.017139]\n",
      "epoch:13 step:12596 [D loss: 0.656052, acc: 63.28%] [G loss: 1.947184]\n",
      "epoch:13 step:12597 [D loss: 0.599721, acc: 71.09%] [G loss: 2.090776]\n",
      "epoch:13 step:12598 [D loss: 0.631181, acc: 65.62%] [G loss: 1.950931]\n",
      "epoch:13 step:12599 [D loss: 0.706940, acc: 57.81%] [G loss: 1.793971]\n",
      "epoch:13 step:12600 [D loss: 0.647948, acc: 61.72%] [G loss: 1.888813]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.092996\n",
      "FID: 16.736334\n",
      "0 = 12.63031021213528\n",
      "1 = 0.08038865334453521\n",
      "2 = 0.8935999870300293\n",
      "3 = 0.8988000154495239\n",
      "4 = 0.8884000182151794\n",
      "5 = 0.8895487189292908\n",
      "6 = 0.8988000154495239\n",
      "7 = 6.746215338230106\n",
      "8 = 0.07589829048774684\n",
      "9 = 0.7402999997138977\n",
      "10 = 0.751800000667572\n",
      "11 = 0.7287999987602234\n",
      "12 = 0.7348973751068115\n",
      "13 = 0.751800000667572\n",
      "14 = 7.093025207519531\n",
      "15 = 9.492145538330078\n",
      "16 = 0.10772272944450378\n",
      "17 = 7.092995643615723\n",
      "18 = 16.7363338470459\n",
      "epoch:13 step:12601 [D loss: 0.674844, acc: 57.03%] [G loss: 1.919734]\n",
      "epoch:13 step:12602 [D loss: 0.659219, acc: 60.94%] [G loss: 1.707514]\n",
      "epoch:13 step:12603 [D loss: 0.647096, acc: 64.84%] [G loss: 1.904627]\n",
      "epoch:13 step:12604 [D loss: 0.653750, acc: 63.28%] [G loss: 1.877133]\n",
      "epoch:13 step:12605 [D loss: 0.667444, acc: 58.59%] [G loss: 1.916307]\n",
      "epoch:13 step:12606 [D loss: 0.614003, acc: 67.19%] [G loss: 1.965099]\n",
      "epoch:13 step:12607 [D loss: 0.587391, acc: 69.53%] [G loss: 2.051007]\n",
      "epoch:13 step:12608 [D loss: 0.606600, acc: 67.97%] [G loss: 2.143033]\n",
      "epoch:13 step:12609 [D loss: 0.611332, acc: 67.97%] [G loss: 2.066366]\n",
      "epoch:13 step:12610 [D loss: 0.668247, acc: 64.06%] [G loss: 2.118026]\n",
      "epoch:13 step:12611 [D loss: 0.574325, acc: 71.88%] [G loss: 2.073129]\n",
      "epoch:13 step:12612 [D loss: 0.555703, acc: 74.22%] [G loss: 2.088763]\n",
      "epoch:13 step:12613 [D loss: 0.685050, acc: 58.59%] [G loss: 1.917663]\n",
      "epoch:13 step:12614 [D loss: 0.648840, acc: 60.94%] [G loss: 1.956409]\n",
      "epoch:13 step:12615 [D loss: 0.650626, acc: 61.72%] [G loss: 2.032825]\n",
      "epoch:13 step:12616 [D loss: 0.635772, acc: 67.97%] [G loss: 2.092657]\n",
      "epoch:13 step:12617 [D loss: 0.641854, acc: 61.72%] [G loss: 2.072525]\n",
      "epoch:13 step:12618 [D loss: 0.719742, acc: 53.12%] [G loss: 1.743240]\n",
      "epoch:13 step:12619 [D loss: 0.647710, acc: 61.72%] [G loss: 1.768411]\n",
      "epoch:13 step:12620 [D loss: 0.676679, acc: 59.38%] [G loss: 1.769270]\n",
      "epoch:13 step:12621 [D loss: 0.700422, acc: 57.03%] [G loss: 1.868040]\n",
      "epoch:13 step:12622 [D loss: 0.659271, acc: 57.03%] [G loss: 1.851132]\n",
      "epoch:13 step:12623 [D loss: 0.662437, acc: 64.84%] [G loss: 1.935818]\n",
      "epoch:13 step:12624 [D loss: 0.610680, acc: 67.19%] [G loss: 1.901125]\n",
      "epoch:13 step:12625 [D loss: 0.656591, acc: 60.16%] [G loss: 1.802365]\n",
      "epoch:13 step:12626 [D loss: 0.651101, acc: 61.72%] [G loss: 1.938544]\n",
      "epoch:13 step:12627 [D loss: 0.651291, acc: 65.62%] [G loss: 1.838907]\n",
      "epoch:13 step:12628 [D loss: 0.638148, acc: 64.06%] [G loss: 2.035398]\n",
      "epoch:13 step:12629 [D loss: 0.650538, acc: 57.81%] [G loss: 1.969365]\n",
      "epoch:13 step:12630 [D loss: 0.671795, acc: 56.25%] [G loss: 1.906630]\n",
      "epoch:13 step:12631 [D loss: 0.688101, acc: 54.69%] [G loss: 1.956883]\n",
      "epoch:13 step:12632 [D loss: 0.618939, acc: 64.06%] [G loss: 1.924583]\n",
      "epoch:13 step:12633 [D loss: 0.653327, acc: 61.72%] [G loss: 1.936752]\n",
      "epoch:13 step:12634 [D loss: 0.652766, acc: 60.94%] [G loss: 1.797346]\n",
      "epoch:13 step:12635 [D loss: 0.629699, acc: 68.75%] [G loss: 1.916955]\n",
      "epoch:13 step:12636 [D loss: 0.677956, acc: 61.72%] [G loss: 1.809965]\n",
      "epoch:13 step:12637 [D loss: 0.659081, acc: 58.59%] [G loss: 2.030979]\n",
      "epoch:13 step:12638 [D loss: 0.592878, acc: 71.88%] [G loss: 2.021587]\n",
      "epoch:13 step:12639 [D loss: 0.702649, acc: 56.25%] [G loss: 1.783816]\n",
      "epoch:13 step:12640 [D loss: 0.687316, acc: 60.16%] [G loss: 1.810507]\n",
      "epoch:13 step:12641 [D loss: 0.676635, acc: 58.59%] [G loss: 1.752182]\n",
      "epoch:13 step:12642 [D loss: 0.645303, acc: 59.38%] [G loss: 1.766056]\n",
      "epoch:13 step:12643 [D loss: 0.657520, acc: 62.50%] [G loss: 1.799482]\n",
      "epoch:13 step:12644 [D loss: 0.607037, acc: 64.06%] [G loss: 1.886958]\n",
      "epoch:13 step:12645 [D loss: 0.622221, acc: 65.62%] [G loss: 1.975087]\n",
      "epoch:13 step:12646 [D loss: 0.622724, acc: 64.06%] [G loss: 2.080755]\n",
      "epoch:13 step:12647 [D loss: 0.629269, acc: 67.19%] [G loss: 1.892118]\n",
      "epoch:13 step:12648 [D loss: 0.649040, acc: 61.72%] [G loss: 1.838365]\n",
      "epoch:13 step:12649 [D loss: 0.570050, acc: 74.22%] [G loss: 1.981817]\n",
      "epoch:13 step:12650 [D loss: 0.588301, acc: 69.53%] [G loss: 2.127649]\n",
      "epoch:13 step:12651 [D loss: 0.604735, acc: 67.19%] [G loss: 1.967331]\n",
      "epoch:13 step:12652 [D loss: 0.646018, acc: 61.72%] [G loss: 2.153072]\n",
      "epoch:13 step:12653 [D loss: 0.623333, acc: 68.75%] [G loss: 2.237653]\n",
      "epoch:13 step:12654 [D loss: 0.695299, acc: 57.03%] [G loss: 2.005186]\n",
      "epoch:13 step:12655 [D loss: 0.647891, acc: 66.41%] [G loss: 2.069676]\n",
      "epoch:13 step:12656 [D loss: 0.711318, acc: 52.34%] [G loss: 2.024622]\n",
      "epoch:13 step:12657 [D loss: 0.605788, acc: 67.19%] [G loss: 2.108961]\n",
      "epoch:13 step:12658 [D loss: 0.650261, acc: 64.06%] [G loss: 1.861223]\n",
      "epoch:13 step:12659 [D loss: 0.665222, acc: 60.16%] [G loss: 1.936966]\n",
      "epoch:13 step:12660 [D loss: 0.621763, acc: 63.28%] [G loss: 2.024524]\n",
      "epoch:13 step:12661 [D loss: 0.647665, acc: 57.81%] [G loss: 1.996825]\n",
      "epoch:13 step:12662 [D loss: 0.649399, acc: 60.94%] [G loss: 2.097474]\n",
      "epoch:13 step:12663 [D loss: 0.670619, acc: 63.28%] [G loss: 1.776583]\n",
      "epoch:13 step:12664 [D loss: 0.656633, acc: 64.84%] [G loss: 1.865514]\n",
      "epoch:13 step:12665 [D loss: 0.601822, acc: 68.75%] [G loss: 2.045533]\n",
      "epoch:13 step:12666 [D loss: 0.687556, acc: 57.81%] [G loss: 1.859426]\n",
      "epoch:13 step:12667 [D loss: 0.637260, acc: 66.41%] [G loss: 1.940679]\n",
      "epoch:13 step:12668 [D loss: 0.668243, acc: 57.03%] [G loss: 1.899270]\n",
      "epoch:13 step:12669 [D loss: 0.589221, acc: 64.84%] [G loss: 2.100487]\n",
      "epoch:13 step:12670 [D loss: 0.631511, acc: 68.75%] [G loss: 1.895892]\n",
      "epoch:13 step:12671 [D loss: 0.678588, acc: 57.03%] [G loss: 1.894379]\n",
      "epoch:13 step:12672 [D loss: 0.627694, acc: 66.41%] [G loss: 2.033710]\n",
      "epoch:13 step:12673 [D loss: 0.686035, acc: 61.72%] [G loss: 1.858413]\n",
      "epoch:13 step:12674 [D loss: 0.673220, acc: 60.94%] [G loss: 1.890043]\n",
      "epoch:13 step:12675 [D loss: 0.582312, acc: 67.97%] [G loss: 2.087394]\n",
      "epoch:13 step:12676 [D loss: 0.568817, acc: 72.66%] [G loss: 2.053522]\n",
      "epoch:13 step:12677 [D loss: 0.579753, acc: 67.97%] [G loss: 2.097908]\n",
      "epoch:13 step:12678 [D loss: 0.646986, acc: 64.84%] [G loss: 2.078777]\n",
      "epoch:13 step:12679 [D loss: 0.611972, acc: 64.84%] [G loss: 2.008713]\n",
      "epoch:13 step:12680 [D loss: 0.632683, acc: 64.84%] [G loss: 2.143047]\n",
      "epoch:13 step:12681 [D loss: 0.700416, acc: 55.47%] [G loss: 1.960979]\n",
      "epoch:13 step:12682 [D loss: 0.674051, acc: 54.69%] [G loss: 1.904593]\n",
      "epoch:13 step:12683 [D loss: 0.667093, acc: 62.50%] [G loss: 1.880560]\n",
      "epoch:13 step:12684 [D loss: 0.617686, acc: 67.97%] [G loss: 2.045660]\n",
      "epoch:13 step:12685 [D loss: 0.649483, acc: 63.28%] [G loss: 2.131691]\n",
      "epoch:13 step:12686 [D loss: 0.590832, acc: 71.09%] [G loss: 2.042295]\n",
      "epoch:13 step:12687 [D loss: 0.641961, acc: 59.38%] [G loss: 1.874249]\n",
      "epoch:13 step:12688 [D loss: 0.610949, acc: 65.62%] [G loss: 1.901365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12689 [D loss: 0.633337, acc: 60.94%] [G loss: 2.178432]\n",
      "epoch:13 step:12690 [D loss: 0.622751, acc: 66.41%] [G loss: 2.106348]\n",
      "epoch:13 step:12691 [D loss: 0.676803, acc: 58.59%] [G loss: 1.878480]\n",
      "epoch:13 step:12692 [D loss: 0.737077, acc: 48.44%] [G loss: 1.810929]\n",
      "epoch:13 step:12693 [D loss: 0.693295, acc: 60.16%] [G loss: 1.893352]\n",
      "epoch:13 step:12694 [D loss: 0.649853, acc: 63.28%] [G loss: 1.970139]\n",
      "epoch:13 step:12695 [D loss: 0.633551, acc: 61.72%] [G loss: 1.927297]\n",
      "epoch:13 step:12696 [D loss: 0.619634, acc: 68.75%] [G loss: 1.848875]\n",
      "epoch:13 step:12697 [D loss: 0.609838, acc: 68.75%] [G loss: 2.048502]\n",
      "epoch:13 step:12698 [D loss: 0.665019, acc: 60.16%] [G loss: 1.894007]\n",
      "epoch:13 step:12699 [D loss: 0.658981, acc: 58.59%] [G loss: 1.808060]\n",
      "epoch:13 step:12700 [D loss: 0.635140, acc: 60.94%] [G loss: 1.995905]\n",
      "epoch:13 step:12701 [D loss: 0.653916, acc: 63.28%] [G loss: 1.975633]\n",
      "epoch:13 step:12702 [D loss: 0.636352, acc: 64.84%] [G loss: 1.914044]\n",
      "epoch:13 step:12703 [D loss: 0.661589, acc: 63.28%] [G loss: 2.008954]\n",
      "epoch:13 step:12704 [D loss: 0.649793, acc: 61.72%] [G loss: 2.214868]\n",
      "epoch:13 step:12705 [D loss: 0.622480, acc: 64.06%] [G loss: 2.105384]\n",
      "epoch:13 step:12706 [D loss: 0.584979, acc: 66.41%] [G loss: 1.921275]\n",
      "epoch:13 step:12707 [D loss: 0.659878, acc: 64.06%] [G loss: 1.898829]\n",
      "epoch:13 step:12708 [D loss: 0.591164, acc: 71.09%] [G loss: 2.094213]\n",
      "epoch:13 step:12709 [D loss: 0.674114, acc: 60.94%] [G loss: 1.766558]\n",
      "epoch:13 step:12710 [D loss: 0.700443, acc: 53.12%] [G loss: 1.806105]\n",
      "epoch:13 step:12711 [D loss: 0.670309, acc: 58.59%] [G loss: 1.739315]\n",
      "epoch:13 step:12712 [D loss: 0.679490, acc: 57.03%] [G loss: 1.736136]\n",
      "epoch:13 step:12713 [D loss: 0.630504, acc: 67.97%] [G loss: 2.036107]\n",
      "epoch:13 step:12714 [D loss: 0.649726, acc: 61.72%] [G loss: 2.062206]\n",
      "epoch:13 step:12715 [D loss: 0.593480, acc: 67.19%] [G loss: 1.993710]\n",
      "epoch:13 step:12716 [D loss: 0.663433, acc: 64.06%] [G loss: 1.918875]\n",
      "epoch:13 step:12717 [D loss: 0.596529, acc: 71.88%] [G loss: 2.073220]\n",
      "epoch:13 step:12718 [D loss: 0.689749, acc: 60.16%] [G loss: 1.759610]\n",
      "epoch:13 step:12719 [D loss: 0.645452, acc: 62.50%] [G loss: 1.844869]\n",
      "epoch:13 step:12720 [D loss: 0.703592, acc: 54.69%] [G loss: 1.934196]\n",
      "epoch:13 step:12721 [D loss: 0.689902, acc: 54.69%] [G loss: 1.896522]\n",
      "epoch:13 step:12722 [D loss: 0.643375, acc: 60.94%] [G loss: 1.903499]\n",
      "epoch:13 step:12723 [D loss: 0.706768, acc: 53.12%] [G loss: 1.821767]\n",
      "epoch:13 step:12724 [D loss: 0.641006, acc: 64.84%] [G loss: 1.941592]\n",
      "epoch:13 step:12725 [D loss: 0.711085, acc: 58.59%] [G loss: 1.858774]\n",
      "epoch:13 step:12726 [D loss: 0.668345, acc: 57.03%] [G loss: 2.068118]\n",
      "epoch:13 step:12727 [D loss: 0.620915, acc: 65.62%] [G loss: 2.189699]\n",
      "epoch:13 step:12728 [D loss: 0.645835, acc: 63.28%] [G loss: 2.074658]\n",
      "epoch:13 step:12729 [D loss: 0.623873, acc: 67.19%] [G loss: 2.051360]\n",
      "epoch:13 step:12730 [D loss: 0.656502, acc: 60.16%] [G loss: 1.990496]\n",
      "epoch:13 step:12731 [D loss: 0.600047, acc: 71.88%] [G loss: 2.033912]\n",
      "epoch:13 step:12732 [D loss: 0.606314, acc: 65.62%] [G loss: 2.152022]\n",
      "epoch:13 step:12733 [D loss: 0.594181, acc: 69.53%] [G loss: 2.035820]\n",
      "epoch:13 step:12734 [D loss: 0.662274, acc: 66.41%] [G loss: 1.906896]\n",
      "epoch:13 step:12735 [D loss: 0.583570, acc: 71.09%] [G loss: 2.134173]\n",
      "epoch:13 step:12736 [D loss: 0.618209, acc: 65.62%] [G loss: 1.940243]\n",
      "epoch:13 step:12737 [D loss: 0.566840, acc: 71.09%] [G loss: 2.002265]\n",
      "epoch:13 step:12738 [D loss: 0.625756, acc: 65.62%] [G loss: 1.937956]\n",
      "epoch:13 step:12739 [D loss: 0.627122, acc: 62.50%] [G loss: 2.040403]\n",
      "epoch:13 step:12740 [D loss: 0.689708, acc: 61.72%] [G loss: 2.021783]\n",
      "epoch:13 step:12741 [D loss: 0.632126, acc: 60.94%] [G loss: 1.857902]\n",
      "epoch:13 step:12742 [D loss: 0.633679, acc: 61.72%] [G loss: 1.969203]\n",
      "epoch:13 step:12743 [D loss: 0.688151, acc: 60.94%] [G loss: 1.986331]\n",
      "epoch:13 step:12744 [D loss: 0.633683, acc: 62.50%] [G loss: 1.921751]\n",
      "epoch:13 step:12745 [D loss: 0.627862, acc: 69.53%] [G loss: 2.165209]\n",
      "epoch:13 step:12746 [D loss: 0.658366, acc: 64.06%] [G loss: 1.820985]\n",
      "epoch:13 step:12747 [D loss: 0.681110, acc: 57.03%] [G loss: 1.810362]\n",
      "epoch:13 step:12748 [D loss: 0.690765, acc: 54.69%] [G loss: 1.852380]\n",
      "epoch:13 step:12749 [D loss: 0.643749, acc: 63.28%] [G loss: 1.992543]\n",
      "epoch:13 step:12750 [D loss: 0.676169, acc: 57.03%] [G loss: 1.883733]\n",
      "epoch:13 step:12751 [D loss: 0.667689, acc: 63.28%] [G loss: 1.938694]\n",
      "epoch:13 step:12752 [D loss: 0.641059, acc: 60.94%] [G loss: 1.902263]\n",
      "epoch:13 step:12753 [D loss: 0.658841, acc: 60.16%] [G loss: 1.925447]\n",
      "epoch:13 step:12754 [D loss: 0.674125, acc: 64.84%] [G loss: 1.761200]\n",
      "epoch:13 step:12755 [D loss: 0.582044, acc: 71.88%] [G loss: 2.076257]\n",
      "epoch:13 step:12756 [D loss: 0.657050, acc: 59.38%] [G loss: 1.840446]\n",
      "epoch:13 step:12757 [D loss: 0.638711, acc: 60.94%] [G loss: 1.865592]\n",
      "epoch:13 step:12758 [D loss: 0.683647, acc: 55.47%] [G loss: 1.822609]\n",
      "epoch:13 step:12759 [D loss: 0.694997, acc: 59.38%] [G loss: 1.856508]\n",
      "epoch:13 step:12760 [D loss: 0.641266, acc: 64.06%] [G loss: 1.910238]\n",
      "epoch:13 step:12761 [D loss: 0.679564, acc: 59.38%] [G loss: 1.838610]\n",
      "epoch:13 step:12762 [D loss: 0.669601, acc: 62.50%] [G loss: 1.886260]\n",
      "epoch:13 step:12763 [D loss: 0.666642, acc: 54.69%] [G loss: 1.889637]\n",
      "epoch:13 step:12764 [D loss: 0.666459, acc: 60.94%] [G loss: 1.883902]\n",
      "epoch:13 step:12765 [D loss: 0.646777, acc: 60.16%] [G loss: 1.919308]\n",
      "epoch:13 step:12766 [D loss: 0.651851, acc: 63.28%] [G loss: 1.957758]\n",
      "epoch:13 step:12767 [D loss: 0.610347, acc: 66.41%] [G loss: 1.739334]\n",
      "epoch:13 step:12768 [D loss: 0.648290, acc: 58.59%] [G loss: 1.812904]\n",
      "epoch:13 step:12769 [D loss: 0.608504, acc: 68.75%] [G loss: 1.947280]\n",
      "epoch:13 step:12770 [D loss: 0.609405, acc: 65.62%] [G loss: 2.162827]\n",
      "epoch:13 step:12771 [D loss: 0.651972, acc: 60.94%] [G loss: 1.822768]\n",
      "epoch:13 step:12772 [D loss: 0.651231, acc: 62.50%] [G loss: 1.898967]\n",
      "epoch:13 step:12773 [D loss: 0.676812, acc: 59.38%] [G loss: 1.975425]\n",
      "epoch:13 step:12774 [D loss: 0.663147, acc: 58.59%] [G loss: 1.855387]\n",
      "epoch:13 step:12775 [D loss: 0.626895, acc: 60.16%] [G loss: 1.918952]\n",
      "epoch:13 step:12776 [D loss: 0.669190, acc: 62.50%] [G loss: 1.772297]\n",
      "epoch:13 step:12777 [D loss: 0.661729, acc: 57.81%] [G loss: 1.691822]\n",
      "epoch:13 step:12778 [D loss: 0.682742, acc: 61.72%] [G loss: 1.906530]\n",
      "epoch:13 step:12779 [D loss: 0.673277, acc: 58.59%] [G loss: 1.897449]\n",
      "epoch:13 step:12780 [D loss: 0.673295, acc: 61.72%] [G loss: 1.845585]\n",
      "epoch:13 step:12781 [D loss: 0.609109, acc: 64.06%] [G loss: 1.765212]\n",
      "epoch:13 step:12782 [D loss: 0.678334, acc: 60.94%] [G loss: 1.966585]\n",
      "epoch:13 step:12783 [D loss: 0.603672, acc: 64.06%] [G loss: 1.967635]\n",
      "epoch:13 step:12784 [D loss: 0.624001, acc: 60.94%] [G loss: 1.925946]\n",
      "epoch:13 step:12785 [D loss: 0.675530, acc: 59.38%] [G loss: 1.995875]\n",
      "epoch:13 step:12786 [D loss: 0.630263, acc: 68.75%] [G loss: 2.087369]\n",
      "epoch:13 step:12787 [D loss: 0.687488, acc: 55.47%] [G loss: 1.924610]\n",
      "epoch:13 step:12788 [D loss: 0.671911, acc: 59.38%] [G loss: 2.026763]\n",
      "epoch:13 step:12789 [D loss: 0.643457, acc: 64.84%] [G loss: 1.901705]\n",
      "epoch:13 step:12790 [D loss: 0.644662, acc: 65.62%] [G loss: 2.012534]\n",
      "epoch:13 step:12791 [D loss: 0.606496, acc: 67.19%] [G loss: 2.061276]\n",
      "epoch:13 step:12792 [D loss: 0.675349, acc: 55.47%] [G loss: 1.853002]\n",
      "epoch:13 step:12793 [D loss: 0.655589, acc: 60.94%] [G loss: 1.751045]\n",
      "epoch:13 step:12794 [D loss: 0.615723, acc: 66.41%] [G loss: 1.985144]\n",
      "epoch:13 step:12795 [D loss: 0.664482, acc: 60.16%] [G loss: 1.838744]\n",
      "epoch:13 step:12796 [D loss: 0.699682, acc: 54.69%] [G loss: 1.915357]\n",
      "epoch:13 step:12797 [D loss: 0.614712, acc: 66.41%] [G loss: 1.772709]\n",
      "epoch:13 step:12798 [D loss: 0.658451, acc: 60.16%] [G loss: 1.867247]\n",
      "epoch:13 step:12799 [D loss: 0.644017, acc: 64.06%] [G loss: 1.768133]\n",
      "epoch:13 step:12800 [D loss: 0.574410, acc: 71.88%] [G loss: 1.899821]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.917592\n",
      "FID: 19.632235\n",
      "0 = 12.67089774103167\n",
      "1 = 0.08002228068981361\n",
      "2 = 0.8855999708175659\n",
      "3 = 0.8944000005722046\n",
      "4 = 0.876800000667572\n",
      "5 = 0.8789308071136475\n",
      "6 = 0.8944000005722046\n",
      "7 = 7.03650970380308\n",
      "8 = 0.08659940187807101\n",
      "9 = 0.746999979019165\n",
      "10 = 0.7562000155448914\n",
      "11 = 0.7378000020980835\n",
      "12 = 0.7425373196601868\n",
      "13 = 0.7562000155448914\n",
      "14 = 6.917618274688721\n",
      "15 = 9.362271308898926\n",
      "16 = 0.13655231893062592\n",
      "17 = 6.917591571807861\n",
      "18 = 19.632234573364258\n",
      "epoch:13 step:12801 [D loss: 0.663491, acc: 63.28%] [G loss: 1.959174]\n",
      "epoch:13 step:12802 [D loss: 0.637617, acc: 62.50%] [G loss: 1.971784]\n",
      "epoch:13 step:12803 [D loss: 0.651087, acc: 64.06%] [G loss: 1.925918]\n",
      "epoch:13 step:12804 [D loss: 0.658657, acc: 60.94%] [G loss: 1.986779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12805 [D loss: 0.631284, acc: 67.97%] [G loss: 2.093174]\n",
      "epoch:13 step:12806 [D loss: 0.672566, acc: 53.91%] [G loss: 1.728133]\n",
      "epoch:13 step:12807 [D loss: 0.651845, acc: 61.72%] [G loss: 1.929835]\n",
      "epoch:13 step:12808 [D loss: 0.633465, acc: 63.28%] [G loss: 1.937831]\n",
      "epoch:13 step:12809 [D loss: 0.744491, acc: 56.25%] [G loss: 1.892562]\n",
      "epoch:13 step:12810 [D loss: 0.667179, acc: 62.50%] [G loss: 1.834194]\n",
      "epoch:13 step:12811 [D loss: 0.645898, acc: 58.59%] [G loss: 1.841178]\n",
      "epoch:13 step:12812 [D loss: 0.588675, acc: 69.53%] [G loss: 1.919718]\n",
      "epoch:13 step:12813 [D loss: 0.654732, acc: 57.81%] [G loss: 1.926129]\n",
      "epoch:13 step:12814 [D loss: 0.611248, acc: 67.19%] [G loss: 2.012597]\n",
      "epoch:13 step:12815 [D loss: 0.612842, acc: 69.53%] [G loss: 1.996589]\n",
      "epoch:13 step:12816 [D loss: 0.627031, acc: 64.84%] [G loss: 1.992767]\n",
      "epoch:13 step:12817 [D loss: 0.619270, acc: 64.06%] [G loss: 1.872323]\n",
      "epoch:13 step:12818 [D loss: 0.629071, acc: 61.72%] [G loss: 2.169832]\n",
      "epoch:13 step:12819 [D loss: 0.668237, acc: 59.38%] [G loss: 2.058757]\n",
      "epoch:13 step:12820 [D loss: 0.606900, acc: 71.09%] [G loss: 2.047569]\n",
      "epoch:13 step:12821 [D loss: 0.687756, acc: 62.50%] [G loss: 2.051657]\n",
      "epoch:13 step:12822 [D loss: 0.640006, acc: 58.59%] [G loss: 2.029232]\n",
      "epoch:13 step:12823 [D loss: 0.610027, acc: 71.88%] [G loss: 2.165871]\n",
      "epoch:13 step:12824 [D loss: 0.639805, acc: 66.41%] [G loss: 2.017897]\n",
      "epoch:13 step:12825 [D loss: 0.639388, acc: 58.59%] [G loss: 2.113387]\n",
      "epoch:13 step:12826 [D loss: 0.670652, acc: 56.25%] [G loss: 2.004584]\n",
      "epoch:13 step:12827 [D loss: 0.645579, acc: 63.28%] [G loss: 2.010077]\n",
      "epoch:13 step:12828 [D loss: 0.601058, acc: 68.75%] [G loss: 2.092219]\n",
      "epoch:13 step:12829 [D loss: 0.582467, acc: 72.66%] [G loss: 2.350232]\n",
      "epoch:13 step:12830 [D loss: 0.568261, acc: 69.53%] [G loss: 2.013088]\n",
      "epoch:13 step:12831 [D loss: 0.583443, acc: 70.31%] [G loss: 2.241652]\n",
      "epoch:13 step:12832 [D loss: 0.598213, acc: 63.28%] [G loss: 2.071597]\n",
      "epoch:13 step:12833 [D loss: 0.661014, acc: 57.81%] [G loss: 1.947796]\n",
      "epoch:13 step:12834 [D loss: 0.643864, acc: 62.50%] [G loss: 1.845370]\n",
      "epoch:13 step:12835 [D loss: 0.674464, acc: 57.81%] [G loss: 2.020632]\n",
      "epoch:13 step:12836 [D loss: 0.635469, acc: 67.19%] [G loss: 1.894414]\n",
      "epoch:13 step:12837 [D loss: 0.656675, acc: 60.16%] [G loss: 1.827348]\n",
      "epoch:13 step:12838 [D loss: 0.665446, acc: 63.28%] [G loss: 1.948465]\n",
      "epoch:13 step:12839 [D loss: 0.681629, acc: 58.59%] [G loss: 1.783650]\n",
      "epoch:13 step:12840 [D loss: 0.666917, acc: 64.84%] [G loss: 1.862553]\n",
      "epoch:13 step:12841 [D loss: 0.640586, acc: 60.94%] [G loss: 1.956943]\n",
      "epoch:13 step:12842 [D loss: 0.664973, acc: 61.72%] [G loss: 1.786262]\n",
      "epoch:13 step:12843 [D loss: 0.645410, acc: 62.50%] [G loss: 1.835746]\n",
      "epoch:13 step:12844 [D loss: 0.652763, acc: 63.28%] [G loss: 2.056794]\n",
      "epoch:13 step:12845 [D loss: 0.655957, acc: 64.06%] [G loss: 1.852744]\n",
      "epoch:13 step:12846 [D loss: 0.629610, acc: 63.28%] [G loss: 1.975871]\n",
      "epoch:13 step:12847 [D loss: 0.650133, acc: 63.28%] [G loss: 1.790511]\n",
      "epoch:13 step:12848 [D loss: 0.672432, acc: 57.81%] [G loss: 1.752262]\n",
      "epoch:13 step:12849 [D loss: 0.681473, acc: 59.38%] [G loss: 1.913556]\n",
      "epoch:13 step:12850 [D loss: 0.678861, acc: 59.38%] [G loss: 1.800514]\n",
      "epoch:13 step:12851 [D loss: 0.647093, acc: 57.03%] [G loss: 1.760346]\n",
      "epoch:13 step:12852 [D loss: 0.635385, acc: 65.62%] [G loss: 1.675241]\n",
      "epoch:13 step:12853 [D loss: 0.665880, acc: 57.03%] [G loss: 1.807488]\n",
      "epoch:13 step:12854 [D loss: 0.684750, acc: 54.69%] [G loss: 1.817803]\n",
      "epoch:13 step:12855 [D loss: 0.641296, acc: 64.06%] [G loss: 1.912278]\n",
      "epoch:13 step:12856 [D loss: 0.695943, acc: 54.69%] [G loss: 1.734524]\n",
      "epoch:13 step:12857 [D loss: 0.607488, acc: 68.75%] [G loss: 1.902680]\n",
      "epoch:13 step:12858 [D loss: 0.641259, acc: 62.50%] [G loss: 1.894771]\n",
      "epoch:13 step:12859 [D loss: 0.654523, acc: 63.28%] [G loss: 1.859293]\n",
      "epoch:13 step:12860 [D loss: 0.637502, acc: 63.28%] [G loss: 1.938721]\n",
      "epoch:13 step:12861 [D loss: 0.654307, acc: 61.72%] [G loss: 1.811157]\n",
      "epoch:13 step:12862 [D loss: 0.592861, acc: 67.97%] [G loss: 2.118169]\n",
      "epoch:13 step:12863 [D loss: 0.674130, acc: 58.59%] [G loss: 2.002706]\n",
      "epoch:13 step:12864 [D loss: 0.588251, acc: 70.31%] [G loss: 1.870036]\n",
      "epoch:13 step:12865 [D loss: 0.621314, acc: 62.50%] [G loss: 1.949265]\n",
      "epoch:13 step:12866 [D loss: 0.629367, acc: 63.28%] [G loss: 1.990848]\n",
      "epoch:13 step:12867 [D loss: 0.642315, acc: 66.41%] [G loss: 1.890351]\n",
      "epoch:13 step:12868 [D loss: 0.612448, acc: 67.97%] [G loss: 1.939656]\n",
      "epoch:13 step:12869 [D loss: 0.680092, acc: 56.25%] [G loss: 1.906568]\n",
      "epoch:13 step:12870 [D loss: 0.670005, acc: 57.03%] [G loss: 1.825950]\n",
      "epoch:13 step:12871 [D loss: 0.660345, acc: 60.16%] [G loss: 1.870026]\n",
      "epoch:13 step:12872 [D loss: 0.590381, acc: 66.41%] [G loss: 2.083242]\n",
      "epoch:13 step:12873 [D loss: 0.671632, acc: 56.25%] [G loss: 2.015135]\n",
      "epoch:13 step:12874 [D loss: 0.653907, acc: 65.62%] [G loss: 2.123042]\n",
      "epoch:13 step:12875 [D loss: 0.610151, acc: 71.09%] [G loss: 2.157340]\n",
      "epoch:13 step:12876 [D loss: 0.624893, acc: 64.84%] [G loss: 2.112771]\n",
      "epoch:13 step:12877 [D loss: 0.635282, acc: 58.59%] [G loss: 1.990209]\n",
      "epoch:13 step:12878 [D loss: 0.690546, acc: 56.25%] [G loss: 2.048017]\n",
      "epoch:13 step:12879 [D loss: 0.663304, acc: 60.94%] [G loss: 1.929435]\n",
      "epoch:13 step:12880 [D loss: 0.609689, acc: 66.41%] [G loss: 2.162560]\n",
      "epoch:13 step:12881 [D loss: 0.616718, acc: 67.19%] [G loss: 1.928128]\n",
      "epoch:13 step:12882 [D loss: 0.637624, acc: 64.84%] [G loss: 1.966027]\n",
      "epoch:13 step:12883 [D loss: 0.659241, acc: 61.72%] [G loss: 1.856904]\n",
      "epoch:13 step:12884 [D loss: 0.672441, acc: 54.69%] [G loss: 1.866971]\n",
      "epoch:13 step:12885 [D loss: 0.685581, acc: 58.59%] [G loss: 1.775507]\n",
      "epoch:13 step:12886 [D loss: 0.650606, acc: 60.94%] [G loss: 1.890678]\n",
      "epoch:13 step:12887 [D loss: 0.615672, acc: 68.75%] [G loss: 1.976360]\n",
      "epoch:13 step:12888 [D loss: 0.604786, acc: 70.31%] [G loss: 2.117296]\n",
      "epoch:13 step:12889 [D loss: 0.562413, acc: 75.00%] [G loss: 2.152326]\n",
      "epoch:13 step:12890 [D loss: 0.580163, acc: 68.75%] [G loss: 2.095607]\n",
      "epoch:13 step:12891 [D loss: 0.640202, acc: 60.94%] [G loss: 1.826150]\n",
      "epoch:13 step:12892 [D loss: 0.672359, acc: 62.50%] [G loss: 1.887646]\n",
      "epoch:13 step:12893 [D loss: 0.614291, acc: 67.97%] [G loss: 1.856200]\n",
      "epoch:13 step:12894 [D loss: 0.661474, acc: 60.16%] [G loss: 1.955081]\n",
      "epoch:13 step:12895 [D loss: 0.660542, acc: 59.38%] [G loss: 2.045137]\n",
      "epoch:13 step:12896 [D loss: 0.616389, acc: 60.94%] [G loss: 1.865265]\n",
      "epoch:13 step:12897 [D loss: 0.689430, acc: 50.78%] [G loss: 1.762946]\n",
      "epoch:13 step:12898 [D loss: 0.611409, acc: 65.62%] [G loss: 1.983506]\n",
      "epoch:13 step:12899 [D loss: 0.630536, acc: 67.19%] [G loss: 1.916367]\n",
      "epoch:13 step:12900 [D loss: 0.634331, acc: 64.84%] [G loss: 2.069437]\n",
      "epoch:13 step:12901 [D loss: 0.677779, acc: 55.47%] [G loss: 1.939541]\n",
      "epoch:13 step:12902 [D loss: 0.684369, acc: 60.94%] [G loss: 1.907809]\n",
      "epoch:13 step:12903 [D loss: 0.655030, acc: 56.25%] [G loss: 1.766879]\n",
      "epoch:13 step:12904 [D loss: 0.665286, acc: 61.72%] [G loss: 1.996298]\n",
      "epoch:13 step:12905 [D loss: 0.615833, acc: 60.94%] [G loss: 1.876926]\n",
      "epoch:13 step:12906 [D loss: 0.652003, acc: 64.06%] [G loss: 1.855804]\n",
      "epoch:13 step:12907 [D loss: 0.628356, acc: 62.50%] [G loss: 1.873203]\n",
      "epoch:13 step:12908 [D loss: 0.675713, acc: 60.16%] [G loss: 1.846927]\n",
      "epoch:13 step:12909 [D loss: 0.664099, acc: 67.19%] [G loss: 1.890939]\n",
      "epoch:13 step:12910 [D loss: 0.633907, acc: 65.62%] [G loss: 1.920504]\n",
      "epoch:13 step:12911 [D loss: 0.606296, acc: 72.66%] [G loss: 1.950403]\n",
      "epoch:13 step:12912 [D loss: 0.628809, acc: 65.62%] [G loss: 1.846906]\n",
      "epoch:13 step:12913 [D loss: 0.635610, acc: 60.16%] [G loss: 1.896419]\n",
      "epoch:13 step:12914 [D loss: 0.626865, acc: 64.06%] [G loss: 1.948921]\n",
      "epoch:13 step:12915 [D loss: 0.682468, acc: 59.38%] [G loss: 1.886900]\n",
      "epoch:13 step:12916 [D loss: 0.602872, acc: 65.62%] [G loss: 2.028246]\n",
      "epoch:13 step:12917 [D loss: 0.642792, acc: 67.97%] [G loss: 2.050013]\n",
      "epoch:13 step:12918 [D loss: 0.641223, acc: 62.50%] [G loss: 1.884781]\n",
      "epoch:13 step:12919 [D loss: 0.693954, acc: 57.81%] [G loss: 1.856288]\n",
      "epoch:13 step:12920 [D loss: 0.630144, acc: 64.84%] [G loss: 1.977233]\n",
      "epoch:13 step:12921 [D loss: 0.589836, acc: 68.75%] [G loss: 1.924921]\n",
      "epoch:13 step:12922 [D loss: 0.698033, acc: 52.34%] [G loss: 1.948424]\n",
      "epoch:13 step:12923 [D loss: 0.651842, acc: 60.16%] [G loss: 1.784734]\n",
      "epoch:13 step:12924 [D loss: 0.642238, acc: 63.28%] [G loss: 1.734010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12925 [D loss: 0.645375, acc: 65.62%] [G loss: 1.793387]\n",
      "epoch:13 step:12926 [D loss: 0.624469, acc: 64.06%] [G loss: 2.052478]\n",
      "epoch:13 step:12927 [D loss: 0.639147, acc: 63.28%] [G loss: 2.123330]\n",
      "epoch:13 step:12928 [D loss: 0.626130, acc: 63.28%] [G loss: 1.874131]\n",
      "epoch:13 step:12929 [D loss: 0.692665, acc: 60.16%] [G loss: 1.807840]\n",
      "epoch:13 step:12930 [D loss: 0.640740, acc: 60.94%] [G loss: 1.903285]\n",
      "epoch:13 step:12931 [D loss: 0.750650, acc: 57.03%] [G loss: 1.819329]\n",
      "epoch:13 step:12932 [D loss: 0.652069, acc: 63.28%] [G loss: 1.875671]\n",
      "epoch:13 step:12933 [D loss: 0.668277, acc: 57.81%] [G loss: 1.710201]\n",
      "epoch:13 step:12934 [D loss: 0.596666, acc: 70.31%] [G loss: 1.861369]\n",
      "epoch:13 step:12935 [D loss: 0.652957, acc: 66.41%] [G loss: 1.883000]\n",
      "epoch:13 step:12936 [D loss: 0.624123, acc: 67.19%] [G loss: 1.935276]\n",
      "epoch:13 step:12937 [D loss: 0.624086, acc: 70.31%] [G loss: 1.915727]\n",
      "epoch:13 step:12938 [D loss: 0.606063, acc: 64.06%] [G loss: 2.097189]\n",
      "epoch:13 step:12939 [D loss: 0.660127, acc: 63.28%] [G loss: 1.774276]\n",
      "epoch:13 step:12940 [D loss: 0.636023, acc: 63.28%] [G loss: 1.848619]\n",
      "epoch:13 step:12941 [D loss: 0.624978, acc: 66.41%] [G loss: 2.001577]\n",
      "epoch:13 step:12942 [D loss: 0.610746, acc: 66.41%] [G loss: 1.944652]\n",
      "epoch:13 step:12943 [D loss: 0.621997, acc: 65.62%] [G loss: 1.921390]\n",
      "epoch:13 step:12944 [D loss: 0.586943, acc: 71.09%] [G loss: 1.992499]\n",
      "epoch:13 step:12945 [D loss: 0.628045, acc: 64.84%] [G loss: 1.999166]\n",
      "epoch:13 step:12946 [D loss: 0.769051, acc: 50.78%] [G loss: 1.756736]\n",
      "epoch:13 step:12947 [D loss: 0.696466, acc: 61.72%] [G loss: 1.880905]\n",
      "epoch:13 step:12948 [D loss: 0.705661, acc: 55.47%] [G loss: 1.841729]\n",
      "epoch:13 step:12949 [D loss: 0.653808, acc: 62.50%] [G loss: 1.902265]\n",
      "epoch:13 step:12950 [D loss: 0.661617, acc: 56.25%] [G loss: 1.903777]\n",
      "epoch:13 step:12951 [D loss: 0.655975, acc: 58.59%] [G loss: 1.926127]\n",
      "epoch:13 step:12952 [D loss: 0.689679, acc: 54.69%] [G loss: 1.921322]\n",
      "epoch:13 step:12953 [D loss: 0.612766, acc: 67.97%] [G loss: 1.890549]\n",
      "epoch:13 step:12954 [D loss: 0.618868, acc: 66.41%] [G loss: 1.918928]\n",
      "epoch:13 step:12955 [D loss: 0.677762, acc: 60.94%] [G loss: 2.108506]\n",
      "epoch:13 step:12956 [D loss: 0.627672, acc: 66.41%] [G loss: 2.226308]\n",
      "epoch:13 step:12957 [D loss: 0.622100, acc: 59.38%] [G loss: 1.809896]\n",
      "epoch:13 step:12958 [D loss: 0.688572, acc: 60.94%] [G loss: 1.933331]\n",
      "epoch:13 step:12959 [D loss: 0.673026, acc: 61.72%] [G loss: 1.835043]\n",
      "epoch:13 step:12960 [D loss: 0.627146, acc: 66.41%] [G loss: 1.887455]\n",
      "epoch:13 step:12961 [D loss: 0.646933, acc: 61.72%] [G loss: 2.068322]\n",
      "epoch:13 step:12962 [D loss: 0.586855, acc: 68.75%] [G loss: 2.246499]\n",
      "epoch:13 step:12963 [D loss: 0.560061, acc: 75.78%] [G loss: 2.054414]\n",
      "epoch:13 step:12964 [D loss: 0.616412, acc: 68.75%] [G loss: 2.026409]\n",
      "epoch:13 step:12965 [D loss: 0.697015, acc: 58.59%] [G loss: 1.840317]\n",
      "epoch:13 step:12966 [D loss: 0.677896, acc: 56.25%] [G loss: 1.876952]\n",
      "epoch:13 step:12967 [D loss: 0.579883, acc: 73.44%] [G loss: 2.146635]\n",
      "epoch:13 step:12968 [D loss: 0.683553, acc: 59.38%] [G loss: 1.976043]\n",
      "epoch:13 step:12969 [D loss: 0.665088, acc: 60.16%] [G loss: 1.941133]\n",
      "epoch:13 step:12970 [D loss: 0.639528, acc: 67.97%] [G loss: 1.833231]\n",
      "epoch:13 step:12971 [D loss: 0.600767, acc: 69.53%] [G loss: 1.996473]\n",
      "epoch:13 step:12972 [D loss: 0.661540, acc: 58.59%] [G loss: 1.945041]\n",
      "epoch:13 step:12973 [D loss: 0.611047, acc: 67.97%] [G loss: 2.270232]\n",
      "epoch:13 step:12974 [D loss: 0.643526, acc: 60.94%] [G loss: 1.961663]\n",
      "epoch:13 step:12975 [D loss: 0.734040, acc: 47.66%] [G loss: 1.779117]\n",
      "epoch:13 step:12976 [D loss: 0.690683, acc: 53.91%] [G loss: 1.850178]\n",
      "epoch:13 step:12977 [D loss: 0.706602, acc: 50.00%] [G loss: 1.738492]\n",
      "epoch:13 step:12978 [D loss: 0.662251, acc: 60.94%] [G loss: 1.761838]\n",
      "epoch:13 step:12979 [D loss: 0.600938, acc: 67.19%] [G loss: 1.915149]\n",
      "epoch:13 step:12980 [D loss: 0.650170, acc: 59.38%] [G loss: 1.764903]\n",
      "epoch:13 step:12981 [D loss: 0.679655, acc: 58.59%] [G loss: 1.752588]\n",
      "epoch:13 step:12982 [D loss: 0.715280, acc: 50.78%] [G loss: 1.764924]\n",
      "epoch:13 step:12983 [D loss: 0.660571, acc: 60.16%] [G loss: 1.940878]\n",
      "epoch:13 step:12984 [D loss: 0.624554, acc: 60.16%] [G loss: 1.860995]\n",
      "epoch:13 step:12985 [D loss: 0.627658, acc: 63.28%] [G loss: 2.083757]\n",
      "epoch:13 step:12986 [D loss: 0.577226, acc: 70.31%] [G loss: 2.165804]\n",
      "epoch:13 step:12987 [D loss: 0.627975, acc: 63.28%] [G loss: 1.966660]\n",
      "epoch:13 step:12988 [D loss: 0.618464, acc: 64.06%] [G loss: 1.989959]\n",
      "epoch:13 step:12989 [D loss: 0.616812, acc: 64.06%] [G loss: 1.921611]\n",
      "epoch:13 step:12990 [D loss: 0.655717, acc: 60.16%] [G loss: 1.958860]\n",
      "epoch:13 step:12991 [D loss: 0.618502, acc: 64.84%] [G loss: 1.952121]\n",
      "epoch:13 step:12992 [D loss: 0.645521, acc: 65.62%] [G loss: 1.848611]\n",
      "epoch:13 step:12993 [D loss: 0.688438, acc: 56.25%] [G loss: 1.812545]\n",
      "epoch:13 step:12994 [D loss: 0.714404, acc: 53.12%] [G loss: 1.798867]\n",
      "epoch:13 step:12995 [D loss: 0.629396, acc: 66.41%] [G loss: 1.839379]\n",
      "epoch:13 step:12996 [D loss: 0.611307, acc: 67.97%] [G loss: 2.002183]\n",
      "epoch:13 step:12997 [D loss: 0.597772, acc: 67.19%] [G loss: 2.184440]\n",
      "epoch:13 step:12998 [D loss: 0.648723, acc: 63.28%] [G loss: 1.792824]\n",
      "epoch:13 step:12999 [D loss: 0.654336, acc: 61.72%] [G loss: 1.879450]\n",
      "epoch:13 step:13000 [D loss: 0.651817, acc: 67.97%] [G loss: 1.830129]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.862778\n",
      "FID: 21.561749\n",
      "0 = 12.816949651527391\n",
      "1 = 0.08487109114307655\n",
      "2 = 0.8896999955177307\n",
      "3 = 0.9093999862670898\n",
      "4 = 0.8700000047683716\n",
      "5 = 0.874927818775177\n",
      "6 = 0.9093999862670898\n",
      "7 = 7.104820276856449\n",
      "8 = 0.0923535935888761\n",
      "9 = 0.7454000115394592\n",
      "10 = 0.7552000284194946\n",
      "11 = 0.7355999946594238\n",
      "12 = 0.7406826019287109\n",
      "13 = 0.7552000284194946\n",
      "14 = 6.862802982330322\n",
      "15 = 9.2819242477417\n",
      "16 = 0.15633445978164673\n",
      "17 = 6.8627777099609375\n",
      "18 = 21.561748504638672\n",
      "epoch:13 step:13001 [D loss: 0.708652, acc: 53.91%] [G loss: 1.845206]\n",
      "epoch:13 step:13002 [D loss: 0.689172, acc: 58.59%] [G loss: 1.861964]\n",
      "epoch:13 step:13003 [D loss: 0.631190, acc: 65.62%] [G loss: 1.923902]\n",
      "epoch:13 step:13004 [D loss: 0.604488, acc: 69.53%] [G loss: 1.945954]\n",
      "epoch:13 step:13005 [D loss: 0.621463, acc: 66.41%] [G loss: 1.915950]\n",
      "epoch:13 step:13006 [D loss: 0.636057, acc: 63.28%] [G loss: 1.981409]\n",
      "epoch:13 step:13007 [D loss: 0.649801, acc: 67.19%] [G loss: 1.933853]\n",
      "epoch:13 step:13008 [D loss: 0.672948, acc: 57.81%] [G loss: 1.868275]\n",
      "epoch:13 step:13009 [D loss: 0.682329, acc: 54.69%] [G loss: 1.805095]\n",
      "epoch:13 step:13010 [D loss: 0.678897, acc: 57.03%] [G loss: 1.801161]\n",
      "epoch:13 step:13011 [D loss: 0.665152, acc: 60.16%] [G loss: 1.673845]\n",
      "epoch:13 step:13012 [D loss: 0.611605, acc: 67.19%] [G loss: 2.000297]\n",
      "epoch:13 step:13013 [D loss: 0.624380, acc: 65.62%] [G loss: 1.933979]\n",
      "epoch:13 step:13014 [D loss: 0.599101, acc: 68.75%] [G loss: 1.978282]\n",
      "epoch:13 step:13015 [D loss: 0.719979, acc: 52.34%] [G loss: 1.890007]\n",
      "epoch:13 step:13016 [D loss: 0.604440, acc: 67.97%] [G loss: 1.858618]\n",
      "epoch:13 step:13017 [D loss: 0.683199, acc: 60.16%] [G loss: 1.899516]\n",
      "epoch:13 step:13018 [D loss: 0.584559, acc: 67.97%] [G loss: 1.974042]\n",
      "epoch:13 step:13019 [D loss: 0.599902, acc: 68.75%] [G loss: 1.945253]\n",
      "epoch:13 step:13020 [D loss: 0.659832, acc: 59.38%] [G loss: 1.879745]\n",
      "epoch:13 step:13021 [D loss: 0.630076, acc: 64.84%] [G loss: 1.891247]\n",
      "epoch:13 step:13022 [D loss: 0.683088, acc: 57.81%] [G loss: 2.055487]\n",
      "epoch:13 step:13023 [D loss: 0.641821, acc: 62.50%] [G loss: 1.977624]\n",
      "epoch:13 step:13024 [D loss: 0.634949, acc: 69.53%] [G loss: 1.959098]\n",
      "epoch:13 step:13025 [D loss: 0.650746, acc: 59.38%] [G loss: 2.040792]\n",
      "epoch:13 step:13026 [D loss: 0.715886, acc: 55.47%] [G loss: 1.932104]\n",
      "epoch:13 step:13027 [D loss: 0.587901, acc: 67.97%] [G loss: 1.919518]\n",
      "epoch:13 step:13028 [D loss: 0.591798, acc: 64.84%] [G loss: 1.769981]\n",
      "epoch:13 step:13029 [D loss: 0.624186, acc: 63.28%] [G loss: 1.969269]\n",
      "epoch:13 step:13030 [D loss: 0.599970, acc: 63.28%] [G loss: 2.113837]\n",
      "epoch:13 step:13031 [D loss: 0.699329, acc: 55.47%] [G loss: 1.985945]\n",
      "epoch:13 step:13032 [D loss: 0.673193, acc: 59.38%] [G loss: 1.933971]\n",
      "epoch:13 step:13033 [D loss: 0.651978, acc: 58.59%] [G loss: 2.024915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:13034 [D loss: 0.573454, acc: 71.09%] [G loss: 2.011971]\n",
      "epoch:13 step:13035 [D loss: 0.697721, acc: 61.72%] [G loss: 1.847712]\n",
      "epoch:13 step:13036 [D loss: 0.632021, acc: 62.50%] [G loss: 1.839923]\n",
      "epoch:13 step:13037 [D loss: 0.737231, acc: 50.00%] [G loss: 1.790178]\n",
      "epoch:13 step:13038 [D loss: 0.556335, acc: 70.31%] [G loss: 1.951392]\n",
      "epoch:13 step:13039 [D loss: 0.643910, acc: 64.84%] [G loss: 1.897893]\n",
      "epoch:13 step:13040 [D loss: 0.688853, acc: 57.03%] [G loss: 1.815241]\n",
      "epoch:13 step:13041 [D loss: 0.639153, acc: 60.16%] [G loss: 1.909837]\n",
      "epoch:13 step:13042 [D loss: 0.638304, acc: 63.28%] [G loss: 2.003358]\n",
      "epoch:13 step:13043 [D loss: 0.615620, acc: 66.41%] [G loss: 1.951661]\n",
      "epoch:13 step:13044 [D loss: 0.681170, acc: 56.25%] [G loss: 1.894465]\n",
      "epoch:13 step:13045 [D loss: 0.637354, acc: 64.84%] [G loss: 1.839505]\n",
      "epoch:13 step:13046 [D loss: 0.709382, acc: 57.03%] [G loss: 1.766028]\n",
      "epoch:13 step:13047 [D loss: 0.626995, acc: 66.41%] [G loss: 1.831640]\n",
      "epoch:13 step:13048 [D loss: 0.677754, acc: 60.16%] [G loss: 1.787950]\n",
      "epoch:13 step:13049 [D loss: 0.633980, acc: 60.16%] [G loss: 1.998291]\n",
      "epoch:13 step:13050 [D loss: 0.646101, acc: 63.28%] [G loss: 1.827131]\n",
      "epoch:13 step:13051 [D loss: 0.554669, acc: 75.00%] [G loss: 1.886232]\n",
      "epoch:13 step:13052 [D loss: 0.658152, acc: 64.84%] [G loss: 1.996935]\n",
      "epoch:13 step:13053 [D loss: 0.616885, acc: 72.66%] [G loss: 1.897285]\n",
      "epoch:13 step:13054 [D loss: 0.659841, acc: 60.94%] [G loss: 1.914693]\n",
      "epoch:13 step:13055 [D loss: 0.721889, acc: 54.69%] [G loss: 1.833551]\n",
      "epoch:13 step:13056 [D loss: 0.558359, acc: 73.44%] [G loss: 1.942605]\n",
      "epoch:13 step:13057 [D loss: 0.652946, acc: 63.28%] [G loss: 1.843363]\n",
      "epoch:13 step:13058 [D loss: 0.672624, acc: 59.38%] [G loss: 2.032175]\n",
      "epoch:13 step:13059 [D loss: 0.624422, acc: 59.38%] [G loss: 1.892839]\n",
      "epoch:13 step:13060 [D loss: 0.656247, acc: 59.38%] [G loss: 1.940753]\n",
      "epoch:13 step:13061 [D loss: 0.619022, acc: 62.50%] [G loss: 2.011068]\n",
      "epoch:13 step:13062 [D loss: 0.633643, acc: 64.06%] [G loss: 1.837156]\n",
      "epoch:13 step:13063 [D loss: 0.628269, acc: 68.75%] [G loss: 1.989255]\n",
      "epoch:13 step:13064 [D loss: 0.599520, acc: 65.62%] [G loss: 1.994001]\n",
      "epoch:13 step:13065 [D loss: 0.608709, acc: 63.28%] [G loss: 2.145246]\n",
      "epoch:13 step:13066 [D loss: 0.701008, acc: 60.16%] [G loss: 1.879674]\n",
      "epoch:13 step:13067 [D loss: 0.582814, acc: 67.97%] [G loss: 1.972941]\n",
      "epoch:13 step:13068 [D loss: 0.695587, acc: 53.91%] [G loss: 1.922656]\n",
      "epoch:13 step:13069 [D loss: 0.607682, acc: 69.53%] [G loss: 1.983155]\n",
      "epoch:13 step:13070 [D loss: 0.607067, acc: 65.62%] [G loss: 1.833989]\n",
      "epoch:13 step:13071 [D loss: 0.597579, acc: 73.44%] [G loss: 2.176243]\n",
      "epoch:13 step:13072 [D loss: 0.657919, acc: 61.72%] [G loss: 1.883237]\n",
      "epoch:13 step:13073 [D loss: 0.667665, acc: 60.16%] [G loss: 1.897810]\n",
      "epoch:13 step:13074 [D loss: 0.614928, acc: 65.62%] [G loss: 2.131573]\n",
      "epoch:13 step:13075 [D loss: 0.609242, acc: 65.62%] [G loss: 2.134552]\n",
      "epoch:13 step:13076 [D loss: 0.598057, acc: 68.75%] [G loss: 2.029511]\n",
      "epoch:13 step:13077 [D loss: 0.702990, acc: 55.47%] [G loss: 1.768615]\n",
      "epoch:13 step:13078 [D loss: 0.642127, acc: 63.28%] [G loss: 1.908815]\n",
      "epoch:13 step:13079 [D loss: 0.620417, acc: 66.41%] [G loss: 1.845572]\n",
      "epoch:13 step:13080 [D loss: 0.652312, acc: 60.16%] [G loss: 2.158866]\n",
      "epoch:13 step:13081 [D loss: 0.607565, acc: 69.53%] [G loss: 1.980024]\n",
      "epoch:13 step:13082 [D loss: 0.638971, acc: 64.06%] [G loss: 1.898289]\n",
      "epoch:13 step:13083 [D loss: 0.631883, acc: 64.06%] [G loss: 1.973354]\n",
      "epoch:13 step:13084 [D loss: 0.610692, acc: 64.06%] [G loss: 1.964072]\n",
      "epoch:13 step:13085 [D loss: 0.631066, acc: 62.50%] [G loss: 1.960765]\n",
      "epoch:13 step:13086 [D loss: 0.620704, acc: 69.53%] [G loss: 2.088026]\n",
      "epoch:13 step:13087 [D loss: 0.643921, acc: 65.62%] [G loss: 2.014347]\n",
      "epoch:13 step:13088 [D loss: 0.638545, acc: 65.62%] [G loss: 2.039488]\n",
      "epoch:13 step:13089 [D loss: 0.629942, acc: 62.50%] [G loss: 2.055340]\n",
      "epoch:13 step:13090 [D loss: 0.646336, acc: 63.28%] [G loss: 2.075495]\n",
      "epoch:13 step:13091 [D loss: 0.636014, acc: 63.28%] [G loss: 2.123377]\n",
      "epoch:13 step:13092 [D loss: 0.639624, acc: 64.06%] [G loss: 2.055245]\n",
      "epoch:13 step:13093 [D loss: 0.594119, acc: 65.62%] [G loss: 1.984343]\n",
      "epoch:13 step:13094 [D loss: 0.643201, acc: 61.72%] [G loss: 2.140486]\n",
      "epoch:13 step:13095 [D loss: 0.705940, acc: 56.25%] [G loss: 2.066271]\n",
      "epoch:13 step:13096 [D loss: 0.666532, acc: 57.81%] [G loss: 2.003151]\n",
      "epoch:13 step:13097 [D loss: 0.632546, acc: 66.41%] [G loss: 2.048707]\n",
      "epoch:13 step:13098 [D loss: 0.616638, acc: 64.84%] [G loss: 2.148800]\n",
      "epoch:13 step:13099 [D loss: 0.568252, acc: 73.44%] [G loss: 2.090925]\n",
      "epoch:13 step:13100 [D loss: 0.597156, acc: 68.75%] [G loss: 2.078362]\n",
      "epoch:13 step:13101 [D loss: 0.726579, acc: 53.12%] [G loss: 1.830964]\n",
      "epoch:13 step:13102 [D loss: 0.623058, acc: 67.19%] [G loss: 2.036148]\n",
      "epoch:13 step:13103 [D loss: 0.664112, acc: 57.03%] [G loss: 1.981321]\n",
      "epoch:13 step:13104 [D loss: 0.627231, acc: 64.06%] [G loss: 2.095594]\n",
      "epoch:13 step:13105 [D loss: 0.564439, acc: 75.00%] [G loss: 2.144857]\n",
      "epoch:13 step:13106 [D loss: 0.588972, acc: 69.53%] [G loss: 2.342179]\n",
      "epoch:13 step:13107 [D loss: 0.587350, acc: 67.19%] [G loss: 2.306391]\n",
      "epoch:13 step:13108 [D loss: 0.661643, acc: 62.50%] [G loss: 2.076896]\n",
      "epoch:13 step:13109 [D loss: 0.753570, acc: 54.69%] [G loss: 1.849356]\n",
      "epoch:13 step:13110 [D loss: 0.664590, acc: 60.16%] [G loss: 1.976566]\n",
      "epoch:13 step:13111 [D loss: 0.612371, acc: 65.62%] [G loss: 1.963716]\n",
      "epoch:13 step:13112 [D loss: 0.672270, acc: 64.84%] [G loss: 2.048443]\n",
      "epoch:13 step:13113 [D loss: 0.586473, acc: 69.53%] [G loss: 2.008651]\n",
      "epoch:13 step:13114 [D loss: 0.621166, acc: 67.97%] [G loss: 2.176724]\n",
      "epoch:13 step:13115 [D loss: 0.664801, acc: 64.84%] [G loss: 1.954762]\n",
      "epoch:13 step:13116 [D loss: 0.594417, acc: 72.66%] [G loss: 1.949621]\n",
      "epoch:13 step:13117 [D loss: 0.631721, acc: 69.53%] [G loss: 2.111957]\n",
      "epoch:13 step:13118 [D loss: 0.581408, acc: 71.88%] [G loss: 2.376678]\n",
      "epoch:14 step:13119 [D loss: 0.707880, acc: 60.94%] [G loss: 1.892354]\n",
      "epoch:14 step:13120 [D loss: 0.630577, acc: 64.84%] [G loss: 2.073461]\n",
      "epoch:14 step:13121 [D loss: 0.616380, acc: 60.94%] [G loss: 1.956349]\n",
      "epoch:14 step:13122 [D loss: 0.576478, acc: 76.56%] [G loss: 2.048502]\n",
      "epoch:14 step:13123 [D loss: 0.663286, acc: 64.84%] [G loss: 1.817027]\n",
      "epoch:14 step:13124 [D loss: 0.640194, acc: 60.16%] [G loss: 1.931661]\n",
      "epoch:14 step:13125 [D loss: 0.588886, acc: 69.53%] [G loss: 2.007093]\n",
      "epoch:14 step:13126 [D loss: 0.684167, acc: 63.28%] [G loss: 1.927541]\n",
      "epoch:14 step:13127 [D loss: 0.610953, acc: 68.75%] [G loss: 2.176093]\n",
      "epoch:14 step:13128 [D loss: 0.654328, acc: 59.38%] [G loss: 2.000876]\n",
      "epoch:14 step:13129 [D loss: 0.635763, acc: 60.16%] [G loss: 2.079698]\n",
      "epoch:14 step:13130 [D loss: 0.627971, acc: 64.84%] [G loss: 2.036874]\n",
      "epoch:14 step:13131 [D loss: 0.644314, acc: 71.09%] [G loss: 1.999003]\n",
      "epoch:14 step:13132 [D loss: 0.649818, acc: 60.94%] [G loss: 1.965095]\n",
      "epoch:14 step:13133 [D loss: 0.623308, acc: 65.62%] [G loss: 2.330253]\n",
      "epoch:14 step:13134 [D loss: 0.603745, acc: 67.97%] [G loss: 2.010847]\n",
      "epoch:14 step:13135 [D loss: 0.605739, acc: 60.94%] [G loss: 1.971906]\n",
      "epoch:14 step:13136 [D loss: 0.695029, acc: 57.81%] [G loss: 1.855714]\n",
      "epoch:14 step:13137 [D loss: 0.672077, acc: 58.59%] [G loss: 1.868319]\n",
      "epoch:14 step:13138 [D loss: 0.674135, acc: 58.59%] [G loss: 1.794633]\n",
      "epoch:14 step:13139 [D loss: 0.684616, acc: 53.12%] [G loss: 1.978203]\n",
      "epoch:14 step:13140 [D loss: 0.691698, acc: 58.59%] [G loss: 1.889938]\n",
      "epoch:14 step:13141 [D loss: 0.572266, acc: 75.78%] [G loss: 2.178836]\n",
      "epoch:14 step:13142 [D loss: 0.598054, acc: 67.19%] [G loss: 1.957622]\n",
      "epoch:14 step:13143 [D loss: 0.638322, acc: 67.97%] [G loss: 2.152007]\n",
      "epoch:14 step:13144 [D loss: 0.668397, acc: 60.16%] [G loss: 1.934395]\n",
      "epoch:14 step:13145 [D loss: 0.666293, acc: 57.03%] [G loss: 1.960384]\n",
      "epoch:14 step:13146 [D loss: 0.646155, acc: 66.41%] [G loss: 1.968029]\n",
      "epoch:14 step:13147 [D loss: 0.592963, acc: 67.97%] [G loss: 1.854900]\n",
      "epoch:14 step:13148 [D loss: 0.642112, acc: 64.84%] [G loss: 1.868592]\n",
      "epoch:14 step:13149 [D loss: 0.684008, acc: 60.94%] [G loss: 1.842558]\n",
      "epoch:14 step:13150 [D loss: 0.632837, acc: 60.94%] [G loss: 1.847754]\n",
      "epoch:14 step:13151 [D loss: 0.620057, acc: 67.97%] [G loss: 1.913272]\n",
      "epoch:14 step:13152 [D loss: 0.666144, acc: 59.38%] [G loss: 1.853970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13153 [D loss: 0.651239, acc: 60.16%] [G loss: 1.921889]\n",
      "epoch:14 step:13154 [D loss: 0.619277, acc: 66.41%] [G loss: 1.922144]\n",
      "epoch:14 step:13155 [D loss: 0.632913, acc: 65.62%] [G loss: 2.104215]\n",
      "epoch:14 step:13156 [D loss: 0.660525, acc: 62.50%] [G loss: 2.056915]\n",
      "epoch:14 step:13157 [D loss: 0.586011, acc: 67.19%] [G loss: 1.919215]\n",
      "epoch:14 step:13158 [D loss: 0.583301, acc: 66.41%] [G loss: 2.254651]\n",
      "epoch:14 step:13159 [D loss: 0.642787, acc: 59.38%] [G loss: 1.904687]\n",
      "epoch:14 step:13160 [D loss: 0.659563, acc: 60.94%] [G loss: 2.038281]\n",
      "epoch:14 step:13161 [D loss: 0.639076, acc: 65.62%] [G loss: 2.102124]\n",
      "epoch:14 step:13162 [D loss: 0.642817, acc: 60.16%] [G loss: 1.888512]\n",
      "epoch:14 step:13163 [D loss: 0.621050, acc: 68.75%] [G loss: 1.948790]\n",
      "epoch:14 step:13164 [D loss: 0.653908, acc: 63.28%] [G loss: 1.953825]\n",
      "epoch:14 step:13165 [D loss: 0.592025, acc: 70.31%] [G loss: 1.891152]\n",
      "epoch:14 step:13166 [D loss: 0.680638, acc: 57.81%] [G loss: 1.958300]\n",
      "epoch:14 step:13167 [D loss: 0.585318, acc: 68.75%] [G loss: 1.892985]\n",
      "epoch:14 step:13168 [D loss: 0.663485, acc: 67.19%] [G loss: 1.984571]\n",
      "epoch:14 step:13169 [D loss: 0.586208, acc: 71.09%] [G loss: 1.980649]\n",
      "epoch:14 step:13170 [D loss: 0.678448, acc: 56.25%] [G loss: 1.987384]\n",
      "epoch:14 step:13171 [D loss: 0.619426, acc: 71.09%] [G loss: 1.905307]\n",
      "epoch:14 step:13172 [D loss: 0.597300, acc: 67.97%] [G loss: 1.989527]\n",
      "epoch:14 step:13173 [D loss: 0.612061, acc: 70.31%] [G loss: 2.153610]\n",
      "epoch:14 step:13174 [D loss: 0.619613, acc: 65.62%] [G loss: 2.209481]\n",
      "epoch:14 step:13175 [D loss: 0.579485, acc: 70.31%] [G loss: 2.142125]\n",
      "epoch:14 step:13176 [D loss: 0.630650, acc: 60.94%] [G loss: 1.977767]\n",
      "epoch:14 step:13177 [D loss: 0.633186, acc: 70.31%] [G loss: 2.044670]\n",
      "epoch:14 step:13178 [D loss: 0.657336, acc: 58.59%] [G loss: 1.879280]\n",
      "epoch:14 step:13179 [D loss: 0.722562, acc: 54.69%] [G loss: 1.826804]\n",
      "epoch:14 step:13180 [D loss: 0.624397, acc: 64.06%] [G loss: 1.967430]\n",
      "epoch:14 step:13181 [D loss: 0.610029, acc: 67.19%] [G loss: 2.067695]\n",
      "epoch:14 step:13182 [D loss: 0.602065, acc: 67.97%] [G loss: 2.094897]\n",
      "epoch:14 step:13183 [D loss: 0.663698, acc: 62.50%] [G loss: 2.030889]\n",
      "epoch:14 step:13184 [D loss: 0.620218, acc: 67.19%] [G loss: 1.931712]\n",
      "epoch:14 step:13185 [D loss: 0.700067, acc: 57.03%] [G loss: 1.900565]\n",
      "epoch:14 step:13186 [D loss: 0.587764, acc: 72.66%] [G loss: 2.101378]\n",
      "epoch:14 step:13187 [D loss: 0.636199, acc: 68.75%] [G loss: 2.131721]\n",
      "epoch:14 step:13188 [D loss: 0.635320, acc: 61.72%] [G loss: 2.119886]\n",
      "epoch:14 step:13189 [D loss: 0.692856, acc: 56.25%] [G loss: 1.872833]\n",
      "epoch:14 step:13190 [D loss: 0.680908, acc: 53.12%] [G loss: 1.996970]\n",
      "epoch:14 step:13191 [D loss: 0.618106, acc: 67.19%] [G loss: 1.872653]\n",
      "epoch:14 step:13192 [D loss: 0.674153, acc: 57.81%] [G loss: 2.006722]\n",
      "epoch:14 step:13193 [D loss: 0.662341, acc: 60.94%] [G loss: 2.003496]\n",
      "epoch:14 step:13194 [D loss: 0.579816, acc: 66.41%] [G loss: 2.142444]\n",
      "epoch:14 step:13195 [D loss: 0.650443, acc: 61.72%] [G loss: 2.018232]\n",
      "epoch:14 step:13196 [D loss: 0.698922, acc: 60.16%] [G loss: 1.959362]\n",
      "epoch:14 step:13197 [D loss: 0.604952, acc: 61.72%] [G loss: 1.935826]\n",
      "epoch:14 step:13198 [D loss: 0.717622, acc: 58.59%] [G loss: 1.908436]\n",
      "epoch:14 step:13199 [D loss: 0.663779, acc: 58.59%] [G loss: 1.877652]\n",
      "epoch:14 step:13200 [D loss: 0.670322, acc: 56.25%] [G loss: 1.915625]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.168433\n",
      "FID: 16.168705\n",
      "0 = 12.796737357473326\n",
      "1 = 0.08987824151105284\n",
      "2 = 0.890999972820282\n",
      "3 = 0.9067999720573425\n",
      "4 = 0.8751999735832214\n",
      "5 = 0.8790228962898254\n",
      "6 = 0.9067999720573425\n",
      "7 = 6.696773004269594\n",
      "8 = 0.07680075805409153\n",
      "9 = 0.7336999773979187\n",
      "10 = 0.7447999715805054\n",
      "11 = 0.722599983215332\n",
      "12 = 0.7286245226860046\n",
      "13 = 0.7447999715805054\n",
      "14 = 7.168463706970215\n",
      "15 = 9.409153938293457\n",
      "16 = 0.12598949670791626\n",
      "17 = 7.168432712554932\n",
      "18 = 16.168704986572266\n",
      "epoch:14 step:13201 [D loss: 0.615288, acc: 71.88%] [G loss: 1.954700]\n",
      "epoch:14 step:13202 [D loss: 0.658576, acc: 61.72%] [G loss: 2.016019]\n",
      "epoch:14 step:13203 [D loss: 0.646788, acc: 64.84%] [G loss: 1.769280]\n",
      "epoch:14 step:13204 [D loss: 0.633334, acc: 69.53%] [G loss: 1.875121]\n",
      "epoch:14 step:13205 [D loss: 0.679290, acc: 57.81%] [G loss: 1.782400]\n",
      "epoch:14 step:13206 [D loss: 0.604914, acc: 67.97%] [G loss: 1.880612]\n",
      "epoch:14 step:13207 [D loss: 0.656792, acc: 63.28%] [G loss: 1.955832]\n",
      "epoch:14 step:13208 [D loss: 0.666802, acc: 57.81%] [G loss: 1.926837]\n",
      "epoch:14 step:13209 [D loss: 0.712417, acc: 57.03%] [G loss: 1.940655]\n",
      "epoch:14 step:13210 [D loss: 0.627835, acc: 60.94%] [G loss: 2.042379]\n",
      "epoch:14 step:13211 [D loss: 0.631881, acc: 64.06%] [G loss: 2.050752]\n",
      "epoch:14 step:13212 [D loss: 0.653906, acc: 62.50%] [G loss: 1.850717]\n",
      "epoch:14 step:13213 [D loss: 0.637325, acc: 65.62%] [G loss: 1.846302]\n",
      "epoch:14 step:13214 [D loss: 0.631036, acc: 61.72%] [G loss: 1.912174]\n",
      "epoch:14 step:13215 [D loss: 0.662362, acc: 60.94%] [G loss: 1.943214]\n",
      "epoch:14 step:13216 [D loss: 0.697697, acc: 60.16%] [G loss: 1.889627]\n",
      "epoch:14 step:13217 [D loss: 0.625566, acc: 62.50%] [G loss: 1.894096]\n",
      "epoch:14 step:13218 [D loss: 0.605846, acc: 69.53%] [G loss: 2.053613]\n",
      "epoch:14 step:13219 [D loss: 0.631881, acc: 60.94%] [G loss: 1.833663]\n",
      "epoch:14 step:13220 [D loss: 0.609585, acc: 66.41%] [G loss: 2.045252]\n",
      "epoch:14 step:13221 [D loss: 0.669212, acc: 54.69%] [G loss: 1.860329]\n",
      "epoch:14 step:13222 [D loss: 0.611919, acc: 67.97%] [G loss: 1.982952]\n",
      "epoch:14 step:13223 [D loss: 0.699971, acc: 55.47%] [G loss: 1.831763]\n",
      "epoch:14 step:13224 [D loss: 0.643975, acc: 65.62%] [G loss: 2.024639]\n",
      "epoch:14 step:13225 [D loss: 0.615586, acc: 65.62%] [G loss: 1.997495]\n",
      "epoch:14 step:13226 [D loss: 0.662721, acc: 56.25%] [G loss: 1.816164]\n",
      "epoch:14 step:13227 [D loss: 0.675632, acc: 59.38%] [G loss: 1.877589]\n",
      "epoch:14 step:13228 [D loss: 0.662061, acc: 59.38%] [G loss: 1.889672]\n",
      "epoch:14 step:13229 [D loss: 0.598590, acc: 68.75%] [G loss: 2.062656]\n",
      "epoch:14 step:13230 [D loss: 0.612446, acc: 67.19%] [G loss: 1.907106]\n",
      "epoch:14 step:13231 [D loss: 0.614323, acc: 61.72%] [G loss: 1.890960]\n",
      "epoch:14 step:13232 [D loss: 0.710715, acc: 61.72%] [G loss: 2.059666]\n",
      "epoch:14 step:13233 [D loss: 0.586700, acc: 68.75%] [G loss: 2.093737]\n",
      "epoch:14 step:13234 [D loss: 0.522563, acc: 77.34%] [G loss: 2.095722]\n",
      "epoch:14 step:13235 [D loss: 0.600738, acc: 64.06%] [G loss: 2.151219]\n",
      "epoch:14 step:13236 [D loss: 0.644709, acc: 65.62%] [G loss: 1.988796]\n",
      "epoch:14 step:13237 [D loss: 0.581611, acc: 68.75%] [G loss: 2.338911]\n",
      "epoch:14 step:13238 [D loss: 0.646374, acc: 60.16%] [G loss: 2.124924]\n",
      "epoch:14 step:13239 [D loss: 0.615353, acc: 68.75%] [G loss: 2.260930]\n",
      "epoch:14 step:13240 [D loss: 0.659128, acc: 61.72%] [G loss: 2.066979]\n",
      "epoch:14 step:13241 [D loss: 0.614468, acc: 68.75%] [G loss: 2.066039]\n",
      "epoch:14 step:13242 [D loss: 0.631477, acc: 62.50%] [G loss: 2.033536]\n",
      "epoch:14 step:13243 [D loss: 0.678052, acc: 56.25%] [G loss: 1.915934]\n",
      "epoch:14 step:13244 [D loss: 0.606839, acc: 67.97%] [G loss: 2.227746]\n",
      "epoch:14 step:13245 [D loss: 0.644093, acc: 58.59%] [G loss: 1.913847]\n",
      "epoch:14 step:13246 [D loss: 0.605394, acc: 62.50%] [G loss: 1.962654]\n",
      "epoch:14 step:13247 [D loss: 0.636160, acc: 61.72%] [G loss: 1.891257]\n",
      "epoch:14 step:13248 [D loss: 0.668540, acc: 58.59%] [G loss: 1.986906]\n",
      "epoch:14 step:13249 [D loss: 0.595561, acc: 68.75%] [G loss: 2.127228]\n",
      "epoch:14 step:13250 [D loss: 0.644382, acc: 59.38%] [G loss: 2.004823]\n",
      "epoch:14 step:13251 [D loss: 0.637337, acc: 62.50%] [G loss: 1.922576]\n",
      "epoch:14 step:13252 [D loss: 0.755403, acc: 46.88%] [G loss: 1.838033]\n",
      "epoch:14 step:13253 [D loss: 0.643946, acc: 67.97%] [G loss: 1.873631]\n",
      "epoch:14 step:13254 [D loss: 0.624749, acc: 59.38%] [G loss: 1.866785]\n",
      "epoch:14 step:13255 [D loss: 0.643658, acc: 64.84%] [G loss: 1.987837]\n",
      "epoch:14 step:13256 [D loss: 0.618234, acc: 66.41%] [G loss: 1.842812]\n",
      "epoch:14 step:13257 [D loss: 0.642806, acc: 67.19%] [G loss: 1.835915]\n",
      "epoch:14 step:13258 [D loss: 0.671835, acc: 59.38%] [G loss: 1.905715]\n",
      "epoch:14 step:13259 [D loss: 0.671281, acc: 62.50%] [G loss: 1.913641]\n",
      "epoch:14 step:13260 [D loss: 0.652897, acc: 60.16%] [G loss: 1.806260]\n",
      "epoch:14 step:13261 [D loss: 0.690676, acc: 55.47%] [G loss: 1.812137]\n",
      "epoch:14 step:13262 [D loss: 0.620667, acc: 64.06%] [G loss: 1.888483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13263 [D loss: 0.644820, acc: 67.19%] [G loss: 1.900589]\n",
      "epoch:14 step:13264 [D loss: 0.657274, acc: 62.50%] [G loss: 2.030475]\n",
      "epoch:14 step:13265 [D loss: 0.659493, acc: 57.81%] [G loss: 1.926441]\n",
      "epoch:14 step:13266 [D loss: 0.650817, acc: 60.16%] [G loss: 1.872534]\n",
      "epoch:14 step:13267 [D loss: 0.671139, acc: 63.28%] [G loss: 1.983984]\n",
      "epoch:14 step:13268 [D loss: 0.625554, acc: 64.84%] [G loss: 1.903483]\n",
      "epoch:14 step:13269 [D loss: 0.649673, acc: 63.28%] [G loss: 1.906826]\n",
      "epoch:14 step:13270 [D loss: 0.623065, acc: 60.94%] [G loss: 2.060843]\n",
      "epoch:14 step:13271 [D loss: 0.661788, acc: 58.59%] [G loss: 1.723055]\n",
      "epoch:14 step:13272 [D loss: 0.624164, acc: 63.28%] [G loss: 2.165862]\n",
      "epoch:14 step:13273 [D loss: 0.683147, acc: 59.38%] [G loss: 1.959430]\n",
      "epoch:14 step:13274 [D loss: 0.624503, acc: 62.50%] [G loss: 1.972381]\n",
      "epoch:14 step:13275 [D loss: 0.626762, acc: 64.06%] [G loss: 1.860376]\n",
      "epoch:14 step:13276 [D loss: 0.628004, acc: 63.28%] [G loss: 1.908042]\n",
      "epoch:14 step:13277 [D loss: 0.640116, acc: 59.38%] [G loss: 1.866401]\n",
      "epoch:14 step:13278 [D loss: 0.681390, acc: 61.72%] [G loss: 1.808991]\n",
      "epoch:14 step:13279 [D loss: 0.685170, acc: 63.28%] [G loss: 1.816598]\n",
      "epoch:14 step:13280 [D loss: 0.666685, acc: 60.94%] [G loss: 1.886158]\n",
      "epoch:14 step:13281 [D loss: 0.685571, acc: 60.94%] [G loss: 1.950673]\n",
      "epoch:14 step:13282 [D loss: 0.713570, acc: 54.69%] [G loss: 1.851731]\n",
      "epoch:14 step:13283 [D loss: 0.672757, acc: 59.38%] [G loss: 1.774679]\n",
      "epoch:14 step:13284 [D loss: 0.615169, acc: 62.50%] [G loss: 1.818623]\n",
      "epoch:14 step:13285 [D loss: 0.623360, acc: 64.06%] [G loss: 1.870373]\n",
      "epoch:14 step:13286 [D loss: 0.618952, acc: 64.84%] [G loss: 1.893929]\n",
      "epoch:14 step:13287 [D loss: 0.603754, acc: 67.19%] [G loss: 1.984501]\n",
      "epoch:14 step:13288 [D loss: 0.659739, acc: 57.81%] [G loss: 1.800763]\n",
      "epoch:14 step:13289 [D loss: 0.613135, acc: 65.62%] [G loss: 2.023535]\n",
      "epoch:14 step:13290 [D loss: 0.629632, acc: 63.28%] [G loss: 1.867055]\n",
      "epoch:14 step:13291 [D loss: 0.635730, acc: 67.19%] [G loss: 1.883954]\n",
      "epoch:14 step:13292 [D loss: 0.624531, acc: 66.41%] [G loss: 1.939913]\n",
      "epoch:14 step:13293 [D loss: 0.671932, acc: 62.50%] [G loss: 1.889097]\n",
      "epoch:14 step:13294 [D loss: 0.709581, acc: 59.38%] [G loss: 1.847588]\n",
      "epoch:14 step:13295 [D loss: 0.676979, acc: 58.59%] [G loss: 1.826574]\n",
      "epoch:14 step:13296 [D loss: 0.674141, acc: 60.94%] [G loss: 1.908899]\n",
      "epoch:14 step:13297 [D loss: 0.657265, acc: 57.81%] [G loss: 1.830181]\n",
      "epoch:14 step:13298 [D loss: 0.700054, acc: 55.47%] [G loss: 1.833812]\n",
      "epoch:14 step:13299 [D loss: 0.684038, acc: 57.03%] [G loss: 1.965085]\n",
      "epoch:14 step:13300 [D loss: 0.705371, acc: 52.34%] [G loss: 1.847168]\n",
      "epoch:14 step:13301 [D loss: 0.668349, acc: 62.50%] [G loss: 1.804488]\n",
      "epoch:14 step:13302 [D loss: 0.648134, acc: 57.03%] [G loss: 2.062021]\n",
      "epoch:14 step:13303 [D loss: 0.648350, acc: 67.97%] [G loss: 1.976072]\n",
      "epoch:14 step:13304 [D loss: 0.636518, acc: 60.16%] [G loss: 1.808107]\n",
      "epoch:14 step:13305 [D loss: 0.619580, acc: 72.66%] [G loss: 1.815001]\n",
      "epoch:14 step:13306 [D loss: 0.652992, acc: 60.16%] [G loss: 1.894588]\n",
      "epoch:14 step:13307 [D loss: 0.690998, acc: 59.38%] [G loss: 1.772726]\n",
      "epoch:14 step:13308 [D loss: 0.619237, acc: 64.84%] [G loss: 1.987668]\n",
      "epoch:14 step:13309 [D loss: 0.629357, acc: 62.50%] [G loss: 1.931226]\n",
      "epoch:14 step:13310 [D loss: 0.638892, acc: 64.06%] [G loss: 1.943808]\n",
      "epoch:14 step:13311 [D loss: 0.653071, acc: 60.16%] [G loss: 2.030218]\n",
      "epoch:14 step:13312 [D loss: 0.611101, acc: 64.06%] [G loss: 1.961416]\n",
      "epoch:14 step:13313 [D loss: 0.628183, acc: 63.28%] [G loss: 2.015846]\n",
      "epoch:14 step:13314 [D loss: 0.685436, acc: 54.69%] [G loss: 1.736791]\n",
      "epoch:14 step:13315 [D loss: 0.645444, acc: 67.97%] [G loss: 1.905719]\n",
      "epoch:14 step:13316 [D loss: 0.614337, acc: 60.16%] [G loss: 1.966030]\n",
      "epoch:14 step:13317 [D loss: 0.629516, acc: 65.62%] [G loss: 1.974792]\n",
      "epoch:14 step:13318 [D loss: 0.679271, acc: 59.38%] [G loss: 1.859472]\n",
      "epoch:14 step:13319 [D loss: 0.620604, acc: 63.28%] [G loss: 1.830412]\n",
      "epoch:14 step:13320 [D loss: 0.633975, acc: 60.94%] [G loss: 2.030478]\n",
      "epoch:14 step:13321 [D loss: 0.654553, acc: 61.72%] [G loss: 1.819260]\n",
      "epoch:14 step:13322 [D loss: 0.638799, acc: 66.41%] [G loss: 1.854599]\n",
      "epoch:14 step:13323 [D loss: 0.678549, acc: 59.38%] [G loss: 1.864048]\n",
      "epoch:14 step:13324 [D loss: 0.611505, acc: 67.19%] [G loss: 2.021173]\n",
      "epoch:14 step:13325 [D loss: 0.578193, acc: 68.75%] [G loss: 2.227651]\n",
      "epoch:14 step:13326 [D loss: 0.548949, acc: 71.09%] [G loss: 2.325081]\n",
      "epoch:14 step:13327 [D loss: 0.556394, acc: 71.88%] [G loss: 2.254089]\n",
      "epoch:14 step:13328 [D loss: 0.634931, acc: 60.16%] [G loss: 1.928613]\n",
      "epoch:14 step:13329 [D loss: 0.635439, acc: 65.62%] [G loss: 1.816744]\n",
      "epoch:14 step:13330 [D loss: 0.640114, acc: 67.97%] [G loss: 1.966516]\n",
      "epoch:14 step:13331 [D loss: 0.761571, acc: 52.34%] [G loss: 1.889458]\n",
      "epoch:14 step:13332 [D loss: 0.628565, acc: 63.28%] [G loss: 1.902241]\n",
      "epoch:14 step:13333 [D loss: 0.699119, acc: 54.69%] [G loss: 1.914662]\n",
      "epoch:14 step:13334 [D loss: 0.655281, acc: 62.50%] [G loss: 1.926570]\n",
      "epoch:14 step:13335 [D loss: 0.613528, acc: 67.19%] [G loss: 2.022088]\n",
      "epoch:14 step:13336 [D loss: 0.596075, acc: 68.75%] [G loss: 2.017532]\n",
      "epoch:14 step:13337 [D loss: 0.604056, acc: 67.97%] [G loss: 2.077790]\n",
      "epoch:14 step:13338 [D loss: 0.690561, acc: 58.59%] [G loss: 1.821281]\n",
      "epoch:14 step:13339 [D loss: 0.620463, acc: 66.41%] [G loss: 1.980889]\n",
      "epoch:14 step:13340 [D loss: 0.625946, acc: 61.72%] [G loss: 2.024348]\n",
      "epoch:14 step:13341 [D loss: 0.674405, acc: 60.94%] [G loss: 2.023250]\n",
      "epoch:14 step:13342 [D loss: 0.622163, acc: 65.62%] [G loss: 1.919174]\n",
      "epoch:14 step:13343 [D loss: 0.698642, acc: 54.69%] [G loss: 1.933315]\n",
      "epoch:14 step:13344 [D loss: 0.680764, acc: 59.38%] [G loss: 1.948466]\n",
      "epoch:14 step:13345 [D loss: 0.609729, acc: 67.19%] [G loss: 1.824719]\n",
      "epoch:14 step:13346 [D loss: 0.640100, acc: 61.72%] [G loss: 1.915952]\n",
      "epoch:14 step:13347 [D loss: 0.597050, acc: 70.31%] [G loss: 2.156755]\n",
      "epoch:14 step:13348 [D loss: 0.581633, acc: 70.31%] [G loss: 2.170065]\n",
      "epoch:14 step:13349 [D loss: 0.613783, acc: 64.84%] [G loss: 2.082406]\n",
      "epoch:14 step:13350 [D loss: 0.555216, acc: 71.88%] [G loss: 2.257171]\n",
      "epoch:14 step:13351 [D loss: 0.633615, acc: 63.28%] [G loss: 2.003760]\n",
      "epoch:14 step:13352 [D loss: 0.672536, acc: 57.81%] [G loss: 1.974571]\n",
      "epoch:14 step:13353 [D loss: 0.685920, acc: 59.38%] [G loss: 1.832700]\n",
      "epoch:14 step:13354 [D loss: 0.618672, acc: 63.28%] [G loss: 1.863537]\n",
      "epoch:14 step:13355 [D loss: 0.654004, acc: 60.94%] [G loss: 1.831750]\n",
      "epoch:14 step:13356 [D loss: 0.622910, acc: 68.75%] [G loss: 2.055787]\n",
      "epoch:14 step:13357 [D loss: 0.692012, acc: 57.81%] [G loss: 1.862456]\n",
      "epoch:14 step:13358 [D loss: 0.599148, acc: 71.09%] [G loss: 1.920450]\n",
      "epoch:14 step:13359 [D loss: 0.641026, acc: 64.84%] [G loss: 2.038801]\n",
      "epoch:14 step:13360 [D loss: 0.622782, acc: 67.97%] [G loss: 2.028510]\n",
      "epoch:14 step:13361 [D loss: 0.673848, acc: 60.16%] [G loss: 1.921237]\n",
      "epoch:14 step:13362 [D loss: 0.637971, acc: 60.94%] [G loss: 1.987299]\n",
      "epoch:14 step:13363 [D loss: 0.616770, acc: 64.84%] [G loss: 1.983037]\n",
      "epoch:14 step:13364 [D loss: 0.638474, acc: 64.06%] [G loss: 2.028752]\n",
      "epoch:14 step:13365 [D loss: 0.668597, acc: 57.03%] [G loss: 1.963320]\n",
      "epoch:14 step:13366 [D loss: 0.598425, acc: 71.09%] [G loss: 2.057545]\n",
      "epoch:14 step:13367 [D loss: 0.756691, acc: 52.34%] [G loss: 1.803144]\n",
      "epoch:14 step:13368 [D loss: 0.691102, acc: 57.03%] [G loss: 1.807383]\n",
      "epoch:14 step:13369 [D loss: 0.665031, acc: 57.81%] [G loss: 1.773551]\n",
      "epoch:14 step:13370 [D loss: 0.702294, acc: 59.38%] [G loss: 1.896501]\n",
      "epoch:14 step:13371 [D loss: 0.613273, acc: 70.31%] [G loss: 2.002574]\n",
      "epoch:14 step:13372 [D loss: 0.634194, acc: 64.06%] [G loss: 1.894071]\n",
      "epoch:14 step:13373 [D loss: 0.692360, acc: 57.81%] [G loss: 1.826530]\n",
      "epoch:14 step:13374 [D loss: 0.672181, acc: 58.59%] [G loss: 1.879046]\n",
      "epoch:14 step:13375 [D loss: 0.630929, acc: 64.06%] [G loss: 1.993859]\n",
      "epoch:14 step:13376 [D loss: 0.624875, acc: 64.84%] [G loss: 1.830437]\n",
      "epoch:14 step:13377 [D loss: 0.657894, acc: 61.72%] [G loss: 1.907739]\n",
      "epoch:14 step:13378 [D loss: 0.620860, acc: 64.06%] [G loss: 1.951484]\n",
      "epoch:14 step:13379 [D loss: 0.602567, acc: 65.62%] [G loss: 2.041332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13380 [D loss: 0.606543, acc: 69.53%] [G loss: 2.165359]\n",
      "epoch:14 step:13381 [D loss: 0.611351, acc: 67.19%] [G loss: 1.908262]\n",
      "epoch:14 step:13382 [D loss: 0.644352, acc: 60.16%] [G loss: 1.874224]\n",
      "epoch:14 step:13383 [D loss: 0.617201, acc: 67.19%] [G loss: 2.002047]\n",
      "epoch:14 step:13384 [D loss: 0.648323, acc: 58.59%] [G loss: 1.838763]\n",
      "epoch:14 step:13385 [D loss: 0.647492, acc: 67.19%] [G loss: 1.884289]\n",
      "epoch:14 step:13386 [D loss: 0.667366, acc: 62.50%] [G loss: 1.809264]\n",
      "epoch:14 step:13387 [D loss: 0.623668, acc: 65.62%] [G loss: 1.956895]\n",
      "epoch:14 step:13388 [D loss: 0.649326, acc: 62.50%] [G loss: 2.030114]\n",
      "epoch:14 step:13389 [D loss: 0.612981, acc: 63.28%] [G loss: 2.159877]\n",
      "epoch:14 step:13390 [D loss: 0.644855, acc: 61.72%] [G loss: 2.048663]\n",
      "epoch:14 step:13391 [D loss: 0.645402, acc: 60.94%] [G loss: 1.988511]\n",
      "epoch:14 step:13392 [D loss: 0.654116, acc: 64.06%] [G loss: 1.938451]\n",
      "epoch:14 step:13393 [D loss: 0.617974, acc: 63.28%] [G loss: 1.909744]\n",
      "epoch:14 step:13394 [D loss: 0.567192, acc: 71.09%] [G loss: 2.115037]\n",
      "epoch:14 step:13395 [D loss: 0.610854, acc: 64.84%] [G loss: 1.852251]\n",
      "epoch:14 step:13396 [D loss: 0.684234, acc: 59.38%] [G loss: 1.845030]\n",
      "epoch:14 step:13397 [D loss: 0.630665, acc: 67.19%] [G loss: 1.912432]\n",
      "epoch:14 step:13398 [D loss: 0.600442, acc: 72.66%] [G loss: 1.966896]\n",
      "epoch:14 step:13399 [D loss: 0.671192, acc: 60.94%] [G loss: 1.869616]\n",
      "epoch:14 step:13400 [D loss: 0.630587, acc: 65.62%] [G loss: 1.885838]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 6.941294\n",
      "FID: 18.987188\n",
      "0 = 12.981974939727769\n",
      "1 = 0.0930864374593162\n",
      "2 = 0.8956000208854675\n",
      "3 = 0.9120000004768372\n",
      "4 = 0.8791999816894531\n",
      "5 = 0.8830364346504211\n",
      "6 = 0.9120000004768372\n",
      "7 = 6.99681342025994\n",
      "8 = 0.0847720095431844\n",
      "9 = 0.7513999938964844\n",
      "10 = 0.7631999850273132\n",
      "11 = 0.7396000027656555\n",
      "12 = 0.7456037402153015\n",
      "13 = 0.7631999850273132\n",
      "14 = 6.9413228034973145\n",
      "15 = 9.384739875793457\n",
      "16 = 0.13754096627235413\n",
      "17 = 6.941293716430664\n",
      "18 = 18.9871883392334\n",
      "epoch:14 step:13401 [D loss: 0.636084, acc: 61.72%] [G loss: 1.847352]\n",
      "epoch:14 step:13402 [D loss: 0.683930, acc: 60.16%] [G loss: 1.871457]\n",
      "epoch:14 step:13403 [D loss: 0.630029, acc: 62.50%] [G loss: 1.954176]\n",
      "epoch:14 step:13404 [D loss: 0.627981, acc: 65.62%] [G loss: 1.934314]\n",
      "epoch:14 step:13405 [D loss: 0.603239, acc: 73.44%] [G loss: 1.951009]\n",
      "epoch:14 step:13406 [D loss: 0.718166, acc: 57.81%] [G loss: 1.846155]\n",
      "epoch:14 step:13407 [D loss: 0.629914, acc: 62.50%] [G loss: 2.069343]\n",
      "epoch:14 step:13408 [D loss: 0.630114, acc: 63.28%] [G loss: 1.955162]\n",
      "epoch:14 step:13409 [D loss: 0.623243, acc: 64.84%] [G loss: 1.933057]\n",
      "epoch:14 step:13410 [D loss: 0.659660, acc: 61.72%] [G loss: 1.889351]\n",
      "epoch:14 step:13411 [D loss: 0.607043, acc: 68.75%] [G loss: 1.888740]\n",
      "epoch:14 step:13412 [D loss: 0.638391, acc: 63.28%] [G loss: 1.896766]\n",
      "epoch:14 step:13413 [D loss: 0.669311, acc: 62.50%] [G loss: 1.849114]\n",
      "epoch:14 step:13414 [D loss: 0.638348, acc: 64.06%] [G loss: 1.916841]\n",
      "epoch:14 step:13415 [D loss: 0.608310, acc: 66.41%] [G loss: 1.920630]\n",
      "epoch:14 step:13416 [D loss: 0.666019, acc: 64.06%] [G loss: 2.110004]\n",
      "epoch:14 step:13417 [D loss: 0.596290, acc: 67.19%] [G loss: 1.986068]\n",
      "epoch:14 step:13418 [D loss: 0.664353, acc: 57.03%] [G loss: 1.973830]\n",
      "epoch:14 step:13419 [D loss: 0.675386, acc: 55.47%] [G loss: 1.848141]\n",
      "epoch:14 step:13420 [D loss: 0.608999, acc: 65.62%] [G loss: 2.039206]\n",
      "epoch:14 step:13421 [D loss: 0.649287, acc: 66.41%] [G loss: 1.960531]\n",
      "epoch:14 step:13422 [D loss: 0.692442, acc: 53.91%] [G loss: 1.947136]\n",
      "epoch:14 step:13423 [D loss: 0.660687, acc: 60.16%] [G loss: 1.881598]\n",
      "epoch:14 step:13424 [D loss: 0.685710, acc: 53.91%] [G loss: 1.909619]\n",
      "epoch:14 step:13425 [D loss: 0.591712, acc: 71.88%] [G loss: 1.885791]\n",
      "epoch:14 step:13426 [D loss: 0.652007, acc: 60.94%] [G loss: 1.884958]\n",
      "epoch:14 step:13427 [D loss: 0.591291, acc: 67.19%] [G loss: 1.953346]\n",
      "epoch:14 step:13428 [D loss: 0.665269, acc: 60.16%] [G loss: 1.925084]\n",
      "epoch:14 step:13429 [D loss: 0.650440, acc: 67.19%] [G loss: 1.930765]\n",
      "epoch:14 step:13430 [D loss: 0.551184, acc: 75.00%] [G loss: 2.350511]\n",
      "epoch:14 step:13431 [D loss: 0.607613, acc: 66.41%] [G loss: 2.022337]\n",
      "epoch:14 step:13432 [D loss: 0.582491, acc: 75.78%] [G loss: 2.258126]\n",
      "epoch:14 step:13433 [D loss: 0.590113, acc: 69.53%] [G loss: 2.107573]\n",
      "epoch:14 step:13434 [D loss: 0.653671, acc: 61.72%] [G loss: 1.791320]\n",
      "epoch:14 step:13435 [D loss: 0.770207, acc: 48.44%] [G loss: 1.886564]\n",
      "epoch:14 step:13436 [D loss: 0.642178, acc: 67.19%] [G loss: 1.970837]\n",
      "epoch:14 step:13437 [D loss: 0.655432, acc: 58.59%] [G loss: 1.919120]\n",
      "epoch:14 step:13438 [D loss: 0.631157, acc: 65.62%] [G loss: 1.855439]\n",
      "epoch:14 step:13439 [D loss: 0.608881, acc: 65.62%] [G loss: 2.138615]\n",
      "epoch:14 step:13440 [D loss: 0.720264, acc: 58.59%] [G loss: 2.026359]\n",
      "epoch:14 step:13441 [D loss: 0.641760, acc: 61.72%] [G loss: 1.813281]\n",
      "epoch:14 step:13442 [D loss: 0.636471, acc: 64.84%] [G loss: 1.898576]\n",
      "epoch:14 step:13443 [D loss: 0.631695, acc: 65.62%] [G loss: 1.938534]\n",
      "epoch:14 step:13444 [D loss: 0.636786, acc: 63.28%] [G loss: 1.923078]\n",
      "epoch:14 step:13445 [D loss: 0.651160, acc: 62.50%] [G loss: 1.808027]\n",
      "epoch:14 step:13446 [D loss: 0.618930, acc: 60.94%] [G loss: 1.928619]\n",
      "epoch:14 step:13447 [D loss: 0.617508, acc: 67.97%] [G loss: 1.855926]\n",
      "epoch:14 step:13448 [D loss: 0.659028, acc: 58.59%] [G loss: 2.022552]\n",
      "epoch:14 step:13449 [D loss: 0.633808, acc: 60.94%] [G loss: 1.966577]\n",
      "epoch:14 step:13450 [D loss: 0.598857, acc: 64.84%] [G loss: 2.048166]\n",
      "epoch:14 step:13451 [D loss: 0.609729, acc: 67.97%] [G loss: 1.976028]\n",
      "epoch:14 step:13452 [D loss: 0.664729, acc: 61.72%] [G loss: 2.046086]\n",
      "epoch:14 step:13453 [D loss: 0.570912, acc: 70.31%] [G loss: 2.372480]\n",
      "epoch:14 step:13454 [D loss: 0.659597, acc: 57.81%] [G loss: 2.121387]\n",
      "epoch:14 step:13455 [D loss: 0.586525, acc: 70.31%] [G loss: 2.187561]\n",
      "epoch:14 step:13456 [D loss: 0.639400, acc: 63.28%] [G loss: 2.087366]\n",
      "epoch:14 step:13457 [D loss: 0.632758, acc: 64.84%] [G loss: 1.940744]\n",
      "epoch:14 step:13458 [D loss: 0.648105, acc: 62.50%] [G loss: 1.880139]\n",
      "epoch:14 step:13459 [D loss: 0.671186, acc: 57.81%] [G loss: 1.869588]\n",
      "epoch:14 step:13460 [D loss: 0.690122, acc: 57.03%] [G loss: 2.016081]\n",
      "epoch:14 step:13461 [D loss: 0.622680, acc: 65.62%] [G loss: 1.998568]\n",
      "epoch:14 step:13462 [D loss: 0.628674, acc: 60.94%] [G loss: 2.013943]\n",
      "epoch:14 step:13463 [D loss: 0.629352, acc: 64.06%] [G loss: 2.233393]\n",
      "epoch:14 step:13464 [D loss: 0.583975, acc: 64.84%] [G loss: 2.207528]\n",
      "epoch:14 step:13465 [D loss: 0.634832, acc: 61.72%] [G loss: 2.396947]\n",
      "epoch:14 step:13466 [D loss: 0.655957, acc: 61.72%] [G loss: 2.047441]\n",
      "epoch:14 step:13467 [D loss: 0.761111, acc: 50.78%] [G loss: 1.678117]\n",
      "epoch:14 step:13468 [D loss: 0.616336, acc: 68.75%] [G loss: 1.951344]\n",
      "epoch:14 step:13469 [D loss: 0.624082, acc: 63.28%] [G loss: 1.876869]\n",
      "epoch:14 step:13470 [D loss: 0.697744, acc: 57.03%] [G loss: 1.953397]\n",
      "epoch:14 step:13471 [D loss: 0.640197, acc: 60.94%] [G loss: 1.857055]\n",
      "epoch:14 step:13472 [D loss: 0.621274, acc: 64.84%] [G loss: 2.172622]\n",
      "epoch:14 step:13473 [D loss: 0.696062, acc: 55.47%] [G loss: 1.827472]\n",
      "epoch:14 step:13474 [D loss: 0.671410, acc: 61.72%] [G loss: 1.833706]\n",
      "epoch:14 step:13475 [D loss: 0.624722, acc: 64.06%] [G loss: 2.005096]\n",
      "epoch:14 step:13476 [D loss: 0.634574, acc: 65.62%] [G loss: 1.985657]\n",
      "epoch:14 step:13477 [D loss: 0.607667, acc: 70.31%] [G loss: 2.018215]\n",
      "epoch:14 step:13478 [D loss: 0.622932, acc: 70.31%] [G loss: 1.986518]\n",
      "epoch:14 step:13479 [D loss: 0.690116, acc: 60.94%] [G loss: 1.791417]\n",
      "epoch:14 step:13480 [D loss: 0.634490, acc: 67.19%] [G loss: 1.879470]\n",
      "epoch:14 step:13481 [D loss: 0.617045, acc: 66.41%] [G loss: 2.072073]\n",
      "epoch:14 step:13482 [D loss: 0.584752, acc: 71.09%] [G loss: 2.045160]\n",
      "epoch:14 step:13483 [D loss: 0.625692, acc: 61.72%] [G loss: 1.998098]\n",
      "epoch:14 step:13484 [D loss: 0.695493, acc: 53.91%] [G loss: 2.045538]\n",
      "epoch:14 step:13485 [D loss: 0.618836, acc: 67.19%] [G loss: 2.063087]\n",
      "epoch:14 step:13486 [D loss: 0.618430, acc: 67.97%] [G loss: 1.857512]\n",
      "epoch:14 step:13487 [D loss: 0.680078, acc: 58.59%] [G loss: 2.044084]\n",
      "epoch:14 step:13488 [D loss: 0.605917, acc: 64.84%] [G loss: 2.073238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13489 [D loss: 0.604557, acc: 64.84%] [G loss: 2.003791]\n",
      "epoch:14 step:13490 [D loss: 0.655681, acc: 64.84%] [G loss: 1.868934]\n",
      "epoch:14 step:13491 [D loss: 0.645846, acc: 64.84%] [G loss: 1.822972]\n",
      "epoch:14 step:13492 [D loss: 0.617859, acc: 62.50%] [G loss: 2.024796]\n",
      "epoch:14 step:13493 [D loss: 0.643219, acc: 60.94%] [G loss: 1.806652]\n",
      "epoch:14 step:13494 [D loss: 0.665738, acc: 64.84%] [G loss: 1.964826]\n",
      "epoch:14 step:13495 [D loss: 0.685216, acc: 54.69%] [G loss: 1.797178]\n",
      "epoch:14 step:13496 [D loss: 0.627019, acc: 62.50%] [G loss: 1.899030]\n",
      "epoch:14 step:13497 [D loss: 0.638223, acc: 63.28%] [G loss: 1.864584]\n",
      "epoch:14 step:13498 [D loss: 0.643541, acc: 64.84%] [G loss: 1.982865]\n",
      "epoch:14 step:13499 [D loss: 0.648563, acc: 64.84%] [G loss: 2.037709]\n",
      "epoch:14 step:13500 [D loss: 0.652421, acc: 60.94%] [G loss: 1.797363]\n",
      "epoch:14 step:13501 [D loss: 0.634285, acc: 64.06%] [G loss: 1.922069]\n",
      "epoch:14 step:13502 [D loss: 0.616627, acc: 64.06%] [G loss: 1.798912]\n",
      "epoch:14 step:13503 [D loss: 0.631028, acc: 67.19%] [G loss: 1.978242]\n",
      "epoch:14 step:13504 [D loss: 0.708563, acc: 55.47%] [G loss: 1.809472]\n",
      "epoch:14 step:13505 [D loss: 0.646653, acc: 63.28%] [G loss: 1.774141]\n",
      "epoch:14 step:13506 [D loss: 0.647337, acc: 62.50%] [G loss: 1.936096]\n",
      "epoch:14 step:13507 [D loss: 0.634201, acc: 63.28%] [G loss: 1.975345]\n",
      "epoch:14 step:13508 [D loss: 0.663428, acc: 57.03%] [G loss: 1.900025]\n",
      "epoch:14 step:13509 [D loss: 0.630181, acc: 62.50%] [G loss: 1.943251]\n",
      "epoch:14 step:13510 [D loss: 0.666567, acc: 57.81%] [G loss: 1.894498]\n",
      "epoch:14 step:13511 [D loss: 0.647389, acc: 60.94%] [G loss: 1.952314]\n",
      "epoch:14 step:13512 [D loss: 0.702078, acc: 55.47%] [G loss: 1.834054]\n",
      "epoch:14 step:13513 [D loss: 0.630153, acc: 68.75%] [G loss: 1.890707]\n",
      "epoch:14 step:13514 [D loss: 0.674047, acc: 54.69%] [G loss: 1.787797]\n",
      "epoch:14 step:13515 [D loss: 0.707986, acc: 56.25%] [G loss: 1.846966]\n",
      "epoch:14 step:13516 [D loss: 0.605190, acc: 66.41%] [G loss: 1.943868]\n",
      "epoch:14 step:13517 [D loss: 0.594468, acc: 68.75%] [G loss: 2.030532]\n",
      "epoch:14 step:13518 [D loss: 0.645912, acc: 60.16%] [G loss: 1.912750]\n",
      "epoch:14 step:13519 [D loss: 0.682668, acc: 57.03%] [G loss: 1.867694]\n",
      "epoch:14 step:13520 [D loss: 0.657511, acc: 61.72%] [G loss: 1.996172]\n",
      "epoch:14 step:13521 [D loss: 0.645356, acc: 64.84%] [G loss: 1.850197]\n",
      "epoch:14 step:13522 [D loss: 0.619608, acc: 62.50%] [G loss: 2.068513]\n",
      "epoch:14 step:13523 [D loss: 0.579733, acc: 69.53%] [G loss: 2.045535]\n",
      "epoch:14 step:13524 [D loss: 0.644581, acc: 65.62%] [G loss: 2.003167]\n",
      "epoch:14 step:13525 [D loss: 0.647494, acc: 56.25%] [G loss: 2.016439]\n",
      "epoch:14 step:13526 [D loss: 0.648654, acc: 67.19%] [G loss: 1.879198]\n",
      "epoch:14 step:13527 [D loss: 0.662324, acc: 60.16%] [G loss: 1.844569]\n",
      "epoch:14 step:13528 [D loss: 0.638492, acc: 66.41%] [G loss: 1.944366]\n",
      "epoch:14 step:13529 [D loss: 0.642707, acc: 62.50%] [G loss: 1.879149]\n",
      "epoch:14 step:13530 [D loss: 0.647756, acc: 60.16%] [G loss: 1.927397]\n",
      "epoch:14 step:13531 [D loss: 0.676469, acc: 62.50%] [G loss: 1.948790]\n",
      "epoch:14 step:13532 [D loss: 0.680543, acc: 64.84%] [G loss: 2.042670]\n",
      "epoch:14 step:13533 [D loss: 0.641588, acc: 62.50%] [G loss: 2.018011]\n",
      "epoch:14 step:13534 [D loss: 0.598488, acc: 71.88%] [G loss: 2.070033]\n",
      "epoch:14 step:13535 [D loss: 0.661117, acc: 65.62%] [G loss: 2.022202]\n",
      "epoch:14 step:13536 [D loss: 0.665874, acc: 62.50%] [G loss: 1.902575]\n",
      "epoch:14 step:13537 [D loss: 0.694860, acc: 55.47%] [G loss: 1.954365]\n",
      "epoch:14 step:13538 [D loss: 0.649589, acc: 59.38%] [G loss: 1.918958]\n",
      "epoch:14 step:13539 [D loss: 0.700786, acc: 58.59%] [G loss: 1.959665]\n",
      "epoch:14 step:13540 [D loss: 0.651623, acc: 64.06%] [G loss: 1.886246]\n",
      "epoch:14 step:13541 [D loss: 0.680319, acc: 57.03%] [G loss: 1.915310]\n",
      "epoch:14 step:13542 [D loss: 0.667277, acc: 57.81%] [G loss: 1.797449]\n",
      "epoch:14 step:13543 [D loss: 0.689428, acc: 63.28%] [G loss: 1.909976]\n",
      "epoch:14 step:13544 [D loss: 0.619606, acc: 67.97%] [G loss: 2.061991]\n",
      "epoch:14 step:13545 [D loss: 0.637651, acc: 63.28%] [G loss: 2.141218]\n",
      "epoch:14 step:13546 [D loss: 0.590680, acc: 68.75%] [G loss: 2.060156]\n",
      "epoch:14 step:13547 [D loss: 0.611236, acc: 69.53%] [G loss: 2.178477]\n",
      "epoch:14 step:13548 [D loss: 0.603758, acc: 63.28%] [G loss: 2.264270]\n",
      "epoch:14 step:13549 [D loss: 0.666156, acc: 60.94%] [G loss: 1.955795]\n",
      "epoch:14 step:13550 [D loss: 0.623786, acc: 64.06%] [G loss: 1.958927]\n",
      "epoch:14 step:13551 [D loss: 0.682935, acc: 60.16%] [G loss: 1.952049]\n",
      "epoch:14 step:13552 [D loss: 0.568928, acc: 71.88%] [G loss: 2.213581]\n",
      "epoch:14 step:13553 [D loss: 0.583447, acc: 72.66%] [G loss: 2.019573]\n",
      "epoch:14 step:13554 [D loss: 0.623090, acc: 57.81%] [G loss: 1.973435]\n",
      "epoch:14 step:13555 [D loss: 0.703050, acc: 56.25%] [G loss: 1.762497]\n",
      "epoch:14 step:13556 [D loss: 0.682143, acc: 53.12%] [G loss: 1.878522]\n",
      "epoch:14 step:13557 [D loss: 0.644200, acc: 62.50%] [G loss: 1.821707]\n",
      "epoch:14 step:13558 [D loss: 0.665440, acc: 60.94%] [G loss: 1.802660]\n",
      "epoch:14 step:13559 [D loss: 0.690938, acc: 55.47%] [G loss: 1.963893]\n",
      "epoch:14 step:13560 [D loss: 0.645184, acc: 64.06%] [G loss: 1.866108]\n",
      "epoch:14 step:13561 [D loss: 0.646821, acc: 59.38%] [G loss: 1.838743]\n",
      "epoch:14 step:13562 [D loss: 0.672245, acc: 57.81%] [G loss: 1.909152]\n",
      "epoch:14 step:13563 [D loss: 0.619273, acc: 64.84%] [G loss: 1.986742]\n",
      "epoch:14 step:13564 [D loss: 0.662828, acc: 59.38%] [G loss: 1.801081]\n",
      "epoch:14 step:13565 [D loss: 0.653096, acc: 62.50%] [G loss: 1.942622]\n",
      "epoch:14 step:13566 [D loss: 0.673393, acc: 62.50%] [G loss: 1.840484]\n",
      "epoch:14 step:13567 [D loss: 0.637599, acc: 61.72%] [G loss: 2.005282]\n",
      "epoch:14 step:13568 [D loss: 0.662655, acc: 57.81%] [G loss: 1.910815]\n",
      "epoch:14 step:13569 [D loss: 0.646428, acc: 64.06%] [G loss: 1.879325]\n",
      "epoch:14 step:13570 [D loss: 0.646501, acc: 57.03%] [G loss: 1.881047]\n",
      "epoch:14 step:13571 [D loss: 0.614964, acc: 67.19%] [G loss: 1.991718]\n",
      "epoch:14 step:13572 [D loss: 0.637079, acc: 62.50%] [G loss: 1.848684]\n",
      "epoch:14 step:13573 [D loss: 0.653509, acc: 57.81%] [G loss: 1.937105]\n",
      "epoch:14 step:13574 [D loss: 0.639045, acc: 65.62%] [G loss: 1.889849]\n",
      "epoch:14 step:13575 [D loss: 0.637270, acc: 60.94%] [G loss: 1.950228]\n",
      "epoch:14 step:13576 [D loss: 0.650170, acc: 62.50%] [G loss: 1.857810]\n",
      "epoch:14 step:13577 [D loss: 0.624227, acc: 64.06%] [G loss: 1.873230]\n",
      "epoch:14 step:13578 [D loss: 0.687449, acc: 54.69%] [G loss: 1.809061]\n",
      "epoch:14 step:13579 [D loss: 0.638077, acc: 62.50%] [G loss: 1.803516]\n",
      "epoch:14 step:13580 [D loss: 0.658258, acc: 64.06%] [G loss: 1.937275]\n",
      "epoch:14 step:13581 [D loss: 0.625031, acc: 71.88%] [G loss: 1.791698]\n",
      "epoch:14 step:13582 [D loss: 0.674576, acc: 60.16%] [G loss: 1.896472]\n",
      "epoch:14 step:13583 [D loss: 0.656049, acc: 59.38%] [G loss: 1.854707]\n",
      "epoch:14 step:13584 [D loss: 0.663073, acc: 57.81%] [G loss: 1.926512]\n",
      "epoch:14 step:13585 [D loss: 0.681635, acc: 60.16%] [G loss: 1.888862]\n",
      "epoch:14 step:13586 [D loss: 0.666611, acc: 57.81%] [G loss: 1.988776]\n",
      "epoch:14 step:13587 [D loss: 0.649975, acc: 59.38%] [G loss: 2.040159]\n",
      "epoch:14 step:13588 [D loss: 0.609963, acc: 64.06%] [G loss: 1.932134]\n",
      "epoch:14 step:13589 [D loss: 0.559727, acc: 71.88%] [G loss: 2.155227]\n",
      "epoch:14 step:13590 [D loss: 0.612192, acc: 64.84%] [G loss: 2.136117]\n",
      "epoch:14 step:13591 [D loss: 0.688676, acc: 56.25%] [G loss: 1.715022]\n",
      "epoch:14 step:13592 [D loss: 0.647786, acc: 67.19%] [G loss: 1.988597]\n",
      "epoch:14 step:13593 [D loss: 0.662464, acc: 62.50%] [G loss: 1.835992]\n",
      "epoch:14 step:13594 [D loss: 0.649198, acc: 61.72%] [G loss: 1.870394]\n",
      "epoch:14 step:13595 [D loss: 0.662177, acc: 58.59%] [G loss: 1.697260]\n",
      "epoch:14 step:13596 [D loss: 0.637524, acc: 64.84%] [G loss: 2.028949]\n",
      "epoch:14 step:13597 [D loss: 0.627595, acc: 65.62%] [G loss: 2.090768]\n",
      "epoch:14 step:13598 [D loss: 0.657752, acc: 61.72%] [G loss: 1.963139]\n",
      "epoch:14 step:13599 [D loss: 0.617877, acc: 68.75%] [G loss: 1.963589]\n",
      "epoch:14 step:13600 [D loss: 0.678675, acc: 57.03%] [G loss: 1.785404]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.017125\n",
      "FID: 17.587814\n",
      "0 = 12.722809628152842\n",
      "1 = 0.08853658025754763\n",
      "2 = 0.8866999745368958\n",
      "3 = 0.9067999720573425\n",
      "4 = 0.866599977016449\n",
      "5 = 0.8717554211616516\n",
      "6 = 0.9067999720573425\n",
      "7 = 6.793193654978253\n",
      "8 = 0.08325087918570513\n",
      "9 = 0.7444999814033508\n",
      "10 = 0.7487999796867371\n",
      "11 = 0.7401999831199646\n",
      "12 = 0.7424152493476868\n",
      "13 = 0.7487999796867371\n",
      "14 = 7.017152309417725\n",
      "15 = 9.386943817138672\n",
      "16 = 0.1339063048362732\n",
      "17 = 7.017125129699707\n",
      "18 = 17.587814331054688\n",
      "epoch:14 step:13601 [D loss: 0.631141, acc: 60.94%] [G loss: 1.790109]\n",
      "epoch:14 step:13602 [D loss: 0.663424, acc: 57.81%] [G loss: 2.042616]\n",
      "epoch:14 step:13603 [D loss: 0.656551, acc: 62.50%] [G loss: 1.801878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13604 [D loss: 0.676337, acc: 57.03%] [G loss: 1.952944]\n",
      "epoch:14 step:13605 [D loss: 0.630488, acc: 67.19%] [G loss: 1.882197]\n",
      "epoch:14 step:13606 [D loss: 0.654279, acc: 59.38%] [G loss: 2.082383]\n",
      "epoch:14 step:13607 [D loss: 0.603306, acc: 74.22%] [G loss: 1.909499]\n",
      "epoch:14 step:13608 [D loss: 0.640836, acc: 64.84%] [G loss: 1.842916]\n",
      "epoch:14 step:13609 [D loss: 0.626903, acc: 59.38%] [G loss: 1.953726]\n",
      "epoch:14 step:13610 [D loss: 0.656088, acc: 56.25%] [G loss: 1.830367]\n",
      "epoch:14 step:13611 [D loss: 0.640046, acc: 63.28%] [G loss: 1.872879]\n",
      "epoch:14 step:13612 [D loss: 0.605988, acc: 68.75%] [G loss: 1.951871]\n",
      "epoch:14 step:13613 [D loss: 0.596865, acc: 71.09%] [G loss: 2.124570]\n",
      "epoch:14 step:13614 [D loss: 0.665110, acc: 60.16%] [G loss: 2.024133]\n",
      "epoch:14 step:13615 [D loss: 0.622800, acc: 64.84%] [G loss: 2.036530]\n",
      "epoch:14 step:13616 [D loss: 0.633343, acc: 67.19%] [G loss: 2.070960]\n",
      "epoch:14 step:13617 [D loss: 0.595292, acc: 67.19%] [G loss: 2.049226]\n",
      "epoch:14 step:13618 [D loss: 0.720066, acc: 54.69%] [G loss: 1.701286]\n",
      "epoch:14 step:13619 [D loss: 0.679446, acc: 58.59%] [G loss: 1.769158]\n",
      "epoch:14 step:13620 [D loss: 0.725651, acc: 54.69%] [G loss: 1.760109]\n",
      "epoch:14 step:13621 [D loss: 0.607035, acc: 65.62%] [G loss: 1.894940]\n",
      "epoch:14 step:13622 [D loss: 0.587611, acc: 67.97%] [G loss: 2.160891]\n",
      "epoch:14 step:13623 [D loss: 0.636860, acc: 64.84%] [G loss: 1.897530]\n",
      "epoch:14 step:13624 [D loss: 0.715514, acc: 54.69%] [G loss: 1.704033]\n",
      "epoch:14 step:13625 [D loss: 0.689687, acc: 60.16%] [G loss: 1.835717]\n",
      "epoch:14 step:13626 [D loss: 0.627078, acc: 67.97%] [G loss: 1.991395]\n",
      "epoch:14 step:13627 [D loss: 0.640308, acc: 65.62%] [G loss: 1.911415]\n",
      "epoch:14 step:13628 [D loss: 0.669931, acc: 61.72%] [G loss: 1.992811]\n",
      "epoch:14 step:13629 [D loss: 0.688250, acc: 54.69%] [G loss: 1.913525]\n",
      "epoch:14 step:13630 [D loss: 0.640344, acc: 63.28%] [G loss: 1.859206]\n",
      "epoch:14 step:13631 [D loss: 0.662872, acc: 57.81%] [G loss: 1.722306]\n",
      "epoch:14 step:13632 [D loss: 0.623521, acc: 63.28%] [G loss: 2.074966]\n",
      "epoch:14 step:13633 [D loss: 0.616226, acc: 61.72%] [G loss: 1.799259]\n",
      "epoch:14 step:13634 [D loss: 0.601674, acc: 70.31%] [G loss: 1.935662]\n",
      "epoch:14 step:13635 [D loss: 0.649049, acc: 59.38%] [G loss: 2.003597]\n",
      "epoch:14 step:13636 [D loss: 0.613069, acc: 64.84%] [G loss: 1.939029]\n",
      "epoch:14 step:13637 [D loss: 0.621269, acc: 66.41%] [G loss: 1.973132]\n",
      "epoch:14 step:13638 [D loss: 0.593779, acc: 69.53%] [G loss: 1.955236]\n",
      "epoch:14 step:13639 [D loss: 0.661529, acc: 64.84%] [G loss: 2.022388]\n",
      "epoch:14 step:13640 [D loss: 0.611807, acc: 63.28%] [G loss: 2.053801]\n",
      "epoch:14 step:13641 [D loss: 0.620827, acc: 64.84%] [G loss: 1.993733]\n",
      "epoch:14 step:13642 [D loss: 0.701781, acc: 56.25%] [G loss: 1.880712]\n",
      "epoch:14 step:13643 [D loss: 0.692471, acc: 58.59%] [G loss: 1.812713]\n",
      "epoch:14 step:13644 [D loss: 0.652694, acc: 60.94%] [G loss: 1.718567]\n",
      "epoch:14 step:13645 [D loss: 0.686636, acc: 58.59%] [G loss: 1.873887]\n",
      "epoch:14 step:13646 [D loss: 0.686879, acc: 53.12%] [G loss: 1.638404]\n",
      "epoch:14 step:13647 [D loss: 0.685126, acc: 54.69%] [G loss: 1.693483]\n",
      "epoch:14 step:13648 [D loss: 0.678800, acc: 59.38%] [G loss: 1.797983]\n",
      "epoch:14 step:13649 [D loss: 0.691339, acc: 59.38%] [G loss: 1.700227]\n",
      "epoch:14 step:13650 [D loss: 0.605376, acc: 68.75%] [G loss: 1.824201]\n",
      "epoch:14 step:13651 [D loss: 0.663427, acc: 60.16%] [G loss: 1.901498]\n",
      "epoch:14 step:13652 [D loss: 0.590517, acc: 66.41%] [G loss: 1.951622]\n",
      "epoch:14 step:13653 [D loss: 0.649501, acc: 60.94%] [G loss: 1.856986]\n",
      "epoch:14 step:13654 [D loss: 0.593195, acc: 64.06%] [G loss: 1.976896]\n",
      "epoch:14 step:13655 [D loss: 0.651330, acc: 65.62%] [G loss: 1.928899]\n",
      "epoch:14 step:13656 [D loss: 0.692524, acc: 55.47%] [G loss: 1.709378]\n",
      "epoch:14 step:13657 [D loss: 0.647692, acc: 60.94%] [G loss: 1.812581]\n",
      "epoch:14 step:13658 [D loss: 0.632214, acc: 65.62%] [G loss: 2.002195]\n",
      "epoch:14 step:13659 [D loss: 0.657238, acc: 64.06%] [G loss: 1.775513]\n",
      "epoch:14 step:13660 [D loss: 0.698147, acc: 56.25%] [G loss: 1.815929]\n",
      "epoch:14 step:13661 [D loss: 0.681445, acc: 57.03%] [G loss: 1.831393]\n",
      "epoch:14 step:13662 [D loss: 0.651642, acc: 64.84%] [G loss: 1.850478]\n",
      "epoch:14 step:13663 [D loss: 0.624893, acc: 61.72%] [G loss: 2.007119]\n",
      "epoch:14 step:13664 [D loss: 0.650402, acc: 64.06%] [G loss: 1.872685]\n",
      "epoch:14 step:13665 [D loss: 0.631169, acc: 61.72%] [G loss: 1.895950]\n",
      "epoch:14 step:13666 [D loss: 0.634453, acc: 64.06%] [G loss: 1.875443]\n",
      "epoch:14 step:13667 [D loss: 0.642070, acc: 62.50%] [G loss: 1.889876]\n",
      "epoch:14 step:13668 [D loss: 0.622462, acc: 61.72%] [G loss: 1.964690]\n",
      "epoch:14 step:13669 [D loss: 0.576901, acc: 71.88%] [G loss: 2.058834]\n",
      "epoch:14 step:13670 [D loss: 0.636721, acc: 66.41%] [G loss: 2.007562]\n",
      "epoch:14 step:13671 [D loss: 0.633491, acc: 63.28%] [G loss: 1.876912]\n",
      "epoch:14 step:13672 [D loss: 0.627610, acc: 69.53%] [G loss: 2.059672]\n",
      "epoch:14 step:13673 [D loss: 0.625042, acc: 64.84%] [G loss: 2.144224]\n",
      "epoch:14 step:13674 [D loss: 0.583485, acc: 71.88%] [G loss: 1.994562]\n",
      "epoch:14 step:13675 [D loss: 0.627238, acc: 68.75%] [G loss: 2.055402]\n",
      "epoch:14 step:13676 [D loss: 0.628494, acc: 63.28%] [G loss: 1.936340]\n",
      "epoch:14 step:13677 [D loss: 0.667613, acc: 58.59%] [G loss: 1.851605]\n",
      "epoch:14 step:13678 [D loss: 0.621067, acc: 68.75%] [G loss: 1.814950]\n",
      "epoch:14 step:13679 [D loss: 0.621701, acc: 66.41%] [G loss: 1.974121]\n",
      "epoch:14 step:13680 [D loss: 0.628171, acc: 61.72%] [G loss: 1.840024]\n",
      "epoch:14 step:13681 [D loss: 0.710376, acc: 57.81%] [G loss: 2.109888]\n",
      "epoch:14 step:13682 [D loss: 0.635152, acc: 64.84%] [G loss: 1.918050]\n",
      "epoch:14 step:13683 [D loss: 0.570581, acc: 71.09%] [G loss: 1.957804]\n",
      "epoch:14 step:13684 [D loss: 0.652203, acc: 62.50%] [G loss: 1.845915]\n",
      "epoch:14 step:13685 [D loss: 0.672827, acc: 61.72%] [G loss: 1.940018]\n",
      "epoch:14 step:13686 [D loss: 0.668989, acc: 58.59%] [G loss: 1.850464]\n",
      "epoch:14 step:13687 [D loss: 0.653087, acc: 63.28%] [G loss: 1.867203]\n",
      "epoch:14 step:13688 [D loss: 0.637360, acc: 67.97%] [G loss: 1.867244]\n",
      "epoch:14 step:13689 [D loss: 0.633621, acc: 60.94%] [G loss: 2.028087]\n",
      "epoch:14 step:13690 [D loss: 0.634578, acc: 63.28%] [G loss: 1.818159]\n",
      "epoch:14 step:13691 [D loss: 0.603507, acc: 65.62%] [G loss: 1.903822]\n",
      "epoch:14 step:13692 [D loss: 0.642675, acc: 59.38%] [G loss: 2.061638]\n",
      "epoch:14 step:13693 [D loss: 0.666769, acc: 63.28%] [G loss: 1.897432]\n",
      "epoch:14 step:13694 [D loss: 0.659064, acc: 58.59%] [G loss: 1.784787]\n",
      "epoch:14 step:13695 [D loss: 0.642552, acc: 63.28%] [G loss: 1.736434]\n",
      "epoch:14 step:13696 [D loss: 0.688267, acc: 59.38%] [G loss: 1.825799]\n",
      "epoch:14 step:13697 [D loss: 0.639998, acc: 64.06%] [G loss: 1.884772]\n",
      "epoch:14 step:13698 [D loss: 0.659504, acc: 59.38%] [G loss: 1.885620]\n",
      "epoch:14 step:13699 [D loss: 0.699094, acc: 58.59%] [G loss: 1.866630]\n",
      "epoch:14 step:13700 [D loss: 0.639140, acc: 63.28%] [G loss: 2.028389]\n",
      "epoch:14 step:13701 [D loss: 0.659356, acc: 61.72%] [G loss: 1.878286]\n",
      "epoch:14 step:13702 [D loss: 0.633880, acc: 63.28%] [G loss: 1.848748]\n",
      "epoch:14 step:13703 [D loss: 0.646247, acc: 61.72%] [G loss: 1.940303]\n",
      "epoch:14 step:13704 [D loss: 0.644921, acc: 62.50%] [G loss: 1.814421]\n",
      "epoch:14 step:13705 [D loss: 0.690037, acc: 60.94%] [G loss: 1.843022]\n",
      "epoch:14 step:13706 [D loss: 0.610436, acc: 62.50%] [G loss: 2.106359]\n",
      "epoch:14 step:13707 [D loss: 0.612342, acc: 66.41%] [G loss: 1.896316]\n",
      "epoch:14 step:13708 [D loss: 0.717809, acc: 50.78%] [G loss: 1.845908]\n",
      "epoch:14 step:13709 [D loss: 0.616147, acc: 73.44%] [G loss: 2.005933]\n",
      "epoch:14 step:13710 [D loss: 0.600831, acc: 73.44%] [G loss: 2.038085]\n",
      "epoch:14 step:13711 [D loss: 0.654077, acc: 60.94%] [G loss: 1.997769]\n",
      "epoch:14 step:13712 [D loss: 0.738827, acc: 59.38%] [G loss: 1.846720]\n",
      "epoch:14 step:13713 [D loss: 0.641053, acc: 60.16%] [G loss: 1.789175]\n",
      "epoch:14 step:13714 [D loss: 0.691317, acc: 57.03%] [G loss: 1.750010]\n",
      "epoch:14 step:13715 [D loss: 0.652817, acc: 60.16%] [G loss: 1.770450]\n",
      "epoch:14 step:13716 [D loss: 0.622599, acc: 64.06%] [G loss: 1.872374]\n",
      "epoch:14 step:13717 [D loss: 0.673401, acc: 57.03%] [G loss: 1.713845]\n",
      "epoch:14 step:13718 [D loss: 0.713004, acc: 50.78%] [G loss: 1.832164]\n",
      "epoch:14 step:13719 [D loss: 0.685847, acc: 59.38%] [G loss: 1.866308]\n",
      "epoch:14 step:13720 [D loss: 0.637867, acc: 64.84%] [G loss: 1.903031]\n",
      "epoch:14 step:13721 [D loss: 0.616897, acc: 61.72%] [G loss: 1.955249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13722 [D loss: 0.665456, acc: 60.16%] [G loss: 1.889377]\n",
      "epoch:14 step:13723 [D loss: 0.629287, acc: 63.28%] [G loss: 1.936723]\n",
      "epoch:14 step:13724 [D loss: 0.654465, acc: 64.84%] [G loss: 1.756690]\n",
      "epoch:14 step:13725 [D loss: 0.675366, acc: 59.38%] [G loss: 2.019850]\n",
      "epoch:14 step:13726 [D loss: 0.623005, acc: 64.84%] [G loss: 1.943539]\n",
      "epoch:14 step:13727 [D loss: 0.652179, acc: 67.19%] [G loss: 1.875494]\n",
      "epoch:14 step:13728 [D loss: 0.687509, acc: 60.16%] [G loss: 1.813857]\n",
      "epoch:14 step:13729 [D loss: 0.648159, acc: 60.16%] [G loss: 1.785415]\n",
      "epoch:14 step:13730 [D loss: 0.694995, acc: 59.38%] [G loss: 1.672700]\n",
      "epoch:14 step:13731 [D loss: 0.614824, acc: 64.84%] [G loss: 1.794867]\n",
      "epoch:14 step:13732 [D loss: 0.676006, acc: 61.72%] [G loss: 1.705189]\n",
      "epoch:14 step:13733 [D loss: 0.636081, acc: 63.28%] [G loss: 1.801393]\n",
      "epoch:14 step:13734 [D loss: 0.639157, acc: 67.97%] [G loss: 1.852038]\n",
      "epoch:14 step:13735 [D loss: 0.694372, acc: 56.25%] [G loss: 1.787114]\n",
      "epoch:14 step:13736 [D loss: 0.636937, acc: 61.72%] [G loss: 1.958168]\n",
      "epoch:14 step:13737 [D loss: 0.672279, acc: 64.84%] [G loss: 1.773857]\n",
      "epoch:14 step:13738 [D loss: 0.643580, acc: 61.72%] [G loss: 1.816647]\n",
      "epoch:14 step:13739 [D loss: 0.659277, acc: 59.38%] [G loss: 1.840275]\n",
      "epoch:14 step:13740 [D loss: 0.593782, acc: 71.09%] [G loss: 2.078066]\n",
      "epoch:14 step:13741 [D loss: 0.657422, acc: 63.28%] [G loss: 2.020151]\n",
      "epoch:14 step:13742 [D loss: 0.644747, acc: 63.28%] [G loss: 1.937215]\n",
      "epoch:14 step:13743 [D loss: 0.689073, acc: 53.91%] [G loss: 1.709832]\n",
      "epoch:14 step:13744 [D loss: 0.610404, acc: 69.53%] [G loss: 1.896794]\n",
      "epoch:14 step:13745 [D loss: 0.627255, acc: 62.50%] [G loss: 1.928850]\n",
      "epoch:14 step:13746 [D loss: 0.676596, acc: 59.38%] [G loss: 1.829774]\n",
      "epoch:14 step:13747 [D loss: 0.632755, acc: 62.50%] [G loss: 1.895896]\n",
      "epoch:14 step:13748 [D loss: 0.698946, acc: 57.03%] [G loss: 1.875131]\n",
      "epoch:14 step:13749 [D loss: 0.588563, acc: 72.66%] [G loss: 1.956857]\n",
      "epoch:14 step:13750 [D loss: 0.650494, acc: 60.94%] [G loss: 2.013848]\n",
      "epoch:14 step:13751 [D loss: 0.644920, acc: 61.72%] [G loss: 1.950284]\n",
      "epoch:14 step:13752 [D loss: 0.595180, acc: 66.41%] [G loss: 1.986779]\n",
      "epoch:14 step:13753 [D loss: 0.609598, acc: 64.06%] [G loss: 1.922092]\n",
      "epoch:14 step:13754 [D loss: 0.627975, acc: 67.19%] [G loss: 1.918173]\n",
      "epoch:14 step:13755 [D loss: 0.587412, acc: 71.88%] [G loss: 1.992592]\n",
      "epoch:14 step:13756 [D loss: 0.589271, acc: 73.44%] [G loss: 2.076291]\n",
      "epoch:14 step:13757 [D loss: 0.673524, acc: 58.59%] [G loss: 1.923147]\n",
      "epoch:14 step:13758 [D loss: 0.646768, acc: 60.94%] [G loss: 1.943453]\n",
      "epoch:14 step:13759 [D loss: 0.576962, acc: 71.09%] [G loss: 2.060141]\n",
      "epoch:14 step:13760 [D loss: 0.625508, acc: 66.41%] [G loss: 2.114643]\n",
      "epoch:14 step:13761 [D loss: 0.658601, acc: 60.94%] [G loss: 1.925926]\n",
      "epoch:14 step:13762 [D loss: 0.635972, acc: 58.59%] [G loss: 2.116769]\n",
      "epoch:14 step:13763 [D loss: 0.661341, acc: 58.59%] [G loss: 1.950769]\n",
      "epoch:14 step:13764 [D loss: 0.638690, acc: 64.84%] [G loss: 1.982453]\n",
      "epoch:14 step:13765 [D loss: 0.659599, acc: 62.50%] [G loss: 2.283830]\n",
      "epoch:14 step:13766 [D loss: 0.590760, acc: 67.97%] [G loss: 2.214692]\n",
      "epoch:14 step:13767 [D loss: 0.629361, acc: 62.50%] [G loss: 2.035450]\n",
      "epoch:14 step:13768 [D loss: 0.588481, acc: 68.75%] [G loss: 2.046549]\n",
      "epoch:14 step:13769 [D loss: 0.637201, acc: 66.41%] [G loss: 2.050017]\n",
      "epoch:14 step:13770 [D loss: 0.605649, acc: 67.19%] [G loss: 1.957041]\n",
      "epoch:14 step:13771 [D loss: 0.647180, acc: 66.41%] [G loss: 1.993010]\n",
      "epoch:14 step:13772 [D loss: 0.605857, acc: 64.84%] [G loss: 2.018705]\n",
      "epoch:14 step:13773 [D loss: 0.618812, acc: 60.94%] [G loss: 1.909381]\n",
      "epoch:14 step:13774 [D loss: 0.639544, acc: 64.84%] [G loss: 1.802824]\n",
      "epoch:14 step:13775 [D loss: 0.659446, acc: 64.84%] [G loss: 1.778769]\n",
      "epoch:14 step:13776 [D loss: 0.653444, acc: 62.50%] [G loss: 1.722660]\n",
      "epoch:14 step:13777 [D loss: 0.691852, acc: 57.03%] [G loss: 1.959362]\n",
      "epoch:14 step:13778 [D loss: 0.649850, acc: 61.72%] [G loss: 1.944082]\n",
      "epoch:14 step:13779 [D loss: 0.597366, acc: 69.53%] [G loss: 1.990423]\n",
      "epoch:14 step:13780 [D loss: 0.668009, acc: 58.59%] [G loss: 1.841396]\n",
      "epoch:14 step:13781 [D loss: 0.621616, acc: 66.41%] [G loss: 1.912509]\n",
      "epoch:14 step:13782 [D loss: 0.620952, acc: 63.28%] [G loss: 1.933119]\n",
      "epoch:14 step:13783 [D loss: 0.621432, acc: 64.84%] [G loss: 2.047388]\n",
      "epoch:14 step:13784 [D loss: 0.707925, acc: 52.34%] [G loss: 1.955280]\n",
      "epoch:14 step:13785 [D loss: 0.647359, acc: 66.41%] [G loss: 1.823438]\n",
      "epoch:14 step:13786 [D loss: 0.620675, acc: 64.06%] [G loss: 1.946503]\n",
      "epoch:14 step:13787 [D loss: 0.644354, acc: 64.84%] [G loss: 1.885241]\n",
      "epoch:14 step:13788 [D loss: 0.670381, acc: 56.25%] [G loss: 1.880705]\n",
      "epoch:14 step:13789 [D loss: 0.642723, acc: 60.16%] [G loss: 1.994790]\n",
      "epoch:14 step:13790 [D loss: 0.699462, acc: 53.12%] [G loss: 1.922708]\n",
      "epoch:14 step:13791 [D loss: 0.678722, acc: 60.94%] [G loss: 1.891885]\n",
      "epoch:14 step:13792 [D loss: 0.619341, acc: 63.28%] [G loss: 1.973743]\n",
      "epoch:14 step:13793 [D loss: 0.621064, acc: 63.28%] [G loss: 1.924219]\n",
      "epoch:14 step:13794 [D loss: 0.601151, acc: 70.31%] [G loss: 1.888918]\n",
      "epoch:14 step:13795 [D loss: 0.591793, acc: 65.62%] [G loss: 2.052917]\n",
      "epoch:14 step:13796 [D loss: 0.651457, acc: 65.62%] [G loss: 1.847409]\n",
      "epoch:14 step:13797 [D loss: 0.596395, acc: 67.97%] [G loss: 1.811717]\n",
      "epoch:14 step:13798 [D loss: 0.671721, acc: 60.94%] [G loss: 1.934791]\n",
      "epoch:14 step:13799 [D loss: 0.615561, acc: 66.41%] [G loss: 1.874535]\n",
      "epoch:14 step:13800 [D loss: 0.697940, acc: 56.25%] [G loss: 1.889596]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.012264\n",
      "FID: 18.385201\n",
      "0 = 12.93660922498703\n",
      "1 = 0.09555540556735705\n",
      "2 = 0.8910999894142151\n",
      "3 = 0.8980000019073486\n",
      "4 = 0.8841999769210815\n",
      "5 = 0.8857762813568115\n",
      "6 = 0.8980000019073486\n",
      "7 = 6.919245199537278\n",
      "8 = 0.08411840345528274\n",
      "9 = 0.7394000291824341\n",
      "10 = 0.7437999844551086\n",
      "11 = 0.7350000143051147\n",
      "12 = 0.7373116612434387\n",
      "13 = 0.7437999844551086\n",
      "14 = 7.012295246124268\n",
      "15 = 9.434746742248535\n",
      "16 = 0.12329338490962982\n",
      "17 = 7.012264251708984\n",
      "18 = 18.38520050048828\n",
      "epoch:14 step:13801 [D loss: 0.665430, acc: 53.91%] [G loss: 1.857956]\n",
      "epoch:14 step:13802 [D loss: 0.721460, acc: 52.34%] [G loss: 1.779362]\n",
      "epoch:14 step:13803 [D loss: 0.635597, acc: 60.94%] [G loss: 1.902247]\n",
      "epoch:14 step:13804 [D loss: 0.647993, acc: 62.50%] [G loss: 1.810255]\n",
      "epoch:14 step:13805 [D loss: 0.576371, acc: 71.88%] [G loss: 1.890718]\n",
      "epoch:14 step:13806 [D loss: 0.670805, acc: 61.72%] [G loss: 1.774457]\n",
      "epoch:14 step:13807 [D loss: 0.682662, acc: 57.03%] [G loss: 1.820809]\n",
      "epoch:14 step:13808 [D loss: 0.671792, acc: 61.72%] [G loss: 2.068896]\n",
      "epoch:14 step:13809 [D loss: 0.633558, acc: 64.06%] [G loss: 1.987838]\n",
      "epoch:14 step:13810 [D loss: 0.587349, acc: 71.09%] [G loss: 1.985352]\n",
      "epoch:14 step:13811 [D loss: 0.588992, acc: 66.41%] [G loss: 2.036873]\n",
      "epoch:14 step:13812 [D loss: 0.595413, acc: 68.75%] [G loss: 1.966901]\n",
      "epoch:14 step:13813 [D loss: 0.639467, acc: 63.28%] [G loss: 2.004309]\n",
      "epoch:14 step:13814 [D loss: 0.636949, acc: 67.19%] [G loss: 1.940528]\n",
      "epoch:14 step:13815 [D loss: 0.619632, acc: 67.19%] [G loss: 1.981339]\n",
      "epoch:14 step:13816 [D loss: 0.661888, acc: 59.38%] [G loss: 1.964232]\n",
      "epoch:14 step:13817 [D loss: 0.625318, acc: 64.06%] [G loss: 2.031369]\n",
      "epoch:14 step:13818 [D loss: 0.592614, acc: 67.19%] [G loss: 2.018632]\n",
      "epoch:14 step:13819 [D loss: 0.604391, acc: 67.97%] [G loss: 2.024179]\n",
      "epoch:14 step:13820 [D loss: 0.660269, acc: 63.28%] [G loss: 1.868452]\n",
      "epoch:14 step:13821 [D loss: 0.677145, acc: 58.59%] [G loss: 1.877538]\n",
      "epoch:14 step:13822 [D loss: 0.652151, acc: 58.59%] [G loss: 1.819120]\n",
      "epoch:14 step:13823 [D loss: 0.629231, acc: 63.28%] [G loss: 1.927099]\n",
      "epoch:14 step:13824 [D loss: 0.639808, acc: 65.62%] [G loss: 1.987665]\n",
      "epoch:14 step:13825 [D loss: 0.614526, acc: 64.84%] [G loss: 2.109875]\n",
      "epoch:14 step:13826 [D loss: 0.619368, acc: 67.19%] [G loss: 2.100254]\n",
      "epoch:14 step:13827 [D loss: 0.604079, acc: 64.84%] [G loss: 1.974295]\n",
      "epoch:14 step:13828 [D loss: 0.634581, acc: 59.38%] [G loss: 1.988899]\n",
      "epoch:14 step:13829 [D loss: 0.621254, acc: 69.53%] [G loss: 1.980655]\n",
      "epoch:14 step:13830 [D loss: 0.619931, acc: 67.19%] [G loss: 2.105766]\n",
      "epoch:14 step:13831 [D loss: 0.678699, acc: 63.28%] [G loss: 2.004820]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13832 [D loss: 0.605517, acc: 67.97%] [G loss: 2.018217]\n",
      "epoch:14 step:13833 [D loss: 0.741429, acc: 54.69%] [G loss: 1.861304]\n",
      "epoch:14 step:13834 [D loss: 0.668150, acc: 59.38%] [G loss: 1.871135]\n",
      "epoch:14 step:13835 [D loss: 0.686921, acc: 57.03%] [G loss: 1.854011]\n",
      "epoch:14 step:13836 [D loss: 0.626474, acc: 60.16%] [G loss: 1.917477]\n",
      "epoch:14 step:13837 [D loss: 0.613507, acc: 67.19%] [G loss: 2.143972]\n",
      "epoch:14 step:13838 [D loss: 0.676570, acc: 60.16%] [G loss: 1.987255]\n",
      "epoch:14 step:13839 [D loss: 0.658222, acc: 53.12%] [G loss: 2.117475]\n",
      "epoch:14 step:13840 [D loss: 0.683846, acc: 56.25%] [G loss: 1.969503]\n",
      "epoch:14 step:13841 [D loss: 0.666510, acc: 62.50%] [G loss: 2.005804]\n",
      "epoch:14 step:13842 [D loss: 0.621888, acc: 61.72%] [G loss: 2.081699]\n",
      "epoch:14 step:13843 [D loss: 0.676226, acc: 61.72%] [G loss: 2.041042]\n",
      "epoch:14 step:13844 [D loss: 0.624678, acc: 67.19%] [G loss: 1.926641]\n",
      "epoch:14 step:13845 [D loss: 0.686590, acc: 60.16%] [G loss: 1.881891]\n",
      "epoch:14 step:13846 [D loss: 0.639638, acc: 63.28%] [G loss: 1.939405]\n",
      "epoch:14 step:13847 [D loss: 0.693903, acc: 56.25%] [G loss: 1.709932]\n",
      "epoch:14 step:13848 [D loss: 0.589615, acc: 67.97%] [G loss: 1.950860]\n",
      "epoch:14 step:13849 [D loss: 0.596604, acc: 64.84%] [G loss: 1.826861]\n",
      "epoch:14 step:13850 [D loss: 0.617088, acc: 65.62%] [G loss: 2.022285]\n",
      "epoch:14 step:13851 [D loss: 0.605731, acc: 67.19%] [G loss: 1.905757]\n",
      "epoch:14 step:13852 [D loss: 0.718035, acc: 51.56%] [G loss: 1.916802]\n",
      "epoch:14 step:13853 [D loss: 0.668627, acc: 57.03%] [G loss: 1.848590]\n",
      "epoch:14 step:13854 [D loss: 0.639931, acc: 63.28%] [G loss: 1.970243]\n",
      "epoch:14 step:13855 [D loss: 0.637588, acc: 62.50%] [G loss: 1.934676]\n",
      "epoch:14 step:13856 [D loss: 0.655457, acc: 57.81%] [G loss: 1.781458]\n",
      "epoch:14 step:13857 [D loss: 0.633247, acc: 68.75%] [G loss: 1.856121]\n",
      "epoch:14 step:13858 [D loss: 0.619013, acc: 68.75%] [G loss: 1.914986]\n",
      "epoch:14 step:13859 [D loss: 0.685930, acc: 57.03%] [G loss: 1.924870]\n",
      "epoch:14 step:13860 [D loss: 0.669163, acc: 59.38%] [G loss: 1.927792]\n",
      "epoch:14 step:13861 [D loss: 0.684375, acc: 61.72%] [G loss: 1.906612]\n",
      "epoch:14 step:13862 [D loss: 0.643788, acc: 62.50%] [G loss: 1.815292]\n",
      "epoch:14 step:13863 [D loss: 0.660940, acc: 57.03%] [G loss: 1.815735]\n",
      "epoch:14 step:13864 [D loss: 0.637222, acc: 67.97%] [G loss: 2.031335]\n",
      "epoch:14 step:13865 [D loss: 0.700399, acc: 64.84%] [G loss: 2.041688]\n",
      "epoch:14 step:13866 [D loss: 0.617158, acc: 67.97%] [G loss: 1.852205]\n",
      "epoch:14 step:13867 [D loss: 0.643711, acc: 59.38%] [G loss: 1.863449]\n",
      "epoch:14 step:13868 [D loss: 0.648819, acc: 58.59%] [G loss: 1.802610]\n",
      "epoch:14 step:13869 [D loss: 0.682117, acc: 64.06%] [G loss: 1.820838]\n",
      "epoch:14 step:13870 [D loss: 0.674762, acc: 55.47%] [G loss: 1.742292]\n",
      "epoch:14 step:13871 [D loss: 0.650689, acc: 63.28%] [G loss: 1.781011]\n",
      "epoch:14 step:13872 [D loss: 0.680818, acc: 58.59%] [G loss: 1.908442]\n",
      "epoch:14 step:13873 [D loss: 0.607903, acc: 67.19%] [G loss: 1.886397]\n",
      "epoch:14 step:13874 [D loss: 0.614644, acc: 60.94%] [G loss: 1.831224]\n",
      "epoch:14 step:13875 [D loss: 0.644589, acc: 67.19%] [G loss: 1.926617]\n",
      "epoch:14 step:13876 [D loss: 0.705135, acc: 53.12%] [G loss: 1.784197]\n",
      "epoch:14 step:13877 [D loss: 0.649832, acc: 60.16%] [G loss: 1.809013]\n",
      "epoch:14 step:13878 [D loss: 0.666908, acc: 55.47%] [G loss: 1.880028]\n",
      "epoch:14 step:13879 [D loss: 0.656889, acc: 60.16%] [G loss: 1.823081]\n",
      "epoch:14 step:13880 [D loss: 0.666547, acc: 58.59%] [G loss: 1.799735]\n",
      "epoch:14 step:13881 [D loss: 0.577167, acc: 73.44%] [G loss: 1.957838]\n",
      "epoch:14 step:13882 [D loss: 0.681206, acc: 55.47%] [G loss: 1.914800]\n",
      "epoch:14 step:13883 [D loss: 0.767351, acc: 50.78%] [G loss: 1.747376]\n",
      "epoch:14 step:13884 [D loss: 0.675003, acc: 60.16%] [G loss: 1.806135]\n",
      "epoch:14 step:13885 [D loss: 0.636638, acc: 66.41%] [G loss: 1.942775]\n",
      "epoch:14 step:13886 [D loss: 0.654000, acc: 63.28%] [G loss: 1.744716]\n",
      "epoch:14 step:13887 [D loss: 0.643591, acc: 58.59%] [G loss: 1.828343]\n",
      "epoch:14 step:13888 [D loss: 0.627222, acc: 69.53%] [G loss: 1.878134]\n",
      "epoch:14 step:13889 [D loss: 0.713916, acc: 53.12%] [G loss: 1.761286]\n",
      "epoch:14 step:13890 [D loss: 0.640937, acc: 66.41%] [G loss: 1.782463]\n",
      "epoch:14 step:13891 [D loss: 0.641830, acc: 61.72%] [G loss: 1.885762]\n",
      "epoch:14 step:13892 [D loss: 0.673314, acc: 57.81%] [G loss: 1.952451]\n",
      "epoch:14 step:13893 [D loss: 0.590736, acc: 68.75%] [G loss: 2.042991]\n",
      "epoch:14 step:13894 [D loss: 0.692142, acc: 52.34%] [G loss: 1.932738]\n",
      "epoch:14 step:13895 [D loss: 0.638429, acc: 62.50%] [G loss: 1.938505]\n",
      "epoch:14 step:13896 [D loss: 0.577166, acc: 73.44%] [G loss: 1.937708]\n",
      "epoch:14 step:13897 [D loss: 0.642570, acc: 62.50%] [G loss: 1.910055]\n",
      "epoch:14 step:13898 [D loss: 0.676616, acc: 60.16%] [G loss: 2.039460]\n",
      "epoch:14 step:13899 [D loss: 0.589422, acc: 71.88%] [G loss: 1.958468]\n",
      "epoch:14 step:13900 [D loss: 0.612103, acc: 70.31%] [G loss: 2.214661]\n",
      "epoch:14 step:13901 [D loss: 0.690825, acc: 57.03%] [G loss: 2.048819]\n",
      "epoch:14 step:13902 [D loss: 0.706466, acc: 55.47%] [G loss: 1.792452]\n",
      "epoch:14 step:13903 [D loss: 0.648911, acc: 66.41%] [G loss: 1.882541]\n",
      "epoch:14 step:13904 [D loss: 0.579814, acc: 69.53%] [G loss: 2.108778]\n",
      "epoch:14 step:13905 [D loss: 0.644397, acc: 60.16%] [G loss: 1.973856]\n",
      "epoch:14 step:13906 [D loss: 0.653803, acc: 60.94%] [G loss: 1.796497]\n",
      "epoch:14 step:13907 [D loss: 0.609617, acc: 67.19%] [G loss: 1.964201]\n",
      "epoch:14 step:13908 [D loss: 0.612790, acc: 68.75%] [G loss: 1.970212]\n",
      "epoch:14 step:13909 [D loss: 0.684589, acc: 58.59%] [G loss: 1.981761]\n",
      "epoch:14 step:13910 [D loss: 0.614782, acc: 69.53%] [G loss: 2.220854]\n",
      "epoch:14 step:13911 [D loss: 0.607309, acc: 69.53%] [G loss: 2.095042]\n",
      "epoch:14 step:13912 [D loss: 0.682519, acc: 53.91%] [G loss: 1.831698]\n",
      "epoch:14 step:13913 [D loss: 0.721071, acc: 46.09%] [G loss: 1.839946]\n",
      "epoch:14 step:13914 [D loss: 0.621139, acc: 68.75%] [G loss: 1.854834]\n",
      "epoch:14 step:13915 [D loss: 0.661792, acc: 59.38%] [G loss: 1.936011]\n",
      "epoch:14 step:13916 [D loss: 0.660273, acc: 62.50%] [G loss: 1.969349]\n",
      "epoch:14 step:13917 [D loss: 0.689317, acc: 59.38%] [G loss: 1.868144]\n",
      "epoch:14 step:13918 [D loss: 0.671003, acc: 58.59%] [G loss: 1.910318]\n",
      "epoch:14 step:13919 [D loss: 0.629695, acc: 59.38%] [G loss: 1.894436]\n",
      "epoch:14 step:13920 [D loss: 0.668635, acc: 60.94%] [G loss: 1.916015]\n",
      "epoch:14 step:13921 [D loss: 0.605112, acc: 69.53%] [G loss: 2.036087]\n",
      "epoch:14 step:13922 [D loss: 0.590588, acc: 67.97%] [G loss: 1.843711]\n",
      "epoch:14 step:13923 [D loss: 0.602759, acc: 67.19%] [G loss: 2.073767]\n",
      "epoch:14 step:13924 [D loss: 0.582123, acc: 63.28%] [G loss: 2.043375]\n",
      "epoch:14 step:13925 [D loss: 0.617751, acc: 66.41%] [G loss: 1.927715]\n",
      "epoch:14 step:13926 [D loss: 0.690954, acc: 58.59%] [G loss: 1.979301]\n",
      "epoch:14 step:13927 [D loss: 0.599530, acc: 71.88%] [G loss: 1.993649]\n",
      "epoch:14 step:13928 [D loss: 0.636898, acc: 64.06%] [G loss: 1.954425]\n",
      "epoch:14 step:13929 [D loss: 0.646905, acc: 61.72%] [G loss: 1.833436]\n",
      "epoch:14 step:13930 [D loss: 0.700719, acc: 55.47%] [G loss: 1.858545]\n",
      "epoch:14 step:13931 [D loss: 0.653190, acc: 61.72%] [G loss: 1.913754]\n",
      "epoch:14 step:13932 [D loss: 0.640900, acc: 66.41%] [G loss: 1.933291]\n",
      "epoch:14 step:13933 [D loss: 0.609432, acc: 69.53%] [G loss: 2.222157]\n",
      "epoch:14 step:13934 [D loss: 0.620219, acc: 66.41%] [G loss: 2.025640]\n",
      "epoch:14 step:13935 [D loss: 0.663186, acc: 60.16%] [G loss: 1.866258]\n",
      "epoch:14 step:13936 [D loss: 0.719544, acc: 56.25%] [G loss: 1.887937]\n",
      "epoch:14 step:13937 [D loss: 0.626143, acc: 64.84%] [G loss: 1.929979]\n",
      "epoch:14 step:13938 [D loss: 0.666066, acc: 57.81%] [G loss: 1.753223]\n",
      "epoch:14 step:13939 [D loss: 0.664564, acc: 60.16%] [G loss: 1.932327]\n",
      "epoch:14 step:13940 [D loss: 0.674981, acc: 67.19%] [G loss: 1.951987]\n",
      "epoch:14 step:13941 [D loss: 0.620649, acc: 68.75%] [G loss: 1.899388]\n",
      "epoch:14 step:13942 [D loss: 0.604355, acc: 69.53%] [G loss: 1.840107]\n",
      "epoch:14 step:13943 [D loss: 0.600507, acc: 71.09%] [G loss: 1.913831]\n",
      "epoch:14 step:13944 [D loss: 0.663103, acc: 60.16%] [G loss: 1.993740]\n",
      "epoch:14 step:13945 [D loss: 0.623784, acc: 67.19%] [G loss: 1.779440]\n",
      "epoch:14 step:13946 [D loss: 0.680918, acc: 58.59%] [G loss: 1.750635]\n",
      "epoch:14 step:13947 [D loss: 0.639716, acc: 67.19%] [G loss: 1.864276]\n",
      "epoch:14 step:13948 [D loss: 0.664346, acc: 61.72%] [G loss: 1.815678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13949 [D loss: 0.665100, acc: 57.03%] [G loss: 1.975076]\n",
      "epoch:14 step:13950 [D loss: 0.652430, acc: 64.06%] [G loss: 1.844746]\n",
      "epoch:14 step:13951 [D loss: 0.629933, acc: 64.84%] [G loss: 2.012836]\n",
      "epoch:14 step:13952 [D loss: 0.612305, acc: 63.28%] [G loss: 1.914051]\n",
      "epoch:14 step:13953 [D loss: 0.647451, acc: 65.62%] [G loss: 1.892695]\n",
      "epoch:14 step:13954 [D loss: 0.642626, acc: 60.94%] [G loss: 1.779490]\n",
      "epoch:14 step:13955 [D loss: 0.628752, acc: 67.97%] [G loss: 1.953319]\n",
      "epoch:14 step:13956 [D loss: 0.633918, acc: 66.41%] [G loss: 1.888939]\n",
      "epoch:14 step:13957 [D loss: 0.711851, acc: 57.03%] [G loss: 1.805327]\n",
      "epoch:14 step:13958 [D loss: 0.614436, acc: 66.41%] [G loss: 2.030144]\n",
      "epoch:14 step:13959 [D loss: 0.621624, acc: 60.94%] [G loss: 1.801687]\n",
      "epoch:14 step:13960 [D loss: 0.665842, acc: 57.81%] [G loss: 2.028102]\n",
      "epoch:14 step:13961 [D loss: 0.617865, acc: 67.97%] [G loss: 2.049970]\n",
      "epoch:14 step:13962 [D loss: 0.690362, acc: 57.81%] [G loss: 1.967598]\n",
      "epoch:14 step:13963 [D loss: 0.732884, acc: 55.47%] [G loss: 2.041149]\n",
      "epoch:14 step:13964 [D loss: 0.646784, acc: 60.94%] [G loss: 1.854875]\n",
      "epoch:14 step:13965 [D loss: 0.653461, acc: 62.50%] [G loss: 1.811128]\n",
      "epoch:14 step:13966 [D loss: 0.634606, acc: 63.28%] [G loss: 1.981285]\n",
      "epoch:14 step:13967 [D loss: 0.654077, acc: 58.59%] [G loss: 1.831908]\n",
      "epoch:14 step:13968 [D loss: 0.650935, acc: 60.94%] [G loss: 1.813319]\n",
      "epoch:14 step:13969 [D loss: 0.685504, acc: 53.91%] [G loss: 1.777883]\n",
      "epoch:14 step:13970 [D loss: 0.653275, acc: 57.03%] [G loss: 1.823174]\n",
      "epoch:14 step:13971 [D loss: 0.642482, acc: 61.72%] [G loss: 1.870402]\n",
      "epoch:14 step:13972 [D loss: 0.620835, acc: 61.72%] [G loss: 1.998318]\n",
      "epoch:14 step:13973 [D loss: 0.681466, acc: 56.25%] [G loss: 1.863106]\n",
      "epoch:14 step:13974 [D loss: 0.629385, acc: 60.94%] [G loss: 1.786279]\n",
      "epoch:14 step:13975 [D loss: 0.652788, acc: 66.41%] [G loss: 1.903788]\n",
      "epoch:14 step:13976 [D loss: 0.729229, acc: 53.91%] [G loss: 1.905336]\n",
      "epoch:14 step:13977 [D loss: 0.668495, acc: 55.47%] [G loss: 1.815375]\n",
      "epoch:14 step:13978 [D loss: 0.611563, acc: 68.75%] [G loss: 1.848266]\n",
      "epoch:14 step:13979 [D loss: 0.667369, acc: 59.38%] [G loss: 1.798299]\n",
      "epoch:14 step:13980 [D loss: 0.602938, acc: 71.88%] [G loss: 1.785549]\n",
      "epoch:14 step:13981 [D loss: 0.636905, acc: 60.94%] [G loss: 1.829365]\n",
      "epoch:14 step:13982 [D loss: 0.623408, acc: 67.97%] [G loss: 1.899290]\n",
      "epoch:14 step:13983 [D loss: 0.641334, acc: 61.72%] [G loss: 1.944596]\n",
      "epoch:14 step:13984 [D loss: 0.638172, acc: 56.25%] [G loss: 1.823830]\n",
      "epoch:14 step:13985 [D loss: 0.709596, acc: 53.91%] [G loss: 1.884309]\n",
      "epoch:14 step:13986 [D loss: 0.648969, acc: 59.38%] [G loss: 1.824378]\n",
      "epoch:14 step:13987 [D loss: 0.644146, acc: 65.62%] [G loss: 1.935115]\n",
      "epoch:14 step:13988 [D loss: 0.650442, acc: 61.72%] [G loss: 1.957073]\n",
      "epoch:14 step:13989 [D loss: 0.692550, acc: 57.81%] [G loss: 1.817525]\n",
      "epoch:14 step:13990 [D loss: 0.647739, acc: 64.84%] [G loss: 1.835305]\n",
      "epoch:14 step:13991 [D loss: 0.658567, acc: 63.28%] [G loss: 1.895759]\n",
      "epoch:14 step:13992 [D loss: 0.663449, acc: 59.38%] [G loss: 1.874511]\n",
      "epoch:14 step:13993 [D loss: 0.627183, acc: 67.97%] [G loss: 2.087705]\n",
      "epoch:14 step:13994 [D loss: 0.600306, acc: 71.88%] [G loss: 1.928299]\n",
      "epoch:14 step:13995 [D loss: 0.631960, acc: 64.06%] [G loss: 1.793768]\n",
      "epoch:14 step:13996 [D loss: 0.683243, acc: 57.81%] [G loss: 1.874009]\n",
      "epoch:14 step:13997 [D loss: 0.611349, acc: 63.28%] [G loss: 1.961020]\n",
      "epoch:14 step:13998 [D loss: 0.658957, acc: 62.50%] [G loss: 1.893983]\n",
      "epoch:14 step:13999 [D loss: 0.643493, acc: 60.94%] [G loss: 1.845500]\n",
      "epoch:14 step:14000 [D loss: 0.621081, acc: 60.94%] [G loss: 2.001635]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.231924\n",
      "FID: 15.156081\n",
      "0 = 12.663603585672353\n",
      "1 = 0.08383164824150369\n",
      "2 = 0.8802000284194946\n",
      "3 = 0.8984000086784363\n",
      "4 = 0.8619999885559082\n",
      "5 = 0.8668467998504639\n",
      "6 = 0.8984000086784363\n",
      "7 = 6.679188939547528\n",
      "8 = 0.07471740251743657\n",
      "9 = 0.7312999963760376\n",
      "10 = 0.7437999844551086\n",
      "11 = 0.7188000082969666\n",
      "12 = 0.7256585359573364\n",
      "13 = 0.7437999844551086\n",
      "14 = 7.2319536209106445\n",
      "15 = 9.462989807128906\n",
      "16 = 0.10982239246368408\n",
      "17 = 7.231923580169678\n",
      "18 = 15.156081199645996\n",
      "epoch:14 step:14001 [D loss: 0.721405, acc: 55.47%] [G loss: 1.854781]\n",
      "epoch:14 step:14002 [D loss: 0.643729, acc: 63.28%] [G loss: 2.094016]\n",
      "epoch:14 step:14003 [D loss: 0.606498, acc: 64.84%] [G loss: 1.871557]\n",
      "epoch:14 step:14004 [D loss: 0.661737, acc: 60.16%] [G loss: 1.933889]\n",
      "epoch:14 step:14005 [D loss: 0.658491, acc: 61.72%] [G loss: 1.861255]\n",
      "epoch:14 step:14006 [D loss: 0.637390, acc: 61.72%] [G loss: 1.865110]\n",
      "epoch:14 step:14007 [D loss: 0.633181, acc: 67.19%] [G loss: 1.824448]\n",
      "epoch:14 step:14008 [D loss: 0.656873, acc: 60.16%] [G loss: 1.933401]\n",
      "epoch:14 step:14009 [D loss: 0.689883, acc: 58.59%] [G loss: 1.812719]\n",
      "epoch:14 step:14010 [D loss: 0.638886, acc: 61.72%] [G loss: 1.924101]\n",
      "epoch:14 step:14011 [D loss: 0.693767, acc: 57.81%] [G loss: 1.978563]\n",
      "epoch:14 step:14012 [D loss: 0.630704, acc: 64.06%] [G loss: 1.928417]\n",
      "epoch:14 step:14013 [D loss: 0.606991, acc: 64.84%] [G loss: 1.894607]\n",
      "epoch:14 step:14014 [D loss: 0.659079, acc: 62.50%] [G loss: 1.994314]\n",
      "epoch:14 step:14015 [D loss: 0.641882, acc: 62.50%] [G loss: 1.980114]\n",
      "epoch:14 step:14016 [D loss: 0.672874, acc: 60.94%] [G loss: 1.936214]\n",
      "epoch:14 step:14017 [D loss: 0.605215, acc: 66.41%] [G loss: 2.059812]\n",
      "epoch:14 step:14018 [D loss: 0.640760, acc: 66.41%] [G loss: 1.975646]\n",
      "epoch:14 step:14019 [D loss: 0.615328, acc: 69.53%] [G loss: 2.003506]\n",
      "epoch:14 step:14020 [D loss: 0.660359, acc: 64.06%] [G loss: 1.871117]\n",
      "epoch:14 step:14021 [D loss: 0.634405, acc: 63.28%] [G loss: 1.940881]\n",
      "epoch:14 step:14022 [D loss: 0.654340, acc: 63.28%] [G loss: 2.018381]\n",
      "epoch:14 step:14023 [D loss: 0.645834, acc: 57.81%] [G loss: 1.903869]\n",
      "epoch:14 step:14024 [D loss: 0.615964, acc: 67.19%] [G loss: 2.059459]\n",
      "epoch:14 step:14025 [D loss: 0.616952, acc: 68.75%] [G loss: 2.077412]\n",
      "epoch:14 step:14026 [D loss: 0.669072, acc: 60.16%] [G loss: 1.997015]\n",
      "epoch:14 step:14027 [D loss: 0.579807, acc: 67.97%] [G loss: 1.905772]\n",
      "epoch:14 step:14028 [D loss: 0.647319, acc: 62.50%] [G loss: 2.043663]\n",
      "epoch:14 step:14029 [D loss: 0.580018, acc: 68.75%] [G loss: 2.110931]\n",
      "epoch:14 step:14030 [D loss: 0.632397, acc: 71.09%] [G loss: 2.062257]\n",
      "epoch:14 step:14031 [D loss: 0.700934, acc: 53.91%] [G loss: 1.943409]\n",
      "epoch:14 step:14032 [D loss: 0.665737, acc: 56.25%] [G loss: 2.044823]\n",
      "epoch:14 step:14033 [D loss: 0.657859, acc: 60.94%] [G loss: 2.058936]\n",
      "epoch:14 step:14034 [D loss: 0.599687, acc: 67.19%] [G loss: 1.998856]\n",
      "epoch:14 step:14035 [D loss: 0.641260, acc: 63.28%] [G loss: 2.026240]\n",
      "epoch:14 step:14036 [D loss: 0.668904, acc: 58.59%] [G loss: 2.001089]\n",
      "epoch:14 step:14037 [D loss: 0.608423, acc: 65.62%] [G loss: 2.250727]\n",
      "epoch:14 step:14038 [D loss: 0.766756, acc: 52.34%] [G loss: 1.878760]\n",
      "epoch:14 step:14039 [D loss: 0.644282, acc: 57.81%] [G loss: 2.039216]\n",
      "epoch:14 step:14040 [D loss: 0.606870, acc: 68.75%] [G loss: 2.058964]\n",
      "epoch:14 step:14041 [D loss: 0.554069, acc: 72.66%] [G loss: 2.152866]\n",
      "epoch:14 step:14042 [D loss: 0.576951, acc: 74.22%] [G loss: 2.304530]\n",
      "epoch:14 step:14043 [D loss: 0.643879, acc: 67.19%] [G loss: 2.175043]\n",
      "epoch:14 step:14044 [D loss: 0.593466, acc: 64.06%] [G loss: 2.004004]\n",
      "epoch:14 step:14045 [D loss: 0.627007, acc: 60.94%] [G loss: 2.139862]\n",
      "epoch:14 step:14046 [D loss: 0.777497, acc: 52.34%] [G loss: 1.777105]\n",
      "epoch:14 step:14047 [D loss: 0.688504, acc: 53.12%] [G loss: 1.914273]\n",
      "epoch:14 step:14048 [D loss: 0.602604, acc: 63.28%] [G loss: 1.999922]\n",
      "epoch:14 step:14049 [D loss: 0.644673, acc: 60.16%] [G loss: 2.018166]\n",
      "epoch:14 step:14050 [D loss: 0.607129, acc: 66.41%] [G loss: 1.963143]\n",
      "epoch:14 step:14051 [D loss: 0.629253, acc: 65.62%] [G loss: 2.084318]\n",
      "epoch:14 step:14052 [D loss: 0.618500, acc: 67.19%] [G loss: 1.952827]\n",
      "epoch:14 step:14053 [D loss: 0.657232, acc: 64.06%] [G loss: 1.985702]\n",
      "epoch:14 step:14054 [D loss: 0.619948, acc: 68.75%] [G loss: 2.092185]\n",
      "epoch:14 step:14055 [D loss: 0.614981, acc: 70.31%] [G loss: 2.232596]\n",
      "epoch:15 step:14056 [D loss: 0.655000, acc: 60.94%] [G loss: 1.851203]\n",
      "epoch:15 step:14057 [D loss: 0.640370, acc: 59.38%] [G loss: 2.002723]\n",
      "epoch:15 step:14058 [D loss: 0.671770, acc: 58.59%] [G loss: 2.037631]\n",
      "epoch:15 step:14059 [D loss: 0.665003, acc: 57.81%] [G loss: 2.028640]\n",
      "epoch:15 step:14060 [D loss: 0.609437, acc: 66.41%] [G loss: 1.922927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14061 [D loss: 0.637290, acc: 64.84%] [G loss: 1.999764]\n",
      "epoch:15 step:14062 [D loss: 0.672587, acc: 59.38%] [G loss: 1.871053]\n",
      "epoch:15 step:14063 [D loss: 0.688610, acc: 53.91%] [G loss: 1.873525]\n",
      "epoch:15 step:14064 [D loss: 0.621592, acc: 68.75%] [G loss: 2.034139]\n",
      "epoch:15 step:14065 [D loss: 0.567396, acc: 71.09%] [G loss: 2.066012]\n",
      "epoch:15 step:14066 [D loss: 0.661075, acc: 63.28%] [G loss: 1.976785]\n",
      "epoch:15 step:14067 [D loss: 0.645864, acc: 64.06%] [G loss: 1.923604]\n",
      "epoch:15 step:14068 [D loss: 0.659332, acc: 57.81%] [G loss: 1.766594]\n",
      "epoch:15 step:14069 [D loss: 0.670249, acc: 58.59%] [G loss: 1.988603]\n",
      "epoch:15 step:14070 [D loss: 0.535771, acc: 75.00%] [G loss: 2.256682]\n",
      "epoch:15 step:14071 [D loss: 0.627407, acc: 60.94%] [G loss: 2.053814]\n",
      "epoch:15 step:14072 [D loss: 0.631033, acc: 65.62%] [G loss: 1.932949]\n",
      "epoch:15 step:14073 [D loss: 0.646191, acc: 60.16%] [G loss: 1.890409]\n",
      "epoch:15 step:14074 [D loss: 0.649170, acc: 64.06%] [G loss: 1.797646]\n",
      "epoch:15 step:14075 [D loss: 0.653658, acc: 59.38%] [G loss: 1.808036]\n",
      "epoch:15 step:14076 [D loss: 0.642558, acc: 60.94%] [G loss: 2.035258]\n",
      "epoch:15 step:14077 [D loss: 0.666732, acc: 59.38%] [G loss: 1.956368]\n",
      "epoch:15 step:14078 [D loss: 0.675231, acc: 57.03%] [G loss: 2.117532]\n",
      "epoch:15 step:14079 [D loss: 0.574882, acc: 73.44%] [G loss: 2.088015]\n",
      "epoch:15 step:14080 [D loss: 0.669447, acc: 59.38%] [G loss: 2.067860]\n",
      "epoch:15 step:14081 [D loss: 0.668821, acc: 61.72%] [G loss: 1.926770]\n",
      "epoch:15 step:14082 [D loss: 0.659650, acc: 64.06%] [G loss: 1.965555]\n",
      "epoch:15 step:14083 [D loss: 0.618807, acc: 65.62%] [G loss: 1.782534]\n",
      "epoch:15 step:14084 [D loss: 0.598071, acc: 69.53%] [G loss: 1.941332]\n",
      "epoch:15 step:14085 [D loss: 0.642489, acc: 64.06%] [G loss: 1.990208]\n",
      "epoch:15 step:14086 [D loss: 0.626144, acc: 58.59%] [G loss: 1.769766]\n",
      "epoch:15 step:14087 [D loss: 0.642152, acc: 67.19%] [G loss: 1.781784]\n",
      "epoch:15 step:14088 [D loss: 0.645956, acc: 61.72%] [G loss: 1.806377]\n",
      "epoch:15 step:14089 [D loss: 0.646895, acc: 67.19%] [G loss: 1.834451]\n",
      "epoch:15 step:14090 [D loss: 0.656485, acc: 63.28%] [G loss: 1.972502]\n",
      "epoch:15 step:14091 [D loss: 0.629862, acc: 63.28%] [G loss: 1.985465]\n",
      "epoch:15 step:14092 [D loss: 0.673252, acc: 62.50%] [G loss: 1.962725]\n",
      "epoch:15 step:14093 [D loss: 0.665071, acc: 60.94%] [G loss: 1.733883]\n",
      "epoch:15 step:14094 [D loss: 0.687793, acc: 58.59%] [G loss: 1.896194]\n",
      "epoch:15 step:14095 [D loss: 0.633312, acc: 67.19%] [G loss: 2.065598]\n",
      "epoch:15 step:14096 [D loss: 0.616111, acc: 67.19%] [G loss: 1.860394]\n",
      "epoch:15 step:14097 [D loss: 0.581719, acc: 73.44%] [G loss: 2.042260]\n",
      "epoch:15 step:14098 [D loss: 0.663108, acc: 63.28%] [G loss: 1.799055]\n",
      "epoch:15 step:14099 [D loss: 0.650327, acc: 66.41%] [G loss: 1.756281]\n",
      "epoch:15 step:14100 [D loss: 0.641751, acc: 69.53%] [G loss: 1.963372]\n",
      "epoch:15 step:14101 [D loss: 0.686797, acc: 57.03%] [G loss: 1.765791]\n",
      "epoch:15 step:14102 [D loss: 0.622318, acc: 61.72%] [G loss: 1.988674]\n",
      "epoch:15 step:14103 [D loss: 0.597696, acc: 69.53%] [G loss: 1.914261]\n",
      "epoch:15 step:14104 [D loss: 0.612882, acc: 68.75%] [G loss: 1.935645]\n",
      "epoch:15 step:14105 [D loss: 0.639007, acc: 64.06%] [G loss: 1.957153]\n",
      "epoch:15 step:14106 [D loss: 0.643082, acc: 63.28%] [G loss: 1.935658]\n",
      "epoch:15 step:14107 [D loss: 0.628650, acc: 63.28%] [G loss: 1.957504]\n",
      "epoch:15 step:14108 [D loss: 0.624423, acc: 62.50%] [G loss: 1.889688]\n",
      "epoch:15 step:14109 [D loss: 0.630898, acc: 70.31%] [G loss: 2.046834]\n",
      "epoch:15 step:14110 [D loss: 0.629884, acc: 62.50%] [G loss: 2.031326]\n",
      "epoch:15 step:14111 [D loss: 0.614886, acc: 65.62%] [G loss: 1.930091]\n",
      "epoch:15 step:14112 [D loss: 0.634112, acc: 66.41%] [G loss: 2.079867]\n",
      "epoch:15 step:14113 [D loss: 0.647834, acc: 57.03%] [G loss: 1.817658]\n",
      "epoch:15 step:14114 [D loss: 0.663722, acc: 64.84%] [G loss: 1.830977]\n",
      "epoch:15 step:14115 [D loss: 0.642887, acc: 60.94%] [G loss: 1.814042]\n",
      "epoch:15 step:14116 [D loss: 0.630567, acc: 62.50%] [G loss: 1.966507]\n",
      "epoch:15 step:14117 [D loss: 0.653517, acc: 59.38%] [G loss: 1.908028]\n",
      "epoch:15 step:14118 [D loss: 0.614595, acc: 64.06%] [G loss: 2.004797]\n",
      "epoch:15 step:14119 [D loss: 0.656922, acc: 58.59%] [G loss: 1.933018]\n",
      "epoch:15 step:14120 [D loss: 0.620061, acc: 67.19%] [G loss: 1.915820]\n",
      "epoch:15 step:14121 [D loss: 0.663219, acc: 58.59%] [G loss: 1.968766]\n",
      "epoch:15 step:14122 [D loss: 0.673422, acc: 57.81%] [G loss: 1.893433]\n",
      "epoch:15 step:14123 [D loss: 0.645175, acc: 57.81%] [G loss: 1.926551]\n",
      "epoch:15 step:14124 [D loss: 0.579708, acc: 75.00%] [G loss: 2.030252]\n",
      "epoch:15 step:14125 [D loss: 0.621489, acc: 63.28%] [G loss: 2.064929]\n",
      "epoch:15 step:14126 [D loss: 0.648347, acc: 64.06%] [G loss: 1.931352]\n",
      "epoch:15 step:14127 [D loss: 0.644737, acc: 64.06%] [G loss: 1.913663]\n",
      "epoch:15 step:14128 [D loss: 0.675478, acc: 63.28%] [G loss: 1.804702]\n",
      "epoch:15 step:14129 [D loss: 0.648320, acc: 58.59%] [G loss: 2.038034]\n",
      "epoch:15 step:14130 [D loss: 0.621564, acc: 67.97%] [G loss: 2.078903]\n",
      "epoch:15 step:14131 [D loss: 0.642147, acc: 59.38%] [G loss: 1.958288]\n",
      "epoch:15 step:14132 [D loss: 0.637285, acc: 67.97%] [G loss: 2.117008]\n",
      "epoch:15 step:14133 [D loss: 0.608395, acc: 67.97%] [G loss: 1.704358]\n",
      "epoch:15 step:14134 [D loss: 0.675230, acc: 60.94%] [G loss: 1.791680]\n",
      "epoch:15 step:14135 [D loss: 0.690186, acc: 55.47%] [G loss: 1.921152]\n",
      "epoch:15 step:14136 [D loss: 0.691433, acc: 61.72%] [G loss: 1.730892]\n",
      "epoch:15 step:14137 [D loss: 0.625795, acc: 66.41%] [G loss: 1.769688]\n",
      "epoch:15 step:14138 [D loss: 0.636770, acc: 64.84%] [G loss: 1.800040]\n",
      "epoch:15 step:14139 [D loss: 0.625288, acc: 62.50%] [G loss: 1.930550]\n",
      "epoch:15 step:14140 [D loss: 0.630764, acc: 66.41%] [G loss: 1.808620]\n",
      "epoch:15 step:14141 [D loss: 0.629936, acc: 65.62%] [G loss: 1.657853]\n",
      "epoch:15 step:14142 [D loss: 0.659545, acc: 63.28%] [G loss: 1.879714]\n",
      "epoch:15 step:14143 [D loss: 0.597597, acc: 72.66%] [G loss: 1.956089]\n",
      "epoch:15 step:14144 [D loss: 0.644306, acc: 63.28%] [G loss: 1.914200]\n",
      "epoch:15 step:14145 [D loss: 0.685763, acc: 59.38%] [G loss: 1.867337]\n",
      "epoch:15 step:14146 [D loss: 0.647947, acc: 68.75%] [G loss: 1.892961]\n",
      "epoch:15 step:14147 [D loss: 0.632031, acc: 67.19%] [G loss: 1.980905]\n",
      "epoch:15 step:14148 [D loss: 0.582358, acc: 67.19%] [G loss: 2.006231]\n",
      "epoch:15 step:14149 [D loss: 0.619087, acc: 65.62%] [G loss: 1.834987]\n",
      "epoch:15 step:14150 [D loss: 0.660680, acc: 57.81%] [G loss: 1.885221]\n",
      "epoch:15 step:14151 [D loss: 0.620519, acc: 63.28%] [G loss: 1.942506]\n",
      "epoch:15 step:14152 [D loss: 0.602882, acc: 65.62%] [G loss: 2.007566]\n",
      "epoch:15 step:14153 [D loss: 0.684836, acc: 61.72%] [G loss: 1.827577]\n",
      "epoch:15 step:14154 [D loss: 0.663136, acc: 62.50%] [G loss: 1.739681]\n",
      "epoch:15 step:14155 [D loss: 0.617400, acc: 67.19%] [G loss: 1.919506]\n",
      "epoch:15 step:14156 [D loss: 0.627489, acc: 64.06%] [G loss: 1.995925]\n",
      "epoch:15 step:14157 [D loss: 0.637864, acc: 62.50%] [G loss: 1.917368]\n",
      "epoch:15 step:14158 [D loss: 0.629451, acc: 66.41%] [G loss: 1.985997]\n",
      "epoch:15 step:14159 [D loss: 0.626363, acc: 64.84%] [G loss: 1.816907]\n",
      "epoch:15 step:14160 [D loss: 0.675937, acc: 57.03%] [G loss: 2.063216]\n",
      "epoch:15 step:14161 [D loss: 0.584889, acc: 68.75%] [G loss: 2.034078]\n",
      "epoch:15 step:14162 [D loss: 0.601475, acc: 69.53%] [G loss: 2.115428]\n",
      "epoch:15 step:14163 [D loss: 0.730373, acc: 52.34%] [G loss: 1.944423]\n",
      "epoch:15 step:14164 [D loss: 0.706257, acc: 51.56%] [G loss: 1.964290]\n",
      "epoch:15 step:14165 [D loss: 0.685516, acc: 57.03%] [G loss: 1.845170]\n",
      "epoch:15 step:14166 [D loss: 0.665974, acc: 61.72%] [G loss: 1.978212]\n",
      "epoch:15 step:14167 [D loss: 0.592827, acc: 71.88%] [G loss: 1.953054]\n",
      "epoch:15 step:14168 [D loss: 0.583103, acc: 69.53%] [G loss: 2.140799]\n",
      "epoch:15 step:14169 [D loss: 0.655868, acc: 59.38%] [G loss: 2.022747]\n",
      "epoch:15 step:14170 [D loss: 0.609959, acc: 69.53%] [G loss: 2.243737]\n",
      "epoch:15 step:14171 [D loss: 0.650004, acc: 60.16%] [G loss: 2.020674]\n",
      "epoch:15 step:14172 [D loss: 0.606853, acc: 70.31%] [G loss: 2.201175]\n",
      "epoch:15 step:14173 [D loss: 0.694717, acc: 57.81%] [G loss: 2.009287]\n",
      "epoch:15 step:14174 [D loss: 0.629681, acc: 62.50%] [G loss: 2.503913]\n",
      "epoch:15 step:14175 [D loss: 0.696889, acc: 53.91%] [G loss: 2.043700]\n",
      "epoch:15 step:14176 [D loss: 0.650940, acc: 64.84%] [G loss: 1.905938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14177 [D loss: 0.624439, acc: 62.50%] [G loss: 2.025855]\n",
      "epoch:15 step:14178 [D loss: 0.635997, acc: 60.94%] [G loss: 2.015093]\n",
      "epoch:15 step:14179 [D loss: 0.677122, acc: 58.59%] [G loss: 1.950341]\n",
      "epoch:15 step:14180 [D loss: 0.672778, acc: 57.81%] [G loss: 1.814329]\n",
      "epoch:15 step:14181 [D loss: 0.616910, acc: 64.84%] [G loss: 2.078775]\n",
      "epoch:15 step:14182 [D loss: 0.621150, acc: 68.75%] [G loss: 1.899359]\n",
      "epoch:15 step:14183 [D loss: 0.643468, acc: 59.38%] [G loss: 1.792199]\n",
      "epoch:15 step:14184 [D loss: 0.623388, acc: 63.28%] [G loss: 1.810195]\n",
      "epoch:15 step:14185 [D loss: 0.621203, acc: 67.97%] [G loss: 2.002541]\n",
      "epoch:15 step:14186 [D loss: 0.615809, acc: 64.84%] [G loss: 2.008043]\n",
      "epoch:15 step:14187 [D loss: 0.687403, acc: 58.59%] [G loss: 1.887830]\n",
      "epoch:15 step:14188 [D loss: 0.672085, acc: 56.25%] [G loss: 1.846224]\n",
      "epoch:15 step:14189 [D loss: 0.687092, acc: 54.69%] [G loss: 1.826296]\n",
      "epoch:15 step:14190 [D loss: 0.642820, acc: 62.50%] [G loss: 1.833097]\n",
      "epoch:15 step:14191 [D loss: 0.613557, acc: 64.84%] [G loss: 1.888654]\n",
      "epoch:15 step:14192 [D loss: 0.736934, acc: 53.12%] [G loss: 1.796587]\n",
      "epoch:15 step:14193 [D loss: 0.631299, acc: 71.09%] [G loss: 1.908749]\n",
      "epoch:15 step:14194 [D loss: 0.687089, acc: 52.34%] [G loss: 1.742254]\n",
      "epoch:15 step:14195 [D loss: 0.654278, acc: 57.03%] [G loss: 1.893733]\n",
      "epoch:15 step:14196 [D loss: 0.654641, acc: 63.28%] [G loss: 1.967598]\n",
      "epoch:15 step:14197 [D loss: 0.645377, acc: 64.06%] [G loss: 1.970321]\n",
      "epoch:15 step:14198 [D loss: 0.675357, acc: 57.81%] [G loss: 1.741634]\n",
      "epoch:15 step:14199 [D loss: 0.654243, acc: 62.50%] [G loss: 1.834996]\n",
      "epoch:15 step:14200 [D loss: 0.669402, acc: 63.28%] [G loss: 1.901683]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.163122\n",
      "FID: 16.229382\n",
      "0 = 12.921090565299979\n",
      "1 = 0.08963452731985595\n",
      "2 = 0.8970999717712402\n",
      "3 = 0.9204000234603882\n",
      "4 = 0.8737999796867371\n",
      "5 = 0.8794190883636475\n",
      "6 = 0.9204000234603882\n",
      "7 = 6.7666614267110905\n",
      "8 = 0.07788047670850345\n",
      "9 = 0.7269999980926514\n",
      "10 = 0.7396000027656555\n",
      "11 = 0.7143999934196472\n",
      "12 = 0.7214202284812927\n",
      "13 = 0.7396000027656555\n",
      "14 = 7.16314697265625\n",
      "15 = 9.432783126831055\n",
      "16 = 0.12367124855518341\n",
      "17 = 7.163121700286865\n",
      "18 = 16.229381561279297\n",
      "epoch:15 step:14201 [D loss: 0.693911, acc: 55.47%] [G loss: 1.912493]\n",
      "epoch:15 step:14202 [D loss: 0.616733, acc: 66.41%] [G loss: 1.795342]\n",
      "epoch:15 step:14203 [D loss: 0.695251, acc: 55.47%] [G loss: 1.870093]\n",
      "epoch:15 step:14204 [D loss: 0.703612, acc: 51.56%] [G loss: 1.989559]\n",
      "epoch:15 step:14205 [D loss: 0.636896, acc: 67.19%] [G loss: 1.799116]\n",
      "epoch:15 step:14206 [D loss: 0.635996, acc: 70.31%] [G loss: 1.992593]\n",
      "epoch:15 step:14207 [D loss: 0.627964, acc: 63.28%] [G loss: 1.925489]\n",
      "epoch:15 step:14208 [D loss: 0.661747, acc: 59.38%] [G loss: 1.824980]\n",
      "epoch:15 step:14209 [D loss: 0.619996, acc: 63.28%] [G loss: 1.869796]\n",
      "epoch:15 step:14210 [D loss: 0.650913, acc: 64.84%] [G loss: 1.965877]\n",
      "epoch:15 step:14211 [D loss: 0.617393, acc: 66.41%] [G loss: 1.810174]\n",
      "epoch:15 step:14212 [D loss: 0.682903, acc: 64.06%] [G loss: 1.780410]\n",
      "epoch:15 step:14213 [D loss: 0.659827, acc: 60.94%] [G loss: 1.870722]\n",
      "epoch:15 step:14214 [D loss: 0.629213, acc: 62.50%] [G loss: 1.895144]\n",
      "epoch:15 step:14215 [D loss: 0.638739, acc: 63.28%] [G loss: 1.819338]\n",
      "epoch:15 step:14216 [D loss: 0.675722, acc: 64.06%] [G loss: 1.947708]\n",
      "epoch:15 step:14217 [D loss: 0.636862, acc: 62.50%] [G loss: 1.940196]\n",
      "epoch:15 step:14218 [D loss: 0.666742, acc: 62.50%] [G loss: 1.886361]\n",
      "epoch:15 step:14219 [D loss: 0.658612, acc: 62.50%] [G loss: 1.741689]\n",
      "epoch:15 step:14220 [D loss: 0.571782, acc: 74.22%] [G loss: 1.829266]\n",
      "epoch:15 step:14221 [D loss: 0.600240, acc: 67.19%] [G loss: 1.817633]\n",
      "epoch:15 step:14222 [D loss: 0.705558, acc: 53.12%] [G loss: 1.837170]\n",
      "epoch:15 step:14223 [D loss: 0.637888, acc: 66.41%] [G loss: 1.914512]\n",
      "epoch:15 step:14224 [D loss: 0.657179, acc: 62.50%] [G loss: 1.881329]\n",
      "epoch:15 step:14225 [D loss: 0.642148, acc: 60.16%] [G loss: 1.959314]\n",
      "epoch:15 step:14226 [D loss: 0.653044, acc: 57.81%] [G loss: 1.943776]\n",
      "epoch:15 step:14227 [D loss: 0.625866, acc: 64.06%] [G loss: 1.949310]\n",
      "epoch:15 step:14228 [D loss: 0.692889, acc: 60.16%] [G loss: 1.979748]\n",
      "epoch:15 step:14229 [D loss: 0.614537, acc: 68.75%] [G loss: 1.863552]\n",
      "epoch:15 step:14230 [D loss: 0.678090, acc: 58.59%] [G loss: 1.791844]\n",
      "epoch:15 step:14231 [D loss: 0.664189, acc: 61.72%] [G loss: 1.943126]\n",
      "epoch:15 step:14232 [D loss: 0.634362, acc: 65.62%] [G loss: 1.782502]\n",
      "epoch:15 step:14233 [D loss: 0.666963, acc: 55.47%] [G loss: 1.876250]\n",
      "epoch:15 step:14234 [D loss: 0.628819, acc: 68.75%] [G loss: 1.886132]\n",
      "epoch:15 step:14235 [D loss: 0.593855, acc: 68.75%] [G loss: 1.892581]\n",
      "epoch:15 step:14236 [D loss: 0.665017, acc: 57.03%] [G loss: 1.996530]\n",
      "epoch:15 step:14237 [D loss: 0.683604, acc: 54.69%] [G loss: 1.892065]\n",
      "epoch:15 step:14238 [D loss: 0.664900, acc: 59.38%] [G loss: 1.959592]\n",
      "epoch:15 step:14239 [D loss: 0.653517, acc: 63.28%] [G loss: 1.835646]\n",
      "epoch:15 step:14240 [D loss: 0.644406, acc: 60.94%] [G loss: 1.931602]\n",
      "epoch:15 step:14241 [D loss: 0.711521, acc: 52.34%] [G loss: 1.863870]\n",
      "epoch:15 step:14242 [D loss: 0.615318, acc: 67.97%] [G loss: 1.974988]\n",
      "epoch:15 step:14243 [D loss: 0.654985, acc: 59.38%] [G loss: 1.871879]\n",
      "epoch:15 step:14244 [D loss: 0.635068, acc: 63.28%] [G loss: 1.814849]\n",
      "epoch:15 step:14245 [D loss: 0.633977, acc: 64.06%] [G loss: 1.851012]\n",
      "epoch:15 step:14246 [D loss: 0.615168, acc: 65.62%] [G loss: 1.924389]\n",
      "epoch:15 step:14247 [D loss: 0.629465, acc: 62.50%] [G loss: 1.741454]\n",
      "epoch:15 step:14248 [D loss: 0.627025, acc: 67.97%] [G loss: 2.055782]\n",
      "epoch:15 step:14249 [D loss: 0.636497, acc: 67.19%] [G loss: 1.998613]\n",
      "epoch:15 step:14250 [D loss: 0.690023, acc: 53.12%] [G loss: 1.894706]\n",
      "epoch:15 step:14251 [D loss: 0.675723, acc: 55.47%] [G loss: 1.932369]\n",
      "epoch:15 step:14252 [D loss: 0.607667, acc: 67.97%] [G loss: 2.017768]\n",
      "epoch:15 step:14253 [D loss: 0.589742, acc: 64.84%] [G loss: 2.163651]\n",
      "epoch:15 step:14254 [D loss: 0.631015, acc: 64.06%] [G loss: 1.840847]\n",
      "epoch:15 step:14255 [D loss: 0.636875, acc: 64.84%] [G loss: 1.973240]\n",
      "epoch:15 step:14256 [D loss: 0.674271, acc: 62.50%] [G loss: 1.966303]\n",
      "epoch:15 step:14257 [D loss: 0.642844, acc: 57.81%] [G loss: 2.050603]\n",
      "epoch:15 step:14258 [D loss: 0.638647, acc: 66.41%] [G loss: 1.975140]\n",
      "epoch:15 step:14259 [D loss: 0.640749, acc: 67.19%] [G loss: 1.916673]\n",
      "epoch:15 step:14260 [D loss: 0.642561, acc: 66.41%] [G loss: 1.840732]\n",
      "epoch:15 step:14261 [D loss: 0.624841, acc: 66.41%] [G loss: 2.180919]\n",
      "epoch:15 step:14262 [D loss: 0.611462, acc: 62.50%] [G loss: 2.292961]\n",
      "epoch:15 step:14263 [D loss: 0.567138, acc: 69.53%] [G loss: 2.225897]\n",
      "epoch:15 step:14264 [D loss: 0.556426, acc: 71.88%] [G loss: 2.264257]\n",
      "epoch:15 step:14265 [D loss: 0.732462, acc: 53.12%] [G loss: 1.937374]\n",
      "epoch:15 step:14266 [D loss: 0.661033, acc: 64.84%] [G loss: 1.841640]\n",
      "epoch:15 step:14267 [D loss: 0.663348, acc: 60.16%] [G loss: 1.847464]\n",
      "epoch:15 step:14268 [D loss: 0.680236, acc: 60.94%] [G loss: 1.849264]\n",
      "epoch:15 step:14269 [D loss: 0.661053, acc: 59.38%] [G loss: 1.771492]\n",
      "epoch:15 step:14270 [D loss: 0.612810, acc: 66.41%] [G loss: 1.866726]\n",
      "epoch:15 step:14271 [D loss: 0.677154, acc: 57.81%] [G loss: 1.862787]\n",
      "epoch:15 step:14272 [D loss: 0.695112, acc: 57.81%] [G loss: 1.977741]\n",
      "epoch:15 step:14273 [D loss: 0.589398, acc: 67.19%] [G loss: 1.937568]\n",
      "epoch:15 step:14274 [D loss: 0.622981, acc: 64.84%] [G loss: 2.071812]\n",
      "epoch:15 step:14275 [D loss: 0.731929, acc: 57.03%] [G loss: 1.794055]\n",
      "epoch:15 step:14276 [D loss: 0.649912, acc: 64.06%] [G loss: 1.888458]\n",
      "epoch:15 step:14277 [D loss: 0.616132, acc: 68.75%] [G loss: 1.795078]\n",
      "epoch:15 step:14278 [D loss: 0.591806, acc: 65.62%] [G loss: 1.952833]\n",
      "epoch:15 step:14279 [D loss: 0.642574, acc: 60.16%] [G loss: 1.910472]\n",
      "epoch:15 step:14280 [D loss: 0.701405, acc: 55.47%] [G loss: 1.810362]\n",
      "epoch:15 step:14281 [D loss: 0.643757, acc: 63.28%] [G loss: 1.905122]\n",
      "epoch:15 step:14282 [D loss: 0.651524, acc: 56.25%] [G loss: 1.798525]\n",
      "epoch:15 step:14283 [D loss: 0.694759, acc: 52.34%] [G loss: 1.671045]\n",
      "epoch:15 step:14284 [D loss: 0.632284, acc: 63.28%] [G loss: 1.926012]\n",
      "epoch:15 step:14285 [D loss: 0.587107, acc: 68.75%] [G loss: 2.061004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14286 [D loss: 0.583814, acc: 71.88%] [G loss: 2.158015]\n",
      "epoch:15 step:14287 [D loss: 0.582999, acc: 72.66%] [G loss: 2.206293]\n",
      "epoch:15 step:14288 [D loss: 0.677329, acc: 57.03%] [G loss: 2.035523]\n",
      "epoch:15 step:14289 [D loss: 0.653803, acc: 64.06%] [G loss: 1.883247]\n",
      "epoch:15 step:14290 [D loss: 0.634455, acc: 62.50%] [G loss: 1.929112]\n",
      "epoch:15 step:14291 [D loss: 0.615349, acc: 66.41%] [G loss: 1.916823]\n",
      "epoch:15 step:14292 [D loss: 0.600137, acc: 67.97%] [G loss: 1.878806]\n",
      "epoch:15 step:14293 [D loss: 0.625173, acc: 63.28%] [G loss: 2.011358]\n",
      "epoch:15 step:14294 [D loss: 0.642699, acc: 62.50%] [G loss: 1.905225]\n",
      "epoch:15 step:14295 [D loss: 0.627633, acc: 60.94%] [G loss: 1.907668]\n",
      "epoch:15 step:14296 [D loss: 0.606587, acc: 67.19%] [G loss: 1.979161]\n",
      "epoch:15 step:14297 [D loss: 0.558842, acc: 73.44%] [G loss: 2.010001]\n",
      "epoch:15 step:14298 [D loss: 0.577783, acc: 70.31%] [G loss: 1.958666]\n",
      "epoch:15 step:14299 [D loss: 0.622400, acc: 63.28%] [G loss: 2.054215]\n",
      "epoch:15 step:14300 [D loss: 0.678830, acc: 56.25%] [G loss: 1.938177]\n",
      "epoch:15 step:14301 [D loss: 0.643242, acc: 67.97%] [G loss: 2.058447]\n",
      "epoch:15 step:14302 [D loss: 0.657661, acc: 60.94%] [G loss: 2.079872]\n",
      "epoch:15 step:14303 [D loss: 0.633535, acc: 60.94%] [G loss: 2.214138]\n",
      "epoch:15 step:14304 [D loss: 0.710180, acc: 52.34%] [G loss: 1.894464]\n",
      "epoch:15 step:14305 [D loss: 0.713414, acc: 55.47%] [G loss: 1.742292]\n",
      "epoch:15 step:14306 [D loss: 0.600098, acc: 68.75%] [G loss: 1.794466]\n",
      "epoch:15 step:14307 [D loss: 0.669642, acc: 58.59%] [G loss: 1.840891]\n",
      "epoch:15 step:14308 [D loss: 0.620740, acc: 64.84%] [G loss: 1.906132]\n",
      "epoch:15 step:14309 [D loss: 0.626575, acc: 60.94%] [G loss: 1.899671]\n",
      "epoch:15 step:14310 [D loss: 0.613118, acc: 63.28%] [G loss: 1.933670]\n",
      "epoch:15 step:14311 [D loss: 0.605937, acc: 67.19%] [G loss: 1.869420]\n",
      "epoch:15 step:14312 [D loss: 0.667737, acc: 63.28%] [G loss: 1.790498]\n",
      "epoch:15 step:14313 [D loss: 0.634027, acc: 67.19%] [G loss: 1.923065]\n",
      "epoch:15 step:14314 [D loss: 0.674061, acc: 60.94%] [G loss: 1.898602]\n",
      "epoch:15 step:14315 [D loss: 0.645970, acc: 64.84%] [G loss: 1.950179]\n",
      "epoch:15 step:14316 [D loss: 0.608895, acc: 65.62%] [G loss: 1.979775]\n",
      "epoch:15 step:14317 [D loss: 0.622479, acc: 64.06%] [G loss: 1.957228]\n",
      "epoch:15 step:14318 [D loss: 0.671667, acc: 59.38%] [G loss: 2.001394]\n",
      "epoch:15 step:14319 [D loss: 0.639598, acc: 65.62%] [G loss: 2.089880]\n",
      "epoch:15 step:14320 [D loss: 0.621846, acc: 67.19%] [G loss: 1.902669]\n",
      "epoch:15 step:14321 [D loss: 0.591126, acc: 69.53%] [G loss: 1.884490]\n",
      "epoch:15 step:14322 [D loss: 0.664470, acc: 58.59%] [G loss: 1.884566]\n",
      "epoch:15 step:14323 [D loss: 0.603009, acc: 68.75%] [G loss: 1.905752]\n",
      "epoch:15 step:14324 [D loss: 0.639434, acc: 60.94%] [G loss: 2.102117]\n",
      "epoch:15 step:14325 [D loss: 0.662098, acc: 59.38%] [G loss: 2.103811]\n",
      "epoch:15 step:14326 [D loss: 0.608528, acc: 65.62%] [G loss: 1.901896]\n",
      "epoch:15 step:14327 [D loss: 0.580126, acc: 69.53%] [G loss: 2.051211]\n",
      "epoch:15 step:14328 [D loss: 0.682080, acc: 53.91%] [G loss: 1.819341]\n",
      "epoch:15 step:14329 [D loss: 0.627313, acc: 64.06%] [G loss: 2.089569]\n",
      "epoch:15 step:14330 [D loss: 0.570394, acc: 75.00%] [G loss: 2.251561]\n",
      "epoch:15 step:14331 [D loss: 0.642051, acc: 64.84%] [G loss: 1.982152]\n",
      "epoch:15 step:14332 [D loss: 0.628398, acc: 64.06%] [G loss: 1.958392]\n",
      "epoch:15 step:14333 [D loss: 0.672263, acc: 60.94%] [G loss: 1.948890]\n",
      "epoch:15 step:14334 [D loss: 0.627078, acc: 61.72%] [G loss: 1.988476]\n",
      "epoch:15 step:14335 [D loss: 0.581269, acc: 69.53%] [G loss: 1.995707]\n",
      "epoch:15 step:14336 [D loss: 0.681747, acc: 60.16%] [G loss: 1.977497]\n",
      "epoch:15 step:14337 [D loss: 0.637407, acc: 67.19%] [G loss: 1.924965]\n",
      "epoch:15 step:14338 [D loss: 0.613215, acc: 67.19%] [G loss: 1.948467]\n",
      "epoch:15 step:14339 [D loss: 0.658555, acc: 60.16%] [G loss: 1.892715]\n",
      "epoch:15 step:14340 [D loss: 0.664758, acc: 62.50%] [G loss: 1.840209]\n",
      "epoch:15 step:14341 [D loss: 0.634109, acc: 61.72%] [G loss: 1.915190]\n",
      "epoch:15 step:14342 [D loss: 0.713047, acc: 57.81%] [G loss: 1.983389]\n",
      "epoch:15 step:14343 [D loss: 0.649272, acc: 60.94%] [G loss: 1.898721]\n",
      "epoch:15 step:14344 [D loss: 0.653125, acc: 57.81%] [G loss: 2.054141]\n",
      "epoch:15 step:14345 [D loss: 0.657230, acc: 60.16%] [G loss: 1.929243]\n",
      "epoch:15 step:14346 [D loss: 0.659113, acc: 63.28%] [G loss: 1.974948]\n",
      "epoch:15 step:14347 [D loss: 0.695683, acc: 59.38%] [G loss: 1.934751]\n",
      "epoch:15 step:14348 [D loss: 0.612022, acc: 67.97%] [G loss: 1.944959]\n",
      "epoch:15 step:14349 [D loss: 0.644647, acc: 59.38%] [G loss: 2.021265]\n",
      "epoch:15 step:14350 [D loss: 0.673561, acc: 56.25%] [G loss: 1.822673]\n",
      "epoch:15 step:14351 [D loss: 0.637914, acc: 60.94%] [G loss: 1.881012]\n",
      "epoch:15 step:14352 [D loss: 0.622234, acc: 63.28%] [G loss: 1.785209]\n",
      "epoch:15 step:14353 [D loss: 0.634349, acc: 61.72%] [G loss: 1.859602]\n",
      "epoch:15 step:14354 [D loss: 0.646601, acc: 65.62%] [G loss: 1.941528]\n",
      "epoch:15 step:14355 [D loss: 0.639741, acc: 65.62%] [G loss: 1.852253]\n",
      "epoch:15 step:14356 [D loss: 0.637830, acc: 59.38%] [G loss: 1.848164]\n",
      "epoch:15 step:14357 [D loss: 0.616876, acc: 67.97%] [G loss: 2.088440]\n",
      "epoch:15 step:14358 [D loss: 0.695096, acc: 57.81%] [G loss: 1.875379]\n",
      "epoch:15 step:14359 [D loss: 0.644969, acc: 66.41%] [G loss: 1.866613]\n",
      "epoch:15 step:14360 [D loss: 0.661491, acc: 61.72%] [G loss: 1.851831]\n",
      "epoch:15 step:14361 [D loss: 0.617007, acc: 64.06%] [G loss: 2.000569]\n",
      "epoch:15 step:14362 [D loss: 0.604067, acc: 64.84%] [G loss: 1.967549]\n",
      "epoch:15 step:14363 [D loss: 0.711947, acc: 58.59%] [G loss: 1.801492]\n",
      "epoch:15 step:14364 [D loss: 0.606491, acc: 67.97%] [G loss: 2.076145]\n",
      "epoch:15 step:14365 [D loss: 0.624425, acc: 67.97%] [G loss: 1.949955]\n",
      "epoch:15 step:14366 [D loss: 0.635340, acc: 64.84%] [G loss: 1.807612]\n",
      "epoch:15 step:14367 [D loss: 0.584641, acc: 72.66%] [G loss: 2.239123]\n",
      "epoch:15 step:14368 [D loss: 0.550491, acc: 76.56%] [G loss: 2.240238]\n",
      "epoch:15 step:14369 [D loss: 0.570062, acc: 71.88%] [G loss: 2.250417]\n",
      "epoch:15 step:14370 [D loss: 0.598920, acc: 68.75%] [G loss: 2.442524]\n",
      "epoch:15 step:14371 [D loss: 0.672925, acc: 60.94%] [G loss: 1.787183]\n",
      "epoch:15 step:14372 [D loss: 0.632444, acc: 64.06%] [G loss: 1.950581]\n",
      "epoch:15 step:14373 [D loss: 0.614758, acc: 64.06%] [G loss: 2.053763]\n",
      "epoch:15 step:14374 [D loss: 0.624538, acc: 64.06%] [G loss: 1.879075]\n",
      "epoch:15 step:14375 [D loss: 0.599232, acc: 72.66%] [G loss: 1.907696]\n",
      "epoch:15 step:14376 [D loss: 0.643805, acc: 59.38%] [G loss: 1.985497]\n",
      "epoch:15 step:14377 [D loss: 0.644554, acc: 62.50%] [G loss: 1.900791]\n",
      "epoch:15 step:14378 [D loss: 0.676298, acc: 64.06%] [G loss: 2.025970]\n",
      "epoch:15 step:14379 [D loss: 0.653418, acc: 61.72%] [G loss: 1.855221]\n",
      "epoch:15 step:14380 [D loss: 0.666921, acc: 57.81%] [G loss: 1.855010]\n",
      "epoch:15 step:14381 [D loss: 0.615690, acc: 67.19%] [G loss: 1.860555]\n",
      "epoch:15 step:14382 [D loss: 0.651258, acc: 60.94%] [G loss: 1.965135]\n",
      "epoch:15 step:14383 [D loss: 0.663206, acc: 59.38%] [G loss: 1.952257]\n",
      "epoch:15 step:14384 [D loss: 0.646025, acc: 58.59%] [G loss: 1.855365]\n",
      "epoch:15 step:14385 [D loss: 0.639076, acc: 66.41%] [G loss: 2.198150]\n",
      "epoch:15 step:14386 [D loss: 0.641089, acc: 60.94%] [G loss: 1.905576]\n",
      "epoch:15 step:14387 [D loss: 0.618027, acc: 70.31%] [G loss: 2.048233]\n",
      "epoch:15 step:14388 [D loss: 0.650808, acc: 60.16%] [G loss: 2.070660]\n",
      "epoch:15 step:14389 [D loss: 0.623968, acc: 63.28%] [G loss: 2.061810]\n",
      "epoch:15 step:14390 [D loss: 0.632497, acc: 63.28%] [G loss: 2.076560]\n",
      "epoch:15 step:14391 [D loss: 0.648729, acc: 62.50%] [G loss: 1.988324]\n",
      "epoch:15 step:14392 [D loss: 0.556832, acc: 76.56%] [G loss: 2.128351]\n",
      "epoch:15 step:14393 [D loss: 0.671749, acc: 56.25%] [G loss: 2.013063]\n",
      "epoch:15 step:14394 [D loss: 0.617367, acc: 68.75%] [G loss: 2.076833]\n",
      "epoch:15 step:14395 [D loss: 0.630385, acc: 62.50%] [G loss: 2.079150]\n",
      "epoch:15 step:14396 [D loss: 0.642948, acc: 60.16%] [G loss: 1.822352]\n",
      "epoch:15 step:14397 [D loss: 0.670168, acc: 59.38%] [G loss: 1.987992]\n",
      "epoch:15 step:14398 [D loss: 0.603600, acc: 68.75%] [G loss: 2.016602]\n",
      "epoch:15 step:14399 [D loss: 0.620259, acc: 64.06%] [G loss: 2.054833]\n",
      "epoch:15 step:14400 [D loss: 0.577483, acc: 65.62%] [G loss: 2.408012]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute score in space: 1\n",
      "IS socre: 7.129735\n",
      "FID: 16.493382\n",
      "0 = 12.909594641637783\n",
      "1 = 0.08946679689631906\n",
      "2 = 0.8952000141143799\n",
      "3 = 0.9168000221252441\n",
      "4 = 0.8736000061035156\n",
      "5 = 0.8788343667984009\n",
      "6 = 0.9168000221252441\n",
      "7 = 6.739031312584889\n",
      "8 = 0.07881470459192313\n",
      "9 = 0.7422999739646912\n",
      "10 = 0.7483999729156494\n",
      "11 = 0.7361999750137329\n",
      "12 = 0.7393795847892761\n",
      "13 = 0.7483999729156494\n",
      "14 = 7.129766464233398\n",
      "15 = 9.385371208190918\n",
      "16 = 0.1326146423816681\n",
      "17 = 7.129734516143799\n",
      "18 = 16.49338150024414\n",
      "epoch:15 step:14401 [D loss: 0.570305, acc: 68.75%] [G loss: 2.241643]\n",
      "epoch:15 step:14402 [D loss: 0.550143, acc: 75.78%] [G loss: 2.560630]\n",
      "epoch:15 step:14403 [D loss: 0.740496, acc: 55.47%] [G loss: 1.956620]\n",
      "epoch:15 step:14404 [D loss: 0.735323, acc: 56.25%] [G loss: 1.814569]\n",
      "epoch:15 step:14405 [D loss: 0.635889, acc: 62.50%] [G loss: 2.021037]\n",
      "epoch:15 step:14406 [D loss: 0.638106, acc: 60.94%] [G loss: 1.857041]\n",
      "epoch:15 step:14407 [D loss: 0.688255, acc: 60.16%] [G loss: 1.839701]\n",
      "epoch:15 step:14408 [D loss: 0.704141, acc: 59.38%] [G loss: 1.925765]\n",
      "epoch:15 step:14409 [D loss: 0.614776, acc: 64.84%] [G loss: 2.156322]\n",
      "epoch:15 step:14410 [D loss: 0.690866, acc: 60.94%] [G loss: 1.846061]\n",
      "epoch:15 step:14411 [D loss: 0.649243, acc: 57.03%] [G loss: 1.859610]\n",
      "epoch:15 step:14412 [D loss: 0.643464, acc: 65.62%] [G loss: 1.832030]\n",
      "epoch:15 step:14413 [D loss: 0.596677, acc: 70.31%] [G loss: 1.928675]\n",
      "epoch:15 step:14414 [D loss: 0.624382, acc: 67.97%] [G loss: 1.965898]\n",
      "epoch:15 step:14415 [D loss: 0.625220, acc: 66.41%] [G loss: 1.985979]\n",
      "epoch:15 step:14416 [D loss: 0.676469, acc: 55.47%] [G loss: 1.870613]\n",
      "epoch:15 step:14417 [D loss: 0.630988, acc: 66.41%] [G loss: 1.862362]\n",
      "epoch:15 step:14418 [D loss: 0.634545, acc: 66.41%] [G loss: 1.908752]\n",
      "epoch:15 step:14419 [D loss: 0.633548, acc: 60.16%] [G loss: 1.975622]\n",
      "epoch:15 step:14420 [D loss: 0.646449, acc: 63.28%] [G loss: 1.976609]\n",
      "epoch:15 step:14421 [D loss: 0.646901, acc: 61.72%] [G loss: 1.954477]\n",
      "epoch:15 step:14422 [D loss: 0.624568, acc: 62.50%] [G loss: 1.914237]\n",
      "epoch:15 step:14423 [D loss: 0.644538, acc: 64.06%] [G loss: 1.944586]\n",
      "epoch:15 step:14424 [D loss: 0.647602, acc: 63.28%] [G loss: 1.919409]\n",
      "epoch:15 step:14425 [D loss: 0.649731, acc: 59.38%] [G loss: 1.959285]\n",
      "epoch:15 step:14426 [D loss: 0.612912, acc: 63.28%] [G loss: 2.199894]\n",
      "epoch:15 step:14427 [D loss: 0.629273, acc: 65.62%] [G loss: 2.174434]\n",
      "epoch:15 step:14428 [D loss: 0.689108, acc: 54.69%] [G loss: 1.838442]\n",
      "epoch:15 step:14429 [D loss: 0.645824, acc: 62.50%] [G loss: 2.038775]\n",
      "epoch:15 step:14430 [D loss: 0.707152, acc: 53.12%] [G loss: 1.951319]\n",
      "epoch:15 step:14431 [D loss: 0.698086, acc: 53.91%] [G loss: 1.890469]\n",
      "epoch:15 step:14432 [D loss: 0.702035, acc: 60.16%] [G loss: 1.801531]\n",
      "epoch:15 step:14433 [D loss: 0.630674, acc: 63.28%] [G loss: 1.904932]\n",
      "epoch:15 step:14434 [D loss: 0.626523, acc: 68.75%] [G loss: 2.018291]\n",
      "epoch:15 step:14435 [D loss: 0.608113, acc: 66.41%] [G loss: 2.021358]\n",
      "epoch:15 step:14436 [D loss: 0.610688, acc: 69.53%] [G loss: 2.118690]\n",
      "epoch:15 step:14437 [D loss: 0.674351, acc: 59.38%] [G loss: 1.847257]\n",
      "epoch:15 step:14438 [D loss: 0.618842, acc: 67.97%] [G loss: 2.002788]\n",
      "epoch:15 step:14439 [D loss: 0.669638, acc: 62.50%] [G loss: 2.063389]\n",
      "epoch:15 step:14440 [D loss: 0.649934, acc: 68.75%] [G loss: 1.875218]\n",
      "epoch:15 step:14441 [D loss: 0.593589, acc: 69.53%] [G loss: 1.884451]\n",
      "epoch:15 step:14442 [D loss: 0.701925, acc: 56.25%] [G loss: 1.905696]\n",
      "epoch:15 step:14443 [D loss: 0.663347, acc: 62.50%] [G loss: 1.831224]\n",
      "epoch:15 step:14444 [D loss: 0.608230, acc: 64.84%] [G loss: 1.924797]\n",
      "epoch:15 step:14445 [D loss: 0.669051, acc: 62.50%] [G loss: 1.837312]\n",
      "epoch:15 step:14446 [D loss: 0.632785, acc: 66.41%] [G loss: 1.809503]\n",
      "epoch:15 step:14447 [D loss: 0.606717, acc: 67.97%] [G loss: 1.813085]\n",
      "epoch:15 step:14448 [D loss: 0.653021, acc: 64.84%] [G loss: 1.914437]\n",
      "epoch:15 step:14449 [D loss: 0.694165, acc: 55.47%] [G loss: 1.816888]\n",
      "epoch:15 step:14450 [D loss: 0.584621, acc: 72.66%] [G loss: 1.944426]\n",
      "epoch:15 step:14451 [D loss: 0.682752, acc: 60.94%] [G loss: 1.853827]\n",
      "epoch:15 step:14452 [D loss: 0.669170, acc: 59.38%] [G loss: 1.911728]\n",
      "epoch:15 step:14453 [D loss: 0.591542, acc: 64.84%] [G loss: 1.949340]\n",
      "epoch:15 step:14454 [D loss: 0.609609, acc: 67.19%] [G loss: 2.045775]\n",
      "epoch:15 step:14455 [D loss: 0.662070, acc: 60.94%] [G loss: 1.883040]\n",
      "epoch:15 step:14456 [D loss: 0.640101, acc: 62.50%] [G loss: 1.954879]\n",
      "epoch:15 step:14457 [D loss: 0.573982, acc: 69.53%] [G loss: 1.973759]\n",
      "epoch:15 step:14458 [D loss: 0.595212, acc: 66.41%] [G loss: 2.111886]\n",
      "epoch:15 step:14459 [D loss: 0.618430, acc: 61.72%] [G loss: 2.067765]\n",
      "epoch:15 step:14460 [D loss: 0.623336, acc: 70.31%] [G loss: 2.152448]\n",
      "epoch:15 step:14461 [D loss: 0.585782, acc: 69.53%] [G loss: 2.131006]\n",
      "epoch:15 step:14462 [D loss: 0.642870, acc: 62.50%] [G loss: 2.024512]\n",
      "epoch:15 step:14463 [D loss: 0.663389, acc: 60.94%] [G loss: 1.805420]\n",
      "epoch:15 step:14464 [D loss: 0.637937, acc: 67.19%] [G loss: 2.039127]\n",
      "epoch:15 step:14465 [D loss: 0.625067, acc: 66.41%] [G loss: 2.050752]\n",
      "epoch:15 step:14466 [D loss: 0.656507, acc: 59.38%] [G loss: 2.002577]\n",
      "epoch:15 step:14467 [D loss: 0.655776, acc: 61.72%] [G loss: 2.053075]\n",
      "epoch:15 step:14468 [D loss: 0.619649, acc: 66.41%] [G loss: 2.014786]\n",
      "epoch:15 step:14469 [D loss: 0.683030, acc: 61.72%] [G loss: 2.153979]\n",
      "epoch:15 step:14470 [D loss: 0.675865, acc: 64.06%] [G loss: 1.949223]\n",
      "epoch:15 step:14471 [D loss: 0.601533, acc: 69.53%] [G loss: 2.100389]\n",
      "epoch:15 step:14472 [D loss: 0.685500, acc: 60.94%] [G loss: 1.799502]\n",
      "epoch:15 step:14473 [D loss: 0.627735, acc: 60.94%] [G loss: 2.104880]\n",
      "epoch:15 step:14474 [D loss: 0.658549, acc: 58.59%] [G loss: 2.038051]\n",
      "epoch:15 step:14475 [D loss: 0.611132, acc: 69.53%] [G loss: 1.872307]\n",
      "epoch:15 step:14476 [D loss: 0.677908, acc: 60.94%] [G loss: 1.855031]\n",
      "epoch:15 step:14477 [D loss: 0.659892, acc: 64.06%] [G loss: 1.875002]\n",
      "epoch:15 step:14478 [D loss: 0.665082, acc: 56.25%] [G loss: 1.962577]\n",
      "epoch:15 step:14479 [D loss: 0.678744, acc: 59.38%] [G loss: 1.906869]\n",
      "epoch:15 step:14480 [D loss: 0.643178, acc: 60.94%] [G loss: 1.877995]\n",
      "epoch:15 step:14481 [D loss: 0.634969, acc: 65.62%] [G loss: 2.060172]\n",
      "epoch:15 step:14482 [D loss: 0.605334, acc: 68.75%] [G loss: 2.275127]\n",
      "epoch:15 step:14483 [D loss: 0.576954, acc: 74.22%] [G loss: 2.263049]\n",
      "epoch:15 step:14484 [D loss: 0.644908, acc: 64.06%] [G loss: 2.098745]\n",
      "epoch:15 step:14485 [D loss: 0.565444, acc: 73.44%] [G loss: 2.168652]\n",
      "epoch:15 step:14486 [D loss: 0.710395, acc: 63.28%] [G loss: 1.909460]\n",
      "epoch:15 step:14487 [D loss: 0.691804, acc: 57.03%] [G loss: 1.821688]\n",
      "epoch:15 step:14488 [D loss: 0.704413, acc: 53.12%] [G loss: 1.876861]\n",
      "epoch:15 step:14489 [D loss: 0.629162, acc: 65.62%] [G loss: 1.921305]\n",
      "epoch:15 step:14490 [D loss: 0.630941, acc: 62.50%] [G loss: 1.893019]\n",
      "epoch:15 step:14491 [D loss: 0.631498, acc: 61.72%] [G loss: 1.963003]\n",
      "epoch:15 step:14492 [D loss: 0.736848, acc: 53.12%] [G loss: 1.762408]\n",
      "epoch:15 step:14493 [D loss: 0.660730, acc: 64.06%] [G loss: 1.798459]\n",
      "epoch:15 step:14494 [D loss: 0.689017, acc: 60.94%] [G loss: 1.772312]\n",
      "epoch:15 step:14495 [D loss: 0.693009, acc: 54.69%] [G loss: 1.856669]\n",
      "epoch:15 step:14496 [D loss: 0.624120, acc: 67.97%] [G loss: 1.838562]\n",
      "epoch:15 step:14497 [D loss: 0.669245, acc: 63.28%] [G loss: 1.768980]\n",
      "epoch:15 step:14498 [D loss: 0.679955, acc: 57.81%] [G loss: 1.868520]\n",
      "epoch:15 step:14499 [D loss: 0.664422, acc: 60.94%] [G loss: 1.565112]\n",
      "epoch:15 step:14500 [D loss: 0.642214, acc: 61.72%] [G loss: 1.820004]\n",
      "epoch:15 step:14501 [D loss: 0.674412, acc: 57.03%] [G loss: 1.680660]\n",
      "epoch:15 step:14502 [D loss: 0.646849, acc: 64.06%] [G loss: 1.864339]\n",
      "epoch:15 step:14503 [D loss: 0.656045, acc: 65.62%] [G loss: 1.759386]\n",
      "epoch:15 step:14504 [D loss: 0.609305, acc: 67.19%] [G loss: 1.870776]\n",
      "epoch:15 step:14505 [D loss: 0.644861, acc: 61.72%] [G loss: 1.863781]\n",
      "epoch:15 step:14506 [D loss: 0.662517, acc: 59.38%] [G loss: 1.906794]\n",
      "epoch:15 step:14507 [D loss: 0.671620, acc: 59.38%] [G loss: 1.875834]\n",
      "epoch:15 step:14508 [D loss: 0.620155, acc: 64.06%] [G loss: 1.817355]\n",
      "epoch:15 step:14509 [D loss: 0.675170, acc: 57.81%] [G loss: 1.806236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14510 [D loss: 0.665919, acc: 64.06%] [G loss: 1.974371]\n",
      "epoch:15 step:14511 [D loss: 0.659702, acc: 61.72%] [G loss: 1.931311]\n",
      "epoch:15 step:14512 [D loss: 0.596309, acc: 65.62%] [G loss: 1.952863]\n",
      "epoch:15 step:14513 [D loss: 0.641355, acc: 60.16%] [G loss: 1.733048]\n",
      "epoch:15 step:14514 [D loss: 0.671307, acc: 55.47%] [G loss: 1.752553]\n",
      "epoch:15 step:14515 [D loss: 0.677197, acc: 53.91%] [G loss: 1.767841]\n",
      "epoch:15 step:14516 [D loss: 0.646011, acc: 61.72%] [G loss: 1.725797]\n",
      "epoch:15 step:14517 [D loss: 0.599735, acc: 67.19%] [G loss: 1.881926]\n",
      "epoch:15 step:14518 [D loss: 0.600090, acc: 69.53%] [G loss: 2.020042]\n",
      "epoch:15 step:14519 [D loss: 0.680252, acc: 57.03%] [G loss: 1.894883]\n",
      "epoch:15 step:14520 [D loss: 0.676580, acc: 58.59%] [G loss: 1.890386]\n",
      "epoch:15 step:14521 [D loss: 0.626894, acc: 69.53%] [G loss: 1.979021]\n",
      "epoch:15 step:14522 [D loss: 0.629407, acc: 64.06%] [G loss: 1.879852]\n",
      "epoch:15 step:14523 [D loss: 0.596557, acc: 65.62%] [G loss: 2.190337]\n",
      "epoch:15 step:14524 [D loss: 0.558460, acc: 71.09%] [G loss: 2.112218]\n",
      "epoch:15 step:14525 [D loss: 0.609486, acc: 71.09%] [G loss: 2.188945]\n",
      "epoch:15 step:14526 [D loss: 0.572834, acc: 70.31%] [G loss: 2.388392]\n",
      "epoch:15 step:14527 [D loss: 0.599721, acc: 68.75%] [G loss: 2.273154]\n",
      "epoch:15 step:14528 [D loss: 0.689502, acc: 53.91%] [G loss: 1.850600]\n",
      "epoch:15 step:14529 [D loss: 0.700682, acc: 53.91%] [G loss: 1.850319]\n",
      "epoch:15 step:14530 [D loss: 0.677062, acc: 59.38%] [G loss: 1.860470]\n",
      "epoch:15 step:14531 [D loss: 0.642006, acc: 67.19%] [G loss: 1.946034]\n",
      "epoch:15 step:14532 [D loss: 0.638229, acc: 58.59%] [G loss: 1.780083]\n",
      "epoch:15 step:14533 [D loss: 0.708241, acc: 60.94%] [G loss: 1.776693]\n",
      "epoch:15 step:14534 [D loss: 0.639820, acc: 62.50%] [G loss: 1.979971]\n",
      "epoch:15 step:14535 [D loss: 0.579993, acc: 69.53%] [G loss: 2.038278]\n",
      "epoch:15 step:14536 [D loss: 0.584800, acc: 70.31%] [G loss: 2.320391]\n",
      "epoch:15 step:14537 [D loss: 0.659662, acc: 64.84%] [G loss: 1.822065]\n",
      "epoch:15 step:14538 [D loss: 0.714267, acc: 53.91%] [G loss: 1.723142]\n",
      "epoch:15 step:14539 [D loss: 0.640732, acc: 62.50%] [G loss: 1.841365]\n",
      "epoch:15 step:14540 [D loss: 0.653856, acc: 60.94%] [G loss: 1.770051]\n",
      "epoch:15 step:14541 [D loss: 0.616395, acc: 64.06%] [G loss: 2.017559]\n",
      "epoch:15 step:14542 [D loss: 0.764740, acc: 50.78%] [G loss: 1.795537]\n",
      "epoch:15 step:14543 [D loss: 0.640784, acc: 63.28%] [G loss: 1.958969]\n",
      "epoch:15 step:14544 [D loss: 0.659289, acc: 59.38%] [G loss: 1.946345]\n",
      "epoch:15 step:14545 [D loss: 0.638817, acc: 64.06%] [G loss: 1.983026]\n",
      "epoch:15 step:14546 [D loss: 0.691976, acc: 54.69%] [G loss: 1.788344]\n",
      "epoch:15 step:14547 [D loss: 0.687930, acc: 53.91%] [G loss: 1.853522]\n",
      "epoch:15 step:14548 [D loss: 0.672101, acc: 61.72%] [G loss: 1.920007]\n",
      "epoch:15 step:14549 [D loss: 0.627904, acc: 60.16%] [G loss: 2.063801]\n",
      "epoch:15 step:14550 [D loss: 0.591951, acc: 68.75%] [G loss: 1.977823]\n",
      "epoch:15 step:14551 [D loss: 0.600652, acc: 66.41%] [G loss: 1.902371]\n",
      "epoch:15 step:14552 [D loss: 0.601030, acc: 66.41%] [G loss: 1.933373]\n",
      "epoch:15 step:14553 [D loss: 0.624138, acc: 64.06%] [G loss: 1.908137]\n",
      "epoch:15 step:14554 [D loss: 0.550442, acc: 75.00%] [G loss: 2.055415]\n",
      "epoch:15 step:14555 [D loss: 0.665329, acc: 61.72%] [G loss: 1.750391]\n",
      "epoch:15 step:14556 [D loss: 0.740323, acc: 50.00%] [G loss: 1.828105]\n",
      "epoch:15 step:14557 [D loss: 0.648449, acc: 63.28%] [G loss: 1.717232]\n",
      "epoch:15 step:14558 [D loss: 0.645096, acc: 61.72%] [G loss: 1.798452]\n",
      "epoch:15 step:14559 [D loss: 0.635535, acc: 64.06%] [G loss: 1.937859]\n",
      "epoch:15 step:14560 [D loss: 0.633096, acc: 64.06%] [G loss: 1.954538]\n",
      "epoch:15 step:14561 [D loss: 0.646303, acc: 60.16%] [G loss: 1.859832]\n",
      "epoch:15 step:14562 [D loss: 0.655020, acc: 57.03%] [G loss: 1.944969]\n",
      "epoch:15 step:14563 [D loss: 0.580689, acc: 69.53%] [G loss: 2.111314]\n",
      "epoch:15 step:14564 [D loss: 0.681543, acc: 63.28%] [G loss: 1.861578]\n",
      "epoch:15 step:14565 [D loss: 0.645352, acc: 63.28%] [G loss: 1.793428]\n",
      "epoch:15 step:14566 [D loss: 0.744570, acc: 46.09%] [G loss: 1.621408]\n",
      "epoch:15 step:14567 [D loss: 0.684252, acc: 53.91%] [G loss: 1.922109]\n",
      "epoch:15 step:14568 [D loss: 0.627683, acc: 64.84%] [G loss: 1.828279]\n",
      "epoch:15 step:14569 [D loss: 0.686646, acc: 58.59%] [G loss: 1.827129]\n",
      "epoch:15 step:14570 [D loss: 0.622101, acc: 69.53%] [G loss: 1.823096]\n",
      "epoch:15 step:14571 [D loss: 0.652972, acc: 61.72%] [G loss: 2.067259]\n",
      "epoch:15 step:14572 [D loss: 0.686462, acc: 57.81%] [G loss: 2.062304]\n",
      "epoch:15 step:14573 [D loss: 0.667823, acc: 58.59%] [G loss: 1.804023]\n",
      "epoch:15 step:14574 [D loss: 0.592720, acc: 69.53%] [G loss: 2.084065]\n",
      "epoch:15 step:14575 [D loss: 0.556440, acc: 71.88%] [G loss: 2.011848]\n",
      "epoch:15 step:14576 [D loss: 0.691703, acc: 59.38%] [G loss: 1.879100]\n",
      "epoch:15 step:14577 [D loss: 0.637452, acc: 63.28%] [G loss: 1.957678]\n",
      "epoch:15 step:14578 [D loss: 0.643931, acc: 62.50%] [G loss: 1.976495]\n",
      "epoch:15 step:14579 [D loss: 0.593115, acc: 71.09%] [G loss: 1.800881]\n",
      "epoch:15 step:14580 [D loss: 0.652389, acc: 60.94%] [G loss: 1.946193]\n",
      "epoch:15 step:14581 [D loss: 0.602895, acc: 65.62%] [G loss: 1.941115]\n",
      "epoch:15 step:14582 [D loss: 0.642971, acc: 67.97%] [G loss: 1.957682]\n",
      "epoch:15 step:14583 [D loss: 0.661864, acc: 60.16%] [G loss: 1.735121]\n",
      "epoch:15 step:14584 [D loss: 0.691853, acc: 61.72%] [G loss: 1.882022]\n",
      "epoch:15 step:14585 [D loss: 0.648735, acc: 60.16%] [G loss: 1.871469]\n",
      "epoch:15 step:14586 [D loss: 0.666476, acc: 60.94%] [G loss: 1.851479]\n",
      "epoch:15 step:14587 [D loss: 0.591046, acc: 72.66%] [G loss: 2.037784]\n",
      "epoch:15 step:14588 [D loss: 0.668511, acc: 57.81%] [G loss: 1.905494]\n",
      "epoch:15 step:14589 [D loss: 0.635891, acc: 62.50%] [G loss: 2.024480]\n",
      "epoch:15 step:14590 [D loss: 0.636487, acc: 64.06%] [G loss: 1.890237]\n",
      "epoch:15 step:14591 [D loss: 0.669346, acc: 56.25%] [G loss: 1.964556]\n",
      "epoch:15 step:14592 [D loss: 0.638637, acc: 57.81%] [G loss: 1.943434]\n",
      "epoch:15 step:14593 [D loss: 0.639106, acc: 59.38%] [G loss: 1.737045]\n",
      "epoch:15 step:14594 [D loss: 0.633857, acc: 64.06%] [G loss: 1.903060]\n",
      "epoch:15 step:14595 [D loss: 0.594961, acc: 66.41%] [G loss: 1.875259]\n",
      "epoch:15 step:14596 [D loss: 0.657602, acc: 60.94%] [G loss: 1.887280]\n",
      "epoch:15 step:14597 [D loss: 0.704949, acc: 57.81%] [G loss: 1.757867]\n",
      "epoch:15 step:14598 [D loss: 0.641259, acc: 58.59%] [G loss: 1.996635]\n",
      "epoch:15 step:14599 [D loss: 0.673480, acc: 58.59%] [G loss: 1.814590]\n",
      "epoch:15 step:14600 [D loss: 0.609881, acc: 67.97%] [G loss: 1.990401]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.060319\n",
      "FID: 16.684486\n",
      "0 = 12.601724291181556\n",
      "1 = 0.07675961402545556\n",
      "2 = 0.885200023651123\n",
      "3 = 0.8996000289916992\n",
      "4 = 0.8708000183105469\n",
      "5 = 0.8744167685508728\n",
      "6 = 0.8996000289916992\n",
      "7 = 6.766915252840518\n",
      "8 = 0.08062409156464959\n",
      "9 = 0.7401999831199646\n",
      "10 = 0.7487999796867371\n",
      "11 = 0.7315999865531921\n",
      "12 = 0.7361384034156799\n",
      "13 = 0.7487999796867371\n",
      "14 = 7.060346603393555\n",
      "15 = 9.396162986755371\n",
      "16 = 0.130608469247818\n",
      "17 = 7.060319423675537\n",
      "18 = 16.684486389160156\n",
      "epoch:15 step:14601 [D loss: 0.638016, acc: 64.84%] [G loss: 2.010164]\n",
      "epoch:15 step:14602 [D loss: 0.613534, acc: 68.75%] [G loss: 1.996504]\n",
      "epoch:15 step:14603 [D loss: 0.618444, acc: 62.50%] [G loss: 1.789750]\n",
      "epoch:15 step:14604 [D loss: 0.615448, acc: 59.38%] [G loss: 2.009475]\n",
      "epoch:15 step:14605 [D loss: 0.639831, acc: 60.94%] [G loss: 2.093996]\n",
      "epoch:15 step:14606 [D loss: 0.593481, acc: 69.53%] [G loss: 1.955797]\n",
      "epoch:15 step:14607 [D loss: 0.620036, acc: 66.41%] [G loss: 1.921248]\n",
      "epoch:15 step:14608 [D loss: 0.676546, acc: 58.59%] [G loss: 1.790356]\n",
      "epoch:15 step:14609 [D loss: 0.627322, acc: 66.41%] [G loss: 2.063308]\n",
      "epoch:15 step:14610 [D loss: 0.604266, acc: 69.53%] [G loss: 1.992332]\n",
      "epoch:15 step:14611 [D loss: 0.636564, acc: 66.41%] [G loss: 2.068038]\n",
      "epoch:15 step:14612 [D loss: 0.672042, acc: 65.62%] [G loss: 1.999804]\n",
      "epoch:15 step:14613 [D loss: 0.598372, acc: 64.84%] [G loss: 2.037763]\n",
      "epoch:15 step:14614 [D loss: 0.689399, acc: 57.81%] [G loss: 1.725149]\n",
      "epoch:15 step:14615 [D loss: 0.724412, acc: 53.91%] [G loss: 1.781382]\n",
      "epoch:15 step:14616 [D loss: 0.660732, acc: 60.94%] [G loss: 1.855465]\n",
      "epoch:15 step:14617 [D loss: 0.666660, acc: 65.62%] [G loss: 1.738802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14618 [D loss: 0.647150, acc: 65.62%] [G loss: 1.913203]\n",
      "epoch:15 step:14619 [D loss: 0.622600, acc: 65.62%] [G loss: 2.096363]\n",
      "epoch:15 step:14620 [D loss: 0.664395, acc: 54.69%] [G loss: 1.852667]\n",
      "epoch:15 step:14621 [D loss: 0.705047, acc: 55.47%] [G loss: 1.808561]\n",
      "epoch:15 step:14622 [D loss: 0.648455, acc: 58.59%] [G loss: 1.798308]\n",
      "epoch:15 step:14623 [D loss: 0.676362, acc: 59.38%] [G loss: 1.856549]\n",
      "epoch:15 step:14624 [D loss: 0.639452, acc: 65.62%] [G loss: 1.860172]\n",
      "epoch:15 step:14625 [D loss: 0.675770, acc: 60.94%] [G loss: 1.930954]\n",
      "epoch:15 step:14626 [D loss: 0.639507, acc: 64.84%] [G loss: 1.926590]\n",
      "epoch:15 step:14627 [D loss: 0.684947, acc: 57.03%] [G loss: 1.813121]\n",
      "epoch:15 step:14628 [D loss: 0.615675, acc: 67.97%] [G loss: 1.918658]\n",
      "epoch:15 step:14629 [D loss: 0.593820, acc: 71.88%] [G loss: 2.021880]\n",
      "epoch:15 step:14630 [D loss: 0.666341, acc: 64.06%] [G loss: 1.962975]\n",
      "epoch:15 step:14631 [D loss: 0.659366, acc: 64.06%] [G loss: 1.813718]\n",
      "epoch:15 step:14632 [D loss: 0.698288, acc: 50.00%] [G loss: 1.835699]\n",
      "epoch:15 step:14633 [D loss: 0.654948, acc: 60.16%] [G loss: 1.749595]\n",
      "epoch:15 step:14634 [D loss: 0.633656, acc: 62.50%] [G loss: 1.790391]\n",
      "epoch:15 step:14635 [D loss: 0.680498, acc: 57.03%] [G loss: 1.743894]\n",
      "epoch:15 step:14636 [D loss: 0.616914, acc: 66.41%] [G loss: 1.887376]\n",
      "epoch:15 step:14637 [D loss: 0.605223, acc: 71.88%] [G loss: 1.942293]\n",
      "epoch:15 step:14638 [D loss: 0.652623, acc: 60.94%] [G loss: 1.953338]\n",
      "epoch:15 step:14639 [D loss: 0.628309, acc: 63.28%] [G loss: 1.857537]\n",
      "epoch:15 step:14640 [D loss: 0.632468, acc: 66.41%] [G loss: 2.066324]\n",
      "epoch:15 step:14641 [D loss: 0.609931, acc: 61.72%] [G loss: 1.946828]\n",
      "epoch:15 step:14642 [D loss: 0.673153, acc: 54.69%] [G loss: 1.954105]\n",
      "epoch:15 step:14643 [D loss: 0.682754, acc: 59.38%] [G loss: 1.954415]\n",
      "epoch:15 step:14644 [D loss: 0.698620, acc: 60.94%] [G loss: 1.963599]\n",
      "epoch:15 step:14645 [D loss: 0.672656, acc: 60.94%] [G loss: 1.878901]\n",
      "epoch:15 step:14646 [D loss: 0.687894, acc: 59.38%] [G loss: 1.898529]\n",
      "epoch:15 step:14647 [D loss: 0.622564, acc: 66.41%] [G loss: 1.939259]\n",
      "epoch:15 step:14648 [D loss: 0.656132, acc: 63.28%] [G loss: 1.767562]\n",
      "epoch:15 step:14649 [D loss: 0.663638, acc: 58.59%] [G loss: 1.764615]\n",
      "epoch:15 step:14650 [D loss: 0.652161, acc: 62.50%] [G loss: 1.775446]\n",
      "epoch:15 step:14651 [D loss: 0.643397, acc: 61.72%] [G loss: 1.880184]\n",
      "epoch:15 step:14652 [D loss: 0.681546, acc: 57.81%] [G loss: 1.813769]\n",
      "epoch:15 step:14653 [D loss: 0.684217, acc: 53.91%] [G loss: 1.904771]\n",
      "epoch:15 step:14654 [D loss: 0.633886, acc: 60.16%] [G loss: 1.864027]\n",
      "epoch:15 step:14655 [D loss: 0.634996, acc: 64.06%] [G loss: 1.734697]\n",
      "epoch:15 step:14656 [D loss: 0.685434, acc: 59.38%] [G loss: 1.878943]\n",
      "epoch:15 step:14657 [D loss: 0.636561, acc: 65.62%] [G loss: 1.815398]\n",
      "epoch:15 step:14658 [D loss: 0.655267, acc: 68.75%] [G loss: 1.827084]\n",
      "epoch:15 step:14659 [D loss: 0.646345, acc: 63.28%] [G loss: 1.908508]\n",
      "epoch:15 step:14660 [D loss: 0.660303, acc: 63.28%] [G loss: 2.120828]\n",
      "epoch:15 step:14661 [D loss: 0.661519, acc: 55.47%] [G loss: 1.723097]\n",
      "epoch:15 step:14662 [D loss: 0.618169, acc: 65.62%] [G loss: 2.037705]\n",
      "epoch:15 step:14663 [D loss: 0.645438, acc: 65.62%] [G loss: 1.899492]\n",
      "epoch:15 step:14664 [D loss: 0.616229, acc: 69.53%] [G loss: 1.811863]\n",
      "epoch:15 step:14665 [D loss: 0.609289, acc: 64.84%] [G loss: 1.941947]\n",
      "epoch:15 step:14666 [D loss: 0.687670, acc: 58.59%] [G loss: 1.829773]\n",
      "epoch:15 step:14667 [D loss: 0.707256, acc: 53.12%] [G loss: 1.753948]\n",
      "epoch:15 step:14668 [D loss: 0.707462, acc: 56.25%] [G loss: 1.962901]\n",
      "epoch:15 step:14669 [D loss: 0.651635, acc: 60.94%] [G loss: 1.943347]\n",
      "epoch:15 step:14670 [D loss: 0.739871, acc: 54.69%] [G loss: 1.788148]\n",
      "epoch:15 step:14671 [D loss: 0.665135, acc: 61.72%] [G loss: 1.927209]\n",
      "epoch:15 step:14672 [D loss: 0.648396, acc: 64.84%] [G loss: 1.740539]\n",
      "epoch:15 step:14673 [D loss: 0.644278, acc: 61.72%] [G loss: 1.834173]\n",
      "epoch:15 step:14674 [D loss: 0.665746, acc: 58.59%] [G loss: 1.742263]\n",
      "epoch:15 step:14675 [D loss: 0.698459, acc: 56.25%] [G loss: 1.822558]\n",
      "epoch:15 step:14676 [D loss: 0.649880, acc: 60.16%] [G loss: 1.733601]\n",
      "epoch:15 step:14677 [D loss: 0.632486, acc: 66.41%] [G loss: 1.947943]\n",
      "epoch:15 step:14678 [D loss: 0.692077, acc: 54.69%] [G loss: 1.951826]\n",
      "epoch:15 step:14679 [D loss: 0.644153, acc: 63.28%] [G loss: 1.963491]\n",
      "epoch:15 step:14680 [D loss: 0.673932, acc: 62.50%] [G loss: 1.631535]\n",
      "epoch:15 step:14681 [D loss: 0.649652, acc: 57.81%] [G loss: 1.969793]\n",
      "epoch:15 step:14682 [D loss: 0.689073, acc: 59.38%] [G loss: 1.806923]\n",
      "epoch:15 step:14683 [D loss: 0.626025, acc: 64.06%] [G loss: 1.869057]\n",
      "epoch:15 step:14684 [D loss: 0.635752, acc: 67.97%] [G loss: 1.817431]\n",
      "epoch:15 step:14685 [D loss: 0.619379, acc: 64.84%] [G loss: 1.831077]\n",
      "epoch:15 step:14686 [D loss: 0.618282, acc: 67.19%] [G loss: 1.977740]\n",
      "epoch:15 step:14687 [D loss: 0.658457, acc: 60.16%] [G loss: 1.931277]\n",
      "epoch:15 step:14688 [D loss: 0.622090, acc: 64.06%] [G loss: 2.062109]\n",
      "epoch:15 step:14689 [D loss: 0.666581, acc: 59.38%] [G loss: 1.921771]\n",
      "epoch:15 step:14690 [D loss: 0.639385, acc: 64.06%] [G loss: 1.964681]\n",
      "epoch:15 step:14691 [D loss: 0.619296, acc: 62.50%] [G loss: 1.953802]\n",
      "epoch:15 step:14692 [D loss: 0.651575, acc: 64.84%] [G loss: 1.877233]\n",
      "epoch:15 step:14693 [D loss: 0.613599, acc: 67.19%] [G loss: 2.087080]\n",
      "epoch:15 step:14694 [D loss: 0.657820, acc: 57.81%] [G loss: 1.924724]\n",
      "epoch:15 step:14695 [D loss: 0.666358, acc: 55.47%] [G loss: 1.969322]\n",
      "epoch:15 step:14696 [D loss: 0.574821, acc: 72.66%] [G loss: 2.081358]\n",
      "epoch:15 step:14697 [D loss: 0.617865, acc: 69.53%] [G loss: 2.112194]\n",
      "epoch:15 step:14698 [D loss: 0.666756, acc: 59.38%] [G loss: 1.895067]\n",
      "epoch:15 step:14699 [D loss: 0.644941, acc: 64.06%] [G loss: 1.871857]\n",
      "epoch:15 step:14700 [D loss: 0.629828, acc: 67.97%] [G loss: 1.912004]\n",
      "epoch:15 step:14701 [D loss: 0.666065, acc: 60.16%] [G loss: 2.023014]\n",
      "epoch:15 step:14702 [D loss: 0.618160, acc: 69.53%] [G loss: 1.994476]\n",
      "epoch:15 step:14703 [D loss: 0.578522, acc: 69.53%] [G loss: 2.161870]\n",
      "epoch:15 step:14704 [D loss: 0.534239, acc: 76.56%] [G loss: 2.175298]\n",
      "epoch:15 step:14705 [D loss: 0.614556, acc: 65.62%] [G loss: 2.040925]\n",
      "epoch:15 step:14706 [D loss: 0.655733, acc: 58.59%] [G loss: 1.922967]\n",
      "epoch:15 step:14707 [D loss: 0.590199, acc: 68.75%] [G loss: 2.045171]\n",
      "epoch:15 step:14708 [D loss: 0.642239, acc: 65.62%] [G loss: 1.959935]\n",
      "epoch:15 step:14709 [D loss: 0.625422, acc: 63.28%] [G loss: 2.120421]\n",
      "epoch:15 step:14710 [D loss: 0.651040, acc: 65.62%] [G loss: 1.858390]\n",
      "epoch:15 step:14711 [D loss: 0.674322, acc: 54.69%] [G loss: 1.887541]\n",
      "epoch:15 step:14712 [D loss: 0.648207, acc: 60.16%] [G loss: 1.813327]\n",
      "epoch:15 step:14713 [D loss: 0.666733, acc: 61.72%] [G loss: 1.731114]\n",
      "epoch:15 step:14714 [D loss: 0.643071, acc: 65.62%] [G loss: 2.076773]\n",
      "epoch:15 step:14715 [D loss: 0.590807, acc: 70.31%] [G loss: 1.854040]\n",
      "epoch:15 step:14716 [D loss: 0.661559, acc: 67.19%] [G loss: 1.949662]\n",
      "epoch:15 step:14717 [D loss: 0.640381, acc: 62.50%] [G loss: 1.859821]\n",
      "epoch:15 step:14718 [D loss: 0.671526, acc: 53.12%] [G loss: 2.004084]\n",
      "epoch:15 step:14719 [D loss: 0.633686, acc: 69.53%] [G loss: 2.029517]\n",
      "epoch:15 step:14720 [D loss: 0.706929, acc: 57.03%] [G loss: 1.850352]\n",
      "epoch:15 step:14721 [D loss: 0.682503, acc: 56.25%] [G loss: 1.750430]\n",
      "epoch:15 step:14722 [D loss: 0.694967, acc: 60.94%] [G loss: 1.816143]\n",
      "epoch:15 step:14723 [D loss: 0.656024, acc: 56.25%] [G loss: 1.865265]\n",
      "epoch:15 step:14724 [D loss: 0.630831, acc: 64.06%] [G loss: 1.882397]\n",
      "epoch:15 step:14725 [D loss: 0.700231, acc: 51.56%] [G loss: 1.824854]\n",
      "epoch:15 step:14726 [D loss: 0.656907, acc: 60.16%] [G loss: 1.848067]\n",
      "epoch:15 step:14727 [D loss: 0.652815, acc: 58.59%] [G loss: 1.819867]\n",
      "epoch:15 step:14728 [D loss: 0.679572, acc: 65.62%] [G loss: 1.890888]\n",
      "epoch:15 step:14729 [D loss: 0.622093, acc: 70.31%] [G loss: 1.852308]\n",
      "epoch:15 step:14730 [D loss: 0.614328, acc: 67.97%] [G loss: 1.890548]\n",
      "epoch:15 step:14731 [D loss: 0.628155, acc: 64.06%] [G loss: 1.826046]\n",
      "epoch:15 step:14732 [D loss: 0.599699, acc: 72.66%] [G loss: 1.908488]\n",
      "epoch:15 step:14733 [D loss: 0.632305, acc: 62.50%] [G loss: 1.903692]\n",
      "epoch:15 step:14734 [D loss: 0.569691, acc: 71.88%] [G loss: 2.046789]\n",
      "epoch:15 step:14735 [D loss: 0.566760, acc: 72.66%] [G loss: 1.983630]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14736 [D loss: 0.586773, acc: 68.75%] [G loss: 1.987341]\n",
      "epoch:15 step:14737 [D loss: 0.627275, acc: 67.19%] [G loss: 1.981517]\n",
      "epoch:15 step:14738 [D loss: 0.616483, acc: 67.97%] [G loss: 1.901093]\n",
      "epoch:15 step:14739 [D loss: 0.652551, acc: 58.59%] [G loss: 2.018117]\n",
      "epoch:15 step:14740 [D loss: 0.612903, acc: 69.53%] [G loss: 1.966893]\n",
      "epoch:15 step:14741 [D loss: 0.631965, acc: 62.50%] [G loss: 2.051534]\n",
      "epoch:15 step:14742 [D loss: 0.625476, acc: 63.28%] [G loss: 1.920507]\n",
      "epoch:15 step:14743 [D loss: 0.597567, acc: 71.88%] [G loss: 1.898517]\n",
      "epoch:15 step:14744 [D loss: 0.697152, acc: 60.94%] [G loss: 2.031060]\n",
      "epoch:15 step:14745 [D loss: 0.631961, acc: 63.28%] [G loss: 2.152100]\n",
      "epoch:15 step:14746 [D loss: 0.662822, acc: 60.16%] [G loss: 2.029411]\n",
      "epoch:15 step:14747 [D loss: 0.608585, acc: 66.41%] [G loss: 1.890198]\n",
      "epoch:15 step:14748 [D loss: 0.629184, acc: 65.62%] [G loss: 2.033057]\n",
      "epoch:15 step:14749 [D loss: 0.592815, acc: 67.97%] [G loss: 2.308524]\n",
      "epoch:15 step:14750 [D loss: 0.571157, acc: 71.88%] [G loss: 2.120927]\n",
      "epoch:15 step:14751 [D loss: 0.646594, acc: 56.25%] [G loss: 1.991560]\n",
      "epoch:15 step:14752 [D loss: 0.665755, acc: 62.50%] [G loss: 1.895885]\n",
      "epoch:15 step:14753 [D loss: 0.633888, acc: 60.16%] [G loss: 1.899920]\n",
      "epoch:15 step:14754 [D loss: 0.595408, acc: 66.41%] [G loss: 2.010774]\n",
      "epoch:15 step:14755 [D loss: 0.648371, acc: 60.94%] [G loss: 2.100957]\n",
      "epoch:15 step:14756 [D loss: 0.596374, acc: 71.09%] [G loss: 2.131879]\n",
      "epoch:15 step:14757 [D loss: 0.720037, acc: 58.59%] [G loss: 1.863771]\n",
      "epoch:15 step:14758 [D loss: 0.639851, acc: 66.41%] [G loss: 1.919652]\n",
      "epoch:15 step:14759 [D loss: 0.660862, acc: 58.59%] [G loss: 2.019337]\n",
      "epoch:15 step:14760 [D loss: 0.705745, acc: 58.59%] [G loss: 1.892158]\n",
      "epoch:15 step:14761 [D loss: 0.618292, acc: 67.97%] [G loss: 1.925148]\n",
      "epoch:15 step:14762 [D loss: 0.621314, acc: 67.19%] [G loss: 2.126110]\n",
      "epoch:15 step:14763 [D loss: 0.617490, acc: 63.28%] [G loss: 1.960960]\n",
      "epoch:15 step:14764 [D loss: 0.659121, acc: 69.53%] [G loss: 1.994934]\n",
      "epoch:15 step:14765 [D loss: 0.683002, acc: 56.25%] [G loss: 1.950918]\n",
      "epoch:15 step:14766 [D loss: 0.645638, acc: 59.38%] [G loss: 2.009710]\n",
      "epoch:15 step:14767 [D loss: 0.666159, acc: 65.62%] [G loss: 2.067594]\n",
      "epoch:15 step:14768 [D loss: 0.708034, acc: 57.03%] [G loss: 1.872527]\n",
      "epoch:15 step:14769 [D loss: 0.641132, acc: 62.50%] [G loss: 1.967623]\n",
      "epoch:15 step:14770 [D loss: 0.601982, acc: 71.88%] [G loss: 1.962356]\n",
      "epoch:15 step:14771 [D loss: 0.705378, acc: 55.47%] [G loss: 1.768242]\n",
      "epoch:15 step:14772 [D loss: 0.674429, acc: 61.72%] [G loss: 1.911015]\n",
      "epoch:15 step:14773 [D loss: 0.656107, acc: 62.50%] [G loss: 1.880630]\n",
      "epoch:15 step:14774 [D loss: 0.592066, acc: 72.66%] [G loss: 2.020737]\n",
      "epoch:15 step:14775 [D loss: 0.593574, acc: 67.19%] [G loss: 2.042586]\n",
      "epoch:15 step:14776 [D loss: 0.607913, acc: 66.41%] [G loss: 1.994461]\n",
      "epoch:15 step:14777 [D loss: 0.619634, acc: 71.09%] [G loss: 1.819982]\n",
      "epoch:15 step:14778 [D loss: 0.648285, acc: 63.28%] [G loss: 1.917684]\n",
      "epoch:15 step:14779 [D loss: 0.641707, acc: 65.62%] [G loss: 2.054519]\n",
      "epoch:15 step:14780 [D loss: 0.666755, acc: 66.41%] [G loss: 1.822221]\n",
      "epoch:15 step:14781 [D loss: 0.646361, acc: 64.84%] [G loss: 1.845709]\n",
      "epoch:15 step:14782 [D loss: 0.629785, acc: 68.75%] [G loss: 1.922916]\n",
      "epoch:15 step:14783 [D loss: 0.677947, acc: 61.72%] [G loss: 1.867867]\n",
      "epoch:15 step:14784 [D loss: 0.615243, acc: 65.62%] [G loss: 1.915946]\n",
      "epoch:15 step:14785 [D loss: 0.657889, acc: 61.72%] [G loss: 1.822016]\n",
      "epoch:15 step:14786 [D loss: 0.638669, acc: 63.28%] [G loss: 1.827043]\n",
      "epoch:15 step:14787 [D loss: 0.660581, acc: 63.28%] [G loss: 1.932500]\n",
      "epoch:15 step:14788 [D loss: 0.623496, acc: 64.84%] [G loss: 1.906157]\n",
      "epoch:15 step:14789 [D loss: 0.698205, acc: 57.03%] [G loss: 1.813881]\n",
      "epoch:15 step:14790 [D loss: 0.648492, acc: 67.19%] [G loss: 2.009042]\n",
      "epoch:15 step:14791 [D loss: 0.648502, acc: 62.50%] [G loss: 1.893732]\n",
      "epoch:15 step:14792 [D loss: 0.639282, acc: 61.72%] [G loss: 1.832064]\n",
      "epoch:15 step:14793 [D loss: 0.645664, acc: 65.62%] [G loss: 1.876424]\n",
      "epoch:15 step:14794 [D loss: 0.672380, acc: 60.94%] [G loss: 1.825421]\n",
      "epoch:15 step:14795 [D loss: 0.621068, acc: 67.19%] [G loss: 1.885343]\n",
      "epoch:15 step:14796 [D loss: 0.682887, acc: 57.81%] [G loss: 1.839991]\n",
      "epoch:15 step:14797 [D loss: 0.619782, acc: 64.06%] [G loss: 1.863019]\n",
      "epoch:15 step:14798 [D loss: 0.590362, acc: 71.09%] [G loss: 1.780268]\n",
      "epoch:15 step:14799 [D loss: 0.727141, acc: 55.47%] [G loss: 1.774004]\n",
      "epoch:15 step:14800 [D loss: 0.661237, acc: 61.72%] [G loss: 1.888751]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.014686\n",
      "FID: 19.921663\n",
      "0 = 12.657984668207154\n",
      "1 = 0.07500880507599239\n",
      "2 = 0.8859000205993652\n",
      "3 = 0.9056000113487244\n",
      "4 = 0.8661999702453613\n",
      "5 = 0.8712719082832336\n",
      "6 = 0.9056000113487244\n",
      "7 = 6.963955802500246\n",
      "8 = 0.09173581953930456\n",
      "9 = 0.7523000240325928\n",
      "10 = 0.7598000168800354\n",
      "11 = 0.7447999715805054\n",
      "12 = 0.7485714554786682\n",
      "13 = 0.7598000168800354\n",
      "14 = 7.014715671539307\n",
      "15 = 9.404062271118164\n",
      "16 = 0.12811724841594696\n",
      "17 = 7.014686107635498\n",
      "18 = 19.921663284301758\n",
      "epoch:15 step:14801 [D loss: 0.601788, acc: 67.19%] [G loss: 1.990490]\n",
      "epoch:15 step:14802 [D loss: 0.586039, acc: 66.41%] [G loss: 2.083782]\n",
      "epoch:15 step:14803 [D loss: 0.630998, acc: 66.41%] [G loss: 1.798408]\n",
      "epoch:15 step:14804 [D loss: 0.700551, acc: 54.69%] [G loss: 1.861036]\n",
      "epoch:15 step:14805 [D loss: 0.613087, acc: 67.97%] [G loss: 1.889253]\n",
      "epoch:15 step:14806 [D loss: 0.641647, acc: 64.84%] [G loss: 1.849114]\n",
      "epoch:15 step:14807 [D loss: 0.660998, acc: 58.59%] [G loss: 1.911322]\n",
      "epoch:15 step:14808 [D loss: 0.692058, acc: 64.06%] [G loss: 1.878416]\n",
      "epoch:15 step:14809 [D loss: 0.624030, acc: 65.62%] [G loss: 1.943708]\n",
      "epoch:15 step:14810 [D loss: 0.652866, acc: 63.28%] [G loss: 1.779057]\n",
      "epoch:15 step:14811 [D loss: 0.622866, acc: 67.97%] [G loss: 2.023633]\n",
      "epoch:15 step:14812 [D loss: 0.654121, acc: 60.94%] [G loss: 1.935755]\n",
      "epoch:15 step:14813 [D loss: 0.655835, acc: 56.25%] [G loss: 1.911126]\n",
      "epoch:15 step:14814 [D loss: 0.664588, acc: 57.03%] [G loss: 1.817279]\n",
      "epoch:15 step:14815 [D loss: 0.601419, acc: 71.09%] [G loss: 1.921474]\n",
      "epoch:15 step:14816 [D loss: 0.654813, acc: 60.16%] [G loss: 1.887352]\n",
      "epoch:15 step:14817 [D loss: 0.676618, acc: 60.16%] [G loss: 1.843928]\n",
      "epoch:15 step:14818 [D loss: 0.664520, acc: 61.72%] [G loss: 1.831637]\n",
      "epoch:15 step:14819 [D loss: 0.649063, acc: 58.59%] [G loss: 1.842543]\n",
      "epoch:15 step:14820 [D loss: 0.676906, acc: 56.25%] [G loss: 1.709544]\n",
      "epoch:15 step:14821 [D loss: 0.699373, acc: 56.25%] [G loss: 1.819404]\n",
      "epoch:15 step:14822 [D loss: 0.651049, acc: 66.41%] [G loss: 1.936013]\n",
      "epoch:15 step:14823 [D loss: 0.651929, acc: 65.62%] [G loss: 1.903862]\n",
      "epoch:15 step:14824 [D loss: 0.650396, acc: 67.97%] [G loss: 1.865029]\n",
      "epoch:15 step:14825 [D loss: 0.586865, acc: 69.53%] [G loss: 1.921752]\n",
      "epoch:15 step:14826 [D loss: 0.641482, acc: 60.94%] [G loss: 1.872782]\n",
      "epoch:15 step:14827 [D loss: 0.618801, acc: 70.31%] [G loss: 1.969401]\n",
      "epoch:15 step:14828 [D loss: 0.661048, acc: 57.81%] [G loss: 1.862191]\n",
      "epoch:15 step:14829 [D loss: 0.624500, acc: 61.72%] [G loss: 2.077380]\n",
      "epoch:15 step:14830 [D loss: 0.597531, acc: 72.66%] [G loss: 2.116153]\n",
      "epoch:15 step:14831 [D loss: 0.614125, acc: 64.84%] [G loss: 1.802919]\n",
      "epoch:15 step:14832 [D loss: 0.590315, acc: 67.19%] [G loss: 1.969380]\n",
      "epoch:15 step:14833 [D loss: 0.646191, acc: 64.84%] [G loss: 1.934373]\n",
      "epoch:15 step:14834 [D loss: 0.629803, acc: 64.06%] [G loss: 1.956936]\n",
      "epoch:15 step:14835 [D loss: 0.598414, acc: 70.31%] [G loss: 2.175151]\n",
      "epoch:15 step:14836 [D loss: 0.526373, acc: 74.22%] [G loss: 2.304134]\n",
      "epoch:15 step:14837 [D loss: 0.628408, acc: 62.50%] [G loss: 2.322751]\n",
      "epoch:15 step:14838 [D loss: 0.640475, acc: 62.50%] [G loss: 1.934539]\n",
      "epoch:15 step:14839 [D loss: 0.686541, acc: 57.81%] [G loss: 1.917819]\n",
      "epoch:15 step:14840 [D loss: 0.661441, acc: 60.94%] [G loss: 2.003042]\n",
      "epoch:15 step:14841 [D loss: 0.617127, acc: 69.53%] [G loss: 2.139824]\n",
      "epoch:15 step:14842 [D loss: 0.681751, acc: 58.59%] [G loss: 1.983427]\n",
      "epoch:15 step:14843 [D loss: 0.635419, acc: 61.72%] [G loss: 2.011830]\n",
      "epoch:15 step:14844 [D loss: 0.626511, acc: 68.75%] [G loss: 2.231396]\n",
      "epoch:15 step:14845 [D loss: 0.623106, acc: 62.50%] [G loss: 1.988987]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14846 [D loss: 0.718663, acc: 53.91%] [G loss: 1.958531]\n",
      "epoch:15 step:14847 [D loss: 0.633588, acc: 67.19%] [G loss: 2.009566]\n",
      "epoch:15 step:14848 [D loss: 0.661636, acc: 64.06%] [G loss: 1.983835]\n",
      "epoch:15 step:14849 [D loss: 0.701133, acc: 55.47%] [G loss: 1.739173]\n",
      "epoch:15 step:14850 [D loss: 0.654607, acc: 61.72%] [G loss: 1.832366]\n",
      "epoch:15 step:14851 [D loss: 0.641488, acc: 60.16%] [G loss: 1.944018]\n",
      "epoch:15 step:14852 [D loss: 0.640493, acc: 64.84%] [G loss: 1.904114]\n",
      "epoch:15 step:14853 [D loss: 0.674736, acc: 57.81%] [G loss: 1.922826]\n",
      "epoch:15 step:14854 [D loss: 0.601726, acc: 67.19%] [G loss: 1.825573]\n",
      "epoch:15 step:14855 [D loss: 0.657362, acc: 62.50%] [G loss: 1.856227]\n",
      "epoch:15 step:14856 [D loss: 0.664217, acc: 54.69%] [G loss: 1.943653]\n",
      "epoch:15 step:14857 [D loss: 0.608641, acc: 69.53%] [G loss: 1.969486]\n",
      "epoch:15 step:14858 [D loss: 0.662173, acc: 60.94%] [G loss: 1.831445]\n",
      "epoch:15 step:14859 [D loss: 0.648662, acc: 62.50%] [G loss: 1.919365]\n",
      "epoch:15 step:14860 [D loss: 0.583059, acc: 72.66%] [G loss: 2.038995]\n",
      "epoch:15 step:14861 [D loss: 0.619633, acc: 67.19%] [G loss: 2.081235]\n",
      "epoch:15 step:14862 [D loss: 0.676803, acc: 60.16%] [G loss: 1.978326]\n",
      "epoch:15 step:14863 [D loss: 0.634351, acc: 65.62%] [G loss: 1.990186]\n",
      "epoch:15 step:14864 [D loss: 0.643101, acc: 60.16%] [G loss: 1.883743]\n",
      "epoch:15 step:14865 [D loss: 0.649268, acc: 65.62%] [G loss: 1.920935]\n",
      "epoch:15 step:14866 [D loss: 0.611481, acc: 65.62%] [G loss: 1.908200]\n",
      "epoch:15 step:14867 [D loss: 0.689979, acc: 56.25%] [G loss: 1.636949]\n",
      "epoch:15 step:14868 [D loss: 0.704304, acc: 58.59%] [G loss: 1.757904]\n",
      "epoch:15 step:14869 [D loss: 0.648536, acc: 57.03%] [G loss: 1.902293]\n",
      "epoch:15 step:14870 [D loss: 0.651168, acc: 59.38%] [G loss: 2.081949]\n",
      "epoch:15 step:14871 [D loss: 0.638421, acc: 61.72%] [G loss: 2.045522]\n",
      "epoch:15 step:14872 [D loss: 0.615108, acc: 68.75%] [G loss: 1.811771]\n",
      "epoch:15 step:14873 [D loss: 0.685906, acc: 55.47%] [G loss: 1.801044]\n",
      "epoch:15 step:14874 [D loss: 0.689825, acc: 57.81%] [G loss: 1.900784]\n",
      "epoch:15 step:14875 [D loss: 0.696021, acc: 54.69%] [G loss: 1.776060]\n",
      "epoch:15 step:14876 [D loss: 0.662664, acc: 57.81%] [G loss: 1.860672]\n",
      "epoch:15 step:14877 [D loss: 0.638583, acc: 63.28%] [G loss: 1.833676]\n",
      "epoch:15 step:14878 [D loss: 0.591740, acc: 73.44%] [G loss: 2.042798]\n",
      "epoch:15 step:14879 [D loss: 0.661441, acc: 62.50%] [G loss: 1.855728]\n",
      "epoch:15 step:14880 [D loss: 0.638687, acc: 60.94%] [G loss: 2.002829]\n",
      "epoch:15 step:14881 [D loss: 0.620159, acc: 65.62%] [G loss: 1.862869]\n",
      "epoch:15 step:14882 [D loss: 0.657537, acc: 59.38%] [G loss: 1.766710]\n",
      "epoch:15 step:14883 [D loss: 0.649624, acc: 62.50%] [G loss: 1.867330]\n",
      "epoch:15 step:14884 [D loss: 0.657059, acc: 62.50%] [G loss: 1.780214]\n",
      "epoch:15 step:14885 [D loss: 0.686042, acc: 57.03%] [G loss: 1.817055]\n",
      "epoch:15 step:14886 [D loss: 0.665545, acc: 60.94%] [G loss: 1.850620]\n",
      "epoch:15 step:14887 [D loss: 0.663199, acc: 57.81%] [G loss: 1.897572]\n",
      "epoch:15 step:14888 [D loss: 0.652660, acc: 63.28%] [G loss: 1.880366]\n",
      "epoch:15 step:14889 [D loss: 0.628170, acc: 67.97%] [G loss: 1.851833]\n",
      "epoch:15 step:14890 [D loss: 0.658808, acc: 59.38%] [G loss: 1.892992]\n",
      "epoch:15 step:14891 [D loss: 0.665186, acc: 61.72%] [G loss: 1.914999]\n",
      "epoch:15 step:14892 [D loss: 0.575429, acc: 69.53%] [G loss: 1.945063]\n",
      "epoch:15 step:14893 [D loss: 0.662622, acc: 64.06%] [G loss: 1.730037]\n",
      "epoch:15 step:14894 [D loss: 0.678413, acc: 57.81%] [G loss: 1.987157]\n",
      "epoch:15 step:14895 [D loss: 0.630076, acc: 64.06%] [G loss: 1.839995]\n",
      "epoch:15 step:14896 [D loss: 0.582177, acc: 66.41%] [G loss: 1.792580]\n",
      "epoch:15 step:14897 [D loss: 0.623080, acc: 62.50%] [G loss: 2.014237]\n",
      "epoch:15 step:14898 [D loss: 0.636902, acc: 67.19%] [G loss: 1.866556]\n",
      "epoch:15 step:14899 [D loss: 0.655720, acc: 58.59%] [G loss: 1.885125]\n",
      "epoch:15 step:14900 [D loss: 0.657470, acc: 62.50%] [G loss: 2.104788]\n",
      "epoch:15 step:14901 [D loss: 0.647541, acc: 62.50%] [G loss: 2.002732]\n",
      "epoch:15 step:14902 [D loss: 0.651820, acc: 60.16%] [G loss: 1.863286]\n",
      "epoch:15 step:14903 [D loss: 0.654115, acc: 62.50%] [G loss: 1.791866]\n",
      "epoch:15 step:14904 [D loss: 0.657829, acc: 61.72%] [G loss: 2.222684]\n",
      "epoch:15 step:14905 [D loss: 0.661034, acc: 56.25%] [G loss: 1.889423]\n",
      "epoch:15 step:14906 [D loss: 0.639611, acc: 62.50%] [G loss: 1.997356]\n",
      "epoch:15 step:14907 [D loss: 0.633176, acc: 63.28%] [G loss: 1.848706]\n",
      "epoch:15 step:14908 [D loss: 0.581061, acc: 71.09%] [G loss: 2.049358]\n",
      "epoch:15 step:14909 [D loss: 0.701339, acc: 53.91%] [G loss: 1.848717]\n",
      "epoch:15 step:14910 [D loss: 0.627316, acc: 67.19%] [G loss: 1.867167]\n",
      "epoch:15 step:14911 [D loss: 0.669995, acc: 60.16%] [G loss: 1.835522]\n",
      "epoch:15 step:14912 [D loss: 0.653766, acc: 57.81%] [G loss: 2.051798]\n",
      "epoch:15 step:14913 [D loss: 0.678994, acc: 56.25%] [G loss: 1.763637]\n",
      "epoch:15 step:14914 [D loss: 0.702989, acc: 53.12%] [G loss: 1.867530]\n",
      "epoch:15 step:14915 [D loss: 0.610030, acc: 65.62%] [G loss: 1.990882]\n",
      "epoch:15 step:14916 [D loss: 0.667382, acc: 58.59%] [G loss: 1.810418]\n",
      "epoch:15 step:14917 [D loss: 0.677018, acc: 57.03%] [G loss: 1.760430]\n",
      "epoch:15 step:14918 [D loss: 0.677105, acc: 59.38%] [G loss: 1.737733]\n",
      "epoch:15 step:14919 [D loss: 0.607090, acc: 68.75%] [G loss: 1.746252]\n",
      "epoch:15 step:14920 [D loss: 0.646084, acc: 63.28%] [G loss: 1.760000]\n",
      "epoch:15 step:14921 [D loss: 0.663991, acc: 64.06%] [G loss: 1.862784]\n",
      "epoch:15 step:14922 [D loss: 0.715150, acc: 50.78%] [G loss: 1.703054]\n",
      "epoch:15 step:14923 [D loss: 0.611336, acc: 67.19%] [G loss: 1.985666]\n",
      "epoch:15 step:14924 [D loss: 0.634332, acc: 63.28%] [G loss: 1.809764]\n",
      "epoch:15 step:14925 [D loss: 0.634453, acc: 58.59%] [G loss: 1.830449]\n",
      "epoch:15 step:14926 [D loss: 0.625001, acc: 64.84%] [G loss: 1.874486]\n",
      "epoch:15 step:14927 [D loss: 0.669957, acc: 60.16%] [G loss: 1.762370]\n",
      "epoch:15 step:14928 [D loss: 0.726079, acc: 53.91%] [G loss: 1.785790]\n",
      "epoch:15 step:14929 [D loss: 0.642849, acc: 61.72%] [G loss: 1.739766]\n",
      "epoch:15 step:14930 [D loss: 0.606154, acc: 64.84%] [G loss: 1.951194]\n",
      "epoch:15 step:14931 [D loss: 0.629550, acc: 64.84%] [G loss: 1.962919]\n",
      "epoch:15 step:14932 [D loss: 0.640682, acc: 63.28%] [G loss: 1.947856]\n",
      "epoch:15 step:14933 [D loss: 0.592951, acc: 66.41%] [G loss: 1.972331]\n",
      "epoch:15 step:14934 [D loss: 0.608001, acc: 64.84%] [G loss: 1.852949]\n",
      "epoch:15 step:14935 [D loss: 0.675209, acc: 57.03%] [G loss: 1.828640]\n",
      "epoch:15 step:14936 [D loss: 0.694959, acc: 57.03%] [G loss: 1.870431]\n",
      "epoch:15 step:14937 [D loss: 0.592591, acc: 73.44%] [G loss: 2.013402]\n",
      "epoch:15 step:14938 [D loss: 0.681171, acc: 57.81%] [G loss: 1.909060]\n",
      "epoch:15 step:14939 [D loss: 0.580090, acc: 69.53%] [G loss: 2.070547]\n",
      "epoch:15 step:14940 [D loss: 0.606299, acc: 67.97%] [G loss: 1.983503]\n",
      "epoch:15 step:14941 [D loss: 0.624583, acc: 64.06%] [G loss: 2.146747]\n",
      "epoch:15 step:14942 [D loss: 0.629733, acc: 67.19%] [G loss: 1.987848]\n",
      "epoch:15 step:14943 [D loss: 0.624244, acc: 68.75%] [G loss: 1.990609]\n",
      "epoch:15 step:14944 [D loss: 0.585190, acc: 70.31%] [G loss: 1.927720]\n",
      "epoch:15 step:14945 [D loss: 0.633969, acc: 69.53%] [G loss: 2.001519]\n",
      "epoch:15 step:14946 [D loss: 0.644188, acc: 58.59%] [G loss: 1.999797]\n",
      "epoch:15 step:14947 [D loss: 0.742496, acc: 60.16%] [G loss: 1.846395]\n",
      "epoch:15 step:14948 [D loss: 0.606606, acc: 64.84%] [G loss: 2.115388]\n",
      "epoch:15 step:14949 [D loss: 0.618544, acc: 71.09%] [G loss: 2.028987]\n",
      "epoch:15 step:14950 [D loss: 0.647129, acc: 60.94%] [G loss: 1.917283]\n",
      "epoch:15 step:14951 [D loss: 0.659403, acc: 61.72%] [G loss: 1.945627]\n",
      "epoch:15 step:14952 [D loss: 0.627164, acc: 60.94%] [G loss: 1.908338]\n",
      "epoch:15 step:14953 [D loss: 0.623406, acc: 67.19%] [G loss: 1.976056]\n",
      "epoch:15 step:14954 [D loss: 0.610147, acc: 68.75%] [G loss: 2.037919]\n",
      "epoch:15 step:14955 [D loss: 0.625481, acc: 65.62%] [G loss: 2.134442]\n",
      "epoch:15 step:14956 [D loss: 0.633344, acc: 60.94%] [G loss: 1.920245]\n",
      "epoch:15 step:14957 [D loss: 0.675571, acc: 54.69%] [G loss: 1.964041]\n",
      "epoch:15 step:14958 [D loss: 0.637689, acc: 64.84%] [G loss: 1.903631]\n",
      "epoch:15 step:14959 [D loss: 0.670939, acc: 62.50%] [G loss: 2.024135]\n",
      "epoch:15 step:14960 [D loss: 0.703846, acc: 50.78%] [G loss: 1.914939]\n",
      "epoch:15 step:14961 [D loss: 0.601287, acc: 67.97%] [G loss: 1.967140]\n",
      "epoch:15 step:14962 [D loss: 0.671155, acc: 62.50%] [G loss: 2.007891]\n",
      "epoch:15 step:14963 [D loss: 0.636246, acc: 65.62%] [G loss: 1.990406]\n",
      "epoch:15 step:14964 [D loss: 0.615593, acc: 68.75%] [G loss: 1.984189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14965 [D loss: 0.610538, acc: 64.84%] [G loss: 2.102271]\n",
      "epoch:15 step:14966 [D loss: 0.640906, acc: 64.06%] [G loss: 2.108675]\n",
      "epoch:15 step:14967 [D loss: 0.594998, acc: 69.53%] [G loss: 1.966002]\n",
      "epoch:15 step:14968 [D loss: 0.667212, acc: 57.03%] [G loss: 2.021520]\n",
      "epoch:15 step:14969 [D loss: 0.694039, acc: 57.03%] [G loss: 1.867103]\n",
      "epoch:15 step:14970 [D loss: 0.671493, acc: 60.94%] [G loss: 1.896314]\n",
      "epoch:15 step:14971 [D loss: 0.618388, acc: 64.84%] [G loss: 2.030667]\n",
      "epoch:15 step:14972 [D loss: 0.665519, acc: 58.59%] [G loss: 1.948369]\n",
      "epoch:15 step:14973 [D loss: 0.581578, acc: 75.00%] [G loss: 2.054510]\n",
      "epoch:15 step:14974 [D loss: 0.590280, acc: 67.97%] [G loss: 2.277802]\n",
      "epoch:15 step:14975 [D loss: 0.691266, acc: 56.25%] [G loss: 1.862999]\n",
      "epoch:15 step:14976 [D loss: 0.591779, acc: 70.31%] [G loss: 2.027681]\n",
      "epoch:15 step:14977 [D loss: 0.703239, acc: 56.25%] [G loss: 1.990478]\n",
      "epoch:15 step:14978 [D loss: 0.526405, acc: 78.91%] [G loss: 2.114193]\n",
      "epoch:15 step:14979 [D loss: 0.608260, acc: 67.19%] [G loss: 2.157741]\n",
      "epoch:15 step:14980 [D loss: 0.582992, acc: 70.31%] [G loss: 2.198252]\n",
      "epoch:15 step:14981 [D loss: 0.621334, acc: 62.50%] [G loss: 2.231173]\n",
      "epoch:15 step:14982 [D loss: 0.614645, acc: 67.97%] [G loss: 2.161756]\n",
      "epoch:15 step:14983 [D loss: 0.815459, acc: 46.09%] [G loss: 1.769260]\n",
      "epoch:15 step:14984 [D loss: 0.770653, acc: 44.53%] [G loss: 1.827515]\n",
      "epoch:15 step:14985 [D loss: 0.601438, acc: 68.75%] [G loss: 1.950177]\n",
      "epoch:15 step:14986 [D loss: 0.657948, acc: 59.38%] [G loss: 1.906407]\n",
      "epoch:15 step:14987 [D loss: 0.650583, acc: 61.72%] [G loss: 2.020992]\n",
      "epoch:15 step:14988 [D loss: 0.630585, acc: 64.06%] [G loss: 2.001011]\n",
      "epoch:15 step:14989 [D loss: 0.569496, acc: 71.09%] [G loss: 1.979020]\n",
      "epoch:15 step:14990 [D loss: 0.599792, acc: 70.31%] [G loss: 1.982716]\n",
      "epoch:15 step:14991 [D loss: 0.627130, acc: 62.50%] [G loss: 2.111432]\n",
      "epoch:15 step:14992 [D loss: 0.545491, acc: 78.12%] [G loss: 2.635293]\n",
      "epoch:16 step:14993 [D loss: 0.747752, acc: 52.34%] [G loss: 2.059176]\n",
      "epoch:16 step:14994 [D loss: 0.689666, acc: 59.38%] [G loss: 1.929891]\n",
      "epoch:16 step:14995 [D loss: 0.634306, acc: 63.28%] [G loss: 1.856138]\n",
      "epoch:16 step:14996 [D loss: 0.710639, acc: 57.03%] [G loss: 1.801499]\n",
      "epoch:16 step:14997 [D loss: 0.608575, acc: 67.19%] [G loss: 1.821222]\n",
      "epoch:16 step:14998 [D loss: 0.667571, acc: 61.72%] [G loss: 1.910901]\n",
      "epoch:16 step:14999 [D loss: 0.702275, acc: 57.03%] [G loss: 1.875349]\n",
      "epoch:16 step:15000 [D loss: 0.640098, acc: 64.06%] [G loss: 1.917854]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.307084\n",
      "FID: 14.194555\n",
      "0 = 12.931144967222176\n",
      "1 = 0.09703107996632394\n",
      "2 = 0.8942000269889832\n",
      "3 = 0.9164000153541565\n",
      "4 = 0.871999979019165\n",
      "5 = 0.8774415850639343\n",
      "6 = 0.9164000153541565\n",
      "7 = 6.549082688307781\n",
      "8 = 0.07382014801688536\n",
      "9 = 0.7285000085830688\n",
      "10 = 0.7365999817848206\n",
      "11 = 0.7203999757766724\n",
      "12 = 0.7248573303222656\n",
      "13 = 0.7365999817848206\n",
      "14 = 7.307111740112305\n",
      "15 = 9.470279693603516\n",
      "16 = 0.10588495433330536\n",
      "17 = 7.307084083557129\n",
      "18 = 14.194555282592773\n",
      "epoch:16 step:15001 [D loss: 0.614349, acc: 63.28%] [G loss: 2.033069]\n",
      "epoch:16 step:15002 [D loss: 0.627598, acc: 65.62%] [G loss: 2.025970]\n",
      "epoch:16 step:15003 [D loss: 0.659873, acc: 64.84%] [G loss: 1.855219]\n",
      "epoch:16 step:15004 [D loss: 0.610695, acc: 65.62%] [G loss: 1.809024]\n",
      "epoch:16 step:15005 [D loss: 0.644525, acc: 65.62%] [G loss: 1.957096]\n",
      "epoch:16 step:15006 [D loss: 0.621807, acc: 67.19%] [G loss: 1.839231]\n",
      "epoch:16 step:15007 [D loss: 0.593660, acc: 66.41%] [G loss: 2.080036]\n",
      "epoch:16 step:15008 [D loss: 0.591834, acc: 70.31%] [G loss: 1.991411]\n",
      "epoch:16 step:15009 [D loss: 0.645487, acc: 63.28%] [G loss: 1.950172]\n",
      "epoch:16 step:15010 [D loss: 0.640454, acc: 58.59%] [G loss: 1.977038]\n",
      "epoch:16 step:15011 [D loss: 0.633991, acc: 63.28%] [G loss: 1.995001]\n",
      "epoch:16 step:15012 [D loss: 0.705530, acc: 56.25%] [G loss: 1.745647]\n",
      "epoch:16 step:15013 [D loss: 0.639731, acc: 61.72%] [G loss: 1.917433]\n",
      "epoch:16 step:15014 [D loss: 0.663015, acc: 62.50%] [G loss: 1.886998]\n",
      "epoch:16 step:15015 [D loss: 0.663228, acc: 60.94%] [G loss: 2.114761]\n",
      "epoch:16 step:15016 [D loss: 0.649762, acc: 60.16%] [G loss: 1.912068]\n",
      "epoch:16 step:15017 [D loss: 0.641533, acc: 65.62%] [G loss: 2.079388]\n",
      "epoch:16 step:15018 [D loss: 0.623753, acc: 69.53%] [G loss: 1.767485]\n",
      "epoch:16 step:15019 [D loss: 0.661307, acc: 57.03%] [G loss: 1.861982]\n",
      "epoch:16 step:15020 [D loss: 0.600447, acc: 65.62%] [G loss: 1.919045]\n",
      "epoch:16 step:15021 [D loss: 0.612782, acc: 70.31%] [G loss: 1.934361]\n",
      "epoch:16 step:15022 [D loss: 0.659299, acc: 59.38%] [G loss: 1.950930]\n",
      "epoch:16 step:15023 [D loss: 0.691021, acc: 60.94%] [G loss: 1.846908]\n",
      "epoch:16 step:15024 [D loss: 0.701682, acc: 57.81%] [G loss: 1.699889]\n",
      "epoch:16 step:15025 [D loss: 0.608003, acc: 67.19%] [G loss: 1.760601]\n",
      "epoch:16 step:15026 [D loss: 0.624618, acc: 65.62%] [G loss: 1.878542]\n",
      "epoch:16 step:15027 [D loss: 0.649624, acc: 65.62%] [G loss: 1.791134]\n",
      "epoch:16 step:15028 [D loss: 0.616701, acc: 64.06%] [G loss: 1.962428]\n",
      "epoch:16 step:15029 [D loss: 0.638532, acc: 67.97%] [G loss: 1.880749]\n",
      "epoch:16 step:15030 [D loss: 0.606300, acc: 68.75%] [G loss: 2.000993]\n",
      "epoch:16 step:15031 [D loss: 0.653922, acc: 64.84%] [G loss: 1.939220]\n",
      "epoch:16 step:15032 [D loss: 0.589650, acc: 69.53%] [G loss: 2.128893]\n",
      "epoch:16 step:15033 [D loss: 0.672346, acc: 59.38%] [G loss: 1.847945]\n",
      "epoch:16 step:15034 [D loss: 0.587299, acc: 67.19%] [G loss: 1.910287]\n",
      "epoch:16 step:15035 [D loss: 0.622250, acc: 66.41%] [G loss: 1.983124]\n",
      "epoch:16 step:15036 [D loss: 0.693530, acc: 57.81%] [G loss: 1.925542]\n",
      "epoch:16 step:15037 [D loss: 0.613678, acc: 67.97%] [G loss: 2.013202]\n",
      "epoch:16 step:15038 [D loss: 0.653539, acc: 60.94%] [G loss: 1.913141]\n",
      "epoch:16 step:15039 [D loss: 0.642239, acc: 61.72%] [G loss: 1.944925]\n",
      "epoch:16 step:15040 [D loss: 0.604123, acc: 71.88%] [G loss: 1.982214]\n",
      "epoch:16 step:15041 [D loss: 0.576729, acc: 70.31%] [G loss: 2.091384]\n",
      "epoch:16 step:15042 [D loss: 0.648677, acc: 60.94%] [G loss: 1.976664]\n",
      "epoch:16 step:15043 [D loss: 0.637842, acc: 64.06%] [G loss: 1.875889]\n",
      "epoch:16 step:15044 [D loss: 0.632365, acc: 64.06%] [G loss: 1.789019]\n",
      "epoch:16 step:15045 [D loss: 0.631594, acc: 65.62%] [G loss: 1.919053]\n",
      "epoch:16 step:15046 [D loss: 0.640557, acc: 63.28%] [G loss: 1.971747]\n",
      "epoch:16 step:15047 [D loss: 0.669145, acc: 59.38%] [G loss: 2.032179]\n",
      "epoch:16 step:15048 [D loss: 0.648036, acc: 63.28%] [G loss: 2.101573]\n",
      "epoch:16 step:15049 [D loss: 0.607686, acc: 65.62%] [G loss: 2.016623]\n",
      "epoch:16 step:15050 [D loss: 0.674133, acc: 59.38%] [G loss: 1.974648]\n",
      "epoch:16 step:15051 [D loss: 0.662312, acc: 57.81%] [G loss: 1.918608]\n",
      "epoch:16 step:15052 [D loss: 0.606491, acc: 60.94%] [G loss: 1.842607]\n",
      "epoch:16 step:15053 [D loss: 0.660561, acc: 60.16%] [G loss: 1.805011]\n",
      "epoch:16 step:15054 [D loss: 0.604586, acc: 67.97%] [G loss: 1.940437]\n",
      "epoch:16 step:15055 [D loss: 0.632710, acc: 63.28%] [G loss: 1.844841]\n",
      "epoch:16 step:15056 [D loss: 0.648494, acc: 62.50%] [G loss: 1.990716]\n",
      "epoch:16 step:15057 [D loss: 0.682900, acc: 53.91%] [G loss: 1.848193]\n",
      "epoch:16 step:15058 [D loss: 0.671640, acc: 64.06%] [G loss: 1.923280]\n",
      "epoch:16 step:15059 [D loss: 0.636718, acc: 63.28%] [G loss: 1.916170]\n",
      "epoch:16 step:15060 [D loss: 0.653703, acc: 64.06%] [G loss: 1.935239]\n",
      "epoch:16 step:15061 [D loss: 0.599561, acc: 74.22%] [G loss: 2.092119]\n",
      "epoch:16 step:15062 [D loss: 0.606715, acc: 66.41%] [G loss: 1.903806]\n",
      "epoch:16 step:15063 [D loss: 0.630637, acc: 67.97%] [G loss: 1.764833]\n",
      "epoch:16 step:15064 [D loss: 0.688700, acc: 59.38%] [G loss: 1.837262]\n",
      "epoch:16 step:15065 [D loss: 0.665339, acc: 57.81%] [G loss: 1.834167]\n",
      "epoch:16 step:15066 [D loss: 0.645584, acc: 64.06%] [G loss: 1.944947]\n",
      "epoch:16 step:15067 [D loss: 0.642398, acc: 60.16%] [G loss: 1.950092]\n",
      "epoch:16 step:15068 [D loss: 0.640241, acc: 60.94%] [G loss: 2.058069]\n",
      "epoch:16 step:15069 [D loss: 0.612273, acc: 66.41%] [G loss: 1.885954]\n",
      "epoch:16 step:15070 [D loss: 0.616158, acc: 66.41%] [G loss: 1.840761]\n",
      "epoch:16 step:15071 [D loss: 0.687156, acc: 52.34%] [G loss: 1.819555]\n",
      "epoch:16 step:15072 [D loss: 0.640809, acc: 61.72%] [G loss: 1.953300]\n",
      "epoch:16 step:15073 [D loss: 0.682924, acc: 55.47%] [G loss: 1.846611]\n",
      "epoch:16 step:15074 [D loss: 0.651742, acc: 61.72%] [G loss: 1.900026]\n",
      "epoch:16 step:15075 [D loss: 0.622866, acc: 64.84%] [G loss: 2.003664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15076 [D loss: 0.637554, acc: 65.62%] [G loss: 1.870269]\n",
      "epoch:16 step:15077 [D loss: 0.616785, acc: 63.28%] [G loss: 1.821544]\n",
      "epoch:16 step:15078 [D loss: 0.620833, acc: 68.75%] [G loss: 1.890106]\n",
      "epoch:16 step:15079 [D loss: 0.644421, acc: 64.06%] [G loss: 1.941284]\n",
      "epoch:16 step:15080 [D loss: 0.648526, acc: 65.62%] [G loss: 1.945714]\n",
      "epoch:16 step:15081 [D loss: 0.584309, acc: 66.41%] [G loss: 1.970706]\n",
      "epoch:16 step:15082 [D loss: 0.638444, acc: 64.06%] [G loss: 1.916074]\n",
      "epoch:16 step:15083 [D loss: 0.660480, acc: 59.38%] [G loss: 1.803562]\n",
      "epoch:16 step:15084 [D loss: 0.629783, acc: 70.31%] [G loss: 1.938784]\n",
      "epoch:16 step:15085 [D loss: 0.599335, acc: 71.88%] [G loss: 1.983119]\n",
      "epoch:16 step:15086 [D loss: 0.611319, acc: 68.75%] [G loss: 1.906421]\n",
      "epoch:16 step:15087 [D loss: 0.628066, acc: 67.19%] [G loss: 1.856677]\n",
      "epoch:16 step:15088 [D loss: 0.666413, acc: 60.16%] [G loss: 1.883041]\n",
      "epoch:16 step:15089 [D loss: 0.621281, acc: 65.62%] [G loss: 2.080684]\n",
      "epoch:16 step:15090 [D loss: 0.709749, acc: 53.91%] [G loss: 1.823450]\n",
      "epoch:16 step:15091 [D loss: 0.714595, acc: 54.69%] [G loss: 1.926464]\n",
      "epoch:16 step:15092 [D loss: 0.667017, acc: 66.41%] [G loss: 1.968260]\n",
      "epoch:16 step:15093 [D loss: 0.659257, acc: 57.81%] [G loss: 1.956131]\n",
      "epoch:16 step:15094 [D loss: 0.691217, acc: 54.69%] [G loss: 1.833944]\n",
      "epoch:16 step:15095 [D loss: 0.603508, acc: 66.41%] [G loss: 1.966961]\n",
      "epoch:16 step:15096 [D loss: 0.667670, acc: 56.25%] [G loss: 1.897528]\n",
      "epoch:16 step:15097 [D loss: 0.664762, acc: 57.81%] [G loss: 1.808083]\n",
      "epoch:16 step:15098 [D loss: 0.626196, acc: 66.41%] [G loss: 1.971468]\n",
      "epoch:16 step:15099 [D loss: 0.586123, acc: 67.19%] [G loss: 1.899458]\n",
      "epoch:16 step:15100 [D loss: 0.702817, acc: 55.47%] [G loss: 1.744544]\n",
      "epoch:16 step:15101 [D loss: 0.685430, acc: 59.38%] [G loss: 1.894671]\n",
      "epoch:16 step:15102 [D loss: 0.639014, acc: 60.94%] [G loss: 1.822252]\n",
      "epoch:16 step:15103 [D loss: 0.672901, acc: 61.72%] [G loss: 1.869490]\n",
      "epoch:16 step:15104 [D loss: 0.640467, acc: 66.41%] [G loss: 1.994130]\n",
      "epoch:16 step:15105 [D loss: 0.675886, acc: 60.94%] [G loss: 1.891056]\n",
      "epoch:16 step:15106 [D loss: 0.616803, acc: 68.75%] [G loss: 1.849633]\n",
      "epoch:16 step:15107 [D loss: 0.607450, acc: 70.31%] [G loss: 2.180387]\n",
      "epoch:16 step:15108 [D loss: 0.609331, acc: 66.41%] [G loss: 2.081875]\n",
      "epoch:16 step:15109 [D loss: 0.659894, acc: 63.28%] [G loss: 2.212734]\n",
      "epoch:16 step:15110 [D loss: 0.653620, acc: 67.19%] [G loss: 1.973072]\n",
      "epoch:16 step:15111 [D loss: 0.592510, acc: 66.41%] [G loss: 2.160795]\n",
      "epoch:16 step:15112 [D loss: 0.666603, acc: 57.81%] [G loss: 1.903687]\n",
      "epoch:16 step:15113 [D loss: 0.684162, acc: 57.81%] [G loss: 1.934423]\n",
      "epoch:16 step:15114 [D loss: 0.606390, acc: 64.84%] [G loss: 2.081687]\n",
      "epoch:16 step:15115 [D loss: 0.682414, acc: 58.59%] [G loss: 1.871048]\n",
      "epoch:16 step:15116 [D loss: 0.618418, acc: 63.28%] [G loss: 1.976528]\n",
      "epoch:16 step:15117 [D loss: 0.685609, acc: 54.69%] [G loss: 1.786723]\n",
      "epoch:16 step:15118 [D loss: 0.630968, acc: 64.06%] [G loss: 1.870498]\n",
      "epoch:16 step:15119 [D loss: 0.690052, acc: 56.25%] [G loss: 1.858539]\n",
      "epoch:16 step:15120 [D loss: 0.655672, acc: 60.94%] [G loss: 1.919748]\n",
      "epoch:16 step:15121 [D loss: 0.642897, acc: 64.06%] [G loss: 1.748970]\n",
      "epoch:16 step:15122 [D loss: 0.633975, acc: 64.06%] [G loss: 1.960345]\n",
      "epoch:16 step:15123 [D loss: 0.610078, acc: 64.84%] [G loss: 2.016971]\n",
      "epoch:16 step:15124 [D loss: 0.677653, acc: 56.25%] [G loss: 1.851600]\n",
      "epoch:16 step:15125 [D loss: 0.660918, acc: 58.59%] [G loss: 1.933454]\n",
      "epoch:16 step:15126 [D loss: 0.708553, acc: 50.78%] [G loss: 1.802061]\n",
      "epoch:16 step:15127 [D loss: 0.695186, acc: 57.03%] [G loss: 1.914685]\n",
      "epoch:16 step:15128 [D loss: 0.642276, acc: 60.94%] [G loss: 1.760002]\n",
      "epoch:16 step:15129 [D loss: 0.651988, acc: 64.84%] [G loss: 1.883374]\n",
      "epoch:16 step:15130 [D loss: 0.597120, acc: 68.75%] [G loss: 1.892994]\n",
      "epoch:16 step:15131 [D loss: 0.664261, acc: 60.16%] [G loss: 1.788854]\n",
      "epoch:16 step:15132 [D loss: 0.594100, acc: 76.56%] [G loss: 1.946347]\n",
      "epoch:16 step:15133 [D loss: 0.653392, acc: 60.16%] [G loss: 1.852186]\n",
      "epoch:16 step:15134 [D loss: 0.640672, acc: 59.38%] [G loss: 1.947840]\n",
      "epoch:16 step:15135 [D loss: 0.736726, acc: 54.69%] [G loss: 1.836013]\n",
      "epoch:16 step:15136 [D loss: 0.641618, acc: 58.59%] [G loss: 1.888822]\n",
      "epoch:16 step:15137 [D loss: 0.649463, acc: 63.28%] [G loss: 1.996728]\n",
      "epoch:16 step:15138 [D loss: 0.627146, acc: 64.84%] [G loss: 1.935710]\n",
      "epoch:16 step:15139 [D loss: 0.703086, acc: 56.25%] [G loss: 1.918119]\n",
      "epoch:16 step:15140 [D loss: 0.626536, acc: 62.50%] [G loss: 1.857273]\n",
      "epoch:16 step:15141 [D loss: 0.667172, acc: 59.38%] [G loss: 1.890058]\n",
      "epoch:16 step:15142 [D loss: 0.626344, acc: 67.97%] [G loss: 1.971880]\n",
      "epoch:16 step:15143 [D loss: 0.653487, acc: 58.59%] [G loss: 1.988316]\n",
      "epoch:16 step:15144 [D loss: 0.648570, acc: 63.28%] [G loss: 1.933002]\n",
      "epoch:16 step:15145 [D loss: 0.647631, acc: 65.62%] [G loss: 1.906254]\n",
      "epoch:16 step:15146 [D loss: 0.609215, acc: 70.31%] [G loss: 1.969849]\n",
      "epoch:16 step:15147 [D loss: 0.631253, acc: 66.41%] [G loss: 1.806985]\n",
      "epoch:16 step:15148 [D loss: 0.670506, acc: 55.47%] [G loss: 1.935471]\n",
      "epoch:16 step:15149 [D loss: 0.611010, acc: 68.75%] [G loss: 1.953185]\n",
      "epoch:16 step:15150 [D loss: 0.645129, acc: 61.72%] [G loss: 1.907779]\n",
      "epoch:16 step:15151 [D loss: 0.584867, acc: 71.09%] [G loss: 1.959482]\n",
      "epoch:16 step:15152 [D loss: 0.651606, acc: 60.16%] [G loss: 1.756910]\n",
      "epoch:16 step:15153 [D loss: 0.672025, acc: 60.94%] [G loss: 1.886096]\n",
      "epoch:16 step:15154 [D loss: 0.626733, acc: 64.84%] [G loss: 1.779179]\n",
      "epoch:16 step:15155 [D loss: 0.651205, acc: 58.59%] [G loss: 1.886852]\n",
      "epoch:16 step:15156 [D loss: 0.624785, acc: 68.75%] [G loss: 1.791133]\n",
      "epoch:16 step:15157 [D loss: 0.613635, acc: 71.09%] [G loss: 1.927918]\n",
      "epoch:16 step:15158 [D loss: 0.684834, acc: 59.38%] [G loss: 1.853864]\n",
      "epoch:16 step:15159 [D loss: 0.630094, acc: 62.50%] [G loss: 1.814861]\n",
      "epoch:16 step:15160 [D loss: 0.632840, acc: 62.50%] [G loss: 1.928674]\n",
      "epoch:16 step:15161 [D loss: 0.666950, acc: 58.59%] [G loss: 1.952186]\n",
      "epoch:16 step:15162 [D loss: 0.616750, acc: 65.62%] [G loss: 1.839326]\n",
      "epoch:16 step:15163 [D loss: 0.634205, acc: 65.62%] [G loss: 1.846551]\n",
      "epoch:16 step:15164 [D loss: 0.606255, acc: 67.97%] [G loss: 1.852224]\n",
      "epoch:16 step:15165 [D loss: 0.638129, acc: 58.59%] [G loss: 1.815489]\n",
      "epoch:16 step:15166 [D loss: 0.630931, acc: 70.31%] [G loss: 1.770493]\n",
      "epoch:16 step:15167 [D loss: 0.620666, acc: 64.06%] [G loss: 1.875463]\n",
      "epoch:16 step:15168 [D loss: 0.642739, acc: 59.38%] [G loss: 1.905277]\n",
      "epoch:16 step:15169 [D loss: 0.650851, acc: 60.94%] [G loss: 1.871355]\n",
      "epoch:16 step:15170 [D loss: 0.687227, acc: 53.12%] [G loss: 1.818345]\n",
      "epoch:16 step:15171 [D loss: 0.626603, acc: 66.41%] [G loss: 1.859939]\n",
      "epoch:16 step:15172 [D loss: 0.662978, acc: 55.47%] [G loss: 1.851477]\n",
      "epoch:16 step:15173 [D loss: 0.662671, acc: 60.16%] [G loss: 1.818424]\n",
      "epoch:16 step:15174 [D loss: 0.679128, acc: 59.38%] [G loss: 1.945802]\n",
      "epoch:16 step:15175 [D loss: 0.710068, acc: 52.34%] [G loss: 1.825273]\n",
      "epoch:16 step:15176 [D loss: 0.646940, acc: 61.72%] [G loss: 1.851586]\n",
      "epoch:16 step:15177 [D loss: 0.644585, acc: 60.16%] [G loss: 1.877041]\n",
      "epoch:16 step:15178 [D loss: 0.634748, acc: 64.84%] [G loss: 1.888883]\n",
      "epoch:16 step:15179 [D loss: 0.625426, acc: 64.06%] [G loss: 2.023952]\n",
      "epoch:16 step:15180 [D loss: 0.655444, acc: 59.38%] [G loss: 1.874956]\n",
      "epoch:16 step:15181 [D loss: 0.683975, acc: 58.59%] [G loss: 1.965606]\n",
      "epoch:16 step:15182 [D loss: 0.676841, acc: 54.69%] [G loss: 1.834198]\n",
      "epoch:16 step:15183 [D loss: 0.602783, acc: 68.75%] [G loss: 1.943493]\n",
      "epoch:16 step:15184 [D loss: 0.597571, acc: 67.19%] [G loss: 1.941893]\n",
      "epoch:16 step:15185 [D loss: 0.641506, acc: 59.38%] [G loss: 1.839417]\n",
      "epoch:16 step:15186 [D loss: 0.590935, acc: 70.31%] [G loss: 1.992649]\n",
      "epoch:16 step:15187 [D loss: 0.593996, acc: 69.53%] [G loss: 1.894524]\n",
      "epoch:16 step:15188 [D loss: 0.705757, acc: 60.94%] [G loss: 1.834655]\n",
      "epoch:16 step:15189 [D loss: 0.639932, acc: 68.75%] [G loss: 1.948423]\n",
      "epoch:16 step:15190 [D loss: 0.607951, acc: 69.53%] [G loss: 1.864255]\n",
      "epoch:16 step:15191 [D loss: 0.690735, acc: 52.34%] [G loss: 1.860616]\n",
      "epoch:16 step:15192 [D loss: 0.642782, acc: 63.28%] [G loss: 1.836681]\n",
      "epoch:16 step:15193 [D loss: 0.632131, acc: 63.28%] [G loss: 1.813383]\n",
      "epoch:16 step:15194 [D loss: 0.642011, acc: 59.38%] [G loss: 1.852113]\n",
      "epoch:16 step:15195 [D loss: 0.653807, acc: 60.94%] [G loss: 1.913028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15196 [D loss: 0.696294, acc: 57.03%] [G loss: 1.913225]\n",
      "epoch:16 step:15197 [D loss: 0.645109, acc: 61.72%] [G loss: 1.926084]\n",
      "epoch:16 step:15198 [D loss: 0.598680, acc: 63.28%] [G loss: 2.187602]\n",
      "epoch:16 step:15199 [D loss: 0.588386, acc: 72.66%] [G loss: 1.942632]\n",
      "epoch:16 step:15200 [D loss: 0.561504, acc: 76.56%] [G loss: 2.138618]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.302082\n",
      "FID: 13.600338\n",
      "0 = 12.882000176715872\n",
      "1 = 0.09688230917644286\n",
      "2 = 0.8867999911308289\n",
      "3 = 0.9034000039100647\n",
      "4 = 0.870199978351593\n",
      "5 = 0.8743708729743958\n",
      "6 = 0.9034000039100647\n",
      "7 = 6.475806416726123\n",
      "8 = 0.07145191560477093\n",
      "9 = 0.7257000207901001\n",
      "10 = 0.7376000285148621\n",
      "11 = 0.7138000130653381\n",
      "12 = 0.7204532027244568\n",
      "13 = 0.7376000285148621\n",
      "14 = 7.302117824554443\n",
      "15 = 9.461873054504395\n",
      "16 = 0.11173322796821594\n",
      "17 = 7.302082061767578\n",
      "18 = 13.600337982177734\n",
      "epoch:16 step:15201 [D loss: 0.606296, acc: 65.62%] [G loss: 2.041687]\n",
      "epoch:16 step:15202 [D loss: 0.629064, acc: 69.53%] [G loss: 1.777410]\n",
      "epoch:16 step:15203 [D loss: 0.621230, acc: 61.72%] [G loss: 1.813878]\n",
      "epoch:16 step:15204 [D loss: 0.626078, acc: 66.41%] [G loss: 1.845453]\n",
      "epoch:16 step:15205 [D loss: 0.658890, acc: 69.53%] [G loss: 1.881036]\n",
      "epoch:16 step:15206 [D loss: 0.670123, acc: 60.94%] [G loss: 1.946551]\n",
      "epoch:16 step:15207 [D loss: 0.664895, acc: 57.81%] [G loss: 1.861161]\n",
      "epoch:16 step:15208 [D loss: 0.641384, acc: 61.72%] [G loss: 2.033707]\n",
      "epoch:16 step:15209 [D loss: 0.620403, acc: 71.88%] [G loss: 1.871459]\n",
      "epoch:16 step:15210 [D loss: 0.570369, acc: 75.78%] [G loss: 2.058424]\n",
      "epoch:16 step:15211 [D loss: 0.593327, acc: 67.97%] [G loss: 2.181397]\n",
      "epoch:16 step:15212 [D loss: 0.679001, acc: 56.25%] [G loss: 1.814321]\n",
      "epoch:16 step:15213 [D loss: 0.626089, acc: 67.19%] [G loss: 1.961872]\n",
      "epoch:16 step:15214 [D loss: 0.614445, acc: 67.19%] [G loss: 1.972638]\n",
      "epoch:16 step:15215 [D loss: 0.602060, acc: 68.75%] [G loss: 1.974263]\n",
      "epoch:16 step:15216 [D loss: 0.662784, acc: 60.94%] [G loss: 1.827434]\n",
      "epoch:16 step:15217 [D loss: 0.678098, acc: 61.72%] [G loss: 2.001074]\n",
      "epoch:16 step:15218 [D loss: 0.675699, acc: 64.06%] [G loss: 1.869197]\n",
      "epoch:16 step:15219 [D loss: 0.614551, acc: 64.06%] [G loss: 1.805216]\n",
      "epoch:16 step:15220 [D loss: 0.686733, acc: 61.72%] [G loss: 1.726533]\n",
      "epoch:16 step:15221 [D loss: 0.609298, acc: 67.19%] [G loss: 2.105557]\n",
      "epoch:16 step:15222 [D loss: 0.611126, acc: 67.97%] [G loss: 2.125668]\n",
      "epoch:16 step:15223 [D loss: 0.567443, acc: 67.97%] [G loss: 2.357341]\n",
      "epoch:16 step:15224 [D loss: 0.529681, acc: 74.22%] [G loss: 2.510418]\n",
      "epoch:16 step:15225 [D loss: 0.663105, acc: 65.62%] [G loss: 1.964964]\n",
      "epoch:16 step:15226 [D loss: 0.667343, acc: 63.28%] [G loss: 1.940926]\n",
      "epoch:16 step:15227 [D loss: 0.724957, acc: 53.91%] [G loss: 1.828267]\n",
      "epoch:16 step:15228 [D loss: 0.672674, acc: 58.59%] [G loss: 1.839741]\n",
      "epoch:16 step:15229 [D loss: 0.673921, acc: 60.16%] [G loss: 1.867893]\n",
      "epoch:16 step:15230 [D loss: 0.665492, acc: 61.72%] [G loss: 1.932827]\n",
      "epoch:16 step:15231 [D loss: 0.681383, acc: 57.81%] [G loss: 1.905023]\n",
      "epoch:16 step:15232 [D loss: 0.618702, acc: 64.06%] [G loss: 1.896448]\n",
      "epoch:16 step:15233 [D loss: 0.608441, acc: 63.28%] [G loss: 2.055984]\n",
      "epoch:16 step:15234 [D loss: 0.619339, acc: 67.19%] [G loss: 2.059086]\n",
      "epoch:16 step:15235 [D loss: 0.666047, acc: 59.38%] [G loss: 1.986586]\n",
      "epoch:16 step:15236 [D loss: 0.699136, acc: 57.03%] [G loss: 1.854346]\n",
      "epoch:16 step:15237 [D loss: 0.628142, acc: 66.41%] [G loss: 2.017363]\n",
      "epoch:16 step:15238 [D loss: 0.626034, acc: 65.62%] [G loss: 1.968222]\n",
      "epoch:16 step:15239 [D loss: 0.652168, acc: 63.28%] [G loss: 2.040385]\n",
      "epoch:16 step:15240 [D loss: 0.652749, acc: 60.16%] [G loss: 2.127887]\n",
      "epoch:16 step:15241 [D loss: 0.667219, acc: 57.03%] [G loss: 1.711903]\n",
      "epoch:16 step:15242 [D loss: 0.681567, acc: 52.34%] [G loss: 1.674583]\n",
      "epoch:16 step:15243 [D loss: 0.680978, acc: 56.25%] [G loss: 1.806247]\n",
      "epoch:16 step:15244 [D loss: 0.683858, acc: 57.03%] [G loss: 1.771078]\n",
      "epoch:16 step:15245 [D loss: 0.689678, acc: 59.38%] [G loss: 1.808852]\n",
      "epoch:16 step:15246 [D loss: 0.680695, acc: 59.38%] [G loss: 1.792307]\n",
      "epoch:16 step:15247 [D loss: 0.652382, acc: 60.94%] [G loss: 1.810762]\n",
      "epoch:16 step:15248 [D loss: 0.675640, acc: 57.81%] [G loss: 1.810151]\n",
      "epoch:16 step:15249 [D loss: 0.671582, acc: 62.50%] [G loss: 1.815639]\n",
      "epoch:16 step:15250 [D loss: 0.655669, acc: 62.50%] [G loss: 1.874598]\n",
      "epoch:16 step:15251 [D loss: 0.680790, acc: 55.47%] [G loss: 1.802061]\n",
      "epoch:16 step:15252 [D loss: 0.652048, acc: 59.38%] [G loss: 1.697097]\n",
      "epoch:16 step:15253 [D loss: 0.691279, acc: 59.38%] [G loss: 1.926080]\n",
      "epoch:16 step:15254 [D loss: 0.579455, acc: 71.09%] [G loss: 2.029499]\n",
      "epoch:16 step:15255 [D loss: 0.638033, acc: 64.84%] [G loss: 1.904768]\n",
      "epoch:16 step:15256 [D loss: 0.590994, acc: 70.31%] [G loss: 2.070160]\n",
      "epoch:16 step:15257 [D loss: 0.649648, acc: 62.50%] [G loss: 1.953580]\n",
      "epoch:16 step:15258 [D loss: 0.636604, acc: 61.72%] [G loss: 1.872493]\n",
      "epoch:16 step:15259 [D loss: 0.659634, acc: 57.03%] [G loss: 1.888898]\n",
      "epoch:16 step:15260 [D loss: 0.672292, acc: 59.38%] [G loss: 1.834289]\n",
      "epoch:16 step:15261 [D loss: 0.667512, acc: 60.16%] [G loss: 1.870403]\n",
      "epoch:16 step:15262 [D loss: 0.643271, acc: 64.06%] [G loss: 2.027071]\n",
      "epoch:16 step:15263 [D loss: 0.622958, acc: 60.94%] [G loss: 1.892028]\n",
      "epoch:16 step:15264 [D loss: 0.589486, acc: 68.75%] [G loss: 1.910295]\n",
      "epoch:16 step:15265 [D loss: 0.619821, acc: 64.84%] [G loss: 1.882337]\n",
      "epoch:16 step:15266 [D loss: 0.620431, acc: 68.75%] [G loss: 2.161608]\n",
      "epoch:16 step:15267 [D loss: 0.622832, acc: 65.62%] [G loss: 2.064305]\n",
      "epoch:16 step:15268 [D loss: 0.651276, acc: 57.81%] [G loss: 2.115804]\n",
      "epoch:16 step:15269 [D loss: 0.606459, acc: 71.88%] [G loss: 1.901615]\n",
      "epoch:16 step:15270 [D loss: 0.586225, acc: 65.62%] [G loss: 1.932845]\n",
      "epoch:16 step:15271 [D loss: 0.668611, acc: 58.59%] [G loss: 1.948797]\n",
      "epoch:16 step:15272 [D loss: 0.612545, acc: 71.09%] [G loss: 1.971044]\n",
      "epoch:16 step:15273 [D loss: 0.726176, acc: 50.00%] [G loss: 1.836188]\n",
      "epoch:16 step:15274 [D loss: 0.630649, acc: 66.41%] [G loss: 1.819423]\n",
      "epoch:16 step:15275 [D loss: 0.628151, acc: 68.75%] [G loss: 1.950021]\n",
      "epoch:16 step:15276 [D loss: 0.688677, acc: 65.62%] [G loss: 1.932917]\n",
      "epoch:16 step:15277 [D loss: 0.643690, acc: 64.84%] [G loss: 1.903920]\n",
      "epoch:16 step:15278 [D loss: 0.646256, acc: 58.59%] [G loss: 2.045897]\n",
      "epoch:16 step:15279 [D loss: 0.657317, acc: 65.62%] [G loss: 1.957674]\n",
      "epoch:16 step:15280 [D loss: 0.672074, acc: 57.81%] [G loss: 1.925906]\n",
      "epoch:16 step:15281 [D loss: 0.646605, acc: 63.28%] [G loss: 1.891657]\n",
      "epoch:16 step:15282 [D loss: 0.672553, acc: 58.59%] [G loss: 1.952702]\n",
      "epoch:16 step:15283 [D loss: 0.642886, acc: 63.28%] [G loss: 1.866901]\n",
      "epoch:16 step:15284 [D loss: 0.647267, acc: 59.38%] [G loss: 1.814844]\n",
      "epoch:16 step:15285 [D loss: 0.618476, acc: 67.19%] [G loss: 2.015155]\n",
      "epoch:16 step:15286 [D loss: 0.676135, acc: 61.72%] [G loss: 1.880400]\n",
      "epoch:16 step:15287 [D loss: 0.664580, acc: 59.38%] [G loss: 1.791709]\n",
      "epoch:16 step:15288 [D loss: 0.609837, acc: 65.62%] [G loss: 2.003708]\n",
      "epoch:16 step:15289 [D loss: 0.671854, acc: 57.03%] [G loss: 1.819464]\n",
      "epoch:16 step:15290 [D loss: 0.617806, acc: 67.19%] [G loss: 1.797837]\n",
      "epoch:16 step:15291 [D loss: 0.613097, acc: 67.97%] [G loss: 1.938474]\n",
      "epoch:16 step:15292 [D loss: 0.656746, acc: 64.84%] [G loss: 2.047600]\n",
      "epoch:16 step:15293 [D loss: 0.647587, acc: 64.84%] [G loss: 1.871965]\n",
      "epoch:16 step:15294 [D loss: 0.619077, acc: 65.62%] [G loss: 2.054098]\n",
      "epoch:16 step:15295 [D loss: 0.633360, acc: 64.84%] [G loss: 1.931479]\n",
      "epoch:16 step:15296 [D loss: 0.611265, acc: 67.97%] [G loss: 1.948835]\n",
      "epoch:16 step:15297 [D loss: 0.629324, acc: 65.62%] [G loss: 1.882693]\n",
      "epoch:16 step:15298 [D loss: 0.639240, acc: 60.16%] [G loss: 1.950956]\n",
      "epoch:16 step:15299 [D loss: 0.624753, acc: 63.28%] [G loss: 1.773140]\n",
      "epoch:16 step:15300 [D loss: 0.644120, acc: 60.94%] [G loss: 1.906144]\n",
      "epoch:16 step:15301 [D loss: 0.590113, acc: 71.88%] [G loss: 1.952811]\n",
      "epoch:16 step:15302 [D loss: 0.645428, acc: 61.72%] [G loss: 1.877877]\n",
      "epoch:16 step:15303 [D loss: 0.617640, acc: 66.41%] [G loss: 1.920241]\n",
      "epoch:16 step:15304 [D loss: 0.629533, acc: 64.06%] [G loss: 2.197245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15305 [D loss: 0.650672, acc: 56.25%] [G loss: 2.157036]\n",
      "epoch:16 step:15306 [D loss: 0.613091, acc: 64.84%] [G loss: 2.263018]\n",
      "epoch:16 step:15307 [D loss: 0.579282, acc: 63.28%] [G loss: 2.178470]\n",
      "epoch:16 step:15308 [D loss: 0.703969, acc: 55.47%] [G loss: 1.799801]\n",
      "epoch:16 step:15309 [D loss: 0.673222, acc: 59.38%] [G loss: 1.778691]\n",
      "epoch:16 step:15310 [D loss: 0.612040, acc: 64.84%] [G loss: 1.985211]\n",
      "epoch:16 step:15311 [D loss: 0.594947, acc: 65.62%] [G loss: 1.885933]\n",
      "epoch:16 step:15312 [D loss: 0.628109, acc: 67.19%] [G loss: 1.911210]\n",
      "epoch:16 step:15313 [D loss: 0.602461, acc: 65.62%] [G loss: 2.059985]\n",
      "epoch:16 step:15314 [D loss: 0.680798, acc: 54.69%] [G loss: 2.034160]\n",
      "epoch:16 step:15315 [D loss: 0.674191, acc: 59.38%] [G loss: 2.053838]\n",
      "epoch:16 step:15316 [D loss: 0.662358, acc: 60.94%] [G loss: 1.919124]\n",
      "epoch:16 step:15317 [D loss: 0.634063, acc: 57.81%] [G loss: 1.839820]\n",
      "epoch:16 step:15318 [D loss: 0.624380, acc: 67.19%] [G loss: 1.868272]\n",
      "epoch:16 step:15319 [D loss: 0.637415, acc: 68.75%] [G loss: 1.853985]\n",
      "epoch:16 step:15320 [D loss: 0.692290, acc: 58.59%] [G loss: 1.889548]\n",
      "epoch:16 step:15321 [D loss: 0.643109, acc: 65.62%] [G loss: 1.880154]\n",
      "epoch:16 step:15322 [D loss: 0.603738, acc: 65.62%] [G loss: 1.857759]\n",
      "epoch:16 step:15323 [D loss: 0.643954, acc: 62.50%] [G loss: 1.946459]\n",
      "epoch:16 step:15324 [D loss: 0.632484, acc: 67.97%] [G loss: 2.226329]\n",
      "epoch:16 step:15325 [D loss: 0.639136, acc: 64.84%] [G loss: 1.930890]\n",
      "epoch:16 step:15326 [D loss: 0.687190, acc: 61.72%] [G loss: 1.850575]\n",
      "epoch:16 step:15327 [D loss: 0.650460, acc: 64.84%] [G loss: 1.859277]\n",
      "epoch:16 step:15328 [D loss: 0.619905, acc: 66.41%] [G loss: 1.927874]\n",
      "epoch:16 step:15329 [D loss: 0.616553, acc: 64.84%] [G loss: 1.964097]\n",
      "epoch:16 step:15330 [D loss: 0.585874, acc: 69.53%] [G loss: 1.947112]\n",
      "epoch:16 step:15331 [D loss: 0.581860, acc: 68.75%] [G loss: 1.947963]\n",
      "epoch:16 step:15332 [D loss: 0.660081, acc: 57.81%] [G loss: 1.976794]\n",
      "epoch:16 step:15333 [D loss: 0.693277, acc: 59.38%] [G loss: 1.779830]\n",
      "epoch:16 step:15334 [D loss: 0.705221, acc: 50.78%] [G loss: 1.932293]\n",
      "epoch:16 step:15335 [D loss: 0.727471, acc: 52.34%] [G loss: 1.846984]\n",
      "epoch:16 step:15336 [D loss: 0.628188, acc: 64.06%] [G loss: 1.987418]\n",
      "epoch:16 step:15337 [D loss: 0.610698, acc: 67.97%] [G loss: 2.101755]\n",
      "epoch:16 step:15338 [D loss: 0.591717, acc: 69.53%] [G loss: 2.168789]\n",
      "epoch:16 step:15339 [D loss: 0.557263, acc: 73.44%] [G loss: 2.186843]\n",
      "epoch:16 step:15340 [D loss: 0.634742, acc: 64.06%] [G loss: 1.935811]\n",
      "epoch:16 step:15341 [D loss: 0.715841, acc: 53.91%] [G loss: 1.716218]\n",
      "epoch:16 step:15342 [D loss: 0.607538, acc: 68.75%] [G loss: 1.877128]\n",
      "epoch:16 step:15343 [D loss: 0.661285, acc: 60.16%] [G loss: 1.953856]\n",
      "epoch:16 step:15344 [D loss: 0.665252, acc: 60.94%] [G loss: 1.928926]\n",
      "epoch:16 step:15345 [D loss: 0.606588, acc: 69.53%] [G loss: 2.063187]\n",
      "epoch:16 step:15346 [D loss: 0.636116, acc: 64.06%] [G loss: 2.143571]\n",
      "epoch:16 step:15347 [D loss: 0.704357, acc: 58.59%] [G loss: 2.018072]\n",
      "epoch:16 step:15348 [D loss: 0.643847, acc: 58.59%] [G loss: 1.896785]\n",
      "epoch:16 step:15349 [D loss: 0.624978, acc: 66.41%] [G loss: 1.888398]\n",
      "epoch:16 step:15350 [D loss: 0.609899, acc: 67.19%] [G loss: 2.025811]\n",
      "epoch:16 step:15351 [D loss: 0.644436, acc: 63.28%] [G loss: 2.094153]\n",
      "epoch:16 step:15352 [D loss: 0.609636, acc: 68.75%] [G loss: 2.036522]\n",
      "epoch:16 step:15353 [D loss: 0.665195, acc: 65.62%] [G loss: 1.940737]\n",
      "epoch:16 step:15354 [D loss: 0.664827, acc: 58.59%] [G loss: 1.870855]\n",
      "epoch:16 step:15355 [D loss: 0.681669, acc: 61.72%] [G loss: 1.860552]\n",
      "epoch:16 step:15356 [D loss: 0.661386, acc: 62.50%] [G loss: 1.929642]\n",
      "epoch:16 step:15357 [D loss: 0.634212, acc: 64.84%] [G loss: 1.899075]\n",
      "epoch:16 step:15358 [D loss: 0.571275, acc: 69.53%] [G loss: 2.070772]\n",
      "epoch:16 step:15359 [D loss: 0.676248, acc: 57.03%] [G loss: 2.032437]\n",
      "epoch:16 step:15360 [D loss: 0.660333, acc: 58.59%] [G loss: 1.969464]\n",
      "epoch:16 step:15361 [D loss: 0.663590, acc: 59.38%] [G loss: 1.840245]\n",
      "epoch:16 step:15362 [D loss: 0.592203, acc: 67.19%] [G loss: 1.969817]\n",
      "epoch:16 step:15363 [D loss: 0.596236, acc: 65.62%] [G loss: 1.968006]\n",
      "epoch:16 step:15364 [D loss: 0.668320, acc: 63.28%] [G loss: 1.949416]\n",
      "epoch:16 step:15365 [D loss: 0.655175, acc: 58.59%] [G loss: 1.788522]\n",
      "epoch:16 step:15366 [D loss: 0.633352, acc: 61.72%] [G loss: 1.971487]\n",
      "epoch:16 step:15367 [D loss: 0.642154, acc: 64.06%] [G loss: 1.882599]\n",
      "epoch:16 step:15368 [D loss: 0.736898, acc: 52.34%] [G loss: 1.884117]\n",
      "epoch:16 step:15369 [D loss: 0.676575, acc: 51.56%] [G loss: 1.870521]\n",
      "epoch:16 step:15370 [D loss: 0.602891, acc: 71.88%] [G loss: 1.931155]\n",
      "epoch:16 step:15371 [D loss: 0.623416, acc: 68.75%] [G loss: 1.944310]\n",
      "epoch:16 step:15372 [D loss: 0.645481, acc: 62.50%] [G loss: 1.945500]\n",
      "epoch:16 step:15373 [D loss: 0.605078, acc: 67.19%] [G loss: 1.939780]\n",
      "epoch:16 step:15374 [D loss: 0.660002, acc: 65.62%] [G loss: 1.935751]\n",
      "epoch:16 step:15375 [D loss: 0.603056, acc: 69.53%] [G loss: 1.926951]\n",
      "epoch:16 step:15376 [D loss: 0.683590, acc: 53.12%] [G loss: 1.854224]\n",
      "epoch:16 step:15377 [D loss: 0.625010, acc: 67.19%] [G loss: 2.099417]\n",
      "epoch:16 step:15378 [D loss: 0.680051, acc: 60.16%] [G loss: 1.729723]\n",
      "epoch:16 step:15379 [D loss: 0.650102, acc: 65.62%] [G loss: 1.855426]\n",
      "epoch:16 step:15380 [D loss: 0.700422, acc: 60.16%] [G loss: 1.794500]\n",
      "epoch:16 step:15381 [D loss: 0.666100, acc: 64.84%] [G loss: 1.872094]\n",
      "epoch:16 step:15382 [D loss: 0.617503, acc: 64.84%] [G loss: 1.938607]\n",
      "epoch:16 step:15383 [D loss: 0.634152, acc: 60.94%] [G loss: 1.880311]\n",
      "epoch:16 step:15384 [D loss: 0.672785, acc: 63.28%] [G loss: 1.909085]\n",
      "epoch:16 step:15385 [D loss: 0.657069, acc: 67.97%] [G loss: 1.880082]\n",
      "epoch:16 step:15386 [D loss: 0.619654, acc: 65.62%] [G loss: 1.929294]\n",
      "epoch:16 step:15387 [D loss: 0.645505, acc: 57.03%] [G loss: 1.917923]\n",
      "epoch:16 step:15388 [D loss: 0.657592, acc: 60.16%] [G loss: 1.783326]\n",
      "epoch:16 step:15389 [D loss: 0.646423, acc: 61.72%] [G loss: 1.842216]\n",
      "epoch:16 step:15390 [D loss: 0.645812, acc: 65.62%] [G loss: 1.870786]\n",
      "epoch:16 step:15391 [D loss: 0.611162, acc: 65.62%] [G loss: 1.911345]\n",
      "epoch:16 step:15392 [D loss: 0.669455, acc: 59.38%] [G loss: 2.008504]\n",
      "epoch:16 step:15393 [D loss: 0.625267, acc: 64.06%] [G loss: 1.907181]\n",
      "epoch:16 step:15394 [D loss: 0.601160, acc: 67.97%] [G loss: 2.006221]\n",
      "epoch:16 step:15395 [D loss: 0.666574, acc: 59.38%] [G loss: 1.913955]\n",
      "epoch:16 step:15396 [D loss: 0.626675, acc: 67.19%] [G loss: 2.100325]\n",
      "epoch:16 step:15397 [D loss: 0.564603, acc: 71.88%] [G loss: 2.077875]\n",
      "epoch:16 step:15398 [D loss: 0.602137, acc: 64.84%] [G loss: 2.150017]\n",
      "epoch:16 step:15399 [D loss: 0.631940, acc: 67.19%] [G loss: 1.982239]\n",
      "epoch:16 step:15400 [D loss: 0.682289, acc: 55.47%] [G loss: 1.889806]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.162948\n",
      "FID: 15.194211\n",
      "0 = 12.779000957775132\n",
      "1 = 0.08804646557067072\n",
      "2 = 0.8934999704360962\n",
      "3 = 0.920199990272522\n",
      "4 = 0.8668000102043152\n",
      "5 = 0.8735523223876953\n",
      "6 = 0.920199990272522\n",
      "7 = 6.590548330211621\n",
      "8 = 0.07255865946750052\n",
      "9 = 0.7340999841690063\n",
      "10 = 0.7423999905586243\n",
      "11 = 0.7257999777793884\n",
      "12 = 0.730277419090271\n",
      "13 = 0.7423999905586243\n",
      "14 = 7.162974834442139\n",
      "15 = 9.401248931884766\n",
      "16 = 0.13262632489204407\n",
      "17 = 7.162948131561279\n",
      "18 = 15.19421100616455\n",
      "epoch:16 step:15401 [D loss: 0.617473, acc: 67.97%] [G loss: 2.061583]\n",
      "epoch:16 step:15402 [D loss: 0.630528, acc: 64.06%] [G loss: 2.018731]\n",
      "epoch:16 step:15403 [D loss: 0.759252, acc: 50.78%] [G loss: 1.940642]\n",
      "epoch:16 step:15404 [D loss: 0.648681, acc: 64.84%] [G loss: 1.914844]\n",
      "epoch:16 step:15405 [D loss: 0.600024, acc: 69.53%] [G loss: 1.971137]\n",
      "epoch:16 step:15406 [D loss: 0.638206, acc: 60.94%] [G loss: 2.081506]\n",
      "epoch:16 step:15407 [D loss: 0.598852, acc: 69.53%] [G loss: 2.002495]\n",
      "epoch:16 step:15408 [D loss: 0.698006, acc: 55.47%] [G loss: 1.983956]\n",
      "epoch:16 step:15409 [D loss: 0.619826, acc: 64.06%] [G loss: 1.947758]\n",
      "epoch:16 step:15410 [D loss: 0.610858, acc: 65.62%] [G loss: 1.834856]\n",
      "epoch:16 step:15411 [D loss: 0.653017, acc: 58.59%] [G loss: 1.984924]\n",
      "epoch:16 step:15412 [D loss: 0.615418, acc: 65.62%] [G loss: 2.011620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15413 [D loss: 0.726809, acc: 57.81%] [G loss: 1.971587]\n",
      "epoch:16 step:15414 [D loss: 0.700596, acc: 60.94%] [G loss: 1.785541]\n",
      "epoch:16 step:15415 [D loss: 0.705623, acc: 53.12%] [G loss: 1.793852]\n",
      "epoch:16 step:15416 [D loss: 0.684418, acc: 59.38%] [G loss: 1.868172]\n",
      "epoch:16 step:15417 [D loss: 0.652654, acc: 62.50%] [G loss: 1.847321]\n",
      "epoch:16 step:15418 [D loss: 0.655116, acc: 60.16%] [G loss: 1.968961]\n",
      "epoch:16 step:15419 [D loss: 0.649165, acc: 63.28%] [G loss: 1.967423]\n",
      "epoch:16 step:15420 [D loss: 0.626399, acc: 60.94%] [G loss: 2.099345]\n",
      "epoch:16 step:15421 [D loss: 0.622193, acc: 61.72%] [G loss: 2.142018]\n",
      "epoch:16 step:15422 [D loss: 0.615009, acc: 70.31%] [G loss: 2.177049]\n",
      "epoch:16 step:15423 [D loss: 0.651425, acc: 60.16%] [G loss: 1.860840]\n",
      "epoch:16 step:15424 [D loss: 0.661316, acc: 60.16%] [G loss: 1.742218]\n",
      "epoch:16 step:15425 [D loss: 0.660403, acc: 60.94%] [G loss: 2.025635]\n",
      "epoch:16 step:15426 [D loss: 0.608845, acc: 64.84%] [G loss: 2.069485]\n",
      "epoch:16 step:15427 [D loss: 0.600466, acc: 67.97%] [G loss: 1.929448]\n",
      "epoch:16 step:15428 [D loss: 0.587083, acc: 69.53%] [G loss: 1.931296]\n",
      "epoch:16 step:15429 [D loss: 0.716377, acc: 53.91%] [G loss: 1.899585]\n",
      "epoch:16 step:15430 [D loss: 0.720295, acc: 49.22%] [G loss: 1.783780]\n",
      "epoch:16 step:15431 [D loss: 0.660119, acc: 59.38%] [G loss: 1.804282]\n",
      "epoch:16 step:15432 [D loss: 0.671749, acc: 60.16%] [G loss: 1.821503]\n",
      "epoch:16 step:15433 [D loss: 0.668497, acc: 60.16%] [G loss: 1.838082]\n",
      "epoch:16 step:15434 [D loss: 0.677863, acc: 59.38%] [G loss: 1.806605]\n",
      "epoch:16 step:15435 [D loss: 0.656893, acc: 58.59%] [G loss: 1.827595]\n",
      "epoch:16 step:15436 [D loss: 0.693691, acc: 54.69%] [G loss: 1.709935]\n",
      "epoch:16 step:15437 [D loss: 0.637269, acc: 64.84%] [G loss: 1.895652]\n",
      "epoch:16 step:15438 [D loss: 0.640127, acc: 59.38%] [G loss: 1.816537]\n",
      "epoch:16 step:15439 [D loss: 0.670922, acc: 65.62%] [G loss: 1.887635]\n",
      "epoch:16 step:15440 [D loss: 0.708154, acc: 54.69%] [G loss: 1.791820]\n",
      "epoch:16 step:15441 [D loss: 0.597519, acc: 71.09%] [G loss: 1.992887]\n",
      "epoch:16 step:15442 [D loss: 0.696756, acc: 66.41%] [G loss: 1.789324]\n",
      "epoch:16 step:15443 [D loss: 0.632204, acc: 63.28%] [G loss: 1.869219]\n",
      "epoch:16 step:15444 [D loss: 0.627356, acc: 70.31%] [G loss: 1.932122]\n",
      "epoch:16 step:15445 [D loss: 0.631316, acc: 62.50%] [G loss: 1.908605]\n",
      "epoch:16 step:15446 [D loss: 0.611662, acc: 66.41%] [G loss: 1.970519]\n",
      "epoch:16 step:15447 [D loss: 0.645719, acc: 67.97%] [G loss: 1.829424]\n",
      "epoch:16 step:15448 [D loss: 0.620561, acc: 64.84%] [G loss: 1.937727]\n",
      "epoch:16 step:15449 [D loss: 0.604765, acc: 66.41%] [G loss: 1.912109]\n",
      "epoch:16 step:15450 [D loss: 0.651074, acc: 64.06%] [G loss: 1.904550]\n",
      "epoch:16 step:15451 [D loss: 0.617533, acc: 63.28%] [G loss: 1.919628]\n",
      "epoch:16 step:15452 [D loss: 0.668378, acc: 65.62%] [G loss: 1.832687]\n",
      "epoch:16 step:15453 [D loss: 0.638144, acc: 64.06%] [G loss: 1.885831]\n",
      "epoch:16 step:15454 [D loss: 0.665457, acc: 62.50%] [G loss: 1.907155]\n",
      "epoch:16 step:15455 [D loss: 0.622282, acc: 60.94%] [G loss: 1.834053]\n",
      "epoch:16 step:15456 [D loss: 0.662229, acc: 59.38%] [G loss: 1.874832]\n",
      "epoch:16 step:15457 [D loss: 0.678766, acc: 53.91%] [G loss: 1.774111]\n",
      "epoch:16 step:15458 [D loss: 0.657203, acc: 64.06%] [G loss: 1.922080]\n",
      "epoch:16 step:15459 [D loss: 0.657524, acc: 57.03%] [G loss: 1.946436]\n",
      "epoch:16 step:15460 [D loss: 0.603578, acc: 66.41%] [G loss: 2.097713]\n",
      "epoch:16 step:15461 [D loss: 0.604616, acc: 69.53%] [G loss: 2.040984]\n",
      "epoch:16 step:15462 [D loss: 0.644148, acc: 57.81%] [G loss: 2.039106]\n",
      "epoch:16 step:15463 [D loss: 0.607221, acc: 69.53%] [G loss: 2.175637]\n",
      "epoch:16 step:15464 [D loss: 0.599614, acc: 64.06%] [G loss: 2.047513]\n",
      "epoch:16 step:15465 [D loss: 0.673172, acc: 60.16%] [G loss: 1.817624]\n",
      "epoch:16 step:15466 [D loss: 0.645395, acc: 65.62%] [G loss: 1.905329]\n",
      "epoch:16 step:15467 [D loss: 0.697317, acc: 54.69%] [G loss: 2.150519]\n",
      "epoch:16 step:15468 [D loss: 0.706475, acc: 57.03%] [G loss: 2.108274]\n",
      "epoch:16 step:15469 [D loss: 0.730144, acc: 56.25%] [G loss: 1.773417]\n",
      "epoch:16 step:15470 [D loss: 0.695686, acc: 55.47%] [G loss: 1.822771]\n",
      "epoch:16 step:15471 [D loss: 0.643607, acc: 63.28%] [G loss: 1.986881]\n",
      "epoch:16 step:15472 [D loss: 0.639660, acc: 67.19%] [G loss: 1.905312]\n",
      "epoch:16 step:15473 [D loss: 0.616980, acc: 70.31%] [G loss: 2.058680]\n",
      "epoch:16 step:15474 [D loss: 0.715233, acc: 56.25%] [G loss: 1.693040]\n",
      "epoch:16 step:15475 [D loss: 0.679070, acc: 59.38%] [G loss: 1.697926]\n",
      "epoch:16 step:15476 [D loss: 0.629093, acc: 64.06%] [G loss: 1.926228]\n",
      "epoch:16 step:15477 [D loss: 0.672174, acc: 58.59%] [G loss: 1.738204]\n",
      "epoch:16 step:15478 [D loss: 0.696750, acc: 56.25%] [G loss: 1.833348]\n",
      "epoch:16 step:15479 [D loss: 0.664079, acc: 62.50%] [G loss: 1.953012]\n",
      "epoch:16 step:15480 [D loss: 0.628145, acc: 67.19%] [G loss: 2.008668]\n",
      "epoch:16 step:15481 [D loss: 0.633033, acc: 64.06%] [G loss: 1.828769]\n",
      "epoch:16 step:15482 [D loss: 0.604627, acc: 69.53%] [G loss: 1.907766]\n",
      "epoch:16 step:15483 [D loss: 0.587872, acc: 67.19%] [G loss: 1.968128]\n",
      "epoch:16 step:15484 [D loss: 0.634000, acc: 64.06%] [G loss: 1.886221]\n",
      "epoch:16 step:15485 [D loss: 0.644463, acc: 66.41%] [G loss: 1.878210]\n",
      "epoch:16 step:15486 [D loss: 0.619988, acc: 66.41%] [G loss: 2.003010]\n",
      "epoch:16 step:15487 [D loss: 0.567223, acc: 74.22%] [G loss: 2.239535]\n",
      "epoch:16 step:15488 [D loss: 0.714330, acc: 53.91%] [G loss: 1.895136]\n",
      "epoch:16 step:15489 [D loss: 0.590622, acc: 67.97%] [G loss: 1.980296]\n",
      "epoch:16 step:15490 [D loss: 0.630658, acc: 63.28%] [G loss: 2.027561]\n",
      "epoch:16 step:15491 [D loss: 0.544178, acc: 76.56%] [G loss: 2.248513]\n",
      "epoch:16 step:15492 [D loss: 0.774123, acc: 49.22%] [G loss: 1.761671]\n",
      "epoch:16 step:15493 [D loss: 0.690431, acc: 60.16%] [G loss: 1.694695]\n",
      "epoch:16 step:15494 [D loss: 0.732932, acc: 53.12%] [G loss: 1.852805]\n",
      "epoch:16 step:15495 [D loss: 0.665787, acc: 61.72%] [G loss: 1.833386]\n",
      "epoch:16 step:15496 [D loss: 0.625057, acc: 63.28%] [G loss: 2.029859]\n",
      "epoch:16 step:15497 [D loss: 0.627944, acc: 66.41%] [G loss: 1.917148]\n",
      "epoch:16 step:15498 [D loss: 0.652268, acc: 60.16%] [G loss: 1.754464]\n",
      "epoch:16 step:15499 [D loss: 0.667714, acc: 57.81%] [G loss: 1.859824]\n",
      "epoch:16 step:15500 [D loss: 0.629840, acc: 60.16%] [G loss: 2.065198]\n",
      "epoch:16 step:15501 [D loss: 0.634081, acc: 64.84%] [G loss: 1.870753]\n",
      "epoch:16 step:15502 [D loss: 0.619351, acc: 67.19%] [G loss: 1.768224]\n",
      "epoch:16 step:15503 [D loss: 0.680099, acc: 53.12%] [G loss: 1.780425]\n",
      "epoch:16 step:15504 [D loss: 0.675900, acc: 60.94%] [G loss: 1.797321]\n",
      "epoch:16 step:15505 [D loss: 0.628461, acc: 59.38%] [G loss: 2.002049]\n",
      "epoch:16 step:15506 [D loss: 0.642392, acc: 61.72%] [G loss: 1.803738]\n",
      "epoch:16 step:15507 [D loss: 0.670105, acc: 54.69%] [G loss: 1.849980]\n",
      "epoch:16 step:15508 [D loss: 0.621407, acc: 64.06%] [G loss: 1.850607]\n",
      "epoch:16 step:15509 [D loss: 0.675693, acc: 58.59%] [G loss: 1.812762]\n",
      "epoch:16 step:15510 [D loss: 0.651982, acc: 59.38%] [G loss: 1.735176]\n",
      "epoch:16 step:15511 [D loss: 0.625778, acc: 66.41%] [G loss: 1.892293]\n",
      "epoch:16 step:15512 [D loss: 0.612554, acc: 66.41%] [G loss: 2.057711]\n",
      "epoch:16 step:15513 [D loss: 0.584504, acc: 71.09%] [G loss: 2.030379]\n",
      "epoch:16 step:15514 [D loss: 0.564738, acc: 71.88%] [G loss: 2.091695]\n",
      "epoch:16 step:15515 [D loss: 0.633055, acc: 64.06%] [G loss: 2.092060]\n",
      "epoch:16 step:15516 [D loss: 0.658987, acc: 62.50%] [G loss: 1.890964]\n",
      "epoch:16 step:15517 [D loss: 0.642642, acc: 63.28%] [G loss: 1.952590]\n",
      "epoch:16 step:15518 [D loss: 0.690394, acc: 61.72%] [G loss: 1.939813]\n",
      "epoch:16 step:15519 [D loss: 0.617900, acc: 66.41%] [G loss: 1.793754]\n",
      "epoch:16 step:15520 [D loss: 0.676997, acc: 60.94%] [G loss: 1.781793]\n",
      "epoch:16 step:15521 [D loss: 0.685678, acc: 58.59%] [G loss: 1.835227]\n",
      "epoch:16 step:15522 [D loss: 0.669315, acc: 61.72%] [G loss: 1.835365]\n",
      "epoch:16 step:15523 [D loss: 0.665470, acc: 60.94%] [G loss: 1.944545]\n",
      "epoch:16 step:15524 [D loss: 0.616182, acc: 63.28%] [G loss: 1.931115]\n",
      "epoch:16 step:15525 [D loss: 0.640094, acc: 60.16%] [G loss: 1.743185]\n",
      "epoch:16 step:15526 [D loss: 0.614938, acc: 69.53%] [G loss: 1.921953]\n",
      "epoch:16 step:15527 [D loss: 0.655071, acc: 60.94%] [G loss: 1.788485]\n",
      "epoch:16 step:15528 [D loss: 0.582508, acc: 75.78%] [G loss: 1.966230]\n",
      "epoch:16 step:15529 [D loss: 0.656136, acc: 57.81%] [G loss: 1.874005]\n",
      "epoch:16 step:15530 [D loss: 0.655163, acc: 62.50%] [G loss: 1.845174]\n",
      "epoch:16 step:15531 [D loss: 0.583362, acc: 69.53%] [G loss: 1.914180]\n",
      "epoch:16 step:15532 [D loss: 0.690060, acc: 59.38%] [G loss: 1.821422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15533 [D loss: 0.608667, acc: 69.53%] [G loss: 1.813079]\n",
      "epoch:16 step:15534 [D loss: 0.650490, acc: 56.25%] [G loss: 1.991773]\n",
      "epoch:16 step:15535 [D loss: 0.717298, acc: 54.69%] [G loss: 1.801162]\n",
      "epoch:16 step:15536 [D loss: 0.594922, acc: 72.66%] [G loss: 1.953327]\n",
      "epoch:16 step:15537 [D loss: 0.663997, acc: 60.16%] [G loss: 2.013196]\n",
      "epoch:16 step:15538 [D loss: 0.604566, acc: 78.12%] [G loss: 1.846831]\n",
      "epoch:16 step:15539 [D loss: 0.617982, acc: 63.28%] [G loss: 1.927269]\n",
      "epoch:16 step:15540 [D loss: 0.656495, acc: 58.59%] [G loss: 2.052673]\n",
      "epoch:16 step:15541 [D loss: 0.578152, acc: 71.09%] [G loss: 1.933998]\n",
      "epoch:16 step:15542 [D loss: 0.644951, acc: 64.84%] [G loss: 1.962118]\n",
      "epoch:16 step:15543 [D loss: 0.588135, acc: 69.53%] [G loss: 2.014166]\n",
      "epoch:16 step:15544 [D loss: 0.580238, acc: 72.66%] [G loss: 2.153419]\n",
      "epoch:16 step:15545 [D loss: 0.659280, acc: 58.59%] [G loss: 1.799429]\n",
      "epoch:16 step:15546 [D loss: 0.590751, acc: 70.31%] [G loss: 2.117835]\n",
      "epoch:16 step:15547 [D loss: 0.647329, acc: 66.41%] [G loss: 1.925302]\n",
      "epoch:16 step:15548 [D loss: 0.611687, acc: 67.19%] [G loss: 2.140500]\n",
      "epoch:16 step:15549 [D loss: 0.576942, acc: 71.09%] [G loss: 2.210999]\n",
      "epoch:16 step:15550 [D loss: 0.607906, acc: 66.41%] [G loss: 2.214634]\n",
      "epoch:16 step:15551 [D loss: 0.670676, acc: 57.03%] [G loss: 1.910554]\n",
      "epoch:16 step:15552 [D loss: 0.624052, acc: 68.75%] [G loss: 1.848800]\n",
      "epoch:16 step:15553 [D loss: 0.622313, acc: 67.97%] [G loss: 1.824976]\n",
      "epoch:16 step:15554 [D loss: 0.585196, acc: 69.53%] [G loss: 1.858363]\n",
      "epoch:16 step:15555 [D loss: 0.684577, acc: 60.94%] [G loss: 1.951313]\n",
      "epoch:16 step:15556 [D loss: 0.668225, acc: 57.81%] [G loss: 1.977748]\n",
      "epoch:16 step:15557 [D loss: 0.604422, acc: 67.97%] [G loss: 1.959268]\n",
      "epoch:16 step:15558 [D loss: 0.655992, acc: 62.50%] [G loss: 1.855264]\n",
      "epoch:16 step:15559 [D loss: 0.664957, acc: 57.81%] [G loss: 1.956840]\n",
      "epoch:16 step:15560 [D loss: 0.652011, acc: 60.16%] [G loss: 1.900748]\n",
      "epoch:16 step:15561 [D loss: 0.674192, acc: 61.72%] [G loss: 1.817572]\n",
      "epoch:16 step:15562 [D loss: 0.690789, acc: 62.50%] [G loss: 1.823375]\n",
      "epoch:16 step:15563 [D loss: 0.644309, acc: 64.84%] [G loss: 1.945378]\n",
      "epoch:16 step:15564 [D loss: 0.709210, acc: 54.69%] [G loss: 1.894485]\n",
      "epoch:16 step:15565 [D loss: 0.634681, acc: 65.62%] [G loss: 1.881420]\n",
      "epoch:16 step:15566 [D loss: 0.632217, acc: 67.19%] [G loss: 2.058640]\n",
      "epoch:16 step:15567 [D loss: 0.621764, acc: 67.19%] [G loss: 1.902577]\n",
      "epoch:16 step:15568 [D loss: 0.663337, acc: 60.16%] [G loss: 1.814710]\n",
      "epoch:16 step:15569 [D loss: 0.728764, acc: 60.94%] [G loss: 1.868341]\n",
      "epoch:16 step:15570 [D loss: 0.670826, acc: 58.59%] [G loss: 1.977954]\n",
      "epoch:16 step:15571 [D loss: 0.628232, acc: 64.06%] [G loss: 1.887167]\n",
      "epoch:16 step:15572 [D loss: 0.670894, acc: 64.06%] [G loss: 1.828522]\n",
      "epoch:16 step:15573 [D loss: 0.605772, acc: 64.06%] [G loss: 1.826834]\n",
      "epoch:16 step:15574 [D loss: 0.612022, acc: 65.62%] [G loss: 1.863606]\n",
      "epoch:16 step:15575 [D loss: 0.661418, acc: 67.97%] [G loss: 1.832782]\n",
      "epoch:16 step:15576 [D loss: 0.654843, acc: 64.84%] [G loss: 1.826669]\n",
      "epoch:16 step:15577 [D loss: 0.601587, acc: 73.44%] [G loss: 1.908643]\n",
      "epoch:16 step:15578 [D loss: 0.629635, acc: 65.62%] [G loss: 1.913301]\n",
      "epoch:16 step:15579 [D loss: 0.663304, acc: 60.94%] [G loss: 1.963715]\n",
      "epoch:16 step:15580 [D loss: 0.634659, acc: 63.28%] [G loss: 2.029852]\n",
      "epoch:16 step:15581 [D loss: 0.593577, acc: 68.75%] [G loss: 2.145421]\n",
      "epoch:16 step:15582 [D loss: 0.700815, acc: 62.50%] [G loss: 2.067736]\n",
      "epoch:16 step:15583 [D loss: 0.624483, acc: 66.41%] [G loss: 1.970676]\n",
      "epoch:16 step:15584 [D loss: 0.608829, acc: 65.62%] [G loss: 1.921512]\n",
      "epoch:16 step:15585 [D loss: 0.590836, acc: 69.53%] [G loss: 1.985297]\n",
      "epoch:16 step:15586 [D loss: 0.704811, acc: 51.56%] [G loss: 1.930739]\n",
      "epoch:16 step:15587 [D loss: 0.700284, acc: 53.91%] [G loss: 1.961036]\n",
      "epoch:16 step:15588 [D loss: 0.610452, acc: 67.19%] [G loss: 1.864222]\n",
      "epoch:16 step:15589 [D loss: 0.680221, acc: 54.69%] [G loss: 1.770113]\n",
      "epoch:16 step:15590 [D loss: 0.640411, acc: 62.50%] [G loss: 1.912163]\n",
      "epoch:16 step:15591 [D loss: 0.660697, acc: 63.28%] [G loss: 1.878582]\n",
      "epoch:16 step:15592 [D loss: 0.671830, acc: 61.72%] [G loss: 1.965481]\n",
      "epoch:16 step:15593 [D loss: 0.647769, acc: 67.97%] [G loss: 1.959138]\n",
      "epoch:16 step:15594 [D loss: 0.645133, acc: 65.62%] [G loss: 1.862183]\n",
      "epoch:16 step:15595 [D loss: 0.611696, acc: 68.75%] [G loss: 1.890577]\n",
      "epoch:16 step:15596 [D loss: 0.646186, acc: 59.38%] [G loss: 1.903107]\n",
      "epoch:16 step:15597 [D loss: 0.580069, acc: 69.53%] [G loss: 1.933070]\n",
      "epoch:16 step:15598 [D loss: 0.721111, acc: 53.91%] [G loss: 1.866684]\n",
      "epoch:16 step:15599 [D loss: 0.631963, acc: 61.72%] [G loss: 1.879444]\n",
      "epoch:16 step:15600 [D loss: 0.661052, acc: 60.94%] [G loss: 1.978747]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.209576\n",
      "FID: 15.798367\n",
      "0 = 12.930149312591528\n",
      "1 = 0.0953018182490999\n",
      "2 = 0.8888999819755554\n",
      "3 = 0.9160000085830688\n",
      "4 = 0.8618000149726868\n",
      "5 = 0.8689053058624268\n",
      "6 = 0.9160000085830688\n",
      "7 = 6.668358148980139\n",
      "8 = 0.07971182059359949\n",
      "9 = 0.7271000146865845\n",
      "10 = 0.7411999702453613\n",
      "11 = 0.7129999995231628\n",
      "12 = 0.7208714485168457\n",
      "13 = 0.7411999702453613\n",
      "14 = 7.209605693817139\n",
      "15 = 9.402688026428223\n",
      "16 = 0.12792934477329254\n",
      "17 = 7.20957612991333\n",
      "18 = 15.79836654663086\n",
      "epoch:16 step:15601 [D loss: 0.641521, acc: 66.41%] [G loss: 1.968312]\n",
      "epoch:16 step:15602 [D loss: 0.645925, acc: 64.06%] [G loss: 1.800680]\n",
      "epoch:16 step:15603 [D loss: 0.608366, acc: 71.09%] [G loss: 1.835289]\n",
      "epoch:16 step:15604 [D loss: 0.643502, acc: 61.72%] [G loss: 1.808443]\n",
      "epoch:16 step:15605 [D loss: 0.631346, acc: 59.38%] [G loss: 1.943717]\n",
      "epoch:16 step:15606 [D loss: 0.660714, acc: 60.16%] [G loss: 1.771718]\n",
      "epoch:16 step:15607 [D loss: 0.696276, acc: 60.16%] [G loss: 1.848167]\n",
      "epoch:16 step:15608 [D loss: 0.687049, acc: 60.16%] [G loss: 1.767270]\n",
      "epoch:16 step:15609 [D loss: 0.657113, acc: 58.59%] [G loss: 1.907352]\n",
      "epoch:16 step:15610 [D loss: 0.653225, acc: 62.50%] [G loss: 1.803693]\n",
      "epoch:16 step:15611 [D loss: 0.705028, acc: 50.78%] [G loss: 1.841486]\n",
      "epoch:16 step:15612 [D loss: 0.631620, acc: 64.06%] [G loss: 1.866238]\n",
      "epoch:16 step:15613 [D loss: 0.649008, acc: 61.72%] [G loss: 1.986957]\n",
      "epoch:16 step:15614 [D loss: 0.631123, acc: 59.38%] [G loss: 1.874166]\n",
      "epoch:16 step:15615 [D loss: 0.674832, acc: 62.50%] [G loss: 1.860312]\n",
      "epoch:16 step:15616 [D loss: 0.616831, acc: 62.50%] [G loss: 1.992984]\n",
      "epoch:16 step:15617 [D loss: 0.634896, acc: 72.66%] [G loss: 1.732281]\n",
      "epoch:16 step:15618 [D loss: 0.681879, acc: 61.72%] [G loss: 1.756613]\n",
      "epoch:16 step:15619 [D loss: 0.686403, acc: 59.38%] [G loss: 1.818626]\n",
      "epoch:16 step:15620 [D loss: 0.639864, acc: 68.75%] [G loss: 1.712962]\n",
      "epoch:16 step:15621 [D loss: 0.575277, acc: 71.88%] [G loss: 1.954853]\n",
      "epoch:16 step:15622 [D loss: 0.653276, acc: 62.50%] [G loss: 1.748490]\n",
      "epoch:16 step:15623 [D loss: 0.607261, acc: 64.84%] [G loss: 1.945814]\n",
      "epoch:16 step:15624 [D loss: 0.646719, acc: 68.75%] [G loss: 1.988959]\n",
      "epoch:16 step:15625 [D loss: 0.616819, acc: 67.97%] [G loss: 2.081026]\n",
      "epoch:16 step:15626 [D loss: 0.636385, acc: 60.94%] [G loss: 2.056842]\n",
      "epoch:16 step:15627 [D loss: 0.646911, acc: 61.72%] [G loss: 2.030982]\n",
      "epoch:16 step:15628 [D loss: 0.646114, acc: 64.06%] [G loss: 1.841900]\n",
      "epoch:16 step:15629 [D loss: 0.631974, acc: 67.97%] [G loss: 1.905849]\n",
      "epoch:16 step:15630 [D loss: 0.633525, acc: 62.50%] [G loss: 2.000784]\n",
      "epoch:16 step:15631 [D loss: 0.638119, acc: 60.16%] [G loss: 1.876635]\n",
      "epoch:16 step:15632 [D loss: 0.658741, acc: 64.84%] [G loss: 1.884499]\n",
      "epoch:16 step:15633 [D loss: 0.637035, acc: 64.84%] [G loss: 1.999448]\n",
      "epoch:16 step:15634 [D loss: 0.619673, acc: 67.97%] [G loss: 2.084441]\n",
      "epoch:16 step:15635 [D loss: 0.617085, acc: 66.41%] [G loss: 1.846252]\n",
      "epoch:16 step:15636 [D loss: 0.658604, acc: 55.47%] [G loss: 2.012452]\n",
      "epoch:16 step:15637 [D loss: 0.655720, acc: 67.19%] [G loss: 1.946240]\n",
      "epoch:16 step:15638 [D loss: 0.642890, acc: 61.72%] [G loss: 2.094647]\n",
      "epoch:16 step:15639 [D loss: 0.600895, acc: 62.50%] [G loss: 2.131243]\n",
      "epoch:16 step:15640 [D loss: 0.584148, acc: 66.41%] [G loss: 2.273616]\n",
      "epoch:16 step:15641 [D loss: 0.625429, acc: 68.75%] [G loss: 2.287984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15642 [D loss: 0.602649, acc: 67.19%] [G loss: 2.063886]\n",
      "epoch:16 step:15643 [D loss: 0.666278, acc: 55.47%] [G loss: 1.917594]\n",
      "epoch:16 step:15644 [D loss: 0.673698, acc: 60.16%] [G loss: 1.922738]\n",
      "epoch:16 step:15645 [D loss: 0.624314, acc: 64.06%] [G loss: 2.031111]\n",
      "epoch:16 step:15646 [D loss: 0.665438, acc: 59.38%] [G loss: 2.102942]\n",
      "epoch:16 step:15647 [D loss: 0.655149, acc: 61.72%] [G loss: 1.979707]\n",
      "epoch:16 step:15648 [D loss: 0.651957, acc: 60.94%] [G loss: 1.813569]\n",
      "epoch:16 step:15649 [D loss: 0.669634, acc: 57.03%] [G loss: 1.697119]\n",
      "epoch:16 step:15650 [D loss: 0.701532, acc: 53.91%] [G loss: 1.755152]\n",
      "epoch:16 step:15651 [D loss: 0.641031, acc: 64.84%] [G loss: 1.801484]\n",
      "epoch:16 step:15652 [D loss: 0.665102, acc: 59.38%] [G loss: 1.849299]\n",
      "epoch:16 step:15653 [D loss: 0.638547, acc: 64.06%] [G loss: 1.873755]\n",
      "epoch:16 step:15654 [D loss: 0.650310, acc: 64.06%] [G loss: 1.894293]\n",
      "epoch:16 step:15655 [D loss: 0.631588, acc: 67.19%] [G loss: 1.843545]\n",
      "epoch:16 step:15656 [D loss: 0.668571, acc: 61.72%] [G loss: 1.827235]\n",
      "epoch:16 step:15657 [D loss: 0.683217, acc: 58.59%] [G loss: 1.902289]\n",
      "epoch:16 step:15658 [D loss: 0.660212, acc: 58.59%] [G loss: 1.786506]\n",
      "epoch:16 step:15659 [D loss: 0.686723, acc: 60.94%] [G loss: 1.693206]\n",
      "epoch:16 step:15660 [D loss: 0.689104, acc: 58.59%] [G loss: 1.771120]\n",
      "epoch:16 step:15661 [D loss: 0.674708, acc: 53.91%] [G loss: 1.765881]\n",
      "epoch:16 step:15662 [D loss: 0.635352, acc: 66.41%] [G loss: 1.789590]\n",
      "epoch:16 step:15663 [D loss: 0.686313, acc: 54.69%] [G loss: 1.780317]\n",
      "epoch:16 step:15664 [D loss: 0.636126, acc: 62.50%] [G loss: 1.820218]\n",
      "epoch:16 step:15665 [D loss: 0.641037, acc: 60.94%] [G loss: 1.795545]\n",
      "epoch:16 step:15666 [D loss: 0.657714, acc: 64.06%] [G loss: 1.865865]\n",
      "epoch:16 step:15667 [D loss: 0.633569, acc: 64.06%] [G loss: 1.777714]\n",
      "epoch:16 step:15668 [D loss: 0.643782, acc: 65.62%] [G loss: 1.769021]\n",
      "epoch:16 step:15669 [D loss: 0.634649, acc: 69.53%] [G loss: 1.950080]\n",
      "epoch:16 step:15670 [D loss: 0.616412, acc: 63.28%] [G loss: 1.956262]\n",
      "epoch:16 step:15671 [D loss: 0.649917, acc: 61.72%] [G loss: 1.907596]\n",
      "epoch:16 step:15672 [D loss: 0.621415, acc: 65.62%] [G loss: 1.816217]\n",
      "epoch:16 step:15673 [D loss: 0.615779, acc: 64.84%] [G loss: 1.977088]\n",
      "epoch:16 step:15674 [D loss: 0.589297, acc: 68.75%] [G loss: 1.890339]\n",
      "epoch:16 step:15675 [D loss: 0.646762, acc: 61.72%] [G loss: 1.904181]\n",
      "epoch:16 step:15676 [D loss: 0.659172, acc: 58.59%] [G loss: 1.820509]\n",
      "epoch:16 step:15677 [D loss: 0.597534, acc: 66.41%] [G loss: 1.772595]\n",
      "epoch:16 step:15678 [D loss: 0.703913, acc: 60.94%] [G loss: 1.887019]\n",
      "epoch:16 step:15679 [D loss: 0.659791, acc: 67.19%] [G loss: 1.794197]\n",
      "epoch:16 step:15680 [D loss: 0.642362, acc: 64.06%] [G loss: 1.832854]\n",
      "epoch:16 step:15681 [D loss: 0.630515, acc: 67.97%] [G loss: 1.964320]\n",
      "epoch:16 step:15682 [D loss: 0.630575, acc: 70.31%] [G loss: 1.981318]\n",
      "epoch:16 step:15683 [D loss: 0.639032, acc: 62.50%] [G loss: 2.259961]\n",
      "epoch:16 step:15684 [D loss: 0.614209, acc: 67.19%] [G loss: 2.029628]\n",
      "epoch:16 step:15685 [D loss: 0.587218, acc: 68.75%] [G loss: 2.070508]\n",
      "epoch:16 step:15686 [D loss: 0.588309, acc: 70.31%] [G loss: 2.175825]\n",
      "epoch:16 step:15687 [D loss: 0.619009, acc: 66.41%] [G loss: 1.990905]\n",
      "epoch:16 step:15688 [D loss: 0.648993, acc: 58.59%] [G loss: 1.959565]\n",
      "epoch:16 step:15689 [D loss: 0.661723, acc: 60.16%] [G loss: 1.812154]\n",
      "epoch:16 step:15690 [D loss: 0.701045, acc: 53.91%] [G loss: 1.874681]\n",
      "epoch:16 step:15691 [D loss: 0.612963, acc: 67.97%] [G loss: 1.979913]\n",
      "epoch:16 step:15692 [D loss: 0.639350, acc: 63.28%] [G loss: 2.014727]\n",
      "epoch:16 step:15693 [D loss: 0.634676, acc: 61.72%] [G loss: 1.932942]\n",
      "epoch:16 step:15694 [D loss: 0.698305, acc: 58.59%] [G loss: 1.796144]\n",
      "epoch:16 step:15695 [D loss: 0.671033, acc: 60.16%] [G loss: 1.833988]\n",
      "epoch:16 step:15696 [D loss: 0.731992, acc: 47.66%] [G loss: 1.816181]\n",
      "epoch:16 step:15697 [D loss: 0.658070, acc: 61.72%] [G loss: 1.923281]\n",
      "epoch:16 step:15698 [D loss: 0.647341, acc: 60.16%] [G loss: 1.830864]\n",
      "epoch:16 step:15699 [D loss: 0.622323, acc: 68.75%] [G loss: 2.119015]\n",
      "epoch:16 step:15700 [D loss: 0.588438, acc: 71.09%] [G loss: 2.027188]\n",
      "epoch:16 step:15701 [D loss: 0.605483, acc: 72.66%] [G loss: 2.033592]\n",
      "epoch:16 step:15702 [D loss: 0.637393, acc: 67.97%] [G loss: 1.935489]\n",
      "epoch:16 step:15703 [D loss: 0.695241, acc: 55.47%] [G loss: 1.872084]\n",
      "epoch:16 step:15704 [D loss: 0.578532, acc: 69.53%] [G loss: 2.037993]\n",
      "epoch:16 step:15705 [D loss: 0.676164, acc: 59.38%] [G loss: 1.963835]\n",
      "epoch:16 step:15706 [D loss: 0.671856, acc: 63.28%] [G loss: 1.968380]\n",
      "epoch:16 step:15707 [D loss: 0.665783, acc: 57.81%] [G loss: 2.012518]\n",
      "epoch:16 step:15708 [D loss: 0.724272, acc: 53.91%] [G loss: 1.860042]\n",
      "epoch:16 step:15709 [D loss: 0.670901, acc: 59.38%] [G loss: 1.857253]\n",
      "epoch:16 step:15710 [D loss: 0.640234, acc: 61.72%] [G loss: 1.902776]\n",
      "epoch:16 step:15711 [D loss: 0.683185, acc: 60.94%] [G loss: 2.016863]\n",
      "epoch:16 step:15712 [D loss: 0.606722, acc: 64.06%] [G loss: 1.954426]\n",
      "epoch:16 step:15713 [D loss: 0.613436, acc: 64.06%] [G loss: 1.950007]\n",
      "epoch:16 step:15714 [D loss: 0.746972, acc: 47.66%] [G loss: 1.810716]\n",
      "epoch:16 step:15715 [D loss: 0.652612, acc: 64.84%] [G loss: 1.827190]\n",
      "epoch:16 step:15716 [D loss: 0.642228, acc: 65.62%] [G loss: 1.990875]\n",
      "epoch:16 step:15717 [D loss: 0.617996, acc: 63.28%] [G loss: 2.001622]\n",
      "epoch:16 step:15718 [D loss: 0.643983, acc: 64.84%] [G loss: 1.902128]\n",
      "epoch:16 step:15719 [D loss: 0.678805, acc: 57.81%] [G loss: 1.778690]\n",
      "epoch:16 step:15720 [D loss: 0.642729, acc: 67.19%] [G loss: 1.858709]\n",
      "epoch:16 step:15721 [D loss: 0.618361, acc: 68.75%] [G loss: 1.978577]\n",
      "epoch:16 step:15722 [D loss: 0.596620, acc: 64.84%] [G loss: 1.964337]\n",
      "epoch:16 step:15723 [D loss: 0.680675, acc: 56.25%] [G loss: 1.800226]\n",
      "epoch:16 step:15724 [D loss: 0.636817, acc: 62.50%] [G loss: 1.880626]\n",
      "epoch:16 step:15725 [D loss: 0.654849, acc: 61.72%] [G loss: 1.949918]\n",
      "epoch:16 step:15726 [D loss: 0.686701, acc: 59.38%] [G loss: 1.835205]\n",
      "epoch:16 step:15727 [D loss: 0.633183, acc: 64.84%] [G loss: 2.016202]\n",
      "epoch:16 step:15728 [D loss: 0.660313, acc: 60.16%] [G loss: 2.056336]\n",
      "epoch:16 step:15729 [D loss: 0.596682, acc: 68.75%] [G loss: 1.872355]\n",
      "epoch:16 step:15730 [D loss: 0.635820, acc: 63.28%] [G loss: 1.907335]\n",
      "epoch:16 step:15731 [D loss: 0.700218, acc: 59.38%] [G loss: 1.917370]\n",
      "epoch:16 step:15732 [D loss: 0.628112, acc: 64.84%] [G loss: 1.822872]\n",
      "epoch:16 step:15733 [D loss: 0.675575, acc: 57.81%] [G loss: 1.704831]\n",
      "epoch:16 step:15734 [D loss: 0.613795, acc: 67.97%] [G loss: 1.968734]\n",
      "epoch:16 step:15735 [D loss: 0.631779, acc: 61.72%] [G loss: 1.928388]\n",
      "epoch:16 step:15736 [D loss: 0.586048, acc: 67.97%] [G loss: 1.849186]\n",
      "epoch:16 step:15737 [D loss: 0.661134, acc: 61.72%] [G loss: 1.939431]\n",
      "epoch:16 step:15738 [D loss: 0.585524, acc: 66.41%] [G loss: 2.131722]\n",
      "epoch:16 step:15739 [D loss: 0.614831, acc: 71.09%] [G loss: 2.058967]\n",
      "epoch:16 step:15740 [D loss: 0.696003, acc: 54.69%] [G loss: 1.865166]\n",
      "epoch:16 step:15741 [D loss: 0.641359, acc: 60.16%] [G loss: 1.860857]\n",
      "epoch:16 step:15742 [D loss: 0.655965, acc: 57.81%] [G loss: 1.787437]\n",
      "epoch:16 step:15743 [D loss: 0.671150, acc: 60.94%] [G loss: 1.778438]\n",
      "epoch:16 step:15744 [D loss: 0.707775, acc: 54.69%] [G loss: 1.827327]\n",
      "epoch:16 step:15745 [D loss: 0.600783, acc: 69.53%] [G loss: 1.967824]\n",
      "epoch:16 step:15746 [D loss: 0.587443, acc: 71.09%] [G loss: 1.976045]\n",
      "epoch:16 step:15747 [D loss: 0.646708, acc: 57.81%] [G loss: 1.968145]\n",
      "epoch:16 step:15748 [D loss: 0.647811, acc: 61.72%] [G loss: 1.938806]\n",
      "epoch:16 step:15749 [D loss: 0.595952, acc: 64.84%] [G loss: 1.879334]\n",
      "epoch:16 step:15750 [D loss: 0.602859, acc: 63.28%] [G loss: 1.820760]\n",
      "epoch:16 step:15751 [D loss: 0.620748, acc: 71.88%] [G loss: 1.923691]\n",
      "epoch:16 step:15752 [D loss: 0.623194, acc: 67.97%] [G loss: 2.051443]\n",
      "epoch:16 step:15753 [D loss: 0.632258, acc: 68.75%] [G loss: 1.942372]\n",
      "epoch:16 step:15754 [D loss: 0.638814, acc: 60.94%] [G loss: 1.927163]\n",
      "epoch:16 step:15755 [D loss: 0.631895, acc: 57.03%] [G loss: 2.026210]\n",
      "epoch:16 step:15756 [D loss: 0.612560, acc: 64.84%] [G loss: 1.914289]\n",
      "epoch:16 step:15757 [D loss: 0.689862, acc: 60.16%] [G loss: 1.844652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15758 [D loss: 0.644548, acc: 64.84%] [G loss: 1.849648]\n",
      "epoch:16 step:15759 [D loss: 0.680856, acc: 59.38%] [G loss: 1.932260]\n",
      "epoch:16 step:15760 [D loss: 0.669242, acc: 61.72%] [G loss: 1.970757]\n",
      "epoch:16 step:15761 [D loss: 0.613260, acc: 66.41%] [G loss: 2.132443]\n",
      "epoch:16 step:15762 [D loss: 0.590704, acc: 67.19%] [G loss: 2.022489]\n",
      "epoch:16 step:15763 [D loss: 0.635170, acc: 64.84%] [G loss: 1.881137]\n",
      "epoch:16 step:15764 [D loss: 0.673783, acc: 53.91%] [G loss: 1.886302]\n",
      "epoch:16 step:15765 [D loss: 0.640835, acc: 66.41%] [G loss: 1.905992]\n",
      "epoch:16 step:15766 [D loss: 0.614956, acc: 64.84%] [G loss: 2.211726]\n",
      "epoch:16 step:15767 [D loss: 0.572626, acc: 72.66%] [G loss: 2.286575]\n",
      "epoch:16 step:15768 [D loss: 0.639569, acc: 61.72%] [G loss: 2.026022]\n",
      "epoch:16 step:15769 [D loss: 0.614165, acc: 67.97%] [G loss: 2.063687]\n",
      "epoch:16 step:15770 [D loss: 0.718962, acc: 53.12%] [G loss: 1.976682]\n",
      "epoch:16 step:15771 [D loss: 0.640538, acc: 68.75%] [G loss: 2.016144]\n",
      "epoch:16 step:15772 [D loss: 0.637745, acc: 63.28%] [G loss: 1.986198]\n",
      "epoch:16 step:15773 [D loss: 0.645930, acc: 67.97%] [G loss: 2.016805]\n",
      "epoch:16 step:15774 [D loss: 0.643353, acc: 60.94%] [G loss: 2.268479]\n",
      "epoch:16 step:15775 [D loss: 0.676032, acc: 61.72%] [G loss: 1.836074]\n",
      "epoch:16 step:15776 [D loss: 0.665875, acc: 59.38%] [G loss: 1.794768]\n",
      "epoch:16 step:15777 [D loss: 0.678191, acc: 65.62%] [G loss: 1.957171]\n",
      "epoch:16 step:15778 [D loss: 0.656482, acc: 60.94%] [G loss: 1.950898]\n",
      "epoch:16 step:15779 [D loss: 0.663826, acc: 61.72%] [G loss: 1.912539]\n",
      "epoch:16 step:15780 [D loss: 0.642922, acc: 60.94%] [G loss: 1.880462]\n",
      "epoch:16 step:15781 [D loss: 0.691071, acc: 58.59%] [G loss: 1.926594]\n",
      "epoch:16 step:15782 [D loss: 0.606548, acc: 67.19%] [G loss: 2.038741]\n",
      "epoch:16 step:15783 [D loss: 0.590675, acc: 71.09%] [G loss: 2.019337]\n",
      "epoch:16 step:15784 [D loss: 0.627546, acc: 64.06%] [G loss: 2.021249]\n",
      "epoch:16 step:15785 [D loss: 0.607906, acc: 67.19%] [G loss: 2.074606]\n",
      "epoch:16 step:15786 [D loss: 0.682952, acc: 57.03%] [G loss: 1.766592]\n",
      "epoch:16 step:15787 [D loss: 0.631384, acc: 63.28%] [G loss: 1.930675]\n",
      "epoch:16 step:15788 [D loss: 0.620393, acc: 69.53%] [G loss: 1.912395]\n",
      "epoch:16 step:15789 [D loss: 0.610132, acc: 71.09%] [G loss: 1.762800]\n",
      "epoch:16 step:15790 [D loss: 0.693656, acc: 63.28%] [G loss: 1.955177]\n",
      "epoch:16 step:15791 [D loss: 0.639478, acc: 64.06%] [G loss: 1.849777]\n",
      "epoch:16 step:15792 [D loss: 0.711364, acc: 54.69%] [G loss: 1.769268]\n",
      "epoch:16 step:15793 [D loss: 0.738596, acc: 51.56%] [G loss: 1.774894]\n",
      "epoch:16 step:15794 [D loss: 0.645275, acc: 60.94%] [G loss: 1.796883]\n",
      "epoch:16 step:15795 [D loss: 0.598865, acc: 69.53%] [G loss: 1.836568]\n",
      "epoch:16 step:15796 [D loss: 0.647334, acc: 60.94%] [G loss: 1.924781]\n",
      "epoch:16 step:15797 [D loss: 0.629790, acc: 65.62%] [G loss: 1.973501]\n",
      "epoch:16 step:15798 [D loss: 0.623648, acc: 69.53%] [G loss: 1.997838]\n",
      "epoch:16 step:15799 [D loss: 0.613646, acc: 67.97%] [G loss: 1.897476]\n",
      "epoch:16 step:15800 [D loss: 0.696440, acc: 62.50%] [G loss: 1.942508]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.193062\n",
      "FID: 15.124714\n",
      "0 = 12.718257579040511\n",
      "1 = 0.08377830596384066\n",
      "2 = 0.8841000199317932\n",
      "3 = 0.9057999849319458\n",
      "4 = 0.8623999953269958\n",
      "5 = 0.868123471736908\n",
      "6 = 0.9057999849319458\n",
      "7 = 6.620334078550338\n",
      "8 = 0.07401367767329935\n",
      "9 = 0.7337999939918518\n",
      "10 = 0.7411999702453613\n",
      "11 = 0.7264000177383423\n",
      "12 = 0.7303902506828308\n",
      "13 = 0.7411999702453613\n",
      "14 = 7.193089008331299\n",
      "15 = 9.463691711425781\n",
      "16 = 0.11546505987644196\n",
      "17 = 7.1930623054504395\n",
      "18 = 15.124713897705078\n",
      "epoch:16 step:15801 [D loss: 0.632665, acc: 67.97%] [G loss: 1.923425]\n",
      "epoch:16 step:15802 [D loss: 0.657622, acc: 65.62%] [G loss: 1.863401]\n",
      "epoch:16 step:15803 [D loss: 0.617230, acc: 64.06%] [G loss: 1.792876]\n",
      "epoch:16 step:15804 [D loss: 0.634015, acc: 64.84%] [G loss: 1.819987]\n",
      "epoch:16 step:15805 [D loss: 0.675467, acc: 63.28%] [G loss: 1.833169]\n",
      "epoch:16 step:15806 [D loss: 0.681806, acc: 58.59%] [G loss: 1.854471]\n",
      "epoch:16 step:15807 [D loss: 0.604252, acc: 71.09%] [G loss: 2.239405]\n",
      "epoch:16 step:15808 [D loss: 0.587111, acc: 67.97%] [G loss: 2.136229]\n",
      "epoch:16 step:15809 [D loss: 0.685590, acc: 59.38%] [G loss: 1.780808]\n",
      "epoch:16 step:15810 [D loss: 0.720893, acc: 54.69%] [G loss: 1.861089]\n",
      "epoch:16 step:15811 [D loss: 0.612282, acc: 61.72%] [G loss: 1.862536]\n",
      "epoch:16 step:15812 [D loss: 0.704468, acc: 53.91%] [G loss: 1.700063]\n",
      "epoch:16 step:15813 [D loss: 0.638829, acc: 66.41%] [G loss: 1.788376]\n",
      "epoch:16 step:15814 [D loss: 0.644430, acc: 61.72%] [G loss: 1.920041]\n",
      "epoch:16 step:15815 [D loss: 0.559581, acc: 75.78%] [G loss: 2.055483]\n",
      "epoch:16 step:15816 [D loss: 0.681691, acc: 59.38%] [G loss: 1.875017]\n",
      "epoch:16 step:15817 [D loss: 0.652713, acc: 64.06%] [G loss: 2.067650]\n",
      "epoch:16 step:15818 [D loss: 0.657382, acc: 60.94%] [G loss: 1.983836]\n",
      "epoch:16 step:15819 [D loss: 0.676804, acc: 57.03%] [G loss: 1.752483]\n",
      "epoch:16 step:15820 [D loss: 0.685722, acc: 60.16%] [G loss: 1.785775]\n",
      "epoch:16 step:15821 [D loss: 0.656734, acc: 64.06%] [G loss: 1.767539]\n",
      "epoch:16 step:15822 [D loss: 0.710886, acc: 53.91%] [G loss: 1.674806]\n",
      "epoch:16 step:15823 [D loss: 0.669689, acc: 59.38%] [G loss: 1.895887]\n",
      "epoch:16 step:15824 [D loss: 0.618117, acc: 65.62%] [G loss: 1.839615]\n",
      "epoch:16 step:15825 [D loss: 0.594386, acc: 69.53%] [G loss: 1.946325]\n",
      "epoch:16 step:15826 [D loss: 0.646691, acc: 64.06%] [G loss: 1.960653]\n",
      "epoch:16 step:15827 [D loss: 0.655350, acc: 63.28%] [G loss: 1.799408]\n",
      "epoch:16 step:15828 [D loss: 0.621478, acc: 68.75%] [G loss: 1.906239]\n",
      "epoch:16 step:15829 [D loss: 0.663202, acc: 60.94%] [G loss: 1.908619]\n",
      "epoch:16 step:15830 [D loss: 0.637248, acc: 63.28%] [G loss: 1.854586]\n",
      "epoch:16 step:15831 [D loss: 0.657717, acc: 64.84%] [G loss: 1.733643]\n",
      "epoch:16 step:15832 [D loss: 0.619832, acc: 64.84%] [G loss: 1.974688]\n",
      "epoch:16 step:15833 [D loss: 0.640552, acc: 63.28%] [G loss: 1.915593]\n",
      "epoch:16 step:15834 [D loss: 0.617934, acc: 71.88%] [G loss: 2.040678]\n",
      "epoch:16 step:15835 [D loss: 0.657452, acc: 68.75%] [G loss: 1.848930]\n",
      "epoch:16 step:15836 [D loss: 0.616390, acc: 68.75%] [G loss: 1.887553]\n",
      "epoch:16 step:15837 [D loss: 0.624455, acc: 63.28%] [G loss: 2.017558]\n",
      "epoch:16 step:15838 [D loss: 0.663008, acc: 60.16%] [G loss: 1.894454]\n",
      "epoch:16 step:15839 [D loss: 0.633915, acc: 67.19%] [G loss: 1.917769]\n",
      "epoch:16 step:15840 [D loss: 0.644170, acc: 62.50%] [G loss: 1.927902]\n",
      "epoch:16 step:15841 [D loss: 0.638982, acc: 64.84%] [G loss: 1.905426]\n",
      "epoch:16 step:15842 [D loss: 0.661932, acc: 57.03%] [G loss: 1.865169]\n",
      "epoch:16 step:15843 [D loss: 0.635733, acc: 68.75%] [G loss: 1.839088]\n",
      "epoch:16 step:15844 [D loss: 0.653289, acc: 53.91%] [G loss: 1.908113]\n",
      "epoch:16 step:15845 [D loss: 0.611243, acc: 64.84%] [G loss: 1.890807]\n",
      "epoch:16 step:15846 [D loss: 0.665960, acc: 60.16%] [G loss: 1.844153]\n",
      "epoch:16 step:15847 [D loss: 0.657619, acc: 59.38%] [G loss: 1.690962]\n",
      "epoch:16 step:15848 [D loss: 0.665980, acc: 53.91%] [G loss: 1.792585]\n",
      "epoch:16 step:15849 [D loss: 0.629782, acc: 67.19%] [G loss: 1.950978]\n",
      "epoch:16 step:15850 [D loss: 0.631132, acc: 64.84%] [G loss: 1.844489]\n",
      "epoch:16 step:15851 [D loss: 0.756316, acc: 50.00%] [G loss: 1.807930]\n",
      "epoch:16 step:15852 [D loss: 0.607311, acc: 71.88%] [G loss: 2.014916]\n",
      "epoch:16 step:15853 [D loss: 0.644071, acc: 61.72%] [G loss: 1.914028]\n",
      "epoch:16 step:15854 [D loss: 0.653482, acc: 65.62%] [G loss: 1.727312]\n",
      "epoch:16 step:15855 [D loss: 0.695342, acc: 61.72%] [G loss: 1.807156]\n",
      "epoch:16 step:15856 [D loss: 0.655664, acc: 59.38%] [G loss: 1.975577]\n",
      "epoch:16 step:15857 [D loss: 0.685368, acc: 56.25%] [G loss: 1.763612]\n",
      "epoch:16 step:15858 [D loss: 0.646591, acc: 61.72%] [G loss: 1.835599]\n",
      "epoch:16 step:15859 [D loss: 0.609942, acc: 66.41%] [G loss: 1.740540]\n",
      "epoch:16 step:15860 [D loss: 0.645315, acc: 57.03%] [G loss: 1.915714]\n",
      "epoch:16 step:15861 [D loss: 0.681455, acc: 55.47%] [G loss: 1.824072]\n",
      "epoch:16 step:15862 [D loss: 0.631981, acc: 65.62%] [G loss: 1.889442]\n",
      "epoch:16 step:15863 [D loss: 0.633202, acc: 61.72%] [G loss: 1.957343]\n",
      "epoch:16 step:15864 [D loss: 0.597541, acc: 68.75%] [G loss: 1.791162]\n",
      "epoch:16 step:15865 [D loss: 0.660551, acc: 58.59%] [G loss: 1.723019]\n",
      "epoch:16 step:15866 [D loss: 0.707901, acc: 53.12%] [G loss: 1.789796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15867 [D loss: 0.591932, acc: 68.75%] [G loss: 1.929229]\n",
      "epoch:16 step:15868 [D loss: 0.597803, acc: 66.41%] [G loss: 1.804038]\n",
      "epoch:16 step:15869 [D loss: 0.618435, acc: 62.50%] [G loss: 1.930764]\n",
      "epoch:16 step:15870 [D loss: 0.634015, acc: 63.28%] [G loss: 1.925454]\n",
      "epoch:16 step:15871 [D loss: 0.621186, acc: 67.19%] [G loss: 1.866054]\n",
      "epoch:16 step:15872 [D loss: 0.668274, acc: 65.62%] [G loss: 1.838860]\n",
      "epoch:16 step:15873 [D loss: 0.649587, acc: 62.50%] [G loss: 1.746270]\n",
      "epoch:16 step:15874 [D loss: 0.657157, acc: 57.03%] [G loss: 1.910541]\n",
      "epoch:16 step:15875 [D loss: 0.699409, acc: 56.25%] [G loss: 1.871479]\n",
      "epoch:16 step:15876 [D loss: 0.618728, acc: 64.84%] [G loss: 2.009473]\n",
      "epoch:16 step:15877 [D loss: 0.628432, acc: 66.41%] [G loss: 2.022534]\n",
      "epoch:16 step:15878 [D loss: 0.585576, acc: 71.88%] [G loss: 2.069786]\n",
      "epoch:16 step:15879 [D loss: 0.689662, acc: 63.28%] [G loss: 2.064180]\n",
      "epoch:16 step:15880 [D loss: 0.679431, acc: 61.72%] [G loss: 1.964947]\n",
      "epoch:16 step:15881 [D loss: 0.680728, acc: 60.16%] [G loss: 1.728573]\n",
      "epoch:16 step:15882 [D loss: 0.653738, acc: 71.09%] [G loss: 2.068137]\n",
      "epoch:16 step:15883 [D loss: 0.639491, acc: 64.84%] [G loss: 1.899047]\n",
      "epoch:16 step:15884 [D loss: 0.658792, acc: 60.16%] [G loss: 1.909127]\n",
      "epoch:16 step:15885 [D loss: 0.710626, acc: 55.47%] [G loss: 1.925571]\n",
      "epoch:16 step:15886 [D loss: 0.609046, acc: 64.06%] [G loss: 2.032314]\n",
      "epoch:16 step:15887 [D loss: 0.624961, acc: 65.62%] [G loss: 1.878205]\n",
      "epoch:16 step:15888 [D loss: 0.640000, acc: 61.72%] [G loss: 2.020633]\n",
      "epoch:16 step:15889 [D loss: 0.674359, acc: 58.59%] [G loss: 1.953304]\n",
      "epoch:16 step:15890 [D loss: 0.608878, acc: 69.53%] [G loss: 1.917782]\n",
      "epoch:16 step:15891 [D loss: 0.626298, acc: 63.28%] [G loss: 2.036395]\n",
      "epoch:16 step:15892 [D loss: 0.653067, acc: 57.81%] [G loss: 1.932663]\n",
      "epoch:16 step:15893 [D loss: 0.680737, acc: 55.47%] [G loss: 1.936371]\n",
      "epoch:16 step:15894 [D loss: 0.593927, acc: 66.41%] [G loss: 1.833159]\n",
      "epoch:16 step:15895 [D loss: 0.645205, acc: 62.50%] [G loss: 1.917278]\n",
      "epoch:16 step:15896 [D loss: 0.599733, acc: 69.53%] [G loss: 2.119581]\n",
      "epoch:16 step:15897 [D loss: 0.593792, acc: 64.06%] [G loss: 2.040869]\n",
      "epoch:16 step:15898 [D loss: 0.643445, acc: 66.41%] [G loss: 1.996868]\n",
      "epoch:16 step:15899 [D loss: 0.635477, acc: 62.50%] [G loss: 1.967447]\n",
      "epoch:16 step:15900 [D loss: 0.676000, acc: 56.25%] [G loss: 1.952019]\n",
      "epoch:16 step:15901 [D loss: 0.637681, acc: 60.94%] [G loss: 2.024244]\n",
      "epoch:16 step:15902 [D loss: 0.651317, acc: 63.28%] [G loss: 2.102285]\n",
      "epoch:16 step:15903 [D loss: 0.632236, acc: 67.19%] [G loss: 2.061484]\n",
      "epoch:16 step:15904 [D loss: 0.652371, acc: 60.16%] [G loss: 2.202859]\n",
      "epoch:16 step:15905 [D loss: 0.698862, acc: 58.59%] [G loss: 1.876841]\n",
      "epoch:16 step:15906 [D loss: 0.693024, acc: 57.03%] [G loss: 1.860037]\n",
      "epoch:16 step:15907 [D loss: 0.657512, acc: 64.06%] [G loss: 1.835820]\n",
      "epoch:16 step:15908 [D loss: 0.658868, acc: 60.16%] [G loss: 1.907851]\n",
      "epoch:16 step:15909 [D loss: 0.578583, acc: 71.09%] [G loss: 2.007539]\n",
      "epoch:16 step:15910 [D loss: 0.574282, acc: 70.31%] [G loss: 2.003853]\n",
      "epoch:16 step:15911 [D loss: 0.608286, acc: 66.41%] [G loss: 2.117148]\n",
      "epoch:16 step:15912 [D loss: 0.694595, acc: 56.25%] [G loss: 1.740202]\n",
      "epoch:16 step:15913 [D loss: 0.660410, acc: 63.28%] [G loss: 2.052653]\n",
      "epoch:16 step:15914 [D loss: 0.678826, acc: 54.69%] [G loss: 1.899959]\n",
      "epoch:16 step:15915 [D loss: 0.612179, acc: 71.09%] [G loss: 2.225115]\n",
      "epoch:16 step:15916 [D loss: 0.612855, acc: 67.97%] [G loss: 2.146123]\n",
      "epoch:16 step:15917 [D loss: 0.614004, acc: 66.41%] [G loss: 2.102482]\n",
      "epoch:16 step:15918 [D loss: 0.646910, acc: 62.50%] [G loss: 2.156406]\n",
      "epoch:16 step:15919 [D loss: 0.611894, acc: 63.28%] [G loss: 2.194415]\n",
      "epoch:16 step:15920 [D loss: 0.743261, acc: 54.69%] [G loss: 1.712770]\n",
      "epoch:16 step:15921 [D loss: 0.768527, acc: 39.84%] [G loss: 1.948508]\n",
      "epoch:16 step:15922 [D loss: 0.577869, acc: 74.22%] [G loss: 2.059400]\n",
      "epoch:16 step:15923 [D loss: 0.592928, acc: 69.53%] [G loss: 2.008330]\n",
      "epoch:16 step:15924 [D loss: 0.662223, acc: 60.16%] [G loss: 2.027119]\n",
      "epoch:16 step:15925 [D loss: 0.649715, acc: 62.50%] [G loss: 1.923802]\n",
      "epoch:16 step:15926 [D loss: 0.627690, acc: 62.50%] [G loss: 1.911144]\n",
      "epoch:16 step:15927 [D loss: 0.654503, acc: 60.16%] [G loss: 1.971406]\n",
      "epoch:16 step:15928 [D loss: 0.637243, acc: 61.72%] [G loss: 1.948541]\n",
      "epoch:16 step:15929 [D loss: 0.560772, acc: 73.44%] [G loss: 2.321857]\n",
      "epoch:17 step:15930 [D loss: 0.660597, acc: 62.50%] [G loss: 1.938647]\n",
      "epoch:17 step:15931 [D loss: 0.624738, acc: 64.84%] [G loss: 1.915499]\n",
      "epoch:17 step:15932 [D loss: 0.647020, acc: 64.06%] [G loss: 1.990741]\n",
      "epoch:17 step:15933 [D loss: 0.615752, acc: 64.06%] [G loss: 1.882558]\n",
      "epoch:17 step:15934 [D loss: 0.627981, acc: 64.84%] [G loss: 2.009960]\n",
      "epoch:17 step:15935 [D loss: 0.644633, acc: 60.16%] [G loss: 2.177064]\n",
      "epoch:17 step:15936 [D loss: 0.663626, acc: 60.16%] [G loss: 2.109120]\n",
      "epoch:17 step:15937 [D loss: 0.618278, acc: 65.62%] [G loss: 1.968832]\n",
      "epoch:17 step:15938 [D loss: 0.598321, acc: 64.84%] [G loss: 1.893365]\n",
      "epoch:17 step:15939 [D loss: 0.625090, acc: 67.19%] [G loss: 1.924438]\n",
      "epoch:17 step:15940 [D loss: 0.636440, acc: 59.38%] [G loss: 1.987060]\n",
      "epoch:17 step:15941 [D loss: 0.606059, acc: 66.41%] [G loss: 1.961278]\n",
      "epoch:17 step:15942 [D loss: 0.661376, acc: 65.62%] [G loss: 1.872868]\n",
      "epoch:17 step:15943 [D loss: 0.657209, acc: 59.38%] [G loss: 1.946782]\n",
      "epoch:17 step:15944 [D loss: 0.615801, acc: 64.06%] [G loss: 2.112922]\n",
      "epoch:17 step:15945 [D loss: 0.534288, acc: 77.34%] [G loss: 2.210510]\n",
      "epoch:17 step:15946 [D loss: 0.685091, acc: 55.47%] [G loss: 1.931159]\n",
      "epoch:17 step:15947 [D loss: 0.674839, acc: 58.59%] [G loss: 2.033653]\n",
      "epoch:17 step:15948 [D loss: 0.699261, acc: 55.47%] [G loss: 1.825396]\n",
      "epoch:17 step:15949 [D loss: 0.649689, acc: 64.06%] [G loss: 1.827472]\n",
      "epoch:17 step:15950 [D loss: 0.640032, acc: 59.38%] [G loss: 1.920659]\n",
      "epoch:17 step:15951 [D loss: 0.670433, acc: 66.41%] [G loss: 1.864183]\n",
      "epoch:17 step:15952 [D loss: 0.641654, acc: 61.72%] [G loss: 2.004833]\n",
      "epoch:17 step:15953 [D loss: 0.636462, acc: 63.28%] [G loss: 1.807684]\n",
      "epoch:17 step:15954 [D loss: 0.631421, acc: 61.72%] [G loss: 2.061565]\n",
      "epoch:17 step:15955 [D loss: 0.689541, acc: 55.47%] [G loss: 1.798064]\n",
      "epoch:17 step:15956 [D loss: 0.688685, acc: 56.25%] [G loss: 1.933954]\n",
      "epoch:17 step:15957 [D loss: 0.665538, acc: 56.25%] [G loss: 1.894922]\n",
      "epoch:17 step:15958 [D loss: 0.635830, acc: 68.75%] [G loss: 1.813321]\n",
      "epoch:17 step:15959 [D loss: 0.622549, acc: 64.06%] [G loss: 1.954412]\n",
      "epoch:17 step:15960 [D loss: 0.680951, acc: 54.69%] [G loss: 1.787675]\n",
      "epoch:17 step:15961 [D loss: 0.667801, acc: 58.59%] [G loss: 1.761690]\n",
      "epoch:17 step:15962 [D loss: 0.643830, acc: 65.62%] [G loss: 1.902267]\n",
      "epoch:17 step:15963 [D loss: 0.609344, acc: 66.41%] [G loss: 1.841816]\n",
      "epoch:17 step:15964 [D loss: 0.665215, acc: 64.06%] [G loss: 1.868216]\n",
      "epoch:17 step:15965 [D loss: 0.626584, acc: 68.75%] [G loss: 1.917200]\n",
      "epoch:17 step:15966 [D loss: 0.669722, acc: 57.03%] [G loss: 1.966404]\n",
      "epoch:17 step:15967 [D loss: 0.677995, acc: 57.81%] [G loss: 1.821956]\n",
      "epoch:17 step:15968 [D loss: 0.663058, acc: 59.38%] [G loss: 1.912261]\n",
      "epoch:17 step:15969 [D loss: 0.626882, acc: 68.75%] [G loss: 2.061792]\n",
      "epoch:17 step:15970 [D loss: 0.640558, acc: 58.59%] [G loss: 1.868734]\n",
      "epoch:17 step:15971 [D loss: 0.632444, acc: 63.28%] [G loss: 1.901874]\n",
      "epoch:17 step:15972 [D loss: 0.677210, acc: 60.16%] [G loss: 1.923420]\n",
      "epoch:17 step:15973 [D loss: 0.684014, acc: 55.47%] [G loss: 1.838192]\n",
      "epoch:17 step:15974 [D loss: 0.636152, acc: 65.62%] [G loss: 1.792733]\n",
      "epoch:17 step:15975 [D loss: 0.668228, acc: 54.69%] [G loss: 1.800895]\n",
      "epoch:17 step:15976 [D loss: 0.675986, acc: 59.38%] [G loss: 1.962342]\n",
      "epoch:17 step:15977 [D loss: 0.655313, acc: 58.59%] [G loss: 1.875569]\n",
      "epoch:17 step:15978 [D loss: 0.622413, acc: 61.72%] [G loss: 1.875021]\n",
      "epoch:17 step:15979 [D loss: 0.613050, acc: 67.97%] [G loss: 2.055780]\n",
      "epoch:17 step:15980 [D loss: 0.666753, acc: 60.16%] [G loss: 1.877187]\n",
      "epoch:17 step:15981 [D loss: 0.642664, acc: 61.72%] [G loss: 1.968820]\n",
      "epoch:17 step:15982 [D loss: 0.596834, acc: 72.66%] [G loss: 1.912883]\n",
      "epoch:17 step:15983 [D loss: 0.622284, acc: 63.28%] [G loss: 1.969045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:15984 [D loss: 0.593663, acc: 66.41%] [G loss: 1.924906]\n",
      "epoch:17 step:15985 [D loss: 0.638519, acc: 62.50%] [G loss: 1.950553]\n",
      "epoch:17 step:15986 [D loss: 0.649589, acc: 62.50%] [G loss: 1.839754]\n",
      "epoch:17 step:15987 [D loss: 0.672357, acc: 61.72%] [G loss: 1.865093]\n",
      "epoch:17 step:15988 [D loss: 0.671869, acc: 57.03%] [G loss: 1.896914]\n",
      "epoch:17 step:15989 [D loss: 0.615085, acc: 66.41%] [G loss: 1.865351]\n",
      "epoch:17 step:15990 [D loss: 0.694053, acc: 64.06%] [G loss: 1.925115]\n",
      "epoch:17 step:15991 [D loss: 0.629455, acc: 62.50%] [G loss: 1.884039]\n",
      "epoch:17 step:15992 [D loss: 0.661831, acc: 63.28%] [G loss: 1.944195]\n",
      "epoch:17 step:15993 [D loss: 0.630848, acc: 63.28%] [G loss: 1.865446]\n",
      "epoch:17 step:15994 [D loss: 0.632390, acc: 63.28%] [G loss: 1.844747]\n",
      "epoch:17 step:15995 [D loss: 0.658400, acc: 58.59%] [G loss: 1.786598]\n",
      "epoch:17 step:15996 [D loss: 0.627392, acc: 66.41%] [G loss: 1.972828]\n",
      "epoch:17 step:15997 [D loss: 0.600316, acc: 65.62%] [G loss: 2.005272]\n",
      "epoch:17 step:15998 [D loss: 0.646458, acc: 64.06%] [G loss: 2.025444]\n",
      "epoch:17 step:15999 [D loss: 0.618747, acc: 65.62%] [G loss: 2.046603]\n",
      "epoch:17 step:16000 [D loss: 0.679401, acc: 59.38%] [G loss: 1.869830]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.336645\n",
      "FID: 12.761156\n",
      "0 = 12.992830225944548\n",
      "1 = 0.09547115940980676\n",
      "2 = 0.8919000029563904\n",
      "3 = 0.9093999862670898\n",
      "4 = 0.8744000196456909\n",
      "5 = 0.8786473274230957\n",
      "6 = 0.9093999862670898\n",
      "7 = 6.540528600502017\n",
      "8 = 0.06737219685401054\n",
      "9 = 0.7382000088691711\n",
      "10 = 0.7534000277519226\n",
      "11 = 0.7229999899864197\n",
      "12 = 0.7311723828315735\n",
      "13 = 0.7534000277519226\n",
      "14 = 7.336672306060791\n",
      "15 = 9.375669479370117\n",
      "16 = 0.13447938859462738\n",
      "17 = 7.336644649505615\n",
      "18 = 12.76115608215332\n",
      "epoch:17 step:16001 [D loss: 0.675917, acc: 55.47%] [G loss: 1.848917]\n",
      "epoch:17 step:16002 [D loss: 0.628426, acc: 64.84%] [G loss: 1.801137]\n",
      "epoch:17 step:16003 [D loss: 0.672195, acc: 60.16%] [G loss: 1.788405]\n",
      "epoch:17 step:16004 [D loss: 0.608680, acc: 64.06%] [G loss: 1.913207]\n",
      "epoch:17 step:16005 [D loss: 0.602804, acc: 67.97%] [G loss: 2.027993]\n",
      "epoch:17 step:16006 [D loss: 0.594608, acc: 73.44%] [G loss: 2.051318]\n",
      "epoch:17 step:16007 [D loss: 0.674166, acc: 59.38%] [G loss: 1.820178]\n",
      "epoch:17 step:16008 [D loss: 0.621934, acc: 64.06%] [G loss: 1.893230]\n",
      "epoch:17 step:16009 [D loss: 0.715762, acc: 53.12%] [G loss: 1.796627]\n",
      "epoch:17 step:16010 [D loss: 0.625218, acc: 65.62%] [G loss: 1.863058]\n",
      "epoch:17 step:16011 [D loss: 0.632781, acc: 60.94%] [G loss: 2.076201]\n",
      "epoch:17 step:16012 [D loss: 0.617147, acc: 70.31%] [G loss: 2.045165]\n",
      "epoch:17 step:16013 [D loss: 0.653028, acc: 60.94%] [G loss: 2.034849]\n",
      "epoch:17 step:16014 [D loss: 0.673514, acc: 62.50%] [G loss: 1.837194]\n",
      "epoch:17 step:16015 [D loss: 0.661615, acc: 64.84%] [G loss: 1.830493]\n",
      "epoch:17 step:16016 [D loss: 0.673090, acc: 64.84%] [G loss: 1.771530]\n",
      "epoch:17 step:16017 [D loss: 0.680954, acc: 59.38%] [G loss: 1.903934]\n",
      "epoch:17 step:16018 [D loss: 0.662499, acc: 60.16%] [G loss: 2.042032]\n",
      "epoch:17 step:16019 [D loss: 0.665620, acc: 59.38%] [G loss: 1.743932]\n",
      "epoch:17 step:16020 [D loss: 0.638943, acc: 58.59%] [G loss: 1.953810]\n",
      "epoch:17 step:16021 [D loss: 0.622019, acc: 66.41%] [G loss: 1.956538]\n",
      "epoch:17 step:16022 [D loss: 0.600054, acc: 64.84%] [G loss: 2.018389]\n",
      "epoch:17 step:16023 [D loss: 0.628316, acc: 61.72%] [G loss: 1.876760]\n",
      "epoch:17 step:16024 [D loss: 0.662995, acc: 58.59%] [G loss: 1.798700]\n",
      "epoch:17 step:16025 [D loss: 0.668824, acc: 61.72%] [G loss: 1.910743]\n",
      "epoch:17 step:16026 [D loss: 0.599476, acc: 65.62%] [G loss: 1.947633]\n",
      "epoch:17 step:16027 [D loss: 0.707110, acc: 55.47%] [G loss: 1.771784]\n",
      "epoch:17 step:16028 [D loss: 0.686990, acc: 56.25%] [G loss: 1.803630]\n",
      "epoch:17 step:16029 [D loss: 0.649968, acc: 63.28%] [G loss: 1.969635]\n",
      "epoch:17 step:16030 [D loss: 0.624066, acc: 64.84%] [G loss: 1.719894]\n",
      "epoch:17 step:16031 [D loss: 0.650872, acc: 58.59%] [G loss: 2.030439]\n",
      "epoch:17 step:16032 [D loss: 0.637471, acc: 64.84%] [G loss: 1.842129]\n",
      "epoch:17 step:16033 [D loss: 0.656115, acc: 60.94%] [G loss: 1.818883]\n",
      "epoch:17 step:16034 [D loss: 0.666375, acc: 57.81%] [G loss: 1.899447]\n",
      "epoch:17 step:16035 [D loss: 0.616125, acc: 66.41%] [G loss: 1.985596]\n",
      "epoch:17 step:16036 [D loss: 0.611753, acc: 63.28%] [G loss: 1.989321]\n",
      "epoch:17 step:16037 [D loss: 0.651530, acc: 63.28%] [G loss: 1.748985]\n",
      "epoch:17 step:16038 [D loss: 0.684379, acc: 60.16%] [G loss: 1.802344]\n",
      "epoch:17 step:16039 [D loss: 0.632334, acc: 67.19%] [G loss: 1.925688]\n",
      "epoch:17 step:16040 [D loss: 0.623051, acc: 58.59%] [G loss: 1.982191]\n",
      "epoch:17 step:16041 [D loss: 0.636973, acc: 66.41%] [G loss: 1.948214]\n",
      "epoch:17 step:16042 [D loss: 0.611591, acc: 67.19%] [G loss: 1.968929]\n",
      "epoch:17 step:16043 [D loss: 0.614160, acc: 63.28%] [G loss: 2.033752]\n",
      "epoch:17 step:16044 [D loss: 0.646386, acc: 62.50%] [G loss: 2.018059]\n",
      "epoch:17 step:16045 [D loss: 0.603830, acc: 63.28%] [G loss: 1.973053]\n",
      "epoch:17 step:16046 [D loss: 0.580297, acc: 71.09%] [G loss: 2.160323]\n",
      "epoch:17 step:16047 [D loss: 0.665686, acc: 63.28%] [G loss: 2.021274]\n",
      "epoch:17 step:16048 [D loss: 0.595395, acc: 69.53%] [G loss: 2.357643]\n",
      "epoch:17 step:16049 [D loss: 0.646321, acc: 65.62%] [G loss: 1.845004]\n",
      "epoch:17 step:16050 [D loss: 0.686341, acc: 56.25%] [G loss: 1.920449]\n",
      "epoch:17 step:16051 [D loss: 0.668784, acc: 61.72%] [G loss: 2.207122]\n",
      "epoch:17 step:16052 [D loss: 0.673089, acc: 58.59%] [G loss: 1.936090]\n",
      "epoch:17 step:16053 [D loss: 0.639682, acc: 60.94%] [G loss: 2.104025]\n",
      "epoch:17 step:16054 [D loss: 0.718059, acc: 51.56%] [G loss: 1.756080]\n",
      "epoch:17 step:16055 [D loss: 0.628991, acc: 62.50%] [G loss: 2.043371]\n",
      "epoch:17 step:16056 [D loss: 0.644777, acc: 57.03%] [G loss: 1.723146]\n",
      "epoch:17 step:16057 [D loss: 0.640731, acc: 64.06%] [G loss: 1.765521]\n",
      "epoch:17 step:16058 [D loss: 0.672469, acc: 56.25%] [G loss: 1.835239]\n",
      "epoch:17 step:16059 [D loss: 0.590242, acc: 68.75%] [G loss: 1.985139]\n",
      "epoch:17 step:16060 [D loss: 0.652019, acc: 59.38%] [G loss: 1.900766]\n",
      "epoch:17 step:16061 [D loss: 0.650243, acc: 60.94%] [G loss: 1.829720]\n",
      "epoch:17 step:16062 [D loss: 0.679154, acc: 53.91%] [G loss: 1.759528]\n",
      "epoch:17 step:16063 [D loss: 0.673676, acc: 60.94%] [G loss: 1.666307]\n",
      "epoch:17 step:16064 [D loss: 0.656129, acc: 59.38%] [G loss: 1.727319]\n",
      "epoch:17 step:16065 [D loss: 0.636723, acc: 61.72%] [G loss: 1.732937]\n",
      "epoch:17 step:16066 [D loss: 0.654313, acc: 66.41%] [G loss: 1.798656]\n",
      "epoch:17 step:16067 [D loss: 0.669819, acc: 61.72%] [G loss: 1.840641]\n",
      "epoch:17 step:16068 [D loss: 0.591185, acc: 74.22%] [G loss: 2.010090]\n",
      "epoch:17 step:16069 [D loss: 0.748835, acc: 50.00%] [G loss: 1.919458]\n",
      "epoch:17 step:16070 [D loss: 0.610102, acc: 67.19%] [G loss: 1.966956]\n",
      "epoch:17 step:16071 [D loss: 0.630011, acc: 60.94%] [G loss: 1.940975]\n",
      "epoch:17 step:16072 [D loss: 0.663894, acc: 57.81%] [G loss: 1.817525]\n",
      "epoch:17 step:16073 [D loss: 0.658446, acc: 57.03%] [G loss: 1.881381]\n",
      "epoch:17 step:16074 [D loss: 0.673611, acc: 60.16%] [G loss: 1.866165]\n",
      "epoch:17 step:16075 [D loss: 0.646562, acc: 55.47%] [G loss: 1.947617]\n",
      "epoch:17 step:16076 [D loss: 0.621636, acc: 64.06%] [G loss: 1.849849]\n",
      "epoch:17 step:16077 [D loss: 0.664976, acc: 60.16%] [G loss: 1.755612]\n",
      "epoch:17 step:16078 [D loss: 0.687088, acc: 55.47%] [G loss: 1.884944]\n",
      "epoch:17 step:16079 [D loss: 0.623088, acc: 65.62%] [G loss: 1.927494]\n",
      "epoch:17 step:16080 [D loss: 0.624065, acc: 67.97%] [G loss: 1.896630]\n",
      "epoch:17 step:16081 [D loss: 0.629108, acc: 65.62%] [G loss: 1.976755]\n",
      "epoch:17 step:16082 [D loss: 0.668964, acc: 64.84%] [G loss: 2.047124]\n",
      "epoch:17 step:16083 [D loss: 0.663966, acc: 60.16%] [G loss: 2.200970]\n",
      "epoch:17 step:16084 [D loss: 0.679259, acc: 58.59%] [G loss: 2.031806]\n",
      "epoch:17 step:16085 [D loss: 0.630620, acc: 64.06%] [G loss: 1.882749]\n",
      "epoch:17 step:16086 [D loss: 0.662066, acc: 62.50%] [G loss: 1.981017]\n",
      "epoch:17 step:16087 [D loss: 0.653877, acc: 59.38%] [G loss: 2.010591]\n",
      "epoch:17 step:16088 [D loss: 0.652104, acc: 60.94%] [G loss: 1.855561]\n",
      "epoch:17 step:16089 [D loss: 0.675453, acc: 58.59%] [G loss: 1.925595]\n",
      "epoch:17 step:16090 [D loss: 0.598572, acc: 72.66%] [G loss: 1.879482]\n",
      "epoch:17 step:16091 [D loss: 0.601071, acc: 67.19%] [G loss: 1.921832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16092 [D loss: 0.660578, acc: 59.38%] [G loss: 1.808730]\n",
      "epoch:17 step:16093 [D loss: 0.582782, acc: 68.75%] [G loss: 1.903006]\n",
      "epoch:17 step:16094 [D loss: 0.662501, acc: 58.59%] [G loss: 1.762187]\n",
      "epoch:17 step:16095 [D loss: 0.673704, acc: 61.72%] [G loss: 1.840940]\n",
      "epoch:17 step:16096 [D loss: 0.649474, acc: 59.38%] [G loss: 1.774400]\n",
      "epoch:17 step:16097 [D loss: 0.682831, acc: 60.16%] [G loss: 1.985426]\n",
      "epoch:17 step:16098 [D loss: 0.661033, acc: 59.38%] [G loss: 1.884448]\n",
      "epoch:17 step:16099 [D loss: 0.655615, acc: 64.84%] [G loss: 1.945967]\n",
      "epoch:17 step:16100 [D loss: 0.643907, acc: 60.16%] [G loss: 1.761250]\n",
      "epoch:17 step:16101 [D loss: 0.635053, acc: 75.00%] [G loss: 1.896051]\n",
      "epoch:17 step:16102 [D loss: 0.689571, acc: 53.91%] [G loss: 1.862370]\n",
      "epoch:17 step:16103 [D loss: 0.646662, acc: 61.72%] [G loss: 1.854951]\n",
      "epoch:17 step:16104 [D loss: 0.688599, acc: 57.03%] [G loss: 1.818272]\n",
      "epoch:17 step:16105 [D loss: 0.672445, acc: 53.12%] [G loss: 1.798182]\n",
      "epoch:17 step:16106 [D loss: 0.640220, acc: 60.16%] [G loss: 1.954569]\n",
      "epoch:17 step:16107 [D loss: 0.675899, acc: 55.47%] [G loss: 1.820654]\n",
      "epoch:17 step:16108 [D loss: 0.625415, acc: 59.38%] [G loss: 1.844504]\n",
      "epoch:17 step:16109 [D loss: 0.678526, acc: 56.25%] [G loss: 1.853208]\n",
      "epoch:17 step:16110 [D loss: 0.641889, acc: 61.72%] [G loss: 1.879424]\n",
      "epoch:17 step:16111 [D loss: 0.625819, acc: 62.50%] [G loss: 1.847586]\n",
      "epoch:17 step:16112 [D loss: 0.660052, acc: 59.38%] [G loss: 1.799260]\n",
      "epoch:17 step:16113 [D loss: 0.661368, acc: 59.38%] [G loss: 1.729747]\n",
      "epoch:17 step:16114 [D loss: 0.664283, acc: 61.72%] [G loss: 1.964686]\n",
      "epoch:17 step:16115 [D loss: 0.652860, acc: 64.06%] [G loss: 1.930115]\n",
      "epoch:17 step:16116 [D loss: 0.599383, acc: 71.09%] [G loss: 1.794009]\n",
      "epoch:17 step:16117 [D loss: 0.727468, acc: 53.12%] [G loss: 1.829876]\n",
      "epoch:17 step:16118 [D loss: 0.663043, acc: 59.38%] [G loss: 1.864064]\n",
      "epoch:17 step:16119 [D loss: 0.598867, acc: 69.53%] [G loss: 1.786670]\n",
      "epoch:17 step:16120 [D loss: 0.638167, acc: 60.16%] [G loss: 1.909336]\n",
      "epoch:17 step:16121 [D loss: 0.634541, acc: 61.72%] [G loss: 1.890333]\n",
      "epoch:17 step:16122 [D loss: 0.623301, acc: 63.28%] [G loss: 1.956261]\n",
      "epoch:17 step:16123 [D loss: 0.625748, acc: 68.75%] [G loss: 2.054393]\n",
      "epoch:17 step:16124 [D loss: 0.597190, acc: 67.19%] [G loss: 1.968055]\n",
      "epoch:17 step:16125 [D loss: 0.651478, acc: 57.03%] [G loss: 1.910848]\n",
      "epoch:17 step:16126 [D loss: 0.565060, acc: 72.66%] [G loss: 2.008597]\n",
      "epoch:17 step:16127 [D loss: 0.634274, acc: 63.28%] [G loss: 2.097836]\n",
      "epoch:17 step:16128 [D loss: 0.623677, acc: 65.62%] [G loss: 2.039048]\n",
      "epoch:17 step:16129 [D loss: 0.741288, acc: 51.56%] [G loss: 1.753945]\n",
      "epoch:17 step:16130 [D loss: 0.609429, acc: 68.75%] [G loss: 1.857269]\n",
      "epoch:17 step:16131 [D loss: 0.687819, acc: 53.91%] [G loss: 1.873772]\n",
      "epoch:17 step:16132 [D loss: 0.685514, acc: 57.81%] [G loss: 1.852714]\n",
      "epoch:17 step:16133 [D loss: 0.664232, acc: 59.38%] [G loss: 1.958818]\n",
      "epoch:17 step:16134 [D loss: 0.705299, acc: 56.25%] [G loss: 1.709791]\n",
      "epoch:17 step:16135 [D loss: 0.626464, acc: 61.72%] [G loss: 1.924878]\n",
      "epoch:17 step:16136 [D loss: 0.580390, acc: 71.09%] [G loss: 2.059650]\n",
      "epoch:17 step:16137 [D loss: 0.638751, acc: 64.06%] [G loss: 2.058308]\n",
      "epoch:17 step:16138 [D loss: 0.602623, acc: 67.97%] [G loss: 2.191293]\n",
      "epoch:17 step:16139 [D loss: 0.668687, acc: 56.25%] [G loss: 1.781516]\n",
      "epoch:17 step:16140 [D loss: 0.650919, acc: 65.62%] [G loss: 1.850033]\n",
      "epoch:17 step:16141 [D loss: 0.713556, acc: 54.69%] [G loss: 1.757934]\n",
      "epoch:17 step:16142 [D loss: 0.631001, acc: 60.16%] [G loss: 1.852463]\n",
      "epoch:17 step:16143 [D loss: 0.648263, acc: 64.06%] [G loss: 1.780459]\n",
      "epoch:17 step:16144 [D loss: 0.677029, acc: 63.28%] [G loss: 1.804758]\n",
      "epoch:17 step:16145 [D loss: 0.626186, acc: 62.50%] [G loss: 1.922055]\n",
      "epoch:17 step:16146 [D loss: 0.632421, acc: 64.84%] [G loss: 1.865253]\n",
      "epoch:17 step:16147 [D loss: 0.621972, acc: 67.97%] [G loss: 2.012680]\n",
      "epoch:17 step:16148 [D loss: 0.543798, acc: 75.78%] [G loss: 2.286544]\n",
      "epoch:17 step:16149 [D loss: 0.669508, acc: 58.59%] [G loss: 1.812952]\n",
      "epoch:17 step:16150 [D loss: 0.645062, acc: 61.72%] [G loss: 1.928559]\n",
      "epoch:17 step:16151 [D loss: 0.659707, acc: 57.03%] [G loss: 1.874151]\n",
      "epoch:17 step:16152 [D loss: 0.611674, acc: 68.75%] [G loss: 1.998911]\n",
      "epoch:17 step:16153 [D loss: 0.667301, acc: 62.50%] [G loss: 1.907028]\n",
      "epoch:17 step:16154 [D loss: 0.636078, acc: 68.75%] [G loss: 1.914510]\n",
      "epoch:17 step:16155 [D loss: 0.665182, acc: 56.25%] [G loss: 1.782691]\n",
      "epoch:17 step:16156 [D loss: 0.664603, acc: 55.47%] [G loss: 1.772637]\n",
      "epoch:17 step:16157 [D loss: 0.671073, acc: 57.81%] [G loss: 1.702574]\n",
      "epoch:17 step:16158 [D loss: 0.584731, acc: 71.88%] [G loss: 2.028295]\n",
      "epoch:17 step:16159 [D loss: 0.604184, acc: 72.66%] [G loss: 2.082963]\n",
      "epoch:17 step:16160 [D loss: 0.616521, acc: 67.19%] [G loss: 2.169842]\n",
      "epoch:17 step:16161 [D loss: 0.574592, acc: 69.53%] [G loss: 2.095052]\n",
      "epoch:17 step:16162 [D loss: 0.636238, acc: 62.50%] [G loss: 1.984454]\n",
      "epoch:17 step:16163 [D loss: 0.693757, acc: 53.12%] [G loss: 1.888781]\n",
      "epoch:17 step:16164 [D loss: 0.645115, acc: 67.97%] [G loss: 1.852119]\n",
      "epoch:17 step:16165 [D loss: 0.664159, acc: 57.81%] [G loss: 1.943008]\n",
      "epoch:17 step:16166 [D loss: 0.658021, acc: 57.81%] [G loss: 1.883356]\n",
      "epoch:17 step:16167 [D loss: 0.613058, acc: 68.75%] [G loss: 1.915568]\n",
      "epoch:17 step:16168 [D loss: 0.644970, acc: 61.72%] [G loss: 1.815840]\n",
      "epoch:17 step:16169 [D loss: 0.661893, acc: 60.16%] [G loss: 1.884660]\n",
      "epoch:17 step:16170 [D loss: 0.642071, acc: 64.06%] [G loss: 1.909224]\n",
      "epoch:17 step:16171 [D loss: 0.686986, acc: 57.81%] [G loss: 1.968426]\n",
      "epoch:17 step:16172 [D loss: 0.599951, acc: 69.53%] [G loss: 1.901990]\n",
      "epoch:17 step:16173 [D loss: 0.624740, acc: 61.72%] [G loss: 1.794624]\n",
      "epoch:17 step:16174 [D loss: 0.604854, acc: 69.53%] [G loss: 2.015339]\n",
      "epoch:17 step:16175 [D loss: 0.634727, acc: 63.28%] [G loss: 2.041190]\n",
      "epoch:17 step:16176 [D loss: 0.614736, acc: 67.97%] [G loss: 1.966327]\n",
      "epoch:17 step:16177 [D loss: 0.585066, acc: 71.09%] [G loss: 2.309325]\n",
      "epoch:17 step:16178 [D loss: 0.699302, acc: 54.69%] [G loss: 1.829054]\n",
      "epoch:17 step:16179 [D loss: 0.692871, acc: 58.59%] [G loss: 1.730577]\n",
      "epoch:17 step:16180 [D loss: 0.638555, acc: 66.41%] [G loss: 1.766296]\n",
      "epoch:17 step:16181 [D loss: 0.660700, acc: 59.38%] [G loss: 1.742286]\n",
      "epoch:17 step:16182 [D loss: 0.615537, acc: 67.97%] [G loss: 2.089235]\n",
      "epoch:17 step:16183 [D loss: 0.696317, acc: 53.12%] [G loss: 1.889342]\n",
      "epoch:17 step:16184 [D loss: 0.589084, acc: 68.75%] [G loss: 1.812161]\n",
      "epoch:17 step:16185 [D loss: 0.675815, acc: 63.28%] [G loss: 1.938136]\n",
      "epoch:17 step:16186 [D loss: 0.654131, acc: 65.62%] [G loss: 1.810234]\n",
      "epoch:17 step:16187 [D loss: 0.651838, acc: 62.50%] [G loss: 1.893137]\n",
      "epoch:17 step:16188 [D loss: 0.625313, acc: 64.06%] [G loss: 1.897622]\n",
      "epoch:17 step:16189 [D loss: 0.651966, acc: 63.28%] [G loss: 1.833223]\n",
      "epoch:17 step:16190 [D loss: 0.597979, acc: 75.00%] [G loss: 1.990629]\n",
      "epoch:17 step:16191 [D loss: 0.649027, acc: 62.50%] [G loss: 1.958808]\n",
      "epoch:17 step:16192 [D loss: 0.654959, acc: 60.16%] [G loss: 1.882086]\n",
      "epoch:17 step:16193 [D loss: 0.597077, acc: 64.84%] [G loss: 2.026945]\n",
      "epoch:17 step:16194 [D loss: 0.691192, acc: 56.25%] [G loss: 1.784214]\n",
      "epoch:17 step:16195 [D loss: 0.656975, acc: 63.28%] [G loss: 1.813911]\n",
      "epoch:17 step:16196 [D loss: 0.651967, acc: 64.84%] [G loss: 1.833598]\n",
      "epoch:17 step:16197 [D loss: 0.661277, acc: 55.47%] [G loss: 1.869926]\n",
      "epoch:17 step:16198 [D loss: 0.619950, acc: 63.28%] [G loss: 2.075376]\n",
      "epoch:17 step:16199 [D loss: 0.597014, acc: 70.31%] [G loss: 2.037067]\n",
      "epoch:17 step:16200 [D loss: 0.606826, acc: 69.53%] [G loss: 1.973626]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.139998\n",
      "FID: 17.100492\n",
      "0 = 13.069471954822573\n",
      "1 = 0.09732610208831483\n",
      "2 = 0.892300009727478\n",
      "3 = 0.9160000085830688\n",
      "4 = 0.8686000108718872\n",
      "5 = 0.874546468257904\n",
      "6 = 0.9160000085830688\n",
      "7 = 6.776380746614936\n",
      "8 = 0.08580561835471703\n",
      "9 = 0.7278000116348267\n",
      "10 = 0.7390000224113464\n",
      "11 = 0.7166000008583069\n",
      "12 = 0.7228090763092041\n",
      "13 = 0.7390000224113464\n",
      "14 = 7.140025615692139\n",
      "15 = 9.360729217529297\n",
      "16 = 0.13454537093639374\n",
      "17 = 7.139997959136963\n",
      "18 = 17.100492477416992\n",
      "epoch:17 step:16201 [D loss: 0.654051, acc: 65.62%] [G loss: 1.942780]\n",
      "epoch:17 step:16202 [D loss: 0.651433, acc: 64.06%] [G loss: 1.841779]\n",
      "epoch:17 step:16203 [D loss: 0.658819, acc: 66.41%] [G loss: 1.961843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16204 [D loss: 0.629670, acc: 67.19%] [G loss: 2.126653]\n",
      "epoch:17 step:16205 [D loss: 0.624120, acc: 60.94%] [G loss: 2.084241]\n",
      "epoch:17 step:16206 [D loss: 0.678380, acc: 54.69%] [G loss: 1.951630]\n",
      "epoch:17 step:16207 [D loss: 0.615411, acc: 70.31%] [G loss: 1.882864]\n",
      "epoch:17 step:16208 [D loss: 0.579216, acc: 69.53%] [G loss: 1.975524]\n",
      "epoch:17 step:16209 [D loss: 0.634395, acc: 64.84%] [G loss: 2.043316]\n",
      "epoch:17 step:16210 [D loss: 0.624613, acc: 67.19%] [G loss: 1.956879]\n",
      "epoch:17 step:16211 [D loss: 0.680086, acc: 57.03%] [G loss: 1.835980]\n",
      "epoch:17 step:16212 [D loss: 0.656654, acc: 59.38%] [G loss: 1.880870]\n",
      "epoch:17 step:16213 [D loss: 0.687699, acc: 58.59%] [G loss: 1.879710]\n",
      "epoch:17 step:16214 [D loss: 0.657343, acc: 61.72%] [G loss: 1.825628]\n",
      "epoch:17 step:16215 [D loss: 0.605877, acc: 69.53%] [G loss: 2.037253]\n",
      "epoch:17 step:16216 [D loss: 0.648833, acc: 60.94%] [G loss: 1.771699]\n",
      "epoch:17 step:16217 [D loss: 0.660346, acc: 59.38%] [G loss: 1.906889]\n",
      "epoch:17 step:16218 [D loss: 0.656093, acc: 66.41%] [G loss: 1.936339]\n",
      "epoch:17 step:16219 [D loss: 0.594248, acc: 67.97%] [G loss: 1.960370]\n",
      "epoch:17 step:16220 [D loss: 0.667125, acc: 57.81%] [G loss: 1.838035]\n",
      "epoch:17 step:16221 [D loss: 0.623166, acc: 61.72%] [G loss: 1.886985]\n",
      "epoch:17 step:16222 [D loss: 0.604386, acc: 66.41%] [G loss: 1.931647]\n",
      "epoch:17 step:16223 [D loss: 0.626586, acc: 65.62%] [G loss: 1.907308]\n",
      "epoch:17 step:16224 [D loss: 0.758249, acc: 50.78%] [G loss: 1.970956]\n",
      "epoch:17 step:16225 [D loss: 0.605205, acc: 65.62%] [G loss: 2.037602]\n",
      "epoch:17 step:16226 [D loss: 0.611173, acc: 66.41%] [G loss: 1.904091]\n",
      "epoch:17 step:16227 [D loss: 0.667282, acc: 64.84%] [G loss: 1.933901]\n",
      "epoch:17 step:16228 [D loss: 0.632168, acc: 66.41%] [G loss: 2.037441]\n",
      "epoch:17 step:16229 [D loss: 0.593027, acc: 75.00%] [G loss: 1.938502]\n",
      "epoch:17 step:16230 [D loss: 0.630388, acc: 59.38%] [G loss: 1.881280]\n",
      "epoch:17 step:16231 [D loss: 0.598708, acc: 70.31%] [G loss: 1.932040]\n",
      "epoch:17 step:16232 [D loss: 0.649495, acc: 57.81%] [G loss: 1.974191]\n",
      "epoch:17 step:16233 [D loss: 0.714447, acc: 53.91%] [G loss: 1.733102]\n",
      "epoch:17 step:16234 [D loss: 0.664565, acc: 63.28%] [G loss: 1.971454]\n",
      "epoch:17 step:16235 [D loss: 0.657504, acc: 61.72%] [G loss: 1.857582]\n",
      "epoch:17 step:16236 [D loss: 0.646692, acc: 68.75%] [G loss: 1.935108]\n",
      "epoch:17 step:16237 [D loss: 0.683737, acc: 57.03%] [G loss: 1.833049]\n",
      "epoch:17 step:16238 [D loss: 0.637259, acc: 62.50%] [G loss: 2.033997]\n",
      "epoch:17 step:16239 [D loss: 0.665286, acc: 63.28%] [G loss: 1.814955]\n",
      "epoch:17 step:16240 [D loss: 0.627491, acc: 66.41%] [G loss: 1.993846]\n",
      "epoch:17 step:16241 [D loss: 0.555879, acc: 73.44%] [G loss: 2.119160]\n",
      "epoch:17 step:16242 [D loss: 0.608311, acc: 67.19%] [G loss: 2.078793]\n",
      "epoch:17 step:16243 [D loss: 0.586110, acc: 69.53%] [G loss: 2.248315]\n",
      "epoch:17 step:16244 [D loss: 0.621481, acc: 66.41%] [G loss: 2.217073]\n",
      "epoch:17 step:16245 [D loss: 0.742433, acc: 48.44%] [G loss: 1.834374]\n",
      "epoch:17 step:16246 [D loss: 0.673638, acc: 60.16%] [G loss: 1.801088]\n",
      "epoch:17 step:16247 [D loss: 0.641223, acc: 67.19%] [G loss: 1.919313]\n",
      "epoch:17 step:16248 [D loss: 0.654108, acc: 62.50%] [G loss: 1.777857]\n",
      "epoch:17 step:16249 [D loss: 0.637496, acc: 61.72%] [G loss: 1.840095]\n",
      "epoch:17 step:16250 [D loss: 0.653617, acc: 60.16%] [G loss: 1.939513]\n",
      "epoch:17 step:16251 [D loss: 0.628922, acc: 65.62%] [G loss: 1.923525]\n",
      "epoch:17 step:16252 [D loss: 0.685992, acc: 57.03%] [G loss: 1.721043]\n",
      "epoch:17 step:16253 [D loss: 0.681650, acc: 54.69%] [G loss: 1.720655]\n",
      "epoch:17 step:16254 [D loss: 0.686401, acc: 58.59%] [G loss: 1.756306]\n",
      "epoch:17 step:16255 [D loss: 0.625722, acc: 65.62%] [G loss: 1.804716]\n",
      "epoch:17 step:16256 [D loss: 0.647855, acc: 66.41%] [G loss: 1.766488]\n",
      "epoch:17 step:16257 [D loss: 0.623406, acc: 68.75%] [G loss: 1.907484]\n",
      "epoch:17 step:16258 [D loss: 0.686402, acc: 61.72%] [G loss: 1.759184]\n",
      "epoch:17 step:16259 [D loss: 0.655073, acc: 60.94%] [G loss: 2.034345]\n",
      "epoch:17 step:16260 [D loss: 0.637058, acc: 61.72%] [G loss: 1.910153]\n",
      "epoch:17 step:16261 [D loss: 0.586551, acc: 65.62%] [G loss: 1.997285]\n",
      "epoch:17 step:16262 [D loss: 0.644483, acc: 61.72%] [G loss: 1.984283]\n",
      "epoch:17 step:16263 [D loss: 0.608352, acc: 69.53%] [G loss: 1.942827]\n",
      "epoch:17 step:16264 [D loss: 0.613662, acc: 68.75%] [G loss: 2.064162]\n",
      "epoch:17 step:16265 [D loss: 0.710694, acc: 56.25%] [G loss: 2.047968]\n",
      "epoch:17 step:16266 [D loss: 0.631711, acc: 64.84%] [G loss: 1.959627]\n",
      "epoch:17 step:16267 [D loss: 0.595917, acc: 64.84%] [G loss: 2.035043]\n",
      "epoch:17 step:16268 [D loss: 0.650732, acc: 60.94%] [G loss: 2.049002]\n",
      "epoch:17 step:16269 [D loss: 0.620766, acc: 68.75%] [G loss: 1.934732]\n",
      "epoch:17 step:16270 [D loss: 0.700796, acc: 54.69%] [G loss: 1.868309]\n",
      "epoch:17 step:16271 [D loss: 0.690743, acc: 52.34%] [G loss: 1.730963]\n",
      "epoch:17 step:16272 [D loss: 0.627774, acc: 64.84%] [G loss: 1.905208]\n",
      "epoch:17 step:16273 [D loss: 0.696609, acc: 54.69%] [G loss: 1.956978]\n",
      "epoch:17 step:16274 [D loss: 0.592875, acc: 67.97%] [G loss: 2.368111]\n",
      "epoch:17 step:16275 [D loss: 0.616218, acc: 68.75%] [G loss: 2.199464]\n",
      "epoch:17 step:16276 [D loss: 0.587580, acc: 70.31%] [G loss: 2.179019]\n",
      "epoch:17 step:16277 [D loss: 0.682871, acc: 60.16%] [G loss: 1.846111]\n",
      "epoch:17 step:16278 [D loss: 0.708103, acc: 52.34%] [G loss: 1.691183]\n",
      "epoch:17 step:16279 [D loss: 0.623789, acc: 66.41%] [G loss: 1.910671]\n",
      "epoch:17 step:16280 [D loss: 0.635558, acc: 67.19%] [G loss: 1.819567]\n",
      "epoch:17 step:16281 [D loss: 0.680415, acc: 53.12%] [G loss: 1.813277]\n",
      "epoch:17 step:16282 [D loss: 0.626662, acc: 62.50%] [G loss: 1.799613]\n",
      "epoch:17 step:16283 [D loss: 0.633957, acc: 64.06%] [G loss: 2.080711]\n",
      "epoch:17 step:16284 [D loss: 0.713980, acc: 46.88%] [G loss: 1.688831]\n",
      "epoch:17 step:16285 [D loss: 0.649757, acc: 62.50%] [G loss: 1.848085]\n",
      "epoch:17 step:16286 [D loss: 0.630923, acc: 64.06%] [G loss: 1.796020]\n",
      "epoch:17 step:16287 [D loss: 0.578247, acc: 72.66%] [G loss: 2.109601]\n",
      "epoch:17 step:16288 [D loss: 0.592357, acc: 69.53%] [G loss: 2.026547]\n",
      "epoch:17 step:16289 [D loss: 0.648909, acc: 61.72%] [G loss: 1.874083]\n",
      "epoch:17 step:16290 [D loss: 0.679819, acc: 58.59%] [G loss: 1.894894]\n",
      "epoch:17 step:16291 [D loss: 0.668215, acc: 57.81%] [G loss: 1.771576]\n",
      "epoch:17 step:16292 [D loss: 0.633080, acc: 60.94%] [G loss: 1.855449]\n",
      "epoch:17 step:16293 [D loss: 0.639667, acc: 63.28%] [G loss: 1.914054]\n",
      "epoch:17 step:16294 [D loss: 0.651252, acc: 64.84%] [G loss: 1.758366]\n",
      "epoch:17 step:16295 [D loss: 0.652531, acc: 58.59%] [G loss: 1.897410]\n",
      "epoch:17 step:16296 [D loss: 0.622760, acc: 62.50%] [G loss: 2.054457]\n",
      "epoch:17 step:16297 [D loss: 0.626180, acc: 66.41%] [G loss: 1.747161]\n",
      "epoch:17 step:16298 [D loss: 0.602957, acc: 71.09%] [G loss: 1.828926]\n",
      "epoch:17 step:16299 [D loss: 0.611499, acc: 66.41%] [G loss: 2.142326]\n",
      "epoch:17 step:16300 [D loss: 0.585998, acc: 70.31%] [G loss: 2.018970]\n",
      "epoch:17 step:16301 [D loss: 0.708537, acc: 57.81%] [G loss: 1.851570]\n",
      "epoch:17 step:16302 [D loss: 0.720299, acc: 51.56%] [G loss: 1.723079]\n",
      "epoch:17 step:16303 [D loss: 0.632551, acc: 68.75%] [G loss: 1.846102]\n",
      "epoch:17 step:16304 [D loss: 0.678484, acc: 56.25%] [G loss: 1.813970]\n",
      "epoch:17 step:16305 [D loss: 0.642321, acc: 62.50%] [G loss: 1.865605]\n",
      "epoch:17 step:16306 [D loss: 0.733890, acc: 50.78%] [G loss: 1.823022]\n",
      "epoch:17 step:16307 [D loss: 0.647396, acc: 62.50%] [G loss: 1.865388]\n",
      "epoch:17 step:16308 [D loss: 0.704799, acc: 57.81%] [G loss: 1.919250]\n",
      "epoch:17 step:16309 [D loss: 0.675373, acc: 61.72%] [G loss: 1.859485]\n",
      "epoch:17 step:16310 [D loss: 0.604523, acc: 72.66%] [G loss: 1.994947]\n",
      "epoch:17 step:16311 [D loss: 0.582416, acc: 73.44%] [G loss: 1.993665]\n",
      "epoch:17 step:16312 [D loss: 0.717099, acc: 54.69%] [G loss: 1.922397]\n",
      "epoch:17 step:16313 [D loss: 0.647545, acc: 62.50%] [G loss: 1.903263]\n",
      "epoch:17 step:16314 [D loss: 0.617339, acc: 67.19%] [G loss: 1.903817]\n",
      "epoch:17 step:16315 [D loss: 0.677006, acc: 57.03%] [G loss: 1.737444]\n",
      "epoch:17 step:16316 [D loss: 0.625793, acc: 65.62%] [G loss: 1.821558]\n",
      "epoch:17 step:16317 [D loss: 0.666502, acc: 57.81%] [G loss: 1.847478]\n",
      "epoch:17 step:16318 [D loss: 0.651014, acc: 64.06%] [G loss: 1.893821]\n",
      "epoch:17 step:16319 [D loss: 0.672863, acc: 62.50%] [G loss: 1.782135]\n",
      "epoch:17 step:16320 [D loss: 0.693088, acc: 60.94%] [G loss: 1.853760]\n",
      "epoch:17 step:16321 [D loss: 0.638048, acc: 69.53%] [G loss: 1.883671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16322 [D loss: 0.611415, acc: 67.97%] [G loss: 1.852045]\n",
      "epoch:17 step:16323 [D loss: 0.673483, acc: 58.59%] [G loss: 1.928966]\n",
      "epoch:17 step:16324 [D loss: 0.686914, acc: 52.34%] [G loss: 1.894565]\n",
      "epoch:17 step:16325 [D loss: 0.677369, acc: 53.12%] [G loss: 1.778689]\n",
      "epoch:17 step:16326 [D loss: 0.671661, acc: 62.50%] [G loss: 1.764965]\n",
      "epoch:17 step:16327 [D loss: 0.627648, acc: 66.41%] [G loss: 1.982386]\n",
      "epoch:17 step:16328 [D loss: 0.638340, acc: 64.06%] [G loss: 1.914850]\n",
      "epoch:17 step:16329 [D loss: 0.706207, acc: 54.69%] [G loss: 1.935186]\n",
      "epoch:17 step:16330 [D loss: 0.643352, acc: 61.72%] [G loss: 1.877500]\n",
      "epoch:17 step:16331 [D loss: 0.592229, acc: 69.53%] [G loss: 1.888666]\n",
      "epoch:17 step:16332 [D loss: 0.626695, acc: 63.28%] [G loss: 1.882916]\n",
      "epoch:17 step:16333 [D loss: 0.724177, acc: 57.03%] [G loss: 1.903570]\n",
      "epoch:17 step:16334 [D loss: 0.580661, acc: 67.97%] [G loss: 1.987636]\n",
      "epoch:17 step:16335 [D loss: 0.625108, acc: 60.94%] [G loss: 1.958662]\n",
      "epoch:17 step:16336 [D loss: 0.589996, acc: 69.53%] [G loss: 1.838801]\n",
      "epoch:17 step:16337 [D loss: 0.650458, acc: 62.50%] [G loss: 1.853918]\n",
      "epoch:17 step:16338 [D loss: 0.635320, acc: 65.62%] [G loss: 1.801738]\n",
      "epoch:17 step:16339 [D loss: 0.648919, acc: 60.16%] [G loss: 1.950600]\n",
      "epoch:17 step:16340 [D loss: 0.621332, acc: 64.84%] [G loss: 1.834693]\n",
      "epoch:17 step:16341 [D loss: 0.637622, acc: 63.28%] [G loss: 1.823175]\n",
      "epoch:17 step:16342 [D loss: 0.626764, acc: 66.41%] [G loss: 2.002571]\n",
      "epoch:17 step:16343 [D loss: 0.677496, acc: 63.28%] [G loss: 2.053084]\n",
      "epoch:17 step:16344 [D loss: 0.675982, acc: 56.25%] [G loss: 2.007128]\n",
      "epoch:17 step:16345 [D loss: 0.651388, acc: 64.06%] [G loss: 1.895600]\n",
      "epoch:17 step:16346 [D loss: 0.607195, acc: 68.75%] [G loss: 1.958007]\n",
      "epoch:17 step:16347 [D loss: 0.636240, acc: 65.62%] [G loss: 1.896122]\n",
      "epoch:17 step:16348 [D loss: 0.661752, acc: 61.72%] [G loss: 1.850199]\n",
      "epoch:17 step:16349 [D loss: 0.665924, acc: 62.50%] [G loss: 1.916097]\n",
      "epoch:17 step:16350 [D loss: 0.642612, acc: 64.06%] [G loss: 1.859358]\n",
      "epoch:17 step:16351 [D loss: 0.661183, acc: 57.81%] [G loss: 1.927047]\n",
      "epoch:17 step:16352 [D loss: 0.664818, acc: 60.16%] [G loss: 1.852539]\n",
      "epoch:17 step:16353 [D loss: 0.606702, acc: 69.53%] [G loss: 1.867284]\n",
      "epoch:17 step:16354 [D loss: 0.619090, acc: 63.28%] [G loss: 1.944120]\n",
      "epoch:17 step:16355 [D loss: 0.631858, acc: 67.19%] [G loss: 1.920497]\n",
      "epoch:17 step:16356 [D loss: 0.627770, acc: 67.19%] [G loss: 2.026932]\n",
      "epoch:17 step:16357 [D loss: 0.569523, acc: 72.66%] [G loss: 2.190948]\n",
      "epoch:17 step:16358 [D loss: 0.554256, acc: 74.22%] [G loss: 2.215993]\n",
      "epoch:17 step:16359 [D loss: 0.621641, acc: 65.62%] [G loss: 2.087132]\n",
      "epoch:17 step:16360 [D loss: 0.617542, acc: 66.41%] [G loss: 1.996041]\n",
      "epoch:17 step:16361 [D loss: 0.727561, acc: 55.47%] [G loss: 1.885431]\n",
      "epoch:17 step:16362 [D loss: 0.648495, acc: 59.38%] [G loss: 1.994828]\n",
      "epoch:17 step:16363 [D loss: 0.670438, acc: 64.06%] [G loss: 1.871349]\n",
      "epoch:17 step:16364 [D loss: 0.625947, acc: 67.97%] [G loss: 1.966118]\n",
      "epoch:17 step:16365 [D loss: 0.690547, acc: 60.16%] [G loss: 1.963456]\n",
      "epoch:17 step:16366 [D loss: 0.699506, acc: 53.91%] [G loss: 1.838303]\n",
      "epoch:17 step:16367 [D loss: 0.664174, acc: 62.50%] [G loss: 1.875251]\n",
      "epoch:17 step:16368 [D loss: 0.655365, acc: 64.06%] [G loss: 1.813427]\n",
      "epoch:17 step:16369 [D loss: 0.652406, acc: 65.62%] [G loss: 1.750467]\n",
      "epoch:17 step:16370 [D loss: 0.651235, acc: 58.59%] [G loss: 1.930254]\n",
      "epoch:17 step:16371 [D loss: 0.671428, acc: 64.84%] [G loss: 1.931739]\n",
      "epoch:17 step:16372 [D loss: 0.724044, acc: 52.34%] [G loss: 1.818654]\n",
      "epoch:17 step:16373 [D loss: 0.659383, acc: 58.59%] [G loss: 1.744502]\n",
      "epoch:17 step:16374 [D loss: 0.650866, acc: 61.72%] [G loss: 1.813096]\n",
      "epoch:17 step:16375 [D loss: 0.620960, acc: 67.97%] [G loss: 1.758138]\n",
      "epoch:17 step:16376 [D loss: 0.642164, acc: 66.41%] [G loss: 1.867825]\n",
      "epoch:17 step:16377 [D loss: 0.685565, acc: 59.38%] [G loss: 1.827608]\n",
      "epoch:17 step:16378 [D loss: 0.624954, acc: 64.06%] [G loss: 1.959882]\n",
      "epoch:17 step:16379 [D loss: 0.656027, acc: 58.59%] [G loss: 1.905622]\n",
      "epoch:17 step:16380 [D loss: 0.702759, acc: 60.94%] [G loss: 1.848109]\n",
      "epoch:17 step:16381 [D loss: 0.644665, acc: 64.84%] [G loss: 1.819947]\n",
      "epoch:17 step:16382 [D loss: 0.694885, acc: 57.81%] [G loss: 1.859188]\n",
      "epoch:17 step:16383 [D loss: 0.675587, acc: 59.38%] [G loss: 1.814173]\n",
      "epoch:17 step:16384 [D loss: 0.685688, acc: 53.12%] [G loss: 1.917058]\n",
      "epoch:17 step:16385 [D loss: 0.627625, acc: 61.72%] [G loss: 1.919395]\n",
      "epoch:17 step:16386 [D loss: 0.641537, acc: 60.94%] [G loss: 1.911360]\n",
      "epoch:17 step:16387 [D loss: 0.653946, acc: 62.50%] [G loss: 1.870649]\n",
      "epoch:17 step:16388 [D loss: 0.642933, acc: 63.28%] [G loss: 1.933891]\n",
      "epoch:17 step:16389 [D loss: 0.681860, acc: 56.25%] [G loss: 1.813657]\n",
      "epoch:17 step:16390 [D loss: 0.662535, acc: 56.25%] [G loss: 1.781704]\n",
      "epoch:17 step:16391 [D loss: 0.615349, acc: 63.28%] [G loss: 1.760767]\n",
      "epoch:17 step:16392 [D loss: 0.638103, acc: 67.97%] [G loss: 1.902456]\n",
      "epoch:17 step:16393 [D loss: 0.643602, acc: 64.84%] [G loss: 1.739355]\n",
      "epoch:17 step:16394 [D loss: 0.672924, acc: 57.81%] [G loss: 1.805861]\n",
      "epoch:17 step:16395 [D loss: 0.645131, acc: 65.62%] [G loss: 1.794863]\n",
      "epoch:17 step:16396 [D loss: 0.669848, acc: 60.16%] [G loss: 1.939853]\n",
      "epoch:17 step:16397 [D loss: 0.670037, acc: 60.16%] [G loss: 1.933979]\n",
      "epoch:17 step:16398 [D loss: 0.636654, acc: 59.38%] [G loss: 1.952928]\n",
      "epoch:17 step:16399 [D loss: 0.586766, acc: 66.41%] [G loss: 2.041340]\n",
      "epoch:17 step:16400 [D loss: 0.612096, acc: 64.06%] [G loss: 2.296038]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.307380\n",
      "FID: 12.223987\n",
      "0 = 12.703599072074903\n",
      "1 = 0.08767130295807017\n",
      "2 = 0.8784000277519226\n",
      "3 = 0.8934000134468079\n",
      "4 = 0.8633999824523926\n",
      "5 = 0.8673786520957947\n",
      "6 = 0.8934000134468079\n",
      "7 = 6.351207057046889\n",
      "8 = 0.06463320457572894\n",
      "9 = 0.7203999757766724\n",
      "10 = 0.7333999872207642\n",
      "11 = 0.7074000239372253\n",
      "12 = 0.7148148417472839\n",
      "13 = 0.7333999872207642\n",
      "14 = 7.307412147521973\n",
      "15 = 9.493528366088867\n",
      "16 = 0.10543468594551086\n",
      "17 = 7.307380199432373\n",
      "18 = 12.223986625671387\n",
      "epoch:17 step:16401 [D loss: 0.666153, acc: 60.94%] [G loss: 1.998814]\n",
      "epoch:17 step:16402 [D loss: 0.722902, acc: 52.34%] [G loss: 1.716472]\n",
      "epoch:17 step:16403 [D loss: 0.708436, acc: 60.16%] [G loss: 1.992546]\n",
      "epoch:17 step:16404 [D loss: 0.645797, acc: 60.94%] [G loss: 1.888472]\n",
      "epoch:17 step:16405 [D loss: 0.713657, acc: 50.00%] [G loss: 1.789631]\n",
      "epoch:17 step:16406 [D loss: 0.688448, acc: 56.25%] [G loss: 1.744013]\n",
      "epoch:17 step:16407 [D loss: 0.682751, acc: 57.03%] [G loss: 1.690837]\n",
      "epoch:17 step:16408 [D loss: 0.639871, acc: 62.50%] [G loss: 1.973939]\n",
      "epoch:17 step:16409 [D loss: 0.620039, acc: 66.41%] [G loss: 1.972553]\n",
      "epoch:17 step:16410 [D loss: 0.595838, acc: 71.09%] [G loss: 2.068795]\n",
      "epoch:17 step:16411 [D loss: 0.697024, acc: 57.81%] [G loss: 1.758364]\n",
      "epoch:17 step:16412 [D loss: 0.680198, acc: 57.03%] [G loss: 1.837122]\n",
      "epoch:17 step:16413 [D loss: 0.660010, acc: 61.72%] [G loss: 1.911768]\n",
      "epoch:17 step:16414 [D loss: 0.681558, acc: 60.16%] [G loss: 1.828831]\n",
      "epoch:17 step:16415 [D loss: 0.660507, acc: 56.25%] [G loss: 1.878230]\n",
      "epoch:17 step:16416 [D loss: 0.618735, acc: 68.75%] [G loss: 1.883341]\n",
      "epoch:17 step:16417 [D loss: 0.588207, acc: 67.97%] [G loss: 2.230258]\n",
      "epoch:17 step:16418 [D loss: 0.648188, acc: 62.50%] [G loss: 1.885257]\n",
      "epoch:17 step:16419 [D loss: 0.654781, acc: 67.19%] [G loss: 1.903242]\n",
      "epoch:17 step:16420 [D loss: 0.638778, acc: 64.06%] [G loss: 1.968868]\n",
      "epoch:17 step:16421 [D loss: 0.614291, acc: 68.75%] [G loss: 1.846216]\n",
      "epoch:17 step:16422 [D loss: 0.633209, acc: 60.94%] [G loss: 1.962636]\n",
      "epoch:17 step:16423 [D loss: 0.639088, acc: 63.28%] [G loss: 2.096582]\n",
      "epoch:17 step:16424 [D loss: 0.615755, acc: 64.84%] [G loss: 2.024683]\n",
      "epoch:17 step:16425 [D loss: 0.605968, acc: 70.31%] [G loss: 1.883799]\n",
      "epoch:17 step:16426 [D loss: 0.615172, acc: 64.06%] [G loss: 2.046615]\n",
      "epoch:17 step:16427 [D loss: 0.642999, acc: 66.41%] [G loss: 2.007792]\n",
      "epoch:17 step:16428 [D loss: 0.586197, acc: 70.31%] [G loss: 2.016423]\n",
      "epoch:17 step:16429 [D loss: 0.706687, acc: 59.38%] [G loss: 1.752483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16430 [D loss: 0.738639, acc: 50.78%] [G loss: 1.660057]\n",
      "epoch:17 step:16431 [D loss: 0.688683, acc: 59.38%] [G loss: 1.775244]\n",
      "epoch:17 step:16432 [D loss: 0.645980, acc: 60.94%] [G loss: 1.871941]\n",
      "epoch:17 step:16433 [D loss: 0.594446, acc: 71.88%] [G loss: 2.016008]\n",
      "epoch:17 step:16434 [D loss: 0.654291, acc: 56.25%] [G loss: 1.894138]\n",
      "epoch:17 step:16435 [D loss: 0.674606, acc: 57.81%] [G loss: 1.683326]\n",
      "epoch:17 step:16436 [D loss: 0.716528, acc: 57.03%] [G loss: 1.980457]\n",
      "epoch:17 step:16437 [D loss: 0.632439, acc: 64.84%] [G loss: 1.957412]\n",
      "epoch:17 step:16438 [D loss: 0.682303, acc: 60.94%] [G loss: 1.950511]\n",
      "epoch:17 step:16439 [D loss: 0.629879, acc: 60.94%] [G loss: 1.935448]\n",
      "epoch:17 step:16440 [D loss: 0.671124, acc: 57.03%] [G loss: 1.791321]\n",
      "epoch:17 step:16441 [D loss: 0.680895, acc: 51.56%] [G loss: 1.742478]\n",
      "epoch:17 step:16442 [D loss: 0.697898, acc: 57.03%] [G loss: 1.840404]\n",
      "epoch:17 step:16443 [D loss: 0.608471, acc: 65.62%] [G loss: 1.827373]\n",
      "epoch:17 step:16444 [D loss: 0.615264, acc: 63.28%] [G loss: 1.986816]\n",
      "epoch:17 step:16445 [D loss: 0.592366, acc: 70.31%] [G loss: 2.005223]\n",
      "epoch:17 step:16446 [D loss: 0.638568, acc: 60.94%] [G loss: 1.971273]\n",
      "epoch:17 step:16447 [D loss: 0.648188, acc: 59.38%] [G loss: 1.950663]\n",
      "epoch:17 step:16448 [D loss: 0.685324, acc: 60.94%] [G loss: 1.963512]\n",
      "epoch:17 step:16449 [D loss: 0.627592, acc: 65.62%] [G loss: 1.797136]\n",
      "epoch:17 step:16450 [D loss: 0.631558, acc: 66.41%] [G loss: 1.890897]\n",
      "epoch:17 step:16451 [D loss: 0.605719, acc: 66.41%] [G loss: 1.849279]\n",
      "epoch:17 step:16452 [D loss: 0.635234, acc: 63.28%] [G loss: 1.964325]\n",
      "epoch:17 step:16453 [D loss: 0.649856, acc: 60.16%] [G loss: 1.922090]\n",
      "epoch:17 step:16454 [D loss: 0.682235, acc: 61.72%] [G loss: 1.961190]\n",
      "epoch:17 step:16455 [D loss: 0.656123, acc: 63.28%] [G loss: 1.850896]\n",
      "epoch:17 step:16456 [D loss: 0.608715, acc: 64.84%] [G loss: 1.906115]\n",
      "epoch:17 step:16457 [D loss: 0.669531, acc: 58.59%] [G loss: 1.680870]\n",
      "epoch:17 step:16458 [D loss: 0.697010, acc: 56.25%] [G loss: 1.794727]\n",
      "epoch:17 step:16459 [D loss: 0.680440, acc: 59.38%] [G loss: 1.759274]\n",
      "epoch:17 step:16460 [D loss: 0.639149, acc: 58.59%] [G loss: 1.849464]\n",
      "epoch:17 step:16461 [D loss: 0.636943, acc: 67.97%] [G loss: 1.931300]\n",
      "epoch:17 step:16462 [D loss: 0.651042, acc: 57.81%] [G loss: 1.857007]\n",
      "epoch:17 step:16463 [D loss: 0.631424, acc: 67.19%] [G loss: 1.942299]\n",
      "epoch:17 step:16464 [D loss: 0.667195, acc: 59.38%] [G loss: 1.769706]\n",
      "epoch:17 step:16465 [D loss: 0.618572, acc: 71.09%] [G loss: 2.029245]\n",
      "epoch:17 step:16466 [D loss: 0.616506, acc: 64.84%] [G loss: 1.841237]\n",
      "epoch:17 step:16467 [D loss: 0.671129, acc: 58.59%] [G loss: 1.901262]\n",
      "epoch:17 step:16468 [D loss: 0.669937, acc: 60.16%] [G loss: 1.940972]\n",
      "epoch:17 step:16469 [D loss: 0.710049, acc: 55.47%] [G loss: 1.759048]\n",
      "epoch:17 step:16470 [D loss: 0.669294, acc: 60.94%] [G loss: 1.725267]\n",
      "epoch:17 step:16471 [D loss: 0.653661, acc: 60.16%] [G loss: 1.714551]\n",
      "epoch:17 step:16472 [D loss: 0.674749, acc: 57.81%] [G loss: 1.816053]\n",
      "epoch:17 step:16473 [D loss: 0.677377, acc: 58.59%] [G loss: 1.775302]\n",
      "epoch:17 step:16474 [D loss: 0.697979, acc: 54.69%] [G loss: 1.929878]\n",
      "epoch:17 step:16475 [D loss: 0.637282, acc: 64.84%] [G loss: 1.871982]\n",
      "epoch:17 step:16476 [D loss: 0.654447, acc: 64.06%] [G loss: 2.019791]\n",
      "epoch:17 step:16477 [D loss: 0.673553, acc: 58.59%] [G loss: 1.987349]\n",
      "epoch:17 step:16478 [D loss: 0.651353, acc: 62.50%] [G loss: 1.862793]\n",
      "epoch:17 step:16479 [D loss: 0.602229, acc: 67.97%] [G loss: 1.937088]\n",
      "epoch:17 step:16480 [D loss: 0.603194, acc: 70.31%] [G loss: 2.114077]\n",
      "epoch:17 step:16481 [D loss: 0.584709, acc: 69.53%] [G loss: 1.884895]\n",
      "epoch:17 step:16482 [D loss: 0.648525, acc: 57.03%] [G loss: 1.754214]\n",
      "epoch:17 step:16483 [D loss: 0.583132, acc: 75.00%] [G loss: 2.180495]\n",
      "epoch:17 step:16484 [D loss: 0.679778, acc: 57.81%] [G loss: 1.883406]\n",
      "epoch:17 step:16485 [D loss: 0.640629, acc: 62.50%] [G loss: 1.921058]\n",
      "epoch:17 step:16486 [D loss: 0.623587, acc: 63.28%] [G loss: 2.030367]\n",
      "epoch:17 step:16487 [D loss: 0.700995, acc: 53.12%] [G loss: 1.916331]\n",
      "epoch:17 step:16488 [D loss: 0.688660, acc: 55.47%] [G loss: 1.845249]\n",
      "epoch:17 step:16489 [D loss: 0.620187, acc: 69.53%] [G loss: 1.789717]\n",
      "epoch:17 step:16490 [D loss: 0.644430, acc: 64.06%] [G loss: 1.919859]\n",
      "epoch:17 step:16491 [D loss: 0.662587, acc: 61.72%] [G loss: 1.871883]\n",
      "epoch:17 step:16492 [D loss: 0.618803, acc: 67.19%] [G loss: 1.926701]\n",
      "epoch:17 step:16493 [D loss: 0.625985, acc: 67.19%] [G loss: 2.042974]\n",
      "epoch:17 step:16494 [D loss: 0.665925, acc: 57.81%] [G loss: 1.920145]\n",
      "epoch:17 step:16495 [D loss: 0.683440, acc: 59.38%] [G loss: 1.799766]\n",
      "epoch:17 step:16496 [D loss: 0.641751, acc: 60.94%] [G loss: 1.744457]\n",
      "epoch:17 step:16497 [D loss: 0.658162, acc: 64.06%] [G loss: 1.845114]\n",
      "epoch:17 step:16498 [D loss: 0.650048, acc: 60.94%] [G loss: 1.873151]\n",
      "epoch:17 step:16499 [D loss: 0.676560, acc: 64.06%] [G loss: 2.060809]\n",
      "epoch:17 step:16500 [D loss: 0.595446, acc: 71.09%] [G loss: 1.868256]\n",
      "epoch:17 step:16501 [D loss: 0.620148, acc: 64.06%] [G loss: 1.765518]\n",
      "epoch:17 step:16502 [D loss: 0.622147, acc: 61.72%] [G loss: 1.834788]\n",
      "epoch:17 step:16503 [D loss: 0.590925, acc: 69.53%] [G loss: 2.061556]\n",
      "epoch:17 step:16504 [D loss: 0.589614, acc: 68.75%] [G loss: 1.970072]\n",
      "epoch:17 step:16505 [D loss: 0.641376, acc: 57.03%] [G loss: 1.743122]\n",
      "epoch:17 step:16506 [D loss: 0.697922, acc: 52.34%] [G loss: 1.711410]\n",
      "epoch:17 step:16507 [D loss: 0.696101, acc: 62.50%] [G loss: 1.882386]\n",
      "epoch:17 step:16508 [D loss: 0.656408, acc: 57.03%] [G loss: 1.885769]\n",
      "epoch:17 step:16509 [D loss: 0.619688, acc: 64.06%] [G loss: 1.902536]\n",
      "epoch:17 step:16510 [D loss: 0.649916, acc: 59.38%] [G loss: 1.923707]\n",
      "epoch:17 step:16511 [D loss: 0.608211, acc: 71.09%] [G loss: 1.941863]\n",
      "epoch:17 step:16512 [D loss: 0.612909, acc: 64.06%] [G loss: 1.969726]\n",
      "epoch:17 step:16513 [D loss: 0.654014, acc: 63.28%] [G loss: 1.792499]\n",
      "epoch:17 step:16514 [D loss: 0.629054, acc: 67.19%] [G loss: 1.937497]\n",
      "epoch:17 step:16515 [D loss: 0.641847, acc: 60.94%] [G loss: 1.962663]\n",
      "epoch:17 step:16516 [D loss: 0.683142, acc: 56.25%] [G loss: 1.938090]\n",
      "epoch:17 step:16517 [D loss: 0.658126, acc: 64.06%] [G loss: 2.112957]\n",
      "epoch:17 step:16518 [D loss: 0.660925, acc: 60.16%] [G loss: 2.001831]\n",
      "epoch:17 step:16519 [D loss: 0.658164, acc: 60.16%] [G loss: 1.857879]\n",
      "epoch:17 step:16520 [D loss: 0.664438, acc: 58.59%] [G loss: 1.750543]\n",
      "epoch:17 step:16521 [D loss: 0.636578, acc: 61.72%] [G loss: 2.022784]\n",
      "epoch:17 step:16522 [D loss: 0.674626, acc: 57.03%] [G loss: 1.928160]\n",
      "epoch:17 step:16523 [D loss: 0.655625, acc: 57.81%] [G loss: 1.784336]\n",
      "epoch:17 step:16524 [D loss: 0.642681, acc: 60.94%] [G loss: 1.713525]\n",
      "epoch:17 step:16525 [D loss: 0.706673, acc: 52.34%] [G loss: 1.710329]\n",
      "epoch:17 step:16526 [D loss: 0.670195, acc: 57.03%] [G loss: 1.819728]\n",
      "epoch:17 step:16527 [D loss: 0.657502, acc: 57.81%] [G loss: 1.849712]\n",
      "epoch:17 step:16528 [D loss: 0.669303, acc: 64.84%] [G loss: 1.834591]\n",
      "epoch:17 step:16529 [D loss: 0.597799, acc: 69.53%] [G loss: 1.821839]\n",
      "epoch:17 step:16530 [D loss: 0.661582, acc: 58.59%] [G loss: 1.848722]\n",
      "epoch:17 step:16531 [D loss: 0.634514, acc: 66.41%] [G loss: 1.858436]\n",
      "epoch:17 step:16532 [D loss: 0.629449, acc: 64.06%] [G loss: 2.023926]\n",
      "epoch:17 step:16533 [D loss: 0.667020, acc: 59.38%] [G loss: 1.869983]\n",
      "epoch:17 step:16534 [D loss: 0.687372, acc: 55.47%] [G loss: 1.965974]\n",
      "epoch:17 step:16535 [D loss: 0.677858, acc: 61.72%] [G loss: 1.709248]\n",
      "epoch:17 step:16536 [D loss: 0.631920, acc: 67.19%] [G loss: 1.924685]\n",
      "epoch:17 step:16537 [D loss: 0.650650, acc: 59.38%] [G loss: 1.976664]\n",
      "epoch:17 step:16538 [D loss: 0.624247, acc: 62.50%] [G loss: 1.895587]\n",
      "epoch:17 step:16539 [D loss: 0.624780, acc: 62.50%] [G loss: 1.857955]\n",
      "epoch:17 step:16540 [D loss: 0.619904, acc: 65.62%] [G loss: 1.847390]\n",
      "epoch:17 step:16541 [D loss: 0.630300, acc: 67.97%] [G loss: 1.910876]\n",
      "epoch:17 step:16542 [D loss: 0.624950, acc: 64.06%] [G loss: 1.972644]\n",
      "epoch:17 step:16543 [D loss: 0.649161, acc: 59.38%] [G loss: 1.762415]\n",
      "epoch:17 step:16544 [D loss: 0.709526, acc: 56.25%] [G loss: 1.760454]\n",
      "epoch:17 step:16545 [D loss: 0.662669, acc: 62.50%] [G loss: 1.881187]\n",
      "epoch:17 step:16546 [D loss: 0.732185, acc: 47.66%] [G loss: 1.946182]\n",
      "epoch:17 step:16547 [D loss: 0.659603, acc: 57.81%] [G loss: 1.835110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16548 [D loss: 0.581488, acc: 72.66%] [G loss: 1.916350]\n",
      "epoch:17 step:16549 [D loss: 0.638521, acc: 62.50%] [G loss: 1.891360]\n",
      "epoch:17 step:16550 [D loss: 0.611875, acc: 67.97%] [G loss: 1.797896]\n",
      "epoch:17 step:16551 [D loss: 0.693566, acc: 50.00%] [G loss: 1.992645]\n",
      "epoch:17 step:16552 [D loss: 0.640345, acc: 67.19%] [G loss: 1.882694]\n",
      "epoch:17 step:16553 [D loss: 0.609997, acc: 65.62%] [G loss: 1.964349]\n",
      "epoch:17 step:16554 [D loss: 0.673487, acc: 57.03%] [G loss: 1.767189]\n",
      "epoch:17 step:16555 [D loss: 0.649397, acc: 61.72%] [G loss: 1.817784]\n",
      "epoch:17 step:16556 [D loss: 0.631813, acc: 65.62%] [G loss: 1.938058]\n",
      "epoch:17 step:16557 [D loss: 0.719821, acc: 56.25%] [G loss: 1.860860]\n",
      "epoch:17 step:16558 [D loss: 0.635052, acc: 59.38%] [G loss: 1.956881]\n",
      "epoch:17 step:16559 [D loss: 0.612575, acc: 65.62%] [G loss: 1.880777]\n",
      "epoch:17 step:16560 [D loss: 0.627853, acc: 64.84%] [G loss: 2.035259]\n",
      "epoch:17 step:16561 [D loss: 0.640366, acc: 63.28%] [G loss: 1.916792]\n",
      "epoch:17 step:16562 [D loss: 0.601358, acc: 67.19%] [G loss: 1.971342]\n",
      "epoch:17 step:16563 [D loss: 0.580033, acc: 75.00%] [G loss: 2.029569]\n",
      "epoch:17 step:16564 [D loss: 0.631381, acc: 63.28%] [G loss: 2.032379]\n",
      "epoch:17 step:16565 [D loss: 0.646751, acc: 65.62%] [G loss: 1.896389]\n",
      "epoch:17 step:16566 [D loss: 0.622108, acc: 65.62%] [G loss: 2.078321]\n",
      "epoch:17 step:16567 [D loss: 0.605568, acc: 64.84%] [G loss: 1.958766]\n",
      "epoch:17 step:16568 [D loss: 0.675557, acc: 58.59%] [G loss: 1.850175]\n",
      "epoch:17 step:16569 [D loss: 0.645081, acc: 59.38%] [G loss: 1.907080]\n",
      "epoch:17 step:16570 [D loss: 0.622016, acc: 57.81%] [G loss: 1.982581]\n",
      "epoch:17 step:16571 [D loss: 0.622722, acc: 64.06%] [G loss: 2.079578]\n",
      "epoch:17 step:16572 [D loss: 0.666549, acc: 60.16%] [G loss: 1.937681]\n",
      "epoch:17 step:16573 [D loss: 0.672431, acc: 57.03%] [G loss: 1.868112]\n",
      "epoch:17 step:16574 [D loss: 0.610174, acc: 70.31%] [G loss: 1.803458]\n",
      "epoch:17 step:16575 [D loss: 0.637500, acc: 62.50%] [G loss: 1.999846]\n",
      "epoch:17 step:16576 [D loss: 0.615916, acc: 66.41%] [G loss: 2.050202]\n",
      "epoch:17 step:16577 [D loss: 0.568686, acc: 76.56%] [G loss: 2.283307]\n",
      "epoch:17 step:16578 [D loss: 0.612123, acc: 67.97%] [G loss: 2.187068]\n",
      "epoch:17 step:16579 [D loss: 0.602960, acc: 69.53%] [G loss: 2.222311]\n",
      "epoch:17 step:16580 [D loss: 0.622928, acc: 64.84%] [G loss: 1.939228]\n",
      "epoch:17 step:16581 [D loss: 0.672186, acc: 55.47%] [G loss: 1.890262]\n",
      "epoch:17 step:16582 [D loss: 0.655963, acc: 67.97%] [G loss: 1.913790]\n",
      "epoch:17 step:16583 [D loss: 0.585566, acc: 69.53%] [G loss: 2.050173]\n",
      "epoch:17 step:16584 [D loss: 0.697456, acc: 57.03%] [G loss: 1.921722]\n",
      "epoch:17 step:16585 [D loss: 0.660771, acc: 58.59%] [G loss: 1.825976]\n",
      "epoch:17 step:16586 [D loss: 0.634325, acc: 60.16%] [G loss: 1.757453]\n",
      "epoch:17 step:16587 [D loss: 0.660855, acc: 61.72%] [G loss: 1.951500]\n",
      "epoch:17 step:16588 [D loss: 0.625021, acc: 66.41%] [G loss: 1.883103]\n",
      "epoch:17 step:16589 [D loss: 0.608705, acc: 68.75%] [G loss: 1.896481]\n",
      "epoch:17 step:16590 [D loss: 0.591709, acc: 73.44%] [G loss: 1.850752]\n",
      "epoch:17 step:16591 [D loss: 0.601801, acc: 64.84%] [G loss: 2.044080]\n",
      "epoch:17 step:16592 [D loss: 0.662950, acc: 60.16%] [G loss: 1.774506]\n",
      "epoch:17 step:16593 [D loss: 0.751837, acc: 50.78%] [G loss: 1.880313]\n",
      "epoch:17 step:16594 [D loss: 0.636661, acc: 67.19%] [G loss: 1.934801]\n",
      "epoch:17 step:16595 [D loss: 0.623413, acc: 63.28%] [G loss: 1.936408]\n",
      "epoch:17 step:16596 [D loss: 0.664929, acc: 61.72%] [G loss: 1.776408]\n",
      "epoch:17 step:16597 [D loss: 0.665108, acc: 61.72%] [G loss: 1.869079]\n",
      "epoch:17 step:16598 [D loss: 0.652283, acc: 64.06%] [G loss: 1.774788]\n",
      "epoch:17 step:16599 [D loss: 0.685961, acc: 60.94%] [G loss: 1.822558]\n",
      "epoch:17 step:16600 [D loss: 0.632764, acc: 66.41%] [G loss: 1.928065]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.308078\n",
      "FID: 15.182008\n",
      "0 = 12.798410487079604\n",
      "1 = 0.09115244523544008\n",
      "2 = 0.883899986743927\n",
      "3 = 0.8912000060081482\n",
      "4 = 0.8766000270843506\n",
      "5 = 0.8783757090568542\n",
      "6 = 0.8912000060081482\n",
      "7 = 6.580755123913277\n",
      "8 = 0.07580871084482055\n",
      "9 = 0.7293000221252441\n",
      "10 = 0.7390000224113464\n",
      "11 = 0.7196000218391418\n",
      "12 = 0.7249362468719482\n",
      "13 = 0.7390000224113464\n",
      "14 = 7.308111190795898\n",
      "15 = 9.464503288269043\n",
      "16 = 0.11438863724470139\n",
      "17 = 7.308077812194824\n",
      "18 = 15.182007789611816\n",
      "epoch:17 step:16601 [D loss: 0.661505, acc: 56.25%] [G loss: 1.908122]\n",
      "epoch:17 step:16602 [D loss: 0.659923, acc: 63.28%] [G loss: 1.923294]\n",
      "epoch:17 step:16603 [D loss: 0.663111, acc: 60.16%] [G loss: 1.866559]\n",
      "epoch:17 step:16604 [D loss: 0.696262, acc: 53.91%] [G loss: 1.878159]\n",
      "epoch:17 step:16605 [D loss: 0.663111, acc: 62.50%] [G loss: 1.874334]\n",
      "epoch:17 step:16606 [D loss: 0.606264, acc: 68.75%] [G loss: 1.895706]\n",
      "epoch:17 step:16607 [D loss: 0.648929, acc: 62.50%] [G loss: 1.934065]\n",
      "epoch:17 step:16608 [D loss: 0.609721, acc: 67.19%] [G loss: 1.880791]\n",
      "epoch:17 step:16609 [D loss: 0.627139, acc: 63.28%] [G loss: 1.866870]\n",
      "epoch:17 step:16610 [D loss: 0.622434, acc: 65.62%] [G loss: 2.156780]\n",
      "epoch:17 step:16611 [D loss: 0.635054, acc: 64.06%] [G loss: 1.921716]\n",
      "epoch:17 step:16612 [D loss: 0.633590, acc: 64.06%] [G loss: 1.942163]\n",
      "epoch:17 step:16613 [D loss: 0.597861, acc: 69.53%] [G loss: 1.832195]\n",
      "epoch:17 step:16614 [D loss: 0.617173, acc: 70.31%] [G loss: 1.952773]\n",
      "epoch:17 step:16615 [D loss: 0.633684, acc: 63.28%] [G loss: 1.928026]\n",
      "epoch:17 step:16616 [D loss: 0.622455, acc: 66.41%] [G loss: 1.968091]\n",
      "epoch:17 step:16617 [D loss: 0.660237, acc: 64.06%] [G loss: 2.072257]\n",
      "epoch:17 step:16618 [D loss: 0.697419, acc: 54.69%] [G loss: 1.917249]\n",
      "epoch:17 step:16619 [D loss: 0.615855, acc: 67.19%] [G loss: 1.979388]\n",
      "epoch:17 step:16620 [D loss: 0.667022, acc: 59.38%] [G loss: 1.951355]\n",
      "epoch:17 step:16621 [D loss: 0.623793, acc: 60.94%] [G loss: 2.018900]\n",
      "epoch:17 step:16622 [D loss: 0.601506, acc: 67.97%] [G loss: 2.094427]\n",
      "epoch:17 step:16623 [D loss: 0.605856, acc: 70.31%] [G loss: 2.172344]\n",
      "epoch:17 step:16624 [D loss: 0.669023, acc: 61.72%] [G loss: 1.983152]\n",
      "epoch:17 step:16625 [D loss: 0.650872, acc: 56.25%] [G loss: 1.870479]\n",
      "epoch:17 step:16626 [D loss: 0.585992, acc: 69.53%] [G loss: 1.948564]\n",
      "epoch:17 step:16627 [D loss: 0.634881, acc: 60.16%] [G loss: 1.843168]\n",
      "epoch:17 step:16628 [D loss: 0.627374, acc: 64.84%] [G loss: 1.882765]\n",
      "epoch:17 step:16629 [D loss: 0.609265, acc: 67.97%] [G loss: 2.015844]\n",
      "epoch:17 step:16630 [D loss: 0.635362, acc: 65.62%] [G loss: 2.087059]\n",
      "epoch:17 step:16631 [D loss: 0.667408, acc: 60.94%] [G loss: 1.863857]\n",
      "epoch:17 step:16632 [D loss: 0.711561, acc: 59.38%] [G loss: 1.813020]\n",
      "epoch:17 step:16633 [D loss: 0.696364, acc: 58.59%] [G loss: 1.800583]\n",
      "epoch:17 step:16634 [D loss: 0.674494, acc: 60.94%] [G loss: 1.814767]\n",
      "epoch:17 step:16635 [D loss: 0.626689, acc: 60.94%] [G loss: 2.003827]\n",
      "epoch:17 step:16636 [D loss: 0.589545, acc: 72.66%] [G loss: 2.099350]\n",
      "epoch:17 step:16637 [D loss: 0.617427, acc: 64.06%] [G loss: 2.055216]\n",
      "epoch:17 step:16638 [D loss: 0.607462, acc: 67.19%] [G loss: 1.967085]\n",
      "epoch:17 step:16639 [D loss: 0.615858, acc: 67.97%] [G loss: 2.008499]\n",
      "epoch:17 step:16640 [D loss: 0.706851, acc: 60.94%] [G loss: 1.950538]\n",
      "epoch:17 step:16641 [D loss: 0.636215, acc: 61.72%] [G loss: 1.954233]\n",
      "epoch:17 step:16642 [D loss: 0.675225, acc: 56.25%] [G loss: 1.936634]\n",
      "epoch:17 step:16643 [D loss: 0.680770, acc: 57.03%] [G loss: 1.873914]\n",
      "epoch:17 step:16644 [D loss: 0.604179, acc: 68.75%] [G loss: 1.911576]\n",
      "epoch:17 step:16645 [D loss: 0.682549, acc: 53.12%] [G loss: 1.739722]\n",
      "epoch:17 step:16646 [D loss: 0.651116, acc: 68.75%] [G loss: 1.792079]\n",
      "epoch:17 step:16647 [D loss: 0.718569, acc: 54.69%] [G loss: 1.974531]\n",
      "epoch:17 step:16648 [D loss: 0.630141, acc: 64.84%] [G loss: 1.970911]\n",
      "epoch:17 step:16649 [D loss: 0.623610, acc: 65.62%] [G loss: 1.965407]\n",
      "epoch:17 step:16650 [D loss: 0.640103, acc: 60.16%] [G loss: 1.944985]\n",
      "epoch:17 step:16651 [D loss: 0.707664, acc: 59.38%] [G loss: 1.938656]\n",
      "epoch:17 step:16652 [D loss: 0.688241, acc: 57.81%] [G loss: 1.793360]\n",
      "epoch:17 step:16653 [D loss: 0.621182, acc: 62.50%] [G loss: 1.905763]\n",
      "epoch:17 step:16654 [D loss: 0.652513, acc: 62.50%] [G loss: 1.897930]\n",
      "epoch:17 step:16655 [D loss: 0.685346, acc: 60.94%] [G loss: 1.937111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16656 [D loss: 0.657968, acc: 60.16%] [G loss: 1.831514]\n",
      "epoch:17 step:16657 [D loss: 0.708593, acc: 57.81%] [G loss: 1.808522]\n",
      "epoch:17 step:16658 [D loss: 0.630832, acc: 64.84%] [G loss: 1.803518]\n",
      "epoch:17 step:16659 [D loss: 0.675470, acc: 56.25%] [G loss: 1.890175]\n",
      "epoch:17 step:16660 [D loss: 0.643137, acc: 62.50%] [G loss: 1.780385]\n",
      "epoch:17 step:16661 [D loss: 0.644179, acc: 63.28%] [G loss: 1.862940]\n",
      "epoch:17 step:16662 [D loss: 0.650204, acc: 59.38%] [G loss: 1.941947]\n",
      "epoch:17 step:16663 [D loss: 0.683358, acc: 56.25%] [G loss: 1.749693]\n",
      "epoch:17 step:16664 [D loss: 0.668553, acc: 57.03%] [G loss: 1.944351]\n",
      "epoch:17 step:16665 [D loss: 0.669928, acc: 61.72%] [G loss: 1.982227]\n",
      "epoch:17 step:16666 [D loss: 0.663266, acc: 60.94%] [G loss: 1.819681]\n",
      "epoch:17 step:16667 [D loss: 0.635260, acc: 64.84%] [G loss: 1.712627]\n",
      "epoch:17 step:16668 [D loss: 0.754581, acc: 48.44%] [G loss: 1.866400]\n",
      "epoch:17 step:16669 [D loss: 0.622107, acc: 59.38%] [G loss: 1.923483]\n",
      "epoch:17 step:16670 [D loss: 0.681990, acc: 57.03%] [G loss: 1.747705]\n",
      "epoch:17 step:16671 [D loss: 0.668283, acc: 60.94%] [G loss: 1.788428]\n",
      "epoch:17 step:16672 [D loss: 0.690043, acc: 54.69%] [G loss: 1.769955]\n",
      "epoch:17 step:16673 [D loss: 0.657994, acc: 60.16%] [G loss: 1.800670]\n",
      "epoch:17 step:16674 [D loss: 0.643811, acc: 62.50%] [G loss: 1.835626]\n",
      "epoch:17 step:16675 [D loss: 0.562787, acc: 73.44%] [G loss: 2.110653]\n",
      "epoch:17 step:16676 [D loss: 0.676037, acc: 59.38%] [G loss: 1.876963]\n",
      "epoch:17 step:16677 [D loss: 0.661702, acc: 62.50%] [G loss: 1.793082]\n",
      "epoch:17 step:16678 [D loss: 0.641307, acc: 63.28%] [G loss: 1.776798]\n",
      "epoch:17 step:16679 [D loss: 0.636765, acc: 62.50%] [G loss: 1.768856]\n",
      "epoch:17 step:16680 [D loss: 0.640555, acc: 61.72%] [G loss: 1.704091]\n",
      "epoch:17 step:16681 [D loss: 0.643193, acc: 64.84%] [G loss: 1.747388]\n",
      "epoch:17 step:16682 [D loss: 0.682253, acc: 54.69%] [G loss: 1.824274]\n",
      "epoch:17 step:16683 [D loss: 0.605028, acc: 71.88%] [G loss: 1.758918]\n",
      "epoch:17 step:16684 [D loss: 0.656281, acc: 60.94%] [G loss: 1.900968]\n",
      "epoch:17 step:16685 [D loss: 0.659538, acc: 60.16%] [G loss: 1.838500]\n",
      "epoch:17 step:16686 [D loss: 0.654517, acc: 59.38%] [G loss: 1.879205]\n",
      "epoch:17 step:16687 [D loss: 0.616989, acc: 70.31%] [G loss: 1.722715]\n",
      "epoch:17 step:16688 [D loss: 0.610136, acc: 64.84%] [G loss: 1.829172]\n",
      "epoch:17 step:16689 [D loss: 0.582978, acc: 70.31%] [G loss: 1.952901]\n",
      "epoch:17 step:16690 [D loss: 0.563859, acc: 70.31%] [G loss: 1.837652]\n",
      "epoch:17 step:16691 [D loss: 0.611324, acc: 63.28%] [G loss: 1.954099]\n",
      "epoch:17 step:16692 [D loss: 0.629036, acc: 67.97%] [G loss: 2.006689]\n",
      "epoch:17 step:16693 [D loss: 0.635737, acc: 64.84%] [G loss: 1.837175]\n",
      "epoch:17 step:16694 [D loss: 0.736619, acc: 52.34%] [G loss: 1.636800]\n",
      "epoch:17 step:16695 [D loss: 0.735690, acc: 50.78%] [G loss: 1.837579]\n",
      "epoch:17 step:16696 [D loss: 0.684855, acc: 59.38%] [G loss: 1.811719]\n",
      "epoch:17 step:16697 [D loss: 0.672369, acc: 58.59%] [G loss: 1.910220]\n",
      "epoch:17 step:16698 [D loss: 0.626826, acc: 65.62%] [G loss: 1.944642]\n",
      "epoch:17 step:16699 [D loss: 0.658749, acc: 59.38%] [G loss: 1.926086]\n",
      "epoch:17 step:16700 [D loss: 0.599110, acc: 69.53%] [G loss: 1.948911]\n",
      "epoch:17 step:16701 [D loss: 0.659451, acc: 57.03%] [G loss: 1.782938]\n",
      "epoch:17 step:16702 [D loss: 0.621847, acc: 68.75%] [G loss: 1.941017]\n",
      "epoch:17 step:16703 [D loss: 0.639046, acc: 60.94%] [G loss: 2.096035]\n",
      "epoch:17 step:16704 [D loss: 0.640909, acc: 67.19%] [G loss: 2.031456]\n",
      "epoch:17 step:16705 [D loss: 0.663229, acc: 57.03%] [G loss: 1.940019]\n",
      "epoch:17 step:16706 [D loss: 0.674753, acc: 57.81%] [G loss: 1.792346]\n",
      "epoch:17 step:16707 [D loss: 0.647124, acc: 64.06%] [G loss: 2.053265]\n",
      "epoch:17 step:16708 [D loss: 0.655863, acc: 65.62%] [G loss: 1.925625]\n",
      "epoch:17 step:16709 [D loss: 0.640700, acc: 64.84%] [G loss: 1.985494]\n",
      "epoch:17 step:16710 [D loss: 0.593448, acc: 70.31%] [G loss: 1.987176]\n",
      "epoch:17 step:16711 [D loss: 0.614638, acc: 65.62%] [G loss: 2.019373]\n",
      "epoch:17 step:16712 [D loss: 0.667709, acc: 61.72%] [G loss: 2.007731]\n",
      "epoch:17 step:16713 [D loss: 0.660803, acc: 62.50%] [G loss: 1.881013]\n",
      "epoch:17 step:16714 [D loss: 0.675595, acc: 60.16%] [G loss: 1.755534]\n",
      "epoch:17 step:16715 [D loss: 0.625188, acc: 60.16%] [G loss: 1.944120]\n",
      "epoch:17 step:16716 [D loss: 0.682328, acc: 55.47%] [G loss: 1.838313]\n",
      "epoch:17 step:16717 [D loss: 0.704952, acc: 57.81%] [G loss: 1.752600]\n",
      "epoch:17 step:16718 [D loss: 0.638352, acc: 67.19%] [G loss: 1.870592]\n",
      "epoch:17 step:16719 [D loss: 0.594818, acc: 65.62%] [G loss: 1.963597]\n",
      "epoch:17 step:16720 [D loss: 0.643682, acc: 61.72%] [G loss: 2.022735]\n",
      "epoch:17 step:16721 [D loss: 0.620347, acc: 60.94%] [G loss: 1.900627]\n",
      "epoch:17 step:16722 [D loss: 0.664483, acc: 55.47%] [G loss: 1.828623]\n",
      "epoch:17 step:16723 [D loss: 0.728172, acc: 45.31%] [G loss: 1.850085]\n",
      "epoch:17 step:16724 [D loss: 0.661093, acc: 58.59%] [G loss: 1.819548]\n",
      "epoch:17 step:16725 [D loss: 0.627043, acc: 64.06%] [G loss: 1.850780]\n",
      "epoch:17 step:16726 [D loss: 0.635483, acc: 62.50%] [G loss: 1.837597]\n",
      "epoch:17 step:16727 [D loss: 0.650981, acc: 63.28%] [G loss: 1.883844]\n",
      "epoch:17 step:16728 [D loss: 0.651284, acc: 62.50%] [G loss: 1.937172]\n",
      "epoch:17 step:16729 [D loss: 0.749808, acc: 50.78%] [G loss: 1.743199]\n",
      "epoch:17 step:16730 [D loss: 0.704750, acc: 53.12%] [G loss: 1.729764]\n",
      "epoch:17 step:16731 [D loss: 0.639758, acc: 62.50%] [G loss: 1.828671]\n",
      "epoch:17 step:16732 [D loss: 0.651136, acc: 62.50%] [G loss: 1.802326]\n",
      "epoch:17 step:16733 [D loss: 0.619687, acc: 64.06%] [G loss: 1.831415]\n",
      "epoch:17 step:16734 [D loss: 0.650374, acc: 59.38%] [G loss: 2.016452]\n",
      "epoch:17 step:16735 [D loss: 0.603036, acc: 67.19%] [G loss: 1.919692]\n",
      "epoch:17 step:16736 [D loss: 0.628321, acc: 61.72%] [G loss: 2.002331]\n",
      "epoch:17 step:16737 [D loss: 0.681334, acc: 57.81%] [G loss: 1.903432]\n",
      "epoch:17 step:16738 [D loss: 0.668217, acc: 55.47%] [G loss: 1.934662]\n",
      "epoch:17 step:16739 [D loss: 0.600635, acc: 67.19%] [G loss: 1.915125]\n",
      "epoch:17 step:16740 [D loss: 0.660265, acc: 64.06%] [G loss: 1.835018]\n",
      "epoch:17 step:16741 [D loss: 0.681247, acc: 54.69%] [G loss: 1.799251]\n",
      "epoch:17 step:16742 [D loss: 0.669575, acc: 60.94%] [G loss: 1.790943]\n",
      "epoch:17 step:16743 [D loss: 0.568253, acc: 73.44%] [G loss: 2.117965]\n",
      "epoch:17 step:16744 [D loss: 0.695161, acc: 59.38%] [G loss: 1.947526]\n",
      "epoch:17 step:16745 [D loss: 0.660489, acc: 64.84%] [G loss: 1.971936]\n",
      "epoch:17 step:16746 [D loss: 0.705695, acc: 57.81%] [G loss: 1.753085]\n",
      "epoch:17 step:16747 [D loss: 0.719757, acc: 53.91%] [G loss: 1.732271]\n",
      "epoch:17 step:16748 [D loss: 0.648638, acc: 60.94%] [G loss: 1.859748]\n",
      "epoch:17 step:16749 [D loss: 0.670204, acc: 59.38%] [G loss: 1.695014]\n",
      "epoch:17 step:16750 [D loss: 0.638889, acc: 60.94%] [G loss: 1.916683]\n",
      "epoch:17 step:16751 [D loss: 0.638294, acc: 61.72%] [G loss: 1.889803]\n",
      "epoch:17 step:16752 [D loss: 0.643774, acc: 66.41%] [G loss: 1.892691]\n",
      "epoch:17 step:16753 [D loss: 0.657397, acc: 65.62%] [G loss: 1.767790]\n",
      "epoch:17 step:16754 [D loss: 0.599577, acc: 64.06%] [G loss: 1.966720]\n",
      "epoch:17 step:16755 [D loss: 0.647408, acc: 64.06%] [G loss: 1.822172]\n",
      "epoch:17 step:16756 [D loss: 0.702873, acc: 53.12%] [G loss: 1.758832]\n",
      "epoch:17 step:16757 [D loss: 0.683051, acc: 60.16%] [G loss: 1.857160]\n",
      "epoch:17 step:16758 [D loss: 0.680810, acc: 57.03%] [G loss: 1.868498]\n",
      "epoch:17 step:16759 [D loss: 0.673654, acc: 58.59%] [G loss: 1.871291]\n",
      "epoch:17 step:16760 [D loss: 0.668745, acc: 61.72%] [G loss: 1.848550]\n",
      "epoch:17 step:16761 [D loss: 0.650396, acc: 64.84%] [G loss: 1.935003]\n",
      "epoch:17 step:16762 [D loss: 0.603537, acc: 67.97%] [G loss: 1.980128]\n",
      "epoch:17 step:16763 [D loss: 0.650718, acc: 64.84%] [G loss: 1.871961]\n",
      "epoch:17 step:16764 [D loss: 0.684940, acc: 57.03%] [G loss: 1.805300]\n",
      "epoch:17 step:16765 [D loss: 0.606968, acc: 71.88%] [G loss: 1.845911]\n",
      "epoch:17 step:16766 [D loss: 0.632717, acc: 65.62%] [G loss: 1.958380]\n",
      "epoch:17 step:16767 [D loss: 0.648168, acc: 60.94%] [G loss: 1.781250]\n",
      "epoch:17 step:16768 [D loss: 0.662730, acc: 60.94%] [G loss: 1.921336]\n",
      "epoch:17 step:16769 [D loss: 0.627155, acc: 62.50%] [G loss: 1.905483]\n",
      "epoch:17 step:16770 [D loss: 0.613373, acc: 64.06%] [G loss: 1.829995]\n",
      "epoch:17 step:16771 [D loss: 0.581337, acc: 67.19%] [G loss: 1.869099]\n",
      "epoch:17 step:16772 [D loss: 0.677622, acc: 59.38%] [G loss: 1.974521]\n",
      "epoch:17 step:16773 [D loss: 0.671431, acc: 60.94%] [G loss: 1.940539]\n",
      "epoch:17 step:16774 [D loss: 0.692332, acc: 55.47%] [G loss: 1.906920]\n",
      "epoch:17 step:16775 [D loss: 0.635352, acc: 62.50%] [G loss: 2.011724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16776 [D loss: 0.654953, acc: 64.06%] [G loss: 1.816261]\n",
      "epoch:17 step:16777 [D loss: 0.662036, acc: 60.94%] [G loss: 1.921365]\n",
      "epoch:17 step:16778 [D loss: 0.635462, acc: 64.06%] [G loss: 1.869513]\n",
      "epoch:17 step:16779 [D loss: 0.733288, acc: 52.34%] [G loss: 1.709844]\n",
      "epoch:17 step:16780 [D loss: 0.707785, acc: 58.59%] [G loss: 1.808031]\n",
      "epoch:17 step:16781 [D loss: 0.646204, acc: 66.41%] [G loss: 1.937631]\n",
      "epoch:17 step:16782 [D loss: 0.635074, acc: 69.53%] [G loss: 1.834985]\n",
      "epoch:17 step:16783 [D loss: 0.706809, acc: 57.03%] [G loss: 1.937050]\n",
      "epoch:17 step:16784 [D loss: 0.658955, acc: 59.38%] [G loss: 1.776545]\n",
      "epoch:17 step:16785 [D loss: 0.675712, acc: 58.59%] [G loss: 1.765587]\n",
      "epoch:17 step:16786 [D loss: 0.665987, acc: 61.72%] [G loss: 1.951748]\n",
      "epoch:17 step:16787 [D loss: 0.751056, acc: 55.47%] [G loss: 1.793060]\n",
      "epoch:17 step:16788 [D loss: 0.695643, acc: 56.25%] [G loss: 1.701596]\n",
      "epoch:17 step:16789 [D loss: 0.613452, acc: 69.53%] [G loss: 1.839952]\n",
      "epoch:17 step:16790 [D loss: 0.644429, acc: 61.72%] [G loss: 1.802814]\n",
      "epoch:17 step:16791 [D loss: 0.639921, acc: 64.84%] [G loss: 1.750532]\n",
      "epoch:17 step:16792 [D loss: 0.639605, acc: 67.97%] [G loss: 1.746939]\n",
      "epoch:17 step:16793 [D loss: 0.640405, acc: 70.31%] [G loss: 1.826344]\n",
      "epoch:17 step:16794 [D loss: 0.649039, acc: 63.28%] [G loss: 1.762320]\n",
      "epoch:17 step:16795 [D loss: 0.658078, acc: 59.38%] [G loss: 1.788684]\n",
      "epoch:17 step:16796 [D loss: 0.652195, acc: 60.94%] [G loss: 1.847991]\n",
      "epoch:17 step:16797 [D loss: 0.636940, acc: 60.16%] [G loss: 1.880403]\n",
      "epoch:17 step:16798 [D loss: 0.670312, acc: 54.69%] [G loss: 1.759791]\n",
      "epoch:17 step:16799 [D loss: 0.654787, acc: 63.28%] [G loss: 1.788457]\n",
      "epoch:17 step:16800 [D loss: 0.660673, acc: 60.94%] [G loss: 1.765592]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.273207\n",
      "FID: 14.635488\n",
      "0 = 12.797689487218857\n",
      "1 = 0.08958597105416002\n",
      "2 = 0.8894000053405762\n",
      "3 = 0.9133999943733215\n",
      "4 = 0.8654000163078308\n",
      "5 = 0.8715648651123047\n",
      "6 = 0.9133999943733215\n",
      "7 = 6.573312846338753\n",
      "8 = 0.07481067922315689\n",
      "9 = 0.720300018787384\n",
      "10 = 0.7268000245094299\n",
      "11 = 0.7138000130653381\n",
      "12 = 0.7174728512763977\n",
      "13 = 0.7268000245094299\n",
      "14 = 7.273235321044922\n",
      "15 = 9.405163764953613\n",
      "16 = 0.12196611613035202\n",
      "17 = 7.27320671081543\n",
      "18 = 14.63548755645752\n",
      "epoch:17 step:16801 [D loss: 0.694360, acc: 59.38%] [G loss: 1.760880]\n",
      "epoch:17 step:16802 [D loss: 0.699065, acc: 55.47%] [G loss: 1.725572]\n",
      "epoch:17 step:16803 [D loss: 0.678068, acc: 58.59%] [G loss: 1.728731]\n",
      "epoch:17 step:16804 [D loss: 0.639752, acc: 64.06%] [G loss: 1.795544]\n",
      "epoch:17 step:16805 [D loss: 0.684345, acc: 55.47%] [G loss: 1.831698]\n",
      "epoch:17 step:16806 [D loss: 0.669057, acc: 58.59%] [G loss: 1.866391]\n",
      "epoch:17 step:16807 [D loss: 0.666770, acc: 58.59%] [G loss: 1.813505]\n",
      "epoch:17 step:16808 [D loss: 0.671738, acc: 62.50%] [G loss: 1.864713]\n",
      "epoch:17 step:16809 [D loss: 0.659133, acc: 60.16%] [G loss: 1.748077]\n",
      "epoch:17 step:16810 [D loss: 0.696452, acc: 56.25%] [G loss: 1.730477]\n",
      "epoch:17 step:16811 [D loss: 0.654512, acc: 65.62%] [G loss: 1.833323]\n",
      "epoch:17 step:16812 [D loss: 0.645963, acc: 62.50%] [G loss: 1.852230]\n",
      "epoch:17 step:16813 [D loss: 0.634211, acc: 60.94%] [G loss: 1.976075]\n",
      "epoch:17 step:16814 [D loss: 0.690636, acc: 56.25%] [G loss: 1.745289]\n",
      "epoch:17 step:16815 [D loss: 0.657473, acc: 60.94%] [G loss: 1.862144]\n",
      "epoch:17 step:16816 [D loss: 0.593841, acc: 66.41%] [G loss: 1.805799]\n",
      "epoch:17 step:16817 [D loss: 0.647074, acc: 58.59%] [G loss: 1.844436]\n",
      "epoch:17 step:16818 [D loss: 0.662080, acc: 62.50%] [G loss: 1.897293]\n",
      "epoch:17 step:16819 [D loss: 0.629783, acc: 67.97%] [G loss: 1.847254]\n",
      "epoch:17 step:16820 [D loss: 0.661902, acc: 58.59%] [G loss: 1.761080]\n",
      "epoch:17 step:16821 [D loss: 0.702422, acc: 54.69%] [G loss: 1.754354]\n",
      "epoch:17 step:16822 [D loss: 0.655496, acc: 59.38%] [G loss: 1.861485]\n",
      "epoch:17 step:16823 [D loss: 0.623634, acc: 62.50%] [G loss: 1.835325]\n",
      "epoch:17 step:16824 [D loss: 0.630231, acc: 64.84%] [G loss: 1.795562]\n",
      "epoch:17 step:16825 [D loss: 0.693574, acc: 57.81%] [G loss: 1.854784]\n",
      "epoch:17 step:16826 [D loss: 0.642548, acc: 64.84%] [G loss: 2.016546]\n",
      "epoch:17 step:16827 [D loss: 0.624470, acc: 65.62%] [G loss: 1.957844]\n",
      "epoch:17 step:16828 [D loss: 0.629654, acc: 64.06%] [G loss: 1.897844]\n",
      "epoch:17 step:16829 [D loss: 0.614911, acc: 67.97%] [G loss: 1.988319]\n",
      "epoch:17 step:16830 [D loss: 0.626024, acc: 64.06%] [G loss: 1.839800]\n",
      "epoch:17 step:16831 [D loss: 0.687805, acc: 61.72%] [G loss: 1.880680]\n",
      "epoch:17 step:16832 [D loss: 0.642020, acc: 61.72%] [G loss: 1.943855]\n",
      "epoch:17 step:16833 [D loss: 0.660162, acc: 60.94%] [G loss: 1.927949]\n",
      "epoch:17 step:16834 [D loss: 0.647622, acc: 59.38%] [G loss: 1.878944]\n",
      "epoch:17 step:16835 [D loss: 0.634904, acc: 64.84%] [G loss: 1.869834]\n",
      "epoch:17 step:16836 [D loss: 0.644857, acc: 65.62%] [G loss: 1.816686]\n",
      "epoch:17 step:16837 [D loss: 0.637964, acc: 59.38%] [G loss: 1.819741]\n",
      "epoch:17 step:16838 [D loss: 0.596405, acc: 67.19%] [G loss: 1.991444]\n",
      "epoch:17 step:16839 [D loss: 0.647564, acc: 61.72%] [G loss: 1.981793]\n",
      "epoch:17 step:16840 [D loss: 0.648961, acc: 63.28%] [G loss: 1.934291]\n",
      "epoch:17 step:16841 [D loss: 0.632334, acc: 64.84%] [G loss: 2.150018]\n",
      "epoch:17 step:16842 [D loss: 0.604009, acc: 67.19%] [G loss: 2.026477]\n",
      "epoch:17 step:16843 [D loss: 0.635554, acc: 65.62%] [G loss: 1.751857]\n",
      "epoch:17 step:16844 [D loss: 0.590003, acc: 70.31%] [G loss: 2.102033]\n",
      "epoch:17 step:16845 [D loss: 0.598111, acc: 69.53%] [G loss: 1.887046]\n",
      "epoch:17 step:16846 [D loss: 0.646136, acc: 63.28%] [G loss: 2.025650]\n",
      "epoch:17 step:16847 [D loss: 0.564013, acc: 78.12%] [G loss: 2.225353]\n",
      "epoch:17 step:16848 [D loss: 0.598083, acc: 67.19%] [G loss: 2.153506]\n",
      "epoch:17 step:16849 [D loss: 0.697513, acc: 53.91%] [G loss: 1.896598]\n",
      "epoch:17 step:16850 [D loss: 0.616479, acc: 64.84%] [G loss: 1.996709]\n",
      "epoch:17 step:16851 [D loss: 0.666046, acc: 62.50%] [G loss: 2.122598]\n",
      "epoch:17 step:16852 [D loss: 0.610982, acc: 64.84%] [G loss: 2.193522]\n",
      "epoch:17 step:16853 [D loss: 0.584960, acc: 69.53%] [G loss: 2.149845]\n",
      "epoch:17 step:16854 [D loss: 0.570259, acc: 76.56%] [G loss: 2.038099]\n",
      "epoch:17 step:16855 [D loss: 0.674273, acc: 60.16%] [G loss: 2.066542]\n",
      "epoch:17 step:16856 [D loss: 0.583537, acc: 69.53%] [G loss: 2.088855]\n",
      "epoch:17 step:16857 [D loss: 0.752234, acc: 52.34%] [G loss: 1.866788]\n",
      "epoch:17 step:16858 [D loss: 0.714737, acc: 51.56%] [G loss: 1.781898]\n",
      "epoch:17 step:16859 [D loss: 0.654809, acc: 57.81%] [G loss: 1.927324]\n",
      "epoch:17 step:16860 [D loss: 0.598451, acc: 67.97%] [G loss: 1.945882]\n",
      "epoch:17 step:16861 [D loss: 0.658468, acc: 60.94%] [G loss: 1.900909]\n",
      "epoch:17 step:16862 [D loss: 0.627759, acc: 63.28%] [G loss: 1.921783]\n",
      "epoch:17 step:16863 [D loss: 0.604916, acc: 70.31%] [G loss: 2.019788]\n",
      "epoch:17 step:16864 [D loss: 0.601882, acc: 67.97%] [G loss: 1.960281]\n",
      "epoch:17 step:16865 [D loss: 0.699296, acc: 66.41%] [G loss: 2.014785]\n",
      "epoch:17 step:16866 [D loss: 0.576488, acc: 75.78%] [G loss: 2.422389]\n",
      "epoch:18 step:16867 [D loss: 0.654503, acc: 61.72%] [G loss: 1.903618]\n",
      "epoch:18 step:16868 [D loss: 0.672500, acc: 57.03%] [G loss: 1.997984]\n",
      "epoch:18 step:16869 [D loss: 0.618557, acc: 67.97%] [G loss: 2.080165]\n",
      "epoch:18 step:16870 [D loss: 0.634471, acc: 67.19%] [G loss: 1.939734]\n",
      "epoch:18 step:16871 [D loss: 0.622739, acc: 67.19%] [G loss: 1.961573]\n",
      "epoch:18 step:16872 [D loss: 0.633176, acc: 64.06%] [G loss: 1.920609]\n",
      "epoch:18 step:16873 [D loss: 0.601222, acc: 64.06%] [G loss: 2.009632]\n",
      "epoch:18 step:16874 [D loss: 0.625268, acc: 60.94%] [G loss: 1.971824]\n",
      "epoch:18 step:16875 [D loss: 0.671067, acc: 64.06%] [G loss: 2.121246]\n",
      "epoch:18 step:16876 [D loss: 0.630332, acc: 71.09%] [G loss: 1.990214]\n",
      "epoch:18 step:16877 [D loss: 0.590286, acc: 65.62%] [G loss: 1.938172]\n",
      "epoch:18 step:16878 [D loss: 0.641460, acc: 60.94%] [G loss: 1.803481]\n",
      "epoch:18 step:16879 [D loss: 0.644931, acc: 62.50%] [G loss: 1.959213]\n",
      "epoch:18 step:16880 [D loss: 0.667852, acc: 63.28%] [G loss: 1.874599]\n",
      "epoch:18 step:16881 [D loss: 0.624860, acc: 67.19%] [G loss: 2.078074]\n",
      "epoch:18 step:16882 [D loss: 0.583734, acc: 70.31%] [G loss: 1.974105]\n",
      "epoch:18 step:16883 [D loss: 0.639086, acc: 65.62%] [G loss: 1.795044]\n",
      "epoch:18 step:16884 [D loss: 0.617427, acc: 63.28%] [G loss: 1.865798]\n",
      "epoch:18 step:16885 [D loss: 0.655708, acc: 61.72%] [G loss: 1.844363]\n",
      "epoch:18 step:16886 [D loss: 0.741658, acc: 52.34%] [G loss: 1.703222]\n",
      "epoch:18 step:16887 [D loss: 0.643966, acc: 66.41%] [G loss: 1.829372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:16888 [D loss: 0.678150, acc: 59.38%] [G loss: 1.819302]\n",
      "epoch:18 step:16889 [D loss: 0.655617, acc: 64.84%] [G loss: 2.199804]\n",
      "epoch:18 step:16890 [D loss: 0.628036, acc: 64.84%] [G loss: 1.985986]\n",
      "epoch:18 step:16891 [D loss: 0.630081, acc: 67.19%] [G loss: 2.001927]\n",
      "epoch:18 step:16892 [D loss: 0.705736, acc: 58.59%] [G loss: 1.858645]\n",
      "epoch:18 step:16893 [D loss: 0.675128, acc: 54.69%] [G loss: 1.964888]\n",
      "epoch:18 step:16894 [D loss: 0.633142, acc: 60.16%] [G loss: 1.872295]\n",
      "epoch:18 step:16895 [D loss: 0.638557, acc: 68.75%] [G loss: 1.877198]\n",
      "epoch:18 step:16896 [D loss: 0.721273, acc: 56.25%] [G loss: 1.948505]\n",
      "epoch:18 step:16897 [D loss: 0.715194, acc: 51.56%] [G loss: 1.716391]\n",
      "epoch:18 step:16898 [D loss: 0.661578, acc: 57.81%] [G loss: 1.744813]\n",
      "epoch:18 step:16899 [D loss: 0.664048, acc: 60.16%] [G loss: 1.877821]\n",
      "epoch:18 step:16900 [D loss: 0.666713, acc: 59.38%] [G loss: 1.776404]\n",
      "epoch:18 step:16901 [D loss: 0.705228, acc: 54.69%] [G loss: 1.840574]\n",
      "epoch:18 step:16902 [D loss: 0.598964, acc: 69.53%] [G loss: 1.889402]\n",
      "epoch:18 step:16903 [D loss: 0.630145, acc: 63.28%] [G loss: 1.890926]\n",
      "epoch:18 step:16904 [D loss: 0.639042, acc: 67.97%] [G loss: 1.846269]\n",
      "epoch:18 step:16905 [D loss: 0.646044, acc: 61.72%] [G loss: 1.964558]\n",
      "epoch:18 step:16906 [D loss: 0.667369, acc: 61.72%] [G loss: 2.046326]\n",
      "epoch:18 step:16907 [D loss: 0.622674, acc: 64.06%] [G loss: 1.889373]\n",
      "epoch:18 step:16908 [D loss: 0.633788, acc: 62.50%] [G loss: 1.880533]\n",
      "epoch:18 step:16909 [D loss: 0.606697, acc: 66.41%] [G loss: 1.988291]\n",
      "epoch:18 step:16910 [D loss: 0.649104, acc: 60.16%] [G loss: 1.812197]\n",
      "epoch:18 step:16911 [D loss: 0.672365, acc: 60.94%] [G loss: 1.876637]\n",
      "epoch:18 step:16912 [D loss: 0.632715, acc: 65.62%] [G loss: 1.893578]\n",
      "epoch:18 step:16913 [D loss: 0.582130, acc: 71.88%] [G loss: 1.966380]\n",
      "epoch:18 step:16914 [D loss: 0.592631, acc: 72.66%] [G loss: 1.933666]\n",
      "epoch:18 step:16915 [D loss: 0.674128, acc: 58.59%] [G loss: 1.938116]\n",
      "epoch:18 step:16916 [D loss: 0.584456, acc: 68.75%] [G loss: 1.889363]\n",
      "epoch:18 step:16917 [D loss: 0.647264, acc: 63.28%] [G loss: 1.948667]\n",
      "epoch:18 step:16918 [D loss: 0.636715, acc: 71.09%] [G loss: 1.902560]\n",
      "epoch:18 step:16919 [D loss: 0.637539, acc: 59.38%] [G loss: 2.133806]\n",
      "epoch:18 step:16920 [D loss: 0.648019, acc: 61.72%] [G loss: 1.861224]\n",
      "epoch:18 step:16921 [D loss: 0.621813, acc: 63.28%] [G loss: 2.053793]\n",
      "epoch:18 step:16922 [D loss: 0.633773, acc: 64.06%] [G loss: 1.930507]\n",
      "epoch:18 step:16923 [D loss: 0.613174, acc: 67.19%] [G loss: 1.991600]\n",
      "epoch:18 step:16924 [D loss: 0.610255, acc: 70.31%] [G loss: 1.910566]\n",
      "epoch:18 step:16925 [D loss: 0.642077, acc: 64.06%] [G loss: 1.869459]\n",
      "epoch:18 step:16926 [D loss: 0.641481, acc: 63.28%] [G loss: 1.873146]\n",
      "epoch:18 step:16927 [D loss: 0.614620, acc: 62.50%] [G loss: 1.880655]\n",
      "epoch:18 step:16928 [D loss: 0.654692, acc: 60.16%] [G loss: 2.101786]\n",
      "epoch:18 step:16929 [D loss: 0.703482, acc: 61.72%] [G loss: 1.807229]\n",
      "epoch:18 step:16930 [D loss: 0.646044, acc: 64.06%] [G loss: 1.892826]\n",
      "epoch:18 step:16931 [D loss: 0.608822, acc: 67.19%] [G loss: 1.928602]\n",
      "epoch:18 step:16932 [D loss: 0.651961, acc: 64.06%] [G loss: 1.915418]\n",
      "epoch:18 step:16933 [D loss: 0.647854, acc: 59.38%] [G loss: 1.875426]\n",
      "epoch:18 step:16934 [D loss: 0.692778, acc: 54.69%] [G loss: 1.846752]\n",
      "epoch:18 step:16935 [D loss: 0.571483, acc: 74.22%] [G loss: 2.110182]\n",
      "epoch:18 step:16936 [D loss: 0.589158, acc: 70.31%] [G loss: 1.955856]\n",
      "epoch:18 step:16937 [D loss: 0.699729, acc: 53.91%] [G loss: 1.726670]\n",
      "epoch:18 step:16938 [D loss: 0.620211, acc: 69.53%] [G loss: 1.978682]\n",
      "epoch:18 step:16939 [D loss: 0.655473, acc: 58.59%] [G loss: 1.781174]\n",
      "epoch:18 step:16940 [D loss: 0.650029, acc: 60.94%] [G loss: 1.932711]\n",
      "epoch:18 step:16941 [D loss: 0.655583, acc: 61.72%] [G loss: 2.138866]\n",
      "epoch:18 step:16942 [D loss: 0.650542, acc: 63.28%] [G loss: 2.006987]\n",
      "epoch:18 step:16943 [D loss: 0.616741, acc: 62.50%] [G loss: 2.070267]\n",
      "epoch:18 step:16944 [D loss: 0.680826, acc: 58.59%] [G loss: 1.807675]\n",
      "epoch:18 step:16945 [D loss: 0.654546, acc: 57.81%] [G loss: 1.931035]\n",
      "epoch:18 step:16946 [D loss: 0.681087, acc: 62.50%] [G loss: 1.852866]\n",
      "epoch:18 step:16947 [D loss: 0.664264, acc: 59.38%] [G loss: 1.741656]\n",
      "epoch:18 step:16948 [D loss: 0.673048, acc: 61.72%] [G loss: 1.824734]\n",
      "epoch:18 step:16949 [D loss: 0.637637, acc: 63.28%] [G loss: 1.825157]\n",
      "epoch:18 step:16950 [D loss: 0.619441, acc: 60.94%] [G loss: 1.923374]\n",
      "epoch:18 step:16951 [D loss: 0.666955, acc: 59.38%] [G loss: 1.828556]\n",
      "epoch:18 step:16952 [D loss: 0.592928, acc: 65.62%] [G loss: 1.849533]\n",
      "epoch:18 step:16953 [D loss: 0.639367, acc: 67.97%] [G loss: 1.962829]\n",
      "epoch:18 step:16954 [D loss: 0.617772, acc: 64.06%] [G loss: 2.045058]\n",
      "epoch:18 step:16955 [D loss: 0.667334, acc: 62.50%] [G loss: 1.856238]\n",
      "epoch:18 step:16956 [D loss: 0.588139, acc: 71.88%] [G loss: 1.824217]\n",
      "epoch:18 step:16957 [D loss: 0.644802, acc: 59.38%] [G loss: 1.824012]\n",
      "epoch:18 step:16958 [D loss: 0.675976, acc: 62.50%] [G loss: 1.876748]\n",
      "epoch:18 step:16959 [D loss: 0.597040, acc: 68.75%] [G loss: 1.948938]\n",
      "epoch:18 step:16960 [D loss: 0.648659, acc: 62.50%] [G loss: 1.910139]\n",
      "epoch:18 step:16961 [D loss: 0.655384, acc: 60.94%] [G loss: 1.834867]\n",
      "epoch:18 step:16962 [D loss: 0.632165, acc: 63.28%] [G loss: 1.893865]\n",
      "epoch:18 step:16963 [D loss: 0.639090, acc: 66.41%] [G loss: 1.936472]\n",
      "epoch:18 step:16964 [D loss: 0.624642, acc: 65.62%] [G loss: 1.785551]\n",
      "epoch:18 step:16965 [D loss: 0.713891, acc: 49.22%] [G loss: 1.852190]\n",
      "epoch:18 step:16966 [D loss: 0.680429, acc: 58.59%] [G loss: 1.807667]\n",
      "epoch:18 step:16967 [D loss: 0.647776, acc: 62.50%] [G loss: 1.901569]\n",
      "epoch:18 step:16968 [D loss: 0.676255, acc: 59.38%] [G loss: 1.927529]\n",
      "epoch:18 step:16969 [D loss: 0.661348, acc: 60.16%] [G loss: 1.778933]\n",
      "epoch:18 step:16970 [D loss: 0.666009, acc: 64.06%] [G loss: 1.983765]\n",
      "epoch:18 step:16971 [D loss: 0.640166, acc: 63.28%] [G loss: 1.921206]\n",
      "epoch:18 step:16972 [D loss: 0.666142, acc: 64.06%] [G loss: 2.026201]\n",
      "epoch:18 step:16973 [D loss: 0.641524, acc: 57.81%] [G loss: 1.994582]\n",
      "epoch:18 step:16974 [D loss: 0.699539, acc: 57.81%] [G loss: 1.752727]\n",
      "epoch:18 step:16975 [D loss: 0.696952, acc: 57.03%] [G loss: 1.800119]\n",
      "epoch:18 step:16976 [D loss: 0.665117, acc: 61.72%] [G loss: 1.918653]\n",
      "epoch:18 step:16977 [D loss: 0.680400, acc: 56.25%] [G loss: 1.842677]\n",
      "epoch:18 step:16978 [D loss: 0.627261, acc: 63.28%] [G loss: 1.947377]\n",
      "epoch:18 step:16979 [D loss: 0.677672, acc: 58.59%] [G loss: 2.037180]\n",
      "epoch:18 step:16980 [D loss: 0.616854, acc: 71.09%] [G loss: 2.016133]\n",
      "epoch:18 step:16981 [D loss: 0.632984, acc: 67.97%] [G loss: 1.964282]\n",
      "epoch:18 step:16982 [D loss: 0.633255, acc: 64.06%] [G loss: 2.070288]\n",
      "epoch:18 step:16983 [D loss: 0.572462, acc: 72.66%] [G loss: 2.168001]\n",
      "epoch:18 step:16984 [D loss: 0.614954, acc: 70.31%] [G loss: 1.875471]\n",
      "epoch:18 step:16985 [D loss: 0.575367, acc: 71.09%] [G loss: 2.230684]\n",
      "epoch:18 step:16986 [D loss: 0.705535, acc: 56.25%] [G loss: 1.869740]\n",
      "epoch:18 step:16987 [D loss: 0.641507, acc: 60.16%] [G loss: 1.961626]\n",
      "epoch:18 step:16988 [D loss: 0.634320, acc: 64.06%] [G loss: 2.008022]\n",
      "epoch:18 step:16989 [D loss: 0.659034, acc: 60.94%] [G loss: 1.943662]\n",
      "epoch:18 step:16990 [D loss: 0.709081, acc: 53.12%] [G loss: 1.978604]\n",
      "epoch:18 step:16991 [D loss: 0.694455, acc: 62.50%] [G loss: 1.810401]\n",
      "epoch:18 step:16992 [D loss: 0.639163, acc: 63.28%] [G loss: 1.973150]\n",
      "epoch:18 step:16993 [D loss: 0.721245, acc: 52.34%] [G loss: 1.818364]\n",
      "epoch:18 step:16994 [D loss: 0.635812, acc: 66.41%] [G loss: 1.818830]\n",
      "epoch:18 step:16995 [D loss: 0.644035, acc: 63.28%] [G loss: 1.711286]\n",
      "epoch:18 step:16996 [D loss: 0.628766, acc: 67.97%] [G loss: 1.819537]\n",
      "epoch:18 step:16997 [D loss: 0.613962, acc: 68.75%] [G loss: 1.846051]\n",
      "epoch:18 step:16998 [D loss: 0.681101, acc: 57.81%] [G loss: 1.810994]\n",
      "epoch:18 step:16999 [D loss: 0.705098, acc: 48.44%] [G loss: 1.757570]\n",
      "epoch:18 step:17000 [D loss: 0.659360, acc: 64.06%] [G loss: 1.796867]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.314462\n",
      "FID: 14.097105\n",
      "0 = 12.835660151052469\n",
      "1 = 0.09098882321478945\n",
      "2 = 0.8920000195503235\n",
      "3 = 0.9092000126838684\n",
      "4 = 0.8748000264167786\n",
      "5 = 0.8789636492729187\n",
      "6 = 0.9092000126838684\n",
      "7 = 6.508385849308972\n",
      "8 = 0.07289386574429273\n",
      "9 = 0.7296000123023987\n",
      "10 = 0.7404000163078308\n",
      "11 = 0.7188000082969666\n",
      "12 = 0.724745512008667\n",
      "13 = 0.7404000163078308\n",
      "14 = 7.314497947692871\n",
      "15 = 9.458272933959961\n",
      "16 = 0.11411947757005692\n",
      "17 = 7.314462184906006\n",
      "18 = 14.097105026245117\n",
      "epoch:18 step:17001 [D loss: 0.671902, acc: 57.03%] [G loss: 1.810528]\n",
      "epoch:18 step:17002 [D loss: 0.630575, acc: 61.72%] [G loss: 1.733909]\n",
      "epoch:18 step:17003 [D loss: 0.673692, acc: 64.06%] [G loss: 1.754841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17004 [D loss: 0.647671, acc: 61.72%] [G loss: 1.737885]\n",
      "epoch:18 step:17005 [D loss: 0.679262, acc: 59.38%] [G loss: 1.793483]\n",
      "epoch:18 step:17006 [D loss: 0.674163, acc: 55.47%] [G loss: 1.832675]\n",
      "epoch:18 step:17007 [D loss: 0.613639, acc: 64.84%] [G loss: 1.786394]\n",
      "epoch:18 step:17008 [D loss: 0.662024, acc: 57.03%] [G loss: 1.820987]\n",
      "epoch:18 step:17009 [D loss: 0.663589, acc: 58.59%] [G loss: 1.809154]\n",
      "epoch:18 step:17010 [D loss: 0.628625, acc: 66.41%] [G loss: 1.788939]\n",
      "epoch:18 step:17011 [D loss: 0.662825, acc: 58.59%] [G loss: 1.940326]\n",
      "epoch:18 step:17012 [D loss: 0.604174, acc: 67.19%] [G loss: 1.832888]\n",
      "epoch:18 step:17013 [D loss: 0.684344, acc: 53.91%] [G loss: 1.792361]\n",
      "epoch:18 step:17014 [D loss: 0.664902, acc: 58.59%] [G loss: 1.761339]\n",
      "epoch:18 step:17015 [D loss: 0.642077, acc: 61.72%] [G loss: 1.896315]\n",
      "epoch:18 step:17016 [D loss: 0.644633, acc: 60.16%] [G loss: 1.898973]\n",
      "epoch:18 step:17017 [D loss: 0.618780, acc: 69.53%] [G loss: 1.885363]\n",
      "epoch:18 step:17018 [D loss: 0.645072, acc: 64.84%] [G loss: 1.834203]\n",
      "epoch:18 step:17019 [D loss: 0.659595, acc: 59.38%] [G loss: 1.835959]\n",
      "epoch:18 step:17020 [D loss: 0.606157, acc: 64.06%] [G loss: 2.034959]\n",
      "epoch:18 step:17021 [D loss: 0.620328, acc: 69.53%] [G loss: 1.900868]\n",
      "epoch:18 step:17022 [D loss: 0.656661, acc: 57.81%] [G loss: 1.882105]\n",
      "epoch:18 step:17023 [D loss: 0.628502, acc: 67.19%] [G loss: 1.810762]\n",
      "epoch:18 step:17024 [D loss: 0.646166, acc: 62.50%] [G loss: 1.869286]\n",
      "epoch:18 step:17025 [D loss: 0.675422, acc: 57.03%] [G loss: 1.858048]\n",
      "epoch:18 step:17026 [D loss: 0.654510, acc: 63.28%] [G loss: 1.697015]\n",
      "epoch:18 step:17027 [D loss: 0.640536, acc: 64.06%] [G loss: 1.834668]\n",
      "epoch:18 step:17028 [D loss: 0.658459, acc: 65.62%] [G loss: 1.865598]\n",
      "epoch:18 step:17029 [D loss: 0.650342, acc: 61.72%] [G loss: 1.838732]\n",
      "epoch:18 step:17030 [D loss: 0.611650, acc: 66.41%] [G loss: 1.888935]\n",
      "epoch:18 step:17031 [D loss: 0.610324, acc: 68.75%] [G loss: 1.939963]\n",
      "epoch:18 step:17032 [D loss: 0.673967, acc: 62.50%] [G loss: 1.981629]\n",
      "epoch:18 step:17033 [D loss: 0.633049, acc: 64.06%] [G loss: 1.821007]\n",
      "epoch:18 step:17034 [D loss: 0.593442, acc: 67.19%] [G loss: 1.934225]\n",
      "epoch:18 step:17035 [D loss: 0.649152, acc: 60.16%] [G loss: 1.928366]\n",
      "epoch:18 step:17036 [D loss: 0.642143, acc: 61.72%] [G loss: 1.946755]\n",
      "epoch:18 step:17037 [D loss: 0.611351, acc: 67.97%] [G loss: 1.897794]\n",
      "epoch:18 step:17038 [D loss: 0.698895, acc: 51.56%] [G loss: 1.779708]\n",
      "epoch:18 step:17039 [D loss: 0.610088, acc: 70.31%] [G loss: 1.875903]\n",
      "epoch:18 step:17040 [D loss: 0.664027, acc: 57.03%] [G loss: 1.788884]\n",
      "epoch:18 step:17041 [D loss: 0.671054, acc: 57.81%] [G loss: 1.867074]\n",
      "epoch:18 step:17042 [D loss: 0.657833, acc: 60.16%] [G loss: 1.824857]\n",
      "epoch:18 step:17043 [D loss: 0.647183, acc: 61.72%] [G loss: 1.821584]\n",
      "epoch:18 step:17044 [D loss: 0.651132, acc: 57.81%] [G loss: 1.775666]\n",
      "epoch:18 step:17045 [D loss: 0.662101, acc: 58.59%] [G loss: 1.852688]\n",
      "epoch:18 step:17046 [D loss: 0.641684, acc: 66.41%] [G loss: 1.834507]\n",
      "epoch:18 step:17047 [D loss: 0.663391, acc: 64.06%] [G loss: 1.889537]\n",
      "epoch:18 step:17048 [D loss: 0.672251, acc: 59.38%] [G loss: 1.926688]\n",
      "epoch:18 step:17049 [D loss: 0.658003, acc: 57.03%] [G loss: 1.930166]\n",
      "epoch:18 step:17050 [D loss: 0.663703, acc: 62.50%] [G loss: 1.926367]\n",
      "epoch:18 step:17051 [D loss: 0.644943, acc: 60.94%] [G loss: 1.895362]\n",
      "epoch:18 step:17052 [D loss: 0.625887, acc: 64.84%] [G loss: 1.852083]\n",
      "epoch:18 step:17053 [D loss: 0.631075, acc: 66.41%] [G loss: 1.969503]\n",
      "epoch:18 step:17054 [D loss: 0.628907, acc: 63.28%] [G loss: 2.010046]\n",
      "epoch:18 step:17055 [D loss: 0.628136, acc: 67.19%] [G loss: 1.778862]\n",
      "epoch:18 step:17056 [D loss: 0.673690, acc: 55.47%] [G loss: 1.941327]\n",
      "epoch:18 step:17057 [D loss: 0.609035, acc: 70.31%] [G loss: 1.860672]\n",
      "epoch:18 step:17058 [D loss: 0.571240, acc: 72.66%] [G loss: 1.956298]\n",
      "epoch:18 step:17059 [D loss: 0.663717, acc: 60.94%] [G loss: 1.753105]\n",
      "epoch:18 step:17060 [D loss: 0.616554, acc: 66.41%] [G loss: 2.014374]\n",
      "epoch:18 step:17061 [D loss: 0.633866, acc: 61.72%] [G loss: 1.930394]\n",
      "epoch:18 step:17062 [D loss: 0.678596, acc: 58.59%] [G loss: 1.736549]\n",
      "epoch:18 step:17063 [D loss: 0.674544, acc: 66.41%] [G loss: 1.966861]\n",
      "epoch:18 step:17064 [D loss: 0.637499, acc: 67.97%] [G loss: 2.016538]\n",
      "epoch:18 step:17065 [D loss: 0.662950, acc: 62.50%] [G loss: 1.866730]\n",
      "epoch:18 step:17066 [D loss: 0.685501, acc: 57.03%] [G loss: 1.765949]\n",
      "epoch:18 step:17067 [D loss: 0.589991, acc: 71.09%] [G loss: 1.878397]\n",
      "epoch:18 step:17068 [D loss: 0.701578, acc: 52.34%] [G loss: 1.969955]\n",
      "epoch:18 step:17069 [D loss: 0.635330, acc: 60.94%] [G loss: 1.821571]\n",
      "epoch:18 step:17070 [D loss: 0.628057, acc: 64.06%] [G loss: 1.910195]\n",
      "epoch:18 step:17071 [D loss: 0.666099, acc: 60.94%] [G loss: 1.818279]\n",
      "epoch:18 step:17072 [D loss: 0.620178, acc: 64.84%] [G loss: 2.008436]\n",
      "epoch:18 step:17073 [D loss: 0.551437, acc: 76.56%] [G loss: 2.073504]\n",
      "epoch:18 step:17074 [D loss: 0.612213, acc: 66.41%] [G loss: 2.029729]\n",
      "epoch:18 step:17075 [D loss: 0.573301, acc: 70.31%] [G loss: 2.147924]\n",
      "epoch:18 step:17076 [D loss: 0.709981, acc: 56.25%] [G loss: 1.898497]\n",
      "epoch:18 step:17077 [D loss: 0.661783, acc: 60.94%] [G loss: 1.787118]\n",
      "epoch:18 step:17078 [D loss: 0.602115, acc: 69.53%] [G loss: 2.007831]\n",
      "epoch:18 step:17079 [D loss: 0.639246, acc: 67.97%] [G loss: 1.872934]\n",
      "epoch:18 step:17080 [D loss: 0.693670, acc: 57.81%] [G loss: 1.788736]\n",
      "epoch:18 step:17081 [D loss: 0.641077, acc: 62.50%] [G loss: 1.858676]\n",
      "epoch:18 step:17082 [D loss: 0.679540, acc: 59.38%] [G loss: 1.905906]\n",
      "epoch:18 step:17083 [D loss: 0.666287, acc: 61.72%] [G loss: 1.987911]\n",
      "epoch:18 step:17084 [D loss: 0.639471, acc: 59.38%] [G loss: 1.855309]\n",
      "epoch:18 step:17085 [D loss: 0.589789, acc: 67.19%] [G loss: 2.050233]\n",
      "epoch:18 step:17086 [D loss: 0.689150, acc: 55.47%] [G loss: 1.778258]\n",
      "epoch:18 step:17087 [D loss: 0.670079, acc: 58.59%] [G loss: 1.945017]\n",
      "epoch:18 step:17088 [D loss: 0.694563, acc: 57.81%] [G loss: 1.966343]\n",
      "epoch:18 step:17089 [D loss: 0.642944, acc: 57.81%] [G loss: 1.850681]\n",
      "epoch:18 step:17090 [D loss: 0.708901, acc: 59.38%] [G loss: 1.837978]\n",
      "epoch:18 step:17091 [D loss: 0.649311, acc: 60.16%] [G loss: 1.935188]\n",
      "epoch:18 step:17092 [D loss: 0.660179, acc: 54.69%] [G loss: 1.821638]\n",
      "epoch:18 step:17093 [D loss: 0.669250, acc: 54.69%] [G loss: 1.805643]\n",
      "epoch:18 step:17094 [D loss: 0.654638, acc: 63.28%] [G loss: 1.683192]\n",
      "epoch:18 step:17095 [D loss: 0.634281, acc: 63.28%] [G loss: 1.843899]\n",
      "epoch:18 step:17096 [D loss: 0.586128, acc: 69.53%] [G loss: 1.954682]\n",
      "epoch:18 step:17097 [D loss: 0.611092, acc: 67.97%] [G loss: 2.204453]\n",
      "epoch:18 step:17098 [D loss: 0.617331, acc: 69.53%] [G loss: 2.091956]\n",
      "epoch:18 step:17099 [D loss: 0.655535, acc: 66.41%] [G loss: 1.839742]\n",
      "epoch:18 step:17100 [D loss: 0.645072, acc: 65.62%] [G loss: 1.884626]\n",
      "epoch:18 step:17101 [D loss: 0.664558, acc: 56.25%] [G loss: 1.976033]\n",
      "epoch:18 step:17102 [D loss: 0.659752, acc: 59.38%] [G loss: 1.897243]\n",
      "epoch:18 step:17103 [D loss: 0.628954, acc: 67.97%] [G loss: 1.925006]\n",
      "epoch:18 step:17104 [D loss: 0.710808, acc: 54.69%] [G loss: 1.909501]\n",
      "epoch:18 step:17105 [D loss: 0.626378, acc: 62.50%] [G loss: 1.815843]\n",
      "epoch:18 step:17106 [D loss: 0.650697, acc: 61.72%] [G loss: 1.904298]\n",
      "epoch:18 step:17107 [D loss: 0.631058, acc: 64.06%] [G loss: 1.965913]\n",
      "epoch:18 step:17108 [D loss: 0.635235, acc: 67.19%] [G loss: 1.882358]\n",
      "epoch:18 step:17109 [D loss: 0.661058, acc: 60.94%] [G loss: 1.833030]\n",
      "epoch:18 step:17110 [D loss: 0.651259, acc: 58.59%] [G loss: 1.856282]\n",
      "epoch:18 step:17111 [D loss: 0.627874, acc: 64.06%] [G loss: 1.961772]\n",
      "epoch:18 step:17112 [D loss: 0.661186, acc: 62.50%] [G loss: 1.975492]\n",
      "epoch:18 step:17113 [D loss: 0.637938, acc: 67.97%] [G loss: 2.039776]\n",
      "epoch:18 step:17114 [D loss: 0.663809, acc: 62.50%] [G loss: 1.981066]\n",
      "epoch:18 step:17115 [D loss: 0.715293, acc: 57.03%] [G loss: 1.776855]\n",
      "epoch:18 step:17116 [D loss: 0.696967, acc: 54.69%] [G loss: 1.751934]\n",
      "epoch:18 step:17117 [D loss: 0.675850, acc: 54.69%] [G loss: 1.777863]\n",
      "epoch:18 step:17118 [D loss: 0.709493, acc: 54.69%] [G loss: 1.771520]\n",
      "epoch:18 step:17119 [D loss: 0.672664, acc: 55.47%] [G loss: 1.779049]\n",
      "epoch:18 step:17120 [D loss: 0.675955, acc: 57.81%] [G loss: 1.797673]\n",
      "epoch:18 step:17121 [D loss: 0.651101, acc: 64.84%] [G loss: 1.874616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17122 [D loss: 0.639666, acc: 64.06%] [G loss: 1.713098]\n",
      "epoch:18 step:17123 [D loss: 0.696339, acc: 59.38%] [G loss: 1.722721]\n",
      "epoch:18 step:17124 [D loss: 0.634722, acc: 59.38%] [G loss: 1.862086]\n",
      "epoch:18 step:17125 [D loss: 0.658868, acc: 61.72%] [G loss: 1.617518]\n",
      "epoch:18 step:17126 [D loss: 0.635567, acc: 65.62%] [G loss: 1.894814]\n",
      "epoch:18 step:17127 [D loss: 0.657228, acc: 57.81%] [G loss: 1.831126]\n",
      "epoch:18 step:17128 [D loss: 0.661677, acc: 58.59%] [G loss: 1.947513]\n",
      "epoch:18 step:17129 [D loss: 0.660939, acc: 58.59%] [G loss: 1.912311]\n",
      "epoch:18 step:17130 [D loss: 0.592724, acc: 70.31%] [G loss: 2.031423]\n",
      "epoch:18 step:17131 [D loss: 0.648338, acc: 64.84%] [G loss: 1.888241]\n",
      "epoch:18 step:17132 [D loss: 0.667813, acc: 57.81%] [G loss: 1.782476]\n",
      "epoch:18 step:17133 [D loss: 0.702470, acc: 55.47%] [G loss: 1.711160]\n",
      "epoch:18 step:17134 [D loss: 0.635159, acc: 67.19%] [G loss: 1.798659]\n",
      "epoch:18 step:17135 [D loss: 0.638537, acc: 65.62%] [G loss: 1.981066]\n",
      "epoch:18 step:17136 [D loss: 0.614726, acc: 62.50%] [G loss: 2.087836]\n",
      "epoch:18 step:17137 [D loss: 0.641411, acc: 62.50%] [G loss: 1.880087]\n",
      "epoch:18 step:17138 [D loss: 0.562572, acc: 74.22%] [G loss: 1.942282]\n",
      "epoch:18 step:17139 [D loss: 0.693384, acc: 56.25%] [G loss: 1.848014]\n",
      "epoch:18 step:17140 [D loss: 0.625057, acc: 64.06%] [G loss: 2.160831]\n",
      "epoch:18 step:17141 [D loss: 0.588848, acc: 67.97%] [G loss: 1.987830]\n",
      "epoch:18 step:17142 [D loss: 0.617485, acc: 63.28%] [G loss: 1.960875]\n",
      "epoch:18 step:17143 [D loss: 0.576803, acc: 68.75%] [G loss: 1.916711]\n",
      "epoch:18 step:17144 [D loss: 0.643693, acc: 58.59%] [G loss: 1.875945]\n",
      "epoch:18 step:17145 [D loss: 0.643767, acc: 63.28%] [G loss: 1.947092]\n",
      "epoch:18 step:17146 [D loss: 0.611879, acc: 67.19%] [G loss: 1.867508]\n",
      "epoch:18 step:17147 [D loss: 0.689763, acc: 56.25%] [G loss: 1.730003]\n",
      "epoch:18 step:17148 [D loss: 0.657574, acc: 55.47%] [G loss: 1.782665]\n",
      "epoch:18 step:17149 [D loss: 0.654724, acc: 65.62%] [G loss: 1.811576]\n",
      "epoch:18 step:17150 [D loss: 0.694791, acc: 63.28%] [G loss: 1.815486]\n",
      "epoch:18 step:17151 [D loss: 0.679499, acc: 60.16%] [G loss: 1.774906]\n",
      "epoch:18 step:17152 [D loss: 0.616273, acc: 65.62%] [G loss: 1.897873]\n",
      "epoch:18 step:17153 [D loss: 0.619285, acc: 67.19%] [G loss: 1.800618]\n",
      "epoch:18 step:17154 [D loss: 0.658579, acc: 62.50%] [G loss: 1.915678]\n",
      "epoch:18 step:17155 [D loss: 0.652236, acc: 58.59%] [G loss: 1.899734]\n",
      "epoch:18 step:17156 [D loss: 0.667316, acc: 65.62%] [G loss: 1.722558]\n",
      "epoch:18 step:17157 [D loss: 0.684474, acc: 61.72%] [G loss: 1.873687]\n",
      "epoch:18 step:17158 [D loss: 0.633066, acc: 64.84%] [G loss: 1.941016]\n",
      "epoch:18 step:17159 [D loss: 0.590160, acc: 68.75%] [G loss: 2.009439]\n",
      "epoch:18 step:17160 [D loss: 0.718626, acc: 52.34%] [G loss: 1.850987]\n",
      "epoch:18 step:17161 [D loss: 0.679935, acc: 58.59%] [G loss: 1.816370]\n",
      "epoch:18 step:17162 [D loss: 0.662700, acc: 61.72%] [G loss: 1.859656]\n",
      "epoch:18 step:17163 [D loss: 0.606097, acc: 64.06%] [G loss: 1.819788]\n",
      "epoch:18 step:17164 [D loss: 0.614599, acc: 66.41%] [G loss: 1.879221]\n",
      "epoch:18 step:17165 [D loss: 0.605572, acc: 67.19%] [G loss: 1.971937]\n",
      "epoch:18 step:17166 [D loss: 0.634249, acc: 66.41%] [G loss: 1.906987]\n",
      "epoch:18 step:17167 [D loss: 0.690313, acc: 59.38%] [G loss: 1.884717]\n",
      "epoch:18 step:17168 [D loss: 0.601247, acc: 64.06%] [G loss: 1.878726]\n",
      "epoch:18 step:17169 [D loss: 0.660512, acc: 60.94%] [G loss: 1.823165]\n",
      "epoch:18 step:17170 [D loss: 0.665464, acc: 64.84%] [G loss: 1.929743]\n",
      "epoch:18 step:17171 [D loss: 0.616010, acc: 70.31%] [G loss: 1.891272]\n",
      "epoch:18 step:17172 [D loss: 0.587579, acc: 71.09%] [G loss: 1.970743]\n",
      "epoch:18 step:17173 [D loss: 0.643100, acc: 60.16%] [G loss: 1.856221]\n",
      "epoch:18 step:17174 [D loss: 0.656549, acc: 65.62%] [G loss: 1.847401]\n",
      "epoch:18 step:17175 [D loss: 0.648464, acc: 65.62%] [G loss: 1.855101]\n",
      "epoch:18 step:17176 [D loss: 0.655566, acc: 64.84%] [G loss: 1.989800]\n",
      "epoch:18 step:17177 [D loss: 0.598599, acc: 65.62%] [G loss: 1.862659]\n",
      "epoch:18 step:17178 [D loss: 0.633456, acc: 65.62%] [G loss: 2.171207]\n",
      "epoch:18 step:17179 [D loss: 0.580182, acc: 70.31%] [G loss: 1.903438]\n",
      "epoch:18 step:17180 [D loss: 0.554418, acc: 74.22%] [G loss: 2.192640]\n",
      "epoch:18 step:17181 [D loss: 0.553535, acc: 77.34%] [G loss: 2.116860]\n",
      "epoch:18 step:17182 [D loss: 0.714502, acc: 52.34%] [G loss: 1.780334]\n",
      "epoch:18 step:17183 [D loss: 0.722929, acc: 57.81%] [G loss: 1.866114]\n",
      "epoch:18 step:17184 [D loss: 0.624875, acc: 66.41%] [G loss: 2.115449]\n",
      "epoch:18 step:17185 [D loss: 0.631965, acc: 67.19%] [G loss: 1.884648]\n",
      "epoch:18 step:17186 [D loss: 0.639117, acc: 59.38%] [G loss: 1.832806]\n",
      "epoch:18 step:17187 [D loss: 0.642484, acc: 62.50%] [G loss: 1.987700]\n",
      "epoch:18 step:17188 [D loss: 0.652817, acc: 63.28%] [G loss: 1.913127]\n",
      "epoch:18 step:17189 [D loss: 0.660810, acc: 62.50%] [G loss: 1.808064]\n",
      "epoch:18 step:17190 [D loss: 0.671492, acc: 60.94%] [G loss: 1.957227]\n",
      "epoch:18 step:17191 [D loss: 0.681921, acc: 56.25%] [G loss: 1.861324]\n",
      "epoch:18 step:17192 [D loss: 0.672307, acc: 61.72%] [G loss: 1.763694]\n",
      "epoch:18 step:17193 [D loss: 0.654057, acc: 60.94%] [G loss: 1.818239]\n",
      "epoch:18 step:17194 [D loss: 0.683244, acc: 53.91%] [G loss: 1.844572]\n",
      "epoch:18 step:17195 [D loss: 0.678300, acc: 59.38%] [G loss: 1.745721]\n",
      "epoch:18 step:17196 [D loss: 0.587155, acc: 71.09%] [G loss: 1.902486]\n",
      "epoch:18 step:17197 [D loss: 0.575398, acc: 74.22%] [G loss: 1.948224]\n",
      "epoch:18 step:17198 [D loss: 0.587297, acc: 71.09%] [G loss: 2.034448]\n",
      "epoch:18 step:17199 [D loss: 0.667115, acc: 62.50%] [G loss: 1.946000]\n",
      "epoch:18 step:17200 [D loss: 0.664856, acc: 58.59%] [G loss: 2.049557]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.314849\n",
      "FID: 13.950047\n",
      "0 = 12.771908664512642\n",
      "1 = 0.08884169663596721\n",
      "2 = 0.8791999816894531\n",
      "3 = 0.8971999883651733\n",
      "4 = 0.8611999750137329\n",
      "5 = 0.8660231828689575\n",
      "6 = 0.8971999883651733\n",
      "7 = 6.5961373826265675\n",
      "8 = 0.07066511112276465\n",
      "9 = 0.7242000102996826\n",
      "10 = 0.739799976348877\n",
      "11 = 0.7085999846458435\n",
      "12 = 0.7174165844917297\n",
      "13 = 0.739799976348877\n",
      "14 = 7.314877986907959\n",
      "15 = 9.416709899902344\n",
      "16 = 0.12578098475933075\n",
      "17 = 7.314849376678467\n",
      "18 = 13.95004653930664\n",
      "epoch:18 step:17201 [D loss: 0.609542, acc: 67.19%] [G loss: 1.978800]\n",
      "epoch:18 step:17202 [D loss: 0.640075, acc: 62.50%] [G loss: 1.956503]\n",
      "epoch:18 step:17203 [D loss: 0.638713, acc: 60.94%] [G loss: 1.946977]\n",
      "epoch:18 step:17204 [D loss: 0.680941, acc: 60.94%] [G loss: 1.887590]\n",
      "epoch:18 step:17205 [D loss: 0.623047, acc: 66.41%] [G loss: 2.060927]\n",
      "epoch:18 step:17206 [D loss: 0.638311, acc: 62.50%] [G loss: 1.943755]\n",
      "epoch:18 step:17207 [D loss: 0.681523, acc: 59.38%] [G loss: 1.700685]\n",
      "epoch:18 step:17208 [D loss: 0.699089, acc: 56.25%] [G loss: 1.793482]\n",
      "epoch:18 step:17209 [D loss: 0.665521, acc: 57.81%] [G loss: 1.985807]\n",
      "epoch:18 step:17210 [D loss: 0.686491, acc: 52.34%] [G loss: 1.965567]\n",
      "epoch:18 step:17211 [D loss: 0.644972, acc: 59.38%] [G loss: 1.953257]\n",
      "epoch:18 step:17212 [D loss: 0.618488, acc: 66.41%] [G loss: 2.156280]\n",
      "epoch:18 step:17213 [D loss: 0.568877, acc: 75.00%] [G loss: 2.211302]\n",
      "epoch:18 step:17214 [D loss: 0.692422, acc: 60.94%] [G loss: 1.935372]\n",
      "epoch:18 step:17215 [D loss: 0.664501, acc: 61.72%] [G loss: 1.618071]\n",
      "epoch:18 step:17216 [D loss: 0.650301, acc: 62.50%] [G loss: 1.822388]\n",
      "epoch:18 step:17217 [D loss: 0.653937, acc: 61.72%] [G loss: 1.827407]\n",
      "epoch:18 step:17218 [D loss: 0.684354, acc: 57.81%] [G loss: 1.894934]\n",
      "epoch:18 step:17219 [D loss: 0.605679, acc: 70.31%] [G loss: 1.845916]\n",
      "epoch:18 step:17220 [D loss: 0.601426, acc: 65.62%] [G loss: 1.902118]\n",
      "epoch:18 step:17221 [D loss: 0.677371, acc: 59.38%] [G loss: 1.781515]\n",
      "epoch:18 step:17222 [D loss: 0.640718, acc: 62.50%] [G loss: 1.825595]\n",
      "epoch:18 step:17223 [D loss: 0.608414, acc: 64.84%] [G loss: 1.744182]\n",
      "epoch:18 step:17224 [D loss: 0.622065, acc: 65.62%] [G loss: 1.950109]\n",
      "epoch:18 step:17225 [D loss: 0.639565, acc: 62.50%] [G loss: 2.039208]\n",
      "epoch:18 step:17226 [D loss: 0.636438, acc: 64.06%] [G loss: 2.000283]\n",
      "epoch:18 step:17227 [D loss: 0.656959, acc: 60.16%] [G loss: 1.865120]\n",
      "epoch:18 step:17228 [D loss: 0.605319, acc: 67.97%] [G loss: 1.947876]\n",
      "epoch:18 step:17229 [D loss: 0.631508, acc: 64.84%] [G loss: 1.942002]\n",
      "epoch:18 step:17230 [D loss: 0.633832, acc: 63.28%] [G loss: 2.028501]\n",
      "epoch:18 step:17231 [D loss: 0.636343, acc: 62.50%] [G loss: 1.843944]\n",
      "epoch:18 step:17232 [D loss: 0.602010, acc: 68.75%] [G loss: 1.915690]\n",
      "epoch:18 step:17233 [D loss: 0.594201, acc: 69.53%] [G loss: 1.977473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17234 [D loss: 0.653273, acc: 55.47%] [G loss: 1.867261]\n",
      "epoch:18 step:17235 [D loss: 0.650593, acc: 60.94%] [G loss: 1.869328]\n",
      "epoch:18 step:17236 [D loss: 0.641982, acc: 65.62%] [G loss: 2.069343]\n",
      "epoch:18 step:17237 [D loss: 0.649789, acc: 57.81%] [G loss: 2.109318]\n",
      "epoch:18 step:17238 [D loss: 0.667120, acc: 64.06%] [G loss: 1.872955]\n",
      "epoch:18 step:17239 [D loss: 0.702194, acc: 51.56%] [G loss: 1.775095]\n",
      "epoch:18 step:17240 [D loss: 0.685087, acc: 58.59%] [G loss: 2.166631]\n",
      "epoch:18 step:17241 [D loss: 0.657502, acc: 55.47%] [G loss: 1.925521]\n",
      "epoch:18 step:17242 [D loss: 0.694874, acc: 57.81%] [G loss: 1.785942]\n",
      "epoch:18 step:17243 [D loss: 0.674774, acc: 60.16%] [G loss: 1.787677]\n",
      "epoch:18 step:17244 [D loss: 0.650844, acc: 57.03%] [G loss: 1.845744]\n",
      "epoch:18 step:17245 [D loss: 0.591495, acc: 70.31%] [G loss: 2.008826]\n",
      "epoch:18 step:17246 [D loss: 0.616056, acc: 65.62%] [G loss: 2.126650]\n",
      "epoch:18 step:17247 [D loss: 0.611240, acc: 67.19%] [G loss: 2.007056]\n",
      "epoch:18 step:17248 [D loss: 0.645384, acc: 59.38%] [G loss: 1.974689]\n",
      "epoch:18 step:17249 [D loss: 0.683903, acc: 56.25%] [G loss: 1.976133]\n",
      "epoch:18 step:17250 [D loss: 0.610033, acc: 71.09%] [G loss: 1.855001]\n",
      "epoch:18 step:17251 [D loss: 0.623818, acc: 66.41%] [G loss: 1.958860]\n",
      "epoch:18 step:17252 [D loss: 0.664135, acc: 60.16%] [G loss: 1.654411]\n",
      "epoch:18 step:17253 [D loss: 0.648945, acc: 63.28%] [G loss: 1.716080]\n",
      "epoch:18 step:17254 [D loss: 0.626765, acc: 62.50%] [G loss: 1.822316]\n",
      "epoch:18 step:17255 [D loss: 0.650167, acc: 64.84%] [G loss: 1.846878]\n",
      "epoch:18 step:17256 [D loss: 0.697678, acc: 56.25%] [G loss: 1.817805]\n",
      "epoch:18 step:17257 [D loss: 0.669180, acc: 63.28%] [G loss: 1.793742]\n",
      "epoch:18 step:17258 [D loss: 0.635732, acc: 60.94%] [G loss: 1.901277]\n",
      "epoch:18 step:17259 [D loss: 0.663419, acc: 63.28%] [G loss: 1.954035]\n",
      "epoch:18 step:17260 [D loss: 0.668645, acc: 63.28%] [G loss: 1.809999]\n",
      "epoch:18 step:17261 [D loss: 0.666268, acc: 57.03%] [G loss: 1.937568]\n",
      "epoch:18 step:17262 [D loss: 0.635582, acc: 64.06%] [G loss: 1.822798]\n",
      "epoch:18 step:17263 [D loss: 0.639091, acc: 60.16%] [G loss: 1.864630]\n",
      "epoch:18 step:17264 [D loss: 0.612954, acc: 67.97%] [G loss: 1.967579]\n",
      "epoch:18 step:17265 [D loss: 0.642825, acc: 61.72%] [G loss: 1.919154]\n",
      "epoch:18 step:17266 [D loss: 0.692642, acc: 56.25%] [G loss: 1.794780]\n",
      "epoch:18 step:17267 [D loss: 0.665129, acc: 61.72%] [G loss: 1.939427]\n",
      "epoch:18 step:17268 [D loss: 0.668089, acc: 57.81%] [G loss: 1.946322]\n",
      "epoch:18 step:17269 [D loss: 0.683668, acc: 63.28%] [G loss: 1.814837]\n",
      "epoch:18 step:17270 [D loss: 0.583789, acc: 69.53%] [G loss: 1.902302]\n",
      "epoch:18 step:17271 [D loss: 0.613138, acc: 67.19%] [G loss: 1.971703]\n",
      "epoch:18 step:17272 [D loss: 0.656866, acc: 64.84%] [G loss: 1.955511]\n",
      "epoch:18 step:17273 [D loss: 0.622328, acc: 65.62%] [G loss: 1.884977]\n",
      "epoch:18 step:17274 [D loss: 0.685289, acc: 60.94%] [G loss: 1.812632]\n",
      "epoch:18 step:17275 [D loss: 0.686971, acc: 60.94%] [G loss: 1.777378]\n",
      "epoch:18 step:17276 [D loss: 0.636612, acc: 67.19%] [G loss: 1.866231]\n",
      "epoch:18 step:17277 [D loss: 0.667372, acc: 53.91%] [G loss: 1.853097]\n",
      "epoch:18 step:17278 [D loss: 0.614470, acc: 67.97%] [G loss: 1.932894]\n",
      "epoch:18 step:17279 [D loss: 0.629623, acc: 64.84%] [G loss: 1.908736]\n",
      "epoch:18 step:17280 [D loss: 0.656988, acc: 62.50%] [G loss: 1.956344]\n",
      "epoch:18 step:17281 [D loss: 0.635516, acc: 70.31%] [G loss: 2.039100]\n",
      "epoch:18 step:17282 [D loss: 0.563486, acc: 71.09%] [G loss: 2.092896]\n",
      "epoch:18 step:17283 [D loss: 0.660365, acc: 64.06%] [G loss: 1.949642]\n",
      "epoch:18 step:17284 [D loss: 0.697192, acc: 53.91%] [G loss: 1.865791]\n",
      "epoch:18 step:17285 [D loss: 0.656151, acc: 60.16%] [G loss: 1.857683]\n",
      "epoch:18 step:17286 [D loss: 0.667300, acc: 59.38%] [G loss: 1.907866]\n",
      "epoch:18 step:17287 [D loss: 0.713411, acc: 47.66%] [G loss: 1.810525]\n",
      "epoch:18 step:17288 [D loss: 0.674105, acc: 60.94%] [G loss: 1.830226]\n",
      "epoch:18 step:17289 [D loss: 0.702098, acc: 50.00%] [G loss: 1.835745]\n",
      "epoch:18 step:17290 [D loss: 0.702394, acc: 58.59%] [G loss: 1.782444]\n",
      "epoch:18 step:17291 [D loss: 0.670627, acc: 57.81%] [G loss: 1.906847]\n",
      "epoch:18 step:17292 [D loss: 0.607372, acc: 67.19%] [G loss: 1.963408]\n",
      "epoch:18 step:17293 [D loss: 0.661570, acc: 57.81%] [G loss: 2.004105]\n",
      "epoch:18 step:17294 [D loss: 0.570199, acc: 70.31%] [G loss: 2.005871]\n",
      "epoch:18 step:17295 [D loss: 0.664274, acc: 61.72%] [G loss: 2.096587]\n",
      "epoch:18 step:17296 [D loss: 0.578673, acc: 70.31%] [G loss: 2.003510]\n",
      "epoch:18 step:17297 [D loss: 0.625154, acc: 63.28%] [G loss: 1.956259]\n",
      "epoch:18 step:17298 [D loss: 0.624010, acc: 66.41%] [G loss: 1.795456]\n",
      "epoch:18 step:17299 [D loss: 0.728228, acc: 50.78%] [G loss: 1.838004]\n",
      "epoch:18 step:17300 [D loss: 0.655190, acc: 60.16%] [G loss: 1.837181]\n",
      "epoch:18 step:17301 [D loss: 0.662099, acc: 61.72%] [G loss: 1.886577]\n",
      "epoch:18 step:17302 [D loss: 0.689473, acc: 60.16%] [G loss: 1.940912]\n",
      "epoch:18 step:17303 [D loss: 0.682009, acc: 57.81%] [G loss: 1.729514]\n",
      "epoch:18 step:17304 [D loss: 0.656110, acc: 63.28%] [G loss: 1.714717]\n",
      "epoch:18 step:17305 [D loss: 0.663193, acc: 62.50%] [G loss: 1.819105]\n",
      "epoch:18 step:17306 [D loss: 0.664981, acc: 57.03%] [G loss: 1.761656]\n",
      "epoch:18 step:17307 [D loss: 0.665187, acc: 60.94%] [G loss: 1.813988]\n",
      "epoch:18 step:17308 [D loss: 0.666376, acc: 64.06%] [G loss: 1.672891]\n",
      "epoch:18 step:17309 [D loss: 0.647351, acc: 60.94%] [G loss: 1.876714]\n",
      "epoch:18 step:17310 [D loss: 0.682818, acc: 61.72%] [G loss: 1.787633]\n",
      "epoch:18 step:17311 [D loss: 0.640979, acc: 64.06%] [G loss: 1.843308]\n",
      "epoch:18 step:17312 [D loss: 0.671836, acc: 64.84%] [G loss: 1.759117]\n",
      "epoch:18 step:17313 [D loss: 0.669458, acc: 62.50%] [G loss: 1.814549]\n",
      "epoch:18 step:17314 [D loss: 0.684258, acc: 55.47%] [G loss: 1.799272]\n",
      "epoch:18 step:17315 [D loss: 0.644627, acc: 64.06%] [G loss: 1.791467]\n",
      "epoch:18 step:17316 [D loss: 0.610811, acc: 70.31%] [G loss: 1.894989]\n",
      "epoch:18 step:17317 [D loss: 0.583070, acc: 70.31%] [G loss: 1.958929]\n",
      "epoch:18 step:17318 [D loss: 0.665676, acc: 58.59%] [G loss: 1.881720]\n",
      "epoch:18 step:17319 [D loss: 0.619600, acc: 64.06%] [G loss: 1.903535]\n",
      "epoch:18 step:17320 [D loss: 0.622148, acc: 67.97%] [G loss: 1.926198]\n",
      "epoch:18 step:17321 [D loss: 0.631568, acc: 65.62%] [G loss: 1.877828]\n",
      "epoch:18 step:17322 [D loss: 0.623359, acc: 64.84%] [G loss: 1.912640]\n",
      "epoch:18 step:17323 [D loss: 0.615472, acc: 65.62%] [G loss: 2.143252]\n",
      "epoch:18 step:17324 [D loss: 0.659157, acc: 59.38%] [G loss: 1.925487]\n",
      "epoch:18 step:17325 [D loss: 0.698032, acc: 52.34%] [G loss: 1.877082]\n",
      "epoch:18 step:17326 [D loss: 0.668643, acc: 59.38%] [G loss: 1.904011]\n",
      "epoch:18 step:17327 [D loss: 0.675032, acc: 57.81%] [G loss: 1.915914]\n",
      "epoch:18 step:17328 [D loss: 0.614985, acc: 68.75%] [G loss: 1.914189]\n",
      "epoch:18 step:17329 [D loss: 0.620299, acc: 67.19%] [G loss: 1.903558]\n",
      "epoch:18 step:17330 [D loss: 0.650301, acc: 65.62%] [G loss: 1.818374]\n",
      "epoch:18 step:17331 [D loss: 0.607422, acc: 66.41%] [G loss: 1.835742]\n",
      "epoch:18 step:17332 [D loss: 0.637940, acc: 65.62%] [G loss: 1.892747]\n",
      "epoch:18 step:17333 [D loss: 0.636954, acc: 63.28%] [G loss: 1.907220]\n",
      "epoch:18 step:17334 [D loss: 0.632705, acc: 65.62%] [G loss: 1.888313]\n",
      "epoch:18 step:17335 [D loss: 0.630469, acc: 64.84%] [G loss: 1.854955]\n",
      "epoch:18 step:17336 [D loss: 0.625793, acc: 63.28%] [G loss: 2.088751]\n",
      "epoch:18 step:17337 [D loss: 0.635219, acc: 63.28%] [G loss: 2.286527]\n",
      "epoch:18 step:17338 [D loss: 0.613014, acc: 69.53%] [G loss: 2.133971]\n",
      "epoch:18 step:17339 [D loss: 0.723902, acc: 56.25%] [G loss: 1.759727]\n",
      "epoch:18 step:17340 [D loss: 0.665829, acc: 59.38%] [G loss: 2.018094]\n",
      "epoch:18 step:17341 [D loss: 0.638532, acc: 65.62%] [G loss: 1.957835]\n",
      "epoch:18 step:17342 [D loss: 0.654607, acc: 59.38%] [G loss: 1.893708]\n",
      "epoch:18 step:17343 [D loss: 0.635820, acc: 62.50%] [G loss: 1.843843]\n",
      "epoch:18 step:17344 [D loss: 0.704397, acc: 52.34%] [G loss: 1.753229]\n",
      "epoch:18 step:17345 [D loss: 0.623662, acc: 62.50%] [G loss: 2.106976]\n",
      "epoch:18 step:17346 [D loss: 0.647285, acc: 60.94%] [G loss: 2.039937]\n",
      "epoch:18 step:17347 [D loss: 0.628481, acc: 67.19%] [G loss: 2.034607]\n",
      "epoch:18 step:17348 [D loss: 0.691432, acc: 56.25%] [G loss: 1.849165]\n",
      "epoch:18 step:17349 [D loss: 0.712383, acc: 59.38%] [G loss: 1.744701]\n",
      "epoch:18 step:17350 [D loss: 0.635532, acc: 64.84%] [G loss: 2.036146]\n",
      "epoch:18 step:17351 [D loss: 0.665695, acc: 59.38%] [G loss: 1.871416]\n",
      "epoch:18 step:17352 [D loss: 0.641040, acc: 64.06%] [G loss: 1.813363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17353 [D loss: 0.631948, acc: 60.16%] [G loss: 1.922505]\n",
      "epoch:18 step:17354 [D loss: 0.618859, acc: 70.31%] [G loss: 2.026121]\n",
      "epoch:18 step:17355 [D loss: 0.608976, acc: 67.97%] [G loss: 1.761389]\n",
      "epoch:18 step:17356 [D loss: 0.613811, acc: 64.84%] [G loss: 1.819736]\n",
      "epoch:18 step:17357 [D loss: 0.637837, acc: 66.41%] [G loss: 1.905951]\n",
      "epoch:18 step:17358 [D loss: 0.651925, acc: 59.38%] [G loss: 1.859306]\n",
      "epoch:18 step:17359 [D loss: 0.639369, acc: 64.06%] [G loss: 2.051996]\n",
      "epoch:18 step:17360 [D loss: 0.624320, acc: 63.28%] [G loss: 1.924277]\n",
      "epoch:18 step:17361 [D loss: 0.631537, acc: 64.84%] [G loss: 2.055670]\n",
      "epoch:18 step:17362 [D loss: 0.617311, acc: 65.62%] [G loss: 1.786487]\n",
      "epoch:18 step:17363 [D loss: 0.649677, acc: 64.06%] [G loss: 2.042195]\n",
      "epoch:18 step:17364 [D loss: 0.627518, acc: 68.75%] [G loss: 1.958313]\n",
      "epoch:18 step:17365 [D loss: 0.633538, acc: 66.41%] [G loss: 2.036097]\n",
      "epoch:18 step:17366 [D loss: 0.723376, acc: 58.59%] [G loss: 1.835605]\n",
      "epoch:18 step:17367 [D loss: 0.685739, acc: 57.03%] [G loss: 1.730274]\n",
      "epoch:18 step:17368 [D loss: 0.719537, acc: 50.00%] [G loss: 1.591728]\n",
      "epoch:18 step:17369 [D loss: 0.676479, acc: 52.34%] [G loss: 1.856845]\n",
      "epoch:18 step:17370 [D loss: 0.651524, acc: 65.62%] [G loss: 1.949448]\n",
      "epoch:18 step:17371 [D loss: 0.683990, acc: 57.03%] [G loss: 1.767638]\n",
      "epoch:18 step:17372 [D loss: 0.687059, acc: 57.81%] [G loss: 1.743959]\n",
      "epoch:18 step:17373 [D loss: 0.661441, acc: 57.81%] [G loss: 1.744920]\n",
      "epoch:18 step:17374 [D loss: 0.686160, acc: 57.03%] [G loss: 1.924275]\n",
      "epoch:18 step:17375 [D loss: 0.643041, acc: 67.19%] [G loss: 1.812920]\n",
      "epoch:18 step:17376 [D loss: 0.661328, acc: 57.81%] [G loss: 1.762057]\n",
      "epoch:18 step:17377 [D loss: 0.696333, acc: 54.69%] [G loss: 1.863116]\n",
      "epoch:18 step:17378 [D loss: 0.646955, acc: 64.06%] [G loss: 1.808157]\n",
      "epoch:18 step:17379 [D loss: 0.667380, acc: 61.72%] [G loss: 1.843879]\n",
      "epoch:18 step:17380 [D loss: 0.685789, acc: 57.81%] [G loss: 1.800671]\n",
      "epoch:18 step:17381 [D loss: 0.601881, acc: 69.53%] [G loss: 1.938205]\n",
      "epoch:18 step:17382 [D loss: 0.585471, acc: 68.75%] [G loss: 1.905021]\n",
      "epoch:18 step:17383 [D loss: 0.629016, acc: 69.53%] [G loss: 1.966999]\n",
      "epoch:18 step:17384 [D loss: 0.703831, acc: 57.81%] [G loss: 1.836477]\n",
      "epoch:18 step:17385 [D loss: 0.604557, acc: 68.75%] [G loss: 1.931789]\n",
      "epoch:18 step:17386 [D loss: 0.604643, acc: 64.84%] [G loss: 2.058244]\n",
      "epoch:18 step:17387 [D loss: 0.628144, acc: 66.41%] [G loss: 1.992531]\n",
      "epoch:18 step:17388 [D loss: 0.612129, acc: 68.75%] [G loss: 2.037015]\n",
      "epoch:18 step:17389 [D loss: 0.623018, acc: 64.84%] [G loss: 1.925262]\n",
      "epoch:18 step:17390 [D loss: 0.689350, acc: 57.03%] [G loss: 1.834393]\n",
      "epoch:18 step:17391 [D loss: 0.657129, acc: 64.06%] [G loss: 1.893586]\n",
      "epoch:18 step:17392 [D loss: 0.614698, acc: 71.88%] [G loss: 1.930853]\n",
      "epoch:18 step:17393 [D loss: 0.613904, acc: 67.19%] [G loss: 1.812380]\n",
      "epoch:18 step:17394 [D loss: 0.731368, acc: 53.12%] [G loss: 1.709524]\n",
      "epoch:18 step:17395 [D loss: 0.692132, acc: 59.38%] [G loss: 1.704133]\n",
      "epoch:18 step:17396 [D loss: 0.622806, acc: 66.41%] [G loss: 1.800070]\n",
      "epoch:18 step:17397 [D loss: 0.684865, acc: 57.81%] [G loss: 1.766183]\n",
      "epoch:18 step:17398 [D loss: 0.675341, acc: 57.81%] [G loss: 1.858244]\n",
      "epoch:18 step:17399 [D loss: 0.683525, acc: 61.72%] [G loss: 1.775152]\n",
      "epoch:18 step:17400 [D loss: 0.623624, acc: 67.97%] [G loss: 2.000012]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.290112\n",
      "FID: 13.253604\n",
      "0 = 12.73832345237734\n",
      "1 = 0.08162552543342502\n",
      "2 = 0.8851000070571899\n",
      "3 = 0.9067999720573425\n",
      "4 = 0.8633999824523926\n",
      "5 = 0.8690818548202515\n",
      "6 = 0.9067999720573425\n",
      "7 = 6.492737395727607\n",
      "8 = 0.06822234452092037\n",
      "9 = 0.7268999814987183\n",
      "10 = 0.7350000143051147\n",
      "11 = 0.7188000082969666\n",
      "12 = 0.7232828140258789\n",
      "13 = 0.7350000143051147\n",
      "14 = 7.290139675140381\n",
      "15 = 9.430473327636719\n",
      "16 = 0.11825263500213623\n",
      "17 = 7.290112018585205\n",
      "18 = 13.2536039352417\n",
      "epoch:18 step:17401 [D loss: 0.626984, acc: 67.97%] [G loss: 1.695002]\n",
      "epoch:18 step:17402 [D loss: 0.611306, acc: 65.62%] [G loss: 1.983510]\n",
      "epoch:18 step:17403 [D loss: 0.632422, acc: 63.28%] [G loss: 1.725756]\n",
      "epoch:18 step:17404 [D loss: 0.637020, acc: 62.50%] [G loss: 1.725742]\n",
      "epoch:18 step:17405 [D loss: 0.664862, acc: 62.50%] [G loss: 1.866109]\n",
      "epoch:18 step:17406 [D loss: 0.693361, acc: 60.16%] [G loss: 2.001940]\n",
      "epoch:18 step:17407 [D loss: 0.653879, acc: 66.41%] [G loss: 1.849106]\n",
      "epoch:18 step:17408 [D loss: 0.655053, acc: 62.50%] [G loss: 1.881887]\n",
      "epoch:18 step:17409 [D loss: 0.648798, acc: 63.28%] [G loss: 1.816199]\n",
      "epoch:18 step:17410 [D loss: 0.658558, acc: 57.03%] [G loss: 1.779441]\n",
      "epoch:18 step:17411 [D loss: 0.645386, acc: 57.81%] [G loss: 2.043789]\n",
      "epoch:18 step:17412 [D loss: 0.633086, acc: 62.50%] [G loss: 1.799136]\n",
      "epoch:18 step:17413 [D loss: 0.602979, acc: 68.75%] [G loss: 1.962496]\n",
      "epoch:18 step:17414 [D loss: 0.594293, acc: 64.84%] [G loss: 1.894720]\n",
      "epoch:18 step:17415 [D loss: 0.663401, acc: 64.84%] [G loss: 1.938904]\n",
      "epoch:18 step:17416 [D loss: 0.642047, acc: 64.84%] [G loss: 1.977447]\n",
      "epoch:18 step:17417 [D loss: 0.632116, acc: 61.72%] [G loss: 1.952945]\n",
      "epoch:18 step:17418 [D loss: 0.657533, acc: 60.94%] [G loss: 1.968873]\n",
      "epoch:18 step:17419 [D loss: 0.677258, acc: 53.91%] [G loss: 1.789908]\n",
      "epoch:18 step:17420 [D loss: 0.640102, acc: 62.50%] [G loss: 1.955603]\n",
      "epoch:18 step:17421 [D loss: 0.596143, acc: 67.97%] [G loss: 1.849961]\n",
      "epoch:18 step:17422 [D loss: 0.612080, acc: 71.09%] [G loss: 1.992445]\n",
      "epoch:18 step:17423 [D loss: 0.615214, acc: 65.62%] [G loss: 2.248249]\n",
      "epoch:18 step:17424 [D loss: 0.628581, acc: 64.06%] [G loss: 2.050090]\n",
      "epoch:18 step:17425 [D loss: 0.664230, acc: 59.38%] [G loss: 1.893837]\n",
      "epoch:18 step:17426 [D loss: 0.657224, acc: 63.28%] [G loss: 1.839933]\n",
      "epoch:18 step:17427 [D loss: 0.664152, acc: 60.16%] [G loss: 2.022982]\n",
      "epoch:18 step:17428 [D loss: 0.683125, acc: 51.56%] [G loss: 1.819220]\n",
      "epoch:18 step:17429 [D loss: 0.655578, acc: 64.84%] [G loss: 1.871903]\n",
      "epoch:18 step:17430 [D loss: 0.632758, acc: 61.72%] [G loss: 2.010687]\n",
      "epoch:18 step:17431 [D loss: 0.707343, acc: 55.47%] [G loss: 1.809386]\n",
      "epoch:18 step:17432 [D loss: 0.636440, acc: 61.72%] [G loss: 1.880258]\n",
      "epoch:18 step:17433 [D loss: 0.636600, acc: 63.28%] [G loss: 1.918467]\n",
      "epoch:18 step:17434 [D loss: 0.659211, acc: 57.81%] [G loss: 1.847650]\n",
      "epoch:18 step:17435 [D loss: 0.711652, acc: 53.12%] [G loss: 1.822100]\n",
      "epoch:18 step:17436 [D loss: 0.652265, acc: 64.06%] [G loss: 1.756003]\n",
      "epoch:18 step:17437 [D loss: 0.636041, acc: 67.97%] [G loss: 1.860078]\n",
      "epoch:18 step:17438 [D loss: 0.654869, acc: 57.03%] [G loss: 1.869193]\n",
      "epoch:18 step:17439 [D loss: 0.663967, acc: 64.06%] [G loss: 1.834020]\n",
      "epoch:18 step:17440 [D loss: 0.617605, acc: 65.62%] [G loss: 1.864230]\n",
      "epoch:18 step:17441 [D loss: 0.609458, acc: 71.09%] [G loss: 1.785156]\n",
      "epoch:18 step:17442 [D loss: 0.675636, acc: 61.72%] [G loss: 1.734680]\n",
      "epoch:18 step:17443 [D loss: 0.703274, acc: 53.12%] [G loss: 1.649987]\n",
      "epoch:18 step:17444 [D loss: 0.599645, acc: 70.31%] [G loss: 1.899486]\n",
      "epoch:18 step:17445 [D loss: 0.652147, acc: 62.50%] [G loss: 1.765983]\n",
      "epoch:18 step:17446 [D loss: 0.694727, acc: 52.34%] [G loss: 1.747317]\n",
      "epoch:18 step:17447 [D loss: 0.692647, acc: 57.03%] [G loss: 1.784097]\n",
      "epoch:18 step:17448 [D loss: 0.636090, acc: 66.41%] [G loss: 1.855384]\n",
      "epoch:18 step:17449 [D loss: 0.702461, acc: 57.81%] [G loss: 1.811774]\n",
      "epoch:18 step:17450 [D loss: 0.657337, acc: 60.94%] [G loss: 1.856614]\n",
      "epoch:18 step:17451 [D loss: 0.656043, acc: 64.06%] [G loss: 1.815090]\n",
      "epoch:18 step:17452 [D loss: 0.627720, acc: 63.28%] [G loss: 1.801155]\n",
      "epoch:18 step:17453 [D loss: 0.642583, acc: 63.28%] [G loss: 1.832019]\n",
      "epoch:18 step:17454 [D loss: 0.590018, acc: 67.97%] [G loss: 1.948904]\n",
      "epoch:18 step:17455 [D loss: 0.579985, acc: 71.09%] [G loss: 1.942755]\n",
      "epoch:18 step:17456 [D loss: 0.657042, acc: 59.38%] [G loss: 1.800383]\n",
      "epoch:18 step:17457 [D loss: 0.635741, acc: 63.28%] [G loss: 1.823495]\n",
      "epoch:18 step:17458 [D loss: 0.634810, acc: 62.50%] [G loss: 1.793194]\n",
      "epoch:18 step:17459 [D loss: 0.685989, acc: 56.25%] [G loss: 1.899908]\n",
      "epoch:18 step:17460 [D loss: 0.647605, acc: 59.38%] [G loss: 1.891643]\n",
      "epoch:18 step:17461 [D loss: 0.659201, acc: 60.94%] [G loss: 1.837461]\n",
      "epoch:18 step:17462 [D loss: 0.682152, acc: 54.69%] [G loss: 1.732677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17463 [D loss: 0.632678, acc: 62.50%] [G loss: 1.836103]\n",
      "epoch:18 step:17464 [D loss: 0.664202, acc: 67.19%] [G loss: 1.847374]\n",
      "epoch:18 step:17465 [D loss: 0.692232, acc: 61.72%] [G loss: 1.788773]\n",
      "epoch:18 step:17466 [D loss: 0.653524, acc: 62.50%] [G loss: 1.835281]\n",
      "epoch:18 step:17467 [D loss: 0.643023, acc: 66.41%] [G loss: 1.868398]\n",
      "epoch:18 step:17468 [D loss: 0.648727, acc: 62.50%] [G loss: 2.024448]\n",
      "epoch:18 step:17469 [D loss: 0.622979, acc: 61.72%] [G loss: 1.928694]\n",
      "epoch:18 step:17470 [D loss: 0.626167, acc: 60.16%] [G loss: 1.807476]\n",
      "epoch:18 step:17471 [D loss: 0.573976, acc: 68.75%] [G loss: 1.919143]\n",
      "epoch:18 step:17472 [D loss: 0.733902, acc: 45.31%] [G loss: 1.813454]\n",
      "epoch:18 step:17473 [D loss: 0.616793, acc: 64.06%] [G loss: 1.897553]\n",
      "epoch:18 step:17474 [D loss: 0.654534, acc: 61.72%] [G loss: 1.863840]\n",
      "epoch:18 step:17475 [D loss: 0.611943, acc: 67.19%] [G loss: 1.988041]\n",
      "epoch:18 step:17476 [D loss: 0.661219, acc: 59.38%] [G loss: 1.791869]\n",
      "epoch:18 step:17477 [D loss: 0.660772, acc: 58.59%] [G loss: 1.853813]\n",
      "epoch:18 step:17478 [D loss: 0.649087, acc: 63.28%] [G loss: 1.892615]\n",
      "epoch:18 step:17479 [D loss: 0.604773, acc: 69.53%] [G loss: 1.966451]\n",
      "epoch:18 step:17480 [D loss: 0.685465, acc: 64.06%] [G loss: 1.871228]\n",
      "epoch:18 step:17481 [D loss: 0.694544, acc: 53.91%] [G loss: 1.750535]\n",
      "epoch:18 step:17482 [D loss: 0.683509, acc: 54.69%] [G loss: 1.796543]\n",
      "epoch:18 step:17483 [D loss: 0.661314, acc: 63.28%] [G loss: 1.967057]\n",
      "epoch:18 step:17484 [D loss: 0.647458, acc: 60.94%] [G loss: 1.806770]\n",
      "epoch:18 step:17485 [D loss: 0.608999, acc: 72.66%] [G loss: 1.871439]\n",
      "epoch:18 step:17486 [D loss: 0.671679, acc: 60.16%] [G loss: 1.795717]\n",
      "epoch:18 step:17487 [D loss: 0.608025, acc: 64.84%] [G loss: 1.921080]\n",
      "epoch:18 step:17488 [D loss: 0.670529, acc: 64.06%] [G loss: 1.887646]\n",
      "epoch:18 step:17489 [D loss: 0.662938, acc: 57.81%] [G loss: 1.873143]\n",
      "epoch:18 step:17490 [D loss: 0.660397, acc: 57.81%] [G loss: 1.956423]\n",
      "epoch:18 step:17491 [D loss: 0.631628, acc: 64.06%] [G loss: 1.781759]\n",
      "epoch:18 step:17492 [D loss: 0.585297, acc: 69.53%] [G loss: 1.881287]\n",
      "epoch:18 step:17493 [D loss: 0.629040, acc: 64.06%] [G loss: 1.771784]\n",
      "epoch:18 step:17494 [D loss: 0.663311, acc: 60.16%] [G loss: 1.862767]\n",
      "epoch:18 step:17495 [D loss: 0.647784, acc: 57.81%] [G loss: 1.838533]\n",
      "epoch:18 step:17496 [D loss: 0.672500, acc: 59.38%] [G loss: 1.918732]\n",
      "epoch:18 step:17497 [D loss: 0.627837, acc: 60.94%] [G loss: 1.975031]\n",
      "epoch:18 step:17498 [D loss: 0.631712, acc: 63.28%] [G loss: 1.895290]\n",
      "epoch:18 step:17499 [D loss: 0.645981, acc: 64.06%] [G loss: 1.884635]\n",
      "epoch:18 step:17500 [D loss: 0.663955, acc: 62.50%] [G loss: 1.857782]\n",
      "epoch:18 step:17501 [D loss: 0.611419, acc: 70.31%] [G loss: 1.789004]\n",
      "epoch:18 step:17502 [D loss: 0.626472, acc: 63.28%] [G loss: 1.958984]\n",
      "epoch:18 step:17503 [D loss: 0.663605, acc: 64.84%] [G loss: 1.992672]\n",
      "epoch:18 step:17504 [D loss: 0.589767, acc: 66.41%] [G loss: 1.979208]\n",
      "epoch:18 step:17505 [D loss: 0.640143, acc: 67.19%] [G loss: 2.048054]\n",
      "epoch:18 step:17506 [D loss: 0.680238, acc: 57.03%] [G loss: 1.994536]\n",
      "epoch:18 step:17507 [D loss: 0.607362, acc: 68.75%] [G loss: 1.975662]\n",
      "epoch:18 step:17508 [D loss: 0.590835, acc: 70.31%] [G loss: 1.953748]\n",
      "epoch:18 step:17509 [D loss: 0.669046, acc: 63.28%] [G loss: 1.951252]\n",
      "epoch:18 step:17510 [D loss: 0.622892, acc: 66.41%] [G loss: 2.028287]\n",
      "epoch:18 step:17511 [D loss: 0.682125, acc: 60.16%] [G loss: 1.907098]\n",
      "epoch:18 step:17512 [D loss: 0.642965, acc: 63.28%] [G loss: 2.135594]\n",
      "epoch:18 step:17513 [D loss: 0.660350, acc: 61.72%] [G loss: 2.110352]\n",
      "epoch:18 step:17514 [D loss: 0.592350, acc: 68.75%] [G loss: 2.290717]\n",
      "epoch:18 step:17515 [D loss: 0.621714, acc: 68.75%] [G loss: 2.047005]\n",
      "epoch:18 step:17516 [D loss: 0.647611, acc: 60.94%] [G loss: 2.069464]\n",
      "epoch:18 step:17517 [D loss: 0.672653, acc: 57.03%] [G loss: 1.854458]\n",
      "epoch:18 step:17518 [D loss: 0.677180, acc: 58.59%] [G loss: 1.894700]\n",
      "epoch:18 step:17519 [D loss: 0.630026, acc: 63.28%] [G loss: 1.874753]\n",
      "epoch:18 step:17520 [D loss: 0.657704, acc: 62.50%] [G loss: 2.062183]\n",
      "epoch:18 step:17521 [D loss: 0.666338, acc: 59.38%] [G loss: 1.932043]\n",
      "epoch:18 step:17522 [D loss: 0.634383, acc: 62.50%] [G loss: 1.830057]\n",
      "epoch:18 step:17523 [D loss: 0.720592, acc: 52.34%] [G loss: 1.812371]\n",
      "epoch:18 step:17524 [D loss: 0.691352, acc: 57.03%] [G loss: 1.850430]\n",
      "epoch:18 step:17525 [D loss: 0.679339, acc: 57.81%] [G loss: 1.803598]\n",
      "epoch:18 step:17526 [D loss: 0.667685, acc: 57.03%] [G loss: 1.794550]\n",
      "epoch:18 step:17527 [D loss: 0.617968, acc: 61.72%] [G loss: 1.793858]\n",
      "epoch:18 step:17528 [D loss: 0.687937, acc: 55.47%] [G loss: 1.888195]\n",
      "epoch:18 step:17529 [D loss: 0.616315, acc: 65.62%] [G loss: 1.911786]\n",
      "epoch:18 step:17530 [D loss: 0.639725, acc: 60.16%] [G loss: 1.835235]\n",
      "epoch:18 step:17531 [D loss: 0.624209, acc: 65.62%] [G loss: 1.822067]\n",
      "epoch:18 step:17532 [D loss: 0.658475, acc: 57.81%] [G loss: 1.779141]\n",
      "epoch:18 step:17533 [D loss: 0.680901, acc: 54.69%] [G loss: 1.721911]\n",
      "epoch:18 step:17534 [D loss: 0.638452, acc: 67.97%] [G loss: 1.866421]\n",
      "epoch:18 step:17535 [D loss: 0.630657, acc: 64.84%] [G loss: 1.876018]\n",
      "epoch:18 step:17536 [D loss: 0.681992, acc: 56.25%] [G loss: 1.847620]\n",
      "epoch:18 step:17537 [D loss: 0.604998, acc: 70.31%] [G loss: 1.774573]\n",
      "epoch:18 step:17538 [D loss: 0.649201, acc: 63.28%] [G loss: 1.816644]\n",
      "epoch:18 step:17539 [D loss: 0.635814, acc: 66.41%] [G loss: 1.681119]\n",
      "epoch:18 step:17540 [D loss: 0.654918, acc: 60.94%] [G loss: 1.938060]\n",
      "epoch:18 step:17541 [D loss: 0.631866, acc: 65.62%] [G loss: 1.801814]\n",
      "epoch:18 step:17542 [D loss: 0.674065, acc: 59.38%] [G loss: 1.833633]\n",
      "epoch:18 step:17543 [D loss: 0.655917, acc: 57.81%] [G loss: 1.843379]\n",
      "epoch:18 step:17544 [D loss: 0.637532, acc: 60.94%] [G loss: 2.013628]\n",
      "epoch:18 step:17545 [D loss: 0.669073, acc: 61.72%] [G loss: 1.849968]\n",
      "epoch:18 step:17546 [D loss: 0.686935, acc: 58.59%] [G loss: 1.818969]\n",
      "epoch:18 step:17547 [D loss: 0.658884, acc: 67.19%] [G loss: 1.944120]\n",
      "epoch:18 step:17548 [D loss: 0.691878, acc: 60.16%] [G loss: 1.696277]\n",
      "epoch:18 step:17549 [D loss: 0.698103, acc: 55.47%] [G loss: 1.726794]\n",
      "epoch:18 step:17550 [D loss: 0.653917, acc: 62.50%] [G loss: 1.902513]\n",
      "epoch:18 step:17551 [D loss: 0.630578, acc: 63.28%] [G loss: 1.861152]\n",
      "epoch:18 step:17552 [D loss: 0.670773, acc: 60.94%] [G loss: 1.763704]\n",
      "epoch:18 step:17553 [D loss: 0.656522, acc: 64.06%] [G loss: 1.857673]\n",
      "epoch:18 step:17554 [D loss: 0.652382, acc: 55.47%] [G loss: 2.011193]\n",
      "epoch:18 step:17555 [D loss: 0.659832, acc: 57.03%] [G loss: 1.861974]\n",
      "epoch:18 step:17556 [D loss: 0.615300, acc: 63.28%] [G loss: 1.989103]\n",
      "epoch:18 step:17557 [D loss: 0.597019, acc: 71.88%] [G loss: 1.951393]\n",
      "epoch:18 step:17558 [D loss: 0.626821, acc: 67.97%] [G loss: 2.115636]\n",
      "epoch:18 step:17559 [D loss: 0.561862, acc: 75.00%] [G loss: 2.060978]\n",
      "epoch:18 step:17560 [D loss: 0.648880, acc: 62.50%] [G loss: 1.991711]\n",
      "epoch:18 step:17561 [D loss: 0.638522, acc: 60.94%] [G loss: 1.906210]\n",
      "epoch:18 step:17562 [D loss: 0.643881, acc: 62.50%] [G loss: 1.939475]\n",
      "epoch:18 step:17563 [D loss: 0.581264, acc: 70.31%] [G loss: 1.997686]\n",
      "epoch:18 step:17564 [D loss: 0.671681, acc: 60.94%] [G loss: 1.824969]\n",
      "epoch:18 step:17565 [D loss: 0.651667, acc: 61.72%] [G loss: 1.855997]\n",
      "epoch:18 step:17566 [D loss: 0.572119, acc: 71.88%] [G loss: 1.854716]\n",
      "epoch:18 step:17567 [D loss: 0.657545, acc: 67.19%] [G loss: 1.933773]\n",
      "epoch:18 step:17568 [D loss: 0.645135, acc: 60.94%] [G loss: 1.864411]\n",
      "epoch:18 step:17569 [D loss: 0.660034, acc: 61.72%] [G loss: 1.884592]\n",
      "epoch:18 step:17570 [D loss: 0.662221, acc: 63.28%] [G loss: 1.823444]\n",
      "epoch:18 step:17571 [D loss: 0.652334, acc: 57.03%] [G loss: 1.878927]\n",
      "epoch:18 step:17572 [D loss: 0.640929, acc: 63.28%] [G loss: 2.018161]\n",
      "epoch:18 step:17573 [D loss: 0.614361, acc: 71.88%] [G loss: 1.990636]\n",
      "epoch:18 step:17574 [D loss: 0.622014, acc: 66.41%] [G loss: 2.061707]\n",
      "epoch:18 step:17575 [D loss: 0.600032, acc: 67.19%] [G loss: 1.917854]\n",
      "epoch:18 step:17576 [D loss: 0.628050, acc: 61.72%] [G loss: 1.947695]\n",
      "epoch:18 step:17577 [D loss: 0.639256, acc: 62.50%] [G loss: 1.985371]\n",
      "epoch:18 step:17578 [D loss: 0.595318, acc: 67.97%] [G loss: 1.874685]\n",
      "epoch:18 step:17579 [D loss: 0.704153, acc: 55.47%] [G loss: 1.819930]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17580 [D loss: 0.651565, acc: 63.28%] [G loss: 2.101616]\n",
      "epoch:18 step:17581 [D loss: 0.670472, acc: 57.81%] [G loss: 1.883226]\n",
      "epoch:18 step:17582 [D loss: 0.697749, acc: 54.69%] [G loss: 1.849400]\n",
      "epoch:18 step:17583 [D loss: 0.675746, acc: 57.81%] [G loss: 1.792079]\n",
      "epoch:18 step:17584 [D loss: 0.740274, acc: 50.78%] [G loss: 1.882991]\n",
      "epoch:18 step:17585 [D loss: 0.645832, acc: 61.72%] [G loss: 1.955831]\n",
      "epoch:18 step:17586 [D loss: 0.643372, acc: 63.28%] [G loss: 1.925163]\n",
      "epoch:18 step:17587 [D loss: 0.668782, acc: 62.50%] [G loss: 2.006643]\n",
      "epoch:18 step:17588 [D loss: 0.732654, acc: 51.56%] [G loss: 1.825232]\n",
      "epoch:18 step:17589 [D loss: 0.675121, acc: 63.28%] [G loss: 1.823291]\n",
      "epoch:18 step:17590 [D loss: 0.695847, acc: 53.91%] [G loss: 1.745649]\n",
      "epoch:18 step:17591 [D loss: 0.671693, acc: 59.38%] [G loss: 1.832702]\n",
      "epoch:18 step:17592 [D loss: 0.630582, acc: 64.84%] [G loss: 1.893517]\n",
      "epoch:18 step:17593 [D loss: 0.691603, acc: 54.69%] [G loss: 1.751034]\n",
      "epoch:18 step:17594 [D loss: 0.665217, acc: 64.06%] [G loss: 1.843846]\n",
      "epoch:18 step:17595 [D loss: 0.645949, acc: 68.75%] [G loss: 1.876739]\n",
      "epoch:18 step:17596 [D loss: 0.708252, acc: 58.59%] [G loss: 1.741089]\n",
      "epoch:18 step:17597 [D loss: 0.663508, acc: 59.38%] [G loss: 1.730418]\n",
      "epoch:18 step:17598 [D loss: 0.627628, acc: 63.28%] [G loss: 1.809514]\n",
      "epoch:18 step:17599 [D loss: 0.663544, acc: 60.16%] [G loss: 1.709669]\n",
      "epoch:18 step:17600 [D loss: 0.630907, acc: 62.50%] [G loss: 1.766401]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.259912\n",
      "FID: 15.685760\n",
      "0 = 12.684630310249345\n",
      "1 = 0.08191418241638516\n",
      "2 = 0.8787999749183655\n",
      "3 = 0.8984000086784363\n",
      "4 = 0.8592000007629395\n",
      "5 = 0.8645111918449402\n",
      "6 = 0.8984000086784363\n",
      "7 = 6.571803650736844\n",
      "8 = 0.07652241182832599\n",
      "9 = 0.7268999814987183\n",
      "10 = 0.7444000244140625\n",
      "11 = 0.7093999981880188\n",
      "12 = 0.7192270755767822\n",
      "13 = 0.7444000244140625\n",
      "14 = 7.259942531585693\n",
      "15 = 9.430913925170898\n",
      "16 = 0.11557774990797043\n",
      "17 = 7.259912014007568\n",
      "18 = 15.685760498046875\n",
      "epoch:18 step:17601 [D loss: 0.619026, acc: 65.62%] [G loss: 1.988490]\n",
      "epoch:18 step:17602 [D loss: 0.645796, acc: 64.06%] [G loss: 1.877848]\n",
      "epoch:18 step:17603 [D loss: 0.582028, acc: 71.09%] [G loss: 1.919049]\n",
      "epoch:18 step:17604 [D loss: 0.676486, acc: 55.47%] [G loss: 1.894741]\n",
      "epoch:18 step:17605 [D loss: 0.621781, acc: 65.62%] [G loss: 1.870754]\n",
      "epoch:18 step:17606 [D loss: 0.606304, acc: 64.84%] [G loss: 1.924155]\n",
      "epoch:18 step:17607 [D loss: 0.676832, acc: 62.50%] [G loss: 1.781055]\n",
      "epoch:18 step:17608 [D loss: 0.655544, acc: 61.72%] [G loss: 1.835317]\n",
      "epoch:18 step:17609 [D loss: 0.663128, acc: 62.50%] [G loss: 1.787194]\n",
      "epoch:18 step:17610 [D loss: 0.706375, acc: 55.47%] [G loss: 1.848867]\n",
      "epoch:18 step:17611 [D loss: 0.691817, acc: 49.22%] [G loss: 1.975870]\n",
      "epoch:18 step:17612 [D loss: 0.564929, acc: 74.22%] [G loss: 2.095995]\n",
      "epoch:18 step:17613 [D loss: 0.573655, acc: 75.78%] [G loss: 1.975014]\n",
      "epoch:18 step:17614 [D loss: 0.672313, acc: 61.72%] [G loss: 1.921198]\n",
      "epoch:18 step:17615 [D loss: 0.638755, acc: 61.72%] [G loss: 1.869085]\n",
      "epoch:18 step:17616 [D loss: 0.655453, acc: 61.72%] [G loss: 1.799165]\n",
      "epoch:18 step:17617 [D loss: 0.665082, acc: 63.28%] [G loss: 1.721619]\n",
      "epoch:18 step:17618 [D loss: 0.732914, acc: 46.09%] [G loss: 1.794190]\n",
      "epoch:18 step:17619 [D loss: 0.600859, acc: 67.97%] [G loss: 1.885161]\n",
      "epoch:18 step:17620 [D loss: 0.633202, acc: 61.72%] [G loss: 1.905706]\n",
      "epoch:18 step:17621 [D loss: 0.632360, acc: 60.94%] [G loss: 1.822566]\n",
      "epoch:18 step:17622 [D loss: 0.664053, acc: 57.81%] [G loss: 1.872504]\n",
      "epoch:18 step:17623 [D loss: 0.627467, acc: 61.72%] [G loss: 1.869745]\n",
      "epoch:18 step:17624 [D loss: 0.676999, acc: 57.03%] [G loss: 1.681690]\n",
      "epoch:18 step:17625 [D loss: 0.634094, acc: 67.97%] [G loss: 1.799855]\n",
      "epoch:18 step:17626 [D loss: 0.701048, acc: 55.47%] [G loss: 1.797088]\n",
      "epoch:18 step:17627 [D loss: 0.629435, acc: 62.50%] [G loss: 1.951375]\n",
      "epoch:18 step:17628 [D loss: 0.659817, acc: 60.94%] [G loss: 1.940709]\n",
      "epoch:18 step:17629 [D loss: 0.585701, acc: 67.97%] [G loss: 1.914761]\n",
      "epoch:18 step:17630 [D loss: 0.668787, acc: 60.94%] [G loss: 1.907949]\n",
      "epoch:18 step:17631 [D loss: 0.652570, acc: 62.50%] [G loss: 1.699635]\n",
      "epoch:18 step:17632 [D loss: 0.710852, acc: 54.69%] [G loss: 1.729825]\n",
      "epoch:18 step:17633 [D loss: 0.650585, acc: 63.28%] [G loss: 1.780940]\n",
      "epoch:18 step:17634 [D loss: 0.643187, acc: 64.84%] [G loss: 1.867964]\n",
      "epoch:18 step:17635 [D loss: 0.646968, acc: 61.72%] [G loss: 1.825502]\n",
      "epoch:18 step:17636 [D loss: 0.604488, acc: 73.44%] [G loss: 1.890777]\n",
      "epoch:18 step:17637 [D loss: 0.653583, acc: 60.16%] [G loss: 1.751276]\n",
      "epoch:18 step:17638 [D loss: 0.631445, acc: 67.19%] [G loss: 1.896825]\n",
      "epoch:18 step:17639 [D loss: 0.659420, acc: 59.38%] [G loss: 1.785379]\n",
      "epoch:18 step:17640 [D loss: 0.637428, acc: 64.06%] [G loss: 1.998039]\n",
      "epoch:18 step:17641 [D loss: 0.583829, acc: 73.44%] [G loss: 2.082369]\n",
      "epoch:18 step:17642 [D loss: 0.657071, acc: 57.81%] [G loss: 1.931604]\n",
      "epoch:18 step:17643 [D loss: 0.650295, acc: 64.84%] [G loss: 1.856724]\n",
      "epoch:18 step:17644 [D loss: 0.633614, acc: 62.50%] [G loss: 1.817643]\n",
      "epoch:18 step:17645 [D loss: 0.664621, acc: 65.62%] [G loss: 1.940080]\n",
      "epoch:18 step:17646 [D loss: 0.689334, acc: 55.47%] [G loss: 1.980453]\n",
      "epoch:18 step:17647 [D loss: 0.588930, acc: 64.84%] [G loss: 2.104016]\n",
      "epoch:18 step:17648 [D loss: 0.622315, acc: 67.97%] [G loss: 2.086046]\n",
      "epoch:18 step:17649 [D loss: 0.625473, acc: 62.50%] [G loss: 1.949409]\n",
      "epoch:18 step:17650 [D loss: 0.708193, acc: 53.91%] [G loss: 1.676613]\n",
      "epoch:18 step:17651 [D loss: 0.677314, acc: 56.25%] [G loss: 1.943723]\n",
      "epoch:18 step:17652 [D loss: 0.609572, acc: 67.19%] [G loss: 1.982644]\n",
      "epoch:18 step:17653 [D loss: 0.656347, acc: 59.38%] [G loss: 1.824987]\n",
      "epoch:18 step:17654 [D loss: 0.651283, acc: 62.50%] [G loss: 1.747404]\n",
      "epoch:18 step:17655 [D loss: 0.653746, acc: 61.72%] [G loss: 1.875823]\n",
      "epoch:18 step:17656 [D loss: 0.590763, acc: 73.44%] [G loss: 2.002315]\n",
      "epoch:18 step:17657 [D loss: 0.647795, acc: 61.72%] [G loss: 1.953655]\n",
      "epoch:18 step:17658 [D loss: 0.581282, acc: 72.66%] [G loss: 2.014621]\n",
      "epoch:18 step:17659 [D loss: 0.591018, acc: 69.53%] [G loss: 1.973934]\n",
      "epoch:18 step:17660 [D loss: 0.733107, acc: 49.22%] [G loss: 1.779254]\n",
      "epoch:18 step:17661 [D loss: 0.713449, acc: 56.25%] [G loss: 1.864020]\n",
      "epoch:18 step:17662 [D loss: 0.672064, acc: 60.94%] [G loss: 1.803962]\n",
      "epoch:18 step:17663 [D loss: 0.654574, acc: 62.50%] [G loss: 1.770197]\n",
      "epoch:18 step:17664 [D loss: 0.640360, acc: 63.28%] [G loss: 1.969105]\n",
      "epoch:18 step:17665 [D loss: 0.681581, acc: 56.25%] [G loss: 1.927197]\n",
      "epoch:18 step:17666 [D loss: 0.759038, acc: 50.78%] [G loss: 1.732686]\n",
      "epoch:18 step:17667 [D loss: 0.705946, acc: 57.03%] [G loss: 1.713687]\n",
      "epoch:18 step:17668 [D loss: 0.638936, acc: 64.84%] [G loss: 1.850199]\n",
      "epoch:18 step:17669 [D loss: 0.622034, acc: 63.28%] [G loss: 1.761509]\n",
      "epoch:18 step:17670 [D loss: 0.635534, acc: 64.84%] [G loss: 1.911980]\n",
      "epoch:18 step:17671 [D loss: 0.667651, acc: 60.94%] [G loss: 1.832216]\n",
      "epoch:18 step:17672 [D loss: 0.635432, acc: 67.19%] [G loss: 1.986310]\n",
      "epoch:18 step:17673 [D loss: 0.606015, acc: 70.31%] [G loss: 2.064114]\n",
      "epoch:18 step:17674 [D loss: 0.640654, acc: 60.16%] [G loss: 1.914853]\n",
      "epoch:18 step:17675 [D loss: 0.676067, acc: 58.59%] [G loss: 1.760042]\n",
      "epoch:18 step:17676 [D loss: 0.674348, acc: 55.47%] [G loss: 1.877589]\n",
      "epoch:18 step:17677 [D loss: 0.624832, acc: 67.97%] [G loss: 1.825530]\n",
      "epoch:18 step:17678 [D loss: 0.708036, acc: 50.78%] [G loss: 1.804165]\n",
      "epoch:18 step:17679 [D loss: 0.580118, acc: 71.09%] [G loss: 1.810899]\n",
      "epoch:18 step:17680 [D loss: 0.633198, acc: 64.84%] [G loss: 1.946350]\n",
      "epoch:18 step:17681 [D loss: 0.581962, acc: 72.66%] [G loss: 2.147430]\n",
      "epoch:18 step:17682 [D loss: 0.624254, acc: 65.62%] [G loss: 2.070534]\n",
      "epoch:18 step:17683 [D loss: 0.644950, acc: 61.72%] [G loss: 1.820745]\n",
      "epoch:18 step:17684 [D loss: 0.676543, acc: 62.50%] [G loss: 1.814089]\n",
      "epoch:18 step:17685 [D loss: 0.632740, acc: 64.84%] [G loss: 1.947661]\n",
      "epoch:18 step:17686 [D loss: 0.705190, acc: 50.78%] [G loss: 1.762088]\n",
      "epoch:18 step:17687 [D loss: 0.676371, acc: 54.69%] [G loss: 1.845955]\n",
      "epoch:18 step:17688 [D loss: 0.618866, acc: 66.41%] [G loss: 1.888920]\n",
      "epoch:18 step:17689 [D loss: 0.598723, acc: 67.97%] [G loss: 1.987678]\n",
      "epoch:18 step:17690 [D loss: 0.650916, acc: 61.72%] [G loss: 1.795295]\n",
      "epoch:18 step:17691 [D loss: 0.664046, acc: 56.25%] [G loss: 1.859994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17692 [D loss: 0.648631, acc: 63.28%] [G loss: 1.882090]\n",
      "epoch:18 step:17693 [D loss: 0.657408, acc: 58.59%] [G loss: 1.743271]\n",
      "epoch:18 step:17694 [D loss: 0.672411, acc: 57.81%] [G loss: 1.855013]\n",
      "epoch:18 step:17695 [D loss: 0.680108, acc: 58.59%] [G loss: 1.741296]\n",
      "epoch:18 step:17696 [D loss: 0.642866, acc: 62.50%] [G loss: 1.841328]\n",
      "epoch:18 step:17697 [D loss: 0.650891, acc: 60.16%] [G loss: 1.771132]\n",
      "epoch:18 step:17698 [D loss: 0.637352, acc: 61.72%] [G loss: 1.924046]\n",
      "epoch:18 step:17699 [D loss: 0.563823, acc: 74.22%] [G loss: 1.944268]\n",
      "epoch:18 step:17700 [D loss: 0.674895, acc: 64.06%] [G loss: 1.832126]\n",
      "epoch:18 step:17701 [D loss: 0.663123, acc: 63.28%] [G loss: 1.882077]\n",
      "epoch:18 step:17702 [D loss: 0.635198, acc: 58.59%] [G loss: 1.850242]\n",
      "epoch:18 step:17703 [D loss: 0.583231, acc: 71.09%] [G loss: 1.933394]\n",
      "epoch:18 step:17704 [D loss: 0.609935, acc: 64.84%] [G loss: 1.962404]\n",
      "epoch:18 step:17705 [D loss: 0.615108, acc: 64.84%] [G loss: 1.887927]\n",
      "epoch:18 step:17706 [D loss: 0.631559, acc: 60.94%] [G loss: 1.839666]\n",
      "epoch:18 step:17707 [D loss: 0.652273, acc: 62.50%] [G loss: 2.040363]\n",
      "epoch:18 step:17708 [D loss: 0.627648, acc: 67.97%] [G loss: 1.993881]\n",
      "epoch:18 step:17709 [D loss: 0.643273, acc: 59.38%] [G loss: 1.917225]\n",
      "epoch:18 step:17710 [D loss: 0.633703, acc: 60.16%] [G loss: 1.982150]\n",
      "epoch:18 step:17711 [D loss: 0.650929, acc: 60.94%] [G loss: 1.882651]\n",
      "epoch:18 step:17712 [D loss: 0.656514, acc: 62.50%] [G loss: 1.899547]\n",
      "epoch:18 step:17713 [D loss: 0.655204, acc: 64.84%] [G loss: 1.904390]\n",
      "epoch:18 step:17714 [D loss: 0.608414, acc: 67.19%] [G loss: 1.832468]\n",
      "epoch:18 step:17715 [D loss: 0.634080, acc: 62.50%] [G loss: 2.053778]\n",
      "epoch:18 step:17716 [D loss: 0.664652, acc: 57.81%] [G loss: 1.890914]\n",
      "epoch:18 step:17717 [D loss: 0.614296, acc: 68.75%] [G loss: 1.803070]\n",
      "epoch:18 step:17718 [D loss: 0.666320, acc: 61.72%] [G loss: 1.785172]\n",
      "epoch:18 step:17719 [D loss: 0.640975, acc: 67.97%] [G loss: 1.883749]\n",
      "epoch:18 step:17720 [D loss: 0.643485, acc: 57.81%] [G loss: 1.859818]\n",
      "epoch:18 step:17721 [D loss: 0.699186, acc: 53.91%] [G loss: 1.787748]\n",
      "epoch:18 step:17722 [D loss: 0.668478, acc: 57.03%] [G loss: 1.795433]\n",
      "epoch:18 step:17723 [D loss: 0.636350, acc: 62.50%] [G loss: 1.930866]\n",
      "epoch:18 step:17724 [D loss: 0.721967, acc: 51.56%] [G loss: 1.808823]\n",
      "epoch:18 step:17725 [D loss: 0.740731, acc: 52.34%] [G loss: 1.747718]\n",
      "epoch:18 step:17726 [D loss: 0.627778, acc: 64.06%] [G loss: 1.896503]\n",
      "epoch:18 step:17727 [D loss: 0.617326, acc: 68.75%] [G loss: 1.813407]\n",
      "epoch:18 step:17728 [D loss: 0.660517, acc: 61.72%] [G loss: 1.676294]\n",
      "epoch:18 step:17729 [D loss: 0.691513, acc: 56.25%] [G loss: 1.796112]\n",
      "epoch:18 step:17730 [D loss: 0.619087, acc: 61.72%] [G loss: 1.894394]\n",
      "epoch:18 step:17731 [D loss: 0.657541, acc: 61.72%] [G loss: 1.698289]\n",
      "epoch:18 step:17732 [D loss: 0.617995, acc: 67.19%] [G loss: 1.775088]\n",
      "epoch:18 step:17733 [D loss: 0.670079, acc: 57.81%] [G loss: 1.899054]\n",
      "epoch:18 step:17734 [D loss: 0.607948, acc: 68.75%] [G loss: 1.881002]\n",
      "epoch:18 step:17735 [D loss: 0.640965, acc: 64.06%] [G loss: 1.863550]\n",
      "epoch:18 step:17736 [D loss: 0.697581, acc: 56.25%] [G loss: 1.892189]\n",
      "epoch:18 step:17737 [D loss: 0.695205, acc: 59.38%] [G loss: 1.845430]\n",
      "epoch:18 step:17738 [D loss: 0.620276, acc: 68.75%] [G loss: 1.886343]\n",
      "epoch:18 step:17739 [D loss: 0.630718, acc: 61.72%] [G loss: 1.789489]\n",
      "epoch:18 step:17740 [D loss: 0.675362, acc: 57.03%] [G loss: 1.684045]\n",
      "epoch:18 step:17741 [D loss: 0.665807, acc: 64.84%] [G loss: 2.009329]\n",
      "epoch:18 step:17742 [D loss: 0.685258, acc: 55.47%] [G loss: 1.738314]\n",
      "epoch:18 step:17743 [D loss: 0.654482, acc: 59.38%] [G loss: 1.902177]\n",
      "epoch:18 step:17744 [D loss: 0.662556, acc: 63.28%] [G loss: 1.825336]\n",
      "epoch:18 step:17745 [D loss: 0.628263, acc: 66.41%] [G loss: 1.688335]\n",
      "epoch:18 step:17746 [D loss: 0.621284, acc: 67.97%] [G loss: 1.809255]\n",
      "epoch:18 step:17747 [D loss: 0.643534, acc: 64.06%] [G loss: 1.785038]\n",
      "epoch:18 step:17748 [D loss: 0.655915, acc: 57.81%] [G loss: 1.944342]\n",
      "epoch:18 step:17749 [D loss: 0.716555, acc: 55.47%] [G loss: 1.869619]\n",
      "epoch:18 step:17750 [D loss: 0.639402, acc: 68.75%] [G loss: 1.926163]\n",
      "epoch:18 step:17751 [D loss: 0.652252, acc: 60.16%] [G loss: 1.761680]\n",
      "epoch:18 step:17752 [D loss: 0.667438, acc: 53.12%] [G loss: 1.865508]\n",
      "epoch:18 step:17753 [D loss: 0.687894, acc: 58.59%] [G loss: 1.779100]\n",
      "epoch:18 step:17754 [D loss: 0.631737, acc: 64.06%] [G loss: 1.874123]\n",
      "epoch:18 step:17755 [D loss: 0.593990, acc: 68.75%] [G loss: 1.846133]\n",
      "epoch:18 step:17756 [D loss: 0.622761, acc: 66.41%] [G loss: 2.055985]\n",
      "epoch:18 step:17757 [D loss: 0.643388, acc: 58.59%] [G loss: 1.865036]\n",
      "epoch:18 step:17758 [D loss: 0.750192, acc: 52.34%] [G loss: 1.883644]\n",
      "epoch:18 step:17759 [D loss: 0.624869, acc: 67.19%] [G loss: 1.804921]\n",
      "epoch:18 step:17760 [D loss: 0.572934, acc: 71.88%] [G loss: 2.059791]\n",
      "epoch:18 step:17761 [D loss: 0.645307, acc: 64.06%] [G loss: 1.880677]\n",
      "epoch:18 step:17762 [D loss: 0.705436, acc: 57.03%] [G loss: 1.836154]\n",
      "epoch:18 step:17763 [D loss: 0.606515, acc: 67.19%] [G loss: 1.993212]\n",
      "epoch:18 step:17764 [D loss: 0.640362, acc: 67.19%] [G loss: 1.960003]\n",
      "epoch:18 step:17765 [D loss: 0.608285, acc: 64.84%] [G loss: 1.950471]\n",
      "epoch:18 step:17766 [D loss: 0.608315, acc: 68.75%] [G loss: 1.979840]\n",
      "epoch:18 step:17767 [D loss: 0.638686, acc: 60.94%] [G loss: 1.972533]\n",
      "epoch:18 step:17768 [D loss: 0.659410, acc: 57.03%] [G loss: 1.987261]\n",
      "epoch:18 step:17769 [D loss: 0.660847, acc: 61.72%] [G loss: 1.824515]\n",
      "epoch:18 step:17770 [D loss: 0.651593, acc: 54.69%] [G loss: 1.958029]\n",
      "epoch:18 step:17771 [D loss: 0.674124, acc: 63.28%] [G loss: 1.803196]\n",
      "epoch:18 step:17772 [D loss: 0.674160, acc: 60.94%] [G loss: 1.942810]\n",
      "epoch:18 step:17773 [D loss: 0.608506, acc: 71.09%] [G loss: 1.910433]\n",
      "epoch:18 step:17774 [D loss: 0.625014, acc: 64.06%] [G loss: 2.105797]\n",
      "epoch:18 step:17775 [D loss: 0.578711, acc: 67.97%] [G loss: 1.912982]\n",
      "epoch:18 step:17776 [D loss: 0.653148, acc: 63.28%] [G loss: 1.898835]\n",
      "epoch:18 step:17777 [D loss: 0.596397, acc: 73.44%] [G loss: 1.971324]\n",
      "epoch:18 step:17778 [D loss: 0.651894, acc: 61.72%] [G loss: 1.983316]\n",
      "epoch:18 step:17779 [D loss: 0.695161, acc: 52.34%] [G loss: 1.915118]\n",
      "epoch:18 step:17780 [D loss: 0.672167, acc: 55.47%] [G loss: 1.956296]\n",
      "epoch:18 step:17781 [D loss: 0.645529, acc: 60.16%] [G loss: 1.997122]\n",
      "epoch:18 step:17782 [D loss: 0.649517, acc: 67.19%] [G loss: 2.010457]\n",
      "epoch:18 step:17783 [D loss: 0.627120, acc: 70.31%] [G loss: 1.892068]\n",
      "epoch:18 step:17784 [D loss: 0.551914, acc: 73.44%] [G loss: 2.313608]\n",
      "epoch:18 step:17785 [D loss: 0.568449, acc: 71.09%] [G loss: 2.330076]\n",
      "epoch:18 step:17786 [D loss: 0.693916, acc: 57.03%] [G loss: 1.843488]\n",
      "epoch:18 step:17787 [D loss: 0.672673, acc: 57.81%] [G loss: 2.104939]\n",
      "epoch:18 step:17788 [D loss: 0.645639, acc: 67.97%] [G loss: 1.982245]\n",
      "epoch:18 step:17789 [D loss: 0.553602, acc: 71.88%] [G loss: 2.060811]\n",
      "epoch:18 step:17790 [D loss: 0.580427, acc: 68.75%] [G loss: 2.042274]\n",
      "epoch:18 step:17791 [D loss: 0.620113, acc: 67.97%] [G loss: 2.180675]\n",
      "epoch:18 step:17792 [D loss: 0.640495, acc: 64.06%] [G loss: 2.120228]\n",
      "epoch:18 step:17793 [D loss: 0.638309, acc: 67.97%] [G loss: 2.159767]\n",
      "epoch:18 step:17794 [D loss: 0.745760, acc: 52.34%] [G loss: 1.862186]\n",
      "epoch:18 step:17795 [D loss: 0.751712, acc: 48.44%] [G loss: 1.948536]\n",
      "epoch:18 step:17796 [D loss: 0.643998, acc: 66.41%] [G loss: 2.088615]\n",
      "epoch:18 step:17797 [D loss: 0.597598, acc: 64.06%] [G loss: 1.900186]\n",
      "epoch:18 step:17798 [D loss: 0.708605, acc: 55.47%] [G loss: 1.823727]\n",
      "epoch:18 step:17799 [D loss: 0.635535, acc: 67.19%] [G loss: 1.869268]\n",
      "epoch:18 step:17800 [D loss: 0.647860, acc: 60.94%] [G loss: 1.916690]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.325659\n",
      "FID: 14.376288\n",
      "0 = 12.837776894664797\n",
      "1 = 0.08863750581516715\n",
      "2 = 0.8827999830245972\n",
      "3 = 0.8989999890327454\n",
      "4 = 0.866599977016449\n",
      "5 = 0.8707864880561829\n",
      "6 = 0.8989999890327454\n",
      "7 = 6.4979759271621935\n",
      "8 = 0.07727272548777532\n",
      "9 = 0.7257999777793884\n",
      "10 = 0.7337999939918518\n",
      "11 = 0.7178000211715698\n",
      "12 = 0.7222440838813782\n",
      "13 = 0.7337999939918518\n",
      "14 = 7.325689315795898\n",
      "15 = 9.411136627197266\n",
      "16 = 0.1273040622472763\n",
      "17 = 7.325659275054932\n",
      "18 = 14.376288414001465\n",
      "epoch:18 step:17801 [D loss: 0.614604, acc: 71.09%] [G loss: 1.983593]\n",
      "epoch:18 step:17802 [D loss: 0.597974, acc: 69.53%] [G loss: 1.974973]\n",
      "epoch:18 step:17803 [D loss: 0.627044, acc: 62.50%] [G loss: 2.325953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:17804 [D loss: 0.694912, acc: 64.84%] [G loss: 1.876807]\n",
      "epoch:19 step:17805 [D loss: 0.681034, acc: 60.94%] [G loss: 1.780014]\n",
      "epoch:19 step:17806 [D loss: 0.629244, acc: 65.62%] [G loss: 1.990573]\n",
      "epoch:19 step:17807 [D loss: 0.695384, acc: 57.03%] [G loss: 1.816686]\n",
      "epoch:19 step:17808 [D loss: 0.638212, acc: 67.19%] [G loss: 1.827101]\n",
      "epoch:19 step:17809 [D loss: 0.606335, acc: 64.06%] [G loss: 1.870173]\n",
      "epoch:19 step:17810 [D loss: 0.659902, acc: 62.50%] [G loss: 1.950975]\n",
      "epoch:19 step:17811 [D loss: 0.625293, acc: 65.62%] [G loss: 1.845721]\n",
      "epoch:19 step:17812 [D loss: 0.619260, acc: 64.84%] [G loss: 1.920798]\n",
      "epoch:19 step:17813 [D loss: 0.677616, acc: 57.81%] [G loss: 1.910123]\n",
      "epoch:19 step:17814 [D loss: 0.618447, acc: 67.97%] [G loss: 2.014096]\n",
      "epoch:19 step:17815 [D loss: 0.657271, acc: 64.84%] [G loss: 1.893171]\n",
      "epoch:19 step:17816 [D loss: 0.626894, acc: 62.50%] [G loss: 2.005438]\n",
      "epoch:19 step:17817 [D loss: 0.606535, acc: 70.31%] [G loss: 1.997990]\n",
      "epoch:19 step:17818 [D loss: 0.598955, acc: 68.75%] [G loss: 2.214709]\n",
      "epoch:19 step:17819 [D loss: 0.609911, acc: 62.50%] [G loss: 2.033754]\n",
      "epoch:19 step:17820 [D loss: 0.664452, acc: 67.97%] [G loss: 1.835582]\n",
      "epoch:19 step:17821 [D loss: 0.679994, acc: 60.94%] [G loss: 1.793000]\n",
      "epoch:19 step:17822 [D loss: 0.665368, acc: 64.84%] [G loss: 1.980519]\n",
      "epoch:19 step:17823 [D loss: 0.678844, acc: 55.47%] [G loss: 1.753842]\n",
      "epoch:19 step:17824 [D loss: 0.652350, acc: 60.16%] [G loss: 1.849608]\n",
      "epoch:19 step:17825 [D loss: 0.633009, acc: 65.62%] [G loss: 1.925711]\n",
      "epoch:19 step:17826 [D loss: 0.661718, acc: 61.72%] [G loss: 2.051062]\n",
      "epoch:19 step:17827 [D loss: 0.697744, acc: 58.59%] [G loss: 1.974604]\n",
      "epoch:19 step:17828 [D loss: 0.622335, acc: 70.31%] [G loss: 2.009553]\n",
      "epoch:19 step:17829 [D loss: 0.681214, acc: 58.59%] [G loss: 1.798547]\n",
      "epoch:19 step:17830 [D loss: 0.701347, acc: 52.34%] [G loss: 1.688851]\n",
      "epoch:19 step:17831 [D loss: 0.649073, acc: 65.62%] [G loss: 1.785679]\n",
      "epoch:19 step:17832 [D loss: 0.640081, acc: 62.50%] [G loss: 1.878453]\n",
      "epoch:19 step:17833 [D loss: 0.605421, acc: 65.62%] [G loss: 1.841665]\n",
      "epoch:19 step:17834 [D loss: 0.670226, acc: 58.59%] [G loss: 1.755433]\n",
      "epoch:19 step:17835 [D loss: 0.671164, acc: 60.94%] [G loss: 1.743803]\n",
      "epoch:19 step:17836 [D loss: 0.647385, acc: 67.19%] [G loss: 1.738956]\n",
      "epoch:19 step:17837 [D loss: 0.666504, acc: 56.25%] [G loss: 1.866407]\n",
      "epoch:19 step:17838 [D loss: 0.650665, acc: 61.72%] [G loss: 1.892625]\n",
      "epoch:19 step:17839 [D loss: 0.629680, acc: 67.97%] [G loss: 1.856118]\n",
      "epoch:19 step:17840 [D loss: 0.627069, acc: 66.41%] [G loss: 1.923519]\n",
      "epoch:19 step:17841 [D loss: 0.659001, acc: 60.16%] [G loss: 1.848870]\n",
      "epoch:19 step:17842 [D loss: 0.627812, acc: 64.84%] [G loss: 1.970620]\n",
      "epoch:19 step:17843 [D loss: 0.608312, acc: 66.41%] [G loss: 2.157658]\n",
      "epoch:19 step:17844 [D loss: 0.654748, acc: 60.16%] [G loss: 1.778997]\n",
      "epoch:19 step:17845 [D loss: 0.667182, acc: 60.94%] [G loss: 2.092397]\n",
      "epoch:19 step:17846 [D loss: 0.626352, acc: 65.62%] [G loss: 1.850254]\n",
      "epoch:19 step:17847 [D loss: 0.629812, acc: 67.19%] [G loss: 2.040841]\n",
      "epoch:19 step:17848 [D loss: 0.623336, acc: 65.62%] [G loss: 2.004653]\n",
      "epoch:19 step:17849 [D loss: 0.633278, acc: 61.72%] [G loss: 1.854169]\n",
      "epoch:19 step:17850 [D loss: 0.602293, acc: 66.41%] [G loss: 2.003842]\n",
      "epoch:19 step:17851 [D loss: 0.599443, acc: 71.09%] [G loss: 1.969328]\n",
      "epoch:19 step:17852 [D loss: 0.664613, acc: 60.94%] [G loss: 2.033674]\n",
      "epoch:19 step:17853 [D loss: 0.613700, acc: 66.41%] [G loss: 1.935163]\n",
      "epoch:19 step:17854 [D loss: 0.679217, acc: 58.59%] [G loss: 1.817382]\n",
      "epoch:19 step:17855 [D loss: 0.629961, acc: 68.75%] [G loss: 1.865669]\n",
      "epoch:19 step:17856 [D loss: 0.638514, acc: 62.50%] [G loss: 2.103317]\n",
      "epoch:19 step:17857 [D loss: 0.637491, acc: 64.06%] [G loss: 1.934513]\n",
      "epoch:19 step:17858 [D loss: 0.591538, acc: 70.31%] [G loss: 1.952232]\n",
      "epoch:19 step:17859 [D loss: 0.635743, acc: 60.16%] [G loss: 2.137288]\n",
      "epoch:19 step:17860 [D loss: 0.622035, acc: 60.94%] [G loss: 2.020366]\n",
      "epoch:19 step:17861 [D loss: 0.656225, acc: 61.72%] [G loss: 1.956609]\n",
      "epoch:19 step:17862 [D loss: 0.651825, acc: 63.28%] [G loss: 1.918021]\n",
      "epoch:19 step:17863 [D loss: 0.697156, acc: 52.34%] [G loss: 1.771190]\n",
      "epoch:19 step:17864 [D loss: 0.626092, acc: 66.41%] [G loss: 1.873444]\n",
      "epoch:19 step:17865 [D loss: 0.653921, acc: 66.41%] [G loss: 1.970129]\n",
      "epoch:19 step:17866 [D loss: 0.637864, acc: 66.41%] [G loss: 1.864807]\n",
      "epoch:19 step:17867 [D loss: 0.627270, acc: 67.19%] [G loss: 1.904422]\n",
      "epoch:19 step:17868 [D loss: 0.629730, acc: 64.06%] [G loss: 1.890603]\n",
      "epoch:19 step:17869 [D loss: 0.611851, acc: 67.19%] [G loss: 1.808330]\n",
      "epoch:19 step:17870 [D loss: 0.670305, acc: 58.59%] [G loss: 1.993685]\n",
      "epoch:19 step:17871 [D loss: 0.675449, acc: 57.03%] [G loss: 1.936763]\n",
      "epoch:19 step:17872 [D loss: 0.641651, acc: 64.06%] [G loss: 2.005771]\n",
      "epoch:19 step:17873 [D loss: 0.602616, acc: 68.75%] [G loss: 2.010331]\n",
      "epoch:19 step:17874 [D loss: 0.692630, acc: 54.69%] [G loss: 1.830616]\n",
      "epoch:19 step:17875 [D loss: 0.603057, acc: 65.62%] [G loss: 1.878236]\n",
      "epoch:19 step:17876 [D loss: 0.655661, acc: 60.16%] [G loss: 1.896967]\n",
      "epoch:19 step:17877 [D loss: 0.667411, acc: 53.91%] [G loss: 1.893798]\n",
      "epoch:19 step:17878 [D loss: 0.663499, acc: 63.28%] [G loss: 1.981666]\n",
      "epoch:19 step:17879 [D loss: 0.627646, acc: 60.94%] [G loss: 1.938650]\n",
      "epoch:19 step:17880 [D loss: 0.597775, acc: 66.41%] [G loss: 2.020329]\n",
      "epoch:19 step:17881 [D loss: 0.693176, acc: 53.91%] [G loss: 1.936006]\n",
      "epoch:19 step:17882 [D loss: 0.624487, acc: 64.84%] [G loss: 1.783686]\n",
      "epoch:19 step:17883 [D loss: 0.618391, acc: 69.53%] [G loss: 1.867675]\n",
      "epoch:19 step:17884 [D loss: 0.692147, acc: 57.03%] [G loss: 1.815680]\n",
      "epoch:19 step:17885 [D loss: 0.650981, acc: 58.59%] [G loss: 1.896343]\n",
      "epoch:19 step:17886 [D loss: 0.674698, acc: 60.94%] [G loss: 1.791629]\n",
      "epoch:19 step:17887 [D loss: 0.695353, acc: 59.38%] [G loss: 1.883181]\n",
      "epoch:19 step:17888 [D loss: 0.640740, acc: 60.16%] [G loss: 1.815391]\n",
      "epoch:19 step:17889 [D loss: 0.638173, acc: 65.62%] [G loss: 1.871456]\n",
      "epoch:19 step:17890 [D loss: 0.630559, acc: 69.53%] [G loss: 1.982787]\n",
      "epoch:19 step:17891 [D loss: 0.632831, acc: 66.41%] [G loss: 1.954652]\n",
      "epoch:19 step:17892 [D loss: 0.641218, acc: 62.50%] [G loss: 1.878480]\n",
      "epoch:19 step:17893 [D loss: 0.616795, acc: 64.06%] [G loss: 1.814616]\n",
      "epoch:19 step:17894 [D loss: 0.705940, acc: 56.25%] [G loss: 1.814337]\n",
      "epoch:19 step:17895 [D loss: 0.655635, acc: 60.16%] [G loss: 1.881757]\n",
      "epoch:19 step:17896 [D loss: 0.624724, acc: 67.97%] [G loss: 2.088088]\n",
      "epoch:19 step:17897 [D loss: 0.643608, acc: 60.16%] [G loss: 1.956417]\n",
      "epoch:19 step:17898 [D loss: 0.718362, acc: 56.25%] [G loss: 1.822491]\n",
      "epoch:19 step:17899 [D loss: 0.645214, acc: 63.28%] [G loss: 1.926996]\n",
      "epoch:19 step:17900 [D loss: 0.610508, acc: 62.50%] [G loss: 1.975940]\n",
      "epoch:19 step:17901 [D loss: 0.679412, acc: 57.03%] [G loss: 1.809737]\n",
      "epoch:19 step:17902 [D loss: 0.640844, acc: 58.59%] [G loss: 1.800439]\n",
      "epoch:19 step:17903 [D loss: 0.668588, acc: 60.16%] [G loss: 1.875625]\n",
      "epoch:19 step:17904 [D loss: 0.663763, acc: 64.06%] [G loss: 1.918816]\n",
      "epoch:19 step:17905 [D loss: 0.585377, acc: 66.41%] [G loss: 2.061497]\n",
      "epoch:19 step:17906 [D loss: 0.614061, acc: 67.97%] [G loss: 1.860457]\n",
      "epoch:19 step:17907 [D loss: 0.647296, acc: 64.84%] [G loss: 1.918131]\n",
      "epoch:19 step:17908 [D loss: 0.611177, acc: 65.62%] [G loss: 1.931902]\n",
      "epoch:19 step:17909 [D loss: 0.620214, acc: 62.50%] [G loss: 2.118156]\n",
      "epoch:19 step:17910 [D loss: 0.604524, acc: 70.31%] [G loss: 2.125996]\n",
      "epoch:19 step:17911 [D loss: 0.657524, acc: 62.50%] [G loss: 1.808347]\n",
      "epoch:19 step:17912 [D loss: 0.648750, acc: 59.38%] [G loss: 1.794655]\n",
      "epoch:19 step:17913 [D loss: 0.684121, acc: 52.34%] [G loss: 1.783030]\n",
      "epoch:19 step:17914 [D loss: 0.633162, acc: 65.62%] [G loss: 2.035933]\n",
      "epoch:19 step:17915 [D loss: 0.661403, acc: 56.25%] [G loss: 2.183064]\n",
      "epoch:19 step:17916 [D loss: 0.615444, acc: 63.28%] [G loss: 2.101508]\n",
      "epoch:19 step:17917 [D loss: 0.618859, acc: 63.28%] [G loss: 2.070441]\n",
      "epoch:19 step:17918 [D loss: 0.604302, acc: 69.53%] [G loss: 2.200240]\n",
      "epoch:19 step:17919 [D loss: 0.581467, acc: 67.97%] [G loss: 2.175526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:17920 [D loss: 0.611517, acc: 64.84%] [G loss: 2.002714]\n",
      "epoch:19 step:17921 [D loss: 0.673993, acc: 57.81%] [G loss: 2.006461]\n",
      "epoch:19 step:17922 [D loss: 0.605978, acc: 63.28%] [G loss: 2.157788]\n",
      "epoch:19 step:17923 [D loss: 0.760858, acc: 47.66%] [G loss: 1.840991]\n",
      "epoch:19 step:17924 [D loss: 0.638792, acc: 57.81%] [G loss: 1.928609]\n",
      "epoch:19 step:17925 [D loss: 0.637537, acc: 67.19%] [G loss: 1.884370]\n",
      "epoch:19 step:17926 [D loss: 0.677086, acc: 60.16%] [G loss: 1.870839]\n",
      "epoch:19 step:17927 [D loss: 0.645675, acc: 61.72%] [G loss: 1.884635]\n",
      "epoch:19 step:17928 [D loss: 0.688907, acc: 57.81%] [G loss: 1.849826]\n",
      "epoch:19 step:17929 [D loss: 0.620819, acc: 69.53%] [G loss: 1.982383]\n",
      "epoch:19 step:17930 [D loss: 0.698200, acc: 56.25%] [G loss: 1.864494]\n",
      "epoch:19 step:17931 [D loss: 0.664814, acc: 64.06%] [G loss: 1.875437]\n",
      "epoch:19 step:17932 [D loss: 0.657578, acc: 55.47%] [G loss: 1.886579]\n",
      "epoch:19 step:17933 [D loss: 0.695668, acc: 59.38%] [G loss: 1.882351]\n",
      "epoch:19 step:17934 [D loss: 0.625900, acc: 59.38%] [G loss: 1.846666]\n",
      "epoch:19 step:17935 [D loss: 0.642842, acc: 60.94%] [G loss: 1.862515]\n",
      "epoch:19 step:17936 [D loss: 0.652711, acc: 61.72%] [G loss: 1.842805]\n",
      "epoch:19 step:17937 [D loss: 0.695850, acc: 57.81%] [G loss: 1.749723]\n",
      "epoch:19 step:17938 [D loss: 0.604659, acc: 67.19%] [G loss: 1.855358]\n",
      "epoch:19 step:17939 [D loss: 0.676771, acc: 61.72%] [G loss: 1.688970]\n",
      "epoch:19 step:17940 [D loss: 0.658923, acc: 60.94%] [G loss: 1.762649]\n",
      "epoch:19 step:17941 [D loss: 0.687782, acc: 57.81%] [G loss: 1.853285]\n",
      "epoch:19 step:17942 [D loss: 0.674777, acc: 59.38%] [G loss: 1.780717]\n",
      "epoch:19 step:17943 [D loss: 0.631799, acc: 60.16%] [G loss: 1.791017]\n",
      "epoch:19 step:17944 [D loss: 0.645670, acc: 63.28%] [G loss: 1.790030]\n",
      "epoch:19 step:17945 [D loss: 0.596307, acc: 70.31%] [G loss: 1.917697]\n",
      "epoch:19 step:17946 [D loss: 0.649211, acc: 64.84%] [G loss: 1.746957]\n",
      "epoch:19 step:17947 [D loss: 0.673641, acc: 57.81%] [G loss: 1.814072]\n",
      "epoch:19 step:17948 [D loss: 0.665612, acc: 57.81%] [G loss: 1.815265]\n",
      "epoch:19 step:17949 [D loss: 0.651483, acc: 63.28%] [G loss: 1.954839]\n",
      "epoch:19 step:17950 [D loss: 0.676576, acc: 60.16%] [G loss: 1.857294]\n",
      "epoch:19 step:17951 [D loss: 0.608880, acc: 67.97%] [G loss: 1.860043]\n",
      "epoch:19 step:17952 [D loss: 0.634135, acc: 65.62%] [G loss: 1.901185]\n",
      "epoch:19 step:17953 [D loss: 0.708070, acc: 60.94%] [G loss: 2.056698]\n",
      "epoch:19 step:17954 [D loss: 0.655110, acc: 58.59%] [G loss: 1.809486]\n",
      "epoch:19 step:17955 [D loss: 0.651049, acc: 60.16%] [G loss: 1.818062]\n",
      "epoch:19 step:17956 [D loss: 0.606345, acc: 64.06%] [G loss: 1.791365]\n",
      "epoch:19 step:17957 [D loss: 0.660240, acc: 60.16%] [G loss: 1.954006]\n",
      "epoch:19 step:17958 [D loss: 0.615922, acc: 66.41%] [G loss: 1.811265]\n",
      "epoch:19 step:17959 [D loss: 0.625430, acc: 66.41%] [G loss: 1.821637]\n",
      "epoch:19 step:17960 [D loss: 0.630078, acc: 67.19%] [G loss: 1.893702]\n",
      "epoch:19 step:17961 [D loss: 0.615171, acc: 64.84%] [G loss: 1.831719]\n",
      "epoch:19 step:17962 [D loss: 0.648875, acc: 62.50%] [G loss: 1.812044]\n",
      "epoch:19 step:17963 [D loss: 0.716021, acc: 56.25%] [G loss: 1.854290]\n",
      "epoch:19 step:17964 [D loss: 0.676832, acc: 60.94%] [G loss: 1.906967]\n",
      "epoch:19 step:17965 [D loss: 0.670488, acc: 60.16%] [G loss: 1.815198]\n",
      "epoch:19 step:17966 [D loss: 0.628608, acc: 65.62%] [G loss: 1.789328]\n",
      "epoch:19 step:17967 [D loss: 0.651866, acc: 60.16%] [G loss: 1.872696]\n",
      "epoch:19 step:17968 [D loss: 0.665766, acc: 57.81%] [G loss: 1.811752]\n",
      "epoch:19 step:17969 [D loss: 0.664959, acc: 59.38%] [G loss: 1.941699]\n",
      "epoch:19 step:17970 [D loss: 0.625991, acc: 64.84%] [G loss: 1.877185]\n",
      "epoch:19 step:17971 [D loss: 0.615720, acc: 67.19%] [G loss: 1.917252]\n",
      "epoch:19 step:17972 [D loss: 0.708589, acc: 53.12%] [G loss: 1.845742]\n",
      "epoch:19 step:17973 [D loss: 0.669906, acc: 55.47%] [G loss: 1.811434]\n",
      "epoch:19 step:17974 [D loss: 0.655104, acc: 58.59%] [G loss: 1.760364]\n",
      "epoch:19 step:17975 [D loss: 0.673716, acc: 57.81%] [G loss: 1.884634]\n",
      "epoch:19 step:17976 [D loss: 0.673648, acc: 55.47%] [G loss: 1.811110]\n",
      "epoch:19 step:17977 [D loss: 0.693406, acc: 55.47%] [G loss: 1.696238]\n",
      "epoch:19 step:17978 [D loss: 0.665975, acc: 57.81%] [G loss: 1.678644]\n",
      "epoch:19 step:17979 [D loss: 0.686954, acc: 57.81%] [G loss: 1.698316]\n",
      "epoch:19 step:17980 [D loss: 0.686523, acc: 57.81%] [G loss: 1.717239]\n",
      "epoch:19 step:17981 [D loss: 0.676119, acc: 60.94%] [G loss: 1.753653]\n",
      "epoch:19 step:17982 [D loss: 0.627004, acc: 66.41%] [G loss: 1.742757]\n",
      "epoch:19 step:17983 [D loss: 0.653428, acc: 64.06%] [G loss: 1.791458]\n",
      "epoch:19 step:17984 [D loss: 0.661311, acc: 60.16%] [G loss: 1.770982]\n",
      "epoch:19 step:17985 [D loss: 0.690752, acc: 55.47%] [G loss: 1.714193]\n",
      "epoch:19 step:17986 [D loss: 0.677609, acc: 57.81%] [G loss: 1.740297]\n",
      "epoch:19 step:17987 [D loss: 0.626213, acc: 66.41%] [G loss: 1.807172]\n",
      "epoch:19 step:17988 [D loss: 0.625952, acc: 66.41%] [G loss: 1.766007]\n",
      "epoch:19 step:17989 [D loss: 0.633242, acc: 66.41%] [G loss: 1.880313]\n",
      "epoch:19 step:17990 [D loss: 0.667195, acc: 56.25%] [G loss: 1.913018]\n",
      "epoch:19 step:17991 [D loss: 0.660212, acc: 57.03%] [G loss: 1.801836]\n",
      "epoch:19 step:17992 [D loss: 0.647365, acc: 61.72%] [G loss: 1.853693]\n",
      "epoch:19 step:17993 [D loss: 0.640673, acc: 63.28%] [G loss: 1.853492]\n",
      "epoch:19 step:17994 [D loss: 0.618819, acc: 63.28%] [G loss: 1.897551]\n",
      "epoch:19 step:17995 [D loss: 0.607936, acc: 66.41%] [G loss: 1.934108]\n",
      "epoch:19 step:17996 [D loss: 0.658275, acc: 58.59%] [G loss: 1.973853]\n",
      "epoch:19 step:17997 [D loss: 0.605343, acc: 65.62%] [G loss: 1.958273]\n",
      "epoch:19 step:17998 [D loss: 0.667562, acc: 59.38%] [G loss: 1.869604]\n",
      "epoch:19 step:17999 [D loss: 0.632631, acc: 61.72%] [G loss: 1.897412]\n",
      "epoch:19 step:18000 [D loss: 0.655167, acc: 57.81%] [G loss: 1.993854]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.393000\n",
      "FID: 12.512624\n",
      "0 = 12.738484064626702\n",
      "1 = 0.0920963677434571\n",
      "2 = 0.8809999823570251\n",
      "3 = 0.8888000249862671\n",
      "4 = 0.873199999332428\n",
      "5 = 0.8751477003097534\n",
      "6 = 0.8888000249862671\n",
      "7 = 6.342684030735501\n",
      "8 = 0.06814541711994053\n",
      "9 = 0.7186999917030334\n",
      "10 = 0.7346000075340271\n",
      "11 = 0.7027999758720398\n",
      "12 = 0.7119596600532532\n",
      "13 = 0.7346000075340271\n",
      "14 = 7.3930277824401855\n",
      "15 = 9.470329284667969\n",
      "16 = 0.10449879616498947\n",
      "17 = 7.392999649047852\n",
      "18 = 12.51262378692627\n",
      "epoch:19 step:18001 [D loss: 0.600589, acc: 69.53%] [G loss: 2.012609]\n",
      "epoch:19 step:18002 [D loss: 0.688191, acc: 59.38%] [G loss: 1.966033]\n",
      "epoch:19 step:18003 [D loss: 0.670554, acc: 57.03%] [G loss: 1.730596]\n",
      "epoch:19 step:18004 [D loss: 0.658793, acc: 60.94%] [G loss: 1.856208]\n",
      "epoch:19 step:18005 [D loss: 0.663826, acc: 60.16%] [G loss: 1.857523]\n",
      "epoch:19 step:18006 [D loss: 0.636673, acc: 64.06%] [G loss: 1.834984]\n",
      "epoch:19 step:18007 [D loss: 0.637478, acc: 63.28%] [G loss: 1.773590]\n",
      "epoch:19 step:18008 [D loss: 0.667744, acc: 63.28%] [G loss: 1.769135]\n",
      "epoch:19 step:18009 [D loss: 0.606741, acc: 62.50%] [G loss: 1.958213]\n",
      "epoch:19 step:18010 [D loss: 0.594981, acc: 67.97%] [G loss: 2.134079]\n",
      "epoch:19 step:18011 [D loss: 0.567729, acc: 69.53%] [G loss: 2.168982]\n",
      "epoch:19 step:18012 [D loss: 0.580645, acc: 72.66%] [G loss: 1.986200]\n",
      "epoch:19 step:18013 [D loss: 0.657182, acc: 60.16%] [G loss: 1.864122]\n",
      "epoch:19 step:18014 [D loss: 0.655872, acc: 61.72%] [G loss: 1.873153]\n",
      "epoch:19 step:18015 [D loss: 0.682522, acc: 59.38%] [G loss: 1.903879]\n",
      "epoch:19 step:18016 [D loss: 0.634104, acc: 67.19%] [G loss: 1.917374]\n",
      "epoch:19 step:18017 [D loss: 0.743462, acc: 51.56%] [G loss: 1.814166]\n",
      "epoch:19 step:18018 [D loss: 0.674187, acc: 56.25%] [G loss: 1.876555]\n",
      "epoch:19 step:18019 [D loss: 0.657233, acc: 60.94%] [G loss: 2.009531]\n",
      "epoch:19 step:18020 [D loss: 0.662648, acc: 62.50%] [G loss: 1.912784]\n",
      "epoch:19 step:18021 [D loss: 0.601451, acc: 71.09%] [G loss: 2.022110]\n",
      "epoch:19 step:18022 [D loss: 0.623493, acc: 66.41%] [G loss: 2.006823]\n",
      "epoch:19 step:18023 [D loss: 0.661615, acc: 62.50%] [G loss: 1.726126]\n",
      "epoch:19 step:18024 [D loss: 0.665568, acc: 60.16%] [G loss: 2.165410]\n",
      "epoch:19 step:18025 [D loss: 0.596878, acc: 67.19%] [G loss: 2.037093]\n",
      "epoch:19 step:18026 [D loss: 0.672474, acc: 60.16%] [G loss: 1.846805]\n",
      "epoch:19 step:18027 [D loss: 0.659755, acc: 55.47%] [G loss: 1.881064]\n",
      "epoch:19 step:18028 [D loss: 0.652040, acc: 60.94%] [G loss: 1.787924]\n",
      "epoch:19 step:18029 [D loss: 0.652088, acc: 61.72%] [G loss: 1.865705]\n",
      "epoch:19 step:18030 [D loss: 0.652326, acc: 61.72%] [G loss: 1.772122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18031 [D loss: 0.646303, acc: 64.06%] [G loss: 1.948198]\n",
      "epoch:19 step:18032 [D loss: 0.568961, acc: 71.88%] [G loss: 1.988007]\n",
      "epoch:19 step:18033 [D loss: 0.577538, acc: 68.75%] [G loss: 2.104541]\n",
      "epoch:19 step:18034 [D loss: 0.624210, acc: 68.75%] [G loss: 2.142635]\n",
      "epoch:19 step:18035 [D loss: 0.546503, acc: 68.75%] [G loss: 2.321470]\n",
      "epoch:19 step:18036 [D loss: 0.641757, acc: 64.84%] [G loss: 1.945864]\n",
      "epoch:19 step:18037 [D loss: 0.688915, acc: 58.59%] [G loss: 1.875736]\n",
      "epoch:19 step:18038 [D loss: 0.665240, acc: 59.38%] [G loss: 1.965430]\n",
      "epoch:19 step:18039 [D loss: 0.665110, acc: 60.16%] [G loss: 1.903599]\n",
      "epoch:19 step:18040 [D loss: 0.619347, acc: 67.19%] [G loss: 1.988800]\n",
      "epoch:19 step:18041 [D loss: 0.628704, acc: 67.97%] [G loss: 1.965534]\n",
      "epoch:19 step:18042 [D loss: 0.633032, acc: 66.41%] [G loss: 1.870412]\n",
      "epoch:19 step:18043 [D loss: 0.630571, acc: 63.28%] [G loss: 2.010742]\n",
      "epoch:19 step:18044 [D loss: 0.642828, acc: 61.72%] [G loss: 1.955839]\n",
      "epoch:19 step:18045 [D loss: 0.585818, acc: 74.22%] [G loss: 2.098369]\n",
      "epoch:19 step:18046 [D loss: 0.659627, acc: 59.38%] [G loss: 1.858591]\n",
      "epoch:19 step:18047 [D loss: 0.640048, acc: 64.06%] [G loss: 1.977363]\n",
      "epoch:19 step:18048 [D loss: 0.591678, acc: 64.84%] [G loss: 2.060322]\n",
      "epoch:19 step:18049 [D loss: 0.637959, acc: 65.62%] [G loss: 1.885226]\n",
      "epoch:19 step:18050 [D loss: 0.680253, acc: 54.69%] [G loss: 1.947009]\n",
      "epoch:19 step:18051 [D loss: 0.618576, acc: 68.75%] [G loss: 1.949286]\n",
      "epoch:19 step:18052 [D loss: 0.684322, acc: 55.47%] [G loss: 1.825197]\n",
      "epoch:19 step:18053 [D loss: 0.722867, acc: 53.91%] [G loss: 1.702811]\n",
      "epoch:19 step:18054 [D loss: 0.707885, acc: 52.34%] [G loss: 1.784473]\n",
      "epoch:19 step:18055 [D loss: 0.681685, acc: 60.16%] [G loss: 1.704981]\n",
      "epoch:19 step:18056 [D loss: 0.652087, acc: 59.38%] [G loss: 1.787642]\n",
      "epoch:19 step:18057 [D loss: 0.655250, acc: 55.47%] [G loss: 1.704384]\n",
      "epoch:19 step:18058 [D loss: 0.653210, acc: 61.72%] [G loss: 1.791921]\n",
      "epoch:19 step:18059 [D loss: 0.694290, acc: 60.16%] [G loss: 1.854922]\n",
      "epoch:19 step:18060 [D loss: 0.655998, acc: 58.59%] [G loss: 1.773614]\n",
      "epoch:19 step:18061 [D loss: 0.639982, acc: 63.28%] [G loss: 1.767055]\n",
      "epoch:19 step:18062 [D loss: 0.650130, acc: 66.41%] [G loss: 1.820399]\n",
      "epoch:19 step:18063 [D loss: 0.649507, acc: 60.16%] [G loss: 1.776583]\n",
      "epoch:19 step:18064 [D loss: 0.664585, acc: 58.59%] [G loss: 1.828061]\n",
      "epoch:19 step:18065 [D loss: 0.651915, acc: 65.62%] [G loss: 1.876898]\n",
      "epoch:19 step:18066 [D loss: 0.630131, acc: 62.50%] [G loss: 2.004949]\n",
      "epoch:19 step:18067 [D loss: 0.626117, acc: 62.50%] [G loss: 2.055837]\n",
      "epoch:19 step:18068 [D loss: 0.676351, acc: 60.16%] [G loss: 1.808757]\n",
      "epoch:19 step:18069 [D loss: 0.666664, acc: 57.81%] [G loss: 1.863714]\n",
      "epoch:19 step:18070 [D loss: 0.684534, acc: 59.38%] [G loss: 1.775524]\n",
      "epoch:19 step:18071 [D loss: 0.656800, acc: 66.41%] [G loss: 1.819297]\n",
      "epoch:19 step:18072 [D loss: 0.648094, acc: 60.94%] [G loss: 1.762502]\n",
      "epoch:19 step:18073 [D loss: 0.666267, acc: 60.94%] [G loss: 1.806008]\n",
      "epoch:19 step:18074 [D loss: 0.623536, acc: 64.84%] [G loss: 1.886256]\n",
      "epoch:19 step:18075 [D loss: 0.624817, acc: 64.84%] [G loss: 1.782077]\n",
      "epoch:19 step:18076 [D loss: 0.648514, acc: 60.94%] [G loss: 1.970627]\n",
      "epoch:19 step:18077 [D loss: 0.592618, acc: 67.19%] [G loss: 1.857021]\n",
      "epoch:19 step:18078 [D loss: 0.570434, acc: 69.53%] [G loss: 2.006688]\n",
      "epoch:19 step:18079 [D loss: 0.628284, acc: 62.50%] [G loss: 2.070961]\n",
      "epoch:19 step:18080 [D loss: 0.686454, acc: 59.38%] [G loss: 1.869454]\n",
      "epoch:19 step:18081 [D loss: 0.611753, acc: 61.72%] [G loss: 1.806149]\n",
      "epoch:19 step:18082 [D loss: 0.687778, acc: 58.59%] [G loss: 1.834204]\n",
      "epoch:19 step:18083 [D loss: 0.648306, acc: 63.28%] [G loss: 1.910566]\n",
      "epoch:19 step:18084 [D loss: 0.669936, acc: 60.94%] [G loss: 1.901917]\n",
      "epoch:19 step:18085 [D loss: 0.666621, acc: 58.59%] [G loss: 1.874037]\n",
      "epoch:19 step:18086 [D loss: 0.664044, acc: 64.84%] [G loss: 1.843338]\n",
      "epoch:19 step:18087 [D loss: 0.628495, acc: 66.41%] [G loss: 1.822654]\n",
      "epoch:19 step:18088 [D loss: 0.656607, acc: 63.28%] [G loss: 1.888212]\n",
      "epoch:19 step:18089 [D loss: 0.595615, acc: 65.62%] [G loss: 1.924433]\n",
      "epoch:19 step:18090 [D loss: 0.620101, acc: 65.62%] [G loss: 1.814872]\n",
      "epoch:19 step:18091 [D loss: 0.679079, acc: 60.16%] [G loss: 1.836474]\n",
      "epoch:19 step:18092 [D loss: 0.625295, acc: 66.41%] [G loss: 1.858498]\n",
      "epoch:19 step:18093 [D loss: 0.690659, acc: 54.69%] [G loss: 1.896520]\n",
      "epoch:19 step:18094 [D loss: 0.663932, acc: 64.84%] [G loss: 1.865515]\n",
      "epoch:19 step:18095 [D loss: 0.629241, acc: 65.62%] [G loss: 1.818611]\n",
      "epoch:19 step:18096 [D loss: 0.648714, acc: 64.06%] [G loss: 1.788101]\n",
      "epoch:19 step:18097 [D loss: 0.619723, acc: 64.84%] [G loss: 1.904962]\n",
      "epoch:19 step:18098 [D loss: 0.674637, acc: 63.28%] [G loss: 1.854894]\n",
      "epoch:19 step:18099 [D loss: 0.678476, acc: 54.69%] [G loss: 1.916762]\n",
      "epoch:19 step:18100 [D loss: 0.624141, acc: 64.06%] [G loss: 1.882064]\n",
      "epoch:19 step:18101 [D loss: 0.635989, acc: 63.28%] [G loss: 1.959273]\n",
      "epoch:19 step:18102 [D loss: 0.634324, acc: 64.06%] [G loss: 2.072309]\n",
      "epoch:19 step:18103 [D loss: 0.634289, acc: 65.62%] [G loss: 1.904736]\n",
      "epoch:19 step:18104 [D loss: 0.688143, acc: 53.91%] [G loss: 1.789980]\n",
      "epoch:19 step:18105 [D loss: 0.695486, acc: 53.12%] [G loss: 2.081919]\n",
      "epoch:19 step:18106 [D loss: 0.684983, acc: 57.03%] [G loss: 1.789425]\n",
      "epoch:19 step:18107 [D loss: 0.655695, acc: 57.03%] [G loss: 1.771661]\n",
      "epoch:19 step:18108 [D loss: 0.638591, acc: 64.06%] [G loss: 1.816097]\n",
      "epoch:19 step:18109 [D loss: 0.642888, acc: 61.72%] [G loss: 1.881541]\n",
      "epoch:19 step:18110 [D loss: 0.628974, acc: 64.84%] [G loss: 1.832528]\n",
      "epoch:19 step:18111 [D loss: 0.634647, acc: 65.62%] [G loss: 1.735230]\n",
      "epoch:19 step:18112 [D loss: 0.598866, acc: 63.28%] [G loss: 1.853219]\n",
      "epoch:19 step:18113 [D loss: 0.628960, acc: 67.97%] [G loss: 1.808950]\n",
      "epoch:19 step:18114 [D loss: 0.646710, acc: 59.38%] [G loss: 1.824299]\n",
      "epoch:19 step:18115 [D loss: 0.593845, acc: 70.31%] [G loss: 2.029932]\n",
      "epoch:19 step:18116 [D loss: 0.611671, acc: 68.75%] [G loss: 2.147552]\n",
      "epoch:19 step:18117 [D loss: 0.596234, acc: 66.41%] [G loss: 2.085582]\n",
      "epoch:19 step:18118 [D loss: 0.551123, acc: 68.75%] [G loss: 2.184276]\n",
      "epoch:19 step:18119 [D loss: 0.721446, acc: 51.56%] [G loss: 1.786655]\n",
      "epoch:19 step:18120 [D loss: 0.698705, acc: 57.81%] [G loss: 1.829807]\n",
      "epoch:19 step:18121 [D loss: 0.641637, acc: 66.41%] [G loss: 1.873407]\n",
      "epoch:19 step:18122 [D loss: 0.653246, acc: 61.72%] [G loss: 1.960878]\n",
      "epoch:19 step:18123 [D loss: 0.562696, acc: 73.44%] [G loss: 1.845712]\n",
      "epoch:19 step:18124 [D loss: 0.663700, acc: 59.38%] [G loss: 2.038788]\n",
      "epoch:19 step:18125 [D loss: 0.682218, acc: 62.50%] [G loss: 1.858235]\n",
      "epoch:19 step:18126 [D loss: 0.668324, acc: 58.59%] [G loss: 1.881686]\n",
      "epoch:19 step:18127 [D loss: 0.682535, acc: 53.91%] [G loss: 1.886924]\n",
      "epoch:19 step:18128 [D loss: 0.625156, acc: 64.06%] [G loss: 1.928281]\n",
      "epoch:19 step:18129 [D loss: 0.626117, acc: 64.06%] [G loss: 1.867927]\n",
      "epoch:19 step:18130 [D loss: 0.710537, acc: 58.59%] [G loss: 1.798430]\n",
      "epoch:19 step:18131 [D loss: 0.658169, acc: 58.59%] [G loss: 1.879719]\n",
      "epoch:19 step:18132 [D loss: 0.630509, acc: 63.28%] [G loss: 1.965226]\n",
      "epoch:19 step:18133 [D loss: 0.626267, acc: 63.28%] [G loss: 1.921953]\n",
      "epoch:19 step:18134 [D loss: 0.577542, acc: 69.53%] [G loss: 1.825698]\n",
      "epoch:19 step:18135 [D loss: 0.633760, acc: 64.06%] [G loss: 1.832204]\n",
      "epoch:19 step:18136 [D loss: 0.601759, acc: 71.88%] [G loss: 1.917085]\n",
      "epoch:19 step:18137 [D loss: 0.669326, acc: 57.81%] [G loss: 1.955117]\n",
      "epoch:19 step:18138 [D loss: 0.653023, acc: 61.72%] [G loss: 1.869936]\n",
      "epoch:19 step:18139 [D loss: 0.599245, acc: 67.97%] [G loss: 1.853536]\n",
      "epoch:19 step:18140 [D loss: 0.637749, acc: 67.19%] [G loss: 2.097934]\n",
      "epoch:19 step:18141 [D loss: 0.650845, acc: 60.16%] [G loss: 1.975862]\n",
      "epoch:19 step:18142 [D loss: 0.664379, acc: 67.97%] [G loss: 1.953494]\n",
      "epoch:19 step:18143 [D loss: 0.612418, acc: 68.75%] [G loss: 2.175582]\n",
      "epoch:19 step:18144 [D loss: 0.748396, acc: 51.56%] [G loss: 1.759687]\n",
      "epoch:19 step:18145 [D loss: 0.680138, acc: 58.59%] [G loss: 1.843796]\n",
      "epoch:19 step:18146 [D loss: 0.630800, acc: 64.84%] [G loss: 1.911854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18147 [D loss: 0.640244, acc: 64.06%] [G loss: 1.934990]\n",
      "epoch:19 step:18148 [D loss: 0.566258, acc: 75.78%] [G loss: 2.230651]\n",
      "epoch:19 step:18149 [D loss: 0.576678, acc: 72.66%] [G loss: 2.243655]\n",
      "epoch:19 step:18150 [D loss: 0.580116, acc: 71.09%] [G loss: 2.342454]\n",
      "epoch:19 step:18151 [D loss: 0.733009, acc: 58.59%] [G loss: 1.873664]\n",
      "epoch:19 step:18152 [D loss: 0.704854, acc: 51.56%] [G loss: 1.811981]\n",
      "epoch:19 step:18153 [D loss: 0.633990, acc: 67.19%] [G loss: 1.941257]\n",
      "epoch:19 step:18154 [D loss: 0.640385, acc: 63.28%] [G loss: 1.795366]\n",
      "epoch:19 step:18155 [D loss: 0.651000, acc: 63.28%] [G loss: 1.722699]\n",
      "epoch:19 step:18156 [D loss: 0.690058, acc: 62.50%] [G loss: 1.953394]\n",
      "epoch:19 step:18157 [D loss: 0.605280, acc: 63.28%] [G loss: 2.089829]\n",
      "epoch:19 step:18158 [D loss: 0.732991, acc: 57.03%] [G loss: 1.830350]\n",
      "epoch:19 step:18159 [D loss: 0.646273, acc: 60.16%] [G loss: 1.736069]\n",
      "epoch:19 step:18160 [D loss: 0.593246, acc: 69.53%] [G loss: 1.956400]\n",
      "epoch:19 step:18161 [D loss: 0.583732, acc: 70.31%] [G loss: 1.979604]\n",
      "epoch:19 step:18162 [D loss: 0.588087, acc: 67.97%] [G loss: 1.903714]\n",
      "epoch:19 step:18163 [D loss: 0.683351, acc: 59.38%] [G loss: 1.924272]\n",
      "epoch:19 step:18164 [D loss: 0.656887, acc: 62.50%] [G loss: 1.927244]\n",
      "epoch:19 step:18165 [D loss: 0.687057, acc: 53.12%] [G loss: 1.866147]\n",
      "epoch:19 step:18166 [D loss: 0.613151, acc: 70.31%] [G loss: 2.028130]\n",
      "epoch:19 step:18167 [D loss: 0.610679, acc: 69.53%] [G loss: 2.035841]\n",
      "epoch:19 step:18168 [D loss: 0.612253, acc: 71.88%] [G loss: 1.999936]\n",
      "epoch:19 step:18169 [D loss: 0.633734, acc: 64.06%] [G loss: 2.067666]\n",
      "epoch:19 step:18170 [D loss: 0.681766, acc: 56.25%] [G loss: 2.033617]\n",
      "epoch:19 step:18171 [D loss: 0.625689, acc: 65.62%] [G loss: 1.979733]\n",
      "epoch:19 step:18172 [D loss: 0.694600, acc: 60.94%] [G loss: 1.912207]\n",
      "epoch:19 step:18173 [D loss: 0.627877, acc: 66.41%] [G loss: 1.924530]\n",
      "epoch:19 step:18174 [D loss: 0.612968, acc: 65.62%] [G loss: 2.092613]\n",
      "epoch:19 step:18175 [D loss: 0.661819, acc: 58.59%] [G loss: 1.998563]\n",
      "epoch:19 step:18176 [D loss: 0.683328, acc: 60.94%] [G loss: 1.808322]\n",
      "epoch:19 step:18177 [D loss: 0.590923, acc: 64.06%] [G loss: 2.099404]\n",
      "epoch:19 step:18178 [D loss: 0.693284, acc: 53.91%] [G loss: 1.797795]\n",
      "epoch:19 step:18179 [D loss: 0.760477, acc: 51.56%] [G loss: 1.896470]\n",
      "epoch:19 step:18180 [D loss: 0.690286, acc: 60.16%] [G loss: 1.852692]\n",
      "epoch:19 step:18181 [D loss: 0.693354, acc: 50.78%] [G loss: 1.781395]\n",
      "epoch:19 step:18182 [D loss: 0.646283, acc: 63.28%] [G loss: 1.957771]\n",
      "epoch:19 step:18183 [D loss: 0.622140, acc: 68.75%] [G loss: 1.876405]\n",
      "epoch:19 step:18184 [D loss: 0.591979, acc: 64.84%] [G loss: 2.081563]\n",
      "epoch:19 step:18185 [D loss: 0.604204, acc: 65.62%] [G loss: 1.968082]\n",
      "epoch:19 step:18186 [D loss: 0.654075, acc: 60.94%] [G loss: 2.005150]\n",
      "epoch:19 step:18187 [D loss: 0.682334, acc: 55.47%] [G loss: 2.055073]\n",
      "epoch:19 step:18188 [D loss: 0.616330, acc: 71.09%] [G loss: 1.953599]\n",
      "epoch:19 step:18189 [D loss: 0.662158, acc: 60.16%] [G loss: 1.840038]\n",
      "epoch:19 step:18190 [D loss: 0.648424, acc: 65.62%] [G loss: 1.820579]\n",
      "epoch:19 step:18191 [D loss: 0.640019, acc: 65.62%] [G loss: 1.844816]\n",
      "epoch:19 step:18192 [D loss: 0.651318, acc: 60.94%] [G loss: 1.766955]\n",
      "epoch:19 step:18193 [D loss: 0.648869, acc: 66.41%] [G loss: 1.919589]\n",
      "epoch:19 step:18194 [D loss: 0.707492, acc: 53.12%] [G loss: 1.818340]\n",
      "epoch:19 step:18195 [D loss: 0.657485, acc: 59.38%] [G loss: 1.837629]\n",
      "epoch:19 step:18196 [D loss: 0.683010, acc: 58.59%] [G loss: 1.872591]\n",
      "epoch:19 step:18197 [D loss: 0.702889, acc: 61.72%] [G loss: 1.900851]\n",
      "epoch:19 step:18198 [D loss: 0.691398, acc: 57.03%] [G loss: 1.966269]\n",
      "epoch:19 step:18199 [D loss: 0.702806, acc: 58.59%] [G loss: 1.683535]\n",
      "epoch:19 step:18200 [D loss: 0.719694, acc: 53.12%] [G loss: 1.717496]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.344427\n",
      "FID: 14.540794\n",
      "0 = 12.969117356586443\n",
      "1 = 0.10206362994772168\n",
      "2 = 0.8925999999046326\n",
      "3 = 0.9075999855995178\n",
      "4 = 0.8776000142097473\n",
      "5 = 0.8811650276184082\n",
      "6 = 0.9075999855995178\n",
      "7 = 6.449442265021797\n",
      "8 = 0.07664460431422031\n",
      "9 = 0.7221999764442444\n",
      "10 = 0.7296000123023987\n",
      "11 = 0.7148000001907349\n",
      "12 = 0.718959391117096\n",
      "13 = 0.7296000123023987\n",
      "14 = 7.34445333480835\n",
      "15 = 9.41883373260498\n",
      "16 = 0.11824715882539749\n",
      "17 = 7.34442663192749\n",
      "18 = 14.540794372558594\n",
      "epoch:19 step:18201 [D loss: 0.649116, acc: 66.41%] [G loss: 1.788419]\n",
      "epoch:19 step:18202 [D loss: 0.648649, acc: 59.38%] [G loss: 1.872874]\n",
      "epoch:19 step:18203 [D loss: 0.648797, acc: 60.94%] [G loss: 1.887584]\n",
      "epoch:19 step:18204 [D loss: 0.646951, acc: 63.28%] [G loss: 1.827509]\n",
      "epoch:19 step:18205 [D loss: 0.607762, acc: 64.84%] [G loss: 1.862705]\n",
      "epoch:19 step:18206 [D loss: 0.633859, acc: 64.06%] [G loss: 1.820557]\n",
      "epoch:19 step:18207 [D loss: 0.656703, acc: 60.94%] [G loss: 1.807774]\n",
      "epoch:19 step:18208 [D loss: 0.615901, acc: 64.06%] [G loss: 1.887143]\n",
      "epoch:19 step:18209 [D loss: 0.662043, acc: 57.03%] [G loss: 2.008181]\n",
      "epoch:19 step:18210 [D loss: 0.655083, acc: 64.06%] [G loss: 1.797661]\n",
      "epoch:19 step:18211 [D loss: 0.636873, acc: 62.50%] [G loss: 1.850177]\n",
      "epoch:19 step:18212 [D loss: 0.640621, acc: 62.50%] [G loss: 1.907971]\n",
      "epoch:19 step:18213 [D loss: 0.639673, acc: 67.19%] [G loss: 1.977039]\n",
      "epoch:19 step:18214 [D loss: 0.697409, acc: 56.25%] [G loss: 1.751362]\n",
      "epoch:19 step:18215 [D loss: 0.668012, acc: 59.38%] [G loss: 1.868340]\n",
      "epoch:19 step:18216 [D loss: 0.669700, acc: 53.91%] [G loss: 1.792360]\n",
      "epoch:19 step:18217 [D loss: 0.656823, acc: 60.94%] [G loss: 1.914471]\n",
      "epoch:19 step:18218 [D loss: 0.656982, acc: 64.06%] [G loss: 1.992346]\n",
      "epoch:19 step:18219 [D loss: 0.595758, acc: 71.09%] [G loss: 2.016941]\n",
      "epoch:19 step:18220 [D loss: 0.628095, acc: 65.62%] [G loss: 1.991287]\n",
      "epoch:19 step:18221 [D loss: 0.669278, acc: 61.72%] [G loss: 1.868038]\n",
      "epoch:19 step:18222 [D loss: 0.626657, acc: 65.62%] [G loss: 1.913130]\n",
      "epoch:19 step:18223 [D loss: 0.648879, acc: 59.38%] [G loss: 1.886080]\n",
      "epoch:19 step:18224 [D loss: 0.683062, acc: 62.50%] [G loss: 1.797192]\n",
      "epoch:19 step:18225 [D loss: 0.718096, acc: 57.81%] [G loss: 1.791702]\n",
      "epoch:19 step:18226 [D loss: 0.674139, acc: 64.84%] [G loss: 1.800408]\n",
      "epoch:19 step:18227 [D loss: 0.642240, acc: 66.41%] [G loss: 1.800832]\n",
      "epoch:19 step:18228 [D loss: 0.639346, acc: 64.06%] [G loss: 1.842355]\n",
      "epoch:19 step:18229 [D loss: 0.675460, acc: 58.59%] [G loss: 1.740571]\n",
      "epoch:19 step:18230 [D loss: 0.614840, acc: 65.62%] [G loss: 1.935036]\n",
      "epoch:19 step:18231 [D loss: 0.605139, acc: 67.97%] [G loss: 2.174271]\n",
      "epoch:19 step:18232 [D loss: 0.600296, acc: 69.53%] [G loss: 2.059481]\n",
      "epoch:19 step:18233 [D loss: 0.576388, acc: 67.97%] [G loss: 2.117897]\n",
      "epoch:19 step:18234 [D loss: 0.609064, acc: 67.97%] [G loss: 1.969433]\n",
      "epoch:19 step:18235 [D loss: 0.654972, acc: 60.94%] [G loss: 1.786813]\n",
      "epoch:19 step:18236 [D loss: 0.696694, acc: 61.72%] [G loss: 1.805041]\n",
      "epoch:19 step:18237 [D loss: 0.610048, acc: 69.53%] [G loss: 1.952336]\n",
      "epoch:19 step:18238 [D loss: 0.657518, acc: 61.72%] [G loss: 1.922685]\n",
      "epoch:19 step:18239 [D loss: 0.623872, acc: 64.06%] [G loss: 1.922959]\n",
      "epoch:19 step:18240 [D loss: 0.696775, acc: 57.81%] [G loss: 1.669891]\n",
      "epoch:19 step:18241 [D loss: 0.656395, acc: 61.72%] [G loss: 1.824862]\n",
      "epoch:19 step:18242 [D loss: 0.670821, acc: 61.72%] [G loss: 1.735652]\n",
      "epoch:19 step:18243 [D loss: 0.703366, acc: 50.00%] [G loss: 1.783051]\n",
      "epoch:19 step:18244 [D loss: 0.656303, acc: 63.28%] [G loss: 1.707660]\n",
      "epoch:19 step:18245 [D loss: 0.633180, acc: 62.50%] [G loss: 1.879776]\n",
      "epoch:19 step:18246 [D loss: 0.634378, acc: 67.19%] [G loss: 1.775898]\n",
      "epoch:19 step:18247 [D loss: 0.650887, acc: 58.59%] [G loss: 1.852316]\n",
      "epoch:19 step:18248 [D loss: 0.660069, acc: 59.38%] [G loss: 1.758299]\n",
      "epoch:19 step:18249 [D loss: 0.637617, acc: 65.62%] [G loss: 1.855478]\n",
      "epoch:19 step:18250 [D loss: 0.672337, acc: 57.81%] [G loss: 1.686937]\n",
      "epoch:19 step:18251 [D loss: 0.659440, acc: 57.81%] [G loss: 1.794296]\n",
      "epoch:19 step:18252 [D loss: 0.667469, acc: 59.38%] [G loss: 1.700557]\n",
      "epoch:19 step:18253 [D loss: 0.651037, acc: 62.50%] [G loss: 1.731653]\n",
      "epoch:19 step:18254 [D loss: 0.657838, acc: 60.94%] [G loss: 1.828974]\n",
      "epoch:19 step:18255 [D loss: 0.672473, acc: 57.03%] [G loss: 1.805274]\n",
      "epoch:19 step:18256 [D loss: 0.613667, acc: 65.62%] [G loss: 1.900744]\n",
      "epoch:19 step:18257 [D loss: 0.666608, acc: 53.12%] [G loss: 1.912229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18258 [D loss: 0.635816, acc: 60.94%] [G loss: 1.864550]\n",
      "epoch:19 step:18259 [D loss: 0.630589, acc: 60.16%] [G loss: 1.885299]\n",
      "epoch:19 step:18260 [D loss: 0.573384, acc: 72.66%] [G loss: 1.993591]\n",
      "epoch:19 step:18261 [D loss: 0.676311, acc: 52.34%] [G loss: 1.863276]\n",
      "epoch:19 step:18262 [D loss: 0.688349, acc: 54.69%] [G loss: 1.678903]\n",
      "epoch:19 step:18263 [D loss: 0.666215, acc: 59.38%] [G loss: 1.809829]\n",
      "epoch:19 step:18264 [D loss: 0.679630, acc: 53.12%] [G loss: 1.836452]\n",
      "epoch:19 step:18265 [D loss: 0.674005, acc: 61.72%] [G loss: 1.781181]\n",
      "epoch:19 step:18266 [D loss: 0.649805, acc: 64.84%] [G loss: 1.876014]\n",
      "epoch:19 step:18267 [D loss: 0.698617, acc: 56.25%] [G loss: 1.823635]\n",
      "epoch:19 step:18268 [D loss: 0.653220, acc: 60.16%] [G loss: 1.886648]\n",
      "epoch:19 step:18269 [D loss: 0.702514, acc: 48.44%] [G loss: 1.806182]\n",
      "epoch:19 step:18270 [D loss: 0.663934, acc: 60.94%] [G loss: 1.963326]\n",
      "epoch:19 step:18271 [D loss: 0.619844, acc: 61.72%] [G loss: 1.926172]\n",
      "epoch:19 step:18272 [D loss: 0.657295, acc: 63.28%] [G loss: 1.952666]\n",
      "epoch:19 step:18273 [D loss: 0.632434, acc: 64.84%] [G loss: 1.882584]\n",
      "epoch:19 step:18274 [D loss: 0.585466, acc: 68.75%] [G loss: 2.086806]\n",
      "epoch:19 step:18275 [D loss: 0.676893, acc: 62.50%] [G loss: 1.965514]\n",
      "epoch:19 step:18276 [D loss: 0.688878, acc: 58.59%] [G loss: 1.577938]\n",
      "epoch:19 step:18277 [D loss: 0.647025, acc: 64.06%] [G loss: 1.893569]\n",
      "epoch:19 step:18278 [D loss: 0.643322, acc: 64.84%] [G loss: 1.914175]\n",
      "epoch:19 step:18279 [D loss: 0.634714, acc: 63.28%] [G loss: 1.835240]\n",
      "epoch:19 step:18280 [D loss: 0.654257, acc: 61.72%] [G loss: 1.800160]\n",
      "epoch:19 step:18281 [D loss: 0.686624, acc: 59.38%] [G loss: 1.812723]\n",
      "epoch:19 step:18282 [D loss: 0.647384, acc: 64.06%] [G loss: 1.899363]\n",
      "epoch:19 step:18283 [D loss: 0.568690, acc: 72.66%] [G loss: 1.981325]\n",
      "epoch:19 step:18284 [D loss: 0.619693, acc: 67.97%] [G loss: 1.868150]\n",
      "epoch:19 step:18285 [D loss: 0.658976, acc: 59.38%] [G loss: 1.753715]\n",
      "epoch:19 step:18286 [D loss: 0.703152, acc: 50.00%] [G loss: 1.934815]\n",
      "epoch:19 step:18287 [D loss: 0.678440, acc: 55.47%] [G loss: 1.902508]\n",
      "epoch:19 step:18288 [D loss: 0.664430, acc: 56.25%] [G loss: 1.756058]\n",
      "epoch:19 step:18289 [D loss: 0.635733, acc: 59.38%] [G loss: 1.852515]\n",
      "epoch:19 step:18290 [D loss: 0.684674, acc: 54.69%] [G loss: 1.715339]\n",
      "epoch:19 step:18291 [D loss: 0.617542, acc: 68.75%] [G loss: 1.988353]\n",
      "epoch:19 step:18292 [D loss: 0.668131, acc: 56.25%] [G loss: 1.807781]\n",
      "epoch:19 step:18293 [D loss: 0.663767, acc: 62.50%] [G loss: 1.843445]\n",
      "epoch:19 step:18294 [D loss: 0.586065, acc: 75.00%] [G loss: 1.854788]\n",
      "epoch:19 step:18295 [D loss: 0.631415, acc: 57.81%] [G loss: 1.856555]\n",
      "epoch:19 step:18296 [D loss: 0.620566, acc: 67.19%] [G loss: 1.811031]\n",
      "epoch:19 step:18297 [D loss: 0.644262, acc: 66.41%] [G loss: 1.934868]\n",
      "epoch:19 step:18298 [D loss: 0.600416, acc: 67.19%] [G loss: 2.090628]\n",
      "epoch:19 step:18299 [D loss: 0.601978, acc: 70.31%] [G loss: 1.979900]\n",
      "epoch:19 step:18300 [D loss: 0.619479, acc: 65.62%] [G loss: 1.998112]\n",
      "epoch:19 step:18301 [D loss: 0.649177, acc: 60.16%] [G loss: 1.909008]\n",
      "epoch:19 step:18302 [D loss: 0.562988, acc: 70.31%] [G loss: 2.103558]\n",
      "epoch:19 step:18303 [D loss: 0.726618, acc: 55.47%] [G loss: 1.795692]\n",
      "epoch:19 step:18304 [D loss: 0.711376, acc: 56.25%] [G loss: 1.814522]\n",
      "epoch:19 step:18305 [D loss: 0.721532, acc: 56.25%] [G loss: 1.754607]\n",
      "epoch:19 step:18306 [D loss: 0.688387, acc: 55.47%] [G loss: 1.794343]\n",
      "epoch:19 step:18307 [D loss: 0.654425, acc: 61.72%] [G loss: 1.910633]\n",
      "epoch:19 step:18308 [D loss: 0.605938, acc: 67.19%] [G loss: 1.892967]\n",
      "epoch:19 step:18309 [D loss: 0.680886, acc: 54.69%] [G loss: 1.643511]\n",
      "epoch:19 step:18310 [D loss: 0.637218, acc: 60.16%] [G loss: 1.719818]\n",
      "epoch:19 step:18311 [D loss: 0.567188, acc: 78.12%] [G loss: 1.960662]\n",
      "epoch:19 step:18312 [D loss: 0.664140, acc: 60.16%] [G loss: 1.784104]\n",
      "epoch:19 step:18313 [D loss: 0.657207, acc: 61.72%] [G loss: 1.817092]\n",
      "epoch:19 step:18314 [D loss: 0.665879, acc: 64.84%] [G loss: 1.920012]\n",
      "epoch:19 step:18315 [D loss: 0.645485, acc: 60.94%] [G loss: 1.886786]\n",
      "epoch:19 step:18316 [D loss: 0.602131, acc: 69.53%] [G loss: 1.898131]\n",
      "epoch:19 step:18317 [D loss: 0.635163, acc: 61.72%] [G loss: 2.009128]\n",
      "epoch:19 step:18318 [D loss: 0.620744, acc: 71.09%] [G loss: 1.870776]\n",
      "epoch:19 step:18319 [D loss: 0.630710, acc: 66.41%] [G loss: 1.890056]\n",
      "epoch:19 step:18320 [D loss: 0.679657, acc: 57.81%] [G loss: 1.890695]\n",
      "epoch:19 step:18321 [D loss: 0.636707, acc: 64.84%] [G loss: 1.844988]\n",
      "epoch:19 step:18322 [D loss: 0.694237, acc: 56.25%] [G loss: 1.847275]\n",
      "epoch:19 step:18323 [D loss: 0.602215, acc: 63.28%] [G loss: 1.854489]\n",
      "epoch:19 step:18324 [D loss: 0.699305, acc: 55.47%] [G loss: 1.818720]\n",
      "epoch:19 step:18325 [D loss: 0.652525, acc: 60.16%] [G loss: 1.890631]\n",
      "epoch:19 step:18326 [D loss: 0.622563, acc: 66.41%] [G loss: 2.018536]\n",
      "epoch:19 step:18327 [D loss: 0.625257, acc: 66.41%] [G loss: 1.856293]\n",
      "epoch:19 step:18328 [D loss: 0.641160, acc: 63.28%] [G loss: 1.883431]\n",
      "epoch:19 step:18329 [D loss: 0.640164, acc: 64.06%] [G loss: 1.860736]\n",
      "epoch:19 step:18330 [D loss: 0.617740, acc: 65.62%] [G loss: 1.845387]\n",
      "epoch:19 step:18331 [D loss: 0.682208, acc: 54.69%] [G loss: 1.730547]\n",
      "epoch:19 step:18332 [D loss: 0.661130, acc: 60.16%] [G loss: 1.713666]\n",
      "epoch:19 step:18333 [D loss: 0.659834, acc: 58.59%] [G loss: 1.775809]\n",
      "epoch:19 step:18334 [D loss: 0.667199, acc: 60.94%] [G loss: 1.851723]\n",
      "epoch:19 step:18335 [D loss: 0.635152, acc: 67.19%] [G loss: 1.863337]\n",
      "epoch:19 step:18336 [D loss: 0.626683, acc: 66.41%] [G loss: 1.991671]\n",
      "epoch:19 step:18337 [D loss: 0.613095, acc: 67.19%] [G loss: 1.952488]\n",
      "epoch:19 step:18338 [D loss: 0.655538, acc: 59.38%] [G loss: 1.894527]\n",
      "epoch:19 step:18339 [D loss: 0.608800, acc: 64.84%] [G loss: 2.012234]\n",
      "epoch:19 step:18340 [D loss: 0.654558, acc: 60.94%] [G loss: 1.815296]\n",
      "epoch:19 step:18341 [D loss: 0.690045, acc: 59.38%] [G loss: 1.712956]\n",
      "epoch:19 step:18342 [D loss: 0.651379, acc: 59.38%] [G loss: 1.994328]\n",
      "epoch:19 step:18343 [D loss: 0.606914, acc: 67.97%] [G loss: 1.895935]\n",
      "epoch:19 step:18344 [D loss: 0.644041, acc: 60.94%] [G loss: 1.760428]\n",
      "epoch:19 step:18345 [D loss: 0.644836, acc: 61.72%] [G loss: 1.855397]\n",
      "epoch:19 step:18346 [D loss: 0.616621, acc: 63.28%] [G loss: 1.998839]\n",
      "epoch:19 step:18347 [D loss: 0.633322, acc: 63.28%] [G loss: 1.760444]\n",
      "epoch:19 step:18348 [D loss: 0.634725, acc: 64.06%] [G loss: 1.818364]\n",
      "epoch:19 step:18349 [D loss: 0.622940, acc: 69.53%] [G loss: 1.807196]\n",
      "epoch:19 step:18350 [D loss: 0.631794, acc: 66.41%] [G loss: 1.900116]\n",
      "epoch:19 step:18351 [D loss: 0.658214, acc: 60.16%] [G loss: 1.951416]\n",
      "epoch:19 step:18352 [D loss: 0.667075, acc: 62.50%] [G loss: 1.859034]\n",
      "epoch:19 step:18353 [D loss: 0.600652, acc: 72.66%] [G loss: 2.020071]\n",
      "epoch:19 step:18354 [D loss: 0.613700, acc: 68.75%] [G loss: 2.082684]\n",
      "epoch:19 step:18355 [D loss: 0.611813, acc: 65.62%] [G loss: 1.991159]\n",
      "epoch:19 step:18356 [D loss: 0.675102, acc: 59.38%] [G loss: 1.788570]\n",
      "epoch:19 step:18357 [D loss: 0.586129, acc: 71.88%] [G loss: 2.066323]\n",
      "epoch:19 step:18358 [D loss: 0.644733, acc: 57.03%] [G loss: 2.014446]\n",
      "epoch:19 step:18359 [D loss: 0.583156, acc: 72.66%] [G loss: 1.896779]\n",
      "epoch:19 step:18360 [D loss: 0.634348, acc: 62.50%] [G loss: 1.973504]\n",
      "epoch:19 step:18361 [D loss: 0.661152, acc: 58.59%] [G loss: 1.882668]\n",
      "epoch:19 step:18362 [D loss: 0.701172, acc: 56.25%] [G loss: 1.722729]\n",
      "epoch:19 step:18363 [D loss: 0.620694, acc: 67.19%] [G loss: 1.880778]\n",
      "epoch:19 step:18364 [D loss: 0.616064, acc: 69.53%] [G loss: 1.900721]\n",
      "epoch:19 step:18365 [D loss: 0.644073, acc: 64.06%] [G loss: 1.966054]\n",
      "epoch:19 step:18366 [D loss: 0.688776, acc: 66.41%] [G loss: 1.822839]\n",
      "epoch:19 step:18367 [D loss: 0.622029, acc: 60.94%] [G loss: 1.991737]\n",
      "epoch:19 step:18368 [D loss: 0.624604, acc: 68.75%] [G loss: 1.865930]\n",
      "epoch:19 step:18369 [D loss: 0.682642, acc: 55.47%] [G loss: 1.744522]\n",
      "epoch:19 step:18370 [D loss: 0.655732, acc: 57.81%] [G loss: 1.741339]\n",
      "epoch:19 step:18371 [D loss: 0.648831, acc: 60.94%] [G loss: 1.803587]\n",
      "epoch:19 step:18372 [D loss: 0.713966, acc: 57.03%] [G loss: 1.895728]\n",
      "epoch:19 step:18373 [D loss: 0.634182, acc: 64.06%] [G loss: 1.821519]\n",
      "epoch:19 step:18374 [D loss: 0.648699, acc: 61.72%] [G loss: 1.938189]\n",
      "epoch:19 step:18375 [D loss: 0.657472, acc: 63.28%] [G loss: 1.751274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18376 [D loss: 0.647089, acc: 60.94%] [G loss: 1.947979]\n",
      "epoch:19 step:18377 [D loss: 0.620058, acc: 64.06%] [G loss: 1.815800]\n",
      "epoch:19 step:18378 [D loss: 0.641769, acc: 57.81%] [G loss: 1.879413]\n",
      "epoch:19 step:18379 [D loss: 0.682668, acc: 56.25%] [G loss: 1.822621]\n",
      "epoch:19 step:18380 [D loss: 0.626878, acc: 69.53%] [G loss: 1.752693]\n",
      "epoch:19 step:18381 [D loss: 0.683137, acc: 60.16%] [G loss: 1.919982]\n",
      "epoch:19 step:18382 [D loss: 0.635991, acc: 64.06%] [G loss: 1.786002]\n",
      "epoch:19 step:18383 [D loss: 0.656115, acc: 61.72%] [G loss: 1.816677]\n",
      "epoch:19 step:18384 [D loss: 0.691772, acc: 57.03%] [G loss: 1.846521]\n",
      "epoch:19 step:18385 [D loss: 0.627194, acc: 64.06%] [G loss: 1.812387]\n",
      "epoch:19 step:18386 [D loss: 0.635670, acc: 62.50%] [G loss: 1.967666]\n",
      "epoch:19 step:18387 [D loss: 0.617049, acc: 64.84%] [G loss: 1.888227]\n",
      "epoch:19 step:18388 [D loss: 0.659389, acc: 60.16%] [G loss: 1.821363]\n",
      "epoch:19 step:18389 [D loss: 0.654091, acc: 62.50%] [G loss: 1.765974]\n",
      "epoch:19 step:18390 [D loss: 0.678051, acc: 59.38%] [G loss: 1.889205]\n",
      "epoch:19 step:18391 [D loss: 0.607628, acc: 69.53%] [G loss: 1.954826]\n",
      "epoch:19 step:18392 [D loss: 0.646260, acc: 66.41%] [G loss: 1.884408]\n",
      "epoch:19 step:18393 [D loss: 0.695256, acc: 58.59%] [G loss: 1.763965]\n",
      "epoch:19 step:18394 [D loss: 0.604616, acc: 68.75%] [G loss: 1.998359]\n",
      "epoch:19 step:18395 [D loss: 0.685924, acc: 58.59%] [G loss: 1.906408]\n",
      "epoch:19 step:18396 [D loss: 0.657271, acc: 58.59%] [G loss: 1.799481]\n",
      "epoch:19 step:18397 [D loss: 0.664933, acc: 60.94%] [G loss: 1.742872]\n",
      "epoch:19 step:18398 [D loss: 0.645116, acc: 63.28%] [G loss: 1.724623]\n",
      "epoch:19 step:18399 [D loss: 0.646940, acc: 64.06%] [G loss: 1.773500]\n",
      "epoch:19 step:18400 [D loss: 0.674769, acc: 60.16%] [G loss: 1.813686]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.506096\n",
      "FID: 10.958168\n",
      "0 = 12.707884351634968\n",
      "1 = 0.08480223818650325\n",
      "2 = 0.8852999806404114\n",
      "3 = 0.9093999862670898\n",
      "4 = 0.8611999750137329\n",
      "5 = 0.8675824999809265\n",
      "6 = 0.9093999862670898\n",
      "7 = 6.190779075598695\n",
      "8 = 0.06221898800906416\n",
      "9 = 0.715399980545044\n",
      "10 = 0.729200005531311\n",
      "11 = 0.7016000151634216\n",
      "12 = 0.709614634513855\n",
      "13 = 0.729200005531311\n",
      "14 = 7.506125450134277\n",
      "15 = 9.511889457702637\n",
      "16 = 0.09238330274820328\n",
      "17 = 7.506095886230469\n",
      "18 = 10.958168029785156\n",
      "epoch:19 step:18401 [D loss: 0.652374, acc: 61.72%] [G loss: 1.841310]\n",
      "epoch:19 step:18402 [D loss: 0.646285, acc: 62.50%] [G loss: 1.872530]\n",
      "epoch:19 step:18403 [D loss: 0.703261, acc: 52.34%] [G loss: 1.792335]\n",
      "epoch:19 step:18404 [D loss: 0.651070, acc: 63.28%] [G loss: 1.801369]\n",
      "epoch:19 step:18405 [D loss: 0.638063, acc: 66.41%] [G loss: 1.969060]\n",
      "epoch:19 step:18406 [D loss: 0.603879, acc: 64.06%] [G loss: 1.960968]\n",
      "epoch:19 step:18407 [D loss: 0.647394, acc: 61.72%] [G loss: 1.928939]\n",
      "epoch:19 step:18408 [D loss: 0.614887, acc: 61.72%] [G loss: 1.962909]\n",
      "epoch:19 step:18409 [D loss: 0.665044, acc: 57.03%] [G loss: 1.934725]\n",
      "epoch:19 step:18410 [D loss: 0.637639, acc: 61.72%] [G loss: 1.827895]\n",
      "epoch:19 step:18411 [D loss: 0.644996, acc: 64.84%] [G loss: 2.001911]\n",
      "epoch:19 step:18412 [D loss: 0.639151, acc: 66.41%] [G loss: 1.921114]\n",
      "epoch:19 step:18413 [D loss: 0.647165, acc: 64.84%] [G loss: 1.898225]\n",
      "epoch:19 step:18414 [D loss: 0.670407, acc: 57.03%] [G loss: 1.776492]\n",
      "epoch:19 step:18415 [D loss: 0.705010, acc: 50.78%] [G loss: 1.837635]\n",
      "epoch:19 step:18416 [D loss: 0.623034, acc: 65.62%] [G loss: 2.038404]\n",
      "epoch:19 step:18417 [D loss: 0.723867, acc: 53.91%] [G loss: 1.757378]\n",
      "epoch:19 step:18418 [D loss: 0.713070, acc: 53.91%] [G loss: 1.752182]\n",
      "epoch:19 step:18419 [D loss: 0.634462, acc: 65.62%] [G loss: 1.871929]\n",
      "epoch:19 step:18420 [D loss: 0.661307, acc: 60.16%] [G loss: 1.960061]\n",
      "epoch:19 step:18421 [D loss: 0.611754, acc: 64.06%] [G loss: 1.902927]\n",
      "epoch:19 step:18422 [D loss: 0.649676, acc: 60.94%] [G loss: 1.870381]\n",
      "epoch:19 step:18423 [D loss: 0.597844, acc: 70.31%] [G loss: 1.928869]\n",
      "epoch:19 step:18424 [D loss: 0.638196, acc: 62.50%] [G loss: 1.797659]\n",
      "epoch:19 step:18425 [D loss: 0.646912, acc: 62.50%] [G loss: 1.986722]\n",
      "epoch:19 step:18426 [D loss: 0.647716, acc: 66.41%] [G loss: 1.964706]\n",
      "epoch:19 step:18427 [D loss: 0.591400, acc: 72.66%] [G loss: 1.956361]\n",
      "epoch:19 step:18428 [D loss: 0.713142, acc: 52.34%] [G loss: 1.789571]\n",
      "epoch:19 step:18429 [D loss: 0.679361, acc: 59.38%] [G loss: 1.751271]\n",
      "epoch:19 step:18430 [D loss: 0.660845, acc: 60.16%] [G loss: 1.776420]\n",
      "epoch:19 step:18431 [D loss: 0.685321, acc: 52.34%] [G loss: 1.841700]\n",
      "epoch:19 step:18432 [D loss: 0.660573, acc: 63.28%] [G loss: 1.881197]\n",
      "epoch:19 step:18433 [D loss: 0.651317, acc: 60.16%] [G loss: 1.847048]\n",
      "epoch:19 step:18434 [D loss: 0.613783, acc: 69.53%] [G loss: 1.953150]\n",
      "epoch:19 step:18435 [D loss: 0.693851, acc: 61.72%] [G loss: 1.947922]\n",
      "epoch:19 step:18436 [D loss: 0.636903, acc: 64.06%] [G loss: 1.926342]\n",
      "epoch:19 step:18437 [D loss: 0.603550, acc: 72.66%] [G loss: 1.921327]\n",
      "epoch:19 step:18438 [D loss: 0.602309, acc: 63.28%] [G loss: 1.937843]\n",
      "epoch:19 step:18439 [D loss: 0.593044, acc: 67.19%] [G loss: 1.956308]\n",
      "epoch:19 step:18440 [D loss: 0.616819, acc: 71.09%] [G loss: 2.027169]\n",
      "epoch:19 step:18441 [D loss: 0.658968, acc: 57.03%] [G loss: 2.003580]\n",
      "epoch:19 step:18442 [D loss: 0.643904, acc: 61.72%] [G loss: 1.919935]\n",
      "epoch:19 step:18443 [D loss: 0.674330, acc: 58.59%] [G loss: 1.899935]\n",
      "epoch:19 step:18444 [D loss: 0.590044, acc: 71.88%] [G loss: 2.172860]\n",
      "epoch:19 step:18445 [D loss: 0.617107, acc: 65.62%] [G loss: 2.002120]\n",
      "epoch:19 step:18446 [D loss: 0.634100, acc: 64.06%] [G loss: 2.161200]\n",
      "epoch:19 step:18447 [D loss: 0.633807, acc: 64.84%] [G loss: 2.200883]\n",
      "epoch:19 step:18448 [D loss: 0.679314, acc: 57.81%] [G loss: 2.144080]\n",
      "epoch:19 step:18449 [D loss: 0.632189, acc: 67.97%] [G loss: 1.884937]\n",
      "epoch:19 step:18450 [D loss: 0.639524, acc: 67.97%] [G loss: 2.105998]\n",
      "epoch:19 step:18451 [D loss: 0.605055, acc: 67.19%] [G loss: 2.446275]\n",
      "epoch:19 step:18452 [D loss: 0.598138, acc: 71.09%] [G loss: 2.181088]\n",
      "epoch:19 step:18453 [D loss: 0.614701, acc: 62.50%] [G loss: 2.144238]\n",
      "epoch:19 step:18454 [D loss: 0.644232, acc: 57.81%] [G loss: 2.043274]\n",
      "epoch:19 step:18455 [D loss: 0.665600, acc: 62.50%] [G loss: 1.920484]\n",
      "epoch:19 step:18456 [D loss: 0.595021, acc: 70.31%] [G loss: 1.989554]\n",
      "epoch:19 step:18457 [D loss: 0.653997, acc: 57.81%] [G loss: 2.168469]\n",
      "epoch:19 step:18458 [D loss: 0.691278, acc: 57.03%] [G loss: 1.955040]\n",
      "epoch:19 step:18459 [D loss: 0.644465, acc: 60.16%] [G loss: 1.916513]\n",
      "epoch:19 step:18460 [D loss: 0.731622, acc: 52.34%] [G loss: 1.627664]\n",
      "epoch:19 step:18461 [D loss: 0.706387, acc: 55.47%] [G loss: 1.691154]\n",
      "epoch:19 step:18462 [D loss: 0.621579, acc: 60.16%] [G loss: 1.793728]\n",
      "epoch:19 step:18463 [D loss: 0.670695, acc: 55.47%] [G loss: 1.893508]\n",
      "epoch:19 step:18464 [D loss: 0.591945, acc: 69.53%] [G loss: 1.938908]\n",
      "epoch:19 step:18465 [D loss: 0.660868, acc: 60.94%] [G loss: 1.940817]\n",
      "epoch:19 step:18466 [D loss: 0.643880, acc: 64.84%] [G loss: 1.835941]\n",
      "epoch:19 step:18467 [D loss: 0.660156, acc: 60.16%] [G loss: 1.758301]\n",
      "epoch:19 step:18468 [D loss: 0.596181, acc: 67.19%] [G loss: 2.038667]\n",
      "epoch:19 step:18469 [D loss: 0.684430, acc: 56.25%] [G loss: 1.712767]\n",
      "epoch:19 step:18470 [D loss: 0.663196, acc: 60.94%] [G loss: 1.927464]\n",
      "epoch:19 step:18471 [D loss: 0.658066, acc: 59.38%] [G loss: 1.838262]\n",
      "epoch:19 step:18472 [D loss: 0.636721, acc: 68.75%] [G loss: 1.878827]\n",
      "epoch:19 step:18473 [D loss: 0.657457, acc: 60.16%] [G loss: 1.802739]\n",
      "epoch:19 step:18474 [D loss: 0.644078, acc: 61.72%] [G loss: 1.748218]\n",
      "epoch:19 step:18475 [D loss: 0.655628, acc: 61.72%] [G loss: 1.870658]\n",
      "epoch:19 step:18476 [D loss: 0.629075, acc: 62.50%] [G loss: 1.764811]\n",
      "epoch:19 step:18477 [D loss: 0.645752, acc: 59.38%] [G loss: 1.790409]\n",
      "epoch:19 step:18478 [D loss: 0.604148, acc: 63.28%] [G loss: 1.837993]\n",
      "epoch:19 step:18479 [D loss: 0.692166, acc: 55.47%] [G loss: 1.871170]\n",
      "epoch:19 step:18480 [D loss: 0.612382, acc: 64.06%] [G loss: 1.809795]\n",
      "epoch:19 step:18481 [D loss: 0.644773, acc: 61.72%] [G loss: 1.845996]\n",
      "epoch:19 step:18482 [D loss: 0.625855, acc: 67.97%] [G loss: 2.029986]\n",
      "epoch:19 step:18483 [D loss: 0.616614, acc: 61.72%] [G loss: 1.908797]\n",
      "epoch:19 step:18484 [D loss: 0.627017, acc: 64.84%] [G loss: 2.007069]\n",
      "epoch:19 step:18485 [D loss: 0.681737, acc: 59.38%] [G loss: 1.799603]\n",
      "epoch:19 step:18486 [D loss: 0.633816, acc: 65.62%] [G loss: 1.819393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18487 [D loss: 0.640620, acc: 64.06%] [G loss: 1.775558]\n",
      "epoch:19 step:18488 [D loss: 0.661582, acc: 64.06%] [G loss: 1.964767]\n",
      "epoch:19 step:18489 [D loss: 0.632883, acc: 68.75%] [G loss: 1.943563]\n",
      "epoch:19 step:18490 [D loss: 0.655581, acc: 59.38%] [G loss: 1.880911]\n",
      "epoch:19 step:18491 [D loss: 0.691047, acc: 62.50%] [G loss: 1.995689]\n",
      "epoch:19 step:18492 [D loss: 0.602328, acc: 69.53%] [G loss: 1.978614]\n",
      "epoch:19 step:18493 [D loss: 0.606251, acc: 67.19%] [G loss: 1.826948]\n",
      "epoch:19 step:18494 [D loss: 0.659414, acc: 61.72%] [G loss: 2.010951]\n",
      "epoch:19 step:18495 [D loss: 0.588149, acc: 73.44%] [G loss: 1.909075]\n",
      "epoch:19 step:18496 [D loss: 0.590005, acc: 70.31%] [G loss: 1.846220]\n",
      "epoch:19 step:18497 [D loss: 0.580081, acc: 70.31%] [G loss: 2.110555]\n",
      "epoch:19 step:18498 [D loss: 0.639260, acc: 59.38%] [G loss: 2.075376]\n",
      "epoch:19 step:18499 [D loss: 0.608199, acc: 70.31%] [G loss: 1.984675]\n",
      "epoch:19 step:18500 [D loss: 0.657744, acc: 64.06%] [G loss: 1.768259]\n",
      "epoch:19 step:18501 [D loss: 0.664375, acc: 57.81%] [G loss: 1.957394]\n",
      "epoch:19 step:18502 [D loss: 0.648591, acc: 62.50%] [G loss: 1.997373]\n",
      "epoch:19 step:18503 [D loss: 0.698463, acc: 57.03%] [G loss: 1.937275]\n",
      "epoch:19 step:18504 [D loss: 0.585782, acc: 69.53%] [G loss: 2.075207]\n",
      "epoch:19 step:18505 [D loss: 0.694003, acc: 55.47%] [G loss: 1.986020]\n",
      "epoch:19 step:18506 [D loss: 0.695877, acc: 58.59%] [G loss: 1.696646]\n",
      "epoch:19 step:18507 [D loss: 0.692827, acc: 56.25%] [G loss: 1.786815]\n",
      "epoch:19 step:18508 [D loss: 0.676018, acc: 53.12%] [G loss: 1.925345]\n",
      "epoch:19 step:18509 [D loss: 0.655085, acc: 52.34%] [G loss: 1.912192]\n",
      "epoch:19 step:18510 [D loss: 0.646681, acc: 64.84%] [G loss: 1.953200]\n",
      "epoch:19 step:18511 [D loss: 0.604397, acc: 66.41%] [G loss: 1.921656]\n",
      "epoch:19 step:18512 [D loss: 0.657045, acc: 62.50%] [G loss: 1.942708]\n",
      "epoch:19 step:18513 [D loss: 0.644513, acc: 64.84%] [G loss: 1.951274]\n",
      "epoch:19 step:18514 [D loss: 0.627990, acc: 67.19%] [G loss: 2.078098]\n",
      "epoch:19 step:18515 [D loss: 0.662460, acc: 60.94%] [G loss: 1.824343]\n",
      "epoch:19 step:18516 [D loss: 0.678574, acc: 59.38%] [G loss: 1.837024]\n",
      "epoch:19 step:18517 [D loss: 0.603380, acc: 65.62%] [G loss: 1.957864]\n",
      "epoch:19 step:18518 [D loss: 0.636607, acc: 66.41%] [G loss: 1.859010]\n",
      "epoch:19 step:18519 [D loss: 0.676610, acc: 57.03%] [G loss: 1.712981]\n",
      "epoch:19 step:18520 [D loss: 0.669592, acc: 63.28%] [G loss: 1.893397]\n",
      "epoch:19 step:18521 [D loss: 0.626491, acc: 66.41%] [G loss: 1.815307]\n",
      "epoch:19 step:18522 [D loss: 0.596330, acc: 71.09%] [G loss: 2.064636]\n",
      "epoch:19 step:18523 [D loss: 0.686749, acc: 59.38%] [G loss: 1.900450]\n",
      "epoch:19 step:18524 [D loss: 0.642604, acc: 64.06%] [G loss: 2.023651]\n",
      "epoch:19 step:18525 [D loss: 0.653607, acc: 64.06%] [G loss: 1.848186]\n",
      "epoch:19 step:18526 [D loss: 0.635206, acc: 60.16%] [G loss: 1.946726]\n",
      "epoch:19 step:18527 [D loss: 0.613195, acc: 67.19%] [G loss: 1.896351]\n",
      "epoch:19 step:18528 [D loss: 0.672522, acc: 62.50%] [G loss: 1.923751]\n",
      "epoch:19 step:18529 [D loss: 0.683463, acc: 57.03%] [G loss: 1.924727]\n",
      "epoch:19 step:18530 [D loss: 0.694202, acc: 54.69%] [G loss: 1.859368]\n",
      "epoch:19 step:18531 [D loss: 0.661267, acc: 58.59%] [G loss: 1.847869]\n",
      "epoch:19 step:18532 [D loss: 0.663675, acc: 60.94%] [G loss: 1.771845]\n",
      "epoch:19 step:18533 [D loss: 0.600160, acc: 70.31%] [G loss: 1.888816]\n",
      "epoch:19 step:18534 [D loss: 0.607771, acc: 70.31%] [G loss: 1.793706]\n",
      "epoch:19 step:18535 [D loss: 0.680758, acc: 62.50%] [G loss: 1.874211]\n",
      "epoch:19 step:18536 [D loss: 0.629353, acc: 64.06%] [G loss: 1.916818]\n",
      "epoch:19 step:18537 [D loss: 0.700350, acc: 60.94%] [G loss: 1.893199]\n",
      "epoch:19 step:18538 [D loss: 0.658316, acc: 60.16%] [G loss: 1.855934]\n",
      "epoch:19 step:18539 [D loss: 0.646069, acc: 66.41%] [G loss: 1.779844]\n",
      "epoch:19 step:18540 [D loss: 0.683717, acc: 58.59%] [G loss: 1.716241]\n",
      "epoch:19 step:18541 [D loss: 0.641131, acc: 64.84%] [G loss: 1.817526]\n",
      "epoch:19 step:18542 [D loss: 0.676263, acc: 61.72%] [G loss: 1.897594]\n",
      "epoch:19 step:18543 [D loss: 0.623754, acc: 61.72%] [G loss: 1.864473]\n",
      "epoch:19 step:18544 [D loss: 0.665139, acc: 60.16%] [G loss: 1.841357]\n",
      "epoch:19 step:18545 [D loss: 0.660162, acc: 62.50%] [G loss: 1.866664]\n",
      "epoch:19 step:18546 [D loss: 0.624663, acc: 61.72%] [G loss: 1.723735]\n",
      "epoch:19 step:18547 [D loss: 0.639439, acc: 60.94%] [G loss: 1.838691]\n",
      "epoch:19 step:18548 [D loss: 0.602830, acc: 65.62%] [G loss: 1.808985]\n",
      "epoch:19 step:18549 [D loss: 0.607211, acc: 67.97%] [G loss: 1.949538]\n",
      "epoch:19 step:18550 [D loss: 0.620990, acc: 59.38%] [G loss: 2.022834]\n",
      "epoch:19 step:18551 [D loss: 0.670289, acc: 60.16%] [G loss: 1.827511]\n",
      "epoch:19 step:18552 [D loss: 0.635674, acc: 63.28%] [G loss: 1.810259]\n",
      "epoch:19 step:18553 [D loss: 0.659302, acc: 58.59%] [G loss: 1.808728]\n",
      "epoch:19 step:18554 [D loss: 0.722606, acc: 57.03%] [G loss: 1.765288]\n",
      "epoch:19 step:18555 [D loss: 0.630935, acc: 64.84%] [G loss: 1.855479]\n",
      "epoch:19 step:18556 [D loss: 0.651434, acc: 63.28%] [G loss: 1.894081]\n",
      "epoch:19 step:18557 [D loss: 0.639012, acc: 69.53%] [G loss: 1.967533]\n",
      "epoch:19 step:18558 [D loss: 0.596524, acc: 64.84%] [G loss: 1.906030]\n",
      "epoch:19 step:18559 [D loss: 0.635990, acc: 66.41%] [G loss: 1.785487]\n",
      "epoch:19 step:18560 [D loss: 0.675803, acc: 60.16%] [G loss: 1.997238]\n",
      "epoch:19 step:18561 [D loss: 0.651545, acc: 62.50%] [G loss: 1.825055]\n",
      "epoch:19 step:18562 [D loss: 0.692844, acc: 53.91%] [G loss: 1.801626]\n",
      "epoch:19 step:18563 [D loss: 0.648701, acc: 61.72%] [G loss: 1.774791]\n",
      "epoch:19 step:18564 [D loss: 0.661944, acc: 60.94%] [G loss: 1.841794]\n",
      "epoch:19 step:18565 [D loss: 0.652052, acc: 64.84%] [G loss: 1.639450]\n",
      "epoch:19 step:18566 [D loss: 0.661957, acc: 63.28%] [G loss: 1.901459]\n",
      "epoch:19 step:18567 [D loss: 0.681105, acc: 53.91%] [G loss: 1.880927]\n",
      "epoch:19 step:18568 [D loss: 0.742762, acc: 45.31%] [G loss: 1.806123]\n",
      "epoch:19 step:18569 [D loss: 0.668930, acc: 61.72%] [G loss: 1.820501]\n",
      "epoch:19 step:18570 [D loss: 0.605321, acc: 67.19%] [G loss: 2.004403]\n",
      "epoch:19 step:18571 [D loss: 0.683971, acc: 58.59%] [G loss: 1.839397]\n",
      "epoch:19 step:18572 [D loss: 0.655485, acc: 59.38%] [G loss: 1.832578]\n",
      "epoch:19 step:18573 [D loss: 0.655524, acc: 57.03%] [G loss: 1.903252]\n",
      "epoch:19 step:18574 [D loss: 0.662538, acc: 64.84%] [G loss: 1.819538]\n",
      "epoch:19 step:18575 [D loss: 0.628327, acc: 66.41%] [G loss: 1.890462]\n",
      "epoch:19 step:18576 [D loss: 0.624411, acc: 65.62%] [G loss: 1.964118]\n",
      "epoch:19 step:18577 [D loss: 0.630706, acc: 60.94%] [G loss: 2.022643]\n",
      "epoch:19 step:18578 [D loss: 0.611338, acc: 70.31%] [G loss: 2.022619]\n",
      "epoch:19 step:18579 [D loss: 0.624514, acc: 66.41%] [G loss: 2.049763]\n",
      "epoch:19 step:18580 [D loss: 0.654404, acc: 59.38%] [G loss: 1.880383]\n",
      "epoch:19 step:18581 [D loss: 0.637394, acc: 61.72%] [G loss: 1.949809]\n",
      "epoch:19 step:18582 [D loss: 0.625063, acc: 66.41%] [G loss: 1.879608]\n",
      "epoch:19 step:18583 [D loss: 0.651480, acc: 60.94%] [G loss: 1.914301]\n",
      "epoch:19 step:18584 [D loss: 0.606528, acc: 65.62%] [G loss: 2.026897]\n",
      "epoch:19 step:18585 [D loss: 0.624959, acc: 64.06%] [G loss: 1.985961]\n",
      "epoch:19 step:18586 [D loss: 0.694650, acc: 57.81%] [G loss: 1.911612]\n",
      "epoch:19 step:18587 [D loss: 0.674131, acc: 54.69%] [G loss: 1.664745]\n",
      "epoch:19 step:18588 [D loss: 0.660874, acc: 59.38%] [G loss: 1.894770]\n",
      "epoch:19 step:18589 [D loss: 0.634169, acc: 64.06%] [G loss: 1.964002]\n",
      "epoch:19 step:18590 [D loss: 0.710264, acc: 53.12%] [G loss: 1.801504]\n",
      "epoch:19 step:18591 [D loss: 0.703875, acc: 54.69%] [G loss: 1.814216]\n",
      "epoch:19 step:18592 [D loss: 0.651055, acc: 66.41%] [G loss: 1.854286]\n",
      "epoch:19 step:18593 [D loss: 0.631087, acc: 63.28%] [G loss: 1.854163]\n",
      "epoch:19 step:18594 [D loss: 0.643248, acc: 59.38%] [G loss: 1.834768]\n",
      "epoch:19 step:18595 [D loss: 0.638195, acc: 64.84%] [G loss: 1.897147]\n",
      "epoch:19 step:18596 [D loss: 0.671830, acc: 56.25%] [G loss: 1.867179]\n",
      "epoch:19 step:18597 [D loss: 0.677868, acc: 58.59%] [G loss: 1.738628]\n",
      "epoch:19 step:18598 [D loss: 0.695585, acc: 53.91%] [G loss: 1.872411]\n",
      "epoch:19 step:18599 [D loss: 0.667569, acc: 60.94%] [G loss: 1.788700]\n",
      "epoch:19 step:18600 [D loss: 0.692574, acc: 53.91%] [G loss: 1.897927]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.336831\n",
      "FID: 13.732426\n",
      "0 = 12.802338598823535\n",
      "1 = 0.0870935536886411\n",
      "2 = 0.8968999981880188\n",
      "3 = 0.9157999753952026\n",
      "4 = 0.878000020980835\n",
      "5 = 0.8824436068534851\n",
      "6 = 0.9157999753952026\n",
      "7 = 6.4573301487922805\n",
      "8 = 0.06900849729543898\n",
      "9 = 0.7138000130653381\n",
      "10 = 0.722599983215332\n",
      "11 = 0.7049999833106995\n",
      "12 = 0.7101022005081177\n",
      "13 = 0.722599983215332\n",
      "14 = 7.336860179901123\n",
      "15 = 9.445673942565918\n",
      "16 = 0.11424700915813446\n",
      "17 = 7.336831092834473\n",
      "18 = 13.732425689697266\n",
      "epoch:19 step:18601 [D loss: 0.599408, acc: 70.31%] [G loss: 1.854752]\n",
      "epoch:19 step:18602 [D loss: 0.691201, acc: 57.81%] [G loss: 1.810465]\n",
      "epoch:19 step:18603 [D loss: 0.707683, acc: 53.12%] [G loss: 1.794097]\n",
      "epoch:19 step:18604 [D loss: 0.695429, acc: 53.12%] [G loss: 1.867743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18605 [D loss: 0.686345, acc: 57.81%] [G loss: 1.782162]\n",
      "epoch:19 step:18606 [D loss: 0.674040, acc: 56.25%] [G loss: 1.662030]\n",
      "epoch:19 step:18607 [D loss: 0.641016, acc: 63.28%] [G loss: 1.975083]\n",
      "epoch:19 step:18608 [D loss: 0.637927, acc: 62.50%] [G loss: 1.822399]\n",
      "epoch:19 step:18609 [D loss: 0.638656, acc: 60.94%] [G loss: 1.881814]\n",
      "epoch:19 step:18610 [D loss: 0.617386, acc: 62.50%] [G loss: 2.045136]\n",
      "epoch:19 step:18611 [D loss: 0.705308, acc: 53.12%] [G loss: 1.877463]\n",
      "epoch:19 step:18612 [D loss: 0.659842, acc: 63.28%] [G loss: 1.832899]\n",
      "epoch:19 step:18613 [D loss: 0.641127, acc: 69.53%] [G loss: 1.795155]\n",
      "epoch:19 step:18614 [D loss: 0.665703, acc: 60.94%] [G loss: 1.896741]\n",
      "epoch:19 step:18615 [D loss: 0.668498, acc: 61.72%] [G loss: 1.704429]\n",
      "epoch:19 step:18616 [D loss: 0.641434, acc: 63.28%] [G loss: 1.788672]\n",
      "epoch:19 step:18617 [D loss: 0.599638, acc: 70.31%] [G loss: 1.822092]\n",
      "epoch:19 step:18618 [D loss: 0.601979, acc: 64.84%] [G loss: 2.248655]\n",
      "epoch:19 step:18619 [D loss: 0.659142, acc: 64.06%] [G loss: 2.028174]\n",
      "epoch:19 step:18620 [D loss: 0.707750, acc: 60.16%] [G loss: 1.863248]\n",
      "epoch:19 step:18621 [D loss: 0.662875, acc: 64.06%] [G loss: 1.909570]\n",
      "epoch:19 step:18622 [D loss: 0.622535, acc: 67.97%] [G loss: 1.982746]\n",
      "epoch:19 step:18623 [D loss: 0.697825, acc: 57.81%] [G loss: 1.709645]\n",
      "epoch:19 step:18624 [D loss: 0.678128, acc: 58.59%] [G loss: 1.857677]\n",
      "epoch:19 step:18625 [D loss: 0.616419, acc: 68.75%] [G loss: 1.933250]\n",
      "epoch:19 step:18626 [D loss: 0.645830, acc: 66.41%] [G loss: 1.958576]\n",
      "epoch:19 step:18627 [D loss: 0.606707, acc: 67.19%] [G loss: 1.813060]\n",
      "epoch:19 step:18628 [D loss: 0.615328, acc: 67.97%] [G loss: 2.025218]\n",
      "epoch:19 step:18629 [D loss: 0.701009, acc: 57.81%] [G loss: 1.889028]\n",
      "epoch:19 step:18630 [D loss: 0.686278, acc: 53.91%] [G loss: 1.686723]\n",
      "epoch:19 step:18631 [D loss: 0.639332, acc: 61.72%] [G loss: 1.736235]\n",
      "epoch:19 step:18632 [D loss: 0.655302, acc: 62.50%] [G loss: 1.863982]\n",
      "epoch:19 step:18633 [D loss: 0.661905, acc: 60.94%] [G loss: 1.774123]\n",
      "epoch:19 step:18634 [D loss: 0.656197, acc: 60.16%] [G loss: 1.878274]\n",
      "epoch:19 step:18635 [D loss: 0.637827, acc: 58.59%] [G loss: 1.908155]\n",
      "epoch:19 step:18636 [D loss: 0.622079, acc: 59.38%] [G loss: 1.964127]\n",
      "epoch:19 step:18637 [D loss: 0.650781, acc: 64.06%] [G loss: 1.926537]\n",
      "epoch:19 step:18638 [D loss: 0.634733, acc: 71.88%] [G loss: 1.860814]\n",
      "epoch:19 step:18639 [D loss: 0.671723, acc: 53.91%] [G loss: 1.891537]\n",
      "epoch:19 step:18640 [D loss: 0.573444, acc: 76.56%] [G loss: 1.858564]\n",
      "epoch:19 step:18641 [D loss: 0.630343, acc: 67.97%] [G loss: 1.957264]\n",
      "epoch:19 step:18642 [D loss: 0.595487, acc: 66.41%] [G loss: 1.902748]\n",
      "epoch:19 step:18643 [D loss: 0.622445, acc: 60.16%] [G loss: 1.898041]\n",
      "epoch:19 step:18644 [D loss: 0.660123, acc: 58.59%] [G loss: 1.851583]\n",
      "epoch:19 step:18645 [D loss: 0.597412, acc: 74.22%] [G loss: 2.019043]\n",
      "epoch:19 step:18646 [D loss: 0.705038, acc: 58.59%] [G loss: 1.756877]\n",
      "epoch:19 step:18647 [D loss: 0.660124, acc: 59.38%] [G loss: 1.799545]\n",
      "epoch:19 step:18648 [D loss: 0.661272, acc: 64.06%] [G loss: 1.916190]\n",
      "epoch:19 step:18649 [D loss: 0.640637, acc: 64.06%] [G loss: 1.805423]\n",
      "epoch:19 step:18650 [D loss: 0.676287, acc: 54.69%] [G loss: 1.901652]\n",
      "epoch:19 step:18651 [D loss: 0.626096, acc: 67.97%] [G loss: 1.766636]\n",
      "epoch:19 step:18652 [D loss: 0.606145, acc: 70.31%] [G loss: 1.909996]\n",
      "epoch:19 step:18653 [D loss: 0.667771, acc: 55.47%] [G loss: 1.800919]\n",
      "epoch:19 step:18654 [D loss: 0.644761, acc: 62.50%] [G loss: 1.791281]\n",
      "epoch:19 step:18655 [D loss: 0.643973, acc: 60.94%] [G loss: 1.865715]\n",
      "epoch:19 step:18656 [D loss: 0.605239, acc: 65.62%] [G loss: 1.788200]\n",
      "epoch:19 step:18657 [D loss: 0.671074, acc: 57.03%] [G loss: 1.918970]\n",
      "epoch:19 step:18658 [D loss: 0.682299, acc: 56.25%] [G loss: 1.844304]\n",
      "epoch:19 step:18659 [D loss: 0.685303, acc: 55.47%] [G loss: 1.755876]\n",
      "epoch:19 step:18660 [D loss: 0.615487, acc: 67.19%] [G loss: 1.842669]\n",
      "epoch:19 step:18661 [D loss: 0.698327, acc: 61.72%] [G loss: 1.795637]\n",
      "epoch:19 step:18662 [D loss: 0.691930, acc: 56.25%] [G loss: 1.786829]\n",
      "epoch:19 step:18663 [D loss: 0.621797, acc: 67.19%] [G loss: 1.830150]\n",
      "epoch:19 step:18664 [D loss: 0.611132, acc: 70.31%] [G loss: 1.798079]\n",
      "epoch:19 step:18665 [D loss: 0.615170, acc: 64.06%] [G loss: 1.964728]\n",
      "epoch:19 step:18666 [D loss: 0.634687, acc: 63.28%] [G loss: 1.915638]\n",
      "epoch:19 step:18667 [D loss: 0.681096, acc: 61.72%] [G loss: 1.816719]\n",
      "epoch:19 step:18668 [D loss: 0.676581, acc: 60.94%] [G loss: 1.758449]\n",
      "epoch:19 step:18669 [D loss: 0.611582, acc: 67.19%] [G loss: 1.875936]\n",
      "epoch:19 step:18670 [D loss: 0.669558, acc: 60.16%] [G loss: 1.853224]\n",
      "epoch:19 step:18671 [D loss: 0.668376, acc: 62.50%] [G loss: 1.928337]\n",
      "epoch:19 step:18672 [D loss: 0.634881, acc: 67.19%] [G loss: 1.757179]\n",
      "epoch:19 step:18673 [D loss: 0.646769, acc: 58.59%] [G loss: 1.985793]\n",
      "epoch:19 step:18674 [D loss: 0.686863, acc: 63.28%] [G loss: 1.855611]\n",
      "epoch:19 step:18675 [D loss: 0.636632, acc: 65.62%] [G loss: 1.898558]\n",
      "epoch:19 step:18676 [D loss: 0.656822, acc: 61.72%] [G loss: 1.703617]\n",
      "epoch:19 step:18677 [D loss: 0.709309, acc: 56.25%] [G loss: 1.743070]\n",
      "epoch:19 step:18678 [D loss: 0.614749, acc: 59.38%] [G loss: 1.983897]\n",
      "epoch:19 step:18679 [D loss: 0.702181, acc: 57.03%] [G loss: 1.859924]\n",
      "epoch:19 step:18680 [D loss: 0.589367, acc: 67.19%] [G loss: 1.872091]\n",
      "epoch:19 step:18681 [D loss: 0.657174, acc: 63.28%] [G loss: 1.944530]\n",
      "epoch:19 step:18682 [D loss: 0.647917, acc: 60.94%] [G loss: 1.766960]\n",
      "epoch:19 step:18683 [D loss: 0.636151, acc: 63.28%] [G loss: 1.789627]\n",
      "epoch:19 step:18684 [D loss: 0.651183, acc: 60.16%] [G loss: 1.852158]\n",
      "epoch:19 step:18685 [D loss: 0.619782, acc: 61.72%] [G loss: 1.847900]\n",
      "epoch:19 step:18686 [D loss: 0.624455, acc: 64.84%] [G loss: 1.881563]\n",
      "epoch:19 step:18687 [D loss: 0.600928, acc: 67.19%] [G loss: 1.967896]\n",
      "epoch:19 step:18688 [D loss: 0.657748, acc: 62.50%] [G loss: 1.843911]\n",
      "epoch:19 step:18689 [D loss: 0.623202, acc: 64.84%] [G loss: 1.972278]\n",
      "epoch:19 step:18690 [D loss: 0.642993, acc: 62.50%] [G loss: 1.839028]\n",
      "epoch:19 step:18691 [D loss: 0.714851, acc: 51.56%] [G loss: 1.801851]\n",
      "epoch:19 step:18692 [D loss: 0.654706, acc: 58.59%] [G loss: 1.888267]\n",
      "epoch:19 step:18693 [D loss: 0.668061, acc: 61.72%] [G loss: 1.830467]\n",
      "epoch:19 step:18694 [D loss: 0.681146, acc: 54.69%] [G loss: 1.844972]\n",
      "epoch:19 step:18695 [D loss: 0.705153, acc: 57.03%] [G loss: 1.732723]\n",
      "epoch:19 step:18696 [D loss: 0.712289, acc: 54.69%] [G loss: 1.818516]\n",
      "epoch:19 step:18697 [D loss: 0.608232, acc: 69.53%] [G loss: 1.834859]\n",
      "epoch:19 step:18698 [D loss: 0.657840, acc: 59.38%] [G loss: 1.749017]\n",
      "epoch:19 step:18699 [D loss: 0.665064, acc: 60.16%] [G loss: 1.755149]\n",
      "epoch:19 step:18700 [D loss: 0.673314, acc: 61.72%] [G loss: 1.942420]\n",
      "epoch:19 step:18701 [D loss: 0.640158, acc: 66.41%] [G loss: 1.856065]\n",
      "epoch:19 step:18702 [D loss: 0.630983, acc: 67.19%] [G loss: 1.982897]\n",
      "epoch:19 step:18703 [D loss: 0.652297, acc: 63.28%] [G loss: 1.958727]\n",
      "epoch:19 step:18704 [D loss: 0.597111, acc: 69.53%] [G loss: 1.937124]\n",
      "epoch:19 step:18705 [D loss: 0.629597, acc: 64.84%] [G loss: 1.867472]\n",
      "epoch:19 step:18706 [D loss: 0.675979, acc: 60.16%] [G loss: 1.873040]\n",
      "epoch:19 step:18707 [D loss: 0.619027, acc: 67.97%] [G loss: 1.849887]\n",
      "epoch:19 step:18708 [D loss: 0.592132, acc: 70.31%] [G loss: 1.944023]\n",
      "epoch:19 step:18709 [D loss: 0.652901, acc: 60.16%] [G loss: 1.942593]\n",
      "epoch:19 step:18710 [D loss: 0.651946, acc: 60.94%] [G loss: 1.922469]\n",
      "epoch:19 step:18711 [D loss: 0.637329, acc: 64.06%] [G loss: 1.915805]\n",
      "epoch:19 step:18712 [D loss: 0.623294, acc: 63.28%] [G loss: 2.061002]\n",
      "epoch:19 step:18713 [D loss: 0.670917, acc: 54.69%] [G loss: 1.984991]\n",
      "epoch:19 step:18714 [D loss: 0.630670, acc: 65.62%] [G loss: 2.035232]\n",
      "epoch:19 step:18715 [D loss: 0.652166, acc: 60.94%] [G loss: 2.007655]\n",
      "epoch:19 step:18716 [D loss: 0.658000, acc: 63.28%] [G loss: 2.010076]\n",
      "epoch:19 step:18717 [D loss: 0.658905, acc: 57.03%] [G loss: 1.850721]\n",
      "epoch:19 step:18718 [D loss: 0.646028, acc: 66.41%] [G loss: 1.891443]\n",
      "epoch:19 step:18719 [D loss: 0.654266, acc: 62.50%] [G loss: 1.973560]\n",
      "epoch:19 step:18720 [D loss: 0.618567, acc: 65.62%] [G loss: 1.880254]\n",
      "epoch:19 step:18721 [D loss: 0.614714, acc: 71.88%] [G loss: 2.097682]\n",
      "epoch:19 step:18722 [D loss: 0.593864, acc: 71.09%] [G loss: 2.200375]\n",
      "epoch:19 step:18723 [D loss: 0.707685, acc: 54.69%] [G loss: 1.886666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18724 [D loss: 0.617076, acc: 64.84%] [G loss: 2.009157]\n",
      "epoch:19 step:18725 [D loss: 0.668752, acc: 54.69%] [G loss: 1.789326]\n",
      "epoch:19 step:18726 [D loss: 0.599340, acc: 68.75%] [G loss: 2.034018]\n",
      "epoch:19 step:18727 [D loss: 0.594673, acc: 68.75%] [G loss: 2.173476]\n",
      "epoch:19 step:18728 [D loss: 0.531925, acc: 71.88%] [G loss: 2.182761]\n",
      "epoch:19 step:18729 [D loss: 0.638435, acc: 62.50%] [G loss: 2.337663]\n",
      "epoch:19 step:18730 [D loss: 0.588684, acc: 68.75%] [G loss: 2.177704]\n",
      "epoch:19 step:18731 [D loss: 0.758970, acc: 52.34%] [G loss: 1.803996]\n",
      "epoch:19 step:18732 [D loss: 0.819690, acc: 43.75%] [G loss: 1.754536]\n",
      "epoch:19 step:18733 [D loss: 0.579858, acc: 66.41%] [G loss: 2.108884]\n",
      "epoch:19 step:18734 [D loss: 0.609971, acc: 67.97%] [G loss: 2.031866]\n",
      "epoch:19 step:18735 [D loss: 0.636746, acc: 63.28%] [G loss: 1.912682]\n",
      "epoch:19 step:18736 [D loss: 0.634751, acc: 62.50%] [G loss: 1.920578]\n",
      "epoch:19 step:18737 [D loss: 0.626619, acc: 63.28%] [G loss: 1.802183]\n",
      "epoch:19 step:18738 [D loss: 0.682472, acc: 57.03%] [G loss: 1.960862]\n",
      "epoch:19 step:18739 [D loss: 0.595444, acc: 65.62%] [G loss: 2.186471]\n",
      "epoch:19 step:18740 [D loss: 0.643528, acc: 61.72%] [G loss: 2.325170]\n",
      "epoch:20 step:18741 [D loss: 0.647797, acc: 59.38%] [G loss: 1.888918]\n",
      "epoch:20 step:18742 [D loss: 0.638248, acc: 63.28%] [G loss: 2.112606]\n",
      "epoch:20 step:18743 [D loss: 0.630552, acc: 65.62%] [G loss: 1.937653]\n",
      "epoch:20 step:18744 [D loss: 0.643809, acc: 67.97%] [G loss: 1.770899]\n",
      "epoch:20 step:18745 [D loss: 0.637495, acc: 61.72%] [G loss: 2.012729]\n",
      "epoch:20 step:18746 [D loss: 0.609849, acc: 68.75%] [G loss: 2.026371]\n",
      "epoch:20 step:18747 [D loss: 0.637372, acc: 65.62%] [G loss: 1.990867]\n",
      "epoch:20 step:18748 [D loss: 0.614937, acc: 68.75%] [G loss: 2.090287]\n",
      "epoch:20 step:18749 [D loss: 0.649074, acc: 66.41%] [G loss: 2.028446]\n",
      "epoch:20 step:18750 [D loss: 0.573053, acc: 70.31%] [G loss: 2.004678]\n",
      "epoch:20 step:18751 [D loss: 0.632373, acc: 59.38%] [G loss: 1.963382]\n",
      "epoch:20 step:18752 [D loss: 0.617046, acc: 64.06%] [G loss: 1.912535]\n",
      "epoch:20 step:18753 [D loss: 0.594257, acc: 66.41%] [G loss: 2.063234]\n",
      "epoch:20 step:18754 [D loss: 0.635900, acc: 70.31%] [G loss: 1.984867]\n",
      "epoch:20 step:18755 [D loss: 0.628749, acc: 57.03%] [G loss: 2.013263]\n",
      "epoch:20 step:18756 [D loss: 0.593533, acc: 70.31%] [G loss: 1.963249]\n",
      "epoch:20 step:18757 [D loss: 0.665787, acc: 59.38%] [G loss: 1.918579]\n",
      "epoch:20 step:18758 [D loss: 0.656230, acc: 60.16%] [G loss: 1.965475]\n",
      "epoch:20 step:18759 [D loss: 0.601184, acc: 71.09%] [G loss: 1.973610]\n",
      "epoch:20 step:18760 [D loss: 0.708785, acc: 53.91%] [G loss: 1.698530]\n",
      "epoch:20 step:18761 [D loss: 0.683922, acc: 56.25%] [G loss: 1.850515]\n",
      "epoch:20 step:18762 [D loss: 0.715311, acc: 54.69%] [G loss: 1.781697]\n",
      "epoch:20 step:18763 [D loss: 0.628773, acc: 65.62%] [G loss: 1.990211]\n",
      "epoch:20 step:18764 [D loss: 0.607741, acc: 64.84%] [G loss: 1.918860]\n",
      "epoch:20 step:18765 [D loss: 0.609322, acc: 64.06%] [G loss: 1.916434]\n",
      "epoch:20 step:18766 [D loss: 0.655660, acc: 64.06%] [G loss: 1.851706]\n",
      "epoch:20 step:18767 [D loss: 0.754082, acc: 52.34%] [G loss: 1.779296]\n",
      "epoch:20 step:18768 [D loss: 0.649584, acc: 59.38%] [G loss: 1.917870]\n",
      "epoch:20 step:18769 [D loss: 0.635717, acc: 66.41%] [G loss: 1.912917]\n",
      "epoch:20 step:18770 [D loss: 0.676249, acc: 53.91%] [G loss: 1.688360]\n",
      "epoch:20 step:18771 [D loss: 0.656160, acc: 56.25%] [G loss: 1.796711]\n",
      "epoch:20 step:18772 [D loss: 0.671244, acc: 59.38%] [G loss: 1.819591]\n",
      "epoch:20 step:18773 [D loss: 0.607771, acc: 64.06%] [G loss: 1.825652]\n",
      "epoch:20 step:18774 [D loss: 0.679975, acc: 58.59%] [G loss: 1.854311]\n",
      "epoch:20 step:18775 [D loss: 0.660334, acc: 59.38%] [G loss: 1.971208]\n",
      "epoch:20 step:18776 [D loss: 0.621563, acc: 65.62%] [G loss: 1.830774]\n",
      "epoch:20 step:18777 [D loss: 0.698167, acc: 58.59%] [G loss: 1.894125]\n",
      "epoch:20 step:18778 [D loss: 0.641945, acc: 67.19%] [G loss: 1.828343]\n",
      "epoch:20 step:18779 [D loss: 0.616120, acc: 64.84%] [G loss: 1.926987]\n",
      "epoch:20 step:18780 [D loss: 0.598836, acc: 67.19%] [G loss: 2.050645]\n",
      "epoch:20 step:18781 [D loss: 0.638955, acc: 64.84%] [G loss: 1.840550]\n",
      "epoch:20 step:18782 [D loss: 0.616666, acc: 64.06%] [G loss: 1.982055]\n",
      "epoch:20 step:18783 [D loss: 0.619941, acc: 64.06%] [G loss: 1.924578]\n",
      "epoch:20 step:18784 [D loss: 0.665455, acc: 60.16%] [G loss: 1.713148]\n",
      "epoch:20 step:18785 [D loss: 0.596653, acc: 68.75%] [G loss: 2.046085]\n",
      "epoch:20 step:18786 [D loss: 0.676237, acc: 59.38%] [G loss: 1.761817]\n",
      "epoch:20 step:18787 [D loss: 0.617934, acc: 64.84%] [G loss: 1.939986]\n",
      "epoch:20 step:18788 [D loss: 0.624100, acc: 64.06%] [G loss: 1.876196]\n",
      "epoch:20 step:18789 [D loss: 0.561001, acc: 67.19%] [G loss: 2.095153]\n",
      "epoch:20 step:18790 [D loss: 0.590712, acc: 68.75%] [G loss: 2.045021]\n",
      "epoch:20 step:18791 [D loss: 0.642983, acc: 58.59%] [G loss: 1.910510]\n",
      "epoch:20 step:18792 [D loss: 0.616705, acc: 72.66%] [G loss: 1.882727]\n",
      "epoch:20 step:18793 [D loss: 0.636064, acc: 60.16%] [G loss: 2.198731]\n",
      "epoch:20 step:18794 [D loss: 0.700003, acc: 57.03%] [G loss: 2.007904]\n",
      "epoch:20 step:18795 [D loss: 0.615868, acc: 64.84%] [G loss: 2.020931]\n",
      "epoch:20 step:18796 [D loss: 0.688675, acc: 61.72%] [G loss: 2.011853]\n",
      "epoch:20 step:18797 [D loss: 0.635103, acc: 58.59%] [G loss: 2.045279]\n",
      "epoch:20 step:18798 [D loss: 0.627101, acc: 64.84%] [G loss: 1.975988]\n",
      "epoch:20 step:18799 [D loss: 0.665483, acc: 64.84%] [G loss: 1.861763]\n",
      "epoch:20 step:18800 [D loss: 0.696724, acc: 55.47%] [G loss: 1.884802]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.363793\n",
      "FID: 13.018443\n",
      "0 = 12.812949567651717\n",
      "1 = 0.09160317620846721\n",
      "2 = 0.888700008392334\n",
      "3 = 0.9056000113487244\n",
      "4 = 0.8718000054359436\n",
      "5 = 0.8759914636611938\n",
      "6 = 0.9056000113487244\n",
      "7 = 6.409199053668965\n",
      "8 = 0.07194654801221545\n",
      "9 = 0.7149999737739563\n",
      "10 = 0.7246000170707703\n",
      "11 = 0.7053999900817871\n",
      "12 = 0.7109497785568237\n",
      "13 = 0.7246000170707703\n",
      "14 = 7.36382532119751\n",
      "15 = 9.448363304138184\n",
      "16 = 0.11750118434429169\n",
      "17 = 7.36379337310791\n",
      "18 = 13.01844310760498\n",
      "epoch:20 step:18801 [D loss: 0.657938, acc: 62.50%] [G loss: 1.877836]\n",
      "epoch:20 step:18802 [D loss: 0.648459, acc: 57.81%] [G loss: 1.963943]\n",
      "epoch:20 step:18803 [D loss: 0.633783, acc: 64.06%] [G loss: 1.922935]\n",
      "epoch:20 step:18804 [D loss: 0.693471, acc: 65.62%] [G loss: 1.879882]\n",
      "epoch:20 step:18805 [D loss: 0.672157, acc: 63.28%] [G loss: 1.880615]\n",
      "epoch:20 step:18806 [D loss: 0.625952, acc: 65.62%] [G loss: 1.896823]\n",
      "epoch:20 step:18807 [D loss: 0.618574, acc: 62.50%] [G loss: 2.015851]\n",
      "epoch:20 step:18808 [D loss: 0.651325, acc: 64.84%] [G loss: 1.933788]\n",
      "epoch:20 step:18809 [D loss: 0.605039, acc: 66.41%] [G loss: 2.020735]\n",
      "epoch:20 step:18810 [D loss: 0.646585, acc: 61.72%] [G loss: 1.994035]\n",
      "epoch:20 step:18811 [D loss: 0.657021, acc: 58.59%] [G loss: 1.857859]\n",
      "epoch:20 step:18812 [D loss: 0.623962, acc: 66.41%] [G loss: 1.986056]\n",
      "epoch:20 step:18813 [D loss: 0.643381, acc: 66.41%] [G loss: 1.858070]\n",
      "epoch:20 step:18814 [D loss: 0.648326, acc: 60.16%] [G loss: 1.807876]\n",
      "epoch:20 step:18815 [D loss: 0.675769, acc: 59.38%] [G loss: 2.120706]\n",
      "epoch:20 step:18816 [D loss: 0.611537, acc: 66.41%] [G loss: 2.075352]\n",
      "epoch:20 step:18817 [D loss: 0.571715, acc: 70.31%] [G loss: 2.062541]\n",
      "epoch:20 step:18818 [D loss: 0.674383, acc: 62.50%] [G loss: 1.725060]\n",
      "epoch:20 step:18819 [D loss: 0.679529, acc: 55.47%] [G loss: 1.893237]\n",
      "epoch:20 step:18820 [D loss: 0.669573, acc: 56.25%] [G loss: 1.920093]\n",
      "epoch:20 step:18821 [D loss: 0.705158, acc: 53.91%] [G loss: 1.792610]\n",
      "epoch:20 step:18822 [D loss: 0.641867, acc: 60.94%] [G loss: 1.858796]\n",
      "epoch:20 step:18823 [D loss: 0.631958, acc: 67.19%] [G loss: 1.922532]\n",
      "epoch:20 step:18824 [D loss: 0.618741, acc: 58.59%] [G loss: 1.977948]\n",
      "epoch:20 step:18825 [D loss: 0.671151, acc: 58.59%] [G loss: 1.862670]\n",
      "epoch:20 step:18826 [D loss: 0.690850, acc: 50.00%] [G loss: 1.745845]\n",
      "epoch:20 step:18827 [D loss: 0.628854, acc: 67.97%] [G loss: 1.826471]\n",
      "epoch:20 step:18828 [D loss: 0.606701, acc: 66.41%] [G loss: 1.901584]\n",
      "epoch:20 step:18829 [D loss: 0.626040, acc: 61.72%] [G loss: 1.877262]\n",
      "epoch:20 step:18830 [D loss: 0.619875, acc: 64.84%] [G loss: 2.089464]\n",
      "epoch:20 step:18831 [D loss: 0.712456, acc: 57.03%] [G loss: 1.672414]\n",
      "epoch:20 step:18832 [D loss: 0.625015, acc: 67.19%] [G loss: 1.836872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:18833 [D loss: 0.627046, acc: 66.41%] [G loss: 2.177992]\n",
      "epoch:20 step:18834 [D loss: 0.587648, acc: 67.19%] [G loss: 2.006368]\n",
      "epoch:20 step:18835 [D loss: 0.726407, acc: 54.69%] [G loss: 1.861584]\n",
      "epoch:20 step:18836 [D loss: 0.666317, acc: 58.59%] [G loss: 1.860312]\n",
      "epoch:20 step:18837 [D loss: 0.633423, acc: 60.94%] [G loss: 1.885810]\n",
      "epoch:20 step:18838 [D loss: 0.663030, acc: 60.94%] [G loss: 1.852460]\n",
      "epoch:20 step:18839 [D loss: 0.626898, acc: 64.84%] [G loss: 1.892982]\n",
      "epoch:20 step:18840 [D loss: 0.645571, acc: 64.84%] [G loss: 1.809146]\n",
      "epoch:20 step:18841 [D loss: 0.646678, acc: 63.28%] [G loss: 1.817343]\n",
      "epoch:20 step:18842 [D loss: 0.653364, acc: 58.59%] [G loss: 1.992956]\n",
      "epoch:20 step:18843 [D loss: 0.633430, acc: 67.97%] [G loss: 1.867852]\n",
      "epoch:20 step:18844 [D loss: 0.668300, acc: 61.72%] [G loss: 1.832835]\n",
      "epoch:20 step:18845 [D loss: 0.618153, acc: 68.75%] [G loss: 2.023887]\n",
      "epoch:20 step:18846 [D loss: 0.606005, acc: 67.97%] [G loss: 2.019386]\n",
      "epoch:20 step:18847 [D loss: 0.616269, acc: 64.84%] [G loss: 2.097868]\n",
      "epoch:20 step:18848 [D loss: 0.623461, acc: 67.19%] [G loss: 1.824798]\n",
      "epoch:20 step:18849 [D loss: 0.632522, acc: 66.41%] [G loss: 1.923205]\n",
      "epoch:20 step:18850 [D loss: 0.631628, acc: 64.84%] [G loss: 1.844128]\n",
      "epoch:20 step:18851 [D loss: 0.666787, acc: 60.94%] [G loss: 2.042515]\n",
      "epoch:20 step:18852 [D loss: 0.633030, acc: 58.59%] [G loss: 1.866446]\n",
      "epoch:20 step:18853 [D loss: 0.639668, acc: 60.94%] [G loss: 1.943100]\n",
      "epoch:20 step:18854 [D loss: 0.676769, acc: 60.94%] [G loss: 1.951651]\n",
      "epoch:20 step:18855 [D loss: 0.567621, acc: 72.66%] [G loss: 2.125975]\n",
      "epoch:20 step:18856 [D loss: 0.644175, acc: 65.62%] [G loss: 2.088253]\n",
      "epoch:20 step:18857 [D loss: 0.577372, acc: 71.09%] [G loss: 2.176883]\n",
      "epoch:20 step:18858 [D loss: 0.628653, acc: 71.09%] [G loss: 2.086553]\n",
      "epoch:20 step:18859 [D loss: 0.595672, acc: 69.53%] [G loss: 2.329756]\n",
      "epoch:20 step:18860 [D loss: 0.654869, acc: 61.72%] [G loss: 2.024848]\n",
      "epoch:20 step:18861 [D loss: 0.661901, acc: 61.72%] [G loss: 1.964372]\n",
      "epoch:20 step:18862 [D loss: 0.682974, acc: 60.16%] [G loss: 2.062199]\n",
      "epoch:20 step:18863 [D loss: 0.676200, acc: 62.50%] [G loss: 1.994883]\n",
      "epoch:20 step:18864 [D loss: 0.753838, acc: 45.31%] [G loss: 1.812479]\n",
      "epoch:20 step:18865 [D loss: 0.646721, acc: 56.25%] [G loss: 1.883965]\n",
      "epoch:20 step:18866 [D loss: 0.653921, acc: 64.06%] [G loss: 1.941102]\n",
      "epoch:20 step:18867 [D loss: 0.657928, acc: 58.59%] [G loss: 1.793071]\n",
      "epoch:20 step:18868 [D loss: 0.666698, acc: 60.16%] [G loss: 1.943503]\n",
      "epoch:20 step:18869 [D loss: 0.693304, acc: 57.81%] [G loss: 1.690076]\n",
      "epoch:20 step:18870 [D loss: 0.641013, acc: 64.06%] [G loss: 1.970728]\n",
      "epoch:20 step:18871 [D loss: 0.677866, acc: 52.34%] [G loss: 1.808912]\n",
      "epoch:20 step:18872 [D loss: 0.624465, acc: 66.41%] [G loss: 1.843921]\n",
      "epoch:20 step:18873 [D loss: 0.712021, acc: 51.56%] [G loss: 1.721499]\n",
      "epoch:20 step:18874 [D loss: 0.674020, acc: 57.03%] [G loss: 1.800293]\n",
      "epoch:20 step:18875 [D loss: 0.620473, acc: 64.84%] [G loss: 1.843951]\n",
      "epoch:20 step:18876 [D loss: 0.660611, acc: 60.94%] [G loss: 1.791088]\n",
      "epoch:20 step:18877 [D loss: 0.663260, acc: 61.72%] [G loss: 1.756884]\n",
      "epoch:20 step:18878 [D loss: 0.657241, acc: 62.50%] [G loss: 1.799685]\n",
      "epoch:20 step:18879 [D loss: 0.645844, acc: 61.72%] [G loss: 1.862146]\n",
      "epoch:20 step:18880 [D loss: 0.650869, acc: 65.62%] [G loss: 1.862404]\n",
      "epoch:20 step:18881 [D loss: 0.646315, acc: 62.50%] [G loss: 1.777881]\n",
      "epoch:20 step:18882 [D loss: 0.673387, acc: 60.16%] [G loss: 1.825289]\n",
      "epoch:20 step:18883 [D loss: 0.654924, acc: 53.91%] [G loss: 1.826164]\n",
      "epoch:20 step:18884 [D loss: 0.676044, acc: 60.16%] [G loss: 1.977399]\n",
      "epoch:20 step:18885 [D loss: 0.674297, acc: 57.03%] [G loss: 1.814504]\n",
      "epoch:20 step:18886 [D loss: 0.630364, acc: 63.28%] [G loss: 1.871779]\n",
      "epoch:20 step:18887 [D loss: 0.647477, acc: 60.94%] [G loss: 1.869444]\n",
      "epoch:20 step:18888 [D loss: 0.695362, acc: 55.47%] [G loss: 1.710614]\n",
      "epoch:20 step:18889 [D loss: 0.628087, acc: 66.41%] [G loss: 1.727142]\n",
      "epoch:20 step:18890 [D loss: 0.631891, acc: 67.19%] [G loss: 1.893075]\n",
      "epoch:20 step:18891 [D loss: 0.638288, acc: 63.28%] [G loss: 1.890361]\n",
      "epoch:20 step:18892 [D loss: 0.672035, acc: 61.72%] [G loss: 1.959195]\n",
      "epoch:20 step:18893 [D loss: 0.681642, acc: 58.59%] [G loss: 1.718815]\n",
      "epoch:20 step:18894 [D loss: 0.624502, acc: 66.41%] [G loss: 1.966945]\n",
      "epoch:20 step:18895 [D loss: 0.635739, acc: 64.06%] [G loss: 1.791524]\n",
      "epoch:20 step:18896 [D loss: 0.700821, acc: 60.94%] [G loss: 2.026323]\n",
      "epoch:20 step:18897 [D loss: 0.621069, acc: 66.41%] [G loss: 1.915645]\n",
      "epoch:20 step:18898 [D loss: 0.610977, acc: 69.53%] [G loss: 1.743289]\n",
      "epoch:20 step:18899 [D loss: 0.659856, acc: 58.59%] [G loss: 1.828333]\n",
      "epoch:20 step:18900 [D loss: 0.660762, acc: 63.28%] [G loss: 1.884780]\n",
      "epoch:20 step:18901 [D loss: 0.607014, acc: 67.97%] [G loss: 1.973642]\n",
      "epoch:20 step:18902 [D loss: 0.631804, acc: 65.62%] [G loss: 2.027736]\n",
      "epoch:20 step:18903 [D loss: 0.669385, acc: 62.50%] [G loss: 1.949124]\n",
      "epoch:20 step:18904 [D loss: 0.607596, acc: 63.28%] [G loss: 1.893608]\n",
      "epoch:20 step:18905 [D loss: 0.661680, acc: 64.84%] [G loss: 2.000154]\n",
      "epoch:20 step:18906 [D loss: 0.574999, acc: 68.75%] [G loss: 1.919605]\n",
      "epoch:20 step:18907 [D loss: 0.661511, acc: 61.72%] [G loss: 1.899256]\n",
      "epoch:20 step:18908 [D loss: 0.668537, acc: 59.38%] [G loss: 1.820954]\n",
      "epoch:20 step:18909 [D loss: 0.666871, acc: 58.59%] [G loss: 1.849352]\n",
      "epoch:20 step:18910 [D loss: 0.661089, acc: 63.28%] [G loss: 1.823085]\n",
      "epoch:20 step:18911 [D loss: 0.579223, acc: 69.53%] [G loss: 1.878992]\n",
      "epoch:20 step:18912 [D loss: 0.652082, acc: 60.16%] [G loss: 2.070199]\n",
      "epoch:20 step:18913 [D loss: 0.687224, acc: 54.69%] [G loss: 1.758880]\n",
      "epoch:20 step:18914 [D loss: 0.639839, acc: 64.84%] [G loss: 1.776903]\n",
      "epoch:20 step:18915 [D loss: 0.660774, acc: 61.72%] [G loss: 1.890114]\n",
      "epoch:20 step:18916 [D loss: 0.645493, acc: 62.50%] [G loss: 1.813226]\n",
      "epoch:20 step:18917 [D loss: 0.733939, acc: 53.91%] [G loss: 1.757451]\n",
      "epoch:20 step:18918 [D loss: 0.626208, acc: 61.72%] [G loss: 1.790880]\n",
      "epoch:20 step:18919 [D loss: 0.615689, acc: 66.41%] [G loss: 1.895015]\n",
      "epoch:20 step:18920 [D loss: 0.660417, acc: 60.94%] [G loss: 1.831974]\n",
      "epoch:20 step:18921 [D loss: 0.645151, acc: 60.16%] [G loss: 1.685902]\n",
      "epoch:20 step:18922 [D loss: 0.722696, acc: 55.47%] [G loss: 1.736093]\n",
      "epoch:20 step:18923 [D loss: 0.688657, acc: 60.16%] [G loss: 1.787012]\n",
      "epoch:20 step:18924 [D loss: 0.658583, acc: 60.94%] [G loss: 1.844482]\n",
      "epoch:20 step:18925 [D loss: 0.708539, acc: 54.69%] [G loss: 1.868482]\n",
      "epoch:20 step:18926 [D loss: 0.657952, acc: 64.84%] [G loss: 1.819445]\n",
      "epoch:20 step:18927 [D loss: 0.657190, acc: 59.38%] [G loss: 1.884000]\n",
      "epoch:20 step:18928 [D loss: 0.676400, acc: 60.16%] [G loss: 1.926054]\n",
      "epoch:20 step:18929 [D loss: 0.610590, acc: 68.75%] [G loss: 1.802328]\n",
      "epoch:20 step:18930 [D loss: 0.649630, acc: 60.16%] [G loss: 1.909745]\n",
      "epoch:20 step:18931 [D loss: 0.632049, acc: 67.97%] [G loss: 1.820032]\n",
      "epoch:20 step:18932 [D loss: 0.630066, acc: 66.41%] [G loss: 1.879937]\n",
      "epoch:20 step:18933 [D loss: 0.604164, acc: 66.41%] [G loss: 2.062017]\n",
      "epoch:20 step:18934 [D loss: 0.619230, acc: 67.19%] [G loss: 2.043638]\n",
      "epoch:20 step:18935 [D loss: 0.663512, acc: 60.16%] [G loss: 2.029142]\n",
      "epoch:20 step:18936 [D loss: 0.666718, acc: 60.94%] [G loss: 1.933302]\n",
      "epoch:20 step:18937 [D loss: 0.616107, acc: 74.22%] [G loss: 1.822412]\n",
      "epoch:20 step:18938 [D loss: 0.597262, acc: 65.62%] [G loss: 1.874075]\n",
      "epoch:20 step:18939 [D loss: 0.674034, acc: 64.84%] [G loss: 1.913010]\n",
      "epoch:20 step:18940 [D loss: 0.737615, acc: 50.78%] [G loss: 1.837426]\n",
      "epoch:20 step:18941 [D loss: 0.606456, acc: 66.41%] [G loss: 1.955630]\n",
      "epoch:20 step:18942 [D loss: 0.645481, acc: 64.06%] [G loss: 1.964530]\n",
      "epoch:20 step:18943 [D loss: 0.666796, acc: 58.59%] [G loss: 1.836334]\n",
      "epoch:20 step:18944 [D loss: 0.632200, acc: 67.19%] [G loss: 1.894725]\n",
      "epoch:20 step:18945 [D loss: 0.707788, acc: 52.34%] [G loss: 1.842925]\n",
      "epoch:20 step:18946 [D loss: 0.639726, acc: 65.62%] [G loss: 2.057888]\n",
      "epoch:20 step:18947 [D loss: 0.641118, acc: 70.31%] [G loss: 1.997824]\n",
      "epoch:20 step:18948 [D loss: 0.600864, acc: 64.84%] [G loss: 2.174521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:18949 [D loss: 0.524895, acc: 75.00%] [G loss: 2.229183]\n",
      "epoch:20 step:18950 [D loss: 0.691345, acc: 57.81%] [G loss: 1.850395]\n",
      "epoch:20 step:18951 [D loss: 0.650004, acc: 61.72%] [G loss: 1.902280]\n",
      "epoch:20 step:18952 [D loss: 0.646799, acc: 66.41%] [G loss: 1.808071]\n",
      "epoch:20 step:18953 [D loss: 0.676251, acc: 56.25%] [G loss: 1.838815]\n",
      "epoch:20 step:18954 [D loss: 0.718333, acc: 54.69%] [G loss: 1.823229]\n",
      "epoch:20 step:18955 [D loss: 0.656341, acc: 59.38%] [G loss: 1.777673]\n",
      "epoch:20 step:18956 [D loss: 0.629339, acc: 61.72%] [G loss: 1.895486]\n",
      "epoch:20 step:18957 [D loss: 0.669493, acc: 60.94%] [G loss: 1.961221]\n",
      "epoch:20 step:18958 [D loss: 0.626556, acc: 64.84%] [G loss: 1.983077]\n",
      "epoch:20 step:18959 [D loss: 0.581214, acc: 69.53%] [G loss: 2.222404]\n",
      "epoch:20 step:18960 [D loss: 0.727050, acc: 53.12%] [G loss: 1.718565]\n",
      "epoch:20 step:18961 [D loss: 0.651625, acc: 66.41%] [G loss: 2.101337]\n",
      "epoch:20 step:18962 [D loss: 0.658370, acc: 64.84%] [G loss: 2.104951]\n",
      "epoch:20 step:18963 [D loss: 0.620076, acc: 64.84%] [G loss: 2.069847]\n",
      "epoch:20 step:18964 [D loss: 0.638988, acc: 60.16%] [G loss: 1.899218]\n",
      "epoch:20 step:18965 [D loss: 0.633924, acc: 71.88%] [G loss: 1.884750]\n",
      "epoch:20 step:18966 [D loss: 0.677592, acc: 56.25%] [G loss: 1.808029]\n",
      "epoch:20 step:18967 [D loss: 0.680705, acc: 60.94%] [G loss: 1.882172]\n",
      "epoch:20 step:18968 [D loss: 0.654391, acc: 60.16%] [G loss: 1.757479]\n",
      "epoch:20 step:18969 [D loss: 0.617117, acc: 66.41%] [G loss: 2.085643]\n",
      "epoch:20 step:18970 [D loss: 0.608070, acc: 67.19%] [G loss: 2.063812]\n",
      "epoch:20 step:18971 [D loss: 0.583902, acc: 74.22%] [G loss: 2.259132]\n",
      "epoch:20 step:18972 [D loss: 0.563384, acc: 71.09%] [G loss: 2.339665]\n",
      "epoch:20 step:18973 [D loss: 0.697519, acc: 56.25%] [G loss: 1.879681]\n",
      "epoch:20 step:18974 [D loss: 0.656284, acc: 62.50%] [G loss: 1.775274]\n",
      "epoch:20 step:18975 [D loss: 0.666336, acc: 60.16%] [G loss: 1.828720]\n",
      "epoch:20 step:18976 [D loss: 0.627834, acc: 61.72%] [G loss: 1.837978]\n",
      "epoch:20 step:18977 [D loss: 0.671855, acc: 58.59%] [G loss: 1.932279]\n",
      "epoch:20 step:18978 [D loss: 0.680535, acc: 62.50%] [G loss: 2.072226]\n",
      "epoch:20 step:18979 [D loss: 0.643717, acc: 61.72%] [G loss: 1.876720]\n",
      "epoch:20 step:18980 [D loss: 0.663227, acc: 57.03%] [G loss: 1.940260]\n",
      "epoch:20 step:18981 [D loss: 0.602013, acc: 69.53%] [G loss: 1.923806]\n",
      "epoch:20 step:18982 [D loss: 0.600808, acc: 71.09%] [G loss: 1.908714]\n",
      "epoch:20 step:18983 [D loss: 0.645084, acc: 66.41%] [G loss: 1.878896]\n",
      "epoch:20 step:18984 [D loss: 0.610727, acc: 68.75%] [G loss: 1.861959]\n",
      "epoch:20 step:18985 [D loss: 0.621532, acc: 64.84%] [G loss: 1.973832]\n",
      "epoch:20 step:18986 [D loss: 0.701681, acc: 56.25%] [G loss: 1.794487]\n",
      "epoch:20 step:18987 [D loss: 0.652772, acc: 59.38%] [G loss: 1.733522]\n",
      "epoch:20 step:18988 [D loss: 0.624242, acc: 64.06%] [G loss: 2.096408]\n",
      "epoch:20 step:18989 [D loss: 0.724375, acc: 46.88%] [G loss: 1.832185]\n",
      "epoch:20 step:18990 [D loss: 0.670542, acc: 62.50%] [G loss: 1.708905]\n",
      "epoch:20 step:18991 [D loss: 0.680508, acc: 61.72%] [G loss: 1.781492]\n",
      "epoch:20 step:18992 [D loss: 0.665942, acc: 60.16%] [G loss: 1.587946]\n",
      "epoch:20 step:18993 [D loss: 0.636817, acc: 62.50%] [G loss: 1.847535]\n",
      "epoch:20 step:18994 [D loss: 0.634032, acc: 64.06%] [G loss: 1.837261]\n",
      "epoch:20 step:18995 [D loss: 0.654396, acc: 59.38%] [G loss: 1.771609]\n",
      "epoch:20 step:18996 [D loss: 0.624450, acc: 66.41%] [G loss: 1.751102]\n",
      "epoch:20 step:18997 [D loss: 0.639091, acc: 65.62%] [G loss: 1.866381]\n",
      "epoch:20 step:18998 [D loss: 0.634886, acc: 63.28%] [G loss: 1.899242]\n",
      "epoch:20 step:18999 [D loss: 0.677851, acc: 59.38%] [G loss: 1.878622]\n",
      "epoch:20 step:19000 [D loss: 0.643642, acc: 62.50%] [G loss: 1.959904]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.256021\n",
      "FID: 15.504208\n",
      "0 = 12.882926489353167\n",
      "1 = 0.09279977524993252\n",
      "2 = 0.8824999928474426\n",
      "3 = 0.8939999938011169\n",
      "4 = 0.8709999918937683\n",
      "5 = 0.873900294303894\n",
      "6 = 0.8939999938011169\n",
      "7 = 6.56959633157253\n",
      "8 = 0.0805501285639565\n",
      "9 = 0.7221999764442444\n",
      "10 = 0.7324000000953674\n",
      "11 = 0.7120000123977661\n",
      "12 = 0.7177577614784241\n",
      "13 = 0.7324000000953674\n",
      "14 = 7.256051063537598\n",
      "15 = 9.365236282348633\n",
      "16 = 0.13296747207641602\n",
      "17 = 7.256020545959473\n",
      "18 = 15.504207611083984\n",
      "epoch:20 step:19001 [D loss: 0.658066, acc: 63.28%] [G loss: 1.846318]\n",
      "epoch:20 step:19002 [D loss: 0.606426, acc: 67.97%] [G loss: 1.848769]\n",
      "epoch:20 step:19003 [D loss: 0.643548, acc: 64.06%] [G loss: 2.032778]\n",
      "epoch:20 step:19004 [D loss: 0.655626, acc: 61.72%] [G loss: 2.011232]\n",
      "epoch:20 step:19005 [D loss: 0.692036, acc: 59.38%] [G loss: 1.885224]\n",
      "epoch:20 step:19006 [D loss: 0.676263, acc: 64.06%] [G loss: 1.799651]\n",
      "epoch:20 step:19007 [D loss: 0.704935, acc: 57.03%] [G loss: 1.790253]\n",
      "epoch:20 step:19008 [D loss: 0.676810, acc: 57.03%] [G loss: 1.654541]\n",
      "epoch:20 step:19009 [D loss: 0.648718, acc: 60.94%] [G loss: 1.943613]\n",
      "epoch:20 step:19010 [D loss: 0.664645, acc: 62.50%] [G loss: 1.816777]\n",
      "epoch:20 step:19011 [D loss: 0.601384, acc: 68.75%] [G loss: 1.913425]\n",
      "epoch:20 step:19012 [D loss: 0.681657, acc: 60.16%] [G loss: 2.008482]\n",
      "epoch:20 step:19013 [D loss: 0.669229, acc: 60.16%] [G loss: 1.708365]\n",
      "epoch:20 step:19014 [D loss: 0.587206, acc: 71.09%] [G loss: 1.961735]\n",
      "epoch:20 step:19015 [D loss: 0.601575, acc: 67.97%] [G loss: 1.886091]\n",
      "epoch:20 step:19016 [D loss: 0.584317, acc: 71.88%] [G loss: 2.009542]\n",
      "epoch:20 step:19017 [D loss: 0.699155, acc: 56.25%] [G loss: 1.875070]\n",
      "epoch:20 step:19018 [D loss: 0.633648, acc: 60.16%] [G loss: 1.805604]\n",
      "epoch:20 step:19019 [D loss: 0.628119, acc: 71.09%] [G loss: 1.956407]\n",
      "epoch:20 step:19020 [D loss: 0.629226, acc: 62.50%] [G loss: 1.846761]\n",
      "epoch:20 step:19021 [D loss: 0.746073, acc: 46.88%] [G loss: 1.829879]\n",
      "epoch:20 step:19022 [D loss: 0.648659, acc: 57.03%] [G loss: 1.833731]\n",
      "epoch:20 step:19023 [D loss: 0.584986, acc: 73.44%] [G loss: 1.752201]\n",
      "epoch:20 step:19024 [D loss: 0.670132, acc: 59.38%] [G loss: 1.743427]\n",
      "epoch:20 step:19025 [D loss: 0.686345, acc: 58.59%] [G loss: 1.735222]\n",
      "epoch:20 step:19026 [D loss: 0.631663, acc: 66.41%] [G loss: 1.830299]\n",
      "epoch:20 step:19027 [D loss: 0.672001, acc: 57.81%] [G loss: 1.742121]\n",
      "epoch:20 step:19028 [D loss: 0.662742, acc: 61.72%] [G loss: 1.621684]\n",
      "epoch:20 step:19029 [D loss: 0.686978, acc: 51.56%] [G loss: 1.751066]\n",
      "epoch:20 step:19030 [D loss: 0.672833, acc: 58.59%] [G loss: 1.786821]\n",
      "epoch:20 step:19031 [D loss: 0.627100, acc: 67.19%] [G loss: 1.852412]\n",
      "epoch:20 step:19032 [D loss: 0.657717, acc: 60.94%] [G loss: 1.734535]\n",
      "epoch:20 step:19033 [D loss: 0.618261, acc: 66.41%] [G loss: 1.971481]\n",
      "epoch:20 step:19034 [D loss: 0.623385, acc: 67.19%] [G loss: 1.763362]\n",
      "epoch:20 step:19035 [D loss: 0.690733, acc: 58.59%] [G loss: 1.912918]\n",
      "epoch:20 step:19036 [D loss: 0.576619, acc: 72.66%] [G loss: 1.878991]\n",
      "epoch:20 step:19037 [D loss: 0.632959, acc: 67.19%] [G loss: 1.889123]\n",
      "epoch:20 step:19038 [D loss: 0.666637, acc: 58.59%] [G loss: 1.926653]\n",
      "epoch:20 step:19039 [D loss: 0.639577, acc: 60.94%] [G loss: 1.917062]\n",
      "epoch:20 step:19040 [D loss: 0.590152, acc: 68.75%] [G loss: 1.842824]\n",
      "epoch:20 step:19041 [D loss: 0.674575, acc: 55.47%] [G loss: 1.847609]\n",
      "epoch:20 step:19042 [D loss: 0.626218, acc: 68.75%] [G loss: 1.954522]\n",
      "epoch:20 step:19043 [D loss: 0.649114, acc: 60.94%] [G loss: 1.870049]\n",
      "epoch:20 step:19044 [D loss: 0.643576, acc: 59.38%] [G loss: 2.001089]\n",
      "epoch:20 step:19045 [D loss: 0.639692, acc: 62.50%] [G loss: 1.811185]\n",
      "epoch:20 step:19046 [D loss: 0.662295, acc: 57.03%] [G loss: 1.829846]\n",
      "epoch:20 step:19047 [D loss: 0.649303, acc: 64.06%] [G loss: 1.962693]\n",
      "epoch:20 step:19048 [D loss: 0.629384, acc: 63.28%] [G loss: 1.930566]\n",
      "epoch:20 step:19049 [D loss: 0.653606, acc: 67.97%] [G loss: 1.874988]\n",
      "epoch:20 step:19050 [D loss: 0.649307, acc: 64.06%] [G loss: 1.727728]\n",
      "epoch:20 step:19051 [D loss: 0.647894, acc: 67.19%] [G loss: 1.881500]\n",
      "epoch:20 step:19052 [D loss: 0.563540, acc: 71.09%] [G loss: 2.040105]\n",
      "epoch:20 step:19053 [D loss: 0.642060, acc: 61.72%] [G loss: 2.022910]\n",
      "epoch:20 step:19054 [D loss: 0.637197, acc: 57.81%] [G loss: 2.013392]\n",
      "epoch:20 step:19055 [D loss: 0.644575, acc: 64.84%] [G loss: 1.978212]\n",
      "epoch:20 step:19056 [D loss: 0.681884, acc: 58.59%] [G loss: 1.746480]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19057 [D loss: 0.738883, acc: 50.78%] [G loss: 1.820238]\n",
      "epoch:20 step:19058 [D loss: 0.606828, acc: 67.19%] [G loss: 1.929383]\n",
      "epoch:20 step:19059 [D loss: 0.654031, acc: 61.72%] [G loss: 1.795829]\n",
      "epoch:20 step:19060 [D loss: 0.653136, acc: 60.16%] [G loss: 1.862483]\n",
      "epoch:20 step:19061 [D loss: 0.603378, acc: 67.97%] [G loss: 1.873795]\n",
      "epoch:20 step:19062 [D loss: 0.638088, acc: 61.72%] [G loss: 1.920577]\n",
      "epoch:20 step:19063 [D loss: 0.696348, acc: 57.03%] [G loss: 1.770423]\n",
      "epoch:20 step:19064 [D loss: 0.645626, acc: 63.28%] [G loss: 1.836649]\n",
      "epoch:20 step:19065 [D loss: 0.686590, acc: 60.94%] [G loss: 1.798592]\n",
      "epoch:20 step:19066 [D loss: 0.640960, acc: 63.28%] [G loss: 1.831686]\n",
      "epoch:20 step:19067 [D loss: 0.642765, acc: 65.62%] [G loss: 1.816852]\n",
      "epoch:20 step:19068 [D loss: 0.686300, acc: 55.47%] [G loss: 1.788084]\n",
      "epoch:20 step:19069 [D loss: 0.615836, acc: 64.84%] [G loss: 1.921980]\n",
      "epoch:20 step:19070 [D loss: 0.609941, acc: 67.19%] [G loss: 1.954420]\n",
      "epoch:20 step:19071 [D loss: 0.584106, acc: 71.88%] [G loss: 1.950810]\n",
      "epoch:20 step:19072 [D loss: 0.598169, acc: 69.53%] [G loss: 1.945174]\n",
      "epoch:20 step:19073 [D loss: 0.626593, acc: 61.72%] [G loss: 1.915155]\n",
      "epoch:20 step:19074 [D loss: 0.684272, acc: 56.25%] [G loss: 1.995127]\n",
      "epoch:20 step:19075 [D loss: 0.592859, acc: 67.97%] [G loss: 1.984539]\n",
      "epoch:20 step:19076 [D loss: 0.603785, acc: 68.75%] [G loss: 2.013006]\n",
      "epoch:20 step:19077 [D loss: 0.608678, acc: 65.62%] [G loss: 1.918066]\n",
      "epoch:20 step:19078 [D loss: 0.649850, acc: 63.28%] [G loss: 1.878015]\n",
      "epoch:20 step:19079 [D loss: 0.608270, acc: 69.53%] [G loss: 2.005026]\n",
      "epoch:20 step:19080 [D loss: 0.625324, acc: 63.28%] [G loss: 1.930688]\n",
      "epoch:20 step:19081 [D loss: 0.731698, acc: 54.69%] [G loss: 1.704912]\n",
      "epoch:20 step:19082 [D loss: 0.698449, acc: 56.25%] [G loss: 1.828186]\n",
      "epoch:20 step:19083 [D loss: 0.666228, acc: 60.94%] [G loss: 1.810741]\n",
      "epoch:20 step:19084 [D loss: 0.626819, acc: 57.81%] [G loss: 1.921529]\n",
      "epoch:20 step:19085 [D loss: 0.630961, acc: 65.62%] [G loss: 2.236432]\n",
      "epoch:20 step:19086 [D loss: 0.623308, acc: 60.94%] [G loss: 2.276471]\n",
      "epoch:20 step:19087 [D loss: 0.543970, acc: 78.91%] [G loss: 2.372920]\n",
      "epoch:20 step:19088 [D loss: 0.627042, acc: 64.84%] [G loss: 1.877741]\n",
      "epoch:20 step:19089 [D loss: 0.734063, acc: 55.47%] [G loss: 1.835279]\n",
      "epoch:20 step:19090 [D loss: 0.603072, acc: 66.41%] [G loss: 1.739665]\n",
      "epoch:20 step:19091 [D loss: 0.683352, acc: 59.38%] [G loss: 1.760090]\n",
      "epoch:20 step:19092 [D loss: 0.659223, acc: 62.50%] [G loss: 1.832519]\n",
      "epoch:20 step:19093 [D loss: 0.645148, acc: 64.06%] [G loss: 1.846507]\n",
      "epoch:20 step:19094 [D loss: 0.637112, acc: 63.28%] [G loss: 1.902290]\n",
      "epoch:20 step:19095 [D loss: 0.631966, acc: 59.38%] [G loss: 1.806651]\n",
      "epoch:20 step:19096 [D loss: 0.657973, acc: 62.50%] [G loss: 1.779547]\n",
      "epoch:20 step:19097 [D loss: 0.632151, acc: 64.06%] [G loss: 1.856709]\n",
      "epoch:20 step:19098 [D loss: 0.639465, acc: 64.06%] [G loss: 1.926224]\n",
      "epoch:20 step:19099 [D loss: 0.617299, acc: 65.62%] [G loss: 1.824748]\n",
      "epoch:20 step:19100 [D loss: 0.625402, acc: 65.62%] [G loss: 1.998175]\n",
      "epoch:20 step:19101 [D loss: 0.676776, acc: 60.16%] [G loss: 1.838083]\n",
      "epoch:20 step:19102 [D loss: 0.666214, acc: 59.38%] [G loss: 1.862554]\n",
      "epoch:20 step:19103 [D loss: 0.654515, acc: 56.25%] [G loss: 1.922014]\n",
      "epoch:20 step:19104 [D loss: 0.656507, acc: 65.62%] [G loss: 1.832068]\n",
      "epoch:20 step:19105 [D loss: 0.655661, acc: 60.94%] [G loss: 1.875742]\n",
      "epoch:20 step:19106 [D loss: 0.671899, acc: 59.38%] [G loss: 2.095717]\n",
      "epoch:20 step:19107 [D loss: 0.610311, acc: 64.84%] [G loss: 1.924071]\n",
      "epoch:20 step:19108 [D loss: 0.674597, acc: 57.81%] [G loss: 1.929146]\n",
      "epoch:20 step:19109 [D loss: 0.671867, acc: 60.94%] [G loss: 1.944935]\n",
      "epoch:20 step:19110 [D loss: 0.635575, acc: 65.62%] [G loss: 1.864231]\n",
      "epoch:20 step:19111 [D loss: 0.599458, acc: 64.84%] [G loss: 2.060097]\n",
      "epoch:20 step:19112 [D loss: 0.661298, acc: 60.94%] [G loss: 1.872467]\n",
      "epoch:20 step:19113 [D loss: 0.658764, acc: 65.62%] [G loss: 1.713353]\n",
      "epoch:20 step:19114 [D loss: 0.668135, acc: 58.59%] [G loss: 2.059603]\n",
      "epoch:20 step:19115 [D loss: 0.661637, acc: 58.59%] [G loss: 1.784940]\n",
      "epoch:20 step:19116 [D loss: 0.642294, acc: 60.94%] [G loss: 1.747171]\n",
      "epoch:20 step:19117 [D loss: 0.689001, acc: 62.50%] [G loss: 1.848192]\n",
      "epoch:20 step:19118 [D loss: 0.658977, acc: 59.38%] [G loss: 1.775203]\n",
      "epoch:20 step:19119 [D loss: 0.606657, acc: 66.41%] [G loss: 1.795552]\n",
      "epoch:20 step:19120 [D loss: 0.624266, acc: 61.72%] [G loss: 1.871238]\n",
      "epoch:20 step:19121 [D loss: 0.631654, acc: 65.62%] [G loss: 1.984204]\n",
      "epoch:20 step:19122 [D loss: 0.632338, acc: 62.50%] [G loss: 1.805608]\n",
      "epoch:20 step:19123 [D loss: 0.629386, acc: 62.50%] [G loss: 1.845995]\n",
      "epoch:20 step:19124 [D loss: 0.655080, acc: 60.94%] [G loss: 1.892220]\n",
      "epoch:20 step:19125 [D loss: 0.594780, acc: 70.31%] [G loss: 1.922833]\n",
      "epoch:20 step:19126 [D loss: 0.648698, acc: 63.28%] [G loss: 1.803001]\n",
      "epoch:20 step:19127 [D loss: 0.651442, acc: 60.94%] [G loss: 1.676768]\n",
      "epoch:20 step:19128 [D loss: 0.633965, acc: 69.53%] [G loss: 1.732673]\n",
      "epoch:20 step:19129 [D loss: 0.682547, acc: 57.81%] [G loss: 1.845332]\n",
      "epoch:20 step:19130 [D loss: 0.617959, acc: 66.41%] [G loss: 1.917790]\n",
      "epoch:20 step:19131 [D loss: 0.667215, acc: 61.72%] [G loss: 1.879405]\n",
      "epoch:20 step:19132 [D loss: 0.598164, acc: 69.53%] [G loss: 1.890259]\n",
      "epoch:20 step:19133 [D loss: 0.650707, acc: 61.72%] [G loss: 1.956892]\n",
      "epoch:20 step:19134 [D loss: 0.626822, acc: 64.84%] [G loss: 2.031331]\n",
      "epoch:20 step:19135 [D loss: 0.623264, acc: 69.53%] [G loss: 1.911603]\n",
      "epoch:20 step:19136 [D loss: 0.650641, acc: 62.50%] [G loss: 1.865144]\n",
      "epoch:20 step:19137 [D loss: 0.633558, acc: 61.72%] [G loss: 1.927287]\n",
      "epoch:20 step:19138 [D loss: 0.576889, acc: 71.88%] [G loss: 1.955980]\n",
      "epoch:20 step:19139 [D loss: 0.663332, acc: 62.50%] [G loss: 1.859286]\n",
      "epoch:20 step:19140 [D loss: 0.682469, acc: 58.59%] [G loss: 1.802618]\n",
      "epoch:20 step:19141 [D loss: 0.668415, acc: 58.59%] [G loss: 2.001240]\n",
      "epoch:20 step:19142 [D loss: 0.602079, acc: 68.75%] [G loss: 1.848323]\n",
      "epoch:20 step:19143 [D loss: 0.622646, acc: 65.62%] [G loss: 1.888184]\n",
      "epoch:20 step:19144 [D loss: 0.609935, acc: 68.75%] [G loss: 2.018704]\n",
      "epoch:20 step:19145 [D loss: 0.629812, acc: 67.19%] [G loss: 2.018882]\n",
      "epoch:20 step:19146 [D loss: 0.689336, acc: 58.59%] [G loss: 2.010078]\n",
      "epoch:20 step:19147 [D loss: 0.619844, acc: 64.84%] [G loss: 1.886119]\n",
      "epoch:20 step:19148 [D loss: 0.695812, acc: 53.91%] [G loss: 1.936210]\n",
      "epoch:20 step:19149 [D loss: 0.689099, acc: 57.81%] [G loss: 1.909003]\n",
      "epoch:20 step:19150 [D loss: 0.587022, acc: 65.62%] [G loss: 1.868020]\n",
      "epoch:20 step:19151 [D loss: 0.607643, acc: 64.06%] [G loss: 1.911537]\n",
      "epoch:20 step:19152 [D loss: 0.643510, acc: 68.75%] [G loss: 2.046186]\n",
      "epoch:20 step:19153 [D loss: 0.616099, acc: 64.06%] [G loss: 1.956045]\n",
      "epoch:20 step:19154 [D loss: 0.636575, acc: 65.62%] [G loss: 2.073313]\n",
      "epoch:20 step:19155 [D loss: 0.678220, acc: 64.84%] [G loss: 1.953211]\n",
      "epoch:20 step:19156 [D loss: 0.667782, acc: 64.06%] [G loss: 2.124544]\n",
      "epoch:20 step:19157 [D loss: 0.646776, acc: 64.06%] [G loss: 2.018266]\n",
      "epoch:20 step:19158 [D loss: 0.658361, acc: 62.50%] [G loss: 1.889104]\n",
      "epoch:20 step:19159 [D loss: 0.650127, acc: 58.59%] [G loss: 2.077536]\n",
      "epoch:20 step:19160 [D loss: 0.629689, acc: 64.84%] [G loss: 2.010638]\n",
      "epoch:20 step:19161 [D loss: 0.623906, acc: 64.06%] [G loss: 1.819040]\n",
      "epoch:20 step:19162 [D loss: 0.684943, acc: 57.81%] [G loss: 1.776393]\n",
      "epoch:20 step:19163 [D loss: 0.693788, acc: 60.16%] [G loss: 1.915100]\n",
      "epoch:20 step:19164 [D loss: 0.643019, acc: 60.16%] [G loss: 1.852857]\n",
      "epoch:20 step:19165 [D loss: 0.606126, acc: 62.50%] [G loss: 1.931683]\n",
      "epoch:20 step:19166 [D loss: 0.647341, acc: 61.72%] [G loss: 1.976676]\n",
      "epoch:20 step:19167 [D loss: 0.676688, acc: 65.62%] [G loss: 1.965230]\n",
      "epoch:20 step:19168 [D loss: 0.663652, acc: 62.50%] [G loss: 2.021898]\n",
      "epoch:20 step:19169 [D loss: 0.626556, acc: 61.72%] [G loss: 1.973573]\n",
      "epoch:20 step:19170 [D loss: 0.541723, acc: 74.22%] [G loss: 2.137019]\n",
      "epoch:20 step:19171 [D loss: 0.602489, acc: 67.97%] [G loss: 2.067630]\n",
      "epoch:20 step:19172 [D loss: 0.666023, acc: 61.72%] [G loss: 1.830612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19173 [D loss: 0.667053, acc: 60.16%] [G loss: 1.926422]\n",
      "epoch:20 step:19174 [D loss: 0.585375, acc: 74.22%] [G loss: 1.985616]\n",
      "epoch:20 step:19175 [D loss: 0.672795, acc: 62.50%] [G loss: 1.943972]\n",
      "epoch:20 step:19176 [D loss: 0.647889, acc: 66.41%] [G loss: 1.974110]\n",
      "epoch:20 step:19177 [D loss: 0.684844, acc: 55.47%] [G loss: 1.644921]\n",
      "epoch:20 step:19178 [D loss: 0.652154, acc: 60.16%] [G loss: 1.779017]\n",
      "epoch:20 step:19179 [D loss: 0.660578, acc: 54.69%] [G loss: 1.977464]\n",
      "epoch:20 step:19180 [D loss: 0.647994, acc: 60.94%] [G loss: 1.845853]\n",
      "epoch:20 step:19181 [D loss: 0.643800, acc: 62.50%] [G loss: 1.904870]\n",
      "epoch:20 step:19182 [D loss: 0.652452, acc: 58.59%] [G loss: 1.887666]\n",
      "epoch:20 step:19183 [D loss: 0.694635, acc: 53.91%] [G loss: 1.745645]\n",
      "epoch:20 step:19184 [D loss: 0.669290, acc: 60.94%] [G loss: 1.886712]\n",
      "epoch:20 step:19185 [D loss: 0.614547, acc: 68.75%] [G loss: 1.790524]\n",
      "epoch:20 step:19186 [D loss: 0.647980, acc: 60.94%] [G loss: 1.633244]\n",
      "epoch:20 step:19187 [D loss: 0.643815, acc: 64.84%] [G loss: 1.773256]\n",
      "epoch:20 step:19188 [D loss: 0.663717, acc: 64.84%] [G loss: 1.771158]\n",
      "epoch:20 step:19189 [D loss: 0.683200, acc: 58.59%] [G loss: 1.809156]\n",
      "epoch:20 step:19190 [D loss: 0.613743, acc: 67.97%] [G loss: 1.817845]\n",
      "epoch:20 step:19191 [D loss: 0.618548, acc: 69.53%] [G loss: 1.901553]\n",
      "epoch:20 step:19192 [D loss: 0.650594, acc: 66.41%] [G loss: 1.783046]\n",
      "epoch:20 step:19193 [D loss: 0.687862, acc: 55.47%] [G loss: 1.991492]\n",
      "epoch:20 step:19194 [D loss: 0.646634, acc: 57.81%] [G loss: 1.827996]\n",
      "epoch:20 step:19195 [D loss: 0.659589, acc: 60.16%] [G loss: 1.944103]\n",
      "epoch:20 step:19196 [D loss: 0.687966, acc: 57.03%] [G loss: 1.954184]\n",
      "epoch:20 step:19197 [D loss: 0.611522, acc: 62.50%] [G loss: 1.988204]\n",
      "epoch:20 step:19198 [D loss: 0.651775, acc: 60.94%] [G loss: 1.679688]\n",
      "epoch:20 step:19199 [D loss: 0.712303, acc: 53.91%] [G loss: 1.685734]\n",
      "epoch:20 step:19200 [D loss: 0.649233, acc: 60.16%] [G loss: 1.825587]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.354504\n",
      "FID: 13.639813\n",
      "0 = 12.530799430274994\n",
      "1 = 0.08262997070688695\n",
      "2 = 0.8741000294685364\n",
      "3 = 0.8838000297546387\n",
      "4 = 0.8644000291824341\n",
      "5 = 0.8669805526733398\n",
      "6 = 0.8838000297546387\n",
      "7 = 6.403953739404687\n",
      "8 = 0.07178232603881661\n",
      "9 = 0.7129999995231628\n",
      "10 = 0.7264000177383423\n",
      "11 = 0.6995999813079834\n",
      "12 = 0.7074406147003174\n",
      "13 = 0.7264000177383423\n",
      "14 = 7.354534149169922\n",
      "15 = 9.35338020324707\n",
      "16 = 0.13944654166698456\n",
      "17 = 7.354503631591797\n",
      "18 = 13.639813423156738\n",
      "epoch:20 step:19201 [D loss: 0.710288, acc: 54.69%] [G loss: 1.837345]\n",
      "epoch:20 step:19202 [D loss: 0.657830, acc: 60.94%] [G loss: 1.807454]\n",
      "epoch:20 step:19203 [D loss: 0.655997, acc: 62.50%] [G loss: 1.767880]\n",
      "epoch:20 step:19204 [D loss: 0.635602, acc: 66.41%] [G loss: 1.781014]\n",
      "epoch:20 step:19205 [D loss: 0.663317, acc: 61.72%] [G loss: 1.844657]\n",
      "epoch:20 step:19206 [D loss: 0.692463, acc: 55.47%] [G loss: 1.930100]\n",
      "epoch:20 step:19207 [D loss: 0.650020, acc: 63.28%] [G loss: 1.936561]\n",
      "epoch:20 step:19208 [D loss: 0.613453, acc: 64.84%] [G loss: 1.940006]\n",
      "epoch:20 step:19209 [D loss: 0.638743, acc: 62.50%] [G loss: 2.017087]\n",
      "epoch:20 step:19210 [D loss: 0.654791, acc: 64.06%] [G loss: 1.882046]\n",
      "epoch:20 step:19211 [D loss: 0.622224, acc: 71.09%] [G loss: 2.078384]\n",
      "epoch:20 step:19212 [D loss: 0.647013, acc: 64.06%] [G loss: 2.015001]\n",
      "epoch:20 step:19213 [D loss: 0.726261, acc: 55.47%] [G loss: 1.741204]\n",
      "epoch:20 step:19214 [D loss: 0.627423, acc: 67.19%] [G loss: 1.918102]\n",
      "epoch:20 step:19215 [D loss: 0.611782, acc: 71.88%] [G loss: 1.885442]\n",
      "epoch:20 step:19216 [D loss: 0.643389, acc: 65.62%] [G loss: 1.866405]\n",
      "epoch:20 step:19217 [D loss: 0.678209, acc: 56.25%] [G loss: 1.787465]\n",
      "epoch:20 step:19218 [D loss: 0.668102, acc: 61.72%] [G loss: 1.828393]\n",
      "epoch:20 step:19219 [D loss: 0.623818, acc: 64.84%] [G loss: 1.868995]\n",
      "epoch:20 step:19220 [D loss: 0.655318, acc: 59.38%] [G loss: 1.938941]\n",
      "epoch:20 step:19221 [D loss: 0.605835, acc: 64.84%] [G loss: 2.010544]\n",
      "epoch:20 step:19222 [D loss: 0.679401, acc: 62.50%] [G loss: 1.754951]\n",
      "epoch:20 step:19223 [D loss: 0.700543, acc: 55.47%] [G loss: 1.828046]\n",
      "epoch:20 step:19224 [D loss: 0.622339, acc: 63.28%] [G loss: 1.987275]\n",
      "epoch:20 step:19225 [D loss: 0.653898, acc: 60.16%] [G loss: 1.794903]\n",
      "epoch:20 step:19226 [D loss: 0.639067, acc: 63.28%] [G loss: 1.773026]\n",
      "epoch:20 step:19227 [D loss: 0.687695, acc: 56.25%] [G loss: 1.892021]\n",
      "epoch:20 step:19228 [D loss: 0.617481, acc: 65.62%] [G loss: 1.888376]\n",
      "epoch:20 step:19229 [D loss: 0.677839, acc: 56.25%] [G loss: 1.786729]\n",
      "epoch:20 step:19230 [D loss: 0.652632, acc: 60.94%] [G loss: 1.816392]\n",
      "epoch:20 step:19231 [D loss: 0.653730, acc: 62.50%] [G loss: 1.977658]\n",
      "epoch:20 step:19232 [D loss: 0.651053, acc: 64.06%] [G loss: 1.702942]\n",
      "epoch:20 step:19233 [D loss: 0.577973, acc: 71.09%] [G loss: 2.035011]\n",
      "epoch:20 step:19234 [D loss: 0.640074, acc: 66.41%] [G loss: 1.968073]\n",
      "epoch:20 step:19235 [D loss: 0.650301, acc: 63.28%] [G loss: 2.185744]\n",
      "epoch:20 step:19236 [D loss: 0.633551, acc: 70.31%] [G loss: 1.881786]\n",
      "epoch:20 step:19237 [D loss: 0.694199, acc: 57.81%] [G loss: 1.935737]\n",
      "epoch:20 step:19238 [D loss: 0.691339, acc: 54.69%] [G loss: 1.926079]\n",
      "epoch:20 step:19239 [D loss: 0.612919, acc: 66.41%] [G loss: 1.962750]\n",
      "epoch:20 step:19240 [D loss: 0.643539, acc: 61.72%] [G loss: 1.615280]\n",
      "epoch:20 step:19241 [D loss: 0.754286, acc: 50.00%] [G loss: 1.657543]\n",
      "epoch:20 step:19242 [D loss: 0.706558, acc: 54.69%] [G loss: 1.651146]\n",
      "epoch:20 step:19243 [D loss: 0.651299, acc: 57.81%] [G loss: 1.870055]\n",
      "epoch:20 step:19244 [D loss: 0.636506, acc: 64.84%] [G loss: 2.024700]\n",
      "epoch:20 step:19245 [D loss: 0.677904, acc: 56.25%] [G loss: 1.803055]\n",
      "epoch:20 step:19246 [D loss: 0.628757, acc: 65.62%] [G loss: 1.769885]\n",
      "epoch:20 step:19247 [D loss: 0.657786, acc: 60.94%] [G loss: 1.889093]\n",
      "epoch:20 step:19248 [D loss: 0.645524, acc: 66.41%] [G loss: 2.022939]\n",
      "epoch:20 step:19249 [D loss: 0.667120, acc: 58.59%] [G loss: 1.816529]\n",
      "epoch:20 step:19250 [D loss: 0.637178, acc: 63.28%] [G loss: 1.853084]\n",
      "epoch:20 step:19251 [D loss: 0.669196, acc: 59.38%] [G loss: 1.921494]\n",
      "epoch:20 step:19252 [D loss: 0.714053, acc: 56.25%] [G loss: 1.857659]\n",
      "epoch:20 step:19253 [D loss: 0.657548, acc: 54.69%] [G loss: 1.811213]\n",
      "epoch:20 step:19254 [D loss: 0.626880, acc: 67.19%] [G loss: 1.837430]\n",
      "epoch:20 step:19255 [D loss: 0.621335, acc: 64.84%] [G loss: 1.869613]\n",
      "epoch:20 step:19256 [D loss: 0.655304, acc: 63.28%] [G loss: 2.028958]\n",
      "epoch:20 step:19257 [D loss: 0.662266, acc: 60.16%] [G loss: 1.919330]\n",
      "epoch:20 step:19258 [D loss: 0.670763, acc: 61.72%] [G loss: 1.749913]\n",
      "epoch:20 step:19259 [D loss: 0.609996, acc: 63.28%] [G loss: 1.901672]\n",
      "epoch:20 step:19260 [D loss: 0.687173, acc: 56.25%] [G loss: 1.870499]\n",
      "epoch:20 step:19261 [D loss: 0.628346, acc: 65.62%] [G loss: 1.892997]\n",
      "epoch:20 step:19262 [D loss: 0.676244, acc: 54.69%] [G loss: 1.939343]\n",
      "epoch:20 step:19263 [D loss: 0.625949, acc: 67.97%] [G loss: 1.869916]\n",
      "epoch:20 step:19264 [D loss: 0.635792, acc: 66.41%] [G loss: 1.949919]\n",
      "epoch:20 step:19265 [D loss: 0.647090, acc: 64.84%] [G loss: 1.875510]\n",
      "epoch:20 step:19266 [D loss: 0.640744, acc: 65.62%] [G loss: 1.878302]\n",
      "epoch:20 step:19267 [D loss: 0.639711, acc: 64.06%] [G loss: 1.973791]\n",
      "epoch:20 step:19268 [D loss: 0.669381, acc: 61.72%] [G loss: 1.736650]\n",
      "epoch:20 step:19269 [D loss: 0.726463, acc: 47.66%] [G loss: 1.679829]\n",
      "epoch:20 step:19270 [D loss: 0.688825, acc: 55.47%] [G loss: 1.601209]\n",
      "epoch:20 step:19271 [D loss: 0.658377, acc: 63.28%] [G loss: 1.791069]\n",
      "epoch:20 step:19272 [D loss: 0.595341, acc: 71.09%] [G loss: 1.794118]\n",
      "epoch:20 step:19273 [D loss: 0.666816, acc: 58.59%] [G loss: 1.854013]\n",
      "epoch:20 step:19274 [D loss: 0.586431, acc: 71.88%] [G loss: 1.960352]\n",
      "epoch:20 step:19275 [D loss: 0.638316, acc: 65.62%] [G loss: 1.804872]\n",
      "epoch:20 step:19276 [D loss: 0.589232, acc: 70.31%] [G loss: 1.903961]\n",
      "epoch:20 step:19277 [D loss: 0.700482, acc: 56.25%] [G loss: 1.876501]\n",
      "epoch:20 step:19278 [D loss: 0.695886, acc: 52.34%] [G loss: 1.726189]\n",
      "epoch:20 step:19279 [D loss: 0.648770, acc: 59.38%] [G loss: 1.809876]\n",
      "epoch:20 step:19280 [D loss: 0.662492, acc: 67.19%] [G loss: 1.842393]\n",
      "epoch:20 step:19281 [D loss: 0.610718, acc: 71.09%] [G loss: 1.896806]\n",
      "epoch:20 step:19282 [D loss: 0.694897, acc: 56.25%] [G loss: 1.782089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19283 [D loss: 0.654813, acc: 58.59%] [G loss: 1.926198]\n",
      "epoch:20 step:19284 [D loss: 0.632506, acc: 64.06%] [G loss: 1.891319]\n",
      "epoch:20 step:19285 [D loss: 0.644915, acc: 64.84%] [G loss: 2.080560]\n",
      "epoch:20 step:19286 [D loss: 0.653680, acc: 60.16%] [G loss: 1.892469]\n",
      "epoch:20 step:19287 [D loss: 0.642170, acc: 58.59%] [G loss: 1.983626]\n",
      "epoch:20 step:19288 [D loss: 0.675517, acc: 60.16%] [G loss: 1.834943]\n",
      "epoch:20 step:19289 [D loss: 0.595224, acc: 67.19%] [G loss: 1.958514]\n",
      "epoch:20 step:19290 [D loss: 0.645277, acc: 64.84%] [G loss: 1.967444]\n",
      "epoch:20 step:19291 [D loss: 0.645995, acc: 62.50%] [G loss: 2.106324]\n",
      "epoch:20 step:19292 [D loss: 0.622386, acc: 65.62%] [G loss: 1.964626]\n",
      "epoch:20 step:19293 [D loss: 0.689493, acc: 56.25%] [G loss: 1.824786]\n",
      "epoch:20 step:19294 [D loss: 0.627496, acc: 64.84%] [G loss: 1.956275]\n",
      "epoch:20 step:19295 [D loss: 0.590201, acc: 67.19%] [G loss: 1.965568]\n",
      "epoch:20 step:19296 [D loss: 0.624875, acc: 66.41%] [G loss: 2.020197]\n",
      "epoch:20 step:19297 [D loss: 0.639829, acc: 64.06%] [G loss: 2.043163]\n",
      "epoch:20 step:19298 [D loss: 0.639294, acc: 66.41%] [G loss: 1.911347]\n",
      "epoch:20 step:19299 [D loss: 0.684097, acc: 51.56%] [G loss: 1.825677]\n",
      "epoch:20 step:19300 [D loss: 0.664186, acc: 60.94%] [G loss: 1.882947]\n",
      "epoch:20 step:19301 [D loss: 0.655541, acc: 64.84%] [G loss: 1.864828]\n",
      "epoch:20 step:19302 [D loss: 0.625795, acc: 60.94%] [G loss: 1.949086]\n",
      "epoch:20 step:19303 [D loss: 0.685772, acc: 53.91%] [G loss: 1.861543]\n",
      "epoch:20 step:19304 [D loss: 0.648083, acc: 64.84%] [G loss: 1.993291]\n",
      "epoch:20 step:19305 [D loss: 0.675776, acc: 58.59%] [G loss: 1.803400]\n",
      "epoch:20 step:19306 [D loss: 0.716582, acc: 52.34%] [G loss: 1.756883]\n",
      "epoch:20 step:19307 [D loss: 0.653468, acc: 59.38%] [G loss: 1.819918]\n",
      "epoch:20 step:19308 [D loss: 0.663803, acc: 64.06%] [G loss: 1.808026]\n",
      "epoch:20 step:19309 [D loss: 0.694409, acc: 53.91%] [G loss: 1.658554]\n",
      "epoch:20 step:19310 [D loss: 0.621999, acc: 64.06%] [G loss: 1.798960]\n",
      "epoch:20 step:19311 [D loss: 0.685188, acc: 58.59%] [G loss: 1.769745]\n",
      "epoch:20 step:19312 [D loss: 0.680835, acc: 59.38%] [G loss: 1.805510]\n",
      "epoch:20 step:19313 [D loss: 0.669707, acc: 58.59%] [G loss: 1.692345]\n",
      "epoch:20 step:19314 [D loss: 0.620170, acc: 65.62%] [G loss: 1.969758]\n",
      "epoch:20 step:19315 [D loss: 0.634036, acc: 64.06%] [G loss: 1.773823]\n",
      "epoch:20 step:19316 [D loss: 0.669806, acc: 60.94%] [G loss: 1.747178]\n",
      "epoch:20 step:19317 [D loss: 0.663422, acc: 60.16%] [G loss: 1.759802]\n",
      "epoch:20 step:19318 [D loss: 0.622904, acc: 64.84%] [G loss: 1.859791]\n",
      "epoch:20 step:19319 [D loss: 0.668828, acc: 55.47%] [G loss: 1.828070]\n",
      "epoch:20 step:19320 [D loss: 0.706224, acc: 54.69%] [G loss: 1.834962]\n",
      "epoch:20 step:19321 [D loss: 0.697781, acc: 57.81%] [G loss: 1.722020]\n",
      "epoch:20 step:19322 [D loss: 0.642848, acc: 62.50%] [G loss: 1.855856]\n",
      "epoch:20 step:19323 [D loss: 0.685125, acc: 60.94%] [G loss: 1.844371]\n",
      "epoch:20 step:19324 [D loss: 0.640448, acc: 62.50%] [G loss: 1.693009]\n",
      "epoch:20 step:19325 [D loss: 0.621080, acc: 65.62%] [G loss: 1.823175]\n",
      "epoch:20 step:19326 [D loss: 0.616356, acc: 64.84%] [G loss: 1.851023]\n",
      "epoch:20 step:19327 [D loss: 0.656201, acc: 60.16%] [G loss: 1.886459]\n",
      "epoch:20 step:19328 [D loss: 0.677516, acc: 53.12%] [G loss: 1.916656]\n",
      "epoch:20 step:19329 [D loss: 0.598989, acc: 67.97%] [G loss: 1.938265]\n",
      "epoch:20 step:19330 [D loss: 0.645991, acc: 56.25%] [G loss: 1.726835]\n",
      "epoch:20 step:19331 [D loss: 0.653770, acc: 57.03%] [G loss: 1.943689]\n",
      "epoch:20 step:19332 [D loss: 0.617745, acc: 64.84%] [G loss: 1.821092]\n",
      "epoch:20 step:19333 [D loss: 0.646148, acc: 60.94%] [G loss: 1.860107]\n",
      "epoch:20 step:19334 [D loss: 0.693734, acc: 57.81%] [G loss: 1.899727]\n",
      "epoch:20 step:19335 [D loss: 0.678797, acc: 60.16%] [G loss: 1.825564]\n",
      "epoch:20 step:19336 [D loss: 0.623056, acc: 70.31%] [G loss: 1.878881]\n",
      "epoch:20 step:19337 [D loss: 0.684337, acc: 56.25%] [G loss: 1.927249]\n",
      "epoch:20 step:19338 [D loss: 0.664369, acc: 58.59%] [G loss: 1.927180]\n",
      "epoch:20 step:19339 [D loss: 0.612651, acc: 67.97%] [G loss: 1.743900]\n",
      "epoch:20 step:19340 [D loss: 0.655493, acc: 59.38%] [G loss: 1.964866]\n",
      "epoch:20 step:19341 [D loss: 0.661718, acc: 63.28%] [G loss: 1.837959]\n",
      "epoch:20 step:19342 [D loss: 0.677316, acc: 57.03%] [G loss: 1.812935]\n",
      "epoch:20 step:19343 [D loss: 0.641939, acc: 67.97%] [G loss: 1.896292]\n",
      "epoch:20 step:19344 [D loss: 0.646344, acc: 60.16%] [G loss: 1.880688]\n",
      "epoch:20 step:19345 [D loss: 0.666613, acc: 60.94%] [G loss: 1.960937]\n",
      "epoch:20 step:19346 [D loss: 0.660228, acc: 64.84%] [G loss: 1.795449]\n",
      "epoch:20 step:19347 [D loss: 0.636678, acc: 57.81%] [G loss: 1.777089]\n",
      "epoch:20 step:19348 [D loss: 0.631516, acc: 60.16%] [G loss: 1.901965]\n",
      "epoch:20 step:19349 [D loss: 0.624981, acc: 60.16%] [G loss: 1.865877]\n",
      "epoch:20 step:19350 [D loss: 0.645879, acc: 60.16%] [G loss: 1.960860]\n",
      "epoch:20 step:19351 [D loss: 0.659527, acc: 64.06%] [G loss: 1.734983]\n",
      "epoch:20 step:19352 [D loss: 0.685221, acc: 57.03%] [G loss: 1.830150]\n",
      "epoch:20 step:19353 [D loss: 0.622138, acc: 63.28%] [G loss: 1.857458]\n",
      "epoch:20 step:19354 [D loss: 0.624008, acc: 62.50%] [G loss: 1.757134]\n",
      "epoch:20 step:19355 [D loss: 0.679679, acc: 61.72%] [G loss: 1.786304]\n",
      "epoch:20 step:19356 [D loss: 0.672458, acc: 60.94%] [G loss: 1.885751]\n",
      "epoch:20 step:19357 [D loss: 0.642461, acc: 67.97%] [G loss: 1.906534]\n",
      "epoch:20 step:19358 [D loss: 0.647817, acc: 62.50%] [G loss: 1.842837]\n",
      "epoch:20 step:19359 [D loss: 0.650689, acc: 60.16%] [G loss: 1.872457]\n",
      "epoch:20 step:19360 [D loss: 0.619217, acc: 65.62%] [G loss: 1.853334]\n",
      "epoch:20 step:19361 [D loss: 0.608773, acc: 67.97%] [G loss: 1.778436]\n",
      "epoch:20 step:19362 [D loss: 0.615237, acc: 66.41%] [G loss: 2.013713]\n",
      "epoch:20 step:19363 [D loss: 0.630295, acc: 62.50%] [G loss: 1.901777]\n",
      "epoch:20 step:19364 [D loss: 0.674705, acc: 55.47%] [G loss: 1.951268]\n",
      "epoch:20 step:19365 [D loss: 0.666248, acc: 60.94%] [G loss: 1.928067]\n",
      "epoch:20 step:19366 [D loss: 0.621366, acc: 64.06%] [G loss: 1.782280]\n",
      "epoch:20 step:19367 [D loss: 0.628457, acc: 67.19%] [G loss: 1.880360]\n",
      "epoch:20 step:19368 [D loss: 0.664714, acc: 57.03%] [G loss: 1.877270]\n",
      "epoch:20 step:19369 [D loss: 0.600635, acc: 66.41%] [G loss: 1.880707]\n",
      "epoch:20 step:19370 [D loss: 0.635017, acc: 60.16%] [G loss: 1.853974]\n",
      "epoch:20 step:19371 [D loss: 0.612238, acc: 65.62%] [G loss: 1.977679]\n",
      "epoch:20 step:19372 [D loss: 0.619316, acc: 67.97%] [G loss: 1.933565]\n",
      "epoch:20 step:19373 [D loss: 0.645597, acc: 64.84%] [G loss: 1.960692]\n",
      "epoch:20 step:19374 [D loss: 0.626094, acc: 65.62%] [G loss: 2.000596]\n",
      "epoch:20 step:19375 [D loss: 0.588631, acc: 68.75%] [G loss: 1.996169]\n",
      "epoch:20 step:19376 [D loss: 0.617145, acc: 67.97%] [G loss: 2.040200]\n",
      "epoch:20 step:19377 [D loss: 0.617045, acc: 64.06%] [G loss: 2.077545]\n",
      "epoch:20 step:19378 [D loss: 0.620684, acc: 65.62%] [G loss: 1.973029]\n",
      "epoch:20 step:19379 [D loss: 0.709322, acc: 54.69%] [G loss: 1.899854]\n",
      "epoch:20 step:19380 [D loss: 0.665714, acc: 57.81%] [G loss: 1.949204]\n",
      "epoch:20 step:19381 [D loss: 0.654061, acc: 63.28%] [G loss: 2.033754]\n",
      "epoch:20 step:19382 [D loss: 0.651696, acc: 57.03%] [G loss: 1.947909]\n",
      "epoch:20 step:19383 [D loss: 0.608145, acc: 70.31%] [G loss: 2.019545]\n",
      "epoch:20 step:19384 [D loss: 0.633843, acc: 60.16%] [G loss: 1.905061]\n",
      "epoch:20 step:19385 [D loss: 0.625970, acc: 64.84%] [G loss: 1.945630]\n",
      "epoch:20 step:19386 [D loss: 0.637007, acc: 64.06%] [G loss: 2.085036]\n",
      "epoch:20 step:19387 [D loss: 0.654380, acc: 65.62%] [G loss: 1.999560]\n",
      "epoch:20 step:19388 [D loss: 0.614329, acc: 65.62%] [G loss: 2.389516]\n",
      "epoch:20 step:19389 [D loss: 0.587862, acc: 69.53%] [G loss: 2.103790]\n",
      "epoch:20 step:19390 [D loss: 0.678934, acc: 62.50%] [G loss: 2.019683]\n",
      "epoch:20 step:19391 [D loss: 0.661349, acc: 60.16%] [G loss: 1.824959]\n",
      "epoch:20 step:19392 [D loss: 0.662827, acc: 62.50%] [G loss: 1.977821]\n",
      "epoch:20 step:19393 [D loss: 0.600492, acc: 67.19%] [G loss: 2.011557]\n",
      "epoch:20 step:19394 [D loss: 0.578607, acc: 71.09%] [G loss: 2.012160]\n",
      "epoch:20 step:19395 [D loss: 0.651139, acc: 62.50%] [G loss: 1.901487]\n",
      "epoch:20 step:19396 [D loss: 0.672102, acc: 64.06%] [G loss: 1.828246]\n",
      "epoch:20 step:19397 [D loss: 0.654013, acc: 57.03%] [G loss: 1.682540]\n",
      "epoch:20 step:19398 [D loss: 0.659392, acc: 61.72%] [G loss: 1.780387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19399 [D loss: 0.658159, acc: 64.84%] [G loss: 1.849489]\n",
      "epoch:20 step:19400 [D loss: 0.628751, acc: 66.41%] [G loss: 1.889077]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.231228\n",
      "FID: 15.420978\n",
      "0 = 12.733929429292735\n",
      "1 = 0.08977140748312608\n",
      "2 = 0.8884000182151794\n",
      "3 = 0.894599974155426\n",
      "4 = 0.8822000026702881\n",
      "5 = 0.883642852306366\n",
      "6 = 0.894599974155426\n",
      "7 = 6.594031513774381\n",
      "8 = 0.07730511557762665\n",
      "9 = 0.7239999771118164\n",
      "10 = 0.7342000007629395\n",
      "11 = 0.7138000130653381\n",
      "12 = 0.7195217609405518\n",
      "13 = 0.7342000007629395\n",
      "14 = 7.231256008148193\n",
      "15 = 9.420902252197266\n",
      "16 = 0.12120866030454636\n",
      "17 = 7.231227874755859\n",
      "18 = 15.420977592468262\n",
      "epoch:20 step:19401 [D loss: 0.632173, acc: 61.72%] [G loss: 1.819880]\n",
      "epoch:20 step:19402 [D loss: 0.688251, acc: 58.59%] [G loss: 1.949981]\n",
      "epoch:20 step:19403 [D loss: 0.657645, acc: 57.81%] [G loss: 1.922034]\n",
      "epoch:20 step:19404 [D loss: 0.656296, acc: 60.94%] [G loss: 1.741796]\n",
      "epoch:20 step:19405 [D loss: 0.657519, acc: 62.50%] [G loss: 1.901663]\n",
      "epoch:20 step:19406 [D loss: 0.702252, acc: 57.03%] [G loss: 1.697093]\n",
      "epoch:20 step:19407 [D loss: 0.696663, acc: 57.81%] [G loss: 1.705986]\n",
      "epoch:20 step:19408 [D loss: 0.662720, acc: 57.03%] [G loss: 1.830261]\n",
      "epoch:20 step:19409 [D loss: 0.639131, acc: 64.84%] [G loss: 1.715347]\n",
      "epoch:20 step:19410 [D loss: 0.629685, acc: 64.84%] [G loss: 1.671019]\n",
      "epoch:20 step:19411 [D loss: 0.692232, acc: 60.16%] [G loss: 1.793067]\n",
      "epoch:20 step:19412 [D loss: 0.658840, acc: 60.94%] [G loss: 1.835435]\n",
      "epoch:20 step:19413 [D loss: 0.666015, acc: 60.94%] [G loss: 1.822654]\n",
      "epoch:20 step:19414 [D loss: 0.631542, acc: 62.50%] [G loss: 1.881476]\n",
      "epoch:20 step:19415 [D loss: 0.713275, acc: 56.25%] [G loss: 1.748595]\n",
      "epoch:20 step:19416 [D loss: 0.676444, acc: 57.03%] [G loss: 1.813983]\n",
      "epoch:20 step:19417 [D loss: 0.591525, acc: 71.09%] [G loss: 1.923664]\n",
      "epoch:20 step:19418 [D loss: 0.665607, acc: 57.03%] [G loss: 1.880894]\n",
      "epoch:20 step:19419 [D loss: 0.607193, acc: 65.62%] [G loss: 1.872844]\n",
      "epoch:20 step:19420 [D loss: 0.670305, acc: 58.59%] [G loss: 1.791438]\n",
      "epoch:20 step:19421 [D loss: 0.651296, acc: 63.28%] [G loss: 1.987149]\n",
      "epoch:20 step:19422 [D loss: 0.678294, acc: 64.06%] [G loss: 1.682015]\n",
      "epoch:20 step:19423 [D loss: 0.631593, acc: 64.06%] [G loss: 1.739161]\n",
      "epoch:20 step:19424 [D loss: 0.682008, acc: 58.59%] [G loss: 1.786033]\n",
      "epoch:20 step:19425 [D loss: 0.666723, acc: 60.94%] [G loss: 1.821454]\n",
      "epoch:20 step:19426 [D loss: 0.615684, acc: 63.28%] [G loss: 1.878931]\n",
      "epoch:20 step:19427 [D loss: 0.644609, acc: 64.06%] [G loss: 1.950747]\n",
      "epoch:20 step:19428 [D loss: 0.655165, acc: 60.16%] [G loss: 1.979500]\n",
      "epoch:20 step:19429 [D loss: 0.649936, acc: 63.28%] [G loss: 1.839120]\n",
      "epoch:20 step:19430 [D loss: 0.624992, acc: 61.72%] [G loss: 2.043648]\n",
      "epoch:20 step:19431 [D loss: 0.591149, acc: 68.75%] [G loss: 2.117763]\n",
      "epoch:20 step:19432 [D loss: 0.633027, acc: 61.72%] [G loss: 1.949776]\n",
      "epoch:20 step:19433 [D loss: 0.580119, acc: 71.09%] [G loss: 1.987095]\n",
      "epoch:20 step:19434 [D loss: 0.590245, acc: 68.75%] [G loss: 2.143975]\n",
      "epoch:20 step:19435 [D loss: 0.670153, acc: 60.94%] [G loss: 2.057597]\n",
      "epoch:20 step:19436 [D loss: 0.677032, acc: 59.38%] [G loss: 1.805874]\n",
      "epoch:20 step:19437 [D loss: 0.624021, acc: 63.28%] [G loss: 1.940955]\n",
      "epoch:20 step:19438 [D loss: 0.651808, acc: 56.25%] [G loss: 1.830514]\n",
      "epoch:20 step:19439 [D loss: 0.642428, acc: 63.28%] [G loss: 2.045209]\n",
      "epoch:20 step:19440 [D loss: 0.676792, acc: 57.81%] [G loss: 1.980102]\n",
      "epoch:20 step:19441 [D loss: 0.619078, acc: 67.19%] [G loss: 2.052404]\n",
      "epoch:20 step:19442 [D loss: 0.655442, acc: 57.81%] [G loss: 1.813641]\n",
      "epoch:20 step:19443 [D loss: 0.682614, acc: 61.72%] [G loss: 1.698693]\n",
      "epoch:20 step:19444 [D loss: 0.684068, acc: 60.94%] [G loss: 1.777841]\n",
      "epoch:20 step:19445 [D loss: 0.685881, acc: 58.59%] [G loss: 1.777580]\n",
      "epoch:20 step:19446 [D loss: 0.675433, acc: 59.38%] [G loss: 1.833776]\n",
      "epoch:20 step:19447 [D loss: 0.644548, acc: 64.06%] [G loss: 1.970565]\n",
      "epoch:20 step:19448 [D loss: 0.573781, acc: 68.75%] [G loss: 2.074313]\n",
      "epoch:20 step:19449 [D loss: 0.656645, acc: 58.59%] [G loss: 1.784191]\n",
      "epoch:20 step:19450 [D loss: 0.692638, acc: 59.38%] [G loss: 1.819160]\n",
      "epoch:20 step:19451 [D loss: 0.613572, acc: 68.75%] [G loss: 1.925697]\n",
      "epoch:20 step:19452 [D loss: 0.655617, acc: 60.16%] [G loss: 1.969609]\n",
      "epoch:20 step:19453 [D loss: 0.636299, acc: 69.53%] [G loss: 1.875759]\n",
      "epoch:20 step:19454 [D loss: 0.616982, acc: 67.19%] [G loss: 1.918198]\n",
      "epoch:20 step:19455 [D loss: 0.628180, acc: 63.28%] [G loss: 2.013946]\n",
      "epoch:20 step:19456 [D loss: 0.676944, acc: 60.16%] [G loss: 1.862829]\n",
      "epoch:20 step:19457 [D loss: 0.643416, acc: 62.50%] [G loss: 1.918108]\n",
      "epoch:20 step:19458 [D loss: 0.615785, acc: 67.19%] [G loss: 1.816210]\n",
      "epoch:20 step:19459 [D loss: 0.596888, acc: 67.19%] [G loss: 2.149379]\n",
      "epoch:20 step:19460 [D loss: 0.628973, acc: 65.62%] [G loss: 1.904993]\n",
      "epoch:20 step:19461 [D loss: 0.629004, acc: 64.84%] [G loss: 2.034862]\n",
      "epoch:20 step:19462 [D loss: 0.671380, acc: 55.47%] [G loss: 1.830904]\n",
      "epoch:20 step:19463 [D loss: 0.626007, acc: 67.19%] [G loss: 1.834498]\n",
      "epoch:20 step:19464 [D loss: 0.666923, acc: 57.81%] [G loss: 1.889302]\n",
      "epoch:20 step:19465 [D loss: 0.607687, acc: 64.84%] [G loss: 1.945042]\n",
      "epoch:20 step:19466 [D loss: 0.607023, acc: 64.06%] [G loss: 1.937511]\n",
      "epoch:20 step:19467 [D loss: 0.682863, acc: 53.91%] [G loss: 1.895638]\n",
      "epoch:20 step:19468 [D loss: 0.681727, acc: 65.62%] [G loss: 2.043202]\n",
      "epoch:20 step:19469 [D loss: 0.687642, acc: 60.16%] [G loss: 1.717812]\n",
      "epoch:20 step:19470 [D loss: 0.674536, acc: 57.03%] [G loss: 1.863051]\n",
      "epoch:20 step:19471 [D loss: 0.686347, acc: 57.81%] [G loss: 1.868445]\n",
      "epoch:20 step:19472 [D loss: 0.647267, acc: 59.38%] [G loss: 1.897159]\n",
      "epoch:20 step:19473 [D loss: 0.617329, acc: 63.28%] [G loss: 1.946550]\n",
      "epoch:20 step:19474 [D loss: 0.686352, acc: 62.50%] [G loss: 1.773293]\n",
      "epoch:20 step:19475 [D loss: 0.667005, acc: 60.16%] [G loss: 1.862961]\n",
      "epoch:20 step:19476 [D loss: 0.591935, acc: 67.97%] [G loss: 1.869061]\n",
      "epoch:20 step:19477 [D loss: 0.626521, acc: 64.84%] [G loss: 1.897501]\n",
      "epoch:20 step:19478 [D loss: 0.651691, acc: 57.03%] [G loss: 1.901389]\n",
      "epoch:20 step:19479 [D loss: 0.657675, acc: 67.97%] [G loss: 1.755005]\n",
      "epoch:20 step:19480 [D loss: 0.665997, acc: 63.28%] [G loss: 1.896517]\n",
      "epoch:20 step:19481 [D loss: 0.652203, acc: 62.50%] [G loss: 1.899054]\n",
      "epoch:20 step:19482 [D loss: 0.622632, acc: 64.06%] [G loss: 1.792096]\n",
      "epoch:20 step:19483 [D loss: 0.624668, acc: 64.84%] [G loss: 1.900554]\n",
      "epoch:20 step:19484 [D loss: 0.654642, acc: 56.25%] [G loss: 1.892417]\n",
      "epoch:20 step:19485 [D loss: 0.628544, acc: 64.06%] [G loss: 1.962484]\n",
      "epoch:20 step:19486 [D loss: 0.644365, acc: 62.50%] [G loss: 2.059786]\n",
      "epoch:20 step:19487 [D loss: 0.561207, acc: 73.44%] [G loss: 1.879883]\n",
      "epoch:20 step:19488 [D loss: 0.638143, acc: 61.72%] [G loss: 1.869571]\n",
      "epoch:20 step:19489 [D loss: 0.649000, acc: 65.62%] [G loss: 1.775281]\n",
      "epoch:20 step:19490 [D loss: 0.671181, acc: 57.03%] [G loss: 1.875591]\n",
      "epoch:20 step:19491 [D loss: 0.658535, acc: 61.72%] [G loss: 1.791276]\n",
      "epoch:20 step:19492 [D loss: 0.631213, acc: 66.41%] [G loss: 1.790006]\n",
      "epoch:20 step:19493 [D loss: 0.631142, acc: 65.62%] [G loss: 1.830118]\n",
      "epoch:20 step:19494 [D loss: 0.610212, acc: 64.06%] [G loss: 2.045132]\n",
      "epoch:20 step:19495 [D loss: 0.647418, acc: 64.84%] [G loss: 1.929444]\n",
      "epoch:20 step:19496 [D loss: 0.653026, acc: 67.97%] [G loss: 1.959839]\n",
      "epoch:20 step:19497 [D loss: 0.654799, acc: 66.41%] [G loss: 1.757870]\n",
      "epoch:20 step:19498 [D loss: 0.641051, acc: 61.72%] [G loss: 1.836287]\n",
      "epoch:20 step:19499 [D loss: 0.680884, acc: 60.94%] [G loss: 1.681568]\n",
      "epoch:20 step:19500 [D loss: 0.675815, acc: 55.47%] [G loss: 1.815814]\n",
      "epoch:20 step:19501 [D loss: 0.669316, acc: 63.28%] [G loss: 1.954460]\n",
      "epoch:20 step:19502 [D loss: 0.624962, acc: 63.28%] [G loss: 1.894711]\n",
      "epoch:20 step:19503 [D loss: 0.611520, acc: 63.28%] [G loss: 1.940385]\n",
      "epoch:20 step:19504 [D loss: 0.725578, acc: 51.56%] [G loss: 1.889497]\n",
      "epoch:20 step:19505 [D loss: 0.691066, acc: 51.56%] [G loss: 1.809372]\n",
      "epoch:20 step:19506 [D loss: 0.679924, acc: 60.94%] [G loss: 1.713774]\n",
      "epoch:20 step:19507 [D loss: 0.654724, acc: 60.16%] [G loss: 1.834324]\n",
      "epoch:20 step:19508 [D loss: 0.708191, acc: 54.69%] [G loss: 1.767439]\n",
      "epoch:20 step:19509 [D loss: 0.680975, acc: 65.62%] [G loss: 1.806201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19510 [D loss: 0.632501, acc: 66.41%] [G loss: 1.863422]\n",
      "epoch:20 step:19511 [D loss: 0.684084, acc: 57.81%] [G loss: 1.747004]\n",
      "epoch:20 step:19512 [D loss: 0.658994, acc: 58.59%] [G loss: 1.875348]\n",
      "epoch:20 step:19513 [D loss: 0.671560, acc: 61.72%] [G loss: 1.824398]\n",
      "epoch:20 step:19514 [D loss: 0.628510, acc: 67.19%] [G loss: 1.954161]\n",
      "epoch:20 step:19515 [D loss: 0.636644, acc: 63.28%] [G loss: 1.842963]\n",
      "epoch:20 step:19516 [D loss: 0.634180, acc: 61.72%] [G loss: 1.815020]\n",
      "epoch:20 step:19517 [D loss: 0.623494, acc: 66.41%] [G loss: 1.786439]\n",
      "epoch:20 step:19518 [D loss: 0.619972, acc: 60.94%] [G loss: 1.812991]\n",
      "epoch:20 step:19519 [D loss: 0.691058, acc: 50.78%] [G loss: 1.763484]\n",
      "epoch:20 step:19520 [D loss: 0.589383, acc: 69.53%] [G loss: 2.104292]\n",
      "epoch:20 step:19521 [D loss: 0.621203, acc: 68.75%] [G loss: 2.091761]\n",
      "epoch:20 step:19522 [D loss: 0.595891, acc: 65.62%] [G loss: 2.146112]\n",
      "epoch:20 step:19523 [D loss: 0.655466, acc: 55.47%] [G loss: 1.843318]\n",
      "epoch:20 step:19524 [D loss: 0.663398, acc: 56.25%] [G loss: 1.722261]\n",
      "epoch:20 step:19525 [D loss: 0.660366, acc: 60.16%] [G loss: 1.904449]\n",
      "epoch:20 step:19526 [D loss: 0.647455, acc: 60.16%] [G loss: 1.976980]\n",
      "epoch:20 step:19527 [D loss: 0.738930, acc: 50.78%] [G loss: 1.933341]\n",
      "epoch:20 step:19528 [D loss: 0.710129, acc: 57.03%] [G loss: 1.836018]\n",
      "epoch:20 step:19529 [D loss: 0.637978, acc: 61.72%] [G loss: 1.846508]\n",
      "epoch:20 step:19530 [D loss: 0.614471, acc: 67.97%] [G loss: 1.874008]\n",
      "epoch:20 step:19531 [D loss: 0.627929, acc: 67.97%] [G loss: 1.918403]\n",
      "epoch:20 step:19532 [D loss: 0.674803, acc: 65.62%] [G loss: 1.909456]\n",
      "epoch:20 step:19533 [D loss: 0.647226, acc: 62.50%] [G loss: 1.820138]\n",
      "epoch:20 step:19534 [D loss: 0.702795, acc: 52.34%] [G loss: 1.834841]\n",
      "epoch:20 step:19535 [D loss: 0.663091, acc: 61.72%] [G loss: 1.784504]\n",
      "epoch:20 step:19536 [D loss: 0.666332, acc: 57.81%] [G loss: 1.800390]\n",
      "epoch:20 step:19537 [D loss: 0.667226, acc: 60.94%] [G loss: 1.770788]\n",
      "epoch:20 step:19538 [D loss: 0.719024, acc: 55.47%] [G loss: 1.823142]\n",
      "epoch:20 step:19539 [D loss: 0.740210, acc: 50.00%] [G loss: 1.839483]\n",
      "epoch:20 step:19540 [D loss: 0.708274, acc: 50.78%] [G loss: 1.803632]\n",
      "epoch:20 step:19541 [D loss: 0.686105, acc: 57.81%] [G loss: 1.835198]\n",
      "epoch:20 step:19542 [D loss: 0.675773, acc: 57.03%] [G loss: 1.779162]\n",
      "epoch:20 step:19543 [D loss: 0.678618, acc: 53.91%] [G loss: 1.794873]\n",
      "epoch:20 step:19544 [D loss: 0.645009, acc: 62.50%] [G loss: 1.823546]\n",
      "epoch:20 step:19545 [D loss: 0.650081, acc: 57.03%] [G loss: 1.767281]\n",
      "epoch:20 step:19546 [D loss: 0.655396, acc: 59.38%] [G loss: 1.843985]\n",
      "epoch:20 step:19547 [D loss: 0.664996, acc: 60.94%] [G loss: 1.890406]\n",
      "epoch:20 step:19548 [D loss: 0.639079, acc: 67.19%] [G loss: 1.744192]\n",
      "epoch:20 step:19549 [D loss: 0.636519, acc: 61.72%] [G loss: 1.857565]\n",
      "epoch:20 step:19550 [D loss: 0.629298, acc: 60.94%] [G loss: 1.674403]\n",
      "epoch:20 step:19551 [D loss: 0.600997, acc: 67.19%] [G loss: 1.814375]\n",
      "epoch:20 step:19552 [D loss: 0.678390, acc: 60.16%] [G loss: 1.794995]\n",
      "epoch:20 step:19553 [D loss: 0.648717, acc: 62.50%] [G loss: 1.838649]\n",
      "epoch:20 step:19554 [D loss: 0.692856, acc: 51.56%] [G loss: 1.832793]\n",
      "epoch:20 step:19555 [D loss: 0.608879, acc: 69.53%] [G loss: 2.087831]\n",
      "epoch:20 step:19556 [D loss: 0.612789, acc: 59.38%] [G loss: 1.869186]\n",
      "epoch:20 step:19557 [D loss: 0.636626, acc: 62.50%] [G loss: 1.790152]\n",
      "epoch:20 step:19558 [D loss: 0.647063, acc: 66.41%] [G loss: 1.746810]\n",
      "epoch:20 step:19559 [D loss: 0.609531, acc: 67.19%] [G loss: 1.972049]\n",
      "epoch:20 step:19560 [D loss: 0.725570, acc: 52.34%] [G loss: 1.705237]\n",
      "epoch:20 step:19561 [D loss: 0.733636, acc: 49.22%] [G loss: 1.732992]\n",
      "epoch:20 step:19562 [D loss: 0.681550, acc: 64.84%] [G loss: 1.808762]\n",
      "epoch:20 step:19563 [D loss: 0.612087, acc: 63.28%] [G loss: 1.803210]\n",
      "epoch:20 step:19564 [D loss: 0.633034, acc: 67.19%] [G loss: 1.894422]\n",
      "epoch:20 step:19565 [D loss: 0.625067, acc: 66.41%] [G loss: 1.906526]\n",
      "epoch:20 step:19566 [D loss: 0.661656, acc: 57.81%] [G loss: 1.983364]\n",
      "epoch:20 step:19567 [D loss: 0.682299, acc: 53.12%] [G loss: 1.740190]\n",
      "epoch:20 step:19568 [D loss: 0.693562, acc: 56.25%] [G loss: 1.772142]\n",
      "epoch:20 step:19569 [D loss: 0.635243, acc: 57.81%] [G loss: 1.773368]\n",
      "epoch:20 step:19570 [D loss: 0.674428, acc: 58.59%] [G loss: 1.685782]\n",
      "epoch:20 step:19571 [D loss: 0.705529, acc: 53.91%] [G loss: 1.656660]\n",
      "epoch:20 step:19572 [D loss: 0.622902, acc: 65.62%] [G loss: 1.915615]\n",
      "epoch:20 step:19573 [D loss: 0.622422, acc: 65.62%] [G loss: 1.997747]\n",
      "epoch:20 step:19574 [D loss: 0.621800, acc: 70.31%] [G loss: 1.808077]\n",
      "epoch:20 step:19575 [D loss: 0.695613, acc: 57.81%] [G loss: 1.831207]\n",
      "epoch:20 step:19576 [D loss: 0.613641, acc: 62.50%] [G loss: 1.865216]\n",
      "epoch:20 step:19577 [D loss: 0.631722, acc: 62.50%] [G loss: 1.949720]\n",
      "epoch:20 step:19578 [D loss: 0.685610, acc: 55.47%] [G loss: 1.874599]\n",
      "epoch:20 step:19579 [D loss: 0.622530, acc: 68.75%] [G loss: 1.911048]\n",
      "epoch:20 step:19580 [D loss: 0.635403, acc: 60.16%] [G loss: 1.795715]\n",
      "epoch:20 step:19581 [D loss: 0.650308, acc: 64.84%] [G loss: 2.068647]\n",
      "epoch:20 step:19582 [D loss: 0.621517, acc: 58.59%] [G loss: 1.794224]\n",
      "epoch:20 step:19583 [D loss: 0.657617, acc: 61.72%] [G loss: 1.820884]\n",
      "epoch:20 step:19584 [D loss: 0.625710, acc: 64.84%] [G loss: 1.815797]\n",
      "epoch:20 step:19585 [D loss: 0.626800, acc: 69.53%] [G loss: 1.944124]\n",
      "epoch:20 step:19586 [D loss: 0.600861, acc: 71.88%] [G loss: 1.946146]\n",
      "epoch:20 step:19587 [D loss: 0.671348, acc: 58.59%] [G loss: 1.815503]\n",
      "epoch:20 step:19588 [D loss: 0.632003, acc: 64.06%] [G loss: 1.924380]\n",
      "epoch:20 step:19589 [D loss: 0.597245, acc: 67.19%] [G loss: 1.934268]\n",
      "epoch:20 step:19590 [D loss: 0.698392, acc: 56.25%] [G loss: 1.751264]\n",
      "epoch:20 step:19591 [D loss: 0.712981, acc: 55.47%] [G loss: 1.931907]\n",
      "epoch:20 step:19592 [D loss: 0.636069, acc: 63.28%] [G loss: 1.897698]\n",
      "epoch:20 step:19593 [D loss: 0.608426, acc: 71.09%] [G loss: 1.852614]\n",
      "epoch:20 step:19594 [D loss: 0.662787, acc: 59.38%] [G loss: 1.857476]\n",
      "epoch:20 step:19595 [D loss: 0.634583, acc: 62.50%] [G loss: 1.884013]\n",
      "epoch:20 step:19596 [D loss: 0.626608, acc: 63.28%] [G loss: 1.855850]\n",
      "epoch:20 step:19597 [D loss: 0.605352, acc: 62.50%] [G loss: 1.994408]\n",
      "epoch:20 step:19598 [D loss: 0.677477, acc: 60.16%] [G loss: 1.725287]\n",
      "epoch:20 step:19599 [D loss: 0.712942, acc: 54.69%] [G loss: 1.849980]\n",
      "epoch:20 step:19600 [D loss: 0.616728, acc: 67.19%] [G loss: 1.872445]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.460679\n",
      "FID: 13.928938\n",
      "0 = 12.651212591171289\n",
      "1 = 0.08312907617656555\n",
      "2 = 0.8795999884605408\n",
      "3 = 0.8998000025749207\n",
      "4 = 0.8593999743461609\n",
      "5 = 0.8648596405982971\n",
      "6 = 0.8998000025749207\n",
      "7 = 6.447460563027876\n",
      "8 = 0.06998792544176174\n",
      "9 = 0.7171000242233276\n",
      "10 = 0.732200026512146\n",
      "11 = 0.7020000219345093\n",
      "12 = 0.7107357978820801\n",
      "13 = 0.732200026512146\n",
      "14 = 7.4607038497924805\n",
      "15 = 9.457592010498047\n",
      "16 = 0.11270143836736679\n",
      "17 = 7.460679054260254\n",
      "18 = 13.928937911987305\n",
      "epoch:20 step:19601 [D loss: 0.642020, acc: 66.41%] [G loss: 1.818610]\n",
      "epoch:20 step:19602 [D loss: 0.683048, acc: 54.69%] [G loss: 1.845206]\n",
      "epoch:20 step:19603 [D loss: 0.623464, acc: 66.41%] [G loss: 1.874760]\n",
      "epoch:20 step:19604 [D loss: 0.652462, acc: 60.94%] [G loss: 1.826106]\n",
      "epoch:20 step:19605 [D loss: 0.652804, acc: 58.59%] [G loss: 1.802770]\n",
      "epoch:20 step:19606 [D loss: 0.638292, acc: 68.75%] [G loss: 1.798344]\n",
      "epoch:20 step:19607 [D loss: 0.690369, acc: 56.25%] [G loss: 1.743653]\n",
      "epoch:20 step:19608 [D loss: 0.689650, acc: 59.38%] [G loss: 1.823007]\n",
      "epoch:20 step:19609 [D loss: 0.689504, acc: 56.25%] [G loss: 1.814072]\n",
      "epoch:20 step:19610 [D loss: 0.662351, acc: 59.38%] [G loss: 1.748050]\n",
      "epoch:20 step:19611 [D loss: 0.641055, acc: 67.19%] [G loss: 1.728984]\n",
      "epoch:20 step:19612 [D loss: 0.694597, acc: 60.16%] [G loss: 1.775270]\n",
      "epoch:20 step:19613 [D loss: 0.685448, acc: 58.59%] [G loss: 1.744889]\n",
      "epoch:20 step:19614 [D loss: 0.638934, acc: 63.28%] [G loss: 1.760263]\n",
      "epoch:20 step:19615 [D loss: 0.591498, acc: 72.66%] [G loss: 2.017414]\n",
      "epoch:20 step:19616 [D loss: 0.657283, acc: 60.94%] [G loss: 1.782385]\n",
      "epoch:20 step:19617 [D loss: 0.645548, acc: 63.28%] [G loss: 1.850778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19618 [D loss: 0.633394, acc: 62.50%] [G loss: 1.839862]\n",
      "epoch:20 step:19619 [D loss: 0.661058, acc: 57.03%] [G loss: 1.898398]\n",
      "epoch:20 step:19620 [D loss: 0.622513, acc: 71.09%] [G loss: 1.725970]\n",
      "epoch:20 step:19621 [D loss: 0.619423, acc: 68.75%] [G loss: 1.818319]\n",
      "epoch:20 step:19622 [D loss: 0.648966, acc: 67.19%] [G loss: 1.855342]\n",
      "epoch:20 step:19623 [D loss: 0.650389, acc: 62.50%] [G loss: 1.974400]\n",
      "epoch:20 step:19624 [D loss: 0.616237, acc: 65.62%] [G loss: 1.823869]\n",
      "epoch:20 step:19625 [D loss: 0.660614, acc: 62.50%] [G loss: 1.931015]\n",
      "epoch:20 step:19626 [D loss: 0.642605, acc: 65.62%] [G loss: 2.135413]\n",
      "epoch:20 step:19627 [D loss: 0.715050, acc: 53.12%] [G loss: 1.844575]\n",
      "epoch:20 step:19628 [D loss: 0.642400, acc: 62.50%] [G loss: 1.952080]\n",
      "epoch:20 step:19629 [D loss: 0.593000, acc: 67.97%] [G loss: 1.923209]\n",
      "epoch:20 step:19630 [D loss: 0.643282, acc: 61.72%] [G loss: 1.949382]\n",
      "epoch:20 step:19631 [D loss: 0.675580, acc: 61.72%] [G loss: 1.714643]\n",
      "epoch:20 step:19632 [D loss: 0.662589, acc: 59.38%] [G loss: 1.877924]\n",
      "epoch:20 step:19633 [D loss: 0.629884, acc: 66.41%] [G loss: 1.938577]\n",
      "epoch:20 step:19634 [D loss: 0.677012, acc: 64.06%] [G loss: 1.894960]\n",
      "epoch:20 step:19635 [D loss: 0.614823, acc: 64.84%] [G loss: 1.887995]\n",
      "epoch:20 step:19636 [D loss: 0.677952, acc: 60.16%] [G loss: 1.875744]\n",
      "epoch:20 step:19637 [D loss: 0.670017, acc: 61.72%] [G loss: 1.944355]\n",
      "epoch:20 step:19638 [D loss: 0.615863, acc: 65.62%] [G loss: 1.870553]\n",
      "epoch:20 step:19639 [D loss: 0.628485, acc: 64.84%] [G loss: 1.962653]\n",
      "epoch:20 step:19640 [D loss: 0.624325, acc: 60.16%] [G loss: 1.976018]\n",
      "epoch:20 step:19641 [D loss: 0.640104, acc: 67.19%] [G loss: 1.869345]\n",
      "epoch:20 step:19642 [D loss: 0.659016, acc: 62.50%] [G loss: 1.929910]\n",
      "epoch:20 step:19643 [D loss: 0.685780, acc: 55.47%] [G loss: 1.859473]\n",
      "epoch:20 step:19644 [D loss: 0.688291, acc: 56.25%] [G loss: 1.943005]\n",
      "epoch:20 step:19645 [D loss: 0.651864, acc: 62.50%] [G loss: 1.944243]\n",
      "epoch:20 step:19646 [D loss: 0.637240, acc: 64.84%] [G loss: 1.968805]\n",
      "epoch:20 step:19647 [D loss: 0.645839, acc: 59.38%] [G loss: 2.010211]\n",
      "epoch:20 step:19648 [D loss: 0.645295, acc: 60.94%] [G loss: 1.770883]\n",
      "epoch:20 step:19649 [D loss: 0.632432, acc: 62.50%] [G loss: 1.972502]\n",
      "epoch:20 step:19650 [D loss: 0.648203, acc: 60.16%] [G loss: 1.985814]\n",
      "epoch:20 step:19651 [D loss: 0.633244, acc: 67.19%] [G loss: 1.828176]\n",
      "epoch:20 step:19652 [D loss: 0.632379, acc: 62.50%] [G loss: 2.051452]\n",
      "epoch:20 step:19653 [D loss: 0.678083, acc: 58.59%] [G loss: 1.921705]\n",
      "epoch:20 step:19654 [D loss: 0.639905, acc: 57.81%] [G loss: 1.871034]\n",
      "epoch:20 step:19655 [D loss: 0.662345, acc: 60.94%] [G loss: 1.922350]\n",
      "epoch:20 step:19656 [D loss: 0.620774, acc: 65.62%] [G loss: 1.953259]\n",
      "epoch:20 step:19657 [D loss: 0.715143, acc: 58.59%] [G loss: 1.963246]\n",
      "epoch:20 step:19658 [D loss: 0.589213, acc: 70.31%] [G loss: 2.086648]\n",
      "epoch:20 step:19659 [D loss: 0.561715, acc: 75.78%] [G loss: 1.888554]\n",
      "epoch:20 step:19660 [D loss: 0.768254, acc: 52.34%] [G loss: 1.753773]\n",
      "epoch:20 step:19661 [D loss: 0.633730, acc: 60.94%] [G loss: 2.004818]\n",
      "epoch:20 step:19662 [D loss: 0.698190, acc: 61.72%] [G loss: 1.849935]\n",
      "epoch:20 step:19663 [D loss: 0.586737, acc: 68.75%] [G loss: 1.956509]\n",
      "epoch:20 step:19664 [D loss: 0.591536, acc: 71.09%] [G loss: 2.063541]\n",
      "epoch:20 step:19665 [D loss: 0.593619, acc: 75.00%] [G loss: 2.063630]\n",
      "epoch:20 step:19666 [D loss: 0.595681, acc: 64.84%] [G loss: 2.139416]\n",
      "epoch:20 step:19667 [D loss: 0.645706, acc: 64.84%] [G loss: 2.125199]\n",
      "epoch:20 step:19668 [D loss: 0.732927, acc: 51.56%] [G loss: 1.846086]\n",
      "epoch:20 step:19669 [D loss: 0.730524, acc: 52.34%] [G loss: 1.910556]\n",
      "epoch:20 step:19670 [D loss: 0.559062, acc: 74.22%] [G loss: 1.967026]\n",
      "epoch:20 step:19671 [D loss: 0.615763, acc: 64.06%] [G loss: 2.018209]\n",
      "epoch:20 step:19672 [D loss: 0.629519, acc: 61.72%] [G loss: 1.878648]\n",
      "epoch:20 step:19673 [D loss: 0.615633, acc: 66.41%] [G loss: 1.962668]\n",
      "epoch:20 step:19674 [D loss: 0.609892, acc: 69.53%] [G loss: 1.882633]\n",
      "epoch:20 step:19675 [D loss: 0.600074, acc: 64.84%] [G loss: 1.994913]\n",
      "epoch:20 step:19676 [D loss: 0.592910, acc: 62.50%] [G loss: 2.121452]\n",
      "epoch:20 step:19677 [D loss: 0.621636, acc: 63.28%] [G loss: 2.406636]\n",
      "epoch:21 step:19678 [D loss: 0.625314, acc: 64.84%] [G loss: 1.846873]\n",
      "epoch:21 step:19679 [D loss: 0.637224, acc: 62.50%] [G loss: 1.934228]\n",
      "epoch:21 step:19680 [D loss: 0.709461, acc: 53.91%] [G loss: 1.860241]\n",
      "epoch:21 step:19681 [D loss: 0.638540, acc: 62.50%] [G loss: 1.938499]\n",
      "epoch:21 step:19682 [D loss: 0.585728, acc: 70.31%] [G loss: 1.866460]\n",
      "epoch:21 step:19683 [D loss: 0.580742, acc: 71.88%] [G loss: 2.103898]\n",
      "epoch:21 step:19684 [D loss: 0.625214, acc: 60.94%] [G loss: 2.114019]\n",
      "epoch:21 step:19685 [D loss: 0.662883, acc: 59.38%] [G loss: 1.941278]\n",
      "epoch:21 step:19686 [D loss: 0.611916, acc: 65.62%] [G loss: 2.062038]\n",
      "epoch:21 step:19687 [D loss: 0.670171, acc: 57.81%] [G loss: 2.012571]\n",
      "epoch:21 step:19688 [D loss: 0.604676, acc: 67.97%] [G loss: 2.030806]\n",
      "epoch:21 step:19689 [D loss: 0.567840, acc: 72.66%] [G loss: 1.897658]\n",
      "epoch:21 step:19690 [D loss: 0.660412, acc: 62.50%] [G loss: 1.926097]\n",
      "epoch:21 step:19691 [D loss: 0.589562, acc: 68.75%] [G loss: 2.046840]\n",
      "epoch:21 step:19692 [D loss: 0.617803, acc: 64.06%] [G loss: 2.082477]\n",
      "epoch:21 step:19693 [D loss: 0.580123, acc: 69.53%] [G loss: 2.240116]\n",
      "epoch:21 step:19694 [D loss: 0.634444, acc: 59.38%] [G loss: 1.986051]\n",
      "epoch:21 step:19695 [D loss: 0.689902, acc: 64.84%] [G loss: 1.900595]\n",
      "epoch:21 step:19696 [D loss: 0.691558, acc: 53.12%] [G loss: 1.863377]\n",
      "epoch:21 step:19697 [D loss: 0.680886, acc: 57.03%] [G loss: 1.753293]\n",
      "epoch:21 step:19698 [D loss: 0.677019, acc: 63.28%] [G loss: 1.990978]\n",
      "epoch:21 step:19699 [D loss: 0.635517, acc: 57.81%] [G loss: 1.852542]\n",
      "epoch:21 step:19700 [D loss: 0.580802, acc: 68.75%] [G loss: 1.991145]\n",
      "epoch:21 step:19701 [D loss: 0.626180, acc: 67.19%] [G loss: 1.933150]\n",
      "epoch:21 step:19702 [D loss: 0.570809, acc: 67.19%] [G loss: 1.997764]\n",
      "epoch:21 step:19703 [D loss: 0.648234, acc: 64.06%] [G loss: 1.777083]\n",
      "epoch:21 step:19704 [D loss: 0.695921, acc: 60.16%] [G loss: 1.809920]\n",
      "epoch:21 step:19705 [D loss: 0.662610, acc: 61.72%] [G loss: 1.748036]\n",
      "epoch:21 step:19706 [D loss: 0.600181, acc: 69.53%] [G loss: 1.952482]\n",
      "epoch:21 step:19707 [D loss: 0.649467, acc: 62.50%] [G loss: 1.849911]\n",
      "epoch:21 step:19708 [D loss: 0.689914, acc: 57.03%] [G loss: 1.827970]\n",
      "epoch:21 step:19709 [D loss: 0.679410, acc: 50.00%] [G loss: 1.845353]\n",
      "epoch:21 step:19710 [D loss: 0.643248, acc: 60.16%] [G loss: 1.905275]\n",
      "epoch:21 step:19711 [D loss: 0.622871, acc: 62.50%] [G loss: 1.853166]\n",
      "epoch:21 step:19712 [D loss: 0.624732, acc: 60.94%] [G loss: 1.845111]\n",
      "epoch:21 step:19713 [D loss: 0.593641, acc: 70.31%] [G loss: 1.956470]\n",
      "epoch:21 step:19714 [D loss: 0.612278, acc: 68.75%] [G loss: 2.052080]\n",
      "epoch:21 step:19715 [D loss: 0.684846, acc: 61.72%] [G loss: 1.981478]\n",
      "epoch:21 step:19716 [D loss: 0.660033, acc: 57.03%] [G loss: 1.880975]\n",
      "epoch:21 step:19717 [D loss: 0.574991, acc: 71.88%] [G loss: 2.091169]\n",
      "epoch:21 step:19718 [D loss: 0.647339, acc: 60.94%] [G loss: 1.888922]\n",
      "epoch:21 step:19719 [D loss: 0.600129, acc: 68.75%] [G loss: 1.968677]\n",
      "epoch:21 step:19720 [D loss: 0.661546, acc: 57.81%] [G loss: 1.916001]\n",
      "epoch:21 step:19721 [D loss: 0.656816, acc: 57.81%] [G loss: 1.859541]\n",
      "epoch:21 step:19722 [D loss: 0.644696, acc: 64.84%] [G loss: 1.990045]\n",
      "epoch:21 step:19723 [D loss: 0.653515, acc: 62.50%] [G loss: 1.918023]\n",
      "epoch:21 step:19724 [D loss: 0.683048, acc: 62.50%] [G loss: 1.884592]\n",
      "epoch:21 step:19725 [D loss: 0.609847, acc: 66.41%] [G loss: 1.971378]\n",
      "epoch:21 step:19726 [D loss: 0.625426, acc: 64.84%] [G loss: 1.856472]\n",
      "epoch:21 step:19727 [D loss: 0.625555, acc: 64.84%] [G loss: 1.912716]\n",
      "epoch:21 step:19728 [D loss: 0.646311, acc: 60.94%] [G loss: 1.876611]\n",
      "epoch:21 step:19729 [D loss: 0.659813, acc: 62.50%] [G loss: 1.863215]\n",
      "epoch:21 step:19730 [D loss: 0.637052, acc: 64.06%] [G loss: 2.081029]\n",
      "epoch:21 step:19731 [D loss: 0.594719, acc: 69.53%] [G loss: 2.052520]\n",
      "epoch:21 step:19732 [D loss: 0.601840, acc: 70.31%] [G loss: 2.049456]\n",
      "epoch:21 step:19733 [D loss: 0.634829, acc: 66.41%] [G loss: 2.077085]\n",
      "epoch:21 step:19734 [D loss: 0.624646, acc: 63.28%] [G loss: 1.985950]\n",
      "epoch:21 step:19735 [D loss: 0.638635, acc: 67.19%] [G loss: 1.981840]\n",
      "epoch:21 step:19736 [D loss: 0.623762, acc: 67.19%] [G loss: 2.029467]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19737 [D loss: 0.685571, acc: 57.03%] [G loss: 1.755400]\n",
      "epoch:21 step:19738 [D loss: 0.610944, acc: 61.72%] [G loss: 1.907529]\n",
      "epoch:21 step:19739 [D loss: 0.649336, acc: 67.19%] [G loss: 1.971670]\n",
      "epoch:21 step:19740 [D loss: 0.651500, acc: 61.72%] [G loss: 1.897320]\n",
      "epoch:21 step:19741 [D loss: 0.647351, acc: 61.72%] [G loss: 1.947727]\n",
      "epoch:21 step:19742 [D loss: 0.601972, acc: 69.53%] [G loss: 1.950403]\n",
      "epoch:21 step:19743 [D loss: 0.666653, acc: 61.72%] [G loss: 1.792790]\n",
      "epoch:21 step:19744 [D loss: 0.648786, acc: 64.06%] [G loss: 1.831562]\n",
      "epoch:21 step:19745 [D loss: 0.658320, acc: 60.94%] [G loss: 1.836411]\n",
      "epoch:21 step:19746 [D loss: 0.625522, acc: 60.16%] [G loss: 1.983036]\n",
      "epoch:21 step:19747 [D loss: 0.673352, acc: 58.59%] [G loss: 1.905123]\n",
      "epoch:21 step:19748 [D loss: 0.658089, acc: 60.94%] [G loss: 1.806382]\n",
      "epoch:21 step:19749 [D loss: 0.674658, acc: 60.94%] [G loss: 1.923121]\n",
      "epoch:21 step:19750 [D loss: 0.625483, acc: 61.72%] [G loss: 1.966736]\n",
      "epoch:21 step:19751 [D loss: 0.626784, acc: 60.94%] [G loss: 1.977825]\n",
      "epoch:21 step:19752 [D loss: 0.660873, acc: 64.06%] [G loss: 1.892212]\n",
      "epoch:21 step:19753 [D loss: 0.590368, acc: 69.53%] [G loss: 1.909123]\n",
      "epoch:21 step:19754 [D loss: 0.585527, acc: 63.28%] [G loss: 2.041393]\n",
      "epoch:21 step:19755 [D loss: 0.659315, acc: 57.03%] [G loss: 2.033263]\n",
      "epoch:21 step:19756 [D loss: 0.639457, acc: 64.06%] [G loss: 1.928231]\n",
      "epoch:21 step:19757 [D loss: 0.632276, acc: 68.75%] [G loss: 1.887161]\n",
      "epoch:21 step:19758 [D loss: 0.693932, acc: 59.38%] [G loss: 1.975600]\n",
      "epoch:21 step:19759 [D loss: 0.688336, acc: 57.81%] [G loss: 1.948990]\n",
      "epoch:21 step:19760 [D loss: 0.706334, acc: 52.34%] [G loss: 1.728565]\n",
      "epoch:21 step:19761 [D loss: 0.646899, acc: 64.84%] [G loss: 1.899912]\n",
      "epoch:21 step:19762 [D loss: 0.671409, acc: 59.38%] [G loss: 1.864178]\n",
      "epoch:21 step:19763 [D loss: 0.597384, acc: 64.84%] [G loss: 2.029864]\n",
      "epoch:21 step:19764 [D loss: 0.618514, acc: 67.97%] [G loss: 1.898410]\n",
      "epoch:21 step:19765 [D loss: 0.637731, acc: 63.28%] [G loss: 1.952823]\n",
      "epoch:21 step:19766 [D loss: 0.666831, acc: 63.28%] [G loss: 1.878357]\n",
      "epoch:21 step:19767 [D loss: 0.598880, acc: 70.31%] [G loss: 1.822672]\n",
      "epoch:21 step:19768 [D loss: 0.631188, acc: 60.94%] [G loss: 1.851179]\n",
      "epoch:21 step:19769 [D loss: 0.611776, acc: 67.97%] [G loss: 1.926811]\n",
      "epoch:21 step:19770 [D loss: 0.681860, acc: 60.16%] [G loss: 2.000387]\n",
      "epoch:21 step:19771 [D loss: 0.650190, acc: 60.16%] [G loss: 1.908610]\n",
      "epoch:21 step:19772 [D loss: 0.669010, acc: 62.50%] [G loss: 1.900121]\n",
      "epoch:21 step:19773 [D loss: 0.655913, acc: 60.94%] [G loss: 2.019841]\n",
      "epoch:21 step:19774 [D loss: 0.634909, acc: 57.03%] [G loss: 1.965632]\n",
      "epoch:21 step:19775 [D loss: 0.755674, acc: 42.19%] [G loss: 1.710700]\n",
      "epoch:21 step:19776 [D loss: 0.676927, acc: 59.38%] [G loss: 1.775265]\n",
      "epoch:21 step:19777 [D loss: 0.603936, acc: 64.84%] [G loss: 1.869538]\n",
      "epoch:21 step:19778 [D loss: 0.655871, acc: 64.06%] [G loss: 1.829418]\n",
      "epoch:21 step:19779 [D loss: 0.664348, acc: 60.16%] [G loss: 1.922096]\n",
      "epoch:21 step:19780 [D loss: 0.648906, acc: 61.72%] [G loss: 1.922193]\n",
      "epoch:21 step:19781 [D loss: 0.683016, acc: 58.59%] [G loss: 1.851081]\n",
      "epoch:21 step:19782 [D loss: 0.656404, acc: 58.59%] [G loss: 1.967484]\n",
      "epoch:21 step:19783 [D loss: 0.616787, acc: 63.28%] [G loss: 1.945437]\n",
      "epoch:21 step:19784 [D loss: 0.610892, acc: 67.97%] [G loss: 2.156353]\n",
      "epoch:21 step:19785 [D loss: 0.703899, acc: 53.12%] [G loss: 1.784683]\n",
      "epoch:21 step:19786 [D loss: 0.678336, acc: 62.50%] [G loss: 1.795685]\n",
      "epoch:21 step:19787 [D loss: 0.664867, acc: 61.72%] [G loss: 1.859792]\n",
      "epoch:21 step:19788 [D loss: 0.642483, acc: 60.94%] [G loss: 2.017504]\n",
      "epoch:21 step:19789 [D loss: 0.653126, acc: 65.62%] [G loss: 1.999526]\n",
      "epoch:21 step:19790 [D loss: 0.636972, acc: 60.16%] [G loss: 1.952055]\n",
      "epoch:21 step:19791 [D loss: 0.672300, acc: 60.16%] [G loss: 2.144269]\n",
      "epoch:21 step:19792 [D loss: 0.595397, acc: 65.62%] [G loss: 2.035224]\n",
      "epoch:21 step:19793 [D loss: 0.631105, acc: 63.28%] [G loss: 2.057896]\n",
      "epoch:21 step:19794 [D loss: 0.727878, acc: 56.25%] [G loss: 2.023525]\n",
      "epoch:21 step:19795 [D loss: 0.628439, acc: 59.38%] [G loss: 2.119701]\n",
      "epoch:21 step:19796 [D loss: 0.628888, acc: 64.84%] [G loss: 2.038480]\n",
      "epoch:21 step:19797 [D loss: 0.679370, acc: 60.16%] [G loss: 1.934938]\n",
      "epoch:21 step:19798 [D loss: 0.700578, acc: 57.81%] [G loss: 1.876300]\n",
      "epoch:21 step:19799 [D loss: 0.699361, acc: 55.47%] [G loss: 2.023365]\n",
      "epoch:21 step:19800 [D loss: 0.663776, acc: 58.59%] [G loss: 1.979553]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.359824\n",
      "FID: 13.030622\n",
      "0 = 12.947199310779542\n",
      "1 = 0.09239359947924664\n",
      "2 = 0.8891000151634216\n",
      "3 = 0.9118000268936157\n",
      "4 = 0.8664000034332275\n",
      "5 = 0.8722020387649536\n",
      "6 = 0.9118000268936157\n",
      "7 = 6.492730913996707\n",
      "8 = 0.06814279306413872\n",
      "9 = 0.7330999970436096\n",
      "10 = 0.7409999966621399\n",
      "11 = 0.7251999974250793\n",
      "12 = 0.7294743061065674\n",
      "13 = 0.7409999966621399\n",
      "14 = 7.359852313995361\n",
      "15 = 9.36368179321289\n",
      "16 = 0.13900144398212433\n",
      "17 = 7.359824180603027\n",
      "18 = 13.030621528625488\n",
      "epoch:21 step:19801 [D loss: 0.664608, acc: 59.38%] [G loss: 1.898563]\n",
      "epoch:21 step:19802 [D loss: 0.736531, acc: 50.78%] [G loss: 1.735390]\n",
      "epoch:21 step:19803 [D loss: 0.659702, acc: 61.72%] [G loss: 1.922751]\n",
      "epoch:21 step:19804 [D loss: 0.651300, acc: 58.59%] [G loss: 1.742936]\n",
      "epoch:21 step:19805 [D loss: 0.607904, acc: 66.41%] [G loss: 1.851523]\n",
      "epoch:21 step:19806 [D loss: 0.624532, acc: 67.97%] [G loss: 1.850426]\n",
      "epoch:21 step:19807 [D loss: 0.637758, acc: 59.38%] [G loss: 2.107240]\n",
      "epoch:21 step:19808 [D loss: 0.599282, acc: 64.84%] [G loss: 1.908172]\n",
      "epoch:21 step:19809 [D loss: 0.653017, acc: 60.94%] [G loss: 1.823663]\n",
      "epoch:21 step:19810 [D loss: 0.687249, acc: 52.34%] [G loss: 1.775731]\n",
      "epoch:21 step:19811 [D loss: 0.670502, acc: 59.38%] [G loss: 1.728179]\n",
      "epoch:21 step:19812 [D loss: 0.655079, acc: 60.16%] [G loss: 1.796428]\n",
      "epoch:21 step:19813 [D loss: 0.689635, acc: 59.38%] [G loss: 1.689003]\n",
      "epoch:21 step:19814 [D loss: 0.708222, acc: 57.03%] [G loss: 1.787146]\n",
      "epoch:21 step:19815 [D loss: 0.652565, acc: 59.38%] [G loss: 1.929533]\n",
      "epoch:21 step:19816 [D loss: 0.662750, acc: 56.25%] [G loss: 1.872981]\n",
      "epoch:21 step:19817 [D loss: 0.642075, acc: 63.28%] [G loss: 1.870812]\n",
      "epoch:21 step:19818 [D loss: 0.606503, acc: 67.19%] [G loss: 1.891004]\n",
      "epoch:21 step:19819 [D loss: 0.638430, acc: 62.50%] [G loss: 1.784655]\n",
      "epoch:21 step:19820 [D loss: 0.653025, acc: 64.06%] [G loss: 1.795828]\n",
      "epoch:21 step:19821 [D loss: 0.561999, acc: 74.22%] [G loss: 1.960190]\n",
      "epoch:21 step:19822 [D loss: 0.696737, acc: 55.47%] [G loss: 2.033753]\n",
      "epoch:21 step:19823 [D loss: 0.667023, acc: 56.25%] [G loss: 1.828574]\n",
      "epoch:21 step:19824 [D loss: 0.658015, acc: 53.12%] [G loss: 1.897029]\n",
      "epoch:21 step:19825 [D loss: 0.672812, acc: 60.94%] [G loss: 1.824060]\n",
      "epoch:21 step:19826 [D loss: 0.638328, acc: 60.16%] [G loss: 2.028125]\n",
      "epoch:21 step:19827 [D loss: 0.618228, acc: 66.41%] [G loss: 1.957085]\n",
      "epoch:21 step:19828 [D loss: 0.627872, acc: 65.62%] [G loss: 1.874945]\n",
      "epoch:21 step:19829 [D loss: 0.651373, acc: 64.06%] [G loss: 1.796213]\n",
      "epoch:21 step:19830 [D loss: 0.635229, acc: 62.50%] [G loss: 1.888503]\n",
      "epoch:21 step:19831 [D loss: 0.628657, acc: 68.75%] [G loss: 2.062220]\n",
      "epoch:21 step:19832 [D loss: 0.668467, acc: 60.16%] [G loss: 1.886895]\n",
      "epoch:21 step:19833 [D loss: 0.615020, acc: 63.28%] [G loss: 1.948300]\n",
      "epoch:21 step:19834 [D loss: 0.656352, acc: 59.38%] [G loss: 1.807676]\n",
      "epoch:21 step:19835 [D loss: 0.635032, acc: 64.06%] [G loss: 1.848211]\n",
      "epoch:21 step:19836 [D loss: 0.695106, acc: 57.03%] [G loss: 1.974195]\n",
      "epoch:21 step:19837 [D loss: 0.646974, acc: 62.50%] [G loss: 1.740664]\n",
      "epoch:21 step:19838 [D loss: 0.679477, acc: 55.47%] [G loss: 1.782472]\n",
      "epoch:21 step:19839 [D loss: 0.649626, acc: 64.84%] [G loss: 1.842614]\n",
      "epoch:21 step:19840 [D loss: 0.607302, acc: 65.62%] [G loss: 1.853966]\n",
      "epoch:21 step:19841 [D loss: 0.671144, acc: 60.94%] [G loss: 1.903974]\n",
      "epoch:21 step:19842 [D loss: 0.654840, acc: 62.50%] [G loss: 2.012845]\n",
      "epoch:21 step:19843 [D loss: 0.653025, acc: 61.72%] [G loss: 1.881716]\n",
      "epoch:21 step:19844 [D loss: 0.668620, acc: 61.72%] [G loss: 1.816856]\n",
      "epoch:21 step:19845 [D loss: 0.612135, acc: 71.09%] [G loss: 1.819769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19846 [D loss: 0.650267, acc: 60.16%] [G loss: 1.909580]\n",
      "epoch:21 step:19847 [D loss: 0.665385, acc: 56.25%] [G loss: 1.821049]\n",
      "epoch:21 step:19848 [D loss: 0.635163, acc: 67.19%] [G loss: 1.938582]\n",
      "epoch:21 step:19849 [D loss: 0.661181, acc: 64.84%] [G loss: 1.817799]\n",
      "epoch:21 step:19850 [D loss: 0.728108, acc: 53.91%] [G loss: 1.754185]\n",
      "epoch:21 step:19851 [D loss: 0.681742, acc: 58.59%] [G loss: 1.849849]\n",
      "epoch:21 step:19852 [D loss: 0.612236, acc: 65.62%] [G loss: 1.822691]\n",
      "epoch:21 step:19853 [D loss: 0.649058, acc: 59.38%] [G loss: 1.815137]\n",
      "epoch:21 step:19854 [D loss: 0.682915, acc: 56.25%] [G loss: 1.873791]\n",
      "epoch:21 step:19855 [D loss: 0.632728, acc: 61.72%] [G loss: 1.827414]\n",
      "epoch:21 step:19856 [D loss: 0.719900, acc: 50.78%] [G loss: 1.796720]\n",
      "epoch:21 step:19857 [D loss: 0.644179, acc: 62.50%] [G loss: 1.760388]\n",
      "epoch:21 step:19858 [D loss: 0.677781, acc: 54.69%] [G loss: 1.700229]\n",
      "epoch:21 step:19859 [D loss: 0.669459, acc: 56.25%] [G loss: 1.698433]\n",
      "epoch:21 step:19860 [D loss: 0.706890, acc: 58.59%] [G loss: 1.675478]\n",
      "epoch:21 step:19861 [D loss: 0.669640, acc: 60.94%] [G loss: 1.812676]\n",
      "epoch:21 step:19862 [D loss: 0.649555, acc: 59.38%] [G loss: 1.846022]\n",
      "epoch:21 step:19863 [D loss: 0.600699, acc: 68.75%] [G loss: 1.815311]\n",
      "epoch:21 step:19864 [D loss: 0.684134, acc: 55.47%] [G loss: 1.822120]\n",
      "epoch:21 step:19865 [D loss: 0.656630, acc: 58.59%] [G loss: 1.893452]\n",
      "epoch:21 step:19866 [D loss: 0.658578, acc: 58.59%] [G loss: 1.783680]\n",
      "epoch:21 step:19867 [D loss: 0.625145, acc: 66.41%] [G loss: 1.817284]\n",
      "epoch:21 step:19868 [D loss: 0.627991, acc: 64.06%] [G loss: 1.831641]\n",
      "epoch:21 step:19869 [D loss: 0.634092, acc: 65.62%] [G loss: 1.997814]\n",
      "epoch:21 step:19870 [D loss: 0.580674, acc: 70.31%] [G loss: 1.857526]\n",
      "epoch:21 step:19871 [D loss: 0.643484, acc: 60.16%] [G loss: 1.987671]\n",
      "epoch:21 step:19872 [D loss: 0.649652, acc: 64.84%] [G loss: 1.931890]\n",
      "epoch:21 step:19873 [D loss: 0.647173, acc: 59.38%] [G loss: 1.880816]\n",
      "epoch:21 step:19874 [D loss: 0.634221, acc: 66.41%] [G loss: 1.956386]\n",
      "epoch:21 step:19875 [D loss: 0.634167, acc: 64.84%] [G loss: 1.929793]\n",
      "epoch:21 step:19876 [D loss: 0.635982, acc: 58.59%] [G loss: 1.823226]\n",
      "epoch:21 step:19877 [D loss: 0.623316, acc: 63.28%] [G loss: 1.823500]\n",
      "epoch:21 step:19878 [D loss: 0.694468, acc: 55.47%] [G loss: 1.974105]\n",
      "epoch:21 step:19879 [D loss: 0.715985, acc: 55.47%] [G loss: 1.935754]\n",
      "epoch:21 step:19880 [D loss: 0.654415, acc: 60.94%] [G loss: 1.883337]\n",
      "epoch:21 step:19881 [D loss: 0.714090, acc: 53.12%] [G loss: 1.769190]\n",
      "epoch:21 step:19882 [D loss: 0.680572, acc: 58.59%] [G loss: 1.697690]\n",
      "epoch:21 step:19883 [D loss: 0.619664, acc: 66.41%] [G loss: 1.814695]\n",
      "epoch:21 step:19884 [D loss: 0.598016, acc: 64.06%] [G loss: 2.105757]\n",
      "epoch:21 step:19885 [D loss: 0.624306, acc: 63.28%] [G loss: 2.121898]\n",
      "epoch:21 step:19886 [D loss: 0.605642, acc: 62.50%] [G loss: 2.087900]\n",
      "epoch:21 step:19887 [D loss: 0.676931, acc: 60.16%] [G loss: 1.858781]\n",
      "epoch:21 step:19888 [D loss: 0.704750, acc: 50.78%] [G loss: 1.726861]\n",
      "epoch:21 step:19889 [D loss: 0.696049, acc: 57.03%] [G loss: 1.738049]\n",
      "epoch:21 step:19890 [D loss: 0.695590, acc: 50.78%] [G loss: 1.765886]\n",
      "epoch:21 step:19891 [D loss: 0.669752, acc: 59.38%] [G loss: 1.709341]\n",
      "epoch:21 step:19892 [D loss: 0.624572, acc: 60.94%] [G loss: 1.784209]\n",
      "epoch:21 step:19893 [D loss: 0.674208, acc: 62.50%] [G loss: 1.964291]\n",
      "epoch:21 step:19894 [D loss: 0.643799, acc: 64.84%] [G loss: 1.877774]\n",
      "epoch:21 step:19895 [D loss: 0.596662, acc: 67.19%] [G loss: 1.999685]\n",
      "epoch:21 step:19896 [D loss: 0.658282, acc: 59.38%] [G loss: 1.996248]\n",
      "epoch:21 step:19897 [D loss: 0.670043, acc: 57.03%] [G loss: 1.683695]\n",
      "epoch:21 step:19898 [D loss: 0.689293, acc: 58.59%] [G loss: 1.903331]\n",
      "epoch:21 step:19899 [D loss: 0.648930, acc: 62.50%] [G loss: 2.037826]\n",
      "epoch:21 step:19900 [D loss: 0.679034, acc: 57.81%] [G loss: 1.877020]\n",
      "epoch:21 step:19901 [D loss: 0.660560, acc: 59.38%] [G loss: 1.858677]\n",
      "epoch:21 step:19902 [D loss: 0.594222, acc: 65.62%] [G loss: 1.978601]\n",
      "epoch:21 step:19903 [D loss: 0.670312, acc: 57.81%] [G loss: 1.884214]\n",
      "epoch:21 step:19904 [D loss: 0.693513, acc: 63.28%] [G loss: 1.907233]\n",
      "epoch:21 step:19905 [D loss: 0.705794, acc: 53.91%] [G loss: 1.756190]\n",
      "epoch:21 step:19906 [D loss: 0.591691, acc: 70.31%] [G loss: 1.982565]\n",
      "epoch:21 step:19907 [D loss: 0.585909, acc: 70.31%] [G loss: 2.082546]\n",
      "epoch:21 step:19908 [D loss: 0.588723, acc: 71.09%] [G loss: 2.215781]\n",
      "epoch:21 step:19909 [D loss: 0.564425, acc: 74.22%] [G loss: 2.336444]\n",
      "epoch:21 step:19910 [D loss: 0.618808, acc: 68.75%] [G loss: 1.810466]\n",
      "epoch:21 step:19911 [D loss: 0.690067, acc: 52.34%] [G loss: 1.801229]\n",
      "epoch:21 step:19912 [D loss: 0.677444, acc: 63.28%] [G loss: 1.872051]\n",
      "epoch:21 step:19913 [D loss: 0.657594, acc: 60.94%] [G loss: 1.834613]\n",
      "epoch:21 step:19914 [D loss: 0.600623, acc: 66.41%] [G loss: 1.865753]\n",
      "epoch:21 step:19915 [D loss: 0.656117, acc: 63.28%] [G loss: 1.920913]\n",
      "epoch:21 step:19916 [D loss: 0.670376, acc: 57.81%] [G loss: 1.789979]\n",
      "epoch:21 step:19917 [D loss: 0.654557, acc: 60.94%] [G loss: 1.857688]\n",
      "epoch:21 step:19918 [D loss: 0.641383, acc: 65.62%] [G loss: 1.778982]\n",
      "epoch:21 step:19919 [D loss: 0.615888, acc: 68.75%] [G loss: 1.851912]\n",
      "epoch:21 step:19920 [D loss: 0.664562, acc: 60.16%] [G loss: 1.967245]\n",
      "epoch:21 step:19921 [D loss: 0.666892, acc: 56.25%] [G loss: 1.857318]\n",
      "epoch:21 step:19922 [D loss: 0.615408, acc: 61.72%] [G loss: 1.764180]\n",
      "epoch:21 step:19923 [D loss: 0.664055, acc: 59.38%] [G loss: 1.813379]\n",
      "epoch:21 step:19924 [D loss: 0.648260, acc: 66.41%] [G loss: 1.993432]\n",
      "epoch:21 step:19925 [D loss: 0.718036, acc: 52.34%] [G loss: 2.088779]\n",
      "epoch:21 step:19926 [D loss: 0.696500, acc: 55.47%] [G loss: 1.763535]\n",
      "epoch:21 step:19927 [D loss: 0.729752, acc: 48.44%] [G loss: 1.611623]\n",
      "epoch:21 step:19928 [D loss: 0.684735, acc: 53.12%] [G loss: 1.708580]\n",
      "epoch:21 step:19929 [D loss: 0.656326, acc: 58.59%] [G loss: 1.652288]\n",
      "epoch:21 step:19930 [D loss: 0.678991, acc: 60.16%] [G loss: 1.780675]\n",
      "epoch:21 step:19931 [D loss: 0.663377, acc: 64.06%] [G loss: 1.723126]\n",
      "epoch:21 step:19932 [D loss: 0.604505, acc: 73.44%] [G loss: 1.862044]\n",
      "epoch:21 step:19933 [D loss: 0.640636, acc: 63.28%] [G loss: 1.729406]\n",
      "epoch:21 step:19934 [D loss: 0.695942, acc: 57.03%] [G loss: 1.797929]\n",
      "epoch:21 step:19935 [D loss: 0.626102, acc: 64.84%] [G loss: 1.799688]\n",
      "epoch:21 step:19936 [D loss: 0.653247, acc: 66.41%] [G loss: 1.813655]\n",
      "epoch:21 step:19937 [D loss: 0.595595, acc: 70.31%] [G loss: 1.929286]\n",
      "epoch:21 step:19938 [D loss: 0.675597, acc: 60.94%] [G loss: 1.830974]\n",
      "epoch:21 step:19939 [D loss: 0.624911, acc: 64.06%] [G loss: 1.919849]\n",
      "epoch:21 step:19940 [D loss: 0.632184, acc: 70.31%] [G loss: 2.096272]\n",
      "epoch:21 step:19941 [D loss: 0.661334, acc: 57.81%] [G loss: 1.854567]\n",
      "epoch:21 step:19942 [D loss: 0.657173, acc: 59.38%] [G loss: 1.780469]\n",
      "epoch:21 step:19943 [D loss: 0.619964, acc: 66.41%] [G loss: 1.911613]\n",
      "epoch:21 step:19944 [D loss: 0.625248, acc: 67.19%] [G loss: 1.846643]\n",
      "epoch:21 step:19945 [D loss: 0.683536, acc: 54.69%] [G loss: 1.741667]\n",
      "epoch:21 step:19946 [D loss: 0.592963, acc: 67.97%] [G loss: 1.980914]\n",
      "epoch:21 step:19947 [D loss: 0.632448, acc: 64.84%] [G loss: 1.845878]\n",
      "epoch:21 step:19948 [D loss: 0.652379, acc: 53.12%] [G loss: 1.884663]\n",
      "epoch:21 step:19949 [D loss: 0.623709, acc: 70.31%] [G loss: 1.963837]\n",
      "epoch:21 step:19950 [D loss: 0.661980, acc: 63.28%] [G loss: 1.830788]\n",
      "epoch:21 step:19951 [D loss: 0.601800, acc: 67.19%] [G loss: 1.921450]\n",
      "epoch:21 step:19952 [D loss: 0.601086, acc: 64.84%] [G loss: 2.007811]\n",
      "epoch:21 step:19953 [D loss: 0.609476, acc: 67.97%] [G loss: 2.000628]\n",
      "epoch:21 step:19954 [D loss: 0.652714, acc: 64.06%] [G loss: 1.927444]\n",
      "epoch:21 step:19955 [D loss: 0.665276, acc: 61.72%] [G loss: 1.799840]\n",
      "epoch:21 step:19956 [D loss: 0.638611, acc: 65.62%] [G loss: 1.822153]\n",
      "epoch:21 step:19957 [D loss: 0.624491, acc: 67.19%] [G loss: 1.893645]\n",
      "epoch:21 step:19958 [D loss: 0.654055, acc: 61.72%] [G loss: 1.895191]\n",
      "epoch:21 step:19959 [D loss: 0.642041, acc: 62.50%] [G loss: 1.813872]\n",
      "epoch:21 step:19960 [D loss: 0.619516, acc: 68.75%] [G loss: 1.833602]\n",
      "epoch:21 step:19961 [D loss: 0.619799, acc: 64.84%] [G loss: 1.954829]\n",
      "epoch:21 step:19962 [D loss: 0.641185, acc: 61.72%] [G loss: 1.844523]\n",
      "epoch:21 step:19963 [D loss: 0.627462, acc: 68.75%] [G loss: 1.966210]\n",
      "epoch:21 step:19964 [D loss: 0.659092, acc: 64.06%] [G loss: 1.794247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19965 [D loss: 0.647358, acc: 62.50%] [G loss: 2.005055]\n",
      "epoch:21 step:19966 [D loss: 0.629627, acc: 65.62%] [G loss: 2.018843]\n",
      "epoch:21 step:19967 [D loss: 0.619717, acc: 66.41%] [G loss: 1.779864]\n",
      "epoch:21 step:19968 [D loss: 0.637968, acc: 62.50%] [G loss: 1.974297]\n",
      "epoch:21 step:19969 [D loss: 0.659899, acc: 62.50%] [G loss: 1.969761]\n",
      "epoch:21 step:19970 [D loss: 0.686985, acc: 63.28%] [G loss: 1.975002]\n",
      "epoch:21 step:19971 [D loss: 0.630824, acc: 73.44%] [G loss: 1.890274]\n",
      "epoch:21 step:19972 [D loss: 0.641625, acc: 62.50%] [G loss: 1.783845]\n",
      "epoch:21 step:19973 [D loss: 0.670766, acc: 57.81%] [G loss: 1.984963]\n",
      "epoch:21 step:19974 [D loss: 0.625517, acc: 65.62%] [G loss: 1.964637]\n",
      "epoch:21 step:19975 [D loss: 0.613007, acc: 67.97%] [G loss: 2.119110]\n",
      "epoch:21 step:19976 [D loss: 0.649994, acc: 63.28%] [G loss: 1.816836]\n",
      "epoch:21 step:19977 [D loss: 0.633313, acc: 65.62%] [G loss: 1.934928]\n",
      "epoch:21 step:19978 [D loss: 0.628378, acc: 60.16%] [G loss: 1.902339]\n",
      "epoch:21 step:19979 [D loss: 0.570279, acc: 71.88%] [G loss: 1.971147]\n",
      "epoch:21 step:19980 [D loss: 0.583530, acc: 74.22%] [G loss: 1.983989]\n",
      "epoch:21 step:19981 [D loss: 0.620448, acc: 66.41%] [G loss: 1.922407]\n",
      "epoch:21 step:19982 [D loss: 0.658539, acc: 57.81%] [G loss: 1.893686]\n",
      "epoch:21 step:19983 [D loss: 0.623126, acc: 64.84%] [G loss: 1.934764]\n",
      "epoch:21 step:19984 [D loss: 0.631914, acc: 64.06%] [G loss: 1.961971]\n",
      "epoch:21 step:19985 [D loss: 0.635598, acc: 66.41%] [G loss: 1.783082]\n",
      "epoch:21 step:19986 [D loss: 0.665169, acc: 60.94%] [G loss: 1.965546]\n",
      "epoch:21 step:19987 [D loss: 0.687974, acc: 57.81%] [G loss: 1.783231]\n",
      "epoch:21 step:19988 [D loss: 0.694366, acc: 57.81%] [G loss: 1.758763]\n",
      "epoch:21 step:19989 [D loss: 0.596906, acc: 68.75%] [G loss: 2.066487]\n",
      "epoch:21 step:19990 [D loss: 0.644080, acc: 64.84%] [G loss: 2.054838]\n",
      "epoch:21 step:19991 [D loss: 0.602611, acc: 68.75%] [G loss: 2.204947]\n",
      "epoch:21 step:19992 [D loss: 0.600501, acc: 67.97%] [G loss: 2.045230]\n",
      "epoch:21 step:19993 [D loss: 0.706247, acc: 52.34%] [G loss: 1.766499]\n",
      "epoch:21 step:19994 [D loss: 0.635604, acc: 67.19%] [G loss: 1.812989]\n",
      "epoch:21 step:19995 [D loss: 0.618360, acc: 67.97%] [G loss: 1.887854]\n",
      "epoch:21 step:19996 [D loss: 0.643073, acc: 60.94%] [G loss: 1.836049]\n",
      "epoch:21 step:19997 [D loss: 0.654005, acc: 60.94%] [G loss: 1.826108]\n",
      "epoch:21 step:19998 [D loss: 0.639499, acc: 59.38%] [G loss: 1.951440]\n",
      "epoch:21 step:19999 [D loss: 0.593257, acc: 64.06%] [G loss: 1.840293]\n",
      "epoch:21 step:20000 [D loss: 0.721828, acc: 51.56%] [G loss: 1.811915]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.317034\n",
      "FID: 15.020992\n",
      "0 = 12.933719658374809\n",
      "1 = 0.10212609829072783\n",
      "2 = 0.8885999917984009\n",
      "3 = 0.902999997138977\n",
      "4 = 0.8741999864578247\n",
      "5 = 0.8777216076850891\n",
      "6 = 0.902999997138977\n",
      "7 = 6.499253780066986\n",
      "8 = 0.07532044934625443\n",
      "9 = 0.7148000001907349\n",
      "10 = 0.7232000231742859\n",
      "11 = 0.7063999772071838\n",
      "12 = 0.7112509608268738\n",
      "13 = 0.7232000231742859\n",
      "14 = 7.317060947418213\n",
      "15 = 9.305834770202637\n",
      "16 = 0.14633053541183472\n",
      "17 = 7.3170342445373535\n",
      "18 = 15.020992279052734\n",
      "epoch:21 step:20001 [D loss: 0.674901, acc: 60.16%] [G loss: 1.775503]\n",
      "epoch:21 step:20002 [D loss: 0.621714, acc: 64.06%] [G loss: 1.846665]\n",
      "epoch:21 step:20003 [D loss: 0.717781, acc: 58.59%] [G loss: 1.856936]\n",
      "epoch:21 step:20004 [D loss: 0.657877, acc: 64.06%] [G loss: 1.840550]\n",
      "epoch:21 step:20005 [D loss: 0.652599, acc: 64.06%] [G loss: 1.904302]\n",
      "epoch:21 step:20006 [D loss: 0.599397, acc: 69.53%] [G loss: 1.808462]\n",
      "epoch:21 step:20007 [D loss: 0.628857, acc: 67.19%] [G loss: 1.968912]\n",
      "epoch:21 step:20008 [D loss: 0.666446, acc: 60.16%] [G loss: 1.875229]\n",
      "epoch:21 step:20009 [D loss: 0.658882, acc: 59.38%] [G loss: 1.932596]\n",
      "epoch:21 step:20010 [D loss: 0.610546, acc: 66.41%] [G loss: 2.091604]\n",
      "epoch:21 step:20011 [D loss: 0.653125, acc: 62.50%] [G loss: 1.950056]\n",
      "epoch:21 step:20012 [D loss: 0.624894, acc: 64.06%] [G loss: 1.959493]\n",
      "epoch:21 step:20013 [D loss: 0.594495, acc: 69.53%] [G loss: 1.921547]\n",
      "epoch:21 step:20014 [D loss: 0.600026, acc: 68.75%] [G loss: 1.930709]\n",
      "epoch:21 step:20015 [D loss: 0.585861, acc: 69.53%] [G loss: 1.914997]\n",
      "epoch:21 step:20016 [D loss: 0.633319, acc: 72.66%] [G loss: 2.062208]\n",
      "epoch:21 step:20017 [D loss: 0.626359, acc: 65.62%] [G loss: 1.997942]\n",
      "epoch:21 step:20018 [D loss: 0.701064, acc: 52.34%] [G loss: 1.916212]\n",
      "epoch:21 step:20019 [D loss: 0.681941, acc: 57.03%] [G loss: 1.915098]\n",
      "epoch:21 step:20020 [D loss: 0.701323, acc: 49.22%] [G loss: 1.831668]\n",
      "epoch:21 step:20021 [D loss: 0.655268, acc: 57.81%] [G loss: 1.848275]\n",
      "epoch:21 step:20022 [D loss: 0.609987, acc: 67.19%] [G loss: 2.215144]\n",
      "epoch:21 step:20023 [D loss: 0.566985, acc: 75.00%] [G loss: 2.189792]\n",
      "epoch:21 step:20024 [D loss: 0.578972, acc: 65.62%] [G loss: 2.172229]\n",
      "epoch:21 step:20025 [D loss: 0.625558, acc: 67.97%] [G loss: 1.989535]\n",
      "epoch:21 step:20026 [D loss: 0.702326, acc: 57.03%] [G loss: 1.871294]\n",
      "epoch:21 step:20027 [D loss: 0.637253, acc: 64.06%] [G loss: 1.968526]\n",
      "epoch:21 step:20028 [D loss: 0.636548, acc: 64.06%] [G loss: 1.950879]\n",
      "epoch:21 step:20029 [D loss: 0.746524, acc: 50.78%] [G loss: 1.785233]\n",
      "epoch:21 step:20030 [D loss: 0.721825, acc: 57.81%] [G loss: 1.886959]\n",
      "epoch:21 step:20031 [D loss: 0.652958, acc: 59.38%] [G loss: 1.984913]\n",
      "epoch:21 step:20032 [D loss: 0.663231, acc: 57.03%] [G loss: 1.881345]\n",
      "epoch:21 step:20033 [D loss: 0.674711, acc: 57.03%] [G loss: 1.871753]\n",
      "epoch:21 step:20034 [D loss: 0.711865, acc: 53.91%] [G loss: 1.901191]\n",
      "epoch:21 step:20035 [D loss: 0.626149, acc: 64.06%] [G loss: 1.855382]\n",
      "epoch:21 step:20036 [D loss: 0.641134, acc: 67.97%] [G loss: 1.951491]\n",
      "epoch:21 step:20037 [D loss: 0.644896, acc: 61.72%] [G loss: 1.863226]\n",
      "epoch:21 step:20038 [D loss: 0.621436, acc: 67.19%] [G loss: 1.810984]\n",
      "epoch:21 step:20039 [D loss: 0.613522, acc: 65.62%] [G loss: 1.959798]\n",
      "epoch:21 step:20040 [D loss: 0.651582, acc: 62.50%] [G loss: 1.863315]\n",
      "epoch:21 step:20041 [D loss: 0.606992, acc: 75.00%] [G loss: 1.931597]\n",
      "epoch:21 step:20042 [D loss: 0.738146, acc: 56.25%] [G loss: 1.772385]\n",
      "epoch:21 step:20043 [D loss: 0.686425, acc: 58.59%] [G loss: 1.947765]\n",
      "epoch:21 step:20044 [D loss: 0.610742, acc: 64.06%] [G loss: 1.981966]\n",
      "epoch:21 step:20045 [D loss: 0.634772, acc: 63.28%] [G loss: 1.787713]\n",
      "epoch:21 step:20046 [D loss: 0.660711, acc: 64.06%] [G loss: 1.842882]\n",
      "epoch:21 step:20047 [D loss: 0.658428, acc: 61.72%] [G loss: 1.992576]\n",
      "epoch:21 step:20048 [D loss: 0.590365, acc: 71.09%] [G loss: 2.071458]\n",
      "epoch:21 step:20049 [D loss: 0.674084, acc: 56.25%] [G loss: 1.947689]\n",
      "epoch:21 step:20050 [D loss: 0.632087, acc: 66.41%] [G loss: 1.765617]\n",
      "epoch:21 step:20051 [D loss: 0.625440, acc: 65.62%] [G loss: 1.928577]\n",
      "epoch:21 step:20052 [D loss: 0.681210, acc: 61.72%] [G loss: 1.885644]\n",
      "epoch:21 step:20053 [D loss: 0.704836, acc: 57.81%] [G loss: 1.945739]\n",
      "epoch:21 step:20054 [D loss: 0.729642, acc: 53.91%] [G loss: 1.754288]\n",
      "epoch:21 step:20055 [D loss: 0.622765, acc: 60.94%] [G loss: 1.783574]\n",
      "epoch:21 step:20056 [D loss: 0.635820, acc: 61.72%] [G loss: 1.873908]\n",
      "epoch:21 step:20057 [D loss: 0.603335, acc: 69.53%] [G loss: 1.888861]\n",
      "epoch:21 step:20058 [D loss: 0.613405, acc: 63.28%] [G loss: 1.877383]\n",
      "epoch:21 step:20059 [D loss: 0.630798, acc: 64.06%] [G loss: 1.784536]\n",
      "epoch:21 step:20060 [D loss: 0.657460, acc: 63.28%] [G loss: 1.985638]\n",
      "epoch:21 step:20061 [D loss: 0.664309, acc: 54.69%] [G loss: 1.996679]\n",
      "epoch:21 step:20062 [D loss: 0.630375, acc: 62.50%] [G loss: 1.906375]\n",
      "epoch:21 step:20063 [D loss: 0.665377, acc: 62.50%] [G loss: 1.824503]\n",
      "epoch:21 step:20064 [D loss: 0.702843, acc: 55.47%] [G loss: 1.802023]\n",
      "epoch:21 step:20065 [D loss: 0.686692, acc: 54.69%] [G loss: 1.842655]\n",
      "epoch:21 step:20066 [D loss: 0.669090, acc: 57.03%] [G loss: 1.864001]\n",
      "epoch:21 step:20067 [D loss: 0.591447, acc: 66.41%] [G loss: 1.872644]\n",
      "epoch:21 step:20068 [D loss: 0.649400, acc: 64.06%] [G loss: 1.793500]\n",
      "epoch:21 step:20069 [D loss: 0.658980, acc: 56.25%] [G loss: 1.832329]\n",
      "epoch:21 step:20070 [D loss: 0.653195, acc: 60.16%] [G loss: 1.970214]\n",
      "epoch:21 step:20071 [D loss: 0.601662, acc: 67.19%] [G loss: 1.966114]\n",
      "epoch:21 step:20072 [D loss: 0.628590, acc: 60.94%] [G loss: 1.948503]\n",
      "epoch:21 step:20073 [D loss: 0.669005, acc: 59.38%] [G loss: 1.804951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20074 [D loss: 0.660725, acc: 59.38%] [G loss: 1.758708]\n",
      "epoch:21 step:20075 [D loss: 0.625565, acc: 67.97%] [G loss: 1.933577]\n",
      "epoch:21 step:20076 [D loss: 0.625574, acc: 68.75%] [G loss: 1.880285]\n",
      "epoch:21 step:20077 [D loss: 0.652994, acc: 60.16%] [G loss: 1.796690]\n",
      "epoch:21 step:20078 [D loss: 0.655492, acc: 57.81%] [G loss: 1.833333]\n",
      "epoch:21 step:20079 [D loss: 0.648055, acc: 61.72%] [G loss: 1.911082]\n",
      "epoch:21 step:20080 [D loss: 0.627465, acc: 61.72%] [G loss: 2.033189]\n",
      "epoch:21 step:20081 [D loss: 0.639148, acc: 66.41%] [G loss: 1.903334]\n",
      "epoch:21 step:20082 [D loss: 0.560449, acc: 68.75%] [G loss: 2.029193]\n",
      "epoch:21 step:20083 [D loss: 0.596043, acc: 64.84%] [G loss: 1.973658]\n",
      "epoch:21 step:20084 [D loss: 0.625777, acc: 65.62%] [G loss: 2.103958]\n",
      "epoch:21 step:20085 [D loss: 0.656068, acc: 58.59%] [G loss: 2.012269]\n",
      "epoch:21 step:20086 [D loss: 0.699093, acc: 61.72%] [G loss: 2.006883]\n",
      "epoch:21 step:20087 [D loss: 0.644330, acc: 66.41%] [G loss: 1.990206]\n",
      "epoch:21 step:20088 [D loss: 0.679240, acc: 61.72%] [G loss: 1.823985]\n",
      "epoch:21 step:20089 [D loss: 0.677985, acc: 60.16%] [G loss: 1.863930]\n",
      "epoch:21 step:20090 [D loss: 0.628748, acc: 64.06%] [G loss: 2.032482]\n",
      "epoch:21 step:20091 [D loss: 0.607523, acc: 71.09%] [G loss: 2.023630]\n",
      "epoch:21 step:20092 [D loss: 0.611336, acc: 66.41%] [G loss: 2.043213]\n",
      "epoch:21 step:20093 [D loss: 0.585367, acc: 68.75%] [G loss: 2.230908]\n",
      "epoch:21 step:20094 [D loss: 0.690674, acc: 59.38%] [G loss: 1.998194]\n",
      "epoch:21 step:20095 [D loss: 0.696531, acc: 51.56%] [G loss: 1.791963]\n",
      "epoch:21 step:20096 [D loss: 0.631269, acc: 64.84%] [G loss: 1.865952]\n",
      "epoch:21 step:20097 [D loss: 0.709876, acc: 59.38%] [G loss: 1.923788]\n",
      "epoch:21 step:20098 [D loss: 0.663117, acc: 61.72%] [G loss: 1.785045]\n",
      "epoch:21 step:20099 [D loss: 0.620668, acc: 64.06%] [G loss: 1.894207]\n",
      "epoch:21 step:20100 [D loss: 0.683237, acc: 57.81%] [G loss: 1.801599]\n",
      "epoch:21 step:20101 [D loss: 0.683257, acc: 55.47%] [G loss: 1.854535]\n",
      "epoch:21 step:20102 [D loss: 0.654267, acc: 60.94%] [G loss: 1.887772]\n",
      "epoch:21 step:20103 [D loss: 0.677050, acc: 52.34%] [G loss: 1.848609]\n",
      "epoch:21 step:20104 [D loss: 0.628028, acc: 66.41%] [G loss: 1.871560]\n",
      "epoch:21 step:20105 [D loss: 0.600754, acc: 71.09%] [G loss: 2.023999]\n",
      "epoch:21 step:20106 [D loss: 0.595914, acc: 67.19%] [G loss: 1.966103]\n",
      "epoch:21 step:20107 [D loss: 0.665856, acc: 65.62%] [G loss: 1.983025]\n",
      "epoch:21 step:20108 [D loss: 0.563069, acc: 74.22%] [G loss: 1.966901]\n",
      "epoch:21 step:20109 [D loss: 0.729750, acc: 50.00%] [G loss: 1.762988]\n",
      "epoch:21 step:20110 [D loss: 0.612577, acc: 65.62%] [G loss: 1.881827]\n",
      "epoch:21 step:20111 [D loss: 0.555464, acc: 75.00%] [G loss: 2.137519]\n",
      "epoch:21 step:20112 [D loss: 0.635463, acc: 66.41%] [G loss: 1.978384]\n",
      "epoch:21 step:20113 [D loss: 0.655258, acc: 57.81%] [G loss: 2.059184]\n",
      "epoch:21 step:20114 [D loss: 0.680408, acc: 54.69%] [G loss: 1.733646]\n",
      "epoch:21 step:20115 [D loss: 0.657500, acc: 65.62%] [G loss: 1.807123]\n",
      "epoch:21 step:20116 [D loss: 0.683467, acc: 56.25%] [G loss: 1.761327]\n",
      "epoch:21 step:20117 [D loss: 0.655025, acc: 56.25%] [G loss: 1.845700]\n",
      "epoch:21 step:20118 [D loss: 0.646012, acc: 60.94%] [G loss: 1.754864]\n",
      "epoch:21 step:20119 [D loss: 0.712826, acc: 57.03%] [G loss: 1.757407]\n",
      "epoch:21 step:20120 [D loss: 0.712789, acc: 55.47%] [G loss: 1.758808]\n",
      "epoch:21 step:20121 [D loss: 0.678396, acc: 57.03%] [G loss: 1.728956]\n",
      "epoch:21 step:20122 [D loss: 0.652412, acc: 67.19%] [G loss: 1.722388]\n",
      "epoch:21 step:20123 [D loss: 0.643265, acc: 62.50%] [G loss: 1.840697]\n",
      "epoch:21 step:20124 [D loss: 0.625070, acc: 65.62%] [G loss: 1.793008]\n",
      "epoch:21 step:20125 [D loss: 0.669897, acc: 54.69%] [G loss: 1.785491]\n",
      "epoch:21 step:20126 [D loss: 0.651906, acc: 60.94%] [G loss: 1.866993]\n",
      "epoch:21 step:20127 [D loss: 0.680535, acc: 59.38%] [G loss: 1.796794]\n",
      "epoch:21 step:20128 [D loss: 0.622506, acc: 63.28%] [G loss: 1.910017]\n",
      "epoch:21 step:20129 [D loss: 0.651801, acc: 61.72%] [G loss: 1.893010]\n",
      "epoch:21 step:20130 [D loss: 0.608530, acc: 67.97%] [G loss: 1.929144]\n",
      "epoch:21 step:20131 [D loss: 0.679489, acc: 56.25%] [G loss: 1.827703]\n",
      "epoch:21 step:20132 [D loss: 0.660383, acc: 63.28%] [G loss: 1.856554]\n",
      "epoch:21 step:20133 [D loss: 0.650731, acc: 61.72%] [G loss: 1.781915]\n",
      "epoch:21 step:20134 [D loss: 0.601733, acc: 67.97%] [G loss: 1.782348]\n",
      "epoch:21 step:20135 [D loss: 0.659583, acc: 58.59%] [G loss: 1.755075]\n",
      "epoch:21 step:20136 [D loss: 0.717101, acc: 55.47%] [G loss: 1.797456]\n",
      "epoch:21 step:20137 [D loss: 0.683476, acc: 54.69%] [G loss: 1.695337]\n",
      "epoch:21 step:20138 [D loss: 0.644400, acc: 62.50%] [G loss: 1.869000]\n",
      "epoch:21 step:20139 [D loss: 0.650475, acc: 62.50%] [G loss: 1.830420]\n",
      "epoch:21 step:20140 [D loss: 0.675282, acc: 57.81%] [G loss: 1.856047]\n",
      "epoch:21 step:20141 [D loss: 0.637499, acc: 67.97%] [G loss: 1.941503]\n",
      "epoch:21 step:20142 [D loss: 0.661655, acc: 54.69%] [G loss: 1.693700]\n",
      "epoch:21 step:20143 [D loss: 0.618821, acc: 68.75%] [G loss: 1.864891]\n",
      "epoch:21 step:20144 [D loss: 0.658144, acc: 57.81%] [G loss: 1.882572]\n",
      "epoch:21 step:20145 [D loss: 0.626503, acc: 64.06%] [G loss: 1.975421]\n",
      "epoch:21 step:20146 [D loss: 0.631247, acc: 67.19%] [G loss: 2.010968]\n",
      "epoch:21 step:20147 [D loss: 0.653280, acc: 66.41%] [G loss: 2.078870]\n",
      "epoch:21 step:20148 [D loss: 0.600364, acc: 68.75%] [G loss: 2.131840]\n",
      "epoch:21 step:20149 [D loss: 0.594274, acc: 68.75%] [G loss: 2.091429]\n",
      "epoch:21 step:20150 [D loss: 0.724751, acc: 55.47%] [G loss: 1.811685]\n",
      "epoch:21 step:20151 [D loss: 0.642902, acc: 67.19%] [G loss: 2.008565]\n",
      "epoch:21 step:20152 [D loss: 0.667771, acc: 61.72%] [G loss: 1.827342]\n",
      "epoch:21 step:20153 [D loss: 0.614441, acc: 66.41%] [G loss: 1.906258]\n",
      "epoch:21 step:20154 [D loss: 0.642393, acc: 64.06%] [G loss: 1.688390]\n",
      "epoch:21 step:20155 [D loss: 0.710973, acc: 53.91%] [G loss: 1.927698]\n",
      "epoch:21 step:20156 [D loss: 0.645250, acc: 62.50%] [G loss: 1.937553]\n",
      "epoch:21 step:20157 [D loss: 0.613119, acc: 65.62%] [G loss: 1.950131]\n",
      "epoch:21 step:20158 [D loss: 0.615542, acc: 67.97%] [G loss: 2.007801]\n",
      "epoch:21 step:20159 [D loss: 0.711050, acc: 54.69%] [G loss: 1.715179]\n",
      "epoch:21 step:20160 [D loss: 0.720268, acc: 56.25%] [G loss: 1.761945]\n",
      "epoch:21 step:20161 [D loss: 0.631274, acc: 64.84%] [G loss: 1.895707]\n",
      "epoch:21 step:20162 [D loss: 0.672701, acc: 62.50%] [G loss: 1.916577]\n",
      "epoch:21 step:20163 [D loss: 0.641892, acc: 59.38%] [G loss: 1.790257]\n",
      "epoch:21 step:20164 [D loss: 0.669900, acc: 61.72%] [G loss: 1.964959]\n",
      "epoch:21 step:20165 [D loss: 0.651118, acc: 62.50%] [G loss: 2.020799]\n",
      "epoch:21 step:20166 [D loss: 0.678030, acc: 57.81%] [G loss: 1.744536]\n",
      "epoch:21 step:20167 [D loss: 0.649115, acc: 60.94%] [G loss: 1.854130]\n",
      "epoch:21 step:20168 [D loss: 0.687005, acc: 57.81%] [G loss: 1.783671]\n",
      "epoch:21 step:20169 [D loss: 0.631536, acc: 67.19%] [G loss: 1.895987]\n",
      "epoch:21 step:20170 [D loss: 0.568531, acc: 71.09%] [G loss: 1.818551]\n",
      "epoch:21 step:20171 [D loss: 0.639378, acc: 61.72%] [G loss: 1.929246]\n",
      "epoch:21 step:20172 [D loss: 0.642301, acc: 60.16%] [G loss: 1.988034]\n",
      "epoch:21 step:20173 [D loss: 0.650739, acc: 68.75%] [G loss: 1.942281]\n",
      "epoch:21 step:20174 [D loss: 0.574850, acc: 70.31%] [G loss: 2.077883]\n",
      "epoch:21 step:20175 [D loss: 0.607245, acc: 68.75%] [G loss: 1.974573]\n",
      "epoch:21 step:20176 [D loss: 0.604261, acc: 68.75%] [G loss: 2.150551]\n",
      "epoch:21 step:20177 [D loss: 0.697351, acc: 58.59%] [G loss: 1.920323]\n",
      "epoch:21 step:20178 [D loss: 0.726605, acc: 52.34%] [G loss: 1.707915]\n",
      "epoch:21 step:20179 [D loss: 0.669562, acc: 56.25%] [G loss: 1.707859]\n",
      "epoch:21 step:20180 [D loss: 0.647600, acc: 64.06%] [G loss: 1.783409]\n",
      "epoch:21 step:20181 [D loss: 0.637772, acc: 63.28%] [G loss: 1.934894]\n",
      "epoch:21 step:20182 [D loss: 0.636835, acc: 64.84%] [G loss: 1.832311]\n",
      "epoch:21 step:20183 [D loss: 0.708141, acc: 51.56%] [G loss: 1.677167]\n",
      "epoch:21 step:20184 [D loss: 0.630410, acc: 63.28%] [G loss: 1.761219]\n",
      "epoch:21 step:20185 [D loss: 0.641644, acc: 59.38%] [G loss: 1.989815]\n",
      "epoch:21 step:20186 [D loss: 0.634920, acc: 64.06%] [G loss: 2.012332]\n",
      "epoch:21 step:20187 [D loss: 0.656523, acc: 59.38%] [G loss: 1.819036]\n",
      "epoch:21 step:20188 [D loss: 0.661075, acc: 60.94%] [G loss: 1.750846]\n",
      "epoch:21 step:20189 [D loss: 0.650964, acc: 60.16%] [G loss: 1.810951]\n",
      "epoch:21 step:20190 [D loss: 0.645857, acc: 60.16%] [G loss: 1.872688]\n",
      "epoch:21 step:20191 [D loss: 0.683639, acc: 57.03%] [G loss: 1.850871]\n",
      "epoch:21 step:20192 [D loss: 0.673035, acc: 60.16%] [G loss: 1.939424]\n",
      "epoch:21 step:20193 [D loss: 0.586506, acc: 72.66%] [G loss: 1.797394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20194 [D loss: 0.676712, acc: 57.03%] [G loss: 1.780938]\n",
      "epoch:21 step:20195 [D loss: 0.667106, acc: 60.16%] [G loss: 1.773338]\n",
      "epoch:21 step:20196 [D loss: 0.608447, acc: 67.97%] [G loss: 1.892919]\n",
      "epoch:21 step:20197 [D loss: 0.613277, acc: 68.75%] [G loss: 1.845179]\n",
      "epoch:21 step:20198 [D loss: 0.614034, acc: 64.06%] [G loss: 2.015221]\n",
      "epoch:21 step:20199 [D loss: 0.625535, acc: 62.50%] [G loss: 1.998167]\n",
      "epoch:21 step:20200 [D loss: 0.615127, acc: 71.09%] [G loss: 1.986897]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.445698\n",
      "FID: 12.322179\n",
      "0 = 12.625271256494488\n",
      "1 = 0.08310325969265833\n",
      "2 = 0.8791000247001648\n",
      "3 = 0.8942000269889832\n",
      "4 = 0.8640000224113464\n",
      "5 = 0.867986798286438\n",
      "6 = 0.8942000269889832\n",
      "7 = 6.314691982138158\n",
      "8 = 0.06422615546898074\n",
      "9 = 0.7132999897003174\n",
      "10 = 0.7218000292778015\n",
      "11 = 0.704800009727478\n",
      "12 = 0.7097344994544983\n",
      "13 = 0.7218000292778015\n",
      "14 = 7.445727348327637\n",
      "15 = 9.402007102966309\n",
      "16 = 0.12323977053165436\n",
      "17 = 7.445698261260986\n",
      "18 = 12.322178840637207\n",
      "epoch:21 step:20201 [D loss: 0.643822, acc: 60.94%] [G loss: 1.828938]\n",
      "epoch:21 step:20202 [D loss: 0.663649, acc: 57.03%] [G loss: 1.846573]\n",
      "epoch:21 step:20203 [D loss: 0.635340, acc: 63.28%] [G loss: 1.891097]\n",
      "epoch:21 step:20204 [D loss: 0.643755, acc: 62.50%] [G loss: 1.851113]\n",
      "epoch:21 step:20205 [D loss: 0.729095, acc: 53.91%] [G loss: 1.831548]\n",
      "epoch:21 step:20206 [D loss: 0.617483, acc: 66.41%] [G loss: 1.699801]\n",
      "epoch:21 step:20207 [D loss: 0.704342, acc: 56.25%] [G loss: 1.806056]\n",
      "epoch:21 step:20208 [D loss: 0.638764, acc: 58.59%] [G loss: 1.952371]\n",
      "epoch:21 step:20209 [D loss: 0.607804, acc: 64.06%] [G loss: 1.931153]\n",
      "epoch:21 step:20210 [D loss: 0.659831, acc: 59.38%] [G loss: 1.923594]\n",
      "epoch:21 step:20211 [D loss: 0.603237, acc: 67.97%] [G loss: 1.953526]\n",
      "epoch:21 step:20212 [D loss: 0.595203, acc: 71.09%] [G loss: 1.911546]\n",
      "epoch:21 step:20213 [D loss: 0.633063, acc: 60.94%] [G loss: 1.991345]\n",
      "epoch:21 step:20214 [D loss: 0.636457, acc: 61.72%] [G loss: 1.835953]\n",
      "epoch:21 step:20215 [D loss: 0.652589, acc: 61.72%] [G loss: 1.800457]\n",
      "epoch:21 step:20216 [D loss: 0.677637, acc: 59.38%] [G loss: 1.881584]\n",
      "epoch:21 step:20217 [D loss: 0.687724, acc: 57.03%] [G loss: 1.720657]\n",
      "epoch:21 step:20218 [D loss: 0.656873, acc: 58.59%] [G loss: 1.815300]\n",
      "epoch:21 step:20219 [D loss: 0.645991, acc: 64.06%] [G loss: 1.721663]\n",
      "epoch:21 step:20220 [D loss: 0.649554, acc: 61.72%] [G loss: 1.978055]\n",
      "epoch:21 step:20221 [D loss: 0.638768, acc: 64.06%] [G loss: 1.860056]\n",
      "epoch:21 step:20222 [D loss: 0.672424, acc: 57.03%] [G loss: 1.852052]\n",
      "epoch:21 step:20223 [D loss: 0.616968, acc: 67.19%] [G loss: 1.719234]\n",
      "epoch:21 step:20224 [D loss: 0.669633, acc: 63.28%] [G loss: 1.998972]\n",
      "epoch:21 step:20225 [D loss: 0.636747, acc: 61.72%] [G loss: 2.002258]\n",
      "epoch:21 step:20226 [D loss: 0.657859, acc: 60.16%] [G loss: 1.921605]\n",
      "epoch:21 step:20227 [D loss: 0.572076, acc: 69.53%] [G loss: 1.926455]\n",
      "epoch:21 step:20228 [D loss: 0.640174, acc: 64.84%] [G loss: 2.044181]\n",
      "epoch:21 step:20229 [D loss: 0.614348, acc: 64.84%] [G loss: 2.197015]\n",
      "epoch:21 step:20230 [D loss: 0.667753, acc: 57.81%] [G loss: 1.917953]\n",
      "epoch:21 step:20231 [D loss: 0.669477, acc: 63.28%] [G loss: 2.017944]\n",
      "epoch:21 step:20232 [D loss: 0.646418, acc: 60.94%] [G loss: 1.941848]\n",
      "epoch:21 step:20233 [D loss: 0.637029, acc: 64.06%] [G loss: 1.978927]\n",
      "epoch:21 step:20234 [D loss: 0.613184, acc: 66.41%] [G loss: 1.930465]\n",
      "epoch:21 step:20235 [D loss: 0.600094, acc: 70.31%] [G loss: 2.036460]\n",
      "epoch:21 step:20236 [D loss: 0.683368, acc: 52.34%] [G loss: 1.821414]\n",
      "epoch:21 step:20237 [D loss: 0.690736, acc: 59.38%] [G loss: 1.857895]\n",
      "epoch:21 step:20238 [D loss: 0.628313, acc: 60.16%] [G loss: 1.922518]\n",
      "epoch:21 step:20239 [D loss: 0.626189, acc: 67.19%] [G loss: 1.940708]\n",
      "epoch:21 step:20240 [D loss: 0.656997, acc: 61.72%] [G loss: 1.870590]\n",
      "epoch:21 step:20241 [D loss: 0.576281, acc: 72.66%] [G loss: 2.180756]\n",
      "epoch:21 step:20242 [D loss: 0.719751, acc: 55.47%] [G loss: 1.733854]\n",
      "epoch:21 step:20243 [D loss: 0.649948, acc: 57.03%] [G loss: 1.746372]\n",
      "epoch:21 step:20244 [D loss: 0.648889, acc: 58.59%] [G loss: 1.765964]\n",
      "epoch:21 step:20245 [D loss: 0.708565, acc: 53.91%] [G loss: 1.811910]\n",
      "epoch:21 step:20246 [D loss: 0.651947, acc: 57.81%] [G loss: 1.762989]\n",
      "epoch:21 step:20247 [D loss: 0.640102, acc: 63.28%] [G loss: 1.895198]\n",
      "epoch:21 step:20248 [D loss: 0.624355, acc: 67.97%] [G loss: 1.971163]\n",
      "epoch:21 step:20249 [D loss: 0.657748, acc: 60.16%] [G loss: 1.801963]\n",
      "epoch:21 step:20250 [D loss: 0.627791, acc: 67.19%] [G loss: 1.833230]\n",
      "epoch:21 step:20251 [D loss: 0.612228, acc: 64.06%] [G loss: 2.029630]\n",
      "epoch:21 step:20252 [D loss: 0.642827, acc: 67.19%] [G loss: 1.902326]\n",
      "epoch:21 step:20253 [D loss: 0.671631, acc: 60.16%] [G loss: 1.730181]\n",
      "epoch:21 step:20254 [D loss: 0.698190, acc: 56.25%] [G loss: 1.708719]\n",
      "epoch:21 step:20255 [D loss: 0.661330, acc: 60.94%] [G loss: 1.898149]\n",
      "epoch:21 step:20256 [D loss: 0.631855, acc: 64.84%] [G loss: 1.732837]\n",
      "epoch:21 step:20257 [D loss: 0.682997, acc: 59.38%] [G loss: 1.748030]\n",
      "epoch:21 step:20258 [D loss: 0.638397, acc: 63.28%] [G loss: 1.871042]\n",
      "epoch:21 step:20259 [D loss: 0.623399, acc: 67.19%] [G loss: 1.902637]\n",
      "epoch:21 step:20260 [D loss: 0.644436, acc: 60.94%] [G loss: 1.882649]\n",
      "epoch:21 step:20261 [D loss: 0.708403, acc: 55.47%] [G loss: 1.765510]\n",
      "epoch:21 step:20262 [D loss: 0.671285, acc: 65.62%] [G loss: 1.821786]\n",
      "epoch:21 step:20263 [D loss: 0.679681, acc: 59.38%] [G loss: 1.837475]\n",
      "epoch:21 step:20264 [D loss: 0.680489, acc: 54.69%] [G loss: 1.804320]\n",
      "epoch:21 step:20265 [D loss: 0.614969, acc: 69.53%] [G loss: 2.192394]\n",
      "epoch:21 step:20266 [D loss: 0.676435, acc: 55.47%] [G loss: 1.986595]\n",
      "epoch:21 step:20267 [D loss: 0.646660, acc: 60.94%] [G loss: 1.882361]\n",
      "epoch:21 step:20268 [D loss: 0.616990, acc: 68.75%] [G loss: 2.001538]\n",
      "epoch:21 step:20269 [D loss: 0.649131, acc: 60.94%] [G loss: 1.954022]\n",
      "epoch:21 step:20270 [D loss: 0.638332, acc: 61.72%] [G loss: 1.838033]\n",
      "epoch:21 step:20271 [D loss: 0.660180, acc: 56.25%] [G loss: 1.870324]\n",
      "epoch:21 step:20272 [D loss: 0.600275, acc: 67.19%] [G loss: 1.831131]\n",
      "epoch:21 step:20273 [D loss: 0.719271, acc: 52.34%] [G loss: 1.841247]\n",
      "epoch:21 step:20274 [D loss: 0.693264, acc: 60.16%] [G loss: 1.836225]\n",
      "epoch:21 step:20275 [D loss: 0.604980, acc: 66.41%] [G loss: 1.960592]\n",
      "epoch:21 step:20276 [D loss: 0.649275, acc: 59.38%] [G loss: 1.870482]\n",
      "epoch:21 step:20277 [D loss: 0.645944, acc: 57.81%] [G loss: 1.923163]\n",
      "epoch:21 step:20278 [D loss: 0.674648, acc: 60.16%] [G loss: 1.965838]\n",
      "epoch:21 step:20279 [D loss: 0.608445, acc: 69.53%] [G loss: 1.812315]\n",
      "epoch:21 step:20280 [D loss: 0.643175, acc: 61.72%] [G loss: 2.061136]\n",
      "epoch:21 step:20281 [D loss: 0.678418, acc: 58.59%] [G loss: 1.715163]\n",
      "epoch:21 step:20282 [D loss: 0.610840, acc: 67.97%] [G loss: 1.900944]\n",
      "epoch:21 step:20283 [D loss: 0.615159, acc: 65.62%] [G loss: 1.850607]\n",
      "epoch:21 step:20284 [D loss: 0.627108, acc: 65.62%] [G loss: 1.893917]\n",
      "epoch:21 step:20285 [D loss: 0.713897, acc: 50.00%] [G loss: 1.734614]\n",
      "epoch:21 step:20286 [D loss: 0.601878, acc: 69.53%] [G loss: 1.876927]\n",
      "epoch:21 step:20287 [D loss: 0.700337, acc: 59.38%] [G loss: 1.888843]\n",
      "epoch:21 step:20288 [D loss: 0.646223, acc: 57.81%] [G loss: 1.733141]\n",
      "epoch:21 step:20289 [D loss: 0.674691, acc: 60.16%] [G loss: 1.757878]\n",
      "epoch:21 step:20290 [D loss: 0.631217, acc: 60.16%] [G loss: 1.878742]\n",
      "epoch:21 step:20291 [D loss: 0.658328, acc: 60.16%] [G loss: 1.836480]\n",
      "epoch:21 step:20292 [D loss: 0.650497, acc: 58.59%] [G loss: 1.751679]\n",
      "epoch:21 step:20293 [D loss: 0.652303, acc: 59.38%] [G loss: 1.819664]\n",
      "epoch:21 step:20294 [D loss: 0.609215, acc: 70.31%] [G loss: 1.759158]\n",
      "epoch:21 step:20295 [D loss: 0.613340, acc: 66.41%] [G loss: 1.869096]\n",
      "epoch:21 step:20296 [D loss: 0.654351, acc: 65.62%] [G loss: 1.874349]\n",
      "epoch:21 step:20297 [D loss: 0.610958, acc: 65.62%] [G loss: 1.867451]\n",
      "epoch:21 step:20298 [D loss: 0.663471, acc: 61.72%] [G loss: 1.821876]\n",
      "epoch:21 step:20299 [D loss: 0.662303, acc: 60.94%] [G loss: 1.872117]\n",
      "epoch:21 step:20300 [D loss: 0.622523, acc: 64.06%] [G loss: 1.827339]\n",
      "epoch:21 step:20301 [D loss: 0.611508, acc: 64.84%] [G loss: 2.028081]\n",
      "epoch:21 step:20302 [D loss: 0.648759, acc: 57.03%] [G loss: 1.783646]\n",
      "epoch:21 step:20303 [D loss: 0.676979, acc: 59.38%] [G loss: 1.762574]\n",
      "epoch:21 step:20304 [D loss: 0.634835, acc: 60.16%] [G loss: 1.812708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20305 [D loss: 0.701396, acc: 58.59%] [G loss: 1.652884]\n",
      "epoch:21 step:20306 [D loss: 0.640351, acc: 62.50%] [G loss: 1.938744]\n",
      "epoch:21 step:20307 [D loss: 0.668586, acc: 60.94%] [G loss: 1.957452]\n",
      "epoch:21 step:20308 [D loss: 0.651304, acc: 61.72%] [G loss: 1.961251]\n",
      "epoch:21 step:20309 [D loss: 0.657937, acc: 63.28%] [G loss: 1.908467]\n",
      "epoch:21 step:20310 [D loss: 0.632127, acc: 60.94%] [G loss: 1.880265]\n",
      "epoch:21 step:20311 [D loss: 0.602166, acc: 67.19%] [G loss: 1.970196]\n",
      "epoch:21 step:20312 [D loss: 0.638046, acc: 62.50%] [G loss: 1.937213]\n",
      "epoch:21 step:20313 [D loss: 0.665130, acc: 60.16%] [G loss: 1.906967]\n",
      "epoch:21 step:20314 [D loss: 0.627295, acc: 62.50%] [G loss: 1.880873]\n",
      "epoch:21 step:20315 [D loss: 0.601899, acc: 71.09%] [G loss: 2.087311]\n",
      "epoch:21 step:20316 [D loss: 0.603147, acc: 69.53%] [G loss: 1.965738]\n",
      "epoch:21 step:20317 [D loss: 0.635481, acc: 60.16%] [G loss: 1.897546]\n",
      "epoch:21 step:20318 [D loss: 0.627625, acc: 62.50%] [G loss: 1.926380]\n",
      "epoch:21 step:20319 [D loss: 0.613886, acc: 67.19%] [G loss: 2.007592]\n",
      "epoch:21 step:20320 [D loss: 0.579387, acc: 69.53%] [G loss: 1.955635]\n",
      "epoch:21 step:20321 [D loss: 0.703341, acc: 56.25%] [G loss: 1.903835]\n",
      "epoch:21 step:20322 [D loss: 0.618893, acc: 65.62%] [G loss: 2.006075]\n",
      "epoch:21 step:20323 [D loss: 0.672451, acc: 60.94%] [G loss: 1.913130]\n",
      "epoch:21 step:20324 [D loss: 0.662292, acc: 57.81%] [G loss: 2.070216]\n",
      "epoch:21 step:20325 [D loss: 0.585299, acc: 68.75%] [G loss: 2.262658]\n",
      "epoch:21 step:20326 [D loss: 0.615224, acc: 71.09%] [G loss: 2.148211]\n",
      "epoch:21 step:20327 [D loss: 0.636830, acc: 67.19%] [G loss: 2.212202]\n",
      "epoch:21 step:20328 [D loss: 0.658511, acc: 60.94%] [G loss: 1.993229]\n",
      "epoch:21 step:20329 [D loss: 0.658814, acc: 63.28%] [G loss: 1.873236]\n",
      "epoch:21 step:20330 [D loss: 0.643233, acc: 61.72%] [G loss: 2.011015]\n",
      "epoch:21 step:20331 [D loss: 0.621050, acc: 64.06%] [G loss: 1.984811]\n",
      "epoch:21 step:20332 [D loss: 0.707393, acc: 56.25%] [G loss: 1.868572]\n",
      "epoch:21 step:20333 [D loss: 0.688595, acc: 58.59%] [G loss: 1.765462]\n",
      "epoch:21 step:20334 [D loss: 0.663198, acc: 60.94%] [G loss: 1.723445]\n",
      "epoch:21 step:20335 [D loss: 0.682582, acc: 57.81%] [G loss: 1.811666]\n",
      "epoch:21 step:20336 [D loss: 0.623493, acc: 64.84%] [G loss: 1.879322]\n",
      "epoch:21 step:20337 [D loss: 0.668237, acc: 63.28%] [G loss: 1.824903]\n",
      "epoch:21 step:20338 [D loss: 0.643206, acc: 63.28%] [G loss: 1.887635]\n",
      "epoch:21 step:20339 [D loss: 0.614177, acc: 61.72%] [G loss: 1.866842]\n",
      "epoch:21 step:20340 [D loss: 0.650747, acc: 64.06%] [G loss: 1.849506]\n",
      "epoch:21 step:20341 [D loss: 0.687704, acc: 59.38%] [G loss: 1.848907]\n",
      "epoch:21 step:20342 [D loss: 0.655488, acc: 63.28%] [G loss: 1.842483]\n",
      "epoch:21 step:20343 [D loss: 0.659242, acc: 61.72%] [G loss: 1.837511]\n",
      "epoch:21 step:20344 [D loss: 0.714753, acc: 50.00%] [G loss: 1.793344]\n",
      "epoch:21 step:20345 [D loss: 0.633722, acc: 64.84%] [G loss: 1.850645]\n",
      "epoch:21 step:20346 [D loss: 0.681314, acc: 59.38%] [G loss: 1.814159]\n",
      "epoch:21 step:20347 [D loss: 0.675070, acc: 60.16%] [G loss: 1.823211]\n",
      "epoch:21 step:20348 [D loss: 0.696953, acc: 57.03%] [G loss: 1.875224]\n",
      "epoch:21 step:20349 [D loss: 0.641644, acc: 63.28%] [G loss: 1.803288]\n",
      "epoch:21 step:20350 [D loss: 0.668827, acc: 55.47%] [G loss: 1.838988]\n",
      "epoch:21 step:20351 [D loss: 0.692872, acc: 57.81%] [G loss: 1.823801]\n",
      "epoch:21 step:20352 [D loss: 0.656196, acc: 64.84%] [G loss: 1.731373]\n",
      "epoch:21 step:20353 [D loss: 0.666626, acc: 61.72%] [G loss: 1.828977]\n",
      "epoch:21 step:20354 [D loss: 0.631999, acc: 71.09%] [G loss: 1.760309]\n",
      "epoch:21 step:20355 [D loss: 0.611623, acc: 65.62%] [G loss: 1.908239]\n",
      "epoch:21 step:20356 [D loss: 0.608538, acc: 64.06%] [G loss: 1.858334]\n",
      "epoch:21 step:20357 [D loss: 0.688880, acc: 57.81%] [G loss: 1.924829]\n",
      "epoch:21 step:20358 [D loss: 0.570906, acc: 69.53%] [G loss: 1.885278]\n",
      "epoch:21 step:20359 [D loss: 0.644655, acc: 64.84%] [G loss: 1.781747]\n",
      "epoch:21 step:20360 [D loss: 0.609273, acc: 65.62%] [G loss: 1.904802]\n",
      "epoch:21 step:20361 [D loss: 0.667324, acc: 58.59%] [G loss: 1.715713]\n",
      "epoch:21 step:20362 [D loss: 0.652483, acc: 61.72%] [G loss: 1.865593]\n",
      "epoch:21 step:20363 [D loss: 0.639273, acc: 57.81%] [G loss: 1.953844]\n",
      "epoch:21 step:20364 [D loss: 0.656703, acc: 63.28%] [G loss: 1.921340]\n",
      "epoch:21 step:20365 [D loss: 0.641141, acc: 65.62%] [G loss: 1.963848]\n",
      "epoch:21 step:20366 [D loss: 0.649272, acc: 62.50%] [G loss: 2.079115]\n",
      "epoch:21 step:20367 [D loss: 0.617798, acc: 64.84%] [G loss: 2.082468]\n",
      "epoch:21 step:20368 [D loss: 0.608643, acc: 64.06%] [G loss: 1.932994]\n",
      "epoch:21 step:20369 [D loss: 0.627012, acc: 64.06%] [G loss: 2.079184]\n",
      "epoch:21 step:20370 [D loss: 0.642804, acc: 63.28%] [G loss: 1.884334]\n",
      "epoch:21 step:20371 [D loss: 0.636719, acc: 64.84%] [G loss: 1.996960]\n",
      "epoch:21 step:20372 [D loss: 0.608121, acc: 68.75%] [G loss: 2.047272]\n",
      "epoch:21 step:20373 [D loss: 0.631058, acc: 64.84%] [G loss: 1.916745]\n",
      "epoch:21 step:20374 [D loss: 0.591258, acc: 69.53%] [G loss: 1.919472]\n",
      "epoch:21 step:20375 [D loss: 0.638459, acc: 63.28%] [G loss: 1.993272]\n",
      "epoch:21 step:20376 [D loss: 0.656430, acc: 60.16%] [G loss: 2.009408]\n",
      "epoch:21 step:20377 [D loss: 0.613056, acc: 67.19%] [G loss: 1.842340]\n",
      "epoch:21 step:20378 [D loss: 0.599421, acc: 70.31%] [G loss: 2.057722]\n",
      "epoch:21 step:20379 [D loss: 0.660108, acc: 60.94%] [G loss: 1.786070]\n",
      "epoch:21 step:20380 [D loss: 0.666990, acc: 57.03%] [G loss: 1.873671]\n",
      "epoch:21 step:20381 [D loss: 0.695737, acc: 53.12%] [G loss: 1.855754]\n",
      "epoch:21 step:20382 [D loss: 0.670093, acc: 58.59%] [G loss: 1.930439]\n",
      "epoch:21 step:20383 [D loss: 0.651157, acc: 63.28%] [G loss: 2.027236]\n",
      "epoch:21 step:20384 [D loss: 0.643887, acc: 62.50%] [G loss: 2.050729]\n",
      "epoch:21 step:20385 [D loss: 0.564974, acc: 72.66%] [G loss: 2.091933]\n",
      "epoch:21 step:20386 [D loss: 0.661382, acc: 65.62%] [G loss: 1.935785]\n",
      "epoch:21 step:20387 [D loss: 0.657545, acc: 57.81%] [G loss: 1.759228]\n",
      "epoch:21 step:20388 [D loss: 0.664960, acc: 63.28%] [G loss: 1.897129]\n",
      "epoch:21 step:20389 [D loss: 0.630595, acc: 66.41%] [G loss: 1.839318]\n",
      "epoch:21 step:20390 [D loss: 0.671722, acc: 55.47%] [G loss: 1.850976]\n",
      "epoch:21 step:20391 [D loss: 0.652499, acc: 60.94%] [G loss: 1.840660]\n",
      "epoch:21 step:20392 [D loss: 0.725350, acc: 54.69%] [G loss: 1.751333]\n",
      "epoch:21 step:20393 [D loss: 0.657123, acc: 62.50%] [G loss: 1.735661]\n",
      "epoch:21 step:20394 [D loss: 0.653847, acc: 65.62%] [G loss: 1.789710]\n",
      "epoch:21 step:20395 [D loss: 0.634773, acc: 69.53%] [G loss: 1.835075]\n",
      "epoch:21 step:20396 [D loss: 0.644071, acc: 59.38%] [G loss: 2.005766]\n",
      "epoch:21 step:20397 [D loss: 0.637016, acc: 64.84%] [G loss: 1.926641]\n",
      "epoch:21 step:20398 [D loss: 0.627466, acc: 65.62%] [G loss: 2.012763]\n",
      "epoch:21 step:20399 [D loss: 0.660641, acc: 63.28%] [G loss: 1.740392]\n",
      "epoch:21 step:20400 [D loss: 0.621275, acc: 67.19%] [G loss: 2.013440]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.356392\n",
      "FID: 13.668366\n",
      "0 = 12.747125263452489\n",
      "1 = 0.08083009051343236\n",
      "2 = 0.8871999979019165\n",
      "3 = 0.9061999917030334\n",
      "4 = 0.8682000041007996\n",
      "5 = 0.8730250597000122\n",
      "6 = 0.9061999917030334\n",
      "7 = 6.485157270026189\n",
      "8 = 0.07272030867482761\n",
      "9 = 0.7185999751091003\n",
      "10 = 0.7289999723434448\n",
      "11 = 0.7081999778747559\n",
      "12 = 0.7141457796096802\n",
      "13 = 0.7289999723434448\n",
      "14 = 7.356426239013672\n",
      "15 = 9.439315795898438\n",
      "16 = 0.11394927650690079\n",
      "17 = 7.3563923835754395\n",
      "18 = 13.668366432189941\n",
      "epoch:21 step:20401 [D loss: 0.700613, acc: 53.12%] [G loss: 1.921961]\n",
      "epoch:21 step:20402 [D loss: 0.619805, acc: 67.97%] [G loss: 1.968050]\n",
      "epoch:21 step:20403 [D loss: 0.649687, acc: 61.72%] [G loss: 1.856557]\n",
      "epoch:21 step:20404 [D loss: 0.650247, acc: 65.62%] [G loss: 1.839839]\n",
      "epoch:21 step:20405 [D loss: 0.626321, acc: 66.41%] [G loss: 1.961626]\n",
      "epoch:21 step:20406 [D loss: 0.694978, acc: 54.69%] [G loss: 1.819183]\n",
      "epoch:21 step:20407 [D loss: 0.655138, acc: 57.81%] [G loss: 1.945766]\n",
      "epoch:21 step:20408 [D loss: 0.660701, acc: 60.94%] [G loss: 1.972445]\n",
      "epoch:21 step:20409 [D loss: 0.676452, acc: 53.12%] [G loss: 1.813048]\n",
      "epoch:21 step:20410 [D loss: 0.606196, acc: 68.75%] [G loss: 1.944057]\n",
      "epoch:21 step:20411 [D loss: 0.665973, acc: 57.81%] [G loss: 1.746443]\n",
      "epoch:21 step:20412 [D loss: 0.659249, acc: 62.50%] [G loss: 1.835325]\n",
      "epoch:21 step:20413 [D loss: 0.649700, acc: 57.03%] [G loss: 1.942395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20414 [D loss: 0.638181, acc: 62.50%] [G loss: 1.735138]\n",
      "epoch:21 step:20415 [D loss: 0.679480, acc: 60.94%] [G loss: 1.789783]\n",
      "epoch:21 step:20416 [D loss: 0.653785, acc: 63.28%] [G loss: 1.893187]\n",
      "epoch:21 step:20417 [D loss: 0.689176, acc: 55.47%] [G loss: 1.780905]\n",
      "epoch:21 step:20418 [D loss: 0.640624, acc: 65.62%] [G loss: 1.824371]\n",
      "epoch:21 step:20419 [D loss: 0.654537, acc: 57.81%] [G loss: 1.688550]\n",
      "epoch:21 step:20420 [D loss: 0.631111, acc: 60.94%] [G loss: 1.825634]\n",
      "epoch:21 step:20421 [D loss: 0.681681, acc: 60.94%] [G loss: 1.938154]\n",
      "epoch:21 step:20422 [D loss: 0.661441, acc: 57.03%] [G loss: 1.860660]\n",
      "epoch:21 step:20423 [D loss: 0.589420, acc: 70.31%] [G loss: 1.923126]\n",
      "epoch:21 step:20424 [D loss: 0.599626, acc: 67.19%] [G loss: 2.055347]\n",
      "epoch:21 step:20425 [D loss: 0.656973, acc: 61.72%] [G loss: 1.843936]\n",
      "epoch:21 step:20426 [D loss: 0.648297, acc: 64.06%] [G loss: 1.838381]\n",
      "epoch:21 step:20427 [D loss: 0.641617, acc: 60.16%] [G loss: 1.880444]\n",
      "epoch:21 step:20428 [D loss: 0.625296, acc: 66.41%] [G loss: 1.827579]\n",
      "epoch:21 step:20429 [D loss: 0.680226, acc: 51.56%] [G loss: 1.648731]\n",
      "epoch:21 step:20430 [D loss: 0.638669, acc: 59.38%] [G loss: 1.918257]\n",
      "epoch:21 step:20431 [D loss: 0.673581, acc: 58.59%] [G loss: 1.869730]\n",
      "epoch:21 step:20432 [D loss: 0.652016, acc: 60.94%] [G loss: 1.962592]\n",
      "epoch:21 step:20433 [D loss: 0.665634, acc: 63.28%] [G loss: 1.854612]\n",
      "epoch:21 step:20434 [D loss: 0.649579, acc: 62.50%] [G loss: 1.910252]\n",
      "epoch:21 step:20435 [D loss: 0.725650, acc: 53.12%] [G loss: 1.775331]\n",
      "epoch:21 step:20436 [D loss: 0.624840, acc: 67.19%] [G loss: 1.824138]\n",
      "epoch:21 step:20437 [D loss: 0.609119, acc: 66.41%] [G loss: 1.904916]\n",
      "epoch:21 step:20438 [D loss: 0.635291, acc: 62.50%] [G loss: 1.856881]\n",
      "epoch:21 step:20439 [D loss: 0.629095, acc: 64.84%] [G loss: 1.928322]\n",
      "epoch:21 step:20440 [D loss: 0.612583, acc: 62.50%] [G loss: 1.975746]\n",
      "epoch:21 step:20441 [D loss: 0.615851, acc: 66.41%] [G loss: 1.810360]\n",
      "epoch:21 step:20442 [D loss: 0.732166, acc: 52.34%] [G loss: 1.715978]\n",
      "epoch:21 step:20443 [D loss: 0.656999, acc: 64.06%] [G loss: 1.785826]\n",
      "epoch:21 step:20444 [D loss: 0.687317, acc: 64.06%] [G loss: 1.673686]\n",
      "epoch:21 step:20445 [D loss: 0.663363, acc: 64.06%] [G loss: 1.820153]\n",
      "epoch:21 step:20446 [D loss: 0.656337, acc: 59.38%] [G loss: 1.835893]\n",
      "epoch:21 step:20447 [D loss: 0.649516, acc: 58.59%] [G loss: 1.849600]\n",
      "epoch:21 step:20448 [D loss: 0.635555, acc: 64.84%] [G loss: 1.905776]\n",
      "epoch:21 step:20449 [D loss: 0.637083, acc: 63.28%] [G loss: 1.921499]\n",
      "epoch:21 step:20450 [D loss: 0.670429, acc: 60.16%] [G loss: 1.798921]\n",
      "epoch:21 step:20451 [D loss: 0.671965, acc: 60.16%] [G loss: 2.067946]\n",
      "epoch:21 step:20452 [D loss: 0.617273, acc: 67.19%] [G loss: 2.303732]\n",
      "epoch:21 step:20453 [D loss: 0.629849, acc: 63.28%] [G loss: 1.840617]\n",
      "epoch:21 step:20454 [D loss: 0.619496, acc: 67.19%] [G loss: 1.868399]\n",
      "epoch:21 step:20455 [D loss: 0.639896, acc: 63.28%] [G loss: 1.923375]\n",
      "epoch:21 step:20456 [D loss: 0.691339, acc: 58.59%] [G loss: 1.668832]\n",
      "epoch:21 step:20457 [D loss: 0.629043, acc: 64.84%] [G loss: 1.874540]\n",
      "epoch:21 step:20458 [D loss: 0.579468, acc: 70.31%] [G loss: 2.148989]\n",
      "epoch:21 step:20459 [D loss: 0.629109, acc: 65.62%] [G loss: 2.036715]\n",
      "epoch:21 step:20460 [D loss: 0.685333, acc: 56.25%] [G loss: 1.828623]\n",
      "epoch:21 step:20461 [D loss: 0.689968, acc: 56.25%] [G loss: 1.779103]\n",
      "epoch:21 step:20462 [D loss: 0.642404, acc: 60.94%] [G loss: 1.808706]\n",
      "epoch:21 step:20463 [D loss: 0.604990, acc: 65.62%] [G loss: 1.967599]\n",
      "epoch:21 step:20464 [D loss: 0.652070, acc: 64.84%] [G loss: 1.893137]\n",
      "epoch:21 step:20465 [D loss: 0.712151, acc: 56.25%] [G loss: 1.757536]\n",
      "epoch:21 step:20466 [D loss: 0.657901, acc: 60.16%] [G loss: 1.851823]\n",
      "epoch:21 step:20467 [D loss: 0.639664, acc: 63.28%] [G loss: 1.886461]\n",
      "epoch:21 step:20468 [D loss: 0.658980, acc: 60.16%] [G loss: 1.943351]\n",
      "epoch:21 step:20469 [D loss: 0.640445, acc: 62.50%] [G loss: 1.947499]\n",
      "epoch:21 step:20470 [D loss: 0.714492, acc: 57.03%] [G loss: 1.967938]\n",
      "epoch:21 step:20471 [D loss: 0.685824, acc: 50.00%] [G loss: 1.750289]\n",
      "epoch:21 step:20472 [D loss: 0.651020, acc: 63.28%] [G loss: 1.905950]\n",
      "epoch:21 step:20473 [D loss: 0.700273, acc: 52.34%] [G loss: 1.864391]\n",
      "epoch:21 step:20474 [D loss: 0.709591, acc: 56.25%] [G loss: 1.763769]\n",
      "epoch:21 step:20475 [D loss: 0.668107, acc: 64.84%] [G loss: 1.835634]\n",
      "epoch:21 step:20476 [D loss: 0.692706, acc: 54.69%] [G loss: 1.800671]\n",
      "epoch:21 step:20477 [D loss: 0.701580, acc: 51.56%] [G loss: 1.725698]\n",
      "epoch:21 step:20478 [D loss: 0.672024, acc: 59.38%] [G loss: 1.779788]\n",
      "epoch:21 step:20479 [D loss: 0.634852, acc: 67.19%] [G loss: 1.932945]\n",
      "epoch:21 step:20480 [D loss: 0.658676, acc: 59.38%] [G loss: 1.921334]\n",
      "epoch:21 step:20481 [D loss: 0.612092, acc: 67.19%] [G loss: 1.837087]\n",
      "epoch:21 step:20482 [D loss: 0.648445, acc: 60.94%] [G loss: 1.908942]\n",
      "epoch:21 step:20483 [D loss: 0.646183, acc: 64.06%] [G loss: 1.884125]\n",
      "epoch:21 step:20484 [D loss: 0.678076, acc: 60.94%] [G loss: 1.873244]\n",
      "epoch:21 step:20485 [D loss: 0.651100, acc: 59.38%] [G loss: 1.921944]\n",
      "epoch:21 step:20486 [D loss: 0.684945, acc: 57.81%] [G loss: 1.816313]\n",
      "epoch:21 step:20487 [D loss: 0.656644, acc: 57.03%] [G loss: 1.729283]\n",
      "epoch:21 step:20488 [D loss: 0.674464, acc: 56.25%] [G loss: 1.836792]\n",
      "epoch:21 step:20489 [D loss: 0.694565, acc: 54.69%] [G loss: 1.817414]\n",
      "epoch:21 step:20490 [D loss: 0.641589, acc: 64.06%] [G loss: 1.769610]\n",
      "epoch:21 step:20491 [D loss: 0.632962, acc: 66.41%] [G loss: 1.719333]\n",
      "epoch:21 step:20492 [D loss: 0.631733, acc: 68.75%] [G loss: 1.906497]\n",
      "epoch:21 step:20493 [D loss: 0.668869, acc: 54.69%] [G loss: 1.909782]\n",
      "epoch:21 step:20494 [D loss: 0.689664, acc: 55.47%] [G loss: 1.652132]\n",
      "epoch:21 step:20495 [D loss: 0.662828, acc: 59.38%] [G loss: 1.752306]\n",
      "epoch:21 step:20496 [D loss: 0.668693, acc: 60.94%] [G loss: 1.674218]\n",
      "epoch:21 step:20497 [D loss: 0.709707, acc: 56.25%] [G loss: 1.696680]\n",
      "epoch:21 step:20498 [D loss: 0.645309, acc: 60.94%] [G loss: 1.829465]\n",
      "epoch:21 step:20499 [D loss: 0.637379, acc: 64.84%] [G loss: 1.840755]\n",
      "epoch:21 step:20500 [D loss: 0.610711, acc: 65.62%] [G loss: 1.810590]\n",
      "epoch:21 step:20501 [D loss: 0.640534, acc: 67.19%] [G loss: 1.855145]\n",
      "epoch:21 step:20502 [D loss: 0.591080, acc: 69.53%] [G loss: 1.949320]\n",
      "epoch:21 step:20503 [D loss: 0.650270, acc: 60.94%] [G loss: 1.767545]\n",
      "epoch:21 step:20504 [D loss: 0.669811, acc: 53.91%] [G loss: 1.743681]\n",
      "epoch:21 step:20505 [D loss: 0.663744, acc: 62.50%] [G loss: 1.765182]\n",
      "epoch:21 step:20506 [D loss: 0.703656, acc: 54.69%] [G loss: 1.717935]\n",
      "epoch:21 step:20507 [D loss: 0.697996, acc: 55.47%] [G loss: 1.773297]\n",
      "epoch:21 step:20508 [D loss: 0.628039, acc: 70.31%] [G loss: 1.794102]\n",
      "epoch:21 step:20509 [D loss: 0.692980, acc: 61.72%] [G loss: 1.821120]\n",
      "epoch:21 step:20510 [D loss: 0.581001, acc: 67.19%] [G loss: 1.878190]\n",
      "epoch:21 step:20511 [D loss: 0.643383, acc: 62.50%] [G loss: 1.733675]\n",
      "epoch:21 step:20512 [D loss: 0.672116, acc: 63.28%] [G loss: 1.793970]\n",
      "epoch:21 step:20513 [D loss: 0.645398, acc: 58.59%] [G loss: 1.850514]\n",
      "epoch:21 step:20514 [D loss: 0.613766, acc: 67.19%] [G loss: 1.973531]\n",
      "epoch:21 step:20515 [D loss: 0.637434, acc: 60.16%] [G loss: 1.991778]\n",
      "epoch:21 step:20516 [D loss: 0.644552, acc: 63.28%] [G loss: 1.903908]\n",
      "epoch:21 step:20517 [D loss: 0.626039, acc: 64.06%] [G loss: 1.896270]\n",
      "epoch:21 step:20518 [D loss: 0.617738, acc: 64.06%] [G loss: 1.787949]\n",
      "epoch:21 step:20519 [D loss: 0.590577, acc: 66.41%] [G loss: 2.025607]\n",
      "epoch:21 step:20520 [D loss: 0.693646, acc: 59.38%] [G loss: 1.922741]\n",
      "epoch:21 step:20521 [D loss: 0.682760, acc: 61.72%] [G loss: 1.788972]\n",
      "epoch:21 step:20522 [D loss: 0.607123, acc: 67.19%] [G loss: 2.069126]\n",
      "epoch:21 step:20523 [D loss: 0.604266, acc: 69.53%] [G loss: 2.027003]\n",
      "epoch:21 step:20524 [D loss: 0.632398, acc: 68.75%] [G loss: 1.803071]\n",
      "epoch:21 step:20525 [D loss: 0.648768, acc: 59.38%] [G loss: 1.777463]\n",
      "epoch:21 step:20526 [D loss: 0.631244, acc: 64.84%] [G loss: 1.925717]\n",
      "epoch:21 step:20527 [D loss: 0.647077, acc: 64.84%] [G loss: 1.880560]\n",
      "epoch:21 step:20528 [D loss: 0.645878, acc: 64.06%] [G loss: 1.730718]\n",
      "epoch:21 step:20529 [D loss: 0.682759, acc: 55.47%] [G loss: 1.934925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20530 [D loss: 0.658812, acc: 65.62%] [G loss: 1.814988]\n",
      "epoch:21 step:20531 [D loss: 0.593653, acc: 67.97%] [G loss: 1.912901]\n",
      "epoch:21 step:20532 [D loss: 0.627896, acc: 65.62%] [G loss: 1.782063]\n",
      "epoch:21 step:20533 [D loss: 0.611418, acc: 67.19%] [G loss: 1.802440]\n",
      "epoch:21 step:20534 [D loss: 0.631515, acc: 58.59%] [G loss: 1.860792]\n",
      "epoch:21 step:20535 [D loss: 0.670737, acc: 58.59%] [G loss: 1.809196]\n",
      "epoch:21 step:20536 [D loss: 0.714928, acc: 58.59%] [G loss: 1.994941]\n",
      "epoch:21 step:20537 [D loss: 0.572863, acc: 69.53%] [G loss: 1.944953]\n",
      "epoch:21 step:20538 [D loss: 0.667518, acc: 60.94%] [G loss: 2.008149]\n",
      "epoch:21 step:20539 [D loss: 0.687795, acc: 54.69%] [G loss: 1.751152]\n",
      "epoch:21 step:20540 [D loss: 0.631375, acc: 60.16%] [G loss: 1.779671]\n",
      "epoch:21 step:20541 [D loss: 0.650139, acc: 60.94%] [G loss: 1.920809]\n",
      "epoch:21 step:20542 [D loss: 0.693277, acc: 55.47%] [G loss: 1.888213]\n",
      "epoch:21 step:20543 [D loss: 0.666820, acc: 63.28%] [G loss: 1.802010]\n",
      "epoch:21 step:20544 [D loss: 0.612744, acc: 71.09%] [G loss: 1.934246]\n",
      "epoch:21 step:20545 [D loss: 0.657871, acc: 59.38%] [G loss: 1.936392]\n",
      "epoch:21 step:20546 [D loss: 0.650156, acc: 60.16%] [G loss: 1.807799]\n",
      "epoch:21 step:20547 [D loss: 0.652159, acc: 60.16%] [G loss: 1.915803]\n",
      "epoch:21 step:20548 [D loss: 0.661989, acc: 60.16%] [G loss: 2.126870]\n",
      "epoch:21 step:20549 [D loss: 0.640429, acc: 67.97%] [G loss: 1.786197]\n",
      "epoch:21 step:20550 [D loss: 0.649513, acc: 64.06%] [G loss: 1.821426]\n",
      "epoch:21 step:20551 [D loss: 0.678690, acc: 58.59%] [G loss: 1.917871]\n",
      "epoch:21 step:20552 [D loss: 0.634690, acc: 66.41%] [G loss: 2.000717]\n",
      "epoch:21 step:20553 [D loss: 0.653506, acc: 60.94%] [G loss: 1.817189]\n",
      "epoch:21 step:20554 [D loss: 0.637064, acc: 65.62%] [G loss: 1.855626]\n",
      "epoch:21 step:20555 [D loss: 0.689744, acc: 55.47%] [G loss: 1.839747]\n",
      "epoch:21 step:20556 [D loss: 0.637174, acc: 64.06%] [G loss: 1.721317]\n",
      "epoch:21 step:20557 [D loss: 0.644633, acc: 62.50%] [G loss: 1.826296]\n",
      "epoch:21 step:20558 [D loss: 0.674468, acc: 58.59%] [G loss: 1.813043]\n",
      "epoch:21 step:20559 [D loss: 0.636330, acc: 67.97%] [G loss: 1.941170]\n",
      "epoch:21 step:20560 [D loss: 0.672271, acc: 57.81%] [G loss: 1.696138]\n",
      "epoch:21 step:20561 [D loss: 0.653104, acc: 61.72%] [G loss: 2.052064]\n",
      "epoch:21 step:20562 [D loss: 0.678447, acc: 64.06%] [G loss: 1.797606]\n",
      "epoch:21 step:20563 [D loss: 0.658802, acc: 61.72%] [G loss: 1.940470]\n",
      "epoch:21 step:20564 [D loss: 0.615774, acc: 65.62%] [G loss: 1.871481]\n",
      "epoch:21 step:20565 [D loss: 0.662420, acc: 64.84%] [G loss: 1.893738]\n",
      "epoch:21 step:20566 [D loss: 0.663076, acc: 63.28%] [G loss: 1.881844]\n",
      "epoch:21 step:20567 [D loss: 0.652745, acc: 61.72%] [G loss: 1.909907]\n",
      "epoch:21 step:20568 [D loss: 0.675644, acc: 56.25%] [G loss: 1.927565]\n",
      "epoch:21 step:20569 [D loss: 0.709756, acc: 57.03%] [G loss: 1.918854]\n",
      "epoch:21 step:20570 [D loss: 0.651946, acc: 60.94%] [G loss: 1.832601]\n",
      "epoch:21 step:20571 [D loss: 0.639738, acc: 62.50%] [G loss: 1.800590]\n",
      "epoch:21 step:20572 [D loss: 0.675648, acc: 60.94%] [G loss: 1.731123]\n",
      "epoch:21 step:20573 [D loss: 0.692012, acc: 60.16%] [G loss: 1.739948]\n",
      "epoch:21 step:20574 [D loss: 0.621415, acc: 62.50%] [G loss: 1.840133]\n",
      "epoch:21 step:20575 [D loss: 0.627250, acc: 66.41%] [G loss: 1.927918]\n",
      "epoch:21 step:20576 [D loss: 0.579491, acc: 71.88%] [G loss: 1.997234]\n",
      "epoch:21 step:20577 [D loss: 0.710975, acc: 59.38%] [G loss: 1.775719]\n",
      "epoch:21 step:20578 [D loss: 0.656748, acc: 60.94%] [G loss: 1.898276]\n",
      "epoch:21 step:20579 [D loss: 0.632861, acc: 65.62%] [G loss: 1.849209]\n",
      "epoch:21 step:20580 [D loss: 0.650786, acc: 67.97%] [G loss: 1.777746]\n",
      "epoch:21 step:20581 [D loss: 0.663317, acc: 57.03%] [G loss: 1.879217]\n",
      "epoch:21 step:20582 [D loss: 0.607069, acc: 66.41%] [G loss: 1.948591]\n",
      "epoch:21 step:20583 [D loss: 0.620069, acc: 63.28%] [G loss: 1.933563]\n",
      "epoch:21 step:20584 [D loss: 0.626831, acc: 67.19%] [G loss: 1.923482]\n",
      "epoch:21 step:20585 [D loss: 0.687831, acc: 54.69%] [G loss: 1.763544]\n",
      "epoch:21 step:20586 [D loss: 0.635532, acc: 59.38%] [G loss: 1.941254]\n",
      "epoch:21 step:20587 [D loss: 0.702290, acc: 60.16%] [G loss: 1.993050]\n",
      "epoch:21 step:20588 [D loss: 0.632052, acc: 65.62%] [G loss: 1.960523]\n",
      "epoch:21 step:20589 [D loss: 0.653052, acc: 64.06%] [G loss: 2.103755]\n",
      "epoch:21 step:20590 [D loss: 0.676548, acc: 58.59%] [G loss: 1.772776]\n",
      "epoch:21 step:20591 [D loss: 0.681764, acc: 59.38%] [G loss: 1.864640]\n",
      "epoch:21 step:20592 [D loss: 0.600800, acc: 68.75%] [G loss: 1.971856]\n",
      "epoch:21 step:20593 [D loss: 0.638859, acc: 64.84%] [G loss: 2.096133]\n",
      "epoch:21 step:20594 [D loss: 0.664894, acc: 60.16%] [G loss: 1.913312]\n",
      "epoch:21 step:20595 [D loss: 0.645354, acc: 66.41%] [G loss: 1.901478]\n",
      "epoch:21 step:20596 [D loss: 0.579821, acc: 70.31%] [G loss: 2.159482]\n",
      "epoch:21 step:20597 [D loss: 0.662088, acc: 60.94%] [G loss: 1.818494]\n",
      "epoch:21 step:20598 [D loss: 0.643781, acc: 64.84%] [G loss: 1.993285]\n",
      "epoch:21 step:20599 [D loss: 0.671835, acc: 57.81%] [G loss: 1.928840]\n",
      "epoch:21 step:20600 [D loss: 0.622099, acc: 66.41%] [G loss: 2.139836]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.262943\n",
      "FID: 15.599603\n",
      "0 = 12.660484039640451\n",
      "1 = 0.0821379920750834\n",
      "2 = 0.8695999979972839\n",
      "3 = 0.88919997215271\n",
      "4 = 0.8500000238418579\n",
      "5 = 0.8556581735610962\n",
      "6 = 0.88919997215271\n",
      "7 = 6.572287079036228\n",
      "8 = 0.08309608702369746\n",
      "9 = 0.7189000248908997\n",
      "10 = 0.7264000177383423\n",
      "11 = 0.7113999724388123\n",
      "12 = 0.7156650424003601\n",
      "13 = 0.7264000177383423\n",
      "14 = 7.262974262237549\n",
      "15 = 9.413246154785156\n",
      "16 = 0.12044353783130646\n",
      "17 = 7.262942790985107\n",
      "18 = 15.599602699279785\n",
      "epoch:21 step:20601 [D loss: 0.589424, acc: 71.09%] [G loss: 1.969432]\n",
      "epoch:21 step:20602 [D loss: 0.618316, acc: 61.72%] [G loss: 2.100257]\n",
      "epoch:21 step:20603 [D loss: 0.644138, acc: 62.50%] [G loss: 1.944180]\n",
      "epoch:21 step:20604 [D loss: 0.625343, acc: 66.41%] [G loss: 2.096656]\n",
      "epoch:21 step:20605 [D loss: 0.720244, acc: 56.25%] [G loss: 1.878919]\n",
      "epoch:21 step:20606 [D loss: 0.715180, acc: 51.56%] [G loss: 1.730332]\n",
      "epoch:21 step:20607 [D loss: 0.621261, acc: 68.75%] [G loss: 1.992028]\n",
      "epoch:21 step:20608 [D loss: 0.581827, acc: 70.31%] [G loss: 2.142774]\n",
      "epoch:21 step:20609 [D loss: 0.695667, acc: 53.12%] [G loss: 1.737490]\n",
      "epoch:21 step:20610 [D loss: 0.697048, acc: 55.47%] [G loss: 1.710473]\n",
      "epoch:21 step:20611 [D loss: 0.608654, acc: 71.09%] [G loss: 1.822657]\n",
      "epoch:21 step:20612 [D loss: 0.647156, acc: 66.41%] [G loss: 1.950921]\n",
      "epoch:21 step:20613 [D loss: 0.613408, acc: 65.62%] [G loss: 1.943388]\n",
      "epoch:21 step:20614 [D loss: 0.665233, acc: 67.97%] [G loss: 2.214678]\n",
      "epoch:22 step:20615 [D loss: 0.644533, acc: 65.62%] [G loss: 1.762561]\n",
      "epoch:22 step:20616 [D loss: 0.646249, acc: 64.06%] [G loss: 1.960143]\n",
      "epoch:22 step:20617 [D loss: 0.617775, acc: 64.84%] [G loss: 1.970137]\n",
      "epoch:22 step:20618 [D loss: 0.638150, acc: 67.19%] [G loss: 1.863599]\n",
      "epoch:22 step:20619 [D loss: 0.614961, acc: 72.66%] [G loss: 1.862218]\n",
      "epoch:22 step:20620 [D loss: 0.599894, acc: 73.44%] [G loss: 2.095669]\n",
      "epoch:22 step:20621 [D loss: 0.642398, acc: 57.81%] [G loss: 2.038838]\n",
      "epoch:22 step:20622 [D loss: 0.652544, acc: 61.72%] [G loss: 1.948116]\n",
      "epoch:22 step:20623 [D loss: 0.631156, acc: 64.06%] [G loss: 1.875356]\n",
      "epoch:22 step:20624 [D loss: 0.612637, acc: 67.97%] [G loss: 2.003798]\n",
      "epoch:22 step:20625 [D loss: 0.657162, acc: 65.62%] [G loss: 1.846592]\n",
      "epoch:22 step:20626 [D loss: 0.681666, acc: 57.03%] [G loss: 1.854521]\n",
      "epoch:22 step:20627 [D loss: 0.631738, acc: 64.06%] [G loss: 1.890099]\n",
      "epoch:22 step:20628 [D loss: 0.635306, acc: 64.84%] [G loss: 1.943182]\n",
      "epoch:22 step:20629 [D loss: 0.569417, acc: 67.19%] [G loss: 2.063638]\n",
      "epoch:22 step:20630 [D loss: 0.600971, acc: 67.97%] [G loss: 2.011413]\n",
      "epoch:22 step:20631 [D loss: 0.643870, acc: 63.28%] [G loss: 1.873693]\n",
      "epoch:22 step:20632 [D loss: 0.669805, acc: 65.62%] [G loss: 1.883619]\n",
      "epoch:22 step:20633 [D loss: 0.681149, acc: 57.81%] [G loss: 1.893888]\n",
      "epoch:22 step:20634 [D loss: 0.777277, acc: 44.53%] [G loss: 1.698170]\n",
      "epoch:22 step:20635 [D loss: 0.675211, acc: 54.69%] [G loss: 1.780506]\n",
      "epoch:22 step:20636 [D loss: 0.666839, acc: 59.38%] [G loss: 1.819394]\n",
      "epoch:22 step:20637 [D loss: 0.658265, acc: 60.94%] [G loss: 1.951311]\n",
      "epoch:22 step:20638 [D loss: 0.644528, acc: 61.72%] [G loss: 2.044269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20639 [D loss: 0.619830, acc: 66.41%] [G loss: 2.066006]\n",
      "epoch:22 step:20640 [D loss: 0.719587, acc: 55.47%] [G loss: 1.724631]\n",
      "epoch:22 step:20641 [D loss: 0.702656, acc: 53.91%] [G loss: 1.680599]\n",
      "epoch:22 step:20642 [D loss: 0.598498, acc: 66.41%] [G loss: 1.719256]\n",
      "epoch:22 step:20643 [D loss: 0.608828, acc: 74.22%] [G loss: 1.866379]\n",
      "epoch:22 step:20644 [D loss: 0.634420, acc: 61.72%] [G loss: 1.736269]\n",
      "epoch:22 step:20645 [D loss: 0.626285, acc: 63.28%] [G loss: 1.694757]\n",
      "epoch:22 step:20646 [D loss: 0.688335, acc: 57.81%] [G loss: 1.785742]\n",
      "epoch:22 step:20647 [D loss: 0.715224, acc: 48.44%] [G loss: 1.836159]\n",
      "epoch:22 step:20648 [D loss: 0.662744, acc: 58.59%] [G loss: 1.706144]\n",
      "epoch:22 step:20649 [D loss: 0.668024, acc: 58.59%] [G loss: 1.713697]\n",
      "epoch:22 step:20650 [D loss: 0.587835, acc: 69.53%] [G loss: 1.839995]\n",
      "epoch:22 step:20651 [D loss: 0.651932, acc: 59.38%] [G loss: 1.794381]\n",
      "epoch:22 step:20652 [D loss: 0.618200, acc: 70.31%] [G loss: 1.904175]\n",
      "epoch:22 step:20653 [D loss: 0.628820, acc: 67.19%] [G loss: 1.870744]\n",
      "epoch:22 step:20654 [D loss: 0.597674, acc: 71.88%] [G loss: 2.078716]\n",
      "epoch:22 step:20655 [D loss: 0.633623, acc: 62.50%] [G loss: 1.747340]\n",
      "epoch:22 step:20656 [D loss: 0.685243, acc: 51.56%] [G loss: 1.926721]\n",
      "epoch:22 step:20657 [D loss: 0.629722, acc: 64.84%] [G loss: 1.815786]\n",
      "epoch:22 step:20658 [D loss: 0.683613, acc: 53.12%] [G loss: 1.737839]\n",
      "epoch:22 step:20659 [D loss: 0.630040, acc: 64.84%] [G loss: 1.758853]\n",
      "epoch:22 step:20660 [D loss: 0.628751, acc: 63.28%] [G loss: 1.845643]\n",
      "epoch:22 step:20661 [D loss: 0.610172, acc: 71.88%] [G loss: 1.896214]\n",
      "epoch:22 step:20662 [D loss: 0.622384, acc: 64.84%] [G loss: 2.010836]\n",
      "epoch:22 step:20663 [D loss: 0.644780, acc: 60.94%] [G loss: 2.026048]\n",
      "epoch:22 step:20664 [D loss: 0.610352, acc: 67.97%] [G loss: 1.952100]\n",
      "epoch:22 step:20665 [D loss: 0.697342, acc: 56.25%] [G loss: 1.710828]\n",
      "epoch:22 step:20666 [D loss: 0.608214, acc: 67.19%] [G loss: 1.813573]\n",
      "epoch:22 step:20667 [D loss: 0.668329, acc: 60.94%] [G loss: 1.918753]\n",
      "epoch:22 step:20668 [D loss: 0.634348, acc: 62.50%] [G loss: 1.930923]\n",
      "epoch:22 step:20669 [D loss: 0.633999, acc: 62.50%] [G loss: 2.032301]\n",
      "epoch:22 step:20670 [D loss: 0.629674, acc: 60.94%] [G loss: 1.914588]\n",
      "epoch:22 step:20671 [D loss: 0.677572, acc: 59.38%] [G loss: 1.981931]\n",
      "epoch:22 step:20672 [D loss: 0.646271, acc: 67.97%] [G loss: 1.872145]\n",
      "epoch:22 step:20673 [D loss: 0.638350, acc: 64.06%] [G loss: 1.939804]\n",
      "epoch:22 step:20674 [D loss: 0.611662, acc: 66.41%] [G loss: 1.819583]\n",
      "epoch:22 step:20675 [D loss: 0.622151, acc: 60.94%] [G loss: 1.875739]\n",
      "epoch:22 step:20676 [D loss: 0.646955, acc: 62.50%] [G loss: 1.816383]\n",
      "epoch:22 step:20677 [D loss: 0.692487, acc: 61.72%] [G loss: 1.770192]\n",
      "epoch:22 step:20678 [D loss: 0.615809, acc: 65.62%] [G loss: 2.063207]\n",
      "epoch:22 step:20679 [D loss: 0.630496, acc: 69.53%] [G loss: 1.934949]\n",
      "epoch:22 step:20680 [D loss: 0.639556, acc: 60.16%] [G loss: 1.779891]\n",
      "epoch:22 step:20681 [D loss: 0.645179, acc: 65.62%] [G loss: 1.924794]\n",
      "epoch:22 step:20682 [D loss: 0.652575, acc: 66.41%] [G loss: 1.988929]\n",
      "epoch:22 step:20683 [D loss: 0.602321, acc: 67.97%] [G loss: 2.062468]\n",
      "epoch:22 step:20684 [D loss: 0.631808, acc: 67.97%] [G loss: 1.882695]\n",
      "epoch:22 step:20685 [D loss: 0.660911, acc: 60.16%] [G loss: 1.771898]\n",
      "epoch:22 step:20686 [D loss: 0.690384, acc: 56.25%] [G loss: 1.869233]\n",
      "epoch:22 step:20687 [D loss: 0.669423, acc: 62.50%] [G loss: 1.710451]\n",
      "epoch:22 step:20688 [D loss: 0.658034, acc: 58.59%] [G loss: 1.963365]\n",
      "epoch:22 step:20689 [D loss: 0.621084, acc: 62.50%] [G loss: 2.054681]\n",
      "epoch:22 step:20690 [D loss: 0.628787, acc: 61.72%] [G loss: 1.979578]\n",
      "epoch:22 step:20691 [D loss: 0.637663, acc: 60.16%] [G loss: 2.089054]\n",
      "epoch:22 step:20692 [D loss: 0.642588, acc: 61.72%] [G loss: 1.725632]\n",
      "epoch:22 step:20693 [D loss: 0.687964, acc: 58.59%] [G loss: 1.797173]\n",
      "epoch:22 step:20694 [D loss: 0.630800, acc: 62.50%] [G loss: 1.792733]\n",
      "epoch:22 step:20695 [D loss: 0.706025, acc: 53.12%] [G loss: 1.756395]\n",
      "epoch:22 step:20696 [D loss: 0.644047, acc: 62.50%] [G loss: 1.769503]\n",
      "epoch:22 step:20697 [D loss: 0.662951, acc: 55.47%] [G loss: 1.847914]\n",
      "epoch:22 step:20698 [D loss: 0.677796, acc: 60.16%] [G loss: 1.756632]\n",
      "epoch:22 step:20699 [D loss: 0.669080, acc: 60.16%] [G loss: 1.762592]\n",
      "epoch:22 step:20700 [D loss: 0.660405, acc: 64.06%] [G loss: 1.716780]\n",
      "epoch:22 step:20701 [D loss: 0.667401, acc: 62.50%] [G loss: 1.771670]\n",
      "epoch:22 step:20702 [D loss: 0.612689, acc: 66.41%] [G loss: 1.785186]\n",
      "epoch:22 step:20703 [D loss: 0.635684, acc: 62.50%] [G loss: 1.988310]\n",
      "epoch:22 step:20704 [D loss: 0.654636, acc: 66.41%] [G loss: 1.839017]\n",
      "epoch:22 step:20705 [D loss: 0.664308, acc: 57.03%] [G loss: 1.767857]\n",
      "epoch:22 step:20706 [D loss: 0.637856, acc: 64.06%] [G loss: 1.945997]\n",
      "epoch:22 step:20707 [D loss: 0.587952, acc: 65.62%] [G loss: 1.992843]\n",
      "epoch:22 step:20708 [D loss: 0.658803, acc: 54.69%] [G loss: 1.854822]\n",
      "epoch:22 step:20709 [D loss: 0.637346, acc: 66.41%] [G loss: 1.966310]\n",
      "epoch:22 step:20710 [D loss: 0.650463, acc: 61.72%] [G loss: 1.834027]\n",
      "epoch:22 step:20711 [D loss: 0.663947, acc: 59.38%] [G loss: 1.829852]\n",
      "epoch:22 step:20712 [D loss: 0.678370, acc: 59.38%] [G loss: 1.824617]\n",
      "epoch:22 step:20713 [D loss: 0.663348, acc: 64.06%] [G loss: 1.845181]\n",
      "epoch:22 step:20714 [D loss: 0.604571, acc: 71.09%] [G loss: 1.896737]\n",
      "epoch:22 step:20715 [D loss: 0.648379, acc: 63.28%] [G loss: 1.904327]\n",
      "epoch:22 step:20716 [D loss: 0.653942, acc: 64.06%] [G loss: 1.819527]\n",
      "epoch:22 step:20717 [D loss: 0.659971, acc: 60.16%] [G loss: 1.851821]\n",
      "epoch:22 step:20718 [D loss: 0.624837, acc: 64.84%] [G loss: 1.851646]\n",
      "epoch:22 step:20719 [D loss: 0.655462, acc: 60.16%] [G loss: 1.863361]\n",
      "epoch:22 step:20720 [D loss: 0.601025, acc: 63.28%] [G loss: 2.007123]\n",
      "epoch:22 step:20721 [D loss: 0.634119, acc: 63.28%] [G loss: 1.982155]\n",
      "epoch:22 step:20722 [D loss: 0.731179, acc: 54.69%] [G loss: 1.804108]\n",
      "epoch:22 step:20723 [D loss: 0.696786, acc: 53.91%] [G loss: 1.672882]\n",
      "epoch:22 step:20724 [D loss: 0.631469, acc: 60.94%] [G loss: 1.818875]\n",
      "epoch:22 step:20725 [D loss: 0.641230, acc: 60.94%] [G loss: 1.942572]\n",
      "epoch:22 step:20726 [D loss: 0.639409, acc: 63.28%] [G loss: 1.901635]\n",
      "epoch:22 step:20727 [D loss: 0.619238, acc: 65.62%] [G loss: 1.936797]\n",
      "epoch:22 step:20728 [D loss: 0.622441, acc: 59.38%] [G loss: 1.879900]\n",
      "epoch:22 step:20729 [D loss: 0.608508, acc: 67.19%] [G loss: 2.021033]\n",
      "epoch:22 step:20730 [D loss: 0.667121, acc: 60.16%] [G loss: 2.221247]\n",
      "epoch:22 step:20731 [D loss: 0.633455, acc: 71.09%] [G loss: 2.060447]\n",
      "epoch:22 step:20732 [D loss: 0.622052, acc: 64.84%] [G loss: 2.057464]\n",
      "epoch:22 step:20733 [D loss: 0.558996, acc: 74.22%] [G loss: 2.289204]\n",
      "epoch:22 step:20734 [D loss: 0.640929, acc: 64.06%] [G loss: 1.842551]\n",
      "epoch:22 step:20735 [D loss: 0.658719, acc: 68.75%] [G loss: 1.884727]\n",
      "epoch:22 step:20736 [D loss: 0.652200, acc: 60.16%] [G loss: 2.030648]\n",
      "epoch:22 step:20737 [D loss: 0.667808, acc: 59.38%] [G loss: 1.800131]\n",
      "epoch:22 step:20738 [D loss: 0.679154, acc: 61.72%] [G loss: 1.825539]\n",
      "epoch:22 step:20739 [D loss: 0.689359, acc: 56.25%] [G loss: 1.718875]\n",
      "epoch:22 step:20740 [D loss: 0.666846, acc: 63.28%] [G loss: 1.894566]\n",
      "epoch:22 step:20741 [D loss: 0.675468, acc: 57.03%] [G loss: 1.688929]\n",
      "epoch:22 step:20742 [D loss: 0.626324, acc: 67.97%] [G loss: 1.838302]\n",
      "epoch:22 step:20743 [D loss: 0.667008, acc: 58.59%] [G loss: 1.868092]\n",
      "epoch:22 step:20744 [D loss: 0.596232, acc: 67.97%] [G loss: 1.936638]\n",
      "epoch:22 step:20745 [D loss: 0.697558, acc: 56.25%] [G loss: 1.810280]\n",
      "epoch:22 step:20746 [D loss: 0.626293, acc: 63.28%] [G loss: 1.837464]\n",
      "epoch:22 step:20747 [D loss: 0.682022, acc: 57.81%] [G loss: 1.845375]\n",
      "epoch:22 step:20748 [D loss: 0.671401, acc: 55.47%] [G loss: 1.903165]\n",
      "epoch:22 step:20749 [D loss: 0.679193, acc: 63.28%] [G loss: 1.895600]\n",
      "epoch:22 step:20750 [D loss: 0.629024, acc: 64.06%] [G loss: 1.940349]\n",
      "epoch:22 step:20751 [D loss: 0.698234, acc: 57.81%] [G loss: 1.796062]\n",
      "epoch:22 step:20752 [D loss: 0.648812, acc: 63.28%] [G loss: 1.896578]\n",
      "epoch:22 step:20753 [D loss: 0.620351, acc: 61.72%] [G loss: 1.857203]\n",
      "epoch:22 step:20754 [D loss: 0.665363, acc: 57.81%] [G loss: 1.815966]\n",
      "epoch:22 step:20755 [D loss: 0.640159, acc: 63.28%] [G loss: 1.891460]\n",
      "epoch:22 step:20756 [D loss: 0.669324, acc: 60.94%] [G loss: 1.913449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20757 [D loss: 0.668107, acc: 61.72%] [G loss: 1.906859]\n",
      "epoch:22 step:20758 [D loss: 0.665844, acc: 62.50%] [G loss: 1.851063]\n",
      "epoch:22 step:20759 [D loss: 0.622984, acc: 67.19%] [G loss: 1.746767]\n",
      "epoch:22 step:20760 [D loss: 0.620733, acc: 67.19%] [G loss: 1.964055]\n",
      "epoch:22 step:20761 [D loss: 0.720368, acc: 56.25%] [G loss: 1.891729]\n",
      "epoch:22 step:20762 [D loss: 0.646583, acc: 61.72%] [G loss: 1.884142]\n",
      "epoch:22 step:20763 [D loss: 0.642848, acc: 57.03%] [G loss: 1.942002]\n",
      "epoch:22 step:20764 [D loss: 0.646473, acc: 59.38%] [G loss: 1.961139]\n",
      "epoch:22 step:20765 [D loss: 0.657680, acc: 59.38%] [G loss: 1.943867]\n",
      "epoch:22 step:20766 [D loss: 0.617146, acc: 68.75%] [G loss: 1.818058]\n",
      "epoch:22 step:20767 [D loss: 0.692635, acc: 61.72%] [G loss: 1.802102]\n",
      "epoch:22 step:20768 [D loss: 0.691185, acc: 57.03%] [G loss: 1.947843]\n",
      "epoch:22 step:20769 [D loss: 0.661142, acc: 61.72%] [G loss: 1.965159]\n",
      "epoch:22 step:20770 [D loss: 0.648945, acc: 61.72%] [G loss: 1.886830]\n",
      "epoch:22 step:20771 [D loss: 0.656541, acc: 59.38%] [G loss: 1.830896]\n",
      "epoch:22 step:20772 [D loss: 0.652166, acc: 64.84%] [G loss: 1.845323]\n",
      "epoch:22 step:20773 [D loss: 0.656491, acc: 62.50%] [G loss: 1.831779]\n",
      "epoch:22 step:20774 [D loss: 0.676755, acc: 60.94%] [G loss: 1.709401]\n",
      "epoch:22 step:20775 [D loss: 0.666891, acc: 59.38%] [G loss: 1.802298]\n",
      "epoch:22 step:20776 [D loss: 0.634616, acc: 64.84%] [G loss: 1.831838]\n",
      "epoch:22 step:20777 [D loss: 0.608789, acc: 67.19%] [G loss: 1.881131]\n",
      "epoch:22 step:20778 [D loss: 0.659345, acc: 60.94%] [G loss: 1.857886]\n",
      "epoch:22 step:20779 [D loss: 0.632550, acc: 60.94%] [G loss: 1.781919]\n",
      "epoch:22 step:20780 [D loss: 0.590696, acc: 70.31%] [G loss: 1.913134]\n",
      "epoch:22 step:20781 [D loss: 0.610443, acc: 71.88%] [G loss: 1.829152]\n",
      "epoch:22 step:20782 [D loss: 0.673114, acc: 64.06%] [G loss: 1.892333]\n",
      "epoch:22 step:20783 [D loss: 0.681270, acc: 57.81%] [G loss: 1.807659]\n",
      "epoch:22 step:20784 [D loss: 0.619817, acc: 64.06%] [G loss: 1.861643]\n",
      "epoch:22 step:20785 [D loss: 0.663429, acc: 57.03%] [G loss: 1.900705]\n",
      "epoch:22 step:20786 [D loss: 0.605614, acc: 66.41%] [G loss: 1.843859]\n",
      "epoch:22 step:20787 [D loss: 0.630111, acc: 64.84%] [G loss: 1.761265]\n",
      "epoch:22 step:20788 [D loss: 0.681538, acc: 56.25%] [G loss: 1.873502]\n",
      "epoch:22 step:20789 [D loss: 0.610199, acc: 67.19%] [G loss: 1.810174]\n",
      "epoch:22 step:20790 [D loss: 0.709826, acc: 53.91%] [G loss: 1.786294]\n",
      "epoch:22 step:20791 [D loss: 0.672878, acc: 65.62%] [G loss: 1.807223]\n",
      "epoch:22 step:20792 [D loss: 0.630691, acc: 64.06%] [G loss: 1.839113]\n",
      "epoch:22 step:20793 [D loss: 0.698871, acc: 60.16%] [G loss: 1.814574]\n",
      "epoch:22 step:20794 [D loss: 0.628937, acc: 64.84%] [G loss: 1.806266]\n",
      "epoch:22 step:20795 [D loss: 0.626216, acc: 67.97%] [G loss: 1.794596]\n",
      "epoch:22 step:20796 [D loss: 0.664832, acc: 60.94%] [G loss: 1.812719]\n",
      "epoch:22 step:20797 [D loss: 0.608173, acc: 71.88%] [G loss: 1.840273]\n",
      "epoch:22 step:20798 [D loss: 0.717884, acc: 57.81%] [G loss: 1.828548]\n",
      "epoch:22 step:20799 [D loss: 0.705637, acc: 50.78%] [G loss: 1.820805]\n",
      "epoch:22 step:20800 [D loss: 0.641508, acc: 60.16%] [G loss: 1.943286]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.549167\n",
      "FID: 11.021353\n",
      "0 = 12.898233005857483\n",
      "1 = 0.09509708262560618\n",
      "2 = 0.880299985408783\n",
      "3 = 0.9021999835968018\n",
      "4 = 0.8583999872207642\n",
      "5 = 0.8643418550491333\n",
      "6 = 0.9021999835968018\n",
      "7 = 6.276491710233707\n",
      "8 = 0.06263813013640225\n",
      "9 = 0.718500018119812\n",
      "10 = 0.7319999933242798\n",
      "11 = 0.7049999833106995\n",
      "12 = 0.7127556204795837\n",
      "13 = 0.7319999933242798\n",
      "14 = 7.549200534820557\n",
      "15 = 9.377118110656738\n",
      "16 = 0.13210873305797577\n",
      "17 = 7.549166679382324\n",
      "18 = 11.021352767944336\n",
      "epoch:22 step:20801 [D loss: 0.692724, acc: 58.59%] [G loss: 1.769848]\n",
      "epoch:22 step:20802 [D loss: 0.594779, acc: 69.53%] [G loss: 1.982729]\n",
      "epoch:22 step:20803 [D loss: 0.656912, acc: 60.94%] [G loss: 1.827314]\n",
      "epoch:22 step:20804 [D loss: 0.631395, acc: 65.62%] [G loss: 1.848470]\n",
      "epoch:22 step:20805 [D loss: 0.656257, acc: 60.94%] [G loss: 1.716083]\n",
      "epoch:22 step:20806 [D loss: 0.582227, acc: 67.97%] [G loss: 1.901267]\n",
      "epoch:22 step:20807 [D loss: 0.626966, acc: 64.84%] [G loss: 1.997670]\n",
      "epoch:22 step:20808 [D loss: 0.591296, acc: 69.53%] [G loss: 2.036309]\n",
      "epoch:22 step:20809 [D loss: 0.630315, acc: 64.06%] [G loss: 1.847014]\n",
      "epoch:22 step:20810 [D loss: 0.648513, acc: 60.94%] [G loss: 1.838284]\n",
      "epoch:22 step:20811 [D loss: 0.662836, acc: 57.81%] [G loss: 2.033090]\n",
      "epoch:22 step:20812 [D loss: 0.677870, acc: 63.28%] [G loss: 1.970351]\n",
      "epoch:22 step:20813 [D loss: 0.657587, acc: 60.94%] [G loss: 1.930008]\n",
      "epoch:22 step:20814 [D loss: 0.725083, acc: 50.78%] [G loss: 1.781565]\n",
      "epoch:22 step:20815 [D loss: 0.661252, acc: 57.03%] [G loss: 1.878641]\n",
      "epoch:22 step:20816 [D loss: 0.666413, acc: 57.81%] [G loss: 1.827681]\n",
      "epoch:22 step:20817 [D loss: 0.651015, acc: 58.59%] [G loss: 1.912972]\n",
      "epoch:22 step:20818 [D loss: 0.646240, acc: 64.84%] [G loss: 1.878197]\n",
      "epoch:22 step:20819 [D loss: 0.682100, acc: 58.59%] [G loss: 1.941957]\n",
      "epoch:22 step:20820 [D loss: 0.608531, acc: 65.62%] [G loss: 1.971616]\n",
      "epoch:22 step:20821 [D loss: 0.631017, acc: 63.28%] [G loss: 2.057532]\n",
      "epoch:22 step:20822 [D loss: 0.574756, acc: 71.09%] [G loss: 2.074085]\n",
      "epoch:22 step:20823 [D loss: 0.652611, acc: 58.59%] [G loss: 2.040498]\n",
      "epoch:22 step:20824 [D loss: 0.638577, acc: 65.62%] [G loss: 1.778259]\n",
      "epoch:22 step:20825 [D loss: 0.658550, acc: 63.28%] [G loss: 1.737339]\n",
      "epoch:22 step:20826 [D loss: 0.670173, acc: 64.06%] [G loss: 1.896512]\n",
      "epoch:22 step:20827 [D loss: 0.670579, acc: 60.16%] [G loss: 1.809484]\n",
      "epoch:22 step:20828 [D loss: 0.677756, acc: 62.50%] [G loss: 1.906995]\n",
      "epoch:22 step:20829 [D loss: 0.690070, acc: 56.25%] [G loss: 1.926452]\n",
      "epoch:22 step:20830 [D loss: 0.598519, acc: 71.88%] [G loss: 1.992167]\n",
      "epoch:22 step:20831 [D loss: 0.657333, acc: 54.69%] [G loss: 1.968484]\n",
      "epoch:22 step:20832 [D loss: 0.564569, acc: 71.88%] [G loss: 2.041263]\n",
      "epoch:22 step:20833 [D loss: 0.588626, acc: 65.62%] [G loss: 2.007097]\n",
      "epoch:22 step:20834 [D loss: 0.723933, acc: 50.00%] [G loss: 1.658126]\n",
      "epoch:22 step:20835 [D loss: 0.693771, acc: 58.59%] [G loss: 1.917288]\n",
      "epoch:22 step:20836 [D loss: 0.661525, acc: 63.28%] [G loss: 1.921805]\n",
      "epoch:22 step:20837 [D loss: 0.637463, acc: 64.06%] [G loss: 1.969493]\n",
      "epoch:22 step:20838 [D loss: 0.668875, acc: 59.38%] [G loss: 1.767628]\n",
      "epoch:22 step:20839 [D loss: 0.683371, acc: 53.12%] [G loss: 1.871207]\n",
      "epoch:22 step:20840 [D loss: 0.627340, acc: 67.19%] [G loss: 1.871489]\n",
      "epoch:22 step:20841 [D loss: 0.643259, acc: 63.28%] [G loss: 1.921518]\n",
      "epoch:22 step:20842 [D loss: 0.675382, acc: 58.59%] [G loss: 1.963733]\n",
      "epoch:22 step:20843 [D loss: 0.604649, acc: 62.50%] [G loss: 2.157119]\n",
      "epoch:22 step:20844 [D loss: 0.604529, acc: 68.75%] [G loss: 2.257457]\n",
      "epoch:22 step:20845 [D loss: 0.577535, acc: 73.44%] [G loss: 2.251647]\n",
      "epoch:22 step:20846 [D loss: 0.548942, acc: 75.78%] [G loss: 2.252738]\n",
      "epoch:22 step:20847 [D loss: 0.645776, acc: 64.84%] [G loss: 1.798040]\n",
      "epoch:22 step:20848 [D loss: 0.661999, acc: 64.06%] [G loss: 2.014104]\n",
      "epoch:22 step:20849 [D loss: 0.666169, acc: 59.38%] [G loss: 1.825264]\n",
      "epoch:22 step:20850 [D loss: 0.670656, acc: 61.72%] [G loss: 1.902745]\n",
      "epoch:22 step:20851 [D loss: 0.620668, acc: 60.94%] [G loss: 2.053811]\n",
      "epoch:22 step:20852 [D loss: 0.612854, acc: 64.84%] [G loss: 2.072369]\n",
      "epoch:22 step:20853 [D loss: 0.631728, acc: 63.28%] [G loss: 1.953602]\n",
      "epoch:22 step:20854 [D loss: 0.680351, acc: 62.50%] [G loss: 1.983326]\n",
      "epoch:22 step:20855 [D loss: 0.614953, acc: 69.53%] [G loss: 1.997986]\n",
      "epoch:22 step:20856 [D loss: 0.600056, acc: 67.97%] [G loss: 2.051500]\n",
      "epoch:22 step:20857 [D loss: 0.657552, acc: 61.72%] [G loss: 1.973438]\n",
      "epoch:22 step:20858 [D loss: 0.646950, acc: 63.28%] [G loss: 1.982756]\n",
      "epoch:22 step:20859 [D loss: 0.593453, acc: 65.62%] [G loss: 1.966050]\n",
      "epoch:22 step:20860 [D loss: 0.752620, acc: 46.09%] [G loss: 1.790248]\n",
      "epoch:22 step:20861 [D loss: 0.647574, acc: 60.16%] [G loss: 1.758479]\n",
      "epoch:22 step:20862 [D loss: 0.629337, acc: 60.16%] [G loss: 2.043013]\n",
      "epoch:22 step:20863 [D loss: 0.646178, acc: 64.06%] [G loss: 1.788936]\n",
      "epoch:22 step:20864 [D loss: 0.730304, acc: 50.78%] [G loss: 1.728861]\n",
      "epoch:22 step:20865 [D loss: 0.687351, acc: 61.72%] [G loss: 1.785253]\n",
      "epoch:22 step:20866 [D loss: 0.707883, acc: 53.91%] [G loss: 1.768160]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20867 [D loss: 0.633861, acc: 63.28%] [G loss: 1.817635]\n",
      "epoch:22 step:20868 [D loss: 0.665014, acc: 61.72%] [G loss: 1.718716]\n",
      "epoch:22 step:20869 [D loss: 0.666476, acc: 60.94%] [G loss: 1.744175]\n",
      "epoch:22 step:20870 [D loss: 0.677415, acc: 62.50%] [G loss: 1.849308]\n",
      "epoch:22 step:20871 [D loss: 0.698694, acc: 54.69%] [G loss: 1.730437]\n",
      "epoch:22 step:20872 [D loss: 0.596245, acc: 71.88%] [G loss: 1.831115]\n",
      "epoch:22 step:20873 [D loss: 0.670372, acc: 66.41%] [G loss: 1.858325]\n",
      "epoch:22 step:20874 [D loss: 0.645602, acc: 60.16%] [G loss: 1.953758]\n",
      "epoch:22 step:20875 [D loss: 0.648189, acc: 62.50%] [G loss: 1.849777]\n",
      "epoch:22 step:20876 [D loss: 0.563925, acc: 73.44%] [G loss: 1.921832]\n",
      "epoch:22 step:20877 [D loss: 0.689692, acc: 57.03%] [G loss: 1.989573]\n",
      "epoch:22 step:20878 [D loss: 0.643186, acc: 63.28%] [G loss: 1.865390]\n",
      "epoch:22 step:20879 [D loss: 0.658330, acc: 62.50%] [G loss: 1.876089]\n",
      "epoch:22 step:20880 [D loss: 0.577921, acc: 73.44%] [G loss: 1.807821]\n",
      "epoch:22 step:20881 [D loss: 0.664076, acc: 57.81%] [G loss: 1.868313]\n",
      "epoch:22 step:20882 [D loss: 0.695391, acc: 57.81%] [G loss: 1.860700]\n",
      "epoch:22 step:20883 [D loss: 0.671163, acc: 55.47%] [G loss: 1.966151]\n",
      "epoch:22 step:20884 [D loss: 0.593108, acc: 67.19%] [G loss: 1.803168]\n",
      "epoch:22 step:20885 [D loss: 0.596335, acc: 69.53%] [G loss: 1.836441]\n",
      "epoch:22 step:20886 [D loss: 0.627673, acc: 66.41%] [G loss: 2.016140]\n",
      "epoch:22 step:20887 [D loss: 0.636092, acc: 59.38%] [G loss: 1.962124]\n",
      "epoch:22 step:20888 [D loss: 0.610887, acc: 67.19%] [G loss: 1.964035]\n",
      "epoch:22 step:20889 [D loss: 0.584199, acc: 71.09%] [G loss: 2.003773]\n",
      "epoch:22 step:20890 [D loss: 0.615972, acc: 67.97%] [G loss: 2.069559]\n",
      "epoch:22 step:20891 [D loss: 0.676488, acc: 53.91%] [G loss: 1.899863]\n",
      "epoch:22 step:20892 [D loss: 0.625598, acc: 64.06%] [G loss: 1.906139]\n",
      "epoch:22 step:20893 [D loss: 0.669018, acc: 60.16%] [G loss: 1.862403]\n",
      "epoch:22 step:20894 [D loss: 0.637937, acc: 66.41%] [G loss: 1.933866]\n",
      "epoch:22 step:20895 [D loss: 0.687376, acc: 57.81%] [G loss: 1.838673]\n",
      "epoch:22 step:20896 [D loss: 0.638305, acc: 63.28%] [G loss: 1.981040]\n",
      "epoch:22 step:20897 [D loss: 0.614950, acc: 66.41%] [G loss: 1.871346]\n",
      "epoch:22 step:20898 [D loss: 0.660053, acc: 61.72%] [G loss: 1.807111]\n",
      "epoch:22 step:20899 [D loss: 0.661774, acc: 58.59%] [G loss: 1.883296]\n",
      "epoch:22 step:20900 [D loss: 0.599675, acc: 69.53%] [G loss: 2.014778]\n",
      "epoch:22 step:20901 [D loss: 0.640045, acc: 63.28%] [G loss: 1.928788]\n",
      "epoch:22 step:20902 [D loss: 0.653628, acc: 60.16%] [G loss: 1.801573]\n",
      "epoch:22 step:20903 [D loss: 0.632225, acc: 59.38%] [G loss: 1.901936]\n",
      "epoch:22 step:20904 [D loss: 0.624661, acc: 64.06%] [G loss: 1.871140]\n",
      "epoch:22 step:20905 [D loss: 0.685102, acc: 58.59%] [G loss: 1.831094]\n",
      "epoch:22 step:20906 [D loss: 0.696229, acc: 56.25%] [G loss: 1.953913]\n",
      "epoch:22 step:20907 [D loss: 0.666445, acc: 63.28%] [G loss: 1.885665]\n",
      "epoch:22 step:20908 [D loss: 0.699411, acc: 58.59%] [G loss: 1.826598]\n",
      "epoch:22 step:20909 [D loss: 0.614029, acc: 64.84%] [G loss: 1.896038]\n",
      "epoch:22 step:20910 [D loss: 0.660888, acc: 63.28%] [G loss: 1.952504]\n",
      "epoch:22 step:20911 [D loss: 0.603563, acc: 70.31%] [G loss: 1.910460]\n",
      "epoch:22 step:20912 [D loss: 0.605034, acc: 67.19%] [G loss: 1.823459]\n",
      "epoch:22 step:20913 [D loss: 0.669370, acc: 59.38%] [G loss: 1.839598]\n",
      "epoch:22 step:20914 [D loss: 0.651743, acc: 60.16%] [G loss: 2.019485]\n",
      "epoch:22 step:20915 [D loss: 0.683967, acc: 59.38%] [G loss: 1.904194]\n",
      "epoch:22 step:20916 [D loss: 0.624988, acc: 67.97%] [G loss: 1.879507]\n",
      "epoch:22 step:20917 [D loss: 0.617776, acc: 63.28%] [G loss: 1.948153]\n",
      "epoch:22 step:20918 [D loss: 0.621714, acc: 70.31%] [G loss: 1.707953]\n",
      "epoch:22 step:20919 [D loss: 0.652577, acc: 60.16%] [G loss: 2.018181]\n",
      "epoch:22 step:20920 [D loss: 0.627057, acc: 64.06%] [G loss: 1.733829]\n",
      "epoch:22 step:20921 [D loss: 0.635075, acc: 60.16%] [G loss: 1.741304]\n",
      "epoch:22 step:20922 [D loss: 0.635911, acc: 62.50%] [G loss: 1.836869]\n",
      "epoch:22 step:20923 [D loss: 0.641310, acc: 59.38%] [G loss: 1.834677]\n",
      "epoch:22 step:20924 [D loss: 0.627607, acc: 64.84%] [G loss: 1.944827]\n",
      "epoch:22 step:20925 [D loss: 0.651958, acc: 64.84%] [G loss: 1.897288]\n",
      "epoch:22 step:20926 [D loss: 0.542399, acc: 75.78%] [G loss: 2.199654]\n",
      "epoch:22 step:20927 [D loss: 0.572603, acc: 71.09%] [G loss: 2.040031]\n",
      "epoch:22 step:20928 [D loss: 0.617727, acc: 69.53%] [G loss: 2.178372]\n",
      "epoch:22 step:20929 [D loss: 0.634666, acc: 70.31%] [G loss: 2.253180]\n",
      "epoch:22 step:20930 [D loss: 0.673343, acc: 57.03%] [G loss: 1.729015]\n",
      "epoch:22 step:20931 [D loss: 0.689257, acc: 57.81%] [G loss: 1.927895]\n",
      "epoch:22 step:20932 [D loss: 0.603799, acc: 67.97%] [G loss: 2.019561]\n",
      "epoch:22 step:20933 [D loss: 0.645469, acc: 58.59%] [G loss: 1.927782]\n",
      "epoch:22 step:20934 [D loss: 0.625319, acc: 69.53%] [G loss: 1.800608]\n",
      "epoch:22 step:20935 [D loss: 0.587111, acc: 71.88%] [G loss: 2.048262]\n",
      "epoch:22 step:20936 [D loss: 0.615268, acc: 65.62%] [G loss: 1.984007]\n",
      "epoch:22 step:20937 [D loss: 0.658147, acc: 55.47%] [G loss: 1.868236]\n",
      "epoch:22 step:20938 [D loss: 0.654863, acc: 62.50%] [G loss: 1.846379]\n",
      "epoch:22 step:20939 [D loss: 0.648271, acc: 64.06%] [G loss: 1.860051]\n",
      "epoch:22 step:20940 [D loss: 0.621565, acc: 63.28%] [G loss: 1.906559]\n",
      "epoch:22 step:20941 [D loss: 0.719384, acc: 49.22%] [G loss: 1.874037]\n",
      "epoch:22 step:20942 [D loss: 0.630987, acc: 62.50%] [G loss: 1.867443]\n",
      "epoch:22 step:20943 [D loss: 0.681606, acc: 63.28%] [G loss: 1.882881]\n",
      "epoch:22 step:20944 [D loss: 0.601728, acc: 69.53%] [G loss: 2.111057]\n",
      "epoch:22 step:20945 [D loss: 0.663111, acc: 60.94%] [G loss: 1.859983]\n",
      "epoch:22 step:20946 [D loss: 0.663622, acc: 60.16%] [G loss: 2.011969]\n",
      "epoch:22 step:20947 [D loss: 0.627619, acc: 66.41%] [G loss: 1.866867]\n",
      "epoch:22 step:20948 [D loss: 0.671169, acc: 57.81%] [G loss: 1.837930]\n",
      "epoch:22 step:20949 [D loss: 0.662138, acc: 59.38%] [G loss: 2.082528]\n",
      "epoch:22 step:20950 [D loss: 0.624122, acc: 64.84%] [G loss: 1.955165]\n",
      "epoch:22 step:20951 [D loss: 0.613159, acc: 69.53%] [G loss: 2.100042]\n",
      "epoch:22 step:20952 [D loss: 0.647649, acc: 59.38%] [G loss: 1.873803]\n",
      "epoch:22 step:20953 [D loss: 0.657865, acc: 63.28%] [G loss: 1.844758]\n",
      "epoch:22 step:20954 [D loss: 0.604439, acc: 69.53%] [G loss: 2.068663]\n",
      "epoch:22 step:20955 [D loss: 0.745135, acc: 57.81%] [G loss: 1.786322]\n",
      "epoch:22 step:20956 [D loss: 0.650258, acc: 60.94%] [G loss: 1.750030]\n",
      "epoch:22 step:20957 [D loss: 0.713201, acc: 53.12%] [G loss: 1.777724]\n",
      "epoch:22 step:20958 [D loss: 0.637150, acc: 59.38%] [G loss: 1.784109]\n",
      "epoch:22 step:20959 [D loss: 0.568455, acc: 71.88%] [G loss: 2.169061]\n",
      "epoch:22 step:20960 [D loss: 0.582015, acc: 70.31%] [G loss: 2.197271]\n",
      "epoch:22 step:20961 [D loss: 0.610694, acc: 69.53%] [G loss: 2.125938]\n",
      "epoch:22 step:20962 [D loss: 0.685788, acc: 57.03%] [G loss: 1.814744]\n",
      "epoch:22 step:20963 [D loss: 0.706856, acc: 52.34%] [G loss: 1.688684]\n",
      "epoch:22 step:20964 [D loss: 0.673311, acc: 57.03%] [G loss: 1.808003]\n",
      "epoch:22 step:20965 [D loss: 0.684527, acc: 53.91%] [G loss: 1.806880]\n",
      "epoch:22 step:20966 [D loss: 0.708522, acc: 53.91%] [G loss: 1.869916]\n",
      "epoch:22 step:20967 [D loss: 0.648717, acc: 57.03%] [G loss: 1.846418]\n",
      "epoch:22 step:20968 [D loss: 0.636345, acc: 66.41%] [G loss: 2.031673]\n",
      "epoch:22 step:20969 [D loss: 0.694031, acc: 53.91%] [G loss: 1.705515]\n",
      "epoch:22 step:20970 [D loss: 0.714789, acc: 59.38%] [G loss: 1.721057]\n",
      "epoch:22 step:20971 [D loss: 0.660857, acc: 61.72%] [G loss: 1.736174]\n",
      "epoch:22 step:20972 [D loss: 0.639701, acc: 66.41%] [G loss: 1.913375]\n",
      "epoch:22 step:20973 [D loss: 0.580901, acc: 71.09%] [G loss: 1.970073]\n",
      "epoch:22 step:20974 [D loss: 0.609319, acc: 67.19%] [G loss: 1.940453]\n",
      "epoch:22 step:20975 [D loss: 0.660077, acc: 62.50%] [G loss: 1.917574]\n",
      "epoch:22 step:20976 [D loss: 0.655389, acc: 59.38%] [G loss: 1.822578]\n",
      "epoch:22 step:20977 [D loss: 0.644255, acc: 64.06%] [G loss: 1.854169]\n",
      "epoch:22 step:20978 [D loss: 0.601341, acc: 71.88%] [G loss: 2.011173]\n",
      "epoch:22 step:20979 [D loss: 0.635765, acc: 67.19%] [G loss: 1.885107]\n",
      "epoch:22 step:20980 [D loss: 0.652234, acc: 61.72%] [G loss: 1.883954]\n",
      "epoch:22 step:20981 [D loss: 0.619142, acc: 67.19%] [G loss: 2.099291]\n",
      "epoch:22 step:20982 [D loss: 0.645665, acc: 65.62%] [G loss: 1.891416]\n",
      "epoch:22 step:20983 [D loss: 0.651106, acc: 63.28%] [G loss: 1.955343]\n",
      "epoch:22 step:20984 [D loss: 0.622332, acc: 67.97%] [G loss: 1.988700]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20985 [D loss: 0.593789, acc: 66.41%] [G loss: 2.037245]\n",
      "epoch:22 step:20986 [D loss: 0.663474, acc: 57.81%] [G loss: 1.961022]\n",
      "epoch:22 step:20987 [D loss: 0.691300, acc: 56.25%] [G loss: 1.763399]\n",
      "epoch:22 step:20988 [D loss: 0.591596, acc: 71.09%] [G loss: 2.029585]\n",
      "epoch:22 step:20989 [D loss: 0.671568, acc: 61.72%] [G loss: 1.867130]\n",
      "epoch:22 step:20990 [D loss: 0.655999, acc: 63.28%] [G loss: 1.836424]\n",
      "epoch:22 step:20991 [D loss: 0.708698, acc: 57.03%] [G loss: 1.860002]\n",
      "epoch:22 step:20992 [D loss: 0.644289, acc: 60.16%] [G loss: 1.805689]\n",
      "epoch:22 step:20993 [D loss: 0.605277, acc: 67.97%] [G loss: 1.887457]\n",
      "epoch:22 step:20994 [D loss: 0.684161, acc: 62.50%] [G loss: 1.854300]\n",
      "epoch:22 step:20995 [D loss: 0.680102, acc: 58.59%] [G loss: 1.874569]\n",
      "epoch:22 step:20996 [D loss: 0.601818, acc: 75.78%] [G loss: 1.850514]\n",
      "epoch:22 step:20997 [D loss: 0.630816, acc: 64.06%] [G loss: 1.983679]\n",
      "epoch:22 step:20998 [D loss: 0.635654, acc: 64.84%] [G loss: 2.009982]\n",
      "epoch:22 step:20999 [D loss: 0.608924, acc: 67.19%] [G loss: 1.973885]\n",
      "epoch:22 step:21000 [D loss: 0.638570, acc: 63.28%] [G loss: 1.827597]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.343232\n",
      "FID: 11.555017\n",
      "0 = 12.949336334419257\n",
      "1 = 0.09680229658287173\n",
      "2 = 0.8881999850273132\n",
      "3 = 0.9146000146865845\n",
      "4 = 0.8618000149726868\n",
      "5 = 0.8687310218811035\n",
      "6 = 0.9146000146865845\n",
      "7 = 6.345570418584334\n",
      "8 = 0.0647143071600747\n",
      "9 = 0.7232999801635742\n",
      "10 = 0.7332000136375427\n",
      "11 = 0.7134000062942505\n",
      "12 = 0.7189645171165466\n",
      "13 = 0.7332000136375427\n",
      "14 = 7.343258857727051\n",
      "15 = 9.468811988830566\n",
      "16 = 0.10777667909860611\n",
      "17 = 7.343231678009033\n",
      "18 = 11.55501651763916\n",
      "epoch:22 step:21001 [D loss: 0.708539, acc: 53.91%] [G loss: 1.766466]\n",
      "epoch:22 step:21002 [D loss: 0.625055, acc: 64.06%] [G loss: 1.762800]\n",
      "epoch:22 step:21003 [D loss: 0.659256, acc: 59.38%] [G loss: 1.803863]\n",
      "epoch:22 step:21004 [D loss: 0.641606, acc: 62.50%] [G loss: 1.881247]\n",
      "epoch:22 step:21005 [D loss: 0.672256, acc: 59.38%] [G loss: 1.786701]\n",
      "epoch:22 step:21006 [D loss: 0.650787, acc: 63.28%] [G loss: 1.879836]\n",
      "epoch:22 step:21007 [D loss: 0.644051, acc: 61.72%] [G loss: 1.896271]\n",
      "epoch:22 step:21008 [D loss: 0.648604, acc: 62.50%] [G loss: 1.811747]\n",
      "epoch:22 step:21009 [D loss: 0.644915, acc: 60.16%] [G loss: 1.974609]\n",
      "epoch:22 step:21010 [D loss: 0.709567, acc: 55.47%] [G loss: 1.832936]\n",
      "epoch:22 step:21011 [D loss: 0.644197, acc: 66.41%] [G loss: 1.815955]\n",
      "epoch:22 step:21012 [D loss: 0.566588, acc: 75.78%] [G loss: 1.864524]\n",
      "epoch:22 step:21013 [D loss: 0.635060, acc: 63.28%] [G loss: 1.991230]\n",
      "epoch:22 step:21014 [D loss: 0.653514, acc: 56.25%] [G loss: 1.892249]\n",
      "epoch:22 step:21015 [D loss: 0.696359, acc: 62.50%] [G loss: 1.872988]\n",
      "epoch:22 step:21016 [D loss: 0.649827, acc: 60.16%] [G loss: 1.942170]\n",
      "epoch:22 step:21017 [D loss: 0.613675, acc: 63.28%] [G loss: 1.982525]\n",
      "epoch:22 step:21018 [D loss: 0.608677, acc: 67.19%] [G loss: 2.111426]\n",
      "epoch:22 step:21019 [D loss: 0.604535, acc: 68.75%] [G loss: 2.097568]\n",
      "epoch:22 step:21020 [D loss: 0.560507, acc: 72.66%] [G loss: 2.223319]\n",
      "epoch:22 step:21021 [D loss: 0.637643, acc: 64.06%] [G loss: 1.991953]\n",
      "epoch:22 step:21022 [D loss: 0.644231, acc: 57.81%] [G loss: 1.804234]\n",
      "epoch:22 step:21023 [D loss: 0.684009, acc: 59.38%] [G loss: 1.848589]\n",
      "epoch:22 step:21024 [D loss: 0.651835, acc: 57.03%] [G loss: 1.948476]\n",
      "epoch:22 step:21025 [D loss: 0.642537, acc: 67.19%] [G loss: 1.940920]\n",
      "epoch:22 step:21026 [D loss: 0.625914, acc: 69.53%] [G loss: 2.003235]\n",
      "epoch:22 step:21027 [D loss: 0.648364, acc: 63.28%] [G loss: 1.871233]\n",
      "epoch:22 step:21028 [D loss: 0.642951, acc: 61.72%] [G loss: 2.059241]\n",
      "epoch:22 step:21029 [D loss: 0.635243, acc: 66.41%] [G loss: 2.041122]\n",
      "epoch:22 step:21030 [D loss: 0.605459, acc: 66.41%] [G loss: 2.071048]\n",
      "epoch:22 step:21031 [D loss: 0.617520, acc: 64.06%] [G loss: 2.213064]\n",
      "epoch:22 step:21032 [D loss: 0.735379, acc: 56.25%] [G loss: 1.895308]\n",
      "epoch:22 step:21033 [D loss: 0.593875, acc: 72.66%] [G loss: 2.006261]\n",
      "epoch:22 step:21034 [D loss: 0.685108, acc: 60.16%] [G loss: 1.922109]\n",
      "epoch:22 step:21035 [D loss: 0.642640, acc: 64.06%] [G loss: 1.772585]\n",
      "epoch:22 step:21036 [D loss: 0.675755, acc: 60.16%] [G loss: 1.847672]\n",
      "epoch:22 step:21037 [D loss: 0.654629, acc: 62.50%] [G loss: 1.974453]\n",
      "epoch:22 step:21038 [D loss: 0.684653, acc: 62.50%] [G loss: 1.820907]\n",
      "epoch:22 step:21039 [D loss: 0.615273, acc: 65.62%] [G loss: 1.821669]\n",
      "epoch:22 step:21040 [D loss: 0.678919, acc: 62.50%] [G loss: 1.787684]\n",
      "epoch:22 step:21041 [D loss: 0.625054, acc: 69.53%] [G loss: 1.855435]\n",
      "epoch:22 step:21042 [D loss: 0.551296, acc: 75.78%] [G loss: 2.140727]\n",
      "epoch:22 step:21043 [D loss: 0.620810, acc: 64.06%] [G loss: 2.222034]\n",
      "epoch:22 step:21044 [D loss: 0.649053, acc: 61.72%] [G loss: 2.184623]\n",
      "epoch:22 step:21045 [D loss: 0.564340, acc: 72.66%] [G loss: 2.045652]\n",
      "epoch:22 step:21046 [D loss: 0.654276, acc: 64.84%] [G loss: 1.790897]\n",
      "epoch:22 step:21047 [D loss: 0.620740, acc: 64.06%] [G loss: 1.846101]\n",
      "epoch:22 step:21048 [D loss: 0.598977, acc: 69.53%] [G loss: 2.138366]\n",
      "epoch:22 step:21049 [D loss: 0.632487, acc: 64.84%] [G loss: 2.022421]\n",
      "epoch:22 step:21050 [D loss: 0.663820, acc: 58.59%] [G loss: 2.150668]\n",
      "epoch:22 step:21051 [D loss: 0.683869, acc: 60.94%] [G loss: 1.716317]\n",
      "epoch:22 step:21052 [D loss: 0.698123, acc: 53.12%] [G loss: 1.756295]\n",
      "epoch:22 step:21053 [D loss: 0.691127, acc: 59.38%] [G loss: 1.698437]\n",
      "epoch:22 step:21054 [D loss: 0.666585, acc: 61.72%] [G loss: 1.844038]\n",
      "epoch:22 step:21055 [D loss: 0.676130, acc: 54.69%] [G loss: 1.885167]\n",
      "epoch:22 step:21056 [D loss: 0.649266, acc: 59.38%] [G loss: 1.867113]\n",
      "epoch:22 step:21057 [D loss: 0.726710, acc: 55.47%] [G loss: 1.796492]\n",
      "epoch:22 step:21058 [D loss: 0.653394, acc: 62.50%] [G loss: 1.684752]\n",
      "epoch:22 step:21059 [D loss: 0.642734, acc: 64.84%] [G loss: 1.831132]\n",
      "epoch:22 step:21060 [D loss: 0.669367, acc: 60.94%] [G loss: 1.800372]\n",
      "epoch:22 step:21061 [D loss: 0.688188, acc: 58.59%] [G loss: 1.928192]\n",
      "epoch:22 step:21062 [D loss: 0.616621, acc: 67.97%] [G loss: 1.696117]\n",
      "epoch:22 step:21063 [D loss: 0.633029, acc: 62.50%] [G loss: 1.938132]\n",
      "epoch:22 step:21064 [D loss: 0.591973, acc: 68.75%] [G loss: 1.854131]\n",
      "epoch:22 step:21065 [D loss: 0.596449, acc: 69.53%] [G loss: 1.939362]\n",
      "epoch:22 step:21066 [D loss: 0.612007, acc: 67.97%] [G loss: 1.834996]\n",
      "epoch:22 step:21067 [D loss: 0.668521, acc: 56.25%] [G loss: 1.975914]\n",
      "epoch:22 step:21068 [D loss: 0.680116, acc: 62.50%] [G loss: 1.919313]\n",
      "epoch:22 step:21069 [D loss: 0.651138, acc: 60.16%] [G loss: 1.902405]\n",
      "epoch:22 step:21070 [D loss: 0.649389, acc: 58.59%] [G loss: 1.942330]\n",
      "epoch:22 step:21071 [D loss: 0.664739, acc: 62.50%] [G loss: 1.977729]\n",
      "epoch:22 step:21072 [D loss: 0.654594, acc: 57.03%] [G loss: 1.834176]\n",
      "epoch:22 step:21073 [D loss: 0.685896, acc: 53.12%] [G loss: 1.818103]\n",
      "epoch:22 step:21074 [D loss: 0.674335, acc: 58.59%] [G loss: 1.769217]\n",
      "epoch:22 step:21075 [D loss: 0.614683, acc: 66.41%] [G loss: 1.850380]\n",
      "epoch:22 step:21076 [D loss: 0.678535, acc: 58.59%] [G loss: 1.808479]\n",
      "epoch:22 step:21077 [D loss: 0.640396, acc: 64.84%] [G loss: 1.868361]\n",
      "epoch:22 step:21078 [D loss: 0.691630, acc: 58.59%] [G loss: 1.962313]\n",
      "epoch:22 step:21079 [D loss: 0.663510, acc: 60.16%] [G loss: 1.977750]\n",
      "epoch:22 step:21080 [D loss: 0.629085, acc: 65.62%] [G loss: 2.081851]\n",
      "epoch:22 step:21081 [D loss: 0.629217, acc: 67.19%] [G loss: 1.888877]\n",
      "epoch:22 step:21082 [D loss: 0.565148, acc: 71.88%] [G loss: 2.241566]\n",
      "epoch:22 step:21083 [D loss: 0.598021, acc: 67.97%] [G loss: 2.066027]\n",
      "epoch:22 step:21084 [D loss: 0.639318, acc: 66.41%] [G loss: 2.069449]\n",
      "epoch:22 step:21085 [D loss: 0.584817, acc: 64.84%] [G loss: 2.213177]\n",
      "epoch:22 step:21086 [D loss: 0.597784, acc: 67.19%] [G loss: 2.172980]\n",
      "epoch:22 step:21087 [D loss: 0.713802, acc: 53.91%] [G loss: 1.728563]\n",
      "epoch:22 step:21088 [D loss: 0.654862, acc: 64.06%] [G loss: 1.801561]\n",
      "epoch:22 step:21089 [D loss: 0.653971, acc: 65.62%] [G loss: 1.926583]\n",
      "epoch:22 step:21090 [D loss: 0.683516, acc: 56.25%] [G loss: 1.878286]\n",
      "epoch:22 step:21091 [D loss: 0.686917, acc: 56.25%] [G loss: 1.704305]\n",
      "epoch:22 step:21092 [D loss: 0.639053, acc: 62.50%] [G loss: 1.877980]\n",
      "epoch:22 step:21093 [D loss: 0.629609, acc: 58.59%] [G loss: 1.891823]\n",
      "epoch:22 step:21094 [D loss: 0.635416, acc: 64.84%] [G loss: 2.126548]\n",
      "epoch:22 step:21095 [D loss: 0.547828, acc: 76.56%] [G loss: 2.118308]\n",
      "epoch:22 step:21096 [D loss: 0.638175, acc: 62.50%] [G loss: 1.819663]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21097 [D loss: 0.690879, acc: 55.47%] [G loss: 1.810331]\n",
      "epoch:22 step:21098 [D loss: 0.618387, acc: 64.84%] [G loss: 2.021643]\n",
      "epoch:22 step:21099 [D loss: 0.657799, acc: 63.28%] [G loss: 1.861028]\n",
      "epoch:22 step:21100 [D loss: 0.658983, acc: 60.16%] [G loss: 1.869859]\n",
      "epoch:22 step:21101 [D loss: 0.662918, acc: 54.69%] [G loss: 1.861133]\n",
      "epoch:22 step:21102 [D loss: 0.662960, acc: 62.50%] [G loss: 2.078394]\n",
      "epoch:22 step:21103 [D loss: 0.600347, acc: 63.28%] [G loss: 1.972097]\n",
      "epoch:22 step:21104 [D loss: 0.628878, acc: 65.62%] [G loss: 1.815208]\n",
      "epoch:22 step:21105 [D loss: 0.679594, acc: 62.50%] [G loss: 1.816917]\n",
      "epoch:22 step:21106 [D loss: 0.673020, acc: 61.72%] [G loss: 1.916147]\n",
      "epoch:22 step:21107 [D loss: 0.656502, acc: 59.38%] [G loss: 1.895960]\n",
      "epoch:22 step:21108 [D loss: 0.609394, acc: 61.72%] [G loss: 2.037340]\n",
      "epoch:22 step:21109 [D loss: 0.613451, acc: 67.19%] [G loss: 1.918939]\n",
      "epoch:22 step:21110 [D loss: 0.629655, acc: 66.41%] [G loss: 1.848302]\n",
      "epoch:22 step:21111 [D loss: 0.596376, acc: 67.19%] [G loss: 2.031150]\n",
      "epoch:22 step:21112 [D loss: 0.607878, acc: 64.84%] [G loss: 1.965929]\n",
      "epoch:22 step:21113 [D loss: 0.573631, acc: 73.44%] [G loss: 2.105897]\n",
      "epoch:22 step:21114 [D loss: 0.728235, acc: 46.88%] [G loss: 1.815426]\n",
      "epoch:22 step:21115 [D loss: 0.709698, acc: 53.12%] [G loss: 1.795452]\n",
      "epoch:22 step:21116 [D loss: 0.701937, acc: 55.47%] [G loss: 1.760573]\n",
      "epoch:22 step:21117 [D loss: 0.663868, acc: 58.59%] [G loss: 1.853058]\n",
      "epoch:22 step:21118 [D loss: 0.647747, acc: 57.81%] [G loss: 1.972328]\n",
      "epoch:22 step:21119 [D loss: 0.671879, acc: 58.59%] [G loss: 1.869143]\n",
      "epoch:22 step:21120 [D loss: 0.675563, acc: 55.47%] [G loss: 1.814567]\n",
      "epoch:22 step:21121 [D loss: 0.656004, acc: 60.16%] [G loss: 1.726575]\n",
      "epoch:22 step:21122 [D loss: 0.616871, acc: 70.31%] [G loss: 2.009403]\n",
      "epoch:22 step:21123 [D loss: 0.656377, acc: 63.28%] [G loss: 1.870458]\n",
      "epoch:22 step:21124 [D loss: 0.618167, acc: 65.62%] [G loss: 1.898032]\n",
      "epoch:22 step:21125 [D loss: 0.703297, acc: 54.69%] [G loss: 1.719120]\n",
      "epoch:22 step:21126 [D loss: 0.646788, acc: 61.72%] [G loss: 1.869526]\n",
      "epoch:22 step:21127 [D loss: 0.690701, acc: 59.38%] [G loss: 1.877175]\n",
      "epoch:22 step:21128 [D loss: 0.616743, acc: 66.41%] [G loss: 1.862599]\n",
      "epoch:22 step:21129 [D loss: 0.682305, acc: 57.81%] [G loss: 1.793199]\n",
      "epoch:22 step:21130 [D loss: 0.615359, acc: 69.53%] [G loss: 1.833770]\n",
      "epoch:22 step:21131 [D loss: 0.644830, acc: 60.16%] [G loss: 1.975363]\n",
      "epoch:22 step:21132 [D loss: 0.673599, acc: 57.81%] [G loss: 1.738708]\n",
      "epoch:22 step:21133 [D loss: 0.606448, acc: 68.75%] [G loss: 2.067695]\n",
      "epoch:22 step:21134 [D loss: 0.583652, acc: 69.53%] [G loss: 1.914606]\n",
      "epoch:22 step:21135 [D loss: 0.647308, acc: 60.16%] [G loss: 1.947871]\n",
      "epoch:22 step:21136 [D loss: 0.597680, acc: 65.62%] [G loss: 1.913213]\n",
      "epoch:22 step:21137 [D loss: 0.652763, acc: 61.72%] [G loss: 1.780505]\n",
      "epoch:22 step:21138 [D loss: 0.638303, acc: 70.31%] [G loss: 1.933471]\n",
      "epoch:22 step:21139 [D loss: 0.690605, acc: 57.03%] [G loss: 1.859757]\n",
      "epoch:22 step:21140 [D loss: 0.626887, acc: 61.72%] [G loss: 1.785784]\n",
      "epoch:22 step:21141 [D loss: 0.666918, acc: 61.72%] [G loss: 1.837798]\n",
      "epoch:22 step:21142 [D loss: 0.704214, acc: 54.69%] [G loss: 1.766711]\n",
      "epoch:22 step:21143 [D loss: 0.696529, acc: 57.03%] [G loss: 1.702286]\n",
      "epoch:22 step:21144 [D loss: 0.677343, acc: 62.50%] [G loss: 1.773330]\n",
      "epoch:22 step:21145 [D loss: 0.627012, acc: 60.16%] [G loss: 1.868163]\n",
      "epoch:22 step:21146 [D loss: 0.635103, acc: 61.72%] [G loss: 1.832289]\n",
      "epoch:22 step:21147 [D loss: 0.649377, acc: 64.84%] [G loss: 1.745165]\n",
      "epoch:22 step:21148 [D loss: 0.615251, acc: 66.41%] [G loss: 1.998504]\n",
      "epoch:22 step:21149 [D loss: 0.650771, acc: 64.06%] [G loss: 1.866811]\n",
      "epoch:22 step:21150 [D loss: 0.604759, acc: 71.09%] [G loss: 1.904033]\n",
      "epoch:22 step:21151 [D loss: 0.613949, acc: 68.75%] [G loss: 1.775498]\n",
      "epoch:22 step:21152 [D loss: 0.667442, acc: 63.28%] [G loss: 1.747196]\n",
      "epoch:22 step:21153 [D loss: 0.624247, acc: 67.97%] [G loss: 1.915171]\n",
      "epoch:22 step:21154 [D loss: 0.659852, acc: 56.25%] [G loss: 1.797915]\n",
      "epoch:22 step:21155 [D loss: 0.626807, acc: 64.84%] [G loss: 1.835142]\n",
      "epoch:22 step:21156 [D loss: 0.666002, acc: 61.72%] [G loss: 1.840503]\n",
      "epoch:22 step:21157 [D loss: 0.683338, acc: 55.47%] [G loss: 1.858111]\n",
      "epoch:22 step:21158 [D loss: 0.661064, acc: 55.47%] [G loss: 1.767259]\n",
      "epoch:22 step:21159 [D loss: 0.600267, acc: 68.75%] [G loss: 1.902596]\n",
      "epoch:22 step:21160 [D loss: 0.626612, acc: 65.62%] [G loss: 1.776244]\n",
      "epoch:22 step:21161 [D loss: 0.642235, acc: 68.75%] [G loss: 1.917927]\n",
      "epoch:22 step:21162 [D loss: 0.654252, acc: 64.06%] [G loss: 1.911943]\n",
      "epoch:22 step:21163 [D loss: 0.654837, acc: 64.06%] [G loss: 1.951298]\n",
      "epoch:22 step:21164 [D loss: 0.647289, acc: 60.16%] [G loss: 1.881634]\n",
      "epoch:22 step:21165 [D loss: 0.596690, acc: 71.88%] [G loss: 2.021905]\n",
      "epoch:22 step:21166 [D loss: 0.593908, acc: 68.75%] [G loss: 2.088305]\n",
      "epoch:22 step:21167 [D loss: 0.637352, acc: 67.97%] [G loss: 1.736634]\n",
      "epoch:22 step:21168 [D loss: 0.647788, acc: 63.28%] [G loss: 1.966362]\n",
      "epoch:22 step:21169 [D loss: 0.593282, acc: 66.41%] [G loss: 1.933921]\n",
      "epoch:22 step:21170 [D loss: 0.649207, acc: 67.19%] [G loss: 1.901508]\n",
      "epoch:22 step:21171 [D loss: 0.621389, acc: 61.72%] [G loss: 2.012060]\n",
      "epoch:22 step:21172 [D loss: 0.633794, acc: 65.62%] [G loss: 2.030673]\n",
      "epoch:22 step:21173 [D loss: 0.676054, acc: 57.03%] [G loss: 1.801952]\n",
      "epoch:22 step:21174 [D loss: 0.681367, acc: 53.91%] [G loss: 1.900722]\n",
      "epoch:22 step:21175 [D loss: 0.601874, acc: 73.44%] [G loss: 1.908244]\n",
      "epoch:22 step:21176 [D loss: 0.583795, acc: 69.53%] [G loss: 1.940871]\n",
      "epoch:22 step:21177 [D loss: 0.586782, acc: 71.09%] [G loss: 1.880534]\n",
      "epoch:22 step:21178 [D loss: 0.615230, acc: 67.97%] [G loss: 2.053866]\n",
      "epoch:22 step:21179 [D loss: 0.669424, acc: 58.59%] [G loss: 1.804733]\n",
      "epoch:22 step:21180 [D loss: 0.696948, acc: 57.03%] [G loss: 1.855653]\n",
      "epoch:22 step:21181 [D loss: 0.623627, acc: 63.28%] [G loss: 1.912313]\n",
      "epoch:22 step:21182 [D loss: 0.649178, acc: 62.50%] [G loss: 1.802074]\n",
      "epoch:22 step:21183 [D loss: 0.646411, acc: 60.94%] [G loss: 1.882568]\n",
      "epoch:22 step:21184 [D loss: 0.624471, acc: 64.06%] [G loss: 1.731172]\n",
      "epoch:22 step:21185 [D loss: 0.663735, acc: 56.25%] [G loss: 1.953422]\n",
      "epoch:22 step:21186 [D loss: 0.653715, acc: 60.94%] [G loss: 1.996158]\n",
      "epoch:22 step:21187 [D loss: 0.638271, acc: 65.62%] [G loss: 1.916808]\n",
      "epoch:22 step:21188 [D loss: 0.663991, acc: 67.97%] [G loss: 1.997980]\n",
      "epoch:22 step:21189 [D loss: 0.619182, acc: 64.06%] [G loss: 1.976303]\n",
      "epoch:22 step:21190 [D loss: 0.664500, acc: 60.16%] [G loss: 1.763865]\n",
      "epoch:22 step:21191 [D loss: 0.676608, acc: 58.59%] [G loss: 1.715851]\n",
      "epoch:22 step:21192 [D loss: 0.662231, acc: 64.84%] [G loss: 1.897647]\n",
      "epoch:22 step:21193 [D loss: 0.660878, acc: 59.38%] [G loss: 1.649979]\n",
      "epoch:22 step:21194 [D loss: 0.648017, acc: 57.03%] [G loss: 1.761775]\n",
      "epoch:22 step:21195 [D loss: 0.680413, acc: 62.50%] [G loss: 1.852721]\n",
      "epoch:22 step:21196 [D loss: 0.646255, acc: 62.50%] [G loss: 1.841574]\n",
      "epoch:22 step:21197 [D loss: 0.634980, acc: 61.72%] [G loss: 2.022962]\n",
      "epoch:22 step:21198 [D loss: 0.688679, acc: 55.47%] [G loss: 1.781769]\n",
      "epoch:22 step:21199 [D loss: 0.663263, acc: 60.94%] [G loss: 1.881220]\n",
      "epoch:22 step:21200 [D loss: 0.632520, acc: 62.50%] [G loss: 1.880967]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.539258\n",
      "FID: 10.321691\n",
      "0 = 12.573717558479313\n",
      "1 = 0.08319501523894271\n",
      "2 = 0.879800021648407\n",
      "3 = 0.8953999876976013\n",
      "4 = 0.8641999959945679\n",
      "5 = 0.8683087825775146\n",
      "6 = 0.8953999876976013\n",
      "7 = 6.152151681792725\n",
      "8 = 0.058468494049767984\n",
      "9 = 0.7027999758720398\n",
      "10 = 0.7174000144004822\n",
      "11 = 0.6881999969482422\n",
      "12 = 0.6970462203025818\n",
      "13 = 0.7174000144004822\n",
      "14 = 7.539289951324463\n",
      "15 = 9.466690063476562\n",
      "16 = 0.10767457634210587\n",
      "17 = 7.539257526397705\n",
      "18 = 10.321690559387207\n",
      "epoch:22 step:21201 [D loss: 0.677818, acc: 53.91%] [G loss: 1.949916]\n",
      "epoch:22 step:21202 [D loss: 0.585369, acc: 68.75%] [G loss: 2.002923]\n",
      "epoch:22 step:21203 [D loss: 0.646367, acc: 62.50%] [G loss: 1.974957]\n",
      "epoch:22 step:21204 [D loss: 0.669808, acc: 57.81%] [G loss: 1.948206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21205 [D loss: 0.636190, acc: 63.28%] [G loss: 1.980510]\n",
      "epoch:22 step:21206 [D loss: 0.635206, acc: 62.50%] [G loss: 1.938693]\n",
      "epoch:22 step:21207 [D loss: 0.669083, acc: 64.84%] [G loss: 1.924810]\n",
      "epoch:22 step:21208 [D loss: 0.663812, acc: 59.38%] [G loss: 1.724182]\n",
      "epoch:22 step:21209 [D loss: 0.633325, acc: 60.94%] [G loss: 1.888727]\n",
      "epoch:22 step:21210 [D loss: 0.610271, acc: 64.84%] [G loss: 1.813351]\n",
      "epoch:22 step:21211 [D loss: 0.677608, acc: 60.94%] [G loss: 1.804904]\n",
      "epoch:22 step:21212 [D loss: 0.682445, acc: 56.25%] [G loss: 1.870102]\n",
      "epoch:22 step:21213 [D loss: 0.688528, acc: 53.12%] [G loss: 1.821345]\n",
      "epoch:22 step:21214 [D loss: 0.627022, acc: 66.41%] [G loss: 1.954037]\n",
      "epoch:22 step:21215 [D loss: 0.696097, acc: 54.69%] [G loss: 1.877821]\n",
      "epoch:22 step:21216 [D loss: 0.691935, acc: 54.69%] [G loss: 1.863315]\n",
      "epoch:22 step:21217 [D loss: 0.673977, acc: 62.50%] [G loss: 1.885291]\n",
      "epoch:22 step:21218 [D loss: 0.634457, acc: 57.81%] [G loss: 1.925955]\n",
      "epoch:22 step:21219 [D loss: 0.646687, acc: 63.28%] [G loss: 1.922533]\n",
      "epoch:22 step:21220 [D loss: 0.634837, acc: 65.62%] [G loss: 1.744686]\n",
      "epoch:22 step:21221 [D loss: 0.631554, acc: 64.84%] [G loss: 1.786619]\n",
      "epoch:22 step:21222 [D loss: 0.654449, acc: 59.38%] [G loss: 1.776511]\n",
      "epoch:22 step:21223 [D loss: 0.671085, acc: 57.03%] [G loss: 1.794720]\n",
      "epoch:22 step:21224 [D loss: 0.618141, acc: 63.28%] [G loss: 1.856424]\n",
      "epoch:22 step:21225 [D loss: 0.640473, acc: 65.62%] [G loss: 1.851432]\n",
      "epoch:22 step:21226 [D loss: 0.662557, acc: 58.59%] [G loss: 1.758547]\n",
      "epoch:22 step:21227 [D loss: 0.620500, acc: 60.94%] [G loss: 1.964670]\n",
      "epoch:22 step:21228 [D loss: 0.636964, acc: 66.41%] [G loss: 1.728118]\n",
      "epoch:22 step:21229 [D loss: 0.670085, acc: 55.47%] [G loss: 1.911551]\n",
      "epoch:22 step:21230 [D loss: 0.666388, acc: 60.94%] [G loss: 1.910396]\n",
      "epoch:22 step:21231 [D loss: 0.600274, acc: 64.84%] [G loss: 1.899794]\n",
      "epoch:22 step:21232 [D loss: 0.681132, acc: 56.25%] [G loss: 1.828789]\n",
      "epoch:22 step:21233 [D loss: 0.644813, acc: 64.06%] [G loss: 1.858044]\n",
      "epoch:22 step:21234 [D loss: 0.640445, acc: 64.84%] [G loss: 1.957293]\n",
      "epoch:22 step:21235 [D loss: 0.647073, acc: 60.94%] [G loss: 1.873929]\n",
      "epoch:22 step:21236 [D loss: 0.632217, acc: 67.19%] [G loss: 1.980528]\n",
      "epoch:22 step:21237 [D loss: 0.659908, acc: 59.38%] [G loss: 1.820644]\n",
      "epoch:22 step:21238 [D loss: 0.626544, acc: 63.28%] [G loss: 1.908633]\n",
      "epoch:22 step:21239 [D loss: 0.678823, acc: 53.12%] [G loss: 1.752167]\n",
      "epoch:22 step:21240 [D loss: 0.694584, acc: 52.34%] [G loss: 1.826010]\n",
      "epoch:22 step:21241 [D loss: 0.633925, acc: 64.84%] [G loss: 1.781810]\n",
      "epoch:22 step:21242 [D loss: 0.709224, acc: 53.12%] [G loss: 1.765395]\n",
      "epoch:22 step:21243 [D loss: 0.627789, acc: 64.06%] [G loss: 1.799673]\n",
      "epoch:22 step:21244 [D loss: 0.697451, acc: 59.38%] [G loss: 1.926684]\n",
      "epoch:22 step:21245 [D loss: 0.612843, acc: 65.62%] [G loss: 1.878940]\n",
      "epoch:22 step:21246 [D loss: 0.585645, acc: 67.97%] [G loss: 1.995365]\n",
      "epoch:22 step:21247 [D loss: 0.637522, acc: 63.28%] [G loss: 1.916280]\n",
      "epoch:22 step:21248 [D loss: 0.618953, acc: 73.44%] [G loss: 2.061701]\n",
      "epoch:22 step:21249 [D loss: 0.639242, acc: 62.50%] [G loss: 2.062306]\n",
      "epoch:22 step:21250 [D loss: 0.594651, acc: 69.53%] [G loss: 1.882152]\n",
      "epoch:22 step:21251 [D loss: 0.604800, acc: 68.75%] [G loss: 2.213716]\n",
      "epoch:22 step:21252 [D loss: 0.661908, acc: 58.59%] [G loss: 1.887735]\n",
      "epoch:22 step:21253 [D loss: 0.617605, acc: 60.94%] [G loss: 1.910205]\n",
      "epoch:22 step:21254 [D loss: 0.642430, acc: 62.50%] [G loss: 1.916680]\n",
      "epoch:22 step:21255 [D loss: 0.663749, acc: 59.38%] [G loss: 2.007225]\n",
      "epoch:22 step:21256 [D loss: 0.605746, acc: 69.53%] [G loss: 2.055610]\n",
      "epoch:22 step:21257 [D loss: 0.659867, acc: 60.16%] [G loss: 1.897728]\n",
      "epoch:22 step:21258 [D loss: 0.654424, acc: 60.94%] [G loss: 2.067874]\n",
      "epoch:22 step:21259 [D loss: 0.601952, acc: 69.53%] [G loss: 1.945539]\n",
      "epoch:22 step:21260 [D loss: 0.591121, acc: 70.31%] [G loss: 1.882810]\n",
      "epoch:22 step:21261 [D loss: 0.698424, acc: 58.59%] [G loss: 2.003244]\n",
      "epoch:22 step:21262 [D loss: 0.573887, acc: 72.66%] [G loss: 2.081690]\n",
      "epoch:22 step:21263 [D loss: 0.645978, acc: 61.72%] [G loss: 2.153878]\n",
      "epoch:22 step:21264 [D loss: 0.627705, acc: 65.62%] [G loss: 2.111151]\n",
      "epoch:22 step:21265 [D loss: 0.615902, acc: 64.06%] [G loss: 1.945677]\n",
      "epoch:22 step:21266 [D loss: 0.671086, acc: 59.38%] [G loss: 1.869510]\n",
      "epoch:22 step:21267 [D loss: 0.640339, acc: 65.62%] [G loss: 2.125155]\n",
      "epoch:22 step:21268 [D loss: 0.621598, acc: 65.62%] [G loss: 1.968625]\n",
      "epoch:22 step:21269 [D loss: 0.691740, acc: 63.28%] [G loss: 2.056219]\n",
      "epoch:22 step:21270 [D loss: 0.740728, acc: 52.34%] [G loss: 1.879911]\n",
      "epoch:22 step:21271 [D loss: 0.653275, acc: 61.72%] [G loss: 1.974871]\n",
      "epoch:22 step:21272 [D loss: 0.660588, acc: 59.38%] [G loss: 1.831330]\n",
      "epoch:22 step:21273 [D loss: 0.622429, acc: 60.94%] [G loss: 1.853374]\n",
      "epoch:22 step:21274 [D loss: 0.727259, acc: 55.47%] [G loss: 1.821337]\n",
      "epoch:22 step:21275 [D loss: 0.651146, acc: 63.28%] [G loss: 1.837083]\n",
      "epoch:22 step:21276 [D loss: 0.666420, acc: 57.81%] [G loss: 1.887585]\n",
      "epoch:22 step:21277 [D loss: 0.662098, acc: 61.72%] [G loss: 1.988433]\n",
      "epoch:22 step:21278 [D loss: 0.656281, acc: 61.72%] [G loss: 1.835496]\n",
      "epoch:22 step:21279 [D loss: 0.671501, acc: 60.16%] [G loss: 1.862118]\n",
      "epoch:22 step:21280 [D loss: 0.630936, acc: 69.53%] [G loss: 1.823963]\n",
      "epoch:22 step:21281 [D loss: 0.673784, acc: 59.38%] [G loss: 1.787270]\n",
      "epoch:22 step:21282 [D loss: 0.662395, acc: 56.25%] [G loss: 1.795645]\n",
      "epoch:22 step:21283 [D loss: 0.609714, acc: 68.75%] [G loss: 1.859860]\n",
      "epoch:22 step:21284 [D loss: 0.683843, acc: 59.38%] [G loss: 1.835147]\n",
      "epoch:22 step:21285 [D loss: 0.670583, acc: 57.81%] [G loss: 1.807359]\n",
      "epoch:22 step:21286 [D loss: 0.631945, acc: 64.06%] [G loss: 1.946631]\n",
      "epoch:22 step:21287 [D loss: 0.652668, acc: 64.84%] [G loss: 1.782045]\n",
      "epoch:22 step:21288 [D loss: 0.625473, acc: 67.19%] [G loss: 1.878112]\n",
      "epoch:22 step:21289 [D loss: 0.677979, acc: 57.03%] [G loss: 1.842914]\n",
      "epoch:22 step:21290 [D loss: 0.645628, acc: 61.72%] [G loss: 1.760557]\n",
      "epoch:22 step:21291 [D loss: 0.584743, acc: 70.31%] [G loss: 1.976126]\n",
      "epoch:22 step:21292 [D loss: 0.590963, acc: 67.19%] [G loss: 1.956383]\n",
      "epoch:22 step:21293 [D loss: 0.629836, acc: 65.62%] [G loss: 1.839138]\n",
      "epoch:22 step:21294 [D loss: 0.677292, acc: 58.59%] [G loss: 1.749982]\n",
      "epoch:22 step:21295 [D loss: 0.639334, acc: 64.06%] [G loss: 1.915803]\n",
      "epoch:22 step:21296 [D loss: 0.731773, acc: 57.03%] [G loss: 1.903667]\n",
      "epoch:22 step:21297 [D loss: 0.631356, acc: 64.06%] [G loss: 1.785210]\n",
      "epoch:22 step:21298 [D loss: 0.638449, acc: 64.06%] [G loss: 1.810309]\n",
      "epoch:22 step:21299 [D loss: 0.641778, acc: 60.94%] [G loss: 1.989457]\n",
      "epoch:22 step:21300 [D loss: 0.603817, acc: 66.41%] [G loss: 1.909874]\n",
      "epoch:22 step:21301 [D loss: 0.694203, acc: 62.50%] [G loss: 1.951012]\n",
      "epoch:22 step:21302 [D loss: 0.656009, acc: 60.94%] [G loss: 1.915842]\n",
      "epoch:22 step:21303 [D loss: 0.622458, acc: 66.41%] [G loss: 1.987737]\n",
      "epoch:22 step:21304 [D loss: 0.627390, acc: 64.84%] [G loss: 2.035013]\n",
      "epoch:22 step:21305 [D loss: 0.628339, acc: 64.84%] [G loss: 2.020135]\n",
      "epoch:22 step:21306 [D loss: 0.660403, acc: 61.72%] [G loss: 1.890043]\n",
      "epoch:22 step:21307 [D loss: 0.634761, acc: 66.41%] [G loss: 2.076344]\n",
      "epoch:22 step:21308 [D loss: 0.637715, acc: 67.19%] [G loss: 2.159166]\n",
      "epoch:22 step:21309 [D loss: 0.604055, acc: 64.84%] [G loss: 1.986512]\n",
      "epoch:22 step:21310 [D loss: 0.640126, acc: 64.84%] [G loss: 1.773198]\n",
      "epoch:22 step:21311 [D loss: 0.676822, acc: 58.59%] [G loss: 1.862443]\n",
      "epoch:22 step:21312 [D loss: 0.677357, acc: 60.16%] [G loss: 1.807447]\n",
      "epoch:22 step:21313 [D loss: 0.647499, acc: 62.50%] [G loss: 1.952051]\n",
      "epoch:22 step:21314 [D loss: 0.647094, acc: 58.59%] [G loss: 1.921225]\n",
      "epoch:22 step:21315 [D loss: 0.644426, acc: 60.94%] [G loss: 1.961341]\n",
      "epoch:22 step:21316 [D loss: 0.657796, acc: 59.38%] [G loss: 1.824306]\n",
      "epoch:22 step:21317 [D loss: 0.638245, acc: 62.50%] [G loss: 1.870216]\n",
      "epoch:22 step:21318 [D loss: 0.680012, acc: 60.16%] [G loss: 1.741737]\n",
      "epoch:22 step:21319 [D loss: 0.670894, acc: 59.38%] [G loss: 1.813115]\n",
      "epoch:22 step:21320 [D loss: 0.663262, acc: 61.72%] [G loss: 1.920837]\n",
      "epoch:22 step:21321 [D loss: 0.576578, acc: 71.09%] [G loss: 1.862909]\n",
      "epoch:22 step:21322 [D loss: 0.597350, acc: 65.62%] [G loss: 1.995307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21323 [D loss: 0.653780, acc: 64.06%] [G loss: 1.984002]\n",
      "epoch:22 step:21324 [D loss: 0.686408, acc: 57.03%] [G loss: 1.864190]\n",
      "epoch:22 step:21325 [D loss: 0.598274, acc: 66.41%] [G loss: 1.953553]\n",
      "epoch:22 step:21326 [D loss: 0.626086, acc: 70.31%] [G loss: 2.087645]\n",
      "epoch:22 step:21327 [D loss: 0.595909, acc: 68.75%] [G loss: 1.895102]\n",
      "epoch:22 step:21328 [D loss: 0.633449, acc: 62.50%] [G loss: 1.884658]\n",
      "epoch:22 step:21329 [D loss: 0.659786, acc: 58.59%] [G loss: 1.883227]\n",
      "epoch:22 step:21330 [D loss: 0.702147, acc: 60.94%] [G loss: 1.778045]\n",
      "epoch:22 step:21331 [D loss: 0.657524, acc: 62.50%] [G loss: 1.953096]\n",
      "epoch:22 step:21332 [D loss: 0.673646, acc: 57.81%] [G loss: 1.769716]\n",
      "epoch:22 step:21333 [D loss: 0.620779, acc: 65.62%] [G loss: 2.067474]\n",
      "epoch:22 step:21334 [D loss: 0.620992, acc: 64.84%] [G loss: 2.142874]\n",
      "epoch:22 step:21335 [D loss: 0.633590, acc: 56.25%] [G loss: 2.081999]\n",
      "epoch:22 step:21336 [D loss: 0.714297, acc: 55.47%] [G loss: 1.799415]\n",
      "epoch:22 step:21337 [D loss: 0.688035, acc: 56.25%] [G loss: 1.784347]\n",
      "epoch:22 step:21338 [D loss: 0.648981, acc: 60.94%] [G loss: 1.944369]\n",
      "epoch:22 step:21339 [D loss: 0.592981, acc: 69.53%] [G loss: 1.942929]\n",
      "epoch:22 step:21340 [D loss: 0.602117, acc: 62.50%] [G loss: 1.948590]\n",
      "epoch:22 step:21341 [D loss: 0.639498, acc: 67.97%] [G loss: 1.894242]\n",
      "epoch:22 step:21342 [D loss: 0.646110, acc: 63.28%] [G loss: 1.921015]\n",
      "epoch:22 step:21343 [D loss: 0.677205, acc: 56.25%] [G loss: 1.965367]\n",
      "epoch:22 step:21344 [D loss: 0.608706, acc: 68.75%] [G loss: 1.990490]\n",
      "epoch:22 step:21345 [D loss: 0.686454, acc: 57.81%] [G loss: 1.812848]\n",
      "epoch:22 step:21346 [D loss: 0.665491, acc: 61.72%] [G loss: 1.757091]\n",
      "epoch:22 step:21347 [D loss: 0.605414, acc: 67.19%] [G loss: 1.960743]\n",
      "epoch:22 step:21348 [D loss: 0.654997, acc: 64.06%] [G loss: 1.749895]\n",
      "epoch:22 step:21349 [D loss: 0.672061, acc: 57.81%] [G loss: 2.082117]\n",
      "epoch:22 step:21350 [D loss: 0.596691, acc: 67.97%] [G loss: 1.808597]\n",
      "epoch:22 step:21351 [D loss: 0.611061, acc: 67.19%] [G loss: 1.903426]\n",
      "epoch:22 step:21352 [D loss: 0.614244, acc: 68.75%] [G loss: 1.922403]\n",
      "epoch:22 step:21353 [D loss: 0.705005, acc: 60.16%] [G loss: 1.858548]\n",
      "epoch:22 step:21354 [D loss: 0.614233, acc: 64.06%] [G loss: 1.938321]\n",
      "epoch:22 step:21355 [D loss: 0.705175, acc: 58.59%] [G loss: 1.879579]\n",
      "epoch:22 step:21356 [D loss: 0.667443, acc: 60.16%] [G loss: 1.925856]\n",
      "epoch:22 step:21357 [D loss: 0.688004, acc: 52.34%] [G loss: 1.804647]\n",
      "epoch:22 step:21358 [D loss: 0.667199, acc: 60.16%] [G loss: 1.890246]\n",
      "epoch:22 step:21359 [D loss: 0.605236, acc: 66.41%] [G loss: 1.852074]\n",
      "epoch:22 step:21360 [D loss: 0.618064, acc: 65.62%] [G loss: 1.960406]\n",
      "epoch:22 step:21361 [D loss: 0.606520, acc: 68.75%] [G loss: 1.996832]\n",
      "epoch:22 step:21362 [D loss: 0.617796, acc: 66.41%] [G loss: 1.954530]\n",
      "epoch:22 step:21363 [D loss: 0.646046, acc: 64.06%] [G loss: 1.841768]\n",
      "epoch:22 step:21364 [D loss: 0.584187, acc: 72.66%] [G loss: 1.881318]\n",
      "epoch:22 step:21365 [D loss: 0.628847, acc: 64.06%] [G loss: 1.788245]\n",
      "epoch:22 step:21366 [D loss: 0.654552, acc: 67.19%] [G loss: 1.740642]\n",
      "epoch:22 step:21367 [D loss: 0.638148, acc: 63.28%] [G loss: 1.907632]\n",
      "epoch:22 step:21368 [D loss: 0.643166, acc: 60.94%] [G loss: 1.933387]\n",
      "epoch:22 step:21369 [D loss: 0.650748, acc: 64.06%] [G loss: 1.894429]\n",
      "epoch:22 step:21370 [D loss: 0.585757, acc: 67.97%] [G loss: 1.833888]\n",
      "epoch:22 step:21371 [D loss: 0.659069, acc: 60.94%] [G loss: 1.953139]\n",
      "epoch:22 step:21372 [D loss: 0.633484, acc: 61.72%] [G loss: 1.881784]\n",
      "epoch:22 step:21373 [D loss: 0.680714, acc: 57.81%] [G loss: 1.896321]\n",
      "epoch:22 step:21374 [D loss: 0.615765, acc: 65.62%] [G loss: 1.980686]\n",
      "epoch:22 step:21375 [D loss: 0.620619, acc: 64.84%] [G loss: 1.963903]\n",
      "epoch:22 step:21376 [D loss: 0.641230, acc: 59.38%] [G loss: 1.874824]\n",
      "epoch:22 step:21377 [D loss: 0.597221, acc: 69.53%] [G loss: 1.919096]\n",
      "epoch:22 step:21378 [D loss: 0.681363, acc: 57.03%] [G loss: 1.901944]\n",
      "epoch:22 step:21379 [D loss: 0.718505, acc: 57.03%] [G loss: 1.883552]\n",
      "epoch:22 step:21380 [D loss: 0.677149, acc: 59.38%] [G loss: 1.875426]\n",
      "epoch:22 step:21381 [D loss: 0.688403, acc: 55.47%] [G loss: 1.808393]\n",
      "epoch:22 step:21382 [D loss: 0.701828, acc: 50.00%] [G loss: 1.830750]\n",
      "epoch:22 step:21383 [D loss: 0.648474, acc: 66.41%] [G loss: 1.919935]\n",
      "epoch:22 step:21384 [D loss: 0.629276, acc: 66.41%] [G loss: 1.954741]\n",
      "epoch:22 step:21385 [D loss: 0.601214, acc: 67.97%] [G loss: 1.971482]\n",
      "epoch:22 step:21386 [D loss: 0.623329, acc: 68.75%] [G loss: 1.910101]\n",
      "epoch:22 step:21387 [D loss: 0.640457, acc: 60.94%] [G loss: 1.876529]\n",
      "epoch:22 step:21388 [D loss: 0.588282, acc: 67.19%] [G loss: 2.221483]\n",
      "epoch:22 step:21389 [D loss: 0.614435, acc: 64.84%] [G loss: 1.892887]\n",
      "epoch:22 step:21390 [D loss: 0.618328, acc: 68.75%] [G loss: 1.982970]\n",
      "epoch:22 step:21391 [D loss: 0.622618, acc: 67.97%] [G loss: 1.873434]\n",
      "epoch:22 step:21392 [D loss: 0.627939, acc: 60.94%] [G loss: 2.078675]\n",
      "epoch:22 step:21393 [D loss: 0.678655, acc: 51.56%] [G loss: 2.004674]\n",
      "epoch:22 step:21394 [D loss: 0.645773, acc: 61.72%] [G loss: 1.925185]\n",
      "epoch:22 step:21395 [D loss: 0.633231, acc: 64.84%] [G loss: 2.143132]\n",
      "epoch:22 step:21396 [D loss: 0.559956, acc: 73.44%] [G loss: 2.238517]\n",
      "epoch:22 step:21397 [D loss: 0.696818, acc: 57.03%] [G loss: 1.896201]\n",
      "epoch:22 step:21398 [D loss: 0.716659, acc: 50.00%] [G loss: 1.723510]\n",
      "epoch:22 step:21399 [D loss: 0.650668, acc: 64.06%] [G loss: 1.904406]\n",
      "epoch:22 step:21400 [D loss: 0.570192, acc: 71.09%] [G loss: 2.054427]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.525798\n",
      "FID: 11.511164\n",
      "0 = 12.84292344779968\n",
      "1 = 0.08995977456471246\n",
      "2 = 0.8801000118255615\n",
      "3 = 0.9049999713897705\n",
      "4 = 0.8551999926567078\n",
      "5 = 0.8620689511299133\n",
      "6 = 0.9049999713897705\n",
      "7 = 6.2660979219317365\n",
      "8 = 0.06627777237246842\n",
      "9 = 0.7077999711036682\n",
      "10 = 0.7184000015258789\n",
      "11 = 0.6972000002861023\n",
      "12 = 0.7034860849380493\n",
      "13 = 0.7184000015258789\n",
      "14 = 7.525831699371338\n",
      "15 = 9.475564956665039\n",
      "16 = 0.10175955295562744\n",
      "17 = 7.5257978439331055\n",
      "18 = 11.511163711547852\n",
      "epoch:22 step:21401 [D loss: 0.705779, acc: 54.69%] [G loss: 1.819685]\n",
      "epoch:22 step:21402 [D loss: 0.713784, acc: 59.38%] [G loss: 1.799457]\n",
      "epoch:22 step:21403 [D loss: 0.617615, acc: 63.28%] [G loss: 2.079216]\n",
      "epoch:22 step:21404 [D loss: 0.610437, acc: 62.50%] [G loss: 1.938071]\n",
      "epoch:22 step:21405 [D loss: 0.622265, acc: 65.62%] [G loss: 1.902353]\n",
      "epoch:22 step:21406 [D loss: 0.602477, acc: 64.84%] [G loss: 1.944322]\n",
      "epoch:22 step:21407 [D loss: 0.640281, acc: 61.72%] [G loss: 1.956670]\n",
      "epoch:22 step:21408 [D loss: 0.754839, acc: 50.78%] [G loss: 1.776615]\n",
      "epoch:22 step:21409 [D loss: 0.680505, acc: 62.50%] [G loss: 1.789659]\n",
      "epoch:22 step:21410 [D loss: 0.626722, acc: 65.62%] [G loss: 1.866982]\n",
      "epoch:22 step:21411 [D loss: 0.643502, acc: 61.72%] [G loss: 1.835289]\n",
      "epoch:22 step:21412 [D loss: 0.669592, acc: 57.81%] [G loss: 1.907539]\n",
      "epoch:22 step:21413 [D loss: 0.647074, acc: 60.94%] [G loss: 1.693313]\n",
      "epoch:22 step:21414 [D loss: 0.681483, acc: 55.47%] [G loss: 1.672606]\n",
      "epoch:22 step:21415 [D loss: 0.700372, acc: 49.22%] [G loss: 1.598844]\n",
      "epoch:22 step:21416 [D loss: 0.680036, acc: 60.94%] [G loss: 1.691659]\n",
      "epoch:22 step:21417 [D loss: 0.646222, acc: 64.06%] [G loss: 1.844998]\n",
      "epoch:22 step:21418 [D loss: 0.658327, acc: 58.59%] [G loss: 1.930715]\n",
      "epoch:22 step:21419 [D loss: 0.677976, acc: 57.03%] [G loss: 1.885482]\n",
      "epoch:22 step:21420 [D loss: 0.642637, acc: 60.94%] [G loss: 1.888537]\n",
      "epoch:22 step:21421 [D loss: 0.661205, acc: 60.94%] [G loss: 1.951492]\n",
      "epoch:22 step:21422 [D loss: 0.611316, acc: 69.53%] [G loss: 1.996293]\n",
      "epoch:22 step:21423 [D loss: 0.620612, acc: 61.72%] [G loss: 1.967526]\n",
      "epoch:22 step:21424 [D loss: 0.752339, acc: 51.56%] [G loss: 1.790409]\n",
      "epoch:22 step:21425 [D loss: 0.586329, acc: 71.09%] [G loss: 1.913073]\n",
      "epoch:22 step:21426 [D loss: 0.703572, acc: 52.34%] [G loss: 1.737515]\n",
      "epoch:22 step:21427 [D loss: 0.664440, acc: 63.28%] [G loss: 1.829354]\n",
      "epoch:22 step:21428 [D loss: 0.661481, acc: 62.50%] [G loss: 1.874465]\n",
      "epoch:22 step:21429 [D loss: 0.681590, acc: 57.81%] [G loss: 2.137410]\n",
      "epoch:22 step:21430 [D loss: 0.575450, acc: 71.88%] [G loss: 2.017791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21431 [D loss: 0.644445, acc: 60.94%] [G loss: 1.818152]\n",
      "epoch:22 step:21432 [D loss: 0.651819, acc: 59.38%] [G loss: 1.772456]\n",
      "epoch:22 step:21433 [D loss: 0.626401, acc: 64.06%] [G loss: 1.853355]\n",
      "epoch:22 step:21434 [D loss: 0.701660, acc: 53.91%] [G loss: 1.751495]\n",
      "epoch:22 step:21435 [D loss: 0.656809, acc: 58.59%] [G loss: 1.822924]\n",
      "epoch:22 step:21436 [D loss: 0.669116, acc: 61.72%] [G loss: 1.893665]\n",
      "epoch:22 step:21437 [D loss: 0.577525, acc: 70.31%] [G loss: 1.940792]\n",
      "epoch:22 step:21438 [D loss: 0.663714, acc: 62.50%] [G loss: 1.745914]\n",
      "epoch:22 step:21439 [D loss: 0.642540, acc: 62.50%] [G loss: 2.066061]\n",
      "epoch:22 step:21440 [D loss: 0.628569, acc: 67.19%] [G loss: 1.865464]\n",
      "epoch:22 step:21441 [D loss: 0.645767, acc: 59.38%] [G loss: 1.786718]\n",
      "epoch:22 step:21442 [D loss: 0.679091, acc: 56.25%] [G loss: 1.808722]\n",
      "epoch:22 step:21443 [D loss: 0.662536, acc: 60.16%] [G loss: 1.682352]\n",
      "epoch:22 step:21444 [D loss: 0.669632, acc: 58.59%] [G loss: 1.797879]\n",
      "epoch:22 step:21445 [D loss: 0.656599, acc: 60.94%] [G loss: 1.870537]\n",
      "epoch:22 step:21446 [D loss: 0.595091, acc: 70.31%] [G loss: 1.832232]\n",
      "epoch:22 step:21447 [D loss: 0.619005, acc: 66.41%] [G loss: 1.810570]\n",
      "epoch:22 step:21448 [D loss: 0.674008, acc: 60.16%] [G loss: 1.896633]\n",
      "epoch:22 step:21449 [D loss: 0.621593, acc: 71.09%] [G loss: 1.877249]\n",
      "epoch:22 step:21450 [D loss: 0.651654, acc: 60.16%] [G loss: 2.003170]\n",
      "epoch:22 step:21451 [D loss: 0.602327, acc: 67.97%] [G loss: 1.988153]\n",
      "epoch:22 step:21452 [D loss: 0.662586, acc: 59.38%] [G loss: 1.837021]\n",
      "epoch:22 step:21453 [D loss: 0.659875, acc: 63.28%] [G loss: 1.727234]\n",
      "epoch:22 step:21454 [D loss: 0.601755, acc: 69.53%] [G loss: 2.008351]\n",
      "epoch:22 step:21455 [D loss: 0.578147, acc: 73.44%] [G loss: 2.101636]\n",
      "epoch:22 step:21456 [D loss: 0.621342, acc: 66.41%] [G loss: 1.861623]\n",
      "epoch:22 step:21457 [D loss: 0.637460, acc: 64.06%] [G loss: 1.917791]\n",
      "epoch:22 step:21458 [D loss: 0.643020, acc: 62.50%] [G loss: 1.959315]\n",
      "epoch:22 step:21459 [D loss: 0.610035, acc: 67.19%] [G loss: 2.086804]\n",
      "epoch:22 step:21460 [D loss: 0.722221, acc: 53.91%] [G loss: 1.929726]\n",
      "epoch:22 step:21461 [D loss: 0.675656, acc: 56.25%] [G loss: 1.747272]\n",
      "epoch:22 step:21462 [D loss: 0.642743, acc: 62.50%] [G loss: 1.832683]\n",
      "epoch:22 step:21463 [D loss: 0.654031, acc: 64.06%] [G loss: 1.896762]\n",
      "epoch:22 step:21464 [D loss: 0.668105, acc: 57.03%] [G loss: 1.733032]\n",
      "epoch:22 step:21465 [D loss: 0.659653, acc: 61.72%] [G loss: 1.771763]\n",
      "epoch:22 step:21466 [D loss: 0.636869, acc: 62.50%] [G loss: 1.945373]\n",
      "epoch:22 step:21467 [D loss: 0.684790, acc: 57.81%] [G loss: 1.955312]\n",
      "epoch:22 step:21468 [D loss: 0.606006, acc: 67.19%] [G loss: 1.889100]\n",
      "epoch:22 step:21469 [D loss: 0.664714, acc: 54.69%] [G loss: 1.751572]\n",
      "epoch:22 step:21470 [D loss: 0.672374, acc: 57.03%] [G loss: 1.731669]\n",
      "epoch:22 step:21471 [D loss: 0.653704, acc: 59.38%] [G loss: 1.899037]\n",
      "epoch:22 step:21472 [D loss: 0.669851, acc: 60.16%] [G loss: 1.881428]\n",
      "epoch:22 step:21473 [D loss: 0.724386, acc: 53.91%] [G loss: 1.816435]\n",
      "epoch:22 step:21474 [D loss: 0.612843, acc: 73.44%] [G loss: 1.939357]\n",
      "epoch:22 step:21475 [D loss: 0.676922, acc: 57.81%] [G loss: 1.803850]\n",
      "epoch:22 step:21476 [D loss: 0.641837, acc: 58.59%] [G loss: 1.654942]\n",
      "epoch:22 step:21477 [D loss: 0.624937, acc: 61.72%] [G loss: 1.952613]\n",
      "epoch:22 step:21478 [D loss: 0.646780, acc: 67.97%] [G loss: 1.785959]\n",
      "epoch:22 step:21479 [D loss: 0.713650, acc: 58.59%] [G loss: 1.884051]\n",
      "epoch:22 step:21480 [D loss: 0.701166, acc: 57.03%] [G loss: 1.880856]\n",
      "epoch:22 step:21481 [D loss: 0.646882, acc: 59.38%] [G loss: 1.848689]\n",
      "epoch:22 step:21482 [D loss: 0.644684, acc: 60.94%] [G loss: 1.704436]\n",
      "epoch:22 step:21483 [D loss: 0.654394, acc: 59.38%] [G loss: 1.887555]\n",
      "epoch:22 step:21484 [D loss: 0.690439, acc: 48.44%] [G loss: 1.765690]\n",
      "epoch:22 step:21485 [D loss: 0.661791, acc: 65.62%] [G loss: 1.895302]\n",
      "epoch:22 step:21486 [D loss: 0.636389, acc: 64.84%] [G loss: 1.936736]\n",
      "epoch:22 step:21487 [D loss: 0.679475, acc: 58.59%] [G loss: 1.729241]\n",
      "epoch:22 step:21488 [D loss: 0.667297, acc: 60.94%] [G loss: 1.945355]\n",
      "epoch:22 step:21489 [D loss: 0.604884, acc: 63.28%] [G loss: 1.906186]\n",
      "epoch:22 step:21490 [D loss: 0.675644, acc: 54.69%] [G loss: 1.848108]\n",
      "epoch:22 step:21491 [D loss: 0.690313, acc: 58.59%] [G loss: 1.809796]\n",
      "epoch:22 step:21492 [D loss: 0.657673, acc: 64.06%] [G loss: 1.765319]\n",
      "epoch:22 step:21493 [D loss: 0.641068, acc: 63.28%] [G loss: 1.734820]\n",
      "epoch:22 step:21494 [D loss: 0.653633, acc: 64.06%] [G loss: 1.915158]\n",
      "epoch:22 step:21495 [D loss: 0.675902, acc: 53.91%] [G loss: 1.847017]\n",
      "epoch:22 step:21496 [D loss: 0.628160, acc: 61.72%] [G loss: 1.950975]\n",
      "epoch:22 step:21497 [D loss: 0.630566, acc: 61.72%] [G loss: 1.832328]\n",
      "epoch:22 step:21498 [D loss: 0.633657, acc: 67.97%] [G loss: 2.073574]\n",
      "epoch:22 step:21499 [D loss: 0.625847, acc: 64.84%] [G loss: 1.882240]\n",
      "epoch:22 step:21500 [D loss: 0.659823, acc: 60.94%] [G loss: 2.165819]\n",
      "epoch:22 step:21501 [D loss: 0.686164, acc: 57.81%] [G loss: 1.950880]\n",
      "epoch:22 step:21502 [D loss: 0.600851, acc: 70.31%] [G loss: 1.937174]\n",
      "epoch:22 step:21503 [D loss: 0.615384, acc: 71.88%] [G loss: 1.818612]\n",
      "epoch:22 step:21504 [D loss: 0.654852, acc: 61.72%] [G loss: 1.891490]\n",
      "epoch:22 step:21505 [D loss: 0.681450, acc: 57.81%] [G loss: 1.797911]\n",
      "epoch:22 step:21506 [D loss: 0.632105, acc: 67.19%] [G loss: 1.964776]\n",
      "epoch:22 step:21507 [D loss: 0.660870, acc: 60.94%] [G loss: 1.867388]\n",
      "epoch:22 step:21508 [D loss: 0.639752, acc: 64.06%] [G loss: 2.003094]\n",
      "epoch:22 step:21509 [D loss: 0.697704, acc: 57.03%] [G loss: 1.834663]\n",
      "epoch:22 step:21510 [D loss: 0.650913, acc: 62.50%] [G loss: 1.814896]\n",
      "epoch:22 step:21511 [D loss: 0.663413, acc: 64.06%] [G loss: 1.782082]\n",
      "epoch:22 step:21512 [D loss: 0.672816, acc: 53.91%] [G loss: 1.919830]\n",
      "epoch:22 step:21513 [D loss: 0.599757, acc: 63.28%] [G loss: 2.081180]\n",
      "epoch:22 step:21514 [D loss: 0.674096, acc: 59.38%] [G loss: 2.019491]\n",
      "epoch:22 step:21515 [D loss: 0.671925, acc: 60.16%] [G loss: 1.920657]\n",
      "epoch:22 step:21516 [D loss: 0.654998, acc: 60.94%] [G loss: 1.733881]\n",
      "epoch:22 step:21517 [D loss: 0.708479, acc: 54.69%] [G loss: 1.791313]\n",
      "epoch:22 step:21518 [D loss: 0.637609, acc: 64.84%] [G loss: 1.894880]\n",
      "epoch:22 step:21519 [D loss: 0.601187, acc: 66.41%] [G loss: 1.860651]\n",
      "epoch:22 step:21520 [D loss: 0.674695, acc: 55.47%] [G loss: 1.893117]\n",
      "epoch:22 step:21521 [D loss: 0.579497, acc: 72.66%] [G loss: 1.913590]\n",
      "epoch:22 step:21522 [D loss: 0.676403, acc: 60.94%] [G loss: 1.970866]\n",
      "epoch:22 step:21523 [D loss: 0.583890, acc: 71.88%] [G loss: 1.922412]\n",
      "epoch:22 step:21524 [D loss: 0.591730, acc: 63.28%] [G loss: 2.033654]\n",
      "epoch:22 step:21525 [D loss: 0.572717, acc: 71.88%] [G loss: 2.045599]\n",
      "epoch:22 step:21526 [D loss: 0.614025, acc: 64.84%] [G loss: 2.075519]\n",
      "epoch:22 step:21527 [D loss: 0.657335, acc: 54.69%] [G loss: 1.923508]\n",
      "epoch:22 step:21528 [D loss: 0.618460, acc: 67.19%] [G loss: 1.849350]\n",
      "epoch:22 step:21529 [D loss: 0.672533, acc: 62.50%] [G loss: 2.064533]\n",
      "epoch:22 step:21530 [D loss: 0.650375, acc: 57.03%] [G loss: 2.007213]\n",
      "epoch:22 step:21531 [D loss: 0.660523, acc: 60.16%] [G loss: 2.066815]\n",
      "epoch:22 step:21532 [D loss: 0.572417, acc: 73.44%] [G loss: 2.166656]\n",
      "epoch:22 step:21533 [D loss: 0.549835, acc: 71.88%] [G loss: 2.212084]\n",
      "epoch:22 step:21534 [D loss: 0.756853, acc: 50.00%] [G loss: 1.844863]\n",
      "epoch:22 step:21535 [D loss: 0.595918, acc: 71.09%] [G loss: 2.034100]\n",
      "epoch:22 step:21536 [D loss: 0.662571, acc: 60.94%] [G loss: 2.132617]\n",
      "epoch:22 step:21537 [D loss: 0.565804, acc: 73.44%] [G loss: 1.996161]\n",
      "epoch:22 step:21538 [D loss: 0.584670, acc: 70.31%] [G loss: 2.219390]\n",
      "epoch:22 step:21539 [D loss: 0.553895, acc: 71.88%] [G loss: 2.008722]\n",
      "epoch:22 step:21540 [D loss: 0.672844, acc: 60.94%] [G loss: 2.173749]\n",
      "epoch:22 step:21541 [D loss: 0.618576, acc: 61.72%] [G loss: 2.187092]\n",
      "epoch:22 step:21542 [D loss: 0.742024, acc: 60.16%] [G loss: 1.794387]\n",
      "epoch:22 step:21543 [D loss: 0.706244, acc: 53.12%] [G loss: 1.905473]\n",
      "epoch:22 step:21544 [D loss: 0.637303, acc: 62.50%] [G loss: 2.021560]\n",
      "epoch:22 step:21545 [D loss: 0.657932, acc: 66.41%] [G loss: 1.971537]\n",
      "epoch:22 step:21546 [D loss: 0.683099, acc: 61.72%] [G loss: 1.896086]\n",
      "epoch:22 step:21547 [D loss: 0.661533, acc: 65.62%] [G loss: 1.954727]\n",
      "epoch:22 step:21548 [D loss: 0.656019, acc: 60.16%] [G loss: 1.959944]\n",
      "epoch:22 step:21549 [D loss: 0.625131, acc: 71.09%] [G loss: 2.004277]\n",
      "epoch:22 step:21550 [D loss: 0.658783, acc: 67.97%] [G loss: 2.013254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21551 [D loss: 0.586741, acc: 71.88%] [G loss: 2.241137]\n",
      "epoch:23 step:21552 [D loss: 0.667338, acc: 58.59%] [G loss: 2.014612]\n",
      "epoch:23 step:21553 [D loss: 0.630015, acc: 65.62%] [G loss: 2.023888]\n",
      "epoch:23 step:21554 [D loss: 0.636009, acc: 70.31%] [G loss: 1.943463]\n",
      "epoch:23 step:21555 [D loss: 0.675401, acc: 61.72%] [G loss: 1.825883]\n",
      "epoch:23 step:21556 [D loss: 0.648075, acc: 57.81%] [G loss: 1.850669]\n",
      "epoch:23 step:21557 [D loss: 0.617964, acc: 67.19%] [G loss: 2.042751]\n",
      "epoch:23 step:21558 [D loss: 0.641969, acc: 64.06%] [G loss: 2.049057]\n",
      "epoch:23 step:21559 [D loss: 0.699303, acc: 55.47%] [G loss: 1.974444]\n",
      "epoch:23 step:21560 [D loss: 0.638185, acc: 59.38%] [G loss: 2.053755]\n",
      "epoch:23 step:21561 [D loss: 0.629915, acc: 64.84%] [G loss: 2.063301]\n",
      "epoch:23 step:21562 [D loss: 0.696178, acc: 57.03%] [G loss: 1.885351]\n",
      "epoch:23 step:21563 [D loss: 0.630925, acc: 64.84%] [G loss: 1.946082]\n",
      "epoch:23 step:21564 [D loss: 0.649970, acc: 69.53%] [G loss: 1.803758]\n",
      "epoch:23 step:21565 [D loss: 0.619478, acc: 66.41%] [G loss: 1.838508]\n",
      "epoch:23 step:21566 [D loss: 0.579627, acc: 67.97%] [G loss: 2.100344]\n",
      "epoch:23 step:21567 [D loss: 0.622368, acc: 68.75%] [G loss: 2.083457]\n",
      "epoch:23 step:21568 [D loss: 0.624641, acc: 66.41%] [G loss: 1.993015]\n",
      "epoch:23 step:21569 [D loss: 0.641274, acc: 66.41%] [G loss: 1.910122]\n",
      "epoch:23 step:21570 [D loss: 0.667284, acc: 57.03%] [G loss: 1.946310]\n",
      "epoch:23 step:21571 [D loss: 0.771493, acc: 47.66%] [G loss: 1.611816]\n",
      "epoch:23 step:21572 [D loss: 0.725441, acc: 49.22%] [G loss: 1.808434]\n",
      "epoch:23 step:21573 [D loss: 0.700754, acc: 53.12%] [G loss: 1.639506]\n",
      "epoch:23 step:21574 [D loss: 0.677683, acc: 60.16%] [G loss: 1.896023]\n",
      "epoch:23 step:21575 [D loss: 0.618150, acc: 64.06%] [G loss: 1.929817]\n",
      "epoch:23 step:21576 [D loss: 0.645803, acc: 63.28%] [G loss: 1.928591]\n",
      "epoch:23 step:21577 [D loss: 0.690156, acc: 58.59%] [G loss: 1.694803]\n",
      "epoch:23 step:21578 [D loss: 0.681087, acc: 58.59%] [G loss: 1.813855]\n",
      "epoch:23 step:21579 [D loss: 0.666193, acc: 60.94%] [G loss: 1.784285]\n",
      "epoch:23 step:21580 [D loss: 0.648202, acc: 62.50%] [G loss: 1.710351]\n",
      "epoch:23 step:21581 [D loss: 0.655335, acc: 57.03%] [G loss: 1.849974]\n",
      "epoch:23 step:21582 [D loss: 0.678227, acc: 53.91%] [G loss: 1.697758]\n",
      "epoch:23 step:21583 [D loss: 0.675802, acc: 62.50%] [G loss: 1.750042]\n",
      "epoch:23 step:21584 [D loss: 0.636209, acc: 65.62%] [G loss: 1.863753]\n",
      "epoch:23 step:21585 [D loss: 0.635590, acc: 68.75%] [G loss: 1.742227]\n",
      "epoch:23 step:21586 [D loss: 0.672749, acc: 54.69%] [G loss: 1.738250]\n",
      "epoch:23 step:21587 [D loss: 0.606366, acc: 72.66%] [G loss: 1.849511]\n",
      "epoch:23 step:21588 [D loss: 0.632351, acc: 65.62%] [G loss: 1.934335]\n",
      "epoch:23 step:21589 [D loss: 0.689989, acc: 55.47%] [G loss: 2.039863]\n",
      "epoch:23 step:21590 [D loss: 0.615547, acc: 69.53%] [G loss: 2.185647]\n",
      "epoch:23 step:21591 [D loss: 0.647159, acc: 63.28%] [G loss: 1.996403]\n",
      "epoch:23 step:21592 [D loss: 0.617089, acc: 65.62%] [G loss: 2.005846]\n",
      "epoch:23 step:21593 [D loss: 0.642798, acc: 59.38%] [G loss: 1.896736]\n",
      "epoch:23 step:21594 [D loss: 0.654994, acc: 64.06%] [G loss: 1.875645]\n",
      "epoch:23 step:21595 [D loss: 0.650754, acc: 64.84%] [G loss: 1.784940]\n",
      "epoch:23 step:21596 [D loss: 0.638924, acc: 62.50%] [G loss: 1.796009]\n",
      "epoch:23 step:21597 [D loss: 0.646717, acc: 61.72%] [G loss: 1.767685]\n",
      "epoch:23 step:21598 [D loss: 0.679037, acc: 57.81%] [G loss: 1.876289]\n",
      "epoch:23 step:21599 [D loss: 0.627679, acc: 70.31%] [G loss: 1.844051]\n",
      "epoch:23 step:21600 [D loss: 0.601209, acc: 69.53%] [G loss: 1.986047]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.443119\n",
      "FID: 11.826957\n",
      "0 = 12.847251269149789\n",
      "1 = 0.09735120815160261\n",
      "2 = 0.8840000033378601\n",
      "3 = 0.896399974822998\n",
      "4 = 0.8715999722480774\n",
      "5 = 0.8747072815895081\n",
      "6 = 0.896399974822998\n",
      "7 = 6.308998757588869\n",
      "8 = 0.06334590669939809\n",
      "9 = 0.7164999842643738\n",
      "10 = 0.7202000021934509\n",
      "11 = 0.7128000259399414\n",
      "12 = 0.7149096727371216\n",
      "13 = 0.7202000021934509\n",
      "14 = 7.443151950836182\n",
      "15 = 9.391801834106445\n",
      "16 = 0.12641960382461548\n",
      "17 = 7.443119049072266\n",
      "18 = 11.826956748962402\n",
      "epoch:23 step:21601 [D loss: 0.647327, acc: 64.84%] [G loss: 1.947845]\n",
      "epoch:23 step:21602 [D loss: 0.662635, acc: 60.94%] [G loss: 1.801357]\n",
      "epoch:23 step:21603 [D loss: 0.648274, acc: 61.72%] [G loss: 1.759965]\n",
      "epoch:23 step:21604 [D loss: 0.741719, acc: 47.66%] [G loss: 1.913895]\n",
      "epoch:23 step:21605 [D loss: 0.633145, acc: 60.94%] [G loss: 1.944414]\n",
      "epoch:23 step:21606 [D loss: 0.605173, acc: 68.75%] [G loss: 1.958680]\n",
      "epoch:23 step:21607 [D loss: 0.650931, acc: 64.84%] [G loss: 1.912439]\n",
      "epoch:23 step:21608 [D loss: 0.675375, acc: 60.16%] [G loss: 1.808228]\n",
      "epoch:23 step:21609 [D loss: 0.626607, acc: 64.84%] [G loss: 1.935405]\n",
      "epoch:23 step:21610 [D loss: 0.641945, acc: 64.84%] [G loss: 1.911039]\n",
      "epoch:23 step:21611 [D loss: 0.638789, acc: 60.94%] [G loss: 1.863717]\n",
      "epoch:23 step:21612 [D loss: 0.614395, acc: 67.97%] [G loss: 1.742159]\n",
      "epoch:23 step:21613 [D loss: 0.639381, acc: 61.72%] [G loss: 1.922940]\n",
      "epoch:23 step:21614 [D loss: 0.659868, acc: 60.16%] [G loss: 1.796620]\n",
      "epoch:23 step:21615 [D loss: 0.623938, acc: 65.62%] [G loss: 1.971598]\n",
      "epoch:23 step:21616 [D loss: 0.600079, acc: 67.19%] [G loss: 1.780184]\n",
      "epoch:23 step:21617 [D loss: 0.621629, acc: 65.62%] [G loss: 1.911836]\n",
      "epoch:23 step:21618 [D loss: 0.621465, acc: 68.75%] [G loss: 2.042356]\n",
      "epoch:23 step:21619 [D loss: 0.646249, acc: 64.06%] [G loss: 1.811177]\n",
      "epoch:23 step:21620 [D loss: 0.618943, acc: 64.84%] [G loss: 1.795074]\n",
      "epoch:23 step:21621 [D loss: 0.640738, acc: 64.84%] [G loss: 1.803420]\n",
      "epoch:23 step:21622 [D loss: 0.630768, acc: 64.84%] [G loss: 1.887128]\n",
      "epoch:23 step:21623 [D loss: 0.644274, acc: 60.16%] [G loss: 1.944820]\n",
      "epoch:23 step:21624 [D loss: 0.598432, acc: 71.88%] [G loss: 1.886857]\n",
      "epoch:23 step:21625 [D loss: 0.668886, acc: 59.38%] [G loss: 1.921784]\n",
      "epoch:23 step:21626 [D loss: 0.635095, acc: 63.28%] [G loss: 2.081163]\n",
      "epoch:23 step:21627 [D loss: 0.633978, acc: 62.50%] [G loss: 1.960350]\n",
      "epoch:23 step:21628 [D loss: 0.534315, acc: 73.44%] [G loss: 2.121598]\n",
      "epoch:23 step:21629 [D loss: 0.738126, acc: 53.12%] [G loss: 1.944184]\n",
      "epoch:23 step:21630 [D loss: 0.668194, acc: 60.94%] [G loss: 1.876972]\n",
      "epoch:23 step:21631 [D loss: 0.583905, acc: 70.31%] [G loss: 1.787496]\n",
      "epoch:23 step:21632 [D loss: 0.686592, acc: 56.25%] [G loss: 1.721209]\n",
      "epoch:23 step:21633 [D loss: 0.663717, acc: 62.50%] [G loss: 1.801219]\n",
      "epoch:23 step:21634 [D loss: 0.672674, acc: 62.50%] [G loss: 1.929726]\n",
      "epoch:23 step:21635 [D loss: 0.618496, acc: 61.72%] [G loss: 1.870654]\n",
      "epoch:23 step:21636 [D loss: 0.652605, acc: 65.62%] [G loss: 1.856163]\n",
      "epoch:23 step:21637 [D loss: 0.692124, acc: 57.81%] [G loss: 1.781662]\n",
      "epoch:23 step:21638 [D loss: 0.635934, acc: 63.28%] [G loss: 1.881487]\n",
      "epoch:23 step:21639 [D loss: 0.617844, acc: 68.75%] [G loss: 1.791185]\n",
      "epoch:23 step:21640 [D loss: 0.671777, acc: 56.25%] [G loss: 1.862164]\n",
      "epoch:23 step:21641 [D loss: 0.611370, acc: 66.41%] [G loss: 1.865904]\n",
      "epoch:23 step:21642 [D loss: 0.617496, acc: 67.97%] [G loss: 1.838349]\n",
      "epoch:23 step:21643 [D loss: 0.607306, acc: 64.84%] [G loss: 1.983983]\n",
      "epoch:23 step:21644 [D loss: 0.607790, acc: 64.06%] [G loss: 2.152468]\n",
      "epoch:23 step:21645 [D loss: 0.654121, acc: 63.28%] [G loss: 1.958157]\n",
      "epoch:23 step:21646 [D loss: 0.618587, acc: 66.41%] [G loss: 1.826576]\n",
      "epoch:23 step:21647 [D loss: 0.624744, acc: 60.16%] [G loss: 1.871097]\n",
      "epoch:23 step:21648 [D loss: 0.654490, acc: 58.59%] [G loss: 1.787768]\n",
      "epoch:23 step:21649 [D loss: 0.633109, acc: 64.84%] [G loss: 1.784354]\n",
      "epoch:23 step:21650 [D loss: 0.627270, acc: 64.84%] [G loss: 1.902574]\n",
      "epoch:23 step:21651 [D loss: 0.658768, acc: 60.94%] [G loss: 1.926543]\n",
      "epoch:23 step:21652 [D loss: 0.615011, acc: 65.62%] [G loss: 1.897546]\n",
      "epoch:23 step:21653 [D loss: 0.721349, acc: 50.78%] [G loss: 1.885279]\n",
      "epoch:23 step:21654 [D loss: 0.644348, acc: 64.06%] [G loss: 1.942501]\n",
      "epoch:23 step:21655 [D loss: 0.636370, acc: 64.06%] [G loss: 1.887847]\n",
      "epoch:23 step:21656 [D loss: 0.649419, acc: 60.16%] [G loss: 1.962853]\n",
      "epoch:23 step:21657 [D loss: 0.651173, acc: 67.19%] [G loss: 1.912951]\n",
      "epoch:23 step:21658 [D loss: 0.632412, acc: 64.06%] [G loss: 2.144364]\n",
      "epoch:23 step:21659 [D loss: 0.771316, acc: 50.00%] [G loss: 1.691971]\n",
      "epoch:23 step:21660 [D loss: 0.674892, acc: 62.50%] [G loss: 1.803844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21661 [D loss: 0.701274, acc: 60.16%] [G loss: 1.695138]\n",
      "epoch:23 step:21662 [D loss: 0.700217, acc: 61.72%] [G loss: 1.919988]\n",
      "epoch:23 step:21663 [D loss: 0.623240, acc: 65.62%] [G loss: 1.924500]\n",
      "epoch:23 step:21664 [D loss: 0.677292, acc: 64.06%] [G loss: 1.830200]\n",
      "epoch:23 step:21665 [D loss: 0.630821, acc: 65.62%] [G loss: 1.958808]\n",
      "epoch:23 step:21666 [D loss: 0.604985, acc: 65.62%] [G loss: 1.969576]\n",
      "epoch:23 step:21667 [D loss: 0.629779, acc: 64.84%] [G loss: 2.133861]\n",
      "epoch:23 step:21668 [D loss: 0.695125, acc: 60.94%] [G loss: 2.026871]\n",
      "epoch:23 step:21669 [D loss: 0.627833, acc: 67.97%] [G loss: 1.865901]\n",
      "epoch:23 step:21670 [D loss: 0.613578, acc: 67.19%] [G loss: 2.009133]\n",
      "epoch:23 step:21671 [D loss: 0.700084, acc: 60.16%] [G loss: 1.831211]\n",
      "epoch:23 step:21672 [D loss: 0.683094, acc: 59.38%] [G loss: 1.747846]\n",
      "epoch:23 step:21673 [D loss: 0.631800, acc: 68.75%] [G loss: 1.952240]\n",
      "epoch:23 step:21674 [D loss: 0.632786, acc: 64.06%] [G loss: 1.936784]\n",
      "epoch:23 step:21675 [D loss: 0.637604, acc: 67.19%] [G loss: 1.946516]\n",
      "epoch:23 step:21676 [D loss: 0.688052, acc: 59.38%] [G loss: 1.717578]\n",
      "epoch:23 step:21677 [D loss: 0.599824, acc: 71.09%] [G loss: 1.982277]\n",
      "epoch:23 step:21678 [D loss: 0.676383, acc: 62.50%] [G loss: 1.844131]\n",
      "epoch:23 step:21679 [D loss: 0.648033, acc: 64.06%] [G loss: 1.816499]\n",
      "epoch:23 step:21680 [D loss: 0.628458, acc: 63.28%] [G loss: 1.917963]\n",
      "epoch:23 step:21681 [D loss: 0.584413, acc: 65.62%] [G loss: 1.820656]\n",
      "epoch:23 step:21682 [D loss: 0.669698, acc: 57.03%] [G loss: 1.844193]\n",
      "epoch:23 step:21683 [D loss: 0.635449, acc: 63.28%] [G loss: 1.934785]\n",
      "epoch:23 step:21684 [D loss: 0.682197, acc: 57.81%] [G loss: 1.841547]\n",
      "epoch:23 step:21685 [D loss: 0.721774, acc: 55.47%] [G loss: 1.730768]\n",
      "epoch:23 step:21686 [D loss: 0.665672, acc: 62.50%] [G loss: 1.854795]\n",
      "epoch:23 step:21687 [D loss: 0.624591, acc: 60.16%] [G loss: 1.751357]\n",
      "epoch:23 step:21688 [D loss: 0.720811, acc: 62.50%] [G loss: 1.695172]\n",
      "epoch:23 step:21689 [D loss: 0.637187, acc: 63.28%] [G loss: 1.863085]\n",
      "epoch:23 step:21690 [D loss: 0.696485, acc: 55.47%] [G loss: 1.665744]\n",
      "epoch:23 step:21691 [D loss: 0.684106, acc: 57.03%] [G loss: 1.761178]\n",
      "epoch:23 step:21692 [D loss: 0.716472, acc: 55.47%] [G loss: 1.684849]\n",
      "epoch:23 step:21693 [D loss: 0.644236, acc: 59.38%] [G loss: 1.697203]\n",
      "epoch:23 step:21694 [D loss: 0.669384, acc: 57.03%] [G loss: 1.705132]\n",
      "epoch:23 step:21695 [D loss: 0.653009, acc: 62.50%] [G loss: 1.780275]\n",
      "epoch:23 step:21696 [D loss: 0.670543, acc: 62.50%] [G loss: 1.767978]\n",
      "epoch:23 step:21697 [D loss: 0.633827, acc: 60.16%] [G loss: 1.847701]\n",
      "epoch:23 step:21698 [D loss: 0.665600, acc: 57.81%] [G loss: 1.715409]\n",
      "epoch:23 step:21699 [D loss: 0.649083, acc: 61.72%] [G loss: 1.698320]\n",
      "epoch:23 step:21700 [D loss: 0.613709, acc: 65.62%] [G loss: 1.854585]\n",
      "epoch:23 step:21701 [D loss: 0.641040, acc: 63.28%] [G loss: 1.764095]\n",
      "epoch:23 step:21702 [D loss: 0.625372, acc: 64.84%] [G loss: 1.858360]\n",
      "epoch:23 step:21703 [D loss: 0.674864, acc: 56.25%] [G loss: 1.856405]\n",
      "epoch:23 step:21704 [D loss: 0.697483, acc: 55.47%] [G loss: 1.851639]\n",
      "epoch:23 step:21705 [D loss: 0.639013, acc: 62.50%] [G loss: 1.926705]\n",
      "epoch:23 step:21706 [D loss: 0.604691, acc: 72.66%] [G loss: 1.795712]\n",
      "epoch:23 step:21707 [D loss: 0.679753, acc: 57.03%] [G loss: 1.858368]\n",
      "epoch:23 step:21708 [D loss: 0.659302, acc: 57.81%] [G loss: 1.742618]\n",
      "epoch:23 step:21709 [D loss: 0.713016, acc: 53.12%] [G loss: 1.799726]\n",
      "epoch:23 step:21710 [D loss: 0.647629, acc: 62.50%] [G loss: 1.889206]\n",
      "epoch:23 step:21711 [D loss: 0.697900, acc: 52.34%] [G loss: 1.768855]\n",
      "epoch:23 step:21712 [D loss: 0.642074, acc: 60.16%] [G loss: 1.865204]\n",
      "epoch:23 step:21713 [D loss: 0.618536, acc: 63.28%] [G loss: 1.815982]\n",
      "epoch:23 step:21714 [D loss: 0.627027, acc: 67.19%] [G loss: 1.761491]\n",
      "epoch:23 step:21715 [D loss: 0.612847, acc: 66.41%] [G loss: 1.861226]\n",
      "epoch:23 step:21716 [D loss: 0.637441, acc: 66.41%] [G loss: 1.684809]\n",
      "epoch:23 step:21717 [D loss: 0.614076, acc: 64.84%] [G loss: 1.797329]\n",
      "epoch:23 step:21718 [D loss: 0.619604, acc: 67.97%] [G loss: 1.854197]\n",
      "epoch:23 step:21719 [D loss: 0.658427, acc: 61.72%] [G loss: 1.815416]\n",
      "epoch:23 step:21720 [D loss: 0.647297, acc: 59.38%] [G loss: 2.103622]\n",
      "epoch:23 step:21721 [D loss: 0.670971, acc: 57.03%] [G loss: 1.827865]\n",
      "epoch:23 step:21722 [D loss: 0.601189, acc: 67.97%] [G loss: 1.810251]\n",
      "epoch:23 step:21723 [D loss: 0.648395, acc: 64.06%] [G loss: 1.809908]\n",
      "epoch:23 step:21724 [D loss: 0.658558, acc: 60.94%] [G loss: 1.761603]\n",
      "epoch:23 step:21725 [D loss: 0.687458, acc: 57.03%] [G loss: 1.717731]\n",
      "epoch:23 step:21726 [D loss: 0.685507, acc: 53.91%] [G loss: 1.794913]\n",
      "epoch:23 step:21727 [D loss: 0.637259, acc: 60.16%] [G loss: 1.769098]\n",
      "epoch:23 step:21728 [D loss: 0.644307, acc: 67.97%] [G loss: 1.800925]\n",
      "epoch:23 step:21729 [D loss: 0.637734, acc: 66.41%] [G loss: 1.799801]\n",
      "epoch:23 step:21730 [D loss: 0.657806, acc: 60.16%] [G loss: 1.857586]\n",
      "epoch:23 step:21731 [D loss: 0.686124, acc: 59.38%] [G loss: 1.934706]\n",
      "epoch:23 step:21732 [D loss: 0.641938, acc: 64.06%] [G loss: 1.801980]\n",
      "epoch:23 step:21733 [D loss: 0.694271, acc: 54.69%] [G loss: 1.805973]\n",
      "epoch:23 step:21734 [D loss: 0.667227, acc: 59.38%] [G loss: 1.751347]\n",
      "epoch:23 step:21735 [D loss: 0.658886, acc: 59.38%] [G loss: 1.735591]\n",
      "epoch:23 step:21736 [D loss: 0.656983, acc: 61.72%] [G loss: 1.798503]\n",
      "epoch:23 step:21737 [D loss: 0.600682, acc: 67.97%] [G loss: 1.853841]\n",
      "epoch:23 step:21738 [D loss: 0.654726, acc: 64.06%] [G loss: 1.886369]\n",
      "epoch:23 step:21739 [D loss: 0.687083, acc: 60.16%] [G loss: 1.864010]\n",
      "epoch:23 step:21740 [D loss: 0.643754, acc: 65.62%] [G loss: 1.929523]\n",
      "epoch:23 step:21741 [D loss: 0.631098, acc: 65.62%] [G loss: 1.948017]\n",
      "epoch:23 step:21742 [D loss: 0.683410, acc: 60.16%] [G loss: 2.023403]\n",
      "epoch:23 step:21743 [D loss: 0.625787, acc: 65.62%] [G loss: 1.920465]\n",
      "epoch:23 step:21744 [D loss: 0.685735, acc: 60.16%] [G loss: 1.760890]\n",
      "epoch:23 step:21745 [D loss: 0.636109, acc: 66.41%] [G loss: 2.006072]\n",
      "epoch:23 step:21746 [D loss: 0.680227, acc: 55.47%] [G loss: 1.917668]\n",
      "epoch:23 step:21747 [D loss: 0.690670, acc: 55.47%] [G loss: 1.832003]\n",
      "epoch:23 step:21748 [D loss: 0.727828, acc: 53.91%] [G loss: 1.781583]\n",
      "epoch:23 step:21749 [D loss: 0.666748, acc: 58.59%] [G loss: 1.867565]\n",
      "epoch:23 step:21750 [D loss: 0.651088, acc: 57.03%] [G loss: 1.756727]\n",
      "epoch:23 step:21751 [D loss: 0.686580, acc: 60.94%] [G loss: 1.758057]\n",
      "epoch:23 step:21752 [D loss: 0.664978, acc: 57.81%] [G loss: 1.845592]\n",
      "epoch:23 step:21753 [D loss: 0.654401, acc: 62.50%] [G loss: 1.812516]\n",
      "epoch:23 step:21754 [D loss: 0.693597, acc: 53.12%] [G loss: 1.808053]\n",
      "epoch:23 step:21755 [D loss: 0.656085, acc: 59.38%] [G loss: 1.868982]\n",
      "epoch:23 step:21756 [D loss: 0.662557, acc: 53.91%] [G loss: 1.831256]\n",
      "epoch:23 step:21757 [D loss: 0.634636, acc: 64.06%] [G loss: 1.911884]\n",
      "epoch:23 step:21758 [D loss: 0.629115, acc: 64.84%] [G loss: 1.996355]\n",
      "epoch:23 step:21759 [D loss: 0.629922, acc: 63.28%] [G loss: 2.086909]\n",
      "epoch:23 step:21760 [D loss: 0.619048, acc: 65.62%] [G loss: 2.032424]\n",
      "epoch:23 step:21761 [D loss: 0.665254, acc: 62.50%] [G loss: 1.706428]\n",
      "epoch:23 step:21762 [D loss: 0.691673, acc: 57.03%] [G loss: 1.695481]\n",
      "epoch:23 step:21763 [D loss: 0.654622, acc: 61.72%] [G loss: 1.727721]\n",
      "epoch:23 step:21764 [D loss: 0.641535, acc: 61.72%] [G loss: 1.752481]\n",
      "epoch:23 step:21765 [D loss: 0.654254, acc: 59.38%] [G loss: 1.744040]\n",
      "epoch:23 step:21766 [D loss: 0.711759, acc: 51.56%] [G loss: 1.729902]\n",
      "epoch:23 step:21767 [D loss: 0.667659, acc: 60.94%] [G loss: 1.940017]\n",
      "epoch:23 step:21768 [D loss: 0.621244, acc: 66.41%] [G loss: 1.852206]\n",
      "epoch:23 step:21769 [D loss: 0.647734, acc: 59.38%] [G loss: 1.981936]\n",
      "epoch:23 step:21770 [D loss: 0.657229, acc: 61.72%] [G loss: 2.054470]\n",
      "epoch:23 step:21771 [D loss: 0.730463, acc: 50.00%] [G loss: 1.659861]\n",
      "epoch:23 step:21772 [D loss: 0.668250, acc: 60.16%] [G loss: 1.969195]\n",
      "epoch:23 step:21773 [D loss: 0.668911, acc: 60.94%] [G loss: 1.803514]\n",
      "epoch:23 step:21774 [D loss: 0.675452, acc: 62.50%] [G loss: 1.802041]\n",
      "epoch:23 step:21775 [D loss: 0.653857, acc: 58.59%] [G loss: 1.779160]\n",
      "epoch:23 step:21776 [D loss: 0.645058, acc: 60.94%] [G loss: 1.776029]\n",
      "epoch:23 step:21777 [D loss: 0.671773, acc: 56.25%] [G loss: 1.776487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21778 [D loss: 0.639654, acc: 64.84%] [G loss: 1.876063]\n",
      "epoch:23 step:21779 [D loss: 0.671830, acc: 54.69%] [G loss: 1.738815]\n",
      "epoch:23 step:21780 [D loss: 0.560263, acc: 74.22%] [G loss: 2.062818]\n",
      "epoch:23 step:21781 [D loss: 0.585317, acc: 67.97%] [G loss: 2.265408]\n",
      "epoch:23 step:21782 [D loss: 0.558423, acc: 74.22%] [G loss: 2.228166]\n",
      "epoch:23 step:21783 [D loss: 0.551353, acc: 75.78%] [G loss: 2.357841]\n",
      "epoch:23 step:21784 [D loss: 0.720568, acc: 55.47%] [G loss: 1.849195]\n",
      "epoch:23 step:21785 [D loss: 0.728572, acc: 57.03%] [G loss: 1.875172]\n",
      "epoch:23 step:21786 [D loss: 0.702081, acc: 57.03%] [G loss: 1.816431]\n",
      "epoch:23 step:21787 [D loss: 0.624045, acc: 62.50%] [G loss: 1.866467]\n",
      "epoch:23 step:21788 [D loss: 0.676935, acc: 59.38%] [G loss: 1.830914]\n",
      "epoch:23 step:21789 [D loss: 0.657444, acc: 67.19%] [G loss: 1.894870]\n",
      "epoch:23 step:21790 [D loss: 0.649330, acc: 64.84%] [G loss: 1.859535]\n",
      "epoch:23 step:21791 [D loss: 0.630231, acc: 63.28%] [G loss: 1.822715]\n",
      "epoch:23 step:21792 [D loss: 0.628397, acc: 63.28%] [G loss: 1.876278]\n",
      "epoch:23 step:21793 [D loss: 0.634952, acc: 64.06%] [G loss: 1.892294]\n",
      "epoch:23 step:21794 [D loss: 0.612263, acc: 66.41%] [G loss: 1.892342]\n",
      "epoch:23 step:21795 [D loss: 0.652516, acc: 62.50%] [G loss: 1.762192]\n",
      "epoch:23 step:21796 [D loss: 0.665768, acc: 59.38%] [G loss: 1.898373]\n",
      "epoch:23 step:21797 [D loss: 0.606126, acc: 67.97%] [G loss: 1.771441]\n",
      "epoch:23 step:21798 [D loss: 0.683428, acc: 55.47%] [G loss: 1.828931]\n",
      "epoch:23 step:21799 [D loss: 0.668615, acc: 57.81%] [G loss: 1.872145]\n",
      "epoch:23 step:21800 [D loss: 0.637313, acc: 61.72%] [G loss: 1.825233]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.239207\n",
      "FID: 13.835509\n",
      "0 = 12.769728579998002\n",
      "1 = 0.08422886945124958\n",
      "2 = 0.8833000063896179\n",
      "3 = 0.9017999768257141\n",
      "4 = 0.864799976348877\n",
      "5 = 0.8696238994598389\n",
      "6 = 0.9017999768257141\n",
      "7 = 6.436672096276282\n",
      "8 = 0.0721269255964518\n",
      "9 = 0.7260000109672546\n",
      "10 = 0.7296000123023987\n",
      "11 = 0.7224000096321106\n",
      "12 = 0.7243844270706177\n",
      "13 = 0.7296000123023987\n",
      "14 = 7.239234447479248\n",
      "15 = 9.452118873596191\n",
      "16 = 0.10908738523721695\n",
      "17 = 7.239206790924072\n",
      "18 = 13.835509300231934\n",
      "epoch:23 step:21801 [D loss: 0.693348, acc: 57.81%] [G loss: 1.729580]\n",
      "epoch:23 step:21802 [D loss: 0.669611, acc: 57.81%] [G loss: 1.735548]\n",
      "epoch:23 step:21803 [D loss: 0.682216, acc: 58.59%] [G loss: 1.804734]\n",
      "epoch:23 step:21804 [D loss: 0.629169, acc: 66.41%] [G loss: 1.750252]\n",
      "epoch:23 step:21805 [D loss: 0.653304, acc: 65.62%] [G loss: 1.725167]\n",
      "epoch:23 step:21806 [D loss: 0.671990, acc: 57.81%] [G loss: 1.798105]\n",
      "epoch:23 step:21807 [D loss: 0.620985, acc: 64.06%] [G loss: 1.770821]\n",
      "epoch:23 step:21808 [D loss: 0.688927, acc: 52.34%] [G loss: 1.728908]\n",
      "epoch:23 step:21809 [D loss: 0.648987, acc: 64.84%] [G loss: 1.846181]\n",
      "epoch:23 step:21810 [D loss: 0.651236, acc: 65.62%] [G loss: 1.852247]\n",
      "epoch:23 step:21811 [D loss: 0.655993, acc: 62.50%] [G loss: 1.759464]\n",
      "epoch:23 step:21812 [D loss: 0.593263, acc: 63.28%] [G loss: 2.014157]\n",
      "epoch:23 step:21813 [D loss: 0.638606, acc: 62.50%] [G loss: 1.945099]\n",
      "epoch:23 step:21814 [D loss: 0.700306, acc: 57.03%] [G loss: 1.736212]\n",
      "epoch:23 step:21815 [D loss: 0.627330, acc: 66.41%] [G loss: 1.989882]\n",
      "epoch:23 step:21816 [D loss: 0.706398, acc: 57.81%] [G loss: 1.845158]\n",
      "epoch:23 step:21817 [D loss: 0.681146, acc: 60.94%] [G loss: 1.786628]\n",
      "epoch:23 step:21818 [D loss: 0.678917, acc: 57.81%] [G loss: 1.702147]\n",
      "epoch:23 step:21819 [D loss: 0.674592, acc: 59.38%] [G loss: 1.764597]\n",
      "epoch:23 step:21820 [D loss: 0.642962, acc: 58.59%] [G loss: 1.965867]\n",
      "epoch:23 step:21821 [D loss: 0.645624, acc: 67.97%] [G loss: 1.787849]\n",
      "epoch:23 step:21822 [D loss: 0.678259, acc: 57.03%] [G loss: 1.851836]\n",
      "epoch:23 step:21823 [D loss: 0.638107, acc: 61.72%] [G loss: 1.763264]\n",
      "epoch:23 step:21824 [D loss: 0.683311, acc: 53.91%] [G loss: 1.708331]\n",
      "epoch:23 step:21825 [D loss: 0.620201, acc: 60.94%] [G loss: 1.882129]\n",
      "epoch:23 step:21826 [D loss: 0.642510, acc: 67.19%] [G loss: 1.889917]\n",
      "epoch:23 step:21827 [D loss: 0.590601, acc: 70.31%] [G loss: 1.922729]\n",
      "epoch:23 step:21828 [D loss: 0.658641, acc: 56.25%] [G loss: 1.882631]\n",
      "epoch:23 step:21829 [D loss: 0.647731, acc: 63.28%] [G loss: 1.885808]\n",
      "epoch:23 step:21830 [D loss: 0.645769, acc: 57.81%] [G loss: 1.893149]\n",
      "epoch:23 step:21831 [D loss: 0.634925, acc: 61.72%] [G loss: 1.976397]\n",
      "epoch:23 step:21832 [D loss: 0.723248, acc: 53.12%] [G loss: 1.822226]\n",
      "epoch:23 step:21833 [D loss: 0.603861, acc: 67.97%] [G loss: 1.791739]\n",
      "epoch:23 step:21834 [D loss: 0.627203, acc: 69.53%] [G loss: 1.867322]\n",
      "epoch:23 step:21835 [D loss: 0.647394, acc: 63.28%] [G loss: 1.866432]\n",
      "epoch:23 step:21836 [D loss: 0.690088, acc: 56.25%] [G loss: 1.886791]\n",
      "epoch:23 step:21837 [D loss: 0.652563, acc: 67.19%] [G loss: 1.887800]\n",
      "epoch:23 step:21838 [D loss: 0.671397, acc: 62.50%] [G loss: 1.816789]\n",
      "epoch:23 step:21839 [D loss: 0.681095, acc: 60.94%] [G loss: 1.738466]\n",
      "epoch:23 step:21840 [D loss: 0.667776, acc: 62.50%] [G loss: 1.766936]\n",
      "epoch:23 step:21841 [D loss: 0.681959, acc: 60.16%] [G loss: 1.830297]\n",
      "epoch:23 step:21842 [D loss: 0.656621, acc: 60.94%] [G loss: 1.816689]\n",
      "epoch:23 step:21843 [D loss: 0.608823, acc: 71.09%] [G loss: 1.791898]\n",
      "epoch:23 step:21844 [D loss: 0.664263, acc: 63.28%] [G loss: 1.834695]\n",
      "epoch:23 step:21845 [D loss: 0.611541, acc: 68.75%] [G loss: 1.815141]\n",
      "epoch:23 step:21846 [D loss: 0.661526, acc: 60.94%] [G loss: 1.966344]\n",
      "epoch:23 step:21847 [D loss: 0.662174, acc: 63.28%] [G loss: 1.855143]\n",
      "epoch:23 step:21848 [D loss: 0.615096, acc: 70.31%] [G loss: 1.787284]\n",
      "epoch:23 step:21849 [D loss: 0.667640, acc: 61.72%] [G loss: 1.855861]\n",
      "epoch:23 step:21850 [D loss: 0.626491, acc: 71.88%] [G loss: 1.984959]\n",
      "epoch:23 step:21851 [D loss: 0.615775, acc: 63.28%] [G loss: 1.843571]\n",
      "epoch:23 step:21852 [D loss: 0.684481, acc: 56.25%] [G loss: 1.722517]\n",
      "epoch:23 step:21853 [D loss: 0.625112, acc: 61.72%] [G loss: 1.841378]\n",
      "epoch:23 step:21854 [D loss: 0.668902, acc: 59.38%] [G loss: 1.736799]\n",
      "epoch:23 step:21855 [D loss: 0.660754, acc: 60.16%] [G loss: 1.775873]\n",
      "epoch:23 step:21856 [D loss: 0.647546, acc: 61.72%] [G loss: 1.935785]\n",
      "epoch:23 step:21857 [D loss: 0.636978, acc: 65.62%] [G loss: 1.885019]\n",
      "epoch:23 step:21858 [D loss: 0.598607, acc: 70.31%] [G loss: 1.810174]\n",
      "epoch:23 step:21859 [D loss: 0.693065, acc: 59.38%] [G loss: 1.790843]\n",
      "epoch:23 step:21860 [D loss: 0.633967, acc: 64.84%] [G loss: 1.922954]\n",
      "epoch:23 step:21861 [D loss: 0.635897, acc: 62.50%] [G loss: 1.744773]\n",
      "epoch:23 step:21862 [D loss: 0.648087, acc: 62.50%] [G loss: 1.731327]\n",
      "epoch:23 step:21863 [D loss: 0.623126, acc: 65.62%] [G loss: 2.113560]\n",
      "epoch:23 step:21864 [D loss: 0.603956, acc: 68.75%] [G loss: 1.991523]\n",
      "epoch:23 step:21865 [D loss: 0.605376, acc: 65.62%] [G loss: 2.260770]\n",
      "epoch:23 step:21866 [D loss: 0.553812, acc: 73.44%] [G loss: 2.088373]\n",
      "epoch:23 step:21867 [D loss: 0.705970, acc: 57.03%] [G loss: 1.748047]\n",
      "epoch:23 step:21868 [D loss: 0.679707, acc: 58.59%] [G loss: 1.781092]\n",
      "epoch:23 step:21869 [D loss: 0.672423, acc: 61.72%] [G loss: 1.908113]\n",
      "epoch:23 step:21870 [D loss: 0.655667, acc: 54.69%] [G loss: 1.842097]\n",
      "epoch:23 step:21871 [D loss: 0.691017, acc: 55.47%] [G loss: 1.833759]\n",
      "epoch:23 step:21872 [D loss: 0.678238, acc: 55.47%] [G loss: 1.865010]\n",
      "epoch:23 step:21873 [D loss: 0.624437, acc: 66.41%] [G loss: 1.625443]\n",
      "epoch:23 step:21874 [D loss: 0.670794, acc: 62.50%] [G loss: 1.810833]\n",
      "epoch:23 step:21875 [D loss: 0.634542, acc: 61.72%] [G loss: 1.753955]\n",
      "epoch:23 step:21876 [D loss: 0.639379, acc: 61.72%] [G loss: 1.865965]\n",
      "epoch:23 step:21877 [D loss: 0.654351, acc: 59.38%] [G loss: 1.866878]\n",
      "epoch:23 step:21878 [D loss: 0.669339, acc: 59.38%] [G loss: 1.759809]\n",
      "epoch:23 step:21879 [D loss: 0.654124, acc: 64.06%] [G loss: 1.805356]\n",
      "epoch:23 step:21880 [D loss: 0.632744, acc: 67.19%] [G loss: 1.783883]\n",
      "epoch:23 step:21881 [D loss: 0.621417, acc: 64.06%] [G loss: 1.902023]\n",
      "epoch:23 step:21882 [D loss: 0.621466, acc: 66.41%] [G loss: 1.884183]\n",
      "epoch:23 step:21883 [D loss: 0.636523, acc: 62.50%] [G loss: 1.899832]\n",
      "epoch:23 step:21884 [D loss: 0.654535, acc: 59.38%] [G loss: 1.843290]\n",
      "epoch:23 step:21885 [D loss: 0.626382, acc: 65.62%] [G loss: 1.813732]\n",
      "epoch:23 step:21886 [D loss: 0.643651, acc: 61.72%] [G loss: 1.977899]\n",
      "epoch:23 step:21887 [D loss: 0.613218, acc: 67.97%] [G loss: 1.950590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21888 [D loss: 0.629120, acc: 67.19%] [G loss: 2.044887]\n",
      "epoch:23 step:21889 [D loss: 0.623710, acc: 61.72%] [G loss: 2.013825]\n",
      "epoch:23 step:21890 [D loss: 0.609322, acc: 68.75%] [G loss: 1.981781]\n",
      "epoch:23 step:21891 [D loss: 0.636522, acc: 62.50%] [G loss: 1.908663]\n",
      "epoch:23 step:21892 [D loss: 0.670010, acc: 60.94%] [G loss: 1.872076]\n",
      "epoch:23 step:21893 [D loss: 0.650614, acc: 60.16%] [G loss: 1.806995]\n",
      "epoch:23 step:21894 [D loss: 0.631217, acc: 65.62%] [G loss: 1.825002]\n",
      "epoch:23 step:21895 [D loss: 0.631868, acc: 67.19%] [G loss: 2.005438]\n",
      "epoch:23 step:21896 [D loss: 0.564132, acc: 78.12%] [G loss: 2.199433]\n",
      "epoch:23 step:21897 [D loss: 0.620014, acc: 66.41%] [G loss: 2.206532]\n",
      "epoch:23 step:21898 [D loss: 0.569604, acc: 70.31%] [G loss: 2.193699]\n",
      "epoch:23 step:21899 [D loss: 0.696318, acc: 50.78%] [G loss: 1.909604]\n",
      "epoch:23 step:21900 [D loss: 0.645842, acc: 66.41%] [G loss: 1.744496]\n",
      "epoch:23 step:21901 [D loss: 0.652694, acc: 64.06%] [G loss: 1.844099]\n",
      "epoch:23 step:21902 [D loss: 0.670450, acc: 53.91%] [G loss: 1.762927]\n",
      "epoch:23 step:21903 [D loss: 0.645530, acc: 61.72%] [G loss: 1.711350]\n",
      "epoch:23 step:21904 [D loss: 0.670651, acc: 58.59%] [G loss: 1.696883]\n",
      "epoch:23 step:21905 [D loss: 0.623827, acc: 64.84%] [G loss: 1.993224]\n",
      "epoch:23 step:21906 [D loss: 0.663329, acc: 57.03%] [G loss: 1.886962]\n",
      "epoch:23 step:21907 [D loss: 0.659293, acc: 59.38%] [G loss: 1.813382]\n",
      "epoch:23 step:21908 [D loss: 0.654666, acc: 66.41%] [G loss: 1.790690]\n",
      "epoch:23 step:21909 [D loss: 0.585594, acc: 71.09%] [G loss: 1.848938]\n",
      "epoch:23 step:21910 [D loss: 0.637825, acc: 62.50%] [G loss: 1.915229]\n",
      "epoch:23 step:21911 [D loss: 0.623634, acc: 60.94%] [G loss: 1.896478]\n",
      "epoch:23 step:21912 [D loss: 0.684116, acc: 56.25%] [G loss: 1.867259]\n",
      "epoch:23 step:21913 [D loss: 0.685818, acc: 54.69%] [G loss: 1.876209]\n",
      "epoch:23 step:21914 [D loss: 0.643337, acc: 63.28%] [G loss: 1.851529]\n",
      "epoch:23 step:21915 [D loss: 0.669369, acc: 56.25%] [G loss: 1.841997]\n",
      "epoch:23 step:21916 [D loss: 0.629703, acc: 65.62%] [G loss: 1.914953]\n",
      "epoch:23 step:21917 [D loss: 0.676338, acc: 60.94%] [G loss: 1.847889]\n",
      "epoch:23 step:21918 [D loss: 0.599726, acc: 69.53%] [G loss: 1.884716]\n",
      "epoch:23 step:21919 [D loss: 0.662896, acc: 59.38%] [G loss: 1.819053]\n",
      "epoch:23 step:21920 [D loss: 0.651922, acc: 64.84%] [G loss: 1.997769]\n",
      "epoch:23 step:21921 [D loss: 0.632422, acc: 66.41%] [G loss: 2.018358]\n",
      "epoch:23 step:21922 [D loss: 0.626465, acc: 67.19%] [G loss: 2.001818]\n",
      "epoch:23 step:21923 [D loss: 0.641945, acc: 64.06%] [G loss: 1.913193]\n",
      "epoch:23 step:21924 [D loss: 0.685181, acc: 56.25%] [G loss: 1.857075]\n",
      "epoch:23 step:21925 [D loss: 0.682062, acc: 53.91%] [G loss: 1.925720]\n",
      "epoch:23 step:21926 [D loss: 0.651320, acc: 63.28%] [G loss: 1.773813]\n",
      "epoch:23 step:21927 [D loss: 0.695524, acc: 59.38%] [G loss: 1.837088]\n",
      "epoch:23 step:21928 [D loss: 0.720642, acc: 48.44%] [G loss: 1.645424]\n",
      "epoch:23 step:21929 [D loss: 0.660053, acc: 63.28%] [G loss: 1.716188]\n",
      "epoch:23 step:21930 [D loss: 0.620360, acc: 65.62%] [G loss: 1.964765]\n",
      "epoch:23 step:21931 [D loss: 0.638193, acc: 58.59%] [G loss: 1.871534]\n",
      "epoch:23 step:21932 [D loss: 0.602173, acc: 73.44%] [G loss: 1.783163]\n",
      "epoch:23 step:21933 [D loss: 0.668008, acc: 60.16%] [G loss: 1.827960]\n",
      "epoch:23 step:21934 [D loss: 0.657582, acc: 60.16%] [G loss: 1.812335]\n",
      "epoch:23 step:21935 [D loss: 0.640196, acc: 59.38%] [G loss: 1.885869]\n",
      "epoch:23 step:21936 [D loss: 0.667187, acc: 60.94%] [G loss: 1.877818]\n",
      "epoch:23 step:21937 [D loss: 0.643555, acc: 64.84%] [G loss: 1.796162]\n",
      "epoch:23 step:21938 [D loss: 0.641801, acc: 64.84%] [G loss: 1.850492]\n",
      "epoch:23 step:21939 [D loss: 0.637762, acc: 62.50%] [G loss: 1.834189]\n",
      "epoch:23 step:21940 [D loss: 0.641808, acc: 60.16%] [G loss: 1.863336]\n",
      "epoch:23 step:21941 [D loss: 0.671162, acc: 60.94%] [G loss: 1.836257]\n",
      "epoch:23 step:21942 [D loss: 0.655006, acc: 60.94%] [G loss: 1.758439]\n",
      "epoch:23 step:21943 [D loss: 0.616509, acc: 64.06%] [G loss: 1.809369]\n",
      "epoch:23 step:21944 [D loss: 0.618700, acc: 64.84%] [G loss: 1.791805]\n",
      "epoch:23 step:21945 [D loss: 0.643249, acc: 60.94%] [G loss: 1.802217]\n",
      "epoch:23 step:21946 [D loss: 0.643377, acc: 63.28%] [G loss: 1.967555]\n",
      "epoch:23 step:21947 [D loss: 0.680252, acc: 57.03%] [G loss: 1.674847]\n",
      "epoch:23 step:21948 [D loss: 0.655099, acc: 62.50%] [G loss: 1.806261]\n",
      "epoch:23 step:21949 [D loss: 0.659567, acc: 59.38%] [G loss: 1.828739]\n",
      "epoch:23 step:21950 [D loss: 0.622370, acc: 71.09%] [G loss: 1.930499]\n",
      "epoch:23 step:21951 [D loss: 0.625962, acc: 60.16%] [G loss: 1.768908]\n",
      "epoch:23 step:21952 [D loss: 0.630943, acc: 65.62%] [G loss: 1.829822]\n",
      "epoch:23 step:21953 [D loss: 0.600812, acc: 67.97%] [G loss: 1.841253]\n",
      "epoch:23 step:21954 [D loss: 0.658750, acc: 62.50%] [G loss: 1.890480]\n",
      "epoch:23 step:21955 [D loss: 0.657960, acc: 64.06%] [G loss: 1.891626]\n",
      "epoch:23 step:21956 [D loss: 0.649604, acc: 64.06%] [G loss: 2.016101]\n",
      "epoch:23 step:21957 [D loss: 0.626143, acc: 71.09%] [G loss: 1.912913]\n",
      "epoch:23 step:21958 [D loss: 0.644390, acc: 63.28%] [G loss: 1.827538]\n",
      "epoch:23 step:21959 [D loss: 0.639594, acc: 66.41%] [G loss: 1.863775]\n",
      "epoch:23 step:21960 [D loss: 0.673077, acc: 59.38%] [G loss: 1.839402]\n",
      "epoch:23 step:21961 [D loss: 0.632296, acc: 70.31%] [G loss: 1.866511]\n",
      "epoch:23 step:21962 [D loss: 0.680765, acc: 56.25%] [G loss: 1.786586]\n",
      "epoch:23 step:21963 [D loss: 0.635245, acc: 63.28%] [G loss: 1.982992]\n",
      "epoch:23 step:21964 [D loss: 0.662855, acc: 55.47%] [G loss: 1.933393]\n",
      "epoch:23 step:21965 [D loss: 0.624110, acc: 68.75%] [G loss: 2.064298]\n",
      "epoch:23 step:21966 [D loss: 0.644714, acc: 62.50%] [G loss: 2.061603]\n",
      "epoch:23 step:21967 [D loss: 0.633182, acc: 60.94%] [G loss: 2.164357]\n",
      "epoch:23 step:21968 [D loss: 0.599387, acc: 68.75%] [G loss: 2.101129]\n",
      "epoch:23 step:21969 [D loss: 0.643754, acc: 64.06%] [G loss: 1.904770]\n",
      "epoch:23 step:21970 [D loss: 0.653314, acc: 60.94%] [G loss: 1.941809]\n",
      "epoch:23 step:21971 [D loss: 0.661359, acc: 63.28%] [G loss: 1.758709]\n",
      "epoch:23 step:21972 [D loss: 0.684930, acc: 55.47%] [G loss: 1.834536]\n",
      "epoch:23 step:21973 [D loss: 0.650666, acc: 64.06%] [G loss: 1.878921]\n",
      "epoch:23 step:21974 [D loss: 0.630950, acc: 60.94%] [G loss: 1.877510]\n",
      "epoch:23 step:21975 [D loss: 0.667494, acc: 63.28%] [G loss: 1.876775]\n",
      "epoch:23 step:21976 [D loss: 0.670448, acc: 62.50%] [G loss: 1.837920]\n",
      "epoch:23 step:21977 [D loss: 0.622751, acc: 71.09%] [G loss: 1.841659]\n",
      "epoch:23 step:21978 [D loss: 0.620501, acc: 63.28%] [G loss: 1.906713]\n",
      "epoch:23 step:21979 [D loss: 0.577156, acc: 75.78%] [G loss: 2.107534]\n",
      "epoch:23 step:21980 [D loss: 0.571637, acc: 70.31%] [G loss: 2.018740]\n",
      "epoch:23 step:21981 [D loss: 0.535785, acc: 76.56%] [G loss: 2.209641]\n",
      "epoch:23 step:21982 [D loss: 0.644340, acc: 60.94%] [G loss: 2.016503]\n",
      "epoch:23 step:21983 [D loss: 0.702357, acc: 55.47%] [G loss: 1.896640]\n",
      "epoch:23 step:21984 [D loss: 0.648242, acc: 60.16%] [G loss: 1.926117]\n",
      "epoch:23 step:21985 [D loss: 0.620806, acc: 67.19%] [G loss: 1.983595]\n",
      "epoch:23 step:21986 [D loss: 0.616507, acc: 69.53%] [G loss: 2.021300]\n",
      "epoch:23 step:21987 [D loss: 0.609001, acc: 70.31%] [G loss: 1.916452]\n",
      "epoch:23 step:21988 [D loss: 0.708608, acc: 53.91%] [G loss: 1.721529]\n",
      "epoch:23 step:21989 [D loss: 0.646052, acc: 67.19%] [G loss: 1.817387]\n",
      "epoch:23 step:21990 [D loss: 0.738486, acc: 51.56%] [G loss: 1.697941]\n",
      "epoch:23 step:21991 [D loss: 0.682292, acc: 58.59%] [G loss: 1.808472]\n",
      "epoch:23 step:21992 [D loss: 0.693678, acc: 55.47%] [G loss: 1.704417]\n",
      "epoch:23 step:21993 [D loss: 0.621409, acc: 64.06%] [G loss: 1.823183]\n",
      "epoch:23 step:21994 [D loss: 0.652778, acc: 67.97%] [G loss: 1.734739]\n",
      "epoch:23 step:21995 [D loss: 0.689907, acc: 57.03%] [G loss: 1.717628]\n",
      "epoch:23 step:21996 [D loss: 0.651647, acc: 64.06%] [G loss: 1.798171]\n",
      "epoch:23 step:21997 [D loss: 0.650538, acc: 57.81%] [G loss: 1.850490]\n",
      "epoch:23 step:21998 [D loss: 0.667043, acc: 61.72%] [G loss: 1.857025]\n",
      "epoch:23 step:21999 [D loss: 0.662415, acc: 55.47%] [G loss: 1.762576]\n",
      "epoch:23 step:22000 [D loss: 0.640167, acc: 64.06%] [G loss: 1.914974]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.496350\n",
      "FID: 10.853010\n",
      "0 = 12.692818306875239\n",
      "1 = 0.08679740019741532\n",
      "2 = 0.8651000261306763\n",
      "3 = 0.88919997215271\n",
      "4 = 0.8410000205039978\n",
      "5 = 0.8483113646507263\n",
      "6 = 0.88919997215271\n",
      "7 = 6.196866501069068\n",
      "8 = 0.06042961332665535\n",
      "9 = 0.7055000066757202\n",
      "10 = 0.7229999899864197\n",
      "11 = 0.6880000233650208\n",
      "12 = 0.6985507011413574\n",
      "13 = 0.7229999899864197\n",
      "14 = 7.496382236480713\n",
      "15 = 9.423169136047363\n",
      "16 = 0.12127195298671722\n",
      "17 = 7.496349811553955\n",
      "18 = 10.853010177612305\n",
      "epoch:23 step:22001 [D loss: 0.661651, acc: 60.16%] [G loss: 1.807625]\n",
      "epoch:23 step:22002 [D loss: 0.727669, acc: 43.75%] [G loss: 1.832200]\n",
      "epoch:23 step:22003 [D loss: 0.650994, acc: 63.28%] [G loss: 1.912586]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22004 [D loss: 0.623344, acc: 65.62%] [G loss: 1.825683]\n",
      "epoch:23 step:22005 [D loss: 0.659747, acc: 60.16%] [G loss: 1.889428]\n",
      "epoch:23 step:22006 [D loss: 0.692175, acc: 54.69%] [G loss: 1.756538]\n",
      "epoch:23 step:22007 [D loss: 0.647656, acc: 61.72%] [G loss: 1.813260]\n",
      "epoch:23 step:22008 [D loss: 0.652728, acc: 65.62%] [G loss: 1.957472]\n",
      "epoch:23 step:22009 [D loss: 0.674744, acc: 60.94%] [G loss: 1.750019]\n",
      "epoch:23 step:22010 [D loss: 0.681980, acc: 50.00%] [G loss: 1.900219]\n",
      "epoch:23 step:22011 [D loss: 0.650142, acc: 56.25%] [G loss: 1.831290]\n",
      "epoch:23 step:22012 [D loss: 0.656863, acc: 55.47%] [G loss: 1.914533]\n",
      "epoch:23 step:22013 [D loss: 0.657242, acc: 60.94%] [G loss: 1.848389]\n",
      "epoch:23 step:22014 [D loss: 0.684959, acc: 57.03%] [G loss: 1.747222]\n",
      "epoch:23 step:22015 [D loss: 0.601484, acc: 71.88%] [G loss: 1.761345]\n",
      "epoch:23 step:22016 [D loss: 0.638604, acc: 64.06%] [G loss: 1.798139]\n",
      "epoch:23 step:22017 [D loss: 0.711909, acc: 58.59%] [G loss: 1.835477]\n",
      "epoch:23 step:22018 [D loss: 0.690599, acc: 56.25%] [G loss: 1.797721]\n",
      "epoch:23 step:22019 [D loss: 0.617379, acc: 66.41%] [G loss: 1.882039]\n",
      "epoch:23 step:22020 [D loss: 0.561453, acc: 73.44%] [G loss: 2.147022]\n",
      "epoch:23 step:22021 [D loss: 0.659336, acc: 58.59%] [G loss: 1.997000]\n",
      "epoch:23 step:22022 [D loss: 0.561083, acc: 68.75%] [G loss: 2.246837]\n",
      "epoch:23 step:22023 [D loss: 0.635476, acc: 64.06%] [G loss: 1.973881]\n",
      "epoch:23 step:22024 [D loss: 0.674545, acc: 62.50%] [G loss: 1.754758]\n",
      "epoch:23 step:22025 [D loss: 0.632530, acc: 65.62%] [G loss: 1.923367]\n",
      "epoch:23 step:22026 [D loss: 0.689458, acc: 57.81%] [G loss: 1.882149]\n",
      "epoch:23 step:22027 [D loss: 0.641891, acc: 64.84%] [G loss: 1.796243]\n",
      "epoch:23 step:22028 [D loss: 0.662630, acc: 60.94%] [G loss: 1.711194]\n",
      "epoch:23 step:22029 [D loss: 0.650151, acc: 61.72%] [G loss: 1.788073]\n",
      "epoch:23 step:22030 [D loss: 0.623405, acc: 63.28%] [G loss: 2.105641]\n",
      "epoch:23 step:22031 [D loss: 0.654020, acc: 60.16%] [G loss: 1.901599]\n",
      "epoch:23 step:22032 [D loss: 0.557217, acc: 71.88%] [G loss: 1.956142]\n",
      "epoch:23 step:22033 [D loss: 0.656908, acc: 63.28%] [G loss: 1.807332]\n",
      "epoch:23 step:22034 [D loss: 0.683698, acc: 54.69%] [G loss: 1.806428]\n",
      "epoch:23 step:22035 [D loss: 0.687929, acc: 60.94%] [G loss: 1.840030]\n",
      "epoch:23 step:22036 [D loss: 0.655384, acc: 60.16%] [G loss: 1.813226]\n",
      "epoch:23 step:22037 [D loss: 0.693701, acc: 58.59%] [G loss: 1.861269]\n",
      "epoch:23 step:22038 [D loss: 0.689878, acc: 50.78%] [G loss: 1.877031]\n",
      "epoch:23 step:22039 [D loss: 0.629125, acc: 55.47%] [G loss: 1.899864]\n",
      "epoch:23 step:22040 [D loss: 0.676262, acc: 60.16%] [G loss: 1.800643]\n",
      "epoch:23 step:22041 [D loss: 0.739535, acc: 53.91%] [G loss: 1.853244]\n",
      "epoch:23 step:22042 [D loss: 0.627965, acc: 63.28%] [G loss: 1.750083]\n",
      "epoch:23 step:22043 [D loss: 0.715267, acc: 53.91%] [G loss: 1.631865]\n",
      "epoch:23 step:22044 [D loss: 0.653372, acc: 62.50%] [G loss: 1.737550]\n",
      "epoch:23 step:22045 [D loss: 0.634990, acc: 66.41%] [G loss: 1.933256]\n",
      "epoch:23 step:22046 [D loss: 0.602125, acc: 68.75%] [G loss: 1.903263]\n",
      "epoch:23 step:22047 [D loss: 0.668879, acc: 62.50%] [G loss: 1.855570]\n",
      "epoch:23 step:22048 [D loss: 0.647850, acc: 60.94%] [G loss: 1.947193]\n",
      "epoch:23 step:22049 [D loss: 0.669838, acc: 61.72%] [G loss: 2.044348]\n",
      "epoch:23 step:22050 [D loss: 0.617013, acc: 68.75%] [G loss: 1.895726]\n",
      "epoch:23 step:22051 [D loss: 0.646560, acc: 58.59%] [G loss: 1.829834]\n",
      "epoch:23 step:22052 [D loss: 0.714132, acc: 53.91%] [G loss: 1.815846]\n",
      "epoch:23 step:22053 [D loss: 0.720558, acc: 51.56%] [G loss: 1.676050]\n",
      "epoch:23 step:22054 [D loss: 0.665660, acc: 57.03%] [G loss: 1.734108]\n",
      "epoch:23 step:22055 [D loss: 0.605090, acc: 70.31%] [G loss: 1.752844]\n",
      "epoch:23 step:22056 [D loss: 0.653355, acc: 60.94%] [G loss: 1.692908]\n",
      "epoch:23 step:22057 [D loss: 0.699971, acc: 54.69%] [G loss: 1.682892]\n",
      "epoch:23 step:22058 [D loss: 0.670192, acc: 59.38%] [G loss: 1.854107]\n",
      "epoch:23 step:22059 [D loss: 0.624367, acc: 66.41%] [G loss: 1.967597]\n",
      "epoch:23 step:22060 [D loss: 0.637503, acc: 64.06%] [G loss: 1.850598]\n",
      "epoch:23 step:22061 [D loss: 0.649801, acc: 64.06%] [G loss: 1.862929]\n",
      "epoch:23 step:22062 [D loss: 0.643053, acc: 66.41%] [G loss: 1.764328]\n",
      "epoch:23 step:22063 [D loss: 0.642118, acc: 62.50%] [G loss: 1.847975]\n",
      "epoch:23 step:22064 [D loss: 0.652500, acc: 60.16%] [G loss: 1.963965]\n",
      "epoch:23 step:22065 [D loss: 0.629917, acc: 64.84%] [G loss: 1.854408]\n",
      "epoch:23 step:22066 [D loss: 0.626344, acc: 65.62%] [G loss: 1.872525]\n",
      "epoch:23 step:22067 [D loss: 0.619061, acc: 66.41%] [G loss: 1.994470]\n",
      "epoch:23 step:22068 [D loss: 0.623196, acc: 57.81%] [G loss: 1.844623]\n",
      "epoch:23 step:22069 [D loss: 0.655676, acc: 59.38%] [G loss: 1.852092]\n",
      "epoch:23 step:22070 [D loss: 0.651595, acc: 65.62%] [G loss: 1.861148]\n",
      "epoch:23 step:22071 [D loss: 0.585810, acc: 70.31%] [G loss: 1.928336]\n",
      "epoch:23 step:22072 [D loss: 0.649065, acc: 58.59%] [G loss: 1.902446]\n",
      "epoch:23 step:22073 [D loss: 0.651857, acc: 63.28%] [G loss: 1.936598]\n",
      "epoch:23 step:22074 [D loss: 0.650106, acc: 63.28%] [G loss: 2.060074]\n",
      "epoch:23 step:22075 [D loss: 0.659237, acc: 59.38%] [G loss: 1.895179]\n",
      "epoch:23 step:22076 [D loss: 0.705603, acc: 55.47%] [G loss: 1.847039]\n",
      "epoch:23 step:22077 [D loss: 0.660641, acc: 59.38%] [G loss: 1.885940]\n",
      "epoch:23 step:22078 [D loss: 0.679903, acc: 51.56%] [G loss: 1.704210]\n",
      "epoch:23 step:22079 [D loss: 0.654631, acc: 57.03%] [G loss: 1.740295]\n",
      "epoch:23 step:22080 [D loss: 0.674989, acc: 56.25%] [G loss: 1.779084]\n",
      "epoch:23 step:22081 [D loss: 0.659675, acc: 60.16%] [G loss: 1.809419]\n",
      "epoch:23 step:22082 [D loss: 0.672350, acc: 60.94%] [G loss: 1.756889]\n",
      "epoch:23 step:22083 [D loss: 0.620327, acc: 60.16%] [G loss: 1.883665]\n",
      "epoch:23 step:22084 [D loss: 0.623721, acc: 62.50%] [G loss: 2.016782]\n",
      "epoch:23 step:22085 [D loss: 0.604643, acc: 67.97%] [G loss: 1.977226]\n",
      "epoch:23 step:22086 [D loss: 0.638876, acc: 64.06%] [G loss: 1.718612]\n",
      "epoch:23 step:22087 [D loss: 0.623689, acc: 67.97%] [G loss: 1.881265]\n",
      "epoch:23 step:22088 [D loss: 0.678242, acc: 58.59%] [G loss: 1.734974]\n",
      "epoch:23 step:22089 [D loss: 0.708825, acc: 53.91%] [G loss: 1.857344]\n",
      "epoch:23 step:22090 [D loss: 0.690817, acc: 56.25%] [G loss: 1.781039]\n",
      "epoch:23 step:22091 [D loss: 0.647467, acc: 59.38%] [G loss: 1.785134]\n",
      "epoch:23 step:22092 [D loss: 0.640234, acc: 61.72%] [G loss: 1.894892]\n",
      "epoch:23 step:22093 [D loss: 0.672654, acc: 57.03%] [G loss: 1.736686]\n",
      "epoch:23 step:22094 [D loss: 0.606692, acc: 63.28%] [G loss: 1.856792]\n",
      "epoch:23 step:22095 [D loss: 0.659207, acc: 60.94%] [G loss: 1.874338]\n",
      "epoch:23 step:22096 [D loss: 0.591072, acc: 71.88%] [G loss: 2.001872]\n",
      "epoch:23 step:22097 [D loss: 0.674980, acc: 57.03%] [G loss: 1.863174]\n",
      "epoch:23 step:22098 [D loss: 0.674979, acc: 56.25%] [G loss: 1.894556]\n",
      "epoch:23 step:22099 [D loss: 0.641515, acc: 60.16%] [G loss: 1.869019]\n",
      "epoch:23 step:22100 [D loss: 0.603822, acc: 68.75%] [G loss: 1.965975]\n",
      "epoch:23 step:22101 [D loss: 0.622790, acc: 64.84%] [G loss: 1.967804]\n",
      "epoch:23 step:22102 [D loss: 0.607394, acc: 67.97%] [G loss: 2.119184]\n",
      "epoch:23 step:22103 [D loss: 0.617286, acc: 65.62%] [G loss: 1.996939]\n",
      "epoch:23 step:22104 [D loss: 0.654452, acc: 60.94%] [G loss: 1.926869]\n",
      "epoch:23 step:22105 [D loss: 0.612939, acc: 69.53%] [G loss: 1.934229]\n",
      "epoch:23 step:22106 [D loss: 0.623130, acc: 66.41%] [G loss: 1.950006]\n",
      "epoch:23 step:22107 [D loss: 0.635738, acc: 67.19%] [G loss: 1.843311]\n",
      "epoch:23 step:22108 [D loss: 0.585785, acc: 70.31%] [G loss: 1.951141]\n",
      "epoch:23 step:22109 [D loss: 0.669451, acc: 56.25%] [G loss: 2.108442]\n",
      "epoch:23 step:22110 [D loss: 0.659871, acc: 55.47%] [G loss: 1.861760]\n",
      "epoch:23 step:22111 [D loss: 0.700112, acc: 54.69%] [G loss: 1.841423]\n",
      "epoch:23 step:22112 [D loss: 0.638660, acc: 67.97%] [G loss: 1.917588]\n",
      "epoch:23 step:22113 [D loss: 0.650418, acc: 65.62%] [G loss: 1.921177]\n",
      "epoch:23 step:22114 [D loss: 0.650141, acc: 66.41%] [G loss: 1.861341]\n",
      "epoch:23 step:22115 [D loss: 0.577475, acc: 70.31%] [G loss: 2.115621]\n",
      "epoch:23 step:22116 [D loss: 0.687993, acc: 59.38%] [G loss: 1.772583]\n",
      "epoch:23 step:22117 [D loss: 0.702780, acc: 53.12%] [G loss: 1.754782]\n",
      "epoch:23 step:22118 [D loss: 0.689226, acc: 54.69%] [G loss: 1.808629]\n",
      "epoch:23 step:22119 [D loss: 0.699911, acc: 58.59%] [G loss: 1.848428]\n",
      "epoch:23 step:22120 [D loss: 0.631828, acc: 63.28%] [G loss: 1.811584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22121 [D loss: 0.656006, acc: 57.03%] [G loss: 1.761048]\n",
      "epoch:23 step:22122 [D loss: 0.618242, acc: 66.41%] [G loss: 1.819183]\n",
      "epoch:23 step:22123 [D loss: 0.665152, acc: 60.94%] [G loss: 1.917747]\n",
      "epoch:23 step:22124 [D loss: 0.664835, acc: 57.81%] [G loss: 1.823130]\n",
      "epoch:23 step:22125 [D loss: 0.619632, acc: 68.75%] [G loss: 1.969965]\n",
      "epoch:23 step:22126 [D loss: 0.663461, acc: 57.81%] [G loss: 1.799293]\n",
      "epoch:23 step:22127 [D loss: 0.678786, acc: 60.16%] [G loss: 1.851455]\n",
      "epoch:23 step:22128 [D loss: 0.692225, acc: 55.47%] [G loss: 1.828304]\n",
      "epoch:23 step:22129 [D loss: 0.605220, acc: 67.19%] [G loss: 1.800732]\n",
      "epoch:23 step:22130 [D loss: 0.638719, acc: 64.84%] [G loss: 1.798181]\n",
      "epoch:23 step:22131 [D loss: 0.640172, acc: 64.06%] [G loss: 1.780181]\n",
      "epoch:23 step:22132 [D loss: 0.657176, acc: 59.38%] [G loss: 1.781988]\n",
      "epoch:23 step:22133 [D loss: 0.639914, acc: 61.72%] [G loss: 1.959252]\n",
      "epoch:23 step:22134 [D loss: 0.658219, acc: 63.28%] [G loss: 1.827458]\n",
      "epoch:23 step:22135 [D loss: 0.694442, acc: 50.00%] [G loss: 1.737513]\n",
      "epoch:23 step:22136 [D loss: 0.702631, acc: 51.56%] [G loss: 1.763374]\n",
      "epoch:23 step:22137 [D loss: 0.672637, acc: 59.38%] [G loss: 1.870758]\n",
      "epoch:23 step:22138 [D loss: 0.625722, acc: 61.72%] [G loss: 1.925050]\n",
      "epoch:23 step:22139 [D loss: 0.629352, acc: 66.41%] [G loss: 1.929517]\n",
      "epoch:23 step:22140 [D loss: 0.635700, acc: 66.41%] [G loss: 1.824323]\n",
      "epoch:23 step:22141 [D loss: 0.686396, acc: 60.16%] [G loss: 1.892466]\n",
      "epoch:23 step:22142 [D loss: 0.654760, acc: 63.28%] [G loss: 1.961810]\n",
      "epoch:23 step:22143 [D loss: 0.633814, acc: 67.97%] [G loss: 1.856509]\n",
      "epoch:23 step:22144 [D loss: 0.640015, acc: 63.28%] [G loss: 1.876733]\n",
      "epoch:23 step:22145 [D loss: 0.651654, acc: 64.06%] [G loss: 1.865859]\n",
      "epoch:23 step:22146 [D loss: 0.622579, acc: 66.41%] [G loss: 1.751529]\n",
      "epoch:23 step:22147 [D loss: 0.661100, acc: 59.38%] [G loss: 1.730748]\n",
      "epoch:23 step:22148 [D loss: 0.692932, acc: 53.91%] [G loss: 1.831568]\n",
      "epoch:23 step:22149 [D loss: 0.641179, acc: 63.28%] [G loss: 1.842722]\n",
      "epoch:23 step:22150 [D loss: 0.682614, acc: 54.69%] [G loss: 1.734708]\n",
      "epoch:23 step:22151 [D loss: 0.643181, acc: 60.94%] [G loss: 1.920279]\n",
      "epoch:23 step:22152 [D loss: 0.676690, acc: 61.72%] [G loss: 1.979060]\n",
      "epoch:23 step:22153 [D loss: 0.636749, acc: 60.16%] [G loss: 1.896957]\n",
      "epoch:23 step:22154 [D loss: 0.721840, acc: 50.00%] [G loss: 2.017450]\n",
      "epoch:23 step:22155 [D loss: 0.645548, acc: 62.50%] [G loss: 2.044479]\n",
      "epoch:23 step:22156 [D loss: 0.612339, acc: 64.84%] [G loss: 1.854211]\n",
      "epoch:23 step:22157 [D loss: 0.667839, acc: 60.16%] [G loss: 1.771155]\n",
      "epoch:23 step:22158 [D loss: 0.660580, acc: 60.94%] [G loss: 1.868821]\n",
      "epoch:23 step:22159 [D loss: 0.673087, acc: 57.03%] [G loss: 1.944534]\n",
      "epoch:23 step:22160 [D loss: 0.597508, acc: 68.75%] [G loss: 1.765518]\n",
      "epoch:23 step:22161 [D loss: 0.640834, acc: 63.28%] [G loss: 1.843089]\n",
      "epoch:23 step:22162 [D loss: 0.658747, acc: 61.72%] [G loss: 1.744649]\n",
      "epoch:23 step:22163 [D loss: 0.630491, acc: 67.19%] [G loss: 1.791552]\n",
      "epoch:23 step:22164 [D loss: 0.631796, acc: 66.41%] [G loss: 1.871850]\n",
      "epoch:23 step:22165 [D loss: 0.681780, acc: 55.47%] [G loss: 1.818745]\n",
      "epoch:23 step:22166 [D loss: 0.636315, acc: 60.94%] [G loss: 1.764661]\n",
      "epoch:23 step:22167 [D loss: 0.633151, acc: 62.50%] [G loss: 1.886712]\n",
      "epoch:23 step:22168 [D loss: 0.699205, acc: 57.81%] [G loss: 1.931281]\n",
      "epoch:23 step:22169 [D loss: 0.629738, acc: 66.41%] [G loss: 1.867913]\n",
      "epoch:23 step:22170 [D loss: 0.651771, acc: 60.94%] [G loss: 1.891967]\n",
      "epoch:23 step:22171 [D loss: 0.644193, acc: 60.16%] [G loss: 1.790248]\n",
      "epoch:23 step:22172 [D loss: 0.634544, acc: 63.28%] [G loss: 1.846021]\n",
      "epoch:23 step:22173 [D loss: 0.692040, acc: 57.03%] [G loss: 1.897847]\n",
      "epoch:23 step:22174 [D loss: 0.628527, acc: 61.72%] [G loss: 1.885208]\n",
      "epoch:23 step:22175 [D loss: 0.580919, acc: 71.09%] [G loss: 2.132562]\n",
      "epoch:23 step:22176 [D loss: 0.684470, acc: 57.81%] [G loss: 1.739232]\n",
      "epoch:23 step:22177 [D loss: 0.611486, acc: 67.19%] [G loss: 1.809073]\n",
      "epoch:23 step:22178 [D loss: 0.615635, acc: 63.28%] [G loss: 1.882630]\n",
      "epoch:23 step:22179 [D loss: 0.679871, acc: 54.69%] [G loss: 1.809968]\n",
      "epoch:23 step:22180 [D loss: 0.663784, acc: 64.84%] [G loss: 1.896838]\n",
      "epoch:23 step:22181 [D loss: 0.667230, acc: 55.47%] [G loss: 1.844452]\n",
      "epoch:23 step:22182 [D loss: 0.616892, acc: 64.84%] [G loss: 1.904961]\n",
      "epoch:23 step:22183 [D loss: 0.659425, acc: 66.41%] [G loss: 1.919643]\n",
      "epoch:23 step:22184 [D loss: 0.683831, acc: 61.72%] [G loss: 1.852171]\n",
      "epoch:23 step:22185 [D loss: 0.634821, acc: 64.06%] [G loss: 2.040597]\n",
      "epoch:23 step:22186 [D loss: 0.596989, acc: 67.97%] [G loss: 1.994287]\n",
      "epoch:23 step:22187 [D loss: 0.632820, acc: 62.50%] [G loss: 1.893440]\n",
      "epoch:23 step:22188 [D loss: 0.628182, acc: 64.06%] [G loss: 1.967459]\n",
      "epoch:23 step:22189 [D loss: 0.628383, acc: 64.84%] [G loss: 1.954382]\n",
      "epoch:23 step:22190 [D loss: 0.667349, acc: 56.25%] [G loss: 1.775638]\n",
      "epoch:23 step:22191 [D loss: 0.668755, acc: 58.59%] [G loss: 1.825456]\n",
      "epoch:23 step:22192 [D loss: 0.656124, acc: 66.41%] [G loss: 1.855693]\n",
      "epoch:23 step:22193 [D loss: 0.621706, acc: 64.84%] [G loss: 1.905034]\n",
      "epoch:23 step:22194 [D loss: 0.682071, acc: 60.16%] [G loss: 1.949830]\n",
      "epoch:23 step:22195 [D loss: 0.644772, acc: 69.53%] [G loss: 1.971375]\n",
      "epoch:23 step:22196 [D loss: 0.657122, acc: 57.81%] [G loss: 1.782348]\n",
      "epoch:23 step:22197 [D loss: 0.611552, acc: 65.62%] [G loss: 1.947679]\n",
      "epoch:23 step:22198 [D loss: 0.599542, acc: 67.97%] [G loss: 2.006945]\n",
      "epoch:23 step:22199 [D loss: 0.636196, acc: 65.62%] [G loss: 2.274786]\n",
      "epoch:23 step:22200 [D loss: 0.638667, acc: 65.62%] [G loss: 2.059962]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.534407\n",
      "FID: 10.273587\n",
      "0 = 12.805040501928305\n",
      "1 = 0.09503187548285094\n",
      "2 = 0.8737000226974487\n",
      "3 = 0.8912000060081482\n",
      "4 = 0.8561999797821045\n",
      "5 = 0.8610628247261047\n",
      "6 = 0.8912000060081482\n",
      "7 = 6.180443639183044\n",
      "8 = 0.06114540823464303\n",
      "9 = 0.7055000066757202\n",
      "10 = 0.7211999893188477\n",
      "11 = 0.6898000240325928\n",
      "12 = 0.6992437243461609\n",
      "13 = 0.7211999893188477\n",
      "14 = 7.5344367027282715\n",
      "15 = 9.442253112792969\n",
      "16 = 0.105239637196064\n",
      "17 = 7.534407138824463\n",
      "18 = 10.273587226867676\n",
      "epoch:23 step:22201 [D loss: 0.651368, acc: 60.94%] [G loss: 2.022098]\n",
      "epoch:23 step:22202 [D loss: 0.653915, acc: 60.94%] [G loss: 1.921081]\n",
      "epoch:23 step:22203 [D loss: 0.599857, acc: 74.22%] [G loss: 1.945012]\n",
      "epoch:23 step:22204 [D loss: 0.680671, acc: 61.72%] [G loss: 2.010353]\n",
      "epoch:23 step:22205 [D loss: 0.655109, acc: 64.06%] [G loss: 1.873849]\n",
      "epoch:23 step:22206 [D loss: 0.668591, acc: 62.50%] [G loss: 1.897179]\n",
      "epoch:23 step:22207 [D loss: 0.666410, acc: 63.28%] [G loss: 1.723395]\n",
      "epoch:23 step:22208 [D loss: 0.705556, acc: 55.47%] [G loss: 1.641410]\n",
      "epoch:23 step:22209 [D loss: 0.652046, acc: 57.81%] [G loss: 1.706074]\n",
      "epoch:23 step:22210 [D loss: 0.680953, acc: 57.03%] [G loss: 1.800004]\n",
      "epoch:23 step:22211 [D loss: 0.619862, acc: 67.19%] [G loss: 1.911945]\n",
      "epoch:23 step:22212 [D loss: 0.651467, acc: 53.12%] [G loss: 1.883312]\n",
      "epoch:23 step:22213 [D loss: 0.670962, acc: 54.69%] [G loss: 1.865647]\n",
      "epoch:23 step:22214 [D loss: 0.634243, acc: 62.50%] [G loss: 1.782939]\n",
      "epoch:23 step:22215 [D loss: 0.667553, acc: 56.25%] [G loss: 1.674637]\n",
      "epoch:23 step:22216 [D loss: 0.693126, acc: 60.94%] [G loss: 1.708971]\n",
      "epoch:23 step:22217 [D loss: 0.662937, acc: 54.69%] [G loss: 1.776241]\n",
      "epoch:23 step:22218 [D loss: 0.665001, acc: 56.25%] [G loss: 1.809152]\n",
      "epoch:23 step:22219 [D loss: 0.605737, acc: 69.53%] [G loss: 1.793715]\n",
      "epoch:23 step:22220 [D loss: 0.643309, acc: 67.19%] [G loss: 1.777457]\n",
      "epoch:23 step:22221 [D loss: 0.707439, acc: 57.03%] [G loss: 1.837587]\n",
      "epoch:23 step:22222 [D loss: 0.663406, acc: 62.50%] [G loss: 1.807230]\n",
      "epoch:23 step:22223 [D loss: 0.667596, acc: 58.59%] [G loss: 1.802963]\n",
      "epoch:23 step:22224 [D loss: 0.692117, acc: 60.94%] [G loss: 1.808307]\n",
      "epoch:23 step:22225 [D loss: 0.644339, acc: 63.28%] [G loss: 1.718851]\n",
      "epoch:23 step:22226 [D loss: 0.674812, acc: 61.72%] [G loss: 1.745017]\n",
      "epoch:23 step:22227 [D loss: 0.675452, acc: 56.25%] [G loss: 1.874942]\n",
      "epoch:23 step:22228 [D loss: 0.608662, acc: 67.19%] [G loss: 1.884349]\n",
      "epoch:23 step:22229 [D loss: 0.652221, acc: 56.25%] [G loss: 1.884241]\n",
      "epoch:23 step:22230 [D loss: 0.634606, acc: 64.06%] [G loss: 1.840292]\n",
      "epoch:23 step:22231 [D loss: 0.640965, acc: 64.06%] [G loss: 1.901742]\n",
      "epoch:23 step:22232 [D loss: 0.637009, acc: 60.94%] [G loss: 2.017828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22233 [D loss: 0.666220, acc: 59.38%] [G loss: 1.743321]\n",
      "epoch:23 step:22234 [D loss: 0.652477, acc: 60.94%] [G loss: 1.798636]\n",
      "epoch:23 step:22235 [D loss: 0.647648, acc: 66.41%] [G loss: 1.913514]\n",
      "epoch:23 step:22236 [D loss: 0.633348, acc: 63.28%] [G loss: 1.816935]\n",
      "epoch:23 step:22237 [D loss: 0.647202, acc: 66.41%] [G loss: 1.870069]\n",
      "epoch:23 step:22238 [D loss: 0.667996, acc: 65.62%] [G loss: 1.806728]\n",
      "epoch:23 step:22239 [D loss: 0.648612, acc: 61.72%] [G loss: 1.776449]\n",
      "epoch:23 step:22240 [D loss: 0.630807, acc: 62.50%] [G loss: 1.974466]\n",
      "epoch:23 step:22241 [D loss: 0.645415, acc: 60.94%] [G loss: 1.985198]\n",
      "epoch:23 step:22242 [D loss: 0.647138, acc: 62.50%] [G loss: 1.859922]\n",
      "epoch:23 step:22243 [D loss: 0.605568, acc: 63.28%] [G loss: 1.893895]\n",
      "epoch:23 step:22244 [D loss: 0.612042, acc: 67.97%] [G loss: 1.965359]\n",
      "epoch:23 step:22245 [D loss: 0.560621, acc: 73.44%] [G loss: 2.125718]\n",
      "epoch:23 step:22246 [D loss: 0.647200, acc: 57.81%] [G loss: 1.831772]\n",
      "epoch:23 step:22247 [D loss: 0.638259, acc: 63.28%] [G loss: 1.961913]\n",
      "epoch:23 step:22248 [D loss: 0.607989, acc: 65.62%] [G loss: 1.922022]\n",
      "epoch:23 step:22249 [D loss: 0.657894, acc: 60.94%] [G loss: 1.705817]\n",
      "epoch:23 step:22250 [D loss: 0.675522, acc: 58.59%] [G loss: 1.985341]\n",
      "epoch:23 step:22251 [D loss: 0.605873, acc: 65.62%] [G loss: 1.987917]\n",
      "epoch:23 step:22252 [D loss: 0.647015, acc: 64.84%] [G loss: 1.986710]\n",
      "epoch:23 step:22253 [D loss: 0.623651, acc: 62.50%] [G loss: 1.852228]\n",
      "epoch:23 step:22254 [D loss: 0.652874, acc: 64.84%] [G loss: 1.754644]\n",
      "epoch:23 step:22255 [D loss: 0.655308, acc: 59.38%] [G loss: 1.843882]\n",
      "epoch:23 step:22256 [D loss: 0.677663, acc: 60.16%] [G loss: 1.828440]\n",
      "epoch:23 step:22257 [D loss: 0.621558, acc: 65.62%] [G loss: 1.907565]\n",
      "epoch:23 step:22258 [D loss: 0.591603, acc: 73.44%] [G loss: 1.980214]\n",
      "epoch:23 step:22259 [D loss: 0.608201, acc: 67.97%] [G loss: 1.921258]\n",
      "epoch:23 step:22260 [D loss: 0.629934, acc: 63.28%] [G loss: 1.934228]\n",
      "epoch:23 step:22261 [D loss: 0.596646, acc: 65.62%] [G loss: 1.778613]\n",
      "epoch:23 step:22262 [D loss: 0.655640, acc: 57.81%] [G loss: 2.050296]\n",
      "epoch:23 step:22263 [D loss: 0.603880, acc: 67.19%] [G loss: 1.996941]\n",
      "epoch:23 step:22264 [D loss: 0.604251, acc: 68.75%] [G loss: 2.013732]\n",
      "epoch:23 step:22265 [D loss: 0.728231, acc: 56.25%] [G loss: 1.992216]\n",
      "epoch:23 step:22266 [D loss: 0.646886, acc: 57.03%] [G loss: 1.877239]\n",
      "epoch:23 step:22267 [D loss: 0.644636, acc: 64.06%] [G loss: 1.810884]\n",
      "epoch:23 step:22268 [D loss: 0.661006, acc: 61.72%] [G loss: 1.774667]\n",
      "epoch:23 step:22269 [D loss: 0.626988, acc: 67.97%] [G loss: 1.951539]\n",
      "epoch:23 step:22270 [D loss: 0.681755, acc: 64.06%] [G loss: 1.991309]\n",
      "epoch:23 step:22271 [D loss: 0.669946, acc: 58.59%] [G loss: 1.970248]\n",
      "epoch:23 step:22272 [D loss: 0.622912, acc: 67.19%] [G loss: 2.076992]\n",
      "epoch:23 step:22273 [D loss: 0.713800, acc: 59.38%] [G loss: 1.905358]\n",
      "epoch:23 step:22274 [D loss: 0.654281, acc: 61.72%] [G loss: 1.802739]\n",
      "epoch:23 step:22275 [D loss: 0.643330, acc: 63.28%] [G loss: 2.058329]\n",
      "epoch:23 step:22276 [D loss: 0.605303, acc: 65.62%] [G loss: 1.922817]\n",
      "epoch:23 step:22277 [D loss: 0.592459, acc: 69.53%] [G loss: 2.071700]\n",
      "epoch:23 step:22278 [D loss: 0.626787, acc: 64.84%] [G loss: 1.865933]\n",
      "epoch:23 step:22279 [D loss: 0.636236, acc: 64.06%] [G loss: 1.921716]\n",
      "epoch:23 step:22280 [D loss: 0.659848, acc: 58.59%] [G loss: 1.754518]\n",
      "epoch:23 step:22281 [D loss: 0.657788, acc: 60.16%] [G loss: 1.823749]\n",
      "epoch:23 step:22282 [D loss: 0.673406, acc: 57.81%] [G loss: 1.848227]\n",
      "epoch:23 step:22283 [D loss: 0.633636, acc: 64.06%] [G loss: 1.819682]\n",
      "epoch:23 step:22284 [D loss: 0.641703, acc: 60.94%] [G loss: 1.823749]\n",
      "epoch:23 step:22285 [D loss: 0.741194, acc: 55.47%] [G loss: 1.731005]\n",
      "epoch:23 step:22286 [D loss: 0.619844, acc: 70.31%] [G loss: 1.971836]\n",
      "epoch:23 step:22287 [D loss: 0.631032, acc: 60.16%] [G loss: 1.842144]\n",
      "epoch:23 step:22288 [D loss: 0.641659, acc: 64.06%] [G loss: 1.912358]\n",
      "epoch:23 step:22289 [D loss: 0.648402, acc: 64.06%] [G loss: 1.825629]\n",
      "epoch:23 step:22290 [D loss: 0.702945, acc: 53.12%] [G loss: 1.773474]\n",
      "epoch:23 step:22291 [D loss: 0.697749, acc: 52.34%] [G loss: 1.853605]\n",
      "epoch:23 step:22292 [D loss: 0.675779, acc: 56.25%] [G loss: 1.730586]\n",
      "epoch:23 step:22293 [D loss: 0.693180, acc: 53.12%] [G loss: 1.732163]\n",
      "epoch:23 step:22294 [D loss: 0.660183, acc: 60.94%] [G loss: 1.757028]\n",
      "epoch:23 step:22295 [D loss: 0.668888, acc: 57.81%] [G loss: 1.716294]\n",
      "epoch:23 step:22296 [D loss: 0.675035, acc: 56.25%] [G loss: 1.875199]\n",
      "epoch:23 step:22297 [D loss: 0.609648, acc: 71.88%] [G loss: 1.902871]\n",
      "epoch:23 step:22298 [D loss: 0.578382, acc: 68.75%] [G loss: 1.999441]\n",
      "epoch:23 step:22299 [D loss: 0.673165, acc: 60.94%] [G loss: 1.805407]\n",
      "epoch:23 step:22300 [D loss: 0.670211, acc: 56.25%] [G loss: 1.725036]\n",
      "epoch:23 step:22301 [D loss: 0.655511, acc: 58.59%] [G loss: 1.754117]\n",
      "epoch:23 step:22302 [D loss: 0.648931, acc: 61.72%] [G loss: 1.760275]\n",
      "epoch:23 step:22303 [D loss: 0.626791, acc: 60.94%] [G loss: 1.722925]\n",
      "epoch:23 step:22304 [D loss: 0.611022, acc: 67.97%] [G loss: 1.928834]\n",
      "epoch:23 step:22305 [D loss: 0.599508, acc: 65.62%] [G loss: 1.894893]\n",
      "epoch:23 step:22306 [D loss: 0.603870, acc: 67.19%] [G loss: 1.929900]\n",
      "epoch:23 step:22307 [D loss: 0.666057, acc: 60.16%] [G loss: 1.916461]\n",
      "epoch:23 step:22308 [D loss: 0.655767, acc: 58.59%] [G loss: 1.849855]\n",
      "epoch:23 step:22309 [D loss: 0.611749, acc: 66.41%] [G loss: 1.825424]\n",
      "epoch:23 step:22310 [D loss: 0.681491, acc: 60.16%] [G loss: 1.760654]\n",
      "epoch:23 step:22311 [D loss: 0.650391, acc: 62.50%] [G loss: 1.937212]\n",
      "epoch:23 step:22312 [D loss: 0.657024, acc: 60.94%] [G loss: 1.838955]\n",
      "epoch:23 step:22313 [D loss: 0.622524, acc: 65.62%] [G loss: 1.874613]\n",
      "epoch:23 step:22314 [D loss: 0.686961, acc: 53.91%] [G loss: 1.920153]\n",
      "epoch:23 step:22315 [D loss: 0.632355, acc: 61.72%] [G loss: 1.877083]\n",
      "epoch:23 step:22316 [D loss: 0.672393, acc: 59.38%] [G loss: 1.622243]\n",
      "epoch:23 step:22317 [D loss: 0.686478, acc: 58.59%] [G loss: 1.697607]\n",
      "epoch:23 step:22318 [D loss: 0.614028, acc: 65.62%] [G loss: 1.842345]\n",
      "epoch:23 step:22319 [D loss: 0.679792, acc: 57.03%] [G loss: 1.842123]\n",
      "epoch:23 step:22320 [D loss: 0.637522, acc: 64.06%] [G loss: 2.035080]\n",
      "epoch:23 step:22321 [D loss: 0.659423, acc: 58.59%] [G loss: 1.926425]\n",
      "epoch:23 step:22322 [D loss: 0.664319, acc: 60.94%] [G loss: 1.747375]\n",
      "epoch:23 step:22323 [D loss: 0.651805, acc: 59.38%] [G loss: 1.963511]\n",
      "epoch:23 step:22324 [D loss: 0.679804, acc: 60.16%] [G loss: 1.844820]\n",
      "epoch:23 step:22325 [D loss: 0.589514, acc: 69.53%] [G loss: 2.122986]\n",
      "epoch:23 step:22326 [D loss: 0.629493, acc: 67.19%] [G loss: 2.180497]\n",
      "epoch:23 step:22327 [D loss: 0.684351, acc: 54.69%] [G loss: 1.935578]\n",
      "epoch:23 step:22328 [D loss: 0.665065, acc: 57.03%] [G loss: 1.725485]\n",
      "epoch:23 step:22329 [D loss: 0.680521, acc: 60.94%] [G loss: 1.846318]\n",
      "epoch:23 step:22330 [D loss: 0.672326, acc: 60.16%] [G loss: 1.847325]\n",
      "epoch:23 step:22331 [D loss: 0.677259, acc: 60.16%] [G loss: 1.865170]\n",
      "epoch:23 step:22332 [D loss: 0.678552, acc: 58.59%] [G loss: 2.069484]\n",
      "epoch:23 step:22333 [D loss: 0.632278, acc: 60.94%] [G loss: 1.993852]\n",
      "epoch:23 step:22334 [D loss: 0.630069, acc: 65.62%] [G loss: 1.800841]\n",
      "epoch:23 step:22335 [D loss: 0.704115, acc: 54.69%] [G loss: 1.778167]\n",
      "epoch:23 step:22336 [D loss: 0.654761, acc: 59.38%] [G loss: 1.949969]\n",
      "epoch:23 step:22337 [D loss: 0.644854, acc: 58.59%] [G loss: 1.947504]\n",
      "epoch:23 step:22338 [D loss: 0.632144, acc: 60.94%] [G loss: 1.862561]\n",
      "epoch:23 step:22339 [D loss: 0.693706, acc: 57.03%] [G loss: 1.912153]\n",
      "epoch:23 step:22340 [D loss: 0.659366, acc: 62.50%] [G loss: 1.918812]\n",
      "epoch:23 step:22341 [D loss: 0.614096, acc: 70.31%] [G loss: 1.905093]\n",
      "epoch:23 step:22342 [D loss: 0.682988, acc: 60.16%] [G loss: 1.930991]\n",
      "epoch:23 step:22343 [D loss: 0.672729, acc: 54.69%] [G loss: 1.949046]\n",
      "epoch:23 step:22344 [D loss: 0.646694, acc: 62.50%] [G loss: 1.865614]\n",
      "epoch:23 step:22345 [D loss: 0.703262, acc: 53.12%] [G loss: 1.785903]\n",
      "epoch:23 step:22346 [D loss: 0.714619, acc: 56.25%] [G loss: 1.722714]\n",
      "epoch:23 step:22347 [D loss: 0.684421, acc: 57.03%] [G loss: 1.781111]\n",
      "epoch:23 step:22348 [D loss: 0.737290, acc: 57.03%] [G loss: 1.696803]\n",
      "epoch:23 step:22349 [D loss: 0.658431, acc: 60.16%] [G loss: 1.828283]\n",
      "epoch:23 step:22350 [D loss: 0.635454, acc: 65.62%] [G loss: 1.807787]\n",
      "epoch:23 step:22351 [D loss: 0.733105, acc: 48.44%] [G loss: 1.575371]\n",
      "epoch:23 step:22352 [D loss: 0.681064, acc: 61.72%] [G loss: 1.591868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22353 [D loss: 0.631712, acc: 67.19%] [G loss: 1.805460]\n",
      "epoch:23 step:22354 [D loss: 0.626361, acc: 62.50%] [G loss: 1.777782]\n",
      "epoch:23 step:22355 [D loss: 0.620126, acc: 65.62%] [G loss: 1.864032]\n",
      "epoch:23 step:22356 [D loss: 0.611975, acc: 67.19%] [G loss: 1.808545]\n",
      "epoch:23 step:22357 [D loss: 0.628541, acc: 62.50%] [G loss: 1.881439]\n",
      "epoch:23 step:22358 [D loss: 0.616074, acc: 64.84%] [G loss: 1.779821]\n",
      "epoch:23 step:22359 [D loss: 0.652688, acc: 64.84%] [G loss: 1.852442]\n",
      "epoch:23 step:22360 [D loss: 0.622452, acc: 65.62%] [G loss: 1.803561]\n",
      "epoch:23 step:22361 [D loss: 0.661912, acc: 60.94%] [G loss: 1.738971]\n",
      "epoch:23 step:22362 [D loss: 0.689691, acc: 59.38%] [G loss: 1.768305]\n",
      "epoch:23 step:22363 [D loss: 0.673107, acc: 60.94%] [G loss: 1.717979]\n",
      "epoch:23 step:22364 [D loss: 0.641766, acc: 62.50%] [G loss: 1.899812]\n",
      "epoch:23 step:22365 [D loss: 0.663085, acc: 60.16%] [G loss: 1.796622]\n",
      "epoch:23 step:22366 [D loss: 0.603563, acc: 68.75%] [G loss: 1.933147]\n",
      "epoch:23 step:22367 [D loss: 0.597681, acc: 64.06%] [G loss: 1.921729]\n",
      "epoch:23 step:22368 [D loss: 0.641222, acc: 67.19%] [G loss: 1.797850]\n",
      "epoch:23 step:22369 [D loss: 0.648282, acc: 64.84%] [G loss: 1.736714]\n",
      "epoch:23 step:22370 [D loss: 0.623621, acc: 60.94%] [G loss: 1.904257]\n",
      "epoch:23 step:22371 [D loss: 0.674444, acc: 58.59%] [G loss: 1.644934]\n",
      "epoch:23 step:22372 [D loss: 0.679838, acc: 60.16%] [G loss: 1.888428]\n",
      "epoch:23 step:22373 [D loss: 0.589051, acc: 67.19%] [G loss: 1.833616]\n",
      "epoch:23 step:22374 [D loss: 0.598022, acc: 66.41%] [G loss: 1.876401]\n",
      "epoch:23 step:22375 [D loss: 0.633995, acc: 64.06%] [G loss: 1.840828]\n",
      "epoch:23 step:22376 [D loss: 0.614495, acc: 65.62%] [G loss: 2.048674]\n",
      "epoch:23 step:22377 [D loss: 0.641464, acc: 60.16%] [G loss: 2.015002]\n",
      "epoch:23 step:22378 [D loss: 0.634529, acc: 61.72%] [G loss: 1.844469]\n",
      "epoch:23 step:22379 [D loss: 0.736242, acc: 51.56%] [G loss: 1.771623]\n",
      "epoch:23 step:22380 [D loss: 0.681991, acc: 57.03%] [G loss: 1.830182]\n",
      "epoch:23 step:22381 [D loss: 0.678069, acc: 60.16%] [G loss: 1.740623]\n",
      "epoch:23 step:22382 [D loss: 0.609567, acc: 69.53%] [G loss: 1.827034]\n",
      "epoch:23 step:22383 [D loss: 0.660839, acc: 66.41%] [G loss: 1.891332]\n",
      "epoch:23 step:22384 [D loss: 0.608885, acc: 66.41%] [G loss: 2.002473]\n",
      "epoch:23 step:22385 [D loss: 0.637650, acc: 65.62%] [G loss: 1.796335]\n",
      "epoch:23 step:22386 [D loss: 0.651381, acc: 67.97%] [G loss: 1.709430]\n",
      "epoch:23 step:22387 [D loss: 0.632348, acc: 64.06%] [G loss: 1.818168]\n",
      "epoch:23 step:22388 [D loss: 0.598373, acc: 67.97%] [G loss: 2.009098]\n",
      "epoch:23 step:22389 [D loss: 0.692598, acc: 56.25%] [G loss: 1.891442]\n",
      "epoch:23 step:22390 [D loss: 0.611551, acc: 65.62%] [G loss: 1.864285]\n",
      "epoch:23 step:22391 [D loss: 0.631166, acc: 64.84%] [G loss: 1.880191]\n",
      "epoch:23 step:22392 [D loss: 0.636089, acc: 69.53%] [G loss: 2.000692]\n",
      "epoch:23 step:22393 [D loss: 0.625153, acc: 67.19%] [G loss: 1.905815]\n",
      "epoch:23 step:22394 [D loss: 0.674809, acc: 57.81%] [G loss: 1.722827]\n",
      "epoch:23 step:22395 [D loss: 0.654087, acc: 59.38%] [G loss: 1.903361]\n",
      "epoch:23 step:22396 [D loss: 0.620937, acc: 66.41%] [G loss: 1.964409]\n",
      "epoch:23 step:22397 [D loss: 0.675011, acc: 57.03%] [G loss: 1.889209]\n",
      "epoch:23 step:22398 [D loss: 0.598291, acc: 70.31%] [G loss: 1.759918]\n",
      "epoch:23 step:22399 [D loss: 0.670991, acc: 56.25%] [G loss: 1.911586]\n",
      "epoch:23 step:22400 [D loss: 0.648597, acc: 63.28%] [G loss: 2.161380]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.438377\n",
      "FID: 12.896820\n",
      "0 = 12.484160782432527\n",
      "1 = 0.07830104067562724\n",
      "2 = 0.871999979019165\n",
      "3 = 0.8920000195503235\n",
      "4 = 0.8519999980926514\n",
      "5 = 0.857692301273346\n",
      "6 = 0.8920000195503235\n",
      "7 = 6.266373145723331\n",
      "8 = 0.06818651414363085\n",
      "9 = 0.7056999802589417\n",
      "10 = 0.7164000272750854\n",
      "11 = 0.6949999928474426\n",
      "12 = 0.701390266418457\n",
      "13 = 0.7164000272750854\n",
      "14 = 7.438404560089111\n",
      "15 = 9.481046676635742\n",
      "16 = 0.10640493035316467\n",
      "17 = 7.4383769035339355\n",
      "18 = 12.896820068359375\n",
      "epoch:23 step:22401 [D loss: 0.706905, acc: 54.69%] [G loss: 1.878650]\n",
      "epoch:23 step:22402 [D loss: 0.635375, acc: 64.84%] [G loss: 1.818624]\n",
      "epoch:23 step:22403 [D loss: 0.649764, acc: 59.38%] [G loss: 1.822620]\n",
      "epoch:23 step:22404 [D loss: 0.654557, acc: 62.50%] [G loss: 1.737069]\n",
      "epoch:23 step:22405 [D loss: 0.671809, acc: 56.25%] [G loss: 1.748364]\n",
      "epoch:23 step:22406 [D loss: 0.684648, acc: 58.59%] [G loss: 1.763758]\n",
      "epoch:23 step:22407 [D loss: 0.665722, acc: 56.25%] [G loss: 1.719731]\n",
      "epoch:23 step:22408 [D loss: 0.714130, acc: 56.25%] [G loss: 1.703050]\n",
      "epoch:23 step:22409 [D loss: 0.679462, acc: 60.94%] [G loss: 1.851592]\n",
      "epoch:23 step:22410 [D loss: 0.617972, acc: 67.97%] [G loss: 1.920797]\n",
      "epoch:23 step:22411 [D loss: 0.644404, acc: 63.28%] [G loss: 1.846162]\n",
      "epoch:23 step:22412 [D loss: 0.656490, acc: 63.28%] [G loss: 1.781132]\n",
      "epoch:23 step:22413 [D loss: 0.605258, acc: 69.53%] [G loss: 1.734305]\n",
      "epoch:23 step:22414 [D loss: 0.659663, acc: 60.16%] [G loss: 1.869980]\n",
      "epoch:23 step:22415 [D loss: 0.662941, acc: 60.16%] [G loss: 1.832380]\n",
      "epoch:23 step:22416 [D loss: 0.679341, acc: 53.91%] [G loss: 1.739542]\n",
      "epoch:23 step:22417 [D loss: 0.664915, acc: 59.38%] [G loss: 1.885279]\n",
      "epoch:23 step:22418 [D loss: 0.656861, acc: 60.16%] [G loss: 1.793871]\n",
      "epoch:23 step:22419 [D loss: 0.635571, acc: 60.94%] [G loss: 1.783235]\n",
      "epoch:23 step:22420 [D loss: 0.644312, acc: 66.41%] [G loss: 1.824114]\n",
      "epoch:23 step:22421 [D loss: 0.686806, acc: 57.81%] [G loss: 1.756918]\n",
      "epoch:23 step:22422 [D loss: 0.673594, acc: 60.16%] [G loss: 1.763065]\n",
      "epoch:23 step:22423 [D loss: 0.645317, acc: 67.19%] [G loss: 1.737150]\n",
      "epoch:23 step:22424 [D loss: 0.684464, acc: 54.69%] [G loss: 1.704285]\n",
      "epoch:23 step:22425 [D loss: 0.700876, acc: 54.69%] [G loss: 1.808121]\n",
      "epoch:23 step:22426 [D loss: 0.639052, acc: 68.75%] [G loss: 1.880274]\n",
      "epoch:23 step:22427 [D loss: 0.656229, acc: 62.50%] [G loss: 1.941373]\n",
      "epoch:23 step:22428 [D loss: 0.650800, acc: 64.06%] [G loss: 1.824294]\n",
      "epoch:23 step:22429 [D loss: 0.637146, acc: 62.50%] [G loss: 1.748495]\n",
      "epoch:23 step:22430 [D loss: 0.622953, acc: 67.19%] [G loss: 1.853007]\n",
      "epoch:23 step:22431 [D loss: 0.632873, acc: 64.06%] [G loss: 1.774531]\n",
      "epoch:23 step:22432 [D loss: 0.637329, acc: 65.62%] [G loss: 1.864275]\n",
      "epoch:23 step:22433 [D loss: 0.613258, acc: 68.75%] [G loss: 1.852611]\n",
      "epoch:23 step:22434 [D loss: 0.657411, acc: 63.28%] [G loss: 1.831827]\n",
      "epoch:23 step:22435 [D loss: 0.597417, acc: 66.41%] [G loss: 1.912513]\n",
      "epoch:23 step:22436 [D loss: 0.619703, acc: 64.06%] [G loss: 1.948033]\n",
      "epoch:23 step:22437 [D loss: 0.626545, acc: 63.28%] [G loss: 1.957817]\n",
      "epoch:23 step:22438 [D loss: 0.638192, acc: 67.19%] [G loss: 2.038980]\n",
      "epoch:23 step:22439 [D loss: 0.617339, acc: 62.50%] [G loss: 1.848505]\n",
      "epoch:23 step:22440 [D loss: 0.686776, acc: 63.28%] [G loss: 1.919041]\n",
      "epoch:23 step:22441 [D loss: 0.657003, acc: 59.38%] [G loss: 2.015063]\n",
      "epoch:23 step:22442 [D loss: 0.707800, acc: 57.03%] [G loss: 1.805502]\n",
      "epoch:23 step:22443 [D loss: 0.682934, acc: 64.06%] [G loss: 1.782912]\n",
      "epoch:23 step:22444 [D loss: 0.674970, acc: 55.47%] [G loss: 1.869852]\n",
      "epoch:23 step:22445 [D loss: 0.618314, acc: 67.19%] [G loss: 1.824026]\n",
      "epoch:23 step:22446 [D loss: 0.682756, acc: 56.25%] [G loss: 1.826906]\n",
      "epoch:23 step:22447 [D loss: 0.623064, acc: 68.75%] [G loss: 1.828212]\n",
      "epoch:23 step:22448 [D loss: 0.699345, acc: 56.25%] [G loss: 1.782172]\n",
      "epoch:23 step:22449 [D loss: 0.626371, acc: 61.72%] [G loss: 1.831152]\n",
      "epoch:23 step:22450 [D loss: 0.633603, acc: 59.38%] [G loss: 1.996609]\n",
      "epoch:23 step:22451 [D loss: 0.631979, acc: 61.72%] [G loss: 1.857534]\n",
      "epoch:23 step:22452 [D loss: 0.638884, acc: 60.94%] [G loss: 1.811989]\n",
      "epoch:23 step:22453 [D loss: 0.595598, acc: 67.19%] [G loss: 1.783383]\n",
      "epoch:23 step:22454 [D loss: 0.595042, acc: 67.19%] [G loss: 1.853549]\n",
      "epoch:23 step:22455 [D loss: 0.677914, acc: 59.38%] [G loss: 1.878188]\n",
      "epoch:23 step:22456 [D loss: 0.650022, acc: 62.50%] [G loss: 2.028861]\n",
      "epoch:23 step:22457 [D loss: 0.604332, acc: 66.41%] [G loss: 2.217951]\n",
      "epoch:23 step:22458 [D loss: 0.660058, acc: 61.72%] [G loss: 1.874431]\n",
      "epoch:23 step:22459 [D loss: 0.624611, acc: 64.06%] [G loss: 1.902433]\n",
      "epoch:23 step:22460 [D loss: 0.569243, acc: 71.88%] [G loss: 2.023032]\n",
      "epoch:23 step:22461 [D loss: 0.619916, acc: 66.41%] [G loss: 2.064546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22462 [D loss: 0.673089, acc: 55.47%] [G loss: 1.924823]\n",
      "epoch:23 step:22463 [D loss: 0.644482, acc: 61.72%] [G loss: 2.121424]\n",
      "epoch:23 step:22464 [D loss: 0.697653, acc: 54.69%] [G loss: 1.726004]\n",
      "epoch:23 step:22465 [D loss: 0.669621, acc: 57.03%] [G loss: 1.799056]\n",
      "epoch:23 step:22466 [D loss: 0.660926, acc: 60.94%] [G loss: 1.829077]\n",
      "epoch:23 step:22467 [D loss: 0.652173, acc: 60.16%] [G loss: 1.850472]\n",
      "epoch:23 step:22468 [D loss: 0.629908, acc: 61.72%] [G loss: 1.996420]\n",
      "epoch:23 step:22469 [D loss: 0.601165, acc: 66.41%] [G loss: 2.011225]\n",
      "epoch:23 step:22470 [D loss: 0.619608, acc: 63.28%] [G loss: 2.085643]\n",
      "epoch:23 step:22471 [D loss: 0.736951, acc: 50.78%] [G loss: 1.822705]\n",
      "epoch:23 step:22472 [D loss: 0.671893, acc: 61.72%] [G loss: 1.895400]\n",
      "epoch:23 step:22473 [D loss: 0.588015, acc: 68.75%] [G loss: 1.906091]\n",
      "epoch:23 step:22474 [D loss: 0.618820, acc: 66.41%] [G loss: 2.032812]\n",
      "epoch:23 step:22475 [D loss: 0.549946, acc: 75.00%] [G loss: 2.007406]\n",
      "epoch:23 step:22476 [D loss: 0.608011, acc: 72.66%] [G loss: 2.037956]\n",
      "epoch:23 step:22477 [D loss: 0.624775, acc: 69.53%] [G loss: 2.045574]\n",
      "epoch:23 step:22478 [D loss: 0.634933, acc: 61.72%] [G loss: 2.078876]\n",
      "epoch:23 step:22479 [D loss: 0.779804, acc: 47.66%] [G loss: 1.842678]\n",
      "epoch:23 step:22480 [D loss: 0.686415, acc: 53.12%] [G loss: 1.945922]\n",
      "epoch:23 step:22481 [D loss: 0.649744, acc: 57.03%] [G loss: 2.261972]\n",
      "epoch:23 step:22482 [D loss: 0.625908, acc: 65.62%] [G loss: 1.849769]\n",
      "epoch:23 step:22483 [D loss: 0.648092, acc: 65.62%] [G loss: 1.917440]\n",
      "epoch:23 step:22484 [D loss: 0.616834, acc: 70.31%] [G loss: 1.810104]\n",
      "epoch:23 step:22485 [D loss: 0.614514, acc: 67.97%] [G loss: 1.937174]\n",
      "epoch:23 step:22486 [D loss: 0.649902, acc: 63.28%] [G loss: 2.132573]\n",
      "epoch:23 step:22487 [D loss: 0.610489, acc: 68.75%] [G loss: 2.037963]\n",
      "epoch:23 step:22488 [D loss: 0.576562, acc: 74.22%] [G loss: 2.527340]\n",
      "epoch:24 step:22489 [D loss: 0.697631, acc: 55.47%] [G loss: 1.939376]\n",
      "epoch:24 step:22490 [D loss: 0.647695, acc: 61.72%] [G loss: 2.039830]\n",
      "epoch:24 step:22491 [D loss: 0.655713, acc: 62.50%] [G loss: 1.885487]\n",
      "epoch:24 step:22492 [D loss: 0.645508, acc: 59.38%] [G loss: 1.806227]\n",
      "epoch:24 step:22493 [D loss: 0.601117, acc: 64.84%] [G loss: 1.858821]\n",
      "epoch:24 step:22494 [D loss: 0.607071, acc: 64.06%] [G loss: 1.814586]\n",
      "epoch:24 step:22495 [D loss: 0.650792, acc: 61.72%] [G loss: 1.890565]\n",
      "epoch:24 step:22496 [D loss: 0.612400, acc: 69.53%] [G loss: 1.955820]\n",
      "epoch:24 step:22497 [D loss: 0.646476, acc: 63.28%] [G loss: 2.012464]\n",
      "epoch:24 step:22498 [D loss: 0.551040, acc: 75.00%] [G loss: 1.920866]\n",
      "epoch:24 step:22499 [D loss: 0.620613, acc: 67.19%] [G loss: 2.010802]\n",
      "epoch:24 step:22500 [D loss: 0.609876, acc: 66.41%] [G loss: 1.931836]\n",
      "epoch:24 step:22501 [D loss: 0.648863, acc: 66.41%] [G loss: 2.029092]\n",
      "epoch:24 step:22502 [D loss: 0.641855, acc: 61.72%] [G loss: 2.027387]\n",
      "epoch:24 step:22503 [D loss: 0.608965, acc: 67.19%] [G loss: 2.068811]\n",
      "epoch:24 step:22504 [D loss: 0.564695, acc: 69.53%] [G loss: 1.956919]\n",
      "epoch:24 step:22505 [D loss: 0.665276, acc: 53.91%] [G loss: 1.935792]\n",
      "epoch:24 step:22506 [D loss: 0.629226, acc: 62.50%] [G loss: 1.890811]\n",
      "epoch:24 step:22507 [D loss: 0.653866, acc: 64.06%] [G loss: 1.963938]\n",
      "epoch:24 step:22508 [D loss: 0.752744, acc: 49.22%] [G loss: 1.655376]\n",
      "epoch:24 step:22509 [D loss: 0.649580, acc: 62.50%] [G loss: 1.904114]\n",
      "epoch:24 step:22510 [D loss: 0.640892, acc: 60.16%] [G loss: 1.656558]\n",
      "epoch:24 step:22511 [D loss: 0.628467, acc: 62.50%] [G loss: 2.111434]\n",
      "epoch:24 step:22512 [D loss: 0.608419, acc: 68.75%] [G loss: 1.954368]\n",
      "epoch:24 step:22513 [D loss: 0.660650, acc: 61.72%] [G loss: 2.033639]\n",
      "epoch:24 step:22514 [D loss: 0.741268, acc: 51.56%] [G loss: 2.029707]\n",
      "epoch:24 step:22515 [D loss: 0.660696, acc: 55.47%] [G loss: 1.707882]\n",
      "epoch:24 step:22516 [D loss: 0.676497, acc: 54.69%] [G loss: 1.775488]\n",
      "epoch:24 step:22517 [D loss: 0.627482, acc: 67.19%] [G loss: 1.906091]\n",
      "epoch:24 step:22518 [D loss: 0.675370, acc: 58.59%] [G loss: 1.701440]\n",
      "epoch:24 step:22519 [D loss: 0.663818, acc: 55.47%] [G loss: 1.805739]\n",
      "epoch:24 step:22520 [D loss: 0.696603, acc: 53.91%] [G loss: 1.738814]\n",
      "epoch:24 step:22521 [D loss: 0.697977, acc: 50.78%] [G loss: 1.930257]\n",
      "epoch:24 step:22522 [D loss: 0.678101, acc: 61.72%] [G loss: 1.834809]\n",
      "epoch:24 step:22523 [D loss: 0.601136, acc: 71.88%] [G loss: 1.864654]\n",
      "epoch:24 step:22524 [D loss: 0.617216, acc: 64.84%] [G loss: 2.006872]\n",
      "epoch:24 step:22525 [D loss: 0.719323, acc: 53.12%] [G loss: 1.992586]\n",
      "epoch:24 step:22526 [D loss: 0.662752, acc: 59.38%] [G loss: 1.964620]\n",
      "epoch:24 step:22527 [D loss: 0.636241, acc: 64.06%] [G loss: 1.918759]\n",
      "epoch:24 step:22528 [D loss: 0.608223, acc: 66.41%] [G loss: 2.069953]\n",
      "epoch:24 step:22529 [D loss: 0.573685, acc: 72.66%] [G loss: 1.793177]\n",
      "epoch:24 step:22530 [D loss: 0.636991, acc: 61.72%] [G loss: 1.969178]\n",
      "epoch:24 step:22531 [D loss: 0.631348, acc: 59.38%] [G loss: 1.903064]\n",
      "epoch:24 step:22532 [D loss: 0.651592, acc: 61.72%] [G loss: 1.955346]\n",
      "epoch:24 step:22533 [D loss: 0.629565, acc: 64.06%] [G loss: 1.907792]\n",
      "epoch:24 step:22534 [D loss: 0.693878, acc: 60.16%] [G loss: 1.770419]\n",
      "epoch:24 step:22535 [D loss: 0.677703, acc: 56.25%] [G loss: 1.849832]\n",
      "epoch:24 step:22536 [D loss: 0.599406, acc: 73.44%] [G loss: 1.838822]\n",
      "epoch:24 step:22537 [D loss: 0.585178, acc: 68.75%] [G loss: 1.934137]\n",
      "epoch:24 step:22538 [D loss: 0.630544, acc: 66.41%] [G loss: 1.931155]\n",
      "epoch:24 step:22539 [D loss: 0.688499, acc: 57.81%] [G loss: 1.758838]\n",
      "epoch:24 step:22540 [D loss: 0.666206, acc: 58.59%] [G loss: 1.903559]\n",
      "epoch:24 step:22541 [D loss: 0.671376, acc: 58.59%] [G loss: 1.826414]\n",
      "epoch:24 step:22542 [D loss: 0.657654, acc: 64.06%] [G loss: 2.026047]\n",
      "epoch:24 step:22543 [D loss: 0.622752, acc: 65.62%] [G loss: 2.040837]\n",
      "epoch:24 step:22544 [D loss: 0.656405, acc: 62.50%] [G loss: 2.018364]\n",
      "epoch:24 step:22545 [D loss: 0.613695, acc: 70.31%] [G loss: 1.836786]\n",
      "epoch:24 step:22546 [D loss: 0.629292, acc: 57.03%] [G loss: 1.958401]\n",
      "epoch:24 step:22547 [D loss: 0.677309, acc: 60.16%] [G loss: 1.973167]\n",
      "epoch:24 step:22548 [D loss: 0.670457, acc: 57.03%] [G loss: 1.802763]\n",
      "epoch:24 step:22549 [D loss: 0.667601, acc: 66.41%] [G loss: 1.811033]\n",
      "epoch:24 step:22550 [D loss: 0.636202, acc: 60.94%] [G loss: 1.921879]\n",
      "epoch:24 step:22551 [D loss: 0.627806, acc: 66.41%] [G loss: 1.891658]\n",
      "epoch:24 step:22552 [D loss: 0.713007, acc: 53.91%] [G loss: 1.761662]\n",
      "epoch:24 step:22553 [D loss: 0.655535, acc: 59.38%] [G loss: 1.790686]\n",
      "epoch:24 step:22554 [D loss: 0.610970, acc: 64.84%] [G loss: 1.759018]\n",
      "epoch:24 step:22555 [D loss: 0.652866, acc: 61.72%] [G loss: 1.946602]\n",
      "epoch:24 step:22556 [D loss: 0.667460, acc: 67.19%] [G loss: 1.918413]\n",
      "epoch:24 step:22557 [D loss: 0.620033, acc: 66.41%] [G loss: 1.922401]\n",
      "epoch:24 step:22558 [D loss: 0.622779, acc: 61.72%] [G loss: 1.857382]\n",
      "epoch:24 step:22559 [D loss: 0.698331, acc: 53.12%] [G loss: 1.653663]\n",
      "epoch:24 step:22560 [D loss: 0.632141, acc: 61.72%] [G loss: 1.809218]\n",
      "epoch:24 step:22561 [D loss: 0.657723, acc: 59.38%] [G loss: 1.801209]\n",
      "epoch:24 step:22562 [D loss: 0.588471, acc: 69.53%] [G loss: 1.982929]\n",
      "epoch:24 step:22563 [D loss: 0.612425, acc: 66.41%] [G loss: 2.015429]\n",
      "epoch:24 step:22564 [D loss: 0.667195, acc: 60.16%] [G loss: 2.026896]\n",
      "epoch:24 step:22565 [D loss: 0.589763, acc: 67.19%] [G loss: 2.014646]\n",
      "epoch:24 step:22566 [D loss: 0.679747, acc: 61.72%] [G loss: 1.851494]\n",
      "epoch:24 step:22567 [D loss: 0.638711, acc: 60.94%] [G loss: 1.886347]\n",
      "epoch:24 step:22568 [D loss: 0.678108, acc: 58.59%] [G loss: 1.914726]\n",
      "epoch:24 step:22569 [D loss: 0.714557, acc: 53.91%] [G loss: 1.784721]\n",
      "epoch:24 step:22570 [D loss: 0.658973, acc: 64.06%] [G loss: 1.855844]\n",
      "epoch:24 step:22571 [D loss: 0.636315, acc: 63.28%] [G loss: 1.847648]\n",
      "epoch:24 step:22572 [D loss: 0.598972, acc: 69.53%] [G loss: 1.849486]\n",
      "epoch:24 step:22573 [D loss: 0.658798, acc: 64.06%] [G loss: 1.781447]\n",
      "epoch:24 step:22574 [D loss: 0.634692, acc: 57.81%] [G loss: 1.738164]\n",
      "epoch:24 step:22575 [D loss: 0.651236, acc: 64.06%] [G loss: 1.814414]\n",
      "epoch:24 step:22576 [D loss: 0.654512, acc: 59.38%] [G loss: 2.030979]\n",
      "epoch:24 step:22577 [D loss: 0.636540, acc: 64.84%] [G loss: 2.034337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22578 [D loss: 0.680935, acc: 57.81%] [G loss: 1.756153]\n",
      "epoch:24 step:22579 [D loss: 0.657613, acc: 60.94%] [G loss: 1.906142]\n",
      "epoch:24 step:22580 [D loss: 0.607791, acc: 70.31%] [G loss: 1.964151]\n",
      "epoch:24 step:22581 [D loss: 0.676693, acc: 58.59%] [G loss: 2.074310]\n",
      "epoch:24 step:22582 [D loss: 0.651702, acc: 60.94%] [G loss: 1.897422]\n",
      "epoch:24 step:22583 [D loss: 0.706958, acc: 60.94%] [G loss: 1.721633]\n",
      "epoch:24 step:22584 [D loss: 0.693379, acc: 61.72%] [G loss: 1.783902]\n",
      "epoch:24 step:22585 [D loss: 0.633789, acc: 62.50%] [G loss: 1.954434]\n",
      "epoch:24 step:22586 [D loss: 0.656985, acc: 57.03%] [G loss: 1.778344]\n",
      "epoch:24 step:22587 [D loss: 0.683867, acc: 57.03%] [G loss: 1.886390]\n",
      "epoch:24 step:22588 [D loss: 0.586890, acc: 71.88%] [G loss: 2.043164]\n",
      "epoch:24 step:22589 [D loss: 0.665257, acc: 61.72%] [G loss: 1.770687]\n",
      "epoch:24 step:22590 [D loss: 0.653566, acc: 62.50%] [G loss: 1.938338]\n",
      "epoch:24 step:22591 [D loss: 0.638584, acc: 64.84%] [G loss: 1.959530]\n",
      "epoch:24 step:22592 [D loss: 0.689915, acc: 59.38%] [G loss: 1.777080]\n",
      "epoch:24 step:22593 [D loss: 0.664607, acc: 64.06%] [G loss: 1.838063]\n",
      "epoch:24 step:22594 [D loss: 0.653684, acc: 63.28%] [G loss: 1.867522]\n",
      "epoch:24 step:22595 [D loss: 0.600566, acc: 67.19%] [G loss: 1.971146]\n",
      "epoch:24 step:22596 [D loss: 0.664693, acc: 57.03%] [G loss: 1.734074]\n",
      "epoch:24 step:22597 [D loss: 0.671985, acc: 56.25%] [G loss: 1.755863]\n",
      "epoch:24 step:22598 [D loss: 0.711018, acc: 51.56%] [G loss: 1.829127]\n",
      "epoch:24 step:22599 [D loss: 0.645684, acc: 60.94%] [G loss: 1.898042]\n",
      "epoch:24 step:22600 [D loss: 0.646488, acc: 56.25%] [G loss: 1.884522]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.554745\n",
      "FID: 9.229743\n",
      "0 = 12.695321884155229\n",
      "1 = 0.09074743484780999\n",
      "2 = 0.8756999969482422\n",
      "3 = 0.8949999809265137\n",
      "4 = 0.8564000129699707\n",
      "5 = 0.8617369532585144\n",
      "6 = 0.8949999809265137\n",
      "7 = 6.128604935038097\n",
      "8 = 0.05828573010281474\n",
      "9 = 0.7038999795913696\n",
      "10 = 0.7174000144004822\n",
      "11 = 0.6904000043869019\n",
      "12 = 0.6985394358634949\n",
      "13 = 0.7174000144004822\n",
      "14 = 7.554777145385742\n",
      "15 = 9.44003677368164\n",
      "16 = 0.10816323012113571\n",
      "17 = 7.554745197296143\n",
      "18 = 9.229743003845215\n",
      "epoch:24 step:22601 [D loss: 0.589900, acc: 68.75%] [G loss: 1.987740]\n",
      "epoch:24 step:22602 [D loss: 0.629701, acc: 67.97%] [G loss: 2.004495]\n",
      "epoch:24 step:22603 [D loss: 0.625734, acc: 64.84%] [G loss: 2.021759]\n",
      "epoch:24 step:22604 [D loss: 0.613351, acc: 68.75%] [G loss: 2.057862]\n",
      "epoch:24 step:22605 [D loss: 0.625651, acc: 64.06%] [G loss: 2.049047]\n",
      "epoch:24 step:22606 [D loss: 0.642270, acc: 62.50%] [G loss: 1.989271]\n",
      "epoch:24 step:22607 [D loss: 0.620667, acc: 67.19%] [G loss: 2.090198]\n",
      "epoch:24 step:22608 [D loss: 0.695565, acc: 54.69%] [G loss: 1.975336]\n",
      "epoch:24 step:22609 [D loss: 0.598584, acc: 68.75%] [G loss: 2.025361]\n",
      "epoch:24 step:22610 [D loss: 0.670472, acc: 62.50%] [G loss: 2.137160]\n",
      "epoch:24 step:22611 [D loss: 0.714288, acc: 53.91%] [G loss: 1.761008]\n",
      "epoch:24 step:22612 [D loss: 0.639313, acc: 60.16%] [G loss: 1.925046]\n",
      "epoch:24 step:22613 [D loss: 0.720752, acc: 51.56%] [G loss: 1.692100]\n",
      "epoch:24 step:22614 [D loss: 0.607059, acc: 69.53%] [G loss: 2.013181]\n",
      "epoch:24 step:22615 [D loss: 0.663876, acc: 58.59%] [G loss: 1.844573]\n",
      "epoch:24 step:22616 [D loss: 0.616865, acc: 67.97%] [G loss: 1.730109]\n",
      "epoch:24 step:22617 [D loss: 0.758676, acc: 53.91%] [G loss: 1.726702]\n",
      "epoch:24 step:22618 [D loss: 0.647109, acc: 65.62%] [G loss: 1.861609]\n",
      "epoch:24 step:22619 [D loss: 0.635475, acc: 64.84%] [G loss: 1.930853]\n",
      "epoch:24 step:22620 [D loss: 0.606053, acc: 67.19%] [G loss: 1.928922]\n",
      "epoch:24 step:22621 [D loss: 0.688005, acc: 54.69%] [G loss: 1.745431]\n",
      "epoch:24 step:22622 [D loss: 0.692375, acc: 60.16%] [G loss: 1.762347]\n",
      "epoch:24 step:22623 [D loss: 0.660721, acc: 58.59%] [G loss: 1.870325]\n",
      "epoch:24 step:22624 [D loss: 0.681229, acc: 56.25%] [G loss: 1.756154]\n",
      "epoch:24 step:22625 [D loss: 0.692026, acc: 56.25%] [G loss: 1.845206]\n",
      "epoch:24 step:22626 [D loss: 0.661976, acc: 55.47%] [G loss: 1.744367]\n",
      "epoch:24 step:22627 [D loss: 0.612143, acc: 69.53%] [G loss: 1.778561]\n",
      "epoch:24 step:22628 [D loss: 0.656063, acc: 65.62%] [G loss: 1.879864]\n",
      "epoch:24 step:22629 [D loss: 0.683682, acc: 53.91%] [G loss: 1.855503]\n",
      "epoch:24 step:22630 [D loss: 0.671774, acc: 63.28%] [G loss: 1.787380]\n",
      "epoch:24 step:22631 [D loss: 0.637245, acc: 61.72%] [G loss: 1.709004]\n",
      "epoch:24 step:22632 [D loss: 0.666138, acc: 60.16%] [G loss: 1.822147]\n",
      "epoch:24 step:22633 [D loss: 0.650041, acc: 64.06%] [G loss: 1.905359]\n",
      "epoch:24 step:22634 [D loss: 0.602082, acc: 67.19%] [G loss: 1.975270]\n",
      "epoch:24 step:22635 [D loss: 0.666951, acc: 56.25%] [G loss: 1.769802]\n",
      "epoch:24 step:22636 [D loss: 0.653670, acc: 56.25%] [G loss: 1.764737]\n",
      "epoch:24 step:22637 [D loss: 0.664196, acc: 56.25%] [G loss: 1.978080]\n",
      "epoch:24 step:22638 [D loss: 0.711236, acc: 54.69%] [G loss: 1.928056]\n",
      "epoch:24 step:22639 [D loss: 0.658170, acc: 60.94%] [G loss: 1.791138]\n",
      "epoch:24 step:22640 [D loss: 0.661398, acc: 60.16%] [G loss: 1.808758]\n",
      "epoch:24 step:22641 [D loss: 0.687096, acc: 54.69%] [G loss: 1.820639]\n",
      "epoch:24 step:22642 [D loss: 0.608487, acc: 65.62%] [G loss: 1.897882]\n",
      "epoch:24 step:22643 [D loss: 0.647793, acc: 60.94%] [G loss: 1.904947]\n",
      "epoch:24 step:22644 [D loss: 0.704533, acc: 53.12%] [G loss: 1.786863]\n",
      "epoch:24 step:22645 [D loss: 0.668608, acc: 54.69%] [G loss: 1.869581]\n",
      "epoch:24 step:22646 [D loss: 0.644667, acc: 56.25%] [G loss: 1.917542]\n",
      "epoch:24 step:22647 [D loss: 0.658586, acc: 60.16%] [G loss: 1.772169]\n",
      "epoch:24 step:22648 [D loss: 0.660235, acc: 62.50%] [G loss: 1.741754]\n",
      "epoch:24 step:22649 [D loss: 0.665736, acc: 60.94%] [G loss: 1.839428]\n",
      "epoch:24 step:22650 [D loss: 0.582637, acc: 70.31%] [G loss: 1.808232]\n",
      "epoch:24 step:22651 [D loss: 0.674325, acc: 53.12%] [G loss: 1.749186]\n",
      "epoch:24 step:22652 [D loss: 0.627607, acc: 68.75%] [G loss: 2.014847]\n",
      "epoch:24 step:22653 [D loss: 0.660298, acc: 60.16%] [G loss: 1.786675]\n",
      "epoch:24 step:22654 [D loss: 0.658058, acc: 53.91%] [G loss: 1.820786]\n",
      "epoch:24 step:22655 [D loss: 0.614935, acc: 67.97%] [G loss: 1.828413]\n",
      "epoch:24 step:22656 [D loss: 0.656566, acc: 61.72%] [G loss: 1.849870]\n",
      "epoch:24 step:22657 [D loss: 0.677321, acc: 56.25%] [G loss: 1.811561]\n",
      "epoch:24 step:22658 [D loss: 0.654320, acc: 62.50%] [G loss: 1.840222]\n",
      "epoch:24 step:22659 [D loss: 0.604164, acc: 64.84%] [G loss: 1.951004]\n",
      "epoch:24 step:22660 [D loss: 0.699167, acc: 58.59%] [G loss: 1.748071]\n",
      "epoch:24 step:22661 [D loss: 0.699391, acc: 60.94%] [G loss: 1.708030]\n",
      "epoch:24 step:22662 [D loss: 0.651184, acc: 66.41%] [G loss: 1.767259]\n",
      "epoch:24 step:22663 [D loss: 0.653419, acc: 58.59%] [G loss: 1.676035]\n",
      "epoch:24 step:22664 [D loss: 0.641514, acc: 60.94%] [G loss: 1.696327]\n",
      "epoch:24 step:22665 [D loss: 0.671771, acc: 60.16%] [G loss: 1.787668]\n",
      "epoch:24 step:22666 [D loss: 0.742957, acc: 52.34%] [G loss: 1.697394]\n",
      "epoch:24 step:22667 [D loss: 0.651601, acc: 64.06%] [G loss: 1.715516]\n",
      "epoch:24 step:22668 [D loss: 0.655390, acc: 61.72%] [G loss: 1.929216]\n",
      "epoch:24 step:22669 [D loss: 0.683699, acc: 53.12%] [G loss: 1.833710]\n",
      "epoch:24 step:22670 [D loss: 0.658227, acc: 60.16%] [G loss: 1.762906]\n",
      "epoch:24 step:22671 [D loss: 0.659026, acc: 60.94%] [G loss: 1.679349]\n",
      "epoch:24 step:22672 [D loss: 0.667941, acc: 59.38%] [G loss: 1.815199]\n",
      "epoch:24 step:22673 [D loss: 0.633977, acc: 58.59%] [G loss: 1.862257]\n",
      "epoch:24 step:22674 [D loss: 0.642090, acc: 64.84%] [G loss: 1.760082]\n",
      "epoch:24 step:22675 [D loss: 0.679202, acc: 56.25%] [G loss: 1.770258]\n",
      "epoch:24 step:22676 [D loss: 0.672355, acc: 59.38%] [G loss: 1.743663]\n",
      "epoch:24 step:22677 [D loss: 0.667441, acc: 62.50%] [G loss: 1.802399]\n",
      "epoch:24 step:22678 [D loss: 0.656894, acc: 59.38%] [G loss: 1.781249]\n",
      "epoch:24 step:22679 [D loss: 0.602288, acc: 71.09%] [G loss: 2.017360]\n",
      "epoch:24 step:22680 [D loss: 0.632594, acc: 65.62%] [G loss: 1.865961]\n",
      "epoch:24 step:22681 [D loss: 0.614397, acc: 66.41%] [G loss: 1.941301]\n",
      "epoch:24 step:22682 [D loss: 0.629096, acc: 65.62%] [G loss: 1.922885]\n",
      "epoch:24 step:22683 [D loss: 0.661191, acc: 57.03%] [G loss: 1.977751]\n",
      "epoch:24 step:22684 [D loss: 0.703428, acc: 60.94%] [G loss: 1.751270]\n",
      "epoch:24 step:22685 [D loss: 0.692762, acc: 62.50%] [G loss: 1.957414]\n",
      "epoch:24 step:22686 [D loss: 0.605356, acc: 64.84%] [G loss: 1.891378]\n",
      "epoch:24 step:22687 [D loss: 0.649479, acc: 60.16%] [G loss: 1.837336]\n",
      "epoch:24 step:22688 [D loss: 0.676366, acc: 57.81%] [G loss: 1.737562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22689 [D loss: 0.640927, acc: 59.38%] [G loss: 1.775620]\n",
      "epoch:24 step:22690 [D loss: 0.644406, acc: 66.41%] [G loss: 1.932511]\n",
      "epoch:24 step:22691 [D loss: 0.710030, acc: 53.12%] [G loss: 1.873061]\n",
      "epoch:24 step:22692 [D loss: 0.666240, acc: 60.94%] [G loss: 1.848885]\n",
      "epoch:24 step:22693 [D loss: 0.687782, acc: 50.78%] [G loss: 1.792674]\n",
      "epoch:24 step:22694 [D loss: 0.614638, acc: 73.44%] [G loss: 1.900671]\n",
      "epoch:24 step:22695 [D loss: 0.608209, acc: 69.53%] [G loss: 2.172712]\n",
      "epoch:24 step:22696 [D loss: 0.638291, acc: 63.28%] [G loss: 2.020540]\n",
      "epoch:24 step:22697 [D loss: 0.642615, acc: 60.16%] [G loss: 1.849784]\n",
      "epoch:24 step:22698 [D loss: 0.666659, acc: 64.06%] [G loss: 1.881460]\n",
      "epoch:24 step:22699 [D loss: 0.692584, acc: 57.81%] [G loss: 1.755771]\n",
      "epoch:24 step:22700 [D loss: 0.644459, acc: 60.16%] [G loss: 1.805342]\n",
      "epoch:24 step:22701 [D loss: 0.634533, acc: 62.50%] [G loss: 1.704934]\n",
      "epoch:24 step:22702 [D loss: 0.654276, acc: 56.25%] [G loss: 1.786642]\n",
      "epoch:24 step:22703 [D loss: 0.665240, acc: 55.47%] [G loss: 1.762648]\n",
      "epoch:24 step:22704 [D loss: 0.635983, acc: 59.38%] [G loss: 1.924174]\n",
      "epoch:24 step:22705 [D loss: 0.615280, acc: 66.41%] [G loss: 1.855026]\n",
      "epoch:24 step:22706 [D loss: 0.610149, acc: 64.06%] [G loss: 2.140880]\n",
      "epoch:24 step:22707 [D loss: 0.578312, acc: 68.75%] [G loss: 2.168857]\n",
      "epoch:24 step:22708 [D loss: 0.764903, acc: 44.53%] [G loss: 1.696117]\n",
      "epoch:24 step:22709 [D loss: 0.684245, acc: 52.34%] [G loss: 1.845522]\n",
      "epoch:24 step:22710 [D loss: 0.662929, acc: 60.94%] [G loss: 1.954508]\n",
      "epoch:24 step:22711 [D loss: 0.675814, acc: 61.72%] [G loss: 1.702080]\n",
      "epoch:24 step:22712 [D loss: 0.679026, acc: 61.72%] [G loss: 1.872091]\n",
      "epoch:24 step:22713 [D loss: 0.641170, acc: 66.41%] [G loss: 1.832677]\n",
      "epoch:24 step:22714 [D loss: 0.718040, acc: 53.91%] [G loss: 1.720777]\n",
      "epoch:24 step:22715 [D loss: 0.626023, acc: 64.06%] [G loss: 1.793825]\n",
      "epoch:24 step:22716 [D loss: 0.667087, acc: 61.72%] [G loss: 1.668709]\n",
      "epoch:24 step:22717 [D loss: 0.578177, acc: 72.66%] [G loss: 2.041457]\n",
      "epoch:24 step:22718 [D loss: 0.607442, acc: 66.41%] [G loss: 2.163723]\n",
      "epoch:24 step:22719 [D loss: 0.566800, acc: 75.78%] [G loss: 2.241268]\n",
      "epoch:24 step:22720 [D loss: 0.595780, acc: 71.88%] [G loss: 2.275015]\n",
      "epoch:24 step:22721 [D loss: 0.696759, acc: 61.72%] [G loss: 1.791767]\n",
      "epoch:24 step:22722 [D loss: 0.649364, acc: 58.59%] [G loss: 1.819601]\n",
      "epoch:24 step:22723 [D loss: 0.692402, acc: 53.12%] [G loss: 1.780571]\n",
      "epoch:24 step:22724 [D loss: 0.623630, acc: 69.53%] [G loss: 2.013653]\n",
      "epoch:24 step:22725 [D loss: 0.720612, acc: 55.47%] [G loss: 1.743464]\n",
      "epoch:24 step:22726 [D loss: 0.669638, acc: 60.94%] [G loss: 1.946463]\n",
      "epoch:24 step:22727 [D loss: 0.667927, acc: 57.03%] [G loss: 1.788677]\n",
      "epoch:24 step:22728 [D loss: 0.610163, acc: 65.62%] [G loss: 1.983424]\n",
      "epoch:24 step:22729 [D loss: 0.647541, acc: 60.16%] [G loss: 1.892159]\n",
      "epoch:24 step:22730 [D loss: 0.637892, acc: 59.38%] [G loss: 1.828702]\n",
      "epoch:24 step:22731 [D loss: 0.668739, acc: 62.50%] [G loss: 1.929964]\n",
      "epoch:24 step:22732 [D loss: 0.653434, acc: 60.94%] [G loss: 1.821347]\n",
      "epoch:24 step:22733 [D loss: 0.605657, acc: 65.62%] [G loss: 1.822021]\n",
      "epoch:24 step:22734 [D loss: 0.619997, acc: 65.62%] [G loss: 1.905096]\n",
      "epoch:24 step:22735 [D loss: 0.632277, acc: 62.50%] [G loss: 1.917796]\n",
      "epoch:24 step:22736 [D loss: 0.616826, acc: 65.62%] [G loss: 1.982351]\n",
      "epoch:24 step:22737 [D loss: 0.671852, acc: 57.03%] [G loss: 1.753607]\n",
      "epoch:24 step:22738 [D loss: 0.714575, acc: 53.91%] [G loss: 1.707929]\n",
      "epoch:24 step:22739 [D loss: 0.674256, acc: 56.25%] [G loss: 1.761391]\n",
      "epoch:24 step:22740 [D loss: 0.702050, acc: 55.47%] [G loss: 1.667341]\n",
      "epoch:24 step:22741 [D loss: 0.632862, acc: 64.84%] [G loss: 1.728136]\n",
      "epoch:24 step:22742 [D loss: 0.646491, acc: 61.72%] [G loss: 1.725192]\n",
      "epoch:24 step:22743 [D loss: 0.682448, acc: 62.50%] [G loss: 1.886703]\n",
      "epoch:24 step:22744 [D loss: 0.620624, acc: 61.72%] [G loss: 1.768848]\n",
      "epoch:24 step:22745 [D loss: 0.624175, acc: 64.06%] [G loss: 1.937972]\n",
      "epoch:24 step:22746 [D loss: 0.643056, acc: 63.28%] [G loss: 1.912833]\n",
      "epoch:24 step:22747 [D loss: 0.667155, acc: 61.72%] [G loss: 1.851666]\n",
      "epoch:24 step:22748 [D loss: 0.588542, acc: 71.09%] [G loss: 1.979923]\n",
      "epoch:24 step:22749 [D loss: 0.633956, acc: 60.94%] [G loss: 1.949183]\n",
      "epoch:24 step:22750 [D loss: 0.641006, acc: 60.16%] [G loss: 1.997587]\n",
      "epoch:24 step:22751 [D loss: 0.723949, acc: 53.91%] [G loss: 1.886833]\n",
      "epoch:24 step:22752 [D loss: 0.663765, acc: 64.06%] [G loss: 1.964879]\n",
      "epoch:24 step:22753 [D loss: 0.659632, acc: 60.16%] [G loss: 1.794122]\n",
      "epoch:24 step:22754 [D loss: 0.670030, acc: 53.91%] [G loss: 1.915133]\n",
      "epoch:24 step:22755 [D loss: 0.657940, acc: 64.84%] [G loss: 1.887425]\n",
      "epoch:24 step:22756 [D loss: 0.655182, acc: 60.94%] [G loss: 1.744342]\n",
      "epoch:24 step:22757 [D loss: 0.583415, acc: 69.53%] [G loss: 1.943765]\n",
      "epoch:24 step:22758 [D loss: 0.692511, acc: 55.47%] [G loss: 1.814393]\n",
      "epoch:24 step:22759 [D loss: 0.639628, acc: 62.50%] [G loss: 1.883537]\n",
      "epoch:24 step:22760 [D loss: 0.641188, acc: 61.72%] [G loss: 1.869499]\n",
      "epoch:24 step:22761 [D loss: 0.677347, acc: 50.78%] [G loss: 1.802535]\n",
      "epoch:24 step:22762 [D loss: 0.610544, acc: 67.19%] [G loss: 1.896259]\n",
      "epoch:24 step:22763 [D loss: 0.633500, acc: 64.06%] [G loss: 1.866680]\n",
      "epoch:24 step:22764 [D loss: 0.644277, acc: 62.50%] [G loss: 2.050828]\n",
      "epoch:24 step:22765 [D loss: 0.631214, acc: 60.94%] [G loss: 1.832213]\n",
      "epoch:24 step:22766 [D loss: 0.649648, acc: 60.16%] [G loss: 1.729402]\n",
      "epoch:24 step:22767 [D loss: 0.669794, acc: 58.59%] [G loss: 1.819071]\n",
      "epoch:24 step:22768 [D loss: 0.618892, acc: 66.41%] [G loss: 1.912227]\n",
      "epoch:24 step:22769 [D loss: 0.732845, acc: 53.91%] [G loss: 1.740542]\n",
      "epoch:24 step:22770 [D loss: 0.656246, acc: 61.72%] [G loss: 1.881548]\n",
      "epoch:24 step:22771 [D loss: 0.590505, acc: 74.22%] [G loss: 1.897924]\n",
      "epoch:24 step:22772 [D loss: 0.662875, acc: 58.59%] [G loss: 1.832469]\n",
      "epoch:24 step:22773 [D loss: 0.655912, acc: 62.50%] [G loss: 1.749584]\n",
      "epoch:24 step:22774 [D loss: 0.611290, acc: 64.06%] [G loss: 1.861012]\n",
      "epoch:24 step:22775 [D loss: 0.658314, acc: 63.28%] [G loss: 1.885359]\n",
      "epoch:24 step:22776 [D loss: 0.659397, acc: 60.94%] [G loss: 1.837261]\n",
      "epoch:24 step:22777 [D loss: 0.666412, acc: 64.84%] [G loss: 1.823364]\n",
      "epoch:24 step:22778 [D loss: 0.649890, acc: 61.72%] [G loss: 1.766419]\n",
      "epoch:24 step:22779 [D loss: 0.686002, acc: 53.91%] [G loss: 1.757222]\n",
      "epoch:24 step:22780 [D loss: 0.634850, acc: 67.97%] [G loss: 1.861548]\n",
      "epoch:24 step:22781 [D loss: 0.661460, acc: 62.50%] [G loss: 1.829809]\n",
      "epoch:24 step:22782 [D loss: 0.649370, acc: 60.94%] [G loss: 1.832822]\n",
      "epoch:24 step:22783 [D loss: 0.621317, acc: 67.97%] [G loss: 1.857441]\n",
      "epoch:24 step:22784 [D loss: 0.641776, acc: 63.28%] [G loss: 1.943479]\n",
      "epoch:24 step:22785 [D loss: 0.659662, acc: 60.16%] [G loss: 1.917622]\n",
      "epoch:24 step:22786 [D loss: 0.602093, acc: 64.84%] [G loss: 2.058791]\n",
      "epoch:24 step:22787 [D loss: 0.638023, acc: 57.03%] [G loss: 2.130381]\n",
      "epoch:24 step:22788 [D loss: 0.646488, acc: 59.38%] [G loss: 1.827085]\n",
      "epoch:24 step:22789 [D loss: 0.658887, acc: 60.94%] [G loss: 1.666294]\n",
      "epoch:24 step:22790 [D loss: 0.636659, acc: 62.50%] [G loss: 1.952939]\n",
      "epoch:24 step:22791 [D loss: 0.618985, acc: 68.75%] [G loss: 1.771256]\n",
      "epoch:24 step:22792 [D loss: 0.611473, acc: 66.41%] [G loss: 1.864365]\n",
      "epoch:24 step:22793 [D loss: 0.646320, acc: 63.28%] [G loss: 1.805298]\n",
      "epoch:24 step:22794 [D loss: 0.655192, acc: 61.72%] [G loss: 1.880669]\n",
      "epoch:24 step:22795 [D loss: 0.680621, acc: 59.38%] [G loss: 1.889798]\n",
      "epoch:24 step:22796 [D loss: 0.658064, acc: 60.94%] [G loss: 1.731771]\n",
      "epoch:24 step:22797 [D loss: 0.643224, acc: 66.41%] [G loss: 1.774517]\n",
      "epoch:24 step:22798 [D loss: 0.613207, acc: 65.62%] [G loss: 1.910218]\n",
      "epoch:24 step:22799 [D loss: 0.620555, acc: 67.19%] [G loss: 1.887919]\n",
      "epoch:24 step:22800 [D loss: 0.631098, acc: 66.41%] [G loss: 2.114755]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.538849\n",
      "FID: 10.217254\n",
      "0 = 12.691871955108656\n",
      "1 = 0.08929074137023278\n",
      "2 = 0.866599977016449\n",
      "3 = 0.8913999795913696\n",
      "4 = 0.8417999744415283\n",
      "5 = 0.8492758870124817\n",
      "6 = 0.8913999795913696\n",
      "7 = 6.062024449467666\n",
      "8 = 0.05973617725811626\n",
      "9 = 0.6952999830245972\n",
      "10 = 0.7089999914169312\n",
      "11 = 0.6815999746322632\n",
      "12 = 0.6900914907455444\n",
      "13 = 0.7089999914169312\n",
      "14 = 7.538878440856934\n",
      "15 = 9.45312213897705\n",
      "16 = 0.10684777051210403\n",
      "17 = 7.538848876953125\n",
      "18 = 10.217253684997559\n",
      "epoch:24 step:22801 [D loss: 0.606194, acc: 62.50%] [G loss: 2.043044]\n",
      "epoch:24 step:22802 [D loss: 0.623087, acc: 65.62%] [G loss: 2.054417]\n",
      "epoch:24 step:22803 [D loss: 0.584654, acc: 67.97%] [G loss: 2.097094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22804 [D loss: 0.702434, acc: 56.25%] [G loss: 1.864216]\n",
      "epoch:24 step:22805 [D loss: 0.687953, acc: 60.16%] [G loss: 1.749923]\n",
      "epoch:24 step:22806 [D loss: 0.615894, acc: 60.94%] [G loss: 2.073857]\n",
      "epoch:24 step:22807 [D loss: 0.662252, acc: 62.50%] [G loss: 1.872769]\n",
      "epoch:24 step:22808 [D loss: 0.626427, acc: 61.72%] [G loss: 1.865470]\n",
      "epoch:24 step:22809 [D loss: 0.580095, acc: 75.00%] [G loss: 1.932050]\n",
      "epoch:24 step:22810 [D loss: 0.663641, acc: 54.69%] [G loss: 1.810366]\n",
      "epoch:24 step:22811 [D loss: 0.677758, acc: 58.59%] [G loss: 1.800406]\n",
      "epoch:24 step:22812 [D loss: 0.687177, acc: 58.59%] [G loss: 1.837316]\n",
      "epoch:24 step:22813 [D loss: 0.670539, acc: 53.91%] [G loss: 1.764416]\n",
      "epoch:24 step:22814 [D loss: 0.632309, acc: 64.06%] [G loss: 1.785776]\n",
      "epoch:24 step:22815 [D loss: 0.688875, acc: 54.69%] [G loss: 1.881955]\n",
      "epoch:24 step:22816 [D loss: 0.638035, acc: 60.16%] [G loss: 1.951943]\n",
      "epoch:24 step:22817 [D loss: 0.641567, acc: 63.28%] [G loss: 1.879330]\n",
      "epoch:24 step:22818 [D loss: 0.648194, acc: 60.94%] [G loss: 2.061558]\n",
      "epoch:24 step:22819 [D loss: 0.619650, acc: 65.62%] [G loss: 2.075752]\n",
      "epoch:24 step:22820 [D loss: 0.637013, acc: 61.72%] [G loss: 1.916937]\n",
      "epoch:24 step:22821 [D loss: 0.656751, acc: 59.38%] [G loss: 1.941265]\n",
      "epoch:24 step:22822 [D loss: 0.647023, acc: 63.28%] [G loss: 1.983797]\n",
      "epoch:24 step:22823 [D loss: 0.638735, acc: 61.72%] [G loss: 1.890691]\n",
      "epoch:24 step:22824 [D loss: 0.641752, acc: 60.16%] [G loss: 1.936044]\n",
      "epoch:24 step:22825 [D loss: 0.633115, acc: 68.75%] [G loss: 2.042107]\n",
      "epoch:24 step:22826 [D loss: 0.683476, acc: 59.38%] [G loss: 1.809571]\n",
      "epoch:24 step:22827 [D loss: 0.649222, acc: 59.38%] [G loss: 1.932577]\n",
      "epoch:24 step:22828 [D loss: 0.619019, acc: 71.09%] [G loss: 1.987291]\n",
      "epoch:24 step:22829 [D loss: 0.676481, acc: 57.81%] [G loss: 1.791566]\n",
      "epoch:24 step:22830 [D loss: 0.696148, acc: 52.34%] [G loss: 1.785119]\n",
      "epoch:24 step:22831 [D loss: 0.662907, acc: 60.94%] [G loss: 1.778035]\n",
      "epoch:24 step:22832 [D loss: 0.681169, acc: 60.94%] [G loss: 1.802551]\n",
      "epoch:24 step:22833 [D loss: 0.606950, acc: 66.41%] [G loss: 1.985290]\n",
      "epoch:24 step:22834 [D loss: 0.600417, acc: 71.09%] [G loss: 2.064844]\n",
      "epoch:24 step:22835 [D loss: 0.546283, acc: 75.00%] [G loss: 2.268772]\n",
      "epoch:24 step:22836 [D loss: 0.695467, acc: 57.03%] [G loss: 1.885708]\n",
      "epoch:24 step:22837 [D loss: 0.740388, acc: 50.00%] [G loss: 1.692274]\n",
      "epoch:24 step:22838 [D loss: 0.647750, acc: 62.50%] [G loss: 1.816082]\n",
      "epoch:24 step:22839 [D loss: 0.659273, acc: 70.31%] [G loss: 1.830804]\n",
      "epoch:24 step:22840 [D loss: 0.674043, acc: 60.16%] [G loss: 1.812658]\n",
      "epoch:24 step:22841 [D loss: 0.695916, acc: 56.25%] [G loss: 1.780154]\n",
      "epoch:24 step:22842 [D loss: 0.637668, acc: 67.97%] [G loss: 2.016901]\n",
      "epoch:24 step:22843 [D loss: 0.626649, acc: 59.38%] [G loss: 1.721459]\n",
      "epoch:24 step:22844 [D loss: 0.731591, acc: 53.91%] [G loss: 1.688081]\n",
      "epoch:24 step:22845 [D loss: 0.680440, acc: 52.34%] [G loss: 1.782169]\n",
      "epoch:24 step:22846 [D loss: 0.584484, acc: 68.75%] [G loss: 1.792816]\n",
      "epoch:24 step:22847 [D loss: 0.629333, acc: 66.41%] [G loss: 1.809899]\n",
      "epoch:24 step:22848 [D loss: 0.615433, acc: 68.75%] [G loss: 1.842516]\n",
      "epoch:24 step:22849 [D loss: 0.638953, acc: 64.06%] [G loss: 1.846561]\n",
      "epoch:24 step:22850 [D loss: 0.613575, acc: 65.62%] [G loss: 1.909612]\n",
      "epoch:24 step:22851 [D loss: 0.664882, acc: 54.69%] [G loss: 1.897590]\n",
      "epoch:24 step:22852 [D loss: 0.634110, acc: 67.97%] [G loss: 1.872398]\n",
      "epoch:24 step:22853 [D loss: 0.657838, acc: 61.72%] [G loss: 1.977006]\n",
      "epoch:24 step:22854 [D loss: 0.622427, acc: 67.97%] [G loss: 1.923805]\n",
      "epoch:24 step:22855 [D loss: 0.629905, acc: 61.72%] [G loss: 1.842202]\n",
      "epoch:24 step:22856 [D loss: 0.649578, acc: 59.38%] [G loss: 1.779293]\n",
      "epoch:24 step:22857 [D loss: 0.641712, acc: 64.84%] [G loss: 1.908607]\n",
      "epoch:24 step:22858 [D loss: 0.617262, acc: 61.72%] [G loss: 1.905147]\n",
      "epoch:24 step:22859 [D loss: 0.627021, acc: 70.31%] [G loss: 1.989295]\n",
      "epoch:24 step:22860 [D loss: 0.659490, acc: 62.50%] [G loss: 1.738971]\n",
      "epoch:24 step:22861 [D loss: 0.717205, acc: 55.47%] [G loss: 1.636001]\n",
      "epoch:24 step:22862 [D loss: 0.603892, acc: 68.75%] [G loss: 1.789592]\n",
      "epoch:24 step:22863 [D loss: 0.665607, acc: 56.25%] [G loss: 1.868167]\n",
      "epoch:24 step:22864 [D loss: 0.700041, acc: 51.56%] [G loss: 1.883337]\n",
      "epoch:24 step:22865 [D loss: 0.710317, acc: 56.25%] [G loss: 1.647110]\n",
      "epoch:24 step:22866 [D loss: 0.655038, acc: 60.16%] [G loss: 1.719612]\n",
      "epoch:24 step:22867 [D loss: 0.712187, acc: 50.78%] [G loss: 1.828003]\n",
      "epoch:24 step:22868 [D loss: 0.594170, acc: 65.62%] [G loss: 1.996034]\n",
      "epoch:24 step:22869 [D loss: 0.604968, acc: 63.28%] [G loss: 1.904801]\n",
      "epoch:24 step:22870 [D loss: 0.674729, acc: 59.38%] [G loss: 1.750985]\n",
      "epoch:24 step:22871 [D loss: 0.575803, acc: 73.44%] [G loss: 1.815853]\n",
      "epoch:24 step:22872 [D loss: 0.604672, acc: 65.62%] [G loss: 1.871163]\n",
      "epoch:24 step:22873 [D loss: 0.607594, acc: 67.97%] [G loss: 1.768229]\n",
      "epoch:24 step:22874 [D loss: 0.684977, acc: 56.25%] [G loss: 1.828676]\n",
      "epoch:24 step:22875 [D loss: 0.633881, acc: 62.50%] [G loss: 1.841387]\n",
      "epoch:24 step:22876 [D loss: 0.638619, acc: 66.41%] [G loss: 1.869779]\n",
      "epoch:24 step:22877 [D loss: 0.618414, acc: 63.28%] [G loss: 1.895024]\n",
      "epoch:24 step:22878 [D loss: 0.658329, acc: 57.03%] [G loss: 1.909569]\n",
      "epoch:24 step:22879 [D loss: 0.678297, acc: 57.81%] [G loss: 1.824288]\n",
      "epoch:24 step:22880 [D loss: 0.677014, acc: 60.16%] [G loss: 1.876619]\n",
      "epoch:24 step:22881 [D loss: 0.645475, acc: 62.50%] [G loss: 1.812377]\n",
      "epoch:24 step:22882 [D loss: 0.634866, acc: 64.06%] [G loss: 1.816040]\n",
      "epoch:24 step:22883 [D loss: 0.663616, acc: 58.59%] [G loss: 1.853155]\n",
      "epoch:24 step:22884 [D loss: 0.662039, acc: 56.25%] [G loss: 1.761648]\n",
      "epoch:24 step:22885 [D loss: 0.664001, acc: 61.72%] [G loss: 1.706707]\n",
      "epoch:24 step:22886 [D loss: 0.635647, acc: 60.16%] [G loss: 1.810362]\n",
      "epoch:24 step:22887 [D loss: 0.669496, acc: 55.47%] [G loss: 1.767288]\n",
      "epoch:24 step:22888 [D loss: 0.671515, acc: 55.47%] [G loss: 1.759842]\n",
      "epoch:24 step:22889 [D loss: 0.672557, acc: 61.72%] [G loss: 1.767511]\n",
      "epoch:24 step:22890 [D loss: 0.650931, acc: 64.06%] [G loss: 1.795893]\n",
      "epoch:24 step:22891 [D loss: 0.682286, acc: 53.12%] [G loss: 1.728322]\n",
      "epoch:24 step:22892 [D loss: 0.589472, acc: 67.19%] [G loss: 1.944476]\n",
      "epoch:24 step:22893 [D loss: 0.602155, acc: 67.97%] [G loss: 1.978765]\n",
      "epoch:24 step:22894 [D loss: 0.668808, acc: 57.03%] [G loss: 1.965188]\n",
      "epoch:24 step:22895 [D loss: 0.637535, acc: 62.50%] [G loss: 1.864779]\n",
      "epoch:24 step:22896 [D loss: 0.676790, acc: 53.12%] [G loss: 1.767264]\n",
      "epoch:24 step:22897 [D loss: 0.721198, acc: 50.00%] [G loss: 1.870908]\n",
      "epoch:24 step:22898 [D loss: 0.659042, acc: 51.56%] [G loss: 1.794505]\n",
      "epoch:24 step:22899 [D loss: 0.672192, acc: 57.81%] [G loss: 1.794596]\n",
      "epoch:24 step:22900 [D loss: 0.603829, acc: 69.53%] [G loss: 1.868359]\n",
      "epoch:24 step:22901 [D loss: 0.703477, acc: 59.38%] [G loss: 1.855724]\n",
      "epoch:24 step:22902 [D loss: 0.611675, acc: 66.41%] [G loss: 1.853461]\n",
      "epoch:24 step:22903 [D loss: 0.640808, acc: 58.59%] [G loss: 1.977346]\n",
      "epoch:24 step:22904 [D loss: 0.576187, acc: 72.66%] [G loss: 2.113969]\n",
      "epoch:24 step:22905 [D loss: 0.610715, acc: 63.28%] [G loss: 2.066197]\n",
      "epoch:24 step:22906 [D loss: 0.709778, acc: 60.94%] [G loss: 1.834652]\n",
      "epoch:24 step:22907 [D loss: 0.655512, acc: 60.16%] [G loss: 1.860516]\n",
      "epoch:24 step:22908 [D loss: 0.646960, acc: 63.28%] [G loss: 1.873360]\n",
      "epoch:24 step:22909 [D loss: 0.675620, acc: 58.59%] [G loss: 1.813813]\n",
      "epoch:24 step:22910 [D loss: 0.643362, acc: 60.94%] [G loss: 1.786247]\n",
      "epoch:24 step:22911 [D loss: 0.653277, acc: 61.72%] [G loss: 1.764734]\n",
      "epoch:24 step:22912 [D loss: 0.658680, acc: 63.28%] [G loss: 1.772553]\n",
      "epoch:24 step:22913 [D loss: 0.660781, acc: 60.16%] [G loss: 1.814344]\n",
      "epoch:24 step:22914 [D loss: 0.628044, acc: 64.06%] [G loss: 1.991730]\n",
      "epoch:24 step:22915 [D loss: 0.665064, acc: 58.59%] [G loss: 2.001226]\n",
      "epoch:24 step:22916 [D loss: 0.564839, acc: 69.53%] [G loss: 2.034114]\n",
      "epoch:24 step:22917 [D loss: 0.627913, acc: 60.16%] [G loss: 1.968879]\n",
      "epoch:24 step:22918 [D loss: 0.569915, acc: 70.31%] [G loss: 2.068237]\n",
      "epoch:24 step:22919 [D loss: 0.605998, acc: 73.44%] [G loss: 2.092062]\n",
      "epoch:24 step:22920 [D loss: 0.690436, acc: 64.06%] [G loss: 1.873313]\n",
      "epoch:24 step:22921 [D loss: 0.622969, acc: 67.19%] [G loss: 1.926683]\n",
      "epoch:24 step:22922 [D loss: 0.590448, acc: 70.31%] [G loss: 1.981364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22923 [D loss: 0.619061, acc: 66.41%] [G loss: 2.003965]\n",
      "epoch:24 step:22924 [D loss: 0.635516, acc: 66.41%] [G loss: 2.011419]\n",
      "epoch:24 step:22925 [D loss: 0.769013, acc: 44.53%] [G loss: 1.778492]\n",
      "epoch:24 step:22926 [D loss: 0.700554, acc: 54.69%] [G loss: 1.737712]\n",
      "epoch:24 step:22927 [D loss: 0.638273, acc: 61.72%] [G loss: 1.896216]\n",
      "epoch:24 step:22928 [D loss: 0.695394, acc: 56.25%] [G loss: 1.835227]\n",
      "epoch:24 step:22929 [D loss: 0.717277, acc: 55.47%] [G loss: 1.716183]\n",
      "epoch:24 step:22930 [D loss: 0.639163, acc: 69.53%] [G loss: 1.927074]\n",
      "epoch:24 step:22931 [D loss: 0.647686, acc: 61.72%] [G loss: 1.827916]\n",
      "epoch:24 step:22932 [D loss: 0.627783, acc: 66.41%] [G loss: 1.901774]\n",
      "epoch:24 step:22933 [D loss: 0.691772, acc: 62.50%] [G loss: 1.790890]\n",
      "epoch:24 step:22934 [D loss: 0.639686, acc: 59.38%] [G loss: 1.929826]\n",
      "epoch:24 step:22935 [D loss: 0.664120, acc: 54.69%] [G loss: 1.769263]\n",
      "epoch:24 step:22936 [D loss: 0.716059, acc: 50.00%] [G loss: 1.762514]\n",
      "epoch:24 step:22937 [D loss: 0.642335, acc: 64.84%] [G loss: 1.904792]\n",
      "epoch:24 step:22938 [D loss: 0.634989, acc: 62.50%] [G loss: 1.929233]\n",
      "epoch:24 step:22939 [D loss: 0.681060, acc: 61.72%] [G loss: 1.958701]\n",
      "epoch:24 step:22940 [D loss: 0.656258, acc: 58.59%] [G loss: 1.832164]\n",
      "epoch:24 step:22941 [D loss: 0.628852, acc: 65.62%] [G loss: 1.892980]\n",
      "epoch:24 step:22942 [D loss: 0.638335, acc: 61.72%] [G loss: 1.719528]\n",
      "epoch:24 step:22943 [D loss: 0.653192, acc: 55.47%] [G loss: 1.866248]\n",
      "epoch:24 step:22944 [D loss: 0.661387, acc: 58.59%] [G loss: 1.897865]\n",
      "epoch:24 step:22945 [D loss: 0.583065, acc: 70.31%] [G loss: 1.832152]\n",
      "epoch:24 step:22946 [D loss: 0.668068, acc: 57.03%] [G loss: 1.728750]\n",
      "epoch:24 step:22947 [D loss: 0.674705, acc: 58.59%] [G loss: 1.842127]\n",
      "epoch:24 step:22948 [D loss: 0.673759, acc: 56.25%] [G loss: 1.731738]\n",
      "epoch:24 step:22949 [D loss: 0.665465, acc: 58.59%] [G loss: 1.833274]\n",
      "epoch:24 step:22950 [D loss: 0.672227, acc: 58.59%] [G loss: 1.788448]\n",
      "epoch:24 step:22951 [D loss: 0.633429, acc: 65.62%] [G loss: 1.777277]\n",
      "epoch:24 step:22952 [D loss: 0.659604, acc: 63.28%] [G loss: 1.894054]\n",
      "epoch:24 step:22953 [D loss: 0.644187, acc: 62.50%] [G loss: 1.782829]\n",
      "epoch:24 step:22954 [D loss: 0.644347, acc: 61.72%] [G loss: 1.917604]\n",
      "epoch:24 step:22955 [D loss: 0.671806, acc: 60.94%] [G loss: 1.825031]\n",
      "epoch:24 step:22956 [D loss: 0.682165, acc: 58.59%] [G loss: 1.946810]\n",
      "epoch:24 step:22957 [D loss: 0.638642, acc: 55.47%] [G loss: 1.955394]\n",
      "epoch:24 step:22958 [D loss: 0.702878, acc: 58.59%] [G loss: 1.959891]\n",
      "epoch:24 step:22959 [D loss: 0.688881, acc: 61.72%] [G loss: 1.984502]\n",
      "epoch:24 step:22960 [D loss: 0.679330, acc: 62.50%] [G loss: 1.861629]\n",
      "epoch:24 step:22961 [D loss: 0.644647, acc: 64.84%] [G loss: 1.712996]\n",
      "epoch:24 step:22962 [D loss: 0.682949, acc: 57.03%] [G loss: 1.851884]\n",
      "epoch:24 step:22963 [D loss: 0.654752, acc: 61.72%] [G loss: 1.836534]\n",
      "epoch:24 step:22964 [D loss: 0.619472, acc: 67.19%] [G loss: 1.987027]\n",
      "epoch:24 step:22965 [D loss: 0.696545, acc: 60.94%] [G loss: 1.783673]\n",
      "epoch:24 step:22966 [D loss: 0.647413, acc: 60.16%] [G loss: 1.792299]\n",
      "epoch:24 step:22967 [D loss: 0.650430, acc: 64.06%] [G loss: 1.817124]\n",
      "epoch:24 step:22968 [D loss: 0.647507, acc: 63.28%] [G loss: 1.859143]\n",
      "epoch:24 step:22969 [D loss: 0.573744, acc: 69.53%] [G loss: 2.051841]\n",
      "epoch:24 step:22970 [D loss: 0.655779, acc: 60.94%] [G loss: 1.766968]\n",
      "epoch:24 step:22971 [D loss: 0.677043, acc: 55.47%] [G loss: 1.811040]\n",
      "epoch:24 step:22972 [D loss: 0.633454, acc: 67.19%] [G loss: 1.963842]\n",
      "epoch:24 step:22973 [D loss: 0.647379, acc: 64.84%] [G loss: 1.743375]\n",
      "epoch:24 step:22974 [D loss: 0.650369, acc: 57.81%] [G loss: 1.781936]\n",
      "epoch:24 step:22975 [D loss: 0.665149, acc: 59.38%] [G loss: 1.888074]\n",
      "epoch:24 step:22976 [D loss: 0.584769, acc: 69.53%] [G loss: 2.025826]\n",
      "epoch:24 step:22977 [D loss: 0.647634, acc: 59.38%] [G loss: 1.872419]\n",
      "epoch:24 step:22978 [D loss: 0.692682, acc: 55.47%] [G loss: 1.810489]\n",
      "epoch:24 step:22979 [D loss: 0.623723, acc: 65.62%] [G loss: 1.975889]\n",
      "epoch:24 step:22980 [D loss: 0.654886, acc: 64.06%] [G loss: 1.838964]\n",
      "epoch:24 step:22981 [D loss: 0.605279, acc: 71.88%] [G loss: 2.011348]\n",
      "epoch:24 step:22982 [D loss: 0.602609, acc: 70.31%] [G loss: 1.801648]\n",
      "epoch:24 step:22983 [D loss: 0.650590, acc: 67.97%] [G loss: 1.965707]\n",
      "epoch:24 step:22984 [D loss: 0.623602, acc: 66.41%] [G loss: 1.992701]\n",
      "epoch:24 step:22985 [D loss: 0.632518, acc: 62.50%] [G loss: 1.990822]\n",
      "epoch:24 step:22986 [D loss: 0.618518, acc: 57.81%] [G loss: 2.051142]\n",
      "epoch:24 step:22987 [D loss: 0.637287, acc: 58.59%] [G loss: 2.064003]\n",
      "epoch:24 step:22988 [D loss: 0.667505, acc: 58.59%] [G loss: 1.965907]\n",
      "epoch:24 step:22989 [D loss: 0.726052, acc: 53.91%] [G loss: 1.741483]\n",
      "epoch:24 step:22990 [D loss: 0.703335, acc: 51.56%] [G loss: 1.768123]\n",
      "epoch:24 step:22991 [D loss: 0.695060, acc: 53.91%] [G loss: 1.862384]\n",
      "epoch:24 step:22992 [D loss: 0.642707, acc: 62.50%] [G loss: 1.919148]\n",
      "epoch:24 step:22993 [D loss: 0.616254, acc: 67.19%] [G loss: 1.919747]\n",
      "epoch:24 step:22994 [D loss: 0.677061, acc: 59.38%] [G loss: 1.728183]\n",
      "epoch:24 step:22995 [D loss: 0.676945, acc: 57.81%] [G loss: 1.836107]\n",
      "epoch:24 step:22996 [D loss: 0.591844, acc: 65.62%] [G loss: 1.976009]\n",
      "epoch:24 step:22997 [D loss: 0.657405, acc: 63.28%] [G loss: 1.876827]\n",
      "epoch:24 step:22998 [D loss: 0.680625, acc: 57.81%] [G loss: 1.870596]\n",
      "epoch:24 step:22999 [D loss: 0.699289, acc: 51.56%] [G loss: 1.866426]\n",
      "epoch:24 step:23000 [D loss: 0.646392, acc: 62.50%] [G loss: 1.800096]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.474298\n",
      "FID: 12.124943\n",
      "0 = 12.711860287570941\n",
      "1 = 0.08984706706682336\n",
      "2 = 0.8740000128746033\n",
      "3 = 0.8998000025749207\n",
      "4 = 0.8482000231742859\n",
      "5 = 0.8556485176086426\n",
      "6 = 0.8998000025749207\n",
      "7 = 6.221549665975567\n",
      "8 = 0.06648070705170948\n",
      "9 = 0.707099974155426\n",
      "10 = 0.7156000137329102\n",
      "11 = 0.6985999941825867\n",
      "12 = 0.7036381363868713\n",
      "13 = 0.7156000137329102\n",
      "14 = 7.474332809448242\n",
      "15 = 9.401988983154297\n",
      "16 = 0.11868997663259506\n",
      "17 = 7.474298477172852\n",
      "18 = 12.124942779541016\n",
      "epoch:24 step:23001 [D loss: 0.603593, acc: 67.19%] [G loss: 1.857193]\n",
      "epoch:24 step:23002 [D loss: 0.657645, acc: 60.16%] [G loss: 1.897810]\n",
      "epoch:24 step:23003 [D loss: 0.616829, acc: 68.75%] [G loss: 1.850387]\n",
      "epoch:24 step:23004 [D loss: 0.572353, acc: 70.31%] [G loss: 1.921200]\n",
      "epoch:24 step:23005 [D loss: 0.697919, acc: 54.69%] [G loss: 1.914797]\n",
      "epoch:24 step:23006 [D loss: 0.674188, acc: 57.81%] [G loss: 1.871335]\n",
      "epoch:24 step:23007 [D loss: 0.592670, acc: 64.84%] [G loss: 1.958577]\n",
      "epoch:24 step:23008 [D loss: 0.597216, acc: 70.31%] [G loss: 2.043273]\n",
      "epoch:24 step:23009 [D loss: 0.618022, acc: 64.84%] [G loss: 1.934665]\n",
      "epoch:24 step:23010 [D loss: 0.646619, acc: 65.62%] [G loss: 2.012414]\n",
      "epoch:24 step:23011 [D loss: 0.603875, acc: 64.06%] [G loss: 2.029356]\n",
      "epoch:24 step:23012 [D loss: 0.665060, acc: 68.75%] [G loss: 1.890131]\n",
      "epoch:24 step:23013 [D loss: 0.626388, acc: 69.53%] [G loss: 1.804581]\n",
      "epoch:24 step:23014 [D loss: 0.629859, acc: 67.19%] [G loss: 2.014246]\n",
      "epoch:24 step:23015 [D loss: 0.618967, acc: 65.62%] [G loss: 1.869937]\n",
      "epoch:24 step:23016 [D loss: 0.696309, acc: 56.25%] [G loss: 1.685987]\n",
      "epoch:24 step:23017 [D loss: 0.671315, acc: 56.25%] [G loss: 1.839606]\n",
      "epoch:24 step:23018 [D loss: 0.699676, acc: 54.69%] [G loss: 1.759429]\n",
      "epoch:24 step:23019 [D loss: 0.657837, acc: 60.16%] [G loss: 1.772822]\n",
      "epoch:24 step:23020 [D loss: 0.637139, acc: 64.06%] [G loss: 1.867514]\n",
      "epoch:24 step:23021 [D loss: 0.639906, acc: 60.16%] [G loss: 1.843113]\n",
      "epoch:24 step:23022 [D loss: 0.585549, acc: 69.53%] [G loss: 1.904695]\n",
      "epoch:24 step:23023 [D loss: 0.633365, acc: 66.41%] [G loss: 1.857829]\n",
      "epoch:24 step:23024 [D loss: 0.656126, acc: 62.50%] [G loss: 1.964392]\n",
      "epoch:24 step:23025 [D loss: 0.634589, acc: 63.28%] [G loss: 1.917448]\n",
      "epoch:24 step:23026 [D loss: 0.702280, acc: 50.78%] [G loss: 1.744634]\n",
      "epoch:24 step:23027 [D loss: 0.639340, acc: 60.94%] [G loss: 1.825719]\n",
      "epoch:24 step:23028 [D loss: 0.676571, acc: 60.94%] [G loss: 1.747188]\n",
      "epoch:24 step:23029 [D loss: 0.698466, acc: 57.03%] [G loss: 1.786500]\n",
      "epoch:24 step:23030 [D loss: 0.674616, acc: 57.81%] [G loss: 1.742532]\n",
      "epoch:24 step:23031 [D loss: 0.650548, acc: 63.28%] [G loss: 1.730959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23032 [D loss: 0.634697, acc: 60.94%] [G loss: 1.809644]\n",
      "epoch:24 step:23033 [D loss: 0.631427, acc: 65.62%] [G loss: 1.908761]\n",
      "epoch:24 step:23034 [D loss: 0.699647, acc: 58.59%] [G loss: 1.917408]\n",
      "epoch:24 step:23035 [D loss: 0.635194, acc: 62.50%] [G loss: 1.787409]\n",
      "epoch:24 step:23036 [D loss: 0.655191, acc: 61.72%] [G loss: 1.968944]\n",
      "epoch:24 step:23037 [D loss: 0.620943, acc: 67.97%] [G loss: 2.042910]\n",
      "epoch:24 step:23038 [D loss: 0.638192, acc: 64.84%] [G loss: 2.086551]\n",
      "epoch:24 step:23039 [D loss: 0.595227, acc: 69.53%] [G loss: 2.006124]\n",
      "epoch:24 step:23040 [D loss: 0.604999, acc: 63.28%] [G loss: 2.178490]\n",
      "epoch:24 step:23041 [D loss: 0.655223, acc: 63.28%] [G loss: 1.918905]\n",
      "epoch:24 step:23042 [D loss: 0.618950, acc: 64.84%] [G loss: 1.907909]\n",
      "epoch:24 step:23043 [D loss: 0.623120, acc: 64.84%] [G loss: 1.970890]\n",
      "epoch:24 step:23044 [D loss: 0.625934, acc: 67.19%] [G loss: 1.978878]\n",
      "epoch:24 step:23045 [D loss: 0.619075, acc: 62.50%] [G loss: 1.935800]\n",
      "epoch:24 step:23046 [D loss: 0.602596, acc: 66.41%] [G loss: 1.955356]\n",
      "epoch:24 step:23047 [D loss: 0.718359, acc: 50.78%] [G loss: 1.715957]\n",
      "epoch:24 step:23048 [D loss: 0.670997, acc: 57.81%] [G loss: 1.893430]\n",
      "epoch:24 step:23049 [D loss: 0.622020, acc: 68.75%] [G loss: 1.821961]\n",
      "epoch:24 step:23050 [D loss: 0.657645, acc: 61.72%] [G loss: 2.001737]\n",
      "epoch:24 step:23051 [D loss: 0.636426, acc: 68.75%] [G loss: 1.862722]\n",
      "epoch:24 step:23052 [D loss: 0.605195, acc: 68.75%] [G loss: 1.919874]\n",
      "epoch:24 step:23053 [D loss: 0.699137, acc: 53.12%] [G loss: 1.840284]\n",
      "epoch:24 step:23054 [D loss: 0.748442, acc: 46.88%] [G loss: 1.762317]\n",
      "epoch:24 step:23055 [D loss: 0.680180, acc: 53.91%] [G loss: 1.771601]\n",
      "epoch:24 step:23056 [D loss: 0.677459, acc: 60.94%] [G loss: 1.814821]\n",
      "epoch:24 step:23057 [D loss: 0.667905, acc: 58.59%] [G loss: 1.717764]\n",
      "epoch:24 step:23058 [D loss: 0.639524, acc: 62.50%] [G loss: 1.854863]\n",
      "epoch:24 step:23059 [D loss: 0.644863, acc: 64.06%] [G loss: 1.963742]\n",
      "epoch:24 step:23060 [D loss: 0.670705, acc: 61.72%] [G loss: 1.760279]\n",
      "epoch:24 step:23061 [D loss: 0.678733, acc: 57.03%] [G loss: 1.836433]\n",
      "epoch:24 step:23062 [D loss: 0.647696, acc: 60.16%] [G loss: 1.715303]\n",
      "epoch:24 step:23063 [D loss: 0.648722, acc: 59.38%] [G loss: 1.885743]\n",
      "epoch:24 step:23064 [D loss: 0.651453, acc: 56.25%] [G loss: 1.822001]\n",
      "epoch:24 step:23065 [D loss: 0.650032, acc: 64.84%] [G loss: 1.726682]\n",
      "epoch:24 step:23066 [D loss: 0.646832, acc: 59.38%] [G loss: 1.772096]\n",
      "epoch:24 step:23067 [D loss: 0.672908, acc: 66.41%] [G loss: 1.704381]\n",
      "epoch:24 step:23068 [D loss: 0.683831, acc: 53.91%] [G loss: 1.788220]\n",
      "epoch:24 step:23069 [D loss: 0.613008, acc: 64.06%] [G loss: 1.785468]\n",
      "epoch:24 step:23070 [D loss: 0.660703, acc: 64.84%] [G loss: 1.951318]\n",
      "epoch:24 step:23071 [D loss: 0.659652, acc: 62.50%] [G loss: 1.787841]\n",
      "epoch:24 step:23072 [D loss: 0.670401, acc: 57.03%] [G loss: 1.773064]\n",
      "epoch:24 step:23073 [D loss: 0.652237, acc: 61.72%] [G loss: 1.942286]\n",
      "epoch:24 step:23074 [D loss: 0.638994, acc: 63.28%] [G loss: 1.650108]\n",
      "epoch:24 step:23075 [D loss: 0.664335, acc: 59.38%] [G loss: 1.933378]\n",
      "epoch:24 step:23076 [D loss: 0.624270, acc: 65.62%] [G loss: 2.129399]\n",
      "epoch:24 step:23077 [D loss: 0.638210, acc: 64.84%] [G loss: 1.970139]\n",
      "epoch:24 step:23078 [D loss: 0.669437, acc: 60.94%] [G loss: 1.911073]\n",
      "epoch:24 step:23079 [D loss: 0.618738, acc: 71.09%] [G loss: 1.960768]\n",
      "epoch:24 step:23080 [D loss: 0.651876, acc: 62.50%] [G loss: 1.818656]\n",
      "epoch:24 step:23081 [D loss: 0.662227, acc: 56.25%] [G loss: 1.756399]\n",
      "epoch:24 step:23082 [D loss: 0.673134, acc: 57.03%] [G loss: 1.791685]\n",
      "epoch:24 step:23083 [D loss: 0.634615, acc: 65.62%] [G loss: 1.866600]\n",
      "epoch:24 step:23084 [D loss: 0.628102, acc: 63.28%] [G loss: 1.865767]\n",
      "epoch:24 step:23085 [D loss: 0.644127, acc: 64.84%] [G loss: 1.798748]\n",
      "epoch:24 step:23086 [D loss: 0.639976, acc: 67.97%] [G loss: 1.936036]\n",
      "epoch:24 step:23087 [D loss: 0.663841, acc: 53.12%] [G loss: 1.897165]\n",
      "epoch:24 step:23088 [D loss: 0.654201, acc: 57.03%] [G loss: 1.945970]\n",
      "epoch:24 step:23089 [D loss: 0.637112, acc: 60.94%] [G loss: 1.825987]\n",
      "epoch:24 step:23090 [D loss: 0.691876, acc: 57.03%] [G loss: 1.821394]\n",
      "epoch:24 step:23091 [D loss: 0.663822, acc: 58.59%] [G loss: 1.853841]\n",
      "epoch:24 step:23092 [D loss: 0.622948, acc: 62.50%] [G loss: 2.041338]\n",
      "epoch:24 step:23093 [D loss: 0.672740, acc: 64.84%] [G loss: 2.001334]\n",
      "epoch:24 step:23094 [D loss: 0.660266, acc: 61.72%] [G loss: 1.731229]\n",
      "epoch:24 step:23095 [D loss: 0.638649, acc: 63.28%] [G loss: 1.928558]\n",
      "epoch:24 step:23096 [D loss: 0.690229, acc: 57.81%] [G loss: 1.932740]\n",
      "epoch:24 step:23097 [D loss: 0.639437, acc: 57.03%] [G loss: 1.802817]\n",
      "epoch:24 step:23098 [D loss: 0.641532, acc: 67.97%] [G loss: 1.848412]\n",
      "epoch:24 step:23099 [D loss: 0.661179, acc: 60.94%] [G loss: 1.794869]\n",
      "epoch:24 step:23100 [D loss: 0.663590, acc: 60.94%] [G loss: 1.731491]\n",
      "epoch:24 step:23101 [D loss: 0.611517, acc: 69.53%] [G loss: 1.729662]\n",
      "epoch:24 step:23102 [D loss: 0.699591, acc: 60.16%] [G loss: 1.707202]\n",
      "epoch:24 step:23103 [D loss: 0.689180, acc: 54.69%] [G loss: 1.778784]\n",
      "epoch:24 step:23104 [D loss: 0.659712, acc: 62.50%] [G loss: 1.910665]\n",
      "epoch:24 step:23105 [D loss: 0.681266, acc: 59.38%] [G loss: 1.776114]\n",
      "epoch:24 step:23106 [D loss: 0.645696, acc: 64.84%] [G loss: 1.759079]\n",
      "epoch:24 step:23107 [D loss: 0.606198, acc: 63.28%] [G loss: 1.873514]\n",
      "epoch:24 step:23108 [D loss: 0.640627, acc: 60.94%] [G loss: 1.806893]\n",
      "epoch:24 step:23109 [D loss: 0.622238, acc: 59.38%] [G loss: 1.729105]\n",
      "epoch:24 step:23110 [D loss: 0.685677, acc: 57.03%] [G loss: 1.925907]\n",
      "epoch:24 step:23111 [D loss: 0.703082, acc: 57.81%] [G loss: 1.905061]\n",
      "epoch:24 step:23112 [D loss: 0.576995, acc: 70.31%] [G loss: 2.026331]\n",
      "epoch:24 step:23113 [D loss: 0.641706, acc: 68.75%] [G loss: 1.827698]\n",
      "epoch:24 step:23114 [D loss: 0.651765, acc: 59.38%] [G loss: 1.792431]\n",
      "epoch:24 step:23115 [D loss: 0.628263, acc: 64.06%] [G loss: 1.714509]\n",
      "epoch:24 step:23116 [D loss: 0.726686, acc: 47.66%] [G loss: 1.813027]\n",
      "epoch:24 step:23117 [D loss: 0.641139, acc: 62.50%] [G loss: 1.787855]\n",
      "epoch:24 step:23118 [D loss: 0.627553, acc: 63.28%] [G loss: 1.906979]\n",
      "epoch:24 step:23119 [D loss: 0.684927, acc: 57.81%] [G loss: 1.767542]\n",
      "epoch:24 step:23120 [D loss: 0.600803, acc: 69.53%] [G loss: 1.822658]\n",
      "epoch:24 step:23121 [D loss: 0.636268, acc: 65.62%] [G loss: 1.933412]\n",
      "epoch:24 step:23122 [D loss: 0.633934, acc: 60.94%] [G loss: 1.822087]\n",
      "epoch:24 step:23123 [D loss: 0.660233, acc: 63.28%] [G loss: 2.028696]\n",
      "epoch:24 step:23124 [D loss: 0.644860, acc: 60.16%] [G loss: 1.845913]\n",
      "epoch:24 step:23125 [D loss: 0.683861, acc: 60.16%] [G loss: 1.894963]\n",
      "epoch:24 step:23126 [D loss: 0.634212, acc: 67.97%] [G loss: 1.931903]\n",
      "epoch:24 step:23127 [D loss: 0.630188, acc: 64.06%] [G loss: 1.725724]\n",
      "epoch:24 step:23128 [D loss: 0.670399, acc: 57.81%] [G loss: 1.812325]\n",
      "epoch:24 step:23129 [D loss: 0.667994, acc: 60.94%] [G loss: 1.860897]\n",
      "epoch:24 step:23130 [D loss: 0.613047, acc: 70.31%] [G loss: 2.075801]\n",
      "epoch:24 step:23131 [D loss: 0.628018, acc: 61.72%] [G loss: 1.943961]\n",
      "epoch:24 step:23132 [D loss: 0.662787, acc: 59.38%] [G loss: 1.900803]\n",
      "epoch:24 step:23133 [D loss: 0.631959, acc: 63.28%] [G loss: 1.887082]\n",
      "epoch:24 step:23134 [D loss: 0.701635, acc: 57.03%] [G loss: 1.803962]\n",
      "epoch:24 step:23135 [D loss: 0.619366, acc: 72.66%] [G loss: 2.019115]\n",
      "epoch:24 step:23136 [D loss: 0.594614, acc: 70.31%] [G loss: 2.331007]\n",
      "epoch:24 step:23137 [D loss: 0.630065, acc: 65.62%] [G loss: 1.973198]\n",
      "epoch:24 step:23138 [D loss: 0.620326, acc: 64.84%] [G loss: 2.126549]\n",
      "epoch:24 step:23139 [D loss: 0.640741, acc: 64.06%] [G loss: 1.944024]\n",
      "epoch:24 step:23140 [D loss: 0.624132, acc: 65.62%] [G loss: 1.915466]\n",
      "epoch:24 step:23141 [D loss: 0.614201, acc: 64.84%] [G loss: 1.967690]\n",
      "epoch:24 step:23142 [D loss: 0.643837, acc: 64.06%] [G loss: 2.049585]\n",
      "epoch:24 step:23143 [D loss: 0.637584, acc: 62.50%] [G loss: 1.923637]\n",
      "epoch:24 step:23144 [D loss: 0.691602, acc: 55.47%] [G loss: 1.778490]\n",
      "epoch:24 step:23145 [D loss: 0.612912, acc: 71.09%] [G loss: 1.769445]\n",
      "epoch:24 step:23146 [D loss: 0.672763, acc: 60.94%] [G loss: 1.755134]\n",
      "epoch:24 step:23147 [D loss: 0.648646, acc: 64.06%] [G loss: 1.974328]\n",
      "epoch:24 step:23148 [D loss: 0.651264, acc: 57.81%] [G loss: 1.894289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23149 [D loss: 0.631768, acc: 63.28%] [G loss: 1.875346]\n",
      "epoch:24 step:23150 [D loss: 0.666280, acc: 57.03%] [G loss: 1.824577]\n",
      "epoch:24 step:23151 [D loss: 0.700165, acc: 51.56%] [G loss: 1.841877]\n",
      "epoch:24 step:23152 [D loss: 0.692293, acc: 56.25%] [G loss: 1.903721]\n",
      "epoch:24 step:23153 [D loss: 0.647312, acc: 67.19%] [G loss: 1.891790]\n",
      "epoch:24 step:23154 [D loss: 0.664387, acc: 59.38%] [G loss: 1.731729]\n",
      "epoch:24 step:23155 [D loss: 0.646231, acc: 66.41%] [G loss: 1.754783]\n",
      "epoch:24 step:23156 [D loss: 0.610865, acc: 67.97%] [G loss: 1.792174]\n",
      "epoch:24 step:23157 [D loss: 0.648737, acc: 64.06%] [G loss: 1.875165]\n",
      "epoch:24 step:23158 [D loss: 0.698201, acc: 55.47%] [G loss: 1.740958]\n",
      "epoch:24 step:23159 [D loss: 0.669556, acc: 54.69%] [G loss: 1.964727]\n",
      "epoch:24 step:23160 [D loss: 0.656929, acc: 60.94%] [G loss: 1.813772]\n",
      "epoch:24 step:23161 [D loss: 0.656434, acc: 60.94%] [G loss: 1.851521]\n",
      "epoch:24 step:23162 [D loss: 0.667412, acc: 58.59%] [G loss: 1.827138]\n",
      "epoch:24 step:23163 [D loss: 0.639100, acc: 62.50%] [G loss: 1.800259]\n",
      "epoch:24 step:23164 [D loss: 0.673463, acc: 56.25%] [G loss: 1.936890]\n",
      "epoch:24 step:23165 [D loss: 0.616760, acc: 68.75%] [G loss: 1.874813]\n",
      "epoch:24 step:23166 [D loss: 0.643109, acc: 61.72%] [G loss: 1.799151]\n",
      "epoch:24 step:23167 [D loss: 0.648615, acc: 64.84%] [G loss: 1.871084]\n",
      "epoch:24 step:23168 [D loss: 0.663827, acc: 62.50%] [G loss: 1.958340]\n",
      "epoch:24 step:23169 [D loss: 0.698327, acc: 57.03%] [G loss: 1.914963]\n",
      "epoch:24 step:23170 [D loss: 0.675831, acc: 58.59%] [G loss: 1.873264]\n",
      "epoch:24 step:23171 [D loss: 0.669618, acc: 58.59%] [G loss: 1.700784]\n",
      "epoch:24 step:23172 [D loss: 0.641542, acc: 63.28%] [G loss: 1.811875]\n",
      "epoch:24 step:23173 [D loss: 0.638134, acc: 59.38%] [G loss: 1.890138]\n",
      "epoch:24 step:23174 [D loss: 0.650903, acc: 64.84%] [G loss: 1.969717]\n",
      "epoch:24 step:23175 [D loss: 0.606851, acc: 67.97%] [G loss: 1.839220]\n",
      "epoch:24 step:23176 [D loss: 0.640334, acc: 60.16%] [G loss: 1.835963]\n",
      "epoch:24 step:23177 [D loss: 0.639414, acc: 64.84%] [G loss: 2.080441]\n",
      "epoch:24 step:23178 [D loss: 0.611421, acc: 60.16%] [G loss: 1.959940]\n",
      "epoch:24 step:23179 [D loss: 0.595734, acc: 67.97%] [G loss: 1.965161]\n",
      "epoch:24 step:23180 [D loss: 0.624853, acc: 63.28%] [G loss: 2.109174]\n",
      "epoch:24 step:23181 [D loss: 0.610618, acc: 63.28%] [G loss: 2.000688]\n",
      "epoch:24 step:23182 [D loss: 0.649454, acc: 60.94%] [G loss: 2.123227]\n",
      "epoch:24 step:23183 [D loss: 0.644241, acc: 61.72%] [G loss: 1.885645]\n",
      "epoch:24 step:23184 [D loss: 0.715281, acc: 51.56%] [G loss: 1.785045]\n",
      "epoch:24 step:23185 [D loss: 0.677074, acc: 60.16%] [G loss: 1.831715]\n",
      "epoch:24 step:23186 [D loss: 0.624409, acc: 64.84%] [G loss: 1.755596]\n",
      "epoch:24 step:23187 [D loss: 0.653814, acc: 60.16%] [G loss: 1.913380]\n",
      "epoch:24 step:23188 [D loss: 0.611954, acc: 72.66%] [G loss: 1.987484]\n",
      "epoch:24 step:23189 [D loss: 0.646732, acc: 60.94%] [G loss: 1.891772]\n",
      "epoch:24 step:23190 [D loss: 0.645290, acc: 63.28%] [G loss: 1.899952]\n",
      "epoch:24 step:23191 [D loss: 0.639289, acc: 66.41%] [G loss: 1.878331]\n",
      "epoch:24 step:23192 [D loss: 0.649344, acc: 57.81%] [G loss: 1.802580]\n",
      "epoch:24 step:23193 [D loss: 0.665371, acc: 64.06%] [G loss: 1.783903]\n",
      "epoch:24 step:23194 [D loss: 0.619150, acc: 65.62%] [G loss: 1.820633]\n",
      "epoch:24 step:23195 [D loss: 0.628666, acc: 65.62%] [G loss: 1.991639]\n",
      "epoch:24 step:23196 [D loss: 0.574868, acc: 68.75%] [G loss: 2.021343]\n",
      "epoch:24 step:23197 [D loss: 0.646840, acc: 63.28%] [G loss: 1.927740]\n",
      "epoch:24 step:23198 [D loss: 0.730427, acc: 57.81%] [G loss: 1.797678]\n",
      "epoch:24 step:23199 [D loss: 0.657062, acc: 65.62%] [G loss: 1.940431]\n",
      "epoch:24 step:23200 [D loss: 0.649825, acc: 62.50%] [G loss: 1.982857]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.431972\n",
      "FID: 10.149901\n",
      "0 = 12.798738634347918\n",
      "1 = 0.09001144018578332\n",
      "2 = 0.8748999834060669\n",
      "3 = 0.8988000154495239\n",
      "4 = 0.8510000109672546\n",
      "5 = 0.8577972650527954\n",
      "6 = 0.8988000154495239\n",
      "7 = 6.197730295062072\n",
      "8 = 0.05984185039005285\n",
      "9 = 0.7148000001907349\n",
      "10 = 0.7305999994277954\n",
      "11 = 0.6990000009536743\n",
      "12 = 0.7082202434539795\n",
      "13 = 0.7305999994277954\n",
      "14 = 7.432000160217285\n",
      "15 = 9.450944900512695\n",
      "16 = 0.11036968976259232\n",
      "17 = 7.431972026824951\n",
      "18 = 10.149901390075684\n",
      "epoch:24 step:23201 [D loss: 0.662993, acc: 63.28%] [G loss: 1.860349]\n",
      "epoch:24 step:23202 [D loss: 0.652661, acc: 58.59%] [G loss: 1.818325]\n",
      "epoch:24 step:23203 [D loss: 0.657978, acc: 64.84%] [G loss: 1.864328]\n",
      "epoch:24 step:23204 [D loss: 0.681041, acc: 57.81%] [G loss: 1.777740]\n",
      "epoch:24 step:23205 [D loss: 0.673520, acc: 56.25%] [G loss: 1.859672]\n",
      "epoch:24 step:23206 [D loss: 0.634562, acc: 60.94%] [G loss: 1.897639]\n",
      "epoch:24 step:23207 [D loss: 0.594334, acc: 73.44%] [G loss: 2.018902]\n",
      "epoch:24 step:23208 [D loss: 0.637391, acc: 64.06%] [G loss: 1.950463]\n",
      "epoch:24 step:23209 [D loss: 0.691653, acc: 52.34%] [G loss: 2.147541]\n",
      "epoch:24 step:23210 [D loss: 0.672966, acc: 58.59%] [G loss: 1.745213]\n",
      "epoch:24 step:23211 [D loss: 0.669937, acc: 57.81%] [G loss: 1.798426]\n",
      "epoch:24 step:23212 [D loss: 0.595320, acc: 67.97%] [G loss: 1.972881]\n",
      "epoch:24 step:23213 [D loss: 0.647273, acc: 60.16%] [G loss: 1.928997]\n",
      "epoch:24 step:23214 [D loss: 0.633831, acc: 63.28%] [G loss: 1.848790]\n",
      "epoch:24 step:23215 [D loss: 0.655718, acc: 57.03%] [G loss: 1.820199]\n",
      "epoch:24 step:23216 [D loss: 0.619996, acc: 64.84%] [G loss: 1.841089]\n",
      "epoch:24 step:23217 [D loss: 0.681873, acc: 57.03%] [G loss: 1.846622]\n",
      "epoch:24 step:23218 [D loss: 0.657114, acc: 60.16%] [G loss: 1.970103]\n",
      "epoch:24 step:23219 [D loss: 0.672256, acc: 60.16%] [G loss: 1.855485]\n",
      "epoch:24 step:23220 [D loss: 0.688036, acc: 61.72%] [G loss: 1.916419]\n",
      "epoch:24 step:23221 [D loss: 0.634676, acc: 66.41%] [G loss: 1.918624]\n",
      "epoch:24 step:23222 [D loss: 0.670913, acc: 57.81%] [G loss: 1.754399]\n",
      "epoch:24 step:23223 [D loss: 0.654071, acc: 57.81%] [G loss: 1.904615]\n",
      "epoch:24 step:23224 [D loss: 0.634597, acc: 64.06%] [G loss: 1.990140]\n",
      "epoch:24 step:23225 [D loss: 0.631013, acc: 66.41%] [G loss: 1.828778]\n",
      "epoch:24 step:23226 [D loss: 0.657780, acc: 57.81%] [G loss: 1.751533]\n",
      "epoch:24 step:23227 [D loss: 0.636942, acc: 62.50%] [G loss: 1.818099]\n",
      "epoch:24 step:23228 [D loss: 0.635865, acc: 63.28%] [G loss: 1.919784]\n",
      "epoch:24 step:23229 [D loss: 0.676099, acc: 57.03%] [G loss: 1.731368]\n",
      "epoch:24 step:23230 [D loss: 0.655877, acc: 61.72%] [G loss: 1.831988]\n",
      "epoch:24 step:23231 [D loss: 0.645609, acc: 62.50%] [G loss: 1.815500]\n",
      "epoch:24 step:23232 [D loss: 0.654890, acc: 60.94%] [G loss: 1.873808]\n",
      "epoch:24 step:23233 [D loss: 0.625675, acc: 67.19%] [G loss: 1.893127]\n",
      "epoch:24 step:23234 [D loss: 0.621932, acc: 61.72%] [G loss: 1.983198]\n",
      "epoch:24 step:23235 [D loss: 0.651769, acc: 64.84%] [G loss: 1.987224]\n",
      "epoch:24 step:23236 [D loss: 0.723663, acc: 50.78%] [G loss: 1.852443]\n",
      "epoch:24 step:23237 [D loss: 0.708850, acc: 52.34%] [G loss: 1.778765]\n",
      "epoch:24 step:23238 [D loss: 0.621518, acc: 64.84%] [G loss: 1.828971]\n",
      "epoch:24 step:23239 [D loss: 0.654870, acc: 64.84%] [G loss: 1.806002]\n",
      "epoch:24 step:23240 [D loss: 0.671597, acc: 57.03%] [G loss: 1.702669]\n",
      "epoch:24 step:23241 [D loss: 0.669947, acc: 55.47%] [G loss: 1.793065]\n",
      "epoch:24 step:23242 [D loss: 0.653710, acc: 62.50%] [G loss: 1.920178]\n",
      "epoch:24 step:23243 [D loss: 0.648604, acc: 62.50%] [G loss: 1.840637]\n",
      "epoch:24 step:23244 [D loss: 0.621163, acc: 65.62%] [G loss: 1.844274]\n",
      "epoch:24 step:23245 [D loss: 0.649337, acc: 60.94%] [G loss: 1.866561]\n",
      "epoch:24 step:23246 [D loss: 0.657378, acc: 66.41%] [G loss: 1.769495]\n",
      "epoch:24 step:23247 [D loss: 0.668725, acc: 60.16%] [G loss: 1.667928]\n",
      "epoch:24 step:23248 [D loss: 0.650629, acc: 60.16%] [G loss: 1.792137]\n",
      "epoch:24 step:23249 [D loss: 0.636243, acc: 59.38%] [G loss: 1.794922]\n",
      "epoch:24 step:23250 [D loss: 0.636737, acc: 61.72%] [G loss: 1.794533]\n",
      "epoch:24 step:23251 [D loss: 0.616047, acc: 71.09%] [G loss: 1.892970]\n",
      "epoch:24 step:23252 [D loss: 0.651800, acc: 63.28%] [G loss: 1.886619]\n",
      "epoch:24 step:23253 [D loss: 0.728699, acc: 51.56%] [G loss: 1.695277]\n",
      "epoch:24 step:23254 [D loss: 0.694543, acc: 54.69%] [G loss: 1.710053]\n",
      "epoch:24 step:23255 [D loss: 0.646759, acc: 65.62%] [G loss: 1.739658]\n",
      "epoch:24 step:23256 [D loss: 0.699617, acc: 55.47%] [G loss: 1.996137]\n",
      "epoch:24 step:23257 [D loss: 0.643297, acc: 61.72%] [G loss: 1.933050]\n",
      "epoch:24 step:23258 [D loss: 0.673113, acc: 54.69%] [G loss: 1.804335]\n",
      "epoch:24 step:23259 [D loss: 0.684597, acc: 56.25%] [G loss: 1.843543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23260 [D loss: 0.623617, acc: 64.06%] [G loss: 1.776635]\n",
      "epoch:24 step:23261 [D loss: 0.624984, acc: 67.19%] [G loss: 1.916141]\n",
      "epoch:24 step:23262 [D loss: 0.615770, acc: 69.53%] [G loss: 2.094238]\n",
      "epoch:24 step:23263 [D loss: 0.618557, acc: 65.62%] [G loss: 2.164182]\n",
      "epoch:24 step:23264 [D loss: 0.639580, acc: 61.72%] [G loss: 1.909793]\n",
      "epoch:24 step:23265 [D loss: 0.661304, acc: 58.59%] [G loss: 1.986523]\n",
      "epoch:24 step:23266 [D loss: 0.693686, acc: 56.25%] [G loss: 1.824250]\n",
      "epoch:24 step:23267 [D loss: 0.671432, acc: 60.94%] [G loss: 1.820467]\n",
      "epoch:24 step:23268 [D loss: 0.619056, acc: 67.19%] [G loss: 1.878101]\n",
      "epoch:24 step:23269 [D loss: 0.623053, acc: 60.94%] [G loss: 1.945716]\n",
      "epoch:24 step:23270 [D loss: 0.579828, acc: 70.31%] [G loss: 2.009419]\n",
      "epoch:24 step:23271 [D loss: 0.676356, acc: 56.25%] [G loss: 1.824221]\n",
      "epoch:24 step:23272 [D loss: 0.723313, acc: 49.22%] [G loss: 1.646961]\n",
      "epoch:24 step:23273 [D loss: 0.682236, acc: 60.16%] [G loss: 1.867974]\n",
      "epoch:24 step:23274 [D loss: 0.605556, acc: 66.41%] [G loss: 2.090648]\n",
      "epoch:24 step:23275 [D loss: 0.683067, acc: 58.59%] [G loss: 1.856825]\n",
      "epoch:24 step:23276 [D loss: 0.707404, acc: 51.56%] [G loss: 1.733490]\n",
      "epoch:24 step:23277 [D loss: 0.699686, acc: 57.03%] [G loss: 1.867334]\n",
      "epoch:24 step:23278 [D loss: 0.606483, acc: 70.31%] [G loss: 1.905338]\n",
      "epoch:24 step:23279 [D loss: 0.645893, acc: 60.16%] [G loss: 1.961295]\n",
      "epoch:24 step:23280 [D loss: 0.623940, acc: 72.66%] [G loss: 1.901133]\n",
      "epoch:24 step:23281 [D loss: 0.579965, acc: 70.31%] [G loss: 2.080695]\n",
      "epoch:24 step:23282 [D loss: 0.661782, acc: 56.25%] [G loss: 1.715743]\n",
      "epoch:24 step:23283 [D loss: 0.652433, acc: 62.50%] [G loss: 1.726284]\n",
      "epoch:24 step:23284 [D loss: 0.629935, acc: 66.41%] [G loss: 1.811074]\n",
      "epoch:24 step:23285 [D loss: 0.650198, acc: 62.50%] [G loss: 1.838616]\n",
      "epoch:24 step:23286 [D loss: 0.644872, acc: 63.28%] [G loss: 1.833989]\n",
      "epoch:24 step:23287 [D loss: 0.673856, acc: 53.91%] [G loss: 1.808120]\n",
      "epoch:24 step:23288 [D loss: 0.730583, acc: 50.78%] [G loss: 1.637013]\n",
      "epoch:24 step:23289 [D loss: 0.648521, acc: 61.72%] [G loss: 1.757760]\n",
      "epoch:24 step:23290 [D loss: 0.687747, acc: 56.25%] [G loss: 1.811103]\n",
      "epoch:24 step:23291 [D loss: 0.619057, acc: 67.19%] [G loss: 1.763941]\n",
      "epoch:24 step:23292 [D loss: 0.644704, acc: 59.38%] [G loss: 1.879537]\n",
      "epoch:24 step:23293 [D loss: 0.623550, acc: 65.62%] [G loss: 1.885444]\n",
      "epoch:24 step:23294 [D loss: 0.639459, acc: 63.28%] [G loss: 1.858097]\n",
      "epoch:24 step:23295 [D loss: 0.631152, acc: 64.06%] [G loss: 1.872005]\n",
      "epoch:24 step:23296 [D loss: 0.626213, acc: 64.84%] [G loss: 2.014640]\n",
      "epoch:24 step:23297 [D loss: 0.714383, acc: 53.91%] [G loss: 1.953144]\n",
      "epoch:24 step:23298 [D loss: 0.617212, acc: 63.28%] [G loss: 1.857383]\n",
      "epoch:24 step:23299 [D loss: 0.661227, acc: 60.16%] [G loss: 1.817612]\n",
      "epoch:24 step:23300 [D loss: 0.691552, acc: 58.59%] [G loss: 1.785933]\n",
      "epoch:24 step:23301 [D loss: 0.641219, acc: 62.50%] [G loss: 1.727796]\n",
      "epoch:24 step:23302 [D loss: 0.643019, acc: 60.16%] [G loss: 1.953514]\n",
      "epoch:24 step:23303 [D loss: 0.685746, acc: 61.72%] [G loss: 2.132540]\n",
      "epoch:24 step:23304 [D loss: 0.615539, acc: 61.72%] [G loss: 2.116272]\n",
      "epoch:24 step:23305 [D loss: 0.638286, acc: 61.72%] [G loss: 1.831867]\n",
      "epoch:24 step:23306 [D loss: 0.645621, acc: 65.62%] [G loss: 1.853597]\n",
      "epoch:24 step:23307 [D loss: 0.669656, acc: 56.25%] [G loss: 1.859822]\n",
      "epoch:24 step:23308 [D loss: 0.696408, acc: 53.12%] [G loss: 1.693063]\n",
      "epoch:24 step:23309 [D loss: 0.647581, acc: 63.28%] [G loss: 1.863422]\n",
      "epoch:24 step:23310 [D loss: 0.617905, acc: 67.97%] [G loss: 1.855197]\n",
      "epoch:24 step:23311 [D loss: 0.655531, acc: 63.28%] [G loss: 2.028658]\n",
      "epoch:24 step:23312 [D loss: 0.682495, acc: 61.72%] [G loss: 1.891340]\n",
      "epoch:24 step:23313 [D loss: 0.664595, acc: 61.72%] [G loss: 1.996615]\n",
      "epoch:24 step:23314 [D loss: 0.648610, acc: 60.16%] [G loss: 1.854173]\n",
      "epoch:24 step:23315 [D loss: 0.666901, acc: 60.16%] [G loss: 1.813625]\n",
      "epoch:24 step:23316 [D loss: 0.695213, acc: 53.91%] [G loss: 1.836150]\n",
      "epoch:24 step:23317 [D loss: 0.718686, acc: 54.69%] [G loss: 1.643069]\n",
      "epoch:24 step:23318 [D loss: 0.628949, acc: 69.53%] [G loss: 1.809634]\n",
      "epoch:24 step:23319 [D loss: 0.668256, acc: 60.16%] [G loss: 1.855732]\n",
      "epoch:24 step:23320 [D loss: 0.624327, acc: 61.72%] [G loss: 1.777431]\n",
      "epoch:24 step:23321 [D loss: 0.620289, acc: 66.41%] [G loss: 1.888557]\n",
      "epoch:24 step:23322 [D loss: 0.684245, acc: 59.38%] [G loss: 1.766556]\n",
      "epoch:24 step:23323 [D loss: 0.651221, acc: 60.94%] [G loss: 1.844721]\n",
      "epoch:24 step:23324 [D loss: 0.657869, acc: 60.16%] [G loss: 1.783250]\n",
      "epoch:24 step:23325 [D loss: 0.640119, acc: 63.28%] [G loss: 1.802740]\n",
      "epoch:24 step:23326 [D loss: 0.654182, acc: 60.94%] [G loss: 1.877034]\n",
      "epoch:24 step:23327 [D loss: 0.652352, acc: 67.97%] [G loss: 1.890522]\n",
      "epoch:24 step:23328 [D loss: 0.647804, acc: 65.62%] [G loss: 1.899590]\n",
      "epoch:24 step:23329 [D loss: 0.610559, acc: 65.62%] [G loss: 1.951298]\n",
      "epoch:24 step:23330 [D loss: 0.610691, acc: 67.19%] [G loss: 1.937852]\n",
      "epoch:24 step:23331 [D loss: 0.648076, acc: 62.50%] [G loss: 1.890530]\n",
      "epoch:24 step:23332 [D loss: 0.666945, acc: 58.59%] [G loss: 1.774577]\n",
      "epoch:24 step:23333 [D loss: 0.614712, acc: 64.06%] [G loss: 2.035997]\n",
      "epoch:24 step:23334 [D loss: 0.615972, acc: 65.62%] [G loss: 1.873576]\n",
      "epoch:24 step:23335 [D loss: 0.648191, acc: 55.47%] [G loss: 1.939978]\n",
      "epoch:24 step:23336 [D loss: 0.642975, acc: 63.28%] [G loss: 1.810030]\n",
      "epoch:24 step:23337 [D loss: 0.619469, acc: 64.06%] [G loss: 1.960938]\n",
      "epoch:24 step:23338 [D loss: 0.644296, acc: 60.94%] [G loss: 1.757287]\n",
      "epoch:24 step:23339 [D loss: 0.646683, acc: 63.28%] [G loss: 1.812750]\n",
      "epoch:24 step:23340 [D loss: 0.651862, acc: 61.72%] [G loss: 1.890040]\n",
      "epoch:24 step:23341 [D loss: 0.632153, acc: 59.38%] [G loss: 1.939946]\n",
      "epoch:24 step:23342 [D loss: 0.673064, acc: 54.69%] [G loss: 1.887578]\n",
      "epoch:24 step:23343 [D loss: 0.658881, acc: 59.38%] [G loss: 1.747249]\n",
      "epoch:24 step:23344 [D loss: 0.690205, acc: 50.78%] [G loss: 1.814857]\n",
      "epoch:24 step:23345 [D loss: 0.669177, acc: 60.16%] [G loss: 1.812569]\n",
      "epoch:24 step:23346 [D loss: 0.682046, acc: 60.94%] [G loss: 1.773051]\n",
      "epoch:24 step:23347 [D loss: 0.697809, acc: 61.72%] [G loss: 1.858330]\n",
      "epoch:24 step:23348 [D loss: 0.662185, acc: 60.94%] [G loss: 1.802229]\n",
      "epoch:24 step:23349 [D loss: 0.671590, acc: 61.72%] [G loss: 1.735729]\n",
      "epoch:24 step:23350 [D loss: 0.672485, acc: 60.94%] [G loss: 1.764982]\n",
      "epoch:24 step:23351 [D loss: 0.661470, acc: 60.94%] [G loss: 1.814315]\n",
      "epoch:24 step:23352 [D loss: 0.614407, acc: 69.53%] [G loss: 1.792832]\n",
      "epoch:24 step:23353 [D loss: 0.669777, acc: 56.25%] [G loss: 1.799185]\n",
      "epoch:24 step:23354 [D loss: 0.696142, acc: 56.25%] [G loss: 1.805754]\n",
      "epoch:24 step:23355 [D loss: 0.649900, acc: 64.84%] [G loss: 1.615647]\n",
      "epoch:24 step:23356 [D loss: 0.640938, acc: 65.62%] [G loss: 1.916518]\n",
      "epoch:24 step:23357 [D loss: 0.659393, acc: 60.16%] [G loss: 1.862839]\n",
      "epoch:24 step:23358 [D loss: 0.719683, acc: 53.91%] [G loss: 1.844686]\n",
      "epoch:24 step:23359 [D loss: 0.723814, acc: 58.59%] [G loss: 1.782394]\n",
      "epoch:24 step:23360 [D loss: 0.651149, acc: 63.28%] [G loss: 1.768126]\n",
      "epoch:24 step:23361 [D loss: 0.681396, acc: 54.69%] [G loss: 1.704029]\n",
      "epoch:24 step:23362 [D loss: 0.642240, acc: 64.06%] [G loss: 1.719770]\n",
      "epoch:24 step:23363 [D loss: 0.653773, acc: 64.06%] [G loss: 1.926803]\n",
      "epoch:24 step:23364 [D loss: 0.653791, acc: 62.50%] [G loss: 1.833640]\n",
      "epoch:24 step:23365 [D loss: 0.651250, acc: 59.38%] [G loss: 1.793871]\n",
      "epoch:24 step:23366 [D loss: 0.650879, acc: 61.72%] [G loss: 1.861382]\n",
      "epoch:24 step:23367 [D loss: 0.642365, acc: 67.19%] [G loss: 1.744880]\n",
      "epoch:24 step:23368 [D loss: 0.694208, acc: 55.47%] [G loss: 1.698673]\n",
      "epoch:24 step:23369 [D loss: 0.658816, acc: 53.91%] [G loss: 1.853934]\n",
      "epoch:24 step:23370 [D loss: 0.647799, acc: 57.81%] [G loss: 1.867250]\n",
      "epoch:24 step:23371 [D loss: 0.681531, acc: 61.72%] [G loss: 1.798830]\n",
      "epoch:24 step:23372 [D loss: 0.626618, acc: 63.28%] [G loss: 1.905670]\n",
      "epoch:24 step:23373 [D loss: 0.630652, acc: 64.84%] [G loss: 1.832044]\n",
      "epoch:24 step:23374 [D loss: 0.618482, acc: 57.81%] [G loss: 2.033625]\n",
      "epoch:24 step:23375 [D loss: 0.693641, acc: 57.81%] [G loss: 1.906316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23376 [D loss: 0.633945, acc: 65.62%] [G loss: 1.817811]\n",
      "epoch:24 step:23377 [D loss: 0.614590, acc: 67.19%] [G loss: 1.814901]\n",
      "epoch:24 step:23378 [D loss: 0.666687, acc: 64.84%] [G loss: 1.889720]\n",
      "epoch:24 step:23379 [D loss: 0.677489, acc: 60.94%] [G loss: 1.773179]\n",
      "epoch:24 step:23380 [D loss: 0.701200, acc: 56.25%] [G loss: 1.824324]\n",
      "epoch:24 step:23381 [D loss: 0.653229, acc: 57.81%] [G loss: 1.790762]\n",
      "epoch:24 step:23382 [D loss: 0.672799, acc: 63.28%] [G loss: 1.905701]\n",
      "epoch:24 step:23383 [D loss: 0.645214, acc: 63.28%] [G loss: 1.905561]\n",
      "epoch:24 step:23384 [D loss: 0.725024, acc: 53.12%] [G loss: 1.833952]\n",
      "epoch:24 step:23385 [D loss: 0.656191, acc: 58.59%] [G loss: 1.907074]\n",
      "epoch:24 step:23386 [D loss: 0.613306, acc: 69.53%] [G loss: 1.830611]\n",
      "epoch:24 step:23387 [D loss: 0.649092, acc: 62.50%] [G loss: 2.026129]\n",
      "epoch:24 step:23388 [D loss: 0.599069, acc: 67.19%] [G loss: 1.896149]\n",
      "epoch:24 step:23389 [D loss: 0.657843, acc: 57.03%] [G loss: 1.932599]\n",
      "epoch:24 step:23390 [D loss: 0.694855, acc: 55.47%] [G loss: 1.767286]\n",
      "epoch:24 step:23391 [D loss: 0.630500, acc: 61.72%] [G loss: 1.839252]\n",
      "epoch:24 step:23392 [D loss: 0.673002, acc: 56.25%] [G loss: 1.873347]\n",
      "epoch:24 step:23393 [D loss: 0.629382, acc: 64.84%] [G loss: 1.906821]\n",
      "epoch:24 step:23394 [D loss: 0.603474, acc: 67.19%] [G loss: 2.093254]\n",
      "epoch:24 step:23395 [D loss: 0.710910, acc: 54.69%] [G loss: 2.077906]\n",
      "epoch:24 step:23396 [D loss: 0.650851, acc: 60.94%] [G loss: 2.007726]\n",
      "epoch:24 step:23397 [D loss: 0.622244, acc: 64.84%] [G loss: 2.019881]\n",
      "epoch:24 step:23398 [D loss: 0.650335, acc: 57.03%] [G loss: 1.937167]\n",
      "epoch:24 step:23399 [D loss: 0.644836, acc: 59.38%] [G loss: 1.954546]\n",
      "epoch:24 step:23400 [D loss: 0.671619, acc: 61.72%] [G loss: 1.855815]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.607230\n",
      "FID: 10.038519\n",
      "0 = 12.56134083375929\n",
      "1 = 0.08171383436308909\n",
      "2 = 0.8769999742507935\n",
      "3 = 0.895799994468689\n",
      "4 = 0.8582000136375427\n",
      "5 = 0.8633384704589844\n",
      "6 = 0.895799994468689\n",
      "7 = 6.049129694616793\n",
      "8 = 0.057959621975484064\n",
      "9 = 0.707099974155426\n",
      "10 = 0.7193999886512756\n",
      "11 = 0.6948000192642212\n",
      "12 = 0.7021276354789734\n",
      "13 = 0.7193999886512756\n",
      "14 = 7.607259750366211\n",
      "15 = 9.534978866577148\n",
      "16 = 0.07942277193069458\n",
      "17 = 7.607229709625244\n",
      "18 = 10.038518905639648\n",
      "epoch:24 step:23401 [D loss: 0.660392, acc: 61.72%] [G loss: 1.881218]\n",
      "epoch:24 step:23402 [D loss: 0.673118, acc: 59.38%] [G loss: 1.842497]\n",
      "epoch:24 step:23403 [D loss: 0.675548, acc: 60.16%] [G loss: 2.016062]\n",
      "epoch:24 step:23404 [D loss: 0.642633, acc: 60.94%] [G loss: 1.921757]\n",
      "epoch:24 step:23405 [D loss: 0.676082, acc: 58.59%] [G loss: 1.868431]\n",
      "epoch:24 step:23406 [D loss: 0.564988, acc: 74.22%] [G loss: 2.077610]\n",
      "epoch:24 step:23407 [D loss: 0.627333, acc: 59.38%] [G loss: 2.116237]\n",
      "epoch:24 step:23408 [D loss: 0.681750, acc: 60.16%] [G loss: 1.856309]\n",
      "epoch:24 step:23409 [D loss: 0.574953, acc: 71.88%] [G loss: 1.944334]\n",
      "epoch:24 step:23410 [D loss: 0.614159, acc: 65.62%] [G loss: 1.951973]\n",
      "epoch:24 step:23411 [D loss: 0.579236, acc: 71.88%] [G loss: 2.169633]\n",
      "epoch:24 step:23412 [D loss: 0.577018, acc: 71.09%] [G loss: 2.144817]\n",
      "epoch:24 step:23413 [D loss: 0.631095, acc: 64.84%] [G loss: 2.049791]\n",
      "epoch:24 step:23414 [D loss: 0.635120, acc: 65.62%] [G loss: 2.115398]\n",
      "epoch:24 step:23415 [D loss: 0.669647, acc: 60.94%] [G loss: 1.823063]\n",
      "epoch:24 step:23416 [D loss: 0.780591, acc: 47.66%] [G loss: 1.773377]\n",
      "epoch:24 step:23417 [D loss: 0.737610, acc: 48.44%] [G loss: 1.783953]\n",
      "epoch:24 step:23418 [D loss: 0.621524, acc: 63.28%] [G loss: 1.937660]\n",
      "epoch:24 step:23419 [D loss: 0.603876, acc: 64.84%] [G loss: 1.911345]\n",
      "epoch:24 step:23420 [D loss: 0.678306, acc: 58.59%] [G loss: 1.919499]\n",
      "epoch:24 step:23421 [D loss: 0.625399, acc: 69.53%] [G loss: 1.863724]\n",
      "epoch:24 step:23422 [D loss: 0.655854, acc: 61.72%] [G loss: 1.771610]\n",
      "epoch:24 step:23423 [D loss: 0.617847, acc: 61.72%] [G loss: 1.954548]\n",
      "epoch:24 step:23424 [D loss: 0.586331, acc: 72.66%] [G loss: 2.004597]\n",
      "epoch:24 step:23425 [D loss: 0.651296, acc: 61.72%] [G loss: 2.292007]\n",
      "epoch:25 step:23426 [D loss: 0.662493, acc: 62.50%] [G loss: 1.859933]\n",
      "epoch:25 step:23427 [D loss: 0.643298, acc: 62.50%] [G loss: 1.799663]\n",
      "epoch:25 step:23428 [D loss: 0.642427, acc: 63.28%] [G loss: 1.891429]\n",
      "epoch:25 step:23429 [D loss: 0.677094, acc: 57.03%] [G loss: 1.820318]\n",
      "epoch:25 step:23430 [D loss: 0.692950, acc: 64.06%] [G loss: 1.770734]\n",
      "epoch:25 step:23431 [D loss: 0.598095, acc: 68.75%] [G loss: 1.914723]\n",
      "epoch:25 step:23432 [D loss: 0.634807, acc: 63.28%] [G loss: 1.887127]\n",
      "epoch:25 step:23433 [D loss: 0.654954, acc: 62.50%] [G loss: 1.815059]\n",
      "epoch:25 step:23434 [D loss: 0.665948, acc: 61.72%] [G loss: 1.852693]\n",
      "epoch:25 step:23435 [D loss: 0.579372, acc: 66.41%] [G loss: 1.879756]\n",
      "epoch:25 step:23436 [D loss: 0.671683, acc: 60.94%] [G loss: 1.956329]\n",
      "epoch:25 step:23437 [D loss: 0.633206, acc: 63.28%] [G loss: 1.842696]\n",
      "epoch:25 step:23438 [D loss: 0.607566, acc: 71.88%] [G loss: 1.813683]\n",
      "epoch:25 step:23439 [D loss: 0.647293, acc: 60.16%] [G loss: 1.941670]\n",
      "epoch:25 step:23440 [D loss: 0.595511, acc: 67.19%] [G loss: 2.122098]\n",
      "epoch:25 step:23441 [D loss: 0.673283, acc: 65.62%] [G loss: 2.008498]\n",
      "epoch:25 step:23442 [D loss: 0.582110, acc: 72.66%] [G loss: 1.888061]\n",
      "epoch:25 step:23443 [D loss: 0.667536, acc: 59.38%] [G loss: 1.950291]\n",
      "epoch:25 step:23444 [D loss: 0.643026, acc: 61.72%] [G loss: 1.847572]\n",
      "epoch:25 step:23445 [D loss: 0.692315, acc: 53.12%] [G loss: 1.681640]\n",
      "epoch:25 step:23446 [D loss: 0.664474, acc: 56.25%] [G loss: 1.787130]\n",
      "epoch:25 step:23447 [D loss: 0.649267, acc: 58.59%] [G loss: 1.754160]\n",
      "epoch:25 step:23448 [D loss: 0.662912, acc: 56.25%] [G loss: 2.022499]\n",
      "epoch:25 step:23449 [D loss: 0.639926, acc: 64.06%] [G loss: 1.970039]\n",
      "epoch:25 step:23450 [D loss: 0.590385, acc: 67.97%] [G loss: 1.905956]\n",
      "epoch:25 step:23451 [D loss: 0.640594, acc: 68.75%] [G loss: 1.860620]\n",
      "epoch:25 step:23452 [D loss: 0.629925, acc: 63.28%] [G loss: 1.839988]\n",
      "epoch:25 step:23453 [D loss: 0.630895, acc: 66.41%] [G loss: 1.898188]\n",
      "epoch:25 step:23454 [D loss: 0.592075, acc: 70.31%] [G loss: 1.960357]\n",
      "epoch:25 step:23455 [D loss: 0.639109, acc: 64.84%] [G loss: 1.880486]\n",
      "epoch:25 step:23456 [D loss: 0.695381, acc: 58.59%] [G loss: 1.771240]\n",
      "epoch:25 step:23457 [D loss: 0.703454, acc: 55.47%] [G loss: 1.778214]\n",
      "epoch:25 step:23458 [D loss: 0.617420, acc: 67.97%] [G loss: 1.958729]\n",
      "epoch:25 step:23459 [D loss: 0.597554, acc: 64.06%] [G loss: 1.868811]\n",
      "epoch:25 step:23460 [D loss: 0.597148, acc: 65.62%] [G loss: 1.939646]\n",
      "epoch:25 step:23461 [D loss: 0.634150, acc: 68.75%] [G loss: 2.045662]\n",
      "epoch:25 step:23462 [D loss: 0.662140, acc: 58.59%] [G loss: 1.977323]\n",
      "epoch:25 step:23463 [D loss: 0.714772, acc: 57.03%] [G loss: 1.953298]\n",
      "epoch:25 step:23464 [D loss: 0.690478, acc: 53.91%] [G loss: 1.820732]\n",
      "epoch:25 step:23465 [D loss: 0.607420, acc: 64.84%] [G loss: 1.853051]\n",
      "epoch:25 step:23466 [D loss: 0.605616, acc: 68.75%] [G loss: 1.806239]\n",
      "epoch:25 step:23467 [D loss: 0.583941, acc: 74.22%] [G loss: 2.114557]\n",
      "epoch:25 step:23468 [D loss: 0.606550, acc: 66.41%] [G loss: 2.053818]\n",
      "epoch:25 step:23469 [D loss: 0.651127, acc: 57.81%] [G loss: 1.739510]\n",
      "epoch:25 step:23470 [D loss: 0.603501, acc: 69.53%] [G loss: 1.913964]\n",
      "epoch:25 step:23471 [D loss: 0.673018, acc: 57.03%] [G loss: 1.796670]\n",
      "epoch:25 step:23472 [D loss: 0.641542, acc: 66.41%] [G loss: 1.905119]\n",
      "epoch:25 step:23473 [D loss: 0.623576, acc: 65.62%] [G loss: 2.050466]\n",
      "epoch:25 step:23474 [D loss: 0.610984, acc: 63.28%] [G loss: 2.025373]\n",
      "epoch:25 step:23475 [D loss: 0.629778, acc: 64.06%] [G loss: 1.926162]\n",
      "epoch:25 step:23476 [D loss: 0.659130, acc: 56.25%] [G loss: 1.909941]\n",
      "epoch:25 step:23477 [D loss: 0.669227, acc: 57.81%] [G loss: 2.053716]\n",
      "epoch:25 step:23478 [D loss: 0.646158, acc: 64.06%] [G loss: 1.962971]\n",
      "epoch:25 step:23479 [D loss: 0.649529, acc: 64.06%] [G loss: 2.029188]\n",
      "epoch:25 step:23480 [D loss: 0.614793, acc: 70.31%] [G loss: 1.979668]\n",
      "epoch:25 step:23481 [D loss: 0.665043, acc: 58.59%] [G loss: 2.002009]\n",
      "epoch:25 step:23482 [D loss: 0.617009, acc: 64.84%] [G loss: 1.978832]\n",
      "epoch:25 step:23483 [D loss: 0.621458, acc: 63.28%] [G loss: 1.855901]\n",
      "epoch:25 step:23484 [D loss: 0.622389, acc: 67.97%] [G loss: 1.888772]\n",
      "epoch:25 step:23485 [D loss: 0.669654, acc: 57.03%] [G loss: 1.827857]\n",
      "epoch:25 step:23486 [D loss: 0.693928, acc: 56.25%] [G loss: 1.839316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23487 [D loss: 0.686694, acc: 54.69%] [G loss: 1.921132]\n",
      "epoch:25 step:23488 [D loss: 0.643019, acc: 64.84%] [G loss: 1.788306]\n",
      "epoch:25 step:23489 [D loss: 0.656468, acc: 60.16%] [G loss: 2.007566]\n",
      "epoch:25 step:23490 [D loss: 0.629317, acc: 64.06%] [G loss: 1.890822]\n",
      "epoch:25 step:23491 [D loss: 0.673179, acc: 59.38%] [G loss: 1.866089]\n",
      "epoch:25 step:23492 [D loss: 0.661674, acc: 59.38%] [G loss: 1.931653]\n",
      "epoch:25 step:23493 [D loss: 0.655954, acc: 58.59%] [G loss: 1.837376]\n",
      "epoch:25 step:23494 [D loss: 0.627249, acc: 66.41%] [G loss: 1.834763]\n",
      "epoch:25 step:23495 [D loss: 0.611383, acc: 67.19%] [G loss: 1.955771]\n",
      "epoch:25 step:23496 [D loss: 0.600386, acc: 67.97%] [G loss: 1.806951]\n",
      "epoch:25 step:23497 [D loss: 0.630906, acc: 64.84%] [G loss: 1.794293]\n",
      "epoch:25 step:23498 [D loss: 0.671358, acc: 57.03%] [G loss: 1.771952]\n",
      "epoch:25 step:23499 [D loss: 0.592322, acc: 68.75%] [G loss: 2.032782]\n",
      "epoch:25 step:23500 [D loss: 0.644027, acc: 62.50%] [G loss: 2.004351]\n",
      "epoch:25 step:23501 [D loss: 0.590393, acc: 66.41%] [G loss: 1.999038]\n",
      "epoch:25 step:23502 [D loss: 0.589886, acc: 69.53%] [G loss: 2.039728]\n",
      "epoch:25 step:23503 [D loss: 0.685903, acc: 56.25%] [G loss: 1.689507]\n",
      "epoch:25 step:23504 [D loss: 0.666710, acc: 55.47%] [G loss: 1.689970]\n",
      "epoch:25 step:23505 [D loss: 0.741199, acc: 47.66%] [G loss: 1.729799]\n",
      "epoch:25 step:23506 [D loss: 0.712426, acc: 53.12%] [G loss: 1.788771]\n",
      "epoch:25 step:23507 [D loss: 0.658275, acc: 65.62%] [G loss: 1.771523]\n",
      "epoch:25 step:23508 [D loss: 0.644296, acc: 57.03%] [G loss: 1.804678]\n",
      "epoch:25 step:23509 [D loss: 0.684743, acc: 54.69%] [G loss: 1.813837]\n",
      "epoch:25 step:23510 [D loss: 0.650022, acc: 60.16%] [G loss: 1.755526]\n",
      "epoch:25 step:23511 [D loss: 0.694984, acc: 50.78%] [G loss: 1.667168]\n",
      "epoch:25 step:23512 [D loss: 0.653435, acc: 56.25%] [G loss: 1.834420]\n",
      "epoch:25 step:23513 [D loss: 0.694950, acc: 56.25%] [G loss: 1.674189]\n",
      "epoch:25 step:23514 [D loss: 0.674538, acc: 63.28%] [G loss: 1.793132]\n",
      "epoch:25 step:23515 [D loss: 0.639444, acc: 64.84%] [G loss: 1.783558]\n",
      "epoch:25 step:23516 [D loss: 0.651752, acc: 61.72%] [G loss: 1.904793]\n",
      "epoch:25 step:23517 [D loss: 0.605263, acc: 67.19%] [G loss: 1.901993]\n",
      "epoch:25 step:23518 [D loss: 0.632115, acc: 63.28%] [G loss: 1.925478]\n",
      "epoch:25 step:23519 [D loss: 0.655409, acc: 56.25%] [G loss: 1.830989]\n",
      "epoch:25 step:23520 [D loss: 0.623347, acc: 66.41%] [G loss: 1.848656]\n",
      "epoch:25 step:23521 [D loss: 0.635361, acc: 64.84%] [G loss: 1.882682]\n",
      "epoch:25 step:23522 [D loss: 0.627857, acc: 60.94%] [G loss: 1.972325]\n",
      "epoch:25 step:23523 [D loss: 0.700276, acc: 53.91%] [G loss: 1.826016]\n",
      "epoch:25 step:23524 [D loss: 0.622555, acc: 64.84%] [G loss: 1.749588]\n",
      "epoch:25 step:23525 [D loss: 0.654588, acc: 57.81%] [G loss: 1.931905]\n",
      "epoch:25 step:23526 [D loss: 0.626477, acc: 64.84%] [G loss: 1.873098]\n",
      "epoch:25 step:23527 [D loss: 0.686838, acc: 53.91%] [G loss: 1.879075]\n",
      "epoch:25 step:23528 [D loss: 0.646756, acc: 61.72%] [G loss: 1.984185]\n",
      "epoch:25 step:23529 [D loss: 0.662624, acc: 58.59%] [G loss: 1.818079]\n",
      "epoch:25 step:23530 [D loss: 0.660682, acc: 60.16%] [G loss: 1.839306]\n",
      "epoch:25 step:23531 [D loss: 0.685921, acc: 58.59%] [G loss: 1.940775]\n",
      "epoch:25 step:23532 [D loss: 0.629992, acc: 63.28%] [G loss: 2.041112]\n",
      "epoch:25 step:23533 [D loss: 0.706166, acc: 57.03%] [G loss: 1.720451]\n",
      "epoch:25 step:23534 [D loss: 0.679798, acc: 53.91%] [G loss: 1.714101]\n",
      "epoch:25 step:23535 [D loss: 0.651078, acc: 60.16%] [G loss: 1.796659]\n",
      "epoch:25 step:23536 [D loss: 0.635700, acc: 63.28%] [G loss: 1.951632]\n",
      "epoch:25 step:23537 [D loss: 0.614564, acc: 63.28%] [G loss: 2.009031]\n",
      "epoch:25 step:23538 [D loss: 0.643316, acc: 60.94%] [G loss: 2.008050]\n",
      "epoch:25 step:23539 [D loss: 0.582856, acc: 66.41%] [G loss: 2.055068]\n",
      "epoch:25 step:23540 [D loss: 0.607397, acc: 64.84%] [G loss: 2.148626]\n",
      "epoch:25 step:23541 [D loss: 0.586301, acc: 71.88%] [G loss: 2.213939]\n",
      "epoch:25 step:23542 [D loss: 0.632656, acc: 63.28%] [G loss: 2.094195]\n",
      "epoch:25 step:23543 [D loss: 0.613718, acc: 67.19%] [G loss: 2.152731]\n",
      "epoch:25 step:23544 [D loss: 0.579722, acc: 66.41%] [G loss: 2.218116]\n",
      "epoch:25 step:23545 [D loss: 0.638527, acc: 60.94%] [G loss: 1.944900]\n",
      "epoch:25 step:23546 [D loss: 0.698030, acc: 60.16%] [G loss: 1.954637]\n",
      "epoch:25 step:23547 [D loss: 0.642461, acc: 67.19%] [G loss: 2.129684]\n",
      "epoch:25 step:23548 [D loss: 0.712375, acc: 60.16%] [G loss: 1.912221]\n",
      "epoch:25 step:23549 [D loss: 0.670443, acc: 58.59%] [G loss: 1.884297]\n",
      "epoch:25 step:23550 [D loss: 0.709893, acc: 50.00%] [G loss: 1.819327]\n",
      "epoch:25 step:23551 [D loss: 0.663437, acc: 56.25%] [G loss: 1.881037]\n",
      "epoch:25 step:23552 [D loss: 0.685025, acc: 57.03%] [G loss: 1.782251]\n",
      "epoch:25 step:23553 [D loss: 0.677115, acc: 56.25%] [G loss: 1.821443]\n",
      "epoch:25 step:23554 [D loss: 0.569832, acc: 73.44%] [G loss: 1.959943]\n",
      "epoch:25 step:23555 [D loss: 0.673034, acc: 57.81%] [G loss: 1.753454]\n",
      "epoch:25 step:23556 [D loss: 0.658548, acc: 59.38%] [G loss: 1.851131]\n",
      "epoch:25 step:23557 [D loss: 0.674279, acc: 55.47%] [G loss: 1.862650]\n",
      "epoch:25 step:23558 [D loss: 0.694587, acc: 57.81%] [G loss: 1.686347]\n",
      "epoch:25 step:23559 [D loss: 0.685912, acc: 53.12%] [G loss: 1.848831]\n",
      "epoch:25 step:23560 [D loss: 0.662720, acc: 59.38%] [G loss: 1.759072]\n",
      "epoch:25 step:23561 [D loss: 0.699602, acc: 49.22%] [G loss: 1.822476]\n",
      "epoch:25 step:23562 [D loss: 0.640366, acc: 63.28%] [G loss: 1.648520]\n",
      "epoch:25 step:23563 [D loss: 0.718216, acc: 50.78%] [G loss: 1.777859]\n",
      "epoch:25 step:23564 [D loss: 0.648804, acc: 61.72%] [G loss: 1.818558]\n",
      "epoch:25 step:23565 [D loss: 0.652641, acc: 61.72%] [G loss: 1.765816]\n",
      "epoch:25 step:23566 [D loss: 0.662368, acc: 53.91%] [G loss: 1.849109]\n",
      "epoch:25 step:23567 [D loss: 0.685371, acc: 57.03%] [G loss: 1.756684]\n",
      "epoch:25 step:23568 [D loss: 0.687151, acc: 58.59%] [G loss: 1.852173]\n",
      "epoch:25 step:23569 [D loss: 0.638529, acc: 60.94%] [G loss: 1.909965]\n",
      "epoch:25 step:23570 [D loss: 0.658944, acc: 59.38%] [G loss: 1.786361]\n",
      "epoch:25 step:23571 [D loss: 0.621380, acc: 62.50%] [G loss: 2.084658]\n",
      "epoch:25 step:23572 [D loss: 0.687991, acc: 56.25%] [G loss: 1.741348]\n",
      "epoch:25 step:23573 [D loss: 0.683173, acc: 58.59%] [G loss: 1.674184]\n",
      "epoch:25 step:23574 [D loss: 0.628943, acc: 63.28%] [G loss: 1.907506]\n",
      "epoch:25 step:23575 [D loss: 0.635905, acc: 62.50%] [G loss: 1.970319]\n",
      "epoch:25 step:23576 [D loss: 0.622259, acc: 62.50%] [G loss: 1.918244]\n",
      "epoch:25 step:23577 [D loss: 0.608048, acc: 71.09%] [G loss: 1.904598]\n",
      "epoch:25 step:23578 [D loss: 0.679065, acc: 61.72%] [G loss: 1.772194]\n",
      "epoch:25 step:23579 [D loss: 0.615219, acc: 65.62%] [G loss: 1.982916]\n",
      "epoch:25 step:23580 [D loss: 0.668365, acc: 62.50%] [G loss: 1.788612]\n",
      "epoch:25 step:23581 [D loss: 0.716399, acc: 53.91%] [G loss: 1.815144]\n",
      "epoch:25 step:23582 [D loss: 0.659022, acc: 62.50%] [G loss: 1.705057]\n",
      "epoch:25 step:23583 [D loss: 0.673288, acc: 61.72%] [G loss: 1.757082]\n",
      "epoch:25 step:23584 [D loss: 0.675809, acc: 60.16%] [G loss: 1.794082]\n",
      "epoch:25 step:23585 [D loss: 0.692508, acc: 60.16%] [G loss: 1.666086]\n",
      "epoch:25 step:23586 [D loss: 0.679464, acc: 56.25%] [G loss: 1.829119]\n",
      "epoch:25 step:23587 [D loss: 0.653542, acc: 58.59%] [G loss: 1.802311]\n",
      "epoch:25 step:23588 [D loss: 0.602463, acc: 70.31%] [G loss: 1.744830]\n",
      "epoch:25 step:23589 [D loss: 0.626034, acc: 61.72%] [G loss: 1.697333]\n",
      "epoch:25 step:23590 [D loss: 0.666896, acc: 60.16%] [G loss: 1.872677]\n",
      "epoch:25 step:23591 [D loss: 0.654906, acc: 60.94%] [G loss: 1.892016]\n",
      "epoch:25 step:23592 [D loss: 0.669745, acc: 55.47%] [G loss: 1.856726]\n",
      "epoch:25 step:23593 [D loss: 0.647554, acc: 58.59%] [G loss: 1.797119]\n",
      "epoch:25 step:23594 [D loss: 0.666875, acc: 63.28%] [G loss: 1.876176]\n",
      "epoch:25 step:23595 [D loss: 0.663335, acc: 57.03%] [G loss: 1.815319]\n",
      "epoch:25 step:23596 [D loss: 0.600210, acc: 67.19%] [G loss: 1.919065]\n",
      "epoch:25 step:23597 [D loss: 0.626582, acc: 68.75%] [G loss: 1.862819]\n",
      "epoch:25 step:23598 [D loss: 0.659874, acc: 56.25%] [G loss: 1.777492]\n",
      "epoch:25 step:23599 [D loss: 0.678832, acc: 58.59%] [G loss: 1.778596]\n",
      "epoch:25 step:23600 [D loss: 0.631518, acc: 64.06%] [G loss: 1.894232]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.490294\n",
      "FID: 11.134083\n",
      "0 = 12.667971193504394\n",
      "1 = 0.08866103858103663\n",
      "2 = 0.8695999979972839\n",
      "3 = 0.894599974155426\n",
      "4 = 0.8446000218391418\n",
      "5 = 0.8519999980926514\n",
      "6 = 0.894599974155426\n",
      "7 = 6.173197543489936\n",
      "8 = 0.0647483767744171\n",
      "9 = 0.7002000212669373\n",
      "10 = 0.7146000266075134\n",
      "11 = 0.6858000159263611\n",
      "12 = 0.6945956349372864\n",
      "13 = 0.7146000266075134\n",
      "14 = 7.490325450897217\n",
      "15 = 9.406856536865234\n",
      "16 = 0.12155773490667343\n",
      "17 = 7.490293502807617\n",
      "18 = 11.134082794189453\n",
      "epoch:25 step:23601 [D loss: 0.686082, acc: 60.94%] [G loss: 1.884751]\n",
      "epoch:25 step:23602 [D loss: 0.644795, acc: 61.72%] [G loss: 1.889154]\n",
      "epoch:25 step:23603 [D loss: 0.659568, acc: 60.94%] [G loss: 1.776709]\n",
      "epoch:25 step:23604 [D loss: 0.652636, acc: 59.38%] [G loss: 1.855070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23605 [D loss: 0.689192, acc: 58.59%] [G loss: 1.785551]\n",
      "epoch:25 step:23606 [D loss: 0.728667, acc: 49.22%] [G loss: 1.700463]\n",
      "epoch:25 step:23607 [D loss: 0.641510, acc: 65.62%] [G loss: 1.706441]\n",
      "epoch:25 step:23608 [D loss: 0.671535, acc: 57.03%] [G loss: 1.850759]\n",
      "epoch:25 step:23609 [D loss: 0.657877, acc: 60.16%] [G loss: 1.747026]\n",
      "epoch:25 step:23610 [D loss: 0.629129, acc: 66.41%] [G loss: 1.751572]\n",
      "epoch:25 step:23611 [D loss: 0.631483, acc: 62.50%] [G loss: 1.908879]\n",
      "epoch:25 step:23612 [D loss: 0.650999, acc: 67.97%] [G loss: 1.814679]\n",
      "epoch:25 step:23613 [D loss: 0.604393, acc: 64.06%] [G loss: 1.874034]\n",
      "epoch:25 step:23614 [D loss: 0.653068, acc: 64.84%] [G loss: 1.940839]\n",
      "epoch:25 step:23615 [D loss: 0.699956, acc: 61.72%] [G loss: 1.792251]\n",
      "epoch:25 step:23616 [D loss: 0.658484, acc: 59.38%] [G loss: 1.859025]\n",
      "epoch:25 step:23617 [D loss: 0.616368, acc: 63.28%] [G loss: 1.863212]\n",
      "epoch:25 step:23618 [D loss: 0.677200, acc: 59.38%] [G loss: 1.763351]\n",
      "epoch:25 step:23619 [D loss: 0.628800, acc: 63.28%] [G loss: 1.978467]\n",
      "epoch:25 step:23620 [D loss: 0.593597, acc: 69.53%] [G loss: 1.948778]\n",
      "epoch:25 step:23621 [D loss: 0.626685, acc: 66.41%] [G loss: 1.853358]\n",
      "epoch:25 step:23622 [D loss: 0.574515, acc: 71.88%] [G loss: 1.939984]\n",
      "epoch:25 step:23623 [D loss: 0.639600, acc: 62.50%] [G loss: 1.877150]\n",
      "epoch:25 step:23624 [D loss: 0.644765, acc: 61.72%] [G loss: 2.045874]\n",
      "epoch:25 step:23625 [D loss: 0.686157, acc: 57.81%] [G loss: 1.785697]\n",
      "epoch:25 step:23626 [D loss: 0.647491, acc: 60.94%] [G loss: 1.893405]\n",
      "epoch:25 step:23627 [D loss: 0.659055, acc: 59.38%] [G loss: 1.906784]\n",
      "epoch:25 step:23628 [D loss: 0.608713, acc: 65.62%] [G loss: 1.948011]\n",
      "epoch:25 step:23629 [D loss: 0.686027, acc: 53.12%] [G loss: 1.882912]\n",
      "epoch:25 step:23630 [D loss: 0.648735, acc: 67.97%] [G loss: 1.820971]\n",
      "epoch:25 step:23631 [D loss: 0.564555, acc: 72.66%] [G loss: 1.993005]\n",
      "epoch:25 step:23632 [D loss: 0.648565, acc: 57.03%] [G loss: 2.027373]\n",
      "epoch:25 step:23633 [D loss: 0.612382, acc: 64.84%] [G loss: 2.191029]\n",
      "epoch:25 step:23634 [D loss: 0.564279, acc: 69.53%] [G loss: 2.117132]\n",
      "epoch:25 step:23635 [D loss: 0.678549, acc: 58.59%] [G loss: 1.804867]\n",
      "epoch:25 step:23636 [D loss: 0.669792, acc: 59.38%] [G loss: 1.878469]\n",
      "epoch:25 step:23637 [D loss: 0.679799, acc: 64.06%] [G loss: 1.759545]\n",
      "epoch:25 step:23638 [D loss: 0.651379, acc: 58.59%] [G loss: 1.892139]\n",
      "epoch:25 step:23639 [D loss: 0.662671, acc: 61.72%] [G loss: 1.647793]\n",
      "epoch:25 step:23640 [D loss: 0.667823, acc: 60.94%] [G loss: 1.847244]\n",
      "epoch:25 step:23641 [D loss: 0.624350, acc: 64.06%] [G loss: 1.986535]\n",
      "epoch:25 step:23642 [D loss: 0.654052, acc: 72.66%] [G loss: 1.861960]\n",
      "epoch:25 step:23643 [D loss: 0.584827, acc: 73.44%] [G loss: 1.861299]\n",
      "epoch:25 step:23644 [D loss: 0.567027, acc: 70.31%] [G loss: 2.136204]\n",
      "epoch:25 step:23645 [D loss: 0.697157, acc: 50.78%] [G loss: 1.725927]\n",
      "epoch:25 step:23646 [D loss: 0.654644, acc: 66.41%] [G loss: 1.906754]\n",
      "epoch:25 step:23647 [D loss: 0.623605, acc: 69.53%] [G loss: 1.877826]\n",
      "epoch:25 step:23648 [D loss: 0.633789, acc: 62.50%] [G loss: 1.864252]\n",
      "epoch:25 step:23649 [D loss: 0.657573, acc: 64.06%] [G loss: 1.807689]\n",
      "epoch:25 step:23650 [D loss: 0.619128, acc: 61.72%] [G loss: 1.872835]\n",
      "epoch:25 step:23651 [D loss: 0.713070, acc: 54.69%] [G loss: 1.810936]\n",
      "epoch:25 step:23652 [D loss: 0.620782, acc: 65.62%] [G loss: 1.850867]\n",
      "epoch:25 step:23653 [D loss: 0.648497, acc: 60.16%] [G loss: 1.734133]\n",
      "epoch:25 step:23654 [D loss: 0.621048, acc: 64.06%] [G loss: 2.008482]\n",
      "epoch:25 step:23655 [D loss: 0.600662, acc: 63.28%] [G loss: 2.056902]\n",
      "epoch:25 step:23656 [D loss: 0.553497, acc: 69.53%] [G loss: 2.264389]\n",
      "epoch:25 step:23657 [D loss: 0.625641, acc: 69.53%] [G loss: 2.373142]\n",
      "epoch:25 step:23658 [D loss: 0.669651, acc: 64.84%] [G loss: 1.859425]\n",
      "epoch:25 step:23659 [D loss: 0.685491, acc: 57.03%] [G loss: 1.878850]\n",
      "epoch:25 step:23660 [D loss: 0.656892, acc: 60.94%] [G loss: 1.860812]\n",
      "epoch:25 step:23661 [D loss: 0.618610, acc: 71.09%] [G loss: 1.827291]\n",
      "epoch:25 step:23662 [D loss: 0.692543, acc: 50.78%] [G loss: 1.745915]\n",
      "epoch:25 step:23663 [D loss: 0.638946, acc: 67.19%] [G loss: 1.907824]\n",
      "epoch:25 step:23664 [D loss: 0.672128, acc: 58.59%] [G loss: 1.818287]\n",
      "epoch:25 step:23665 [D loss: 0.626116, acc: 63.28%] [G loss: 1.902047]\n",
      "epoch:25 step:23666 [D loss: 0.630479, acc: 66.41%] [G loss: 1.986176]\n",
      "epoch:25 step:23667 [D loss: 0.598098, acc: 70.31%] [G loss: 2.020654]\n",
      "epoch:25 step:23668 [D loss: 0.684247, acc: 57.03%] [G loss: 1.997891]\n",
      "epoch:25 step:23669 [D loss: 0.600078, acc: 67.19%] [G loss: 2.013967]\n",
      "epoch:25 step:23670 [D loss: 0.619616, acc: 67.97%] [G loss: 2.065717]\n",
      "epoch:25 step:23671 [D loss: 0.650036, acc: 61.72%] [G loss: 1.954598]\n",
      "epoch:25 step:23672 [D loss: 0.695850, acc: 57.03%] [G loss: 1.946757]\n",
      "epoch:25 step:23673 [D loss: 0.597871, acc: 68.75%] [G loss: 1.998989]\n",
      "epoch:25 step:23674 [D loss: 0.710469, acc: 56.25%] [G loss: 1.816912]\n",
      "epoch:25 step:23675 [D loss: 0.734927, acc: 53.12%] [G loss: 1.718249]\n",
      "epoch:25 step:23676 [D loss: 0.671379, acc: 54.69%] [G loss: 1.764481]\n",
      "epoch:25 step:23677 [D loss: 0.707992, acc: 58.59%] [G loss: 1.795935]\n",
      "epoch:25 step:23678 [D loss: 0.628351, acc: 66.41%] [G loss: 1.758132]\n",
      "epoch:25 step:23679 [D loss: 0.671708, acc: 62.50%] [G loss: 1.845653]\n",
      "epoch:25 step:23680 [D loss: 0.628881, acc: 64.84%] [G loss: 1.925538]\n",
      "epoch:25 step:23681 [D loss: 0.663383, acc: 58.59%] [G loss: 1.818882]\n",
      "epoch:25 step:23682 [D loss: 0.654376, acc: 61.72%] [G loss: 1.876606]\n",
      "epoch:25 step:23683 [D loss: 0.635086, acc: 63.28%] [G loss: 1.878582]\n",
      "epoch:25 step:23684 [D loss: 0.646720, acc: 58.59%] [G loss: 1.812511]\n",
      "epoch:25 step:23685 [D loss: 0.658258, acc: 64.84%] [G loss: 1.922254]\n",
      "epoch:25 step:23686 [D loss: 0.646363, acc: 64.06%] [G loss: 1.890649]\n",
      "epoch:25 step:23687 [D loss: 0.616127, acc: 61.72%] [G loss: 1.864561]\n",
      "epoch:25 step:23688 [D loss: 0.634059, acc: 61.72%] [G loss: 1.827870]\n",
      "epoch:25 step:23689 [D loss: 0.649789, acc: 66.41%] [G loss: 1.955638]\n",
      "epoch:25 step:23690 [D loss: 0.682096, acc: 61.72%] [G loss: 1.896559]\n",
      "epoch:25 step:23691 [D loss: 0.650725, acc: 64.84%] [G loss: 1.813879]\n",
      "epoch:25 step:23692 [D loss: 0.720462, acc: 56.25%] [G loss: 1.852909]\n",
      "epoch:25 step:23693 [D loss: 0.650333, acc: 60.94%] [G loss: 1.752761]\n",
      "epoch:25 step:23694 [D loss: 0.610298, acc: 63.28%] [G loss: 1.855546]\n",
      "epoch:25 step:23695 [D loss: 0.587094, acc: 70.31%] [G loss: 1.974135]\n",
      "epoch:25 step:23696 [D loss: 0.607996, acc: 69.53%] [G loss: 1.944215]\n",
      "epoch:25 step:23697 [D loss: 0.616909, acc: 67.19%] [G loss: 1.945221]\n",
      "epoch:25 step:23698 [D loss: 0.666371, acc: 58.59%] [G loss: 1.820079]\n",
      "epoch:25 step:23699 [D loss: 0.593714, acc: 64.84%] [G loss: 2.116585]\n",
      "epoch:25 step:23700 [D loss: 0.616490, acc: 63.28%] [G loss: 1.980145]\n",
      "epoch:25 step:23701 [D loss: 0.627814, acc: 63.28%] [G loss: 1.926035]\n",
      "epoch:25 step:23702 [D loss: 0.681998, acc: 53.91%] [G loss: 1.750218]\n",
      "epoch:25 step:23703 [D loss: 0.663277, acc: 59.38%] [G loss: 1.821391]\n",
      "epoch:25 step:23704 [D loss: 0.668033, acc: 60.16%] [G loss: 1.781783]\n",
      "epoch:25 step:23705 [D loss: 0.657322, acc: 57.81%] [G loss: 1.855633]\n",
      "epoch:25 step:23706 [D loss: 0.694779, acc: 56.25%] [G loss: 1.822041]\n",
      "epoch:25 step:23707 [D loss: 0.661231, acc: 57.81%] [G loss: 1.824597]\n",
      "epoch:25 step:23708 [D loss: 0.657416, acc: 63.28%] [G loss: 1.731901]\n",
      "epoch:25 step:23709 [D loss: 0.690991, acc: 59.38%] [G loss: 1.836994]\n",
      "epoch:25 step:23710 [D loss: 0.653800, acc: 60.94%] [G loss: 1.755160]\n",
      "epoch:25 step:23711 [D loss: 0.577622, acc: 73.44%] [G loss: 1.990033]\n",
      "epoch:25 step:23712 [D loss: 0.670946, acc: 53.12%] [G loss: 1.908032]\n",
      "epoch:25 step:23713 [D loss: 0.645678, acc: 62.50%] [G loss: 2.132252]\n",
      "epoch:25 step:23714 [D loss: 0.610045, acc: 71.88%] [G loss: 1.882329]\n",
      "epoch:25 step:23715 [D loss: 0.698596, acc: 60.16%] [G loss: 1.848178]\n",
      "epoch:25 step:23716 [D loss: 0.679021, acc: 60.16%] [G loss: 1.735059]\n",
      "epoch:25 step:23717 [D loss: 0.677134, acc: 60.16%] [G loss: 1.824439]\n",
      "epoch:25 step:23718 [D loss: 0.622941, acc: 69.53%] [G loss: 2.030643]\n",
      "epoch:25 step:23719 [D loss: 0.653743, acc: 60.16%] [G loss: 1.681749]\n",
      "epoch:25 step:23720 [D loss: 0.680973, acc: 60.16%] [G loss: 1.872363]\n",
      "epoch:25 step:23721 [D loss: 0.641134, acc: 60.16%] [G loss: 1.852366]\n",
      "epoch:25 step:23722 [D loss: 0.630410, acc: 61.72%] [G loss: 1.829037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23723 [D loss: 0.611983, acc: 64.06%] [G loss: 2.157575]\n",
      "epoch:25 step:23724 [D loss: 0.611069, acc: 65.62%] [G loss: 1.960798]\n",
      "epoch:25 step:23725 [D loss: 0.670628, acc: 61.72%] [G loss: 2.003645]\n",
      "epoch:25 step:23726 [D loss: 0.648329, acc: 64.06%] [G loss: 1.762311]\n",
      "epoch:25 step:23727 [D loss: 0.593294, acc: 71.09%] [G loss: 2.013846]\n",
      "epoch:25 step:23728 [D loss: 0.692148, acc: 58.59%] [G loss: 1.706727]\n",
      "epoch:25 step:23729 [D loss: 0.628394, acc: 67.19%] [G loss: 1.923166]\n",
      "epoch:25 step:23730 [D loss: 0.622681, acc: 67.97%] [G loss: 1.886158]\n",
      "epoch:25 step:23731 [D loss: 0.677504, acc: 55.47%] [G loss: 1.840526]\n",
      "epoch:25 step:23732 [D loss: 0.598322, acc: 68.75%] [G loss: 1.862275]\n",
      "epoch:25 step:23733 [D loss: 0.664691, acc: 60.16%] [G loss: 1.712354]\n",
      "epoch:25 step:23734 [D loss: 0.678638, acc: 57.03%] [G loss: 1.865495]\n",
      "epoch:25 step:23735 [D loss: 0.689500, acc: 57.81%] [G loss: 1.905291]\n",
      "epoch:25 step:23736 [D loss: 0.628676, acc: 62.50%] [G loss: 1.934982]\n",
      "epoch:25 step:23737 [D loss: 0.572723, acc: 68.75%] [G loss: 2.062607]\n",
      "epoch:25 step:23738 [D loss: 0.594213, acc: 69.53%] [G loss: 2.056096]\n",
      "epoch:25 step:23739 [D loss: 0.618214, acc: 64.84%] [G loss: 2.141687]\n",
      "epoch:25 step:23740 [D loss: 0.576792, acc: 70.31%] [G loss: 2.178778]\n",
      "epoch:25 step:23741 [D loss: 0.722204, acc: 52.34%] [G loss: 1.705951]\n",
      "epoch:25 step:23742 [D loss: 0.690721, acc: 55.47%] [G loss: 1.777485]\n",
      "epoch:25 step:23743 [D loss: 0.650413, acc: 60.94%] [G loss: 1.922343]\n",
      "epoch:25 step:23744 [D loss: 0.658861, acc: 57.03%] [G loss: 1.882438]\n",
      "epoch:25 step:23745 [D loss: 0.646237, acc: 63.28%] [G loss: 1.901231]\n",
      "epoch:25 step:23746 [D loss: 0.603321, acc: 67.97%] [G loss: 1.906806]\n",
      "epoch:25 step:23747 [D loss: 0.651571, acc: 59.38%] [G loss: 1.839696]\n",
      "epoch:25 step:23748 [D loss: 0.645578, acc: 66.41%] [G loss: 1.838102]\n",
      "epoch:25 step:23749 [D loss: 0.678261, acc: 58.59%] [G loss: 1.716129]\n",
      "epoch:25 step:23750 [D loss: 0.639641, acc: 60.94%] [G loss: 1.792752]\n",
      "epoch:25 step:23751 [D loss: 0.621786, acc: 63.28%] [G loss: 1.787267]\n",
      "epoch:25 step:23752 [D loss: 0.640659, acc: 62.50%] [G loss: 1.730704]\n",
      "epoch:25 step:23753 [D loss: 0.598727, acc: 68.75%] [G loss: 1.859033]\n",
      "epoch:25 step:23754 [D loss: 0.645991, acc: 60.16%] [G loss: 1.995438]\n",
      "epoch:25 step:23755 [D loss: 0.597445, acc: 73.44%] [G loss: 2.097601]\n",
      "epoch:25 step:23756 [D loss: 0.619975, acc: 69.53%] [G loss: 1.923939]\n",
      "epoch:25 step:23757 [D loss: 0.615426, acc: 65.62%] [G loss: 1.930252]\n",
      "epoch:25 step:23758 [D loss: 0.585795, acc: 70.31%] [G loss: 1.949894]\n",
      "epoch:25 step:23759 [D loss: 0.673490, acc: 60.16%] [G loss: 1.979120]\n",
      "epoch:25 step:23760 [D loss: 0.624176, acc: 61.72%] [G loss: 2.001653]\n",
      "epoch:25 step:23761 [D loss: 0.621150, acc: 68.75%] [G loss: 1.970821]\n",
      "epoch:25 step:23762 [D loss: 0.611424, acc: 65.62%] [G loss: 1.943295]\n",
      "epoch:25 step:23763 [D loss: 0.623768, acc: 64.06%] [G loss: 1.941453]\n",
      "epoch:25 step:23764 [D loss: 0.575319, acc: 70.31%] [G loss: 2.194229]\n",
      "epoch:25 step:23765 [D loss: 0.604337, acc: 67.97%] [G loss: 1.914880]\n",
      "epoch:25 step:23766 [D loss: 0.703229, acc: 50.78%] [G loss: 1.843026]\n",
      "epoch:25 step:23767 [D loss: 0.686447, acc: 57.81%] [G loss: 1.969792]\n",
      "epoch:25 step:23768 [D loss: 0.635936, acc: 65.62%] [G loss: 1.863996]\n",
      "epoch:25 step:23769 [D loss: 0.653803, acc: 60.94%] [G loss: 1.924150]\n",
      "epoch:25 step:23770 [D loss: 0.651315, acc: 60.94%] [G loss: 2.018475]\n",
      "epoch:25 step:23771 [D loss: 0.637326, acc: 66.41%] [G loss: 2.165194]\n",
      "epoch:25 step:23772 [D loss: 0.546709, acc: 74.22%] [G loss: 2.286855]\n",
      "epoch:25 step:23773 [D loss: 0.636426, acc: 62.50%] [G loss: 1.929799]\n",
      "epoch:25 step:23774 [D loss: 0.728424, acc: 53.12%] [G loss: 1.772532]\n",
      "epoch:25 step:23775 [D loss: 0.612833, acc: 64.06%] [G loss: 1.913322]\n",
      "epoch:25 step:23776 [D loss: 0.635310, acc: 60.94%] [G loss: 1.924346]\n",
      "epoch:25 step:23777 [D loss: 0.687153, acc: 60.94%] [G loss: 1.871595]\n",
      "epoch:25 step:23778 [D loss: 0.682187, acc: 58.59%] [G loss: 2.082865]\n",
      "epoch:25 step:23779 [D loss: 0.668592, acc: 64.06%] [G loss: 2.014542]\n",
      "epoch:25 step:23780 [D loss: 0.713869, acc: 54.69%] [G loss: 1.809381]\n",
      "epoch:25 step:23781 [D loss: 0.721902, acc: 49.22%] [G loss: 1.784101]\n",
      "epoch:25 step:23782 [D loss: 0.684622, acc: 57.81%] [G loss: 1.797363]\n",
      "epoch:25 step:23783 [D loss: 0.598045, acc: 68.75%] [G loss: 1.912856]\n",
      "epoch:25 step:23784 [D loss: 0.644024, acc: 61.72%] [G loss: 2.019922]\n",
      "epoch:25 step:23785 [D loss: 0.660470, acc: 64.06%] [G loss: 1.924842]\n",
      "epoch:25 step:23786 [D loss: 0.633210, acc: 61.72%] [G loss: 1.895435]\n",
      "epoch:25 step:23787 [D loss: 0.605393, acc: 67.19%] [G loss: 1.808924]\n",
      "epoch:25 step:23788 [D loss: 0.662875, acc: 59.38%] [G loss: 1.899171]\n",
      "epoch:25 step:23789 [D loss: 0.671807, acc: 58.59%] [G loss: 1.949847]\n",
      "epoch:25 step:23790 [D loss: 0.660386, acc: 60.94%] [G loss: 1.915204]\n",
      "epoch:25 step:23791 [D loss: 0.642674, acc: 64.84%] [G loss: 2.040382]\n",
      "epoch:25 step:23792 [D loss: 0.690623, acc: 51.56%] [G loss: 1.939963]\n",
      "epoch:25 step:23793 [D loss: 0.609792, acc: 64.84%] [G loss: 1.821442]\n",
      "epoch:25 step:23794 [D loss: 0.661287, acc: 66.41%] [G loss: 1.958510]\n",
      "epoch:25 step:23795 [D loss: 0.635847, acc: 61.72%] [G loss: 1.964640]\n",
      "epoch:25 step:23796 [D loss: 0.644602, acc: 58.59%] [G loss: 2.091630]\n",
      "epoch:25 step:23797 [D loss: 0.629270, acc: 64.84%] [G loss: 2.049911]\n",
      "epoch:25 step:23798 [D loss: 0.657044, acc: 60.94%] [G loss: 1.681962]\n",
      "epoch:25 step:23799 [D loss: 0.614441, acc: 68.75%] [G loss: 1.978211]\n",
      "epoch:25 step:23800 [D loss: 0.659835, acc: 60.16%] [G loss: 1.798000]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.660254\n",
      "FID: 8.450068\n",
      "0 = 12.83843124656679\n",
      "1 = 0.099369133350255\n",
      "2 = 0.8651999831199646\n",
      "3 = 0.8967999815940857\n",
      "4 = 0.8335999846458435\n",
      "5 = 0.8434913754463196\n",
      "6 = 0.8967999815940857\n",
      "7 = 5.966668713235851\n",
      "8 = 0.05553219887046758\n",
      "9 = 0.6983000040054321\n",
      "10 = 0.7167999744415283\n",
      "11 = 0.6797999739646912\n",
      "12 = 0.6912246942520142\n",
      "13 = 0.7167999744415283\n",
      "14 = 7.660282611846924\n",
      "15 = 9.429621696472168\n",
      "16 = 0.1107492744922638\n",
      "17 = 7.66025447845459\n",
      "18 = 8.450067520141602\n",
      "epoch:25 step:23801 [D loss: 0.649497, acc: 60.16%] [G loss: 1.886514]\n",
      "epoch:25 step:23802 [D loss: 0.709053, acc: 51.56%] [G loss: 1.635113]\n",
      "epoch:25 step:23803 [D loss: 0.644382, acc: 62.50%] [G loss: 1.878777]\n",
      "epoch:25 step:23804 [D loss: 0.629420, acc: 62.50%] [G loss: 1.921323]\n",
      "epoch:25 step:23805 [D loss: 0.620756, acc: 65.62%] [G loss: 1.770007]\n",
      "epoch:25 step:23806 [D loss: 0.642427, acc: 63.28%] [G loss: 1.907892]\n",
      "epoch:25 step:23807 [D loss: 0.622829, acc: 67.97%] [G loss: 1.848613]\n",
      "epoch:25 step:23808 [D loss: 0.659266, acc: 64.06%] [G loss: 1.877161]\n",
      "epoch:25 step:23809 [D loss: 0.662285, acc: 57.81%] [G loss: 1.907714]\n",
      "epoch:25 step:23810 [D loss: 0.686255, acc: 60.94%] [G loss: 1.903454]\n",
      "epoch:25 step:23811 [D loss: 0.673685, acc: 57.81%] [G loss: 1.940997]\n",
      "epoch:25 step:23812 [D loss: 0.663042, acc: 58.59%] [G loss: 1.851963]\n",
      "epoch:25 step:23813 [D loss: 0.662379, acc: 59.38%] [G loss: 1.722659]\n",
      "epoch:25 step:23814 [D loss: 0.604487, acc: 64.84%] [G loss: 1.746978]\n",
      "epoch:25 step:23815 [D loss: 0.646484, acc: 64.06%] [G loss: 1.795732]\n",
      "epoch:25 step:23816 [D loss: 0.716305, acc: 57.03%] [G loss: 1.791676]\n",
      "epoch:25 step:23817 [D loss: 0.686237, acc: 56.25%] [G loss: 1.760819]\n",
      "epoch:25 step:23818 [D loss: 0.676248, acc: 57.03%] [G loss: 1.786826]\n",
      "epoch:25 step:23819 [D loss: 0.691349, acc: 57.81%] [G loss: 1.849310]\n",
      "epoch:25 step:23820 [D loss: 0.652729, acc: 67.19%] [G loss: 1.885647]\n",
      "epoch:25 step:23821 [D loss: 0.660033, acc: 57.81%] [G loss: 1.852286]\n",
      "epoch:25 step:23822 [D loss: 0.646246, acc: 61.72%] [G loss: 1.802340]\n",
      "epoch:25 step:23823 [D loss: 0.631097, acc: 71.09%] [G loss: 1.786310]\n",
      "epoch:25 step:23824 [D loss: 0.679291, acc: 59.38%] [G loss: 1.770920]\n",
      "epoch:25 step:23825 [D loss: 0.682975, acc: 60.94%] [G loss: 1.864259]\n",
      "epoch:25 step:23826 [D loss: 0.656951, acc: 60.16%] [G loss: 1.878302]\n",
      "epoch:25 step:23827 [D loss: 0.650739, acc: 65.62%] [G loss: 2.050932]\n",
      "epoch:25 step:23828 [D loss: 0.658859, acc: 59.38%] [G loss: 1.927156]\n",
      "epoch:25 step:23829 [D loss: 0.602615, acc: 67.19%] [G loss: 1.908668]\n",
      "epoch:25 step:23830 [D loss: 0.609451, acc: 66.41%] [G loss: 2.045970]\n",
      "epoch:25 step:23831 [D loss: 0.629591, acc: 67.19%] [G loss: 1.978034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23832 [D loss: 0.656898, acc: 57.81%] [G loss: 1.891397]\n",
      "epoch:25 step:23833 [D loss: 0.705119, acc: 59.38%] [G loss: 1.892213]\n",
      "epoch:25 step:23834 [D loss: 0.656704, acc: 57.81%] [G loss: 1.826489]\n",
      "epoch:25 step:23835 [D loss: 0.646989, acc: 61.72%] [G loss: 1.967493]\n",
      "epoch:25 step:23836 [D loss: 0.663460, acc: 60.16%] [G loss: 1.915564]\n",
      "epoch:25 step:23837 [D loss: 0.694168, acc: 57.03%] [G loss: 1.860310]\n",
      "epoch:25 step:23838 [D loss: 0.654521, acc: 59.38%] [G loss: 1.858571]\n",
      "epoch:25 step:23839 [D loss: 0.661050, acc: 60.94%] [G loss: 2.000242]\n",
      "epoch:25 step:23840 [D loss: 0.620599, acc: 65.62%] [G loss: 1.926880]\n",
      "epoch:25 step:23841 [D loss: 0.650788, acc: 62.50%] [G loss: 2.087923]\n",
      "epoch:25 step:23842 [D loss: 0.648593, acc: 58.59%] [G loss: 1.887396]\n",
      "epoch:25 step:23843 [D loss: 0.652619, acc: 63.28%] [G loss: 1.824749]\n",
      "epoch:25 step:23844 [D loss: 0.627946, acc: 64.06%] [G loss: 1.851484]\n",
      "epoch:25 step:23845 [D loss: 0.644456, acc: 58.59%] [G loss: 1.924925]\n",
      "epoch:25 step:23846 [D loss: 0.663899, acc: 65.62%] [G loss: 1.777439]\n",
      "epoch:25 step:23847 [D loss: 0.729886, acc: 53.91%] [G loss: 1.791102]\n",
      "epoch:25 step:23848 [D loss: 0.676348, acc: 56.25%] [G loss: 1.700098]\n",
      "epoch:25 step:23849 [D loss: 0.746271, acc: 50.78%] [G loss: 1.830636]\n",
      "epoch:25 step:23850 [D loss: 0.662836, acc: 60.16%] [G loss: 1.696435]\n",
      "epoch:25 step:23851 [D loss: 0.660748, acc: 55.47%] [G loss: 1.895402]\n",
      "epoch:25 step:23852 [D loss: 0.600731, acc: 69.53%] [G loss: 1.956007]\n",
      "epoch:25 step:23853 [D loss: 0.617123, acc: 67.97%] [G loss: 1.985674]\n",
      "epoch:25 step:23854 [D loss: 0.629669, acc: 63.28%] [G loss: 1.996597]\n",
      "epoch:25 step:23855 [D loss: 0.650740, acc: 66.41%] [G loss: 1.915682]\n",
      "epoch:25 step:23856 [D loss: 0.668853, acc: 57.81%] [G loss: 1.840446]\n",
      "epoch:25 step:23857 [D loss: 0.650300, acc: 64.06%] [G loss: 1.832802]\n",
      "epoch:25 step:23858 [D loss: 0.687877, acc: 56.25%] [G loss: 1.722667]\n",
      "epoch:25 step:23859 [D loss: 0.650705, acc: 63.28%] [G loss: 1.972131]\n",
      "epoch:25 step:23860 [D loss: 0.633281, acc: 64.06%] [G loss: 1.976465]\n",
      "epoch:25 step:23861 [D loss: 0.625416, acc: 67.19%] [G loss: 1.980529]\n",
      "epoch:25 step:23862 [D loss: 0.749546, acc: 46.09%] [G loss: 1.749577]\n",
      "epoch:25 step:23863 [D loss: 0.638602, acc: 62.50%] [G loss: 1.728232]\n",
      "epoch:25 step:23864 [D loss: 0.704027, acc: 57.81%] [G loss: 1.871367]\n",
      "epoch:25 step:23865 [D loss: 0.664160, acc: 62.50%] [G loss: 1.702667]\n",
      "epoch:25 step:23866 [D loss: 0.658448, acc: 57.03%] [G loss: 1.816128]\n",
      "epoch:25 step:23867 [D loss: 0.625097, acc: 64.84%] [G loss: 1.989135]\n",
      "epoch:25 step:23868 [D loss: 0.635702, acc: 62.50%] [G loss: 1.907191]\n",
      "epoch:25 step:23869 [D loss: 0.687331, acc: 61.72%] [G loss: 1.803592]\n",
      "epoch:25 step:23870 [D loss: 0.627192, acc: 63.28%] [G loss: 1.795241]\n",
      "epoch:25 step:23871 [D loss: 0.620241, acc: 61.72%] [G loss: 1.854883]\n",
      "epoch:25 step:23872 [D loss: 0.639087, acc: 69.53%] [G loss: 1.824550]\n",
      "epoch:25 step:23873 [D loss: 0.702390, acc: 56.25%] [G loss: 1.851001]\n",
      "epoch:25 step:23874 [D loss: 0.643480, acc: 63.28%] [G loss: 1.795123]\n",
      "epoch:25 step:23875 [D loss: 0.629128, acc: 64.84%] [G loss: 1.750961]\n",
      "epoch:25 step:23876 [D loss: 0.650135, acc: 62.50%] [G loss: 1.796002]\n",
      "epoch:25 step:23877 [D loss: 0.637269, acc: 61.72%] [G loss: 1.788062]\n",
      "epoch:25 step:23878 [D loss: 0.644327, acc: 60.94%] [G loss: 1.906616]\n",
      "epoch:25 step:23879 [D loss: 0.610932, acc: 66.41%] [G loss: 1.886199]\n",
      "epoch:25 step:23880 [D loss: 0.642795, acc: 65.62%] [G loss: 1.860685]\n",
      "epoch:25 step:23881 [D loss: 0.622797, acc: 64.06%] [G loss: 1.906476]\n",
      "epoch:25 step:23882 [D loss: 0.645880, acc: 64.84%] [G loss: 1.900775]\n",
      "epoch:25 step:23883 [D loss: 0.706292, acc: 57.81%] [G loss: 1.649188]\n",
      "epoch:25 step:23884 [D loss: 0.724116, acc: 49.22%] [G loss: 1.816266]\n",
      "epoch:25 step:23885 [D loss: 0.662014, acc: 64.06%] [G loss: 1.773214]\n",
      "epoch:25 step:23886 [D loss: 0.660042, acc: 59.38%] [G loss: 1.725414]\n",
      "epoch:25 step:23887 [D loss: 0.610705, acc: 71.09%] [G loss: 1.830339]\n",
      "epoch:25 step:23888 [D loss: 0.709240, acc: 60.94%] [G loss: 1.863891]\n",
      "epoch:25 step:23889 [D loss: 0.588995, acc: 68.75%] [G loss: 1.788111]\n",
      "epoch:25 step:23890 [D loss: 0.667829, acc: 62.50%] [G loss: 1.846415]\n",
      "epoch:25 step:23891 [D loss: 0.702860, acc: 58.59%] [G loss: 1.885185]\n",
      "epoch:25 step:23892 [D loss: 0.655563, acc: 60.16%] [G loss: 1.918307]\n",
      "epoch:25 step:23893 [D loss: 0.656867, acc: 60.16%] [G loss: 1.980390]\n",
      "epoch:25 step:23894 [D loss: 0.611119, acc: 71.09%] [G loss: 1.978605]\n",
      "epoch:25 step:23895 [D loss: 0.661389, acc: 63.28%] [G loss: 1.840024]\n",
      "epoch:25 step:23896 [D loss: 0.608540, acc: 68.75%] [G loss: 2.058165]\n",
      "epoch:25 step:23897 [D loss: 0.591583, acc: 66.41%] [G loss: 2.025566]\n",
      "epoch:25 step:23898 [D loss: 0.706746, acc: 51.56%] [G loss: 1.869614]\n",
      "epoch:25 step:23899 [D loss: 0.674846, acc: 60.16%] [G loss: 1.887825]\n",
      "epoch:25 step:23900 [D loss: 0.686561, acc: 59.38%] [G loss: 1.823380]\n",
      "epoch:25 step:23901 [D loss: 0.646198, acc: 64.06%] [G loss: 1.947764]\n",
      "epoch:25 step:23902 [D loss: 0.718093, acc: 49.22%] [G loss: 1.708296]\n",
      "epoch:25 step:23903 [D loss: 0.688272, acc: 55.47%] [G loss: 1.858200]\n",
      "epoch:25 step:23904 [D loss: 0.646529, acc: 64.06%] [G loss: 1.900128]\n",
      "epoch:25 step:23905 [D loss: 0.589783, acc: 67.19%] [G loss: 1.922845]\n",
      "epoch:25 step:23906 [D loss: 0.557687, acc: 75.78%] [G loss: 2.185000]\n",
      "epoch:25 step:23907 [D loss: 0.672512, acc: 61.72%] [G loss: 1.841566]\n",
      "epoch:25 step:23908 [D loss: 0.712797, acc: 52.34%] [G loss: 1.791271]\n",
      "epoch:25 step:23909 [D loss: 0.666167, acc: 63.28%] [G loss: 2.029224]\n",
      "epoch:25 step:23910 [D loss: 0.642708, acc: 68.75%] [G loss: 1.777060]\n",
      "epoch:25 step:23911 [D loss: 0.626774, acc: 67.19%] [G loss: 1.838645]\n",
      "epoch:25 step:23912 [D loss: 0.622756, acc: 65.62%] [G loss: 1.843863]\n",
      "epoch:25 step:23913 [D loss: 0.601776, acc: 67.97%] [G loss: 1.997258]\n",
      "epoch:25 step:23914 [D loss: 0.640498, acc: 60.16%] [G loss: 1.814450]\n",
      "epoch:25 step:23915 [D loss: 0.625105, acc: 64.84%] [G loss: 1.840355]\n",
      "epoch:25 step:23916 [D loss: 0.635428, acc: 62.50%] [G loss: 1.909427]\n",
      "epoch:25 step:23917 [D loss: 0.657693, acc: 58.59%] [G loss: 1.932020]\n",
      "epoch:25 step:23918 [D loss: 0.640332, acc: 63.28%] [G loss: 1.781704]\n",
      "epoch:25 step:23919 [D loss: 0.669300, acc: 58.59%] [G loss: 1.922314]\n",
      "epoch:25 step:23920 [D loss: 0.634755, acc: 59.38%] [G loss: 2.062286]\n",
      "epoch:25 step:23921 [D loss: 0.604963, acc: 69.53%] [G loss: 1.835988]\n",
      "epoch:25 step:23922 [D loss: 0.689843, acc: 58.59%] [G loss: 1.916689]\n",
      "epoch:25 step:23923 [D loss: 0.612928, acc: 63.28%] [G loss: 1.971751]\n",
      "epoch:25 step:23924 [D loss: 0.606553, acc: 70.31%] [G loss: 2.046498]\n",
      "epoch:25 step:23925 [D loss: 0.670659, acc: 57.81%] [G loss: 1.744694]\n",
      "epoch:25 step:23926 [D loss: 0.656619, acc: 62.50%] [G loss: 1.655726]\n",
      "epoch:25 step:23927 [D loss: 0.696072, acc: 52.34%] [G loss: 1.603972]\n",
      "epoch:25 step:23928 [D loss: 0.671995, acc: 59.38%] [G loss: 1.851581]\n",
      "epoch:25 step:23929 [D loss: 0.619640, acc: 68.75%] [G loss: 1.939915]\n",
      "epoch:25 step:23930 [D loss: 0.645885, acc: 64.06%] [G loss: 1.935274]\n",
      "epoch:25 step:23931 [D loss: 0.671361, acc: 61.72%] [G loss: 1.723520]\n",
      "epoch:25 step:23932 [D loss: 0.679839, acc: 60.16%] [G loss: 1.837095]\n",
      "epoch:25 step:23933 [D loss: 0.603068, acc: 67.19%] [G loss: 2.028577]\n",
      "epoch:25 step:23934 [D loss: 0.666552, acc: 62.50%] [G loss: 1.821769]\n",
      "epoch:25 step:23935 [D loss: 0.631749, acc: 67.97%] [G loss: 1.864674]\n",
      "epoch:25 step:23936 [D loss: 0.673698, acc: 53.12%] [G loss: 1.772882]\n",
      "epoch:25 step:23937 [D loss: 0.621940, acc: 60.94%] [G loss: 1.789507]\n",
      "epoch:25 step:23938 [D loss: 0.641848, acc: 61.72%] [G loss: 1.847580]\n",
      "epoch:25 step:23939 [D loss: 0.591823, acc: 70.31%] [G loss: 1.793442]\n",
      "epoch:25 step:23940 [D loss: 0.613020, acc: 65.62%] [G loss: 2.076840]\n",
      "epoch:25 step:23941 [D loss: 0.586719, acc: 68.75%] [G loss: 1.987183]\n",
      "epoch:25 step:23942 [D loss: 0.637746, acc: 68.75%] [G loss: 2.033330]\n",
      "epoch:25 step:23943 [D loss: 0.675622, acc: 60.94%] [G loss: 1.830218]\n",
      "epoch:25 step:23944 [D loss: 0.626721, acc: 63.28%] [G loss: 1.947227]\n",
      "epoch:25 step:23945 [D loss: 0.581908, acc: 67.19%] [G loss: 2.053634]\n",
      "epoch:25 step:23946 [D loss: 0.600431, acc: 66.41%] [G loss: 2.032514]\n",
      "epoch:25 step:23947 [D loss: 0.618761, acc: 66.41%] [G loss: 2.085563]\n",
      "epoch:25 step:23948 [D loss: 0.611945, acc: 67.19%] [G loss: 2.100003]\n",
      "epoch:25 step:23949 [D loss: 0.633876, acc: 66.41%] [G loss: 2.057439]\n",
      "epoch:25 step:23950 [D loss: 0.682777, acc: 61.72%] [G loss: 1.936615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23951 [D loss: 0.648510, acc: 56.25%] [G loss: 2.016706]\n",
      "epoch:25 step:23952 [D loss: 0.671716, acc: 57.81%] [G loss: 1.812533]\n",
      "epoch:25 step:23953 [D loss: 0.708058, acc: 57.03%] [G loss: 1.710560]\n",
      "epoch:25 step:23954 [D loss: 0.738932, acc: 50.00%] [G loss: 1.703423]\n",
      "epoch:25 step:23955 [D loss: 0.619132, acc: 62.50%] [G loss: 1.760504]\n",
      "epoch:25 step:23956 [D loss: 0.684104, acc: 58.59%] [G loss: 1.746085]\n",
      "epoch:25 step:23957 [D loss: 0.630367, acc: 67.97%] [G loss: 1.906842]\n",
      "epoch:25 step:23958 [D loss: 0.641023, acc: 59.38%] [G loss: 1.962982]\n",
      "epoch:25 step:23959 [D loss: 0.622405, acc: 64.84%] [G loss: 2.003894]\n",
      "epoch:25 step:23960 [D loss: 0.643314, acc: 62.50%] [G loss: 1.792865]\n",
      "epoch:25 step:23961 [D loss: 0.594569, acc: 71.09%] [G loss: 2.096500]\n",
      "epoch:25 step:23962 [D loss: 0.718962, acc: 54.69%] [G loss: 1.787798]\n",
      "epoch:25 step:23963 [D loss: 0.690269, acc: 60.94%] [G loss: 1.753203]\n",
      "epoch:25 step:23964 [D loss: 0.607131, acc: 65.62%] [G loss: 1.770367]\n",
      "epoch:25 step:23965 [D loss: 0.642641, acc: 64.06%] [G loss: 1.747594]\n",
      "epoch:25 step:23966 [D loss: 0.644585, acc: 67.97%] [G loss: 1.728148]\n",
      "epoch:25 step:23967 [D loss: 0.697062, acc: 53.91%] [G loss: 1.814618]\n",
      "epoch:25 step:23968 [D loss: 0.696284, acc: 59.38%] [G loss: 1.726476]\n",
      "epoch:25 step:23969 [D loss: 0.639900, acc: 63.28%] [G loss: 1.931655]\n",
      "epoch:25 step:23970 [D loss: 0.651874, acc: 60.16%] [G loss: 1.927630]\n",
      "epoch:25 step:23971 [D loss: 0.651403, acc: 63.28%] [G loss: 1.842072]\n",
      "epoch:25 step:23972 [D loss: 0.622537, acc: 62.50%] [G loss: 1.911149]\n",
      "epoch:25 step:23973 [D loss: 0.597883, acc: 65.62%] [G loss: 1.917611]\n",
      "epoch:25 step:23974 [D loss: 0.652244, acc: 61.72%] [G loss: 1.942735]\n",
      "epoch:25 step:23975 [D loss: 0.630425, acc: 66.41%] [G loss: 1.941403]\n",
      "epoch:25 step:23976 [D loss: 0.622238, acc: 64.84%] [G loss: 1.993573]\n",
      "epoch:25 step:23977 [D loss: 0.611831, acc: 74.22%] [G loss: 1.954210]\n",
      "epoch:25 step:23978 [D loss: 0.733370, acc: 52.34%] [G loss: 1.685612]\n",
      "epoch:25 step:23979 [D loss: 0.601165, acc: 66.41%] [G loss: 2.064476]\n",
      "epoch:25 step:23980 [D loss: 0.656354, acc: 61.72%] [G loss: 1.957335]\n",
      "epoch:25 step:23981 [D loss: 0.647471, acc: 59.38%] [G loss: 1.932106]\n",
      "epoch:25 step:23982 [D loss: 0.607717, acc: 66.41%] [G loss: 2.004773]\n",
      "epoch:25 step:23983 [D loss: 0.648013, acc: 65.62%] [G loss: 1.754981]\n",
      "epoch:25 step:23984 [D loss: 0.704787, acc: 54.69%] [G loss: 1.708233]\n",
      "epoch:25 step:23985 [D loss: 0.710167, acc: 50.78%] [G loss: 1.728690]\n",
      "epoch:25 step:23986 [D loss: 0.619614, acc: 63.28%] [G loss: 1.933962]\n",
      "epoch:25 step:23987 [D loss: 0.617579, acc: 67.19%] [G loss: 1.797180]\n",
      "epoch:25 step:23988 [D loss: 0.647155, acc: 63.28%] [G loss: 1.986659]\n",
      "epoch:25 step:23989 [D loss: 0.581297, acc: 67.97%] [G loss: 1.947945]\n",
      "epoch:25 step:23990 [D loss: 0.653445, acc: 58.59%] [G loss: 1.770742]\n",
      "epoch:25 step:23991 [D loss: 0.680316, acc: 60.16%] [G loss: 1.695265]\n",
      "epoch:25 step:23992 [D loss: 0.660313, acc: 57.03%] [G loss: 1.914964]\n",
      "epoch:25 step:23993 [D loss: 0.704274, acc: 53.12%] [G loss: 1.711332]\n",
      "epoch:25 step:23994 [D loss: 0.666943, acc: 56.25%] [G loss: 1.716386]\n",
      "epoch:25 step:23995 [D loss: 0.682986, acc: 54.69%] [G loss: 1.741623]\n",
      "epoch:25 step:23996 [D loss: 0.638346, acc: 66.41%] [G loss: 1.853121]\n",
      "epoch:25 step:23997 [D loss: 0.665501, acc: 57.03%] [G loss: 1.736636]\n",
      "epoch:25 step:23998 [D loss: 0.646577, acc: 60.94%] [G loss: 1.713310]\n",
      "epoch:25 step:23999 [D loss: 0.674333, acc: 58.59%] [G loss: 1.828646]\n",
      "epoch:25 step:24000 [D loss: 0.658154, acc: 64.06%] [G loss: 1.817975]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.513245\n",
      "FID: 11.019442\n",
      "0 = 12.504687117147476\n",
      "1 = 0.07936936245820225\n",
      "2 = 0.8730000257492065\n",
      "3 = 0.8870000243186951\n",
      "4 = 0.859000027179718\n",
      "5 = 0.862840473651886\n",
      "6 = 0.8870000243186951\n",
      "7 = 6.12486278461217\n",
      "8 = 0.06392662712117958\n",
      "9 = 0.6980000138282776\n",
      "10 = 0.7138000130653381\n",
      "11 = 0.682200014591217\n",
      "12 = 0.6919348835945129\n",
      "13 = 0.7138000130653381\n",
      "14 = 7.51328182220459\n",
      "15 = 9.495174407958984\n",
      "16 = 0.09994477778673172\n",
      "17 = 7.513245105743408\n",
      "18 = 11.019441604614258\n",
      "epoch:25 step:24001 [D loss: 0.687383, acc: 51.56%] [G loss: 1.767491]\n",
      "epoch:25 step:24002 [D loss: 0.673075, acc: 57.03%] [G loss: 1.720080]\n",
      "epoch:25 step:24003 [D loss: 0.647198, acc: 59.38%] [G loss: 1.796372]\n",
      "epoch:25 step:24004 [D loss: 0.679353, acc: 59.38%] [G loss: 1.783415]\n",
      "epoch:25 step:24005 [D loss: 0.632543, acc: 64.06%] [G loss: 1.834249]\n",
      "epoch:25 step:24006 [D loss: 0.637713, acc: 60.16%] [G loss: 1.769556]\n",
      "epoch:25 step:24007 [D loss: 0.609646, acc: 64.06%] [G loss: 1.794038]\n",
      "epoch:25 step:24008 [D loss: 0.618256, acc: 68.75%] [G loss: 1.866955]\n",
      "epoch:25 step:24009 [D loss: 0.668305, acc: 55.47%] [G loss: 1.985256]\n",
      "epoch:25 step:24010 [D loss: 0.653669, acc: 62.50%] [G loss: 1.964350]\n",
      "epoch:25 step:24011 [D loss: 0.622180, acc: 60.16%] [G loss: 1.861601]\n",
      "epoch:25 step:24012 [D loss: 0.697208, acc: 52.34%] [G loss: 1.796550]\n",
      "epoch:25 step:24013 [D loss: 0.634813, acc: 63.28%] [G loss: 2.018017]\n",
      "epoch:25 step:24014 [D loss: 0.642357, acc: 63.28%] [G loss: 1.935973]\n",
      "epoch:25 step:24015 [D loss: 0.685751, acc: 50.78%] [G loss: 1.817055]\n",
      "epoch:25 step:24016 [D loss: 0.655698, acc: 62.50%] [G loss: 1.804271]\n",
      "epoch:25 step:24017 [D loss: 0.694840, acc: 60.94%] [G loss: 1.850355]\n",
      "epoch:25 step:24018 [D loss: 0.610649, acc: 58.59%] [G loss: 1.780403]\n",
      "epoch:25 step:24019 [D loss: 0.664470, acc: 63.28%] [G loss: 1.743527]\n",
      "epoch:25 step:24020 [D loss: 0.623867, acc: 70.31%] [G loss: 1.842341]\n",
      "epoch:25 step:24021 [D loss: 0.669578, acc: 58.59%] [G loss: 1.764800]\n",
      "epoch:25 step:24022 [D loss: 0.650099, acc: 65.62%] [G loss: 1.787537]\n",
      "epoch:25 step:24023 [D loss: 0.650330, acc: 60.16%] [G loss: 1.878026]\n",
      "epoch:25 step:24024 [D loss: 0.667331, acc: 58.59%] [G loss: 1.740382]\n",
      "epoch:25 step:24025 [D loss: 0.681443, acc: 60.16%] [G loss: 1.835917]\n",
      "epoch:25 step:24026 [D loss: 0.704212, acc: 59.38%] [G loss: 1.686091]\n",
      "epoch:25 step:24027 [D loss: 0.681857, acc: 55.47%] [G loss: 1.902523]\n",
      "epoch:25 step:24028 [D loss: 0.615452, acc: 63.28%] [G loss: 1.833870]\n",
      "epoch:25 step:24029 [D loss: 0.631490, acc: 67.19%] [G loss: 1.926968]\n",
      "epoch:25 step:24030 [D loss: 0.616244, acc: 63.28%] [G loss: 2.073175]\n",
      "epoch:25 step:24031 [D loss: 0.683370, acc: 58.59%] [G loss: 1.858562]\n",
      "epoch:25 step:24032 [D loss: 0.659282, acc: 62.50%] [G loss: 1.908218]\n",
      "epoch:25 step:24033 [D loss: 0.616982, acc: 68.75%] [G loss: 1.861403]\n",
      "epoch:25 step:24034 [D loss: 0.598253, acc: 65.62%] [G loss: 1.906247]\n",
      "epoch:25 step:24035 [D loss: 0.617912, acc: 68.75%] [G loss: 1.968393]\n",
      "epoch:25 step:24036 [D loss: 0.679661, acc: 58.59%] [G loss: 1.700316]\n",
      "epoch:25 step:24037 [D loss: 0.673576, acc: 57.81%] [G loss: 1.894637]\n",
      "epoch:25 step:24038 [D loss: 0.653114, acc: 63.28%] [G loss: 1.861409]\n",
      "epoch:25 step:24039 [D loss: 0.651130, acc: 63.28%] [G loss: 1.667485]\n",
      "epoch:25 step:24040 [D loss: 0.683330, acc: 57.81%] [G loss: 1.710218]\n",
      "epoch:25 step:24041 [D loss: 0.687363, acc: 59.38%] [G loss: 2.001568]\n",
      "epoch:25 step:24042 [D loss: 0.662767, acc: 56.25%] [G loss: 1.765074]\n",
      "epoch:25 step:24043 [D loss: 0.632308, acc: 60.16%] [G loss: 1.899430]\n",
      "epoch:25 step:24044 [D loss: 0.663040, acc: 58.59%] [G loss: 1.846055]\n",
      "epoch:25 step:24045 [D loss: 0.641422, acc: 61.72%] [G loss: 1.833503]\n",
      "epoch:25 step:24046 [D loss: 0.722644, acc: 47.66%] [G loss: 1.713841]\n",
      "epoch:25 step:24047 [D loss: 0.581806, acc: 71.09%] [G loss: 2.066805]\n",
      "epoch:25 step:24048 [D loss: 0.676317, acc: 57.81%] [G loss: 1.797543]\n",
      "epoch:25 step:24049 [D loss: 0.557750, acc: 71.09%] [G loss: 1.937809]\n",
      "epoch:25 step:24050 [D loss: 0.700811, acc: 54.69%] [G loss: 1.827604]\n",
      "epoch:25 step:24051 [D loss: 0.639915, acc: 60.94%] [G loss: 1.863000]\n",
      "epoch:25 step:24052 [D loss: 0.649233, acc: 60.94%] [G loss: 1.943304]\n",
      "epoch:25 step:24053 [D loss: 0.610049, acc: 65.62%] [G loss: 1.802775]\n",
      "epoch:25 step:24054 [D loss: 0.601023, acc: 72.66%] [G loss: 1.937717]\n",
      "epoch:25 step:24055 [D loss: 0.677082, acc: 56.25%] [G loss: 1.876443]\n",
      "epoch:25 step:24056 [D loss: 0.571467, acc: 70.31%] [G loss: 1.971209]\n",
      "epoch:25 step:24057 [D loss: 0.662652, acc: 67.19%] [G loss: 1.937702]\n",
      "epoch:25 step:24058 [D loss: 0.660592, acc: 59.38%] [G loss: 1.853953]\n",
      "epoch:25 step:24059 [D loss: 0.615541, acc: 64.84%] [G loss: 1.935053]\n",
      "epoch:25 step:24060 [D loss: 0.629319, acc: 66.41%] [G loss: 1.859950]\n",
      "epoch:25 step:24061 [D loss: 0.681353, acc: 55.47%] [G loss: 1.817007]\n",
      "epoch:25 step:24062 [D loss: 0.612257, acc: 62.50%] [G loss: 2.016634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24063 [D loss: 0.587279, acc: 70.31%] [G loss: 1.884830]\n",
      "epoch:25 step:24064 [D loss: 0.634356, acc: 67.97%] [G loss: 1.858146]\n",
      "epoch:25 step:24065 [D loss: 0.664376, acc: 58.59%] [G loss: 1.869713]\n",
      "epoch:25 step:24066 [D loss: 0.704294, acc: 56.25%] [G loss: 1.934608]\n",
      "epoch:25 step:24067 [D loss: 0.624500, acc: 64.84%] [G loss: 1.967584]\n",
      "epoch:25 step:24068 [D loss: 0.684715, acc: 56.25%] [G loss: 1.917994]\n",
      "epoch:25 step:24069 [D loss: 0.659435, acc: 59.38%] [G loss: 1.958999]\n",
      "epoch:25 step:24070 [D loss: 0.644962, acc: 64.06%] [G loss: 1.989368]\n",
      "epoch:25 step:24071 [D loss: 0.640767, acc: 65.62%] [G loss: 2.014004]\n",
      "epoch:25 step:24072 [D loss: 0.630071, acc: 65.62%] [G loss: 1.994967]\n",
      "epoch:25 step:24073 [D loss: 0.534663, acc: 72.66%] [G loss: 2.237644]\n",
      "epoch:25 step:24074 [D loss: 0.633452, acc: 61.72%] [G loss: 2.026746]\n",
      "epoch:25 step:24075 [D loss: 0.714745, acc: 57.03%] [G loss: 1.956310]\n",
      "epoch:25 step:24076 [D loss: 0.645697, acc: 61.72%] [G loss: 1.892827]\n",
      "epoch:25 step:24077 [D loss: 0.666824, acc: 60.94%] [G loss: 2.007459]\n",
      "epoch:25 step:24078 [D loss: 0.611050, acc: 64.84%] [G loss: 2.114268]\n",
      "epoch:25 step:24079 [D loss: 0.645316, acc: 67.19%] [G loss: 1.958274]\n",
      "epoch:25 step:24080 [D loss: 0.652324, acc: 57.81%] [G loss: 1.969399]\n",
      "epoch:25 step:24081 [D loss: 0.643173, acc: 63.28%] [G loss: 1.753548]\n",
      "epoch:25 step:24082 [D loss: 0.688155, acc: 58.59%] [G loss: 1.849622]\n",
      "epoch:25 step:24083 [D loss: 0.718978, acc: 54.69%] [G loss: 1.778106]\n",
      "epoch:25 step:24084 [D loss: 0.682956, acc: 60.94%] [G loss: 1.786923]\n",
      "epoch:25 step:24085 [D loss: 0.606089, acc: 66.41%] [G loss: 1.862154]\n",
      "epoch:25 step:24086 [D loss: 0.660132, acc: 60.16%] [G loss: 1.859404]\n",
      "epoch:25 step:24087 [D loss: 0.666272, acc: 57.03%] [G loss: 1.865492]\n",
      "epoch:25 step:24088 [D loss: 0.692854, acc: 60.94%] [G loss: 1.816863]\n",
      "epoch:25 step:24089 [D loss: 0.662863, acc: 60.16%] [G loss: 1.783204]\n",
      "epoch:25 step:24090 [D loss: 0.674735, acc: 59.38%] [G loss: 1.900255]\n",
      "epoch:25 step:24091 [D loss: 0.681845, acc: 56.25%] [G loss: 1.866307]\n",
      "epoch:25 step:24092 [D loss: 0.684518, acc: 57.03%] [G loss: 1.810923]\n",
      "epoch:25 step:24093 [D loss: 0.630606, acc: 60.16%] [G loss: 1.799635]\n",
      "epoch:25 step:24094 [D loss: 0.663804, acc: 60.16%] [G loss: 1.730699]\n",
      "epoch:25 step:24095 [D loss: 0.667549, acc: 60.94%] [G loss: 1.699888]\n",
      "epoch:25 step:24096 [D loss: 0.649309, acc: 64.84%] [G loss: 1.715588]\n",
      "epoch:25 step:24097 [D loss: 0.689535, acc: 59.38%] [G loss: 1.879058]\n",
      "epoch:25 step:24098 [D loss: 0.634784, acc: 65.62%] [G loss: 1.826272]\n",
      "epoch:25 step:24099 [D loss: 0.664267, acc: 57.81%] [G loss: 1.948016]\n",
      "epoch:25 step:24100 [D loss: 0.704866, acc: 59.38%] [G loss: 1.761377]\n",
      "epoch:25 step:24101 [D loss: 0.683322, acc: 54.69%] [G loss: 1.730156]\n",
      "epoch:25 step:24102 [D loss: 0.635421, acc: 62.50%] [G loss: 1.877531]\n",
      "epoch:25 step:24103 [D loss: 0.610031, acc: 70.31%] [G loss: 1.756319]\n",
      "epoch:25 step:24104 [D loss: 0.668697, acc: 57.81%] [G loss: 1.860857]\n",
      "epoch:25 step:24105 [D loss: 0.654358, acc: 60.94%] [G loss: 1.880763]\n",
      "epoch:25 step:24106 [D loss: 0.594981, acc: 66.41%] [G loss: 1.976472]\n",
      "epoch:25 step:24107 [D loss: 0.686140, acc: 57.81%] [G loss: 1.712392]\n",
      "epoch:25 step:24108 [D loss: 0.669200, acc: 57.03%] [G loss: 1.804222]\n",
      "epoch:25 step:24109 [D loss: 0.661275, acc: 57.03%] [G loss: 1.735190]\n",
      "epoch:25 step:24110 [D loss: 0.629112, acc: 66.41%] [G loss: 2.020733]\n",
      "epoch:25 step:24111 [D loss: 0.659137, acc: 65.62%] [G loss: 1.957182]\n",
      "epoch:25 step:24112 [D loss: 0.640906, acc: 67.19%] [G loss: 1.898493]\n",
      "epoch:25 step:24113 [D loss: 0.643761, acc: 63.28%] [G loss: 1.971513]\n",
      "epoch:25 step:24114 [D loss: 0.620580, acc: 63.28%] [G loss: 1.900797]\n",
      "epoch:25 step:24115 [D loss: 0.587143, acc: 68.75%] [G loss: 1.991297]\n",
      "epoch:25 step:24116 [D loss: 0.613321, acc: 67.97%] [G loss: 2.090995]\n",
      "epoch:25 step:24117 [D loss: 0.620074, acc: 67.97%] [G loss: 2.020538]\n",
      "epoch:25 step:24118 [D loss: 0.626400, acc: 63.28%] [G loss: 1.974898]\n",
      "epoch:25 step:24119 [D loss: 0.596317, acc: 70.31%] [G loss: 2.029818]\n",
      "epoch:25 step:24120 [D loss: 0.615576, acc: 66.41%] [G loss: 2.006984]\n",
      "epoch:25 step:24121 [D loss: 0.641731, acc: 66.41%] [G loss: 1.807296]\n",
      "epoch:25 step:24122 [D loss: 0.668713, acc: 62.50%] [G loss: 1.813443]\n",
      "epoch:25 step:24123 [D loss: 0.632827, acc: 66.41%] [G loss: 1.830573]\n",
      "epoch:25 step:24124 [D loss: 0.650396, acc: 58.59%] [G loss: 1.942987]\n",
      "epoch:25 step:24125 [D loss: 0.678356, acc: 60.16%] [G loss: 1.859298]\n",
      "epoch:25 step:24126 [D loss: 0.638415, acc: 60.16%] [G loss: 1.835206]\n",
      "epoch:25 step:24127 [D loss: 0.713096, acc: 51.56%] [G loss: 1.785624]\n",
      "epoch:25 step:24128 [D loss: 0.742702, acc: 51.56%] [G loss: 1.656006]\n",
      "epoch:25 step:24129 [D loss: 0.685117, acc: 54.69%] [G loss: 1.789096]\n",
      "epoch:25 step:24130 [D loss: 0.663442, acc: 59.38%] [G loss: 1.786611]\n",
      "epoch:25 step:24131 [D loss: 0.635866, acc: 62.50%] [G loss: 1.875448]\n",
      "epoch:25 step:24132 [D loss: 0.648229, acc: 60.16%] [G loss: 1.968591]\n",
      "epoch:25 step:24133 [D loss: 0.616457, acc: 65.62%] [G loss: 1.977258]\n",
      "epoch:25 step:24134 [D loss: 0.587837, acc: 70.31%] [G loss: 2.010818]\n",
      "epoch:25 step:24135 [D loss: 0.704159, acc: 55.47%] [G loss: 1.813274]\n",
      "epoch:25 step:24136 [D loss: 0.659859, acc: 69.53%] [G loss: 1.834513]\n",
      "epoch:25 step:24137 [D loss: 0.646587, acc: 63.28%] [G loss: 1.933947]\n",
      "epoch:25 step:24138 [D loss: 0.663535, acc: 58.59%] [G loss: 1.902385]\n",
      "epoch:25 step:24139 [D loss: 0.636379, acc: 60.94%] [G loss: 1.902955]\n",
      "epoch:25 step:24140 [D loss: 0.686248, acc: 54.69%] [G loss: 1.648495]\n",
      "epoch:25 step:24141 [D loss: 0.657066, acc: 57.03%] [G loss: 1.762146]\n",
      "epoch:25 step:24142 [D loss: 0.667267, acc: 56.25%] [G loss: 1.792304]\n",
      "epoch:25 step:24143 [D loss: 0.653009, acc: 60.16%] [G loss: 1.892250]\n",
      "epoch:25 step:24144 [D loss: 0.586784, acc: 68.75%] [G loss: 2.069476]\n",
      "epoch:25 step:24145 [D loss: 0.634152, acc: 60.94%] [G loss: 1.984717]\n",
      "epoch:25 step:24146 [D loss: 0.599183, acc: 72.66%] [G loss: 2.040778]\n",
      "epoch:25 step:24147 [D loss: 0.677093, acc: 53.91%] [G loss: 1.757834]\n",
      "epoch:25 step:24148 [D loss: 0.652470, acc: 59.38%] [G loss: 1.861628]\n",
      "epoch:25 step:24149 [D loss: 0.626734, acc: 64.84%] [G loss: 1.947694]\n",
      "epoch:25 step:24150 [D loss: 0.606532, acc: 69.53%] [G loss: 1.924210]\n",
      "epoch:25 step:24151 [D loss: 0.652361, acc: 62.50%] [G loss: 1.843650]\n",
      "epoch:25 step:24152 [D loss: 0.711519, acc: 52.34%] [G loss: 1.801212]\n",
      "epoch:25 step:24153 [D loss: 0.662145, acc: 61.72%] [G loss: 1.849879]\n",
      "epoch:25 step:24154 [D loss: 0.650262, acc: 60.16%] [G loss: 1.782494]\n",
      "epoch:25 step:24155 [D loss: 0.676760, acc: 57.81%] [G loss: 1.848454]\n",
      "epoch:25 step:24156 [D loss: 0.705544, acc: 53.12%] [G loss: 1.773992]\n",
      "epoch:25 step:24157 [D loss: 0.671407, acc: 60.16%] [G loss: 1.761491]\n",
      "epoch:25 step:24158 [D loss: 0.643125, acc: 60.94%] [G loss: 1.879714]\n",
      "epoch:25 step:24159 [D loss: 0.659021, acc: 60.16%] [G loss: 1.745459]\n",
      "epoch:25 step:24160 [D loss: 0.662451, acc: 64.06%] [G loss: 1.861598]\n",
      "epoch:25 step:24161 [D loss: 0.733621, acc: 50.78%] [G loss: 1.921215]\n",
      "epoch:25 step:24162 [D loss: 0.675216, acc: 60.94%] [G loss: 1.869642]\n",
      "epoch:25 step:24163 [D loss: 0.616342, acc: 63.28%] [G loss: 1.800369]\n",
      "epoch:25 step:24164 [D loss: 0.626041, acc: 65.62%] [G loss: 1.871639]\n",
      "epoch:25 step:24165 [D loss: 0.629602, acc: 67.19%] [G loss: 1.961829]\n",
      "epoch:25 step:24166 [D loss: 0.682400, acc: 56.25%] [G loss: 1.827431]\n",
      "epoch:25 step:24167 [D loss: 0.613143, acc: 62.50%] [G loss: 1.843392]\n",
      "epoch:25 step:24168 [D loss: 0.608409, acc: 64.84%] [G loss: 1.911822]\n",
      "epoch:25 step:24169 [D loss: 0.679531, acc: 57.03%] [G loss: 1.936219]\n",
      "epoch:25 step:24170 [D loss: 0.682637, acc: 59.38%] [G loss: 1.831336]\n",
      "epoch:25 step:24171 [D loss: 0.589722, acc: 69.53%] [G loss: 2.115846]\n",
      "epoch:25 step:24172 [D loss: 0.593593, acc: 71.88%] [G loss: 2.054584]\n",
      "epoch:25 step:24173 [D loss: 0.659221, acc: 59.38%] [G loss: 1.774831]\n",
      "epoch:25 step:24174 [D loss: 0.627069, acc: 66.41%] [G loss: 1.726921]\n",
      "epoch:25 step:24175 [D loss: 0.630494, acc: 63.28%] [G loss: 1.831940]\n",
      "epoch:25 step:24176 [D loss: 0.652234, acc: 59.38%] [G loss: 1.795151]\n",
      "epoch:25 step:24177 [D loss: 0.648597, acc: 66.41%] [G loss: 1.804875]\n",
      "epoch:25 step:24178 [D loss: 0.642284, acc: 64.84%] [G loss: 1.813768]\n",
      "epoch:25 step:24179 [D loss: 0.646684, acc: 62.50%] [G loss: 1.833537]\n",
      "epoch:25 step:24180 [D loss: 0.617065, acc: 64.06%] [G loss: 1.889464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24181 [D loss: 0.695268, acc: 53.91%] [G loss: 1.758392]\n",
      "epoch:25 step:24182 [D loss: 0.632359, acc: 64.84%] [G loss: 1.944050]\n",
      "epoch:25 step:24183 [D loss: 0.669033, acc: 61.72%] [G loss: 1.843862]\n",
      "epoch:25 step:24184 [D loss: 0.677147, acc: 55.47%] [G loss: 1.797307]\n",
      "epoch:25 step:24185 [D loss: 0.625705, acc: 64.84%] [G loss: 1.847807]\n",
      "epoch:25 step:24186 [D loss: 0.650173, acc: 60.16%] [G loss: 1.854159]\n",
      "epoch:25 step:24187 [D loss: 0.674529, acc: 60.16%] [G loss: 1.715211]\n",
      "epoch:25 step:24188 [D loss: 0.680787, acc: 58.59%] [G loss: 1.835489]\n",
      "epoch:25 step:24189 [D loss: 0.624853, acc: 62.50%] [G loss: 1.796764]\n",
      "epoch:25 step:24190 [D loss: 0.695254, acc: 56.25%] [G loss: 1.665807]\n",
      "epoch:25 step:24191 [D loss: 0.665623, acc: 60.94%] [G loss: 1.849472]\n",
      "epoch:25 step:24192 [D loss: 0.633836, acc: 69.53%] [G loss: 1.812075]\n",
      "epoch:25 step:24193 [D loss: 0.675003, acc: 61.72%] [G loss: 1.766298]\n",
      "epoch:25 step:24194 [D loss: 0.611931, acc: 67.19%] [G loss: 1.869579]\n",
      "epoch:25 step:24195 [D loss: 0.673580, acc: 57.81%] [G loss: 1.960131]\n",
      "epoch:25 step:24196 [D loss: 0.655524, acc: 61.72%] [G loss: 1.693209]\n",
      "epoch:25 step:24197 [D loss: 0.654280, acc: 57.81%] [G loss: 1.794284]\n",
      "epoch:25 step:24198 [D loss: 0.650100, acc: 62.50%] [G loss: 1.826726]\n",
      "epoch:25 step:24199 [D loss: 0.619553, acc: 62.50%] [G loss: 2.020303]\n",
      "epoch:25 step:24200 [D loss: 0.608286, acc: 63.28%] [G loss: 2.089610]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.616273\n",
      "FID: 10.215783\n",
      "0 = 12.644315283536931\n",
      "1 = 0.08824526973506985\n",
      "2 = 0.8672000169754028\n",
      "3 = 0.8859999775886536\n",
      "4 = 0.8483999967575073\n",
      "5 = 0.8538935780525208\n",
      "6 = 0.8859999775886536\n",
      "7 = 6.123953100144862\n",
      "8 = 0.06222281981741324\n",
      "9 = 0.7153000235557556\n",
      "10 = 0.7221999764442444\n",
      "11 = 0.7084000110626221\n",
      "12 = 0.7123693227767944\n",
      "13 = 0.7221999764442444\n",
      "14 = 7.616304874420166\n",
      "15 = 9.46667194366455\n",
      "16 = 0.10460738837718964\n",
      "17 = 7.616272926330566\n",
      "18 = 10.21578311920166\n",
      "epoch:25 step:24201 [D loss: 0.654068, acc: 59.38%] [G loss: 1.985999]\n",
      "epoch:25 step:24202 [D loss: 0.667907, acc: 59.38%] [G loss: 1.816825]\n",
      "epoch:25 step:24203 [D loss: 0.663287, acc: 61.72%] [G loss: 1.937212]\n",
      "epoch:25 step:24204 [D loss: 0.699464, acc: 60.16%] [G loss: 1.787493]\n",
      "epoch:25 step:24205 [D loss: 0.638415, acc: 63.28%] [G loss: 1.897497]\n",
      "epoch:25 step:24206 [D loss: 0.634397, acc: 60.94%] [G loss: 2.095997]\n",
      "epoch:25 step:24207 [D loss: 0.595094, acc: 71.09%] [G loss: 2.033134]\n",
      "epoch:25 step:24208 [D loss: 0.652937, acc: 59.38%] [G loss: 1.891912]\n",
      "epoch:25 step:24209 [D loss: 0.699628, acc: 52.34%] [G loss: 1.707868]\n",
      "epoch:25 step:24210 [D loss: 0.705480, acc: 62.50%] [G loss: 1.850623]\n",
      "epoch:25 step:24211 [D loss: 0.629163, acc: 62.50%] [G loss: 1.982526]\n",
      "epoch:25 step:24212 [D loss: 0.651684, acc: 59.38%] [G loss: 1.764208]\n",
      "epoch:25 step:24213 [D loss: 0.647717, acc: 63.28%] [G loss: 1.774300]\n",
      "epoch:25 step:24214 [D loss: 0.654683, acc: 67.19%] [G loss: 1.853370]\n",
      "epoch:25 step:24215 [D loss: 0.666536, acc: 57.81%] [G loss: 1.975020]\n",
      "epoch:25 step:24216 [D loss: 0.632298, acc: 62.50%] [G loss: 1.926416]\n",
      "epoch:25 step:24217 [D loss: 0.551569, acc: 75.00%] [G loss: 2.134790]\n",
      "epoch:25 step:24218 [D loss: 0.664090, acc: 64.06%] [G loss: 1.948546]\n",
      "epoch:25 step:24219 [D loss: 0.724949, acc: 51.56%] [G loss: 1.798312]\n",
      "epoch:25 step:24220 [D loss: 0.680796, acc: 57.81%] [G loss: 1.796679]\n",
      "epoch:25 step:24221 [D loss: 0.692200, acc: 56.25%] [G loss: 1.883992]\n",
      "epoch:25 step:24222 [D loss: 0.672597, acc: 56.25%] [G loss: 1.798472]\n",
      "epoch:25 step:24223 [D loss: 0.632725, acc: 65.62%] [G loss: 1.896579]\n",
      "epoch:25 step:24224 [D loss: 0.676204, acc: 60.16%] [G loss: 1.736773]\n",
      "epoch:25 step:24225 [D loss: 0.663984, acc: 60.94%] [G loss: 1.800348]\n",
      "epoch:25 step:24226 [D loss: 0.673667, acc: 55.47%] [G loss: 1.741757]\n",
      "epoch:25 step:24227 [D loss: 0.679727, acc: 53.91%] [G loss: 1.753048]\n",
      "epoch:25 step:24228 [D loss: 0.690257, acc: 59.38%] [G loss: 1.666037]\n",
      "epoch:25 step:24229 [D loss: 0.637447, acc: 63.28%] [G loss: 1.933181]\n",
      "epoch:25 step:24230 [D loss: 0.632075, acc: 63.28%] [G loss: 1.897709]\n",
      "epoch:25 step:24231 [D loss: 0.678894, acc: 65.62%] [G loss: 1.808550]\n",
      "epoch:25 step:24232 [D loss: 0.626765, acc: 66.41%] [G loss: 1.834069]\n",
      "epoch:25 step:24233 [D loss: 0.631299, acc: 63.28%] [G loss: 1.924448]\n",
      "epoch:25 step:24234 [D loss: 0.650424, acc: 57.03%] [G loss: 2.018746]\n",
      "epoch:25 step:24235 [D loss: 0.724129, acc: 48.44%] [G loss: 1.836901]\n",
      "epoch:25 step:24236 [D loss: 0.615994, acc: 61.72%] [G loss: 1.850658]\n",
      "epoch:25 step:24237 [D loss: 0.642691, acc: 60.94%] [G loss: 1.785393]\n",
      "epoch:25 step:24238 [D loss: 0.630985, acc: 64.84%] [G loss: 1.815364]\n",
      "epoch:25 step:24239 [D loss: 0.653914, acc: 57.81%] [G loss: 1.890755]\n",
      "epoch:25 step:24240 [D loss: 0.589749, acc: 69.53%] [G loss: 1.927365]\n",
      "epoch:25 step:24241 [D loss: 0.593033, acc: 64.84%] [G loss: 2.123698]\n",
      "epoch:25 step:24242 [D loss: 0.723318, acc: 64.06%] [G loss: 1.690045]\n",
      "epoch:25 step:24243 [D loss: 0.673209, acc: 63.28%] [G loss: 1.711680]\n",
      "epoch:25 step:24244 [D loss: 0.609015, acc: 65.62%] [G loss: 1.908458]\n",
      "epoch:25 step:24245 [D loss: 0.683905, acc: 57.03%] [G loss: 1.678496]\n",
      "epoch:25 step:24246 [D loss: 0.645042, acc: 60.16%] [G loss: 1.767772]\n",
      "epoch:25 step:24247 [D loss: 0.677437, acc: 57.03%] [G loss: 1.918869]\n",
      "epoch:25 step:24248 [D loss: 0.639813, acc: 60.16%] [G loss: 1.865169]\n",
      "epoch:25 step:24249 [D loss: 0.650847, acc: 59.38%] [G loss: 1.875141]\n",
      "epoch:25 step:24250 [D loss: 0.585675, acc: 70.31%] [G loss: 1.953863]\n",
      "epoch:25 step:24251 [D loss: 0.680769, acc: 52.34%] [G loss: 1.810386]\n",
      "epoch:25 step:24252 [D loss: 0.662103, acc: 63.28%] [G loss: 1.829612]\n",
      "epoch:25 step:24253 [D loss: 0.663666, acc: 62.50%] [G loss: 1.714664]\n",
      "epoch:25 step:24254 [D loss: 0.702870, acc: 53.91%] [G loss: 1.750709]\n",
      "epoch:25 step:24255 [D loss: 0.678113, acc: 59.38%] [G loss: 1.899985]\n",
      "epoch:25 step:24256 [D loss: 0.671888, acc: 59.38%] [G loss: 1.716608]\n",
      "epoch:25 step:24257 [D loss: 0.653255, acc: 63.28%] [G loss: 1.794581]\n",
      "epoch:25 step:24258 [D loss: 0.605109, acc: 65.62%] [G loss: 1.950231]\n",
      "epoch:25 step:24259 [D loss: 0.619736, acc: 67.19%] [G loss: 1.884300]\n",
      "epoch:25 step:24260 [D loss: 0.633739, acc: 64.84%] [G loss: 1.780522]\n",
      "epoch:25 step:24261 [D loss: 0.634845, acc: 64.84%] [G loss: 1.743056]\n",
      "epoch:25 step:24262 [D loss: 0.645931, acc: 57.81%] [G loss: 1.767564]\n",
      "epoch:25 step:24263 [D loss: 0.628101, acc: 63.28%] [G loss: 1.852634]\n",
      "epoch:25 step:24264 [D loss: 0.618613, acc: 63.28%] [G loss: 1.975144]\n",
      "epoch:25 step:24265 [D loss: 0.644092, acc: 55.47%] [G loss: 1.716516]\n",
      "epoch:25 step:24266 [D loss: 0.596400, acc: 67.19%] [G loss: 1.918603]\n",
      "epoch:25 step:24267 [D loss: 0.627808, acc: 62.50%] [G loss: 1.860795]\n",
      "epoch:25 step:24268 [D loss: 0.640674, acc: 57.03%] [G loss: 1.859828]\n",
      "epoch:25 step:24269 [D loss: 0.666965, acc: 60.16%] [G loss: 1.857004]\n",
      "epoch:25 step:24270 [D loss: 0.666337, acc: 62.50%] [G loss: 1.955843]\n",
      "epoch:25 step:24271 [D loss: 0.700461, acc: 56.25%] [G loss: 1.843922]\n",
      "epoch:25 step:24272 [D loss: 0.632128, acc: 64.06%] [G loss: 1.809644]\n",
      "epoch:25 step:24273 [D loss: 0.626788, acc: 62.50%] [G loss: 1.882647]\n",
      "epoch:25 step:24274 [D loss: 0.648399, acc: 62.50%] [G loss: 1.937893]\n",
      "epoch:25 step:24275 [D loss: 0.692028, acc: 54.69%] [G loss: 1.803204]\n",
      "epoch:25 step:24276 [D loss: 0.638924, acc: 64.06%] [G loss: 1.765606]\n",
      "epoch:25 step:24277 [D loss: 0.656742, acc: 62.50%] [G loss: 1.874813]\n",
      "epoch:25 step:24278 [D loss: 0.609973, acc: 67.97%] [G loss: 1.811011]\n",
      "epoch:25 step:24279 [D loss: 0.617355, acc: 65.62%] [G loss: 1.793446]\n",
      "epoch:25 step:24280 [D loss: 0.707828, acc: 51.56%] [G loss: 1.807529]\n",
      "epoch:25 step:24281 [D loss: 0.668514, acc: 57.81%] [G loss: 1.800207]\n",
      "epoch:25 step:24282 [D loss: 0.653356, acc: 62.50%] [G loss: 1.931269]\n",
      "epoch:25 step:24283 [D loss: 0.710051, acc: 52.34%] [G loss: 1.853258]\n",
      "epoch:25 step:24284 [D loss: 0.732006, acc: 53.12%] [G loss: 1.767711]\n",
      "epoch:25 step:24285 [D loss: 0.628183, acc: 66.41%] [G loss: 1.972254]\n",
      "epoch:25 step:24286 [D loss: 0.661490, acc: 57.81%] [G loss: 1.656313]\n",
      "epoch:25 step:24287 [D loss: 0.657317, acc: 58.59%] [G loss: 1.769205]\n",
      "epoch:25 step:24288 [D loss: 0.639738, acc: 64.06%] [G loss: 1.775625]\n",
      "epoch:25 step:24289 [D loss: 0.621263, acc: 66.41%] [G loss: 1.859978]\n",
      "epoch:25 step:24290 [D loss: 0.657258, acc: 64.06%] [G loss: 1.817261]\n",
      "epoch:25 step:24291 [D loss: 0.622676, acc: 63.28%] [G loss: 1.738802]\n",
      "epoch:25 step:24292 [D loss: 0.723234, acc: 53.91%] [G loss: 1.714197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24293 [D loss: 0.659815, acc: 57.81%] [G loss: 1.846602]\n",
      "epoch:25 step:24294 [D loss: 0.693059, acc: 50.78%] [G loss: 1.690195]\n",
      "epoch:25 step:24295 [D loss: 0.703943, acc: 57.03%] [G loss: 1.843683]\n",
      "epoch:25 step:24296 [D loss: 0.654924, acc: 62.50%] [G loss: 1.803567]\n",
      "epoch:25 step:24297 [D loss: 0.644654, acc: 64.84%] [G loss: 1.755244]\n",
      "epoch:25 step:24298 [D loss: 0.670543, acc: 57.03%] [G loss: 1.694425]\n",
      "epoch:25 step:24299 [D loss: 0.641785, acc: 62.50%] [G loss: 1.780331]\n",
      "epoch:25 step:24300 [D loss: 0.624820, acc: 63.28%] [G loss: 1.828413]\n",
      "epoch:25 step:24301 [D loss: 0.644065, acc: 60.94%] [G loss: 1.742646]\n",
      "epoch:25 step:24302 [D loss: 0.609434, acc: 67.97%] [G loss: 1.840170]\n",
      "epoch:25 step:24303 [D loss: 0.661421, acc: 59.38%] [G loss: 1.794370]\n",
      "epoch:25 step:24304 [D loss: 0.659512, acc: 57.81%] [G loss: 1.699947]\n",
      "epoch:25 step:24305 [D loss: 0.671439, acc: 58.59%] [G loss: 1.831739]\n",
      "epoch:25 step:24306 [D loss: 0.626006, acc: 65.62%] [G loss: 1.888982]\n",
      "epoch:25 step:24307 [D loss: 0.614504, acc: 61.72%] [G loss: 1.783058]\n",
      "epoch:25 step:24308 [D loss: 0.720573, acc: 54.69%] [G loss: 1.744845]\n",
      "epoch:25 step:24309 [D loss: 0.673430, acc: 51.56%] [G loss: 1.965406]\n",
      "epoch:25 step:24310 [D loss: 0.652782, acc: 62.50%] [G loss: 1.816115]\n",
      "epoch:25 step:24311 [D loss: 0.669825, acc: 58.59%] [G loss: 1.915570]\n",
      "epoch:25 step:24312 [D loss: 0.651029, acc: 60.94%] [G loss: 1.870715]\n",
      "epoch:25 step:24313 [D loss: 0.637542, acc: 59.38%] [G loss: 1.828133]\n",
      "epoch:25 step:24314 [D loss: 0.642248, acc: 64.06%] [G loss: 1.879824]\n",
      "epoch:25 step:24315 [D loss: 0.644449, acc: 60.16%] [G loss: 1.903969]\n",
      "epoch:25 step:24316 [D loss: 0.689749, acc: 51.56%] [G loss: 1.816031]\n",
      "epoch:25 step:24317 [D loss: 0.653530, acc: 65.62%] [G loss: 1.742487]\n",
      "epoch:25 step:24318 [D loss: 0.633870, acc: 67.97%] [G loss: 1.941507]\n",
      "epoch:25 step:24319 [D loss: 0.627293, acc: 64.84%] [G loss: 1.986882]\n",
      "epoch:25 step:24320 [D loss: 0.627626, acc: 64.06%] [G loss: 1.872558]\n",
      "epoch:25 step:24321 [D loss: 0.725296, acc: 57.03%] [G loss: 1.790869]\n",
      "epoch:25 step:24322 [D loss: 0.644709, acc: 62.50%] [G loss: 1.804729]\n",
      "epoch:25 step:24323 [D loss: 0.653255, acc: 61.72%] [G loss: 1.871639]\n",
      "epoch:25 step:24324 [D loss: 0.650925, acc: 60.94%] [G loss: 1.844974]\n",
      "epoch:25 step:24325 [D loss: 0.655627, acc: 57.81%] [G loss: 1.852148]\n",
      "epoch:25 step:24326 [D loss: 0.647564, acc: 60.16%] [G loss: 1.825724]\n",
      "epoch:25 step:24327 [D loss: 0.684811, acc: 58.59%] [G loss: 1.860574]\n",
      "epoch:25 step:24328 [D loss: 0.670001, acc: 63.28%] [G loss: 1.727613]\n",
      "epoch:25 step:24329 [D loss: 0.662580, acc: 58.59%] [G loss: 1.781168]\n",
      "epoch:25 step:24330 [D loss: 0.589467, acc: 67.97%] [G loss: 1.813372]\n",
      "epoch:25 step:24331 [D loss: 0.657254, acc: 64.06%] [G loss: 1.902192]\n",
      "epoch:25 step:24332 [D loss: 0.667228, acc: 60.94%] [G loss: 1.961142]\n",
      "epoch:25 step:24333 [D loss: 0.664594, acc: 64.84%] [G loss: 2.031870]\n",
      "epoch:25 step:24334 [D loss: 0.652176, acc: 65.62%] [G loss: 1.859554]\n",
      "epoch:25 step:24335 [D loss: 0.645321, acc: 64.06%] [G loss: 1.852050]\n",
      "epoch:25 step:24336 [D loss: 0.659755, acc: 56.25%] [G loss: 1.761661]\n",
      "epoch:25 step:24337 [D loss: 0.648262, acc: 64.84%] [G loss: 1.934615]\n",
      "epoch:25 step:24338 [D loss: 0.714212, acc: 52.34%] [G loss: 1.886338]\n",
      "epoch:25 step:24339 [D loss: 0.687105, acc: 55.47%] [G loss: 1.703101]\n",
      "epoch:25 step:24340 [D loss: 0.616015, acc: 66.41%] [G loss: 1.994413]\n",
      "epoch:25 step:24341 [D loss: 0.674937, acc: 57.81%] [G loss: 1.851155]\n",
      "epoch:25 step:24342 [D loss: 0.704826, acc: 51.56%] [G loss: 1.806405]\n",
      "epoch:25 step:24343 [D loss: 0.571625, acc: 68.75%] [G loss: 2.002271]\n",
      "epoch:25 step:24344 [D loss: 0.588487, acc: 71.09%] [G loss: 2.085879]\n",
      "epoch:25 step:24345 [D loss: 0.721070, acc: 54.69%] [G loss: 1.811050]\n",
      "epoch:25 step:24346 [D loss: 0.661813, acc: 60.94%] [G loss: 1.811744]\n",
      "epoch:25 step:24347 [D loss: 0.647913, acc: 62.50%] [G loss: 1.829629]\n",
      "epoch:25 step:24348 [D loss: 0.570924, acc: 73.44%] [G loss: 1.948250]\n",
      "epoch:25 step:24349 [D loss: 0.658548, acc: 67.19%] [G loss: 2.073009]\n",
      "epoch:25 step:24350 [D loss: 0.591560, acc: 71.09%] [G loss: 2.126346]\n",
      "epoch:25 step:24351 [D loss: 0.620350, acc: 67.97%] [G loss: 1.850588]\n",
      "epoch:25 step:24352 [D loss: 0.687675, acc: 60.16%] [G loss: 1.939694]\n",
      "epoch:25 step:24353 [D loss: 0.721832, acc: 57.03%] [G loss: 1.669688]\n",
      "epoch:25 step:24354 [D loss: 0.797017, acc: 40.62%] [G loss: 1.871146]\n",
      "epoch:25 step:24355 [D loss: 0.554238, acc: 71.88%] [G loss: 2.138755]\n",
      "epoch:25 step:24356 [D loss: 0.618819, acc: 61.72%] [G loss: 1.950061]\n",
      "epoch:25 step:24357 [D loss: 0.741545, acc: 55.47%] [G loss: 1.758452]\n",
      "epoch:25 step:24358 [D loss: 0.642096, acc: 58.59%] [G loss: 1.921425]\n",
      "epoch:25 step:24359 [D loss: 0.631428, acc: 67.97%] [G loss: 1.867573]\n",
      "epoch:25 step:24360 [D loss: 0.571016, acc: 77.34%] [G loss: 1.967166]\n",
      "epoch:25 step:24361 [D loss: 0.583765, acc: 64.84%] [G loss: 1.992170]\n",
      "epoch:25 step:24362 [D loss: 0.634188, acc: 70.31%] [G loss: 2.290206]\n",
      "epoch:26 step:24363 [D loss: 0.598468, acc: 71.09%] [G loss: 1.995606]\n",
      "epoch:26 step:24364 [D loss: 0.609928, acc: 66.41%] [G loss: 1.932155]\n",
      "epoch:26 step:24365 [D loss: 0.629934, acc: 59.38%] [G loss: 1.999761]\n",
      "epoch:26 step:24366 [D loss: 0.652754, acc: 62.50%] [G loss: 1.847102]\n",
      "epoch:26 step:24367 [D loss: 0.633884, acc: 66.41%] [G loss: 1.867705]\n",
      "epoch:26 step:24368 [D loss: 0.653557, acc: 60.16%] [G loss: 1.939124]\n",
      "epoch:26 step:24369 [D loss: 0.680865, acc: 57.03%] [G loss: 1.958763]\n",
      "epoch:26 step:24370 [D loss: 0.646277, acc: 61.72%] [G loss: 2.001233]\n",
      "epoch:26 step:24371 [D loss: 0.636583, acc: 63.28%] [G loss: 1.974273]\n",
      "epoch:26 step:24372 [D loss: 0.665573, acc: 57.81%] [G loss: 1.976284]\n",
      "epoch:26 step:24373 [D loss: 0.670374, acc: 60.94%] [G loss: 1.963111]\n",
      "epoch:26 step:24374 [D loss: 0.647572, acc: 61.72%] [G loss: 1.804695]\n",
      "epoch:26 step:24375 [D loss: 0.630457, acc: 67.19%] [G loss: 1.857997]\n",
      "epoch:26 step:24376 [D loss: 0.666466, acc: 64.84%] [G loss: 1.724347]\n",
      "epoch:26 step:24377 [D loss: 0.577246, acc: 75.00%] [G loss: 2.047441]\n",
      "epoch:26 step:24378 [D loss: 0.624755, acc: 63.28%] [G loss: 2.006018]\n",
      "epoch:26 step:24379 [D loss: 0.588799, acc: 66.41%] [G loss: 1.886195]\n",
      "epoch:26 step:24380 [D loss: 0.641969, acc: 65.62%] [G loss: 1.993026]\n",
      "epoch:26 step:24381 [D loss: 0.690582, acc: 57.03%] [G loss: 1.832718]\n",
      "epoch:26 step:24382 [D loss: 0.708941, acc: 51.56%] [G loss: 1.674259]\n",
      "epoch:26 step:24383 [D loss: 0.646444, acc: 63.28%] [G loss: 1.729499]\n",
      "epoch:26 step:24384 [D loss: 0.660871, acc: 60.16%] [G loss: 1.842638]\n",
      "epoch:26 step:24385 [D loss: 0.673596, acc: 58.59%] [G loss: 2.060605]\n",
      "epoch:26 step:24386 [D loss: 0.636898, acc: 67.19%] [G loss: 1.921417]\n",
      "epoch:26 step:24387 [D loss: 0.617850, acc: 68.75%] [G loss: 2.004778]\n",
      "epoch:26 step:24388 [D loss: 0.634769, acc: 63.28%] [G loss: 1.860538]\n",
      "epoch:26 step:24389 [D loss: 0.652711, acc: 61.72%] [G loss: 1.765518]\n",
      "epoch:26 step:24390 [D loss: 0.639031, acc: 66.41%] [G loss: 1.940232]\n",
      "epoch:26 step:24391 [D loss: 0.635197, acc: 64.84%] [G loss: 1.892622]\n",
      "epoch:26 step:24392 [D loss: 0.665810, acc: 54.69%] [G loss: 1.896139]\n",
      "epoch:26 step:24393 [D loss: 0.690732, acc: 56.25%] [G loss: 1.755293]\n",
      "epoch:26 step:24394 [D loss: 0.708488, acc: 53.91%] [G loss: 1.767490]\n",
      "epoch:26 step:24395 [D loss: 0.656390, acc: 53.91%] [G loss: 1.860406]\n",
      "epoch:26 step:24396 [D loss: 0.633125, acc: 66.41%] [G loss: 1.902559]\n",
      "epoch:26 step:24397 [D loss: 0.642797, acc: 61.72%] [G loss: 1.772212]\n",
      "epoch:26 step:24398 [D loss: 0.633710, acc: 67.19%] [G loss: 1.849699]\n",
      "epoch:26 step:24399 [D loss: 0.648579, acc: 60.94%] [G loss: 1.893798]\n",
      "epoch:26 step:24400 [D loss: 0.682936, acc: 58.59%] [G loss: 2.006352]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.656006\n",
      "FID: 9.199095\n",
      "0 = 12.69946395034789\n",
      "1 = 0.0934978909994118\n",
      "2 = 0.8777999877929688\n",
      "3 = 0.8981999754905701\n",
      "4 = 0.8574000000953674\n",
      "5 = 0.8629900217056274\n",
      "6 = 0.8981999754905701\n",
      "7 = 6.007494361376768\n",
      "8 = 0.053660976408365146\n",
      "9 = 0.710099995136261\n",
      "10 = 0.7156000137329102\n",
      "11 = 0.7045999765396118\n",
      "12 = 0.7078140377998352\n",
      "13 = 0.7156000137329102\n",
      "14 = 7.656044960021973\n",
      "15 = 9.414512634277344\n",
      "16 = 0.12028202414512634\n",
      "17 = 7.656006336212158\n",
      "18 = 9.199094772338867\n",
      "epoch:26 step:24401 [D loss: 0.633965, acc: 67.19%] [G loss: 1.851508]\n",
      "epoch:26 step:24402 [D loss: 0.618039, acc: 63.28%] [G loss: 2.034570]\n",
      "epoch:26 step:24403 [D loss: 0.633163, acc: 63.28%] [G loss: 1.853121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24404 [D loss: 0.638186, acc: 60.94%] [G loss: 1.901744]\n",
      "epoch:26 step:24405 [D loss: 0.637155, acc: 59.38%] [G loss: 2.069542]\n",
      "epoch:26 step:24406 [D loss: 0.617570, acc: 64.84%] [G loss: 1.885112]\n",
      "epoch:26 step:24407 [D loss: 0.696407, acc: 55.47%] [G loss: 1.890074]\n",
      "epoch:26 step:24408 [D loss: 0.597980, acc: 73.44%] [G loss: 1.815331]\n",
      "epoch:26 step:24409 [D loss: 0.580118, acc: 67.19%] [G loss: 1.918472]\n",
      "epoch:26 step:24410 [D loss: 0.616107, acc: 70.31%] [G loss: 1.920978]\n",
      "epoch:26 step:24411 [D loss: 0.686612, acc: 57.81%] [G loss: 1.933805]\n",
      "epoch:26 step:24412 [D loss: 0.580968, acc: 71.09%] [G loss: 2.047892]\n",
      "epoch:26 step:24413 [D loss: 0.635980, acc: 63.28%] [G loss: 1.949583]\n",
      "epoch:26 step:24414 [D loss: 0.616856, acc: 66.41%] [G loss: 1.871685]\n",
      "epoch:26 step:24415 [D loss: 0.659962, acc: 64.84%] [G loss: 2.023580]\n",
      "epoch:26 step:24416 [D loss: 0.635274, acc: 64.06%] [G loss: 1.985937]\n",
      "epoch:26 step:24417 [D loss: 0.614559, acc: 60.94%] [G loss: 1.883555]\n",
      "epoch:26 step:24418 [D loss: 0.632921, acc: 64.06%] [G loss: 1.995615]\n",
      "epoch:26 step:24419 [D loss: 0.643176, acc: 66.41%] [G loss: 1.976546]\n",
      "epoch:26 step:24420 [D loss: 0.665386, acc: 60.16%] [G loss: 1.983895]\n",
      "epoch:26 step:24421 [D loss: 0.610106, acc: 69.53%] [G loss: 2.009394]\n",
      "epoch:26 step:24422 [D loss: 0.616788, acc: 63.28%] [G loss: 1.932805]\n",
      "epoch:26 step:24423 [D loss: 0.670295, acc: 59.38%] [G loss: 1.831752]\n",
      "epoch:26 step:24424 [D loss: 0.614697, acc: 64.84%] [G loss: 1.972318]\n",
      "epoch:26 step:24425 [D loss: 0.654517, acc: 65.62%] [G loss: 1.967713]\n",
      "epoch:26 step:24426 [D loss: 0.707922, acc: 56.25%] [G loss: 1.869030]\n",
      "epoch:26 step:24427 [D loss: 0.682142, acc: 54.69%] [G loss: 1.889871]\n",
      "epoch:26 step:24428 [D loss: 0.655682, acc: 58.59%] [G loss: 1.820922]\n",
      "epoch:26 step:24429 [D loss: 0.645385, acc: 57.03%] [G loss: 1.890770]\n",
      "epoch:26 step:24430 [D loss: 0.646253, acc: 64.84%] [G loss: 2.015432]\n",
      "epoch:26 step:24431 [D loss: 0.606409, acc: 64.84%] [G loss: 2.023563]\n",
      "epoch:26 step:24432 [D loss: 0.607747, acc: 64.84%] [G loss: 1.915133]\n",
      "epoch:26 step:24433 [D loss: 0.652294, acc: 59.38%] [G loss: 1.742383]\n",
      "epoch:26 step:24434 [D loss: 0.661381, acc: 60.94%] [G loss: 1.913062]\n",
      "epoch:26 step:24435 [D loss: 0.672818, acc: 56.25%] [G loss: 1.859596]\n",
      "epoch:26 step:24436 [D loss: 0.633320, acc: 58.59%] [G loss: 1.961903]\n",
      "epoch:26 step:24437 [D loss: 0.655471, acc: 59.38%] [G loss: 2.052580]\n",
      "epoch:26 step:24438 [D loss: 0.665088, acc: 55.47%] [G loss: 1.876560]\n",
      "epoch:26 step:24439 [D loss: 0.588501, acc: 68.75%] [G loss: 2.110765]\n",
      "epoch:26 step:24440 [D loss: 0.697586, acc: 60.16%] [G loss: 1.750912]\n",
      "epoch:26 step:24441 [D loss: 0.662859, acc: 63.28%] [G loss: 1.770889]\n",
      "epoch:26 step:24442 [D loss: 0.675543, acc: 61.72%] [G loss: 1.910298]\n",
      "epoch:26 step:24443 [D loss: 0.668202, acc: 60.16%] [G loss: 1.662983]\n",
      "epoch:26 step:24444 [D loss: 0.623759, acc: 64.84%] [G loss: 1.824048]\n",
      "epoch:26 step:24445 [D loss: 0.666601, acc: 59.38%] [G loss: 1.814056]\n",
      "epoch:26 step:24446 [D loss: 0.596355, acc: 64.84%] [G loss: 1.849646]\n",
      "epoch:26 step:24447 [D loss: 0.685225, acc: 60.16%] [G loss: 1.850157]\n",
      "epoch:26 step:24448 [D loss: 0.677165, acc: 60.16%] [G loss: 1.831313]\n",
      "epoch:26 step:24449 [D loss: 0.684391, acc: 54.69%] [G loss: 1.748009]\n",
      "epoch:26 step:24450 [D loss: 0.641592, acc: 60.16%] [G loss: 1.900299]\n",
      "epoch:26 step:24451 [D loss: 0.634635, acc: 62.50%] [G loss: 1.908826]\n",
      "epoch:26 step:24452 [D loss: 0.619124, acc: 67.97%] [G loss: 1.818565]\n",
      "epoch:26 step:24453 [D loss: 0.691786, acc: 57.03%] [G loss: 1.827896]\n",
      "epoch:26 step:24454 [D loss: 0.667660, acc: 61.72%] [G loss: 1.940562]\n",
      "epoch:26 step:24455 [D loss: 0.618029, acc: 68.75%] [G loss: 1.921830]\n",
      "epoch:26 step:24456 [D loss: 0.642339, acc: 62.50%] [G loss: 1.829854]\n",
      "epoch:26 step:24457 [D loss: 0.680100, acc: 59.38%] [G loss: 1.846210]\n",
      "epoch:26 step:24458 [D loss: 0.642840, acc: 62.50%] [G loss: 1.814101]\n",
      "epoch:26 step:24459 [D loss: 0.667994, acc: 63.28%] [G loss: 1.826470]\n",
      "epoch:26 step:24460 [D loss: 0.709945, acc: 53.12%] [G loss: 1.708320]\n",
      "epoch:26 step:24461 [D loss: 0.657916, acc: 62.50%] [G loss: 1.696138]\n",
      "epoch:26 step:24462 [D loss: 0.646297, acc: 60.16%] [G loss: 1.824043]\n",
      "epoch:26 step:24463 [D loss: 0.624199, acc: 71.88%] [G loss: 1.818800]\n",
      "epoch:26 step:24464 [D loss: 0.669319, acc: 57.81%] [G loss: 1.741936]\n",
      "epoch:26 step:24465 [D loss: 0.618632, acc: 61.72%] [G loss: 1.777246]\n",
      "epoch:26 step:24466 [D loss: 0.685243, acc: 58.59%] [G loss: 1.748784]\n",
      "epoch:26 step:24467 [D loss: 0.675486, acc: 56.25%] [G loss: 1.898205]\n",
      "epoch:26 step:24468 [D loss: 0.628353, acc: 64.84%] [G loss: 1.865236]\n",
      "epoch:26 step:24469 [D loss: 0.648751, acc: 64.06%] [G loss: 2.042270]\n",
      "epoch:26 step:24470 [D loss: 0.647468, acc: 60.94%] [G loss: 1.661305]\n",
      "epoch:26 step:24471 [D loss: 0.658892, acc: 59.38%] [G loss: 1.710029]\n",
      "epoch:26 step:24472 [D loss: 0.653532, acc: 58.59%] [G loss: 1.782836]\n",
      "epoch:26 step:24473 [D loss: 0.635120, acc: 59.38%] [G loss: 1.803224]\n",
      "epoch:26 step:24474 [D loss: 0.633133, acc: 64.84%] [G loss: 2.062693]\n",
      "epoch:26 step:24475 [D loss: 0.631240, acc: 67.19%] [G loss: 2.014233]\n",
      "epoch:26 step:24476 [D loss: 0.573459, acc: 69.53%] [G loss: 2.043072]\n",
      "epoch:26 step:24477 [D loss: 0.640932, acc: 67.19%] [G loss: 1.970027]\n",
      "epoch:26 step:24478 [D loss: 0.634833, acc: 66.41%] [G loss: 2.155660]\n",
      "epoch:26 step:24479 [D loss: 0.664182, acc: 60.16%] [G loss: 1.938896]\n",
      "epoch:26 step:24480 [D loss: 0.640531, acc: 64.06%] [G loss: 1.916042]\n",
      "epoch:26 step:24481 [D loss: 0.597161, acc: 67.97%] [G loss: 2.082058]\n",
      "epoch:26 step:24482 [D loss: 0.615917, acc: 67.19%] [G loss: 1.912757]\n",
      "epoch:26 step:24483 [D loss: 0.605625, acc: 69.53%] [G loss: 1.851620]\n",
      "epoch:26 step:24484 [D loss: 0.704732, acc: 57.81%] [G loss: 2.031832]\n",
      "epoch:26 step:24485 [D loss: 0.662891, acc: 58.59%] [G loss: 1.896155]\n",
      "epoch:26 step:24486 [D loss: 0.603125, acc: 69.53%] [G loss: 1.773745]\n",
      "epoch:26 step:24487 [D loss: 0.686051, acc: 54.69%] [G loss: 1.734260]\n",
      "epoch:26 step:24488 [D loss: 0.614060, acc: 64.06%] [G loss: 1.964161]\n",
      "epoch:26 step:24489 [D loss: 0.672554, acc: 57.81%] [G loss: 1.838470]\n",
      "epoch:26 step:24490 [D loss: 0.672181, acc: 60.94%] [G loss: 1.824905]\n",
      "epoch:26 step:24491 [D loss: 0.681723, acc: 54.69%] [G loss: 1.906447]\n",
      "epoch:26 step:24492 [D loss: 0.629088, acc: 61.72%] [G loss: 1.925838]\n",
      "epoch:26 step:24493 [D loss: 0.673967, acc: 60.16%] [G loss: 1.885258]\n",
      "epoch:26 step:24494 [D loss: 0.656998, acc: 60.94%] [G loss: 1.836221]\n",
      "epoch:26 step:24495 [D loss: 0.664135, acc: 63.28%] [G loss: 1.754638]\n",
      "epoch:26 step:24496 [D loss: 0.647701, acc: 65.62%] [G loss: 1.793552]\n",
      "epoch:26 step:24497 [D loss: 0.618513, acc: 66.41%] [G loss: 1.815747]\n",
      "epoch:26 step:24498 [D loss: 0.708953, acc: 57.03%] [G loss: 1.754700]\n",
      "epoch:26 step:24499 [D loss: 0.691885, acc: 53.12%] [G loss: 1.810355]\n",
      "epoch:26 step:24500 [D loss: 0.643068, acc: 65.62%] [G loss: 1.904449]\n",
      "epoch:26 step:24501 [D loss: 0.682759, acc: 55.47%] [G loss: 1.865125]\n",
      "epoch:26 step:24502 [D loss: 0.674049, acc: 60.94%] [G loss: 1.736875]\n",
      "epoch:26 step:24503 [D loss: 0.650410, acc: 60.16%] [G loss: 1.842159]\n",
      "epoch:26 step:24504 [D loss: 0.675820, acc: 57.03%] [G loss: 1.719231]\n",
      "epoch:26 step:24505 [D loss: 0.712427, acc: 57.03%] [G loss: 1.677365]\n",
      "epoch:26 step:24506 [D loss: 0.690231, acc: 60.16%] [G loss: 1.931857]\n",
      "epoch:26 step:24507 [D loss: 0.688396, acc: 58.59%] [G loss: 1.774304]\n",
      "epoch:26 step:24508 [D loss: 0.662218, acc: 60.16%] [G loss: 1.854902]\n",
      "epoch:26 step:24509 [D loss: 0.646503, acc: 60.94%] [G loss: 1.774566]\n",
      "epoch:26 step:24510 [D loss: 0.677285, acc: 60.16%] [G loss: 1.805896]\n",
      "epoch:26 step:24511 [D loss: 0.639279, acc: 64.84%] [G loss: 1.873322]\n",
      "epoch:26 step:24512 [D loss: 0.596169, acc: 67.97%] [G loss: 1.811324]\n",
      "epoch:26 step:24513 [D loss: 0.617877, acc: 62.50%] [G loss: 1.812281]\n",
      "epoch:26 step:24514 [D loss: 0.684952, acc: 51.56%] [G loss: 1.852002]\n",
      "epoch:26 step:24515 [D loss: 0.638121, acc: 64.06%] [G loss: 1.790766]\n",
      "epoch:26 step:24516 [D loss: 0.630779, acc: 60.16%] [G loss: 1.878506]\n",
      "epoch:26 step:24517 [D loss: 0.590740, acc: 70.31%] [G loss: 1.946607]\n",
      "epoch:26 step:24518 [D loss: 0.617540, acc: 62.50%] [G loss: 1.938509]\n",
      "epoch:26 step:24519 [D loss: 0.616326, acc: 68.75%] [G loss: 1.889212]\n",
      "epoch:26 step:24520 [D loss: 0.633892, acc: 63.28%] [G loss: 1.949376]\n",
      "epoch:26 step:24521 [D loss: 0.603898, acc: 63.28%] [G loss: 1.884815]\n",
      "epoch:26 step:24522 [D loss: 0.645776, acc: 65.62%] [G loss: 1.752585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24523 [D loss: 0.648225, acc: 62.50%] [G loss: 1.871876]\n",
      "epoch:26 step:24524 [D loss: 0.665196, acc: 57.81%] [G loss: 1.840370]\n",
      "epoch:26 step:24525 [D loss: 0.703884, acc: 54.69%] [G loss: 1.785606]\n",
      "epoch:26 step:24526 [D loss: 0.669229, acc: 64.84%] [G loss: 1.716989]\n",
      "epoch:26 step:24527 [D loss: 0.630788, acc: 65.62%] [G loss: 1.862579]\n",
      "epoch:26 step:24528 [D loss: 0.631478, acc: 60.94%] [G loss: 1.798141]\n",
      "epoch:26 step:24529 [D loss: 0.670925, acc: 60.94%] [G loss: 1.976403]\n",
      "epoch:26 step:24530 [D loss: 0.644327, acc: 64.06%] [G loss: 1.866953]\n",
      "epoch:26 step:24531 [D loss: 0.673262, acc: 57.03%] [G loss: 1.955265]\n",
      "epoch:26 step:24532 [D loss: 0.627508, acc: 60.94%] [G loss: 1.797094]\n",
      "epoch:26 step:24533 [D loss: 0.605097, acc: 71.09%] [G loss: 1.860383]\n",
      "epoch:26 step:24534 [D loss: 0.652566, acc: 60.94%] [G loss: 1.918593]\n",
      "epoch:26 step:24535 [D loss: 0.681244, acc: 55.47%] [G loss: 1.864850]\n",
      "epoch:26 step:24536 [D loss: 0.661464, acc: 63.28%] [G loss: 1.802680]\n",
      "epoch:26 step:24537 [D loss: 0.660841, acc: 56.25%] [G loss: 1.902171]\n",
      "epoch:26 step:24538 [D loss: 0.682136, acc: 57.03%] [G loss: 1.718565]\n",
      "epoch:26 step:24539 [D loss: 0.612543, acc: 67.97%] [G loss: 1.823928]\n",
      "epoch:26 step:24540 [D loss: 0.710619, acc: 53.91%] [G loss: 1.852633]\n",
      "epoch:26 step:24541 [D loss: 0.615364, acc: 70.31%] [G loss: 1.870068]\n",
      "epoch:26 step:24542 [D loss: 0.667112, acc: 53.91%] [G loss: 1.702649]\n",
      "epoch:26 step:24543 [D loss: 0.677280, acc: 65.62%] [G loss: 1.850606]\n",
      "epoch:26 step:24544 [D loss: 0.673549, acc: 60.94%] [G loss: 1.930225]\n",
      "epoch:26 step:24545 [D loss: 0.670979, acc: 57.03%] [G loss: 1.765420]\n",
      "epoch:26 step:24546 [D loss: 0.623742, acc: 66.41%] [G loss: 1.858953]\n",
      "epoch:26 step:24547 [D loss: 0.640942, acc: 62.50%] [G loss: 1.914823]\n",
      "epoch:26 step:24548 [D loss: 0.707076, acc: 59.38%] [G loss: 1.744638]\n",
      "epoch:26 step:24549 [D loss: 0.640264, acc: 60.94%] [G loss: 1.887581]\n",
      "epoch:26 step:24550 [D loss: 0.623310, acc: 67.97%] [G loss: 1.917743]\n",
      "epoch:26 step:24551 [D loss: 0.646921, acc: 63.28%] [G loss: 1.867147]\n",
      "epoch:26 step:24552 [D loss: 0.635001, acc: 62.50%] [G loss: 1.909747]\n",
      "epoch:26 step:24553 [D loss: 0.646847, acc: 59.38%] [G loss: 1.974438]\n",
      "epoch:26 step:24554 [D loss: 0.612673, acc: 62.50%] [G loss: 1.864512]\n",
      "epoch:26 step:24555 [D loss: 0.660256, acc: 61.72%] [G loss: 1.830575]\n",
      "epoch:26 step:24556 [D loss: 0.681840, acc: 54.69%] [G loss: 1.999930]\n",
      "epoch:26 step:24557 [D loss: 0.595925, acc: 65.62%] [G loss: 2.024068]\n",
      "epoch:26 step:24558 [D loss: 0.683510, acc: 63.28%] [G loss: 1.818774]\n",
      "epoch:26 step:24559 [D loss: 0.666296, acc: 58.59%] [G loss: 1.826758]\n",
      "epoch:26 step:24560 [D loss: 0.663050, acc: 57.81%] [G loss: 1.900633]\n",
      "epoch:26 step:24561 [D loss: 0.634581, acc: 62.50%] [G loss: 1.947582]\n",
      "epoch:26 step:24562 [D loss: 0.720741, acc: 54.69%] [G loss: 1.691797]\n",
      "epoch:26 step:24563 [D loss: 0.691942, acc: 57.03%] [G loss: 1.761516]\n",
      "epoch:26 step:24564 [D loss: 0.672584, acc: 53.91%] [G loss: 1.852468]\n",
      "epoch:26 step:24565 [D loss: 0.698856, acc: 46.88%] [G loss: 1.702100]\n",
      "epoch:26 step:24566 [D loss: 0.653113, acc: 59.38%] [G loss: 1.881330]\n",
      "epoch:26 step:24567 [D loss: 0.676088, acc: 56.25%] [G loss: 1.802663]\n",
      "epoch:26 step:24568 [D loss: 0.575981, acc: 71.88%] [G loss: 1.879997]\n",
      "epoch:26 step:24569 [D loss: 0.604730, acc: 66.41%] [G loss: 2.097984]\n",
      "epoch:26 step:24570 [D loss: 0.591398, acc: 77.34%] [G loss: 2.193978]\n",
      "epoch:26 step:24571 [D loss: 0.611586, acc: 63.28%] [G loss: 2.008910]\n",
      "epoch:26 step:24572 [D loss: 0.576851, acc: 74.22%] [G loss: 1.716954]\n",
      "epoch:26 step:24573 [D loss: 0.683854, acc: 55.47%] [G loss: 1.747200]\n",
      "epoch:26 step:24574 [D loss: 0.657487, acc: 63.28%] [G loss: 1.789323]\n",
      "epoch:26 step:24575 [D loss: 0.642932, acc: 60.94%] [G loss: 1.845978]\n",
      "epoch:26 step:24576 [D loss: 0.673697, acc: 57.81%] [G loss: 1.812458]\n",
      "epoch:26 step:24577 [D loss: 0.655673, acc: 64.06%] [G loss: 1.751365]\n",
      "epoch:26 step:24578 [D loss: 0.612582, acc: 69.53%] [G loss: 2.100129]\n",
      "epoch:26 step:24579 [D loss: 0.691323, acc: 59.38%] [G loss: 1.911162]\n",
      "epoch:26 step:24580 [D loss: 0.618113, acc: 67.19%] [G loss: 1.919843]\n",
      "epoch:26 step:24581 [D loss: 0.627087, acc: 62.50%] [G loss: 1.964357]\n",
      "epoch:26 step:24582 [D loss: 0.753647, acc: 49.22%] [G loss: 1.732553]\n",
      "epoch:26 step:24583 [D loss: 0.638231, acc: 65.62%] [G loss: 2.038586]\n",
      "epoch:26 step:24584 [D loss: 0.692346, acc: 56.25%] [G loss: 1.907231]\n",
      "epoch:26 step:24585 [D loss: 0.582056, acc: 71.09%] [G loss: 1.907139]\n",
      "epoch:26 step:24586 [D loss: 0.705127, acc: 53.12%] [G loss: 1.872379]\n",
      "epoch:26 step:24587 [D loss: 0.652607, acc: 62.50%] [G loss: 1.853367]\n",
      "epoch:26 step:24588 [D loss: 0.628651, acc: 63.28%] [G loss: 1.868419]\n",
      "epoch:26 step:24589 [D loss: 0.684187, acc: 58.59%] [G loss: 1.822902]\n",
      "epoch:26 step:24590 [D loss: 0.727705, acc: 46.09%] [G loss: 1.673632]\n",
      "epoch:26 step:24591 [D loss: 0.571433, acc: 70.31%] [G loss: 1.898278]\n",
      "epoch:26 step:24592 [D loss: 0.624675, acc: 65.62%] [G loss: 2.139186]\n",
      "epoch:26 step:24593 [D loss: 0.600591, acc: 64.06%] [G loss: 2.201155]\n",
      "epoch:26 step:24594 [D loss: 0.594885, acc: 64.84%] [G loss: 2.355868]\n",
      "epoch:26 step:24595 [D loss: 0.648209, acc: 60.94%] [G loss: 1.774392]\n",
      "epoch:26 step:24596 [D loss: 0.688650, acc: 58.59%] [G loss: 1.876291]\n",
      "epoch:26 step:24597 [D loss: 0.655874, acc: 64.84%] [G loss: 1.695094]\n",
      "epoch:26 step:24598 [D loss: 0.651456, acc: 60.16%] [G loss: 1.848071]\n",
      "epoch:26 step:24599 [D loss: 0.663573, acc: 60.94%] [G loss: 1.918540]\n",
      "epoch:26 step:24600 [D loss: 0.614598, acc: 67.97%] [G loss: 1.856823]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.570939\n",
      "FID: 10.885281\n",
      "0 = 12.782380934905948\n",
      "1 = 0.0948265570402352\n",
      "2 = 0.8754000067710876\n",
      "3 = 0.8849999904632568\n",
      "4 = 0.8658000230789185\n",
      "5 = 0.8683280944824219\n",
      "6 = 0.8849999904632568\n",
      "7 = 6.118505052363881\n",
      "8 = 0.0627920954570883\n",
      "9 = 0.701200008392334\n",
      "10 = 0.7138000130653381\n",
      "11 = 0.6886000037193298\n",
      "12 = 0.6962543725967407\n",
      "13 = 0.7138000130653381\n",
      "14 = 7.57096529006958\n",
      "15 = 9.454444885253906\n",
      "16 = 0.10643848031759262\n",
      "17 = 7.570938587188721\n",
      "18 = 10.88528060913086\n",
      "epoch:26 step:24601 [D loss: 0.664291, acc: 58.59%] [G loss: 1.822072]\n",
      "epoch:26 step:24602 [D loss: 0.644794, acc: 60.16%] [G loss: 1.903776]\n",
      "epoch:26 step:24603 [D loss: 0.612714, acc: 71.09%] [G loss: 1.844324]\n",
      "epoch:26 step:24604 [D loss: 0.601023, acc: 69.53%] [G loss: 2.009099]\n",
      "epoch:26 step:24605 [D loss: 0.632992, acc: 59.38%] [G loss: 1.968360]\n",
      "epoch:26 step:24606 [D loss: 0.673989, acc: 60.16%] [G loss: 2.009299]\n",
      "epoch:26 step:24607 [D loss: 0.568003, acc: 70.31%] [G loss: 2.051055]\n",
      "epoch:26 step:24608 [D loss: 0.747239, acc: 56.25%] [G loss: 1.976344]\n",
      "epoch:26 step:24609 [D loss: 0.654181, acc: 59.38%] [G loss: 1.823004]\n",
      "epoch:26 step:24610 [D loss: 0.657721, acc: 62.50%] [G loss: 1.967130]\n",
      "epoch:26 step:24611 [D loss: 0.719969, acc: 51.56%] [G loss: 1.730551]\n",
      "epoch:26 step:24612 [D loss: 0.698719, acc: 56.25%] [G loss: 1.726807]\n",
      "epoch:26 step:24613 [D loss: 0.669406, acc: 60.94%] [G loss: 1.754650]\n",
      "epoch:26 step:24614 [D loss: 0.678864, acc: 59.38%] [G loss: 1.767427]\n",
      "epoch:26 step:24615 [D loss: 0.678169, acc: 55.47%] [G loss: 1.800808]\n",
      "epoch:26 step:24616 [D loss: 0.630219, acc: 60.94%] [G loss: 1.917948]\n",
      "epoch:26 step:24617 [D loss: 0.664470, acc: 59.38%] [G loss: 1.671181]\n",
      "epoch:26 step:24618 [D loss: 0.617979, acc: 68.75%] [G loss: 1.858977]\n",
      "epoch:26 step:24619 [D loss: 0.639228, acc: 64.84%] [G loss: 1.790986]\n",
      "epoch:26 step:24620 [D loss: 0.650883, acc: 61.72%] [G loss: 1.906566]\n",
      "epoch:26 step:24621 [D loss: 0.690154, acc: 52.34%] [G loss: 1.982259]\n",
      "epoch:26 step:24622 [D loss: 0.629669, acc: 69.53%] [G loss: 1.806856]\n",
      "epoch:26 step:24623 [D loss: 0.653930, acc: 62.50%] [G loss: 1.746029]\n",
      "epoch:26 step:24624 [D loss: 0.618994, acc: 64.84%] [G loss: 1.861984]\n",
      "epoch:26 step:24625 [D loss: 0.647019, acc: 60.16%] [G loss: 1.796181]\n",
      "epoch:26 step:24626 [D loss: 0.615802, acc: 64.06%] [G loss: 2.003138]\n",
      "epoch:26 step:24627 [D loss: 0.649730, acc: 56.25%] [G loss: 1.852514]\n",
      "epoch:26 step:24628 [D loss: 0.709756, acc: 60.16%] [G loss: 1.784345]\n",
      "epoch:26 step:24629 [D loss: 0.675595, acc: 57.03%] [G loss: 1.762930]\n",
      "epoch:26 step:24630 [D loss: 0.649681, acc: 61.72%] [G loss: 1.783113]\n",
      "epoch:26 step:24631 [D loss: 0.705816, acc: 51.56%] [G loss: 1.846846]\n",
      "epoch:26 step:24632 [D loss: 0.633055, acc: 66.41%] [G loss: 1.749973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24633 [D loss: 0.641144, acc: 64.06%] [G loss: 1.849731]\n",
      "epoch:26 step:24634 [D loss: 0.620661, acc: 66.41%] [G loss: 1.765226]\n",
      "epoch:26 step:24635 [D loss: 0.665802, acc: 63.28%] [G loss: 1.789153]\n",
      "epoch:26 step:24636 [D loss: 0.618650, acc: 66.41%] [G loss: 1.948209]\n",
      "epoch:26 step:24637 [D loss: 0.652612, acc: 60.94%] [G loss: 1.836547]\n",
      "epoch:26 step:24638 [D loss: 0.630802, acc: 58.59%] [G loss: 2.161114]\n",
      "epoch:26 step:24639 [D loss: 0.641655, acc: 62.50%] [G loss: 1.792638]\n",
      "epoch:26 step:24640 [D loss: 0.612313, acc: 68.75%] [G loss: 1.751809]\n",
      "epoch:26 step:24641 [D loss: 0.668885, acc: 56.25%] [G loss: 1.882325]\n",
      "epoch:26 step:24642 [D loss: 0.638248, acc: 64.84%] [G loss: 2.069029]\n",
      "epoch:26 step:24643 [D loss: 0.673107, acc: 60.94%] [G loss: 1.763495]\n",
      "epoch:26 step:24644 [D loss: 0.580452, acc: 67.97%] [G loss: 1.889417]\n",
      "epoch:26 step:24645 [D loss: 0.607774, acc: 68.75%] [G loss: 1.839909]\n",
      "epoch:26 step:24646 [D loss: 0.660084, acc: 58.59%] [G loss: 1.849625]\n",
      "epoch:26 step:24647 [D loss: 0.653764, acc: 60.16%] [G loss: 1.807660]\n",
      "epoch:26 step:24648 [D loss: 0.599844, acc: 69.53%] [G loss: 2.079388]\n",
      "epoch:26 step:24649 [D loss: 0.615613, acc: 69.53%] [G loss: 1.895777]\n",
      "epoch:26 step:24650 [D loss: 0.632769, acc: 67.19%] [G loss: 1.818070]\n",
      "epoch:26 step:24651 [D loss: 0.681460, acc: 55.47%] [G loss: 1.817083]\n",
      "epoch:26 step:24652 [D loss: 0.690289, acc: 60.94%] [G loss: 1.798368]\n",
      "epoch:26 step:24653 [D loss: 0.685659, acc: 57.03%] [G loss: 1.899760]\n",
      "epoch:26 step:24654 [D loss: 0.627772, acc: 65.62%] [G loss: 1.845121]\n",
      "epoch:26 step:24655 [D loss: 0.655243, acc: 60.94%] [G loss: 1.834909]\n",
      "epoch:26 step:24656 [D loss: 0.691623, acc: 60.16%] [G loss: 1.886405]\n",
      "epoch:26 step:24657 [D loss: 0.644981, acc: 61.72%] [G loss: 1.856405]\n",
      "epoch:26 step:24658 [D loss: 0.552383, acc: 71.88%] [G loss: 1.916646]\n",
      "epoch:26 step:24659 [D loss: 0.657764, acc: 60.16%] [G loss: 1.762506]\n",
      "epoch:26 step:24660 [D loss: 0.641311, acc: 65.62%] [G loss: 1.946153]\n",
      "epoch:26 step:24661 [D loss: 0.628845, acc: 65.62%] [G loss: 1.952116]\n",
      "epoch:26 step:24662 [D loss: 0.643168, acc: 58.59%] [G loss: 1.765692]\n",
      "epoch:26 step:24663 [D loss: 0.682468, acc: 58.59%] [G loss: 1.735817]\n",
      "epoch:26 step:24664 [D loss: 0.616295, acc: 65.62%] [G loss: 1.901518]\n",
      "epoch:26 step:24665 [D loss: 0.686619, acc: 57.03%] [G loss: 1.868553]\n",
      "epoch:26 step:24666 [D loss: 0.654665, acc: 57.81%] [G loss: 1.841759]\n",
      "epoch:26 step:24667 [D loss: 0.638586, acc: 66.41%] [G loss: 1.812803]\n",
      "epoch:26 step:24668 [D loss: 0.677214, acc: 57.81%] [G loss: 1.936088]\n",
      "epoch:26 step:24669 [D loss: 0.652877, acc: 55.47%] [G loss: 1.874509]\n",
      "epoch:26 step:24670 [D loss: 0.654107, acc: 63.28%] [G loss: 1.817947]\n",
      "epoch:26 step:24671 [D loss: 0.649726, acc: 64.84%] [G loss: 1.886694]\n",
      "epoch:26 step:24672 [D loss: 0.596682, acc: 67.97%] [G loss: 1.904348]\n",
      "epoch:26 step:24673 [D loss: 0.655223, acc: 62.50%] [G loss: 1.872166]\n",
      "epoch:26 step:24674 [D loss: 0.610265, acc: 68.75%] [G loss: 2.084002]\n",
      "epoch:26 step:24675 [D loss: 0.608027, acc: 64.84%] [G loss: 2.071206]\n",
      "epoch:26 step:24676 [D loss: 0.535035, acc: 75.00%] [G loss: 2.217997]\n",
      "epoch:26 step:24677 [D loss: 0.593668, acc: 70.31%] [G loss: 2.264523]\n",
      "epoch:26 step:24678 [D loss: 0.689819, acc: 61.72%] [G loss: 1.824858]\n",
      "epoch:26 step:24679 [D loss: 0.660220, acc: 61.72%] [G loss: 1.898832]\n",
      "epoch:26 step:24680 [D loss: 0.714624, acc: 56.25%] [G loss: 1.995777]\n",
      "epoch:26 step:24681 [D loss: 0.635151, acc: 61.72%] [G loss: 1.764062]\n",
      "epoch:26 step:24682 [D loss: 0.628446, acc: 69.53%] [G loss: 1.894320]\n",
      "epoch:26 step:24683 [D loss: 0.652848, acc: 65.62%] [G loss: 1.817947]\n",
      "epoch:26 step:24684 [D loss: 0.663271, acc: 64.06%] [G loss: 1.772209]\n",
      "epoch:26 step:24685 [D loss: 0.713398, acc: 58.59%] [G loss: 1.774841]\n",
      "epoch:26 step:24686 [D loss: 0.667317, acc: 57.81%] [G loss: 1.774839]\n",
      "epoch:26 step:24687 [D loss: 0.659811, acc: 59.38%] [G loss: 1.887048]\n",
      "epoch:26 step:24688 [D loss: 0.648616, acc: 63.28%] [G loss: 1.884804]\n",
      "epoch:26 step:24689 [D loss: 0.605798, acc: 72.66%] [G loss: 1.777137]\n",
      "epoch:26 step:24690 [D loss: 0.674220, acc: 58.59%] [G loss: 1.792158]\n",
      "epoch:26 step:24691 [D loss: 0.695448, acc: 60.16%] [G loss: 1.848747]\n",
      "epoch:26 step:24692 [D loss: 0.584372, acc: 71.09%] [G loss: 2.002936]\n",
      "epoch:26 step:24693 [D loss: 0.665680, acc: 59.38%] [G loss: 1.889945]\n",
      "epoch:26 step:24694 [D loss: 0.605621, acc: 67.97%] [G loss: 1.913144]\n",
      "epoch:26 step:24695 [D loss: 0.642906, acc: 60.94%] [G loss: 1.818538]\n",
      "epoch:26 step:24696 [D loss: 0.669452, acc: 60.94%] [G loss: 1.920725]\n",
      "epoch:26 step:24697 [D loss: 0.628404, acc: 66.41%] [G loss: 1.993842]\n",
      "epoch:26 step:24698 [D loss: 0.620861, acc: 63.28%] [G loss: 1.886082]\n",
      "epoch:26 step:24699 [D loss: 0.636751, acc: 63.28%] [G loss: 1.908399]\n",
      "epoch:26 step:24700 [D loss: 0.663010, acc: 64.06%] [G loss: 1.899257]\n",
      "epoch:26 step:24701 [D loss: 0.633626, acc: 69.53%] [G loss: 1.912043]\n",
      "epoch:26 step:24702 [D loss: 0.658527, acc: 58.59%] [G loss: 1.921566]\n",
      "epoch:26 step:24703 [D loss: 0.689742, acc: 57.03%] [G loss: 1.874972]\n",
      "epoch:26 step:24704 [D loss: 0.700746, acc: 54.69%] [G loss: 1.791003]\n",
      "epoch:26 step:24705 [D loss: 0.679163, acc: 60.94%] [G loss: 1.921707]\n",
      "epoch:26 step:24706 [D loss: 0.653713, acc: 60.16%] [G loss: 1.856078]\n",
      "epoch:26 step:24707 [D loss: 0.573158, acc: 70.31%] [G loss: 2.068449]\n",
      "epoch:26 step:24708 [D loss: 0.605372, acc: 67.19%] [G loss: 2.217764]\n",
      "epoch:26 step:24709 [D loss: 0.610449, acc: 65.62%] [G loss: 2.278172]\n",
      "epoch:26 step:24710 [D loss: 0.666770, acc: 59.38%] [G loss: 1.907463]\n",
      "epoch:26 step:24711 [D loss: 0.739673, acc: 55.47%] [G loss: 1.702047]\n",
      "epoch:26 step:24712 [D loss: 0.644262, acc: 64.84%] [G loss: 1.818483]\n",
      "epoch:26 step:24713 [D loss: 0.670688, acc: 62.50%] [G loss: 1.755402]\n",
      "epoch:26 step:24714 [D loss: 0.732251, acc: 54.69%] [G loss: 1.781504]\n",
      "epoch:26 step:24715 [D loss: 0.674585, acc: 52.34%] [G loss: 1.927044]\n",
      "epoch:26 step:24716 [D loss: 0.631862, acc: 64.84%] [G loss: 1.944416]\n",
      "epoch:26 step:24717 [D loss: 0.700468, acc: 57.03%] [G loss: 1.789208]\n",
      "epoch:26 step:24718 [D loss: 0.642857, acc: 62.50%] [G loss: 1.847376]\n",
      "epoch:26 step:24719 [D loss: 0.636011, acc: 71.09%] [G loss: 1.924744]\n",
      "epoch:26 step:24720 [D loss: 0.609459, acc: 67.19%] [G loss: 1.918739]\n",
      "epoch:26 step:24721 [D loss: 0.603532, acc: 64.84%] [G loss: 2.030105]\n",
      "epoch:26 step:24722 [D loss: 0.617737, acc: 67.19%] [G loss: 2.043974]\n",
      "epoch:26 step:24723 [D loss: 0.674852, acc: 63.28%] [G loss: 1.872068]\n",
      "epoch:26 step:24724 [D loss: 0.668462, acc: 54.69%] [G loss: 1.822566]\n",
      "epoch:26 step:24725 [D loss: 0.612046, acc: 64.06%] [G loss: 1.831139]\n",
      "epoch:26 step:24726 [D loss: 0.632861, acc: 67.19%] [G loss: 2.002297]\n",
      "epoch:26 step:24727 [D loss: 0.655053, acc: 62.50%] [G loss: 1.862436]\n",
      "epoch:26 step:24728 [D loss: 0.674134, acc: 57.81%] [G loss: 1.929885]\n",
      "epoch:26 step:24729 [D loss: 0.667695, acc: 57.81%] [G loss: 1.834256]\n",
      "epoch:26 step:24730 [D loss: 0.678349, acc: 53.91%] [G loss: 1.803238]\n",
      "epoch:26 step:24731 [D loss: 0.622960, acc: 68.75%] [G loss: 1.858772]\n",
      "epoch:26 step:24732 [D loss: 0.642354, acc: 60.94%] [G loss: 2.084373]\n",
      "epoch:26 step:24733 [D loss: 0.619435, acc: 66.41%] [G loss: 1.948763]\n",
      "epoch:26 step:24734 [D loss: 0.636614, acc: 60.94%] [G loss: 1.906247]\n",
      "epoch:26 step:24735 [D loss: 0.660922, acc: 64.84%] [G loss: 1.783053]\n",
      "epoch:26 step:24736 [D loss: 0.618317, acc: 64.84%] [G loss: 1.948429]\n",
      "epoch:26 step:24737 [D loss: 0.715088, acc: 54.69%] [G loss: 1.851044]\n",
      "epoch:26 step:24738 [D loss: 0.698205, acc: 51.56%] [G loss: 1.915415]\n",
      "epoch:26 step:24739 [D loss: 0.678200, acc: 60.94%] [G loss: 1.735334]\n",
      "epoch:26 step:24740 [D loss: 0.644069, acc: 63.28%] [G loss: 1.731572]\n",
      "epoch:26 step:24741 [D loss: 0.582536, acc: 68.75%] [G loss: 1.885999]\n",
      "epoch:26 step:24742 [D loss: 0.700675, acc: 56.25%] [G loss: 1.927699]\n",
      "epoch:26 step:24743 [D loss: 0.611530, acc: 71.09%] [G loss: 1.978683]\n",
      "epoch:26 step:24744 [D loss: 0.648876, acc: 65.62%] [G loss: 1.913881]\n",
      "epoch:26 step:24745 [D loss: 0.639742, acc: 57.81%] [G loss: 1.747906]\n",
      "epoch:26 step:24746 [D loss: 0.604346, acc: 67.97%] [G loss: 1.955008]\n",
      "epoch:26 step:24747 [D loss: 0.647352, acc: 60.94%] [G loss: 2.012509]\n",
      "epoch:26 step:24748 [D loss: 0.700880, acc: 53.91%] [G loss: 1.853525]\n",
      "epoch:26 step:24749 [D loss: 0.648473, acc: 64.06%] [G loss: 1.814804]\n",
      "epoch:26 step:24750 [D loss: 0.611036, acc: 63.28%] [G loss: 1.828017]\n",
      "epoch:26 step:24751 [D loss: 0.622108, acc: 62.50%] [G loss: 1.856808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24752 [D loss: 0.643826, acc: 59.38%] [G loss: 1.891438]\n",
      "epoch:26 step:24753 [D loss: 0.648388, acc: 58.59%] [G loss: 1.833515]\n",
      "epoch:26 step:24754 [D loss: 0.669838, acc: 57.03%] [G loss: 1.899386]\n",
      "epoch:26 step:24755 [D loss: 0.645254, acc: 61.72%] [G loss: 2.044812]\n",
      "epoch:26 step:24756 [D loss: 0.649836, acc: 63.28%] [G loss: 1.815598]\n",
      "epoch:26 step:24757 [D loss: 0.679209, acc: 58.59%] [G loss: 1.890520]\n",
      "epoch:26 step:24758 [D loss: 0.692912, acc: 56.25%] [G loss: 1.784096]\n",
      "epoch:26 step:24759 [D loss: 0.699509, acc: 54.69%] [G loss: 1.658216]\n",
      "epoch:26 step:24760 [D loss: 0.677255, acc: 63.28%] [G loss: 1.830272]\n",
      "epoch:26 step:24761 [D loss: 0.658093, acc: 66.41%] [G loss: 1.775496]\n",
      "epoch:26 step:24762 [D loss: 0.638355, acc: 62.50%] [G loss: 1.728693]\n",
      "epoch:26 step:24763 [D loss: 0.709983, acc: 54.69%] [G loss: 1.950211]\n",
      "epoch:26 step:24764 [D loss: 0.633495, acc: 64.84%] [G loss: 2.009830]\n",
      "epoch:26 step:24765 [D loss: 0.633685, acc: 61.72%] [G loss: 1.905781]\n",
      "epoch:26 step:24766 [D loss: 0.648585, acc: 68.75%] [G loss: 1.875888]\n",
      "epoch:26 step:24767 [D loss: 0.583655, acc: 70.31%] [G loss: 2.095478]\n",
      "epoch:26 step:24768 [D loss: 0.622888, acc: 70.31%] [G loss: 1.963710]\n",
      "epoch:26 step:24769 [D loss: 0.629071, acc: 65.62%] [G loss: 1.950137]\n",
      "epoch:26 step:24770 [D loss: 0.657213, acc: 62.50%] [G loss: 1.861760]\n",
      "epoch:26 step:24771 [D loss: 0.628903, acc: 63.28%] [G loss: 1.908509]\n",
      "epoch:26 step:24772 [D loss: 0.632445, acc: 62.50%] [G loss: 1.814392]\n",
      "epoch:26 step:24773 [D loss: 0.665115, acc: 64.06%] [G loss: 1.949911]\n",
      "epoch:26 step:24774 [D loss: 0.602029, acc: 68.75%] [G loss: 1.929612]\n",
      "epoch:26 step:24775 [D loss: 0.691384, acc: 57.03%] [G loss: 1.890196]\n",
      "epoch:26 step:24776 [D loss: 0.630231, acc: 64.06%] [G loss: 2.010278]\n",
      "epoch:26 step:24777 [D loss: 0.678961, acc: 64.06%] [G loss: 2.033472]\n",
      "epoch:26 step:24778 [D loss: 0.641807, acc: 60.16%] [G loss: 2.069194]\n",
      "epoch:26 step:24779 [D loss: 0.679565, acc: 57.03%] [G loss: 1.798496]\n",
      "epoch:26 step:24780 [D loss: 0.713367, acc: 54.69%] [G loss: 1.757062]\n",
      "epoch:26 step:24781 [D loss: 0.602523, acc: 67.97%] [G loss: 1.888951]\n",
      "epoch:26 step:24782 [D loss: 0.649047, acc: 60.94%] [G loss: 1.908980]\n",
      "epoch:26 step:24783 [D loss: 0.700493, acc: 58.59%] [G loss: 1.927787]\n",
      "epoch:26 step:24784 [D loss: 0.648333, acc: 61.72%] [G loss: 1.756202]\n",
      "epoch:26 step:24785 [D loss: 0.680621, acc: 62.50%] [G loss: 1.899657]\n",
      "epoch:26 step:24786 [D loss: 0.692476, acc: 60.94%] [G loss: 1.877440]\n",
      "epoch:26 step:24787 [D loss: 0.659375, acc: 58.59%] [G loss: 1.752362]\n",
      "epoch:26 step:24788 [D loss: 0.646688, acc: 60.94%] [G loss: 1.866500]\n",
      "epoch:26 step:24789 [D loss: 0.639665, acc: 65.62%] [G loss: 1.903936]\n",
      "epoch:26 step:24790 [D loss: 0.592355, acc: 65.62%] [G loss: 2.029500]\n",
      "epoch:26 step:24791 [D loss: 0.604061, acc: 64.06%] [G loss: 2.045056]\n",
      "epoch:26 step:24792 [D loss: 0.618549, acc: 68.75%] [G loss: 2.134440]\n",
      "epoch:26 step:24793 [D loss: 0.642447, acc: 69.53%] [G loss: 1.867985]\n",
      "epoch:26 step:24794 [D loss: 0.693919, acc: 57.03%] [G loss: 1.683415]\n",
      "epoch:26 step:24795 [D loss: 0.648405, acc: 62.50%] [G loss: 1.849934]\n",
      "epoch:26 step:24796 [D loss: 0.621209, acc: 67.19%] [G loss: 1.783839]\n",
      "epoch:26 step:24797 [D loss: 0.688358, acc: 62.50%] [G loss: 1.920137]\n",
      "epoch:26 step:24798 [D loss: 0.633888, acc: 60.16%] [G loss: 1.802979]\n",
      "epoch:26 step:24799 [D loss: 0.715288, acc: 50.78%] [G loss: 1.755449]\n",
      "epoch:26 step:24800 [D loss: 0.685643, acc: 54.69%] [G loss: 1.784606]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.567675\n",
      "FID: 11.769661\n",
      "0 = 12.587466048145313\n",
      "1 = 0.08070389546915616\n",
      "2 = 0.8773000240325928\n",
      "3 = 0.8939999938011169\n",
      "4 = 0.8605999946594238\n",
      "5 = 0.8651054501533508\n",
      "6 = 0.8939999938011169\n",
      "7 = 6.133140979480747\n",
      "8 = 0.06522165505666104\n",
      "9 = 0.7059999704360962\n",
      "10 = 0.7167999744415283\n",
      "11 = 0.6952000260353088\n",
      "12 = 0.7016444802284241\n",
      "13 = 0.7167999744415283\n",
      "14 = 7.567702770233154\n",
      "15 = 9.427582740783691\n",
      "16 = 0.1126733124256134\n",
      "17 = 7.56767463684082\n",
      "18 = 11.769660949707031\n",
      "epoch:26 step:24801 [D loss: 0.662240, acc: 57.03%] [G loss: 1.732573]\n",
      "epoch:26 step:24802 [D loss: 0.639295, acc: 60.16%] [G loss: 1.783157]\n",
      "epoch:26 step:24803 [D loss: 0.683737, acc: 51.56%] [G loss: 1.860762]\n",
      "epoch:26 step:24804 [D loss: 0.656657, acc: 59.38%] [G loss: 1.801377]\n",
      "epoch:26 step:24805 [D loss: 0.690729, acc: 56.25%] [G loss: 1.759921]\n",
      "epoch:26 step:24806 [D loss: 0.699589, acc: 54.69%] [G loss: 1.750205]\n",
      "epoch:26 step:24807 [D loss: 0.697320, acc: 57.81%] [G loss: 1.709809]\n",
      "epoch:26 step:24808 [D loss: 0.632099, acc: 55.47%] [G loss: 1.833425]\n",
      "epoch:26 step:24809 [D loss: 0.658580, acc: 68.75%] [G loss: 1.710492]\n",
      "epoch:26 step:24810 [D loss: 0.652236, acc: 60.16%] [G loss: 1.835502]\n",
      "epoch:26 step:24811 [D loss: 0.631591, acc: 64.84%] [G loss: 1.774351]\n",
      "epoch:26 step:24812 [D loss: 0.647616, acc: 64.84%] [G loss: 1.907808]\n",
      "epoch:26 step:24813 [D loss: 0.615076, acc: 66.41%] [G loss: 1.811267]\n",
      "epoch:26 step:24814 [D loss: 0.678978, acc: 60.94%] [G loss: 1.851876]\n",
      "epoch:26 step:24815 [D loss: 0.622356, acc: 67.19%] [G loss: 1.916750]\n",
      "epoch:26 step:24816 [D loss: 0.654004, acc: 59.38%] [G loss: 1.907252]\n",
      "epoch:26 step:24817 [D loss: 0.622355, acc: 64.84%] [G loss: 1.903010]\n",
      "epoch:26 step:24818 [D loss: 0.608747, acc: 66.41%] [G loss: 1.931070]\n",
      "epoch:26 step:24819 [D loss: 0.667597, acc: 57.81%] [G loss: 1.917460]\n",
      "epoch:26 step:24820 [D loss: 0.640403, acc: 62.50%] [G loss: 1.780440]\n",
      "epoch:26 step:24821 [D loss: 0.689403, acc: 59.38%] [G loss: 1.827963]\n",
      "epoch:26 step:24822 [D loss: 0.649337, acc: 64.84%] [G loss: 1.698936]\n",
      "epoch:26 step:24823 [D loss: 0.673281, acc: 60.94%] [G loss: 1.707146]\n",
      "epoch:26 step:24824 [D loss: 0.637548, acc: 62.50%] [G loss: 1.807191]\n",
      "epoch:26 step:24825 [D loss: 0.693794, acc: 57.81%] [G loss: 1.922775]\n",
      "epoch:26 step:24826 [D loss: 0.608429, acc: 69.53%] [G loss: 1.914523]\n",
      "epoch:26 step:24827 [D loss: 0.651704, acc: 65.62%] [G loss: 1.709723]\n",
      "epoch:26 step:24828 [D loss: 0.641910, acc: 61.72%] [G loss: 1.843106]\n",
      "epoch:26 step:24829 [D loss: 0.670187, acc: 61.72%] [G loss: 1.944980]\n",
      "epoch:26 step:24830 [D loss: 0.611375, acc: 66.41%] [G loss: 2.033651]\n",
      "epoch:26 step:24831 [D loss: 0.664769, acc: 57.81%] [G loss: 2.090162]\n",
      "epoch:26 step:24832 [D loss: 0.678059, acc: 55.47%] [G loss: 2.010575]\n",
      "epoch:26 step:24833 [D loss: 0.627545, acc: 67.19%] [G loss: 2.153728]\n",
      "epoch:26 step:24834 [D loss: 0.653717, acc: 64.84%] [G loss: 2.038118]\n",
      "epoch:26 step:24835 [D loss: 0.716675, acc: 56.25%] [G loss: 1.757287]\n",
      "epoch:26 step:24836 [D loss: 0.692413, acc: 59.38%] [G loss: 1.948359]\n",
      "epoch:26 step:24837 [D loss: 0.668179, acc: 57.03%] [G loss: 1.755904]\n",
      "epoch:26 step:24838 [D loss: 0.676401, acc: 60.16%] [G loss: 1.836136]\n",
      "epoch:26 step:24839 [D loss: 0.677634, acc: 59.38%] [G loss: 1.778803]\n",
      "epoch:26 step:24840 [D loss: 0.679041, acc: 56.25%] [G loss: 1.784407]\n",
      "epoch:26 step:24841 [D loss: 0.610610, acc: 69.53%] [G loss: 1.784790]\n",
      "epoch:26 step:24842 [D loss: 0.618043, acc: 67.19%] [G loss: 2.128832]\n",
      "epoch:26 step:24843 [D loss: 0.575547, acc: 73.44%] [G loss: 1.914826]\n",
      "epoch:26 step:24844 [D loss: 0.659604, acc: 60.16%] [G loss: 1.833005]\n",
      "epoch:26 step:24845 [D loss: 0.685578, acc: 54.69%] [G loss: 1.829096]\n",
      "epoch:26 step:24846 [D loss: 0.566204, acc: 74.22%] [G loss: 1.936024]\n",
      "epoch:26 step:24847 [D loss: 0.650772, acc: 61.72%] [G loss: 1.814591]\n",
      "epoch:26 step:24848 [D loss: 0.646378, acc: 63.28%] [G loss: 1.791674]\n",
      "epoch:26 step:24849 [D loss: 0.674433, acc: 63.28%] [G loss: 1.971334]\n",
      "epoch:26 step:24850 [D loss: 0.590685, acc: 68.75%] [G loss: 2.006690]\n",
      "epoch:26 step:24851 [D loss: 0.681699, acc: 55.47%] [G loss: 1.806867]\n",
      "epoch:26 step:24852 [D loss: 0.636926, acc: 67.97%] [G loss: 1.990075]\n",
      "epoch:26 step:24853 [D loss: 0.682485, acc: 57.81%] [G loss: 1.774444]\n",
      "epoch:26 step:24854 [D loss: 0.630716, acc: 64.06%] [G loss: 1.877575]\n",
      "epoch:26 step:24855 [D loss: 0.658845, acc: 60.16%] [G loss: 1.797900]\n",
      "epoch:26 step:24856 [D loss: 0.708270, acc: 53.12%] [G loss: 1.940080]\n",
      "epoch:26 step:24857 [D loss: 0.593679, acc: 71.09%] [G loss: 1.956553]\n",
      "epoch:26 step:24858 [D loss: 0.635041, acc: 65.62%] [G loss: 1.905875]\n",
      "epoch:26 step:24859 [D loss: 0.644354, acc: 62.50%] [G loss: 1.892924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24860 [D loss: 0.674192, acc: 59.38%] [G loss: 2.108955]\n",
      "epoch:26 step:24861 [D loss: 0.603035, acc: 67.97%] [G loss: 2.019927]\n",
      "epoch:26 step:24862 [D loss: 0.687610, acc: 57.03%] [G loss: 1.870273]\n",
      "epoch:26 step:24863 [D loss: 0.710115, acc: 56.25%] [G loss: 1.763022]\n",
      "epoch:26 step:24864 [D loss: 0.633422, acc: 63.28%] [G loss: 1.841217]\n",
      "epoch:26 step:24865 [D loss: 0.657634, acc: 58.59%] [G loss: 1.856758]\n",
      "epoch:26 step:24866 [D loss: 0.603511, acc: 69.53%] [G loss: 1.974968]\n",
      "epoch:26 step:24867 [D loss: 0.676145, acc: 60.94%] [G loss: 2.027872]\n",
      "epoch:26 step:24868 [D loss: 0.666048, acc: 57.81%] [G loss: 1.650635]\n",
      "epoch:26 step:24869 [D loss: 0.694331, acc: 59.38%] [G loss: 1.836834]\n",
      "epoch:26 step:24870 [D loss: 0.605544, acc: 63.28%] [G loss: 1.857252]\n",
      "epoch:26 step:24871 [D loss: 0.654076, acc: 65.62%] [G loss: 1.857708]\n",
      "epoch:26 step:24872 [D loss: 0.663778, acc: 59.38%] [G loss: 1.923873]\n",
      "epoch:26 step:24873 [D loss: 0.743416, acc: 48.44%] [G loss: 1.751371]\n",
      "epoch:26 step:24874 [D loss: 0.703515, acc: 57.03%] [G loss: 1.770297]\n",
      "epoch:26 step:24875 [D loss: 0.673523, acc: 57.81%] [G loss: 1.953444]\n",
      "epoch:26 step:24876 [D loss: 0.669311, acc: 60.94%] [G loss: 1.725757]\n",
      "epoch:26 step:24877 [D loss: 0.639069, acc: 59.38%] [G loss: 1.861150]\n",
      "epoch:26 step:24878 [D loss: 0.645983, acc: 65.62%] [G loss: 1.948968]\n",
      "epoch:26 step:24879 [D loss: 0.612635, acc: 69.53%] [G loss: 1.925931]\n",
      "epoch:26 step:24880 [D loss: 0.673496, acc: 49.22%] [G loss: 1.694911]\n",
      "epoch:26 step:24881 [D loss: 0.625796, acc: 64.06%] [G loss: 1.748290]\n",
      "epoch:26 step:24882 [D loss: 0.614795, acc: 65.62%] [G loss: 1.882153]\n",
      "epoch:26 step:24883 [D loss: 0.576954, acc: 74.22%] [G loss: 1.913666]\n",
      "epoch:26 step:24884 [D loss: 0.649133, acc: 64.84%] [G loss: 1.854869]\n",
      "epoch:26 step:24885 [D loss: 0.615171, acc: 65.62%] [G loss: 1.902850]\n",
      "epoch:26 step:24886 [D loss: 0.718237, acc: 57.03%] [G loss: 1.820313]\n",
      "epoch:26 step:24887 [D loss: 0.657270, acc: 59.38%] [G loss: 1.869210]\n",
      "epoch:26 step:24888 [D loss: 0.643735, acc: 63.28%] [G loss: 1.900998]\n",
      "epoch:26 step:24889 [D loss: 0.659629, acc: 60.94%] [G loss: 1.891669]\n",
      "epoch:26 step:24890 [D loss: 0.668391, acc: 58.59%] [G loss: 1.681720]\n",
      "epoch:26 step:24891 [D loss: 0.621051, acc: 68.75%] [G loss: 1.779872]\n",
      "epoch:26 step:24892 [D loss: 0.654222, acc: 60.94%] [G loss: 1.666789]\n",
      "epoch:26 step:24893 [D loss: 0.687120, acc: 55.47%] [G loss: 1.715899]\n",
      "epoch:26 step:24894 [D loss: 0.607448, acc: 68.75%] [G loss: 1.904525]\n",
      "epoch:26 step:24895 [D loss: 0.619204, acc: 68.75%] [G loss: 1.870058]\n",
      "epoch:26 step:24896 [D loss: 0.615203, acc: 68.75%] [G loss: 1.986851]\n",
      "epoch:26 step:24897 [D loss: 0.662057, acc: 60.94%] [G loss: 1.777294]\n",
      "epoch:26 step:24898 [D loss: 0.631168, acc: 67.97%] [G loss: 2.025721]\n",
      "epoch:26 step:24899 [D loss: 0.650696, acc: 55.47%] [G loss: 1.771037]\n",
      "epoch:26 step:24900 [D loss: 0.711393, acc: 60.16%] [G loss: 1.756817]\n",
      "epoch:26 step:24901 [D loss: 0.662486, acc: 54.69%] [G loss: 1.818614]\n",
      "epoch:26 step:24902 [D loss: 0.665953, acc: 60.94%] [G loss: 1.748281]\n",
      "epoch:26 step:24903 [D loss: 0.679917, acc: 58.59%] [G loss: 1.804244]\n",
      "epoch:26 step:24904 [D loss: 0.677197, acc: 57.81%] [G loss: 1.827048]\n",
      "epoch:26 step:24905 [D loss: 0.640318, acc: 61.72%] [G loss: 1.904115]\n",
      "epoch:26 step:24906 [D loss: 0.693852, acc: 51.56%] [G loss: 1.750913]\n",
      "epoch:26 step:24907 [D loss: 0.637593, acc: 60.94%] [G loss: 1.893646]\n",
      "epoch:26 step:24908 [D loss: 0.646000, acc: 61.72%] [G loss: 1.817851]\n",
      "epoch:26 step:24909 [D loss: 0.691858, acc: 55.47%] [G loss: 1.798001]\n",
      "epoch:26 step:24910 [D loss: 0.610438, acc: 67.19%] [G loss: 1.879319]\n",
      "epoch:26 step:24911 [D loss: 0.638118, acc: 60.94%] [G loss: 1.965305]\n",
      "epoch:26 step:24912 [D loss: 0.630596, acc: 64.06%] [G loss: 1.868520]\n",
      "epoch:26 step:24913 [D loss: 0.624764, acc: 65.62%] [G loss: 2.090780]\n",
      "epoch:26 step:24914 [D loss: 0.642025, acc: 66.41%] [G loss: 1.933058]\n",
      "epoch:26 step:24915 [D loss: 0.692847, acc: 51.56%] [G loss: 1.769922]\n",
      "epoch:26 step:24916 [D loss: 0.616146, acc: 64.06%] [G loss: 1.998072]\n",
      "epoch:26 step:24917 [D loss: 0.658299, acc: 60.94%] [G loss: 1.877568]\n",
      "epoch:26 step:24918 [D loss: 0.600586, acc: 67.97%] [G loss: 2.023684]\n",
      "epoch:26 step:24919 [D loss: 0.613003, acc: 64.84%] [G loss: 2.041881]\n",
      "epoch:26 step:24920 [D loss: 0.672613, acc: 64.84%] [G loss: 2.079256]\n",
      "epoch:26 step:24921 [D loss: 0.675745, acc: 54.69%] [G loss: 1.764132]\n",
      "epoch:26 step:24922 [D loss: 0.681919, acc: 56.25%] [G loss: 1.746932]\n",
      "epoch:26 step:24923 [D loss: 0.645285, acc: 62.50%] [G loss: 1.841525]\n",
      "epoch:26 step:24924 [D loss: 0.676391, acc: 60.16%] [G loss: 1.932593]\n",
      "epoch:26 step:24925 [D loss: 0.645965, acc: 64.06%] [G loss: 1.883077]\n",
      "epoch:26 step:24926 [D loss: 0.632762, acc: 65.62%] [G loss: 2.004045]\n",
      "epoch:26 step:24927 [D loss: 0.707394, acc: 59.38%] [G loss: 1.717474]\n",
      "epoch:26 step:24928 [D loss: 0.779861, acc: 49.22%] [G loss: 1.735367]\n",
      "epoch:26 step:24929 [D loss: 0.636260, acc: 61.72%] [G loss: 1.694335]\n",
      "epoch:26 step:24930 [D loss: 0.679574, acc: 57.81%] [G loss: 1.783152]\n",
      "epoch:26 step:24931 [D loss: 0.676009, acc: 55.47%] [G loss: 1.738556]\n",
      "epoch:26 step:24932 [D loss: 0.648569, acc: 60.16%] [G loss: 1.869270]\n",
      "epoch:26 step:24933 [D loss: 0.623477, acc: 63.28%] [G loss: 1.838450]\n",
      "epoch:26 step:24934 [D loss: 0.640704, acc: 62.50%] [G loss: 1.712250]\n",
      "epoch:26 step:24935 [D loss: 0.677890, acc: 56.25%] [G loss: 1.744823]\n",
      "epoch:26 step:24936 [D loss: 0.630882, acc: 67.97%] [G loss: 1.896461]\n",
      "epoch:26 step:24937 [D loss: 0.597986, acc: 70.31%] [G loss: 1.777473]\n",
      "epoch:26 step:24938 [D loss: 0.704896, acc: 52.34%] [G loss: 1.757930]\n",
      "epoch:26 step:24939 [D loss: 0.674311, acc: 60.16%] [G loss: 1.838016]\n",
      "epoch:26 step:24940 [D loss: 0.632409, acc: 61.72%] [G loss: 1.755625]\n",
      "epoch:26 step:24941 [D loss: 0.643863, acc: 60.94%] [G loss: 1.707194]\n",
      "epoch:26 step:24942 [D loss: 0.640242, acc: 64.84%] [G loss: 1.811543]\n",
      "epoch:26 step:24943 [D loss: 0.648058, acc: 62.50%] [G loss: 1.729663]\n",
      "epoch:26 step:24944 [D loss: 0.662648, acc: 55.47%] [G loss: 1.767060]\n",
      "epoch:26 step:24945 [D loss: 0.682123, acc: 58.59%] [G loss: 1.852202]\n",
      "epoch:26 step:24946 [D loss: 0.697309, acc: 53.91%] [G loss: 1.753424]\n",
      "epoch:26 step:24947 [D loss: 0.635163, acc: 66.41%] [G loss: 1.981653]\n",
      "epoch:26 step:24948 [D loss: 0.642087, acc: 60.16%] [G loss: 1.853098]\n",
      "epoch:26 step:24949 [D loss: 0.637133, acc: 60.94%] [G loss: 1.847438]\n",
      "epoch:26 step:24950 [D loss: 0.655614, acc: 62.50%] [G loss: 1.944097]\n",
      "epoch:26 step:24951 [D loss: 0.667306, acc: 58.59%] [G loss: 1.931521]\n",
      "epoch:26 step:24952 [D loss: 0.596014, acc: 67.97%] [G loss: 1.890231]\n",
      "epoch:26 step:24953 [D loss: 0.637706, acc: 64.84%] [G loss: 1.872712]\n",
      "epoch:26 step:24954 [D loss: 0.620828, acc: 64.84%] [G loss: 2.077569]\n",
      "epoch:26 step:24955 [D loss: 0.641982, acc: 62.50%] [G loss: 1.830284]\n",
      "epoch:26 step:24956 [D loss: 0.690973, acc: 54.69%] [G loss: 1.811490]\n",
      "epoch:26 step:24957 [D loss: 0.632261, acc: 63.28%] [G loss: 1.717896]\n",
      "epoch:26 step:24958 [D loss: 0.692166, acc: 57.81%] [G loss: 1.751436]\n",
      "epoch:26 step:24959 [D loss: 0.663231, acc: 58.59%] [G loss: 1.732082]\n",
      "epoch:26 step:24960 [D loss: 0.677373, acc: 64.84%] [G loss: 1.887018]\n",
      "epoch:26 step:24961 [D loss: 0.615719, acc: 63.28%] [G loss: 1.878866]\n",
      "epoch:26 step:24962 [D loss: 0.655865, acc: 65.62%] [G loss: 1.843442]\n",
      "epoch:26 step:24963 [D loss: 0.698502, acc: 58.59%] [G loss: 1.716112]\n",
      "epoch:26 step:24964 [D loss: 0.655157, acc: 62.50%] [G loss: 1.827271]\n",
      "epoch:26 step:24965 [D loss: 0.644495, acc: 58.59%] [G loss: 1.904046]\n",
      "epoch:26 step:24966 [D loss: 0.645735, acc: 64.06%] [G loss: 1.975915]\n",
      "epoch:26 step:24967 [D loss: 0.649512, acc: 63.28%] [G loss: 1.880037]\n",
      "epoch:26 step:24968 [D loss: 0.698992, acc: 48.44%] [G loss: 1.849517]\n",
      "epoch:26 step:24969 [D loss: 0.613840, acc: 66.41%] [G loss: 1.929235]\n",
      "epoch:26 step:24970 [D loss: 0.628390, acc: 65.62%] [G loss: 1.820282]\n",
      "epoch:26 step:24971 [D loss: 0.660486, acc: 57.81%] [G loss: 1.825854]\n",
      "epoch:26 step:24972 [D loss: 0.670199, acc: 62.50%] [G loss: 1.825885]\n",
      "epoch:26 step:24973 [D loss: 0.684771, acc: 60.16%] [G loss: 1.699137]\n",
      "epoch:26 step:24974 [D loss: 0.669034, acc: 60.16%] [G loss: 1.865905]\n",
      "epoch:26 step:24975 [D loss: 0.661277, acc: 60.16%] [G loss: 1.813116]\n",
      "epoch:26 step:24976 [D loss: 0.711758, acc: 53.91%] [G loss: 1.676098]\n",
      "epoch:26 step:24977 [D loss: 0.703018, acc: 58.59%] [G loss: 1.830010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24978 [D loss: 0.655633, acc: 62.50%] [G loss: 1.773938]\n",
      "epoch:26 step:24979 [D loss: 0.647435, acc: 59.38%] [G loss: 1.734661]\n",
      "epoch:26 step:24980 [D loss: 0.671123, acc: 62.50%] [G loss: 1.822086]\n",
      "epoch:26 step:24981 [D loss: 0.692230, acc: 55.47%] [G loss: 1.687209]\n",
      "epoch:26 step:24982 [D loss: 0.639545, acc: 64.06%] [G loss: 1.815766]\n",
      "epoch:26 step:24983 [D loss: 0.627466, acc: 65.62%] [G loss: 1.791149]\n",
      "epoch:26 step:24984 [D loss: 0.689176, acc: 54.69%] [G loss: 1.964585]\n",
      "epoch:26 step:24985 [D loss: 0.701188, acc: 56.25%] [G loss: 1.737519]\n",
      "epoch:26 step:24986 [D loss: 0.596524, acc: 71.09%] [G loss: 1.974143]\n",
      "epoch:26 step:24987 [D loss: 0.661912, acc: 64.84%] [G loss: 1.798178]\n",
      "epoch:26 step:24988 [D loss: 0.668430, acc: 61.72%] [G loss: 1.795408]\n",
      "epoch:26 step:24989 [D loss: 0.678610, acc: 59.38%] [G loss: 1.641641]\n",
      "epoch:26 step:24990 [D loss: 0.659247, acc: 59.38%] [G loss: 1.770227]\n",
      "epoch:26 step:24991 [D loss: 0.641546, acc: 61.72%] [G loss: 1.864859]\n",
      "epoch:26 step:24992 [D loss: 0.656347, acc: 64.84%] [G loss: 1.758842]\n",
      "epoch:26 step:24993 [D loss: 0.595653, acc: 71.88%] [G loss: 1.897665]\n",
      "epoch:26 step:24994 [D loss: 0.612030, acc: 64.84%] [G loss: 1.915950]\n",
      "epoch:26 step:24995 [D loss: 0.632690, acc: 66.41%] [G loss: 2.031050]\n",
      "epoch:26 step:24996 [D loss: 0.671395, acc: 61.72%] [G loss: 1.910238]\n",
      "epoch:26 step:24997 [D loss: 0.651331, acc: 57.03%] [G loss: 2.006381]\n",
      "epoch:26 step:24998 [D loss: 0.637204, acc: 59.38%] [G loss: 2.005675]\n",
      "epoch:26 step:24999 [D loss: 0.656018, acc: 64.84%] [G loss: 1.885203]\n",
      "epoch:26 step:25000 [D loss: 0.593665, acc: 70.31%] [G loss: 1.899992]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.552904\n",
      "FID: 10.592173\n",
      "0 = 12.482599062776574\n",
      "1 = 0.08157512464779758\n",
      "2 = 0.8629000186920166\n",
      "3 = 0.8795999884605408\n",
      "4 = 0.8461999893188477\n",
      "5 = 0.8511708974838257\n",
      "6 = 0.8795999884605408\n",
      "7 = 6.032886049008391\n",
      "8 = 0.05795447983627548\n",
      "9 = 0.7116000056266785\n",
      "10 = 0.72079998254776\n",
      "11 = 0.7024000287055969\n",
      "12 = 0.7077769041061401\n",
      "13 = 0.72079998254776\n",
      "14 = 7.552936553955078\n",
      "15 = 9.49286937713623\n",
      "16 = 0.10047633945941925\n",
      "17 = 7.552903652191162\n",
      "18 = 10.592172622680664\n",
      "epoch:26 step:25001 [D loss: 0.675258, acc: 58.59%] [G loss: 1.899887]\n",
      "epoch:26 step:25002 [D loss: 0.652872, acc: 60.94%] [G loss: 1.794756]\n",
      "epoch:26 step:25003 [D loss: 0.652094, acc: 60.16%] [G loss: 1.933060]\n",
      "epoch:26 step:25004 [D loss: 0.629381, acc: 64.84%] [G loss: 1.796881]\n",
      "epoch:26 step:25005 [D loss: 0.642656, acc: 66.41%] [G loss: 1.860078]\n",
      "epoch:26 step:25006 [D loss: 0.640469, acc: 64.06%] [G loss: 2.048750]\n",
      "epoch:26 step:25007 [D loss: 0.587578, acc: 68.75%] [G loss: 2.046037]\n",
      "epoch:26 step:25008 [D loss: 0.640120, acc: 66.41%] [G loss: 2.204936]\n",
      "epoch:26 step:25009 [D loss: 0.632350, acc: 64.06%] [G loss: 2.203672]\n",
      "epoch:26 step:25010 [D loss: 0.592355, acc: 69.53%] [G loss: 2.358539]\n",
      "epoch:26 step:25011 [D loss: 0.602049, acc: 66.41%] [G loss: 2.161669]\n",
      "epoch:26 step:25012 [D loss: 0.591726, acc: 71.09%] [G loss: 2.194202]\n",
      "epoch:26 step:25013 [D loss: 0.659482, acc: 61.72%] [G loss: 1.955986]\n",
      "epoch:26 step:25014 [D loss: 0.718503, acc: 53.91%] [G loss: 1.774532]\n",
      "epoch:26 step:25015 [D loss: 0.721273, acc: 55.47%] [G loss: 1.905680]\n",
      "epoch:26 step:25016 [D loss: 0.582702, acc: 72.66%] [G loss: 2.025294]\n",
      "epoch:26 step:25017 [D loss: 0.639328, acc: 60.16%] [G loss: 1.911500]\n",
      "epoch:26 step:25018 [D loss: 0.701633, acc: 59.38%] [G loss: 1.861274]\n",
      "epoch:26 step:25019 [D loss: 0.689028, acc: 60.16%] [G loss: 1.723130]\n",
      "epoch:26 step:25020 [D loss: 0.672849, acc: 58.59%] [G loss: 1.837157]\n",
      "epoch:26 step:25021 [D loss: 0.653846, acc: 59.38%] [G loss: 1.815572]\n",
      "epoch:26 step:25022 [D loss: 0.638114, acc: 57.03%] [G loss: 1.857776]\n",
      "epoch:26 step:25023 [D loss: 0.670382, acc: 60.94%] [G loss: 1.710812]\n",
      "epoch:26 step:25024 [D loss: 0.648738, acc: 65.62%] [G loss: 1.726885]\n",
      "epoch:26 step:25025 [D loss: 0.674908, acc: 57.03%] [G loss: 1.770465]\n",
      "epoch:26 step:25026 [D loss: 0.668759, acc: 58.59%] [G loss: 1.803045]\n",
      "epoch:26 step:25027 [D loss: 0.663748, acc: 64.06%] [G loss: 1.905030]\n",
      "epoch:26 step:25028 [D loss: 0.647130, acc: 60.16%] [G loss: 1.776334]\n",
      "epoch:26 step:25029 [D loss: 0.647080, acc: 60.16%] [G loss: 1.721243]\n",
      "epoch:26 step:25030 [D loss: 0.619741, acc: 65.62%] [G loss: 1.819090]\n",
      "epoch:26 step:25031 [D loss: 0.674360, acc: 60.94%] [G loss: 1.779202]\n",
      "epoch:26 step:25032 [D loss: 0.718096, acc: 57.03%] [G loss: 1.708765]\n",
      "epoch:26 step:25033 [D loss: 0.684966, acc: 57.81%] [G loss: 1.787181]\n",
      "epoch:26 step:25034 [D loss: 0.643078, acc: 63.28%] [G loss: 1.730599]\n",
      "epoch:26 step:25035 [D loss: 0.680870, acc: 55.47%] [G loss: 1.731540]\n",
      "epoch:26 step:25036 [D loss: 0.646226, acc: 63.28%] [G loss: 1.822853]\n",
      "epoch:26 step:25037 [D loss: 0.654352, acc: 60.94%] [G loss: 1.766537]\n",
      "epoch:26 step:25038 [D loss: 0.648315, acc: 58.59%] [G loss: 1.683239]\n",
      "epoch:26 step:25039 [D loss: 0.652359, acc: 67.97%] [G loss: 1.928309]\n",
      "epoch:26 step:25040 [D loss: 0.649894, acc: 63.28%] [G loss: 1.832489]\n",
      "epoch:26 step:25041 [D loss: 0.614226, acc: 64.06%] [G loss: 1.872648]\n",
      "epoch:26 step:25042 [D loss: 0.637331, acc: 64.06%] [G loss: 1.837657]\n",
      "epoch:26 step:25043 [D loss: 0.580584, acc: 74.22%] [G loss: 2.030082]\n",
      "epoch:26 step:25044 [D loss: 0.729155, acc: 50.78%] [G loss: 1.840843]\n",
      "epoch:26 step:25045 [D loss: 0.616982, acc: 68.75%] [G loss: 1.729818]\n",
      "epoch:26 step:25046 [D loss: 0.637733, acc: 61.72%] [G loss: 1.784747]\n",
      "epoch:26 step:25047 [D loss: 0.624969, acc: 66.41%] [G loss: 1.669389]\n",
      "epoch:26 step:25048 [D loss: 0.652840, acc: 54.69%] [G loss: 1.895578]\n",
      "epoch:26 step:25049 [D loss: 0.650696, acc: 59.38%] [G loss: 1.849915]\n",
      "epoch:26 step:25050 [D loss: 0.639901, acc: 68.75%] [G loss: 1.804651]\n",
      "epoch:26 step:25051 [D loss: 0.602651, acc: 64.06%] [G loss: 1.794371]\n",
      "epoch:26 step:25052 [D loss: 0.642087, acc: 60.94%] [G loss: 1.884068]\n",
      "epoch:26 step:25053 [D loss: 0.601642, acc: 63.28%] [G loss: 1.997046]\n",
      "epoch:26 step:25054 [D loss: 0.640715, acc: 62.50%] [G loss: 1.898201]\n",
      "epoch:26 step:25055 [D loss: 0.568189, acc: 69.53%] [G loss: 2.042507]\n",
      "epoch:26 step:25056 [D loss: 0.638815, acc: 65.62%] [G loss: 2.033305]\n",
      "epoch:26 step:25057 [D loss: 0.643003, acc: 61.72%] [G loss: 1.934985]\n",
      "epoch:26 step:25058 [D loss: 0.671598, acc: 57.03%] [G loss: 1.818299]\n",
      "epoch:26 step:25059 [D loss: 0.634922, acc: 69.53%] [G loss: 1.797369]\n",
      "epoch:26 step:25060 [D loss: 0.657474, acc: 62.50%] [G loss: 1.820992]\n",
      "epoch:26 step:25061 [D loss: 0.620501, acc: 64.84%] [G loss: 1.794138]\n",
      "epoch:26 step:25062 [D loss: 0.616223, acc: 64.84%] [G loss: 2.113320]\n",
      "epoch:26 step:25063 [D loss: 0.614530, acc: 66.41%] [G loss: 1.781252]\n",
      "epoch:26 step:25064 [D loss: 0.760190, acc: 51.56%] [G loss: 1.790704]\n",
      "epoch:26 step:25065 [D loss: 0.684086, acc: 53.91%] [G loss: 1.802752]\n",
      "epoch:26 step:25066 [D loss: 0.710164, acc: 53.12%] [G loss: 1.708259]\n",
      "epoch:26 step:25067 [D loss: 0.671356, acc: 56.25%] [G loss: 1.748333]\n",
      "epoch:26 step:25068 [D loss: 0.599394, acc: 68.75%] [G loss: 1.788356]\n",
      "epoch:26 step:25069 [D loss: 0.605291, acc: 70.31%] [G loss: 1.956056]\n",
      "epoch:26 step:25070 [D loss: 0.597933, acc: 70.31%] [G loss: 2.006361]\n",
      "epoch:26 step:25071 [D loss: 0.666121, acc: 61.72%] [G loss: 1.969781]\n",
      "epoch:26 step:25072 [D loss: 0.618586, acc: 66.41%] [G loss: 1.836596]\n",
      "epoch:26 step:25073 [D loss: 0.661170, acc: 60.16%] [G loss: 1.911033]\n",
      "epoch:26 step:25074 [D loss: 0.633094, acc: 64.84%] [G loss: 1.833040]\n",
      "epoch:26 step:25075 [D loss: 0.677747, acc: 56.25%] [G loss: 1.903115]\n",
      "epoch:26 step:25076 [D loss: 0.641375, acc: 64.06%] [G loss: 1.765362]\n",
      "epoch:26 step:25077 [D loss: 0.669138, acc: 62.50%] [G loss: 1.827334]\n",
      "epoch:26 step:25078 [D loss: 0.704947, acc: 56.25%] [G loss: 1.719812]\n",
      "epoch:26 step:25079 [D loss: 0.720979, acc: 59.38%] [G loss: 1.784908]\n",
      "epoch:26 step:25080 [D loss: 0.652075, acc: 54.69%] [G loss: 1.721646]\n",
      "epoch:26 step:25081 [D loss: 0.593740, acc: 64.84%] [G loss: 1.961157]\n",
      "epoch:26 step:25082 [D loss: 0.650304, acc: 67.97%] [G loss: 1.815604]\n",
      "epoch:26 step:25083 [D loss: 0.604132, acc: 63.28%] [G loss: 1.949482]\n",
      "epoch:26 step:25084 [D loss: 0.681559, acc: 54.69%] [G loss: 1.837909]\n",
      "epoch:26 step:25085 [D loss: 0.675333, acc: 57.81%] [G loss: 1.864817]\n",
      "epoch:26 step:25086 [D loss: 0.663616, acc: 59.38%] [G loss: 1.838718]\n",
      "epoch:26 step:25087 [D loss: 0.657869, acc: 64.06%] [G loss: 1.832669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25088 [D loss: 0.628517, acc: 66.41%] [G loss: 1.967908]\n",
      "epoch:26 step:25089 [D loss: 0.637301, acc: 65.62%] [G loss: 1.683943]\n",
      "epoch:26 step:25090 [D loss: 0.683433, acc: 53.12%] [G loss: 1.783172]\n",
      "epoch:26 step:25091 [D loss: 0.699503, acc: 58.59%] [G loss: 1.855932]\n",
      "epoch:26 step:25092 [D loss: 0.661320, acc: 62.50%] [G loss: 1.798716]\n",
      "epoch:26 step:25093 [D loss: 0.654895, acc: 59.38%] [G loss: 1.642910]\n",
      "epoch:26 step:25094 [D loss: 0.647356, acc: 62.50%] [G loss: 1.970488]\n",
      "epoch:26 step:25095 [D loss: 0.616415, acc: 67.19%] [G loss: 1.801926]\n",
      "epoch:26 step:25096 [D loss: 0.638225, acc: 65.62%] [G loss: 1.811112]\n",
      "epoch:26 step:25097 [D loss: 0.637285, acc: 67.19%] [G loss: 1.982867]\n",
      "epoch:26 step:25098 [D loss: 0.604934, acc: 67.19%] [G loss: 1.805994]\n",
      "epoch:26 step:25099 [D loss: 0.697757, acc: 53.12%] [G loss: 1.774538]\n",
      "epoch:26 step:25100 [D loss: 0.651433, acc: 56.25%] [G loss: 1.745661]\n",
      "epoch:26 step:25101 [D loss: 0.675456, acc: 59.38%] [G loss: 1.750916]\n",
      "epoch:26 step:25102 [D loss: 0.630071, acc: 63.28%] [G loss: 1.980606]\n",
      "epoch:26 step:25103 [D loss: 0.656872, acc: 60.16%] [G loss: 1.824041]\n",
      "epoch:26 step:25104 [D loss: 0.687616, acc: 51.56%] [G loss: 1.810227]\n",
      "epoch:26 step:25105 [D loss: 0.624717, acc: 64.06%] [G loss: 1.839570]\n",
      "epoch:26 step:25106 [D loss: 0.659938, acc: 61.72%] [G loss: 1.847654]\n",
      "epoch:26 step:25107 [D loss: 0.702665, acc: 58.59%] [G loss: 1.878374]\n",
      "epoch:26 step:25108 [D loss: 0.678064, acc: 63.28%] [G loss: 2.043768]\n",
      "epoch:26 step:25109 [D loss: 0.610089, acc: 70.31%] [G loss: 1.903953]\n",
      "epoch:26 step:25110 [D loss: 0.618477, acc: 66.41%] [G loss: 1.814556]\n",
      "epoch:26 step:25111 [D loss: 0.645842, acc: 62.50%] [G loss: 1.775918]\n",
      "epoch:26 step:25112 [D loss: 0.681507, acc: 57.81%] [G loss: 1.850647]\n",
      "epoch:26 step:25113 [D loss: 0.648203, acc: 65.62%] [G loss: 1.926463]\n",
      "epoch:26 step:25114 [D loss: 0.664396, acc: 62.50%] [G loss: 1.786378]\n",
      "epoch:26 step:25115 [D loss: 0.639291, acc: 61.72%] [G loss: 1.873554]\n",
      "epoch:26 step:25116 [D loss: 0.658755, acc: 58.59%] [G loss: 1.811399]\n",
      "epoch:26 step:25117 [D loss: 0.634064, acc: 61.72%] [G loss: 1.762879]\n",
      "epoch:26 step:25118 [D loss: 0.632959, acc: 66.41%] [G loss: 1.886088]\n",
      "epoch:26 step:25119 [D loss: 0.657118, acc: 58.59%] [G loss: 1.784990]\n",
      "epoch:26 step:25120 [D loss: 0.647078, acc: 67.19%] [G loss: 1.751604]\n",
      "epoch:26 step:25121 [D loss: 0.692878, acc: 56.25%] [G loss: 1.801768]\n",
      "epoch:26 step:25122 [D loss: 0.659403, acc: 64.06%] [G loss: 1.827761]\n",
      "epoch:26 step:25123 [D loss: 0.648644, acc: 59.38%] [G loss: 1.890941]\n",
      "epoch:26 step:25124 [D loss: 0.660635, acc: 60.16%] [G loss: 1.783607]\n",
      "epoch:26 step:25125 [D loss: 0.625571, acc: 60.16%] [G loss: 1.871622]\n",
      "epoch:26 step:25126 [D loss: 0.686836, acc: 57.03%] [G loss: 1.817183]\n",
      "epoch:26 step:25127 [D loss: 0.655941, acc: 65.62%] [G loss: 1.735962]\n",
      "epoch:26 step:25128 [D loss: 0.702060, acc: 57.03%] [G loss: 1.805909]\n",
      "epoch:26 step:25129 [D loss: 0.672792, acc: 57.03%] [G loss: 1.796708]\n",
      "epoch:26 step:25130 [D loss: 0.639897, acc: 64.06%] [G loss: 1.875087]\n",
      "epoch:26 step:25131 [D loss: 0.614213, acc: 65.62%] [G loss: 1.979881]\n",
      "epoch:26 step:25132 [D loss: 0.634756, acc: 67.19%] [G loss: 1.953278]\n",
      "epoch:26 step:25133 [D loss: 0.635742, acc: 67.97%] [G loss: 1.854342]\n",
      "epoch:26 step:25134 [D loss: 0.665793, acc: 58.59%] [G loss: 1.893907]\n",
      "epoch:26 step:25135 [D loss: 0.661299, acc: 62.50%] [G loss: 1.720323]\n",
      "epoch:26 step:25136 [D loss: 0.623857, acc: 62.50%] [G loss: 1.944257]\n",
      "epoch:26 step:25137 [D loss: 0.609811, acc: 65.62%] [G loss: 2.030513]\n",
      "epoch:26 step:25138 [D loss: 0.644295, acc: 64.84%] [G loss: 1.883445]\n",
      "epoch:26 step:25139 [D loss: 0.712224, acc: 55.47%] [G loss: 1.884061]\n",
      "epoch:26 step:25140 [D loss: 0.652095, acc: 60.16%] [G loss: 1.753382]\n",
      "epoch:26 step:25141 [D loss: 0.639958, acc: 67.97%] [G loss: 1.857994]\n",
      "epoch:26 step:25142 [D loss: 0.640336, acc: 63.28%] [G loss: 1.875980]\n",
      "epoch:26 step:25143 [D loss: 0.561183, acc: 69.53%] [G loss: 2.023718]\n",
      "epoch:26 step:25144 [D loss: 0.609630, acc: 63.28%] [G loss: 2.086274]\n",
      "epoch:26 step:25145 [D loss: 0.660025, acc: 63.28%] [G loss: 1.906255]\n",
      "epoch:26 step:25146 [D loss: 0.668891, acc: 58.59%] [G loss: 1.757104]\n",
      "epoch:26 step:25147 [D loss: 0.644955, acc: 70.31%] [G loss: 1.907558]\n",
      "epoch:26 step:25148 [D loss: 0.671253, acc: 57.03%] [G loss: 1.978777]\n",
      "epoch:26 step:25149 [D loss: 0.647261, acc: 60.16%] [G loss: 1.847631]\n",
      "epoch:26 step:25150 [D loss: 0.674448, acc: 60.94%] [G loss: 1.721593]\n",
      "epoch:26 step:25151 [D loss: 0.654497, acc: 63.28%] [G loss: 1.905254]\n",
      "epoch:26 step:25152 [D loss: 0.648867, acc: 65.62%] [G loss: 1.852777]\n",
      "epoch:26 step:25153 [D loss: 0.619882, acc: 64.06%] [G loss: 1.957918]\n",
      "epoch:26 step:25154 [D loss: 0.629760, acc: 59.38%] [G loss: 2.048423]\n",
      "epoch:26 step:25155 [D loss: 0.643853, acc: 61.72%] [G loss: 1.903732]\n",
      "epoch:26 step:25156 [D loss: 0.705205, acc: 49.22%] [G loss: 1.732292]\n",
      "epoch:26 step:25157 [D loss: 0.710636, acc: 55.47%] [G loss: 1.783097]\n",
      "epoch:26 step:25158 [D loss: 0.668866, acc: 59.38%] [G loss: 1.757585]\n",
      "epoch:26 step:25159 [D loss: 0.663395, acc: 55.47%] [G loss: 1.715775]\n",
      "epoch:26 step:25160 [D loss: 0.680428, acc: 57.81%] [G loss: 1.761615]\n",
      "epoch:26 step:25161 [D loss: 0.647674, acc: 61.72%] [G loss: 1.781337]\n",
      "epoch:26 step:25162 [D loss: 0.674109, acc: 57.03%] [G loss: 1.642296]\n",
      "epoch:26 step:25163 [D loss: 0.656512, acc: 66.41%] [G loss: 1.649544]\n",
      "epoch:26 step:25164 [D loss: 0.657052, acc: 57.03%] [G loss: 1.747043]\n",
      "epoch:26 step:25165 [D loss: 0.651989, acc: 60.16%] [G loss: 1.853457]\n",
      "epoch:26 step:25166 [D loss: 0.654263, acc: 60.16%] [G loss: 1.918796]\n",
      "epoch:26 step:25167 [D loss: 0.643549, acc: 64.84%] [G loss: 1.845034]\n",
      "epoch:26 step:25168 [D loss: 0.644941, acc: 64.84%] [G loss: 1.942947]\n",
      "epoch:26 step:25169 [D loss: 0.697550, acc: 56.25%] [G loss: 1.756781]\n",
      "epoch:26 step:25170 [D loss: 0.588324, acc: 69.53%] [G loss: 1.952752]\n",
      "epoch:26 step:25171 [D loss: 0.664343, acc: 62.50%] [G loss: 1.793812]\n",
      "epoch:26 step:25172 [D loss: 0.684901, acc: 56.25%] [G loss: 1.845504]\n",
      "epoch:26 step:25173 [D loss: 0.651073, acc: 60.94%] [G loss: 1.849457]\n",
      "epoch:26 step:25174 [D loss: 0.673707, acc: 57.03%] [G loss: 1.879986]\n",
      "epoch:26 step:25175 [D loss: 0.706804, acc: 56.25%] [G loss: 1.874490]\n",
      "epoch:26 step:25176 [D loss: 0.637719, acc: 64.84%] [G loss: 2.011761]\n",
      "epoch:26 step:25177 [D loss: 0.643085, acc: 61.72%] [G loss: 2.074569]\n",
      "epoch:26 step:25178 [D loss: 0.576172, acc: 75.00%] [G loss: 1.972589]\n",
      "epoch:26 step:25179 [D loss: 0.685638, acc: 57.03%] [G loss: 1.845094]\n",
      "epoch:26 step:25180 [D loss: 0.668044, acc: 56.25%] [G loss: 1.714314]\n",
      "epoch:26 step:25181 [D loss: 0.638929, acc: 60.16%] [G loss: 1.929680]\n",
      "epoch:26 step:25182 [D loss: 0.691495, acc: 54.69%] [G loss: 1.614812]\n",
      "epoch:26 step:25183 [D loss: 0.640482, acc: 63.28%] [G loss: 1.769019]\n",
      "epoch:26 step:25184 [D loss: 0.656204, acc: 66.41%] [G loss: 1.896930]\n",
      "epoch:26 step:25185 [D loss: 0.596818, acc: 70.31%] [G loss: 1.811661]\n",
      "epoch:26 step:25186 [D loss: 0.678489, acc: 54.69%] [G loss: 1.831316]\n",
      "epoch:26 step:25187 [D loss: 0.626246, acc: 64.84%] [G loss: 1.834898]\n",
      "epoch:26 step:25188 [D loss: 0.609174, acc: 66.41%] [G loss: 1.961540]\n",
      "epoch:26 step:25189 [D loss: 0.675671, acc: 58.59%] [G loss: 1.807856]\n",
      "epoch:26 step:25190 [D loss: 0.673049, acc: 60.16%] [G loss: 1.763508]\n",
      "epoch:26 step:25191 [D loss: 0.697876, acc: 50.78%] [G loss: 1.792031]\n",
      "epoch:26 step:25192 [D loss: 0.656296, acc: 63.28%] [G loss: 1.785326]\n",
      "epoch:26 step:25193 [D loss: 0.628520, acc: 60.16%] [G loss: 1.909179]\n",
      "epoch:26 step:25194 [D loss: 0.648567, acc: 62.50%] [G loss: 1.803947]\n",
      "epoch:26 step:25195 [D loss: 0.585511, acc: 71.09%] [G loss: 1.985331]\n",
      "epoch:26 step:25196 [D loss: 0.647645, acc: 68.75%] [G loss: 1.860504]\n",
      "epoch:26 step:25197 [D loss: 0.685884, acc: 57.03%] [G loss: 1.846263]\n",
      "epoch:26 step:25198 [D loss: 0.650391, acc: 66.41%] [G loss: 1.736579]\n",
      "epoch:26 step:25199 [D loss: 0.589491, acc: 68.75%] [G loss: 1.966933]\n",
      "epoch:26 step:25200 [D loss: 0.657467, acc: 57.81%] [G loss: 1.855914]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.521503\n",
      "FID: 11.857781\n",
      "0 = 12.562173764181146\n",
      "1 = 0.08564222065191279\n",
      "2 = 0.8673999905586243\n",
      "3 = 0.8866000175476074\n",
      "4 = 0.8482000231742859\n",
      "5 = 0.8538135886192322\n",
      "6 = 0.8866000175476074\n",
      "7 = 6.202895256018631\n",
      "8 = 0.0661418362909178\n",
      "9 = 0.7045000195503235\n",
      "10 = 0.7232000231742859\n",
      "11 = 0.6858000159263611\n",
      "12 = 0.6971274614334106\n",
      "13 = 0.7232000231742859\n",
      "14 = 7.521540641784668\n",
      "15 = 9.339078903198242\n",
      "16 = 0.13258761167526245\n",
      "17 = 7.52150297164917\n",
      "18 = 11.857781410217285\n",
      "epoch:26 step:25201 [D loss: 0.649773, acc: 60.16%] [G loss: 1.893670]\n",
      "epoch:26 step:25202 [D loss: 0.647728, acc: 61.72%] [G loss: 1.895728]\n",
      "epoch:26 step:25203 [D loss: 0.649066, acc: 62.50%] [G loss: 1.864804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25204 [D loss: 0.623245, acc: 67.19%] [G loss: 1.957674]\n",
      "epoch:26 step:25205 [D loss: 0.693623, acc: 59.38%] [G loss: 1.870306]\n",
      "epoch:26 step:25206 [D loss: 0.673971, acc: 58.59%] [G loss: 1.749960]\n",
      "epoch:26 step:25207 [D loss: 0.645052, acc: 60.16%] [G loss: 2.011419]\n",
      "epoch:26 step:25208 [D loss: 0.621088, acc: 67.97%] [G loss: 1.922548]\n",
      "epoch:26 step:25209 [D loss: 0.649938, acc: 67.19%] [G loss: 1.914831]\n",
      "epoch:26 step:25210 [D loss: 0.681978, acc: 59.38%] [G loss: 1.853932]\n",
      "epoch:26 step:25211 [D loss: 0.652099, acc: 67.97%] [G loss: 1.971807]\n",
      "epoch:26 step:25212 [D loss: 0.630125, acc: 61.72%] [G loss: 1.888508]\n",
      "epoch:26 step:25213 [D loss: 0.659103, acc: 60.94%] [G loss: 1.779927]\n",
      "epoch:26 step:25214 [D loss: 0.664360, acc: 56.25%] [G loss: 1.885607]\n",
      "epoch:26 step:25215 [D loss: 0.672540, acc: 61.72%] [G loss: 1.953325]\n",
      "epoch:26 step:25216 [D loss: 0.673842, acc: 57.81%] [G loss: 1.740903]\n",
      "epoch:26 step:25217 [D loss: 0.640902, acc: 60.16%] [G loss: 1.753989]\n",
      "epoch:26 step:25218 [D loss: 0.689551, acc: 58.59%] [G loss: 1.808094]\n",
      "epoch:26 step:25219 [D loss: 0.636985, acc: 62.50%] [G loss: 1.958945]\n",
      "epoch:26 step:25220 [D loss: 0.701765, acc: 57.03%] [G loss: 1.796937]\n",
      "epoch:26 step:25221 [D loss: 0.687997, acc: 57.81%] [G loss: 1.774091]\n",
      "epoch:26 step:25222 [D loss: 0.624957, acc: 66.41%] [G loss: 1.784979]\n",
      "epoch:26 step:25223 [D loss: 0.653933, acc: 58.59%] [G loss: 1.628700]\n",
      "epoch:26 step:25224 [D loss: 0.656113, acc: 57.81%] [G loss: 1.668205]\n",
      "epoch:26 step:25225 [D loss: 0.647558, acc: 61.72%] [G loss: 1.817896]\n",
      "epoch:26 step:25226 [D loss: 0.703225, acc: 57.81%] [G loss: 1.690487]\n",
      "epoch:26 step:25227 [D loss: 0.691456, acc: 59.38%] [G loss: 1.834926]\n",
      "epoch:26 step:25228 [D loss: 0.655176, acc: 60.16%] [G loss: 1.751671]\n",
      "epoch:26 step:25229 [D loss: 0.667127, acc: 57.03%] [G loss: 1.719898]\n",
      "epoch:26 step:25230 [D loss: 0.630759, acc: 60.94%] [G loss: 1.862336]\n",
      "epoch:26 step:25231 [D loss: 0.661433, acc: 60.94%] [G loss: 1.735022]\n",
      "epoch:26 step:25232 [D loss: 0.682763, acc: 54.69%] [G loss: 1.788390]\n",
      "epoch:26 step:25233 [D loss: 0.644692, acc: 67.97%] [G loss: 1.836821]\n",
      "epoch:26 step:25234 [D loss: 0.630148, acc: 65.62%] [G loss: 1.847301]\n",
      "epoch:26 step:25235 [D loss: 0.653348, acc: 62.50%] [G loss: 1.796999]\n",
      "epoch:26 step:25236 [D loss: 0.652000, acc: 59.38%] [G loss: 1.725416]\n",
      "epoch:26 step:25237 [D loss: 0.625997, acc: 63.28%] [G loss: 1.949709]\n",
      "epoch:26 step:25238 [D loss: 0.655126, acc: 60.94%] [G loss: 1.743086]\n",
      "epoch:26 step:25239 [D loss: 0.624671, acc: 65.62%] [G loss: 1.937262]\n",
      "epoch:26 step:25240 [D loss: 0.665804, acc: 57.81%] [G loss: 1.705034]\n",
      "epoch:26 step:25241 [D loss: 0.644053, acc: 61.72%] [G loss: 1.835952]\n",
      "epoch:26 step:25242 [D loss: 0.646694, acc: 61.72%] [G loss: 1.931243]\n",
      "epoch:26 step:25243 [D loss: 0.628959, acc: 64.06%] [G loss: 1.845209]\n",
      "epoch:26 step:25244 [D loss: 0.640375, acc: 62.50%] [G loss: 1.859518]\n",
      "epoch:26 step:25245 [D loss: 0.658893, acc: 57.81%] [G loss: 1.832972]\n",
      "epoch:26 step:25246 [D loss: 0.628985, acc: 66.41%] [G loss: 1.861746]\n",
      "epoch:26 step:25247 [D loss: 0.637378, acc: 65.62%] [G loss: 1.789168]\n",
      "epoch:26 step:25248 [D loss: 0.619799, acc: 64.06%] [G loss: 1.984628]\n",
      "epoch:26 step:25249 [D loss: 0.639335, acc: 64.84%] [G loss: 1.849930]\n",
      "epoch:26 step:25250 [D loss: 0.596455, acc: 69.53%] [G loss: 1.911716]\n",
      "epoch:26 step:25251 [D loss: 0.629525, acc: 64.84%] [G loss: 1.849866]\n",
      "epoch:26 step:25252 [D loss: 0.585993, acc: 67.97%] [G loss: 1.946582]\n",
      "epoch:26 step:25253 [D loss: 0.707367, acc: 55.47%] [G loss: 1.857025]\n",
      "epoch:26 step:25254 [D loss: 0.673867, acc: 60.16%] [G loss: 1.775856]\n",
      "epoch:26 step:25255 [D loss: 0.625865, acc: 68.75%] [G loss: 1.968567]\n",
      "epoch:26 step:25256 [D loss: 0.667841, acc: 56.25%] [G loss: 1.976858]\n",
      "epoch:26 step:25257 [D loss: 0.661936, acc: 60.94%] [G loss: 1.850175]\n",
      "epoch:26 step:25258 [D loss: 0.675223, acc: 60.94%] [G loss: 1.850979]\n",
      "epoch:26 step:25259 [D loss: 0.669439, acc: 60.16%] [G loss: 1.804790]\n",
      "epoch:26 step:25260 [D loss: 0.665799, acc: 60.94%] [G loss: 1.736931]\n",
      "epoch:26 step:25261 [D loss: 0.669693, acc: 59.38%] [G loss: 1.815366]\n",
      "epoch:26 step:25262 [D loss: 0.661558, acc: 60.94%] [G loss: 1.873553]\n",
      "epoch:26 step:25263 [D loss: 0.656870, acc: 65.62%] [G loss: 1.909070]\n",
      "epoch:26 step:25264 [D loss: 0.662650, acc: 63.28%] [G loss: 1.759948]\n",
      "epoch:26 step:25265 [D loss: 0.616522, acc: 69.53%] [G loss: 1.774281]\n",
      "epoch:26 step:25266 [D loss: 0.638454, acc: 61.72%] [G loss: 1.924284]\n",
      "epoch:26 step:25267 [D loss: 0.684483, acc: 62.50%] [G loss: 2.026722]\n",
      "epoch:26 step:25268 [D loss: 0.662999, acc: 62.50%] [G loss: 1.858398]\n",
      "epoch:26 step:25269 [D loss: 0.659226, acc: 60.16%] [G loss: 1.817473]\n",
      "epoch:26 step:25270 [D loss: 0.685633, acc: 54.69%] [G loss: 1.846466]\n",
      "epoch:26 step:25271 [D loss: 0.621530, acc: 61.72%] [G loss: 1.969765]\n",
      "epoch:26 step:25272 [D loss: 0.606368, acc: 65.62%] [G loss: 1.879441]\n",
      "epoch:26 step:25273 [D loss: 0.618458, acc: 66.41%] [G loss: 1.923089]\n",
      "epoch:26 step:25274 [D loss: 0.624522, acc: 64.06%] [G loss: 1.993081]\n",
      "epoch:26 step:25275 [D loss: 0.661672, acc: 60.94%] [G loss: 1.721244]\n",
      "epoch:26 step:25276 [D loss: 0.688789, acc: 55.47%] [G loss: 1.868536]\n",
      "epoch:26 step:25277 [D loss: 0.622142, acc: 70.31%] [G loss: 1.904263]\n",
      "epoch:26 step:25278 [D loss: 0.653861, acc: 60.94%] [G loss: 1.912703]\n",
      "epoch:26 step:25279 [D loss: 0.667683, acc: 57.03%] [G loss: 1.848180]\n",
      "epoch:26 step:25280 [D loss: 0.602593, acc: 64.06%] [G loss: 1.987512]\n",
      "epoch:26 step:25281 [D loss: 0.595887, acc: 70.31%] [G loss: 2.013876]\n",
      "epoch:26 step:25282 [D loss: 0.723223, acc: 49.22%] [G loss: 1.887141]\n",
      "epoch:26 step:25283 [D loss: 0.669112, acc: 57.81%] [G loss: 1.853976]\n",
      "epoch:26 step:25284 [D loss: 0.677213, acc: 60.94%] [G loss: 1.795670]\n",
      "epoch:26 step:25285 [D loss: 0.588373, acc: 71.88%] [G loss: 1.955666]\n",
      "epoch:26 step:25286 [D loss: 0.596963, acc: 68.75%] [G loss: 2.033297]\n",
      "epoch:26 step:25287 [D loss: 0.568896, acc: 71.88%] [G loss: 2.037718]\n",
      "epoch:26 step:25288 [D loss: 0.662351, acc: 63.28%] [G loss: 2.007169]\n",
      "epoch:26 step:25289 [D loss: 0.637840, acc: 60.16%] [G loss: 2.062711]\n",
      "epoch:26 step:25290 [D loss: 0.769295, acc: 50.78%] [G loss: 1.771725]\n",
      "epoch:26 step:25291 [D loss: 0.733395, acc: 48.44%] [G loss: 1.929560]\n",
      "epoch:26 step:25292 [D loss: 0.602030, acc: 58.59%] [G loss: 2.024294]\n",
      "epoch:26 step:25293 [D loss: 0.617726, acc: 62.50%] [G loss: 2.103820]\n",
      "epoch:26 step:25294 [D loss: 0.637093, acc: 60.94%] [G loss: 1.819133]\n",
      "epoch:26 step:25295 [D loss: 0.648295, acc: 60.94%] [G loss: 1.815791]\n",
      "epoch:26 step:25296 [D loss: 0.621373, acc: 62.50%] [G loss: 1.904481]\n",
      "epoch:26 step:25297 [D loss: 0.627110, acc: 63.28%] [G loss: 1.923661]\n",
      "epoch:26 step:25298 [D loss: 0.624128, acc: 64.84%] [G loss: 1.996609]\n",
      "epoch:26 step:25299 [D loss: 0.587039, acc: 67.97%] [G loss: 2.438387]\n",
      "epoch:27 step:25300 [D loss: 0.663017, acc: 57.81%] [G loss: 1.899819]\n",
      "epoch:27 step:25301 [D loss: 0.690901, acc: 57.03%] [G loss: 1.738913]\n",
      "epoch:27 step:25302 [D loss: 0.638900, acc: 64.06%] [G loss: 1.877721]\n",
      "epoch:27 step:25303 [D loss: 0.719833, acc: 55.47%] [G loss: 1.826844]\n",
      "epoch:27 step:25304 [D loss: 0.674078, acc: 57.81%] [G loss: 1.775920]\n",
      "epoch:27 step:25305 [D loss: 0.628323, acc: 64.06%] [G loss: 1.949314]\n",
      "epoch:27 step:25306 [D loss: 0.648207, acc: 65.62%] [G loss: 1.966725]\n",
      "epoch:27 step:25307 [D loss: 0.673572, acc: 61.72%] [G loss: 1.931687]\n",
      "epoch:27 step:25308 [D loss: 0.633361, acc: 64.06%] [G loss: 1.965458]\n",
      "epoch:27 step:25309 [D loss: 0.640565, acc: 67.19%] [G loss: 1.992274]\n",
      "epoch:27 step:25310 [D loss: 0.614497, acc: 66.41%] [G loss: 1.851024]\n",
      "epoch:27 step:25311 [D loss: 0.656381, acc: 64.84%] [G loss: 1.857928]\n",
      "epoch:27 step:25312 [D loss: 0.633059, acc: 67.19%] [G loss: 1.702321]\n",
      "epoch:27 step:25313 [D loss: 0.621971, acc: 65.62%] [G loss: 1.938704]\n",
      "epoch:27 step:25314 [D loss: 0.586994, acc: 71.09%] [G loss: 2.078349]\n",
      "epoch:27 step:25315 [D loss: 0.669950, acc: 63.28%] [G loss: 2.138064]\n",
      "epoch:27 step:25316 [D loss: 0.605361, acc: 68.75%] [G loss: 1.861424]\n",
      "epoch:27 step:25317 [D loss: 0.653034, acc: 61.72%] [G loss: 1.883342]\n",
      "epoch:27 step:25318 [D loss: 0.652070, acc: 60.16%] [G loss: 2.001688]\n",
      "epoch:27 step:25319 [D loss: 0.748544, acc: 50.00%] [G loss: 1.690567]\n",
      "epoch:27 step:25320 [D loss: 0.672400, acc: 55.47%] [G loss: 1.823345]\n",
      "epoch:27 step:25321 [D loss: 0.727582, acc: 53.91%] [G loss: 1.795802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25322 [D loss: 0.621831, acc: 63.28%] [G loss: 1.922612]\n",
      "epoch:27 step:25323 [D loss: 0.587840, acc: 69.53%] [G loss: 2.004739]\n",
      "epoch:27 step:25324 [D loss: 0.635477, acc: 62.50%] [G loss: 2.078442]\n",
      "epoch:27 step:25325 [D loss: 0.601956, acc: 67.19%] [G loss: 1.995133]\n",
      "epoch:27 step:25326 [D loss: 0.628945, acc: 64.84%] [G loss: 1.878485]\n",
      "epoch:27 step:25327 [D loss: 0.677059, acc: 53.12%] [G loss: 1.771667]\n",
      "epoch:27 step:25328 [D loss: 0.614986, acc: 64.06%] [G loss: 1.954113]\n",
      "epoch:27 step:25329 [D loss: 0.707578, acc: 56.25%] [G loss: 1.925207]\n",
      "epoch:27 step:25330 [D loss: 0.733428, acc: 53.12%] [G loss: 1.801045]\n",
      "epoch:27 step:25331 [D loss: 0.675595, acc: 60.94%] [G loss: 1.909268]\n",
      "epoch:27 step:25332 [D loss: 0.623319, acc: 62.50%] [G loss: 1.891530]\n",
      "epoch:27 step:25333 [D loss: 0.646371, acc: 63.28%] [G loss: 1.761595]\n",
      "epoch:27 step:25334 [D loss: 0.611177, acc: 68.75%] [G loss: 1.882228]\n",
      "epoch:27 step:25335 [D loss: 0.637965, acc: 67.19%] [G loss: 1.881038]\n",
      "epoch:27 step:25336 [D loss: 0.634435, acc: 65.62%] [G loss: 2.027037]\n",
      "epoch:27 step:25337 [D loss: 0.666395, acc: 64.06%] [G loss: 2.007160]\n",
      "epoch:27 step:25338 [D loss: 0.613443, acc: 67.97%] [G loss: 1.888335]\n",
      "epoch:27 step:25339 [D loss: 0.602357, acc: 69.53%] [G loss: 2.005396]\n",
      "epoch:27 step:25340 [D loss: 0.639652, acc: 60.94%] [G loss: 1.904931]\n",
      "epoch:27 step:25341 [D loss: 0.606167, acc: 63.28%] [G loss: 1.971141]\n",
      "epoch:27 step:25342 [D loss: 0.628824, acc: 63.28%] [G loss: 1.866265]\n",
      "epoch:27 step:25343 [D loss: 0.646063, acc: 60.94%] [G loss: 1.943251]\n",
      "epoch:27 step:25344 [D loss: 0.662214, acc: 64.06%] [G loss: 1.892847]\n",
      "epoch:27 step:25345 [D loss: 0.643570, acc: 64.84%] [G loss: 1.881459]\n",
      "epoch:27 step:25346 [D loss: 0.622759, acc: 67.97%] [G loss: 2.067765]\n",
      "epoch:27 step:25347 [D loss: 0.607004, acc: 67.19%] [G loss: 1.957016]\n",
      "epoch:27 step:25348 [D loss: 0.617373, acc: 64.06%] [G loss: 1.902999]\n",
      "epoch:27 step:25349 [D loss: 0.638987, acc: 68.75%] [G loss: 1.919500]\n",
      "epoch:27 step:25350 [D loss: 0.657544, acc: 63.28%] [G loss: 1.930642]\n",
      "epoch:27 step:25351 [D loss: 0.651862, acc: 57.81%] [G loss: 1.890650]\n",
      "epoch:27 step:25352 [D loss: 0.683071, acc: 59.38%] [G loss: 2.051709]\n",
      "epoch:27 step:25353 [D loss: 0.640502, acc: 59.38%] [G loss: 1.882515]\n",
      "epoch:27 step:25354 [D loss: 0.620323, acc: 65.62%] [G loss: 1.957059]\n",
      "epoch:27 step:25355 [D loss: 0.590128, acc: 64.84%] [G loss: 1.907815]\n",
      "epoch:27 step:25356 [D loss: 0.671702, acc: 58.59%] [G loss: 1.977960]\n",
      "epoch:27 step:25357 [D loss: 0.635905, acc: 67.97%] [G loss: 1.865512]\n",
      "epoch:27 step:25358 [D loss: 0.624056, acc: 66.41%] [G loss: 1.881304]\n",
      "epoch:27 step:25359 [D loss: 0.668310, acc: 54.69%] [G loss: 1.784518]\n",
      "epoch:27 step:25360 [D loss: 0.600677, acc: 63.28%] [G loss: 1.899417]\n",
      "epoch:27 step:25361 [D loss: 0.661876, acc: 59.38%] [G loss: 1.844233]\n",
      "epoch:27 step:25362 [D loss: 0.655137, acc: 61.72%] [G loss: 1.865547]\n",
      "epoch:27 step:25363 [D loss: 0.648604, acc: 62.50%] [G loss: 1.810571]\n",
      "epoch:27 step:25364 [D loss: 0.650310, acc: 63.28%] [G loss: 1.876956]\n",
      "epoch:27 step:25365 [D loss: 0.668728, acc: 63.28%] [G loss: 1.874807]\n",
      "epoch:27 step:25366 [D loss: 0.658811, acc: 61.72%] [G loss: 1.909344]\n",
      "epoch:27 step:25367 [D loss: 0.632826, acc: 64.84%] [G loss: 1.948100]\n",
      "epoch:27 step:25368 [D loss: 0.635959, acc: 60.16%] [G loss: 1.871578]\n",
      "epoch:27 step:25369 [D loss: 0.640004, acc: 63.28%] [G loss: 1.981091]\n",
      "epoch:27 step:25370 [D loss: 0.692508, acc: 50.00%] [G loss: 1.706034]\n",
      "epoch:27 step:25371 [D loss: 0.685781, acc: 62.50%] [G loss: 1.958495]\n",
      "epoch:27 step:25372 [D loss: 0.594289, acc: 68.75%] [G loss: 1.867696]\n",
      "epoch:27 step:25373 [D loss: 0.621633, acc: 66.41%] [G loss: 1.948547]\n",
      "epoch:27 step:25374 [D loss: 0.703977, acc: 55.47%] [G loss: 1.966054]\n",
      "epoch:27 step:25375 [D loss: 0.686647, acc: 60.16%] [G loss: 1.969958]\n",
      "epoch:27 step:25376 [D loss: 0.607506, acc: 65.62%] [G loss: 2.025896]\n",
      "epoch:27 step:25377 [D loss: 0.698706, acc: 56.25%] [G loss: 1.843516]\n",
      "epoch:27 step:25378 [D loss: 0.682881, acc: 57.03%] [G loss: 1.863529]\n",
      "epoch:27 step:25379 [D loss: 0.638934, acc: 67.19%] [G loss: 1.783081]\n",
      "epoch:27 step:25380 [D loss: 0.649180, acc: 61.72%] [G loss: 1.736490]\n",
      "epoch:27 step:25381 [D loss: 0.720571, acc: 55.47%] [G loss: 1.591509]\n",
      "epoch:27 step:25382 [D loss: 0.674003, acc: 59.38%] [G loss: 1.856654]\n",
      "epoch:27 step:25383 [D loss: 0.671053, acc: 57.03%] [G loss: 1.809063]\n",
      "epoch:27 step:25384 [D loss: 0.703008, acc: 55.47%] [G loss: 1.672105]\n",
      "epoch:27 step:25385 [D loss: 0.689604, acc: 60.94%] [G loss: 1.799826]\n",
      "epoch:27 step:25386 [D loss: 0.633693, acc: 66.41%] [G loss: 1.871254]\n",
      "epoch:27 step:25387 [D loss: 0.722827, acc: 56.25%] [G loss: 1.891600]\n",
      "epoch:27 step:25388 [D loss: 0.650364, acc: 62.50%] [G loss: 1.851452]\n",
      "epoch:27 step:25389 [D loss: 0.638395, acc: 64.06%] [G loss: 1.766948]\n",
      "epoch:27 step:25390 [D loss: 0.639981, acc: 67.97%] [G loss: 1.762154]\n",
      "epoch:27 step:25391 [D loss: 0.638452, acc: 62.50%] [G loss: 2.064401]\n",
      "epoch:27 step:25392 [D loss: 0.628294, acc: 63.28%] [G loss: 1.966268]\n",
      "epoch:27 step:25393 [D loss: 0.632147, acc: 59.38%] [G loss: 1.827538]\n",
      "epoch:27 step:25394 [D loss: 0.645922, acc: 64.06%] [G loss: 1.763875]\n",
      "epoch:27 step:25395 [D loss: 0.666556, acc: 62.50%] [G loss: 1.863264]\n",
      "epoch:27 step:25396 [D loss: 0.641922, acc: 62.50%] [G loss: 1.870834]\n",
      "epoch:27 step:25397 [D loss: 0.681314, acc: 56.25%] [G loss: 1.741275]\n",
      "epoch:27 step:25398 [D loss: 0.652818, acc: 60.16%] [G loss: 1.711389]\n",
      "epoch:27 step:25399 [D loss: 0.650639, acc: 57.81%] [G loss: 1.837470]\n",
      "epoch:27 step:25400 [D loss: 0.669491, acc: 64.06%] [G loss: 1.848871]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.598469\n",
      "FID: 10.102388\n",
      "0 = 12.818634851837148\n",
      "1 = 0.10009765625\n",
      "2 = 0.8730000257492065\n",
      "3 = 0.8889999985694885\n",
      "4 = 0.8569999933242798\n",
      "5 = 0.8614341020584106\n",
      "6 = 0.8889999985694885\n",
      "7 = 6.1350347889065695\n",
      "8 = 0.06095013609723112\n",
      "9 = 0.7069000005722046\n",
      "10 = 0.7175999879837036\n",
      "11 = 0.6962000131607056\n",
      "12 = 0.7025651335716248\n",
      "13 = 0.7175999879837036\n",
      "14 = 7.598499298095703\n",
      "15 = 9.404309272766113\n",
      "16 = 0.12302622944116592\n",
      "17 = 7.598468780517578\n",
      "18 = 10.102388381958008\n",
      "epoch:27 step:25401 [D loss: 0.658262, acc: 58.59%] [G loss: 1.802759]\n",
      "epoch:27 step:25402 [D loss: 0.704510, acc: 53.91%] [G loss: 1.821372]\n",
      "epoch:27 step:25403 [D loss: 0.639428, acc: 64.84%] [G loss: 1.966681]\n",
      "epoch:27 step:25404 [D loss: 0.629091, acc: 62.50%] [G loss: 1.845416]\n",
      "epoch:27 step:25405 [D loss: 0.626152, acc: 65.62%] [G loss: 1.883926]\n",
      "epoch:27 step:25406 [D loss: 0.641675, acc: 67.97%] [G loss: 2.126080]\n",
      "epoch:27 step:25407 [D loss: 0.664193, acc: 63.28%] [G loss: 1.674344]\n",
      "epoch:27 step:25408 [D loss: 0.739735, acc: 50.00%] [G loss: 1.756448]\n",
      "epoch:27 step:25409 [D loss: 0.718859, acc: 57.81%] [G loss: 1.830390]\n",
      "epoch:27 step:25410 [D loss: 0.600789, acc: 69.53%] [G loss: 2.026427]\n",
      "epoch:27 step:25411 [D loss: 0.655142, acc: 61.72%] [G loss: 1.887429]\n",
      "epoch:27 step:25412 [D loss: 0.666497, acc: 59.38%] [G loss: 2.007348]\n",
      "epoch:27 step:25413 [D loss: 0.592292, acc: 67.97%] [G loss: 2.065136]\n",
      "epoch:27 step:25414 [D loss: 0.597866, acc: 71.09%] [G loss: 2.316675]\n",
      "epoch:27 step:25415 [D loss: 0.634166, acc: 69.53%] [G loss: 2.136468]\n",
      "epoch:27 step:25416 [D loss: 0.656714, acc: 57.81%] [G loss: 1.985602]\n",
      "epoch:27 step:25417 [D loss: 0.653769, acc: 63.28%] [G loss: 1.992877]\n",
      "epoch:27 step:25418 [D loss: 0.589634, acc: 67.97%] [G loss: 2.142374]\n",
      "epoch:27 step:25419 [D loss: 0.652338, acc: 57.81%] [G loss: 1.952165]\n",
      "epoch:27 step:25420 [D loss: 0.687009, acc: 62.50%] [G loss: 1.867254]\n",
      "epoch:27 step:25421 [D loss: 0.688048, acc: 56.25%] [G loss: 2.115015]\n",
      "epoch:27 step:25422 [D loss: 0.709960, acc: 53.91%] [G loss: 1.908598]\n",
      "epoch:27 step:25423 [D loss: 0.712725, acc: 53.91%] [G loss: 1.889687]\n",
      "epoch:27 step:25424 [D loss: 0.698991, acc: 57.81%] [G loss: 1.706188]\n",
      "epoch:27 step:25425 [D loss: 0.650621, acc: 60.16%] [G loss: 1.846947]\n",
      "epoch:27 step:25426 [D loss: 0.678917, acc: 58.59%] [G loss: 1.879986]\n",
      "epoch:27 step:25427 [D loss: 0.651380, acc: 58.59%] [G loss: 1.750519]\n",
      "epoch:27 step:25428 [D loss: 0.661004, acc: 58.59%] [G loss: 1.708898]\n",
      "epoch:27 step:25429 [D loss: 0.622983, acc: 65.62%] [G loss: 1.832043]\n",
      "epoch:27 step:25430 [D loss: 0.645432, acc: 65.62%] [G loss: 1.855276]\n",
      "epoch:27 step:25431 [D loss: 0.705742, acc: 51.56%] [G loss: 1.804741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25432 [D loss: 0.715398, acc: 47.66%] [G loss: 1.711887]\n",
      "epoch:27 step:25433 [D loss: 0.711423, acc: 54.69%] [G loss: 1.838428]\n",
      "epoch:27 step:25434 [D loss: 0.649675, acc: 58.59%] [G loss: 1.674908]\n",
      "epoch:27 step:25435 [D loss: 0.692790, acc: 53.91%] [G loss: 1.661868]\n",
      "epoch:27 step:25436 [D loss: 0.694242, acc: 60.94%] [G loss: 1.725296]\n",
      "epoch:27 step:25437 [D loss: 0.642892, acc: 62.50%] [G loss: 1.772894]\n",
      "epoch:27 step:25438 [D loss: 0.639823, acc: 58.59%] [G loss: 1.840858]\n",
      "epoch:27 step:25439 [D loss: 0.678190, acc: 61.72%] [G loss: 1.927036]\n",
      "epoch:27 step:25440 [D loss: 0.670856, acc: 56.25%] [G loss: 1.765003]\n",
      "epoch:27 step:25441 [D loss: 0.671825, acc: 56.25%] [G loss: 1.818711]\n",
      "epoch:27 step:25442 [D loss: 0.694891, acc: 58.59%] [G loss: 1.767335]\n",
      "epoch:27 step:25443 [D loss: 0.626821, acc: 62.50%] [G loss: 1.807630]\n",
      "epoch:27 step:25444 [D loss: 0.655325, acc: 63.28%] [G loss: 1.780228]\n",
      "epoch:27 step:25445 [D loss: 0.652068, acc: 62.50%] [G loss: 1.756129]\n",
      "epoch:27 step:25446 [D loss: 0.632550, acc: 67.19%] [G loss: 1.761609]\n",
      "epoch:27 step:25447 [D loss: 0.605073, acc: 67.19%] [G loss: 1.866607]\n",
      "epoch:27 step:25448 [D loss: 0.607348, acc: 64.84%] [G loss: 1.889158]\n",
      "epoch:27 step:25449 [D loss: 0.619773, acc: 66.41%] [G loss: 1.947956]\n",
      "epoch:27 step:25450 [D loss: 0.663317, acc: 59.38%] [G loss: 1.940253]\n",
      "epoch:27 step:25451 [D loss: 0.663073, acc: 58.59%] [G loss: 1.912700]\n",
      "epoch:27 step:25452 [D loss: 0.658710, acc: 61.72%] [G loss: 1.770982]\n",
      "epoch:27 step:25453 [D loss: 0.667249, acc: 61.72%] [G loss: 1.878565]\n",
      "epoch:27 step:25454 [D loss: 0.648035, acc: 65.62%] [G loss: 1.869549]\n",
      "epoch:27 step:25455 [D loss: 0.624275, acc: 63.28%] [G loss: 1.836365]\n",
      "epoch:27 step:25456 [D loss: 0.701902, acc: 57.03%] [G loss: 1.857980]\n",
      "epoch:27 step:25457 [D loss: 0.635580, acc: 67.19%] [G loss: 1.777860]\n",
      "epoch:27 step:25458 [D loss: 0.624156, acc: 66.41%] [G loss: 1.893893]\n",
      "epoch:27 step:25459 [D loss: 0.675973, acc: 57.81%] [G loss: 1.743154]\n",
      "epoch:27 step:25460 [D loss: 0.655297, acc: 61.72%] [G loss: 1.876350]\n",
      "epoch:27 step:25461 [D loss: 0.659397, acc: 64.84%] [G loss: 1.764353]\n",
      "epoch:27 step:25462 [D loss: 0.680695, acc: 56.25%] [G loss: 1.765572]\n",
      "epoch:27 step:25463 [D loss: 0.659340, acc: 64.06%] [G loss: 1.795052]\n",
      "epoch:27 step:25464 [D loss: 0.608742, acc: 68.75%] [G loss: 1.896526]\n",
      "epoch:27 step:25465 [D loss: 0.636162, acc: 68.75%] [G loss: 1.869303]\n",
      "epoch:27 step:25466 [D loss: 0.672407, acc: 57.81%] [G loss: 1.877768]\n",
      "epoch:27 step:25467 [D loss: 0.608574, acc: 63.28%] [G loss: 1.851120]\n",
      "epoch:27 step:25468 [D loss: 0.681482, acc: 60.94%] [G loss: 1.828956]\n",
      "epoch:27 step:25469 [D loss: 0.679561, acc: 50.78%] [G loss: 1.630217]\n",
      "epoch:27 step:25470 [D loss: 0.623975, acc: 67.97%] [G loss: 1.821348]\n",
      "epoch:27 step:25471 [D loss: 0.616464, acc: 67.97%] [G loss: 1.847819]\n",
      "epoch:27 step:25472 [D loss: 0.670389, acc: 60.16%] [G loss: 1.640951]\n",
      "epoch:27 step:25473 [D loss: 0.663048, acc: 53.91%] [G loss: 1.774573]\n",
      "epoch:27 step:25474 [D loss: 0.630968, acc: 64.06%] [G loss: 1.790934]\n",
      "epoch:27 step:25475 [D loss: 0.688781, acc: 57.81%] [G loss: 1.765145]\n",
      "epoch:27 step:25476 [D loss: 0.691946, acc: 57.81%] [G loss: 1.789747]\n",
      "epoch:27 step:25477 [D loss: 0.648146, acc: 61.72%] [G loss: 1.711662]\n",
      "epoch:27 step:25478 [D loss: 0.699154, acc: 59.38%] [G loss: 1.798646]\n",
      "epoch:27 step:25479 [D loss: 0.718109, acc: 54.69%] [G loss: 1.718177]\n",
      "epoch:27 step:25480 [D loss: 0.648445, acc: 62.50%] [G loss: 1.753731]\n",
      "epoch:27 step:25481 [D loss: 0.675708, acc: 57.03%] [G loss: 1.802073]\n",
      "epoch:27 step:25482 [D loss: 0.652607, acc: 59.38%] [G loss: 1.759725]\n",
      "epoch:27 step:25483 [D loss: 0.644957, acc: 64.84%] [G loss: 1.782439]\n",
      "epoch:27 step:25484 [D loss: 0.670213, acc: 60.94%] [G loss: 1.829594]\n",
      "epoch:27 step:25485 [D loss: 0.633036, acc: 67.19%] [G loss: 1.899839]\n",
      "epoch:27 step:25486 [D loss: 0.698151, acc: 54.69%] [G loss: 1.841413]\n",
      "epoch:27 step:25487 [D loss: 0.695253, acc: 56.25%] [G loss: 1.841755]\n",
      "epoch:27 step:25488 [D loss: 0.628432, acc: 65.62%] [G loss: 1.811036]\n",
      "epoch:27 step:25489 [D loss: 0.631914, acc: 61.72%] [G loss: 1.910846]\n",
      "epoch:27 step:25490 [D loss: 0.619190, acc: 64.06%] [G loss: 1.846356]\n",
      "epoch:27 step:25491 [D loss: 0.654771, acc: 56.25%] [G loss: 1.770721]\n",
      "epoch:27 step:25492 [D loss: 0.635965, acc: 59.38%] [G loss: 1.900559]\n",
      "epoch:27 step:25493 [D loss: 0.591263, acc: 70.31%] [G loss: 1.952995]\n",
      "epoch:27 step:25494 [D loss: 0.670121, acc: 62.50%] [G loss: 1.785602]\n",
      "epoch:27 step:25495 [D loss: 0.717210, acc: 57.81%] [G loss: 1.731530]\n",
      "epoch:27 step:25496 [D loss: 0.620577, acc: 65.62%] [G loss: 1.857834]\n",
      "epoch:27 step:25497 [D loss: 0.652420, acc: 66.41%] [G loss: 1.860224]\n",
      "epoch:27 step:25498 [D loss: 0.619643, acc: 66.41%] [G loss: 1.875843]\n",
      "epoch:27 step:25499 [D loss: 0.691799, acc: 55.47%] [G loss: 1.752888]\n",
      "epoch:27 step:25500 [D loss: 0.656154, acc: 57.03%] [G loss: 1.898830]\n",
      "epoch:27 step:25501 [D loss: 0.611432, acc: 70.31%] [G loss: 1.851332]\n",
      "epoch:27 step:25502 [D loss: 0.690208, acc: 57.03%] [G loss: 1.819717]\n",
      "epoch:27 step:25503 [D loss: 0.613569, acc: 67.19%] [G loss: 1.826027]\n",
      "epoch:27 step:25504 [D loss: 0.679533, acc: 58.59%] [G loss: 1.679550]\n",
      "epoch:27 step:25505 [D loss: 0.643654, acc: 64.06%] [G loss: 2.027871]\n",
      "epoch:27 step:25506 [D loss: 0.603375, acc: 64.84%] [G loss: 1.975605]\n",
      "epoch:27 step:25507 [D loss: 0.573999, acc: 66.41%] [G loss: 1.960853]\n",
      "epoch:27 step:25508 [D loss: 0.584260, acc: 67.19%] [G loss: 2.117955]\n",
      "epoch:27 step:25509 [D loss: 0.667277, acc: 60.94%] [G loss: 1.874436]\n",
      "epoch:27 step:25510 [D loss: 0.667093, acc: 59.38%] [G loss: 1.718107]\n",
      "epoch:27 step:25511 [D loss: 0.677660, acc: 58.59%] [G loss: 1.832085]\n",
      "epoch:27 step:25512 [D loss: 0.664919, acc: 59.38%] [G loss: 1.803092]\n",
      "epoch:27 step:25513 [D loss: 0.617435, acc: 68.75%] [G loss: 1.732088]\n",
      "epoch:27 step:25514 [D loss: 0.657900, acc: 58.59%] [G loss: 1.703756]\n",
      "epoch:27 step:25515 [D loss: 0.661125, acc: 62.50%] [G loss: 2.117055]\n",
      "epoch:27 step:25516 [D loss: 0.607980, acc: 67.97%] [G loss: 2.011811]\n",
      "epoch:27 step:25517 [D loss: 0.592786, acc: 64.84%] [G loss: 1.954928]\n",
      "epoch:27 step:25518 [D loss: 0.612147, acc: 64.06%] [G loss: 2.032399]\n",
      "epoch:27 step:25519 [D loss: 0.677934, acc: 55.47%] [G loss: 1.760250]\n",
      "epoch:27 step:25520 [D loss: 0.681242, acc: 61.72%] [G loss: 1.926461]\n",
      "epoch:27 step:25521 [D loss: 0.637672, acc: 67.19%] [G loss: 1.963184]\n",
      "epoch:27 step:25522 [D loss: 0.657735, acc: 55.47%] [G loss: 1.854426]\n",
      "epoch:27 step:25523 [D loss: 0.643487, acc: 64.06%] [G loss: 1.880817]\n",
      "epoch:27 step:25524 [D loss: 0.658395, acc: 58.59%] [G loss: 1.751428]\n",
      "epoch:27 step:25525 [D loss: 0.618812, acc: 65.62%] [G loss: 1.751639]\n",
      "epoch:27 step:25526 [D loss: 0.654835, acc: 59.38%] [G loss: 1.813966]\n",
      "epoch:27 step:25527 [D loss: 0.654378, acc: 61.72%] [G loss: 1.809050]\n",
      "epoch:27 step:25528 [D loss: 0.621885, acc: 69.53%] [G loss: 2.208719]\n",
      "epoch:27 step:25529 [D loss: 0.608358, acc: 69.53%] [G loss: 2.301377]\n",
      "epoch:27 step:25530 [D loss: 0.617951, acc: 69.53%] [G loss: 2.287716]\n",
      "epoch:27 step:25531 [D loss: 0.582296, acc: 68.75%] [G loss: 2.290315]\n",
      "epoch:27 step:25532 [D loss: 0.689357, acc: 53.12%] [G loss: 1.853266]\n",
      "epoch:27 step:25533 [D loss: 0.652104, acc: 60.94%] [G loss: 1.776424]\n",
      "epoch:27 step:25534 [D loss: 0.692199, acc: 58.59%] [G loss: 1.727550]\n",
      "epoch:27 step:25535 [D loss: 0.654217, acc: 57.03%] [G loss: 1.891570]\n",
      "epoch:27 step:25536 [D loss: 0.622110, acc: 63.28%] [G loss: 1.738991]\n",
      "epoch:27 step:25537 [D loss: 0.608728, acc: 64.84%] [G loss: 1.904974]\n",
      "epoch:27 step:25538 [D loss: 0.608698, acc: 67.97%] [G loss: 1.725735]\n",
      "epoch:27 step:25539 [D loss: 0.633071, acc: 59.38%] [G loss: 1.798873]\n",
      "epoch:27 step:25540 [D loss: 0.606486, acc: 75.00%] [G loss: 2.022417]\n",
      "epoch:27 step:25541 [D loss: 0.701801, acc: 54.69%] [G loss: 1.831910]\n",
      "epoch:27 step:25542 [D loss: 0.592734, acc: 68.75%] [G loss: 1.941609]\n",
      "epoch:27 step:25543 [D loss: 0.663322, acc: 59.38%] [G loss: 1.849472]\n",
      "epoch:27 step:25544 [D loss: 0.608946, acc: 61.72%] [G loss: 1.869208]\n",
      "epoch:27 step:25545 [D loss: 0.630560, acc: 65.62%] [G loss: 2.037375]\n",
      "epoch:27 step:25546 [D loss: 0.643684, acc: 62.50%] [G loss: 2.010623]\n",
      "epoch:27 step:25547 [D loss: 0.646518, acc: 60.16%] [G loss: 2.048596]\n",
      "epoch:27 step:25548 [D loss: 0.650229, acc: 60.16%] [G loss: 1.827686]\n",
      "epoch:27 step:25549 [D loss: 0.657610, acc: 63.28%] [G loss: 1.859507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25550 [D loss: 0.665595, acc: 60.16%] [G loss: 1.808953]\n",
      "epoch:27 step:25551 [D loss: 0.662426, acc: 58.59%] [G loss: 1.882665]\n",
      "epoch:27 step:25552 [D loss: 0.675049, acc: 59.38%] [G loss: 1.797066]\n",
      "epoch:27 step:25553 [D loss: 0.681814, acc: 57.03%] [G loss: 1.804363]\n",
      "epoch:27 step:25554 [D loss: 0.636955, acc: 64.06%] [G loss: 1.923717]\n",
      "epoch:27 step:25555 [D loss: 0.636910, acc: 60.16%] [G loss: 1.820954]\n",
      "epoch:27 step:25556 [D loss: 0.655581, acc: 62.50%] [G loss: 1.754502]\n",
      "epoch:27 step:25557 [D loss: 0.693830, acc: 54.69%] [G loss: 1.914784]\n",
      "epoch:27 step:25558 [D loss: 0.633575, acc: 64.06%] [G loss: 1.810522]\n",
      "epoch:27 step:25559 [D loss: 0.626118, acc: 65.62%] [G loss: 1.748479]\n",
      "epoch:27 step:25560 [D loss: 0.638229, acc: 66.41%] [G loss: 2.016086]\n",
      "epoch:27 step:25561 [D loss: 0.658058, acc: 59.38%] [G loss: 2.000559]\n",
      "epoch:27 step:25562 [D loss: 0.613829, acc: 67.97%] [G loss: 2.011008]\n",
      "epoch:27 step:25563 [D loss: 0.609963, acc: 68.75%] [G loss: 2.068161]\n",
      "epoch:27 step:25564 [D loss: 0.654005, acc: 66.41%] [G loss: 1.886766]\n",
      "epoch:27 step:25565 [D loss: 0.674314, acc: 60.16%] [G loss: 1.796038]\n",
      "epoch:27 step:25566 [D loss: 0.653791, acc: 57.81%] [G loss: 1.865528]\n",
      "epoch:27 step:25567 [D loss: 0.696527, acc: 51.56%] [G loss: 1.878482]\n",
      "epoch:27 step:25568 [D loss: 0.616866, acc: 65.62%] [G loss: 1.950311]\n",
      "epoch:27 step:25569 [D loss: 0.642087, acc: 63.28%] [G loss: 1.927256]\n",
      "epoch:27 step:25570 [D loss: 0.666351, acc: 57.03%] [G loss: 1.710341]\n",
      "epoch:27 step:25571 [D loss: 0.603446, acc: 67.19%] [G loss: 1.915471]\n",
      "epoch:27 step:25572 [D loss: 0.671824, acc: 57.81%] [G loss: 1.926260]\n",
      "epoch:27 step:25573 [D loss: 0.651095, acc: 61.72%] [G loss: 1.951627]\n",
      "epoch:27 step:25574 [D loss: 0.637676, acc: 65.62%] [G loss: 2.035721]\n",
      "epoch:27 step:25575 [D loss: 0.611180, acc: 64.84%] [G loss: 2.170596]\n",
      "epoch:27 step:25576 [D loss: 0.666663, acc: 58.59%] [G loss: 1.795041]\n",
      "epoch:27 step:25577 [D loss: 0.707885, acc: 49.22%] [G loss: 1.799126]\n",
      "epoch:27 step:25578 [D loss: 0.663826, acc: 58.59%] [G loss: 1.870308]\n",
      "epoch:27 step:25579 [D loss: 0.651934, acc: 64.84%] [G loss: 1.970586]\n",
      "epoch:27 step:25580 [D loss: 0.665975, acc: 60.16%] [G loss: 1.807382]\n",
      "epoch:27 step:25581 [D loss: 0.665713, acc: 62.50%] [G loss: 1.927308]\n",
      "epoch:27 step:25582 [D loss: 0.587456, acc: 72.66%] [G loss: 1.906629]\n",
      "epoch:27 step:25583 [D loss: 0.666278, acc: 63.28%] [G loss: 1.745737]\n",
      "epoch:27 step:25584 [D loss: 0.656421, acc: 60.94%] [G loss: 1.940610]\n",
      "epoch:27 step:25585 [D loss: 0.668627, acc: 57.03%] [G loss: 1.949979]\n",
      "epoch:27 step:25586 [D loss: 0.618809, acc: 63.28%] [G loss: 1.763894]\n",
      "epoch:27 step:25587 [D loss: 0.620543, acc: 65.62%] [G loss: 1.817322]\n",
      "epoch:27 step:25588 [D loss: 0.596767, acc: 69.53%] [G loss: 1.842280]\n",
      "epoch:27 step:25589 [D loss: 0.657113, acc: 56.25%] [G loss: 1.848975]\n",
      "epoch:27 step:25590 [D loss: 0.649641, acc: 66.41%] [G loss: 1.904605]\n",
      "epoch:27 step:25591 [D loss: 0.615977, acc: 67.19%] [G loss: 1.984834]\n",
      "epoch:27 step:25592 [D loss: 0.637638, acc: 64.84%] [G loss: 1.818565]\n",
      "epoch:27 step:25593 [D loss: 0.689535, acc: 50.78%] [G loss: 1.803053]\n",
      "epoch:27 step:25594 [D loss: 0.650230, acc: 57.03%] [G loss: 1.854529]\n",
      "epoch:27 step:25595 [D loss: 0.621533, acc: 67.19%] [G loss: 1.817085]\n",
      "epoch:27 step:25596 [D loss: 0.651400, acc: 59.38%] [G loss: 1.762477]\n",
      "epoch:27 step:25597 [D loss: 0.622408, acc: 71.09%] [G loss: 1.852315]\n",
      "epoch:27 step:25598 [D loss: 0.650887, acc: 61.72%] [G loss: 1.966172]\n",
      "epoch:27 step:25599 [D loss: 0.621889, acc: 63.28%] [G loss: 1.878304]\n",
      "epoch:27 step:25600 [D loss: 0.674465, acc: 58.59%] [G loss: 1.700264]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.494204\n",
      "FID: 12.436612\n",
      "0 = 12.720114389657988\n",
      "1 = 0.08426565926931495\n",
      "2 = 0.8719000220298767\n",
      "3 = 0.8912000060081482\n",
      "4 = 0.8525999784469604\n",
      "5 = 0.8580781817436218\n",
      "6 = 0.8912000060081482\n",
      "7 = 6.216890317511574\n",
      "8 = 0.06831052942391665\n",
      "9 = 0.7073000073432922\n",
      "10 = 0.7175999879837036\n",
      "11 = 0.6970000267028809\n",
      "12 = 0.7031158208847046\n",
      "13 = 0.7175999879837036\n",
      "14 = 7.494226932525635\n",
      "15 = 9.453385353088379\n",
      "16 = 0.10794685781002045\n",
      "17 = 7.494203567504883\n",
      "18 = 12.436612129211426\n",
      "epoch:27 step:25601 [D loss: 0.642078, acc: 59.38%] [G loss: 2.045532]\n",
      "epoch:27 step:25602 [D loss: 0.676097, acc: 57.81%] [G loss: 1.748191]\n",
      "epoch:27 step:25603 [D loss: 0.632731, acc: 67.97%] [G loss: 1.837866]\n",
      "epoch:27 step:25604 [D loss: 0.631883, acc: 63.28%] [G loss: 1.934708]\n",
      "epoch:27 step:25605 [D loss: 0.630921, acc: 68.75%] [G loss: 1.710324]\n",
      "epoch:27 step:25606 [D loss: 0.681874, acc: 53.91%] [G loss: 1.859959]\n",
      "epoch:27 step:25607 [D loss: 0.689242, acc: 53.91%] [G loss: 1.705435]\n",
      "epoch:27 step:25608 [D loss: 0.650396, acc: 64.84%] [G loss: 1.727124]\n",
      "epoch:27 step:25609 [D loss: 0.619544, acc: 66.41%] [G loss: 1.792717]\n",
      "epoch:27 step:25610 [D loss: 0.633864, acc: 66.41%] [G loss: 1.729851]\n",
      "epoch:27 step:25611 [D loss: 0.603559, acc: 69.53%] [G loss: 2.102676]\n",
      "epoch:27 step:25612 [D loss: 0.587665, acc: 70.31%] [G loss: 1.989101]\n",
      "epoch:27 step:25613 [D loss: 0.656274, acc: 58.59%] [G loss: 2.055160]\n",
      "epoch:27 step:25614 [D loss: 0.610313, acc: 62.50%] [G loss: 1.963962]\n",
      "epoch:27 step:25615 [D loss: 0.788158, acc: 52.34%] [G loss: 1.742122]\n",
      "epoch:27 step:25616 [D loss: 0.703352, acc: 53.12%] [G loss: 1.819375]\n",
      "epoch:27 step:25617 [D loss: 0.610915, acc: 70.31%] [G loss: 1.937080]\n",
      "epoch:27 step:25618 [D loss: 0.665821, acc: 64.06%] [G loss: 1.700366]\n",
      "epoch:27 step:25619 [D loss: 0.701605, acc: 56.25%] [G loss: 1.730653]\n",
      "epoch:27 step:25620 [D loss: 0.658315, acc: 58.59%] [G loss: 1.839432]\n",
      "epoch:27 step:25621 [D loss: 0.679013, acc: 57.81%] [G loss: 1.716872]\n",
      "epoch:27 step:25622 [D loss: 0.727250, acc: 50.78%] [G loss: 1.718657]\n",
      "epoch:27 step:25623 [D loss: 0.627621, acc: 66.41%] [G loss: 1.775470]\n",
      "epoch:27 step:25624 [D loss: 0.609609, acc: 68.75%] [G loss: 1.916745]\n",
      "epoch:27 step:25625 [D loss: 0.645854, acc: 61.72%] [G loss: 1.864006]\n",
      "epoch:27 step:25626 [D loss: 0.621212, acc: 68.75%] [G loss: 1.839948]\n",
      "epoch:27 step:25627 [D loss: 0.662546, acc: 63.28%] [G loss: 1.770444]\n",
      "epoch:27 step:25628 [D loss: 0.662604, acc: 59.38%] [G loss: 1.841609]\n",
      "epoch:27 step:25629 [D loss: 0.612651, acc: 65.62%] [G loss: 1.971589]\n",
      "epoch:27 step:25630 [D loss: 0.627724, acc: 66.41%] [G loss: 1.933581]\n",
      "epoch:27 step:25631 [D loss: 0.574829, acc: 70.31%] [G loss: 1.942234]\n",
      "epoch:27 step:25632 [D loss: 0.678278, acc: 59.38%] [G loss: 1.895651]\n",
      "epoch:27 step:25633 [D loss: 0.641377, acc: 60.94%] [G loss: 1.930255]\n",
      "epoch:27 step:25634 [D loss: 0.633938, acc: 66.41%] [G loss: 1.998822]\n",
      "epoch:27 step:25635 [D loss: 0.626990, acc: 67.97%] [G loss: 1.904384]\n",
      "epoch:27 step:25636 [D loss: 0.598536, acc: 73.44%] [G loss: 2.069832]\n",
      "epoch:27 step:25637 [D loss: 0.638328, acc: 60.94%] [G loss: 1.983592]\n",
      "epoch:27 step:25638 [D loss: 0.656714, acc: 64.06%] [G loss: 1.989837]\n",
      "epoch:27 step:25639 [D loss: 0.611173, acc: 67.97%] [G loss: 2.019098]\n",
      "epoch:27 step:25640 [D loss: 0.746179, acc: 52.34%] [G loss: 1.785175]\n",
      "epoch:27 step:25641 [D loss: 0.663399, acc: 64.06%] [G loss: 1.796611]\n",
      "epoch:27 step:25642 [D loss: 0.669471, acc: 54.69%] [G loss: 1.905149]\n",
      "epoch:27 step:25643 [D loss: 0.603568, acc: 71.09%] [G loss: 1.811168]\n",
      "epoch:27 step:25644 [D loss: 0.666457, acc: 62.50%] [G loss: 2.071299]\n",
      "epoch:27 step:25645 [D loss: 0.637204, acc: 60.16%] [G loss: 1.940761]\n",
      "epoch:27 step:25646 [D loss: 0.578128, acc: 71.09%] [G loss: 2.172992]\n",
      "epoch:27 step:25647 [D loss: 0.679617, acc: 57.03%] [G loss: 1.780168]\n",
      "epoch:27 step:25648 [D loss: 0.751655, acc: 50.78%] [G loss: 1.736271]\n",
      "epoch:27 step:25649 [D loss: 0.626997, acc: 65.62%] [G loss: 1.933583]\n",
      "epoch:27 step:25650 [D loss: 0.693266, acc: 59.38%] [G loss: 1.825283]\n",
      "epoch:27 step:25651 [D loss: 0.656216, acc: 60.16%] [G loss: 1.864159]\n",
      "epoch:27 step:25652 [D loss: 0.648798, acc: 60.94%] [G loss: 1.930081]\n",
      "epoch:27 step:25653 [D loss: 0.647145, acc: 60.16%] [G loss: 1.966506]\n",
      "epoch:27 step:25654 [D loss: 0.664001, acc: 56.25%] [G loss: 1.802173]\n",
      "epoch:27 step:25655 [D loss: 0.649103, acc: 57.03%] [G loss: 1.814030]\n",
      "epoch:27 step:25656 [D loss: 0.631063, acc: 64.84%] [G loss: 1.746730]\n",
      "epoch:27 step:25657 [D loss: 0.637613, acc: 63.28%] [G loss: 2.015903]\n",
      "epoch:27 step:25658 [D loss: 0.614302, acc: 64.84%] [G loss: 1.868425]\n",
      "epoch:27 step:25659 [D loss: 0.655093, acc: 61.72%] [G loss: 1.864457]\n",
      "epoch:27 step:25660 [D loss: 0.649938, acc: 61.72%] [G loss: 1.798971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25661 [D loss: 0.616868, acc: 63.28%] [G loss: 1.748944]\n",
      "epoch:27 step:25662 [D loss: 0.661687, acc: 63.28%] [G loss: 1.858728]\n",
      "epoch:27 step:25663 [D loss: 0.640741, acc: 61.72%] [G loss: 1.862995]\n",
      "epoch:27 step:25664 [D loss: 0.662146, acc: 60.16%] [G loss: 1.896469]\n",
      "epoch:27 step:25665 [D loss: 0.693196, acc: 56.25%] [G loss: 1.852072]\n",
      "epoch:27 step:25666 [D loss: 0.619491, acc: 65.62%] [G loss: 2.046920]\n",
      "epoch:27 step:25667 [D loss: 0.612847, acc: 71.88%] [G loss: 1.845529]\n",
      "epoch:27 step:25668 [D loss: 0.729141, acc: 62.50%] [G loss: 1.901316]\n",
      "epoch:27 step:25669 [D loss: 0.647325, acc: 64.84%] [G loss: 1.869345]\n",
      "epoch:27 step:25670 [D loss: 0.637900, acc: 68.75%] [G loss: 1.923930]\n",
      "epoch:27 step:25671 [D loss: 0.660911, acc: 61.72%] [G loss: 1.920111]\n",
      "epoch:27 step:25672 [D loss: 0.681282, acc: 50.78%] [G loss: 1.840842]\n",
      "epoch:27 step:25673 [D loss: 0.585270, acc: 71.09%] [G loss: 1.880072]\n",
      "epoch:27 step:25674 [D loss: 0.636191, acc: 65.62%] [G loss: 1.767217]\n",
      "epoch:27 step:25675 [D loss: 0.686591, acc: 53.91%] [G loss: 1.839803]\n",
      "epoch:27 step:25676 [D loss: 0.648931, acc: 57.81%] [G loss: 1.860754]\n",
      "epoch:27 step:25677 [D loss: 0.619828, acc: 60.94%] [G loss: 1.741159]\n",
      "epoch:27 step:25678 [D loss: 0.622122, acc: 66.41%] [G loss: 1.966462]\n",
      "epoch:27 step:25679 [D loss: 0.667673, acc: 59.38%] [G loss: 1.975603]\n",
      "epoch:27 step:25680 [D loss: 0.645941, acc: 62.50%] [G loss: 2.040069]\n",
      "epoch:27 step:25681 [D loss: 0.683153, acc: 53.91%] [G loss: 1.843273]\n",
      "epoch:27 step:25682 [D loss: 0.672756, acc: 60.16%] [G loss: 2.005265]\n",
      "epoch:27 step:25683 [D loss: 0.674868, acc: 60.94%] [G loss: 1.839805]\n",
      "epoch:27 step:25684 [D loss: 0.679271, acc: 58.59%] [G loss: 1.807500]\n",
      "epoch:27 step:25685 [D loss: 0.679315, acc: 59.38%] [G loss: 1.755380]\n",
      "epoch:27 step:25686 [D loss: 0.650619, acc: 65.62%] [G loss: 1.822851]\n",
      "epoch:27 step:25687 [D loss: 0.725342, acc: 57.03%] [G loss: 1.810583]\n",
      "epoch:27 step:25688 [D loss: 0.633698, acc: 66.41%] [G loss: 1.751447]\n",
      "epoch:27 step:25689 [D loss: 0.636720, acc: 64.06%] [G loss: 1.966022]\n",
      "epoch:27 step:25690 [D loss: 0.647046, acc: 63.28%] [G loss: 1.765739]\n",
      "epoch:27 step:25691 [D loss: 0.671651, acc: 57.03%] [G loss: 1.848996]\n",
      "epoch:27 step:25692 [D loss: 0.668583, acc: 53.91%] [G loss: 1.888868]\n",
      "epoch:27 step:25693 [D loss: 0.659896, acc: 62.50%] [G loss: 1.828970]\n",
      "epoch:27 step:25694 [D loss: 0.647636, acc: 65.62%] [G loss: 1.945323]\n",
      "epoch:27 step:25695 [D loss: 0.686494, acc: 60.16%] [G loss: 1.693211]\n",
      "epoch:27 step:25696 [D loss: 0.670115, acc: 65.62%] [G loss: 1.717113]\n",
      "epoch:27 step:25697 [D loss: 0.617250, acc: 66.41%] [G loss: 1.771029]\n",
      "epoch:27 step:25698 [D loss: 0.627055, acc: 62.50%] [G loss: 1.925909]\n",
      "epoch:27 step:25699 [D loss: 0.626864, acc: 64.06%] [G loss: 1.838877]\n",
      "epoch:27 step:25700 [D loss: 0.636512, acc: 63.28%] [G loss: 1.890564]\n",
      "epoch:27 step:25701 [D loss: 0.575383, acc: 75.00%] [G loss: 1.999825]\n",
      "epoch:27 step:25702 [D loss: 0.674417, acc: 59.38%] [G loss: 1.877214]\n",
      "epoch:27 step:25703 [D loss: 0.678674, acc: 57.81%] [G loss: 1.996159]\n",
      "epoch:27 step:25704 [D loss: 0.584909, acc: 73.44%] [G loss: 2.118096]\n",
      "epoch:27 step:25705 [D loss: 0.625261, acc: 69.53%] [G loss: 2.059278]\n",
      "epoch:27 step:25706 [D loss: 0.688931, acc: 54.69%] [G loss: 2.009860]\n",
      "epoch:27 step:25707 [D loss: 0.716702, acc: 57.03%] [G loss: 1.794774]\n",
      "epoch:27 step:25708 [D loss: 0.649982, acc: 56.25%] [G loss: 2.009943]\n",
      "epoch:27 step:25709 [D loss: 0.667365, acc: 61.72%] [G loss: 1.836274]\n",
      "epoch:27 step:25710 [D loss: 0.678624, acc: 57.81%] [G loss: 1.804796]\n",
      "epoch:27 step:25711 [D loss: 0.580741, acc: 71.09%] [G loss: 1.843966]\n",
      "epoch:27 step:25712 [D loss: 0.672879, acc: 55.47%] [G loss: 1.860828]\n",
      "epoch:27 step:25713 [D loss: 0.637624, acc: 62.50%] [G loss: 1.910963]\n",
      "epoch:27 step:25714 [D loss: 0.675485, acc: 57.81%] [G loss: 1.886596]\n",
      "epoch:27 step:25715 [D loss: 0.599564, acc: 70.31%] [G loss: 2.033716]\n",
      "epoch:27 step:25716 [D loss: 0.646853, acc: 59.38%] [G loss: 1.902985]\n",
      "epoch:27 step:25717 [D loss: 0.669994, acc: 60.94%] [G loss: 1.949147]\n",
      "epoch:27 step:25718 [D loss: 0.710220, acc: 54.69%] [G loss: 1.926664]\n",
      "epoch:27 step:25719 [D loss: 0.636010, acc: 64.06%] [G loss: 1.918223]\n",
      "epoch:27 step:25720 [D loss: 0.669500, acc: 60.16%] [G loss: 1.784070]\n",
      "epoch:27 step:25721 [D loss: 0.690797, acc: 55.47%] [G loss: 1.897443]\n",
      "epoch:27 step:25722 [D loss: 0.668819, acc: 57.81%] [G loss: 1.892668]\n",
      "epoch:27 step:25723 [D loss: 0.664617, acc: 57.81%] [G loss: 1.787952]\n",
      "epoch:27 step:25724 [D loss: 0.692063, acc: 58.59%] [G loss: 1.843614]\n",
      "epoch:27 step:25725 [D loss: 0.628822, acc: 64.84%] [G loss: 1.930617]\n",
      "epoch:27 step:25726 [D loss: 0.596426, acc: 70.31%] [G loss: 1.876370]\n",
      "epoch:27 step:25727 [D loss: 0.576924, acc: 67.97%] [G loss: 2.076556]\n",
      "epoch:27 step:25728 [D loss: 0.640997, acc: 67.19%] [G loss: 1.978643]\n",
      "epoch:27 step:25729 [D loss: 0.639763, acc: 66.41%] [G loss: 1.977920]\n",
      "epoch:27 step:25730 [D loss: 0.632142, acc: 64.84%] [G loss: 1.966603]\n",
      "epoch:27 step:25731 [D loss: 0.708441, acc: 57.81%] [G loss: 1.789519]\n",
      "epoch:27 step:25732 [D loss: 0.653920, acc: 60.16%] [G loss: 1.712974]\n",
      "epoch:27 step:25733 [D loss: 0.666376, acc: 60.94%] [G loss: 1.800536]\n",
      "epoch:27 step:25734 [D loss: 0.674289, acc: 58.59%] [G loss: 1.879179]\n",
      "epoch:27 step:25735 [D loss: 0.629782, acc: 66.41%] [G loss: 1.915986]\n",
      "epoch:27 step:25736 [D loss: 0.691470, acc: 48.44%] [G loss: 1.671245]\n",
      "epoch:27 step:25737 [D loss: 0.683382, acc: 53.12%] [G loss: 1.755075]\n",
      "epoch:27 step:25738 [D loss: 0.690747, acc: 57.81%] [G loss: 1.741937]\n",
      "epoch:27 step:25739 [D loss: 0.711043, acc: 55.47%] [G loss: 1.739782]\n",
      "epoch:27 step:25740 [D loss: 0.656157, acc: 60.16%] [G loss: 1.762728]\n",
      "epoch:27 step:25741 [D loss: 0.621494, acc: 66.41%] [G loss: 1.775619]\n",
      "epoch:27 step:25742 [D loss: 0.675108, acc: 53.91%] [G loss: 1.775681]\n",
      "epoch:27 step:25743 [D loss: 0.688503, acc: 57.03%] [G loss: 1.748748]\n",
      "epoch:27 step:25744 [D loss: 0.638313, acc: 64.84%] [G loss: 1.739310]\n",
      "epoch:27 step:25745 [D loss: 0.669938, acc: 62.50%] [G loss: 1.629464]\n",
      "epoch:27 step:25746 [D loss: 0.605825, acc: 62.50%] [G loss: 1.931056]\n",
      "epoch:27 step:25747 [D loss: 0.700128, acc: 51.56%] [G loss: 1.833321]\n",
      "epoch:27 step:25748 [D loss: 0.630149, acc: 63.28%] [G loss: 1.934025]\n",
      "epoch:27 step:25749 [D loss: 0.685154, acc: 62.50%] [G loss: 1.845300]\n",
      "epoch:27 step:25750 [D loss: 0.697747, acc: 57.03%] [G loss: 1.757291]\n",
      "epoch:27 step:25751 [D loss: 0.650720, acc: 63.28%] [G loss: 1.838676]\n",
      "epoch:27 step:25752 [D loss: 0.641188, acc: 64.84%] [G loss: 1.786182]\n",
      "epoch:27 step:25753 [D loss: 0.630887, acc: 65.62%] [G loss: 1.837670]\n",
      "epoch:27 step:25754 [D loss: 0.640899, acc: 62.50%] [G loss: 1.800715]\n",
      "epoch:27 step:25755 [D loss: 0.625695, acc: 67.19%] [G loss: 1.836649]\n",
      "epoch:27 step:25756 [D loss: 0.622000, acc: 63.28%] [G loss: 1.832482]\n",
      "epoch:27 step:25757 [D loss: 0.666920, acc: 56.25%] [G loss: 1.704331]\n",
      "epoch:27 step:25758 [D loss: 0.689870, acc: 51.56%] [G loss: 1.763724]\n",
      "epoch:27 step:25759 [D loss: 0.687897, acc: 60.16%] [G loss: 1.767784]\n",
      "epoch:27 step:25760 [D loss: 0.688549, acc: 55.47%] [G loss: 1.743269]\n",
      "epoch:27 step:25761 [D loss: 0.682479, acc: 63.28%] [G loss: 1.851897]\n",
      "epoch:27 step:25762 [D loss: 0.651997, acc: 58.59%] [G loss: 1.753211]\n",
      "epoch:27 step:25763 [D loss: 0.697497, acc: 53.91%] [G loss: 1.699382]\n",
      "epoch:27 step:25764 [D loss: 0.616750, acc: 65.62%] [G loss: 1.704593]\n",
      "epoch:27 step:25765 [D loss: 0.669665, acc: 60.16%] [G loss: 1.763643]\n",
      "epoch:27 step:25766 [D loss: 0.707992, acc: 55.47%] [G loss: 1.925963]\n",
      "epoch:27 step:25767 [D loss: 0.626196, acc: 64.06%] [G loss: 1.980256]\n",
      "epoch:27 step:25768 [D loss: 0.641292, acc: 62.50%] [G loss: 1.919493]\n",
      "epoch:27 step:25769 [D loss: 0.621589, acc: 64.06%] [G loss: 2.053926]\n",
      "epoch:27 step:25770 [D loss: 0.569680, acc: 73.44%] [G loss: 2.245299]\n",
      "epoch:27 step:25771 [D loss: 0.593292, acc: 71.88%] [G loss: 2.061375]\n",
      "epoch:27 step:25772 [D loss: 0.657373, acc: 60.94%] [G loss: 1.786953]\n",
      "epoch:27 step:25773 [D loss: 0.630888, acc: 67.19%] [G loss: 1.794029]\n",
      "epoch:27 step:25774 [D loss: 0.658847, acc: 60.94%] [G loss: 1.841093]\n",
      "epoch:27 step:25775 [D loss: 0.589936, acc: 71.09%] [G loss: 1.854629]\n",
      "epoch:27 step:25776 [D loss: 0.622119, acc: 68.75%] [G loss: 1.841997]\n",
      "epoch:27 step:25777 [D loss: 0.656553, acc: 59.38%] [G loss: 1.863630]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25778 [D loss: 0.606610, acc: 66.41%] [G loss: 1.886295]\n",
      "epoch:27 step:25779 [D loss: 0.665793, acc: 57.81%] [G loss: 1.863918]\n",
      "epoch:27 step:25780 [D loss: 0.684137, acc: 53.91%] [G loss: 2.215324]\n",
      "epoch:27 step:25781 [D loss: 0.672737, acc: 60.94%] [G loss: 1.634703]\n",
      "epoch:27 step:25782 [D loss: 0.649417, acc: 62.50%] [G loss: 1.791487]\n",
      "epoch:27 step:25783 [D loss: 0.683878, acc: 61.72%] [G loss: 1.914354]\n",
      "epoch:27 step:25784 [D loss: 0.685744, acc: 60.16%] [G loss: 1.799372]\n",
      "epoch:27 step:25785 [D loss: 0.637688, acc: 60.16%] [G loss: 1.896288]\n",
      "epoch:27 step:25786 [D loss: 0.664321, acc: 60.94%] [G loss: 1.806581]\n",
      "epoch:27 step:25787 [D loss: 0.589523, acc: 66.41%] [G loss: 1.983304]\n",
      "epoch:27 step:25788 [D loss: 0.630932, acc: 67.97%] [G loss: 1.805971]\n",
      "epoch:27 step:25789 [D loss: 0.654331, acc: 60.16%] [G loss: 1.717222]\n",
      "epoch:27 step:25790 [D loss: 0.661350, acc: 61.72%] [G loss: 1.881171]\n",
      "epoch:27 step:25791 [D loss: 0.631907, acc: 64.84%] [G loss: 1.874550]\n",
      "epoch:27 step:25792 [D loss: 0.615128, acc: 67.19%] [G loss: 2.001845]\n",
      "epoch:27 step:25793 [D loss: 0.633491, acc: 62.50%] [G loss: 1.824552]\n",
      "epoch:27 step:25794 [D loss: 0.587639, acc: 70.31%] [G loss: 1.919580]\n",
      "epoch:27 step:25795 [D loss: 0.626637, acc: 65.62%] [G loss: 1.989399]\n",
      "epoch:27 step:25796 [D loss: 0.638716, acc: 57.81%] [G loss: 1.948023]\n",
      "epoch:27 step:25797 [D loss: 0.658872, acc: 61.72%] [G loss: 1.891096]\n",
      "epoch:27 step:25798 [D loss: 0.629599, acc: 67.19%] [G loss: 1.911128]\n",
      "epoch:27 step:25799 [D loss: 0.675312, acc: 61.72%] [G loss: 1.894369]\n",
      "epoch:27 step:25800 [D loss: 0.760807, acc: 45.31%] [G loss: 1.711505]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.542763\n",
      "FID: 10.802943\n",
      "0 = 12.510926871156677\n",
      "1 = 0.07524444164139173\n",
      "2 = 0.8693000078201294\n",
      "3 = 0.8880000114440918\n",
      "4 = 0.850600004196167\n",
      "5 = 0.8559861183166504\n",
      "6 = 0.8880000114440918\n",
      "7 = 6.069720096480832\n",
      "8 = 0.06149727401274414\n",
      "9 = 0.6996999979019165\n",
      "10 = 0.7107999920845032\n",
      "11 = 0.6886000037193298\n",
      "12 = 0.69536292552948\n",
      "13 = 0.7107999920845032\n",
      "14 = 7.542794227600098\n",
      "15 = 9.504022598266602\n",
      "16 = 0.0883297398686409\n",
      "17 = 7.5427632331848145\n",
      "18 = 10.802943229675293\n",
      "epoch:27 step:25801 [D loss: 0.649795, acc: 60.16%] [G loss: 1.751614]\n",
      "epoch:27 step:25802 [D loss: 0.679874, acc: 59.38%] [G loss: 1.816012]\n",
      "epoch:27 step:25803 [D loss: 0.639554, acc: 63.28%] [G loss: 1.898517]\n",
      "epoch:27 step:25804 [D loss: 0.678530, acc: 58.59%] [G loss: 1.836807]\n",
      "epoch:27 step:25805 [D loss: 0.687380, acc: 58.59%] [G loss: 1.711611]\n",
      "epoch:27 step:25806 [D loss: 0.640862, acc: 60.94%] [G loss: 1.876411]\n",
      "epoch:27 step:25807 [D loss: 0.642678, acc: 60.16%] [G loss: 1.984326]\n",
      "epoch:27 step:25808 [D loss: 0.672742, acc: 64.84%] [G loss: 1.763606]\n",
      "epoch:27 step:25809 [D loss: 0.640681, acc: 65.62%] [G loss: 1.830125]\n",
      "epoch:27 step:25810 [D loss: 0.676012, acc: 60.16%] [G loss: 1.695034]\n",
      "epoch:27 step:25811 [D loss: 0.644118, acc: 57.03%] [G loss: 1.869816]\n",
      "epoch:27 step:25812 [D loss: 0.661831, acc: 62.50%] [G loss: 1.664357]\n",
      "epoch:27 step:25813 [D loss: 0.686248, acc: 53.91%] [G loss: 1.780438]\n",
      "epoch:27 step:25814 [D loss: 0.665395, acc: 57.81%] [G loss: 1.696124]\n",
      "epoch:27 step:25815 [D loss: 0.674407, acc: 59.38%] [G loss: 1.901261]\n",
      "epoch:27 step:25816 [D loss: 0.635084, acc: 67.97%] [G loss: 1.874821]\n",
      "epoch:27 step:25817 [D loss: 0.655165, acc: 62.50%] [G loss: 1.780436]\n",
      "epoch:27 step:25818 [D loss: 0.627105, acc: 62.50%] [G loss: 1.771950]\n",
      "epoch:27 step:25819 [D loss: 0.603790, acc: 64.06%] [G loss: 1.740545]\n",
      "epoch:27 step:25820 [D loss: 0.648904, acc: 66.41%] [G loss: 1.832175]\n",
      "epoch:27 step:25821 [D loss: 0.664166, acc: 57.03%] [G loss: 1.878152]\n",
      "epoch:27 step:25822 [D loss: 0.585733, acc: 70.31%] [G loss: 1.979491]\n",
      "epoch:27 step:25823 [D loss: 0.632293, acc: 64.06%] [G loss: 1.853385]\n",
      "epoch:27 step:25824 [D loss: 0.630743, acc: 63.28%] [G loss: 1.840562]\n",
      "epoch:27 step:25825 [D loss: 0.630703, acc: 61.72%] [G loss: 1.846815]\n",
      "epoch:27 step:25826 [D loss: 0.615009, acc: 64.06%] [G loss: 1.790371]\n",
      "epoch:27 step:25827 [D loss: 0.647070, acc: 63.28%] [G loss: 1.707183]\n",
      "epoch:27 step:25828 [D loss: 0.703875, acc: 54.69%] [G loss: 1.771179]\n",
      "epoch:27 step:25829 [D loss: 0.627022, acc: 61.72%] [G loss: 1.960336]\n",
      "epoch:27 step:25830 [D loss: 0.684809, acc: 53.12%] [G loss: 1.732023]\n",
      "epoch:27 step:25831 [D loss: 0.653765, acc: 60.16%] [G loss: 1.944316]\n",
      "epoch:27 step:25832 [D loss: 0.637034, acc: 61.72%] [G loss: 1.891925]\n",
      "epoch:27 step:25833 [D loss: 0.656839, acc: 60.16%] [G loss: 1.896097]\n",
      "epoch:27 step:25834 [D loss: 0.613578, acc: 69.53%] [G loss: 1.945192]\n",
      "epoch:27 step:25835 [D loss: 0.602225, acc: 72.66%] [G loss: 2.009315]\n",
      "epoch:27 step:25836 [D loss: 0.661677, acc: 65.62%] [G loss: 1.886198]\n",
      "epoch:27 step:25837 [D loss: 0.744866, acc: 52.34%] [G loss: 1.798588]\n",
      "epoch:27 step:25838 [D loss: 0.662391, acc: 57.03%] [G loss: 1.844404]\n",
      "epoch:27 step:25839 [D loss: 0.697534, acc: 55.47%] [G loss: 1.798902]\n",
      "epoch:27 step:25840 [D loss: 0.649810, acc: 60.94%] [G loss: 1.706537]\n",
      "epoch:27 step:25841 [D loss: 0.696775, acc: 57.81%] [G loss: 1.723787]\n",
      "epoch:27 step:25842 [D loss: 0.550319, acc: 75.78%] [G loss: 1.752330]\n",
      "epoch:27 step:25843 [D loss: 0.635951, acc: 62.50%] [G loss: 1.775487]\n",
      "epoch:27 step:25844 [D loss: 0.692467, acc: 53.91%] [G loss: 1.864513]\n",
      "epoch:27 step:25845 [D loss: 0.654381, acc: 58.59%] [G loss: 1.732589]\n",
      "epoch:27 step:25846 [D loss: 0.629960, acc: 66.41%] [G loss: 1.922615]\n",
      "epoch:27 step:25847 [D loss: 0.666161, acc: 60.94%] [G loss: 1.843035]\n",
      "epoch:27 step:25848 [D loss: 0.625865, acc: 64.06%] [G loss: 1.943032]\n",
      "epoch:27 step:25849 [D loss: 0.654370, acc: 60.16%] [G loss: 1.997267]\n",
      "epoch:27 step:25850 [D loss: 0.616154, acc: 67.19%] [G loss: 1.992797]\n",
      "epoch:27 step:25851 [D loss: 0.617648, acc: 66.41%] [G loss: 1.920473]\n",
      "epoch:27 step:25852 [D loss: 0.662838, acc: 57.03%] [G loss: 1.791844]\n",
      "epoch:27 step:25853 [D loss: 0.601928, acc: 67.97%] [G loss: 2.152990]\n",
      "epoch:27 step:25854 [D loss: 0.591156, acc: 65.62%] [G loss: 2.021534]\n",
      "epoch:27 step:25855 [D loss: 0.640459, acc: 63.28%] [G loss: 1.885301]\n",
      "epoch:27 step:25856 [D loss: 0.637992, acc: 64.84%] [G loss: 1.936500]\n",
      "epoch:27 step:25857 [D loss: 0.619698, acc: 63.28%] [G loss: 2.090623]\n",
      "epoch:27 step:25858 [D loss: 0.684691, acc: 53.91%] [G loss: 1.737967]\n",
      "epoch:27 step:25859 [D loss: 0.657822, acc: 63.28%] [G loss: 1.789908]\n",
      "epoch:27 step:25860 [D loss: 0.651300, acc: 60.94%] [G loss: 1.778795]\n",
      "epoch:27 step:25861 [D loss: 0.652488, acc: 57.81%] [G loss: 2.045205]\n",
      "epoch:27 step:25862 [D loss: 0.637799, acc: 62.50%] [G loss: 1.972856]\n",
      "epoch:27 step:25863 [D loss: 0.601290, acc: 67.97%] [G loss: 2.107413]\n",
      "epoch:27 step:25864 [D loss: 0.669764, acc: 57.03%] [G loss: 1.852143]\n",
      "epoch:27 step:25865 [D loss: 0.685199, acc: 54.69%] [G loss: 1.735072]\n",
      "epoch:27 step:25866 [D loss: 0.689843, acc: 55.47%] [G loss: 1.872591]\n",
      "epoch:27 step:25867 [D loss: 0.673337, acc: 56.25%] [G loss: 1.810371]\n",
      "epoch:27 step:25868 [D loss: 0.675568, acc: 55.47%] [G loss: 1.825963]\n",
      "epoch:27 step:25869 [D loss: 0.617931, acc: 63.28%] [G loss: 1.857688]\n",
      "epoch:27 step:25870 [D loss: 0.618752, acc: 64.06%] [G loss: 1.864825]\n",
      "epoch:27 step:25871 [D loss: 0.620836, acc: 66.41%] [G loss: 1.806939]\n",
      "epoch:27 step:25872 [D loss: 0.651436, acc: 64.06%] [G loss: 1.798903]\n",
      "epoch:27 step:25873 [D loss: 0.645447, acc: 60.94%] [G loss: 2.034250]\n",
      "epoch:27 step:25874 [D loss: 0.609115, acc: 68.75%] [G loss: 1.932740]\n",
      "epoch:27 step:25875 [D loss: 0.635908, acc: 56.25%] [G loss: 1.807270]\n",
      "epoch:27 step:25876 [D loss: 0.707378, acc: 53.91%] [G loss: 1.796284]\n",
      "epoch:27 step:25877 [D loss: 0.617302, acc: 66.41%] [G loss: 1.719509]\n",
      "epoch:27 step:25878 [D loss: 0.624673, acc: 63.28%] [G loss: 1.797602]\n",
      "epoch:27 step:25879 [D loss: 0.661297, acc: 58.59%] [G loss: 1.753909]\n",
      "epoch:27 step:25880 [D loss: 0.678084, acc: 54.69%] [G loss: 1.769950]\n",
      "epoch:27 step:25881 [D loss: 0.645160, acc: 64.06%] [G loss: 1.956540]\n",
      "epoch:27 step:25882 [D loss: 0.655172, acc: 62.50%] [G loss: 1.950896]\n",
      "epoch:27 step:25883 [D loss: 0.639292, acc: 67.19%] [G loss: 1.877246]\n",
      "epoch:27 step:25884 [D loss: 0.629425, acc: 67.19%] [G loss: 1.892688]\n",
      "epoch:27 step:25885 [D loss: 0.707729, acc: 54.69%] [G loss: 1.841523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25886 [D loss: 0.661073, acc: 56.25%] [G loss: 1.998268]\n",
      "epoch:27 step:25887 [D loss: 0.659122, acc: 60.94%] [G loss: 2.152717]\n",
      "epoch:27 step:25888 [D loss: 0.614004, acc: 71.09%] [G loss: 1.994236]\n",
      "epoch:27 step:25889 [D loss: 0.631179, acc: 63.28%] [G loss: 1.903182]\n",
      "epoch:27 step:25890 [D loss: 0.637573, acc: 63.28%] [G loss: 1.798675]\n",
      "epoch:27 step:25891 [D loss: 0.698607, acc: 55.47%] [G loss: 1.876511]\n",
      "epoch:27 step:25892 [D loss: 0.666733, acc: 62.50%] [G loss: 1.861372]\n",
      "epoch:27 step:25893 [D loss: 0.649000, acc: 64.84%] [G loss: 1.800231]\n",
      "epoch:27 step:25894 [D loss: 0.673396, acc: 67.19%] [G loss: 1.859758]\n",
      "epoch:27 step:25895 [D loss: 0.731468, acc: 54.69%] [G loss: 1.723407]\n",
      "epoch:27 step:25896 [D loss: 0.700992, acc: 60.16%] [G loss: 1.683874]\n",
      "epoch:27 step:25897 [D loss: 0.639185, acc: 66.41%] [G loss: 1.848708]\n",
      "epoch:27 step:25898 [D loss: 0.671482, acc: 60.16%] [G loss: 1.819588]\n",
      "epoch:27 step:25899 [D loss: 0.639279, acc: 64.84%] [G loss: 1.734298]\n",
      "epoch:27 step:25900 [D loss: 0.632041, acc: 64.06%] [G loss: 1.873290]\n",
      "epoch:27 step:25901 [D loss: 0.672359, acc: 59.38%] [G loss: 1.904610]\n",
      "epoch:27 step:25902 [D loss: 0.681779, acc: 59.38%] [G loss: 1.917095]\n",
      "epoch:27 step:25903 [D loss: 0.648563, acc: 56.25%] [G loss: 1.860221]\n",
      "epoch:27 step:25904 [D loss: 0.623676, acc: 62.50%] [G loss: 1.914222]\n",
      "epoch:27 step:25905 [D loss: 0.643327, acc: 63.28%] [G loss: 1.813538]\n",
      "epoch:27 step:25906 [D loss: 0.648549, acc: 62.50%] [G loss: 1.872814]\n",
      "epoch:27 step:25907 [D loss: 0.611460, acc: 67.19%] [G loss: 1.839876]\n",
      "epoch:27 step:25908 [D loss: 0.663411, acc: 65.62%] [G loss: 1.761056]\n",
      "epoch:27 step:25909 [D loss: 0.665060, acc: 62.50%] [G loss: 1.870118]\n",
      "epoch:27 step:25910 [D loss: 0.661301, acc: 58.59%] [G loss: 1.819936]\n",
      "epoch:27 step:25911 [D loss: 0.692711, acc: 53.12%] [G loss: 1.746410]\n",
      "epoch:27 step:25912 [D loss: 0.634328, acc: 61.72%] [G loss: 1.756788]\n",
      "epoch:27 step:25913 [D loss: 0.657389, acc: 60.16%] [G loss: 1.624698]\n",
      "epoch:27 step:25914 [D loss: 0.712202, acc: 54.69%] [G loss: 1.711057]\n",
      "epoch:27 step:25915 [D loss: 0.676664, acc: 59.38%] [G loss: 1.692940]\n",
      "epoch:27 step:25916 [D loss: 0.690034, acc: 63.28%] [G loss: 1.820114]\n",
      "epoch:27 step:25917 [D loss: 0.628489, acc: 64.06%] [G loss: 1.811436]\n",
      "epoch:27 step:25918 [D loss: 0.638889, acc: 61.72%] [G loss: 1.757777]\n",
      "epoch:27 step:25919 [D loss: 0.633708, acc: 64.84%] [G loss: 1.730572]\n",
      "epoch:27 step:25920 [D loss: 0.655237, acc: 60.16%] [G loss: 1.857922]\n",
      "epoch:27 step:25921 [D loss: 0.700105, acc: 53.12%] [G loss: 2.000347]\n",
      "epoch:27 step:25922 [D loss: 0.688411, acc: 62.50%] [G loss: 1.822850]\n",
      "epoch:27 step:25923 [D loss: 0.653580, acc: 60.16%] [G loss: 2.010601]\n",
      "epoch:27 step:25924 [D loss: 0.666636, acc: 57.81%] [G loss: 1.687639]\n",
      "epoch:27 step:25925 [D loss: 0.687952, acc: 59.38%] [G loss: 1.845107]\n",
      "epoch:27 step:25926 [D loss: 0.644321, acc: 60.16%] [G loss: 1.804069]\n",
      "epoch:27 step:25927 [D loss: 0.689965, acc: 52.34%] [G loss: 1.631950]\n",
      "epoch:27 step:25928 [D loss: 0.650731, acc: 63.28%] [G loss: 1.879205]\n",
      "epoch:27 step:25929 [D loss: 0.624277, acc: 68.75%] [G loss: 1.807030]\n",
      "epoch:27 step:25930 [D loss: 0.630760, acc: 64.06%] [G loss: 1.786690]\n",
      "epoch:27 step:25931 [D loss: 0.582435, acc: 72.66%] [G loss: 1.944360]\n",
      "epoch:27 step:25932 [D loss: 0.618268, acc: 64.84%] [G loss: 1.783080]\n",
      "epoch:27 step:25933 [D loss: 0.593331, acc: 71.88%] [G loss: 1.938499]\n",
      "epoch:27 step:25934 [D loss: 0.571895, acc: 71.09%] [G loss: 1.962384]\n",
      "epoch:27 step:25935 [D loss: 0.652614, acc: 66.41%] [G loss: 1.930174]\n",
      "epoch:27 step:25936 [D loss: 0.687441, acc: 58.59%] [G loss: 2.040434]\n",
      "epoch:27 step:25937 [D loss: 0.621987, acc: 67.97%] [G loss: 1.854893]\n",
      "epoch:27 step:25938 [D loss: 0.656765, acc: 61.72%] [G loss: 1.863110]\n",
      "epoch:27 step:25939 [D loss: 0.654550, acc: 64.84%] [G loss: 1.827574]\n",
      "epoch:27 step:25940 [D loss: 0.678893, acc: 60.94%] [G loss: 1.704835]\n",
      "epoch:27 step:25941 [D loss: 0.663211, acc: 60.16%] [G loss: 1.965084]\n",
      "epoch:27 step:25942 [D loss: 0.684600, acc: 61.72%] [G loss: 1.932957]\n",
      "epoch:27 step:25943 [D loss: 0.616307, acc: 67.19%] [G loss: 1.927473]\n",
      "epoch:27 step:25944 [D loss: 0.701506, acc: 63.28%] [G loss: 1.892127]\n",
      "epoch:27 step:25945 [D loss: 0.607113, acc: 67.97%] [G loss: 1.938524]\n",
      "epoch:27 step:25946 [D loss: 0.682354, acc: 57.03%] [G loss: 2.267284]\n",
      "epoch:27 step:25947 [D loss: 0.656848, acc: 64.06%] [G loss: 2.182336]\n",
      "epoch:27 step:25948 [D loss: 0.579080, acc: 72.66%] [G loss: 2.013261]\n",
      "epoch:27 step:25949 [D loss: 0.654371, acc: 67.97%] [G loss: 2.015366]\n",
      "epoch:27 step:25950 [D loss: 0.647593, acc: 64.84%] [G loss: 1.827655]\n",
      "epoch:27 step:25951 [D loss: 0.608353, acc: 69.53%] [G loss: 1.830386]\n",
      "epoch:27 step:25952 [D loss: 0.640243, acc: 66.41%] [G loss: 1.929201]\n",
      "epoch:27 step:25953 [D loss: 0.634447, acc: 60.94%] [G loss: 1.848016]\n",
      "epoch:27 step:25954 [D loss: 0.663241, acc: 60.16%] [G loss: 1.854741]\n",
      "epoch:27 step:25955 [D loss: 0.670433, acc: 52.34%] [G loss: 1.824563]\n",
      "epoch:27 step:25956 [D loss: 0.678665, acc: 59.38%] [G loss: 1.696191]\n",
      "epoch:27 step:25957 [D loss: 0.668958, acc: 58.59%] [G loss: 1.829659]\n",
      "epoch:27 step:25958 [D loss: 0.617439, acc: 63.28%] [G loss: 1.765315]\n",
      "epoch:27 step:25959 [D loss: 0.654688, acc: 60.16%] [G loss: 1.749391]\n",
      "epoch:27 step:25960 [D loss: 0.657019, acc: 60.94%] [G loss: 1.904432]\n",
      "epoch:27 step:25961 [D loss: 0.685292, acc: 57.03%] [G loss: 1.799136]\n",
      "epoch:27 step:25962 [D loss: 0.681802, acc: 60.16%] [G loss: 1.782829]\n",
      "epoch:27 step:25963 [D loss: 0.683883, acc: 55.47%] [G loss: 1.825214]\n",
      "epoch:27 step:25964 [D loss: 0.639788, acc: 62.50%] [G loss: 1.829477]\n",
      "epoch:27 step:25965 [D loss: 0.651310, acc: 60.16%] [G loss: 1.670363]\n",
      "epoch:27 step:25966 [D loss: 0.627719, acc: 69.53%] [G loss: 1.765533]\n",
      "epoch:27 step:25967 [D loss: 0.683188, acc: 57.81%] [G loss: 1.880555]\n",
      "epoch:27 step:25968 [D loss: 0.651701, acc: 63.28%] [G loss: 1.724206]\n",
      "epoch:27 step:25969 [D loss: 0.680348, acc: 57.81%] [G loss: 1.812264]\n",
      "epoch:27 step:25970 [D loss: 0.640839, acc: 62.50%] [G loss: 1.804955]\n",
      "epoch:27 step:25971 [D loss: 0.636211, acc: 67.97%] [G loss: 1.785365]\n",
      "epoch:27 step:25972 [D loss: 0.623299, acc: 63.28%] [G loss: 1.898321]\n",
      "epoch:27 step:25973 [D loss: 0.610798, acc: 69.53%] [G loss: 1.968181]\n",
      "epoch:27 step:25974 [D loss: 0.652724, acc: 57.81%] [G loss: 1.847253]\n",
      "epoch:27 step:25975 [D loss: 0.653280, acc: 60.94%] [G loss: 1.850567]\n",
      "epoch:27 step:25976 [D loss: 0.663998, acc: 61.72%] [G loss: 1.732122]\n",
      "epoch:27 step:25977 [D loss: 0.616973, acc: 61.72%] [G loss: 1.858982]\n",
      "epoch:27 step:25978 [D loss: 0.614060, acc: 68.75%] [G loss: 1.959962]\n",
      "epoch:27 step:25979 [D loss: 0.619888, acc: 67.97%] [G loss: 1.875219]\n",
      "epoch:27 step:25980 [D loss: 0.633872, acc: 63.28%] [G loss: 1.849684]\n",
      "epoch:27 step:25981 [D loss: 0.708479, acc: 57.03%] [G loss: 1.723188]\n",
      "epoch:27 step:25982 [D loss: 0.684022, acc: 54.69%] [G loss: 1.823000]\n",
      "epoch:27 step:25983 [D loss: 0.636523, acc: 60.94%] [G loss: 1.748815]\n",
      "epoch:27 step:25984 [D loss: 0.615852, acc: 66.41%] [G loss: 1.835026]\n",
      "epoch:27 step:25985 [D loss: 0.604948, acc: 70.31%] [G loss: 1.754248]\n",
      "epoch:27 step:25986 [D loss: 0.617710, acc: 61.72%] [G loss: 1.856679]\n",
      "epoch:27 step:25987 [D loss: 0.649902, acc: 67.97%] [G loss: 1.936098]\n",
      "epoch:27 step:25988 [D loss: 0.638182, acc: 60.16%] [G loss: 2.096456]\n",
      "epoch:27 step:25989 [D loss: 0.665004, acc: 62.50%] [G loss: 1.949342]\n",
      "epoch:27 step:25990 [D loss: 0.651398, acc: 60.94%] [G loss: 2.007746]\n",
      "epoch:27 step:25991 [D loss: 0.663368, acc: 60.16%] [G loss: 1.929703]\n",
      "epoch:27 step:25992 [D loss: 0.601656, acc: 70.31%] [G loss: 2.072272]\n",
      "epoch:27 step:25993 [D loss: 0.577193, acc: 68.75%] [G loss: 2.065056]\n",
      "epoch:27 step:25994 [D loss: 0.647975, acc: 65.62%] [G loss: 2.109712]\n",
      "epoch:27 step:25995 [D loss: 0.650658, acc: 60.94%] [G loss: 1.892279]\n",
      "epoch:27 step:25996 [D loss: 0.655313, acc: 57.81%] [G loss: 1.934107]\n",
      "epoch:27 step:25997 [D loss: 0.672127, acc: 60.94%] [G loss: 1.758436]\n",
      "epoch:27 step:25998 [D loss: 0.650496, acc: 62.50%] [G loss: 1.948493]\n",
      "epoch:27 step:25999 [D loss: 0.661789, acc: 61.72%] [G loss: 1.798813]\n",
      "epoch:27 step:26000 [D loss: 0.615835, acc: 65.62%] [G loss: 2.033248]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute score in space: 1\n",
      "IS socre: 7.488870\n",
      "FID: 12.121718\n",
      "0 = 12.670491278696018\n",
      "1 = 0.08653950079269426\n",
      "2 = 0.8726000189781189\n",
      "3 = 0.8971999883651733\n",
      "4 = 0.8479999899864197\n",
      "5 = 0.8551276922225952\n",
      "6 = 0.8971999883651733\n",
      "7 = 6.204507860553269\n",
      "8 = 0.06709648887896634\n",
      "9 = 0.70169997215271\n",
      "10 = 0.7116000056266785\n",
      "11 = 0.6917999982833862\n",
      "12 = 0.697783887386322\n",
      "13 = 0.7116000056266785\n",
      "14 = 7.488911151885986\n",
      "15 = 9.476520538330078\n",
      "16 = 0.10453533381223679\n",
      "17 = 7.488869667053223\n",
      "18 = 12.121718406677246\n",
      "epoch:27 step:26001 [D loss: 0.644530, acc: 61.72%] [G loss: 1.760489]\n",
      "epoch:27 step:26002 [D loss: 0.693200, acc: 53.91%] [G loss: 1.664097]\n",
      "epoch:27 step:26003 [D loss: 0.678309, acc: 59.38%] [G loss: 1.740105]\n",
      "epoch:27 step:26004 [D loss: 0.690604, acc: 61.72%] [G loss: 1.718503]\n",
      "epoch:27 step:26005 [D loss: 0.635246, acc: 65.62%] [G loss: 1.849029]\n",
      "epoch:27 step:26006 [D loss: 0.623894, acc: 62.50%] [G loss: 1.998192]\n",
      "epoch:27 step:26007 [D loss: 0.689001, acc: 57.03%] [G loss: 1.969109]\n",
      "epoch:27 step:26008 [D loss: 0.652561, acc: 60.16%] [G loss: 1.997584]\n",
      "epoch:27 step:26009 [D loss: 0.665639, acc: 61.72%] [G loss: 1.723178]\n",
      "epoch:27 step:26010 [D loss: 0.619599, acc: 68.75%] [G loss: 1.919608]\n",
      "epoch:27 step:26011 [D loss: 0.595397, acc: 69.53%] [G loss: 1.839259]\n",
      "epoch:27 step:26012 [D loss: 0.666740, acc: 62.50%] [G loss: 1.802738]\n",
      "epoch:27 step:26013 [D loss: 0.650576, acc: 58.59%] [G loss: 1.968599]\n",
      "epoch:27 step:26014 [D loss: 0.653042, acc: 66.41%] [G loss: 2.026307]\n",
      "epoch:27 step:26015 [D loss: 0.683484, acc: 60.94%] [G loss: 1.839530]\n",
      "epoch:27 step:26016 [D loss: 0.662418, acc: 61.72%] [G loss: 1.938779]\n",
      "epoch:27 step:26017 [D loss: 0.682055, acc: 58.59%] [G loss: 2.004907]\n",
      "epoch:27 step:26018 [D loss: 0.672992, acc: 62.50%] [G loss: 1.984617]\n",
      "epoch:27 step:26019 [D loss: 0.611331, acc: 72.66%] [G loss: 1.904331]\n",
      "epoch:27 step:26020 [D loss: 0.681886, acc: 57.03%] [G loss: 1.903577]\n",
      "epoch:27 step:26021 [D loss: 0.692106, acc: 54.69%] [G loss: 1.785815]\n",
      "epoch:27 step:26022 [D loss: 0.631443, acc: 66.41%] [G loss: 1.793351]\n",
      "epoch:27 step:26023 [D loss: 0.619621, acc: 64.84%] [G loss: 1.855332]\n",
      "epoch:27 step:26024 [D loss: 0.657427, acc: 60.94%] [G loss: 1.939479]\n",
      "epoch:27 step:26025 [D loss: 0.613265, acc: 66.41%] [G loss: 2.129845]\n",
      "epoch:27 step:26026 [D loss: 0.656877, acc: 58.59%] [G loss: 1.952128]\n",
      "epoch:27 step:26027 [D loss: 0.670977, acc: 57.03%] [G loss: 1.849054]\n",
      "epoch:27 step:26028 [D loss: 0.738307, acc: 53.12%] [G loss: 1.633673]\n",
      "epoch:27 step:26029 [D loss: 0.670436, acc: 63.28%] [G loss: 1.874557]\n",
      "epoch:27 step:26030 [D loss: 0.677627, acc: 58.59%] [G loss: 1.795924]\n",
      "epoch:27 step:26031 [D loss: 0.670445, acc: 60.16%] [G loss: 1.919896]\n",
      "epoch:27 step:26032 [D loss: 0.606254, acc: 67.97%] [G loss: 1.859039]\n",
      "epoch:27 step:26033 [D loss: 0.674126, acc: 57.03%] [G loss: 1.751239]\n",
      "epoch:27 step:26034 [D loss: 0.627972, acc: 65.62%] [G loss: 1.852916]\n",
      "epoch:27 step:26035 [D loss: 0.673368, acc: 57.03%] [G loss: 1.832703]\n",
      "epoch:27 step:26036 [D loss: 0.612916, acc: 67.19%] [G loss: 1.876488]\n",
      "epoch:27 step:26037 [D loss: 0.627365, acc: 64.06%] [G loss: 1.912620]\n",
      "epoch:27 step:26038 [D loss: 0.689738, acc: 54.69%] [G loss: 1.669884]\n",
      "epoch:27 step:26039 [D loss: 0.636148, acc: 63.28%] [G loss: 1.938165]\n",
      "epoch:27 step:26040 [D loss: 0.705804, acc: 55.47%] [G loss: 1.863950]\n",
      "epoch:27 step:26041 [D loss: 0.711713, acc: 57.81%] [G loss: 1.885631]\n",
      "epoch:27 step:26042 [D loss: 0.617290, acc: 65.62%] [G loss: 1.814727]\n",
      "epoch:27 step:26043 [D loss: 0.666095, acc: 61.72%] [G loss: 1.719110]\n",
      "epoch:27 step:26044 [D loss: 0.695759, acc: 53.91%] [G loss: 1.750511]\n",
      "epoch:27 step:26045 [D loss: 0.585136, acc: 72.66%] [G loss: 1.880948]\n",
      "epoch:27 step:26046 [D loss: 0.600625, acc: 67.19%] [G loss: 1.919174]\n",
      "epoch:27 step:26047 [D loss: 0.677486, acc: 57.03%] [G loss: 1.852285]\n",
      "epoch:27 step:26048 [D loss: 0.617606, acc: 60.94%] [G loss: 1.751136]\n",
      "epoch:27 step:26049 [D loss: 0.635129, acc: 67.19%] [G loss: 1.823239]\n",
      "epoch:27 step:26050 [D loss: 0.628042, acc: 64.84%] [G loss: 1.766546]\n",
      "epoch:27 step:26051 [D loss: 0.724658, acc: 54.69%] [G loss: 1.797109]\n",
      "epoch:27 step:26052 [D loss: 0.648634, acc: 59.38%] [G loss: 1.816674]\n",
      "epoch:27 step:26053 [D loss: 0.642519, acc: 62.50%] [G loss: 1.912667]\n",
      "epoch:27 step:26054 [D loss: 0.690306, acc: 53.91%] [G loss: 1.843093]\n",
      "epoch:27 step:26055 [D loss: 0.649614, acc: 57.81%] [G loss: 1.773906]\n",
      "epoch:27 step:26056 [D loss: 0.688971, acc: 54.69%] [G loss: 1.798499]\n",
      "epoch:27 step:26057 [D loss: 0.664667, acc: 62.50%] [G loss: 1.740837]\n",
      "epoch:27 step:26058 [D loss: 0.652754, acc: 57.81%] [G loss: 1.877084]\n",
      "epoch:27 step:26059 [D loss: 0.703310, acc: 53.91%] [G loss: 1.932899]\n",
      "epoch:27 step:26060 [D loss: 0.697749, acc: 56.25%] [G loss: 1.831535]\n",
      "epoch:27 step:26061 [D loss: 0.644539, acc: 64.06%] [G loss: 1.691435]\n",
      "epoch:27 step:26062 [D loss: 0.634948, acc: 64.06%] [G loss: 1.876779]\n",
      "epoch:27 step:26063 [D loss: 0.675676, acc: 54.69%] [G loss: 1.766818]\n",
      "epoch:27 step:26064 [D loss: 0.662592, acc: 60.94%] [G loss: 1.588227]\n",
      "epoch:27 step:26065 [D loss: 0.734286, acc: 55.47%] [G loss: 1.702298]\n",
      "epoch:27 step:26066 [D loss: 0.675860, acc: 55.47%] [G loss: 1.771193]\n",
      "epoch:27 step:26067 [D loss: 0.670144, acc: 60.16%] [G loss: 1.864969]\n",
      "epoch:27 step:26068 [D loss: 0.622861, acc: 64.06%] [G loss: 1.879136]\n",
      "epoch:27 step:26069 [D loss: 0.623847, acc: 62.50%] [G loss: 1.792154]\n",
      "epoch:27 step:26070 [D loss: 0.685787, acc: 57.03%] [G loss: 1.751300]\n",
      "epoch:27 step:26071 [D loss: 0.671400, acc: 55.47%] [G loss: 1.823249]\n",
      "epoch:27 step:26072 [D loss: 0.678539, acc: 57.81%] [G loss: 1.842502]\n",
      "epoch:27 step:26073 [D loss: 0.608868, acc: 67.19%] [G loss: 2.111429]\n",
      "epoch:27 step:26074 [D loss: 0.590191, acc: 70.31%] [G loss: 2.119061]\n",
      "epoch:27 step:26075 [D loss: 0.663694, acc: 59.38%] [G loss: 1.852196]\n",
      "epoch:27 step:26076 [D loss: 0.682439, acc: 58.59%] [G loss: 1.722715]\n",
      "epoch:27 step:26077 [D loss: 0.715274, acc: 53.12%] [G loss: 1.772406]\n",
      "epoch:27 step:26078 [D loss: 0.684130, acc: 60.94%] [G loss: 1.884155]\n",
      "epoch:27 step:26079 [D loss: 0.679888, acc: 60.16%] [G loss: 1.913908]\n",
      "epoch:27 step:26080 [D loss: 0.650972, acc: 63.28%] [G loss: 2.017601]\n",
      "epoch:27 step:26081 [D loss: 0.607319, acc: 65.62%] [G loss: 2.019414]\n",
      "epoch:27 step:26082 [D loss: 0.689388, acc: 60.16%] [G loss: 1.853151]\n",
      "epoch:27 step:26083 [D loss: 0.703731, acc: 55.47%] [G loss: 1.828392]\n",
      "epoch:27 step:26084 [D loss: 0.660237, acc: 59.38%] [G loss: 1.843246]\n",
      "epoch:27 step:26085 [D loss: 0.582972, acc: 70.31%] [G loss: 1.954226]\n",
      "epoch:27 step:26086 [D loss: 0.636588, acc: 64.06%] [G loss: 1.855688]\n",
      "epoch:27 step:26087 [D loss: 0.650544, acc: 66.41%] [G loss: 1.834979]\n",
      "epoch:27 step:26088 [D loss: 0.650175, acc: 62.50%] [G loss: 1.801426]\n",
      "epoch:27 step:26089 [D loss: 0.651117, acc: 60.94%] [G loss: 1.850159]\n",
      "epoch:27 step:26090 [D loss: 0.671700, acc: 58.59%] [G loss: 1.843781]\n",
      "epoch:27 step:26091 [D loss: 0.624163, acc: 64.84%] [G loss: 1.986055]\n",
      "epoch:27 step:26092 [D loss: 0.614946, acc: 67.19%] [G loss: 1.962945]\n",
      "epoch:27 step:26093 [D loss: 0.709804, acc: 58.59%] [G loss: 1.699706]\n",
      "epoch:27 step:26094 [D loss: 0.691852, acc: 57.03%] [G loss: 1.699930]\n",
      "epoch:27 step:26095 [D loss: 0.677970, acc: 57.81%] [G loss: 1.774993]\n",
      "epoch:27 step:26096 [D loss: 0.640457, acc: 59.38%] [G loss: 1.848633]\n",
      "epoch:27 step:26097 [D loss: 0.641538, acc: 60.94%] [G loss: 1.815145]\n",
      "epoch:27 step:26098 [D loss: 0.736901, acc: 53.91%] [G loss: 1.720085]\n",
      "epoch:27 step:26099 [D loss: 0.704999, acc: 55.47%] [G loss: 1.620488]\n",
      "epoch:27 step:26100 [D loss: 0.683333, acc: 51.56%] [G loss: 1.666528]\n",
      "epoch:27 step:26101 [D loss: 0.645443, acc: 60.16%] [G loss: 1.771925]\n",
      "epoch:27 step:26102 [D loss: 0.644593, acc: 60.16%] [G loss: 1.838068]\n",
      "epoch:27 step:26103 [D loss: 0.618024, acc: 64.84%] [G loss: 1.835459]\n",
      "epoch:27 step:26104 [D loss: 0.646911, acc: 57.81%] [G loss: 1.851915]\n",
      "epoch:27 step:26105 [D loss: 0.620187, acc: 64.06%] [G loss: 1.752816]\n",
      "epoch:27 step:26106 [D loss: 0.674080, acc: 57.81%] [G loss: 1.931516]\n",
      "epoch:27 step:26107 [D loss: 0.667842, acc: 58.59%] [G loss: 1.812054]\n",
      "epoch:27 step:26108 [D loss: 0.641580, acc: 65.62%] [G loss: 1.802073]\n",
      "epoch:27 step:26109 [D loss: 0.667328, acc: 56.25%] [G loss: 1.761123]\n",
      "epoch:27 step:26110 [D loss: 0.650067, acc: 57.03%] [G loss: 1.830390]\n",
      "epoch:27 step:26111 [D loss: 0.664025, acc: 57.81%] [G loss: 1.770493]\n",
      "epoch:27 step:26112 [D loss: 0.651953, acc: 61.72%] [G loss: 1.813789]\n",
      "epoch:27 step:26113 [D loss: 0.606692, acc: 68.75%] [G loss: 1.815269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:26114 [D loss: 0.664136, acc: 61.72%] [G loss: 1.801397]\n",
      "epoch:27 step:26115 [D loss: 0.626614, acc: 66.41%] [G loss: 2.040643]\n",
      "epoch:27 step:26116 [D loss: 0.618050, acc: 65.62%] [G loss: 1.804392]\n",
      "epoch:27 step:26117 [D loss: 0.691234, acc: 50.00%] [G loss: 1.784788]\n",
      "epoch:27 step:26118 [D loss: 0.651893, acc: 60.94%] [G loss: 1.821052]\n",
      "epoch:27 step:26119 [D loss: 0.714927, acc: 48.44%] [G loss: 1.688106]\n",
      "epoch:27 step:26120 [D loss: 0.680303, acc: 58.59%] [G loss: 1.858485]\n",
      "epoch:27 step:26121 [D loss: 0.654149, acc: 64.06%] [G loss: 1.812509]\n",
      "epoch:27 step:26122 [D loss: 0.616507, acc: 63.28%] [G loss: 1.813356]\n",
      "epoch:27 step:26123 [D loss: 0.649558, acc: 56.25%] [G loss: 1.770157]\n",
      "epoch:27 step:26124 [D loss: 0.650802, acc: 62.50%] [G loss: 1.999905]\n",
      "epoch:27 step:26125 [D loss: 0.643530, acc: 60.16%] [G loss: 1.815632]\n",
      "epoch:27 step:26126 [D loss: 0.674769, acc: 54.69%] [G loss: 1.766414]\n",
      "epoch:27 step:26127 [D loss: 0.746237, acc: 49.22%] [G loss: 1.664836]\n",
      "epoch:27 step:26128 [D loss: 0.678632, acc: 56.25%] [G loss: 1.713435]\n",
      "epoch:27 step:26129 [D loss: 0.673426, acc: 64.06%] [G loss: 1.740863]\n",
      "epoch:27 step:26130 [D loss: 0.648367, acc: 57.03%] [G loss: 1.846268]\n",
      "epoch:27 step:26131 [D loss: 0.632303, acc: 64.84%] [G loss: 1.735867]\n",
      "epoch:27 step:26132 [D loss: 0.607761, acc: 64.84%] [G loss: 1.767979]\n",
      "epoch:27 step:26133 [D loss: 0.672756, acc: 56.25%] [G loss: 1.836619]\n",
      "epoch:27 step:26134 [D loss: 0.621205, acc: 69.53%] [G loss: 1.795493]\n",
      "epoch:27 step:26135 [D loss: 0.615589, acc: 67.19%] [G loss: 1.826413]\n",
      "epoch:27 step:26136 [D loss: 0.607376, acc: 65.62%] [G loss: 1.883224]\n",
      "epoch:27 step:26137 [D loss: 0.641173, acc: 63.28%] [G loss: 1.963348]\n",
      "epoch:27 step:26138 [D loss: 0.658233, acc: 58.59%] [G loss: 1.801427]\n",
      "epoch:27 step:26139 [D loss: 0.641550, acc: 67.19%] [G loss: 1.915356]\n",
      "epoch:27 step:26140 [D loss: 0.635908, acc: 61.72%] [G loss: 1.837920]\n",
      "epoch:27 step:26141 [D loss: 0.574681, acc: 77.34%] [G loss: 1.930514]\n",
      "epoch:27 step:26142 [D loss: 0.619861, acc: 65.62%] [G loss: 1.889643]\n",
      "epoch:27 step:26143 [D loss: 0.629438, acc: 63.28%] [G loss: 1.970556]\n",
      "epoch:27 step:26144 [D loss: 0.629003, acc: 66.41%] [G loss: 1.893616]\n",
      "epoch:27 step:26145 [D loss: 0.595439, acc: 72.66%] [G loss: 1.864010]\n",
      "epoch:27 step:26146 [D loss: 0.646681, acc: 56.25%] [G loss: 1.956044]\n",
      "epoch:27 step:26147 [D loss: 0.633211, acc: 67.19%] [G loss: 1.812423]\n",
      "epoch:27 step:26148 [D loss: 0.634742, acc: 64.06%] [G loss: 1.979923]\n",
      "epoch:27 step:26149 [D loss: 0.680710, acc: 58.59%] [G loss: 1.795989]\n",
      "epoch:27 step:26150 [D loss: 0.651202, acc: 61.72%] [G loss: 1.748500]\n",
      "epoch:27 step:26151 [D loss: 0.669060, acc: 56.25%] [G loss: 1.943605]\n",
      "epoch:27 step:26152 [D loss: 0.667722, acc: 55.47%] [G loss: 1.821709]\n",
      "epoch:27 step:26153 [D loss: 0.682388, acc: 60.94%] [G loss: 1.811708]\n",
      "epoch:27 step:26154 [D loss: 0.659838, acc: 58.59%] [G loss: 1.740698]\n",
      "epoch:27 step:26155 [D loss: 0.679784, acc: 57.81%] [G loss: 1.751837]\n",
      "epoch:27 step:26156 [D loss: 0.650193, acc: 58.59%] [G loss: 1.852665]\n",
      "epoch:27 step:26157 [D loss: 0.722559, acc: 53.91%] [G loss: 1.772329]\n",
      "epoch:27 step:26158 [D loss: 0.696411, acc: 54.69%] [G loss: 1.716406]\n",
      "epoch:27 step:26159 [D loss: 0.667691, acc: 61.72%] [G loss: 1.852774]\n",
      "epoch:27 step:26160 [D loss: 0.685675, acc: 57.03%] [G loss: 1.813441]\n",
      "epoch:27 step:26161 [D loss: 0.688417, acc: 60.94%] [G loss: 1.798158]\n",
      "epoch:27 step:26162 [D loss: 0.630700, acc: 64.84%] [G loss: 1.923484]\n",
      "epoch:27 step:26163 [D loss: 0.643499, acc: 67.19%] [G loss: 1.828497]\n",
      "epoch:27 step:26164 [D loss: 0.661253, acc: 62.50%] [G loss: 1.700435]\n",
      "epoch:27 step:26165 [D loss: 0.667848, acc: 64.06%] [G loss: 1.884544]\n",
      "epoch:27 step:26166 [D loss: 0.677417, acc: 57.03%] [G loss: 1.765729]\n",
      "epoch:27 step:26167 [D loss: 0.662903, acc: 58.59%] [G loss: 1.838781]\n",
      "epoch:27 step:26168 [D loss: 0.650698, acc: 61.72%] [G loss: 1.742385]\n",
      "epoch:27 step:26169 [D loss: 0.648489, acc: 61.72%] [G loss: 1.762353]\n",
      "epoch:27 step:26170 [D loss: 0.652703, acc: 65.62%] [G loss: 1.848428]\n",
      "epoch:27 step:26171 [D loss: 0.657468, acc: 60.94%] [G loss: 1.742018]\n",
      "epoch:27 step:26172 [D loss: 0.650724, acc: 57.81%] [G loss: 1.752656]\n",
      "epoch:27 step:26173 [D loss: 0.697807, acc: 53.91%] [G loss: 1.741535]\n",
      "epoch:27 step:26174 [D loss: 0.623029, acc: 65.62%] [G loss: 1.752980]\n",
      "epoch:27 step:26175 [D loss: 0.624797, acc: 64.84%] [G loss: 1.834340]\n",
      "epoch:27 step:26176 [D loss: 0.686901, acc: 60.16%] [G loss: 1.781660]\n",
      "epoch:27 step:26177 [D loss: 0.689077, acc: 56.25%] [G loss: 1.798873]\n",
      "epoch:27 step:26178 [D loss: 0.646914, acc: 63.28%] [G loss: 1.806772]\n",
      "epoch:27 step:26179 [D loss: 0.656325, acc: 62.50%] [G loss: 1.784575]\n",
      "epoch:27 step:26180 [D loss: 0.588410, acc: 67.19%] [G loss: 1.796519]\n",
      "epoch:27 step:26181 [D loss: 0.631000, acc: 67.97%] [G loss: 1.843048]\n",
      "epoch:27 step:26182 [D loss: 0.653627, acc: 61.72%] [G loss: 1.788634]\n",
      "epoch:27 step:26183 [D loss: 0.600421, acc: 67.19%] [G loss: 1.837914]\n",
      "epoch:27 step:26184 [D loss: 0.692083, acc: 53.91%] [G loss: 1.811795]\n",
      "epoch:27 step:26185 [D loss: 0.682149, acc: 60.94%] [G loss: 1.914566]\n",
      "epoch:27 step:26186 [D loss: 0.652097, acc: 64.06%] [G loss: 1.799168]\n",
      "epoch:27 step:26187 [D loss: 0.655302, acc: 60.94%] [G loss: 1.795042]\n",
      "epoch:27 step:26188 [D loss: 0.626068, acc: 62.50%] [G loss: 1.912869]\n",
      "epoch:27 step:26189 [D loss: 0.675220, acc: 54.69%] [G loss: 1.954431]\n",
      "epoch:27 step:26190 [D loss: 0.682067, acc: 60.94%] [G loss: 1.847541]\n",
      "epoch:27 step:26191 [D loss: 0.691323, acc: 56.25%] [G loss: 1.815409]\n",
      "epoch:27 step:26192 [D loss: 0.654693, acc: 64.84%] [G loss: 1.951141]\n",
      "epoch:27 step:26193 [D loss: 0.609273, acc: 67.97%] [G loss: 1.828532]\n",
      "epoch:27 step:26194 [D loss: 0.587131, acc: 71.09%] [G loss: 1.976126]\n",
      "epoch:27 step:26195 [D loss: 0.645291, acc: 62.50%] [G loss: 1.807399]\n",
      "epoch:27 step:26196 [D loss: 0.624783, acc: 62.50%] [G loss: 1.924443]\n",
      "epoch:27 step:26197 [D loss: 0.700238, acc: 63.28%] [G loss: 1.745147]\n",
      "epoch:27 step:26198 [D loss: 0.619698, acc: 60.16%] [G loss: 1.978103]\n",
      "epoch:27 step:26199 [D loss: 0.625975, acc: 64.06%] [G loss: 1.927262]\n",
      "epoch:27 step:26200 [D loss: 0.577001, acc: 72.66%] [G loss: 2.124188]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.683664\n",
      "FID: 9.084289\n",
      "0 = 12.724164321756353\n",
      "1 = 0.09221666812130294\n",
      "2 = 0.878600001335144\n",
      "3 = 0.8998000025749207\n",
      "4 = 0.8574000000953674\n",
      "5 = 0.863200306892395\n",
      "6 = 0.8998000025749207\n",
      "7 = 5.969715844726561\n",
      "8 = 0.05702656765881419\n",
      "9 = 0.7056999802589417\n",
      "10 = 0.7218000292778015\n",
      "11 = 0.6895999908447266\n",
      "12 = 0.6992830634117126\n",
      "13 = 0.7218000292778015\n",
      "14 = 7.683684825897217\n",
      "15 = 9.484933853149414\n",
      "16 = 0.08874665945768356\n",
      "17 = 7.683664321899414\n",
      "18 = 9.084288597106934\n",
      "epoch:27 step:26201 [D loss: 0.650322, acc: 65.62%] [G loss: 1.864680]\n",
      "epoch:27 step:26202 [D loss: 0.633711, acc: 67.19%] [G loss: 1.939669]\n",
      "epoch:27 step:26203 [D loss: 0.676009, acc: 62.50%] [G loss: 1.936203]\n",
      "epoch:27 step:26204 [D loss: 0.700779, acc: 57.03%] [G loss: 2.032746]\n",
      "epoch:27 step:26205 [D loss: 0.673080, acc: 59.38%] [G loss: 2.034193]\n",
      "epoch:27 step:26206 [D loss: 0.606448, acc: 67.97%] [G loss: 1.892896]\n",
      "epoch:27 step:26207 [D loss: 0.659075, acc: 65.62%] [G loss: 1.923166]\n",
      "epoch:27 step:26208 [D loss: 0.596579, acc: 69.53%] [G loss: 2.005900]\n",
      "epoch:27 step:26209 [D loss: 0.613312, acc: 64.84%] [G loss: 1.771675]\n",
      "epoch:27 step:26210 [D loss: 0.632505, acc: 67.97%] [G loss: 1.904496]\n",
      "epoch:27 step:26211 [D loss: 0.664468, acc: 61.72%] [G loss: 2.089357]\n",
      "epoch:27 step:26212 [D loss: 0.653207, acc: 66.41%] [G loss: 1.851870]\n",
      "epoch:27 step:26213 [D loss: 0.713838, acc: 60.16%] [G loss: 1.960177]\n",
      "epoch:27 step:26214 [D loss: 0.718438, acc: 56.25%] [G loss: 1.810781]\n",
      "epoch:27 step:26215 [D loss: 0.652293, acc: 61.72%] [G loss: 1.933287]\n",
      "epoch:27 step:26216 [D loss: 0.648228, acc: 59.38%] [G loss: 1.945160]\n",
      "epoch:27 step:26217 [D loss: 0.593196, acc: 69.53%] [G loss: 2.056827]\n",
      "epoch:27 step:26218 [D loss: 0.579406, acc: 69.53%] [G loss: 2.202706]\n",
      "epoch:27 step:26219 [D loss: 0.692088, acc: 54.69%] [G loss: 1.853516]\n",
      "epoch:27 step:26220 [D loss: 0.657135, acc: 64.06%] [G loss: 1.961663]\n",
      "epoch:27 step:26221 [D loss: 0.644227, acc: 66.41%] [G loss: 1.880853]\n",
      "epoch:27 step:26222 [D loss: 0.672060, acc: 64.06%] [G loss: 1.889256]\n",
      "epoch:27 step:26223 [D loss: 0.579190, acc: 67.19%] [G loss: 2.073722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:26224 [D loss: 0.615902, acc: 67.19%] [G loss: 2.128335]\n",
      "epoch:27 step:26225 [D loss: 0.613587, acc: 58.59%] [G loss: 2.112033]\n",
      "epoch:27 step:26226 [D loss: 0.659250, acc: 60.94%] [G loss: 1.996137]\n",
      "epoch:27 step:26227 [D loss: 0.810766, acc: 43.75%] [G loss: 1.621149]\n",
      "epoch:27 step:26228 [D loss: 0.758568, acc: 47.66%] [G loss: 1.871977]\n",
      "epoch:27 step:26229 [D loss: 0.583240, acc: 70.31%] [G loss: 1.905834]\n",
      "epoch:27 step:26230 [D loss: 0.629861, acc: 67.97%] [G loss: 1.840740]\n",
      "epoch:27 step:26231 [D loss: 0.728233, acc: 53.91%] [G loss: 1.842699]\n",
      "epoch:27 step:26232 [D loss: 0.633334, acc: 66.41%] [G loss: 1.857675]\n",
      "epoch:27 step:26233 [D loss: 0.644992, acc: 60.94%] [G loss: 1.882841]\n",
      "epoch:27 step:26234 [D loss: 0.616763, acc: 64.06%] [G loss: 1.837114]\n",
      "epoch:27 step:26235 [D loss: 0.561013, acc: 68.75%] [G loss: 2.097493]\n",
      "epoch:27 step:26236 [D loss: 0.618181, acc: 67.19%] [G loss: 2.299114]\n",
      "epoch:28 step:26237 [D loss: 0.600846, acc: 64.84%] [G loss: 1.855483]\n",
      "epoch:28 step:26238 [D loss: 0.641525, acc: 60.94%] [G loss: 1.979615]\n",
      "epoch:28 step:26239 [D loss: 0.646692, acc: 61.72%] [G loss: 1.948657]\n",
      "epoch:28 step:26240 [D loss: 0.638762, acc: 61.72%] [G loss: 1.880216]\n",
      "epoch:28 step:26241 [D loss: 0.614219, acc: 64.06%] [G loss: 1.891109]\n",
      "epoch:28 step:26242 [D loss: 0.652083, acc: 61.72%] [G loss: 2.017510]\n",
      "epoch:28 step:26243 [D loss: 0.596340, acc: 69.53%] [G loss: 2.052494]\n",
      "epoch:28 step:26244 [D loss: 0.682139, acc: 61.72%] [G loss: 1.891761]\n",
      "epoch:28 step:26245 [D loss: 0.613109, acc: 64.06%] [G loss: 1.970040]\n",
      "epoch:28 step:26246 [D loss: 0.633697, acc: 59.38%] [G loss: 2.023474]\n",
      "epoch:28 step:26247 [D loss: 0.666790, acc: 64.84%] [G loss: 1.994425]\n",
      "epoch:28 step:26248 [D loss: 0.651999, acc: 62.50%] [G loss: 1.941249]\n",
      "epoch:28 step:26249 [D loss: 0.628714, acc: 64.06%] [G loss: 2.043947]\n",
      "epoch:28 step:26250 [D loss: 0.703454, acc: 55.47%] [G loss: 1.937393]\n",
      "epoch:28 step:26251 [D loss: 0.631076, acc: 65.62%] [G loss: 1.977320]\n",
      "epoch:28 step:26252 [D loss: 0.635728, acc: 66.41%] [G loss: 2.031285]\n",
      "epoch:28 step:26253 [D loss: 0.610676, acc: 67.97%] [G loss: 1.850907]\n",
      "epoch:28 step:26254 [D loss: 0.683178, acc: 51.56%] [G loss: 1.801651]\n",
      "epoch:28 step:26255 [D loss: 0.710435, acc: 55.47%] [G loss: 1.756137]\n",
      "epoch:28 step:26256 [D loss: 0.731819, acc: 52.34%] [G loss: 1.655259]\n",
      "epoch:28 step:26257 [D loss: 0.680716, acc: 57.81%] [G loss: 1.819501]\n",
      "epoch:28 step:26258 [D loss: 0.662258, acc: 58.59%] [G loss: 1.708194]\n",
      "epoch:28 step:26259 [D loss: 0.657406, acc: 64.06%] [G loss: 1.886448]\n",
      "epoch:28 step:26260 [D loss: 0.626173, acc: 70.31%] [G loss: 1.831591]\n",
      "epoch:28 step:26261 [D loss: 0.622097, acc: 67.19%] [G loss: 2.093161]\n",
      "epoch:28 step:26262 [D loss: 0.676209, acc: 55.47%] [G loss: 1.743621]\n",
      "epoch:28 step:26263 [D loss: 0.651703, acc: 56.25%] [G loss: 1.745104]\n",
      "epoch:28 step:26264 [D loss: 0.633178, acc: 64.84%] [G loss: 1.787965]\n",
      "epoch:28 step:26265 [D loss: 0.637285, acc: 64.84%] [G loss: 1.722817]\n",
      "epoch:28 step:26266 [D loss: 0.643575, acc: 64.84%] [G loss: 1.830550]\n",
      "epoch:28 step:26267 [D loss: 0.658145, acc: 61.72%] [G loss: 1.735457]\n",
      "epoch:28 step:26268 [D loss: 0.700636, acc: 59.38%] [G loss: 1.811464]\n",
      "epoch:28 step:26269 [D loss: 0.675634, acc: 56.25%] [G loss: 1.913841]\n",
      "epoch:28 step:26270 [D loss: 0.627601, acc: 60.94%] [G loss: 1.890263]\n",
      "epoch:28 step:26271 [D loss: 0.654597, acc: 61.72%] [G loss: 1.795758]\n",
      "epoch:28 step:26272 [D loss: 0.639563, acc: 67.19%] [G loss: 1.901301]\n",
      "epoch:28 step:26273 [D loss: 0.661487, acc: 64.84%] [G loss: 1.822059]\n",
      "epoch:28 step:26274 [D loss: 0.673258, acc: 63.28%] [G loss: 1.856455]\n",
      "epoch:28 step:26275 [D loss: 0.666916, acc: 58.59%] [G loss: 1.882236]\n",
      "epoch:28 step:26276 [D loss: 0.646781, acc: 58.59%] [G loss: 1.945146]\n",
      "epoch:28 step:26277 [D loss: 0.676691, acc: 53.91%] [G loss: 1.802511]\n",
      "epoch:28 step:26278 [D loss: 0.658072, acc: 60.16%] [G loss: 1.995608]\n",
      "epoch:28 step:26279 [D loss: 0.694556, acc: 53.91%] [G loss: 1.776997]\n",
      "epoch:28 step:26280 [D loss: 0.666955, acc: 58.59%] [G loss: 1.971885]\n",
      "epoch:28 step:26281 [D loss: 0.668095, acc: 60.94%] [G loss: 1.938657]\n",
      "epoch:28 step:26282 [D loss: 0.651363, acc: 57.81%] [G loss: 1.689994]\n",
      "epoch:28 step:26283 [D loss: 0.615975, acc: 71.88%] [G loss: 1.795197]\n",
      "epoch:28 step:26284 [D loss: 0.652981, acc: 60.94%] [G loss: 1.885139]\n",
      "epoch:28 step:26285 [D loss: 0.618210, acc: 66.41%] [G loss: 1.872939]\n",
      "epoch:28 step:26286 [D loss: 0.620814, acc: 63.28%] [G loss: 1.904538]\n",
      "epoch:28 step:26287 [D loss: 0.662643, acc: 62.50%] [G loss: 1.918461]\n",
      "epoch:28 step:26288 [D loss: 0.653752, acc: 62.50%] [G loss: 1.852501]\n",
      "epoch:28 step:26289 [D loss: 0.650080, acc: 57.81%] [G loss: 1.958899]\n",
      "epoch:28 step:26290 [D loss: 0.613955, acc: 63.28%] [G loss: 1.820119]\n",
      "epoch:28 step:26291 [D loss: 0.602954, acc: 65.62%] [G loss: 1.845554]\n",
      "epoch:28 step:26292 [D loss: 0.617307, acc: 63.28%] [G loss: 1.851574]\n",
      "epoch:28 step:26293 [D loss: 0.676075, acc: 61.72%] [G loss: 1.788999]\n",
      "epoch:28 step:26294 [D loss: 0.622877, acc: 67.19%] [G loss: 1.877695]\n",
      "epoch:28 step:26295 [D loss: 0.685826, acc: 58.59%] [G loss: 1.732086]\n",
      "epoch:28 step:26296 [D loss: 0.692679, acc: 50.78%] [G loss: 1.844583]\n",
      "epoch:28 step:26297 [D loss: 0.682396, acc: 64.06%] [G loss: 1.742646]\n",
      "epoch:28 step:26298 [D loss: 0.648502, acc: 63.28%] [G loss: 1.842444]\n",
      "epoch:28 step:26299 [D loss: 0.692498, acc: 58.59%] [G loss: 1.838906]\n",
      "epoch:28 step:26300 [D loss: 0.678209, acc: 56.25%] [G loss: 1.935240]\n",
      "epoch:28 step:26301 [D loss: 0.674857, acc: 59.38%] [G loss: 1.866692]\n",
      "epoch:28 step:26302 [D loss: 0.627988, acc: 64.84%] [G loss: 1.698549]\n",
      "epoch:28 step:26303 [D loss: 0.634674, acc: 62.50%] [G loss: 1.805541]\n",
      "epoch:28 step:26304 [D loss: 0.634106, acc: 67.97%] [G loss: 1.864369]\n",
      "epoch:28 step:26305 [D loss: 0.597387, acc: 64.84%] [G loss: 2.090964]\n",
      "epoch:28 step:26306 [D loss: 0.639692, acc: 57.81%] [G loss: 1.864873]\n",
      "epoch:28 step:26307 [D loss: 0.667073, acc: 63.28%] [G loss: 1.821446]\n",
      "epoch:28 step:26308 [D loss: 0.657628, acc: 58.59%] [G loss: 1.926332]\n",
      "epoch:28 step:26309 [D loss: 0.582568, acc: 71.88%] [G loss: 1.841088]\n",
      "epoch:28 step:26310 [D loss: 0.706612, acc: 56.25%] [G loss: 1.863870]\n",
      "epoch:28 step:26311 [D loss: 0.644987, acc: 60.94%] [G loss: 1.972637]\n",
      "epoch:28 step:26312 [D loss: 0.679662, acc: 55.47%] [G loss: 1.824522]\n",
      "epoch:28 step:26313 [D loss: 0.597962, acc: 69.53%] [G loss: 1.848334]\n",
      "epoch:28 step:26314 [D loss: 0.638677, acc: 59.38%] [G loss: 1.878305]\n",
      "epoch:28 step:26315 [D loss: 0.657167, acc: 57.81%] [G loss: 1.741507]\n",
      "epoch:28 step:26316 [D loss: 0.620874, acc: 61.72%] [G loss: 1.716103]\n",
      "epoch:28 step:26317 [D loss: 0.718694, acc: 55.47%] [G loss: 1.674516]\n",
      "epoch:28 step:26318 [D loss: 0.654967, acc: 61.72%] [G loss: 1.644759]\n",
      "epoch:28 step:26319 [D loss: 0.666646, acc: 59.38%] [G loss: 1.871512]\n",
      "epoch:28 step:26320 [D loss: 0.609789, acc: 65.62%] [G loss: 1.825009]\n",
      "epoch:28 step:26321 [D loss: 0.657242, acc: 57.81%] [G loss: 1.700995]\n",
      "epoch:28 step:26322 [D loss: 0.672813, acc: 60.16%] [G loss: 1.829128]\n",
      "epoch:28 step:26323 [D loss: 0.659894, acc: 55.47%] [G loss: 1.792510]\n",
      "epoch:28 step:26324 [D loss: 0.634726, acc: 60.94%] [G loss: 1.960012]\n",
      "epoch:28 step:26325 [D loss: 0.645565, acc: 62.50%] [G loss: 1.798191]\n",
      "epoch:28 step:26326 [D loss: 0.672127, acc: 57.81%] [G loss: 1.754497]\n",
      "epoch:28 step:26327 [D loss: 0.656127, acc: 64.84%] [G loss: 1.765221]\n",
      "epoch:28 step:26328 [D loss: 0.630804, acc: 67.19%] [G loss: 1.855201]\n",
      "epoch:28 step:26329 [D loss: 0.611809, acc: 67.19%] [G loss: 1.930043]\n",
      "epoch:28 step:26330 [D loss: 0.635059, acc: 67.19%] [G loss: 1.833143]\n",
      "epoch:28 step:26331 [D loss: 0.674060, acc: 60.94%] [G loss: 1.813965]\n",
      "epoch:28 step:26332 [D loss: 0.636280, acc: 62.50%] [G loss: 1.789930]\n",
      "epoch:28 step:26333 [D loss: 0.629098, acc: 63.28%] [G loss: 1.807855]\n",
      "epoch:28 step:26334 [D loss: 0.663366, acc: 60.94%] [G loss: 1.824829]\n",
      "epoch:28 step:26335 [D loss: 0.722964, acc: 50.78%] [G loss: 1.751004]\n",
      "epoch:28 step:26336 [D loss: 0.647643, acc: 60.94%] [G loss: 1.900163]\n",
      "epoch:28 step:26337 [D loss: 0.662571, acc: 57.81%] [G loss: 1.834267]\n",
      "epoch:28 step:26338 [D loss: 0.674953, acc: 61.72%] [G loss: 1.785768]\n",
      "epoch:28 step:26339 [D loss: 0.695202, acc: 57.03%] [G loss: 1.865949]\n",
      "epoch:28 step:26340 [D loss: 0.647136, acc: 62.50%] [G loss: 1.860707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26341 [D loss: 0.674653, acc: 57.03%] [G loss: 1.847649]\n",
      "epoch:28 step:26342 [D loss: 0.650570, acc: 58.59%] [G loss: 1.882427]\n",
      "epoch:28 step:26343 [D loss: 0.623848, acc: 67.97%] [G loss: 2.059331]\n",
      "epoch:28 step:26344 [D loss: 0.694336, acc: 54.69%] [G loss: 1.823990]\n",
      "epoch:28 step:26345 [D loss: 0.682390, acc: 57.81%] [G loss: 1.650933]\n",
      "epoch:28 step:26346 [D loss: 0.655045, acc: 64.84%] [G loss: 1.896733]\n",
      "epoch:28 step:26347 [D loss: 0.637286, acc: 63.28%] [G loss: 1.782410]\n",
      "epoch:28 step:26348 [D loss: 0.635765, acc: 64.84%] [G loss: 1.911856]\n",
      "epoch:28 step:26349 [D loss: 0.654691, acc: 60.94%] [G loss: 2.001130]\n",
      "epoch:28 step:26350 [D loss: 0.625646, acc: 62.50%] [G loss: 1.941692]\n",
      "epoch:28 step:26351 [D loss: 0.598937, acc: 68.75%] [G loss: 2.086032]\n",
      "epoch:28 step:26352 [D loss: 0.635186, acc: 64.84%] [G loss: 1.972007]\n",
      "epoch:28 step:26353 [D loss: 0.614032, acc: 64.84%] [G loss: 2.158172]\n",
      "epoch:28 step:26354 [D loss: 0.628344, acc: 64.06%] [G loss: 1.953611]\n",
      "epoch:28 step:26355 [D loss: 0.539613, acc: 74.22%] [G loss: 2.275052]\n",
      "epoch:28 step:26356 [D loss: 0.687308, acc: 59.38%] [G loss: 1.934081]\n",
      "epoch:28 step:26357 [D loss: 0.706242, acc: 56.25%] [G loss: 1.822477]\n",
      "epoch:28 step:26358 [D loss: 0.581672, acc: 71.88%] [G loss: 2.044182]\n",
      "epoch:28 step:26359 [D loss: 0.704157, acc: 50.00%] [G loss: 1.833312]\n",
      "epoch:28 step:26360 [D loss: 0.733116, acc: 51.56%] [G loss: 1.817468]\n",
      "epoch:28 step:26361 [D loss: 0.684143, acc: 57.03%] [G loss: 1.703081]\n",
      "epoch:28 step:26362 [D loss: 0.593972, acc: 67.97%] [G loss: 1.969583]\n",
      "epoch:28 step:26363 [D loss: 0.662325, acc: 58.59%] [G loss: 1.750955]\n",
      "epoch:28 step:26364 [D loss: 0.676755, acc: 57.03%] [G loss: 1.776987]\n",
      "epoch:28 step:26365 [D loss: 0.628720, acc: 69.53%] [G loss: 1.743943]\n",
      "epoch:28 step:26366 [D loss: 0.635630, acc: 64.84%] [G loss: 1.785845]\n",
      "epoch:28 step:26367 [D loss: 0.598071, acc: 71.88%] [G loss: 1.963614]\n",
      "epoch:28 step:26368 [D loss: 0.625724, acc: 65.62%] [G loss: 1.988992]\n",
      "epoch:28 step:26369 [D loss: 0.693691, acc: 50.00%] [G loss: 1.664594]\n",
      "epoch:28 step:26370 [D loss: 0.688140, acc: 54.69%] [G loss: 1.721679]\n",
      "epoch:28 step:26371 [D loss: 0.686489, acc: 57.81%] [G loss: 1.740734]\n",
      "epoch:28 step:26372 [D loss: 0.659338, acc: 64.84%] [G loss: 1.750874]\n",
      "epoch:28 step:26373 [D loss: 0.713791, acc: 52.34%] [G loss: 1.651580]\n",
      "epoch:28 step:26374 [D loss: 0.636709, acc: 65.62%] [G loss: 1.827828]\n",
      "epoch:28 step:26375 [D loss: 0.652110, acc: 60.94%] [G loss: 1.773875]\n",
      "epoch:28 step:26376 [D loss: 0.699160, acc: 57.81%] [G loss: 1.755833]\n",
      "epoch:28 step:26377 [D loss: 0.686994, acc: 54.69%] [G loss: 1.739433]\n",
      "epoch:28 step:26378 [D loss: 0.662571, acc: 60.16%] [G loss: 1.842003]\n",
      "epoch:28 step:26379 [D loss: 0.695874, acc: 56.25%] [G loss: 1.662355]\n",
      "epoch:28 step:26380 [D loss: 0.678905, acc: 60.16%] [G loss: 1.763395]\n",
      "epoch:28 step:26381 [D loss: 0.661862, acc: 59.38%] [G loss: 1.792779]\n",
      "epoch:28 step:26382 [D loss: 0.596902, acc: 71.09%] [G loss: 1.989205]\n",
      "epoch:28 step:26383 [D loss: 0.646576, acc: 62.50%] [G loss: 1.859798]\n",
      "epoch:28 step:26384 [D loss: 0.671638, acc: 57.81%] [G loss: 1.785556]\n",
      "epoch:28 step:26385 [D loss: 0.628145, acc: 65.62%] [G loss: 1.991221]\n",
      "epoch:28 step:26386 [D loss: 0.671956, acc: 57.03%] [G loss: 1.966689]\n",
      "epoch:28 step:26387 [D loss: 0.650433, acc: 60.94%] [G loss: 1.776557]\n",
      "epoch:28 step:26388 [D loss: 0.627098, acc: 66.41%] [G loss: 1.814743]\n",
      "epoch:28 step:26389 [D loss: 0.645677, acc: 66.41%] [G loss: 1.788168]\n",
      "epoch:28 step:26390 [D loss: 0.669943, acc: 57.81%] [G loss: 1.908355]\n",
      "epoch:28 step:26391 [D loss: 0.631707, acc: 67.19%] [G loss: 1.843345]\n",
      "epoch:28 step:26392 [D loss: 0.630015, acc: 65.62%] [G loss: 1.921274]\n",
      "epoch:28 step:26393 [D loss: 0.650515, acc: 64.06%] [G loss: 1.751882]\n",
      "epoch:28 step:26394 [D loss: 0.664192, acc: 58.59%] [G loss: 1.699676]\n",
      "epoch:28 step:26395 [D loss: 0.637966, acc: 68.75%] [G loss: 1.825108]\n",
      "epoch:28 step:26396 [D loss: 0.689997, acc: 55.47%] [G loss: 1.709746]\n",
      "epoch:28 step:26397 [D loss: 0.649746, acc: 60.94%] [G loss: 1.769653]\n",
      "epoch:28 step:26398 [D loss: 0.600306, acc: 70.31%] [G loss: 1.798753]\n",
      "epoch:28 step:26399 [D loss: 0.607364, acc: 67.97%] [G loss: 1.935135]\n",
      "epoch:28 step:26400 [D loss: 0.659576, acc: 64.84%] [G loss: 1.844045]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.503511\n",
      "FID: 11.415671\n",
      "0 = 12.58360029048921\n",
      "1 = 0.08302791576417283\n",
      "2 = 0.8813999891281128\n",
      "3 = 0.8921999931335449\n",
      "4 = 0.8705999851226807\n",
      "5 = 0.8733359575271606\n",
      "6 = 0.8921999931335449\n",
      "7 = 6.167667711877816\n",
      "8 = 0.06609135183758709\n",
      "9 = 0.7114999890327454\n",
      "10 = 0.718999981880188\n",
      "11 = 0.7039999961853027\n",
      "12 = 0.7083743810653687\n",
      "13 = 0.718999981880188\n",
      "14 = 7.5035400390625\n",
      "15 = 9.317788124084473\n",
      "16 = 0.1445382833480835\n",
      "17 = 7.50351095199585\n",
      "18 = 11.415671348571777\n",
      "epoch:28 step:26401 [D loss: 0.601754, acc: 66.41%] [G loss: 1.775964]\n",
      "epoch:28 step:26402 [D loss: 0.638535, acc: 64.84%] [G loss: 1.833306]\n",
      "epoch:28 step:26403 [D loss: 0.668263, acc: 60.16%] [G loss: 1.816905]\n",
      "epoch:28 step:26404 [D loss: 0.643651, acc: 64.84%] [G loss: 1.813961]\n",
      "epoch:28 step:26405 [D loss: 0.632517, acc: 62.50%] [G loss: 1.898972]\n",
      "epoch:28 step:26406 [D loss: 0.638981, acc: 64.84%] [G loss: 1.853036]\n",
      "epoch:28 step:26407 [D loss: 0.632953, acc: 63.28%] [G loss: 1.796580]\n",
      "epoch:28 step:26408 [D loss: 0.586102, acc: 65.62%] [G loss: 1.751643]\n",
      "epoch:28 step:26409 [D loss: 0.629632, acc: 60.94%] [G loss: 1.962623]\n",
      "epoch:28 step:26410 [D loss: 0.679432, acc: 60.16%] [G loss: 1.827784]\n",
      "epoch:28 step:26411 [D loss: 0.669262, acc: 63.28%] [G loss: 1.801931]\n",
      "epoch:28 step:26412 [D loss: 0.656855, acc: 69.53%] [G loss: 1.796396]\n",
      "epoch:28 step:26413 [D loss: 0.652331, acc: 57.03%] [G loss: 1.739346]\n",
      "epoch:28 step:26414 [D loss: 0.680485, acc: 58.59%] [G loss: 1.715830]\n",
      "epoch:28 step:26415 [D loss: 0.722153, acc: 53.91%] [G loss: 1.763575]\n",
      "epoch:28 step:26416 [D loss: 0.626351, acc: 67.97%] [G loss: 1.711153]\n",
      "epoch:28 step:26417 [D loss: 0.705513, acc: 52.34%] [G loss: 1.797417]\n",
      "epoch:28 step:26418 [D loss: 0.645352, acc: 62.50%] [G loss: 1.805092]\n",
      "epoch:28 step:26419 [D loss: 0.621856, acc: 66.41%] [G loss: 1.738740]\n",
      "epoch:28 step:26420 [D loss: 0.658574, acc: 60.94%] [G loss: 1.836579]\n",
      "epoch:28 step:26421 [D loss: 0.632473, acc: 63.28%] [G loss: 1.846015]\n",
      "epoch:28 step:26422 [D loss: 0.649011, acc: 65.62%] [G loss: 1.831454]\n",
      "epoch:28 step:26423 [D loss: 0.706088, acc: 51.56%] [G loss: 1.833291]\n",
      "epoch:28 step:26424 [D loss: 0.669955, acc: 61.72%] [G loss: 1.831576]\n",
      "epoch:28 step:26425 [D loss: 0.640484, acc: 60.16%] [G loss: 1.792791]\n",
      "epoch:28 step:26426 [D loss: 0.614185, acc: 66.41%] [G loss: 1.870473]\n",
      "epoch:28 step:26427 [D loss: 0.637225, acc: 63.28%] [G loss: 1.849371]\n",
      "epoch:28 step:26428 [D loss: 0.638892, acc: 59.38%] [G loss: 1.999778]\n",
      "epoch:28 step:26429 [D loss: 0.636246, acc: 62.50%] [G loss: 1.874609]\n",
      "epoch:28 step:26430 [D loss: 0.621744, acc: 71.88%] [G loss: 1.997286]\n",
      "epoch:28 step:26431 [D loss: 0.613263, acc: 61.72%] [G loss: 2.046499]\n",
      "epoch:28 step:26432 [D loss: 0.636173, acc: 58.59%] [G loss: 1.952110]\n",
      "epoch:28 step:26433 [D loss: 0.683076, acc: 59.38%] [G loss: 2.126833]\n",
      "epoch:28 step:26434 [D loss: 0.671317, acc: 60.94%] [G loss: 2.052576]\n",
      "epoch:28 step:26435 [D loss: 0.643848, acc: 64.06%] [G loss: 1.765053]\n",
      "epoch:28 step:26436 [D loss: 0.669246, acc: 56.25%] [G loss: 1.817523]\n",
      "epoch:28 step:26437 [D loss: 0.659345, acc: 62.50%] [G loss: 1.710986]\n",
      "epoch:28 step:26438 [D loss: 0.734260, acc: 48.44%] [G loss: 1.893546]\n",
      "epoch:28 step:26439 [D loss: 0.691572, acc: 50.78%] [G loss: 1.907970]\n",
      "epoch:28 step:26440 [D loss: 0.676203, acc: 57.81%] [G loss: 1.711960]\n",
      "epoch:28 step:26441 [D loss: 0.666044, acc: 61.72%] [G loss: 1.768124]\n",
      "epoch:28 step:26442 [D loss: 0.595706, acc: 69.53%] [G loss: 1.938817]\n",
      "epoch:28 step:26443 [D loss: 0.605417, acc: 71.09%] [G loss: 1.867017]\n",
      "epoch:28 step:26444 [D loss: 0.579921, acc: 71.09%] [G loss: 2.179298]\n",
      "epoch:28 step:26445 [D loss: 0.603324, acc: 69.53%] [G loss: 2.113470]\n",
      "epoch:28 step:26446 [D loss: 0.693707, acc: 63.28%] [G loss: 1.757055]\n",
      "epoch:28 step:26447 [D loss: 0.694459, acc: 60.94%] [G loss: 1.611031]\n",
      "epoch:28 step:26448 [D loss: 0.689556, acc: 53.12%] [G loss: 1.794374]\n",
      "epoch:28 step:26449 [D loss: 0.647096, acc: 61.72%] [G loss: 1.745581]\n",
      "epoch:28 step:26450 [D loss: 0.720046, acc: 51.56%] [G loss: 1.849071]\n",
      "epoch:28 step:26451 [D loss: 0.692854, acc: 57.03%] [G loss: 1.767279]\n",
      "epoch:28 step:26452 [D loss: 0.623916, acc: 62.50%] [G loss: 1.895712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26453 [D loss: 0.586707, acc: 65.62%] [G loss: 1.835605]\n",
      "epoch:28 step:26454 [D loss: 0.618181, acc: 64.06%] [G loss: 1.975795]\n",
      "epoch:28 step:26455 [D loss: 0.607112, acc: 71.88%] [G loss: 1.952001]\n",
      "epoch:28 step:26456 [D loss: 0.694047, acc: 53.12%] [G loss: 1.852845]\n",
      "epoch:28 step:26457 [D loss: 0.748320, acc: 49.22%] [G loss: 1.956829]\n",
      "epoch:28 step:26458 [D loss: 0.615925, acc: 68.75%] [G loss: 1.876930]\n",
      "epoch:28 step:26459 [D loss: 0.663534, acc: 63.28%] [G loss: 1.881902]\n",
      "epoch:28 step:26460 [D loss: 0.702303, acc: 55.47%] [G loss: 1.660114]\n",
      "epoch:28 step:26461 [D loss: 0.629934, acc: 65.62%] [G loss: 1.872616]\n",
      "epoch:28 step:26462 [D loss: 0.718813, acc: 53.91%] [G loss: 1.769314]\n",
      "epoch:28 step:26463 [D loss: 0.648784, acc: 60.16%] [G loss: 1.737511]\n",
      "epoch:28 step:26464 [D loss: 0.734731, acc: 54.69%] [G loss: 1.690923]\n",
      "epoch:28 step:26465 [D loss: 0.640454, acc: 65.62%] [G loss: 1.911577]\n",
      "epoch:28 step:26466 [D loss: 0.630324, acc: 62.50%] [G loss: 1.941087]\n",
      "epoch:28 step:26467 [D loss: 0.548365, acc: 77.34%] [G loss: 2.180946]\n",
      "epoch:28 step:26468 [D loss: 0.603140, acc: 64.84%] [G loss: 2.304407]\n",
      "epoch:28 step:26469 [D loss: 0.714456, acc: 57.03%] [G loss: 1.733209]\n",
      "epoch:28 step:26470 [D loss: 0.693003, acc: 58.59%] [G loss: 1.831319]\n",
      "epoch:28 step:26471 [D loss: 0.670788, acc: 59.38%] [G loss: 1.708125]\n",
      "epoch:28 step:26472 [D loss: 0.651485, acc: 64.84%] [G loss: 1.817162]\n",
      "epoch:28 step:26473 [D loss: 0.588042, acc: 74.22%] [G loss: 1.970845]\n",
      "epoch:28 step:26474 [D loss: 0.638602, acc: 65.62%] [G loss: 1.871191]\n",
      "epoch:28 step:26475 [D loss: 0.628536, acc: 66.41%] [G loss: 1.757876]\n",
      "epoch:28 step:26476 [D loss: 0.674443, acc: 59.38%] [G loss: 1.889925]\n",
      "epoch:28 step:26477 [D loss: 0.611974, acc: 67.19%] [G loss: 1.920795]\n",
      "epoch:28 step:26478 [D loss: 0.605246, acc: 67.97%] [G loss: 2.039332]\n",
      "epoch:28 step:26479 [D loss: 0.633258, acc: 61.72%] [G loss: 1.740872]\n",
      "epoch:28 step:26480 [D loss: 0.635473, acc: 57.81%] [G loss: 1.940450]\n",
      "epoch:28 step:26481 [D loss: 0.607818, acc: 67.97%] [G loss: 1.967309]\n",
      "epoch:28 step:26482 [D loss: 0.736408, acc: 50.00%] [G loss: 1.802716]\n",
      "epoch:28 step:26483 [D loss: 0.669161, acc: 61.72%] [G loss: 1.913398]\n",
      "epoch:28 step:26484 [D loss: 0.640805, acc: 63.28%] [G loss: 1.849363]\n",
      "epoch:28 step:26485 [D loss: 0.684655, acc: 55.47%] [G loss: 1.729875]\n",
      "epoch:28 step:26486 [D loss: 0.684725, acc: 59.38%] [G loss: 1.698301]\n",
      "epoch:28 step:26487 [D loss: 0.718414, acc: 55.47%] [G loss: 1.696302]\n",
      "epoch:28 step:26488 [D loss: 0.668002, acc: 55.47%] [G loss: 1.711686]\n",
      "epoch:28 step:26489 [D loss: 0.617854, acc: 63.28%] [G loss: 1.784310]\n",
      "epoch:28 step:26490 [D loss: 0.672660, acc: 60.16%] [G loss: 1.719261]\n",
      "epoch:28 step:26491 [D loss: 0.635810, acc: 60.16%] [G loss: 1.696366]\n",
      "epoch:28 step:26492 [D loss: 0.669500, acc: 61.72%] [G loss: 1.893563]\n",
      "epoch:28 step:26493 [D loss: 0.678671, acc: 59.38%] [G loss: 1.753712]\n",
      "epoch:28 step:26494 [D loss: 0.662832, acc: 61.72%] [G loss: 1.922371]\n",
      "epoch:28 step:26495 [D loss: 0.628576, acc: 64.84%] [G loss: 1.818219]\n",
      "epoch:28 step:26496 [D loss: 0.672323, acc: 58.59%] [G loss: 1.878743]\n",
      "epoch:28 step:26497 [D loss: 0.630237, acc: 64.84%] [G loss: 1.881862]\n",
      "epoch:28 step:26498 [D loss: 0.643851, acc: 61.72%] [G loss: 1.899764]\n",
      "epoch:28 step:26499 [D loss: 0.624417, acc: 65.62%] [G loss: 1.836915]\n",
      "epoch:28 step:26500 [D loss: 0.657068, acc: 61.72%] [G loss: 1.961859]\n",
      "epoch:28 step:26501 [D loss: 0.648994, acc: 63.28%] [G loss: 1.858961]\n",
      "epoch:28 step:26502 [D loss: 0.654498, acc: 64.06%] [G loss: 1.882514]\n",
      "epoch:28 step:26503 [D loss: 0.656475, acc: 62.50%] [G loss: 1.848573]\n",
      "epoch:28 step:26504 [D loss: 0.644629, acc: 65.62%] [G loss: 1.873034]\n",
      "epoch:28 step:26505 [D loss: 0.624198, acc: 70.31%] [G loss: 1.986435]\n",
      "epoch:28 step:26506 [D loss: 0.616726, acc: 67.19%] [G loss: 2.013367]\n",
      "epoch:28 step:26507 [D loss: 0.612238, acc: 70.31%] [G loss: 1.965158]\n",
      "epoch:28 step:26508 [D loss: 0.584598, acc: 67.97%] [G loss: 1.989917]\n",
      "epoch:28 step:26509 [D loss: 0.639290, acc: 60.94%] [G loss: 1.778990]\n",
      "epoch:28 step:26510 [D loss: 0.657480, acc: 59.38%] [G loss: 1.954110]\n",
      "epoch:28 step:26511 [D loss: 0.581259, acc: 65.62%] [G loss: 2.019299]\n",
      "epoch:28 step:26512 [D loss: 0.611860, acc: 70.31%] [G loss: 2.064456]\n",
      "epoch:28 step:26513 [D loss: 0.631938, acc: 64.84%] [G loss: 1.785257]\n",
      "epoch:28 step:26514 [D loss: 0.711001, acc: 55.47%] [G loss: 1.861004]\n",
      "epoch:28 step:26515 [D loss: 0.654486, acc: 60.16%] [G loss: 1.805896]\n",
      "epoch:28 step:26516 [D loss: 0.602437, acc: 73.44%] [G loss: 1.909105]\n",
      "epoch:28 step:26517 [D loss: 0.651524, acc: 60.16%] [G loss: 1.908227]\n",
      "epoch:28 step:26518 [D loss: 0.673147, acc: 60.94%] [G loss: 1.804510]\n",
      "epoch:28 step:26519 [D loss: 0.610815, acc: 70.31%] [G loss: 1.971447]\n",
      "epoch:28 step:26520 [D loss: 0.628131, acc: 64.06%] [G loss: 1.933551]\n",
      "epoch:28 step:26521 [D loss: 0.636900, acc: 60.16%] [G loss: 1.847644]\n",
      "epoch:28 step:26522 [D loss: 0.671421, acc: 58.59%] [G loss: 1.909292]\n",
      "epoch:28 step:26523 [D loss: 0.724554, acc: 55.47%] [G loss: 1.778996]\n",
      "epoch:28 step:26524 [D loss: 0.637227, acc: 62.50%] [G loss: 1.869908]\n",
      "epoch:28 step:26525 [D loss: 0.666861, acc: 57.81%] [G loss: 1.936130]\n",
      "epoch:28 step:26526 [D loss: 0.629945, acc: 67.97%] [G loss: 1.831985]\n",
      "epoch:28 step:26527 [D loss: 0.632985, acc: 63.28%] [G loss: 1.881218]\n",
      "epoch:28 step:26528 [D loss: 0.629631, acc: 60.94%] [G loss: 1.855996]\n",
      "epoch:28 step:26529 [D loss: 0.629731, acc: 60.16%] [G loss: 1.805578]\n",
      "epoch:28 step:26530 [D loss: 0.608647, acc: 67.97%] [G loss: 1.739814]\n",
      "epoch:28 step:26531 [D loss: 0.653981, acc: 60.94%] [G loss: 1.882895]\n",
      "epoch:28 step:26532 [D loss: 0.673799, acc: 56.25%] [G loss: 1.871268]\n",
      "epoch:28 step:26533 [D loss: 0.674016, acc: 59.38%] [G loss: 1.754233]\n",
      "epoch:28 step:26534 [D loss: 0.642783, acc: 64.06%] [G loss: 1.905844]\n",
      "epoch:28 step:26535 [D loss: 0.638461, acc: 60.94%] [G loss: 1.932788]\n",
      "epoch:28 step:26536 [D loss: 0.602167, acc: 65.62%] [G loss: 1.857160]\n",
      "epoch:28 step:26537 [D loss: 0.671208, acc: 60.16%] [G loss: 1.845591]\n",
      "epoch:28 step:26538 [D loss: 0.644625, acc: 62.50%] [G loss: 1.822163]\n",
      "epoch:28 step:26539 [D loss: 0.622983, acc: 65.62%] [G loss: 1.829376]\n",
      "epoch:28 step:26540 [D loss: 0.720608, acc: 56.25%] [G loss: 1.832976]\n",
      "epoch:28 step:26541 [D loss: 0.692916, acc: 58.59%] [G loss: 1.818797]\n",
      "epoch:28 step:26542 [D loss: 0.674372, acc: 59.38%] [G loss: 1.786915]\n",
      "epoch:28 step:26543 [D loss: 0.618913, acc: 65.62%] [G loss: 1.747104]\n",
      "epoch:28 step:26544 [D loss: 0.681281, acc: 56.25%] [G loss: 1.788386]\n",
      "epoch:28 step:26545 [D loss: 0.631913, acc: 63.28%] [G loss: 1.878841]\n",
      "epoch:28 step:26546 [D loss: 0.639205, acc: 64.84%] [G loss: 1.809793]\n",
      "epoch:28 step:26547 [D loss: 0.660772, acc: 60.94%] [G loss: 1.880175]\n",
      "epoch:28 step:26548 [D loss: 0.658861, acc: 60.94%] [G loss: 2.001793]\n",
      "epoch:28 step:26549 [D loss: 0.630069, acc: 64.84%] [G loss: 1.958236]\n",
      "epoch:28 step:26550 [D loss: 0.644053, acc: 57.81%] [G loss: 2.148761]\n",
      "epoch:28 step:26551 [D loss: 0.641632, acc: 64.06%] [G loss: 1.986749]\n",
      "epoch:28 step:26552 [D loss: 0.702940, acc: 57.81%] [G loss: 1.731679]\n",
      "epoch:28 step:26553 [D loss: 0.691009, acc: 57.81%] [G loss: 1.781381]\n",
      "epoch:28 step:26554 [D loss: 0.649666, acc: 56.25%] [G loss: 1.850241]\n",
      "epoch:28 step:26555 [D loss: 0.666280, acc: 58.59%] [G loss: 1.729605]\n",
      "epoch:28 step:26556 [D loss: 0.670323, acc: 60.94%] [G loss: 1.801001]\n",
      "epoch:28 step:26557 [D loss: 0.650559, acc: 64.84%] [G loss: 1.808551]\n",
      "epoch:28 step:26558 [D loss: 0.687760, acc: 54.69%] [G loss: 1.858137]\n",
      "epoch:28 step:26559 [D loss: 0.691129, acc: 58.59%] [G loss: 1.858266]\n",
      "epoch:28 step:26560 [D loss: 0.665674, acc: 57.81%] [G loss: 1.816292]\n",
      "epoch:28 step:26561 [D loss: 0.679676, acc: 54.69%] [G loss: 1.785700]\n",
      "epoch:28 step:26562 [D loss: 0.639315, acc: 65.62%] [G loss: 1.801208]\n",
      "epoch:28 step:26563 [D loss: 0.668887, acc: 58.59%] [G loss: 1.641410]\n",
      "epoch:28 step:26564 [D loss: 0.633734, acc: 63.28%] [G loss: 1.795576]\n",
      "epoch:28 step:26565 [D loss: 0.624415, acc: 64.84%] [G loss: 1.882732]\n",
      "epoch:28 step:26566 [D loss: 0.643727, acc: 64.06%] [G loss: 1.808207]\n",
      "epoch:28 step:26567 [D loss: 0.633030, acc: 67.97%] [G loss: 1.857097]\n",
      "epoch:28 step:26568 [D loss: 0.685572, acc: 54.69%] [G loss: 1.866432]\n",
      "epoch:28 step:26569 [D loss: 0.640123, acc: 60.16%] [G loss: 1.816715]\n",
      "epoch:28 step:26570 [D loss: 0.670410, acc: 57.03%] [G loss: 1.889639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26571 [D loss: 0.603806, acc: 64.06%] [G loss: 1.894287]\n",
      "epoch:28 step:26572 [D loss: 0.662768, acc: 62.50%] [G loss: 1.830752]\n",
      "epoch:28 step:26573 [D loss: 0.663953, acc: 59.38%] [G loss: 1.946306]\n",
      "epoch:28 step:26574 [D loss: 0.635375, acc: 60.94%] [G loss: 2.050343]\n",
      "epoch:28 step:26575 [D loss: 0.572440, acc: 75.00%] [G loss: 1.987758]\n",
      "epoch:28 step:26576 [D loss: 0.665569, acc: 62.50%] [G loss: 1.912258]\n",
      "epoch:28 step:26577 [D loss: 0.639895, acc: 64.06%] [G loss: 1.846645]\n",
      "epoch:28 step:26578 [D loss: 0.681376, acc: 54.69%] [G loss: 1.784781]\n",
      "epoch:28 step:26579 [D loss: 0.681197, acc: 61.72%] [G loss: 1.964401]\n",
      "epoch:28 step:26580 [D loss: 0.637114, acc: 60.94%] [G loss: 1.753884]\n",
      "epoch:28 step:26581 [D loss: 0.559339, acc: 72.66%] [G loss: 2.012645]\n",
      "epoch:28 step:26582 [D loss: 0.597737, acc: 71.88%] [G loss: 2.163339]\n",
      "epoch:28 step:26583 [D loss: 0.567282, acc: 75.78%] [G loss: 2.285064]\n",
      "epoch:28 step:26584 [D loss: 0.647586, acc: 61.72%] [G loss: 1.895390]\n",
      "epoch:28 step:26585 [D loss: 0.670749, acc: 60.94%] [G loss: 1.670782]\n",
      "epoch:28 step:26586 [D loss: 0.593064, acc: 64.84%] [G loss: 1.760185]\n",
      "epoch:28 step:26587 [D loss: 0.685268, acc: 53.12%] [G loss: 1.899022]\n",
      "epoch:28 step:26588 [D loss: 0.676749, acc: 54.69%] [G loss: 1.901410]\n",
      "epoch:28 step:26589 [D loss: 0.615042, acc: 66.41%] [G loss: 1.874827]\n",
      "epoch:28 step:26590 [D loss: 0.684912, acc: 58.59%] [G loss: 1.959520]\n",
      "epoch:28 step:26591 [D loss: 0.663054, acc: 61.72%] [G loss: 1.819334]\n",
      "epoch:28 step:26592 [D loss: 0.679133, acc: 57.81%] [G loss: 1.869478]\n",
      "epoch:28 step:26593 [D loss: 0.627661, acc: 64.84%] [G loss: 1.891953]\n",
      "epoch:28 step:26594 [D loss: 0.611098, acc: 67.19%] [G loss: 1.904397]\n",
      "epoch:28 step:26595 [D loss: 0.601710, acc: 67.97%] [G loss: 1.996328]\n",
      "epoch:28 step:26596 [D loss: 0.596925, acc: 64.84%] [G loss: 2.053375]\n",
      "epoch:28 step:26597 [D loss: 0.618952, acc: 63.28%] [G loss: 1.873136]\n",
      "epoch:28 step:26598 [D loss: 0.669297, acc: 55.47%] [G loss: 1.900603]\n",
      "epoch:28 step:26599 [D loss: 0.653637, acc: 65.62%] [G loss: 2.006718]\n",
      "epoch:28 step:26600 [D loss: 0.597772, acc: 64.84%] [G loss: 1.951441]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.707782\n",
      "FID: 7.723529\n",
      "0 = 12.540811500263208\n",
      "1 = 0.08549524430104696\n",
      "2 = 0.8569999933242798\n",
      "3 = 0.8697999715805054\n",
      "4 = 0.8442000150680542\n",
      "5 = 0.8480889201164246\n",
      "6 = 0.8697999715805054\n",
      "7 = 5.883639592182623\n",
      "8 = 0.048843991562727396\n",
      "9 = 0.6920999884605408\n",
      "10 = 0.7117999792098999\n",
      "11 = 0.6723999977111816\n",
      "12 = 0.6848181486129761\n",
      "13 = 0.7117999792098999\n",
      "14 = 7.7078142166137695\n",
      "15 = 9.459193229675293\n",
      "16 = 0.10762369632720947\n",
      "17 = 7.70778226852417\n",
      "18 = 7.723528861999512\n",
      "epoch:28 step:26601 [D loss: 0.646304, acc: 67.97%] [G loss: 1.990436]\n",
      "epoch:28 step:26602 [D loss: 0.653162, acc: 63.28%] [G loss: 1.951642]\n",
      "epoch:28 step:26603 [D loss: 0.655154, acc: 60.94%] [G loss: 2.033819]\n",
      "epoch:28 step:26604 [D loss: 0.632476, acc: 59.38%] [G loss: 1.772013]\n",
      "epoch:28 step:26605 [D loss: 0.656628, acc: 58.59%] [G loss: 1.898734]\n",
      "epoch:28 step:26606 [D loss: 0.694392, acc: 54.69%] [G loss: 2.034940]\n",
      "epoch:28 step:26607 [D loss: 0.608321, acc: 63.28%] [G loss: 2.003162]\n",
      "epoch:28 step:26608 [D loss: 0.614785, acc: 61.72%] [G loss: 1.962166]\n",
      "epoch:28 step:26609 [D loss: 0.684465, acc: 56.25%] [G loss: 1.830723]\n",
      "epoch:28 step:26610 [D loss: 0.627121, acc: 67.97%] [G loss: 1.969791]\n",
      "epoch:28 step:26611 [D loss: 0.650202, acc: 60.16%] [G loss: 1.848013]\n",
      "epoch:28 step:26612 [D loss: 0.652622, acc: 61.72%] [G loss: 1.746582]\n",
      "epoch:28 step:26613 [D loss: 0.664878, acc: 59.38%] [G loss: 1.893358]\n",
      "epoch:28 step:26614 [D loss: 0.685043, acc: 57.03%] [G loss: 1.816215]\n",
      "epoch:28 step:26615 [D loss: 0.650276, acc: 57.81%] [G loss: 1.892290]\n",
      "epoch:28 step:26616 [D loss: 0.634738, acc: 66.41%] [G loss: 1.917764]\n",
      "epoch:28 step:26617 [D loss: 0.616826, acc: 65.62%] [G loss: 1.831988]\n",
      "epoch:28 step:26618 [D loss: 0.664779, acc: 64.84%] [G loss: 1.851375]\n",
      "epoch:28 step:26619 [D loss: 0.638833, acc: 64.84%] [G loss: 1.882136]\n",
      "epoch:28 step:26620 [D loss: 0.622818, acc: 64.84%] [G loss: 2.260775]\n",
      "epoch:28 step:26621 [D loss: 0.666356, acc: 59.38%] [G loss: 2.027958]\n",
      "epoch:28 step:26622 [D loss: 0.646702, acc: 65.62%] [G loss: 1.751069]\n",
      "epoch:28 step:26623 [D loss: 0.675347, acc: 60.94%] [G loss: 1.791271]\n",
      "epoch:28 step:26624 [D loss: 0.679245, acc: 60.16%] [G loss: 1.766462]\n",
      "epoch:28 step:26625 [D loss: 0.676408, acc: 61.72%] [G loss: 1.817345]\n",
      "epoch:28 step:26626 [D loss: 0.660931, acc: 60.16%] [G loss: 1.918282]\n",
      "epoch:28 step:26627 [D loss: 0.694414, acc: 62.50%] [G loss: 1.737033]\n",
      "epoch:28 step:26628 [D loss: 0.620391, acc: 64.84%] [G loss: 1.915382]\n",
      "epoch:28 step:26629 [D loss: 0.621368, acc: 62.50%] [G loss: 1.762544]\n",
      "epoch:28 step:26630 [D loss: 0.631000, acc: 62.50%] [G loss: 1.875165]\n",
      "epoch:28 step:26631 [D loss: 0.645685, acc: 64.06%] [G loss: 1.959343]\n",
      "epoch:28 step:26632 [D loss: 0.677880, acc: 59.38%] [G loss: 1.809348]\n",
      "epoch:28 step:26633 [D loss: 0.642051, acc: 60.94%] [G loss: 1.726697]\n",
      "epoch:28 step:26634 [D loss: 0.621563, acc: 67.19%] [G loss: 1.829149]\n",
      "epoch:28 step:26635 [D loss: 0.655549, acc: 61.72%] [G loss: 1.773189]\n",
      "epoch:28 step:26636 [D loss: 0.666526, acc: 57.03%] [G loss: 1.823975]\n",
      "epoch:28 step:26637 [D loss: 0.644603, acc: 63.28%] [G loss: 1.796622]\n",
      "epoch:28 step:26638 [D loss: 0.635943, acc: 69.53%] [G loss: 1.983240]\n",
      "epoch:28 step:26639 [D loss: 0.608788, acc: 64.84%] [G loss: 2.091961]\n",
      "epoch:28 step:26640 [D loss: 0.709141, acc: 55.47%] [G loss: 1.870228]\n",
      "epoch:28 step:26641 [D loss: 0.617069, acc: 63.28%] [G loss: 2.051127]\n",
      "epoch:28 step:26642 [D loss: 0.622849, acc: 68.75%] [G loss: 2.217632]\n",
      "epoch:28 step:26643 [D loss: 0.678640, acc: 57.81%] [G loss: 1.985792]\n",
      "epoch:28 step:26644 [D loss: 0.673344, acc: 57.81%] [G loss: 1.818469]\n",
      "epoch:28 step:26645 [D loss: 0.662977, acc: 55.47%] [G loss: 1.873165]\n",
      "epoch:28 step:26646 [D loss: 0.666614, acc: 60.16%] [G loss: 1.723939]\n",
      "epoch:28 step:26647 [D loss: 0.662539, acc: 60.94%] [G loss: 1.841818]\n",
      "epoch:28 step:26648 [D loss: 0.615692, acc: 67.97%] [G loss: 1.977243]\n",
      "epoch:28 step:26649 [D loss: 0.663636, acc: 61.72%] [G loss: 1.855853]\n",
      "epoch:28 step:26650 [D loss: 0.651548, acc: 58.59%] [G loss: 1.953492]\n",
      "epoch:28 step:26651 [D loss: 0.668350, acc: 59.38%] [G loss: 1.730364]\n",
      "epoch:28 step:26652 [D loss: 0.628189, acc: 68.75%] [G loss: 2.083013]\n",
      "epoch:28 step:26653 [D loss: 0.586683, acc: 65.62%] [G loss: 1.999267]\n",
      "epoch:28 step:26654 [D loss: 0.684744, acc: 64.06%] [G loss: 1.910905]\n",
      "epoch:28 step:26655 [D loss: 0.705446, acc: 56.25%] [G loss: 1.854197]\n",
      "epoch:28 step:26656 [D loss: 0.677800, acc: 56.25%] [G loss: 1.797802]\n",
      "epoch:28 step:26657 [D loss: 0.652720, acc: 61.72%] [G loss: 1.872051]\n",
      "epoch:28 step:26658 [D loss: 0.671272, acc: 60.94%] [G loss: 1.780157]\n",
      "epoch:28 step:26659 [D loss: 0.617818, acc: 65.62%] [G loss: 1.774043]\n",
      "epoch:28 step:26660 [D loss: 0.699827, acc: 53.91%] [G loss: 1.789543]\n",
      "epoch:28 step:26661 [D loss: 0.609544, acc: 63.28%] [G loss: 1.871119]\n",
      "epoch:28 step:26662 [D loss: 0.597359, acc: 67.97%] [G loss: 1.984605]\n",
      "epoch:28 step:26663 [D loss: 0.656340, acc: 65.62%] [G loss: 1.999214]\n",
      "epoch:28 step:26664 [D loss: 0.593262, acc: 69.53%] [G loss: 1.970254]\n",
      "epoch:28 step:26665 [D loss: 0.577951, acc: 71.09%] [G loss: 2.052292]\n",
      "epoch:28 step:26666 [D loss: 0.561485, acc: 71.09%] [G loss: 2.089835]\n",
      "epoch:28 step:26667 [D loss: 0.654415, acc: 60.94%] [G loss: 1.982418]\n",
      "epoch:28 step:26668 [D loss: 0.663021, acc: 60.16%] [G loss: 1.777191]\n",
      "epoch:28 step:26669 [D loss: 0.654209, acc: 60.94%] [G loss: 1.823073]\n",
      "epoch:28 step:26670 [D loss: 0.701455, acc: 58.59%] [G loss: 1.888026]\n",
      "epoch:28 step:26671 [D loss: 0.683269, acc: 58.59%] [G loss: 1.843711]\n",
      "epoch:28 step:26672 [D loss: 0.672403, acc: 57.81%] [G loss: 1.922585]\n",
      "epoch:28 step:26673 [D loss: 0.683472, acc: 60.94%] [G loss: 1.744530]\n",
      "epoch:28 step:26674 [D loss: 0.664919, acc: 60.94%] [G loss: 1.825928]\n",
      "epoch:28 step:26675 [D loss: 0.700583, acc: 59.38%] [G loss: 1.747808]\n",
      "epoch:28 step:26676 [D loss: 0.647917, acc: 63.28%] [G loss: 1.878931]\n",
      "epoch:28 step:26677 [D loss: 0.668023, acc: 55.47%] [G loss: 1.701531]\n",
      "epoch:28 step:26678 [D loss: 0.679698, acc: 60.16%] [G loss: 1.693246]\n",
      "epoch:28 step:26679 [D loss: 0.653226, acc: 59.38%] [G loss: 1.890836]\n",
      "epoch:28 step:26680 [D loss: 0.630346, acc: 57.81%] [G loss: 1.825782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26681 [D loss: 0.671375, acc: 50.00%] [G loss: 1.847379]\n",
      "epoch:28 step:26682 [D loss: 0.654607, acc: 62.50%] [G loss: 1.821978]\n",
      "epoch:28 step:26683 [D loss: 0.635720, acc: 64.84%] [G loss: 1.913939]\n",
      "epoch:28 step:26684 [D loss: 0.668431, acc: 58.59%] [G loss: 1.745589]\n",
      "epoch:28 step:26685 [D loss: 0.670333, acc: 57.03%] [G loss: 1.788104]\n",
      "epoch:28 step:26686 [D loss: 0.627218, acc: 69.53%] [G loss: 1.769668]\n",
      "epoch:28 step:26687 [D loss: 0.620293, acc: 67.97%] [G loss: 1.906630]\n",
      "epoch:28 step:26688 [D loss: 0.628412, acc: 65.62%] [G loss: 1.883512]\n",
      "epoch:28 step:26689 [D loss: 0.580533, acc: 67.19%] [G loss: 1.976828]\n",
      "epoch:28 step:26690 [D loss: 0.645583, acc: 65.62%] [G loss: 1.907467]\n",
      "epoch:28 step:26691 [D loss: 0.654992, acc: 63.28%] [G loss: 1.901131]\n",
      "epoch:28 step:26692 [D loss: 0.648453, acc: 65.62%] [G loss: 1.964961]\n",
      "epoch:28 step:26693 [D loss: 0.636521, acc: 64.06%] [G loss: 2.001168]\n",
      "epoch:28 step:26694 [D loss: 0.659909, acc: 60.16%] [G loss: 1.728811]\n",
      "epoch:28 step:26695 [D loss: 0.639582, acc: 63.28%] [G loss: 1.779238]\n",
      "epoch:28 step:26696 [D loss: 0.653993, acc: 61.72%] [G loss: 1.632179]\n",
      "epoch:28 step:26697 [D loss: 0.695341, acc: 56.25%] [G loss: 1.804911]\n",
      "epoch:28 step:26698 [D loss: 0.686582, acc: 59.38%] [G loss: 1.768304]\n",
      "epoch:28 step:26699 [D loss: 0.639230, acc: 65.62%] [G loss: 1.779648]\n",
      "epoch:28 step:26700 [D loss: 0.664137, acc: 57.03%] [G loss: 1.805734]\n",
      "epoch:28 step:26701 [D loss: 0.642949, acc: 61.72%] [G loss: 1.890283]\n",
      "epoch:28 step:26702 [D loss: 0.632034, acc: 64.06%] [G loss: 1.803350]\n",
      "epoch:28 step:26703 [D loss: 0.683307, acc: 59.38%] [G loss: 1.826699]\n",
      "epoch:28 step:26704 [D loss: 0.647201, acc: 57.81%] [G loss: 1.890960]\n",
      "epoch:28 step:26705 [D loss: 0.634191, acc: 65.62%] [G loss: 1.969424]\n",
      "epoch:28 step:26706 [D loss: 0.611202, acc: 63.28%] [G loss: 1.945778]\n",
      "epoch:28 step:26707 [D loss: 0.600828, acc: 69.53%] [G loss: 2.174792]\n",
      "epoch:28 step:26708 [D loss: 0.614552, acc: 64.84%] [G loss: 2.165190]\n",
      "epoch:28 step:26709 [D loss: 0.726151, acc: 56.25%] [G loss: 1.884984]\n",
      "epoch:28 step:26710 [D loss: 0.694326, acc: 60.94%] [G loss: 1.935711]\n",
      "epoch:28 step:26711 [D loss: 0.680793, acc: 57.81%] [G loss: 1.867307]\n",
      "epoch:28 step:26712 [D loss: 0.669088, acc: 59.38%] [G loss: 1.757892]\n",
      "epoch:28 step:26713 [D loss: 0.680182, acc: 61.72%] [G loss: 1.856461]\n",
      "epoch:28 step:26714 [D loss: 0.646202, acc: 60.94%] [G loss: 1.851752]\n",
      "epoch:28 step:26715 [D loss: 0.620681, acc: 67.97%] [G loss: 1.942029]\n",
      "epoch:28 step:26716 [D loss: 0.649325, acc: 64.06%] [G loss: 1.696787]\n",
      "epoch:28 step:26717 [D loss: 0.645633, acc: 60.16%] [G loss: 2.210071]\n",
      "epoch:28 step:26718 [D loss: 0.668962, acc: 60.94%] [G loss: 1.786110]\n",
      "epoch:28 step:26719 [D loss: 0.708842, acc: 52.34%] [G loss: 1.861957]\n",
      "epoch:28 step:26720 [D loss: 0.630898, acc: 65.62%] [G loss: 1.845090]\n",
      "epoch:28 step:26721 [D loss: 0.646327, acc: 60.94%] [G loss: 1.811718]\n",
      "epoch:28 step:26722 [D loss: 0.655245, acc: 64.84%] [G loss: 1.811117]\n",
      "epoch:28 step:26723 [D loss: 0.682201, acc: 57.03%] [G loss: 1.857100]\n",
      "epoch:28 step:26724 [D loss: 0.597078, acc: 66.41%] [G loss: 2.029034]\n",
      "epoch:28 step:26725 [D loss: 0.611344, acc: 67.19%] [G loss: 1.810179]\n",
      "epoch:28 step:26726 [D loss: 0.670168, acc: 57.81%] [G loss: 1.738669]\n",
      "epoch:28 step:26727 [D loss: 0.693065, acc: 55.47%] [G loss: 1.957290]\n",
      "epoch:28 step:26728 [D loss: 0.615924, acc: 66.41%] [G loss: 1.901717]\n",
      "epoch:28 step:26729 [D loss: 0.647443, acc: 61.72%] [G loss: 1.815762]\n",
      "epoch:28 step:26730 [D loss: 0.684113, acc: 59.38%] [G loss: 1.865107]\n",
      "epoch:28 step:26731 [D loss: 0.604633, acc: 71.09%] [G loss: 2.003899]\n",
      "epoch:28 step:26732 [D loss: 0.631443, acc: 68.75%] [G loss: 1.991316]\n",
      "epoch:28 step:26733 [D loss: 0.666884, acc: 59.38%] [G loss: 1.730631]\n",
      "epoch:28 step:26734 [D loss: 0.607486, acc: 69.53%] [G loss: 1.845611]\n",
      "epoch:28 step:26735 [D loss: 0.710622, acc: 54.69%] [G loss: 1.899053]\n",
      "epoch:28 step:26736 [D loss: 0.684660, acc: 56.25%] [G loss: 1.749642]\n",
      "epoch:28 step:26737 [D loss: 0.709389, acc: 56.25%] [G loss: 1.725783]\n",
      "epoch:28 step:26738 [D loss: 0.680057, acc: 56.25%] [G loss: 1.707879]\n",
      "epoch:28 step:26739 [D loss: 0.653489, acc: 60.94%] [G loss: 1.755218]\n",
      "epoch:28 step:26740 [D loss: 0.675750, acc: 60.16%] [G loss: 1.835949]\n",
      "epoch:28 step:26741 [D loss: 0.625770, acc: 63.28%] [G loss: 1.939052]\n",
      "epoch:28 step:26742 [D loss: 0.697296, acc: 59.38%] [G loss: 1.743431]\n",
      "epoch:28 step:26743 [D loss: 0.689570, acc: 58.59%] [G loss: 1.833525]\n",
      "epoch:28 step:26744 [D loss: 0.641164, acc: 59.38%] [G loss: 1.998865]\n",
      "epoch:28 step:26745 [D loss: 0.635741, acc: 67.19%] [G loss: 1.842807]\n",
      "epoch:28 step:26746 [D loss: 0.668570, acc: 56.25%] [G loss: 1.817436]\n",
      "epoch:28 step:26747 [D loss: 0.666166, acc: 58.59%] [G loss: 1.872537]\n",
      "epoch:28 step:26748 [D loss: 0.659929, acc: 60.16%] [G loss: 1.771206]\n",
      "epoch:28 step:26749 [D loss: 0.652234, acc: 54.69%] [G loss: 1.853326]\n",
      "epoch:28 step:26750 [D loss: 0.696875, acc: 59.38%] [G loss: 1.800828]\n",
      "epoch:28 step:26751 [D loss: 0.632642, acc: 62.50%] [G loss: 1.768921]\n",
      "epoch:28 step:26752 [D loss: 0.638186, acc: 63.28%] [G loss: 1.851063]\n",
      "epoch:28 step:26753 [D loss: 0.636803, acc: 60.94%] [G loss: 1.858059]\n",
      "epoch:28 step:26754 [D loss: 0.682445, acc: 57.03%] [G loss: 1.730467]\n",
      "epoch:28 step:26755 [D loss: 0.632534, acc: 62.50%] [G loss: 1.868793]\n",
      "epoch:28 step:26756 [D loss: 0.639203, acc: 59.38%] [G loss: 1.873323]\n",
      "epoch:28 step:26757 [D loss: 0.601449, acc: 69.53%] [G loss: 1.817269]\n",
      "epoch:28 step:26758 [D loss: 0.633718, acc: 58.59%] [G loss: 1.900683]\n",
      "epoch:28 step:26759 [D loss: 0.610206, acc: 62.50%] [G loss: 2.008268]\n",
      "epoch:28 step:26760 [D loss: 0.651106, acc: 64.06%] [G loss: 1.866895]\n",
      "epoch:28 step:26761 [D loss: 0.704318, acc: 57.81%] [G loss: 1.844895]\n",
      "epoch:28 step:26762 [D loss: 0.658221, acc: 64.06%] [G loss: 1.845506]\n",
      "epoch:28 step:26763 [D loss: 0.695122, acc: 57.81%] [G loss: 1.823678]\n",
      "epoch:28 step:26764 [D loss: 0.706541, acc: 54.69%] [G loss: 1.656774]\n",
      "epoch:28 step:26765 [D loss: 0.733189, acc: 51.56%] [G loss: 1.756617]\n",
      "epoch:28 step:26766 [D loss: 0.689682, acc: 53.91%] [G loss: 1.757929]\n",
      "epoch:28 step:26767 [D loss: 0.687291, acc: 55.47%] [G loss: 1.711233]\n",
      "epoch:28 step:26768 [D loss: 0.671468, acc: 52.34%] [G loss: 1.802495]\n",
      "epoch:28 step:26769 [D loss: 0.675018, acc: 57.03%] [G loss: 1.822651]\n",
      "epoch:28 step:26770 [D loss: 0.640449, acc: 59.38%] [G loss: 1.888579]\n",
      "epoch:28 step:26771 [D loss: 0.678733, acc: 57.03%] [G loss: 1.845841]\n",
      "epoch:28 step:26772 [D loss: 0.612652, acc: 68.75%] [G loss: 1.732960]\n",
      "epoch:28 step:26773 [D loss: 0.648867, acc: 64.06%] [G loss: 1.694945]\n",
      "epoch:28 step:26774 [D loss: 0.647215, acc: 64.84%] [G loss: 1.860080]\n",
      "epoch:28 step:26775 [D loss: 0.664448, acc: 59.38%] [G loss: 1.751429]\n",
      "epoch:28 step:26776 [D loss: 0.678030, acc: 59.38%] [G loss: 1.734186]\n",
      "epoch:28 step:26777 [D loss: 0.652688, acc: 59.38%] [G loss: 1.759301]\n",
      "epoch:28 step:26778 [D loss: 0.664562, acc: 57.81%] [G loss: 1.721772]\n",
      "epoch:28 step:26779 [D loss: 0.649097, acc: 64.06%] [G loss: 1.746081]\n",
      "epoch:28 step:26780 [D loss: 0.698400, acc: 56.25%] [G loss: 1.767946]\n",
      "epoch:28 step:26781 [D loss: 0.618344, acc: 64.84%] [G loss: 1.820292]\n",
      "epoch:28 step:26782 [D loss: 0.643354, acc: 58.59%] [G loss: 1.681021]\n",
      "epoch:28 step:26783 [D loss: 0.643326, acc: 63.28%] [G loss: 1.758574]\n",
      "epoch:28 step:26784 [D loss: 0.643844, acc: 64.06%] [G loss: 1.827037]\n",
      "epoch:28 step:26785 [D loss: 0.603062, acc: 68.75%] [G loss: 1.852834]\n",
      "epoch:28 step:26786 [D loss: 0.617260, acc: 64.84%] [G loss: 1.888695]\n",
      "epoch:28 step:26787 [D loss: 0.597968, acc: 67.97%] [G loss: 2.014305]\n",
      "epoch:28 step:26788 [D loss: 0.619038, acc: 63.28%] [G loss: 1.969234]\n",
      "epoch:28 step:26789 [D loss: 0.589643, acc: 68.75%] [G loss: 1.837666]\n",
      "epoch:28 step:26790 [D loss: 0.613538, acc: 66.41%] [G loss: 2.100698]\n",
      "epoch:28 step:26791 [D loss: 0.664009, acc: 63.28%] [G loss: 1.807581]\n",
      "epoch:28 step:26792 [D loss: 0.619622, acc: 64.84%] [G loss: 2.088860]\n",
      "epoch:28 step:26793 [D loss: 0.604448, acc: 67.97%] [G loss: 1.932477]\n",
      "epoch:28 step:26794 [D loss: 0.657236, acc: 62.50%] [G loss: 1.880107]\n",
      "epoch:28 step:26795 [D loss: 0.650353, acc: 61.72%] [G loss: 1.879278]\n",
      "epoch:28 step:26796 [D loss: 0.704475, acc: 54.69%] [G loss: 1.840360]\n",
      "epoch:28 step:26797 [D loss: 0.630617, acc: 67.19%] [G loss: 1.919945]\n",
      "epoch:28 step:26798 [D loss: 0.642009, acc: 63.28%] [G loss: 2.014547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26799 [D loss: 0.613908, acc: 70.31%] [G loss: 1.955141]\n",
      "epoch:28 step:26800 [D loss: 0.633608, acc: 63.28%] [G loss: 2.012750]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.491239\n",
      "FID: 12.836496\n",
      "0 = 12.580307941484463\n",
      "1 = 0.08296256234467121\n",
      "2 = 0.8666999936103821\n",
      "3 = 0.8841999769210815\n",
      "4 = 0.8492000102996826\n",
      "5 = 0.8542995452880859\n",
      "6 = 0.8841999769210815\n",
      "7 = 6.21065190494061\n",
      "8 = 0.06921720412618454\n",
      "9 = 0.7038000226020813\n",
      "10 = 0.7111999988555908\n",
      "11 = 0.696399986743927\n",
      "12 = 0.7008277773857117\n",
      "13 = 0.7111999988555908\n",
      "14 = 7.491281032562256\n",
      "15 = 9.394108772277832\n",
      "16 = 0.12252965569496155\n",
      "17 = 7.491239070892334\n",
      "18 = 12.836496353149414\n",
      "epoch:28 step:26801 [D loss: 0.685671, acc: 57.03%] [G loss: 1.830983]\n",
      "epoch:28 step:26802 [D loss: 0.700793, acc: 53.12%] [G loss: 1.764522]\n",
      "epoch:28 step:26803 [D loss: 0.664492, acc: 58.59%] [G loss: 1.850765]\n",
      "epoch:28 step:26804 [D loss: 0.642948, acc: 67.19%] [G loss: 1.863847]\n",
      "epoch:28 step:26805 [D loss: 0.704101, acc: 53.12%] [G loss: 1.892325]\n",
      "epoch:28 step:26806 [D loss: 0.683272, acc: 58.59%] [G loss: 1.794259]\n",
      "epoch:28 step:26807 [D loss: 0.642971, acc: 60.94%] [G loss: 1.734373]\n",
      "epoch:28 step:26808 [D loss: 0.665383, acc: 57.81%] [G loss: 1.778025]\n",
      "epoch:28 step:26809 [D loss: 0.632290, acc: 63.28%] [G loss: 1.714730]\n",
      "epoch:28 step:26810 [D loss: 0.629924, acc: 71.88%] [G loss: 1.890548]\n",
      "epoch:28 step:26811 [D loss: 0.653149, acc: 62.50%] [G loss: 1.821010]\n",
      "epoch:28 step:26812 [D loss: 0.659104, acc: 60.16%] [G loss: 1.665408]\n",
      "epoch:28 step:26813 [D loss: 0.678498, acc: 54.69%] [G loss: 1.702995]\n",
      "epoch:28 step:26814 [D loss: 0.668466, acc: 60.16%] [G loss: 1.769145]\n",
      "epoch:28 step:26815 [D loss: 0.681416, acc: 60.16%] [G loss: 1.669142]\n",
      "epoch:28 step:26816 [D loss: 0.607976, acc: 69.53%] [G loss: 1.847000]\n",
      "epoch:28 step:26817 [D loss: 0.645947, acc: 63.28%] [G loss: 1.788540]\n",
      "epoch:28 step:26818 [D loss: 0.639040, acc: 60.94%] [G loss: 1.832529]\n",
      "epoch:28 step:26819 [D loss: 0.614443, acc: 67.97%] [G loss: 1.754903]\n",
      "epoch:28 step:26820 [D loss: 0.656970, acc: 60.94%] [G loss: 1.798610]\n",
      "epoch:28 step:26821 [D loss: 0.699455, acc: 51.56%] [G loss: 1.821253]\n",
      "epoch:28 step:26822 [D loss: 0.662224, acc: 59.38%] [G loss: 1.771052]\n",
      "epoch:28 step:26823 [D loss: 0.680439, acc: 57.81%] [G loss: 1.800382]\n",
      "epoch:28 step:26824 [D loss: 0.633203, acc: 66.41%] [G loss: 1.897109]\n",
      "epoch:28 step:26825 [D loss: 0.656094, acc: 60.94%] [G loss: 1.942885]\n",
      "epoch:28 step:26826 [D loss: 0.620682, acc: 67.19%] [G loss: 1.888880]\n",
      "epoch:28 step:26827 [D loss: 0.638728, acc: 69.53%] [G loss: 1.923194]\n",
      "epoch:28 step:26828 [D loss: 0.657658, acc: 59.38%] [G loss: 1.803153]\n",
      "epoch:28 step:26829 [D loss: 0.663244, acc: 64.84%] [G loss: 1.924945]\n",
      "epoch:28 step:26830 [D loss: 0.654029, acc: 61.72%] [G loss: 1.952738]\n",
      "epoch:28 step:26831 [D loss: 0.672279, acc: 58.59%] [G loss: 1.870860]\n",
      "epoch:28 step:26832 [D loss: 0.613575, acc: 66.41%] [G loss: 1.812168]\n",
      "epoch:28 step:26833 [D loss: 0.681137, acc: 56.25%] [G loss: 1.755918]\n",
      "epoch:28 step:26834 [D loss: 0.677037, acc: 60.16%] [G loss: 1.894352]\n",
      "epoch:28 step:26835 [D loss: 0.675360, acc: 58.59%] [G loss: 1.778905]\n",
      "epoch:28 step:26836 [D loss: 0.712238, acc: 50.00%] [G loss: 1.779510]\n",
      "epoch:28 step:26837 [D loss: 0.664319, acc: 61.72%] [G loss: 1.827004]\n",
      "epoch:28 step:26838 [D loss: 0.638923, acc: 68.75%] [G loss: 1.898930]\n",
      "epoch:28 step:26839 [D loss: 0.644459, acc: 57.03%] [G loss: 1.842541]\n",
      "epoch:28 step:26840 [D loss: 0.601897, acc: 62.50%] [G loss: 1.966284]\n",
      "epoch:28 step:26841 [D loss: 0.637650, acc: 64.84%] [G loss: 1.764753]\n",
      "epoch:28 step:26842 [D loss: 0.639744, acc: 64.84%] [G loss: 1.871657]\n",
      "epoch:28 step:26843 [D loss: 0.653316, acc: 64.06%] [G loss: 1.858295]\n",
      "epoch:28 step:26844 [D loss: 0.638652, acc: 63.28%] [G loss: 1.828691]\n",
      "epoch:28 step:26845 [D loss: 0.662400, acc: 61.72%] [G loss: 1.942924]\n",
      "epoch:28 step:26846 [D loss: 0.627061, acc: 65.62%] [G loss: 1.922764]\n",
      "epoch:28 step:26847 [D loss: 0.701536, acc: 55.47%] [G loss: 1.823250]\n",
      "epoch:28 step:26848 [D loss: 0.647136, acc: 64.84%] [G loss: 1.847040]\n",
      "epoch:28 step:26849 [D loss: 0.654027, acc: 59.38%] [G loss: 1.792717]\n",
      "epoch:28 step:26850 [D loss: 0.662692, acc: 53.91%] [G loss: 1.736256]\n",
      "epoch:28 step:26851 [D loss: 0.669742, acc: 60.16%] [G loss: 1.832398]\n",
      "epoch:28 step:26852 [D loss: 0.657001, acc: 57.81%] [G loss: 1.840711]\n",
      "epoch:28 step:26853 [D loss: 0.700058, acc: 55.47%] [G loss: 1.934066]\n",
      "epoch:28 step:26854 [D loss: 0.646605, acc: 60.16%] [G loss: 1.928414]\n",
      "epoch:28 step:26855 [D loss: 0.641446, acc: 60.16%] [G loss: 1.841900]\n",
      "epoch:28 step:26856 [D loss: 0.617360, acc: 64.84%] [G loss: 1.739929]\n",
      "epoch:28 step:26857 [D loss: 0.629623, acc: 63.28%] [G loss: 1.881725]\n",
      "epoch:28 step:26858 [D loss: 0.626618, acc: 64.06%] [G loss: 2.029417]\n",
      "epoch:28 step:26859 [D loss: 0.716563, acc: 54.69%] [G loss: 1.810590]\n",
      "epoch:28 step:26860 [D loss: 0.614150, acc: 64.06%] [G loss: 1.947353]\n",
      "epoch:28 step:26861 [D loss: 0.698311, acc: 57.81%] [G loss: 1.727539]\n",
      "epoch:28 step:26862 [D loss: 0.641581, acc: 61.72%] [G loss: 1.773507]\n",
      "epoch:28 step:26863 [D loss: 0.632604, acc: 66.41%] [G loss: 1.821741]\n",
      "epoch:28 step:26864 [D loss: 0.597729, acc: 67.19%] [G loss: 1.828988]\n",
      "epoch:28 step:26865 [D loss: 0.633830, acc: 58.59%] [G loss: 1.696896]\n",
      "epoch:28 step:26866 [D loss: 0.658759, acc: 57.81%] [G loss: 1.906857]\n",
      "epoch:28 step:26867 [D loss: 0.645814, acc: 66.41%] [G loss: 2.005034]\n",
      "epoch:28 step:26868 [D loss: 0.633799, acc: 64.06%] [G loss: 1.850647]\n",
      "epoch:28 step:26869 [D loss: 0.694170, acc: 57.81%] [G loss: 1.985444]\n",
      "epoch:28 step:26870 [D loss: 0.639967, acc: 65.62%] [G loss: 1.927198]\n",
      "epoch:28 step:26871 [D loss: 0.642860, acc: 63.28%] [G loss: 1.946616]\n",
      "epoch:28 step:26872 [D loss: 0.665943, acc: 60.94%] [G loss: 1.961758]\n",
      "epoch:28 step:26873 [D loss: 0.657608, acc: 59.38%] [G loss: 1.911119]\n",
      "epoch:28 step:26874 [D loss: 0.631423, acc: 61.72%] [G loss: 1.981717]\n",
      "epoch:28 step:26875 [D loss: 0.645216, acc: 62.50%] [G loss: 1.988136]\n",
      "epoch:28 step:26876 [D loss: 0.672355, acc: 60.94%] [G loss: 1.908095]\n",
      "epoch:28 step:26877 [D loss: 0.662194, acc: 65.62%] [G loss: 1.813645]\n",
      "epoch:28 step:26878 [D loss: 0.592544, acc: 70.31%] [G loss: 1.943697]\n",
      "epoch:28 step:26879 [D loss: 0.605847, acc: 71.09%] [G loss: 1.715582]\n",
      "epoch:28 step:26880 [D loss: 0.611259, acc: 65.62%] [G loss: 1.925645]\n",
      "epoch:28 step:26881 [D loss: 0.687749, acc: 61.72%] [G loss: 1.833656]\n",
      "epoch:28 step:26882 [D loss: 0.574707, acc: 74.22%] [G loss: 1.916298]\n",
      "epoch:28 step:26883 [D loss: 0.611420, acc: 72.66%] [G loss: 1.986360]\n",
      "epoch:28 step:26884 [D loss: 0.652088, acc: 57.81%] [G loss: 1.997012]\n",
      "epoch:28 step:26885 [D loss: 0.647135, acc: 61.72%] [G loss: 2.001320]\n",
      "epoch:28 step:26886 [D loss: 0.584706, acc: 70.31%] [G loss: 2.065068]\n",
      "epoch:28 step:26887 [D loss: 0.618079, acc: 62.50%] [G loss: 1.961148]\n",
      "epoch:28 step:26888 [D loss: 0.628778, acc: 67.19%] [G loss: 1.863277]\n",
      "epoch:28 step:26889 [D loss: 0.595027, acc: 66.41%] [G loss: 1.997762]\n",
      "epoch:28 step:26890 [D loss: 0.616128, acc: 67.97%] [G loss: 2.124355]\n",
      "epoch:28 step:26891 [D loss: 0.644243, acc: 57.81%] [G loss: 1.893332]\n",
      "epoch:28 step:26892 [D loss: 0.671911, acc: 58.59%] [G loss: 1.777783]\n",
      "epoch:28 step:26893 [D loss: 0.667703, acc: 64.84%] [G loss: 1.767006]\n",
      "epoch:28 step:26894 [D loss: 0.635018, acc: 59.38%] [G loss: 1.828707]\n",
      "epoch:28 step:26895 [D loss: 0.650873, acc: 58.59%] [G loss: 1.812868]\n",
      "epoch:28 step:26896 [D loss: 0.692231, acc: 60.16%] [G loss: 1.813369]\n",
      "epoch:28 step:26897 [D loss: 0.669155, acc: 56.25%] [G loss: 1.829333]\n",
      "epoch:28 step:26898 [D loss: 0.681069, acc: 60.94%] [G loss: 1.911467]\n",
      "epoch:28 step:26899 [D loss: 0.697566, acc: 57.81%] [G loss: 1.839607]\n",
      "epoch:28 step:26900 [D loss: 0.664750, acc: 57.03%] [G loss: 1.892614]\n",
      "epoch:28 step:26901 [D loss: 0.618521, acc: 67.19%] [G loss: 1.883721]\n",
      "epoch:28 step:26902 [D loss: 0.699447, acc: 57.03%] [G loss: 1.719872]\n",
      "epoch:28 step:26903 [D loss: 0.683037, acc: 60.16%] [G loss: 1.689713]\n",
      "epoch:28 step:26904 [D loss: 0.683471, acc: 60.16%] [G loss: 1.812808]\n",
      "epoch:28 step:26905 [D loss: 0.664443, acc: 61.72%] [G loss: 1.813160]\n",
      "epoch:28 step:26906 [D loss: 0.642791, acc: 57.03%] [G loss: 1.752293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26907 [D loss: 0.649399, acc: 60.94%] [G loss: 1.753943]\n",
      "epoch:28 step:26908 [D loss: 0.685114, acc: 58.59%] [G loss: 1.816773]\n",
      "epoch:28 step:26909 [D loss: 0.652391, acc: 62.50%] [G loss: 1.831181]\n",
      "epoch:28 step:26910 [D loss: 0.688153, acc: 57.81%] [G loss: 1.790261]\n",
      "epoch:28 step:26911 [D loss: 0.640010, acc: 64.84%] [G loss: 1.794113]\n",
      "epoch:28 step:26912 [D loss: 0.614838, acc: 64.06%] [G loss: 1.916866]\n",
      "epoch:28 step:26913 [D loss: 0.618820, acc: 67.19%] [G loss: 2.000604]\n",
      "epoch:28 step:26914 [D loss: 0.647576, acc: 60.16%] [G loss: 1.837417]\n",
      "epoch:28 step:26915 [D loss: 0.647994, acc: 61.72%] [G loss: 2.020868]\n",
      "epoch:28 step:26916 [D loss: 0.670433, acc: 58.59%] [G loss: 1.912795]\n",
      "epoch:28 step:26917 [D loss: 0.626819, acc: 57.81%] [G loss: 1.951299]\n",
      "epoch:28 step:26918 [D loss: 0.667909, acc: 60.94%] [G loss: 1.747007]\n",
      "epoch:28 step:26919 [D loss: 0.632375, acc: 65.62%] [G loss: 1.847713]\n",
      "epoch:28 step:26920 [D loss: 0.689884, acc: 53.12%] [G loss: 1.796071]\n",
      "epoch:28 step:26921 [D loss: 0.617612, acc: 63.28%] [G loss: 1.868817]\n",
      "epoch:28 step:26922 [D loss: 0.674551, acc: 59.38%] [G loss: 1.821862]\n",
      "epoch:28 step:26923 [D loss: 0.640489, acc: 68.75%] [G loss: 1.917673]\n",
      "epoch:28 step:26924 [D loss: 0.677669, acc: 57.81%] [G loss: 1.915436]\n",
      "epoch:28 step:26925 [D loss: 0.621907, acc: 62.50%] [G loss: 1.820007]\n",
      "epoch:28 step:26926 [D loss: 0.637885, acc: 63.28%] [G loss: 1.928097]\n",
      "epoch:28 step:26927 [D loss: 0.646793, acc: 62.50%] [G loss: 1.907733]\n",
      "epoch:28 step:26928 [D loss: 0.575857, acc: 70.31%] [G loss: 1.934800]\n",
      "epoch:28 step:26929 [D loss: 0.647076, acc: 56.25%] [G loss: 1.920439]\n",
      "epoch:28 step:26930 [D loss: 0.614375, acc: 66.41%] [G loss: 2.036454]\n",
      "epoch:28 step:26931 [D loss: 0.648928, acc: 63.28%] [G loss: 1.809656]\n",
      "epoch:28 step:26932 [D loss: 0.648741, acc: 60.94%] [G loss: 1.770504]\n",
      "epoch:28 step:26933 [D loss: 0.717967, acc: 54.69%] [G loss: 1.850771]\n",
      "epoch:28 step:26934 [D loss: 0.714967, acc: 56.25%] [G loss: 1.781635]\n",
      "epoch:28 step:26935 [D loss: 0.629327, acc: 67.97%] [G loss: 1.990273]\n",
      "epoch:28 step:26936 [D loss: 0.668491, acc: 59.38%] [G loss: 1.796598]\n",
      "epoch:28 step:26937 [D loss: 0.658909, acc: 61.72%] [G loss: 1.818257]\n",
      "epoch:28 step:26938 [D loss: 0.733507, acc: 51.56%] [G loss: 1.892525]\n",
      "epoch:28 step:26939 [D loss: 0.645565, acc: 66.41%] [G loss: 1.883832]\n",
      "epoch:28 step:26940 [D loss: 0.641057, acc: 59.38%] [G loss: 1.777296]\n",
      "epoch:28 step:26941 [D loss: 0.658460, acc: 58.59%] [G loss: 1.753189]\n",
      "epoch:28 step:26942 [D loss: 0.634709, acc: 64.06%] [G loss: 1.976736]\n",
      "epoch:28 step:26943 [D loss: 0.636329, acc: 60.94%] [G loss: 1.985408]\n",
      "epoch:28 step:26944 [D loss: 0.623468, acc: 69.53%] [G loss: 1.979223]\n",
      "epoch:28 step:26945 [D loss: 0.642642, acc: 65.62%] [G loss: 1.908183]\n",
      "epoch:28 step:26946 [D loss: 0.712011, acc: 54.69%] [G loss: 1.666721]\n",
      "epoch:28 step:26947 [D loss: 0.614535, acc: 64.84%] [G loss: 1.945327]\n",
      "epoch:28 step:26948 [D loss: 0.642126, acc: 60.16%] [G loss: 1.916823]\n",
      "epoch:28 step:26949 [D loss: 0.665775, acc: 59.38%] [G loss: 1.831426]\n",
      "epoch:28 step:26950 [D loss: 0.616205, acc: 64.06%] [G loss: 1.848294]\n",
      "epoch:28 step:26951 [D loss: 0.697890, acc: 56.25%] [G loss: 1.823892]\n",
      "epoch:28 step:26952 [D loss: 0.665138, acc: 60.16%] [G loss: 1.916121]\n",
      "epoch:28 step:26953 [D loss: 0.620819, acc: 67.97%] [G loss: 1.805301]\n",
      "epoch:28 step:26954 [D loss: 0.660027, acc: 60.94%] [G loss: 1.734552]\n",
      "epoch:28 step:26955 [D loss: 0.591942, acc: 69.53%] [G loss: 2.054222]\n",
      "epoch:28 step:26956 [D loss: 0.619179, acc: 63.28%] [G loss: 2.145216]\n",
      "epoch:28 step:26957 [D loss: 0.628540, acc: 60.94%] [G loss: 1.935102]\n",
      "epoch:28 step:26958 [D loss: 0.686844, acc: 57.81%] [G loss: 1.745607]\n",
      "epoch:28 step:26959 [D loss: 0.686018, acc: 59.38%] [G loss: 1.862975]\n",
      "epoch:28 step:26960 [D loss: 0.580780, acc: 73.44%] [G loss: 2.048633]\n",
      "epoch:28 step:26961 [D loss: 0.651085, acc: 64.84%] [G loss: 1.899935]\n",
      "epoch:28 step:26962 [D loss: 0.632808, acc: 59.38%] [G loss: 1.995195]\n",
      "epoch:28 step:26963 [D loss: 0.619903, acc: 70.31%] [G loss: 1.869536]\n",
      "epoch:28 step:26964 [D loss: 0.682434, acc: 64.06%] [G loss: 1.938852]\n",
      "epoch:28 step:26965 [D loss: 0.659373, acc: 58.59%] [G loss: 1.923235]\n",
      "epoch:28 step:26966 [D loss: 0.655736, acc: 64.84%] [G loss: 1.992133]\n",
      "epoch:28 step:26967 [D loss: 0.655970, acc: 60.16%] [G loss: 1.748491]\n",
      "epoch:28 step:26968 [D loss: 0.652566, acc: 57.03%] [G loss: 1.860881]\n",
      "epoch:28 step:26969 [D loss: 0.622525, acc: 66.41%] [G loss: 1.762574]\n",
      "epoch:28 step:26970 [D loss: 0.751072, acc: 46.88%] [G loss: 1.735923]\n",
      "epoch:28 step:26971 [D loss: 0.696377, acc: 63.28%] [G loss: 1.884918]\n",
      "epoch:28 step:26972 [D loss: 0.588843, acc: 71.88%] [G loss: 1.870353]\n",
      "epoch:28 step:26973 [D loss: 0.628135, acc: 64.06%] [G loss: 1.871181]\n",
      "epoch:28 step:26974 [D loss: 0.680743, acc: 57.03%] [G loss: 1.876687]\n",
      "epoch:28 step:26975 [D loss: 0.733154, acc: 51.56%] [G loss: 1.716525]\n",
      "epoch:28 step:26976 [D loss: 0.658529, acc: 64.06%] [G loss: 1.769956]\n",
      "epoch:28 step:26977 [D loss: 0.643194, acc: 65.62%] [G loss: 1.817639]\n",
      "epoch:28 step:26978 [D loss: 0.624342, acc: 61.72%] [G loss: 1.730398]\n",
      "epoch:28 step:26979 [D loss: 0.674030, acc: 55.47%] [G loss: 1.869976]\n",
      "epoch:28 step:26980 [D loss: 0.662835, acc: 60.94%] [G loss: 1.800883]\n",
      "epoch:28 step:26981 [D loss: 0.669081, acc: 60.94%] [G loss: 1.811826]\n",
      "epoch:28 step:26982 [D loss: 0.609348, acc: 63.28%] [G loss: 1.949563]\n",
      "epoch:28 step:26983 [D loss: 0.654881, acc: 58.59%] [G loss: 1.917185]\n",
      "epoch:28 step:26984 [D loss: 0.637341, acc: 64.84%] [G loss: 1.739168]\n",
      "epoch:28 step:26985 [D loss: 0.653070, acc: 59.38%] [G loss: 1.757255]\n",
      "epoch:28 step:26986 [D loss: 0.651393, acc: 60.94%] [G loss: 1.782156]\n",
      "epoch:28 step:26987 [D loss: 0.610620, acc: 66.41%] [G loss: 1.840384]\n",
      "epoch:28 step:26988 [D loss: 0.672196, acc: 59.38%] [G loss: 1.832627]\n",
      "epoch:28 step:26989 [D loss: 0.671132, acc: 56.25%] [G loss: 1.801147]\n",
      "epoch:28 step:26990 [D loss: 0.630848, acc: 65.62%] [G loss: 1.719355]\n",
      "epoch:28 step:26991 [D loss: 0.620926, acc: 66.41%] [G loss: 1.841177]\n",
      "epoch:28 step:26992 [D loss: 0.637009, acc: 64.06%] [G loss: 1.908638]\n",
      "epoch:28 step:26993 [D loss: 0.618810, acc: 66.41%] [G loss: 1.891325]\n",
      "epoch:28 step:26994 [D loss: 0.668588, acc: 57.03%] [G loss: 1.855237]\n",
      "epoch:28 step:26995 [D loss: 0.633514, acc: 65.62%] [G loss: 1.755581]\n",
      "epoch:28 step:26996 [D loss: 0.671368, acc: 57.03%] [G loss: 1.765977]\n",
      "epoch:28 step:26997 [D loss: 0.664931, acc: 64.06%] [G loss: 1.950932]\n",
      "epoch:28 step:26998 [D loss: 0.659716, acc: 60.94%] [G loss: 1.815998]\n",
      "epoch:28 step:26999 [D loss: 0.603557, acc: 60.16%] [G loss: 1.873354]\n",
      "epoch:28 step:27000 [D loss: 0.620554, acc: 63.28%] [G loss: 1.848279]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.665536\n",
      "FID: 9.353815\n",
      "0 = 12.391515483617786\n",
      "1 = 0.07267439420832734\n",
      "2 = 0.8632000088691711\n",
      "3 = 0.8805999755859375\n",
      "4 = 0.84579998254776\n",
      "5 = 0.8509857058525085\n",
      "6 = 0.8805999755859375\n",
      "7 = 5.968497065365326\n",
      "8 = 0.05741614488310152\n",
      "9 = 0.6988999843597412\n",
      "10 = 0.699999988079071\n",
      "11 = 0.6977999806404114\n",
      "12 = 0.6984633803367615\n",
      "13 = 0.699999988079071\n",
      "14 = 7.665566444396973\n",
      "15 = 9.452695846557617\n",
      "16 = 0.1023881807923317\n",
      "17 = 7.665536403656006\n",
      "18 = 9.353815078735352\n",
      "epoch:28 step:27001 [D loss: 0.699552, acc: 53.91%] [G loss: 1.687555]\n",
      "epoch:28 step:27002 [D loss: 0.638394, acc: 64.84%] [G loss: 1.815366]\n",
      "epoch:28 step:27003 [D loss: 0.623341, acc: 67.19%] [G loss: 1.818840]\n",
      "epoch:28 step:27004 [D loss: 0.710503, acc: 60.16%] [G loss: 1.848984]\n",
      "epoch:28 step:27005 [D loss: 0.630192, acc: 68.75%] [G loss: 2.052845]\n",
      "epoch:28 step:27006 [D loss: 0.622739, acc: 64.84%] [G loss: 1.806297]\n",
      "epoch:28 step:27007 [D loss: 0.609730, acc: 67.97%] [G loss: 1.769189]\n",
      "epoch:28 step:27008 [D loss: 0.643444, acc: 62.50%] [G loss: 1.926808]\n",
      "epoch:28 step:27009 [D loss: 0.623001, acc: 60.94%] [G loss: 1.945091]\n",
      "epoch:28 step:27010 [D loss: 0.650879, acc: 60.16%] [G loss: 2.125294]\n",
      "epoch:28 step:27011 [D loss: 0.635322, acc: 63.28%] [G loss: 2.098930]\n",
      "epoch:28 step:27012 [D loss: 0.635898, acc: 66.41%] [G loss: 2.054275]\n",
      "epoch:28 step:27013 [D loss: 0.729179, acc: 56.25%] [G loss: 2.010558]\n",
      "epoch:28 step:27014 [D loss: 0.650213, acc: 61.72%] [G loss: 1.903567]\n",
      "epoch:28 step:27015 [D loss: 0.643297, acc: 66.41%] [G loss: 1.858357]\n",
      "epoch:28 step:27016 [D loss: 0.649131, acc: 62.50%] [G loss: 1.961268]\n",
      "epoch:28 step:27017 [D loss: 0.610049, acc: 66.41%] [G loss: 2.077202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:27018 [D loss: 0.611845, acc: 66.41%] [G loss: 1.991521]\n",
      "epoch:28 step:27019 [D loss: 0.695242, acc: 57.03%] [G loss: 1.847490]\n",
      "epoch:28 step:27020 [D loss: 0.757732, acc: 50.00%] [G loss: 1.806677]\n",
      "epoch:28 step:27021 [D loss: 0.729161, acc: 47.66%] [G loss: 1.811488]\n",
      "epoch:28 step:27022 [D loss: 0.705656, acc: 58.59%] [G loss: 1.876615]\n",
      "epoch:28 step:27023 [D loss: 0.643644, acc: 60.16%] [G loss: 1.834389]\n",
      "epoch:28 step:27024 [D loss: 0.636806, acc: 64.06%] [G loss: 1.775259]\n",
      "epoch:28 step:27025 [D loss: 0.688161, acc: 55.47%] [G loss: 1.905328]\n",
      "epoch:28 step:27026 [D loss: 0.646256, acc: 63.28%] [G loss: 1.922158]\n",
      "epoch:28 step:27027 [D loss: 0.719173, acc: 53.12%] [G loss: 1.792547]\n",
      "epoch:28 step:27028 [D loss: 0.640856, acc: 57.03%] [G loss: 2.014040]\n",
      "epoch:28 step:27029 [D loss: 0.661195, acc: 63.28%] [G loss: 1.960588]\n",
      "epoch:28 step:27030 [D loss: 0.666266, acc: 60.16%] [G loss: 1.727213]\n",
      "epoch:28 step:27031 [D loss: 0.679337, acc: 60.16%] [G loss: 1.814719]\n",
      "epoch:28 step:27032 [D loss: 0.680516, acc: 57.81%] [G loss: 1.830106]\n",
      "epoch:28 step:27033 [D loss: 0.626423, acc: 62.50%] [G loss: 1.814688]\n",
      "epoch:28 step:27034 [D loss: 0.662909, acc: 59.38%] [G loss: 1.784971]\n",
      "epoch:28 step:27035 [D loss: 0.654617, acc: 63.28%] [G loss: 1.737298]\n",
      "epoch:28 step:27036 [D loss: 0.703888, acc: 53.12%] [G loss: 1.818817]\n",
      "epoch:28 step:27037 [D loss: 0.686952, acc: 56.25%] [G loss: 1.772715]\n",
      "epoch:28 step:27038 [D loss: 0.666902, acc: 59.38%] [G loss: 1.735775]\n",
      "epoch:28 step:27039 [D loss: 0.627807, acc: 63.28%] [G loss: 1.794753]\n",
      "epoch:28 step:27040 [D loss: 0.607753, acc: 68.75%] [G loss: 1.808746]\n",
      "epoch:28 step:27041 [D loss: 0.595609, acc: 71.09%] [G loss: 1.958540]\n",
      "epoch:28 step:27042 [D loss: 0.684548, acc: 57.81%] [G loss: 1.864086]\n",
      "epoch:28 step:27043 [D loss: 0.658570, acc: 57.81%] [G loss: 1.862724]\n",
      "epoch:28 step:27044 [D loss: 0.633689, acc: 62.50%] [G loss: 1.863217]\n",
      "epoch:28 step:27045 [D loss: 0.673470, acc: 64.84%] [G loss: 1.847897]\n",
      "epoch:28 step:27046 [D loss: 0.637325, acc: 60.94%] [G loss: 1.865549]\n",
      "epoch:28 step:27047 [D loss: 0.598162, acc: 64.06%] [G loss: 1.862988]\n",
      "epoch:28 step:27048 [D loss: 0.675147, acc: 60.94%] [G loss: 1.813517]\n",
      "epoch:28 step:27049 [D loss: 0.619709, acc: 66.41%] [G loss: 1.888558]\n",
      "epoch:28 step:27050 [D loss: 0.608837, acc: 60.94%] [G loss: 1.740117]\n",
      "epoch:28 step:27051 [D loss: 0.601144, acc: 68.75%] [G loss: 1.881933]\n",
      "epoch:28 step:27052 [D loss: 0.596368, acc: 65.62%] [G loss: 1.949438]\n",
      "epoch:28 step:27053 [D loss: 0.643531, acc: 64.84%] [G loss: 2.051878]\n",
      "epoch:28 step:27054 [D loss: 0.662394, acc: 61.72%] [G loss: 1.759174]\n",
      "epoch:28 step:27055 [D loss: 0.690851, acc: 58.59%] [G loss: 1.897210]\n",
      "epoch:28 step:27056 [D loss: 0.795105, acc: 39.06%] [G loss: 1.668471]\n",
      "epoch:28 step:27057 [D loss: 0.698628, acc: 55.47%] [G loss: 1.745968]\n",
      "epoch:28 step:27058 [D loss: 0.624614, acc: 65.62%] [G loss: 1.774061]\n",
      "epoch:28 step:27059 [D loss: 0.593220, acc: 72.66%] [G loss: 1.920835]\n",
      "epoch:28 step:27060 [D loss: 0.668947, acc: 58.59%] [G loss: 1.831070]\n",
      "epoch:28 step:27061 [D loss: 0.629700, acc: 61.72%] [G loss: 1.973745]\n",
      "epoch:28 step:27062 [D loss: 0.632033, acc: 61.72%] [G loss: 1.947718]\n",
      "epoch:28 step:27063 [D loss: 0.671685, acc: 57.03%] [G loss: 1.794909]\n",
      "epoch:28 step:27064 [D loss: 0.700155, acc: 53.91%] [G loss: 1.785188]\n",
      "epoch:28 step:27065 [D loss: 0.653780, acc: 61.72%] [G loss: 1.864921]\n",
      "epoch:28 step:27066 [D loss: 0.657512, acc: 58.59%] [G loss: 1.787703]\n",
      "epoch:28 step:27067 [D loss: 0.646056, acc: 62.50%] [G loss: 1.741527]\n",
      "epoch:28 step:27068 [D loss: 0.620704, acc: 66.41%] [G loss: 1.899901]\n",
      "epoch:28 step:27069 [D loss: 0.630037, acc: 64.06%] [G loss: 1.930525]\n",
      "epoch:28 step:27070 [D loss: 0.618871, acc: 68.75%] [G loss: 1.872391]\n",
      "epoch:28 step:27071 [D loss: 0.680306, acc: 59.38%] [G loss: 1.806261]\n",
      "epoch:28 step:27072 [D loss: 0.605317, acc: 68.75%] [G loss: 1.956123]\n",
      "epoch:28 step:27073 [D loss: 0.665237, acc: 60.16%] [G loss: 1.802579]\n",
      "epoch:28 step:27074 [D loss: 0.642460, acc: 64.06%] [G loss: 1.962676]\n",
      "epoch:28 step:27075 [D loss: 0.706521, acc: 60.16%] [G loss: 1.946437]\n",
      "epoch:28 step:27076 [D loss: 0.667907, acc: 60.16%] [G loss: 1.807130]\n",
      "epoch:28 step:27077 [D loss: 0.639895, acc: 67.19%] [G loss: 1.803015]\n",
      "epoch:28 step:27078 [D loss: 0.615111, acc: 64.84%] [G loss: 1.840195]\n",
      "epoch:28 step:27079 [D loss: 0.695142, acc: 57.03%] [G loss: 1.687805]\n",
      "epoch:28 step:27080 [D loss: 0.670395, acc: 58.59%] [G loss: 1.847553]\n",
      "epoch:28 step:27081 [D loss: 0.656954, acc: 63.28%] [G loss: 1.908707]\n",
      "epoch:28 step:27082 [D loss: 0.627384, acc: 64.06%] [G loss: 1.732364]\n",
      "epoch:28 step:27083 [D loss: 0.578180, acc: 73.44%] [G loss: 1.867694]\n",
      "epoch:28 step:27084 [D loss: 0.626071, acc: 66.41%] [G loss: 1.874524]\n",
      "epoch:28 step:27085 [D loss: 0.643727, acc: 63.28%] [G loss: 2.041229]\n",
      "epoch:28 step:27086 [D loss: 0.722109, acc: 49.22%] [G loss: 1.767469]\n",
      "epoch:28 step:27087 [D loss: 0.673279, acc: 57.03%] [G loss: 1.751714]\n",
      "epoch:28 step:27088 [D loss: 0.644793, acc: 68.75%] [G loss: 1.884651]\n",
      "epoch:28 step:27089 [D loss: 0.615637, acc: 64.06%] [G loss: 1.796896]\n",
      "epoch:28 step:27090 [D loss: 0.646672, acc: 66.41%] [G loss: 1.878964]\n",
      "epoch:28 step:27091 [D loss: 0.678959, acc: 54.69%] [G loss: 1.745740]\n",
      "epoch:28 step:27092 [D loss: 0.700316, acc: 59.38%] [G loss: 1.794074]\n",
      "epoch:28 step:27093 [D loss: 0.624625, acc: 62.50%] [G loss: 1.856423]\n",
      "epoch:28 step:27094 [D loss: 0.697899, acc: 57.81%] [G loss: 1.729580]\n",
      "epoch:28 step:27095 [D loss: 0.767138, acc: 47.66%] [G loss: 1.714900]\n",
      "epoch:28 step:27096 [D loss: 0.635784, acc: 64.06%] [G loss: 1.840683]\n",
      "epoch:28 step:27097 [D loss: 0.682609, acc: 50.00%] [G loss: 1.814925]\n",
      "epoch:28 step:27098 [D loss: 0.673741, acc: 60.16%] [G loss: 1.797789]\n",
      "epoch:28 step:27099 [D loss: 0.630982, acc: 63.28%] [G loss: 1.938994]\n",
      "epoch:28 step:27100 [D loss: 0.677412, acc: 59.38%] [G loss: 1.785449]\n",
      "epoch:28 step:27101 [D loss: 0.668255, acc: 58.59%] [G loss: 1.679246]\n",
      "epoch:28 step:27102 [D loss: 0.635740, acc: 67.97%] [G loss: 1.782106]\n",
      "epoch:28 step:27103 [D loss: 0.660384, acc: 60.16%] [G loss: 1.673974]\n",
      "epoch:28 step:27104 [D loss: 0.651081, acc: 63.28%] [G loss: 1.845392]\n",
      "epoch:28 step:27105 [D loss: 0.651503, acc: 61.72%] [G loss: 1.729217]\n",
      "epoch:28 step:27106 [D loss: 0.671017, acc: 57.03%] [G loss: 1.844017]\n",
      "epoch:28 step:27107 [D loss: 0.668903, acc: 58.59%] [G loss: 1.776313]\n",
      "epoch:28 step:27108 [D loss: 0.649244, acc: 63.28%] [G loss: 1.765671]\n",
      "epoch:28 step:27109 [D loss: 0.691178, acc: 53.91%] [G loss: 1.780566]\n",
      "epoch:28 step:27110 [D loss: 0.690452, acc: 54.69%] [G loss: 1.893205]\n",
      "epoch:28 step:27111 [D loss: 0.683244, acc: 55.47%] [G loss: 1.891193]\n",
      "epoch:28 step:27112 [D loss: 0.643029, acc: 65.62%] [G loss: 1.808669]\n",
      "epoch:28 step:27113 [D loss: 0.690938, acc: 55.47%] [G loss: 1.744471]\n",
      "epoch:28 step:27114 [D loss: 0.633952, acc: 66.41%] [G loss: 1.898755]\n",
      "epoch:28 step:27115 [D loss: 0.655257, acc: 59.38%] [G loss: 1.738274]\n",
      "epoch:28 step:27116 [D loss: 0.661163, acc: 60.94%] [G loss: 1.695577]\n",
      "epoch:28 step:27117 [D loss: 0.661008, acc: 60.94%] [G loss: 1.816482]\n",
      "epoch:28 step:27118 [D loss: 0.669228, acc: 55.47%] [G loss: 1.867678]\n",
      "epoch:28 step:27119 [D loss: 0.664663, acc: 53.91%] [G loss: 1.764130]\n",
      "epoch:28 step:27120 [D loss: 0.650433, acc: 63.28%] [G loss: 1.941730]\n",
      "epoch:28 step:27121 [D loss: 0.627731, acc: 64.06%] [G loss: 1.872755]\n",
      "epoch:28 step:27122 [D loss: 0.642665, acc: 64.06%] [G loss: 1.992814]\n",
      "epoch:28 step:27123 [D loss: 0.694636, acc: 53.12%] [G loss: 1.776639]\n",
      "epoch:28 step:27124 [D loss: 0.660948, acc: 58.59%] [G loss: 1.854516]\n",
      "epoch:28 step:27125 [D loss: 0.656105, acc: 62.50%] [G loss: 1.870719]\n",
      "epoch:28 step:27126 [D loss: 0.615963, acc: 64.06%] [G loss: 1.874523]\n",
      "epoch:28 step:27127 [D loss: 0.695766, acc: 56.25%] [G loss: 1.728911]\n",
      "epoch:28 step:27128 [D loss: 0.595790, acc: 65.62%] [G loss: 1.888345]\n",
      "epoch:28 step:27129 [D loss: 0.684477, acc: 60.94%] [G loss: 1.865060]\n",
      "epoch:28 step:27130 [D loss: 0.655357, acc: 61.72%] [G loss: 1.924319]\n",
      "epoch:28 step:27131 [D loss: 0.642721, acc: 58.59%] [G loss: 1.811876]\n",
      "epoch:28 step:27132 [D loss: 0.668685, acc: 57.03%] [G loss: 1.679795]\n",
      "epoch:28 step:27133 [D loss: 0.652584, acc: 60.94%] [G loss: 1.911272]\n",
      "epoch:28 step:27134 [D loss: 0.607124, acc: 64.06%] [G loss: 1.960288]\n",
      "epoch:28 step:27135 [D loss: 0.644942, acc: 62.50%] [G loss: 1.926699]\n",
      "epoch:28 step:27136 [D loss: 0.644854, acc: 64.84%] [G loss: 1.860667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:27137 [D loss: 0.601839, acc: 71.88%] [G loss: 1.872913]\n",
      "epoch:28 step:27138 [D loss: 0.699827, acc: 54.69%] [G loss: 1.905806]\n",
      "epoch:28 step:27139 [D loss: 0.679022, acc: 57.81%] [G loss: 2.030258]\n",
      "epoch:28 step:27140 [D loss: 0.669733, acc: 61.72%] [G loss: 1.866474]\n",
      "epoch:28 step:27141 [D loss: 0.666148, acc: 58.59%] [G loss: 2.021260]\n",
      "epoch:28 step:27142 [D loss: 0.658822, acc: 60.16%] [G loss: 1.996616]\n",
      "epoch:28 step:27143 [D loss: 0.665561, acc: 61.72%] [G loss: 1.875764]\n",
      "epoch:28 step:27144 [D loss: 0.654399, acc: 67.97%] [G loss: 1.958944]\n",
      "epoch:28 step:27145 [D loss: 0.585152, acc: 67.19%] [G loss: 1.955173]\n",
      "epoch:28 step:27146 [D loss: 0.643775, acc: 63.28%] [G loss: 1.894112]\n",
      "epoch:28 step:27147 [D loss: 0.627936, acc: 66.41%] [G loss: 2.047443]\n",
      "epoch:28 step:27148 [D loss: 0.673693, acc: 57.03%] [G loss: 2.166249]\n",
      "epoch:28 step:27149 [D loss: 0.668719, acc: 59.38%] [G loss: 1.816542]\n",
      "epoch:28 step:27150 [D loss: 0.680528, acc: 59.38%] [G loss: 1.825229]\n",
      "epoch:28 step:27151 [D loss: 0.695892, acc: 58.59%] [G loss: 1.874932]\n",
      "epoch:28 step:27152 [D loss: 0.642547, acc: 57.81%] [G loss: 1.962793]\n",
      "epoch:28 step:27153 [D loss: 0.656564, acc: 62.50%] [G loss: 1.860384]\n",
      "epoch:28 step:27154 [D loss: 0.602451, acc: 67.97%] [G loss: 2.027962]\n",
      "epoch:28 step:27155 [D loss: 0.599161, acc: 68.75%] [G loss: 2.227005]\n",
      "epoch:28 step:27156 [D loss: 0.758143, acc: 52.34%] [G loss: 1.791604]\n",
      "epoch:28 step:27157 [D loss: 0.608362, acc: 64.06%] [G loss: 2.014777]\n",
      "epoch:28 step:27158 [D loss: 0.607096, acc: 68.75%] [G loss: 1.885342]\n",
      "epoch:28 step:27159 [D loss: 0.614539, acc: 66.41%] [G loss: 2.047682]\n",
      "epoch:28 step:27160 [D loss: 0.603007, acc: 68.75%] [G loss: 2.071581]\n",
      "epoch:28 step:27161 [D loss: 0.652475, acc: 63.28%] [G loss: 2.058373]\n",
      "epoch:28 step:27162 [D loss: 0.606168, acc: 62.50%] [G loss: 2.079890]\n",
      "epoch:28 step:27163 [D loss: 0.697657, acc: 55.47%] [G loss: 1.910487]\n",
      "epoch:28 step:27164 [D loss: 0.747045, acc: 52.34%] [G loss: 1.737591]\n",
      "epoch:28 step:27165 [D loss: 0.716514, acc: 54.69%] [G loss: 1.848073]\n",
      "epoch:28 step:27166 [D loss: 0.571239, acc: 72.66%] [G loss: 2.015837]\n",
      "epoch:28 step:27167 [D loss: 0.665730, acc: 59.38%] [G loss: 1.999563]\n",
      "epoch:28 step:27168 [D loss: 0.655173, acc: 56.25%] [G loss: 1.795017]\n",
      "epoch:28 step:27169 [D loss: 0.631237, acc: 65.62%] [G loss: 1.959616]\n",
      "epoch:28 step:27170 [D loss: 0.610460, acc: 66.41%] [G loss: 1.929691]\n",
      "epoch:28 step:27171 [D loss: 0.624543, acc: 62.50%] [G loss: 1.983257]\n",
      "epoch:28 step:27172 [D loss: 0.599490, acc: 73.44%] [G loss: 2.018793]\n",
      "epoch:28 step:27173 [D loss: 0.582289, acc: 72.66%] [G loss: 2.321924]\n",
      "epoch:29 step:27174 [D loss: 0.673164, acc: 60.16%] [G loss: 2.012648]\n",
      "epoch:29 step:27175 [D loss: 0.660658, acc: 58.59%] [G loss: 1.953971]\n",
      "epoch:29 step:27176 [D loss: 0.684526, acc: 57.81%] [G loss: 1.913062]\n",
      "epoch:29 step:27177 [D loss: 0.613650, acc: 63.28%] [G loss: 1.877023]\n",
      "epoch:29 step:27178 [D loss: 0.666034, acc: 63.28%] [G loss: 1.852478]\n",
      "epoch:29 step:27179 [D loss: 0.603366, acc: 66.41%] [G loss: 2.017477]\n",
      "epoch:29 step:27180 [D loss: 0.632514, acc: 62.50%] [G loss: 1.960799]\n",
      "epoch:29 step:27181 [D loss: 0.677427, acc: 60.94%] [G loss: 1.861817]\n",
      "epoch:29 step:27182 [D loss: 0.613731, acc: 63.28%] [G loss: 1.944583]\n",
      "epoch:29 step:27183 [D loss: 0.604112, acc: 71.88%] [G loss: 2.045857]\n",
      "epoch:29 step:27184 [D loss: 0.677889, acc: 57.03%] [G loss: 1.933703]\n",
      "epoch:29 step:27185 [D loss: 0.618579, acc: 65.62%] [G loss: 1.796786]\n",
      "epoch:29 step:27186 [D loss: 0.609954, acc: 60.94%] [G loss: 1.980527]\n",
      "epoch:29 step:27187 [D loss: 0.607640, acc: 68.75%] [G loss: 1.892558]\n",
      "epoch:29 step:27188 [D loss: 0.557883, acc: 74.22%] [G loss: 2.010638]\n",
      "epoch:29 step:27189 [D loss: 0.618830, acc: 67.19%] [G loss: 2.024457]\n",
      "epoch:29 step:27190 [D loss: 0.627243, acc: 67.19%] [G loss: 2.079418]\n",
      "epoch:29 step:27191 [D loss: 0.642067, acc: 57.03%] [G loss: 1.954602]\n",
      "epoch:29 step:27192 [D loss: 0.645531, acc: 65.62%] [G loss: 1.904780]\n",
      "epoch:29 step:27193 [D loss: 0.774265, acc: 44.53%] [G loss: 1.749250]\n",
      "epoch:29 step:27194 [D loss: 0.706777, acc: 52.34%] [G loss: 1.753426]\n",
      "epoch:29 step:27195 [D loss: 0.693836, acc: 57.81%] [G loss: 1.872343]\n",
      "epoch:29 step:27196 [D loss: 0.612450, acc: 71.09%] [G loss: 1.864630]\n",
      "epoch:29 step:27197 [D loss: 0.612847, acc: 64.84%] [G loss: 1.963657]\n",
      "epoch:29 step:27198 [D loss: 0.631237, acc: 64.06%] [G loss: 2.055333]\n",
      "epoch:29 step:27199 [D loss: 0.648114, acc: 67.19%] [G loss: 1.843233]\n",
      "epoch:29 step:27200 [D loss: 0.666110, acc: 62.50%] [G loss: 1.861566]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.711686\n",
      "FID: 9.178050\n",
      "0 = 12.605778198385206\n",
      "1 = 0.08722007115729215\n",
      "2 = 0.8716999888420105\n",
      "3 = 0.8813999891281128\n",
      "4 = 0.8619999885559082\n",
      "5 = 0.8646262288093567\n",
      "6 = 0.8813999891281128\n",
      "7 = 5.9751216561734735\n",
      "8 = 0.0565258046190374\n",
      "9 = 0.7049000263214111\n",
      "10 = 0.7138000130653381\n",
      "11 = 0.6959999799728394\n",
      "12 = 0.7013165354728699\n",
      "13 = 0.7138000130653381\n",
      "14 = 7.711716651916504\n",
      "15 = 9.499054908752441\n",
      "16 = 0.09181510657072067\n",
      "17 = 7.711685657501221\n",
      "18 = 9.17805004119873\n",
      "epoch:29 step:27201 [D loss: 0.650860, acc: 60.94%] [G loss: 1.794626]\n",
      "epoch:29 step:27202 [D loss: 0.572256, acc: 72.66%] [G loss: 1.882892]\n",
      "epoch:29 step:27203 [D loss: 0.666882, acc: 57.81%] [G loss: 1.913556]\n",
      "epoch:29 step:27204 [D loss: 0.672703, acc: 55.47%] [G loss: 1.701352]\n",
      "epoch:29 step:27205 [D loss: 0.691613, acc: 57.03%] [G loss: 1.805597]\n",
      "epoch:29 step:27206 [D loss: 0.644817, acc: 64.06%] [G loss: 1.760727]\n",
      "epoch:29 step:27207 [D loss: 0.636217, acc: 64.06%] [G loss: 1.926137]\n",
      "epoch:29 step:27208 [D loss: 0.617802, acc: 67.19%] [G loss: 1.966787]\n",
      "epoch:29 step:27209 [D loss: 0.629953, acc: 62.50%] [G loss: 1.879587]\n",
      "epoch:29 step:27210 [D loss: 0.625120, acc: 64.06%] [G loss: 2.029735]\n",
      "epoch:29 step:27211 [D loss: 0.616350, acc: 64.06%] [G loss: 1.966895]\n",
      "epoch:29 step:27212 [D loss: 0.618860, acc: 64.06%] [G loss: 1.848516]\n",
      "epoch:29 step:27213 [D loss: 0.591572, acc: 67.97%] [G loss: 1.979432]\n",
      "epoch:29 step:27214 [D loss: 0.634038, acc: 64.06%] [G loss: 1.801201]\n",
      "epoch:29 step:27215 [D loss: 0.630187, acc: 63.28%] [G loss: 1.974630]\n",
      "epoch:29 step:27216 [D loss: 0.647771, acc: 61.72%] [G loss: 1.973000]\n",
      "epoch:29 step:27217 [D loss: 0.652956, acc: 62.50%] [G loss: 1.812302]\n",
      "epoch:29 step:27218 [D loss: 0.627898, acc: 65.62%] [G loss: 1.846478]\n",
      "epoch:29 step:27219 [D loss: 0.636992, acc: 58.59%] [G loss: 1.895418]\n",
      "epoch:29 step:27220 [D loss: 0.638145, acc: 63.28%] [G loss: 1.875115]\n",
      "epoch:29 step:27221 [D loss: 0.621865, acc: 62.50%] [G loss: 2.016415]\n",
      "epoch:29 step:27222 [D loss: 0.660985, acc: 64.06%] [G loss: 2.045067]\n",
      "epoch:29 step:27223 [D loss: 0.622568, acc: 61.72%] [G loss: 1.944203]\n",
      "epoch:29 step:27224 [D loss: 0.670310, acc: 62.50%] [G loss: 1.902690]\n",
      "epoch:29 step:27225 [D loss: 0.651752, acc: 63.28%] [G loss: 1.869607]\n",
      "epoch:29 step:27226 [D loss: 0.614762, acc: 66.41%] [G loss: 1.980053]\n",
      "epoch:29 step:27227 [D loss: 0.666761, acc: 60.94%] [G loss: 1.927586]\n",
      "epoch:29 step:27228 [D loss: 0.576292, acc: 70.31%] [G loss: 2.128999]\n",
      "epoch:29 step:27229 [D loss: 0.604372, acc: 70.31%] [G loss: 1.980922]\n",
      "epoch:29 step:27230 [D loss: 0.672751, acc: 54.69%] [G loss: 2.057762]\n",
      "epoch:29 step:27231 [D loss: 0.682062, acc: 62.50%] [G loss: 1.805437]\n",
      "epoch:29 step:27232 [D loss: 0.644659, acc: 60.94%] [G loss: 1.900160]\n",
      "epoch:29 step:27233 [D loss: 0.673155, acc: 61.72%] [G loss: 1.989578]\n",
      "epoch:29 step:27234 [D loss: 0.664699, acc: 60.94%] [G loss: 1.822104]\n",
      "epoch:29 step:27235 [D loss: 0.649563, acc: 66.41%] [G loss: 1.769157]\n",
      "epoch:29 step:27236 [D loss: 0.670819, acc: 58.59%] [G loss: 1.820808]\n",
      "epoch:29 step:27237 [D loss: 0.670787, acc: 64.06%] [G loss: 1.874754]\n",
      "epoch:29 step:27238 [D loss: 0.641596, acc: 60.94%] [G loss: 1.885280]\n",
      "epoch:29 step:27239 [D loss: 0.658538, acc: 60.94%] [G loss: 1.810563]\n",
      "epoch:29 step:27240 [D loss: 0.603495, acc: 70.31%] [G loss: 1.956497]\n",
      "epoch:29 step:27241 [D loss: 0.632656, acc: 64.84%] [G loss: 1.920665]\n",
      "epoch:29 step:27242 [D loss: 0.640526, acc: 64.06%] [G loss: 2.015808]\n",
      "epoch:29 step:27243 [D loss: 0.654533, acc: 64.06%] [G loss: 1.900670]\n",
      "epoch:29 step:27244 [D loss: 0.697583, acc: 55.47%] [G loss: 1.850957]\n",
      "epoch:29 step:27245 [D loss: 0.657311, acc: 62.50%] [G loss: 1.809709]\n",
      "epoch:29 step:27246 [D loss: 0.718361, acc: 54.69%] [G loss: 1.672034]\n",
      "epoch:29 step:27247 [D loss: 0.629426, acc: 60.94%] [G loss: 1.963542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27248 [D loss: 0.631268, acc: 61.72%] [G loss: 1.946970]\n",
      "epoch:29 step:27249 [D loss: 0.656019, acc: 64.84%] [G loss: 1.946960]\n",
      "epoch:29 step:27250 [D loss: 0.622153, acc: 68.75%] [G loss: 1.904918]\n",
      "epoch:29 step:27251 [D loss: 0.707476, acc: 54.69%] [G loss: 1.896811]\n",
      "epoch:29 step:27252 [D loss: 0.660747, acc: 65.62%] [G loss: 1.687516]\n",
      "epoch:29 step:27253 [D loss: 0.665409, acc: 58.59%] [G loss: 1.832928]\n",
      "epoch:29 step:27254 [D loss: 0.687643, acc: 56.25%] [G loss: 1.781303]\n",
      "epoch:29 step:27255 [D loss: 0.711895, acc: 51.56%] [G loss: 1.714730]\n",
      "epoch:29 step:27256 [D loss: 0.633213, acc: 65.62%] [G loss: 1.803155]\n",
      "epoch:29 step:27257 [D loss: 0.624997, acc: 64.06%] [G loss: 1.831591]\n",
      "epoch:29 step:27258 [D loss: 0.613077, acc: 67.97%] [G loss: 1.798308]\n",
      "epoch:29 step:27259 [D loss: 0.692711, acc: 56.25%] [G loss: 1.751497]\n",
      "epoch:29 step:27260 [D loss: 0.691863, acc: 56.25%] [G loss: 1.746456]\n",
      "epoch:29 step:27261 [D loss: 0.640435, acc: 61.72%] [G loss: 1.811692]\n",
      "epoch:29 step:27262 [D loss: 0.668376, acc: 63.28%] [G loss: 1.800949]\n",
      "epoch:29 step:27263 [D loss: 0.622538, acc: 67.97%] [G loss: 1.827869]\n",
      "epoch:29 step:27264 [D loss: 0.650646, acc: 62.50%] [G loss: 1.725050]\n",
      "epoch:29 step:27265 [D loss: 0.638432, acc: 64.06%] [G loss: 1.797373]\n",
      "epoch:29 step:27266 [D loss: 0.635126, acc: 58.59%] [G loss: 1.916821]\n",
      "epoch:29 step:27267 [D loss: 0.643482, acc: 63.28%] [G loss: 1.842976]\n",
      "epoch:29 step:27268 [D loss: 0.641031, acc: 60.16%] [G loss: 1.768546]\n",
      "epoch:29 step:27269 [D loss: 0.657072, acc: 67.19%] [G loss: 1.880264]\n",
      "epoch:29 step:27270 [D loss: 0.700415, acc: 51.56%] [G loss: 1.900270]\n",
      "epoch:29 step:27271 [D loss: 0.675349, acc: 64.84%] [G loss: 1.713476]\n",
      "epoch:29 step:27272 [D loss: 0.635793, acc: 62.50%] [G loss: 1.787982]\n",
      "epoch:29 step:27273 [D loss: 0.632038, acc: 60.94%] [G loss: 1.973969]\n",
      "epoch:29 step:27274 [D loss: 0.663886, acc: 62.50%] [G loss: 1.871277]\n",
      "epoch:29 step:27275 [D loss: 0.636387, acc: 69.53%] [G loss: 1.975691]\n",
      "epoch:29 step:27276 [D loss: 0.681571, acc: 54.69%] [G loss: 1.833551]\n",
      "epoch:29 step:27277 [D loss: 0.651168, acc: 62.50%] [G loss: 1.774940]\n",
      "epoch:29 step:27278 [D loss: 0.675215, acc: 60.16%] [G loss: 1.735573]\n",
      "epoch:29 step:27279 [D loss: 0.681277, acc: 58.59%] [G loss: 1.846441]\n",
      "epoch:29 step:27280 [D loss: 0.642469, acc: 64.06%] [G loss: 2.076931]\n",
      "epoch:29 step:27281 [D loss: 0.744000, acc: 48.44%] [G loss: 1.744981]\n",
      "epoch:29 step:27282 [D loss: 0.633115, acc: 67.19%] [G loss: 1.788522]\n",
      "epoch:29 step:27283 [D loss: 0.627085, acc: 64.84%] [G loss: 1.730008]\n",
      "epoch:29 step:27284 [D loss: 0.640779, acc: 61.72%] [G loss: 1.936464]\n",
      "epoch:29 step:27285 [D loss: 0.651110, acc: 57.81%] [G loss: 1.957178]\n",
      "epoch:29 step:27286 [D loss: 0.591082, acc: 67.19%] [G loss: 2.143776]\n",
      "epoch:29 step:27287 [D loss: 0.598323, acc: 73.44%] [G loss: 2.132122]\n",
      "epoch:29 step:27288 [D loss: 0.585069, acc: 70.31%] [G loss: 2.189567]\n",
      "epoch:29 step:27289 [D loss: 0.664911, acc: 64.06%] [G loss: 2.061131]\n",
      "epoch:29 step:27290 [D loss: 0.652620, acc: 61.72%] [G loss: 2.073811]\n",
      "epoch:29 step:27291 [D loss: 0.587926, acc: 74.22%] [G loss: 1.938765]\n",
      "epoch:29 step:27292 [D loss: 0.609602, acc: 64.84%] [G loss: 2.212325]\n",
      "epoch:29 step:27293 [D loss: 0.634085, acc: 61.72%] [G loss: 1.933477]\n",
      "epoch:29 step:27294 [D loss: 0.630171, acc: 68.75%] [G loss: 2.021574]\n",
      "epoch:29 step:27295 [D loss: 0.643694, acc: 58.59%] [G loss: 2.168636]\n",
      "epoch:29 step:27296 [D loss: 0.620557, acc: 64.06%] [G loss: 1.811466]\n",
      "epoch:29 step:27297 [D loss: 0.692298, acc: 58.59%] [G loss: 2.097197]\n",
      "epoch:29 step:27298 [D loss: 0.659995, acc: 64.84%] [G loss: 1.785532]\n",
      "epoch:29 step:27299 [D loss: 0.632024, acc: 64.06%] [G loss: 1.972253]\n",
      "epoch:29 step:27300 [D loss: 0.613502, acc: 61.72%] [G loss: 1.939798]\n",
      "epoch:29 step:27301 [D loss: 0.665546, acc: 57.81%] [G loss: 1.875290]\n",
      "epoch:29 step:27302 [D loss: 0.620609, acc: 69.53%] [G loss: 1.867186]\n",
      "epoch:29 step:27303 [D loss: 0.577537, acc: 69.53%] [G loss: 1.809898]\n",
      "epoch:29 step:27304 [D loss: 0.602001, acc: 67.97%] [G loss: 1.905249]\n",
      "epoch:29 step:27305 [D loss: 0.663108, acc: 59.38%] [G loss: 1.834568]\n",
      "epoch:29 step:27306 [D loss: 0.710565, acc: 56.25%] [G loss: 1.781394]\n",
      "epoch:29 step:27307 [D loss: 0.669159, acc: 55.47%] [G loss: 1.733451]\n",
      "epoch:29 step:27308 [D loss: 0.676610, acc: 60.16%] [G loss: 1.866920]\n",
      "epoch:29 step:27309 [D loss: 0.668844, acc: 63.28%] [G loss: 1.756274]\n",
      "epoch:29 step:27310 [D loss: 0.685584, acc: 57.03%] [G loss: 1.797959]\n",
      "epoch:29 step:27311 [D loss: 0.694599, acc: 53.12%] [G loss: 1.827456]\n",
      "epoch:29 step:27312 [D loss: 0.709998, acc: 58.59%] [G loss: 1.691186]\n",
      "epoch:29 step:27313 [D loss: 0.628304, acc: 61.72%] [G loss: 1.728984]\n",
      "epoch:29 step:27314 [D loss: 0.699913, acc: 60.16%] [G loss: 1.843283]\n",
      "epoch:29 step:27315 [D loss: 0.675183, acc: 57.03%] [G loss: 1.747581]\n",
      "epoch:29 step:27316 [D loss: 0.716762, acc: 56.25%] [G loss: 1.757833]\n",
      "epoch:29 step:27317 [D loss: 0.583818, acc: 71.09%] [G loss: 1.904546]\n",
      "epoch:29 step:27318 [D loss: 0.642196, acc: 68.75%] [G loss: 1.908618]\n",
      "epoch:29 step:27319 [D loss: 0.707133, acc: 50.78%] [G loss: 1.989624]\n",
      "epoch:29 step:27320 [D loss: 0.684942, acc: 60.16%] [G loss: 1.841614]\n",
      "epoch:29 step:27321 [D loss: 0.681962, acc: 57.81%] [G loss: 1.791698]\n",
      "epoch:29 step:27322 [D loss: 0.656505, acc: 59.38%] [G loss: 1.815182]\n",
      "epoch:29 step:27323 [D loss: 0.660911, acc: 62.50%] [G loss: 1.787048]\n",
      "epoch:29 step:27324 [D loss: 0.651810, acc: 67.19%] [G loss: 1.907966]\n",
      "epoch:29 step:27325 [D loss: 0.616453, acc: 64.06%] [G loss: 1.856763]\n",
      "epoch:29 step:27326 [D loss: 0.657325, acc: 64.84%] [G loss: 1.846959]\n",
      "epoch:29 step:27327 [D loss: 0.634388, acc: 58.59%] [G loss: 1.948593]\n",
      "epoch:29 step:27328 [D loss: 0.700831, acc: 58.59%] [G loss: 1.814590]\n",
      "epoch:29 step:27329 [D loss: 0.659518, acc: 58.59%] [G loss: 1.832330]\n",
      "epoch:29 step:27330 [D loss: 0.656693, acc: 60.16%] [G loss: 1.761397]\n",
      "epoch:29 step:27331 [D loss: 0.668930, acc: 60.94%] [G loss: 1.836346]\n",
      "epoch:29 step:27332 [D loss: 0.676464, acc: 58.59%] [G loss: 1.728072]\n",
      "epoch:29 step:27333 [D loss: 0.691233, acc: 55.47%] [G loss: 1.681525]\n",
      "epoch:29 step:27334 [D loss: 0.679955, acc: 54.69%] [G loss: 1.748948]\n",
      "epoch:29 step:27335 [D loss: 0.669640, acc: 57.03%] [G loss: 1.696396]\n",
      "epoch:29 step:27336 [D loss: 0.654054, acc: 60.94%] [G loss: 1.815175]\n",
      "epoch:29 step:27337 [D loss: 0.655936, acc: 60.16%] [G loss: 1.799685]\n",
      "epoch:29 step:27338 [D loss: 0.740048, acc: 48.44%] [G loss: 1.774576]\n",
      "epoch:29 step:27339 [D loss: 0.697148, acc: 56.25%] [G loss: 1.702601]\n",
      "epoch:29 step:27340 [D loss: 0.635781, acc: 60.16%] [G loss: 1.749710]\n",
      "epoch:29 step:27341 [D loss: 0.697005, acc: 58.59%] [G loss: 1.822674]\n",
      "epoch:29 step:27342 [D loss: 0.701385, acc: 62.50%] [G loss: 1.786083]\n",
      "epoch:29 step:27343 [D loss: 0.652119, acc: 61.72%] [G loss: 1.722689]\n",
      "epoch:29 step:27344 [D loss: 0.615413, acc: 69.53%] [G loss: 1.741122]\n",
      "epoch:29 step:27345 [D loss: 0.677494, acc: 57.03%] [G loss: 1.839375]\n",
      "epoch:29 step:27346 [D loss: 0.668169, acc: 65.62%] [G loss: 1.651929]\n",
      "epoch:29 step:27347 [D loss: 0.636273, acc: 65.62%] [G loss: 1.724499]\n",
      "epoch:29 step:27348 [D loss: 0.746734, acc: 57.03%] [G loss: 1.696142]\n",
      "epoch:29 step:27349 [D loss: 0.631762, acc: 67.97%] [G loss: 1.813507]\n",
      "epoch:29 step:27350 [D loss: 0.689757, acc: 53.91%] [G loss: 1.679928]\n",
      "epoch:29 step:27351 [D loss: 0.656623, acc: 57.81%] [G loss: 1.800671]\n",
      "epoch:29 step:27352 [D loss: 0.599955, acc: 67.19%] [G loss: 1.777204]\n",
      "epoch:29 step:27353 [D loss: 0.705530, acc: 53.91%] [G loss: 1.730544]\n",
      "epoch:29 step:27354 [D loss: 0.679825, acc: 61.72%] [G loss: 1.757627]\n",
      "epoch:29 step:27355 [D loss: 0.676973, acc: 60.94%] [G loss: 1.769577]\n",
      "epoch:29 step:27356 [D loss: 0.666320, acc: 55.47%] [G loss: 1.708859]\n",
      "epoch:29 step:27357 [D loss: 0.641503, acc: 62.50%] [G loss: 1.863914]\n",
      "epoch:29 step:27358 [D loss: 0.627192, acc: 62.50%] [G loss: 1.715180]\n",
      "epoch:29 step:27359 [D loss: 0.645368, acc: 60.16%] [G loss: 1.829854]\n",
      "epoch:29 step:27360 [D loss: 0.673114, acc: 57.03%] [G loss: 1.834772]\n",
      "epoch:29 step:27361 [D loss: 0.656792, acc: 58.59%] [G loss: 1.913010]\n",
      "epoch:29 step:27362 [D loss: 0.661884, acc: 63.28%] [G loss: 1.727459]\n",
      "epoch:29 step:27363 [D loss: 0.620224, acc: 60.94%] [G loss: 1.826735]\n",
      "epoch:29 step:27364 [D loss: 0.654948, acc: 57.81%] [G loss: 1.815518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27365 [D loss: 0.670402, acc: 59.38%] [G loss: 1.708485]\n",
      "epoch:29 step:27366 [D loss: 0.639391, acc: 64.06%] [G loss: 1.788338]\n",
      "epoch:29 step:27367 [D loss: 0.633161, acc: 64.06%] [G loss: 1.882285]\n",
      "epoch:29 step:27368 [D loss: 0.602793, acc: 71.09%] [G loss: 1.879674]\n",
      "epoch:29 step:27369 [D loss: 0.672849, acc: 59.38%] [G loss: 1.830349]\n",
      "epoch:29 step:27370 [D loss: 0.663227, acc: 63.28%] [G loss: 1.948114]\n",
      "epoch:29 step:27371 [D loss: 0.622019, acc: 64.06%] [G loss: 1.779815]\n",
      "epoch:29 step:27372 [D loss: 0.611454, acc: 65.62%] [G loss: 1.896287]\n",
      "epoch:29 step:27373 [D loss: 0.701043, acc: 57.03%] [G loss: 1.831451]\n",
      "epoch:29 step:27374 [D loss: 0.674126, acc: 64.06%] [G loss: 1.848250]\n",
      "epoch:29 step:27375 [D loss: 0.661627, acc: 58.59%] [G loss: 1.831387]\n",
      "epoch:29 step:27376 [D loss: 0.639708, acc: 64.84%] [G loss: 1.886852]\n",
      "epoch:29 step:27377 [D loss: 0.677492, acc: 55.47%] [G loss: 1.857870]\n",
      "epoch:29 step:27378 [D loss: 0.680305, acc: 59.38%] [G loss: 1.856506]\n",
      "epoch:29 step:27379 [D loss: 0.634699, acc: 57.81%] [G loss: 1.892224]\n",
      "epoch:29 step:27380 [D loss: 0.592224, acc: 69.53%] [G loss: 1.937950]\n",
      "epoch:29 step:27381 [D loss: 0.595791, acc: 64.06%] [G loss: 2.010265]\n",
      "epoch:29 step:27382 [D loss: 0.600194, acc: 71.88%] [G loss: 2.199061]\n",
      "epoch:29 step:27383 [D loss: 0.684504, acc: 60.16%] [G loss: 1.780500]\n",
      "epoch:29 step:27384 [D loss: 0.628069, acc: 67.19%] [G loss: 1.756406]\n",
      "epoch:29 step:27385 [D loss: 0.665446, acc: 57.81%] [G loss: 1.763175]\n",
      "epoch:29 step:27386 [D loss: 0.663088, acc: 61.72%] [G loss: 1.833962]\n",
      "epoch:29 step:27387 [D loss: 0.675198, acc: 58.59%] [G loss: 1.748106]\n",
      "epoch:29 step:27388 [D loss: 0.683637, acc: 58.59%] [G loss: 1.896813]\n",
      "epoch:29 step:27389 [D loss: 0.617140, acc: 62.50%] [G loss: 1.924774]\n",
      "epoch:29 step:27390 [D loss: 0.685040, acc: 54.69%] [G loss: 1.859838]\n",
      "epoch:29 step:27391 [D loss: 0.642159, acc: 62.50%] [G loss: 2.043102]\n",
      "epoch:29 step:27392 [D loss: 0.609355, acc: 65.62%] [G loss: 1.996322]\n",
      "epoch:29 step:27393 [D loss: 0.712512, acc: 52.34%] [G loss: 1.661232]\n",
      "epoch:29 step:27394 [D loss: 0.638736, acc: 63.28%] [G loss: 1.894100]\n",
      "epoch:29 step:27395 [D loss: 0.630577, acc: 65.62%] [G loss: 1.830657]\n",
      "epoch:29 step:27396 [D loss: 0.637355, acc: 64.84%] [G loss: 1.915178]\n",
      "epoch:29 step:27397 [D loss: 0.619803, acc: 72.66%] [G loss: 1.942248]\n",
      "epoch:29 step:27398 [D loss: 0.642539, acc: 65.62%] [G loss: 1.870068]\n",
      "epoch:29 step:27399 [D loss: 0.676848, acc: 54.69%] [G loss: 1.843710]\n",
      "epoch:29 step:27400 [D loss: 0.669031, acc: 62.50%] [G loss: 1.888780]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.632894\n",
      "FID: 9.253789\n",
      "0 = 12.858146225261656\n",
      "1 = 0.10488958638798039\n",
      "2 = 0.8701000213623047\n",
      "3 = 0.8835999965667725\n",
      "4 = 0.8565999865531921\n",
      "5 = 0.8603699803352356\n",
      "6 = 0.8835999965667725\n",
      "7 = 5.967606948947912\n",
      "8 = 0.05769472239404974\n",
      "9 = 0.7003999948501587\n",
      "10 = 0.7116000056266785\n",
      "11 = 0.6891999840736389\n",
      "12 = 0.6960093975067139\n",
      "13 = 0.7116000056266785\n",
      "14 = 7.632918357849121\n",
      "15 = 9.45156192779541\n",
      "16 = 0.10544247180223465\n",
      "17 = 7.6328935623168945\n",
      "18 = 9.253788948059082\n",
      "epoch:29 step:27401 [D loss: 0.664719, acc: 58.59%] [G loss: 1.739440]\n",
      "epoch:29 step:27402 [D loss: 0.601958, acc: 67.19%] [G loss: 1.997841]\n",
      "epoch:29 step:27403 [D loss: 0.646841, acc: 57.03%] [G loss: 2.131053]\n",
      "epoch:29 step:27404 [D loss: 0.565958, acc: 64.06%] [G loss: 2.342320]\n",
      "epoch:29 step:27405 [D loss: 0.558947, acc: 73.44%] [G loss: 2.260084]\n",
      "epoch:29 step:27406 [D loss: 0.653962, acc: 64.06%] [G loss: 1.851417]\n",
      "epoch:29 step:27407 [D loss: 0.669393, acc: 56.25%] [G loss: 1.866083]\n",
      "epoch:29 step:27408 [D loss: 0.678883, acc: 57.81%] [G loss: 1.885164]\n",
      "epoch:29 step:27409 [D loss: 0.670069, acc: 63.28%] [G loss: 1.958062]\n",
      "epoch:29 step:27410 [D loss: 0.660798, acc: 62.50%] [G loss: 1.918119]\n",
      "epoch:29 step:27411 [D loss: 0.635852, acc: 60.16%] [G loss: 1.987296]\n",
      "epoch:29 step:27412 [D loss: 0.657403, acc: 61.72%] [G loss: 1.886951]\n",
      "epoch:29 step:27413 [D loss: 0.653988, acc: 59.38%] [G loss: 1.933911]\n",
      "epoch:29 step:27414 [D loss: 0.619387, acc: 65.62%] [G loss: 1.986986]\n",
      "epoch:29 step:27415 [D loss: 0.657122, acc: 58.59%] [G loss: 1.866364]\n",
      "epoch:29 step:27416 [D loss: 0.625802, acc: 64.84%] [G loss: 1.952221]\n",
      "epoch:29 step:27417 [D loss: 0.656126, acc: 64.06%] [G loss: 1.890255]\n",
      "epoch:29 step:27418 [D loss: 0.615925, acc: 68.75%] [G loss: 2.008462]\n",
      "epoch:29 step:27419 [D loss: 0.639649, acc: 60.16%] [G loss: 2.024371]\n",
      "epoch:29 step:27420 [D loss: 0.661062, acc: 60.16%] [G loss: 1.897514]\n",
      "epoch:29 step:27421 [D loss: 0.611818, acc: 64.84%] [G loss: 2.087648]\n",
      "epoch:29 step:27422 [D loss: 0.700834, acc: 56.25%] [G loss: 1.759571]\n",
      "epoch:29 step:27423 [D loss: 0.677548, acc: 59.38%] [G loss: 1.747695]\n",
      "epoch:29 step:27424 [D loss: 0.700238, acc: 59.38%] [G loss: 1.662398]\n",
      "epoch:29 step:27425 [D loss: 0.695444, acc: 56.25%] [G loss: 1.716363]\n",
      "epoch:29 step:27426 [D loss: 0.650416, acc: 58.59%] [G loss: 1.730711]\n",
      "epoch:29 step:27427 [D loss: 0.624668, acc: 64.06%] [G loss: 1.735574]\n",
      "epoch:29 step:27428 [D loss: 0.648042, acc: 62.50%] [G loss: 1.825266]\n",
      "epoch:29 step:27429 [D loss: 0.637411, acc: 67.97%] [G loss: 1.851622]\n",
      "epoch:29 step:27430 [D loss: 0.632012, acc: 62.50%] [G loss: 1.862836]\n",
      "epoch:29 step:27431 [D loss: 0.661909, acc: 60.94%] [G loss: 1.813430]\n",
      "epoch:29 step:27432 [D loss: 0.643330, acc: 65.62%] [G loss: 1.790890]\n",
      "epoch:29 step:27433 [D loss: 0.641229, acc: 67.97%] [G loss: 1.912798]\n",
      "epoch:29 step:27434 [D loss: 0.667463, acc: 60.16%] [G loss: 1.811428]\n",
      "epoch:29 step:27435 [D loss: 0.652912, acc: 65.62%] [G loss: 1.847918]\n",
      "epoch:29 step:27436 [D loss: 0.658936, acc: 60.16%] [G loss: 1.743594]\n",
      "epoch:29 step:27437 [D loss: 0.591506, acc: 67.97%] [G loss: 2.014496]\n",
      "epoch:29 step:27438 [D loss: 0.677348, acc: 57.81%] [G loss: 1.768641]\n",
      "epoch:29 step:27439 [D loss: 0.725552, acc: 50.00%] [G loss: 1.754458]\n",
      "epoch:29 step:27440 [D loss: 0.641971, acc: 61.72%] [G loss: 1.877211]\n",
      "epoch:29 step:27441 [D loss: 0.693226, acc: 60.94%] [G loss: 1.771898]\n",
      "epoch:29 step:27442 [D loss: 0.656181, acc: 60.94%] [G loss: 1.871433]\n",
      "epoch:29 step:27443 [D loss: 0.628052, acc: 66.41%] [G loss: 1.810674]\n",
      "epoch:29 step:27444 [D loss: 0.637309, acc: 67.97%] [G loss: 1.802841]\n",
      "epoch:29 step:27445 [D loss: 0.589247, acc: 72.66%] [G loss: 1.927640]\n",
      "epoch:29 step:27446 [D loss: 0.686541, acc: 54.69%] [G loss: 1.814274]\n",
      "epoch:29 step:27447 [D loss: 0.631315, acc: 61.72%] [G loss: 2.046534]\n",
      "epoch:29 step:27448 [D loss: 0.568874, acc: 70.31%] [G loss: 2.023168]\n",
      "epoch:29 step:27449 [D loss: 0.594405, acc: 68.75%] [G loss: 2.093961]\n",
      "epoch:29 step:27450 [D loss: 0.673956, acc: 60.16%] [G loss: 1.920244]\n",
      "epoch:29 step:27451 [D loss: 0.699771, acc: 57.81%] [G loss: 1.719388]\n",
      "epoch:29 step:27452 [D loss: 0.691376, acc: 50.00%] [G loss: 1.662887]\n",
      "epoch:29 step:27453 [D loss: 0.644894, acc: 61.72%] [G loss: 1.740041]\n",
      "epoch:29 step:27454 [D loss: 0.628018, acc: 66.41%] [G loss: 1.767235]\n",
      "epoch:29 step:27455 [D loss: 0.658991, acc: 64.84%] [G loss: 1.840290]\n",
      "epoch:29 step:27456 [D loss: 0.668735, acc: 61.72%] [G loss: 1.766505]\n",
      "epoch:29 step:27457 [D loss: 0.698288, acc: 54.69%] [G loss: 1.815210]\n",
      "epoch:29 step:27458 [D loss: 0.666226, acc: 58.59%] [G loss: 1.674372]\n",
      "epoch:29 step:27459 [D loss: 0.667288, acc: 57.81%] [G loss: 1.865195]\n",
      "epoch:29 step:27460 [D loss: 0.655612, acc: 64.84%] [G loss: 1.728956]\n",
      "epoch:29 step:27461 [D loss: 0.725054, acc: 55.47%] [G loss: 1.846624]\n",
      "epoch:29 step:27462 [D loss: 0.623049, acc: 60.94%] [G loss: 1.869289]\n",
      "epoch:29 step:27463 [D loss: 0.691914, acc: 62.50%] [G loss: 1.780854]\n",
      "epoch:29 step:27464 [D loss: 0.655614, acc: 60.94%] [G loss: 1.824972]\n",
      "epoch:29 step:27465 [D loss: 0.694610, acc: 51.56%] [G loss: 1.744496]\n",
      "epoch:29 step:27466 [D loss: 0.649603, acc: 62.50%] [G loss: 1.935054]\n",
      "epoch:29 step:27467 [D loss: 0.675009, acc: 54.69%] [G loss: 1.784125]\n",
      "epoch:29 step:27468 [D loss: 0.666173, acc: 63.28%] [G loss: 1.800720]\n",
      "epoch:29 step:27469 [D loss: 0.596047, acc: 69.53%] [G loss: 1.814450]\n",
      "epoch:29 step:27470 [D loss: 0.595617, acc: 69.53%] [G loss: 1.855338]\n",
      "epoch:29 step:27471 [D loss: 0.612023, acc: 64.84%] [G loss: 2.068217]\n",
      "epoch:29 step:27472 [D loss: 0.660346, acc: 64.06%] [G loss: 1.867003]\n",
      "epoch:29 step:27473 [D loss: 0.641713, acc: 68.75%] [G loss: 1.852503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27474 [D loss: 0.707417, acc: 57.03%] [G loss: 1.831401]\n",
      "epoch:29 step:27475 [D loss: 0.647568, acc: 59.38%] [G loss: 1.932420]\n",
      "epoch:29 step:27476 [D loss: 0.675311, acc: 60.94%] [G loss: 1.690703]\n",
      "epoch:29 step:27477 [D loss: 0.672102, acc: 64.06%] [G loss: 1.865193]\n",
      "epoch:29 step:27478 [D loss: 0.643816, acc: 61.72%] [G loss: 1.830524]\n",
      "epoch:29 step:27479 [D loss: 0.636815, acc: 64.06%] [G loss: 1.809983]\n",
      "epoch:29 step:27480 [D loss: 0.633427, acc: 63.28%] [G loss: 1.894082]\n",
      "epoch:29 step:27481 [D loss: 0.625231, acc: 63.28%] [G loss: 1.702495]\n",
      "epoch:29 step:27482 [D loss: 0.664368, acc: 67.97%] [G loss: 1.763571]\n",
      "epoch:29 step:27483 [D loss: 0.649989, acc: 71.09%] [G loss: 1.824471]\n",
      "epoch:29 step:27484 [D loss: 0.623492, acc: 59.38%] [G loss: 1.948291]\n",
      "epoch:29 step:27485 [D loss: 0.629145, acc: 67.19%] [G loss: 2.067670]\n",
      "epoch:29 step:27486 [D loss: 0.623098, acc: 67.19%] [G loss: 1.970461]\n",
      "epoch:29 step:27487 [D loss: 0.607974, acc: 65.62%] [G loss: 2.178807]\n",
      "epoch:29 step:27488 [D loss: 0.643503, acc: 62.50%] [G loss: 1.993307]\n",
      "epoch:29 step:27489 [D loss: 0.658043, acc: 64.84%] [G loss: 1.788583]\n",
      "epoch:29 step:27490 [D loss: 0.646434, acc: 64.06%] [G loss: 1.746238]\n",
      "epoch:29 step:27491 [D loss: 0.647991, acc: 58.59%] [G loss: 1.977668]\n",
      "epoch:29 step:27492 [D loss: 0.670907, acc: 60.94%] [G loss: 1.739105]\n",
      "epoch:29 step:27493 [D loss: 0.650340, acc: 63.28%] [G loss: 1.882671]\n",
      "epoch:29 step:27494 [D loss: 0.618953, acc: 62.50%] [G loss: 2.016502]\n",
      "epoch:29 step:27495 [D loss: 0.677376, acc: 60.16%] [G loss: 1.972798]\n",
      "epoch:29 step:27496 [D loss: 0.616843, acc: 69.53%] [G loss: 1.741121]\n",
      "epoch:29 step:27497 [D loss: 0.699259, acc: 52.34%] [G loss: 1.715559]\n",
      "epoch:29 step:27498 [D loss: 0.654635, acc: 64.84%] [G loss: 1.766033]\n",
      "epoch:29 step:27499 [D loss: 0.677284, acc: 61.72%] [G loss: 1.769565]\n",
      "epoch:29 step:27500 [D loss: 0.652031, acc: 61.72%] [G loss: 1.844346]\n",
      "epoch:29 step:27501 [D loss: 0.681024, acc: 59.38%] [G loss: 1.900291]\n",
      "epoch:29 step:27502 [D loss: 0.675997, acc: 55.47%] [G loss: 1.794759]\n",
      "epoch:29 step:27503 [D loss: 0.639261, acc: 67.19%] [G loss: 1.783586]\n",
      "epoch:29 step:27504 [D loss: 0.631493, acc: 64.84%] [G loss: 1.927715]\n",
      "epoch:29 step:27505 [D loss: 0.630545, acc: 64.84%] [G loss: 1.898408]\n",
      "epoch:29 step:27506 [D loss: 0.656458, acc: 60.94%] [G loss: 1.811857]\n",
      "epoch:29 step:27507 [D loss: 0.597185, acc: 65.62%] [G loss: 2.025020]\n",
      "epoch:29 step:27508 [D loss: 0.627696, acc: 65.62%] [G loss: 1.845581]\n",
      "epoch:29 step:27509 [D loss: 0.597704, acc: 68.75%] [G loss: 2.046339]\n",
      "epoch:29 step:27510 [D loss: 0.647864, acc: 59.38%] [G loss: 1.908289]\n",
      "epoch:29 step:27511 [D loss: 0.618269, acc: 64.84%] [G loss: 2.019125]\n",
      "epoch:29 step:27512 [D loss: 0.702560, acc: 57.81%] [G loss: 1.883420]\n",
      "epoch:29 step:27513 [D loss: 0.610971, acc: 64.06%] [G loss: 1.853457]\n",
      "epoch:29 step:27514 [D loss: 0.727510, acc: 47.66%] [G loss: 1.822466]\n",
      "epoch:29 step:27515 [D loss: 0.653530, acc: 62.50%] [G loss: 1.730992]\n",
      "epoch:29 step:27516 [D loss: 0.667515, acc: 60.16%] [G loss: 1.834376]\n",
      "epoch:29 step:27517 [D loss: 0.708248, acc: 60.94%] [G loss: 1.760386]\n",
      "epoch:29 step:27518 [D loss: 0.628084, acc: 65.62%] [G loss: 2.054094]\n",
      "epoch:29 step:27519 [D loss: 0.632744, acc: 57.03%] [G loss: 1.983915]\n",
      "epoch:29 step:27520 [D loss: 0.593381, acc: 73.44%] [G loss: 2.109493]\n",
      "epoch:29 step:27521 [D loss: 0.651168, acc: 61.72%] [G loss: 1.840577]\n",
      "epoch:29 step:27522 [D loss: 0.752995, acc: 42.19%] [G loss: 1.742066]\n",
      "epoch:29 step:27523 [D loss: 0.683316, acc: 59.38%] [G loss: 1.870520]\n",
      "epoch:29 step:27524 [D loss: 0.630467, acc: 64.84%] [G loss: 1.724418]\n",
      "epoch:29 step:27525 [D loss: 0.706108, acc: 50.00%] [G loss: 1.805870]\n",
      "epoch:29 step:27526 [D loss: 0.699318, acc: 56.25%] [G loss: 1.894040]\n",
      "epoch:29 step:27527 [D loss: 0.651372, acc: 61.72%] [G loss: 2.074676]\n",
      "epoch:29 step:27528 [D loss: 0.677006, acc: 58.59%] [G loss: 1.692324]\n",
      "epoch:29 step:27529 [D loss: 0.665486, acc: 59.38%] [G loss: 1.885334]\n",
      "epoch:29 step:27530 [D loss: 0.662445, acc: 60.94%] [G loss: 1.783865]\n",
      "epoch:29 step:27531 [D loss: 0.612827, acc: 63.28%] [G loss: 1.936237]\n",
      "epoch:29 step:27532 [D loss: 0.622437, acc: 62.50%] [G loss: 2.013440]\n",
      "epoch:29 step:27533 [D loss: 0.626874, acc: 64.06%] [G loss: 2.030798]\n",
      "epoch:29 step:27534 [D loss: 0.644900, acc: 63.28%] [G loss: 1.831389]\n",
      "epoch:29 step:27535 [D loss: 0.657944, acc: 61.72%] [G loss: 1.762906]\n",
      "epoch:29 step:27536 [D loss: 0.626303, acc: 70.31%] [G loss: 1.934609]\n",
      "epoch:29 step:27537 [D loss: 0.695356, acc: 57.81%] [G loss: 1.850992]\n",
      "epoch:29 step:27538 [D loss: 0.651853, acc: 61.72%] [G loss: 1.873024]\n",
      "epoch:29 step:27539 [D loss: 0.678522, acc: 63.28%] [G loss: 1.822312]\n",
      "epoch:29 step:27540 [D loss: 0.649078, acc: 61.72%] [G loss: 2.077125]\n",
      "epoch:29 step:27541 [D loss: 0.664646, acc: 58.59%] [G loss: 1.778104]\n",
      "epoch:29 step:27542 [D loss: 0.578435, acc: 73.44%] [G loss: 1.915225]\n",
      "epoch:29 step:27543 [D loss: 0.642818, acc: 65.62%] [G loss: 2.005383]\n",
      "epoch:29 step:27544 [D loss: 0.669656, acc: 55.47%] [G loss: 1.982158]\n",
      "epoch:29 step:27545 [D loss: 0.619698, acc: 66.41%] [G loss: 1.949232]\n",
      "epoch:29 step:27546 [D loss: 0.657069, acc: 64.84%] [G loss: 1.765179]\n",
      "epoch:29 step:27547 [D loss: 0.628711, acc: 65.62%] [G loss: 1.990011]\n",
      "epoch:29 step:27548 [D loss: 0.636619, acc: 60.94%] [G loss: 1.820402]\n",
      "epoch:29 step:27549 [D loss: 0.697816, acc: 53.91%] [G loss: 1.794951]\n",
      "epoch:29 step:27550 [D loss: 0.720453, acc: 56.25%] [G loss: 1.776422]\n",
      "epoch:29 step:27551 [D loss: 0.645153, acc: 64.06%] [G loss: 1.774756]\n",
      "epoch:29 step:27552 [D loss: 0.665549, acc: 59.38%] [G loss: 1.845915]\n",
      "epoch:29 step:27553 [D loss: 0.641958, acc: 61.72%] [G loss: 1.799943]\n",
      "epoch:29 step:27554 [D loss: 0.629213, acc: 64.06%] [G loss: 1.963042]\n",
      "epoch:29 step:27555 [D loss: 0.663437, acc: 60.16%] [G loss: 1.857249]\n",
      "epoch:29 step:27556 [D loss: 0.723557, acc: 50.78%] [G loss: 1.762147]\n",
      "epoch:29 step:27557 [D loss: 0.629913, acc: 62.50%] [G loss: 1.774171]\n",
      "epoch:29 step:27558 [D loss: 0.640964, acc: 62.50%] [G loss: 1.846457]\n",
      "epoch:29 step:27559 [D loss: 0.700761, acc: 57.03%] [G loss: 1.756192]\n",
      "epoch:29 step:27560 [D loss: 0.696100, acc: 59.38%] [G loss: 1.829666]\n",
      "epoch:29 step:27561 [D loss: 0.684845, acc: 57.03%] [G loss: 1.732468]\n",
      "epoch:29 step:27562 [D loss: 0.654054, acc: 67.19%] [G loss: 1.958850]\n",
      "epoch:29 step:27563 [D loss: 0.615943, acc: 71.88%] [G loss: 1.847025]\n",
      "epoch:29 step:27564 [D loss: 0.682434, acc: 57.03%] [G loss: 1.776103]\n",
      "epoch:29 step:27565 [D loss: 0.656836, acc: 57.81%] [G loss: 1.768912]\n",
      "epoch:29 step:27566 [D loss: 0.615493, acc: 67.19%] [G loss: 1.957451]\n",
      "epoch:29 step:27567 [D loss: 0.655608, acc: 65.62%] [G loss: 1.755635]\n",
      "epoch:29 step:27568 [D loss: 0.656110, acc: 63.28%] [G loss: 1.856616]\n",
      "epoch:29 step:27569 [D loss: 0.653400, acc: 60.94%] [G loss: 1.882531]\n",
      "epoch:29 step:27570 [D loss: 0.693212, acc: 54.69%] [G loss: 1.716248]\n",
      "epoch:29 step:27571 [D loss: 0.667051, acc: 60.16%] [G loss: 2.007451]\n",
      "epoch:29 step:27572 [D loss: 0.630374, acc: 62.50%] [G loss: 1.803823]\n",
      "epoch:29 step:27573 [D loss: 0.651443, acc: 64.06%] [G loss: 1.873903]\n",
      "epoch:29 step:27574 [D loss: 0.650539, acc: 64.06%] [G loss: 1.854548]\n",
      "epoch:29 step:27575 [D loss: 0.601225, acc: 70.31%] [G loss: 1.981403]\n",
      "epoch:29 step:27576 [D loss: 0.675719, acc: 59.38%] [G loss: 1.697536]\n",
      "epoch:29 step:27577 [D loss: 0.670069, acc: 57.03%] [G loss: 1.776062]\n",
      "epoch:29 step:27578 [D loss: 0.667577, acc: 62.50%] [G loss: 1.890465]\n",
      "epoch:29 step:27579 [D loss: 0.618447, acc: 66.41%] [G loss: 1.903685]\n",
      "epoch:29 step:27580 [D loss: 0.640881, acc: 64.06%] [G loss: 1.930633]\n",
      "epoch:29 step:27581 [D loss: 0.644715, acc: 59.38%] [G loss: 1.801433]\n",
      "epoch:29 step:27582 [D loss: 0.671942, acc: 61.72%] [G loss: 1.851514]\n",
      "epoch:29 step:27583 [D loss: 0.641629, acc: 69.53%] [G loss: 1.848547]\n",
      "epoch:29 step:27584 [D loss: 0.702248, acc: 53.91%] [G loss: 1.734555]\n",
      "epoch:29 step:27585 [D loss: 0.619898, acc: 70.31%] [G loss: 1.931148]\n",
      "epoch:29 step:27586 [D loss: 0.659608, acc: 63.28%] [G loss: 1.867272]\n",
      "epoch:29 step:27587 [D loss: 0.661095, acc: 55.47%] [G loss: 1.946688]\n",
      "epoch:29 step:27588 [D loss: 0.641491, acc: 64.84%] [G loss: 1.950521]\n",
      "epoch:29 step:27589 [D loss: 0.621017, acc: 64.84%] [G loss: 1.969775]\n",
      "epoch:29 step:27590 [D loss: 0.605237, acc: 69.53%] [G loss: 1.928790]\n",
      "epoch:29 step:27591 [D loss: 0.675151, acc: 60.94%] [G loss: 1.841597]\n",
      "epoch:29 step:27592 [D loss: 0.659132, acc: 57.81%] [G loss: 1.736827]\n",
      "epoch:29 step:27593 [D loss: 0.688365, acc: 53.91%] [G loss: 1.813176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27594 [D loss: 0.692360, acc: 60.16%] [G loss: 1.745431]\n",
      "epoch:29 step:27595 [D loss: 0.644853, acc: 63.28%] [G loss: 1.787381]\n",
      "epoch:29 step:27596 [D loss: 0.684972, acc: 58.59%] [G loss: 1.851109]\n",
      "epoch:29 step:27597 [D loss: 0.672408, acc: 53.91%] [G loss: 1.896325]\n",
      "epoch:29 step:27598 [D loss: 0.654892, acc: 61.72%] [G loss: 1.908780]\n",
      "epoch:29 step:27599 [D loss: 0.617019, acc: 63.28%] [G loss: 1.865726]\n",
      "epoch:29 step:27600 [D loss: 0.647895, acc: 61.72%] [G loss: 1.916752]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.642209\n",
      "FID: 9.350879\n",
      "0 = 12.58121820693019\n",
      "1 = 0.08042571759720513\n",
      "2 = 0.8704000115394592\n",
      "3 = 0.8912000060081482\n",
      "4 = 0.8496000170707703\n",
      "5 = 0.855606734752655\n",
      "6 = 0.8912000060081482\n",
      "7 = 5.9727376619577415\n",
      "8 = 0.05407921505115728\n",
      "9 = 0.6998000144958496\n",
      "10 = 0.7099999785423279\n",
      "11 = 0.6895999908447266\n",
      "12 = 0.695805549621582\n",
      "13 = 0.7099999785423279\n",
      "14 = 7.642241477966309\n",
      "15 = 9.414155006408691\n",
      "16 = 0.11538553237915039\n",
      "17 = 7.642209053039551\n",
      "18 = 9.350878715515137\n",
      "epoch:29 step:27601 [D loss: 0.631038, acc: 66.41%] [G loss: 1.880031]\n",
      "epoch:29 step:27602 [D loss: 0.613335, acc: 67.97%] [G loss: 2.079607]\n",
      "epoch:29 step:27603 [D loss: 0.572468, acc: 73.44%] [G loss: 2.111637]\n",
      "epoch:29 step:27604 [D loss: 0.570050, acc: 74.22%] [G loss: 1.822192]\n",
      "epoch:29 step:27605 [D loss: 0.722511, acc: 50.78%] [G loss: 1.740812]\n",
      "epoch:29 step:27606 [D loss: 0.673216, acc: 55.47%] [G loss: 1.697710]\n",
      "epoch:29 step:27607 [D loss: 0.621889, acc: 66.41%] [G loss: 1.851177]\n",
      "epoch:29 step:27608 [D loss: 0.633780, acc: 62.50%] [G loss: 1.979402]\n",
      "epoch:29 step:27609 [D loss: 0.609529, acc: 67.19%] [G loss: 1.940533]\n",
      "epoch:29 step:27610 [D loss: 0.725246, acc: 55.47%] [G loss: 1.714395]\n",
      "epoch:29 step:27611 [D loss: 0.602019, acc: 70.31%] [G loss: 1.745971]\n",
      "epoch:29 step:27612 [D loss: 0.676655, acc: 58.59%] [G loss: 1.774304]\n",
      "epoch:29 step:27613 [D loss: 0.690774, acc: 53.91%] [G loss: 1.790336]\n",
      "epoch:29 step:27614 [D loss: 0.669239, acc: 62.50%] [G loss: 1.750213]\n",
      "epoch:29 step:27615 [D loss: 0.665828, acc: 57.03%] [G loss: 1.720277]\n",
      "epoch:29 step:27616 [D loss: 0.644236, acc: 61.72%] [G loss: 1.821131]\n",
      "epoch:29 step:27617 [D loss: 0.664096, acc: 61.72%] [G loss: 1.650798]\n",
      "epoch:29 step:27618 [D loss: 0.642827, acc: 60.94%] [G loss: 1.927376]\n",
      "epoch:29 step:27619 [D loss: 0.645766, acc: 66.41%] [G loss: 1.879004]\n",
      "epoch:29 step:27620 [D loss: 0.631388, acc: 60.94%] [G loss: 1.929684]\n",
      "epoch:29 step:27621 [D loss: 0.675968, acc: 57.03%] [G loss: 1.774050]\n",
      "epoch:29 step:27622 [D loss: 0.681530, acc: 57.81%] [G loss: 1.840895]\n",
      "epoch:29 step:27623 [D loss: 0.666507, acc: 60.94%] [G loss: 1.735363]\n",
      "epoch:29 step:27624 [D loss: 0.660723, acc: 64.06%] [G loss: 1.895470]\n",
      "epoch:29 step:27625 [D loss: 0.626503, acc: 65.62%] [G loss: 1.919694]\n",
      "epoch:29 step:27626 [D loss: 0.639231, acc: 67.19%] [G loss: 1.857041]\n",
      "epoch:29 step:27627 [D loss: 0.662910, acc: 60.94%] [G loss: 1.789660]\n",
      "epoch:29 step:27628 [D loss: 0.673252, acc: 58.59%] [G loss: 1.871268]\n",
      "epoch:29 step:27629 [D loss: 0.631189, acc: 67.97%] [G loss: 1.985260]\n",
      "epoch:29 step:27630 [D loss: 0.617920, acc: 60.94%] [G loss: 1.886722]\n",
      "epoch:29 step:27631 [D loss: 0.689105, acc: 56.25%] [G loss: 1.817506]\n",
      "epoch:29 step:27632 [D loss: 0.700846, acc: 55.47%] [G loss: 1.789603]\n",
      "epoch:29 step:27633 [D loss: 0.692221, acc: 59.38%] [G loss: 1.644381]\n",
      "epoch:29 step:27634 [D loss: 0.638327, acc: 62.50%] [G loss: 1.879453]\n",
      "epoch:29 step:27635 [D loss: 0.655151, acc: 59.38%] [G loss: 1.880209]\n",
      "epoch:29 step:27636 [D loss: 0.681812, acc: 55.47%] [G loss: 1.788192]\n",
      "epoch:29 step:27637 [D loss: 0.682600, acc: 56.25%] [G loss: 1.811173]\n",
      "epoch:29 step:27638 [D loss: 0.588229, acc: 67.97%] [G loss: 1.814927]\n",
      "epoch:29 step:27639 [D loss: 0.716316, acc: 51.56%] [G loss: 1.895668]\n",
      "epoch:29 step:27640 [D loss: 0.619319, acc: 67.19%] [G loss: 1.744734]\n",
      "epoch:29 step:27641 [D loss: 0.726368, acc: 57.03%] [G loss: 2.067447]\n",
      "epoch:29 step:27642 [D loss: 0.607911, acc: 70.31%] [G loss: 1.898125]\n",
      "epoch:29 step:27643 [D loss: 0.678180, acc: 58.59%] [G loss: 1.924767]\n",
      "epoch:29 step:27644 [D loss: 0.641714, acc: 65.62%] [G loss: 1.993487]\n",
      "epoch:29 step:27645 [D loss: 0.601329, acc: 67.97%] [G loss: 1.910233]\n",
      "epoch:29 step:27646 [D loss: 0.704737, acc: 53.12%] [G loss: 1.814027]\n",
      "epoch:29 step:27647 [D loss: 0.687929, acc: 55.47%] [G loss: 1.768575]\n",
      "epoch:29 step:27648 [D loss: 0.689213, acc: 56.25%] [G loss: 1.799302]\n",
      "epoch:29 step:27649 [D loss: 0.665009, acc: 63.28%] [G loss: 1.861386]\n",
      "epoch:29 step:27650 [D loss: 0.713373, acc: 56.25%] [G loss: 1.666656]\n",
      "epoch:29 step:27651 [D loss: 0.706102, acc: 53.91%] [G loss: 1.757107]\n",
      "epoch:29 step:27652 [D loss: 0.651282, acc: 59.38%] [G loss: 1.804151]\n",
      "epoch:29 step:27653 [D loss: 0.671237, acc: 56.25%] [G loss: 1.839084]\n",
      "epoch:29 step:27654 [D loss: 0.601697, acc: 67.97%] [G loss: 1.892245]\n",
      "epoch:29 step:27655 [D loss: 0.670773, acc: 58.59%] [G loss: 1.739808]\n",
      "epoch:29 step:27656 [D loss: 0.664709, acc: 56.25%] [G loss: 1.810851]\n",
      "epoch:29 step:27657 [D loss: 0.636662, acc: 64.06%] [G loss: 1.866802]\n",
      "epoch:29 step:27658 [D loss: 0.647840, acc: 57.03%] [G loss: 1.821419]\n",
      "epoch:29 step:27659 [D loss: 0.627169, acc: 62.50%] [G loss: 1.875257]\n",
      "epoch:29 step:27660 [D loss: 0.659374, acc: 60.16%] [G loss: 1.835030]\n",
      "epoch:29 step:27661 [D loss: 0.620329, acc: 64.84%] [G loss: 1.987151]\n",
      "epoch:29 step:27662 [D loss: 0.678866, acc: 57.03%] [G loss: 1.816413]\n",
      "epoch:29 step:27663 [D loss: 0.718698, acc: 54.69%] [G loss: 1.683548]\n",
      "epoch:29 step:27664 [D loss: 0.655411, acc: 62.50%] [G loss: 1.883944]\n",
      "epoch:29 step:27665 [D loss: 0.671529, acc: 60.94%] [G loss: 1.789478]\n",
      "epoch:29 step:27666 [D loss: 0.684189, acc: 57.03%] [G loss: 1.763596]\n",
      "epoch:29 step:27667 [D loss: 0.643563, acc: 60.16%] [G loss: 1.812492]\n",
      "epoch:29 step:27668 [D loss: 0.602855, acc: 71.88%] [G loss: 1.917673]\n",
      "epoch:29 step:27669 [D loss: 0.662410, acc: 61.72%] [G loss: 1.823712]\n",
      "epoch:29 step:27670 [D loss: 0.673607, acc: 62.50%] [G loss: 1.875105]\n",
      "epoch:29 step:27671 [D loss: 0.606254, acc: 67.97%] [G loss: 2.003298]\n",
      "epoch:29 step:27672 [D loss: 0.616185, acc: 67.97%] [G loss: 1.984775]\n",
      "epoch:29 step:27673 [D loss: 0.678648, acc: 56.25%] [G loss: 1.781123]\n",
      "epoch:29 step:27674 [D loss: 0.667868, acc: 56.25%] [G loss: 1.717338]\n",
      "epoch:29 step:27675 [D loss: 0.676120, acc: 54.69%] [G loss: 1.611923]\n",
      "epoch:29 step:27676 [D loss: 0.663746, acc: 62.50%] [G loss: 1.728431]\n",
      "epoch:29 step:27677 [D loss: 0.612801, acc: 63.28%] [G loss: 1.961873]\n",
      "epoch:29 step:27678 [D loss: 0.640645, acc: 59.38%] [G loss: 1.844255]\n",
      "epoch:29 step:27679 [D loss: 0.655202, acc: 63.28%] [G loss: 1.708182]\n",
      "epoch:29 step:27680 [D loss: 0.676761, acc: 54.69%] [G loss: 1.736234]\n",
      "epoch:29 step:27681 [D loss: 0.635050, acc: 62.50%] [G loss: 1.845609]\n",
      "epoch:29 step:27682 [D loss: 0.646369, acc: 64.06%] [G loss: 1.809574]\n",
      "epoch:29 step:27683 [D loss: 0.636284, acc: 67.19%] [G loss: 1.764936]\n",
      "epoch:29 step:27684 [D loss: 0.713060, acc: 52.34%] [G loss: 1.759210]\n",
      "epoch:29 step:27685 [D loss: 0.649253, acc: 60.16%] [G loss: 1.705943]\n",
      "epoch:29 step:27686 [D loss: 0.640903, acc: 60.16%] [G loss: 1.815875]\n",
      "epoch:29 step:27687 [D loss: 0.605069, acc: 67.97%] [G loss: 1.805326]\n",
      "epoch:29 step:27688 [D loss: 0.679234, acc: 58.59%] [G loss: 1.908085]\n",
      "epoch:29 step:27689 [D loss: 0.667839, acc: 62.50%] [G loss: 1.939976]\n",
      "epoch:29 step:27690 [D loss: 0.632133, acc: 62.50%] [G loss: 1.879294]\n",
      "epoch:29 step:27691 [D loss: 0.695976, acc: 53.91%] [G loss: 1.726666]\n",
      "epoch:29 step:27692 [D loss: 0.639601, acc: 61.72%] [G loss: 1.841304]\n",
      "epoch:29 step:27693 [D loss: 0.584978, acc: 68.75%] [G loss: 1.880021]\n",
      "epoch:29 step:27694 [D loss: 0.590308, acc: 68.75%] [G loss: 1.780646]\n",
      "epoch:29 step:27695 [D loss: 0.590189, acc: 67.97%] [G loss: 1.989390]\n",
      "epoch:29 step:27696 [D loss: 0.614139, acc: 65.62%] [G loss: 2.007630]\n",
      "epoch:29 step:27697 [D loss: 0.647384, acc: 60.94%] [G loss: 1.848142]\n",
      "epoch:29 step:27698 [D loss: 0.671808, acc: 60.94%] [G loss: 1.815881]\n",
      "epoch:29 step:27699 [D loss: 0.643052, acc: 66.41%] [G loss: 1.727890]\n",
      "epoch:29 step:27700 [D loss: 0.638544, acc: 64.84%] [G loss: 1.829541]\n",
      "epoch:29 step:27701 [D loss: 0.705720, acc: 55.47%] [G loss: 1.732603]\n",
      "epoch:29 step:27702 [D loss: 0.677676, acc: 56.25%] [G loss: 1.757336]\n",
      "epoch:29 step:27703 [D loss: 0.702296, acc: 57.81%] [G loss: 1.664097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27704 [D loss: 0.639642, acc: 63.28%] [G loss: 1.841460]\n",
      "epoch:29 step:27705 [D loss: 0.658222, acc: 59.38%] [G loss: 1.829145]\n",
      "epoch:29 step:27706 [D loss: 0.643306, acc: 59.38%] [G loss: 1.735463]\n",
      "epoch:29 step:27707 [D loss: 0.672516, acc: 59.38%] [G loss: 1.909762]\n",
      "epoch:29 step:27708 [D loss: 0.684806, acc: 60.94%] [G loss: 1.745418]\n",
      "epoch:29 step:27709 [D loss: 0.660167, acc: 62.50%] [G loss: 2.001008]\n",
      "epoch:29 step:27710 [D loss: 0.660660, acc: 60.16%] [G loss: 1.750010]\n",
      "epoch:29 step:27711 [D loss: 0.682787, acc: 55.47%] [G loss: 1.795164]\n",
      "epoch:29 step:27712 [D loss: 0.675614, acc: 57.03%] [G loss: 1.789207]\n",
      "epoch:29 step:27713 [D loss: 0.651959, acc: 57.03%] [G loss: 1.796255]\n",
      "epoch:29 step:27714 [D loss: 0.700130, acc: 60.16%] [G loss: 1.731826]\n",
      "epoch:29 step:27715 [D loss: 0.685896, acc: 56.25%] [G loss: 1.886628]\n",
      "epoch:29 step:27716 [D loss: 0.704273, acc: 52.34%] [G loss: 1.797720]\n",
      "epoch:29 step:27717 [D loss: 0.667273, acc: 53.91%] [G loss: 1.758840]\n",
      "epoch:29 step:27718 [D loss: 0.591404, acc: 69.53%] [G loss: 1.864526]\n",
      "epoch:29 step:27719 [D loss: 0.700414, acc: 48.44%] [G loss: 1.799261]\n",
      "epoch:29 step:27720 [D loss: 0.631039, acc: 65.62%] [G loss: 1.857618]\n",
      "epoch:29 step:27721 [D loss: 0.634814, acc: 64.06%] [G loss: 1.792769]\n",
      "epoch:29 step:27722 [D loss: 0.576192, acc: 71.09%] [G loss: 1.992575]\n",
      "epoch:29 step:27723 [D loss: 0.625245, acc: 68.75%] [G loss: 1.855467]\n",
      "epoch:29 step:27724 [D loss: 0.582027, acc: 71.88%] [G loss: 1.886461]\n",
      "epoch:29 step:27725 [D loss: 0.654747, acc: 65.62%] [G loss: 1.895633]\n",
      "epoch:29 step:27726 [D loss: 0.591796, acc: 72.66%] [G loss: 1.906907]\n",
      "epoch:29 step:27727 [D loss: 0.609900, acc: 66.41%] [G loss: 1.929623]\n",
      "epoch:29 step:27728 [D loss: 0.646521, acc: 67.97%] [G loss: 1.860952]\n",
      "epoch:29 step:27729 [D loss: 0.623120, acc: 68.75%] [G loss: 2.055563]\n",
      "epoch:29 step:27730 [D loss: 0.612472, acc: 65.62%] [G loss: 2.023780]\n",
      "epoch:29 step:27731 [D loss: 0.607477, acc: 70.31%] [G loss: 1.884922]\n",
      "epoch:29 step:27732 [D loss: 0.712902, acc: 50.00%] [G loss: 1.742022]\n",
      "epoch:29 step:27733 [D loss: 0.678721, acc: 56.25%] [G loss: 1.726154]\n",
      "epoch:29 step:27734 [D loss: 0.670457, acc: 57.81%] [G loss: 1.951057]\n",
      "epoch:29 step:27735 [D loss: 0.627684, acc: 65.62%] [G loss: 1.853805]\n",
      "epoch:29 step:27736 [D loss: 0.634047, acc: 62.50%] [G loss: 2.147622]\n",
      "epoch:29 step:27737 [D loss: 0.629216, acc: 67.19%] [G loss: 2.121053]\n",
      "epoch:29 step:27738 [D loss: 0.676548, acc: 57.03%] [G loss: 1.829742]\n",
      "epoch:29 step:27739 [D loss: 0.724338, acc: 53.12%] [G loss: 1.706151]\n",
      "epoch:29 step:27740 [D loss: 0.682845, acc: 60.16%] [G loss: 1.763601]\n",
      "epoch:29 step:27741 [D loss: 0.665007, acc: 63.28%] [G loss: 1.753715]\n",
      "epoch:29 step:27742 [D loss: 0.666123, acc: 62.50%] [G loss: 1.877521]\n",
      "epoch:29 step:27743 [D loss: 0.665719, acc: 59.38%] [G loss: 1.811110]\n",
      "epoch:29 step:27744 [D loss: 0.618295, acc: 65.62%] [G loss: 1.884214]\n",
      "epoch:29 step:27745 [D loss: 0.641700, acc: 61.72%] [G loss: 1.803619]\n",
      "epoch:29 step:27746 [D loss: 0.681273, acc: 60.16%] [G loss: 1.782841]\n",
      "epoch:29 step:27747 [D loss: 0.606959, acc: 67.19%] [G loss: 1.884494]\n",
      "epoch:29 step:27748 [D loss: 0.640004, acc: 64.06%] [G loss: 1.763560]\n",
      "epoch:29 step:27749 [D loss: 0.672816, acc: 56.25%] [G loss: 1.788924]\n",
      "epoch:29 step:27750 [D loss: 0.663541, acc: 60.94%] [G loss: 1.725256]\n",
      "epoch:29 step:27751 [D loss: 0.620360, acc: 68.75%] [G loss: 1.779323]\n",
      "epoch:29 step:27752 [D loss: 0.600719, acc: 71.09%] [G loss: 1.890256]\n",
      "epoch:29 step:27753 [D loss: 0.663598, acc: 59.38%] [G loss: 1.808298]\n",
      "epoch:29 step:27754 [D loss: 0.685623, acc: 55.47%] [G loss: 1.782353]\n",
      "epoch:29 step:27755 [D loss: 0.624989, acc: 66.41%] [G loss: 1.829143]\n",
      "epoch:29 step:27756 [D loss: 0.635431, acc: 61.72%] [G loss: 1.863371]\n",
      "epoch:29 step:27757 [D loss: 0.649718, acc: 64.84%] [G loss: 1.830500]\n",
      "epoch:29 step:27758 [D loss: 0.679032, acc: 56.25%] [G loss: 1.836193]\n",
      "epoch:29 step:27759 [D loss: 0.647677, acc: 64.84%] [G loss: 1.710022]\n",
      "epoch:29 step:27760 [D loss: 0.659332, acc: 62.50%] [G loss: 1.846458]\n",
      "epoch:29 step:27761 [D loss: 0.660090, acc: 63.28%] [G loss: 1.839730]\n",
      "epoch:29 step:27762 [D loss: 0.612004, acc: 67.97%] [G loss: 1.953361]\n",
      "epoch:29 step:27763 [D loss: 0.597240, acc: 68.75%] [G loss: 1.922974]\n",
      "epoch:29 step:27764 [D loss: 0.629732, acc: 65.62%] [G loss: 1.770120]\n",
      "epoch:29 step:27765 [D loss: 0.593191, acc: 72.66%] [G loss: 1.977369]\n",
      "epoch:29 step:27766 [D loss: 0.667947, acc: 56.25%] [G loss: 1.714215]\n",
      "epoch:29 step:27767 [D loss: 0.656867, acc: 56.25%] [G loss: 1.734755]\n",
      "epoch:29 step:27768 [D loss: 0.632305, acc: 61.72%] [G loss: 1.874123]\n",
      "epoch:29 step:27769 [D loss: 0.639580, acc: 63.28%] [G loss: 1.826563]\n",
      "epoch:29 step:27770 [D loss: 0.690914, acc: 53.12%] [G loss: 1.829484]\n",
      "epoch:29 step:27771 [D loss: 0.634035, acc: 60.16%] [G loss: 1.953707]\n",
      "epoch:29 step:27772 [D loss: 0.612082, acc: 68.75%] [G loss: 1.862612]\n",
      "epoch:29 step:27773 [D loss: 0.679623, acc: 56.25%] [G loss: 1.846954]\n",
      "epoch:29 step:27774 [D loss: 0.738000, acc: 57.03%] [G loss: 1.764975]\n",
      "epoch:29 step:27775 [D loss: 0.599589, acc: 67.19%] [G loss: 2.049453]\n",
      "epoch:29 step:27776 [D loss: 0.634973, acc: 62.50%] [G loss: 2.006425]\n",
      "epoch:29 step:27777 [D loss: 0.688622, acc: 53.12%] [G loss: 1.937902]\n",
      "epoch:29 step:27778 [D loss: 0.671222, acc: 54.69%] [G loss: 1.975669]\n",
      "epoch:29 step:27779 [D loss: 0.648928, acc: 58.59%] [G loss: 1.841764]\n",
      "epoch:29 step:27780 [D loss: 0.611890, acc: 67.97%] [G loss: 1.828437]\n",
      "epoch:29 step:27781 [D loss: 0.620005, acc: 63.28%] [G loss: 1.905535]\n",
      "epoch:29 step:27782 [D loss: 0.717583, acc: 56.25%] [G loss: 1.720324]\n",
      "epoch:29 step:27783 [D loss: 0.637542, acc: 59.38%] [G loss: 1.747214]\n",
      "epoch:29 step:27784 [D loss: 0.698109, acc: 53.91%] [G loss: 1.821939]\n",
      "epoch:29 step:27785 [D loss: 0.669303, acc: 60.94%] [G loss: 1.741199]\n",
      "epoch:29 step:27786 [D loss: 0.666538, acc: 55.47%] [G loss: 1.725972]\n",
      "epoch:29 step:27787 [D loss: 0.684465, acc: 57.03%] [G loss: 1.750779]\n",
      "epoch:29 step:27788 [D loss: 0.680331, acc: 57.03%] [G loss: 1.788092]\n",
      "epoch:29 step:27789 [D loss: 0.684057, acc: 60.94%] [G loss: 1.824146]\n",
      "epoch:29 step:27790 [D loss: 0.711133, acc: 52.34%] [G loss: 1.742712]\n",
      "epoch:29 step:27791 [D loss: 0.594802, acc: 65.62%] [G loss: 1.880487]\n",
      "epoch:29 step:27792 [D loss: 0.649768, acc: 63.28%] [G loss: 1.841514]\n",
      "epoch:29 step:27793 [D loss: 0.624699, acc: 62.50%] [G loss: 1.850206]\n",
      "epoch:29 step:27794 [D loss: 0.670015, acc: 60.94%] [G loss: 1.865650]\n",
      "epoch:29 step:27795 [D loss: 0.634421, acc: 62.50%] [G loss: 1.994032]\n",
      "epoch:29 step:27796 [D loss: 0.695824, acc: 56.25%] [G loss: 1.845147]\n",
      "epoch:29 step:27797 [D loss: 0.601254, acc: 67.97%] [G loss: 2.034047]\n",
      "epoch:29 step:27798 [D loss: 0.702754, acc: 59.38%] [G loss: 1.787284]\n",
      "epoch:29 step:27799 [D loss: 0.677340, acc: 57.03%] [G loss: 1.891452]\n",
      "epoch:29 step:27800 [D loss: 0.660026, acc: 57.03%] [G loss: 1.806118]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.744457\n",
      "FID: 8.265736\n",
      "0 = 12.66433464303018\n",
      "1 = 0.09048432580086568\n",
      "2 = 0.8647000193595886\n",
      "3 = 0.890999972820282\n",
      "4 = 0.8384000062942505\n",
      "5 = 0.8464754223823547\n",
      "6 = 0.890999972820282\n",
      "7 = 5.8506549968242725\n",
      "8 = 0.05462424843836348\n",
      "9 = 0.6833000183105469\n",
      "10 = 0.7002000212669373\n",
      "11 = 0.6664000153541565\n",
      "12 = 0.6773070096969604\n",
      "13 = 0.7002000212669373\n",
      "14 = 7.744487762451172\n",
      "15 = 9.499150276184082\n",
      "16 = 0.09183482825756073\n",
      "17 = 7.744456768035889\n",
      "18 = 8.265735626220703\n",
      "epoch:29 step:27801 [D loss: 0.670909, acc: 60.16%] [G loss: 1.792394]\n",
      "epoch:29 step:27802 [D loss: 0.704740, acc: 59.38%] [G loss: 1.780394]\n",
      "epoch:29 step:27803 [D loss: 0.584356, acc: 72.66%] [G loss: 1.929295]\n",
      "epoch:29 step:27804 [D loss: 0.649659, acc: 66.41%] [G loss: 1.994991]\n",
      "epoch:29 step:27805 [D loss: 0.681889, acc: 57.03%] [G loss: 1.728289]\n",
      "epoch:29 step:27806 [D loss: 0.652904, acc: 61.72%] [G loss: 1.857974]\n",
      "epoch:29 step:27807 [D loss: 0.678623, acc: 57.81%] [G loss: 1.853644]\n",
      "epoch:29 step:27808 [D loss: 0.641073, acc: 60.16%] [G loss: 1.982871]\n",
      "epoch:29 step:27809 [D loss: 0.648374, acc: 57.03%] [G loss: 1.842466]\n",
      "epoch:29 step:27810 [D loss: 0.638192, acc: 65.62%] [G loss: 1.774677]\n",
      "epoch:29 step:27811 [D loss: 0.629213, acc: 64.06%] [G loss: 1.863513]\n",
      "epoch:29 step:27812 [D loss: 0.636583, acc: 63.28%] [G loss: 1.856999]\n",
      "epoch:29 step:27813 [D loss: 0.609580, acc: 67.19%] [G loss: 1.922070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27814 [D loss: 0.612847, acc: 60.94%] [G loss: 1.880526]\n",
      "epoch:29 step:27815 [D loss: 0.625674, acc: 66.41%] [G loss: 1.966522]\n",
      "epoch:29 step:27816 [D loss: 0.653932, acc: 61.72%] [G loss: 1.950204]\n",
      "epoch:29 step:27817 [D loss: 0.617262, acc: 67.19%] [G loss: 1.969875]\n",
      "epoch:29 step:27818 [D loss: 0.670595, acc: 56.25%] [G loss: 1.976392]\n",
      "epoch:29 step:27819 [D loss: 0.649994, acc: 62.50%] [G loss: 1.982695]\n",
      "epoch:29 step:27820 [D loss: 0.657993, acc: 61.72%] [G loss: 2.041049]\n",
      "epoch:29 step:27821 [D loss: 0.578504, acc: 67.97%] [G loss: 2.261811]\n",
      "epoch:29 step:27822 [D loss: 0.563778, acc: 70.31%] [G loss: 2.134505]\n",
      "epoch:29 step:27823 [D loss: 0.582538, acc: 71.09%] [G loss: 2.134214]\n",
      "epoch:29 step:27824 [D loss: 0.616587, acc: 64.84%] [G loss: 2.035203]\n",
      "epoch:29 step:27825 [D loss: 0.633019, acc: 67.97%] [G loss: 1.855678]\n",
      "epoch:29 step:27826 [D loss: 0.669902, acc: 55.47%] [G loss: 2.185912]\n",
      "epoch:29 step:27827 [D loss: 0.637447, acc: 61.72%] [G loss: 2.041265]\n",
      "epoch:29 step:27828 [D loss: 0.638377, acc: 66.41%] [G loss: 1.841036]\n",
      "epoch:29 step:27829 [D loss: 0.696279, acc: 54.69%] [G loss: 1.786850]\n",
      "epoch:29 step:27830 [D loss: 0.666458, acc: 60.16%] [G loss: 1.639820]\n",
      "epoch:29 step:27831 [D loss: 0.718466, acc: 54.69%] [G loss: 1.879433]\n",
      "epoch:29 step:27832 [D loss: 0.623563, acc: 67.97%] [G loss: 1.805285]\n",
      "epoch:29 step:27833 [D loss: 0.703785, acc: 60.16%] [G loss: 1.764380]\n",
      "epoch:29 step:27834 [D loss: 0.625706, acc: 67.97%] [G loss: 1.862503]\n",
      "epoch:29 step:27835 [D loss: 0.669508, acc: 57.03%] [G loss: 1.767847]\n",
      "epoch:29 step:27836 [D loss: 0.663089, acc: 63.28%] [G loss: 1.667639]\n",
      "epoch:29 step:27837 [D loss: 0.642714, acc: 57.81%] [G loss: 1.851057]\n",
      "epoch:29 step:27838 [D loss: 0.627025, acc: 68.75%] [G loss: 1.855392]\n",
      "epoch:29 step:27839 [D loss: 0.703183, acc: 59.38%] [G loss: 1.804960]\n",
      "epoch:29 step:27840 [D loss: 0.697934, acc: 51.56%] [G loss: 1.732642]\n",
      "epoch:29 step:27841 [D loss: 0.644955, acc: 66.41%] [G loss: 1.855108]\n",
      "epoch:29 step:27842 [D loss: 0.671325, acc: 64.06%] [G loss: 1.868022]\n",
      "epoch:29 step:27843 [D loss: 0.706004, acc: 50.00%] [G loss: 1.656156]\n",
      "epoch:29 step:27844 [D loss: 0.624802, acc: 62.50%] [G loss: 1.839347]\n",
      "epoch:29 step:27845 [D loss: 0.618391, acc: 69.53%] [G loss: 1.803323]\n",
      "epoch:29 step:27846 [D loss: 0.650450, acc: 64.06%] [G loss: 1.870035]\n",
      "epoch:29 step:27847 [D loss: 0.633824, acc: 64.06%] [G loss: 1.807098]\n",
      "epoch:29 step:27848 [D loss: 0.692051, acc: 54.69%] [G loss: 1.750463]\n",
      "epoch:29 step:27849 [D loss: 0.656507, acc: 60.94%] [G loss: 1.843147]\n",
      "epoch:29 step:27850 [D loss: 0.624414, acc: 65.62%] [G loss: 1.872423]\n",
      "epoch:29 step:27851 [D loss: 0.626565, acc: 67.97%] [G loss: 1.922798]\n",
      "epoch:29 step:27852 [D loss: 0.651896, acc: 61.72%] [G loss: 1.887671]\n",
      "epoch:29 step:27853 [D loss: 0.664671, acc: 60.94%] [G loss: 1.911656]\n",
      "epoch:29 step:27854 [D loss: 0.624855, acc: 64.06%] [G loss: 1.969144]\n",
      "epoch:29 step:27855 [D loss: 0.659135, acc: 56.25%] [G loss: 1.728177]\n",
      "epoch:29 step:27856 [D loss: 0.632653, acc: 67.19%] [G loss: 1.884033]\n",
      "epoch:29 step:27857 [D loss: 0.626372, acc: 68.75%] [G loss: 1.769304]\n",
      "epoch:29 step:27858 [D loss: 0.639428, acc: 66.41%] [G loss: 1.816757]\n",
      "epoch:29 step:27859 [D loss: 0.659656, acc: 64.06%] [G loss: 1.798793]\n",
      "epoch:29 step:27860 [D loss: 0.630081, acc: 64.84%] [G loss: 1.927772]\n",
      "epoch:29 step:27861 [D loss: 0.660367, acc: 62.50%] [G loss: 1.927839]\n",
      "epoch:29 step:27862 [D loss: 0.629799, acc: 65.62%] [G loss: 1.932914]\n",
      "epoch:29 step:27863 [D loss: 0.612719, acc: 62.50%] [G loss: 2.058900]\n",
      "epoch:29 step:27864 [D loss: 0.648162, acc: 58.59%] [G loss: 1.919554]\n",
      "epoch:29 step:27865 [D loss: 0.648018, acc: 65.62%] [G loss: 2.043295]\n",
      "epoch:29 step:27866 [D loss: 0.675279, acc: 57.03%] [G loss: 2.004626]\n",
      "epoch:29 step:27867 [D loss: 0.598301, acc: 68.75%] [G loss: 2.126914]\n",
      "epoch:29 step:27868 [D loss: 0.656909, acc: 61.72%] [G loss: 1.981332]\n",
      "epoch:29 step:27869 [D loss: 0.643923, acc: 61.72%] [G loss: 1.870591]\n",
      "epoch:29 step:27870 [D loss: 0.657154, acc: 63.28%] [G loss: 1.929122]\n",
      "epoch:29 step:27871 [D loss: 0.695873, acc: 58.59%] [G loss: 1.732470]\n",
      "epoch:29 step:27872 [D loss: 0.634916, acc: 64.84%] [G loss: 1.932795]\n",
      "epoch:29 step:27873 [D loss: 0.660274, acc: 56.25%] [G loss: 1.852375]\n",
      "epoch:29 step:27874 [D loss: 0.624571, acc: 64.84%] [G loss: 2.004148]\n",
      "epoch:29 step:27875 [D loss: 0.662133, acc: 63.28%] [G loss: 1.806291]\n",
      "epoch:29 step:27876 [D loss: 0.670643, acc: 59.38%] [G loss: 1.745548]\n",
      "epoch:29 step:27877 [D loss: 0.680165, acc: 57.03%] [G loss: 1.810120]\n",
      "epoch:29 step:27878 [D loss: 0.672973, acc: 56.25%] [G loss: 1.709315]\n",
      "epoch:29 step:27879 [D loss: 0.614643, acc: 68.75%] [G loss: 1.937664]\n",
      "epoch:29 step:27880 [D loss: 0.642266, acc: 64.06%] [G loss: 1.906055]\n",
      "epoch:29 step:27881 [D loss: 0.654295, acc: 60.94%] [G loss: 1.924148]\n",
      "epoch:29 step:27882 [D loss: 0.644844, acc: 68.75%] [G loss: 1.916623]\n",
      "epoch:29 step:27883 [D loss: 0.669588, acc: 64.06%] [G loss: 1.779451]\n",
      "epoch:29 step:27884 [D loss: 0.622258, acc: 68.75%] [G loss: 1.937350]\n",
      "epoch:29 step:27885 [D loss: 0.701975, acc: 57.81%] [G loss: 1.958265]\n",
      "epoch:29 step:27886 [D loss: 0.680304, acc: 55.47%] [G loss: 1.850305]\n",
      "epoch:29 step:27887 [D loss: 0.630791, acc: 64.06%] [G loss: 1.856272]\n",
      "epoch:29 step:27888 [D loss: 0.650909, acc: 60.94%] [G loss: 1.825229]\n",
      "epoch:29 step:27889 [D loss: 0.653124, acc: 61.72%] [G loss: 1.797803]\n",
      "epoch:29 step:27890 [D loss: 0.630262, acc: 65.62%] [G loss: 1.936779]\n",
      "epoch:29 step:27891 [D loss: 0.636055, acc: 62.50%] [G loss: 1.925367]\n",
      "epoch:29 step:27892 [D loss: 0.609884, acc: 67.97%] [G loss: 2.006292]\n",
      "epoch:29 step:27893 [D loss: 0.632679, acc: 67.97%] [G loss: 2.035036]\n",
      "epoch:29 step:27894 [D loss: 0.613474, acc: 62.50%] [G loss: 1.963658]\n",
      "epoch:29 step:27895 [D loss: 0.660179, acc: 62.50%] [G loss: 1.843648]\n",
      "epoch:29 step:27896 [D loss: 0.688770, acc: 56.25%] [G loss: 1.844928]\n",
      "epoch:29 step:27897 [D loss: 0.612769, acc: 64.84%] [G loss: 1.803076]\n",
      "epoch:29 step:27898 [D loss: 0.618704, acc: 65.62%] [G loss: 1.872397]\n",
      "epoch:29 step:27899 [D loss: 0.695874, acc: 54.69%] [G loss: 1.827071]\n",
      "epoch:29 step:27900 [D loss: 0.669664, acc: 59.38%] [G loss: 1.890213]\n",
      "epoch:29 step:27901 [D loss: 0.668154, acc: 60.16%] [G loss: 1.857443]\n",
      "epoch:29 step:27902 [D loss: 0.635964, acc: 60.94%] [G loss: 1.918108]\n",
      "epoch:29 step:27903 [D loss: 0.637462, acc: 61.72%] [G loss: 1.871194]\n",
      "epoch:29 step:27904 [D loss: 0.700649, acc: 57.03%] [G loss: 1.751356]\n",
      "epoch:29 step:27905 [D loss: 0.613128, acc: 61.72%] [G loss: 1.796640]\n",
      "epoch:29 step:27906 [D loss: 0.649173, acc: 63.28%] [G loss: 1.904227]\n",
      "epoch:29 step:27907 [D loss: 0.646576, acc: 60.94%] [G loss: 1.725111]\n",
      "epoch:29 step:27908 [D loss: 0.638060, acc: 67.19%] [G loss: 1.864702]\n",
      "epoch:29 step:27909 [D loss: 0.658849, acc: 62.50%] [G loss: 1.924984]\n",
      "epoch:29 step:27910 [D loss: 0.646199, acc: 60.16%] [G loss: 1.797905]\n",
      "epoch:29 step:27911 [D loss: 0.668344, acc: 62.50%] [G loss: 1.861928]\n",
      "epoch:29 step:27912 [D loss: 0.671664, acc: 60.16%] [G loss: 1.738769]\n",
      "epoch:29 step:27913 [D loss: 0.636086, acc: 61.72%] [G loss: 1.855308]\n",
      "epoch:29 step:27914 [D loss: 0.666163, acc: 56.25%] [G loss: 1.922620]\n",
      "epoch:29 step:27915 [D loss: 0.663352, acc: 61.72%] [G loss: 1.852164]\n",
      "epoch:29 step:27916 [D loss: 0.672360, acc: 57.03%] [G loss: 1.922642]\n",
      "epoch:29 step:27917 [D loss: 0.660967, acc: 61.72%] [G loss: 1.847164]\n",
      "epoch:29 step:27918 [D loss: 0.703081, acc: 48.44%] [G loss: 1.819009]\n",
      "epoch:29 step:27919 [D loss: 0.622508, acc: 67.19%] [G loss: 1.843298]\n",
      "epoch:29 step:27920 [D loss: 0.612953, acc: 64.06%] [G loss: 1.851674]\n",
      "epoch:29 step:27921 [D loss: 0.640765, acc: 62.50%] [G loss: 1.755225]\n",
      "epoch:29 step:27922 [D loss: 0.608789, acc: 60.16%] [G loss: 1.764102]\n",
      "epoch:29 step:27923 [D loss: 0.654214, acc: 67.19%] [G loss: 1.820096]\n",
      "epoch:29 step:27924 [D loss: 0.689899, acc: 53.91%] [G loss: 1.770278]\n",
      "epoch:29 step:27925 [D loss: 0.643364, acc: 60.94%] [G loss: 1.727432]\n",
      "epoch:29 step:27926 [D loss: 0.626677, acc: 65.62%] [G loss: 1.827330]\n",
      "epoch:29 step:27927 [D loss: 0.614706, acc: 66.41%] [G loss: 1.877588]\n",
      "epoch:29 step:27928 [D loss: 0.634095, acc: 65.62%] [G loss: 1.753982]\n",
      "epoch:29 step:27929 [D loss: 0.695463, acc: 61.72%] [G loss: 1.885008]\n",
      "epoch:29 step:27930 [D loss: 0.610653, acc: 62.50%] [G loss: 1.797276]\n",
      "epoch:29 step:27931 [D loss: 0.656399, acc: 59.38%] [G loss: 1.768671]\n",
      "epoch:29 step:27932 [D loss: 0.671211, acc: 59.38%] [G loss: 1.719536]\n",
      "epoch:29 step:27933 [D loss: 0.632330, acc: 63.28%] [G loss: 1.875278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27934 [D loss: 0.665873, acc: 59.38%] [G loss: 1.895398]\n",
      "epoch:29 step:27935 [D loss: 0.670193, acc: 56.25%] [G loss: 1.888926]\n",
      "epoch:29 step:27936 [D loss: 0.614653, acc: 70.31%] [G loss: 1.905644]\n",
      "epoch:29 step:27937 [D loss: 0.666847, acc: 60.16%] [G loss: 1.858295]\n",
      "epoch:29 step:27938 [D loss: 0.732007, acc: 49.22%] [G loss: 1.772274]\n",
      "epoch:29 step:27939 [D loss: 0.635896, acc: 61.72%] [G loss: 1.899807]\n",
      "epoch:29 step:27940 [D loss: 0.673870, acc: 54.69%] [G loss: 1.760249]\n",
      "epoch:29 step:27941 [D loss: 0.631886, acc: 58.59%] [G loss: 1.887446]\n",
      "epoch:29 step:27942 [D loss: 0.671951, acc: 57.03%] [G loss: 2.039793]\n",
      "epoch:29 step:27943 [D loss: 0.672875, acc: 63.28%] [G loss: 1.907422]\n",
      "epoch:29 step:27944 [D loss: 0.598642, acc: 70.31%] [G loss: 1.837236]\n",
      "epoch:29 step:27945 [D loss: 0.668471, acc: 57.81%] [G loss: 1.839425]\n",
      "epoch:29 step:27946 [D loss: 0.621547, acc: 70.31%] [G loss: 1.878045]\n",
      "epoch:29 step:27947 [D loss: 0.618340, acc: 67.19%] [G loss: 1.847743]\n",
      "epoch:29 step:27948 [D loss: 0.651410, acc: 58.59%] [G loss: 1.961756]\n",
      "epoch:29 step:27949 [D loss: 0.628741, acc: 62.50%] [G loss: 2.006820]\n",
      "epoch:29 step:27950 [D loss: 0.618518, acc: 63.28%] [G loss: 1.920793]\n",
      "epoch:29 step:27951 [D loss: 0.613204, acc: 65.62%] [G loss: 1.881589]\n",
      "epoch:29 step:27952 [D loss: 0.630993, acc: 60.16%] [G loss: 1.936773]\n",
      "epoch:29 step:27953 [D loss: 0.644192, acc: 64.84%] [G loss: 1.979967]\n",
      "epoch:29 step:27954 [D loss: 0.616430, acc: 70.31%] [G loss: 2.064723]\n",
      "epoch:29 step:27955 [D loss: 0.575705, acc: 68.75%] [G loss: 2.171848]\n",
      "epoch:29 step:27956 [D loss: 0.660188, acc: 59.38%] [G loss: 1.895325]\n",
      "epoch:29 step:27957 [D loss: 0.652075, acc: 56.25%] [G loss: 1.701548]\n",
      "epoch:29 step:27958 [D loss: 0.707484, acc: 54.69%] [G loss: 1.989082]\n",
      "epoch:29 step:27959 [D loss: 0.592748, acc: 67.19%] [G loss: 2.110193]\n",
      "epoch:29 step:27960 [D loss: 0.655441, acc: 59.38%] [G loss: 1.984951]\n",
      "epoch:29 step:27961 [D loss: 0.638060, acc: 60.94%] [G loss: 1.840389]\n",
      "epoch:29 step:27962 [D loss: 0.665063, acc: 60.16%] [G loss: 1.930510]\n",
      "epoch:29 step:27963 [D loss: 0.626631, acc: 65.62%] [G loss: 1.939385]\n",
      "epoch:29 step:27964 [D loss: 0.611815, acc: 65.62%] [G loss: 2.024911]\n",
      "epoch:29 step:27965 [D loss: 0.602328, acc: 64.84%] [G loss: 2.042106]\n",
      "epoch:29 step:27966 [D loss: 0.633350, acc: 62.50%] [G loss: 2.037165]\n",
      "epoch:29 step:27967 [D loss: 0.722279, acc: 49.22%] [G loss: 1.792324]\n",
      "epoch:29 step:27968 [D loss: 0.675143, acc: 54.69%] [G loss: 1.750746]\n",
      "epoch:29 step:27969 [D loss: 0.629379, acc: 62.50%] [G loss: 1.808823]\n",
      "epoch:29 step:27970 [D loss: 0.686886, acc: 56.25%] [G loss: 1.932601]\n",
      "epoch:29 step:27971 [D loss: 0.643155, acc: 59.38%] [G loss: 1.889878]\n",
      "epoch:29 step:27972 [D loss: 0.706407, acc: 55.47%] [G loss: 1.740878]\n",
      "epoch:29 step:27973 [D loss: 0.688205, acc: 54.69%] [G loss: 1.732855]\n",
      "epoch:29 step:27974 [D loss: 0.729004, acc: 50.00%] [G loss: 1.824845]\n",
      "epoch:29 step:27975 [D loss: 0.668508, acc: 54.69%] [G loss: 1.846525]\n",
      "epoch:29 step:27976 [D loss: 0.656319, acc: 60.16%] [G loss: 1.789840]\n",
      "epoch:29 step:27977 [D loss: 0.596434, acc: 64.06%] [G loss: 2.044392]\n",
      "epoch:29 step:27978 [D loss: 0.652819, acc: 64.84%] [G loss: 1.912419]\n",
      "epoch:29 step:27979 [D loss: 0.654063, acc: 63.28%] [G loss: 1.902521]\n",
      "epoch:29 step:27980 [D loss: 0.645618, acc: 60.16%] [G loss: 1.883415]\n",
      "epoch:29 step:27981 [D loss: 0.691646, acc: 55.47%] [G loss: 1.867735]\n",
      "epoch:29 step:27982 [D loss: 0.660449, acc: 57.81%] [G loss: 1.970781]\n",
      "epoch:29 step:27983 [D loss: 0.628588, acc: 66.41%] [G loss: 1.767297]\n",
      "epoch:29 step:27984 [D loss: 0.643106, acc: 64.06%] [G loss: 1.827848]\n",
      "epoch:29 step:27985 [D loss: 0.627774, acc: 57.03%] [G loss: 1.777991]\n",
      "epoch:29 step:27986 [D loss: 0.652248, acc: 58.59%] [G loss: 1.812045]\n",
      "epoch:29 step:27987 [D loss: 0.694285, acc: 54.69%] [G loss: 1.744250]\n",
      "epoch:29 step:27988 [D loss: 0.656904, acc: 56.25%] [G loss: 2.159509]\n",
      "epoch:29 step:27989 [D loss: 0.620643, acc: 67.19%] [G loss: 1.951696]\n",
      "epoch:29 step:27990 [D loss: 0.707095, acc: 55.47%] [G loss: 1.770708]\n",
      "epoch:29 step:27991 [D loss: 0.668671, acc: 59.38%] [G loss: 1.747815]\n",
      "epoch:29 step:27992 [D loss: 0.637590, acc: 63.28%] [G loss: 1.817636]\n",
      "epoch:29 step:27993 [D loss: 0.700275, acc: 53.91%] [G loss: 1.585677]\n",
      "epoch:29 step:27994 [D loss: 0.669670, acc: 60.94%] [G loss: 1.798617]\n",
      "epoch:29 step:27995 [D loss: 0.683079, acc: 56.25%] [G loss: 1.815783]\n",
      "epoch:29 step:27996 [D loss: 0.616915, acc: 64.06%] [G loss: 1.860186]\n",
      "epoch:29 step:27997 [D loss: 0.635673, acc: 67.19%] [G loss: 1.955117]\n",
      "epoch:29 step:27998 [D loss: 0.592179, acc: 74.22%] [G loss: 1.905300]\n",
      "epoch:29 step:27999 [D loss: 0.694917, acc: 50.00%] [G loss: 1.814482]\n",
      "epoch:29 step:28000 [D loss: 0.643139, acc: 64.06%] [G loss: 1.747338]\n",
      "tfgan\n",
      "/real/\n",
      "./fake\n",
      "compute score in space: 0\n",
      "compute score in space: 1\n",
      "IS socre: 7.572058\n",
      "FID: 10.202795\n",
      "0 = 12.704012920045837\n",
      "1 = 0.09096720311989938\n",
      "2 = 0.8708999752998352\n",
      "3 = 0.8939999938011169\n",
      "4 = 0.8478000164031982\n",
      "5 = 0.8545210957527161\n",
      "6 = 0.8939999938011169\n",
      "7 = 6.116732761669174\n",
      "8 = 0.061585410255576414\n",
      "9 = 0.692300021648407\n",
      "10 = 0.7095999717712402\n",
      "11 = 0.675000011920929\n",
      "12 = 0.6858689188957214\n",
      "13 = 0.7095999717712402\n",
      "14 = 7.5721025466918945\n",
      "15 = 9.365887641906738\n",
      "16 = 0.1295936405658722\n",
      "17 = 7.572057723999023\n",
      "18 = 10.202795028686523\n",
      "epoch:29 step:28001 [D loss: 0.696460, acc: 60.16%] [G loss: 1.680989]\n",
      "epoch:29 step:28002 [D loss: 0.625936, acc: 68.75%] [G loss: 1.788578]\n",
      "epoch:29 step:28003 [D loss: 0.721275, acc: 50.78%] [G loss: 1.642594]\n",
      "epoch:29 step:28004 [D loss: 0.656080, acc: 60.94%] [G loss: 1.844989]\n",
      "epoch:29 step:28005 [D loss: 0.640396, acc: 64.06%] [G loss: 1.822496]\n",
      "epoch:29 step:28006 [D loss: 0.603860, acc: 67.97%] [G loss: 1.831290]\n",
      "epoch:29 step:28007 [D loss: 0.662020, acc: 61.72%] [G loss: 1.751615]\n",
      "epoch:29 step:28008 [D loss: 0.640051, acc: 58.59%] [G loss: 1.750575]\n",
      "epoch:29 step:28009 [D loss: 0.673843, acc: 56.25%] [G loss: 1.855738]\n",
      "epoch:29 step:28010 [D loss: 0.644173, acc: 67.19%] [G loss: 1.819551]\n",
      "epoch:29 step:28011 [D loss: 0.661305, acc: 63.28%] [G loss: 1.777165]\n",
      "epoch:29 step:28012 [D loss: 0.637722, acc: 60.16%] [G loss: 1.821612]\n",
      "epoch:29 step:28013 [D loss: 0.612179, acc: 64.84%] [G loss: 1.999066]\n",
      "epoch:29 step:28014 [D loss: 0.625792, acc: 68.75%] [G loss: 1.920907]\n",
      "epoch:29 step:28015 [D loss: 0.620336, acc: 66.41%] [G loss: 2.036164]\n",
      "epoch:29 step:28016 [D loss: 0.652957, acc: 58.59%] [G loss: 1.813098]\n",
      "epoch:29 step:28017 [D loss: 0.648489, acc: 62.50%] [G loss: 1.891847]\n",
      "epoch:29 step:28018 [D loss: 0.623063, acc: 65.62%] [G loss: 2.035924]\n",
      "epoch:29 step:28019 [D loss: 0.673460, acc: 62.50%] [G loss: 1.885015]\n",
      "epoch:29 step:28020 [D loss: 0.620623, acc: 64.84%] [G loss: 1.909163]\n",
      "epoch:29 step:28021 [D loss: 0.660363, acc: 60.94%] [G loss: 2.026149]\n",
      "epoch:29 step:28022 [D loss: 0.584666, acc: 74.22%] [G loss: 2.097072]\n",
      "epoch:29 step:28023 [D loss: 0.708415, acc: 55.47%] [G loss: 1.785822]\n",
      "epoch:29 step:28024 [D loss: 0.645246, acc: 60.94%] [G loss: 1.899670]\n",
      "epoch:29 step:28025 [D loss: 0.707667, acc: 56.25%] [G loss: 1.872208]\n",
      "epoch:29 step:28026 [D loss: 0.706824, acc: 55.47%] [G loss: 1.809654]\n",
      "epoch:29 step:28027 [D loss: 0.627546, acc: 68.75%] [G loss: 1.813085]\n",
      "epoch:29 step:28028 [D loss: 0.676363, acc: 56.25%] [G loss: 1.719858]\n",
      "epoch:29 step:28029 [D loss: 0.652492, acc: 59.38%] [G loss: 1.761375]\n",
      "epoch:29 step:28030 [D loss: 0.651438, acc: 60.94%] [G loss: 1.844372]\n",
      "epoch:29 step:28031 [D loss: 0.664937, acc: 59.38%] [G loss: 1.693803]\n",
      "epoch:29 step:28032 [D loss: 0.729790, acc: 50.00%] [G loss: 1.715258]\n",
      "epoch:29 step:28033 [D loss: 0.590396, acc: 72.66%] [G loss: 1.929321]\n",
      "epoch:29 step:28034 [D loss: 0.694156, acc: 58.59%] [G loss: 1.789646]\n",
      "epoch:29 step:28035 [D loss: 0.679758, acc: 60.16%] [G loss: 1.902764]\n",
      "epoch:29 step:28036 [D loss: 0.639799, acc: 59.38%] [G loss: 1.842464]\n",
      "epoch:29 step:28037 [D loss: 0.645034, acc: 63.28%] [G loss: 1.835866]\n",
      "epoch:29 step:28038 [D loss: 0.654566, acc: 64.84%] [G loss: 1.872117]\n",
      "epoch:29 step:28039 [D loss: 0.654862, acc: 61.72%] [G loss: 1.876128]\n",
      "epoch:29 step:28040 [D loss: 0.672301, acc: 57.03%] [G loss: 1.760748]\n",
      "epoch:29 step:28041 [D loss: 0.639426, acc: 64.06%] [G loss: 1.862464]\n",
      "epoch:29 step:28042 [D loss: 0.669820, acc: 63.28%] [G loss: 1.683383]\n",
      "epoch:29 step:28043 [D loss: 0.691344, acc: 58.59%] [G loss: 1.881841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:28044 [D loss: 0.654026, acc: 63.28%] [G loss: 1.782991]\n",
      "epoch:29 step:28045 [D loss: 0.650904, acc: 63.28%] [G loss: 1.835670]\n",
      "epoch:29 step:28046 [D loss: 0.690566, acc: 56.25%] [G loss: 1.779463]\n",
      "epoch:29 step:28047 [D loss: 0.666742, acc: 61.72%] [G loss: 1.687043]\n",
      "epoch:29 step:28048 [D loss: 0.640897, acc: 63.28%] [G loss: 1.806522]\n",
      "epoch:29 step:28049 [D loss: 0.651667, acc: 62.50%] [G loss: 1.952697]\n",
      "epoch:29 step:28050 [D loss: 0.664765, acc: 57.81%] [G loss: 1.801570]\n",
      "epoch:29 step:28051 [D loss: 0.610371, acc: 69.53%] [G loss: 1.772430]\n",
      "epoch:29 step:28052 [D loss: 0.675212, acc: 66.41%] [G loss: 1.783020]\n",
      "epoch:29 step:28053 [D loss: 0.637926, acc: 59.38%] [G loss: 1.891750]\n",
      "epoch:29 step:28054 [D loss: 0.715461, acc: 55.47%] [G loss: 1.857477]\n",
      "epoch:29 step:28055 [D loss: 0.664111, acc: 60.16%] [G loss: 1.742107]\n",
      "epoch:29 step:28056 [D loss: 0.664323, acc: 60.16%] [G loss: 1.846752]\n",
      "epoch:29 step:28057 [D loss: 0.642769, acc: 63.28%] [G loss: 1.927038]\n",
      "epoch:29 step:28058 [D loss: 0.627941, acc: 62.50%] [G loss: 1.867486]\n",
      "epoch:29 step:28059 [D loss: 0.631657, acc: 67.19%] [G loss: 1.843940]\n",
      "epoch:29 step:28060 [D loss: 0.637617, acc: 65.62%] [G loss: 1.827676]\n",
      "epoch:29 step:28061 [D loss: 0.655681, acc: 64.06%] [G loss: 1.833248]\n",
      "epoch:29 step:28062 [D loss: 0.649193, acc: 59.38%] [G loss: 1.822027]\n",
      "epoch:29 step:28063 [D loss: 0.632372, acc: 67.97%] [G loss: 1.846864]\n",
      "epoch:29 step:28064 [D loss: 0.653541, acc: 60.94%] [G loss: 1.762898]\n",
      "epoch:29 step:28065 [D loss: 0.640721, acc: 63.28%] [G loss: 1.927160]\n",
      "epoch:29 step:28066 [D loss: 0.623485, acc: 63.28%] [G loss: 1.861017]\n",
      "epoch:29 step:28067 [D loss: 0.680639, acc: 60.94%] [G loss: 1.838208]\n",
      "epoch:29 step:28068 [D loss: 0.627137, acc: 66.41%] [G loss: 1.876831]\n",
      "epoch:29 step:28069 [D loss: 0.646641, acc: 64.06%] [G loss: 1.751012]\n",
      "epoch:29 step:28070 [D loss: 0.672157, acc: 57.81%] [G loss: 1.906699]\n",
      "epoch:29 step:28071 [D loss: 0.669367, acc: 57.81%] [G loss: 2.009332]\n",
      "epoch:29 step:28072 [D loss: 0.622440, acc: 64.84%] [G loss: 1.822708]\n",
      "epoch:29 step:28073 [D loss: 0.647537, acc: 60.16%] [G loss: 1.830024]\n",
      "epoch:29 step:28074 [D loss: 0.685055, acc: 59.38%] [G loss: 1.912678]\n",
      "epoch:29 step:28075 [D loss: 0.654382, acc: 64.06%] [G loss: 1.985321]\n",
      "epoch:29 step:28076 [D loss: 0.643035, acc: 67.97%] [G loss: 1.847964]\n",
      "epoch:29 step:28077 [D loss: 0.676605, acc: 58.59%] [G loss: 1.891916]\n",
      "epoch:29 step:28078 [D loss: 0.646075, acc: 65.62%] [G loss: 1.848444]\n",
      "epoch:29 step:28079 [D loss: 0.632928, acc: 60.94%] [G loss: 1.966375]\n",
      "epoch:29 step:28080 [D loss: 0.649564, acc: 60.94%] [G loss: 1.791905]\n",
      "epoch:29 step:28081 [D loss: 0.593229, acc: 68.75%] [G loss: 1.938128]\n",
      "epoch:29 step:28082 [D loss: 0.643818, acc: 60.16%] [G loss: 2.078578]\n",
      "epoch:29 step:28083 [D loss: 0.620280, acc: 68.75%] [G loss: 1.942282]\n",
      "epoch:29 step:28084 [D loss: 0.619418, acc: 65.62%] [G loss: 1.937644]\n",
      "epoch:29 step:28085 [D loss: 0.586396, acc: 68.75%] [G loss: 1.891131]\n",
      "epoch:29 step:28086 [D loss: 0.721341, acc: 53.12%] [G loss: 1.798969]\n",
      "epoch:29 step:28087 [D loss: 0.750924, acc: 50.00%] [G loss: 1.697633]\n",
      "epoch:29 step:28088 [D loss: 0.723526, acc: 50.78%] [G loss: 1.904273]\n",
      "epoch:29 step:28089 [D loss: 0.604278, acc: 61.72%] [G loss: 1.863864]\n",
      "epoch:29 step:28090 [D loss: 0.624188, acc: 63.28%] [G loss: 1.966080]\n",
      "epoch:29 step:28091 [D loss: 0.566377, acc: 75.78%] [G loss: 2.013905]\n",
      "epoch:29 step:28092 [D loss: 0.638877, acc: 61.72%] [G loss: 2.284067]\n",
      "epoch:29 step:28093 [D loss: 0.712480, acc: 56.25%] [G loss: 1.748570]\n",
      "epoch:29 step:28094 [D loss: 0.651925, acc: 64.06%] [G loss: 2.001979]\n",
      "epoch:29 step:28095 [D loss: 0.693424, acc: 54.69%] [G loss: 1.949165]\n",
      "epoch:29 step:28096 [D loss: 0.645508, acc: 68.75%] [G loss: 2.002578]\n",
      "epoch:29 step:28097 [D loss: 0.584652, acc: 69.53%] [G loss: 2.030225]\n",
      "epoch:29 step:28098 [D loss: 0.636421, acc: 66.41%] [G loss: 2.037708]\n",
      "epoch:29 step:28099 [D loss: 0.580778, acc: 64.84%] [G loss: 2.197139]\n",
      "epoch:29 step:28100 [D loss: 0.615465, acc: 63.28%] [G loss: 2.021302]\n",
      "epoch:29 step:28101 [D loss: 0.810236, acc: 50.00%] [G loss: 1.776517]\n",
      "epoch:29 step:28102 [D loss: 0.753598, acc: 53.12%] [G loss: 1.787573]\n",
      "epoch:29 step:28103 [D loss: 0.638383, acc: 64.84%] [G loss: 1.978226]\n",
      "epoch:29 step:28104 [D loss: 0.621413, acc: 64.84%] [G loss: 2.117143]\n",
      "epoch:29 step:28105 [D loss: 0.645546, acc: 64.84%] [G loss: 1.836192]\n",
      "epoch:29 step:28106 [D loss: 0.656276, acc: 67.19%] [G loss: 1.937875]\n",
      "epoch:29 step:28107 [D loss: 0.651318, acc: 65.62%] [G loss: 1.848645]\n",
      "epoch:29 step:28108 [D loss: 0.599744, acc: 73.44%] [G loss: 2.009720]\n",
      "epoch:29 step:28109 [D loss: 0.600818, acc: 73.44%] [G loss: 2.163832]\n",
      "epoch:29 step:28110 [D loss: 0.581892, acc: 71.88%] [G loss: 2.357088]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from keras.datasets import mnist,fashion_mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D, concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class BIGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "        self.x = []\n",
    "        self.y = np.zeros((31, 1), dtype=np.int)\n",
    "        self.y = list(self.y)\n",
    "        for i in range(31):\n",
    "            self.y[i] = []\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # Build the encoder\n",
    "        self.encoder = self.build_encoder()\n",
    "\n",
    "        # The part of the bigan that trains the discriminator and encoder\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Generate image from sampled noise\n",
    "        z = Input(shape=(self.latent_dim, ))\n",
    "        img_ = self.generator(z)\n",
    "\n",
    "        # Encode image\n",
    "        img = Input(shape=self.img_shape)\n",
    "        z_ = self.encoder(img)\n",
    "\n",
    "        # Latent -> img is fake, and img -> latent is valid\n",
    "        fake = self.discriminator([z, img_])\n",
    "        valid = self.discriminator([z_, img])\n",
    "\n",
    "        # Set up and compile the combined model\n",
    "        # Trains generator to fool the discriminator\n",
    "        self.bigan_generator = Model([z, img], [fake, valid])\n",
    "        self.bigan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_encoder(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(self.latent_dim))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        z = model(img)\n",
    "\n",
    "        return Model(img, z)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        gen_img = model(z)\n",
    "\n",
    "        return Model(z, gen_img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        z = Input(shape=(self.latent_dim, ))\n",
    "        img = Input(shape=self.img_shape)\n",
    "        d_in = concatenate([z, Flatten()(img)])\n",
    "\n",
    "        model = Dense(1024)(d_in)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        model = Dense(1024)(model)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        model = Dense(1024)(model)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        validity = Dense(1, activation=\"sigmoid\")(model)\n",
    "\n",
    "        return Model([z, img], validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (X_test, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "        # X_test = X_test / 127.5 - 1.\n",
    "        X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                z = np.random.normal(size=(batch_size, self.latent_dim))\n",
    "                imgs_ = self.generator.predict(z)\n",
    "\n",
    "                # Select a random batch of images and encode\n",
    "                # idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                # imgs = X_train[idx]\n",
    "                z_ = self.encoder.predict(image_batch)\n",
    "\n",
    "                # Train the discriminator (img -> z is valid, z -> img is fake)\n",
    "                d_loss_real = self.discriminator.train_on_batch([z_, image_batch], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([z, imgs_], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Train the generator (z -> img is valid and img -> z is is invalid)\n",
    "                g_loss = self.bigan_generator.train_on_batch([z, image_batch], [valid, fake])\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"epoch:%d step:%d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch,global_step, d_loss[0],\n",
    "                                                                                   100 * d_loss[1], g_loss[0]))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                sampleSize = 5000\n",
    "                # If at save interval => save generated image samples\n",
    "                if global_step % sample_interval == 0:\n",
    "                    s = self.metrics(global_step, X_test, sampleSize)\n",
    "        for i in range(len(s)):\n",
    "            self.y[i] = [float(j) / max(self.y[i]) for j in self.y[i]]#对值进行归一化处理\n",
    "\n",
    "        for i in range(len(s)):\n",
    "            font1={'size':8}\n",
    "\n",
    "            plt.plot(self.x, self.y[i], label=labels_name[i])\n",
    "            plt.legend(loc='lower right',prop=font1)\n",
    "            plt.savefig('saved_models_bigan/{}.png'.format(labels_name[i]))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "    def metrics(self, epoch, X_test, sampleSize):\n",
    "        self.x.append(epoch)\n",
    "        r, c = 10, sampleSize // 10\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "#         sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise])\n",
    "        x_dataset = MyDataset(X_test[:sampleSize])\n",
    "        # print(x_dataset[0].shape)\n",
    "        x_real_loader = Data.DataLoader(dataset=x_dataset, batch_size=sampleSize, shuffle=True)\n",
    "        x_fake_dataset = MyDataset(gen_imgs)\n",
    "        x_fake_loader = Data.DataLoader(dataset=x_fake_dataset, batch_size=sampleSize, shuffle=True)\n",
    "        s = compute_score_raw(x_real_loader, x_fake_loader, 256, '/real/', './fake', conv_model='tfgan',\n",
    "                              workers=int(1))\n",
    "        real_images = tf.convert_to_tensor(X_test)  # real images\n",
    "#         # MNIST_CLASSIFIER_FROZEN_GRAPH = '.\\classify_mnist_graph_def.pb'\n",
    "        gen_imgs = np.array(gen_imgs)\n",
    "        eval_images = tf.convert_to_tensor(gen_imgs)\n",
    "        eval_score = utils.mnist_score(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH)  # IS score\n",
    "        frechet_distance = utils.mnist_frechet_distance(real_images, eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH)\n",
    "        mnist_score, f_distance = sess.run([eval_score, frechet_distance])\n",
    "        # print(mnist_score)\n",
    "        # print(f_distance)\n",
    "        # s[14]=mnist_score\n",
    "        # s[16]=f_distance\n",
    "        s[17] = mnist_score\n",
    "        s[18] = f_distance\n",
    "        print('IS socre: %f' % mnist_score)\n",
    "        print('FID: %f' % f_distance)\n",
    "\n",
    "        for i in range(len(s)):\n",
    "            print(i, \"=\", s[i])\n",
    "        for i in range(len(s)):\n",
    "            self.y[i].append(s[i])\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('epoch:' + str(epoch))\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('%.8f' % (i) for i in s)\n",
    "        f.writelines('\\n')\n",
    "        return s\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    bigan = BIGAN()\n",
    "    bigan.train(epochs=30, batch_size=64, sample_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
