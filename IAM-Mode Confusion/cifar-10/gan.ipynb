{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x7f90c844c2e8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# #指定使用那块GUP训练\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "config = tf.ConfigProto()\n",
    "# 设置最大占有GPU不超过显存的70%\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7 \n",
    "# # 重点：设置动态分配GPU\n",
    "config.gpu_options.allow_growth = True\n",
    "# 创建session时\n",
    "tf.Session(config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (9): ReLU()\n",
      "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (12): ReLU()\n",
      "  (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (14): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (16): ReLU()\n",
      "  (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (19): ReLU()\n",
      "  (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (21): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (22): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (23): ReLU()\n",
      "  (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,704,961\n",
      "Trainable params: 1,704,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3072)              3148800   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 3,838,720\n",
      "Trainable params: 3,835,136\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "(50000, 32, 32, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:1 [D loss: 0.738315, acc.: 48.44%] [G loss: 0.658707]\n",
      "epoch:0 step:2 [D loss: 0.595730, acc.: 60.94%] [G loss: 0.624397]\n",
      "epoch:0 step:3 [D loss: 0.567147, acc.: 59.38%] [G loss: 0.672657]\n",
      "epoch:0 step:4 [D loss: 0.499619, acc.: 67.97%] [G loss: 0.705538]\n",
      "epoch:0 step:5 [D loss: 0.519506, acc.: 63.28%] [G loss: 0.749025]\n",
      "epoch:0 step:6 [D loss: 0.484215, acc.: 70.31%] [G loss: 0.814351]\n",
      "epoch:0 step:7 [D loss: 0.475841, acc.: 72.66%] [G loss: 0.893869]\n",
      "epoch:0 step:8 [D loss: 0.517761, acc.: 65.62%] [G loss: 0.938398]\n",
      "epoch:0 step:9 [D loss: 0.470900, acc.: 75.00%] [G loss: 0.961686]\n",
      "epoch:0 step:10 [D loss: 0.466170, acc.: 75.00%] [G loss: 1.005195]\n",
      "epoch:0 step:11 [D loss: 0.452090, acc.: 75.78%] [G loss: 1.004033]\n",
      "epoch:0 step:12 [D loss: 0.409472, acc.: 81.25%] [G loss: 1.076959]\n",
      "epoch:0 step:13 [D loss: 0.432678, acc.: 78.12%] [G loss: 1.044729]\n",
      "epoch:0 step:14 [D loss: 0.438130, acc.: 78.91%] [G loss: 1.126150]\n",
      "epoch:0 step:15 [D loss: 0.374202, acc.: 82.81%] [G loss: 1.120651]\n",
      "epoch:0 step:16 [D loss: 0.405688, acc.: 78.91%] [G loss: 1.186460]\n",
      "epoch:0 step:17 [D loss: 0.371257, acc.: 83.59%] [G loss: 1.212480]\n",
      "epoch:0 step:18 [D loss: 0.291157, acc.: 91.41%] [G loss: 1.265394]\n",
      "epoch:0 step:19 [D loss: 0.376996, acc.: 85.94%] [G loss: 1.340252]\n",
      "epoch:0 step:20 [D loss: 0.394909, acc.: 85.94%] [G loss: 1.308265]\n",
      "epoch:0 step:21 [D loss: 0.372441, acc.: 85.16%] [G loss: 1.343325]\n",
      "epoch:0 step:22 [D loss: 0.401005, acc.: 80.47%] [G loss: 1.378224]\n",
      "epoch:0 step:23 [D loss: 0.386799, acc.: 83.59%] [G loss: 1.352301]\n",
      "epoch:0 step:24 [D loss: 0.526160, acc.: 76.56%] [G loss: 1.444505]\n",
      "epoch:0 step:25 [D loss: 0.418857, acc.: 81.25%] [G loss: 1.487811]\n",
      "epoch:0 step:26 [D loss: 0.350477, acc.: 85.94%] [G loss: 1.612932]\n",
      "epoch:0 step:27 [D loss: 0.305446, acc.: 89.84%] [G loss: 1.714375]\n",
      "epoch:0 step:28 [D loss: 0.521929, acc.: 75.00%] [G loss: 1.776693]\n",
      "epoch:0 step:29 [D loss: 0.367054, acc.: 87.50%] [G loss: 1.727174]\n",
      "epoch:0 step:30 [D loss: 0.491486, acc.: 83.59%] [G loss: 1.584224]\n",
      "epoch:0 step:31 [D loss: 0.463867, acc.: 84.38%] [G loss: 1.583081]\n",
      "epoch:0 step:32 [D loss: 0.555513, acc.: 76.56%] [G loss: 1.786106]\n",
      "epoch:0 step:33 [D loss: 0.560355, acc.: 77.34%] [G loss: 1.680098]\n",
      "epoch:0 step:34 [D loss: 0.454951, acc.: 80.47%] [G loss: 1.628150]\n",
      "epoch:0 step:35 [D loss: 0.423669, acc.: 83.59%] [G loss: 1.911080]\n",
      "epoch:0 step:36 [D loss: 0.743391, acc.: 65.62%] [G loss: 1.608534]\n",
      "epoch:0 step:37 [D loss: 0.525115, acc.: 76.56%] [G loss: 1.575614]\n",
      "epoch:0 step:38 [D loss: 0.565122, acc.: 69.53%] [G loss: 1.481525]\n",
      "epoch:0 step:39 [D loss: 0.705783, acc.: 67.19%] [G loss: 1.565689]\n",
      "epoch:0 step:40 [D loss: 0.650256, acc.: 67.97%] [G loss: 1.582474]\n",
      "epoch:0 step:41 [D loss: 0.624537, acc.: 70.31%] [G loss: 2.059522]\n",
      "epoch:0 step:42 [D loss: 0.601058, acc.: 71.88%] [G loss: 2.126668]\n",
      "epoch:0 step:43 [D loss: 0.555080, acc.: 83.59%] [G loss: 1.683802]\n",
      "epoch:0 step:44 [D loss: 0.455845, acc.: 82.81%] [G loss: 2.024448]\n",
      "epoch:0 step:45 [D loss: 0.495837, acc.: 78.12%] [G loss: 1.863039]\n",
      "epoch:0 step:46 [D loss: 0.587082, acc.: 71.88%] [G loss: 1.870155]\n",
      "epoch:0 step:47 [D loss: 0.642767, acc.: 67.19%] [G loss: 1.824490]\n",
      "epoch:0 step:48 [D loss: 0.688191, acc.: 65.62%] [G loss: 1.757520]\n",
      "epoch:0 step:49 [D loss: 0.766692, acc.: 60.94%] [G loss: 2.148758]\n",
      "epoch:0 step:50 [D loss: 0.591188, acc.: 72.66%] [G loss: 1.806767]\n",
      "epoch:0 step:51 [D loss: 0.580870, acc.: 74.22%] [G loss: 1.483050]\n",
      "epoch:0 step:52 [D loss: 0.822584, acc.: 61.72%] [G loss: 1.576831]\n",
      "epoch:0 step:53 [D loss: 0.619959, acc.: 64.84%] [G loss: 1.828665]\n",
      "epoch:0 step:54 [D loss: 0.660958, acc.: 65.62%] [G loss: 2.072998]\n",
      "epoch:0 step:55 [D loss: 0.631960, acc.: 70.31%] [G loss: 1.714586]\n",
      "epoch:0 step:56 [D loss: 0.582994, acc.: 67.97%] [G loss: 1.728101]\n",
      "epoch:0 step:57 [D loss: 0.570728, acc.: 62.50%] [G loss: 1.952589]\n",
      "epoch:0 step:58 [D loss: 0.707940, acc.: 64.06%] [G loss: 1.865488]\n",
      "epoch:0 step:59 [D loss: 0.630802, acc.: 67.97%] [G loss: 1.799136]\n",
      "epoch:0 step:60 [D loss: 0.745428, acc.: 61.72%] [G loss: 1.781163]\n",
      "epoch:0 step:61 [D loss: 0.800472, acc.: 55.47%] [G loss: 1.631137]\n",
      "epoch:0 step:62 [D loss: 0.678442, acc.: 62.50%] [G loss: 1.814124]\n",
      "epoch:0 step:63 [D loss: 0.544831, acc.: 75.00%] [G loss: 2.057168]\n",
      "epoch:0 step:64 [D loss: 0.744116, acc.: 60.94%] [G loss: 1.553016]\n",
      "epoch:0 step:65 [D loss: 0.503771, acc.: 75.78%] [G loss: 1.705963]\n",
      "epoch:0 step:66 [D loss: 0.568165, acc.: 71.88%] [G loss: 1.929568]\n",
      "epoch:0 step:67 [D loss: 0.709633, acc.: 60.94%] [G loss: 1.767303]\n",
      "epoch:0 step:68 [D loss: 0.693068, acc.: 61.72%] [G loss: 1.747774]\n",
      "epoch:0 step:69 [D loss: 0.634543, acc.: 68.75%] [G loss: 1.679899]\n",
      "epoch:0 step:70 [D loss: 0.632523, acc.: 66.41%] [G loss: 1.711889]\n",
      "epoch:0 step:71 [D loss: 0.700144, acc.: 60.94%] [G loss: 1.507583]\n",
      "epoch:0 step:72 [D loss: 0.782273, acc.: 58.59%] [G loss: 1.674910]\n",
      "epoch:0 step:73 [D loss: 0.691959, acc.: 59.38%] [G loss: 2.067292]\n",
      "epoch:0 step:74 [D loss: 0.634981, acc.: 66.41%] [G loss: 1.699810]\n",
      "epoch:0 step:75 [D loss: 0.562534, acc.: 75.00%] [G loss: 1.781918]\n",
      "epoch:0 step:76 [D loss: 0.717846, acc.: 54.69%] [G loss: 1.753827]\n",
      "epoch:0 step:77 [D loss: 0.765423, acc.: 53.91%] [G loss: 1.618065]\n",
      "epoch:0 step:78 [D loss: 0.626145, acc.: 68.75%] [G loss: 1.731437]\n",
      "epoch:0 step:79 [D loss: 0.729358, acc.: 61.72%] [G loss: 1.603074]\n",
      "epoch:0 step:80 [D loss: 0.657333, acc.: 68.75%] [G loss: 1.841422]\n",
      "epoch:0 step:81 [D loss: 0.729092, acc.: 62.50%] [G loss: 1.775917]\n",
      "epoch:0 step:82 [D loss: 0.778883, acc.: 58.59%] [G loss: 1.754171]\n",
      "epoch:0 step:83 [D loss: 0.784626, acc.: 53.91%] [G loss: 1.585388]\n",
      "epoch:0 step:84 [D loss: 0.847507, acc.: 54.69%] [G loss: 1.777720]\n",
      "epoch:0 step:85 [D loss: 0.830436, acc.: 46.09%] [G loss: 1.726489]\n",
      "epoch:0 step:86 [D loss: 0.943281, acc.: 45.31%] [G loss: 1.482051]\n",
      "epoch:0 step:87 [D loss: 0.797567, acc.: 49.22%] [G loss: 1.601423]\n",
      "epoch:0 step:88 [D loss: 0.744918, acc.: 55.47%] [G loss: 1.705415]\n",
      "epoch:0 step:89 [D loss: 0.690034, acc.: 64.84%] [G loss: 1.691483]\n",
      "epoch:0 step:90 [D loss: 0.783477, acc.: 58.59%] [G loss: 1.754502]\n",
      "epoch:0 step:91 [D loss: 0.867560, acc.: 44.53%] [G loss: 1.882530]\n",
      "epoch:0 step:92 [D loss: 0.807332, acc.: 66.41%] [G loss: 1.624477]\n",
      "epoch:0 step:93 [D loss: 0.954801, acc.: 42.19%] [G loss: 1.458117]\n",
      "epoch:0 step:94 [D loss: 0.671417, acc.: 62.50%] [G loss: 1.662983]\n",
      "epoch:0 step:95 [D loss: 0.828149, acc.: 49.22%] [G loss: 1.657676]\n",
      "epoch:0 step:96 [D loss: 0.717794, acc.: 52.34%] [G loss: 1.613981]\n",
      "epoch:0 step:97 [D loss: 0.670729, acc.: 53.91%] [G loss: 1.612483]\n",
      "epoch:0 step:98 [D loss: 0.686097, acc.: 59.38%] [G loss: 1.568667]\n",
      "epoch:0 step:99 [D loss: 0.719847, acc.: 53.12%] [G loss: 1.577660]\n",
      "epoch:0 step:100 [D loss: 0.760167, acc.: 58.59%] [G loss: 1.542542]\n",
      "epoch:0 step:101 [D loss: 0.662453, acc.: 58.59%] [G loss: 1.687010]\n",
      "epoch:0 step:102 [D loss: 0.743286, acc.: 57.03%] [G loss: 1.590334]\n",
      "epoch:0 step:103 [D loss: 0.692752, acc.: 59.38%] [G loss: 1.598961]\n",
      "epoch:0 step:104 [D loss: 0.745807, acc.: 51.56%] [G loss: 1.922127]\n",
      "epoch:0 step:105 [D loss: 0.736655, acc.: 56.25%] [G loss: 1.875693]\n",
      "epoch:0 step:106 [D loss: 0.711752, acc.: 62.50%] [G loss: 1.534985]\n",
      "epoch:0 step:107 [D loss: 0.719508, acc.: 60.16%] [G loss: 1.390029]\n",
      "epoch:0 step:108 [D loss: 0.686930, acc.: 61.72%] [G loss: 1.422409]\n",
      "epoch:0 step:109 [D loss: 0.767941, acc.: 53.91%] [G loss: 1.685138]\n",
      "epoch:0 step:110 [D loss: 0.633968, acc.: 58.59%] [G loss: 1.752527]\n",
      "epoch:0 step:111 [D loss: 0.905713, acc.: 44.53%] [G loss: 1.665447]\n",
      "epoch:0 step:112 [D loss: 0.703808, acc.: 62.50%] [G loss: 1.499417]\n",
      "epoch:0 step:113 [D loss: 0.728730, acc.: 57.81%] [G loss: 1.478350]\n",
      "epoch:0 step:114 [D loss: 0.767798, acc.: 46.09%] [G loss: 1.427578]\n",
      "epoch:0 step:115 [D loss: 0.614847, acc.: 70.31%] [G loss: 1.588583]\n",
      "epoch:0 step:116 [D loss: 0.604374, acc.: 69.53%] [G loss: 1.817288]\n",
      "epoch:0 step:117 [D loss: 0.680005, acc.: 67.19%] [G loss: 1.751334]\n",
      "epoch:0 step:118 [D loss: 0.711735, acc.: 60.16%] [G loss: 1.398282]\n",
      "epoch:0 step:119 [D loss: 0.656689, acc.: 66.41%] [G loss: 1.617107]\n",
      "epoch:0 step:120 [D loss: 0.610141, acc.: 70.31%] [G loss: 1.743616]\n",
      "epoch:0 step:121 [D loss: 0.610867, acc.: 65.62%] [G loss: 1.686864]\n",
      "epoch:0 step:122 [D loss: 0.657348, acc.: 61.72%] [G loss: 1.353031]\n",
      "epoch:0 step:123 [D loss: 0.617782, acc.: 59.38%] [G loss: 1.391871]\n",
      "epoch:0 step:124 [D loss: 0.615422, acc.: 73.44%] [G loss: 1.513140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:125 [D loss: 0.604209, acc.: 69.53%] [G loss: 1.549466]\n",
      "epoch:0 step:126 [D loss: 0.657025, acc.: 64.84%] [G loss: 1.446825]\n",
      "epoch:0 step:127 [D loss: 0.604687, acc.: 73.44%] [G loss: 1.453085]\n",
      "epoch:0 step:128 [D loss: 0.658838, acc.: 57.81%] [G loss: 1.603979]\n",
      "epoch:0 step:129 [D loss: 0.716307, acc.: 54.69%] [G loss: 1.458737]\n",
      "epoch:0 step:130 [D loss: 0.674877, acc.: 58.59%] [G loss: 1.382178]\n",
      "epoch:0 step:131 [D loss: 0.766708, acc.: 54.69%] [G loss: 1.525617]\n",
      "epoch:0 step:132 [D loss: 0.605930, acc.: 66.41%] [G loss: 1.776689]\n",
      "epoch:0 step:133 [D loss: 0.686491, acc.: 59.38%] [G loss: 1.569793]\n",
      "epoch:0 step:134 [D loss: 0.681302, acc.: 59.38%] [G loss: 1.380345]\n",
      "epoch:0 step:135 [D loss: 0.704147, acc.: 58.59%] [G loss: 1.415540]\n",
      "epoch:0 step:136 [D loss: 0.632356, acc.: 64.06%] [G loss: 1.437289]\n",
      "epoch:0 step:137 [D loss: 0.644284, acc.: 68.75%] [G loss: 1.547157]\n",
      "epoch:0 step:138 [D loss: 0.641474, acc.: 66.41%] [G loss: 1.392242]\n",
      "epoch:0 step:139 [D loss: 0.644494, acc.: 67.19%] [G loss: 1.432099]\n",
      "epoch:0 step:140 [D loss: 0.705313, acc.: 61.72%] [G loss: 1.485823]\n",
      "epoch:0 step:141 [D loss: 0.648381, acc.: 67.97%] [G loss: 1.474170]\n",
      "epoch:0 step:142 [D loss: 0.669185, acc.: 65.62%] [G loss: 1.425697]\n",
      "epoch:0 step:143 [D loss: 0.640370, acc.: 68.75%] [G loss: 1.581396]\n",
      "epoch:0 step:144 [D loss: 0.802861, acc.: 57.81%] [G loss: 1.564772]\n",
      "epoch:0 step:145 [D loss: 0.715755, acc.: 60.94%] [G loss: 1.531198]\n",
      "epoch:0 step:146 [D loss: 0.622519, acc.: 62.50%] [G loss: 1.617284]\n",
      "epoch:0 step:147 [D loss: 0.664403, acc.: 66.41%] [G loss: 1.594954]\n",
      "epoch:0 step:148 [D loss: 0.687143, acc.: 57.03%] [G loss: 1.411281]\n",
      "epoch:0 step:149 [D loss: 0.718380, acc.: 58.59%] [G loss: 1.323870]\n",
      "epoch:0 step:150 [D loss: 0.692880, acc.: 61.72%] [G loss: 1.416727]\n",
      "epoch:0 step:151 [D loss: 0.593848, acc.: 67.19%] [G loss: 1.337570]\n",
      "epoch:0 step:152 [D loss: 0.702495, acc.: 54.69%] [G loss: 1.339051]\n",
      "epoch:0 step:153 [D loss: 0.693683, acc.: 59.38%] [G loss: 1.428315]\n",
      "epoch:0 step:154 [D loss: 0.616088, acc.: 67.97%] [G loss: 1.352688]\n",
      "epoch:0 step:155 [D loss: 0.706213, acc.: 54.69%] [G loss: 1.447862]\n",
      "epoch:0 step:156 [D loss: 0.734712, acc.: 50.78%] [G loss: 1.465009]\n",
      "epoch:0 step:157 [D loss: 0.662720, acc.: 61.72%] [G loss: 1.486041]\n",
      "epoch:0 step:158 [D loss: 0.709547, acc.: 58.59%] [G loss: 1.377151]\n",
      "epoch:0 step:159 [D loss: 0.667232, acc.: 60.16%] [G loss: 1.280435]\n",
      "epoch:0 step:160 [D loss: 0.632162, acc.: 63.28%] [G loss: 1.313451]\n",
      "epoch:0 step:161 [D loss: 0.625261, acc.: 71.09%] [G loss: 1.402550]\n",
      "epoch:0 step:162 [D loss: 0.635048, acc.: 63.28%] [G loss: 1.406504]\n",
      "epoch:0 step:163 [D loss: 0.624731, acc.: 69.53%] [G loss: 1.571099]\n",
      "epoch:0 step:164 [D loss: 0.674704, acc.: 60.94%] [G loss: 1.408758]\n",
      "epoch:0 step:165 [D loss: 0.670688, acc.: 60.16%] [G loss: 1.332956]\n",
      "epoch:0 step:166 [D loss: 0.619869, acc.: 66.41%] [G loss: 1.482519]\n",
      "epoch:0 step:167 [D loss: 0.722224, acc.: 60.94%] [G loss: 1.498835]\n",
      "epoch:0 step:168 [D loss: 0.624418, acc.: 64.84%] [G loss: 1.496142]\n",
      "epoch:0 step:169 [D loss: 0.682698, acc.: 56.25%] [G loss: 1.498802]\n",
      "epoch:0 step:170 [D loss: 0.638731, acc.: 60.94%] [G loss: 1.474362]\n",
      "epoch:0 step:171 [D loss: 0.617520, acc.: 70.31%] [G loss: 1.562832]\n",
      "epoch:0 step:172 [D loss: 0.691794, acc.: 68.75%] [G loss: 1.626563]\n",
      "epoch:0 step:173 [D loss: 0.718009, acc.: 57.81%] [G loss: 1.444561]\n",
      "epoch:0 step:174 [D loss: 0.756626, acc.: 56.25%] [G loss: 1.569411]\n",
      "epoch:0 step:175 [D loss: 0.835484, acc.: 46.88%] [G loss: 1.463625]\n",
      "epoch:0 step:176 [D loss: 0.725144, acc.: 62.50%] [G loss: 1.529252]\n",
      "epoch:0 step:177 [D loss: 0.668759, acc.: 59.38%] [G loss: 1.340314]\n",
      "epoch:0 step:178 [D loss: 0.711026, acc.: 61.72%] [G loss: 1.530336]\n",
      "epoch:0 step:179 [D loss: 0.695756, acc.: 58.59%] [G loss: 1.588293]\n",
      "epoch:0 step:180 [D loss: 0.687953, acc.: 62.50%] [G loss: 1.537521]\n",
      "epoch:0 step:181 [D loss: 0.732099, acc.: 50.00%] [G loss: 1.452828]\n",
      "epoch:0 step:182 [D loss: 0.796806, acc.: 52.34%] [G loss: 1.432239]\n",
      "epoch:0 step:183 [D loss: 0.770180, acc.: 43.75%] [G loss: 1.404413]\n",
      "epoch:0 step:184 [D loss: 0.734560, acc.: 50.78%] [G loss: 1.275328]\n",
      "epoch:0 step:185 [D loss: 0.716624, acc.: 57.03%] [G loss: 1.333463]\n",
      "epoch:0 step:186 [D loss: 0.687330, acc.: 60.16%] [G loss: 1.502635]\n",
      "epoch:0 step:187 [D loss: 0.685325, acc.: 59.38%] [G loss: 1.450697]\n",
      "epoch:0 step:188 [D loss: 0.776790, acc.: 54.69%] [G loss: 1.403669]\n",
      "epoch:0 step:189 [D loss: 0.663873, acc.: 57.03%] [G loss: 1.421472]\n",
      "epoch:0 step:190 [D loss: 0.691885, acc.: 60.16%] [G loss: 1.350604]\n",
      "epoch:0 step:191 [D loss: 0.704647, acc.: 57.81%] [G loss: 1.558298]\n",
      "epoch:0 step:192 [D loss: 0.676940, acc.: 59.38%] [G loss: 1.421737]\n",
      "epoch:0 step:193 [D loss: 0.738035, acc.: 50.00%] [G loss: 1.315826]\n",
      "epoch:0 step:194 [D loss: 0.672664, acc.: 64.84%] [G loss: 1.383487]\n",
      "epoch:0 step:195 [D loss: 0.659599, acc.: 63.28%] [G loss: 1.384625]\n",
      "epoch:0 step:196 [D loss: 0.669654, acc.: 59.38%] [G loss: 1.439096]\n",
      "epoch:0 step:197 [D loss: 0.765323, acc.: 50.00%] [G loss: 1.256505]\n",
      "epoch:0 step:198 [D loss: 0.661734, acc.: 59.38%] [G loss: 1.422183]\n",
      "epoch:0 step:199 [D loss: 0.670740, acc.: 60.94%] [G loss: 1.578324]\n",
      "epoch:0 step:200 [D loss: 0.699937, acc.: 57.81%] [G loss: 1.443732]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:41: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:201 [D loss: 0.720004, acc.: 58.59%] [G loss: 1.319124]\n",
      "epoch:0 step:202 [D loss: 0.721752, acc.: 58.59%] [G loss: 1.430658]\n",
      "epoch:0 step:203 [D loss: 0.637097, acc.: 70.31%] [G loss: 1.445238]\n",
      "epoch:0 step:204 [D loss: 0.649855, acc.: 60.16%] [G loss: 1.431673]\n",
      "epoch:0 step:205 [D loss: 0.777353, acc.: 46.88%] [G loss: 1.209685]\n",
      "epoch:0 step:206 [D loss: 0.802152, acc.: 43.75%] [G loss: 1.289249]\n",
      "epoch:0 step:207 [D loss: 0.626333, acc.: 61.72%] [G loss: 1.308912]\n",
      "epoch:0 step:208 [D loss: 0.653610, acc.: 64.84%] [G loss: 1.328336]\n",
      "epoch:0 step:209 [D loss: 0.718488, acc.: 60.16%] [G loss: 1.373764]\n",
      "epoch:0 step:210 [D loss: 0.673695, acc.: 56.25%] [G loss: 1.448408]\n",
      "epoch:0 step:211 [D loss: 0.614278, acc.: 67.19%] [G loss: 1.314063]\n",
      "epoch:0 step:212 [D loss: 0.696020, acc.: 56.25%] [G loss: 1.324877]\n",
      "epoch:0 step:213 [D loss: 0.768990, acc.: 46.09%] [G loss: 1.395663]\n",
      "epoch:0 step:214 [D loss: 0.771794, acc.: 50.78%] [G loss: 1.364855]\n",
      "epoch:0 step:215 [D loss: 0.631939, acc.: 65.62%] [G loss: 1.239072]\n",
      "epoch:0 step:216 [D loss: 0.651572, acc.: 61.72%] [G loss: 1.276916]\n",
      "epoch:0 step:217 [D loss: 0.725237, acc.: 52.34%] [G loss: 1.299899]\n",
      "epoch:0 step:218 [D loss: 0.660216, acc.: 60.16%] [G loss: 1.450300]\n",
      "epoch:0 step:219 [D loss: 0.658180, acc.: 60.94%] [G loss: 1.429020]\n",
      "epoch:0 step:220 [D loss: 0.695874, acc.: 58.59%] [G loss: 1.309823]\n",
      "epoch:0 step:221 [D loss: 0.630419, acc.: 67.97%] [G loss: 1.323956]\n",
      "epoch:0 step:222 [D loss: 0.694969, acc.: 55.47%] [G loss: 1.321791]\n",
      "epoch:0 step:223 [D loss: 0.677705, acc.: 63.28%] [G loss: 1.375593]\n",
      "epoch:0 step:224 [D loss: 0.661319, acc.: 60.94%] [G loss: 1.243016]\n",
      "epoch:0 step:225 [D loss: 0.702399, acc.: 60.16%] [G loss: 1.311494]\n",
      "epoch:0 step:226 [D loss: 0.666318, acc.: 60.16%] [G loss: 1.372198]\n",
      "epoch:0 step:227 [D loss: 0.678565, acc.: 57.81%] [G loss: 1.319622]\n",
      "epoch:0 step:228 [D loss: 0.725692, acc.: 49.22%] [G loss: 1.367701]\n",
      "epoch:0 step:229 [D loss: 0.684199, acc.: 53.12%] [G loss: 1.465159]\n",
      "epoch:0 step:230 [D loss: 0.667661, acc.: 51.56%] [G loss: 1.367040]\n",
      "epoch:0 step:231 [D loss: 0.680117, acc.: 63.28%] [G loss: 1.371754]\n",
      "epoch:0 step:232 [D loss: 0.755486, acc.: 44.53%] [G loss: 1.412496]\n",
      "epoch:0 step:233 [D loss: 0.725610, acc.: 52.34%] [G loss: 1.368738]\n",
      "epoch:0 step:234 [D loss: 0.662975, acc.: 64.84%] [G loss: 1.302070]\n",
      "epoch:0 step:235 [D loss: 0.694422, acc.: 57.03%] [G loss: 1.266870]\n",
      "epoch:0 step:236 [D loss: 0.726505, acc.: 50.00%] [G loss: 1.303938]\n",
      "epoch:0 step:237 [D loss: 0.647434, acc.: 64.84%] [G loss: 1.354300]\n",
      "epoch:0 step:238 [D loss: 0.699456, acc.: 57.81%] [G loss: 1.402732]\n",
      "epoch:0 step:239 [D loss: 0.641824, acc.: 67.19%] [G loss: 1.321951]\n",
      "epoch:0 step:240 [D loss: 0.661198, acc.: 63.28%] [G loss: 1.331069]\n",
      "epoch:0 step:241 [D loss: 0.636929, acc.: 66.41%] [G loss: 1.263670]\n",
      "epoch:0 step:242 [D loss: 0.722106, acc.: 52.34%] [G loss: 1.167536]\n",
      "epoch:0 step:243 [D loss: 0.609835, acc.: 69.53%] [G loss: 1.315442]\n",
      "epoch:0 step:244 [D loss: 0.614968, acc.: 67.97%] [G loss: 1.399503]\n",
      "epoch:0 step:245 [D loss: 0.677726, acc.: 62.50%] [G loss: 1.389293]\n",
      "epoch:0 step:246 [D loss: 0.655816, acc.: 61.72%] [G loss: 1.453026]\n",
      "epoch:0 step:247 [D loss: 0.644378, acc.: 62.50%] [G loss: 1.412668]\n",
      "epoch:0 step:248 [D loss: 0.719280, acc.: 52.34%] [G loss: 1.431980]\n",
      "epoch:0 step:249 [D loss: 0.704292, acc.: 60.16%] [G loss: 1.382167]\n",
      "epoch:0 step:250 [D loss: 0.644006, acc.: 66.41%] [G loss: 1.376895]\n",
      "epoch:0 step:251 [D loss: 0.696431, acc.: 57.81%] [G loss: 1.388288]\n",
      "epoch:0 step:252 [D loss: 0.759594, acc.: 57.03%] [G loss: 1.223953]\n",
      "epoch:0 step:253 [D loss: 0.726864, acc.: 53.91%] [G loss: 1.425599]\n",
      "epoch:0 step:254 [D loss: 0.678742, acc.: 60.16%] [G loss: 1.359433]\n",
      "epoch:0 step:255 [D loss: 0.751749, acc.: 50.00%] [G loss: 1.268957]\n",
      "epoch:0 step:256 [D loss: 0.701488, acc.: 59.38%] [G loss: 1.328516]\n",
      "epoch:0 step:257 [D loss: 0.707587, acc.: 50.00%] [G loss: 1.190562]\n",
      "epoch:0 step:258 [D loss: 0.624070, acc.: 63.28%] [G loss: 1.274348]\n",
      "epoch:0 step:259 [D loss: 0.684869, acc.: 63.28%] [G loss: 1.340167]\n",
      "epoch:0 step:260 [D loss: 0.676153, acc.: 57.81%] [G loss: 1.330528]\n",
      "epoch:0 step:261 [D loss: 0.651739, acc.: 60.16%] [G loss: 1.380907]\n",
      "epoch:0 step:262 [D loss: 0.650326, acc.: 67.97%] [G loss: 1.307418]\n",
      "epoch:0 step:263 [D loss: 0.713365, acc.: 59.38%] [G loss: 1.337659]\n",
      "epoch:0 step:264 [D loss: 0.625826, acc.: 61.72%] [G loss: 1.346919]\n",
      "epoch:0 step:265 [D loss: 0.711156, acc.: 53.12%] [G loss: 1.362242]\n",
      "epoch:0 step:266 [D loss: 0.639827, acc.: 63.28%] [G loss: 1.274800]\n",
      "epoch:0 step:267 [D loss: 0.784350, acc.: 57.03%] [G loss: 1.259267]\n",
      "epoch:0 step:268 [D loss: 0.664930, acc.: 62.50%] [G loss: 1.324068]\n",
      "epoch:0 step:269 [D loss: 0.673301, acc.: 62.50%] [G loss: 1.215623]\n",
      "epoch:0 step:270 [D loss: 0.646829, acc.: 59.38%] [G loss: 1.304677]\n",
      "epoch:0 step:271 [D loss: 0.599670, acc.: 66.41%] [G loss: 1.354247]\n",
      "epoch:0 step:272 [D loss: 0.632979, acc.: 65.62%] [G loss: 1.390721]\n",
      "epoch:0 step:273 [D loss: 0.639617, acc.: 57.03%] [G loss: 1.455449]\n",
      "epoch:0 step:274 [D loss: 0.697886, acc.: 58.59%] [G loss: 1.346872]\n",
      "epoch:0 step:275 [D loss: 0.679479, acc.: 57.81%] [G loss: 1.306249]\n",
      "epoch:0 step:276 [D loss: 0.661424, acc.: 61.72%] [G loss: 1.416781]\n",
      "epoch:0 step:277 [D loss: 0.620128, acc.: 63.28%] [G loss: 1.360287]\n",
      "epoch:0 step:278 [D loss: 0.673243, acc.: 64.84%] [G loss: 1.413816]\n",
      "epoch:0 step:279 [D loss: 0.656828, acc.: 64.06%] [G loss: 1.330680]\n",
      "epoch:0 step:280 [D loss: 0.725744, acc.: 54.69%] [G loss: 1.317678]\n",
      "epoch:0 step:281 [D loss: 0.653276, acc.: 63.28%] [G loss: 1.239037]\n",
      "epoch:0 step:282 [D loss: 0.632746, acc.: 61.72%] [G loss: 1.328648]\n",
      "epoch:0 step:283 [D loss: 0.696691, acc.: 61.72%] [G loss: 1.376608]\n",
      "epoch:0 step:284 [D loss: 0.625258, acc.: 70.31%] [G loss: 1.264635]\n",
      "epoch:0 step:285 [D loss: 0.684263, acc.: 60.94%] [G loss: 1.233400]\n",
      "epoch:0 step:286 [D loss: 0.643906, acc.: 58.59%] [G loss: 1.425246]\n",
      "epoch:0 step:287 [D loss: 0.668379, acc.: 61.72%] [G loss: 1.327618]\n",
      "epoch:0 step:288 [D loss: 0.651373, acc.: 63.28%] [G loss: 1.284386]\n",
      "epoch:0 step:289 [D loss: 0.636720, acc.: 66.41%] [G loss: 1.348818]\n",
      "epoch:0 step:290 [D loss: 0.673975, acc.: 58.59%] [G loss: 1.226022]\n",
      "epoch:0 step:291 [D loss: 0.663297, acc.: 67.19%] [G loss: 1.317586]\n",
      "epoch:0 step:292 [D loss: 0.710246, acc.: 53.12%] [G loss: 1.323794]\n",
      "epoch:0 step:293 [D loss: 0.636882, acc.: 68.75%] [G loss: 1.296106]\n",
      "epoch:0 step:294 [D loss: 0.681013, acc.: 57.03%] [G loss: 1.235225]\n",
      "epoch:0 step:295 [D loss: 0.642492, acc.: 63.28%] [G loss: 1.278294]\n",
      "epoch:0 step:296 [D loss: 0.697943, acc.: 60.94%] [G loss: 1.271250]\n",
      "epoch:0 step:297 [D loss: 0.669831, acc.: 57.81%] [G loss: 1.205230]\n",
      "epoch:0 step:298 [D loss: 0.640726, acc.: 63.28%] [G loss: 1.272151]\n",
      "epoch:0 step:299 [D loss: 0.607449, acc.: 67.19%] [G loss: 1.222075]\n",
      "epoch:0 step:300 [D loss: 0.654877, acc.: 65.62%] [G loss: 1.266014]\n",
      "epoch:0 step:301 [D loss: 0.631978, acc.: 61.72%] [G loss: 1.255185]\n",
      "epoch:0 step:302 [D loss: 0.654191, acc.: 60.16%] [G loss: 1.326137]\n",
      "epoch:0 step:303 [D loss: 0.592010, acc.: 70.31%] [G loss: 1.386000]\n",
      "epoch:0 step:304 [D loss: 0.598298, acc.: 65.62%] [G loss: 1.276404]\n",
      "epoch:0 step:305 [D loss: 0.650775, acc.: 64.06%] [G loss: 1.280465]\n",
      "epoch:0 step:306 [D loss: 0.669965, acc.: 58.59%] [G loss: 1.266326]\n",
      "epoch:0 step:307 [D loss: 0.663340, acc.: 62.50%] [G loss: 1.248735]\n",
      "epoch:0 step:308 [D loss: 0.688925, acc.: 62.50%] [G loss: 1.281987]\n",
      "epoch:0 step:309 [D loss: 0.635601, acc.: 63.28%] [G loss: 1.338852]\n",
      "epoch:0 step:310 [D loss: 0.616109, acc.: 61.72%] [G loss: 1.385163]\n",
      "epoch:0 step:311 [D loss: 0.631777, acc.: 62.50%] [G loss: 1.274234]\n",
      "epoch:0 step:312 [D loss: 0.675378, acc.: 61.72%] [G loss: 1.291191]\n",
      "epoch:0 step:313 [D loss: 0.588797, acc.: 72.66%] [G loss: 1.229928]\n",
      "epoch:0 step:314 [D loss: 0.686137, acc.: 62.50%] [G loss: 1.318989]\n",
      "epoch:0 step:315 [D loss: 0.719630, acc.: 53.91%] [G loss: 1.273198]\n",
      "epoch:0 step:316 [D loss: 0.693286, acc.: 59.38%] [G loss: 1.467594]\n",
      "epoch:0 step:317 [D loss: 0.637667, acc.: 60.94%] [G loss: 1.246762]\n",
      "epoch:0 step:318 [D loss: 0.679097, acc.: 57.03%] [G loss: 1.256301]\n",
      "epoch:0 step:319 [D loss: 0.662764, acc.: 62.50%] [G loss: 1.301551]\n",
      "epoch:0 step:320 [D loss: 0.699851, acc.: 53.91%] [G loss: 1.252760]\n",
      "epoch:0 step:321 [D loss: 0.681904, acc.: 58.59%] [G loss: 1.281913]\n",
      "epoch:0 step:322 [D loss: 0.683205, acc.: 55.47%] [G loss: 1.360717]\n",
      "epoch:0 step:323 [D loss: 0.727770, acc.: 50.00%] [G loss: 1.290230]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:324 [D loss: 0.745360, acc.: 49.22%] [G loss: 1.214566]\n",
      "epoch:0 step:325 [D loss: 0.680995, acc.: 59.38%] [G loss: 1.262541]\n",
      "epoch:0 step:326 [D loss: 0.787976, acc.: 51.56%] [G loss: 1.330896]\n",
      "epoch:0 step:327 [D loss: 0.663303, acc.: 57.03%] [G loss: 1.258778]\n",
      "epoch:0 step:328 [D loss: 0.686115, acc.: 55.47%] [G loss: 1.222726]\n",
      "epoch:0 step:329 [D loss: 0.722295, acc.: 56.25%] [G loss: 1.171080]\n",
      "epoch:0 step:330 [D loss: 0.643669, acc.: 64.84%] [G loss: 1.222829]\n",
      "epoch:0 step:331 [D loss: 0.678787, acc.: 59.38%] [G loss: 1.330952]\n",
      "epoch:0 step:332 [D loss: 0.674300, acc.: 64.06%] [G loss: 1.335330]\n",
      "epoch:0 step:333 [D loss: 0.713900, acc.: 60.16%] [G loss: 1.303342]\n",
      "epoch:0 step:334 [D loss: 0.671468, acc.: 65.62%] [G loss: 1.313739]\n",
      "epoch:0 step:335 [D loss: 0.726481, acc.: 44.53%] [G loss: 1.358295]\n",
      "epoch:0 step:336 [D loss: 0.670161, acc.: 55.47%] [G loss: 1.289667]\n",
      "epoch:0 step:337 [D loss: 0.732468, acc.: 53.91%] [G loss: 1.273577]\n",
      "epoch:0 step:338 [D loss: 0.619357, acc.: 64.06%] [G loss: 1.345867]\n",
      "epoch:0 step:339 [D loss: 0.737964, acc.: 56.25%] [G loss: 1.314014]\n",
      "epoch:0 step:340 [D loss: 0.682455, acc.: 60.16%] [G loss: 1.345483]\n",
      "epoch:0 step:341 [D loss: 0.758251, acc.: 58.59%] [G loss: 1.284577]\n",
      "epoch:0 step:342 [D loss: 0.668055, acc.: 60.94%] [G loss: 1.272025]\n",
      "epoch:0 step:343 [D loss: 0.694512, acc.: 63.28%] [G loss: 1.229288]\n",
      "epoch:0 step:344 [D loss: 0.637544, acc.: 66.41%] [G loss: 1.229816]\n",
      "epoch:0 step:345 [D loss: 0.680328, acc.: 55.47%] [G loss: 1.241072]\n",
      "epoch:0 step:346 [D loss: 0.758455, acc.: 48.44%] [G loss: 1.330619]\n",
      "epoch:0 step:347 [D loss: 0.647191, acc.: 62.50%] [G loss: 1.313245]\n",
      "epoch:0 step:348 [D loss: 0.692782, acc.: 59.38%] [G loss: 1.221040]\n",
      "epoch:0 step:349 [D loss: 0.631855, acc.: 63.28%] [G loss: 1.359050]\n",
      "epoch:0 step:350 [D loss: 0.710035, acc.: 60.16%] [G loss: 1.399831]\n",
      "epoch:0 step:351 [D loss: 0.768442, acc.: 64.06%] [G loss: 1.273415]\n",
      "epoch:0 step:352 [D loss: 0.676346, acc.: 63.28%] [G loss: 1.321200]\n",
      "epoch:0 step:353 [D loss: 0.578250, acc.: 71.09%] [G loss: 1.411398]\n",
      "epoch:0 step:354 [D loss: 0.743536, acc.: 59.38%] [G loss: 1.291299]\n",
      "epoch:0 step:355 [D loss: 0.639897, acc.: 60.16%] [G loss: 1.288103]\n",
      "epoch:0 step:356 [D loss: 0.682300, acc.: 62.50%] [G loss: 1.309406]\n",
      "epoch:0 step:357 [D loss: 0.704169, acc.: 56.25%] [G loss: 1.281353]\n",
      "epoch:0 step:358 [D loss: 0.696844, acc.: 61.72%] [G loss: 1.282436]\n",
      "epoch:0 step:359 [D loss: 0.698379, acc.: 53.91%] [G loss: 1.290588]\n",
      "epoch:0 step:360 [D loss: 0.679672, acc.: 55.47%] [G loss: 1.321470]\n",
      "epoch:0 step:361 [D loss: 0.739634, acc.: 47.66%] [G loss: 1.169825]\n",
      "epoch:0 step:362 [D loss: 0.752175, acc.: 50.00%] [G loss: 1.194959]\n",
      "epoch:0 step:363 [D loss: 0.699814, acc.: 57.81%] [G loss: 1.309542]\n",
      "epoch:0 step:364 [D loss: 0.651228, acc.: 61.72%] [G loss: 1.270720]\n",
      "epoch:0 step:365 [D loss: 0.670482, acc.: 57.81%] [G loss: 1.317900]\n",
      "epoch:0 step:366 [D loss: 0.648006, acc.: 74.22%] [G loss: 1.269343]\n",
      "epoch:0 step:367 [D loss: 0.652501, acc.: 65.62%] [G loss: 1.189143]\n",
      "epoch:0 step:368 [D loss: 0.658156, acc.: 56.25%] [G loss: 1.256878]\n",
      "epoch:0 step:369 [D loss: 0.638546, acc.: 64.84%] [G loss: 1.315204]\n",
      "epoch:0 step:370 [D loss: 0.619661, acc.: 67.19%] [G loss: 1.412465]\n",
      "epoch:0 step:371 [D loss: 0.731174, acc.: 53.91%] [G loss: 1.316645]\n",
      "epoch:0 step:372 [D loss: 0.703865, acc.: 61.72%] [G loss: 1.324427]\n",
      "epoch:0 step:373 [D loss: 0.721753, acc.: 55.47%] [G loss: 1.292133]\n",
      "epoch:0 step:374 [D loss: 0.773679, acc.: 48.44%] [G loss: 1.099056]\n",
      "epoch:0 step:375 [D loss: 0.714092, acc.: 53.12%] [G loss: 1.212923]\n",
      "epoch:0 step:376 [D loss: 0.666440, acc.: 66.41%] [G loss: 1.283901]\n",
      "epoch:0 step:377 [D loss: 0.687606, acc.: 59.38%] [G loss: 1.253003]\n",
      "epoch:0 step:378 [D loss: 0.717270, acc.: 56.25%] [G loss: 1.195467]\n",
      "epoch:0 step:379 [D loss: 0.628026, acc.: 61.72%] [G loss: 1.177448]\n",
      "epoch:0 step:380 [D loss: 0.598096, acc.: 64.06%] [G loss: 1.122415]\n",
      "epoch:0 step:381 [D loss: 0.729996, acc.: 55.47%] [G loss: 1.160733]\n",
      "epoch:0 step:382 [D loss: 0.683941, acc.: 60.94%] [G loss: 1.278381]\n",
      "epoch:0 step:383 [D loss: 0.677324, acc.: 59.38%] [G loss: 1.245491]\n",
      "epoch:0 step:384 [D loss: 0.682938, acc.: 57.03%] [G loss: 1.230363]\n",
      "epoch:0 step:385 [D loss: 0.688548, acc.: 57.03%] [G loss: 1.308423]\n",
      "epoch:0 step:386 [D loss: 0.679043, acc.: 57.81%] [G loss: 1.221319]\n",
      "epoch:0 step:387 [D loss: 0.629673, acc.: 61.72%] [G loss: 1.198287]\n",
      "epoch:0 step:388 [D loss: 0.673957, acc.: 57.81%] [G loss: 1.297978]\n",
      "epoch:0 step:389 [D loss: 0.684825, acc.: 53.12%] [G loss: 1.198218]\n",
      "epoch:0 step:390 [D loss: 0.671756, acc.: 57.03%] [G loss: 1.277971]\n",
      "epoch:0 step:391 [D loss: 0.705792, acc.: 52.34%] [G loss: 1.168371]\n",
      "epoch:0 step:392 [D loss: 0.655979, acc.: 69.53%] [G loss: 1.274128]\n",
      "epoch:0 step:393 [D loss: 0.704226, acc.: 53.91%] [G loss: 1.228936]\n",
      "epoch:0 step:394 [D loss: 0.756495, acc.: 46.88%] [G loss: 1.219943]\n",
      "epoch:0 step:395 [D loss: 0.716756, acc.: 48.44%] [G loss: 1.290280]\n",
      "epoch:0 step:396 [D loss: 0.704933, acc.: 54.69%] [G loss: 1.235828]\n",
      "epoch:0 step:397 [D loss: 0.748935, acc.: 50.78%] [G loss: 1.189674]\n",
      "epoch:0 step:398 [D loss: 0.655031, acc.: 64.06%] [G loss: 1.329082]\n",
      "epoch:0 step:399 [D loss: 0.662482, acc.: 55.47%] [G loss: 1.324710]\n",
      "epoch:0 step:400 [D loss: 0.703618, acc.: 59.38%] [G loss: 1.226439]\n",
      "epoch:0 step:401 [D loss: 0.672965, acc.: 53.12%] [G loss: 1.190975]\n",
      "epoch:0 step:402 [D loss: 0.705984, acc.: 52.34%] [G loss: 1.201280]\n",
      "epoch:0 step:403 [D loss: 0.706168, acc.: 51.56%] [G loss: 1.262803]\n",
      "epoch:0 step:404 [D loss: 0.689787, acc.: 55.47%] [G loss: 1.181125]\n",
      "epoch:0 step:405 [D loss: 0.682130, acc.: 53.91%] [G loss: 1.291004]\n",
      "epoch:0 step:406 [D loss: 0.696608, acc.: 48.44%] [G loss: 1.352592]\n",
      "epoch:0 step:407 [D loss: 0.714238, acc.: 51.56%] [G loss: 1.199014]\n",
      "epoch:0 step:408 [D loss: 0.742904, acc.: 44.53%] [G loss: 1.165919]\n",
      "epoch:0 step:409 [D loss: 0.709242, acc.: 53.12%] [G loss: 1.219569]\n",
      "epoch:0 step:410 [D loss: 0.688973, acc.: 57.81%] [G loss: 1.180108]\n",
      "epoch:0 step:411 [D loss: 0.676126, acc.: 57.81%] [G loss: 1.167590]\n",
      "epoch:0 step:412 [D loss: 0.706714, acc.: 56.25%] [G loss: 1.161449]\n",
      "epoch:0 step:413 [D loss: 0.651421, acc.: 61.72%] [G loss: 1.269838]\n",
      "epoch:0 step:414 [D loss: 0.655799, acc.: 56.25%] [G loss: 1.310555]\n",
      "epoch:0 step:415 [D loss: 0.656041, acc.: 60.94%] [G loss: 1.273412]\n",
      "epoch:0 step:416 [D loss: 0.666176, acc.: 64.06%] [G loss: 1.276335]\n",
      "epoch:0 step:417 [D loss: 0.635819, acc.: 62.50%] [G loss: 1.315215]\n",
      "epoch:0 step:418 [D loss: 0.678226, acc.: 59.38%] [G loss: 1.341112]\n",
      "epoch:0 step:419 [D loss: 0.611728, acc.: 64.84%] [G loss: 1.308225]\n",
      "epoch:0 step:420 [D loss: 0.664984, acc.: 63.28%] [G loss: 1.271348]\n",
      "epoch:0 step:421 [D loss: 0.718453, acc.: 57.03%] [G loss: 1.375336]\n",
      "epoch:0 step:422 [D loss: 0.666147, acc.: 66.41%] [G loss: 1.337000]\n",
      "epoch:0 step:423 [D loss: 0.615633, acc.: 64.84%] [G loss: 1.304450]\n",
      "epoch:0 step:424 [D loss: 0.669846, acc.: 63.28%] [G loss: 1.281230]\n",
      "epoch:0 step:425 [D loss: 0.664016, acc.: 56.25%] [G loss: 1.306637]\n",
      "epoch:0 step:426 [D loss: 0.676119, acc.: 59.38%] [G loss: 1.284908]\n",
      "epoch:0 step:427 [D loss: 0.684681, acc.: 60.94%] [G loss: 1.292030]\n",
      "epoch:0 step:428 [D loss: 0.716030, acc.: 48.44%] [G loss: 1.205238]\n",
      "epoch:0 step:429 [D loss: 0.680134, acc.: 61.72%] [G loss: 1.081626]\n",
      "epoch:0 step:430 [D loss: 0.611829, acc.: 64.84%] [G loss: 1.312246]\n",
      "epoch:0 step:431 [D loss: 0.644323, acc.: 60.16%] [G loss: 1.370236]\n",
      "epoch:0 step:432 [D loss: 0.628220, acc.: 72.66%] [G loss: 1.300074]\n",
      "epoch:0 step:433 [D loss: 0.652300, acc.: 63.28%] [G loss: 1.329388]\n",
      "epoch:0 step:434 [D loss: 0.748446, acc.: 46.09%] [G loss: 1.172719]\n",
      "epoch:0 step:435 [D loss: 0.652671, acc.: 59.38%] [G loss: 1.198758]\n",
      "epoch:0 step:436 [D loss: 0.663164, acc.: 61.72%] [G loss: 1.265361]\n",
      "epoch:0 step:437 [D loss: 0.677003, acc.: 60.16%] [G loss: 1.258127]\n",
      "epoch:0 step:438 [D loss: 0.697428, acc.: 53.12%] [G loss: 1.268155]\n",
      "epoch:0 step:439 [D loss: 0.697970, acc.: 58.59%] [G loss: 1.234479]\n",
      "epoch:0 step:440 [D loss: 0.675127, acc.: 53.12%] [G loss: 1.327038]\n",
      "epoch:0 step:441 [D loss: 0.666895, acc.: 59.38%] [G loss: 1.178968]\n",
      "epoch:0 step:442 [D loss: 0.704323, acc.: 55.47%] [G loss: 1.242390]\n",
      "epoch:0 step:443 [D loss: 0.645394, acc.: 62.50%] [G loss: 1.245372]\n",
      "epoch:0 step:444 [D loss: 0.655584, acc.: 58.59%] [G loss: 1.259074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:445 [D loss: 0.611987, acc.: 63.28%] [G loss: 1.234618]\n",
      "epoch:0 step:446 [D loss: 0.690705, acc.: 50.78%] [G loss: 1.260860]\n",
      "epoch:0 step:447 [D loss: 0.696805, acc.: 57.03%] [G loss: 1.219861]\n",
      "epoch:0 step:448 [D loss: 0.681950, acc.: 53.91%] [G loss: 1.166988]\n",
      "epoch:0 step:449 [D loss: 0.728808, acc.: 52.34%] [G loss: 1.371395]\n",
      "epoch:0 step:450 [D loss: 0.712570, acc.: 61.72%] [G loss: 1.259403]\n",
      "epoch:0 step:451 [D loss: 0.638494, acc.: 60.16%] [G loss: 1.213354]\n",
      "epoch:0 step:452 [D loss: 0.779786, acc.: 45.31%] [G loss: 1.110907]\n",
      "epoch:0 step:453 [D loss: 0.690528, acc.: 56.25%] [G loss: 1.147372]\n",
      "epoch:0 step:454 [D loss: 0.683804, acc.: 57.81%] [G loss: 1.162176]\n",
      "epoch:0 step:455 [D loss: 0.683175, acc.: 57.81%] [G loss: 1.254979]\n",
      "epoch:0 step:456 [D loss: 0.664808, acc.: 60.16%] [G loss: 1.189623]\n",
      "epoch:0 step:457 [D loss: 0.672713, acc.: 60.16%] [G loss: 1.172876]\n",
      "epoch:0 step:458 [D loss: 0.666343, acc.: 55.47%] [G loss: 1.268419]\n",
      "epoch:0 step:459 [D loss: 0.676765, acc.: 57.03%] [G loss: 1.234725]\n",
      "epoch:0 step:460 [D loss: 0.692117, acc.: 57.81%] [G loss: 1.161655]\n",
      "epoch:0 step:461 [D loss: 0.763537, acc.: 53.12%] [G loss: 1.213339]\n",
      "epoch:0 step:462 [D loss: 0.677192, acc.: 50.78%] [G loss: 1.144563]\n",
      "epoch:0 step:463 [D loss: 0.674827, acc.: 60.94%] [G loss: 1.227064]\n",
      "epoch:0 step:464 [D loss: 0.676930, acc.: 59.38%] [G loss: 1.191082]\n",
      "epoch:0 step:465 [D loss: 0.694299, acc.: 60.16%] [G loss: 1.118438]\n",
      "epoch:0 step:466 [D loss: 0.732304, acc.: 53.91%] [G loss: 1.250876]\n",
      "epoch:0 step:467 [D loss: 0.653912, acc.: 60.94%] [G loss: 1.203452]\n",
      "epoch:0 step:468 [D loss: 0.696734, acc.: 60.94%] [G loss: 1.231461]\n",
      "epoch:0 step:469 [D loss: 0.741738, acc.: 52.34%] [G loss: 1.140371]\n",
      "epoch:0 step:470 [D loss: 0.756662, acc.: 43.75%] [G loss: 1.201377]\n",
      "epoch:0 step:471 [D loss: 0.710682, acc.: 50.00%] [G loss: 1.232786]\n",
      "epoch:0 step:472 [D loss: 0.754513, acc.: 46.09%] [G loss: 1.244425]\n",
      "epoch:0 step:473 [D loss: 0.688742, acc.: 53.91%] [G loss: 1.153890]\n",
      "epoch:0 step:474 [D loss: 0.683978, acc.: 60.16%] [G loss: 1.179009]\n",
      "epoch:0 step:475 [D loss: 0.680356, acc.: 56.25%] [G loss: 1.261800]\n",
      "epoch:0 step:476 [D loss: 0.714162, acc.: 48.44%] [G loss: 1.162767]\n",
      "epoch:0 step:477 [D loss: 0.702003, acc.: 51.56%] [G loss: 1.157112]\n",
      "epoch:0 step:478 [D loss: 0.710392, acc.: 53.91%] [G loss: 1.215575]\n",
      "epoch:0 step:479 [D loss: 0.699496, acc.: 61.72%] [G loss: 1.132809]\n",
      "epoch:0 step:480 [D loss: 0.714170, acc.: 50.78%] [G loss: 1.185000]\n",
      "epoch:0 step:481 [D loss: 0.709496, acc.: 53.12%] [G loss: 1.279706]\n",
      "epoch:0 step:482 [D loss: 0.646238, acc.: 62.50%] [G loss: 1.283122]\n",
      "epoch:0 step:483 [D loss: 0.708095, acc.: 63.28%] [G loss: 1.299917]\n",
      "epoch:0 step:484 [D loss: 0.707416, acc.: 56.25%] [G loss: 1.262099]\n",
      "epoch:0 step:485 [D loss: 0.700923, acc.: 59.38%] [G loss: 1.301244]\n",
      "epoch:0 step:486 [D loss: 0.689210, acc.: 53.12%] [G loss: 1.177467]\n",
      "epoch:0 step:487 [D loss: 0.665778, acc.: 58.59%] [G loss: 1.132981]\n",
      "epoch:0 step:488 [D loss: 0.654242, acc.: 57.81%] [G loss: 1.232279]\n",
      "epoch:0 step:489 [D loss: 0.724139, acc.: 56.25%] [G loss: 1.243226]\n",
      "epoch:0 step:490 [D loss: 0.730363, acc.: 53.91%] [G loss: 1.193437]\n",
      "epoch:0 step:491 [D loss: 0.645001, acc.: 60.94%] [G loss: 1.178370]\n",
      "epoch:0 step:492 [D loss: 0.665614, acc.: 62.50%] [G loss: 1.191213]\n",
      "epoch:0 step:493 [D loss: 0.637816, acc.: 61.72%] [G loss: 1.215528]\n",
      "epoch:0 step:494 [D loss: 0.682844, acc.: 58.59%] [G loss: 1.206784]\n",
      "epoch:0 step:495 [D loss: 0.693433, acc.: 57.03%] [G loss: 1.297726]\n",
      "epoch:0 step:496 [D loss: 0.692049, acc.: 60.16%] [G loss: 1.180607]\n",
      "epoch:0 step:497 [D loss: 0.713724, acc.: 53.12%] [G loss: 1.270126]\n",
      "epoch:0 step:498 [D loss: 0.685221, acc.: 57.81%] [G loss: 1.169142]\n",
      "epoch:0 step:499 [D loss: 0.717232, acc.: 53.12%] [G loss: 1.208870]\n",
      "epoch:0 step:500 [D loss: 0.731195, acc.: 61.72%] [G loss: 1.305978]\n",
      "epoch:0 step:501 [D loss: 0.681275, acc.: 61.72%] [G loss: 1.160026]\n",
      "epoch:0 step:502 [D loss: 0.674049, acc.: 59.38%] [G loss: 1.155650]\n",
      "epoch:0 step:503 [D loss: 0.657462, acc.: 63.28%] [G loss: 1.195048]\n",
      "epoch:0 step:504 [D loss: 0.712417, acc.: 53.12%] [G loss: 1.155328]\n",
      "epoch:0 step:505 [D loss: 0.655215, acc.: 60.16%] [G loss: 1.166816]\n",
      "epoch:0 step:506 [D loss: 0.672817, acc.: 64.84%] [G loss: 1.157750]\n",
      "epoch:0 step:507 [D loss: 0.690527, acc.: 60.16%] [G loss: 1.234333]\n",
      "epoch:0 step:508 [D loss: 0.699164, acc.: 59.38%] [G loss: 1.310307]\n",
      "epoch:0 step:509 [D loss: 0.697378, acc.: 49.22%] [G loss: 1.198822]\n",
      "epoch:0 step:510 [D loss: 0.681372, acc.: 58.59%] [G loss: 1.175187]\n",
      "epoch:0 step:511 [D loss: 0.665008, acc.: 57.81%] [G loss: 1.253018]\n",
      "epoch:0 step:512 [D loss: 0.632598, acc.: 67.97%] [G loss: 1.301478]\n",
      "epoch:0 step:513 [D loss: 0.666292, acc.: 58.59%] [G loss: 1.309211]\n",
      "epoch:0 step:514 [D loss: 0.686836, acc.: 60.94%] [G loss: 1.210221]\n",
      "epoch:0 step:515 [D loss: 0.721014, acc.: 50.00%] [G loss: 1.206718]\n",
      "epoch:0 step:516 [D loss: 0.690618, acc.: 55.47%] [G loss: 1.137802]\n",
      "epoch:0 step:517 [D loss: 0.675251, acc.: 57.03%] [G loss: 1.125167]\n",
      "epoch:0 step:518 [D loss: 0.706428, acc.: 56.25%] [G loss: 1.167792]\n",
      "epoch:0 step:519 [D loss: 0.695463, acc.: 50.78%] [G loss: 1.206090]\n",
      "epoch:0 step:520 [D loss: 0.732424, acc.: 53.12%] [G loss: 1.141535]\n",
      "epoch:0 step:521 [D loss: 0.643051, acc.: 62.50%] [G loss: 1.124805]\n",
      "epoch:0 step:522 [D loss: 0.664460, acc.: 55.47%] [G loss: 1.158191]\n",
      "epoch:0 step:523 [D loss: 0.662118, acc.: 59.38%] [G loss: 1.186896]\n",
      "epoch:0 step:524 [D loss: 0.637393, acc.: 66.41%] [G loss: 1.256305]\n",
      "epoch:0 step:525 [D loss: 0.682373, acc.: 64.06%] [G loss: 1.228199]\n",
      "epoch:0 step:526 [D loss: 0.662207, acc.: 56.25%] [G loss: 1.183712]\n",
      "epoch:0 step:527 [D loss: 0.699175, acc.: 59.38%] [G loss: 1.221770]\n",
      "epoch:0 step:528 [D loss: 0.706897, acc.: 55.47%] [G loss: 1.233957]\n",
      "epoch:0 step:529 [D loss: 0.688211, acc.: 55.47%] [G loss: 1.095904]\n",
      "epoch:0 step:530 [D loss: 0.670176, acc.: 63.28%] [G loss: 1.228439]\n",
      "epoch:0 step:531 [D loss: 0.671355, acc.: 58.59%] [G loss: 1.236183]\n",
      "epoch:0 step:532 [D loss: 0.696785, acc.: 53.91%] [G loss: 1.194249]\n",
      "epoch:0 step:533 [D loss: 0.665442, acc.: 59.38%] [G loss: 1.193200]\n",
      "epoch:0 step:534 [D loss: 0.688295, acc.: 59.38%] [G loss: 1.208398]\n",
      "epoch:0 step:535 [D loss: 0.666224, acc.: 62.50%] [G loss: 1.239265]\n",
      "epoch:0 step:536 [D loss: 0.625665, acc.: 65.62%] [G loss: 1.192441]\n",
      "epoch:0 step:537 [D loss: 0.788704, acc.: 47.66%] [G loss: 1.121180]\n",
      "epoch:0 step:538 [D loss: 0.691397, acc.: 63.28%] [G loss: 1.119451]\n",
      "epoch:0 step:539 [D loss: 0.670615, acc.: 53.91%] [G loss: 1.107117]\n",
      "epoch:0 step:540 [D loss: 0.638899, acc.: 66.41%] [G loss: 1.093146]\n",
      "epoch:0 step:541 [D loss: 0.690695, acc.: 55.47%] [G loss: 1.218209]\n",
      "epoch:0 step:542 [D loss: 0.647726, acc.: 62.50%] [G loss: 1.214875]\n",
      "epoch:0 step:543 [D loss: 0.681635, acc.: 54.69%] [G loss: 1.266410]\n",
      "epoch:0 step:544 [D loss: 0.636274, acc.: 65.62%] [G loss: 1.227766]\n",
      "epoch:0 step:545 [D loss: 0.717887, acc.: 51.56%] [G loss: 1.097930]\n",
      "epoch:0 step:546 [D loss: 0.684873, acc.: 60.94%] [G loss: 1.125440]\n",
      "epoch:0 step:547 [D loss: 0.694206, acc.: 54.69%] [G loss: 1.160659]\n",
      "epoch:0 step:548 [D loss: 0.717611, acc.: 59.38%] [G loss: 1.170217]\n",
      "epoch:0 step:549 [D loss: 0.641199, acc.: 60.16%] [G loss: 1.154146]\n",
      "epoch:0 step:550 [D loss: 0.625971, acc.: 68.75%] [G loss: 1.200003]\n",
      "epoch:0 step:551 [D loss: 0.653545, acc.: 64.84%] [G loss: 1.193640]\n",
      "epoch:0 step:552 [D loss: 0.728626, acc.: 50.78%] [G loss: 1.352890]\n",
      "epoch:0 step:553 [D loss: 0.685943, acc.: 60.94%] [G loss: 1.239591]\n",
      "epoch:0 step:554 [D loss: 0.649813, acc.: 60.94%] [G loss: 1.198337]\n",
      "epoch:0 step:555 [D loss: 0.723565, acc.: 56.25%] [G loss: 1.185203]\n",
      "epoch:0 step:556 [D loss: 0.666077, acc.: 53.91%] [G loss: 1.140605]\n",
      "epoch:0 step:557 [D loss: 0.677145, acc.: 54.69%] [G loss: 1.148942]\n",
      "epoch:0 step:558 [D loss: 0.689348, acc.: 53.12%] [G loss: 1.167708]\n",
      "epoch:0 step:559 [D loss: 0.680230, acc.: 56.25%] [G loss: 1.281485]\n",
      "epoch:0 step:560 [D loss: 0.753303, acc.: 46.88%] [G loss: 1.150884]\n",
      "epoch:0 step:561 [D loss: 0.665404, acc.: 56.25%] [G loss: 1.208554]\n",
      "epoch:0 step:562 [D loss: 0.677784, acc.: 59.38%] [G loss: 1.175394]\n",
      "epoch:0 step:563 [D loss: 0.703623, acc.: 50.00%] [G loss: 1.187639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:564 [D loss: 0.765194, acc.: 44.53%] [G loss: 1.079819]\n",
      "epoch:0 step:565 [D loss: 0.654146, acc.: 64.84%] [G loss: 1.223936]\n",
      "epoch:0 step:566 [D loss: 0.652269, acc.: 62.50%] [G loss: 1.201101]\n",
      "epoch:0 step:567 [D loss: 0.687878, acc.: 50.00%] [G loss: 1.162611]\n",
      "epoch:0 step:568 [D loss: 0.679552, acc.: 62.50%] [G loss: 1.107263]\n",
      "epoch:0 step:569 [D loss: 0.693534, acc.: 51.56%] [G loss: 1.211213]\n",
      "epoch:0 step:570 [D loss: 0.703652, acc.: 50.00%] [G loss: 1.239773]\n",
      "epoch:0 step:571 [D loss: 0.650448, acc.: 64.84%] [G loss: 1.169730]\n",
      "epoch:0 step:572 [D loss: 0.656809, acc.: 63.28%] [G loss: 1.138069]\n",
      "epoch:0 step:573 [D loss: 0.680239, acc.: 58.59%] [G loss: 1.257602]\n",
      "epoch:0 step:574 [D loss: 0.678587, acc.: 57.03%] [G loss: 1.234193]\n",
      "epoch:0 step:575 [D loss: 0.648268, acc.: 61.72%] [G loss: 1.262126]\n",
      "epoch:0 step:576 [D loss: 0.635858, acc.: 59.38%] [G loss: 1.289265]\n",
      "epoch:0 step:577 [D loss: 0.645033, acc.: 62.50%] [G loss: 1.297849]\n",
      "epoch:0 step:578 [D loss: 0.712294, acc.: 53.91%] [G loss: 1.141600]\n",
      "epoch:0 step:579 [D loss: 0.612598, acc.: 67.97%] [G loss: 1.216182]\n",
      "epoch:0 step:580 [D loss: 0.657565, acc.: 64.06%] [G loss: 1.223290]\n",
      "epoch:0 step:581 [D loss: 0.650947, acc.: 57.03%] [G loss: 1.196507]\n",
      "epoch:0 step:582 [D loss: 0.713549, acc.: 57.81%] [G loss: 1.229300]\n",
      "epoch:0 step:583 [D loss: 0.691284, acc.: 54.69%] [G loss: 1.215025]\n",
      "epoch:0 step:584 [D loss: 0.639270, acc.: 62.50%] [G loss: 1.304666]\n",
      "epoch:0 step:585 [D loss: 0.730835, acc.: 56.25%] [G loss: 1.228619]\n",
      "epoch:0 step:586 [D loss: 0.676532, acc.: 55.47%] [G loss: 1.182761]\n",
      "epoch:0 step:587 [D loss: 0.667019, acc.: 57.81%] [G loss: 1.248320]\n",
      "epoch:0 step:588 [D loss: 0.641310, acc.: 56.25%] [G loss: 1.155292]\n",
      "epoch:0 step:589 [D loss: 0.687067, acc.: 55.47%] [G loss: 1.201162]\n",
      "epoch:0 step:590 [D loss: 0.664925, acc.: 55.47%] [G loss: 1.155073]\n",
      "epoch:0 step:591 [D loss: 0.681640, acc.: 60.94%] [G loss: 1.186225]\n",
      "epoch:0 step:592 [D loss: 0.675191, acc.: 47.66%] [G loss: 1.228723]\n",
      "epoch:0 step:593 [D loss: 0.674145, acc.: 55.47%] [G loss: 1.100745]\n",
      "epoch:0 step:594 [D loss: 0.666635, acc.: 62.50%] [G loss: 1.068342]\n",
      "epoch:0 step:595 [D loss: 0.617821, acc.: 68.75%] [G loss: 1.119645]\n",
      "epoch:0 step:596 [D loss: 0.656573, acc.: 60.16%] [G loss: 1.188419]\n",
      "epoch:0 step:597 [D loss: 0.710627, acc.: 48.44%] [G loss: 1.123869]\n",
      "epoch:0 step:598 [D loss: 0.655671, acc.: 62.50%] [G loss: 1.149414]\n",
      "epoch:0 step:599 [D loss: 0.617175, acc.: 65.62%] [G loss: 1.212145]\n",
      "epoch:0 step:600 [D loss: 0.688251, acc.: 55.47%] [G loss: 1.161135]\n",
      "epoch:0 step:601 [D loss: 0.666411, acc.: 62.50%] [G loss: 1.162233]\n",
      "epoch:0 step:602 [D loss: 0.699690, acc.: 53.12%] [G loss: 1.220713]\n",
      "epoch:0 step:603 [D loss: 0.647931, acc.: 64.84%] [G loss: 1.204160]\n",
      "epoch:0 step:604 [D loss: 0.682784, acc.: 50.78%] [G loss: 1.157248]\n",
      "epoch:0 step:605 [D loss: 0.707959, acc.: 53.12%] [G loss: 1.197125]\n",
      "epoch:0 step:606 [D loss: 0.662957, acc.: 59.38%] [G loss: 1.183440]\n",
      "epoch:0 step:607 [D loss: 0.687267, acc.: 52.34%] [G loss: 1.191911]\n",
      "epoch:0 step:608 [D loss: 0.665481, acc.: 53.91%] [G loss: 1.163209]\n",
      "epoch:0 step:609 [D loss: 0.656498, acc.: 60.16%] [G loss: 1.171098]\n",
      "epoch:0 step:610 [D loss: 0.672473, acc.: 60.16%] [G loss: 1.217674]\n",
      "epoch:0 step:611 [D loss: 0.752284, acc.: 50.78%] [G loss: 1.158154]\n",
      "epoch:0 step:612 [D loss: 0.705160, acc.: 50.00%] [G loss: 1.157431]\n",
      "epoch:0 step:613 [D loss: 0.693066, acc.: 52.34%] [G loss: 1.127170]\n",
      "epoch:0 step:614 [D loss: 0.678586, acc.: 58.59%] [G loss: 1.159409]\n",
      "epoch:0 step:615 [D loss: 0.668540, acc.: 56.25%] [G loss: 1.089144]\n",
      "epoch:0 step:616 [D loss: 0.644713, acc.: 61.72%] [G loss: 1.160473]\n",
      "epoch:0 step:617 [D loss: 0.697616, acc.: 53.91%] [G loss: 1.186811]\n",
      "epoch:0 step:618 [D loss: 0.668639, acc.: 57.81%] [G loss: 1.137191]\n",
      "epoch:0 step:619 [D loss: 0.655555, acc.: 62.50%] [G loss: 1.157993]\n",
      "epoch:0 step:620 [D loss: 0.642016, acc.: 67.97%] [G loss: 1.134007]\n",
      "epoch:0 step:621 [D loss: 0.699420, acc.: 56.25%] [G loss: 1.088134]\n",
      "epoch:0 step:622 [D loss: 0.643110, acc.: 64.06%] [G loss: 1.076939]\n",
      "epoch:0 step:623 [D loss: 0.622327, acc.: 64.06%] [G loss: 1.131305]\n",
      "epoch:0 step:624 [D loss: 0.698147, acc.: 52.34%] [G loss: 1.133886]\n",
      "epoch:0 step:625 [D loss: 0.652737, acc.: 60.16%] [G loss: 1.070811]\n",
      "epoch:0 step:626 [D loss: 0.616565, acc.: 67.19%] [G loss: 1.151515]\n",
      "epoch:0 step:627 [D loss: 0.651937, acc.: 63.28%] [G loss: 1.117720]\n",
      "epoch:0 step:628 [D loss: 0.621922, acc.: 63.28%] [G loss: 1.109329]\n",
      "epoch:0 step:629 [D loss: 0.650323, acc.: 64.06%] [G loss: 1.128345]\n",
      "epoch:0 step:630 [D loss: 0.635428, acc.: 62.50%] [G loss: 1.239523]\n",
      "epoch:0 step:631 [D loss: 0.689055, acc.: 54.69%] [G loss: 1.124250]\n",
      "epoch:0 step:632 [D loss: 0.683976, acc.: 63.28%] [G loss: 1.238473]\n",
      "epoch:0 step:633 [D loss: 0.680442, acc.: 56.25%] [G loss: 1.146723]\n",
      "epoch:0 step:634 [D loss: 0.675394, acc.: 58.59%] [G loss: 1.177675]\n",
      "epoch:0 step:635 [D loss: 0.707561, acc.: 58.59%] [G loss: 1.212482]\n",
      "epoch:0 step:636 [D loss: 0.625979, acc.: 64.06%] [G loss: 1.301037]\n",
      "epoch:0 step:637 [D loss: 0.797523, acc.: 43.75%] [G loss: 1.286469]\n",
      "epoch:0 step:638 [D loss: 0.769320, acc.: 46.09%] [G loss: 1.140599]\n",
      "epoch:0 step:639 [D loss: 0.700796, acc.: 51.56%] [G loss: 1.128566]\n",
      "epoch:0 step:640 [D loss: 0.678876, acc.: 53.12%] [G loss: 1.228446]\n",
      "epoch:0 step:641 [D loss: 0.720286, acc.: 54.69%] [G loss: 1.067631]\n",
      "epoch:0 step:642 [D loss: 0.698382, acc.: 51.56%] [G loss: 1.071807]\n",
      "epoch:0 step:643 [D loss: 0.662556, acc.: 60.94%] [G loss: 1.130380]\n",
      "epoch:0 step:644 [D loss: 0.688059, acc.: 57.03%] [G loss: 1.155449]\n",
      "epoch:0 step:645 [D loss: 0.668499, acc.: 56.25%] [G loss: 1.211202]\n",
      "epoch:0 step:646 [D loss: 0.698645, acc.: 49.22%] [G loss: 1.161014]\n",
      "epoch:0 step:647 [D loss: 0.713164, acc.: 50.78%] [G loss: 1.168038]\n",
      "epoch:0 step:648 [D loss: 0.658844, acc.: 58.59%] [G loss: 1.133974]\n",
      "epoch:0 step:649 [D loss: 0.657102, acc.: 58.59%] [G loss: 1.113361]\n",
      "epoch:0 step:650 [D loss: 0.709478, acc.: 53.12%] [G loss: 1.138996]\n",
      "epoch:0 step:651 [D loss: 0.663498, acc.: 61.72%] [G loss: 1.128028]\n",
      "epoch:0 step:652 [D loss: 0.674898, acc.: 64.06%] [G loss: 1.235215]\n",
      "epoch:0 step:653 [D loss: 0.680168, acc.: 57.81%] [G loss: 1.192030]\n",
      "epoch:0 step:654 [D loss: 0.697312, acc.: 57.81%] [G loss: 1.175568]\n",
      "epoch:0 step:655 [D loss: 0.688651, acc.: 58.59%] [G loss: 1.190953]\n",
      "epoch:0 step:656 [D loss: 0.662463, acc.: 61.72%] [G loss: 1.211822]\n",
      "epoch:0 step:657 [D loss: 0.668142, acc.: 59.38%] [G loss: 1.127374]\n",
      "epoch:0 step:658 [D loss: 0.723084, acc.: 51.56%] [G loss: 1.210639]\n",
      "epoch:0 step:659 [D loss: 0.645278, acc.: 68.75%] [G loss: 1.226889]\n",
      "epoch:0 step:660 [D loss: 0.682209, acc.: 51.56%] [G loss: 1.232695]\n",
      "epoch:0 step:661 [D loss: 0.698422, acc.: 57.81%] [G loss: 1.140309]\n",
      "epoch:0 step:662 [D loss: 0.676766, acc.: 55.47%] [G loss: 1.228671]\n",
      "epoch:0 step:663 [D loss: 0.670765, acc.: 63.28%] [G loss: 1.337616]\n",
      "epoch:0 step:664 [D loss: 0.678231, acc.: 58.59%] [G loss: 1.137609]\n",
      "epoch:0 step:665 [D loss: 0.684067, acc.: 58.59%] [G loss: 1.095525]\n",
      "epoch:0 step:666 [D loss: 0.637268, acc.: 66.41%] [G loss: 1.238997]\n",
      "epoch:0 step:667 [D loss: 0.629749, acc.: 63.28%] [G loss: 1.226591]\n",
      "epoch:0 step:668 [D loss: 0.755497, acc.: 44.53%] [G loss: 1.083923]\n",
      "epoch:0 step:669 [D loss: 0.678414, acc.: 57.81%] [G loss: 1.256947]\n",
      "epoch:0 step:670 [D loss: 0.712562, acc.: 50.00%] [G loss: 1.151185]\n",
      "epoch:0 step:671 [D loss: 0.675644, acc.: 54.69%] [G loss: 1.125268]\n",
      "epoch:0 step:672 [D loss: 0.701824, acc.: 53.12%] [G loss: 1.136393]\n",
      "epoch:0 step:673 [D loss: 0.672050, acc.: 60.16%] [G loss: 1.157190]\n",
      "epoch:0 step:674 [D loss: 0.667763, acc.: 64.84%] [G loss: 1.161574]\n",
      "epoch:0 step:675 [D loss: 0.667990, acc.: 62.50%] [G loss: 1.112660]\n",
      "epoch:0 step:676 [D loss: 0.671849, acc.: 53.12%] [G loss: 1.095695]\n",
      "epoch:0 step:677 [D loss: 0.671985, acc.: 62.50%] [G loss: 1.112108]\n",
      "epoch:0 step:678 [D loss: 0.706272, acc.: 55.47%] [G loss: 1.291483]\n",
      "epoch:0 step:679 [D loss: 0.705763, acc.: 51.56%] [G loss: 1.189359]\n",
      "epoch:0 step:680 [D loss: 0.689716, acc.: 53.91%] [G loss: 1.092159]\n",
      "epoch:0 step:681 [D loss: 0.674697, acc.: 57.81%] [G loss: 1.215988]\n",
      "epoch:0 step:682 [D loss: 0.719751, acc.: 52.34%] [G loss: 1.216137]\n",
      "epoch:0 step:683 [D loss: 0.666549, acc.: 63.28%] [G loss: 1.088001]\n",
      "epoch:0 step:684 [D loss: 0.689711, acc.: 61.72%] [G loss: 1.034608]\n",
      "epoch:0 step:685 [D loss: 0.644910, acc.: 64.06%] [G loss: 1.079768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:686 [D loss: 0.678384, acc.: 58.59%] [G loss: 1.130398]\n",
      "epoch:0 step:687 [D loss: 0.649523, acc.: 63.28%] [G loss: 1.234549]\n",
      "epoch:0 step:688 [D loss: 0.656695, acc.: 65.62%] [G loss: 1.203698]\n",
      "epoch:0 step:689 [D loss: 0.670065, acc.: 60.16%] [G loss: 1.216407]\n",
      "epoch:0 step:690 [D loss: 0.644238, acc.: 60.16%] [G loss: 1.154857]\n",
      "epoch:0 step:691 [D loss: 0.682720, acc.: 59.38%] [G loss: 1.211989]\n",
      "epoch:0 step:692 [D loss: 0.738795, acc.: 52.34%] [G loss: 1.073668]\n",
      "epoch:0 step:693 [D loss: 0.660264, acc.: 57.03%] [G loss: 1.127380]\n",
      "epoch:0 step:694 [D loss: 0.662751, acc.: 61.72%] [G loss: 1.161263]\n",
      "epoch:0 step:695 [D loss: 0.666239, acc.: 60.94%] [G loss: 1.203574]\n",
      "epoch:0 step:696 [D loss: 0.679674, acc.: 60.16%] [G loss: 1.130511]\n",
      "epoch:0 step:697 [D loss: 0.663854, acc.: 57.03%] [G loss: 1.135766]\n",
      "epoch:0 step:698 [D loss: 0.666091, acc.: 53.91%] [G loss: 1.171497]\n",
      "epoch:0 step:699 [D loss: 0.691010, acc.: 61.72%] [G loss: 1.155559]\n",
      "epoch:0 step:700 [D loss: 0.685503, acc.: 62.50%] [G loss: 1.095237]\n",
      "epoch:0 step:701 [D loss: 0.655525, acc.: 60.94%] [G loss: 1.139363]\n",
      "epoch:0 step:702 [D loss: 0.679993, acc.: 60.94%] [G loss: 1.113152]\n",
      "epoch:0 step:703 [D loss: 0.618669, acc.: 72.66%] [G loss: 1.093833]\n",
      "epoch:0 step:704 [D loss: 0.703651, acc.: 60.16%] [G loss: 1.169083]\n",
      "epoch:0 step:705 [D loss: 0.632136, acc.: 60.16%] [G loss: 1.189885]\n",
      "epoch:0 step:706 [D loss: 0.683604, acc.: 61.72%] [G loss: 1.205881]\n",
      "epoch:0 step:707 [D loss: 0.655657, acc.: 56.25%] [G loss: 1.155904]\n",
      "epoch:0 step:708 [D loss: 0.688806, acc.: 55.47%] [G loss: 1.149184]\n",
      "epoch:0 step:709 [D loss: 0.650462, acc.: 64.84%] [G loss: 1.121288]\n",
      "epoch:0 step:710 [D loss: 0.688583, acc.: 53.91%] [G loss: 1.068209]\n",
      "epoch:0 step:711 [D loss: 0.650748, acc.: 60.16%] [G loss: 1.136801]\n",
      "epoch:0 step:712 [D loss: 0.634131, acc.: 67.19%] [G loss: 1.183721]\n",
      "epoch:0 step:713 [D loss: 0.647946, acc.: 62.50%] [G loss: 1.150511]\n",
      "epoch:0 step:714 [D loss: 0.675465, acc.: 61.72%] [G loss: 1.181659]\n",
      "epoch:0 step:715 [D loss: 0.675630, acc.: 63.28%] [G loss: 1.179048]\n",
      "epoch:0 step:716 [D loss: 0.668602, acc.: 60.94%] [G loss: 1.169137]\n",
      "epoch:0 step:717 [D loss: 0.631620, acc.: 64.84%] [G loss: 1.187758]\n",
      "epoch:0 step:718 [D loss: 0.704387, acc.: 53.91%] [G loss: 1.178977]\n",
      "epoch:0 step:719 [D loss: 0.631693, acc.: 62.50%] [G loss: 1.196085]\n",
      "epoch:0 step:720 [D loss: 0.657474, acc.: 64.06%] [G loss: 1.228449]\n",
      "epoch:0 step:721 [D loss: 0.686242, acc.: 54.69%] [G loss: 1.130569]\n",
      "epoch:0 step:722 [D loss: 0.676100, acc.: 56.25%] [G loss: 1.093201]\n",
      "epoch:0 step:723 [D loss: 0.683894, acc.: 62.50%] [G loss: 1.013524]\n",
      "epoch:0 step:724 [D loss: 0.694694, acc.: 52.34%] [G loss: 1.101036]\n",
      "epoch:0 step:725 [D loss: 0.644442, acc.: 62.50%] [G loss: 1.142271]\n",
      "epoch:0 step:726 [D loss: 0.661177, acc.: 59.38%] [G loss: 1.132009]\n",
      "epoch:0 step:727 [D loss: 0.679979, acc.: 60.94%] [G loss: 1.157696]\n",
      "epoch:0 step:728 [D loss: 0.694076, acc.: 57.03%] [G loss: 1.165252]\n",
      "epoch:0 step:729 [D loss: 0.642847, acc.: 60.94%] [G loss: 1.156745]\n",
      "epoch:0 step:730 [D loss: 0.699251, acc.: 54.69%] [G loss: 1.083631]\n",
      "epoch:0 step:731 [D loss: 0.674419, acc.: 60.94%] [G loss: 1.084537]\n",
      "epoch:0 step:732 [D loss: 0.646278, acc.: 58.59%] [G loss: 1.116608]\n",
      "epoch:0 step:733 [D loss: 0.659476, acc.: 54.69%] [G loss: 1.106864]\n",
      "epoch:0 step:734 [D loss: 0.657941, acc.: 60.16%] [G loss: 1.136302]\n",
      "epoch:0 step:735 [D loss: 0.690440, acc.: 58.59%] [G loss: 1.115981]\n",
      "epoch:0 step:736 [D loss: 0.652934, acc.: 59.38%] [G loss: 1.096025]\n",
      "epoch:0 step:737 [D loss: 0.662693, acc.: 61.72%] [G loss: 1.113936]\n",
      "epoch:0 step:738 [D loss: 0.703999, acc.: 51.56%] [G loss: 1.040758]\n",
      "epoch:0 step:739 [D loss: 0.617435, acc.: 64.84%] [G loss: 1.047147]\n",
      "epoch:0 step:740 [D loss: 0.657480, acc.: 60.94%] [G loss: 1.121637]\n",
      "epoch:0 step:741 [D loss: 0.639745, acc.: 63.28%] [G loss: 1.132263]\n",
      "epoch:0 step:742 [D loss: 0.640531, acc.: 63.28%] [G loss: 1.148550]\n",
      "epoch:0 step:743 [D loss: 0.648258, acc.: 57.81%] [G loss: 1.146515]\n",
      "epoch:0 step:744 [D loss: 0.681346, acc.: 53.91%] [G loss: 1.152841]\n",
      "epoch:0 step:745 [D loss: 0.673151, acc.: 60.94%] [G loss: 1.251204]\n",
      "epoch:0 step:746 [D loss: 0.655877, acc.: 55.47%] [G loss: 1.142652]\n",
      "epoch:0 step:747 [D loss: 0.698218, acc.: 57.03%] [G loss: 1.181705]\n",
      "epoch:0 step:748 [D loss: 0.652981, acc.: 61.72%] [G loss: 1.216261]\n",
      "epoch:0 step:749 [D loss: 0.687085, acc.: 57.81%] [G loss: 1.222601]\n",
      "epoch:0 step:750 [D loss: 0.655482, acc.: 58.59%] [G loss: 1.224895]\n",
      "epoch:0 step:751 [D loss: 0.715266, acc.: 55.47%] [G loss: 1.187023]\n",
      "epoch:0 step:752 [D loss: 0.693002, acc.: 56.25%] [G loss: 1.092538]\n",
      "epoch:0 step:753 [D loss: 0.670975, acc.: 57.81%] [G loss: 1.152848]\n",
      "epoch:0 step:754 [D loss: 0.669650, acc.: 58.59%] [G loss: 1.142623]\n",
      "epoch:0 step:755 [D loss: 0.686576, acc.: 53.91%] [G loss: 1.085744]\n",
      "epoch:0 step:756 [D loss: 0.649102, acc.: 67.19%] [G loss: 1.064891]\n",
      "epoch:0 step:757 [D loss: 0.645095, acc.: 67.19%] [G loss: 1.104592]\n",
      "epoch:0 step:758 [D loss: 0.665014, acc.: 59.38%] [G loss: 1.151025]\n",
      "epoch:0 step:759 [D loss: 0.691593, acc.: 55.47%] [G loss: 1.088668]\n",
      "epoch:0 step:760 [D loss: 0.668859, acc.: 57.81%] [G loss: 1.044229]\n",
      "epoch:0 step:761 [D loss: 0.642678, acc.: 66.41%] [G loss: 1.051592]\n",
      "epoch:0 step:762 [D loss: 0.637798, acc.: 64.06%] [G loss: 1.063129]\n",
      "epoch:0 step:763 [D loss: 0.673268, acc.: 57.81%] [G loss: 1.113600]\n",
      "epoch:0 step:764 [D loss: 0.603670, acc.: 71.09%] [G loss: 1.160395]\n",
      "epoch:0 step:765 [D loss: 0.767242, acc.: 45.31%] [G loss: 1.085357]\n",
      "epoch:0 step:766 [D loss: 0.640592, acc.: 61.72%] [G loss: 1.103022]\n",
      "epoch:0 step:767 [D loss: 0.659548, acc.: 60.94%] [G loss: 1.131783]\n",
      "epoch:0 step:768 [D loss: 0.649913, acc.: 58.59%] [G loss: 1.194178]\n",
      "epoch:0 step:769 [D loss: 0.667032, acc.: 55.47%] [G loss: 1.249793]\n",
      "epoch:0 step:770 [D loss: 0.640601, acc.: 60.94%] [G loss: 1.151333]\n",
      "epoch:0 step:771 [D loss: 0.680475, acc.: 58.59%] [G loss: 1.138016]\n",
      "epoch:0 step:772 [D loss: 0.685589, acc.: 59.38%] [G loss: 1.138232]\n",
      "epoch:0 step:773 [D loss: 0.655708, acc.: 60.94%] [G loss: 1.094566]\n",
      "epoch:0 step:774 [D loss: 0.639983, acc.: 66.41%] [G loss: 1.117509]\n",
      "epoch:0 step:775 [D loss: 0.643238, acc.: 59.38%] [G loss: 1.144099]\n",
      "epoch:0 step:776 [D loss: 0.665271, acc.: 58.59%] [G loss: 1.086308]\n",
      "epoch:0 step:777 [D loss: 0.616112, acc.: 65.62%] [G loss: 1.072647]\n",
      "epoch:0 step:778 [D loss: 0.668771, acc.: 62.50%] [G loss: 1.159493]\n",
      "epoch:0 step:779 [D loss: 0.617567, acc.: 64.84%] [G loss: 1.167482]\n",
      "epoch:0 step:780 [D loss: 0.666948, acc.: 64.84%] [G loss: 1.183945]\n",
      "epoch:0 step:781 [D loss: 0.671656, acc.: 57.81%] [G loss: 1.200680]\n",
      "epoch:1 step:782 [D loss: 0.717652, acc.: 55.47%] [G loss: 1.023666]\n",
      "epoch:1 step:783 [D loss: 0.643274, acc.: 62.50%] [G loss: 1.076130]\n",
      "epoch:1 step:784 [D loss: 0.691491, acc.: 57.03%] [G loss: 1.050363]\n",
      "epoch:1 step:785 [D loss: 0.608287, acc.: 70.31%] [G loss: 1.161742]\n",
      "epoch:1 step:786 [D loss: 0.653122, acc.: 60.16%] [G loss: 1.190214]\n",
      "epoch:1 step:787 [D loss: 0.712904, acc.: 53.12%] [G loss: 1.093323]\n",
      "epoch:1 step:788 [D loss: 0.665076, acc.: 60.16%] [G loss: 1.069250]\n",
      "epoch:1 step:789 [D loss: 0.671154, acc.: 58.59%] [G loss: 1.137525]\n",
      "epoch:1 step:790 [D loss: 0.645688, acc.: 64.84%] [G loss: 1.121757]\n",
      "epoch:1 step:791 [D loss: 0.648735, acc.: 57.81%] [G loss: 1.096410]\n",
      "epoch:1 step:792 [D loss: 0.665627, acc.: 59.38%] [G loss: 1.180547]\n",
      "epoch:1 step:793 [D loss: 0.612520, acc.: 64.84%] [G loss: 1.190913]\n",
      "epoch:1 step:794 [D loss: 0.742393, acc.: 50.00%] [G loss: 1.185910]\n",
      "epoch:1 step:795 [D loss: 0.687812, acc.: 62.50%] [G loss: 1.296284]\n",
      "epoch:1 step:796 [D loss: 0.736531, acc.: 50.78%] [G loss: 0.993017]\n",
      "epoch:1 step:797 [D loss: 0.658634, acc.: 60.94%] [G loss: 1.112971]\n",
      "epoch:1 step:798 [D loss: 0.670324, acc.: 54.69%] [G loss: 1.140216]\n",
      "epoch:1 step:799 [D loss: 0.627508, acc.: 67.19%] [G loss: 1.099342]\n",
      "epoch:1 step:800 [D loss: 0.636850, acc.: 64.84%] [G loss: 1.127979]\n",
      "epoch:1 step:801 [D loss: 0.679530, acc.: 61.72%] [G loss: 1.125449]\n",
      "epoch:1 step:802 [D loss: 0.622135, acc.: 67.97%] [G loss: 1.106724]\n",
      "epoch:1 step:803 [D loss: 0.657512, acc.: 59.38%] [G loss: 1.205407]\n",
      "epoch:1 step:804 [D loss: 0.705273, acc.: 48.44%] [G loss: 1.161812]\n",
      "epoch:1 step:805 [D loss: 0.654531, acc.: 69.53%] [G loss: 1.207204]\n",
      "epoch:1 step:806 [D loss: 0.684759, acc.: 59.38%] [G loss: 1.144535]\n",
      "epoch:1 step:807 [D loss: 0.675224, acc.: 61.72%] [G loss: 1.079592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:808 [D loss: 0.666742, acc.: 61.72%] [G loss: 1.142018]\n",
      "epoch:1 step:809 [D loss: 0.690467, acc.: 55.47%] [G loss: 1.144573]\n",
      "epoch:1 step:810 [D loss: 0.642941, acc.: 62.50%] [G loss: 1.183366]\n",
      "epoch:1 step:811 [D loss: 0.680546, acc.: 57.81%] [G loss: 1.208145]\n",
      "epoch:1 step:812 [D loss: 0.677291, acc.: 53.91%] [G loss: 1.191743]\n",
      "epoch:1 step:813 [D loss: 0.693758, acc.: 58.59%] [G loss: 1.119356]\n",
      "epoch:1 step:814 [D loss: 0.666263, acc.: 59.38%] [G loss: 1.125028]\n",
      "epoch:1 step:815 [D loss: 0.639634, acc.: 64.06%] [G loss: 1.069944]\n",
      "epoch:1 step:816 [D loss: 0.643822, acc.: 61.72%] [G loss: 1.151233]\n",
      "epoch:1 step:817 [D loss: 0.689874, acc.: 52.34%] [G loss: 1.182603]\n",
      "epoch:1 step:818 [D loss: 0.706604, acc.: 57.03%] [G loss: 1.193712]\n",
      "epoch:1 step:819 [D loss: 0.660577, acc.: 64.84%] [G loss: 1.294517]\n",
      "epoch:1 step:820 [D loss: 0.697338, acc.: 57.03%] [G loss: 1.111680]\n",
      "epoch:1 step:821 [D loss: 0.675786, acc.: 57.03%] [G loss: 1.149400]\n",
      "epoch:1 step:822 [D loss: 0.670953, acc.: 56.25%] [G loss: 1.089985]\n",
      "epoch:1 step:823 [D loss: 0.628521, acc.: 63.28%] [G loss: 1.115867]\n",
      "epoch:1 step:824 [D loss: 0.622465, acc.: 67.19%] [G loss: 1.202444]\n",
      "epoch:1 step:825 [D loss: 0.702725, acc.: 54.69%] [G loss: 1.080135]\n",
      "epoch:1 step:826 [D loss: 0.623906, acc.: 70.31%] [G loss: 1.090947]\n",
      "epoch:1 step:827 [D loss: 0.654289, acc.: 60.16%] [G loss: 1.108112]\n",
      "epoch:1 step:828 [D loss: 0.656923, acc.: 62.50%] [G loss: 1.142268]\n",
      "epoch:1 step:829 [D loss: 0.627509, acc.: 67.97%] [G loss: 1.201590]\n",
      "epoch:1 step:830 [D loss: 0.633981, acc.: 60.94%] [G loss: 1.236721]\n",
      "epoch:1 step:831 [D loss: 0.764736, acc.: 52.34%] [G loss: 1.135698]\n",
      "epoch:1 step:832 [D loss: 0.698967, acc.: 62.50%] [G loss: 1.147840]\n",
      "epoch:1 step:833 [D loss: 0.705026, acc.: 51.56%] [G loss: 1.098458]\n",
      "epoch:1 step:834 [D loss: 0.664724, acc.: 65.62%] [G loss: 1.154463]\n",
      "epoch:1 step:835 [D loss: 0.622923, acc.: 66.41%] [G loss: 1.108751]\n",
      "epoch:1 step:836 [D loss: 0.669907, acc.: 62.50%] [G loss: 1.127609]\n",
      "epoch:1 step:837 [D loss: 0.686610, acc.: 55.47%] [G loss: 1.093672]\n",
      "epoch:1 step:838 [D loss: 0.660180, acc.: 55.47%] [G loss: 1.036261]\n",
      "epoch:1 step:839 [D loss: 0.728324, acc.: 51.56%] [G loss: 1.036677]\n",
      "epoch:1 step:840 [D loss: 0.669127, acc.: 55.47%] [G loss: 1.071266]\n",
      "epoch:1 step:841 [D loss: 0.649535, acc.: 60.16%] [G loss: 1.092264]\n",
      "epoch:1 step:842 [D loss: 0.689812, acc.: 57.03%] [G loss: 1.129200]\n",
      "epoch:1 step:843 [D loss: 0.647088, acc.: 65.62%] [G loss: 1.101974]\n",
      "epoch:1 step:844 [D loss: 0.644055, acc.: 64.06%] [G loss: 1.137990]\n",
      "epoch:1 step:845 [D loss: 0.733364, acc.: 55.47%] [G loss: 1.092912]\n",
      "epoch:1 step:846 [D loss: 0.643447, acc.: 61.72%] [G loss: 1.088234]\n",
      "epoch:1 step:847 [D loss: 0.646844, acc.: 64.06%] [G loss: 1.208074]\n",
      "epoch:1 step:848 [D loss: 0.651962, acc.: 59.38%] [G loss: 1.213067]\n",
      "epoch:1 step:849 [D loss: 0.708887, acc.: 49.22%] [G loss: 1.103495]\n",
      "epoch:1 step:850 [D loss: 0.702526, acc.: 59.38%] [G loss: 1.063447]\n",
      "epoch:1 step:851 [D loss: 0.632366, acc.: 70.31%] [G loss: 1.101993]\n",
      "epoch:1 step:852 [D loss: 0.703150, acc.: 49.22%] [G loss: 1.036321]\n",
      "epoch:1 step:853 [D loss: 0.721068, acc.: 53.12%] [G loss: 1.058143]\n",
      "epoch:1 step:854 [D loss: 0.667805, acc.: 61.72%] [G loss: 1.124303]\n",
      "epoch:1 step:855 [D loss: 0.613546, acc.: 72.66%] [G loss: 1.145799]\n",
      "epoch:1 step:856 [D loss: 0.665341, acc.: 53.91%] [G loss: 1.089782]\n",
      "epoch:1 step:857 [D loss: 0.701724, acc.: 50.78%] [G loss: 1.155573]\n",
      "epoch:1 step:858 [D loss: 0.682042, acc.: 64.06%] [G loss: 1.103410]\n",
      "epoch:1 step:859 [D loss: 0.673964, acc.: 57.03%] [G loss: 1.094532]\n",
      "epoch:1 step:860 [D loss: 0.690449, acc.: 54.69%] [G loss: 1.117684]\n",
      "epoch:1 step:861 [D loss: 0.637933, acc.: 65.62%] [G loss: 1.189115]\n",
      "epoch:1 step:862 [D loss: 0.695390, acc.: 62.50%] [G loss: 1.278412]\n",
      "epoch:1 step:863 [D loss: 0.706176, acc.: 51.56%] [G loss: 1.161303]\n",
      "epoch:1 step:864 [D loss: 0.682363, acc.: 62.50%] [G loss: 1.155969]\n",
      "epoch:1 step:865 [D loss: 0.672046, acc.: 53.12%] [G loss: 1.176516]\n",
      "epoch:1 step:866 [D loss: 0.737849, acc.: 53.12%] [G loss: 1.125646]\n",
      "epoch:1 step:867 [D loss: 0.640550, acc.: 61.72%] [G loss: 1.023230]\n",
      "epoch:1 step:868 [D loss: 0.664585, acc.: 56.25%] [G loss: 1.121636]\n",
      "epoch:1 step:869 [D loss: 0.635020, acc.: 65.62%] [G loss: 1.139997]\n",
      "epoch:1 step:870 [D loss: 0.645138, acc.: 64.06%] [G loss: 1.063596]\n",
      "epoch:1 step:871 [D loss: 0.632998, acc.: 64.84%] [G loss: 1.185769]\n",
      "epoch:1 step:872 [D loss: 0.670034, acc.: 58.59%] [G loss: 1.223519]\n",
      "epoch:1 step:873 [D loss: 0.698953, acc.: 53.91%] [G loss: 1.204857]\n",
      "epoch:1 step:874 [D loss: 0.678698, acc.: 55.47%] [G loss: 1.102133]\n",
      "epoch:1 step:875 [D loss: 0.668707, acc.: 56.25%] [G loss: 1.144439]\n",
      "epoch:1 step:876 [D loss: 0.731257, acc.: 55.47%] [G loss: 1.150772]\n",
      "epoch:1 step:877 [D loss: 0.680079, acc.: 63.28%] [G loss: 1.112945]\n",
      "epoch:1 step:878 [D loss: 0.694647, acc.: 50.78%] [G loss: 1.066108]\n",
      "epoch:1 step:879 [D loss: 0.680099, acc.: 59.38%] [G loss: 1.075634]\n",
      "epoch:1 step:880 [D loss: 0.626636, acc.: 65.62%] [G loss: 1.164108]\n",
      "epoch:1 step:881 [D loss: 0.689684, acc.: 62.50%] [G loss: 1.081827]\n",
      "epoch:1 step:882 [D loss: 0.685931, acc.: 57.03%] [G loss: 1.062382]\n",
      "epoch:1 step:883 [D loss: 0.639813, acc.: 67.97%] [G loss: 1.073048]\n",
      "epoch:1 step:884 [D loss: 0.618874, acc.: 67.97%] [G loss: 1.011824]\n",
      "epoch:1 step:885 [D loss: 0.637652, acc.: 57.81%] [G loss: 1.109491]\n",
      "epoch:1 step:886 [D loss: 0.637341, acc.: 64.84%] [G loss: 1.085986]\n",
      "epoch:1 step:887 [D loss: 0.650128, acc.: 60.94%] [G loss: 1.168505]\n",
      "epoch:1 step:888 [D loss: 0.648167, acc.: 67.97%] [G loss: 1.345095]\n",
      "epoch:1 step:889 [D loss: 0.679907, acc.: 62.50%] [G loss: 1.155426]\n",
      "epoch:1 step:890 [D loss: 0.690802, acc.: 55.47%] [G loss: 1.207336]\n",
      "epoch:1 step:891 [D loss: 0.648955, acc.: 58.59%] [G loss: 1.291388]\n",
      "epoch:1 step:892 [D loss: 0.741909, acc.: 56.25%] [G loss: 1.330956]\n",
      "epoch:1 step:893 [D loss: 0.789257, acc.: 57.81%] [G loss: 1.140618]\n",
      "epoch:1 step:894 [D loss: 0.684746, acc.: 55.47%] [G loss: 1.070699]\n",
      "epoch:1 step:895 [D loss: 0.685665, acc.: 60.94%] [G loss: 0.994287]\n",
      "epoch:1 step:896 [D loss: 0.681180, acc.: 55.47%] [G loss: 0.992416]\n",
      "epoch:1 step:897 [D loss: 0.645887, acc.: 64.84%] [G loss: 1.062284]\n",
      "epoch:1 step:898 [D loss: 0.676085, acc.: 58.59%] [G loss: 1.099149]\n",
      "epoch:1 step:899 [D loss: 0.663783, acc.: 58.59%] [G loss: 1.024276]\n",
      "epoch:1 step:900 [D loss: 0.622124, acc.: 66.41%] [G loss: 1.052698]\n",
      "epoch:1 step:901 [D loss: 0.656515, acc.: 60.16%] [G loss: 1.157761]\n",
      "epoch:1 step:902 [D loss: 0.662128, acc.: 62.50%] [G loss: 1.031793]\n",
      "epoch:1 step:903 [D loss: 0.621439, acc.: 67.19%] [G loss: 1.048869]\n",
      "epoch:1 step:904 [D loss: 0.702199, acc.: 53.91%] [G loss: 1.069887]\n",
      "epoch:1 step:905 [D loss: 0.592332, acc.: 78.91%] [G loss: 1.181657]\n",
      "epoch:1 step:906 [D loss: 0.639687, acc.: 66.41%] [G loss: 1.091273]\n",
      "epoch:1 step:907 [D loss: 0.645422, acc.: 61.72%] [G loss: 1.102037]\n",
      "epoch:1 step:908 [D loss: 0.641648, acc.: 64.06%] [G loss: 1.122386]\n",
      "epoch:1 step:909 [D loss: 0.686885, acc.: 57.03%] [G loss: 1.075593]\n",
      "epoch:1 step:910 [D loss: 0.651257, acc.: 61.72%] [G loss: 1.097536]\n",
      "epoch:1 step:911 [D loss: 0.649977, acc.: 66.41%] [G loss: 1.119120]\n",
      "epoch:1 step:912 [D loss: 0.699339, acc.: 57.03%] [G loss: 1.046181]\n",
      "epoch:1 step:913 [D loss: 0.676301, acc.: 63.28%] [G loss: 1.047044]\n",
      "epoch:1 step:914 [D loss: 0.613078, acc.: 69.53%] [G loss: 1.041852]\n",
      "epoch:1 step:915 [D loss: 0.652986, acc.: 58.59%] [G loss: 1.032604]\n",
      "epoch:1 step:916 [D loss: 0.655670, acc.: 64.06%] [G loss: 1.101241]\n",
      "epoch:1 step:917 [D loss: 0.668895, acc.: 56.25%] [G loss: 1.179802]\n",
      "epoch:1 step:918 [D loss: 0.691597, acc.: 57.03%] [G loss: 1.144888]\n",
      "epoch:1 step:919 [D loss: 0.704136, acc.: 58.59%] [G loss: 1.082767]\n",
      "epoch:1 step:920 [D loss: 0.696983, acc.: 54.69%] [G loss: 1.081783]\n",
      "epoch:1 step:921 [D loss: 0.664365, acc.: 57.03%] [G loss: 1.030300]\n",
      "epoch:1 step:922 [D loss: 0.671108, acc.: 60.94%] [G loss: 1.042937]\n",
      "epoch:1 step:923 [D loss: 0.646679, acc.: 65.62%] [G loss: 1.040431]\n",
      "epoch:1 step:924 [D loss: 0.647958, acc.: 60.94%] [G loss: 1.062059]\n",
      "epoch:1 step:925 [D loss: 0.641120, acc.: 67.97%] [G loss: 1.118113]\n",
      "epoch:1 step:926 [D loss: 0.684293, acc.: 55.47%] [G loss: 1.096070]\n",
      "epoch:1 step:927 [D loss: 0.658917, acc.: 63.28%] [G loss: 1.136177]\n",
      "epoch:1 step:928 [D loss: 0.639478, acc.: 67.19%] [G loss: 1.131565]\n",
      "epoch:1 step:929 [D loss: 0.655546, acc.: 64.84%] [G loss: 1.095509]\n",
      "epoch:1 step:930 [D loss: 0.675090, acc.: 60.16%] [G loss: 1.091824]\n",
      "epoch:1 step:931 [D loss: 0.625041, acc.: 69.53%] [G loss: 1.099920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:932 [D loss: 0.616448, acc.: 75.78%] [G loss: 1.100757]\n",
      "epoch:1 step:933 [D loss: 0.659691, acc.: 61.72%] [G loss: 1.102340]\n",
      "epoch:1 step:934 [D loss: 0.671730, acc.: 60.16%] [G loss: 1.116268]\n",
      "epoch:1 step:935 [D loss: 0.654055, acc.: 63.28%] [G loss: 1.065734]\n",
      "epoch:1 step:936 [D loss: 0.678243, acc.: 59.38%] [G loss: 1.048281]\n",
      "epoch:1 step:937 [D loss: 0.650991, acc.: 64.06%] [G loss: 1.030172]\n",
      "epoch:1 step:938 [D loss: 0.677410, acc.: 55.47%] [G loss: 1.108683]\n",
      "epoch:1 step:939 [D loss: 0.670493, acc.: 59.38%] [G loss: 1.090653]\n",
      "epoch:1 step:940 [D loss: 0.663757, acc.: 61.72%] [G loss: 1.115963]\n",
      "epoch:1 step:941 [D loss: 0.682108, acc.: 57.81%] [G loss: 1.083097]\n",
      "epoch:1 step:942 [D loss: 0.650406, acc.: 62.50%] [G loss: 1.088308]\n",
      "epoch:1 step:943 [D loss: 0.627458, acc.: 67.19%] [G loss: 1.090937]\n",
      "epoch:1 step:944 [D loss: 0.631216, acc.: 69.53%] [G loss: 1.054667]\n",
      "epoch:1 step:945 [D loss: 0.668057, acc.: 56.25%] [G loss: 1.075450]\n",
      "epoch:1 step:946 [D loss: 0.636422, acc.: 58.59%] [G loss: 1.089492]\n",
      "epoch:1 step:947 [D loss: 0.666290, acc.: 59.38%] [G loss: 1.045632]\n",
      "epoch:1 step:948 [D loss: 0.670039, acc.: 60.94%] [G loss: 1.046330]\n",
      "epoch:1 step:949 [D loss: 0.611684, acc.: 69.53%] [G loss: 1.135399]\n",
      "epoch:1 step:950 [D loss: 0.661340, acc.: 57.03%] [G loss: 1.008360]\n",
      "epoch:1 step:951 [D loss: 0.659748, acc.: 59.38%] [G loss: 1.089924]\n",
      "epoch:1 step:952 [D loss: 0.657229, acc.: 62.50%] [G loss: 1.105186]\n",
      "epoch:1 step:953 [D loss: 0.614820, acc.: 67.97%] [G loss: 1.062137]\n",
      "epoch:1 step:954 [D loss: 0.635583, acc.: 68.75%] [G loss: 1.108769]\n",
      "epoch:1 step:955 [D loss: 0.653106, acc.: 67.97%] [G loss: 1.088377]\n",
      "epoch:1 step:956 [D loss: 0.655097, acc.: 64.84%] [G loss: 1.161387]\n",
      "epoch:1 step:957 [D loss: 0.660696, acc.: 67.19%] [G loss: 1.110763]\n",
      "epoch:1 step:958 [D loss: 0.662726, acc.: 60.16%] [G loss: 1.096834]\n",
      "epoch:1 step:959 [D loss: 0.669032, acc.: 60.16%] [G loss: 1.166351]\n",
      "epoch:1 step:960 [D loss: 0.616846, acc.: 67.97%] [G loss: 1.201386]\n",
      "epoch:1 step:961 [D loss: 0.691663, acc.: 68.75%] [G loss: 1.131905]\n",
      "epoch:1 step:962 [D loss: 0.643020, acc.: 64.06%] [G loss: 1.154978]\n",
      "epoch:1 step:963 [D loss: 0.682260, acc.: 55.47%] [G loss: 1.062918]\n",
      "epoch:1 step:964 [D loss: 0.650748, acc.: 62.50%] [G loss: 1.069455]\n",
      "epoch:1 step:965 [D loss: 0.630395, acc.: 65.62%] [G loss: 1.065571]\n",
      "epoch:1 step:966 [D loss: 0.676263, acc.: 64.84%] [G loss: 1.122388]\n",
      "epoch:1 step:967 [D loss: 0.646757, acc.: 69.53%] [G loss: 1.119771]\n",
      "epoch:1 step:968 [D loss: 0.656227, acc.: 53.12%] [G loss: 1.107726]\n",
      "epoch:1 step:969 [D loss: 0.605786, acc.: 74.22%] [G loss: 1.184840]\n",
      "epoch:1 step:970 [D loss: 0.639434, acc.: 64.84%] [G loss: 1.081583]\n",
      "epoch:1 step:971 [D loss: 0.660326, acc.: 57.81%] [G loss: 1.090728]\n",
      "epoch:1 step:972 [D loss: 0.602815, acc.: 71.88%] [G loss: 1.094742]\n",
      "epoch:1 step:973 [D loss: 0.656509, acc.: 64.06%] [G loss: 1.041826]\n",
      "epoch:1 step:974 [D loss: 0.646870, acc.: 68.75%] [G loss: 1.071701]\n",
      "epoch:1 step:975 [D loss: 0.677393, acc.: 54.69%] [G loss: 1.053791]\n",
      "epoch:1 step:976 [D loss: 0.640250, acc.: 66.41%] [G loss: 1.255736]\n",
      "epoch:1 step:977 [D loss: 0.602288, acc.: 66.41%] [G loss: 1.259391]\n",
      "epoch:1 step:978 [D loss: 0.748203, acc.: 49.22%] [G loss: 1.222972]\n",
      "epoch:1 step:979 [D loss: 0.643543, acc.: 61.72%] [G loss: 1.154991]\n",
      "epoch:1 step:980 [D loss: 0.635749, acc.: 64.84%] [G loss: 1.102971]\n",
      "epoch:1 step:981 [D loss: 0.649584, acc.: 59.38%] [G loss: 1.136811]\n",
      "epoch:1 step:982 [D loss: 0.662748, acc.: 59.38%] [G loss: 1.122160]\n",
      "epoch:1 step:983 [D loss: 0.633069, acc.: 67.97%] [G loss: 1.091971]\n",
      "epoch:1 step:984 [D loss: 0.628221, acc.: 64.84%] [G loss: 1.008068]\n",
      "epoch:1 step:985 [D loss: 0.709942, acc.: 52.34%] [G loss: 1.053690]\n",
      "epoch:1 step:986 [D loss: 0.663012, acc.: 63.28%] [G loss: 1.043143]\n",
      "epoch:1 step:987 [D loss: 0.651468, acc.: 65.62%] [G loss: 1.087159]\n",
      "epoch:1 step:988 [D loss: 0.728558, acc.: 50.78%] [G loss: 0.961501]\n",
      "epoch:1 step:989 [D loss: 0.631165, acc.: 68.75%] [G loss: 1.036920]\n",
      "epoch:1 step:990 [D loss: 0.641236, acc.: 58.59%] [G loss: 0.970268]\n",
      "epoch:1 step:991 [D loss: 0.583611, acc.: 73.44%] [G loss: 1.107285]\n",
      "epoch:1 step:992 [D loss: 0.642352, acc.: 67.19%] [G loss: 1.291607]\n",
      "epoch:1 step:993 [D loss: 0.681866, acc.: 56.25%] [G loss: 1.090231]\n",
      "epoch:1 step:994 [D loss: 0.646143, acc.: 65.62%] [G loss: 1.033906]\n",
      "epoch:1 step:995 [D loss: 0.676930, acc.: 53.12%] [G loss: 1.080583]\n",
      "epoch:1 step:996 [D loss: 0.686273, acc.: 58.59%] [G loss: 1.028826]\n",
      "epoch:1 step:997 [D loss: 0.672601, acc.: 58.59%] [G loss: 0.980958]\n",
      "epoch:1 step:998 [D loss: 0.640282, acc.: 67.97%] [G loss: 0.976924]\n",
      "epoch:1 step:999 [D loss: 0.647593, acc.: 59.38%] [G loss: 0.985103]\n",
      "epoch:1 step:1000 [D loss: 0.606927, acc.: 74.22%] [G loss: 1.091419]\n",
      "epoch:1 step:1001 [D loss: 0.653266, acc.: 64.06%] [G loss: 1.014996]\n",
      "epoch:1 step:1002 [D loss: 0.644534, acc.: 65.62%] [G loss: 1.054228]\n",
      "epoch:1 step:1003 [D loss: 0.678611, acc.: 67.97%] [G loss: 1.046291]\n",
      "epoch:1 step:1004 [D loss: 0.663878, acc.: 62.50%] [G loss: 1.050735]\n",
      "epoch:1 step:1005 [D loss: 0.640947, acc.: 60.16%] [G loss: 1.003826]\n",
      "epoch:1 step:1006 [D loss: 0.668677, acc.: 63.28%] [G loss: 1.044459]\n",
      "epoch:1 step:1007 [D loss: 0.661904, acc.: 60.16%] [G loss: 1.008927]\n",
      "epoch:1 step:1008 [D loss: 0.607571, acc.: 67.97%] [G loss: 1.035911]\n",
      "epoch:1 step:1009 [D loss: 0.621764, acc.: 64.06%] [G loss: 1.058614]\n",
      "epoch:1 step:1010 [D loss: 0.632253, acc.: 66.41%] [G loss: 1.096576]\n",
      "epoch:1 step:1011 [D loss: 0.635954, acc.: 68.75%] [G loss: 1.058501]\n",
      "epoch:1 step:1012 [D loss: 0.608507, acc.: 71.09%] [G loss: 1.085972]\n",
      "epoch:1 step:1013 [D loss: 0.676864, acc.: 55.47%] [G loss: 1.069533]\n",
      "epoch:1 step:1014 [D loss: 0.629688, acc.: 66.41%] [G loss: 1.087472]\n",
      "epoch:1 step:1015 [D loss: 0.613809, acc.: 68.75%] [G loss: 1.122565]\n",
      "epoch:1 step:1016 [D loss: 0.675050, acc.: 60.16%] [G loss: 1.075293]\n",
      "epoch:1 step:1017 [D loss: 0.691007, acc.: 57.81%] [G loss: 1.056105]\n",
      "epoch:1 step:1018 [D loss: 0.666264, acc.: 59.38%] [G loss: 1.014827]\n",
      "epoch:1 step:1019 [D loss: 0.627080, acc.: 69.53%] [G loss: 1.093802]\n",
      "epoch:1 step:1020 [D loss: 0.658012, acc.: 69.53%] [G loss: 1.071160]\n",
      "epoch:1 step:1021 [D loss: 0.625150, acc.: 65.62%] [G loss: 1.125909]\n",
      "epoch:1 step:1022 [D loss: 0.661131, acc.: 60.94%] [G loss: 1.189983]\n",
      "epoch:1 step:1023 [D loss: 0.688685, acc.: 55.47%] [G loss: 1.048177]\n",
      "epoch:1 step:1024 [D loss: 0.651098, acc.: 64.06%] [G loss: 1.036719]\n",
      "epoch:1 step:1025 [D loss: 0.649902, acc.: 61.72%] [G loss: 1.044998]\n",
      "epoch:1 step:1026 [D loss: 0.652818, acc.: 63.28%] [G loss: 1.011334]\n",
      "epoch:1 step:1027 [D loss: 0.682115, acc.: 56.25%] [G loss: 1.010263]\n",
      "epoch:1 step:1028 [D loss: 0.612272, acc.: 67.19%] [G loss: 1.014170]\n",
      "epoch:1 step:1029 [D loss: 0.603199, acc.: 67.97%] [G loss: 1.030511]\n",
      "epoch:1 step:1030 [D loss: 0.667576, acc.: 61.72%] [G loss: 1.046896]\n",
      "epoch:1 step:1031 [D loss: 0.637607, acc.: 66.41%] [G loss: 1.063641]\n",
      "epoch:1 step:1032 [D loss: 0.647016, acc.: 62.50%] [G loss: 0.993268]\n",
      "epoch:1 step:1033 [D loss: 0.711522, acc.: 54.69%] [G loss: 1.050859]\n",
      "epoch:1 step:1034 [D loss: 0.639717, acc.: 64.84%] [G loss: 1.088334]\n",
      "epoch:1 step:1035 [D loss: 0.713290, acc.: 51.56%] [G loss: 1.001896]\n",
      "epoch:1 step:1036 [D loss: 0.637251, acc.: 66.41%] [G loss: 1.024535]\n",
      "epoch:1 step:1037 [D loss: 0.652190, acc.: 61.72%] [G loss: 1.092569]\n",
      "epoch:1 step:1038 [D loss: 0.665613, acc.: 60.94%] [G loss: 1.055295]\n",
      "epoch:1 step:1039 [D loss: 0.642148, acc.: 65.62%] [G loss: 1.080172]\n",
      "epoch:1 step:1040 [D loss: 0.680957, acc.: 57.81%] [G loss: 1.030260]\n",
      "epoch:1 step:1041 [D loss: 0.649067, acc.: 64.06%] [G loss: 1.103040]\n",
      "epoch:1 step:1042 [D loss: 0.657173, acc.: 59.38%] [G loss: 1.060468]\n",
      "epoch:1 step:1043 [D loss: 0.684488, acc.: 55.47%] [G loss: 1.009444]\n",
      "epoch:1 step:1044 [D loss: 0.698485, acc.: 56.25%] [G loss: 1.138633]\n",
      "epoch:1 step:1045 [D loss: 0.697692, acc.: 56.25%] [G loss: 1.024294]\n",
      "epoch:1 step:1046 [D loss: 0.679715, acc.: 57.03%] [G loss: 1.030254]\n",
      "epoch:1 step:1047 [D loss: 0.696555, acc.: 54.69%] [G loss: 1.069317]\n",
      "epoch:1 step:1048 [D loss: 0.618733, acc.: 68.75%] [G loss: 1.132546]\n",
      "epoch:1 step:1049 [D loss: 0.677890, acc.: 52.34%] [G loss: 1.070276]\n",
      "epoch:1 step:1050 [D loss: 0.685122, acc.: 64.06%] [G loss: 0.964040]\n",
      "epoch:1 step:1051 [D loss: 0.664496, acc.: 61.72%] [G loss: 1.000946]\n",
      "epoch:1 step:1052 [D loss: 0.675339, acc.: 57.81%] [G loss: 1.068027]\n",
      "epoch:1 step:1053 [D loss: 0.629057, acc.: 68.75%] [G loss: 1.112557]\n",
      "epoch:1 step:1054 [D loss: 0.682942, acc.: 53.91%] [G loss: 1.104943]\n",
      "epoch:1 step:1055 [D loss: 0.644168, acc.: 64.84%] [G loss: 1.268516]\n",
      "epoch:1 step:1056 [D loss: 0.778077, acc.: 51.56%] [G loss: 1.227130]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1057 [D loss: 0.654107, acc.: 67.97%] [G loss: 1.122615]\n",
      "epoch:1 step:1058 [D loss: 0.753570, acc.: 41.41%] [G loss: 1.018406]\n",
      "epoch:1 step:1059 [D loss: 0.689097, acc.: 57.03%] [G loss: 1.054135]\n",
      "epoch:1 step:1060 [D loss: 0.630365, acc.: 67.97%] [G loss: 1.020938]\n",
      "epoch:1 step:1061 [D loss: 0.661731, acc.: 62.50%] [G loss: 1.015963]\n",
      "epoch:1 step:1062 [D loss: 0.659909, acc.: 58.59%] [G loss: 0.961596]\n",
      "epoch:1 step:1063 [D loss: 0.653027, acc.: 62.50%] [G loss: 0.948203]\n",
      "epoch:1 step:1064 [D loss: 0.632725, acc.: 66.41%] [G loss: 0.981913]\n",
      "epoch:1 step:1065 [D loss: 0.619951, acc.: 70.31%] [G loss: 1.021649]\n",
      "epoch:1 step:1066 [D loss: 0.659760, acc.: 64.06%] [G loss: 1.038994]\n",
      "epoch:1 step:1067 [D loss: 0.652927, acc.: 57.81%] [G loss: 0.989068]\n",
      "epoch:1 step:1068 [D loss: 0.629974, acc.: 65.62%] [G loss: 1.011372]\n",
      "epoch:1 step:1069 [D loss: 0.650078, acc.: 62.50%] [G loss: 1.047282]\n",
      "epoch:1 step:1070 [D loss: 0.607348, acc.: 69.53%] [G loss: 0.979162]\n",
      "epoch:1 step:1071 [D loss: 0.634646, acc.: 62.50%] [G loss: 1.104129]\n",
      "epoch:1 step:1072 [D loss: 0.634400, acc.: 66.41%] [G loss: 1.089132]\n",
      "epoch:1 step:1073 [D loss: 0.652585, acc.: 60.16%] [G loss: 1.016589]\n",
      "epoch:1 step:1074 [D loss: 0.645555, acc.: 62.50%] [G loss: 1.106760]\n",
      "epoch:1 step:1075 [D loss: 0.638216, acc.: 57.03%] [G loss: 0.970117]\n",
      "epoch:1 step:1076 [D loss: 0.618184, acc.: 66.41%] [G loss: 1.035583]\n",
      "epoch:1 step:1077 [D loss: 0.647884, acc.: 67.97%] [G loss: 1.176210]\n",
      "epoch:1 step:1078 [D loss: 0.687957, acc.: 58.59%] [G loss: 0.987437]\n",
      "epoch:1 step:1079 [D loss: 0.660832, acc.: 59.38%] [G loss: 0.981453]\n",
      "epoch:1 step:1080 [D loss: 0.613853, acc.: 69.53%] [G loss: 1.093168]\n",
      "epoch:1 step:1081 [D loss: 0.640966, acc.: 63.28%] [G loss: 1.011626]\n",
      "epoch:1 step:1082 [D loss: 0.644061, acc.: 62.50%] [G loss: 1.042806]\n",
      "epoch:1 step:1083 [D loss: 0.653193, acc.: 60.94%] [G loss: 1.059009]\n",
      "epoch:1 step:1084 [D loss: 0.647564, acc.: 66.41%] [G loss: 1.049591]\n",
      "epoch:1 step:1085 [D loss: 0.627995, acc.: 66.41%] [G loss: 1.054365]\n",
      "epoch:1 step:1086 [D loss: 0.682350, acc.: 53.91%] [G loss: 1.041540]\n",
      "epoch:1 step:1087 [D loss: 0.645653, acc.: 65.62%] [G loss: 0.996490]\n",
      "epoch:1 step:1088 [D loss: 0.621655, acc.: 66.41%] [G loss: 1.086586]\n",
      "epoch:1 step:1089 [D loss: 0.630397, acc.: 70.31%] [G loss: 0.986017]\n",
      "epoch:1 step:1090 [D loss: 0.642056, acc.: 62.50%] [G loss: 1.002041]\n",
      "epoch:1 step:1091 [D loss: 0.652916, acc.: 58.59%] [G loss: 1.040572]\n",
      "epoch:1 step:1092 [D loss: 0.642274, acc.: 60.16%] [G loss: 1.079583]\n",
      "epoch:1 step:1093 [D loss: 0.630851, acc.: 69.53%] [G loss: 1.099984]\n",
      "epoch:1 step:1094 [D loss: 0.659229, acc.: 58.59%] [G loss: 1.072228]\n",
      "epoch:1 step:1095 [D loss: 0.615325, acc.: 71.88%] [G loss: 1.129164]\n",
      "epoch:1 step:1096 [D loss: 0.613652, acc.: 72.66%] [G loss: 1.085614]\n",
      "epoch:1 step:1097 [D loss: 0.664390, acc.: 64.06%] [G loss: 1.140419]\n",
      "epoch:1 step:1098 [D loss: 0.706426, acc.: 51.56%] [G loss: 0.999089]\n",
      "epoch:1 step:1099 [D loss: 0.641467, acc.: 64.06%] [G loss: 1.051267]\n",
      "epoch:1 step:1100 [D loss: 0.617803, acc.: 65.62%] [G loss: 1.166988]\n",
      "epoch:1 step:1101 [D loss: 0.779411, acc.: 50.00%] [G loss: 1.098201]\n",
      "epoch:1 step:1102 [D loss: 0.655809, acc.: 63.28%] [G loss: 1.029593]\n",
      "epoch:1 step:1103 [D loss: 0.708821, acc.: 59.38%] [G loss: 1.179002]\n",
      "epoch:1 step:1104 [D loss: 0.731556, acc.: 57.03%] [G loss: 1.049464]\n",
      "epoch:1 step:1105 [D loss: 0.659532, acc.: 61.72%] [G loss: 1.002825]\n",
      "epoch:1 step:1106 [D loss: 0.641261, acc.: 65.62%] [G loss: 0.982262]\n",
      "epoch:1 step:1107 [D loss: 0.677517, acc.: 61.72%] [G loss: 0.974171]\n",
      "epoch:1 step:1108 [D loss: 0.641868, acc.: 64.84%] [G loss: 0.973541]\n",
      "epoch:1 step:1109 [D loss: 0.648317, acc.: 66.41%] [G loss: 0.938981]\n",
      "epoch:1 step:1110 [D loss: 0.636906, acc.: 64.06%] [G loss: 0.990105]\n",
      "epoch:1 step:1111 [D loss: 0.643984, acc.: 63.28%] [G loss: 0.930661]\n",
      "epoch:1 step:1112 [D loss: 0.674563, acc.: 55.47%] [G loss: 1.029085]\n",
      "epoch:1 step:1113 [D loss: 0.649689, acc.: 59.38%] [G loss: 1.099990]\n",
      "epoch:1 step:1114 [D loss: 0.644240, acc.: 67.19%] [G loss: 1.118948]\n",
      "epoch:1 step:1115 [D loss: 0.679846, acc.: 59.38%] [G loss: 0.986119]\n",
      "epoch:1 step:1116 [D loss: 0.705934, acc.: 53.12%] [G loss: 0.992618]\n",
      "epoch:1 step:1117 [D loss: 0.629456, acc.: 71.88%] [G loss: 1.070092]\n",
      "epoch:1 step:1118 [D loss: 0.682944, acc.: 56.25%] [G loss: 1.040446]\n",
      "epoch:1 step:1119 [D loss: 0.677285, acc.: 57.81%] [G loss: 0.984986]\n",
      "epoch:1 step:1120 [D loss: 0.616856, acc.: 68.75%] [G loss: 1.139224]\n",
      "epoch:1 step:1121 [D loss: 0.769382, acc.: 50.78%] [G loss: 1.051343]\n",
      "epoch:1 step:1122 [D loss: 0.694973, acc.: 60.94%] [G loss: 0.985947]\n",
      "epoch:1 step:1123 [D loss: 0.674499, acc.: 58.59%] [G loss: 0.975080]\n",
      "epoch:1 step:1124 [D loss: 0.610019, acc.: 69.53%] [G loss: 1.065922]\n",
      "epoch:1 step:1125 [D loss: 0.701366, acc.: 50.78%] [G loss: 0.935881]\n",
      "epoch:1 step:1126 [D loss: 0.685329, acc.: 54.69%] [G loss: 0.917794]\n",
      "epoch:1 step:1127 [D loss: 0.660607, acc.: 62.50%] [G loss: 0.954386]\n",
      "epoch:1 step:1128 [D loss: 0.678104, acc.: 58.59%] [G loss: 0.916985]\n",
      "epoch:1 step:1129 [D loss: 0.629913, acc.: 64.84%] [G loss: 0.986827]\n",
      "epoch:1 step:1130 [D loss: 0.621191, acc.: 68.75%] [G loss: 0.947687]\n",
      "epoch:1 step:1131 [D loss: 0.675560, acc.: 58.59%] [G loss: 1.024628]\n",
      "epoch:1 step:1132 [D loss: 0.744695, acc.: 54.69%] [G loss: 1.266876]\n",
      "epoch:1 step:1133 [D loss: 0.776848, acc.: 47.66%] [G loss: 0.930538]\n",
      "epoch:1 step:1134 [D loss: 0.625290, acc.: 64.84%] [G loss: 0.992315]\n",
      "epoch:1 step:1135 [D loss: 0.681874, acc.: 60.94%] [G loss: 0.973561]\n",
      "epoch:1 step:1136 [D loss: 0.646879, acc.: 60.94%] [G loss: 1.070678]\n",
      "epoch:1 step:1137 [D loss: 0.690952, acc.: 59.38%] [G loss: 1.062277]\n",
      "epoch:1 step:1138 [D loss: 0.647742, acc.: 57.03%] [G loss: 1.038012]\n",
      "epoch:1 step:1139 [D loss: 0.661803, acc.: 56.25%] [G loss: 1.083013]\n",
      "epoch:1 step:1140 [D loss: 0.653695, acc.: 64.84%] [G loss: 1.126143]\n",
      "epoch:1 step:1141 [D loss: 0.683343, acc.: 64.06%] [G loss: 1.062317]\n",
      "epoch:1 step:1142 [D loss: 0.718502, acc.: 46.09%] [G loss: 0.910495]\n",
      "epoch:1 step:1143 [D loss: 0.706958, acc.: 52.34%] [G loss: 0.933558]\n",
      "epoch:1 step:1144 [D loss: 0.677096, acc.: 61.72%] [G loss: 0.915727]\n",
      "epoch:1 step:1145 [D loss: 0.682486, acc.: 57.81%] [G loss: 0.940545]\n",
      "epoch:1 step:1146 [D loss: 0.633241, acc.: 64.84%] [G loss: 0.948650]\n",
      "epoch:1 step:1147 [D loss: 0.685583, acc.: 57.03%] [G loss: 0.973270]\n",
      "epoch:1 step:1148 [D loss: 0.655688, acc.: 60.94%] [G loss: 1.054497]\n",
      "epoch:1 step:1149 [D loss: 0.666390, acc.: 61.72%] [G loss: 0.971391]\n",
      "epoch:1 step:1150 [D loss: 0.677209, acc.: 60.16%] [G loss: 0.990977]\n",
      "epoch:1 step:1151 [D loss: 0.617023, acc.: 65.62%] [G loss: 1.173585]\n",
      "epoch:1 step:1152 [D loss: 0.769814, acc.: 52.34%] [G loss: 1.093372]\n",
      "epoch:1 step:1153 [D loss: 0.652995, acc.: 67.97%] [G loss: 1.101128]\n",
      "epoch:1 step:1154 [D loss: 0.640822, acc.: 67.19%] [G loss: 1.130819]\n",
      "epoch:1 step:1155 [D loss: 0.769583, acc.: 48.44%] [G loss: 0.987588]\n",
      "epoch:1 step:1156 [D loss: 0.677116, acc.: 58.59%] [G loss: 0.931569]\n",
      "epoch:1 step:1157 [D loss: 0.635748, acc.: 67.97%] [G loss: 0.908531]\n",
      "epoch:1 step:1158 [D loss: 0.640043, acc.: 63.28%] [G loss: 0.957164]\n",
      "epoch:1 step:1159 [D loss: 0.666965, acc.: 60.94%] [G loss: 0.931470]\n",
      "epoch:1 step:1160 [D loss: 0.630305, acc.: 64.06%] [G loss: 0.919535]\n",
      "epoch:1 step:1161 [D loss: 0.656368, acc.: 60.94%] [G loss: 0.943795]\n",
      "epoch:1 step:1162 [D loss: 0.657890, acc.: 60.94%] [G loss: 0.927869]\n",
      "epoch:1 step:1163 [D loss: 0.629548, acc.: 69.53%] [G loss: 1.012533]\n",
      "epoch:1 step:1164 [D loss: 0.674934, acc.: 57.03%] [G loss: 0.935847]\n",
      "epoch:1 step:1165 [D loss: 0.654853, acc.: 62.50%] [G loss: 0.994568]\n",
      "epoch:1 step:1166 [D loss: 0.654481, acc.: 64.06%] [G loss: 1.047467]\n",
      "epoch:1 step:1167 [D loss: 0.708837, acc.: 50.00%] [G loss: 0.928972]\n",
      "epoch:1 step:1168 [D loss: 0.641824, acc.: 66.41%] [G loss: 0.927976]\n",
      "epoch:1 step:1169 [D loss: 0.666035, acc.: 64.06%] [G loss: 0.930313]\n",
      "epoch:1 step:1170 [D loss: 0.651073, acc.: 61.72%] [G loss: 0.948793]\n",
      "epoch:1 step:1171 [D loss: 0.673669, acc.: 53.12%] [G loss: 0.930185]\n",
      "epoch:1 step:1172 [D loss: 0.646465, acc.: 62.50%] [G loss: 0.998500]\n",
      "epoch:1 step:1173 [D loss: 0.629114, acc.: 70.31%] [G loss: 1.038275]\n",
      "epoch:1 step:1174 [D loss: 0.656668, acc.: 61.72%] [G loss: 1.003303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1175 [D loss: 0.670821, acc.: 56.25%] [G loss: 0.988387]\n",
      "epoch:1 step:1176 [D loss: 0.650317, acc.: 62.50%] [G loss: 1.011114]\n",
      "epoch:1 step:1177 [D loss: 0.652387, acc.: 64.84%] [G loss: 0.971865]\n",
      "epoch:1 step:1178 [D loss: 0.680332, acc.: 56.25%] [G loss: 0.912314]\n",
      "epoch:1 step:1179 [D loss: 0.650903, acc.: 60.16%] [G loss: 1.052049]\n",
      "epoch:1 step:1180 [D loss: 0.652656, acc.: 60.16%] [G loss: 0.937905]\n",
      "epoch:1 step:1181 [D loss: 0.676793, acc.: 60.94%] [G loss: 0.964352]\n",
      "epoch:1 step:1182 [D loss: 0.693882, acc.: 54.69%] [G loss: 0.997416]\n",
      "epoch:1 step:1183 [D loss: 0.645982, acc.: 64.06%] [G loss: 1.013307]\n",
      "epoch:1 step:1184 [D loss: 0.664930, acc.: 61.72%] [G loss: 0.944530]\n",
      "epoch:1 step:1185 [D loss: 0.646375, acc.: 63.28%] [G loss: 0.918311]\n",
      "epoch:1 step:1186 [D loss: 0.613727, acc.: 71.88%] [G loss: 0.977731]\n",
      "epoch:1 step:1187 [D loss: 0.644861, acc.: 64.84%] [G loss: 0.920735]\n",
      "epoch:1 step:1188 [D loss: 0.663654, acc.: 61.72%] [G loss: 0.976241]\n",
      "epoch:1 step:1189 [D loss: 0.655252, acc.: 64.84%] [G loss: 0.964444]\n",
      "epoch:1 step:1190 [D loss: 0.676107, acc.: 54.69%] [G loss: 0.934893]\n",
      "epoch:1 step:1191 [D loss: 0.636213, acc.: 61.72%] [G loss: 0.987974]\n",
      "epoch:1 step:1192 [D loss: 0.687954, acc.: 57.03%] [G loss: 1.144211]\n",
      "epoch:1 step:1193 [D loss: 0.711458, acc.: 58.59%] [G loss: 1.000061]\n",
      "epoch:1 step:1194 [D loss: 0.646816, acc.: 64.06%] [G loss: 1.039797]\n",
      "epoch:1 step:1195 [D loss: 0.688139, acc.: 50.78%] [G loss: 0.941617]\n",
      "epoch:1 step:1196 [D loss: 0.647290, acc.: 65.62%] [G loss: 1.051486]\n",
      "epoch:1 step:1197 [D loss: 0.680466, acc.: 55.47%] [G loss: 0.967548]\n",
      "epoch:1 step:1198 [D loss: 0.665948, acc.: 57.03%] [G loss: 0.956239]\n",
      "epoch:1 step:1199 [D loss: 0.623487, acc.: 72.66%] [G loss: 0.970762]\n",
      "epoch:1 step:1200 [D loss: 0.631122, acc.: 64.06%] [G loss: 0.966605]\n",
      "epoch:1 step:1201 [D loss: 0.663026, acc.: 60.94%] [G loss: 0.973008]\n",
      "epoch:1 step:1202 [D loss: 0.647385, acc.: 64.06%] [G loss: 1.086624]\n",
      "epoch:1 step:1203 [D loss: 0.710794, acc.: 54.69%] [G loss: 1.047562]\n",
      "epoch:1 step:1204 [D loss: 0.614463, acc.: 69.53%] [G loss: 1.047405]\n",
      "epoch:1 step:1205 [D loss: 0.660964, acc.: 54.69%] [G loss: 0.992855]\n",
      "epoch:1 step:1206 [D loss: 0.675155, acc.: 57.03%] [G loss: 1.018952]\n",
      "epoch:1 step:1207 [D loss: 0.663223, acc.: 60.16%] [G loss: 1.006857]\n",
      "epoch:1 step:1208 [D loss: 0.642660, acc.: 64.84%] [G loss: 1.084356]\n",
      "epoch:1 step:1209 [D loss: 0.644663, acc.: 64.06%] [G loss: 1.038049]\n",
      "epoch:1 step:1210 [D loss: 0.670538, acc.: 59.38%] [G loss: 1.078933]\n",
      "epoch:1 step:1211 [D loss: 0.737535, acc.: 47.66%] [G loss: 1.028039]\n",
      "epoch:1 step:1212 [D loss: 0.624524, acc.: 67.19%] [G loss: 1.012746]\n",
      "epoch:1 step:1213 [D loss: 0.679903, acc.: 57.81%] [G loss: 0.995092]\n",
      "epoch:1 step:1214 [D loss: 0.655782, acc.: 65.62%] [G loss: 1.168742]\n",
      "epoch:1 step:1215 [D loss: 0.639676, acc.: 63.28%] [G loss: 1.014724]\n",
      "epoch:1 step:1216 [D loss: 0.652450, acc.: 58.59%] [G loss: 0.969992]\n",
      "epoch:1 step:1217 [D loss: 0.684577, acc.: 53.12%] [G loss: 1.031558]\n",
      "epoch:1 step:1218 [D loss: 0.623777, acc.: 68.75%] [G loss: 1.060900]\n",
      "epoch:1 step:1219 [D loss: 0.751408, acc.: 50.00%] [G loss: 1.084060]\n",
      "epoch:1 step:1220 [D loss: 0.676720, acc.: 64.06%] [G loss: 1.006319]\n",
      "epoch:1 step:1221 [D loss: 0.655373, acc.: 60.94%] [G loss: 1.070410]\n",
      "epoch:1 step:1222 [D loss: 0.675275, acc.: 58.59%] [G loss: 1.043259]\n",
      "epoch:1 step:1223 [D loss: 0.648501, acc.: 57.03%] [G loss: 0.965225]\n",
      "epoch:1 step:1224 [D loss: 0.626848, acc.: 65.62%] [G loss: 0.986888]\n",
      "epoch:1 step:1225 [D loss: 0.689037, acc.: 57.81%] [G loss: 0.978135]\n",
      "epoch:1 step:1226 [D loss: 0.706417, acc.: 53.12%] [G loss: 0.910920]\n",
      "epoch:1 step:1227 [D loss: 0.681474, acc.: 57.03%] [G loss: 0.960298]\n",
      "epoch:1 step:1228 [D loss: 0.642585, acc.: 61.72%] [G loss: 0.984827]\n",
      "epoch:1 step:1229 [D loss: 0.656577, acc.: 60.94%] [G loss: 0.929195]\n",
      "epoch:1 step:1230 [D loss: 0.730968, acc.: 50.00%] [G loss: 0.970930]\n",
      "epoch:1 step:1231 [D loss: 0.690697, acc.: 49.22%] [G loss: 0.934286]\n",
      "epoch:1 step:1232 [D loss: 0.673258, acc.: 60.16%] [G loss: 1.009527]\n",
      "epoch:1 step:1233 [D loss: 0.660462, acc.: 58.59%] [G loss: 1.060809]\n",
      "epoch:1 step:1234 [D loss: 0.679096, acc.: 53.91%] [G loss: 1.046056]\n",
      "epoch:1 step:1235 [D loss: 0.674930, acc.: 65.62%] [G loss: 0.984723]\n",
      "epoch:1 step:1236 [D loss: 0.665247, acc.: 64.84%] [G loss: 1.023491]\n",
      "epoch:1 step:1237 [D loss: 0.711928, acc.: 53.12%] [G loss: 0.914645]\n",
      "epoch:1 step:1238 [D loss: 0.652276, acc.: 58.59%] [G loss: 0.850151]\n",
      "epoch:1 step:1239 [D loss: 0.627542, acc.: 65.62%] [G loss: 0.889149]\n",
      "epoch:1 step:1240 [D loss: 0.637859, acc.: 64.06%] [G loss: 0.940222]\n",
      "epoch:1 step:1241 [D loss: 0.622644, acc.: 68.75%] [G loss: 0.982975]\n",
      "epoch:1 step:1242 [D loss: 0.663078, acc.: 61.72%] [G loss: 0.996921]\n",
      "epoch:1 step:1243 [D loss: 0.677043, acc.: 60.16%] [G loss: 0.933889]\n",
      "epoch:1 step:1244 [D loss: 0.674435, acc.: 57.81%] [G loss: 0.968630]\n",
      "epoch:1 step:1245 [D loss: 0.596440, acc.: 72.66%] [G loss: 0.932144]\n",
      "epoch:1 step:1246 [D loss: 0.717053, acc.: 53.12%] [G loss: 0.999833]\n",
      "epoch:1 step:1247 [D loss: 0.690915, acc.: 54.69%] [G loss: 0.967503]\n",
      "epoch:1 step:1248 [D loss: 0.708059, acc.: 50.78%] [G loss: 0.959572]\n",
      "epoch:1 step:1249 [D loss: 0.658486, acc.: 54.69%] [G loss: 0.938106]\n",
      "epoch:1 step:1250 [D loss: 0.667357, acc.: 57.03%] [G loss: 0.935040]\n",
      "epoch:1 step:1251 [D loss: 0.673477, acc.: 60.16%] [G loss: 0.978258]\n",
      "epoch:1 step:1252 [D loss: 0.629986, acc.: 60.94%] [G loss: 1.033708]\n",
      "epoch:1 step:1253 [D loss: 0.703475, acc.: 58.59%] [G loss: 0.888814]\n",
      "epoch:1 step:1254 [D loss: 0.673663, acc.: 57.03%] [G loss: 0.908204]\n",
      "epoch:1 step:1255 [D loss: 0.621069, acc.: 71.88%] [G loss: 0.968826]\n",
      "epoch:1 step:1256 [D loss: 0.639327, acc.: 66.41%] [G loss: 0.954802]\n",
      "epoch:1 step:1257 [D loss: 0.658343, acc.: 64.06%] [G loss: 0.960210]\n",
      "epoch:1 step:1258 [D loss: 0.604825, acc.: 68.75%] [G loss: 0.911820]\n",
      "epoch:1 step:1259 [D loss: 0.622734, acc.: 64.06%] [G loss: 0.985157]\n",
      "epoch:1 step:1260 [D loss: 0.630141, acc.: 64.06%] [G loss: 1.093276]\n",
      "epoch:1 step:1261 [D loss: 0.683486, acc.: 57.03%] [G loss: 0.957917]\n",
      "epoch:1 step:1262 [D loss: 0.639236, acc.: 58.59%] [G loss: 0.949437]\n",
      "epoch:1 step:1263 [D loss: 0.670165, acc.: 60.16%] [G loss: 0.950531]\n",
      "epoch:1 step:1264 [D loss: 0.620594, acc.: 70.31%] [G loss: 0.979038]\n",
      "epoch:1 step:1265 [D loss: 0.703846, acc.: 51.56%] [G loss: 0.947829]\n",
      "epoch:1 step:1266 [D loss: 0.631312, acc.: 66.41%] [G loss: 0.923624]\n",
      "epoch:1 step:1267 [D loss: 0.658043, acc.: 60.94%] [G loss: 0.974837]\n",
      "epoch:1 step:1268 [D loss: 0.636612, acc.: 69.53%] [G loss: 0.974234]\n",
      "epoch:1 step:1269 [D loss: 0.669655, acc.: 63.28%] [G loss: 0.947227]\n",
      "epoch:1 step:1270 [D loss: 0.704197, acc.: 51.56%] [G loss: 1.006719]\n",
      "epoch:1 step:1271 [D loss: 0.672339, acc.: 59.38%] [G loss: 0.916828]\n",
      "epoch:1 step:1272 [D loss: 0.673523, acc.: 57.81%] [G loss: 0.987343]\n",
      "epoch:1 step:1273 [D loss: 0.683490, acc.: 59.38%] [G loss: 0.990775]\n",
      "epoch:1 step:1274 [D loss: 0.681601, acc.: 57.81%] [G loss: 0.937845]\n",
      "epoch:1 step:1275 [D loss: 0.648829, acc.: 65.62%] [G loss: 0.993054]\n",
      "epoch:1 step:1276 [D loss: 0.681388, acc.: 53.91%] [G loss: 1.007558]\n",
      "epoch:1 step:1277 [D loss: 0.659265, acc.: 64.06%] [G loss: 1.020204]\n",
      "epoch:1 step:1278 [D loss: 0.692482, acc.: 53.12%] [G loss: 0.972869]\n",
      "epoch:1 step:1279 [D loss: 0.678206, acc.: 57.81%] [G loss: 0.936011]\n",
      "epoch:1 step:1280 [D loss: 0.632022, acc.: 64.84%] [G loss: 0.969579]\n",
      "epoch:1 step:1281 [D loss: 0.789748, acc.: 50.00%] [G loss: 1.023187]\n",
      "epoch:1 step:1282 [D loss: 0.677721, acc.: 57.03%] [G loss: 0.984200]\n",
      "epoch:1 step:1283 [D loss: 0.660986, acc.: 59.38%] [G loss: 0.919436]\n",
      "epoch:1 step:1284 [D loss: 0.673849, acc.: 58.59%] [G loss: 1.074458]\n",
      "epoch:1 step:1285 [D loss: 0.732360, acc.: 50.00%] [G loss: 1.007339]\n",
      "epoch:1 step:1286 [D loss: 0.709923, acc.: 55.47%] [G loss: 0.927194]\n",
      "epoch:1 step:1287 [D loss: 0.622562, acc.: 70.31%] [G loss: 0.964702]\n",
      "epoch:1 step:1288 [D loss: 0.682559, acc.: 52.34%] [G loss: 0.936531]\n",
      "epoch:1 step:1289 [D loss: 0.643376, acc.: 64.84%] [G loss: 0.994027]\n",
      "epoch:1 step:1290 [D loss: 0.647929, acc.: 65.62%] [G loss: 0.953130]\n",
      "epoch:1 step:1291 [D loss: 0.679421, acc.: 57.03%] [G loss: 0.974621]\n",
      "epoch:1 step:1292 [D loss: 0.645105, acc.: 62.50%] [G loss: 0.945385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1293 [D loss: 0.614413, acc.: 66.41%] [G loss: 0.974744]\n",
      "epoch:1 step:1294 [D loss: 0.607351, acc.: 69.53%] [G loss: 1.048483]\n",
      "epoch:1 step:1295 [D loss: 0.664196, acc.: 58.59%] [G loss: 1.068145]\n",
      "epoch:1 step:1296 [D loss: 0.715170, acc.: 46.88%] [G loss: 1.003690]\n",
      "epoch:1 step:1297 [D loss: 0.659044, acc.: 60.16%] [G loss: 0.947452]\n",
      "epoch:1 step:1298 [D loss: 0.637558, acc.: 66.41%] [G loss: 0.969516]\n",
      "epoch:1 step:1299 [D loss: 0.624816, acc.: 64.84%] [G loss: 0.991661]\n",
      "epoch:1 step:1300 [D loss: 0.661926, acc.: 60.94%] [G loss: 0.973267]\n",
      "epoch:1 step:1301 [D loss: 0.674484, acc.: 57.81%] [G loss: 1.104677]\n",
      "epoch:1 step:1302 [D loss: 0.672365, acc.: 61.72%] [G loss: 0.994214]\n",
      "epoch:1 step:1303 [D loss: 0.642937, acc.: 67.97%] [G loss: 0.915776]\n",
      "epoch:1 step:1304 [D loss: 0.674577, acc.: 60.16%] [G loss: 0.902614]\n",
      "epoch:1 step:1305 [D loss: 0.633933, acc.: 66.41%] [G loss: 1.016531]\n",
      "epoch:1 step:1306 [D loss: 0.671048, acc.: 60.94%] [G loss: 0.998660]\n",
      "epoch:1 step:1307 [D loss: 0.690349, acc.: 54.69%] [G loss: 1.034466]\n",
      "epoch:1 step:1308 [D loss: 0.703941, acc.: 58.59%] [G loss: 1.134569]\n",
      "epoch:1 step:1309 [D loss: 0.723351, acc.: 50.78%] [G loss: 0.949060]\n",
      "epoch:1 step:1310 [D loss: 0.705066, acc.: 50.78%] [G loss: 0.923988]\n",
      "epoch:1 step:1311 [D loss: 0.662043, acc.: 65.62%] [G loss: 0.993601]\n",
      "epoch:1 step:1312 [D loss: 0.649811, acc.: 53.91%] [G loss: 0.917688]\n",
      "epoch:1 step:1313 [D loss: 0.686787, acc.: 51.56%] [G loss: 0.955586]\n",
      "epoch:1 step:1314 [D loss: 0.637154, acc.: 58.59%] [G loss: 1.060521]\n",
      "epoch:1 step:1315 [D loss: 0.673996, acc.: 53.91%] [G loss: 0.920906]\n",
      "epoch:1 step:1316 [D loss: 0.657449, acc.: 61.72%] [G loss: 1.002765]\n",
      "epoch:1 step:1317 [D loss: 0.643358, acc.: 67.97%] [G loss: 0.905652]\n",
      "epoch:1 step:1318 [D loss: 0.660874, acc.: 60.94%] [G loss: 0.962319]\n",
      "epoch:1 step:1319 [D loss: 0.677004, acc.: 67.97%] [G loss: 0.962715]\n",
      "epoch:1 step:1320 [D loss: 0.681227, acc.: 57.81%] [G loss: 0.949893]\n",
      "epoch:1 step:1321 [D loss: 0.648520, acc.: 57.81%] [G loss: 1.013421]\n",
      "epoch:1 step:1322 [D loss: 0.632711, acc.: 60.94%] [G loss: 1.014164]\n",
      "epoch:1 step:1323 [D loss: 0.630211, acc.: 67.19%] [G loss: 0.981326]\n",
      "epoch:1 step:1324 [D loss: 0.663002, acc.: 56.25%] [G loss: 0.910267]\n",
      "epoch:1 step:1325 [D loss: 0.600613, acc.: 69.53%] [G loss: 1.042922]\n",
      "epoch:1 step:1326 [D loss: 0.671931, acc.: 58.59%] [G loss: 0.969133]\n",
      "epoch:1 step:1327 [D loss: 0.672310, acc.: 58.59%] [G loss: 0.999495]\n",
      "epoch:1 step:1328 [D loss: 0.667600, acc.: 54.69%] [G loss: 0.962003]\n",
      "epoch:1 step:1329 [D loss: 0.677925, acc.: 55.47%] [G loss: 0.995954]\n",
      "epoch:1 step:1330 [D loss: 0.651649, acc.: 57.81%] [G loss: 0.976622]\n",
      "epoch:1 step:1331 [D loss: 0.632467, acc.: 64.06%] [G loss: 1.007934]\n",
      "epoch:1 step:1332 [D loss: 0.646375, acc.: 59.38%] [G loss: 1.044811]\n",
      "epoch:1 step:1333 [D loss: 0.658755, acc.: 67.19%] [G loss: 1.200954]\n",
      "epoch:1 step:1334 [D loss: 0.665688, acc.: 57.03%] [G loss: 0.989760]\n",
      "epoch:1 step:1335 [D loss: 0.639770, acc.: 64.06%] [G loss: 1.028730]\n",
      "epoch:1 step:1336 [D loss: 0.637533, acc.: 64.84%] [G loss: 1.056893]\n",
      "epoch:1 step:1337 [D loss: 0.664970, acc.: 60.16%] [G loss: 0.993725]\n",
      "epoch:1 step:1338 [D loss: 0.654147, acc.: 60.16%] [G loss: 0.999552]\n",
      "epoch:1 step:1339 [D loss: 0.675985, acc.: 53.12%] [G loss: 1.051408]\n",
      "epoch:1 step:1340 [D loss: 0.626887, acc.: 69.53%] [G loss: 1.134806]\n",
      "epoch:1 step:1341 [D loss: 0.649572, acc.: 58.59%] [G loss: 1.013797]\n",
      "epoch:1 step:1342 [D loss: 0.621574, acc.: 69.53%] [G loss: 1.033962]\n",
      "epoch:1 step:1343 [D loss: 0.631814, acc.: 63.28%] [G loss: 1.011155]\n",
      "epoch:1 step:1344 [D loss: 0.707672, acc.: 61.72%] [G loss: 1.087153]\n",
      "epoch:1 step:1345 [D loss: 0.748752, acc.: 51.56%] [G loss: 0.897038]\n",
      "epoch:1 step:1346 [D loss: 0.648634, acc.: 66.41%] [G loss: 1.040612]\n",
      "epoch:1 step:1347 [D loss: 0.650321, acc.: 66.41%] [G loss: 0.977016]\n",
      "epoch:1 step:1348 [D loss: 0.662436, acc.: 57.03%] [G loss: 1.059173]\n",
      "epoch:1 step:1349 [D loss: 0.648966, acc.: 66.41%] [G loss: 0.998371]\n",
      "epoch:1 step:1350 [D loss: 0.643297, acc.: 59.38%] [G loss: 1.018689]\n",
      "epoch:1 step:1351 [D loss: 0.699094, acc.: 52.34%] [G loss: 0.977377]\n",
      "epoch:1 step:1352 [D loss: 0.688541, acc.: 63.28%] [G loss: 0.997535]\n",
      "epoch:1 step:1353 [D loss: 0.672893, acc.: 53.91%] [G loss: 1.008499]\n",
      "epoch:1 step:1354 [D loss: 0.678701, acc.: 55.47%] [G loss: 0.944326]\n",
      "epoch:1 step:1355 [D loss: 0.678820, acc.: 59.38%] [G loss: 0.953665]\n",
      "epoch:1 step:1356 [D loss: 0.672407, acc.: 54.69%] [G loss: 0.942337]\n",
      "epoch:1 step:1357 [D loss: 0.662704, acc.: 60.94%] [G loss: 0.983295]\n",
      "epoch:1 step:1358 [D loss: 0.662413, acc.: 60.16%] [G loss: 0.979514]\n",
      "epoch:1 step:1359 [D loss: 0.655651, acc.: 60.94%] [G loss: 1.049582]\n",
      "epoch:1 step:1360 [D loss: 0.609735, acc.: 71.88%] [G loss: 1.118140]\n",
      "epoch:1 step:1361 [D loss: 0.621481, acc.: 61.72%] [G loss: 1.025456]\n",
      "epoch:1 step:1362 [D loss: 0.664049, acc.: 57.81%] [G loss: 1.029076]\n",
      "epoch:1 step:1363 [D loss: 0.678465, acc.: 69.53%] [G loss: 1.059994]\n",
      "epoch:1 step:1364 [D loss: 0.706266, acc.: 46.88%] [G loss: 0.867875]\n",
      "epoch:1 step:1365 [D loss: 0.619684, acc.: 68.75%] [G loss: 1.043524]\n",
      "epoch:1 step:1366 [D loss: 0.613008, acc.: 67.97%] [G loss: 1.199604]\n",
      "epoch:1 step:1367 [D loss: 0.632990, acc.: 63.28%] [G loss: 1.112865]\n",
      "epoch:1 step:1368 [D loss: 0.604576, acc.: 70.31%] [G loss: 1.239780]\n",
      "epoch:1 step:1369 [D loss: 0.622399, acc.: 66.41%] [G loss: 1.195058]\n",
      "epoch:1 step:1370 [D loss: 0.642983, acc.: 62.50%] [G loss: 0.999481]\n",
      "epoch:1 step:1371 [D loss: 0.633486, acc.: 62.50%] [G loss: 0.984845]\n",
      "epoch:1 step:1372 [D loss: 0.626191, acc.: 65.62%] [G loss: 1.110748]\n",
      "epoch:1 step:1373 [D loss: 0.643415, acc.: 61.72%] [G loss: 1.099353]\n",
      "epoch:1 step:1374 [D loss: 0.604980, acc.: 71.88%] [G loss: 1.116604]\n",
      "epoch:1 step:1375 [D loss: 0.665701, acc.: 59.38%] [G loss: 0.914375]\n",
      "epoch:1 step:1376 [D loss: 0.699188, acc.: 48.44%] [G loss: 0.966933]\n",
      "epoch:1 step:1377 [D loss: 0.653314, acc.: 64.06%] [G loss: 1.058413]\n",
      "epoch:1 step:1378 [D loss: 0.668792, acc.: 59.38%] [G loss: 1.020127]\n",
      "epoch:1 step:1379 [D loss: 0.600794, acc.: 74.22%] [G loss: 1.150041]\n",
      "epoch:1 step:1380 [D loss: 0.591087, acc.: 75.78%] [G loss: 1.169517]\n",
      "epoch:1 step:1381 [D loss: 0.621298, acc.: 65.62%] [G loss: 1.198187]\n",
      "epoch:1 step:1382 [D loss: 0.618273, acc.: 66.41%] [G loss: 1.100171]\n",
      "epoch:1 step:1383 [D loss: 0.649400, acc.: 58.59%] [G loss: 1.055246]\n",
      "epoch:1 step:1384 [D loss: 0.629103, acc.: 63.28%] [G loss: 0.998466]\n",
      "epoch:1 step:1385 [D loss: 0.669493, acc.: 55.47%] [G loss: 0.977651]\n",
      "epoch:1 step:1386 [D loss: 0.622561, acc.: 66.41%] [G loss: 1.030273]\n",
      "epoch:1 step:1387 [D loss: 0.625657, acc.: 65.62%] [G loss: 1.054512]\n",
      "epoch:1 step:1388 [D loss: 0.638152, acc.: 61.72%] [G loss: 1.247457]\n",
      "epoch:1 step:1389 [D loss: 0.572082, acc.: 75.00%] [G loss: 1.372645]\n",
      "epoch:1 step:1390 [D loss: 0.635562, acc.: 66.41%] [G loss: 1.230076]\n",
      "epoch:1 step:1391 [D loss: 0.580186, acc.: 77.34%] [G loss: 1.092960]\n",
      "epoch:1 step:1392 [D loss: 0.695138, acc.: 50.78%] [G loss: 0.979647]\n",
      "epoch:1 step:1393 [D loss: 0.596984, acc.: 73.44%] [G loss: 1.057154]\n",
      "epoch:1 step:1394 [D loss: 0.665672, acc.: 57.03%] [G loss: 1.031737]\n",
      "epoch:1 step:1395 [D loss: 0.593038, acc.: 73.44%] [G loss: 1.072105]\n",
      "epoch:1 step:1396 [D loss: 0.610345, acc.: 61.72%] [G loss: 1.085440]\n",
      "epoch:1 step:1397 [D loss: 0.609368, acc.: 65.62%] [G loss: 1.119058]\n",
      "epoch:1 step:1398 [D loss: 0.657832, acc.: 63.28%] [G loss: 1.077881]\n",
      "epoch:1 step:1399 [D loss: 0.658289, acc.: 56.25%] [G loss: 1.074543]\n",
      "epoch:1 step:1400 [D loss: 0.624199, acc.: 67.97%] [G loss: 1.057980]\n",
      "epoch:1 step:1401 [D loss: 0.628948, acc.: 69.53%] [G loss: 1.173006]\n",
      "epoch:1 step:1402 [D loss: 0.620642, acc.: 63.28%] [G loss: 1.159942]\n",
      "epoch:1 step:1403 [D loss: 0.641542, acc.: 60.94%] [G loss: 1.112808]\n",
      "epoch:1 step:1404 [D loss: 0.560871, acc.: 72.66%] [G loss: 1.213688]\n",
      "epoch:1 step:1405 [D loss: 0.677362, acc.: 55.47%] [G loss: 1.062206]\n",
      "epoch:1 step:1406 [D loss: 0.622126, acc.: 66.41%] [G loss: 1.032822]\n",
      "epoch:1 step:1407 [D loss: 0.567006, acc.: 75.00%] [G loss: 1.306605]\n",
      "epoch:1 step:1408 [D loss: 0.581460, acc.: 74.22%] [G loss: 1.286773]\n",
      "epoch:1 step:1409 [D loss: 0.563221, acc.: 75.78%] [G loss: 1.437265]\n",
      "epoch:1 step:1410 [D loss: 0.575128, acc.: 69.53%] [G loss: 1.446946]\n",
      "epoch:1 step:1411 [D loss: 0.603387, acc.: 69.53%] [G loss: 1.059600]\n",
      "epoch:1 step:1412 [D loss: 0.582481, acc.: 71.09%] [G loss: 1.127050]\n",
      "epoch:1 step:1413 [D loss: 0.578500, acc.: 67.97%] [G loss: 1.191973]\n",
      "epoch:1 step:1414 [D loss: 0.617387, acc.: 69.53%] [G loss: 1.061858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1415 [D loss: 0.579317, acc.: 70.31%] [G loss: 1.208011]\n",
      "epoch:1 step:1416 [D loss: 0.497416, acc.: 80.47%] [G loss: 1.541273]\n",
      "epoch:1 step:1417 [D loss: 0.533116, acc.: 78.91%] [G loss: 1.522465]\n",
      "epoch:1 step:1418 [D loss: 0.609324, acc.: 67.97%] [G loss: 1.080809]\n",
      "epoch:1 step:1419 [D loss: 0.627524, acc.: 67.97%] [G loss: 1.463974]\n",
      "epoch:1 step:1420 [D loss: 0.620159, acc.: 64.06%] [G loss: 1.253202]\n",
      "epoch:1 step:1421 [D loss: 0.615158, acc.: 66.41%] [G loss: 1.178815]\n",
      "epoch:1 step:1422 [D loss: 0.570329, acc.: 68.75%] [G loss: 1.382766]\n",
      "epoch:1 step:1423 [D loss: 0.671109, acc.: 52.34%] [G loss: 0.969887]\n",
      "epoch:1 step:1424 [D loss: 0.651824, acc.: 58.59%] [G loss: 1.081422]\n",
      "epoch:1 step:1425 [D loss: 0.608527, acc.: 72.66%] [G loss: 1.055060]\n",
      "epoch:1 step:1426 [D loss: 0.579183, acc.: 69.53%] [G loss: 1.194579]\n",
      "epoch:1 step:1427 [D loss: 0.598991, acc.: 72.66%] [G loss: 1.149212]\n",
      "epoch:1 step:1428 [D loss: 0.589197, acc.: 62.50%] [G loss: 1.273979]\n",
      "epoch:1 step:1429 [D loss: 0.517778, acc.: 80.47%] [G loss: 1.309862]\n",
      "epoch:1 step:1430 [D loss: 0.561673, acc.: 78.91%] [G loss: 1.103776]\n",
      "epoch:1 step:1431 [D loss: 0.620624, acc.: 59.38%] [G loss: 1.255578]\n",
      "epoch:1 step:1432 [D loss: 0.568404, acc.: 71.88%] [G loss: 1.338071]\n",
      "epoch:1 step:1433 [D loss: 0.573184, acc.: 76.56%] [G loss: 1.212481]\n",
      "epoch:1 step:1434 [D loss: 0.546013, acc.: 78.91%] [G loss: 1.323592]\n",
      "epoch:1 step:1435 [D loss: 0.474556, acc.: 84.38%] [G loss: 1.605873]\n",
      "epoch:1 step:1436 [D loss: 0.589424, acc.: 72.66%] [G loss: 1.373260]\n",
      "epoch:1 step:1437 [D loss: 0.581443, acc.: 65.62%] [G loss: 1.863873]\n",
      "epoch:1 step:1438 [D loss: 0.834126, acc.: 57.81%] [G loss: 1.502746]\n",
      "epoch:1 step:1439 [D loss: 0.672783, acc.: 62.50%] [G loss: 1.372562]\n",
      "epoch:1 step:1440 [D loss: 0.646485, acc.: 57.03%] [G loss: 1.238281]\n",
      "epoch:1 step:1441 [D loss: 0.609734, acc.: 66.41%] [G loss: 1.191607]\n",
      "epoch:1 step:1442 [D loss: 0.639092, acc.: 65.62%] [G loss: 1.227108]\n",
      "epoch:1 step:1443 [D loss: 0.608448, acc.: 69.53%] [G loss: 1.324483]\n",
      "epoch:1 step:1444 [D loss: 0.600018, acc.: 66.41%] [G loss: 1.284971]\n",
      "epoch:1 step:1445 [D loss: 0.571512, acc.: 71.88%] [G loss: 1.259625]\n",
      "epoch:1 step:1446 [D loss: 0.595934, acc.: 70.31%] [G loss: 1.253120]\n",
      "epoch:1 step:1447 [D loss: 0.483518, acc.: 82.81%] [G loss: 1.613298]\n",
      "epoch:1 step:1448 [D loss: 0.427234, acc.: 90.62%] [G loss: 1.715867]\n",
      "epoch:1 step:1449 [D loss: 0.564435, acc.: 70.31%] [G loss: 1.431138]\n",
      "epoch:1 step:1450 [D loss: 0.574221, acc.: 71.09%] [G loss: 1.121593]\n",
      "epoch:1 step:1451 [D loss: 0.548871, acc.: 73.44%] [G loss: 1.395385]\n",
      "epoch:1 step:1452 [D loss: 0.610853, acc.: 71.09%] [G loss: 1.127217]\n",
      "epoch:1 step:1453 [D loss: 0.581796, acc.: 71.09%] [G loss: 1.488967]\n",
      "epoch:1 step:1454 [D loss: 0.560954, acc.: 69.53%] [G loss: 1.405428]\n",
      "epoch:1 step:1455 [D loss: 0.600415, acc.: 68.75%] [G loss: 1.355548]\n",
      "epoch:1 step:1456 [D loss: 0.575200, acc.: 69.53%] [G loss: 1.501092]\n",
      "epoch:1 step:1457 [D loss: 0.633757, acc.: 60.94%] [G loss: 1.887142]\n",
      "epoch:1 step:1458 [D loss: 0.599529, acc.: 63.28%] [G loss: 1.944758]\n",
      "epoch:1 step:1459 [D loss: 0.523604, acc.: 79.69%] [G loss: 1.495691]\n",
      "epoch:1 step:1460 [D loss: 0.563606, acc.: 73.44%] [G loss: 1.348413]\n",
      "epoch:1 step:1461 [D loss: 0.609304, acc.: 62.50%] [G loss: 1.448647]\n",
      "epoch:1 step:1462 [D loss: 0.549070, acc.: 69.53%] [G loss: 1.650243]\n",
      "epoch:1 step:1463 [D loss: 0.695300, acc.: 57.03%] [G loss: 1.482983]\n",
      "epoch:1 step:1464 [D loss: 0.667690, acc.: 61.72%] [G loss: 2.062849]\n",
      "epoch:1 step:1465 [D loss: 0.616370, acc.: 73.44%] [G loss: 1.232283]\n",
      "epoch:1 step:1466 [D loss: 0.559408, acc.: 72.66%] [G loss: 1.467434]\n",
      "epoch:1 step:1467 [D loss: 0.533476, acc.: 74.22%] [G loss: 1.511964]\n",
      "epoch:1 step:1468 [D loss: 0.576168, acc.: 76.56%] [G loss: 1.253640]\n",
      "epoch:1 step:1469 [D loss: 0.571883, acc.: 73.44%] [G loss: 1.641424]\n",
      "epoch:1 step:1470 [D loss: 0.551365, acc.: 69.53%] [G loss: 1.294530]\n",
      "epoch:1 step:1471 [D loss: 0.555132, acc.: 75.00%] [G loss: 1.303517]\n",
      "epoch:1 step:1472 [D loss: 0.578984, acc.: 69.53%] [G loss: 1.647749]\n",
      "epoch:1 step:1473 [D loss: 0.616560, acc.: 60.94%] [G loss: 1.420300]\n",
      "epoch:1 step:1474 [D loss: 0.534535, acc.: 71.88%] [G loss: 1.563409]\n",
      "epoch:1 step:1475 [D loss: 0.534524, acc.: 71.88%] [G loss: 1.583721]\n",
      "epoch:1 step:1476 [D loss: 0.596818, acc.: 74.22%] [G loss: 1.305488]\n",
      "epoch:1 step:1477 [D loss: 0.595944, acc.: 69.53%] [G loss: 1.554835]\n",
      "epoch:1 step:1478 [D loss: 0.639676, acc.: 66.41%] [G loss: 1.541229]\n",
      "epoch:1 step:1479 [D loss: 0.641457, acc.: 64.84%] [G loss: 1.358950]\n",
      "epoch:1 step:1480 [D loss: 0.561409, acc.: 71.09%] [G loss: 1.700484]\n",
      "epoch:1 step:1481 [D loss: 0.631443, acc.: 60.94%] [G loss: 1.372817]\n",
      "epoch:1 step:1482 [D loss: 0.568528, acc.: 71.88%] [G loss: 1.515173]\n",
      "epoch:1 step:1483 [D loss: 0.553935, acc.: 67.97%] [G loss: 1.383400]\n",
      "epoch:1 step:1484 [D loss: 0.545111, acc.: 75.78%] [G loss: 1.263836]\n",
      "epoch:1 step:1485 [D loss: 0.560812, acc.: 71.88%] [G loss: 1.632801]\n",
      "epoch:1 step:1486 [D loss: 0.585024, acc.: 67.19%] [G loss: 2.349309]\n",
      "epoch:1 step:1487 [D loss: 0.819967, acc.: 52.34%] [G loss: 1.735204]\n",
      "epoch:1 step:1488 [D loss: 0.766961, acc.: 49.22%] [G loss: 1.445534]\n",
      "epoch:1 step:1489 [D loss: 0.541919, acc.: 75.78%] [G loss: 1.546689]\n",
      "epoch:1 step:1490 [D loss: 0.567441, acc.: 70.31%] [G loss: 1.602578]\n",
      "epoch:1 step:1491 [D loss: 0.491634, acc.: 81.25%] [G loss: 1.547754]\n",
      "epoch:1 step:1492 [D loss: 0.540889, acc.: 78.12%] [G loss: 1.418388]\n",
      "epoch:1 step:1493 [D loss: 0.557731, acc.: 75.00%] [G loss: 1.390174]\n",
      "epoch:1 step:1494 [D loss: 0.587789, acc.: 68.75%] [G loss: 1.575837]\n",
      "epoch:1 step:1495 [D loss: 0.531478, acc.: 79.69%] [G loss: 1.831002]\n",
      "epoch:1 step:1496 [D loss: 0.460568, acc.: 82.03%] [G loss: 2.054641]\n",
      "epoch:1 step:1497 [D loss: 0.562327, acc.: 67.19%] [G loss: 1.368980]\n",
      "epoch:1 step:1498 [D loss: 0.430517, acc.: 85.94%] [G loss: 2.164711]\n",
      "epoch:1 step:1499 [D loss: 0.427769, acc.: 88.28%] [G loss: 1.526482]\n",
      "epoch:1 step:1500 [D loss: 0.543206, acc.: 71.09%] [G loss: 1.460730]\n",
      "epoch:1 step:1501 [D loss: 0.540696, acc.: 78.12%] [G loss: 1.639255]\n",
      "epoch:1 step:1502 [D loss: 0.564750, acc.: 71.88%] [G loss: 1.707937]\n",
      "epoch:1 step:1503 [D loss: 0.592107, acc.: 71.88%] [G loss: 1.659665]\n",
      "epoch:1 step:1504 [D loss: 0.517850, acc.: 75.78%] [G loss: 1.566859]\n",
      "epoch:1 step:1505 [D loss: 0.489892, acc.: 83.59%] [G loss: 1.686507]\n",
      "epoch:1 step:1506 [D loss: 0.570506, acc.: 67.97%] [G loss: 1.591957]\n",
      "epoch:1 step:1507 [D loss: 0.442559, acc.: 82.03%] [G loss: 2.558089]\n",
      "epoch:1 step:1508 [D loss: 0.573808, acc.: 70.31%] [G loss: 1.734951]\n",
      "epoch:1 step:1509 [D loss: 0.512560, acc.: 76.56%] [G loss: 1.222950]\n",
      "epoch:1 step:1510 [D loss: 0.432212, acc.: 84.38%] [G loss: 1.734526]\n",
      "epoch:1 step:1511 [D loss: 0.553278, acc.: 75.78%] [G loss: 1.620281]\n",
      "epoch:1 step:1512 [D loss: 0.554475, acc.: 77.34%] [G loss: 1.913556]\n",
      "epoch:1 step:1513 [D loss: 0.474295, acc.: 78.91%] [G loss: 2.043146]\n",
      "epoch:1 step:1514 [D loss: 0.488975, acc.: 76.56%] [G loss: 2.022929]\n",
      "epoch:1 step:1515 [D loss: 0.539110, acc.: 75.00%] [G loss: 1.563470]\n",
      "epoch:1 step:1516 [D loss: 0.617757, acc.: 67.19%] [G loss: 1.597743]\n",
      "epoch:1 step:1517 [D loss: 0.557558, acc.: 76.56%] [G loss: 1.551564]\n",
      "epoch:1 step:1518 [D loss: 0.563705, acc.: 73.44%] [G loss: 1.502336]\n",
      "epoch:1 step:1519 [D loss: 0.588520, acc.: 63.28%] [G loss: 1.710529]\n",
      "epoch:1 step:1520 [D loss: 0.630433, acc.: 66.41%] [G loss: 1.789338]\n",
      "epoch:1 step:1521 [D loss: 0.585340, acc.: 68.75%] [G loss: 1.438637]\n",
      "epoch:1 step:1522 [D loss: 0.501445, acc.: 80.47%] [G loss: 1.714023]\n",
      "epoch:1 step:1523 [D loss: 0.468666, acc.: 82.81%] [G loss: 1.590730]\n",
      "epoch:1 step:1524 [D loss: 0.606734, acc.: 70.31%] [G loss: 1.254667]\n",
      "epoch:1 step:1525 [D loss: 0.541960, acc.: 76.56%] [G loss: 1.737154]\n",
      "epoch:1 step:1526 [D loss: 0.485897, acc.: 73.44%] [G loss: 2.062355]\n",
      "epoch:1 step:1527 [D loss: 0.515391, acc.: 81.25%] [G loss: 1.386714]\n",
      "epoch:1 step:1528 [D loss: 0.537741, acc.: 73.44%] [G loss: 1.679633]\n",
      "epoch:1 step:1529 [D loss: 0.468923, acc.: 82.03%] [G loss: 2.177225]\n",
      "epoch:1 step:1530 [D loss: 0.519944, acc.: 77.34%] [G loss: 1.600260]\n",
      "epoch:1 step:1531 [D loss: 0.546840, acc.: 74.22%] [G loss: 1.411982]\n",
      "epoch:1 step:1532 [D loss: 0.471591, acc.: 82.81%] [G loss: 1.935700]\n",
      "epoch:1 step:1533 [D loss: 0.482783, acc.: 82.03%] [G loss: 1.546300]\n",
      "epoch:1 step:1534 [D loss: 0.524696, acc.: 75.78%] [G loss: 1.476317]\n",
      "epoch:1 step:1535 [D loss: 0.530118, acc.: 75.00%] [G loss: 1.723442]\n",
      "epoch:1 step:1536 [D loss: 0.512472, acc.: 75.00%] [G loss: 2.037260]\n",
      "epoch:1 step:1537 [D loss: 0.533550, acc.: 71.09%] [G loss: 1.638251]\n",
      "epoch:1 step:1538 [D loss: 0.501732, acc.: 78.12%] [G loss: 1.632867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1539 [D loss: 0.483244, acc.: 78.12%] [G loss: 1.423609]\n",
      "epoch:1 step:1540 [D loss: 0.457695, acc.: 80.47%] [G loss: 1.942696]\n",
      "epoch:1 step:1541 [D loss: 0.498405, acc.: 76.56%] [G loss: 2.208144]\n",
      "epoch:1 step:1542 [D loss: 0.469745, acc.: 77.34%] [G loss: 1.930413]\n",
      "epoch:1 step:1543 [D loss: 0.442891, acc.: 87.50%] [G loss: 1.897164]\n",
      "epoch:1 step:1544 [D loss: 0.551507, acc.: 75.00%] [G loss: 1.215522]\n",
      "epoch:1 step:1545 [D loss: 0.486690, acc.: 83.59%] [G loss: 1.944422]\n",
      "epoch:1 step:1546 [D loss: 0.598401, acc.: 63.28%] [G loss: 2.596773]\n",
      "epoch:1 step:1547 [D loss: 0.773391, acc.: 62.50%] [G loss: 1.573242]\n",
      "epoch:1 step:1548 [D loss: 0.556823, acc.: 75.00%] [G loss: 1.883969]\n",
      "epoch:1 step:1549 [D loss: 0.471785, acc.: 80.47%] [G loss: 1.828713]\n",
      "epoch:1 step:1550 [D loss: 0.481896, acc.: 82.03%] [G loss: 1.714046]\n",
      "epoch:1 step:1551 [D loss: 0.484071, acc.: 86.72%] [G loss: 1.932513]\n",
      "epoch:1 step:1552 [D loss: 0.475080, acc.: 81.25%] [G loss: 1.836761]\n",
      "epoch:1 step:1553 [D loss: 0.456789, acc.: 82.03%] [G loss: 1.620621]\n",
      "epoch:1 step:1554 [D loss: 0.425204, acc.: 87.50%] [G loss: 1.433326]\n",
      "epoch:1 step:1555 [D loss: 0.539626, acc.: 73.44%] [G loss: 1.533429]\n",
      "epoch:1 step:1556 [D loss: 0.459817, acc.: 84.38%] [G loss: 1.612197]\n",
      "epoch:1 step:1557 [D loss: 0.521623, acc.: 79.69%] [G loss: 1.748441]\n",
      "epoch:1 step:1558 [D loss: 0.457655, acc.: 79.69%] [G loss: 2.024382]\n",
      "epoch:1 step:1559 [D loss: 0.507918, acc.: 75.78%] [G loss: 2.021142]\n",
      "epoch:1 step:1560 [D loss: 0.423875, acc.: 84.38%] [G loss: 1.870781]\n",
      "epoch:1 step:1561 [D loss: 0.546504, acc.: 71.09%] [G loss: 1.629047]\n",
      "epoch:1 step:1562 [D loss: 0.494836, acc.: 81.25%] [G loss: 2.472264]\n",
      "epoch:2 step:1563 [D loss: 0.651357, acc.: 58.59%] [G loss: 1.483680]\n",
      "epoch:2 step:1564 [D loss: 0.571019, acc.: 74.22%] [G loss: 1.659674]\n",
      "epoch:2 step:1565 [D loss: 0.542035, acc.: 76.56%] [G loss: 2.275035]\n",
      "epoch:2 step:1566 [D loss: 0.561366, acc.: 71.88%] [G loss: 2.227777]\n",
      "epoch:2 step:1567 [D loss: 0.545746, acc.: 75.78%] [G loss: 1.559318]\n",
      "epoch:2 step:1568 [D loss: 0.563825, acc.: 67.97%] [G loss: 1.449246]\n",
      "epoch:2 step:1569 [D loss: 0.515821, acc.: 80.47%] [G loss: 1.658468]\n",
      "epoch:2 step:1570 [D loss: 0.447953, acc.: 78.91%] [G loss: 1.980077]\n",
      "epoch:2 step:1571 [D loss: 0.550273, acc.: 74.22%] [G loss: 1.511728]\n",
      "epoch:2 step:1572 [D loss: 0.509770, acc.: 75.00%] [G loss: 1.528570]\n",
      "epoch:2 step:1573 [D loss: 0.473898, acc.: 82.03%] [G loss: 1.613679]\n",
      "epoch:2 step:1574 [D loss: 0.444772, acc.: 83.59%] [G loss: 1.806575]\n",
      "epoch:2 step:1575 [D loss: 0.513629, acc.: 71.88%] [G loss: 1.890520]\n",
      "epoch:2 step:1576 [D loss: 0.453874, acc.: 81.25%] [G loss: 1.983158]\n",
      "epoch:2 step:1577 [D loss: 0.546664, acc.: 71.88%] [G loss: 1.658164]\n",
      "epoch:2 step:1578 [D loss: 0.520438, acc.: 71.88%] [G loss: 2.549766]\n",
      "epoch:2 step:1579 [D loss: 0.518923, acc.: 75.78%] [G loss: 2.221861]\n",
      "epoch:2 step:1580 [D loss: 0.463874, acc.: 80.47%] [G loss: 2.142835]\n",
      "epoch:2 step:1581 [D loss: 0.452950, acc.: 81.25%] [G loss: 2.030118]\n",
      "epoch:2 step:1582 [D loss: 0.496235, acc.: 73.44%] [G loss: 2.043086]\n",
      "epoch:2 step:1583 [D loss: 0.470389, acc.: 81.25%] [G loss: 2.275340]\n",
      "epoch:2 step:1584 [D loss: 0.436570, acc.: 86.72%] [G loss: 2.633800]\n",
      "epoch:2 step:1585 [D loss: 0.564514, acc.: 68.75%] [G loss: 1.485433]\n",
      "epoch:2 step:1586 [D loss: 0.505447, acc.: 72.66%] [G loss: 1.606612]\n",
      "epoch:2 step:1587 [D loss: 0.515240, acc.: 79.69%] [G loss: 1.764018]\n",
      "epoch:2 step:1588 [D loss: 0.461593, acc.: 82.81%] [G loss: 1.657624]\n",
      "epoch:2 step:1589 [D loss: 0.464894, acc.: 80.47%] [G loss: 1.878336]\n",
      "epoch:2 step:1590 [D loss: 0.509603, acc.: 73.44%] [G loss: 2.779881]\n",
      "epoch:2 step:1591 [D loss: 0.614338, acc.: 67.97%] [G loss: 3.505514]\n",
      "epoch:2 step:1592 [D loss: 0.678181, acc.: 65.62%] [G loss: 2.336702]\n",
      "epoch:2 step:1593 [D loss: 0.539012, acc.: 70.31%] [G loss: 2.083698]\n",
      "epoch:2 step:1594 [D loss: 0.567026, acc.: 69.53%] [G loss: 2.512279]\n",
      "epoch:2 step:1595 [D loss: 0.512667, acc.: 74.22%] [G loss: 1.571459]\n",
      "epoch:2 step:1596 [D loss: 0.485551, acc.: 81.25%] [G loss: 2.314121]\n",
      "epoch:2 step:1597 [D loss: 0.662397, acc.: 58.59%] [G loss: 1.600390]\n",
      "epoch:2 step:1598 [D loss: 0.499806, acc.: 75.78%] [G loss: 1.855561]\n",
      "epoch:2 step:1599 [D loss: 0.526718, acc.: 77.34%] [G loss: 1.907631]\n",
      "epoch:2 step:1600 [D loss: 0.526873, acc.: 75.78%] [G loss: 1.933432]\n",
      "epoch:2 step:1601 [D loss: 0.468485, acc.: 76.56%] [G loss: 2.082903]\n",
      "epoch:2 step:1602 [D loss: 0.504772, acc.: 76.56%] [G loss: 2.101970]\n",
      "epoch:2 step:1603 [D loss: 0.532723, acc.: 79.69%] [G loss: 1.903293]\n",
      "epoch:2 step:1604 [D loss: 0.514619, acc.: 72.66%] [G loss: 2.426435]\n",
      "epoch:2 step:1605 [D loss: 0.573277, acc.: 66.41%] [G loss: 2.157080]\n",
      "epoch:2 step:1606 [D loss: 0.591185, acc.: 67.97%] [G loss: 1.830532]\n",
      "epoch:2 step:1607 [D loss: 0.470563, acc.: 78.91%] [G loss: 1.938237]\n",
      "epoch:2 step:1608 [D loss: 0.553603, acc.: 73.44%] [G loss: 1.546771]\n",
      "epoch:2 step:1609 [D loss: 0.486596, acc.: 78.12%] [G loss: 2.073119]\n",
      "epoch:2 step:1610 [D loss: 0.511139, acc.: 76.56%] [G loss: 1.779019]\n",
      "epoch:2 step:1611 [D loss: 0.498044, acc.: 76.56%] [G loss: 2.313387]\n",
      "epoch:2 step:1612 [D loss: 0.534473, acc.: 75.78%] [G loss: 2.130400]\n",
      "epoch:2 step:1613 [D loss: 0.474588, acc.: 79.69%] [G loss: 2.564991]\n",
      "epoch:2 step:1614 [D loss: 0.460806, acc.: 75.78%] [G loss: 2.240866]\n",
      "epoch:2 step:1615 [D loss: 0.408086, acc.: 88.28%] [G loss: 1.960971]\n",
      "epoch:2 step:1616 [D loss: 0.434469, acc.: 78.91%] [G loss: 2.529184]\n",
      "epoch:2 step:1617 [D loss: 0.507727, acc.: 75.78%] [G loss: 1.854465]\n",
      "epoch:2 step:1618 [D loss: 0.523678, acc.: 74.22%] [G loss: 2.000432]\n",
      "epoch:2 step:1619 [D loss: 0.523698, acc.: 69.53%] [G loss: 2.186068]\n",
      "epoch:2 step:1620 [D loss: 0.503999, acc.: 73.44%] [G loss: 2.068459]\n",
      "epoch:2 step:1621 [D loss: 0.476014, acc.: 81.25%] [G loss: 2.050219]\n",
      "epoch:2 step:1622 [D loss: 0.502669, acc.: 72.66%] [G loss: 1.748923]\n",
      "epoch:2 step:1623 [D loss: 0.567129, acc.: 67.97%] [G loss: 2.567269]\n",
      "epoch:2 step:1624 [D loss: 0.578047, acc.: 72.66%] [G loss: 2.452034]\n",
      "epoch:2 step:1625 [D loss: 0.527318, acc.: 74.22%] [G loss: 2.109346]\n",
      "epoch:2 step:1626 [D loss: 0.484054, acc.: 77.34%] [G loss: 2.555130]\n",
      "epoch:2 step:1627 [D loss: 0.531249, acc.: 74.22%] [G loss: 2.462690]\n",
      "epoch:2 step:1628 [D loss: 0.513931, acc.: 78.91%] [G loss: 1.639881]\n",
      "epoch:2 step:1629 [D loss: 0.535642, acc.: 75.78%] [G loss: 2.097784]\n",
      "epoch:2 step:1630 [D loss: 0.454908, acc.: 83.59%] [G loss: 2.101228]\n",
      "epoch:2 step:1631 [D loss: 0.587471, acc.: 69.53%] [G loss: 2.573121]\n",
      "epoch:2 step:1632 [D loss: 0.523885, acc.: 74.22%] [G loss: 2.574506]\n",
      "epoch:2 step:1633 [D loss: 0.568910, acc.: 68.75%] [G loss: 1.507097]\n",
      "epoch:2 step:1634 [D loss: 0.556553, acc.: 67.97%] [G loss: 2.413869]\n",
      "epoch:2 step:1635 [D loss: 0.547712, acc.: 71.88%] [G loss: 2.859192]\n",
      "epoch:2 step:1636 [D loss: 0.691228, acc.: 57.81%] [G loss: 2.197770]\n",
      "epoch:2 step:1637 [D loss: 0.729748, acc.: 66.41%] [G loss: 1.933339]\n",
      "epoch:2 step:1638 [D loss: 0.527495, acc.: 76.56%] [G loss: 1.929687]\n",
      "epoch:2 step:1639 [D loss: 0.539947, acc.: 76.56%] [G loss: 2.430759]\n",
      "epoch:2 step:1640 [D loss: 0.530172, acc.: 75.78%] [G loss: 2.070011]\n",
      "epoch:2 step:1641 [D loss: 0.473622, acc.: 78.91%] [G loss: 1.985929]\n",
      "epoch:2 step:1642 [D loss: 0.404806, acc.: 84.38%] [G loss: 2.036189]\n",
      "epoch:2 step:1643 [D loss: 0.563100, acc.: 75.00%] [G loss: 1.566749]\n",
      "epoch:2 step:1644 [D loss: 0.490708, acc.: 79.69%] [G loss: 2.027882]\n",
      "epoch:2 step:1645 [D loss: 0.506049, acc.: 76.56%] [G loss: 1.888816]\n",
      "epoch:2 step:1646 [D loss: 0.464588, acc.: 81.25%] [G loss: 2.358307]\n",
      "epoch:2 step:1647 [D loss: 0.526814, acc.: 73.44%] [G loss: 2.186499]\n",
      "epoch:2 step:1648 [D loss: 0.508325, acc.: 78.91%] [G loss: 1.950543]\n",
      "epoch:2 step:1649 [D loss: 0.502084, acc.: 78.91%] [G loss: 1.321009]\n",
      "epoch:2 step:1650 [D loss: 0.478892, acc.: 84.38%] [G loss: 1.785964]\n",
      "epoch:2 step:1651 [D loss: 0.486013, acc.: 81.25%] [G loss: 2.212725]\n",
      "epoch:2 step:1652 [D loss: 0.502429, acc.: 78.91%] [G loss: 2.139095]\n",
      "epoch:2 step:1653 [D loss: 0.477638, acc.: 82.81%] [G loss: 2.146699]\n",
      "epoch:2 step:1654 [D loss: 0.554257, acc.: 71.09%] [G loss: 1.637120]\n",
      "epoch:2 step:1655 [D loss: 0.576325, acc.: 64.84%] [G loss: 2.040775]\n",
      "epoch:2 step:1656 [D loss: 0.476123, acc.: 77.34%] [G loss: 2.149823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1657 [D loss: 0.581976, acc.: 65.62%] [G loss: 2.885916]\n",
      "epoch:2 step:1658 [D loss: 0.529785, acc.: 70.31%] [G loss: 2.788916]\n",
      "epoch:2 step:1659 [D loss: 0.658152, acc.: 60.16%] [G loss: 1.700108]\n",
      "epoch:2 step:1660 [D loss: 0.500621, acc.: 72.66%] [G loss: 2.315338]\n",
      "epoch:2 step:1661 [D loss: 0.493671, acc.: 76.56%] [G loss: 1.573151]\n",
      "epoch:2 step:1662 [D loss: 0.522570, acc.: 72.66%] [G loss: 1.746015]\n",
      "epoch:2 step:1663 [D loss: 0.481631, acc.: 79.69%] [G loss: 1.886836]\n",
      "epoch:2 step:1664 [D loss: 0.493418, acc.: 82.03%] [G loss: 1.728441]\n",
      "epoch:2 step:1665 [D loss: 0.473430, acc.: 79.69%] [G loss: 2.064540]\n",
      "epoch:2 step:1666 [D loss: 0.478738, acc.: 82.81%] [G loss: 1.906070]\n",
      "epoch:2 step:1667 [D loss: 0.619125, acc.: 57.81%] [G loss: 2.231971]\n",
      "epoch:2 step:1668 [D loss: 0.535634, acc.: 74.22%] [G loss: 3.438521]\n",
      "epoch:2 step:1669 [D loss: 0.656516, acc.: 59.38%] [G loss: 2.320598]\n",
      "epoch:2 step:1670 [D loss: 0.445442, acc.: 76.56%] [G loss: 2.144790]\n",
      "epoch:2 step:1671 [D loss: 0.583142, acc.: 68.75%] [G loss: 1.789726]\n",
      "epoch:2 step:1672 [D loss: 0.421795, acc.: 87.50%] [G loss: 2.007729]\n",
      "epoch:2 step:1673 [D loss: 0.517599, acc.: 75.00%] [G loss: 1.873235]\n",
      "epoch:2 step:1674 [D loss: 0.470991, acc.: 82.81%] [G loss: 1.880223]\n",
      "epoch:2 step:1675 [D loss: 0.455168, acc.: 85.16%] [G loss: 3.125838]\n",
      "epoch:2 step:1676 [D loss: 0.660101, acc.: 59.38%] [G loss: 2.427657]\n",
      "epoch:2 step:1677 [D loss: 0.545145, acc.: 68.75%] [G loss: 2.584702]\n",
      "epoch:2 step:1678 [D loss: 0.629447, acc.: 63.28%] [G loss: 1.903203]\n",
      "epoch:2 step:1679 [D loss: 0.468575, acc.: 80.47%] [G loss: 1.860535]\n",
      "epoch:2 step:1680 [D loss: 0.510048, acc.: 71.88%] [G loss: 2.012921]\n",
      "epoch:2 step:1681 [D loss: 0.516728, acc.: 76.56%] [G loss: 2.391282]\n",
      "epoch:2 step:1682 [D loss: 0.532264, acc.: 68.75%] [G loss: 2.286907]\n",
      "epoch:2 step:1683 [D loss: 0.483137, acc.: 79.69%] [G loss: 2.150942]\n",
      "epoch:2 step:1684 [D loss: 0.498014, acc.: 74.22%] [G loss: 2.551339]\n",
      "epoch:2 step:1685 [D loss: 0.650367, acc.: 61.72%] [G loss: 2.228329]\n",
      "epoch:2 step:1686 [D loss: 0.521860, acc.: 72.66%] [G loss: 2.039923]\n",
      "epoch:2 step:1687 [D loss: 0.517952, acc.: 76.56%] [G loss: 1.676065]\n",
      "epoch:2 step:1688 [D loss: 0.489851, acc.: 81.25%] [G loss: 2.128066]\n",
      "epoch:2 step:1689 [D loss: 0.459044, acc.: 84.38%] [G loss: 1.770661]\n",
      "epoch:2 step:1690 [D loss: 0.475418, acc.: 78.12%] [G loss: 2.066065]\n",
      "epoch:2 step:1691 [D loss: 0.437088, acc.: 85.94%] [G loss: 2.112612]\n",
      "epoch:2 step:1692 [D loss: 0.514022, acc.: 81.25%] [G loss: 2.380019]\n",
      "epoch:2 step:1693 [D loss: 0.544344, acc.: 68.75%] [G loss: 3.179772]\n",
      "epoch:2 step:1694 [D loss: 0.791204, acc.: 54.69%] [G loss: 2.060647]\n",
      "epoch:2 step:1695 [D loss: 0.410165, acc.: 89.84%] [G loss: 1.968612]\n",
      "epoch:2 step:1696 [D loss: 0.530738, acc.: 78.12%] [G loss: 2.465969]\n",
      "epoch:2 step:1697 [D loss: 0.506609, acc.: 75.78%] [G loss: 1.790469]\n",
      "epoch:2 step:1698 [D loss: 0.535519, acc.: 74.22%] [G loss: 2.053029]\n",
      "epoch:2 step:1699 [D loss: 0.548252, acc.: 66.41%] [G loss: 1.815149]\n",
      "epoch:2 step:1700 [D loss: 0.421176, acc.: 88.28%] [G loss: 2.201175]\n",
      "epoch:2 step:1701 [D loss: 0.510147, acc.: 79.69%] [G loss: 1.947631]\n",
      "epoch:2 step:1702 [D loss: 0.476277, acc.: 80.47%] [G loss: 2.260921]\n",
      "epoch:2 step:1703 [D loss: 0.523867, acc.: 77.34%] [G loss: 1.756395]\n",
      "epoch:2 step:1704 [D loss: 0.508643, acc.: 74.22%] [G loss: 2.539236]\n",
      "epoch:2 step:1705 [D loss: 0.491444, acc.: 75.78%] [G loss: 3.440943]\n",
      "epoch:2 step:1706 [D loss: 0.483321, acc.: 76.56%] [G loss: 2.444499]\n",
      "epoch:2 step:1707 [D loss: 0.438333, acc.: 86.72%] [G loss: 2.149508]\n",
      "epoch:2 step:1708 [D loss: 0.439449, acc.: 81.25%] [G loss: 2.476214]\n",
      "epoch:2 step:1709 [D loss: 0.448486, acc.: 84.38%] [G loss: 2.243203]\n",
      "epoch:2 step:1710 [D loss: 0.505623, acc.: 74.22%] [G loss: 3.370164]\n",
      "epoch:2 step:1711 [D loss: 0.630661, acc.: 66.41%] [G loss: 2.953980]\n",
      "epoch:2 step:1712 [D loss: 0.506226, acc.: 74.22%] [G loss: 2.380573]\n",
      "epoch:2 step:1713 [D loss: 0.442896, acc.: 83.59%] [G loss: 1.952739]\n",
      "epoch:2 step:1714 [D loss: 0.452955, acc.: 81.25%] [G loss: 2.124599]\n",
      "epoch:2 step:1715 [D loss: 0.487067, acc.: 75.00%] [G loss: 2.056679]\n",
      "epoch:2 step:1716 [D loss: 0.466203, acc.: 84.38%] [G loss: 1.915138]\n",
      "epoch:2 step:1717 [D loss: 0.539698, acc.: 71.88%] [G loss: 1.581318]\n",
      "epoch:2 step:1718 [D loss: 0.499191, acc.: 78.91%] [G loss: 1.505514]\n",
      "epoch:2 step:1719 [D loss: 0.499867, acc.: 74.22%] [G loss: 2.117188]\n",
      "epoch:2 step:1720 [D loss: 0.516864, acc.: 69.53%] [G loss: 3.208617]\n",
      "epoch:2 step:1721 [D loss: 0.571532, acc.: 78.91%] [G loss: 2.672747]\n",
      "epoch:2 step:1722 [D loss: 0.781125, acc.: 60.94%] [G loss: 3.018984]\n",
      "epoch:2 step:1723 [D loss: 0.742862, acc.: 58.59%] [G loss: 1.420263]\n",
      "epoch:2 step:1724 [D loss: 0.484919, acc.: 75.78%] [G loss: 2.171154]\n",
      "epoch:2 step:1725 [D loss: 0.497058, acc.: 80.47%] [G loss: 1.703935]\n",
      "epoch:2 step:1726 [D loss: 0.524171, acc.: 71.88%] [G loss: 2.160357]\n",
      "epoch:2 step:1727 [D loss: 0.494822, acc.: 75.00%] [G loss: 1.669472]\n",
      "epoch:2 step:1728 [D loss: 0.538992, acc.: 74.22%] [G loss: 1.821143]\n",
      "epoch:2 step:1729 [D loss: 0.538585, acc.: 75.00%] [G loss: 1.799194]\n",
      "epoch:2 step:1730 [D loss: 0.489926, acc.: 82.03%] [G loss: 1.389178]\n",
      "epoch:2 step:1731 [D loss: 0.482836, acc.: 82.03%] [G loss: 2.095655]\n",
      "epoch:2 step:1732 [D loss: 0.485334, acc.: 79.69%] [G loss: 2.061711]\n",
      "epoch:2 step:1733 [D loss: 0.520256, acc.: 77.34%] [G loss: 2.057592]\n",
      "epoch:2 step:1734 [D loss: 0.410963, acc.: 85.16%] [G loss: 2.009341]\n",
      "epoch:2 step:1735 [D loss: 0.421195, acc.: 82.03%] [G loss: 1.841951]\n",
      "epoch:2 step:1736 [D loss: 0.466292, acc.: 83.59%] [G loss: 1.924214]\n",
      "epoch:2 step:1737 [D loss: 0.565115, acc.: 72.66%] [G loss: 1.874504]\n",
      "epoch:2 step:1738 [D loss: 0.474231, acc.: 83.59%] [G loss: 1.941700]\n",
      "epoch:2 step:1739 [D loss: 0.558293, acc.: 72.66%] [G loss: 2.089324]\n",
      "epoch:2 step:1740 [D loss: 0.527651, acc.: 67.97%] [G loss: 2.925617]\n",
      "epoch:2 step:1741 [D loss: 0.556092, acc.: 71.09%] [G loss: 1.830043]\n",
      "epoch:2 step:1742 [D loss: 0.495999, acc.: 79.69%] [G loss: 2.541699]\n",
      "epoch:2 step:1743 [D loss: 0.557810, acc.: 69.53%] [G loss: 1.604694]\n",
      "epoch:2 step:1744 [D loss: 0.460111, acc.: 78.12%] [G loss: 1.950183]\n",
      "epoch:2 step:1745 [D loss: 0.480833, acc.: 84.38%] [G loss: 2.306057]\n",
      "epoch:2 step:1746 [D loss: 0.484600, acc.: 82.03%] [G loss: 2.157720]\n",
      "epoch:2 step:1747 [D loss: 0.517201, acc.: 79.69%] [G loss: 1.994390]\n",
      "epoch:2 step:1748 [D loss: 0.517365, acc.: 71.88%] [G loss: 2.105290]\n",
      "epoch:2 step:1749 [D loss: 0.514667, acc.: 73.44%] [G loss: 2.056571]\n",
      "epoch:2 step:1750 [D loss: 0.457556, acc.: 83.59%] [G loss: 1.886459]\n",
      "epoch:2 step:1751 [D loss: 0.461837, acc.: 82.03%] [G loss: 1.969397]\n",
      "epoch:2 step:1752 [D loss: 0.357709, acc.: 89.84%] [G loss: 2.651937]\n",
      "epoch:2 step:1753 [D loss: 0.418609, acc.: 87.50%] [G loss: 1.782507]\n",
      "epoch:2 step:1754 [D loss: 0.450931, acc.: 85.16%] [G loss: 2.253260]\n",
      "epoch:2 step:1755 [D loss: 0.491887, acc.: 82.81%] [G loss: 2.340256]\n",
      "epoch:2 step:1756 [D loss: 0.464680, acc.: 80.47%] [G loss: 2.836730]\n",
      "epoch:2 step:1757 [D loss: 0.491815, acc.: 79.69%] [G loss: 2.166922]\n",
      "epoch:2 step:1758 [D loss: 0.483687, acc.: 81.25%] [G loss: 2.206915]\n",
      "epoch:2 step:1759 [D loss: 0.538088, acc.: 73.44%] [G loss: 1.661699]\n",
      "epoch:2 step:1760 [D loss: 0.399187, acc.: 85.94%] [G loss: 1.842332]\n",
      "epoch:2 step:1761 [D loss: 0.444195, acc.: 80.47%] [G loss: 2.106751]\n",
      "epoch:2 step:1762 [D loss: 0.446563, acc.: 80.47%] [G loss: 2.345132]\n",
      "epoch:2 step:1763 [D loss: 0.455086, acc.: 78.12%] [G loss: 3.147622]\n",
      "epoch:2 step:1764 [D loss: 0.501112, acc.: 79.69%] [G loss: 2.053735]\n",
      "epoch:2 step:1765 [D loss: 0.464613, acc.: 78.91%] [G loss: 1.966300]\n",
      "epoch:2 step:1766 [D loss: 0.428844, acc.: 85.16%] [G loss: 2.249520]\n",
      "epoch:2 step:1767 [D loss: 0.437149, acc.: 82.03%] [G loss: 2.471236]\n",
      "epoch:2 step:1768 [D loss: 0.534156, acc.: 75.00%] [G loss: 1.840969]\n",
      "epoch:2 step:1769 [D loss: 0.471390, acc.: 75.78%] [G loss: 2.401334]\n",
      "epoch:2 step:1770 [D loss: 0.420110, acc.: 83.59%] [G loss: 2.511768]\n",
      "epoch:2 step:1771 [D loss: 0.563042, acc.: 71.09%] [G loss: 2.287063]\n",
      "epoch:2 step:1772 [D loss: 0.477198, acc.: 82.03%] [G loss: 2.370804]\n",
      "epoch:2 step:1773 [D loss: 0.509389, acc.: 80.47%] [G loss: 2.836993]\n",
      "epoch:2 step:1774 [D loss: 0.680280, acc.: 66.41%] [G loss: 1.978340]\n",
      "epoch:2 step:1775 [D loss: 0.621687, acc.: 67.19%] [G loss: 2.425337]\n",
      "epoch:2 step:1776 [D loss: 0.584718, acc.: 69.53%] [G loss: 2.197934]\n",
      "epoch:2 step:1777 [D loss: 0.421575, acc.: 81.25%] [G loss: 2.017413]\n",
      "epoch:2 step:1778 [D loss: 0.406044, acc.: 82.03%] [G loss: 2.668689]\n",
      "epoch:2 step:1779 [D loss: 0.437508, acc.: 83.59%] [G loss: 2.214828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1780 [D loss: 0.534310, acc.: 74.22%] [G loss: 2.410256]\n",
      "epoch:2 step:1781 [D loss: 0.451462, acc.: 82.03%] [G loss: 1.880399]\n",
      "epoch:2 step:1782 [D loss: 0.513070, acc.: 76.56%] [G loss: 2.065747]\n",
      "epoch:2 step:1783 [D loss: 0.428009, acc.: 85.16%] [G loss: 1.618880]\n",
      "epoch:2 step:1784 [D loss: 0.465761, acc.: 81.25%] [G loss: 2.262263]\n",
      "epoch:2 step:1785 [D loss: 0.486848, acc.: 82.03%] [G loss: 2.273780]\n",
      "epoch:2 step:1786 [D loss: 0.430172, acc.: 84.38%] [G loss: 2.221971]\n",
      "epoch:2 step:1787 [D loss: 0.521700, acc.: 76.56%] [G loss: 1.788600]\n",
      "epoch:2 step:1788 [D loss: 0.521261, acc.: 74.22%] [G loss: 2.131999]\n",
      "epoch:2 step:1789 [D loss: 0.420673, acc.: 82.03%] [G loss: 2.468120]\n",
      "epoch:2 step:1790 [D loss: 0.460960, acc.: 82.03%] [G loss: 2.408051]\n",
      "epoch:2 step:1791 [D loss: 0.417458, acc.: 85.94%] [G loss: 1.894454]\n",
      "epoch:2 step:1792 [D loss: 0.379379, acc.: 86.72%] [G loss: 2.399644]\n",
      "epoch:2 step:1793 [D loss: 0.489391, acc.: 81.25%] [G loss: 1.719568]\n",
      "epoch:2 step:1794 [D loss: 0.496605, acc.: 78.91%] [G loss: 2.366134]\n",
      "epoch:2 step:1795 [D loss: 0.479784, acc.: 71.88%] [G loss: 2.259327]\n",
      "epoch:2 step:1796 [D loss: 0.465390, acc.: 75.78%] [G loss: 2.267931]\n",
      "epoch:2 step:1797 [D loss: 0.498262, acc.: 76.56%] [G loss: 2.229016]\n",
      "epoch:2 step:1798 [D loss: 0.460033, acc.: 74.22%] [G loss: 3.281586]\n",
      "epoch:2 step:1799 [D loss: 0.497613, acc.: 75.00%] [G loss: 3.250073]\n",
      "epoch:2 step:1800 [D loss: 0.474186, acc.: 76.56%] [G loss: 2.285383]\n",
      "epoch:2 step:1801 [D loss: 0.526917, acc.: 72.66%] [G loss: 1.778539]\n",
      "epoch:2 step:1802 [D loss: 0.523834, acc.: 76.56%] [G loss: 1.936034]\n",
      "epoch:2 step:1803 [D loss: 0.466566, acc.: 83.59%] [G loss: 1.922640]\n",
      "epoch:2 step:1804 [D loss: 0.487254, acc.: 75.00%] [G loss: 1.927053]\n",
      "epoch:2 step:1805 [D loss: 0.430287, acc.: 87.50%] [G loss: 2.256277]\n",
      "epoch:2 step:1806 [D loss: 0.544287, acc.: 67.97%] [G loss: 2.497499]\n",
      "epoch:2 step:1807 [D loss: 0.639390, acc.: 71.88%] [G loss: 2.252168]\n",
      "epoch:2 step:1808 [D loss: 0.580091, acc.: 72.66%] [G loss: 1.972719]\n",
      "epoch:2 step:1809 [D loss: 0.514889, acc.: 78.12%] [G loss: 2.020348]\n",
      "epoch:2 step:1810 [D loss: 0.410572, acc.: 88.28%] [G loss: 2.286479]\n",
      "epoch:2 step:1811 [D loss: 0.433514, acc.: 79.69%] [G loss: 2.650849]\n",
      "epoch:2 step:1812 [D loss: 0.489481, acc.: 77.34%] [G loss: 2.961316]\n",
      "epoch:2 step:1813 [D loss: 0.509065, acc.: 72.66%] [G loss: 2.714686]\n",
      "epoch:2 step:1814 [D loss: 0.464645, acc.: 80.47%] [G loss: 1.685044]\n",
      "epoch:2 step:1815 [D loss: 0.402169, acc.: 85.16%] [G loss: 1.936011]\n",
      "epoch:2 step:1816 [D loss: 0.397009, acc.: 85.16%] [G loss: 2.774906]\n",
      "epoch:2 step:1817 [D loss: 0.485035, acc.: 79.69%] [G loss: 2.043876]\n",
      "epoch:2 step:1818 [D loss: 0.474921, acc.: 82.03%] [G loss: 2.988422]\n",
      "epoch:2 step:1819 [D loss: 0.391843, acc.: 85.94%] [G loss: 1.885503]\n",
      "epoch:2 step:1820 [D loss: 0.453184, acc.: 79.69%] [G loss: 1.632939]\n",
      "epoch:2 step:1821 [D loss: 0.444595, acc.: 85.94%] [G loss: 2.255743]\n",
      "epoch:2 step:1822 [D loss: 0.409522, acc.: 84.38%] [G loss: 2.828092]\n",
      "epoch:2 step:1823 [D loss: 0.428969, acc.: 82.03%] [G loss: 2.759061]\n",
      "epoch:2 step:1824 [D loss: 0.415789, acc.: 84.38%] [G loss: 2.315024]\n",
      "epoch:2 step:1825 [D loss: 0.452800, acc.: 81.25%] [G loss: 1.864806]\n",
      "epoch:2 step:1826 [D loss: 0.412083, acc.: 87.50%] [G loss: 2.619226]\n",
      "epoch:2 step:1827 [D loss: 0.459426, acc.: 79.69%] [G loss: 1.913592]\n",
      "epoch:2 step:1828 [D loss: 0.465484, acc.: 81.25%] [G loss: 2.188921]\n",
      "epoch:2 step:1829 [D loss: 0.482985, acc.: 77.34%] [G loss: 2.781978]\n",
      "epoch:2 step:1830 [D loss: 0.447606, acc.: 76.56%] [G loss: 3.144200]\n",
      "epoch:2 step:1831 [D loss: 0.569658, acc.: 71.09%] [G loss: 2.698997]\n",
      "epoch:2 step:1832 [D loss: 0.497852, acc.: 74.22%] [G loss: 1.859747]\n",
      "epoch:2 step:1833 [D loss: 0.446340, acc.: 82.03%] [G loss: 2.042229]\n",
      "epoch:2 step:1834 [D loss: 0.416538, acc.: 84.38%] [G loss: 2.551971]\n",
      "epoch:2 step:1835 [D loss: 0.497972, acc.: 75.00%] [G loss: 2.053249]\n",
      "epoch:2 step:1836 [D loss: 0.411871, acc.: 89.06%] [G loss: 2.174847]\n",
      "epoch:2 step:1837 [D loss: 0.523355, acc.: 70.31%] [G loss: 2.299515]\n",
      "epoch:2 step:1838 [D loss: 0.505443, acc.: 76.56%] [G loss: 2.524782]\n",
      "epoch:2 step:1839 [D loss: 0.636506, acc.: 60.94%] [G loss: 1.753215]\n",
      "epoch:2 step:1840 [D loss: 0.473206, acc.: 83.59%] [G loss: 1.880283]\n",
      "epoch:2 step:1841 [D loss: 0.461067, acc.: 85.94%] [G loss: 2.169889]\n",
      "epoch:2 step:1842 [D loss: 0.479991, acc.: 71.88%] [G loss: 2.762923]\n",
      "epoch:2 step:1843 [D loss: 0.426220, acc.: 87.50%] [G loss: 1.885348]\n",
      "epoch:2 step:1844 [D loss: 0.441579, acc.: 83.59%] [G loss: 2.382467]\n",
      "epoch:2 step:1845 [D loss: 0.454153, acc.: 80.47%] [G loss: 2.458611]\n",
      "epoch:2 step:1846 [D loss: 0.436472, acc.: 84.38%] [G loss: 2.180916]\n",
      "epoch:2 step:1847 [D loss: 0.469114, acc.: 75.78%] [G loss: 3.296225]\n",
      "epoch:2 step:1848 [D loss: 0.526602, acc.: 75.78%] [G loss: 3.094329]\n",
      "epoch:2 step:1849 [D loss: 0.453186, acc.: 79.69%] [G loss: 2.074653]\n",
      "epoch:2 step:1850 [D loss: 0.449656, acc.: 83.59%] [G loss: 2.321706]\n",
      "epoch:2 step:1851 [D loss: 0.476281, acc.: 79.69%] [G loss: 3.047460]\n",
      "epoch:2 step:1852 [D loss: 0.421111, acc.: 79.69%] [G loss: 2.594474]\n",
      "epoch:2 step:1853 [D loss: 0.382955, acc.: 90.62%] [G loss: 2.348361]\n",
      "epoch:2 step:1854 [D loss: 0.428503, acc.: 82.03%] [G loss: 1.892313]\n",
      "epoch:2 step:1855 [D loss: 0.451252, acc.: 78.91%] [G loss: 1.923263]\n",
      "epoch:2 step:1856 [D loss: 0.417009, acc.: 83.59%] [G loss: 2.546590]\n",
      "epoch:2 step:1857 [D loss: 0.560470, acc.: 69.53%] [G loss: 2.801477]\n",
      "epoch:2 step:1858 [D loss: 0.506667, acc.: 79.69%] [G loss: 3.168907]\n",
      "epoch:2 step:1859 [D loss: 0.716615, acc.: 68.75%] [G loss: 2.917446]\n",
      "epoch:2 step:1860 [D loss: 0.680564, acc.: 67.19%] [G loss: 2.943098]\n",
      "epoch:2 step:1861 [D loss: 0.608601, acc.: 70.31%] [G loss: 1.867786]\n",
      "epoch:2 step:1862 [D loss: 0.463039, acc.: 78.91%] [G loss: 2.708527]\n",
      "epoch:2 step:1863 [D loss: 0.429748, acc.: 82.03%] [G loss: 2.121941]\n",
      "epoch:2 step:1864 [D loss: 0.423125, acc.: 86.72%] [G loss: 1.849407]\n",
      "epoch:2 step:1865 [D loss: 0.458083, acc.: 80.47%] [G loss: 1.799045]\n",
      "epoch:2 step:1866 [D loss: 0.490335, acc.: 78.91%] [G loss: 2.071993]\n",
      "epoch:2 step:1867 [D loss: 0.469744, acc.: 75.00%] [G loss: 2.519275]\n",
      "epoch:2 step:1868 [D loss: 0.496358, acc.: 82.03%] [G loss: 2.148036]\n",
      "epoch:2 step:1869 [D loss: 0.355375, acc.: 90.62%] [G loss: 2.337548]\n",
      "epoch:2 step:1870 [D loss: 0.409184, acc.: 82.03%] [G loss: 2.487945]\n",
      "epoch:2 step:1871 [D loss: 0.392068, acc.: 87.50%] [G loss: 2.311897]\n",
      "epoch:2 step:1872 [D loss: 0.444623, acc.: 79.69%] [G loss: 2.293742]\n",
      "epoch:2 step:1873 [D loss: 0.355585, acc.: 89.84%] [G loss: 2.191762]\n",
      "epoch:2 step:1874 [D loss: 0.490349, acc.: 73.44%] [G loss: 1.811759]\n",
      "epoch:2 step:1875 [D loss: 0.453940, acc.: 82.81%] [G loss: 2.239353]\n",
      "epoch:2 step:1876 [D loss: 0.502330, acc.: 75.00%] [G loss: 1.866659]\n",
      "epoch:2 step:1877 [D loss: 0.387256, acc.: 83.59%] [G loss: 2.369973]\n",
      "epoch:2 step:1878 [D loss: 0.510542, acc.: 75.78%] [G loss: 2.217377]\n",
      "epoch:2 step:1879 [D loss: 0.510300, acc.: 75.00%] [G loss: 5.059075]\n",
      "epoch:2 step:1880 [D loss: 0.754455, acc.: 65.62%] [G loss: 2.622300]\n",
      "epoch:2 step:1881 [D loss: 0.578297, acc.: 64.84%] [G loss: 1.446984]\n",
      "epoch:2 step:1882 [D loss: 0.485654, acc.: 78.12%] [G loss: 2.711487]\n",
      "epoch:2 step:1883 [D loss: 0.492612, acc.: 78.12%] [G loss: 2.499648]\n",
      "epoch:2 step:1884 [D loss: 0.391372, acc.: 86.72%] [G loss: 2.897346]\n",
      "epoch:2 step:1885 [D loss: 0.379071, acc.: 89.06%] [G loss: 2.538102]\n",
      "epoch:2 step:1886 [D loss: 0.458978, acc.: 81.25%] [G loss: 2.224577]\n",
      "epoch:2 step:1887 [D loss: 0.431837, acc.: 81.25%] [G loss: 2.266929]\n",
      "epoch:2 step:1888 [D loss: 0.423207, acc.: 82.03%] [G loss: 2.116300]\n",
      "epoch:2 step:1889 [D loss: 0.427263, acc.: 85.16%] [G loss: 1.953153]\n",
      "epoch:2 step:1890 [D loss: 0.436676, acc.: 83.59%] [G loss: 1.830705]\n",
      "epoch:2 step:1891 [D loss: 0.480638, acc.: 75.78%] [G loss: 2.542573]\n",
      "epoch:2 step:1892 [D loss: 0.449261, acc.: 84.38%] [G loss: 2.166519]\n",
      "epoch:2 step:1893 [D loss: 0.476934, acc.: 82.81%] [G loss: 2.121117]\n",
      "epoch:2 step:1894 [D loss: 0.481486, acc.: 78.91%] [G loss: 2.495302]\n",
      "epoch:2 step:1895 [D loss: 0.461264, acc.: 84.38%] [G loss: 2.063239]\n",
      "epoch:2 step:1896 [D loss: 0.453762, acc.: 84.38%] [G loss: 2.182949]\n",
      "epoch:2 step:1897 [D loss: 0.427976, acc.: 78.91%] [G loss: 2.385769]\n",
      "epoch:2 step:1898 [D loss: 0.379903, acc.: 88.28%] [G loss: 1.924057]\n",
      "epoch:2 step:1899 [D loss: 0.445199, acc.: 78.12%] [G loss: 1.963067]\n",
      "epoch:2 step:1900 [D loss: 0.497693, acc.: 74.22%] [G loss: 2.385720]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1901 [D loss: 0.476098, acc.: 79.69%] [G loss: 2.230716]\n",
      "epoch:2 step:1902 [D loss: 0.415576, acc.: 80.47%] [G loss: 2.227322]\n",
      "epoch:2 step:1903 [D loss: 0.449550, acc.: 80.47%] [G loss: 2.394097]\n",
      "epoch:2 step:1904 [D loss: 0.485100, acc.: 76.56%] [G loss: 2.231775]\n",
      "epoch:2 step:1905 [D loss: 0.487640, acc.: 77.34%] [G loss: 3.005234]\n",
      "epoch:2 step:1906 [D loss: 0.539195, acc.: 70.31%] [G loss: 2.153515]\n",
      "epoch:2 step:1907 [D loss: 0.465815, acc.: 78.91%] [G loss: 2.729286]\n",
      "epoch:2 step:1908 [D loss: 0.446452, acc.: 78.12%] [G loss: 3.468869]\n",
      "epoch:2 step:1909 [D loss: 0.465833, acc.: 78.91%] [G loss: 2.220377]\n",
      "epoch:2 step:1910 [D loss: 0.388164, acc.: 87.50%] [G loss: 3.584426]\n",
      "epoch:2 step:1911 [D loss: 0.613564, acc.: 64.06%] [G loss: 3.408959]\n",
      "epoch:2 step:1912 [D loss: 0.515423, acc.: 78.12%] [G loss: 2.077451]\n",
      "epoch:2 step:1913 [D loss: 0.507598, acc.: 78.12%] [G loss: 4.529760]\n",
      "epoch:2 step:1914 [D loss: 0.745904, acc.: 63.28%] [G loss: 3.290864]\n",
      "epoch:2 step:1915 [D loss: 0.526892, acc.: 73.44%] [G loss: 2.579018]\n",
      "epoch:2 step:1916 [D loss: 0.454683, acc.: 78.91%] [G loss: 3.229170]\n",
      "epoch:2 step:1917 [D loss: 0.369855, acc.: 89.06%] [G loss: 2.856495]\n",
      "epoch:2 step:1918 [D loss: 0.404673, acc.: 83.59%] [G loss: 2.426610]\n",
      "epoch:2 step:1919 [D loss: 0.446373, acc.: 79.69%] [G loss: 2.119606]\n",
      "epoch:2 step:1920 [D loss: 0.390543, acc.: 83.59%] [G loss: 2.466089]\n",
      "epoch:2 step:1921 [D loss: 0.465680, acc.: 82.03%] [G loss: 2.268349]\n",
      "epoch:2 step:1922 [D loss: 0.400005, acc.: 83.59%] [G loss: 3.416636]\n",
      "epoch:2 step:1923 [D loss: 0.406841, acc.: 85.94%] [G loss: 2.304236]\n",
      "epoch:2 step:1924 [D loss: 0.416187, acc.: 83.59%] [G loss: 2.087046]\n",
      "epoch:2 step:1925 [D loss: 0.415144, acc.: 84.38%] [G loss: 2.911959]\n",
      "epoch:2 step:1926 [D loss: 0.394161, acc.: 85.16%] [G loss: 2.363744]\n",
      "epoch:2 step:1927 [D loss: 0.409368, acc.: 78.91%] [G loss: 1.885140]\n",
      "epoch:2 step:1928 [D loss: 0.408538, acc.: 85.94%] [G loss: 1.935325]\n",
      "epoch:2 step:1929 [D loss: 0.335120, acc.: 89.84%] [G loss: 2.776370]\n",
      "epoch:2 step:1930 [D loss: 0.353639, acc.: 92.19%] [G loss: 2.486574]\n",
      "epoch:2 step:1931 [D loss: 0.394657, acc.: 87.50%] [G loss: 2.594300]\n",
      "epoch:2 step:1932 [D loss: 0.373659, acc.: 85.16%] [G loss: 3.038444]\n",
      "epoch:2 step:1933 [D loss: 0.465461, acc.: 79.69%] [G loss: 2.735169]\n",
      "epoch:2 step:1934 [D loss: 0.442264, acc.: 79.69%] [G loss: 2.311996]\n",
      "epoch:2 step:1935 [D loss: 0.525997, acc.: 75.00%] [G loss: 2.891689]\n",
      "epoch:2 step:1936 [D loss: 0.428114, acc.: 79.69%] [G loss: 2.996624]\n",
      "epoch:2 step:1937 [D loss: 0.521857, acc.: 78.12%] [G loss: 2.564226]\n",
      "epoch:2 step:1938 [D loss: 0.455244, acc.: 76.56%] [G loss: 3.329407]\n",
      "epoch:2 step:1939 [D loss: 0.404034, acc.: 84.38%] [G loss: 2.515101]\n",
      "epoch:2 step:1940 [D loss: 0.431435, acc.: 82.81%] [G loss: 3.423253]\n",
      "epoch:2 step:1941 [D loss: 0.520514, acc.: 71.09%] [G loss: 3.794128]\n",
      "epoch:2 step:1942 [D loss: 0.510451, acc.: 78.12%] [G loss: 2.108081]\n",
      "epoch:2 step:1943 [D loss: 0.434330, acc.: 78.91%] [G loss: 2.763854]\n",
      "epoch:2 step:1944 [D loss: 0.474584, acc.: 77.34%] [G loss: 2.618789]\n",
      "epoch:2 step:1945 [D loss: 0.501647, acc.: 74.22%] [G loss: 3.136208]\n",
      "epoch:2 step:1946 [D loss: 0.406077, acc.: 87.50%] [G loss: 2.330264]\n",
      "epoch:2 step:1947 [D loss: 0.408460, acc.: 89.06%] [G loss: 2.469259]\n",
      "epoch:2 step:1948 [D loss: 0.407531, acc.: 80.47%] [G loss: 2.119791]\n",
      "epoch:2 step:1949 [D loss: 0.446387, acc.: 79.69%] [G loss: 2.330699]\n",
      "epoch:2 step:1950 [D loss: 0.650060, acc.: 66.41%] [G loss: 1.974537]\n",
      "epoch:2 step:1951 [D loss: 0.476931, acc.: 77.34%] [G loss: 2.458973]\n",
      "epoch:2 step:1952 [D loss: 0.459559, acc.: 79.69%] [G loss: 2.138759]\n",
      "epoch:2 step:1953 [D loss: 0.554022, acc.: 66.41%] [G loss: 2.197744]\n",
      "epoch:2 step:1954 [D loss: 0.482112, acc.: 79.69%] [G loss: 2.510554]\n",
      "epoch:2 step:1955 [D loss: 0.398521, acc.: 87.50%] [G loss: 1.965547]\n",
      "epoch:2 step:1956 [D loss: 0.449148, acc.: 78.91%] [G loss: 1.965953]\n",
      "epoch:2 step:1957 [D loss: 0.457900, acc.: 80.47%] [G loss: 1.790175]\n",
      "epoch:2 step:1958 [D loss: 0.400056, acc.: 87.50%] [G loss: 2.883616]\n",
      "epoch:2 step:1959 [D loss: 0.419245, acc.: 84.38%] [G loss: 2.439582]\n",
      "epoch:2 step:1960 [D loss: 0.441236, acc.: 85.94%] [G loss: 2.197089]\n",
      "epoch:2 step:1961 [D loss: 0.476535, acc.: 74.22%] [G loss: 2.018944]\n",
      "epoch:2 step:1962 [D loss: 0.398233, acc.: 85.16%] [G loss: 3.097254]\n",
      "epoch:2 step:1963 [D loss: 0.483026, acc.: 72.66%] [G loss: 3.809050]\n",
      "epoch:2 step:1964 [D loss: 0.482066, acc.: 78.12%] [G loss: 2.230020]\n",
      "epoch:2 step:1965 [D loss: 0.369998, acc.: 86.72%] [G loss: 2.504272]\n",
      "epoch:2 step:1966 [D loss: 0.429868, acc.: 87.50%] [G loss: 1.947654]\n",
      "epoch:2 step:1967 [D loss: 0.408741, acc.: 85.16%] [G loss: 2.681815]\n",
      "epoch:2 step:1968 [D loss: 0.545651, acc.: 69.53%] [G loss: 1.576408]\n",
      "epoch:2 step:1969 [D loss: 0.444225, acc.: 83.59%] [G loss: 2.099236]\n",
      "epoch:2 step:1970 [D loss: 0.621534, acc.: 73.44%] [G loss: 1.959988]\n",
      "epoch:2 step:1971 [D loss: 0.463289, acc.: 75.78%] [G loss: 2.945040]\n",
      "epoch:2 step:1972 [D loss: 0.559917, acc.: 71.09%] [G loss: 3.952123]\n",
      "epoch:2 step:1973 [D loss: 0.767089, acc.: 60.16%] [G loss: 5.218655]\n",
      "epoch:2 step:1974 [D loss: 0.810813, acc.: 60.94%] [G loss: 2.257113]\n",
      "epoch:2 step:1975 [D loss: 0.336792, acc.: 88.28%] [G loss: 2.630402]\n",
      "epoch:2 step:1976 [D loss: 0.643266, acc.: 59.38%] [G loss: 1.957694]\n",
      "epoch:2 step:1977 [D loss: 0.416936, acc.: 84.38%] [G loss: 2.887081]\n",
      "epoch:2 step:1978 [D loss: 0.455575, acc.: 79.69%] [G loss: 2.815536]\n",
      "epoch:2 step:1979 [D loss: 0.420518, acc.: 82.81%] [G loss: 2.203710]\n",
      "epoch:2 step:1980 [D loss: 0.440895, acc.: 78.91%] [G loss: 2.046605]\n",
      "epoch:2 step:1981 [D loss: 0.407004, acc.: 86.72%] [G loss: 2.179576]\n",
      "epoch:2 step:1982 [D loss: 0.460797, acc.: 81.25%] [G loss: 1.824814]\n",
      "epoch:2 step:1983 [D loss: 0.402512, acc.: 85.94%] [G loss: 2.356431]\n",
      "epoch:2 step:1984 [D loss: 0.402994, acc.: 82.81%] [G loss: 2.244472]\n",
      "epoch:2 step:1985 [D loss: 0.348040, acc.: 89.06%] [G loss: 2.561832]\n",
      "epoch:2 step:1986 [D loss: 0.413976, acc.: 83.59%] [G loss: 2.195868]\n",
      "epoch:2 step:1987 [D loss: 0.398583, acc.: 85.94%] [G loss: 1.836623]\n",
      "epoch:2 step:1988 [D loss: 0.412547, acc.: 88.28%] [G loss: 2.264540]\n",
      "epoch:2 step:1989 [D loss: 0.414238, acc.: 86.72%] [G loss: 2.475090]\n",
      "epoch:2 step:1990 [D loss: 0.414385, acc.: 87.50%] [G loss: 2.127239]\n",
      "epoch:2 step:1991 [D loss: 0.495084, acc.: 76.56%] [G loss: 2.606820]\n",
      "epoch:2 step:1992 [D loss: 0.576877, acc.: 67.97%] [G loss: 2.053817]\n",
      "epoch:2 step:1993 [D loss: 0.469985, acc.: 78.91%] [G loss: 2.277171]\n",
      "epoch:2 step:1994 [D loss: 0.397474, acc.: 85.94%] [G loss: 2.431708]\n",
      "epoch:2 step:1995 [D loss: 0.451175, acc.: 79.69%] [G loss: 2.346603]\n",
      "epoch:2 step:1996 [D loss: 0.362320, acc.: 87.50%] [G loss: 2.051830]\n",
      "epoch:2 step:1997 [D loss: 0.439104, acc.: 78.91%] [G loss: 1.940984]\n",
      "epoch:2 step:1998 [D loss: 0.465885, acc.: 82.81%] [G loss: 2.163986]\n",
      "epoch:2 step:1999 [D loss: 0.427919, acc.: 80.47%] [G loss: 2.540960]\n",
      "epoch:2 step:2000 [D loss: 0.488756, acc.: 78.12%] [G loss: 1.871530]\n",
      "epoch:2 step:2001 [D loss: 0.421680, acc.: 85.16%] [G loss: 2.201320]\n",
      "epoch:2 step:2002 [D loss: 0.450696, acc.: 81.25%] [G loss: 2.152038]\n",
      "epoch:2 step:2003 [D loss: 0.461105, acc.: 82.03%] [G loss: 2.031831]\n",
      "epoch:2 step:2004 [D loss: 0.392215, acc.: 84.38%] [G loss: 2.872537]\n",
      "epoch:2 step:2005 [D loss: 0.539105, acc.: 71.88%] [G loss: 2.197407]\n",
      "epoch:2 step:2006 [D loss: 0.442158, acc.: 78.91%] [G loss: 2.069710]\n",
      "epoch:2 step:2007 [D loss: 0.383121, acc.: 80.47%] [G loss: 2.339286]\n",
      "epoch:2 step:2008 [D loss: 0.351441, acc.: 89.84%] [G loss: 2.373726]\n",
      "epoch:2 step:2009 [D loss: 0.430590, acc.: 85.94%] [G loss: 1.592527]\n",
      "epoch:2 step:2010 [D loss: 0.422309, acc.: 82.03%] [G loss: 1.896591]\n",
      "epoch:2 step:2011 [D loss: 0.471763, acc.: 74.22%] [G loss: 2.125005]\n",
      "epoch:2 step:2012 [D loss: 0.464183, acc.: 77.34%] [G loss: 2.715757]\n",
      "epoch:2 step:2013 [D loss: 0.484656, acc.: 75.00%] [G loss: 2.145988]\n",
      "epoch:2 step:2014 [D loss: 0.400403, acc.: 83.59%] [G loss: 2.865435]\n",
      "epoch:2 step:2015 [D loss: 0.403181, acc.: 84.38%] [G loss: 2.613089]\n",
      "epoch:2 step:2016 [D loss: 0.410599, acc.: 85.94%] [G loss: 2.199625]\n",
      "epoch:2 step:2017 [D loss: 0.368751, acc.: 85.16%] [G loss: 3.096361]\n",
      "epoch:2 step:2018 [D loss: 0.405661, acc.: 81.25%] [G loss: 3.165353]\n",
      "epoch:2 step:2019 [D loss: 0.653607, acc.: 68.75%] [G loss: 3.995373]\n",
      "epoch:2 step:2020 [D loss: 0.805704, acc.: 61.72%] [G loss: 4.004171]\n",
      "epoch:2 step:2021 [D loss: 0.678132, acc.: 67.19%] [G loss: 2.947950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2022 [D loss: 0.441760, acc.: 76.56%] [G loss: 3.874343]\n",
      "epoch:2 step:2023 [D loss: 0.460565, acc.: 78.91%] [G loss: 3.242043]\n",
      "epoch:2 step:2024 [D loss: 0.366247, acc.: 87.50%] [G loss: 2.607251]\n",
      "epoch:2 step:2025 [D loss: 0.487797, acc.: 73.44%] [G loss: 2.435375]\n",
      "epoch:2 step:2026 [D loss: 0.377142, acc.: 82.81%] [G loss: 2.768145]\n",
      "epoch:2 step:2027 [D loss: 0.416689, acc.: 78.12%] [G loss: 2.107718]\n",
      "epoch:2 step:2028 [D loss: 0.415219, acc.: 79.69%] [G loss: 1.912509]\n",
      "epoch:2 step:2029 [D loss: 0.412155, acc.: 84.38%] [G loss: 2.354862]\n",
      "epoch:2 step:2030 [D loss: 0.441193, acc.: 80.47%] [G loss: 2.087937]\n",
      "epoch:2 step:2031 [D loss: 0.471843, acc.: 82.81%] [G loss: 2.266030]\n",
      "epoch:2 step:2032 [D loss: 0.561478, acc.: 75.00%] [G loss: 2.343885]\n",
      "epoch:2 step:2033 [D loss: 0.582481, acc.: 75.78%] [G loss: 2.279563]\n",
      "epoch:2 step:2034 [D loss: 0.524121, acc.: 78.12%] [G loss: 2.771155]\n",
      "epoch:2 step:2035 [D loss: 0.466861, acc.: 81.25%] [G loss: 1.732853]\n",
      "epoch:2 step:2036 [D loss: 0.479614, acc.: 78.12%] [G loss: 2.572959]\n",
      "epoch:2 step:2037 [D loss: 0.471533, acc.: 78.91%] [G loss: 2.447917]\n",
      "epoch:2 step:2038 [D loss: 0.467949, acc.: 78.91%] [G loss: 2.629193]\n",
      "epoch:2 step:2039 [D loss: 0.410738, acc.: 81.25%] [G loss: 2.443111]\n",
      "epoch:2 step:2040 [D loss: 0.381027, acc.: 87.50%] [G loss: 2.425420]\n",
      "epoch:2 step:2041 [D loss: 0.435484, acc.: 80.47%] [G loss: 1.994951]\n",
      "epoch:2 step:2042 [D loss: 0.492906, acc.: 76.56%] [G loss: 2.371237]\n",
      "epoch:2 step:2043 [D loss: 0.537507, acc.: 76.56%] [G loss: 1.926643]\n",
      "epoch:2 step:2044 [D loss: 0.403087, acc.: 82.81%] [G loss: 3.209003]\n",
      "epoch:2 step:2045 [D loss: 0.375660, acc.: 86.72%] [G loss: 2.956525]\n",
      "epoch:2 step:2046 [D loss: 0.455123, acc.: 78.91%] [G loss: 2.085623]\n",
      "epoch:2 step:2047 [D loss: 0.381336, acc.: 81.25%] [G loss: 2.367332]\n",
      "epoch:2 step:2048 [D loss: 0.431236, acc.: 83.59%] [G loss: 2.097659]\n",
      "epoch:2 step:2049 [D loss: 0.442566, acc.: 85.16%] [G loss: 2.218201]\n",
      "epoch:2 step:2050 [D loss: 0.361600, acc.: 89.84%] [G loss: 1.850855]\n",
      "epoch:2 step:2051 [D loss: 0.455145, acc.: 75.00%] [G loss: 2.071605]\n",
      "epoch:2 step:2052 [D loss: 0.440001, acc.: 85.16%] [G loss: 2.270518]\n",
      "epoch:2 step:2053 [D loss: 0.391787, acc.: 85.94%] [G loss: 2.831930]\n",
      "epoch:2 step:2054 [D loss: 0.331541, acc.: 93.75%] [G loss: 2.544427]\n",
      "epoch:2 step:2055 [D loss: 0.463349, acc.: 75.00%] [G loss: 2.821874]\n",
      "epoch:2 step:2056 [D loss: 0.421497, acc.: 82.03%] [G loss: 3.633634]\n",
      "epoch:2 step:2057 [D loss: 0.550349, acc.: 71.88%] [G loss: 2.244466]\n",
      "epoch:2 step:2058 [D loss: 0.446494, acc.: 80.47%] [G loss: 2.739582]\n",
      "epoch:2 step:2059 [D loss: 0.405765, acc.: 82.81%] [G loss: 2.338092]\n",
      "epoch:2 step:2060 [D loss: 0.388625, acc.: 85.16%] [G loss: 2.188917]\n",
      "epoch:2 step:2061 [D loss: 0.454221, acc.: 80.47%] [G loss: 2.379074]\n",
      "epoch:2 step:2062 [D loss: 0.453329, acc.: 79.69%] [G loss: 2.605561]\n",
      "epoch:2 step:2063 [D loss: 0.429844, acc.: 77.34%] [G loss: 2.962617]\n",
      "epoch:2 step:2064 [D loss: 0.457805, acc.: 80.47%] [G loss: 2.208987]\n",
      "epoch:2 step:2065 [D loss: 0.444355, acc.: 83.59%] [G loss: 1.824928]\n",
      "epoch:2 step:2066 [D loss: 0.442516, acc.: 80.47%] [G loss: 2.399228]\n",
      "epoch:2 step:2067 [D loss: 0.423419, acc.: 83.59%] [G loss: 2.351705]\n",
      "epoch:2 step:2068 [D loss: 0.417525, acc.: 85.94%] [G loss: 2.949334]\n",
      "epoch:2 step:2069 [D loss: 0.579858, acc.: 70.31%] [G loss: 2.453869]\n",
      "epoch:2 step:2070 [D loss: 0.546990, acc.: 76.56%] [G loss: 2.005432]\n",
      "epoch:2 step:2071 [D loss: 0.498003, acc.: 82.81%] [G loss: 2.004970]\n",
      "epoch:2 step:2072 [D loss: 0.428260, acc.: 82.81%] [G loss: 2.123429]\n",
      "epoch:2 step:2073 [D loss: 0.470424, acc.: 80.47%] [G loss: 2.006601]\n",
      "epoch:2 step:2074 [D loss: 0.447227, acc.: 79.69%] [G loss: 2.411492]\n",
      "epoch:2 step:2075 [D loss: 0.421007, acc.: 84.38%] [G loss: 2.931054]\n",
      "epoch:2 step:2076 [D loss: 0.480742, acc.: 82.03%] [G loss: 2.524926]\n",
      "epoch:2 step:2077 [D loss: 0.358449, acc.: 85.94%] [G loss: 3.174876]\n",
      "epoch:2 step:2078 [D loss: 0.319553, acc.: 89.84%] [G loss: 2.827622]\n",
      "epoch:2 step:2079 [D loss: 0.312462, acc.: 92.19%] [G loss: 2.933722]\n",
      "epoch:2 step:2080 [D loss: 0.402909, acc.: 81.25%] [G loss: 2.219695]\n",
      "epoch:2 step:2081 [D loss: 0.442235, acc.: 80.47%] [G loss: 1.960852]\n",
      "epoch:2 step:2082 [D loss: 0.396689, acc.: 82.81%] [G loss: 3.028552]\n",
      "epoch:2 step:2083 [D loss: 0.402665, acc.: 82.03%] [G loss: 2.232124]\n",
      "epoch:2 step:2084 [D loss: 0.402327, acc.: 84.38%] [G loss: 1.988340]\n",
      "epoch:2 step:2085 [D loss: 0.409726, acc.: 86.72%] [G loss: 2.131351]\n",
      "epoch:2 step:2086 [D loss: 0.450321, acc.: 81.25%] [G loss: 2.401268]\n",
      "epoch:2 step:2087 [D loss: 0.319311, acc.: 91.41%] [G loss: 3.377452]\n",
      "epoch:2 step:2088 [D loss: 0.455274, acc.: 78.91%] [G loss: 2.537683]\n",
      "epoch:2 step:2089 [D loss: 0.404782, acc.: 84.38%] [G loss: 2.776697]\n",
      "epoch:2 step:2090 [D loss: 0.468476, acc.: 74.22%] [G loss: 2.787680]\n",
      "epoch:2 step:2091 [D loss: 0.497703, acc.: 72.66%] [G loss: 2.818581]\n",
      "epoch:2 step:2092 [D loss: 0.622998, acc.: 69.53%] [G loss: 3.074626]\n",
      "epoch:2 step:2093 [D loss: 0.587347, acc.: 71.88%] [G loss: 4.335855]\n",
      "epoch:2 step:2094 [D loss: 0.809380, acc.: 55.47%] [G loss: 1.790537]\n",
      "epoch:2 step:2095 [D loss: 0.463083, acc.: 75.78%] [G loss: 2.684754]\n",
      "epoch:2 step:2096 [D loss: 0.458324, acc.: 77.34%] [G loss: 2.734610]\n",
      "epoch:2 step:2097 [D loss: 0.436767, acc.: 80.47%] [G loss: 2.508567]\n",
      "epoch:2 step:2098 [D loss: 0.382155, acc.: 86.72%] [G loss: 2.761739]\n",
      "epoch:2 step:2099 [D loss: 0.446792, acc.: 75.78%] [G loss: 2.292785]\n",
      "epoch:2 step:2100 [D loss: 0.403267, acc.: 85.94%] [G loss: 3.114223]\n",
      "epoch:2 step:2101 [D loss: 0.553237, acc.: 72.66%] [G loss: 2.379531]\n",
      "epoch:2 step:2102 [D loss: 0.459205, acc.: 79.69%] [G loss: 2.946318]\n",
      "epoch:2 step:2103 [D loss: 0.386220, acc.: 89.84%] [G loss: 2.373120]\n",
      "epoch:2 step:2104 [D loss: 0.493953, acc.: 78.12%] [G loss: 2.403795]\n",
      "epoch:2 step:2105 [D loss: 0.492666, acc.: 75.78%] [G loss: 2.408050]\n",
      "epoch:2 step:2106 [D loss: 0.464783, acc.: 78.12%] [G loss: 2.261409]\n",
      "epoch:2 step:2107 [D loss: 0.498501, acc.: 72.66%] [G loss: 2.771319]\n",
      "epoch:2 step:2108 [D loss: 0.391500, acc.: 82.81%] [G loss: 2.989567]\n",
      "epoch:2 step:2109 [D loss: 0.391819, acc.: 83.59%] [G loss: 2.658155]\n",
      "epoch:2 step:2110 [D loss: 0.412009, acc.: 82.81%] [G loss: 2.284191]\n",
      "epoch:2 step:2111 [D loss: 0.323048, acc.: 90.62%] [G loss: 2.106843]\n",
      "epoch:2 step:2112 [D loss: 0.381400, acc.: 82.81%] [G loss: 2.891009]\n",
      "epoch:2 step:2113 [D loss: 0.358368, acc.: 89.84%] [G loss: 3.136547]\n",
      "epoch:2 step:2114 [D loss: 0.463351, acc.: 76.56%] [G loss: 2.135410]\n",
      "epoch:2 step:2115 [D loss: 0.411324, acc.: 82.03%] [G loss: 2.406994]\n",
      "epoch:2 step:2116 [D loss: 0.506481, acc.: 72.66%] [G loss: 2.839165]\n",
      "epoch:2 step:2117 [D loss: 0.509689, acc.: 73.44%] [G loss: 3.217577]\n",
      "epoch:2 step:2118 [D loss: 0.495324, acc.: 80.47%] [G loss: 2.986568]\n",
      "epoch:2 step:2119 [D loss: 0.467393, acc.: 77.34%] [G loss: 3.042243]\n",
      "epoch:2 step:2120 [D loss: 0.431518, acc.: 79.69%] [G loss: 2.554285]\n",
      "epoch:2 step:2121 [D loss: 0.380614, acc.: 86.72%] [G loss: 3.032813]\n",
      "epoch:2 step:2122 [D loss: 0.423090, acc.: 79.69%] [G loss: 2.300535]\n",
      "epoch:2 step:2123 [D loss: 0.415179, acc.: 82.03%] [G loss: 2.686476]\n",
      "epoch:2 step:2124 [D loss: 0.451568, acc.: 79.69%] [G loss: 2.014853]\n",
      "epoch:2 step:2125 [D loss: 0.605012, acc.: 71.09%] [G loss: 2.608717]\n",
      "epoch:2 step:2126 [D loss: 0.527363, acc.: 70.31%] [G loss: 3.835301]\n",
      "epoch:2 step:2127 [D loss: 0.441972, acc.: 75.78%] [G loss: 1.997789]\n",
      "epoch:2 step:2128 [D loss: 0.410521, acc.: 82.03%] [G loss: 2.930309]\n",
      "epoch:2 step:2129 [D loss: 0.526138, acc.: 72.66%] [G loss: 2.769116]\n",
      "epoch:2 step:2130 [D loss: 0.412723, acc.: 85.94%] [G loss: 2.084544]\n",
      "epoch:2 step:2131 [D loss: 0.468316, acc.: 78.91%] [G loss: 2.573280]\n",
      "epoch:2 step:2132 [D loss: 0.436603, acc.: 83.59%] [G loss: 2.270967]\n",
      "epoch:2 step:2133 [D loss: 0.462844, acc.: 80.47%] [G loss: 2.316881]\n",
      "epoch:2 step:2134 [D loss: 0.327019, acc.: 90.62%] [G loss: 2.393224]\n",
      "epoch:2 step:2135 [D loss: 0.400360, acc.: 80.47%] [G loss: 3.139295]\n",
      "epoch:2 step:2136 [D loss: 0.404586, acc.: 81.25%] [G loss: 2.392897]\n",
      "epoch:2 step:2137 [D loss: 0.413919, acc.: 84.38%] [G loss: 2.169116]\n",
      "epoch:2 step:2138 [D loss: 0.375741, acc.: 85.94%] [G loss: 2.798079]\n",
      "epoch:2 step:2139 [D loss: 0.364574, acc.: 83.59%] [G loss: 2.693886]\n",
      "epoch:2 step:2140 [D loss: 0.448578, acc.: 73.44%] [G loss: 2.643694]\n",
      "epoch:2 step:2141 [D loss: 0.402149, acc.: 83.59%] [G loss: 2.904312]\n",
      "epoch:2 step:2142 [D loss: 0.422083, acc.: 85.16%] [G loss: 2.820802]\n",
      "epoch:2 step:2143 [D loss: 0.417832, acc.: 82.81%] [G loss: 2.196115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2144 [D loss: 0.413887, acc.: 79.69%] [G loss: 3.613587]\n",
      "epoch:2 step:2145 [D loss: 0.443608, acc.: 77.34%] [G loss: 4.140185]\n",
      "epoch:2 step:2146 [D loss: 0.525406, acc.: 71.88%] [G loss: 2.912010]\n",
      "epoch:2 step:2147 [D loss: 0.298366, acc.: 92.19%] [G loss: 2.555339]\n",
      "epoch:2 step:2148 [D loss: 0.401291, acc.: 79.69%] [G loss: 2.752276]\n",
      "epoch:2 step:2149 [D loss: 0.369757, acc.: 84.38%] [G loss: 3.436916]\n",
      "epoch:2 step:2150 [D loss: 0.372284, acc.: 83.59%] [G loss: 2.825652]\n",
      "epoch:2 step:2151 [D loss: 0.389505, acc.: 87.50%] [G loss: 2.198534]\n",
      "epoch:2 step:2152 [D loss: 0.400202, acc.: 80.47%] [G loss: 2.712065]\n",
      "epoch:2 step:2153 [D loss: 0.415692, acc.: 82.03%] [G loss: 2.716825]\n",
      "epoch:2 step:2154 [D loss: 0.505632, acc.: 72.66%] [G loss: 2.992145]\n",
      "epoch:2 step:2155 [D loss: 0.522872, acc.: 75.00%] [G loss: 4.741447]\n",
      "epoch:2 step:2156 [D loss: 0.619879, acc.: 69.53%] [G loss: 2.422661]\n",
      "epoch:2 step:2157 [D loss: 0.409337, acc.: 82.03%] [G loss: 2.790336]\n",
      "epoch:2 step:2158 [D loss: 0.575578, acc.: 68.75%] [G loss: 2.995811]\n",
      "epoch:2 step:2159 [D loss: 0.484941, acc.: 75.00%] [G loss: 2.600893]\n",
      "epoch:2 step:2160 [D loss: 0.409430, acc.: 86.72%] [G loss: 2.656968]\n",
      "epoch:2 step:2161 [D loss: 0.411825, acc.: 79.69%] [G loss: 2.167531]\n",
      "epoch:2 step:2162 [D loss: 0.444232, acc.: 78.12%] [G loss: 1.733276]\n",
      "epoch:2 step:2163 [D loss: 0.405191, acc.: 82.81%] [G loss: 2.129048]\n",
      "epoch:2 step:2164 [D loss: 0.473378, acc.: 78.91%] [G loss: 2.121791]\n",
      "epoch:2 step:2165 [D loss: 0.410203, acc.: 82.03%] [G loss: 2.225590]\n",
      "epoch:2 step:2166 [D loss: 0.381869, acc.: 85.16%] [G loss: 2.229650]\n",
      "epoch:2 step:2167 [D loss: 0.350288, acc.: 86.72%] [G loss: 3.538430]\n",
      "epoch:2 step:2168 [D loss: 0.462992, acc.: 76.56%] [G loss: 2.879935]\n",
      "epoch:2 step:2169 [D loss: 0.333840, acc.: 90.62%] [G loss: 2.673482]\n",
      "epoch:2 step:2170 [D loss: 0.355460, acc.: 89.84%] [G loss: 2.125381]\n",
      "epoch:2 step:2171 [D loss: 0.412848, acc.: 84.38%] [G loss: 2.481080]\n",
      "epoch:2 step:2172 [D loss: 0.404923, acc.: 84.38%] [G loss: 2.032491]\n",
      "epoch:2 step:2173 [D loss: 0.455256, acc.: 80.47%] [G loss: 2.329220]\n",
      "epoch:2 step:2174 [D loss: 0.415724, acc.: 80.47%] [G loss: 2.949804]\n",
      "epoch:2 step:2175 [D loss: 0.438988, acc.: 81.25%] [G loss: 3.374643]\n",
      "epoch:2 step:2176 [D loss: 0.491647, acc.: 77.34%] [G loss: 3.738839]\n",
      "epoch:2 step:2177 [D loss: 0.518369, acc.: 80.47%] [G loss: 2.593755]\n",
      "epoch:2 step:2178 [D loss: 0.326478, acc.: 87.50%] [G loss: 2.895932]\n",
      "epoch:2 step:2179 [D loss: 0.394772, acc.: 84.38%] [G loss: 2.928255]\n",
      "epoch:2 step:2180 [D loss: 0.355201, acc.: 88.28%] [G loss: 2.231490]\n",
      "epoch:2 step:2181 [D loss: 0.372971, acc.: 82.81%] [G loss: 2.370781]\n",
      "epoch:2 step:2182 [D loss: 0.296152, acc.: 88.28%] [G loss: 2.755570]\n",
      "epoch:2 step:2183 [D loss: 0.321393, acc.: 92.19%] [G loss: 3.361128]\n",
      "epoch:2 step:2184 [D loss: 0.323321, acc.: 89.84%] [G loss: 2.713523]\n",
      "epoch:2 step:2185 [D loss: 0.416871, acc.: 75.00%] [G loss: 2.900491]\n",
      "epoch:2 step:2186 [D loss: 0.376551, acc.: 85.16%] [G loss: 2.255220]\n",
      "epoch:2 step:2187 [D loss: 0.390695, acc.: 82.81%] [G loss: 2.313449]\n",
      "epoch:2 step:2188 [D loss: 0.351010, acc.: 89.06%] [G loss: 2.396656]\n",
      "epoch:2 step:2189 [D loss: 0.361868, acc.: 88.28%] [G loss: 2.214492]\n",
      "epoch:2 step:2190 [D loss: 0.393651, acc.: 84.38%] [G loss: 1.816440]\n",
      "epoch:2 step:2191 [D loss: 0.347008, acc.: 91.41%] [G loss: 2.187821]\n",
      "epoch:2 step:2192 [D loss: 0.379542, acc.: 81.25%] [G loss: 2.239966]\n",
      "epoch:2 step:2193 [D loss: 0.406923, acc.: 79.69%] [G loss: 2.439875]\n",
      "epoch:2 step:2194 [D loss: 0.382560, acc.: 84.38%] [G loss: 3.399845]\n",
      "epoch:2 step:2195 [D loss: 0.498955, acc.: 82.81%] [G loss: 4.171980]\n",
      "epoch:2 step:2196 [D loss: 0.628735, acc.: 71.88%] [G loss: 3.919635]\n",
      "epoch:2 step:2197 [D loss: 0.589146, acc.: 75.78%] [G loss: 3.145182]\n",
      "epoch:2 step:2198 [D loss: 0.348324, acc.: 85.16%] [G loss: 3.113393]\n",
      "epoch:2 step:2199 [D loss: 0.331410, acc.: 89.06%] [G loss: 3.249478]\n",
      "epoch:2 step:2200 [D loss: 0.333602, acc.: 85.16%] [G loss: 3.639546]\n",
      "epoch:2 step:2201 [D loss: 0.378965, acc.: 86.72%] [G loss: 2.611945]\n",
      "epoch:2 step:2202 [D loss: 0.361878, acc.: 85.16%] [G loss: 4.266037]\n",
      "epoch:2 step:2203 [D loss: 0.418317, acc.: 79.69%] [G loss: 3.100870]\n",
      "epoch:2 step:2204 [D loss: 0.404206, acc.: 83.59%] [G loss: 2.408921]\n",
      "epoch:2 step:2205 [D loss: 0.347759, acc.: 87.50%] [G loss: 2.460803]\n",
      "epoch:2 step:2206 [D loss: 0.364739, acc.: 86.72%] [G loss: 2.083692]\n",
      "epoch:2 step:2207 [D loss: 0.406590, acc.: 82.03%] [G loss: 2.621834]\n",
      "epoch:2 step:2208 [D loss: 0.464496, acc.: 76.56%] [G loss: 1.936444]\n",
      "epoch:2 step:2209 [D loss: 0.453171, acc.: 79.69%] [G loss: 1.969562]\n",
      "epoch:2 step:2210 [D loss: 0.395389, acc.: 82.03%] [G loss: 2.213547]\n",
      "epoch:2 step:2211 [D loss: 0.405150, acc.: 81.25%] [G loss: 2.469756]\n",
      "epoch:2 step:2212 [D loss: 0.403982, acc.: 81.25%] [G loss: 2.350933]\n",
      "epoch:2 step:2213 [D loss: 0.392423, acc.: 82.81%] [G loss: 2.365810]\n",
      "epoch:2 step:2214 [D loss: 0.411545, acc.: 79.69%] [G loss: 2.592951]\n",
      "epoch:2 step:2215 [D loss: 0.394218, acc.: 78.91%] [G loss: 2.635853]\n",
      "epoch:2 step:2216 [D loss: 0.379474, acc.: 85.16%] [G loss: 2.273123]\n",
      "epoch:2 step:2217 [D loss: 0.340800, acc.: 90.62%] [G loss: 3.124870]\n",
      "epoch:2 step:2218 [D loss: 0.338340, acc.: 87.50%] [G loss: 3.413172]\n",
      "epoch:2 step:2219 [D loss: 0.446894, acc.: 78.12%] [G loss: 4.193811]\n",
      "epoch:2 step:2220 [D loss: 0.693714, acc.: 61.72%] [G loss: 4.613944]\n",
      "epoch:2 step:2221 [D loss: 0.431720, acc.: 80.47%] [G loss: 2.396716]\n",
      "epoch:2 step:2222 [D loss: 0.415762, acc.: 83.59%] [G loss: 2.651088]\n",
      "epoch:2 step:2223 [D loss: 0.431920, acc.: 81.25%] [G loss: 2.267777]\n",
      "epoch:2 step:2224 [D loss: 0.363134, acc.: 83.59%] [G loss: 3.019823]\n",
      "epoch:2 step:2225 [D loss: 0.352146, acc.: 86.72%] [G loss: 2.507033]\n",
      "epoch:2 step:2226 [D loss: 0.323742, acc.: 90.62%] [G loss: 3.105793]\n",
      "epoch:2 step:2227 [D loss: 0.331374, acc.: 91.41%] [G loss: 3.661963]\n",
      "epoch:2 step:2228 [D loss: 0.321590, acc.: 87.50%] [G loss: 3.065964]\n",
      "epoch:2 step:2229 [D loss: 0.315561, acc.: 88.28%] [G loss: 2.029057]\n",
      "epoch:2 step:2230 [D loss: 0.343518, acc.: 85.16%] [G loss: 4.008080]\n",
      "epoch:2 step:2231 [D loss: 0.342205, acc.: 85.16%] [G loss: 2.916668]\n",
      "epoch:2 step:2232 [D loss: 0.461927, acc.: 75.78%] [G loss: 2.337868]\n",
      "epoch:2 step:2233 [D loss: 0.454642, acc.: 78.12%] [G loss: 3.066684]\n",
      "epoch:2 step:2234 [D loss: 0.334373, acc.: 85.16%] [G loss: 3.480996]\n",
      "epoch:2 step:2235 [D loss: 0.458924, acc.: 77.34%] [G loss: 3.931676]\n",
      "epoch:2 step:2236 [D loss: 0.529960, acc.: 71.88%] [G loss: 2.760305]\n",
      "epoch:2 step:2237 [D loss: 0.352410, acc.: 81.25%] [G loss: 3.730538]\n",
      "epoch:2 step:2238 [D loss: 0.373109, acc.: 83.59%] [G loss: 2.491625]\n",
      "epoch:2 step:2239 [D loss: 0.368892, acc.: 86.72%] [G loss: 2.163537]\n",
      "epoch:2 step:2240 [D loss: 0.397078, acc.: 85.16%] [G loss: 2.413026]\n",
      "epoch:2 step:2241 [D loss: 0.333970, acc.: 85.94%] [G loss: 3.193403]\n",
      "epoch:2 step:2242 [D loss: 0.326758, acc.: 85.16%] [G loss: 2.891763]\n",
      "epoch:2 step:2243 [D loss: 0.544951, acc.: 71.88%] [G loss: 2.227829]\n",
      "epoch:2 step:2244 [D loss: 0.433245, acc.: 80.47%] [G loss: 2.777264]\n",
      "epoch:2 step:2245 [D loss: 0.384123, acc.: 77.34%] [G loss: 3.596257]\n",
      "epoch:2 step:2246 [D loss: 0.538709, acc.: 70.31%] [G loss: 2.825802]\n",
      "epoch:2 step:2247 [D loss: 0.499304, acc.: 75.00%] [G loss: 2.251268]\n",
      "epoch:2 step:2248 [D loss: 0.431959, acc.: 79.69%] [G loss: 3.418066]\n",
      "epoch:2 step:2249 [D loss: 0.530563, acc.: 68.75%] [G loss: 2.302604]\n",
      "epoch:2 step:2250 [D loss: 0.434612, acc.: 81.25%] [G loss: 3.908519]\n",
      "epoch:2 step:2251 [D loss: 0.502809, acc.: 70.31%] [G loss: 2.131228]\n",
      "epoch:2 step:2252 [D loss: 0.359645, acc.: 88.28%] [G loss: 2.723133]\n",
      "epoch:2 step:2253 [D loss: 0.469692, acc.: 82.03%] [G loss: 2.152888]\n",
      "epoch:2 step:2254 [D loss: 0.398599, acc.: 82.81%] [G loss: 3.013266]\n",
      "epoch:2 step:2255 [D loss: 0.377847, acc.: 83.59%] [G loss: 2.393194]\n",
      "epoch:2 step:2256 [D loss: 0.381053, acc.: 85.16%] [G loss: 1.930327]\n",
      "epoch:2 step:2257 [D loss: 0.443291, acc.: 78.12%] [G loss: 2.429151]\n",
      "epoch:2 step:2258 [D loss: 0.529977, acc.: 72.66%] [G loss: 2.680558]\n",
      "epoch:2 step:2259 [D loss: 0.534568, acc.: 76.56%] [G loss: 2.920093]\n",
      "epoch:2 step:2260 [D loss: 0.491370, acc.: 73.44%] [G loss: 3.444632]\n",
      "epoch:2 step:2261 [D loss: 0.374700, acc.: 82.81%] [G loss: 2.764347]\n",
      "epoch:2 step:2262 [D loss: 0.388807, acc.: 82.03%] [G loss: 2.371971]\n",
      "epoch:2 step:2263 [D loss: 0.371416, acc.: 78.12%] [G loss: 3.103921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2264 [D loss: 0.537083, acc.: 70.31%] [G loss: 2.697386]\n",
      "epoch:2 step:2265 [D loss: 0.632381, acc.: 69.53%] [G loss: 4.197826]\n",
      "epoch:2 step:2266 [D loss: 0.994140, acc.: 54.69%] [G loss: 4.031850]\n",
      "epoch:2 step:2267 [D loss: 1.093047, acc.: 66.41%] [G loss: 3.019234]\n",
      "epoch:2 step:2268 [D loss: 0.675396, acc.: 67.97%] [G loss: 2.874322]\n",
      "epoch:2 step:2269 [D loss: 0.526916, acc.: 75.00%] [G loss: 2.357455]\n",
      "epoch:2 step:2270 [D loss: 0.450247, acc.: 78.12%] [G loss: 2.201233]\n",
      "epoch:2 step:2271 [D loss: 0.342349, acc.: 87.50%] [G loss: 2.892551]\n",
      "epoch:2 step:2272 [D loss: 0.400450, acc.: 85.16%] [G loss: 2.067546]\n",
      "epoch:2 step:2273 [D loss: 0.392316, acc.: 85.94%] [G loss: 1.892380]\n",
      "epoch:2 step:2274 [D loss: 0.345462, acc.: 89.06%] [G loss: 2.377701]\n",
      "epoch:2 step:2275 [D loss: 0.483967, acc.: 78.12%] [G loss: 2.575248]\n",
      "epoch:2 step:2276 [D loss: 0.375682, acc.: 87.50%] [G loss: 2.234523]\n",
      "epoch:2 step:2277 [D loss: 0.469903, acc.: 73.44%] [G loss: 2.959954]\n",
      "epoch:2 step:2278 [D loss: 0.514565, acc.: 77.34%] [G loss: 2.341620]\n",
      "epoch:2 step:2279 [D loss: 0.450540, acc.: 78.91%] [G loss: 2.087617]\n",
      "epoch:2 step:2280 [D loss: 0.489214, acc.: 76.56%] [G loss: 2.188034]\n",
      "epoch:2 step:2281 [D loss: 0.528076, acc.: 75.78%] [G loss: 1.658330]\n",
      "epoch:2 step:2282 [D loss: 0.364626, acc.: 91.41%] [G loss: 1.869372]\n",
      "epoch:2 step:2283 [D loss: 0.440833, acc.: 86.72%] [G loss: 1.893748]\n",
      "epoch:2 step:2284 [D loss: 0.396477, acc.: 84.38%] [G loss: 2.157309]\n",
      "epoch:2 step:2285 [D loss: 0.534679, acc.: 78.12%] [G loss: 2.166984]\n",
      "epoch:2 step:2286 [D loss: 0.455224, acc.: 80.47%] [G loss: 2.435113]\n",
      "epoch:2 step:2287 [D loss: 0.436966, acc.: 77.34%] [G loss: 2.239772]\n",
      "epoch:2 step:2288 [D loss: 0.453728, acc.: 81.25%] [G loss: 2.075351]\n",
      "epoch:2 step:2289 [D loss: 0.464074, acc.: 78.91%] [G loss: 2.266651]\n",
      "epoch:2 step:2290 [D loss: 0.328747, acc.: 92.19%] [G loss: 2.284430]\n",
      "epoch:2 step:2291 [D loss: 0.326353, acc.: 89.06%] [G loss: 2.435811]\n",
      "epoch:2 step:2292 [D loss: 0.475988, acc.: 77.34%] [G loss: 2.050199]\n",
      "epoch:2 step:2293 [D loss: 0.271241, acc.: 94.53%] [G loss: 2.649354]\n",
      "epoch:2 step:2294 [D loss: 0.385246, acc.: 84.38%] [G loss: 2.638747]\n",
      "epoch:2 step:2295 [D loss: 0.341623, acc.: 89.84%] [G loss: 2.983282]\n",
      "epoch:2 step:2296 [D loss: 0.395474, acc.: 84.38%] [G loss: 2.706233]\n",
      "epoch:2 step:2297 [D loss: 0.463382, acc.: 81.25%] [G loss: 2.610923]\n",
      "epoch:2 step:2298 [D loss: 0.481797, acc.: 76.56%] [G loss: 2.109954]\n",
      "epoch:2 step:2299 [D loss: 0.396390, acc.: 82.81%] [G loss: 2.193840]\n",
      "epoch:2 step:2300 [D loss: 0.334102, acc.: 86.72%] [G loss: 2.506408]\n",
      "epoch:2 step:2301 [D loss: 0.339533, acc.: 87.50%] [G loss: 2.859651]\n",
      "epoch:2 step:2302 [D loss: 0.380291, acc.: 85.16%] [G loss: 3.018460]\n",
      "epoch:2 step:2303 [D loss: 0.340813, acc.: 88.28%] [G loss: 2.652200]\n",
      "epoch:2 step:2304 [D loss: 0.315056, acc.: 89.06%] [G loss: 2.499774]\n",
      "epoch:2 step:2305 [D loss: 0.500398, acc.: 75.00%] [G loss: 2.162893]\n",
      "epoch:2 step:2306 [D loss: 0.457313, acc.: 82.03%] [G loss: 2.450325]\n",
      "epoch:2 step:2307 [D loss: 0.373010, acc.: 85.16%] [G loss: 2.220829]\n",
      "epoch:2 step:2308 [D loss: 0.400966, acc.: 80.47%] [G loss: 2.634399]\n",
      "epoch:2 step:2309 [D loss: 0.336070, acc.: 87.50%] [G loss: 3.299970]\n",
      "epoch:2 step:2310 [D loss: 0.316075, acc.: 90.62%] [G loss: 3.537942]\n",
      "epoch:2 step:2311 [D loss: 0.377582, acc.: 82.81%] [G loss: 2.819524]\n",
      "epoch:2 step:2312 [D loss: 0.394388, acc.: 82.03%] [G loss: 2.666539]\n",
      "epoch:2 step:2313 [D loss: 0.449030, acc.: 80.47%] [G loss: 2.705925]\n",
      "epoch:2 step:2314 [D loss: 0.450693, acc.: 81.25%] [G loss: 3.771669]\n",
      "epoch:2 step:2315 [D loss: 0.543421, acc.: 76.56%] [G loss: 3.245730]\n",
      "epoch:2 step:2316 [D loss: 0.469992, acc.: 78.12%] [G loss: 2.792207]\n",
      "epoch:2 step:2317 [D loss: 0.349206, acc.: 88.28%] [G loss: 2.802392]\n",
      "epoch:2 step:2318 [D loss: 0.417515, acc.: 81.25%] [G loss: 2.730882]\n",
      "epoch:2 step:2319 [D loss: 0.386761, acc.: 82.03%] [G loss: 2.757784]\n",
      "epoch:2 step:2320 [D loss: 0.489187, acc.: 74.22%] [G loss: 2.241609]\n",
      "epoch:2 step:2321 [D loss: 0.416320, acc.: 80.47%] [G loss: 2.376291]\n",
      "epoch:2 step:2322 [D loss: 0.441052, acc.: 78.12%] [G loss: 2.594427]\n",
      "epoch:2 step:2323 [D loss: 0.372421, acc.: 82.03%] [G loss: 2.344082]\n",
      "epoch:2 step:2324 [D loss: 0.387552, acc.: 82.81%] [G loss: 2.246420]\n",
      "epoch:2 step:2325 [D loss: 0.415378, acc.: 81.25%] [G loss: 2.123353]\n",
      "epoch:2 step:2326 [D loss: 0.378472, acc.: 89.84%] [G loss: 2.650524]\n",
      "epoch:2 step:2327 [D loss: 0.492845, acc.: 75.00%] [G loss: 2.642563]\n",
      "epoch:2 step:2328 [D loss: 0.439170, acc.: 82.03%] [G loss: 1.969586]\n",
      "epoch:2 step:2329 [D loss: 0.411026, acc.: 85.16%] [G loss: 3.040770]\n",
      "epoch:2 step:2330 [D loss: 0.368618, acc.: 85.16%] [G loss: 3.504145]\n",
      "epoch:2 step:2331 [D loss: 0.435567, acc.: 81.25%] [G loss: 1.673066]\n",
      "epoch:2 step:2332 [D loss: 0.353050, acc.: 84.38%] [G loss: 2.688711]\n",
      "epoch:2 step:2333 [D loss: 0.445696, acc.: 78.12%] [G loss: 2.114967]\n",
      "epoch:2 step:2334 [D loss: 0.448211, acc.: 83.59%] [G loss: 2.202436]\n",
      "epoch:2 step:2335 [D loss: 0.400832, acc.: 78.12%] [G loss: 2.883825]\n",
      "epoch:2 step:2336 [D loss: 0.440552, acc.: 80.47%] [G loss: 3.192387]\n",
      "epoch:2 step:2337 [D loss: 0.440245, acc.: 83.59%] [G loss: 3.859443]\n",
      "epoch:2 step:2338 [D loss: 0.477232, acc.: 76.56%] [G loss: 2.410450]\n",
      "epoch:2 step:2339 [D loss: 0.381267, acc.: 85.94%] [G loss: 2.410415]\n",
      "epoch:2 step:2340 [D loss: 0.436561, acc.: 82.81%] [G loss: 2.024726]\n",
      "epoch:2 step:2341 [D loss: 0.402753, acc.: 82.81%] [G loss: 2.481952]\n",
      "epoch:2 step:2342 [D loss: 0.406817, acc.: 86.72%] [G loss: 2.420977]\n",
      "epoch:2 step:2343 [D loss: 0.472642, acc.: 76.56%] [G loss: 2.658113]\n",
      "epoch:3 step:2344 [D loss: 0.463431, acc.: 75.00%] [G loss: 2.711385]\n",
      "epoch:3 step:2345 [D loss: 0.312013, acc.: 89.06%] [G loss: 3.421805]\n",
      "epoch:3 step:2346 [D loss: 0.321634, acc.: 89.84%] [G loss: 2.693151]\n",
      "epoch:3 step:2347 [D loss: 0.372089, acc.: 85.16%] [G loss: 2.642965]\n",
      "epoch:3 step:2348 [D loss: 0.424155, acc.: 83.59%] [G loss: 2.368151]\n",
      "epoch:3 step:2349 [D loss: 0.388825, acc.: 85.94%] [G loss: 2.604142]\n",
      "epoch:3 step:2350 [D loss: 0.437852, acc.: 75.78%] [G loss: 2.823277]\n",
      "epoch:3 step:2351 [D loss: 0.430186, acc.: 78.91%] [G loss: 2.717999]\n",
      "epoch:3 step:2352 [D loss: 0.394544, acc.: 84.38%] [G loss: 2.249685]\n",
      "epoch:3 step:2353 [D loss: 0.410960, acc.: 82.81%] [G loss: 2.659037]\n",
      "epoch:3 step:2354 [D loss: 0.418828, acc.: 82.81%] [G loss: 3.445783]\n",
      "epoch:3 step:2355 [D loss: 0.408161, acc.: 82.03%] [G loss: 2.616265]\n",
      "epoch:3 step:2356 [D loss: 0.438904, acc.: 78.12%] [G loss: 4.500612]\n",
      "epoch:3 step:2357 [D loss: 0.935229, acc.: 57.81%] [G loss: 3.125572]\n",
      "epoch:3 step:2358 [D loss: 0.651514, acc.: 75.00%] [G loss: 2.897453]\n",
      "epoch:3 step:2359 [D loss: 0.350615, acc.: 85.94%] [G loss: 2.705727]\n",
      "epoch:3 step:2360 [D loss: 0.419401, acc.: 81.25%] [G loss: 2.320521]\n",
      "epoch:3 step:2361 [D loss: 0.362855, acc.: 87.50%] [G loss: 2.286827]\n",
      "epoch:3 step:2362 [D loss: 0.500747, acc.: 78.91%] [G loss: 2.262525]\n",
      "epoch:3 step:2363 [D loss: 0.650230, acc.: 61.72%] [G loss: 1.937329]\n",
      "epoch:3 step:2364 [D loss: 0.409063, acc.: 82.03%] [G loss: 2.894091]\n",
      "epoch:3 step:2365 [D loss: 0.503135, acc.: 80.47%] [G loss: 3.045980]\n",
      "epoch:3 step:2366 [D loss: 0.407660, acc.: 82.81%] [G loss: 2.481128]\n",
      "epoch:3 step:2367 [D loss: 0.406406, acc.: 83.59%] [G loss: 2.623533]\n",
      "epoch:3 step:2368 [D loss: 0.367822, acc.: 82.81%] [G loss: 2.349148]\n",
      "epoch:3 step:2369 [D loss: 0.378795, acc.: 85.94%] [G loss: 2.165434]\n",
      "epoch:3 step:2370 [D loss: 0.429592, acc.: 80.47%] [G loss: 2.246960]\n",
      "epoch:3 step:2371 [D loss: 0.408756, acc.: 88.28%] [G loss: 2.104808]\n",
      "epoch:3 step:2372 [D loss: 0.386844, acc.: 85.94%] [G loss: 2.514760]\n",
      "epoch:3 step:2373 [D loss: 0.303446, acc.: 91.41%] [G loss: 2.623556]\n",
      "epoch:3 step:2374 [D loss: 0.355826, acc.: 85.94%] [G loss: 2.444022]\n",
      "epoch:3 step:2375 [D loss: 0.359110, acc.: 88.28%] [G loss: 2.293767]\n",
      "epoch:3 step:2376 [D loss: 0.350189, acc.: 86.72%] [G loss: 3.286129]\n",
      "epoch:3 step:2377 [D loss: 0.362967, acc.: 85.16%] [G loss: 2.995423]\n",
      "epoch:3 step:2378 [D loss: 0.403397, acc.: 79.69%] [G loss: 2.450871]\n",
      "epoch:3 step:2379 [D loss: 0.340732, acc.: 89.84%] [G loss: 2.498071]\n",
      "epoch:3 step:2380 [D loss: 0.419500, acc.: 82.03%] [G loss: 2.256578]\n",
      "epoch:3 step:2381 [D loss: 0.335112, acc.: 87.50%] [G loss: 2.346992]\n",
      "epoch:3 step:2382 [D loss: 0.392221, acc.: 79.69%] [G loss: 2.505694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2383 [D loss: 0.459902, acc.: 75.78%] [G loss: 2.054423]\n",
      "epoch:3 step:2384 [D loss: 0.362294, acc.: 87.50%] [G loss: 3.490259]\n",
      "epoch:3 step:2385 [D loss: 0.385806, acc.: 85.16%] [G loss: 2.844067]\n",
      "epoch:3 step:2386 [D loss: 0.392875, acc.: 86.72%] [G loss: 2.679103]\n",
      "epoch:3 step:2387 [D loss: 0.556025, acc.: 71.88%] [G loss: 3.215159]\n",
      "epoch:3 step:2388 [D loss: 0.501076, acc.: 76.56%] [G loss: 2.081975]\n",
      "epoch:3 step:2389 [D loss: 0.440412, acc.: 81.25%] [G loss: 2.375535]\n",
      "epoch:3 step:2390 [D loss: 0.353648, acc.: 89.84%] [G loss: 2.369380]\n",
      "epoch:3 step:2391 [D loss: 0.301631, acc.: 89.06%] [G loss: 2.580812]\n",
      "epoch:3 step:2392 [D loss: 0.432036, acc.: 81.25%] [G loss: 2.288968]\n",
      "epoch:3 step:2393 [D loss: 0.413254, acc.: 79.69%] [G loss: 2.176104]\n",
      "epoch:3 step:2394 [D loss: 0.347315, acc.: 87.50%] [G loss: 3.964544]\n",
      "epoch:3 step:2395 [D loss: 0.572240, acc.: 71.88%] [G loss: 2.679274]\n",
      "epoch:3 step:2396 [D loss: 0.494952, acc.: 76.56%] [G loss: 3.080368]\n",
      "epoch:3 step:2397 [D loss: 0.320053, acc.: 88.28%] [G loss: 3.863784]\n",
      "epoch:3 step:2398 [D loss: 0.453734, acc.: 79.69%] [G loss: 2.747610]\n",
      "epoch:3 step:2399 [D loss: 0.383740, acc.: 82.81%] [G loss: 2.897473]\n",
      "epoch:3 step:2400 [D loss: 0.435955, acc.: 78.12%] [G loss: 2.448561]\n",
      "epoch:3 step:2401 [D loss: 0.389572, acc.: 81.25%] [G loss: 3.147577]\n",
      "epoch:3 step:2402 [D loss: 0.312257, acc.: 89.84%] [G loss: 2.709903]\n",
      "epoch:3 step:2403 [D loss: 0.363337, acc.: 85.94%] [G loss: 2.606572]\n",
      "epoch:3 step:2404 [D loss: 0.365842, acc.: 88.28%] [G loss: 2.722857]\n",
      "epoch:3 step:2405 [D loss: 0.379744, acc.: 85.94%] [G loss: 2.910400]\n",
      "epoch:3 step:2406 [D loss: 0.443075, acc.: 80.47%] [G loss: 2.623999]\n",
      "epoch:3 step:2407 [D loss: 0.483189, acc.: 75.00%] [G loss: 3.049926]\n",
      "epoch:3 step:2408 [D loss: 0.419166, acc.: 81.25%] [G loss: 3.194518]\n",
      "epoch:3 step:2409 [D loss: 0.435076, acc.: 78.12%] [G loss: 2.351653]\n",
      "epoch:3 step:2410 [D loss: 0.422662, acc.: 85.16%] [G loss: 2.816363]\n",
      "epoch:3 step:2411 [D loss: 0.377641, acc.: 80.47%] [G loss: 2.216321]\n",
      "epoch:3 step:2412 [D loss: 0.432141, acc.: 80.47%] [G loss: 2.542209]\n",
      "epoch:3 step:2413 [D loss: 0.296766, acc.: 87.50%] [G loss: 3.091878]\n",
      "epoch:3 step:2414 [D loss: 0.344321, acc.: 84.38%] [G loss: 2.892154]\n",
      "epoch:3 step:2415 [D loss: 0.371653, acc.: 85.16%] [G loss: 3.730443]\n",
      "epoch:3 step:2416 [D loss: 0.467264, acc.: 76.56%] [G loss: 1.869890]\n",
      "epoch:3 step:2417 [D loss: 0.396727, acc.: 82.03%] [G loss: 2.175002]\n",
      "epoch:3 step:2418 [D loss: 0.446225, acc.: 80.47%] [G loss: 2.357270]\n",
      "epoch:3 step:2419 [D loss: 0.605755, acc.: 72.66%] [G loss: 3.240687]\n",
      "epoch:3 step:2420 [D loss: 0.716886, acc.: 64.06%] [G loss: 4.440956]\n",
      "epoch:3 step:2421 [D loss: 0.896917, acc.: 63.28%] [G loss: 5.342667]\n",
      "epoch:3 step:2422 [D loss: 0.983200, acc.: 54.69%] [G loss: 2.714183]\n",
      "epoch:3 step:2423 [D loss: 0.375028, acc.: 86.72%] [G loss: 2.768261]\n",
      "epoch:3 step:2424 [D loss: 0.626082, acc.: 73.44%] [G loss: 3.649410]\n",
      "epoch:3 step:2425 [D loss: 0.850508, acc.: 60.94%] [G loss: 3.078512]\n",
      "epoch:3 step:2426 [D loss: 0.281864, acc.: 90.62%] [G loss: 2.876562]\n",
      "epoch:3 step:2427 [D loss: 0.415030, acc.: 86.72%] [G loss: 2.182590]\n",
      "epoch:3 step:2428 [D loss: 0.379494, acc.: 82.81%] [G loss: 2.370911]\n",
      "epoch:3 step:2429 [D loss: 0.370842, acc.: 85.16%] [G loss: 2.465034]\n",
      "epoch:3 step:2430 [D loss: 0.444235, acc.: 78.91%] [G loss: 1.748749]\n",
      "epoch:3 step:2431 [D loss: 0.331062, acc.: 87.50%] [G loss: 2.278867]\n",
      "epoch:3 step:2432 [D loss: 0.376030, acc.: 85.16%] [G loss: 2.463219]\n",
      "epoch:3 step:2433 [D loss: 0.371577, acc.: 82.81%] [G loss: 1.952073]\n",
      "epoch:3 step:2434 [D loss: 0.409497, acc.: 82.81%] [G loss: 2.022090]\n",
      "epoch:3 step:2435 [D loss: 0.379201, acc.: 87.50%] [G loss: 2.159053]\n",
      "epoch:3 step:2436 [D loss: 0.323740, acc.: 92.97%] [G loss: 2.666246]\n",
      "epoch:3 step:2437 [D loss: 0.349007, acc.: 85.94%] [G loss: 2.296880]\n",
      "epoch:3 step:2438 [D loss: 0.339669, acc.: 89.06%] [G loss: 2.277292]\n",
      "epoch:3 step:2439 [D loss: 0.329944, acc.: 88.28%] [G loss: 2.251247]\n",
      "epoch:3 step:2440 [D loss: 0.322198, acc.: 88.28%] [G loss: 1.907227]\n",
      "epoch:3 step:2441 [D loss: 0.350179, acc.: 87.50%] [G loss: 2.044605]\n",
      "epoch:3 step:2442 [D loss: 0.369431, acc.: 88.28%] [G loss: 1.894201]\n",
      "epoch:3 step:2443 [D loss: 0.353508, acc.: 87.50%] [G loss: 2.468550]\n",
      "epoch:3 step:2444 [D loss: 0.355040, acc.: 88.28%] [G loss: 1.925620]\n",
      "epoch:3 step:2445 [D loss: 0.335011, acc.: 91.41%] [G loss: 2.597288]\n",
      "epoch:3 step:2446 [D loss: 0.340114, acc.: 86.72%] [G loss: 2.482888]\n",
      "epoch:3 step:2447 [D loss: 0.371371, acc.: 86.72%] [G loss: 2.630756]\n",
      "epoch:3 step:2448 [D loss: 0.416550, acc.: 78.91%] [G loss: 2.260284]\n",
      "epoch:3 step:2449 [D loss: 0.358951, acc.: 86.72%] [G loss: 2.482671]\n",
      "epoch:3 step:2450 [D loss: 0.339165, acc.: 91.41%] [G loss: 2.959960]\n",
      "epoch:3 step:2451 [D loss: 0.343145, acc.: 89.06%] [G loss: 2.887918]\n",
      "epoch:3 step:2452 [D loss: 0.372177, acc.: 84.38%] [G loss: 2.437764]\n",
      "epoch:3 step:2453 [D loss: 0.277303, acc.: 89.06%] [G loss: 2.399582]\n",
      "epoch:3 step:2454 [D loss: 0.427730, acc.: 78.12%] [G loss: 2.226099]\n",
      "epoch:3 step:2455 [D loss: 0.342924, acc.: 89.06%] [G loss: 2.694017]\n",
      "epoch:3 step:2456 [D loss: 0.377597, acc.: 83.59%] [G loss: 3.494721]\n",
      "epoch:3 step:2457 [D loss: 0.361159, acc.: 83.59%] [G loss: 2.506054]\n",
      "epoch:3 step:2458 [D loss: 0.331977, acc.: 85.16%] [G loss: 3.011031]\n",
      "epoch:3 step:2459 [D loss: 0.407099, acc.: 80.47%] [G loss: 2.430050]\n",
      "epoch:3 step:2460 [D loss: 0.292317, acc.: 89.06%] [G loss: 2.989296]\n",
      "epoch:3 step:2461 [D loss: 0.326422, acc.: 88.28%] [G loss: 2.917572]\n",
      "epoch:3 step:2462 [D loss: 0.278405, acc.: 91.41%] [G loss: 3.385483]\n",
      "epoch:3 step:2463 [D loss: 0.435643, acc.: 78.12%] [G loss: 2.388305]\n",
      "epoch:3 step:2464 [D loss: 0.344025, acc.: 86.72%] [G loss: 1.753665]\n",
      "epoch:3 step:2465 [D loss: 0.331113, acc.: 92.19%] [G loss: 2.602813]\n",
      "epoch:3 step:2466 [D loss: 0.365649, acc.: 84.38%] [G loss: 2.708131]\n",
      "epoch:3 step:2467 [D loss: 0.361801, acc.: 82.03%] [G loss: 2.589264]\n",
      "epoch:3 step:2468 [D loss: 0.321936, acc.: 85.94%] [G loss: 3.147113]\n",
      "epoch:3 step:2469 [D loss: 0.399722, acc.: 80.47%] [G loss: 2.952939]\n",
      "epoch:3 step:2470 [D loss: 0.335275, acc.: 88.28%] [G loss: 2.725533]\n",
      "epoch:3 step:2471 [D loss: 0.345942, acc.: 87.50%] [G loss: 2.252481]\n",
      "epoch:3 step:2472 [D loss: 0.446053, acc.: 78.12%] [G loss: 3.646796]\n",
      "epoch:3 step:2473 [D loss: 0.539884, acc.: 78.91%] [G loss: 5.857546]\n",
      "epoch:3 step:2474 [D loss: 0.979075, acc.: 67.19%] [G loss: 4.604329]\n",
      "epoch:3 step:2475 [D loss: 0.787952, acc.: 61.72%] [G loss: 2.789377]\n",
      "epoch:3 step:2476 [D loss: 0.335585, acc.: 85.94%] [G loss: 3.524682]\n",
      "epoch:3 step:2477 [D loss: 0.528655, acc.: 78.91%] [G loss: 2.478192]\n",
      "epoch:3 step:2478 [D loss: 0.365787, acc.: 85.94%] [G loss: 3.949612]\n",
      "epoch:3 step:2479 [D loss: 0.367717, acc.: 83.59%] [G loss: 2.826575]\n",
      "epoch:3 step:2480 [D loss: 0.314070, acc.: 88.28%] [G loss: 2.940264]\n",
      "epoch:3 step:2481 [D loss: 0.469147, acc.: 77.34%] [G loss: 1.629359]\n",
      "epoch:3 step:2482 [D loss: 0.408581, acc.: 82.03%] [G loss: 2.097822]\n",
      "epoch:3 step:2483 [D loss: 0.383112, acc.: 89.84%] [G loss: 2.059204]\n",
      "epoch:3 step:2484 [D loss: 0.397143, acc.: 85.16%] [G loss: 2.227274]\n",
      "epoch:3 step:2485 [D loss: 0.385981, acc.: 83.59%] [G loss: 2.106638]\n",
      "epoch:3 step:2486 [D loss: 0.315947, acc.: 93.75%] [G loss: 2.266541]\n",
      "epoch:3 step:2487 [D loss: 0.379300, acc.: 89.84%] [G loss: 2.296793]\n",
      "epoch:3 step:2488 [D loss: 0.417726, acc.: 82.03%] [G loss: 1.852642]\n",
      "epoch:3 step:2489 [D loss: 0.342223, acc.: 85.94%] [G loss: 2.682540]\n",
      "epoch:3 step:2490 [D loss: 0.389716, acc.: 82.81%] [G loss: 2.214643]\n",
      "epoch:3 step:2491 [D loss: 0.379714, acc.: 85.94%] [G loss: 2.274343]\n",
      "epoch:3 step:2492 [D loss: 0.401001, acc.: 84.38%] [G loss: 1.953786]\n",
      "epoch:3 step:2493 [D loss: 0.324206, acc.: 90.62%] [G loss: 2.488253]\n",
      "epoch:3 step:2494 [D loss: 0.313293, acc.: 90.62%] [G loss: 3.029972]\n",
      "epoch:3 step:2495 [D loss: 0.301931, acc.: 91.41%] [G loss: 3.743650]\n",
      "epoch:3 step:2496 [D loss: 0.321340, acc.: 86.72%] [G loss: 2.983772]\n",
      "epoch:3 step:2497 [D loss: 0.332313, acc.: 85.16%] [G loss: 2.470160]\n",
      "epoch:3 step:2498 [D loss: 0.345841, acc.: 88.28%] [G loss: 2.526474]\n",
      "epoch:3 step:2499 [D loss: 0.243956, acc.: 96.09%] [G loss: 2.627048]\n",
      "epoch:3 step:2500 [D loss: 0.392655, acc.: 83.59%] [G loss: 3.824577]\n",
      "epoch:3 step:2501 [D loss: 0.447091, acc.: 78.91%] [G loss: 2.399432]\n",
      "epoch:3 step:2502 [D loss: 0.381945, acc.: 82.03%] [G loss: 2.562052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2503 [D loss: 0.455457, acc.: 80.47%] [G loss: 2.601806]\n",
      "epoch:3 step:2504 [D loss: 0.370421, acc.: 85.94%] [G loss: 2.566759]\n",
      "epoch:3 step:2505 [D loss: 0.317295, acc.: 88.28%] [G loss: 2.645190]\n",
      "epoch:3 step:2506 [D loss: 0.354145, acc.: 89.06%] [G loss: 2.418532]\n",
      "epoch:3 step:2507 [D loss: 0.333197, acc.: 90.62%] [G loss: 2.564600]\n",
      "epoch:3 step:2508 [D loss: 0.382816, acc.: 84.38%] [G loss: 2.292087]\n",
      "epoch:3 step:2509 [D loss: 0.514633, acc.: 71.88%] [G loss: 2.547666]\n",
      "epoch:3 step:2510 [D loss: 0.396240, acc.: 85.94%] [G loss: 2.890831]\n",
      "epoch:3 step:2511 [D loss: 0.338475, acc.: 84.38%] [G loss: 3.321225]\n",
      "epoch:3 step:2512 [D loss: 0.288390, acc.: 92.19%] [G loss: 3.675132]\n",
      "epoch:3 step:2513 [D loss: 0.376177, acc.: 84.38%] [G loss: 3.002698]\n",
      "epoch:3 step:2514 [D loss: 0.359497, acc.: 85.16%] [G loss: 1.744556]\n",
      "epoch:3 step:2515 [D loss: 0.394583, acc.: 82.81%] [G loss: 2.898781]\n",
      "epoch:3 step:2516 [D loss: 0.341026, acc.: 86.72%] [G loss: 3.304647]\n",
      "epoch:3 step:2517 [D loss: 0.413540, acc.: 76.56%] [G loss: 3.782926]\n",
      "epoch:3 step:2518 [D loss: 0.427485, acc.: 79.69%] [G loss: 2.644507]\n",
      "epoch:3 step:2519 [D loss: 0.343219, acc.: 85.94%] [G loss: 2.285667]\n",
      "epoch:3 step:2520 [D loss: 0.428174, acc.: 84.38%] [G loss: 3.271365]\n",
      "epoch:3 step:2521 [D loss: 0.625146, acc.: 70.31%] [G loss: 4.703249]\n",
      "epoch:3 step:2522 [D loss: 0.671653, acc.: 70.31%] [G loss: 2.195805]\n",
      "epoch:3 step:2523 [D loss: 0.325990, acc.: 85.94%] [G loss: 3.785996]\n",
      "epoch:3 step:2524 [D loss: 0.433611, acc.: 78.91%] [G loss: 4.419512]\n",
      "epoch:3 step:2525 [D loss: 0.433324, acc.: 82.81%] [G loss: 1.575798]\n",
      "epoch:3 step:2526 [D loss: 0.367984, acc.: 84.38%] [G loss: 2.344665]\n",
      "epoch:3 step:2527 [D loss: 0.410831, acc.: 82.81%] [G loss: 2.213684]\n",
      "epoch:3 step:2528 [D loss: 0.378950, acc.: 85.94%] [G loss: 2.408210]\n",
      "epoch:3 step:2529 [D loss: 0.368568, acc.: 89.84%] [G loss: 2.316053]\n",
      "epoch:3 step:2530 [D loss: 0.442303, acc.: 78.91%] [G loss: 2.308104]\n",
      "epoch:3 step:2531 [D loss: 0.367765, acc.: 82.81%] [G loss: 2.391416]\n",
      "epoch:3 step:2532 [D loss: 0.351791, acc.: 86.72%] [G loss: 2.750119]\n",
      "epoch:3 step:2533 [D loss: 0.320147, acc.: 85.94%] [G loss: 2.692328]\n",
      "epoch:3 step:2534 [D loss: 0.326243, acc.: 85.94%] [G loss: 2.824143]\n",
      "epoch:3 step:2535 [D loss: 0.392616, acc.: 83.59%] [G loss: 2.885463]\n",
      "epoch:3 step:2536 [D loss: 0.456132, acc.: 78.12%] [G loss: 4.072073]\n",
      "epoch:3 step:2537 [D loss: 0.646448, acc.: 72.66%] [G loss: 5.391006]\n",
      "epoch:3 step:2538 [D loss: 0.775034, acc.: 67.19%] [G loss: 2.555696]\n",
      "epoch:3 step:2539 [D loss: 0.443675, acc.: 82.81%] [G loss: 2.304483]\n",
      "epoch:3 step:2540 [D loss: 0.437794, acc.: 80.47%] [G loss: 3.529012]\n",
      "epoch:3 step:2541 [D loss: 0.663461, acc.: 60.16%] [G loss: 1.999213]\n",
      "epoch:3 step:2542 [D loss: 0.335572, acc.: 85.16%] [G loss: 3.592789]\n",
      "epoch:3 step:2543 [D loss: 0.414064, acc.: 81.25%] [G loss: 3.586524]\n",
      "epoch:3 step:2544 [D loss: 0.294624, acc.: 87.50%] [G loss: 4.210801]\n",
      "epoch:3 step:2545 [D loss: 0.293574, acc.: 92.97%] [G loss: 3.046201]\n",
      "epoch:3 step:2546 [D loss: 0.360342, acc.: 86.72%] [G loss: 2.304790]\n",
      "epoch:3 step:2547 [D loss: 0.369998, acc.: 87.50%] [G loss: 3.336140]\n",
      "epoch:3 step:2548 [D loss: 0.412685, acc.: 79.69%] [G loss: 2.169322]\n",
      "epoch:3 step:2549 [D loss: 0.431285, acc.: 82.03%] [G loss: 2.150769]\n",
      "epoch:3 step:2550 [D loss: 0.367008, acc.: 83.59%] [G loss: 2.482376]\n",
      "epoch:3 step:2551 [D loss: 0.381839, acc.: 84.38%] [G loss: 2.089334]\n",
      "epoch:3 step:2552 [D loss: 0.380995, acc.: 83.59%] [G loss: 2.546817]\n",
      "epoch:3 step:2553 [D loss: 0.359127, acc.: 84.38%] [G loss: 2.800743]\n",
      "epoch:3 step:2554 [D loss: 0.317262, acc.: 87.50%] [G loss: 3.338559]\n",
      "epoch:3 step:2555 [D loss: 0.292854, acc.: 89.84%] [G loss: 2.570123]\n",
      "epoch:3 step:2556 [D loss: 0.345410, acc.: 82.81%] [G loss: 2.192772]\n",
      "epoch:3 step:2557 [D loss: 0.371650, acc.: 84.38%] [G loss: 2.804667]\n",
      "epoch:3 step:2558 [D loss: 0.464978, acc.: 75.00%] [G loss: 2.750514]\n",
      "epoch:3 step:2559 [D loss: 0.357957, acc.: 82.81%] [G loss: 3.367765]\n",
      "epoch:3 step:2560 [D loss: 0.271933, acc.: 92.19%] [G loss: 3.954648]\n",
      "epoch:3 step:2561 [D loss: 0.415949, acc.: 83.59%] [G loss: 2.290288]\n",
      "epoch:3 step:2562 [D loss: 0.318240, acc.: 89.84%] [G loss: 2.957492]\n",
      "epoch:3 step:2563 [D loss: 0.338050, acc.: 89.06%] [G loss: 2.536669]\n",
      "epoch:3 step:2564 [D loss: 0.301435, acc.: 92.19%] [G loss: 2.736144]\n",
      "epoch:3 step:2565 [D loss: 0.425516, acc.: 79.69%] [G loss: 2.539567]\n",
      "epoch:3 step:2566 [D loss: 0.391034, acc.: 88.28%] [G loss: 2.589991]\n",
      "epoch:3 step:2567 [D loss: 0.377099, acc.: 86.72%] [G loss: 2.432755]\n",
      "epoch:3 step:2568 [D loss: 0.404626, acc.: 82.81%] [G loss: 2.511386]\n",
      "epoch:3 step:2569 [D loss: 0.325450, acc.: 88.28%] [G loss: 3.275764]\n",
      "epoch:3 step:2570 [D loss: 0.330692, acc.: 85.94%] [G loss: 3.172307]\n",
      "epoch:3 step:2571 [D loss: 0.381400, acc.: 80.47%] [G loss: 2.566280]\n",
      "epoch:3 step:2572 [D loss: 0.436220, acc.: 80.47%] [G loss: 3.685339]\n",
      "epoch:3 step:2573 [D loss: 0.444462, acc.: 74.22%] [G loss: 3.335786]\n",
      "epoch:3 step:2574 [D loss: 0.432821, acc.: 78.91%] [G loss: 2.713967]\n",
      "epoch:3 step:2575 [D loss: 0.320258, acc.: 89.06%] [G loss: 2.271406]\n",
      "epoch:3 step:2576 [D loss: 0.455058, acc.: 78.91%] [G loss: 2.591897]\n",
      "epoch:3 step:2577 [D loss: 0.318599, acc.: 89.06%] [G loss: 2.570209]\n",
      "epoch:3 step:2578 [D loss: 0.326143, acc.: 85.94%] [G loss: 3.453042]\n",
      "epoch:3 step:2579 [D loss: 0.396355, acc.: 79.69%] [G loss: 3.061395]\n",
      "epoch:3 step:2580 [D loss: 0.321705, acc.: 86.72%] [G loss: 2.984306]\n",
      "epoch:3 step:2581 [D loss: 0.397615, acc.: 82.81%] [G loss: 2.430990]\n",
      "epoch:3 step:2582 [D loss: 0.399174, acc.: 80.47%] [G loss: 2.846643]\n",
      "epoch:3 step:2583 [D loss: 0.479152, acc.: 73.44%] [G loss: 2.158721]\n",
      "epoch:3 step:2584 [D loss: 0.401091, acc.: 85.94%] [G loss: 2.268052]\n",
      "epoch:3 step:2585 [D loss: 0.352958, acc.: 85.16%] [G loss: 3.374614]\n",
      "epoch:3 step:2586 [D loss: 0.445265, acc.: 75.78%] [G loss: 4.221831]\n",
      "epoch:3 step:2587 [D loss: 0.485423, acc.: 79.69%] [G loss: 3.479488]\n",
      "epoch:3 step:2588 [D loss: 0.339363, acc.: 85.16%] [G loss: 3.725602]\n",
      "epoch:3 step:2589 [D loss: 0.451833, acc.: 82.03%] [G loss: 3.699627]\n",
      "epoch:3 step:2590 [D loss: 0.401882, acc.: 80.47%] [G loss: 2.510858]\n",
      "epoch:3 step:2591 [D loss: 0.317584, acc.: 86.72%] [G loss: 3.375605]\n",
      "epoch:3 step:2592 [D loss: 0.406574, acc.: 77.34%] [G loss: 2.943973]\n",
      "epoch:3 step:2593 [D loss: 0.364227, acc.: 82.81%] [G loss: 2.147449]\n",
      "epoch:3 step:2594 [D loss: 0.374484, acc.: 85.16%] [G loss: 1.946183]\n",
      "epoch:3 step:2595 [D loss: 0.372306, acc.: 88.28%] [G loss: 2.749873]\n",
      "epoch:3 step:2596 [D loss: 0.361260, acc.: 84.38%] [G loss: 2.906209]\n",
      "epoch:3 step:2597 [D loss: 0.360984, acc.: 87.50%] [G loss: 2.943553]\n",
      "epoch:3 step:2598 [D loss: 0.471444, acc.: 78.12%] [G loss: 2.134202]\n",
      "epoch:3 step:2599 [D loss: 0.405103, acc.: 82.81%] [G loss: 2.769423]\n",
      "epoch:3 step:2600 [D loss: 0.416299, acc.: 82.03%] [G loss: 3.769477]\n",
      "epoch:3 step:2601 [D loss: 0.454414, acc.: 77.34%] [G loss: 2.264099]\n",
      "epoch:3 step:2602 [D loss: 0.361032, acc.: 87.50%] [G loss: 3.267906]\n",
      "epoch:3 step:2603 [D loss: 0.532985, acc.: 75.78%] [G loss: 3.590554]\n",
      "epoch:3 step:2604 [D loss: 0.437496, acc.: 81.25%] [G loss: 2.632672]\n",
      "epoch:3 step:2605 [D loss: 0.442795, acc.: 78.91%] [G loss: 2.506948]\n",
      "epoch:3 step:2606 [D loss: 0.336572, acc.: 87.50%] [G loss: 3.041679]\n",
      "epoch:3 step:2607 [D loss: 0.402439, acc.: 83.59%] [G loss: 2.554026]\n",
      "epoch:3 step:2608 [D loss: 0.457021, acc.: 82.03%] [G loss: 3.725289]\n",
      "epoch:3 step:2609 [D loss: 0.420047, acc.: 76.56%] [G loss: 3.351788]\n",
      "epoch:3 step:2610 [D loss: 0.599560, acc.: 73.44%] [G loss: 3.364708]\n",
      "epoch:3 step:2611 [D loss: 0.794718, acc.: 71.09%] [G loss: 2.878830]\n",
      "epoch:3 step:2612 [D loss: 0.472966, acc.: 78.91%] [G loss: 2.473137]\n",
      "epoch:3 step:2613 [D loss: 0.530427, acc.: 70.31%] [G loss: 2.823232]\n",
      "epoch:3 step:2614 [D loss: 0.399493, acc.: 81.25%] [G loss: 2.552297]\n",
      "epoch:3 step:2615 [D loss: 0.335282, acc.: 89.84%] [G loss: 2.408205]\n",
      "epoch:3 step:2616 [D loss: 0.365803, acc.: 86.72%] [G loss: 2.097187]\n",
      "epoch:3 step:2617 [D loss: 0.426697, acc.: 84.38%] [G loss: 2.265886]\n",
      "epoch:3 step:2618 [D loss: 0.511641, acc.: 72.66%] [G loss: 2.087215]\n",
      "epoch:3 step:2619 [D loss: 0.401682, acc.: 82.81%] [G loss: 2.517183]\n",
      "epoch:3 step:2620 [D loss: 0.498818, acc.: 73.44%] [G loss: 3.383551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2621 [D loss: 0.542953, acc.: 71.88%] [G loss: 3.178453]\n",
      "epoch:3 step:2622 [D loss: 0.645062, acc.: 70.31%] [G loss: 3.023790]\n",
      "epoch:3 step:2623 [D loss: 0.549675, acc.: 75.00%] [G loss: 3.030366]\n",
      "epoch:3 step:2624 [D loss: 0.434576, acc.: 75.78%] [G loss: 2.214896]\n",
      "epoch:3 step:2625 [D loss: 0.420164, acc.: 86.72%] [G loss: 2.339194]\n",
      "epoch:3 step:2626 [D loss: 0.442083, acc.: 77.34%] [G loss: 1.887504]\n",
      "epoch:3 step:2627 [D loss: 0.355485, acc.: 89.06%] [G loss: 2.132969]\n",
      "epoch:3 step:2628 [D loss: 0.382049, acc.: 81.25%] [G loss: 2.731739]\n",
      "epoch:3 step:2629 [D loss: 0.480837, acc.: 76.56%] [G loss: 1.875362]\n",
      "epoch:3 step:2630 [D loss: 0.423195, acc.: 82.81%] [G loss: 2.517127]\n",
      "epoch:3 step:2631 [D loss: 0.423111, acc.: 82.03%] [G loss: 2.546664]\n",
      "epoch:3 step:2632 [D loss: 0.372741, acc.: 84.38%] [G loss: 2.441107]\n",
      "epoch:3 step:2633 [D loss: 0.344865, acc.: 86.72%] [G loss: 3.041816]\n",
      "epoch:3 step:2634 [D loss: 0.336608, acc.: 86.72%] [G loss: 2.586260]\n",
      "epoch:3 step:2635 [D loss: 0.414613, acc.: 81.25%] [G loss: 2.756893]\n",
      "epoch:3 step:2636 [D loss: 0.436027, acc.: 82.81%] [G loss: 2.653152]\n",
      "epoch:3 step:2637 [D loss: 0.399106, acc.: 79.69%] [G loss: 2.692109]\n",
      "epoch:3 step:2638 [D loss: 0.344219, acc.: 83.59%] [G loss: 2.843400]\n",
      "epoch:3 step:2639 [D loss: 0.370663, acc.: 85.94%] [G loss: 2.952833]\n",
      "epoch:3 step:2640 [D loss: 0.299628, acc.: 90.62%] [G loss: 2.282583]\n",
      "epoch:3 step:2641 [D loss: 0.431442, acc.: 82.03%] [G loss: 1.986357]\n",
      "epoch:3 step:2642 [D loss: 0.404107, acc.: 84.38%] [G loss: 2.483741]\n",
      "epoch:3 step:2643 [D loss: 0.502926, acc.: 71.88%] [G loss: 2.982158]\n",
      "epoch:3 step:2644 [D loss: 0.403588, acc.: 82.81%] [G loss: 4.146708]\n",
      "epoch:3 step:2645 [D loss: 0.596794, acc.: 72.66%] [G loss: 5.643829]\n",
      "epoch:3 step:2646 [D loss: 0.549396, acc.: 77.34%] [G loss: 3.054632]\n",
      "epoch:3 step:2647 [D loss: 0.377123, acc.: 82.03%] [G loss: 3.473825]\n",
      "epoch:3 step:2648 [D loss: 0.398276, acc.: 81.25%] [G loss: 3.136703]\n",
      "epoch:3 step:2649 [D loss: 0.406798, acc.: 82.81%] [G loss: 2.543123]\n",
      "epoch:3 step:2650 [D loss: 0.338086, acc.: 82.81%] [G loss: 3.141411]\n",
      "epoch:3 step:2651 [D loss: 0.415743, acc.: 82.81%] [G loss: 2.315170]\n",
      "epoch:3 step:2652 [D loss: 0.484008, acc.: 78.12%] [G loss: 3.634743]\n",
      "epoch:3 step:2653 [D loss: 0.431151, acc.: 82.03%] [G loss: 2.549419]\n",
      "epoch:3 step:2654 [D loss: 0.326904, acc.: 85.16%] [G loss: 2.622220]\n",
      "epoch:3 step:2655 [D loss: 0.375881, acc.: 81.25%] [G loss: 2.470043]\n",
      "epoch:3 step:2656 [D loss: 0.392604, acc.: 83.59%] [G loss: 2.776314]\n",
      "epoch:3 step:2657 [D loss: 0.453007, acc.: 82.03%] [G loss: 2.448776]\n",
      "epoch:3 step:2658 [D loss: 0.463849, acc.: 82.81%] [G loss: 2.304788]\n",
      "epoch:3 step:2659 [D loss: 0.483299, acc.: 77.34%] [G loss: 1.816918]\n",
      "epoch:3 step:2660 [D loss: 0.505575, acc.: 74.22%] [G loss: 2.517401]\n",
      "epoch:3 step:2661 [D loss: 0.354417, acc.: 87.50%] [G loss: 3.078168]\n",
      "epoch:3 step:2662 [D loss: 0.334265, acc.: 89.84%] [G loss: 3.866632]\n",
      "epoch:3 step:2663 [D loss: 0.372366, acc.: 85.94%] [G loss: 3.017053]\n",
      "epoch:3 step:2664 [D loss: 0.336597, acc.: 88.28%] [G loss: 3.008277]\n",
      "epoch:3 step:2665 [D loss: 0.431265, acc.: 80.47%] [G loss: 2.780311]\n",
      "epoch:3 step:2666 [D loss: 0.406384, acc.: 78.91%] [G loss: 3.214951]\n",
      "epoch:3 step:2667 [D loss: 0.489403, acc.: 73.44%] [G loss: 2.179382]\n",
      "epoch:3 step:2668 [D loss: 0.411475, acc.: 79.69%] [G loss: 2.215542]\n",
      "epoch:3 step:2669 [D loss: 0.359198, acc.: 88.28%] [G loss: 2.655290]\n",
      "epoch:3 step:2670 [D loss: 0.428433, acc.: 82.03%] [G loss: 3.027923]\n",
      "epoch:3 step:2671 [D loss: 0.347644, acc.: 86.72%] [G loss: 3.036126]\n",
      "epoch:3 step:2672 [D loss: 0.412123, acc.: 87.50%] [G loss: 1.773546]\n",
      "epoch:3 step:2673 [D loss: 0.355358, acc.: 90.62%] [G loss: 2.432643]\n",
      "epoch:3 step:2674 [D loss: 0.409948, acc.: 79.69%] [G loss: 3.050799]\n",
      "epoch:3 step:2675 [D loss: 0.370626, acc.: 86.72%] [G loss: 2.558938]\n",
      "epoch:3 step:2676 [D loss: 0.367868, acc.: 83.59%] [G loss: 2.337047]\n",
      "epoch:3 step:2677 [D loss: 0.345893, acc.: 86.72%] [G loss: 2.247741]\n",
      "epoch:3 step:2678 [D loss: 0.413631, acc.: 82.81%] [G loss: 2.375833]\n",
      "epoch:3 step:2679 [D loss: 0.385868, acc.: 82.03%] [G loss: 2.642329]\n",
      "epoch:3 step:2680 [D loss: 0.359748, acc.: 88.28%] [G loss: 2.541017]\n",
      "epoch:3 step:2681 [D loss: 0.316170, acc.: 88.28%] [G loss: 3.232153]\n",
      "epoch:3 step:2682 [D loss: 0.356976, acc.: 84.38%] [G loss: 3.320786]\n",
      "epoch:3 step:2683 [D loss: 0.430591, acc.: 82.81%] [G loss: 2.496096]\n",
      "epoch:3 step:2684 [D loss: 0.384626, acc.: 82.03%] [G loss: 2.629035]\n",
      "epoch:3 step:2685 [D loss: 0.378198, acc.: 84.38%] [G loss: 2.886481]\n",
      "epoch:3 step:2686 [D loss: 0.364970, acc.: 85.94%] [G loss: 3.432302]\n",
      "epoch:3 step:2687 [D loss: 0.459416, acc.: 71.88%] [G loss: 2.605215]\n",
      "epoch:3 step:2688 [D loss: 0.369980, acc.: 82.81%] [G loss: 2.416117]\n",
      "epoch:3 step:2689 [D loss: 0.388288, acc.: 84.38%] [G loss: 2.483520]\n",
      "epoch:3 step:2690 [D loss: 0.394608, acc.: 82.81%] [G loss: 2.228217]\n",
      "epoch:3 step:2691 [D loss: 0.327665, acc.: 88.28%] [G loss: 2.869605]\n",
      "epoch:3 step:2692 [D loss: 0.452812, acc.: 78.91%] [G loss: 3.615556]\n",
      "epoch:3 step:2693 [D loss: 0.416640, acc.: 81.25%] [G loss: 1.911950]\n",
      "epoch:3 step:2694 [D loss: 0.392219, acc.: 81.25%] [G loss: 4.986560]\n",
      "epoch:3 step:2695 [D loss: 0.858983, acc.: 63.28%] [G loss: 2.789972]\n",
      "epoch:3 step:2696 [D loss: 0.582214, acc.: 71.88%] [G loss: 1.797712]\n",
      "epoch:3 step:2697 [D loss: 0.521928, acc.: 78.12%] [G loss: 2.401981]\n",
      "epoch:3 step:2698 [D loss: 0.423493, acc.: 82.81%] [G loss: 1.872214]\n",
      "epoch:3 step:2699 [D loss: 0.400533, acc.: 82.81%] [G loss: 2.903797]\n",
      "epoch:3 step:2700 [D loss: 0.377045, acc.: 82.81%] [G loss: 2.531473]\n",
      "epoch:3 step:2701 [D loss: 0.395655, acc.: 82.03%] [G loss: 1.980237]\n",
      "epoch:3 step:2702 [D loss: 0.411809, acc.: 85.16%] [G loss: 2.342205]\n",
      "epoch:3 step:2703 [D loss: 0.385491, acc.: 84.38%] [G loss: 2.011051]\n",
      "epoch:3 step:2704 [D loss: 0.430877, acc.: 78.91%] [G loss: 2.001045]\n",
      "epoch:3 step:2705 [D loss: 0.416547, acc.: 82.81%] [G loss: 2.061760]\n",
      "epoch:3 step:2706 [D loss: 0.395682, acc.: 82.81%] [G loss: 2.511558]\n",
      "epoch:3 step:2707 [D loss: 0.375635, acc.: 82.81%] [G loss: 2.837222]\n",
      "epoch:3 step:2708 [D loss: 0.344175, acc.: 85.16%] [G loss: 2.882749]\n",
      "epoch:3 step:2709 [D loss: 0.440124, acc.: 80.47%] [G loss: 2.103476]\n",
      "epoch:3 step:2710 [D loss: 0.374352, acc.: 82.81%] [G loss: 2.311982]\n",
      "epoch:3 step:2711 [D loss: 0.338613, acc.: 88.28%] [G loss: 2.304603]\n",
      "epoch:3 step:2712 [D loss: 0.377632, acc.: 82.03%] [G loss: 1.709313]\n",
      "epoch:3 step:2713 [D loss: 0.337561, acc.: 83.59%] [G loss: 2.914743]\n",
      "epoch:3 step:2714 [D loss: 0.394532, acc.: 79.69%] [G loss: 2.463289]\n",
      "epoch:3 step:2715 [D loss: 0.365347, acc.: 85.94%] [G loss: 2.391816]\n",
      "epoch:3 step:2716 [D loss: 0.439962, acc.: 82.81%] [G loss: 3.636463]\n",
      "epoch:3 step:2717 [D loss: 0.658301, acc.: 63.28%] [G loss: 2.824896]\n",
      "epoch:3 step:2718 [D loss: 0.444244, acc.: 76.56%] [G loss: 2.602683]\n",
      "epoch:3 step:2719 [D loss: 0.373854, acc.: 83.59%] [G loss: 2.868886]\n",
      "epoch:3 step:2720 [D loss: 0.320539, acc.: 89.06%] [G loss: 2.285410]\n",
      "epoch:3 step:2721 [D loss: 0.380288, acc.: 81.25%] [G loss: 2.617189]\n",
      "epoch:3 step:2722 [D loss: 0.347857, acc.: 86.72%] [G loss: 2.888165]\n",
      "epoch:3 step:2723 [D loss: 0.380419, acc.: 84.38%] [G loss: 2.711060]\n",
      "epoch:3 step:2724 [D loss: 0.315989, acc.: 89.06%] [G loss: 3.028214]\n",
      "epoch:3 step:2725 [D loss: 0.365380, acc.: 84.38%] [G loss: 2.901679]\n",
      "epoch:3 step:2726 [D loss: 0.316974, acc.: 82.81%] [G loss: 3.748919]\n",
      "epoch:3 step:2727 [D loss: 0.324281, acc.: 85.94%] [G loss: 2.969229]\n",
      "epoch:3 step:2728 [D loss: 0.308585, acc.: 89.06%] [G loss: 3.170477]\n",
      "epoch:3 step:2729 [D loss: 0.417328, acc.: 75.78%] [G loss: 2.535772]\n",
      "epoch:3 step:2730 [D loss: 0.368434, acc.: 88.28%] [G loss: 2.729133]\n",
      "epoch:3 step:2731 [D loss: 0.418895, acc.: 76.56%] [G loss: 2.557822]\n",
      "epoch:3 step:2732 [D loss: 0.415321, acc.: 82.03%] [G loss: 3.589276]\n",
      "epoch:3 step:2733 [D loss: 0.341089, acc.: 85.16%] [G loss: 3.159572]\n",
      "epoch:3 step:2734 [D loss: 0.447400, acc.: 78.12%] [G loss: 2.207030]\n",
      "epoch:3 step:2735 [D loss: 0.417416, acc.: 80.47%] [G loss: 2.346676]\n",
      "epoch:3 step:2736 [D loss: 0.356483, acc.: 87.50%] [G loss: 2.744687]\n",
      "epoch:3 step:2737 [D loss: 0.348935, acc.: 82.03%] [G loss: 3.569448]\n",
      "epoch:3 step:2738 [D loss: 0.368308, acc.: 83.59%] [G loss: 2.622528]\n",
      "epoch:3 step:2739 [D loss: 0.378905, acc.: 81.25%] [G loss: 3.498342]\n",
      "epoch:3 step:2740 [D loss: 0.394705, acc.: 78.91%] [G loss: 4.341826]\n",
      "epoch:3 step:2741 [D loss: 0.510941, acc.: 77.34%] [G loss: 2.180286]\n",
      "epoch:3 step:2742 [D loss: 0.442759, acc.: 76.56%] [G loss: 3.689279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2743 [D loss: 0.333340, acc.: 85.94%] [G loss: 2.395343]\n",
      "epoch:3 step:2744 [D loss: 0.393132, acc.: 80.47%] [G loss: 4.201066]\n",
      "epoch:3 step:2745 [D loss: 0.413023, acc.: 78.91%] [G loss: 2.615273]\n",
      "epoch:3 step:2746 [D loss: 0.351941, acc.: 83.59%] [G loss: 3.361605]\n",
      "epoch:3 step:2747 [D loss: 0.339676, acc.: 86.72%] [G loss: 3.182961]\n",
      "epoch:3 step:2748 [D loss: 0.344739, acc.: 85.16%] [G loss: 3.599364]\n",
      "epoch:3 step:2749 [D loss: 0.372837, acc.: 85.16%] [G loss: 3.118395]\n",
      "epoch:3 step:2750 [D loss: 0.344986, acc.: 84.38%] [G loss: 3.538203]\n",
      "epoch:3 step:2751 [D loss: 0.360952, acc.: 83.59%] [G loss: 2.655468]\n",
      "epoch:3 step:2752 [D loss: 0.315417, acc.: 92.19%] [G loss: 2.780709]\n",
      "epoch:3 step:2753 [D loss: 0.415003, acc.: 76.56%] [G loss: 4.021792]\n",
      "epoch:3 step:2754 [D loss: 0.580496, acc.: 73.44%] [G loss: 5.203413]\n",
      "epoch:3 step:2755 [D loss: 0.989543, acc.: 62.50%] [G loss: 3.800185]\n",
      "epoch:3 step:2756 [D loss: 0.583669, acc.: 73.44%] [G loss: 2.024406]\n",
      "epoch:3 step:2757 [D loss: 0.323267, acc.: 82.03%] [G loss: 3.191786]\n",
      "epoch:3 step:2758 [D loss: 0.355627, acc.: 86.72%] [G loss: 3.130135]\n",
      "epoch:3 step:2759 [D loss: 0.325452, acc.: 86.72%] [G loss: 2.258438]\n",
      "epoch:3 step:2760 [D loss: 0.374067, acc.: 79.69%] [G loss: 3.789996]\n",
      "epoch:3 step:2761 [D loss: 0.400832, acc.: 82.03%] [G loss: 2.657209]\n",
      "epoch:3 step:2762 [D loss: 0.346135, acc.: 84.38%] [G loss: 2.716131]\n",
      "epoch:3 step:2763 [D loss: 0.395774, acc.: 81.25%] [G loss: 2.475810]\n",
      "epoch:3 step:2764 [D loss: 0.381812, acc.: 84.38%] [G loss: 2.336277]\n",
      "epoch:3 step:2765 [D loss: 0.401834, acc.: 79.69%] [G loss: 2.492325]\n",
      "epoch:3 step:2766 [D loss: 0.437364, acc.: 78.91%] [G loss: 3.824611]\n",
      "epoch:3 step:2767 [D loss: 0.443499, acc.: 78.12%] [G loss: 2.606920]\n",
      "epoch:3 step:2768 [D loss: 0.335378, acc.: 85.94%] [G loss: 3.204189]\n",
      "epoch:3 step:2769 [D loss: 0.331015, acc.: 85.16%] [G loss: 2.514362]\n",
      "epoch:3 step:2770 [D loss: 0.303920, acc.: 86.72%] [G loss: 3.186892]\n",
      "epoch:3 step:2771 [D loss: 0.395619, acc.: 82.03%] [G loss: 2.889735]\n",
      "epoch:3 step:2772 [D loss: 0.379975, acc.: 84.38%] [G loss: 2.304857]\n",
      "epoch:3 step:2773 [D loss: 0.425111, acc.: 81.25%] [G loss: 2.680543]\n",
      "epoch:3 step:2774 [D loss: 0.293698, acc.: 93.75%] [G loss: 3.166330]\n",
      "epoch:3 step:2775 [D loss: 0.490014, acc.: 76.56%] [G loss: 2.455453]\n",
      "epoch:3 step:2776 [D loss: 0.392516, acc.: 81.25%] [G loss: 3.223781]\n",
      "epoch:3 step:2777 [D loss: 0.390234, acc.: 80.47%] [G loss: 2.919739]\n",
      "epoch:3 step:2778 [D loss: 0.318027, acc.: 90.62%] [G loss: 2.733413]\n",
      "epoch:3 step:2779 [D loss: 0.391670, acc.: 81.25%] [G loss: 2.207559]\n",
      "epoch:3 step:2780 [D loss: 0.348180, acc.: 86.72%] [G loss: 3.138038]\n",
      "epoch:3 step:2781 [D loss: 0.485539, acc.: 76.56%] [G loss: 2.025411]\n",
      "epoch:3 step:2782 [D loss: 0.394567, acc.: 81.25%] [G loss: 2.113304]\n",
      "epoch:3 step:2783 [D loss: 0.445051, acc.: 76.56%] [G loss: 2.820598]\n",
      "epoch:3 step:2784 [D loss: 0.433635, acc.: 77.34%] [G loss: 2.670362]\n",
      "epoch:3 step:2785 [D loss: 0.416824, acc.: 80.47%] [G loss: 2.493802]\n",
      "epoch:3 step:2786 [D loss: 0.357992, acc.: 85.16%] [G loss: 2.746547]\n",
      "epoch:3 step:2787 [D loss: 0.511315, acc.: 74.22%] [G loss: 1.962066]\n",
      "epoch:3 step:2788 [D loss: 0.459796, acc.: 74.22%] [G loss: 2.406541]\n",
      "epoch:3 step:2789 [D loss: 0.384779, acc.: 85.94%] [G loss: 2.147240]\n",
      "epoch:3 step:2790 [D loss: 0.342441, acc.: 85.16%] [G loss: 2.573293]\n",
      "epoch:3 step:2791 [D loss: 0.428276, acc.: 82.81%] [G loss: 3.033576]\n",
      "epoch:3 step:2792 [D loss: 0.413406, acc.: 84.38%] [G loss: 2.454120]\n",
      "epoch:3 step:2793 [D loss: 0.367144, acc.: 85.94%] [G loss: 2.645921]\n",
      "epoch:3 step:2794 [D loss: 0.349137, acc.: 85.16%] [G loss: 4.370984]\n",
      "epoch:3 step:2795 [D loss: 0.474019, acc.: 73.44%] [G loss: 2.260521]\n",
      "epoch:3 step:2796 [D loss: 0.304997, acc.: 85.94%] [G loss: 2.869460]\n",
      "epoch:3 step:2797 [D loss: 0.364243, acc.: 84.38%] [G loss: 2.617886]\n",
      "epoch:3 step:2798 [D loss: 0.490395, acc.: 76.56%] [G loss: 3.018528]\n",
      "epoch:3 step:2799 [D loss: 0.566572, acc.: 73.44%] [G loss: 2.913347]\n",
      "epoch:3 step:2800 [D loss: 0.802024, acc.: 69.53%] [G loss: 4.317735]\n",
      "epoch:3 step:2801 [D loss: 1.103203, acc.: 53.91%] [G loss: 4.647330]\n",
      "epoch:3 step:2802 [D loss: 0.577112, acc.: 80.47%] [G loss: 3.091764]\n",
      "epoch:3 step:2803 [D loss: 0.552692, acc.: 67.19%] [G loss: 2.602258]\n",
      "epoch:3 step:2804 [D loss: 0.330310, acc.: 85.16%] [G loss: 3.385198]\n",
      "epoch:3 step:2805 [D loss: 0.446939, acc.: 78.12%] [G loss: 2.536117]\n",
      "epoch:3 step:2806 [D loss: 0.492009, acc.: 75.00%] [G loss: 2.406679]\n",
      "epoch:3 step:2807 [D loss: 0.397901, acc.: 85.16%] [G loss: 2.743862]\n",
      "epoch:3 step:2808 [D loss: 0.364697, acc.: 84.38%] [G loss: 2.970552]\n",
      "epoch:3 step:2809 [D loss: 0.413423, acc.: 76.56%] [G loss: 2.312222]\n",
      "epoch:3 step:2810 [D loss: 0.340981, acc.: 87.50%] [G loss: 2.544044]\n",
      "epoch:3 step:2811 [D loss: 0.379370, acc.: 85.94%] [G loss: 2.538541]\n",
      "epoch:3 step:2812 [D loss: 0.343398, acc.: 86.72%] [G loss: 2.084714]\n",
      "epoch:3 step:2813 [D loss: 0.401145, acc.: 82.81%] [G loss: 2.372325]\n",
      "epoch:3 step:2814 [D loss: 0.372676, acc.: 81.25%] [G loss: 2.090239]\n",
      "epoch:3 step:2815 [D loss: 0.380545, acc.: 83.59%] [G loss: 2.607713]\n",
      "epoch:3 step:2816 [D loss: 0.338498, acc.: 82.03%] [G loss: 2.579524]\n",
      "epoch:3 step:2817 [D loss: 0.364019, acc.: 82.03%] [G loss: 2.781545]\n",
      "epoch:3 step:2818 [D loss: 0.380802, acc.: 81.25%] [G loss: 2.567747]\n",
      "epoch:3 step:2819 [D loss: 0.412392, acc.: 79.69%] [G loss: 2.825000]\n",
      "epoch:3 step:2820 [D loss: 0.327897, acc.: 89.06%] [G loss: 2.810703]\n",
      "epoch:3 step:2821 [D loss: 0.342452, acc.: 86.72%] [G loss: 3.198074]\n",
      "epoch:3 step:2822 [D loss: 0.296304, acc.: 88.28%] [G loss: 2.811288]\n",
      "epoch:3 step:2823 [D loss: 0.330561, acc.: 86.72%] [G loss: 2.139002]\n",
      "epoch:3 step:2824 [D loss: 0.316124, acc.: 85.16%] [G loss: 2.080379]\n",
      "epoch:3 step:2825 [D loss: 0.377466, acc.: 86.72%] [G loss: 3.200360]\n",
      "epoch:3 step:2826 [D loss: 0.455804, acc.: 75.78%] [G loss: 2.699689]\n",
      "epoch:3 step:2827 [D loss: 0.489430, acc.: 77.34%] [G loss: 2.744484]\n",
      "epoch:3 step:2828 [D loss: 0.402794, acc.: 77.34%] [G loss: 2.837923]\n",
      "epoch:3 step:2829 [D loss: 0.380173, acc.: 85.16%] [G loss: 2.297529]\n",
      "epoch:3 step:2830 [D loss: 0.416318, acc.: 81.25%] [G loss: 2.746313]\n",
      "epoch:3 step:2831 [D loss: 0.397573, acc.: 80.47%] [G loss: 2.303459]\n",
      "epoch:3 step:2832 [D loss: 0.420976, acc.: 78.12%] [G loss: 2.835477]\n",
      "epoch:3 step:2833 [D loss: 0.454928, acc.: 77.34%] [G loss: 3.255451]\n",
      "epoch:3 step:2834 [D loss: 0.358016, acc.: 84.38%] [G loss: 2.978177]\n",
      "epoch:3 step:2835 [D loss: 0.379792, acc.: 83.59%] [G loss: 2.268581]\n",
      "epoch:3 step:2836 [D loss: 0.385843, acc.: 83.59%] [G loss: 2.239110]\n",
      "epoch:3 step:2837 [D loss: 0.409020, acc.: 80.47%] [G loss: 2.456470]\n",
      "epoch:3 step:2838 [D loss: 0.346628, acc.: 85.16%] [G loss: 2.432704]\n",
      "epoch:3 step:2839 [D loss: 0.385106, acc.: 86.72%] [G loss: 2.463847]\n",
      "epoch:3 step:2840 [D loss: 0.386686, acc.: 83.59%] [G loss: 2.109145]\n",
      "epoch:3 step:2841 [D loss: 0.389942, acc.: 80.47%] [G loss: 2.586314]\n",
      "epoch:3 step:2842 [D loss: 0.431666, acc.: 80.47%] [G loss: 2.436769]\n",
      "epoch:3 step:2843 [D loss: 0.367604, acc.: 83.59%] [G loss: 2.851736]\n",
      "epoch:3 step:2844 [D loss: 0.375708, acc.: 85.94%] [G loss: 2.539369]\n",
      "epoch:3 step:2845 [D loss: 0.387976, acc.: 77.34%] [G loss: 2.874203]\n",
      "epoch:3 step:2846 [D loss: 0.368768, acc.: 84.38%] [G loss: 2.155030]\n",
      "epoch:3 step:2847 [D loss: 0.354369, acc.: 87.50%] [G loss: 2.969333]\n",
      "epoch:3 step:2848 [D loss: 0.376869, acc.: 84.38%] [G loss: 2.512984]\n",
      "epoch:3 step:2849 [D loss: 0.327671, acc.: 86.72%] [G loss: 3.671132]\n",
      "epoch:3 step:2850 [D loss: 0.454636, acc.: 74.22%] [G loss: 2.800182]\n",
      "epoch:3 step:2851 [D loss: 0.431179, acc.: 79.69%] [G loss: 3.873699]\n",
      "epoch:3 step:2852 [D loss: 0.414337, acc.: 77.34%] [G loss: 3.287411]\n",
      "epoch:3 step:2853 [D loss: 0.375759, acc.: 82.03%] [G loss: 2.929305]\n",
      "epoch:3 step:2854 [D loss: 0.341942, acc.: 87.50%] [G loss: 2.545633]\n",
      "epoch:3 step:2855 [D loss: 0.262373, acc.: 91.41%] [G loss: 3.532225]\n",
      "epoch:3 step:2856 [D loss: 0.382635, acc.: 80.47%] [G loss: 1.623302]\n",
      "epoch:3 step:2857 [D loss: 0.431362, acc.: 79.69%] [G loss: 3.283199]\n",
      "epoch:3 step:2858 [D loss: 0.377977, acc.: 82.03%] [G loss: 2.647353]\n",
      "epoch:3 step:2859 [D loss: 0.385084, acc.: 82.81%] [G loss: 3.691977]\n",
      "epoch:3 step:2860 [D loss: 0.324392, acc.: 88.28%] [G loss: 3.287298]\n",
      "epoch:3 step:2861 [D loss: 0.418116, acc.: 76.56%] [G loss: 2.274626]\n",
      "epoch:3 step:2862 [D loss: 0.368343, acc.: 85.94%] [G loss: 2.797257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2863 [D loss: 0.545764, acc.: 67.97%] [G loss: 4.081220]\n",
      "epoch:3 step:2864 [D loss: 0.561204, acc.: 75.00%] [G loss: 4.035806]\n",
      "epoch:3 step:2865 [D loss: 0.583691, acc.: 72.66%] [G loss: 2.828908]\n",
      "epoch:3 step:2866 [D loss: 0.536955, acc.: 70.31%] [G loss: 2.601668]\n",
      "epoch:3 step:2867 [D loss: 0.476938, acc.: 81.25%] [G loss: 2.907071]\n",
      "epoch:3 step:2868 [D loss: 0.500765, acc.: 70.31%] [G loss: 2.342094]\n",
      "epoch:3 step:2869 [D loss: 0.471172, acc.: 79.69%] [G loss: 2.097250]\n",
      "epoch:3 step:2870 [D loss: 0.326910, acc.: 86.72%] [G loss: 3.453161]\n",
      "epoch:3 step:2871 [D loss: 0.466417, acc.: 78.91%] [G loss: 2.314261]\n",
      "epoch:3 step:2872 [D loss: 0.450991, acc.: 78.91%] [G loss: 2.953465]\n",
      "epoch:3 step:2873 [D loss: 0.489658, acc.: 74.22%] [G loss: 2.745228]\n",
      "epoch:3 step:2874 [D loss: 0.362135, acc.: 86.72%] [G loss: 2.722172]\n",
      "epoch:3 step:2875 [D loss: 0.397171, acc.: 82.81%] [G loss: 2.774255]\n",
      "epoch:3 step:2876 [D loss: 0.313373, acc.: 89.06%] [G loss: 2.390289]\n",
      "epoch:3 step:2877 [D loss: 0.416483, acc.: 79.69%] [G loss: 3.470515]\n",
      "epoch:3 step:2878 [D loss: 0.474213, acc.: 76.56%] [G loss: 2.707987]\n",
      "epoch:3 step:2879 [D loss: 0.454445, acc.: 78.12%] [G loss: 2.734829]\n",
      "epoch:3 step:2880 [D loss: 0.504047, acc.: 75.00%] [G loss: 2.991757]\n",
      "epoch:3 step:2881 [D loss: 0.479768, acc.: 76.56%] [G loss: 4.001282]\n",
      "epoch:3 step:2882 [D loss: 0.454401, acc.: 78.91%] [G loss: 3.456885]\n",
      "epoch:3 step:2883 [D loss: 0.328936, acc.: 88.28%] [G loss: 2.561185]\n",
      "epoch:3 step:2884 [D loss: 0.346709, acc.: 85.16%] [G loss: 3.951463]\n",
      "epoch:3 step:2885 [D loss: 0.417532, acc.: 82.03%] [G loss: 2.541487]\n",
      "epoch:3 step:2886 [D loss: 0.429265, acc.: 82.03%] [G loss: 2.685547]\n",
      "epoch:3 step:2887 [D loss: 0.322028, acc.: 86.72%] [G loss: 2.335489]\n",
      "epoch:3 step:2888 [D loss: 0.403621, acc.: 79.69%] [G loss: 2.686970]\n",
      "epoch:3 step:2889 [D loss: 0.469361, acc.: 82.81%] [G loss: 2.151696]\n",
      "epoch:3 step:2890 [D loss: 0.484756, acc.: 75.78%] [G loss: 3.977291]\n",
      "epoch:3 step:2891 [D loss: 0.331570, acc.: 86.72%] [G loss: 3.677540]\n",
      "epoch:3 step:2892 [D loss: 0.329943, acc.: 81.25%] [G loss: 4.271299]\n",
      "epoch:3 step:2893 [D loss: 0.344140, acc.: 85.94%] [G loss: 3.963066]\n",
      "epoch:3 step:2894 [D loss: 0.288122, acc.: 86.72%] [G loss: 3.459458]\n",
      "epoch:3 step:2895 [D loss: 0.378032, acc.: 78.91%] [G loss: 2.625735]\n",
      "epoch:3 step:2896 [D loss: 0.448786, acc.: 76.56%] [G loss: 2.374155]\n",
      "epoch:3 step:2897 [D loss: 0.343424, acc.: 83.59%] [G loss: 2.204086]\n",
      "epoch:3 step:2898 [D loss: 0.369565, acc.: 86.72%] [G loss: 2.047400]\n",
      "epoch:3 step:2899 [D loss: 0.353239, acc.: 82.81%] [G loss: 2.577267]\n",
      "epoch:3 step:2900 [D loss: 0.353517, acc.: 84.38%] [G loss: 2.375316]\n",
      "epoch:3 step:2901 [D loss: 0.412443, acc.: 78.12%] [G loss: 3.049247]\n",
      "epoch:3 step:2902 [D loss: 0.405166, acc.: 82.81%] [G loss: 3.517754]\n",
      "epoch:3 step:2903 [D loss: 0.390090, acc.: 80.47%] [G loss: 2.267279]\n",
      "epoch:3 step:2904 [D loss: 0.293719, acc.: 89.06%] [G loss: 3.032089]\n",
      "epoch:3 step:2905 [D loss: 0.322382, acc.: 89.84%] [G loss: 2.697313]\n",
      "epoch:3 step:2906 [D loss: 0.447093, acc.: 79.69%] [G loss: 3.110214]\n",
      "epoch:3 step:2907 [D loss: 0.340278, acc.: 82.03%] [G loss: 2.587250]\n",
      "epoch:3 step:2908 [D loss: 0.424494, acc.: 80.47%] [G loss: 2.773887]\n",
      "epoch:3 step:2909 [D loss: 0.334297, acc.: 85.94%] [G loss: 3.677942]\n",
      "epoch:3 step:2910 [D loss: 0.356695, acc.: 85.16%] [G loss: 4.026829]\n",
      "epoch:3 step:2911 [D loss: 0.373297, acc.: 82.03%] [G loss: 3.117551]\n",
      "epoch:3 step:2912 [D loss: 0.312444, acc.: 84.38%] [G loss: 3.671215]\n",
      "epoch:3 step:2913 [D loss: 0.325008, acc.: 87.50%] [G loss: 3.386356]\n",
      "epoch:3 step:2914 [D loss: 0.291420, acc.: 87.50%] [G loss: 2.827955]\n",
      "epoch:3 step:2915 [D loss: 0.366279, acc.: 84.38%] [G loss: 3.452043]\n",
      "epoch:3 step:2916 [D loss: 0.420043, acc.: 78.12%] [G loss: 2.621009]\n",
      "epoch:3 step:2917 [D loss: 0.428992, acc.: 80.47%] [G loss: 2.642812]\n",
      "epoch:3 step:2918 [D loss: 0.419648, acc.: 78.12%] [G loss: 2.905068]\n",
      "epoch:3 step:2919 [D loss: 0.368949, acc.: 84.38%] [G loss: 3.272459]\n",
      "epoch:3 step:2920 [D loss: 0.411922, acc.: 79.69%] [G loss: 3.191659]\n",
      "epoch:3 step:2921 [D loss: 0.487385, acc.: 78.12%] [G loss: 4.161829]\n",
      "epoch:3 step:2922 [D loss: 0.701136, acc.: 66.41%] [G loss: 4.765362]\n",
      "epoch:3 step:2923 [D loss: 0.898211, acc.: 64.06%] [G loss: 3.691231]\n",
      "epoch:3 step:2924 [D loss: 0.548337, acc.: 71.09%] [G loss: 2.826901]\n",
      "epoch:3 step:2925 [D loss: 0.501552, acc.: 78.12%] [G loss: 4.646996]\n",
      "epoch:3 step:2926 [D loss: 0.696750, acc.: 63.28%] [G loss: 2.277645]\n",
      "epoch:3 step:2927 [D loss: 0.391171, acc.: 80.47%] [G loss: 3.007452]\n",
      "epoch:3 step:2928 [D loss: 0.428384, acc.: 82.03%] [G loss: 2.384117]\n",
      "epoch:3 step:2929 [D loss: 0.363145, acc.: 83.59%] [G loss: 3.420411]\n",
      "epoch:3 step:2930 [D loss: 0.334091, acc.: 85.16%] [G loss: 2.093920]\n",
      "epoch:3 step:2931 [D loss: 0.363577, acc.: 82.03%] [G loss: 2.504786]\n",
      "epoch:3 step:2932 [D loss: 0.314649, acc.: 83.59%] [G loss: 3.114270]\n",
      "epoch:3 step:2933 [D loss: 0.419074, acc.: 78.91%] [G loss: 2.209912]\n",
      "epoch:3 step:2934 [D loss: 0.304508, acc.: 89.06%] [G loss: 3.652268]\n",
      "epoch:3 step:2935 [D loss: 0.369785, acc.: 85.16%] [G loss: 2.442986]\n",
      "epoch:3 step:2936 [D loss: 0.327349, acc.: 85.94%] [G loss: 2.438708]\n",
      "epoch:3 step:2937 [D loss: 0.379222, acc.: 82.81%] [G loss: 2.959423]\n",
      "epoch:3 step:2938 [D loss: 0.308158, acc.: 88.28%] [G loss: 2.712937]\n",
      "epoch:3 step:2939 [D loss: 0.380707, acc.: 85.16%] [G loss: 2.919323]\n",
      "epoch:3 step:2940 [D loss: 0.472162, acc.: 75.78%] [G loss: 2.744563]\n",
      "epoch:3 step:2941 [D loss: 0.298433, acc.: 86.72%] [G loss: 3.678239]\n",
      "epoch:3 step:2942 [D loss: 0.435961, acc.: 78.12%] [G loss: 1.947788]\n",
      "epoch:3 step:2943 [D loss: 0.404406, acc.: 82.03%] [G loss: 2.072468]\n",
      "epoch:3 step:2944 [D loss: 0.428446, acc.: 78.12%] [G loss: 3.099125]\n",
      "epoch:3 step:2945 [D loss: 0.445415, acc.: 78.12%] [G loss: 3.660593]\n",
      "epoch:3 step:2946 [D loss: 0.425667, acc.: 77.34%] [G loss: 2.801299]\n",
      "epoch:3 step:2947 [D loss: 0.402851, acc.: 80.47%] [G loss: 3.027404]\n",
      "epoch:3 step:2948 [D loss: 0.393499, acc.: 83.59%] [G loss: 2.986237]\n",
      "epoch:3 step:2949 [D loss: 0.414905, acc.: 78.91%] [G loss: 2.195386]\n",
      "epoch:3 step:2950 [D loss: 0.345102, acc.: 86.72%] [G loss: 2.267035]\n",
      "epoch:3 step:2951 [D loss: 0.396840, acc.: 81.25%] [G loss: 3.147185]\n",
      "epoch:3 step:2952 [D loss: 0.382967, acc.: 85.16%] [G loss: 2.966565]\n",
      "epoch:3 step:2953 [D loss: 0.388643, acc.: 81.25%] [G loss: 2.779802]\n",
      "epoch:3 step:2954 [D loss: 0.432481, acc.: 75.00%] [G loss: 3.445158]\n",
      "epoch:3 step:2955 [D loss: 0.434309, acc.: 79.69%] [G loss: 3.220946]\n",
      "epoch:3 step:2956 [D loss: 0.506338, acc.: 74.22%] [G loss: 2.749320]\n",
      "epoch:3 step:2957 [D loss: 0.523880, acc.: 77.34%] [G loss: 2.992598]\n",
      "epoch:3 step:2958 [D loss: 0.370249, acc.: 84.38%] [G loss: 2.465090]\n",
      "epoch:3 step:2959 [D loss: 0.379083, acc.: 86.72%] [G loss: 3.169421]\n",
      "epoch:3 step:2960 [D loss: 0.549529, acc.: 75.78%] [G loss: 3.806792]\n",
      "epoch:3 step:2961 [D loss: 0.542074, acc.: 71.09%] [G loss: 2.551683]\n",
      "epoch:3 step:2962 [D loss: 0.392423, acc.: 82.81%] [G loss: 3.251908]\n",
      "epoch:3 step:2963 [D loss: 0.364685, acc.: 83.59%] [G loss: 2.361659]\n",
      "epoch:3 step:2964 [D loss: 0.441142, acc.: 78.91%] [G loss: 2.681292]\n",
      "epoch:3 step:2965 [D loss: 0.323915, acc.: 88.28%] [G loss: 3.677662]\n",
      "epoch:3 step:2966 [D loss: 0.443948, acc.: 75.00%] [G loss: 2.286455]\n",
      "epoch:3 step:2967 [D loss: 0.292901, acc.: 89.06%] [G loss: 3.284356]\n",
      "epoch:3 step:2968 [D loss: 0.360976, acc.: 85.94%] [G loss: 2.725060]\n",
      "epoch:3 step:2969 [D loss: 0.416302, acc.: 77.34%] [G loss: 2.561152]\n",
      "epoch:3 step:2970 [D loss: 0.421526, acc.: 78.91%] [G loss: 2.646176]\n",
      "epoch:3 step:2971 [D loss: 0.386064, acc.: 78.12%] [G loss: 2.797902]\n",
      "epoch:3 step:2972 [D loss: 0.307084, acc.: 91.41%] [G loss: 2.267308]\n",
      "epoch:3 step:2973 [D loss: 0.364576, acc.: 85.94%] [G loss: 3.251404]\n",
      "epoch:3 step:2974 [D loss: 0.321201, acc.: 86.72%] [G loss: 3.540505]\n",
      "epoch:3 step:2975 [D loss: 0.526782, acc.: 72.66%] [G loss: 4.629388]\n",
      "epoch:3 step:2976 [D loss: 0.537715, acc.: 75.00%] [G loss: 2.656543]\n",
      "epoch:3 step:2977 [D loss: 0.605578, acc.: 74.22%] [G loss: 3.508500]\n",
      "epoch:3 step:2978 [D loss: 0.533241, acc.: 75.78%] [G loss: 2.320920]\n",
      "epoch:3 step:2979 [D loss: 0.413546, acc.: 78.12%] [G loss: 2.114761]\n",
      "epoch:3 step:2980 [D loss: 0.347588, acc.: 83.59%] [G loss: 2.435563]\n",
      "epoch:3 step:2981 [D loss: 0.340436, acc.: 86.72%] [G loss: 2.485885]\n",
      "epoch:3 step:2982 [D loss: 0.511487, acc.: 75.00%] [G loss: 2.794038]\n",
      "epoch:3 step:2983 [D loss: 0.294631, acc.: 89.06%] [G loss: 2.624632]\n",
      "epoch:3 step:2984 [D loss: 0.353217, acc.: 86.72%] [G loss: 2.441846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2985 [D loss: 0.369720, acc.: 82.81%] [G loss: 2.193583]\n",
      "epoch:3 step:2986 [D loss: 0.405344, acc.: 77.34%] [G loss: 1.513897]\n",
      "epoch:3 step:2987 [D loss: 0.357547, acc.: 80.47%] [G loss: 2.571699]\n",
      "epoch:3 step:2988 [D loss: 0.401206, acc.: 80.47%] [G loss: 2.148126]\n",
      "epoch:3 step:2989 [D loss: 0.395958, acc.: 82.81%] [G loss: 2.951865]\n",
      "epoch:3 step:2990 [D loss: 0.358794, acc.: 88.28%] [G loss: 2.929326]\n",
      "epoch:3 step:2991 [D loss: 0.367628, acc.: 78.91%] [G loss: 2.823687]\n",
      "epoch:3 step:2992 [D loss: 0.417259, acc.: 80.47%] [G loss: 3.420185]\n",
      "epoch:3 step:2993 [D loss: 0.337731, acc.: 85.16%] [G loss: 4.046212]\n",
      "epoch:3 step:2994 [D loss: 0.421367, acc.: 82.03%] [G loss: 4.013624]\n",
      "epoch:3 step:2995 [D loss: 0.847837, acc.: 61.72%] [G loss: 3.557257]\n",
      "epoch:3 step:2996 [D loss: 0.744623, acc.: 67.97%] [G loss: 3.526572]\n",
      "epoch:3 step:2997 [D loss: 0.381643, acc.: 80.47%] [G loss: 2.200808]\n",
      "epoch:3 step:2998 [D loss: 0.447103, acc.: 75.00%] [G loss: 2.887545]\n",
      "epoch:3 step:2999 [D loss: 0.417196, acc.: 81.25%] [G loss: 2.401366]\n",
      "epoch:3 step:3000 [D loss: 0.420275, acc.: 81.25%] [G loss: 2.027946]\n",
      "epoch:3 step:3001 [D loss: 0.276275, acc.: 91.41%] [G loss: 2.981903]\n",
      "epoch:3 step:3002 [D loss: 0.377599, acc.: 82.03%] [G loss: 2.886062]\n",
      "epoch:3 step:3003 [D loss: 0.289126, acc.: 90.62%] [G loss: 3.886373]\n",
      "epoch:3 step:3004 [D loss: 0.383183, acc.: 83.59%] [G loss: 2.559653]\n",
      "epoch:3 step:3005 [D loss: 0.367716, acc.: 84.38%] [G loss: 2.110509]\n",
      "epoch:3 step:3006 [D loss: 0.302061, acc.: 89.06%] [G loss: 2.599502]\n",
      "epoch:3 step:3007 [D loss: 0.384992, acc.: 83.59%] [G loss: 2.260007]\n",
      "epoch:3 step:3008 [D loss: 0.363967, acc.: 82.03%] [G loss: 1.935056]\n",
      "epoch:3 step:3009 [D loss: 0.387933, acc.: 79.69%] [G loss: 2.767869]\n",
      "epoch:3 step:3010 [D loss: 0.285594, acc.: 92.19%] [G loss: 2.261332]\n",
      "epoch:3 step:3011 [D loss: 0.386924, acc.: 80.47%] [G loss: 2.195647]\n",
      "epoch:3 step:3012 [D loss: 0.351977, acc.: 81.25%] [G loss: 2.386550]\n",
      "epoch:3 step:3013 [D loss: 0.488991, acc.: 71.09%] [G loss: 2.155985]\n",
      "epoch:3 step:3014 [D loss: 0.346502, acc.: 86.72%] [G loss: 2.279435]\n",
      "epoch:3 step:3015 [D loss: 0.302199, acc.: 89.84%] [G loss: 3.024650]\n",
      "epoch:3 step:3016 [D loss: 0.353842, acc.: 85.94%] [G loss: 2.443304]\n",
      "epoch:3 step:3017 [D loss: 0.384354, acc.: 86.72%] [G loss: 2.267656]\n",
      "epoch:3 step:3018 [D loss: 0.409436, acc.: 81.25%] [G loss: 3.864226]\n",
      "epoch:3 step:3019 [D loss: 0.548953, acc.: 73.44%] [G loss: 3.538097]\n",
      "epoch:3 step:3020 [D loss: 0.548695, acc.: 69.53%] [G loss: 2.461915]\n",
      "epoch:3 step:3021 [D loss: 0.333051, acc.: 86.72%] [G loss: 2.663865]\n",
      "epoch:3 step:3022 [D loss: 0.379214, acc.: 79.69%] [G loss: 2.142889]\n",
      "epoch:3 step:3023 [D loss: 0.339708, acc.: 85.16%] [G loss: 3.052526]\n",
      "epoch:3 step:3024 [D loss: 0.308291, acc.: 89.06%] [G loss: 2.438611]\n",
      "epoch:3 step:3025 [D loss: 0.449127, acc.: 79.69%] [G loss: 2.643593]\n",
      "epoch:3 step:3026 [D loss: 0.434503, acc.: 81.25%] [G loss: 2.332145]\n",
      "epoch:3 step:3027 [D loss: 0.298087, acc.: 90.62%] [G loss: 2.842298]\n",
      "epoch:3 step:3028 [D loss: 0.364010, acc.: 82.03%] [G loss: 2.171608]\n",
      "epoch:3 step:3029 [D loss: 0.346711, acc.: 82.03%] [G loss: 2.817489]\n",
      "epoch:3 step:3030 [D loss: 0.309604, acc.: 87.50%] [G loss: 4.055238]\n",
      "epoch:3 step:3031 [D loss: 0.354871, acc.: 86.72%] [G loss: 2.508133]\n",
      "epoch:3 step:3032 [D loss: 0.311071, acc.: 88.28%] [G loss: 2.602738]\n",
      "epoch:3 step:3033 [D loss: 0.273639, acc.: 92.97%] [G loss: 2.490759]\n",
      "epoch:3 step:3034 [D loss: 0.390634, acc.: 79.69%] [G loss: 2.024051]\n",
      "epoch:3 step:3035 [D loss: 0.514618, acc.: 68.75%] [G loss: 1.984626]\n",
      "epoch:3 step:3036 [D loss: 0.305306, acc.: 87.50%] [G loss: 1.946700]\n",
      "epoch:3 step:3037 [D loss: 0.412154, acc.: 80.47%] [G loss: 2.288250]\n",
      "epoch:3 step:3038 [D loss: 0.361746, acc.: 86.72%] [G loss: 2.317970]\n",
      "epoch:3 step:3039 [D loss: 0.454799, acc.: 77.34%] [G loss: 2.823601]\n",
      "epoch:3 step:3040 [D loss: 0.453017, acc.: 79.69%] [G loss: 2.594840]\n",
      "epoch:3 step:3041 [D loss: 0.295842, acc.: 91.41%] [G loss: 2.580758]\n",
      "epoch:3 step:3042 [D loss: 0.424073, acc.: 78.91%] [G loss: 2.488899]\n",
      "epoch:3 step:3043 [D loss: 0.348101, acc.: 83.59%] [G loss: 2.407344]\n",
      "epoch:3 step:3044 [D loss: 0.352326, acc.: 82.81%] [G loss: 2.386170]\n",
      "epoch:3 step:3045 [D loss: 0.309666, acc.: 89.84%] [G loss: 3.148835]\n",
      "epoch:3 step:3046 [D loss: 0.359690, acc.: 85.16%] [G loss: 2.672954]\n",
      "epoch:3 step:3047 [D loss: 0.312942, acc.: 84.38%] [G loss: 2.875747]\n",
      "epoch:3 step:3048 [D loss: 0.285507, acc.: 91.41%] [G loss: 2.978283]\n",
      "epoch:3 step:3049 [D loss: 0.381377, acc.: 81.25%] [G loss: 2.588964]\n",
      "epoch:3 step:3050 [D loss: 0.376908, acc.: 82.81%] [G loss: 2.880575]\n",
      "epoch:3 step:3051 [D loss: 0.407604, acc.: 78.12%] [G loss: 2.208271]\n",
      "epoch:3 step:3052 [D loss: 0.351421, acc.: 84.38%] [G loss: 3.150318]\n",
      "epoch:3 step:3053 [D loss: 0.325453, acc.: 85.94%] [G loss: 3.639809]\n",
      "epoch:3 step:3054 [D loss: 0.399322, acc.: 78.91%] [G loss: 3.461136]\n",
      "epoch:3 step:3055 [D loss: 0.285087, acc.: 87.50%] [G loss: 3.195957]\n",
      "epoch:3 step:3056 [D loss: 0.398886, acc.: 78.12%] [G loss: 3.607901]\n",
      "epoch:3 step:3057 [D loss: 0.353026, acc.: 86.72%] [G loss: 2.893329]\n",
      "epoch:3 step:3058 [D loss: 0.279704, acc.: 88.28%] [G loss: 3.614713]\n",
      "epoch:3 step:3059 [D loss: 0.392126, acc.: 82.03%] [G loss: 2.659045]\n",
      "epoch:3 step:3060 [D loss: 0.353538, acc.: 83.59%] [G loss: 2.692935]\n",
      "epoch:3 step:3061 [D loss: 0.338054, acc.: 85.94%] [G loss: 2.994139]\n",
      "epoch:3 step:3062 [D loss: 0.420395, acc.: 78.91%] [G loss: 2.241795]\n",
      "epoch:3 step:3063 [D loss: 0.424113, acc.: 82.03%] [G loss: 4.167965]\n",
      "epoch:3 step:3064 [D loss: 0.573417, acc.: 71.88%] [G loss: 4.426752]\n",
      "epoch:3 step:3065 [D loss: 0.794823, acc.: 63.28%] [G loss: 4.542560]\n",
      "epoch:3 step:3066 [D loss: 0.525521, acc.: 75.78%] [G loss: 2.790373]\n",
      "epoch:3 step:3067 [D loss: 0.341866, acc.: 83.59%] [G loss: 2.937357]\n",
      "epoch:3 step:3068 [D loss: 0.291848, acc.: 90.62%] [G loss: 2.691627]\n",
      "epoch:3 step:3069 [D loss: 0.424039, acc.: 82.03%] [G loss: 2.462985]\n",
      "epoch:3 step:3070 [D loss: 0.377519, acc.: 85.16%] [G loss: 2.028884]\n",
      "epoch:3 step:3071 [D loss: 0.396041, acc.: 81.25%] [G loss: 2.639976]\n",
      "epoch:3 step:3072 [D loss: 0.413038, acc.: 78.12%] [G loss: 2.826879]\n",
      "epoch:3 step:3073 [D loss: 0.403693, acc.: 80.47%] [G loss: 2.907036]\n",
      "epoch:3 step:3074 [D loss: 0.433235, acc.: 75.00%] [G loss: 2.379116]\n",
      "epoch:3 step:3075 [D loss: 0.398326, acc.: 80.47%] [G loss: 2.232349]\n",
      "epoch:3 step:3076 [D loss: 0.354690, acc.: 86.72%] [G loss: 3.170593]\n",
      "epoch:3 step:3077 [D loss: 0.377579, acc.: 82.81%] [G loss: 2.304840]\n",
      "epoch:3 step:3078 [D loss: 0.342117, acc.: 81.25%] [G loss: 3.592678]\n",
      "epoch:3 step:3079 [D loss: 0.387168, acc.: 79.69%] [G loss: 3.019985]\n",
      "epoch:3 step:3080 [D loss: 0.355147, acc.: 85.94%] [G loss: 2.290873]\n",
      "epoch:3 step:3081 [D loss: 0.504483, acc.: 64.06%] [G loss: 1.974754]\n",
      "epoch:3 step:3082 [D loss: 0.483264, acc.: 73.44%] [G loss: 2.055032]\n",
      "epoch:3 step:3083 [D loss: 0.277997, acc.: 89.84%] [G loss: 3.584812]\n",
      "epoch:3 step:3084 [D loss: 0.380810, acc.: 83.59%] [G loss: 1.957063]\n",
      "epoch:3 step:3085 [D loss: 0.363509, acc.: 85.94%] [G loss: 2.618281]\n",
      "epoch:3 step:3086 [D loss: 0.352538, acc.: 85.16%] [G loss: 3.007898]\n",
      "epoch:3 step:3087 [D loss: 0.354151, acc.: 82.03%] [G loss: 3.369857]\n",
      "epoch:3 step:3088 [D loss: 0.345684, acc.: 84.38%] [G loss: 2.440825]\n",
      "epoch:3 step:3089 [D loss: 0.361770, acc.: 81.25%] [G loss: 2.589930]\n",
      "epoch:3 step:3090 [D loss: 0.355071, acc.: 85.16%] [G loss: 3.290797]\n",
      "epoch:3 step:3091 [D loss: 0.286201, acc.: 86.72%] [G loss: 2.269729]\n",
      "epoch:3 step:3092 [D loss: 0.349007, acc.: 82.03%] [G loss: 2.747772]\n",
      "epoch:3 step:3093 [D loss: 0.271296, acc.: 92.19%] [G loss: 3.026701]\n",
      "epoch:3 step:3094 [D loss: 0.418818, acc.: 82.03%] [G loss: 2.681121]\n",
      "epoch:3 step:3095 [D loss: 0.388179, acc.: 84.38%] [G loss: 5.084683]\n",
      "epoch:3 step:3096 [D loss: 0.685927, acc.: 66.41%] [G loss: 3.541261]\n",
      "epoch:3 step:3097 [D loss: 0.696637, acc.: 68.75%] [G loss: 4.648622]\n",
      "epoch:3 step:3098 [D loss: 0.902512, acc.: 63.28%] [G loss: 2.285424]\n",
      "epoch:3 step:3099 [D loss: 0.557497, acc.: 68.75%] [G loss: 2.416293]\n",
      "epoch:3 step:3100 [D loss: 0.437133, acc.: 77.34%] [G loss: 2.323031]\n",
      "epoch:3 step:3101 [D loss: 0.368123, acc.: 82.81%] [G loss: 3.249066]\n",
      "epoch:3 step:3102 [D loss: 0.331905, acc.: 82.81%] [G loss: 2.892068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3103 [D loss: 0.341078, acc.: 83.59%] [G loss: 2.287334]\n",
      "epoch:3 step:3104 [D loss: 0.361134, acc.: 86.72%] [G loss: 2.385812]\n",
      "epoch:3 step:3105 [D loss: 0.471873, acc.: 76.56%] [G loss: 2.035700]\n",
      "epoch:3 step:3106 [D loss: 0.348085, acc.: 85.94%] [G loss: 3.383376]\n",
      "epoch:3 step:3107 [D loss: 0.274100, acc.: 91.41%] [G loss: 3.940425]\n",
      "epoch:3 step:3108 [D loss: 0.352906, acc.: 82.81%] [G loss: 2.925968]\n",
      "epoch:3 step:3109 [D loss: 0.284317, acc.: 88.28%] [G loss: 3.632069]\n",
      "epoch:3 step:3110 [D loss: 0.369674, acc.: 78.12%] [G loss: 2.678989]\n",
      "epoch:3 step:3111 [D loss: 0.399126, acc.: 78.12%] [G loss: 2.574742]\n",
      "epoch:3 step:3112 [D loss: 0.363621, acc.: 83.59%] [G loss: 3.116025]\n",
      "epoch:3 step:3113 [D loss: 0.378449, acc.: 82.03%] [G loss: 2.018458]\n",
      "epoch:3 step:3114 [D loss: 0.382110, acc.: 85.16%] [G loss: 2.882079]\n",
      "epoch:3 step:3115 [D loss: 0.441773, acc.: 80.47%] [G loss: 2.544775]\n",
      "epoch:3 step:3116 [D loss: 0.516328, acc.: 72.66%] [G loss: 1.872778]\n",
      "epoch:3 step:3117 [D loss: 0.436776, acc.: 79.69%] [G loss: 2.449374]\n",
      "epoch:3 step:3118 [D loss: 0.400400, acc.: 85.16%] [G loss: 2.742314]\n",
      "epoch:3 step:3119 [D loss: 0.349692, acc.: 85.16%] [G loss: 2.743545]\n",
      "epoch:3 step:3120 [D loss: 0.388622, acc.: 80.47%] [G loss: 2.419110]\n",
      "epoch:3 step:3121 [D loss: 0.413083, acc.: 74.22%] [G loss: 3.086112]\n",
      "epoch:3 step:3122 [D loss: 0.367533, acc.: 83.59%] [G loss: 2.773965]\n",
      "epoch:3 step:3123 [D loss: 0.360660, acc.: 85.94%] [G loss: 2.542942]\n",
      "epoch:3 step:3124 [D loss: 0.400515, acc.: 78.12%] [G loss: 3.100430]\n",
      "epoch:4 step:3125 [D loss: 0.467705, acc.: 76.56%] [G loss: 2.527912]\n",
      "epoch:4 step:3126 [D loss: 0.441983, acc.: 78.12%] [G loss: 3.103592]\n",
      "epoch:4 step:3127 [D loss: 0.380039, acc.: 80.47%] [G loss: 3.035394]\n",
      "epoch:4 step:3128 [D loss: 0.379032, acc.: 84.38%] [G loss: 3.016804]\n",
      "epoch:4 step:3129 [D loss: 0.401217, acc.: 79.69%] [G loss: 2.654737]\n",
      "epoch:4 step:3130 [D loss: 0.385399, acc.: 80.47%] [G loss: 3.220234]\n",
      "epoch:4 step:3131 [D loss: 0.427628, acc.: 78.91%] [G loss: 3.302810]\n",
      "epoch:4 step:3132 [D loss: 0.477560, acc.: 75.00%] [G loss: 4.939101]\n",
      "epoch:4 step:3133 [D loss: 0.563042, acc.: 71.09%] [G loss: 2.535547]\n",
      "epoch:4 step:3134 [D loss: 0.393809, acc.: 82.03%] [G loss: 3.055831]\n",
      "epoch:4 step:3135 [D loss: 0.474280, acc.: 75.00%] [G loss: 3.196260]\n",
      "epoch:4 step:3136 [D loss: 0.409182, acc.: 79.69%] [G loss: 3.292602]\n",
      "epoch:4 step:3137 [D loss: 0.390214, acc.: 82.03%] [G loss: 2.660431]\n",
      "epoch:4 step:3138 [D loss: 0.441236, acc.: 81.25%] [G loss: 2.189350]\n",
      "epoch:4 step:3139 [D loss: 0.427095, acc.: 80.47%] [G loss: 2.360972]\n",
      "epoch:4 step:3140 [D loss: 0.305090, acc.: 89.06%] [G loss: 3.941658]\n",
      "epoch:4 step:3141 [D loss: 0.426133, acc.: 77.34%] [G loss: 2.448922]\n",
      "epoch:4 step:3142 [D loss: 0.433160, acc.: 80.47%] [G loss: 2.437766]\n",
      "epoch:4 step:3143 [D loss: 0.362358, acc.: 83.59%] [G loss: 3.263206]\n",
      "epoch:4 step:3144 [D loss: 0.538316, acc.: 73.44%] [G loss: 4.718548]\n",
      "epoch:4 step:3145 [D loss: 0.555472, acc.: 74.22%] [G loss: 2.724306]\n",
      "epoch:4 step:3146 [D loss: 0.460557, acc.: 75.00%] [G loss: 2.528095]\n",
      "epoch:4 step:3147 [D loss: 0.384403, acc.: 80.47%] [G loss: 2.229903]\n",
      "epoch:4 step:3148 [D loss: 0.361066, acc.: 85.94%] [G loss: 2.886081]\n",
      "epoch:4 step:3149 [D loss: 0.462473, acc.: 78.12%] [G loss: 2.575051]\n",
      "epoch:4 step:3150 [D loss: 0.334189, acc.: 86.72%] [G loss: 2.768943]\n",
      "epoch:4 step:3151 [D loss: 0.393645, acc.: 80.47%] [G loss: 2.518148]\n",
      "epoch:4 step:3152 [D loss: 0.387413, acc.: 84.38%] [G loss: 3.686209]\n",
      "epoch:4 step:3153 [D loss: 0.537975, acc.: 71.09%] [G loss: 4.148697]\n",
      "epoch:4 step:3154 [D loss: 0.461578, acc.: 78.12%] [G loss: 2.507319]\n",
      "epoch:4 step:3155 [D loss: 0.391293, acc.: 78.91%] [G loss: 4.671494]\n",
      "epoch:4 step:3156 [D loss: 0.536217, acc.: 71.88%] [G loss: 3.082415]\n",
      "epoch:4 step:3157 [D loss: 0.410705, acc.: 78.91%] [G loss: 3.168568]\n",
      "epoch:4 step:3158 [D loss: 0.368242, acc.: 82.81%] [G loss: 3.247386]\n",
      "epoch:4 step:3159 [D loss: 0.435564, acc.: 80.47%] [G loss: 2.802225]\n",
      "epoch:4 step:3160 [D loss: 0.385221, acc.: 82.03%] [G loss: 2.364882]\n",
      "epoch:4 step:3161 [D loss: 0.477562, acc.: 84.38%] [G loss: 2.550257]\n",
      "epoch:4 step:3162 [D loss: 0.434090, acc.: 78.12%] [G loss: 2.452273]\n",
      "epoch:4 step:3163 [D loss: 0.377206, acc.: 85.16%] [G loss: 3.601997]\n",
      "epoch:4 step:3164 [D loss: 0.397363, acc.: 82.03%] [G loss: 3.098707]\n",
      "epoch:4 step:3165 [D loss: 0.296335, acc.: 85.94%] [G loss: 4.057150]\n",
      "epoch:4 step:3166 [D loss: 0.355293, acc.: 85.16%] [G loss: 2.890398]\n",
      "epoch:4 step:3167 [D loss: 0.419741, acc.: 81.25%] [G loss: 3.450746]\n",
      "epoch:4 step:3168 [D loss: 0.859972, acc.: 56.25%] [G loss: 3.161210]\n",
      "epoch:4 step:3169 [D loss: 0.798880, acc.: 55.47%] [G loss: 3.266431]\n",
      "epoch:4 step:3170 [D loss: 0.548893, acc.: 75.00%] [G loss: 2.707794]\n",
      "epoch:4 step:3171 [D loss: 0.335997, acc.: 88.28%] [G loss: 3.427403]\n",
      "epoch:4 step:3172 [D loss: 0.302758, acc.: 86.72%] [G loss: 3.465800]\n",
      "epoch:4 step:3173 [D loss: 0.394579, acc.: 81.25%] [G loss: 3.939241]\n",
      "epoch:4 step:3174 [D loss: 0.374881, acc.: 82.81%] [G loss: 2.568738]\n",
      "epoch:4 step:3175 [D loss: 0.351298, acc.: 87.50%] [G loss: 2.891762]\n",
      "epoch:4 step:3176 [D loss: 0.419249, acc.: 78.12%] [G loss: 3.105399]\n",
      "epoch:4 step:3177 [D loss: 0.498935, acc.: 72.66%] [G loss: 2.224838]\n",
      "epoch:4 step:3178 [D loss: 0.356156, acc.: 85.94%] [G loss: 2.665122]\n",
      "epoch:4 step:3179 [D loss: 0.401975, acc.: 80.47%] [G loss: 2.535615]\n",
      "epoch:4 step:3180 [D loss: 0.428049, acc.: 80.47%] [G loss: 2.107058]\n",
      "epoch:4 step:3181 [D loss: 0.469930, acc.: 76.56%] [G loss: 2.805606]\n",
      "epoch:4 step:3182 [D loss: 0.422563, acc.: 78.91%] [G loss: 3.603486]\n",
      "epoch:4 step:3183 [D loss: 0.510115, acc.: 73.44%] [G loss: 2.712846]\n",
      "epoch:4 step:3184 [D loss: 0.484332, acc.: 74.22%] [G loss: 2.115797]\n",
      "epoch:4 step:3185 [D loss: 0.506223, acc.: 83.59%] [G loss: 2.085424]\n",
      "epoch:4 step:3186 [D loss: 0.521535, acc.: 75.00%] [G loss: 2.311722]\n",
      "epoch:4 step:3187 [D loss: 0.456356, acc.: 81.25%] [G loss: 2.761879]\n",
      "epoch:4 step:3188 [D loss: 0.496168, acc.: 75.78%] [G loss: 2.862159]\n",
      "epoch:4 step:3189 [D loss: 0.468073, acc.: 76.56%] [G loss: 2.924499]\n",
      "epoch:4 step:3190 [D loss: 0.348685, acc.: 85.16%] [G loss: 2.839907]\n",
      "epoch:4 step:3191 [D loss: 0.343093, acc.: 84.38%] [G loss: 2.835862]\n",
      "epoch:4 step:3192 [D loss: 0.328051, acc.: 87.50%] [G loss: 2.563689]\n",
      "epoch:4 step:3193 [D loss: 0.360263, acc.: 85.94%] [G loss: 3.389052]\n",
      "epoch:4 step:3194 [D loss: 0.354787, acc.: 86.72%] [G loss: 2.698187]\n",
      "epoch:4 step:3195 [D loss: 0.401797, acc.: 80.47%] [G loss: 2.427292]\n",
      "epoch:4 step:3196 [D loss: 0.455238, acc.: 82.81%] [G loss: 2.135617]\n",
      "epoch:4 step:3197 [D loss: 0.531507, acc.: 72.66%] [G loss: 2.553148]\n",
      "epoch:4 step:3198 [D loss: 0.643283, acc.: 70.31%] [G loss: 5.987242]\n",
      "epoch:4 step:3199 [D loss: 1.368484, acc.: 62.50%] [G loss: 4.276345]\n",
      "epoch:4 step:3200 [D loss: 0.386516, acc.: 82.81%] [G loss: 4.935697]\n",
      "epoch:4 step:3201 [D loss: 0.418897, acc.: 76.56%] [G loss: 3.722985]\n",
      "epoch:4 step:3202 [D loss: 0.420200, acc.: 75.00%] [G loss: 2.491283]\n",
      "epoch:4 step:3203 [D loss: 0.484624, acc.: 76.56%] [G loss: 2.314381]\n",
      "epoch:4 step:3204 [D loss: 0.420862, acc.: 81.25%] [G loss: 2.608178]\n",
      "epoch:4 step:3205 [D loss: 0.476229, acc.: 74.22%] [G loss: 2.854386]\n",
      "epoch:4 step:3206 [D loss: 0.510057, acc.: 70.31%] [G loss: 2.268358]\n",
      "epoch:4 step:3207 [D loss: 0.388351, acc.: 82.81%] [G loss: 1.704103]\n",
      "epoch:4 step:3208 [D loss: 0.419556, acc.: 83.59%] [G loss: 1.973330]\n",
      "epoch:4 step:3209 [D loss: 0.453220, acc.: 77.34%] [G loss: 1.933712]\n",
      "epoch:4 step:3210 [D loss: 0.512024, acc.: 70.31%] [G loss: 2.194145]\n",
      "epoch:4 step:3211 [D loss: 0.425691, acc.: 77.34%] [G loss: 2.047438]\n",
      "epoch:4 step:3212 [D loss: 0.368120, acc.: 86.72%] [G loss: 2.279804]\n",
      "epoch:4 step:3213 [D loss: 0.461926, acc.: 73.44%] [G loss: 1.951426]\n",
      "epoch:4 step:3214 [D loss: 0.385456, acc.: 89.06%] [G loss: 2.424488]\n",
      "epoch:4 step:3215 [D loss: 0.358456, acc.: 82.81%] [G loss: 2.456574]\n",
      "epoch:4 step:3216 [D loss: 0.398347, acc.: 82.03%] [G loss: 2.082507]\n",
      "epoch:4 step:3217 [D loss: 0.356589, acc.: 82.03%] [G loss: 2.729171]\n",
      "epoch:4 step:3218 [D loss: 0.325195, acc.: 90.62%] [G loss: 2.604732]\n",
      "epoch:4 step:3219 [D loss: 0.400515, acc.: 86.72%] [G loss: 2.442820]\n",
      "epoch:4 step:3220 [D loss: 0.362035, acc.: 85.16%] [G loss: 3.205993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3221 [D loss: 0.400115, acc.: 79.69%] [G loss: 3.030308]\n",
      "epoch:4 step:3222 [D loss: 0.488489, acc.: 77.34%] [G loss: 3.683862]\n",
      "epoch:4 step:3223 [D loss: 0.551208, acc.: 74.22%] [G loss: 3.523982]\n",
      "epoch:4 step:3224 [D loss: 0.570728, acc.: 64.84%] [G loss: 3.394418]\n",
      "epoch:4 step:3225 [D loss: 0.637937, acc.: 70.31%] [G loss: 2.969101]\n",
      "epoch:4 step:3226 [D loss: 0.637237, acc.: 77.34%] [G loss: 4.299644]\n",
      "epoch:4 step:3227 [D loss: 0.817695, acc.: 60.94%] [G loss: 2.539317]\n",
      "epoch:4 step:3228 [D loss: 0.654449, acc.: 65.62%] [G loss: 3.617729]\n",
      "epoch:4 step:3229 [D loss: 0.504729, acc.: 77.34%] [G loss: 2.759966]\n",
      "epoch:4 step:3230 [D loss: 0.264391, acc.: 89.84%] [G loss: 3.331368]\n",
      "epoch:4 step:3231 [D loss: 0.380070, acc.: 82.81%] [G loss: 1.919671]\n",
      "epoch:4 step:3232 [D loss: 0.341683, acc.: 85.94%] [G loss: 2.679415]\n",
      "epoch:4 step:3233 [D loss: 0.394648, acc.: 82.03%] [G loss: 2.284987]\n",
      "epoch:4 step:3234 [D loss: 0.424758, acc.: 78.91%] [G loss: 1.876638]\n",
      "epoch:4 step:3235 [D loss: 0.442172, acc.: 76.56%] [G loss: 2.164928]\n",
      "epoch:4 step:3236 [D loss: 0.388947, acc.: 79.69%] [G loss: 2.114475]\n",
      "epoch:4 step:3237 [D loss: 0.426140, acc.: 81.25%] [G loss: 1.781524]\n",
      "epoch:4 step:3238 [D loss: 0.370427, acc.: 82.81%] [G loss: 2.730099]\n",
      "epoch:4 step:3239 [D loss: 0.380211, acc.: 78.91%] [G loss: 2.637376]\n",
      "epoch:4 step:3240 [D loss: 0.503217, acc.: 75.78%] [G loss: 2.350841]\n",
      "epoch:4 step:3241 [D loss: 0.384658, acc.: 82.03%] [G loss: 3.418489]\n",
      "epoch:4 step:3242 [D loss: 0.445578, acc.: 79.69%] [G loss: 3.681960]\n",
      "epoch:4 step:3243 [D loss: 0.656469, acc.: 71.09%] [G loss: 4.685377]\n",
      "epoch:4 step:3244 [D loss: 0.710390, acc.: 75.78%] [G loss: 2.737365]\n",
      "epoch:4 step:3245 [D loss: 0.426837, acc.: 80.47%] [G loss: 2.348256]\n",
      "epoch:4 step:3246 [D loss: 0.471612, acc.: 73.44%] [G loss: 2.058776]\n",
      "epoch:4 step:3247 [D loss: 0.349888, acc.: 84.38%] [G loss: 3.016616]\n",
      "epoch:4 step:3248 [D loss: 0.502936, acc.: 79.69%] [G loss: 2.301889]\n",
      "epoch:4 step:3249 [D loss: 0.372562, acc.: 82.81%] [G loss: 2.618567]\n",
      "epoch:4 step:3250 [D loss: 0.446350, acc.: 76.56%] [G loss: 2.281225]\n",
      "epoch:4 step:3251 [D loss: 0.374098, acc.: 84.38%] [G loss: 2.967783]\n",
      "epoch:4 step:3252 [D loss: 0.416909, acc.: 78.91%] [G loss: 1.992698]\n",
      "epoch:4 step:3253 [D loss: 0.398642, acc.: 82.81%] [G loss: 2.970161]\n",
      "epoch:4 step:3254 [D loss: 0.445231, acc.: 78.12%] [G loss: 3.449646]\n",
      "epoch:4 step:3255 [D loss: 0.577961, acc.: 67.97%] [G loss: 2.017344]\n",
      "epoch:4 step:3256 [D loss: 0.761115, acc.: 68.75%] [G loss: 3.604410]\n",
      "epoch:4 step:3257 [D loss: 0.438999, acc.: 78.12%] [G loss: 2.101430]\n",
      "epoch:4 step:3258 [D loss: 0.382588, acc.: 80.47%] [G loss: 2.777357]\n",
      "epoch:4 step:3259 [D loss: 0.402566, acc.: 78.91%] [G loss: 2.576381]\n",
      "epoch:4 step:3260 [D loss: 0.419155, acc.: 78.91%] [G loss: 3.197390]\n",
      "epoch:4 step:3261 [D loss: 0.428709, acc.: 80.47%] [G loss: 2.093256]\n",
      "epoch:4 step:3262 [D loss: 0.426022, acc.: 78.91%] [G loss: 2.430159]\n",
      "epoch:4 step:3263 [D loss: 0.364780, acc.: 87.50%] [G loss: 1.928731]\n",
      "epoch:4 step:3264 [D loss: 0.403600, acc.: 82.03%] [G loss: 1.965305]\n",
      "epoch:4 step:3265 [D loss: 0.411165, acc.: 84.38%] [G loss: 2.097361]\n",
      "epoch:4 step:3266 [D loss: 0.347129, acc.: 89.84%] [G loss: 2.370669]\n",
      "epoch:4 step:3267 [D loss: 0.387393, acc.: 80.47%] [G loss: 2.336378]\n",
      "epoch:4 step:3268 [D loss: 0.365697, acc.: 84.38%] [G loss: 2.423980]\n",
      "epoch:4 step:3269 [D loss: 0.425893, acc.: 78.12%] [G loss: 1.689146]\n",
      "epoch:4 step:3270 [D loss: 0.353995, acc.: 85.94%] [G loss: 2.401228]\n",
      "epoch:4 step:3271 [D loss: 0.310243, acc.: 89.06%] [G loss: 3.036666]\n",
      "epoch:4 step:3272 [D loss: 0.343317, acc.: 83.59%] [G loss: 2.982355]\n",
      "epoch:4 step:3273 [D loss: 0.441306, acc.: 72.66%] [G loss: 1.733495]\n",
      "epoch:4 step:3274 [D loss: 0.408818, acc.: 83.59%] [G loss: 1.854792]\n",
      "epoch:4 step:3275 [D loss: 0.339706, acc.: 86.72%] [G loss: 2.112676]\n",
      "epoch:4 step:3276 [D loss: 0.422386, acc.: 82.81%] [G loss: 2.272352]\n",
      "epoch:4 step:3277 [D loss: 0.294867, acc.: 92.97%] [G loss: 3.111350]\n",
      "epoch:4 step:3278 [D loss: 0.321673, acc.: 88.28%] [G loss: 2.712650]\n",
      "epoch:4 step:3279 [D loss: 0.366367, acc.: 82.81%] [G loss: 2.700008]\n",
      "epoch:4 step:3280 [D loss: 0.390839, acc.: 79.69%] [G loss: 2.284805]\n",
      "epoch:4 step:3281 [D loss: 0.417672, acc.: 79.69%] [G loss: 2.419353]\n",
      "epoch:4 step:3282 [D loss: 0.387374, acc.: 83.59%] [G loss: 2.372556]\n",
      "epoch:4 step:3283 [D loss: 0.505375, acc.: 77.34%] [G loss: 3.284267]\n",
      "epoch:4 step:3284 [D loss: 0.738553, acc.: 68.75%] [G loss: 4.926612]\n",
      "epoch:4 step:3285 [D loss: 1.299546, acc.: 47.66%] [G loss: 2.912098]\n",
      "epoch:4 step:3286 [D loss: 0.707566, acc.: 71.88%] [G loss: 1.674215]\n",
      "epoch:4 step:3287 [D loss: 0.424537, acc.: 76.56%] [G loss: 2.336069]\n",
      "epoch:4 step:3288 [D loss: 0.466583, acc.: 75.00%] [G loss: 2.319542]\n",
      "epoch:4 step:3289 [D loss: 0.477091, acc.: 75.00%] [G loss: 2.531925]\n",
      "epoch:4 step:3290 [D loss: 0.536932, acc.: 70.31%] [G loss: 1.920318]\n",
      "epoch:4 step:3291 [D loss: 0.441653, acc.: 78.91%] [G loss: 2.068951]\n",
      "epoch:4 step:3292 [D loss: 0.393439, acc.: 80.47%] [G loss: 1.696196]\n",
      "epoch:4 step:3293 [D loss: 0.458784, acc.: 78.12%] [G loss: 1.569217]\n",
      "epoch:4 step:3294 [D loss: 0.404402, acc.: 78.12%] [G loss: 2.389132]\n",
      "epoch:4 step:3295 [D loss: 0.443259, acc.: 79.69%] [G loss: 1.658919]\n",
      "epoch:4 step:3296 [D loss: 0.375062, acc.: 87.50%] [G loss: 2.051481]\n",
      "epoch:4 step:3297 [D loss: 0.409655, acc.: 85.16%] [G loss: 2.176234]\n",
      "epoch:4 step:3298 [D loss: 0.375814, acc.: 83.59%] [G loss: 2.715351]\n",
      "epoch:4 step:3299 [D loss: 0.390060, acc.: 82.03%] [G loss: 2.412784]\n",
      "epoch:4 step:3300 [D loss: 0.432508, acc.: 72.66%] [G loss: 2.058864]\n",
      "epoch:4 step:3301 [D loss: 0.357116, acc.: 86.72%] [G loss: 2.734432]\n",
      "epoch:4 step:3302 [D loss: 0.471870, acc.: 78.12%] [G loss: 2.043366]\n",
      "epoch:4 step:3303 [D loss: 0.389190, acc.: 81.25%] [G loss: 2.048417]\n",
      "epoch:4 step:3304 [D loss: 0.423520, acc.: 81.25%] [G loss: 1.882439]\n",
      "epoch:4 step:3305 [D loss: 0.378294, acc.: 86.72%] [G loss: 2.555009]\n",
      "epoch:4 step:3306 [D loss: 0.455504, acc.: 78.91%] [G loss: 1.982774]\n",
      "epoch:4 step:3307 [D loss: 0.405513, acc.: 80.47%] [G loss: 2.399077]\n",
      "epoch:4 step:3308 [D loss: 0.381257, acc.: 81.25%] [G loss: 1.908244]\n",
      "epoch:4 step:3309 [D loss: 0.428882, acc.: 81.25%] [G loss: 3.182604]\n",
      "epoch:4 step:3310 [D loss: 0.346885, acc.: 87.50%] [G loss: 3.061777]\n",
      "epoch:4 step:3311 [D loss: 0.348421, acc.: 88.28%] [G loss: 2.392693]\n",
      "epoch:4 step:3312 [D loss: 0.392050, acc.: 81.25%] [G loss: 2.697893]\n",
      "epoch:4 step:3313 [D loss: 0.338098, acc.: 86.72%] [G loss: 2.771002]\n",
      "epoch:4 step:3314 [D loss: 0.338597, acc.: 86.72%] [G loss: 2.616522]\n",
      "epoch:4 step:3315 [D loss: 0.336121, acc.: 84.38%] [G loss: 2.072953]\n",
      "epoch:4 step:3316 [D loss: 0.375267, acc.: 87.50%] [G loss: 2.711903]\n",
      "epoch:4 step:3317 [D loss: 0.478409, acc.: 74.22%] [G loss: 2.589960]\n",
      "epoch:4 step:3318 [D loss: 0.571012, acc.: 72.66%] [G loss: 3.534766]\n",
      "epoch:4 step:3319 [D loss: 0.887021, acc.: 66.41%] [G loss: 4.531525]\n",
      "epoch:4 step:3320 [D loss: 0.659140, acc.: 67.19%] [G loss: 2.244756]\n",
      "epoch:4 step:3321 [D loss: 0.421532, acc.: 77.34%] [G loss: 3.301058]\n",
      "epoch:4 step:3322 [D loss: 0.448083, acc.: 79.69%] [G loss: 2.370641]\n",
      "epoch:4 step:3323 [D loss: 0.435156, acc.: 75.78%] [G loss: 1.998445]\n",
      "epoch:4 step:3324 [D loss: 0.352744, acc.: 82.03%] [G loss: 2.624404]\n",
      "epoch:4 step:3325 [D loss: 0.396160, acc.: 81.25%] [G loss: 2.321323]\n",
      "epoch:4 step:3326 [D loss: 0.360749, acc.: 84.38%] [G loss: 2.083470]\n",
      "epoch:4 step:3327 [D loss: 0.374404, acc.: 83.59%] [G loss: 2.005090]\n",
      "epoch:4 step:3328 [D loss: 0.389371, acc.: 80.47%] [G loss: 2.114124]\n",
      "epoch:4 step:3329 [D loss: 0.424598, acc.: 79.69%] [G loss: 2.121847]\n",
      "epoch:4 step:3330 [D loss: 0.384521, acc.: 79.69%] [G loss: 1.987311]\n",
      "epoch:4 step:3331 [D loss: 0.401713, acc.: 79.69%] [G loss: 2.163474]\n",
      "epoch:4 step:3332 [D loss: 0.329309, acc.: 89.06%] [G loss: 2.415803]\n",
      "epoch:4 step:3333 [D loss: 0.421397, acc.: 78.91%] [G loss: 1.848669]\n",
      "epoch:4 step:3334 [D loss: 0.382940, acc.: 84.38%] [G loss: 2.473731]\n",
      "epoch:4 step:3335 [D loss: 0.503180, acc.: 70.31%] [G loss: 2.135185]\n",
      "epoch:4 step:3336 [D loss: 0.470127, acc.: 77.34%] [G loss: 2.234313]\n",
      "epoch:4 step:3337 [D loss: 0.369180, acc.: 89.06%] [G loss: 2.111818]\n",
      "epoch:4 step:3338 [D loss: 0.370468, acc.: 86.72%] [G loss: 2.524234]\n",
      "epoch:4 step:3339 [D loss: 0.458519, acc.: 80.47%] [G loss: 2.485492]\n",
      "epoch:4 step:3340 [D loss: 0.402016, acc.: 80.47%] [G loss: 2.047355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3341 [D loss: 0.363693, acc.: 84.38%] [G loss: 1.726798]\n",
      "epoch:4 step:3342 [D loss: 0.388009, acc.: 85.16%] [G loss: 2.197134]\n",
      "epoch:4 step:3343 [D loss: 0.361107, acc.: 85.16%] [G loss: 2.985545]\n",
      "epoch:4 step:3344 [D loss: 0.436725, acc.: 78.91%] [G loss: 1.966234]\n",
      "epoch:4 step:3345 [D loss: 0.310742, acc.: 86.72%] [G loss: 2.329432]\n",
      "epoch:4 step:3346 [D loss: 0.353142, acc.: 85.16%] [G loss: 2.281962]\n",
      "epoch:4 step:3347 [D loss: 0.401326, acc.: 76.56%] [G loss: 2.598473]\n",
      "epoch:4 step:3348 [D loss: 0.327903, acc.: 88.28%] [G loss: 3.313399]\n",
      "epoch:4 step:3349 [D loss: 0.327773, acc.: 85.16%] [G loss: 3.203310]\n",
      "epoch:4 step:3350 [D loss: 0.319217, acc.: 89.06%] [G loss: 3.408463]\n",
      "epoch:4 step:3351 [D loss: 0.361014, acc.: 82.81%] [G loss: 2.470098]\n",
      "epoch:4 step:3352 [D loss: 0.321409, acc.: 87.50%] [G loss: 2.452505]\n",
      "epoch:4 step:3353 [D loss: 0.384478, acc.: 84.38%] [G loss: 3.206558]\n",
      "epoch:4 step:3354 [D loss: 0.378292, acc.: 82.81%] [G loss: 2.671238]\n",
      "epoch:4 step:3355 [D loss: 0.403030, acc.: 79.69%] [G loss: 4.171013]\n",
      "epoch:4 step:3356 [D loss: 0.520491, acc.: 74.22%] [G loss: 2.897794]\n",
      "epoch:4 step:3357 [D loss: 0.627505, acc.: 74.22%] [G loss: 2.892162]\n",
      "epoch:4 step:3358 [D loss: 0.555354, acc.: 74.22%] [G loss: 2.054980]\n",
      "epoch:4 step:3359 [D loss: 0.393800, acc.: 83.59%] [G loss: 3.605025]\n",
      "epoch:4 step:3360 [D loss: 0.438165, acc.: 79.69%] [G loss: 3.380576]\n",
      "epoch:4 step:3361 [D loss: 0.415004, acc.: 76.56%] [G loss: 3.615960]\n",
      "epoch:4 step:3362 [D loss: 0.546746, acc.: 72.66%] [G loss: 2.828935]\n",
      "epoch:4 step:3363 [D loss: 0.443362, acc.: 80.47%] [G loss: 2.759914]\n",
      "epoch:4 step:3364 [D loss: 0.428887, acc.: 78.12%] [G loss: 2.109582]\n",
      "epoch:4 step:3365 [D loss: 0.397766, acc.: 81.25%] [G loss: 2.214878]\n",
      "epoch:4 step:3366 [D loss: 0.326215, acc.: 85.16%] [G loss: 2.897141]\n",
      "epoch:4 step:3367 [D loss: 0.288298, acc.: 89.06%] [G loss: 2.834238]\n",
      "epoch:4 step:3368 [D loss: 0.405239, acc.: 77.34%] [G loss: 2.036593]\n",
      "epoch:4 step:3369 [D loss: 0.327750, acc.: 86.72%] [G loss: 3.143140]\n",
      "epoch:4 step:3370 [D loss: 0.465893, acc.: 77.34%] [G loss: 1.946060]\n",
      "epoch:4 step:3371 [D loss: 0.404248, acc.: 79.69%] [G loss: 2.070060]\n",
      "epoch:4 step:3372 [D loss: 0.410743, acc.: 84.38%] [G loss: 2.399403]\n",
      "epoch:4 step:3373 [D loss: 0.480792, acc.: 78.91%] [G loss: 2.360642]\n",
      "epoch:4 step:3374 [D loss: 0.454849, acc.: 82.03%] [G loss: 3.346416]\n",
      "epoch:4 step:3375 [D loss: 0.532012, acc.: 75.00%] [G loss: 3.152894]\n",
      "epoch:4 step:3376 [D loss: 0.473119, acc.: 79.69%] [G loss: 2.840609]\n",
      "epoch:4 step:3377 [D loss: 0.459290, acc.: 76.56%] [G loss: 2.566016]\n",
      "epoch:4 step:3378 [D loss: 0.323520, acc.: 85.16%] [G loss: 3.246105]\n",
      "epoch:4 step:3379 [D loss: 0.490667, acc.: 73.44%] [G loss: 2.269090]\n",
      "epoch:4 step:3380 [D loss: 0.405333, acc.: 85.16%] [G loss: 3.595171]\n",
      "epoch:4 step:3381 [D loss: 0.413507, acc.: 79.69%] [G loss: 2.407229]\n",
      "epoch:4 step:3382 [D loss: 0.404246, acc.: 78.12%] [G loss: 3.045033]\n",
      "epoch:4 step:3383 [D loss: 0.418104, acc.: 81.25%] [G loss: 2.601186]\n",
      "epoch:4 step:3384 [D loss: 0.462615, acc.: 76.56%] [G loss: 3.352829]\n",
      "epoch:4 step:3385 [D loss: 0.445089, acc.: 79.69%] [G loss: 3.814543]\n",
      "epoch:4 step:3386 [D loss: 0.289687, acc.: 89.06%] [G loss: 2.789555]\n",
      "epoch:4 step:3387 [D loss: 0.275619, acc.: 91.41%] [G loss: 2.968132]\n",
      "epoch:4 step:3388 [D loss: 0.371754, acc.: 83.59%] [G loss: 2.793595]\n",
      "epoch:4 step:3389 [D loss: 0.376948, acc.: 82.81%] [G loss: 2.270431]\n",
      "epoch:4 step:3390 [D loss: 0.298998, acc.: 86.72%] [G loss: 2.436541]\n",
      "epoch:4 step:3391 [D loss: 0.444898, acc.: 78.12%] [G loss: 2.411381]\n",
      "epoch:4 step:3392 [D loss: 0.455604, acc.: 74.22%] [G loss: 3.264785]\n",
      "epoch:4 step:3393 [D loss: 0.422390, acc.: 79.69%] [G loss: 3.099798]\n",
      "epoch:4 step:3394 [D loss: 0.475197, acc.: 69.53%] [G loss: 2.512864]\n",
      "epoch:4 step:3395 [D loss: 0.391378, acc.: 82.03%] [G loss: 2.167414]\n",
      "epoch:4 step:3396 [D loss: 0.407320, acc.: 80.47%] [G loss: 2.730287]\n",
      "epoch:4 step:3397 [D loss: 0.351811, acc.: 86.72%] [G loss: 2.882320]\n",
      "epoch:4 step:3398 [D loss: 0.420631, acc.: 82.81%] [G loss: 2.372372]\n",
      "epoch:4 step:3399 [D loss: 0.333719, acc.: 82.03%] [G loss: 2.944004]\n",
      "epoch:4 step:3400 [D loss: 0.334405, acc.: 86.72%] [G loss: 2.707079]\n",
      "epoch:4 step:3401 [D loss: 0.305421, acc.: 89.84%] [G loss: 2.563982]\n",
      "epoch:4 step:3402 [D loss: 0.299914, acc.: 87.50%] [G loss: 3.547475]\n",
      "epoch:4 step:3403 [D loss: 0.402219, acc.: 81.25%] [G loss: 2.374065]\n",
      "epoch:4 step:3404 [D loss: 0.386542, acc.: 85.16%] [G loss: 2.409549]\n",
      "epoch:4 step:3405 [D loss: 0.405629, acc.: 82.81%] [G loss: 3.006994]\n",
      "epoch:4 step:3406 [D loss: 0.437497, acc.: 80.47%] [G loss: 3.567303]\n",
      "epoch:4 step:3407 [D loss: 0.547636, acc.: 76.56%] [G loss: 3.112538]\n",
      "epoch:4 step:3408 [D loss: 0.525637, acc.: 75.78%] [G loss: 3.452319]\n",
      "epoch:4 step:3409 [D loss: 0.454671, acc.: 72.66%] [G loss: 2.335455]\n",
      "epoch:4 step:3410 [D loss: 0.340543, acc.: 86.72%] [G loss: 2.454569]\n",
      "epoch:4 step:3411 [D loss: 0.379148, acc.: 82.03%] [G loss: 3.014824]\n",
      "epoch:4 step:3412 [D loss: 0.454186, acc.: 78.91%] [G loss: 2.589342]\n",
      "epoch:4 step:3413 [D loss: 0.428736, acc.: 76.56%] [G loss: 2.294845]\n",
      "epoch:4 step:3414 [D loss: 0.380671, acc.: 83.59%] [G loss: 1.773566]\n",
      "epoch:4 step:3415 [D loss: 0.375999, acc.: 80.47%] [G loss: 2.927953]\n",
      "epoch:4 step:3416 [D loss: 0.581089, acc.: 69.53%] [G loss: 2.901690]\n",
      "epoch:4 step:3417 [D loss: 0.790285, acc.: 62.50%] [G loss: 5.255863]\n",
      "epoch:4 step:3418 [D loss: 1.045238, acc.: 60.16%] [G loss: 3.951807]\n",
      "epoch:4 step:3419 [D loss: 1.335899, acc.: 54.69%] [G loss: 3.218893]\n",
      "epoch:4 step:3420 [D loss: 0.680633, acc.: 76.56%] [G loss: 2.838914]\n",
      "epoch:4 step:3421 [D loss: 0.572220, acc.: 70.31%] [G loss: 1.808400]\n",
      "epoch:4 step:3422 [D loss: 0.680737, acc.: 75.00%] [G loss: 2.624066]\n",
      "epoch:4 step:3423 [D loss: 0.466318, acc.: 77.34%] [G loss: 2.829528]\n",
      "epoch:4 step:3424 [D loss: 0.818963, acc.: 60.94%] [G loss: 2.813372]\n",
      "epoch:4 step:3425 [D loss: 0.667110, acc.: 65.62%] [G loss: 3.335625]\n",
      "epoch:4 step:3426 [D loss: 0.365508, acc.: 83.59%] [G loss: 2.351146]\n",
      "epoch:4 step:3427 [D loss: 0.493975, acc.: 74.22%] [G loss: 2.066757]\n",
      "epoch:4 step:3428 [D loss: 0.339840, acc.: 85.94%] [G loss: 2.485487]\n",
      "epoch:4 step:3429 [D loss: 0.420149, acc.: 78.12%] [G loss: 2.374659]\n",
      "epoch:4 step:3430 [D loss: 0.406396, acc.: 82.03%] [G loss: 2.181904]\n",
      "epoch:4 step:3431 [D loss: 0.356522, acc.: 85.94%] [G loss: 2.214464]\n",
      "epoch:4 step:3432 [D loss: 0.362362, acc.: 86.72%] [G loss: 1.836355]\n",
      "epoch:4 step:3433 [D loss: 0.357503, acc.: 86.72%] [G loss: 2.382330]\n",
      "epoch:4 step:3434 [D loss: 0.310549, acc.: 86.72%] [G loss: 2.445809]\n",
      "epoch:4 step:3435 [D loss: 0.422833, acc.: 76.56%] [G loss: 2.205417]\n",
      "epoch:4 step:3436 [D loss: 0.319957, acc.: 89.06%] [G loss: 2.265146]\n",
      "epoch:4 step:3437 [D loss: 0.393083, acc.: 82.81%] [G loss: 2.264816]\n",
      "epoch:4 step:3438 [D loss: 0.424434, acc.: 81.25%] [G loss: 1.827058]\n",
      "epoch:4 step:3439 [D loss: 0.429905, acc.: 80.47%] [G loss: 1.885120]\n",
      "epoch:4 step:3440 [D loss: 0.345620, acc.: 86.72%] [G loss: 2.336239]\n",
      "epoch:4 step:3441 [D loss: 0.316838, acc.: 88.28%] [G loss: 2.387175]\n",
      "epoch:4 step:3442 [D loss: 0.423357, acc.: 78.12%] [G loss: 1.887490]\n",
      "epoch:4 step:3443 [D loss: 0.360847, acc.: 83.59%] [G loss: 1.997972]\n",
      "epoch:4 step:3444 [D loss: 0.429452, acc.: 79.69%] [G loss: 1.958322]\n",
      "epoch:4 step:3445 [D loss: 0.331082, acc.: 86.72%] [G loss: 2.496950]\n",
      "epoch:4 step:3446 [D loss: 0.359906, acc.: 87.50%] [G loss: 2.341798]\n",
      "epoch:4 step:3447 [D loss: 0.378133, acc.: 83.59%] [G loss: 2.546597]\n",
      "epoch:4 step:3448 [D loss: 0.366222, acc.: 85.16%] [G loss: 2.349254]\n",
      "epoch:4 step:3449 [D loss: 0.378264, acc.: 84.38%] [G loss: 2.355837]\n",
      "epoch:4 step:3450 [D loss: 0.426687, acc.: 82.03%] [G loss: 2.025813]\n",
      "epoch:4 step:3451 [D loss: 0.380458, acc.: 84.38%] [G loss: 2.215736]\n",
      "epoch:4 step:3452 [D loss: 0.433430, acc.: 77.34%] [G loss: 2.266129]\n",
      "epoch:4 step:3453 [D loss: 0.451613, acc.: 79.69%] [G loss: 1.886133]\n",
      "epoch:4 step:3454 [D loss: 0.436200, acc.: 74.22%] [G loss: 2.465713]\n",
      "epoch:4 step:3455 [D loss: 0.395499, acc.: 78.12%] [G loss: 2.241093]\n",
      "epoch:4 step:3456 [D loss: 0.352787, acc.: 85.94%] [G loss: 3.002784]\n",
      "epoch:4 step:3457 [D loss: 0.394600, acc.: 82.03%] [G loss: 2.200793]\n",
      "epoch:4 step:3458 [D loss: 0.374707, acc.: 82.03%] [G loss: 2.091829]\n",
      "epoch:4 step:3459 [D loss: 0.460141, acc.: 82.81%] [G loss: 2.316321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3460 [D loss: 0.392567, acc.: 82.03%] [G loss: 2.018897]\n",
      "epoch:4 step:3461 [D loss: 0.399148, acc.: 82.81%] [G loss: 2.575950]\n",
      "epoch:4 step:3462 [D loss: 0.425351, acc.: 80.47%] [G loss: 2.117950]\n",
      "epoch:4 step:3463 [D loss: 0.375446, acc.: 80.47%] [G loss: 1.721352]\n",
      "epoch:4 step:3464 [D loss: 0.420913, acc.: 78.12%] [G loss: 2.816091]\n",
      "epoch:4 step:3465 [D loss: 0.528058, acc.: 73.44%] [G loss: 3.846346]\n",
      "epoch:4 step:3466 [D loss: 0.640275, acc.: 64.84%] [G loss: 2.328967]\n",
      "epoch:4 step:3467 [D loss: 0.687886, acc.: 61.72%] [G loss: 1.706005]\n",
      "epoch:4 step:3468 [D loss: 0.463915, acc.: 76.56%] [G loss: 2.392902]\n",
      "epoch:4 step:3469 [D loss: 0.401167, acc.: 81.25%] [G loss: 2.027297]\n",
      "epoch:4 step:3470 [D loss: 0.486600, acc.: 72.66%] [G loss: 2.417449]\n",
      "epoch:4 step:3471 [D loss: 0.409456, acc.: 80.47%] [G loss: 2.316057]\n",
      "epoch:4 step:3472 [D loss: 0.353754, acc.: 87.50%] [G loss: 3.536187]\n",
      "epoch:4 step:3473 [D loss: 0.454900, acc.: 78.91%] [G loss: 2.230953]\n",
      "epoch:4 step:3474 [D loss: 0.326688, acc.: 82.81%] [G loss: 2.380607]\n",
      "epoch:4 step:3475 [D loss: 0.534977, acc.: 78.12%] [G loss: 2.879706]\n",
      "epoch:4 step:3476 [D loss: 0.469676, acc.: 75.78%] [G loss: 1.962716]\n",
      "epoch:4 step:3477 [D loss: 0.328503, acc.: 84.38%] [G loss: 2.889720]\n",
      "epoch:4 step:3478 [D loss: 0.412614, acc.: 82.81%] [G loss: 2.486643]\n",
      "epoch:4 step:3479 [D loss: 0.408883, acc.: 82.03%] [G loss: 2.397600]\n",
      "epoch:4 step:3480 [D loss: 0.384797, acc.: 83.59%] [G loss: 2.005682]\n",
      "epoch:4 step:3481 [D loss: 0.383676, acc.: 80.47%] [G loss: 2.835296]\n",
      "epoch:4 step:3482 [D loss: 0.370424, acc.: 83.59%] [G loss: 2.506141]\n",
      "epoch:4 step:3483 [D loss: 0.365957, acc.: 85.94%] [G loss: 2.336660]\n",
      "epoch:4 step:3484 [D loss: 0.316500, acc.: 89.06%] [G loss: 1.795910]\n",
      "epoch:4 step:3485 [D loss: 0.468070, acc.: 76.56%] [G loss: 1.955185]\n",
      "epoch:4 step:3486 [D loss: 0.442698, acc.: 79.69%] [G loss: 1.868705]\n",
      "epoch:4 step:3487 [D loss: 0.444777, acc.: 77.34%] [G loss: 2.248241]\n",
      "epoch:4 step:3488 [D loss: 0.456208, acc.: 76.56%] [G loss: 1.939050]\n",
      "epoch:4 step:3489 [D loss: 0.365875, acc.: 82.03%] [G loss: 2.282828]\n",
      "epoch:4 step:3490 [D loss: 0.411453, acc.: 78.91%] [G loss: 2.104490]\n",
      "epoch:4 step:3491 [D loss: 0.398041, acc.: 82.03%] [G loss: 2.472723]\n",
      "epoch:4 step:3492 [D loss: 0.499738, acc.: 75.00%] [G loss: 2.940775]\n",
      "epoch:4 step:3493 [D loss: 0.870921, acc.: 60.94%] [G loss: 5.024329]\n",
      "epoch:4 step:3494 [D loss: 1.030773, acc.: 64.06%] [G loss: 2.448217]\n",
      "epoch:4 step:3495 [D loss: 0.489411, acc.: 71.88%] [G loss: 4.146729]\n",
      "epoch:4 step:3496 [D loss: 0.619205, acc.: 67.19%] [G loss: 1.579952]\n",
      "epoch:4 step:3497 [D loss: 0.351547, acc.: 85.16%] [G loss: 2.780709]\n",
      "epoch:4 step:3498 [D loss: 0.490839, acc.: 75.00%] [G loss: 1.405225]\n",
      "epoch:4 step:3499 [D loss: 0.507448, acc.: 70.31%] [G loss: 1.648325]\n",
      "epoch:4 step:3500 [D loss: 0.406140, acc.: 79.69%] [G loss: 2.121919]\n",
      "epoch:4 step:3501 [D loss: 0.309342, acc.: 91.41%] [G loss: 2.085727]\n",
      "epoch:4 step:3502 [D loss: 0.468187, acc.: 77.34%] [G loss: 1.753644]\n",
      "epoch:4 step:3503 [D loss: 0.419771, acc.: 79.69%] [G loss: 1.946509]\n",
      "epoch:4 step:3504 [D loss: 0.421783, acc.: 78.12%] [G loss: 2.561441]\n",
      "epoch:4 step:3505 [D loss: 0.407218, acc.: 82.03%] [G loss: 2.899892]\n",
      "epoch:4 step:3506 [D loss: 0.425742, acc.: 82.03%] [G loss: 2.205687]\n",
      "epoch:4 step:3507 [D loss: 0.512634, acc.: 73.44%] [G loss: 2.037753]\n",
      "epoch:4 step:3508 [D loss: 0.455050, acc.: 75.78%] [G loss: 1.921173]\n",
      "epoch:4 step:3509 [D loss: 0.362693, acc.: 84.38%] [G loss: 2.521714]\n",
      "epoch:4 step:3510 [D loss: 0.367797, acc.: 85.16%] [G loss: 1.926935]\n",
      "epoch:4 step:3511 [D loss: 0.386740, acc.: 83.59%] [G loss: 2.213241]\n",
      "epoch:4 step:3512 [D loss: 0.388679, acc.: 82.03%] [G loss: 2.725207]\n",
      "epoch:4 step:3513 [D loss: 0.390320, acc.: 86.72%] [G loss: 2.318901]\n",
      "epoch:4 step:3514 [D loss: 0.411612, acc.: 78.91%] [G loss: 2.452491]\n",
      "epoch:4 step:3515 [D loss: 0.402872, acc.: 82.03%] [G loss: 2.501022]\n",
      "epoch:4 step:3516 [D loss: 0.378318, acc.: 83.59%] [G loss: 2.287570]\n",
      "epoch:4 step:3517 [D loss: 0.473362, acc.: 79.69%] [G loss: 2.226585]\n",
      "epoch:4 step:3518 [D loss: 0.430662, acc.: 77.34%] [G loss: 2.384181]\n",
      "epoch:4 step:3519 [D loss: 0.423447, acc.: 79.69%] [G loss: 2.009623]\n",
      "epoch:4 step:3520 [D loss: 0.401584, acc.: 79.69%] [G loss: 2.782355]\n",
      "epoch:4 step:3521 [D loss: 0.409893, acc.: 80.47%] [G loss: 1.909299]\n",
      "epoch:4 step:3522 [D loss: 0.469314, acc.: 75.00%] [G loss: 2.525589]\n",
      "epoch:4 step:3523 [D loss: 0.474945, acc.: 74.22%] [G loss: 2.620546]\n",
      "epoch:4 step:3524 [D loss: 0.404126, acc.: 80.47%] [G loss: 2.093466]\n",
      "epoch:4 step:3525 [D loss: 0.420935, acc.: 78.91%] [G loss: 2.455182]\n",
      "epoch:4 step:3526 [D loss: 0.369935, acc.: 83.59%] [G loss: 2.038718]\n",
      "epoch:4 step:3527 [D loss: 0.393666, acc.: 81.25%] [G loss: 2.320620]\n",
      "epoch:4 step:3528 [D loss: 0.446196, acc.: 82.81%] [G loss: 2.182601]\n",
      "epoch:4 step:3529 [D loss: 0.382834, acc.: 78.91%] [G loss: 2.393282]\n",
      "epoch:4 step:3530 [D loss: 0.494386, acc.: 74.22%] [G loss: 2.052810]\n",
      "epoch:4 step:3531 [D loss: 0.415075, acc.: 82.03%] [G loss: 2.631924]\n",
      "epoch:4 step:3532 [D loss: 0.462818, acc.: 78.91%] [G loss: 2.402400]\n",
      "epoch:4 step:3533 [D loss: 0.409146, acc.: 78.12%] [G loss: 1.931046]\n",
      "epoch:4 step:3534 [D loss: 0.379351, acc.: 82.81%] [G loss: 1.911471]\n",
      "epoch:4 step:3535 [D loss: 0.331593, acc.: 86.72%] [G loss: 2.171804]\n",
      "epoch:4 step:3536 [D loss: 0.326838, acc.: 89.06%] [G loss: 2.507049]\n",
      "epoch:4 step:3537 [D loss: 0.369169, acc.: 82.81%] [G loss: 2.521542]\n",
      "epoch:4 step:3538 [D loss: 0.357051, acc.: 82.81%] [G loss: 2.687702]\n",
      "epoch:4 step:3539 [D loss: 0.405914, acc.: 82.03%] [G loss: 2.665407]\n",
      "epoch:4 step:3540 [D loss: 0.390114, acc.: 79.69%] [G loss: 3.247153]\n",
      "epoch:4 step:3541 [D loss: 0.320172, acc.: 89.84%] [G loss: 3.133488]\n",
      "epoch:4 step:3542 [D loss: 0.343997, acc.: 87.50%] [G loss: 2.356757]\n",
      "epoch:4 step:3543 [D loss: 0.307893, acc.: 87.50%] [G loss: 3.577122]\n",
      "epoch:4 step:3544 [D loss: 0.375445, acc.: 82.81%] [G loss: 2.332175]\n",
      "epoch:4 step:3545 [D loss: 0.340118, acc.: 84.38%] [G loss: 2.586878]\n",
      "epoch:4 step:3546 [D loss: 0.350138, acc.: 82.03%] [G loss: 2.376280]\n",
      "epoch:4 step:3547 [D loss: 0.357331, acc.: 84.38%] [G loss: 2.411759]\n",
      "epoch:4 step:3548 [D loss: 0.379102, acc.: 82.81%] [G loss: 2.785012]\n",
      "epoch:4 step:3549 [D loss: 0.375053, acc.: 80.47%] [G loss: 3.019349]\n",
      "epoch:4 step:3550 [D loss: 0.368713, acc.: 85.16%] [G loss: 2.509373]\n",
      "epoch:4 step:3551 [D loss: 0.413698, acc.: 80.47%] [G loss: 2.589711]\n",
      "epoch:4 step:3552 [D loss: 0.447307, acc.: 80.47%] [G loss: 2.116545]\n",
      "epoch:4 step:3553 [D loss: 0.404252, acc.: 81.25%] [G loss: 2.323509]\n",
      "epoch:4 step:3554 [D loss: 0.522914, acc.: 74.22%] [G loss: 2.358892]\n",
      "epoch:4 step:3555 [D loss: 0.554586, acc.: 74.22%] [G loss: 3.739333]\n",
      "epoch:4 step:3556 [D loss: 0.386582, acc.: 85.16%] [G loss: 2.655202]\n",
      "epoch:4 step:3557 [D loss: 0.469619, acc.: 75.00%] [G loss: 3.072085]\n",
      "epoch:4 step:3558 [D loss: 0.488093, acc.: 75.00%] [G loss: 2.738763]\n",
      "epoch:4 step:3559 [D loss: 0.604325, acc.: 66.41%] [G loss: 4.424310]\n",
      "epoch:4 step:3560 [D loss: 1.499814, acc.: 50.78%] [G loss: 6.171053]\n",
      "epoch:4 step:3561 [D loss: 1.398282, acc.: 60.94%] [G loss: 3.105676]\n",
      "epoch:4 step:3562 [D loss: 0.629515, acc.: 69.53%] [G loss: 4.027177]\n",
      "epoch:4 step:3563 [D loss: 0.729010, acc.: 59.38%] [G loss: 1.966768]\n",
      "epoch:4 step:3564 [D loss: 0.540785, acc.: 75.78%] [G loss: 2.471351]\n",
      "epoch:4 step:3565 [D loss: 0.378730, acc.: 85.16%] [G loss: 2.202529]\n",
      "epoch:4 step:3566 [D loss: 0.422610, acc.: 77.34%] [G loss: 2.072921]\n",
      "epoch:4 step:3567 [D loss: 0.454490, acc.: 73.44%] [G loss: 2.492301]\n",
      "epoch:4 step:3568 [D loss: 0.451741, acc.: 75.78%] [G loss: 1.930347]\n",
      "epoch:4 step:3569 [D loss: 0.442454, acc.: 78.91%] [G loss: 1.818956]\n",
      "epoch:4 step:3570 [D loss: 0.370218, acc.: 89.06%] [G loss: 2.163765]\n",
      "epoch:4 step:3571 [D loss: 0.465428, acc.: 74.22%] [G loss: 1.901525]\n",
      "epoch:4 step:3572 [D loss: 0.434303, acc.: 78.91%] [G loss: 2.490273]\n",
      "epoch:4 step:3573 [D loss: 0.443022, acc.: 81.25%] [G loss: 2.018992]\n",
      "epoch:4 step:3574 [D loss: 0.431029, acc.: 78.91%] [G loss: 2.146954]\n",
      "epoch:4 step:3575 [D loss: 0.403317, acc.: 78.91%] [G loss: 1.888866]\n",
      "epoch:4 step:3576 [D loss: 0.340020, acc.: 89.06%] [G loss: 2.153961]\n",
      "epoch:4 step:3577 [D loss: 0.436545, acc.: 78.91%] [G loss: 2.027879]\n",
      "epoch:4 step:3578 [D loss: 0.324093, acc.: 90.62%] [G loss: 2.207474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3579 [D loss: 0.327580, acc.: 89.06%] [G loss: 2.710045]\n",
      "epoch:4 step:3580 [D loss: 0.363196, acc.: 82.03%] [G loss: 2.165367]\n",
      "epoch:4 step:3581 [D loss: 0.446970, acc.: 75.78%] [G loss: 2.136594]\n",
      "epoch:4 step:3582 [D loss: 0.380807, acc.: 79.69%] [G loss: 1.940177]\n",
      "epoch:4 step:3583 [D loss: 0.350391, acc.: 84.38%] [G loss: 2.934598]\n",
      "epoch:4 step:3584 [D loss: 0.392234, acc.: 84.38%] [G loss: 1.838721]\n",
      "epoch:4 step:3585 [D loss: 0.401925, acc.: 80.47%] [G loss: 2.291511]\n",
      "epoch:4 step:3586 [D loss: 0.394013, acc.: 82.03%] [G loss: 2.349772]\n",
      "epoch:4 step:3587 [D loss: 0.381729, acc.: 84.38%] [G loss: 2.074553]\n",
      "epoch:4 step:3588 [D loss: 0.317213, acc.: 88.28%] [G loss: 2.692214]\n",
      "epoch:4 step:3589 [D loss: 0.356351, acc.: 85.94%] [G loss: 3.305399]\n",
      "epoch:4 step:3590 [D loss: 0.464222, acc.: 77.34%] [G loss: 2.481439]\n",
      "epoch:4 step:3591 [D loss: 0.380341, acc.: 83.59%] [G loss: 2.657544]\n",
      "epoch:4 step:3592 [D loss: 0.378125, acc.: 82.03%] [G loss: 2.243187]\n",
      "epoch:4 step:3593 [D loss: 0.445794, acc.: 75.00%] [G loss: 3.406039]\n",
      "epoch:4 step:3594 [D loss: 0.449740, acc.: 78.12%] [G loss: 2.280613]\n",
      "epoch:4 step:3595 [D loss: 0.400312, acc.: 79.69%] [G loss: 2.227779]\n",
      "epoch:4 step:3596 [D loss: 0.387875, acc.: 85.16%] [G loss: 2.499160]\n",
      "epoch:4 step:3597 [D loss: 0.373600, acc.: 79.69%] [G loss: 3.721759]\n",
      "epoch:4 step:3598 [D loss: 0.436783, acc.: 79.69%] [G loss: 3.280945]\n",
      "epoch:4 step:3599 [D loss: 0.433088, acc.: 78.12%] [G loss: 2.827218]\n",
      "epoch:4 step:3600 [D loss: 0.397275, acc.: 80.47%] [G loss: 2.485925]\n",
      "epoch:4 step:3601 [D loss: 0.370184, acc.: 85.16%] [G loss: 2.683877]\n",
      "epoch:4 step:3602 [D loss: 0.291017, acc.: 89.06%] [G loss: 3.147383]\n",
      "epoch:4 step:3603 [D loss: 0.372999, acc.: 81.25%] [G loss: 2.327288]\n",
      "epoch:4 step:3604 [D loss: 0.352749, acc.: 83.59%] [G loss: 2.338572]\n",
      "epoch:4 step:3605 [D loss: 0.442240, acc.: 71.09%] [G loss: 2.743703]\n",
      "epoch:4 step:3606 [D loss: 0.421676, acc.: 74.22%] [G loss: 2.182580]\n",
      "epoch:4 step:3607 [D loss: 0.374885, acc.: 84.38%] [G loss: 3.144481]\n",
      "epoch:4 step:3608 [D loss: 0.459525, acc.: 70.31%] [G loss: 2.247589]\n",
      "epoch:4 step:3609 [D loss: 0.338242, acc.: 83.59%] [G loss: 2.613873]\n",
      "epoch:4 step:3610 [D loss: 0.431325, acc.: 78.12%] [G loss: 2.284559]\n",
      "epoch:4 step:3611 [D loss: 0.330872, acc.: 87.50%] [G loss: 3.566680]\n",
      "epoch:4 step:3612 [D loss: 0.477203, acc.: 75.00%] [G loss: 3.044292]\n",
      "epoch:4 step:3613 [D loss: 0.398656, acc.: 78.91%] [G loss: 3.363622]\n",
      "epoch:4 step:3614 [D loss: 0.405928, acc.: 82.03%] [G loss: 2.872447]\n",
      "epoch:4 step:3615 [D loss: 0.387078, acc.: 82.81%] [G loss: 2.243668]\n",
      "epoch:4 step:3616 [D loss: 0.315771, acc.: 89.06%] [G loss: 2.960738]\n",
      "epoch:4 step:3617 [D loss: 0.369607, acc.: 80.47%] [G loss: 2.590884]\n",
      "epoch:4 step:3618 [D loss: 0.369959, acc.: 86.72%] [G loss: 2.389759]\n",
      "epoch:4 step:3619 [D loss: 0.358719, acc.: 81.25%] [G loss: 2.377184]\n",
      "epoch:4 step:3620 [D loss: 0.459053, acc.: 77.34%] [G loss: 2.660277]\n",
      "epoch:4 step:3621 [D loss: 0.390050, acc.: 82.81%] [G loss: 3.177058]\n",
      "epoch:4 step:3622 [D loss: 0.404637, acc.: 79.69%] [G loss: 2.778404]\n",
      "epoch:4 step:3623 [D loss: 0.341601, acc.: 85.94%] [G loss: 3.487864]\n",
      "epoch:4 step:3624 [D loss: 0.390667, acc.: 81.25%] [G loss: 3.487065]\n",
      "epoch:4 step:3625 [D loss: 0.405669, acc.: 79.69%] [G loss: 2.714833]\n",
      "epoch:4 step:3626 [D loss: 0.530896, acc.: 70.31%] [G loss: 2.514655]\n",
      "epoch:4 step:3627 [D loss: 0.390090, acc.: 79.69%] [G loss: 3.388473]\n",
      "epoch:4 step:3628 [D loss: 0.653160, acc.: 67.97%] [G loss: 5.010612]\n",
      "epoch:4 step:3629 [D loss: 1.109957, acc.: 54.69%] [G loss: 4.623633]\n",
      "epoch:4 step:3630 [D loss: 1.281091, acc.: 57.03%] [G loss: 2.328662]\n",
      "epoch:4 step:3631 [D loss: 0.540617, acc.: 72.66%] [G loss: 2.778580]\n",
      "epoch:4 step:3632 [D loss: 0.536361, acc.: 72.66%] [G loss: 2.409577]\n",
      "epoch:4 step:3633 [D loss: 0.540928, acc.: 72.66%] [G loss: 2.261726]\n",
      "epoch:4 step:3634 [D loss: 0.415287, acc.: 78.91%] [G loss: 2.703275]\n",
      "epoch:4 step:3635 [D loss: 0.453205, acc.: 78.91%] [G loss: 1.707460]\n",
      "epoch:4 step:3636 [D loss: 0.464141, acc.: 74.22%] [G loss: 2.217515]\n",
      "epoch:4 step:3637 [D loss: 0.396524, acc.: 85.94%] [G loss: 1.894785]\n",
      "epoch:4 step:3638 [D loss: 0.430839, acc.: 80.47%] [G loss: 2.225008]\n",
      "epoch:4 step:3639 [D loss: 0.371841, acc.: 85.94%] [G loss: 2.226093]\n",
      "epoch:4 step:3640 [D loss: 0.444546, acc.: 77.34%] [G loss: 2.037879]\n",
      "epoch:4 step:3641 [D loss: 0.396914, acc.: 82.03%] [G loss: 1.848254]\n",
      "epoch:4 step:3642 [D loss: 0.430346, acc.: 82.81%] [G loss: 1.987882]\n",
      "epoch:4 step:3643 [D loss: 0.384617, acc.: 84.38%] [G loss: 2.218240]\n",
      "epoch:4 step:3644 [D loss: 0.411874, acc.: 76.56%] [G loss: 2.295954]\n",
      "epoch:4 step:3645 [D loss: 0.356601, acc.: 88.28%] [G loss: 2.129854]\n",
      "epoch:4 step:3646 [D loss: 0.346626, acc.: 86.72%] [G loss: 2.533322]\n",
      "epoch:4 step:3647 [D loss: 0.351837, acc.: 87.50%] [G loss: 2.309359]\n",
      "epoch:4 step:3648 [D loss: 0.342857, acc.: 86.72%] [G loss: 2.501511]\n",
      "epoch:4 step:3649 [D loss: 0.379505, acc.: 83.59%] [G loss: 2.974313]\n",
      "epoch:4 step:3650 [D loss: 0.346000, acc.: 87.50%] [G loss: 2.135260]\n",
      "epoch:4 step:3651 [D loss: 0.332316, acc.: 84.38%] [G loss: 2.811885]\n",
      "epoch:4 step:3652 [D loss: 0.398538, acc.: 78.12%] [G loss: 2.284671]\n",
      "epoch:4 step:3653 [D loss: 0.412795, acc.: 82.03%] [G loss: 2.500995]\n",
      "epoch:4 step:3654 [D loss: 0.355405, acc.: 84.38%] [G loss: 2.978344]\n",
      "epoch:4 step:3655 [D loss: 0.384407, acc.: 82.03%] [G loss: 2.507459]\n",
      "epoch:4 step:3656 [D loss: 0.392844, acc.: 78.91%] [G loss: 2.279748]\n",
      "epoch:4 step:3657 [D loss: 0.332655, acc.: 85.16%] [G loss: 2.601927]\n",
      "epoch:4 step:3658 [D loss: 0.376892, acc.: 81.25%] [G loss: 3.572447]\n",
      "epoch:4 step:3659 [D loss: 0.473103, acc.: 74.22%] [G loss: 2.689979]\n",
      "epoch:4 step:3660 [D loss: 0.481250, acc.: 73.44%] [G loss: 3.144326]\n",
      "epoch:4 step:3661 [D loss: 0.428792, acc.: 75.78%] [G loss: 2.969463]\n",
      "epoch:4 step:3662 [D loss: 0.376685, acc.: 82.03%] [G loss: 2.735170]\n",
      "epoch:4 step:3663 [D loss: 0.421611, acc.: 79.69%] [G loss: 2.312528]\n",
      "epoch:4 step:3664 [D loss: 0.424742, acc.: 82.03%] [G loss: 2.613682]\n",
      "epoch:4 step:3665 [D loss: 0.342758, acc.: 84.38%] [G loss: 2.753777]\n",
      "epoch:4 step:3666 [D loss: 0.398576, acc.: 82.03%] [G loss: 2.411347]\n",
      "epoch:4 step:3667 [D loss: 0.341142, acc.: 85.94%] [G loss: 3.600103]\n",
      "epoch:4 step:3668 [D loss: 0.327293, acc.: 89.06%] [G loss: 3.301644]\n",
      "epoch:4 step:3669 [D loss: 0.518693, acc.: 76.56%] [G loss: 2.395228]\n",
      "epoch:4 step:3670 [D loss: 0.519246, acc.: 70.31%] [G loss: 1.801438]\n",
      "epoch:4 step:3671 [D loss: 0.408405, acc.: 78.12%] [G loss: 2.267863]\n",
      "epoch:4 step:3672 [D loss: 0.400732, acc.: 81.25%] [G loss: 3.243821]\n",
      "epoch:4 step:3673 [D loss: 0.347273, acc.: 85.16%] [G loss: 1.903281]\n",
      "epoch:4 step:3674 [D loss: 0.371208, acc.: 84.38%] [G loss: 2.449584]\n",
      "epoch:4 step:3675 [D loss: 0.358784, acc.: 84.38%] [G loss: 2.119487]\n",
      "epoch:4 step:3676 [D loss: 0.409075, acc.: 82.03%] [G loss: 2.510567]\n",
      "epoch:4 step:3677 [D loss: 0.383818, acc.: 82.03%] [G loss: 5.991653]\n",
      "epoch:4 step:3678 [D loss: 0.630988, acc.: 71.09%] [G loss: 2.922393]\n",
      "epoch:4 step:3679 [D loss: 0.573191, acc.: 71.09%] [G loss: 3.287583]\n",
      "epoch:4 step:3680 [D loss: 0.492907, acc.: 73.44%] [G loss: 2.641616]\n",
      "epoch:4 step:3681 [D loss: 0.508371, acc.: 71.09%] [G loss: 2.550710]\n",
      "epoch:4 step:3682 [D loss: 0.420910, acc.: 78.91%] [G loss: 2.590755]\n",
      "epoch:4 step:3683 [D loss: 0.403985, acc.: 78.12%] [G loss: 1.944756]\n",
      "epoch:4 step:3684 [D loss: 0.482639, acc.: 78.91%] [G loss: 2.134111]\n",
      "epoch:4 step:3685 [D loss: 0.417723, acc.: 82.03%] [G loss: 2.214487]\n",
      "epoch:4 step:3686 [D loss: 0.378745, acc.: 82.03%] [G loss: 2.582824]\n",
      "epoch:4 step:3687 [D loss: 0.330744, acc.: 86.72%] [G loss: 2.760027]\n",
      "epoch:4 step:3688 [D loss: 0.432781, acc.: 78.91%] [G loss: 3.483056]\n",
      "epoch:4 step:3689 [D loss: 0.368299, acc.: 81.25%] [G loss: 2.976751]\n",
      "epoch:4 step:3690 [D loss: 0.282722, acc.: 87.50%] [G loss: 2.777233]\n",
      "epoch:4 step:3691 [D loss: 0.340440, acc.: 85.16%] [G loss: 2.840434]\n",
      "epoch:4 step:3692 [D loss: 0.344445, acc.: 87.50%] [G loss: 2.477398]\n",
      "epoch:4 step:3693 [D loss: 0.408580, acc.: 78.91%] [G loss: 2.527759]\n",
      "epoch:4 step:3694 [D loss: 0.417059, acc.: 80.47%] [G loss: 2.977262]\n",
      "epoch:4 step:3695 [D loss: 0.313886, acc.: 86.72%] [G loss: 3.089638]\n",
      "epoch:4 step:3696 [D loss: 0.337470, acc.: 85.94%] [G loss: 2.715497]\n",
      "epoch:4 step:3697 [D loss: 0.376303, acc.: 82.03%] [G loss: 2.265405]\n",
      "epoch:4 step:3698 [D loss: 0.431463, acc.: 78.91%] [G loss: 3.179889]\n",
      "epoch:4 step:3699 [D loss: 0.366990, acc.: 80.47%] [G loss: 3.425547]\n",
      "epoch:4 step:3700 [D loss: 0.340897, acc.: 89.06%] [G loss: 2.012455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3701 [D loss: 0.467351, acc.: 78.12%] [G loss: 2.366951]\n",
      "epoch:4 step:3702 [D loss: 0.542159, acc.: 70.31%] [G loss: 3.094125]\n",
      "epoch:4 step:3703 [D loss: 0.505373, acc.: 81.25%] [G loss: 3.714858]\n",
      "epoch:4 step:3704 [D loss: 0.543985, acc.: 67.97%] [G loss: 3.343743]\n",
      "epoch:4 step:3705 [D loss: 0.470803, acc.: 78.12%] [G loss: 2.719896]\n",
      "epoch:4 step:3706 [D loss: 0.421234, acc.: 77.34%] [G loss: 2.192070]\n",
      "epoch:4 step:3707 [D loss: 0.349393, acc.: 85.16%] [G loss: 2.857799]\n",
      "epoch:4 step:3708 [D loss: 0.343348, acc.: 85.94%] [G loss: 1.930513]\n",
      "epoch:4 step:3709 [D loss: 0.400967, acc.: 81.25%] [G loss: 2.721581]\n",
      "epoch:4 step:3710 [D loss: 0.417733, acc.: 79.69%] [G loss: 2.216447]\n",
      "epoch:4 step:3711 [D loss: 0.349920, acc.: 82.03%] [G loss: 2.440578]\n",
      "epoch:4 step:3712 [D loss: 0.381588, acc.: 83.59%] [G loss: 2.786191]\n",
      "epoch:4 step:3713 [D loss: 0.406003, acc.: 81.25%] [G loss: 2.590964]\n",
      "epoch:4 step:3714 [D loss: 0.365208, acc.: 79.69%] [G loss: 2.537293]\n",
      "epoch:4 step:3715 [D loss: 0.367569, acc.: 80.47%] [G loss: 2.275122]\n",
      "epoch:4 step:3716 [D loss: 0.421482, acc.: 77.34%] [G loss: 2.631697]\n",
      "epoch:4 step:3717 [D loss: 0.412967, acc.: 79.69%] [G loss: 2.235944]\n",
      "epoch:4 step:3718 [D loss: 0.385928, acc.: 85.16%] [G loss: 2.358689]\n",
      "epoch:4 step:3719 [D loss: 0.439197, acc.: 75.00%] [G loss: 2.765763]\n",
      "epoch:4 step:3720 [D loss: 0.497025, acc.: 76.56%] [G loss: 4.005843]\n",
      "epoch:4 step:3721 [D loss: 0.671853, acc.: 68.75%] [G loss: 4.444962]\n",
      "epoch:4 step:3722 [D loss: 0.897967, acc.: 67.19%] [G loss: 5.336222]\n",
      "epoch:4 step:3723 [D loss: 1.208767, acc.: 50.78%] [G loss: 4.100833]\n",
      "epoch:4 step:3724 [D loss: 0.675096, acc.: 67.19%] [G loss: 4.637616]\n",
      "epoch:4 step:3725 [D loss: 0.362368, acc.: 81.25%] [G loss: 3.053477]\n",
      "epoch:4 step:3726 [D loss: 0.507805, acc.: 69.53%] [G loss: 3.437329]\n",
      "epoch:4 step:3727 [D loss: 0.383021, acc.: 83.59%] [G loss: 2.323750]\n",
      "epoch:4 step:3728 [D loss: 0.542888, acc.: 71.88%] [G loss: 2.227049]\n",
      "epoch:4 step:3729 [D loss: 0.383728, acc.: 83.59%] [G loss: 2.454261]\n",
      "epoch:4 step:3730 [D loss: 0.426336, acc.: 78.91%] [G loss: 2.374423]\n",
      "epoch:4 step:3731 [D loss: 0.365109, acc.: 84.38%] [G loss: 2.388092]\n",
      "epoch:4 step:3732 [D loss: 0.430132, acc.: 82.03%] [G loss: 2.758966]\n",
      "epoch:4 step:3733 [D loss: 0.378625, acc.: 82.81%] [G loss: 2.705942]\n",
      "epoch:4 step:3734 [D loss: 0.297658, acc.: 89.06%] [G loss: 2.158706]\n",
      "epoch:4 step:3735 [D loss: 0.423333, acc.: 82.81%] [G loss: 2.273112]\n",
      "epoch:4 step:3736 [D loss: 0.443754, acc.: 79.69%] [G loss: 1.840956]\n",
      "epoch:4 step:3737 [D loss: 0.546875, acc.: 65.62%] [G loss: 2.275470]\n",
      "epoch:4 step:3738 [D loss: 0.629431, acc.: 71.09%] [G loss: 2.264983]\n",
      "epoch:4 step:3739 [D loss: 0.404850, acc.: 83.59%] [G loss: 2.531196]\n",
      "epoch:4 step:3740 [D loss: 0.339285, acc.: 83.59%] [G loss: 3.184315]\n",
      "epoch:4 step:3741 [D loss: 0.450254, acc.: 76.56%] [G loss: 3.489967]\n",
      "epoch:4 step:3742 [D loss: 0.382857, acc.: 78.91%] [G loss: 2.732640]\n",
      "epoch:4 step:3743 [D loss: 0.364896, acc.: 84.38%] [G loss: 2.057428]\n",
      "epoch:4 step:3744 [D loss: 0.329013, acc.: 88.28%] [G loss: 2.285237]\n",
      "epoch:4 step:3745 [D loss: 0.371684, acc.: 82.81%] [G loss: 2.697508]\n",
      "epoch:4 step:3746 [D loss: 0.383241, acc.: 81.25%] [G loss: 2.270456]\n",
      "epoch:4 step:3747 [D loss: 0.335419, acc.: 87.50%] [G loss: 2.574381]\n",
      "epoch:4 step:3748 [D loss: 0.278062, acc.: 92.97%] [G loss: 3.282702]\n",
      "epoch:4 step:3749 [D loss: 0.334442, acc.: 84.38%] [G loss: 2.564010]\n",
      "epoch:4 step:3750 [D loss: 0.331092, acc.: 89.06%] [G loss: 2.826682]\n",
      "epoch:4 step:3751 [D loss: 0.389179, acc.: 79.69%] [G loss: 3.564954]\n",
      "epoch:4 step:3752 [D loss: 0.411638, acc.: 77.34%] [G loss: 2.759322]\n",
      "epoch:4 step:3753 [D loss: 0.464345, acc.: 81.25%] [G loss: 2.772364]\n",
      "epoch:4 step:3754 [D loss: 0.488792, acc.: 73.44%] [G loss: 2.299489]\n",
      "epoch:4 step:3755 [D loss: 0.448539, acc.: 75.00%] [G loss: 1.637005]\n",
      "epoch:4 step:3756 [D loss: 0.465820, acc.: 81.25%] [G loss: 2.526922]\n",
      "epoch:4 step:3757 [D loss: 0.503808, acc.: 70.31%] [G loss: 3.715248]\n",
      "epoch:4 step:3758 [D loss: 0.665484, acc.: 57.81%] [G loss: 3.926513]\n",
      "epoch:4 step:3759 [D loss: 0.840652, acc.: 65.62%] [G loss: 3.410485]\n",
      "epoch:4 step:3760 [D loss: 0.607688, acc.: 67.97%] [G loss: 2.737488]\n",
      "epoch:4 step:3761 [D loss: 0.439408, acc.: 77.34%] [G loss: 2.873771]\n",
      "epoch:4 step:3762 [D loss: 0.426067, acc.: 82.81%] [G loss: 2.419001]\n",
      "epoch:4 step:3763 [D loss: 0.420393, acc.: 78.91%] [G loss: 2.723081]\n",
      "epoch:4 step:3764 [D loss: 0.475748, acc.: 75.78%] [G loss: 2.391529]\n",
      "epoch:4 step:3765 [D loss: 0.396747, acc.: 81.25%] [G loss: 2.339068]\n",
      "epoch:4 step:3766 [D loss: 0.397468, acc.: 82.81%] [G loss: 1.945213]\n",
      "epoch:4 step:3767 [D loss: 0.337878, acc.: 85.94%] [G loss: 2.220999]\n",
      "epoch:4 step:3768 [D loss: 0.370199, acc.: 85.94%] [G loss: 2.458660]\n",
      "epoch:4 step:3769 [D loss: 0.477585, acc.: 76.56%] [G loss: 1.931823]\n",
      "epoch:4 step:3770 [D loss: 0.378145, acc.: 82.81%] [G loss: 2.500382]\n",
      "epoch:4 step:3771 [D loss: 0.353543, acc.: 86.72%] [G loss: 2.590952]\n",
      "epoch:4 step:3772 [D loss: 0.494074, acc.: 74.22%] [G loss: 1.876041]\n",
      "epoch:4 step:3773 [D loss: 0.388022, acc.: 82.81%] [G loss: 3.086703]\n",
      "epoch:4 step:3774 [D loss: 0.419720, acc.: 80.47%] [G loss: 3.390058]\n",
      "epoch:4 step:3775 [D loss: 0.518998, acc.: 73.44%] [G loss: 3.106717]\n",
      "epoch:4 step:3776 [D loss: 0.643339, acc.: 67.97%] [G loss: 3.084855]\n",
      "epoch:4 step:3777 [D loss: 0.553091, acc.: 72.66%] [G loss: 4.072309]\n",
      "epoch:4 step:3778 [D loss: 0.577212, acc.: 74.22%] [G loss: 2.410786]\n",
      "epoch:4 step:3779 [D loss: 0.406695, acc.: 81.25%] [G loss: 2.505978]\n",
      "epoch:4 step:3780 [D loss: 0.315716, acc.: 85.94%] [G loss: 3.254344]\n",
      "epoch:4 step:3781 [D loss: 0.389259, acc.: 79.69%] [G loss: 2.901406]\n",
      "epoch:4 step:3782 [D loss: 0.402783, acc.: 82.81%] [G loss: 2.091509]\n",
      "epoch:4 step:3783 [D loss: 0.439860, acc.: 75.78%] [G loss: 2.047865]\n",
      "epoch:4 step:3784 [D loss: 0.504510, acc.: 78.91%] [G loss: 2.296837]\n",
      "epoch:4 step:3785 [D loss: 0.495525, acc.: 74.22%] [G loss: 4.136030]\n",
      "epoch:4 step:3786 [D loss: 0.371020, acc.: 84.38%] [G loss: 2.277868]\n",
      "epoch:4 step:3787 [D loss: 0.463917, acc.: 77.34%] [G loss: 3.583780]\n",
      "epoch:4 step:3788 [D loss: 0.435525, acc.: 77.34%] [G loss: 2.030068]\n",
      "epoch:4 step:3789 [D loss: 0.394843, acc.: 79.69%] [G loss: 2.367702]\n",
      "epoch:4 step:3790 [D loss: 0.408402, acc.: 79.69%] [G loss: 2.373644]\n",
      "epoch:4 step:3791 [D loss: 0.381037, acc.: 83.59%] [G loss: 3.028957]\n",
      "epoch:4 step:3792 [D loss: 0.336846, acc.: 85.94%] [G loss: 3.023382]\n",
      "epoch:4 step:3793 [D loss: 0.401768, acc.: 82.81%] [G loss: 2.873718]\n",
      "epoch:4 step:3794 [D loss: 0.394057, acc.: 80.47%] [G loss: 3.018115]\n",
      "epoch:4 step:3795 [D loss: 0.356697, acc.: 85.16%] [G loss: 2.880892]\n",
      "epoch:4 step:3796 [D loss: 0.400461, acc.: 80.47%] [G loss: 2.187441]\n",
      "epoch:4 step:3797 [D loss: 0.334230, acc.: 84.38%] [G loss: 2.706989]\n",
      "epoch:4 step:3798 [D loss: 0.414495, acc.: 77.34%] [G loss: 2.262926]\n",
      "epoch:4 step:3799 [D loss: 0.426714, acc.: 75.78%] [G loss: 2.733095]\n",
      "epoch:4 step:3800 [D loss: 0.417209, acc.: 80.47%] [G loss: 3.438779]\n",
      "epoch:4 step:3801 [D loss: 0.514197, acc.: 73.44%] [G loss: 2.647730]\n",
      "epoch:4 step:3802 [D loss: 0.453532, acc.: 81.25%] [G loss: 4.234995]\n",
      "epoch:4 step:3803 [D loss: 0.488098, acc.: 73.44%] [G loss: 2.644584]\n",
      "epoch:4 step:3804 [D loss: 0.426408, acc.: 79.69%] [G loss: 2.899274]\n",
      "epoch:4 step:3805 [D loss: 0.398029, acc.: 75.78%] [G loss: 2.932168]\n",
      "epoch:4 step:3806 [D loss: 0.449008, acc.: 72.66%] [G loss: 2.308689]\n",
      "epoch:4 step:3807 [D loss: 0.368304, acc.: 82.81%] [G loss: 4.063708]\n",
      "epoch:4 step:3808 [D loss: 0.577303, acc.: 66.41%] [G loss: 4.152511]\n",
      "epoch:4 step:3809 [D loss: 0.494531, acc.: 73.44%] [G loss: 1.905843]\n",
      "epoch:4 step:3810 [D loss: 0.353741, acc.: 80.47%] [G loss: 2.061386]\n",
      "epoch:4 step:3811 [D loss: 0.457091, acc.: 78.91%] [G loss: 1.858750]\n",
      "epoch:4 step:3812 [D loss: 0.355760, acc.: 82.81%] [G loss: 1.894472]\n",
      "epoch:4 step:3813 [D loss: 0.360265, acc.: 85.94%] [G loss: 2.495116]\n",
      "epoch:4 step:3814 [D loss: 0.406310, acc.: 78.12%] [G loss: 2.052753]\n",
      "epoch:4 step:3815 [D loss: 0.399805, acc.: 80.47%] [G loss: 1.762297]\n",
      "epoch:4 step:3816 [D loss: 0.382080, acc.: 84.38%] [G loss: 2.177269]\n",
      "epoch:4 step:3817 [D loss: 0.383793, acc.: 84.38%] [G loss: 2.504000]\n",
      "epoch:4 step:3818 [D loss: 0.391639, acc.: 82.03%] [G loss: 2.409324]\n",
      "epoch:4 step:3819 [D loss: 0.396160, acc.: 78.91%] [G loss: 2.653317]\n",
      "epoch:4 step:3820 [D loss: 0.300183, acc.: 88.28%] [G loss: 2.715281]\n",
      "epoch:4 step:3821 [D loss: 0.328997, acc.: 84.38%] [G loss: 2.849720]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3822 [D loss: 0.365105, acc.: 84.38%] [G loss: 2.014704]\n",
      "epoch:4 step:3823 [D loss: 0.350161, acc.: 84.38%] [G loss: 2.739448]\n",
      "epoch:4 step:3824 [D loss: 0.297773, acc.: 89.06%] [G loss: 2.695616]\n",
      "epoch:4 step:3825 [D loss: 0.356588, acc.: 85.94%] [G loss: 2.452528]\n",
      "epoch:4 step:3826 [D loss: 0.338311, acc.: 86.72%] [G loss: 2.551495]\n",
      "epoch:4 step:3827 [D loss: 0.375361, acc.: 83.59%] [G loss: 2.675161]\n",
      "epoch:4 step:3828 [D loss: 0.361854, acc.: 82.81%] [G loss: 2.375173]\n",
      "epoch:4 step:3829 [D loss: 0.399255, acc.: 78.91%] [G loss: 2.679534]\n",
      "epoch:4 step:3830 [D loss: 0.489994, acc.: 74.22%] [G loss: 3.641943]\n",
      "epoch:4 step:3831 [D loss: 0.545300, acc.: 72.66%] [G loss: 2.954340]\n",
      "epoch:4 step:3832 [D loss: 0.739112, acc.: 71.09%] [G loss: 3.339749]\n",
      "epoch:4 step:3833 [D loss: 0.623072, acc.: 68.75%] [G loss: 4.077065]\n",
      "epoch:4 step:3834 [D loss: 0.658996, acc.: 65.62%] [G loss: 2.866281]\n",
      "epoch:4 step:3835 [D loss: 0.334477, acc.: 88.28%] [G loss: 2.467041]\n",
      "epoch:4 step:3836 [D loss: 0.464611, acc.: 77.34%] [G loss: 2.335986]\n",
      "epoch:4 step:3837 [D loss: 0.436766, acc.: 78.91%] [G loss: 2.410634]\n",
      "epoch:4 step:3838 [D loss: 0.416326, acc.: 75.78%] [G loss: 1.969371]\n",
      "epoch:4 step:3839 [D loss: 0.411931, acc.: 83.59%] [G loss: 1.950322]\n",
      "epoch:4 step:3840 [D loss: 0.433322, acc.: 79.69%] [G loss: 1.824240]\n",
      "epoch:4 step:3841 [D loss: 0.390227, acc.: 84.38%] [G loss: 2.368939]\n",
      "epoch:4 step:3842 [D loss: 0.459905, acc.: 77.34%] [G loss: 2.435779]\n",
      "epoch:4 step:3843 [D loss: 0.495287, acc.: 75.78%] [G loss: 1.649990]\n",
      "epoch:4 step:3844 [D loss: 0.408889, acc.: 82.03%] [G loss: 1.893340]\n",
      "epoch:4 step:3845 [D loss: 0.371182, acc.: 85.94%] [G loss: 2.636060]\n",
      "epoch:4 step:3846 [D loss: 0.465162, acc.: 78.91%] [G loss: 1.837284]\n",
      "epoch:4 step:3847 [D loss: 0.361788, acc.: 84.38%] [G loss: 3.313180]\n",
      "epoch:4 step:3848 [D loss: 0.559425, acc.: 66.41%] [G loss: 2.276372]\n",
      "epoch:4 step:3849 [D loss: 0.359422, acc.: 82.81%] [G loss: 2.713660]\n",
      "epoch:4 step:3850 [D loss: 0.334074, acc.: 89.06%] [G loss: 2.546946]\n",
      "epoch:4 step:3851 [D loss: 0.372262, acc.: 84.38%] [G loss: 2.154873]\n",
      "epoch:4 step:3852 [D loss: 0.381804, acc.: 84.38%] [G loss: 2.618136]\n",
      "epoch:4 step:3853 [D loss: 0.258395, acc.: 92.97%] [G loss: 2.966736]\n",
      "epoch:4 step:3854 [D loss: 0.448670, acc.: 73.44%] [G loss: 2.327713]\n",
      "epoch:4 step:3855 [D loss: 0.269383, acc.: 89.84%] [G loss: 2.790745]\n",
      "epoch:4 step:3856 [D loss: 0.301148, acc.: 85.94%] [G loss: 3.328779]\n",
      "epoch:4 step:3857 [D loss: 0.304433, acc.: 90.62%] [G loss: 3.034915]\n",
      "epoch:4 step:3858 [D loss: 0.303904, acc.: 87.50%] [G loss: 3.136832]\n",
      "epoch:4 step:3859 [D loss: 0.424299, acc.: 79.69%] [G loss: 2.447093]\n",
      "epoch:4 step:3860 [D loss: 0.382584, acc.: 77.34%] [G loss: 2.483230]\n",
      "epoch:4 step:3861 [D loss: 0.384901, acc.: 81.25%] [G loss: 2.656318]\n",
      "epoch:4 step:3862 [D loss: 0.358883, acc.: 83.59%] [G loss: 2.467024]\n",
      "epoch:4 step:3863 [D loss: 0.355433, acc.: 81.25%] [G loss: 2.339812]\n",
      "epoch:4 step:3864 [D loss: 0.319700, acc.: 89.06%] [G loss: 2.442747]\n",
      "epoch:4 step:3865 [D loss: 0.414995, acc.: 81.25%] [G loss: 2.008434]\n",
      "epoch:4 step:3866 [D loss: 0.424732, acc.: 75.78%] [G loss: 2.192556]\n",
      "epoch:4 step:3867 [D loss: 0.298104, acc.: 87.50%] [G loss: 2.754298]\n",
      "epoch:4 step:3868 [D loss: 0.391522, acc.: 80.47%] [G loss: 2.874958]\n",
      "epoch:4 step:3869 [D loss: 0.408051, acc.: 77.34%] [G loss: 2.329947]\n",
      "epoch:4 step:3870 [D loss: 0.362159, acc.: 84.38%] [G loss: 2.455000]\n",
      "epoch:4 step:3871 [D loss: 0.371748, acc.: 82.81%] [G loss: 2.995879]\n",
      "epoch:4 step:3872 [D loss: 0.299722, acc.: 90.62%] [G loss: 2.806876]\n",
      "epoch:4 step:3873 [D loss: 0.349810, acc.: 84.38%] [G loss: 3.133094]\n",
      "epoch:4 step:3874 [D loss: 0.351963, acc.: 85.94%] [G loss: 2.176647]\n",
      "epoch:4 step:3875 [D loss: 0.344480, acc.: 83.59%] [G loss: 2.607987]\n",
      "epoch:4 step:3876 [D loss: 0.347000, acc.: 84.38%] [G loss: 3.756757]\n",
      "epoch:4 step:3877 [D loss: 0.361685, acc.: 82.03%] [G loss: 2.870591]\n",
      "epoch:4 step:3878 [D loss: 0.400381, acc.: 78.91%] [G loss: 2.944046]\n",
      "epoch:4 step:3879 [D loss: 0.290900, acc.: 86.72%] [G loss: 2.739866]\n",
      "epoch:4 step:3880 [D loss: 0.374499, acc.: 80.47%] [G loss: 3.060603]\n",
      "epoch:4 step:3881 [D loss: 0.324416, acc.: 89.84%] [G loss: 3.203247]\n",
      "epoch:4 step:3882 [D loss: 0.400896, acc.: 76.56%] [G loss: 2.279829]\n",
      "epoch:4 step:3883 [D loss: 0.365702, acc.: 82.81%] [G loss: 2.010314]\n",
      "epoch:4 step:3884 [D loss: 0.369629, acc.: 83.59%] [G loss: 2.367395]\n",
      "epoch:4 step:3885 [D loss: 0.379659, acc.: 80.47%] [G loss: 2.039693]\n",
      "epoch:4 step:3886 [D loss: 0.408338, acc.: 77.34%] [G loss: 2.296975]\n",
      "epoch:4 step:3887 [D loss: 0.476956, acc.: 77.34%] [G loss: 2.480170]\n",
      "epoch:4 step:3888 [D loss: 0.396308, acc.: 83.59%] [G loss: 2.343309]\n",
      "epoch:4 step:3889 [D loss: 0.526354, acc.: 69.53%] [G loss: 4.278172]\n",
      "epoch:4 step:3890 [D loss: 0.873872, acc.: 64.06%] [G loss: 5.955158]\n",
      "epoch:4 step:3891 [D loss: 0.778744, acc.: 67.19%] [G loss: 2.874721]\n",
      "epoch:4 step:3892 [D loss: 0.534734, acc.: 71.88%] [G loss: 2.454263]\n",
      "epoch:4 step:3893 [D loss: 0.361467, acc.: 84.38%] [G loss: 2.464159]\n",
      "epoch:4 step:3894 [D loss: 0.345930, acc.: 84.38%] [G loss: 1.956204]\n",
      "epoch:4 step:3895 [D loss: 0.371345, acc.: 82.03%] [G loss: 2.527219]\n",
      "epoch:4 step:3896 [D loss: 0.363343, acc.: 81.25%] [G loss: 3.616921]\n",
      "epoch:4 step:3897 [D loss: 0.373641, acc.: 79.69%] [G loss: 2.994914]\n",
      "epoch:4 step:3898 [D loss: 0.404195, acc.: 78.12%] [G loss: 3.998296]\n",
      "epoch:4 step:3899 [D loss: 0.458910, acc.: 78.12%] [G loss: 3.141314]\n",
      "epoch:4 step:3900 [D loss: 0.337375, acc.: 83.59%] [G loss: 3.208457]\n",
      "epoch:4 step:3901 [D loss: 0.377797, acc.: 82.03%] [G loss: 2.737340]\n",
      "epoch:4 step:3902 [D loss: 0.331589, acc.: 85.16%] [G loss: 2.607081]\n",
      "epoch:4 step:3903 [D loss: 0.422971, acc.: 78.12%] [G loss: 3.246921]\n",
      "epoch:4 step:3904 [D loss: 0.415949, acc.: 79.69%] [G loss: 2.534784]\n",
      "epoch:4 step:3905 [D loss: 0.388151, acc.: 85.16%] [G loss: 2.709793]\n",
      "epoch:5 step:3906 [D loss: 0.406026, acc.: 81.25%] [G loss: 2.545845]\n",
      "epoch:5 step:3907 [D loss: 0.336100, acc.: 84.38%] [G loss: 2.951158]\n",
      "epoch:5 step:3908 [D loss: 0.412902, acc.: 82.81%] [G loss: 2.766268]\n",
      "epoch:5 step:3909 [D loss: 0.407025, acc.: 81.25%] [G loss: 3.240652]\n",
      "epoch:5 step:3910 [D loss: 0.320265, acc.: 89.06%] [G loss: 3.647721]\n",
      "epoch:5 step:3911 [D loss: 0.315983, acc.: 87.50%] [G loss: 4.248829]\n",
      "epoch:5 step:3912 [D loss: 0.308864, acc.: 89.06%] [G loss: 3.474198]\n",
      "epoch:5 step:3913 [D loss: 0.424001, acc.: 79.69%] [G loss: 2.722963]\n",
      "epoch:5 step:3914 [D loss: 0.315887, acc.: 85.94%] [G loss: 3.166654]\n",
      "epoch:5 step:3915 [D loss: 0.473485, acc.: 75.00%] [G loss: 2.007934]\n",
      "epoch:5 step:3916 [D loss: 0.370424, acc.: 81.25%] [G loss: 3.747902]\n",
      "epoch:5 step:3917 [D loss: 0.400483, acc.: 81.25%] [G loss: 2.600764]\n",
      "epoch:5 step:3918 [D loss: 0.357137, acc.: 85.16%] [G loss: 3.753644]\n",
      "epoch:5 step:3919 [D loss: 0.661479, acc.: 72.66%] [G loss: 5.050914]\n",
      "epoch:5 step:3920 [D loss: 1.072739, acc.: 59.38%] [G loss: 4.564715]\n",
      "epoch:5 step:3921 [D loss: 0.987556, acc.: 58.59%] [G loss: 1.825095]\n",
      "epoch:5 step:3922 [D loss: 0.394697, acc.: 79.69%] [G loss: 2.539597]\n",
      "epoch:5 step:3923 [D loss: 0.471672, acc.: 75.78%] [G loss: 1.721878]\n",
      "epoch:5 step:3924 [D loss: 0.520943, acc.: 71.09%] [G loss: 2.779039]\n",
      "epoch:5 step:3925 [D loss: 0.392588, acc.: 83.59%] [G loss: 2.738817]\n",
      "epoch:5 step:3926 [D loss: 0.374437, acc.: 81.25%] [G loss: 2.914068]\n",
      "epoch:5 step:3927 [D loss: 0.398507, acc.: 78.91%] [G loss: 2.331561]\n",
      "epoch:5 step:3928 [D loss: 0.422940, acc.: 77.34%] [G loss: 2.347546]\n",
      "epoch:5 step:3929 [D loss: 0.405441, acc.: 82.03%] [G loss: 3.450608]\n",
      "epoch:5 step:3930 [D loss: 0.381245, acc.: 82.81%] [G loss: 2.794731]\n",
      "epoch:5 step:3931 [D loss: 0.281550, acc.: 86.72%] [G loss: 2.873695]\n",
      "epoch:5 step:3932 [D loss: 0.361309, acc.: 79.69%] [G loss: 2.361929]\n",
      "epoch:5 step:3933 [D loss: 0.412369, acc.: 83.59%] [G loss: 2.397433]\n",
      "epoch:5 step:3934 [D loss: 0.424534, acc.: 76.56%] [G loss: 2.676118]\n",
      "epoch:5 step:3935 [D loss: 0.400661, acc.: 83.59%] [G loss: 1.655300]\n",
      "epoch:5 step:3936 [D loss: 0.410356, acc.: 79.69%] [G loss: 3.118882]\n",
      "epoch:5 step:3937 [D loss: 0.444576, acc.: 77.34%] [G loss: 2.818984]\n",
      "epoch:5 step:3938 [D loss: 0.338754, acc.: 83.59%] [G loss: 2.680707]\n",
      "epoch:5 step:3939 [D loss: 0.321537, acc.: 88.28%] [G loss: 2.315407]\n",
      "epoch:5 step:3940 [D loss: 0.460161, acc.: 76.56%] [G loss: 1.938681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:3941 [D loss: 0.379848, acc.: 84.38%] [G loss: 2.974061]\n",
      "epoch:5 step:3942 [D loss: 0.395014, acc.: 81.25%] [G loss: 3.198348]\n",
      "epoch:5 step:3943 [D loss: 0.340641, acc.: 82.81%] [G loss: 3.161567]\n",
      "epoch:5 step:3944 [D loss: 0.362544, acc.: 85.16%] [G loss: 2.553240]\n",
      "epoch:5 step:3945 [D loss: 0.490773, acc.: 72.66%] [G loss: 1.751391]\n",
      "epoch:5 step:3946 [D loss: 0.403998, acc.: 83.59%] [G loss: 2.499821]\n",
      "epoch:5 step:3947 [D loss: 0.460133, acc.: 75.78%] [G loss: 3.574818]\n",
      "epoch:5 step:3948 [D loss: 0.609699, acc.: 64.84%] [G loss: 2.970953]\n",
      "epoch:5 step:3949 [D loss: 0.813950, acc.: 58.59%] [G loss: 3.208804]\n",
      "epoch:5 step:3950 [D loss: 1.195204, acc.: 51.56%] [G loss: 3.366309]\n",
      "epoch:5 step:3951 [D loss: 0.445119, acc.: 79.69%] [G loss: 3.019228]\n",
      "epoch:5 step:3952 [D loss: 0.536727, acc.: 68.75%] [G loss: 2.708275]\n",
      "epoch:5 step:3953 [D loss: 0.301815, acc.: 88.28%] [G loss: 2.774696]\n",
      "epoch:5 step:3954 [D loss: 0.405936, acc.: 76.56%] [G loss: 2.791257]\n",
      "epoch:5 step:3955 [D loss: 0.327607, acc.: 85.16%] [G loss: 2.862198]\n",
      "epoch:5 step:3956 [D loss: 0.383802, acc.: 83.59%] [G loss: 2.578023]\n",
      "epoch:5 step:3957 [D loss: 0.417554, acc.: 78.91%] [G loss: 2.630780]\n",
      "epoch:5 step:3958 [D loss: 0.337970, acc.: 82.81%] [G loss: 3.386280]\n",
      "epoch:5 step:3959 [D loss: 0.408946, acc.: 78.12%] [G loss: 2.408692]\n",
      "epoch:5 step:3960 [D loss: 0.361558, acc.: 84.38%] [G loss: 2.435796]\n",
      "epoch:5 step:3961 [D loss: 0.425234, acc.: 82.81%] [G loss: 2.063772]\n",
      "epoch:5 step:3962 [D loss: 0.344538, acc.: 88.28%] [G loss: 1.996860]\n",
      "epoch:5 step:3963 [D loss: 0.374981, acc.: 84.38%] [G loss: 2.228349]\n",
      "epoch:5 step:3964 [D loss: 0.373560, acc.: 87.50%] [G loss: 2.205534]\n",
      "epoch:5 step:3965 [D loss: 0.365774, acc.: 82.03%] [G loss: 2.510735]\n",
      "epoch:5 step:3966 [D loss: 0.409971, acc.: 80.47%] [G loss: 2.665492]\n",
      "epoch:5 step:3967 [D loss: 0.347828, acc.: 85.94%] [G loss: 2.687726]\n",
      "epoch:5 step:3968 [D loss: 0.421277, acc.: 78.91%] [G loss: 2.493642]\n",
      "epoch:5 step:3969 [D loss: 0.458882, acc.: 74.22%] [G loss: 2.925854]\n",
      "epoch:5 step:3970 [D loss: 0.342016, acc.: 88.28%] [G loss: 2.516079]\n",
      "epoch:5 step:3971 [D loss: 0.370829, acc.: 82.03%] [G loss: 2.771693]\n",
      "epoch:5 step:3972 [D loss: 0.301032, acc.: 89.84%] [G loss: 2.638342]\n",
      "epoch:5 step:3973 [D loss: 0.319274, acc.: 90.62%] [G loss: 3.301439]\n",
      "epoch:5 step:3974 [D loss: 0.351203, acc.: 83.59%] [G loss: 3.124384]\n",
      "epoch:5 step:3975 [D loss: 0.360742, acc.: 83.59%] [G loss: 2.306374]\n",
      "epoch:5 step:3976 [D loss: 0.282103, acc.: 90.62%] [G loss: 2.823434]\n",
      "epoch:5 step:3977 [D loss: 0.367758, acc.: 82.03%] [G loss: 2.668729]\n",
      "epoch:5 step:3978 [D loss: 0.399920, acc.: 83.59%] [G loss: 2.325058]\n",
      "epoch:5 step:3979 [D loss: 0.401696, acc.: 83.59%] [G loss: 2.656870]\n",
      "epoch:5 step:3980 [D loss: 0.410560, acc.: 78.91%] [G loss: 2.611578]\n",
      "epoch:5 step:3981 [D loss: 0.438178, acc.: 79.69%] [G loss: 2.388042]\n",
      "epoch:5 step:3982 [D loss: 0.654028, acc.: 64.84%] [G loss: 4.590748]\n",
      "epoch:5 step:3983 [D loss: 0.922790, acc.: 62.50%] [G loss: 5.043921]\n",
      "epoch:5 step:3984 [D loss: 1.193913, acc.: 53.12%] [G loss: 2.160835]\n",
      "epoch:5 step:3985 [D loss: 0.456736, acc.: 71.09%] [G loss: 2.497471]\n",
      "epoch:5 step:3986 [D loss: 0.542381, acc.: 71.88%] [G loss: 1.596239]\n",
      "epoch:5 step:3987 [D loss: 0.530261, acc.: 75.78%] [G loss: 2.969419]\n",
      "epoch:5 step:3988 [D loss: 0.476130, acc.: 71.09%] [G loss: 2.329927]\n",
      "epoch:5 step:3989 [D loss: 0.386461, acc.: 80.47%] [G loss: 2.894539]\n",
      "epoch:5 step:3990 [D loss: 0.450943, acc.: 76.56%] [G loss: 2.117977]\n",
      "epoch:5 step:3991 [D loss: 0.390906, acc.: 75.78%] [G loss: 2.411877]\n",
      "epoch:5 step:3992 [D loss: 0.473330, acc.: 75.78%] [G loss: 2.657182]\n",
      "epoch:5 step:3993 [D loss: 0.332568, acc.: 82.03%] [G loss: 3.995190]\n",
      "epoch:5 step:3994 [D loss: 0.348669, acc.: 85.94%] [G loss: 2.701096]\n",
      "epoch:5 step:3995 [D loss: 0.414490, acc.: 79.69%] [G loss: 2.754184]\n",
      "epoch:5 step:3996 [D loss: 0.381861, acc.: 81.25%] [G loss: 2.040386]\n",
      "epoch:5 step:3997 [D loss: 0.320145, acc.: 89.06%] [G loss: 2.978025]\n",
      "epoch:5 step:3998 [D loss: 0.402228, acc.: 81.25%] [G loss: 2.222585]\n",
      "epoch:5 step:3999 [D loss: 0.325600, acc.: 86.72%] [G loss: 2.205092]\n",
      "epoch:5 step:4000 [D loss: 0.472208, acc.: 76.56%] [G loss: 2.491546]\n",
      "epoch:5 step:4001 [D loss: 0.378341, acc.: 81.25%] [G loss: 2.821775]\n",
      "epoch:5 step:4002 [D loss: 0.382377, acc.: 81.25%] [G loss: 2.873993]\n",
      "epoch:5 step:4003 [D loss: 0.438029, acc.: 78.91%] [G loss: 2.240681]\n",
      "epoch:5 step:4004 [D loss: 0.386718, acc.: 82.03%] [G loss: 2.620818]\n",
      "epoch:5 step:4005 [D loss: 0.453269, acc.: 73.44%] [G loss: 2.662302]\n",
      "epoch:5 step:4006 [D loss: 0.456949, acc.: 75.78%] [G loss: 4.302032]\n",
      "epoch:5 step:4007 [D loss: 0.702383, acc.: 64.84%] [G loss: 4.058915]\n",
      "epoch:5 step:4008 [D loss: 0.955349, acc.: 59.38%] [G loss: 3.822565]\n",
      "epoch:5 step:4009 [D loss: 1.176202, acc.: 47.66%] [G loss: 2.479278]\n",
      "epoch:5 step:4010 [D loss: 0.520334, acc.: 75.00%] [G loss: 2.692137]\n",
      "epoch:5 step:4011 [D loss: 0.573994, acc.: 67.19%] [G loss: 2.154906]\n",
      "epoch:5 step:4012 [D loss: 0.407149, acc.: 82.81%] [G loss: 3.083743]\n",
      "epoch:5 step:4013 [D loss: 0.342769, acc.: 82.81%] [G loss: 3.501024]\n",
      "epoch:5 step:4014 [D loss: 0.481355, acc.: 71.88%] [G loss: 2.251368]\n",
      "epoch:5 step:4015 [D loss: 0.432310, acc.: 78.12%] [G loss: 1.898665]\n",
      "epoch:5 step:4016 [D loss: 0.441430, acc.: 77.34%] [G loss: 2.473938]\n",
      "epoch:5 step:4017 [D loss: 0.405747, acc.: 83.59%] [G loss: 2.426880]\n",
      "epoch:5 step:4018 [D loss: 0.353836, acc.: 86.72%] [G loss: 2.339609]\n",
      "epoch:5 step:4019 [D loss: 0.433169, acc.: 77.34%] [G loss: 1.787871]\n",
      "epoch:5 step:4020 [D loss: 0.386503, acc.: 79.69%] [G loss: 2.215716]\n",
      "epoch:5 step:4021 [D loss: 0.398955, acc.: 79.69%] [G loss: 2.081031]\n",
      "epoch:5 step:4022 [D loss: 0.357387, acc.: 88.28%] [G loss: 2.806305]\n",
      "epoch:5 step:4023 [D loss: 0.330989, acc.: 85.94%] [G loss: 2.716853]\n",
      "epoch:5 step:4024 [D loss: 0.344372, acc.: 82.81%] [G loss: 3.218917]\n",
      "epoch:5 step:4025 [D loss: 0.250108, acc.: 90.62%] [G loss: 3.867593]\n",
      "epoch:5 step:4026 [D loss: 0.367374, acc.: 82.03%] [G loss: 2.481220]\n",
      "epoch:5 step:4027 [D loss: 0.370706, acc.: 77.34%] [G loss: 2.309239]\n",
      "epoch:5 step:4028 [D loss: 0.390000, acc.: 80.47%] [G loss: 2.394180]\n",
      "epoch:5 step:4029 [D loss: 0.388122, acc.: 84.38%] [G loss: 2.743411]\n",
      "epoch:5 step:4030 [D loss: 0.465354, acc.: 78.91%] [G loss: 2.216061]\n",
      "epoch:5 step:4031 [D loss: 0.409326, acc.: 86.72%] [G loss: 2.258669]\n",
      "epoch:5 step:4032 [D loss: 0.329456, acc.: 85.94%] [G loss: 2.540891]\n",
      "epoch:5 step:4033 [D loss: 0.431963, acc.: 78.91%] [G loss: 2.423132]\n",
      "epoch:5 step:4034 [D loss: 0.447005, acc.: 82.03%] [G loss: 2.606361]\n",
      "epoch:5 step:4035 [D loss: 0.302882, acc.: 88.28%] [G loss: 2.294775]\n",
      "epoch:5 step:4036 [D loss: 0.485126, acc.: 76.56%] [G loss: 2.426497]\n",
      "epoch:5 step:4037 [D loss: 0.367469, acc.: 85.94%] [G loss: 2.082207]\n",
      "epoch:5 step:4038 [D loss: 0.417614, acc.: 81.25%] [G loss: 1.930187]\n",
      "epoch:5 step:4039 [D loss: 0.326734, acc.: 88.28%] [G loss: 3.029099]\n",
      "epoch:5 step:4040 [D loss: 0.482962, acc.: 72.66%] [G loss: 2.739172]\n",
      "epoch:5 step:4041 [D loss: 0.439479, acc.: 73.44%] [G loss: 1.942773]\n",
      "epoch:5 step:4042 [D loss: 0.382222, acc.: 83.59%] [G loss: 2.453639]\n",
      "epoch:5 step:4043 [D loss: 0.417530, acc.: 83.59%] [G loss: 2.129501]\n",
      "epoch:5 step:4044 [D loss: 0.341764, acc.: 85.16%] [G loss: 2.523839]\n",
      "epoch:5 step:4045 [D loss: 0.464566, acc.: 75.78%] [G loss: 2.404375]\n",
      "epoch:5 step:4046 [D loss: 0.262533, acc.: 90.62%] [G loss: 3.387472]\n",
      "epoch:5 step:4047 [D loss: 0.435612, acc.: 78.12%] [G loss: 2.374541]\n",
      "epoch:5 step:4048 [D loss: 0.416504, acc.: 78.91%] [G loss: 2.397613]\n",
      "epoch:5 step:4049 [D loss: 0.310996, acc.: 89.84%] [G loss: 2.640974]\n",
      "epoch:5 step:4050 [D loss: 0.301045, acc.: 89.84%] [G loss: 2.543670]\n",
      "epoch:5 step:4051 [D loss: 0.341900, acc.: 85.16%] [G loss: 3.043622]\n",
      "epoch:5 step:4052 [D loss: 0.433938, acc.: 75.78%] [G loss: 1.884541]\n",
      "epoch:5 step:4053 [D loss: 0.420859, acc.: 78.12%] [G loss: 4.012462]\n",
      "epoch:5 step:4054 [D loss: 0.569100, acc.: 71.09%] [G loss: 3.162563]\n",
      "epoch:5 step:4055 [D loss: 0.542366, acc.: 75.00%] [G loss: 2.486884]\n",
      "epoch:5 step:4056 [D loss: 0.404818, acc.: 82.03%] [G loss: 2.041419]\n",
      "epoch:5 step:4057 [D loss: 0.422512, acc.: 78.91%] [G loss: 3.357586]\n",
      "epoch:5 step:4058 [D loss: 0.333114, acc.: 87.50%] [G loss: 2.883385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4059 [D loss: 0.393730, acc.: 82.03%] [G loss: 3.201827]\n",
      "epoch:5 step:4060 [D loss: 0.339811, acc.: 88.28%] [G loss: 3.008723]\n",
      "epoch:5 step:4061 [D loss: 0.355559, acc.: 83.59%] [G loss: 2.837565]\n",
      "epoch:5 step:4062 [D loss: 0.432321, acc.: 78.12%] [G loss: 2.991346]\n",
      "epoch:5 step:4063 [D loss: 0.428403, acc.: 78.12%] [G loss: 3.393732]\n",
      "epoch:5 step:4064 [D loss: 0.581545, acc.: 73.44%] [G loss: 3.670290]\n",
      "epoch:5 step:4065 [D loss: 0.879263, acc.: 66.41%] [G loss: 3.906069]\n",
      "epoch:5 step:4066 [D loss: 0.962246, acc.: 66.41%] [G loss: 2.916427]\n",
      "epoch:5 step:4067 [D loss: 0.391144, acc.: 80.47%] [G loss: 2.681638]\n",
      "epoch:5 step:4068 [D loss: 0.590047, acc.: 70.31%] [G loss: 2.655483]\n",
      "epoch:5 step:4069 [D loss: 0.419419, acc.: 80.47%] [G loss: 2.399260]\n",
      "epoch:5 step:4070 [D loss: 0.404289, acc.: 81.25%] [G loss: 2.088889]\n",
      "epoch:5 step:4071 [D loss: 0.338561, acc.: 85.16%] [G loss: 2.944148]\n",
      "epoch:5 step:4072 [D loss: 0.399828, acc.: 80.47%] [G loss: 3.394627]\n",
      "epoch:5 step:4073 [D loss: 0.420683, acc.: 77.34%] [G loss: 2.203124]\n",
      "epoch:5 step:4074 [D loss: 0.298961, acc.: 90.62%] [G loss: 3.631107]\n",
      "epoch:5 step:4075 [D loss: 0.446453, acc.: 78.91%] [G loss: 2.364774]\n",
      "epoch:5 step:4076 [D loss: 0.326197, acc.: 87.50%] [G loss: 3.192473]\n",
      "epoch:5 step:4077 [D loss: 0.453189, acc.: 74.22%] [G loss: 2.457602]\n",
      "epoch:5 step:4078 [D loss: 0.383358, acc.: 85.94%] [G loss: 2.451357]\n",
      "epoch:5 step:4079 [D loss: 0.341996, acc.: 85.94%] [G loss: 2.660101]\n",
      "epoch:5 step:4080 [D loss: 0.408447, acc.: 82.03%] [G loss: 2.257662]\n",
      "epoch:5 step:4081 [D loss: 0.292887, acc.: 88.28%] [G loss: 3.474498]\n",
      "epoch:5 step:4082 [D loss: 0.409337, acc.: 86.72%] [G loss: 3.239347]\n",
      "epoch:5 step:4083 [D loss: 0.352944, acc.: 80.47%] [G loss: 3.555899]\n",
      "epoch:5 step:4084 [D loss: 0.346367, acc.: 87.50%] [G loss: 1.959749]\n",
      "epoch:5 step:4085 [D loss: 0.418809, acc.: 83.59%] [G loss: 2.368245]\n",
      "epoch:5 step:4086 [D loss: 0.341265, acc.: 87.50%] [G loss: 2.784297]\n",
      "epoch:5 step:4087 [D loss: 0.260164, acc.: 92.19%] [G loss: 3.480299]\n",
      "epoch:5 step:4088 [D loss: 0.317381, acc.: 89.06%] [G loss: 2.698101]\n",
      "epoch:5 step:4089 [D loss: 0.307003, acc.: 89.06%] [G loss: 2.812595]\n",
      "epoch:5 step:4090 [D loss: 0.413833, acc.: 83.59%] [G loss: 2.579088]\n",
      "epoch:5 step:4091 [D loss: 0.355818, acc.: 83.59%] [G loss: 3.591881]\n",
      "epoch:5 step:4092 [D loss: 0.296611, acc.: 88.28%] [G loss: 4.117941]\n",
      "epoch:5 step:4093 [D loss: 0.324308, acc.: 88.28%] [G loss: 2.482035]\n",
      "epoch:5 step:4094 [D loss: 0.330837, acc.: 85.94%] [G loss: 2.705251]\n",
      "epoch:5 step:4095 [D loss: 0.451551, acc.: 78.12%] [G loss: 2.591758]\n",
      "epoch:5 step:4096 [D loss: 0.406938, acc.: 80.47%] [G loss: 3.187745]\n",
      "epoch:5 step:4097 [D loss: 0.468161, acc.: 75.78%] [G loss: 3.395021]\n",
      "epoch:5 step:4098 [D loss: 0.489992, acc.: 71.88%] [G loss: 2.466070]\n",
      "epoch:5 step:4099 [D loss: 0.420683, acc.: 78.91%] [G loss: 4.149466]\n",
      "epoch:5 step:4100 [D loss: 0.432275, acc.: 78.91%] [G loss: 3.464071]\n",
      "epoch:5 step:4101 [D loss: 0.237647, acc.: 91.41%] [G loss: 3.778164]\n",
      "epoch:5 step:4102 [D loss: 0.534567, acc.: 67.97%] [G loss: 2.058084]\n",
      "epoch:5 step:4103 [D loss: 0.408136, acc.: 83.59%] [G loss: 3.210418]\n",
      "epoch:5 step:4104 [D loss: 0.325588, acc.: 86.72%] [G loss: 3.042473]\n",
      "epoch:5 step:4105 [D loss: 0.470368, acc.: 78.91%] [G loss: 3.029234]\n",
      "epoch:5 step:4106 [D loss: 0.367407, acc.: 84.38%] [G loss: 2.815150]\n",
      "epoch:5 step:4107 [D loss: 0.458656, acc.: 80.47%] [G loss: 2.416156]\n",
      "epoch:5 step:4108 [D loss: 0.273647, acc.: 90.62%] [G loss: 3.519576]\n",
      "epoch:5 step:4109 [D loss: 0.373005, acc.: 79.69%] [G loss: 3.292666]\n",
      "epoch:5 step:4110 [D loss: 0.386416, acc.: 82.81%] [G loss: 2.605428]\n",
      "epoch:5 step:4111 [D loss: 0.453812, acc.: 75.00%] [G loss: 2.580559]\n",
      "epoch:5 step:4112 [D loss: 0.426245, acc.: 81.25%] [G loss: 2.792091]\n",
      "epoch:5 step:4113 [D loss: 0.510180, acc.: 74.22%] [G loss: 3.624147]\n",
      "epoch:5 step:4114 [D loss: 0.678298, acc.: 74.22%] [G loss: 4.229264]\n",
      "epoch:5 step:4115 [D loss: 0.378597, acc.: 82.03%] [G loss: 3.642628]\n",
      "epoch:5 step:4116 [D loss: 0.397846, acc.: 79.69%] [G loss: 4.170450]\n",
      "epoch:5 step:4117 [D loss: 0.504917, acc.: 75.00%] [G loss: 4.469869]\n",
      "epoch:5 step:4118 [D loss: 0.731783, acc.: 73.44%] [G loss: 2.980123]\n",
      "epoch:5 step:4119 [D loss: 0.495784, acc.: 78.12%] [G loss: 2.395022]\n",
      "epoch:5 step:4120 [D loss: 0.483179, acc.: 76.56%] [G loss: 2.594759]\n",
      "epoch:5 step:4121 [D loss: 0.372403, acc.: 82.03%] [G loss: 3.513747]\n",
      "epoch:5 step:4122 [D loss: 0.307522, acc.: 89.06%] [G loss: 3.293877]\n",
      "epoch:5 step:4123 [D loss: 0.444549, acc.: 79.69%] [G loss: 2.295401]\n",
      "epoch:5 step:4124 [D loss: 0.388984, acc.: 78.12%] [G loss: 2.901077]\n",
      "epoch:5 step:4125 [D loss: 0.313931, acc.: 88.28%] [G loss: 3.480238]\n",
      "epoch:5 step:4126 [D loss: 0.343757, acc.: 88.28%] [G loss: 2.645210]\n",
      "epoch:5 step:4127 [D loss: 0.340254, acc.: 88.28%] [G loss: 2.991466]\n",
      "epoch:5 step:4128 [D loss: 0.324176, acc.: 86.72%] [G loss: 2.492115]\n",
      "epoch:5 step:4129 [D loss: 0.381651, acc.: 84.38%] [G loss: 2.042833]\n",
      "epoch:5 step:4130 [D loss: 0.425691, acc.: 82.81%] [G loss: 2.165521]\n",
      "epoch:5 step:4131 [D loss: 0.367162, acc.: 84.38%] [G loss: 2.370484]\n",
      "epoch:5 step:4132 [D loss: 0.295874, acc.: 92.97%] [G loss: 2.823090]\n",
      "epoch:5 step:4133 [D loss: 0.442787, acc.: 75.78%] [G loss: 2.968877]\n",
      "epoch:5 step:4134 [D loss: 0.283002, acc.: 92.19%] [G loss: 2.769070]\n",
      "epoch:5 step:4135 [D loss: 0.332025, acc.: 90.62%] [G loss: 2.089624]\n",
      "epoch:5 step:4136 [D loss: 0.307410, acc.: 90.62%] [G loss: 2.774377]\n",
      "epoch:5 step:4137 [D loss: 0.355515, acc.: 85.16%] [G loss: 2.393215]\n",
      "epoch:5 step:4138 [D loss: 0.329254, acc.: 85.16%] [G loss: 2.794306]\n",
      "epoch:5 step:4139 [D loss: 0.357069, acc.: 84.38%] [G loss: 2.576000]\n",
      "epoch:5 step:4140 [D loss: 0.462776, acc.: 73.44%] [G loss: 4.156002]\n",
      "epoch:5 step:4141 [D loss: 0.575243, acc.: 69.53%] [G loss: 2.727901]\n",
      "epoch:5 step:4142 [D loss: 0.630069, acc.: 69.53%] [G loss: 4.495041]\n",
      "epoch:5 step:4143 [D loss: 0.645690, acc.: 75.78%] [G loss: 3.116788]\n",
      "epoch:5 step:4144 [D loss: 0.595344, acc.: 68.75%] [G loss: 2.224031]\n",
      "epoch:5 step:4145 [D loss: 0.401119, acc.: 82.81%] [G loss: 2.036193]\n",
      "epoch:5 step:4146 [D loss: 0.406759, acc.: 80.47%] [G loss: 2.582923]\n",
      "epoch:5 step:4147 [D loss: 0.354132, acc.: 82.81%] [G loss: 3.275238]\n",
      "epoch:5 step:4148 [D loss: 0.368668, acc.: 83.59%] [G loss: 4.140269]\n",
      "epoch:5 step:4149 [D loss: 0.388211, acc.: 80.47%] [G loss: 3.524290]\n",
      "epoch:5 step:4150 [D loss: 0.377154, acc.: 83.59%] [G loss: 2.790990]\n",
      "epoch:5 step:4151 [D loss: 0.418119, acc.: 80.47%] [G loss: 2.041815]\n",
      "epoch:5 step:4152 [D loss: 0.327699, acc.: 84.38%] [G loss: 2.351801]\n",
      "epoch:5 step:4153 [D loss: 0.290486, acc.: 90.62%] [G loss: 2.290501]\n",
      "epoch:5 step:4154 [D loss: 0.348151, acc.: 86.72%] [G loss: 2.335839]\n",
      "epoch:5 step:4155 [D loss: 0.389537, acc.: 82.03%] [G loss: 2.373396]\n",
      "epoch:5 step:4156 [D loss: 0.357550, acc.: 85.94%] [G loss: 3.079288]\n",
      "epoch:5 step:4157 [D loss: 0.416964, acc.: 79.69%] [G loss: 2.089582]\n",
      "epoch:5 step:4158 [D loss: 0.411901, acc.: 81.25%] [G loss: 3.216922]\n",
      "epoch:5 step:4159 [D loss: 0.311819, acc.: 85.94%] [G loss: 3.855048]\n",
      "epoch:5 step:4160 [D loss: 0.322572, acc.: 88.28%] [G loss: 2.710995]\n",
      "epoch:5 step:4161 [D loss: 0.328089, acc.: 86.72%] [G loss: 2.650529]\n",
      "epoch:5 step:4162 [D loss: 0.352118, acc.: 85.16%] [G loss: 3.109431]\n",
      "epoch:5 step:4163 [D loss: 0.388175, acc.: 80.47%] [G loss: 2.988397]\n",
      "epoch:5 step:4164 [D loss: 0.316043, acc.: 89.84%] [G loss: 2.770270]\n",
      "epoch:5 step:4165 [D loss: 0.393780, acc.: 79.69%] [G loss: 3.349713]\n",
      "epoch:5 step:4166 [D loss: 0.379937, acc.: 84.38%] [G loss: 2.848310]\n",
      "epoch:5 step:4167 [D loss: 0.349569, acc.: 87.50%] [G loss: 2.447880]\n",
      "epoch:5 step:4168 [D loss: 0.373906, acc.: 85.16%] [G loss: 2.515781]\n",
      "epoch:5 step:4169 [D loss: 0.370101, acc.: 85.16%] [G loss: 2.063251]\n",
      "epoch:5 step:4170 [D loss: 0.389916, acc.: 85.94%] [G loss: 2.619710]\n",
      "epoch:5 step:4171 [D loss: 0.459529, acc.: 81.25%] [G loss: 4.014530]\n",
      "epoch:5 step:4172 [D loss: 0.807020, acc.: 67.19%] [G loss: 6.413404]\n",
      "epoch:5 step:4173 [D loss: 1.921417, acc.: 50.00%] [G loss: 3.924116]\n",
      "epoch:5 step:4174 [D loss: 0.602751, acc.: 75.00%] [G loss: 2.981200]\n",
      "epoch:5 step:4175 [D loss: 0.593169, acc.: 71.09%] [G loss: 2.103189]\n",
      "epoch:5 step:4176 [D loss: 0.481118, acc.: 75.78%] [G loss: 2.727087]\n",
      "epoch:5 step:4177 [D loss: 0.404253, acc.: 80.47%] [G loss: 2.339026]\n",
      "epoch:5 step:4178 [D loss: 0.451000, acc.: 78.91%] [G loss: 2.586647]\n",
      "epoch:5 step:4179 [D loss: 0.459095, acc.: 77.34%] [G loss: 2.266652]\n",
      "epoch:5 step:4180 [D loss: 0.517135, acc.: 70.31%] [G loss: 1.776205]\n",
      "epoch:5 step:4181 [D loss: 0.419927, acc.: 76.56%] [G loss: 2.049599]\n",
      "epoch:5 step:4182 [D loss: 0.300615, acc.: 89.84%] [G loss: 2.354072]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4183 [D loss: 0.384816, acc.: 84.38%] [G loss: 2.476511]\n",
      "epoch:5 step:4184 [D loss: 0.354919, acc.: 86.72%] [G loss: 2.830953]\n",
      "epoch:5 step:4185 [D loss: 0.354426, acc.: 82.81%] [G loss: 3.313669]\n",
      "epoch:5 step:4186 [D loss: 0.342195, acc.: 81.25%] [G loss: 2.392132]\n",
      "epoch:5 step:4187 [D loss: 0.403271, acc.: 78.91%] [G loss: 2.268445]\n",
      "epoch:5 step:4188 [D loss: 0.337701, acc.: 87.50%] [G loss: 2.286161]\n",
      "epoch:5 step:4189 [D loss: 0.392171, acc.: 79.69%] [G loss: 2.084921]\n",
      "epoch:5 step:4190 [D loss: 0.306490, acc.: 89.84%] [G loss: 3.813736]\n",
      "epoch:5 step:4191 [D loss: 0.463782, acc.: 75.78%] [G loss: 2.126040]\n",
      "epoch:5 step:4192 [D loss: 0.444467, acc.: 78.91%] [G loss: 2.189258]\n",
      "epoch:5 step:4193 [D loss: 0.488039, acc.: 75.78%] [G loss: 2.956187]\n",
      "epoch:5 step:4194 [D loss: 0.424705, acc.: 82.03%] [G loss: 2.471138]\n",
      "epoch:5 step:4195 [D loss: 0.305178, acc.: 87.50%] [G loss: 2.289610]\n",
      "epoch:5 step:4196 [D loss: 0.378391, acc.: 82.81%] [G loss: 2.812112]\n",
      "epoch:5 step:4197 [D loss: 0.381336, acc.: 81.25%] [G loss: 2.682231]\n",
      "epoch:5 step:4198 [D loss: 0.370221, acc.: 82.81%] [G loss: 1.997284]\n",
      "epoch:5 step:4199 [D loss: 0.347612, acc.: 82.03%] [G loss: 2.579448]\n",
      "epoch:5 step:4200 [D loss: 0.366189, acc.: 82.81%] [G loss: 2.211873]\n",
      "epoch:5 step:4201 [D loss: 0.282627, acc.: 89.84%] [G loss: 2.808439]\n",
      "epoch:5 step:4202 [D loss: 0.312663, acc.: 86.72%] [G loss: 3.411119]\n",
      "epoch:5 step:4203 [D loss: 0.295555, acc.: 91.41%] [G loss: 2.958481]\n",
      "epoch:5 step:4204 [D loss: 0.310502, acc.: 86.72%] [G loss: 2.702714]\n",
      "epoch:5 step:4205 [D loss: 0.309387, acc.: 89.06%] [G loss: 3.955588]\n",
      "epoch:5 step:4206 [D loss: 0.267751, acc.: 90.62%] [G loss: 2.475967]\n",
      "epoch:5 step:4207 [D loss: 0.310889, acc.: 87.50%] [G loss: 2.686268]\n",
      "epoch:5 step:4208 [D loss: 0.311712, acc.: 88.28%] [G loss: 2.789872]\n",
      "epoch:5 step:4209 [D loss: 0.357916, acc.: 87.50%] [G loss: 2.291384]\n",
      "epoch:5 step:4210 [D loss: 0.393180, acc.: 85.94%] [G loss: 1.875119]\n",
      "epoch:5 step:4211 [D loss: 0.351716, acc.: 88.28%] [G loss: 2.500261]\n",
      "epoch:5 step:4212 [D loss: 0.382457, acc.: 85.16%] [G loss: 2.659451]\n",
      "epoch:5 step:4213 [D loss: 0.481730, acc.: 71.88%] [G loss: 2.898996]\n",
      "epoch:5 step:4214 [D loss: 0.496295, acc.: 73.44%] [G loss: 4.397680]\n",
      "epoch:5 step:4215 [D loss: 0.524743, acc.: 78.12%] [G loss: 3.410748]\n",
      "epoch:5 step:4216 [D loss: 0.610239, acc.: 74.22%] [G loss: 3.041531]\n",
      "epoch:5 step:4217 [D loss: 0.380950, acc.: 82.03%] [G loss: 3.497220]\n",
      "epoch:5 step:4218 [D loss: 0.466949, acc.: 75.00%] [G loss: 2.626379]\n",
      "epoch:5 step:4219 [D loss: 0.354164, acc.: 86.72%] [G loss: 2.365364]\n",
      "epoch:5 step:4220 [D loss: 0.445355, acc.: 77.34%] [G loss: 1.966551]\n",
      "epoch:5 step:4221 [D loss: 0.384195, acc.: 83.59%] [G loss: 2.910260]\n",
      "epoch:5 step:4222 [D loss: 0.422344, acc.: 78.12%] [G loss: 4.472590]\n",
      "epoch:5 step:4223 [D loss: 0.549203, acc.: 77.34%] [G loss: 4.530605]\n",
      "epoch:5 step:4224 [D loss: 0.852565, acc.: 60.94%] [G loss: 1.842457]\n",
      "epoch:5 step:4225 [D loss: 0.458021, acc.: 77.34%] [G loss: 2.715134]\n",
      "epoch:5 step:4226 [D loss: 0.522258, acc.: 72.66%] [G loss: 2.108393]\n",
      "epoch:5 step:4227 [D loss: 0.403488, acc.: 81.25%] [G loss: 2.719576]\n",
      "epoch:5 step:4228 [D loss: 0.498114, acc.: 74.22%] [G loss: 2.308128]\n",
      "epoch:5 step:4229 [D loss: 0.429749, acc.: 80.47%] [G loss: 2.874931]\n",
      "epoch:5 step:4230 [D loss: 0.381022, acc.: 82.03%] [G loss: 2.220742]\n",
      "epoch:5 step:4231 [D loss: 0.300547, acc.: 87.50%] [G loss: 2.521729]\n",
      "epoch:5 step:4232 [D loss: 0.379957, acc.: 79.69%] [G loss: 2.096906]\n",
      "epoch:5 step:4233 [D loss: 0.478934, acc.: 82.03%] [G loss: 2.216720]\n",
      "epoch:5 step:4234 [D loss: 0.429059, acc.: 82.03%] [G loss: 2.329009]\n",
      "epoch:5 step:4235 [D loss: 0.375156, acc.: 81.25%] [G loss: 1.909019]\n",
      "epoch:5 step:4236 [D loss: 0.370198, acc.: 85.16%] [G loss: 2.543734]\n",
      "epoch:5 step:4237 [D loss: 0.309715, acc.: 86.72%] [G loss: 3.280154]\n",
      "epoch:5 step:4238 [D loss: 0.320672, acc.: 86.72%] [G loss: 4.150001]\n",
      "epoch:5 step:4239 [D loss: 0.429382, acc.: 76.56%] [G loss: 2.182727]\n",
      "epoch:5 step:4240 [D loss: 0.320685, acc.: 85.94%] [G loss: 3.356753]\n",
      "epoch:5 step:4241 [D loss: 0.281289, acc.: 91.41%] [G loss: 3.292102]\n",
      "epoch:5 step:4242 [D loss: 0.292580, acc.: 91.41%] [G loss: 3.488503]\n",
      "epoch:5 step:4243 [D loss: 0.349078, acc.: 85.16%] [G loss: 3.095801]\n",
      "epoch:5 step:4244 [D loss: 0.333939, acc.: 82.81%] [G loss: 3.008198]\n",
      "epoch:5 step:4245 [D loss: 0.401207, acc.: 83.59%] [G loss: 2.347779]\n",
      "epoch:5 step:4246 [D loss: 0.329118, acc.: 82.81%] [G loss: 2.421983]\n",
      "epoch:5 step:4247 [D loss: 0.320005, acc.: 85.94%] [G loss: 2.705716]\n",
      "epoch:5 step:4248 [D loss: 0.371398, acc.: 83.59%] [G loss: 2.022708]\n",
      "epoch:5 step:4249 [D loss: 0.353148, acc.: 83.59%] [G loss: 1.972527]\n",
      "epoch:5 step:4250 [D loss: 0.390319, acc.: 83.59%] [G loss: 1.985149]\n",
      "epoch:5 step:4251 [D loss: 0.347756, acc.: 88.28%] [G loss: 2.293246]\n",
      "epoch:5 step:4252 [D loss: 0.312465, acc.: 90.62%] [G loss: 2.525483]\n",
      "epoch:5 step:4253 [D loss: 0.313813, acc.: 89.84%] [G loss: 2.227494]\n",
      "epoch:5 step:4254 [D loss: 0.321738, acc.: 86.72%] [G loss: 2.649302]\n",
      "epoch:5 step:4255 [D loss: 0.317808, acc.: 87.50%] [G loss: 2.310378]\n",
      "epoch:5 step:4256 [D loss: 0.348731, acc.: 87.50%] [G loss: 2.094552]\n",
      "epoch:5 step:4257 [D loss: 0.364507, acc.: 87.50%] [G loss: 3.593766]\n",
      "epoch:5 step:4258 [D loss: 0.357693, acc.: 83.59%] [G loss: 2.097404]\n",
      "epoch:5 step:4259 [D loss: 0.340342, acc.: 86.72%] [G loss: 2.416249]\n",
      "epoch:5 step:4260 [D loss: 0.464367, acc.: 70.31%] [G loss: 4.179609]\n",
      "epoch:5 step:4261 [D loss: 0.664700, acc.: 72.66%] [G loss: 6.789159]\n",
      "epoch:5 step:4262 [D loss: 1.876819, acc.: 50.78%] [G loss: 3.232447]\n",
      "epoch:5 step:4263 [D loss: 0.994903, acc.: 60.16%] [G loss: 2.828084]\n",
      "epoch:5 step:4264 [D loss: 0.263112, acc.: 87.50%] [G loss: 3.401595]\n",
      "epoch:5 step:4265 [D loss: 0.599580, acc.: 71.09%] [G loss: 3.710371]\n",
      "epoch:5 step:4266 [D loss: 0.348324, acc.: 85.16%] [G loss: 3.010853]\n",
      "epoch:5 step:4267 [D loss: 0.555975, acc.: 70.31%] [G loss: 1.995622]\n",
      "epoch:5 step:4268 [D loss: 0.324898, acc.: 89.06%] [G loss: 2.967131]\n",
      "epoch:5 step:4269 [D loss: 0.319037, acc.: 86.72%] [G loss: 2.909589]\n",
      "epoch:5 step:4270 [D loss: 0.394493, acc.: 82.03%] [G loss: 2.919014]\n",
      "epoch:5 step:4271 [D loss: 0.403020, acc.: 78.12%] [G loss: 2.004877]\n",
      "epoch:5 step:4272 [D loss: 0.450404, acc.: 81.25%] [G loss: 2.181455]\n",
      "epoch:5 step:4273 [D loss: 0.354871, acc.: 88.28%] [G loss: 1.679894]\n",
      "epoch:5 step:4274 [D loss: 0.355544, acc.: 84.38%] [G loss: 2.041335]\n",
      "epoch:5 step:4275 [D loss: 0.325172, acc.: 89.06%] [G loss: 2.362990]\n",
      "epoch:5 step:4276 [D loss: 0.423088, acc.: 79.69%] [G loss: 2.101627]\n",
      "epoch:5 step:4277 [D loss: 0.398483, acc.: 82.81%] [G loss: 2.038025]\n",
      "epoch:5 step:4278 [D loss: 0.341562, acc.: 85.94%] [G loss: 2.373472]\n",
      "epoch:5 step:4279 [D loss: 0.477887, acc.: 75.00%] [G loss: 2.616081]\n",
      "epoch:5 step:4280 [D loss: 0.424540, acc.: 76.56%] [G loss: 2.594100]\n",
      "epoch:5 step:4281 [D loss: 0.374756, acc.: 81.25%] [G loss: 2.342937]\n",
      "epoch:5 step:4282 [D loss: 0.336743, acc.: 90.62%] [G loss: 2.489876]\n",
      "epoch:5 step:4283 [D loss: 0.429963, acc.: 78.91%] [G loss: 2.046083]\n",
      "epoch:5 step:4284 [D loss: 0.343134, acc.: 85.94%] [G loss: 1.999980]\n",
      "epoch:5 step:4285 [D loss: 0.305750, acc.: 87.50%] [G loss: 2.807681]\n",
      "epoch:5 step:4286 [D loss: 0.351855, acc.: 85.16%] [G loss: 2.835780]\n",
      "epoch:5 step:4287 [D loss: 0.408830, acc.: 84.38%] [G loss: 2.008322]\n",
      "epoch:5 step:4288 [D loss: 0.469300, acc.: 74.22%] [G loss: 2.830897]\n",
      "epoch:5 step:4289 [D loss: 0.485037, acc.: 75.00%] [G loss: 2.333591]\n",
      "epoch:5 step:4290 [D loss: 0.388506, acc.: 84.38%] [G loss: 1.767814]\n",
      "epoch:5 step:4291 [D loss: 0.341798, acc.: 85.94%] [G loss: 2.372868]\n",
      "epoch:5 step:4292 [D loss: 0.316047, acc.: 87.50%] [G loss: 2.626971]\n",
      "epoch:5 step:4293 [D loss: 0.370370, acc.: 83.59%] [G loss: 2.676069]\n",
      "epoch:5 step:4294 [D loss: 0.392082, acc.: 82.81%] [G loss: 1.944325]\n",
      "epoch:5 step:4295 [D loss: 0.289272, acc.: 89.84%] [G loss: 3.029381]\n",
      "epoch:5 step:4296 [D loss: 0.445068, acc.: 76.56%] [G loss: 2.492296]\n",
      "epoch:5 step:4297 [D loss: 0.337737, acc.: 85.16%] [G loss: 3.427040]\n",
      "epoch:5 step:4298 [D loss: 0.386284, acc.: 81.25%] [G loss: 2.029572]\n",
      "epoch:5 step:4299 [D loss: 0.373418, acc.: 78.12%] [G loss: 2.252227]\n",
      "epoch:5 step:4300 [D loss: 0.375994, acc.: 85.94%] [G loss: 3.248474]\n",
      "epoch:5 step:4301 [D loss: 0.416976, acc.: 81.25%] [G loss: 2.618217]\n",
      "epoch:5 step:4302 [D loss: 0.317472, acc.: 85.16%] [G loss: 2.358407]\n",
      "epoch:5 step:4303 [D loss: 0.371010, acc.: 78.12%] [G loss: 2.454130]\n",
      "epoch:5 step:4304 [D loss: 0.311127, acc.: 88.28%] [G loss: 2.339167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4305 [D loss: 0.351445, acc.: 84.38%] [G loss: 2.330084]\n",
      "epoch:5 step:4306 [D loss: 0.360539, acc.: 87.50%] [G loss: 2.761742]\n",
      "epoch:5 step:4307 [D loss: 0.314766, acc.: 85.16%] [G loss: 2.365026]\n",
      "epoch:5 step:4308 [D loss: 0.349509, acc.: 83.59%] [G loss: 4.535886]\n",
      "epoch:5 step:4309 [D loss: 0.269061, acc.: 91.41%] [G loss: 2.744269]\n",
      "epoch:5 step:4310 [D loss: 0.302894, acc.: 89.84%] [G loss: 2.474740]\n",
      "epoch:5 step:4311 [D loss: 0.380183, acc.: 81.25%] [G loss: 1.536768]\n",
      "epoch:5 step:4312 [D loss: 0.291555, acc.: 89.84%] [G loss: 2.280432]\n",
      "epoch:5 step:4313 [D loss: 0.358285, acc.: 87.50%] [G loss: 2.401237]\n",
      "epoch:5 step:4314 [D loss: 0.343456, acc.: 85.16%] [G loss: 2.844755]\n",
      "epoch:5 step:4315 [D loss: 0.348623, acc.: 82.81%] [G loss: 2.377621]\n",
      "epoch:5 step:4316 [D loss: 0.387426, acc.: 85.94%] [G loss: 2.321402]\n",
      "epoch:5 step:4317 [D loss: 0.400509, acc.: 77.34%] [G loss: 2.721105]\n",
      "epoch:5 step:4318 [D loss: 0.329634, acc.: 85.16%] [G loss: 3.175493]\n",
      "epoch:5 step:4319 [D loss: 0.370057, acc.: 83.59%] [G loss: 2.932225]\n",
      "epoch:5 step:4320 [D loss: 0.245793, acc.: 92.19%] [G loss: 4.463559]\n",
      "epoch:5 step:4321 [D loss: 0.235428, acc.: 93.75%] [G loss: 3.482644]\n",
      "epoch:5 step:4322 [D loss: 0.279094, acc.: 85.94%] [G loss: 3.481917]\n",
      "epoch:5 step:4323 [D loss: 0.262541, acc.: 90.62%] [G loss: 3.134805]\n",
      "epoch:5 step:4324 [D loss: 0.358495, acc.: 82.03%] [G loss: 2.701410]\n",
      "epoch:5 step:4325 [D loss: 0.349923, acc.: 85.94%] [G loss: 2.735047]\n",
      "epoch:5 step:4326 [D loss: 0.378023, acc.: 82.03%] [G loss: 2.898162]\n",
      "epoch:5 step:4327 [D loss: 0.402213, acc.: 82.03%] [G loss: 3.982533]\n",
      "epoch:5 step:4328 [D loss: 0.300454, acc.: 87.50%] [G loss: 3.959119]\n",
      "epoch:5 step:4329 [D loss: 0.388392, acc.: 84.38%] [G loss: 2.851985]\n",
      "epoch:5 step:4330 [D loss: 0.334484, acc.: 83.59%] [G loss: 2.558820]\n",
      "epoch:5 step:4331 [D loss: 0.335583, acc.: 84.38%] [G loss: 3.167377]\n",
      "epoch:5 step:4332 [D loss: 0.337178, acc.: 85.16%] [G loss: 3.131589]\n",
      "epoch:5 step:4333 [D loss: 0.392579, acc.: 78.12%] [G loss: 2.342776]\n",
      "epoch:5 step:4334 [D loss: 0.431935, acc.: 78.91%] [G loss: 3.114615]\n",
      "epoch:5 step:4335 [D loss: 0.480031, acc.: 74.22%] [G loss: 2.779543]\n",
      "epoch:5 step:4336 [D loss: 0.623140, acc.: 69.53%] [G loss: 3.466008]\n",
      "epoch:5 step:4337 [D loss: 0.592602, acc.: 74.22%] [G loss: 4.529509]\n",
      "epoch:5 step:4338 [D loss: 0.748397, acc.: 70.31%] [G loss: 4.377453]\n",
      "epoch:5 step:4339 [D loss: 0.618303, acc.: 73.44%] [G loss: 3.038189]\n",
      "epoch:5 step:4340 [D loss: 0.704181, acc.: 69.53%] [G loss: 3.333520]\n",
      "epoch:5 step:4341 [D loss: 0.943465, acc.: 53.91%] [G loss: 2.263987]\n",
      "epoch:5 step:4342 [D loss: 0.559111, acc.: 78.91%] [G loss: 2.444048]\n",
      "epoch:5 step:4343 [D loss: 0.636845, acc.: 74.22%] [G loss: 2.518122]\n",
      "epoch:5 step:4344 [D loss: 0.389794, acc.: 80.47%] [G loss: 2.269134]\n",
      "epoch:5 step:4345 [D loss: 0.487963, acc.: 74.22%] [G loss: 2.667310]\n",
      "epoch:5 step:4346 [D loss: 0.609171, acc.: 71.09%] [G loss: 2.257211]\n",
      "epoch:5 step:4347 [D loss: 0.348128, acc.: 90.62%] [G loss: 2.098804]\n",
      "epoch:5 step:4348 [D loss: 0.411776, acc.: 82.81%] [G loss: 1.835947]\n",
      "epoch:5 step:4349 [D loss: 0.537909, acc.: 70.31%] [G loss: 2.021369]\n",
      "epoch:5 step:4350 [D loss: 0.335158, acc.: 83.59%] [G loss: 2.295780]\n",
      "epoch:5 step:4351 [D loss: 0.350013, acc.: 85.16%] [G loss: 1.978433]\n",
      "epoch:5 step:4352 [D loss: 0.339219, acc.: 86.72%] [G loss: 2.178081]\n",
      "epoch:5 step:4353 [D loss: 0.412071, acc.: 80.47%] [G loss: 2.197573]\n",
      "epoch:5 step:4354 [D loss: 0.370605, acc.: 78.91%] [G loss: 2.088863]\n",
      "epoch:5 step:4355 [D loss: 0.290230, acc.: 91.41%] [G loss: 1.963867]\n",
      "epoch:5 step:4356 [D loss: 0.345902, acc.: 86.72%] [G loss: 1.996788]\n",
      "epoch:5 step:4357 [D loss: 0.302808, acc.: 92.97%] [G loss: 2.547288]\n",
      "epoch:5 step:4358 [D loss: 0.295784, acc.: 85.94%] [G loss: 2.196393]\n",
      "epoch:5 step:4359 [D loss: 0.348044, acc.: 85.16%] [G loss: 2.621893]\n",
      "epoch:5 step:4360 [D loss: 0.414201, acc.: 78.91%] [G loss: 2.170602]\n",
      "epoch:5 step:4361 [D loss: 0.356900, acc.: 85.94%] [G loss: 2.301666]\n",
      "epoch:5 step:4362 [D loss: 0.328150, acc.: 85.94%] [G loss: 3.201467]\n",
      "epoch:5 step:4363 [D loss: 0.217966, acc.: 91.41%] [G loss: 4.328732]\n",
      "epoch:5 step:4364 [D loss: 0.355737, acc.: 85.16%] [G loss: 2.803944]\n",
      "epoch:5 step:4365 [D loss: 0.255693, acc.: 92.19%] [G loss: 2.730276]\n",
      "epoch:5 step:4366 [D loss: 0.289810, acc.: 89.06%] [G loss: 3.384798]\n",
      "epoch:5 step:4367 [D loss: 0.346700, acc.: 86.72%] [G loss: 2.407200]\n",
      "epoch:5 step:4368 [D loss: 0.342798, acc.: 85.16%] [G loss: 2.293203]\n",
      "epoch:5 step:4369 [D loss: 0.299346, acc.: 86.72%] [G loss: 3.505863]\n",
      "epoch:5 step:4370 [D loss: 0.270489, acc.: 89.84%] [G loss: 3.280011]\n",
      "epoch:5 step:4371 [D loss: 0.308185, acc.: 88.28%] [G loss: 2.599917]\n",
      "epoch:5 step:4372 [D loss: 0.318822, acc.: 87.50%] [G loss: 1.759849]\n",
      "epoch:5 step:4373 [D loss: 0.297076, acc.: 89.06%] [G loss: 2.577066]\n",
      "epoch:5 step:4374 [D loss: 0.211411, acc.: 94.53%] [G loss: 3.600594]\n",
      "epoch:5 step:4375 [D loss: 0.283973, acc.: 86.72%] [G loss: 3.448242]\n",
      "epoch:5 step:4376 [D loss: 0.370092, acc.: 82.81%] [G loss: 2.241663]\n",
      "epoch:5 step:4377 [D loss: 0.290990, acc.: 89.06%] [G loss: 3.014590]\n",
      "epoch:5 step:4378 [D loss: 0.357944, acc.: 84.38%] [G loss: 1.658705]\n",
      "epoch:5 step:4379 [D loss: 0.394394, acc.: 82.81%] [G loss: 2.382238]\n",
      "epoch:5 step:4380 [D loss: 0.324324, acc.: 84.38%] [G loss: 3.519033]\n",
      "epoch:5 step:4381 [D loss: 0.291985, acc.: 89.06%] [G loss: 2.164825]\n",
      "epoch:5 step:4382 [D loss: 0.317342, acc.: 86.72%] [G loss: 2.897639]\n",
      "epoch:5 step:4383 [D loss: 0.299365, acc.: 90.62%] [G loss: 3.033609]\n",
      "epoch:5 step:4384 [D loss: 0.212012, acc.: 93.75%] [G loss: 3.060775]\n",
      "epoch:5 step:4385 [D loss: 0.315124, acc.: 87.50%] [G loss: 2.443688]\n",
      "epoch:5 step:4386 [D loss: 0.340551, acc.: 83.59%] [G loss: 3.064891]\n",
      "epoch:5 step:4387 [D loss: 0.235494, acc.: 92.97%] [G loss: 2.794331]\n",
      "epoch:5 step:4388 [D loss: 0.317375, acc.: 88.28%] [G loss: 2.253628]\n",
      "epoch:5 step:4389 [D loss: 0.309828, acc.: 88.28%] [G loss: 2.468967]\n",
      "epoch:5 step:4390 [D loss: 0.320387, acc.: 85.94%] [G loss: 2.711570]\n",
      "epoch:5 step:4391 [D loss: 0.367177, acc.: 80.47%] [G loss: 2.114540]\n",
      "epoch:5 step:4392 [D loss: 0.352099, acc.: 82.81%] [G loss: 1.716545]\n",
      "epoch:5 step:4393 [D loss: 0.473794, acc.: 78.12%] [G loss: 2.884583]\n",
      "epoch:5 step:4394 [D loss: 0.479002, acc.: 74.22%] [G loss: 4.620072]\n",
      "epoch:5 step:4395 [D loss: 0.995923, acc.: 63.28%] [G loss: 4.893881]\n",
      "epoch:5 step:4396 [D loss: 1.194216, acc.: 54.69%] [G loss: 2.931886]\n",
      "epoch:5 step:4397 [D loss: 0.693607, acc.: 68.75%] [G loss: 2.014487]\n",
      "epoch:5 step:4398 [D loss: 0.335272, acc.: 84.38%] [G loss: 2.478649]\n",
      "epoch:5 step:4399 [D loss: 0.545649, acc.: 75.00%] [G loss: 4.635771]\n",
      "epoch:5 step:4400 [D loss: 0.576314, acc.: 75.78%] [G loss: 2.038529]\n",
      "epoch:5 step:4401 [D loss: 0.342823, acc.: 81.25%] [G loss: 2.384987]\n",
      "epoch:5 step:4402 [D loss: 0.364349, acc.: 81.25%] [G loss: 2.442579]\n",
      "epoch:5 step:4403 [D loss: 0.290205, acc.: 87.50%] [G loss: 3.362576]\n",
      "epoch:5 step:4404 [D loss: 0.312444, acc.: 80.47%] [G loss: 2.135201]\n",
      "epoch:5 step:4405 [D loss: 0.417576, acc.: 78.91%] [G loss: 1.816224]\n",
      "epoch:5 step:4406 [D loss: 0.333823, acc.: 82.81%] [G loss: 2.206558]\n",
      "epoch:5 step:4407 [D loss: 0.372180, acc.: 84.38%] [G loss: 2.161829]\n",
      "epoch:5 step:4408 [D loss: 0.397238, acc.: 78.91%] [G loss: 2.032148]\n",
      "epoch:5 step:4409 [D loss: 0.366953, acc.: 78.91%] [G loss: 2.218122]\n",
      "epoch:5 step:4410 [D loss: 0.294641, acc.: 89.06%] [G loss: 1.887396]\n",
      "epoch:5 step:4411 [D loss: 0.385889, acc.: 79.69%] [G loss: 1.821418]\n",
      "epoch:5 step:4412 [D loss: 0.417044, acc.: 78.91%] [G loss: 2.031525]\n",
      "epoch:5 step:4413 [D loss: 0.289705, acc.: 89.84%] [G loss: 2.334792]\n",
      "epoch:5 step:4414 [D loss: 0.356041, acc.: 83.59%] [G loss: 3.042757]\n",
      "epoch:5 step:4415 [D loss: 0.334508, acc.: 88.28%] [G loss: 2.501650]\n",
      "epoch:5 step:4416 [D loss: 0.285234, acc.: 86.72%] [G loss: 2.565303]\n",
      "epoch:5 step:4417 [D loss: 0.294124, acc.: 89.84%] [G loss: 2.606748]\n",
      "epoch:5 step:4418 [D loss: 0.363312, acc.: 83.59%] [G loss: 2.596612]\n",
      "epoch:5 step:4419 [D loss: 0.359474, acc.: 83.59%] [G loss: 1.865021]\n",
      "epoch:5 step:4420 [D loss: 0.300815, acc.: 89.84%] [G loss: 2.386085]\n",
      "epoch:5 step:4421 [D loss: 0.283373, acc.: 91.41%] [G loss: 2.363487]\n",
      "epoch:5 step:4422 [D loss: 0.308780, acc.: 88.28%] [G loss: 2.680816]\n",
      "epoch:5 step:4423 [D loss: 0.277945, acc.: 90.62%] [G loss: 2.491535]\n",
      "epoch:5 step:4424 [D loss: 0.396516, acc.: 82.81%] [G loss: 2.728588]\n",
      "epoch:5 step:4425 [D loss: 0.491375, acc.: 72.66%] [G loss: 1.689110]\n",
      "epoch:5 step:4426 [D loss: 0.448336, acc.: 78.91%] [G loss: 1.946654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4427 [D loss: 0.387323, acc.: 84.38%] [G loss: 2.280455]\n",
      "epoch:5 step:4428 [D loss: 0.347770, acc.: 85.16%] [G loss: 2.429192]\n",
      "epoch:5 step:4429 [D loss: 0.437153, acc.: 81.25%] [G loss: 2.361981]\n",
      "epoch:5 step:4430 [D loss: 0.479118, acc.: 72.66%] [G loss: 2.194375]\n",
      "epoch:5 step:4431 [D loss: 0.443166, acc.: 78.91%] [G loss: 2.879461]\n",
      "epoch:5 step:4432 [D loss: 0.388826, acc.: 82.03%] [G loss: 4.171534]\n",
      "epoch:5 step:4433 [D loss: 0.528548, acc.: 74.22%] [G loss: 2.907856]\n",
      "epoch:5 step:4434 [D loss: 0.474101, acc.: 74.22%] [G loss: 2.796474]\n",
      "epoch:5 step:4435 [D loss: 0.307377, acc.: 85.16%] [G loss: 3.338841]\n",
      "epoch:5 step:4436 [D loss: 0.307818, acc.: 89.06%] [G loss: 2.682093]\n",
      "epoch:5 step:4437 [D loss: 0.370465, acc.: 82.81%] [G loss: 2.311693]\n",
      "epoch:5 step:4438 [D loss: 0.293294, acc.: 86.72%] [G loss: 2.080317]\n",
      "epoch:5 step:4439 [D loss: 0.409522, acc.: 82.81%] [G loss: 2.412696]\n",
      "epoch:5 step:4440 [D loss: 0.386441, acc.: 83.59%] [G loss: 2.495573]\n",
      "epoch:5 step:4441 [D loss: 0.279707, acc.: 86.72%] [G loss: 2.859162]\n",
      "epoch:5 step:4442 [D loss: 0.278423, acc.: 89.84%] [G loss: 2.753135]\n",
      "epoch:5 step:4443 [D loss: 0.344514, acc.: 82.03%] [G loss: 3.191919]\n",
      "epoch:5 step:4444 [D loss: 0.385231, acc.: 85.16%] [G loss: 3.164733]\n",
      "epoch:5 step:4445 [D loss: 0.345541, acc.: 85.16%] [G loss: 2.181086]\n",
      "epoch:5 step:4446 [D loss: 0.260090, acc.: 89.06%] [G loss: 2.585853]\n",
      "epoch:5 step:4447 [D loss: 0.345462, acc.: 86.72%] [G loss: 2.863062]\n",
      "epoch:5 step:4448 [D loss: 0.427958, acc.: 78.91%] [G loss: 1.876835]\n",
      "epoch:5 step:4449 [D loss: 0.464398, acc.: 78.12%] [G loss: 2.651624]\n",
      "epoch:5 step:4450 [D loss: 0.447113, acc.: 78.91%] [G loss: 2.189575]\n",
      "epoch:5 step:4451 [D loss: 0.442930, acc.: 73.44%] [G loss: 2.883360]\n",
      "epoch:5 step:4452 [D loss: 0.317265, acc.: 85.94%] [G loss: 3.562752]\n",
      "epoch:5 step:4453 [D loss: 0.215111, acc.: 92.19%] [G loss: 5.905062]\n",
      "epoch:5 step:4454 [D loss: 0.316036, acc.: 83.59%] [G loss: 3.300627]\n",
      "epoch:5 step:4455 [D loss: 0.270826, acc.: 86.72%] [G loss: 3.800148]\n",
      "epoch:5 step:4456 [D loss: 0.402373, acc.: 82.81%] [G loss: 2.703322]\n",
      "epoch:5 step:4457 [D loss: 0.392628, acc.: 81.25%] [G loss: 3.544091]\n",
      "epoch:5 step:4458 [D loss: 0.458803, acc.: 69.53%] [G loss: 3.746840]\n",
      "epoch:5 step:4459 [D loss: 0.470968, acc.: 72.66%] [G loss: 3.847920]\n",
      "epoch:5 step:4460 [D loss: 0.572535, acc.: 72.66%] [G loss: 3.776172]\n",
      "epoch:5 step:4461 [D loss: 0.505480, acc.: 71.09%] [G loss: 2.228328]\n",
      "epoch:5 step:4462 [D loss: 0.402135, acc.: 81.25%] [G loss: 2.207506]\n",
      "epoch:5 step:4463 [D loss: 0.430684, acc.: 77.34%] [G loss: 2.625827]\n",
      "epoch:5 step:4464 [D loss: 0.368569, acc.: 81.25%] [G loss: 3.516016]\n",
      "epoch:5 step:4465 [D loss: 0.425888, acc.: 74.22%] [G loss: 1.777352]\n",
      "epoch:5 step:4466 [D loss: 0.457504, acc.: 74.22%] [G loss: 3.440816]\n",
      "epoch:5 step:4467 [D loss: 0.501208, acc.: 72.66%] [G loss: 2.275486]\n",
      "epoch:5 step:4468 [D loss: 0.410549, acc.: 78.91%] [G loss: 3.214849]\n",
      "epoch:5 step:4469 [D loss: 0.442890, acc.: 77.34%] [G loss: 2.800563]\n",
      "epoch:5 step:4470 [D loss: 0.311381, acc.: 88.28%] [G loss: 2.749314]\n",
      "epoch:5 step:4471 [D loss: 0.507856, acc.: 67.97%] [G loss: 2.881658]\n",
      "epoch:5 step:4472 [D loss: 0.421759, acc.: 77.34%] [G loss: 2.518470]\n",
      "epoch:5 step:4473 [D loss: 0.370388, acc.: 85.94%] [G loss: 2.521366]\n",
      "epoch:5 step:4474 [D loss: 0.361672, acc.: 85.16%] [G loss: 2.404651]\n",
      "epoch:5 step:4475 [D loss: 0.355570, acc.: 82.81%] [G loss: 2.903176]\n",
      "epoch:5 step:4476 [D loss: 0.404949, acc.: 85.94%] [G loss: 3.290651]\n",
      "epoch:5 step:4477 [D loss: 0.315991, acc.: 86.72%] [G loss: 3.092915]\n",
      "epoch:5 step:4478 [D loss: 0.309657, acc.: 90.62%] [G loss: 3.141350]\n",
      "epoch:5 step:4479 [D loss: 0.406080, acc.: 79.69%] [G loss: 2.907780]\n",
      "epoch:5 step:4480 [D loss: 0.337146, acc.: 86.72%] [G loss: 2.312772]\n",
      "epoch:5 step:4481 [D loss: 0.487112, acc.: 73.44%] [G loss: 2.485877]\n",
      "epoch:5 step:4482 [D loss: 0.357315, acc.: 82.03%] [G loss: 3.660472]\n",
      "epoch:5 step:4483 [D loss: 0.321955, acc.: 84.38%] [G loss: 4.460156]\n",
      "epoch:5 step:4484 [D loss: 0.407510, acc.: 77.34%] [G loss: 2.465275]\n",
      "epoch:5 step:4485 [D loss: 0.318720, acc.: 84.38%] [G loss: 4.897723]\n",
      "epoch:5 step:4486 [D loss: 0.283241, acc.: 87.50%] [G loss: 4.313601]\n",
      "epoch:5 step:4487 [D loss: 0.435724, acc.: 77.34%] [G loss: 2.781293]\n",
      "epoch:5 step:4488 [D loss: 0.463492, acc.: 72.66%] [G loss: 3.145340]\n",
      "epoch:5 step:4489 [D loss: 0.355735, acc.: 84.38%] [G loss: 2.617716]\n",
      "epoch:5 step:4490 [D loss: 0.284608, acc.: 89.06%] [G loss: 3.576551]\n",
      "epoch:5 step:4491 [D loss: 0.346568, acc.: 82.81%] [G loss: 2.279078]\n",
      "epoch:5 step:4492 [D loss: 0.339636, acc.: 85.94%] [G loss: 3.162656]\n",
      "epoch:5 step:4493 [D loss: 0.392215, acc.: 77.34%] [G loss: 2.909837]\n",
      "epoch:5 step:4494 [D loss: 0.386516, acc.: 82.03%] [G loss: 2.836037]\n",
      "epoch:5 step:4495 [D loss: 0.443179, acc.: 76.56%] [G loss: 3.330467]\n",
      "epoch:5 step:4496 [D loss: 0.670377, acc.: 69.53%] [G loss: 3.587301]\n",
      "epoch:5 step:4497 [D loss: 1.111421, acc.: 51.56%] [G loss: 3.144805]\n",
      "epoch:5 step:4498 [D loss: 0.622971, acc.: 67.19%] [G loss: 3.131881]\n",
      "epoch:5 step:4499 [D loss: 0.376606, acc.: 82.81%] [G loss: 2.652115]\n",
      "epoch:5 step:4500 [D loss: 0.495405, acc.: 75.78%] [G loss: 2.682425]\n",
      "epoch:5 step:4501 [D loss: 0.413087, acc.: 82.81%] [G loss: 3.481912]\n",
      "epoch:5 step:4502 [D loss: 0.329484, acc.: 85.16%] [G loss: 2.663272]\n",
      "epoch:5 step:4503 [D loss: 0.393273, acc.: 83.59%] [G loss: 2.741702]\n",
      "epoch:5 step:4504 [D loss: 0.363399, acc.: 88.28%] [G loss: 1.647518]\n",
      "epoch:5 step:4505 [D loss: 0.407666, acc.: 77.34%] [G loss: 2.020380]\n",
      "epoch:5 step:4506 [D loss: 0.325902, acc.: 87.50%] [G loss: 3.192123]\n",
      "epoch:5 step:4507 [D loss: 0.365036, acc.: 84.38%] [G loss: 2.608505]\n",
      "epoch:5 step:4508 [D loss: 0.359995, acc.: 85.16%] [G loss: 4.009306]\n",
      "epoch:5 step:4509 [D loss: 0.374668, acc.: 78.12%] [G loss: 2.591110]\n",
      "epoch:5 step:4510 [D loss: 0.362009, acc.: 81.25%] [G loss: 4.558878]\n",
      "epoch:5 step:4511 [D loss: 0.458471, acc.: 75.00%] [G loss: 3.472421]\n",
      "epoch:5 step:4512 [D loss: 0.460970, acc.: 80.47%] [G loss: 3.786252]\n",
      "epoch:5 step:4513 [D loss: 0.380699, acc.: 81.25%] [G loss: 5.772462]\n",
      "epoch:5 step:4514 [D loss: 0.316550, acc.: 87.50%] [G loss: 4.173105]\n",
      "epoch:5 step:4515 [D loss: 0.290952, acc.: 84.38%] [G loss: 4.419792]\n",
      "epoch:5 step:4516 [D loss: 0.175063, acc.: 97.66%] [G loss: 5.093493]\n",
      "epoch:5 step:4517 [D loss: 0.314130, acc.: 85.16%] [G loss: 2.468348]\n",
      "epoch:5 step:4518 [D loss: 0.507986, acc.: 73.44%] [G loss: 2.563448]\n",
      "epoch:5 step:4519 [D loss: 0.500318, acc.: 74.22%] [G loss: 2.217268]\n",
      "epoch:5 step:4520 [D loss: 0.387117, acc.: 83.59%] [G loss: 2.880857]\n",
      "epoch:5 step:4521 [D loss: 0.361621, acc.: 78.91%] [G loss: 2.981093]\n",
      "epoch:5 step:4522 [D loss: 0.433082, acc.: 79.69%] [G loss: 3.161273]\n",
      "epoch:5 step:4523 [D loss: 0.508337, acc.: 72.66%] [G loss: 2.536389]\n",
      "epoch:5 step:4524 [D loss: 0.332315, acc.: 88.28%] [G loss: 2.988771]\n",
      "epoch:5 step:4525 [D loss: 0.332484, acc.: 82.03%] [G loss: 3.048760]\n",
      "epoch:5 step:4526 [D loss: 0.397210, acc.: 80.47%] [G loss: 2.768368]\n",
      "epoch:5 step:4527 [D loss: 0.347952, acc.: 82.03%] [G loss: 2.210593]\n",
      "epoch:5 step:4528 [D loss: 0.425457, acc.: 75.78%] [G loss: 2.371301]\n",
      "epoch:5 step:4529 [D loss: 0.255311, acc.: 90.62%] [G loss: 4.230016]\n",
      "epoch:5 step:4530 [D loss: 0.367678, acc.: 81.25%] [G loss: 2.525429]\n",
      "epoch:5 step:4531 [D loss: 0.422872, acc.: 77.34%] [G loss: 2.905121]\n",
      "epoch:5 step:4532 [D loss: 0.333648, acc.: 81.25%] [G loss: 4.051960]\n",
      "epoch:5 step:4533 [D loss: 0.395249, acc.: 75.78%] [G loss: 2.481270]\n",
      "epoch:5 step:4534 [D loss: 0.415219, acc.: 82.03%] [G loss: 3.075864]\n",
      "epoch:5 step:4535 [D loss: 0.324244, acc.: 85.16%] [G loss: 3.105371]\n",
      "epoch:5 step:4536 [D loss: 0.437911, acc.: 73.44%] [G loss: 3.393444]\n",
      "epoch:5 step:4537 [D loss: 0.506734, acc.: 73.44%] [G loss: 4.786107]\n",
      "epoch:5 step:4538 [D loss: 0.809329, acc.: 66.41%] [G loss: 4.523086]\n",
      "epoch:5 step:4539 [D loss: 1.240302, acc.: 61.72%] [G loss: 5.934705]\n",
      "epoch:5 step:4540 [D loss: 1.340300, acc.: 53.91%] [G loss: 1.528732]\n",
      "epoch:5 step:4541 [D loss: 0.587651, acc.: 62.50%] [G loss: 1.953053]\n",
      "epoch:5 step:4542 [D loss: 0.556743, acc.: 68.75%] [G loss: 2.283756]\n",
      "epoch:5 step:4543 [D loss: 0.492255, acc.: 76.56%] [G loss: 3.974220]\n",
      "epoch:5 step:4544 [D loss: 0.748182, acc.: 69.53%] [G loss: 2.194012]\n",
      "epoch:5 step:4545 [D loss: 0.443479, acc.: 74.22%] [G loss: 3.063874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4546 [D loss: 0.420352, acc.: 78.91%] [G loss: 2.233831]\n",
      "epoch:5 step:4547 [D loss: 0.383261, acc.: 79.69%] [G loss: 2.574619]\n",
      "epoch:5 step:4548 [D loss: 0.429939, acc.: 83.59%] [G loss: 2.307453]\n",
      "epoch:5 step:4549 [D loss: 0.345219, acc.: 87.50%] [G loss: 2.220970]\n",
      "epoch:5 step:4550 [D loss: 0.381930, acc.: 83.59%] [G loss: 1.950233]\n",
      "epoch:5 step:4551 [D loss: 0.408611, acc.: 78.91%] [G loss: 2.055354]\n",
      "epoch:5 step:4552 [D loss: 0.404791, acc.: 80.47%] [G loss: 2.667870]\n",
      "epoch:5 step:4553 [D loss: 0.398978, acc.: 82.03%] [G loss: 2.186562]\n",
      "epoch:5 step:4554 [D loss: 0.359484, acc.: 85.16%] [G loss: 2.824861]\n",
      "epoch:5 step:4555 [D loss: 0.353801, acc.: 88.28%] [G loss: 2.429669]\n",
      "epoch:5 step:4556 [D loss: 0.426006, acc.: 77.34%] [G loss: 2.779150]\n",
      "epoch:5 step:4557 [D loss: 0.331199, acc.: 87.50%] [G loss: 2.724653]\n",
      "epoch:5 step:4558 [D loss: 0.348902, acc.: 86.72%] [G loss: 2.512815]\n",
      "epoch:5 step:4559 [D loss: 0.365254, acc.: 83.59%] [G loss: 2.235848]\n",
      "epoch:5 step:4560 [D loss: 0.432037, acc.: 84.38%] [G loss: 2.573247]\n",
      "epoch:5 step:4561 [D loss: 0.496404, acc.: 78.12%] [G loss: 2.777348]\n",
      "epoch:5 step:4562 [D loss: 0.623489, acc.: 64.84%] [G loss: 3.527203]\n",
      "epoch:5 step:4563 [D loss: 0.662593, acc.: 71.09%] [G loss: 2.944239]\n",
      "epoch:5 step:4564 [D loss: 0.446576, acc.: 82.03%] [G loss: 2.761969]\n",
      "epoch:5 step:4565 [D loss: 0.363238, acc.: 83.59%] [G loss: 3.197529]\n",
      "epoch:5 step:4566 [D loss: 0.447765, acc.: 80.47%] [G loss: 3.163698]\n",
      "epoch:5 step:4567 [D loss: 0.422081, acc.: 77.34%] [G loss: 3.378506]\n",
      "epoch:5 step:4568 [D loss: 0.301284, acc.: 85.94%] [G loss: 3.658064]\n",
      "epoch:5 step:4569 [D loss: 0.414932, acc.: 78.91%] [G loss: 3.107758]\n",
      "epoch:5 step:4570 [D loss: 0.440500, acc.: 75.78%] [G loss: 2.740567]\n",
      "epoch:5 step:4571 [D loss: 0.352882, acc.: 82.81%] [G loss: 2.172262]\n",
      "epoch:5 step:4572 [D loss: 0.290126, acc.: 89.06%] [G loss: 2.970716]\n",
      "epoch:5 step:4573 [D loss: 0.496598, acc.: 67.19%] [G loss: 2.254489]\n",
      "epoch:5 step:4574 [D loss: 0.443549, acc.: 75.78%] [G loss: 2.618599]\n",
      "epoch:5 step:4575 [D loss: 0.502954, acc.: 71.88%] [G loss: 2.362632]\n",
      "epoch:5 step:4576 [D loss: 0.389388, acc.: 85.16%] [G loss: 2.298720]\n",
      "epoch:5 step:4577 [D loss: 0.377239, acc.: 88.28%] [G loss: 2.498103]\n",
      "epoch:5 step:4578 [D loss: 0.363336, acc.: 80.47%] [G loss: 1.836982]\n",
      "epoch:5 step:4579 [D loss: 0.322680, acc.: 87.50%] [G loss: 2.796116]\n",
      "epoch:5 step:4580 [D loss: 0.381398, acc.: 80.47%] [G loss: 2.921087]\n",
      "epoch:5 step:4581 [D loss: 0.450149, acc.: 78.91%] [G loss: 2.335063]\n",
      "epoch:5 step:4582 [D loss: 0.420017, acc.: 79.69%] [G loss: 2.782641]\n",
      "epoch:5 step:4583 [D loss: 0.416057, acc.: 78.12%] [G loss: 2.425670]\n",
      "epoch:5 step:4584 [D loss: 0.556593, acc.: 64.06%] [G loss: 1.934275]\n",
      "epoch:5 step:4585 [D loss: 0.444204, acc.: 77.34%] [G loss: 2.082590]\n",
      "epoch:5 step:4586 [D loss: 0.420411, acc.: 82.03%] [G loss: 2.185635]\n",
      "epoch:5 step:4587 [D loss: 0.456818, acc.: 77.34%] [G loss: 1.451905]\n",
      "epoch:5 step:4588 [D loss: 0.336484, acc.: 86.72%] [G loss: 2.209177]\n",
      "epoch:5 step:4589 [D loss: 0.338105, acc.: 86.72%] [G loss: 2.905003]\n",
      "epoch:5 step:4590 [D loss: 0.302655, acc.: 88.28%] [G loss: 3.513638]\n",
      "epoch:5 step:4591 [D loss: 0.408481, acc.: 79.69%] [G loss: 2.364830]\n",
      "epoch:5 step:4592 [D loss: 0.350181, acc.: 88.28%] [G loss: 2.360776]\n",
      "epoch:5 step:4593 [D loss: 0.366175, acc.: 83.59%] [G loss: 1.874174]\n",
      "epoch:5 step:4594 [D loss: 0.292135, acc.: 85.16%] [G loss: 2.905470]\n",
      "epoch:5 step:4595 [D loss: 0.392627, acc.: 77.34%] [G loss: 3.387548]\n",
      "epoch:5 step:4596 [D loss: 0.340809, acc.: 86.72%] [G loss: 3.031681]\n",
      "epoch:5 step:4597 [D loss: 0.431263, acc.: 75.78%] [G loss: 2.945449]\n",
      "epoch:5 step:4598 [D loss: 0.338249, acc.: 83.59%] [G loss: 3.284702]\n",
      "epoch:5 step:4599 [D loss: 0.345434, acc.: 80.47%] [G loss: 2.841106]\n",
      "epoch:5 step:4600 [D loss: 0.494801, acc.: 67.19%] [G loss: 2.088225]\n",
      "epoch:5 step:4601 [D loss: 0.540937, acc.: 73.44%] [G loss: 4.537042]\n",
      "epoch:5 step:4602 [D loss: 0.686697, acc.: 65.62%] [G loss: 3.671395]\n",
      "epoch:5 step:4603 [D loss: 0.740427, acc.: 67.19%] [G loss: 2.602404]\n",
      "epoch:5 step:4604 [D loss: 0.334096, acc.: 85.16%] [G loss: 3.149724]\n",
      "epoch:5 step:4605 [D loss: 0.415966, acc.: 78.91%] [G loss: 2.492572]\n",
      "epoch:5 step:4606 [D loss: 0.462080, acc.: 75.00%] [G loss: 2.253831]\n",
      "epoch:5 step:4607 [D loss: 0.400599, acc.: 79.69%] [G loss: 2.212499]\n",
      "epoch:5 step:4608 [D loss: 0.444713, acc.: 82.03%] [G loss: 2.124531]\n",
      "epoch:5 step:4609 [D loss: 0.452774, acc.: 78.91%] [G loss: 1.933590]\n",
      "epoch:5 step:4610 [D loss: 0.433053, acc.: 78.91%] [G loss: 2.056733]\n",
      "epoch:5 step:4611 [D loss: 0.501338, acc.: 72.66%] [G loss: 1.894488]\n",
      "epoch:5 step:4612 [D loss: 0.559708, acc.: 73.44%] [G loss: 3.620937]\n",
      "epoch:5 step:4613 [D loss: 0.559557, acc.: 78.12%] [G loss: 4.123844]\n",
      "epoch:5 step:4614 [D loss: 0.523800, acc.: 75.00%] [G loss: 2.384509]\n",
      "epoch:5 step:4615 [D loss: 0.500120, acc.: 76.56%] [G loss: 2.905568]\n",
      "epoch:5 step:4616 [D loss: 0.395853, acc.: 80.47%] [G loss: 2.484937]\n",
      "epoch:5 step:4617 [D loss: 0.464383, acc.: 78.12%] [G loss: 1.943233]\n",
      "epoch:5 step:4618 [D loss: 0.460583, acc.: 77.34%] [G loss: 2.292827]\n",
      "epoch:5 step:4619 [D loss: 0.454982, acc.: 81.25%] [G loss: 2.371439]\n",
      "epoch:5 step:4620 [D loss: 0.451661, acc.: 75.78%] [G loss: 2.165638]\n",
      "epoch:5 step:4621 [D loss: 0.443849, acc.: 76.56%] [G loss: 1.989839]\n",
      "epoch:5 step:4622 [D loss: 0.297916, acc.: 89.84%] [G loss: 2.446841]\n",
      "epoch:5 step:4623 [D loss: 0.443752, acc.: 82.03%] [G loss: 3.292461]\n",
      "epoch:5 step:4624 [D loss: 0.461872, acc.: 78.91%] [G loss: 2.354567]\n",
      "epoch:5 step:4625 [D loss: 0.556968, acc.: 64.06%] [G loss: 3.855660]\n",
      "epoch:5 step:4626 [D loss: 0.649521, acc.: 71.88%] [G loss: 3.319282]\n",
      "epoch:5 step:4627 [D loss: 0.649037, acc.: 67.19%] [G loss: 3.144860]\n",
      "epoch:5 step:4628 [D loss: 0.643071, acc.: 65.62%] [G loss: 1.967890]\n",
      "epoch:5 step:4629 [D loss: 0.421822, acc.: 75.78%] [G loss: 2.651937]\n",
      "epoch:5 step:4630 [D loss: 0.406386, acc.: 81.25%] [G loss: 2.110607]\n",
      "epoch:5 step:4631 [D loss: 0.368986, acc.: 82.81%] [G loss: 2.638470]\n",
      "epoch:5 step:4632 [D loss: 0.452432, acc.: 79.69%] [G loss: 1.699045]\n",
      "epoch:5 step:4633 [D loss: 0.448308, acc.: 78.12%] [G loss: 2.033253]\n",
      "epoch:5 step:4634 [D loss: 0.428524, acc.: 80.47%] [G loss: 2.063368]\n",
      "epoch:5 step:4635 [D loss: 0.398585, acc.: 87.50%] [G loss: 2.360909]\n",
      "epoch:5 step:4636 [D loss: 0.410231, acc.: 79.69%] [G loss: 2.157264]\n",
      "epoch:5 step:4637 [D loss: 0.405740, acc.: 75.00%] [G loss: 2.214278]\n",
      "epoch:5 step:4638 [D loss: 0.388647, acc.: 80.47%] [G loss: 2.313212]\n",
      "epoch:5 step:4639 [D loss: 0.345699, acc.: 85.94%] [G loss: 2.506419]\n",
      "epoch:5 step:4640 [D loss: 0.418424, acc.: 80.47%] [G loss: 3.109406]\n",
      "epoch:5 step:4641 [D loss: 0.320598, acc.: 84.38%] [G loss: 2.785466]\n",
      "epoch:5 step:4642 [D loss: 0.396717, acc.: 82.03%] [G loss: 2.532303]\n",
      "epoch:5 step:4643 [D loss: 0.360909, acc.: 80.47%] [G loss: 2.183098]\n",
      "epoch:5 step:4644 [D loss: 0.386951, acc.: 85.16%] [G loss: 2.217761]\n",
      "epoch:5 step:4645 [D loss: 0.311310, acc.: 86.72%] [G loss: 2.334160]\n",
      "epoch:5 step:4646 [D loss: 0.341343, acc.: 85.16%] [G loss: 2.525921]\n",
      "epoch:5 step:4647 [D loss: 0.314903, acc.: 85.94%] [G loss: 2.834469]\n",
      "epoch:5 step:4648 [D loss: 0.363798, acc.: 82.81%] [G loss: 2.488759]\n",
      "epoch:5 step:4649 [D loss: 0.285237, acc.: 92.97%] [G loss: 2.853549]\n",
      "epoch:5 step:4650 [D loss: 0.429863, acc.: 78.91%] [G loss: 2.348317]\n",
      "epoch:5 step:4651 [D loss: 0.341491, acc.: 85.16%] [G loss: 2.946670]\n",
      "epoch:5 step:4652 [D loss: 0.368883, acc.: 81.25%] [G loss: 1.945551]\n",
      "epoch:5 step:4653 [D loss: 0.394615, acc.: 80.47%] [G loss: 2.796371]\n",
      "epoch:5 step:4654 [D loss: 0.421607, acc.: 78.91%] [G loss: 2.363487]\n",
      "epoch:5 step:4655 [D loss: 0.379621, acc.: 85.16%] [G loss: 2.109956]\n",
      "epoch:5 step:4656 [D loss: 0.427484, acc.: 79.69%] [G loss: 2.109020]\n",
      "epoch:5 step:4657 [D loss: 0.402537, acc.: 79.69%] [G loss: 2.362156]\n",
      "epoch:5 step:4658 [D loss: 0.418088, acc.: 82.03%] [G loss: 2.487010]\n",
      "epoch:5 step:4659 [D loss: 0.449544, acc.: 77.34%] [G loss: 2.460128]\n",
      "epoch:5 step:4660 [D loss: 0.465250, acc.: 75.78%] [G loss: 3.633090]\n",
      "epoch:5 step:4661 [D loss: 0.480469, acc.: 75.78%] [G loss: 3.191141]\n",
      "epoch:5 step:4662 [D loss: 0.556013, acc.: 68.75%] [G loss: 2.142364]\n",
      "epoch:5 step:4663 [D loss: 0.415813, acc.: 80.47%] [G loss: 2.826257]\n",
      "epoch:5 step:4664 [D loss: 0.436091, acc.: 78.12%] [G loss: 2.632082]\n",
      "epoch:5 step:4665 [D loss: 0.337111, acc.: 83.59%] [G loss: 3.240878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4666 [D loss: 0.493752, acc.: 71.88%] [G loss: 3.795053]\n",
      "epoch:5 step:4667 [D loss: 0.489249, acc.: 71.09%] [G loss: 2.569654]\n",
      "epoch:5 step:4668 [D loss: 0.429868, acc.: 78.91%] [G loss: 2.190191]\n",
      "epoch:5 step:4669 [D loss: 0.449992, acc.: 76.56%] [G loss: 2.320270]\n",
      "epoch:5 step:4670 [D loss: 0.422431, acc.: 78.12%] [G loss: 2.865329]\n",
      "epoch:5 step:4671 [D loss: 0.356498, acc.: 82.81%] [G loss: 2.511346]\n",
      "epoch:5 step:4672 [D loss: 0.328186, acc.: 85.16%] [G loss: 2.803437]\n",
      "epoch:5 step:4673 [D loss: 0.407713, acc.: 78.12%] [G loss: 1.828048]\n",
      "epoch:5 step:4674 [D loss: 0.350685, acc.: 84.38%] [G loss: 2.841015]\n",
      "epoch:5 step:4675 [D loss: 0.356984, acc.: 82.03%] [G loss: 2.314177]\n",
      "epoch:5 step:4676 [D loss: 0.323176, acc.: 90.62%] [G loss: 2.809288]\n",
      "epoch:5 step:4677 [D loss: 0.331621, acc.: 86.72%] [G loss: 2.442923]\n",
      "epoch:5 step:4678 [D loss: 0.460456, acc.: 74.22%] [G loss: 2.146903]\n",
      "epoch:5 step:4679 [D loss: 0.373111, acc.: 82.81%] [G loss: 2.515629]\n",
      "epoch:5 step:4680 [D loss: 0.316895, acc.: 82.03%] [G loss: 3.511919]\n",
      "epoch:5 step:4681 [D loss: 0.340831, acc.: 85.16%] [G loss: 2.739933]\n",
      "epoch:5 step:4682 [D loss: 0.471465, acc.: 73.44%] [G loss: 2.817210]\n",
      "epoch:5 step:4683 [D loss: 0.334442, acc.: 84.38%] [G loss: 3.233785]\n",
      "epoch:5 step:4684 [D loss: 0.391865, acc.: 80.47%] [G loss: 3.818186]\n",
      "epoch:5 step:4685 [D loss: 0.386348, acc.: 80.47%] [G loss: 3.988627]\n",
      "epoch:5 step:4686 [D loss: 0.517833, acc.: 66.41%] [G loss: 2.035369]\n",
      "epoch:6 step:4687 [D loss: 0.429391, acc.: 83.59%] [G loss: 2.855890]\n",
      "epoch:6 step:4688 [D loss: 0.311285, acc.: 86.72%] [G loss: 2.636322]\n",
      "epoch:6 step:4689 [D loss: 0.387730, acc.: 81.25%] [G loss: 3.540401]\n",
      "epoch:6 step:4690 [D loss: 0.364441, acc.: 82.81%] [G loss: 3.155962]\n",
      "epoch:6 step:4691 [D loss: 0.375996, acc.: 78.91%] [G loss: 2.529341]\n",
      "epoch:6 step:4692 [D loss: 0.390379, acc.: 82.03%] [G loss: 1.819440]\n",
      "epoch:6 step:4693 [D loss: 0.328098, acc.: 87.50%] [G loss: 2.232325]\n",
      "epoch:6 step:4694 [D loss: 0.440028, acc.: 77.34%] [G loss: 2.080736]\n",
      "epoch:6 step:4695 [D loss: 0.476690, acc.: 77.34%] [G loss: 2.717169]\n",
      "epoch:6 step:4696 [D loss: 0.348413, acc.: 82.03%] [G loss: 3.283437]\n",
      "epoch:6 step:4697 [D loss: 0.397480, acc.: 80.47%] [G loss: 2.536517]\n",
      "epoch:6 step:4698 [D loss: 0.414643, acc.: 78.91%] [G loss: 1.802011]\n",
      "epoch:6 step:4699 [D loss: 0.510109, acc.: 68.75%] [G loss: 3.437447]\n",
      "epoch:6 step:4700 [D loss: 0.599538, acc.: 74.22%] [G loss: 6.277707]\n",
      "epoch:6 step:4701 [D loss: 1.098708, acc.: 57.81%] [G loss: 3.042772]\n",
      "epoch:6 step:4702 [D loss: 0.987060, acc.: 63.28%] [G loss: 2.679621]\n",
      "epoch:6 step:4703 [D loss: 0.429331, acc.: 83.59%] [G loss: 3.896805]\n",
      "epoch:6 step:4704 [D loss: 0.506035, acc.: 76.56%] [G loss: 3.888359]\n",
      "epoch:6 step:4705 [D loss: 0.405218, acc.: 78.91%] [G loss: 2.339498]\n",
      "epoch:6 step:4706 [D loss: 0.402004, acc.: 84.38%] [G loss: 2.664357]\n",
      "epoch:6 step:4707 [D loss: 0.318264, acc.: 86.72%] [G loss: 2.351951]\n",
      "epoch:6 step:4708 [D loss: 0.453740, acc.: 72.66%] [G loss: 2.231039]\n",
      "epoch:6 step:4709 [D loss: 0.386103, acc.: 79.69%] [G loss: 1.811098]\n",
      "epoch:6 step:4710 [D loss: 0.429414, acc.: 78.12%] [G loss: 2.085280]\n",
      "epoch:6 step:4711 [D loss: 0.349454, acc.: 85.94%] [G loss: 2.547191]\n",
      "epoch:6 step:4712 [D loss: 0.319511, acc.: 89.06%] [G loss: 2.219260]\n",
      "epoch:6 step:4713 [D loss: 0.410010, acc.: 81.25%] [G loss: 2.188925]\n",
      "epoch:6 step:4714 [D loss: 0.319520, acc.: 85.16%] [G loss: 2.449794]\n",
      "epoch:6 step:4715 [D loss: 0.399123, acc.: 79.69%] [G loss: 2.266713]\n",
      "epoch:6 step:4716 [D loss: 0.320035, acc.: 91.41%] [G loss: 2.405543]\n",
      "epoch:6 step:4717 [D loss: 0.385695, acc.: 83.59%] [G loss: 2.278805]\n",
      "epoch:6 step:4718 [D loss: 0.244254, acc.: 92.19%] [G loss: 3.097134]\n",
      "epoch:6 step:4719 [D loss: 0.358897, acc.: 82.81%] [G loss: 2.122973]\n",
      "epoch:6 step:4720 [D loss: 0.354359, acc.: 84.38%] [G loss: 1.894701]\n",
      "epoch:6 step:4721 [D loss: 0.404959, acc.: 78.91%] [G loss: 1.798794]\n",
      "epoch:6 step:4722 [D loss: 0.394195, acc.: 81.25%] [G loss: 1.781532]\n",
      "epoch:6 step:4723 [D loss: 0.394387, acc.: 80.47%] [G loss: 1.996440]\n",
      "epoch:6 step:4724 [D loss: 0.399315, acc.: 80.47%] [G loss: 4.077671]\n",
      "epoch:6 step:4725 [D loss: 0.781311, acc.: 67.19%] [G loss: 4.859591]\n",
      "epoch:6 step:4726 [D loss: 0.968407, acc.: 62.50%] [G loss: 2.044361]\n",
      "epoch:6 step:4727 [D loss: 0.517978, acc.: 76.56%] [G loss: 3.493345]\n",
      "epoch:6 step:4728 [D loss: 0.456577, acc.: 77.34%] [G loss: 3.163381]\n",
      "epoch:6 step:4729 [D loss: 0.347771, acc.: 82.03%] [G loss: 1.952837]\n",
      "epoch:6 step:4730 [D loss: 0.425413, acc.: 75.00%] [G loss: 2.228557]\n",
      "epoch:6 step:4731 [D loss: 0.361617, acc.: 87.50%] [G loss: 2.605846]\n",
      "epoch:6 step:4732 [D loss: 0.414188, acc.: 78.12%] [G loss: 2.791446]\n",
      "epoch:6 step:4733 [D loss: 0.414674, acc.: 73.44%] [G loss: 2.573249]\n",
      "epoch:6 step:4734 [D loss: 0.398657, acc.: 79.69%] [G loss: 2.210206]\n",
      "epoch:6 step:4735 [D loss: 0.382205, acc.: 81.25%] [G loss: 2.271475]\n",
      "epoch:6 step:4736 [D loss: 0.409606, acc.: 80.47%] [G loss: 2.172140]\n",
      "epoch:6 step:4737 [D loss: 0.361055, acc.: 86.72%] [G loss: 3.447405]\n",
      "epoch:6 step:4738 [D loss: 0.478788, acc.: 75.78%] [G loss: 2.820190]\n",
      "epoch:6 step:4739 [D loss: 0.358696, acc.: 85.16%] [G loss: 2.122908]\n",
      "epoch:6 step:4740 [D loss: 0.406632, acc.: 78.91%] [G loss: 2.054291]\n",
      "epoch:6 step:4741 [D loss: 0.345047, acc.: 85.94%] [G loss: 2.741910]\n",
      "epoch:6 step:4742 [D loss: 0.349794, acc.: 84.38%] [G loss: 2.563295]\n",
      "epoch:6 step:4743 [D loss: 0.368924, acc.: 80.47%] [G loss: 3.129152]\n",
      "epoch:6 step:4744 [D loss: 0.340587, acc.: 84.38%] [G loss: 2.483854]\n",
      "epoch:6 step:4745 [D loss: 0.401760, acc.: 81.25%] [G loss: 2.066351]\n",
      "epoch:6 step:4746 [D loss: 0.429477, acc.: 77.34%] [G loss: 2.318267]\n",
      "epoch:6 step:4747 [D loss: 0.409546, acc.: 81.25%] [G loss: 1.911603]\n",
      "epoch:6 step:4748 [D loss: 0.317863, acc.: 85.94%] [G loss: 2.088965]\n",
      "epoch:6 step:4749 [D loss: 0.391370, acc.: 76.56%] [G loss: 2.387227]\n",
      "epoch:6 step:4750 [D loss: 0.433833, acc.: 80.47%] [G loss: 2.332244]\n",
      "epoch:6 step:4751 [D loss: 0.492649, acc.: 75.00%] [G loss: 2.104725]\n",
      "epoch:6 step:4752 [D loss: 0.642615, acc.: 69.53%] [G loss: 2.986621]\n",
      "epoch:6 step:4753 [D loss: 0.622606, acc.: 66.41%] [G loss: 3.042038]\n",
      "epoch:6 step:4754 [D loss: 0.629541, acc.: 64.84%] [G loss: 1.830296]\n",
      "epoch:6 step:4755 [D loss: 0.635682, acc.: 66.41%] [G loss: 2.209972]\n",
      "epoch:6 step:4756 [D loss: 0.421134, acc.: 76.56%] [G loss: 3.047476]\n",
      "epoch:6 step:4757 [D loss: 0.376994, acc.: 82.03%] [G loss: 2.982272]\n",
      "epoch:6 step:4758 [D loss: 0.365437, acc.: 85.94%] [G loss: 1.988422]\n",
      "epoch:6 step:4759 [D loss: 0.536629, acc.: 68.75%] [G loss: 1.929236]\n",
      "epoch:6 step:4760 [D loss: 0.387996, acc.: 80.47%] [G loss: 2.414509]\n",
      "epoch:6 step:4761 [D loss: 0.277254, acc.: 89.84%] [G loss: 3.131863]\n",
      "epoch:6 step:4762 [D loss: 0.329218, acc.: 85.94%] [G loss: 2.744684]\n",
      "epoch:6 step:4763 [D loss: 0.354098, acc.: 84.38%] [G loss: 2.382196]\n",
      "epoch:6 step:4764 [D loss: 0.356578, acc.: 84.38%] [G loss: 1.628047]\n",
      "epoch:6 step:4765 [D loss: 0.387051, acc.: 82.81%] [G loss: 1.927022]\n",
      "epoch:6 step:4766 [D loss: 0.308399, acc.: 89.06%] [G loss: 2.089579]\n",
      "epoch:6 step:4767 [D loss: 0.369958, acc.: 82.81%] [G loss: 2.575181]\n",
      "epoch:6 step:4768 [D loss: 0.403605, acc.: 83.59%] [G loss: 3.082452]\n",
      "epoch:6 step:4769 [D loss: 0.269807, acc.: 91.41%] [G loss: 3.175447]\n",
      "epoch:6 step:4770 [D loss: 0.410906, acc.: 81.25%] [G loss: 2.376064]\n",
      "epoch:6 step:4771 [D loss: 0.440527, acc.: 76.56%] [G loss: 2.230148]\n",
      "epoch:6 step:4772 [D loss: 0.430924, acc.: 76.56%] [G loss: 2.372177]\n",
      "epoch:6 step:4773 [D loss: 0.441829, acc.: 79.69%] [G loss: 2.380486]\n",
      "epoch:6 step:4774 [D loss: 0.345017, acc.: 82.81%] [G loss: 2.747689]\n",
      "epoch:6 step:4775 [D loss: 0.339819, acc.: 87.50%] [G loss: 2.538664]\n",
      "epoch:6 step:4776 [D loss: 0.524679, acc.: 67.97%] [G loss: 2.067550]\n",
      "epoch:6 step:4777 [D loss: 0.355404, acc.: 85.94%] [G loss: 2.125994]\n",
      "epoch:6 step:4778 [D loss: 0.339095, acc.: 86.72%] [G loss: 2.244021]\n",
      "epoch:6 step:4779 [D loss: 0.383184, acc.: 78.91%] [G loss: 2.224311]\n",
      "epoch:6 step:4780 [D loss: 0.342479, acc.: 85.94%] [G loss: 2.100250]\n",
      "epoch:6 step:4781 [D loss: 0.344832, acc.: 82.03%] [G loss: 3.274401]\n",
      "epoch:6 step:4782 [D loss: 0.433616, acc.: 82.03%] [G loss: 1.885611]\n",
      "epoch:6 step:4783 [D loss: 0.311916, acc.: 87.50%] [G loss: 1.992294]\n",
      "epoch:6 step:4784 [D loss: 0.395735, acc.: 81.25%] [G loss: 2.427242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:4785 [D loss: 0.367064, acc.: 82.81%] [G loss: 2.032093]\n",
      "epoch:6 step:4786 [D loss: 0.259699, acc.: 93.75%] [G loss: 1.929900]\n",
      "epoch:6 step:4787 [D loss: 0.388535, acc.: 82.03%] [G loss: 2.111373]\n",
      "epoch:6 step:4788 [D loss: 0.373752, acc.: 87.50%] [G loss: 2.479715]\n",
      "epoch:6 step:4789 [D loss: 0.324035, acc.: 89.84%] [G loss: 1.986164]\n",
      "epoch:6 step:4790 [D loss: 0.366645, acc.: 82.81%] [G loss: 2.402444]\n",
      "epoch:6 step:4791 [D loss: 0.380716, acc.: 81.25%] [G loss: 2.177717]\n",
      "epoch:6 step:4792 [D loss: 0.330696, acc.: 87.50%] [G loss: 3.154830]\n",
      "epoch:6 step:4793 [D loss: 0.361061, acc.: 83.59%] [G loss: 2.610012]\n",
      "epoch:6 step:4794 [D loss: 0.362262, acc.: 86.72%] [G loss: 1.476993]\n",
      "epoch:6 step:4795 [D loss: 0.384839, acc.: 82.03%] [G loss: 1.725788]\n",
      "epoch:6 step:4796 [D loss: 0.416232, acc.: 81.25%] [G loss: 1.978561]\n",
      "epoch:6 step:4797 [D loss: 0.434537, acc.: 70.31%] [G loss: 2.466165]\n",
      "epoch:6 step:4798 [D loss: 0.407752, acc.: 85.16%] [G loss: 3.843815]\n",
      "epoch:6 step:4799 [D loss: 0.482860, acc.: 80.47%] [G loss: 2.294874]\n",
      "epoch:6 step:4800 [D loss: 0.366788, acc.: 83.59%] [G loss: 2.527630]\n",
      "epoch:6 step:4801 [D loss: 0.371776, acc.: 82.03%] [G loss: 2.501053]\n",
      "epoch:6 step:4802 [D loss: 0.372014, acc.: 82.03%] [G loss: 3.204471]\n",
      "epoch:6 step:4803 [D loss: 0.296756, acc.: 85.94%] [G loss: 3.832136]\n",
      "epoch:6 step:4804 [D loss: 0.342194, acc.: 83.59%] [G loss: 2.789916]\n",
      "epoch:6 step:4805 [D loss: 0.489817, acc.: 75.78%] [G loss: 2.184710]\n",
      "epoch:6 step:4806 [D loss: 0.348706, acc.: 81.25%] [G loss: 2.827472]\n",
      "epoch:6 step:4807 [D loss: 0.312315, acc.: 85.16%] [G loss: 2.085846]\n",
      "epoch:6 step:4808 [D loss: 0.385573, acc.: 80.47%] [G loss: 1.950169]\n",
      "epoch:6 step:4809 [D loss: 0.337968, acc.: 85.16%] [G loss: 2.711368]\n",
      "epoch:6 step:4810 [D loss: 0.403048, acc.: 81.25%] [G loss: 2.608581]\n",
      "epoch:6 step:4811 [D loss: 0.520752, acc.: 72.66%] [G loss: 5.151331]\n",
      "epoch:6 step:4812 [D loss: 1.144622, acc.: 54.69%] [G loss: 5.370644]\n",
      "epoch:6 step:4813 [D loss: 1.502669, acc.: 53.12%] [G loss: 3.470948]\n",
      "epoch:6 step:4814 [D loss: 0.352553, acc.: 81.25%] [G loss: 3.711289]\n",
      "epoch:6 step:4815 [D loss: 0.632350, acc.: 67.19%] [G loss: 4.930211]\n",
      "epoch:6 step:4816 [D loss: 1.003900, acc.: 57.03%] [G loss: 2.941678]\n",
      "epoch:6 step:4817 [D loss: 0.478664, acc.: 75.78%] [G loss: 3.016771]\n",
      "epoch:6 step:4818 [D loss: 0.486227, acc.: 77.34%] [G loss: 2.106512]\n",
      "epoch:6 step:4819 [D loss: 0.322597, acc.: 85.94%] [G loss: 2.708982]\n",
      "epoch:6 step:4820 [D loss: 0.377843, acc.: 77.34%] [G loss: 2.624703]\n",
      "epoch:6 step:4821 [D loss: 0.470156, acc.: 78.91%] [G loss: 1.838787]\n",
      "epoch:6 step:4822 [D loss: 0.379258, acc.: 80.47%] [G loss: 2.037340]\n",
      "epoch:6 step:4823 [D loss: 0.336448, acc.: 84.38%] [G loss: 2.180214]\n",
      "epoch:6 step:4824 [D loss: 0.373579, acc.: 79.69%] [G loss: 2.420493]\n",
      "epoch:6 step:4825 [D loss: 0.390509, acc.: 78.91%] [G loss: 2.190468]\n",
      "epoch:6 step:4826 [D loss: 0.283629, acc.: 85.94%] [G loss: 3.382502]\n",
      "epoch:6 step:4827 [D loss: 0.281674, acc.: 87.50%] [G loss: 3.117511]\n",
      "epoch:6 step:4828 [D loss: 0.446370, acc.: 74.22%] [G loss: 1.944931]\n",
      "epoch:6 step:4829 [D loss: 0.336800, acc.: 86.72%] [G loss: 3.404681]\n",
      "epoch:6 step:4830 [D loss: 0.356657, acc.: 82.81%] [G loss: 2.860082]\n",
      "epoch:6 step:4831 [D loss: 0.405333, acc.: 78.12%] [G loss: 1.678047]\n",
      "epoch:6 step:4832 [D loss: 0.363000, acc.: 82.81%] [G loss: 2.119488]\n",
      "epoch:6 step:4833 [D loss: 0.289126, acc.: 92.19%] [G loss: 1.971309]\n",
      "epoch:6 step:4834 [D loss: 0.371624, acc.: 80.47%] [G loss: 2.567892]\n",
      "epoch:6 step:4835 [D loss: 0.463305, acc.: 75.78%] [G loss: 2.069473]\n",
      "epoch:6 step:4836 [D loss: 0.323804, acc.: 89.84%] [G loss: 1.769830]\n",
      "epoch:6 step:4837 [D loss: 0.383621, acc.: 78.91%] [G loss: 1.725411]\n",
      "epoch:6 step:4838 [D loss: 0.336165, acc.: 89.84%] [G loss: 2.352876]\n",
      "epoch:6 step:4839 [D loss: 0.343738, acc.: 84.38%] [G loss: 3.198850]\n",
      "epoch:6 step:4840 [D loss: 0.304381, acc.: 88.28%] [G loss: 2.482564]\n",
      "epoch:6 step:4841 [D loss: 0.397911, acc.: 80.47%] [G loss: 2.270137]\n",
      "epoch:6 step:4842 [D loss: 0.317326, acc.: 88.28%] [G loss: 2.805799]\n",
      "epoch:6 step:4843 [D loss: 0.401437, acc.: 78.91%] [G loss: 2.825283]\n",
      "epoch:6 step:4844 [D loss: 0.325803, acc.: 86.72%] [G loss: 3.336262]\n",
      "epoch:6 step:4845 [D loss: 0.445120, acc.: 75.78%] [G loss: 1.978068]\n",
      "epoch:6 step:4846 [D loss: 0.363112, acc.: 82.81%] [G loss: 2.400486]\n",
      "epoch:6 step:4847 [D loss: 0.355065, acc.: 82.81%] [G loss: 3.254307]\n",
      "epoch:6 step:4848 [D loss: 0.336970, acc.: 84.38%] [G loss: 2.252033]\n",
      "epoch:6 step:4849 [D loss: 0.358162, acc.: 82.03%] [G loss: 2.259530]\n",
      "epoch:6 step:4850 [D loss: 0.392963, acc.: 76.56%] [G loss: 2.284425]\n",
      "epoch:6 step:4851 [D loss: 0.443661, acc.: 75.00%] [G loss: 1.824826]\n",
      "epoch:6 step:4852 [D loss: 0.461398, acc.: 77.34%] [G loss: 3.074542]\n",
      "epoch:6 step:4853 [D loss: 0.392531, acc.: 85.16%] [G loss: 3.465811]\n",
      "epoch:6 step:4854 [D loss: 0.402369, acc.: 81.25%] [G loss: 3.276315]\n",
      "epoch:6 step:4855 [D loss: 0.425005, acc.: 79.69%] [G loss: 2.521191]\n",
      "epoch:6 step:4856 [D loss: 0.371893, acc.: 82.03%] [G loss: 2.433410]\n",
      "epoch:6 step:4857 [D loss: 0.464352, acc.: 80.47%] [G loss: 2.058988]\n",
      "epoch:6 step:4858 [D loss: 0.430033, acc.: 80.47%] [G loss: 2.428992]\n",
      "epoch:6 step:4859 [D loss: 0.379326, acc.: 82.03%] [G loss: 2.543576]\n",
      "epoch:6 step:4860 [D loss: 0.280840, acc.: 85.16%] [G loss: 3.425506]\n",
      "epoch:6 step:4861 [D loss: 0.379139, acc.: 80.47%] [G loss: 2.393668]\n",
      "epoch:6 step:4862 [D loss: 0.323262, acc.: 85.94%] [G loss: 2.129212]\n",
      "epoch:6 step:4863 [D loss: 0.389243, acc.: 85.16%] [G loss: 3.034669]\n",
      "epoch:6 step:4864 [D loss: 0.529917, acc.: 73.44%] [G loss: 3.667737]\n",
      "epoch:6 step:4865 [D loss: 0.528093, acc.: 78.91%] [G loss: 2.935397]\n",
      "epoch:6 step:4866 [D loss: 0.338433, acc.: 87.50%] [G loss: 4.235426]\n",
      "epoch:6 step:4867 [D loss: 0.277555, acc.: 89.06%] [G loss: 3.716092]\n",
      "epoch:6 step:4868 [D loss: 0.489349, acc.: 71.88%] [G loss: 2.234767]\n",
      "epoch:6 step:4869 [D loss: 0.419821, acc.: 76.56%] [G loss: 2.626213]\n",
      "epoch:6 step:4870 [D loss: 0.549586, acc.: 69.53%] [G loss: 1.698750]\n",
      "epoch:6 step:4871 [D loss: 0.529081, acc.: 75.78%] [G loss: 2.887581]\n",
      "epoch:6 step:4872 [D loss: 0.474947, acc.: 80.47%] [G loss: 3.536898]\n",
      "epoch:6 step:4873 [D loss: 0.286052, acc.: 84.38%] [G loss: 3.827658]\n",
      "epoch:6 step:4874 [D loss: 0.481953, acc.: 75.78%] [G loss: 2.476194]\n",
      "epoch:6 step:4875 [D loss: 0.468964, acc.: 77.34%] [G loss: 2.042135]\n",
      "epoch:6 step:4876 [D loss: 0.441317, acc.: 71.88%] [G loss: 2.312767]\n",
      "epoch:6 step:4877 [D loss: 0.399801, acc.: 82.81%] [G loss: 2.181830]\n",
      "epoch:6 step:4878 [D loss: 0.337674, acc.: 85.94%] [G loss: 3.340564]\n",
      "epoch:6 step:4879 [D loss: 0.267481, acc.: 85.94%] [G loss: 5.535417]\n",
      "epoch:6 step:4880 [D loss: 0.258288, acc.: 90.62%] [G loss: 4.207301]\n",
      "epoch:6 step:4881 [D loss: 0.392483, acc.: 84.38%] [G loss: 2.937887]\n",
      "epoch:6 step:4882 [D loss: 0.359300, acc.: 83.59%] [G loss: 2.289100]\n",
      "epoch:6 step:4883 [D loss: 0.411755, acc.: 78.91%] [G loss: 4.548442]\n",
      "epoch:6 step:4884 [D loss: 0.909115, acc.: 60.94%] [G loss: 4.745880]\n",
      "epoch:6 step:4885 [D loss: 0.774193, acc.: 58.59%] [G loss: 1.976936]\n",
      "epoch:6 step:4886 [D loss: 0.398640, acc.: 80.47%] [G loss: 3.277612]\n",
      "epoch:6 step:4887 [D loss: 0.424214, acc.: 78.91%] [G loss: 2.594219]\n",
      "epoch:6 step:4888 [D loss: 0.346210, acc.: 81.25%] [G loss: 2.688612]\n",
      "epoch:6 step:4889 [D loss: 0.411978, acc.: 78.91%] [G loss: 2.480518]\n",
      "epoch:6 step:4890 [D loss: 0.301300, acc.: 88.28%] [G loss: 4.306428]\n",
      "epoch:6 step:4891 [D loss: 0.319640, acc.: 89.06%] [G loss: 4.413374]\n",
      "epoch:6 step:4892 [D loss: 0.448406, acc.: 78.91%] [G loss: 1.860838]\n",
      "epoch:6 step:4893 [D loss: 0.450419, acc.: 77.34%] [G loss: 1.939492]\n",
      "epoch:6 step:4894 [D loss: 0.397696, acc.: 82.81%] [G loss: 2.006240]\n",
      "epoch:6 step:4895 [D loss: 0.301842, acc.: 85.94%] [G loss: 4.078770]\n",
      "epoch:6 step:4896 [D loss: 0.192380, acc.: 96.88%] [G loss: 5.003418]\n",
      "epoch:6 step:4897 [D loss: 0.355552, acc.: 81.25%] [G loss: 3.628376]\n",
      "epoch:6 step:4898 [D loss: 0.355657, acc.: 82.81%] [G loss: 3.750991]\n",
      "epoch:6 step:4899 [D loss: 0.465250, acc.: 75.00%] [G loss: 1.734872]\n",
      "epoch:6 step:4900 [D loss: 0.333514, acc.: 85.16%] [G loss: 1.978427]\n",
      "epoch:6 step:4901 [D loss: 0.397171, acc.: 81.25%] [G loss: 1.984896]\n",
      "epoch:6 step:4902 [D loss: 0.451432, acc.: 81.25%] [G loss: 2.174417]\n",
      "epoch:6 step:4903 [D loss: 0.352958, acc.: 86.72%] [G loss: 2.680743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:4904 [D loss: 0.407441, acc.: 79.69%] [G loss: 2.261757]\n",
      "epoch:6 step:4905 [D loss: 0.440277, acc.: 74.22%] [G loss: 2.293067]\n",
      "epoch:6 step:4906 [D loss: 0.393933, acc.: 84.38%] [G loss: 3.248291]\n",
      "epoch:6 step:4907 [D loss: 0.269398, acc.: 92.19%] [G loss: 2.800759]\n",
      "epoch:6 step:4908 [D loss: 0.276607, acc.: 87.50%] [G loss: 3.307974]\n",
      "epoch:6 step:4909 [D loss: 0.399629, acc.: 77.34%] [G loss: 2.045216]\n",
      "epoch:6 step:4910 [D loss: 0.387217, acc.: 82.81%] [G loss: 2.689703]\n",
      "epoch:6 step:4911 [D loss: 0.452339, acc.: 75.78%] [G loss: 2.367435]\n",
      "epoch:6 step:4912 [D loss: 0.398266, acc.: 85.94%] [G loss: 2.937355]\n",
      "epoch:6 step:4913 [D loss: 0.314488, acc.: 86.72%] [G loss: 2.854562]\n",
      "epoch:6 step:4914 [D loss: 0.432395, acc.: 78.12%] [G loss: 2.291333]\n",
      "epoch:6 step:4915 [D loss: 0.469874, acc.: 75.78%] [G loss: 2.814758]\n",
      "epoch:6 step:4916 [D loss: 0.406535, acc.: 83.59%] [G loss: 3.388201]\n",
      "epoch:6 step:4917 [D loss: 0.424367, acc.: 82.03%] [G loss: 4.768385]\n",
      "epoch:6 step:4918 [D loss: 0.443822, acc.: 75.00%] [G loss: 2.627162]\n",
      "epoch:6 step:4919 [D loss: 0.383021, acc.: 81.25%] [G loss: 2.685987]\n",
      "epoch:6 step:4920 [D loss: 0.437359, acc.: 77.34%] [G loss: 1.976789]\n",
      "epoch:6 step:4921 [D loss: 0.414464, acc.: 78.91%] [G loss: 2.568152]\n",
      "epoch:6 step:4922 [D loss: 0.372468, acc.: 82.81%] [G loss: 2.089104]\n",
      "epoch:6 step:4923 [D loss: 0.251480, acc.: 88.28%] [G loss: 4.716063]\n",
      "epoch:6 step:4924 [D loss: 0.345457, acc.: 79.69%] [G loss: 3.775203]\n",
      "epoch:6 step:4925 [D loss: 0.424581, acc.: 78.91%] [G loss: 3.493783]\n",
      "epoch:6 step:4926 [D loss: 0.458947, acc.: 78.12%] [G loss: 6.494929]\n",
      "epoch:6 step:4927 [D loss: 0.539042, acc.: 74.22%] [G loss: 2.652165]\n",
      "epoch:6 step:4928 [D loss: 0.294838, acc.: 89.06%] [G loss: 3.564139]\n",
      "epoch:6 step:4929 [D loss: 0.414259, acc.: 75.00%] [G loss: 2.445421]\n",
      "epoch:6 step:4930 [D loss: 0.414327, acc.: 79.69%] [G loss: 2.105414]\n",
      "epoch:6 step:4931 [D loss: 0.271839, acc.: 89.06%] [G loss: 4.313249]\n",
      "epoch:6 step:4932 [D loss: 0.323466, acc.: 86.72%] [G loss: 2.531712]\n",
      "epoch:6 step:4933 [D loss: 0.504269, acc.: 74.22%] [G loss: 1.674019]\n",
      "epoch:6 step:4934 [D loss: 0.444604, acc.: 75.00%] [G loss: 1.818428]\n",
      "epoch:6 step:4935 [D loss: 0.566276, acc.: 68.75%] [G loss: 3.104202]\n",
      "epoch:6 step:4936 [D loss: 0.445936, acc.: 75.78%] [G loss: 3.396633]\n",
      "epoch:6 step:4937 [D loss: 0.514631, acc.: 74.22%] [G loss: 2.976700]\n",
      "epoch:6 step:4938 [D loss: 0.393416, acc.: 82.81%] [G loss: 2.647308]\n",
      "epoch:6 step:4939 [D loss: 0.264880, acc.: 90.62%] [G loss: 3.260236]\n",
      "epoch:6 step:4940 [D loss: 0.372067, acc.: 78.91%] [G loss: 3.350424]\n",
      "epoch:6 step:4941 [D loss: 0.413786, acc.: 80.47%] [G loss: 2.855342]\n",
      "epoch:6 step:4942 [D loss: 0.453432, acc.: 78.12%] [G loss: 1.483541]\n",
      "epoch:6 step:4943 [D loss: 0.358596, acc.: 83.59%] [G loss: 2.687527]\n",
      "epoch:6 step:4944 [D loss: 0.342801, acc.: 88.28%] [G loss: 3.296191]\n",
      "epoch:6 step:4945 [D loss: 0.424237, acc.: 77.34%] [G loss: 2.520974]\n",
      "epoch:6 step:4946 [D loss: 0.400420, acc.: 78.91%] [G loss: 2.191402]\n",
      "epoch:6 step:4947 [D loss: 0.332908, acc.: 83.59%] [G loss: 2.537413]\n",
      "epoch:6 step:4948 [D loss: 0.427390, acc.: 77.34%] [G loss: 2.116554]\n",
      "epoch:6 step:4949 [D loss: 0.343947, acc.: 82.03%] [G loss: 2.235580]\n",
      "epoch:6 step:4950 [D loss: 0.389589, acc.: 82.03%] [G loss: 2.201435]\n",
      "epoch:6 step:4951 [D loss: 0.399885, acc.: 84.38%] [G loss: 2.389051]\n",
      "epoch:6 step:4952 [D loss: 0.358438, acc.: 82.81%] [G loss: 2.899998]\n",
      "epoch:6 step:4953 [D loss: 0.451906, acc.: 80.47%] [G loss: 2.851612]\n",
      "epoch:6 step:4954 [D loss: 0.325382, acc.: 86.72%] [G loss: 2.909534]\n",
      "epoch:6 step:4955 [D loss: 0.484724, acc.: 75.78%] [G loss: 2.983569]\n",
      "epoch:6 step:4956 [D loss: 0.349168, acc.: 81.25%] [G loss: 2.880141]\n",
      "epoch:6 step:4957 [D loss: 0.417778, acc.: 78.12%] [G loss: 2.705280]\n",
      "epoch:6 step:4958 [D loss: 0.356322, acc.: 83.59%] [G loss: 2.568247]\n",
      "epoch:6 step:4959 [D loss: 0.417235, acc.: 79.69%] [G loss: 2.661913]\n",
      "epoch:6 step:4960 [D loss: 0.399864, acc.: 81.25%] [G loss: 2.795821]\n",
      "epoch:6 step:4961 [D loss: 0.427792, acc.: 75.78%] [G loss: 3.664799]\n",
      "epoch:6 step:4962 [D loss: 0.409650, acc.: 78.91%] [G loss: 4.862895]\n",
      "epoch:6 step:4963 [D loss: 0.465557, acc.: 79.69%] [G loss: 2.738812]\n",
      "epoch:6 step:4964 [D loss: 0.275100, acc.: 90.62%] [G loss: 2.763533]\n",
      "epoch:6 step:4965 [D loss: 0.420348, acc.: 74.22%] [G loss: 1.960912]\n",
      "epoch:6 step:4966 [D loss: 0.476600, acc.: 71.88%] [G loss: 2.119390]\n",
      "epoch:6 step:4967 [D loss: 0.353324, acc.: 86.72%] [G loss: 3.063090]\n",
      "epoch:6 step:4968 [D loss: 0.420876, acc.: 78.91%] [G loss: 2.708704]\n",
      "epoch:6 step:4969 [D loss: 0.261387, acc.: 88.28%] [G loss: 3.192404]\n",
      "epoch:6 step:4970 [D loss: 0.364092, acc.: 81.25%] [G loss: 2.255847]\n",
      "epoch:6 step:4971 [D loss: 0.284513, acc.: 89.06%] [G loss: 3.609133]\n",
      "epoch:6 step:4972 [D loss: 0.292711, acc.: 85.94%] [G loss: 3.367430]\n",
      "epoch:6 step:4973 [D loss: 0.398759, acc.: 77.34%] [G loss: 2.766075]\n",
      "epoch:6 step:4974 [D loss: 0.465342, acc.: 75.00%] [G loss: 1.773182]\n",
      "epoch:6 step:4975 [D loss: 0.482758, acc.: 74.22%] [G loss: 2.084506]\n",
      "epoch:6 step:4976 [D loss: 0.379724, acc.: 79.69%] [G loss: 2.512911]\n",
      "epoch:6 step:4977 [D loss: 0.468747, acc.: 75.78%] [G loss: 2.343751]\n",
      "epoch:6 step:4978 [D loss: 0.339184, acc.: 83.59%] [G loss: 2.722271]\n",
      "epoch:6 step:4979 [D loss: 0.419674, acc.: 75.00%] [G loss: 2.833562]\n",
      "epoch:6 step:4980 [D loss: 0.356530, acc.: 82.81%] [G loss: 2.656914]\n",
      "epoch:6 step:4981 [D loss: 0.356533, acc.: 82.81%] [G loss: 2.985566]\n",
      "epoch:6 step:4982 [D loss: 0.445787, acc.: 77.34%] [G loss: 2.689062]\n",
      "epoch:6 step:4983 [D loss: 0.584050, acc.: 67.97%] [G loss: 3.982346]\n",
      "epoch:6 step:4984 [D loss: 0.497938, acc.: 76.56%] [G loss: 4.329697]\n",
      "epoch:6 step:4985 [D loss: 0.816179, acc.: 67.97%] [G loss: 3.901197]\n",
      "epoch:6 step:4986 [D loss: 0.589938, acc.: 70.31%] [G loss: 2.227468]\n",
      "epoch:6 step:4987 [D loss: 0.684521, acc.: 67.97%] [G loss: 4.460407]\n",
      "epoch:6 step:4988 [D loss: 0.967290, acc.: 61.72%] [G loss: 4.464811]\n",
      "epoch:6 step:4989 [D loss: 0.997321, acc.: 60.16%] [G loss: 3.207783]\n",
      "epoch:6 step:4990 [D loss: 0.544445, acc.: 69.53%] [G loss: 1.151635]\n",
      "epoch:6 step:4991 [D loss: 0.638485, acc.: 60.94%] [G loss: 1.744766]\n",
      "epoch:6 step:4992 [D loss: 0.372201, acc.: 83.59%] [G loss: 2.537321]\n",
      "epoch:6 step:4993 [D loss: 0.392127, acc.: 79.69%] [G loss: 2.242981]\n",
      "epoch:6 step:4994 [D loss: 0.475461, acc.: 74.22%] [G loss: 2.176232]\n",
      "epoch:6 step:4995 [D loss: 0.340101, acc.: 81.25%] [G loss: 2.043911]\n",
      "epoch:6 step:4996 [D loss: 0.391735, acc.: 77.34%] [G loss: 2.217399]\n",
      "epoch:6 step:4997 [D loss: 0.399583, acc.: 81.25%] [G loss: 3.183051]\n",
      "epoch:6 step:4998 [D loss: 0.423680, acc.: 79.69%] [G loss: 2.177383]\n",
      "epoch:6 step:4999 [D loss: 0.359578, acc.: 85.94%] [G loss: 2.520010]\n",
      "epoch:6 step:5000 [D loss: 0.332478, acc.: 88.28%] [G loss: 2.699317]\n",
      "epoch:6 step:5001 [D loss: 0.402609, acc.: 79.69%] [G loss: 2.525694]\n",
      "epoch:6 step:5002 [D loss: 0.264958, acc.: 87.50%] [G loss: 3.084337]\n",
      "epoch:6 step:5003 [D loss: 0.307578, acc.: 85.94%] [G loss: 3.244798]\n",
      "epoch:6 step:5004 [D loss: 0.319523, acc.: 85.94%] [G loss: 2.409951]\n",
      "epoch:6 step:5005 [D loss: 0.344717, acc.: 90.62%] [G loss: 2.078465]\n",
      "epoch:6 step:5006 [D loss: 0.318249, acc.: 88.28%] [G loss: 2.702090]\n",
      "epoch:6 step:5007 [D loss: 0.319034, acc.: 87.50%] [G loss: 2.555526]\n",
      "epoch:6 step:5008 [D loss: 0.455023, acc.: 75.78%] [G loss: 1.701217]\n",
      "epoch:6 step:5009 [D loss: 0.397532, acc.: 82.81%] [G loss: 1.753985]\n",
      "epoch:6 step:5010 [D loss: 0.374095, acc.: 79.69%] [G loss: 2.176657]\n",
      "epoch:6 step:5011 [D loss: 0.362258, acc.: 82.03%] [G loss: 2.754308]\n",
      "epoch:6 step:5012 [D loss: 0.344123, acc.: 87.50%] [G loss: 2.019562]\n",
      "epoch:6 step:5013 [D loss: 0.448344, acc.: 78.12%] [G loss: 2.029171]\n",
      "epoch:6 step:5014 [D loss: 0.391511, acc.: 83.59%] [G loss: 1.858033]\n",
      "epoch:6 step:5015 [D loss: 0.362991, acc.: 85.94%] [G loss: 2.428378]\n",
      "epoch:6 step:5016 [D loss: 0.402935, acc.: 82.03%] [G loss: 1.501534]\n",
      "epoch:6 step:5017 [D loss: 0.396991, acc.: 81.25%] [G loss: 2.091111]\n",
      "epoch:6 step:5018 [D loss: 0.382925, acc.: 85.16%] [G loss: 1.988311]\n",
      "epoch:6 step:5019 [D loss: 0.414516, acc.: 79.69%] [G loss: 2.379505]\n",
      "epoch:6 step:5020 [D loss: 0.378921, acc.: 85.94%] [G loss: 1.558661]\n",
      "epoch:6 step:5021 [D loss: 0.374289, acc.: 83.59%] [G loss: 2.145601]\n",
      "epoch:6 step:5022 [D loss: 0.313013, acc.: 88.28%] [G loss: 2.540774]\n",
      "epoch:6 step:5023 [D loss: 0.274048, acc.: 89.06%] [G loss: 3.057108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5024 [D loss: 0.389019, acc.: 83.59%] [G loss: 2.124849]\n",
      "epoch:6 step:5025 [D loss: 0.413672, acc.: 78.91%] [G loss: 1.588785]\n",
      "epoch:6 step:5026 [D loss: 0.386101, acc.: 80.47%] [G loss: 2.168219]\n",
      "epoch:6 step:5027 [D loss: 0.269121, acc.: 92.19%] [G loss: 2.567545]\n",
      "epoch:6 step:5028 [D loss: 0.425438, acc.: 79.69%] [G loss: 2.103048]\n",
      "epoch:6 step:5029 [D loss: 0.368062, acc.: 84.38%] [G loss: 1.927374]\n",
      "epoch:6 step:5030 [D loss: 0.503045, acc.: 71.88%] [G loss: 1.547541]\n",
      "epoch:6 step:5031 [D loss: 0.339807, acc.: 88.28%] [G loss: 2.404951]\n",
      "epoch:6 step:5032 [D loss: 0.377708, acc.: 79.69%] [G loss: 2.523530]\n",
      "epoch:6 step:5033 [D loss: 0.320652, acc.: 87.50%] [G loss: 3.457266]\n",
      "epoch:6 step:5034 [D loss: 0.358586, acc.: 87.50%] [G loss: 2.571437]\n",
      "epoch:6 step:5035 [D loss: 0.345352, acc.: 83.59%] [G loss: 1.871469]\n",
      "epoch:6 step:5036 [D loss: 0.414276, acc.: 77.34%] [G loss: 1.796180]\n",
      "epoch:6 step:5037 [D loss: 0.353000, acc.: 86.72%] [G loss: 2.318818]\n",
      "epoch:6 step:5038 [D loss: 0.451297, acc.: 72.66%] [G loss: 2.654954]\n",
      "epoch:6 step:5039 [D loss: 0.411537, acc.: 78.91%] [G loss: 2.731609]\n",
      "epoch:6 step:5040 [D loss: 0.335195, acc.: 84.38%] [G loss: 3.256318]\n",
      "epoch:6 step:5041 [D loss: 0.300454, acc.: 85.16%] [G loss: 2.769279]\n",
      "epoch:6 step:5042 [D loss: 0.327113, acc.: 89.06%] [G loss: 2.322327]\n",
      "epoch:6 step:5043 [D loss: 0.440546, acc.: 78.12%] [G loss: 2.802138]\n",
      "epoch:6 step:5044 [D loss: 0.614571, acc.: 71.88%] [G loss: 4.302206]\n",
      "epoch:6 step:5045 [D loss: 0.930930, acc.: 58.59%] [G loss: 2.708436]\n",
      "epoch:6 step:5046 [D loss: 0.560672, acc.: 73.44%] [G loss: 3.743706]\n",
      "epoch:6 step:5047 [D loss: 0.998254, acc.: 61.72%] [G loss: 3.360873]\n",
      "epoch:6 step:5048 [D loss: 0.888441, acc.: 60.94%] [G loss: 2.917655]\n",
      "epoch:6 step:5049 [D loss: 0.452577, acc.: 78.91%] [G loss: 2.679726]\n",
      "epoch:6 step:5050 [D loss: 0.499185, acc.: 70.31%] [G loss: 2.558745]\n",
      "epoch:6 step:5051 [D loss: 0.418519, acc.: 77.34%] [G loss: 2.629148]\n",
      "epoch:6 step:5052 [D loss: 0.605825, acc.: 68.75%] [G loss: 2.475794]\n",
      "epoch:6 step:5053 [D loss: 0.456411, acc.: 78.91%] [G loss: 2.372118]\n",
      "epoch:6 step:5054 [D loss: 0.320038, acc.: 86.72%] [G loss: 2.947659]\n",
      "epoch:6 step:5055 [D loss: 0.340258, acc.: 80.47%] [G loss: 2.455141]\n",
      "epoch:6 step:5056 [D loss: 0.353085, acc.: 84.38%] [G loss: 1.963226]\n",
      "epoch:6 step:5057 [D loss: 0.255388, acc.: 90.62%] [G loss: 3.594978]\n",
      "epoch:6 step:5058 [D loss: 0.268021, acc.: 90.62%] [G loss: 3.707474]\n",
      "epoch:6 step:5059 [D loss: 0.391877, acc.: 81.25%] [G loss: 2.341874]\n",
      "epoch:6 step:5060 [D loss: 0.401915, acc.: 80.47%] [G loss: 1.551572]\n",
      "epoch:6 step:5061 [D loss: 0.330223, acc.: 84.38%] [G loss: 2.314900]\n",
      "epoch:6 step:5062 [D loss: 0.429243, acc.: 82.03%] [G loss: 2.561756]\n",
      "epoch:6 step:5063 [D loss: 0.376356, acc.: 79.69%] [G loss: 1.481495]\n",
      "epoch:6 step:5064 [D loss: 0.432999, acc.: 77.34%] [G loss: 1.405790]\n",
      "epoch:6 step:5065 [D loss: 0.433261, acc.: 79.69%] [G loss: 1.672518]\n",
      "epoch:6 step:5066 [D loss: 0.353371, acc.: 82.81%] [G loss: 1.788270]\n",
      "epoch:6 step:5067 [D loss: 0.346964, acc.: 88.28%] [G loss: 1.920852]\n",
      "epoch:6 step:5068 [D loss: 0.454866, acc.: 74.22%] [G loss: 2.368639]\n",
      "epoch:6 step:5069 [D loss: 0.509161, acc.: 71.09%] [G loss: 2.472136]\n",
      "epoch:6 step:5070 [D loss: 0.356788, acc.: 83.59%] [G loss: 3.714585]\n",
      "epoch:6 step:5071 [D loss: 0.262098, acc.: 89.84%] [G loss: 2.754663]\n",
      "epoch:6 step:5072 [D loss: 0.365784, acc.: 82.03%] [G loss: 1.852867]\n",
      "epoch:6 step:5073 [D loss: 0.375177, acc.: 80.47%] [G loss: 1.628402]\n",
      "epoch:6 step:5074 [D loss: 0.417137, acc.: 79.69%] [G loss: 2.070251]\n",
      "epoch:6 step:5075 [D loss: 0.486659, acc.: 76.56%] [G loss: 2.091364]\n",
      "epoch:6 step:5076 [D loss: 0.439122, acc.: 77.34%] [G loss: 2.096070]\n",
      "epoch:6 step:5077 [D loss: 0.364997, acc.: 85.16%] [G loss: 2.172695]\n",
      "epoch:6 step:5078 [D loss: 0.457937, acc.: 78.12%] [G loss: 2.346727]\n",
      "epoch:6 step:5079 [D loss: 0.401780, acc.: 83.59%] [G loss: 3.324433]\n",
      "epoch:6 step:5080 [D loss: 0.308762, acc.: 87.50%] [G loss: 4.564940]\n",
      "epoch:6 step:5081 [D loss: 0.326109, acc.: 81.25%] [G loss: 2.767484]\n",
      "epoch:6 step:5082 [D loss: 0.290646, acc.: 89.06%] [G loss: 2.953614]\n",
      "epoch:6 step:5083 [D loss: 0.368160, acc.: 83.59%] [G loss: 3.745841]\n",
      "epoch:6 step:5084 [D loss: 0.375918, acc.: 75.78%] [G loss: 2.642811]\n",
      "epoch:6 step:5085 [D loss: 0.337158, acc.: 82.81%] [G loss: 2.558501]\n",
      "epoch:6 step:5086 [D loss: 0.388527, acc.: 81.25%] [G loss: 2.336221]\n",
      "epoch:6 step:5087 [D loss: 0.414534, acc.: 81.25%] [G loss: 1.945060]\n",
      "epoch:6 step:5088 [D loss: 0.257726, acc.: 92.97%] [G loss: 3.167008]\n",
      "epoch:6 step:5089 [D loss: 0.236125, acc.: 92.19%] [G loss: 3.600495]\n",
      "epoch:6 step:5090 [D loss: 0.352408, acc.: 83.59%] [G loss: 2.768533]\n",
      "epoch:6 step:5091 [D loss: 0.232399, acc.: 88.28%] [G loss: 4.257335]\n",
      "epoch:6 step:5092 [D loss: 0.348436, acc.: 82.03%] [G loss: 2.237854]\n",
      "epoch:6 step:5093 [D loss: 0.346162, acc.: 87.50%] [G loss: 3.562720]\n",
      "epoch:6 step:5094 [D loss: 0.335704, acc.: 85.94%] [G loss: 2.936628]\n",
      "epoch:6 step:5095 [D loss: 0.338656, acc.: 81.25%] [G loss: 2.931636]\n",
      "epoch:6 step:5096 [D loss: 0.378354, acc.: 78.91%] [G loss: 1.947800]\n",
      "epoch:6 step:5097 [D loss: 0.475893, acc.: 75.78%] [G loss: 2.842871]\n",
      "epoch:6 step:5098 [D loss: 0.353995, acc.: 84.38%] [G loss: 5.695050]\n",
      "epoch:6 step:5099 [D loss: 0.416864, acc.: 76.56%] [G loss: 2.523529]\n",
      "epoch:6 step:5100 [D loss: 0.271657, acc.: 86.72%] [G loss: 3.037709]\n",
      "epoch:6 step:5101 [D loss: 0.374282, acc.: 78.91%] [G loss: 2.830637]\n",
      "epoch:6 step:5102 [D loss: 0.328896, acc.: 82.81%] [G loss: 2.188170]\n",
      "epoch:6 step:5103 [D loss: 0.527342, acc.: 65.62%] [G loss: 2.131872]\n",
      "epoch:6 step:5104 [D loss: 0.335621, acc.: 88.28%] [G loss: 2.161422]\n",
      "epoch:6 step:5105 [D loss: 0.388958, acc.: 80.47%] [G loss: 1.608035]\n",
      "epoch:6 step:5106 [D loss: 0.332559, acc.: 89.06%] [G loss: 2.352118]\n",
      "epoch:6 step:5107 [D loss: 0.254398, acc.: 89.06%] [G loss: 3.712933]\n",
      "epoch:6 step:5108 [D loss: 0.390234, acc.: 82.81%] [G loss: 1.993154]\n",
      "epoch:6 step:5109 [D loss: 0.282573, acc.: 86.72%] [G loss: 3.672067]\n",
      "epoch:6 step:5110 [D loss: 0.289430, acc.: 87.50%] [G loss: 2.940249]\n",
      "epoch:6 step:5111 [D loss: 0.376850, acc.: 79.69%] [G loss: 3.467809]\n",
      "epoch:6 step:5112 [D loss: 0.327512, acc.: 83.59%] [G loss: 2.334288]\n",
      "epoch:6 step:5113 [D loss: 0.326390, acc.: 85.94%] [G loss: 2.874407]\n",
      "epoch:6 step:5114 [D loss: 0.348756, acc.: 84.38%] [G loss: 2.650455]\n",
      "epoch:6 step:5115 [D loss: 0.433481, acc.: 76.56%] [G loss: 3.840798]\n",
      "epoch:6 step:5116 [D loss: 0.744230, acc.: 65.62%] [G loss: 2.384466]\n",
      "epoch:6 step:5117 [D loss: 0.563409, acc.: 75.78%] [G loss: 3.556648]\n",
      "epoch:6 step:5118 [D loss: 0.614571, acc.: 73.44%] [G loss: 3.279412]\n",
      "epoch:6 step:5119 [D loss: 0.481630, acc.: 70.31%] [G loss: 2.622749]\n",
      "epoch:6 step:5120 [D loss: 0.443593, acc.: 81.25%] [G loss: 2.660274]\n",
      "epoch:6 step:5121 [D loss: 0.536598, acc.: 76.56%] [G loss: 1.982382]\n",
      "epoch:6 step:5122 [D loss: 0.413306, acc.: 78.91%] [G loss: 2.365592]\n",
      "epoch:6 step:5123 [D loss: 0.351902, acc.: 84.38%] [G loss: 3.442229]\n",
      "epoch:6 step:5124 [D loss: 0.344788, acc.: 86.72%] [G loss: 2.599642]\n",
      "epoch:6 step:5125 [D loss: 0.374523, acc.: 81.25%] [G loss: 2.447746]\n",
      "epoch:6 step:5126 [D loss: 0.345296, acc.: 85.94%] [G loss: 2.626187]\n",
      "epoch:6 step:5127 [D loss: 0.367329, acc.: 81.25%] [G loss: 3.063194]\n",
      "epoch:6 step:5128 [D loss: 0.296049, acc.: 87.50%] [G loss: 2.678719]\n",
      "epoch:6 step:5129 [D loss: 0.351932, acc.: 84.38%] [G loss: 2.593849]\n",
      "epoch:6 step:5130 [D loss: 0.423159, acc.: 84.38%] [G loss: 2.118133]\n",
      "epoch:6 step:5131 [D loss: 0.536401, acc.: 75.00%] [G loss: 4.016521]\n",
      "epoch:6 step:5132 [D loss: 0.575570, acc.: 75.78%] [G loss: 2.220776]\n",
      "epoch:6 step:5133 [D loss: 0.336827, acc.: 82.81%] [G loss: 3.156591]\n",
      "epoch:6 step:5134 [D loss: 0.412240, acc.: 78.91%] [G loss: 2.552589]\n",
      "epoch:6 step:5135 [D loss: 0.415876, acc.: 78.12%] [G loss: 2.343065]\n",
      "epoch:6 step:5136 [D loss: 0.442736, acc.: 75.78%] [G loss: 1.891719]\n",
      "epoch:6 step:5137 [D loss: 0.534747, acc.: 78.91%] [G loss: 3.058856]\n",
      "epoch:6 step:5138 [D loss: 0.338295, acc.: 87.50%] [G loss: 2.067566]\n",
      "epoch:6 step:5139 [D loss: 0.417918, acc.: 75.78%] [G loss: 2.309309]\n",
      "epoch:6 step:5140 [D loss: 0.396290, acc.: 82.03%] [G loss: 1.644262]\n",
      "epoch:6 step:5141 [D loss: 0.296315, acc.: 89.06%] [G loss: 2.842230]\n",
      "epoch:6 step:5142 [D loss: 0.447001, acc.: 73.44%] [G loss: 1.744442]\n",
      "epoch:6 step:5143 [D loss: 0.356625, acc.: 85.94%] [G loss: 2.567458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5144 [D loss: 0.269820, acc.: 89.06%] [G loss: 2.456657]\n",
      "epoch:6 step:5145 [D loss: 0.347200, acc.: 84.38%] [G loss: 2.093898]\n",
      "epoch:6 step:5146 [D loss: 0.439635, acc.: 83.59%] [G loss: 2.334642]\n",
      "epoch:6 step:5147 [D loss: 0.460720, acc.: 75.00%] [G loss: 2.120981]\n",
      "epoch:6 step:5148 [D loss: 0.368069, acc.: 82.81%] [G loss: 2.307611]\n",
      "epoch:6 step:5149 [D loss: 0.266258, acc.: 91.41%] [G loss: 3.151782]\n",
      "epoch:6 step:5150 [D loss: 0.345620, acc.: 86.72%] [G loss: 2.175061]\n",
      "epoch:6 step:5151 [D loss: 0.479556, acc.: 77.34%] [G loss: 2.200058]\n",
      "epoch:6 step:5152 [D loss: 0.410249, acc.: 81.25%] [G loss: 2.675067]\n",
      "epoch:6 step:5153 [D loss: 0.383130, acc.: 83.59%] [G loss: 2.133821]\n",
      "epoch:6 step:5154 [D loss: 0.229847, acc.: 92.97%] [G loss: 4.540718]\n",
      "epoch:6 step:5155 [D loss: 0.250876, acc.: 87.50%] [G loss: 3.980363]\n",
      "epoch:6 step:5156 [D loss: 0.366118, acc.: 85.94%] [G loss: 2.739520]\n",
      "epoch:6 step:5157 [D loss: 0.352349, acc.: 84.38%] [G loss: 1.969233]\n",
      "epoch:6 step:5158 [D loss: 0.371014, acc.: 82.81%] [G loss: 3.080999]\n",
      "epoch:6 step:5159 [D loss: 0.399023, acc.: 78.12%] [G loss: 1.785885]\n",
      "epoch:6 step:5160 [D loss: 0.423265, acc.: 78.12%] [G loss: 2.127376]\n",
      "epoch:6 step:5161 [D loss: 0.280065, acc.: 89.06%] [G loss: 2.635343]\n",
      "epoch:6 step:5162 [D loss: 0.248344, acc.: 90.62%] [G loss: 3.456277]\n",
      "epoch:6 step:5163 [D loss: 0.365164, acc.: 82.03%] [G loss: 2.015635]\n",
      "epoch:6 step:5164 [D loss: 0.254179, acc.: 89.84%] [G loss: 2.777378]\n",
      "epoch:6 step:5165 [D loss: 0.195969, acc.: 94.53%] [G loss: 3.957131]\n",
      "epoch:6 step:5166 [D loss: 0.369419, acc.: 79.69%] [G loss: 2.664577]\n",
      "epoch:6 step:5167 [D loss: 0.449181, acc.: 75.78%] [G loss: 2.191847]\n",
      "epoch:6 step:5168 [D loss: 0.427395, acc.: 75.00%] [G loss: 2.687331]\n",
      "epoch:6 step:5169 [D loss: 0.401011, acc.: 82.81%] [G loss: 3.148768]\n",
      "epoch:6 step:5170 [D loss: 0.426928, acc.: 82.03%] [G loss: 3.087111]\n",
      "epoch:6 step:5171 [D loss: 0.389047, acc.: 79.69%] [G loss: 3.692977]\n",
      "epoch:6 step:5172 [D loss: 0.367710, acc.: 84.38%] [G loss: 3.526679]\n",
      "epoch:6 step:5173 [D loss: 0.324482, acc.: 85.16%] [G loss: 2.924365]\n",
      "epoch:6 step:5174 [D loss: 0.475840, acc.: 72.66%] [G loss: 2.755328]\n",
      "epoch:6 step:5175 [D loss: 0.532540, acc.: 68.75%] [G loss: 3.024198]\n",
      "epoch:6 step:5176 [D loss: 0.369679, acc.: 82.03%] [G loss: 3.190882]\n",
      "epoch:6 step:5177 [D loss: 0.496698, acc.: 75.78%] [G loss: 2.147953]\n",
      "epoch:6 step:5178 [D loss: 0.393609, acc.: 82.81%] [G loss: 2.164431]\n",
      "epoch:6 step:5179 [D loss: 0.391384, acc.: 82.81%] [G loss: 2.582893]\n",
      "epoch:6 step:5180 [D loss: 0.264112, acc.: 89.84%] [G loss: 3.187908]\n",
      "epoch:6 step:5181 [D loss: 0.383614, acc.: 82.03%] [G loss: 2.768324]\n",
      "epoch:6 step:5182 [D loss: 0.458032, acc.: 71.09%] [G loss: 2.607790]\n",
      "epoch:6 step:5183 [D loss: 0.366567, acc.: 82.81%] [G loss: 3.451061]\n",
      "epoch:6 step:5184 [D loss: 0.262491, acc.: 91.41%] [G loss: 2.941223]\n",
      "epoch:6 step:5185 [D loss: 0.433385, acc.: 81.25%] [G loss: 2.557475]\n",
      "epoch:6 step:5186 [D loss: 0.366501, acc.: 81.25%] [G loss: 2.860575]\n",
      "epoch:6 step:5187 [D loss: 0.374612, acc.: 85.16%] [G loss: 2.160786]\n",
      "epoch:6 step:5188 [D loss: 0.403607, acc.: 76.56%] [G loss: 2.448286]\n",
      "epoch:6 step:5189 [D loss: 0.404620, acc.: 80.47%] [G loss: 2.588485]\n",
      "epoch:6 step:5190 [D loss: 0.448588, acc.: 75.78%] [G loss: 2.692688]\n",
      "epoch:6 step:5191 [D loss: 0.396205, acc.: 84.38%] [G loss: 2.493989]\n",
      "epoch:6 step:5192 [D loss: 0.429303, acc.: 76.56%] [G loss: 3.121232]\n",
      "epoch:6 step:5193 [D loss: 0.422089, acc.: 76.56%] [G loss: 2.594033]\n",
      "epoch:6 step:5194 [D loss: 0.378876, acc.: 85.16%] [G loss: 2.587377]\n",
      "epoch:6 step:5195 [D loss: 0.280719, acc.: 92.97%] [G loss: 3.357152]\n",
      "epoch:6 step:5196 [D loss: 0.204266, acc.: 91.41%] [G loss: 4.701662]\n",
      "epoch:6 step:5197 [D loss: 0.327968, acc.: 83.59%] [G loss: 3.096704]\n",
      "epoch:6 step:5198 [D loss: 0.241124, acc.: 92.19%] [G loss: 3.235097]\n",
      "epoch:6 step:5199 [D loss: 0.308216, acc.: 88.28%] [G loss: 2.908978]\n",
      "epoch:6 step:5200 [D loss: 0.339923, acc.: 87.50%] [G loss: 2.520244]\n",
      "epoch:6 step:5201 [D loss: 0.346928, acc.: 87.50%] [G loss: 2.159814]\n",
      "epoch:6 step:5202 [D loss: 0.481214, acc.: 77.34%] [G loss: 2.421988]\n",
      "epoch:6 step:5203 [D loss: 0.429055, acc.: 76.56%] [G loss: 2.694754]\n",
      "epoch:6 step:5204 [D loss: 0.350669, acc.: 81.25%] [G loss: 3.508272]\n",
      "epoch:6 step:5205 [D loss: 0.773697, acc.: 69.53%] [G loss: 2.983930]\n",
      "epoch:6 step:5206 [D loss: 0.720015, acc.: 67.19%] [G loss: 5.765071]\n",
      "epoch:6 step:5207 [D loss: 1.650356, acc.: 46.88%] [G loss: 2.270610]\n",
      "epoch:6 step:5208 [D loss: 0.732779, acc.: 75.00%] [G loss: 1.995369]\n",
      "epoch:6 step:5209 [D loss: 0.422456, acc.: 78.91%] [G loss: 3.243409]\n",
      "epoch:6 step:5210 [D loss: 0.344469, acc.: 82.81%] [G loss: 3.366148]\n",
      "epoch:6 step:5211 [D loss: 0.387560, acc.: 80.47%] [G loss: 3.856550]\n",
      "epoch:6 step:5212 [D loss: 0.409079, acc.: 78.91%] [G loss: 2.921442]\n",
      "epoch:6 step:5213 [D loss: 0.567862, acc.: 69.53%] [G loss: 2.278735]\n",
      "epoch:6 step:5214 [D loss: 0.333276, acc.: 83.59%] [G loss: 2.589060]\n",
      "epoch:6 step:5215 [D loss: 0.374546, acc.: 82.81%] [G loss: 2.504540]\n",
      "epoch:6 step:5216 [D loss: 0.408368, acc.: 79.69%] [G loss: 1.797721]\n",
      "epoch:6 step:5217 [D loss: 0.367782, acc.: 85.94%] [G loss: 3.418163]\n",
      "epoch:6 step:5218 [D loss: 0.298194, acc.: 86.72%] [G loss: 4.050430]\n",
      "epoch:6 step:5219 [D loss: 0.454239, acc.: 75.00%] [G loss: 2.694412]\n",
      "epoch:6 step:5220 [D loss: 0.373275, acc.: 79.69%] [G loss: 3.451017]\n",
      "epoch:6 step:5221 [D loss: 0.262631, acc.: 89.06%] [G loss: 2.554581]\n",
      "epoch:6 step:5222 [D loss: 0.346889, acc.: 81.25%] [G loss: 2.069972]\n",
      "epoch:6 step:5223 [D loss: 0.438445, acc.: 79.69%] [G loss: 3.665515]\n",
      "epoch:6 step:5224 [D loss: 0.532165, acc.: 69.53%] [G loss: 2.565258]\n",
      "epoch:6 step:5225 [D loss: 0.432504, acc.: 76.56%] [G loss: 2.207541]\n",
      "epoch:6 step:5226 [D loss: 0.348069, acc.: 85.16%] [G loss: 2.130168]\n",
      "epoch:6 step:5227 [D loss: 0.337509, acc.: 85.16%] [G loss: 3.419801]\n",
      "epoch:6 step:5228 [D loss: 0.326357, acc.: 84.38%] [G loss: 2.502504]\n",
      "epoch:6 step:5229 [D loss: 0.317941, acc.: 85.16%] [G loss: 2.717113]\n",
      "epoch:6 step:5230 [D loss: 0.426863, acc.: 76.56%] [G loss: 1.919482]\n",
      "epoch:6 step:5231 [D loss: 0.358137, acc.: 83.59%] [G loss: 2.861504]\n",
      "epoch:6 step:5232 [D loss: 0.285831, acc.: 89.06%] [G loss: 3.260962]\n",
      "epoch:6 step:5233 [D loss: 0.353077, acc.: 85.16%] [G loss: 2.494343]\n",
      "epoch:6 step:5234 [D loss: 0.322203, acc.: 86.72%] [G loss: 2.237871]\n",
      "epoch:6 step:5235 [D loss: 0.291764, acc.: 88.28%] [G loss: 2.771888]\n",
      "epoch:6 step:5236 [D loss: 0.307336, acc.: 84.38%] [G loss: 2.858477]\n",
      "epoch:6 step:5237 [D loss: 0.351592, acc.: 85.16%] [G loss: 2.395659]\n",
      "epoch:6 step:5238 [D loss: 0.353681, acc.: 84.38%] [G loss: 3.240053]\n",
      "epoch:6 step:5239 [D loss: 0.256901, acc.: 88.28%] [G loss: 3.011967]\n",
      "epoch:6 step:5240 [D loss: 0.351581, acc.: 82.03%] [G loss: 2.248410]\n",
      "epoch:6 step:5241 [D loss: 0.266529, acc.: 92.97%] [G loss: 2.738758]\n",
      "epoch:6 step:5242 [D loss: 0.299108, acc.: 89.06%] [G loss: 2.594208]\n",
      "epoch:6 step:5243 [D loss: 0.410536, acc.: 75.78%] [G loss: 1.779379]\n",
      "epoch:6 step:5244 [D loss: 0.474367, acc.: 75.78%] [G loss: 1.948671]\n",
      "epoch:6 step:5245 [D loss: 0.421130, acc.: 78.91%] [G loss: 2.297790]\n",
      "epoch:6 step:5246 [D loss: 0.302453, acc.: 88.28%] [G loss: 2.554323]\n",
      "epoch:6 step:5247 [D loss: 0.412322, acc.: 79.69%] [G loss: 1.921172]\n",
      "epoch:6 step:5248 [D loss: 0.315606, acc.: 89.06%] [G loss: 2.600869]\n",
      "epoch:6 step:5249 [D loss: 0.414095, acc.: 78.91%] [G loss: 2.528578]\n",
      "epoch:6 step:5250 [D loss: 0.354314, acc.: 82.03%] [G loss: 2.248675]\n",
      "epoch:6 step:5251 [D loss: 0.426695, acc.: 75.00%] [G loss: 2.628090]\n",
      "epoch:6 step:5252 [D loss: 0.363920, acc.: 85.16%] [G loss: 2.391652]\n",
      "epoch:6 step:5253 [D loss: 0.358521, acc.: 85.16%] [G loss: 2.616623]\n",
      "epoch:6 step:5254 [D loss: 0.408455, acc.: 78.12%] [G loss: 2.370327]\n",
      "epoch:6 step:5255 [D loss: 0.375870, acc.: 80.47%] [G loss: 2.561697]\n",
      "epoch:6 step:5256 [D loss: 0.311927, acc.: 89.84%] [G loss: 2.781640]\n",
      "epoch:6 step:5257 [D loss: 0.287526, acc.: 87.50%] [G loss: 3.267431]\n",
      "epoch:6 step:5258 [D loss: 0.412194, acc.: 78.91%] [G loss: 2.368386]\n",
      "epoch:6 step:5259 [D loss: 0.373486, acc.: 83.59%] [G loss: 2.330011]\n",
      "epoch:6 step:5260 [D loss: 0.346145, acc.: 84.38%] [G loss: 3.162022]\n",
      "epoch:6 step:5261 [D loss: 0.364668, acc.: 85.16%] [G loss: 2.678933]\n",
      "epoch:6 step:5262 [D loss: 0.238394, acc.: 94.53%] [G loss: 2.949336]\n",
      "epoch:6 step:5263 [D loss: 0.313678, acc.: 88.28%] [G loss: 2.117923]\n",
      "epoch:6 step:5264 [D loss: 0.290433, acc.: 90.62%] [G loss: 2.941401]\n",
      "epoch:6 step:5265 [D loss: 0.390062, acc.: 85.94%] [G loss: 3.049792]\n",
      "epoch:6 step:5266 [D loss: 0.433800, acc.: 77.34%] [G loss: 3.620142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5267 [D loss: 0.351792, acc.: 85.94%] [G loss: 2.688138]\n",
      "epoch:6 step:5268 [D loss: 0.289713, acc.: 91.41%] [G loss: 3.079409]\n",
      "epoch:6 step:5269 [D loss: 0.377912, acc.: 79.69%] [G loss: 3.317994]\n",
      "epoch:6 step:5270 [D loss: 0.439399, acc.: 78.91%] [G loss: 3.112423]\n",
      "epoch:6 step:5271 [D loss: 0.484963, acc.: 74.22%] [G loss: 2.781176]\n",
      "epoch:6 step:5272 [D loss: 0.444824, acc.: 75.78%] [G loss: 2.010087]\n",
      "epoch:6 step:5273 [D loss: 0.335786, acc.: 85.16%] [G loss: 2.799357]\n",
      "epoch:6 step:5274 [D loss: 0.349302, acc.: 86.72%] [G loss: 3.098884]\n",
      "epoch:6 step:5275 [D loss: 0.244635, acc.: 86.72%] [G loss: 3.388794]\n",
      "epoch:6 step:5276 [D loss: 0.340383, acc.: 84.38%] [G loss: 2.043294]\n",
      "epoch:6 step:5277 [D loss: 0.315899, acc.: 90.62%] [G loss: 2.308766]\n",
      "epoch:6 step:5278 [D loss: 0.318498, acc.: 85.94%] [G loss: 2.397759]\n",
      "epoch:6 step:5279 [D loss: 0.312427, acc.: 85.94%] [G loss: 2.355625]\n",
      "epoch:6 step:5280 [D loss: 0.346325, acc.: 85.16%] [G loss: 2.527178]\n",
      "epoch:6 step:5281 [D loss: 0.303492, acc.: 87.50%] [G loss: 2.435052]\n",
      "epoch:6 step:5282 [D loss: 0.378613, acc.: 81.25%] [G loss: 2.209711]\n",
      "epoch:6 step:5283 [D loss: 0.353021, acc.: 84.38%] [G loss: 2.507776]\n",
      "epoch:6 step:5284 [D loss: 0.375159, acc.: 78.91%] [G loss: 3.369792]\n",
      "epoch:6 step:5285 [D loss: 0.387697, acc.: 80.47%] [G loss: 3.121076]\n",
      "epoch:6 step:5286 [D loss: 0.286488, acc.: 89.84%] [G loss: 3.490084]\n",
      "epoch:6 step:5287 [D loss: 0.291792, acc.: 88.28%] [G loss: 3.168083]\n",
      "epoch:6 step:5288 [D loss: 0.334475, acc.: 83.59%] [G loss: 2.005480]\n",
      "epoch:6 step:5289 [D loss: 0.331532, acc.: 89.84%] [G loss: 2.615891]\n",
      "epoch:6 step:5290 [D loss: 0.347420, acc.: 85.94%] [G loss: 2.519501]\n",
      "epoch:6 step:5291 [D loss: 0.375437, acc.: 85.94%] [G loss: 2.062653]\n",
      "epoch:6 step:5292 [D loss: 0.451123, acc.: 71.88%] [G loss: 3.695567]\n",
      "epoch:6 step:5293 [D loss: 0.570172, acc.: 71.88%] [G loss: 3.698480]\n",
      "epoch:6 step:5294 [D loss: 0.517763, acc.: 73.44%] [G loss: 3.253944]\n",
      "epoch:6 step:5295 [D loss: 0.645258, acc.: 72.66%] [G loss: 3.785562]\n",
      "epoch:6 step:5296 [D loss: 1.062032, acc.: 57.81%] [G loss: 2.819330]\n",
      "epoch:6 step:5297 [D loss: 0.791702, acc.: 66.41%] [G loss: 2.263934]\n",
      "epoch:6 step:5298 [D loss: 0.393643, acc.: 78.91%] [G loss: 2.764568]\n",
      "epoch:6 step:5299 [D loss: 0.310623, acc.: 85.94%] [G loss: 3.027793]\n",
      "epoch:6 step:5300 [D loss: 0.260504, acc.: 89.84%] [G loss: 2.657113]\n",
      "epoch:6 step:5301 [D loss: 0.295491, acc.: 86.72%] [G loss: 2.694406]\n",
      "epoch:6 step:5302 [D loss: 0.333640, acc.: 86.72%] [G loss: 2.259975]\n",
      "epoch:6 step:5303 [D loss: 0.389371, acc.: 82.81%] [G loss: 2.135043]\n",
      "epoch:6 step:5304 [D loss: 0.353385, acc.: 85.16%] [G loss: 2.575676]\n",
      "epoch:6 step:5305 [D loss: 0.290683, acc.: 90.62%] [G loss: 2.698815]\n",
      "epoch:6 step:5306 [D loss: 0.358201, acc.: 82.03%] [G loss: 2.593493]\n",
      "epoch:6 step:5307 [D loss: 0.332721, acc.: 87.50%] [G loss: 2.223995]\n",
      "epoch:6 step:5308 [D loss: 0.356102, acc.: 85.16%] [G loss: 2.562905]\n",
      "epoch:6 step:5309 [D loss: 0.339330, acc.: 88.28%] [G loss: 2.365441]\n",
      "epoch:6 step:5310 [D loss: 0.356416, acc.: 87.50%] [G loss: 2.333059]\n",
      "epoch:6 step:5311 [D loss: 0.366322, acc.: 81.25%] [G loss: 2.408879]\n",
      "epoch:6 step:5312 [D loss: 0.405244, acc.: 80.47%] [G loss: 2.166461]\n",
      "epoch:6 step:5313 [D loss: 0.338729, acc.: 85.94%] [G loss: 2.774373]\n",
      "epoch:6 step:5314 [D loss: 0.317435, acc.: 85.16%] [G loss: 2.346627]\n",
      "epoch:6 step:5315 [D loss: 0.389977, acc.: 78.91%] [G loss: 1.798833]\n",
      "epoch:6 step:5316 [D loss: 0.331570, acc.: 87.50%] [G loss: 1.785614]\n",
      "epoch:6 step:5317 [D loss: 0.307273, acc.: 85.94%] [G loss: 2.343776]\n",
      "epoch:6 step:5318 [D loss: 0.378721, acc.: 81.25%] [G loss: 1.863923]\n",
      "epoch:6 step:5319 [D loss: 0.377020, acc.: 85.94%] [G loss: 2.233827]\n",
      "epoch:6 step:5320 [D loss: 0.313764, acc.: 88.28%] [G loss: 2.447445]\n",
      "epoch:6 step:5321 [D loss: 0.351310, acc.: 84.38%] [G loss: 2.330593]\n",
      "epoch:6 step:5322 [D loss: 0.257397, acc.: 91.41%] [G loss: 3.023482]\n",
      "epoch:6 step:5323 [D loss: 0.253977, acc.: 89.84%] [G loss: 3.008601]\n",
      "epoch:6 step:5324 [D loss: 0.315350, acc.: 88.28%] [G loss: 2.072724]\n",
      "epoch:6 step:5325 [D loss: 0.398822, acc.: 82.81%] [G loss: 2.578698]\n",
      "epoch:6 step:5326 [D loss: 0.440797, acc.: 75.78%] [G loss: 3.014464]\n",
      "epoch:6 step:5327 [D loss: 0.340562, acc.: 82.81%] [G loss: 4.746189]\n",
      "epoch:6 step:5328 [D loss: 0.308883, acc.: 84.38%] [G loss: 2.090445]\n",
      "epoch:6 step:5329 [D loss: 0.313503, acc.: 83.59%] [G loss: 3.490801]\n",
      "epoch:6 step:5330 [D loss: 0.427193, acc.: 75.78%] [G loss: 2.430260]\n",
      "epoch:6 step:5331 [D loss: 0.392273, acc.: 81.25%] [G loss: 2.130215]\n",
      "epoch:6 step:5332 [D loss: 0.386891, acc.: 80.47%] [G loss: 2.066412]\n",
      "epoch:6 step:5333 [D loss: 0.352111, acc.: 82.03%] [G loss: 2.704367]\n",
      "epoch:6 step:5334 [D loss: 0.269834, acc.: 92.19%] [G loss: 3.721920]\n",
      "epoch:6 step:5335 [D loss: 0.393353, acc.: 81.25%] [G loss: 2.328163]\n",
      "epoch:6 step:5336 [D loss: 0.316774, acc.: 89.06%] [G loss: 2.228453]\n",
      "epoch:6 step:5337 [D loss: 0.333196, acc.: 84.38%] [G loss: 3.226350]\n",
      "epoch:6 step:5338 [D loss: 0.267820, acc.: 89.84%] [G loss: 3.308491]\n",
      "epoch:6 step:5339 [D loss: 0.292854, acc.: 86.72%] [G loss: 2.814759]\n",
      "epoch:6 step:5340 [D loss: 0.350413, acc.: 85.94%] [G loss: 2.923916]\n",
      "epoch:6 step:5341 [D loss: 0.372271, acc.: 82.81%] [G loss: 2.139604]\n",
      "epoch:6 step:5342 [D loss: 0.382468, acc.: 79.69%] [G loss: 2.168855]\n",
      "epoch:6 step:5343 [D loss: 0.402866, acc.: 79.69%] [G loss: 2.801903]\n",
      "epoch:6 step:5344 [D loss: 0.458563, acc.: 73.44%] [G loss: 2.498892]\n",
      "epoch:6 step:5345 [D loss: 0.390740, acc.: 77.34%] [G loss: 2.516954]\n",
      "epoch:6 step:5346 [D loss: 0.312375, acc.: 85.94%] [G loss: 3.028197]\n",
      "epoch:6 step:5347 [D loss: 0.451540, acc.: 72.66%] [G loss: 2.324046]\n",
      "epoch:6 step:5348 [D loss: 0.338261, acc.: 81.25%] [G loss: 2.087737]\n",
      "epoch:6 step:5349 [D loss: 0.265132, acc.: 92.19%] [G loss: 2.577382]\n",
      "epoch:6 step:5350 [D loss: 0.224618, acc.: 92.97%] [G loss: 3.316683]\n",
      "epoch:6 step:5351 [D loss: 0.260797, acc.: 88.28%] [G loss: 2.922214]\n",
      "epoch:6 step:5352 [D loss: 0.324668, acc.: 85.16%] [G loss: 2.632215]\n",
      "epoch:6 step:5353 [D loss: 0.302025, acc.: 87.50%] [G loss: 2.653881]\n",
      "epoch:6 step:5354 [D loss: 0.322185, acc.: 83.59%] [G loss: 2.599079]\n",
      "epoch:6 step:5355 [D loss: 0.404428, acc.: 82.03%] [G loss: 2.473771]\n",
      "epoch:6 step:5356 [D loss: 0.316941, acc.: 85.16%] [G loss: 3.459664]\n",
      "epoch:6 step:5357 [D loss: 0.254560, acc.: 92.97%] [G loss: 3.442894]\n",
      "epoch:6 step:5358 [D loss: 0.254862, acc.: 90.62%] [G loss: 3.463743]\n",
      "epoch:6 step:5359 [D loss: 0.358900, acc.: 82.81%] [G loss: 2.115829]\n",
      "epoch:6 step:5360 [D loss: 0.295379, acc.: 89.84%] [G loss: 2.345031]\n",
      "epoch:6 step:5361 [D loss: 0.336999, acc.: 85.16%] [G loss: 3.031376]\n",
      "epoch:6 step:5362 [D loss: 0.333360, acc.: 89.84%] [G loss: 2.205907]\n",
      "epoch:6 step:5363 [D loss: 0.330717, acc.: 83.59%] [G loss: 2.748564]\n",
      "epoch:6 step:5364 [D loss: 0.328521, acc.: 88.28%] [G loss: 2.226145]\n",
      "epoch:6 step:5365 [D loss: 0.323062, acc.: 86.72%] [G loss: 2.232738]\n",
      "epoch:6 step:5366 [D loss: 0.307270, acc.: 90.62%] [G loss: 2.400888]\n",
      "epoch:6 step:5367 [D loss: 0.249782, acc.: 92.97%] [G loss: 3.049417]\n",
      "epoch:6 step:5368 [D loss: 0.398113, acc.: 78.91%] [G loss: 1.984492]\n",
      "epoch:6 step:5369 [D loss: 0.362864, acc.: 82.03%] [G loss: 2.673048]\n",
      "epoch:6 step:5370 [D loss: 0.462615, acc.: 75.78%] [G loss: 4.913492]\n",
      "epoch:6 step:5371 [D loss: 0.682473, acc.: 72.66%] [G loss: 4.263800]\n",
      "epoch:6 step:5372 [D loss: 0.864073, acc.: 64.06%] [G loss: 3.485438]\n",
      "epoch:6 step:5373 [D loss: 0.680620, acc.: 67.97%] [G loss: 2.270447]\n",
      "epoch:6 step:5374 [D loss: 0.372281, acc.: 82.81%] [G loss: 2.298447]\n",
      "epoch:6 step:5375 [D loss: 0.267258, acc.: 90.62%] [G loss: 2.063522]\n",
      "epoch:6 step:5376 [D loss: 0.436402, acc.: 80.47%] [G loss: 2.050884]\n",
      "epoch:6 step:5377 [D loss: 0.360024, acc.: 85.94%] [G loss: 2.157933]\n",
      "epoch:6 step:5378 [D loss: 0.384776, acc.: 79.69%] [G loss: 2.138191]\n",
      "epoch:6 step:5379 [D loss: 0.395032, acc.: 80.47%] [G loss: 1.850115]\n",
      "epoch:6 step:5380 [D loss: 0.300774, acc.: 89.84%] [G loss: 2.521150]\n",
      "epoch:6 step:5381 [D loss: 0.417837, acc.: 79.69%] [G loss: 3.182563]\n",
      "epoch:6 step:5382 [D loss: 0.323913, acc.: 85.94%] [G loss: 3.190012]\n",
      "epoch:6 step:5383 [D loss: 0.261384, acc.: 87.50%] [G loss: 3.927330]\n",
      "epoch:6 step:5384 [D loss: 0.295532, acc.: 87.50%] [G loss: 2.111875]\n",
      "epoch:6 step:5385 [D loss: 0.267933, acc.: 88.28%] [G loss: 2.824318]\n",
      "epoch:6 step:5386 [D loss: 0.302684, acc.: 87.50%] [G loss: 2.302416]\n",
      "epoch:6 step:5387 [D loss: 0.430669, acc.: 83.59%] [G loss: 1.778924]\n",
      "epoch:6 step:5388 [D loss: 0.414650, acc.: 82.03%] [G loss: 1.935593]\n",
      "epoch:6 step:5389 [D loss: 0.361906, acc.: 82.81%] [G loss: 3.245142]\n",
      "epoch:6 step:5390 [D loss: 0.312124, acc.: 84.38%] [G loss: 3.137984]\n",
      "epoch:6 step:5391 [D loss: 0.326607, acc.: 83.59%] [G loss: 1.993859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5392 [D loss: 0.343001, acc.: 82.81%] [G loss: 2.563026]\n",
      "epoch:6 step:5393 [D loss: 0.384505, acc.: 83.59%] [G loss: 2.106095]\n",
      "epoch:6 step:5394 [D loss: 0.314178, acc.: 85.16%] [G loss: 2.718031]\n",
      "epoch:6 step:5395 [D loss: 0.307040, acc.: 87.50%] [G loss: 2.738507]\n",
      "epoch:6 step:5396 [D loss: 0.287898, acc.: 86.72%] [G loss: 2.699679]\n",
      "epoch:6 step:5397 [D loss: 0.342737, acc.: 83.59%] [G loss: 1.878478]\n",
      "epoch:6 step:5398 [D loss: 0.363534, acc.: 81.25%] [G loss: 1.880879]\n",
      "epoch:6 step:5399 [D loss: 0.323443, acc.: 86.72%] [G loss: 2.208987]\n",
      "epoch:6 step:5400 [D loss: 0.353383, acc.: 85.16%] [G loss: 2.600535]\n",
      "epoch:6 step:5401 [D loss: 0.276354, acc.: 90.62%] [G loss: 4.217390]\n",
      "epoch:6 step:5402 [D loss: 0.250327, acc.: 90.62%] [G loss: 2.725254]\n",
      "epoch:6 step:5403 [D loss: 0.344107, acc.: 82.81%] [G loss: 2.770346]\n",
      "epoch:6 step:5404 [D loss: 0.410826, acc.: 80.47%] [G loss: 2.699208]\n",
      "epoch:6 step:5405 [D loss: 0.281877, acc.: 92.97%] [G loss: 3.099449]\n",
      "epoch:6 step:5406 [D loss: 0.356385, acc.: 85.94%] [G loss: 4.545891]\n",
      "epoch:6 step:5407 [D loss: 0.266056, acc.: 91.41%] [G loss: 6.771679]\n",
      "epoch:6 step:5408 [D loss: 0.425089, acc.: 77.34%] [G loss: 3.496994]\n",
      "epoch:6 step:5409 [D loss: 0.274173, acc.: 87.50%] [G loss: 4.884490]\n",
      "epoch:6 step:5410 [D loss: 0.360020, acc.: 77.34%] [G loss: 3.684951]\n",
      "epoch:6 step:5411 [D loss: 0.437554, acc.: 81.25%] [G loss: 3.555071]\n",
      "epoch:6 step:5412 [D loss: 0.262913, acc.: 86.72%] [G loss: 3.572793]\n",
      "epoch:6 step:5413 [D loss: 0.271797, acc.: 88.28%] [G loss: 6.578910]\n",
      "epoch:6 step:5414 [D loss: 0.276439, acc.: 92.97%] [G loss: 3.995191]\n",
      "epoch:6 step:5415 [D loss: 0.308656, acc.: 85.94%] [G loss: 2.869984]\n",
      "epoch:6 step:5416 [D loss: 0.370762, acc.: 85.16%] [G loss: 1.785215]\n",
      "epoch:6 step:5417 [D loss: 0.415334, acc.: 78.12%] [G loss: 1.881273]\n",
      "epoch:6 step:5418 [D loss: 0.502532, acc.: 70.31%] [G loss: 2.407648]\n",
      "epoch:6 step:5419 [D loss: 0.339446, acc.: 88.28%] [G loss: 1.953146]\n",
      "epoch:6 step:5420 [D loss: 0.362965, acc.: 84.38%] [G loss: 2.828244]\n",
      "epoch:6 step:5421 [D loss: 0.351180, acc.: 83.59%] [G loss: 3.976185]\n",
      "epoch:6 step:5422 [D loss: 0.415390, acc.: 77.34%] [G loss: 3.412380]\n",
      "epoch:6 step:5423 [D loss: 0.440830, acc.: 77.34%] [G loss: 3.854940]\n",
      "epoch:6 step:5424 [D loss: 0.505189, acc.: 78.12%] [G loss: 7.526170]\n",
      "epoch:6 step:5425 [D loss: 1.607368, acc.: 50.78%] [G loss: 4.628643]\n",
      "epoch:6 step:5426 [D loss: 1.748953, acc.: 64.06%] [G loss: 3.553441]\n",
      "epoch:6 step:5427 [D loss: 0.515386, acc.: 75.78%] [G loss: 3.732497]\n",
      "epoch:6 step:5428 [D loss: 0.421481, acc.: 76.56%] [G loss: 2.875291]\n",
      "epoch:6 step:5429 [D loss: 0.297102, acc.: 85.94%] [G loss: 5.717890]\n",
      "epoch:6 step:5430 [D loss: 0.376713, acc.: 79.69%] [G loss: 3.636855]\n",
      "epoch:6 step:5431 [D loss: 0.407267, acc.: 80.47%] [G loss: 2.115652]\n",
      "epoch:6 step:5432 [D loss: 0.317300, acc.: 83.59%] [G loss: 3.103213]\n",
      "epoch:6 step:5433 [D loss: 0.401045, acc.: 79.69%] [G loss: 2.395702]\n",
      "epoch:6 step:5434 [D loss: 0.356973, acc.: 86.72%] [G loss: 1.880224]\n",
      "epoch:6 step:5435 [D loss: 0.367849, acc.: 87.50%] [G loss: 2.800663]\n",
      "epoch:6 step:5436 [D loss: 0.251275, acc.: 92.19%] [G loss: 2.428558]\n",
      "epoch:6 step:5437 [D loss: 0.428848, acc.: 81.25%] [G loss: 2.415014]\n",
      "epoch:6 step:5438 [D loss: 0.413263, acc.: 79.69%] [G loss: 2.383288]\n",
      "epoch:6 step:5439 [D loss: 0.358611, acc.: 82.81%] [G loss: 2.622653]\n",
      "epoch:6 step:5440 [D loss: 0.349615, acc.: 82.03%] [G loss: 2.122392]\n",
      "epoch:6 step:5441 [D loss: 0.405590, acc.: 83.59%] [G loss: 2.564508]\n",
      "epoch:6 step:5442 [D loss: 0.345076, acc.: 85.16%] [G loss: 2.048473]\n",
      "epoch:6 step:5443 [D loss: 0.323095, acc.: 85.94%] [G loss: 1.921920]\n",
      "epoch:6 step:5444 [D loss: 0.353772, acc.: 86.72%] [G loss: 2.323604]\n",
      "epoch:6 step:5445 [D loss: 0.376680, acc.: 81.25%] [G loss: 2.378787]\n",
      "epoch:6 step:5446 [D loss: 0.345161, acc.: 86.72%] [G loss: 2.416171]\n",
      "epoch:6 step:5447 [D loss: 0.343268, acc.: 88.28%] [G loss: 2.548833]\n",
      "epoch:6 step:5448 [D loss: 0.436787, acc.: 78.91%] [G loss: 2.652531]\n",
      "epoch:6 step:5449 [D loss: 0.327753, acc.: 82.03%] [G loss: 3.675191]\n",
      "epoch:6 step:5450 [D loss: 0.285688, acc.: 87.50%] [G loss: 3.805364]\n",
      "epoch:6 step:5451 [D loss: 0.288237, acc.: 86.72%] [G loss: 3.838099]\n",
      "epoch:6 step:5452 [D loss: 0.385790, acc.: 77.34%] [G loss: 2.647217]\n",
      "epoch:6 step:5453 [D loss: 0.374979, acc.: 76.56%] [G loss: 1.868603]\n",
      "epoch:6 step:5454 [D loss: 0.302215, acc.: 89.06%] [G loss: 2.505300]\n",
      "epoch:6 step:5455 [D loss: 0.322451, acc.: 91.41%] [G loss: 2.358970]\n",
      "epoch:6 step:5456 [D loss: 0.352414, acc.: 86.72%] [G loss: 2.725373]\n",
      "epoch:6 step:5457 [D loss: 0.364176, acc.: 85.16%] [G loss: 2.117400]\n",
      "epoch:6 step:5458 [D loss: 0.303881, acc.: 88.28%] [G loss: 3.352345]\n",
      "epoch:6 step:5459 [D loss: 0.257553, acc.: 89.06%] [G loss: 3.905802]\n",
      "epoch:6 step:5460 [D loss: 0.295938, acc.: 90.62%] [G loss: 2.713940]\n",
      "epoch:6 step:5461 [D loss: 0.383807, acc.: 83.59%] [G loss: 2.690735]\n",
      "epoch:6 step:5462 [D loss: 0.274505, acc.: 89.06%] [G loss: 2.539001]\n",
      "epoch:6 step:5463 [D loss: 0.350769, acc.: 85.94%] [G loss: 3.143584]\n",
      "epoch:6 step:5464 [D loss: 0.292069, acc.: 87.50%] [G loss: 3.343795]\n",
      "epoch:6 step:5465 [D loss: 0.374240, acc.: 82.03%] [G loss: 3.972652]\n",
      "epoch:6 step:5466 [D loss: 0.356194, acc.: 82.81%] [G loss: 2.362927]\n",
      "epoch:6 step:5467 [D loss: 0.369590, acc.: 83.59%] [G loss: 2.405625]\n",
      "epoch:7 step:5468 [D loss: 0.411991, acc.: 82.03%] [G loss: 2.286213]\n",
      "epoch:7 step:5469 [D loss: 0.379871, acc.: 85.16%] [G loss: 2.085423]\n",
      "epoch:7 step:5470 [D loss: 0.306307, acc.: 85.16%] [G loss: 3.913612]\n",
      "epoch:7 step:5471 [D loss: 0.255724, acc.: 89.84%] [G loss: 2.837355]\n",
      "epoch:7 step:5472 [D loss: 0.380143, acc.: 80.47%] [G loss: 2.447059]\n",
      "epoch:7 step:5473 [D loss: 0.294938, acc.: 85.94%] [G loss: 2.576174]\n",
      "epoch:7 step:5474 [D loss: 0.345277, acc.: 89.84%] [G loss: 2.427855]\n",
      "epoch:7 step:5475 [D loss: 0.293344, acc.: 87.50%] [G loss: 3.321168]\n",
      "epoch:7 step:5476 [D loss: 0.368240, acc.: 81.25%] [G loss: 2.702181]\n",
      "epoch:7 step:5477 [D loss: 0.352634, acc.: 81.25%] [G loss: 2.512478]\n",
      "epoch:7 step:5478 [D loss: 0.333410, acc.: 87.50%] [G loss: 2.481594]\n",
      "epoch:7 step:5479 [D loss: 0.296607, acc.: 87.50%] [G loss: 3.167629]\n",
      "epoch:7 step:5480 [D loss: 0.473871, acc.: 74.22%] [G loss: 1.536834]\n",
      "epoch:7 step:5481 [D loss: 0.419092, acc.: 79.69%] [G loss: 2.737983]\n",
      "epoch:7 step:5482 [D loss: 0.379539, acc.: 82.03%] [G loss: 2.646196]\n",
      "epoch:7 step:5483 [D loss: 0.302003, acc.: 87.50%] [G loss: 2.866922]\n",
      "epoch:7 step:5484 [D loss: 0.356582, acc.: 83.59%] [G loss: 2.545705]\n",
      "epoch:7 step:5485 [D loss: 0.423224, acc.: 81.25%] [G loss: 1.984254]\n",
      "epoch:7 step:5486 [D loss: 0.323959, acc.: 87.50%] [G loss: 1.956222]\n",
      "epoch:7 step:5487 [D loss: 0.372482, acc.: 85.94%] [G loss: 1.671548]\n",
      "epoch:7 step:5488 [D loss: 0.404972, acc.: 83.59%] [G loss: 2.526976]\n",
      "epoch:7 step:5489 [D loss: 0.246384, acc.: 92.19%] [G loss: 3.881095]\n",
      "epoch:7 step:5490 [D loss: 0.351402, acc.: 82.81%] [G loss: 2.614616]\n",
      "epoch:7 step:5491 [D loss: 0.416179, acc.: 78.12%] [G loss: 3.104824]\n",
      "epoch:7 step:5492 [D loss: 0.300703, acc.: 85.94%] [G loss: 2.328217]\n",
      "epoch:7 step:5493 [D loss: 0.407377, acc.: 79.69%] [G loss: 2.515616]\n",
      "epoch:7 step:5494 [D loss: 0.331456, acc.: 82.03%] [G loss: 3.119755]\n",
      "epoch:7 step:5495 [D loss: 0.380481, acc.: 82.03%] [G loss: 2.988273]\n",
      "epoch:7 step:5496 [D loss: 0.498273, acc.: 80.47%] [G loss: 4.721045]\n",
      "epoch:7 step:5497 [D loss: 0.521179, acc.: 75.78%] [G loss: 2.535616]\n",
      "epoch:7 step:5498 [D loss: 0.343378, acc.: 80.47%] [G loss: 4.436101]\n",
      "epoch:7 step:5499 [D loss: 0.604849, acc.: 68.75%] [G loss: 3.993080]\n",
      "epoch:7 step:5500 [D loss: 0.344425, acc.: 82.03%] [G loss: 3.913385]\n",
      "epoch:7 step:5501 [D loss: 0.376471, acc.: 78.12%] [G loss: 4.704210]\n",
      "epoch:7 step:5502 [D loss: 0.519861, acc.: 78.12%] [G loss: 3.110846]\n",
      "epoch:7 step:5503 [D loss: 0.399785, acc.: 79.69%] [G loss: 2.640128]\n",
      "epoch:7 step:5504 [D loss: 0.445190, acc.: 77.34%] [G loss: 2.171336]\n",
      "epoch:7 step:5505 [D loss: 0.352259, acc.: 83.59%] [G loss: 3.957712]\n",
      "epoch:7 step:5506 [D loss: 0.340467, acc.: 85.16%] [G loss: 2.596002]\n",
      "epoch:7 step:5507 [D loss: 0.368498, acc.: 81.25%] [G loss: 2.245267]\n",
      "epoch:7 step:5508 [D loss: 0.344064, acc.: 82.81%] [G loss: 3.284845]\n",
      "epoch:7 step:5509 [D loss: 0.245001, acc.: 91.41%] [G loss: 4.687356]\n",
      "epoch:7 step:5510 [D loss: 0.357293, acc.: 82.03%] [G loss: 3.585649]\n",
      "epoch:7 step:5511 [D loss: 0.334723, acc.: 87.50%] [G loss: 2.657511]\n",
      "epoch:7 step:5512 [D loss: 0.325566, acc.: 89.84%] [G loss: 2.478246]\n",
      "epoch:7 step:5513 [D loss: 0.357199, acc.: 82.03%] [G loss: 2.131217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5514 [D loss: 0.307560, acc.: 89.84%] [G loss: 3.316377]\n",
      "epoch:7 step:5515 [D loss: 0.298044, acc.: 88.28%] [G loss: 2.292090]\n",
      "epoch:7 step:5516 [D loss: 0.310827, acc.: 88.28%] [G loss: 3.058955]\n",
      "epoch:7 step:5517 [D loss: 0.235959, acc.: 90.62%] [G loss: 3.740004]\n",
      "epoch:7 step:5518 [D loss: 0.277965, acc.: 87.50%] [G loss: 2.794196]\n",
      "epoch:7 step:5519 [D loss: 0.338510, acc.: 88.28%] [G loss: 2.109490]\n",
      "epoch:7 step:5520 [D loss: 0.261042, acc.: 92.97%] [G loss: 2.291852]\n",
      "epoch:7 step:5521 [D loss: 0.291113, acc.: 92.97%] [G loss: 2.850292]\n",
      "epoch:7 step:5522 [D loss: 0.251305, acc.: 89.84%] [G loss: 5.543143]\n",
      "epoch:7 step:5523 [D loss: 0.219409, acc.: 89.84%] [G loss: 5.020943]\n",
      "epoch:7 step:5524 [D loss: 0.343913, acc.: 85.16%] [G loss: 3.060720]\n",
      "epoch:7 step:5525 [D loss: 0.332061, acc.: 81.25%] [G loss: 2.990497]\n",
      "epoch:7 step:5526 [D loss: 0.269886, acc.: 92.19%] [G loss: 2.974723]\n",
      "epoch:7 step:5527 [D loss: 0.274008, acc.: 87.50%] [G loss: 3.527673]\n",
      "epoch:7 step:5528 [D loss: 0.431796, acc.: 78.91%] [G loss: 3.985831]\n",
      "epoch:7 step:5529 [D loss: 0.706659, acc.: 73.44%] [G loss: 3.622194]\n",
      "epoch:7 step:5530 [D loss: 0.606342, acc.: 67.97%] [G loss: 2.516244]\n",
      "epoch:7 step:5531 [D loss: 0.383579, acc.: 84.38%] [G loss: 2.251054]\n",
      "epoch:7 step:5532 [D loss: 0.364849, acc.: 84.38%] [G loss: 2.475627]\n",
      "epoch:7 step:5533 [D loss: 0.297630, acc.: 89.84%] [G loss: 3.023234]\n",
      "epoch:7 step:5534 [D loss: 0.375639, acc.: 84.38%] [G loss: 2.425469]\n",
      "epoch:7 step:5535 [D loss: 0.364924, acc.: 82.81%] [G loss: 2.258528]\n",
      "epoch:7 step:5536 [D loss: 0.268462, acc.: 92.97%] [G loss: 2.238923]\n",
      "epoch:7 step:5537 [D loss: 0.362523, acc.: 82.03%] [G loss: 2.242610]\n",
      "epoch:7 step:5538 [D loss: 0.322576, acc.: 86.72%] [G loss: 2.046881]\n",
      "epoch:7 step:5539 [D loss: 0.417555, acc.: 82.81%] [G loss: 2.657687]\n",
      "epoch:7 step:5540 [D loss: 0.339892, acc.: 85.94%] [G loss: 2.337392]\n",
      "epoch:7 step:5541 [D loss: 0.244090, acc.: 90.62%] [G loss: 3.030514]\n",
      "epoch:7 step:5542 [D loss: 0.307868, acc.: 89.06%] [G loss: 2.933147]\n",
      "epoch:7 step:5543 [D loss: 0.320760, acc.: 91.41%] [G loss: 2.440937]\n",
      "epoch:7 step:5544 [D loss: 0.389345, acc.: 78.12%] [G loss: 2.748130]\n",
      "epoch:7 step:5545 [D loss: 0.226086, acc.: 94.53%] [G loss: 3.148385]\n",
      "epoch:7 step:5546 [D loss: 0.419645, acc.: 77.34%] [G loss: 2.679422]\n",
      "epoch:7 step:5547 [D loss: 0.350722, acc.: 82.81%] [G loss: 3.222844]\n",
      "epoch:7 step:5548 [D loss: 0.302421, acc.: 89.84%] [G loss: 2.220558]\n",
      "epoch:7 step:5549 [D loss: 0.291864, acc.: 88.28%] [G loss: 2.473771]\n",
      "epoch:7 step:5550 [D loss: 0.362394, acc.: 83.59%] [G loss: 2.445711]\n",
      "epoch:7 step:5551 [D loss: 0.225921, acc.: 96.09%] [G loss: 3.138543]\n",
      "epoch:7 step:5552 [D loss: 0.267797, acc.: 88.28%] [G loss: 2.665679]\n",
      "epoch:7 step:5553 [D loss: 0.272791, acc.: 89.84%] [G loss: 3.035640]\n",
      "epoch:7 step:5554 [D loss: 0.282156, acc.: 90.62%] [G loss: 2.511861]\n",
      "epoch:7 step:5555 [D loss: 0.284406, acc.: 88.28%] [G loss: 2.415834]\n",
      "epoch:7 step:5556 [D loss: 0.279390, acc.: 89.06%] [G loss: 3.227947]\n",
      "epoch:7 step:5557 [D loss: 0.284228, acc.: 87.50%] [G loss: 3.269083]\n",
      "epoch:7 step:5558 [D loss: 0.283829, acc.: 86.72%] [G loss: 2.333112]\n",
      "epoch:7 step:5559 [D loss: 0.212529, acc.: 94.53%] [G loss: 3.797618]\n",
      "epoch:7 step:5560 [D loss: 0.250306, acc.: 89.84%] [G loss: 3.962687]\n",
      "epoch:7 step:5561 [D loss: 0.299463, acc.: 85.94%] [G loss: 2.529997]\n",
      "epoch:7 step:5562 [D loss: 0.387557, acc.: 85.16%] [G loss: 1.958124]\n",
      "epoch:7 step:5563 [D loss: 0.403857, acc.: 81.25%] [G loss: 2.391289]\n",
      "epoch:7 step:5564 [D loss: 0.326649, acc.: 88.28%] [G loss: 3.090174]\n",
      "epoch:7 step:5565 [D loss: 0.370792, acc.: 80.47%] [G loss: 4.370252]\n",
      "epoch:7 step:5566 [D loss: 0.556548, acc.: 74.22%] [G loss: 2.761153]\n",
      "epoch:7 step:5567 [D loss: 0.583601, acc.: 77.34%] [G loss: 7.550563]\n",
      "epoch:7 step:5568 [D loss: 2.785645, acc.: 51.56%] [G loss: 5.400063]\n",
      "epoch:7 step:5569 [D loss: 3.002329, acc.: 39.84%] [G loss: 1.702923]\n",
      "epoch:7 step:5570 [D loss: 0.529510, acc.: 75.78%] [G loss: 3.596738]\n",
      "epoch:7 step:5571 [D loss: 0.339034, acc.: 81.25%] [G loss: 2.876236]\n",
      "epoch:7 step:5572 [D loss: 0.553590, acc.: 70.31%] [G loss: 2.019449]\n",
      "epoch:7 step:5573 [D loss: 0.386953, acc.: 78.12%] [G loss: 3.282981]\n",
      "epoch:7 step:5574 [D loss: 0.323936, acc.: 82.03%] [G loss: 2.448816]\n",
      "epoch:7 step:5575 [D loss: 0.410981, acc.: 78.91%] [G loss: 2.064444]\n",
      "epoch:7 step:5576 [D loss: 0.497292, acc.: 77.34%] [G loss: 2.140358]\n",
      "epoch:7 step:5577 [D loss: 0.366476, acc.: 83.59%] [G loss: 2.225340]\n",
      "epoch:7 step:5578 [D loss: 0.481690, acc.: 77.34%] [G loss: 2.173321]\n",
      "epoch:7 step:5579 [D loss: 0.233118, acc.: 92.97%] [G loss: 2.418148]\n",
      "epoch:7 step:5580 [D loss: 0.359395, acc.: 82.03%] [G loss: 1.967254]\n",
      "epoch:7 step:5581 [D loss: 0.302100, acc.: 89.06%] [G loss: 2.043464]\n",
      "epoch:7 step:5582 [D loss: 0.277568, acc.: 91.41%] [G loss: 2.684691]\n",
      "epoch:7 step:5583 [D loss: 0.251912, acc.: 93.75%] [G loss: 2.223063]\n",
      "epoch:7 step:5584 [D loss: 0.306330, acc.: 86.72%] [G loss: 2.183873]\n",
      "epoch:7 step:5585 [D loss: 0.344514, acc.: 84.38%] [G loss: 2.958262]\n",
      "epoch:7 step:5586 [D loss: 0.289429, acc.: 87.50%] [G loss: 2.773513]\n",
      "epoch:7 step:5587 [D loss: 0.284801, acc.: 91.41%] [G loss: 2.414204]\n",
      "epoch:7 step:5588 [D loss: 0.368264, acc.: 85.16%] [G loss: 2.202883]\n",
      "epoch:7 step:5589 [D loss: 0.330800, acc.: 89.84%] [G loss: 2.339569]\n",
      "epoch:7 step:5590 [D loss: 0.345010, acc.: 85.16%] [G loss: 2.184261]\n",
      "epoch:7 step:5591 [D loss: 0.203827, acc.: 92.97%] [G loss: 2.616888]\n",
      "epoch:7 step:5592 [D loss: 0.394203, acc.: 85.16%] [G loss: 1.948982]\n",
      "epoch:7 step:5593 [D loss: 0.245042, acc.: 89.84%] [G loss: 2.653628]\n",
      "epoch:7 step:5594 [D loss: 0.306166, acc.: 91.41%] [G loss: 2.275210]\n",
      "epoch:7 step:5595 [D loss: 0.293384, acc.: 91.41%] [G loss: 2.729326]\n",
      "epoch:7 step:5596 [D loss: 0.321477, acc.: 87.50%] [G loss: 2.272017]\n",
      "epoch:7 step:5597 [D loss: 0.429325, acc.: 79.69%] [G loss: 3.076876]\n",
      "epoch:7 step:5598 [D loss: 0.409034, acc.: 78.12%] [G loss: 2.312735]\n",
      "epoch:7 step:5599 [D loss: 0.228562, acc.: 94.53%] [G loss: 2.642089]\n",
      "epoch:7 step:5600 [D loss: 0.289597, acc.: 89.06%] [G loss: 2.551708]\n",
      "epoch:7 step:5601 [D loss: 0.306919, acc.: 88.28%] [G loss: 1.965904]\n",
      "epoch:7 step:5602 [D loss: 0.304161, acc.: 89.84%] [G loss: 2.636239]\n",
      "epoch:7 step:5603 [D loss: 0.296828, acc.: 90.62%] [G loss: 2.343390]\n",
      "epoch:7 step:5604 [D loss: 0.270098, acc.: 90.62%] [G loss: 2.115323]\n",
      "epoch:7 step:5605 [D loss: 0.402303, acc.: 80.47%] [G loss: 2.292465]\n",
      "epoch:7 step:5606 [D loss: 0.283726, acc.: 92.97%] [G loss: 2.655201]\n",
      "epoch:7 step:5607 [D loss: 0.313004, acc.: 89.06%] [G loss: 2.268175]\n",
      "epoch:7 step:5608 [D loss: 0.362070, acc.: 85.94%] [G loss: 2.134769]\n",
      "epoch:7 step:5609 [D loss: 0.291726, acc.: 89.84%] [G loss: 2.118922]\n",
      "epoch:7 step:5610 [D loss: 0.309941, acc.: 85.94%] [G loss: 2.369658]\n",
      "epoch:7 step:5611 [D loss: 0.291078, acc.: 87.50%] [G loss: 3.393157]\n",
      "epoch:7 step:5612 [D loss: 0.381236, acc.: 85.94%] [G loss: 2.026959]\n",
      "epoch:7 step:5613 [D loss: 0.225088, acc.: 91.41%] [G loss: 3.562350]\n",
      "epoch:7 step:5614 [D loss: 0.225729, acc.: 92.19%] [G loss: 3.623654]\n",
      "epoch:7 step:5615 [D loss: 0.322004, acc.: 85.94%] [G loss: 3.170939]\n",
      "epoch:7 step:5616 [D loss: 0.385730, acc.: 84.38%] [G loss: 3.361039]\n",
      "epoch:7 step:5617 [D loss: 0.367179, acc.: 84.38%] [G loss: 2.330122]\n",
      "epoch:7 step:5618 [D loss: 0.265320, acc.: 92.19%] [G loss: 2.563188]\n",
      "epoch:7 step:5619 [D loss: 0.265386, acc.: 92.19%] [G loss: 2.053206]\n",
      "epoch:7 step:5620 [D loss: 0.326347, acc.: 86.72%] [G loss: 2.613250]\n",
      "epoch:7 step:5621 [D loss: 0.292742, acc.: 90.62%] [G loss: 2.283686]\n",
      "epoch:7 step:5622 [D loss: 0.355708, acc.: 82.81%] [G loss: 1.822371]\n",
      "epoch:7 step:5623 [D loss: 0.309729, acc.: 92.97%] [G loss: 1.668027]\n",
      "epoch:7 step:5624 [D loss: 0.356110, acc.: 84.38%] [G loss: 2.314694]\n",
      "epoch:7 step:5625 [D loss: 0.394490, acc.: 82.03%] [G loss: 1.534939]\n",
      "epoch:7 step:5626 [D loss: 0.336171, acc.: 86.72%] [G loss: 2.399029]\n",
      "epoch:7 step:5627 [D loss: 0.387829, acc.: 86.72%] [G loss: 2.226465]\n",
      "epoch:7 step:5628 [D loss: 0.417425, acc.: 82.81%] [G loss: 1.841214]\n",
      "epoch:7 step:5629 [D loss: 0.273702, acc.: 91.41%] [G loss: 2.279836]\n",
      "epoch:7 step:5630 [D loss: 0.224202, acc.: 92.97%] [G loss: 3.004710]\n",
      "epoch:7 step:5631 [D loss: 0.222666, acc.: 93.75%] [G loss: 3.455619]\n",
      "epoch:7 step:5632 [D loss: 0.219500, acc.: 95.31%] [G loss: 2.333805]\n",
      "epoch:7 step:5633 [D loss: 0.319124, acc.: 85.94%] [G loss: 2.557473]\n",
      "epoch:7 step:5634 [D loss: 0.296616, acc.: 89.06%] [G loss: 2.177917]\n",
      "epoch:7 step:5635 [D loss: 0.343159, acc.: 85.94%] [G loss: 2.288659]\n",
      "epoch:7 step:5636 [D loss: 0.303113, acc.: 87.50%] [G loss: 2.340538]\n",
      "epoch:7 step:5637 [D loss: 0.232782, acc.: 95.31%] [G loss: 3.533030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5638 [D loss: 0.262704, acc.: 89.84%] [G loss: 2.706307]\n",
      "epoch:7 step:5639 [D loss: 0.244587, acc.: 92.97%] [G loss: 2.194568]\n",
      "epoch:7 step:5640 [D loss: 0.315729, acc.: 92.97%] [G loss: 2.121334]\n",
      "epoch:7 step:5641 [D loss: 0.409793, acc.: 82.03%] [G loss: 2.756029]\n",
      "epoch:7 step:5642 [D loss: 0.288377, acc.: 92.97%] [G loss: 3.338402]\n",
      "epoch:7 step:5643 [D loss: 0.337244, acc.: 89.84%] [G loss: 2.129771]\n",
      "epoch:7 step:5644 [D loss: 0.337940, acc.: 87.50%] [G loss: 2.497862]\n",
      "epoch:7 step:5645 [D loss: 0.355954, acc.: 83.59%] [G loss: 2.727492]\n",
      "epoch:7 step:5646 [D loss: 0.352097, acc.: 87.50%] [G loss: 2.586272]\n",
      "epoch:7 step:5647 [D loss: 0.306326, acc.: 84.38%] [G loss: 3.760143]\n",
      "epoch:7 step:5648 [D loss: 0.223976, acc.: 90.62%] [G loss: 3.261400]\n",
      "epoch:7 step:5649 [D loss: 0.366328, acc.: 79.69%] [G loss: 2.721407]\n",
      "epoch:7 step:5650 [D loss: 0.258530, acc.: 89.06%] [G loss: 3.658530]\n",
      "epoch:7 step:5651 [D loss: 0.258073, acc.: 89.06%] [G loss: 3.458973]\n",
      "epoch:7 step:5652 [D loss: 0.323068, acc.: 85.16%] [G loss: 2.063628]\n",
      "epoch:7 step:5653 [D loss: 0.334022, acc.: 83.59%] [G loss: 2.603601]\n",
      "epoch:7 step:5654 [D loss: 0.293047, acc.: 85.16%] [G loss: 3.275612]\n",
      "epoch:7 step:5655 [D loss: 0.181438, acc.: 95.31%] [G loss: 4.252162]\n",
      "epoch:7 step:5656 [D loss: 0.302754, acc.: 85.94%] [G loss: 2.492281]\n",
      "epoch:7 step:5657 [D loss: 0.329307, acc.: 83.59%] [G loss: 3.115923]\n",
      "epoch:7 step:5658 [D loss: 0.257106, acc.: 86.72%] [G loss: 4.007547]\n",
      "epoch:7 step:5659 [D loss: 0.279533, acc.: 87.50%] [G loss: 3.070422]\n",
      "epoch:7 step:5660 [D loss: 0.478126, acc.: 76.56%] [G loss: 3.753814]\n",
      "epoch:7 step:5661 [D loss: 0.530614, acc.: 75.78%] [G loss: 4.689345]\n",
      "epoch:7 step:5662 [D loss: 0.573856, acc.: 77.34%] [G loss: 3.772346]\n",
      "epoch:7 step:5663 [D loss: 0.389643, acc.: 78.12%] [G loss: 2.340409]\n",
      "epoch:7 step:5664 [D loss: 0.563753, acc.: 78.12%] [G loss: 2.496114]\n",
      "epoch:7 step:5665 [D loss: 0.372748, acc.: 80.47%] [G loss: 2.745810]\n",
      "epoch:7 step:5666 [D loss: 0.393937, acc.: 79.69%] [G loss: 2.979487]\n",
      "epoch:7 step:5667 [D loss: 0.397933, acc.: 80.47%] [G loss: 2.168705]\n",
      "epoch:7 step:5668 [D loss: 0.407169, acc.: 80.47%] [G loss: 3.194504]\n",
      "epoch:7 step:5669 [D loss: 0.282976, acc.: 92.97%] [G loss: 2.809180]\n",
      "epoch:7 step:5670 [D loss: 0.274199, acc.: 91.41%] [G loss: 3.529899]\n",
      "epoch:7 step:5671 [D loss: 0.332805, acc.: 83.59%] [G loss: 2.884497]\n",
      "epoch:7 step:5672 [D loss: 0.261795, acc.: 88.28%] [G loss: 3.626096]\n",
      "epoch:7 step:5673 [D loss: 0.240738, acc.: 91.41%] [G loss: 2.728089]\n",
      "epoch:7 step:5674 [D loss: 0.359609, acc.: 82.81%] [G loss: 2.386943]\n",
      "epoch:7 step:5675 [D loss: 0.402364, acc.: 79.69%] [G loss: 1.963018]\n",
      "epoch:7 step:5676 [D loss: 0.257604, acc.: 91.41%] [G loss: 2.449614]\n",
      "epoch:7 step:5677 [D loss: 0.306367, acc.: 89.84%] [G loss: 2.714941]\n",
      "epoch:7 step:5678 [D loss: 0.244318, acc.: 88.28%] [G loss: 3.694712]\n",
      "epoch:7 step:5679 [D loss: 0.404219, acc.: 82.81%] [G loss: 3.305219]\n",
      "epoch:7 step:5680 [D loss: 0.572598, acc.: 73.44%] [G loss: 2.433948]\n",
      "epoch:7 step:5681 [D loss: 0.434420, acc.: 81.25%] [G loss: 2.822333]\n",
      "epoch:7 step:5682 [D loss: 0.387299, acc.: 83.59%] [G loss: 2.350075]\n",
      "epoch:7 step:5683 [D loss: 0.352726, acc.: 85.94%] [G loss: 2.429864]\n",
      "epoch:7 step:5684 [D loss: 0.296572, acc.: 89.84%] [G loss: 2.855820]\n",
      "epoch:7 step:5685 [D loss: 0.329754, acc.: 84.38%] [G loss: 2.219759]\n",
      "epoch:7 step:5686 [D loss: 0.325272, acc.: 85.16%] [G loss: 2.984426]\n",
      "epoch:7 step:5687 [D loss: 0.206562, acc.: 92.19%] [G loss: 5.071660]\n",
      "epoch:7 step:5688 [D loss: 0.224988, acc.: 92.97%] [G loss: 3.455481]\n",
      "epoch:7 step:5689 [D loss: 0.339118, acc.: 85.16%] [G loss: 2.956906]\n",
      "epoch:7 step:5690 [D loss: 0.174898, acc.: 92.19%] [G loss: 4.486790]\n",
      "epoch:7 step:5691 [D loss: 0.337194, acc.: 82.03%] [G loss: 2.838464]\n",
      "epoch:7 step:5692 [D loss: 0.337220, acc.: 81.25%] [G loss: 2.926840]\n",
      "epoch:7 step:5693 [D loss: 0.390279, acc.: 79.69%] [G loss: 1.845827]\n",
      "epoch:7 step:5694 [D loss: 0.288435, acc.: 89.06%] [G loss: 2.375805]\n",
      "epoch:7 step:5695 [D loss: 0.336286, acc.: 89.84%] [G loss: 2.042574]\n",
      "epoch:7 step:5696 [D loss: 0.364759, acc.: 87.50%] [G loss: 3.338546]\n",
      "epoch:7 step:5697 [D loss: 0.300851, acc.: 90.62%] [G loss: 3.474288]\n",
      "epoch:7 step:5698 [D loss: 0.201324, acc.: 94.53%] [G loss: 5.248341]\n",
      "epoch:7 step:5699 [D loss: 0.260643, acc.: 91.41%] [G loss: 2.792122]\n",
      "epoch:7 step:5700 [D loss: 0.273891, acc.: 87.50%] [G loss: 3.348589]\n",
      "epoch:7 step:5701 [D loss: 0.278717, acc.: 86.72%] [G loss: 3.327974]\n",
      "epoch:7 step:5702 [D loss: 0.273050, acc.: 88.28%] [G loss: 5.045870]\n",
      "epoch:7 step:5703 [D loss: 0.384840, acc.: 78.91%] [G loss: 2.902880]\n",
      "epoch:7 step:5704 [D loss: 0.300171, acc.: 85.16%] [G loss: 6.477673]\n",
      "epoch:7 step:5705 [D loss: 0.395890, acc.: 77.34%] [G loss: 2.696708]\n",
      "epoch:7 step:5706 [D loss: 0.255522, acc.: 89.06%] [G loss: 4.622490]\n",
      "epoch:7 step:5707 [D loss: 0.239419, acc.: 92.19%] [G loss: 3.990895]\n",
      "epoch:7 step:5708 [D loss: 0.338521, acc.: 86.72%] [G loss: 3.235774]\n",
      "epoch:7 step:5709 [D loss: 0.311008, acc.: 85.16%] [G loss: 2.985847]\n",
      "epoch:7 step:5710 [D loss: 0.402110, acc.: 78.91%] [G loss: 3.238476]\n",
      "epoch:7 step:5711 [D loss: 0.395098, acc.: 83.59%] [G loss: 2.578769]\n",
      "epoch:7 step:5712 [D loss: 0.284428, acc.: 90.62%] [G loss: 2.747828]\n",
      "epoch:7 step:5713 [D loss: 0.382968, acc.: 81.25%] [G loss: 3.880859]\n",
      "epoch:7 step:5714 [D loss: 0.235272, acc.: 92.19%] [G loss: 3.914995]\n",
      "epoch:7 step:5715 [D loss: 0.321038, acc.: 82.81%] [G loss: 2.803730]\n",
      "epoch:7 step:5716 [D loss: 0.353203, acc.: 85.16%] [G loss: 3.564039]\n",
      "epoch:7 step:5717 [D loss: 0.377420, acc.: 81.25%] [G loss: 2.721807]\n",
      "epoch:7 step:5718 [D loss: 0.295764, acc.: 86.72%] [G loss: 3.363107]\n",
      "epoch:7 step:5719 [D loss: 0.363315, acc.: 84.38%] [G loss: 2.907859]\n",
      "epoch:7 step:5720 [D loss: 0.262851, acc.: 88.28%] [G loss: 3.178326]\n",
      "epoch:7 step:5721 [D loss: 0.261044, acc.: 92.97%] [G loss: 2.100720]\n",
      "epoch:7 step:5722 [D loss: 0.411396, acc.: 79.69%] [G loss: 4.108425]\n",
      "epoch:7 step:5723 [D loss: 0.396329, acc.: 78.91%] [G loss: 3.377405]\n",
      "epoch:7 step:5724 [D loss: 0.439077, acc.: 77.34%] [G loss: 3.799129]\n",
      "epoch:7 step:5725 [D loss: 0.227709, acc.: 92.19%] [G loss: 3.695988]\n",
      "epoch:7 step:5726 [D loss: 0.250691, acc.: 89.84%] [G loss: 2.779121]\n",
      "epoch:7 step:5727 [D loss: 0.353496, acc.: 82.81%] [G loss: 2.390103]\n",
      "epoch:7 step:5728 [D loss: 0.316749, acc.: 85.94%] [G loss: 2.755043]\n",
      "epoch:7 step:5729 [D loss: 0.283019, acc.: 90.62%] [G loss: 3.490419]\n",
      "epoch:7 step:5730 [D loss: 0.199978, acc.: 91.41%] [G loss: 4.672705]\n",
      "epoch:7 step:5731 [D loss: 0.207777, acc.: 92.19%] [G loss: 3.429514]\n",
      "epoch:7 step:5732 [D loss: 0.221107, acc.: 94.53%] [G loss: 3.626020]\n",
      "epoch:7 step:5733 [D loss: 0.253118, acc.: 89.06%] [G loss: 3.249714]\n",
      "epoch:7 step:5734 [D loss: 0.400523, acc.: 85.16%] [G loss: 2.817561]\n",
      "epoch:7 step:5735 [D loss: 0.339080, acc.: 80.47%] [G loss: 2.130636]\n",
      "epoch:7 step:5736 [D loss: 0.326774, acc.: 86.72%] [G loss: 2.744687]\n",
      "epoch:7 step:5737 [D loss: 0.272412, acc.: 89.84%] [G loss: 2.875775]\n",
      "epoch:7 step:5738 [D loss: 0.262765, acc.: 88.28%] [G loss: 3.133632]\n",
      "epoch:7 step:5739 [D loss: 0.289788, acc.: 89.84%] [G loss: 3.125023]\n",
      "epoch:7 step:5740 [D loss: 0.349371, acc.: 80.47%] [G loss: 2.291137]\n",
      "epoch:7 step:5741 [D loss: 0.347801, acc.: 84.38%] [G loss: 4.767235]\n",
      "epoch:7 step:5742 [D loss: 0.368263, acc.: 82.03%] [G loss: 4.785936]\n",
      "epoch:7 step:5743 [D loss: 0.971250, acc.: 55.47%] [G loss: 1.793960]\n",
      "epoch:7 step:5744 [D loss: 0.544417, acc.: 75.78%] [G loss: 4.201384]\n",
      "epoch:7 step:5745 [D loss: 1.242542, acc.: 54.69%] [G loss: 5.801591]\n",
      "epoch:7 step:5746 [D loss: 1.962806, acc.: 50.78%] [G loss: 2.032947]\n",
      "epoch:7 step:5747 [D loss: 0.442616, acc.: 83.59%] [G loss: 3.144439]\n",
      "epoch:7 step:5748 [D loss: 0.813884, acc.: 71.09%] [G loss: 5.705913]\n",
      "epoch:7 step:5749 [D loss: 0.539237, acc.: 77.34%] [G loss: 3.891611]\n",
      "epoch:7 step:5750 [D loss: 0.408125, acc.: 79.69%] [G loss: 3.728466]\n",
      "epoch:7 step:5751 [D loss: 0.361655, acc.: 91.41%] [G loss: 3.942480]\n",
      "epoch:7 step:5752 [D loss: 0.357944, acc.: 81.25%] [G loss: 4.237949]\n",
      "epoch:7 step:5753 [D loss: 0.427698, acc.: 75.00%] [G loss: 3.663762]\n",
      "epoch:7 step:5754 [D loss: 0.404839, acc.: 81.25%] [G loss: 2.597271]\n",
      "epoch:7 step:5755 [D loss: 0.256058, acc.: 90.62%] [G loss: 2.855944]\n",
      "epoch:7 step:5756 [D loss: 0.232174, acc.: 92.19%] [G loss: 3.827383]\n",
      "epoch:7 step:5757 [D loss: 0.293646, acc.: 83.59%] [G loss: 3.842648]\n",
      "epoch:7 step:5758 [D loss: 0.217232, acc.: 89.06%] [G loss: 5.027170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5759 [D loss: 0.319088, acc.: 85.94%] [G loss: 2.261575]\n",
      "epoch:7 step:5760 [D loss: 0.355583, acc.: 84.38%] [G loss: 1.887330]\n",
      "epoch:7 step:5761 [D loss: 0.279717, acc.: 90.62%] [G loss: 2.273162]\n",
      "epoch:7 step:5762 [D loss: 0.364231, acc.: 83.59%] [G loss: 2.997119]\n",
      "epoch:7 step:5763 [D loss: 0.287719, acc.: 88.28%] [G loss: 2.103967]\n",
      "epoch:7 step:5764 [D loss: 0.283925, acc.: 92.19%] [G loss: 4.141717]\n",
      "epoch:7 step:5765 [D loss: 0.236827, acc.: 91.41%] [G loss: 4.741450]\n",
      "epoch:7 step:5766 [D loss: 0.304562, acc.: 85.94%] [G loss: 2.175200]\n",
      "epoch:7 step:5767 [D loss: 0.283745, acc.: 83.59%] [G loss: 4.955942]\n",
      "epoch:7 step:5768 [D loss: 0.298505, acc.: 91.41%] [G loss: 3.975559]\n",
      "epoch:7 step:5769 [D loss: 0.197983, acc.: 93.75%] [G loss: 3.296952]\n",
      "epoch:7 step:5770 [D loss: 0.229837, acc.: 91.41%] [G loss: 4.018785]\n",
      "epoch:7 step:5771 [D loss: 0.351088, acc.: 88.28%] [G loss: 2.366187]\n",
      "epoch:7 step:5772 [D loss: 0.321238, acc.: 87.50%] [G loss: 2.669649]\n",
      "epoch:7 step:5773 [D loss: 0.333415, acc.: 85.16%] [G loss: 2.243151]\n",
      "epoch:7 step:5774 [D loss: 0.419885, acc.: 79.69%] [G loss: 1.660025]\n",
      "epoch:7 step:5775 [D loss: 0.427958, acc.: 78.91%] [G loss: 2.364438]\n",
      "epoch:7 step:5776 [D loss: 0.325356, acc.: 84.38%] [G loss: 2.701840]\n",
      "epoch:7 step:5777 [D loss: 0.211826, acc.: 91.41%] [G loss: 4.941256]\n",
      "epoch:7 step:5778 [D loss: 0.240275, acc.: 91.41%] [G loss: 2.790372]\n",
      "epoch:7 step:5779 [D loss: 0.377296, acc.: 82.03%] [G loss: 3.352663]\n",
      "epoch:7 step:5780 [D loss: 0.341803, acc.: 85.94%] [G loss: 3.158880]\n",
      "epoch:7 step:5781 [D loss: 0.265592, acc.: 90.62%] [G loss: 3.785135]\n",
      "epoch:7 step:5782 [D loss: 0.490018, acc.: 77.34%] [G loss: 2.885304]\n",
      "epoch:7 step:5783 [D loss: 0.282076, acc.: 89.06%] [G loss: 2.942729]\n",
      "epoch:7 step:5784 [D loss: 0.302938, acc.: 88.28%] [G loss: 2.423688]\n",
      "epoch:7 step:5785 [D loss: 0.283460, acc.: 89.06%] [G loss: 4.460329]\n",
      "epoch:7 step:5786 [D loss: 0.144666, acc.: 93.75%] [G loss: 4.094917]\n",
      "epoch:7 step:5787 [D loss: 0.336438, acc.: 86.72%] [G loss: 2.021665]\n",
      "epoch:7 step:5788 [D loss: 0.238804, acc.: 89.06%] [G loss: 4.538630]\n",
      "epoch:7 step:5789 [D loss: 0.296776, acc.: 91.41%] [G loss: 3.521462]\n",
      "epoch:7 step:5790 [D loss: 0.397992, acc.: 78.91%] [G loss: 2.457623]\n",
      "epoch:7 step:5791 [D loss: 0.343584, acc.: 85.16%] [G loss: 4.327904]\n",
      "epoch:7 step:5792 [D loss: 0.590312, acc.: 66.41%] [G loss: 2.294885]\n",
      "epoch:7 step:5793 [D loss: 0.375663, acc.: 79.69%] [G loss: 1.814901]\n",
      "epoch:7 step:5794 [D loss: 0.429191, acc.: 84.38%] [G loss: 3.161619]\n",
      "epoch:7 step:5795 [D loss: 0.446859, acc.: 80.47%] [G loss: 2.477619]\n",
      "epoch:7 step:5796 [D loss: 0.283564, acc.: 91.41%] [G loss: 3.371900]\n",
      "epoch:7 step:5797 [D loss: 0.362396, acc.: 85.94%] [G loss: 3.981587]\n",
      "epoch:7 step:5798 [D loss: 0.677204, acc.: 71.88%] [G loss: 3.081821]\n",
      "epoch:7 step:5799 [D loss: 0.594528, acc.: 72.66%] [G loss: 2.353422]\n",
      "epoch:7 step:5800 [D loss: 0.386416, acc.: 82.03%] [G loss: 2.631654]\n",
      "epoch:7 step:5801 [D loss: 0.360132, acc.: 88.28%] [G loss: 2.657709]\n",
      "epoch:7 step:5802 [D loss: 0.321689, acc.: 89.06%] [G loss: 2.800634]\n",
      "epoch:7 step:5803 [D loss: 0.269588, acc.: 92.19%] [G loss: 3.056179]\n",
      "epoch:7 step:5804 [D loss: 0.316002, acc.: 87.50%] [G loss: 2.705910]\n",
      "epoch:7 step:5805 [D loss: 0.319085, acc.: 86.72%] [G loss: 3.207216]\n",
      "epoch:7 step:5806 [D loss: 0.294210, acc.: 88.28%] [G loss: 2.475917]\n",
      "epoch:7 step:5807 [D loss: 0.287376, acc.: 89.06%] [G loss: 2.243578]\n",
      "epoch:7 step:5808 [D loss: 0.310160, acc.: 87.50%] [G loss: 2.295159]\n",
      "epoch:7 step:5809 [D loss: 0.327314, acc.: 87.50%] [G loss: 2.188802]\n",
      "epoch:7 step:5810 [D loss: 0.328237, acc.: 85.16%] [G loss: 2.387025]\n",
      "epoch:7 step:5811 [D loss: 0.306621, acc.: 91.41%] [G loss: 1.660085]\n",
      "epoch:7 step:5812 [D loss: 0.245217, acc.: 91.41%] [G loss: 3.499903]\n",
      "epoch:7 step:5813 [D loss: 0.269249, acc.: 90.62%] [G loss: 3.537930]\n",
      "epoch:7 step:5814 [D loss: 0.242973, acc.: 89.06%] [G loss: 2.366566]\n",
      "epoch:7 step:5815 [D loss: 0.415650, acc.: 78.12%] [G loss: 2.175207]\n",
      "epoch:7 step:5816 [D loss: 0.403878, acc.: 81.25%] [G loss: 3.339549]\n",
      "epoch:7 step:5817 [D loss: 0.440802, acc.: 78.91%] [G loss: 2.749934]\n",
      "epoch:7 step:5818 [D loss: 0.323704, acc.: 89.84%] [G loss: 4.483746]\n",
      "epoch:7 step:5819 [D loss: 0.370634, acc.: 81.25%] [G loss: 4.679997]\n",
      "epoch:7 step:5820 [D loss: 0.229898, acc.: 91.41%] [G loss: 3.831472]\n",
      "epoch:7 step:5821 [D loss: 0.227866, acc.: 89.06%] [G loss: 5.340453]\n",
      "epoch:7 step:5822 [D loss: 0.271360, acc.: 83.59%] [G loss: 4.525412]\n",
      "epoch:7 step:5823 [D loss: 0.273518, acc.: 85.16%] [G loss: 3.012870]\n",
      "epoch:7 step:5824 [D loss: 0.295899, acc.: 88.28%] [G loss: 2.306086]\n",
      "epoch:7 step:5825 [D loss: 0.230409, acc.: 89.84%] [G loss: 4.187697]\n",
      "epoch:7 step:5826 [D loss: 0.249927, acc.: 91.41%] [G loss: 2.416178]\n",
      "epoch:7 step:5827 [D loss: 0.387275, acc.: 78.91%] [G loss: 4.214478]\n",
      "epoch:7 step:5828 [D loss: 0.445561, acc.: 77.34%] [G loss: 2.543688]\n",
      "epoch:7 step:5829 [D loss: 0.382349, acc.: 82.03%] [G loss: 3.672188]\n",
      "epoch:7 step:5830 [D loss: 0.475882, acc.: 75.00%] [G loss: 2.489496]\n",
      "epoch:7 step:5831 [D loss: 0.414652, acc.: 79.69%] [G loss: 2.351113]\n",
      "epoch:7 step:5832 [D loss: 0.409223, acc.: 84.38%] [G loss: 2.863969]\n",
      "epoch:7 step:5833 [D loss: 0.309149, acc.: 87.50%] [G loss: 2.630669]\n",
      "epoch:7 step:5834 [D loss: 0.305701, acc.: 89.84%] [G loss: 5.429610]\n",
      "epoch:7 step:5835 [D loss: 0.358484, acc.: 82.03%] [G loss: 5.216350]\n",
      "epoch:7 step:5836 [D loss: 0.397586, acc.: 80.47%] [G loss: 2.649803]\n",
      "epoch:7 step:5837 [D loss: 0.329139, acc.: 85.16%] [G loss: 3.114940]\n",
      "epoch:7 step:5838 [D loss: 0.201254, acc.: 90.62%] [G loss: 4.443296]\n",
      "epoch:7 step:5839 [D loss: 0.219301, acc.: 92.19%] [G loss: 4.024741]\n",
      "epoch:7 step:5840 [D loss: 0.363504, acc.: 86.72%] [G loss: 2.284826]\n",
      "epoch:7 step:5841 [D loss: 0.383131, acc.: 82.03%] [G loss: 2.560903]\n",
      "epoch:7 step:5842 [D loss: 0.278366, acc.: 92.19%] [G loss: 2.029811]\n",
      "epoch:7 step:5843 [D loss: 0.378045, acc.: 85.94%] [G loss: 2.009589]\n",
      "epoch:7 step:5844 [D loss: 0.323474, acc.: 86.72%] [G loss: 1.868080]\n",
      "epoch:7 step:5845 [D loss: 0.345254, acc.: 87.50%] [G loss: 2.204964]\n",
      "epoch:7 step:5846 [D loss: 0.219221, acc.: 93.75%] [G loss: 2.894614]\n",
      "epoch:7 step:5847 [D loss: 0.255344, acc.: 89.06%] [G loss: 2.744658]\n",
      "epoch:7 step:5848 [D loss: 0.323009, acc.: 86.72%] [G loss: 2.767938]\n",
      "epoch:7 step:5849 [D loss: 0.406622, acc.: 80.47%] [G loss: 2.303370]\n",
      "epoch:7 step:5850 [D loss: 0.349826, acc.: 85.16%] [G loss: 3.392081]\n",
      "epoch:7 step:5851 [D loss: 0.291818, acc.: 83.59%] [G loss: 4.419557]\n",
      "epoch:7 step:5852 [D loss: 0.135481, acc.: 97.66%] [G loss: 5.335558]\n",
      "epoch:7 step:5853 [D loss: 0.351361, acc.: 81.25%] [G loss: 2.768524]\n",
      "epoch:7 step:5854 [D loss: 0.216910, acc.: 92.97%] [G loss: 3.262649]\n",
      "epoch:7 step:5855 [D loss: 0.374878, acc.: 84.38%] [G loss: 2.393991]\n",
      "epoch:7 step:5856 [D loss: 0.330426, acc.: 89.06%] [G loss: 2.799823]\n",
      "epoch:7 step:5857 [D loss: 0.327836, acc.: 86.72%] [G loss: 2.254626]\n",
      "epoch:7 step:5858 [D loss: 0.347518, acc.: 86.72%] [G loss: 3.110119]\n",
      "epoch:7 step:5859 [D loss: 0.301731, acc.: 88.28%] [G loss: 3.098754]\n",
      "epoch:7 step:5860 [D loss: 0.345842, acc.: 84.38%] [G loss: 2.137958]\n",
      "epoch:7 step:5861 [D loss: 0.303414, acc.: 87.50%] [G loss: 2.994544]\n",
      "epoch:7 step:5862 [D loss: 0.354528, acc.: 87.50%] [G loss: 2.434711]\n",
      "epoch:7 step:5863 [D loss: 0.443188, acc.: 78.12%] [G loss: 2.192546]\n",
      "epoch:7 step:5864 [D loss: 0.417482, acc.: 85.16%] [G loss: 3.128221]\n",
      "epoch:7 step:5865 [D loss: 0.411704, acc.: 80.47%] [G loss: 4.445323]\n",
      "epoch:7 step:5866 [D loss: 0.530509, acc.: 82.03%] [G loss: 4.971107]\n",
      "epoch:7 step:5867 [D loss: 0.672268, acc.: 66.41%] [G loss: 2.347772]\n",
      "epoch:7 step:5868 [D loss: 0.351535, acc.: 81.25%] [G loss: 3.805744]\n",
      "epoch:7 step:5869 [D loss: 0.318063, acc.: 90.62%] [G loss: 2.724298]\n",
      "epoch:7 step:5870 [D loss: 0.286036, acc.: 90.62%] [G loss: 2.818126]\n",
      "epoch:7 step:5871 [D loss: 0.339895, acc.: 85.16%] [G loss: 2.116975]\n",
      "epoch:7 step:5872 [D loss: 0.391307, acc.: 78.91%] [G loss: 1.916310]\n",
      "epoch:7 step:5873 [D loss: 0.283149, acc.: 87.50%] [G loss: 2.728545]\n",
      "epoch:7 step:5874 [D loss: 0.323665, acc.: 84.38%] [G loss: 3.491940]\n",
      "epoch:7 step:5875 [D loss: 0.441219, acc.: 80.47%] [G loss: 1.960849]\n",
      "epoch:7 step:5876 [D loss: 0.254135, acc.: 92.19%] [G loss: 2.939080]\n",
      "epoch:7 step:5877 [D loss: 0.321099, acc.: 83.59%] [G loss: 3.345702]\n",
      "epoch:7 step:5878 [D loss: 0.476462, acc.: 78.91%] [G loss: 2.985953]\n",
      "epoch:7 step:5879 [D loss: 0.531565, acc.: 73.44%] [G loss: 5.666866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5880 [D loss: 0.522107, acc.: 73.44%] [G loss: 2.554941]\n",
      "epoch:7 step:5881 [D loss: 0.241888, acc.: 88.28%] [G loss: 4.238115]\n",
      "epoch:7 step:5882 [D loss: 0.247766, acc.: 90.62%] [G loss: 5.001326]\n",
      "epoch:7 step:5883 [D loss: 0.319691, acc.: 83.59%] [G loss: 3.162084]\n",
      "epoch:7 step:5884 [D loss: 0.329068, acc.: 84.38%] [G loss: 2.689407]\n",
      "epoch:7 step:5885 [D loss: 0.325319, acc.: 88.28%] [G loss: 2.829916]\n",
      "epoch:7 step:5886 [D loss: 0.406494, acc.: 81.25%] [G loss: 2.050857]\n",
      "epoch:7 step:5887 [D loss: 0.325844, acc.: 89.84%] [G loss: 2.386803]\n",
      "epoch:7 step:5888 [D loss: 0.296665, acc.: 86.72%] [G loss: 3.433057]\n",
      "epoch:7 step:5889 [D loss: 0.254154, acc.: 90.62%] [G loss: 3.949776]\n",
      "epoch:7 step:5890 [D loss: 0.306427, acc.: 89.84%] [G loss: 2.934722]\n",
      "epoch:7 step:5891 [D loss: 0.280801, acc.: 86.72%] [G loss: 2.634047]\n",
      "epoch:7 step:5892 [D loss: 0.372281, acc.: 79.69%] [G loss: 2.601964]\n",
      "epoch:7 step:5893 [D loss: 0.322963, acc.: 89.06%] [G loss: 2.437160]\n",
      "epoch:7 step:5894 [D loss: 0.235686, acc.: 93.75%] [G loss: 2.531274]\n",
      "epoch:7 step:5895 [D loss: 0.409644, acc.: 82.03%] [G loss: 3.333467]\n",
      "epoch:7 step:5896 [D loss: 0.381573, acc.: 83.59%] [G loss: 3.055307]\n",
      "epoch:7 step:5897 [D loss: 0.632569, acc.: 63.28%] [G loss: 2.583834]\n",
      "epoch:7 step:5898 [D loss: 0.471199, acc.: 82.03%] [G loss: 1.961992]\n",
      "epoch:7 step:5899 [D loss: 0.415239, acc.: 85.94%] [G loss: 3.028357]\n",
      "epoch:7 step:5900 [D loss: 0.395264, acc.: 86.72%] [G loss: 2.275818]\n",
      "epoch:7 step:5901 [D loss: 0.235875, acc.: 92.19%] [G loss: 3.719810]\n",
      "epoch:7 step:5902 [D loss: 0.206191, acc.: 92.19%] [G loss: 4.688624]\n",
      "epoch:7 step:5903 [D loss: 0.286437, acc.: 85.94%] [G loss: 3.010660]\n",
      "epoch:7 step:5904 [D loss: 0.245237, acc.: 91.41%] [G loss: 3.574549]\n",
      "epoch:7 step:5905 [D loss: 0.331334, acc.: 85.94%] [G loss: 2.675103]\n",
      "epoch:7 step:5906 [D loss: 0.234962, acc.: 90.62%] [G loss: 2.825809]\n",
      "epoch:7 step:5907 [D loss: 0.321927, acc.: 88.28%] [G loss: 2.796864]\n",
      "epoch:7 step:5908 [D loss: 0.392604, acc.: 85.16%] [G loss: 2.333342]\n",
      "epoch:7 step:5909 [D loss: 0.387462, acc.: 86.72%] [G loss: 2.283423]\n",
      "epoch:7 step:5910 [D loss: 0.328062, acc.: 87.50%] [G loss: 3.361748]\n",
      "epoch:7 step:5911 [D loss: 0.255250, acc.: 90.62%] [G loss: 4.163069]\n",
      "epoch:7 step:5912 [D loss: 0.292081, acc.: 87.50%] [G loss: 2.664232]\n",
      "epoch:7 step:5913 [D loss: 0.251726, acc.: 88.28%] [G loss: 3.105603]\n",
      "epoch:7 step:5914 [D loss: 0.201626, acc.: 90.62%] [G loss: 5.085346]\n",
      "epoch:7 step:5915 [D loss: 0.439279, acc.: 77.34%] [G loss: 2.787746]\n",
      "epoch:7 step:5916 [D loss: 0.364358, acc.: 82.81%] [G loss: 2.976897]\n",
      "epoch:7 step:5917 [D loss: 0.317713, acc.: 88.28%] [G loss: 2.783882]\n",
      "epoch:7 step:5918 [D loss: 0.403211, acc.: 83.59%] [G loss: 2.948476]\n",
      "epoch:7 step:5919 [D loss: 0.353763, acc.: 84.38%] [G loss: 3.101194]\n",
      "epoch:7 step:5920 [D loss: 0.282184, acc.: 92.19%] [G loss: 2.260164]\n",
      "epoch:7 step:5921 [D loss: 0.235542, acc.: 92.97%] [G loss: 3.169749]\n",
      "epoch:7 step:5922 [D loss: 0.431794, acc.: 80.47%] [G loss: 2.404664]\n",
      "epoch:7 step:5923 [D loss: 0.332279, acc.: 84.38%] [G loss: 2.021064]\n",
      "epoch:7 step:5924 [D loss: 0.307671, acc.: 91.41%] [G loss: 2.514845]\n",
      "epoch:7 step:5925 [D loss: 0.233251, acc.: 90.62%] [G loss: 3.511139]\n",
      "epoch:7 step:5926 [D loss: 0.274885, acc.: 85.94%] [G loss: 4.856799]\n",
      "epoch:7 step:5927 [D loss: 0.358819, acc.: 84.38%] [G loss: 1.991123]\n",
      "epoch:7 step:5928 [D loss: 0.178100, acc.: 95.31%] [G loss: 3.159355]\n",
      "epoch:7 step:5929 [D loss: 0.301443, acc.: 90.62%] [G loss: 2.617970]\n",
      "epoch:7 step:5930 [D loss: 0.340570, acc.: 87.50%] [G loss: 1.993322]\n",
      "epoch:7 step:5931 [D loss: 0.280405, acc.: 92.97%] [G loss: 3.054114]\n",
      "epoch:7 step:5932 [D loss: 0.303477, acc.: 87.50%] [G loss: 3.436900]\n",
      "epoch:7 step:5933 [D loss: 0.217367, acc.: 93.75%] [G loss: 3.515575]\n",
      "epoch:7 step:5934 [D loss: 0.257023, acc.: 92.19%] [G loss: 2.238084]\n",
      "epoch:7 step:5935 [D loss: 0.304311, acc.: 85.94%] [G loss: 3.226212]\n",
      "epoch:7 step:5936 [D loss: 0.153757, acc.: 93.75%] [G loss: 5.178718]\n",
      "epoch:7 step:5937 [D loss: 0.321658, acc.: 85.16%] [G loss: 3.033534]\n",
      "epoch:7 step:5938 [D loss: 0.271300, acc.: 86.72%] [G loss: 3.553512]\n",
      "epoch:7 step:5939 [D loss: 0.262589, acc.: 88.28%] [G loss: 3.477066]\n",
      "epoch:7 step:5940 [D loss: 0.300755, acc.: 87.50%] [G loss: 3.031375]\n",
      "epoch:7 step:5941 [D loss: 0.208330, acc.: 92.97%] [G loss: 3.823818]\n",
      "epoch:7 step:5942 [D loss: 0.331406, acc.: 85.16%] [G loss: 2.378453]\n",
      "epoch:7 step:5943 [D loss: 0.354310, acc.: 82.81%] [G loss: 2.022725]\n",
      "epoch:7 step:5944 [D loss: 0.321257, acc.: 89.06%] [G loss: 2.423574]\n",
      "epoch:7 step:5945 [D loss: 0.327315, acc.: 87.50%] [G loss: 2.159765]\n",
      "epoch:7 step:5946 [D loss: 0.301944, acc.: 91.41%] [G loss: 2.632252]\n",
      "epoch:7 step:5947 [D loss: 0.190238, acc.: 91.41%] [G loss: 4.798857]\n",
      "epoch:7 step:5948 [D loss: 0.292958, acc.: 85.94%] [G loss: 3.102067]\n",
      "epoch:7 step:5949 [D loss: 0.424205, acc.: 77.34%] [G loss: 2.248170]\n",
      "epoch:7 step:5950 [D loss: 0.325596, acc.: 85.94%] [G loss: 2.236771]\n",
      "epoch:7 step:5951 [D loss: 0.218273, acc.: 91.41%] [G loss: 3.162137]\n",
      "epoch:7 step:5952 [D loss: 0.272216, acc.: 88.28%] [G loss: 3.215130]\n",
      "epoch:7 step:5953 [D loss: 0.255566, acc.: 93.75%] [G loss: 2.085781]\n",
      "epoch:7 step:5954 [D loss: 0.289993, acc.: 87.50%] [G loss: 3.771629]\n",
      "epoch:7 step:5955 [D loss: 0.465727, acc.: 82.03%] [G loss: 3.131006]\n",
      "epoch:7 step:5956 [D loss: 0.495693, acc.: 85.16%] [G loss: 3.159062]\n",
      "epoch:7 step:5957 [D loss: 0.381000, acc.: 79.69%] [G loss: 3.644469]\n",
      "epoch:7 step:5958 [D loss: 0.384190, acc.: 82.03%] [G loss: 2.576488]\n",
      "epoch:7 step:5959 [D loss: 0.285497, acc.: 90.62%] [G loss: 3.576669]\n",
      "epoch:7 step:5960 [D loss: 0.231111, acc.: 89.84%] [G loss: 5.232770]\n",
      "epoch:7 step:5961 [D loss: 0.334756, acc.: 84.38%] [G loss: 4.035083]\n",
      "epoch:7 step:5962 [D loss: 0.260566, acc.: 92.19%] [G loss: 2.508576]\n",
      "epoch:7 step:5963 [D loss: 0.325212, acc.: 82.81%] [G loss: 2.847400]\n",
      "epoch:7 step:5964 [D loss: 0.396114, acc.: 83.59%] [G loss: 2.472504]\n",
      "epoch:7 step:5965 [D loss: 0.312771, acc.: 90.62%] [G loss: 2.754028]\n",
      "epoch:7 step:5966 [D loss: 0.336130, acc.: 84.38%] [G loss: 2.626455]\n",
      "epoch:7 step:5967 [D loss: 0.296401, acc.: 86.72%] [G loss: 3.435153]\n",
      "epoch:7 step:5968 [D loss: 0.250178, acc.: 89.06%] [G loss: 4.700199]\n",
      "epoch:7 step:5969 [D loss: 0.304441, acc.: 90.62%] [G loss: 2.902497]\n",
      "epoch:7 step:5970 [D loss: 0.384157, acc.: 79.69%] [G loss: 3.678546]\n",
      "epoch:7 step:5971 [D loss: 0.264564, acc.: 86.72%] [G loss: 4.123679]\n",
      "epoch:7 step:5972 [D loss: 0.289993, acc.: 89.06%] [G loss: 2.625739]\n",
      "epoch:7 step:5973 [D loss: 0.239029, acc.: 90.62%] [G loss: 6.031901]\n",
      "epoch:7 step:5974 [D loss: 0.303505, acc.: 88.28%] [G loss: 2.428842]\n",
      "epoch:7 step:5975 [D loss: 0.409546, acc.: 80.47%] [G loss: 2.111556]\n",
      "epoch:7 step:5976 [D loss: 0.300882, acc.: 87.50%] [G loss: 2.647082]\n",
      "epoch:7 step:5977 [D loss: 0.260268, acc.: 89.06%] [G loss: 2.288178]\n",
      "epoch:7 step:5978 [D loss: 0.323087, acc.: 84.38%] [G loss: 2.693913]\n",
      "epoch:7 step:5979 [D loss: 0.210557, acc.: 92.19%] [G loss: 2.889638]\n",
      "epoch:7 step:5980 [D loss: 0.259095, acc.: 89.06%] [G loss: 4.354176]\n",
      "epoch:7 step:5981 [D loss: 0.270642, acc.: 88.28%] [G loss: 3.084210]\n",
      "epoch:7 step:5982 [D loss: 0.272781, acc.: 90.62%] [G loss: 2.661699]\n",
      "epoch:7 step:5983 [D loss: 0.326297, acc.: 84.38%] [G loss: 1.851225]\n",
      "epoch:7 step:5984 [D loss: 0.389793, acc.: 85.94%] [G loss: 2.650187]\n",
      "epoch:7 step:5985 [D loss: 0.245864, acc.: 93.75%] [G loss: 2.941748]\n",
      "epoch:7 step:5986 [D loss: 0.273741, acc.: 89.84%] [G loss: 3.419708]\n",
      "epoch:7 step:5987 [D loss: 0.345881, acc.: 83.59%] [G loss: 4.045660]\n",
      "epoch:7 step:5988 [D loss: 0.287551, acc.: 91.41%] [G loss: 2.584541]\n",
      "epoch:7 step:5989 [D loss: 0.321572, acc.: 86.72%] [G loss: 2.595060]\n",
      "epoch:7 step:5990 [D loss: 0.347302, acc.: 82.03%] [G loss: 2.371272]\n",
      "epoch:7 step:5991 [D loss: 0.333575, acc.: 82.81%] [G loss: 2.168628]\n",
      "epoch:7 step:5992 [D loss: 0.279932, acc.: 89.06%] [G loss: 2.880622]\n",
      "epoch:7 step:5993 [D loss: 0.387839, acc.: 84.38%] [G loss: 2.277595]\n",
      "epoch:7 step:5994 [D loss: 0.338765, acc.: 84.38%] [G loss: 2.923735]\n",
      "epoch:7 step:5995 [D loss: 0.253293, acc.: 92.19%] [G loss: 3.412095]\n",
      "epoch:7 step:5996 [D loss: 0.288190, acc.: 87.50%] [G loss: 2.521040]\n",
      "epoch:7 step:5997 [D loss: 0.367103, acc.: 85.16%] [G loss: 3.777097]\n",
      "epoch:7 step:5998 [D loss: 0.562549, acc.: 76.56%] [G loss: 6.726346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5999 [D loss: 1.472865, acc.: 53.12%] [G loss: 6.755479]\n",
      "epoch:7 step:6000 [D loss: 2.128981, acc.: 53.12%] [G loss: 2.547837]\n",
      "epoch:7 step:6001 [D loss: 1.930112, acc.: 46.09%] [G loss: 7.509242]\n",
      "epoch:7 step:6002 [D loss: 1.226286, acc.: 69.53%] [G loss: 2.496745]\n",
      "epoch:7 step:6003 [D loss: 0.850401, acc.: 70.31%] [G loss: 1.856708]\n",
      "epoch:7 step:6004 [D loss: 0.574997, acc.: 71.88%] [G loss: 2.819446]\n",
      "epoch:7 step:6005 [D loss: 0.400435, acc.: 82.03%] [G loss: 2.885170]\n",
      "epoch:7 step:6006 [D loss: 0.399418, acc.: 84.38%] [G loss: 1.988626]\n",
      "epoch:7 step:6007 [D loss: 0.334105, acc.: 85.94%] [G loss: 2.077143]\n",
      "epoch:7 step:6008 [D loss: 0.452267, acc.: 75.78%] [G loss: 1.519181]\n",
      "epoch:7 step:6009 [D loss: 0.450767, acc.: 79.69%] [G loss: 1.874463]\n",
      "epoch:7 step:6010 [D loss: 0.496145, acc.: 80.47%] [G loss: 2.126285]\n",
      "epoch:7 step:6011 [D loss: 0.369169, acc.: 86.72%] [G loss: 2.611775]\n",
      "epoch:7 step:6012 [D loss: 0.378823, acc.: 84.38%] [G loss: 2.185730]\n",
      "epoch:7 step:6013 [D loss: 0.219289, acc.: 91.41%] [G loss: 2.693324]\n",
      "epoch:7 step:6014 [D loss: 0.301128, acc.: 90.62%] [G loss: 2.245796]\n",
      "epoch:7 step:6015 [D loss: 0.313656, acc.: 86.72%] [G loss: 2.024423]\n",
      "epoch:7 step:6016 [D loss: 0.414581, acc.: 82.03%] [G loss: 1.832654]\n",
      "epoch:7 step:6017 [D loss: 0.492408, acc.: 79.69%] [G loss: 2.193271]\n",
      "epoch:7 step:6018 [D loss: 0.457788, acc.: 77.34%] [G loss: 2.440773]\n",
      "epoch:7 step:6019 [D loss: 0.357166, acc.: 84.38%] [G loss: 3.935493]\n",
      "epoch:7 step:6020 [D loss: 0.351351, acc.: 82.81%] [G loss: 3.759893]\n",
      "epoch:7 step:6021 [D loss: 0.352019, acc.: 82.81%] [G loss: 2.243862]\n",
      "epoch:7 step:6022 [D loss: 0.303606, acc.: 90.62%] [G loss: 2.592450]\n",
      "epoch:7 step:6023 [D loss: 0.331765, acc.: 88.28%] [G loss: 2.343152]\n",
      "epoch:7 step:6024 [D loss: 0.372466, acc.: 82.03%] [G loss: 1.859508]\n",
      "epoch:7 step:6025 [D loss: 0.432345, acc.: 76.56%] [G loss: 1.823247]\n",
      "epoch:7 step:6026 [D loss: 0.299395, acc.: 90.62%] [G loss: 2.404839]\n",
      "epoch:7 step:6027 [D loss: 0.309996, acc.: 87.50%] [G loss: 3.622931]\n",
      "epoch:7 step:6028 [D loss: 0.362902, acc.: 87.50%] [G loss: 3.023529]\n",
      "epoch:7 step:6029 [D loss: 0.332830, acc.: 89.84%] [G loss: 2.013752]\n",
      "epoch:7 step:6030 [D loss: 0.386757, acc.: 81.25%] [G loss: 1.636972]\n",
      "epoch:7 step:6031 [D loss: 0.425949, acc.: 84.38%] [G loss: 2.389037]\n",
      "epoch:7 step:6032 [D loss: 0.285598, acc.: 82.03%] [G loss: 4.415203]\n",
      "epoch:7 step:6033 [D loss: 0.356773, acc.: 84.38%] [G loss: 2.964771]\n",
      "epoch:7 step:6034 [D loss: 0.511206, acc.: 71.88%] [G loss: 1.806086]\n",
      "epoch:7 step:6035 [D loss: 0.321975, acc.: 89.84%] [G loss: 2.414701]\n",
      "epoch:7 step:6036 [D loss: 0.309460, acc.: 88.28%] [G loss: 2.074717]\n",
      "epoch:7 step:6037 [D loss: 0.252191, acc.: 92.97%] [G loss: 3.175252]\n",
      "epoch:7 step:6038 [D loss: 0.245829, acc.: 91.41%] [G loss: 3.871555]\n",
      "epoch:7 step:6039 [D loss: 0.280013, acc.: 88.28%] [G loss: 2.551812]\n",
      "epoch:7 step:6040 [D loss: 0.360818, acc.: 81.25%] [G loss: 2.983444]\n",
      "epoch:7 step:6041 [D loss: 0.353283, acc.: 84.38%] [G loss: 3.417255]\n",
      "epoch:7 step:6042 [D loss: 0.314474, acc.: 86.72%] [G loss: 2.483723]\n",
      "epoch:7 step:6043 [D loss: 0.403920, acc.: 78.91%] [G loss: 2.469424]\n",
      "epoch:7 step:6044 [D loss: 0.355740, acc.: 85.16%] [G loss: 2.064193]\n",
      "epoch:7 step:6045 [D loss: 0.268742, acc.: 85.16%] [G loss: 4.431022]\n",
      "epoch:7 step:6046 [D loss: 0.242705, acc.: 86.72%] [G loss: 3.658569]\n",
      "epoch:7 step:6047 [D loss: 0.441098, acc.: 78.12%] [G loss: 2.392059]\n",
      "epoch:7 step:6048 [D loss: 0.391322, acc.: 81.25%] [G loss: 2.061917]\n",
      "epoch:7 step:6049 [D loss: 0.289392, acc.: 92.19%] [G loss: 3.058582]\n",
      "epoch:7 step:6050 [D loss: 0.449357, acc.: 82.03%] [G loss: 2.804275]\n",
      "epoch:7 step:6051 [D loss: 0.412586, acc.: 82.81%] [G loss: 2.448733]\n",
      "epoch:7 step:6052 [D loss: 0.318295, acc.: 86.72%] [G loss: 2.064284]\n",
      "epoch:7 step:6053 [D loss: 0.389021, acc.: 84.38%] [G loss: 2.903883]\n",
      "epoch:7 step:6054 [D loss: 0.283039, acc.: 86.72%] [G loss: 4.439637]\n",
      "epoch:7 step:6055 [D loss: 0.212876, acc.: 93.75%] [G loss: 3.659888]\n",
      "epoch:7 step:6056 [D loss: 0.410480, acc.: 82.81%] [G loss: 2.465181]\n",
      "epoch:7 step:6057 [D loss: 0.387267, acc.: 80.47%] [G loss: 2.088356]\n",
      "epoch:7 step:6058 [D loss: 0.329611, acc.: 86.72%] [G loss: 2.091380]\n",
      "epoch:7 step:6059 [D loss: 0.431404, acc.: 82.03%] [G loss: 3.045675]\n",
      "epoch:7 step:6060 [D loss: 0.322277, acc.: 84.38%] [G loss: 4.778372]\n",
      "epoch:7 step:6061 [D loss: 0.308422, acc.: 86.72%] [G loss: 3.461465]\n",
      "epoch:7 step:6062 [D loss: 0.425849, acc.: 75.78%] [G loss: 3.398879]\n",
      "epoch:7 step:6063 [D loss: 0.381257, acc.: 82.81%] [G loss: 3.161776]\n",
      "epoch:7 step:6064 [D loss: 0.307773, acc.: 89.06%] [G loss: 2.625303]\n",
      "epoch:7 step:6065 [D loss: 0.332006, acc.: 86.72%] [G loss: 2.399302]\n",
      "epoch:7 step:6066 [D loss: 0.294869, acc.: 92.19%] [G loss: 2.528513]\n",
      "epoch:7 step:6067 [D loss: 0.264065, acc.: 88.28%] [G loss: 2.548120]\n",
      "epoch:7 step:6068 [D loss: 0.260823, acc.: 88.28%] [G loss: 2.922206]\n",
      "epoch:7 step:6069 [D loss: 0.290104, acc.: 92.19%] [G loss: 2.271754]\n",
      "epoch:7 step:6070 [D loss: 0.268886, acc.: 89.84%] [G loss: 2.833910]\n",
      "epoch:7 step:6071 [D loss: 0.326846, acc.: 84.38%] [G loss: 3.029235]\n",
      "epoch:7 step:6072 [D loss: 0.394716, acc.: 84.38%] [G loss: 3.378254]\n",
      "epoch:7 step:6073 [D loss: 0.386622, acc.: 82.81%] [G loss: 2.857634]\n",
      "epoch:7 step:6074 [D loss: 0.261641, acc.: 89.84%] [G loss: 2.849365]\n",
      "epoch:7 step:6075 [D loss: 0.309429, acc.: 88.28%] [G loss: 2.434737]\n",
      "epoch:7 step:6076 [D loss: 0.277682, acc.: 92.19%] [G loss: 2.661136]\n",
      "epoch:7 step:6077 [D loss: 0.230237, acc.: 89.84%] [G loss: 4.410060]\n",
      "epoch:7 step:6078 [D loss: 0.207871, acc.: 95.31%] [G loss: 4.906990]\n",
      "epoch:7 step:6079 [D loss: 0.309264, acc.: 89.84%] [G loss: 1.995894]\n",
      "epoch:7 step:6080 [D loss: 0.292881, acc.: 86.72%] [G loss: 2.897223]\n",
      "epoch:7 step:6081 [D loss: 0.204306, acc.: 92.97%] [G loss: 3.429347]\n",
      "epoch:7 step:6082 [D loss: 0.235099, acc.: 94.53%] [G loss: 2.107756]\n",
      "epoch:7 step:6083 [D loss: 0.289397, acc.: 87.50%] [G loss: 2.235385]\n",
      "epoch:7 step:6084 [D loss: 0.362179, acc.: 85.94%] [G loss: 2.597527]\n",
      "epoch:7 step:6085 [D loss: 0.379427, acc.: 83.59%] [G loss: 2.818242]\n",
      "epoch:7 step:6086 [D loss: 0.351767, acc.: 85.94%] [G loss: 2.020690]\n",
      "epoch:7 step:6087 [D loss: 0.287416, acc.: 90.62%] [G loss: 2.418282]\n",
      "epoch:7 step:6088 [D loss: 0.360710, acc.: 86.72%] [G loss: 1.950377]\n",
      "epoch:7 step:6089 [D loss: 0.340122, acc.: 89.06%] [G loss: 1.946572]\n",
      "epoch:7 step:6090 [D loss: 0.382352, acc.: 82.81%] [G loss: 2.151289]\n",
      "epoch:7 step:6091 [D loss: 0.310234, acc.: 89.84%] [G loss: 1.864250]\n",
      "epoch:7 step:6092 [D loss: 0.281795, acc.: 86.72%] [G loss: 4.430585]\n",
      "epoch:7 step:6093 [D loss: 0.216883, acc.: 96.09%] [G loss: 3.859840]\n",
      "epoch:7 step:6094 [D loss: 0.237568, acc.: 90.62%] [G loss: 3.809449]\n",
      "epoch:7 step:6095 [D loss: 0.371216, acc.: 84.38%] [G loss: 3.323469]\n",
      "epoch:7 step:6096 [D loss: 0.345273, acc.: 85.16%] [G loss: 2.968005]\n",
      "epoch:7 step:6097 [D loss: 0.267422, acc.: 90.62%] [G loss: 4.389750]\n",
      "epoch:7 step:6098 [D loss: 0.299727, acc.: 85.16%] [G loss: 5.338043]\n",
      "epoch:7 step:6099 [D loss: 0.215995, acc.: 88.28%] [G loss: 3.500953]\n",
      "epoch:7 step:6100 [D loss: 0.314054, acc.: 83.59%] [G loss: 3.583940]\n",
      "epoch:7 step:6101 [D loss: 0.380125, acc.: 79.69%] [G loss: 2.241059]\n",
      "epoch:7 step:6102 [D loss: 0.298030, acc.: 88.28%] [G loss: 2.806879]\n",
      "epoch:7 step:6103 [D loss: 0.396804, acc.: 84.38%] [G loss: 2.275796]\n",
      "epoch:7 step:6104 [D loss: 0.262504, acc.: 94.53%] [G loss: 2.753488]\n",
      "epoch:7 step:6105 [D loss: 0.308108, acc.: 87.50%] [G loss: 3.875902]\n",
      "epoch:7 step:6106 [D loss: 0.349749, acc.: 81.25%] [G loss: 3.251681]\n",
      "epoch:7 step:6107 [D loss: 0.341299, acc.: 83.59%] [G loss: 2.475955]\n",
      "epoch:7 step:6108 [D loss: 0.269475, acc.: 87.50%] [G loss: 6.621480]\n",
      "epoch:7 step:6109 [D loss: 0.335826, acc.: 82.81%] [G loss: 2.906904]\n",
      "epoch:7 step:6110 [D loss: 0.325436, acc.: 82.03%] [G loss: 2.104916]\n",
      "epoch:7 step:6111 [D loss: 0.252878, acc.: 92.19%] [G loss: 4.144578]\n",
      "epoch:7 step:6112 [D loss: 0.296885, acc.: 89.06%] [G loss: 1.953342]\n",
      "epoch:7 step:6113 [D loss: 0.264053, acc.: 91.41%] [G loss: 2.553096]\n",
      "epoch:7 step:6114 [D loss: 0.304313, acc.: 90.62%] [G loss: 2.477325]\n",
      "epoch:7 step:6115 [D loss: 0.358645, acc.: 86.72%] [G loss: 2.344605]\n",
      "epoch:7 step:6116 [D loss: 0.388722, acc.: 85.94%] [G loss: 2.042374]\n",
      "epoch:7 step:6117 [D loss: 0.343463, acc.: 84.38%] [G loss: 2.229906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6118 [D loss: 0.353061, acc.: 84.38%] [G loss: 2.895450]\n",
      "epoch:7 step:6119 [D loss: 0.201776, acc.: 93.75%] [G loss: 4.681943]\n",
      "epoch:7 step:6120 [D loss: 0.299861, acc.: 89.06%] [G loss: 2.802028]\n",
      "epoch:7 step:6121 [D loss: 0.345483, acc.: 85.16%] [G loss: 2.645615]\n",
      "epoch:7 step:6122 [D loss: 0.313990, acc.: 90.62%] [G loss: 1.677592]\n",
      "epoch:7 step:6123 [D loss: 0.235832, acc.: 91.41%] [G loss: 3.279289]\n",
      "epoch:7 step:6124 [D loss: 0.395133, acc.: 83.59%] [G loss: 2.948458]\n",
      "epoch:7 step:6125 [D loss: 0.293415, acc.: 89.84%] [G loss: 3.158678]\n",
      "epoch:7 step:6126 [D loss: 0.337065, acc.: 82.81%] [G loss: 3.440591]\n",
      "epoch:7 step:6127 [D loss: 0.344149, acc.: 88.28%] [G loss: 4.701144]\n",
      "epoch:7 step:6128 [D loss: 0.290998, acc.: 89.06%] [G loss: 3.559837]\n",
      "epoch:7 step:6129 [D loss: 0.424916, acc.: 77.34%] [G loss: 2.704869]\n",
      "epoch:7 step:6130 [D loss: 0.296626, acc.: 87.50%] [G loss: 4.066174]\n",
      "epoch:7 step:6131 [D loss: 0.354616, acc.: 83.59%] [G loss: 5.520474]\n",
      "epoch:7 step:6132 [D loss: 0.553515, acc.: 75.78%] [G loss: 2.914248]\n",
      "epoch:7 step:6133 [D loss: 0.273467, acc.: 87.50%] [G loss: 2.706121]\n",
      "epoch:7 step:6134 [D loss: 0.294626, acc.: 88.28%] [G loss: 1.973480]\n",
      "epoch:7 step:6135 [D loss: 0.330123, acc.: 85.94%] [G loss: 2.109365]\n",
      "epoch:7 step:6136 [D loss: 0.363654, acc.: 84.38%] [G loss: 2.703957]\n",
      "epoch:7 step:6137 [D loss: 0.298555, acc.: 86.72%] [G loss: 3.117358]\n",
      "epoch:7 step:6138 [D loss: 0.235391, acc.: 88.28%] [G loss: 3.086469]\n",
      "epoch:7 step:6139 [D loss: 0.275613, acc.: 90.62%] [G loss: 3.277245]\n",
      "epoch:7 step:6140 [D loss: 0.173142, acc.: 93.75%] [G loss: 3.150426]\n",
      "epoch:7 step:6141 [D loss: 0.214314, acc.: 87.50%] [G loss: 3.386539]\n",
      "epoch:7 step:6142 [D loss: 0.373493, acc.: 82.81%] [G loss: 2.954544]\n",
      "epoch:7 step:6143 [D loss: 0.249465, acc.: 89.84%] [G loss: 3.730662]\n",
      "epoch:7 step:6144 [D loss: 0.339096, acc.: 85.16%] [G loss: 2.687407]\n",
      "epoch:7 step:6145 [D loss: 0.309893, acc.: 88.28%] [G loss: 1.928269]\n",
      "epoch:7 step:6146 [D loss: 0.375814, acc.: 82.81%] [G loss: 2.651548]\n",
      "epoch:7 step:6147 [D loss: 0.287554, acc.: 92.97%] [G loss: 3.727997]\n",
      "epoch:7 step:6148 [D loss: 0.235148, acc.: 91.41%] [G loss: 4.567064]\n",
      "epoch:7 step:6149 [D loss: 0.259192, acc.: 90.62%] [G loss: 2.464226]\n",
      "epoch:7 step:6150 [D loss: 0.349441, acc.: 83.59%] [G loss: 3.597603]\n",
      "epoch:7 step:6151 [D loss: 0.250496, acc.: 89.06%] [G loss: 4.058986]\n",
      "epoch:7 step:6152 [D loss: 0.251539, acc.: 89.06%] [G loss: 3.932345]\n",
      "epoch:7 step:6153 [D loss: 0.270734, acc.: 92.97%] [G loss: 2.535146]\n",
      "epoch:7 step:6154 [D loss: 0.225548, acc.: 94.53%] [G loss: 2.658897]\n",
      "epoch:7 step:6155 [D loss: 0.296265, acc.: 91.41%] [G loss: 3.048483]\n",
      "epoch:7 step:6156 [D loss: 0.291889, acc.: 92.19%] [G loss: 3.261685]\n",
      "epoch:7 step:6157 [D loss: 0.310241, acc.: 88.28%] [G loss: 2.401619]\n",
      "epoch:7 step:6158 [D loss: 0.289612, acc.: 90.62%] [G loss: 2.206157]\n",
      "epoch:7 step:6159 [D loss: 0.370498, acc.: 85.16%] [G loss: 2.228418]\n",
      "epoch:7 step:6160 [D loss: 0.304461, acc.: 86.72%] [G loss: 2.343927]\n",
      "epoch:7 step:6161 [D loss: 0.276937, acc.: 89.06%] [G loss: 2.117579]\n",
      "epoch:7 step:6162 [D loss: 0.353317, acc.: 82.03%] [G loss: 3.859311]\n",
      "epoch:7 step:6163 [D loss: 0.501444, acc.: 73.44%] [G loss: 3.283305]\n",
      "epoch:7 step:6164 [D loss: 0.437456, acc.: 85.94%] [G loss: 4.809276]\n",
      "epoch:7 step:6165 [D loss: 0.692002, acc.: 71.09%] [G loss: 3.423514]\n",
      "epoch:7 step:6166 [D loss: 0.287729, acc.: 87.50%] [G loss: 3.384591]\n",
      "epoch:7 step:6167 [D loss: 0.344636, acc.: 85.16%] [G loss: 5.448271]\n",
      "epoch:7 step:6168 [D loss: 0.403636, acc.: 85.16%] [G loss: 3.595538]\n",
      "epoch:7 step:6169 [D loss: 0.266720, acc.: 85.94%] [G loss: 2.681227]\n",
      "epoch:7 step:6170 [D loss: 0.251303, acc.: 86.72%] [G loss: 3.532362]\n",
      "epoch:7 step:6171 [D loss: 0.302061, acc.: 88.28%] [G loss: 4.441782]\n",
      "epoch:7 step:6172 [D loss: 0.209740, acc.: 93.75%] [G loss: 4.316882]\n",
      "epoch:7 step:6173 [D loss: 0.342659, acc.: 82.03%] [G loss: 3.330600]\n",
      "epoch:7 step:6174 [D loss: 0.386833, acc.: 85.16%] [G loss: 1.859713]\n",
      "epoch:7 step:6175 [D loss: 0.233000, acc.: 89.84%] [G loss: 3.276154]\n",
      "epoch:7 step:6176 [D loss: 0.249273, acc.: 89.84%] [G loss: 5.872050]\n",
      "epoch:7 step:6177 [D loss: 0.209922, acc.: 90.62%] [G loss: 3.070385]\n",
      "epoch:7 step:6178 [D loss: 0.247639, acc.: 91.41%] [G loss: 2.795166]\n",
      "epoch:7 step:6179 [D loss: 0.266749, acc.: 89.06%] [G loss: 3.050680]\n",
      "epoch:7 step:6180 [D loss: 0.336581, acc.: 84.38%] [G loss: 3.796345]\n",
      "epoch:7 step:6181 [D loss: 0.254501, acc.: 87.50%] [G loss: 4.079506]\n",
      "epoch:7 step:6182 [D loss: 0.268720, acc.: 89.06%] [G loss: 4.011599]\n",
      "epoch:7 step:6183 [D loss: 0.281408, acc.: 85.94%] [G loss: 2.509315]\n",
      "epoch:7 step:6184 [D loss: 0.307311, acc.: 89.84%] [G loss: 4.128733]\n",
      "epoch:7 step:6185 [D loss: 0.324840, acc.: 86.72%] [G loss: 2.863730]\n",
      "epoch:7 step:6186 [D loss: 0.316018, acc.: 86.72%] [G loss: 2.733172]\n",
      "epoch:7 step:6187 [D loss: 0.289545, acc.: 86.72%] [G loss: 3.266059]\n",
      "epoch:7 step:6188 [D loss: 0.418770, acc.: 80.47%] [G loss: 2.238841]\n",
      "epoch:7 step:6189 [D loss: 0.294401, acc.: 88.28%] [G loss: 3.525946]\n",
      "epoch:7 step:6190 [D loss: 0.209198, acc.: 92.97%] [G loss: 3.740918]\n",
      "epoch:7 step:6191 [D loss: 0.305691, acc.: 87.50%] [G loss: 3.020191]\n",
      "epoch:7 step:6192 [D loss: 0.144342, acc.: 94.53%] [G loss: 6.404906]\n",
      "epoch:7 step:6193 [D loss: 0.189989, acc.: 96.09%] [G loss: 4.183072]\n",
      "epoch:7 step:6194 [D loss: 0.228517, acc.: 95.31%] [G loss: 3.400719]\n",
      "epoch:7 step:6195 [D loss: 0.279062, acc.: 89.84%] [G loss: 2.691775]\n",
      "epoch:7 step:6196 [D loss: 0.239670, acc.: 92.19%] [G loss: 2.554615]\n",
      "epoch:7 step:6197 [D loss: 0.263136, acc.: 93.75%] [G loss: 2.644506]\n",
      "epoch:7 step:6198 [D loss: 0.163379, acc.: 95.31%] [G loss: 3.791897]\n",
      "epoch:7 step:6199 [D loss: 0.215363, acc.: 89.84%] [G loss: 4.838006]\n",
      "epoch:7 step:6200 [D loss: 0.309456, acc.: 85.94%] [G loss: 2.918966]\n",
      "epoch:7 step:6201 [D loss: 0.229756, acc.: 93.75%] [G loss: 3.395508]\n",
      "epoch:7 step:6202 [D loss: 0.288848, acc.: 89.84%] [G loss: 3.652687]\n",
      "epoch:7 step:6203 [D loss: 0.340681, acc.: 85.16%] [G loss: 2.764158]\n",
      "epoch:7 step:6204 [D loss: 0.299016, acc.: 89.84%] [G loss: 3.407086]\n",
      "epoch:7 step:6205 [D loss: 0.286056, acc.: 90.62%] [G loss: 3.350767]\n",
      "epoch:7 step:6206 [D loss: 0.247723, acc.: 91.41%] [G loss: 3.533262]\n",
      "epoch:7 step:6207 [D loss: 0.172152, acc.: 94.53%] [G loss: 3.953558]\n",
      "epoch:7 step:6208 [D loss: 0.281105, acc.: 89.06%] [G loss: 2.957269]\n",
      "epoch:7 step:6209 [D loss: 0.249200, acc.: 89.84%] [G loss: 3.591847]\n",
      "epoch:7 step:6210 [D loss: 0.286514, acc.: 88.28%] [G loss: 2.137616]\n",
      "epoch:7 step:6211 [D loss: 0.320812, acc.: 87.50%] [G loss: 3.628864]\n",
      "epoch:7 step:6212 [D loss: 0.309876, acc.: 89.06%] [G loss: 3.105119]\n",
      "epoch:7 step:6213 [D loss: 0.322920, acc.: 87.50%] [G loss: 2.681116]\n",
      "epoch:7 step:6214 [D loss: 0.232517, acc.: 93.75%] [G loss: 3.812513]\n",
      "epoch:7 step:6215 [D loss: 0.204309, acc.: 92.97%] [G loss: 3.747144]\n",
      "epoch:7 step:6216 [D loss: 0.267482, acc.: 87.50%] [G loss: 2.571400]\n",
      "epoch:7 step:6217 [D loss: 0.308503, acc.: 85.94%] [G loss: 2.404319]\n",
      "epoch:7 step:6218 [D loss: 0.300118, acc.: 87.50%] [G loss: 2.324187]\n",
      "epoch:7 step:6219 [D loss: 0.352074, acc.: 83.59%] [G loss: 4.680416]\n",
      "epoch:7 step:6220 [D loss: 0.810396, acc.: 69.53%] [G loss: 4.586889]\n",
      "epoch:7 step:6221 [D loss: 1.277992, acc.: 64.06%] [G loss: 8.132837]\n",
      "epoch:7 step:6222 [D loss: 2.341067, acc.: 51.56%] [G loss: 2.798675]\n",
      "epoch:7 step:6223 [D loss: 0.723182, acc.: 74.22%] [G loss: 3.436278]\n",
      "epoch:7 step:6224 [D loss: 0.395280, acc.: 82.81%] [G loss: 3.843964]\n",
      "epoch:7 step:6225 [D loss: 0.519254, acc.: 77.34%] [G loss: 2.166492]\n",
      "epoch:7 step:6226 [D loss: 0.308926, acc.: 84.38%] [G loss: 2.289225]\n",
      "epoch:7 step:6227 [D loss: 0.359675, acc.: 87.50%] [G loss: 3.653212]\n",
      "epoch:7 step:6228 [D loss: 0.327735, acc.: 84.38%] [G loss: 4.089183]\n",
      "epoch:7 step:6229 [D loss: 0.354602, acc.: 88.28%] [G loss: 2.782792]\n",
      "epoch:7 step:6230 [D loss: 0.250587, acc.: 87.50%] [G loss: 2.496885]\n",
      "epoch:7 step:6231 [D loss: 0.276476, acc.: 87.50%] [G loss: 3.123065]\n",
      "epoch:7 step:6232 [D loss: 0.403865, acc.: 77.34%] [G loss: 2.955537]\n",
      "epoch:7 step:6233 [D loss: 0.198047, acc.: 93.75%] [G loss: 4.114320]\n",
      "epoch:7 step:6234 [D loss: 0.236065, acc.: 88.28%] [G loss: 4.431774]\n",
      "epoch:7 step:6235 [D loss: 0.317097, acc.: 81.25%] [G loss: 3.123292]\n",
      "epoch:7 step:6236 [D loss: 0.229030, acc.: 91.41%] [G loss: 3.530930]\n",
      "epoch:7 step:6237 [D loss: 0.221344, acc.: 91.41%] [G loss: 3.326925]\n",
      "epoch:7 step:6238 [D loss: 0.375345, acc.: 85.94%] [G loss: 1.875493]\n",
      "epoch:7 step:6239 [D loss: 0.431204, acc.: 80.47%] [G loss: 2.047503]\n",
      "epoch:7 step:6240 [D loss: 0.396222, acc.: 84.38%] [G loss: 1.668890]\n",
      "epoch:7 step:6241 [D loss: 0.513630, acc.: 78.91%] [G loss: 2.301861]\n",
      "epoch:7 step:6242 [D loss: 0.211972, acc.: 91.41%] [G loss: 3.589642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6243 [D loss: 0.222599, acc.: 91.41%] [G loss: 5.482190]\n",
      "epoch:7 step:6244 [D loss: 0.406335, acc.: 81.25%] [G loss: 2.025440]\n",
      "epoch:7 step:6245 [D loss: 0.236624, acc.: 94.53%] [G loss: 1.986407]\n",
      "epoch:7 step:6246 [D loss: 0.290114, acc.: 87.50%] [G loss: 2.378230]\n",
      "epoch:7 step:6247 [D loss: 0.330431, acc.: 89.06%] [G loss: 1.938576]\n",
      "epoch:7 step:6248 [D loss: 0.363146, acc.: 87.50%] [G loss: 2.673573]\n",
      "epoch:8 step:6249 [D loss: 0.295387, acc.: 88.28%] [G loss: 3.151528]\n",
      "epoch:8 step:6250 [D loss: 0.354182, acc.: 83.59%] [G loss: 2.021983]\n",
      "epoch:8 step:6251 [D loss: 0.412687, acc.: 78.12%] [G loss: 2.830550]\n",
      "epoch:8 step:6252 [D loss: 0.334811, acc.: 84.38%] [G loss: 2.556571]\n",
      "epoch:8 step:6253 [D loss: 0.277917, acc.: 90.62%] [G loss: 1.851429]\n",
      "epoch:8 step:6254 [D loss: 0.418762, acc.: 84.38%] [G loss: 2.098300]\n",
      "epoch:8 step:6255 [D loss: 0.189122, acc.: 92.97%] [G loss: 4.492269]\n",
      "epoch:8 step:6256 [D loss: 0.276602, acc.: 88.28%] [G loss: 4.143684]\n",
      "epoch:8 step:6257 [D loss: 0.256622, acc.: 92.19%] [G loss: 3.457298]\n",
      "epoch:8 step:6258 [D loss: 0.374618, acc.: 84.38%] [G loss: 3.158998]\n",
      "epoch:8 step:6259 [D loss: 0.428517, acc.: 80.47%] [G loss: 2.261893]\n",
      "epoch:8 step:6260 [D loss: 0.352769, acc.: 84.38%] [G loss: 2.729485]\n",
      "epoch:8 step:6261 [D loss: 0.303439, acc.: 89.06%] [G loss: 2.274498]\n",
      "epoch:8 step:6262 [D loss: 0.275230, acc.: 89.84%] [G loss: 2.933897]\n",
      "epoch:8 step:6263 [D loss: 0.190778, acc.: 92.97%] [G loss: 5.757764]\n",
      "epoch:8 step:6264 [D loss: 0.259160, acc.: 92.97%] [G loss: 2.393458]\n",
      "epoch:8 step:6265 [D loss: 0.326399, acc.: 83.59%] [G loss: 2.469133]\n",
      "epoch:8 step:6266 [D loss: 0.246343, acc.: 92.19%] [G loss: 3.090586]\n",
      "epoch:8 step:6267 [D loss: 0.235696, acc.: 92.19%] [G loss: 3.532487]\n",
      "epoch:8 step:6268 [D loss: 0.290763, acc.: 89.84%] [G loss: 3.466012]\n",
      "epoch:8 step:6269 [D loss: 0.333171, acc.: 87.50%] [G loss: 2.475865]\n",
      "epoch:8 step:6270 [D loss: 0.297016, acc.: 89.06%] [G loss: 4.118632]\n",
      "epoch:8 step:6271 [D loss: 0.394784, acc.: 84.38%] [G loss: 1.900135]\n",
      "epoch:8 step:6272 [D loss: 0.362695, acc.: 85.16%] [G loss: 2.305102]\n",
      "epoch:8 step:6273 [D loss: 0.344616, acc.: 84.38%] [G loss: 2.976775]\n",
      "epoch:8 step:6274 [D loss: 0.254611, acc.: 89.84%] [G loss: 6.331245]\n",
      "epoch:8 step:6275 [D loss: 0.282741, acc.: 89.84%] [G loss: 2.632958]\n",
      "epoch:8 step:6276 [D loss: 0.221073, acc.: 93.75%] [G loss: 3.594666]\n",
      "epoch:8 step:6277 [D loss: 0.267436, acc.: 89.84%] [G loss: 4.332845]\n",
      "epoch:8 step:6278 [D loss: 0.336486, acc.: 85.94%] [G loss: 3.077456]\n",
      "epoch:8 step:6279 [D loss: 0.324035, acc.: 86.72%] [G loss: 3.961697]\n",
      "epoch:8 step:6280 [D loss: 0.394962, acc.: 80.47%] [G loss: 2.579045]\n",
      "epoch:8 step:6281 [D loss: 0.284507, acc.: 89.06%] [G loss: 3.252317]\n",
      "epoch:8 step:6282 [D loss: 0.349841, acc.: 85.16%] [G loss: 2.753979]\n",
      "epoch:8 step:6283 [D loss: 0.395894, acc.: 78.91%] [G loss: 2.542034]\n",
      "epoch:8 step:6284 [D loss: 0.302545, acc.: 86.72%] [G loss: 3.308312]\n",
      "epoch:8 step:6285 [D loss: 0.483184, acc.: 81.25%] [G loss: 5.241838]\n",
      "epoch:8 step:6286 [D loss: 1.004541, acc.: 64.84%] [G loss: 7.911048]\n",
      "epoch:8 step:6287 [D loss: 2.333745, acc.: 47.66%] [G loss: 2.271092]\n",
      "epoch:8 step:6288 [D loss: 0.728927, acc.: 75.78%] [G loss: 2.861544]\n",
      "epoch:8 step:6289 [D loss: 0.494337, acc.: 78.91%] [G loss: 4.267550]\n",
      "epoch:8 step:6290 [D loss: 0.451569, acc.: 76.56%] [G loss: 5.375858]\n",
      "epoch:8 step:6291 [D loss: 0.443974, acc.: 77.34%] [G loss: 4.519930]\n",
      "epoch:8 step:6292 [D loss: 0.415982, acc.: 78.91%] [G loss: 5.449073]\n",
      "epoch:8 step:6293 [D loss: 0.283051, acc.: 89.84%] [G loss: 3.727067]\n",
      "epoch:8 step:6294 [D loss: 0.375780, acc.: 82.03%] [G loss: 1.949330]\n",
      "epoch:8 step:6295 [D loss: 0.389860, acc.: 82.03%] [G loss: 2.333385]\n",
      "epoch:8 step:6296 [D loss: 0.389174, acc.: 84.38%] [G loss: 2.073606]\n",
      "epoch:8 step:6297 [D loss: 0.330164, acc.: 88.28%] [G loss: 2.638247]\n",
      "epoch:8 step:6298 [D loss: 0.357378, acc.: 82.03%] [G loss: 2.043182]\n",
      "epoch:8 step:6299 [D loss: 0.364538, acc.: 85.94%] [G loss: 3.024204]\n",
      "epoch:8 step:6300 [D loss: 0.299758, acc.: 88.28%] [G loss: 2.719085]\n",
      "epoch:8 step:6301 [D loss: 0.289785, acc.: 85.94%] [G loss: 2.440279]\n",
      "epoch:8 step:6302 [D loss: 0.224236, acc.: 95.31%] [G loss: 2.551239]\n",
      "epoch:8 step:6303 [D loss: 0.275934, acc.: 89.06%] [G loss: 3.591291]\n",
      "epoch:8 step:6304 [D loss: 0.259053, acc.: 94.53%] [G loss: 3.119632]\n",
      "epoch:8 step:6305 [D loss: 0.371124, acc.: 87.50%] [G loss: 2.168161]\n",
      "epoch:8 step:6306 [D loss: 0.339352, acc.: 83.59%] [G loss: 3.052856]\n",
      "epoch:8 step:6307 [D loss: 0.363835, acc.: 85.16%] [G loss: 2.149455]\n",
      "epoch:8 step:6308 [D loss: 0.312743, acc.: 89.84%] [G loss: 2.496942]\n",
      "epoch:8 step:6309 [D loss: 0.340602, acc.: 85.94%] [G loss: 1.866637]\n",
      "epoch:8 step:6310 [D loss: 0.301715, acc.: 89.84%] [G loss: 1.942305]\n",
      "epoch:8 step:6311 [D loss: 0.398067, acc.: 83.59%] [G loss: 2.171967]\n",
      "epoch:8 step:6312 [D loss: 0.286440, acc.: 89.84%] [G loss: 2.313568]\n",
      "epoch:8 step:6313 [D loss: 0.459893, acc.: 79.69%] [G loss: 2.183128]\n",
      "epoch:8 step:6314 [D loss: 0.300736, acc.: 87.50%] [G loss: 3.010788]\n",
      "epoch:8 step:6315 [D loss: 0.266759, acc.: 88.28%] [G loss: 4.486627]\n",
      "epoch:8 step:6316 [D loss: 0.265676, acc.: 88.28%] [G loss: 3.138524]\n",
      "epoch:8 step:6317 [D loss: 0.236191, acc.: 90.62%] [G loss: 4.508675]\n",
      "epoch:8 step:6318 [D loss: 0.220459, acc.: 95.31%] [G loss: 3.687882]\n",
      "epoch:8 step:6319 [D loss: 0.332125, acc.: 88.28%] [G loss: 2.941248]\n",
      "epoch:8 step:6320 [D loss: 0.489948, acc.: 81.25%] [G loss: 2.315124]\n",
      "epoch:8 step:6321 [D loss: 0.316010, acc.: 88.28%] [G loss: 2.781081]\n",
      "epoch:8 step:6322 [D loss: 0.348454, acc.: 84.38%] [G loss: 2.508079]\n",
      "epoch:8 step:6323 [D loss: 0.349902, acc.: 86.72%] [G loss: 2.734940]\n",
      "epoch:8 step:6324 [D loss: 0.452791, acc.: 79.69%] [G loss: 3.167397]\n",
      "epoch:8 step:6325 [D loss: 0.294130, acc.: 89.06%] [G loss: 3.352473]\n",
      "epoch:8 step:6326 [D loss: 0.355700, acc.: 82.81%] [G loss: 3.089861]\n",
      "epoch:8 step:6327 [D loss: 0.302980, acc.: 85.16%] [G loss: 2.409407]\n",
      "epoch:8 step:6328 [D loss: 0.235084, acc.: 96.88%] [G loss: 2.870633]\n",
      "epoch:8 step:6329 [D loss: 0.305643, acc.: 94.53%] [G loss: 2.079043]\n",
      "epoch:8 step:6330 [D loss: 0.334292, acc.: 89.06%] [G loss: 1.790426]\n",
      "epoch:8 step:6331 [D loss: 0.304845, acc.: 85.16%] [G loss: 2.300003]\n",
      "epoch:8 step:6332 [D loss: 0.311595, acc.: 89.06%] [G loss: 2.283035]\n",
      "epoch:8 step:6333 [D loss: 0.303892, acc.: 91.41%] [G loss: 2.378295]\n",
      "epoch:8 step:6334 [D loss: 0.312966, acc.: 83.59%] [G loss: 3.076360]\n",
      "epoch:8 step:6335 [D loss: 0.284169, acc.: 89.06%] [G loss: 4.251423]\n",
      "epoch:8 step:6336 [D loss: 0.216742, acc.: 90.62%] [G loss: 4.119800]\n",
      "epoch:8 step:6337 [D loss: 0.304937, acc.: 87.50%] [G loss: 2.334179]\n",
      "epoch:8 step:6338 [D loss: 0.364095, acc.: 84.38%] [G loss: 3.772651]\n",
      "epoch:8 step:6339 [D loss: 0.283768, acc.: 89.06%] [G loss: 2.923386]\n",
      "epoch:8 step:6340 [D loss: 0.257559, acc.: 91.41%] [G loss: 4.527614]\n",
      "epoch:8 step:6341 [D loss: 0.454343, acc.: 78.12%] [G loss: 2.736304]\n",
      "epoch:8 step:6342 [D loss: 0.231409, acc.: 92.97%] [G loss: 4.761991]\n",
      "epoch:8 step:6343 [D loss: 0.245446, acc.: 88.28%] [G loss: 4.527150]\n",
      "epoch:8 step:6344 [D loss: 0.308531, acc.: 89.84%] [G loss: 2.500287]\n",
      "epoch:8 step:6345 [D loss: 0.241160, acc.: 93.75%] [G loss: 3.015551]\n",
      "epoch:8 step:6346 [D loss: 0.201313, acc.: 92.97%] [G loss: 3.500411]\n",
      "epoch:8 step:6347 [D loss: 0.381135, acc.: 82.81%] [G loss: 2.487469]\n",
      "epoch:8 step:6348 [D loss: 0.313437, acc.: 85.94%] [G loss: 3.098089]\n",
      "epoch:8 step:6349 [D loss: 0.246728, acc.: 90.62%] [G loss: 4.430819]\n",
      "epoch:8 step:6350 [D loss: 0.326671, acc.: 85.94%] [G loss: 2.020438]\n",
      "epoch:8 step:6351 [D loss: 0.339112, acc.: 86.72%] [G loss: 1.974791]\n",
      "epoch:8 step:6352 [D loss: 0.332293, acc.: 85.16%] [G loss: 3.115716]\n",
      "epoch:8 step:6353 [D loss: 0.266592, acc.: 91.41%] [G loss: 2.740655]\n",
      "epoch:8 step:6354 [D loss: 0.303266, acc.: 85.94%] [G loss: 4.476802]\n",
      "epoch:8 step:6355 [D loss: 0.230004, acc.: 92.19%] [G loss: 2.827082]\n",
      "epoch:8 step:6356 [D loss: 0.279203, acc.: 92.19%] [G loss: 1.845891]\n",
      "epoch:8 step:6357 [D loss: 0.304999, acc.: 90.62%] [G loss: 2.916293]\n",
      "epoch:8 step:6358 [D loss: 0.251624, acc.: 89.06%] [G loss: 2.977972]\n",
      "epoch:8 step:6359 [D loss: 0.287970, acc.: 90.62%] [G loss: 2.735471]\n",
      "epoch:8 step:6360 [D loss: 0.264531, acc.: 88.28%] [G loss: 2.694254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6361 [D loss: 0.311220, acc.: 86.72%] [G loss: 4.146937]\n",
      "epoch:8 step:6362 [D loss: 0.280342, acc.: 92.19%] [G loss: 3.019967]\n",
      "epoch:8 step:6363 [D loss: 0.277525, acc.: 92.19%] [G loss: 2.511487]\n",
      "epoch:8 step:6364 [D loss: 0.299647, acc.: 90.62%] [G loss: 2.107085]\n",
      "epoch:8 step:6365 [D loss: 0.298806, acc.: 90.62%] [G loss: 3.358373]\n",
      "epoch:8 step:6366 [D loss: 0.292177, acc.: 87.50%] [G loss: 2.411164]\n",
      "epoch:8 step:6367 [D loss: 0.240872, acc.: 91.41%] [G loss: 2.708090]\n",
      "epoch:8 step:6368 [D loss: 0.270851, acc.: 88.28%] [G loss: 2.546868]\n",
      "epoch:8 step:6369 [D loss: 0.248264, acc.: 88.28%] [G loss: 3.562952]\n",
      "epoch:8 step:6370 [D loss: 0.222527, acc.: 89.84%] [G loss: 3.563880]\n",
      "epoch:8 step:6371 [D loss: 0.226368, acc.: 92.19%] [G loss: 2.728578]\n",
      "epoch:8 step:6372 [D loss: 0.271049, acc.: 93.75%] [G loss: 2.246567]\n",
      "epoch:8 step:6373 [D loss: 0.358988, acc.: 85.16%] [G loss: 2.734655]\n",
      "epoch:8 step:6374 [D loss: 0.304479, acc.: 92.19%] [G loss: 2.448876]\n",
      "epoch:8 step:6375 [D loss: 0.298260, acc.: 89.06%] [G loss: 2.535945]\n",
      "epoch:8 step:6376 [D loss: 0.195541, acc.: 92.97%] [G loss: 4.155184]\n",
      "epoch:8 step:6377 [D loss: 0.226237, acc.: 88.28%] [G loss: 4.745072]\n",
      "epoch:8 step:6378 [D loss: 0.255706, acc.: 89.84%] [G loss: 2.516382]\n",
      "epoch:8 step:6379 [D loss: 0.229153, acc.: 89.84%] [G loss: 4.103452]\n",
      "epoch:8 step:6380 [D loss: 0.279730, acc.: 89.06%] [G loss: 2.742707]\n",
      "epoch:8 step:6381 [D loss: 0.290117, acc.: 88.28%] [G loss: 2.350655]\n",
      "epoch:8 step:6382 [D loss: 0.336570, acc.: 83.59%] [G loss: 4.155186]\n",
      "epoch:8 step:6383 [D loss: 0.239700, acc.: 89.06%] [G loss: 4.481551]\n",
      "epoch:8 step:6384 [D loss: 0.265885, acc.: 92.19%] [G loss: 2.514229]\n",
      "epoch:8 step:6385 [D loss: 0.242593, acc.: 91.41%] [G loss: 2.447955]\n",
      "epoch:8 step:6386 [D loss: 0.225433, acc.: 92.19%] [G loss: 3.027672]\n",
      "epoch:8 step:6387 [D loss: 0.237830, acc.: 92.97%] [G loss: 3.858247]\n",
      "epoch:8 step:6388 [D loss: 0.275296, acc.: 91.41%] [G loss: 2.429657]\n",
      "epoch:8 step:6389 [D loss: 0.273434, acc.: 92.19%] [G loss: 2.932849]\n",
      "epoch:8 step:6390 [D loss: 0.285644, acc.: 88.28%] [G loss: 2.978850]\n",
      "epoch:8 step:6391 [D loss: 0.301905, acc.: 89.06%] [G loss: 2.555011]\n",
      "epoch:8 step:6392 [D loss: 0.253484, acc.: 87.50%] [G loss: 3.514658]\n",
      "epoch:8 step:6393 [D loss: 0.255799, acc.: 89.84%] [G loss: 3.642276]\n",
      "epoch:8 step:6394 [D loss: 0.365561, acc.: 85.16%] [G loss: 2.685651]\n",
      "epoch:8 step:6395 [D loss: 0.317276, acc.: 85.16%] [G loss: 2.295042]\n",
      "epoch:8 step:6396 [D loss: 0.243831, acc.: 90.62%] [G loss: 4.299883]\n",
      "epoch:8 step:6397 [D loss: 0.364965, acc.: 85.94%] [G loss: 2.282360]\n",
      "epoch:8 step:6398 [D loss: 0.292302, acc.: 89.84%] [G loss: 2.878138]\n",
      "epoch:8 step:6399 [D loss: 0.339010, acc.: 85.16%] [G loss: 2.327678]\n",
      "epoch:8 step:6400 [D loss: 0.333762, acc.: 91.41%] [G loss: 1.914963]\n",
      "epoch:8 step:6401 [D loss: 0.231806, acc.: 95.31%] [G loss: 2.412080]\n",
      "epoch:8 step:6402 [D loss: 0.283539, acc.: 86.72%] [G loss: 2.899763]\n",
      "epoch:8 step:6403 [D loss: 0.212908, acc.: 92.97%] [G loss: 2.958082]\n",
      "epoch:8 step:6404 [D loss: 0.231123, acc.: 92.97%] [G loss: 4.199968]\n",
      "epoch:8 step:6405 [D loss: 0.285217, acc.: 89.06%] [G loss: 4.858587]\n",
      "epoch:8 step:6406 [D loss: 0.369454, acc.: 85.94%] [G loss: 2.583037]\n",
      "epoch:8 step:6407 [D loss: 0.262367, acc.: 91.41%] [G loss: 2.947218]\n",
      "epoch:8 step:6408 [D loss: 0.347682, acc.: 82.03%] [G loss: 6.479088]\n",
      "epoch:8 step:6409 [D loss: 0.765614, acc.: 71.09%] [G loss: 3.304086]\n",
      "epoch:8 step:6410 [D loss: 0.451707, acc.: 82.81%] [G loss: 2.872268]\n",
      "epoch:8 step:6411 [D loss: 0.233922, acc.: 90.62%] [G loss: 3.122619]\n",
      "epoch:8 step:6412 [D loss: 0.261634, acc.: 91.41%] [G loss: 2.675056]\n",
      "epoch:8 step:6413 [D loss: 0.248519, acc.: 91.41%] [G loss: 3.031823]\n",
      "epoch:8 step:6414 [D loss: 0.215217, acc.: 92.97%] [G loss: 3.421078]\n",
      "epoch:8 step:6415 [D loss: 0.395461, acc.: 86.72%] [G loss: 2.020865]\n",
      "epoch:8 step:6416 [D loss: 0.325998, acc.: 87.50%] [G loss: 2.511539]\n",
      "epoch:8 step:6417 [D loss: 0.313992, acc.: 88.28%] [G loss: 2.521835]\n",
      "epoch:8 step:6418 [D loss: 0.336863, acc.: 85.94%] [G loss: 2.602256]\n",
      "epoch:8 step:6419 [D loss: 0.308702, acc.: 85.94%] [G loss: 3.220337]\n",
      "epoch:8 step:6420 [D loss: 0.224852, acc.: 90.62%] [G loss: 4.245083]\n",
      "epoch:8 step:6421 [D loss: 0.272415, acc.: 86.72%] [G loss: 3.235873]\n",
      "epoch:8 step:6422 [D loss: 0.350171, acc.: 85.16%] [G loss: 2.555328]\n",
      "epoch:8 step:6423 [D loss: 0.371122, acc.: 87.50%] [G loss: 3.031162]\n",
      "epoch:8 step:6424 [D loss: 0.216262, acc.: 91.41%] [G loss: 3.849996]\n",
      "epoch:8 step:6425 [D loss: 0.277240, acc.: 88.28%] [G loss: 3.819955]\n",
      "epoch:8 step:6426 [D loss: 0.318817, acc.: 84.38%] [G loss: 2.902323]\n",
      "epoch:8 step:6427 [D loss: 0.237679, acc.: 90.62%] [G loss: 4.096023]\n",
      "epoch:8 step:6428 [D loss: 0.300318, acc.: 87.50%] [G loss: 5.941871]\n",
      "epoch:8 step:6429 [D loss: 0.403234, acc.: 79.69%] [G loss: 4.904122]\n",
      "epoch:8 step:6430 [D loss: 0.236317, acc.: 89.06%] [G loss: 4.945804]\n",
      "epoch:8 step:6431 [D loss: 0.340268, acc.: 83.59%] [G loss: 2.487009]\n",
      "epoch:8 step:6432 [D loss: 0.275742, acc.: 85.94%] [G loss: 3.319482]\n",
      "epoch:8 step:6433 [D loss: 0.153761, acc.: 93.75%] [G loss: 6.409853]\n",
      "epoch:8 step:6434 [D loss: 0.262519, acc.: 91.41%] [G loss: 2.681949]\n",
      "epoch:8 step:6435 [D loss: 0.323215, acc.: 86.72%] [G loss: 2.192356]\n",
      "epoch:8 step:6436 [D loss: 0.295651, acc.: 89.06%] [G loss: 2.130876]\n",
      "epoch:8 step:6437 [D loss: 0.281328, acc.: 91.41%] [G loss: 2.645166]\n",
      "epoch:8 step:6438 [D loss: 0.377853, acc.: 87.50%] [G loss: 2.605719]\n",
      "epoch:8 step:6439 [D loss: 0.242857, acc.: 89.84%] [G loss: 3.571071]\n",
      "epoch:8 step:6440 [D loss: 0.209255, acc.: 92.19%] [G loss: 4.017418]\n",
      "epoch:8 step:6441 [D loss: 0.281526, acc.: 89.06%] [G loss: 2.386868]\n",
      "epoch:8 step:6442 [D loss: 0.267424, acc.: 92.19%] [G loss: 3.166255]\n",
      "epoch:8 step:6443 [D loss: 0.208452, acc.: 93.75%] [G loss: 4.370768]\n",
      "epoch:8 step:6444 [D loss: 0.240158, acc.: 93.75%] [G loss: 4.688445]\n",
      "epoch:8 step:6445 [D loss: 0.315586, acc.: 80.47%] [G loss: 2.814617]\n",
      "epoch:8 step:6446 [D loss: 0.357955, acc.: 82.03%] [G loss: 3.291571]\n",
      "epoch:8 step:6447 [D loss: 0.214404, acc.: 92.19%] [G loss: 3.709256]\n",
      "epoch:8 step:6448 [D loss: 0.165028, acc.: 96.09%] [G loss: 4.233642]\n",
      "epoch:8 step:6449 [D loss: 0.297547, acc.: 89.84%] [G loss: 2.462842]\n",
      "epoch:8 step:6450 [D loss: 0.312554, acc.: 89.84%] [G loss: 4.209651]\n",
      "epoch:8 step:6451 [D loss: 0.292229, acc.: 88.28%] [G loss: 3.411254]\n",
      "epoch:8 step:6452 [D loss: 0.294190, acc.: 87.50%] [G loss: 5.067316]\n",
      "epoch:8 step:6453 [D loss: 0.343967, acc.: 85.16%] [G loss: 3.943473]\n",
      "epoch:8 step:6454 [D loss: 0.284290, acc.: 87.50%] [G loss: 5.528762]\n",
      "epoch:8 step:6455 [D loss: 0.371303, acc.: 81.25%] [G loss: 3.245079]\n",
      "epoch:8 step:6456 [D loss: 0.281480, acc.: 89.06%] [G loss: 2.529633]\n",
      "epoch:8 step:6457 [D loss: 0.212466, acc.: 95.31%] [G loss: 4.050895]\n",
      "epoch:8 step:6458 [D loss: 0.181115, acc.: 91.41%] [G loss: 6.254838]\n",
      "epoch:8 step:6459 [D loss: 0.230532, acc.: 88.28%] [G loss: 5.023499]\n",
      "epoch:8 step:6460 [D loss: 0.242519, acc.: 90.62%] [G loss: 2.335581]\n",
      "epoch:8 step:6461 [D loss: 0.196074, acc.: 96.09%] [G loss: 3.816302]\n",
      "epoch:8 step:6462 [D loss: 0.188241, acc.: 95.31%] [G loss: 3.980169]\n",
      "epoch:8 step:6463 [D loss: 0.293867, acc.: 88.28%] [G loss: 2.726365]\n",
      "epoch:8 step:6464 [D loss: 0.336351, acc.: 88.28%] [G loss: 2.304328]\n",
      "epoch:8 step:6465 [D loss: 0.205517, acc.: 94.53%] [G loss: 3.488315]\n",
      "epoch:8 step:6466 [D loss: 0.344335, acc.: 85.16%] [G loss: 2.222074]\n",
      "epoch:8 step:6467 [D loss: 0.239256, acc.: 93.75%] [G loss: 2.608975]\n",
      "epoch:8 step:6468 [D loss: 0.255509, acc.: 90.62%] [G loss: 2.921243]\n",
      "epoch:8 step:6469 [D loss: 0.240977, acc.: 94.53%] [G loss: 2.899494]\n",
      "epoch:8 step:6470 [D loss: 0.232391, acc.: 90.62%] [G loss: 2.786742]\n",
      "epoch:8 step:6471 [D loss: 0.207667, acc.: 92.97%] [G loss: 3.058392]\n",
      "epoch:8 step:6472 [D loss: 0.352220, acc.: 88.28%] [G loss: 2.460505]\n",
      "epoch:8 step:6473 [D loss: 0.316430, acc.: 87.50%] [G loss: 2.388186]\n",
      "epoch:8 step:6474 [D loss: 0.396292, acc.: 82.03%] [G loss: 3.257207]\n",
      "epoch:8 step:6475 [D loss: 0.271610, acc.: 89.84%] [G loss: 3.035872]\n",
      "epoch:8 step:6476 [D loss: 0.301589, acc.: 86.72%] [G loss: 3.562536]\n",
      "epoch:8 step:6477 [D loss: 0.242583, acc.: 91.41%] [G loss: 4.815175]\n",
      "epoch:8 step:6478 [D loss: 0.359288, acc.: 85.94%] [G loss: 3.477625]\n",
      "epoch:8 step:6479 [D loss: 0.200287, acc.: 92.19%] [G loss: 6.925172]\n",
      "epoch:8 step:6480 [D loss: 0.272085, acc.: 88.28%] [G loss: 2.426641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6481 [D loss: 0.202225, acc.: 94.53%] [G loss: 3.608950]\n",
      "epoch:8 step:6482 [D loss: 0.238413, acc.: 91.41%] [G loss: 3.126704]\n",
      "epoch:8 step:6483 [D loss: 0.289603, acc.: 85.94%] [G loss: 3.226367]\n",
      "epoch:8 step:6484 [D loss: 0.192500, acc.: 92.19%] [G loss: 4.094289]\n",
      "epoch:8 step:6485 [D loss: 0.324762, acc.: 85.94%] [G loss: 2.497688]\n",
      "epoch:8 step:6486 [D loss: 0.288254, acc.: 89.06%] [G loss: 3.429432]\n",
      "epoch:8 step:6487 [D loss: 0.222875, acc.: 92.97%] [G loss: 3.911385]\n",
      "epoch:8 step:6488 [D loss: 0.278253, acc.: 90.62%] [G loss: 2.688221]\n",
      "epoch:8 step:6489 [D loss: 0.310321, acc.: 85.94%] [G loss: 3.633902]\n",
      "epoch:8 step:6490 [D loss: 0.252814, acc.: 90.62%] [G loss: 2.443905]\n",
      "epoch:8 step:6491 [D loss: 0.254349, acc.: 90.62%] [G loss: 2.685749]\n",
      "epoch:8 step:6492 [D loss: 0.250497, acc.: 89.84%] [G loss: 2.593415]\n",
      "epoch:8 step:6493 [D loss: 0.229608, acc.: 89.84%] [G loss: 3.801893]\n",
      "epoch:8 step:6494 [D loss: 0.215568, acc.: 95.31%] [G loss: 2.752255]\n",
      "epoch:8 step:6495 [D loss: 0.313250, acc.: 89.06%] [G loss: 4.815260]\n",
      "epoch:8 step:6496 [D loss: 0.254143, acc.: 90.62%] [G loss: 2.697326]\n",
      "epoch:8 step:6497 [D loss: 0.261754, acc.: 89.06%] [G loss: 3.566591]\n",
      "epoch:8 step:6498 [D loss: 0.223494, acc.: 89.06%] [G loss: 4.366512]\n",
      "epoch:8 step:6499 [D loss: 0.236195, acc.: 92.97%] [G loss: 3.851529]\n",
      "epoch:8 step:6500 [D loss: 0.286794, acc.: 87.50%] [G loss: 3.622759]\n",
      "epoch:8 step:6501 [D loss: 0.361368, acc.: 83.59%] [G loss: 2.682362]\n",
      "epoch:8 step:6502 [D loss: 0.201386, acc.: 95.31%] [G loss: 4.684043]\n",
      "epoch:8 step:6503 [D loss: 0.225935, acc.: 89.84%] [G loss: 6.465474]\n",
      "epoch:8 step:6504 [D loss: 0.274671, acc.: 88.28%] [G loss: 3.630815]\n",
      "epoch:8 step:6505 [D loss: 0.320527, acc.: 84.38%] [G loss: 3.421848]\n",
      "epoch:8 step:6506 [D loss: 0.393835, acc.: 85.16%] [G loss: 3.324132]\n",
      "epoch:8 step:6507 [D loss: 0.408503, acc.: 79.69%] [G loss: 2.848909]\n",
      "epoch:8 step:6508 [D loss: 0.315039, acc.: 88.28%] [G loss: 3.041436]\n",
      "epoch:8 step:6509 [D loss: 0.371912, acc.: 86.72%] [G loss: 3.584617]\n",
      "epoch:8 step:6510 [D loss: 0.454912, acc.: 83.59%] [G loss: 7.954865]\n",
      "epoch:8 step:6511 [D loss: 0.471375, acc.: 82.03%] [G loss: 5.959694]\n",
      "epoch:8 step:6512 [D loss: 0.538737, acc.: 76.56%] [G loss: 2.882496]\n",
      "epoch:8 step:6513 [D loss: 0.399804, acc.: 88.28%] [G loss: 2.988041]\n",
      "epoch:8 step:6514 [D loss: 0.491086, acc.: 80.47%] [G loss: 2.868464]\n",
      "epoch:8 step:6515 [D loss: 0.413659, acc.: 82.81%] [G loss: 2.930008]\n",
      "epoch:8 step:6516 [D loss: 0.399035, acc.: 83.59%] [G loss: 3.015779]\n",
      "epoch:8 step:6517 [D loss: 0.220195, acc.: 92.97%] [G loss: 3.238280]\n",
      "epoch:8 step:6518 [D loss: 0.390139, acc.: 86.72%] [G loss: 2.958090]\n",
      "epoch:8 step:6519 [D loss: 0.287405, acc.: 91.41%] [G loss: 2.718987]\n",
      "epoch:8 step:6520 [D loss: 0.318304, acc.: 89.06%] [G loss: 2.540643]\n",
      "epoch:8 step:6521 [D loss: 0.208793, acc.: 93.75%] [G loss: 3.213772]\n",
      "epoch:8 step:6522 [D loss: 0.325487, acc.: 82.81%] [G loss: 5.870448]\n",
      "epoch:8 step:6523 [D loss: 0.409908, acc.: 77.34%] [G loss: 2.929473]\n",
      "epoch:8 step:6524 [D loss: 0.212987, acc.: 93.75%] [G loss: 3.951744]\n",
      "epoch:8 step:6525 [D loss: 0.227595, acc.: 92.19%] [G loss: 3.583522]\n",
      "epoch:8 step:6526 [D loss: 0.368291, acc.: 80.47%] [G loss: 2.886964]\n",
      "epoch:8 step:6527 [D loss: 0.320766, acc.: 86.72%] [G loss: 2.934420]\n",
      "epoch:8 step:6528 [D loss: 0.315724, acc.: 88.28%] [G loss: 3.052784]\n",
      "epoch:8 step:6529 [D loss: 0.195657, acc.: 93.75%] [G loss: 4.263264]\n",
      "epoch:8 step:6530 [D loss: 0.277278, acc.: 89.06%] [G loss: 3.268797]\n",
      "epoch:8 step:6531 [D loss: 0.246434, acc.: 93.75%] [G loss: 2.498640]\n",
      "epoch:8 step:6532 [D loss: 0.296977, acc.: 88.28%] [G loss: 3.485228]\n",
      "epoch:8 step:6533 [D loss: 0.416997, acc.: 81.25%] [G loss: 4.495711]\n",
      "epoch:8 step:6534 [D loss: 0.449852, acc.: 83.59%] [G loss: 5.669686]\n",
      "epoch:8 step:6535 [D loss: 0.899137, acc.: 65.62%] [G loss: 2.509863]\n",
      "epoch:8 step:6536 [D loss: 0.248605, acc.: 89.84%] [G loss: 3.978369]\n",
      "epoch:8 step:6537 [D loss: 0.218497, acc.: 91.41%] [G loss: 3.458366]\n",
      "epoch:8 step:6538 [D loss: 0.289017, acc.: 87.50%] [G loss: 2.668354]\n",
      "epoch:8 step:6539 [D loss: 0.184686, acc.: 93.75%] [G loss: 3.779318]\n",
      "epoch:8 step:6540 [D loss: 0.296737, acc.: 89.84%] [G loss: 3.208863]\n",
      "epoch:8 step:6541 [D loss: 0.307677, acc.: 88.28%] [G loss: 2.888461]\n",
      "epoch:8 step:6542 [D loss: 0.290199, acc.: 88.28%] [G loss: 3.761269]\n",
      "epoch:8 step:6543 [D loss: 0.220525, acc.: 89.84%] [G loss: 4.802691]\n",
      "epoch:8 step:6544 [D loss: 0.202639, acc.: 93.75%] [G loss: 2.789937]\n",
      "epoch:8 step:6545 [D loss: 0.301222, acc.: 85.94%] [G loss: 2.797306]\n",
      "epoch:8 step:6546 [D loss: 0.223431, acc.: 90.62%] [G loss: 4.426201]\n",
      "epoch:8 step:6547 [D loss: 0.189921, acc.: 92.97%] [G loss: 3.939243]\n",
      "epoch:8 step:6548 [D loss: 0.379415, acc.: 81.25%] [G loss: 2.094601]\n",
      "epoch:8 step:6549 [D loss: 0.171665, acc.: 95.31%] [G loss: 3.839121]\n",
      "epoch:8 step:6550 [D loss: 0.255395, acc.: 90.62%] [G loss: 2.078802]\n",
      "epoch:8 step:6551 [D loss: 0.328222, acc.: 89.84%] [G loss: 3.500996]\n",
      "epoch:8 step:6552 [D loss: 0.190767, acc.: 93.75%] [G loss: 3.781235]\n",
      "epoch:8 step:6553 [D loss: 0.283947, acc.: 88.28%] [G loss: 2.351357]\n",
      "epoch:8 step:6554 [D loss: 0.219805, acc.: 93.75%] [G loss: 2.253060]\n",
      "epoch:8 step:6555 [D loss: 0.132031, acc.: 96.09%] [G loss: 5.437511]\n",
      "epoch:8 step:6556 [D loss: 0.213225, acc.: 89.06%] [G loss: 6.381764]\n",
      "epoch:8 step:6557 [D loss: 0.270176, acc.: 89.84%] [G loss: 2.663563]\n",
      "epoch:8 step:6558 [D loss: 0.223727, acc.: 93.75%] [G loss: 3.791113]\n",
      "epoch:8 step:6559 [D loss: 0.207672, acc.: 92.97%] [G loss: 3.830473]\n",
      "epoch:8 step:6560 [D loss: 0.255468, acc.: 92.19%] [G loss: 3.439500]\n",
      "epoch:8 step:6561 [D loss: 0.315036, acc.: 90.62%] [G loss: 2.812917]\n",
      "epoch:8 step:6562 [D loss: 0.182098, acc.: 95.31%] [G loss: 3.463526]\n",
      "epoch:8 step:6563 [D loss: 0.305634, acc.: 89.06%] [G loss: 4.545291]\n",
      "epoch:8 step:6564 [D loss: 0.202883, acc.: 94.53%] [G loss: 3.272535]\n",
      "epoch:8 step:6565 [D loss: 0.334164, acc.: 85.16%] [G loss: 2.688937]\n",
      "epoch:8 step:6566 [D loss: 0.304695, acc.: 89.84%] [G loss: 3.068670]\n",
      "epoch:8 step:6567 [D loss: 0.279048, acc.: 85.94%] [G loss: 2.361196]\n",
      "epoch:8 step:6568 [D loss: 0.341980, acc.: 85.94%] [G loss: 3.958051]\n",
      "epoch:8 step:6569 [D loss: 0.412880, acc.: 79.69%] [G loss: 3.075309]\n",
      "epoch:8 step:6570 [D loss: 0.276780, acc.: 85.16%] [G loss: 4.675881]\n",
      "epoch:8 step:6571 [D loss: 0.328712, acc.: 87.50%] [G loss: 2.262747]\n",
      "epoch:8 step:6572 [D loss: 0.343707, acc.: 80.47%] [G loss: 2.789276]\n",
      "epoch:8 step:6573 [D loss: 0.307419, acc.: 86.72%] [G loss: 2.425585]\n",
      "epoch:8 step:6574 [D loss: 0.398605, acc.: 83.59%] [G loss: 2.336545]\n",
      "epoch:8 step:6575 [D loss: 0.310790, acc.: 87.50%] [G loss: 2.715055]\n",
      "epoch:8 step:6576 [D loss: 0.320985, acc.: 85.94%] [G loss: 2.476736]\n",
      "epoch:8 step:6577 [D loss: 0.316192, acc.: 91.41%] [G loss: 2.774269]\n",
      "epoch:8 step:6578 [D loss: 0.331180, acc.: 87.50%] [G loss: 2.257273]\n",
      "epoch:8 step:6579 [D loss: 0.290617, acc.: 88.28%] [G loss: 2.996098]\n",
      "epoch:8 step:6580 [D loss: 0.355239, acc.: 90.62%] [G loss: 2.483772]\n",
      "epoch:8 step:6581 [D loss: 0.345905, acc.: 84.38%] [G loss: 3.062152]\n",
      "epoch:8 step:6582 [D loss: 0.355404, acc.: 85.16%] [G loss: 3.689579]\n",
      "epoch:8 step:6583 [D loss: 0.332453, acc.: 88.28%] [G loss: 3.435950]\n",
      "epoch:8 step:6584 [D loss: 0.271623, acc.: 82.81%] [G loss: 5.504022]\n",
      "epoch:8 step:6585 [D loss: 0.291233, acc.: 88.28%] [G loss: 4.113307]\n",
      "epoch:8 step:6586 [D loss: 0.244512, acc.: 89.06%] [G loss: 3.779021]\n",
      "epoch:8 step:6587 [D loss: 0.334397, acc.: 88.28%] [G loss: 3.875449]\n",
      "epoch:8 step:6588 [D loss: 0.357543, acc.: 82.81%] [G loss: 2.951513]\n",
      "epoch:8 step:6589 [D loss: 0.257221, acc.: 90.62%] [G loss: 2.787892]\n",
      "epoch:8 step:6590 [D loss: 0.225400, acc.: 94.53%] [G loss: 2.746940]\n",
      "epoch:8 step:6591 [D loss: 0.322309, acc.: 85.94%] [G loss: 2.895387]\n",
      "epoch:8 step:6592 [D loss: 0.239711, acc.: 89.06%] [G loss: 3.579795]\n",
      "epoch:8 step:6593 [D loss: 0.296800, acc.: 87.50%] [G loss: 2.544636]\n",
      "epoch:8 step:6594 [D loss: 0.252528, acc.: 92.97%] [G loss: 3.023179]\n",
      "epoch:8 step:6595 [D loss: 0.329102, acc.: 87.50%] [G loss: 3.499750]\n",
      "epoch:8 step:6596 [D loss: 0.427910, acc.: 81.25%] [G loss: 2.869117]\n",
      "epoch:8 step:6597 [D loss: 0.336919, acc.: 90.62%] [G loss: 2.458792]\n",
      "epoch:8 step:6598 [D loss: 0.352705, acc.: 85.16%] [G loss: 2.750105]\n",
      "epoch:8 step:6599 [D loss: 0.213837, acc.: 92.97%] [G loss: 5.667529]\n",
      "epoch:8 step:6600 [D loss: 0.438420, acc.: 78.12%] [G loss: 3.040440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6601 [D loss: 0.410115, acc.: 82.81%] [G loss: 3.545076]\n",
      "epoch:8 step:6602 [D loss: 0.379919, acc.: 88.28%] [G loss: 2.076917]\n",
      "epoch:8 step:6603 [D loss: 0.252095, acc.: 89.84%] [G loss: 3.691163]\n",
      "epoch:8 step:6604 [D loss: 0.317381, acc.: 85.16%] [G loss: 2.297872]\n",
      "epoch:8 step:6605 [D loss: 0.293938, acc.: 89.84%] [G loss: 2.912830]\n",
      "epoch:8 step:6606 [D loss: 0.345839, acc.: 84.38%] [G loss: 3.012816]\n",
      "epoch:8 step:6607 [D loss: 0.325223, acc.: 85.16%] [G loss: 2.942070]\n",
      "epoch:8 step:6608 [D loss: 0.360267, acc.: 88.28%] [G loss: 2.540839]\n",
      "epoch:8 step:6609 [D loss: 0.336119, acc.: 88.28%] [G loss: 3.415099]\n",
      "epoch:8 step:6610 [D loss: 0.402462, acc.: 83.59%] [G loss: 5.468022]\n",
      "epoch:8 step:6611 [D loss: 0.594196, acc.: 74.22%] [G loss: 3.415162]\n",
      "epoch:8 step:6612 [D loss: 0.457421, acc.: 77.34%] [G loss: 2.440700]\n",
      "epoch:8 step:6613 [D loss: 0.347738, acc.: 89.06%] [G loss: 2.159815]\n",
      "epoch:8 step:6614 [D loss: 0.340380, acc.: 88.28%] [G loss: 2.173233]\n",
      "epoch:8 step:6615 [D loss: 0.342260, acc.: 88.28%] [G loss: 3.280488]\n",
      "epoch:8 step:6616 [D loss: 0.301895, acc.: 85.16%] [G loss: 3.256650]\n",
      "epoch:8 step:6617 [D loss: 0.293720, acc.: 89.84%] [G loss: 4.892966]\n",
      "epoch:8 step:6618 [D loss: 0.439066, acc.: 84.38%] [G loss: 2.097369]\n",
      "epoch:8 step:6619 [D loss: 0.329382, acc.: 89.06%] [G loss: 2.946412]\n",
      "epoch:8 step:6620 [D loss: 0.292504, acc.: 87.50%] [G loss: 4.247935]\n",
      "epoch:8 step:6621 [D loss: 0.335944, acc.: 85.94%] [G loss: 4.434649]\n",
      "epoch:8 step:6622 [D loss: 0.265266, acc.: 88.28%] [G loss: 3.524875]\n",
      "epoch:8 step:6623 [D loss: 0.333491, acc.: 84.38%] [G loss: 2.626080]\n",
      "epoch:8 step:6624 [D loss: 0.264105, acc.: 92.97%] [G loss: 2.590649]\n",
      "epoch:8 step:6625 [D loss: 0.342118, acc.: 85.94%] [G loss: 2.714168]\n",
      "epoch:8 step:6626 [D loss: 0.308203, acc.: 88.28%] [G loss: 2.516457]\n",
      "epoch:8 step:6627 [D loss: 0.231675, acc.: 92.97%] [G loss: 4.374123]\n",
      "epoch:8 step:6628 [D loss: 0.316791, acc.: 89.06%] [G loss: 3.028969]\n",
      "epoch:8 step:6629 [D loss: 0.262976, acc.: 88.28%] [G loss: 2.618116]\n",
      "epoch:8 step:6630 [D loss: 0.451333, acc.: 75.78%] [G loss: 2.049730]\n",
      "epoch:8 step:6631 [D loss: 0.246917, acc.: 91.41%] [G loss: 3.355771]\n",
      "epoch:8 step:6632 [D loss: 0.295793, acc.: 89.06%] [G loss: 4.744089]\n",
      "epoch:8 step:6633 [D loss: 0.170916, acc.: 94.53%] [G loss: 4.077704]\n",
      "epoch:8 step:6634 [D loss: 0.246746, acc.: 90.62%] [G loss: 2.818918]\n",
      "epoch:8 step:6635 [D loss: 0.193402, acc.: 95.31%] [G loss: 3.883147]\n",
      "epoch:8 step:6636 [D loss: 0.221072, acc.: 93.75%] [G loss: 2.763793]\n",
      "epoch:8 step:6637 [D loss: 0.373637, acc.: 83.59%] [G loss: 1.695177]\n",
      "epoch:8 step:6638 [D loss: 0.409119, acc.: 78.91%] [G loss: 2.597117]\n",
      "epoch:8 step:6639 [D loss: 0.350537, acc.: 85.94%] [G loss: 3.431803]\n",
      "epoch:8 step:6640 [D loss: 0.265369, acc.: 87.50%] [G loss: 3.246483]\n",
      "epoch:8 step:6641 [D loss: 0.253130, acc.: 89.84%] [G loss: 2.810097]\n",
      "epoch:8 step:6642 [D loss: 0.351004, acc.: 83.59%] [G loss: 2.598870]\n",
      "epoch:8 step:6643 [D loss: 0.364640, acc.: 82.81%] [G loss: 3.102499]\n",
      "epoch:8 step:6644 [D loss: 0.374240, acc.: 85.94%] [G loss: 2.873324]\n",
      "epoch:8 step:6645 [D loss: 0.329658, acc.: 86.72%] [G loss: 3.150886]\n",
      "epoch:8 step:6646 [D loss: 0.366772, acc.: 89.06%] [G loss: 3.389522]\n",
      "epoch:8 step:6647 [D loss: 0.305304, acc.: 83.59%] [G loss: 4.449525]\n",
      "epoch:8 step:6648 [D loss: 0.243188, acc.: 93.75%] [G loss: 3.193905]\n",
      "epoch:8 step:6649 [D loss: 0.200754, acc.: 92.19%] [G loss: 3.206501]\n",
      "epoch:8 step:6650 [D loss: 0.285027, acc.: 88.28%] [G loss: 2.579410]\n",
      "epoch:8 step:6651 [D loss: 0.315290, acc.: 88.28%] [G loss: 2.512769]\n",
      "epoch:8 step:6652 [D loss: 0.281719, acc.: 87.50%] [G loss: 3.814829]\n",
      "epoch:8 step:6653 [D loss: 0.212390, acc.: 91.41%] [G loss: 4.334124]\n",
      "epoch:8 step:6654 [D loss: 0.346745, acc.: 87.50%] [G loss: 2.153388]\n",
      "epoch:8 step:6655 [D loss: 0.342067, acc.: 88.28%] [G loss: 1.888246]\n",
      "epoch:8 step:6656 [D loss: 0.288160, acc.: 85.94%] [G loss: 3.010787]\n",
      "epoch:8 step:6657 [D loss: 0.336723, acc.: 85.94%] [G loss: 2.615546]\n",
      "epoch:8 step:6658 [D loss: 0.309440, acc.: 89.84%] [G loss: 3.240200]\n",
      "epoch:8 step:6659 [D loss: 0.514088, acc.: 75.78%] [G loss: 3.285899]\n",
      "epoch:8 step:6660 [D loss: 0.444144, acc.: 83.59%] [G loss: 6.977104]\n",
      "epoch:8 step:6661 [D loss: 1.070445, acc.: 66.41%] [G loss: 3.622354]\n",
      "epoch:8 step:6662 [D loss: 0.626983, acc.: 77.34%] [G loss: 3.176102]\n",
      "epoch:8 step:6663 [D loss: 0.289302, acc.: 90.62%] [G loss: 2.692011]\n",
      "epoch:8 step:6664 [D loss: 0.511696, acc.: 74.22%] [G loss: 5.020392]\n",
      "epoch:8 step:6665 [D loss: 0.432454, acc.: 88.28%] [G loss: 3.151707]\n",
      "epoch:8 step:6666 [D loss: 0.262847, acc.: 88.28%] [G loss: 4.637293]\n",
      "epoch:8 step:6667 [D loss: 0.390300, acc.: 83.59%] [G loss: 2.701611]\n",
      "epoch:8 step:6668 [D loss: 0.278448, acc.: 89.84%] [G loss: 3.557913]\n",
      "epoch:8 step:6669 [D loss: 0.206919, acc.: 91.41%] [G loss: 3.621904]\n",
      "epoch:8 step:6670 [D loss: 0.354068, acc.: 87.50%] [G loss: 4.724423]\n",
      "epoch:8 step:6671 [D loss: 0.278050, acc.: 86.72%] [G loss: 4.684667]\n",
      "epoch:8 step:6672 [D loss: 0.320808, acc.: 85.16%] [G loss: 3.231393]\n",
      "epoch:8 step:6673 [D loss: 0.301326, acc.: 87.50%] [G loss: 2.749972]\n",
      "epoch:8 step:6674 [D loss: 0.390113, acc.: 82.03%] [G loss: 2.935032]\n",
      "epoch:8 step:6675 [D loss: 0.258335, acc.: 87.50%] [G loss: 2.719833]\n",
      "epoch:8 step:6676 [D loss: 0.427596, acc.: 79.69%] [G loss: 2.961222]\n",
      "epoch:8 step:6677 [D loss: 0.437522, acc.: 82.03%] [G loss: 3.884697]\n",
      "epoch:8 step:6678 [D loss: 0.373320, acc.: 82.81%] [G loss: 3.669293]\n",
      "epoch:8 step:6679 [D loss: 0.316903, acc.: 86.72%] [G loss: 3.236654]\n",
      "epoch:8 step:6680 [D loss: 0.495785, acc.: 75.78%] [G loss: 4.115115]\n",
      "epoch:8 step:6681 [D loss: 0.358189, acc.: 82.81%] [G loss: 2.995821]\n",
      "epoch:8 step:6682 [D loss: 0.407055, acc.: 85.94%] [G loss: 4.338522]\n",
      "epoch:8 step:6683 [D loss: 0.464430, acc.: 85.16%] [G loss: 6.257006]\n",
      "epoch:8 step:6684 [D loss: 0.922538, acc.: 68.75%] [G loss: 4.984606]\n",
      "epoch:8 step:6685 [D loss: 1.002538, acc.: 67.19%] [G loss: 4.717144]\n",
      "epoch:8 step:6686 [D loss: 0.737020, acc.: 68.75%] [G loss: 2.378428]\n",
      "epoch:8 step:6687 [D loss: 0.712147, acc.: 67.19%] [G loss: 2.396880]\n",
      "epoch:8 step:6688 [D loss: 0.646242, acc.: 82.03%] [G loss: 5.923480]\n",
      "epoch:8 step:6689 [D loss: 0.676246, acc.: 75.00%] [G loss: 2.034582]\n",
      "epoch:8 step:6690 [D loss: 0.384347, acc.: 82.81%] [G loss: 2.345251]\n",
      "epoch:8 step:6691 [D loss: 0.365932, acc.: 85.16%] [G loss: 3.477521]\n",
      "epoch:8 step:6692 [D loss: 0.321190, acc.: 85.16%] [G loss: 2.295119]\n",
      "epoch:8 step:6693 [D loss: 0.344958, acc.: 86.72%] [G loss: 2.961379]\n",
      "epoch:8 step:6694 [D loss: 0.328207, acc.: 87.50%] [G loss: 2.228015]\n",
      "epoch:8 step:6695 [D loss: 0.288450, acc.: 90.62%] [G loss: 2.434879]\n",
      "epoch:8 step:6696 [D loss: 0.445647, acc.: 79.69%] [G loss: 2.196659]\n",
      "epoch:8 step:6697 [D loss: 0.393122, acc.: 82.81%] [G loss: 2.490545]\n",
      "epoch:8 step:6698 [D loss: 0.237292, acc.: 90.62%] [G loss: 2.580109]\n",
      "epoch:8 step:6699 [D loss: 0.218597, acc.: 91.41%] [G loss: 2.990312]\n",
      "epoch:8 step:6700 [D loss: 0.287569, acc.: 87.50%] [G loss: 2.082685]\n",
      "epoch:8 step:6701 [D loss: 0.245744, acc.: 89.84%] [G loss: 2.582880]\n",
      "epoch:8 step:6702 [D loss: 0.265519, acc.: 85.16%] [G loss: 2.949770]\n",
      "epoch:8 step:6703 [D loss: 0.380670, acc.: 83.59%] [G loss: 1.980469]\n",
      "epoch:8 step:6704 [D loss: 0.421628, acc.: 80.47%] [G loss: 2.194680]\n",
      "epoch:8 step:6705 [D loss: 0.230040, acc.: 89.06%] [G loss: 3.995245]\n",
      "epoch:8 step:6706 [D loss: 0.197367, acc.: 92.97%] [G loss: 3.693073]\n",
      "epoch:8 step:6707 [D loss: 0.246709, acc.: 89.84%] [G loss: 3.237755]\n",
      "epoch:8 step:6708 [D loss: 0.336913, acc.: 85.94%] [G loss: 2.029745]\n",
      "epoch:8 step:6709 [D loss: 0.384889, acc.: 86.72%] [G loss: 2.083196]\n",
      "epoch:8 step:6710 [D loss: 0.221191, acc.: 91.41%] [G loss: 3.593933]\n",
      "epoch:8 step:6711 [D loss: 0.295716, acc.: 88.28%] [G loss: 3.737492]\n",
      "epoch:8 step:6712 [D loss: 0.300077, acc.: 91.41%] [G loss: 2.348980]\n",
      "epoch:8 step:6713 [D loss: 0.308785, acc.: 87.50%] [G loss: 2.468087]\n",
      "epoch:8 step:6714 [D loss: 0.412368, acc.: 78.12%] [G loss: 1.784949]\n",
      "epoch:8 step:6715 [D loss: 0.364213, acc.: 85.16%] [G loss: 2.388518]\n",
      "epoch:8 step:6716 [D loss: 0.390722, acc.: 78.12%] [G loss: 3.292663]\n",
      "epoch:8 step:6717 [D loss: 0.300094, acc.: 87.50%] [G loss: 3.243323]\n",
      "epoch:8 step:6718 [D loss: 0.297902, acc.: 86.72%] [G loss: 2.500849]\n",
      "epoch:8 step:6719 [D loss: 0.332669, acc.: 89.06%] [G loss: 2.720259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6720 [D loss: 0.474960, acc.: 74.22%] [G loss: 2.028845]\n",
      "epoch:8 step:6721 [D loss: 0.311663, acc.: 89.06%] [G loss: 2.234407]\n",
      "epoch:8 step:6722 [D loss: 0.319700, acc.: 86.72%] [G loss: 2.488722]\n",
      "epoch:8 step:6723 [D loss: 0.231037, acc.: 92.19%] [G loss: 3.738078]\n",
      "epoch:8 step:6724 [D loss: 0.304496, acc.: 86.72%] [G loss: 1.974981]\n",
      "epoch:8 step:6725 [D loss: 0.357384, acc.: 82.03%] [G loss: 2.139409]\n",
      "epoch:8 step:6726 [D loss: 0.289277, acc.: 87.50%] [G loss: 3.119907]\n",
      "epoch:8 step:6727 [D loss: 0.386726, acc.: 88.28%] [G loss: 3.747351]\n",
      "epoch:8 step:6728 [D loss: 0.283599, acc.: 86.72%] [G loss: 2.881410]\n",
      "epoch:8 step:6729 [D loss: 0.281642, acc.: 85.94%] [G loss: 4.579871]\n",
      "epoch:8 step:6730 [D loss: 0.412663, acc.: 82.81%] [G loss: 3.576801]\n",
      "epoch:8 step:6731 [D loss: 0.411877, acc.: 78.12%] [G loss: 2.634256]\n",
      "epoch:8 step:6732 [D loss: 0.197070, acc.: 92.19%] [G loss: 4.980562]\n",
      "epoch:8 step:6733 [D loss: 0.173914, acc.: 94.53%] [G loss: 3.478591]\n",
      "epoch:8 step:6734 [D loss: 0.273069, acc.: 92.97%] [G loss: 2.406574]\n",
      "epoch:8 step:6735 [D loss: 0.279579, acc.: 88.28%] [G loss: 2.889918]\n",
      "epoch:8 step:6736 [D loss: 0.273155, acc.: 87.50%] [G loss: 3.793292]\n",
      "epoch:8 step:6737 [D loss: 0.478467, acc.: 78.91%] [G loss: 2.210337]\n",
      "epoch:8 step:6738 [D loss: 0.204239, acc.: 94.53%] [G loss: 5.007978]\n",
      "epoch:8 step:6739 [D loss: 0.216170, acc.: 89.84%] [G loss: 6.575278]\n",
      "epoch:8 step:6740 [D loss: 0.218566, acc.: 92.97%] [G loss: 8.810392]\n",
      "epoch:8 step:6741 [D loss: 0.177620, acc.: 92.19%] [G loss: 7.089302]\n",
      "epoch:8 step:6742 [D loss: 0.354893, acc.: 83.59%] [G loss: 3.174894]\n",
      "epoch:8 step:6743 [D loss: 0.217577, acc.: 93.75%] [G loss: 3.008841]\n",
      "epoch:8 step:6744 [D loss: 0.277486, acc.: 91.41%] [G loss: 2.804910]\n",
      "epoch:8 step:6745 [D loss: 0.333534, acc.: 89.84%] [G loss: 2.444563]\n",
      "epoch:8 step:6746 [D loss: 0.157788, acc.: 95.31%] [G loss: 3.634102]\n",
      "epoch:8 step:6747 [D loss: 0.330967, acc.: 85.94%] [G loss: 2.756085]\n",
      "epoch:8 step:6748 [D loss: 0.234184, acc.: 91.41%] [G loss: 3.574531]\n",
      "epoch:8 step:6749 [D loss: 0.229849, acc.: 92.97%] [G loss: 2.631320]\n",
      "epoch:8 step:6750 [D loss: 0.294880, acc.: 91.41%] [G loss: 1.773843]\n",
      "epoch:8 step:6751 [D loss: 0.395052, acc.: 86.72%] [G loss: 2.143388]\n",
      "epoch:8 step:6752 [D loss: 0.406236, acc.: 81.25%] [G loss: 2.775873]\n",
      "epoch:8 step:6753 [D loss: 0.472778, acc.: 82.81%] [G loss: 3.991303]\n",
      "epoch:8 step:6754 [D loss: 0.438869, acc.: 83.59%] [G loss: 5.019166]\n",
      "epoch:8 step:6755 [D loss: 0.530612, acc.: 75.78%] [G loss: 2.587036]\n",
      "epoch:8 step:6756 [D loss: 0.468707, acc.: 84.38%] [G loss: 2.484522]\n",
      "epoch:8 step:6757 [D loss: 0.253963, acc.: 89.06%] [G loss: 4.079526]\n",
      "epoch:8 step:6758 [D loss: 0.342812, acc.: 85.16%] [G loss: 3.279478]\n",
      "epoch:8 step:6759 [D loss: 0.360493, acc.: 87.50%] [G loss: 3.118615]\n",
      "epoch:8 step:6760 [D loss: 0.228371, acc.: 92.97%] [G loss: 4.402310]\n",
      "epoch:8 step:6761 [D loss: 0.242988, acc.: 88.28%] [G loss: 4.053048]\n",
      "epoch:8 step:6762 [D loss: 0.261122, acc.: 91.41%] [G loss: 2.221670]\n",
      "epoch:8 step:6763 [D loss: 0.319886, acc.: 85.16%] [G loss: 2.471849]\n",
      "epoch:8 step:6764 [D loss: 0.229636, acc.: 91.41%] [G loss: 2.563590]\n",
      "epoch:8 step:6765 [D loss: 0.313067, acc.: 85.94%] [G loss: 2.410347]\n",
      "epoch:8 step:6766 [D loss: 0.347407, acc.: 84.38%] [G loss: 3.094859]\n",
      "epoch:8 step:6767 [D loss: 0.198827, acc.: 91.41%] [G loss: 4.658499]\n",
      "epoch:8 step:6768 [D loss: 0.344227, acc.: 83.59%] [G loss: 4.398872]\n",
      "epoch:8 step:6769 [D loss: 0.291986, acc.: 88.28%] [G loss: 3.016308]\n",
      "epoch:8 step:6770 [D loss: 0.295236, acc.: 85.94%] [G loss: 4.718648]\n",
      "epoch:8 step:6771 [D loss: 0.284976, acc.: 89.06%] [G loss: 3.829537]\n",
      "epoch:8 step:6772 [D loss: 0.237965, acc.: 92.19%] [G loss: 3.140587]\n",
      "epoch:8 step:6773 [D loss: 0.302354, acc.: 90.62%] [G loss: 2.780695]\n",
      "epoch:8 step:6774 [D loss: 0.384080, acc.: 82.03%] [G loss: 2.694544]\n",
      "epoch:8 step:6775 [D loss: 0.253583, acc.: 93.75%] [G loss: 3.860719]\n",
      "epoch:8 step:6776 [D loss: 0.218162, acc.: 89.84%] [G loss: 3.327157]\n",
      "epoch:8 step:6777 [D loss: 0.334722, acc.: 84.38%] [G loss: 1.701245]\n",
      "epoch:8 step:6778 [D loss: 0.282810, acc.: 86.72%] [G loss: 4.301504]\n",
      "epoch:8 step:6779 [D loss: 0.272314, acc.: 88.28%] [G loss: 3.654747]\n",
      "epoch:8 step:6780 [D loss: 0.293898, acc.: 87.50%] [G loss: 3.090977]\n",
      "epoch:8 step:6781 [D loss: 0.359596, acc.: 81.25%] [G loss: 3.101243]\n",
      "epoch:8 step:6782 [D loss: 0.237922, acc.: 90.62%] [G loss: 3.884976]\n",
      "epoch:8 step:6783 [D loss: 0.347592, acc.: 81.25%] [G loss: 5.139047]\n",
      "epoch:8 step:6784 [D loss: 0.304226, acc.: 85.94%] [G loss: 2.876745]\n",
      "epoch:8 step:6785 [D loss: 0.328067, acc.: 89.84%] [G loss: 2.862613]\n",
      "epoch:8 step:6786 [D loss: 0.227817, acc.: 92.97%] [G loss: 3.766214]\n",
      "epoch:8 step:6787 [D loss: 0.209053, acc.: 92.97%] [G loss: 4.237971]\n",
      "epoch:8 step:6788 [D loss: 0.329579, acc.: 87.50%] [G loss: 3.904860]\n",
      "epoch:8 step:6789 [D loss: 0.284174, acc.: 87.50%] [G loss: 2.435580]\n",
      "epoch:8 step:6790 [D loss: 0.353708, acc.: 85.16%] [G loss: 2.450370]\n",
      "epoch:8 step:6791 [D loss: 0.325097, acc.: 85.16%] [G loss: 2.856365]\n",
      "epoch:8 step:6792 [D loss: 0.256065, acc.: 91.41%] [G loss: 2.518026]\n",
      "epoch:8 step:6793 [D loss: 0.232492, acc.: 91.41%] [G loss: 3.790633]\n",
      "epoch:8 step:6794 [D loss: 0.306439, acc.: 85.94%] [G loss: 2.711332]\n",
      "epoch:8 step:6795 [D loss: 0.279843, acc.: 89.84%] [G loss: 2.760140]\n",
      "epoch:8 step:6796 [D loss: 0.269261, acc.: 89.06%] [G loss: 1.957569]\n",
      "epoch:8 step:6797 [D loss: 0.467788, acc.: 82.03%] [G loss: 2.161738]\n",
      "epoch:8 step:6798 [D loss: 0.329560, acc.: 85.94%] [G loss: 2.443937]\n",
      "epoch:8 step:6799 [D loss: 0.248403, acc.: 90.62%] [G loss: 3.349568]\n",
      "epoch:8 step:6800 [D loss: 0.303013, acc.: 90.62%] [G loss: 3.812385]\n",
      "epoch:8 step:6801 [D loss: 0.229230, acc.: 91.41%] [G loss: 6.028433]\n",
      "epoch:8 step:6802 [D loss: 0.289133, acc.: 88.28%] [G loss: 4.066020]\n",
      "epoch:8 step:6803 [D loss: 0.360977, acc.: 78.91%] [G loss: 3.380078]\n",
      "epoch:8 step:6804 [D loss: 0.225208, acc.: 90.62%] [G loss: 5.695780]\n",
      "epoch:8 step:6805 [D loss: 0.256443, acc.: 88.28%] [G loss: 4.137386]\n",
      "epoch:8 step:6806 [D loss: 0.335492, acc.: 85.16%] [G loss: 4.430645]\n",
      "epoch:8 step:6807 [D loss: 0.246627, acc.: 92.19%] [G loss: 5.204369]\n",
      "epoch:8 step:6808 [D loss: 0.225407, acc.: 90.62%] [G loss: 3.813948]\n",
      "epoch:8 step:6809 [D loss: 0.321797, acc.: 89.06%] [G loss: 2.074855]\n",
      "epoch:8 step:6810 [D loss: 0.321447, acc.: 87.50%] [G loss: 2.471604]\n",
      "epoch:8 step:6811 [D loss: 0.320531, acc.: 86.72%] [G loss: 2.919849]\n",
      "epoch:8 step:6812 [D loss: 0.285186, acc.: 85.94%] [G loss: 2.601531]\n",
      "epoch:8 step:6813 [D loss: 0.277540, acc.: 90.62%] [G loss: 3.450492]\n",
      "epoch:8 step:6814 [D loss: 0.397034, acc.: 86.72%] [G loss: 3.030938]\n",
      "epoch:8 step:6815 [D loss: 0.346368, acc.: 89.06%] [G loss: 2.743466]\n",
      "epoch:8 step:6816 [D loss: 0.286538, acc.: 89.84%] [G loss: 2.757426]\n",
      "epoch:8 step:6817 [D loss: 0.266071, acc.: 89.84%] [G loss: 3.456590]\n",
      "epoch:8 step:6818 [D loss: 0.314522, acc.: 89.84%] [G loss: 2.700816]\n",
      "epoch:8 step:6819 [D loss: 0.407704, acc.: 80.47%] [G loss: 2.664163]\n",
      "epoch:8 step:6820 [D loss: 0.204250, acc.: 92.19%] [G loss: 4.039424]\n",
      "epoch:8 step:6821 [D loss: 0.319022, acc.: 84.38%] [G loss: 3.021765]\n",
      "epoch:8 step:6822 [D loss: 0.253623, acc.: 89.84%] [G loss: 3.982040]\n",
      "epoch:8 step:6823 [D loss: 0.343606, acc.: 88.28%] [G loss: 2.843691]\n",
      "epoch:8 step:6824 [D loss: 0.347002, acc.: 85.94%] [G loss: 2.378989]\n",
      "epoch:8 step:6825 [D loss: 0.277073, acc.: 90.62%] [G loss: 2.484668]\n",
      "epoch:8 step:6826 [D loss: 0.336040, acc.: 84.38%] [G loss: 3.247511]\n",
      "epoch:8 step:6827 [D loss: 0.258769, acc.: 89.84%] [G loss: 5.120326]\n",
      "epoch:8 step:6828 [D loss: 0.334925, acc.: 87.50%] [G loss: 2.426849]\n",
      "epoch:8 step:6829 [D loss: 0.308664, acc.: 89.06%] [G loss: 2.648132]\n",
      "epoch:8 step:6830 [D loss: 0.356214, acc.: 84.38%] [G loss: 2.612942]\n",
      "epoch:8 step:6831 [D loss: 0.427157, acc.: 81.25%] [G loss: 4.206435]\n",
      "epoch:8 step:6832 [D loss: 0.529391, acc.: 77.34%] [G loss: 5.740087]\n",
      "epoch:8 step:6833 [D loss: 0.374570, acc.: 84.38%] [G loss: 4.328039]\n",
      "epoch:8 step:6834 [D loss: 0.305450, acc.: 82.81%] [G loss: 3.208974]\n",
      "epoch:8 step:6835 [D loss: 0.396581, acc.: 85.94%] [G loss: 2.931176]\n",
      "epoch:8 step:6836 [D loss: 0.146290, acc.: 96.88%] [G loss: 3.543328]\n",
      "epoch:8 step:6837 [D loss: 0.345413, acc.: 86.72%] [G loss: 3.676879]\n",
      "epoch:8 step:6838 [D loss: 0.277209, acc.: 92.19%] [G loss: 2.776964]\n",
      "epoch:8 step:6839 [D loss: 0.329532, acc.: 89.06%] [G loss: 2.205640]\n",
      "epoch:8 step:6840 [D loss: 0.249192, acc.: 89.06%] [G loss: 4.488336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6841 [D loss: 0.247656, acc.: 87.50%] [G loss: 3.512326]\n",
      "epoch:8 step:6842 [D loss: 0.344941, acc.: 86.72%] [G loss: 2.773161]\n",
      "epoch:8 step:6843 [D loss: 0.445807, acc.: 82.03%] [G loss: 4.966747]\n",
      "epoch:8 step:6844 [D loss: 0.453375, acc.: 82.81%] [G loss: 3.983858]\n",
      "epoch:8 step:6845 [D loss: 0.490348, acc.: 79.69%] [G loss: 2.959553]\n",
      "epoch:8 step:6846 [D loss: 0.544819, acc.: 71.88%] [G loss: 2.495993]\n",
      "epoch:8 step:6847 [D loss: 0.374594, acc.: 82.81%] [G loss: 2.269414]\n",
      "epoch:8 step:6848 [D loss: 0.338907, acc.: 85.16%] [G loss: 3.145195]\n",
      "epoch:8 step:6849 [D loss: 0.324003, acc.: 84.38%] [G loss: 3.358882]\n",
      "epoch:8 step:6850 [D loss: 0.286599, acc.: 91.41%] [G loss: 3.619836]\n",
      "epoch:8 step:6851 [D loss: 0.234846, acc.: 89.84%] [G loss: 5.811878]\n",
      "epoch:8 step:6852 [D loss: 0.229833, acc.: 89.06%] [G loss: 7.858362]\n",
      "epoch:8 step:6853 [D loss: 0.250803, acc.: 86.72%] [G loss: 5.839961]\n",
      "epoch:8 step:6854 [D loss: 0.382486, acc.: 81.25%] [G loss: 2.483083]\n",
      "epoch:8 step:6855 [D loss: 0.223502, acc.: 94.53%] [G loss: 2.887489]\n",
      "epoch:8 step:6856 [D loss: 0.360634, acc.: 85.16%] [G loss: 2.527203]\n",
      "epoch:8 step:6857 [D loss: 0.226968, acc.: 89.84%] [G loss: 3.825164]\n",
      "epoch:8 step:6858 [D loss: 0.294272, acc.: 88.28%] [G loss: 2.995065]\n",
      "epoch:8 step:6859 [D loss: 0.232243, acc.: 92.19%] [G loss: 1.949664]\n",
      "epoch:8 step:6860 [D loss: 0.287602, acc.: 89.06%] [G loss: 3.005258]\n",
      "epoch:8 step:6861 [D loss: 0.272938, acc.: 88.28%] [G loss: 3.878173]\n",
      "epoch:8 step:6862 [D loss: 0.314929, acc.: 84.38%] [G loss: 2.849744]\n",
      "epoch:8 step:6863 [D loss: 0.342281, acc.: 86.72%] [G loss: 2.449604]\n",
      "epoch:8 step:6864 [D loss: 0.252945, acc.: 88.28%] [G loss: 3.038654]\n",
      "epoch:8 step:6865 [D loss: 0.350697, acc.: 83.59%] [G loss: 3.421824]\n",
      "epoch:8 step:6866 [D loss: 0.377913, acc.: 84.38%] [G loss: 3.055389]\n",
      "epoch:8 step:6867 [D loss: 0.315206, acc.: 86.72%] [G loss: 3.294263]\n",
      "epoch:8 step:6868 [D loss: 0.233371, acc.: 90.62%] [G loss: 3.667599]\n",
      "epoch:8 step:6869 [D loss: 0.342630, acc.: 82.03%] [G loss: 3.453282]\n",
      "epoch:8 step:6870 [D loss: 0.348007, acc.: 86.72%] [G loss: 2.008633]\n",
      "epoch:8 step:6871 [D loss: 0.321385, acc.: 88.28%] [G loss: 2.299178]\n",
      "epoch:8 step:6872 [D loss: 0.352032, acc.: 83.59%] [G loss: 3.040174]\n",
      "epoch:8 step:6873 [D loss: 0.323388, acc.: 92.19%] [G loss: 2.703934]\n",
      "epoch:8 step:6874 [D loss: 0.302468, acc.: 89.06%] [G loss: 2.347540]\n",
      "epoch:8 step:6875 [D loss: 0.248179, acc.: 92.97%] [G loss: 3.052659]\n",
      "epoch:8 step:6876 [D loss: 0.413566, acc.: 84.38%] [G loss: 5.321341]\n",
      "epoch:8 step:6877 [D loss: 0.353954, acc.: 87.50%] [G loss: 6.580106]\n",
      "epoch:8 step:6878 [D loss: 0.372861, acc.: 84.38%] [G loss: 3.780469]\n",
      "epoch:8 step:6879 [D loss: 0.285431, acc.: 87.50%] [G loss: 4.574084]\n",
      "epoch:8 step:6880 [D loss: 0.308248, acc.: 89.06%] [G loss: 3.146667]\n",
      "epoch:8 step:6881 [D loss: 0.375751, acc.: 85.94%] [G loss: 3.188818]\n",
      "epoch:8 step:6882 [D loss: 0.330641, acc.: 84.38%] [G loss: 3.058402]\n",
      "epoch:8 step:6883 [D loss: 0.437693, acc.: 80.47%] [G loss: 2.181602]\n",
      "epoch:8 step:6884 [D loss: 0.359234, acc.: 85.16%] [G loss: 2.987314]\n",
      "epoch:8 step:6885 [D loss: 0.281242, acc.: 89.06%] [G loss: 3.386814]\n",
      "epoch:8 step:6886 [D loss: 0.353894, acc.: 84.38%] [G loss: 3.378351]\n",
      "epoch:8 step:6887 [D loss: 0.280161, acc.: 89.06%] [G loss: 2.211637]\n",
      "epoch:8 step:6888 [D loss: 0.273358, acc.: 90.62%] [G loss: 2.754214]\n",
      "epoch:8 step:6889 [D loss: 0.270791, acc.: 90.62%] [G loss: 2.576741]\n",
      "epoch:8 step:6890 [D loss: 0.334559, acc.: 87.50%] [G loss: 2.761847]\n",
      "epoch:8 step:6891 [D loss: 0.248037, acc.: 88.28%] [G loss: 3.267395]\n",
      "epoch:8 step:6892 [D loss: 0.355669, acc.: 85.16%] [G loss: 2.463954]\n",
      "epoch:8 step:6893 [D loss: 0.281686, acc.: 91.41%] [G loss: 2.420325]\n",
      "epoch:8 step:6894 [D loss: 0.436426, acc.: 77.34%] [G loss: 3.302585]\n",
      "epoch:8 step:6895 [D loss: 0.368408, acc.: 86.72%] [G loss: 2.411085]\n",
      "epoch:8 step:6896 [D loss: 0.389160, acc.: 83.59%] [G loss: 2.428118]\n",
      "epoch:8 step:6897 [D loss: 0.332521, acc.: 85.16%] [G loss: 2.731933]\n",
      "epoch:8 step:6898 [D loss: 0.237993, acc.: 92.97%] [G loss: 3.349373]\n",
      "epoch:8 step:6899 [D loss: 0.217575, acc.: 92.97%] [G loss: 2.323778]\n",
      "epoch:8 step:6900 [D loss: 0.330635, acc.: 92.97%] [G loss: 2.308840]\n",
      "epoch:8 step:6901 [D loss: 0.283599, acc.: 90.62%] [G loss: 2.567563]\n",
      "epoch:8 step:6902 [D loss: 0.292183, acc.: 86.72%] [G loss: 2.831124]\n",
      "epoch:8 step:6903 [D loss: 0.270707, acc.: 92.19%] [G loss: 2.960940]\n",
      "epoch:8 step:6904 [D loss: 0.220944, acc.: 89.84%] [G loss: 2.987587]\n",
      "epoch:8 step:6905 [D loss: 0.318541, acc.: 87.50%] [G loss: 3.671012]\n",
      "epoch:8 step:6906 [D loss: 0.208547, acc.: 91.41%] [G loss: 4.801897]\n",
      "epoch:8 step:6907 [D loss: 0.307732, acc.: 86.72%] [G loss: 2.227191]\n",
      "epoch:8 step:6908 [D loss: 0.278376, acc.: 91.41%] [G loss: 2.068726]\n",
      "epoch:8 step:6909 [D loss: 0.356405, acc.: 87.50%] [G loss: 2.420005]\n",
      "epoch:8 step:6910 [D loss: 0.359376, acc.: 82.81%] [G loss: 3.399696]\n",
      "epoch:8 step:6911 [D loss: 0.290480, acc.: 90.62%] [G loss: 3.462317]\n",
      "epoch:8 step:6912 [D loss: 0.273978, acc.: 89.84%] [G loss: 3.387180]\n",
      "epoch:8 step:6913 [D loss: 0.171885, acc.: 94.53%] [G loss: 3.498349]\n",
      "epoch:8 step:6914 [D loss: 0.217733, acc.: 90.62%] [G loss: 3.696875]\n",
      "epoch:8 step:6915 [D loss: 0.179098, acc.: 95.31%] [G loss: 3.848289]\n",
      "epoch:8 step:6916 [D loss: 0.236924, acc.: 89.84%] [G loss: 3.306642]\n",
      "epoch:8 step:6917 [D loss: 0.250896, acc.: 89.06%] [G loss: 3.694190]\n",
      "epoch:8 step:6918 [D loss: 0.296177, acc.: 89.84%] [G loss: 2.594769]\n",
      "epoch:8 step:6919 [D loss: 0.399564, acc.: 80.47%] [G loss: 2.008103]\n",
      "epoch:8 step:6920 [D loss: 0.277668, acc.: 90.62%] [G loss: 2.501778]\n",
      "epoch:8 step:6921 [D loss: 0.262232, acc.: 92.19%] [G loss: 2.640471]\n",
      "epoch:8 step:6922 [D loss: 0.227166, acc.: 90.62%] [G loss: 3.259993]\n",
      "epoch:8 step:6923 [D loss: 0.323606, acc.: 87.50%] [G loss: 2.519555]\n",
      "epoch:8 step:6924 [D loss: 0.277832, acc.: 92.19%] [G loss: 3.199336]\n",
      "epoch:8 step:6925 [D loss: 0.199160, acc.: 92.97%] [G loss: 2.736732]\n",
      "epoch:8 step:6926 [D loss: 0.320673, acc.: 86.72%] [G loss: 3.240196]\n",
      "epoch:8 step:6927 [D loss: 0.353787, acc.: 85.94%] [G loss: 4.813967]\n",
      "epoch:8 step:6928 [D loss: 0.285944, acc.: 83.59%] [G loss: 2.687601]\n",
      "epoch:8 step:6929 [D loss: 0.294280, acc.: 89.84%] [G loss: 2.024702]\n",
      "epoch:8 step:6930 [D loss: 0.492679, acc.: 72.66%] [G loss: 2.988608]\n",
      "epoch:8 step:6931 [D loss: 0.544167, acc.: 82.03%] [G loss: 5.998304]\n",
      "epoch:8 step:6932 [D loss: 1.129808, acc.: 64.06%] [G loss: 5.443590]\n",
      "epoch:8 step:6933 [D loss: 1.891123, acc.: 58.59%] [G loss: 4.114329]\n",
      "epoch:8 step:6934 [D loss: 0.280938, acc.: 86.72%] [G loss: 4.704875]\n",
      "epoch:8 step:6935 [D loss: 0.497872, acc.: 74.22%] [G loss: 3.261555]\n",
      "epoch:8 step:6936 [D loss: 0.307479, acc.: 85.16%] [G loss: 4.105364]\n",
      "epoch:8 step:6937 [D loss: 0.450023, acc.: 78.91%] [G loss: 2.378466]\n",
      "epoch:8 step:6938 [D loss: 0.535270, acc.: 78.12%] [G loss: 1.814045]\n",
      "epoch:8 step:6939 [D loss: 0.242899, acc.: 89.84%] [G loss: 3.141739]\n",
      "epoch:8 step:6940 [D loss: 0.348620, acc.: 84.38%] [G loss: 2.818070]\n",
      "epoch:8 step:6941 [D loss: 0.279817, acc.: 88.28%] [G loss: 3.647766]\n",
      "epoch:8 step:6942 [D loss: 0.262012, acc.: 89.84%] [G loss: 2.887497]\n",
      "epoch:8 step:6943 [D loss: 0.299073, acc.: 85.94%] [G loss: 2.271996]\n",
      "epoch:8 step:6944 [D loss: 0.323423, acc.: 88.28%] [G loss: 2.021424]\n",
      "epoch:8 step:6945 [D loss: 0.318199, acc.: 85.94%] [G loss: 2.337145]\n",
      "epoch:8 step:6946 [D loss: 0.259690, acc.: 91.41%] [G loss: 2.943932]\n",
      "epoch:8 step:6947 [D loss: 0.281876, acc.: 88.28%] [G loss: 2.978568]\n",
      "epoch:8 step:6948 [D loss: 0.361461, acc.: 82.81%] [G loss: 2.054208]\n",
      "epoch:8 step:6949 [D loss: 0.300812, acc.: 88.28%] [G loss: 2.500626]\n",
      "epoch:8 step:6950 [D loss: 0.404356, acc.: 80.47%] [G loss: 2.673038]\n",
      "epoch:8 step:6951 [D loss: 0.332127, acc.: 82.81%] [G loss: 2.948970]\n",
      "epoch:8 step:6952 [D loss: 0.364055, acc.: 82.03%] [G loss: 3.264629]\n",
      "epoch:8 step:6953 [D loss: 0.292902, acc.: 86.72%] [G loss: 3.453233]\n",
      "epoch:8 step:6954 [D loss: 0.397432, acc.: 82.81%] [G loss: 2.377091]\n",
      "epoch:8 step:6955 [D loss: 0.195402, acc.: 94.53%] [G loss: 5.135666]\n",
      "epoch:8 step:6956 [D loss: 0.267095, acc.: 88.28%] [G loss: 3.044281]\n",
      "epoch:8 step:6957 [D loss: 0.276933, acc.: 90.62%] [G loss: 3.353767]\n",
      "epoch:8 step:6958 [D loss: 0.265372, acc.: 87.50%] [G loss: 3.316390]\n",
      "epoch:8 step:6959 [D loss: 0.347883, acc.: 83.59%] [G loss: 1.898734]\n",
      "epoch:8 step:6960 [D loss: 0.284607, acc.: 89.06%] [G loss: 2.961085]\n",
      "epoch:8 step:6961 [D loss: 0.355303, acc.: 85.94%] [G loss: 2.661778]\n",
      "epoch:8 step:6962 [D loss: 0.303995, acc.: 87.50%] [G loss: 2.523515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6963 [D loss: 0.377085, acc.: 79.69%] [G loss: 2.518345]\n",
      "epoch:8 step:6964 [D loss: 0.251907, acc.: 92.19%] [G loss: 3.035847]\n",
      "epoch:8 step:6965 [D loss: 0.244752, acc.: 91.41%] [G loss: 2.547943]\n",
      "epoch:8 step:6966 [D loss: 0.352375, acc.: 83.59%] [G loss: 2.163116]\n",
      "epoch:8 step:6967 [D loss: 0.409849, acc.: 82.03%] [G loss: 2.084508]\n",
      "epoch:8 step:6968 [D loss: 0.343213, acc.: 85.16%] [G loss: 2.225832]\n",
      "epoch:8 step:6969 [D loss: 0.349371, acc.: 85.16%] [G loss: 2.187203]\n",
      "epoch:8 step:6970 [D loss: 0.299982, acc.: 89.84%] [G loss: 2.719018]\n",
      "epoch:8 step:6971 [D loss: 0.310343, acc.: 90.62%] [G loss: 3.444731]\n",
      "epoch:8 step:6972 [D loss: 0.259318, acc.: 89.84%] [G loss: 2.885228]\n",
      "epoch:8 step:6973 [D loss: 0.299697, acc.: 87.50%] [G loss: 3.516741]\n",
      "epoch:8 step:6974 [D loss: 0.261315, acc.: 92.97%] [G loss: 2.517652]\n",
      "epoch:8 step:6975 [D loss: 0.362011, acc.: 83.59%] [G loss: 2.401422]\n",
      "epoch:8 step:6976 [D loss: 0.428267, acc.: 80.47%] [G loss: 2.454508]\n",
      "epoch:8 step:6977 [D loss: 0.207341, acc.: 92.19%] [G loss: 4.216017]\n",
      "epoch:8 step:6978 [D loss: 0.230511, acc.: 96.09%] [G loss: 2.724347]\n",
      "epoch:8 step:6979 [D loss: 0.278280, acc.: 91.41%] [G loss: 2.575581]\n",
      "epoch:8 step:6980 [D loss: 0.336269, acc.: 89.84%] [G loss: 3.467182]\n",
      "epoch:8 step:6981 [D loss: 0.414553, acc.: 81.25%] [G loss: 2.976701]\n",
      "epoch:8 step:6982 [D loss: 0.373852, acc.: 82.81%] [G loss: 3.556189]\n",
      "epoch:8 step:6983 [D loss: 0.312063, acc.: 87.50%] [G loss: 4.928737]\n",
      "epoch:8 step:6984 [D loss: 0.295542, acc.: 85.94%] [G loss: 3.409017]\n",
      "epoch:8 step:6985 [D loss: 0.345773, acc.: 86.72%] [G loss: 2.124686]\n",
      "epoch:8 step:6986 [D loss: 0.268313, acc.: 93.75%] [G loss: 2.381121]\n",
      "epoch:8 step:6987 [D loss: 0.283579, acc.: 89.06%] [G loss: 2.951875]\n",
      "epoch:8 step:6988 [D loss: 0.312779, acc.: 88.28%] [G loss: 2.569029]\n",
      "epoch:8 step:6989 [D loss: 0.309138, acc.: 89.06%] [G loss: 2.947973]\n",
      "epoch:8 step:6990 [D loss: 0.286693, acc.: 88.28%] [G loss: 3.460465]\n",
      "epoch:8 step:6991 [D loss: 0.257026, acc.: 91.41%] [G loss: 3.184127]\n",
      "epoch:8 step:6992 [D loss: 0.184190, acc.: 89.84%] [G loss: 4.873796]\n",
      "epoch:8 step:6993 [D loss: 0.342835, acc.: 84.38%] [G loss: 2.526014]\n",
      "epoch:8 step:6994 [D loss: 0.276979, acc.: 89.06%] [G loss: 3.137614]\n",
      "epoch:8 step:6995 [D loss: 0.297159, acc.: 87.50%] [G loss: 2.649970]\n",
      "epoch:8 step:6996 [D loss: 0.311407, acc.: 86.72%] [G loss: 2.989762]\n",
      "epoch:8 step:6997 [D loss: 0.230142, acc.: 90.62%] [G loss: 2.931003]\n",
      "epoch:8 step:6998 [D loss: 0.366588, acc.: 85.16%] [G loss: 2.848845]\n",
      "epoch:8 step:6999 [D loss: 0.370451, acc.: 84.38%] [G loss: 2.553640]\n",
      "epoch:8 step:7000 [D loss: 0.344992, acc.: 89.06%] [G loss: 3.015491]\n",
      "epoch:8 step:7001 [D loss: 0.292860, acc.: 86.72%] [G loss: 2.982251]\n",
      "epoch:8 step:7002 [D loss: 0.241988, acc.: 89.84%] [G loss: 3.766734]\n",
      "epoch:8 step:7003 [D loss: 0.227733, acc.: 93.75%] [G loss: 3.507398]\n",
      "epoch:8 step:7004 [D loss: 0.261060, acc.: 90.62%] [G loss: 2.853645]\n",
      "epoch:8 step:7005 [D loss: 0.242703, acc.: 91.41%] [G loss: 2.778528]\n",
      "epoch:8 step:7006 [D loss: 0.257185, acc.: 90.62%] [G loss: 3.493526]\n",
      "epoch:8 step:7007 [D loss: 0.148647, acc.: 97.66%] [G loss: 4.087410]\n",
      "epoch:8 step:7008 [D loss: 0.310884, acc.: 85.94%] [G loss: 4.165360]\n",
      "epoch:8 step:7009 [D loss: 0.162215, acc.: 93.75%] [G loss: 4.507812]\n",
      "epoch:8 step:7010 [D loss: 0.307671, acc.: 83.59%] [G loss: 3.619326]\n",
      "epoch:8 step:7011 [D loss: 0.237959, acc.: 89.84%] [G loss: 2.912920]\n",
      "epoch:8 step:7012 [D loss: 0.241997, acc.: 92.19%] [G loss: 3.485886]\n",
      "epoch:8 step:7013 [D loss: 0.352075, acc.: 82.03%] [G loss: 2.943496]\n",
      "epoch:8 step:7014 [D loss: 0.245306, acc.: 92.97%] [G loss: 3.212539]\n",
      "epoch:8 step:7015 [D loss: 0.169571, acc.: 95.31%] [G loss: 3.538039]\n",
      "epoch:8 step:7016 [D loss: 0.273601, acc.: 89.84%] [G loss: 3.919077]\n",
      "epoch:8 step:7017 [D loss: 0.256116, acc.: 91.41%] [G loss: 2.947778]\n",
      "epoch:8 step:7018 [D loss: 0.223842, acc.: 92.19%] [G loss: 2.581276]\n",
      "epoch:8 step:7019 [D loss: 0.303551, acc.: 89.84%] [G loss: 3.344649]\n",
      "epoch:8 step:7020 [D loss: 0.219621, acc.: 86.72%] [G loss: 4.018857]\n",
      "epoch:8 step:7021 [D loss: 0.248401, acc.: 89.06%] [G loss: 2.973138]\n",
      "epoch:8 step:7022 [D loss: 0.361078, acc.: 82.81%] [G loss: 2.391745]\n",
      "epoch:8 step:7023 [D loss: 0.278331, acc.: 87.50%] [G loss: 3.296855]\n",
      "epoch:8 step:7024 [D loss: 0.204133, acc.: 96.09%] [G loss: 3.183501]\n",
      "epoch:8 step:7025 [D loss: 0.344422, acc.: 87.50%] [G loss: 3.569017]\n",
      "epoch:8 step:7026 [D loss: 0.286920, acc.: 89.06%] [G loss: 2.321118]\n",
      "epoch:8 step:7027 [D loss: 0.210699, acc.: 92.97%] [G loss: 3.093208]\n",
      "epoch:8 step:7028 [D loss: 0.280652, acc.: 85.94%] [G loss: 2.024496]\n",
      "epoch:8 step:7029 [D loss: 0.342712, acc.: 85.16%] [G loss: 2.691803]\n",
      "epoch:9 step:7030 [D loss: 0.300363, acc.: 89.84%] [G loss: 2.119552]\n",
      "epoch:9 step:7031 [D loss: 0.249657, acc.: 89.84%] [G loss: 2.520336]\n",
      "epoch:9 step:7032 [D loss: 0.230376, acc.: 88.28%] [G loss: 3.050078]\n",
      "epoch:9 step:7033 [D loss: 0.267281, acc.: 91.41%] [G loss: 4.119820]\n",
      "epoch:9 step:7034 [D loss: 0.234370, acc.: 89.06%] [G loss: 3.499089]\n",
      "epoch:9 step:7035 [D loss: 0.265277, acc.: 89.06%] [G loss: 3.107962]\n",
      "epoch:9 step:7036 [D loss: 0.192923, acc.: 92.97%] [G loss: 3.262694]\n",
      "epoch:9 step:7037 [D loss: 0.241316, acc.: 91.41%] [G loss: 4.362563]\n",
      "epoch:9 step:7038 [D loss: 0.282853, acc.: 90.62%] [G loss: 3.273584]\n",
      "epoch:9 step:7039 [D loss: 0.297961, acc.: 88.28%] [G loss: 5.070193]\n",
      "epoch:9 step:7040 [D loss: 0.469936, acc.: 75.78%] [G loss: 2.581837]\n",
      "epoch:9 step:7041 [D loss: 0.386220, acc.: 82.81%] [G loss: 2.755551]\n",
      "epoch:9 step:7042 [D loss: 0.359912, acc.: 85.16%] [G loss: 2.301810]\n",
      "epoch:9 step:7043 [D loss: 0.278029, acc.: 90.62%] [G loss: 2.672957]\n",
      "epoch:9 step:7044 [D loss: 0.258891, acc.: 91.41%] [G loss: 3.414174]\n",
      "epoch:9 step:7045 [D loss: 0.307315, acc.: 88.28%] [G loss: 2.486255]\n",
      "epoch:9 step:7046 [D loss: 0.184289, acc.: 92.19%] [G loss: 2.753740]\n",
      "epoch:9 step:7047 [D loss: 0.323197, acc.: 85.94%] [G loss: 2.639214]\n",
      "epoch:9 step:7048 [D loss: 0.356021, acc.: 85.94%] [G loss: 2.531255]\n",
      "epoch:9 step:7049 [D loss: 0.224676, acc.: 90.62%] [G loss: 3.155393]\n",
      "epoch:9 step:7050 [D loss: 0.319132, acc.: 87.50%] [G loss: 3.431058]\n",
      "epoch:9 step:7051 [D loss: 0.384336, acc.: 85.94%] [G loss: 2.464530]\n",
      "epoch:9 step:7052 [D loss: 0.258288, acc.: 89.84%] [G loss: 2.586307]\n",
      "epoch:9 step:7053 [D loss: 0.277673, acc.: 91.41%] [G loss: 3.067631]\n",
      "epoch:9 step:7054 [D loss: 0.250258, acc.: 90.62%] [G loss: 3.504048]\n",
      "epoch:9 step:7055 [D loss: 0.259238, acc.: 89.84%] [G loss: 2.725091]\n",
      "epoch:9 step:7056 [D loss: 0.197341, acc.: 93.75%] [G loss: 4.923106]\n",
      "epoch:9 step:7057 [D loss: 0.338814, acc.: 83.59%] [G loss: 3.477165]\n",
      "epoch:9 step:7058 [D loss: 0.453639, acc.: 78.12%] [G loss: 3.562913]\n",
      "epoch:9 step:7059 [D loss: 0.363089, acc.: 83.59%] [G loss: 2.021746]\n",
      "epoch:9 step:7060 [D loss: 0.297516, acc.: 91.41%] [G loss: 4.135877]\n",
      "epoch:9 step:7061 [D loss: 0.457751, acc.: 80.47%] [G loss: 3.384638]\n",
      "epoch:9 step:7062 [D loss: 0.183675, acc.: 93.75%] [G loss: 4.400902]\n",
      "epoch:9 step:7063 [D loss: 0.271274, acc.: 89.06%] [G loss: 5.656987]\n",
      "epoch:9 step:7064 [D loss: 0.252770, acc.: 91.41%] [G loss: 2.501931]\n",
      "epoch:9 step:7065 [D loss: 0.254091, acc.: 92.97%] [G loss: 2.418001]\n",
      "epoch:9 step:7066 [D loss: 0.326647, acc.: 83.59%] [G loss: 2.072646]\n",
      "epoch:9 step:7067 [D loss: 0.302574, acc.: 85.94%] [G loss: 3.047991]\n",
      "epoch:9 step:7068 [D loss: 0.302043, acc.: 85.16%] [G loss: 3.838563]\n",
      "epoch:9 step:7069 [D loss: 0.317231, acc.: 90.62%] [G loss: 2.646923]\n",
      "epoch:9 step:7070 [D loss: 0.323891, acc.: 88.28%] [G loss: 4.426946]\n",
      "epoch:9 step:7071 [D loss: 0.289495, acc.: 90.62%] [G loss: 3.264444]\n",
      "epoch:9 step:7072 [D loss: 0.354460, acc.: 81.25%] [G loss: 6.901808]\n",
      "epoch:9 step:7073 [D loss: 0.859248, acc.: 70.31%] [G loss: 7.851616]\n",
      "epoch:9 step:7074 [D loss: 2.392384, acc.: 47.66%] [G loss: 3.304203]\n",
      "epoch:9 step:7075 [D loss: 0.580960, acc.: 71.09%] [G loss: 2.432707]\n",
      "epoch:9 step:7076 [D loss: 0.514348, acc.: 78.91%] [G loss: 2.700562]\n",
      "epoch:9 step:7077 [D loss: 0.252572, acc.: 89.06%] [G loss: 3.384919]\n",
      "epoch:9 step:7078 [D loss: 0.211615, acc.: 91.41%] [G loss: 3.519027]\n",
      "epoch:9 step:7079 [D loss: 0.382601, acc.: 82.03%] [G loss: 2.770793]\n",
      "epoch:9 step:7080 [D loss: 0.237357, acc.: 93.75%] [G loss: 2.691838]\n",
      "epoch:9 step:7081 [D loss: 0.298174, acc.: 87.50%] [G loss: 3.052727]\n",
      "epoch:9 step:7082 [D loss: 0.269605, acc.: 89.84%] [G loss: 3.044140]\n",
      "epoch:9 step:7083 [D loss: 0.279051, acc.: 88.28%] [G loss: 2.837294]\n",
      "epoch:9 step:7084 [D loss: 0.221257, acc.: 90.62%] [G loss: 4.286216]\n",
      "epoch:9 step:7085 [D loss: 0.274572, acc.: 92.19%] [G loss: 2.728245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7086 [D loss: 0.413829, acc.: 81.25%] [G loss: 2.339565]\n",
      "epoch:9 step:7087 [D loss: 0.272590, acc.: 89.84%] [G loss: 2.984730]\n",
      "epoch:9 step:7088 [D loss: 0.312390, acc.: 89.06%] [G loss: 2.123633]\n",
      "epoch:9 step:7089 [D loss: 0.480137, acc.: 72.66%] [G loss: 3.094398]\n",
      "epoch:9 step:7090 [D loss: 0.198039, acc.: 94.53%] [G loss: 4.719279]\n",
      "epoch:9 step:7091 [D loss: 0.240227, acc.: 89.84%] [G loss: 2.571590]\n",
      "epoch:9 step:7092 [D loss: 0.377438, acc.: 80.47%] [G loss: 2.251783]\n",
      "epoch:9 step:7093 [D loss: 0.305175, acc.: 89.84%] [G loss: 2.416225]\n",
      "epoch:9 step:7094 [D loss: 0.449153, acc.: 82.03%] [G loss: 3.336981]\n",
      "epoch:9 step:7095 [D loss: 0.255457, acc.: 89.06%] [G loss: 4.713925]\n",
      "epoch:9 step:7096 [D loss: 0.337747, acc.: 87.50%] [G loss: 4.094913]\n",
      "epoch:9 step:7097 [D loss: 0.424243, acc.: 76.56%] [G loss: 1.782746]\n",
      "epoch:9 step:7098 [D loss: 0.252004, acc.: 92.19%] [G loss: 3.535048]\n",
      "epoch:9 step:7099 [D loss: 0.254984, acc.: 93.75%] [G loss: 2.802495]\n",
      "epoch:9 step:7100 [D loss: 0.349618, acc.: 85.94%] [G loss: 2.414332]\n",
      "epoch:9 step:7101 [D loss: 0.396424, acc.: 82.03%] [G loss: 1.850757]\n",
      "epoch:9 step:7102 [D loss: 0.352161, acc.: 85.94%] [G loss: 2.534595]\n",
      "epoch:9 step:7103 [D loss: 0.386115, acc.: 84.38%] [G loss: 2.479806]\n",
      "epoch:9 step:7104 [D loss: 0.328149, acc.: 87.50%] [G loss: 2.397329]\n",
      "epoch:9 step:7105 [D loss: 0.512285, acc.: 79.69%] [G loss: 2.213674]\n",
      "epoch:9 step:7106 [D loss: 0.286753, acc.: 86.72%] [G loss: 2.357785]\n",
      "epoch:9 step:7107 [D loss: 0.314030, acc.: 89.06%] [G loss: 3.780966]\n",
      "epoch:9 step:7108 [D loss: 0.341557, acc.: 85.16%] [G loss: 4.149016]\n",
      "epoch:9 step:7109 [D loss: 0.308311, acc.: 87.50%] [G loss: 2.825174]\n",
      "epoch:9 step:7110 [D loss: 0.334170, acc.: 87.50%] [G loss: 2.844269]\n",
      "epoch:9 step:7111 [D loss: 0.330921, acc.: 86.72%] [G loss: 3.738592]\n",
      "epoch:9 step:7112 [D loss: 0.324083, acc.: 85.16%] [G loss: 3.862591]\n",
      "epoch:9 step:7113 [D loss: 0.288123, acc.: 87.50%] [G loss: 2.981527]\n",
      "epoch:9 step:7114 [D loss: 0.255787, acc.: 88.28%] [G loss: 3.842898]\n",
      "epoch:9 step:7115 [D loss: 0.247627, acc.: 89.84%] [G loss: 3.648242]\n",
      "epoch:9 step:7116 [D loss: 0.351687, acc.: 88.28%] [G loss: 2.353462]\n",
      "epoch:9 step:7117 [D loss: 0.347539, acc.: 86.72%] [G loss: 2.838755]\n",
      "epoch:9 step:7118 [D loss: 0.331637, acc.: 85.16%] [G loss: 2.950685]\n",
      "epoch:9 step:7119 [D loss: 0.355172, acc.: 85.16%] [G loss: 3.064743]\n",
      "epoch:9 step:7120 [D loss: 0.332724, acc.: 89.06%] [G loss: 3.215123]\n",
      "epoch:9 step:7121 [D loss: 0.321049, acc.: 83.59%] [G loss: 4.484778]\n",
      "epoch:9 step:7122 [D loss: 0.346552, acc.: 81.25%] [G loss: 2.926402]\n",
      "epoch:9 step:7123 [D loss: 0.388776, acc.: 88.28%] [G loss: 4.532204]\n",
      "epoch:9 step:7124 [D loss: 0.766780, acc.: 71.88%] [G loss: 5.612214]\n",
      "epoch:9 step:7125 [D loss: 1.404927, acc.: 59.38%] [G loss: 5.518948]\n",
      "epoch:9 step:7126 [D loss: 1.442554, acc.: 55.47%] [G loss: 2.707001]\n",
      "epoch:9 step:7127 [D loss: 0.242969, acc.: 89.06%] [G loss: 3.202533]\n",
      "epoch:9 step:7128 [D loss: 0.598014, acc.: 68.75%] [G loss: 2.508837]\n",
      "epoch:9 step:7129 [D loss: 0.412920, acc.: 81.25%] [G loss: 2.689581]\n",
      "epoch:9 step:7130 [D loss: 0.433499, acc.: 78.91%] [G loss: 2.792677]\n",
      "epoch:9 step:7131 [D loss: 0.269403, acc.: 88.28%] [G loss: 2.919165]\n",
      "epoch:9 step:7132 [D loss: 0.491072, acc.: 76.56%] [G loss: 2.218811]\n",
      "epoch:9 step:7133 [D loss: 0.232119, acc.: 88.28%] [G loss: 3.656980]\n",
      "epoch:9 step:7134 [D loss: 0.332373, acc.: 85.94%] [G loss: 2.160031]\n",
      "epoch:9 step:7135 [D loss: 0.316306, acc.: 92.19%] [G loss: 2.903807]\n",
      "epoch:9 step:7136 [D loss: 0.273181, acc.: 88.28%] [G loss: 4.112935]\n",
      "epoch:9 step:7137 [D loss: 0.303411, acc.: 90.62%] [G loss: 2.868820]\n",
      "epoch:9 step:7138 [D loss: 0.322936, acc.: 85.16%] [G loss: 2.472253]\n",
      "epoch:9 step:7139 [D loss: 0.255563, acc.: 89.06%] [G loss: 2.320619]\n",
      "epoch:9 step:7140 [D loss: 0.291587, acc.: 88.28%] [G loss: 2.478518]\n",
      "epoch:9 step:7141 [D loss: 0.259186, acc.: 88.28%] [G loss: 2.780711]\n",
      "epoch:9 step:7142 [D loss: 0.307081, acc.: 90.62%] [G loss: 1.911846]\n",
      "epoch:9 step:7143 [D loss: 0.356742, acc.: 85.16%] [G loss: 2.063490]\n",
      "epoch:9 step:7144 [D loss: 0.329658, acc.: 84.38%] [G loss: 2.103106]\n",
      "epoch:9 step:7145 [D loss: 0.346537, acc.: 88.28%] [G loss: 2.051004]\n",
      "epoch:9 step:7146 [D loss: 0.311309, acc.: 88.28%] [G loss: 2.296692]\n",
      "epoch:9 step:7147 [D loss: 0.263916, acc.: 89.84%] [G loss: 3.319830]\n",
      "epoch:9 step:7148 [D loss: 0.313044, acc.: 88.28%] [G loss: 2.968482]\n",
      "epoch:9 step:7149 [D loss: 0.350189, acc.: 88.28%] [G loss: 2.316279]\n",
      "epoch:9 step:7150 [D loss: 0.396713, acc.: 85.16%] [G loss: 2.790878]\n",
      "epoch:9 step:7151 [D loss: 0.343752, acc.: 89.06%] [G loss: 2.098032]\n",
      "epoch:9 step:7152 [D loss: 0.261277, acc.: 89.84%] [G loss: 2.751673]\n",
      "epoch:9 step:7153 [D loss: 0.264699, acc.: 90.62%] [G loss: 3.005347]\n",
      "epoch:9 step:7154 [D loss: 0.318009, acc.: 87.50%] [G loss: 1.639832]\n",
      "epoch:9 step:7155 [D loss: 0.377000, acc.: 82.81%] [G loss: 2.539262]\n",
      "epoch:9 step:7156 [D loss: 0.222636, acc.: 90.62%] [G loss: 4.003169]\n",
      "epoch:9 step:7157 [D loss: 0.253014, acc.: 89.84%] [G loss: 3.732530]\n",
      "epoch:9 step:7158 [D loss: 0.432835, acc.: 80.47%] [G loss: 2.331321]\n",
      "epoch:9 step:7159 [D loss: 0.310366, acc.: 86.72%] [G loss: 2.455124]\n",
      "epoch:9 step:7160 [D loss: 0.339976, acc.: 89.06%] [G loss: 2.890950]\n",
      "epoch:9 step:7161 [D loss: 0.360509, acc.: 84.38%] [G loss: 2.911069]\n",
      "epoch:9 step:7162 [D loss: 0.264343, acc.: 90.62%] [G loss: 3.129254]\n",
      "epoch:9 step:7163 [D loss: 0.362228, acc.: 88.28%] [G loss: 2.669569]\n",
      "epoch:9 step:7164 [D loss: 0.399393, acc.: 78.91%] [G loss: 2.447785]\n",
      "epoch:9 step:7165 [D loss: 0.233840, acc.: 93.75%] [G loss: 2.747865]\n",
      "epoch:9 step:7166 [D loss: 0.309310, acc.: 86.72%] [G loss: 2.528859]\n",
      "epoch:9 step:7167 [D loss: 0.305296, acc.: 86.72%] [G loss: 2.411383]\n",
      "epoch:9 step:7168 [D loss: 0.318275, acc.: 89.06%] [G loss: 2.149538]\n",
      "epoch:9 step:7169 [D loss: 0.276329, acc.: 90.62%] [G loss: 2.435614]\n",
      "epoch:9 step:7170 [D loss: 0.294410, acc.: 88.28%] [G loss: 2.550720]\n",
      "epoch:9 step:7171 [D loss: 0.423300, acc.: 80.47%] [G loss: 3.325471]\n",
      "epoch:9 step:7172 [D loss: 0.348009, acc.: 88.28%] [G loss: 2.415751]\n",
      "epoch:9 step:7173 [D loss: 0.304723, acc.: 87.50%] [G loss: 4.105103]\n",
      "epoch:9 step:7174 [D loss: 0.259448, acc.: 89.84%] [G loss: 2.838238]\n",
      "epoch:9 step:7175 [D loss: 0.407699, acc.: 81.25%] [G loss: 2.762935]\n",
      "epoch:9 step:7176 [D loss: 0.281339, acc.: 87.50%] [G loss: 3.197723]\n",
      "epoch:9 step:7177 [D loss: 0.349420, acc.: 86.72%] [G loss: 2.628870]\n",
      "epoch:9 step:7178 [D loss: 0.250224, acc.: 90.62%] [G loss: 1.891101]\n",
      "epoch:9 step:7179 [D loss: 0.287923, acc.: 88.28%] [G loss: 2.632882]\n",
      "epoch:9 step:7180 [D loss: 0.305840, acc.: 87.50%] [G loss: 2.489341]\n",
      "epoch:9 step:7181 [D loss: 0.368972, acc.: 84.38%] [G loss: 2.340530]\n",
      "epoch:9 step:7182 [D loss: 0.397264, acc.: 84.38%] [G loss: 2.350938]\n",
      "epoch:9 step:7183 [D loss: 0.375892, acc.: 82.81%] [G loss: 2.540665]\n",
      "epoch:9 step:7184 [D loss: 0.376855, acc.: 85.94%] [G loss: 1.981990]\n",
      "epoch:9 step:7185 [D loss: 0.278417, acc.: 89.84%] [G loss: 2.542915]\n",
      "epoch:9 step:7186 [D loss: 0.423688, acc.: 81.25%] [G loss: 2.570170]\n",
      "epoch:9 step:7187 [D loss: 0.460335, acc.: 76.56%] [G loss: 3.790281]\n",
      "epoch:9 step:7188 [D loss: 0.354631, acc.: 86.72%] [G loss: 2.476081]\n",
      "epoch:9 step:7189 [D loss: 0.430540, acc.: 83.59%] [G loss: 2.504035]\n",
      "epoch:9 step:7190 [D loss: 0.478008, acc.: 82.03%] [G loss: 2.934519]\n",
      "epoch:9 step:7191 [D loss: 0.331220, acc.: 86.72%] [G loss: 2.354211]\n",
      "epoch:9 step:7192 [D loss: 0.236519, acc.: 93.75%] [G loss: 3.194172]\n",
      "epoch:9 step:7193 [D loss: 0.283018, acc.: 88.28%] [G loss: 2.461680]\n",
      "epoch:9 step:7194 [D loss: 0.295664, acc.: 88.28%] [G loss: 2.237209]\n",
      "epoch:9 step:7195 [D loss: 0.288429, acc.: 89.06%] [G loss: 3.238408]\n",
      "epoch:9 step:7196 [D loss: 0.299607, acc.: 89.84%] [G loss: 2.622982]\n",
      "epoch:9 step:7197 [D loss: 0.377174, acc.: 82.81%] [G loss: 2.625027]\n",
      "epoch:9 step:7198 [D loss: 0.332326, acc.: 85.94%] [G loss: 2.526814]\n",
      "epoch:9 step:7199 [D loss: 0.283581, acc.: 86.72%] [G loss: 2.545583]\n",
      "epoch:9 step:7200 [D loss: 0.314367, acc.: 89.84%] [G loss: 3.679225]\n",
      "epoch:9 step:7201 [D loss: 0.246112, acc.: 89.06%] [G loss: 6.597003]\n",
      "epoch:9 step:7202 [D loss: 0.239926, acc.: 89.84%] [G loss: 2.426389]\n",
      "epoch:9 step:7203 [D loss: 0.214011, acc.: 90.62%] [G loss: 4.420191]\n",
      "epoch:9 step:7204 [D loss: 0.298864, acc.: 84.38%] [G loss: 2.911441]\n",
      "epoch:9 step:7205 [D loss: 0.248994, acc.: 92.97%] [G loss: 3.141470]\n",
      "epoch:9 step:7206 [D loss: 0.335392, acc.: 82.03%] [G loss: 3.306088]\n",
      "epoch:9 step:7207 [D loss: 0.234328, acc.: 91.41%] [G loss: 5.240156]\n",
      "epoch:9 step:7208 [D loss: 0.445267, acc.: 79.69%] [G loss: 2.250506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7209 [D loss: 0.316536, acc.: 88.28%] [G loss: 2.201405]\n",
      "epoch:9 step:7210 [D loss: 0.300048, acc.: 89.06%] [G loss: 2.507428]\n",
      "epoch:9 step:7211 [D loss: 0.277729, acc.: 88.28%] [G loss: 2.586729]\n",
      "epoch:9 step:7212 [D loss: 0.318289, acc.: 84.38%] [G loss: 3.089993]\n",
      "epoch:9 step:7213 [D loss: 0.430102, acc.: 78.91%] [G loss: 2.653651]\n",
      "epoch:9 step:7214 [D loss: 0.279033, acc.: 86.72%] [G loss: 4.097316]\n",
      "epoch:9 step:7215 [D loss: 0.289679, acc.: 89.84%] [G loss: 3.197132]\n",
      "epoch:9 step:7216 [D loss: 0.355758, acc.: 82.81%] [G loss: 2.543116]\n",
      "epoch:9 step:7217 [D loss: 0.314639, acc.: 89.06%] [G loss: 2.685889]\n",
      "epoch:9 step:7218 [D loss: 0.245036, acc.: 90.62%] [G loss: 2.796057]\n",
      "epoch:9 step:7219 [D loss: 0.290037, acc.: 85.94%] [G loss: 2.559489]\n",
      "epoch:9 step:7220 [D loss: 0.316549, acc.: 84.38%] [G loss: 2.336285]\n",
      "epoch:9 step:7221 [D loss: 0.225549, acc.: 90.62%] [G loss: 3.708289]\n",
      "epoch:9 step:7222 [D loss: 0.276998, acc.: 89.84%] [G loss: 2.688118]\n",
      "epoch:9 step:7223 [D loss: 0.307571, acc.: 86.72%] [G loss: 2.863113]\n",
      "epoch:9 step:7224 [D loss: 0.338393, acc.: 87.50%] [G loss: 3.673722]\n",
      "epoch:9 step:7225 [D loss: 0.300688, acc.: 87.50%] [G loss: 5.603673]\n",
      "epoch:9 step:7226 [D loss: 0.294828, acc.: 87.50%] [G loss: 3.286830]\n",
      "epoch:9 step:7227 [D loss: 0.264547, acc.: 88.28%] [G loss: 2.685157]\n",
      "epoch:9 step:7228 [D loss: 0.372099, acc.: 83.59%] [G loss: 3.224511]\n",
      "epoch:9 step:7229 [D loss: 0.268124, acc.: 86.72%] [G loss: 3.677876]\n",
      "epoch:9 step:7230 [D loss: 0.275396, acc.: 85.16%] [G loss: 4.364419]\n",
      "epoch:9 step:7231 [D loss: 0.368913, acc.: 81.25%] [G loss: 2.854653]\n",
      "epoch:9 step:7232 [D loss: 0.298451, acc.: 90.62%] [G loss: 2.114685]\n",
      "epoch:9 step:7233 [D loss: 0.378432, acc.: 82.03%] [G loss: 3.098575]\n",
      "epoch:9 step:7234 [D loss: 0.216758, acc.: 91.41%] [G loss: 2.866877]\n",
      "epoch:9 step:7235 [D loss: 0.303426, acc.: 84.38%] [G loss: 3.886441]\n",
      "epoch:9 step:7236 [D loss: 0.276350, acc.: 87.50%] [G loss: 3.127950]\n",
      "epoch:9 step:7237 [D loss: 0.291293, acc.: 89.06%] [G loss: 2.612892]\n",
      "epoch:9 step:7238 [D loss: 0.274142, acc.: 89.06%] [G loss: 2.167434]\n",
      "epoch:9 step:7239 [D loss: 0.388580, acc.: 79.69%] [G loss: 2.757926]\n",
      "epoch:9 step:7240 [D loss: 0.412158, acc.: 82.03%] [G loss: 2.895083]\n",
      "epoch:9 step:7241 [D loss: 0.302735, acc.: 85.16%] [G loss: 3.794771]\n",
      "epoch:9 step:7242 [D loss: 0.366323, acc.: 82.81%] [G loss: 3.946320]\n",
      "epoch:9 step:7243 [D loss: 0.491012, acc.: 77.34%] [G loss: 2.921557]\n",
      "epoch:9 step:7244 [D loss: 0.434220, acc.: 78.91%] [G loss: 3.243449]\n",
      "epoch:9 step:7245 [D loss: 0.704347, acc.: 77.34%] [G loss: 6.133224]\n",
      "epoch:9 step:7246 [D loss: 0.718058, acc.: 80.47%] [G loss: 6.389106]\n",
      "epoch:9 step:7247 [D loss: 1.168227, acc.: 65.62%] [G loss: 3.852565]\n",
      "epoch:9 step:7248 [D loss: 0.492757, acc.: 77.34%] [G loss: 2.649601]\n",
      "epoch:9 step:7249 [D loss: 0.295063, acc.: 91.41%] [G loss: 3.655977]\n",
      "epoch:9 step:7250 [D loss: 0.328022, acc.: 89.84%] [G loss: 2.587243]\n",
      "epoch:9 step:7251 [D loss: 0.389704, acc.: 87.50%] [G loss: 2.683062]\n",
      "epoch:9 step:7252 [D loss: 0.414354, acc.: 89.84%] [G loss: 2.806831]\n",
      "epoch:9 step:7253 [D loss: 0.394989, acc.: 79.69%] [G loss: 3.469219]\n",
      "epoch:9 step:7254 [D loss: 0.296259, acc.: 89.06%] [G loss: 5.743773]\n",
      "epoch:9 step:7255 [D loss: 0.374248, acc.: 85.16%] [G loss: 3.407898]\n",
      "epoch:9 step:7256 [D loss: 0.342956, acc.: 83.59%] [G loss: 3.202933]\n",
      "epoch:9 step:7257 [D loss: 0.283602, acc.: 87.50%] [G loss: 5.709937]\n",
      "epoch:9 step:7258 [D loss: 0.411631, acc.: 81.25%] [G loss: 3.118528]\n",
      "epoch:9 step:7259 [D loss: 0.324512, acc.: 87.50%] [G loss: 3.268088]\n",
      "epoch:9 step:7260 [D loss: 0.416488, acc.: 80.47%] [G loss: 2.288250]\n",
      "epoch:9 step:7261 [D loss: 0.427564, acc.: 82.03%] [G loss: 2.915795]\n",
      "epoch:9 step:7262 [D loss: 0.343262, acc.: 82.03%] [G loss: 2.257135]\n",
      "epoch:9 step:7263 [D loss: 0.327920, acc.: 89.84%] [G loss: 2.660833]\n",
      "epoch:9 step:7264 [D loss: 0.361598, acc.: 83.59%] [G loss: 2.690490]\n",
      "epoch:9 step:7265 [D loss: 0.509451, acc.: 75.78%] [G loss: 3.371016]\n",
      "epoch:9 step:7266 [D loss: 0.403322, acc.: 82.03%] [G loss: 2.824793]\n",
      "epoch:9 step:7267 [D loss: 0.410311, acc.: 81.25%] [G loss: 2.815157]\n",
      "epoch:9 step:7268 [D loss: 0.443903, acc.: 84.38%] [G loss: 3.408011]\n",
      "epoch:9 step:7269 [D loss: 0.398190, acc.: 80.47%] [G loss: 3.621947]\n",
      "epoch:9 step:7270 [D loss: 0.359172, acc.: 85.16%] [G loss: 3.349709]\n",
      "epoch:9 step:7271 [D loss: 0.386942, acc.: 84.38%] [G loss: 3.205907]\n",
      "epoch:9 step:7272 [D loss: 0.235614, acc.: 91.41%] [G loss: 4.221173]\n",
      "epoch:9 step:7273 [D loss: 0.304873, acc.: 87.50%] [G loss: 1.723267]\n",
      "epoch:9 step:7274 [D loss: 0.287866, acc.: 91.41%] [G loss: 3.073358]\n",
      "epoch:9 step:7275 [D loss: 0.224789, acc.: 92.19%] [G loss: 5.596594]\n",
      "epoch:9 step:7276 [D loss: 0.327888, acc.: 84.38%] [G loss: 2.649977]\n",
      "epoch:9 step:7277 [D loss: 0.337519, acc.: 85.94%] [G loss: 2.806697]\n",
      "epoch:9 step:7278 [D loss: 0.381447, acc.: 83.59%] [G loss: 3.389748]\n",
      "epoch:9 step:7279 [D loss: 0.434504, acc.: 82.81%] [G loss: 3.254172]\n",
      "epoch:9 step:7280 [D loss: 0.296855, acc.: 85.16%] [G loss: 4.487139]\n",
      "epoch:9 step:7281 [D loss: 0.330068, acc.: 87.50%] [G loss: 3.170248]\n",
      "epoch:9 step:7282 [D loss: 0.336604, acc.: 85.94%] [G loss: 3.113799]\n",
      "epoch:9 step:7283 [D loss: 0.216736, acc.: 91.41%] [G loss: 3.644806]\n",
      "epoch:9 step:7284 [D loss: 0.396223, acc.: 81.25%] [G loss: 2.904430]\n",
      "epoch:9 step:7285 [D loss: 0.353377, acc.: 85.94%] [G loss: 2.771485]\n",
      "epoch:9 step:7286 [D loss: 0.295359, acc.: 85.94%] [G loss: 4.755707]\n",
      "epoch:9 step:7287 [D loss: 0.336994, acc.: 85.94%] [G loss: 4.035235]\n",
      "epoch:9 step:7288 [D loss: 0.339066, acc.: 85.16%] [G loss: 3.507805]\n",
      "epoch:9 step:7289 [D loss: 0.343159, acc.: 89.06%] [G loss: 3.128146]\n",
      "epoch:9 step:7290 [D loss: 0.307191, acc.: 86.72%] [G loss: 3.497882]\n",
      "epoch:9 step:7291 [D loss: 0.307217, acc.: 85.94%] [G loss: 2.713012]\n",
      "epoch:9 step:7292 [D loss: 0.326242, acc.: 89.06%] [G loss: 2.636887]\n",
      "epoch:9 step:7293 [D loss: 0.302815, acc.: 90.62%] [G loss: 2.845596]\n",
      "epoch:9 step:7294 [D loss: 0.364799, acc.: 84.38%] [G loss: 2.806953]\n",
      "epoch:9 step:7295 [D loss: 0.379839, acc.: 84.38%] [G loss: 2.570148]\n",
      "epoch:9 step:7296 [D loss: 0.292591, acc.: 87.50%] [G loss: 3.004508]\n",
      "epoch:9 step:7297 [D loss: 0.380898, acc.: 84.38%] [G loss: 2.128415]\n",
      "epoch:9 step:7298 [D loss: 0.354261, acc.: 86.72%] [G loss: 2.674038]\n",
      "epoch:9 step:7299 [D loss: 0.306237, acc.: 87.50%] [G loss: 2.655227]\n",
      "epoch:9 step:7300 [D loss: 0.348247, acc.: 89.06%] [G loss: 2.162668]\n",
      "epoch:9 step:7301 [D loss: 0.291616, acc.: 86.72%] [G loss: 3.141501]\n",
      "epoch:9 step:7302 [D loss: 0.277897, acc.: 89.06%] [G loss: 2.742503]\n",
      "epoch:9 step:7303 [D loss: 0.378465, acc.: 81.25%] [G loss: 2.613472]\n",
      "epoch:9 step:7304 [D loss: 0.430985, acc.: 78.12%] [G loss: 3.147091]\n",
      "epoch:9 step:7305 [D loss: 0.375559, acc.: 82.03%] [G loss: 3.678517]\n",
      "epoch:9 step:7306 [D loss: 0.383155, acc.: 82.81%] [G loss: 2.919812]\n",
      "epoch:9 step:7307 [D loss: 0.455295, acc.: 78.12%] [G loss: 2.131586]\n",
      "epoch:9 step:7308 [D loss: 0.433619, acc.: 84.38%] [G loss: 2.830600]\n",
      "epoch:9 step:7309 [D loss: 0.291087, acc.: 89.06%] [G loss: 2.727293]\n",
      "epoch:9 step:7310 [D loss: 0.397106, acc.: 82.81%] [G loss: 2.757400]\n",
      "epoch:9 step:7311 [D loss: 0.275367, acc.: 89.84%] [G loss: 3.275393]\n",
      "epoch:9 step:7312 [D loss: 0.297885, acc.: 86.72%] [G loss: 4.263090]\n",
      "epoch:9 step:7313 [D loss: 0.298410, acc.: 89.06%] [G loss: 2.605200]\n",
      "epoch:9 step:7314 [D loss: 0.400002, acc.: 82.81%] [G loss: 2.400722]\n",
      "epoch:9 step:7315 [D loss: 0.336729, acc.: 84.38%] [G loss: 3.246457]\n",
      "epoch:9 step:7316 [D loss: 0.346484, acc.: 85.16%] [G loss: 2.576297]\n",
      "epoch:9 step:7317 [D loss: 0.288097, acc.: 89.84%] [G loss: 2.726234]\n",
      "epoch:9 step:7318 [D loss: 0.247012, acc.: 92.19%] [G loss: 3.528759]\n",
      "epoch:9 step:7319 [D loss: 0.276930, acc.: 85.16%] [G loss: 4.243386]\n",
      "epoch:9 step:7320 [D loss: 0.297274, acc.: 90.62%] [G loss: 2.187303]\n",
      "epoch:9 step:7321 [D loss: 0.316713, acc.: 89.84%] [G loss: 2.243324]\n",
      "epoch:9 step:7322 [D loss: 0.284639, acc.: 89.06%] [G loss: 2.297472]\n",
      "epoch:9 step:7323 [D loss: 0.427801, acc.: 77.34%] [G loss: 2.236345]\n",
      "epoch:9 step:7324 [D loss: 0.353987, acc.: 83.59%] [G loss: 2.789164]\n",
      "epoch:9 step:7325 [D loss: 0.403135, acc.: 82.81%] [G loss: 3.342076]\n",
      "epoch:9 step:7326 [D loss: 0.232014, acc.: 91.41%] [G loss: 5.989620]\n",
      "epoch:9 step:7327 [D loss: 0.280199, acc.: 88.28%] [G loss: 2.509742]\n",
      "epoch:9 step:7328 [D loss: 0.383870, acc.: 85.16%] [G loss: 2.687087]\n",
      "epoch:9 step:7329 [D loss: 0.369456, acc.: 79.69%] [G loss: 4.210178]\n",
      "epoch:9 step:7330 [D loss: 0.340606, acc.: 82.81%] [G loss: 5.188489]\n",
      "epoch:9 step:7331 [D loss: 0.436071, acc.: 84.38%] [G loss: 2.603616]\n",
      "epoch:9 step:7332 [D loss: 0.212054, acc.: 92.97%] [G loss: 3.684823]\n",
      "epoch:9 step:7333 [D loss: 0.426167, acc.: 82.03%] [G loss: 2.313281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7334 [D loss: 0.486097, acc.: 75.78%] [G loss: 2.134894]\n",
      "epoch:9 step:7335 [D loss: 0.334289, acc.: 85.94%] [G loss: 3.830825]\n",
      "epoch:9 step:7336 [D loss: 0.584054, acc.: 76.56%] [G loss: 4.831190]\n",
      "epoch:9 step:7337 [D loss: 0.921406, acc.: 68.75%] [G loss: 5.261663]\n",
      "epoch:9 step:7338 [D loss: 1.027370, acc.: 65.62%] [G loss: 3.874827]\n",
      "epoch:9 step:7339 [D loss: 0.622432, acc.: 77.34%] [G loss: 2.633571]\n",
      "epoch:9 step:7340 [D loss: 0.467946, acc.: 81.25%] [G loss: 4.101964]\n",
      "epoch:9 step:7341 [D loss: 0.291562, acc.: 90.62%] [G loss: 3.583307]\n",
      "epoch:9 step:7342 [D loss: 0.552442, acc.: 75.78%] [G loss: 4.887381]\n",
      "epoch:9 step:7343 [D loss: 0.326190, acc.: 88.28%] [G loss: 4.129784]\n",
      "epoch:9 step:7344 [D loss: 0.628913, acc.: 76.56%] [G loss: 2.558491]\n",
      "epoch:9 step:7345 [D loss: 0.385871, acc.: 84.38%] [G loss: 3.576778]\n",
      "epoch:9 step:7346 [D loss: 0.448505, acc.: 75.78%] [G loss: 2.453972]\n",
      "epoch:9 step:7347 [D loss: 0.313320, acc.: 87.50%] [G loss: 2.119767]\n",
      "epoch:9 step:7348 [D loss: 0.328420, acc.: 89.84%] [G loss: 2.613352]\n",
      "epoch:9 step:7349 [D loss: 0.413004, acc.: 82.81%] [G loss: 2.861611]\n",
      "epoch:9 step:7350 [D loss: 0.325031, acc.: 89.84%] [G loss: 2.514197]\n",
      "epoch:9 step:7351 [D loss: 0.260278, acc.: 90.62%] [G loss: 3.169550]\n",
      "epoch:9 step:7352 [D loss: 0.246174, acc.: 93.75%] [G loss: 2.847341]\n",
      "epoch:9 step:7353 [D loss: 0.294386, acc.: 89.84%] [G loss: 2.194062]\n",
      "epoch:9 step:7354 [D loss: 0.497849, acc.: 78.12%] [G loss: 1.669305]\n",
      "epoch:9 step:7355 [D loss: 0.261002, acc.: 89.84%] [G loss: 2.580925]\n",
      "epoch:9 step:7356 [D loss: 0.287456, acc.: 90.62%] [G loss: 2.499555]\n",
      "epoch:9 step:7357 [D loss: 0.319110, acc.: 85.94%] [G loss: 2.849243]\n",
      "epoch:9 step:7358 [D loss: 0.360344, acc.: 89.06%] [G loss: 2.806847]\n",
      "epoch:9 step:7359 [D loss: 0.298076, acc.: 90.62%] [G loss: 2.988764]\n",
      "epoch:9 step:7360 [D loss: 0.314065, acc.: 87.50%] [G loss: 2.459779]\n",
      "epoch:9 step:7361 [D loss: 0.392082, acc.: 77.34%] [G loss: 2.594725]\n",
      "epoch:9 step:7362 [D loss: 0.266291, acc.: 87.50%] [G loss: 3.410304]\n",
      "epoch:9 step:7363 [D loss: 0.396496, acc.: 89.06%] [G loss: 3.294429]\n",
      "epoch:9 step:7364 [D loss: 0.312713, acc.: 87.50%] [G loss: 3.177532]\n",
      "epoch:9 step:7365 [D loss: 0.360400, acc.: 82.81%] [G loss: 2.896983]\n",
      "epoch:9 step:7366 [D loss: 0.221968, acc.: 92.19%] [G loss: 3.400076]\n",
      "epoch:9 step:7367 [D loss: 0.484157, acc.: 78.12%] [G loss: 2.036409]\n",
      "epoch:9 step:7368 [D loss: 0.271217, acc.: 90.62%] [G loss: 2.554924]\n",
      "epoch:9 step:7369 [D loss: 0.274025, acc.: 89.06%] [G loss: 2.813683]\n",
      "epoch:9 step:7370 [D loss: 0.305150, acc.: 87.50%] [G loss: 2.609359]\n",
      "epoch:9 step:7371 [D loss: 0.398309, acc.: 78.91%] [G loss: 2.663306]\n",
      "epoch:9 step:7372 [D loss: 0.482494, acc.: 79.69%] [G loss: 2.543344]\n",
      "epoch:9 step:7373 [D loss: 0.373759, acc.: 82.81%] [G loss: 2.646111]\n",
      "epoch:9 step:7374 [D loss: 0.259786, acc.: 89.84%] [G loss: 3.021399]\n",
      "epoch:9 step:7375 [D loss: 0.342369, acc.: 85.94%] [G loss: 4.158710]\n",
      "epoch:9 step:7376 [D loss: 0.236740, acc.: 89.06%] [G loss: 5.271953]\n",
      "epoch:9 step:7377 [D loss: 0.327182, acc.: 85.16%] [G loss: 3.221494]\n",
      "epoch:9 step:7378 [D loss: 0.424161, acc.: 78.12%] [G loss: 4.051734]\n",
      "epoch:9 step:7379 [D loss: 0.348146, acc.: 83.59%] [G loss: 3.716302]\n",
      "epoch:9 step:7380 [D loss: 0.235926, acc.: 92.97%] [G loss: 3.630054]\n",
      "epoch:9 step:7381 [D loss: 0.499301, acc.: 75.00%] [G loss: 2.114883]\n",
      "epoch:9 step:7382 [D loss: 0.290338, acc.: 88.28%] [G loss: 2.899964]\n",
      "epoch:9 step:7383 [D loss: 0.319340, acc.: 88.28%] [G loss: 2.963825]\n",
      "epoch:9 step:7384 [D loss: 0.445489, acc.: 84.38%] [G loss: 2.398748]\n",
      "epoch:9 step:7385 [D loss: 0.271640, acc.: 91.41%] [G loss: 3.159787]\n",
      "epoch:9 step:7386 [D loss: 0.310625, acc.: 85.16%] [G loss: 5.057423]\n",
      "epoch:9 step:7387 [D loss: 0.367733, acc.: 85.94%] [G loss: 3.123682]\n",
      "epoch:9 step:7388 [D loss: 0.339083, acc.: 86.72%] [G loss: 2.835563]\n",
      "epoch:9 step:7389 [D loss: 0.375814, acc.: 82.81%] [G loss: 2.739784]\n",
      "epoch:9 step:7390 [D loss: 0.288712, acc.: 88.28%] [G loss: 2.902700]\n",
      "epoch:9 step:7391 [D loss: 0.425164, acc.: 80.47%] [G loss: 2.240010]\n",
      "epoch:9 step:7392 [D loss: 0.443198, acc.: 80.47%] [G loss: 2.882727]\n",
      "epoch:9 step:7393 [D loss: 0.357120, acc.: 87.50%] [G loss: 2.991989]\n",
      "epoch:9 step:7394 [D loss: 0.359182, acc.: 84.38%] [G loss: 3.787238]\n",
      "epoch:9 step:7395 [D loss: 0.343000, acc.: 85.94%] [G loss: 3.608065]\n",
      "epoch:9 step:7396 [D loss: 0.309706, acc.: 88.28%] [G loss: 3.081047]\n",
      "epoch:9 step:7397 [D loss: 0.393925, acc.: 79.69%] [G loss: 1.857029]\n",
      "epoch:9 step:7398 [D loss: 0.266629, acc.: 92.19%] [G loss: 2.122151]\n",
      "epoch:9 step:7399 [D loss: 0.410768, acc.: 80.47%] [G loss: 2.740810]\n",
      "epoch:9 step:7400 [D loss: 0.372634, acc.: 85.16%] [G loss: 2.407142]\n",
      "epoch:9 step:7401 [D loss: 0.435909, acc.: 81.25%] [G loss: 2.681084]\n",
      "epoch:9 step:7402 [D loss: 0.424292, acc.: 82.81%] [G loss: 5.859603]\n",
      "epoch:9 step:7403 [D loss: 0.683274, acc.: 80.47%] [G loss: 3.873956]\n",
      "epoch:9 step:7404 [D loss: 0.517424, acc.: 76.56%] [G loss: 2.801802]\n",
      "epoch:9 step:7405 [D loss: 0.667039, acc.: 72.66%] [G loss: 2.437492]\n",
      "epoch:9 step:7406 [D loss: 0.397146, acc.: 82.81%] [G loss: 2.744194]\n",
      "epoch:9 step:7407 [D loss: 0.462436, acc.: 77.34%] [G loss: 2.619432]\n",
      "epoch:9 step:7408 [D loss: 0.178496, acc.: 94.53%] [G loss: 3.170435]\n",
      "epoch:9 step:7409 [D loss: 0.258135, acc.: 89.06%] [G loss: 3.715608]\n",
      "epoch:9 step:7410 [D loss: 0.436307, acc.: 78.12%] [G loss: 2.336882]\n",
      "epoch:9 step:7411 [D loss: 0.449928, acc.: 80.47%] [G loss: 2.450166]\n",
      "epoch:9 step:7412 [D loss: 0.306552, acc.: 86.72%] [G loss: 2.135035]\n",
      "epoch:9 step:7413 [D loss: 0.312249, acc.: 88.28%] [G loss: 2.729099]\n",
      "epoch:9 step:7414 [D loss: 0.288128, acc.: 88.28%] [G loss: 3.464065]\n",
      "epoch:9 step:7415 [D loss: 0.262129, acc.: 88.28%] [G loss: 4.011058]\n",
      "epoch:9 step:7416 [D loss: 0.412249, acc.: 82.81%] [G loss: 2.840632]\n",
      "epoch:9 step:7417 [D loss: 0.481965, acc.: 73.44%] [G loss: 2.575655]\n",
      "epoch:9 step:7418 [D loss: 0.307514, acc.: 84.38%] [G loss: 3.041951]\n",
      "epoch:9 step:7419 [D loss: 0.292580, acc.: 91.41%] [G loss: 2.943701]\n",
      "epoch:9 step:7420 [D loss: 0.411474, acc.: 78.91%] [G loss: 3.898767]\n",
      "epoch:9 step:7421 [D loss: 0.312277, acc.: 85.94%] [G loss: 2.294291]\n",
      "epoch:9 step:7422 [D loss: 0.272597, acc.: 91.41%] [G loss: 2.770667]\n",
      "epoch:9 step:7423 [D loss: 0.284963, acc.: 92.97%] [G loss: 2.610548]\n",
      "epoch:9 step:7424 [D loss: 0.329882, acc.: 85.94%] [G loss: 1.881922]\n",
      "epoch:9 step:7425 [D loss: 0.219646, acc.: 96.09%] [G loss: 2.339294]\n",
      "epoch:9 step:7426 [D loss: 0.280267, acc.: 87.50%] [G loss: 2.353270]\n",
      "epoch:9 step:7427 [D loss: 0.315540, acc.: 85.94%] [G loss: 2.330519]\n",
      "epoch:9 step:7428 [D loss: 0.329332, acc.: 87.50%] [G loss: 2.529377]\n",
      "epoch:9 step:7429 [D loss: 0.282752, acc.: 88.28%] [G loss: 3.735148]\n",
      "epoch:9 step:7430 [D loss: 0.270757, acc.: 92.19%] [G loss: 2.749910]\n",
      "epoch:9 step:7431 [D loss: 0.310059, acc.: 89.84%] [G loss: 3.017472]\n",
      "epoch:9 step:7432 [D loss: 0.273112, acc.: 89.84%] [G loss: 2.836486]\n",
      "epoch:9 step:7433 [D loss: 0.366579, acc.: 83.59%] [G loss: 2.186616]\n",
      "epoch:9 step:7434 [D loss: 0.184196, acc.: 93.75%] [G loss: 4.117472]\n",
      "epoch:9 step:7435 [D loss: 0.455100, acc.: 83.59%] [G loss: 2.652645]\n",
      "epoch:9 step:7436 [D loss: 0.291423, acc.: 88.28%] [G loss: 2.777538]\n",
      "epoch:9 step:7437 [D loss: 0.255686, acc.: 88.28%] [G loss: 4.388786]\n",
      "epoch:9 step:7438 [D loss: 0.323411, acc.: 86.72%] [G loss: 2.893724]\n",
      "epoch:9 step:7439 [D loss: 0.352044, acc.: 84.38%] [G loss: 3.251949]\n",
      "epoch:9 step:7440 [D loss: 0.416193, acc.: 81.25%] [G loss: 2.749777]\n",
      "epoch:9 step:7441 [D loss: 0.466689, acc.: 76.56%] [G loss: 4.038878]\n",
      "epoch:9 step:7442 [D loss: 0.398114, acc.: 79.69%] [G loss: 2.417369]\n",
      "epoch:9 step:7443 [D loss: 0.392641, acc.: 81.25%] [G loss: 3.471069]\n",
      "epoch:9 step:7444 [D loss: 0.285424, acc.: 91.41%] [G loss: 4.144125]\n",
      "epoch:9 step:7445 [D loss: 0.327998, acc.: 87.50%] [G loss: 3.505901]\n",
      "epoch:9 step:7446 [D loss: 0.270308, acc.: 91.41%] [G loss: 2.744304]\n",
      "epoch:9 step:7447 [D loss: 0.260588, acc.: 89.06%] [G loss: 3.860687]\n",
      "epoch:9 step:7448 [D loss: 0.263812, acc.: 89.84%] [G loss: 3.218045]\n",
      "epoch:9 step:7449 [D loss: 0.366934, acc.: 75.78%] [G loss: 2.243512]\n",
      "epoch:9 step:7450 [D loss: 0.333321, acc.: 87.50%] [G loss: 2.966015]\n",
      "epoch:9 step:7451 [D loss: 0.313862, acc.: 88.28%] [G loss: 5.037329]\n",
      "epoch:9 step:7452 [D loss: 0.388970, acc.: 85.16%] [G loss: 2.804603]\n",
      "epoch:9 step:7453 [D loss: 0.378690, acc.: 82.03%] [G loss: 2.772426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7454 [D loss: 0.312360, acc.: 83.59%] [G loss: 4.964022]\n",
      "epoch:9 step:7455 [D loss: 0.522817, acc.: 77.34%] [G loss: 4.567186]\n",
      "epoch:9 step:7456 [D loss: 0.494544, acc.: 78.12%] [G loss: 3.237543]\n",
      "epoch:9 step:7457 [D loss: 0.522868, acc.: 71.88%] [G loss: 3.670533]\n",
      "epoch:9 step:7458 [D loss: 0.595680, acc.: 72.66%] [G loss: 4.445825]\n",
      "epoch:9 step:7459 [D loss: 0.345917, acc.: 86.72%] [G loss: 3.317200]\n",
      "epoch:9 step:7460 [D loss: 0.522905, acc.: 78.91%] [G loss: 3.548621]\n",
      "epoch:9 step:7461 [D loss: 0.497367, acc.: 75.00%] [G loss: 2.145785]\n",
      "epoch:9 step:7462 [D loss: 0.321688, acc.: 86.72%] [G loss: 2.964185]\n",
      "epoch:9 step:7463 [D loss: 0.371889, acc.: 85.94%] [G loss: 3.346592]\n",
      "epoch:9 step:7464 [D loss: 0.350641, acc.: 85.16%] [G loss: 2.404817]\n",
      "epoch:9 step:7465 [D loss: 0.372102, acc.: 78.91%] [G loss: 3.389045]\n",
      "epoch:9 step:7466 [D loss: 0.315331, acc.: 88.28%] [G loss: 2.959759]\n",
      "epoch:9 step:7467 [D loss: 0.326056, acc.: 83.59%] [G loss: 2.366211]\n",
      "epoch:9 step:7468 [D loss: 0.301926, acc.: 92.19%] [G loss: 2.560111]\n",
      "epoch:9 step:7469 [D loss: 0.276201, acc.: 85.16%] [G loss: 2.943354]\n",
      "epoch:9 step:7470 [D loss: 0.417090, acc.: 82.03%] [G loss: 1.881814]\n",
      "epoch:9 step:7471 [D loss: 0.254340, acc.: 90.62%] [G loss: 3.030227]\n",
      "epoch:9 step:7472 [D loss: 0.304741, acc.: 87.50%] [G loss: 3.836651]\n",
      "epoch:9 step:7473 [D loss: 0.323483, acc.: 87.50%] [G loss: 3.107029]\n",
      "epoch:9 step:7474 [D loss: 0.291977, acc.: 90.62%] [G loss: 2.416773]\n",
      "epoch:9 step:7475 [D loss: 0.258796, acc.: 89.84%] [G loss: 2.243463]\n",
      "epoch:9 step:7476 [D loss: 0.357290, acc.: 86.72%] [G loss: 2.443398]\n",
      "epoch:9 step:7477 [D loss: 0.375814, acc.: 87.50%] [G loss: 2.423431]\n",
      "epoch:9 step:7478 [D loss: 0.313828, acc.: 89.06%] [G loss: 2.219430]\n",
      "epoch:9 step:7479 [D loss: 0.275220, acc.: 86.72%] [G loss: 2.347029]\n",
      "epoch:9 step:7480 [D loss: 0.309596, acc.: 84.38%] [G loss: 3.046997]\n",
      "epoch:9 step:7481 [D loss: 0.270009, acc.: 89.84%] [G loss: 2.338218]\n",
      "epoch:9 step:7482 [D loss: 0.315105, acc.: 86.72%] [G loss: 3.445218]\n",
      "epoch:9 step:7483 [D loss: 0.222059, acc.: 90.62%] [G loss: 4.940846]\n",
      "epoch:9 step:7484 [D loss: 0.266880, acc.: 89.06%] [G loss: 2.638854]\n",
      "epoch:9 step:7485 [D loss: 0.365503, acc.: 85.16%] [G loss: 1.745621]\n",
      "epoch:9 step:7486 [D loss: 0.346274, acc.: 85.94%] [G loss: 2.648083]\n",
      "epoch:9 step:7487 [D loss: 0.214121, acc.: 92.19%] [G loss: 4.328118]\n",
      "epoch:9 step:7488 [D loss: 0.254145, acc.: 88.28%] [G loss: 2.884092]\n",
      "epoch:9 step:7489 [D loss: 0.272685, acc.: 89.84%] [G loss: 3.915116]\n",
      "epoch:9 step:7490 [D loss: 0.255608, acc.: 89.06%] [G loss: 4.336915]\n",
      "epoch:9 step:7491 [D loss: 0.293376, acc.: 88.28%] [G loss: 2.571543]\n",
      "epoch:9 step:7492 [D loss: 0.319429, acc.: 85.16%] [G loss: 2.600492]\n",
      "epoch:9 step:7493 [D loss: 0.309572, acc.: 89.06%] [G loss: 2.925798]\n",
      "epoch:9 step:7494 [D loss: 0.351056, acc.: 86.72%] [G loss: 3.077838]\n",
      "epoch:9 step:7495 [D loss: 0.335028, acc.: 82.03%] [G loss: 5.972929]\n",
      "epoch:9 step:7496 [D loss: 0.346025, acc.: 88.28%] [G loss: 3.277474]\n",
      "epoch:9 step:7497 [D loss: 0.213233, acc.: 94.53%] [G loss: 2.914673]\n",
      "epoch:9 step:7498 [D loss: 0.313558, acc.: 85.94%] [G loss: 1.670007]\n",
      "epoch:9 step:7499 [D loss: 0.262147, acc.: 90.62%] [G loss: 2.483827]\n",
      "epoch:9 step:7500 [D loss: 0.319477, acc.: 88.28%] [G loss: 2.320281]\n",
      "epoch:9 step:7501 [D loss: 0.328245, acc.: 86.72%] [G loss: 3.664822]\n",
      "epoch:9 step:7502 [D loss: 0.234299, acc.: 94.53%] [G loss: 3.142942]\n",
      "epoch:9 step:7503 [D loss: 0.294612, acc.: 87.50%] [G loss: 3.156971]\n",
      "epoch:9 step:7504 [D loss: 0.249361, acc.: 91.41%] [G loss: 2.536291]\n",
      "epoch:9 step:7505 [D loss: 0.333655, acc.: 87.50%] [G loss: 1.683375]\n",
      "epoch:9 step:7506 [D loss: 0.334990, acc.: 87.50%] [G loss: 2.639297]\n",
      "epoch:9 step:7507 [D loss: 0.295759, acc.: 89.84%] [G loss: 2.539366]\n",
      "epoch:9 step:7508 [D loss: 0.290635, acc.: 87.50%] [G loss: 2.172402]\n",
      "epoch:9 step:7509 [D loss: 0.307095, acc.: 85.94%] [G loss: 2.330225]\n",
      "epoch:9 step:7510 [D loss: 0.417579, acc.: 81.25%] [G loss: 3.167486]\n",
      "epoch:9 step:7511 [D loss: 0.262035, acc.: 89.06%] [G loss: 5.336942]\n",
      "epoch:9 step:7512 [D loss: 0.270702, acc.: 91.41%] [G loss: 3.841381]\n",
      "epoch:9 step:7513 [D loss: 0.358901, acc.: 86.72%] [G loss: 3.115386]\n",
      "epoch:9 step:7514 [D loss: 0.237720, acc.: 90.62%] [G loss: 3.696120]\n",
      "epoch:9 step:7515 [D loss: 0.201443, acc.: 92.19%] [G loss: 6.147861]\n",
      "epoch:9 step:7516 [D loss: 0.335208, acc.: 82.81%] [G loss: 3.060027]\n",
      "epoch:9 step:7517 [D loss: 0.447176, acc.: 80.47%] [G loss: 2.753366]\n",
      "epoch:9 step:7518 [D loss: 0.467702, acc.: 78.12%] [G loss: 3.169263]\n",
      "epoch:9 step:7519 [D loss: 0.370856, acc.: 87.50%] [G loss: 3.041536]\n",
      "epoch:9 step:7520 [D loss: 0.327603, acc.: 86.72%] [G loss: 4.249139]\n",
      "epoch:9 step:7521 [D loss: 0.412689, acc.: 85.16%] [G loss: 3.803182]\n",
      "epoch:9 step:7522 [D loss: 0.301162, acc.: 85.16%] [G loss: 3.112991]\n",
      "epoch:9 step:7523 [D loss: 0.354259, acc.: 84.38%] [G loss: 2.569287]\n",
      "epoch:9 step:7524 [D loss: 0.368649, acc.: 89.84%] [G loss: 2.172699]\n",
      "epoch:9 step:7525 [D loss: 0.301839, acc.: 82.81%] [G loss: 4.061604]\n",
      "epoch:9 step:7526 [D loss: 0.287352, acc.: 89.06%] [G loss: 4.390291]\n",
      "epoch:9 step:7527 [D loss: 0.313921, acc.: 86.72%] [G loss: 3.331225]\n",
      "epoch:9 step:7528 [D loss: 0.388749, acc.: 85.16%] [G loss: 3.335335]\n",
      "epoch:9 step:7529 [D loss: 0.469072, acc.: 75.78%] [G loss: 3.116402]\n",
      "epoch:9 step:7530 [D loss: 0.493843, acc.: 77.34%] [G loss: 4.512540]\n",
      "epoch:9 step:7531 [D loss: 0.517691, acc.: 75.78%] [G loss: 5.852088]\n",
      "epoch:9 step:7532 [D loss: 0.739577, acc.: 71.88%] [G loss: 5.220122]\n",
      "epoch:9 step:7533 [D loss: 1.202275, acc.: 58.59%] [G loss: 6.494267]\n",
      "epoch:9 step:7534 [D loss: 2.538347, acc.: 40.62%] [G loss: 1.696784]\n",
      "epoch:9 step:7535 [D loss: 0.575477, acc.: 78.91%] [G loss: 3.124995]\n",
      "epoch:9 step:7536 [D loss: 0.865244, acc.: 73.44%] [G loss: 2.978062]\n",
      "epoch:9 step:7537 [D loss: 0.230199, acc.: 89.84%] [G loss: 3.978251]\n",
      "epoch:9 step:7538 [D loss: 0.540449, acc.: 73.44%] [G loss: 2.791909]\n",
      "epoch:9 step:7539 [D loss: 0.361013, acc.: 83.59%] [G loss: 2.872399]\n",
      "epoch:9 step:7540 [D loss: 0.354098, acc.: 82.03%] [G loss: 2.904014]\n",
      "epoch:9 step:7541 [D loss: 0.293191, acc.: 88.28%] [G loss: 2.565420]\n",
      "epoch:9 step:7542 [D loss: 0.375718, acc.: 82.81%] [G loss: 2.308247]\n",
      "epoch:9 step:7543 [D loss: 0.258370, acc.: 91.41%] [G loss: 2.585253]\n",
      "epoch:9 step:7544 [D loss: 0.443035, acc.: 79.69%] [G loss: 2.837632]\n",
      "epoch:9 step:7545 [D loss: 0.356655, acc.: 88.28%] [G loss: 2.658256]\n",
      "epoch:9 step:7546 [D loss: 0.506777, acc.: 77.34%] [G loss: 1.829135]\n",
      "epoch:9 step:7547 [D loss: 0.320736, acc.: 88.28%] [G loss: 2.211244]\n",
      "epoch:9 step:7548 [D loss: 0.296258, acc.: 89.06%] [G loss: 2.701972]\n",
      "epoch:9 step:7549 [D loss: 0.508546, acc.: 82.03%] [G loss: 4.095778]\n",
      "epoch:9 step:7550 [D loss: 0.265316, acc.: 85.94%] [G loss: 3.792158]\n",
      "epoch:9 step:7551 [D loss: 0.345430, acc.: 85.94%] [G loss: 2.155935]\n",
      "epoch:9 step:7552 [D loss: 0.265888, acc.: 90.62%] [G loss: 3.771523]\n",
      "epoch:9 step:7553 [D loss: 0.418286, acc.: 77.34%] [G loss: 1.993526]\n",
      "epoch:9 step:7554 [D loss: 0.349492, acc.: 83.59%] [G loss: 2.548766]\n",
      "epoch:9 step:7555 [D loss: 0.413933, acc.: 76.56%] [G loss: 2.399902]\n",
      "epoch:9 step:7556 [D loss: 0.351914, acc.: 85.16%] [G loss: 2.105653]\n",
      "epoch:9 step:7557 [D loss: 0.376357, acc.: 83.59%] [G loss: 2.364863]\n",
      "epoch:9 step:7558 [D loss: 0.270745, acc.: 88.28%] [G loss: 3.124254]\n",
      "epoch:9 step:7559 [D loss: 0.352500, acc.: 82.81%] [G loss: 2.347030]\n",
      "epoch:9 step:7560 [D loss: 0.383867, acc.: 84.38%] [G loss: 2.160797]\n",
      "epoch:9 step:7561 [D loss: 0.394327, acc.: 88.28%] [G loss: 2.333134]\n",
      "epoch:9 step:7562 [D loss: 0.347359, acc.: 85.16%] [G loss: 2.521372]\n",
      "epoch:9 step:7563 [D loss: 0.409557, acc.: 80.47%] [G loss: 2.961920]\n",
      "epoch:9 step:7564 [D loss: 0.417231, acc.: 78.12%] [G loss: 2.243183]\n",
      "epoch:9 step:7565 [D loss: 0.412531, acc.: 84.38%] [G loss: 2.541366]\n",
      "epoch:9 step:7566 [D loss: 0.446470, acc.: 81.25%] [G loss: 1.935842]\n",
      "epoch:9 step:7567 [D loss: 0.281634, acc.: 89.84%] [G loss: 2.925143]\n",
      "epoch:9 step:7568 [D loss: 0.266833, acc.: 89.84%] [G loss: 3.120702]\n",
      "epoch:9 step:7569 [D loss: 0.377541, acc.: 80.47%] [G loss: 2.985843]\n",
      "epoch:9 step:7570 [D loss: 0.346074, acc.: 82.03%] [G loss: 4.090298]\n",
      "epoch:9 step:7571 [D loss: 0.250863, acc.: 90.62%] [G loss: 3.200841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7572 [D loss: 0.431921, acc.: 80.47%] [G loss: 2.005273]\n",
      "epoch:9 step:7573 [D loss: 0.331172, acc.: 85.94%] [G loss: 2.081245]\n",
      "epoch:9 step:7574 [D loss: 0.436670, acc.: 79.69%] [G loss: 2.296032]\n",
      "epoch:9 step:7575 [D loss: 0.269018, acc.: 88.28%] [G loss: 2.969603]\n",
      "epoch:9 step:7576 [D loss: 0.350010, acc.: 82.81%] [G loss: 3.740777]\n",
      "epoch:9 step:7577 [D loss: 0.346330, acc.: 85.16%] [G loss: 3.546800]\n",
      "epoch:9 step:7578 [D loss: 0.299635, acc.: 87.50%] [G loss: 3.306586]\n",
      "epoch:9 step:7579 [D loss: 0.320598, acc.: 85.16%] [G loss: 2.523407]\n",
      "epoch:9 step:7580 [D loss: 0.420088, acc.: 82.03%] [G loss: 3.013750]\n",
      "epoch:9 step:7581 [D loss: 0.260342, acc.: 88.28%] [G loss: 3.408919]\n",
      "epoch:9 step:7582 [D loss: 0.448410, acc.: 79.69%] [G loss: 2.313823]\n",
      "epoch:9 step:7583 [D loss: 0.424721, acc.: 81.25%] [G loss: 2.592003]\n",
      "epoch:9 step:7584 [D loss: 0.470213, acc.: 76.56%] [G loss: 2.544250]\n",
      "epoch:9 step:7585 [D loss: 0.315384, acc.: 85.16%] [G loss: 3.163219]\n",
      "epoch:9 step:7586 [D loss: 0.416471, acc.: 83.59%] [G loss: 2.592859]\n",
      "epoch:9 step:7587 [D loss: 0.298083, acc.: 89.84%] [G loss: 3.288768]\n",
      "epoch:9 step:7588 [D loss: 0.293408, acc.: 87.50%] [G loss: 3.133596]\n",
      "epoch:9 step:7589 [D loss: 0.329748, acc.: 89.06%] [G loss: 2.562182]\n",
      "epoch:9 step:7590 [D loss: 0.334019, acc.: 88.28%] [G loss: 2.955135]\n",
      "epoch:9 step:7591 [D loss: 0.428990, acc.: 82.03%] [G loss: 2.455404]\n",
      "epoch:9 step:7592 [D loss: 0.414980, acc.: 78.91%] [G loss: 2.476402]\n",
      "epoch:9 step:7593 [D loss: 0.307587, acc.: 89.06%] [G loss: 2.291298]\n",
      "epoch:9 step:7594 [D loss: 0.429368, acc.: 83.59%] [G loss: 3.152728]\n",
      "epoch:9 step:7595 [D loss: 0.503547, acc.: 78.91%] [G loss: 2.741201]\n",
      "epoch:9 step:7596 [D loss: 0.291741, acc.: 86.72%] [G loss: 2.873995]\n",
      "epoch:9 step:7597 [D loss: 0.234380, acc.: 91.41%] [G loss: 2.857880]\n",
      "epoch:9 step:7598 [D loss: 0.401602, acc.: 84.38%] [G loss: 3.234581]\n",
      "epoch:9 step:7599 [D loss: 0.335777, acc.: 84.38%] [G loss: 4.111516]\n",
      "epoch:9 step:7600 [D loss: 0.266806, acc.: 88.28%] [G loss: 4.013633]\n",
      "epoch:9 step:7601 [D loss: 0.289538, acc.: 88.28%] [G loss: 3.296004]\n",
      "epoch:9 step:7602 [D loss: 0.479721, acc.: 75.78%] [G loss: 1.941611]\n",
      "epoch:9 step:7603 [D loss: 0.272276, acc.: 86.72%] [G loss: 2.847132]\n",
      "epoch:9 step:7604 [D loss: 0.341483, acc.: 85.16%] [G loss: 2.367795]\n",
      "epoch:9 step:7605 [D loss: 0.320026, acc.: 84.38%] [G loss: 2.764463]\n",
      "epoch:9 step:7606 [D loss: 0.310013, acc.: 88.28%] [G loss: 3.143168]\n",
      "epoch:9 step:7607 [D loss: 0.332022, acc.: 86.72%] [G loss: 2.442895]\n",
      "epoch:9 step:7608 [D loss: 0.346389, acc.: 82.81%] [G loss: 3.022007]\n",
      "epoch:9 step:7609 [D loss: 0.245901, acc.: 87.50%] [G loss: 4.757336]\n",
      "epoch:9 step:7610 [D loss: 0.352963, acc.: 85.16%] [G loss: 3.146825]\n",
      "epoch:9 step:7611 [D loss: 0.316407, acc.: 86.72%] [G loss: 2.650919]\n",
      "epoch:9 step:7612 [D loss: 0.304171, acc.: 85.94%] [G loss: 3.261761]\n",
      "epoch:9 step:7613 [D loss: 0.430274, acc.: 79.69%] [G loss: 3.260345]\n",
      "epoch:9 step:7614 [D loss: 0.460201, acc.: 79.69%] [G loss: 2.047767]\n",
      "epoch:9 step:7615 [D loss: 0.268767, acc.: 86.72%] [G loss: 4.874897]\n",
      "epoch:9 step:7616 [D loss: 0.306309, acc.: 86.72%] [G loss: 2.270337]\n",
      "epoch:9 step:7617 [D loss: 0.383493, acc.: 84.38%] [G loss: 3.855424]\n",
      "epoch:9 step:7618 [D loss: 0.333024, acc.: 86.72%] [G loss: 3.471200]\n",
      "epoch:9 step:7619 [D loss: 0.370744, acc.: 85.94%] [G loss: 4.350508]\n",
      "epoch:9 step:7620 [D loss: 0.200920, acc.: 91.41%] [G loss: 6.202695]\n",
      "epoch:9 step:7621 [D loss: 0.337456, acc.: 86.72%] [G loss: 3.081511]\n",
      "epoch:9 step:7622 [D loss: 0.330494, acc.: 85.94%] [G loss: 4.220032]\n",
      "epoch:9 step:7623 [D loss: 0.241941, acc.: 89.84%] [G loss: 5.566592]\n",
      "epoch:9 step:7624 [D loss: 0.422892, acc.: 78.12%] [G loss: 3.458536]\n",
      "epoch:9 step:7625 [D loss: 0.473736, acc.: 77.34%] [G loss: 2.844927]\n",
      "epoch:9 step:7626 [D loss: 0.366058, acc.: 84.38%] [G loss: 4.499273]\n",
      "epoch:9 step:7627 [D loss: 0.233407, acc.: 91.41%] [G loss: 5.091969]\n",
      "epoch:9 step:7628 [D loss: 0.273053, acc.: 89.84%] [G loss: 4.280096]\n",
      "epoch:9 step:7629 [D loss: 0.304321, acc.: 87.50%] [G loss: 2.904664]\n",
      "epoch:9 step:7630 [D loss: 0.251279, acc.: 90.62%] [G loss: 2.921202]\n",
      "epoch:9 step:7631 [D loss: 0.263391, acc.: 87.50%] [G loss: 3.771331]\n",
      "epoch:9 step:7632 [D loss: 0.304445, acc.: 88.28%] [G loss: 2.846560]\n",
      "epoch:9 step:7633 [D loss: 0.409237, acc.: 83.59%] [G loss: 2.961535]\n",
      "epoch:9 step:7634 [D loss: 0.403515, acc.: 83.59%] [G loss: 2.928954]\n",
      "epoch:9 step:7635 [D loss: 0.351578, acc.: 86.72%] [G loss: 3.005398]\n",
      "epoch:9 step:7636 [D loss: 0.350507, acc.: 84.38%] [G loss: 2.598606]\n",
      "epoch:9 step:7637 [D loss: 0.265556, acc.: 89.84%] [G loss: 2.670446]\n",
      "epoch:9 step:7638 [D loss: 0.256970, acc.: 91.41%] [G loss: 4.315390]\n",
      "epoch:9 step:7639 [D loss: 0.320911, acc.: 84.38%] [G loss: 3.082674]\n",
      "epoch:9 step:7640 [D loss: 0.352772, acc.: 82.81%] [G loss: 2.333511]\n",
      "epoch:9 step:7641 [D loss: 0.285008, acc.: 90.62%] [G loss: 2.848829]\n",
      "epoch:9 step:7642 [D loss: 0.311284, acc.: 88.28%] [G loss: 2.590231]\n",
      "epoch:9 step:7643 [D loss: 0.357825, acc.: 85.16%] [G loss: 2.455974]\n",
      "epoch:9 step:7644 [D loss: 0.360677, acc.: 84.38%] [G loss: 2.205832]\n",
      "epoch:9 step:7645 [D loss: 0.355268, acc.: 85.16%] [G loss: 2.133292]\n",
      "epoch:9 step:7646 [D loss: 0.336181, acc.: 87.50%] [G loss: 2.030809]\n",
      "epoch:9 step:7647 [D loss: 0.379382, acc.: 87.50%] [G loss: 2.690473]\n",
      "epoch:9 step:7648 [D loss: 0.410638, acc.: 81.25%] [G loss: 2.584707]\n",
      "epoch:9 step:7649 [D loss: 0.260273, acc.: 88.28%] [G loss: 2.812600]\n",
      "epoch:9 step:7650 [D loss: 0.238264, acc.: 90.62%] [G loss: 3.808910]\n",
      "epoch:9 step:7651 [D loss: 0.270456, acc.: 88.28%] [G loss: 3.479838]\n",
      "epoch:9 step:7652 [D loss: 0.457859, acc.: 79.69%] [G loss: 2.546885]\n",
      "epoch:9 step:7653 [D loss: 0.302067, acc.: 87.50%] [G loss: 2.234290]\n",
      "epoch:9 step:7654 [D loss: 0.238127, acc.: 94.53%] [G loss: 2.897160]\n",
      "epoch:9 step:7655 [D loss: 0.207324, acc.: 93.75%] [G loss: 2.104863]\n",
      "epoch:9 step:7656 [D loss: 0.281957, acc.: 88.28%] [G loss: 2.203326]\n",
      "epoch:9 step:7657 [D loss: 0.369892, acc.: 85.16%] [G loss: 2.287421]\n",
      "epoch:9 step:7658 [D loss: 0.370110, acc.: 87.50%] [G loss: 3.218145]\n",
      "epoch:9 step:7659 [D loss: 0.279122, acc.: 85.94%] [G loss: 3.294835]\n",
      "epoch:9 step:7660 [D loss: 0.295178, acc.: 90.62%] [G loss: 2.613199]\n",
      "epoch:9 step:7661 [D loss: 0.341379, acc.: 88.28%] [G loss: 3.161008]\n",
      "epoch:9 step:7662 [D loss: 0.363311, acc.: 83.59%] [G loss: 3.173738]\n",
      "epoch:9 step:7663 [D loss: 0.331800, acc.: 83.59%] [G loss: 3.924173]\n",
      "epoch:9 step:7664 [D loss: 0.412494, acc.: 79.69%] [G loss: 2.471825]\n",
      "epoch:9 step:7665 [D loss: 0.322739, acc.: 87.50%] [G loss: 2.448210]\n",
      "epoch:9 step:7666 [D loss: 0.263390, acc.: 91.41%] [G loss: 2.830833]\n",
      "epoch:9 step:7667 [D loss: 0.429501, acc.: 84.38%] [G loss: 2.663216]\n",
      "epoch:9 step:7668 [D loss: 0.396438, acc.: 85.94%] [G loss: 2.789289]\n",
      "epoch:9 step:7669 [D loss: 0.309819, acc.: 86.72%] [G loss: 3.933031]\n",
      "epoch:9 step:7670 [D loss: 0.388243, acc.: 82.03%] [G loss: 2.785150]\n",
      "epoch:9 step:7671 [D loss: 0.324804, acc.: 85.94%] [G loss: 5.119777]\n",
      "epoch:9 step:7672 [D loss: 0.302168, acc.: 89.06%] [G loss: 3.280857]\n",
      "epoch:9 step:7673 [D loss: 0.306744, acc.: 85.94%] [G loss: 3.608312]\n",
      "epoch:9 step:7674 [D loss: 0.307415, acc.: 87.50%] [G loss: 2.505202]\n",
      "epoch:9 step:7675 [D loss: 0.382380, acc.: 85.16%] [G loss: 2.956301]\n",
      "epoch:9 step:7676 [D loss: 0.369554, acc.: 85.16%] [G loss: 2.637892]\n",
      "epoch:9 step:7677 [D loss: 0.485165, acc.: 78.91%] [G loss: 2.789438]\n",
      "epoch:9 step:7678 [D loss: 0.393706, acc.: 85.94%] [G loss: 2.769674]\n",
      "epoch:9 step:7679 [D loss: 0.351137, acc.: 86.72%] [G loss: 2.020432]\n",
      "epoch:9 step:7680 [D loss: 0.356586, acc.: 88.28%] [G loss: 2.492573]\n",
      "epoch:9 step:7681 [D loss: 0.303233, acc.: 89.06%] [G loss: 2.533799]\n",
      "epoch:9 step:7682 [D loss: 0.375059, acc.: 81.25%] [G loss: 3.247491]\n",
      "epoch:9 step:7683 [D loss: 0.302135, acc.: 85.94%] [G loss: 3.308809]\n",
      "epoch:9 step:7684 [D loss: 0.370154, acc.: 82.81%] [G loss: 2.172471]\n",
      "epoch:9 step:7685 [D loss: 0.272844, acc.: 89.84%] [G loss: 2.755153]\n",
      "epoch:9 step:7686 [D loss: 0.344036, acc.: 86.72%] [G loss: 3.620040]\n",
      "epoch:9 step:7687 [D loss: 0.283851, acc.: 90.62%] [G loss: 2.036338]\n",
      "epoch:9 step:7688 [D loss: 0.331044, acc.: 84.38%] [G loss: 2.536085]\n",
      "epoch:9 step:7689 [D loss: 0.276931, acc.: 87.50%] [G loss: 4.406405]\n",
      "epoch:9 step:7690 [D loss: 0.394486, acc.: 82.03%] [G loss: 2.385728]\n",
      "epoch:9 step:7691 [D loss: 0.266253, acc.: 86.72%] [G loss: 5.104330]\n",
      "epoch:9 step:7692 [D loss: 0.302999, acc.: 86.72%] [G loss: 2.332293]\n",
      "epoch:9 step:7693 [D loss: 0.356334, acc.: 80.47%] [G loss: 4.301366]\n",
      "epoch:9 step:7694 [D loss: 0.212895, acc.: 91.41%] [G loss: 4.322460]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7695 [D loss: 0.380161, acc.: 82.03%] [G loss: 2.390683]\n",
      "epoch:9 step:7696 [D loss: 0.360925, acc.: 81.25%] [G loss: 2.893255]\n",
      "epoch:9 step:7697 [D loss: 0.313006, acc.: 87.50%] [G loss: 3.680634]\n",
      "epoch:9 step:7698 [D loss: 0.384599, acc.: 84.38%] [G loss: 3.223206]\n",
      "epoch:9 step:7699 [D loss: 0.556923, acc.: 71.88%] [G loss: 3.477316]\n",
      "epoch:9 step:7700 [D loss: 0.553509, acc.: 73.44%] [G loss: 3.733734]\n",
      "epoch:9 step:7701 [D loss: 0.666194, acc.: 78.12%] [G loss: 6.542350]\n",
      "epoch:9 step:7702 [D loss: 1.535899, acc.: 63.28%] [G loss: 6.188013]\n",
      "epoch:9 step:7703 [D loss: 2.727109, acc.: 50.00%] [G loss: 1.646508]\n",
      "epoch:9 step:7704 [D loss: 0.526209, acc.: 76.56%] [G loss: 4.736314]\n",
      "epoch:9 step:7705 [D loss: 0.640567, acc.: 71.88%] [G loss: 3.271956]\n",
      "epoch:9 step:7706 [D loss: 0.360585, acc.: 88.28%] [G loss: 2.830189]\n",
      "epoch:9 step:7707 [D loss: 0.572594, acc.: 83.59%] [G loss: 5.469191]\n",
      "epoch:9 step:7708 [D loss: 0.818624, acc.: 68.75%] [G loss: 2.362260]\n",
      "epoch:9 step:7709 [D loss: 0.326236, acc.: 86.72%] [G loss: 4.266693]\n",
      "epoch:9 step:7710 [D loss: 0.377608, acc.: 85.94%] [G loss: 3.007653]\n",
      "epoch:9 step:7711 [D loss: 0.373669, acc.: 85.16%] [G loss: 3.024253]\n",
      "epoch:9 step:7712 [D loss: 0.263351, acc.: 90.62%] [G loss: 2.824121]\n",
      "epoch:9 step:7713 [D loss: 0.393350, acc.: 82.03%] [G loss: 2.535946]\n",
      "epoch:9 step:7714 [D loss: 0.332397, acc.: 86.72%] [G loss: 2.827632]\n",
      "epoch:9 step:7715 [D loss: 0.309700, acc.: 90.62%] [G loss: 2.722839]\n",
      "epoch:9 step:7716 [D loss: 0.274667, acc.: 90.62%] [G loss: 2.543540]\n",
      "epoch:9 step:7717 [D loss: 0.301215, acc.: 86.72%] [G loss: 2.837438]\n",
      "epoch:9 step:7718 [D loss: 0.347704, acc.: 82.81%] [G loss: 2.290064]\n",
      "epoch:9 step:7719 [D loss: 0.360844, acc.: 89.84%] [G loss: 2.604247]\n",
      "epoch:9 step:7720 [D loss: 0.230809, acc.: 91.41%] [G loss: 3.103164]\n",
      "epoch:9 step:7721 [D loss: 0.399640, acc.: 77.34%] [G loss: 4.208698]\n",
      "epoch:9 step:7722 [D loss: 0.450203, acc.: 81.25%] [G loss: 3.212279]\n",
      "epoch:9 step:7723 [D loss: 0.323529, acc.: 85.94%] [G loss: 2.951056]\n",
      "epoch:9 step:7724 [D loss: 0.374237, acc.: 82.03%] [G loss: 2.311732]\n",
      "epoch:9 step:7725 [D loss: 0.454765, acc.: 81.25%] [G loss: 2.602015]\n",
      "epoch:9 step:7726 [D loss: 0.435234, acc.: 80.47%] [G loss: 2.085276]\n",
      "epoch:9 step:7727 [D loss: 0.328667, acc.: 86.72%] [G loss: 3.258153]\n",
      "epoch:9 step:7728 [D loss: 0.389676, acc.: 85.16%] [G loss: 4.480410]\n",
      "epoch:9 step:7729 [D loss: 0.304440, acc.: 91.41%] [G loss: 3.002330]\n",
      "epoch:9 step:7730 [D loss: 0.374974, acc.: 85.16%] [G loss: 2.805757]\n",
      "epoch:9 step:7731 [D loss: 0.443157, acc.: 81.25%] [G loss: 2.361859]\n",
      "epoch:9 step:7732 [D loss: 0.375642, acc.: 82.81%] [G loss: 2.967868]\n",
      "epoch:9 step:7733 [D loss: 0.374572, acc.: 83.59%] [G loss: 2.697156]\n",
      "epoch:9 step:7734 [D loss: 0.296172, acc.: 87.50%] [G loss: 2.299838]\n",
      "epoch:9 step:7735 [D loss: 0.333590, acc.: 86.72%] [G loss: 1.793829]\n",
      "epoch:9 step:7736 [D loss: 0.376884, acc.: 87.50%] [G loss: 2.055314]\n",
      "epoch:9 step:7737 [D loss: 0.324255, acc.: 85.94%] [G loss: 2.716667]\n",
      "epoch:9 step:7738 [D loss: 0.431775, acc.: 76.56%] [G loss: 2.198399]\n",
      "epoch:9 step:7739 [D loss: 0.333135, acc.: 86.72%] [G loss: 2.353604]\n",
      "epoch:9 step:7740 [D loss: 0.361106, acc.: 84.38%] [G loss: 2.391572]\n",
      "epoch:9 step:7741 [D loss: 0.328335, acc.: 88.28%] [G loss: 2.746082]\n",
      "epoch:9 step:7742 [D loss: 0.369399, acc.: 84.38%] [G loss: 2.461895]\n",
      "epoch:9 step:7743 [D loss: 0.312871, acc.: 87.50%] [G loss: 2.991921]\n",
      "epoch:9 step:7744 [D loss: 0.363944, acc.: 82.03%] [G loss: 4.052613]\n",
      "epoch:9 step:7745 [D loss: 0.226543, acc.: 92.97%] [G loss: 5.487929]\n",
      "epoch:9 step:7746 [D loss: 0.343821, acc.: 84.38%] [G loss: 2.412330]\n",
      "epoch:9 step:7747 [D loss: 0.385635, acc.: 82.81%] [G loss: 2.718157]\n",
      "epoch:9 step:7748 [D loss: 0.284489, acc.: 87.50%] [G loss: 2.590653]\n",
      "epoch:9 step:7749 [D loss: 0.307320, acc.: 91.41%] [G loss: 2.491588]\n",
      "epoch:9 step:7750 [D loss: 0.347499, acc.: 85.94%] [G loss: 2.437375]\n",
      "epoch:9 step:7751 [D loss: 0.288661, acc.: 90.62%] [G loss: 2.067620]\n",
      "epoch:9 step:7752 [D loss: 0.343366, acc.: 89.06%] [G loss: 2.178464]\n",
      "epoch:9 step:7753 [D loss: 0.412951, acc.: 83.59%] [G loss: 2.073020]\n",
      "epoch:9 step:7754 [D loss: 0.275392, acc.: 89.06%] [G loss: 2.742295]\n",
      "epoch:9 step:7755 [D loss: 0.280161, acc.: 89.06%] [G loss: 3.461299]\n",
      "epoch:9 step:7756 [D loss: 0.323725, acc.: 84.38%] [G loss: 3.403606]\n",
      "epoch:9 step:7757 [D loss: 0.362875, acc.: 85.16%] [G loss: 3.056648]\n",
      "epoch:9 step:7758 [D loss: 0.254285, acc.: 89.84%] [G loss: 2.397774]\n",
      "epoch:9 step:7759 [D loss: 0.375943, acc.: 82.81%] [G loss: 2.502445]\n",
      "epoch:9 step:7760 [D loss: 0.387177, acc.: 83.59%] [G loss: 1.988406]\n",
      "epoch:9 step:7761 [D loss: 0.406933, acc.: 84.38%] [G loss: 2.903543]\n",
      "epoch:9 step:7762 [D loss: 0.373298, acc.: 84.38%] [G loss: 3.852646]\n",
      "epoch:9 step:7763 [D loss: 0.226196, acc.: 90.62%] [G loss: 4.754184]\n",
      "epoch:9 step:7764 [D loss: 0.348849, acc.: 85.16%] [G loss: 2.247322]\n",
      "epoch:9 step:7765 [D loss: 0.364271, acc.: 84.38%] [G loss: 2.126732]\n",
      "epoch:9 step:7766 [D loss: 0.286114, acc.: 86.72%] [G loss: 2.767275]\n",
      "epoch:9 step:7767 [D loss: 0.291468, acc.: 85.16%] [G loss: 3.936490]\n",
      "epoch:9 step:7768 [D loss: 0.287575, acc.: 90.62%] [G loss: 1.983303]\n",
      "epoch:9 step:7769 [D loss: 0.315861, acc.: 93.75%] [G loss: 2.314209]\n",
      "epoch:9 step:7770 [D loss: 0.304341, acc.: 90.62%] [G loss: 2.744774]\n",
      "epoch:9 step:7771 [D loss: 0.325464, acc.: 85.16%] [G loss: 3.489787]\n",
      "epoch:9 step:7772 [D loss: 0.382582, acc.: 85.16%] [G loss: 2.648769]\n",
      "epoch:9 step:7773 [D loss: 0.319371, acc.: 89.84%] [G loss: 3.224874]\n",
      "epoch:9 step:7774 [D loss: 0.234879, acc.: 87.50%] [G loss: 4.170485]\n",
      "epoch:9 step:7775 [D loss: 0.273917, acc.: 88.28%] [G loss: 2.555138]\n",
      "epoch:9 step:7776 [D loss: 0.201613, acc.: 92.97%] [G loss: 2.984952]\n",
      "epoch:9 step:7777 [D loss: 0.427008, acc.: 82.03%] [G loss: 2.573145]\n",
      "epoch:9 step:7778 [D loss: 0.257897, acc.: 91.41%] [G loss: 3.087312]\n",
      "epoch:9 step:7779 [D loss: 0.282243, acc.: 91.41%] [G loss: 3.700851]\n",
      "epoch:9 step:7780 [D loss: 0.328448, acc.: 89.84%] [G loss: 3.528052]\n",
      "epoch:9 step:7781 [D loss: 0.254337, acc.: 92.19%] [G loss: 2.939439]\n",
      "epoch:9 step:7782 [D loss: 0.415468, acc.: 84.38%] [G loss: 2.151959]\n",
      "epoch:9 step:7783 [D loss: 0.416697, acc.: 82.03%] [G loss: 2.320312]\n",
      "epoch:9 step:7784 [D loss: 0.320517, acc.: 87.50%] [G loss: 2.595038]\n",
      "epoch:9 step:7785 [D loss: 0.314360, acc.: 83.59%] [G loss: 3.491086]\n",
      "epoch:9 step:7786 [D loss: 0.249139, acc.: 91.41%] [G loss: 2.350341]\n",
      "epoch:9 step:7787 [D loss: 0.288021, acc.: 89.84%] [G loss: 2.862416]\n",
      "epoch:9 step:7788 [D loss: 0.344315, acc.: 86.72%] [G loss: 1.685987]\n",
      "epoch:9 step:7789 [D loss: 0.270262, acc.: 88.28%] [G loss: 3.150453]\n",
      "epoch:9 step:7790 [D loss: 0.322241, acc.: 88.28%] [G loss: 2.176638]\n",
      "epoch:9 step:7791 [D loss: 0.330144, acc.: 86.72%] [G loss: 2.195249]\n",
      "epoch:9 step:7792 [D loss: 0.255703, acc.: 91.41%] [G loss: 2.644253]\n",
      "epoch:9 step:7793 [D loss: 0.308588, acc.: 86.72%] [G loss: 3.071534]\n",
      "epoch:9 step:7794 [D loss: 0.347425, acc.: 85.94%] [G loss: 2.835752]\n",
      "epoch:9 step:7795 [D loss: 0.409272, acc.: 78.91%] [G loss: 2.503347]\n",
      "epoch:9 step:7796 [D loss: 0.211899, acc.: 89.06%] [G loss: 3.527142]\n",
      "epoch:9 step:7797 [D loss: 0.319154, acc.: 84.38%] [G loss: 2.975901]\n",
      "epoch:9 step:7798 [D loss: 0.299437, acc.: 89.06%] [G loss: 2.360086]\n",
      "epoch:9 step:7799 [D loss: 0.286410, acc.: 88.28%] [G loss: 2.612852]\n",
      "epoch:9 step:7800 [D loss: 0.262151, acc.: 88.28%] [G loss: 3.284171]\n",
      "epoch:9 step:7801 [D loss: 0.233791, acc.: 92.19%] [G loss: 3.986837]\n",
      "epoch:9 step:7802 [D loss: 0.296313, acc.: 88.28%] [G loss: 2.945251]\n",
      "epoch:9 step:7803 [D loss: 0.334165, acc.: 86.72%] [G loss: 2.337202]\n",
      "epoch:9 step:7804 [D loss: 0.304006, acc.: 88.28%] [G loss: 2.446862]\n",
      "epoch:9 step:7805 [D loss: 0.213505, acc.: 91.41%] [G loss: 2.313500]\n",
      "epoch:9 step:7806 [D loss: 0.447284, acc.: 77.34%] [G loss: 3.506936]\n",
      "epoch:9 step:7807 [D loss: 0.405631, acc.: 81.25%] [G loss: 1.933928]\n",
      "epoch:9 step:7808 [D loss: 0.335369, acc.: 85.16%] [G loss: 2.576284]\n",
      "epoch:9 step:7809 [D loss: 0.289791, acc.: 84.38%] [G loss: 3.432109]\n",
      "epoch:9 step:7810 [D loss: 0.240846, acc.: 89.06%] [G loss: 5.764796]\n",
      "epoch:10 step:7811 [D loss: 0.315480, acc.: 84.38%] [G loss: 3.892871]\n",
      "epoch:10 step:7812 [D loss: 0.305750, acc.: 90.62%] [G loss: 3.596623]\n",
      "epoch:10 step:7813 [D loss: 0.298584, acc.: 87.50%] [G loss: 4.238093]\n",
      "epoch:10 step:7814 [D loss: 0.435164, acc.: 78.12%] [G loss: 2.347572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:7815 [D loss: 0.318868, acc.: 89.06%] [G loss: 2.541816]\n",
      "epoch:10 step:7816 [D loss: 0.323277, acc.: 87.50%] [G loss: 3.079750]\n",
      "epoch:10 step:7817 [D loss: 0.300030, acc.: 86.72%] [G loss: 2.529959]\n",
      "epoch:10 step:7818 [D loss: 0.334028, acc.: 87.50%] [G loss: 2.292264]\n",
      "epoch:10 step:7819 [D loss: 0.439011, acc.: 82.81%] [G loss: 2.670104]\n",
      "epoch:10 step:7820 [D loss: 0.504312, acc.: 78.12%] [G loss: 3.549580]\n",
      "epoch:10 step:7821 [D loss: 0.404668, acc.: 82.81%] [G loss: 4.075961]\n",
      "epoch:10 step:7822 [D loss: 0.404722, acc.: 81.25%] [G loss: 3.089806]\n",
      "epoch:10 step:7823 [D loss: 0.369533, acc.: 83.59%] [G loss: 4.567667]\n",
      "epoch:10 step:7824 [D loss: 0.528811, acc.: 73.44%] [G loss: 2.279217]\n",
      "epoch:10 step:7825 [D loss: 0.348549, acc.: 82.81%] [G loss: 3.007139]\n",
      "epoch:10 step:7826 [D loss: 0.401471, acc.: 85.16%] [G loss: 2.254533]\n",
      "epoch:10 step:7827 [D loss: 0.352046, acc.: 85.94%] [G loss: 4.148798]\n",
      "epoch:10 step:7828 [D loss: 0.275341, acc.: 86.72%] [G loss: 4.367153]\n",
      "epoch:10 step:7829 [D loss: 0.268255, acc.: 89.84%] [G loss: 3.958524]\n",
      "epoch:10 step:7830 [D loss: 0.344766, acc.: 85.16%] [G loss: 3.643741]\n",
      "epoch:10 step:7831 [D loss: 0.298766, acc.: 89.06%] [G loss: 2.866023]\n",
      "epoch:10 step:7832 [D loss: 0.302530, acc.: 87.50%] [G loss: 3.369022]\n",
      "epoch:10 step:7833 [D loss: 0.343042, acc.: 82.81%] [G loss: 2.166919]\n",
      "epoch:10 step:7834 [D loss: 0.432051, acc.: 78.91%] [G loss: 2.295391]\n",
      "epoch:10 step:7835 [D loss: 0.281370, acc.: 88.28%] [G loss: 3.906788]\n",
      "epoch:10 step:7836 [D loss: 0.419509, acc.: 82.81%] [G loss: 2.724596]\n",
      "epoch:10 step:7837 [D loss: 0.316773, acc.: 89.06%] [G loss: 3.155453]\n",
      "epoch:10 step:7838 [D loss: 0.317936, acc.: 85.94%] [G loss: 2.949898]\n",
      "epoch:10 step:7839 [D loss: 0.323862, acc.: 85.16%] [G loss: 2.379742]\n",
      "epoch:10 step:7840 [D loss: 0.362965, acc.: 84.38%] [G loss: 2.648120]\n",
      "epoch:10 step:7841 [D loss: 0.342203, acc.: 85.16%] [G loss: 2.501309]\n",
      "epoch:10 step:7842 [D loss: 0.434106, acc.: 83.59%] [G loss: 2.341822]\n",
      "epoch:10 step:7843 [D loss: 0.391466, acc.: 80.47%] [G loss: 2.649380]\n",
      "epoch:10 step:7844 [D loss: 0.315602, acc.: 86.72%] [G loss: 3.476879]\n",
      "epoch:10 step:7845 [D loss: 0.281570, acc.: 85.94%] [G loss: 4.779676]\n",
      "epoch:10 step:7846 [D loss: 0.456869, acc.: 78.12%] [G loss: 1.989800]\n",
      "epoch:10 step:7847 [D loss: 0.345351, acc.: 85.94%] [G loss: 2.530926]\n",
      "epoch:10 step:7848 [D loss: 0.279472, acc.: 90.62%] [G loss: 3.780243]\n",
      "epoch:10 step:7849 [D loss: 0.387251, acc.: 82.81%] [G loss: 2.118069]\n",
      "epoch:10 step:7850 [D loss: 0.412521, acc.: 82.03%] [G loss: 2.740040]\n",
      "epoch:10 step:7851 [D loss: 0.448249, acc.: 77.34%] [G loss: 2.714373]\n",
      "epoch:10 step:7852 [D loss: 0.260871, acc.: 89.06%] [G loss: 4.841094]\n",
      "epoch:10 step:7853 [D loss: 0.282723, acc.: 85.94%] [G loss: 3.219188]\n",
      "epoch:10 step:7854 [D loss: 0.277464, acc.: 87.50%] [G loss: 2.206289]\n",
      "epoch:10 step:7855 [D loss: 0.256734, acc.: 90.62%] [G loss: 3.646947]\n",
      "epoch:10 step:7856 [D loss: 0.274155, acc.: 85.16%] [G loss: 4.378808]\n",
      "epoch:10 step:7857 [D loss: 0.269277, acc.: 89.06%] [G loss: 3.350212]\n",
      "epoch:10 step:7858 [D loss: 0.363124, acc.: 83.59%] [G loss: 1.878835]\n",
      "epoch:10 step:7859 [D loss: 0.231408, acc.: 94.53%] [G loss: 3.131758]\n",
      "epoch:10 step:7860 [D loss: 0.359920, acc.: 83.59%] [G loss: 3.789423]\n",
      "epoch:10 step:7861 [D loss: 0.239446, acc.: 92.19%] [G loss: 2.611556]\n",
      "epoch:10 step:7862 [D loss: 0.308504, acc.: 85.94%] [G loss: 2.837987]\n",
      "epoch:10 step:7863 [D loss: 0.356828, acc.: 85.16%] [G loss: 4.055761]\n",
      "epoch:10 step:7864 [D loss: 0.303364, acc.: 87.50%] [G loss: 3.682380]\n",
      "epoch:10 step:7865 [D loss: 0.269295, acc.: 87.50%] [G loss: 3.960455]\n",
      "epoch:10 step:7866 [D loss: 0.358254, acc.: 81.25%] [G loss: 3.843857]\n",
      "epoch:10 step:7867 [D loss: 0.344074, acc.: 82.03%] [G loss: 3.092162]\n",
      "epoch:10 step:7868 [D loss: 0.309802, acc.: 87.50%] [G loss: 3.275226]\n",
      "epoch:10 step:7869 [D loss: 0.278324, acc.: 88.28%] [G loss: 3.165728]\n",
      "epoch:10 step:7870 [D loss: 0.379309, acc.: 88.28%] [G loss: 2.821674]\n",
      "epoch:10 step:7871 [D loss: 0.331118, acc.: 85.16%] [G loss: 2.798091]\n",
      "epoch:10 step:7872 [D loss: 0.276621, acc.: 87.50%] [G loss: 3.071484]\n",
      "epoch:10 step:7873 [D loss: 0.286674, acc.: 88.28%] [G loss: 3.242066]\n",
      "epoch:10 step:7874 [D loss: 0.283574, acc.: 89.84%] [G loss: 3.234757]\n",
      "epoch:10 step:7875 [D loss: 0.392119, acc.: 82.81%] [G loss: 3.913276]\n",
      "epoch:10 step:7876 [D loss: 0.289549, acc.: 89.84%] [G loss: 3.404399]\n",
      "epoch:10 step:7877 [D loss: 0.352364, acc.: 86.72%] [G loss: 3.185923]\n",
      "epoch:10 step:7878 [D loss: 0.277156, acc.: 91.41%] [G loss: 2.465608]\n",
      "epoch:10 step:7879 [D loss: 0.258300, acc.: 90.62%] [G loss: 3.294556]\n",
      "epoch:10 step:7880 [D loss: 0.230912, acc.: 93.75%] [G loss: 3.289306]\n",
      "epoch:10 step:7881 [D loss: 0.318903, acc.: 83.59%] [G loss: 3.311954]\n",
      "epoch:10 step:7882 [D loss: 0.463810, acc.: 80.47%] [G loss: 3.734009]\n",
      "epoch:10 step:7883 [D loss: 0.595321, acc.: 78.12%] [G loss: 6.832449]\n",
      "epoch:10 step:7884 [D loss: 1.134656, acc.: 57.03%] [G loss: 4.234607]\n",
      "epoch:10 step:7885 [D loss: 1.338409, acc.: 64.06%] [G loss: 6.175455]\n",
      "epoch:10 step:7886 [D loss: 0.291197, acc.: 89.84%] [G loss: 6.919746]\n",
      "epoch:10 step:7887 [D loss: 1.093199, acc.: 63.28%] [G loss: 4.520569]\n",
      "epoch:10 step:7888 [D loss: 0.263061, acc.: 92.19%] [G loss: 4.910076]\n",
      "epoch:10 step:7889 [D loss: 0.695128, acc.: 75.00%] [G loss: 3.701487]\n",
      "epoch:10 step:7890 [D loss: 0.370917, acc.: 85.94%] [G loss: 3.472369]\n",
      "epoch:10 step:7891 [D loss: 0.352318, acc.: 82.81%] [G loss: 2.103930]\n",
      "epoch:10 step:7892 [D loss: 0.335212, acc.: 87.50%] [G loss: 2.758328]\n",
      "epoch:10 step:7893 [D loss: 0.321568, acc.: 89.84%] [G loss: 2.810349]\n",
      "epoch:10 step:7894 [D loss: 0.313976, acc.: 88.28%] [G loss: 3.564339]\n",
      "epoch:10 step:7895 [D loss: 0.310293, acc.: 85.94%] [G loss: 3.378193]\n",
      "epoch:10 step:7896 [D loss: 0.338479, acc.: 86.72%] [G loss: 2.466414]\n",
      "epoch:10 step:7897 [D loss: 0.276965, acc.: 89.84%] [G loss: 2.206019]\n",
      "epoch:10 step:7898 [D loss: 0.311719, acc.: 85.16%] [G loss: 3.113497]\n",
      "epoch:10 step:7899 [D loss: 0.330834, acc.: 84.38%] [G loss: 2.591228]\n",
      "epoch:10 step:7900 [D loss: 0.387249, acc.: 82.81%] [G loss: 3.228835]\n",
      "epoch:10 step:7901 [D loss: 0.273362, acc.: 88.28%] [G loss: 2.874102]\n",
      "epoch:10 step:7902 [D loss: 0.305122, acc.: 88.28%] [G loss: 2.375319]\n",
      "epoch:10 step:7903 [D loss: 0.335128, acc.: 88.28%] [G loss: 2.483642]\n",
      "epoch:10 step:7904 [D loss: 0.416989, acc.: 84.38%] [G loss: 2.671998]\n",
      "epoch:10 step:7905 [D loss: 0.380488, acc.: 82.03%] [G loss: 2.845329]\n",
      "epoch:10 step:7906 [D loss: 0.281199, acc.: 87.50%] [G loss: 2.231194]\n",
      "epoch:10 step:7907 [D loss: 0.326293, acc.: 90.62%] [G loss: 2.483698]\n",
      "epoch:10 step:7908 [D loss: 0.314358, acc.: 90.62%] [G loss: 2.412319]\n",
      "epoch:10 step:7909 [D loss: 0.288152, acc.: 90.62%] [G loss: 2.703248]\n",
      "epoch:10 step:7910 [D loss: 0.389915, acc.: 79.69%] [G loss: 2.685206]\n",
      "epoch:10 step:7911 [D loss: 0.326861, acc.: 86.72%] [G loss: 2.382737]\n",
      "epoch:10 step:7912 [D loss: 0.433556, acc.: 80.47%] [G loss: 2.896071]\n",
      "epoch:10 step:7913 [D loss: 0.384214, acc.: 81.25%] [G loss: 3.613797]\n",
      "epoch:10 step:7914 [D loss: 0.379638, acc.: 80.47%] [G loss: 2.970356]\n",
      "epoch:10 step:7915 [D loss: 0.442073, acc.: 81.25%] [G loss: 2.228558]\n",
      "epoch:10 step:7916 [D loss: 0.354104, acc.: 86.72%] [G loss: 2.736756]\n",
      "epoch:10 step:7917 [D loss: 0.380679, acc.: 82.03%] [G loss: 2.401076]\n",
      "epoch:10 step:7918 [D loss: 0.255073, acc.: 88.28%] [G loss: 4.344489]\n",
      "epoch:10 step:7919 [D loss: 0.303318, acc.: 86.72%] [G loss: 4.094562]\n",
      "epoch:10 step:7920 [D loss: 0.295025, acc.: 90.62%] [G loss: 2.755626]\n",
      "epoch:10 step:7921 [D loss: 0.459912, acc.: 79.69%] [G loss: 2.640092]\n",
      "epoch:10 step:7922 [D loss: 0.278008, acc.: 86.72%] [G loss: 5.588661]\n",
      "epoch:10 step:7923 [D loss: 0.414718, acc.: 80.47%] [G loss: 2.709083]\n",
      "epoch:10 step:7924 [D loss: 0.547108, acc.: 78.12%] [G loss: 3.184943]\n",
      "epoch:10 step:7925 [D loss: 0.275031, acc.: 86.72%] [G loss: 2.999760]\n",
      "epoch:10 step:7926 [D loss: 0.379795, acc.: 84.38%] [G loss: 3.328169]\n",
      "epoch:10 step:7927 [D loss: 0.486505, acc.: 74.22%] [G loss: 2.879926]\n",
      "epoch:10 step:7928 [D loss: 0.388852, acc.: 82.81%] [G loss: 3.057321]\n",
      "epoch:10 step:7929 [D loss: 0.390934, acc.: 84.38%] [G loss: 1.902324]\n",
      "epoch:10 step:7930 [D loss: 0.400902, acc.: 85.16%] [G loss: 2.912195]\n",
      "epoch:10 step:7931 [D loss: 0.226473, acc.: 89.84%] [G loss: 2.706969]\n",
      "epoch:10 step:7932 [D loss: 0.211846, acc.: 93.75%] [G loss: 4.453854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:7933 [D loss: 0.281722, acc.: 91.41%] [G loss: 2.924659]\n",
      "epoch:10 step:7934 [D loss: 0.349239, acc.: 85.94%] [G loss: 2.532184]\n",
      "epoch:10 step:7935 [D loss: 0.349702, acc.: 84.38%] [G loss: 4.166329]\n",
      "epoch:10 step:7936 [D loss: 0.330194, acc.: 89.06%] [G loss: 4.436853]\n",
      "epoch:10 step:7937 [D loss: 0.401156, acc.: 82.81%] [G loss: 2.916572]\n",
      "epoch:10 step:7938 [D loss: 0.396321, acc.: 83.59%] [G loss: 3.283815]\n",
      "epoch:10 step:7939 [D loss: 0.338820, acc.: 84.38%] [G loss: 2.159548]\n",
      "epoch:10 step:7940 [D loss: 0.323416, acc.: 89.06%] [G loss: 2.288865]\n",
      "epoch:10 step:7941 [D loss: 0.394061, acc.: 83.59%] [G loss: 3.012288]\n",
      "epoch:10 step:7942 [D loss: 0.322813, acc.: 87.50%] [G loss: 2.484434]\n",
      "epoch:10 step:7943 [D loss: 0.384798, acc.: 81.25%] [G loss: 2.715406]\n",
      "epoch:10 step:7944 [D loss: 0.269547, acc.: 89.06%] [G loss: 2.759665]\n",
      "epoch:10 step:7945 [D loss: 0.331384, acc.: 85.16%] [G loss: 2.215145]\n",
      "epoch:10 step:7946 [D loss: 0.299914, acc.: 85.94%] [G loss: 2.566589]\n",
      "epoch:10 step:7947 [D loss: 0.400272, acc.: 86.72%] [G loss: 3.143175]\n",
      "epoch:10 step:7948 [D loss: 0.460597, acc.: 79.69%] [G loss: 3.244741]\n",
      "epoch:10 step:7949 [D loss: 0.273029, acc.: 89.84%] [G loss: 3.864497]\n",
      "epoch:10 step:7950 [D loss: 0.209083, acc.: 92.19%] [G loss: 3.484291]\n",
      "epoch:10 step:7951 [D loss: 0.304390, acc.: 87.50%] [G loss: 3.692332]\n",
      "epoch:10 step:7952 [D loss: 0.407164, acc.: 82.03%] [G loss: 2.413261]\n",
      "epoch:10 step:7953 [D loss: 0.356583, acc.: 85.16%] [G loss: 2.483502]\n",
      "epoch:10 step:7954 [D loss: 0.289452, acc.: 86.72%] [G loss: 2.662569]\n",
      "epoch:10 step:7955 [D loss: 0.334072, acc.: 85.16%] [G loss: 3.483006]\n",
      "epoch:10 step:7956 [D loss: 0.232360, acc.: 89.84%] [G loss: 3.865945]\n",
      "epoch:10 step:7957 [D loss: 0.292294, acc.: 86.72%] [G loss: 3.479933]\n",
      "epoch:10 step:7958 [D loss: 0.309323, acc.: 85.94%] [G loss: 3.553379]\n",
      "epoch:10 step:7959 [D loss: 0.224584, acc.: 92.19%] [G loss: 3.414648]\n",
      "epoch:10 step:7960 [D loss: 0.394283, acc.: 81.25%] [G loss: 2.104358]\n",
      "epoch:10 step:7961 [D loss: 0.271883, acc.: 90.62%] [G loss: 2.277431]\n",
      "epoch:10 step:7962 [D loss: 0.416585, acc.: 82.81%] [G loss: 2.272828]\n",
      "epoch:10 step:7963 [D loss: 0.312363, acc.: 89.84%] [G loss: 3.114562]\n",
      "epoch:10 step:7964 [D loss: 0.363232, acc.: 85.94%] [G loss: 2.554879]\n",
      "epoch:10 step:7965 [D loss: 0.383644, acc.: 85.16%] [G loss: 2.357149]\n",
      "epoch:10 step:7966 [D loss: 0.257519, acc.: 89.84%] [G loss: 4.318546]\n",
      "epoch:10 step:7967 [D loss: 0.232943, acc.: 92.19%] [G loss: 4.018870]\n",
      "epoch:10 step:7968 [D loss: 0.282034, acc.: 91.41%] [G loss: 2.107544]\n",
      "epoch:10 step:7969 [D loss: 0.319826, acc.: 88.28%] [G loss: 3.421602]\n",
      "epoch:10 step:7970 [D loss: 0.290967, acc.: 86.72%] [G loss: 5.085992]\n",
      "epoch:10 step:7971 [D loss: 0.370943, acc.: 79.69%] [G loss: 2.177276]\n",
      "epoch:10 step:7972 [D loss: 0.226210, acc.: 91.41%] [G loss: 4.374636]\n",
      "epoch:10 step:7973 [D loss: 0.337659, acc.: 84.38%] [G loss: 2.960223]\n",
      "epoch:10 step:7974 [D loss: 0.389998, acc.: 82.03%] [G loss: 2.732211]\n",
      "epoch:10 step:7975 [D loss: 0.248180, acc.: 89.06%] [G loss: 3.486667]\n",
      "epoch:10 step:7976 [D loss: 0.334349, acc.: 85.16%] [G loss: 3.015458]\n",
      "epoch:10 step:7977 [D loss: 0.299923, acc.: 88.28%] [G loss: 2.798811]\n",
      "epoch:10 step:7978 [D loss: 0.259255, acc.: 90.62%] [G loss: 3.537691]\n",
      "epoch:10 step:7979 [D loss: 0.258165, acc.: 90.62%] [G loss: 4.686010]\n",
      "epoch:10 step:7980 [D loss: 0.184000, acc.: 96.88%] [G loss: 3.111903]\n",
      "epoch:10 step:7981 [D loss: 0.386257, acc.: 82.81%] [G loss: 3.030363]\n",
      "epoch:10 step:7982 [D loss: 0.284804, acc.: 91.41%] [G loss: 3.360556]\n",
      "epoch:10 step:7983 [D loss: 0.323864, acc.: 87.50%] [G loss: 2.675340]\n",
      "epoch:10 step:7984 [D loss: 0.357001, acc.: 78.91%] [G loss: 6.933621]\n",
      "epoch:10 step:7985 [D loss: 0.390967, acc.: 79.69%] [G loss: 3.223499]\n",
      "epoch:10 step:7986 [D loss: 0.232297, acc.: 92.19%] [G loss: 2.582893]\n",
      "epoch:10 step:7987 [D loss: 0.223478, acc.: 89.84%] [G loss: 4.602510]\n",
      "epoch:10 step:7988 [D loss: 0.243069, acc.: 92.97%] [G loss: 2.796680]\n",
      "epoch:10 step:7989 [D loss: 0.255831, acc.: 90.62%] [G loss: 2.350125]\n",
      "epoch:10 step:7990 [D loss: 0.283153, acc.: 85.16%] [G loss: 2.368477]\n",
      "epoch:10 step:7991 [D loss: 0.270397, acc.: 85.16%] [G loss: 3.444234]\n",
      "epoch:10 step:7992 [D loss: 0.323539, acc.: 82.03%] [G loss: 3.909307]\n",
      "epoch:10 step:7993 [D loss: 0.328981, acc.: 88.28%] [G loss: 3.205222]\n",
      "epoch:10 step:7994 [D loss: 0.289560, acc.: 85.94%] [G loss: 2.803373]\n",
      "epoch:10 step:7995 [D loss: 0.338926, acc.: 85.16%] [G loss: 2.832033]\n",
      "epoch:10 step:7996 [D loss: 0.280856, acc.: 87.50%] [G loss: 3.139246]\n",
      "epoch:10 step:7997 [D loss: 0.267544, acc.: 89.06%] [G loss: 3.073878]\n",
      "epoch:10 step:7998 [D loss: 0.275228, acc.: 86.72%] [G loss: 2.879815]\n",
      "epoch:10 step:7999 [D loss: 0.324636, acc.: 85.94%] [G loss: 2.594353]\n",
      "epoch:10 step:8000 [D loss: 0.299298, acc.: 87.50%] [G loss: 2.761937]\n",
      "epoch:10 step:8001 [D loss: 0.408780, acc.: 81.25%] [G loss: 3.013095]\n",
      "epoch:10 step:8002 [D loss: 0.400681, acc.: 85.94%] [G loss: 5.196856]\n",
      "epoch:10 step:8003 [D loss: 0.520623, acc.: 78.91%] [G loss: 5.882515]\n",
      "epoch:10 step:8004 [D loss: 0.848004, acc.: 71.88%] [G loss: 7.021532]\n",
      "epoch:10 step:8005 [D loss: 1.017708, acc.: 67.19%] [G loss: 6.114974]\n",
      "epoch:10 step:8006 [D loss: 0.761630, acc.: 77.34%] [G loss: 4.054508]\n",
      "epoch:10 step:8007 [D loss: 0.602426, acc.: 73.44%] [G loss: 2.649053]\n",
      "epoch:10 step:8008 [D loss: 0.292230, acc.: 83.59%] [G loss: 5.307088]\n",
      "epoch:10 step:8009 [D loss: 0.329192, acc.: 82.03%] [G loss: 3.784312]\n",
      "epoch:10 step:8010 [D loss: 0.333817, acc.: 85.94%] [G loss: 4.545529]\n",
      "epoch:10 step:8011 [D loss: 0.282988, acc.: 89.84%] [G loss: 4.196133]\n",
      "epoch:10 step:8012 [D loss: 0.291326, acc.: 90.62%] [G loss: 2.942055]\n",
      "epoch:10 step:8013 [D loss: 0.223351, acc.: 93.75%] [G loss: 3.020960]\n",
      "epoch:10 step:8014 [D loss: 0.444268, acc.: 81.25%] [G loss: 1.962840]\n",
      "epoch:10 step:8015 [D loss: 0.321471, acc.: 86.72%] [G loss: 3.502242]\n",
      "epoch:10 step:8016 [D loss: 0.275502, acc.: 89.06%] [G loss: 3.937489]\n",
      "epoch:10 step:8017 [D loss: 0.333919, acc.: 82.81%] [G loss: 2.879056]\n",
      "epoch:10 step:8018 [D loss: 0.387430, acc.: 80.47%] [G loss: 3.931261]\n",
      "epoch:10 step:8019 [D loss: 0.228832, acc.: 89.06%] [G loss: 3.926476]\n",
      "epoch:10 step:8020 [D loss: 0.293025, acc.: 88.28%] [G loss: 3.217947]\n",
      "epoch:10 step:8021 [D loss: 0.410221, acc.: 79.69%] [G loss: 2.763263]\n",
      "epoch:10 step:8022 [D loss: 0.275186, acc.: 89.84%] [G loss: 2.297560]\n",
      "epoch:10 step:8023 [D loss: 0.320815, acc.: 87.50%] [G loss: 2.157142]\n",
      "epoch:10 step:8024 [D loss: 0.288719, acc.: 92.97%] [G loss: 2.415041]\n",
      "epoch:10 step:8025 [D loss: 0.280161, acc.: 88.28%] [G loss: 2.557885]\n",
      "epoch:10 step:8026 [D loss: 0.432150, acc.: 79.69%] [G loss: 2.386908]\n",
      "epoch:10 step:8027 [D loss: 0.350625, acc.: 82.03%] [G loss: 2.303214]\n",
      "epoch:10 step:8028 [D loss: 0.247623, acc.: 90.62%] [G loss: 2.913757]\n",
      "epoch:10 step:8029 [D loss: 0.264187, acc.: 91.41%] [G loss: 3.644252]\n",
      "epoch:10 step:8030 [D loss: 0.295896, acc.: 87.50%] [G loss: 2.230961]\n",
      "epoch:10 step:8031 [D loss: 0.306078, acc.: 87.50%] [G loss: 2.362314]\n",
      "epoch:10 step:8032 [D loss: 0.259178, acc.: 92.19%] [G loss: 2.358243]\n",
      "epoch:10 step:8033 [D loss: 0.325411, acc.: 86.72%] [G loss: 2.339493]\n",
      "epoch:10 step:8034 [D loss: 0.334615, acc.: 85.16%] [G loss: 2.357602]\n",
      "epoch:10 step:8035 [D loss: 0.336929, acc.: 85.94%] [G loss: 2.168996]\n",
      "epoch:10 step:8036 [D loss: 0.384813, acc.: 82.03%] [G loss: 3.437319]\n",
      "epoch:10 step:8037 [D loss: 0.195693, acc.: 91.41%] [G loss: 5.131904]\n",
      "epoch:10 step:8038 [D loss: 0.278105, acc.: 89.84%] [G loss: 2.995399]\n",
      "epoch:10 step:8039 [D loss: 0.419104, acc.: 83.59%] [G loss: 2.647435]\n",
      "epoch:10 step:8040 [D loss: 0.374567, acc.: 82.03%] [G loss: 2.674699]\n",
      "epoch:10 step:8041 [D loss: 0.348834, acc.: 87.50%] [G loss: 2.779044]\n",
      "epoch:10 step:8042 [D loss: 0.357149, acc.: 82.81%] [G loss: 2.758430]\n",
      "epoch:10 step:8043 [D loss: 0.367211, acc.: 83.59%] [G loss: 2.695432]\n",
      "epoch:10 step:8044 [D loss: 0.383306, acc.: 83.59%] [G loss: 2.567991]\n",
      "epoch:10 step:8045 [D loss: 0.363992, acc.: 83.59%] [G loss: 2.648477]\n",
      "epoch:10 step:8046 [D loss: 0.341415, acc.: 85.94%] [G loss: 2.920596]\n",
      "epoch:10 step:8047 [D loss: 0.299104, acc.: 85.16%] [G loss: 4.659182]\n",
      "epoch:10 step:8048 [D loss: 0.453545, acc.: 78.91%] [G loss: 2.990119]\n",
      "epoch:10 step:8049 [D loss: 0.369494, acc.: 84.38%] [G loss: 2.436654]\n",
      "epoch:10 step:8050 [D loss: 0.401909, acc.: 84.38%] [G loss: 2.876426]\n",
      "epoch:10 step:8051 [D loss: 0.560006, acc.: 75.00%] [G loss: 2.772261]\n",
      "epoch:10 step:8052 [D loss: 0.491700, acc.: 78.12%] [G loss: 2.461927]\n",
      "epoch:10 step:8053 [D loss: 0.568136, acc.: 69.53%] [G loss: 3.250837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8054 [D loss: 0.467413, acc.: 81.25%] [G loss: 2.701332]\n",
      "epoch:10 step:8055 [D loss: 0.259954, acc.: 89.84%] [G loss: 3.527966]\n",
      "epoch:10 step:8056 [D loss: 0.269973, acc.: 89.06%] [G loss: 3.568262]\n",
      "epoch:10 step:8057 [D loss: 0.367248, acc.: 81.25%] [G loss: 2.725433]\n",
      "epoch:10 step:8058 [D loss: 0.311928, acc.: 87.50%] [G loss: 2.283558]\n",
      "epoch:10 step:8059 [D loss: 0.423122, acc.: 78.91%] [G loss: 3.000716]\n",
      "epoch:10 step:8060 [D loss: 0.494457, acc.: 76.56%] [G loss: 2.610734]\n",
      "epoch:10 step:8061 [D loss: 0.477384, acc.: 81.25%] [G loss: 1.926855]\n",
      "epoch:10 step:8062 [D loss: 0.382617, acc.: 85.94%] [G loss: 3.384724]\n",
      "epoch:10 step:8063 [D loss: 0.324791, acc.: 86.72%] [G loss: 2.345426]\n",
      "epoch:10 step:8064 [D loss: 0.271282, acc.: 89.06%] [G loss: 2.570572]\n",
      "epoch:10 step:8065 [D loss: 0.431348, acc.: 83.59%] [G loss: 2.091865]\n",
      "epoch:10 step:8066 [D loss: 0.294981, acc.: 89.84%] [G loss: 2.448695]\n",
      "epoch:10 step:8067 [D loss: 0.346233, acc.: 87.50%] [G loss: 3.768674]\n",
      "epoch:10 step:8068 [D loss: 0.244192, acc.: 88.28%] [G loss: 4.410466]\n",
      "epoch:10 step:8069 [D loss: 0.282212, acc.: 89.06%] [G loss: 4.304076]\n",
      "epoch:10 step:8070 [D loss: 0.281101, acc.: 86.72%] [G loss: 2.699245]\n",
      "epoch:10 step:8071 [D loss: 0.369037, acc.: 87.50%] [G loss: 3.884991]\n",
      "epoch:10 step:8072 [D loss: 0.352263, acc.: 85.94%] [G loss: 5.770977]\n",
      "epoch:10 step:8073 [D loss: 0.306053, acc.: 87.50%] [G loss: 3.712643]\n",
      "epoch:10 step:8074 [D loss: 0.386093, acc.: 79.69%] [G loss: 2.855431]\n",
      "epoch:10 step:8075 [D loss: 0.418385, acc.: 81.25%] [G loss: 3.365211]\n",
      "epoch:10 step:8076 [D loss: 0.528892, acc.: 76.56%] [G loss: 3.694657]\n",
      "epoch:10 step:8077 [D loss: 0.636724, acc.: 67.97%] [G loss: 4.306683]\n",
      "epoch:10 step:8078 [D loss: 0.973738, acc.: 68.75%] [G loss: 3.291124]\n",
      "epoch:10 step:8079 [D loss: 0.575458, acc.: 70.31%] [G loss: 2.723481]\n",
      "epoch:10 step:8080 [D loss: 0.253586, acc.: 87.50%] [G loss: 3.372501]\n",
      "epoch:10 step:8081 [D loss: 0.284445, acc.: 90.62%] [G loss: 3.961733]\n",
      "epoch:10 step:8082 [D loss: 0.288322, acc.: 88.28%] [G loss: 4.015153]\n",
      "epoch:10 step:8083 [D loss: 0.291169, acc.: 89.84%] [G loss: 2.693839]\n",
      "epoch:10 step:8084 [D loss: 0.321319, acc.: 88.28%] [G loss: 2.392906]\n",
      "epoch:10 step:8085 [D loss: 0.454890, acc.: 82.81%] [G loss: 2.401091]\n",
      "epoch:10 step:8086 [D loss: 0.262386, acc.: 88.28%] [G loss: 3.476351]\n",
      "epoch:10 step:8087 [D loss: 0.292564, acc.: 87.50%] [G loss: 3.357641]\n",
      "epoch:10 step:8088 [D loss: 0.315149, acc.: 85.94%] [G loss: 2.486024]\n",
      "epoch:10 step:8089 [D loss: 0.370610, acc.: 81.25%] [G loss: 2.492272]\n",
      "epoch:10 step:8090 [D loss: 0.231609, acc.: 90.62%] [G loss: 3.872060]\n",
      "epoch:10 step:8091 [D loss: 0.287326, acc.: 88.28%] [G loss: 5.438372]\n",
      "epoch:10 step:8092 [D loss: 0.278571, acc.: 87.50%] [G loss: 2.603888]\n",
      "epoch:10 step:8093 [D loss: 0.268678, acc.: 88.28%] [G loss: 2.522214]\n",
      "epoch:10 step:8094 [D loss: 0.299088, acc.: 85.94%] [G loss: 4.017797]\n",
      "epoch:10 step:8095 [D loss: 0.192710, acc.: 91.41%] [G loss: 4.201116]\n",
      "epoch:10 step:8096 [D loss: 0.337321, acc.: 81.25%] [G loss: 4.653411]\n",
      "epoch:10 step:8097 [D loss: 0.183066, acc.: 93.75%] [G loss: 3.380122]\n",
      "epoch:10 step:8098 [D loss: 0.247705, acc.: 92.19%] [G loss: 3.411659]\n",
      "epoch:10 step:8099 [D loss: 0.404605, acc.: 83.59%] [G loss: 2.054790]\n",
      "epoch:10 step:8100 [D loss: 0.334520, acc.: 87.50%] [G loss: 2.288986]\n",
      "epoch:10 step:8101 [D loss: 0.261438, acc.: 90.62%] [G loss: 4.113118]\n",
      "epoch:10 step:8102 [D loss: 0.380915, acc.: 82.81%] [G loss: 3.960870]\n",
      "epoch:10 step:8103 [D loss: 0.410732, acc.: 85.16%] [G loss: 2.701001]\n",
      "epoch:10 step:8104 [D loss: 0.377481, acc.: 82.03%] [G loss: 2.367746]\n",
      "epoch:10 step:8105 [D loss: 0.319362, acc.: 86.72%] [G loss: 2.863507]\n",
      "epoch:10 step:8106 [D loss: 0.321992, acc.: 88.28%] [G loss: 2.507495]\n",
      "epoch:10 step:8107 [D loss: 0.272261, acc.: 85.16%] [G loss: 3.161488]\n",
      "epoch:10 step:8108 [D loss: 0.370962, acc.: 82.03%] [G loss: 2.940234]\n",
      "epoch:10 step:8109 [D loss: 0.273228, acc.: 88.28%] [G loss: 2.874248]\n",
      "epoch:10 step:8110 [D loss: 0.392925, acc.: 81.25%] [G loss: 2.740022]\n",
      "epoch:10 step:8111 [D loss: 0.292984, acc.: 89.06%] [G loss: 2.621686]\n",
      "epoch:10 step:8112 [D loss: 0.286622, acc.: 89.06%] [G loss: 2.782419]\n",
      "epoch:10 step:8113 [D loss: 0.338414, acc.: 90.62%] [G loss: 3.057276]\n",
      "epoch:10 step:8114 [D loss: 0.266762, acc.: 87.50%] [G loss: 2.796813]\n",
      "epoch:10 step:8115 [D loss: 0.332606, acc.: 84.38%] [G loss: 3.009247]\n",
      "epoch:10 step:8116 [D loss: 0.257670, acc.: 88.28%] [G loss: 3.321824]\n",
      "epoch:10 step:8117 [D loss: 0.255430, acc.: 85.94%] [G loss: 3.050570]\n",
      "epoch:10 step:8118 [D loss: 0.273649, acc.: 87.50%] [G loss: 4.068219]\n",
      "epoch:10 step:8119 [D loss: 0.229634, acc.: 89.84%] [G loss: 5.148822]\n",
      "epoch:10 step:8120 [D loss: 0.138779, acc.: 96.88%] [G loss: 3.163546]\n",
      "epoch:10 step:8121 [D loss: 0.387505, acc.: 82.03%] [G loss: 3.060022]\n",
      "epoch:10 step:8122 [D loss: 0.242994, acc.: 89.06%] [G loss: 6.928161]\n",
      "epoch:10 step:8123 [D loss: 0.357729, acc.: 82.81%] [G loss: 3.434956]\n",
      "epoch:10 step:8124 [D loss: 0.276718, acc.: 91.41%] [G loss: 3.575744]\n",
      "epoch:10 step:8125 [D loss: 0.405216, acc.: 78.91%] [G loss: 3.006106]\n",
      "epoch:10 step:8126 [D loss: 0.308241, acc.: 85.94%] [G loss: 4.580148]\n",
      "epoch:10 step:8127 [D loss: 0.612170, acc.: 75.78%] [G loss: 8.107739]\n",
      "epoch:10 step:8128 [D loss: 2.208278, acc.: 42.97%] [G loss: 1.972657]\n",
      "epoch:10 step:8129 [D loss: 0.629698, acc.: 84.38%] [G loss: 4.445143]\n",
      "epoch:10 step:8130 [D loss: 0.851553, acc.: 59.38%] [G loss: 4.482853]\n",
      "epoch:10 step:8131 [D loss: 0.521259, acc.: 82.03%] [G loss: 3.902468]\n",
      "epoch:10 step:8132 [D loss: 0.448293, acc.: 78.12%] [G loss: 3.514180]\n",
      "epoch:10 step:8133 [D loss: 0.273945, acc.: 88.28%] [G loss: 6.131387]\n",
      "epoch:10 step:8134 [D loss: 0.486771, acc.: 77.34%] [G loss: 3.168494]\n",
      "epoch:10 step:8135 [D loss: 0.414857, acc.: 79.69%] [G loss: 2.738883]\n",
      "epoch:10 step:8136 [D loss: 0.294860, acc.: 86.72%] [G loss: 4.037205]\n",
      "epoch:10 step:8137 [D loss: 0.251078, acc.: 91.41%] [G loss: 3.240925]\n",
      "epoch:10 step:8138 [D loss: 0.319889, acc.: 88.28%] [G loss: 2.716135]\n",
      "epoch:10 step:8139 [D loss: 0.170426, acc.: 92.97%] [G loss: 5.669879]\n",
      "epoch:10 step:8140 [D loss: 0.396042, acc.: 80.47%] [G loss: 2.498544]\n",
      "epoch:10 step:8141 [D loss: 0.284388, acc.: 85.16%] [G loss: 2.937770]\n",
      "epoch:10 step:8142 [D loss: 0.286049, acc.: 90.62%] [G loss: 2.649315]\n",
      "epoch:10 step:8143 [D loss: 0.276364, acc.: 89.84%] [G loss: 2.701854]\n",
      "epoch:10 step:8144 [D loss: 0.388531, acc.: 83.59%] [G loss: 2.156939]\n",
      "epoch:10 step:8145 [D loss: 0.280561, acc.: 89.06%] [G loss: 2.326935]\n",
      "epoch:10 step:8146 [D loss: 0.298053, acc.: 89.84%] [G loss: 2.424717]\n",
      "epoch:10 step:8147 [D loss: 0.314985, acc.: 86.72%] [G loss: 2.590170]\n",
      "epoch:10 step:8148 [D loss: 0.287012, acc.: 88.28%] [G loss: 2.506052]\n",
      "epoch:10 step:8149 [D loss: 0.293035, acc.: 90.62%] [G loss: 3.095586]\n",
      "epoch:10 step:8150 [D loss: 0.307283, acc.: 86.72%] [G loss: 2.943722]\n",
      "epoch:10 step:8151 [D loss: 0.353091, acc.: 86.72%] [G loss: 2.423429]\n",
      "epoch:10 step:8152 [D loss: 0.320482, acc.: 86.72%] [G loss: 2.411065]\n",
      "epoch:10 step:8153 [D loss: 0.412969, acc.: 80.47%] [G loss: 1.876096]\n",
      "epoch:10 step:8154 [D loss: 0.371970, acc.: 84.38%] [G loss: 2.834153]\n",
      "epoch:10 step:8155 [D loss: 0.321872, acc.: 85.94%] [G loss: 2.863731]\n",
      "epoch:10 step:8156 [D loss: 0.373366, acc.: 80.47%] [G loss: 3.682718]\n",
      "epoch:10 step:8157 [D loss: 0.195874, acc.: 94.53%] [G loss: 3.838929]\n",
      "epoch:10 step:8158 [D loss: 0.234772, acc.: 94.53%] [G loss: 2.558051]\n",
      "epoch:10 step:8159 [D loss: 0.337497, acc.: 83.59%] [G loss: 2.338476]\n",
      "epoch:10 step:8160 [D loss: 0.339366, acc.: 85.94%] [G loss: 4.943624]\n",
      "epoch:10 step:8161 [D loss: 0.319312, acc.: 86.72%] [G loss: 5.037351]\n",
      "epoch:10 step:8162 [D loss: 0.214251, acc.: 92.19%] [G loss: 4.369604]\n",
      "epoch:10 step:8163 [D loss: 0.448916, acc.: 81.25%] [G loss: 2.716254]\n",
      "epoch:10 step:8164 [D loss: 0.354216, acc.: 85.16%] [G loss: 3.135355]\n",
      "epoch:10 step:8165 [D loss: 0.371083, acc.: 83.59%] [G loss: 2.680328]\n",
      "epoch:10 step:8166 [D loss: 0.504038, acc.: 78.91%] [G loss: 2.114086]\n",
      "epoch:10 step:8167 [D loss: 0.360095, acc.: 86.72%] [G loss: 3.380620]\n",
      "epoch:10 step:8168 [D loss: 0.367032, acc.: 82.81%] [G loss: 2.494951]\n",
      "epoch:10 step:8169 [D loss: 0.400977, acc.: 84.38%] [G loss: 2.364213]\n",
      "epoch:10 step:8170 [D loss: 0.381290, acc.: 85.16%] [G loss: 2.883235]\n",
      "epoch:10 step:8171 [D loss: 0.326861, acc.: 87.50%] [G loss: 2.772084]\n",
      "epoch:10 step:8172 [D loss: 0.352474, acc.: 85.16%] [G loss: 2.401141]\n",
      "epoch:10 step:8173 [D loss: 0.220423, acc.: 94.53%] [G loss: 3.532330]\n",
      "epoch:10 step:8174 [D loss: 0.285616, acc.: 89.06%] [G loss: 4.300608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8175 [D loss: 0.297107, acc.: 86.72%] [G loss: 4.058779]\n",
      "epoch:10 step:8176 [D loss: 0.257221, acc.: 88.28%] [G loss: 2.827355]\n",
      "epoch:10 step:8177 [D loss: 0.369913, acc.: 84.38%] [G loss: 2.471694]\n",
      "epoch:10 step:8178 [D loss: 0.260527, acc.: 92.19%] [G loss: 2.661983]\n",
      "epoch:10 step:8179 [D loss: 0.254393, acc.: 89.06%] [G loss: 3.724494]\n",
      "epoch:10 step:8180 [D loss: 0.374112, acc.: 81.25%] [G loss: 2.676554]\n",
      "epoch:10 step:8181 [D loss: 0.275614, acc.: 85.94%] [G loss: 4.290702]\n",
      "epoch:10 step:8182 [D loss: 0.276998, acc.: 87.50%] [G loss: 4.159846]\n",
      "epoch:10 step:8183 [D loss: 0.251938, acc.: 89.06%] [G loss: 5.975472]\n",
      "epoch:10 step:8184 [D loss: 0.474566, acc.: 79.69%] [G loss: 2.290806]\n",
      "epoch:10 step:8185 [D loss: 0.500741, acc.: 78.91%] [G loss: 2.814917]\n",
      "epoch:10 step:8186 [D loss: 0.391550, acc.: 84.38%] [G loss: 2.960887]\n",
      "epoch:10 step:8187 [D loss: 0.364915, acc.: 82.03%] [G loss: 2.349503]\n",
      "epoch:10 step:8188 [D loss: 0.410352, acc.: 85.94%] [G loss: 2.182347]\n",
      "epoch:10 step:8189 [D loss: 0.439215, acc.: 82.81%] [G loss: 2.164690]\n",
      "epoch:10 step:8190 [D loss: 0.346393, acc.: 82.03%] [G loss: 2.563748]\n",
      "epoch:10 step:8191 [D loss: 0.336386, acc.: 87.50%] [G loss: 2.310728]\n",
      "epoch:10 step:8192 [D loss: 0.390310, acc.: 81.25%] [G loss: 3.008322]\n",
      "epoch:10 step:8193 [D loss: 0.368035, acc.: 83.59%] [G loss: 3.304597]\n",
      "epoch:10 step:8194 [D loss: 0.371673, acc.: 83.59%] [G loss: 2.355094]\n",
      "epoch:10 step:8195 [D loss: 0.262219, acc.: 87.50%] [G loss: 3.910567]\n",
      "epoch:10 step:8196 [D loss: 0.364222, acc.: 83.59%] [G loss: 4.465511]\n",
      "epoch:10 step:8197 [D loss: 0.356230, acc.: 82.81%] [G loss: 2.160800]\n",
      "epoch:10 step:8198 [D loss: 0.483105, acc.: 78.91%] [G loss: 3.796794]\n",
      "epoch:10 step:8199 [D loss: 0.342899, acc.: 88.28%] [G loss: 5.028957]\n",
      "epoch:10 step:8200 [D loss: 0.319923, acc.: 87.50%] [G loss: 4.450144]\n",
      "epoch:10 step:8201 [D loss: 0.204668, acc.: 91.41%] [G loss: 4.535122]\n",
      "epoch:10 step:8202 [D loss: 0.389665, acc.: 82.03%] [G loss: 2.446183]\n",
      "epoch:10 step:8203 [D loss: 0.334206, acc.: 87.50%] [G loss: 4.130023]\n",
      "epoch:10 step:8204 [D loss: 0.402501, acc.: 81.25%] [G loss: 3.138935]\n",
      "epoch:10 step:8205 [D loss: 0.421161, acc.: 82.03%] [G loss: 3.492554]\n",
      "epoch:10 step:8206 [D loss: 0.480022, acc.: 79.69%] [G loss: 2.573363]\n",
      "epoch:10 step:8207 [D loss: 0.326970, acc.: 88.28%] [G loss: 3.026947]\n",
      "epoch:10 step:8208 [D loss: 0.480076, acc.: 81.25%] [G loss: 6.352801]\n",
      "epoch:10 step:8209 [D loss: 0.703859, acc.: 71.09%] [G loss: 2.782624]\n",
      "epoch:10 step:8210 [D loss: 0.459038, acc.: 82.03%] [G loss: 2.539871]\n",
      "epoch:10 step:8211 [D loss: 0.265214, acc.: 91.41%] [G loss: 3.627455]\n",
      "epoch:10 step:8212 [D loss: 0.365314, acc.: 84.38%] [G loss: 3.016018]\n",
      "epoch:10 step:8213 [D loss: 0.367066, acc.: 83.59%] [G loss: 1.925430]\n",
      "epoch:10 step:8214 [D loss: 0.336435, acc.: 84.38%] [G loss: 2.567346]\n",
      "epoch:10 step:8215 [D loss: 0.436454, acc.: 86.72%] [G loss: 2.891344]\n",
      "epoch:10 step:8216 [D loss: 0.429494, acc.: 82.81%] [G loss: 2.984326]\n",
      "epoch:10 step:8217 [D loss: 0.314703, acc.: 87.50%] [G loss: 2.946062]\n",
      "epoch:10 step:8218 [D loss: 0.408155, acc.: 83.59%] [G loss: 1.654555]\n",
      "epoch:10 step:8219 [D loss: 0.388411, acc.: 83.59%] [G loss: 2.855014]\n",
      "epoch:10 step:8220 [D loss: 0.276714, acc.: 89.06%] [G loss: 2.662056]\n",
      "epoch:10 step:8221 [D loss: 0.270062, acc.: 90.62%] [G loss: 2.964912]\n",
      "epoch:10 step:8222 [D loss: 0.272375, acc.: 86.72%] [G loss: 3.775762]\n",
      "epoch:10 step:8223 [D loss: 0.298786, acc.: 86.72%] [G loss: 3.817848]\n",
      "epoch:10 step:8224 [D loss: 0.390482, acc.: 84.38%] [G loss: 2.365056]\n",
      "epoch:10 step:8225 [D loss: 0.404027, acc.: 82.81%] [G loss: 2.068162]\n",
      "epoch:10 step:8226 [D loss: 0.358544, acc.: 86.72%] [G loss: 1.916922]\n",
      "epoch:10 step:8227 [D loss: 0.393113, acc.: 82.81%] [G loss: 2.453742]\n",
      "epoch:10 step:8228 [D loss: 0.364879, acc.: 85.16%] [G loss: 2.341305]\n",
      "epoch:10 step:8229 [D loss: 0.227011, acc.: 89.06%] [G loss: 3.627863]\n",
      "epoch:10 step:8230 [D loss: 0.256634, acc.: 88.28%] [G loss: 5.153390]\n",
      "epoch:10 step:8231 [D loss: 0.313565, acc.: 89.06%] [G loss: 2.646463]\n",
      "epoch:10 step:8232 [D loss: 0.274609, acc.: 89.06%] [G loss: 3.755337]\n",
      "epoch:10 step:8233 [D loss: 0.346480, acc.: 86.72%] [G loss: 4.666277]\n",
      "epoch:10 step:8234 [D loss: 0.282770, acc.: 90.62%] [G loss: 2.435012]\n",
      "epoch:10 step:8235 [D loss: 0.409267, acc.: 79.69%] [G loss: 2.273940]\n",
      "epoch:10 step:8236 [D loss: 0.314210, acc.: 91.41%] [G loss: 2.292448]\n",
      "epoch:10 step:8237 [D loss: 0.295308, acc.: 89.84%] [G loss: 2.700892]\n",
      "epoch:10 step:8238 [D loss: 0.332879, acc.: 86.72%] [G loss: 2.324581]\n",
      "epoch:10 step:8239 [D loss: 0.316887, acc.: 91.41%] [G loss: 2.665505]\n",
      "epoch:10 step:8240 [D loss: 0.378286, acc.: 78.12%] [G loss: 2.947966]\n",
      "epoch:10 step:8241 [D loss: 0.243817, acc.: 92.19%] [G loss: 3.783813]\n",
      "epoch:10 step:8242 [D loss: 0.315771, acc.: 85.16%] [G loss: 5.412820]\n",
      "epoch:10 step:8243 [D loss: 0.308062, acc.: 83.59%] [G loss: 3.328364]\n",
      "epoch:10 step:8244 [D loss: 0.195104, acc.: 94.53%] [G loss: 7.356618]\n",
      "epoch:10 step:8245 [D loss: 0.382255, acc.: 80.47%] [G loss: 3.903839]\n",
      "epoch:10 step:8246 [D loss: 0.236786, acc.: 87.50%] [G loss: 6.850889]\n",
      "epoch:10 step:8247 [D loss: 0.209901, acc.: 92.19%] [G loss: 6.349843]\n",
      "epoch:10 step:8248 [D loss: 0.281609, acc.: 88.28%] [G loss: 3.384035]\n",
      "epoch:10 step:8249 [D loss: 0.282438, acc.: 89.06%] [G loss: 2.062860]\n",
      "epoch:10 step:8250 [D loss: 0.268640, acc.: 89.84%] [G loss: 3.277192]\n",
      "epoch:10 step:8251 [D loss: 0.317542, acc.: 86.72%] [G loss: 4.741795]\n",
      "epoch:10 step:8252 [D loss: 0.151942, acc.: 94.53%] [G loss: 4.295699]\n",
      "epoch:10 step:8253 [D loss: 0.266765, acc.: 90.62%] [G loss: 3.217324]\n",
      "epoch:10 step:8254 [D loss: 0.265538, acc.: 89.06%] [G loss: 2.895509]\n",
      "epoch:10 step:8255 [D loss: 0.344043, acc.: 84.38%] [G loss: 2.666980]\n",
      "epoch:10 step:8256 [D loss: 0.261279, acc.: 92.19%] [G loss: 2.473242]\n",
      "epoch:10 step:8257 [D loss: 0.303596, acc.: 90.62%] [G loss: 2.433720]\n",
      "epoch:10 step:8258 [D loss: 0.398907, acc.: 80.47%] [G loss: 3.170439]\n",
      "epoch:10 step:8259 [D loss: 0.375055, acc.: 87.50%] [G loss: 2.504599]\n",
      "epoch:10 step:8260 [D loss: 0.257488, acc.: 92.97%] [G loss: 3.228367]\n",
      "epoch:10 step:8261 [D loss: 0.293206, acc.: 88.28%] [G loss: 2.701371]\n",
      "epoch:10 step:8262 [D loss: 0.278155, acc.: 88.28%] [G loss: 3.536192]\n",
      "epoch:10 step:8263 [D loss: 0.305047, acc.: 89.06%] [G loss: 2.810340]\n",
      "epoch:10 step:8264 [D loss: 0.270476, acc.: 88.28%] [G loss: 4.946226]\n",
      "epoch:10 step:8265 [D loss: 0.540738, acc.: 76.56%] [G loss: 3.518694]\n",
      "epoch:10 step:8266 [D loss: 1.107523, acc.: 72.66%] [G loss: 9.240622]\n",
      "epoch:10 step:8267 [D loss: 2.679748, acc.: 46.09%] [G loss: 3.212379]\n",
      "epoch:10 step:8268 [D loss: 0.436589, acc.: 79.69%] [G loss: 3.299931]\n",
      "epoch:10 step:8269 [D loss: 0.326034, acc.: 88.28%] [G loss: 2.884820]\n",
      "epoch:10 step:8270 [D loss: 0.263922, acc.: 92.97%] [G loss: 4.120878]\n",
      "epoch:10 step:8271 [D loss: 0.282173, acc.: 87.50%] [G loss: 4.867759]\n",
      "epoch:10 step:8272 [D loss: 0.334502, acc.: 88.28%] [G loss: 4.850828]\n",
      "epoch:10 step:8273 [D loss: 0.263781, acc.: 89.84%] [G loss: 3.592681]\n",
      "epoch:10 step:8274 [D loss: 0.250911, acc.: 89.06%] [G loss: 3.159518]\n",
      "epoch:10 step:8275 [D loss: 0.350339, acc.: 84.38%] [G loss: 2.492334]\n",
      "epoch:10 step:8276 [D loss: 0.284769, acc.: 89.84%] [G loss: 2.997423]\n",
      "epoch:10 step:8277 [D loss: 0.242083, acc.: 89.06%] [G loss: 3.514285]\n",
      "epoch:10 step:8278 [D loss: 0.267269, acc.: 89.06%] [G loss: 3.487226]\n",
      "epoch:10 step:8279 [D loss: 0.308834, acc.: 88.28%] [G loss: 2.826952]\n",
      "epoch:10 step:8280 [D loss: 0.352004, acc.: 89.84%] [G loss: 2.551735]\n",
      "epoch:10 step:8281 [D loss: 0.336911, acc.: 89.06%] [G loss: 2.623457]\n",
      "epoch:10 step:8282 [D loss: 0.343666, acc.: 86.72%] [G loss: 3.192417]\n",
      "epoch:10 step:8283 [D loss: 0.242154, acc.: 92.19%] [G loss: 4.280858]\n",
      "epoch:10 step:8284 [D loss: 0.404862, acc.: 82.03%] [G loss: 2.329048]\n",
      "epoch:10 step:8285 [D loss: 0.223593, acc.: 93.75%] [G loss: 2.702790]\n",
      "epoch:10 step:8286 [D loss: 0.229523, acc.: 94.53%] [G loss: 2.537603]\n",
      "epoch:10 step:8287 [D loss: 0.284318, acc.: 86.72%] [G loss: 2.470131]\n",
      "epoch:10 step:8288 [D loss: 0.255576, acc.: 89.84%] [G loss: 2.204353]\n",
      "epoch:10 step:8289 [D loss: 0.320564, acc.: 87.50%] [G loss: 2.394945]\n",
      "epoch:10 step:8290 [D loss: 0.284556, acc.: 87.50%] [G loss: 2.408280]\n",
      "epoch:10 step:8291 [D loss: 0.255025, acc.: 88.28%] [G loss: 2.586970]\n",
      "epoch:10 step:8292 [D loss: 0.245700, acc.: 94.53%] [G loss: 2.320803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8293 [D loss: 0.416696, acc.: 82.81%] [G loss: 2.319303]\n",
      "epoch:10 step:8294 [D loss: 0.426803, acc.: 82.03%] [G loss: 2.702867]\n",
      "epoch:10 step:8295 [D loss: 0.262440, acc.: 92.19%] [G loss: 2.755113]\n",
      "epoch:10 step:8296 [D loss: 0.320385, acc.: 87.50%] [G loss: 2.959531]\n",
      "epoch:10 step:8297 [D loss: 0.283247, acc.: 89.06%] [G loss: 2.398996]\n",
      "epoch:10 step:8298 [D loss: 0.353658, acc.: 85.94%] [G loss: 4.005777]\n",
      "epoch:10 step:8299 [D loss: 0.315902, acc.: 85.94%] [G loss: 2.326173]\n",
      "epoch:10 step:8300 [D loss: 0.304065, acc.: 86.72%] [G loss: 2.782573]\n",
      "epoch:10 step:8301 [D loss: 0.345939, acc.: 86.72%] [G loss: 2.510943]\n",
      "epoch:10 step:8302 [D loss: 0.383094, acc.: 83.59%] [G loss: 2.165074]\n",
      "epoch:10 step:8303 [D loss: 0.260153, acc.: 91.41%] [G loss: 3.070358]\n",
      "epoch:10 step:8304 [D loss: 0.277258, acc.: 91.41%] [G loss: 2.267663]\n",
      "epoch:10 step:8305 [D loss: 0.240947, acc.: 88.28%] [G loss: 2.791712]\n",
      "epoch:10 step:8306 [D loss: 0.250749, acc.: 90.62%] [G loss: 2.381527]\n",
      "epoch:10 step:8307 [D loss: 0.248823, acc.: 96.09%] [G loss: 2.520748]\n",
      "epoch:10 step:8308 [D loss: 0.203370, acc.: 93.75%] [G loss: 2.815066]\n",
      "epoch:10 step:8309 [D loss: 0.302294, acc.: 85.94%] [G loss: 3.258362]\n",
      "epoch:10 step:8310 [D loss: 0.307322, acc.: 89.84%] [G loss: 2.668474]\n",
      "epoch:10 step:8311 [D loss: 0.330988, acc.: 86.72%] [G loss: 2.372438]\n",
      "epoch:10 step:8312 [D loss: 0.346271, acc.: 85.16%] [G loss: 3.133055]\n",
      "epoch:10 step:8313 [D loss: 0.320138, acc.: 87.50%] [G loss: 3.042797]\n",
      "epoch:10 step:8314 [D loss: 0.222517, acc.: 93.75%] [G loss: 2.434665]\n",
      "epoch:10 step:8315 [D loss: 0.235073, acc.: 93.75%] [G loss: 2.438385]\n",
      "epoch:10 step:8316 [D loss: 0.391608, acc.: 85.16%] [G loss: 2.015831]\n",
      "epoch:10 step:8317 [D loss: 0.304983, acc.: 88.28%] [G loss: 3.097006]\n",
      "epoch:10 step:8318 [D loss: 0.270648, acc.: 88.28%] [G loss: 2.275441]\n",
      "epoch:10 step:8319 [D loss: 0.304340, acc.: 86.72%] [G loss: 3.481836]\n",
      "epoch:10 step:8320 [D loss: 0.269325, acc.: 88.28%] [G loss: 4.766923]\n",
      "epoch:10 step:8321 [D loss: 0.234549, acc.: 89.06%] [G loss: 4.970549]\n",
      "epoch:10 step:8322 [D loss: 0.239313, acc.: 92.19%] [G loss: 2.318692]\n",
      "epoch:10 step:8323 [D loss: 0.281140, acc.: 89.84%] [G loss: 3.408803]\n",
      "epoch:10 step:8324 [D loss: 0.289721, acc.: 90.62%] [G loss: 2.204948]\n",
      "epoch:10 step:8325 [D loss: 0.248244, acc.: 90.62%] [G loss: 2.182845]\n",
      "epoch:10 step:8326 [D loss: 0.331543, acc.: 84.38%] [G loss: 2.093166]\n",
      "epoch:10 step:8327 [D loss: 0.259376, acc.: 89.06%] [G loss: 2.391566]\n",
      "epoch:10 step:8328 [D loss: 0.244671, acc.: 92.19%] [G loss: 3.221560]\n",
      "epoch:10 step:8329 [D loss: 0.325201, acc.: 85.16%] [G loss: 2.930128]\n",
      "epoch:10 step:8330 [D loss: 0.280762, acc.: 89.84%] [G loss: 2.244006]\n",
      "epoch:10 step:8331 [D loss: 0.271546, acc.: 89.84%] [G loss: 2.367781]\n",
      "epoch:10 step:8332 [D loss: 0.290541, acc.: 89.06%] [G loss: 2.525060]\n",
      "epoch:10 step:8333 [D loss: 0.184208, acc.: 95.31%] [G loss: 3.212783]\n",
      "epoch:10 step:8334 [D loss: 0.336451, acc.: 85.94%] [G loss: 2.396421]\n",
      "epoch:10 step:8335 [D loss: 0.274448, acc.: 90.62%] [G loss: 2.749590]\n",
      "epoch:10 step:8336 [D loss: 0.360982, acc.: 83.59%] [G loss: 2.515874]\n",
      "epoch:10 step:8337 [D loss: 0.335425, acc.: 84.38%] [G loss: 2.791500]\n",
      "epoch:10 step:8338 [D loss: 0.246954, acc.: 92.19%] [G loss: 2.716630]\n",
      "epoch:10 step:8339 [D loss: 0.280780, acc.: 89.06%] [G loss: 2.747988]\n",
      "epoch:10 step:8340 [D loss: 0.341431, acc.: 87.50%] [G loss: 2.908789]\n",
      "epoch:10 step:8341 [D loss: 0.314402, acc.: 89.06%] [G loss: 2.365366]\n",
      "epoch:10 step:8342 [D loss: 0.352917, acc.: 82.03%] [G loss: 2.683101]\n",
      "epoch:10 step:8343 [D loss: 0.287090, acc.: 91.41%] [G loss: 2.309448]\n",
      "epoch:10 step:8344 [D loss: 0.260196, acc.: 90.62%] [G loss: 2.878181]\n",
      "epoch:10 step:8345 [D loss: 0.286790, acc.: 85.16%] [G loss: 4.053231]\n",
      "epoch:10 step:8346 [D loss: 0.251734, acc.: 86.72%] [G loss: 3.108814]\n",
      "epoch:10 step:8347 [D loss: 0.365178, acc.: 82.03%] [G loss: 2.202592]\n",
      "epoch:10 step:8348 [D loss: 0.279065, acc.: 89.06%] [G loss: 2.898849]\n",
      "epoch:10 step:8349 [D loss: 0.253230, acc.: 92.19%] [G loss: 2.436080]\n",
      "epoch:10 step:8350 [D loss: 0.274044, acc.: 87.50%] [G loss: 2.435041]\n",
      "epoch:10 step:8351 [D loss: 0.257256, acc.: 89.06%] [G loss: 2.335818]\n",
      "epoch:10 step:8352 [D loss: 0.312765, acc.: 89.06%] [G loss: 2.609336]\n",
      "epoch:10 step:8353 [D loss: 0.269619, acc.: 86.72%] [G loss: 3.109294]\n",
      "epoch:10 step:8354 [D loss: 0.297134, acc.: 88.28%] [G loss: 1.888589]\n",
      "epoch:10 step:8355 [D loss: 0.275055, acc.: 89.06%] [G loss: 3.819824]\n",
      "epoch:10 step:8356 [D loss: 0.280043, acc.: 89.84%] [G loss: 2.500840]\n",
      "epoch:10 step:8357 [D loss: 0.432470, acc.: 81.25%] [G loss: 2.543847]\n",
      "epoch:10 step:8358 [D loss: 0.252946, acc.: 90.62%] [G loss: 3.444353]\n",
      "epoch:10 step:8359 [D loss: 0.327650, acc.: 84.38%] [G loss: 4.001960]\n",
      "epoch:10 step:8360 [D loss: 0.318138, acc.: 89.84%] [G loss: 2.547834]\n",
      "epoch:10 step:8361 [D loss: 0.370113, acc.: 81.25%] [G loss: 3.165257]\n",
      "epoch:10 step:8362 [D loss: 0.309453, acc.: 89.84%] [G loss: 5.859927]\n",
      "epoch:10 step:8363 [D loss: 0.748693, acc.: 70.31%] [G loss: 3.480009]\n",
      "epoch:10 step:8364 [D loss: 0.588186, acc.: 75.00%] [G loss: 5.321426]\n",
      "epoch:10 step:8365 [D loss: 0.825421, acc.: 61.72%] [G loss: 1.785940]\n",
      "epoch:10 step:8366 [D loss: 0.276948, acc.: 87.50%] [G loss: 3.514519]\n",
      "epoch:10 step:8367 [D loss: 0.343836, acc.: 85.16%] [G loss: 2.446385]\n",
      "epoch:10 step:8368 [D loss: 0.329386, acc.: 85.16%] [G loss: 4.222246]\n",
      "epoch:10 step:8369 [D loss: 0.261305, acc.: 89.84%] [G loss: 2.267956]\n",
      "epoch:10 step:8370 [D loss: 0.383081, acc.: 85.94%] [G loss: 3.173743]\n",
      "epoch:10 step:8371 [D loss: 0.219878, acc.: 88.28%] [G loss: 4.578982]\n",
      "epoch:10 step:8372 [D loss: 0.270145, acc.: 90.62%] [G loss: 3.405991]\n",
      "epoch:10 step:8373 [D loss: 0.273298, acc.: 89.06%] [G loss: 3.159828]\n",
      "epoch:10 step:8374 [D loss: 0.337056, acc.: 86.72%] [G loss: 2.320781]\n",
      "epoch:10 step:8375 [D loss: 0.299915, acc.: 89.84%] [G loss: 2.636515]\n",
      "epoch:10 step:8376 [D loss: 0.290685, acc.: 89.06%] [G loss: 2.722537]\n",
      "epoch:10 step:8377 [D loss: 0.449348, acc.: 79.69%] [G loss: 3.136447]\n",
      "epoch:10 step:8378 [D loss: 0.413920, acc.: 79.69%] [G loss: 2.851073]\n",
      "epoch:10 step:8379 [D loss: 0.371142, acc.: 82.81%] [G loss: 3.322680]\n",
      "epoch:10 step:8380 [D loss: 0.308295, acc.: 85.16%] [G loss: 3.578305]\n",
      "epoch:10 step:8381 [D loss: 0.389727, acc.: 79.69%] [G loss: 3.488587]\n",
      "epoch:10 step:8382 [D loss: 0.300662, acc.: 87.50%] [G loss: 3.576529]\n",
      "epoch:10 step:8383 [D loss: 0.288703, acc.: 86.72%] [G loss: 3.280725]\n",
      "epoch:10 step:8384 [D loss: 0.321896, acc.: 85.16%] [G loss: 3.789053]\n",
      "epoch:10 step:8385 [D loss: 0.318363, acc.: 86.72%] [G loss: 2.810130]\n",
      "epoch:10 step:8386 [D loss: 0.483353, acc.: 80.47%] [G loss: 2.707610]\n",
      "epoch:10 step:8387 [D loss: 0.341858, acc.: 84.38%] [G loss: 3.087033]\n",
      "epoch:10 step:8388 [D loss: 0.326458, acc.: 86.72%] [G loss: 4.346119]\n",
      "epoch:10 step:8389 [D loss: 0.312445, acc.: 85.16%] [G loss: 4.453099]\n",
      "epoch:10 step:8390 [D loss: 0.325034, acc.: 87.50%] [G loss: 3.621947]\n",
      "epoch:10 step:8391 [D loss: 0.269659, acc.: 91.41%] [G loss: 3.765945]\n",
      "epoch:10 step:8392 [D loss: 0.360692, acc.: 84.38%] [G loss: 2.950513]\n",
      "epoch:10 step:8393 [D loss: 0.428278, acc.: 83.59%] [G loss: 2.232879]\n",
      "epoch:10 step:8394 [D loss: 0.306824, acc.: 88.28%] [G loss: 2.512835]\n",
      "epoch:10 step:8395 [D loss: 0.383397, acc.: 86.72%] [G loss: 2.660290]\n",
      "epoch:10 step:8396 [D loss: 0.414761, acc.: 81.25%] [G loss: 2.938142]\n",
      "epoch:10 step:8397 [D loss: 0.432176, acc.: 78.91%] [G loss: 3.448807]\n",
      "epoch:10 step:8398 [D loss: 0.257115, acc.: 90.62%] [G loss: 4.061200]\n",
      "epoch:10 step:8399 [D loss: 0.362467, acc.: 83.59%] [G loss: 4.457258]\n",
      "epoch:10 step:8400 [D loss: 0.322121, acc.: 84.38%] [G loss: 3.395384]\n",
      "epoch:10 step:8401 [D loss: 0.333606, acc.: 82.81%] [G loss: 3.809345]\n",
      "epoch:10 step:8402 [D loss: 0.358173, acc.: 85.16%] [G loss: 2.188957]\n",
      "epoch:10 step:8403 [D loss: 0.308591, acc.: 87.50%] [G loss: 2.494364]\n",
      "epoch:10 step:8404 [D loss: 0.385360, acc.: 82.81%] [G loss: 2.502621]\n",
      "epoch:10 step:8405 [D loss: 0.333345, acc.: 87.50%] [G loss: 3.159990]\n",
      "epoch:10 step:8406 [D loss: 0.285753, acc.: 86.72%] [G loss: 4.660417]\n",
      "epoch:10 step:8407 [D loss: 0.264414, acc.: 88.28%] [G loss: 4.320808]\n",
      "epoch:10 step:8408 [D loss: 0.275191, acc.: 88.28%] [G loss: 2.537208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8409 [D loss: 0.311989, acc.: 84.38%] [G loss: 2.520269]\n",
      "epoch:10 step:8410 [D loss: 0.366492, acc.: 80.47%] [G loss: 2.693848]\n",
      "epoch:10 step:8411 [D loss: 0.534922, acc.: 71.88%] [G loss: 2.626748]\n",
      "epoch:10 step:8412 [D loss: 0.306380, acc.: 87.50%] [G loss: 3.935838]\n",
      "epoch:10 step:8413 [D loss: 0.388872, acc.: 84.38%] [G loss: 3.121472]\n",
      "epoch:10 step:8414 [D loss: 0.307474, acc.: 88.28%] [G loss: 3.170952]\n",
      "epoch:10 step:8415 [D loss: 0.385926, acc.: 88.28%] [G loss: 4.505453]\n",
      "epoch:10 step:8416 [D loss: 0.413751, acc.: 83.59%] [G loss: 3.002931]\n",
      "epoch:10 step:8417 [D loss: 0.323699, acc.: 87.50%] [G loss: 2.551723]\n",
      "epoch:10 step:8418 [D loss: 0.406577, acc.: 78.91%] [G loss: 2.197576]\n",
      "epoch:10 step:8419 [D loss: 0.236185, acc.: 96.09%] [G loss: 2.972917]\n",
      "epoch:10 step:8420 [D loss: 0.356464, acc.: 81.25%] [G loss: 4.725976]\n",
      "epoch:10 step:8421 [D loss: 0.340178, acc.: 84.38%] [G loss: 3.562060]\n",
      "epoch:10 step:8422 [D loss: 0.342115, acc.: 82.81%] [G loss: 2.900013]\n",
      "epoch:10 step:8423 [D loss: 0.387030, acc.: 81.25%] [G loss: 4.323340]\n",
      "epoch:10 step:8424 [D loss: 0.289979, acc.: 85.16%] [G loss: 4.684973]\n",
      "epoch:10 step:8425 [D loss: 0.419380, acc.: 85.16%] [G loss: 4.176674]\n",
      "epoch:10 step:8426 [D loss: 0.294742, acc.: 88.28%] [G loss: 3.268348]\n",
      "epoch:10 step:8427 [D loss: 0.311106, acc.: 86.72%] [G loss: 3.246533]\n",
      "epoch:10 step:8428 [D loss: 0.329570, acc.: 84.38%] [G loss: 3.881692]\n",
      "epoch:10 step:8429 [D loss: 0.343891, acc.: 82.03%] [G loss: 4.891681]\n",
      "epoch:10 step:8430 [D loss: 0.271885, acc.: 89.84%] [G loss: 3.415866]\n",
      "epoch:10 step:8431 [D loss: 0.281032, acc.: 90.62%] [G loss: 4.664897]\n",
      "epoch:10 step:8432 [D loss: 0.249351, acc.: 91.41%] [G loss: 4.196971]\n",
      "epoch:10 step:8433 [D loss: 0.263501, acc.: 88.28%] [G loss: 3.318985]\n",
      "epoch:10 step:8434 [D loss: 0.395660, acc.: 85.94%] [G loss: 2.007330]\n",
      "epoch:10 step:8435 [D loss: 0.263664, acc.: 93.75%] [G loss: 3.317769]\n",
      "epoch:10 step:8436 [D loss: 0.238830, acc.: 89.06%] [G loss: 2.922670]\n",
      "epoch:10 step:8437 [D loss: 0.434146, acc.: 82.81%] [G loss: 3.462972]\n",
      "epoch:10 step:8438 [D loss: 0.338056, acc.: 82.81%] [G loss: 2.499540]\n",
      "epoch:10 step:8439 [D loss: 0.203950, acc.: 92.97%] [G loss: 3.056968]\n",
      "epoch:10 step:8440 [D loss: 0.260148, acc.: 90.62%] [G loss: 4.624890]\n",
      "epoch:10 step:8441 [D loss: 0.384603, acc.: 83.59%] [G loss: 3.892229]\n",
      "epoch:10 step:8442 [D loss: 0.332379, acc.: 84.38%] [G loss: 3.797397]\n",
      "epoch:10 step:8443 [D loss: 0.338074, acc.: 85.94%] [G loss: 3.129809]\n",
      "epoch:10 step:8444 [D loss: 0.385577, acc.: 82.03%] [G loss: 3.790085]\n",
      "epoch:10 step:8445 [D loss: 0.337620, acc.: 87.50%] [G loss: 3.903312]\n",
      "epoch:10 step:8446 [D loss: 0.244331, acc.: 89.84%] [G loss: 3.017188]\n",
      "epoch:10 step:8447 [D loss: 0.333948, acc.: 85.94%] [G loss: 2.627250]\n",
      "epoch:10 step:8448 [D loss: 0.226886, acc.: 91.41%] [G loss: 2.529183]\n",
      "epoch:10 step:8449 [D loss: 0.333485, acc.: 85.94%] [G loss: 3.235600]\n",
      "epoch:10 step:8450 [D loss: 0.301816, acc.: 89.06%] [G loss: 3.318901]\n",
      "epoch:10 step:8451 [D loss: 0.323743, acc.: 87.50%] [G loss: 3.029877]\n",
      "epoch:10 step:8452 [D loss: 0.258766, acc.: 90.62%] [G loss: 2.590113]\n",
      "epoch:10 step:8453 [D loss: 0.262582, acc.: 85.94%] [G loss: 3.619828]\n",
      "epoch:10 step:8454 [D loss: 0.327939, acc.: 86.72%] [G loss: 2.684252]\n",
      "epoch:10 step:8455 [D loss: 0.346330, acc.: 85.94%] [G loss: 2.940662]\n",
      "epoch:10 step:8456 [D loss: 0.261089, acc.: 87.50%] [G loss: 3.305333]\n",
      "epoch:10 step:8457 [D loss: 0.249341, acc.: 89.84%] [G loss: 3.225127]\n",
      "epoch:10 step:8458 [D loss: 0.463859, acc.: 78.12%] [G loss: 2.303986]\n",
      "epoch:10 step:8459 [D loss: 0.267815, acc.: 91.41%] [G loss: 2.519796]\n",
      "epoch:10 step:8460 [D loss: 0.273227, acc.: 89.84%] [G loss: 2.852676]\n",
      "epoch:10 step:8461 [D loss: 0.238365, acc.: 91.41%] [G loss: 4.194073]\n",
      "epoch:10 step:8462 [D loss: 0.250658, acc.: 92.19%] [G loss: 3.577989]\n",
      "epoch:10 step:8463 [D loss: 0.320061, acc.: 88.28%] [G loss: 3.211607]\n",
      "epoch:10 step:8464 [D loss: 0.302258, acc.: 87.50%] [G loss: 2.781154]\n",
      "epoch:10 step:8465 [D loss: 0.252130, acc.: 91.41%] [G loss: 3.178838]\n",
      "epoch:10 step:8466 [D loss: 0.344436, acc.: 86.72%] [G loss: 2.313907]\n",
      "epoch:10 step:8467 [D loss: 0.401524, acc.: 83.59%] [G loss: 2.152629]\n",
      "epoch:10 step:8468 [D loss: 0.398977, acc.: 82.81%] [G loss: 2.707852]\n",
      "epoch:10 step:8469 [D loss: 0.371947, acc.: 85.94%] [G loss: 2.521132]\n",
      "epoch:10 step:8470 [D loss: 0.267109, acc.: 92.97%] [G loss: 2.858443]\n",
      "epoch:10 step:8471 [D loss: 0.472458, acc.: 76.56%] [G loss: 3.421263]\n",
      "epoch:10 step:8472 [D loss: 0.397318, acc.: 84.38%] [G loss: 2.569600]\n",
      "epoch:10 step:8473 [D loss: 0.398422, acc.: 83.59%] [G loss: 2.671244]\n",
      "epoch:10 step:8474 [D loss: 0.341397, acc.: 84.38%] [G loss: 4.010537]\n",
      "epoch:10 step:8475 [D loss: 0.233770, acc.: 89.06%] [G loss: 4.080397]\n",
      "epoch:10 step:8476 [D loss: 0.279125, acc.: 88.28%] [G loss: 3.572952]\n",
      "epoch:10 step:8477 [D loss: 0.231342, acc.: 92.19%] [G loss: 2.563173]\n",
      "epoch:10 step:8478 [D loss: 0.327603, acc.: 85.16%] [G loss: 3.078209]\n",
      "epoch:10 step:8479 [D loss: 0.357224, acc.: 82.81%] [G loss: 3.756877]\n",
      "epoch:10 step:8480 [D loss: 0.271472, acc.: 89.06%] [G loss: 3.985919]\n",
      "epoch:10 step:8481 [D loss: 0.385568, acc.: 82.03%] [G loss: 3.836488]\n",
      "epoch:10 step:8482 [D loss: 0.312394, acc.: 88.28%] [G loss: 2.938210]\n",
      "epoch:10 step:8483 [D loss: 0.401263, acc.: 78.91%] [G loss: 3.187705]\n",
      "epoch:10 step:8484 [D loss: 0.391492, acc.: 80.47%] [G loss: 1.895043]\n",
      "epoch:10 step:8485 [D loss: 0.331319, acc.: 86.72%] [G loss: 3.646505]\n",
      "epoch:10 step:8486 [D loss: 0.430497, acc.: 82.03%] [G loss: 2.371760]\n",
      "epoch:10 step:8487 [D loss: 0.380907, acc.: 87.50%] [G loss: 2.974579]\n",
      "epoch:10 step:8488 [D loss: 0.440095, acc.: 81.25%] [G loss: 5.640583]\n",
      "epoch:10 step:8489 [D loss: 0.376523, acc.: 84.38%] [G loss: 3.212511]\n",
      "epoch:10 step:8490 [D loss: 0.290650, acc.: 87.50%] [G loss: 4.015482]\n",
      "epoch:10 step:8491 [D loss: 0.328273, acc.: 85.94%] [G loss: 2.830901]\n",
      "epoch:10 step:8492 [D loss: 0.455853, acc.: 81.25%] [G loss: 1.963624]\n",
      "epoch:10 step:8493 [D loss: 0.444449, acc.: 78.12%] [G loss: 3.836383]\n",
      "epoch:10 step:8494 [D loss: 0.330272, acc.: 85.16%] [G loss: 3.431723]\n",
      "epoch:10 step:8495 [D loss: 0.249404, acc.: 92.97%] [G loss: 3.915141]\n",
      "epoch:10 step:8496 [D loss: 0.390748, acc.: 80.47%] [G loss: 2.823705]\n",
      "epoch:10 step:8497 [D loss: 0.244868, acc.: 92.19%] [G loss: 4.834967]\n",
      "epoch:10 step:8498 [D loss: 0.286339, acc.: 86.72%] [G loss: 6.076605]\n",
      "epoch:10 step:8499 [D loss: 0.490721, acc.: 75.00%] [G loss: 2.986324]\n",
      "epoch:10 step:8500 [D loss: 0.375751, acc.: 84.38%] [G loss: 3.141036]\n",
      "epoch:10 step:8501 [D loss: 0.311926, acc.: 89.84%] [G loss: 3.473684]\n",
      "epoch:10 step:8502 [D loss: 0.434669, acc.: 78.12%] [G loss: 3.590149]\n",
      "epoch:10 step:8503 [D loss: 0.413429, acc.: 85.16%] [G loss: 4.516547]\n",
      "epoch:10 step:8504 [D loss: 0.359223, acc.: 84.38%] [G loss: 3.976434]\n",
      "epoch:10 step:8505 [D loss: 0.290747, acc.: 88.28%] [G loss: 4.912025]\n",
      "epoch:10 step:8506 [D loss: 0.302091, acc.: 86.72%] [G loss: 2.698275]\n",
      "epoch:10 step:8507 [D loss: 0.582697, acc.: 75.00%] [G loss: 2.261348]\n",
      "epoch:10 step:8508 [D loss: 0.328480, acc.: 85.16%] [G loss: 2.881341]\n",
      "epoch:10 step:8509 [D loss: 0.455346, acc.: 82.81%] [G loss: 2.354139]\n",
      "epoch:10 step:8510 [D loss: 0.431059, acc.: 82.03%] [G loss: 2.651672]\n",
      "epoch:10 step:8511 [D loss: 0.310751, acc.: 88.28%] [G loss: 3.967724]\n",
      "epoch:10 step:8512 [D loss: 0.319251, acc.: 87.50%] [G loss: 4.759226]\n",
      "epoch:10 step:8513 [D loss: 0.357637, acc.: 83.59%] [G loss: 3.208369]\n",
      "epoch:10 step:8514 [D loss: 0.475303, acc.: 79.69%] [G loss: 2.838804]\n",
      "epoch:10 step:8515 [D loss: 0.257608, acc.: 88.28%] [G loss: 5.889514]\n",
      "epoch:10 step:8516 [D loss: 0.471542, acc.: 76.56%] [G loss: 2.506502]\n",
      "epoch:10 step:8517 [D loss: 0.260289, acc.: 88.28%] [G loss: 5.521110]\n",
      "epoch:10 step:8518 [D loss: 0.494504, acc.: 78.12%] [G loss: 3.088169]\n",
      "epoch:10 step:8519 [D loss: 0.469017, acc.: 81.25%] [G loss: 5.321214]\n",
      "epoch:10 step:8520 [D loss: 0.536284, acc.: 73.44%] [G loss: 3.431488]\n",
      "epoch:10 step:8521 [D loss: 0.361308, acc.: 83.59%] [G loss: 2.217625]\n",
      "epoch:10 step:8522 [D loss: 0.348776, acc.: 82.03%] [G loss: 2.720969]\n",
      "epoch:10 step:8523 [D loss: 0.277887, acc.: 89.84%] [G loss: 2.657990]\n",
      "epoch:10 step:8524 [D loss: 0.338485, acc.: 88.28%] [G loss: 3.074196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8525 [D loss: 0.417299, acc.: 79.69%] [G loss: 2.374186]\n",
      "epoch:10 step:8526 [D loss: 0.373087, acc.: 85.16%] [G loss: 2.395925]\n",
      "epoch:10 step:8527 [D loss: 0.290592, acc.: 89.06%] [G loss: 2.107756]\n",
      "epoch:10 step:8528 [D loss: 0.464269, acc.: 79.69%] [G loss: 2.568455]\n",
      "epoch:10 step:8529 [D loss: 0.450416, acc.: 78.12%] [G loss: 3.546499]\n",
      "epoch:10 step:8530 [D loss: 0.311459, acc.: 86.72%] [G loss: 2.339491]\n",
      "epoch:10 step:8531 [D loss: 0.423717, acc.: 82.81%] [G loss: 2.663598]\n",
      "epoch:10 step:8532 [D loss: 0.323849, acc.: 83.59%] [G loss: 3.296543]\n",
      "epoch:10 step:8533 [D loss: 0.260707, acc.: 89.06%] [G loss: 3.943343]\n",
      "epoch:10 step:8534 [D loss: 0.156857, acc.: 97.66%] [G loss: 3.329236]\n",
      "epoch:10 step:8535 [D loss: 0.278702, acc.: 91.41%] [G loss: 2.674123]\n",
      "epoch:10 step:8536 [D loss: 0.239985, acc.: 90.62%] [G loss: 2.999528]\n",
      "epoch:10 step:8537 [D loss: 0.349084, acc.: 83.59%] [G loss: 4.302074]\n",
      "epoch:10 step:8538 [D loss: 0.320606, acc.: 88.28%] [G loss: 3.012048]\n",
      "epoch:10 step:8539 [D loss: 0.325856, acc.: 88.28%] [G loss: 2.359885]\n",
      "epoch:10 step:8540 [D loss: 0.294830, acc.: 86.72%] [G loss: 2.928582]\n",
      "epoch:10 step:8541 [D loss: 0.357545, acc.: 83.59%] [G loss: 3.144710]\n",
      "epoch:10 step:8542 [D loss: 0.331274, acc.: 85.16%] [G loss: 2.511249]\n",
      "epoch:10 step:8543 [D loss: 0.401144, acc.: 84.38%] [G loss: 2.559340]\n",
      "epoch:10 step:8544 [D loss: 0.322953, acc.: 86.72%] [G loss: 2.469917]\n",
      "epoch:10 step:8545 [D loss: 0.428218, acc.: 82.81%] [G loss: 2.472796]\n",
      "epoch:10 step:8546 [D loss: 0.309329, acc.: 88.28%] [G loss: 2.909045]\n",
      "epoch:10 step:8547 [D loss: 0.291341, acc.: 88.28%] [G loss: 3.426231]\n",
      "epoch:10 step:8548 [D loss: 0.332930, acc.: 83.59%] [G loss: 5.404691]\n",
      "epoch:10 step:8549 [D loss: 0.373575, acc.: 81.25%] [G loss: 2.539587]\n",
      "epoch:10 step:8550 [D loss: 0.314519, acc.: 85.94%] [G loss: 2.764773]\n",
      "epoch:10 step:8551 [D loss: 0.299355, acc.: 89.84%] [G loss: 3.029321]\n",
      "epoch:10 step:8552 [D loss: 0.278535, acc.: 87.50%] [G loss: 2.708961]\n",
      "epoch:10 step:8553 [D loss: 0.296929, acc.: 85.16%] [G loss: 3.358910]\n",
      "epoch:10 step:8554 [D loss: 0.293507, acc.: 87.50%] [G loss: 3.812919]\n",
      "epoch:10 step:8555 [D loss: 0.437461, acc.: 83.59%] [G loss: 2.957340]\n",
      "epoch:10 step:8556 [D loss: 0.327712, acc.: 85.94%] [G loss: 2.951992]\n",
      "epoch:10 step:8557 [D loss: 0.221557, acc.: 92.97%] [G loss: 2.820343]\n",
      "epoch:10 step:8558 [D loss: 0.259192, acc.: 92.97%] [G loss: 2.485645]\n",
      "epoch:10 step:8559 [D loss: 0.231452, acc.: 92.19%] [G loss: 3.769852]\n",
      "epoch:10 step:8560 [D loss: 0.272584, acc.: 85.94%] [G loss: 4.400953]\n",
      "epoch:10 step:8561 [D loss: 0.353407, acc.: 84.38%] [G loss: 2.623748]\n",
      "epoch:10 step:8562 [D loss: 0.390760, acc.: 85.16%] [G loss: 3.102258]\n",
      "epoch:10 step:8563 [D loss: 0.304479, acc.: 88.28%] [G loss: 3.171569]\n",
      "epoch:10 step:8564 [D loss: 0.305521, acc.: 83.59%] [G loss: 3.859010]\n",
      "epoch:10 step:8565 [D loss: 0.312937, acc.: 86.72%] [G loss: 3.626924]\n",
      "epoch:10 step:8566 [D loss: 0.233604, acc.: 92.97%] [G loss: 2.938008]\n",
      "epoch:10 step:8567 [D loss: 0.361420, acc.: 82.81%] [G loss: 2.250872]\n",
      "epoch:10 step:8568 [D loss: 0.288250, acc.: 87.50%] [G loss: 3.586660]\n",
      "epoch:10 step:8569 [D loss: 0.282245, acc.: 89.84%] [G loss: 3.449953]\n",
      "epoch:10 step:8570 [D loss: 0.303609, acc.: 86.72%] [G loss: 2.862277]\n",
      "epoch:10 step:8571 [D loss: 0.286477, acc.: 91.41%] [G loss: 4.088064]\n",
      "epoch:10 step:8572 [D loss: 0.381357, acc.: 85.16%] [G loss: 3.313887]\n",
      "epoch:10 step:8573 [D loss: 0.512254, acc.: 78.91%] [G loss: 2.507771]\n",
      "epoch:10 step:8574 [D loss: 0.328273, acc.: 83.59%] [G loss: 3.324612]\n",
      "epoch:10 step:8575 [D loss: 0.369613, acc.: 80.47%] [G loss: 3.692356]\n",
      "epoch:10 step:8576 [D loss: 0.334858, acc.: 86.72%] [G loss: 3.873565]\n",
      "epoch:10 step:8577 [D loss: 0.233665, acc.: 93.75%] [G loss: 3.853518]\n",
      "epoch:10 step:8578 [D loss: 0.414581, acc.: 82.81%] [G loss: 2.825025]\n",
      "epoch:10 step:8579 [D loss: 0.238152, acc.: 90.62%] [G loss: 3.344532]\n",
      "epoch:10 step:8580 [D loss: 0.280747, acc.: 91.41%] [G loss: 2.687699]\n",
      "epoch:10 step:8581 [D loss: 0.250112, acc.: 89.84%] [G loss: 3.592250]\n",
      "epoch:10 step:8582 [D loss: 0.377887, acc.: 82.81%] [G loss: 4.251177]\n",
      "epoch:10 step:8583 [D loss: 0.283758, acc.: 87.50%] [G loss: 3.694396]\n",
      "epoch:10 step:8584 [D loss: 0.306487, acc.: 86.72%] [G loss: 2.530941]\n",
      "epoch:10 step:8585 [D loss: 0.362866, acc.: 86.72%] [G loss: 5.773358]\n",
      "epoch:10 step:8586 [D loss: 0.239856, acc.: 89.84%] [G loss: 5.319782]\n",
      "epoch:10 step:8587 [D loss: 0.351454, acc.: 80.47%] [G loss: 4.178847]\n",
      "epoch:10 step:8588 [D loss: 0.424025, acc.: 78.12%] [G loss: 3.060517]\n",
      "epoch:10 step:8589 [D loss: 0.309542, acc.: 86.72%] [G loss: 2.883788]\n",
      "epoch:10 step:8590 [D loss: 0.345302, acc.: 84.38%] [G loss: 3.001970]\n",
      "epoch:10 step:8591 [D loss: 0.340638, acc.: 84.38%] [G loss: 2.449109]\n",
      "epoch:11 step:8592 [D loss: 0.379282, acc.: 85.16%] [G loss: 2.417875]\n",
      "epoch:11 step:8593 [D loss: 0.237369, acc.: 92.19%] [G loss: 2.874675]\n",
      "epoch:11 step:8594 [D loss: 0.320204, acc.: 86.72%] [G loss: 3.213446]\n",
      "epoch:11 step:8595 [D loss: 0.315022, acc.: 87.50%] [G loss: 2.402527]\n",
      "epoch:11 step:8596 [D loss: 0.243006, acc.: 88.28%] [G loss: 3.580984]\n",
      "epoch:11 step:8597 [D loss: 0.200759, acc.: 89.84%] [G loss: 4.234582]\n",
      "epoch:11 step:8598 [D loss: 0.228856, acc.: 92.19%] [G loss: 4.248592]\n",
      "epoch:11 step:8599 [D loss: 0.265496, acc.: 89.84%] [G loss: 3.200516]\n",
      "epoch:11 step:8600 [D loss: 0.293664, acc.: 85.94%] [G loss: 3.345096]\n",
      "epoch:11 step:8601 [D loss: 0.290335, acc.: 87.50%] [G loss: 3.369696]\n",
      "epoch:11 step:8602 [D loss: 0.296487, acc.: 87.50%] [G loss: 3.606140]\n",
      "epoch:11 step:8603 [D loss: 0.273493, acc.: 85.16%] [G loss: 2.412321]\n",
      "epoch:11 step:8604 [D loss: 0.277079, acc.: 89.06%] [G loss: 2.616905]\n",
      "epoch:11 step:8605 [D loss: 0.312614, acc.: 88.28%] [G loss: 2.352639]\n",
      "epoch:11 step:8606 [D loss: 0.283335, acc.: 90.62%] [G loss: 2.472834]\n",
      "epoch:11 step:8607 [D loss: 0.227553, acc.: 90.62%] [G loss: 3.316986]\n",
      "epoch:11 step:8608 [D loss: 0.229992, acc.: 92.19%] [G loss: 4.533972]\n",
      "epoch:11 step:8609 [D loss: 0.259292, acc.: 88.28%] [G loss: 2.646852]\n",
      "epoch:11 step:8610 [D loss: 0.307382, acc.: 83.59%] [G loss: 4.316806]\n",
      "epoch:11 step:8611 [D loss: 0.235607, acc.: 92.19%] [G loss: 3.230216]\n",
      "epoch:11 step:8612 [D loss: 0.393718, acc.: 79.69%] [G loss: 2.668275]\n",
      "epoch:11 step:8613 [D loss: 0.218852, acc.: 89.84%] [G loss: 3.102110]\n",
      "epoch:11 step:8614 [D loss: 0.339712, acc.: 85.16%] [G loss: 2.625892]\n",
      "epoch:11 step:8615 [D loss: 0.412885, acc.: 82.81%] [G loss: 2.396368]\n",
      "epoch:11 step:8616 [D loss: 0.324340, acc.: 85.94%] [G loss: 2.320594]\n",
      "epoch:11 step:8617 [D loss: 0.272579, acc.: 89.84%] [G loss: 2.290770]\n",
      "epoch:11 step:8618 [D loss: 0.249999, acc.: 91.41%] [G loss: 2.666728]\n",
      "epoch:11 step:8619 [D loss: 0.272844, acc.: 88.28%] [G loss: 2.359186]\n",
      "epoch:11 step:8620 [D loss: 0.295655, acc.: 88.28%] [G loss: 3.229387]\n",
      "epoch:11 step:8621 [D loss: 0.276652, acc.: 87.50%] [G loss: 4.938300]\n",
      "epoch:11 step:8622 [D loss: 0.357087, acc.: 82.81%] [G loss: 4.826511]\n",
      "epoch:11 step:8623 [D loss: 0.443512, acc.: 77.34%] [G loss: 3.521894]\n",
      "epoch:11 step:8624 [D loss: 0.405407, acc.: 83.59%] [G loss: 3.736713]\n",
      "epoch:11 step:8625 [D loss: 0.291161, acc.: 85.16%] [G loss: 7.564870]\n",
      "epoch:11 step:8626 [D loss: 0.326786, acc.: 85.16%] [G loss: 3.048079]\n",
      "epoch:11 step:8627 [D loss: 0.231761, acc.: 89.84%] [G loss: 4.391741]\n",
      "epoch:11 step:8628 [D loss: 0.289918, acc.: 86.72%] [G loss: 2.333747]\n",
      "epoch:11 step:8629 [D loss: 0.218315, acc.: 92.97%] [G loss: 4.611395]\n",
      "epoch:11 step:8630 [D loss: 0.258486, acc.: 92.19%] [G loss: 3.336600]\n",
      "epoch:11 step:8631 [D loss: 0.268227, acc.: 89.06%] [G loss: 2.688825]\n",
      "epoch:11 step:8632 [D loss: 0.311660, acc.: 84.38%] [G loss: 3.207310]\n",
      "epoch:11 step:8633 [D loss: 0.421392, acc.: 82.81%] [G loss: 5.442810]\n",
      "epoch:11 step:8634 [D loss: 0.620852, acc.: 69.53%] [G loss: 6.867015]\n",
      "epoch:11 step:8635 [D loss: 1.381034, acc.: 57.81%] [G loss: 11.269986]\n",
      "epoch:11 step:8636 [D loss: 2.822487, acc.: 45.31%] [G loss: 2.840809]\n",
      "epoch:11 step:8637 [D loss: 0.750403, acc.: 71.88%] [G loss: 2.886767]\n",
      "epoch:11 step:8638 [D loss: 0.357533, acc.: 82.03%] [G loss: 4.246484]\n",
      "epoch:11 step:8639 [D loss: 0.473454, acc.: 84.38%] [G loss: 5.329864]\n",
      "epoch:11 step:8640 [D loss: 0.231665, acc.: 91.41%] [G loss: 4.127614]\n",
      "epoch:11 step:8641 [D loss: 0.309030, acc.: 89.06%] [G loss: 3.395365]\n",
      "epoch:11 step:8642 [D loss: 0.301278, acc.: 89.84%] [G loss: 4.310710]\n",
      "epoch:11 step:8643 [D loss: 0.316984, acc.: 86.72%] [G loss: 2.928193]\n",
      "epoch:11 step:8644 [D loss: 0.253201, acc.: 92.97%] [G loss: 2.726339]\n",
      "epoch:11 step:8645 [D loss: 0.351001, acc.: 84.38%] [G loss: 2.303866]\n",
      "epoch:11 step:8646 [D loss: 0.256485, acc.: 92.19%] [G loss: 2.584145]\n",
      "epoch:11 step:8647 [D loss: 0.408171, acc.: 82.03%] [G loss: 2.521804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8648 [D loss: 0.347530, acc.: 86.72%] [G loss: 2.761895]\n",
      "epoch:11 step:8649 [D loss: 0.368154, acc.: 84.38%] [G loss: 2.372403]\n",
      "epoch:11 step:8650 [D loss: 0.314086, acc.: 86.72%] [G loss: 2.248469]\n",
      "epoch:11 step:8651 [D loss: 0.385222, acc.: 80.47%] [G loss: 2.407952]\n",
      "epoch:11 step:8652 [D loss: 0.361254, acc.: 89.06%] [G loss: 2.055677]\n",
      "epoch:11 step:8653 [D loss: 0.325531, acc.: 85.94%] [G loss: 2.932642]\n",
      "epoch:11 step:8654 [D loss: 0.289044, acc.: 89.06%] [G loss: 2.500418]\n",
      "epoch:11 step:8655 [D loss: 0.458057, acc.: 82.03%] [G loss: 2.207662]\n",
      "epoch:11 step:8656 [D loss: 0.364114, acc.: 85.16%] [G loss: 2.331203]\n",
      "epoch:11 step:8657 [D loss: 0.357119, acc.: 87.50%] [G loss: 2.640383]\n",
      "epoch:11 step:8658 [D loss: 0.307241, acc.: 87.50%] [G loss: 2.887936]\n",
      "epoch:11 step:8659 [D loss: 0.313054, acc.: 88.28%] [G loss: 3.329427]\n",
      "epoch:11 step:8660 [D loss: 0.241597, acc.: 89.84%] [G loss: 4.130362]\n",
      "epoch:11 step:8661 [D loss: 0.378553, acc.: 86.72%] [G loss: 2.446930]\n",
      "epoch:11 step:8662 [D loss: 0.321363, acc.: 88.28%] [G loss: 2.271592]\n",
      "epoch:11 step:8663 [D loss: 0.369033, acc.: 83.59%] [G loss: 3.155769]\n",
      "epoch:11 step:8664 [D loss: 0.276491, acc.: 89.06%] [G loss: 3.868531]\n",
      "epoch:11 step:8665 [D loss: 0.460495, acc.: 74.22%] [G loss: 3.797930]\n",
      "epoch:11 step:8666 [D loss: 0.362276, acc.: 85.94%] [G loss: 3.114016]\n",
      "epoch:11 step:8667 [D loss: 0.263289, acc.: 89.06%] [G loss: 4.865809]\n",
      "epoch:11 step:8668 [D loss: 0.288490, acc.: 89.06%] [G loss: 2.387927]\n",
      "epoch:11 step:8669 [D loss: 0.358620, acc.: 83.59%] [G loss: 2.860483]\n",
      "epoch:11 step:8670 [D loss: 0.408888, acc.: 84.38%] [G loss: 2.712803]\n",
      "epoch:11 step:8671 [D loss: 0.324732, acc.: 88.28%] [G loss: 2.904899]\n",
      "epoch:11 step:8672 [D loss: 0.268346, acc.: 89.84%] [G loss: 2.352131]\n",
      "epoch:11 step:8673 [D loss: 0.264199, acc.: 89.06%] [G loss: 2.416476]\n",
      "epoch:11 step:8674 [D loss: 0.263398, acc.: 88.28%] [G loss: 2.952209]\n",
      "epoch:11 step:8675 [D loss: 0.219269, acc.: 91.41%] [G loss: 2.804008]\n",
      "epoch:11 step:8676 [D loss: 0.278758, acc.: 91.41%] [G loss: 2.297135]\n",
      "epoch:11 step:8677 [D loss: 0.308973, acc.: 88.28%] [G loss: 2.576126]\n",
      "epoch:11 step:8678 [D loss: 0.318169, acc.: 89.84%] [G loss: 2.258303]\n",
      "epoch:11 step:8679 [D loss: 0.388814, acc.: 82.03%] [G loss: 3.182728]\n",
      "epoch:11 step:8680 [D loss: 0.329191, acc.: 85.94%] [G loss: 3.953938]\n",
      "epoch:11 step:8681 [D loss: 0.276433, acc.: 89.84%] [G loss: 3.377400]\n",
      "epoch:11 step:8682 [D loss: 0.357040, acc.: 84.38%] [G loss: 2.168474]\n",
      "epoch:11 step:8683 [D loss: 0.297339, acc.: 90.62%] [G loss: 3.028516]\n",
      "epoch:11 step:8684 [D loss: 0.324641, acc.: 86.72%] [G loss: 3.070141]\n",
      "epoch:11 step:8685 [D loss: 0.296964, acc.: 85.16%] [G loss: 4.903228]\n",
      "epoch:11 step:8686 [D loss: 0.458422, acc.: 77.34%] [G loss: 4.101644]\n",
      "epoch:11 step:8687 [D loss: 0.363965, acc.: 82.81%] [G loss: 2.549671]\n",
      "epoch:11 step:8688 [D loss: 0.349087, acc.: 86.72%] [G loss: 4.474885]\n",
      "epoch:11 step:8689 [D loss: 0.410258, acc.: 83.59%] [G loss: 2.663090]\n",
      "epoch:11 step:8690 [D loss: 0.374792, acc.: 82.03%] [G loss: 3.299906]\n",
      "epoch:11 step:8691 [D loss: 0.335345, acc.: 85.94%] [G loss: 3.358272]\n",
      "epoch:11 step:8692 [D loss: 0.278375, acc.: 89.84%] [G loss: 2.642560]\n",
      "epoch:11 step:8693 [D loss: 0.375080, acc.: 86.72%] [G loss: 3.137873]\n",
      "epoch:11 step:8694 [D loss: 0.289402, acc.: 90.62%] [G loss: 2.168248]\n",
      "epoch:11 step:8695 [D loss: 0.277237, acc.: 95.31%] [G loss: 2.854431]\n",
      "epoch:11 step:8696 [D loss: 0.310674, acc.: 85.16%] [G loss: 3.569547]\n",
      "epoch:11 step:8697 [D loss: 0.283230, acc.: 85.16%] [G loss: 2.932500]\n",
      "epoch:11 step:8698 [D loss: 0.314914, acc.: 87.50%] [G loss: 2.617445]\n",
      "epoch:11 step:8699 [D loss: 0.282947, acc.: 89.06%] [G loss: 2.870408]\n",
      "epoch:11 step:8700 [D loss: 0.387891, acc.: 82.03%] [G loss: 2.482801]\n",
      "epoch:11 step:8701 [D loss: 0.231351, acc.: 91.41%] [G loss: 4.369209]\n",
      "epoch:11 step:8702 [D loss: 0.237266, acc.: 91.41%] [G loss: 5.009711]\n",
      "epoch:11 step:8703 [D loss: 0.227356, acc.: 95.31%] [G loss: 3.002396]\n",
      "epoch:11 step:8704 [D loss: 0.371285, acc.: 83.59%] [G loss: 2.027824]\n",
      "epoch:11 step:8705 [D loss: 0.287446, acc.: 88.28%] [G loss: 2.370176]\n",
      "epoch:11 step:8706 [D loss: 0.261683, acc.: 91.41%] [G loss: 2.665104]\n",
      "epoch:11 step:8707 [D loss: 0.284036, acc.: 89.84%] [G loss: 2.264329]\n",
      "epoch:11 step:8708 [D loss: 0.385091, acc.: 82.81%] [G loss: 2.772252]\n",
      "epoch:11 step:8709 [D loss: 0.271264, acc.: 87.50%] [G loss: 4.442812]\n",
      "epoch:11 step:8710 [D loss: 0.321017, acc.: 88.28%] [G loss: 2.937437]\n",
      "epoch:11 step:8711 [D loss: 0.407123, acc.: 82.81%] [G loss: 2.090126]\n",
      "epoch:11 step:8712 [D loss: 0.313603, acc.: 89.84%] [G loss: 2.783222]\n",
      "epoch:11 step:8713 [D loss: 0.341259, acc.: 86.72%] [G loss: 2.433028]\n",
      "epoch:11 step:8714 [D loss: 0.486494, acc.: 81.25%] [G loss: 2.136931]\n",
      "epoch:11 step:8715 [D loss: 0.311322, acc.: 85.94%] [G loss: 2.336499]\n",
      "epoch:11 step:8716 [D loss: 0.422613, acc.: 78.12%] [G loss: 3.193139]\n",
      "epoch:11 step:8717 [D loss: 0.313500, acc.: 88.28%] [G loss: 2.639386]\n",
      "epoch:11 step:8718 [D loss: 0.383518, acc.: 88.28%] [G loss: 3.049682]\n",
      "epoch:11 step:8719 [D loss: 0.404835, acc.: 82.81%] [G loss: 2.990002]\n",
      "epoch:11 step:8720 [D loss: 0.348528, acc.: 82.81%] [G loss: 4.509075]\n",
      "epoch:11 step:8721 [D loss: 0.408831, acc.: 82.03%] [G loss: 2.774492]\n",
      "epoch:11 step:8722 [D loss: 0.335821, acc.: 82.81%] [G loss: 3.192083]\n",
      "epoch:11 step:8723 [D loss: 0.321478, acc.: 90.62%] [G loss: 3.218798]\n",
      "epoch:11 step:8724 [D loss: 0.278460, acc.: 90.62%] [G loss: 3.157313]\n",
      "epoch:11 step:8725 [D loss: 0.209333, acc.: 91.41%] [G loss: 2.714169]\n",
      "epoch:11 step:8726 [D loss: 0.394211, acc.: 85.16%] [G loss: 2.667455]\n",
      "epoch:11 step:8727 [D loss: 0.314370, acc.: 86.72%] [G loss: 2.923561]\n",
      "epoch:11 step:8728 [D loss: 0.463883, acc.: 80.47%] [G loss: 2.712958]\n",
      "epoch:11 step:8729 [D loss: 0.380877, acc.: 84.38%] [G loss: 2.281629]\n",
      "epoch:11 step:8730 [D loss: 0.361451, acc.: 84.38%] [G loss: 3.944894]\n",
      "epoch:11 step:8731 [D loss: 0.360947, acc.: 87.50%] [G loss: 2.489554]\n",
      "epoch:11 step:8732 [D loss: 0.409153, acc.: 78.91%] [G loss: 2.894595]\n",
      "epoch:11 step:8733 [D loss: 0.600328, acc.: 77.34%] [G loss: 4.726627]\n",
      "epoch:11 step:8734 [D loss: 0.359205, acc.: 83.59%] [G loss: 3.176827]\n",
      "epoch:11 step:8735 [D loss: 0.399241, acc.: 82.03%] [G loss: 8.703637]\n",
      "epoch:11 step:8736 [D loss: 0.389712, acc.: 79.69%] [G loss: 3.405674]\n",
      "epoch:11 step:8737 [D loss: 0.370220, acc.: 84.38%] [G loss: 3.750629]\n",
      "epoch:11 step:8738 [D loss: 0.376141, acc.: 85.16%] [G loss: 3.289993]\n",
      "epoch:11 step:8739 [D loss: 0.414346, acc.: 79.69%] [G loss: 3.929614]\n",
      "epoch:11 step:8740 [D loss: 0.595875, acc.: 72.66%] [G loss: 2.791982]\n",
      "epoch:11 step:8741 [D loss: 0.703809, acc.: 76.56%] [G loss: 2.222104]\n",
      "epoch:11 step:8742 [D loss: 0.471987, acc.: 80.47%] [G loss: 3.273878]\n",
      "epoch:11 step:8743 [D loss: 0.393615, acc.: 81.25%] [G loss: 3.345605]\n",
      "epoch:11 step:8744 [D loss: 0.479106, acc.: 79.69%] [G loss: 2.280991]\n",
      "epoch:11 step:8745 [D loss: 0.257437, acc.: 90.62%] [G loss: 4.116178]\n",
      "epoch:11 step:8746 [D loss: 0.285516, acc.: 86.72%] [G loss: 3.140870]\n",
      "epoch:11 step:8747 [D loss: 0.284434, acc.: 92.19%] [G loss: 2.598527]\n",
      "epoch:11 step:8748 [D loss: 0.313960, acc.: 88.28%] [G loss: 2.464982]\n",
      "epoch:11 step:8749 [D loss: 0.395876, acc.: 79.69%] [G loss: 2.147361]\n",
      "epoch:11 step:8750 [D loss: 0.396854, acc.: 83.59%] [G loss: 2.618411]\n",
      "epoch:11 step:8751 [D loss: 0.498718, acc.: 78.12%] [G loss: 4.060976]\n",
      "epoch:11 step:8752 [D loss: 0.710213, acc.: 73.44%] [G loss: 4.963033]\n",
      "epoch:11 step:8753 [D loss: 0.462906, acc.: 81.25%] [G loss: 3.523982]\n",
      "epoch:11 step:8754 [D loss: 0.429238, acc.: 83.59%] [G loss: 2.507295]\n",
      "epoch:11 step:8755 [D loss: 0.477038, acc.: 77.34%] [G loss: 2.712972]\n",
      "epoch:11 step:8756 [D loss: 0.365655, acc.: 85.16%] [G loss: 2.906674]\n",
      "epoch:11 step:8757 [D loss: 0.401155, acc.: 81.25%] [G loss: 2.662497]\n",
      "epoch:11 step:8758 [D loss: 0.413781, acc.: 85.16%] [G loss: 3.090569]\n",
      "epoch:11 step:8759 [D loss: 0.383132, acc.: 91.41%] [G loss: 1.969889]\n",
      "epoch:11 step:8760 [D loss: 0.356293, acc.: 82.03%] [G loss: 2.651151]\n",
      "epoch:11 step:8761 [D loss: 0.356982, acc.: 87.50%] [G loss: 2.758038]\n",
      "epoch:11 step:8762 [D loss: 0.408723, acc.: 83.59%] [G loss: 2.707125]\n",
      "epoch:11 step:8763 [D loss: 0.289583, acc.: 89.84%] [G loss: 2.833728]\n",
      "epoch:11 step:8764 [D loss: 0.284515, acc.: 89.06%] [G loss: 3.078788]\n",
      "epoch:11 step:8765 [D loss: 0.306267, acc.: 85.16%] [G loss: 2.798795]\n",
      "epoch:11 step:8766 [D loss: 0.279586, acc.: 89.84%] [G loss: 2.667494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8767 [D loss: 0.279855, acc.: 90.62%] [G loss: 2.352648]\n",
      "epoch:11 step:8768 [D loss: 0.351881, acc.: 84.38%] [G loss: 2.640535]\n",
      "epoch:11 step:8769 [D loss: 0.469291, acc.: 78.91%] [G loss: 2.199778]\n",
      "epoch:11 step:8770 [D loss: 0.385632, acc.: 81.25%] [G loss: 3.353381]\n",
      "epoch:11 step:8771 [D loss: 0.336551, acc.: 86.72%] [G loss: 5.475424]\n",
      "epoch:11 step:8772 [D loss: 0.337055, acc.: 82.03%] [G loss: 2.684946]\n",
      "epoch:11 step:8773 [D loss: 0.333365, acc.: 86.72%] [G loss: 2.604198]\n",
      "epoch:11 step:8774 [D loss: 0.272263, acc.: 89.84%] [G loss: 3.679188]\n",
      "epoch:11 step:8775 [D loss: 0.398392, acc.: 85.16%] [G loss: 2.814304]\n",
      "epoch:11 step:8776 [D loss: 0.282783, acc.: 89.84%] [G loss: 2.599983]\n",
      "epoch:11 step:8777 [D loss: 0.379727, acc.: 83.59%] [G loss: 2.394851]\n",
      "epoch:11 step:8778 [D loss: 0.363555, acc.: 85.94%] [G loss: 2.166002]\n",
      "epoch:11 step:8779 [D loss: 0.338328, acc.: 89.84%] [G loss: 1.947657]\n",
      "epoch:11 step:8780 [D loss: 0.335510, acc.: 86.72%] [G loss: 2.456363]\n",
      "epoch:11 step:8781 [D loss: 0.293892, acc.: 89.84%] [G loss: 2.286798]\n",
      "epoch:11 step:8782 [D loss: 0.347104, acc.: 85.16%] [G loss: 2.787024]\n",
      "epoch:11 step:8783 [D loss: 0.210657, acc.: 92.19%] [G loss: 4.019733]\n",
      "epoch:11 step:8784 [D loss: 0.234443, acc.: 89.84%] [G loss: 5.264139]\n",
      "epoch:11 step:8785 [D loss: 0.331901, acc.: 85.94%] [G loss: 3.595726]\n",
      "epoch:11 step:8786 [D loss: 0.359023, acc.: 86.72%] [G loss: 2.410988]\n",
      "epoch:11 step:8787 [D loss: 0.355721, acc.: 85.94%] [G loss: 2.379719]\n",
      "epoch:11 step:8788 [D loss: 0.349400, acc.: 85.94%] [G loss: 3.099496]\n",
      "epoch:11 step:8789 [D loss: 0.431293, acc.: 74.22%] [G loss: 4.038947]\n",
      "epoch:11 step:8790 [D loss: 0.350635, acc.: 81.25%] [G loss: 5.556799]\n",
      "epoch:11 step:8791 [D loss: 0.262732, acc.: 88.28%] [G loss: 5.461418]\n",
      "epoch:11 step:8792 [D loss: 0.271138, acc.: 87.50%] [G loss: 2.604894]\n",
      "epoch:11 step:8793 [D loss: 0.250613, acc.: 90.62%] [G loss: 4.713305]\n",
      "epoch:11 step:8794 [D loss: 0.216373, acc.: 89.06%] [G loss: 6.021807]\n",
      "epoch:11 step:8795 [D loss: 0.194602, acc.: 92.97%] [G loss: 3.507070]\n",
      "epoch:11 step:8796 [D loss: 0.194170, acc.: 94.53%] [G loss: 4.477746]\n",
      "epoch:11 step:8797 [D loss: 0.280248, acc.: 87.50%] [G loss: 3.216521]\n",
      "epoch:11 step:8798 [D loss: 0.332069, acc.: 83.59%] [G loss: 4.399065]\n",
      "epoch:11 step:8799 [D loss: 0.342365, acc.: 85.94%] [G loss: 2.786938]\n",
      "epoch:11 step:8800 [D loss: 0.300867, acc.: 89.84%] [G loss: 2.716552]\n",
      "epoch:11 step:8801 [D loss: 0.294725, acc.: 90.62%] [G loss: 2.483597]\n",
      "epoch:11 step:8802 [D loss: 0.313377, acc.: 85.16%] [G loss: 2.869502]\n",
      "epoch:11 step:8803 [D loss: 0.309215, acc.: 84.38%] [G loss: 2.699445]\n",
      "epoch:11 step:8804 [D loss: 0.322484, acc.: 86.72%] [G loss: 3.389615]\n",
      "epoch:11 step:8805 [D loss: 0.357896, acc.: 86.72%] [G loss: 2.664441]\n",
      "epoch:11 step:8806 [D loss: 0.343480, acc.: 84.38%] [G loss: 2.499419]\n",
      "epoch:11 step:8807 [D loss: 0.363603, acc.: 82.03%] [G loss: 3.550044]\n",
      "epoch:11 step:8808 [D loss: 0.291018, acc.: 85.16%] [G loss: 4.171906]\n",
      "epoch:11 step:8809 [D loss: 0.409966, acc.: 82.81%] [G loss: 2.527271]\n",
      "epoch:11 step:8810 [D loss: 0.244037, acc.: 94.53%] [G loss: 2.200449]\n",
      "epoch:11 step:8811 [D loss: 0.240058, acc.: 92.19%] [G loss: 5.651710]\n",
      "epoch:11 step:8812 [D loss: 0.243018, acc.: 87.50%] [G loss: 3.392952]\n",
      "epoch:11 step:8813 [D loss: 0.278211, acc.: 89.06%] [G loss: 3.318729]\n",
      "epoch:11 step:8814 [D loss: 0.468286, acc.: 75.78%] [G loss: 2.742898]\n",
      "epoch:11 step:8815 [D loss: 0.387513, acc.: 82.81%] [G loss: 2.119358]\n",
      "epoch:11 step:8816 [D loss: 0.395512, acc.: 82.81%] [G loss: 3.250955]\n",
      "epoch:11 step:8817 [D loss: 0.332038, acc.: 85.94%] [G loss: 2.745733]\n",
      "epoch:11 step:8818 [D loss: 0.294839, acc.: 86.72%] [G loss: 3.473632]\n",
      "epoch:11 step:8819 [D loss: 0.312941, acc.: 87.50%] [G loss: 3.860291]\n",
      "epoch:11 step:8820 [D loss: 0.311028, acc.: 84.38%] [G loss: 4.053303]\n",
      "epoch:11 step:8821 [D loss: 0.355476, acc.: 84.38%] [G loss: 3.874136]\n",
      "epoch:11 step:8822 [D loss: 0.344954, acc.: 85.16%] [G loss: 2.970143]\n",
      "epoch:11 step:8823 [D loss: 0.352240, acc.: 85.16%] [G loss: 3.478365]\n",
      "epoch:11 step:8824 [D loss: 0.394934, acc.: 82.81%] [G loss: 3.701766]\n",
      "epoch:11 step:8825 [D loss: 0.323866, acc.: 85.94%] [G loss: 2.992548]\n",
      "epoch:11 step:8826 [D loss: 0.244035, acc.: 89.06%] [G loss: 3.488942]\n",
      "epoch:11 step:8827 [D loss: 0.291207, acc.: 87.50%] [G loss: 3.503929]\n",
      "epoch:11 step:8828 [D loss: 0.310293, acc.: 85.94%] [G loss: 2.564933]\n",
      "epoch:11 step:8829 [D loss: 0.353306, acc.: 84.38%] [G loss: 3.858107]\n",
      "epoch:11 step:8830 [D loss: 0.347822, acc.: 85.16%] [G loss: 4.679883]\n",
      "epoch:11 step:8831 [D loss: 0.550489, acc.: 72.66%] [G loss: 3.533458]\n",
      "epoch:11 step:8832 [D loss: 0.584005, acc.: 78.91%] [G loss: 3.921522]\n",
      "epoch:11 step:8833 [D loss: 0.503414, acc.: 77.34%] [G loss: 2.391492]\n",
      "epoch:11 step:8834 [D loss: 0.304412, acc.: 87.50%] [G loss: 3.591225]\n",
      "epoch:11 step:8835 [D loss: 0.331936, acc.: 85.16%] [G loss: 3.645440]\n",
      "epoch:11 step:8836 [D loss: 0.209902, acc.: 92.97%] [G loss: 2.794678]\n",
      "epoch:11 step:8837 [D loss: 0.326308, acc.: 85.16%] [G loss: 3.872584]\n",
      "epoch:11 step:8838 [D loss: 0.240018, acc.: 92.97%] [G loss: 4.276972]\n",
      "epoch:11 step:8839 [D loss: 0.251818, acc.: 90.62%] [G loss: 3.810958]\n",
      "epoch:11 step:8840 [D loss: 0.280542, acc.: 86.72%] [G loss: 3.026285]\n",
      "epoch:11 step:8841 [D loss: 0.366055, acc.: 81.25%] [G loss: 3.495025]\n",
      "epoch:11 step:8842 [D loss: 0.256404, acc.: 87.50%] [G loss: 3.955614]\n",
      "epoch:11 step:8843 [D loss: 0.254368, acc.: 91.41%] [G loss: 2.714037]\n",
      "epoch:11 step:8844 [D loss: 0.283075, acc.: 85.94%] [G loss: 3.869790]\n",
      "epoch:11 step:8845 [D loss: 0.204644, acc.: 89.84%] [G loss: 4.335776]\n",
      "epoch:11 step:8846 [D loss: 0.337778, acc.: 83.59%] [G loss: 3.817557]\n",
      "epoch:11 step:8847 [D loss: 0.353001, acc.: 82.81%] [G loss: 3.155177]\n",
      "epoch:11 step:8848 [D loss: 0.302349, acc.: 89.06%] [G loss: 4.180138]\n",
      "epoch:11 step:8849 [D loss: 0.365546, acc.: 87.50%] [G loss: 5.211308]\n",
      "epoch:11 step:8850 [D loss: 0.339354, acc.: 86.72%] [G loss: 3.646872]\n",
      "epoch:11 step:8851 [D loss: 0.319925, acc.: 83.59%] [G loss: 3.339222]\n",
      "epoch:11 step:8852 [D loss: 0.372297, acc.: 82.03%] [G loss: 4.041580]\n",
      "epoch:11 step:8853 [D loss: 0.326178, acc.: 85.94%] [G loss: 3.174176]\n",
      "epoch:11 step:8854 [D loss: 0.282898, acc.: 89.84%] [G loss: 3.227346]\n",
      "epoch:11 step:8855 [D loss: 0.284897, acc.: 87.50%] [G loss: 3.162017]\n",
      "epoch:11 step:8856 [D loss: 0.395222, acc.: 85.16%] [G loss: 2.317697]\n",
      "epoch:11 step:8857 [D loss: 0.303493, acc.: 87.50%] [G loss: 4.859233]\n",
      "epoch:11 step:8858 [D loss: 0.409004, acc.: 79.69%] [G loss: 3.260921]\n",
      "epoch:11 step:8859 [D loss: 0.326355, acc.: 89.84%] [G loss: 2.718853]\n",
      "epoch:11 step:8860 [D loss: 0.250307, acc.: 90.62%] [G loss: 2.872613]\n",
      "epoch:11 step:8861 [D loss: 0.274138, acc.: 87.50%] [G loss: 3.761672]\n",
      "epoch:11 step:8862 [D loss: 0.286227, acc.: 88.28%] [G loss: 2.829333]\n",
      "epoch:11 step:8863 [D loss: 0.262256, acc.: 89.06%] [G loss: 3.343518]\n",
      "epoch:11 step:8864 [D loss: 0.257701, acc.: 89.84%] [G loss: 2.915585]\n",
      "epoch:11 step:8865 [D loss: 0.262725, acc.: 91.41%] [G loss: 3.864454]\n",
      "epoch:11 step:8866 [D loss: 0.342781, acc.: 84.38%] [G loss: 4.007935]\n",
      "epoch:11 step:8867 [D loss: 0.339930, acc.: 85.94%] [G loss: 3.775426]\n",
      "epoch:11 step:8868 [D loss: 0.239792, acc.: 89.84%] [G loss: 3.366860]\n",
      "epoch:11 step:8869 [D loss: 0.236809, acc.: 93.75%] [G loss: 3.074530]\n",
      "epoch:11 step:8870 [D loss: 0.261881, acc.: 87.50%] [G loss: 3.290267]\n",
      "epoch:11 step:8871 [D loss: 0.320164, acc.: 88.28%] [G loss: 3.650594]\n",
      "epoch:11 step:8872 [D loss: 0.265812, acc.: 89.06%] [G loss: 3.295251]\n",
      "epoch:11 step:8873 [D loss: 0.340379, acc.: 82.03%] [G loss: 3.476453]\n",
      "epoch:11 step:8874 [D loss: 0.297116, acc.: 88.28%] [G loss: 2.637884]\n",
      "epoch:11 step:8875 [D loss: 0.254137, acc.: 92.19%] [G loss: 2.884553]\n",
      "epoch:11 step:8876 [D loss: 0.302749, acc.: 84.38%] [G loss: 2.414907]\n",
      "epoch:11 step:8877 [D loss: 0.341689, acc.: 86.72%] [G loss: 2.330871]\n",
      "epoch:11 step:8878 [D loss: 0.240926, acc.: 89.84%] [G loss: 3.904604]\n",
      "epoch:11 step:8879 [D loss: 0.492804, acc.: 78.12%] [G loss: 2.936599]\n",
      "epoch:11 step:8880 [D loss: 0.351773, acc.: 85.94%] [G loss: 3.271917]\n",
      "epoch:11 step:8881 [D loss: 0.327585, acc.: 86.72%] [G loss: 3.560245]\n",
      "epoch:11 step:8882 [D loss: 0.361956, acc.: 85.16%] [G loss: 3.764549]\n",
      "epoch:11 step:8883 [D loss: 0.277279, acc.: 89.06%] [G loss: 3.375751]\n",
      "epoch:11 step:8884 [D loss: 0.325571, acc.: 83.59%] [G loss: 3.030359]\n",
      "epoch:11 step:8885 [D loss: 0.333389, acc.: 87.50%] [G loss: 2.609347]\n",
      "epoch:11 step:8886 [D loss: 0.252799, acc.: 91.41%] [G loss: 4.800332]\n",
      "epoch:11 step:8887 [D loss: 0.300404, acc.: 85.16%] [G loss: 3.756050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8888 [D loss: 0.350161, acc.: 86.72%] [G loss: 2.967868]\n",
      "epoch:11 step:8889 [D loss: 0.267525, acc.: 93.75%] [G loss: 2.840099]\n",
      "epoch:11 step:8890 [D loss: 0.366492, acc.: 83.59%] [G loss: 2.741113]\n",
      "epoch:11 step:8891 [D loss: 0.359101, acc.: 82.81%] [G loss: 2.503314]\n",
      "epoch:11 step:8892 [D loss: 0.443338, acc.: 79.69%] [G loss: 2.710693]\n",
      "epoch:11 step:8893 [D loss: 0.396113, acc.: 84.38%] [G loss: 3.530402]\n",
      "epoch:11 step:8894 [D loss: 0.230602, acc.: 91.41%] [G loss: 4.057663]\n",
      "epoch:11 step:8895 [D loss: 0.357751, acc.: 81.25%] [G loss: 4.051643]\n",
      "epoch:11 step:8896 [D loss: 0.362789, acc.: 84.38%] [G loss: 2.624782]\n",
      "epoch:11 step:8897 [D loss: 0.313263, acc.: 86.72%] [G loss: 3.313100]\n",
      "epoch:11 step:8898 [D loss: 0.376013, acc.: 81.25%] [G loss: 2.554917]\n",
      "epoch:11 step:8899 [D loss: 0.371799, acc.: 80.47%] [G loss: 2.508258]\n",
      "epoch:11 step:8900 [D loss: 0.314109, acc.: 85.16%] [G loss: 3.739838]\n",
      "epoch:11 step:8901 [D loss: 0.292003, acc.: 87.50%] [G loss: 3.359821]\n",
      "epoch:11 step:8902 [D loss: 0.375464, acc.: 83.59%] [G loss: 3.029712]\n",
      "epoch:11 step:8903 [D loss: 0.259162, acc.: 90.62%] [G loss: 2.318295]\n",
      "epoch:11 step:8904 [D loss: 0.357784, acc.: 83.59%] [G loss: 2.776419]\n",
      "epoch:11 step:8905 [D loss: 0.241206, acc.: 91.41%] [G loss: 3.538979]\n",
      "epoch:11 step:8906 [D loss: 0.321891, acc.: 90.62%] [G loss: 2.319720]\n",
      "epoch:11 step:8907 [D loss: 0.323118, acc.: 87.50%] [G loss: 2.299630]\n",
      "epoch:11 step:8908 [D loss: 0.439086, acc.: 80.47%] [G loss: 3.844743]\n",
      "epoch:11 step:8909 [D loss: 0.541795, acc.: 81.25%] [G loss: 4.504058]\n",
      "epoch:11 step:8910 [D loss: 0.710125, acc.: 71.09%] [G loss: 1.575558]\n",
      "epoch:11 step:8911 [D loss: 0.510529, acc.: 72.66%] [G loss: 2.462626]\n",
      "epoch:11 step:8912 [D loss: 0.316623, acc.: 85.94%] [G loss: 3.536430]\n",
      "epoch:11 step:8913 [D loss: 0.411999, acc.: 82.03%] [G loss: 2.786372]\n",
      "epoch:11 step:8914 [D loss: 0.302101, acc.: 87.50%] [G loss: 2.873095]\n",
      "epoch:11 step:8915 [D loss: 0.376600, acc.: 84.38%] [G loss: 2.475597]\n",
      "epoch:11 step:8916 [D loss: 0.438385, acc.: 79.69%] [G loss: 3.315054]\n",
      "epoch:11 step:8917 [D loss: 0.529964, acc.: 78.91%] [G loss: 2.551969]\n",
      "epoch:11 step:8918 [D loss: 0.380875, acc.: 84.38%] [G loss: 2.351141]\n",
      "epoch:11 step:8919 [D loss: 0.314428, acc.: 89.84%] [G loss: 2.532617]\n",
      "epoch:11 step:8920 [D loss: 0.339428, acc.: 85.16%] [G loss: 2.589190]\n",
      "epoch:11 step:8921 [D loss: 0.347129, acc.: 82.81%] [G loss: 2.739162]\n",
      "epoch:11 step:8922 [D loss: 0.372575, acc.: 87.50%] [G loss: 2.568141]\n",
      "epoch:11 step:8923 [D loss: 0.389600, acc.: 82.03%] [G loss: 4.389773]\n",
      "epoch:11 step:8924 [D loss: 0.422236, acc.: 78.12%] [G loss: 3.391258]\n",
      "epoch:11 step:8925 [D loss: 0.349865, acc.: 83.59%] [G loss: 2.977243]\n",
      "epoch:11 step:8926 [D loss: 0.347651, acc.: 85.94%] [G loss: 2.269325]\n",
      "epoch:11 step:8927 [D loss: 0.339542, acc.: 88.28%] [G loss: 2.740710]\n",
      "epoch:11 step:8928 [D loss: 0.285768, acc.: 88.28%] [G loss: 2.823103]\n",
      "epoch:11 step:8929 [D loss: 0.406079, acc.: 82.03%] [G loss: 3.154885]\n",
      "epoch:11 step:8930 [D loss: 0.319695, acc.: 85.16%] [G loss: 2.782395]\n",
      "epoch:11 step:8931 [D loss: 0.421996, acc.: 83.59%] [G loss: 3.992048]\n",
      "epoch:11 step:8932 [D loss: 0.456391, acc.: 78.91%] [G loss: 2.148152]\n",
      "epoch:11 step:8933 [D loss: 0.326768, acc.: 82.81%] [G loss: 3.993166]\n",
      "epoch:11 step:8934 [D loss: 0.322871, acc.: 86.72%] [G loss: 3.547626]\n",
      "epoch:11 step:8935 [D loss: 0.342767, acc.: 82.03%] [G loss: 5.769958]\n",
      "epoch:11 step:8936 [D loss: 0.267622, acc.: 89.06%] [G loss: 2.967927]\n",
      "epoch:11 step:8937 [D loss: 0.389230, acc.: 78.91%] [G loss: 4.008858]\n",
      "epoch:11 step:8938 [D loss: 0.303831, acc.: 88.28%] [G loss: 4.591865]\n",
      "epoch:11 step:8939 [D loss: 0.289725, acc.: 85.94%] [G loss: 4.706965]\n",
      "epoch:11 step:8940 [D loss: 0.225121, acc.: 90.62%] [G loss: 4.805315]\n",
      "epoch:11 step:8941 [D loss: 0.270466, acc.: 88.28%] [G loss: 5.488905]\n",
      "epoch:11 step:8942 [D loss: 0.378982, acc.: 83.59%] [G loss: 3.671533]\n",
      "epoch:11 step:8943 [D loss: 0.362270, acc.: 83.59%] [G loss: 3.828852]\n",
      "epoch:11 step:8944 [D loss: 0.354028, acc.: 80.47%] [G loss: 3.049397]\n",
      "epoch:11 step:8945 [D loss: 0.369491, acc.: 85.16%] [G loss: 2.887644]\n",
      "epoch:11 step:8946 [D loss: 0.231192, acc.: 91.41%] [G loss: 2.873864]\n",
      "epoch:11 step:8947 [D loss: 0.234658, acc.: 90.62%] [G loss: 3.071619]\n",
      "epoch:11 step:8948 [D loss: 0.224900, acc.: 91.41%] [G loss: 4.078833]\n",
      "epoch:11 step:8949 [D loss: 0.384971, acc.: 83.59%] [G loss: 3.108467]\n",
      "epoch:11 step:8950 [D loss: 0.325248, acc.: 81.25%] [G loss: 3.997033]\n",
      "epoch:11 step:8951 [D loss: 0.320514, acc.: 84.38%] [G loss: 3.384936]\n",
      "epoch:11 step:8952 [D loss: 0.335846, acc.: 85.16%] [G loss: 3.729912]\n",
      "epoch:11 step:8953 [D loss: 0.282063, acc.: 84.38%] [G loss: 3.463736]\n",
      "epoch:11 step:8954 [D loss: 0.284861, acc.: 87.50%] [G loss: 2.466636]\n",
      "epoch:11 step:8955 [D loss: 0.460112, acc.: 77.34%] [G loss: 2.607487]\n",
      "epoch:11 step:8956 [D loss: 0.452266, acc.: 81.25%] [G loss: 2.438552]\n",
      "epoch:11 step:8957 [D loss: 0.380804, acc.: 87.50%] [G loss: 2.554869]\n",
      "epoch:11 step:8958 [D loss: 0.384673, acc.: 83.59%] [G loss: 2.374458]\n",
      "epoch:11 step:8959 [D loss: 0.468675, acc.: 76.56%] [G loss: 3.140586]\n",
      "epoch:11 step:8960 [D loss: 0.376429, acc.: 81.25%] [G loss: 2.928090]\n",
      "epoch:11 step:8961 [D loss: 0.394180, acc.: 75.78%] [G loss: 4.391097]\n",
      "epoch:11 step:8962 [D loss: 0.358529, acc.: 83.59%] [G loss: 2.506379]\n",
      "epoch:11 step:8963 [D loss: 0.292531, acc.: 88.28%] [G loss: 2.329085]\n",
      "epoch:11 step:8964 [D loss: 0.283827, acc.: 89.06%] [G loss: 3.502984]\n",
      "epoch:11 step:8965 [D loss: 0.286682, acc.: 88.28%] [G loss: 4.178755]\n",
      "epoch:11 step:8966 [D loss: 0.289750, acc.: 89.06%] [G loss: 3.460751]\n",
      "epoch:11 step:8967 [D loss: 0.342852, acc.: 87.50%] [G loss: 2.680617]\n",
      "epoch:11 step:8968 [D loss: 0.361229, acc.: 86.72%] [G loss: 2.766458]\n",
      "epoch:11 step:8969 [D loss: 0.316204, acc.: 85.16%] [G loss: 3.527239]\n",
      "epoch:11 step:8970 [D loss: 0.294853, acc.: 86.72%] [G loss: 2.642830]\n",
      "epoch:11 step:8971 [D loss: 0.308827, acc.: 87.50%] [G loss: 3.100966]\n",
      "epoch:11 step:8972 [D loss: 0.236688, acc.: 88.28%] [G loss: 2.902340]\n",
      "epoch:11 step:8973 [D loss: 0.324237, acc.: 85.16%] [G loss: 2.492368]\n",
      "epoch:11 step:8974 [D loss: 0.337302, acc.: 83.59%] [G loss: 2.609899]\n",
      "epoch:11 step:8975 [D loss: 0.252839, acc.: 89.06%] [G loss: 4.564241]\n",
      "epoch:11 step:8976 [D loss: 0.168819, acc.: 94.53%] [G loss: 4.566817]\n",
      "epoch:11 step:8977 [D loss: 0.320366, acc.: 87.50%] [G loss: 2.821167]\n",
      "epoch:11 step:8978 [D loss: 0.276814, acc.: 87.50%] [G loss: 3.213412]\n",
      "epoch:11 step:8979 [D loss: 0.304256, acc.: 84.38%] [G loss: 4.188449]\n",
      "epoch:11 step:8980 [D loss: 0.250847, acc.: 89.06%] [G loss: 3.374762]\n",
      "epoch:11 step:8981 [D loss: 0.306839, acc.: 89.84%] [G loss: 2.830909]\n",
      "epoch:11 step:8982 [D loss: 0.280211, acc.: 89.06%] [G loss: 2.344552]\n",
      "epoch:11 step:8983 [D loss: 0.280025, acc.: 88.28%] [G loss: 2.550676]\n",
      "epoch:11 step:8984 [D loss: 0.303767, acc.: 92.19%] [G loss: 2.593925]\n",
      "epoch:11 step:8985 [D loss: 0.236905, acc.: 93.75%] [G loss: 3.238047]\n",
      "epoch:11 step:8986 [D loss: 0.312333, acc.: 84.38%] [G loss: 3.310525]\n",
      "epoch:11 step:8987 [D loss: 0.283793, acc.: 88.28%] [G loss: 3.266681]\n",
      "epoch:11 step:8988 [D loss: 0.231285, acc.: 91.41%] [G loss: 6.016816]\n",
      "epoch:11 step:8989 [D loss: 0.328381, acc.: 84.38%] [G loss: 3.505964]\n",
      "epoch:11 step:8990 [D loss: 0.405637, acc.: 80.47%] [G loss: 2.892979]\n",
      "epoch:11 step:8991 [D loss: 0.315525, acc.: 85.16%] [G loss: 3.375411]\n",
      "epoch:11 step:8992 [D loss: 0.390799, acc.: 85.16%] [G loss: 3.292630]\n",
      "epoch:11 step:8993 [D loss: 0.342210, acc.: 85.16%] [G loss: 3.647149]\n",
      "epoch:11 step:8994 [D loss: 0.305665, acc.: 89.06%] [G loss: 2.342786]\n",
      "epoch:11 step:8995 [D loss: 0.335792, acc.: 88.28%] [G loss: 2.707208]\n",
      "epoch:11 step:8996 [D loss: 0.351905, acc.: 82.81%] [G loss: 3.046115]\n",
      "epoch:11 step:8997 [D loss: 0.320626, acc.: 87.50%] [G loss: 3.020298]\n",
      "epoch:11 step:8998 [D loss: 0.325095, acc.: 85.16%] [G loss: 3.281404]\n",
      "epoch:11 step:8999 [D loss: 0.335867, acc.: 84.38%] [G loss: 3.249887]\n",
      "epoch:11 step:9000 [D loss: 0.326349, acc.: 85.16%] [G loss: 2.459460]\n",
      "epoch:11 step:9001 [D loss: 0.269379, acc.: 88.28%] [G loss: 2.795301]\n",
      "epoch:11 step:9002 [D loss: 0.321934, acc.: 90.62%] [G loss: 2.708722]\n",
      "epoch:11 step:9003 [D loss: 0.361662, acc.: 86.72%] [G loss: 2.864766]\n",
      "epoch:11 step:9004 [D loss: 0.295865, acc.: 85.94%] [G loss: 2.821021]\n",
      "epoch:11 step:9005 [D loss: 0.328521, acc.: 88.28%] [G loss: 2.956095]\n",
      "epoch:11 step:9006 [D loss: 0.277321, acc.: 87.50%] [G loss: 3.098302]\n",
      "epoch:11 step:9007 [D loss: 0.221420, acc.: 94.53%] [G loss: 2.660588]\n",
      "epoch:11 step:9008 [D loss: 0.290422, acc.: 89.06%] [G loss: 4.300383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:9009 [D loss: 0.279338, acc.: 85.16%] [G loss: 4.474042]\n",
      "epoch:11 step:9010 [D loss: 0.391073, acc.: 78.91%] [G loss: 3.401322]\n",
      "epoch:11 step:9011 [D loss: 0.236360, acc.: 90.62%] [G loss: 3.229668]\n",
      "epoch:11 step:9012 [D loss: 0.310366, acc.: 86.72%] [G loss: 3.884399]\n",
      "epoch:11 step:9013 [D loss: 0.394756, acc.: 84.38%] [G loss: 3.686291]\n",
      "epoch:11 step:9014 [D loss: 0.301751, acc.: 85.16%] [G loss: 4.057801]\n",
      "epoch:11 step:9015 [D loss: 0.338359, acc.: 86.72%] [G loss: 3.117934]\n",
      "epoch:11 step:9016 [D loss: 0.346399, acc.: 85.16%] [G loss: 2.569805]\n",
      "epoch:11 step:9017 [D loss: 0.216376, acc.: 92.97%] [G loss: 2.677372]\n",
      "epoch:11 step:9018 [D loss: 0.238433, acc.: 89.84%] [G loss: 2.674919]\n",
      "epoch:11 step:9019 [D loss: 0.400345, acc.: 82.03%] [G loss: 2.898486]\n",
      "epoch:11 step:9020 [D loss: 0.357635, acc.: 85.94%] [G loss: 2.404791]\n",
      "epoch:11 step:9021 [D loss: 0.317040, acc.: 87.50%] [G loss: 2.284434]\n",
      "epoch:11 step:9022 [D loss: 0.264091, acc.: 89.84%] [G loss: 2.999164]\n",
      "epoch:11 step:9023 [D loss: 0.382022, acc.: 85.16%] [G loss: 3.909294]\n",
      "epoch:11 step:9024 [D loss: 0.284822, acc.: 86.72%] [G loss: 3.922588]\n",
      "epoch:11 step:9025 [D loss: 0.279261, acc.: 87.50%] [G loss: 4.433240]\n",
      "epoch:11 step:9026 [D loss: 0.321081, acc.: 87.50%] [G loss: 6.055356]\n",
      "epoch:11 step:9027 [D loss: 0.447380, acc.: 79.69%] [G loss: 2.877562]\n",
      "epoch:11 step:9028 [D loss: 0.586435, acc.: 79.69%] [G loss: 4.075010]\n",
      "epoch:11 step:9029 [D loss: 0.567686, acc.: 78.91%] [G loss: 8.324991]\n",
      "epoch:11 step:9030 [D loss: 0.832131, acc.: 66.41%] [G loss: 4.391074]\n",
      "epoch:11 step:9031 [D loss: 0.554793, acc.: 83.59%] [G loss: 7.288951]\n",
      "epoch:11 step:9032 [D loss: 0.329689, acc.: 82.81%] [G loss: 4.294361]\n",
      "epoch:11 step:9033 [D loss: 0.196482, acc.: 92.19%] [G loss: 4.790051]\n",
      "epoch:11 step:9034 [D loss: 0.341916, acc.: 82.81%] [G loss: 3.483099]\n",
      "epoch:11 step:9035 [D loss: 0.340031, acc.: 84.38%] [G loss: 2.170006]\n",
      "epoch:11 step:9036 [D loss: 0.352460, acc.: 83.59%] [G loss: 2.987287]\n",
      "epoch:11 step:9037 [D loss: 0.198071, acc.: 92.97%] [G loss: 3.179407]\n",
      "epoch:11 step:9038 [D loss: 0.304845, acc.: 87.50%] [G loss: 2.422201]\n",
      "epoch:11 step:9039 [D loss: 0.282470, acc.: 90.62%] [G loss: 2.767155]\n",
      "epoch:11 step:9040 [D loss: 0.306785, acc.: 87.50%] [G loss: 2.926795]\n",
      "epoch:11 step:9041 [D loss: 0.256090, acc.: 88.28%] [G loss: 3.278506]\n",
      "epoch:11 step:9042 [D loss: 0.283067, acc.: 89.06%] [G loss: 2.797851]\n",
      "epoch:11 step:9043 [D loss: 0.322122, acc.: 85.16%] [G loss: 3.084531]\n",
      "epoch:11 step:9044 [D loss: 0.216697, acc.: 95.31%] [G loss: 3.155341]\n",
      "epoch:11 step:9045 [D loss: 0.435661, acc.: 81.25%] [G loss: 2.601537]\n",
      "epoch:11 step:9046 [D loss: 0.440021, acc.: 79.69%] [G loss: 2.389550]\n",
      "epoch:11 step:9047 [D loss: 0.348954, acc.: 86.72%] [G loss: 2.395973]\n",
      "epoch:11 step:9048 [D loss: 0.318380, acc.: 87.50%] [G loss: 2.716392]\n",
      "epoch:11 step:9049 [D loss: 0.180212, acc.: 96.09%] [G loss: 3.119395]\n",
      "epoch:11 step:9050 [D loss: 0.367977, acc.: 85.94%] [G loss: 2.983549]\n",
      "epoch:11 step:9051 [D loss: 0.356526, acc.: 82.81%] [G loss: 2.763591]\n",
      "epoch:11 step:9052 [D loss: 0.326890, acc.: 88.28%] [G loss: 2.636929]\n",
      "epoch:11 step:9053 [D loss: 0.290614, acc.: 85.94%] [G loss: 3.158290]\n",
      "epoch:11 step:9054 [D loss: 0.227623, acc.: 88.28%] [G loss: 3.788931]\n",
      "epoch:11 step:9055 [D loss: 0.362709, acc.: 87.50%] [G loss: 3.568331]\n",
      "epoch:11 step:9056 [D loss: 0.309072, acc.: 89.06%] [G loss: 3.825157]\n",
      "epoch:11 step:9057 [D loss: 0.386371, acc.: 81.25%] [G loss: 2.619752]\n",
      "epoch:11 step:9058 [D loss: 0.296965, acc.: 87.50%] [G loss: 4.057998]\n",
      "epoch:11 step:9059 [D loss: 0.338895, acc.: 85.16%] [G loss: 2.655675]\n",
      "epoch:11 step:9060 [D loss: 0.260598, acc.: 88.28%] [G loss: 3.170068]\n",
      "epoch:11 step:9061 [D loss: 0.312663, acc.: 86.72%] [G loss: 3.193960]\n",
      "epoch:11 step:9062 [D loss: 0.275508, acc.: 89.06%] [G loss: 3.079352]\n",
      "epoch:11 step:9063 [D loss: 0.415887, acc.: 82.81%] [G loss: 2.056352]\n",
      "epoch:11 step:9064 [D loss: 0.316384, acc.: 87.50%] [G loss: 3.271300]\n",
      "epoch:11 step:9065 [D loss: 0.212682, acc.: 91.41%] [G loss: 6.097427]\n",
      "epoch:11 step:9066 [D loss: 0.209654, acc.: 92.19%] [G loss: 2.396837]\n",
      "epoch:11 step:9067 [D loss: 0.204324, acc.: 93.75%] [G loss: 6.099679]\n",
      "epoch:11 step:9068 [D loss: 0.295059, acc.: 86.72%] [G loss: 2.417887]\n",
      "epoch:11 step:9069 [D loss: 0.196234, acc.: 93.75%] [G loss: 3.135905]\n",
      "epoch:11 step:9070 [D loss: 0.300991, acc.: 89.84%] [G loss: 2.783690]\n",
      "epoch:11 step:9071 [D loss: 0.273835, acc.: 86.72%] [G loss: 2.319114]\n",
      "epoch:11 step:9072 [D loss: 0.398782, acc.: 82.81%] [G loss: 2.544468]\n",
      "epoch:11 step:9073 [D loss: 0.320834, acc.: 85.94%] [G loss: 2.872324]\n",
      "epoch:11 step:9074 [D loss: 0.302784, acc.: 85.16%] [G loss: 2.487117]\n",
      "epoch:11 step:9075 [D loss: 0.380496, acc.: 86.72%] [G loss: 2.719038]\n",
      "epoch:11 step:9076 [D loss: 0.363759, acc.: 82.03%] [G loss: 3.059697]\n",
      "epoch:11 step:9077 [D loss: 0.307594, acc.: 87.50%] [G loss: 2.834809]\n",
      "epoch:11 step:9078 [D loss: 0.269511, acc.: 88.28%] [G loss: 2.795765]\n",
      "epoch:11 step:9079 [D loss: 0.264937, acc.: 87.50%] [G loss: 2.884803]\n",
      "epoch:11 step:9080 [D loss: 0.232334, acc.: 91.41%] [G loss: 3.039592]\n",
      "epoch:11 step:9081 [D loss: 0.301260, acc.: 87.50%] [G loss: 3.127848]\n",
      "epoch:11 step:9082 [D loss: 0.212098, acc.: 96.09%] [G loss: 3.019285]\n",
      "epoch:11 step:9083 [D loss: 0.321122, acc.: 89.84%] [G loss: 3.027518]\n",
      "epoch:11 step:9084 [D loss: 0.240813, acc.: 92.19%] [G loss: 2.816757]\n",
      "epoch:11 step:9085 [D loss: 0.257105, acc.: 90.62%] [G loss: 2.843355]\n",
      "epoch:11 step:9086 [D loss: 0.331897, acc.: 87.50%] [G loss: 2.008040]\n",
      "epoch:11 step:9087 [D loss: 0.287147, acc.: 87.50%] [G loss: 3.111795]\n",
      "epoch:11 step:9088 [D loss: 0.234206, acc.: 90.62%] [G loss: 3.975244]\n",
      "epoch:11 step:9089 [D loss: 0.318887, acc.: 89.06%] [G loss: 2.722283]\n",
      "epoch:11 step:9090 [D loss: 0.271411, acc.: 88.28%] [G loss: 2.631152]\n",
      "epoch:11 step:9091 [D loss: 0.471263, acc.: 79.69%] [G loss: 3.647187]\n",
      "epoch:11 step:9092 [D loss: 0.478857, acc.: 79.69%] [G loss: 6.935699]\n",
      "epoch:11 step:9093 [D loss: 0.798724, acc.: 67.97%] [G loss: 6.359323]\n",
      "epoch:11 step:9094 [D loss: 1.664351, acc.: 63.28%] [G loss: 6.078382]\n",
      "epoch:11 step:9095 [D loss: 1.288052, acc.: 62.50%] [G loss: 7.760814]\n",
      "epoch:11 step:9096 [D loss: 2.403820, acc.: 39.06%] [G loss: 2.106812]\n",
      "epoch:11 step:9097 [D loss: 0.482705, acc.: 86.72%] [G loss: 3.502114]\n",
      "epoch:11 step:9098 [D loss: 0.348910, acc.: 83.59%] [G loss: 3.011470]\n",
      "epoch:11 step:9099 [D loss: 0.456241, acc.: 82.03%] [G loss: 3.876308]\n",
      "epoch:11 step:9100 [D loss: 0.445459, acc.: 82.81%] [G loss: 3.583743]\n",
      "epoch:11 step:9101 [D loss: 0.451804, acc.: 80.47%] [G loss: 2.544128]\n",
      "epoch:11 step:9102 [D loss: 0.346848, acc.: 88.28%] [G loss: 3.896329]\n",
      "epoch:11 step:9103 [D loss: 0.322554, acc.: 87.50%] [G loss: 3.172230]\n",
      "epoch:11 step:9104 [D loss: 0.461908, acc.: 78.12%] [G loss: 3.303397]\n",
      "epoch:11 step:9105 [D loss: 0.314166, acc.: 88.28%] [G loss: 3.267680]\n",
      "epoch:11 step:9106 [D loss: 0.278095, acc.: 89.84%] [G loss: 3.689482]\n",
      "epoch:11 step:9107 [D loss: 0.380594, acc.: 82.81%] [G loss: 2.586438]\n",
      "epoch:11 step:9108 [D loss: 0.398468, acc.: 81.25%] [G loss: 2.224511]\n",
      "epoch:11 step:9109 [D loss: 0.280324, acc.: 89.84%] [G loss: 2.890862]\n",
      "epoch:11 step:9110 [D loss: 0.300872, acc.: 89.06%] [G loss: 2.665534]\n",
      "epoch:11 step:9111 [D loss: 0.423693, acc.: 82.81%] [G loss: 1.921726]\n",
      "epoch:11 step:9112 [D loss: 0.361713, acc.: 82.81%] [G loss: 3.836800]\n",
      "epoch:11 step:9113 [D loss: 0.403209, acc.: 82.81%] [G loss: 3.439899]\n",
      "epoch:11 step:9114 [D loss: 0.357586, acc.: 85.16%] [G loss: 2.437694]\n",
      "epoch:11 step:9115 [D loss: 0.372009, acc.: 83.59%] [G loss: 3.739473]\n",
      "epoch:11 step:9116 [D loss: 0.357792, acc.: 83.59%] [G loss: 2.964053]\n",
      "epoch:11 step:9117 [D loss: 0.410386, acc.: 84.38%] [G loss: 2.585108]\n",
      "epoch:11 step:9118 [D loss: 0.262227, acc.: 88.28%] [G loss: 3.267882]\n",
      "epoch:11 step:9119 [D loss: 0.280921, acc.: 87.50%] [G loss: 3.089668]\n",
      "epoch:11 step:9120 [D loss: 0.362994, acc.: 81.25%] [G loss: 2.355157]\n",
      "epoch:11 step:9121 [D loss: 0.335517, acc.: 85.94%] [G loss: 2.074137]\n",
      "epoch:11 step:9122 [D loss: 0.395950, acc.: 86.72%] [G loss: 3.173958]\n",
      "epoch:11 step:9123 [D loss: 0.362008, acc.: 84.38%] [G loss: 2.633213]\n",
      "epoch:11 step:9124 [D loss: 0.328324, acc.: 85.16%] [G loss: 4.083643]\n",
      "epoch:11 step:9125 [D loss: 0.351726, acc.: 82.03%] [G loss: 2.889419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:9126 [D loss: 0.419520, acc.: 77.34%] [G loss: 2.555430]\n",
      "epoch:11 step:9127 [D loss: 0.265257, acc.: 87.50%] [G loss: 3.682830]\n",
      "epoch:11 step:9128 [D loss: 0.268395, acc.: 87.50%] [G loss: 4.186783]\n",
      "epoch:11 step:9129 [D loss: 0.312887, acc.: 87.50%] [G loss: 3.364662]\n",
      "epoch:11 step:9130 [D loss: 0.341164, acc.: 85.16%] [G loss: 2.668935]\n",
      "epoch:11 step:9131 [D loss: 0.352785, acc.: 80.47%] [G loss: 2.417865]\n",
      "epoch:11 step:9132 [D loss: 0.367639, acc.: 83.59%] [G loss: 2.980449]\n",
      "epoch:11 step:9133 [D loss: 0.361288, acc.: 84.38%] [G loss: 2.556272]\n",
      "epoch:11 step:9134 [D loss: 0.311906, acc.: 86.72%] [G loss: 3.484494]\n",
      "epoch:11 step:9135 [D loss: 0.339898, acc.: 86.72%] [G loss: 2.690573]\n",
      "epoch:11 step:9136 [D loss: 0.251771, acc.: 92.97%] [G loss: 2.296867]\n",
      "epoch:11 step:9137 [D loss: 0.286811, acc.: 90.62%] [G loss: 2.671808]\n",
      "epoch:11 step:9138 [D loss: 0.391863, acc.: 82.03%] [G loss: 2.968481]\n",
      "epoch:11 step:9139 [D loss: 0.384129, acc.: 80.47%] [G loss: 2.821372]\n",
      "epoch:11 step:9140 [D loss: 0.422090, acc.: 82.03%] [G loss: 3.346925]\n",
      "epoch:11 step:9141 [D loss: 0.317054, acc.: 85.94%] [G loss: 2.993231]\n",
      "epoch:11 step:9142 [D loss: 0.282365, acc.: 89.06%] [G loss: 2.640685]\n",
      "epoch:11 step:9143 [D loss: 0.443202, acc.: 80.47%] [G loss: 2.312686]\n",
      "epoch:11 step:9144 [D loss: 0.327140, acc.: 91.41%] [G loss: 2.935371]\n",
      "epoch:11 step:9145 [D loss: 0.410160, acc.: 78.91%] [G loss: 3.576138]\n",
      "epoch:11 step:9146 [D loss: 0.301385, acc.: 84.38%] [G loss: 3.274282]\n",
      "epoch:11 step:9147 [D loss: 0.323193, acc.: 83.59%] [G loss: 2.649325]\n",
      "epoch:11 step:9148 [D loss: 0.328391, acc.: 83.59%] [G loss: 2.224967]\n",
      "epoch:11 step:9149 [D loss: 0.421098, acc.: 81.25%] [G loss: 3.333799]\n",
      "epoch:11 step:9150 [D loss: 0.249193, acc.: 89.84%] [G loss: 3.633085]\n",
      "epoch:11 step:9151 [D loss: 0.275407, acc.: 88.28%] [G loss: 3.164150]\n",
      "epoch:11 step:9152 [D loss: 0.364996, acc.: 84.38%] [G loss: 2.468496]\n",
      "epoch:11 step:9153 [D loss: 0.394574, acc.: 82.81%] [G loss: 1.943784]\n",
      "epoch:11 step:9154 [D loss: 0.293122, acc.: 87.50%] [G loss: 2.388606]\n",
      "epoch:11 step:9155 [D loss: 0.325070, acc.: 88.28%] [G loss: 2.381423]\n",
      "epoch:11 step:9156 [D loss: 0.324936, acc.: 85.94%] [G loss: 2.819626]\n",
      "epoch:11 step:9157 [D loss: 0.409088, acc.: 82.03%] [G loss: 2.471206]\n",
      "epoch:11 step:9158 [D loss: 0.371745, acc.: 85.16%] [G loss: 2.646924]\n",
      "epoch:11 step:9159 [D loss: 0.380673, acc.: 80.47%] [G loss: 2.734663]\n",
      "epoch:11 step:9160 [D loss: 0.301524, acc.: 85.94%] [G loss: 2.759013]\n",
      "epoch:11 step:9161 [D loss: 0.348302, acc.: 80.47%] [G loss: 2.974721]\n",
      "epoch:11 step:9162 [D loss: 0.401115, acc.: 80.47%] [G loss: 2.941288]\n",
      "epoch:11 step:9163 [D loss: 0.250888, acc.: 89.84%] [G loss: 3.065908]\n",
      "epoch:11 step:9164 [D loss: 0.407012, acc.: 79.69%] [G loss: 3.145407]\n",
      "epoch:11 step:9165 [D loss: 0.386637, acc.: 79.69%] [G loss: 3.670805]\n",
      "epoch:11 step:9166 [D loss: 0.280822, acc.: 89.06%] [G loss: 3.608402]\n",
      "epoch:11 step:9167 [D loss: 0.240528, acc.: 88.28%] [G loss: 3.548677]\n",
      "epoch:11 step:9168 [D loss: 0.327426, acc.: 82.03%] [G loss: 2.791239]\n",
      "epoch:11 step:9169 [D loss: 0.268935, acc.: 89.06%] [G loss: 4.222077]\n",
      "epoch:11 step:9170 [D loss: 0.195521, acc.: 92.19%] [G loss: 6.193813]\n",
      "epoch:11 step:9171 [D loss: 0.305658, acc.: 86.72%] [G loss: 3.466527]\n",
      "epoch:11 step:9172 [D loss: 0.326854, acc.: 85.16%] [G loss: 3.557490]\n",
      "epoch:11 step:9173 [D loss: 0.328283, acc.: 87.50%] [G loss: 2.909600]\n",
      "epoch:11 step:9174 [D loss: 0.441744, acc.: 75.78%] [G loss: 2.805470]\n",
      "epoch:11 step:9175 [D loss: 0.379672, acc.: 85.16%] [G loss: 3.842747]\n",
      "epoch:11 step:9176 [D loss: 0.428675, acc.: 77.34%] [G loss: 3.220221]\n",
      "epoch:11 step:9177 [D loss: 0.401070, acc.: 80.47%] [G loss: 4.402362]\n",
      "epoch:11 step:9178 [D loss: 0.241536, acc.: 89.06%] [G loss: 4.094384]\n",
      "epoch:11 step:9179 [D loss: 0.254731, acc.: 89.84%] [G loss: 4.378654]\n",
      "epoch:11 step:9180 [D loss: 0.363086, acc.: 85.16%] [G loss: 2.958479]\n",
      "epoch:11 step:9181 [D loss: 0.305308, acc.: 86.72%] [G loss: 2.691577]\n",
      "epoch:11 step:9182 [D loss: 0.319610, acc.: 81.25%] [G loss: 3.553074]\n",
      "epoch:11 step:9183 [D loss: 0.311147, acc.: 88.28%] [G loss: 3.132506]\n",
      "epoch:11 step:9184 [D loss: 0.380637, acc.: 84.38%] [G loss: 3.147101]\n",
      "epoch:11 step:9185 [D loss: 0.385077, acc.: 82.81%] [G loss: 2.765190]\n",
      "epoch:11 step:9186 [D loss: 0.426717, acc.: 82.03%] [G loss: 2.980113]\n",
      "epoch:11 step:9187 [D loss: 0.354316, acc.: 83.59%] [G loss: 3.504228]\n",
      "epoch:11 step:9188 [D loss: 0.326001, acc.: 85.16%] [G loss: 2.445502]\n",
      "epoch:11 step:9189 [D loss: 0.343751, acc.: 85.16%] [G loss: 2.426659]\n",
      "epoch:11 step:9190 [D loss: 0.248195, acc.: 89.84%] [G loss: 3.488306]\n",
      "epoch:11 step:9191 [D loss: 0.315081, acc.: 86.72%] [G loss: 3.808410]\n",
      "epoch:11 step:9192 [D loss: 0.237210, acc.: 88.28%] [G loss: 4.663610]\n",
      "epoch:11 step:9193 [D loss: 0.356399, acc.: 84.38%] [G loss: 2.889919]\n",
      "epoch:11 step:9194 [D loss: 0.317446, acc.: 88.28%] [G loss: 2.292325]\n",
      "epoch:11 step:9195 [D loss: 0.309088, acc.: 85.94%] [G loss: 3.130916]\n",
      "epoch:11 step:9196 [D loss: 0.303960, acc.: 84.38%] [G loss: 3.391492]\n",
      "epoch:11 step:9197 [D loss: 0.631369, acc.: 70.31%] [G loss: 2.777376]\n",
      "epoch:11 step:9198 [D loss: 0.505251, acc.: 75.78%] [G loss: 3.718411]\n",
      "epoch:11 step:9199 [D loss: 0.283023, acc.: 85.16%] [G loss: 3.806661]\n",
      "epoch:11 step:9200 [D loss: 0.297664, acc.: 89.84%] [G loss: 2.507327]\n",
      "epoch:11 step:9201 [D loss: 0.323676, acc.: 86.72%] [G loss: 2.987956]\n",
      "epoch:11 step:9202 [D loss: 0.327624, acc.: 83.59%] [G loss: 3.823853]\n",
      "epoch:11 step:9203 [D loss: 0.456556, acc.: 76.56%] [G loss: 2.217461]\n",
      "epoch:11 step:9204 [D loss: 0.412007, acc.: 79.69%] [G loss: 2.895634]\n",
      "epoch:11 step:9205 [D loss: 0.357056, acc.: 82.81%] [G loss: 3.072253]\n",
      "epoch:11 step:9206 [D loss: 0.352228, acc.: 81.25%] [G loss: 2.509836]\n",
      "epoch:11 step:9207 [D loss: 0.375415, acc.: 82.81%] [G loss: 2.837982]\n",
      "epoch:11 step:9208 [D loss: 0.308368, acc.: 83.59%] [G loss: 2.540305]\n",
      "epoch:11 step:9209 [D loss: 0.390802, acc.: 84.38%] [G loss: 2.918914]\n",
      "epoch:11 step:9210 [D loss: 0.198606, acc.: 89.84%] [G loss: 8.052739]\n",
      "epoch:11 step:9211 [D loss: 0.320642, acc.: 86.72%] [G loss: 3.039253]\n",
      "epoch:11 step:9212 [D loss: 0.354690, acc.: 81.25%] [G loss: 2.813759]\n",
      "epoch:11 step:9213 [D loss: 0.349323, acc.: 85.94%] [G loss: 3.556754]\n",
      "epoch:11 step:9214 [D loss: 0.257236, acc.: 89.06%] [G loss: 4.702074]\n",
      "epoch:11 step:9215 [D loss: 0.322138, acc.: 83.59%] [G loss: 2.441679]\n",
      "epoch:11 step:9216 [D loss: 0.462902, acc.: 79.69%] [G loss: 2.842922]\n",
      "epoch:11 step:9217 [D loss: 0.289794, acc.: 85.94%] [G loss: 4.827880]\n",
      "epoch:11 step:9218 [D loss: 0.288720, acc.: 87.50%] [G loss: 2.583294]\n",
      "epoch:11 step:9219 [D loss: 0.411500, acc.: 80.47%] [G loss: 3.203475]\n",
      "epoch:11 step:9220 [D loss: 0.227122, acc.: 91.41%] [G loss: 4.815722]\n",
      "epoch:11 step:9221 [D loss: 0.356939, acc.: 83.59%] [G loss: 2.662598]\n",
      "epoch:11 step:9222 [D loss: 0.320977, acc.: 88.28%] [G loss: 2.644170]\n",
      "epoch:11 step:9223 [D loss: 0.476844, acc.: 77.34%] [G loss: 3.056713]\n",
      "epoch:11 step:9224 [D loss: 0.425913, acc.: 81.25%] [G loss: 3.858090]\n",
      "epoch:11 step:9225 [D loss: 0.540113, acc.: 75.78%] [G loss: 2.548785]\n",
      "epoch:11 step:9226 [D loss: 0.503860, acc.: 78.91%] [G loss: 3.013424]\n",
      "epoch:11 step:9227 [D loss: 0.323590, acc.: 85.94%] [G loss: 3.632966]\n",
      "epoch:11 step:9228 [D loss: 0.440430, acc.: 79.69%] [G loss: 3.040076]\n",
      "epoch:11 step:9229 [D loss: 0.310904, acc.: 88.28%] [G loss: 2.877591]\n",
      "epoch:11 step:9230 [D loss: 0.382838, acc.: 85.94%] [G loss: 2.259723]\n",
      "epoch:11 step:9231 [D loss: 0.391519, acc.: 84.38%] [G loss: 3.583488]\n",
      "epoch:11 step:9232 [D loss: 0.306445, acc.: 90.62%] [G loss: 2.576191]\n",
      "epoch:11 step:9233 [D loss: 0.406044, acc.: 78.12%] [G loss: 2.411067]\n",
      "epoch:11 step:9234 [D loss: 0.338179, acc.: 86.72%] [G loss: 2.452394]\n",
      "epoch:11 step:9235 [D loss: 0.486900, acc.: 75.78%] [G loss: 2.736004]\n",
      "epoch:11 step:9236 [D loss: 0.319390, acc.: 85.16%] [G loss: 3.715267]\n",
      "epoch:11 step:9237 [D loss: 0.372311, acc.: 83.59%] [G loss: 2.863575]\n",
      "epoch:11 step:9238 [D loss: 0.345551, acc.: 82.03%] [G loss: 3.636885]\n",
      "epoch:11 step:9239 [D loss: 0.315921, acc.: 86.72%] [G loss: 4.660765]\n",
      "epoch:11 step:9240 [D loss: 0.464219, acc.: 78.12%] [G loss: 1.998675]\n",
      "epoch:11 step:9241 [D loss: 0.275225, acc.: 88.28%] [G loss: 3.451689]\n",
      "epoch:11 step:9242 [D loss: 0.241275, acc.: 89.06%] [G loss: 5.333853]\n",
      "epoch:11 step:9243 [D loss: 0.400567, acc.: 85.16%] [G loss: 2.635223]\n",
      "epoch:11 step:9244 [D loss: 0.401214, acc.: 84.38%] [G loss: 2.782413]\n",
      "epoch:11 step:9245 [D loss: 0.409587, acc.: 82.81%] [G loss: 3.577278]\n",
      "epoch:11 step:9246 [D loss: 0.568361, acc.: 71.09%] [G loss: 2.630147]\n",
      "epoch:11 step:9247 [D loss: 0.387861, acc.: 80.47%] [G loss: 2.798686]\n",
      "epoch:11 step:9248 [D loss: 0.446204, acc.: 81.25%] [G loss: 2.855896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:9249 [D loss: 0.323562, acc.: 89.06%] [G loss: 3.276108]\n",
      "epoch:11 step:9250 [D loss: 0.386119, acc.: 79.69%] [G loss: 2.935705]\n",
      "epoch:11 step:9251 [D loss: 0.425654, acc.: 82.03%] [G loss: 2.512732]\n",
      "epoch:11 step:9252 [D loss: 0.571044, acc.: 72.66%] [G loss: 6.982935]\n",
      "epoch:11 step:9253 [D loss: 1.000978, acc.: 62.50%] [G loss: 4.658928]\n",
      "epoch:11 step:9254 [D loss: 0.667413, acc.: 67.97%] [G loss: 1.907011]\n",
      "epoch:11 step:9255 [D loss: 0.333592, acc.: 84.38%] [G loss: 2.900999]\n",
      "epoch:11 step:9256 [D loss: 0.447053, acc.: 78.91%] [G loss: 3.734716]\n",
      "epoch:11 step:9257 [D loss: 0.300599, acc.: 86.72%] [G loss: 4.226891]\n",
      "epoch:11 step:9258 [D loss: 0.340074, acc.: 86.72%] [G loss: 3.936784]\n",
      "epoch:11 step:9259 [D loss: 0.349950, acc.: 84.38%] [G loss: 3.465475]\n",
      "epoch:11 step:9260 [D loss: 0.304526, acc.: 87.50%] [G loss: 3.501939]\n",
      "epoch:11 step:9261 [D loss: 0.267712, acc.: 88.28%] [G loss: 3.638598]\n",
      "epoch:11 step:9262 [D loss: 0.429168, acc.: 84.38%] [G loss: 2.595063]\n",
      "epoch:11 step:9263 [D loss: 0.372090, acc.: 80.47%] [G loss: 2.246618]\n",
      "epoch:11 step:9264 [D loss: 0.307790, acc.: 88.28%] [G loss: 2.003138]\n",
      "epoch:11 step:9265 [D loss: 0.417215, acc.: 85.94%] [G loss: 2.204620]\n",
      "epoch:11 step:9266 [D loss: 0.367580, acc.: 80.47%] [G loss: 2.592482]\n",
      "epoch:11 step:9267 [D loss: 0.293018, acc.: 88.28%] [G loss: 2.523394]\n",
      "epoch:11 step:9268 [D loss: 0.355702, acc.: 83.59%] [G loss: 2.674931]\n",
      "epoch:11 step:9269 [D loss: 0.300333, acc.: 88.28%] [G loss: 2.237186]\n",
      "epoch:11 step:9270 [D loss: 0.339785, acc.: 90.62%] [G loss: 2.408564]\n",
      "epoch:11 step:9271 [D loss: 0.435339, acc.: 82.81%] [G loss: 2.374212]\n",
      "epoch:11 step:9272 [D loss: 0.364745, acc.: 85.94%] [G loss: 2.389739]\n",
      "epoch:11 step:9273 [D loss: 0.286395, acc.: 88.28%] [G loss: 2.842828]\n",
      "epoch:11 step:9274 [D loss: 0.328889, acc.: 82.81%] [G loss: 4.066678]\n",
      "epoch:11 step:9275 [D loss: 0.218941, acc.: 92.19%] [G loss: 3.402291]\n",
      "epoch:11 step:9276 [D loss: 0.336435, acc.: 82.81%] [G loss: 2.976740]\n",
      "epoch:11 step:9277 [D loss: 0.264693, acc.: 85.94%] [G loss: 4.361701]\n",
      "epoch:11 step:9278 [D loss: 0.264867, acc.: 89.84%] [G loss: 3.609072]\n",
      "epoch:11 step:9279 [D loss: 0.200644, acc.: 92.97%] [G loss: 3.267701]\n",
      "epoch:11 step:9280 [D loss: 0.255518, acc.: 90.62%] [G loss: 3.762520]\n",
      "epoch:11 step:9281 [D loss: 0.319903, acc.: 87.50%] [G loss: 2.715007]\n",
      "epoch:11 step:9282 [D loss: 0.335638, acc.: 83.59%] [G loss: 2.661732]\n",
      "epoch:11 step:9283 [D loss: 0.480098, acc.: 80.47%] [G loss: 2.458018]\n",
      "epoch:11 step:9284 [D loss: 0.249719, acc.: 92.97%] [G loss: 2.967466]\n",
      "epoch:11 step:9285 [D loss: 0.286671, acc.: 86.72%] [G loss: 3.179732]\n",
      "epoch:11 step:9286 [D loss: 0.265536, acc.: 91.41%] [G loss: 2.628762]\n",
      "epoch:11 step:9287 [D loss: 0.308937, acc.: 84.38%] [G loss: 2.550850]\n",
      "epoch:11 step:9288 [D loss: 0.256446, acc.: 89.84%] [G loss: 2.808537]\n",
      "epoch:11 step:9289 [D loss: 0.292792, acc.: 89.06%] [G loss: 2.356875]\n",
      "epoch:11 step:9290 [D loss: 0.332702, acc.: 89.84%] [G loss: 2.499949]\n",
      "epoch:11 step:9291 [D loss: 0.298314, acc.: 85.94%] [G loss: 3.657304]\n",
      "epoch:11 step:9292 [D loss: 0.285745, acc.: 89.84%] [G loss: 2.610972]\n",
      "epoch:11 step:9293 [D loss: 0.291757, acc.: 84.38%] [G loss: 3.068091]\n",
      "epoch:11 step:9294 [D loss: 0.411780, acc.: 75.78%] [G loss: 3.878301]\n",
      "epoch:11 step:9295 [D loss: 0.451754, acc.: 78.91%] [G loss: 2.993726]\n",
      "epoch:11 step:9296 [D loss: 0.387046, acc.: 85.94%] [G loss: 2.880354]\n",
      "epoch:11 step:9297 [D loss: 0.329798, acc.: 85.16%] [G loss: 2.831296]\n",
      "epoch:11 step:9298 [D loss: 0.388645, acc.: 85.94%] [G loss: 2.317280]\n",
      "epoch:11 step:9299 [D loss: 0.374600, acc.: 84.38%] [G loss: 2.685285]\n",
      "epoch:11 step:9300 [D loss: 0.406535, acc.: 80.47%] [G loss: 4.237064]\n",
      "epoch:11 step:9301 [D loss: 0.287957, acc.: 88.28%] [G loss: 3.132565]\n",
      "epoch:11 step:9302 [D loss: 0.300646, acc.: 85.16%] [G loss: 2.240012]\n",
      "epoch:11 step:9303 [D loss: 0.308644, acc.: 84.38%] [G loss: 2.757637]\n",
      "epoch:11 step:9304 [D loss: 0.394245, acc.: 80.47%] [G loss: 3.463910]\n",
      "epoch:11 step:9305 [D loss: 0.296785, acc.: 85.16%] [G loss: 3.134971]\n",
      "epoch:11 step:9306 [D loss: 0.379387, acc.: 83.59%] [G loss: 2.496696]\n",
      "epoch:11 step:9307 [D loss: 0.204796, acc.: 90.62%] [G loss: 5.101363]\n",
      "epoch:11 step:9308 [D loss: 0.378891, acc.: 81.25%] [G loss: 4.053119]\n",
      "epoch:11 step:9309 [D loss: 0.344702, acc.: 88.28%] [G loss: 2.127272]\n",
      "epoch:11 step:9310 [D loss: 0.334490, acc.: 83.59%] [G loss: 2.901831]\n",
      "epoch:11 step:9311 [D loss: 0.432918, acc.: 78.91%] [G loss: 3.572894]\n",
      "epoch:11 step:9312 [D loss: 0.408343, acc.: 80.47%] [G loss: 4.364910]\n",
      "epoch:11 step:9313 [D loss: 0.749941, acc.: 73.44%] [G loss: 2.983732]\n",
      "epoch:11 step:9314 [D loss: 0.405644, acc.: 81.25%] [G loss: 3.332084]\n",
      "epoch:11 step:9315 [D loss: 0.288102, acc.: 89.06%] [G loss: 3.523321]\n",
      "epoch:11 step:9316 [D loss: 0.350685, acc.: 85.94%] [G loss: 2.453922]\n",
      "epoch:11 step:9317 [D loss: 0.285476, acc.: 90.62%] [G loss: 2.231723]\n",
      "epoch:11 step:9318 [D loss: 0.366754, acc.: 83.59%] [G loss: 3.137411]\n",
      "epoch:11 step:9319 [D loss: 0.370433, acc.: 84.38%] [G loss: 2.416912]\n",
      "epoch:11 step:9320 [D loss: 0.287738, acc.: 92.19%] [G loss: 2.904271]\n",
      "epoch:11 step:9321 [D loss: 0.270061, acc.: 92.97%] [G loss: 4.216666]\n",
      "epoch:11 step:9322 [D loss: 0.342147, acc.: 88.28%] [G loss: 3.479293]\n",
      "epoch:11 step:9323 [D loss: 0.291966, acc.: 88.28%] [G loss: 1.982429]\n",
      "epoch:11 step:9324 [D loss: 0.307398, acc.: 83.59%] [G loss: 2.885234]\n",
      "epoch:11 step:9325 [D loss: 0.338975, acc.: 85.16%] [G loss: 2.976710]\n",
      "epoch:11 step:9326 [D loss: 0.441897, acc.: 83.59%] [G loss: 2.602652]\n",
      "epoch:11 step:9327 [D loss: 0.448770, acc.: 78.12%] [G loss: 2.437728]\n",
      "epoch:11 step:9328 [D loss: 0.320896, acc.: 86.72%] [G loss: 2.655586]\n",
      "epoch:11 step:9329 [D loss: 0.383032, acc.: 85.16%] [G loss: 2.328733]\n",
      "epoch:11 step:9330 [D loss: 0.341596, acc.: 82.81%] [G loss: 2.019724]\n",
      "epoch:11 step:9331 [D loss: 0.329720, acc.: 89.06%] [G loss: 2.493391]\n",
      "epoch:11 step:9332 [D loss: 0.298284, acc.: 90.62%] [G loss: 2.649617]\n",
      "epoch:11 step:9333 [D loss: 0.373555, acc.: 85.94%] [G loss: 3.173266]\n",
      "epoch:11 step:9334 [D loss: 0.298982, acc.: 88.28%] [G loss: 2.873611]\n",
      "epoch:11 step:9335 [D loss: 0.287702, acc.: 90.62%] [G loss: 2.534635]\n",
      "epoch:11 step:9336 [D loss: 0.336356, acc.: 87.50%] [G loss: 2.152034]\n",
      "epoch:11 step:9337 [D loss: 0.316215, acc.: 88.28%] [G loss: 2.385964]\n",
      "epoch:11 step:9338 [D loss: 0.289734, acc.: 89.06%] [G loss: 3.011163]\n",
      "epoch:11 step:9339 [D loss: 0.272225, acc.: 85.94%] [G loss: 4.032104]\n",
      "epoch:11 step:9340 [D loss: 0.279960, acc.: 89.06%] [G loss: 2.822028]\n",
      "epoch:11 step:9341 [D loss: 0.380208, acc.: 82.81%] [G loss: 2.717712]\n",
      "epoch:11 step:9342 [D loss: 0.143485, acc.: 96.09%] [G loss: 4.861823]\n",
      "epoch:11 step:9343 [D loss: 0.221145, acc.: 94.53%] [G loss: 3.996080]\n",
      "epoch:11 step:9344 [D loss: 0.407915, acc.: 84.38%] [G loss: 3.130694]\n",
      "epoch:11 step:9345 [D loss: 0.370638, acc.: 85.94%] [G loss: 2.393224]\n",
      "epoch:11 step:9346 [D loss: 0.321323, acc.: 85.16%] [G loss: 2.503121]\n",
      "epoch:11 step:9347 [D loss: 0.301433, acc.: 86.72%] [G loss: 3.076651]\n",
      "epoch:11 step:9348 [D loss: 0.277539, acc.: 85.94%] [G loss: 2.327051]\n",
      "epoch:11 step:9349 [D loss: 0.343096, acc.: 85.94%] [G loss: 2.323120]\n",
      "epoch:11 step:9350 [D loss: 0.331723, acc.: 85.94%] [G loss: 3.357755]\n",
      "epoch:11 step:9351 [D loss: 0.208988, acc.: 91.41%] [G loss: 3.785086]\n",
      "epoch:11 step:9352 [D loss: 0.210411, acc.: 90.62%] [G loss: 5.787691]\n",
      "epoch:11 step:9353 [D loss: 0.417103, acc.: 82.03%] [G loss: 2.662740]\n",
      "epoch:11 step:9354 [D loss: 0.293073, acc.: 86.72%] [G loss: 3.501281]\n",
      "epoch:11 step:9355 [D loss: 0.294060, acc.: 89.06%] [G loss: 3.588035]\n",
      "epoch:11 step:9356 [D loss: 0.383115, acc.: 84.38%] [G loss: 4.248209]\n",
      "epoch:11 step:9357 [D loss: 0.462514, acc.: 75.00%] [G loss: 2.447835]\n",
      "epoch:11 step:9358 [D loss: 0.286535, acc.: 86.72%] [G loss: 3.960510]\n",
      "epoch:11 step:9359 [D loss: 0.450747, acc.: 80.47%] [G loss: 2.270525]\n",
      "epoch:11 step:9360 [D loss: 0.426761, acc.: 85.94%] [G loss: 4.004201]\n",
      "epoch:11 step:9361 [D loss: 0.415224, acc.: 82.03%] [G loss: 3.146691]\n",
      "epoch:11 step:9362 [D loss: 0.285748, acc.: 88.28%] [G loss: 4.429076]\n",
      "epoch:11 step:9363 [D loss: 0.303076, acc.: 87.50%] [G loss: 4.825275]\n",
      "epoch:11 step:9364 [D loss: 0.354746, acc.: 85.16%] [G loss: 2.474546]\n",
      "epoch:11 step:9365 [D loss: 0.338042, acc.: 86.72%] [G loss: 3.067475]\n",
      "epoch:11 step:9366 [D loss: 0.365292, acc.: 81.25%] [G loss: 4.199080]\n",
      "epoch:11 step:9367 [D loss: 0.333668, acc.: 84.38%] [G loss: 3.220516]\n",
      "epoch:11 step:9368 [D loss: 0.360402, acc.: 84.38%] [G loss: 2.682790]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:9369 [D loss: 0.256748, acc.: 92.19%] [G loss: 2.503052]\n",
      "epoch:11 step:9370 [D loss: 0.337584, acc.: 85.16%] [G loss: 2.139928]\n",
      "epoch:11 step:9371 [D loss: 0.400310, acc.: 82.81%] [G loss: 2.319602]\n",
      "epoch:11 step:9372 [D loss: 0.319423, acc.: 85.16%] [G loss: 3.042660]\n",
      "epoch:12 step:9373 [D loss: 0.315220, acc.: 85.94%] [G loss: 2.544684]\n",
      "epoch:12 step:9374 [D loss: 0.382431, acc.: 81.25%] [G loss: 2.317263]\n",
      "epoch:12 step:9375 [D loss: 0.361558, acc.: 82.03%] [G loss: 2.797302]\n",
      "epoch:12 step:9376 [D loss: 0.274514, acc.: 89.06%] [G loss: 2.735949]\n",
      "epoch:12 step:9377 [D loss: 0.296884, acc.: 87.50%] [G loss: 2.526963]\n",
      "epoch:12 step:9378 [D loss: 0.344407, acc.: 86.72%] [G loss: 2.559903]\n",
      "epoch:12 step:9379 [D loss: 0.341131, acc.: 85.94%] [G loss: 2.221980]\n",
      "epoch:12 step:9380 [D loss: 0.329763, acc.: 84.38%] [G loss: 3.103250]\n",
      "epoch:12 step:9381 [D loss: 0.370886, acc.: 83.59%] [G loss: 3.549616]\n",
      "epoch:12 step:9382 [D loss: 0.435472, acc.: 82.03%] [G loss: 2.697554]\n",
      "epoch:12 step:9383 [D loss: 0.377625, acc.: 79.69%] [G loss: 2.621349]\n",
      "epoch:12 step:9384 [D loss: 0.314165, acc.: 89.06%] [G loss: 2.782799]\n",
      "epoch:12 step:9385 [D loss: 0.364673, acc.: 83.59%] [G loss: 3.866574]\n",
      "epoch:12 step:9386 [D loss: 0.252440, acc.: 87.50%] [G loss: 4.444252]\n",
      "epoch:12 step:9387 [D loss: 0.341094, acc.: 86.72%] [G loss: 3.452228]\n",
      "epoch:12 step:9388 [D loss: 0.345213, acc.: 84.38%] [G loss: 2.306486]\n",
      "epoch:12 step:9389 [D loss: 0.249624, acc.: 88.28%] [G loss: 3.541901]\n",
      "epoch:12 step:9390 [D loss: 0.337977, acc.: 85.16%] [G loss: 3.030850]\n",
      "epoch:12 step:9391 [D loss: 0.332386, acc.: 89.06%] [G loss: 2.425344]\n",
      "epoch:12 step:9392 [D loss: 0.385459, acc.: 82.81%] [G loss: 3.401374]\n",
      "epoch:12 step:9393 [D loss: 0.385090, acc.: 85.94%] [G loss: 2.246496]\n",
      "epoch:12 step:9394 [D loss: 0.270795, acc.: 92.19%] [G loss: 4.792988]\n",
      "epoch:12 step:9395 [D loss: 0.257539, acc.: 89.84%] [G loss: 3.082446]\n",
      "epoch:12 step:9396 [D loss: 0.521512, acc.: 72.66%] [G loss: 3.894439]\n",
      "epoch:12 step:9397 [D loss: 0.326414, acc.: 85.94%] [G loss: 2.562027]\n",
      "epoch:12 step:9398 [D loss: 0.240844, acc.: 92.97%] [G loss: 2.824639]\n",
      "epoch:12 step:9399 [D loss: 0.298785, acc.: 87.50%] [G loss: 2.822658]\n",
      "epoch:12 step:9400 [D loss: 0.195209, acc.: 94.53%] [G loss: 5.060039]\n",
      "epoch:12 step:9401 [D loss: 0.333866, acc.: 87.50%] [G loss: 2.909723]\n",
      "epoch:12 step:9402 [D loss: 0.271192, acc.: 89.06%] [G loss: 2.706452]\n",
      "epoch:12 step:9403 [D loss: 0.408154, acc.: 81.25%] [G loss: 3.925333]\n",
      "epoch:12 step:9404 [D loss: 0.486899, acc.: 78.12%] [G loss: 7.461365]\n",
      "epoch:12 step:9405 [D loss: 0.507821, acc.: 78.12%] [G loss: 3.925308]\n",
      "epoch:12 step:9406 [D loss: 0.226167, acc.: 89.84%] [G loss: 4.543599]\n",
      "epoch:12 step:9407 [D loss: 0.308840, acc.: 88.28%] [G loss: 3.752674]\n",
      "epoch:12 step:9408 [D loss: 0.266785, acc.: 90.62%] [G loss: 2.553477]\n",
      "epoch:12 step:9409 [D loss: 0.308789, acc.: 88.28%] [G loss: 3.357934]\n",
      "epoch:12 step:9410 [D loss: 0.402247, acc.: 79.69%] [G loss: 2.273343]\n",
      "epoch:12 step:9411 [D loss: 0.387137, acc.: 83.59%] [G loss: 2.898583]\n",
      "epoch:12 step:9412 [D loss: 0.265814, acc.: 88.28%] [G loss: 2.394047]\n",
      "epoch:12 step:9413 [D loss: 0.288762, acc.: 92.19%] [G loss: 2.866370]\n",
      "epoch:12 step:9414 [D loss: 0.313785, acc.: 89.06%] [G loss: 2.652117]\n",
      "epoch:12 step:9415 [D loss: 0.354770, acc.: 85.94%] [G loss: 3.773355]\n",
      "epoch:12 step:9416 [D loss: 0.328824, acc.: 83.59%] [G loss: 4.563060]\n",
      "epoch:12 step:9417 [D loss: 0.341635, acc.: 85.16%] [G loss: 3.040313]\n",
      "epoch:12 step:9418 [D loss: 0.383660, acc.: 82.03%] [G loss: 2.509594]\n",
      "epoch:12 step:9419 [D loss: 0.314413, acc.: 88.28%] [G loss: 2.392344]\n",
      "epoch:12 step:9420 [D loss: 0.280785, acc.: 89.84%] [G loss: 2.267820]\n",
      "epoch:12 step:9421 [D loss: 0.333402, acc.: 85.94%] [G loss: 2.695632]\n",
      "epoch:12 step:9422 [D loss: 0.325653, acc.: 85.16%] [G loss: 2.204125]\n",
      "epoch:12 step:9423 [D loss: 0.437590, acc.: 79.69%] [G loss: 2.330798]\n",
      "epoch:12 step:9424 [D loss: 0.322290, acc.: 85.16%] [G loss: 3.053454]\n",
      "epoch:12 step:9425 [D loss: 0.391497, acc.: 85.94%] [G loss: 2.385016]\n",
      "epoch:12 step:9426 [D loss: 0.336416, acc.: 83.59%] [G loss: 3.172325]\n",
      "epoch:12 step:9427 [D loss: 0.268791, acc.: 92.19%] [G loss: 4.651089]\n",
      "epoch:12 step:9428 [D loss: 0.318147, acc.: 83.59%] [G loss: 3.484116]\n",
      "epoch:12 step:9429 [D loss: 0.266964, acc.: 92.19%] [G loss: 2.694837]\n",
      "epoch:12 step:9430 [D loss: 0.429360, acc.: 77.34%] [G loss: 3.319553]\n",
      "epoch:12 step:9431 [D loss: 0.213247, acc.: 89.84%] [G loss: 6.462674]\n",
      "epoch:12 step:9432 [D loss: 0.275171, acc.: 90.62%] [G loss: 4.781526]\n",
      "epoch:12 step:9433 [D loss: 0.223386, acc.: 88.28%] [G loss: 8.093316]\n",
      "epoch:12 step:9434 [D loss: 0.237986, acc.: 89.84%] [G loss: 6.059368]\n",
      "epoch:12 step:9435 [D loss: 0.193605, acc.: 91.41%] [G loss: 3.802603]\n",
      "epoch:12 step:9436 [D loss: 0.254546, acc.: 89.06%] [G loss: 5.115059]\n",
      "epoch:12 step:9437 [D loss: 0.213318, acc.: 92.97%] [G loss: 5.503534]\n",
      "epoch:12 step:9438 [D loss: 0.306574, acc.: 87.50%] [G loss: 2.736123]\n",
      "epoch:12 step:9439 [D loss: 0.208411, acc.: 92.19%] [G loss: 5.099218]\n",
      "epoch:12 step:9440 [D loss: 0.287822, acc.: 89.84%] [G loss: 2.817715]\n",
      "epoch:12 step:9441 [D loss: 0.270819, acc.: 89.84%] [G loss: 3.108247]\n",
      "epoch:12 step:9442 [D loss: 0.324797, acc.: 83.59%] [G loss: 2.637898]\n",
      "epoch:12 step:9443 [D loss: 0.464619, acc.: 78.91%] [G loss: 2.856239]\n",
      "epoch:12 step:9444 [D loss: 0.395446, acc.: 85.16%] [G loss: 3.505207]\n",
      "epoch:12 step:9445 [D loss: 0.474517, acc.: 82.81%] [G loss: 1.805930]\n",
      "epoch:12 step:9446 [D loss: 0.275249, acc.: 92.19%] [G loss: 3.351631]\n",
      "epoch:12 step:9447 [D loss: 0.365968, acc.: 83.59%] [G loss: 3.827535]\n",
      "epoch:12 step:9448 [D loss: 0.344270, acc.: 88.28%] [G loss: 2.348305]\n",
      "epoch:12 step:9449 [D loss: 0.303175, acc.: 84.38%] [G loss: 2.954159]\n",
      "epoch:12 step:9450 [D loss: 0.368315, acc.: 85.16%] [G loss: 2.512302]\n",
      "epoch:12 step:9451 [D loss: 0.389759, acc.: 81.25%] [G loss: 2.813029]\n",
      "epoch:12 step:9452 [D loss: 0.278568, acc.: 87.50%] [G loss: 3.862205]\n",
      "epoch:12 step:9453 [D loss: 0.316557, acc.: 88.28%] [G loss: 3.550802]\n",
      "epoch:12 step:9454 [D loss: 0.330352, acc.: 86.72%] [G loss: 2.782572]\n",
      "epoch:12 step:9455 [D loss: 0.302754, acc.: 86.72%] [G loss: 3.477286]\n",
      "epoch:12 step:9456 [D loss: 0.264141, acc.: 88.28%] [G loss: 4.561343]\n",
      "epoch:12 step:9457 [D loss: 0.259122, acc.: 89.84%] [G loss: 4.398009]\n",
      "epoch:12 step:9458 [D loss: 0.421341, acc.: 80.47%] [G loss: 2.106284]\n",
      "epoch:12 step:9459 [D loss: 0.259842, acc.: 89.06%] [G loss: 2.795921]\n",
      "epoch:12 step:9460 [D loss: 0.319245, acc.: 86.72%] [G loss: 2.718925]\n",
      "epoch:12 step:9461 [D loss: 0.312552, acc.: 87.50%] [G loss: 3.434203]\n",
      "epoch:12 step:9462 [D loss: 0.429331, acc.: 79.69%] [G loss: 2.853869]\n",
      "epoch:12 step:9463 [D loss: 0.395440, acc.: 80.47%] [G loss: 2.288223]\n",
      "epoch:12 step:9464 [D loss: 0.286862, acc.: 87.50%] [G loss: 2.535456]\n",
      "epoch:12 step:9465 [D loss: 0.341340, acc.: 86.72%] [G loss: 2.360402]\n",
      "epoch:12 step:9466 [D loss: 0.247098, acc.: 92.19%] [G loss: 2.625434]\n",
      "epoch:12 step:9467 [D loss: 0.273854, acc.: 89.84%] [G loss: 3.352841]\n",
      "epoch:12 step:9468 [D loss: 0.266757, acc.: 90.62%] [G loss: 3.983220]\n",
      "epoch:12 step:9469 [D loss: 0.388853, acc.: 82.81%] [G loss: 3.121084]\n",
      "epoch:12 step:9470 [D loss: 0.369779, acc.: 84.38%] [G loss: 2.511949]\n",
      "epoch:12 step:9471 [D loss: 0.266059, acc.: 86.72%] [G loss: 3.010386]\n",
      "epoch:12 step:9472 [D loss: 0.349520, acc.: 83.59%] [G loss: 2.946893]\n",
      "epoch:12 step:9473 [D loss: 0.303376, acc.: 86.72%] [G loss: 2.694857]\n",
      "epoch:12 step:9474 [D loss: 0.318650, acc.: 85.16%] [G loss: 2.270327]\n",
      "epoch:12 step:9475 [D loss: 0.339411, acc.: 84.38%] [G loss: 2.606349]\n",
      "epoch:12 step:9476 [D loss: 0.349644, acc.: 82.81%] [G loss: 3.087018]\n",
      "epoch:12 step:9477 [D loss: 0.273767, acc.: 89.84%] [G loss: 3.699326]\n",
      "epoch:12 step:9478 [D loss: 0.326880, acc.: 85.16%] [G loss: 2.792485]\n",
      "epoch:12 step:9479 [D loss: 0.322477, acc.: 87.50%] [G loss: 3.078971]\n",
      "epoch:12 step:9480 [D loss: 0.383095, acc.: 81.25%] [G loss: 3.034697]\n",
      "epoch:12 step:9481 [D loss: 0.359943, acc.: 85.16%] [G loss: 2.133164]\n",
      "epoch:12 step:9482 [D loss: 0.330990, acc.: 85.94%] [G loss: 2.873265]\n",
      "epoch:12 step:9483 [D loss: 0.296563, acc.: 85.94%] [G loss: 4.158466]\n",
      "epoch:12 step:9484 [D loss: 0.256663, acc.: 88.28%] [G loss: 2.994719]\n",
      "epoch:12 step:9485 [D loss: 0.336774, acc.: 85.94%] [G loss: 2.885469]\n",
      "epoch:12 step:9486 [D loss: 0.318313, acc.: 89.06%] [G loss: 3.246100]\n",
      "epoch:12 step:9487 [D loss: 0.373094, acc.: 83.59%] [G loss: 2.764308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9488 [D loss: 0.320571, acc.: 85.16%] [G loss: 3.169298]\n",
      "epoch:12 step:9489 [D loss: 0.405479, acc.: 81.25%] [G loss: 3.176180]\n",
      "epoch:12 step:9490 [D loss: 0.314440, acc.: 89.84%] [G loss: 2.453822]\n",
      "epoch:12 step:9491 [D loss: 0.288626, acc.: 87.50%] [G loss: 3.543478]\n",
      "epoch:12 step:9492 [D loss: 0.314202, acc.: 85.94%] [G loss: 3.625855]\n",
      "epoch:12 step:9493 [D loss: 0.332690, acc.: 84.38%] [G loss: 5.382659]\n",
      "epoch:12 step:9494 [D loss: 0.186401, acc.: 93.75%] [G loss: 4.205452]\n",
      "epoch:12 step:9495 [D loss: 0.247292, acc.: 91.41%] [G loss: 3.853577]\n",
      "epoch:12 step:9496 [D loss: 0.349082, acc.: 89.84%] [G loss: 3.537632]\n",
      "epoch:12 step:9497 [D loss: 0.382539, acc.: 82.03%] [G loss: 2.751256]\n",
      "epoch:12 step:9498 [D loss: 0.305732, acc.: 85.94%] [G loss: 4.029136]\n",
      "epoch:12 step:9499 [D loss: 0.233626, acc.: 90.62%] [G loss: 3.785400]\n",
      "epoch:12 step:9500 [D loss: 0.263842, acc.: 90.62%] [G loss: 3.601431]\n",
      "epoch:12 step:9501 [D loss: 0.350513, acc.: 86.72%] [G loss: 2.679858]\n",
      "epoch:12 step:9502 [D loss: 0.347874, acc.: 84.38%] [G loss: 3.253138]\n",
      "epoch:12 step:9503 [D loss: 0.432897, acc.: 79.69%] [G loss: 3.153156]\n",
      "epoch:12 step:9504 [D loss: 0.277454, acc.: 88.28%] [G loss: 3.039318]\n",
      "epoch:12 step:9505 [D loss: 0.334913, acc.: 89.06%] [G loss: 2.687038]\n",
      "epoch:12 step:9506 [D loss: 0.342035, acc.: 85.94%] [G loss: 2.601501]\n",
      "epoch:12 step:9507 [D loss: 0.330742, acc.: 86.72%] [G loss: 3.203269]\n",
      "epoch:12 step:9508 [D loss: 0.225544, acc.: 91.41%] [G loss: 2.516208]\n",
      "epoch:12 step:9509 [D loss: 0.314528, acc.: 85.94%] [G loss: 2.540289]\n",
      "epoch:12 step:9510 [D loss: 0.248513, acc.: 92.19%] [G loss: 3.271394]\n",
      "epoch:12 step:9511 [D loss: 0.352230, acc.: 86.72%] [G loss: 4.512505]\n",
      "epoch:12 step:9512 [D loss: 0.276058, acc.: 86.72%] [G loss: 2.916097]\n",
      "epoch:12 step:9513 [D loss: 0.264537, acc.: 89.06%] [G loss: 3.125348]\n",
      "epoch:12 step:9514 [D loss: 0.262206, acc.: 88.28%] [G loss: 4.417007]\n",
      "epoch:12 step:9515 [D loss: 0.248212, acc.: 89.84%] [G loss: 3.933144]\n",
      "epoch:12 step:9516 [D loss: 0.339399, acc.: 85.94%] [G loss: 3.431668]\n",
      "epoch:12 step:9517 [D loss: 0.297789, acc.: 86.72%] [G loss: 3.043816]\n",
      "epoch:12 step:9518 [D loss: 0.210391, acc.: 94.53%] [G loss: 2.656388]\n",
      "epoch:12 step:9519 [D loss: 0.309529, acc.: 86.72%] [G loss: 4.024222]\n",
      "epoch:12 step:9520 [D loss: 0.267311, acc.: 89.84%] [G loss: 2.816879]\n",
      "epoch:12 step:9521 [D loss: 0.337964, acc.: 83.59%] [G loss: 4.314433]\n",
      "epoch:12 step:9522 [D loss: 0.361122, acc.: 84.38%] [G loss: 4.805939]\n",
      "epoch:12 step:9523 [D loss: 0.402562, acc.: 82.81%] [G loss: 2.987538]\n",
      "epoch:12 step:9524 [D loss: 0.344647, acc.: 81.25%] [G loss: 3.754303]\n",
      "epoch:12 step:9525 [D loss: 0.344517, acc.: 85.16%] [G loss: 3.910869]\n",
      "epoch:12 step:9526 [D loss: 0.282896, acc.: 91.41%] [G loss: 3.509979]\n",
      "epoch:12 step:9527 [D loss: 0.329897, acc.: 85.16%] [G loss: 2.965694]\n",
      "epoch:12 step:9528 [D loss: 0.257761, acc.: 89.06%] [G loss: 2.895916]\n",
      "epoch:12 step:9529 [D loss: 0.419760, acc.: 78.91%] [G loss: 2.992853]\n",
      "epoch:12 step:9530 [D loss: 0.309295, acc.: 87.50%] [G loss: 2.927399]\n",
      "epoch:12 step:9531 [D loss: 0.377418, acc.: 83.59%] [G loss: 2.747526]\n",
      "epoch:12 step:9532 [D loss: 0.380194, acc.: 81.25%] [G loss: 4.000006]\n",
      "epoch:12 step:9533 [D loss: 0.336911, acc.: 86.72%] [G loss: 3.169521]\n",
      "epoch:12 step:9534 [D loss: 0.216842, acc.: 92.97%] [G loss: 2.824106]\n",
      "epoch:12 step:9535 [D loss: 0.356257, acc.: 88.28%] [G loss: 2.452371]\n",
      "epoch:12 step:9536 [D loss: 0.289704, acc.: 89.84%] [G loss: 2.560750]\n",
      "epoch:12 step:9537 [D loss: 0.318432, acc.: 85.16%] [G loss: 2.404429]\n",
      "epoch:12 step:9538 [D loss: 0.327454, acc.: 86.72%] [G loss: 3.141243]\n",
      "epoch:12 step:9539 [D loss: 0.384779, acc.: 81.25%] [G loss: 4.004397]\n",
      "epoch:12 step:9540 [D loss: 0.351621, acc.: 84.38%] [G loss: 4.737924]\n",
      "epoch:12 step:9541 [D loss: 0.239840, acc.: 87.50%] [G loss: 5.527918]\n",
      "epoch:12 step:9542 [D loss: 0.234877, acc.: 91.41%] [G loss: 3.063668]\n",
      "epoch:12 step:9543 [D loss: 0.342794, acc.: 84.38%] [G loss: 5.423765]\n",
      "epoch:12 step:9544 [D loss: 0.402872, acc.: 82.81%] [G loss: 3.104075]\n",
      "epoch:12 step:9545 [D loss: 0.284453, acc.: 89.06%] [G loss: 3.419886]\n",
      "epoch:12 step:9546 [D loss: 0.397764, acc.: 84.38%] [G loss: 4.031518]\n",
      "epoch:12 step:9547 [D loss: 0.437093, acc.: 82.03%] [G loss: 3.573622]\n",
      "epoch:12 step:9548 [D loss: 0.363119, acc.: 84.38%] [G loss: 8.236488]\n",
      "epoch:12 step:9549 [D loss: 0.528051, acc.: 75.78%] [G loss: 4.384156]\n",
      "epoch:12 step:9550 [D loss: 0.861421, acc.: 67.97%] [G loss: 5.980552]\n",
      "epoch:12 step:9551 [D loss: 0.673614, acc.: 75.78%] [G loss: 2.164136]\n",
      "epoch:12 step:9552 [D loss: 0.302418, acc.: 89.06%] [G loss: 3.763990]\n",
      "epoch:12 step:9553 [D loss: 0.246895, acc.: 89.84%] [G loss: 3.196218]\n",
      "epoch:12 step:9554 [D loss: 0.494332, acc.: 81.25%] [G loss: 2.692071]\n",
      "epoch:12 step:9555 [D loss: 0.270842, acc.: 90.62%] [G loss: 2.599808]\n",
      "epoch:12 step:9556 [D loss: 0.360811, acc.: 83.59%] [G loss: 3.461740]\n",
      "epoch:12 step:9557 [D loss: 0.308217, acc.: 86.72%] [G loss: 3.214942]\n",
      "epoch:12 step:9558 [D loss: 0.293880, acc.: 85.94%] [G loss: 3.889935]\n",
      "epoch:12 step:9559 [D loss: 0.272074, acc.: 89.84%] [G loss: 2.734223]\n",
      "epoch:12 step:9560 [D loss: 0.383791, acc.: 84.38%] [G loss: 4.123388]\n",
      "epoch:12 step:9561 [D loss: 0.297595, acc.: 83.59%] [G loss: 4.075490]\n",
      "epoch:12 step:9562 [D loss: 0.292632, acc.: 88.28%] [G loss: 3.455827]\n",
      "epoch:12 step:9563 [D loss: 0.500946, acc.: 75.00%] [G loss: 3.571059]\n",
      "epoch:12 step:9564 [D loss: 0.321134, acc.: 89.06%] [G loss: 3.022574]\n",
      "epoch:12 step:9565 [D loss: 0.353074, acc.: 77.34%] [G loss: 2.556756]\n",
      "epoch:12 step:9566 [D loss: 0.301230, acc.: 87.50%] [G loss: 2.713889]\n",
      "epoch:12 step:9567 [D loss: 0.241827, acc.: 89.84%] [G loss: 3.386808]\n",
      "epoch:12 step:9568 [D loss: 0.322165, acc.: 85.16%] [G loss: 2.251269]\n",
      "epoch:12 step:9569 [D loss: 0.323180, acc.: 84.38%] [G loss: 3.121037]\n",
      "epoch:12 step:9570 [D loss: 0.301224, acc.: 86.72%] [G loss: 2.465230]\n",
      "epoch:12 step:9571 [D loss: 0.243002, acc.: 92.19%] [G loss: 3.034541]\n",
      "epoch:12 step:9572 [D loss: 0.307393, acc.: 85.94%] [G loss: 5.985033]\n",
      "epoch:12 step:9573 [D loss: 0.285230, acc.: 87.50%] [G loss: 4.092081]\n",
      "epoch:12 step:9574 [D loss: 0.324655, acc.: 87.50%] [G loss: 3.626295]\n",
      "epoch:12 step:9575 [D loss: 0.379015, acc.: 83.59%] [G loss: 3.506181]\n",
      "epoch:12 step:9576 [D loss: 0.408528, acc.: 84.38%] [G loss: 3.422121]\n",
      "epoch:12 step:9577 [D loss: 0.508674, acc.: 82.81%] [G loss: 3.443507]\n",
      "epoch:12 step:9578 [D loss: 0.559623, acc.: 77.34%] [G loss: 3.738831]\n",
      "epoch:12 step:9579 [D loss: 0.352517, acc.: 83.59%] [G loss: 2.787114]\n",
      "epoch:12 step:9580 [D loss: 0.442441, acc.: 84.38%] [G loss: 3.007261]\n",
      "epoch:12 step:9581 [D loss: 0.396554, acc.: 78.91%] [G loss: 2.865137]\n",
      "epoch:12 step:9582 [D loss: 0.327163, acc.: 86.72%] [G loss: 3.155465]\n",
      "epoch:12 step:9583 [D loss: 0.377675, acc.: 82.81%] [G loss: 3.947271]\n",
      "epoch:12 step:9584 [D loss: 0.323434, acc.: 86.72%] [G loss: 2.746243]\n",
      "epoch:12 step:9585 [D loss: 0.319551, acc.: 82.81%] [G loss: 2.330445]\n",
      "epoch:12 step:9586 [D loss: 0.403962, acc.: 82.03%] [G loss: 4.332581]\n",
      "epoch:12 step:9587 [D loss: 0.543163, acc.: 75.00%] [G loss: 4.159378]\n",
      "epoch:12 step:9588 [D loss: 0.456582, acc.: 82.03%] [G loss: 3.347937]\n",
      "epoch:12 step:9589 [D loss: 0.444929, acc.: 79.69%] [G loss: 2.445860]\n",
      "epoch:12 step:9590 [D loss: 0.438244, acc.: 74.22%] [G loss: 2.720556]\n",
      "epoch:12 step:9591 [D loss: 0.267221, acc.: 88.28%] [G loss: 4.854064]\n",
      "epoch:12 step:9592 [D loss: 0.330017, acc.: 82.03%] [G loss: 3.282967]\n",
      "epoch:12 step:9593 [D loss: 0.299605, acc.: 84.38%] [G loss: 2.505411]\n",
      "epoch:12 step:9594 [D loss: 0.284734, acc.: 90.62%] [G loss: 2.904460]\n",
      "epoch:12 step:9595 [D loss: 0.368160, acc.: 81.25%] [G loss: 3.505656]\n",
      "epoch:12 step:9596 [D loss: 0.241502, acc.: 89.84%] [G loss: 3.864831]\n",
      "epoch:12 step:9597 [D loss: 0.301734, acc.: 88.28%] [G loss: 5.287905]\n",
      "epoch:12 step:9598 [D loss: 0.318141, acc.: 85.94%] [G loss: 4.055712]\n",
      "epoch:12 step:9599 [D loss: 0.273299, acc.: 89.06%] [G loss: 5.925816]\n",
      "epoch:12 step:9600 [D loss: 0.224158, acc.: 92.19%] [G loss: 7.117986]\n",
      "epoch:12 step:9601 [D loss: 0.344909, acc.: 83.59%] [G loss: 5.359251]\n",
      "epoch:12 step:9602 [D loss: 0.344159, acc.: 87.50%] [G loss: 4.257441]\n",
      "epoch:12 step:9603 [D loss: 0.265936, acc.: 92.97%] [G loss: 3.981857]\n",
      "epoch:12 step:9604 [D loss: 0.329857, acc.: 79.69%] [G loss: 2.623522]\n",
      "epoch:12 step:9605 [D loss: 0.351562, acc.: 85.16%] [G loss: 2.963720]\n",
      "epoch:12 step:9606 [D loss: 0.466940, acc.: 81.25%] [G loss: 3.138943]\n",
      "epoch:12 step:9607 [D loss: 0.404690, acc.: 80.47%] [G loss: 3.874768]\n",
      "epoch:12 step:9608 [D loss: 0.734659, acc.: 74.22%] [G loss: 4.337067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9609 [D loss: 0.788057, acc.: 72.66%] [G loss: 3.830436]\n",
      "epoch:12 step:9610 [D loss: 0.324840, acc.: 86.72%] [G loss: 4.096432]\n",
      "epoch:12 step:9611 [D loss: 0.522887, acc.: 78.91%] [G loss: 2.358930]\n",
      "epoch:12 step:9612 [D loss: 0.444410, acc.: 79.69%] [G loss: 4.222578]\n",
      "epoch:12 step:9613 [D loss: 0.303493, acc.: 87.50%] [G loss: 3.482298]\n",
      "epoch:12 step:9614 [D loss: 0.617815, acc.: 71.09%] [G loss: 2.641862]\n",
      "epoch:12 step:9615 [D loss: 0.370828, acc.: 88.28%] [G loss: 3.951757]\n",
      "epoch:12 step:9616 [D loss: 0.370296, acc.: 82.81%] [G loss: 5.051768]\n",
      "epoch:12 step:9617 [D loss: 0.332782, acc.: 85.94%] [G loss: 2.995881]\n",
      "epoch:12 step:9618 [D loss: 0.409961, acc.: 85.94%] [G loss: 2.182100]\n",
      "epoch:12 step:9619 [D loss: 0.415530, acc.: 80.47%] [G loss: 3.299030]\n",
      "epoch:12 step:9620 [D loss: 0.271866, acc.: 86.72%] [G loss: 6.970040]\n",
      "epoch:12 step:9621 [D loss: 0.427156, acc.: 79.69%] [G loss: 2.644158]\n",
      "epoch:12 step:9622 [D loss: 0.352112, acc.: 83.59%] [G loss: 3.847952]\n",
      "epoch:12 step:9623 [D loss: 0.389167, acc.: 79.69%] [G loss: 4.009495]\n",
      "epoch:12 step:9624 [D loss: 0.371764, acc.: 80.47%] [G loss: 2.465494]\n",
      "epoch:12 step:9625 [D loss: 0.294222, acc.: 88.28%] [G loss: 2.899630]\n",
      "epoch:12 step:9626 [D loss: 0.307971, acc.: 90.62%] [G loss: 2.861712]\n",
      "epoch:12 step:9627 [D loss: 0.351655, acc.: 86.72%] [G loss: 2.989340]\n",
      "epoch:12 step:9628 [D loss: 0.401506, acc.: 82.81%] [G loss: 2.363231]\n",
      "epoch:12 step:9629 [D loss: 0.392100, acc.: 87.50%] [G loss: 3.207474]\n",
      "epoch:12 step:9630 [D loss: 0.373280, acc.: 85.16%] [G loss: 2.296673]\n",
      "epoch:12 step:9631 [D loss: 0.358973, acc.: 82.03%] [G loss: 2.726100]\n",
      "epoch:12 step:9632 [D loss: 0.253137, acc.: 87.50%] [G loss: 3.231623]\n",
      "epoch:12 step:9633 [D loss: 0.353920, acc.: 81.25%] [G loss: 3.270611]\n",
      "epoch:12 step:9634 [D loss: 0.478215, acc.: 77.34%] [G loss: 3.589376]\n",
      "epoch:12 step:9635 [D loss: 0.359640, acc.: 83.59%] [G loss: 3.065727]\n",
      "epoch:12 step:9636 [D loss: 0.278194, acc.: 87.50%] [G loss: 4.335019]\n",
      "epoch:12 step:9637 [D loss: 0.343236, acc.: 80.47%] [G loss: 2.361823]\n",
      "epoch:12 step:9638 [D loss: 0.348873, acc.: 84.38%] [G loss: 3.330372]\n",
      "epoch:12 step:9639 [D loss: 0.248872, acc.: 87.50%] [G loss: 3.788079]\n",
      "epoch:12 step:9640 [D loss: 0.346274, acc.: 84.38%] [G loss: 2.233728]\n",
      "epoch:12 step:9641 [D loss: 0.302190, acc.: 85.94%] [G loss: 3.661714]\n",
      "epoch:12 step:9642 [D loss: 0.313726, acc.: 83.59%] [G loss: 2.702777]\n",
      "epoch:12 step:9643 [D loss: 0.213824, acc.: 92.19%] [G loss: 3.682353]\n",
      "epoch:12 step:9644 [D loss: 0.303473, acc.: 84.38%] [G loss: 5.265784]\n",
      "epoch:12 step:9645 [D loss: 0.210081, acc.: 92.19%] [G loss: 4.626931]\n",
      "epoch:12 step:9646 [D loss: 0.435709, acc.: 78.12%] [G loss: 2.617952]\n",
      "epoch:12 step:9647 [D loss: 0.314312, acc.: 81.25%] [G loss: 4.610051]\n",
      "epoch:12 step:9648 [D loss: 0.304926, acc.: 85.16%] [G loss: 3.804131]\n",
      "epoch:12 step:9649 [D loss: 0.367953, acc.: 86.72%] [G loss: 4.137773]\n",
      "epoch:12 step:9650 [D loss: 0.531864, acc.: 74.22%] [G loss: 3.519000]\n",
      "epoch:12 step:9651 [D loss: 0.514432, acc.: 78.91%] [G loss: 4.673556]\n",
      "epoch:12 step:9652 [D loss: 0.654523, acc.: 75.00%] [G loss: 4.576887]\n",
      "epoch:12 step:9653 [D loss: 0.517653, acc.: 76.56%] [G loss: 3.975746]\n",
      "epoch:12 step:9654 [D loss: 0.464696, acc.: 77.34%] [G loss: 2.838449]\n",
      "epoch:12 step:9655 [D loss: 0.470834, acc.: 79.69%] [G loss: 2.969280]\n",
      "epoch:12 step:9656 [D loss: 0.388557, acc.: 80.47%] [G loss: 3.525498]\n",
      "epoch:12 step:9657 [D loss: 0.580072, acc.: 73.44%] [G loss: 4.593972]\n",
      "epoch:12 step:9658 [D loss: 0.667913, acc.: 78.91%] [G loss: 2.250639]\n",
      "epoch:12 step:9659 [D loss: 0.523779, acc.: 75.00%] [G loss: 2.383770]\n",
      "epoch:12 step:9660 [D loss: 0.562557, acc.: 68.75%] [G loss: 2.666318]\n",
      "epoch:12 step:9661 [D loss: 0.362919, acc.: 85.94%] [G loss: 2.735766]\n",
      "epoch:12 step:9662 [D loss: 0.308101, acc.: 87.50%] [G loss: 3.067375]\n",
      "epoch:12 step:9663 [D loss: 0.390884, acc.: 81.25%] [G loss: 2.187968]\n",
      "epoch:12 step:9664 [D loss: 0.294702, acc.: 89.06%] [G loss: 3.921386]\n",
      "epoch:12 step:9665 [D loss: 0.211009, acc.: 92.97%] [G loss: 4.422990]\n",
      "epoch:12 step:9666 [D loss: 0.341909, acc.: 85.94%] [G loss: 2.152721]\n",
      "epoch:12 step:9667 [D loss: 0.234110, acc.: 87.50%] [G loss: 3.981449]\n",
      "epoch:12 step:9668 [D loss: 0.223891, acc.: 86.72%] [G loss: 5.659143]\n",
      "epoch:12 step:9669 [D loss: 0.393409, acc.: 81.25%] [G loss: 3.246746]\n",
      "epoch:12 step:9670 [D loss: 0.328739, acc.: 83.59%] [G loss: 2.878905]\n",
      "epoch:12 step:9671 [D loss: 0.307456, acc.: 86.72%] [G loss: 2.703084]\n",
      "epoch:12 step:9672 [D loss: 0.391439, acc.: 82.03%] [G loss: 2.800951]\n",
      "epoch:12 step:9673 [D loss: 0.399367, acc.: 81.25%] [G loss: 2.438823]\n",
      "epoch:12 step:9674 [D loss: 0.376562, acc.: 86.72%] [G loss: 2.833421]\n",
      "epoch:12 step:9675 [D loss: 0.416399, acc.: 81.25%] [G loss: 2.166638]\n",
      "epoch:12 step:9676 [D loss: 0.340397, acc.: 84.38%] [G loss: 3.160354]\n",
      "epoch:12 step:9677 [D loss: 0.244830, acc.: 91.41%] [G loss: 2.802544]\n",
      "epoch:12 step:9678 [D loss: 0.367419, acc.: 84.38%] [G loss: 2.728873]\n",
      "epoch:12 step:9679 [D loss: 0.271917, acc.: 90.62%] [G loss: 3.528906]\n",
      "epoch:12 step:9680 [D loss: 0.354863, acc.: 82.81%] [G loss: 2.894358]\n",
      "epoch:12 step:9681 [D loss: 0.401222, acc.: 78.12%] [G loss: 2.802323]\n",
      "epoch:12 step:9682 [D loss: 0.247419, acc.: 91.41%] [G loss: 2.534106]\n",
      "epoch:12 step:9683 [D loss: 0.399661, acc.: 83.59%] [G loss: 2.541276]\n",
      "epoch:12 step:9684 [D loss: 0.319247, acc.: 84.38%] [G loss: 2.681145]\n",
      "epoch:12 step:9685 [D loss: 0.307842, acc.: 85.16%] [G loss: 3.366825]\n",
      "epoch:12 step:9686 [D loss: 0.397139, acc.: 78.12%] [G loss: 3.115018]\n",
      "epoch:12 step:9687 [D loss: 0.310955, acc.: 89.84%] [G loss: 2.848018]\n",
      "epoch:12 step:9688 [D loss: 0.305407, acc.: 84.38%] [G loss: 3.410308]\n",
      "epoch:12 step:9689 [D loss: 0.283973, acc.: 86.72%] [G loss: 3.238279]\n",
      "epoch:12 step:9690 [D loss: 0.226005, acc.: 90.62%] [G loss: 2.981804]\n",
      "epoch:12 step:9691 [D loss: 0.313094, acc.: 87.50%] [G loss: 4.220650]\n",
      "epoch:12 step:9692 [D loss: 0.241620, acc.: 89.84%] [G loss: 4.009254]\n",
      "epoch:12 step:9693 [D loss: 0.288245, acc.: 88.28%] [G loss: 2.488184]\n",
      "epoch:12 step:9694 [D loss: 0.275524, acc.: 89.84%] [G loss: 3.967733]\n",
      "epoch:12 step:9695 [D loss: 0.249525, acc.: 91.41%] [G loss: 4.018002]\n",
      "epoch:12 step:9696 [D loss: 0.340459, acc.: 86.72%] [G loss: 3.074524]\n",
      "epoch:12 step:9697 [D loss: 0.360047, acc.: 82.81%] [G loss: 2.704684]\n",
      "epoch:12 step:9698 [D loss: 0.357101, acc.: 78.91%] [G loss: 3.567120]\n",
      "epoch:12 step:9699 [D loss: 0.243052, acc.: 88.28%] [G loss: 4.045361]\n",
      "epoch:12 step:9700 [D loss: 0.324202, acc.: 84.38%] [G loss: 3.094740]\n",
      "epoch:12 step:9701 [D loss: 0.296982, acc.: 85.94%] [G loss: 2.902801]\n",
      "epoch:12 step:9702 [D loss: 0.387434, acc.: 83.59%] [G loss: 3.196449]\n",
      "epoch:12 step:9703 [D loss: 0.442382, acc.: 78.91%] [G loss: 2.920003]\n",
      "epoch:12 step:9704 [D loss: 0.453438, acc.: 82.81%] [G loss: 2.415740]\n",
      "epoch:12 step:9705 [D loss: 0.353823, acc.: 85.94%] [G loss: 4.045843]\n",
      "epoch:12 step:9706 [D loss: 0.378218, acc.: 80.47%] [G loss: 3.498841]\n",
      "epoch:12 step:9707 [D loss: 0.290319, acc.: 85.16%] [G loss: 2.396019]\n",
      "epoch:12 step:9708 [D loss: 0.354901, acc.: 86.72%] [G loss: 2.507787]\n",
      "epoch:12 step:9709 [D loss: 0.311489, acc.: 88.28%] [G loss: 2.314005]\n",
      "epoch:12 step:9710 [D loss: 0.434351, acc.: 80.47%] [G loss: 2.310754]\n",
      "epoch:12 step:9711 [D loss: 0.307260, acc.: 85.94%] [G loss: 5.564881]\n",
      "epoch:12 step:9712 [D loss: 0.266183, acc.: 86.72%] [G loss: 6.313733]\n",
      "epoch:12 step:9713 [D loss: 0.342303, acc.: 84.38%] [G loss: 4.635453]\n",
      "epoch:12 step:9714 [D loss: 0.292265, acc.: 87.50%] [G loss: 3.404646]\n",
      "epoch:12 step:9715 [D loss: 0.308111, acc.: 84.38%] [G loss: 3.842863]\n",
      "epoch:12 step:9716 [D loss: 0.349554, acc.: 82.81%] [G loss: 2.543432]\n",
      "epoch:12 step:9717 [D loss: 0.498394, acc.: 76.56%] [G loss: 5.471424]\n",
      "epoch:12 step:9718 [D loss: 0.616647, acc.: 73.44%] [G loss: 5.273088]\n",
      "epoch:12 step:9719 [D loss: 0.790680, acc.: 64.84%] [G loss: 3.554514]\n",
      "epoch:12 step:9720 [D loss: 0.297754, acc.: 85.16%] [G loss: 3.155942]\n",
      "epoch:12 step:9721 [D loss: 0.279996, acc.: 87.50%] [G loss: 3.605792]\n",
      "epoch:12 step:9722 [D loss: 0.335398, acc.: 86.72%] [G loss: 3.745993]\n",
      "epoch:12 step:9723 [D loss: 0.254747, acc.: 87.50%] [G loss: 3.722602]\n",
      "epoch:12 step:9724 [D loss: 0.304721, acc.: 88.28%] [G loss: 3.399447]\n",
      "epoch:12 step:9725 [D loss: 0.261798, acc.: 89.84%] [G loss: 2.524325]\n",
      "epoch:12 step:9726 [D loss: 0.320338, acc.: 86.72%] [G loss: 2.573595]\n",
      "epoch:12 step:9727 [D loss: 0.331413, acc.: 85.16%] [G loss: 2.931245]\n",
      "epoch:12 step:9728 [D loss: 0.275055, acc.: 88.28%] [G loss: 2.540998]\n",
      "epoch:12 step:9729 [D loss: 0.343818, acc.: 85.16%] [G loss: 3.000504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9730 [D loss: 0.322765, acc.: 86.72%] [G loss: 2.128088]\n",
      "epoch:12 step:9731 [D loss: 0.389821, acc.: 84.38%] [G loss: 2.083925]\n",
      "epoch:12 step:9732 [D loss: 0.302936, acc.: 89.06%] [G loss: 2.197859]\n",
      "epoch:12 step:9733 [D loss: 0.299151, acc.: 86.72%] [G loss: 3.046112]\n",
      "epoch:12 step:9734 [D loss: 0.261659, acc.: 91.41%] [G loss: 3.153062]\n",
      "epoch:12 step:9735 [D loss: 0.333556, acc.: 82.81%] [G loss: 3.775692]\n",
      "epoch:12 step:9736 [D loss: 0.284909, acc.: 87.50%] [G loss: 2.264960]\n",
      "epoch:12 step:9737 [D loss: 0.318993, acc.: 85.16%] [G loss: 2.155122]\n",
      "epoch:12 step:9738 [D loss: 0.384782, acc.: 80.47%] [G loss: 2.321743]\n",
      "epoch:12 step:9739 [D loss: 0.392808, acc.: 85.94%] [G loss: 2.287479]\n",
      "epoch:12 step:9740 [D loss: 0.440381, acc.: 80.47%] [G loss: 1.996625]\n",
      "epoch:12 step:9741 [D loss: 0.261849, acc.: 90.62%] [G loss: 2.389211]\n",
      "epoch:12 step:9742 [D loss: 0.446259, acc.: 78.12%] [G loss: 3.009681]\n",
      "epoch:12 step:9743 [D loss: 0.244950, acc.: 91.41%] [G loss: 2.327605]\n",
      "epoch:12 step:9744 [D loss: 0.306501, acc.: 89.84%] [G loss: 3.102607]\n",
      "epoch:12 step:9745 [D loss: 0.265030, acc.: 88.28%] [G loss: 3.219274]\n",
      "epoch:12 step:9746 [D loss: 0.302930, acc.: 86.72%] [G loss: 3.003934]\n",
      "epoch:12 step:9747 [D loss: 0.339291, acc.: 89.06%] [G loss: 2.706034]\n",
      "epoch:12 step:9748 [D loss: 0.266577, acc.: 86.72%] [G loss: 2.733895]\n",
      "epoch:12 step:9749 [D loss: 0.241751, acc.: 89.84%] [G loss: 3.569638]\n",
      "epoch:12 step:9750 [D loss: 0.347195, acc.: 85.16%] [G loss: 2.622424]\n",
      "epoch:12 step:9751 [D loss: 0.255697, acc.: 92.19%] [G loss: 2.556975]\n",
      "epoch:12 step:9752 [D loss: 0.337998, acc.: 87.50%] [G loss: 2.408474]\n",
      "epoch:12 step:9753 [D loss: 0.263955, acc.: 92.97%] [G loss: 2.445308]\n",
      "epoch:12 step:9754 [D loss: 0.355951, acc.: 85.16%] [G loss: 2.654347]\n",
      "epoch:12 step:9755 [D loss: 0.324840, acc.: 85.94%] [G loss: 2.455688]\n",
      "epoch:12 step:9756 [D loss: 0.285360, acc.: 85.16%] [G loss: 4.394468]\n",
      "epoch:12 step:9757 [D loss: 0.275543, acc.: 88.28%] [G loss: 3.186073]\n",
      "epoch:12 step:9758 [D loss: 0.373105, acc.: 84.38%] [G loss: 3.223955]\n",
      "epoch:12 step:9759 [D loss: 0.275198, acc.: 89.06%] [G loss: 2.773491]\n",
      "epoch:12 step:9760 [D loss: 0.319628, acc.: 85.94%] [G loss: 2.894389]\n",
      "epoch:12 step:9761 [D loss: 0.327927, acc.: 85.94%] [G loss: 3.195980]\n",
      "epoch:12 step:9762 [D loss: 0.232116, acc.: 90.62%] [G loss: 3.077956]\n",
      "epoch:12 step:9763 [D loss: 0.175829, acc.: 93.75%] [G loss: 2.907370]\n",
      "epoch:12 step:9764 [D loss: 0.320926, acc.: 82.81%] [G loss: 2.411295]\n",
      "epoch:12 step:9765 [D loss: 0.326595, acc.: 87.50%] [G loss: 2.645674]\n",
      "epoch:12 step:9766 [D loss: 0.416837, acc.: 82.81%] [G loss: 2.400792]\n",
      "epoch:12 step:9767 [D loss: 0.339390, acc.: 85.94%] [G loss: 3.494349]\n",
      "epoch:12 step:9768 [D loss: 0.434348, acc.: 83.59%] [G loss: 2.544439]\n",
      "epoch:12 step:9769 [D loss: 0.342827, acc.: 87.50%] [G loss: 3.825469]\n",
      "epoch:12 step:9770 [D loss: 0.455710, acc.: 75.78%] [G loss: 4.199736]\n",
      "epoch:12 step:9771 [D loss: 0.665775, acc.: 72.66%] [G loss: 6.547683]\n",
      "epoch:12 step:9772 [D loss: 1.456863, acc.: 53.91%] [G loss: 3.240110]\n",
      "epoch:12 step:9773 [D loss: 0.453347, acc.: 81.25%] [G loss: 2.922412]\n",
      "epoch:12 step:9774 [D loss: 0.526632, acc.: 78.91%] [G loss: 3.066620]\n",
      "epoch:12 step:9775 [D loss: 0.272169, acc.: 87.50%] [G loss: 2.932775]\n",
      "epoch:12 step:9776 [D loss: 0.405237, acc.: 82.03%] [G loss: 3.588598]\n",
      "epoch:12 step:9777 [D loss: 0.323544, acc.: 85.16%] [G loss: 3.427385]\n",
      "epoch:12 step:9778 [D loss: 0.397068, acc.: 82.03%] [G loss: 2.518127]\n",
      "epoch:12 step:9779 [D loss: 0.219547, acc.: 92.97%] [G loss: 5.107168]\n",
      "epoch:12 step:9780 [D loss: 0.288529, acc.: 87.50%] [G loss: 2.411821]\n",
      "epoch:12 step:9781 [D loss: 0.244872, acc.: 88.28%] [G loss: 4.160743]\n",
      "epoch:12 step:9782 [D loss: 0.246334, acc.: 90.62%] [G loss: 4.072571]\n",
      "epoch:12 step:9783 [D loss: 0.375491, acc.: 84.38%] [G loss: 3.206996]\n",
      "epoch:12 step:9784 [D loss: 0.318565, acc.: 89.06%] [G loss: 4.057708]\n",
      "epoch:12 step:9785 [D loss: 0.352799, acc.: 85.94%] [G loss: 6.507792]\n",
      "epoch:12 step:9786 [D loss: 0.177988, acc.: 93.75%] [G loss: 4.536768]\n",
      "epoch:12 step:9787 [D loss: 0.580956, acc.: 75.00%] [G loss: 3.056202]\n",
      "epoch:12 step:9788 [D loss: 0.300557, acc.: 87.50%] [G loss: 3.472392]\n",
      "epoch:12 step:9789 [D loss: 0.381342, acc.: 82.81%] [G loss: 3.326711]\n",
      "epoch:12 step:9790 [D loss: 0.352505, acc.: 84.38%] [G loss: 3.039376]\n",
      "epoch:12 step:9791 [D loss: 0.474572, acc.: 75.78%] [G loss: 2.331534]\n",
      "epoch:12 step:9792 [D loss: 0.388545, acc.: 82.81%] [G loss: 3.628870]\n",
      "epoch:12 step:9793 [D loss: 0.325044, acc.: 85.16%] [G loss: 2.790946]\n",
      "epoch:12 step:9794 [D loss: 0.330052, acc.: 85.94%] [G loss: 3.172769]\n",
      "epoch:12 step:9795 [D loss: 0.393699, acc.: 79.69%] [G loss: 3.030510]\n",
      "epoch:12 step:9796 [D loss: 0.334146, acc.: 83.59%] [G loss: 2.415139]\n",
      "epoch:12 step:9797 [D loss: 0.383395, acc.: 85.94%] [G loss: 2.616051]\n",
      "epoch:12 step:9798 [D loss: 0.348062, acc.: 84.38%] [G loss: 2.298981]\n",
      "epoch:12 step:9799 [D loss: 0.293708, acc.: 89.06%] [G loss: 3.074916]\n",
      "epoch:12 step:9800 [D loss: 0.501946, acc.: 75.78%] [G loss: 2.530975]\n",
      "epoch:12 step:9801 [D loss: 0.587080, acc.: 71.09%] [G loss: 3.625098]\n",
      "epoch:12 step:9802 [D loss: 0.485088, acc.: 76.56%] [G loss: 2.844893]\n",
      "epoch:12 step:9803 [D loss: 0.364907, acc.: 85.94%] [G loss: 3.474448]\n",
      "epoch:12 step:9804 [D loss: 0.523518, acc.: 78.12%] [G loss: 4.461758]\n",
      "epoch:12 step:9805 [D loss: 0.379512, acc.: 78.12%] [G loss: 3.322532]\n",
      "epoch:12 step:9806 [D loss: 0.270682, acc.: 85.94%] [G loss: 4.304288]\n",
      "epoch:12 step:9807 [D loss: 0.341167, acc.: 82.81%] [G loss: 4.651107]\n",
      "epoch:12 step:9808 [D loss: 0.370737, acc.: 83.59%] [G loss: 2.838711]\n",
      "epoch:12 step:9809 [D loss: 0.399195, acc.: 89.06%] [G loss: 3.287235]\n",
      "epoch:12 step:9810 [D loss: 0.335514, acc.: 85.16%] [G loss: 3.390220]\n",
      "epoch:12 step:9811 [D loss: 0.303044, acc.: 86.72%] [G loss: 3.833633]\n",
      "epoch:12 step:9812 [D loss: 0.271201, acc.: 88.28%] [G loss: 2.852196]\n",
      "epoch:12 step:9813 [D loss: 0.343688, acc.: 82.81%] [G loss: 2.911657]\n",
      "epoch:12 step:9814 [D loss: 0.221118, acc.: 93.75%] [G loss: 2.589963]\n",
      "epoch:12 step:9815 [D loss: 0.395324, acc.: 82.03%] [G loss: 3.269913]\n",
      "epoch:12 step:9816 [D loss: 0.212888, acc.: 92.19%] [G loss: 3.547968]\n",
      "epoch:12 step:9817 [D loss: 0.377295, acc.: 82.03%] [G loss: 3.363184]\n",
      "epoch:12 step:9818 [D loss: 0.207276, acc.: 91.41%] [G loss: 3.319272]\n",
      "epoch:12 step:9819 [D loss: 0.356587, acc.: 85.16%] [G loss: 2.353447]\n",
      "epoch:12 step:9820 [D loss: 0.303926, acc.: 87.50%] [G loss: 2.584660]\n",
      "epoch:12 step:9821 [D loss: 0.411316, acc.: 85.16%] [G loss: 3.159863]\n",
      "epoch:12 step:9822 [D loss: 0.285513, acc.: 85.16%] [G loss: 3.155592]\n",
      "epoch:12 step:9823 [D loss: 0.339381, acc.: 87.50%] [G loss: 2.844935]\n",
      "epoch:12 step:9824 [D loss: 0.311537, acc.: 88.28%] [G loss: 4.277698]\n",
      "epoch:12 step:9825 [D loss: 0.141481, acc.: 96.09%] [G loss: 5.846995]\n",
      "epoch:12 step:9826 [D loss: 0.232731, acc.: 92.19%] [G loss: 3.913937]\n",
      "epoch:12 step:9827 [D loss: 0.236557, acc.: 91.41%] [G loss: 5.093173]\n",
      "epoch:12 step:9828 [D loss: 0.389159, acc.: 87.50%] [G loss: 3.185833]\n",
      "epoch:12 step:9829 [D loss: 0.300653, acc.: 84.38%] [G loss: 2.553406]\n",
      "epoch:12 step:9830 [D loss: 0.224243, acc.: 92.19%] [G loss: 2.709703]\n",
      "epoch:12 step:9831 [D loss: 0.320481, acc.: 85.94%] [G loss: 3.073911]\n",
      "epoch:12 step:9832 [D loss: 0.275504, acc.: 91.41%] [G loss: 3.387982]\n",
      "epoch:12 step:9833 [D loss: 0.275018, acc.: 86.72%] [G loss: 3.463815]\n",
      "epoch:12 step:9834 [D loss: 0.175872, acc.: 92.97%] [G loss: 5.616005]\n",
      "epoch:12 step:9835 [D loss: 0.245800, acc.: 90.62%] [G loss: 2.965021]\n",
      "epoch:12 step:9836 [D loss: 0.283385, acc.: 92.19%] [G loss: 3.411417]\n",
      "epoch:12 step:9837 [D loss: 0.296069, acc.: 84.38%] [G loss: 3.197927]\n",
      "epoch:12 step:9838 [D loss: 0.365442, acc.: 85.16%] [G loss: 3.311069]\n",
      "epoch:12 step:9839 [D loss: 0.399848, acc.: 83.59%] [G loss: 3.910779]\n",
      "epoch:12 step:9840 [D loss: 0.308793, acc.: 83.59%] [G loss: 2.772199]\n",
      "epoch:12 step:9841 [D loss: 0.293162, acc.: 87.50%] [G loss: 2.486826]\n",
      "epoch:12 step:9842 [D loss: 0.357739, acc.: 85.16%] [G loss: 3.194612]\n",
      "epoch:12 step:9843 [D loss: 0.338989, acc.: 89.84%] [G loss: 3.855513]\n",
      "epoch:12 step:9844 [D loss: 0.259770, acc.: 89.06%] [G loss: 3.189948]\n",
      "epoch:12 step:9845 [D loss: 0.263798, acc.: 90.62%] [G loss: 3.437347]\n",
      "epoch:12 step:9846 [D loss: 0.294625, acc.: 87.50%] [G loss: 3.057698]\n",
      "epoch:12 step:9847 [D loss: 0.304250, acc.: 86.72%] [G loss: 2.132440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9848 [D loss: 0.255412, acc.: 90.62%] [G loss: 2.656479]\n",
      "epoch:12 step:9849 [D loss: 0.310924, acc.: 86.72%] [G loss: 2.340816]\n",
      "epoch:12 step:9850 [D loss: 0.216788, acc.: 93.75%] [G loss: 3.397881]\n",
      "epoch:12 step:9851 [D loss: 0.466547, acc.: 81.25%] [G loss: 2.426422]\n",
      "epoch:12 step:9852 [D loss: 0.259602, acc.: 92.19%] [G loss: 2.792178]\n",
      "epoch:12 step:9853 [D loss: 0.294184, acc.: 87.50%] [G loss: 4.354382]\n",
      "epoch:12 step:9854 [D loss: 0.464663, acc.: 78.12%] [G loss: 5.521142]\n",
      "epoch:12 step:9855 [D loss: 0.520673, acc.: 74.22%] [G loss: 2.591652]\n",
      "epoch:12 step:9856 [D loss: 0.308239, acc.: 88.28%] [G loss: 3.247160]\n",
      "epoch:12 step:9857 [D loss: 0.375369, acc.: 81.25%] [G loss: 2.408457]\n",
      "epoch:12 step:9858 [D loss: 0.321211, acc.: 86.72%] [G loss: 3.038542]\n",
      "epoch:12 step:9859 [D loss: 0.392152, acc.: 82.81%] [G loss: 2.617218]\n",
      "epoch:12 step:9860 [D loss: 0.428837, acc.: 83.59%] [G loss: 3.474949]\n",
      "epoch:12 step:9861 [D loss: 0.455968, acc.: 82.03%] [G loss: 2.679690]\n",
      "epoch:12 step:9862 [D loss: 0.295821, acc.: 88.28%] [G loss: 3.189680]\n",
      "epoch:12 step:9863 [D loss: 0.431287, acc.: 78.91%] [G loss: 4.660938]\n",
      "epoch:12 step:9864 [D loss: 0.527907, acc.: 77.34%] [G loss: 2.932813]\n",
      "epoch:12 step:9865 [D loss: 0.263646, acc.: 89.84%] [G loss: 3.267705]\n",
      "epoch:12 step:9866 [D loss: 0.345699, acc.: 82.03%] [G loss: 3.043513]\n",
      "epoch:12 step:9867 [D loss: 0.210592, acc.: 95.31%] [G loss: 4.511090]\n",
      "epoch:12 step:9868 [D loss: 0.361033, acc.: 82.03%] [G loss: 3.656617]\n",
      "epoch:12 step:9869 [D loss: 0.314906, acc.: 86.72%] [G loss: 3.158718]\n",
      "epoch:12 step:9870 [D loss: 0.299854, acc.: 87.50%] [G loss: 3.499725]\n",
      "epoch:12 step:9871 [D loss: 0.242784, acc.: 87.50%] [G loss: 2.881635]\n",
      "epoch:12 step:9872 [D loss: 0.327835, acc.: 85.94%] [G loss: 3.522450]\n",
      "epoch:12 step:9873 [D loss: 0.268308, acc.: 89.84%] [G loss: 3.436941]\n",
      "epoch:12 step:9874 [D loss: 0.332633, acc.: 85.94%] [G loss: 2.807873]\n",
      "epoch:12 step:9875 [D loss: 0.254749, acc.: 87.50%] [G loss: 2.954132]\n",
      "epoch:12 step:9876 [D loss: 0.285706, acc.: 89.06%] [G loss: 2.885122]\n",
      "epoch:12 step:9877 [D loss: 0.246292, acc.: 89.06%] [G loss: 3.608323]\n",
      "epoch:12 step:9878 [D loss: 0.369225, acc.: 82.81%] [G loss: 2.892152]\n",
      "epoch:12 step:9879 [D loss: 0.259182, acc.: 90.62%] [G loss: 2.894413]\n",
      "epoch:12 step:9880 [D loss: 0.301562, acc.: 88.28%] [G loss: 3.444948]\n",
      "epoch:12 step:9881 [D loss: 0.303254, acc.: 86.72%] [G loss: 5.129268]\n",
      "epoch:12 step:9882 [D loss: 0.394738, acc.: 79.69%] [G loss: 3.021821]\n",
      "epoch:12 step:9883 [D loss: 0.286352, acc.: 87.50%] [G loss: 2.723983]\n",
      "epoch:12 step:9884 [D loss: 0.245718, acc.: 91.41%] [G loss: 3.065521]\n",
      "epoch:12 step:9885 [D loss: 0.274791, acc.: 92.97%] [G loss: 2.562574]\n",
      "epoch:12 step:9886 [D loss: 0.275692, acc.: 87.50%] [G loss: 2.741241]\n",
      "epoch:12 step:9887 [D loss: 0.347323, acc.: 85.16%] [G loss: 3.330447]\n",
      "epoch:12 step:9888 [D loss: 0.258806, acc.: 89.06%] [G loss: 3.211381]\n",
      "epoch:12 step:9889 [D loss: 0.402526, acc.: 81.25%] [G loss: 3.538952]\n",
      "epoch:12 step:9890 [D loss: 0.341855, acc.: 85.16%] [G loss: 3.153983]\n",
      "epoch:12 step:9891 [D loss: 0.504283, acc.: 78.91%] [G loss: 2.723774]\n",
      "epoch:12 step:9892 [D loss: 0.434277, acc.: 82.81%] [G loss: 2.747341]\n",
      "epoch:12 step:9893 [D loss: 0.474331, acc.: 79.69%] [G loss: 2.523196]\n",
      "epoch:12 step:9894 [D loss: 0.371056, acc.: 85.16%] [G loss: 3.150553]\n",
      "epoch:12 step:9895 [D loss: 0.365321, acc.: 82.03%] [G loss: 3.713496]\n",
      "epoch:12 step:9896 [D loss: 0.302369, acc.: 88.28%] [G loss: 6.791203]\n",
      "epoch:12 step:9897 [D loss: 0.242050, acc.: 90.62%] [G loss: 3.393213]\n",
      "epoch:12 step:9898 [D loss: 0.308493, acc.: 85.94%] [G loss: 4.779324]\n",
      "epoch:12 step:9899 [D loss: 0.340777, acc.: 84.38%] [G loss: 3.863142]\n",
      "epoch:12 step:9900 [D loss: 0.249637, acc.: 90.62%] [G loss: 4.158517]\n",
      "epoch:12 step:9901 [D loss: 0.417714, acc.: 84.38%] [G loss: 2.961252]\n",
      "epoch:12 step:9902 [D loss: 0.486965, acc.: 75.78%] [G loss: 3.226699]\n",
      "epoch:12 step:9903 [D loss: 0.395393, acc.: 80.47%] [G loss: 6.980505]\n",
      "epoch:12 step:9904 [D loss: 0.473561, acc.: 81.25%] [G loss: 2.292796]\n",
      "epoch:12 step:9905 [D loss: 0.287915, acc.: 93.75%] [G loss: 3.880236]\n",
      "epoch:12 step:9906 [D loss: 0.273375, acc.: 88.28%] [G loss: 3.126215]\n",
      "epoch:12 step:9907 [D loss: 0.426311, acc.: 83.59%] [G loss: 3.011554]\n",
      "epoch:12 step:9908 [D loss: 0.350195, acc.: 84.38%] [G loss: 3.344306]\n",
      "epoch:12 step:9909 [D loss: 0.353246, acc.: 82.81%] [G loss: 3.329147]\n",
      "epoch:12 step:9910 [D loss: 0.306232, acc.: 91.41%] [G loss: 2.707644]\n",
      "epoch:12 step:9911 [D loss: 0.297064, acc.: 90.62%] [G loss: 3.523503]\n",
      "epoch:12 step:9912 [D loss: 0.323552, acc.: 83.59%] [G loss: 7.191453]\n",
      "epoch:12 step:9913 [D loss: 0.423507, acc.: 81.25%] [G loss: 4.375319]\n",
      "epoch:12 step:9914 [D loss: 0.207152, acc.: 91.41%] [G loss: 8.304122]\n",
      "epoch:12 step:9915 [D loss: 0.238478, acc.: 88.28%] [G loss: 7.030992]\n",
      "epoch:12 step:9916 [D loss: 0.314086, acc.: 88.28%] [G loss: 6.864037]\n",
      "epoch:12 step:9917 [D loss: 0.234536, acc.: 91.41%] [G loss: 5.127920]\n",
      "epoch:12 step:9918 [D loss: 0.149330, acc.: 92.97%] [G loss: 4.765232]\n",
      "epoch:12 step:9919 [D loss: 0.354658, acc.: 77.34%] [G loss: 4.614820]\n",
      "epoch:12 step:9920 [D loss: 0.169551, acc.: 93.75%] [G loss: 5.918227]\n",
      "epoch:12 step:9921 [D loss: 0.301937, acc.: 87.50%] [G loss: 2.548798]\n",
      "epoch:12 step:9922 [D loss: 0.271243, acc.: 85.16%] [G loss: 3.479590]\n",
      "epoch:12 step:9923 [D loss: 0.286528, acc.: 87.50%] [G loss: 2.810349]\n",
      "epoch:12 step:9924 [D loss: 0.473500, acc.: 83.59%] [G loss: 2.805819]\n",
      "epoch:12 step:9925 [D loss: 0.274764, acc.: 86.72%] [G loss: 2.769854]\n",
      "epoch:12 step:9926 [D loss: 0.388712, acc.: 83.59%] [G loss: 2.998134]\n",
      "epoch:12 step:9927 [D loss: 0.361351, acc.: 85.94%] [G loss: 2.144543]\n",
      "epoch:12 step:9928 [D loss: 0.473668, acc.: 76.56%] [G loss: 2.663489]\n",
      "epoch:12 step:9929 [D loss: 0.326649, acc.: 86.72%] [G loss: 2.806591]\n",
      "epoch:12 step:9930 [D loss: 0.449982, acc.: 78.91%] [G loss: 4.207646]\n",
      "epoch:12 step:9931 [D loss: 0.324804, acc.: 88.28%] [G loss: 4.901403]\n",
      "epoch:12 step:9932 [D loss: 0.282992, acc.: 89.06%] [G loss: 3.670168]\n",
      "epoch:12 step:9933 [D loss: 0.373782, acc.: 86.72%] [G loss: 3.646838]\n",
      "epoch:12 step:9934 [D loss: 0.405536, acc.: 82.03%] [G loss: 2.870970]\n",
      "epoch:12 step:9935 [D loss: 0.279764, acc.: 88.28%] [G loss: 2.642766]\n",
      "epoch:12 step:9936 [D loss: 0.381441, acc.: 82.03%] [G loss: 3.042685]\n",
      "epoch:12 step:9937 [D loss: 0.285530, acc.: 90.62%] [G loss: 3.699786]\n",
      "epoch:12 step:9938 [D loss: 0.311659, acc.: 85.94%] [G loss: 2.636054]\n",
      "epoch:12 step:9939 [D loss: 0.310959, acc.: 85.94%] [G loss: 2.260603]\n",
      "epoch:12 step:9940 [D loss: 0.253813, acc.: 89.84%] [G loss: 2.843171]\n",
      "epoch:12 step:9941 [D loss: 0.278104, acc.: 91.41%] [G loss: 3.735647]\n",
      "epoch:12 step:9942 [D loss: 0.376155, acc.: 82.03%] [G loss: 2.237153]\n",
      "epoch:12 step:9943 [D loss: 0.309980, acc.: 86.72%] [G loss: 3.553738]\n",
      "epoch:12 step:9944 [D loss: 0.343819, acc.: 87.50%] [G loss: 3.827762]\n",
      "epoch:12 step:9945 [D loss: 0.347700, acc.: 83.59%] [G loss: 3.117735]\n",
      "epoch:12 step:9946 [D loss: 0.428786, acc.: 80.47%] [G loss: 2.451121]\n",
      "epoch:12 step:9947 [D loss: 0.321672, acc.: 89.06%] [G loss: 2.402937]\n",
      "epoch:12 step:9948 [D loss: 0.435165, acc.: 80.47%] [G loss: 2.750539]\n",
      "epoch:12 step:9949 [D loss: 0.304105, acc.: 85.94%] [G loss: 3.149125]\n",
      "epoch:12 step:9950 [D loss: 0.366599, acc.: 82.03%] [G loss: 4.462797]\n",
      "epoch:12 step:9951 [D loss: 0.418200, acc.: 85.16%] [G loss: 3.874095]\n",
      "epoch:12 step:9952 [D loss: 0.317122, acc.: 83.59%] [G loss: 4.067737]\n",
      "epoch:12 step:9953 [D loss: 0.402742, acc.: 83.59%] [G loss: 2.611633]\n",
      "epoch:12 step:9954 [D loss: 0.358971, acc.: 79.69%] [G loss: 3.827569]\n",
      "epoch:12 step:9955 [D loss: 0.338979, acc.: 86.72%] [G loss: 4.978953]\n",
      "epoch:12 step:9956 [D loss: 0.334825, acc.: 89.84%] [G loss: 3.132646]\n",
      "epoch:12 step:9957 [D loss: 0.299582, acc.: 86.72%] [G loss: 5.692716]\n",
      "epoch:12 step:9958 [D loss: 0.278590, acc.: 88.28%] [G loss: 4.010836]\n",
      "epoch:12 step:9959 [D loss: 0.300018, acc.: 87.50%] [G loss: 3.006834]\n",
      "epoch:12 step:9960 [D loss: 0.364906, acc.: 85.94%] [G loss: 5.319773]\n",
      "epoch:12 step:9961 [D loss: 0.412739, acc.: 81.25%] [G loss: 2.508754]\n",
      "epoch:12 step:9962 [D loss: 0.336826, acc.: 85.16%] [G loss: 3.519149]\n",
      "epoch:12 step:9963 [D loss: 0.359126, acc.: 78.12%] [G loss: 3.226329]\n",
      "epoch:12 step:9964 [D loss: 0.559990, acc.: 71.09%] [G loss: 2.674619]\n",
      "epoch:12 step:9965 [D loss: 0.389288, acc.: 83.59%] [G loss: 3.593681]\n",
      "epoch:12 step:9966 [D loss: 0.274284, acc.: 89.06%] [G loss: 3.647145]\n",
      "epoch:12 step:9967 [D loss: 0.496014, acc.: 78.91%] [G loss: 3.051002]\n",
      "epoch:12 step:9968 [D loss: 0.372781, acc.: 81.25%] [G loss: 3.602446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9969 [D loss: 0.336880, acc.: 83.59%] [G loss: 3.176105]\n",
      "epoch:12 step:9970 [D loss: 0.458909, acc.: 79.69%] [G loss: 3.048956]\n",
      "epoch:12 step:9971 [D loss: 0.463281, acc.: 82.81%] [G loss: 2.565133]\n",
      "epoch:12 step:9972 [D loss: 0.385666, acc.: 85.94%] [G loss: 2.919549]\n",
      "epoch:12 step:9973 [D loss: 0.369771, acc.: 85.94%] [G loss: 2.538008]\n",
      "epoch:12 step:9974 [D loss: 0.277256, acc.: 89.84%] [G loss: 3.047485]\n",
      "epoch:12 step:9975 [D loss: 0.326911, acc.: 85.16%] [G loss: 3.053794]\n",
      "epoch:12 step:9976 [D loss: 0.279571, acc.: 90.62%] [G loss: 2.689192]\n",
      "epoch:12 step:9977 [D loss: 0.399124, acc.: 81.25%] [G loss: 2.020258]\n",
      "epoch:12 step:9978 [D loss: 0.318426, acc.: 85.94%] [G loss: 2.924968]\n",
      "epoch:12 step:9979 [D loss: 0.359000, acc.: 85.16%] [G loss: 2.176007]\n",
      "epoch:12 step:9980 [D loss: 0.502373, acc.: 79.69%] [G loss: 4.064708]\n",
      "epoch:12 step:9981 [D loss: 0.350671, acc.: 85.94%] [G loss: 3.579021]\n",
      "epoch:12 step:9982 [D loss: 0.470672, acc.: 78.12%] [G loss: 2.307957]\n",
      "epoch:12 step:9983 [D loss: 0.317872, acc.: 86.72%] [G loss: 2.706392]\n",
      "epoch:12 step:9984 [D loss: 0.229002, acc.: 91.41%] [G loss: 3.967485]\n",
      "epoch:12 step:9985 [D loss: 0.400296, acc.: 82.03%] [G loss: 2.795480]\n",
      "epoch:12 step:9986 [D loss: 0.335593, acc.: 87.50%] [G loss: 2.727451]\n",
      "epoch:12 step:9987 [D loss: 0.256668, acc.: 89.06%] [G loss: 3.298568]\n",
      "epoch:12 step:9988 [D loss: 0.357508, acc.: 85.16%] [G loss: 4.118337]\n",
      "epoch:12 step:9989 [D loss: 0.416601, acc.: 81.25%] [G loss: 2.420272]\n",
      "epoch:12 step:9990 [D loss: 0.373522, acc.: 81.25%] [G loss: 2.812101]\n",
      "epoch:12 step:9991 [D loss: 0.337115, acc.: 86.72%] [G loss: 2.554820]\n",
      "epoch:12 step:9992 [D loss: 0.378909, acc.: 82.81%] [G loss: 2.687265]\n",
      "epoch:12 step:9993 [D loss: 0.336897, acc.: 86.72%] [G loss: 2.794605]\n",
      "epoch:12 step:9994 [D loss: 0.287402, acc.: 87.50%] [G loss: 3.236216]\n",
      "epoch:12 step:9995 [D loss: 0.244377, acc.: 89.06%] [G loss: 4.227059]\n",
      "epoch:12 step:9996 [D loss: 0.312193, acc.: 84.38%] [G loss: 3.043849]\n",
      "epoch:12 step:9997 [D loss: 0.498203, acc.: 75.78%] [G loss: 2.351974]\n",
      "epoch:12 step:9998 [D loss: 0.248366, acc.: 88.28%] [G loss: 3.010543]\n",
      "epoch:12 step:9999 [D loss: 0.350919, acc.: 83.59%] [G loss: 2.832757]\n",
      "epoch:12 step:10000 [D loss: 0.402703, acc.: 80.47%] [G loss: 4.506859]\n",
      "epoch:12 step:10001 [D loss: 0.454219, acc.: 78.91%] [G loss: 3.806324]\n",
      "epoch:12 step:10002 [D loss: 0.368098, acc.: 83.59%] [G loss: 3.726619]\n",
      "epoch:12 step:10003 [D loss: 0.387375, acc.: 80.47%] [G loss: 2.784189]\n",
      "epoch:12 step:10004 [D loss: 0.337743, acc.: 84.38%] [G loss: 4.377769]\n",
      "epoch:12 step:10005 [D loss: 0.371552, acc.: 85.94%] [G loss: 4.168227]\n",
      "epoch:12 step:10006 [D loss: 0.272002, acc.: 89.06%] [G loss: 3.621716]\n",
      "epoch:12 step:10007 [D loss: 0.350459, acc.: 86.72%] [G loss: 3.220429]\n",
      "epoch:12 step:10008 [D loss: 0.263556, acc.: 88.28%] [G loss: 3.301629]\n",
      "epoch:12 step:10009 [D loss: 0.351910, acc.: 87.50%] [G loss: 2.632009]\n",
      "epoch:12 step:10010 [D loss: 0.341748, acc.: 85.94%] [G loss: 2.898643]\n",
      "epoch:12 step:10011 [D loss: 0.298426, acc.: 86.72%] [G loss: 2.990394]\n",
      "epoch:12 step:10012 [D loss: 0.296476, acc.: 89.84%] [G loss: 2.956418]\n",
      "epoch:12 step:10013 [D loss: 0.384228, acc.: 83.59%] [G loss: 2.988463]\n",
      "epoch:12 step:10014 [D loss: 0.365334, acc.: 86.72%] [G loss: 3.475212]\n",
      "epoch:12 step:10015 [D loss: 0.370329, acc.: 81.25%] [G loss: 2.458376]\n",
      "epoch:12 step:10016 [D loss: 0.345463, acc.: 84.38%] [G loss: 2.684832]\n",
      "epoch:12 step:10017 [D loss: 0.317722, acc.: 87.50%] [G loss: 2.457207]\n",
      "epoch:12 step:10018 [D loss: 0.368588, acc.: 85.94%] [G loss: 4.506600]\n",
      "epoch:12 step:10019 [D loss: 0.386706, acc.: 81.25%] [G loss: 4.932142]\n",
      "epoch:12 step:10020 [D loss: 0.389197, acc.: 82.81%] [G loss: 4.399208]\n",
      "epoch:12 step:10021 [D loss: 0.326594, acc.: 83.59%] [G loss: 3.582609]\n",
      "epoch:12 step:10022 [D loss: 0.314905, acc.: 88.28%] [G loss: 2.372126]\n",
      "epoch:12 step:10023 [D loss: 0.308527, acc.: 85.16%] [G loss: 4.591357]\n",
      "epoch:12 step:10024 [D loss: 0.217301, acc.: 91.41%] [G loss: 3.625728]\n",
      "epoch:12 step:10025 [D loss: 0.300957, acc.: 88.28%] [G loss: 3.535951]\n",
      "epoch:12 step:10026 [D loss: 0.301800, acc.: 87.50%] [G loss: 2.636189]\n",
      "epoch:12 step:10027 [D loss: 0.303554, acc.: 87.50%] [G loss: 2.458309]\n",
      "epoch:12 step:10028 [D loss: 0.355816, acc.: 82.03%] [G loss: 2.661677]\n",
      "epoch:12 step:10029 [D loss: 0.495099, acc.: 78.12%] [G loss: 2.685171]\n",
      "epoch:12 step:10030 [D loss: 0.315333, acc.: 85.16%] [G loss: 3.796049]\n",
      "epoch:12 step:10031 [D loss: 0.353763, acc.: 83.59%] [G loss: 3.316858]\n",
      "epoch:12 step:10032 [D loss: 0.371090, acc.: 84.38%] [G loss: 2.950815]\n",
      "epoch:12 step:10033 [D loss: 0.361250, acc.: 85.16%] [G loss: 5.255768]\n",
      "epoch:12 step:10034 [D loss: 0.406045, acc.: 82.03%] [G loss: 3.052931]\n",
      "epoch:12 step:10035 [D loss: 0.315084, acc.: 89.06%] [G loss: 2.161286]\n",
      "epoch:12 step:10036 [D loss: 0.410257, acc.: 80.47%] [G loss: 3.648265]\n",
      "epoch:12 step:10037 [D loss: 0.270534, acc.: 85.94%] [G loss: 5.774539]\n",
      "epoch:12 step:10038 [D loss: 0.326256, acc.: 89.84%] [G loss: 2.871525]\n",
      "epoch:12 step:10039 [D loss: 0.246974, acc.: 88.28%] [G loss: 3.899533]\n",
      "epoch:12 step:10040 [D loss: 0.267753, acc.: 88.28%] [G loss: 5.925848]\n",
      "epoch:12 step:10041 [D loss: 0.260025, acc.: 88.28%] [G loss: 4.152342]\n",
      "epoch:12 step:10042 [D loss: 0.363209, acc.: 83.59%] [G loss: 2.476774]\n",
      "epoch:12 step:10043 [D loss: 0.347393, acc.: 82.81%] [G loss: 2.533091]\n",
      "epoch:12 step:10044 [D loss: 0.315742, acc.: 85.94%] [G loss: 2.784781]\n",
      "epoch:12 step:10045 [D loss: 0.356162, acc.: 85.16%] [G loss: 3.905732]\n",
      "epoch:12 step:10046 [D loss: 0.418730, acc.: 81.25%] [G loss: 3.953965]\n",
      "epoch:12 step:10047 [D loss: 0.612669, acc.: 75.00%] [G loss: 8.198013]\n",
      "epoch:12 step:10048 [D loss: 2.036396, acc.: 54.69%] [G loss: 8.794688]\n",
      "epoch:12 step:10049 [D loss: 2.295555, acc.: 42.97%] [G loss: 3.308728]\n",
      "epoch:12 step:10050 [D loss: 0.780999, acc.: 72.66%] [G loss: 4.136562]\n",
      "epoch:12 step:10051 [D loss: 0.897633, acc.: 74.22%] [G loss: 4.749674]\n",
      "epoch:12 step:10052 [D loss: 0.243279, acc.: 88.28%] [G loss: 3.929776]\n",
      "epoch:12 step:10053 [D loss: 0.517188, acc.: 79.69%] [G loss: 3.519573]\n",
      "epoch:12 step:10054 [D loss: 0.425944, acc.: 79.69%] [G loss: 3.509043]\n",
      "epoch:12 step:10055 [D loss: 0.352477, acc.: 83.59%] [G loss: 2.332910]\n",
      "epoch:12 step:10056 [D loss: 0.384614, acc.: 89.84%] [G loss: 2.430779]\n",
      "epoch:12 step:10057 [D loss: 0.346608, acc.: 85.16%] [G loss: 3.313560]\n",
      "epoch:12 step:10058 [D loss: 0.333850, acc.: 85.16%] [G loss: 3.939015]\n",
      "epoch:12 step:10059 [D loss: 0.304337, acc.: 84.38%] [G loss: 2.593460]\n",
      "epoch:12 step:10060 [D loss: 0.360377, acc.: 82.03%] [G loss: 2.599438]\n",
      "epoch:12 step:10061 [D loss: 0.278769, acc.: 92.97%] [G loss: 3.200463]\n",
      "epoch:12 step:10062 [D loss: 0.294714, acc.: 89.84%] [G loss: 3.010750]\n",
      "epoch:12 step:10063 [D loss: 0.291938, acc.: 90.62%] [G loss: 2.456131]\n",
      "epoch:12 step:10064 [D loss: 0.424665, acc.: 79.69%] [G loss: 2.856394]\n",
      "epoch:12 step:10065 [D loss: 0.291817, acc.: 90.62%] [G loss: 3.445068]\n",
      "epoch:12 step:10066 [D loss: 0.346186, acc.: 85.16%] [G loss: 2.754124]\n",
      "epoch:12 step:10067 [D loss: 0.268554, acc.: 89.06%] [G loss: 2.919107]\n",
      "epoch:12 step:10068 [D loss: 0.362201, acc.: 81.25%] [G loss: 3.069249]\n",
      "epoch:12 step:10069 [D loss: 0.486572, acc.: 78.12%] [G loss: 1.942657]\n",
      "epoch:12 step:10070 [D loss: 0.328882, acc.: 84.38%] [G loss: 2.253083]\n",
      "epoch:12 step:10071 [D loss: 0.369719, acc.: 81.25%] [G loss: 2.542434]\n",
      "epoch:12 step:10072 [D loss: 0.421897, acc.: 81.25%] [G loss: 2.393773]\n",
      "epoch:12 step:10073 [D loss: 0.488939, acc.: 77.34%] [G loss: 2.344690]\n",
      "epoch:12 step:10074 [D loss: 0.348456, acc.: 84.38%] [G loss: 2.072092]\n",
      "epoch:12 step:10075 [D loss: 0.426172, acc.: 78.12%] [G loss: 3.375513]\n",
      "epoch:12 step:10076 [D loss: 0.451272, acc.: 77.34%] [G loss: 2.680403]\n",
      "epoch:12 step:10077 [D loss: 0.367690, acc.: 84.38%] [G loss: 2.927674]\n",
      "epoch:12 step:10078 [D loss: 0.492138, acc.: 79.69%] [G loss: 3.128511]\n",
      "epoch:12 step:10079 [D loss: 0.451458, acc.: 79.69%] [G loss: 2.990928]\n",
      "epoch:12 step:10080 [D loss: 0.337988, acc.: 87.50%] [G loss: 3.090985]\n",
      "epoch:12 step:10081 [D loss: 0.388515, acc.: 82.81%] [G loss: 2.614978]\n",
      "epoch:12 step:10082 [D loss: 0.321042, acc.: 87.50%] [G loss: 3.049690]\n",
      "epoch:12 step:10083 [D loss: 0.382269, acc.: 83.59%] [G loss: 2.614195]\n",
      "epoch:12 step:10084 [D loss: 0.363389, acc.: 84.38%] [G loss: 2.855674]\n",
      "epoch:12 step:10085 [D loss: 0.339126, acc.: 83.59%] [G loss: 2.376449]\n",
      "epoch:12 step:10086 [D loss: 0.388106, acc.: 82.03%] [G loss: 2.218411]\n",
      "epoch:12 step:10087 [D loss: 0.385080, acc.: 82.81%] [G loss: 2.175074]\n",
      "epoch:12 step:10088 [D loss: 0.286803, acc.: 87.50%] [G loss: 2.421035]\n",
      "epoch:12 step:10089 [D loss: 0.274799, acc.: 89.84%] [G loss: 3.425453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:10090 [D loss: 0.346532, acc.: 83.59%] [G loss: 3.497275]\n",
      "epoch:12 step:10091 [D loss: 0.253856, acc.: 89.84%] [G loss: 2.807603]\n",
      "epoch:12 step:10092 [D loss: 0.289088, acc.: 87.50%] [G loss: 2.591545]\n",
      "epoch:12 step:10093 [D loss: 0.419775, acc.: 81.25%] [G loss: 2.495059]\n",
      "epoch:12 step:10094 [D loss: 0.278316, acc.: 88.28%] [G loss: 2.474251]\n",
      "epoch:12 step:10095 [D loss: 0.231875, acc.: 90.62%] [G loss: 3.634590]\n",
      "epoch:12 step:10096 [D loss: 0.214345, acc.: 89.84%] [G loss: 3.299334]\n",
      "epoch:12 step:10097 [D loss: 0.321670, acc.: 87.50%] [G loss: 2.404780]\n",
      "epoch:12 step:10098 [D loss: 0.286827, acc.: 89.06%] [G loss: 2.809952]\n",
      "epoch:12 step:10099 [D loss: 0.342404, acc.: 85.94%] [G loss: 2.468782]\n",
      "epoch:12 step:10100 [D loss: 0.324992, acc.: 88.28%] [G loss: 1.884225]\n",
      "epoch:12 step:10101 [D loss: 0.316040, acc.: 87.50%] [G loss: 2.671619]\n",
      "epoch:12 step:10102 [D loss: 0.280188, acc.: 89.06%] [G loss: 2.612499]\n",
      "epoch:12 step:10103 [D loss: 0.304370, acc.: 86.72%] [G loss: 2.405829]\n",
      "epoch:12 step:10104 [D loss: 0.394108, acc.: 82.81%] [G loss: 2.379153]\n",
      "epoch:12 step:10105 [D loss: 0.393806, acc.: 87.50%] [G loss: 2.008651]\n",
      "epoch:12 step:10106 [D loss: 0.435652, acc.: 77.34%] [G loss: 2.565076]\n",
      "epoch:12 step:10107 [D loss: 0.397585, acc.: 82.81%] [G loss: 1.884546]\n",
      "epoch:12 step:10108 [D loss: 0.329058, acc.: 86.72%] [G loss: 2.499472]\n",
      "epoch:12 step:10109 [D loss: 0.362151, acc.: 83.59%] [G loss: 2.477255]\n",
      "epoch:12 step:10110 [D loss: 0.380811, acc.: 85.16%] [G loss: 2.121494]\n",
      "epoch:12 step:10111 [D loss: 0.314213, acc.: 88.28%] [G loss: 2.826468]\n",
      "epoch:12 step:10112 [D loss: 0.253558, acc.: 88.28%] [G loss: 3.243740]\n",
      "epoch:12 step:10113 [D loss: 0.342739, acc.: 85.94%] [G loss: 3.768998]\n",
      "epoch:12 step:10114 [D loss: 0.320773, acc.: 85.16%] [G loss: 4.445001]\n",
      "epoch:12 step:10115 [D loss: 0.290572, acc.: 89.84%] [G loss: 5.176246]\n",
      "epoch:12 step:10116 [D loss: 0.227797, acc.: 95.31%] [G loss: 2.950454]\n",
      "epoch:12 step:10117 [D loss: 0.339218, acc.: 85.16%] [G loss: 2.910727]\n",
      "epoch:12 step:10118 [D loss: 0.488304, acc.: 75.00%] [G loss: 2.460299]\n",
      "epoch:12 step:10119 [D loss: 0.210100, acc.: 91.41%] [G loss: 3.612022]\n",
      "epoch:12 step:10120 [D loss: 0.257160, acc.: 89.84%] [G loss: 3.486058]\n",
      "epoch:12 step:10121 [D loss: 0.280973, acc.: 86.72%] [G loss: 3.068354]\n",
      "epoch:12 step:10122 [D loss: 0.275394, acc.: 89.84%] [G loss: 4.720896]\n",
      "epoch:12 step:10123 [D loss: 0.248140, acc.: 87.50%] [G loss: 4.372744]\n",
      "epoch:12 step:10124 [D loss: 0.502422, acc.: 80.47%] [G loss: 2.599936]\n",
      "epoch:12 step:10125 [D loss: 0.290251, acc.: 89.06%] [G loss: 2.985501]\n",
      "epoch:12 step:10126 [D loss: 0.370787, acc.: 85.94%] [G loss: 3.206763]\n",
      "epoch:12 step:10127 [D loss: 0.275762, acc.: 89.06%] [G loss: 3.225213]\n",
      "epoch:12 step:10128 [D loss: 0.360696, acc.: 84.38%] [G loss: 2.235797]\n",
      "epoch:12 step:10129 [D loss: 0.311456, acc.: 85.16%] [G loss: 2.449952]\n",
      "epoch:12 step:10130 [D loss: 0.367776, acc.: 85.94%] [G loss: 2.226660]\n",
      "epoch:12 step:10131 [D loss: 0.312028, acc.: 88.28%] [G loss: 2.732861]\n",
      "epoch:12 step:10132 [D loss: 0.213383, acc.: 92.19%] [G loss: 3.043965]\n",
      "epoch:12 step:10133 [D loss: 0.220072, acc.: 92.97%] [G loss: 3.878303]\n",
      "epoch:12 step:10134 [D loss: 0.287372, acc.: 89.84%] [G loss: 2.657604]\n",
      "epoch:12 step:10135 [D loss: 0.267763, acc.: 91.41%] [G loss: 2.789721]\n",
      "epoch:12 step:10136 [D loss: 0.237499, acc.: 91.41%] [G loss: 2.333204]\n",
      "epoch:12 step:10137 [D loss: 0.406990, acc.: 81.25%] [G loss: 2.234007]\n",
      "epoch:12 step:10138 [D loss: 0.314445, acc.: 87.50%] [G loss: 2.968018]\n",
      "epoch:12 step:10139 [D loss: 0.338640, acc.: 88.28%] [G loss: 2.726155]\n",
      "epoch:12 step:10140 [D loss: 0.318764, acc.: 85.16%] [G loss: 3.268609]\n",
      "epoch:12 step:10141 [D loss: 0.257381, acc.: 89.06%] [G loss: 3.552274]\n",
      "epoch:12 step:10142 [D loss: 0.349094, acc.: 85.16%] [G loss: 2.713507]\n",
      "epoch:12 step:10143 [D loss: 0.407047, acc.: 78.91%] [G loss: 2.487886]\n",
      "epoch:12 step:10144 [D loss: 0.280075, acc.: 89.06%] [G loss: 3.007866]\n",
      "epoch:12 step:10145 [D loss: 0.228924, acc.: 92.19%] [G loss: 4.508361]\n",
      "epoch:12 step:10146 [D loss: 0.221996, acc.: 93.75%] [G loss: 3.806873]\n",
      "epoch:12 step:10147 [D loss: 0.275057, acc.: 88.28%] [G loss: 3.507576]\n",
      "epoch:12 step:10148 [D loss: 0.231736, acc.: 92.19%] [G loss: 3.804729]\n",
      "epoch:12 step:10149 [D loss: 0.330055, acc.: 83.59%] [G loss: 3.691032]\n",
      "epoch:12 step:10150 [D loss: 0.275438, acc.: 89.06%] [G loss: 3.201177]\n",
      "epoch:12 step:10151 [D loss: 0.366811, acc.: 82.03%] [G loss: 2.918472]\n",
      "epoch:12 step:10152 [D loss: 0.267171, acc.: 90.62%] [G loss: 2.455882]\n",
      "epoch:12 step:10153 [D loss: 0.356837, acc.: 81.25%] [G loss: 3.300066]\n",
      "epoch:13 step:10154 [D loss: 0.345964, acc.: 83.59%] [G loss: 2.683789]\n",
      "epoch:13 step:10155 [D loss: 0.351950, acc.: 85.16%] [G loss: 2.163554]\n",
      "epoch:13 step:10156 [D loss: 0.418830, acc.: 77.34%] [G loss: 3.447453]\n",
      "epoch:13 step:10157 [D loss: 0.268714, acc.: 89.06%] [G loss: 3.824933]\n",
      "epoch:13 step:10158 [D loss: 0.279299, acc.: 92.19%] [G loss: 2.656104]\n",
      "epoch:13 step:10159 [D loss: 0.275800, acc.: 86.72%] [G loss: 3.155286]\n",
      "epoch:13 step:10160 [D loss: 0.365143, acc.: 85.16%] [G loss: 2.588348]\n",
      "epoch:13 step:10161 [D loss: 0.375818, acc.: 85.94%] [G loss: 2.869323]\n",
      "epoch:13 step:10162 [D loss: 0.256373, acc.: 86.72%] [G loss: 3.944896]\n",
      "epoch:13 step:10163 [D loss: 0.286495, acc.: 89.06%] [G loss: 4.577005]\n",
      "epoch:13 step:10164 [D loss: 0.367248, acc.: 82.03%] [G loss: 3.104678]\n",
      "epoch:13 step:10165 [D loss: 0.283741, acc.: 88.28%] [G loss: 3.806928]\n",
      "epoch:13 step:10166 [D loss: 0.369124, acc.: 83.59%] [G loss: 4.075751]\n",
      "epoch:13 step:10167 [D loss: 0.410992, acc.: 81.25%] [G loss: 2.589665]\n",
      "epoch:13 step:10168 [D loss: 0.476452, acc.: 77.34%] [G loss: 3.752714]\n",
      "epoch:13 step:10169 [D loss: 0.356386, acc.: 82.81%] [G loss: 4.170208]\n",
      "epoch:13 step:10170 [D loss: 0.244985, acc.: 89.84%] [G loss: 3.767685]\n",
      "epoch:13 step:10171 [D loss: 0.336121, acc.: 80.47%] [G loss: 2.593929]\n",
      "epoch:13 step:10172 [D loss: 0.414348, acc.: 78.12%] [G loss: 3.321710]\n",
      "epoch:13 step:10173 [D loss: 0.362869, acc.: 85.16%] [G loss: 4.032628]\n",
      "epoch:13 step:10174 [D loss: 0.349784, acc.: 85.94%] [G loss: 4.603118]\n",
      "epoch:13 step:10175 [D loss: 0.266478, acc.: 87.50%] [G loss: 2.931944]\n",
      "epoch:13 step:10176 [D loss: 0.389033, acc.: 83.59%] [G loss: 2.774159]\n",
      "epoch:13 step:10177 [D loss: 0.300426, acc.: 83.59%] [G loss: 4.500754]\n",
      "epoch:13 step:10178 [D loss: 0.268538, acc.: 87.50%] [G loss: 3.811382]\n",
      "epoch:13 step:10179 [D loss: 0.313601, acc.: 82.81%] [G loss: 4.595915]\n",
      "epoch:13 step:10180 [D loss: 0.371534, acc.: 84.38%] [G loss: 3.029851]\n",
      "epoch:13 step:10181 [D loss: 0.354197, acc.: 86.72%] [G loss: 5.221160]\n",
      "epoch:13 step:10182 [D loss: 0.399729, acc.: 80.47%] [G loss: 3.593559]\n",
      "epoch:13 step:10183 [D loss: 0.395763, acc.: 78.12%] [G loss: 3.392045]\n",
      "epoch:13 step:10184 [D loss: 0.342694, acc.: 82.81%] [G loss: 3.537482]\n",
      "epoch:13 step:10185 [D loss: 0.472876, acc.: 75.78%] [G loss: 4.494730]\n",
      "epoch:13 step:10186 [D loss: 0.412911, acc.: 82.03%] [G loss: 3.408476]\n",
      "epoch:13 step:10187 [D loss: 0.385192, acc.: 82.03%] [G loss: 3.363042]\n",
      "epoch:13 step:10188 [D loss: 0.449012, acc.: 75.78%] [G loss: 4.555736]\n",
      "epoch:13 step:10189 [D loss: 0.632889, acc.: 73.44%] [G loss: 5.744995]\n",
      "epoch:13 step:10190 [D loss: 0.942607, acc.: 71.09%] [G loss: 5.838492]\n",
      "epoch:13 step:10191 [D loss: 1.644289, acc.: 57.03%] [G loss: 8.485092]\n",
      "epoch:13 step:10192 [D loss: 2.739639, acc.: 53.91%] [G loss: 4.467574]\n",
      "epoch:13 step:10193 [D loss: 0.801222, acc.: 74.22%] [G loss: 2.583777]\n",
      "epoch:13 step:10194 [D loss: 0.601057, acc.: 79.69%] [G loss: 4.006701]\n",
      "epoch:13 step:10195 [D loss: 0.495633, acc.: 76.56%] [G loss: 3.947526]\n",
      "epoch:13 step:10196 [D loss: 0.320619, acc.: 89.06%] [G loss: 4.072176]\n",
      "epoch:13 step:10197 [D loss: 0.394587, acc.: 80.47%] [G loss: 2.993624]\n",
      "epoch:13 step:10198 [D loss: 0.293046, acc.: 89.06%] [G loss: 3.699709]\n",
      "epoch:13 step:10199 [D loss: 0.327477, acc.: 83.59%] [G loss: 5.123969]\n",
      "epoch:13 step:10200 [D loss: 0.400416, acc.: 83.59%] [G loss: 4.679268]\n",
      "epoch:13 step:10201 [D loss: 0.373355, acc.: 82.81%] [G loss: 2.555440]\n",
      "epoch:13 step:10202 [D loss: 0.297357, acc.: 86.72%] [G loss: 2.851049]\n",
      "epoch:13 step:10203 [D loss: 0.366338, acc.: 84.38%] [G loss: 3.145051]\n",
      "epoch:13 step:10204 [D loss: 0.359533, acc.: 89.06%] [G loss: 2.217609]\n",
      "epoch:13 step:10205 [D loss: 0.377222, acc.: 84.38%] [G loss: 2.751908]\n",
      "epoch:13 step:10206 [D loss: 0.370320, acc.: 83.59%] [G loss: 2.280751]\n",
      "epoch:13 step:10207 [D loss: 0.392906, acc.: 80.47%] [G loss: 2.836765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10208 [D loss: 0.343615, acc.: 82.81%] [G loss: 2.224445]\n",
      "epoch:13 step:10209 [D loss: 0.375689, acc.: 85.16%] [G loss: 2.348932]\n",
      "epoch:13 step:10210 [D loss: 0.423465, acc.: 78.91%] [G loss: 2.229221]\n",
      "epoch:13 step:10211 [D loss: 0.324198, acc.: 85.94%] [G loss: 2.824113]\n",
      "epoch:13 step:10212 [D loss: 0.362118, acc.: 83.59%] [G loss: 2.653441]\n",
      "epoch:13 step:10213 [D loss: 0.297807, acc.: 88.28%] [G loss: 4.241184]\n",
      "epoch:13 step:10214 [D loss: 0.317303, acc.: 82.81%] [G loss: 2.950197]\n",
      "epoch:13 step:10215 [D loss: 0.281803, acc.: 88.28%] [G loss: 3.275709]\n",
      "epoch:13 step:10216 [D loss: 0.228640, acc.: 91.41%] [G loss: 4.207562]\n",
      "epoch:13 step:10217 [D loss: 0.312985, acc.: 87.50%] [G loss: 4.168242]\n",
      "epoch:13 step:10218 [D loss: 0.460216, acc.: 76.56%] [G loss: 2.754809]\n",
      "epoch:13 step:10219 [D loss: 0.339599, acc.: 82.81%] [G loss: 3.242570]\n",
      "epoch:13 step:10220 [D loss: 0.378663, acc.: 83.59%] [G loss: 2.485478]\n",
      "epoch:13 step:10221 [D loss: 0.430752, acc.: 75.00%] [G loss: 2.428753]\n",
      "epoch:13 step:10222 [D loss: 0.287976, acc.: 88.28%] [G loss: 2.519077]\n",
      "epoch:13 step:10223 [D loss: 0.356178, acc.: 84.38%] [G loss: 2.413749]\n",
      "epoch:13 step:10224 [D loss: 0.348324, acc.: 82.03%] [G loss: 2.303860]\n",
      "epoch:13 step:10225 [D loss: 0.455339, acc.: 77.34%] [G loss: 2.675156]\n",
      "epoch:13 step:10226 [D loss: 0.413970, acc.: 81.25%] [G loss: 2.329640]\n",
      "epoch:13 step:10227 [D loss: 0.238239, acc.: 89.06%] [G loss: 3.485965]\n",
      "epoch:13 step:10228 [D loss: 0.349772, acc.: 82.03%] [G loss: 3.703901]\n",
      "epoch:13 step:10229 [D loss: 0.376816, acc.: 80.47%] [G loss: 3.113360]\n",
      "epoch:13 step:10230 [D loss: 0.493285, acc.: 81.25%] [G loss: 2.126358]\n",
      "epoch:13 step:10231 [D loss: 0.392792, acc.: 82.81%] [G loss: 1.891531]\n",
      "epoch:13 step:10232 [D loss: 0.305371, acc.: 86.72%] [G loss: 2.582698]\n",
      "epoch:13 step:10233 [D loss: 0.379929, acc.: 85.16%] [G loss: 2.405244]\n",
      "epoch:13 step:10234 [D loss: 0.382481, acc.: 83.59%] [G loss: 2.783426]\n",
      "epoch:13 step:10235 [D loss: 0.349153, acc.: 87.50%] [G loss: 2.459894]\n",
      "epoch:13 step:10236 [D loss: 0.288221, acc.: 87.50%] [G loss: 3.984386]\n",
      "epoch:13 step:10237 [D loss: 0.294610, acc.: 88.28%] [G loss: 2.620032]\n",
      "epoch:13 step:10238 [D loss: 0.279979, acc.: 85.94%] [G loss: 4.608302]\n",
      "epoch:13 step:10239 [D loss: 0.276936, acc.: 87.50%] [G loss: 3.122749]\n",
      "epoch:13 step:10240 [D loss: 0.316712, acc.: 85.16%] [G loss: 2.736497]\n",
      "epoch:13 step:10241 [D loss: 0.367155, acc.: 83.59%] [G loss: 2.992691]\n",
      "epoch:13 step:10242 [D loss: 0.297433, acc.: 85.94%] [G loss: 3.735181]\n",
      "epoch:13 step:10243 [D loss: 0.300593, acc.: 85.16%] [G loss: 4.322846]\n",
      "epoch:13 step:10244 [D loss: 0.401494, acc.: 83.59%] [G loss: 2.784259]\n",
      "epoch:13 step:10245 [D loss: 0.248108, acc.: 91.41%] [G loss: 2.393470]\n",
      "epoch:13 step:10246 [D loss: 0.384060, acc.: 79.69%] [G loss: 2.316202]\n",
      "epoch:13 step:10247 [D loss: 0.398780, acc.: 81.25%] [G loss: 2.645625]\n",
      "epoch:13 step:10248 [D loss: 0.321098, acc.: 90.62%] [G loss: 3.450464]\n",
      "epoch:13 step:10249 [D loss: 0.302014, acc.: 89.06%] [G loss: 2.494546]\n",
      "epoch:13 step:10250 [D loss: 0.316619, acc.: 84.38%] [G loss: 2.574710]\n",
      "epoch:13 step:10251 [D loss: 0.301270, acc.: 90.62%] [G loss: 3.634729]\n",
      "epoch:13 step:10252 [D loss: 0.337598, acc.: 84.38%] [G loss: 3.416996]\n",
      "epoch:13 step:10253 [D loss: 0.351177, acc.: 87.50%] [G loss: 2.734500]\n",
      "epoch:13 step:10254 [D loss: 0.534246, acc.: 74.22%] [G loss: 2.967957]\n",
      "epoch:13 step:10255 [D loss: 0.304478, acc.: 84.38%] [G loss: 4.714238]\n",
      "epoch:13 step:10256 [D loss: 0.351619, acc.: 84.38%] [G loss: 3.854712]\n",
      "epoch:13 step:10257 [D loss: 0.485609, acc.: 82.03%] [G loss: 2.365722]\n",
      "epoch:13 step:10258 [D loss: 0.270577, acc.: 90.62%] [G loss: 3.339038]\n",
      "epoch:13 step:10259 [D loss: 0.347340, acc.: 86.72%] [G loss: 3.669361]\n",
      "epoch:13 step:10260 [D loss: 0.315532, acc.: 85.94%] [G loss: 3.739732]\n",
      "epoch:13 step:10261 [D loss: 0.487066, acc.: 78.91%] [G loss: 3.543644]\n",
      "epoch:13 step:10262 [D loss: 0.337429, acc.: 86.72%] [G loss: 2.844090]\n",
      "epoch:13 step:10263 [D loss: 0.423565, acc.: 78.91%] [G loss: 2.882058]\n",
      "epoch:13 step:10264 [D loss: 0.331684, acc.: 85.16%] [G loss: 3.039319]\n",
      "epoch:13 step:10265 [D loss: 0.202343, acc.: 92.19%] [G loss: 4.924186]\n",
      "epoch:13 step:10266 [D loss: 0.262942, acc.: 90.62%] [G loss: 3.831561]\n",
      "epoch:13 step:10267 [D loss: 0.328945, acc.: 90.62%] [G loss: 3.904032]\n",
      "epoch:13 step:10268 [D loss: 0.294671, acc.: 87.50%] [G loss: 3.503583]\n",
      "epoch:13 step:10269 [D loss: 0.298653, acc.: 88.28%] [G loss: 3.446324]\n",
      "epoch:13 step:10270 [D loss: 0.306084, acc.: 86.72%] [G loss: 3.782834]\n",
      "epoch:13 step:10271 [D loss: 0.206110, acc.: 92.97%] [G loss: 3.839032]\n",
      "epoch:13 step:10272 [D loss: 0.374534, acc.: 82.81%] [G loss: 2.593882]\n",
      "epoch:13 step:10273 [D loss: 0.333666, acc.: 85.94%] [G loss: 2.739876]\n",
      "epoch:13 step:10274 [D loss: 0.295594, acc.: 89.06%] [G loss: 2.773691]\n",
      "epoch:13 step:10275 [D loss: 0.241785, acc.: 89.06%] [G loss: 2.859078]\n",
      "epoch:13 step:10276 [D loss: 0.302897, acc.: 86.72%] [G loss: 2.669044]\n",
      "epoch:13 step:10277 [D loss: 0.294443, acc.: 86.72%] [G loss: 2.522958]\n",
      "epoch:13 step:10278 [D loss: 0.462972, acc.: 77.34%] [G loss: 3.088844]\n",
      "epoch:13 step:10279 [D loss: 0.257669, acc.: 89.06%] [G loss: 3.263678]\n",
      "epoch:13 step:10280 [D loss: 0.355711, acc.: 89.84%] [G loss: 3.340772]\n",
      "epoch:13 step:10281 [D loss: 0.297096, acc.: 88.28%] [G loss: 3.370718]\n",
      "epoch:13 step:10282 [D loss: 0.250711, acc.: 85.94%] [G loss: 3.319890]\n",
      "epoch:13 step:10283 [D loss: 0.329266, acc.: 84.38%] [G loss: 3.355144]\n",
      "epoch:13 step:10284 [D loss: 0.368236, acc.: 85.94%] [G loss: 2.999478]\n",
      "epoch:13 step:10285 [D loss: 0.314204, acc.: 87.50%] [G loss: 3.625609]\n",
      "epoch:13 step:10286 [D loss: 0.301830, acc.: 86.72%] [G loss: 5.428140]\n",
      "epoch:13 step:10287 [D loss: 0.305737, acc.: 83.59%] [G loss: 3.888849]\n",
      "epoch:13 step:10288 [D loss: 0.355337, acc.: 85.16%] [G loss: 4.237599]\n",
      "epoch:13 step:10289 [D loss: 0.420641, acc.: 76.56%] [G loss: 4.084542]\n",
      "epoch:13 step:10290 [D loss: 0.273524, acc.: 92.97%] [G loss: 5.269279]\n",
      "epoch:13 step:10291 [D loss: 0.261308, acc.: 89.84%] [G loss: 4.025236]\n",
      "epoch:13 step:10292 [D loss: 0.296797, acc.: 85.94%] [G loss: 3.199582]\n",
      "epoch:13 step:10293 [D loss: 0.325676, acc.: 85.16%] [G loss: 2.594507]\n",
      "epoch:13 step:10294 [D loss: 0.400392, acc.: 82.81%] [G loss: 2.706679]\n",
      "epoch:13 step:10295 [D loss: 0.506703, acc.: 77.34%] [G loss: 2.930811]\n",
      "epoch:13 step:10296 [D loss: 0.255504, acc.: 87.50%] [G loss: 5.338573]\n",
      "epoch:13 step:10297 [D loss: 0.306028, acc.: 85.16%] [G loss: 3.700301]\n",
      "epoch:13 step:10298 [D loss: 0.250591, acc.: 90.62%] [G loss: 2.999880]\n",
      "epoch:13 step:10299 [D loss: 0.446444, acc.: 78.91%] [G loss: 2.875520]\n",
      "epoch:13 step:10300 [D loss: 0.340563, acc.: 85.16%] [G loss: 3.043600]\n",
      "epoch:13 step:10301 [D loss: 0.275754, acc.: 91.41%] [G loss: 3.680405]\n",
      "epoch:13 step:10302 [D loss: 0.316871, acc.: 82.81%] [G loss: 2.509766]\n",
      "epoch:13 step:10303 [D loss: 0.313026, acc.: 85.94%] [G loss: 3.199590]\n",
      "epoch:13 step:10304 [D loss: 0.292493, acc.: 88.28%] [G loss: 3.359685]\n",
      "epoch:13 step:10305 [D loss: 0.402093, acc.: 84.38%] [G loss: 4.278101]\n",
      "epoch:13 step:10306 [D loss: 0.215942, acc.: 92.19%] [G loss: 3.555342]\n",
      "epoch:13 step:10307 [D loss: 0.335647, acc.: 89.06%] [G loss: 3.138637]\n",
      "epoch:13 step:10308 [D loss: 0.393084, acc.: 80.47%] [G loss: 2.882005]\n",
      "epoch:13 step:10309 [D loss: 0.332240, acc.: 83.59%] [G loss: 3.007013]\n",
      "epoch:13 step:10310 [D loss: 0.577118, acc.: 69.53%] [G loss: 2.264942]\n",
      "epoch:13 step:10311 [D loss: 0.388942, acc.: 79.69%] [G loss: 2.802860]\n",
      "epoch:13 step:10312 [D loss: 0.293112, acc.: 89.06%] [G loss: 2.531690]\n",
      "epoch:13 step:10313 [D loss: 0.370137, acc.: 82.03%] [G loss: 2.767378]\n",
      "epoch:13 step:10314 [D loss: 0.300320, acc.: 88.28%] [G loss: 3.653289]\n",
      "epoch:13 step:10315 [D loss: 0.294140, acc.: 88.28%] [G loss: 3.202327]\n",
      "epoch:13 step:10316 [D loss: 0.341248, acc.: 82.03%] [G loss: 2.668253]\n",
      "epoch:13 step:10317 [D loss: 0.286172, acc.: 87.50%] [G loss: 3.106855]\n",
      "epoch:13 step:10318 [D loss: 0.304151, acc.: 86.72%] [G loss: 3.598915]\n",
      "epoch:13 step:10319 [D loss: 0.336391, acc.: 86.72%] [G loss: 2.475678]\n",
      "epoch:13 step:10320 [D loss: 0.480567, acc.: 80.47%] [G loss: 2.947805]\n",
      "epoch:13 step:10321 [D loss: 0.380383, acc.: 85.94%] [G loss: 2.801146]\n",
      "epoch:13 step:10322 [D loss: 0.430209, acc.: 79.69%] [G loss: 2.407211]\n",
      "epoch:13 step:10323 [D loss: 0.279210, acc.: 88.28%] [G loss: 3.298557]\n",
      "epoch:13 step:10324 [D loss: 0.351685, acc.: 84.38%] [G loss: 2.966771]\n",
      "epoch:13 step:10325 [D loss: 0.174970, acc.: 93.75%] [G loss: 3.283854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10326 [D loss: 0.387274, acc.: 83.59%] [G loss: 2.435587]\n",
      "epoch:13 step:10327 [D loss: 0.258634, acc.: 86.72%] [G loss: 3.337423]\n",
      "epoch:13 step:10328 [D loss: 0.228682, acc.: 89.06%] [G loss: 4.550323]\n",
      "epoch:13 step:10329 [D loss: 0.301878, acc.: 88.28%] [G loss: 2.890663]\n",
      "epoch:13 step:10330 [D loss: 0.314395, acc.: 85.94%] [G loss: 2.545624]\n",
      "epoch:13 step:10331 [D loss: 0.285507, acc.: 88.28%] [G loss: 2.533388]\n",
      "epoch:13 step:10332 [D loss: 0.285328, acc.: 86.72%] [G loss: 4.075397]\n",
      "epoch:13 step:10333 [D loss: 0.258069, acc.: 88.28%] [G loss: 4.239624]\n",
      "epoch:13 step:10334 [D loss: 0.305796, acc.: 87.50%] [G loss: 3.003032]\n",
      "epoch:13 step:10335 [D loss: 0.308156, acc.: 88.28%] [G loss: 4.820680]\n",
      "epoch:13 step:10336 [D loss: 0.471642, acc.: 80.47%] [G loss: 3.372051]\n",
      "epoch:13 step:10337 [D loss: 0.244202, acc.: 87.50%] [G loss: 3.482510]\n",
      "epoch:13 step:10338 [D loss: 0.274385, acc.: 88.28%] [G loss: 2.565872]\n",
      "epoch:13 step:10339 [D loss: 0.268067, acc.: 88.28%] [G loss: 2.925869]\n",
      "epoch:13 step:10340 [D loss: 0.351877, acc.: 85.94%] [G loss: 3.281714]\n",
      "epoch:13 step:10341 [D loss: 0.361952, acc.: 82.81%] [G loss: 2.947358]\n",
      "epoch:13 step:10342 [D loss: 0.301659, acc.: 86.72%] [G loss: 3.343684]\n",
      "epoch:13 step:10343 [D loss: 0.430288, acc.: 81.25%] [G loss: 2.194340]\n",
      "epoch:13 step:10344 [D loss: 0.348610, acc.: 79.69%] [G loss: 2.943038]\n",
      "epoch:13 step:10345 [D loss: 0.267103, acc.: 89.06%] [G loss: 3.631026]\n",
      "epoch:13 step:10346 [D loss: 0.287201, acc.: 90.62%] [G loss: 2.970558]\n",
      "epoch:13 step:10347 [D loss: 0.323981, acc.: 90.62%] [G loss: 2.899457]\n",
      "epoch:13 step:10348 [D loss: 0.438624, acc.: 82.03%] [G loss: 2.383979]\n",
      "epoch:13 step:10349 [D loss: 0.355111, acc.: 84.38%] [G loss: 2.737345]\n",
      "epoch:13 step:10350 [D loss: 0.363622, acc.: 82.03%] [G loss: 2.428821]\n",
      "epoch:13 step:10351 [D loss: 0.381728, acc.: 83.59%] [G loss: 2.525502]\n",
      "epoch:13 step:10352 [D loss: 0.323456, acc.: 87.50%] [G loss: 2.544354]\n",
      "epoch:13 step:10353 [D loss: 0.369730, acc.: 81.25%] [G loss: 3.420054]\n",
      "epoch:13 step:10354 [D loss: 0.286808, acc.: 87.50%] [G loss: 2.682478]\n",
      "epoch:13 step:10355 [D loss: 0.291164, acc.: 89.06%] [G loss: 3.149520]\n",
      "epoch:13 step:10356 [D loss: 0.336535, acc.: 86.72%] [G loss: 2.776498]\n",
      "epoch:13 step:10357 [D loss: 0.439614, acc.: 77.34%] [G loss: 5.081296]\n",
      "epoch:13 step:10358 [D loss: 0.437801, acc.: 79.69%] [G loss: 4.486413]\n",
      "epoch:13 step:10359 [D loss: 0.544578, acc.: 72.66%] [G loss: 2.930371]\n",
      "epoch:13 step:10360 [D loss: 0.453291, acc.: 78.12%] [G loss: 4.982739]\n",
      "epoch:13 step:10361 [D loss: 0.278202, acc.: 87.50%] [G loss: 4.607275]\n",
      "epoch:13 step:10362 [D loss: 0.322151, acc.: 86.72%] [G loss: 3.610305]\n",
      "epoch:13 step:10363 [D loss: 0.261153, acc.: 90.62%] [G loss: 2.732919]\n",
      "epoch:13 step:10364 [D loss: 0.363855, acc.: 81.25%] [G loss: 2.179999]\n",
      "epoch:13 step:10365 [D loss: 0.320596, acc.: 87.50%] [G loss: 3.117871]\n",
      "epoch:13 step:10366 [D loss: 0.306643, acc.: 88.28%] [G loss: 2.575288]\n",
      "epoch:13 step:10367 [D loss: 0.432464, acc.: 82.81%] [G loss: 2.930618]\n",
      "epoch:13 step:10368 [D loss: 0.287829, acc.: 89.84%] [G loss: 2.867473]\n",
      "epoch:13 step:10369 [D loss: 0.294337, acc.: 87.50%] [G loss: 3.131856]\n",
      "epoch:13 step:10370 [D loss: 0.328524, acc.: 85.94%] [G loss: 3.882675]\n",
      "epoch:13 step:10371 [D loss: 0.258739, acc.: 89.84%] [G loss: 2.881366]\n",
      "epoch:13 step:10372 [D loss: 0.277312, acc.: 86.72%] [G loss: 2.451879]\n",
      "epoch:13 step:10373 [D loss: 0.286475, acc.: 88.28%] [G loss: 2.795764]\n",
      "epoch:13 step:10374 [D loss: 0.326469, acc.: 86.72%] [G loss: 3.566054]\n",
      "epoch:13 step:10375 [D loss: 0.340651, acc.: 87.50%] [G loss: 3.163869]\n",
      "epoch:13 step:10376 [D loss: 0.302460, acc.: 90.62%] [G loss: 2.351601]\n",
      "epoch:13 step:10377 [D loss: 0.350005, acc.: 83.59%] [G loss: 2.526153]\n",
      "epoch:13 step:10378 [D loss: 0.317317, acc.: 84.38%] [G loss: 2.267733]\n",
      "epoch:13 step:10379 [D loss: 0.495197, acc.: 79.69%] [G loss: 2.675109]\n",
      "epoch:13 step:10380 [D loss: 0.343578, acc.: 85.94%] [G loss: 2.836865]\n",
      "epoch:13 step:10381 [D loss: 0.341446, acc.: 84.38%] [G loss: 3.963191]\n",
      "epoch:13 step:10382 [D loss: 0.455237, acc.: 77.34%] [G loss: 5.895223]\n",
      "epoch:13 step:10383 [D loss: 0.588888, acc.: 75.00%] [G loss: 3.994879]\n",
      "epoch:13 step:10384 [D loss: 0.362020, acc.: 82.81%] [G loss: 4.319625]\n",
      "epoch:13 step:10385 [D loss: 0.223275, acc.: 92.97%] [G loss: 2.904064]\n",
      "epoch:13 step:10386 [D loss: 0.298726, acc.: 87.50%] [G loss: 3.241956]\n",
      "epoch:13 step:10387 [D loss: 0.422417, acc.: 82.03%] [G loss: 3.035310]\n",
      "epoch:13 step:10388 [D loss: 0.279877, acc.: 89.06%] [G loss: 3.586764]\n",
      "epoch:13 step:10389 [D loss: 0.352388, acc.: 84.38%] [G loss: 2.873172]\n",
      "epoch:13 step:10390 [D loss: 0.380199, acc.: 83.59%] [G loss: 3.427299]\n",
      "epoch:13 step:10391 [D loss: 0.381772, acc.: 82.03%] [G loss: 4.387426]\n",
      "epoch:13 step:10392 [D loss: 0.275945, acc.: 87.50%] [G loss: 3.608344]\n",
      "epoch:13 step:10393 [D loss: 0.457758, acc.: 78.12%] [G loss: 2.892076]\n",
      "epoch:13 step:10394 [D loss: 0.374714, acc.: 83.59%] [G loss: 3.623156]\n",
      "epoch:13 step:10395 [D loss: 0.345170, acc.: 86.72%] [G loss: 2.794551]\n",
      "epoch:13 step:10396 [D loss: 0.293739, acc.: 91.41%] [G loss: 3.030567]\n",
      "epoch:13 step:10397 [D loss: 0.336159, acc.: 83.59%] [G loss: 2.770531]\n",
      "epoch:13 step:10398 [D loss: 0.378163, acc.: 85.94%] [G loss: 3.266550]\n",
      "epoch:13 step:10399 [D loss: 0.383763, acc.: 79.69%] [G loss: 2.480308]\n",
      "epoch:13 step:10400 [D loss: 0.355552, acc.: 83.59%] [G loss: 2.193390]\n",
      "epoch:13 step:10401 [D loss: 0.305565, acc.: 87.50%] [G loss: 3.013988]\n",
      "epoch:13 step:10402 [D loss: 0.489002, acc.: 78.12%] [G loss: 3.624405]\n",
      "epoch:13 step:10403 [D loss: 0.517939, acc.: 81.25%] [G loss: 5.932837]\n",
      "epoch:13 step:10404 [D loss: 0.599752, acc.: 77.34%] [G loss: 2.719328]\n",
      "epoch:13 step:10405 [D loss: 0.324478, acc.: 83.59%] [G loss: 3.819322]\n",
      "epoch:13 step:10406 [D loss: 0.345280, acc.: 83.59%] [G loss: 3.324324]\n",
      "epoch:13 step:10407 [D loss: 0.255259, acc.: 88.28%] [G loss: 4.814210]\n",
      "epoch:13 step:10408 [D loss: 0.212576, acc.: 91.41%] [G loss: 4.258904]\n",
      "epoch:13 step:10409 [D loss: 0.299819, acc.: 90.62%] [G loss: 3.740264]\n",
      "epoch:13 step:10410 [D loss: 0.325715, acc.: 87.50%] [G loss: 2.968305]\n",
      "epoch:13 step:10411 [D loss: 0.347862, acc.: 88.28%] [G loss: 2.574955]\n",
      "epoch:13 step:10412 [D loss: 0.311698, acc.: 89.06%] [G loss: 2.702557]\n",
      "epoch:13 step:10413 [D loss: 0.293826, acc.: 87.50%] [G loss: 2.666646]\n",
      "epoch:13 step:10414 [D loss: 0.315957, acc.: 88.28%] [G loss: 3.563999]\n",
      "epoch:13 step:10415 [D loss: 0.438152, acc.: 76.56%] [G loss: 4.220716]\n",
      "epoch:13 step:10416 [D loss: 0.406099, acc.: 85.16%] [G loss: 2.823401]\n",
      "epoch:13 step:10417 [D loss: 0.358624, acc.: 81.25%] [G loss: 3.770508]\n",
      "epoch:13 step:10418 [D loss: 0.489469, acc.: 81.25%] [G loss: 4.435048]\n",
      "epoch:13 step:10419 [D loss: 0.364628, acc.: 81.25%] [G loss: 4.207491]\n",
      "epoch:13 step:10420 [D loss: 0.327997, acc.: 88.28%] [G loss: 3.176071]\n",
      "epoch:13 step:10421 [D loss: 0.327099, acc.: 85.16%] [G loss: 2.219365]\n",
      "epoch:13 step:10422 [D loss: 0.325319, acc.: 85.16%] [G loss: 2.458356]\n",
      "epoch:13 step:10423 [D loss: 0.273202, acc.: 91.41%] [G loss: 2.933327]\n",
      "epoch:13 step:10424 [D loss: 0.253919, acc.: 91.41%] [G loss: 3.336349]\n",
      "epoch:13 step:10425 [D loss: 0.332721, acc.: 82.03%] [G loss: 2.718728]\n",
      "epoch:13 step:10426 [D loss: 0.315886, acc.: 82.81%] [G loss: 3.301547]\n",
      "epoch:13 step:10427 [D loss: 0.248833, acc.: 90.62%] [G loss: 3.697659]\n",
      "epoch:13 step:10428 [D loss: 0.360233, acc.: 83.59%] [G loss: 6.243011]\n",
      "epoch:13 step:10429 [D loss: 0.330027, acc.: 86.72%] [G loss: 2.937237]\n",
      "epoch:13 step:10430 [D loss: 0.350978, acc.: 81.25%] [G loss: 3.903148]\n",
      "epoch:13 step:10431 [D loss: 0.426931, acc.: 78.91%] [G loss: 5.264237]\n",
      "epoch:13 step:10432 [D loss: 0.256959, acc.: 89.84%] [G loss: 5.028551]\n",
      "epoch:13 step:10433 [D loss: 0.335756, acc.: 88.28%] [G loss: 2.881227]\n",
      "epoch:13 step:10434 [D loss: 0.412078, acc.: 84.38%] [G loss: 3.040306]\n",
      "epoch:13 step:10435 [D loss: 0.218380, acc.: 92.97%] [G loss: 5.299480]\n",
      "epoch:13 step:10436 [D loss: 0.303859, acc.: 84.38%] [G loss: 6.791216]\n",
      "epoch:13 step:10437 [D loss: 0.407772, acc.: 81.25%] [G loss: 2.452846]\n",
      "epoch:13 step:10438 [D loss: 0.264933, acc.: 87.50%] [G loss: 4.395571]\n",
      "epoch:13 step:10439 [D loss: 0.251935, acc.: 89.06%] [G loss: 4.821455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10440 [D loss: 0.231795, acc.: 87.50%] [G loss: 3.985846]\n",
      "epoch:13 step:10441 [D loss: 0.387334, acc.: 84.38%] [G loss: 2.862664]\n",
      "epoch:13 step:10442 [D loss: 0.322295, acc.: 86.72%] [G loss: 3.143025]\n",
      "epoch:13 step:10443 [D loss: 0.335200, acc.: 80.47%] [G loss: 4.004106]\n",
      "epoch:13 step:10444 [D loss: 0.316499, acc.: 85.94%] [G loss: 4.311719]\n",
      "epoch:13 step:10445 [D loss: 0.342773, acc.: 82.03%] [G loss: 2.047284]\n",
      "epoch:13 step:10446 [D loss: 0.406422, acc.: 80.47%] [G loss: 2.584773]\n",
      "epoch:13 step:10447 [D loss: 0.308228, acc.: 85.94%] [G loss: 2.681293]\n",
      "epoch:13 step:10448 [D loss: 0.312973, acc.: 87.50%] [G loss: 2.413119]\n",
      "epoch:13 step:10449 [D loss: 0.303888, acc.: 90.62%] [G loss: 2.537928]\n",
      "epoch:13 step:10450 [D loss: 0.393894, acc.: 82.03%] [G loss: 3.009806]\n",
      "epoch:13 step:10451 [D loss: 0.316119, acc.: 88.28%] [G loss: 2.904453]\n",
      "epoch:13 step:10452 [D loss: 0.329973, acc.: 85.94%] [G loss: 3.529531]\n",
      "epoch:13 step:10453 [D loss: 0.369937, acc.: 79.69%] [G loss: 3.499021]\n",
      "epoch:13 step:10454 [D loss: 0.380752, acc.: 81.25%] [G loss: 3.027173]\n",
      "epoch:13 step:10455 [D loss: 0.472375, acc.: 79.69%] [G loss: 3.960653]\n",
      "epoch:13 step:10456 [D loss: 0.476656, acc.: 79.69%] [G loss: 2.493449]\n",
      "epoch:13 step:10457 [D loss: 0.403071, acc.: 80.47%] [G loss: 3.073731]\n",
      "epoch:13 step:10458 [D loss: 0.401687, acc.: 80.47%] [G loss: 2.814524]\n",
      "epoch:13 step:10459 [D loss: 0.298223, acc.: 88.28%] [G loss: 3.370859]\n",
      "epoch:13 step:10460 [D loss: 0.448079, acc.: 82.03%] [G loss: 3.203776]\n",
      "epoch:13 step:10461 [D loss: 0.375763, acc.: 83.59%] [G loss: 2.692722]\n",
      "epoch:13 step:10462 [D loss: 0.489644, acc.: 78.91%] [G loss: 2.610850]\n",
      "epoch:13 step:10463 [D loss: 0.293985, acc.: 89.84%] [G loss: 3.350149]\n",
      "epoch:13 step:10464 [D loss: 0.315279, acc.: 86.72%] [G loss: 4.281313]\n",
      "epoch:13 step:10465 [D loss: 0.319427, acc.: 86.72%] [G loss: 4.326929]\n",
      "epoch:13 step:10466 [D loss: 0.414735, acc.: 82.03%] [G loss: 2.878186]\n",
      "epoch:13 step:10467 [D loss: 0.326914, acc.: 85.16%] [G loss: 2.509364]\n",
      "epoch:13 step:10468 [D loss: 0.529453, acc.: 76.56%] [G loss: 5.575754]\n",
      "epoch:13 step:10469 [D loss: 0.885698, acc.: 74.22%] [G loss: 8.840078]\n",
      "epoch:13 step:10470 [D loss: 2.637439, acc.: 52.34%] [G loss: 6.044141]\n",
      "epoch:13 step:10471 [D loss: 1.276398, acc.: 66.41%] [G loss: 2.716639]\n",
      "epoch:13 step:10472 [D loss: 0.444731, acc.: 75.00%] [G loss: 3.103702]\n",
      "epoch:13 step:10473 [D loss: 0.455030, acc.: 76.56%] [G loss: 3.848909]\n",
      "epoch:13 step:10474 [D loss: 0.525793, acc.: 78.12%] [G loss: 4.279309]\n",
      "epoch:13 step:10475 [D loss: 0.617239, acc.: 85.16%] [G loss: 5.170113]\n",
      "epoch:13 step:10476 [D loss: 0.472373, acc.: 82.03%] [G loss: 3.481378]\n",
      "epoch:13 step:10477 [D loss: 0.412683, acc.: 82.81%] [G loss: 2.170250]\n",
      "epoch:13 step:10478 [D loss: 0.366299, acc.: 80.47%] [G loss: 2.887902]\n",
      "epoch:13 step:10479 [D loss: 0.386489, acc.: 85.94%] [G loss: 3.371300]\n",
      "epoch:13 step:10480 [D loss: 0.397974, acc.: 82.81%] [G loss: 2.850628]\n",
      "epoch:13 step:10481 [D loss: 0.249168, acc.: 90.62%] [G loss: 3.264994]\n",
      "epoch:13 step:10482 [D loss: 0.380190, acc.: 81.25%] [G loss: 2.409734]\n",
      "epoch:13 step:10483 [D loss: 0.371193, acc.: 85.16%] [G loss: 2.518237]\n",
      "epoch:13 step:10484 [D loss: 0.342593, acc.: 85.94%] [G loss: 2.482545]\n",
      "epoch:13 step:10485 [D loss: 0.327923, acc.: 89.06%] [G loss: 2.227967]\n",
      "epoch:13 step:10486 [D loss: 0.317824, acc.: 86.72%] [G loss: 2.090596]\n",
      "epoch:13 step:10487 [D loss: 0.428531, acc.: 78.12%] [G loss: 2.240448]\n",
      "epoch:13 step:10488 [D loss: 0.304209, acc.: 90.62%] [G loss: 2.295808]\n",
      "epoch:13 step:10489 [D loss: 0.338355, acc.: 83.59%] [G loss: 2.127315]\n",
      "epoch:13 step:10490 [D loss: 0.319894, acc.: 85.94%] [G loss: 2.133049]\n",
      "epoch:13 step:10491 [D loss: 0.451539, acc.: 75.78%] [G loss: 2.124331]\n",
      "epoch:13 step:10492 [D loss: 0.365011, acc.: 87.50%] [G loss: 2.256298]\n",
      "epoch:13 step:10493 [D loss: 0.434505, acc.: 82.81%] [G loss: 2.715893]\n",
      "epoch:13 step:10494 [D loss: 0.306973, acc.: 88.28%] [G loss: 2.856677]\n",
      "epoch:13 step:10495 [D loss: 0.390213, acc.: 84.38%] [G loss: 2.637170]\n",
      "epoch:13 step:10496 [D loss: 0.330771, acc.: 85.94%] [G loss: 2.635595]\n",
      "epoch:13 step:10497 [D loss: 0.290527, acc.: 88.28%] [G loss: 2.470168]\n",
      "epoch:13 step:10498 [D loss: 0.325153, acc.: 82.81%] [G loss: 2.559650]\n",
      "epoch:13 step:10499 [D loss: 0.433790, acc.: 82.03%] [G loss: 2.281945]\n",
      "epoch:13 step:10500 [D loss: 0.336955, acc.: 84.38%] [G loss: 2.892963]\n",
      "epoch:13 step:10501 [D loss: 0.376388, acc.: 83.59%] [G loss: 2.298719]\n",
      "epoch:13 step:10502 [D loss: 0.362694, acc.: 84.38%] [G loss: 2.717519]\n",
      "epoch:13 step:10503 [D loss: 0.337242, acc.: 85.94%] [G loss: 2.586445]\n",
      "epoch:13 step:10504 [D loss: 0.247939, acc.: 89.06%] [G loss: 3.911474]\n",
      "epoch:13 step:10505 [D loss: 0.292636, acc.: 85.16%] [G loss: 4.049181]\n",
      "epoch:13 step:10506 [D loss: 0.264194, acc.: 87.50%] [G loss: 3.753718]\n",
      "epoch:13 step:10507 [D loss: 0.318783, acc.: 85.94%] [G loss: 3.230452]\n",
      "epoch:13 step:10508 [D loss: 0.256251, acc.: 89.84%] [G loss: 2.762942]\n",
      "epoch:13 step:10509 [D loss: 0.390175, acc.: 81.25%] [G loss: 2.600240]\n",
      "epoch:13 step:10510 [D loss: 0.392621, acc.: 84.38%] [G loss: 2.184909]\n",
      "epoch:13 step:10511 [D loss: 0.393564, acc.: 82.81%] [G loss: 2.340801]\n",
      "epoch:13 step:10512 [D loss: 0.332754, acc.: 86.72%] [G loss: 3.099123]\n",
      "epoch:13 step:10513 [D loss: 0.404705, acc.: 78.12%] [G loss: 2.171050]\n",
      "epoch:13 step:10514 [D loss: 0.355148, acc.: 89.84%] [G loss: 2.539984]\n",
      "epoch:13 step:10515 [D loss: 0.340441, acc.: 83.59%] [G loss: 2.372879]\n",
      "epoch:13 step:10516 [D loss: 0.281763, acc.: 92.19%] [G loss: 2.437840]\n",
      "epoch:13 step:10517 [D loss: 0.433288, acc.: 78.91%] [G loss: 3.348049]\n",
      "epoch:13 step:10518 [D loss: 0.247400, acc.: 88.28%] [G loss: 5.007383]\n",
      "epoch:13 step:10519 [D loss: 0.389101, acc.: 83.59%] [G loss: 4.238509]\n",
      "epoch:13 step:10520 [D loss: 0.383567, acc.: 84.38%] [G loss: 1.801763]\n",
      "epoch:13 step:10521 [D loss: 0.344898, acc.: 83.59%] [G loss: 2.366518]\n",
      "epoch:13 step:10522 [D loss: 0.286932, acc.: 85.94%] [G loss: 3.737976]\n",
      "epoch:13 step:10523 [D loss: 0.355164, acc.: 84.38%] [G loss: 3.388463]\n",
      "epoch:13 step:10524 [D loss: 0.303689, acc.: 90.62%] [G loss: 2.241846]\n",
      "epoch:13 step:10525 [D loss: 0.407462, acc.: 78.12%] [G loss: 2.288721]\n",
      "epoch:13 step:10526 [D loss: 0.319135, acc.: 89.06%] [G loss: 2.229373]\n",
      "epoch:13 step:10527 [D loss: 0.383465, acc.: 84.38%] [G loss: 2.798208]\n",
      "epoch:13 step:10528 [D loss: 0.332711, acc.: 84.38%] [G loss: 2.660647]\n",
      "epoch:13 step:10529 [D loss: 0.339719, acc.: 82.81%] [G loss: 2.548399]\n",
      "epoch:13 step:10530 [D loss: 0.277141, acc.: 89.06%] [G loss: 3.635744]\n",
      "epoch:13 step:10531 [D loss: 0.318264, acc.: 82.81%] [G loss: 5.347639]\n",
      "epoch:13 step:10532 [D loss: 0.319508, acc.: 88.28%] [G loss: 3.246831]\n",
      "epoch:13 step:10533 [D loss: 0.304261, acc.: 82.03%] [G loss: 4.211134]\n",
      "epoch:13 step:10534 [D loss: 0.291429, acc.: 88.28%] [G loss: 3.128620]\n",
      "epoch:13 step:10535 [D loss: 0.288917, acc.: 92.19%] [G loss: 2.644747]\n",
      "epoch:13 step:10536 [D loss: 0.358331, acc.: 83.59%] [G loss: 2.655960]\n",
      "epoch:13 step:10537 [D loss: 0.333786, acc.: 86.72%] [G loss: 2.885354]\n",
      "epoch:13 step:10538 [D loss: 0.304116, acc.: 86.72%] [G loss: 2.609581]\n",
      "epoch:13 step:10539 [D loss: 0.379602, acc.: 84.38%] [G loss: 2.761399]\n",
      "epoch:13 step:10540 [D loss: 0.365275, acc.: 85.16%] [G loss: 2.636109]\n",
      "epoch:13 step:10541 [D loss: 0.549035, acc.: 75.78%] [G loss: 2.092443]\n",
      "epoch:13 step:10542 [D loss: 0.362856, acc.: 83.59%] [G loss: 2.884611]\n",
      "epoch:13 step:10543 [D loss: 0.332431, acc.: 83.59%] [G loss: 2.701804]\n",
      "epoch:13 step:10544 [D loss: 0.372767, acc.: 81.25%] [G loss: 3.204043]\n",
      "epoch:13 step:10545 [D loss: 0.291259, acc.: 90.62%] [G loss: 2.512667]\n",
      "epoch:13 step:10546 [D loss: 0.369414, acc.: 85.16%] [G loss: 2.514628]\n",
      "epoch:13 step:10547 [D loss: 0.273759, acc.: 88.28%] [G loss: 2.729786]\n",
      "epoch:13 step:10548 [D loss: 0.353187, acc.: 81.25%] [G loss: 3.109756]\n",
      "epoch:13 step:10549 [D loss: 0.328272, acc.: 88.28%] [G loss: 3.849498]\n",
      "epoch:13 step:10550 [D loss: 0.280806, acc.: 82.03%] [G loss: 5.312986]\n",
      "epoch:13 step:10551 [D loss: 0.310023, acc.: 85.16%] [G loss: 3.299170]\n",
      "epoch:13 step:10552 [D loss: 0.352227, acc.: 86.72%] [G loss: 2.932589]\n",
      "epoch:13 step:10553 [D loss: 0.290420, acc.: 88.28%] [G loss: 3.100503]\n",
      "epoch:13 step:10554 [D loss: 0.324255, acc.: 86.72%] [G loss: 2.803281]\n",
      "epoch:13 step:10555 [D loss: 0.459137, acc.: 80.47%] [G loss: 3.038572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10556 [D loss: 0.466053, acc.: 81.25%] [G loss: 2.782933]\n",
      "epoch:13 step:10557 [D loss: 0.298618, acc.: 89.84%] [G loss: 2.925532]\n",
      "epoch:13 step:10558 [D loss: 0.358438, acc.: 85.16%] [G loss: 3.390338]\n",
      "epoch:13 step:10559 [D loss: 0.311401, acc.: 82.03%] [G loss: 5.475053]\n",
      "epoch:13 step:10560 [D loss: 0.412285, acc.: 81.25%] [G loss: 2.244200]\n",
      "epoch:13 step:10561 [D loss: 0.403578, acc.: 82.03%] [G loss: 2.795393]\n",
      "epoch:13 step:10562 [D loss: 0.246329, acc.: 92.19%] [G loss: 2.975273]\n",
      "epoch:13 step:10563 [D loss: 0.427602, acc.: 82.03%] [G loss: 3.612555]\n",
      "epoch:13 step:10564 [D loss: 0.393386, acc.: 82.03%] [G loss: 3.182442]\n",
      "epoch:13 step:10565 [D loss: 0.279736, acc.: 91.41%] [G loss: 4.949024]\n",
      "epoch:13 step:10566 [D loss: 0.411976, acc.: 78.12%] [G loss: 2.463932]\n",
      "epoch:13 step:10567 [D loss: 0.359212, acc.: 86.72%] [G loss: 3.135183]\n",
      "epoch:13 step:10568 [D loss: 0.418588, acc.: 83.59%] [G loss: 1.995232]\n",
      "epoch:13 step:10569 [D loss: 0.369360, acc.: 85.16%] [G loss: 2.631422]\n",
      "epoch:13 step:10570 [D loss: 0.301471, acc.: 87.50%] [G loss: 2.510904]\n",
      "epoch:13 step:10571 [D loss: 0.405814, acc.: 79.69%] [G loss: 4.147791]\n",
      "epoch:13 step:10572 [D loss: 0.349132, acc.: 84.38%] [G loss: 3.991205]\n",
      "epoch:13 step:10573 [D loss: 0.304297, acc.: 85.16%] [G loss: 2.898258]\n",
      "epoch:13 step:10574 [D loss: 0.313563, acc.: 87.50%] [G loss: 2.528159]\n",
      "epoch:13 step:10575 [D loss: 0.394906, acc.: 81.25%] [G loss: 2.755511]\n",
      "epoch:13 step:10576 [D loss: 0.345791, acc.: 82.81%] [G loss: 3.306543]\n",
      "epoch:13 step:10577 [D loss: 0.291287, acc.: 88.28%] [G loss: 3.651381]\n",
      "epoch:13 step:10578 [D loss: 0.263145, acc.: 88.28%] [G loss: 6.245708]\n",
      "epoch:13 step:10579 [D loss: 0.214201, acc.: 92.19%] [G loss: 4.182277]\n",
      "epoch:13 step:10580 [D loss: 0.281121, acc.: 87.50%] [G loss: 4.635088]\n",
      "epoch:13 step:10581 [D loss: 0.306262, acc.: 87.50%] [G loss: 4.079157]\n",
      "epoch:13 step:10582 [D loss: 0.508323, acc.: 76.56%] [G loss: 4.147988]\n",
      "epoch:13 step:10583 [D loss: 0.351454, acc.: 85.94%] [G loss: 3.452385]\n",
      "epoch:13 step:10584 [D loss: 0.358213, acc.: 85.94%] [G loss: 5.177416]\n",
      "epoch:13 step:10585 [D loss: 0.354882, acc.: 84.38%] [G loss: 4.724670]\n",
      "epoch:13 step:10586 [D loss: 0.327642, acc.: 86.72%] [G loss: 3.300045]\n",
      "epoch:13 step:10587 [D loss: 0.361342, acc.: 86.72%] [G loss: 3.987908]\n",
      "epoch:13 step:10588 [D loss: 0.450379, acc.: 82.03%] [G loss: 2.192341]\n",
      "epoch:13 step:10589 [D loss: 0.375313, acc.: 79.69%] [G loss: 3.951743]\n",
      "epoch:13 step:10590 [D loss: 0.394082, acc.: 86.72%] [G loss: 5.342547]\n",
      "epoch:13 step:10591 [D loss: 0.311867, acc.: 85.94%] [G loss: 3.829495]\n",
      "epoch:13 step:10592 [D loss: 0.455302, acc.: 81.25%] [G loss: 3.733821]\n",
      "epoch:13 step:10593 [D loss: 0.250077, acc.: 88.28%] [G loss: 2.969837]\n",
      "epoch:13 step:10594 [D loss: 0.420971, acc.: 81.25%] [G loss: 4.299929]\n",
      "epoch:13 step:10595 [D loss: 0.309755, acc.: 86.72%] [G loss: 2.445970]\n",
      "epoch:13 step:10596 [D loss: 0.319504, acc.: 86.72%] [G loss: 5.890736]\n",
      "epoch:13 step:10597 [D loss: 0.483407, acc.: 76.56%] [G loss: 2.983369]\n",
      "epoch:13 step:10598 [D loss: 0.347259, acc.: 83.59%] [G loss: 4.404455]\n",
      "epoch:13 step:10599 [D loss: 0.285547, acc.: 87.50%] [G loss: 3.759157]\n",
      "epoch:13 step:10600 [D loss: 0.294859, acc.: 85.16%] [G loss: 3.052813]\n",
      "epoch:13 step:10601 [D loss: 0.363930, acc.: 82.81%] [G loss: 3.445271]\n",
      "epoch:13 step:10602 [D loss: 0.270934, acc.: 86.72%] [G loss: 5.666448]\n",
      "epoch:13 step:10603 [D loss: 0.268827, acc.: 91.41%] [G loss: 3.367879]\n",
      "epoch:13 step:10604 [D loss: 0.260067, acc.: 89.06%] [G loss: 3.949862]\n",
      "epoch:13 step:10605 [D loss: 0.295840, acc.: 86.72%] [G loss: 3.913980]\n",
      "epoch:13 step:10606 [D loss: 0.299110, acc.: 86.72%] [G loss: 3.449543]\n",
      "epoch:13 step:10607 [D loss: 0.359065, acc.: 84.38%] [G loss: 2.910769]\n",
      "epoch:13 step:10608 [D loss: 0.392798, acc.: 82.03%] [G loss: 3.391320]\n",
      "epoch:13 step:10609 [D loss: 0.379870, acc.: 83.59%] [G loss: 2.001923]\n",
      "epoch:13 step:10610 [D loss: 0.476546, acc.: 80.47%] [G loss: 3.339324]\n",
      "epoch:13 step:10611 [D loss: 0.266244, acc.: 87.50%] [G loss: 3.014976]\n",
      "epoch:13 step:10612 [D loss: 0.298059, acc.: 87.50%] [G loss: 3.790187]\n",
      "epoch:13 step:10613 [D loss: 0.443853, acc.: 79.69%] [G loss: 2.815403]\n",
      "epoch:13 step:10614 [D loss: 0.309369, acc.: 85.16%] [G loss: 3.601006]\n",
      "epoch:13 step:10615 [D loss: 0.262960, acc.: 93.75%] [G loss: 3.072852]\n",
      "epoch:13 step:10616 [D loss: 0.334650, acc.: 82.81%] [G loss: 3.027313]\n",
      "epoch:13 step:10617 [D loss: 0.479021, acc.: 80.47%] [G loss: 3.652788]\n",
      "epoch:13 step:10618 [D loss: 0.347053, acc.: 84.38%] [G loss: 3.882140]\n",
      "epoch:13 step:10619 [D loss: 0.369534, acc.: 82.81%] [G loss: 3.310814]\n",
      "epoch:13 step:10620 [D loss: 0.461219, acc.: 80.47%] [G loss: 3.166351]\n",
      "epoch:13 step:10621 [D loss: 0.430332, acc.: 78.12%] [G loss: 1.951021]\n",
      "epoch:13 step:10622 [D loss: 0.440759, acc.: 80.47%] [G loss: 3.727827]\n",
      "epoch:13 step:10623 [D loss: 0.215907, acc.: 88.28%] [G loss: 4.863013]\n",
      "epoch:13 step:10624 [D loss: 0.324928, acc.: 81.25%] [G loss: 3.088983]\n",
      "epoch:13 step:10625 [D loss: 0.423355, acc.: 82.03%] [G loss: 1.846391]\n",
      "epoch:13 step:10626 [D loss: 0.356001, acc.: 86.72%] [G loss: 2.487797]\n",
      "epoch:13 step:10627 [D loss: 0.326178, acc.: 85.94%] [G loss: 3.812790]\n",
      "epoch:13 step:10628 [D loss: 0.354621, acc.: 82.03%] [G loss: 2.770882]\n",
      "epoch:13 step:10629 [D loss: 0.263542, acc.: 88.28%] [G loss: 2.565586]\n",
      "epoch:13 step:10630 [D loss: 0.250663, acc.: 91.41%] [G loss: 2.266713]\n",
      "epoch:13 step:10631 [D loss: 0.312273, acc.: 85.16%] [G loss: 2.613326]\n",
      "epoch:13 step:10632 [D loss: 0.396616, acc.: 87.50%] [G loss: 2.608482]\n",
      "epoch:13 step:10633 [D loss: 0.221098, acc.: 94.53%] [G loss: 3.101617]\n",
      "epoch:13 step:10634 [D loss: 0.344322, acc.: 83.59%] [G loss: 3.372007]\n",
      "epoch:13 step:10635 [D loss: 0.299938, acc.: 84.38%] [G loss: 2.909964]\n",
      "epoch:13 step:10636 [D loss: 0.409566, acc.: 79.69%] [G loss: 1.871863]\n",
      "epoch:13 step:10637 [D loss: 0.399733, acc.: 85.16%] [G loss: 3.102341]\n",
      "epoch:13 step:10638 [D loss: 0.259407, acc.: 88.28%] [G loss: 2.936447]\n",
      "epoch:13 step:10639 [D loss: 0.361975, acc.: 82.81%] [G loss: 2.502981]\n",
      "epoch:13 step:10640 [D loss: 0.281605, acc.: 87.50%] [G loss: 3.528596]\n",
      "epoch:13 step:10641 [D loss: 0.339306, acc.: 88.28%] [G loss: 2.371497]\n",
      "epoch:13 step:10642 [D loss: 0.258377, acc.: 89.84%] [G loss: 2.719624]\n",
      "epoch:13 step:10643 [D loss: 0.294084, acc.: 89.06%] [G loss: 2.981566]\n",
      "epoch:13 step:10644 [D loss: 0.388006, acc.: 84.38%] [G loss: 2.348779]\n",
      "epoch:13 step:10645 [D loss: 0.351706, acc.: 85.94%] [G loss: 2.580265]\n",
      "epoch:13 step:10646 [D loss: 0.472539, acc.: 77.34%] [G loss: 3.095789]\n",
      "epoch:13 step:10647 [D loss: 0.380727, acc.: 84.38%] [G loss: 2.456098]\n",
      "epoch:13 step:10648 [D loss: 0.382242, acc.: 85.16%] [G loss: 2.962324]\n",
      "epoch:13 step:10649 [D loss: 0.323929, acc.: 91.41%] [G loss: 2.527382]\n",
      "epoch:13 step:10650 [D loss: 0.336514, acc.: 85.16%] [G loss: 3.120212]\n",
      "epoch:13 step:10651 [D loss: 0.256797, acc.: 89.06%] [G loss: 4.405772]\n",
      "epoch:13 step:10652 [D loss: 0.265280, acc.: 87.50%] [G loss: 3.876420]\n",
      "epoch:13 step:10653 [D loss: 0.286969, acc.: 85.94%] [G loss: 3.519054]\n",
      "epoch:13 step:10654 [D loss: 0.317201, acc.: 86.72%] [G loss: 3.363602]\n",
      "epoch:13 step:10655 [D loss: 0.288007, acc.: 85.16%] [G loss: 4.284472]\n",
      "epoch:13 step:10656 [D loss: 0.209469, acc.: 92.97%] [G loss: 3.515644]\n",
      "epoch:13 step:10657 [D loss: 0.279934, acc.: 89.84%] [G loss: 2.708834]\n",
      "epoch:13 step:10658 [D loss: 0.291363, acc.: 87.50%] [G loss: 5.008186]\n",
      "epoch:13 step:10659 [D loss: 0.245615, acc.: 91.41%] [G loss: 4.981339]\n",
      "epoch:13 step:10660 [D loss: 0.273517, acc.: 86.72%] [G loss: 4.640320]\n",
      "epoch:13 step:10661 [D loss: 0.337514, acc.: 86.72%] [G loss: 4.295130]\n",
      "epoch:13 step:10662 [D loss: 0.302911, acc.: 89.06%] [G loss: 3.138231]\n",
      "epoch:13 step:10663 [D loss: 0.191928, acc.: 96.09%] [G loss: 5.551560]\n",
      "epoch:13 step:10664 [D loss: 0.226366, acc.: 89.84%] [G loss: 5.158991]\n",
      "epoch:13 step:10665 [D loss: 0.306270, acc.: 87.50%] [G loss: 3.127229]\n",
      "epoch:13 step:10666 [D loss: 0.242820, acc.: 92.19%] [G loss: 2.980812]\n",
      "epoch:13 step:10667 [D loss: 0.276760, acc.: 87.50%] [G loss: 3.366995]\n",
      "epoch:13 step:10668 [D loss: 0.294737, acc.: 90.62%] [G loss: 2.670558]\n",
      "epoch:13 step:10669 [D loss: 0.366579, acc.: 85.94%] [G loss: 2.856985]\n",
      "epoch:13 step:10670 [D loss: 0.362565, acc.: 86.72%] [G loss: 3.658652]\n",
      "epoch:13 step:10671 [D loss: 0.207529, acc.: 91.41%] [G loss: 2.879415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10672 [D loss: 0.287450, acc.: 87.50%] [G loss: 2.657248]\n",
      "epoch:13 step:10673 [D loss: 0.297511, acc.: 91.41%] [G loss: 3.314535]\n",
      "epoch:13 step:10674 [D loss: 0.295065, acc.: 85.94%] [G loss: 2.528383]\n",
      "epoch:13 step:10675 [D loss: 0.312368, acc.: 85.94%] [G loss: 2.539649]\n",
      "epoch:13 step:10676 [D loss: 0.358874, acc.: 87.50%] [G loss: 2.417651]\n",
      "epoch:13 step:10677 [D loss: 0.287166, acc.: 88.28%] [G loss: 2.509660]\n",
      "epoch:13 step:10678 [D loss: 0.312897, acc.: 86.72%] [G loss: 2.886966]\n",
      "epoch:13 step:10679 [D loss: 0.371569, acc.: 82.81%] [G loss: 2.619517]\n",
      "epoch:13 step:10680 [D loss: 0.420969, acc.: 82.81%] [G loss: 4.249897]\n",
      "epoch:13 step:10681 [D loss: 0.433904, acc.: 83.59%] [G loss: 6.864169]\n",
      "epoch:13 step:10682 [D loss: 0.615153, acc.: 77.34%] [G loss: 5.944041]\n",
      "epoch:13 step:10683 [D loss: 1.259326, acc.: 66.41%] [G loss: 7.721114]\n",
      "epoch:13 step:10684 [D loss: 1.866756, acc.: 60.16%] [G loss: 2.947516]\n",
      "epoch:13 step:10685 [D loss: 0.495278, acc.: 78.91%] [G loss: 3.218125]\n",
      "epoch:13 step:10686 [D loss: 0.380618, acc.: 83.59%] [G loss: 2.534173]\n",
      "epoch:13 step:10687 [D loss: 0.272273, acc.: 92.19%] [G loss: 2.937652]\n",
      "epoch:13 step:10688 [D loss: 0.298084, acc.: 82.03%] [G loss: 3.082539]\n",
      "epoch:13 step:10689 [D loss: 0.347104, acc.: 85.94%] [G loss: 2.658721]\n",
      "epoch:13 step:10690 [D loss: 0.403926, acc.: 81.25%] [G loss: 2.562861]\n",
      "epoch:13 step:10691 [D loss: 0.398272, acc.: 80.47%] [G loss: 2.352027]\n",
      "epoch:13 step:10692 [D loss: 0.297084, acc.: 85.94%] [G loss: 2.791516]\n",
      "epoch:13 step:10693 [D loss: 0.354531, acc.: 85.16%] [G loss: 2.657287]\n",
      "epoch:13 step:10694 [D loss: 0.369673, acc.: 87.50%] [G loss: 2.052011]\n",
      "epoch:13 step:10695 [D loss: 0.338409, acc.: 85.16%] [G loss: 2.451592]\n",
      "epoch:13 step:10696 [D loss: 0.354080, acc.: 88.28%] [G loss: 2.494959]\n",
      "epoch:13 step:10697 [D loss: 0.342077, acc.: 85.94%] [G loss: 2.999241]\n",
      "epoch:13 step:10698 [D loss: 0.315055, acc.: 88.28%] [G loss: 3.460848]\n",
      "epoch:13 step:10699 [D loss: 0.236616, acc.: 94.53%] [G loss: 2.819324]\n",
      "epoch:13 step:10700 [D loss: 0.310613, acc.: 86.72%] [G loss: 2.836935]\n",
      "epoch:13 step:10701 [D loss: 0.289856, acc.: 91.41%] [G loss: 2.725085]\n",
      "epoch:13 step:10702 [D loss: 0.326843, acc.: 88.28%] [G loss: 3.137821]\n",
      "epoch:13 step:10703 [D loss: 0.279465, acc.: 88.28%] [G loss: 3.381287]\n",
      "epoch:13 step:10704 [D loss: 0.266277, acc.: 90.62%] [G loss: 2.979348]\n",
      "epoch:13 step:10705 [D loss: 0.379527, acc.: 85.16%] [G loss: 2.831501]\n",
      "epoch:13 step:10706 [D loss: 0.351424, acc.: 84.38%] [G loss: 2.330113]\n",
      "epoch:13 step:10707 [D loss: 0.307267, acc.: 84.38%] [G loss: 2.256852]\n",
      "epoch:13 step:10708 [D loss: 0.310907, acc.: 89.84%] [G loss: 2.209404]\n",
      "epoch:13 step:10709 [D loss: 0.390681, acc.: 85.16%] [G loss: 2.307826]\n",
      "epoch:13 step:10710 [D loss: 0.341840, acc.: 86.72%] [G loss: 2.526902]\n",
      "epoch:13 step:10711 [D loss: 0.309840, acc.: 85.94%] [G loss: 2.546803]\n",
      "epoch:13 step:10712 [D loss: 0.337999, acc.: 84.38%] [G loss: 2.720679]\n",
      "epoch:13 step:10713 [D loss: 0.339835, acc.: 87.50%] [G loss: 1.954708]\n",
      "epoch:13 step:10714 [D loss: 0.418699, acc.: 78.91%] [G loss: 2.747235]\n",
      "epoch:13 step:10715 [D loss: 0.266528, acc.: 92.97%] [G loss: 2.221544]\n",
      "epoch:13 step:10716 [D loss: 0.303036, acc.: 86.72%] [G loss: 3.304071]\n",
      "epoch:13 step:10717 [D loss: 0.325914, acc.: 85.16%] [G loss: 2.319309]\n",
      "epoch:13 step:10718 [D loss: 0.203365, acc.: 96.09%] [G loss: 3.425988]\n",
      "epoch:13 step:10719 [D loss: 0.364356, acc.: 84.38%] [G loss: 3.232594]\n",
      "epoch:13 step:10720 [D loss: 0.263664, acc.: 86.72%] [G loss: 3.719907]\n",
      "epoch:13 step:10721 [D loss: 0.294123, acc.: 87.50%] [G loss: 2.022335]\n",
      "epoch:13 step:10722 [D loss: 0.295579, acc.: 91.41%] [G loss: 2.876776]\n",
      "epoch:13 step:10723 [D loss: 0.309750, acc.: 89.84%] [G loss: 2.710597]\n",
      "epoch:13 step:10724 [D loss: 0.322733, acc.: 88.28%] [G loss: 2.941926]\n",
      "epoch:13 step:10725 [D loss: 0.325508, acc.: 85.94%] [G loss: 3.554608]\n",
      "epoch:13 step:10726 [D loss: 0.281165, acc.: 88.28%] [G loss: 2.964805]\n",
      "epoch:13 step:10727 [D loss: 0.360238, acc.: 86.72%] [G loss: 2.085651]\n",
      "epoch:13 step:10728 [D loss: 0.331152, acc.: 85.16%] [G loss: 2.519581]\n",
      "epoch:13 step:10729 [D loss: 0.300526, acc.: 88.28%] [G loss: 2.888779]\n",
      "epoch:13 step:10730 [D loss: 0.248911, acc.: 89.84%] [G loss: 4.158891]\n",
      "epoch:13 step:10731 [D loss: 0.338759, acc.: 82.03%] [G loss: 3.690832]\n",
      "epoch:13 step:10732 [D loss: 0.367299, acc.: 85.16%] [G loss: 2.920928]\n",
      "epoch:13 step:10733 [D loss: 0.336008, acc.: 85.94%] [G loss: 2.881744]\n",
      "epoch:13 step:10734 [D loss: 0.376858, acc.: 84.38%] [G loss: 2.688487]\n",
      "epoch:13 step:10735 [D loss: 0.274870, acc.: 89.06%] [G loss: 2.828237]\n",
      "epoch:13 step:10736 [D loss: 0.361843, acc.: 85.94%] [G loss: 2.766983]\n",
      "epoch:13 step:10737 [D loss: 0.348145, acc.: 87.50%] [G loss: 3.297155]\n",
      "epoch:13 step:10738 [D loss: 0.309619, acc.: 88.28%] [G loss: 3.052589]\n",
      "epoch:13 step:10739 [D loss: 0.256681, acc.: 85.94%] [G loss: 3.873382]\n",
      "epoch:13 step:10740 [D loss: 0.286285, acc.: 89.06%] [G loss: 3.022968]\n",
      "epoch:13 step:10741 [D loss: 0.227784, acc.: 92.19%] [G loss: 3.433352]\n",
      "epoch:13 step:10742 [D loss: 0.298829, acc.: 88.28%] [G loss: 5.160175]\n",
      "epoch:13 step:10743 [D loss: 0.310569, acc.: 82.81%] [G loss: 4.532078]\n",
      "epoch:13 step:10744 [D loss: 0.327426, acc.: 86.72%] [G loss: 3.457757]\n",
      "epoch:13 step:10745 [D loss: 0.246655, acc.: 89.84%] [G loss: 2.931834]\n",
      "epoch:13 step:10746 [D loss: 0.273831, acc.: 89.84%] [G loss: 3.392769]\n",
      "epoch:13 step:10747 [D loss: 0.355031, acc.: 82.81%] [G loss: 2.851588]\n",
      "epoch:13 step:10748 [D loss: 0.289513, acc.: 87.50%] [G loss: 4.310440]\n",
      "epoch:13 step:10749 [D loss: 0.245956, acc.: 89.84%] [G loss: 4.498310]\n",
      "epoch:13 step:10750 [D loss: 0.418903, acc.: 82.81%] [G loss: 2.408187]\n",
      "epoch:13 step:10751 [D loss: 0.381527, acc.: 82.81%] [G loss: 3.548094]\n",
      "epoch:13 step:10752 [D loss: 0.259974, acc.: 88.28%] [G loss: 4.213368]\n",
      "epoch:13 step:10753 [D loss: 0.412763, acc.: 85.16%] [G loss: 3.046202]\n",
      "epoch:13 step:10754 [D loss: 0.250562, acc.: 89.84%] [G loss: 4.236333]\n",
      "epoch:13 step:10755 [D loss: 0.360351, acc.: 82.03%] [G loss: 3.303429]\n",
      "epoch:13 step:10756 [D loss: 0.389717, acc.: 79.69%] [G loss: 3.019508]\n",
      "epoch:13 step:10757 [D loss: 0.261801, acc.: 88.28%] [G loss: 2.951748]\n",
      "epoch:13 step:10758 [D loss: 0.410530, acc.: 80.47%] [G loss: 3.707794]\n",
      "epoch:13 step:10759 [D loss: 0.386311, acc.: 84.38%] [G loss: 3.399883]\n",
      "epoch:13 step:10760 [D loss: 0.299681, acc.: 89.06%] [G loss: 2.815573]\n",
      "epoch:13 step:10761 [D loss: 0.403101, acc.: 80.47%] [G loss: 2.149112]\n",
      "epoch:13 step:10762 [D loss: 0.364064, acc.: 82.81%] [G loss: 2.064200]\n",
      "epoch:13 step:10763 [D loss: 0.298908, acc.: 87.50%] [G loss: 3.482510]\n",
      "epoch:13 step:10764 [D loss: 0.305389, acc.: 90.62%] [G loss: 3.854085]\n",
      "epoch:13 step:10765 [D loss: 0.393357, acc.: 80.47%] [G loss: 2.648951]\n",
      "epoch:13 step:10766 [D loss: 0.392003, acc.: 82.03%] [G loss: 3.055903]\n",
      "epoch:13 step:10767 [D loss: 0.307077, acc.: 88.28%] [G loss: 2.743094]\n",
      "epoch:13 step:10768 [D loss: 0.344864, acc.: 88.28%] [G loss: 3.081674]\n",
      "epoch:13 step:10769 [D loss: 0.249224, acc.: 91.41%] [G loss: 3.032071]\n",
      "epoch:13 step:10770 [D loss: 0.310573, acc.: 85.94%] [G loss: 3.360627]\n",
      "epoch:13 step:10771 [D loss: 0.282670, acc.: 89.84%] [G loss: 2.889360]\n",
      "epoch:13 step:10772 [D loss: 0.249775, acc.: 87.50%] [G loss: 3.194988]\n",
      "epoch:13 step:10773 [D loss: 0.328361, acc.: 86.72%] [G loss: 3.993574]\n",
      "epoch:13 step:10774 [D loss: 0.272024, acc.: 89.06%] [G loss: 2.257003]\n",
      "epoch:13 step:10775 [D loss: 0.299461, acc.: 86.72%] [G loss: 4.215491]\n",
      "epoch:13 step:10776 [D loss: 0.282481, acc.: 86.72%] [G loss: 4.794603]\n",
      "epoch:13 step:10777 [D loss: 0.345987, acc.: 86.72%] [G loss: 3.089427]\n",
      "epoch:13 step:10778 [D loss: 0.348522, acc.: 86.72%] [G loss: 2.567557]\n",
      "epoch:13 step:10779 [D loss: 0.310568, acc.: 92.19%] [G loss: 2.742970]\n",
      "epoch:13 step:10780 [D loss: 0.303647, acc.: 88.28%] [G loss: 2.724642]\n",
      "epoch:13 step:10781 [D loss: 0.336725, acc.: 86.72%] [G loss: 2.501843]\n",
      "epoch:13 step:10782 [D loss: 0.288027, acc.: 89.84%] [G loss: 2.592111]\n",
      "epoch:13 step:10783 [D loss: 0.348788, acc.: 83.59%] [G loss: 2.945023]\n",
      "epoch:13 step:10784 [D loss: 0.372368, acc.: 82.03%] [G loss: 2.295987]\n",
      "epoch:13 step:10785 [D loss: 0.337905, acc.: 84.38%] [G loss: 3.494331]\n",
      "epoch:13 step:10786 [D loss: 0.473462, acc.: 78.91%] [G loss: 4.894705]\n",
      "epoch:13 step:10787 [D loss: 0.806469, acc.: 66.41%] [G loss: 3.333090]\n",
      "epoch:13 step:10788 [D loss: 0.763151, acc.: 78.91%] [G loss: 8.931679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10789 [D loss: 1.521830, acc.: 63.28%] [G loss: 2.483225]\n",
      "epoch:13 step:10790 [D loss: 0.578267, acc.: 75.00%] [G loss: 2.759629]\n",
      "epoch:13 step:10791 [D loss: 0.431804, acc.: 85.16%] [G loss: 4.640894]\n",
      "epoch:13 step:10792 [D loss: 0.382403, acc.: 82.81%] [G loss: 3.781534]\n",
      "epoch:13 step:10793 [D loss: 0.335745, acc.: 83.59%] [G loss: 2.632071]\n",
      "epoch:13 step:10794 [D loss: 0.264494, acc.: 89.84%] [G loss: 3.456404]\n",
      "epoch:13 step:10795 [D loss: 0.319568, acc.: 84.38%] [G loss: 3.001129]\n",
      "epoch:13 step:10796 [D loss: 0.255761, acc.: 89.06%] [G loss: 2.144462]\n",
      "epoch:13 step:10797 [D loss: 0.303583, acc.: 87.50%] [G loss: 2.678931]\n",
      "epoch:13 step:10798 [D loss: 0.388033, acc.: 83.59%] [G loss: 2.476379]\n",
      "epoch:13 step:10799 [D loss: 0.362652, acc.: 84.38%] [G loss: 2.137163]\n",
      "epoch:13 step:10800 [D loss: 0.293501, acc.: 89.06%] [G loss: 2.297481]\n",
      "epoch:13 step:10801 [D loss: 0.275303, acc.: 88.28%] [G loss: 2.626371]\n",
      "epoch:13 step:10802 [D loss: 0.408875, acc.: 86.72%] [G loss: 2.302535]\n",
      "epoch:13 step:10803 [D loss: 0.366534, acc.: 81.25%] [G loss: 2.322827]\n",
      "epoch:13 step:10804 [D loss: 0.367021, acc.: 84.38%] [G loss: 2.523818]\n",
      "epoch:13 step:10805 [D loss: 0.264993, acc.: 90.62%] [G loss: 2.488951]\n",
      "epoch:13 step:10806 [D loss: 0.313897, acc.: 87.50%] [G loss: 3.346815]\n",
      "epoch:13 step:10807 [D loss: 0.284801, acc.: 85.94%] [G loss: 4.054006]\n",
      "epoch:13 step:10808 [D loss: 0.381099, acc.: 82.03%] [G loss: 1.760712]\n",
      "epoch:13 step:10809 [D loss: 0.391176, acc.: 81.25%] [G loss: 3.402708]\n",
      "epoch:13 step:10810 [D loss: 0.349118, acc.: 85.16%] [G loss: 2.807269]\n",
      "epoch:13 step:10811 [D loss: 0.259708, acc.: 90.62%] [G loss: 3.126094]\n",
      "epoch:13 step:10812 [D loss: 0.362513, acc.: 86.72%] [G loss: 2.218264]\n",
      "epoch:13 step:10813 [D loss: 0.348045, acc.: 86.72%] [G loss: 2.791846]\n",
      "epoch:13 step:10814 [D loss: 0.361026, acc.: 83.59%] [G loss: 2.243917]\n",
      "epoch:13 step:10815 [D loss: 0.328518, acc.: 84.38%] [G loss: 2.829834]\n",
      "epoch:13 step:10816 [D loss: 0.322860, acc.: 82.03%] [G loss: 2.388300]\n",
      "epoch:13 step:10817 [D loss: 0.330017, acc.: 85.94%] [G loss: 2.816953]\n",
      "epoch:13 step:10818 [D loss: 0.312796, acc.: 85.16%] [G loss: 2.540056]\n",
      "epoch:13 step:10819 [D loss: 0.306675, acc.: 85.94%] [G loss: 3.169568]\n",
      "epoch:13 step:10820 [D loss: 0.282267, acc.: 90.62%] [G loss: 2.686778]\n",
      "epoch:13 step:10821 [D loss: 0.284716, acc.: 86.72%] [G loss: 3.655940]\n",
      "epoch:13 step:10822 [D loss: 0.447560, acc.: 80.47%] [G loss: 2.249780]\n",
      "epoch:13 step:10823 [D loss: 0.259473, acc.: 87.50%] [G loss: 3.070272]\n",
      "epoch:13 step:10824 [D loss: 0.311487, acc.: 87.50%] [G loss: 3.352171]\n",
      "epoch:13 step:10825 [D loss: 0.315548, acc.: 85.94%] [G loss: 3.102894]\n",
      "epoch:13 step:10826 [D loss: 0.322130, acc.: 87.50%] [G loss: 2.598515]\n",
      "epoch:13 step:10827 [D loss: 0.352436, acc.: 83.59%] [G loss: 2.664911]\n",
      "epoch:13 step:10828 [D loss: 0.362748, acc.: 83.59%] [G loss: 2.860299]\n",
      "epoch:13 step:10829 [D loss: 0.341000, acc.: 89.06%] [G loss: 3.273019]\n",
      "epoch:13 step:10830 [D loss: 0.333904, acc.: 87.50%] [G loss: 3.021244]\n",
      "epoch:13 step:10831 [D loss: 0.265218, acc.: 87.50%] [G loss: 2.942605]\n",
      "epoch:13 step:10832 [D loss: 0.250468, acc.: 92.19%] [G loss: 3.510698]\n",
      "epoch:13 step:10833 [D loss: 0.291281, acc.: 87.50%] [G loss: 3.912581]\n",
      "epoch:13 step:10834 [D loss: 0.189311, acc.: 93.75%] [G loss: 3.781248]\n",
      "epoch:13 step:10835 [D loss: 0.206610, acc.: 96.09%] [G loss: 3.247811]\n",
      "epoch:13 step:10836 [D loss: 0.295894, acc.: 86.72%] [G loss: 3.340239]\n",
      "epoch:13 step:10837 [D loss: 0.196075, acc.: 95.31%] [G loss: 4.026568]\n",
      "epoch:13 step:10838 [D loss: 0.233442, acc.: 92.97%] [G loss: 2.871582]\n",
      "epoch:13 step:10839 [D loss: 0.354410, acc.: 82.03%] [G loss: 2.236306]\n",
      "epoch:13 step:10840 [D loss: 0.292396, acc.: 87.50%] [G loss: 3.034015]\n",
      "epoch:13 step:10841 [D loss: 0.244065, acc.: 91.41%] [G loss: 3.209918]\n",
      "epoch:13 step:10842 [D loss: 0.304737, acc.: 84.38%] [G loss: 3.016329]\n",
      "epoch:13 step:10843 [D loss: 0.214979, acc.: 92.19%] [G loss: 2.834495]\n",
      "epoch:13 step:10844 [D loss: 0.249256, acc.: 91.41%] [G loss: 2.919997]\n",
      "epoch:13 step:10845 [D loss: 0.390000, acc.: 84.38%] [G loss: 3.363800]\n",
      "epoch:13 step:10846 [D loss: 0.265037, acc.: 88.28%] [G loss: 2.966500]\n",
      "epoch:13 step:10847 [D loss: 0.426454, acc.: 85.16%] [G loss: 3.869066]\n",
      "epoch:13 step:10848 [D loss: 0.530885, acc.: 78.12%] [G loss: 2.679676]\n",
      "epoch:13 step:10849 [D loss: 0.631388, acc.: 75.78%] [G loss: 6.502306]\n",
      "epoch:13 step:10850 [D loss: 0.458916, acc.: 80.47%] [G loss: 5.886402]\n",
      "epoch:13 step:10851 [D loss: 0.229470, acc.: 91.41%] [G loss: 3.842466]\n",
      "epoch:13 step:10852 [D loss: 0.395272, acc.: 80.47%] [G loss: 3.853637]\n",
      "epoch:13 step:10853 [D loss: 0.232449, acc.: 91.41%] [G loss: 3.508040]\n",
      "epoch:13 step:10854 [D loss: 0.394414, acc.: 85.16%] [G loss: 3.363100]\n",
      "epoch:13 step:10855 [D loss: 0.317263, acc.: 85.94%] [G loss: 3.731672]\n",
      "epoch:13 step:10856 [D loss: 0.388595, acc.: 78.91%] [G loss: 2.674935]\n",
      "epoch:13 step:10857 [D loss: 0.369252, acc.: 82.81%] [G loss: 2.486373]\n",
      "epoch:13 step:10858 [D loss: 0.352440, acc.: 84.38%] [G loss: 3.057678]\n",
      "epoch:13 step:10859 [D loss: 0.287033, acc.: 89.84%] [G loss: 3.554200]\n",
      "epoch:13 step:10860 [D loss: 0.254611, acc.: 89.06%] [G loss: 3.555655]\n",
      "epoch:13 step:10861 [D loss: 0.295970, acc.: 84.38%] [G loss: 3.234226]\n",
      "epoch:13 step:10862 [D loss: 0.329902, acc.: 83.59%] [G loss: 2.619084]\n",
      "epoch:13 step:10863 [D loss: 0.318510, acc.: 85.16%] [G loss: 2.931002]\n",
      "epoch:13 step:10864 [D loss: 0.283088, acc.: 86.72%] [G loss: 5.395214]\n",
      "epoch:13 step:10865 [D loss: 0.309573, acc.: 83.59%] [G loss: 2.826125]\n",
      "epoch:13 step:10866 [D loss: 0.251046, acc.: 89.84%] [G loss: 4.256567]\n",
      "epoch:13 step:10867 [D loss: 0.254995, acc.: 89.84%] [G loss: 2.644358]\n",
      "epoch:13 step:10868 [D loss: 0.325869, acc.: 85.16%] [G loss: 3.355405]\n",
      "epoch:13 step:10869 [D loss: 0.287179, acc.: 87.50%] [G loss: 2.967765]\n",
      "epoch:13 step:10870 [D loss: 0.264829, acc.: 86.72%] [G loss: 3.757882]\n",
      "epoch:13 step:10871 [D loss: 0.427112, acc.: 78.91%] [G loss: 3.483362]\n",
      "epoch:13 step:10872 [D loss: 0.238053, acc.: 89.84%] [G loss: 4.425505]\n",
      "epoch:13 step:10873 [D loss: 0.223385, acc.: 94.53%] [G loss: 4.323676]\n",
      "epoch:13 step:10874 [D loss: 0.306760, acc.: 85.94%] [G loss: 2.472688]\n",
      "epoch:13 step:10875 [D loss: 0.321265, acc.: 85.94%] [G loss: 2.388605]\n",
      "epoch:13 step:10876 [D loss: 0.329399, acc.: 89.06%] [G loss: 2.912497]\n",
      "epoch:13 step:10877 [D loss: 0.298827, acc.: 89.84%] [G loss: 2.732408]\n",
      "epoch:13 step:10878 [D loss: 0.274374, acc.: 90.62%] [G loss: 2.658013]\n",
      "epoch:13 step:10879 [D loss: 0.256114, acc.: 90.62%] [G loss: 2.789942]\n",
      "epoch:13 step:10880 [D loss: 0.372338, acc.: 86.72%] [G loss: 2.939060]\n",
      "epoch:13 step:10881 [D loss: 0.369924, acc.: 82.81%] [G loss: 2.595537]\n",
      "epoch:13 step:10882 [D loss: 0.404209, acc.: 80.47%] [G loss: 3.369168]\n",
      "epoch:13 step:10883 [D loss: 0.425804, acc.: 79.69%] [G loss: 2.794428]\n",
      "epoch:13 step:10884 [D loss: 0.346138, acc.: 82.03%] [G loss: 2.741556]\n",
      "epoch:13 step:10885 [D loss: 0.292096, acc.: 85.94%] [G loss: 3.351186]\n",
      "epoch:13 step:10886 [D loss: 0.362985, acc.: 83.59%] [G loss: 3.519847]\n",
      "epoch:13 step:10887 [D loss: 0.325633, acc.: 86.72%] [G loss: 2.551411]\n",
      "epoch:13 step:10888 [D loss: 0.274266, acc.: 88.28%] [G loss: 3.045295]\n",
      "epoch:13 step:10889 [D loss: 0.356920, acc.: 83.59%] [G loss: 2.146133]\n",
      "epoch:13 step:10890 [D loss: 0.283703, acc.: 89.84%] [G loss: 2.795401]\n",
      "epoch:13 step:10891 [D loss: 0.285597, acc.: 87.50%] [G loss: 2.825747]\n",
      "epoch:13 step:10892 [D loss: 0.312178, acc.: 85.16%] [G loss: 2.529908]\n",
      "epoch:13 step:10893 [D loss: 0.239118, acc.: 90.62%] [G loss: 2.751871]\n",
      "epoch:13 step:10894 [D loss: 0.266206, acc.: 89.06%] [G loss: 3.158248]\n",
      "epoch:13 step:10895 [D loss: 0.275182, acc.: 89.84%] [G loss: 2.392541]\n",
      "epoch:13 step:10896 [D loss: 0.275898, acc.: 89.84%] [G loss: 3.330816]\n",
      "epoch:13 step:10897 [D loss: 0.243032, acc.: 90.62%] [G loss: 3.067992]\n",
      "epoch:13 step:10898 [D loss: 0.331185, acc.: 88.28%] [G loss: 3.446785]\n",
      "epoch:13 step:10899 [D loss: 0.209589, acc.: 93.75%] [G loss: 3.609593]\n",
      "epoch:13 step:10900 [D loss: 0.233668, acc.: 89.84%] [G loss: 4.422526]\n",
      "epoch:13 step:10901 [D loss: 0.283328, acc.: 88.28%] [G loss: 3.747520]\n",
      "epoch:13 step:10902 [D loss: 0.199936, acc.: 94.53%] [G loss: 2.886617]\n",
      "epoch:13 step:10903 [D loss: 0.242289, acc.: 89.84%] [G loss: 2.935764]\n",
      "epoch:13 step:10904 [D loss: 0.303302, acc.: 89.06%] [G loss: 2.606705]\n",
      "epoch:13 step:10905 [D loss: 0.276155, acc.: 89.06%] [G loss: 4.018669]\n",
      "epoch:13 step:10906 [D loss: 0.350749, acc.: 85.16%] [G loss: 3.356766]\n",
      "epoch:13 step:10907 [D loss: 0.561408, acc.: 68.75%] [G loss: 3.037403]\n",
      "epoch:13 step:10908 [D loss: 0.386548, acc.: 85.16%] [G loss: 2.450876]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10909 [D loss: 0.370761, acc.: 83.59%] [G loss: 2.782222]\n",
      "epoch:13 step:10910 [D loss: 0.305887, acc.: 87.50%] [G loss: 3.002031]\n",
      "epoch:13 step:10911 [D loss: 0.324845, acc.: 87.50%] [G loss: 2.501938]\n",
      "epoch:13 step:10912 [D loss: 0.319962, acc.: 82.03%] [G loss: 4.599103]\n",
      "epoch:13 step:10913 [D loss: 0.403958, acc.: 86.72%] [G loss: 2.774097]\n",
      "epoch:13 step:10914 [D loss: 0.437863, acc.: 77.34%] [G loss: 3.743173]\n",
      "epoch:13 step:10915 [D loss: 0.350703, acc.: 87.50%] [G loss: 2.951584]\n",
      "epoch:13 step:10916 [D loss: 0.248730, acc.: 92.19%] [G loss: 3.664895]\n",
      "epoch:13 step:10917 [D loss: 0.356327, acc.: 85.94%] [G loss: 2.702687]\n",
      "epoch:13 step:10918 [D loss: 0.433196, acc.: 81.25%] [G loss: 2.517026]\n",
      "epoch:13 step:10919 [D loss: 0.307806, acc.: 86.72%] [G loss: 2.281914]\n",
      "epoch:13 step:10920 [D loss: 0.334481, acc.: 88.28%] [G loss: 2.866241]\n",
      "epoch:13 step:10921 [D loss: 0.288531, acc.: 90.62%] [G loss: 3.643488]\n",
      "epoch:13 step:10922 [D loss: 0.297102, acc.: 86.72%] [G loss: 2.615326]\n",
      "epoch:13 step:10923 [D loss: 0.355965, acc.: 85.94%] [G loss: 2.959118]\n",
      "epoch:13 step:10924 [D loss: 0.278971, acc.: 91.41%] [G loss: 2.913973]\n",
      "epoch:13 step:10925 [D loss: 0.289400, acc.: 89.06%] [G loss: 3.979699]\n",
      "epoch:13 step:10926 [D loss: 0.349065, acc.: 82.03%] [G loss: 2.667584]\n",
      "epoch:13 step:10927 [D loss: 0.321704, acc.: 82.03%] [G loss: 2.292581]\n",
      "epoch:13 step:10928 [D loss: 0.324547, acc.: 86.72%] [G loss: 2.764303]\n",
      "epoch:13 step:10929 [D loss: 0.304681, acc.: 88.28%] [G loss: 3.212183]\n",
      "epoch:13 step:10930 [D loss: 0.462449, acc.: 77.34%] [G loss: 2.323434]\n",
      "epoch:13 step:10931 [D loss: 0.324820, acc.: 85.16%] [G loss: 2.635868]\n",
      "epoch:13 step:10932 [D loss: 0.394942, acc.: 87.50%] [G loss: 3.166262]\n",
      "epoch:13 step:10933 [D loss: 0.349287, acc.: 82.03%] [G loss: 3.596372]\n",
      "epoch:13 step:10934 [D loss: 0.454523, acc.: 78.12%] [G loss: 4.232976]\n",
      "epoch:14 step:10935 [D loss: 0.383047, acc.: 84.38%] [G loss: 2.811629]\n",
      "epoch:14 step:10936 [D loss: 0.258878, acc.: 90.62%] [G loss: 3.304052]\n",
      "epoch:14 step:10937 [D loss: 0.258134, acc.: 89.84%] [G loss: 3.313192]\n",
      "epoch:14 step:10938 [D loss: 0.289659, acc.: 91.41%] [G loss: 2.638190]\n",
      "epoch:14 step:10939 [D loss: 0.291363, acc.: 87.50%] [G loss: 3.497045]\n",
      "epoch:14 step:10940 [D loss: 0.202853, acc.: 93.75%] [G loss: 3.920935]\n",
      "epoch:14 step:10941 [D loss: 0.272501, acc.: 89.84%] [G loss: 2.679135]\n",
      "epoch:14 step:10942 [D loss: 0.254331, acc.: 92.19%] [G loss: 2.787381]\n",
      "epoch:14 step:10943 [D loss: 0.305599, acc.: 88.28%] [G loss: 3.273686]\n",
      "epoch:14 step:10944 [D loss: 0.244747, acc.: 89.06%] [G loss: 3.205550]\n",
      "epoch:14 step:10945 [D loss: 0.310036, acc.: 85.94%] [G loss: 2.582174]\n",
      "epoch:14 step:10946 [D loss: 0.239951, acc.: 94.53%] [G loss: 3.102547]\n",
      "epoch:14 step:10947 [D loss: 0.306880, acc.: 85.16%] [G loss: 3.823136]\n",
      "epoch:14 step:10948 [D loss: 0.329892, acc.: 85.94%] [G loss: 3.717858]\n",
      "epoch:14 step:10949 [D loss: 0.397473, acc.: 82.81%] [G loss: 2.856735]\n",
      "epoch:14 step:10950 [D loss: 0.371018, acc.: 81.25%] [G loss: 3.844856]\n",
      "epoch:14 step:10951 [D loss: 0.355064, acc.: 82.81%] [G loss: 2.722040]\n",
      "epoch:14 step:10952 [D loss: 0.382227, acc.: 83.59%] [G loss: 2.831016]\n",
      "epoch:14 step:10953 [D loss: 0.323010, acc.: 91.41%] [G loss: 3.049799]\n",
      "epoch:14 step:10954 [D loss: 0.284986, acc.: 88.28%] [G loss: 4.103844]\n",
      "epoch:14 step:10955 [D loss: 0.331671, acc.: 89.84%] [G loss: 2.997793]\n",
      "epoch:14 step:10956 [D loss: 0.373484, acc.: 85.16%] [G loss: 4.042080]\n",
      "epoch:14 step:10957 [D loss: 0.351617, acc.: 83.59%] [G loss: 3.293397]\n",
      "epoch:14 step:10958 [D loss: 0.303059, acc.: 85.94%] [G loss: 2.646219]\n",
      "epoch:14 step:10959 [D loss: 0.303948, acc.: 89.84%] [G loss: 3.099291]\n",
      "epoch:14 step:10960 [D loss: 0.278995, acc.: 85.16%] [G loss: 2.880512]\n",
      "epoch:14 step:10961 [D loss: 0.298072, acc.: 89.84%] [G loss: 3.013658]\n",
      "epoch:14 step:10962 [D loss: 0.319443, acc.: 85.94%] [G loss: 2.231471]\n",
      "epoch:14 step:10963 [D loss: 0.427189, acc.: 80.47%] [G loss: 2.626600]\n",
      "epoch:14 step:10964 [D loss: 0.187993, acc.: 92.19%] [G loss: 3.143724]\n",
      "epoch:14 step:10965 [D loss: 0.237941, acc.: 89.06%] [G loss: 6.531212]\n",
      "epoch:14 step:10966 [D loss: 0.353131, acc.: 84.38%] [G loss: 2.586667]\n",
      "epoch:14 step:10967 [D loss: 0.174224, acc.: 92.97%] [G loss: 3.818383]\n",
      "epoch:14 step:10968 [D loss: 0.379525, acc.: 83.59%] [G loss: 3.088661]\n",
      "epoch:14 step:10969 [D loss: 0.241241, acc.: 92.19%] [G loss: 3.375972]\n",
      "epoch:14 step:10970 [D loss: 0.337347, acc.: 85.94%] [G loss: 3.846801]\n",
      "epoch:14 step:10971 [D loss: 0.449376, acc.: 83.59%] [G loss: 2.450438]\n",
      "epoch:14 step:10972 [D loss: 0.217575, acc.: 92.19%] [G loss: 2.187657]\n",
      "epoch:14 step:10973 [D loss: 0.310560, acc.: 84.38%] [G loss: 3.159911]\n",
      "epoch:14 step:10974 [D loss: 0.199398, acc.: 92.97%] [G loss: 6.421920]\n",
      "epoch:14 step:10975 [D loss: 0.329557, acc.: 83.59%] [G loss: 4.834884]\n",
      "epoch:14 step:10976 [D loss: 0.283170, acc.: 87.50%] [G loss: 2.826951]\n",
      "epoch:14 step:10977 [D loss: 0.265676, acc.: 89.84%] [G loss: 4.764973]\n",
      "epoch:14 step:10978 [D loss: 0.310018, acc.: 84.38%] [G loss: 5.234711]\n",
      "epoch:14 step:10979 [D loss: 0.316927, acc.: 87.50%] [G loss: 2.585176]\n",
      "epoch:14 step:10980 [D loss: 0.330785, acc.: 86.72%] [G loss: 3.113079]\n",
      "epoch:14 step:10981 [D loss: 0.287327, acc.: 85.16%] [G loss: 3.134047]\n",
      "epoch:14 step:10982 [D loss: 0.247965, acc.: 87.50%] [G loss: 3.231208]\n",
      "epoch:14 step:10983 [D loss: 0.335455, acc.: 85.16%] [G loss: 2.536108]\n",
      "epoch:14 step:10984 [D loss: 0.350439, acc.: 85.94%] [G loss: 3.421774]\n",
      "epoch:14 step:10985 [D loss: 0.352086, acc.: 82.81%] [G loss: 3.753433]\n",
      "epoch:14 step:10986 [D loss: 0.260416, acc.: 85.94%] [G loss: 3.249788]\n",
      "epoch:14 step:10987 [D loss: 0.358485, acc.: 85.16%] [G loss: 5.680001]\n",
      "epoch:14 step:10988 [D loss: 0.561187, acc.: 74.22%] [G loss: 4.965098]\n",
      "epoch:14 step:10989 [D loss: 0.391639, acc.: 82.03%] [G loss: 4.086364]\n",
      "epoch:14 step:10990 [D loss: 0.453251, acc.: 78.12%] [G loss: 3.567201]\n",
      "epoch:14 step:10991 [D loss: 0.490393, acc.: 78.91%] [G loss: 3.671286]\n",
      "epoch:14 step:10992 [D loss: 0.418967, acc.: 81.25%] [G loss: 3.592024]\n",
      "epoch:14 step:10993 [D loss: 0.296162, acc.: 85.94%] [G loss: 3.031217]\n",
      "epoch:14 step:10994 [D loss: 0.370446, acc.: 82.81%] [G loss: 2.701738]\n",
      "epoch:14 step:10995 [D loss: 0.352770, acc.: 79.69%] [G loss: 2.824928]\n",
      "epoch:14 step:10996 [D loss: 0.390330, acc.: 82.81%] [G loss: 3.592746]\n",
      "epoch:14 step:10997 [D loss: 0.540568, acc.: 78.12%] [G loss: 2.701995]\n",
      "epoch:14 step:10998 [D loss: 0.418868, acc.: 79.69%] [G loss: 2.372534]\n",
      "epoch:14 step:10999 [D loss: 0.412304, acc.: 80.47%] [G loss: 2.467165]\n",
      "epoch:14 step:11000 [D loss: 0.347958, acc.: 85.94%] [G loss: 3.292375]\n",
      "epoch:14 step:11001 [D loss: 0.329050, acc.: 85.16%] [G loss: 3.017694]\n",
      "epoch:14 step:11002 [D loss: 0.347168, acc.: 81.25%] [G loss: 3.732283]\n",
      "epoch:14 step:11003 [D loss: 0.227494, acc.: 88.28%] [G loss: 6.431344]\n",
      "epoch:14 step:11004 [D loss: 0.282288, acc.: 90.62%] [G loss: 2.900287]\n",
      "epoch:14 step:11005 [D loss: 0.287168, acc.: 85.94%] [G loss: 5.788361]\n",
      "epoch:14 step:11006 [D loss: 0.283737, acc.: 92.19%] [G loss: 3.236436]\n",
      "epoch:14 step:11007 [D loss: 0.298124, acc.: 87.50%] [G loss: 1.945813]\n",
      "epoch:14 step:11008 [D loss: 0.344295, acc.: 86.72%] [G loss: 2.368675]\n",
      "epoch:14 step:11009 [D loss: 0.298257, acc.: 86.72%] [G loss: 3.148744]\n",
      "epoch:14 step:11010 [D loss: 0.453216, acc.: 78.91%] [G loss: 2.722301]\n",
      "epoch:14 step:11011 [D loss: 0.508773, acc.: 78.91%] [G loss: 2.807804]\n",
      "epoch:14 step:11012 [D loss: 0.344387, acc.: 84.38%] [G loss: 4.763603]\n",
      "epoch:14 step:11013 [D loss: 0.566515, acc.: 71.88%] [G loss: 5.083323]\n",
      "epoch:14 step:11014 [D loss: 0.410744, acc.: 81.25%] [G loss: 3.788885]\n",
      "epoch:14 step:11015 [D loss: 0.372260, acc.: 82.03%] [G loss: 5.080260]\n",
      "epoch:14 step:11016 [D loss: 0.449038, acc.: 80.47%] [G loss: 3.086648]\n",
      "epoch:14 step:11017 [D loss: 0.459416, acc.: 81.25%] [G loss: 3.441699]\n",
      "epoch:14 step:11018 [D loss: 0.374447, acc.: 78.91%] [G loss: 2.649494]\n",
      "epoch:14 step:11019 [D loss: 0.293946, acc.: 85.16%] [G loss: 3.929195]\n",
      "epoch:14 step:11020 [D loss: 0.485202, acc.: 77.34%] [G loss: 4.509825]\n",
      "epoch:14 step:11021 [D loss: 0.293317, acc.: 88.28%] [G loss: 5.389066]\n",
      "epoch:14 step:11022 [D loss: 0.302988, acc.: 85.94%] [G loss: 5.091752]\n",
      "epoch:14 step:11023 [D loss: 0.325065, acc.: 85.16%] [G loss: 4.337379]\n",
      "epoch:14 step:11024 [D loss: 0.384636, acc.: 82.81%] [G loss: 3.769031]\n",
      "epoch:14 step:11025 [D loss: 0.301506, acc.: 87.50%] [G loss: 6.035717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11026 [D loss: 0.470883, acc.: 78.91%] [G loss: 2.598920]\n",
      "epoch:14 step:11027 [D loss: 0.364175, acc.: 79.69%] [G loss: 3.127931]\n",
      "epoch:14 step:11028 [D loss: 0.384004, acc.: 83.59%] [G loss: 3.401666]\n",
      "epoch:14 step:11029 [D loss: 0.469767, acc.: 80.47%] [G loss: 3.595464]\n",
      "epoch:14 step:11030 [D loss: 0.488818, acc.: 75.78%] [G loss: 4.174689]\n",
      "epoch:14 step:11031 [D loss: 0.487043, acc.: 80.47%] [G loss: 3.472952]\n",
      "epoch:14 step:11032 [D loss: 0.301710, acc.: 87.50%] [G loss: 4.019153]\n",
      "epoch:14 step:11033 [D loss: 0.462757, acc.: 76.56%] [G loss: 3.050310]\n",
      "epoch:14 step:11034 [D loss: 0.461293, acc.: 81.25%] [G loss: 2.892826]\n",
      "epoch:14 step:11035 [D loss: 0.289519, acc.: 89.84%] [G loss: 2.622030]\n",
      "epoch:14 step:11036 [D loss: 0.337284, acc.: 83.59%] [G loss: 2.763046]\n",
      "epoch:14 step:11037 [D loss: 0.479977, acc.: 77.34%] [G loss: 3.400883]\n",
      "epoch:14 step:11038 [D loss: 0.272422, acc.: 86.72%] [G loss: 5.229246]\n",
      "epoch:14 step:11039 [D loss: 0.436198, acc.: 85.94%] [G loss: 2.423624]\n",
      "epoch:14 step:11040 [D loss: 0.223136, acc.: 89.84%] [G loss: 5.291714]\n",
      "epoch:14 step:11041 [D loss: 0.210365, acc.: 93.75%] [G loss: 5.889811]\n",
      "epoch:14 step:11042 [D loss: 0.455459, acc.: 81.25%] [G loss: 3.046402]\n",
      "epoch:14 step:11043 [D loss: 0.383069, acc.: 82.81%] [G loss: 3.328074]\n",
      "epoch:14 step:11044 [D loss: 0.255516, acc.: 88.28%] [G loss: 3.364542]\n",
      "epoch:14 step:11045 [D loss: 0.382849, acc.: 83.59%] [G loss: 2.595202]\n",
      "epoch:14 step:11046 [D loss: 0.267518, acc.: 88.28%] [G loss: 2.660310]\n",
      "epoch:14 step:11047 [D loss: 0.349291, acc.: 85.94%] [G loss: 3.062186]\n",
      "epoch:14 step:11048 [D loss: 0.342960, acc.: 86.72%] [G loss: 2.762658]\n",
      "epoch:14 step:11049 [D loss: 0.242358, acc.: 89.84%] [G loss: 2.904277]\n",
      "epoch:14 step:11050 [D loss: 0.435433, acc.: 85.94%] [G loss: 4.338067]\n",
      "epoch:14 step:11051 [D loss: 0.249905, acc.: 88.28%] [G loss: 5.642257]\n",
      "epoch:14 step:11052 [D loss: 0.401880, acc.: 85.94%] [G loss: 2.709873]\n",
      "epoch:14 step:11053 [D loss: 0.308971, acc.: 85.94%] [G loss: 2.850667]\n",
      "epoch:14 step:11054 [D loss: 0.459345, acc.: 78.12%] [G loss: 2.887054]\n",
      "epoch:14 step:11055 [D loss: 0.303850, acc.: 85.94%] [G loss: 3.564272]\n",
      "epoch:14 step:11056 [D loss: 0.445631, acc.: 76.56%] [G loss: 2.766881]\n",
      "epoch:14 step:11057 [D loss: 0.371307, acc.: 86.72%] [G loss: 2.460012]\n",
      "epoch:14 step:11058 [D loss: 0.423896, acc.: 80.47%] [G loss: 2.777219]\n",
      "epoch:14 step:11059 [D loss: 0.443231, acc.: 78.12%] [G loss: 5.048343]\n",
      "epoch:14 step:11060 [D loss: 0.348618, acc.: 82.03%] [G loss: 4.112813]\n",
      "epoch:14 step:11061 [D loss: 0.306246, acc.: 89.84%] [G loss: 3.213319]\n",
      "epoch:14 step:11062 [D loss: 0.317785, acc.: 87.50%] [G loss: 3.103920]\n",
      "epoch:14 step:11063 [D loss: 0.381061, acc.: 85.94%] [G loss: 2.707136]\n",
      "epoch:14 step:11064 [D loss: 0.337991, acc.: 82.81%] [G loss: 3.269765]\n",
      "epoch:14 step:11065 [D loss: 0.506064, acc.: 74.22%] [G loss: 2.838974]\n",
      "epoch:14 step:11066 [D loss: 0.375754, acc.: 84.38%] [G loss: 5.300067]\n",
      "epoch:14 step:11067 [D loss: 0.238527, acc.: 92.19%] [G loss: 4.136108]\n",
      "epoch:14 step:11068 [D loss: 0.368829, acc.: 85.16%] [G loss: 2.682412]\n",
      "epoch:14 step:11069 [D loss: 0.193798, acc.: 92.97%] [G loss: 3.852582]\n",
      "epoch:14 step:11070 [D loss: 0.302017, acc.: 88.28%] [G loss: 2.672134]\n",
      "epoch:14 step:11071 [D loss: 0.316562, acc.: 89.06%] [G loss: 2.397191]\n",
      "epoch:14 step:11072 [D loss: 0.408384, acc.: 85.16%] [G loss: 2.474597]\n",
      "epoch:14 step:11073 [D loss: 0.232679, acc.: 91.41%] [G loss: 2.568376]\n",
      "epoch:14 step:11074 [D loss: 0.370611, acc.: 86.72%] [G loss: 2.714777]\n",
      "epoch:14 step:11075 [D loss: 0.363713, acc.: 84.38%] [G loss: 2.983012]\n",
      "epoch:14 step:11076 [D loss: 0.416002, acc.: 84.38%] [G loss: 3.019958]\n",
      "epoch:14 step:11077 [D loss: 0.342317, acc.: 88.28%] [G loss: 3.208545]\n",
      "epoch:14 step:11078 [D loss: 0.300278, acc.: 87.50%] [G loss: 4.078450]\n",
      "epoch:14 step:11079 [D loss: 0.529923, acc.: 75.78%] [G loss: 2.136721]\n",
      "epoch:14 step:11080 [D loss: 0.331538, acc.: 85.16%] [G loss: 3.183200]\n",
      "epoch:14 step:11081 [D loss: 0.357105, acc.: 85.94%] [G loss: 3.465105]\n",
      "epoch:14 step:11082 [D loss: 0.281307, acc.: 85.94%] [G loss: 3.260689]\n",
      "epoch:14 step:11083 [D loss: 0.269994, acc.: 86.72%] [G loss: 4.247000]\n",
      "epoch:14 step:11084 [D loss: 0.312125, acc.: 88.28%] [G loss: 3.818202]\n",
      "epoch:14 step:11085 [D loss: 0.252378, acc.: 91.41%] [G loss: 3.786117]\n",
      "epoch:14 step:11086 [D loss: 0.389233, acc.: 79.69%] [G loss: 2.638564]\n",
      "epoch:14 step:11087 [D loss: 0.274389, acc.: 86.72%] [G loss: 3.030875]\n",
      "epoch:14 step:11088 [D loss: 0.353900, acc.: 85.16%] [G loss: 3.057060]\n",
      "epoch:14 step:11089 [D loss: 0.318480, acc.: 85.94%] [G loss: 2.894982]\n",
      "epoch:14 step:11090 [D loss: 0.241714, acc.: 91.41%] [G loss: 3.718730]\n",
      "epoch:14 step:11091 [D loss: 0.371145, acc.: 80.47%] [G loss: 2.760343]\n",
      "epoch:14 step:11092 [D loss: 0.172119, acc.: 92.19%] [G loss: 4.792243]\n",
      "epoch:14 step:11093 [D loss: 0.216067, acc.: 90.62%] [G loss: 7.744059]\n",
      "epoch:14 step:11094 [D loss: 0.357761, acc.: 82.03%] [G loss: 3.342263]\n",
      "epoch:14 step:11095 [D loss: 0.293181, acc.: 85.94%] [G loss: 4.530495]\n",
      "epoch:14 step:11096 [D loss: 0.304085, acc.: 85.94%] [G loss: 2.884650]\n",
      "epoch:14 step:11097 [D loss: 0.266909, acc.: 92.19%] [G loss: 2.544192]\n",
      "epoch:14 step:11098 [D loss: 0.304410, acc.: 85.94%] [G loss: 3.201974]\n",
      "epoch:14 step:11099 [D loss: 0.523885, acc.: 78.91%] [G loss: 2.186484]\n",
      "epoch:14 step:11100 [D loss: 0.284420, acc.: 89.06%] [G loss: 2.574421]\n",
      "epoch:14 step:11101 [D loss: 0.265506, acc.: 90.62%] [G loss: 3.626878]\n",
      "epoch:14 step:11102 [D loss: 0.346629, acc.: 84.38%] [G loss: 2.881099]\n",
      "epoch:14 step:11103 [D loss: 0.309186, acc.: 87.50%] [G loss: 2.142985]\n",
      "epoch:14 step:11104 [D loss: 0.330544, acc.: 84.38%] [G loss: 3.471371]\n",
      "epoch:14 step:11105 [D loss: 0.407328, acc.: 85.16%] [G loss: 3.189569]\n",
      "epoch:14 step:11106 [D loss: 0.224398, acc.: 93.75%] [G loss: 4.817084]\n",
      "epoch:14 step:11107 [D loss: 0.283022, acc.: 87.50%] [G loss: 3.480482]\n",
      "epoch:14 step:11108 [D loss: 0.559267, acc.: 79.69%] [G loss: 2.419937]\n",
      "epoch:14 step:11109 [D loss: 0.360543, acc.: 85.16%] [G loss: 3.448917]\n",
      "epoch:14 step:11110 [D loss: 0.330618, acc.: 87.50%] [G loss: 2.258491]\n",
      "epoch:14 step:11111 [D loss: 0.337654, acc.: 88.28%] [G loss: 2.161491]\n",
      "epoch:14 step:11112 [D loss: 0.290099, acc.: 86.72%] [G loss: 2.526747]\n",
      "epoch:14 step:11113 [D loss: 0.303296, acc.: 88.28%] [G loss: 3.020126]\n",
      "epoch:14 step:11114 [D loss: 0.322420, acc.: 86.72%] [G loss: 2.736487]\n",
      "epoch:14 step:11115 [D loss: 0.253078, acc.: 89.06%] [G loss: 2.746155]\n",
      "epoch:14 step:11116 [D loss: 0.345984, acc.: 85.16%] [G loss: 3.411523]\n",
      "epoch:14 step:11117 [D loss: 0.416139, acc.: 84.38%] [G loss: 2.265925]\n",
      "epoch:14 step:11118 [D loss: 0.310372, acc.: 87.50%] [G loss: 2.682991]\n",
      "epoch:14 step:11119 [D loss: 0.322840, acc.: 86.72%] [G loss: 3.150140]\n",
      "epoch:14 step:11120 [D loss: 0.280374, acc.: 85.16%] [G loss: 2.722947]\n",
      "epoch:14 step:11121 [D loss: 0.338267, acc.: 84.38%] [G loss: 3.939985]\n",
      "epoch:14 step:11122 [D loss: 0.440825, acc.: 78.12%] [G loss: 3.211556]\n",
      "epoch:14 step:11123 [D loss: 0.254296, acc.: 92.19%] [G loss: 3.304309]\n",
      "epoch:14 step:11124 [D loss: 0.461971, acc.: 76.56%] [G loss: 4.205230]\n",
      "epoch:14 step:11125 [D loss: 0.502732, acc.: 77.34%] [G loss: 2.466063]\n",
      "epoch:14 step:11126 [D loss: 0.401224, acc.: 85.16%] [G loss: 3.428071]\n",
      "epoch:14 step:11127 [D loss: 0.359905, acc.: 84.38%] [G loss: 3.135094]\n",
      "epoch:14 step:11128 [D loss: 0.351079, acc.: 83.59%] [G loss: 2.968210]\n",
      "epoch:14 step:11129 [D loss: 0.388833, acc.: 84.38%] [G loss: 3.604289]\n",
      "epoch:14 step:11130 [D loss: 0.429956, acc.: 82.81%] [G loss: 2.557230]\n",
      "epoch:14 step:11131 [D loss: 0.443145, acc.: 82.03%] [G loss: 2.944668]\n",
      "epoch:14 step:11132 [D loss: 0.231653, acc.: 89.84%] [G loss: 3.354030]\n",
      "epoch:14 step:11133 [D loss: 0.291984, acc.: 85.94%] [G loss: 2.909586]\n",
      "epoch:14 step:11134 [D loss: 0.289861, acc.: 87.50%] [G loss: 3.500094]\n",
      "epoch:14 step:11135 [D loss: 0.246310, acc.: 89.84%] [G loss: 5.886200]\n",
      "epoch:14 step:11136 [D loss: 0.335002, acc.: 86.72%] [G loss: 6.041544]\n",
      "epoch:14 step:11137 [D loss: 0.284521, acc.: 88.28%] [G loss: 3.699529]\n",
      "epoch:14 step:11138 [D loss: 0.291783, acc.: 89.06%] [G loss: 4.575714]\n",
      "epoch:14 step:11139 [D loss: 0.356246, acc.: 82.81%] [G loss: 2.379186]\n",
      "epoch:14 step:11140 [D loss: 0.292547, acc.: 89.06%] [G loss: 4.011113]\n",
      "epoch:14 step:11141 [D loss: 0.361537, acc.: 82.81%] [G loss: 3.944594]\n",
      "epoch:14 step:11142 [D loss: 0.286111, acc.: 87.50%] [G loss: 3.627985]\n",
      "epoch:14 step:11143 [D loss: 0.411524, acc.: 84.38%] [G loss: 3.141600]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11144 [D loss: 0.339080, acc.: 84.38%] [G loss: 4.266168]\n",
      "epoch:14 step:11145 [D loss: 0.433286, acc.: 81.25%] [G loss: 3.816547]\n",
      "epoch:14 step:11146 [D loss: 0.424037, acc.: 81.25%] [G loss: 3.761763]\n",
      "epoch:14 step:11147 [D loss: 0.433195, acc.: 81.25%] [G loss: 3.496212]\n",
      "epoch:14 step:11148 [D loss: 0.290285, acc.: 87.50%] [G loss: 4.045742]\n",
      "epoch:14 step:11149 [D loss: 0.362332, acc.: 85.16%] [G loss: 2.914135]\n",
      "epoch:14 step:11150 [D loss: 0.330291, acc.: 85.16%] [G loss: 3.094097]\n",
      "epoch:14 step:11151 [D loss: 0.312635, acc.: 86.72%] [G loss: 2.950549]\n",
      "epoch:14 step:11152 [D loss: 0.308639, acc.: 85.94%] [G loss: 3.115079]\n",
      "epoch:14 step:11153 [D loss: 0.290241, acc.: 88.28%] [G loss: 2.763854]\n",
      "epoch:14 step:11154 [D loss: 0.358613, acc.: 85.16%] [G loss: 2.483619]\n",
      "epoch:14 step:11155 [D loss: 0.266670, acc.: 89.06%] [G loss: 3.010316]\n",
      "epoch:14 step:11156 [D loss: 0.423589, acc.: 80.47%] [G loss: 2.361303]\n",
      "epoch:14 step:11157 [D loss: 0.304136, acc.: 89.06%] [G loss: 2.766446]\n",
      "epoch:14 step:11158 [D loss: 0.344631, acc.: 85.94%] [G loss: 2.634849]\n",
      "epoch:14 step:11159 [D loss: 0.398266, acc.: 80.47%] [G loss: 2.825430]\n",
      "epoch:14 step:11160 [D loss: 0.263697, acc.: 92.19%] [G loss: 2.483353]\n",
      "epoch:14 step:11161 [D loss: 0.341067, acc.: 88.28%] [G loss: 2.473647]\n",
      "epoch:14 step:11162 [D loss: 0.398271, acc.: 82.03%] [G loss: 2.904846]\n",
      "epoch:14 step:11163 [D loss: 0.419678, acc.: 82.03%] [G loss: 4.555150]\n",
      "epoch:14 step:11164 [D loss: 0.316521, acc.: 85.16%] [G loss: 3.157483]\n",
      "epoch:14 step:11165 [D loss: 0.224760, acc.: 91.41%] [G loss: 3.650811]\n",
      "epoch:14 step:11166 [D loss: 0.304837, acc.: 88.28%] [G loss: 2.690110]\n",
      "epoch:14 step:11167 [D loss: 0.243813, acc.: 90.62%] [G loss: 3.341980]\n",
      "epoch:14 step:11168 [D loss: 0.303741, acc.: 88.28%] [G loss: 2.886263]\n",
      "epoch:14 step:11169 [D loss: 0.319188, acc.: 85.94%] [G loss: 2.406852]\n",
      "epoch:14 step:11170 [D loss: 0.170345, acc.: 94.53%] [G loss: 3.763640]\n",
      "epoch:14 step:11171 [D loss: 0.334584, acc.: 84.38%] [G loss: 3.888596]\n",
      "epoch:14 step:11172 [D loss: 0.270387, acc.: 89.06%] [G loss: 3.149433]\n",
      "epoch:14 step:11173 [D loss: 0.277145, acc.: 89.84%] [G loss: 2.588476]\n",
      "epoch:14 step:11174 [D loss: 0.274352, acc.: 86.72%] [G loss: 3.540040]\n",
      "epoch:14 step:11175 [D loss: 0.231570, acc.: 90.62%] [G loss: 3.494430]\n",
      "epoch:14 step:11176 [D loss: 0.320547, acc.: 85.16%] [G loss: 4.388258]\n",
      "epoch:14 step:11177 [D loss: 0.291275, acc.: 89.06%] [G loss: 2.820531]\n",
      "epoch:14 step:11178 [D loss: 0.433052, acc.: 78.91%] [G loss: 3.592406]\n",
      "epoch:14 step:11179 [D loss: 0.246331, acc.: 91.41%] [G loss: 2.744424]\n",
      "epoch:14 step:11180 [D loss: 0.270798, acc.: 87.50%] [G loss: 2.802144]\n",
      "epoch:14 step:11181 [D loss: 0.324705, acc.: 86.72%] [G loss: 2.983554]\n",
      "epoch:14 step:11182 [D loss: 0.268477, acc.: 88.28%] [G loss: 2.977906]\n",
      "epoch:14 step:11183 [D loss: 0.349439, acc.: 84.38%] [G loss: 3.173241]\n",
      "epoch:14 step:11184 [D loss: 0.545404, acc.: 68.75%] [G loss: 3.475025]\n",
      "epoch:14 step:11185 [D loss: 0.378902, acc.: 85.16%] [G loss: 3.844031]\n",
      "epoch:14 step:11186 [D loss: 0.265863, acc.: 88.28%] [G loss: 2.903623]\n",
      "epoch:14 step:11187 [D loss: 0.376544, acc.: 85.16%] [G loss: 2.776514]\n",
      "epoch:14 step:11188 [D loss: 0.318470, acc.: 85.16%] [G loss: 2.623949]\n",
      "epoch:14 step:11189 [D loss: 0.306674, acc.: 86.72%] [G loss: 4.331022]\n",
      "epoch:14 step:11190 [D loss: 0.334771, acc.: 84.38%] [G loss: 5.830683]\n",
      "epoch:14 step:11191 [D loss: 0.312603, acc.: 83.59%] [G loss: 4.396807]\n",
      "epoch:14 step:11192 [D loss: 0.326417, acc.: 86.72%] [G loss: 2.755976]\n",
      "epoch:14 step:11193 [D loss: 0.394781, acc.: 82.03%] [G loss: 2.133779]\n",
      "epoch:14 step:11194 [D loss: 0.348768, acc.: 85.16%] [G loss: 2.425698]\n",
      "epoch:14 step:11195 [D loss: 0.377133, acc.: 79.69%] [G loss: 3.448983]\n",
      "epoch:14 step:11196 [D loss: 0.365111, acc.: 85.94%] [G loss: 4.653394]\n",
      "epoch:14 step:11197 [D loss: 0.180615, acc.: 92.97%] [G loss: 4.082076]\n",
      "epoch:14 step:11198 [D loss: 0.392303, acc.: 78.91%] [G loss: 2.131509]\n",
      "epoch:14 step:11199 [D loss: 0.332105, acc.: 85.94%] [G loss: 3.217702]\n",
      "epoch:14 step:11200 [D loss: 0.194625, acc.: 93.75%] [G loss: 4.677989]\n",
      "epoch:14 step:11201 [D loss: 0.381506, acc.: 82.81%] [G loss: 2.148047]\n",
      "epoch:14 step:11202 [D loss: 0.369351, acc.: 84.38%] [G loss: 2.577284]\n",
      "epoch:14 step:11203 [D loss: 0.431782, acc.: 78.12%] [G loss: 3.559902]\n",
      "epoch:14 step:11204 [D loss: 0.315814, acc.: 89.06%] [G loss: 5.293823]\n",
      "epoch:14 step:11205 [D loss: 0.385998, acc.: 83.59%] [G loss: 4.509931]\n",
      "epoch:14 step:11206 [D loss: 0.268254, acc.: 85.94%] [G loss: 3.719734]\n",
      "epoch:14 step:11207 [D loss: 0.291168, acc.: 84.38%] [G loss: 3.854435]\n",
      "epoch:14 step:11208 [D loss: 0.262166, acc.: 89.06%] [G loss: 4.176408]\n",
      "epoch:14 step:11209 [D loss: 0.334950, acc.: 84.38%] [G loss: 6.504476]\n",
      "epoch:14 step:11210 [D loss: 0.425167, acc.: 77.34%] [G loss: 3.126937]\n",
      "epoch:14 step:11211 [D loss: 0.408800, acc.: 82.81%] [G loss: 2.688275]\n",
      "epoch:14 step:11212 [D loss: 0.328780, acc.: 81.25%] [G loss: 3.332423]\n",
      "epoch:14 step:11213 [D loss: 0.320049, acc.: 86.72%] [G loss: 2.438550]\n",
      "epoch:14 step:11214 [D loss: 0.265302, acc.: 91.41%] [G loss: 3.616323]\n",
      "epoch:14 step:11215 [D loss: 0.406581, acc.: 82.03%] [G loss: 4.208855]\n",
      "epoch:14 step:11216 [D loss: 0.374300, acc.: 84.38%] [G loss: 5.305002]\n",
      "epoch:14 step:11217 [D loss: 0.242141, acc.: 91.41%] [G loss: 3.754391]\n",
      "epoch:14 step:11218 [D loss: 0.309677, acc.: 88.28%] [G loss: 3.567204]\n",
      "epoch:14 step:11219 [D loss: 0.290591, acc.: 88.28%] [G loss: 4.909572]\n",
      "epoch:14 step:11220 [D loss: 0.323742, acc.: 85.16%] [G loss: 3.442288]\n",
      "epoch:14 step:11221 [D loss: 0.334689, acc.: 86.72%] [G loss: 3.201917]\n",
      "epoch:14 step:11222 [D loss: 0.409460, acc.: 82.03%] [G loss: 2.300431]\n",
      "epoch:14 step:11223 [D loss: 0.431011, acc.: 84.38%] [G loss: 3.308658]\n",
      "epoch:14 step:11224 [D loss: 0.413131, acc.: 80.47%] [G loss: 3.024524]\n",
      "epoch:14 step:11225 [D loss: 0.385322, acc.: 83.59%] [G loss: 2.530047]\n",
      "epoch:14 step:11226 [D loss: 0.359181, acc.: 85.16%] [G loss: 2.965057]\n",
      "epoch:14 step:11227 [D loss: 0.285743, acc.: 86.72%] [G loss: 2.774111]\n",
      "epoch:14 step:11228 [D loss: 0.267231, acc.: 91.41%] [G loss: 2.300353]\n",
      "epoch:14 step:11229 [D loss: 0.411085, acc.: 82.03%] [G loss: 3.062605]\n",
      "epoch:14 step:11230 [D loss: 0.442732, acc.: 78.12%] [G loss: 3.446665]\n",
      "epoch:14 step:11231 [D loss: 0.461880, acc.: 78.12%] [G loss: 5.346905]\n",
      "epoch:14 step:11232 [D loss: 0.428697, acc.: 78.91%] [G loss: 5.840578]\n",
      "epoch:14 step:11233 [D loss: 0.375823, acc.: 84.38%] [G loss: 4.830523]\n",
      "epoch:14 step:11234 [D loss: 0.379362, acc.: 79.69%] [G loss: 3.690751]\n",
      "epoch:14 step:11235 [D loss: 0.322033, acc.: 83.59%] [G loss: 3.857747]\n",
      "epoch:14 step:11236 [D loss: 0.243036, acc.: 89.06%] [G loss: 5.054837]\n",
      "epoch:14 step:11237 [D loss: 0.335402, acc.: 82.81%] [G loss: 3.396080]\n",
      "epoch:14 step:11238 [D loss: 0.355112, acc.: 85.94%] [G loss: 2.148772]\n",
      "epoch:14 step:11239 [D loss: 0.372522, acc.: 85.16%] [G loss: 3.051023]\n",
      "epoch:14 step:11240 [D loss: 0.289998, acc.: 84.38%] [G loss: 3.295418]\n",
      "epoch:14 step:11241 [D loss: 0.313276, acc.: 86.72%] [G loss: 2.465240]\n",
      "epoch:14 step:11242 [D loss: 0.376904, acc.: 83.59%] [G loss: 2.453387]\n",
      "epoch:14 step:11243 [D loss: 0.384098, acc.: 81.25%] [G loss: 2.821821]\n",
      "epoch:14 step:11244 [D loss: 0.271799, acc.: 89.06%] [G loss: 3.043681]\n",
      "epoch:14 step:11245 [D loss: 0.361831, acc.: 82.81%] [G loss: 2.959083]\n",
      "epoch:14 step:11246 [D loss: 0.368982, acc.: 83.59%] [G loss: 2.884518]\n",
      "epoch:14 step:11247 [D loss: 0.319929, acc.: 86.72%] [G loss: 3.159474]\n",
      "epoch:14 step:11248 [D loss: 0.333377, acc.: 85.16%] [G loss: 2.847170]\n",
      "epoch:14 step:11249 [D loss: 0.368851, acc.: 82.03%] [G loss: 2.495200]\n",
      "epoch:14 step:11250 [D loss: 0.337060, acc.: 86.72%] [G loss: 3.772199]\n",
      "epoch:14 step:11251 [D loss: 0.301590, acc.: 88.28%] [G loss: 3.821714]\n",
      "epoch:14 step:11252 [D loss: 0.271265, acc.: 89.84%] [G loss: 5.438795]\n",
      "epoch:14 step:11253 [D loss: 0.231782, acc.: 89.84%] [G loss: 5.516484]\n",
      "epoch:14 step:11254 [D loss: 0.214392, acc.: 89.84%] [G loss: 5.548716]\n",
      "epoch:14 step:11255 [D loss: 0.324193, acc.: 86.72%] [G loss: 3.738640]\n",
      "epoch:14 step:11256 [D loss: 0.212702, acc.: 90.62%] [G loss: 4.606865]\n",
      "epoch:14 step:11257 [D loss: 0.242800, acc.: 89.84%] [G loss: 3.876700]\n",
      "epoch:14 step:11258 [D loss: 0.311287, acc.: 85.16%] [G loss: 4.134385]\n",
      "epoch:14 step:11259 [D loss: 0.350813, acc.: 85.94%] [G loss: 3.041743]\n",
      "epoch:14 step:11260 [D loss: 0.387179, acc.: 82.81%] [G loss: 2.443054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11261 [D loss: 0.334666, acc.: 85.94%] [G loss: 2.776280]\n",
      "epoch:14 step:11262 [D loss: 0.259776, acc.: 88.28%] [G loss: 3.056479]\n",
      "epoch:14 step:11263 [D loss: 0.405541, acc.: 82.03%] [G loss: 3.020758]\n",
      "epoch:14 step:11264 [D loss: 0.250614, acc.: 89.84%] [G loss: 3.742855]\n",
      "epoch:14 step:11265 [D loss: 0.255138, acc.: 89.84%] [G loss: 3.826486]\n",
      "epoch:14 step:11266 [D loss: 0.286091, acc.: 89.06%] [G loss: 3.288732]\n",
      "epoch:14 step:11267 [D loss: 0.314143, acc.: 85.16%] [G loss: 3.776103]\n",
      "epoch:14 step:11268 [D loss: 0.348281, acc.: 87.50%] [G loss: 2.953855]\n",
      "epoch:14 step:11269 [D loss: 0.311689, acc.: 89.06%] [G loss: 2.835434]\n",
      "epoch:14 step:11270 [D loss: 0.291576, acc.: 86.72%] [G loss: 3.918591]\n",
      "epoch:14 step:11271 [D loss: 0.192340, acc.: 95.31%] [G loss: 3.769897]\n",
      "epoch:14 step:11272 [D loss: 0.227570, acc.: 92.97%] [G loss: 3.300988]\n",
      "epoch:14 step:11273 [D loss: 0.264953, acc.: 86.72%] [G loss: 2.433118]\n",
      "epoch:14 step:11274 [D loss: 0.309339, acc.: 89.06%] [G loss: 3.633025]\n",
      "epoch:14 step:11275 [D loss: 0.342239, acc.: 86.72%] [G loss: 2.970061]\n",
      "epoch:14 step:11276 [D loss: 0.380715, acc.: 86.72%] [G loss: 3.205100]\n",
      "epoch:14 step:11277 [D loss: 0.348941, acc.: 82.81%] [G loss: 3.693504]\n",
      "epoch:14 step:11278 [D loss: 0.315481, acc.: 83.59%] [G loss: 4.098488]\n",
      "epoch:14 step:11279 [D loss: 0.313344, acc.: 86.72%] [G loss: 1.881678]\n",
      "epoch:14 step:11280 [D loss: 0.274969, acc.: 87.50%] [G loss: 2.908093]\n",
      "epoch:14 step:11281 [D loss: 0.366381, acc.: 85.16%] [G loss: 3.554330]\n",
      "epoch:14 step:11282 [D loss: 0.371447, acc.: 82.03%] [G loss: 5.911919]\n",
      "epoch:14 step:11283 [D loss: 0.742116, acc.: 71.09%] [G loss: 4.181905]\n",
      "epoch:14 step:11284 [D loss: 0.725469, acc.: 75.78%] [G loss: 4.899154]\n",
      "epoch:14 step:11285 [D loss: 0.281533, acc.: 85.94%] [G loss: 3.129830]\n",
      "epoch:14 step:11286 [D loss: 0.427292, acc.: 82.03%] [G loss: 4.686935]\n",
      "epoch:14 step:11287 [D loss: 0.290870, acc.: 84.38%] [G loss: 3.612002]\n",
      "epoch:14 step:11288 [D loss: 0.310284, acc.: 84.38%] [G loss: 2.709327]\n",
      "epoch:14 step:11289 [D loss: 0.300015, acc.: 90.62%] [G loss: 2.688298]\n",
      "epoch:14 step:11290 [D loss: 0.421936, acc.: 82.81%] [G loss: 2.512446]\n",
      "epoch:14 step:11291 [D loss: 0.314198, acc.: 85.16%] [G loss: 3.720675]\n",
      "epoch:14 step:11292 [D loss: 0.386546, acc.: 85.94%] [G loss: 2.348407]\n",
      "epoch:14 step:11293 [D loss: 0.262763, acc.: 89.06%] [G loss: 2.891862]\n",
      "epoch:14 step:11294 [D loss: 0.325770, acc.: 88.28%] [G loss: 2.322334]\n",
      "epoch:14 step:11295 [D loss: 0.303215, acc.: 89.06%] [G loss: 2.726261]\n",
      "epoch:14 step:11296 [D loss: 0.406463, acc.: 85.94%] [G loss: 2.075970]\n",
      "epoch:14 step:11297 [D loss: 0.295148, acc.: 85.94%] [G loss: 2.975816]\n",
      "epoch:14 step:11298 [D loss: 0.332076, acc.: 85.94%] [G loss: 2.556285]\n",
      "epoch:14 step:11299 [D loss: 0.288089, acc.: 86.72%] [G loss: 2.735996]\n",
      "epoch:14 step:11300 [D loss: 0.343808, acc.: 88.28%] [G loss: 3.076280]\n",
      "epoch:14 step:11301 [D loss: 0.328938, acc.: 85.94%] [G loss: 3.285927]\n",
      "epoch:14 step:11302 [D loss: 0.309119, acc.: 85.94%] [G loss: 2.566274]\n",
      "epoch:14 step:11303 [D loss: 0.240342, acc.: 90.62%] [G loss: 2.845396]\n",
      "epoch:14 step:11304 [D loss: 0.225215, acc.: 91.41%] [G loss: 4.892969]\n",
      "epoch:14 step:11305 [D loss: 0.259569, acc.: 88.28%] [G loss: 3.163348]\n",
      "epoch:14 step:11306 [D loss: 0.309679, acc.: 86.72%] [G loss: 3.115227]\n",
      "epoch:14 step:11307 [D loss: 0.285375, acc.: 89.06%] [G loss: 3.106264]\n",
      "epoch:14 step:11308 [D loss: 0.290838, acc.: 87.50%] [G loss: 4.129785]\n",
      "epoch:14 step:11309 [D loss: 0.367815, acc.: 87.50%] [G loss: 4.634148]\n",
      "epoch:14 step:11310 [D loss: 0.418438, acc.: 84.38%] [G loss: 2.860271]\n",
      "epoch:14 step:11311 [D loss: 0.298669, acc.: 85.94%] [G loss: 2.893303]\n",
      "epoch:14 step:11312 [D loss: 0.346996, acc.: 85.16%] [G loss: 3.197583]\n",
      "epoch:14 step:11313 [D loss: 0.232928, acc.: 92.97%] [G loss: 5.112398]\n",
      "epoch:14 step:11314 [D loss: 0.214905, acc.: 89.06%] [G loss: 4.554560]\n",
      "epoch:14 step:11315 [D loss: 0.209714, acc.: 94.53%] [G loss: 4.814301]\n",
      "epoch:14 step:11316 [D loss: 0.181573, acc.: 94.53%] [G loss: 3.595831]\n",
      "epoch:14 step:11317 [D loss: 0.379748, acc.: 83.59%] [G loss: 3.474153]\n",
      "epoch:14 step:11318 [D loss: 0.310314, acc.: 89.84%] [G loss: 3.341220]\n",
      "epoch:14 step:11319 [D loss: 0.352938, acc.: 84.38%] [G loss: 3.023844]\n",
      "epoch:14 step:11320 [D loss: 0.284680, acc.: 85.94%] [G loss: 3.002560]\n",
      "epoch:14 step:11321 [D loss: 0.403135, acc.: 82.03%] [G loss: 4.252397]\n",
      "epoch:14 step:11322 [D loss: 0.376554, acc.: 82.03%] [G loss: 4.235933]\n",
      "epoch:14 step:11323 [D loss: 0.323526, acc.: 83.59%] [G loss: 2.680552]\n",
      "epoch:14 step:11324 [D loss: 0.264720, acc.: 85.16%] [G loss: 4.696445]\n",
      "epoch:14 step:11325 [D loss: 0.322267, acc.: 85.94%] [G loss: 3.559448]\n",
      "epoch:14 step:11326 [D loss: 0.409925, acc.: 85.16%] [G loss: 3.816172]\n",
      "epoch:14 step:11327 [D loss: 0.387000, acc.: 81.25%] [G loss: 2.866434]\n",
      "epoch:14 step:11328 [D loss: 0.318436, acc.: 86.72%] [G loss: 2.742244]\n",
      "epoch:14 step:11329 [D loss: 0.380746, acc.: 78.91%] [G loss: 2.704132]\n",
      "epoch:14 step:11330 [D loss: 0.339963, acc.: 85.16%] [G loss: 2.666852]\n",
      "epoch:14 step:11331 [D loss: 0.313072, acc.: 82.03%] [G loss: 2.913992]\n",
      "epoch:14 step:11332 [D loss: 0.406696, acc.: 80.47%] [G loss: 2.588614]\n",
      "epoch:14 step:11333 [D loss: 0.399081, acc.: 83.59%] [G loss: 2.867256]\n",
      "epoch:14 step:11334 [D loss: 0.413805, acc.: 79.69%] [G loss: 3.557382]\n",
      "epoch:14 step:11335 [D loss: 0.258182, acc.: 90.62%] [G loss: 3.756573]\n",
      "epoch:14 step:11336 [D loss: 0.271008, acc.: 92.19%] [G loss: 3.925528]\n",
      "epoch:14 step:11337 [D loss: 0.291458, acc.: 85.94%] [G loss: 2.823839]\n",
      "epoch:14 step:11338 [D loss: 0.319012, acc.: 85.94%] [G loss: 3.453422]\n",
      "epoch:14 step:11339 [D loss: 0.300802, acc.: 83.59%] [G loss: 4.167318]\n",
      "epoch:14 step:11340 [D loss: 0.262197, acc.: 87.50%] [G loss: 4.245563]\n",
      "epoch:14 step:11341 [D loss: 0.349299, acc.: 81.25%] [G loss: 4.113679]\n",
      "epoch:14 step:11342 [D loss: 0.235255, acc.: 86.72%] [G loss: 4.563563]\n",
      "epoch:14 step:11343 [D loss: 0.360206, acc.: 84.38%] [G loss: 2.666254]\n",
      "epoch:14 step:11344 [D loss: 0.315248, acc.: 85.94%] [G loss: 3.259979]\n",
      "epoch:14 step:11345 [D loss: 0.318835, acc.: 88.28%] [G loss: 2.560114]\n",
      "epoch:14 step:11346 [D loss: 0.320846, acc.: 86.72%] [G loss: 2.684472]\n",
      "epoch:14 step:11347 [D loss: 0.292828, acc.: 89.06%] [G loss: 2.561891]\n",
      "epoch:14 step:11348 [D loss: 0.408064, acc.: 79.69%] [G loss: 3.035240]\n",
      "epoch:14 step:11349 [D loss: 0.300037, acc.: 85.94%] [G loss: 2.587246]\n",
      "epoch:14 step:11350 [D loss: 0.295824, acc.: 86.72%] [G loss: 3.786564]\n",
      "epoch:14 step:11351 [D loss: 0.266809, acc.: 86.72%] [G loss: 4.770655]\n",
      "epoch:14 step:11352 [D loss: 0.264808, acc.: 90.62%] [G loss: 4.958023]\n",
      "epoch:14 step:11353 [D loss: 0.334178, acc.: 85.16%] [G loss: 2.866594]\n",
      "epoch:14 step:11354 [D loss: 0.323985, acc.: 87.50%] [G loss: 3.316305]\n",
      "epoch:14 step:11355 [D loss: 0.326940, acc.: 84.38%] [G loss: 3.474965]\n",
      "epoch:14 step:11356 [D loss: 0.271454, acc.: 85.94%] [G loss: 3.189745]\n",
      "epoch:14 step:11357 [D loss: 0.268732, acc.: 89.84%] [G loss: 2.620962]\n",
      "epoch:14 step:11358 [D loss: 0.279812, acc.: 89.84%] [G loss: 2.886991]\n",
      "epoch:14 step:11359 [D loss: 0.379707, acc.: 82.03%] [G loss: 2.584911]\n",
      "epoch:14 step:11360 [D loss: 0.320049, acc.: 85.94%] [G loss: 3.590723]\n",
      "epoch:14 step:11361 [D loss: 0.249785, acc.: 91.41%] [G loss: 3.940107]\n",
      "epoch:14 step:11362 [D loss: 0.404339, acc.: 81.25%] [G loss: 3.584423]\n",
      "epoch:14 step:11363 [D loss: 0.407715, acc.: 82.81%] [G loss: 3.600660]\n",
      "epoch:14 step:11364 [D loss: 0.397432, acc.: 78.12%] [G loss: 3.820489]\n",
      "epoch:14 step:11365 [D loss: 0.351923, acc.: 84.38%] [G loss: 2.172000]\n",
      "epoch:14 step:11366 [D loss: 0.369371, acc.: 84.38%] [G loss: 2.666447]\n",
      "epoch:14 step:11367 [D loss: 0.228059, acc.: 92.19%] [G loss: 2.703272]\n",
      "epoch:14 step:11368 [D loss: 0.368996, acc.: 83.59%] [G loss: 3.949079]\n",
      "epoch:14 step:11369 [D loss: 0.339825, acc.: 86.72%] [G loss: 5.886973]\n",
      "epoch:14 step:11370 [D loss: 0.667750, acc.: 75.78%] [G loss: 5.044346]\n",
      "epoch:14 step:11371 [D loss: 1.587573, acc.: 63.28%] [G loss: 4.367421]\n",
      "epoch:14 step:11372 [D loss: 0.337153, acc.: 81.25%] [G loss: 6.490133]\n",
      "epoch:14 step:11373 [D loss: 0.552756, acc.: 79.69%] [G loss: 4.676377]\n",
      "epoch:14 step:11374 [D loss: 0.488555, acc.: 78.12%] [G loss: 3.267206]\n",
      "epoch:14 step:11375 [D loss: 0.477065, acc.: 82.03%] [G loss: 4.318284]\n",
      "epoch:14 step:11376 [D loss: 0.371381, acc.: 84.38%] [G loss: 2.455920]\n",
      "epoch:14 step:11377 [D loss: 0.362267, acc.: 82.03%] [G loss: 2.690041]\n",
      "epoch:14 step:11378 [D loss: 0.331764, acc.: 86.72%] [G loss: 2.901088]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11379 [D loss: 0.305302, acc.: 84.38%] [G loss: 2.958837]\n",
      "epoch:14 step:11380 [D loss: 0.309447, acc.: 89.06%] [G loss: 2.820302]\n",
      "epoch:14 step:11381 [D loss: 0.512420, acc.: 75.00%] [G loss: 3.833548]\n",
      "epoch:14 step:11382 [D loss: 0.366526, acc.: 85.94%] [G loss: 2.236503]\n",
      "epoch:14 step:11383 [D loss: 0.445505, acc.: 74.22%] [G loss: 4.198429]\n",
      "epoch:14 step:11384 [D loss: 0.356293, acc.: 82.81%] [G loss: 1.979690]\n",
      "epoch:14 step:11385 [D loss: 0.316946, acc.: 86.72%] [G loss: 3.484718]\n",
      "epoch:14 step:11386 [D loss: 0.330867, acc.: 85.94%] [G loss: 3.209920]\n",
      "epoch:14 step:11387 [D loss: 0.250220, acc.: 88.28%] [G loss: 3.882637]\n",
      "epoch:14 step:11388 [D loss: 0.265118, acc.: 89.06%] [G loss: 3.359848]\n",
      "epoch:14 step:11389 [D loss: 0.387073, acc.: 80.47%] [G loss: 3.174448]\n",
      "epoch:14 step:11390 [D loss: 0.247828, acc.: 89.84%] [G loss: 5.896291]\n",
      "epoch:14 step:11391 [D loss: 0.283697, acc.: 89.84%] [G loss: 3.259418]\n",
      "epoch:14 step:11392 [D loss: 0.223995, acc.: 93.75%] [G loss: 3.142385]\n",
      "epoch:14 step:11393 [D loss: 0.291984, acc.: 89.84%] [G loss: 3.651825]\n",
      "epoch:14 step:11394 [D loss: 0.348350, acc.: 84.38%] [G loss: 2.595201]\n",
      "epoch:14 step:11395 [D loss: 0.289936, acc.: 88.28%] [G loss: 2.770968]\n",
      "epoch:14 step:11396 [D loss: 0.291225, acc.: 87.50%] [G loss: 4.462080]\n",
      "epoch:14 step:11397 [D loss: 0.364685, acc.: 81.25%] [G loss: 2.233565]\n",
      "epoch:14 step:11398 [D loss: 0.274475, acc.: 89.84%] [G loss: 3.191268]\n",
      "epoch:14 step:11399 [D loss: 0.303161, acc.: 87.50%] [G loss: 3.669588]\n",
      "epoch:14 step:11400 [D loss: 0.260185, acc.: 89.84%] [G loss: 3.341470]\n",
      "epoch:14 step:11401 [D loss: 0.350419, acc.: 82.81%] [G loss: 3.423287]\n",
      "epoch:14 step:11402 [D loss: 0.471993, acc.: 81.25%] [G loss: 2.569109]\n",
      "epoch:14 step:11403 [D loss: 0.257423, acc.: 88.28%] [G loss: 4.338479]\n",
      "epoch:14 step:11404 [D loss: 0.340633, acc.: 86.72%] [G loss: 3.303800]\n",
      "epoch:14 step:11405 [D loss: 0.316900, acc.: 88.28%] [G loss: 3.031990]\n",
      "epoch:14 step:11406 [D loss: 0.328361, acc.: 82.03%] [G loss: 2.927059]\n",
      "epoch:14 step:11407 [D loss: 0.444312, acc.: 82.81%] [G loss: 2.925848]\n",
      "epoch:14 step:11408 [D loss: 0.400190, acc.: 79.69%] [G loss: 2.567595]\n",
      "epoch:14 step:11409 [D loss: 0.284237, acc.: 89.06%] [G loss: 2.597311]\n",
      "epoch:14 step:11410 [D loss: 0.336687, acc.: 83.59%] [G loss: 2.454350]\n",
      "epoch:14 step:11411 [D loss: 0.299548, acc.: 89.06%] [G loss: 2.904490]\n",
      "epoch:14 step:11412 [D loss: 0.337829, acc.: 84.38%] [G loss: 2.598609]\n",
      "epoch:14 step:11413 [D loss: 0.295641, acc.: 86.72%] [G loss: 2.196441]\n",
      "epoch:14 step:11414 [D loss: 0.279018, acc.: 87.50%] [G loss: 3.400633]\n",
      "epoch:14 step:11415 [D loss: 0.240012, acc.: 89.06%] [G loss: 3.000285]\n",
      "epoch:14 step:11416 [D loss: 0.369929, acc.: 82.81%] [G loss: 3.216413]\n",
      "epoch:14 step:11417 [D loss: 0.343202, acc.: 82.81%] [G loss: 3.777525]\n",
      "epoch:14 step:11418 [D loss: 0.326851, acc.: 85.16%] [G loss: 4.692495]\n",
      "epoch:14 step:11419 [D loss: 0.332139, acc.: 85.16%] [G loss: 3.309090]\n",
      "epoch:14 step:11420 [D loss: 0.274479, acc.: 88.28%] [G loss: 3.408494]\n",
      "epoch:14 step:11421 [D loss: 0.326513, acc.: 87.50%] [G loss: 4.809436]\n",
      "epoch:14 step:11422 [D loss: 0.293408, acc.: 87.50%] [G loss: 4.570716]\n",
      "epoch:14 step:11423 [D loss: 0.283605, acc.: 90.62%] [G loss: 4.351830]\n",
      "epoch:14 step:11424 [D loss: 0.282351, acc.: 87.50%] [G loss: 3.045215]\n",
      "epoch:14 step:11425 [D loss: 0.330630, acc.: 85.94%] [G loss: 2.109394]\n",
      "epoch:14 step:11426 [D loss: 0.475114, acc.: 79.69%] [G loss: 3.091176]\n",
      "epoch:14 step:11427 [D loss: 0.239297, acc.: 88.28%] [G loss: 4.185604]\n",
      "epoch:14 step:11428 [D loss: 0.277485, acc.: 88.28%] [G loss: 3.050765]\n",
      "epoch:14 step:11429 [D loss: 0.406566, acc.: 83.59%] [G loss: 2.481956]\n",
      "epoch:14 step:11430 [D loss: 0.371452, acc.: 86.72%] [G loss: 4.470381]\n",
      "epoch:14 step:11431 [D loss: 0.327228, acc.: 84.38%] [G loss: 2.799263]\n",
      "epoch:14 step:11432 [D loss: 0.241991, acc.: 89.06%] [G loss: 4.463494]\n",
      "epoch:14 step:11433 [D loss: 0.537045, acc.: 77.34%] [G loss: 2.876713]\n",
      "epoch:14 step:11434 [D loss: 0.348462, acc.: 88.28%] [G loss: 3.562937]\n",
      "epoch:14 step:11435 [D loss: 0.339944, acc.: 80.47%] [G loss: 4.385281]\n",
      "epoch:14 step:11436 [D loss: 0.412283, acc.: 84.38%] [G loss: 2.961924]\n",
      "epoch:14 step:11437 [D loss: 0.444902, acc.: 83.59%] [G loss: 2.622022]\n",
      "epoch:14 step:11438 [D loss: 0.280454, acc.: 87.50%] [G loss: 3.491728]\n",
      "epoch:14 step:11439 [D loss: 0.304803, acc.: 89.06%] [G loss: 3.032763]\n",
      "epoch:14 step:11440 [D loss: 0.345157, acc.: 87.50%] [G loss: 2.218325]\n",
      "epoch:14 step:11441 [D loss: 0.311272, acc.: 84.38%] [G loss: 2.988879]\n",
      "epoch:14 step:11442 [D loss: 0.473548, acc.: 79.69%] [G loss: 3.483814]\n",
      "epoch:14 step:11443 [D loss: 0.412794, acc.: 82.03%] [G loss: 4.041519]\n",
      "epoch:14 step:11444 [D loss: 0.637184, acc.: 70.31%] [G loss: 3.166695]\n",
      "epoch:14 step:11445 [D loss: 0.506794, acc.: 79.69%] [G loss: 3.193167]\n",
      "epoch:14 step:11446 [D loss: 0.529555, acc.: 75.78%] [G loss: 2.871976]\n",
      "epoch:14 step:11447 [D loss: 0.411995, acc.: 81.25%] [G loss: 3.695391]\n",
      "epoch:14 step:11448 [D loss: 0.399299, acc.: 78.91%] [G loss: 6.754406]\n",
      "epoch:14 step:11449 [D loss: 0.820318, acc.: 71.88%] [G loss: 5.189812]\n",
      "epoch:14 step:11450 [D loss: 0.526025, acc.: 77.34%] [G loss: 5.810916]\n",
      "epoch:14 step:11451 [D loss: 0.399563, acc.: 85.94%] [G loss: 7.102745]\n",
      "epoch:14 step:11452 [D loss: 0.699661, acc.: 77.34%] [G loss: 5.125196]\n",
      "epoch:14 step:11453 [D loss: 0.475921, acc.: 82.03%] [G loss: 3.847366]\n",
      "epoch:14 step:11454 [D loss: 0.369000, acc.: 86.72%] [G loss: 3.184913]\n",
      "epoch:14 step:11455 [D loss: 0.630125, acc.: 81.25%] [G loss: 2.046404]\n",
      "epoch:14 step:11456 [D loss: 0.308964, acc.: 87.50%] [G loss: 3.459711]\n",
      "epoch:14 step:11457 [D loss: 0.237474, acc.: 89.06%] [G loss: 3.179231]\n",
      "epoch:14 step:11458 [D loss: 0.435477, acc.: 80.47%] [G loss: 3.650951]\n",
      "epoch:14 step:11459 [D loss: 0.301678, acc.: 86.72%] [G loss: 4.111339]\n",
      "epoch:14 step:11460 [D loss: 0.405838, acc.: 83.59%] [G loss: 2.792541]\n",
      "epoch:14 step:11461 [D loss: 0.444803, acc.: 78.12%] [G loss: 2.323056]\n",
      "epoch:14 step:11462 [D loss: 0.309540, acc.: 85.16%] [G loss: 3.439868]\n",
      "epoch:14 step:11463 [D loss: 0.402629, acc.: 82.03%] [G loss: 3.665111]\n",
      "epoch:14 step:11464 [D loss: 0.490578, acc.: 79.69%] [G loss: 3.085532]\n",
      "epoch:14 step:11465 [D loss: 0.295959, acc.: 88.28%] [G loss: 2.901484]\n",
      "epoch:14 step:11466 [D loss: 0.343562, acc.: 85.94%] [G loss: 4.581612]\n",
      "epoch:14 step:11467 [D loss: 0.200564, acc.: 93.75%] [G loss: 3.665213]\n",
      "epoch:14 step:11468 [D loss: 0.411240, acc.: 84.38%] [G loss: 3.198072]\n",
      "epoch:14 step:11469 [D loss: 0.289722, acc.: 85.94%] [G loss: 4.499214]\n",
      "epoch:14 step:11470 [D loss: 0.415420, acc.: 84.38%] [G loss: 2.619127]\n",
      "epoch:14 step:11471 [D loss: 0.290042, acc.: 84.38%] [G loss: 3.707815]\n",
      "epoch:14 step:11472 [D loss: 0.355346, acc.: 83.59%] [G loss: 3.146523]\n",
      "epoch:14 step:11473 [D loss: 0.256296, acc.: 93.75%] [G loss: 3.103765]\n",
      "epoch:14 step:11474 [D loss: 0.410421, acc.: 83.59%] [G loss: 3.051250]\n",
      "epoch:14 step:11475 [D loss: 0.359038, acc.: 85.16%] [G loss: 3.779005]\n",
      "epoch:14 step:11476 [D loss: 0.337505, acc.: 86.72%] [G loss: 5.471056]\n",
      "epoch:14 step:11477 [D loss: 0.281499, acc.: 85.16%] [G loss: 3.211314]\n",
      "epoch:14 step:11478 [D loss: 0.388160, acc.: 83.59%] [G loss: 2.971869]\n",
      "epoch:14 step:11479 [D loss: 0.268883, acc.: 89.06%] [G loss: 2.642878]\n",
      "epoch:14 step:11480 [D loss: 0.264116, acc.: 89.84%] [G loss: 2.842985]\n",
      "epoch:14 step:11481 [D loss: 0.539988, acc.: 77.34%] [G loss: 2.781644]\n",
      "epoch:14 step:11482 [D loss: 0.332425, acc.: 85.94%] [G loss: 2.942468]\n",
      "epoch:14 step:11483 [D loss: 0.405378, acc.: 82.03%] [G loss: 3.936061]\n",
      "epoch:14 step:11484 [D loss: 0.263405, acc.: 90.62%] [G loss: 5.463453]\n",
      "epoch:14 step:11485 [D loss: 0.234569, acc.: 90.62%] [G loss: 2.728426]\n",
      "epoch:14 step:11486 [D loss: 0.435172, acc.: 81.25%] [G loss: 4.668985]\n",
      "epoch:14 step:11487 [D loss: 0.450148, acc.: 79.69%] [G loss: 4.124475]\n",
      "epoch:14 step:11488 [D loss: 0.320275, acc.: 85.94%] [G loss: 4.434710]\n",
      "epoch:14 step:11489 [D loss: 0.383019, acc.: 82.03%] [G loss: 2.987886]\n",
      "epoch:14 step:11490 [D loss: 0.328006, acc.: 84.38%] [G loss: 3.659044]\n",
      "epoch:14 step:11491 [D loss: 0.325848, acc.: 88.28%] [G loss: 2.758272]\n",
      "epoch:14 step:11492 [D loss: 0.243139, acc.: 90.62%] [G loss: 2.850835]\n",
      "epoch:14 step:11493 [D loss: 0.287066, acc.: 88.28%] [G loss: 2.662397]\n",
      "epoch:14 step:11494 [D loss: 0.282843, acc.: 85.16%] [G loss: 2.337118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11495 [D loss: 0.280190, acc.: 89.06%] [G loss: 2.616547]\n",
      "epoch:14 step:11496 [D loss: 0.337160, acc.: 84.38%] [G loss: 3.258132]\n",
      "epoch:14 step:11497 [D loss: 0.348447, acc.: 82.81%] [G loss: 3.137687]\n",
      "epoch:14 step:11498 [D loss: 0.262616, acc.: 89.06%] [G loss: 2.959846]\n",
      "epoch:14 step:11499 [D loss: 0.375509, acc.: 80.47%] [G loss: 2.694612]\n",
      "epoch:14 step:11500 [D loss: 0.278873, acc.: 88.28%] [G loss: 3.645478]\n",
      "epoch:14 step:11501 [D loss: 0.198882, acc.: 92.97%] [G loss: 3.708249]\n",
      "epoch:14 step:11502 [D loss: 0.281701, acc.: 89.84%] [G loss: 4.032875]\n",
      "epoch:14 step:11503 [D loss: 0.355744, acc.: 80.47%] [G loss: 3.290363]\n",
      "epoch:14 step:11504 [D loss: 0.312466, acc.: 86.72%] [G loss: 2.477754]\n",
      "epoch:14 step:11505 [D loss: 0.308184, acc.: 86.72%] [G loss: 2.212841]\n",
      "epoch:14 step:11506 [D loss: 0.340009, acc.: 84.38%] [G loss: 2.995308]\n",
      "epoch:14 step:11507 [D loss: 0.233626, acc.: 92.19%] [G loss: 2.725925]\n",
      "epoch:14 step:11508 [D loss: 0.334316, acc.: 84.38%] [G loss: 2.731013]\n",
      "epoch:14 step:11509 [D loss: 0.373590, acc.: 83.59%] [G loss: 2.288087]\n",
      "epoch:14 step:11510 [D loss: 0.293090, acc.: 89.84%] [G loss: 2.587999]\n",
      "epoch:14 step:11511 [D loss: 0.283769, acc.: 87.50%] [G loss: 3.289108]\n",
      "epoch:14 step:11512 [D loss: 0.330521, acc.: 82.81%] [G loss: 4.204407]\n",
      "epoch:14 step:11513 [D loss: 0.293349, acc.: 87.50%] [G loss: 3.251433]\n",
      "epoch:14 step:11514 [D loss: 0.335140, acc.: 90.62%] [G loss: 2.819623]\n",
      "epoch:14 step:11515 [D loss: 0.392230, acc.: 80.47%] [G loss: 2.575421]\n",
      "epoch:14 step:11516 [D loss: 0.277038, acc.: 90.62%] [G loss: 2.984838]\n",
      "epoch:14 step:11517 [D loss: 0.377613, acc.: 85.94%] [G loss: 3.303841]\n",
      "epoch:14 step:11518 [D loss: 0.355988, acc.: 87.50%] [G loss: 2.500851]\n",
      "epoch:14 step:11519 [D loss: 0.395804, acc.: 79.69%] [G loss: 2.662418]\n",
      "epoch:14 step:11520 [D loss: 0.342091, acc.: 85.94%] [G loss: 3.655682]\n",
      "epoch:14 step:11521 [D loss: 0.310000, acc.: 82.81%] [G loss: 3.949123]\n",
      "epoch:14 step:11522 [D loss: 0.222581, acc.: 92.97%] [G loss: 3.404235]\n",
      "epoch:14 step:11523 [D loss: 0.310113, acc.: 85.16%] [G loss: 3.193270]\n",
      "epoch:14 step:11524 [D loss: 0.420667, acc.: 78.12%] [G loss: 2.825130]\n",
      "epoch:14 step:11525 [D loss: 0.308425, acc.: 85.16%] [G loss: 3.291047]\n",
      "epoch:14 step:11526 [D loss: 0.383224, acc.: 85.16%] [G loss: 2.388783]\n",
      "epoch:14 step:11527 [D loss: 0.265078, acc.: 89.84%] [G loss: 2.557222]\n",
      "epoch:14 step:11528 [D loss: 0.309115, acc.: 88.28%] [G loss: 2.979899]\n",
      "epoch:14 step:11529 [D loss: 0.337797, acc.: 85.94%] [G loss: 2.406520]\n",
      "epoch:14 step:11530 [D loss: 0.240368, acc.: 94.53%] [G loss: 3.030832]\n",
      "epoch:14 step:11531 [D loss: 0.297163, acc.: 85.16%] [G loss: 2.384203]\n",
      "epoch:14 step:11532 [D loss: 0.306592, acc.: 89.06%] [G loss: 2.714226]\n",
      "epoch:14 step:11533 [D loss: 0.414490, acc.: 80.47%] [G loss: 2.343211]\n",
      "epoch:14 step:11534 [D loss: 0.317899, acc.: 89.06%] [G loss: 3.324593]\n",
      "epoch:14 step:11535 [D loss: 0.302864, acc.: 83.59%] [G loss: 4.505505]\n",
      "epoch:14 step:11536 [D loss: 0.390309, acc.: 78.91%] [G loss: 2.702280]\n",
      "epoch:14 step:11537 [D loss: 0.215200, acc.: 94.53%] [G loss: 2.812538]\n",
      "epoch:14 step:11538 [D loss: 0.269862, acc.: 89.84%] [G loss: 4.533852]\n",
      "epoch:14 step:11539 [D loss: 0.262084, acc.: 89.06%] [G loss: 5.504783]\n",
      "epoch:14 step:11540 [D loss: 0.450996, acc.: 76.56%] [G loss: 2.305039]\n",
      "epoch:14 step:11541 [D loss: 0.265683, acc.: 89.06%] [G loss: 3.295828]\n",
      "epoch:14 step:11542 [D loss: 0.235440, acc.: 89.06%] [G loss: 3.236788]\n",
      "epoch:14 step:11543 [D loss: 0.303769, acc.: 88.28%] [G loss: 2.867699]\n",
      "epoch:14 step:11544 [D loss: 0.298518, acc.: 88.28%] [G loss: 5.564829]\n",
      "epoch:14 step:11545 [D loss: 0.268861, acc.: 89.06%] [G loss: 3.502334]\n",
      "epoch:14 step:11546 [D loss: 0.292091, acc.: 85.94%] [G loss: 2.937753]\n",
      "epoch:14 step:11547 [D loss: 0.385470, acc.: 79.69%] [G loss: 4.301199]\n",
      "epoch:14 step:11548 [D loss: 0.238657, acc.: 87.50%] [G loss: 4.041255]\n",
      "epoch:14 step:11549 [D loss: 0.337174, acc.: 86.72%] [G loss: 2.984809]\n",
      "epoch:14 step:11550 [D loss: 0.297317, acc.: 89.06%] [G loss: 3.302740]\n",
      "epoch:14 step:11551 [D loss: 0.264702, acc.: 89.06%] [G loss: 4.068246]\n",
      "epoch:14 step:11552 [D loss: 0.413325, acc.: 75.78%] [G loss: 2.999794]\n",
      "epoch:14 step:11553 [D loss: 0.351779, acc.: 83.59%] [G loss: 3.879751]\n",
      "epoch:14 step:11554 [D loss: 0.231508, acc.: 91.41%] [G loss: 3.561719]\n",
      "epoch:14 step:11555 [D loss: 0.237969, acc.: 89.06%] [G loss: 2.867892]\n",
      "epoch:14 step:11556 [D loss: 0.375577, acc.: 81.25%] [G loss: 2.906350]\n",
      "epoch:14 step:11557 [D loss: 0.392114, acc.: 82.81%] [G loss: 3.075223]\n",
      "epoch:14 step:11558 [D loss: 0.412033, acc.: 82.03%] [G loss: 2.631735]\n",
      "epoch:14 step:11559 [D loss: 0.323571, acc.: 82.81%] [G loss: 3.413976]\n",
      "epoch:14 step:11560 [D loss: 0.392686, acc.: 81.25%] [G loss: 3.082570]\n",
      "epoch:14 step:11561 [D loss: 0.395038, acc.: 79.69%] [G loss: 2.727137]\n",
      "epoch:14 step:11562 [D loss: 0.428598, acc.: 75.00%] [G loss: 2.379987]\n",
      "epoch:14 step:11563 [D loss: 0.369074, acc.: 86.72%] [G loss: 2.640012]\n",
      "epoch:14 step:11564 [D loss: 0.332726, acc.: 89.84%] [G loss: 3.785425]\n",
      "epoch:14 step:11565 [D loss: 0.335425, acc.: 85.94%] [G loss: 6.362179]\n",
      "epoch:14 step:11566 [D loss: 0.858971, acc.: 69.53%] [G loss: 9.287252]\n",
      "epoch:14 step:11567 [D loss: 2.229375, acc.: 58.59%] [G loss: 4.716521]\n",
      "epoch:14 step:11568 [D loss: 0.464064, acc.: 71.88%] [G loss: 3.439734]\n",
      "epoch:14 step:11569 [D loss: 0.515315, acc.: 77.34%] [G loss: 4.937147]\n",
      "epoch:14 step:11570 [D loss: 0.330207, acc.: 89.84%] [G loss: 3.995392]\n",
      "epoch:14 step:11571 [D loss: 0.528795, acc.: 79.69%] [G loss: 3.209863]\n",
      "epoch:14 step:11572 [D loss: 0.409509, acc.: 82.03%] [G loss: 3.959150]\n",
      "epoch:14 step:11573 [D loss: 0.350141, acc.: 85.94%] [G loss: 4.449355]\n",
      "epoch:14 step:11574 [D loss: 0.189160, acc.: 92.97%] [G loss: 4.213836]\n",
      "epoch:14 step:11575 [D loss: 0.332379, acc.: 88.28%] [G loss: 2.099366]\n",
      "epoch:14 step:11576 [D loss: 0.341968, acc.: 80.47%] [G loss: 2.756117]\n",
      "epoch:14 step:11577 [D loss: 0.274421, acc.: 86.72%] [G loss: 2.900096]\n",
      "epoch:14 step:11578 [D loss: 0.460885, acc.: 71.88%] [G loss: 2.502632]\n",
      "epoch:14 step:11579 [D loss: 0.427579, acc.: 77.34%] [G loss: 2.724926]\n",
      "epoch:14 step:11580 [D loss: 0.367018, acc.: 81.25%] [G loss: 2.552339]\n",
      "epoch:14 step:11581 [D loss: 0.280713, acc.: 84.38%] [G loss: 2.526582]\n",
      "epoch:14 step:11582 [D loss: 0.332758, acc.: 89.84%] [G loss: 2.495312]\n",
      "epoch:14 step:11583 [D loss: 0.331597, acc.: 84.38%] [G loss: 2.804274]\n",
      "epoch:14 step:11584 [D loss: 0.403190, acc.: 79.69%] [G loss: 3.414858]\n",
      "epoch:14 step:11585 [D loss: 0.429295, acc.: 79.69%] [G loss: 4.050895]\n",
      "epoch:14 step:11586 [D loss: 0.292652, acc.: 89.84%] [G loss: 3.409086]\n",
      "epoch:14 step:11587 [D loss: 0.202782, acc.: 93.75%] [G loss: 3.584677]\n",
      "epoch:14 step:11588 [D loss: 0.382493, acc.: 82.03%] [G loss: 3.088561]\n",
      "epoch:14 step:11589 [D loss: 0.329924, acc.: 85.16%] [G loss: 3.237693]\n",
      "epoch:14 step:11590 [D loss: 0.359324, acc.: 82.81%] [G loss: 2.858285]\n",
      "epoch:14 step:11591 [D loss: 0.487036, acc.: 72.66%] [G loss: 2.137610]\n",
      "epoch:14 step:11592 [D loss: 0.367290, acc.: 81.25%] [G loss: 2.539165]\n",
      "epoch:14 step:11593 [D loss: 0.427471, acc.: 78.91%] [G loss: 3.219319]\n",
      "epoch:14 step:11594 [D loss: 0.400424, acc.: 82.03%] [G loss: 4.114305]\n",
      "epoch:14 step:11595 [D loss: 0.339130, acc.: 82.81%] [G loss: 4.250105]\n",
      "epoch:14 step:11596 [D loss: 0.300106, acc.: 88.28%] [G loss: 3.260349]\n",
      "epoch:14 step:11597 [D loss: 0.276428, acc.: 92.19%] [G loss: 3.234963]\n",
      "epoch:14 step:11598 [D loss: 0.342153, acc.: 78.91%] [G loss: 4.201686]\n",
      "epoch:14 step:11599 [D loss: 0.287294, acc.: 91.41%] [G loss: 3.361919]\n",
      "epoch:14 step:11600 [D loss: 0.340180, acc.: 87.50%] [G loss: 3.574908]\n",
      "epoch:14 step:11601 [D loss: 0.231436, acc.: 90.62%] [G loss: 3.984043]\n",
      "epoch:14 step:11602 [D loss: 0.371364, acc.: 80.47%] [G loss: 2.612494]\n",
      "epoch:14 step:11603 [D loss: 0.413887, acc.: 77.34%] [G loss: 2.947109]\n",
      "epoch:14 step:11604 [D loss: 0.298236, acc.: 87.50%] [G loss: 3.360039]\n",
      "epoch:14 step:11605 [D loss: 0.509659, acc.: 75.00%] [G loss: 2.824551]\n",
      "epoch:14 step:11606 [D loss: 0.256557, acc.: 90.62%] [G loss: 4.617777]\n",
      "epoch:14 step:11607 [D loss: 0.370658, acc.: 82.81%] [G loss: 3.745532]\n",
      "epoch:14 step:11608 [D loss: 0.377936, acc.: 83.59%] [G loss: 4.420521]\n",
      "epoch:14 step:11609 [D loss: 0.278766, acc.: 89.06%] [G loss: 3.352822]\n",
      "epoch:14 step:11610 [D loss: 0.369844, acc.: 80.47%] [G loss: 2.782168]\n",
      "epoch:14 step:11611 [D loss: 0.275027, acc.: 87.50%] [G loss: 3.932546]\n",
      "epoch:14 step:11612 [D loss: 0.293046, acc.: 89.06%] [G loss: 2.363367]\n",
      "epoch:14 step:11613 [D loss: 0.258157, acc.: 85.16%] [G loss: 4.096082]\n",
      "epoch:14 step:11614 [D loss: 0.425568, acc.: 82.03%] [G loss: 3.372928]\n",
      "epoch:14 step:11615 [D loss: 0.239054, acc.: 92.19%] [G loss: 3.193248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11616 [D loss: 0.386533, acc.: 82.81%] [G loss: 2.659539]\n",
      "epoch:14 step:11617 [D loss: 0.301833, acc.: 88.28%] [G loss: 2.864104]\n",
      "epoch:14 step:11618 [D loss: 0.293753, acc.: 89.06%] [G loss: 2.910269]\n",
      "epoch:14 step:11619 [D loss: 0.293808, acc.: 85.16%] [G loss: 3.208364]\n",
      "epoch:14 step:11620 [D loss: 0.256619, acc.: 88.28%] [G loss: 2.965461]\n",
      "epoch:14 step:11621 [D loss: 0.276997, acc.: 89.06%] [G loss: 3.000731]\n",
      "epoch:14 step:11622 [D loss: 0.327901, acc.: 86.72%] [G loss: 3.691998]\n",
      "epoch:14 step:11623 [D loss: 0.327966, acc.: 84.38%] [G loss: 3.958603]\n",
      "epoch:14 step:11624 [D loss: 0.277166, acc.: 86.72%] [G loss: 5.511118]\n",
      "epoch:14 step:11625 [D loss: 0.189743, acc.: 95.31%] [G loss: 3.815570]\n",
      "epoch:14 step:11626 [D loss: 0.288117, acc.: 85.94%] [G loss: 2.796483]\n",
      "epoch:14 step:11627 [D loss: 0.298395, acc.: 84.38%] [G loss: 3.612772]\n",
      "epoch:14 step:11628 [D loss: 0.305115, acc.: 87.50%] [G loss: 2.602381]\n",
      "epoch:14 step:11629 [D loss: 0.308372, acc.: 89.84%] [G loss: 2.361461]\n",
      "epoch:14 step:11630 [D loss: 0.338056, acc.: 86.72%] [G loss: 3.318810]\n",
      "epoch:14 step:11631 [D loss: 0.347593, acc.: 79.69%] [G loss: 3.928790]\n",
      "epoch:14 step:11632 [D loss: 0.190957, acc.: 91.41%] [G loss: 5.697576]\n",
      "epoch:14 step:11633 [D loss: 0.290574, acc.: 87.50%] [G loss: 2.764008]\n",
      "epoch:14 step:11634 [D loss: 0.292596, acc.: 89.06%] [G loss: 3.726013]\n",
      "epoch:14 step:11635 [D loss: 0.318501, acc.: 87.50%] [G loss: 3.981477]\n",
      "epoch:14 step:11636 [D loss: 0.297961, acc.: 91.41%] [G loss: 2.709694]\n",
      "epoch:14 step:11637 [D loss: 0.340236, acc.: 85.16%] [G loss: 3.395450]\n",
      "epoch:14 step:11638 [D loss: 0.327869, acc.: 84.38%] [G loss: 4.699304]\n",
      "epoch:14 step:11639 [D loss: 0.381425, acc.: 82.81%] [G loss: 2.731005]\n",
      "epoch:14 step:11640 [D loss: 0.239820, acc.: 89.06%] [G loss: 3.535768]\n",
      "epoch:14 step:11641 [D loss: 0.300522, acc.: 85.16%] [G loss: 3.014197]\n",
      "epoch:14 step:11642 [D loss: 0.248099, acc.: 88.28%] [G loss: 3.219300]\n",
      "epoch:14 step:11643 [D loss: 0.525293, acc.: 78.12%] [G loss: 2.628757]\n",
      "epoch:14 step:11644 [D loss: 0.367026, acc.: 82.03%] [G loss: 2.854810]\n",
      "epoch:14 step:11645 [D loss: 0.301852, acc.: 88.28%] [G loss: 3.349920]\n",
      "epoch:14 step:11646 [D loss: 0.234077, acc.: 92.19%] [G loss: 4.140022]\n",
      "epoch:14 step:11647 [D loss: 0.404012, acc.: 82.03%] [G loss: 3.511078]\n",
      "epoch:14 step:11648 [D loss: 0.298399, acc.: 85.16%] [G loss: 4.585496]\n",
      "epoch:14 step:11649 [D loss: 0.518856, acc.: 78.91%] [G loss: 2.657928]\n",
      "epoch:14 step:11650 [D loss: 0.395789, acc.: 80.47%] [G loss: 3.209265]\n",
      "epoch:14 step:11651 [D loss: 0.395029, acc.: 82.03%] [G loss: 2.444683]\n",
      "epoch:14 step:11652 [D loss: 0.395991, acc.: 79.69%] [G loss: 2.288625]\n",
      "epoch:14 step:11653 [D loss: 0.430310, acc.: 80.47%] [G loss: 2.776694]\n",
      "epoch:14 step:11654 [D loss: 0.428177, acc.: 82.81%] [G loss: 2.159007]\n",
      "epoch:14 step:11655 [D loss: 0.398656, acc.: 82.03%] [G loss: 2.944005]\n",
      "epoch:14 step:11656 [D loss: 0.222432, acc.: 90.62%] [G loss: 3.473791]\n",
      "epoch:14 step:11657 [D loss: 0.373494, acc.: 84.38%] [G loss: 2.107104]\n",
      "epoch:14 step:11658 [D loss: 0.321528, acc.: 84.38%] [G loss: 2.871060]\n",
      "epoch:14 step:11659 [D loss: 0.304967, acc.: 87.50%] [G loss: 4.495475]\n",
      "epoch:14 step:11660 [D loss: 0.251817, acc.: 88.28%] [G loss: 3.317351]\n",
      "epoch:14 step:11661 [D loss: 0.373085, acc.: 82.81%] [G loss: 2.635860]\n",
      "epoch:14 step:11662 [D loss: 0.384417, acc.: 85.94%] [G loss: 3.496569]\n",
      "epoch:14 step:11663 [D loss: 0.522202, acc.: 72.66%] [G loss: 3.795776]\n",
      "epoch:14 step:11664 [D loss: 0.462095, acc.: 77.34%] [G loss: 3.038430]\n",
      "epoch:14 step:11665 [D loss: 0.409668, acc.: 80.47%] [G loss: 2.332592]\n",
      "epoch:14 step:11666 [D loss: 0.354086, acc.: 85.94%] [G loss: 2.735930]\n",
      "epoch:14 step:11667 [D loss: 0.284010, acc.: 88.28%] [G loss: 3.523947]\n",
      "epoch:14 step:11668 [D loss: 0.378912, acc.: 83.59%] [G loss: 4.707187]\n",
      "epoch:14 step:11669 [D loss: 0.321718, acc.: 85.94%] [G loss: 4.239025]\n",
      "epoch:14 step:11670 [D loss: 0.224536, acc.: 88.28%] [G loss: 4.656294]\n",
      "epoch:14 step:11671 [D loss: 0.333654, acc.: 85.94%] [G loss: 2.533039]\n",
      "epoch:14 step:11672 [D loss: 0.336618, acc.: 83.59%] [G loss: 2.072628]\n",
      "epoch:14 step:11673 [D loss: 0.371592, acc.: 84.38%] [G loss: 2.517359]\n",
      "epoch:14 step:11674 [D loss: 0.432515, acc.: 82.81%] [G loss: 3.130000]\n",
      "epoch:14 step:11675 [D loss: 0.259735, acc.: 87.50%] [G loss: 3.234485]\n",
      "epoch:14 step:11676 [D loss: 0.298233, acc.: 85.94%] [G loss: 4.012190]\n",
      "epoch:14 step:11677 [D loss: 0.307971, acc.: 86.72%] [G loss: 3.082800]\n",
      "epoch:14 step:11678 [D loss: 0.329465, acc.: 84.38%] [G loss: 2.907889]\n",
      "epoch:14 step:11679 [D loss: 0.266931, acc.: 91.41%] [G loss: 3.434829]\n",
      "epoch:14 step:11680 [D loss: 0.252117, acc.: 89.84%] [G loss: 4.590510]\n",
      "epoch:14 step:11681 [D loss: 0.392604, acc.: 84.38%] [G loss: 3.912173]\n",
      "epoch:14 step:11682 [D loss: 0.416816, acc.: 79.69%] [G loss: 2.819033]\n",
      "epoch:14 step:11683 [D loss: 0.249425, acc.: 89.84%] [G loss: 3.994411]\n",
      "epoch:14 step:11684 [D loss: 0.409795, acc.: 78.91%] [G loss: 2.693072]\n",
      "epoch:14 step:11685 [D loss: 0.353758, acc.: 83.59%] [G loss: 2.071710]\n",
      "epoch:14 step:11686 [D loss: 0.292200, acc.: 86.72%] [G loss: 2.833790]\n",
      "epoch:14 step:11687 [D loss: 0.295698, acc.: 89.84%] [G loss: 3.170089]\n",
      "epoch:14 step:11688 [D loss: 0.380982, acc.: 85.94%] [G loss: 2.573594]\n",
      "epoch:14 step:11689 [D loss: 0.381221, acc.: 82.81%] [G loss: 2.916677]\n",
      "epoch:14 step:11690 [D loss: 0.364240, acc.: 85.16%] [G loss: 2.764754]\n",
      "epoch:14 step:11691 [D loss: 0.344696, acc.: 81.25%] [G loss: 2.730135]\n",
      "epoch:14 step:11692 [D loss: 0.417383, acc.: 79.69%] [G loss: 3.586384]\n",
      "epoch:14 step:11693 [D loss: 0.453919, acc.: 83.59%] [G loss: 2.452550]\n",
      "epoch:14 step:11694 [D loss: 0.483533, acc.: 72.66%] [G loss: 2.500829]\n",
      "epoch:14 step:11695 [D loss: 0.330165, acc.: 86.72%] [G loss: 2.868694]\n",
      "epoch:14 step:11696 [D loss: 0.418623, acc.: 78.12%] [G loss: 2.991515]\n",
      "epoch:14 step:11697 [D loss: 0.518704, acc.: 70.31%] [G loss: 3.091505]\n",
      "epoch:14 step:11698 [D loss: 0.364447, acc.: 84.38%] [G loss: 2.648349]\n",
      "epoch:14 step:11699 [D loss: 0.391272, acc.: 81.25%] [G loss: 4.057098]\n",
      "epoch:14 step:11700 [D loss: 0.352950, acc.: 86.72%] [G loss: 2.465692]\n",
      "epoch:14 step:11701 [D loss: 0.208356, acc.: 91.41%] [G loss: 4.203544]\n",
      "epoch:14 step:11702 [D loss: 0.440566, acc.: 84.38%] [G loss: 2.669839]\n",
      "epoch:14 step:11703 [D loss: 0.287453, acc.: 85.94%] [G loss: 3.636490]\n",
      "epoch:14 step:11704 [D loss: 0.330533, acc.: 86.72%] [G loss: 3.357041]\n",
      "epoch:14 step:11705 [D loss: 0.309056, acc.: 84.38%] [G loss: 4.526455]\n",
      "epoch:14 step:11706 [D loss: 0.305098, acc.: 90.62%] [G loss: 5.316121]\n",
      "epoch:14 step:11707 [D loss: 0.294443, acc.: 87.50%] [G loss: 4.606126]\n",
      "epoch:14 step:11708 [D loss: 0.283029, acc.: 89.06%] [G loss: 4.361428]\n",
      "epoch:14 step:11709 [D loss: 0.308819, acc.: 85.16%] [G loss: 3.910049]\n",
      "epoch:14 step:11710 [D loss: 0.275189, acc.: 90.62%] [G loss: 3.921631]\n",
      "epoch:14 step:11711 [D loss: 0.405070, acc.: 82.03%] [G loss: 3.501306]\n",
      "epoch:14 step:11712 [D loss: 0.336449, acc.: 85.16%] [G loss: 3.390213]\n",
      "epoch:14 step:11713 [D loss: 0.249423, acc.: 87.50%] [G loss: 3.608378]\n",
      "epoch:14 step:11714 [D loss: 0.255331, acc.: 90.62%] [G loss: 4.785716]\n",
      "epoch:14 step:11715 [D loss: 0.332767, acc.: 85.94%] [G loss: 2.455272]\n",
      "epoch:15 step:11716 [D loss: 0.387313, acc.: 80.47%] [G loss: 2.609060]\n",
      "epoch:15 step:11717 [D loss: 0.287126, acc.: 86.72%] [G loss: 2.934293]\n",
      "epoch:15 step:11718 [D loss: 0.389201, acc.: 82.03%] [G loss: 3.379411]\n",
      "epoch:15 step:11719 [D loss: 0.319038, acc.: 86.72%] [G loss: 3.897262]\n",
      "epoch:15 step:11720 [D loss: 0.294078, acc.: 87.50%] [G loss: 4.002428]\n",
      "epoch:15 step:11721 [D loss: 0.337020, acc.: 85.16%] [G loss: 2.487502]\n",
      "epoch:15 step:11722 [D loss: 0.217310, acc.: 92.19%] [G loss: 3.545957]\n",
      "epoch:15 step:11723 [D loss: 0.231103, acc.: 89.84%] [G loss: 4.862198]\n",
      "epoch:15 step:11724 [D loss: 0.258527, acc.: 90.62%] [G loss: 4.711669]\n",
      "epoch:15 step:11725 [D loss: 0.227272, acc.: 91.41%] [G loss: 4.039077]\n",
      "epoch:15 step:11726 [D loss: 0.313028, acc.: 85.94%] [G loss: 4.587727]\n",
      "epoch:15 step:11727 [D loss: 0.336838, acc.: 86.72%] [G loss: 4.671796]\n",
      "epoch:15 step:11728 [D loss: 0.248960, acc.: 91.41%] [G loss: 4.067361]\n",
      "epoch:15 step:11729 [D loss: 0.329319, acc.: 87.50%] [G loss: 4.039571]\n",
      "epoch:15 step:11730 [D loss: 0.501423, acc.: 75.78%] [G loss: 2.955404]\n",
      "epoch:15 step:11731 [D loss: 0.308978, acc.: 85.94%] [G loss: 2.814763]\n",
      "epoch:15 step:11732 [D loss: 0.344854, acc.: 84.38%] [G loss: 3.803856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:11733 [D loss: 0.281173, acc.: 85.94%] [G loss: 5.538271]\n",
      "epoch:15 step:11734 [D loss: 0.375053, acc.: 80.47%] [G loss: 2.720698]\n",
      "epoch:15 step:11735 [D loss: 0.266447, acc.: 86.72%] [G loss: 2.326398]\n",
      "epoch:15 step:11736 [D loss: 0.398830, acc.: 85.16%] [G loss: 2.734785]\n",
      "epoch:15 step:11737 [D loss: 0.303108, acc.: 87.50%] [G loss: 2.699097]\n",
      "epoch:15 step:11738 [D loss: 0.272937, acc.: 89.06%] [G loss: 4.115645]\n",
      "epoch:15 step:11739 [D loss: 0.329275, acc.: 85.16%] [G loss: 6.929034]\n",
      "epoch:15 step:11740 [D loss: 0.335277, acc.: 86.72%] [G loss: 4.510951]\n",
      "epoch:15 step:11741 [D loss: 0.261867, acc.: 86.72%] [G loss: 2.428879]\n",
      "epoch:15 step:11742 [D loss: 0.222686, acc.: 92.97%] [G loss: 5.693856]\n",
      "epoch:15 step:11743 [D loss: 0.258437, acc.: 92.19%] [G loss: 3.550760]\n",
      "epoch:15 step:11744 [D loss: 0.322618, acc.: 85.16%] [G loss: 2.780676]\n",
      "epoch:15 step:11745 [D loss: 0.237872, acc.: 89.84%] [G loss: 3.323901]\n",
      "epoch:15 step:11746 [D loss: 0.247691, acc.: 92.19%] [G loss: 5.424401]\n",
      "epoch:15 step:11747 [D loss: 0.251444, acc.: 88.28%] [G loss: 4.254231]\n",
      "epoch:15 step:11748 [D loss: 0.271141, acc.: 90.62%] [G loss: 3.140902]\n",
      "epoch:15 step:11749 [D loss: 0.311912, acc.: 82.03%] [G loss: 3.043304]\n",
      "epoch:15 step:11750 [D loss: 0.312932, acc.: 87.50%] [G loss: 3.498778]\n",
      "epoch:15 step:11751 [D loss: 0.339365, acc.: 86.72%] [G loss: 2.474674]\n",
      "epoch:15 step:11752 [D loss: 0.296303, acc.: 85.94%] [G loss: 4.699736]\n",
      "epoch:15 step:11753 [D loss: 0.422004, acc.: 84.38%] [G loss: 5.803871]\n",
      "epoch:15 step:11754 [D loss: 0.820581, acc.: 71.09%] [G loss: 5.506509]\n",
      "epoch:15 step:11755 [D loss: 1.517596, acc.: 57.03%] [G loss: 3.019158]\n",
      "epoch:15 step:11756 [D loss: 0.398135, acc.: 78.12%] [G loss: 4.189041]\n",
      "epoch:15 step:11757 [D loss: 0.571871, acc.: 72.66%] [G loss: 4.601039]\n",
      "epoch:15 step:11758 [D loss: 0.572923, acc.: 76.56%] [G loss: 5.290873]\n",
      "epoch:15 step:11759 [D loss: 0.939140, acc.: 66.41%] [G loss: 6.483429]\n",
      "epoch:15 step:11760 [D loss: 1.624329, acc.: 50.78%] [G loss: 3.810540]\n",
      "epoch:15 step:11761 [D loss: 0.386990, acc.: 85.94%] [G loss: 7.239789]\n",
      "epoch:15 step:11762 [D loss: 0.482779, acc.: 77.34%] [G loss: 2.281612]\n",
      "epoch:15 step:11763 [D loss: 0.309816, acc.: 89.06%] [G loss: 3.656246]\n",
      "epoch:15 step:11764 [D loss: 0.363829, acc.: 84.38%] [G loss: 3.487515]\n",
      "epoch:15 step:11765 [D loss: 0.518355, acc.: 79.69%] [G loss: 4.059294]\n",
      "epoch:15 step:11766 [D loss: 0.205073, acc.: 94.53%] [G loss: 4.918266]\n",
      "epoch:15 step:11767 [D loss: 0.337258, acc.: 85.94%] [G loss: 3.090956]\n",
      "epoch:15 step:11768 [D loss: 0.334612, acc.: 83.59%] [G loss: 4.178165]\n",
      "epoch:15 step:11769 [D loss: 0.300695, acc.: 85.16%] [G loss: 4.190973]\n",
      "epoch:15 step:11770 [D loss: 0.334953, acc.: 85.94%] [G loss: 3.319538]\n",
      "epoch:15 step:11771 [D loss: 0.296983, acc.: 89.06%] [G loss: 2.598138]\n",
      "epoch:15 step:11772 [D loss: 0.260659, acc.: 91.41%] [G loss: 2.572625]\n",
      "epoch:15 step:11773 [D loss: 0.397751, acc.: 79.69%] [G loss: 2.796081]\n",
      "epoch:15 step:11774 [D loss: 0.353686, acc.: 81.25%] [G loss: 2.472770]\n",
      "epoch:15 step:11775 [D loss: 0.395634, acc.: 78.12%] [G loss: 2.681560]\n",
      "epoch:15 step:11776 [D loss: 0.416804, acc.: 81.25%] [G loss: 2.451387]\n",
      "epoch:15 step:11777 [D loss: 0.265159, acc.: 92.19%] [G loss: 2.740173]\n",
      "epoch:15 step:11778 [D loss: 0.275192, acc.: 90.62%] [G loss: 2.521727]\n",
      "epoch:15 step:11779 [D loss: 0.328556, acc.: 85.16%] [G loss: 2.107223]\n",
      "epoch:15 step:11780 [D loss: 0.408278, acc.: 82.03%] [G loss: 2.897058]\n",
      "epoch:15 step:11781 [D loss: 0.320038, acc.: 84.38%] [G loss: 3.258392]\n",
      "epoch:15 step:11782 [D loss: 0.358553, acc.: 82.81%] [G loss: 3.107877]\n",
      "epoch:15 step:11783 [D loss: 0.329418, acc.: 83.59%] [G loss: 3.053157]\n",
      "epoch:15 step:11784 [D loss: 0.371577, acc.: 82.81%] [G loss: 3.589190]\n",
      "epoch:15 step:11785 [D loss: 0.388950, acc.: 81.25%] [G loss: 2.307193]\n",
      "epoch:15 step:11786 [D loss: 0.330521, acc.: 87.50%] [G loss: 2.845782]\n",
      "epoch:15 step:11787 [D loss: 0.233035, acc.: 89.06%] [G loss: 2.543101]\n",
      "epoch:15 step:11788 [D loss: 0.422259, acc.: 81.25%] [G loss: 2.753291]\n",
      "epoch:15 step:11789 [D loss: 0.457995, acc.: 84.38%] [G loss: 2.778891]\n",
      "epoch:15 step:11790 [D loss: 0.374699, acc.: 83.59%] [G loss: 2.998711]\n",
      "epoch:15 step:11791 [D loss: 0.438331, acc.: 77.34%] [G loss: 2.821803]\n",
      "epoch:15 step:11792 [D loss: 0.396096, acc.: 82.81%] [G loss: 2.367189]\n",
      "epoch:15 step:11793 [D loss: 0.311302, acc.: 89.06%] [G loss: 2.920249]\n",
      "epoch:15 step:11794 [D loss: 0.351462, acc.: 82.81%] [G loss: 3.236654]\n",
      "epoch:15 step:11795 [D loss: 0.218772, acc.: 93.75%] [G loss: 3.267724]\n",
      "epoch:15 step:11796 [D loss: 0.336726, acc.: 82.81%] [G loss: 2.940968]\n",
      "epoch:15 step:11797 [D loss: 0.315910, acc.: 85.94%] [G loss: 2.688079]\n",
      "epoch:15 step:11798 [D loss: 0.319228, acc.: 86.72%] [G loss: 2.680578]\n",
      "epoch:15 step:11799 [D loss: 0.330399, acc.: 85.16%] [G loss: 2.814325]\n",
      "epoch:15 step:11800 [D loss: 0.382507, acc.: 82.03%] [G loss: 2.314347]\n",
      "epoch:15 step:11801 [D loss: 0.306477, acc.: 88.28%] [G loss: 2.765775]\n",
      "epoch:15 step:11802 [D loss: 0.283276, acc.: 86.72%] [G loss: 3.468576]\n",
      "epoch:15 step:11803 [D loss: 0.342936, acc.: 82.03%] [G loss: 3.146816]\n",
      "epoch:15 step:11804 [D loss: 0.371652, acc.: 81.25%] [G loss: 2.544056]\n",
      "epoch:15 step:11805 [D loss: 0.345724, acc.: 82.81%] [G loss: 3.442066]\n",
      "epoch:15 step:11806 [D loss: 0.209276, acc.: 89.84%] [G loss: 4.037163]\n",
      "epoch:15 step:11807 [D loss: 0.288398, acc.: 86.72%] [G loss: 3.023262]\n",
      "epoch:15 step:11808 [D loss: 0.282159, acc.: 89.06%] [G loss: 4.052195]\n",
      "epoch:15 step:11809 [D loss: 0.260129, acc.: 88.28%] [G loss: 3.795061]\n",
      "epoch:15 step:11810 [D loss: 0.396496, acc.: 83.59%] [G loss: 2.569443]\n",
      "epoch:15 step:11811 [D loss: 0.259077, acc.: 88.28%] [G loss: 3.936381]\n",
      "epoch:15 step:11812 [D loss: 0.252793, acc.: 91.41%] [G loss: 3.408366]\n",
      "epoch:15 step:11813 [D loss: 0.346522, acc.: 83.59%] [G loss: 2.827464]\n",
      "epoch:15 step:11814 [D loss: 0.344044, acc.: 83.59%] [G loss: 2.962740]\n",
      "epoch:15 step:11815 [D loss: 0.366290, acc.: 82.81%] [G loss: 2.385864]\n",
      "epoch:15 step:11816 [D loss: 0.256678, acc.: 89.84%] [G loss: 2.667283]\n",
      "epoch:15 step:11817 [D loss: 0.382180, acc.: 86.72%] [G loss: 2.610385]\n",
      "epoch:15 step:11818 [D loss: 0.358239, acc.: 85.94%] [G loss: 2.853023]\n",
      "epoch:15 step:11819 [D loss: 0.313208, acc.: 86.72%] [G loss: 3.733651]\n",
      "epoch:15 step:11820 [D loss: 0.389676, acc.: 83.59%] [G loss: 2.909999]\n",
      "epoch:15 step:11821 [D loss: 0.426984, acc.: 78.91%] [G loss: 3.175883]\n",
      "epoch:15 step:11822 [D loss: 0.215997, acc.: 91.41%] [G loss: 4.263590]\n",
      "epoch:15 step:11823 [D loss: 0.230210, acc.: 92.97%] [G loss: 2.737755]\n",
      "epoch:15 step:11824 [D loss: 0.278576, acc.: 89.06%] [G loss: 2.755090]\n",
      "epoch:15 step:11825 [D loss: 0.292391, acc.: 86.72%] [G loss: 2.386238]\n",
      "epoch:15 step:11826 [D loss: 0.486750, acc.: 78.91%] [G loss: 3.672238]\n",
      "epoch:15 step:11827 [D loss: 0.255421, acc.: 87.50%] [G loss: 4.589378]\n",
      "epoch:15 step:11828 [D loss: 0.316998, acc.: 85.16%] [G loss: 5.275982]\n",
      "epoch:15 step:11829 [D loss: 0.284525, acc.: 89.06%] [G loss: 3.494863]\n",
      "epoch:15 step:11830 [D loss: 0.268135, acc.: 88.28%] [G loss: 3.125838]\n",
      "epoch:15 step:11831 [D loss: 0.410491, acc.: 82.81%] [G loss: 3.078569]\n",
      "epoch:15 step:11832 [D loss: 0.294614, acc.: 86.72%] [G loss: 2.746548]\n",
      "epoch:15 step:11833 [D loss: 0.221726, acc.: 92.19%] [G loss: 3.707783]\n",
      "epoch:15 step:11834 [D loss: 0.206800, acc.: 89.06%] [G loss: 4.513010]\n",
      "epoch:15 step:11835 [D loss: 0.248482, acc.: 89.84%] [G loss: 6.138253]\n",
      "epoch:15 step:11836 [D loss: 0.221922, acc.: 93.75%] [G loss: 4.578593]\n",
      "epoch:15 step:11837 [D loss: 0.306482, acc.: 88.28%] [G loss: 4.242272]\n",
      "epoch:15 step:11838 [D loss: 0.297182, acc.: 89.06%] [G loss: 3.774973]\n",
      "epoch:15 step:11839 [D loss: 0.391830, acc.: 82.81%] [G loss: 4.704400]\n",
      "epoch:15 step:11840 [D loss: 0.471704, acc.: 81.25%] [G loss: 3.242660]\n",
      "epoch:15 step:11841 [D loss: 0.386152, acc.: 83.59%] [G loss: 2.657952]\n",
      "epoch:15 step:11842 [D loss: 0.309240, acc.: 85.16%] [G loss: 3.726230]\n",
      "epoch:15 step:11843 [D loss: 0.266640, acc.: 90.62%] [G loss: 3.346364]\n",
      "epoch:15 step:11844 [D loss: 0.355981, acc.: 85.94%] [G loss: 2.887465]\n",
      "epoch:15 step:11845 [D loss: 0.383702, acc.: 82.81%] [G loss: 2.319501]\n",
      "epoch:15 step:11846 [D loss: 0.309523, acc.: 87.50%] [G loss: 3.003343]\n",
      "epoch:15 step:11847 [D loss: 0.357810, acc.: 84.38%] [G loss: 2.352077]\n",
      "epoch:15 step:11848 [D loss: 0.303057, acc.: 85.16%] [G loss: 3.282139]\n",
      "epoch:15 step:11849 [D loss: 0.304950, acc.: 84.38%] [G loss: 3.549404]\n",
      "epoch:15 step:11850 [D loss: 0.305099, acc.: 87.50%] [G loss: 3.233823]\n",
      "epoch:15 step:11851 [D loss: 0.407349, acc.: 81.25%] [G loss: 2.664803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:11852 [D loss: 0.324548, acc.: 85.16%] [G loss: 3.393041]\n",
      "epoch:15 step:11853 [D loss: 0.312513, acc.: 87.50%] [G loss: 3.255114]\n",
      "epoch:15 step:11854 [D loss: 0.339289, acc.: 85.16%] [G loss: 2.782698]\n",
      "epoch:15 step:11855 [D loss: 0.390043, acc.: 83.59%] [G loss: 2.263319]\n",
      "epoch:15 step:11856 [D loss: 0.353147, acc.: 82.81%] [G loss: 2.374399]\n",
      "epoch:15 step:11857 [D loss: 0.409829, acc.: 85.16%] [G loss: 2.129002]\n",
      "epoch:15 step:11858 [D loss: 0.478249, acc.: 78.91%] [G loss: 2.334658]\n",
      "epoch:15 step:11859 [D loss: 0.268412, acc.: 89.06%] [G loss: 2.972823]\n",
      "epoch:15 step:11860 [D loss: 0.320349, acc.: 84.38%] [G loss: 3.111164]\n",
      "epoch:15 step:11861 [D loss: 0.383771, acc.: 85.94%] [G loss: 2.717317]\n",
      "epoch:15 step:11862 [D loss: 0.231494, acc.: 94.53%] [G loss: 2.849474]\n",
      "epoch:15 step:11863 [D loss: 0.375698, acc.: 86.72%] [G loss: 3.538056]\n",
      "epoch:15 step:11864 [D loss: 0.313045, acc.: 88.28%] [G loss: 3.917710]\n",
      "epoch:15 step:11865 [D loss: 0.329434, acc.: 85.94%] [G loss: 3.584954]\n",
      "epoch:15 step:11866 [D loss: 0.276911, acc.: 87.50%] [G loss: 3.406386]\n",
      "epoch:15 step:11867 [D loss: 0.298364, acc.: 86.72%] [G loss: 4.179782]\n",
      "epoch:15 step:11868 [D loss: 0.330835, acc.: 88.28%] [G loss: 2.789677]\n",
      "epoch:15 step:11869 [D loss: 0.333608, acc.: 88.28%] [G loss: 2.564718]\n",
      "epoch:15 step:11870 [D loss: 0.349470, acc.: 82.03%] [G loss: 2.489467]\n",
      "epoch:15 step:11871 [D loss: 0.327995, acc.: 85.94%] [G loss: 3.334944]\n",
      "epoch:15 step:11872 [D loss: 0.288332, acc.: 86.72%] [G loss: 4.634196]\n",
      "epoch:15 step:11873 [D loss: 0.372616, acc.: 82.81%] [G loss: 2.345703]\n",
      "epoch:15 step:11874 [D loss: 0.264279, acc.: 86.72%] [G loss: 5.335389]\n",
      "epoch:15 step:11875 [D loss: 0.315221, acc.: 85.16%] [G loss: 3.605809]\n",
      "epoch:15 step:11876 [D loss: 0.464651, acc.: 76.56%] [G loss: 2.618574]\n",
      "epoch:15 step:11877 [D loss: 0.378053, acc.: 85.94%] [G loss: 3.009150]\n",
      "epoch:15 step:11878 [D loss: 0.285160, acc.: 86.72%] [G loss: 2.949413]\n",
      "epoch:15 step:11879 [D loss: 0.351438, acc.: 85.16%] [G loss: 4.635017]\n",
      "epoch:15 step:11880 [D loss: 0.311879, acc.: 85.16%] [G loss: 4.631366]\n",
      "epoch:15 step:11881 [D loss: 0.313966, acc.: 87.50%] [G loss: 3.829434]\n",
      "epoch:15 step:11882 [D loss: 0.322462, acc.: 89.06%] [G loss: 2.555102]\n",
      "epoch:15 step:11883 [D loss: 0.229918, acc.: 90.62%] [G loss: 3.543560]\n",
      "epoch:15 step:11884 [D loss: 0.361794, acc.: 87.50%] [G loss: 2.304023]\n",
      "epoch:15 step:11885 [D loss: 0.182894, acc.: 89.84%] [G loss: 3.510658]\n",
      "epoch:15 step:11886 [D loss: 0.309149, acc.: 89.06%] [G loss: 2.109237]\n",
      "epoch:15 step:11887 [D loss: 0.328919, acc.: 88.28%] [G loss: 3.076595]\n",
      "epoch:15 step:11888 [D loss: 0.320732, acc.: 86.72%] [G loss: 2.635516]\n",
      "epoch:15 step:11889 [D loss: 0.448460, acc.: 77.34%] [G loss: 2.755950]\n",
      "epoch:15 step:11890 [D loss: 0.379613, acc.: 80.47%] [G loss: 2.460963]\n",
      "epoch:15 step:11891 [D loss: 0.463092, acc.: 78.91%] [G loss: 2.946646]\n",
      "epoch:15 step:11892 [D loss: 0.433872, acc.: 83.59%] [G loss: 3.508911]\n",
      "epoch:15 step:11893 [D loss: 0.263046, acc.: 89.84%] [G loss: 3.918422]\n",
      "epoch:15 step:11894 [D loss: 0.278894, acc.: 90.62%] [G loss: 3.563463]\n",
      "epoch:15 step:11895 [D loss: 0.213688, acc.: 94.53%] [G loss: 4.270865]\n",
      "epoch:15 step:11896 [D loss: 0.225981, acc.: 92.19%] [G loss: 3.620164]\n",
      "epoch:15 step:11897 [D loss: 0.271624, acc.: 89.06%] [G loss: 3.342412]\n",
      "epoch:15 step:11898 [D loss: 0.327054, acc.: 86.72%] [G loss: 2.726296]\n",
      "epoch:15 step:11899 [D loss: 0.394418, acc.: 78.12%] [G loss: 2.438183]\n",
      "epoch:15 step:11900 [D loss: 0.234130, acc.: 92.97%] [G loss: 3.841483]\n",
      "epoch:15 step:11901 [D loss: 0.448607, acc.: 78.12%] [G loss: 2.593154]\n",
      "epoch:15 step:11902 [D loss: 0.384510, acc.: 83.59%] [G loss: 3.521973]\n",
      "epoch:15 step:11903 [D loss: 0.336034, acc.: 84.38%] [G loss: 2.737451]\n",
      "epoch:15 step:11904 [D loss: 0.355985, acc.: 85.16%] [G loss: 2.831670]\n",
      "epoch:15 step:11905 [D loss: 0.317278, acc.: 86.72%] [G loss: 3.102269]\n",
      "epoch:15 step:11906 [D loss: 0.290806, acc.: 88.28%] [G loss: 3.045214]\n",
      "epoch:15 step:11907 [D loss: 0.235182, acc.: 90.62%] [G loss: 2.818171]\n",
      "epoch:15 step:11908 [D loss: 0.227919, acc.: 93.75%] [G loss: 3.298670]\n",
      "epoch:15 step:11909 [D loss: 0.215730, acc.: 92.19%] [G loss: 5.459159]\n",
      "epoch:15 step:11910 [D loss: 0.262975, acc.: 89.84%] [G loss: 3.923365]\n",
      "epoch:15 step:11911 [D loss: 0.296037, acc.: 86.72%] [G loss: 5.054346]\n",
      "epoch:15 step:11912 [D loss: 0.294498, acc.: 86.72%] [G loss: 3.969651]\n",
      "epoch:15 step:11913 [D loss: 0.279391, acc.: 88.28%] [G loss: 3.694391]\n",
      "epoch:15 step:11914 [D loss: 0.232192, acc.: 90.62%] [G loss: 7.094230]\n",
      "epoch:15 step:11915 [D loss: 0.317700, acc.: 85.16%] [G loss: 4.791814]\n",
      "epoch:15 step:11916 [D loss: 0.405543, acc.: 85.94%] [G loss: 3.113920]\n",
      "epoch:15 step:11917 [D loss: 0.320558, acc.: 85.16%] [G loss: 3.327175]\n",
      "epoch:15 step:11918 [D loss: 0.253899, acc.: 90.62%] [G loss: 4.686338]\n",
      "epoch:15 step:11919 [D loss: 0.276138, acc.: 89.06%] [G loss: 3.436861]\n",
      "epoch:15 step:11920 [D loss: 0.293111, acc.: 85.16%] [G loss: 3.173820]\n",
      "epoch:15 step:11921 [D loss: 0.314471, acc.: 85.16%] [G loss: 3.144034]\n",
      "epoch:15 step:11922 [D loss: 0.286282, acc.: 89.84%] [G loss: 2.434044]\n",
      "epoch:15 step:11923 [D loss: 0.397240, acc.: 84.38%] [G loss: 2.376893]\n",
      "epoch:15 step:11924 [D loss: 0.377400, acc.: 86.72%] [G loss: 2.628242]\n",
      "epoch:15 step:11925 [D loss: 0.326494, acc.: 84.38%] [G loss: 3.476267]\n",
      "epoch:15 step:11926 [D loss: 0.283687, acc.: 86.72%] [G loss: 5.622061]\n",
      "epoch:15 step:11927 [D loss: 0.573864, acc.: 78.12%] [G loss: 6.802005]\n",
      "epoch:15 step:11928 [D loss: 1.391672, acc.: 64.84%] [G loss: 6.096178]\n",
      "epoch:15 step:11929 [D loss: 1.492455, acc.: 61.72%] [G loss: 4.348475]\n",
      "epoch:15 step:11930 [D loss: 0.447898, acc.: 78.12%] [G loss: 4.031488]\n",
      "epoch:15 step:11931 [D loss: 0.589909, acc.: 75.00%] [G loss: 2.565227]\n",
      "epoch:15 step:11932 [D loss: 0.371189, acc.: 82.03%] [G loss: 4.002259]\n",
      "epoch:15 step:11933 [D loss: 0.294807, acc.: 85.16%] [G loss: 4.151318]\n",
      "epoch:15 step:11934 [D loss: 0.149408, acc.: 97.66%] [G loss: 4.285990]\n",
      "epoch:15 step:11935 [D loss: 0.250415, acc.: 92.97%] [G loss: 3.022319]\n",
      "epoch:15 step:11936 [D loss: 0.374464, acc.: 85.16%] [G loss: 3.432232]\n",
      "epoch:15 step:11937 [D loss: 0.312138, acc.: 85.94%] [G loss: 3.430475]\n",
      "epoch:15 step:11938 [D loss: 0.319361, acc.: 85.94%] [G loss: 4.073235]\n",
      "epoch:15 step:11939 [D loss: 0.342979, acc.: 85.16%] [G loss: 2.765651]\n",
      "epoch:15 step:11940 [D loss: 0.269362, acc.: 91.41%] [G loss: 3.941429]\n",
      "epoch:15 step:11941 [D loss: 0.341031, acc.: 84.38%] [G loss: 3.744458]\n",
      "epoch:15 step:11942 [D loss: 0.260805, acc.: 92.19%] [G loss: 3.407876]\n",
      "epoch:15 step:11943 [D loss: 0.318137, acc.: 85.94%] [G loss: 2.169181]\n",
      "epoch:15 step:11944 [D loss: 0.325574, acc.: 88.28%] [G loss: 2.273398]\n",
      "epoch:15 step:11945 [D loss: 0.330813, acc.: 84.38%] [G loss: 2.287809]\n",
      "epoch:15 step:11946 [D loss: 0.268303, acc.: 89.06%] [G loss: 2.883612]\n",
      "epoch:15 step:11947 [D loss: 0.341883, acc.: 86.72%] [G loss: 2.834575]\n",
      "epoch:15 step:11948 [D loss: 0.442801, acc.: 82.03%] [G loss: 2.812455]\n",
      "epoch:15 step:11949 [D loss: 0.364394, acc.: 82.81%] [G loss: 2.863545]\n",
      "epoch:15 step:11950 [D loss: 0.224196, acc.: 87.50%] [G loss: 2.806730]\n",
      "epoch:15 step:11951 [D loss: 0.322443, acc.: 85.94%] [G loss: 2.679289]\n",
      "epoch:15 step:11952 [D loss: 0.316466, acc.: 84.38%] [G loss: 3.150314]\n",
      "epoch:15 step:11953 [D loss: 0.407659, acc.: 78.12%] [G loss: 3.339743]\n",
      "epoch:15 step:11954 [D loss: 0.283970, acc.: 88.28%] [G loss: 3.640925]\n",
      "epoch:15 step:11955 [D loss: 0.265870, acc.: 86.72%] [G loss: 4.375193]\n",
      "epoch:15 step:11956 [D loss: 0.324599, acc.: 86.72%] [G loss: 3.064537]\n",
      "epoch:15 step:11957 [D loss: 0.340429, acc.: 82.03%] [G loss: 2.346031]\n",
      "epoch:15 step:11958 [D loss: 0.396817, acc.: 83.59%] [G loss: 3.285593]\n",
      "epoch:15 step:11959 [D loss: 0.474360, acc.: 79.69%] [G loss: 2.940259]\n",
      "epoch:15 step:11960 [D loss: 0.366716, acc.: 82.03%] [G loss: 2.572106]\n",
      "epoch:15 step:11961 [D loss: 0.332748, acc.: 85.94%] [G loss: 2.529483]\n",
      "epoch:15 step:11962 [D loss: 0.296572, acc.: 88.28%] [G loss: 3.500270]\n",
      "epoch:15 step:11963 [D loss: 0.316197, acc.: 84.38%] [G loss: 2.830424]\n",
      "epoch:15 step:11964 [D loss: 0.363820, acc.: 85.16%] [G loss: 2.937044]\n",
      "epoch:15 step:11965 [D loss: 0.439605, acc.: 82.03%] [G loss: 3.650952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:11966 [D loss: 0.435938, acc.: 81.25%] [G loss: 3.244863]\n",
      "epoch:15 step:11967 [D loss: 0.376070, acc.: 77.34%] [G loss: 2.514425]\n",
      "epoch:15 step:11968 [D loss: 0.243697, acc.: 89.84%] [G loss: 3.122920]\n",
      "epoch:15 step:11969 [D loss: 0.433471, acc.: 79.69%] [G loss: 3.390243]\n",
      "epoch:15 step:11970 [D loss: 0.334167, acc.: 82.81%] [G loss: 2.941225]\n",
      "epoch:15 step:11971 [D loss: 0.387258, acc.: 83.59%] [G loss: 3.216431]\n",
      "epoch:15 step:11972 [D loss: 0.400870, acc.: 80.47%] [G loss: 2.348841]\n",
      "epoch:15 step:11973 [D loss: 0.335616, acc.: 85.16%] [G loss: 3.212649]\n",
      "epoch:15 step:11974 [D loss: 0.355627, acc.: 81.25%] [G loss: 3.333928]\n",
      "epoch:15 step:11975 [D loss: 0.365306, acc.: 87.50%] [G loss: 2.716631]\n",
      "epoch:15 step:11976 [D loss: 0.353318, acc.: 89.06%] [G loss: 3.508862]\n",
      "epoch:15 step:11977 [D loss: 0.297481, acc.: 86.72%] [G loss: 3.591442]\n",
      "epoch:15 step:11978 [D loss: 0.231451, acc.: 90.62%] [G loss: 3.336097]\n",
      "epoch:15 step:11979 [D loss: 0.397639, acc.: 82.81%] [G loss: 2.832066]\n",
      "epoch:15 step:11980 [D loss: 0.318486, acc.: 86.72%] [G loss: 3.272462]\n",
      "epoch:15 step:11981 [D loss: 0.350253, acc.: 80.47%] [G loss: 2.489931]\n",
      "epoch:15 step:11982 [D loss: 0.405310, acc.: 82.03%] [G loss: 3.319926]\n",
      "epoch:15 step:11983 [D loss: 0.279328, acc.: 87.50%] [G loss: 2.805138]\n",
      "epoch:15 step:11984 [D loss: 0.297300, acc.: 84.38%] [G loss: 3.710210]\n",
      "epoch:15 step:11985 [D loss: 0.300426, acc.: 88.28%] [G loss: 2.336230]\n",
      "epoch:15 step:11986 [D loss: 0.301999, acc.: 85.94%] [G loss: 3.935009]\n",
      "epoch:15 step:11987 [D loss: 0.410660, acc.: 78.91%] [G loss: 3.406641]\n",
      "epoch:15 step:11988 [D loss: 0.216748, acc.: 91.41%] [G loss: 4.584679]\n",
      "epoch:15 step:11989 [D loss: 0.278579, acc.: 88.28%] [G loss: 4.445909]\n",
      "epoch:15 step:11990 [D loss: 0.361411, acc.: 82.81%] [G loss: 3.121130]\n",
      "epoch:15 step:11991 [D loss: 0.385972, acc.: 88.28%] [G loss: 3.019902]\n",
      "epoch:15 step:11992 [D loss: 0.285644, acc.: 89.84%] [G loss: 2.995347]\n",
      "epoch:15 step:11993 [D loss: 0.295309, acc.: 89.06%] [G loss: 2.878783]\n",
      "epoch:15 step:11994 [D loss: 0.360784, acc.: 84.38%] [G loss: 3.010509]\n",
      "epoch:15 step:11995 [D loss: 0.351346, acc.: 81.25%] [G loss: 3.016721]\n",
      "epoch:15 step:11996 [D loss: 0.304573, acc.: 82.81%] [G loss: 4.106806]\n",
      "epoch:15 step:11997 [D loss: 0.547359, acc.: 80.47%] [G loss: 3.344116]\n",
      "epoch:15 step:11998 [D loss: 0.357937, acc.: 82.81%] [G loss: 3.652483]\n",
      "epoch:15 step:11999 [D loss: 0.446117, acc.: 78.91%] [G loss: 3.960013]\n",
      "epoch:15 step:12000 [D loss: 0.292399, acc.: 88.28%] [G loss: 2.529176]\n",
      "epoch:15 step:12001 [D loss: 0.355413, acc.: 86.72%] [G loss: 3.823313]\n",
      "epoch:15 step:12002 [D loss: 0.226980, acc.: 94.53%] [G loss: 3.106896]\n",
      "epoch:15 step:12003 [D loss: 0.263926, acc.: 89.06%] [G loss: 4.356921]\n",
      "epoch:15 step:12004 [D loss: 0.288525, acc.: 88.28%] [G loss: 5.928199]\n",
      "epoch:15 step:12005 [D loss: 0.266945, acc.: 89.84%] [G loss: 3.372232]\n",
      "epoch:15 step:12006 [D loss: 0.271874, acc.: 89.84%] [G loss: 4.526498]\n",
      "epoch:15 step:12007 [D loss: 0.226339, acc.: 89.84%] [G loss: 6.097519]\n",
      "epoch:15 step:12008 [D loss: 0.206641, acc.: 91.41%] [G loss: 5.484643]\n",
      "epoch:15 step:12009 [D loss: 0.276008, acc.: 88.28%] [G loss: 3.677436]\n",
      "epoch:15 step:12010 [D loss: 0.404784, acc.: 80.47%] [G loss: 4.635413]\n",
      "epoch:15 step:12011 [D loss: 0.253104, acc.: 89.06%] [G loss: 4.471517]\n",
      "epoch:15 step:12012 [D loss: 0.286506, acc.: 87.50%] [G loss: 2.677473]\n",
      "epoch:15 step:12013 [D loss: 0.199447, acc.: 93.75%] [G loss: 4.076866]\n",
      "epoch:15 step:12014 [D loss: 0.321458, acc.: 85.94%] [G loss: 3.148333]\n",
      "epoch:15 step:12015 [D loss: 0.325553, acc.: 85.94%] [G loss: 2.692008]\n",
      "epoch:15 step:12016 [D loss: 0.312511, acc.: 87.50%] [G loss: 3.187605]\n",
      "epoch:15 step:12017 [D loss: 0.276497, acc.: 89.06%] [G loss: 2.372112]\n",
      "epoch:15 step:12018 [D loss: 0.271864, acc.: 89.84%] [G loss: 2.354250]\n",
      "epoch:15 step:12019 [D loss: 0.366722, acc.: 83.59%] [G loss: 3.351338]\n",
      "epoch:15 step:12020 [D loss: 0.303337, acc.: 86.72%] [G loss: 2.986318]\n",
      "epoch:15 step:12021 [D loss: 0.269791, acc.: 88.28%] [G loss: 3.100129]\n",
      "epoch:15 step:12022 [D loss: 0.238265, acc.: 89.84%] [G loss: 3.658119]\n",
      "epoch:15 step:12023 [D loss: 0.211230, acc.: 91.41%] [G loss: 3.694537]\n",
      "epoch:15 step:12024 [D loss: 0.294311, acc.: 89.06%] [G loss: 4.026052]\n",
      "epoch:15 step:12025 [D loss: 0.316312, acc.: 89.06%] [G loss: 3.957114]\n",
      "epoch:15 step:12026 [D loss: 0.406203, acc.: 82.03%] [G loss: 2.966921]\n",
      "epoch:15 step:12027 [D loss: 0.324940, acc.: 87.50%] [G loss: 2.747000]\n",
      "epoch:15 step:12028 [D loss: 0.304369, acc.: 88.28%] [G loss: 2.920742]\n",
      "epoch:15 step:12029 [D loss: 0.295987, acc.: 91.41%] [G loss: 3.163874]\n",
      "epoch:15 step:12030 [D loss: 0.291091, acc.: 87.50%] [G loss: 3.044122]\n",
      "epoch:15 step:12031 [D loss: 0.211359, acc.: 92.19%] [G loss: 3.812778]\n",
      "epoch:15 step:12032 [D loss: 0.244197, acc.: 89.06%] [G loss: 3.251040]\n",
      "epoch:15 step:12033 [D loss: 0.351557, acc.: 83.59%] [G loss: 2.612780]\n",
      "epoch:15 step:12034 [D loss: 0.266284, acc.: 88.28%] [G loss: 2.645717]\n",
      "epoch:15 step:12035 [D loss: 0.348405, acc.: 87.50%] [G loss: 2.531557]\n",
      "epoch:15 step:12036 [D loss: 0.271124, acc.: 92.19%] [G loss: 3.167132]\n",
      "epoch:15 step:12037 [D loss: 0.262776, acc.: 88.28%] [G loss: 3.251011]\n",
      "epoch:15 step:12038 [D loss: 0.280446, acc.: 90.62%] [G loss: 3.261098]\n",
      "epoch:15 step:12039 [D loss: 0.360054, acc.: 85.94%] [G loss: 3.382699]\n",
      "epoch:15 step:12040 [D loss: 0.339054, acc.: 84.38%] [G loss: 4.635469]\n",
      "epoch:15 step:12041 [D loss: 0.425232, acc.: 81.25%] [G loss: 3.255311]\n",
      "epoch:15 step:12042 [D loss: 0.331795, acc.: 87.50%] [G loss: 2.942172]\n",
      "epoch:15 step:12043 [D loss: 0.280787, acc.: 89.06%] [G loss: 4.150603]\n",
      "epoch:15 step:12044 [D loss: 0.427358, acc.: 77.34%] [G loss: 3.830935]\n",
      "epoch:15 step:12045 [D loss: 0.319691, acc.: 86.72%] [G loss: 2.576723]\n",
      "epoch:15 step:12046 [D loss: 0.317168, acc.: 83.59%] [G loss: 3.289876]\n",
      "epoch:15 step:12047 [D loss: 0.275852, acc.: 87.50%] [G loss: 4.757001]\n",
      "epoch:15 step:12048 [D loss: 0.413626, acc.: 82.03%] [G loss: 2.820147]\n",
      "epoch:15 step:12049 [D loss: 0.271493, acc.: 91.41%] [G loss: 2.413889]\n",
      "epoch:15 step:12050 [D loss: 0.417237, acc.: 82.03%] [G loss: 4.369566]\n",
      "epoch:15 step:12051 [D loss: 0.282583, acc.: 87.50%] [G loss: 3.508435]\n",
      "epoch:15 step:12052 [D loss: 0.182497, acc.: 92.19%] [G loss: 5.505564]\n",
      "epoch:15 step:12053 [D loss: 0.358588, acc.: 78.91%] [G loss: 2.927428]\n",
      "epoch:15 step:12054 [D loss: 0.290455, acc.: 89.84%] [G loss: 2.953050]\n",
      "epoch:15 step:12055 [D loss: 0.318359, acc.: 87.50%] [G loss: 5.760864]\n",
      "epoch:15 step:12056 [D loss: 0.508726, acc.: 79.69%] [G loss: 5.206731]\n",
      "epoch:15 step:12057 [D loss: 0.496403, acc.: 75.00%] [G loss: 3.894792]\n",
      "epoch:15 step:12058 [D loss: 0.432087, acc.: 79.69%] [G loss: 2.466347]\n",
      "epoch:15 step:12059 [D loss: 0.221862, acc.: 92.19%] [G loss: 2.879588]\n",
      "epoch:15 step:12060 [D loss: 0.259647, acc.: 89.06%] [G loss: 4.787131]\n",
      "epoch:15 step:12061 [D loss: 0.276004, acc.: 87.50%] [G loss: 3.772867]\n",
      "epoch:15 step:12062 [D loss: 0.302164, acc.: 82.03%] [G loss: 3.926290]\n",
      "epoch:15 step:12063 [D loss: 0.259579, acc.: 89.06%] [G loss: 2.581467]\n",
      "epoch:15 step:12064 [D loss: 0.270942, acc.: 88.28%] [G loss: 3.682898]\n",
      "epoch:15 step:12065 [D loss: 0.279095, acc.: 89.06%] [G loss: 3.143449]\n",
      "epoch:15 step:12066 [D loss: 0.303099, acc.: 86.72%] [G loss: 3.256642]\n",
      "epoch:15 step:12067 [D loss: 0.333939, acc.: 89.06%] [G loss: 2.539749]\n",
      "epoch:15 step:12068 [D loss: 0.313945, acc.: 84.38%] [G loss: 3.121286]\n",
      "epoch:15 step:12069 [D loss: 0.293221, acc.: 88.28%] [G loss: 3.496015]\n",
      "epoch:15 step:12070 [D loss: 0.285399, acc.: 82.81%] [G loss: 3.291672]\n",
      "epoch:15 step:12071 [D loss: 0.350746, acc.: 82.81%] [G loss: 2.979888]\n",
      "epoch:15 step:12072 [D loss: 0.396557, acc.: 79.69%] [G loss: 4.137360]\n",
      "epoch:15 step:12073 [D loss: 0.307784, acc.: 89.06%] [G loss: 3.797200]\n",
      "epoch:15 step:12074 [D loss: 0.338499, acc.: 82.03%] [G loss: 3.110694]\n",
      "epoch:15 step:12075 [D loss: 0.250171, acc.: 89.84%] [G loss: 3.167857]\n",
      "epoch:15 step:12076 [D loss: 0.216174, acc.: 91.41%] [G loss: 2.850024]\n",
      "epoch:15 step:12077 [D loss: 0.394067, acc.: 84.38%] [G loss: 2.983181]\n",
      "epoch:15 step:12078 [D loss: 0.260879, acc.: 92.97%] [G loss: 2.848189]\n",
      "epoch:15 step:12079 [D loss: 0.332669, acc.: 87.50%] [G loss: 3.194701]\n",
      "epoch:15 step:12080 [D loss: 0.365524, acc.: 82.03%] [G loss: 2.279775]\n",
      "epoch:15 step:12081 [D loss: 0.314841, acc.: 88.28%] [G loss: 3.744952]\n",
      "epoch:15 step:12082 [D loss: 0.341138, acc.: 85.94%] [G loss: 4.558161]\n",
      "epoch:15 step:12083 [D loss: 0.465604, acc.: 77.34%] [G loss: 7.376016]\n",
      "epoch:15 step:12084 [D loss: 0.642249, acc.: 74.22%] [G loss: 4.541907]\n",
      "epoch:15 step:12085 [D loss: 0.601464, acc.: 79.69%] [G loss: 3.590833]\n",
      "epoch:15 step:12086 [D loss: 0.264307, acc.: 88.28%] [G loss: 4.494629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12087 [D loss: 0.381557, acc.: 80.47%] [G loss: 6.423265]\n",
      "epoch:15 step:12088 [D loss: 0.235826, acc.: 89.84%] [G loss: 4.976798]\n",
      "epoch:15 step:12089 [D loss: 0.384006, acc.: 82.03%] [G loss: 3.504290]\n",
      "epoch:15 step:12090 [D loss: 0.417905, acc.: 82.81%] [G loss: 4.565324]\n",
      "epoch:15 step:12091 [D loss: 0.384136, acc.: 82.81%] [G loss: 3.697472]\n",
      "epoch:15 step:12092 [D loss: 0.346472, acc.: 89.06%] [G loss: 4.067405]\n",
      "epoch:15 step:12093 [D loss: 0.403772, acc.: 85.94%] [G loss: 4.444788]\n",
      "epoch:15 step:12094 [D loss: 0.567893, acc.: 76.56%] [G loss: 2.944785]\n",
      "epoch:15 step:12095 [D loss: 0.308382, acc.: 87.50%] [G loss: 2.633182]\n",
      "epoch:15 step:12096 [D loss: 0.319604, acc.: 89.84%] [G loss: 4.002887]\n",
      "epoch:15 step:12097 [D loss: 0.383035, acc.: 80.47%] [G loss: 2.389797]\n",
      "epoch:15 step:12098 [D loss: 0.302695, acc.: 85.16%] [G loss: 4.109259]\n",
      "epoch:15 step:12099 [D loss: 0.310099, acc.: 88.28%] [G loss: 2.802594]\n",
      "epoch:15 step:12100 [D loss: 0.362314, acc.: 87.50%] [G loss: 3.038073]\n",
      "epoch:15 step:12101 [D loss: 0.287027, acc.: 88.28%] [G loss: 4.825548]\n",
      "epoch:15 step:12102 [D loss: 0.361580, acc.: 85.16%] [G loss: 2.552542]\n",
      "epoch:15 step:12103 [D loss: 0.371886, acc.: 83.59%] [G loss: 3.159687]\n",
      "epoch:15 step:12104 [D loss: 0.393741, acc.: 85.16%] [G loss: 3.519410]\n",
      "epoch:15 step:12105 [D loss: 0.313252, acc.: 86.72%] [G loss: 4.159173]\n",
      "epoch:15 step:12106 [D loss: 0.450338, acc.: 79.69%] [G loss: 3.738163]\n",
      "epoch:15 step:12107 [D loss: 0.313188, acc.: 82.03%] [G loss: 3.633959]\n",
      "epoch:15 step:12108 [D loss: 0.229438, acc.: 89.06%] [G loss: 5.589497]\n",
      "epoch:15 step:12109 [D loss: 0.330268, acc.: 84.38%] [G loss: 2.751122]\n",
      "epoch:15 step:12110 [D loss: 0.393132, acc.: 82.81%] [G loss: 3.158557]\n",
      "epoch:15 step:12111 [D loss: 0.490272, acc.: 78.12%] [G loss: 2.365514]\n",
      "epoch:15 step:12112 [D loss: 0.369920, acc.: 83.59%] [G loss: 3.175794]\n",
      "epoch:15 step:12113 [D loss: 0.302749, acc.: 86.72%] [G loss: 2.606958]\n",
      "epoch:15 step:12114 [D loss: 0.343083, acc.: 85.16%] [G loss: 4.474050]\n",
      "epoch:15 step:12115 [D loss: 0.262659, acc.: 88.28%] [G loss: 3.785982]\n",
      "epoch:15 step:12116 [D loss: 0.279032, acc.: 89.06%] [G loss: 2.545694]\n",
      "epoch:15 step:12117 [D loss: 0.388599, acc.: 84.38%] [G loss: 3.076232]\n",
      "epoch:15 step:12118 [D loss: 0.322049, acc.: 86.72%] [G loss: 3.215420]\n",
      "epoch:15 step:12119 [D loss: 0.282439, acc.: 87.50%] [G loss: 3.072616]\n",
      "epoch:15 step:12120 [D loss: 0.242613, acc.: 88.28%] [G loss: 4.976623]\n",
      "epoch:15 step:12121 [D loss: 0.346365, acc.: 85.94%] [G loss: 3.366072]\n",
      "epoch:15 step:12122 [D loss: 0.345615, acc.: 84.38%] [G loss: 2.118929]\n",
      "epoch:15 step:12123 [D loss: 0.389175, acc.: 76.56%] [G loss: 2.369388]\n",
      "epoch:15 step:12124 [D loss: 0.300805, acc.: 85.94%] [G loss: 2.322586]\n",
      "epoch:15 step:12125 [D loss: 0.390877, acc.: 88.28%] [G loss: 2.683596]\n",
      "epoch:15 step:12126 [D loss: 0.323134, acc.: 92.19%] [G loss: 2.661705]\n",
      "epoch:15 step:12127 [D loss: 0.297665, acc.: 85.94%] [G loss: 2.954764]\n",
      "epoch:15 step:12128 [D loss: 0.301623, acc.: 87.50%] [G loss: 2.732983]\n",
      "epoch:15 step:12129 [D loss: 0.386202, acc.: 85.94%] [G loss: 3.056262]\n",
      "epoch:15 step:12130 [D loss: 0.295688, acc.: 87.50%] [G loss: 3.637496]\n",
      "epoch:15 step:12131 [D loss: 0.216785, acc.: 91.41%] [G loss: 3.450003]\n",
      "epoch:15 step:12132 [D loss: 0.385043, acc.: 82.03%] [G loss: 2.829314]\n",
      "epoch:15 step:12133 [D loss: 0.319364, acc.: 85.94%] [G loss: 3.998811]\n",
      "epoch:15 step:12134 [D loss: 0.318977, acc.: 85.16%] [G loss: 3.536813]\n",
      "epoch:15 step:12135 [D loss: 0.383708, acc.: 85.16%] [G loss: 2.534946]\n",
      "epoch:15 step:12136 [D loss: 0.261898, acc.: 89.06%] [G loss: 3.564117]\n",
      "epoch:15 step:12137 [D loss: 0.286709, acc.: 90.62%] [G loss: 3.051420]\n",
      "epoch:15 step:12138 [D loss: 0.295523, acc.: 85.16%] [G loss: 3.787627]\n",
      "epoch:15 step:12139 [D loss: 0.467435, acc.: 76.56%] [G loss: 3.656018]\n",
      "epoch:15 step:12140 [D loss: 0.250661, acc.: 87.50%] [G loss: 3.053293]\n",
      "epoch:15 step:12141 [D loss: 0.293858, acc.: 85.16%] [G loss: 4.938814]\n",
      "epoch:15 step:12142 [D loss: 0.192378, acc.: 91.41%] [G loss: 5.768790]\n",
      "epoch:15 step:12143 [D loss: 0.378974, acc.: 83.59%] [G loss: 3.065789]\n",
      "epoch:15 step:12144 [D loss: 0.266262, acc.: 88.28%] [G loss: 3.268975]\n",
      "epoch:15 step:12145 [D loss: 0.392347, acc.: 82.03%] [G loss: 3.173063]\n",
      "epoch:15 step:12146 [D loss: 0.363408, acc.: 84.38%] [G loss: 2.711280]\n",
      "epoch:15 step:12147 [D loss: 0.461626, acc.: 83.59%] [G loss: 2.843261]\n",
      "epoch:15 step:12148 [D loss: 0.294560, acc.: 85.94%] [G loss: 3.143404]\n",
      "epoch:15 step:12149 [D loss: 0.300078, acc.: 86.72%] [G loss: 5.403010]\n",
      "epoch:15 step:12150 [D loss: 0.492442, acc.: 78.91%] [G loss: 4.823018]\n",
      "epoch:15 step:12151 [D loss: 0.715856, acc.: 73.44%] [G loss: 6.124352]\n",
      "epoch:15 step:12152 [D loss: 1.258132, acc.: 67.97%] [G loss: 5.942776]\n",
      "epoch:15 step:12153 [D loss: 0.366821, acc.: 84.38%] [G loss: 2.944729]\n",
      "epoch:15 step:12154 [D loss: 0.492681, acc.: 76.56%] [G loss: 4.428392]\n",
      "epoch:15 step:12155 [D loss: 0.300056, acc.: 85.16%] [G loss: 4.658967]\n",
      "epoch:15 step:12156 [D loss: 0.365460, acc.: 82.81%] [G loss: 2.925975]\n",
      "epoch:15 step:12157 [D loss: 0.319775, acc.: 88.28%] [G loss: 3.179392]\n",
      "epoch:15 step:12158 [D loss: 0.306298, acc.: 85.94%] [G loss: 3.448886]\n",
      "epoch:15 step:12159 [D loss: 0.286503, acc.: 87.50%] [G loss: 2.675518]\n",
      "epoch:15 step:12160 [D loss: 0.297346, acc.: 86.72%] [G loss: 3.374216]\n",
      "epoch:15 step:12161 [D loss: 0.302061, acc.: 87.50%] [G loss: 3.195467]\n",
      "epoch:15 step:12162 [D loss: 0.368063, acc.: 85.94%] [G loss: 2.865952]\n",
      "epoch:15 step:12163 [D loss: 0.316046, acc.: 86.72%] [G loss: 3.089159]\n",
      "epoch:15 step:12164 [D loss: 0.405750, acc.: 81.25%] [G loss: 2.742894]\n",
      "epoch:15 step:12165 [D loss: 0.326086, acc.: 83.59%] [G loss: 3.735716]\n",
      "epoch:15 step:12166 [D loss: 0.336929, acc.: 85.94%] [G loss: 2.318938]\n",
      "epoch:15 step:12167 [D loss: 0.321789, acc.: 85.94%] [G loss: 3.036667]\n",
      "epoch:15 step:12168 [D loss: 0.324283, acc.: 88.28%] [G loss: 3.127249]\n",
      "epoch:15 step:12169 [D loss: 0.337943, acc.: 82.81%] [G loss: 3.789717]\n",
      "epoch:15 step:12170 [D loss: 0.342628, acc.: 85.16%] [G loss: 2.082062]\n",
      "epoch:15 step:12171 [D loss: 0.305529, acc.: 89.84%] [G loss: 2.747168]\n",
      "epoch:15 step:12172 [D loss: 0.264451, acc.: 88.28%] [G loss: 2.815779]\n",
      "epoch:15 step:12173 [D loss: 0.208424, acc.: 92.19%] [G loss: 3.686664]\n",
      "epoch:15 step:12174 [D loss: 0.283077, acc.: 90.62%] [G loss: 3.140673]\n",
      "epoch:15 step:12175 [D loss: 0.304937, acc.: 89.84%] [G loss: 3.070906]\n",
      "epoch:15 step:12176 [D loss: 0.348128, acc.: 87.50%] [G loss: 3.395908]\n",
      "epoch:15 step:12177 [D loss: 0.316798, acc.: 89.06%] [G loss: 3.445091]\n",
      "epoch:15 step:12178 [D loss: 0.228584, acc.: 89.84%] [G loss: 5.510919]\n",
      "epoch:15 step:12179 [D loss: 0.213998, acc.: 91.41%] [G loss: 4.628453]\n",
      "epoch:15 step:12180 [D loss: 0.194743, acc.: 92.97%] [G loss: 3.836585]\n",
      "epoch:15 step:12181 [D loss: 0.357295, acc.: 82.81%] [G loss: 4.663966]\n",
      "epoch:15 step:12182 [D loss: 0.257887, acc.: 91.41%] [G loss: 5.373602]\n",
      "epoch:15 step:12183 [D loss: 0.294919, acc.: 86.72%] [G loss: 3.509513]\n",
      "epoch:15 step:12184 [D loss: 0.224345, acc.: 91.41%] [G loss: 3.455829]\n",
      "epoch:15 step:12185 [D loss: 0.226259, acc.: 91.41%] [G loss: 2.658409]\n",
      "epoch:15 step:12186 [D loss: 0.213957, acc.: 91.41%] [G loss: 2.786327]\n",
      "epoch:15 step:12187 [D loss: 0.308119, acc.: 85.16%] [G loss: 3.034850]\n",
      "epoch:15 step:12188 [D loss: 0.316223, acc.: 86.72%] [G loss: 2.777025]\n",
      "epoch:15 step:12189 [D loss: 0.385738, acc.: 86.72%] [G loss: 2.390509]\n",
      "epoch:15 step:12190 [D loss: 0.349798, acc.: 78.91%] [G loss: 2.547512]\n",
      "epoch:15 step:12191 [D loss: 0.315185, acc.: 87.50%] [G loss: 2.811890]\n",
      "epoch:15 step:12192 [D loss: 0.232732, acc.: 91.41%] [G loss: 3.506941]\n",
      "epoch:15 step:12193 [D loss: 0.218916, acc.: 91.41%] [G loss: 3.682311]\n",
      "epoch:15 step:12194 [D loss: 0.324243, acc.: 85.16%] [G loss: 2.794876]\n",
      "epoch:15 step:12195 [D loss: 0.290059, acc.: 88.28%] [G loss: 2.766614]\n",
      "epoch:15 step:12196 [D loss: 0.355673, acc.: 82.81%] [G loss: 2.220667]\n",
      "epoch:15 step:12197 [D loss: 0.373625, acc.: 78.91%] [G loss: 2.303747]\n",
      "epoch:15 step:12198 [D loss: 0.352128, acc.: 85.94%] [G loss: 2.700083]\n",
      "epoch:15 step:12199 [D loss: 0.525414, acc.: 75.78%] [G loss: 2.394572]\n",
      "epoch:15 step:12200 [D loss: 0.261333, acc.: 88.28%] [G loss: 2.748167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12201 [D loss: 0.389234, acc.: 80.47%] [G loss: 3.690628]\n",
      "epoch:15 step:12202 [D loss: 0.269547, acc.: 87.50%] [G loss: 5.521741]\n",
      "epoch:15 step:12203 [D loss: 0.380257, acc.: 84.38%] [G loss: 2.357610]\n",
      "epoch:15 step:12204 [D loss: 0.273280, acc.: 87.50%] [G loss: 3.222051]\n",
      "epoch:15 step:12205 [D loss: 0.272072, acc.: 89.84%] [G loss: 3.199881]\n",
      "epoch:15 step:12206 [D loss: 0.420933, acc.: 82.81%] [G loss: 2.782815]\n",
      "epoch:15 step:12207 [D loss: 0.413225, acc.: 82.81%] [G loss: 2.610953]\n",
      "epoch:15 step:12208 [D loss: 0.348081, acc.: 86.72%] [G loss: 3.438499]\n",
      "epoch:15 step:12209 [D loss: 0.348379, acc.: 84.38%] [G loss: 2.352880]\n",
      "epoch:15 step:12210 [D loss: 0.355213, acc.: 85.94%] [G loss: 2.854692]\n",
      "epoch:15 step:12211 [D loss: 0.338560, acc.: 85.16%] [G loss: 2.278678]\n",
      "epoch:15 step:12212 [D loss: 0.269421, acc.: 89.84%] [G loss: 2.662436]\n",
      "epoch:15 step:12213 [D loss: 0.281715, acc.: 90.62%] [G loss: 2.567261]\n",
      "epoch:15 step:12214 [D loss: 0.360778, acc.: 86.72%] [G loss: 2.596606]\n",
      "epoch:15 step:12215 [D loss: 0.324808, acc.: 89.06%] [G loss: 3.043908]\n",
      "epoch:15 step:12216 [D loss: 0.390830, acc.: 82.81%] [G loss: 3.930182]\n",
      "epoch:15 step:12217 [D loss: 0.485729, acc.: 76.56%] [G loss: 4.087266]\n",
      "epoch:15 step:12218 [D loss: 0.315323, acc.: 85.94%] [G loss: 3.141265]\n",
      "epoch:15 step:12219 [D loss: 0.432374, acc.: 80.47%] [G loss: 2.658790]\n",
      "epoch:15 step:12220 [D loss: 0.311086, acc.: 92.97%] [G loss: 3.370831]\n",
      "epoch:15 step:12221 [D loss: 0.324901, acc.: 85.94%] [G loss: 4.647129]\n",
      "epoch:15 step:12222 [D loss: 0.268826, acc.: 91.41%] [G loss: 3.877701]\n",
      "epoch:15 step:12223 [D loss: 0.285880, acc.: 89.84%] [G loss: 3.354193]\n",
      "epoch:15 step:12224 [D loss: 0.312690, acc.: 89.06%] [G loss: 2.418750]\n",
      "epoch:15 step:12225 [D loss: 0.272656, acc.: 86.72%] [G loss: 3.504441]\n",
      "epoch:15 step:12226 [D loss: 0.238806, acc.: 90.62%] [G loss: 3.237649]\n",
      "epoch:15 step:12227 [D loss: 0.276964, acc.: 91.41%] [G loss: 3.101304]\n",
      "epoch:15 step:12228 [D loss: 0.340284, acc.: 85.94%] [G loss: 2.645620]\n",
      "epoch:15 step:12229 [D loss: 0.363518, acc.: 84.38%] [G loss: 2.077160]\n",
      "epoch:15 step:12230 [D loss: 0.281739, acc.: 89.84%] [G loss: 2.784299]\n",
      "epoch:15 step:12231 [D loss: 0.236379, acc.: 92.19%] [G loss: 2.404401]\n",
      "epoch:15 step:12232 [D loss: 0.410898, acc.: 78.91%] [G loss: 2.865751]\n",
      "epoch:15 step:12233 [D loss: 0.267640, acc.: 86.72%] [G loss: 4.315136]\n",
      "epoch:15 step:12234 [D loss: 0.384156, acc.: 85.16%] [G loss: 3.324398]\n",
      "epoch:15 step:12235 [D loss: 0.342402, acc.: 86.72%] [G loss: 2.572626]\n",
      "epoch:15 step:12236 [D loss: 0.291566, acc.: 87.50%] [G loss: 2.755218]\n",
      "epoch:15 step:12237 [D loss: 0.309327, acc.: 90.62%] [G loss: 3.106877]\n",
      "epoch:15 step:12238 [D loss: 0.385499, acc.: 85.94%] [G loss: 2.632237]\n",
      "epoch:15 step:12239 [D loss: 0.329491, acc.: 85.94%] [G loss: 2.447283]\n",
      "epoch:15 step:12240 [D loss: 0.264267, acc.: 89.84%] [G loss: 3.262094]\n",
      "epoch:15 step:12241 [D loss: 0.245641, acc.: 92.19%] [G loss: 3.687471]\n",
      "epoch:15 step:12242 [D loss: 0.363398, acc.: 83.59%] [G loss: 3.201649]\n",
      "epoch:15 step:12243 [D loss: 0.222665, acc.: 92.19%] [G loss: 3.068859]\n",
      "epoch:15 step:12244 [D loss: 0.294988, acc.: 89.06%] [G loss: 2.640086]\n",
      "epoch:15 step:12245 [D loss: 0.427820, acc.: 82.81%] [G loss: 3.541152]\n",
      "epoch:15 step:12246 [D loss: 0.259161, acc.: 92.97%] [G loss: 3.129382]\n",
      "epoch:15 step:12247 [D loss: 0.312402, acc.: 84.38%] [G loss: 3.699272]\n",
      "epoch:15 step:12248 [D loss: 0.399537, acc.: 79.69%] [G loss: 3.431963]\n",
      "epoch:15 step:12249 [D loss: 0.230895, acc.: 90.62%] [G loss: 3.409337]\n",
      "epoch:15 step:12250 [D loss: 0.230353, acc.: 91.41%] [G loss: 4.451330]\n",
      "epoch:15 step:12251 [D loss: 0.336786, acc.: 86.72%] [G loss: 3.345888]\n",
      "epoch:15 step:12252 [D loss: 0.297670, acc.: 86.72%] [G loss: 3.639324]\n",
      "epoch:15 step:12253 [D loss: 0.303044, acc.: 87.50%] [G loss: 3.461883]\n",
      "epoch:15 step:12254 [D loss: 0.255168, acc.: 89.84%] [G loss: 3.305974]\n",
      "epoch:15 step:12255 [D loss: 0.311439, acc.: 86.72%] [G loss: 3.718457]\n",
      "epoch:15 step:12256 [D loss: 0.363594, acc.: 83.59%] [G loss: 2.724235]\n",
      "epoch:15 step:12257 [D loss: 0.466940, acc.: 77.34%] [G loss: 2.248639]\n",
      "epoch:15 step:12258 [D loss: 0.301979, acc.: 89.84%] [G loss: 3.195602]\n",
      "epoch:15 step:12259 [D loss: 0.370012, acc.: 82.03%] [G loss: 3.139575]\n",
      "epoch:15 step:12260 [D loss: 0.276015, acc.: 90.62%] [G loss: 3.021753]\n",
      "epoch:15 step:12261 [D loss: 0.334616, acc.: 85.16%] [G loss: 2.446166]\n",
      "epoch:15 step:12262 [D loss: 0.415110, acc.: 80.47%] [G loss: 2.227499]\n",
      "epoch:15 step:12263 [D loss: 0.398184, acc.: 84.38%] [G loss: 2.320828]\n",
      "epoch:15 step:12264 [D loss: 0.345199, acc.: 84.38%] [G loss: 2.832496]\n",
      "epoch:15 step:12265 [D loss: 0.258279, acc.: 89.84%] [G loss: 2.793491]\n",
      "epoch:15 step:12266 [D loss: 0.323594, acc.: 88.28%] [G loss: 2.870605]\n",
      "epoch:15 step:12267 [D loss: 0.390549, acc.: 82.03%] [G loss: 3.433526]\n",
      "epoch:15 step:12268 [D loss: 0.390596, acc.: 82.03%] [G loss: 3.452630]\n",
      "epoch:15 step:12269 [D loss: 0.387721, acc.: 84.38%] [G loss: 3.287345]\n",
      "epoch:15 step:12270 [D loss: 0.203828, acc.: 88.28%] [G loss: 6.526271]\n",
      "epoch:15 step:12271 [D loss: 0.455219, acc.: 75.00%] [G loss: 3.108428]\n",
      "epoch:15 step:12272 [D loss: 0.269090, acc.: 89.06%] [G loss: 3.348290]\n",
      "epoch:15 step:12273 [D loss: 0.252648, acc.: 88.28%] [G loss: 3.030326]\n",
      "epoch:15 step:12274 [D loss: 0.306129, acc.: 87.50%] [G loss: 3.651544]\n",
      "epoch:15 step:12275 [D loss: 0.459617, acc.: 79.69%] [G loss: 2.534877]\n",
      "epoch:15 step:12276 [D loss: 0.272324, acc.: 85.16%] [G loss: 3.756076]\n",
      "epoch:15 step:12277 [D loss: 0.355441, acc.: 84.38%] [G loss: 4.124363]\n",
      "epoch:15 step:12278 [D loss: 0.499416, acc.: 80.47%] [G loss: 3.545735]\n",
      "epoch:15 step:12279 [D loss: 0.315943, acc.: 84.38%] [G loss: 2.783782]\n",
      "epoch:15 step:12280 [D loss: 0.292801, acc.: 82.81%] [G loss: 6.167374]\n",
      "epoch:15 step:12281 [D loss: 0.310650, acc.: 86.72%] [G loss: 3.041533]\n",
      "epoch:15 step:12282 [D loss: 0.328781, acc.: 84.38%] [G loss: 3.695574]\n",
      "epoch:15 step:12283 [D loss: 0.358028, acc.: 80.47%] [G loss: 2.640527]\n",
      "epoch:15 step:12284 [D loss: 0.341515, acc.: 82.81%] [G loss: 3.330775]\n",
      "epoch:15 step:12285 [D loss: 0.275831, acc.: 85.94%] [G loss: 3.385672]\n",
      "epoch:15 step:12286 [D loss: 0.305841, acc.: 83.59%] [G loss: 3.604171]\n",
      "epoch:15 step:12287 [D loss: 0.333118, acc.: 89.06%] [G loss: 3.007667]\n",
      "epoch:15 step:12288 [D loss: 0.234736, acc.: 89.84%] [G loss: 4.770271]\n",
      "epoch:15 step:12289 [D loss: 0.366316, acc.: 85.16%] [G loss: 3.183694]\n",
      "epoch:15 step:12290 [D loss: 0.352387, acc.: 86.72%] [G loss: 2.606329]\n",
      "epoch:15 step:12291 [D loss: 0.372629, acc.: 85.16%] [G loss: 3.720476]\n",
      "epoch:15 step:12292 [D loss: 0.258207, acc.: 88.28%] [G loss: 4.479303]\n",
      "epoch:15 step:12293 [D loss: 0.540241, acc.: 79.69%] [G loss: 8.821865]\n",
      "epoch:15 step:12294 [D loss: 0.683380, acc.: 76.56%] [G loss: 6.104791]\n",
      "epoch:15 step:12295 [D loss: 0.878082, acc.: 67.97%] [G loss: 6.052111]\n",
      "epoch:15 step:12296 [D loss: 0.553896, acc.: 80.47%] [G loss: 2.874247]\n",
      "epoch:15 step:12297 [D loss: 0.513508, acc.: 73.44%] [G loss: 4.155853]\n",
      "epoch:15 step:12298 [D loss: 0.400475, acc.: 84.38%] [G loss: 4.295668]\n",
      "epoch:15 step:12299 [D loss: 0.494218, acc.: 82.81%] [G loss: 3.105816]\n",
      "epoch:15 step:12300 [D loss: 0.648797, acc.: 80.47%] [G loss: 5.665738]\n",
      "epoch:15 step:12301 [D loss: 0.296553, acc.: 86.72%] [G loss: 4.356827]\n",
      "epoch:15 step:12302 [D loss: 0.254301, acc.: 89.06%] [G loss: 4.796930]\n",
      "epoch:15 step:12303 [D loss: 0.273302, acc.: 88.28%] [G loss: 3.605885]\n",
      "epoch:15 step:12304 [D loss: 0.444811, acc.: 80.47%] [G loss: 3.376185]\n",
      "epoch:15 step:12305 [D loss: 0.411023, acc.: 75.78%] [G loss: 3.049901]\n",
      "epoch:15 step:12306 [D loss: 0.322693, acc.: 85.94%] [G loss: 3.453260]\n",
      "epoch:15 step:12307 [D loss: 0.272497, acc.: 89.84%] [G loss: 5.238264]\n",
      "epoch:15 step:12308 [D loss: 0.314812, acc.: 85.16%] [G loss: 5.433901]\n",
      "epoch:15 step:12309 [D loss: 0.302622, acc.: 89.06%] [G loss: 2.590243]\n",
      "epoch:15 step:12310 [D loss: 0.271879, acc.: 88.28%] [G loss: 3.944253]\n",
      "epoch:15 step:12311 [D loss: 0.297757, acc.: 87.50%] [G loss: 3.214252]\n",
      "epoch:15 step:12312 [D loss: 0.297142, acc.: 89.06%] [G loss: 4.578214]\n",
      "epoch:15 step:12313 [D loss: 0.363770, acc.: 82.03%] [G loss: 3.265596]\n",
      "epoch:15 step:12314 [D loss: 0.355977, acc.: 83.59%] [G loss: 2.097562]\n",
      "epoch:15 step:12315 [D loss: 0.390365, acc.: 84.38%] [G loss: 2.765699]\n",
      "epoch:15 step:12316 [D loss: 0.483774, acc.: 76.56%] [G loss: 2.271437]\n",
      "epoch:15 step:12317 [D loss: 0.315348, acc.: 91.41%] [G loss: 2.538942]\n",
      "epoch:15 step:12318 [D loss: 0.322993, acc.: 85.16%] [G loss: 3.822214]\n",
      "epoch:15 step:12319 [D loss: 0.413925, acc.: 78.91%] [G loss: 2.132921]\n",
      "epoch:15 step:12320 [D loss: 0.372603, acc.: 85.94%] [G loss: 4.291362]\n",
      "epoch:15 step:12321 [D loss: 0.316729, acc.: 86.72%] [G loss: 2.957242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12322 [D loss: 0.310623, acc.: 87.50%] [G loss: 3.596235]\n",
      "epoch:15 step:12323 [D loss: 0.331451, acc.: 87.50%] [G loss: 2.326670]\n",
      "epoch:15 step:12324 [D loss: 0.323250, acc.: 88.28%] [G loss: 2.932344]\n",
      "epoch:15 step:12325 [D loss: 0.441186, acc.: 81.25%] [G loss: 2.569769]\n",
      "epoch:15 step:12326 [D loss: 0.300030, acc.: 89.06%] [G loss: 2.426930]\n",
      "epoch:15 step:12327 [D loss: 0.336896, acc.: 89.06%] [G loss: 3.064272]\n",
      "epoch:15 step:12328 [D loss: 0.432970, acc.: 82.81%] [G loss: 3.905550]\n",
      "epoch:15 step:12329 [D loss: 0.363889, acc.: 86.72%] [G loss: 2.396037]\n",
      "epoch:15 step:12330 [D loss: 0.332971, acc.: 88.28%] [G loss: 2.618001]\n",
      "epoch:15 step:12331 [D loss: 0.253426, acc.: 88.28%] [G loss: 2.988155]\n",
      "epoch:15 step:12332 [D loss: 0.359056, acc.: 87.50%] [G loss: 2.215832]\n",
      "epoch:15 step:12333 [D loss: 0.382453, acc.: 81.25%] [G loss: 2.341481]\n",
      "epoch:15 step:12334 [D loss: 0.296246, acc.: 86.72%] [G loss: 2.454542]\n",
      "epoch:15 step:12335 [D loss: 0.282206, acc.: 88.28%] [G loss: 2.578302]\n",
      "epoch:15 step:12336 [D loss: 0.422567, acc.: 78.91%] [G loss: 2.916859]\n",
      "epoch:15 step:12337 [D loss: 0.242220, acc.: 92.19%] [G loss: 3.125423]\n",
      "epoch:15 step:12338 [D loss: 0.391175, acc.: 82.81%] [G loss: 2.585572]\n",
      "epoch:15 step:12339 [D loss: 0.285500, acc.: 89.06%] [G loss: 3.286665]\n",
      "epoch:15 step:12340 [D loss: 0.323256, acc.: 89.84%] [G loss: 3.041562]\n",
      "epoch:15 step:12341 [D loss: 0.273330, acc.: 88.28%] [G loss: 2.523121]\n",
      "epoch:15 step:12342 [D loss: 0.319248, acc.: 88.28%] [G loss: 2.817908]\n",
      "epoch:15 step:12343 [D loss: 0.411734, acc.: 80.47%] [G loss: 1.939645]\n",
      "epoch:15 step:12344 [D loss: 0.320835, acc.: 87.50%] [G loss: 2.608358]\n",
      "epoch:15 step:12345 [D loss: 0.339109, acc.: 83.59%] [G loss: 2.887760]\n",
      "epoch:15 step:12346 [D loss: 0.323123, acc.: 85.94%] [G loss: 1.940429]\n",
      "epoch:15 step:12347 [D loss: 0.382119, acc.: 82.81%] [G loss: 2.458181]\n",
      "epoch:15 step:12348 [D loss: 0.438488, acc.: 78.91%] [G loss: 2.265835]\n",
      "epoch:15 step:12349 [D loss: 0.330084, acc.: 87.50%] [G loss: 2.656220]\n",
      "epoch:15 step:12350 [D loss: 0.420401, acc.: 79.69%] [G loss: 2.463315]\n",
      "epoch:15 step:12351 [D loss: 0.340991, acc.: 82.03%] [G loss: 2.907481]\n",
      "epoch:15 step:12352 [D loss: 0.327422, acc.: 89.84%] [G loss: 2.557033]\n",
      "epoch:15 step:12353 [D loss: 0.329882, acc.: 88.28%] [G loss: 3.517672]\n",
      "epoch:15 step:12354 [D loss: 0.408740, acc.: 82.03%] [G loss: 3.746037]\n",
      "epoch:15 step:12355 [D loss: 0.201977, acc.: 92.19%] [G loss: 3.905767]\n",
      "epoch:15 step:12356 [D loss: 0.318157, acc.: 85.94%] [G loss: 3.390675]\n",
      "epoch:15 step:12357 [D loss: 0.214367, acc.: 92.19%] [G loss: 3.842931]\n",
      "epoch:15 step:12358 [D loss: 0.383179, acc.: 81.25%] [G loss: 2.726137]\n",
      "epoch:15 step:12359 [D loss: 0.271089, acc.: 87.50%] [G loss: 2.897670]\n",
      "epoch:15 step:12360 [D loss: 0.358812, acc.: 82.81%] [G loss: 2.565496]\n",
      "epoch:15 step:12361 [D loss: 0.370955, acc.: 84.38%] [G loss: 2.297102]\n",
      "epoch:15 step:12362 [D loss: 0.322531, acc.: 87.50%] [G loss: 2.088125]\n",
      "epoch:15 step:12363 [D loss: 0.447947, acc.: 78.12%] [G loss: 2.716544]\n",
      "epoch:15 step:12364 [D loss: 0.279380, acc.: 89.84%] [G loss: 2.517766]\n",
      "epoch:15 step:12365 [D loss: 0.297772, acc.: 86.72%] [G loss: 2.438307]\n",
      "epoch:15 step:12366 [D loss: 0.346396, acc.: 82.81%] [G loss: 2.515938]\n",
      "epoch:15 step:12367 [D loss: 0.323312, acc.: 86.72%] [G loss: 2.738808]\n",
      "epoch:15 step:12368 [D loss: 0.333819, acc.: 85.16%] [G loss: 3.951250]\n",
      "epoch:15 step:12369 [D loss: 0.311454, acc.: 88.28%] [G loss: 2.846825]\n",
      "epoch:15 step:12370 [D loss: 0.240277, acc.: 90.62%] [G loss: 3.337741]\n",
      "epoch:15 step:12371 [D loss: 0.265297, acc.: 88.28%] [G loss: 2.881795]\n",
      "epoch:15 step:12372 [D loss: 0.342417, acc.: 83.59%] [G loss: 3.441637]\n",
      "epoch:15 step:12373 [D loss: 0.255014, acc.: 91.41%] [G loss: 2.732249]\n",
      "epoch:15 step:12374 [D loss: 0.282378, acc.: 92.19%] [G loss: 2.363570]\n",
      "epoch:15 step:12375 [D loss: 0.361329, acc.: 85.94%] [G loss: 2.362818]\n",
      "epoch:15 step:12376 [D loss: 0.368807, acc.: 85.16%] [G loss: 1.994769]\n",
      "epoch:15 step:12377 [D loss: 0.287065, acc.: 92.19%] [G loss: 2.267021]\n",
      "epoch:15 step:12378 [D loss: 0.318027, acc.: 88.28%] [G loss: 3.096677]\n",
      "epoch:15 step:12379 [D loss: 0.285288, acc.: 89.84%] [G loss: 3.384227]\n",
      "epoch:15 step:12380 [D loss: 0.271024, acc.: 91.41%] [G loss: 3.615144]\n",
      "epoch:15 step:12381 [D loss: 0.249470, acc.: 89.84%] [G loss: 3.567869]\n",
      "epoch:15 step:12382 [D loss: 0.229970, acc.: 91.41%] [G loss: 3.832735]\n",
      "epoch:15 step:12383 [D loss: 0.253985, acc.: 88.28%] [G loss: 4.275117]\n",
      "epoch:15 step:12384 [D loss: 0.292501, acc.: 87.50%] [G loss: 3.804721]\n",
      "epoch:15 step:12385 [D loss: 0.276232, acc.: 86.72%] [G loss: 2.532994]\n",
      "epoch:15 step:12386 [D loss: 0.399043, acc.: 83.59%] [G loss: 2.369820]\n",
      "epoch:15 step:12387 [D loss: 0.277320, acc.: 89.84%] [G loss: 4.051194]\n",
      "epoch:15 step:12388 [D loss: 0.289368, acc.: 87.50%] [G loss: 2.328612]\n",
      "epoch:15 step:12389 [D loss: 0.347730, acc.: 82.81%] [G loss: 3.512286]\n",
      "epoch:15 step:12390 [D loss: 0.192254, acc.: 91.41%] [G loss: 6.887147]\n",
      "epoch:15 step:12391 [D loss: 0.245032, acc.: 88.28%] [G loss: 2.689355]\n",
      "epoch:15 step:12392 [D loss: 0.235178, acc.: 91.41%] [G loss: 5.489261]\n",
      "epoch:15 step:12393 [D loss: 0.305372, acc.: 86.72%] [G loss: 2.267540]\n",
      "epoch:15 step:12394 [D loss: 0.238906, acc.: 89.84%] [G loss: 2.927821]\n",
      "epoch:15 step:12395 [D loss: 0.354907, acc.: 85.16%] [G loss: 3.065562]\n",
      "epoch:15 step:12396 [D loss: 0.247968, acc.: 89.06%] [G loss: 5.958693]\n",
      "epoch:15 step:12397 [D loss: 0.295859, acc.: 86.72%] [G loss: 3.467559]\n",
      "epoch:15 step:12398 [D loss: 0.258976, acc.: 88.28%] [G loss: 4.515485]\n",
      "epoch:15 step:12399 [D loss: 0.309128, acc.: 85.16%] [G loss: 3.566405]\n",
      "epoch:15 step:12400 [D loss: 0.388529, acc.: 83.59%] [G loss: 2.431790]\n",
      "epoch:15 step:12401 [D loss: 0.275072, acc.: 89.84%] [G loss: 2.843762]\n",
      "epoch:15 step:12402 [D loss: 0.342031, acc.: 86.72%] [G loss: 2.721960]\n",
      "epoch:15 step:12403 [D loss: 0.322288, acc.: 89.84%] [G loss: 3.481836]\n",
      "epoch:15 step:12404 [D loss: 0.245887, acc.: 92.19%] [G loss: 2.939250]\n",
      "epoch:15 step:12405 [D loss: 0.378824, acc.: 83.59%] [G loss: 2.194603]\n",
      "epoch:15 step:12406 [D loss: 0.322587, acc.: 87.50%] [G loss: 2.773113]\n",
      "epoch:15 step:12407 [D loss: 0.426249, acc.: 81.25%] [G loss: 3.467388]\n",
      "epoch:15 step:12408 [D loss: 0.358887, acc.: 86.72%] [G loss: 2.534119]\n",
      "epoch:15 step:12409 [D loss: 0.227716, acc.: 91.41%] [G loss: 3.141421]\n",
      "epoch:15 step:12410 [D loss: 0.233211, acc.: 92.19%] [G loss: 3.089720]\n",
      "epoch:15 step:12411 [D loss: 0.360439, acc.: 82.81%] [G loss: 2.987339]\n",
      "epoch:15 step:12412 [D loss: 0.451199, acc.: 84.38%] [G loss: 3.113371]\n",
      "epoch:15 step:12413 [D loss: 0.383128, acc.: 78.12%] [G loss: 2.888389]\n",
      "epoch:15 step:12414 [D loss: 0.272286, acc.: 88.28%] [G loss: 2.491838]\n",
      "epoch:15 step:12415 [D loss: 0.258769, acc.: 87.50%] [G loss: 3.158085]\n",
      "epoch:15 step:12416 [D loss: 0.345545, acc.: 85.16%] [G loss: 3.036120]\n",
      "epoch:15 step:12417 [D loss: 0.273627, acc.: 91.41%] [G loss: 2.572647]\n",
      "epoch:15 step:12418 [D loss: 0.319750, acc.: 86.72%] [G loss: 3.546712]\n",
      "epoch:15 step:12419 [D loss: 0.437240, acc.: 79.69%] [G loss: 5.937903]\n",
      "epoch:15 step:12420 [D loss: 0.787668, acc.: 72.66%] [G loss: 5.539477]\n",
      "epoch:15 step:12421 [D loss: 1.130484, acc.: 67.97%] [G loss: 8.283719]\n",
      "epoch:15 step:12422 [D loss: 2.282379, acc.: 52.34%] [G loss: 3.879774]\n",
      "epoch:15 step:12423 [D loss: 0.508561, acc.: 75.78%] [G loss: 2.801109]\n",
      "epoch:15 step:12424 [D loss: 0.453761, acc.: 82.03%] [G loss: 4.071719]\n",
      "epoch:15 step:12425 [D loss: 0.458023, acc.: 77.34%] [G loss: 2.821834]\n",
      "epoch:15 step:12426 [D loss: 0.396920, acc.: 85.94%] [G loss: 4.934075]\n",
      "epoch:15 step:12427 [D loss: 0.370260, acc.: 82.03%] [G loss: 3.051723]\n",
      "epoch:15 step:12428 [D loss: 0.305084, acc.: 85.94%] [G loss: 3.382825]\n",
      "epoch:15 step:12429 [D loss: 0.372538, acc.: 85.94%] [G loss: 5.200943]\n",
      "epoch:15 step:12430 [D loss: 0.408178, acc.: 85.94%] [G loss: 2.678305]\n",
      "epoch:15 step:12431 [D loss: 0.352154, acc.: 84.38%] [G loss: 4.118802]\n",
      "epoch:15 step:12432 [D loss: 0.350468, acc.: 85.94%] [G loss: 3.263374]\n",
      "epoch:15 step:12433 [D loss: 0.392528, acc.: 82.03%] [G loss: 3.565389]\n",
      "epoch:15 step:12434 [D loss: 0.313136, acc.: 87.50%] [G loss: 2.831172]\n",
      "epoch:15 step:12435 [D loss: 0.399230, acc.: 81.25%] [G loss: 3.112875]\n",
      "epoch:15 step:12436 [D loss: 0.313728, acc.: 85.94%] [G loss: 3.721577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12437 [D loss: 0.255277, acc.: 89.06%] [G loss: 2.471610]\n",
      "epoch:15 step:12438 [D loss: 0.320683, acc.: 85.16%] [G loss: 2.938022]\n",
      "epoch:15 step:12439 [D loss: 0.242457, acc.: 89.84%] [G loss: 2.758219]\n",
      "epoch:15 step:12440 [D loss: 0.344294, acc.: 83.59%] [G loss: 2.409123]\n",
      "epoch:15 step:12441 [D loss: 0.356859, acc.: 89.84%] [G loss: 2.650777]\n",
      "epoch:15 step:12442 [D loss: 0.333089, acc.: 86.72%] [G loss: 3.076937]\n",
      "epoch:15 step:12443 [D loss: 0.390500, acc.: 81.25%] [G loss: 3.226342]\n",
      "epoch:15 step:12444 [D loss: 0.325506, acc.: 84.38%] [G loss: 2.831210]\n",
      "epoch:15 step:12445 [D loss: 0.295259, acc.: 88.28%] [G loss: 3.012868]\n",
      "epoch:15 step:12446 [D loss: 0.353978, acc.: 82.81%] [G loss: 3.148713]\n",
      "epoch:15 step:12447 [D loss: 0.286831, acc.: 86.72%] [G loss: 3.097231]\n",
      "epoch:15 step:12448 [D loss: 0.380291, acc.: 84.38%] [G loss: 2.621155]\n",
      "epoch:15 step:12449 [D loss: 0.392315, acc.: 85.94%] [G loss: 2.659164]\n",
      "epoch:15 step:12450 [D loss: 0.339257, acc.: 86.72%] [G loss: 2.335584]\n",
      "epoch:15 step:12451 [D loss: 0.317212, acc.: 85.16%] [G loss: 3.322213]\n",
      "epoch:15 step:12452 [D loss: 0.405028, acc.: 81.25%] [G loss: 2.395699]\n",
      "epoch:15 step:12453 [D loss: 0.310843, acc.: 85.16%] [G loss: 3.807274]\n",
      "epoch:15 step:12454 [D loss: 0.359286, acc.: 81.25%] [G loss: 3.340208]\n",
      "epoch:15 step:12455 [D loss: 0.299025, acc.: 86.72%] [G loss: 3.864097]\n",
      "epoch:15 step:12456 [D loss: 0.309127, acc.: 87.50%] [G loss: 3.452153]\n",
      "epoch:15 step:12457 [D loss: 0.306922, acc.: 84.38%] [G loss: 5.442664]\n",
      "epoch:15 step:12458 [D loss: 0.253620, acc.: 92.97%] [G loss: 6.016285]\n",
      "epoch:15 step:12459 [D loss: 0.247429, acc.: 89.84%] [G loss: 3.483755]\n",
      "epoch:15 step:12460 [D loss: 0.235114, acc.: 91.41%] [G loss: 3.563358]\n",
      "epoch:15 step:12461 [D loss: 0.247844, acc.: 90.62%] [G loss: 2.297577]\n",
      "epoch:15 step:12462 [D loss: 0.341437, acc.: 87.50%] [G loss: 3.465252]\n",
      "epoch:15 step:12463 [D loss: 0.243387, acc.: 90.62%] [G loss: 5.196064]\n",
      "epoch:15 step:12464 [D loss: 0.328908, acc.: 87.50%] [G loss: 4.111631]\n",
      "epoch:15 step:12465 [D loss: 0.335068, acc.: 85.16%] [G loss: 4.461370]\n",
      "epoch:15 step:12466 [D loss: 0.268085, acc.: 88.28%] [G loss: 3.141336]\n",
      "epoch:15 step:12467 [D loss: 0.391095, acc.: 80.47%] [G loss: 2.647874]\n",
      "epoch:15 step:12468 [D loss: 0.263717, acc.: 89.06%] [G loss: 2.591570]\n",
      "epoch:15 step:12469 [D loss: 0.377873, acc.: 82.81%] [G loss: 2.187733]\n",
      "epoch:15 step:12470 [D loss: 0.266831, acc.: 89.06%] [G loss: 2.538623]\n",
      "epoch:15 step:12471 [D loss: 0.416998, acc.: 82.03%] [G loss: 2.609430]\n",
      "epoch:15 step:12472 [D loss: 0.361301, acc.: 87.50%] [G loss: 2.694867]\n",
      "epoch:15 step:12473 [D loss: 0.376313, acc.: 82.03%] [G loss: 2.070423]\n",
      "epoch:15 step:12474 [D loss: 0.366552, acc.: 84.38%] [G loss: 2.235244]\n",
      "epoch:15 step:12475 [D loss: 0.352698, acc.: 82.03%] [G loss: 2.376165]\n",
      "epoch:15 step:12476 [D loss: 0.254968, acc.: 89.84%] [G loss: 2.797617]\n",
      "epoch:15 step:12477 [D loss: 0.337888, acc.: 84.38%] [G loss: 2.560283]\n",
      "epoch:15 step:12478 [D loss: 0.337454, acc.: 85.94%] [G loss: 2.771404]\n",
      "epoch:15 step:12479 [D loss: 0.281845, acc.: 85.94%] [G loss: 4.052423]\n",
      "epoch:15 step:12480 [D loss: 0.532411, acc.: 77.34%] [G loss: 3.368575]\n",
      "epoch:15 step:12481 [D loss: 0.381379, acc.: 83.59%] [G loss: 2.363369]\n",
      "epoch:15 step:12482 [D loss: 0.286912, acc.: 86.72%] [G loss: 3.768310]\n",
      "epoch:15 step:12483 [D loss: 0.375480, acc.: 83.59%] [G loss: 3.363330]\n",
      "epoch:15 step:12484 [D loss: 0.272533, acc.: 87.50%] [G loss: 2.966087]\n",
      "epoch:15 step:12485 [D loss: 0.365683, acc.: 83.59%] [G loss: 2.283432]\n",
      "epoch:15 step:12486 [D loss: 0.316707, acc.: 88.28%] [G loss: 2.768758]\n",
      "epoch:15 step:12487 [D loss: 0.337139, acc.: 85.16%] [G loss: 1.992886]\n",
      "epoch:15 step:12488 [D loss: 0.284799, acc.: 89.06%] [G loss: 3.046119]\n",
      "epoch:15 step:12489 [D loss: 0.292598, acc.: 85.94%] [G loss: 2.415637]\n",
      "epoch:15 step:12490 [D loss: 0.388080, acc.: 81.25%] [G loss: 3.896924]\n",
      "epoch:15 step:12491 [D loss: 0.291633, acc.: 89.06%] [G loss: 3.079803]\n",
      "epoch:15 step:12492 [D loss: 0.451905, acc.: 75.00%] [G loss: 2.335632]\n",
      "epoch:15 step:12493 [D loss: 0.326153, acc.: 84.38%] [G loss: 2.427798]\n",
      "epoch:15 step:12494 [D loss: 0.385156, acc.: 82.03%] [G loss: 2.785589]\n",
      "epoch:15 step:12495 [D loss: 0.306113, acc.: 85.94%] [G loss: 3.651745]\n",
      "epoch:15 step:12496 [D loss: 0.251666, acc.: 89.84%] [G loss: 4.463091]\n",
      "epoch:16 step:12497 [D loss: 0.413584, acc.: 78.91%] [G loss: 2.626453]\n",
      "epoch:16 step:12498 [D loss: 0.232689, acc.: 90.62%] [G loss: 2.626854]\n",
      "epoch:16 step:12499 [D loss: 0.362062, acc.: 86.72%] [G loss: 5.133396]\n",
      "epoch:16 step:12500 [D loss: 0.294861, acc.: 83.59%] [G loss: 3.529586]\n",
      "epoch:16 step:12501 [D loss: 0.268836, acc.: 89.84%] [G loss: 3.132835]\n",
      "epoch:16 step:12502 [D loss: 0.240891, acc.: 89.84%] [G loss: 4.082314]\n",
      "epoch:16 step:12503 [D loss: 0.248217, acc.: 87.50%] [G loss: 3.409998]\n",
      "epoch:16 step:12504 [D loss: 0.211479, acc.: 92.97%] [G loss: 2.514019]\n",
      "epoch:16 step:12505 [D loss: 0.342546, acc.: 85.94%] [G loss: 2.235903]\n",
      "epoch:16 step:12506 [D loss: 0.205786, acc.: 90.62%] [G loss: 5.319362]\n",
      "epoch:16 step:12507 [D loss: 0.308969, acc.: 88.28%] [G loss: 3.247502]\n",
      "epoch:16 step:12508 [D loss: 0.440987, acc.: 78.12%] [G loss: 2.239995]\n",
      "epoch:16 step:12509 [D loss: 0.275963, acc.: 86.72%] [G loss: 3.881784]\n",
      "epoch:16 step:12510 [D loss: 0.455808, acc.: 82.03%] [G loss: 3.780503]\n",
      "epoch:16 step:12511 [D loss: 0.561396, acc.: 72.66%] [G loss: 3.804901]\n",
      "epoch:16 step:12512 [D loss: 0.664963, acc.: 68.75%] [G loss: 3.739203]\n",
      "epoch:16 step:12513 [D loss: 0.306711, acc.: 89.06%] [G loss: 3.713326]\n",
      "epoch:16 step:12514 [D loss: 0.473128, acc.: 81.25%] [G loss: 2.971149]\n",
      "epoch:16 step:12515 [D loss: 0.369411, acc.: 82.81%] [G loss: 3.688163]\n",
      "epoch:16 step:12516 [D loss: 0.421327, acc.: 80.47%] [G loss: 2.599272]\n",
      "epoch:16 step:12517 [D loss: 0.289484, acc.: 89.84%] [G loss: 3.491492]\n",
      "epoch:16 step:12518 [D loss: 0.293510, acc.: 85.16%] [G loss: 3.375250]\n",
      "epoch:16 step:12519 [D loss: 0.228779, acc.: 89.06%] [G loss: 4.567595]\n",
      "epoch:16 step:12520 [D loss: 0.340349, acc.: 82.03%] [G loss: 2.929670]\n",
      "epoch:16 step:12521 [D loss: 0.306682, acc.: 87.50%] [G loss: 4.013152]\n",
      "epoch:16 step:12522 [D loss: 0.368275, acc.: 80.47%] [G loss: 2.983766]\n",
      "epoch:16 step:12523 [D loss: 0.326094, acc.: 85.94%] [G loss: 2.918053]\n",
      "epoch:16 step:12524 [D loss: 0.422727, acc.: 77.34%] [G loss: 2.387589]\n",
      "epoch:16 step:12525 [D loss: 0.460063, acc.: 81.25%] [G loss: 2.820400]\n",
      "epoch:16 step:12526 [D loss: 0.340313, acc.: 85.16%] [G loss: 3.436749]\n",
      "epoch:16 step:12527 [D loss: 0.291077, acc.: 88.28%] [G loss: 4.146053]\n",
      "epoch:16 step:12528 [D loss: 0.367691, acc.: 85.16%] [G loss: 3.355736]\n",
      "epoch:16 step:12529 [D loss: 0.430840, acc.: 82.03%] [G loss: 2.634800]\n",
      "epoch:16 step:12530 [D loss: 0.286808, acc.: 88.28%] [G loss: 2.822756]\n",
      "epoch:16 step:12531 [D loss: 0.333423, acc.: 88.28%] [G loss: 2.183524]\n",
      "epoch:16 step:12532 [D loss: 0.317504, acc.: 84.38%] [G loss: 3.554392]\n",
      "epoch:16 step:12533 [D loss: 0.413049, acc.: 79.69%] [G loss: 2.983875]\n",
      "epoch:16 step:12534 [D loss: 0.447021, acc.: 79.69%] [G loss: 2.568181]\n",
      "epoch:16 step:12535 [D loss: 0.378899, acc.: 82.81%] [G loss: 2.937538]\n",
      "epoch:16 step:12536 [D loss: 0.340028, acc.: 91.41%] [G loss: 2.755692]\n",
      "epoch:16 step:12537 [D loss: 0.362733, acc.: 84.38%] [G loss: 3.680468]\n",
      "epoch:16 step:12538 [D loss: 0.328983, acc.: 84.38%] [G loss: 3.899826]\n",
      "epoch:16 step:12539 [D loss: 0.322199, acc.: 86.72%] [G loss: 4.330311]\n",
      "epoch:16 step:12540 [D loss: 0.290499, acc.: 85.94%] [G loss: 2.899687]\n",
      "epoch:16 step:12541 [D loss: 0.267212, acc.: 91.41%] [G loss: 2.653679]\n",
      "epoch:16 step:12542 [D loss: 0.369692, acc.: 82.03%] [G loss: 2.610209]\n",
      "epoch:16 step:12543 [D loss: 0.262387, acc.: 90.62%] [G loss: 3.683093]\n",
      "epoch:16 step:12544 [D loss: 0.281732, acc.: 89.06%] [G loss: 3.643629]\n",
      "epoch:16 step:12545 [D loss: 0.313770, acc.: 85.94%] [G loss: 3.074740]\n",
      "epoch:16 step:12546 [D loss: 0.401506, acc.: 83.59%] [G loss: 2.460650]\n",
      "epoch:16 step:12547 [D loss: 0.228826, acc.: 87.50%] [G loss: 5.371131]\n",
      "epoch:16 step:12548 [D loss: 0.272497, acc.: 89.84%] [G loss: 2.439396]\n",
      "epoch:16 step:12549 [D loss: 0.297976, acc.: 86.72%] [G loss: 3.614003]\n",
      "epoch:16 step:12550 [D loss: 0.320771, acc.: 85.94%] [G loss: 3.492735]\n",
      "epoch:16 step:12551 [D loss: 0.234460, acc.: 90.62%] [G loss: 3.772969]\n",
      "epoch:16 step:12552 [D loss: 0.395824, acc.: 79.69%] [G loss: 3.315015]\n",
      "epoch:16 step:12553 [D loss: 0.301327, acc.: 82.81%] [G loss: 2.831632]\n",
      "epoch:16 step:12554 [D loss: 0.310731, acc.: 87.50%] [G loss: 2.691217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12555 [D loss: 0.253340, acc.: 88.28%] [G loss: 2.458777]\n",
      "epoch:16 step:12556 [D loss: 0.310917, acc.: 86.72%] [G loss: 2.639426]\n",
      "epoch:16 step:12557 [D loss: 0.312851, acc.: 85.94%] [G loss: 2.368295]\n",
      "epoch:16 step:12558 [D loss: 0.340531, acc.: 86.72%] [G loss: 2.342645]\n",
      "epoch:16 step:12559 [D loss: 0.370913, acc.: 79.69%] [G loss: 3.113517]\n",
      "epoch:16 step:12560 [D loss: 0.315927, acc.: 83.59%] [G loss: 3.034019]\n",
      "epoch:16 step:12561 [D loss: 0.393119, acc.: 79.69%] [G loss: 4.013480]\n",
      "epoch:16 step:12562 [D loss: 0.322034, acc.: 85.16%] [G loss: 3.630429]\n",
      "epoch:16 step:12563 [D loss: 0.329306, acc.: 85.16%] [G loss: 3.383388]\n",
      "epoch:16 step:12564 [D loss: 0.274803, acc.: 87.50%] [G loss: 5.803041]\n",
      "epoch:16 step:12565 [D loss: 0.319652, acc.: 89.84%] [G loss: 3.690259]\n",
      "epoch:16 step:12566 [D loss: 0.294532, acc.: 88.28%] [G loss: 3.351568]\n",
      "epoch:16 step:12567 [D loss: 0.303805, acc.: 89.84%] [G loss: 2.920229]\n",
      "epoch:16 step:12568 [D loss: 0.285809, acc.: 86.72%] [G loss: 3.331676]\n",
      "epoch:16 step:12569 [D loss: 0.407597, acc.: 84.38%] [G loss: 3.690902]\n",
      "epoch:16 step:12570 [D loss: 0.506999, acc.: 77.34%] [G loss: 3.678168]\n",
      "epoch:16 step:12571 [D loss: 0.638224, acc.: 75.78%] [G loss: 4.936414]\n",
      "epoch:16 step:12572 [D loss: 0.783329, acc.: 67.97%] [G loss: 5.736028]\n",
      "epoch:16 step:12573 [D loss: 1.312062, acc.: 63.28%] [G loss: 7.846808]\n",
      "epoch:16 step:12574 [D loss: 1.928898, acc.: 53.91%] [G loss: 3.336090]\n",
      "epoch:16 step:12575 [D loss: 0.738299, acc.: 67.97%] [G loss: 2.500141]\n",
      "epoch:16 step:12576 [D loss: 0.381885, acc.: 85.94%] [G loss: 3.532340]\n",
      "epoch:16 step:12577 [D loss: 0.516082, acc.: 82.03%] [G loss: 3.408521]\n",
      "epoch:16 step:12578 [D loss: 0.458354, acc.: 83.59%] [G loss: 3.373353]\n",
      "epoch:16 step:12579 [D loss: 0.349349, acc.: 85.94%] [G loss: 3.095488]\n",
      "epoch:16 step:12580 [D loss: 0.310592, acc.: 86.72%] [G loss: 4.024083]\n",
      "epoch:16 step:12581 [D loss: 0.371018, acc.: 81.25%] [G loss: 3.496098]\n",
      "epoch:16 step:12582 [D loss: 0.324147, acc.: 81.25%] [G loss: 2.464425]\n",
      "epoch:16 step:12583 [D loss: 0.271259, acc.: 89.84%] [G loss: 2.817044]\n",
      "epoch:16 step:12584 [D loss: 0.328064, acc.: 85.94%] [G loss: 3.417766]\n",
      "epoch:16 step:12585 [D loss: 0.319601, acc.: 82.81%] [G loss: 5.067883]\n",
      "epoch:16 step:12586 [D loss: 0.382555, acc.: 75.00%] [G loss: 3.413098]\n",
      "epoch:16 step:12587 [D loss: 0.254514, acc.: 88.28%] [G loss: 2.551589]\n",
      "epoch:16 step:12588 [D loss: 0.271800, acc.: 89.84%] [G loss: 2.696960]\n",
      "epoch:16 step:12589 [D loss: 0.293858, acc.: 86.72%] [G loss: 2.522544]\n",
      "epoch:16 step:12590 [D loss: 0.400639, acc.: 86.72%] [G loss: 2.388120]\n",
      "epoch:16 step:12591 [D loss: 0.346834, acc.: 85.94%] [G loss: 2.454128]\n",
      "epoch:16 step:12592 [D loss: 0.289352, acc.: 89.06%] [G loss: 4.191157]\n",
      "epoch:16 step:12593 [D loss: 0.287507, acc.: 88.28%] [G loss: 5.284560]\n",
      "epoch:16 step:12594 [D loss: 0.398845, acc.: 82.03%] [G loss: 2.979265]\n",
      "epoch:16 step:12595 [D loss: 0.204277, acc.: 92.19%] [G loss: 6.342326]\n",
      "epoch:16 step:12596 [D loss: 0.294075, acc.: 82.81%] [G loss: 5.503017]\n",
      "epoch:16 step:12597 [D loss: 0.315321, acc.: 82.81%] [G loss: 4.697216]\n",
      "epoch:16 step:12598 [D loss: 0.238014, acc.: 89.84%] [G loss: 3.383838]\n",
      "epoch:16 step:12599 [D loss: 0.455474, acc.: 78.12%] [G loss: 3.664438]\n",
      "epoch:16 step:12600 [D loss: 0.327110, acc.: 85.16%] [G loss: 2.625321]\n",
      "epoch:16 step:12601 [D loss: 0.353666, acc.: 83.59%] [G loss: 2.634634]\n",
      "epoch:16 step:12602 [D loss: 0.322475, acc.: 84.38%] [G loss: 2.405920]\n",
      "epoch:16 step:12603 [D loss: 0.328799, acc.: 85.94%] [G loss: 2.177705]\n",
      "epoch:16 step:12604 [D loss: 0.354294, acc.: 82.03%] [G loss: 2.488181]\n",
      "epoch:16 step:12605 [D loss: 0.463688, acc.: 79.69%] [G loss: 2.704797]\n",
      "epoch:16 step:12606 [D loss: 0.330587, acc.: 85.94%] [G loss: 2.922362]\n",
      "epoch:16 step:12607 [D loss: 0.413106, acc.: 82.03%] [G loss: 2.381536]\n",
      "epoch:16 step:12608 [D loss: 0.433129, acc.: 79.69%] [G loss: 2.486513]\n",
      "epoch:16 step:12609 [D loss: 0.303217, acc.: 88.28%] [G loss: 3.042605]\n",
      "epoch:16 step:12610 [D loss: 0.368971, acc.: 85.94%] [G loss: 2.392233]\n",
      "epoch:16 step:12611 [D loss: 0.306850, acc.: 87.50%] [G loss: 2.389221]\n",
      "epoch:16 step:12612 [D loss: 0.373926, acc.: 79.69%] [G loss: 2.692272]\n",
      "epoch:16 step:12613 [D loss: 0.382233, acc.: 83.59%] [G loss: 3.119703]\n",
      "epoch:16 step:12614 [D loss: 0.316444, acc.: 88.28%] [G loss: 3.151458]\n",
      "epoch:16 step:12615 [D loss: 0.315103, acc.: 85.16%] [G loss: 2.308221]\n",
      "epoch:16 step:12616 [D loss: 0.345748, acc.: 82.03%] [G loss: 3.579484]\n",
      "epoch:16 step:12617 [D loss: 0.304556, acc.: 88.28%] [G loss: 2.162396]\n",
      "epoch:16 step:12618 [D loss: 0.357539, acc.: 82.81%] [G loss: 3.550519]\n",
      "epoch:16 step:12619 [D loss: 0.342920, acc.: 86.72%] [G loss: 2.627274]\n",
      "epoch:16 step:12620 [D loss: 0.451674, acc.: 78.12%] [G loss: 2.715448]\n",
      "epoch:16 step:12621 [D loss: 0.387679, acc.: 82.81%] [G loss: 2.527466]\n",
      "epoch:16 step:12622 [D loss: 0.319508, acc.: 88.28%] [G loss: 2.302881]\n",
      "epoch:16 step:12623 [D loss: 0.254655, acc.: 91.41%] [G loss: 2.501653]\n",
      "epoch:16 step:12624 [D loss: 0.274090, acc.: 88.28%] [G loss: 2.985561]\n",
      "epoch:16 step:12625 [D loss: 0.327792, acc.: 85.16%] [G loss: 2.572062]\n",
      "epoch:16 step:12626 [D loss: 0.476623, acc.: 78.12%] [G loss: 2.406508]\n",
      "epoch:16 step:12627 [D loss: 0.403568, acc.: 85.16%] [G loss: 2.350686]\n",
      "epoch:16 step:12628 [D loss: 0.264472, acc.: 88.28%] [G loss: 2.973801]\n",
      "epoch:16 step:12629 [D loss: 0.282104, acc.: 85.94%] [G loss: 2.528134]\n",
      "epoch:16 step:12630 [D loss: 0.285858, acc.: 86.72%] [G loss: 3.161704]\n",
      "epoch:16 step:12631 [D loss: 0.293769, acc.: 86.72%] [G loss: 3.216139]\n",
      "epoch:16 step:12632 [D loss: 0.337152, acc.: 85.16%] [G loss: 3.131840]\n",
      "epoch:16 step:12633 [D loss: 0.210661, acc.: 90.62%] [G loss: 3.677976]\n",
      "epoch:16 step:12634 [D loss: 0.379334, acc.: 89.06%] [G loss: 2.595391]\n",
      "epoch:16 step:12635 [D loss: 0.371469, acc.: 80.47%] [G loss: 2.632042]\n",
      "epoch:16 step:12636 [D loss: 0.361325, acc.: 82.81%] [G loss: 3.051333]\n",
      "epoch:16 step:12637 [D loss: 0.315071, acc.: 86.72%] [G loss: 2.980012]\n",
      "epoch:16 step:12638 [D loss: 0.442426, acc.: 78.91%] [G loss: 2.437264]\n",
      "epoch:16 step:12639 [D loss: 0.352020, acc.: 85.94%] [G loss: 2.751983]\n",
      "epoch:16 step:12640 [D loss: 0.386171, acc.: 78.12%] [G loss: 3.614510]\n",
      "epoch:16 step:12641 [D loss: 0.410331, acc.: 79.69%] [G loss: 4.057393]\n",
      "epoch:16 step:12642 [D loss: 0.471827, acc.: 80.47%] [G loss: 2.603738]\n",
      "epoch:16 step:12643 [D loss: 0.292152, acc.: 87.50%] [G loss: 3.820848]\n",
      "epoch:16 step:12644 [D loss: 0.459781, acc.: 75.00%] [G loss: 5.310949]\n",
      "epoch:16 step:12645 [D loss: 0.357867, acc.: 82.03%] [G loss: 3.952108]\n",
      "epoch:16 step:12646 [D loss: 0.252827, acc.: 90.62%] [G loss: 4.223292]\n",
      "epoch:16 step:12647 [D loss: 0.252066, acc.: 91.41%] [G loss: 4.506269]\n",
      "epoch:16 step:12648 [D loss: 0.343444, acc.: 87.50%] [G loss: 2.705418]\n",
      "epoch:16 step:12649 [D loss: 0.380665, acc.: 83.59%] [G loss: 2.452151]\n",
      "epoch:16 step:12650 [D loss: 0.401667, acc.: 82.81%] [G loss: 2.519278]\n",
      "epoch:16 step:12651 [D loss: 0.436035, acc.: 82.81%] [G loss: 1.846400]\n",
      "epoch:16 step:12652 [D loss: 0.294608, acc.: 90.62%] [G loss: 2.530902]\n",
      "epoch:16 step:12653 [D loss: 0.372704, acc.: 85.94%] [G loss: 3.512564]\n",
      "epoch:16 step:12654 [D loss: 0.445988, acc.: 76.56%] [G loss: 2.007821]\n",
      "epoch:16 step:12655 [D loss: 0.358601, acc.: 85.94%] [G loss: 3.704609]\n",
      "epoch:16 step:12656 [D loss: 0.495102, acc.: 73.44%] [G loss: 3.793263]\n",
      "epoch:16 step:12657 [D loss: 0.474518, acc.: 77.34%] [G loss: 3.403205]\n",
      "epoch:16 step:12658 [D loss: 0.384216, acc.: 83.59%] [G loss: 4.381654]\n",
      "epoch:16 step:12659 [D loss: 0.345872, acc.: 82.81%] [G loss: 2.465505]\n",
      "epoch:16 step:12660 [D loss: 0.269473, acc.: 86.72%] [G loss: 2.613286]\n",
      "epoch:16 step:12661 [D loss: 0.347738, acc.: 83.59%] [G loss: 3.220744]\n",
      "epoch:16 step:12662 [D loss: 0.396408, acc.: 82.81%] [G loss: 3.070986]\n",
      "epoch:16 step:12663 [D loss: 0.370736, acc.: 83.59%] [G loss: 3.233324]\n",
      "epoch:16 step:12664 [D loss: 0.512369, acc.: 75.00%] [G loss: 3.537460]\n",
      "epoch:16 step:12665 [D loss: 0.509709, acc.: 77.34%] [G loss: 3.262262]\n",
      "epoch:16 step:12666 [D loss: 0.259482, acc.: 89.06%] [G loss: 3.066584]\n",
      "epoch:16 step:12667 [D loss: 0.352499, acc.: 81.25%] [G loss: 6.031679]\n",
      "epoch:16 step:12668 [D loss: 0.340332, acc.: 87.50%] [G loss: 4.155594]\n",
      "epoch:16 step:12669 [D loss: 0.305874, acc.: 84.38%] [G loss: 4.038237]\n",
      "epoch:16 step:12670 [D loss: 0.349145, acc.: 85.94%] [G loss: 2.985960]\n",
      "epoch:16 step:12671 [D loss: 0.291004, acc.: 89.06%] [G loss: 2.682737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12672 [D loss: 0.476163, acc.: 80.47%] [G loss: 3.158093]\n",
      "epoch:16 step:12673 [D loss: 0.559687, acc.: 74.22%] [G loss: 3.057280]\n",
      "epoch:16 step:12674 [D loss: 0.217936, acc.: 91.41%] [G loss: 5.427061]\n",
      "epoch:16 step:12675 [D loss: 0.327520, acc.: 84.38%] [G loss: 2.995860]\n",
      "epoch:16 step:12676 [D loss: 0.416456, acc.: 85.94%] [G loss: 2.839282]\n",
      "epoch:16 step:12677 [D loss: 0.276570, acc.: 88.28%] [G loss: 6.598986]\n",
      "epoch:16 step:12678 [D loss: 0.265777, acc.: 85.94%] [G loss: 5.031512]\n",
      "epoch:16 step:12679 [D loss: 0.284625, acc.: 89.06%] [G loss: 3.336497]\n",
      "epoch:16 step:12680 [D loss: 0.263265, acc.: 89.84%] [G loss: 4.213500]\n",
      "epoch:16 step:12681 [D loss: 0.401347, acc.: 82.81%] [G loss: 2.770536]\n",
      "epoch:16 step:12682 [D loss: 0.278425, acc.: 90.62%] [G loss: 3.935062]\n",
      "epoch:16 step:12683 [D loss: 0.343407, acc.: 85.16%] [G loss: 2.529823]\n",
      "epoch:16 step:12684 [D loss: 0.269806, acc.: 89.06%] [G loss: 2.982503]\n",
      "epoch:16 step:12685 [D loss: 0.409340, acc.: 79.69%] [G loss: 2.846253]\n",
      "epoch:16 step:12686 [D loss: 0.338678, acc.: 85.94%] [G loss: 3.592548]\n",
      "epoch:16 step:12687 [D loss: 0.374667, acc.: 84.38%] [G loss: 3.388713]\n",
      "epoch:16 step:12688 [D loss: 0.361437, acc.: 83.59%] [G loss: 2.508075]\n",
      "epoch:16 step:12689 [D loss: 0.292739, acc.: 86.72%] [G loss: 3.827894]\n",
      "epoch:16 step:12690 [D loss: 0.357970, acc.: 81.25%] [G loss: 2.313838]\n",
      "epoch:16 step:12691 [D loss: 0.295461, acc.: 89.06%] [G loss: 3.314039]\n",
      "epoch:16 step:12692 [D loss: 0.261483, acc.: 88.28%] [G loss: 2.944919]\n",
      "epoch:16 step:12693 [D loss: 0.358704, acc.: 84.38%] [G loss: 2.882527]\n",
      "epoch:16 step:12694 [D loss: 0.386915, acc.: 80.47%] [G loss: 2.390419]\n",
      "epoch:16 step:12695 [D loss: 0.463199, acc.: 81.25%] [G loss: 2.753174]\n",
      "epoch:16 step:12696 [D loss: 0.315631, acc.: 85.16%] [G loss: 2.540834]\n",
      "epoch:16 step:12697 [D loss: 0.368487, acc.: 87.50%] [G loss: 3.287807]\n",
      "epoch:16 step:12698 [D loss: 0.406029, acc.: 82.03%] [G loss: 4.172402]\n",
      "epoch:16 step:12699 [D loss: 0.308255, acc.: 85.94%] [G loss: 2.907795]\n",
      "epoch:16 step:12700 [D loss: 0.384704, acc.: 85.16%] [G loss: 5.704913]\n",
      "epoch:16 step:12701 [D loss: 0.381244, acc.: 82.81%] [G loss: 4.185374]\n",
      "epoch:16 step:12702 [D loss: 0.728324, acc.: 71.09%] [G loss: 3.394370]\n",
      "epoch:16 step:12703 [D loss: 0.505226, acc.: 78.12%] [G loss: 4.505889]\n",
      "epoch:16 step:12704 [D loss: 0.346419, acc.: 85.16%] [G loss: 3.532449]\n",
      "epoch:16 step:12705 [D loss: 0.319659, acc.: 89.06%] [G loss: 2.402816]\n",
      "epoch:16 step:12706 [D loss: 0.429515, acc.: 81.25%] [G loss: 3.236446]\n",
      "epoch:16 step:12707 [D loss: 0.370297, acc.: 81.25%] [G loss: 2.578948]\n",
      "epoch:16 step:12708 [D loss: 0.369371, acc.: 85.16%] [G loss: 2.366870]\n",
      "epoch:16 step:12709 [D loss: 0.325468, acc.: 88.28%] [G loss: 2.891710]\n",
      "epoch:16 step:12710 [D loss: 0.400155, acc.: 79.69%] [G loss: 2.511846]\n",
      "epoch:16 step:12711 [D loss: 0.455872, acc.: 80.47%] [G loss: 3.051048]\n",
      "epoch:16 step:12712 [D loss: 0.347059, acc.: 81.25%] [G loss: 2.997603]\n",
      "epoch:16 step:12713 [D loss: 0.336925, acc.: 86.72%] [G loss: 2.114545]\n",
      "epoch:16 step:12714 [D loss: 0.451216, acc.: 83.59%] [G loss: 2.796423]\n",
      "epoch:16 step:12715 [D loss: 0.509319, acc.: 82.03%] [G loss: 3.182643]\n",
      "epoch:16 step:12716 [D loss: 0.466993, acc.: 78.91%] [G loss: 2.440506]\n",
      "epoch:16 step:12717 [D loss: 0.400803, acc.: 78.91%] [G loss: 2.618544]\n",
      "epoch:16 step:12718 [D loss: 0.312381, acc.: 84.38%] [G loss: 2.207703]\n",
      "epoch:16 step:12719 [D loss: 0.357143, acc.: 85.94%] [G loss: 2.969171]\n",
      "epoch:16 step:12720 [D loss: 0.250029, acc.: 90.62%] [G loss: 3.934605]\n",
      "epoch:16 step:12721 [D loss: 0.421856, acc.: 80.47%] [G loss: 4.666248]\n",
      "epoch:16 step:12722 [D loss: 0.282368, acc.: 89.06%] [G loss: 2.791625]\n",
      "epoch:16 step:12723 [D loss: 0.261583, acc.: 89.06%] [G loss: 3.269852]\n",
      "epoch:16 step:12724 [D loss: 0.363312, acc.: 84.38%] [G loss: 8.930820]\n",
      "epoch:16 step:12725 [D loss: 0.606095, acc.: 71.88%] [G loss: 3.815596]\n",
      "epoch:16 step:12726 [D loss: 0.461778, acc.: 77.34%] [G loss: 3.093026]\n",
      "epoch:16 step:12727 [D loss: 0.306540, acc.: 85.94%] [G loss: 3.155249]\n",
      "epoch:16 step:12728 [D loss: 0.411586, acc.: 86.72%] [G loss: 2.478295]\n",
      "epoch:16 step:12729 [D loss: 0.379543, acc.: 82.81%] [G loss: 2.725582]\n",
      "epoch:16 step:12730 [D loss: 0.359756, acc.: 84.38%] [G loss: 2.266792]\n",
      "epoch:16 step:12731 [D loss: 0.381791, acc.: 87.50%] [G loss: 2.650193]\n",
      "epoch:16 step:12732 [D loss: 0.393783, acc.: 86.72%] [G loss: 2.798468]\n",
      "epoch:16 step:12733 [D loss: 0.482347, acc.: 78.91%] [G loss: 1.952935]\n",
      "epoch:16 step:12734 [D loss: 0.366112, acc.: 87.50%] [G loss: 2.690371]\n",
      "epoch:16 step:12735 [D loss: 0.489641, acc.: 78.12%] [G loss: 2.982025]\n",
      "epoch:16 step:12736 [D loss: 0.382092, acc.: 84.38%] [G loss: 2.369723]\n",
      "epoch:16 step:12737 [D loss: 0.275083, acc.: 89.06%] [G loss: 2.397653]\n",
      "epoch:16 step:12738 [D loss: 0.356174, acc.: 85.16%] [G loss: 2.375456]\n",
      "epoch:16 step:12739 [D loss: 0.303234, acc.: 85.94%] [G loss: 2.555161]\n",
      "epoch:16 step:12740 [D loss: 0.242476, acc.: 92.19%] [G loss: 2.618677]\n",
      "epoch:16 step:12741 [D loss: 0.255106, acc.: 91.41%] [G loss: 3.402020]\n",
      "epoch:16 step:12742 [D loss: 0.327035, acc.: 85.94%] [G loss: 3.056021]\n",
      "epoch:16 step:12743 [D loss: 0.329389, acc.: 85.16%] [G loss: 2.512483]\n",
      "epoch:16 step:12744 [D loss: 0.374906, acc.: 85.16%] [G loss: 2.912064]\n",
      "epoch:16 step:12745 [D loss: 0.333051, acc.: 82.81%] [G loss: 3.132168]\n",
      "epoch:16 step:12746 [D loss: 0.296892, acc.: 82.03%] [G loss: 3.453257]\n",
      "epoch:16 step:12747 [D loss: 0.317349, acc.: 87.50%] [G loss: 2.209486]\n",
      "epoch:16 step:12748 [D loss: 0.289987, acc.: 88.28%] [G loss: 4.904298]\n",
      "epoch:16 step:12749 [D loss: 0.275757, acc.: 89.84%] [G loss: 2.791878]\n",
      "epoch:16 step:12750 [D loss: 0.234434, acc.: 87.50%] [G loss: 3.521907]\n",
      "epoch:16 step:12751 [D loss: 0.219018, acc.: 89.84%] [G loss: 3.403667]\n",
      "epoch:16 step:12752 [D loss: 0.435737, acc.: 81.25%] [G loss: 2.821638]\n",
      "epoch:16 step:12753 [D loss: 0.384198, acc.: 81.25%] [G loss: 3.053306]\n",
      "epoch:16 step:12754 [D loss: 0.322459, acc.: 82.81%] [G loss: 2.665042]\n",
      "epoch:16 step:12755 [D loss: 0.336126, acc.: 84.38%] [G loss: 2.581728]\n",
      "epoch:16 step:12756 [D loss: 0.308853, acc.: 89.84%] [G loss: 2.612632]\n",
      "epoch:16 step:12757 [D loss: 0.368603, acc.: 84.38%] [G loss: 2.964803]\n",
      "epoch:16 step:12758 [D loss: 0.394703, acc.: 81.25%] [G loss: 2.372865]\n",
      "epoch:16 step:12759 [D loss: 0.437198, acc.: 78.91%] [G loss: 2.482293]\n",
      "epoch:16 step:12760 [D loss: 0.427765, acc.: 82.03%] [G loss: 2.761914]\n",
      "epoch:16 step:12761 [D loss: 0.424249, acc.: 80.47%] [G loss: 2.784197]\n",
      "epoch:16 step:12762 [D loss: 0.296355, acc.: 86.72%] [G loss: 2.507957]\n",
      "epoch:16 step:12763 [D loss: 0.441031, acc.: 80.47%] [G loss: 3.519375]\n",
      "epoch:16 step:12764 [D loss: 0.313605, acc.: 85.94%] [G loss: 2.930363]\n",
      "epoch:16 step:12765 [D loss: 0.318409, acc.: 89.06%] [G loss: 3.669023]\n",
      "epoch:16 step:12766 [D loss: 0.229587, acc.: 90.62%] [G loss: 3.884296]\n",
      "epoch:16 step:12767 [D loss: 0.244933, acc.: 90.62%] [G loss: 3.493719]\n",
      "epoch:16 step:12768 [D loss: 0.268874, acc.: 89.84%] [G loss: 3.804517]\n",
      "epoch:16 step:12769 [D loss: 0.374517, acc.: 82.03%] [G loss: 2.972713]\n",
      "epoch:16 step:12770 [D loss: 0.356357, acc.: 83.59%] [G loss: 3.865613]\n",
      "epoch:16 step:12771 [D loss: 0.499532, acc.: 72.66%] [G loss: 2.724859]\n",
      "epoch:16 step:12772 [D loss: 0.354370, acc.: 83.59%] [G loss: 3.714886]\n",
      "epoch:16 step:12773 [D loss: 0.249727, acc.: 88.28%] [G loss: 6.719436]\n",
      "epoch:16 step:12774 [D loss: 0.381063, acc.: 83.59%] [G loss: 2.904656]\n",
      "epoch:16 step:12775 [D loss: 0.288171, acc.: 87.50%] [G loss: 5.315094]\n",
      "epoch:16 step:12776 [D loss: 0.244459, acc.: 89.06%] [G loss: 3.045585]\n",
      "epoch:16 step:12777 [D loss: 0.287055, acc.: 88.28%] [G loss: 5.246578]\n",
      "epoch:16 step:12778 [D loss: 0.187658, acc.: 92.19%] [G loss: 5.091685]\n",
      "epoch:16 step:12779 [D loss: 0.322112, acc.: 87.50%] [G loss: 5.154973]\n",
      "epoch:16 step:12780 [D loss: 0.268277, acc.: 88.28%] [G loss: 3.841732]\n",
      "epoch:16 step:12781 [D loss: 0.211614, acc.: 93.75%] [G loss: 3.630594]\n",
      "epoch:16 step:12782 [D loss: 0.304474, acc.: 85.16%] [G loss: 3.351206]\n",
      "epoch:16 step:12783 [D loss: 0.382845, acc.: 82.03%] [G loss: 5.242834]\n",
      "epoch:16 step:12784 [D loss: 0.367940, acc.: 84.38%] [G loss: 2.843269]\n",
      "epoch:16 step:12785 [D loss: 0.310150, acc.: 88.28%] [G loss: 4.759654]\n",
      "epoch:16 step:12786 [D loss: 0.287161, acc.: 86.72%] [G loss: 3.700066]\n",
      "epoch:16 step:12787 [D loss: 0.322620, acc.: 88.28%] [G loss: 3.426425]\n",
      "epoch:16 step:12788 [D loss: 0.363970, acc.: 82.03%] [G loss: 3.131310]\n",
      "epoch:16 step:12789 [D loss: 0.266439, acc.: 90.62%] [G loss: 4.390135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12790 [D loss: 0.351040, acc.: 89.06%] [G loss: 3.947080]\n",
      "epoch:16 step:12791 [D loss: 0.364950, acc.: 82.81%] [G loss: 3.806065]\n",
      "epoch:16 step:12792 [D loss: 0.364010, acc.: 85.94%] [G loss: 3.718117]\n",
      "epoch:16 step:12793 [D loss: 0.364674, acc.: 84.38%] [G loss: 2.869268]\n",
      "epoch:16 step:12794 [D loss: 0.227978, acc.: 90.62%] [G loss: 3.884254]\n",
      "epoch:16 step:12795 [D loss: 0.282868, acc.: 87.50%] [G loss: 2.899904]\n",
      "epoch:16 step:12796 [D loss: 0.356197, acc.: 85.16%] [G loss: 2.849602]\n",
      "epoch:16 step:12797 [D loss: 0.267474, acc.: 91.41%] [G loss: 3.141487]\n",
      "epoch:16 step:12798 [D loss: 0.200822, acc.: 89.06%] [G loss: 5.735891]\n",
      "epoch:16 step:12799 [D loss: 0.347652, acc.: 82.03%] [G loss: 4.192043]\n",
      "epoch:16 step:12800 [D loss: 0.378052, acc.: 81.25%] [G loss: 2.731412]\n",
      "epoch:16 step:12801 [D loss: 0.307203, acc.: 87.50%] [G loss: 4.131445]\n",
      "epoch:16 step:12802 [D loss: 0.309948, acc.: 85.94%] [G loss: 3.320759]\n",
      "epoch:16 step:12803 [D loss: 0.366579, acc.: 84.38%] [G loss: 5.115794]\n",
      "epoch:16 step:12804 [D loss: 0.432197, acc.: 79.69%] [G loss: 3.221697]\n",
      "epoch:16 step:12805 [D loss: 0.468841, acc.: 78.12%] [G loss: 5.240395]\n",
      "epoch:16 step:12806 [D loss: 0.535776, acc.: 78.91%] [G loss: 2.377669]\n",
      "epoch:16 step:12807 [D loss: 0.319255, acc.: 87.50%] [G loss: 6.286591]\n",
      "epoch:16 step:12808 [D loss: 0.333264, acc.: 85.16%] [G loss: 3.153731]\n",
      "epoch:16 step:12809 [D loss: 0.275633, acc.: 85.94%] [G loss: 5.574945]\n",
      "epoch:16 step:12810 [D loss: 0.257756, acc.: 89.06%] [G loss: 3.591980]\n",
      "epoch:16 step:12811 [D loss: 0.450738, acc.: 79.69%] [G loss: 2.747262]\n",
      "epoch:16 step:12812 [D loss: 0.459458, acc.: 82.03%] [G loss: 3.888955]\n",
      "epoch:16 step:12813 [D loss: 0.810101, acc.: 74.22%] [G loss: 7.588627]\n",
      "epoch:16 step:12814 [D loss: 2.534930, acc.: 50.78%] [G loss: 3.742943]\n",
      "epoch:16 step:12815 [D loss: 0.373615, acc.: 90.62%] [G loss: 4.856496]\n",
      "epoch:16 step:12816 [D loss: 0.865878, acc.: 66.41%] [G loss: 5.936290]\n",
      "epoch:16 step:12817 [D loss: 0.437363, acc.: 80.47%] [G loss: 4.347951]\n",
      "epoch:16 step:12818 [D loss: 0.428728, acc.: 80.47%] [G loss: 4.051406]\n",
      "epoch:16 step:12819 [D loss: 0.592443, acc.: 84.38%] [G loss: 6.196908]\n",
      "epoch:16 step:12820 [D loss: 1.486185, acc.: 60.16%] [G loss: 4.511124]\n",
      "epoch:16 step:12821 [D loss: 0.360065, acc.: 83.59%] [G loss: 5.056868]\n",
      "epoch:16 step:12822 [D loss: 0.651821, acc.: 77.34%] [G loss: 3.114028]\n",
      "epoch:16 step:12823 [D loss: 0.408663, acc.: 80.47%] [G loss: 3.371634]\n",
      "epoch:16 step:12824 [D loss: 0.482303, acc.: 82.03%] [G loss: 2.837221]\n",
      "epoch:16 step:12825 [D loss: 0.340271, acc.: 86.72%] [G loss: 2.887509]\n",
      "epoch:16 step:12826 [D loss: 0.387664, acc.: 79.69%] [G loss: 2.362778]\n",
      "epoch:16 step:12827 [D loss: 0.293308, acc.: 87.50%] [G loss: 2.530982]\n",
      "epoch:16 step:12828 [D loss: 0.431846, acc.: 77.34%] [G loss: 2.779337]\n",
      "epoch:16 step:12829 [D loss: 0.379096, acc.: 85.16%] [G loss: 2.869262]\n",
      "epoch:16 step:12830 [D loss: 0.404753, acc.: 83.59%] [G loss: 2.899986]\n",
      "epoch:16 step:12831 [D loss: 0.361862, acc.: 82.03%] [G loss: 2.496949]\n",
      "epoch:16 step:12832 [D loss: 0.328960, acc.: 83.59%] [G loss: 2.753124]\n",
      "epoch:16 step:12833 [D loss: 0.373154, acc.: 81.25%] [G loss: 2.356466]\n",
      "epoch:16 step:12834 [D loss: 0.375177, acc.: 86.72%] [G loss: 2.870554]\n",
      "epoch:16 step:12835 [D loss: 0.261227, acc.: 94.53%] [G loss: 2.634218]\n",
      "epoch:16 step:12836 [D loss: 0.500927, acc.: 75.00%] [G loss: 2.174592]\n",
      "epoch:16 step:12837 [D loss: 0.320161, acc.: 85.94%] [G loss: 2.370524]\n",
      "epoch:16 step:12838 [D loss: 0.414442, acc.: 82.03%] [G loss: 2.262639]\n",
      "epoch:16 step:12839 [D loss: 0.326945, acc.: 84.38%] [G loss: 2.801189]\n",
      "epoch:16 step:12840 [D loss: 0.326979, acc.: 85.94%] [G loss: 2.427116]\n",
      "epoch:16 step:12841 [D loss: 0.350528, acc.: 83.59%] [G loss: 3.618351]\n",
      "epoch:16 step:12842 [D loss: 0.343627, acc.: 86.72%] [G loss: 2.095713]\n",
      "epoch:16 step:12843 [D loss: 0.302484, acc.: 82.81%] [G loss: 2.674273]\n",
      "epoch:16 step:12844 [D loss: 0.307245, acc.: 90.62%] [G loss: 2.926370]\n",
      "epoch:16 step:12845 [D loss: 0.302725, acc.: 84.38%] [G loss: 2.671667]\n",
      "epoch:16 step:12846 [D loss: 0.393980, acc.: 81.25%] [G loss: 2.628960]\n",
      "epoch:16 step:12847 [D loss: 0.436003, acc.: 78.91%] [G loss: 2.953722]\n",
      "epoch:16 step:12848 [D loss: 0.291569, acc.: 83.59%] [G loss: 3.881135]\n",
      "epoch:16 step:12849 [D loss: 0.359306, acc.: 81.25%] [G loss: 2.998911]\n",
      "epoch:16 step:12850 [D loss: 0.275197, acc.: 86.72%] [G loss: 3.000741]\n",
      "epoch:16 step:12851 [D loss: 0.252387, acc.: 89.84%] [G loss: 3.022362]\n",
      "epoch:16 step:12852 [D loss: 0.319686, acc.: 87.50%] [G loss: 2.545401]\n",
      "epoch:16 step:12853 [D loss: 0.285485, acc.: 89.84%] [G loss: 2.539273]\n",
      "epoch:16 step:12854 [D loss: 0.347834, acc.: 86.72%] [G loss: 2.634737]\n",
      "epoch:16 step:12855 [D loss: 0.282039, acc.: 89.06%] [G loss: 3.305348]\n",
      "epoch:16 step:12856 [D loss: 0.421029, acc.: 81.25%] [G loss: 2.640199]\n",
      "epoch:16 step:12857 [D loss: 0.362181, acc.: 82.03%] [G loss: 2.626862]\n",
      "epoch:16 step:12858 [D loss: 0.373171, acc.: 87.50%] [G loss: 3.262906]\n",
      "epoch:16 step:12859 [D loss: 0.329678, acc.: 83.59%] [G loss: 3.577048]\n",
      "epoch:16 step:12860 [D loss: 0.336647, acc.: 83.59%] [G loss: 4.016167]\n",
      "epoch:16 step:12861 [D loss: 0.319526, acc.: 84.38%] [G loss: 5.069818]\n",
      "epoch:16 step:12862 [D loss: 0.305988, acc.: 86.72%] [G loss: 3.462064]\n",
      "epoch:16 step:12863 [D loss: 0.303932, acc.: 85.16%] [G loss: 8.875916]\n",
      "epoch:16 step:12864 [D loss: 0.298668, acc.: 85.16%] [G loss: 2.951262]\n",
      "epoch:16 step:12865 [D loss: 0.246389, acc.: 91.41%] [G loss: 6.964595]\n",
      "epoch:16 step:12866 [D loss: 0.255208, acc.: 89.06%] [G loss: 3.142335]\n",
      "epoch:16 step:12867 [D loss: 0.224054, acc.: 92.97%] [G loss: 4.830065]\n",
      "epoch:16 step:12868 [D loss: 0.249757, acc.: 92.97%] [G loss: 3.304533]\n",
      "epoch:16 step:12869 [D loss: 0.316903, acc.: 83.59%] [G loss: 2.862733]\n",
      "epoch:16 step:12870 [D loss: 0.252208, acc.: 89.06%] [G loss: 2.444035]\n",
      "epoch:16 step:12871 [D loss: 0.353090, acc.: 84.38%] [G loss: 2.906901]\n",
      "epoch:16 step:12872 [D loss: 0.262586, acc.: 90.62%] [G loss: 3.826722]\n",
      "epoch:16 step:12873 [D loss: 0.234032, acc.: 91.41%] [G loss: 2.947260]\n",
      "epoch:16 step:12874 [D loss: 0.337115, acc.: 82.81%] [G loss: 2.633687]\n",
      "epoch:16 step:12875 [D loss: 0.297672, acc.: 88.28%] [G loss: 2.139085]\n",
      "epoch:16 step:12876 [D loss: 0.319404, acc.: 86.72%] [G loss: 3.143213]\n",
      "epoch:16 step:12877 [D loss: 0.331587, acc.: 90.62%] [G loss: 3.033912]\n",
      "epoch:16 step:12878 [D loss: 0.287238, acc.: 86.72%] [G loss: 2.624548]\n",
      "epoch:16 step:12879 [D loss: 0.296310, acc.: 89.06%] [G loss: 2.706621]\n",
      "epoch:16 step:12880 [D loss: 0.362448, acc.: 85.16%] [G loss: 2.496655]\n",
      "epoch:16 step:12881 [D loss: 0.259519, acc.: 89.06%] [G loss: 4.311674]\n",
      "epoch:16 step:12882 [D loss: 0.453913, acc.: 78.12%] [G loss: 2.351219]\n",
      "epoch:16 step:12883 [D loss: 0.276333, acc.: 86.72%] [G loss: 2.854745]\n",
      "epoch:16 step:12884 [D loss: 0.365079, acc.: 82.03%] [G loss: 2.851146]\n",
      "epoch:16 step:12885 [D loss: 0.319390, acc.: 84.38%] [G loss: 3.173726]\n",
      "epoch:16 step:12886 [D loss: 0.282297, acc.: 90.62%] [G loss: 2.727647]\n",
      "epoch:16 step:12887 [D loss: 0.428030, acc.: 82.03%] [G loss: 3.980322]\n",
      "epoch:16 step:12888 [D loss: 0.360328, acc.: 84.38%] [G loss: 2.607812]\n",
      "epoch:16 step:12889 [D loss: 0.314670, acc.: 85.16%] [G loss: 2.738297]\n",
      "epoch:16 step:12890 [D loss: 0.419988, acc.: 82.03%] [G loss: 2.380178]\n",
      "epoch:16 step:12891 [D loss: 0.453144, acc.: 80.47%] [G loss: 2.656577]\n",
      "epoch:16 step:12892 [D loss: 0.290560, acc.: 87.50%] [G loss: 3.172349]\n",
      "epoch:16 step:12893 [D loss: 0.362288, acc.: 80.47%] [G loss: 3.206306]\n",
      "epoch:16 step:12894 [D loss: 0.391632, acc.: 82.03%] [G loss: 2.778774]\n",
      "epoch:16 step:12895 [D loss: 0.349024, acc.: 83.59%] [G loss: 3.034929]\n",
      "epoch:16 step:12896 [D loss: 0.317731, acc.: 88.28%] [G loss: 2.724591]\n",
      "epoch:16 step:12897 [D loss: 0.334414, acc.: 85.16%] [G loss: 2.673864]\n",
      "epoch:16 step:12898 [D loss: 0.323149, acc.: 84.38%] [G loss: 3.047122]\n",
      "epoch:16 step:12899 [D loss: 0.303531, acc.: 88.28%] [G loss: 2.803622]\n",
      "epoch:16 step:12900 [D loss: 0.332174, acc.: 85.94%] [G loss: 2.595008]\n",
      "epoch:16 step:12901 [D loss: 0.320043, acc.: 87.50%] [G loss: 2.197651]\n",
      "epoch:16 step:12902 [D loss: 0.357939, acc.: 81.25%] [G loss: 3.022987]\n",
      "epoch:16 step:12903 [D loss: 0.247932, acc.: 88.28%] [G loss: 3.416700]\n",
      "epoch:16 step:12904 [D loss: 0.333611, acc.: 86.72%] [G loss: 4.038502]\n",
      "epoch:16 step:12905 [D loss: 0.297968, acc.: 88.28%] [G loss: 2.321740]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12906 [D loss: 0.384957, acc.: 82.81%] [G loss: 2.773086]\n",
      "epoch:16 step:12907 [D loss: 0.328289, acc.: 89.06%] [G loss: 1.968971]\n",
      "epoch:16 step:12908 [D loss: 0.272958, acc.: 91.41%] [G loss: 2.667944]\n",
      "epoch:16 step:12909 [D loss: 0.238401, acc.: 92.19%] [G loss: 2.872742]\n",
      "epoch:16 step:12910 [D loss: 0.378028, acc.: 85.16%] [G loss: 2.553078]\n",
      "epoch:16 step:12911 [D loss: 0.286851, acc.: 89.06%] [G loss: 2.521794]\n",
      "epoch:16 step:12912 [D loss: 0.286886, acc.: 89.06%] [G loss: 2.793096]\n",
      "epoch:16 step:12913 [D loss: 0.334861, acc.: 85.16%] [G loss: 1.685141]\n",
      "epoch:16 step:12914 [D loss: 0.243472, acc.: 89.84%] [G loss: 2.749275]\n",
      "epoch:16 step:12915 [D loss: 0.285906, acc.: 87.50%] [G loss: 2.265647]\n",
      "epoch:16 step:12916 [D loss: 0.288314, acc.: 85.94%] [G loss: 3.046873]\n",
      "epoch:16 step:12917 [D loss: 0.340507, acc.: 84.38%] [G loss: 2.638829]\n",
      "epoch:16 step:12918 [D loss: 0.434056, acc.: 80.47%] [G loss: 3.283261]\n",
      "epoch:16 step:12919 [D loss: 0.293919, acc.: 87.50%] [G loss: 3.263163]\n",
      "epoch:16 step:12920 [D loss: 0.309916, acc.: 84.38%] [G loss: 4.035701]\n",
      "epoch:16 step:12921 [D loss: 0.413520, acc.: 78.12%] [G loss: 2.754495]\n",
      "epoch:16 step:12922 [D loss: 0.253287, acc.: 91.41%] [G loss: 3.315369]\n",
      "epoch:16 step:12923 [D loss: 0.214745, acc.: 88.28%] [G loss: 5.205846]\n",
      "epoch:16 step:12924 [D loss: 0.395073, acc.: 80.47%] [G loss: 4.070087]\n",
      "epoch:16 step:12925 [D loss: 0.409669, acc.: 79.69%] [G loss: 2.574636]\n",
      "epoch:16 step:12926 [D loss: 0.198081, acc.: 94.53%] [G loss: 4.941771]\n",
      "epoch:16 step:12927 [D loss: 0.231565, acc.: 93.75%] [G loss: 3.944371]\n",
      "epoch:16 step:12928 [D loss: 0.324524, acc.: 85.94%] [G loss: 3.221283]\n",
      "epoch:16 step:12929 [D loss: 0.242899, acc.: 91.41%] [G loss: 3.946762]\n",
      "epoch:16 step:12930 [D loss: 0.292827, acc.: 86.72%] [G loss: 5.964731]\n",
      "epoch:16 step:12931 [D loss: 0.515201, acc.: 77.34%] [G loss: 4.217996]\n",
      "epoch:16 step:12932 [D loss: 0.603719, acc.: 73.44%] [G loss: 6.514767]\n",
      "epoch:16 step:12933 [D loss: 0.776475, acc.: 75.78%] [G loss: 4.909599]\n",
      "epoch:16 step:12934 [D loss: 0.598014, acc.: 74.22%] [G loss: 2.973215]\n",
      "epoch:16 step:12935 [D loss: 0.377589, acc.: 81.25%] [G loss: 2.808989]\n",
      "epoch:16 step:12936 [D loss: 0.348039, acc.: 85.16%] [G loss: 2.953314]\n",
      "epoch:16 step:12937 [D loss: 0.343110, acc.: 85.16%] [G loss: 2.376551]\n",
      "epoch:16 step:12938 [D loss: 0.290215, acc.: 88.28%] [G loss: 3.722687]\n",
      "epoch:16 step:12939 [D loss: 0.339159, acc.: 85.94%] [G loss: 3.541065]\n",
      "epoch:16 step:12940 [D loss: 0.394822, acc.: 82.81%] [G loss: 2.774218]\n",
      "epoch:16 step:12941 [D loss: 0.408671, acc.: 80.47%] [G loss: 2.724864]\n",
      "epoch:16 step:12942 [D loss: 0.302049, acc.: 88.28%] [G loss: 3.229799]\n",
      "epoch:16 step:12943 [D loss: 0.255622, acc.: 89.84%] [G loss: 3.006131]\n",
      "epoch:16 step:12944 [D loss: 0.351118, acc.: 85.16%] [G loss: 2.774049]\n",
      "epoch:16 step:12945 [D loss: 0.321464, acc.: 85.94%] [G loss: 2.467700]\n",
      "epoch:16 step:12946 [D loss: 0.297658, acc.: 87.50%] [G loss: 3.171332]\n",
      "epoch:16 step:12947 [D loss: 0.230906, acc.: 90.62%] [G loss: 3.445939]\n",
      "epoch:16 step:12948 [D loss: 0.306166, acc.: 85.16%] [G loss: 3.026429]\n",
      "epoch:16 step:12949 [D loss: 0.316690, acc.: 86.72%] [G loss: 4.206726]\n",
      "epoch:16 step:12950 [D loss: 0.189157, acc.: 92.97%] [G loss: 6.297192]\n",
      "epoch:16 step:12951 [D loss: 0.340055, acc.: 83.59%] [G loss: 4.406727]\n",
      "epoch:16 step:12952 [D loss: 0.272813, acc.: 85.16%] [G loss: 3.627744]\n",
      "epoch:16 step:12953 [D loss: 0.300223, acc.: 91.41%] [G loss: 3.134772]\n",
      "epoch:16 step:12954 [D loss: 0.339080, acc.: 89.06%] [G loss: 2.778281]\n",
      "epoch:16 step:12955 [D loss: 0.269630, acc.: 92.19%] [G loss: 2.666658]\n",
      "epoch:16 step:12956 [D loss: 0.367168, acc.: 84.38%] [G loss: 2.368420]\n",
      "epoch:16 step:12957 [D loss: 0.426863, acc.: 81.25%] [G loss: 2.172988]\n",
      "epoch:16 step:12958 [D loss: 0.414457, acc.: 83.59%] [G loss: 2.193286]\n",
      "epoch:16 step:12959 [D loss: 0.388952, acc.: 84.38%] [G loss: 2.877399]\n",
      "epoch:16 step:12960 [D loss: 0.292847, acc.: 88.28%] [G loss: 4.111313]\n",
      "epoch:16 step:12961 [D loss: 0.328308, acc.: 84.38%] [G loss: 3.245533]\n",
      "epoch:16 step:12962 [D loss: 0.320364, acc.: 86.72%] [G loss: 2.891151]\n",
      "epoch:16 step:12963 [D loss: 0.391153, acc.: 80.47%] [G loss: 2.959740]\n",
      "epoch:16 step:12964 [D loss: 0.323018, acc.: 86.72%] [G loss: 2.854266]\n",
      "epoch:16 step:12965 [D loss: 0.342722, acc.: 85.16%] [G loss: 2.089818]\n",
      "epoch:16 step:12966 [D loss: 0.278241, acc.: 90.62%] [G loss: 2.568952]\n",
      "epoch:16 step:12967 [D loss: 0.335526, acc.: 85.16%] [G loss: 2.660994]\n",
      "epoch:16 step:12968 [D loss: 0.381819, acc.: 82.81%] [G loss: 2.053841]\n",
      "epoch:16 step:12969 [D loss: 0.340470, acc.: 85.16%] [G loss: 2.276596]\n",
      "epoch:16 step:12970 [D loss: 0.347157, acc.: 84.38%] [G loss: 2.168077]\n",
      "epoch:16 step:12971 [D loss: 0.422629, acc.: 80.47%] [G loss: 2.043069]\n",
      "epoch:16 step:12972 [D loss: 0.375549, acc.: 85.94%] [G loss: 2.620699]\n",
      "epoch:16 step:12973 [D loss: 0.291103, acc.: 87.50%] [G loss: 3.078182]\n",
      "epoch:16 step:12974 [D loss: 0.278203, acc.: 88.28%] [G loss: 2.834676]\n",
      "epoch:16 step:12975 [D loss: 0.263606, acc.: 88.28%] [G loss: 2.806033]\n",
      "epoch:16 step:12976 [D loss: 0.295421, acc.: 89.06%] [G loss: 2.485031]\n",
      "epoch:16 step:12977 [D loss: 0.353559, acc.: 83.59%] [G loss: 3.338355]\n",
      "epoch:16 step:12978 [D loss: 0.343738, acc.: 82.03%] [G loss: 3.654748]\n",
      "epoch:16 step:12979 [D loss: 0.311752, acc.: 86.72%] [G loss: 3.872398]\n",
      "epoch:16 step:12980 [D loss: 0.325061, acc.: 86.72%] [G loss: 2.788502]\n",
      "epoch:16 step:12981 [D loss: 0.215166, acc.: 92.97%] [G loss: 2.834608]\n",
      "epoch:16 step:12982 [D loss: 0.351146, acc.: 86.72%] [G loss: 5.178591]\n",
      "epoch:16 step:12983 [D loss: 0.327127, acc.: 84.38%] [G loss: 4.932373]\n",
      "epoch:16 step:12984 [D loss: 0.272779, acc.: 85.94%] [G loss: 3.936712]\n",
      "epoch:16 step:12985 [D loss: 0.349782, acc.: 82.03%] [G loss: 2.729465]\n",
      "epoch:16 step:12986 [D loss: 0.313502, acc.: 84.38%] [G loss: 3.060313]\n",
      "epoch:16 step:12987 [D loss: 0.267850, acc.: 85.94%] [G loss: 2.977501]\n",
      "epoch:16 step:12988 [D loss: 0.355371, acc.: 86.72%] [G loss: 3.656328]\n",
      "epoch:16 step:12989 [D loss: 0.275172, acc.: 87.50%] [G loss: 4.505878]\n",
      "epoch:16 step:12990 [D loss: 0.365917, acc.: 85.16%] [G loss: 5.556497]\n",
      "epoch:16 step:12991 [D loss: 0.779632, acc.: 67.19%] [G loss: 3.791785]\n",
      "epoch:16 step:12992 [D loss: 0.705548, acc.: 80.47%] [G loss: 5.381966]\n",
      "epoch:16 step:12993 [D loss: 0.685651, acc.: 70.31%] [G loss: 4.506011]\n",
      "epoch:16 step:12994 [D loss: 0.172320, acc.: 92.97%] [G loss: 4.534572]\n",
      "epoch:16 step:12995 [D loss: 0.795433, acc.: 68.75%] [G loss: 6.900690]\n",
      "epoch:16 step:12996 [D loss: 0.785292, acc.: 75.78%] [G loss: 3.421040]\n",
      "epoch:16 step:12997 [D loss: 0.453561, acc.: 79.69%] [G loss: 3.949994]\n",
      "epoch:16 step:12998 [D loss: 0.360090, acc.: 80.47%] [G loss: 2.628139]\n",
      "epoch:16 step:12999 [D loss: 0.390919, acc.: 81.25%] [G loss: 2.629592]\n",
      "epoch:16 step:13000 [D loss: 0.331153, acc.: 84.38%] [G loss: 2.763132]\n",
      "epoch:16 step:13001 [D loss: 0.293462, acc.: 89.06%] [G loss: 3.724051]\n",
      "epoch:16 step:13002 [D loss: 0.273999, acc.: 88.28%] [G loss: 2.640616]\n",
      "epoch:16 step:13003 [D loss: 0.373951, acc.: 82.03%] [G loss: 2.606314]\n",
      "epoch:16 step:13004 [D loss: 0.320348, acc.: 82.81%] [G loss: 3.563341]\n",
      "epoch:16 step:13005 [D loss: 0.352578, acc.: 82.81%] [G loss: 2.203166]\n",
      "epoch:16 step:13006 [D loss: 0.292054, acc.: 86.72%] [G loss: 2.993858]\n",
      "epoch:16 step:13007 [D loss: 0.244668, acc.: 91.41%] [G loss: 3.721465]\n",
      "epoch:16 step:13008 [D loss: 0.245913, acc.: 90.62%] [G loss: 3.371135]\n",
      "epoch:16 step:13009 [D loss: 0.331803, acc.: 86.72%] [G loss: 2.427035]\n",
      "epoch:16 step:13010 [D loss: 0.376393, acc.: 82.81%] [G loss: 2.157809]\n",
      "epoch:16 step:13011 [D loss: 0.351742, acc.: 83.59%] [G loss: 2.900645]\n",
      "epoch:16 step:13012 [D loss: 0.430800, acc.: 78.91%] [G loss: 2.788969]\n",
      "epoch:16 step:13013 [D loss: 0.204916, acc.: 91.41%] [G loss: 3.726794]\n",
      "epoch:16 step:13014 [D loss: 0.331033, acc.: 87.50%] [G loss: 3.380178]\n",
      "epoch:16 step:13015 [D loss: 0.350002, acc.: 85.16%] [G loss: 2.242560]\n",
      "epoch:16 step:13016 [D loss: 0.452421, acc.: 78.91%] [G loss: 2.687773]\n",
      "epoch:16 step:13017 [D loss: 0.319149, acc.: 85.16%] [G loss: 3.427300]\n",
      "epoch:16 step:13018 [D loss: 0.336963, acc.: 85.16%] [G loss: 4.950229]\n",
      "epoch:16 step:13019 [D loss: 0.272251, acc.: 90.62%] [G loss: 3.107864]\n",
      "epoch:16 step:13020 [D loss: 0.308392, acc.: 87.50%] [G loss: 2.597915]\n",
      "epoch:16 step:13021 [D loss: 0.386467, acc.: 82.03%] [G loss: 2.313233]\n",
      "epoch:16 step:13022 [D loss: 0.257990, acc.: 91.41%] [G loss: 2.631254]\n",
      "epoch:16 step:13023 [D loss: 0.318070, acc.: 82.81%] [G loss: 3.758901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:13024 [D loss: 0.296636, acc.: 89.84%] [G loss: 2.260527]\n",
      "epoch:16 step:13025 [D loss: 0.301375, acc.: 86.72%] [G loss: 2.905248]\n",
      "epoch:16 step:13026 [D loss: 0.458108, acc.: 82.81%] [G loss: 3.161620]\n",
      "epoch:16 step:13027 [D loss: 0.351599, acc.: 81.25%] [G loss: 3.367480]\n",
      "epoch:16 step:13028 [D loss: 0.283933, acc.: 88.28%] [G loss: 3.813801]\n",
      "epoch:16 step:13029 [D loss: 0.350299, acc.: 87.50%] [G loss: 2.557488]\n",
      "epoch:16 step:13030 [D loss: 0.211345, acc.: 92.97%] [G loss: 3.318010]\n",
      "epoch:16 step:13031 [D loss: 0.396048, acc.: 82.81%] [G loss: 2.629515]\n",
      "epoch:16 step:13032 [D loss: 0.262055, acc.: 89.84%] [G loss: 2.906474]\n",
      "epoch:16 step:13033 [D loss: 0.342554, acc.: 80.47%] [G loss: 2.503432]\n",
      "epoch:16 step:13034 [D loss: 0.383636, acc.: 85.16%] [G loss: 1.974846]\n",
      "epoch:16 step:13035 [D loss: 0.272758, acc.: 89.06%] [G loss: 2.680384]\n",
      "epoch:16 step:13036 [D loss: 0.308639, acc.: 88.28%] [G loss: 2.586090]\n",
      "epoch:16 step:13037 [D loss: 0.302043, acc.: 88.28%] [G loss: 3.664566]\n",
      "epoch:16 step:13038 [D loss: 0.283766, acc.: 89.84%] [G loss: 2.889463]\n",
      "epoch:16 step:13039 [D loss: 0.301604, acc.: 89.06%] [G loss: 2.687787]\n",
      "epoch:16 step:13040 [D loss: 0.384045, acc.: 84.38%] [G loss: 2.448516]\n",
      "epoch:16 step:13041 [D loss: 0.339022, acc.: 83.59%] [G loss: 2.822662]\n",
      "epoch:16 step:13042 [D loss: 0.251384, acc.: 92.19%] [G loss: 2.887665]\n",
      "epoch:16 step:13043 [D loss: 0.390556, acc.: 82.03%] [G loss: 2.962034]\n",
      "epoch:16 step:13044 [D loss: 0.380094, acc.: 80.47%] [G loss: 3.046250]\n",
      "epoch:16 step:13045 [D loss: 0.459509, acc.: 82.03%] [G loss: 5.792383]\n",
      "epoch:16 step:13046 [D loss: 0.360571, acc.: 82.03%] [G loss: 3.281328]\n",
      "epoch:16 step:13047 [D loss: 0.321731, acc.: 85.94%] [G loss: 6.471743]\n",
      "epoch:16 step:13048 [D loss: 0.364883, acc.: 85.16%] [G loss: 3.054567]\n",
      "epoch:16 step:13049 [D loss: 0.238687, acc.: 92.97%] [G loss: 5.587598]\n",
      "epoch:16 step:13050 [D loss: 0.295391, acc.: 89.84%] [G loss: 3.164805]\n",
      "epoch:16 step:13051 [D loss: 0.222601, acc.: 92.19%] [G loss: 3.420033]\n",
      "epoch:16 step:13052 [D loss: 0.290784, acc.: 85.94%] [G loss: 2.846759]\n",
      "epoch:16 step:13053 [D loss: 0.300493, acc.: 85.94%] [G loss: 2.876991]\n",
      "epoch:16 step:13054 [D loss: 0.319634, acc.: 86.72%] [G loss: 3.256847]\n",
      "epoch:16 step:13055 [D loss: 0.343169, acc.: 82.81%] [G loss: 4.250367]\n",
      "epoch:16 step:13056 [D loss: 0.272511, acc.: 89.06%] [G loss: 5.221341]\n",
      "epoch:16 step:13057 [D loss: 0.304345, acc.: 87.50%] [G loss: 2.113811]\n",
      "epoch:16 step:13058 [D loss: 0.395298, acc.: 79.69%] [G loss: 2.947266]\n",
      "epoch:16 step:13059 [D loss: 0.287753, acc.: 89.84%] [G loss: 2.888421]\n",
      "epoch:16 step:13060 [D loss: 0.331252, acc.: 82.81%] [G loss: 3.600547]\n",
      "epoch:16 step:13061 [D loss: 0.267652, acc.: 89.84%] [G loss: 2.771528]\n",
      "epoch:16 step:13062 [D loss: 0.394737, acc.: 82.81%] [G loss: 3.611565]\n",
      "epoch:16 step:13063 [D loss: 0.286229, acc.: 89.84%] [G loss: 3.981141]\n",
      "epoch:16 step:13064 [D loss: 0.276309, acc.: 88.28%] [G loss: 3.706022]\n",
      "epoch:16 step:13065 [D loss: 0.293190, acc.: 84.38%] [G loss: 3.698596]\n",
      "epoch:16 step:13066 [D loss: 0.366547, acc.: 87.50%] [G loss: 2.245874]\n",
      "epoch:16 step:13067 [D loss: 0.333369, acc.: 82.81%] [G loss: 3.231594]\n",
      "epoch:16 step:13068 [D loss: 0.332362, acc.: 85.16%] [G loss: 3.975033]\n",
      "epoch:16 step:13069 [D loss: 0.267152, acc.: 89.84%] [G loss: 3.405663]\n",
      "epoch:16 step:13070 [D loss: 0.398605, acc.: 82.81%] [G loss: 2.833632]\n",
      "epoch:16 step:13071 [D loss: 0.293810, acc.: 84.38%] [G loss: 2.271458]\n",
      "epoch:16 step:13072 [D loss: 0.276609, acc.: 87.50%] [G loss: 3.803675]\n",
      "epoch:16 step:13073 [D loss: 0.382865, acc.: 81.25%] [G loss: 2.539434]\n",
      "epoch:16 step:13074 [D loss: 0.348026, acc.: 82.81%] [G loss: 2.970081]\n",
      "epoch:16 step:13075 [D loss: 0.293714, acc.: 85.94%] [G loss: 3.108595]\n",
      "epoch:16 step:13076 [D loss: 0.341645, acc.: 85.16%] [G loss: 2.559391]\n",
      "epoch:16 step:13077 [D loss: 0.247543, acc.: 89.84%] [G loss: 2.908479]\n",
      "epoch:16 step:13078 [D loss: 0.248344, acc.: 92.19%] [G loss: 3.525807]\n",
      "epoch:16 step:13079 [D loss: 0.354543, acc.: 82.81%] [G loss: 3.576324]\n",
      "epoch:16 step:13080 [D loss: 0.279078, acc.: 89.84%] [G loss: 2.596882]\n",
      "epoch:16 step:13081 [D loss: 0.352973, acc.: 85.94%] [G loss: 2.460617]\n",
      "epoch:16 step:13082 [D loss: 0.255988, acc.: 87.50%] [G loss: 4.180699]\n",
      "epoch:16 step:13083 [D loss: 0.297386, acc.: 88.28%] [G loss: 4.499699]\n",
      "epoch:16 step:13084 [D loss: 0.235569, acc.: 89.84%] [G loss: 3.163565]\n",
      "epoch:16 step:13085 [D loss: 0.348176, acc.: 82.03%] [G loss: 3.050909]\n",
      "epoch:16 step:13086 [D loss: 0.365960, acc.: 83.59%] [G loss: 4.416809]\n",
      "epoch:16 step:13087 [D loss: 0.440195, acc.: 80.47%] [G loss: 4.025646]\n",
      "epoch:16 step:13088 [D loss: 0.490025, acc.: 77.34%] [G loss: 2.384204]\n",
      "epoch:16 step:13089 [D loss: 0.337135, acc.: 84.38%] [G loss: 3.045541]\n",
      "epoch:16 step:13090 [D loss: 0.430797, acc.: 80.47%] [G loss: 2.621675]\n",
      "epoch:16 step:13091 [D loss: 0.445998, acc.: 79.69%] [G loss: 2.700308]\n",
      "epoch:16 step:13092 [D loss: 0.285215, acc.: 85.94%] [G loss: 2.692353]\n",
      "epoch:16 step:13093 [D loss: 0.326183, acc.: 84.38%] [G loss: 2.392401]\n",
      "epoch:16 step:13094 [D loss: 0.398211, acc.: 87.50%] [G loss: 3.965153]\n",
      "epoch:16 step:13095 [D loss: 0.296710, acc.: 88.28%] [G loss: 3.619542]\n",
      "epoch:16 step:13096 [D loss: 0.387496, acc.: 78.12%] [G loss: 2.826917]\n",
      "epoch:16 step:13097 [D loss: 0.288733, acc.: 89.84%] [G loss: 2.820038]\n",
      "epoch:16 step:13098 [D loss: 0.253501, acc.: 91.41%] [G loss: 3.105569]\n",
      "epoch:16 step:13099 [D loss: 0.386731, acc.: 80.47%] [G loss: 2.970598]\n",
      "epoch:16 step:13100 [D loss: 0.384004, acc.: 81.25%] [G loss: 2.518238]\n",
      "epoch:16 step:13101 [D loss: 0.390533, acc.: 81.25%] [G loss: 3.698966]\n",
      "epoch:16 step:13102 [D loss: 0.317588, acc.: 86.72%] [G loss: 2.828534]\n",
      "epoch:16 step:13103 [D loss: 0.347251, acc.: 82.03%] [G loss: 5.063433]\n",
      "epoch:16 step:13104 [D loss: 0.337559, acc.: 85.94%] [G loss: 5.477169]\n",
      "epoch:16 step:13105 [D loss: 0.269817, acc.: 89.84%] [G loss: 3.626238]\n",
      "epoch:16 step:13106 [D loss: 0.211873, acc.: 92.19%] [G loss: 3.450345]\n",
      "epoch:16 step:13107 [D loss: 0.385673, acc.: 81.25%] [G loss: 3.583278]\n",
      "epoch:16 step:13108 [D loss: 0.294949, acc.: 83.59%] [G loss: 3.435612]\n",
      "epoch:16 step:13109 [D loss: 0.363944, acc.: 81.25%] [G loss: 2.774214]\n",
      "epoch:16 step:13110 [D loss: 0.302113, acc.: 88.28%] [G loss: 3.154108]\n",
      "epoch:16 step:13111 [D loss: 0.356861, acc.: 89.06%] [G loss: 2.928650]\n",
      "epoch:16 step:13112 [D loss: 0.361438, acc.: 82.03%] [G loss: 3.231652]\n",
      "epoch:16 step:13113 [D loss: 0.277992, acc.: 87.50%] [G loss: 2.926658]\n",
      "epoch:16 step:13114 [D loss: 0.445782, acc.: 78.12%] [G loss: 3.021856]\n",
      "epoch:16 step:13115 [D loss: 0.346109, acc.: 85.94%] [G loss: 3.559496]\n",
      "epoch:16 step:13116 [D loss: 0.393294, acc.: 81.25%] [G loss: 2.279885]\n",
      "epoch:16 step:13117 [D loss: 0.212185, acc.: 95.31%] [G loss: 3.325312]\n",
      "epoch:16 step:13118 [D loss: 0.314318, acc.: 84.38%] [G loss: 3.010505]\n",
      "epoch:16 step:13119 [D loss: 0.423813, acc.: 77.34%] [G loss: 4.371531]\n",
      "epoch:16 step:13120 [D loss: 0.422814, acc.: 82.03%] [G loss: 3.932698]\n",
      "epoch:16 step:13121 [D loss: 0.480060, acc.: 75.78%] [G loss: 4.510764]\n",
      "epoch:16 step:13122 [D loss: 0.222801, acc.: 92.19%] [G loss: 4.827384]\n",
      "epoch:16 step:13123 [D loss: 0.332235, acc.: 81.25%] [G loss: 3.627447]\n",
      "epoch:16 step:13124 [D loss: 0.339592, acc.: 88.28%] [G loss: 3.261712]\n",
      "epoch:16 step:13125 [D loss: 0.345263, acc.: 85.94%] [G loss: 4.017999]\n",
      "epoch:16 step:13126 [D loss: 0.297949, acc.: 85.94%] [G loss: 3.158433]\n",
      "epoch:16 step:13127 [D loss: 0.423814, acc.: 75.00%] [G loss: 2.952689]\n",
      "epoch:16 step:13128 [D loss: 0.268018, acc.: 87.50%] [G loss: 3.668136]\n",
      "epoch:16 step:13129 [D loss: 0.375290, acc.: 83.59%] [G loss: 3.034597]\n",
      "epoch:16 step:13130 [D loss: 0.316520, acc.: 86.72%] [G loss: 3.921029]\n",
      "epoch:16 step:13131 [D loss: 0.300781, acc.: 83.59%] [G loss: 4.639184]\n",
      "epoch:16 step:13132 [D loss: 0.302007, acc.: 89.06%] [G loss: 3.178204]\n",
      "epoch:16 step:13133 [D loss: 0.457663, acc.: 73.44%] [G loss: 3.228707]\n",
      "epoch:16 step:13134 [D loss: 0.504786, acc.: 80.47%] [G loss: 3.811638]\n",
      "epoch:16 step:13135 [D loss: 0.558115, acc.: 76.56%] [G loss: 5.172697]\n",
      "epoch:16 step:13136 [D loss: 0.616663, acc.: 72.66%] [G loss: 5.394500]\n",
      "epoch:16 step:13137 [D loss: 0.896255, acc.: 71.88%] [G loss: 7.149569]\n",
      "epoch:16 step:13138 [D loss: 0.554564, acc.: 71.88%] [G loss: 4.526196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:13139 [D loss: 0.350588, acc.: 85.16%] [G loss: 7.343988]\n",
      "epoch:16 step:13140 [D loss: 0.284755, acc.: 89.06%] [G loss: 7.447638]\n",
      "epoch:16 step:13141 [D loss: 0.507371, acc.: 78.12%] [G loss: 5.022047]\n",
      "epoch:16 step:13142 [D loss: 0.291806, acc.: 87.50%] [G loss: 3.302341]\n",
      "epoch:16 step:13143 [D loss: 0.413216, acc.: 85.94%] [G loss: 2.978745]\n",
      "epoch:16 step:13144 [D loss: 0.369330, acc.: 81.25%] [G loss: 3.039621]\n",
      "epoch:16 step:13145 [D loss: 0.275708, acc.: 89.06%] [G loss: 3.152486]\n",
      "epoch:16 step:13146 [D loss: 0.328634, acc.: 85.94%] [G loss: 2.954484]\n",
      "epoch:16 step:13147 [D loss: 0.259960, acc.: 89.84%] [G loss: 3.626036]\n",
      "epoch:16 step:13148 [D loss: 0.240238, acc.: 92.97%] [G loss: 2.923876]\n",
      "epoch:16 step:13149 [D loss: 0.272839, acc.: 92.97%] [G loss: 3.921138]\n",
      "epoch:16 step:13150 [D loss: 0.397815, acc.: 81.25%] [G loss: 2.621172]\n",
      "epoch:16 step:13151 [D loss: 0.372482, acc.: 83.59%] [G loss: 2.588410]\n",
      "epoch:16 step:13152 [D loss: 0.258224, acc.: 92.19%] [G loss: 2.941622]\n",
      "epoch:16 step:13153 [D loss: 0.426692, acc.: 82.03%] [G loss: 2.711927]\n",
      "epoch:16 step:13154 [D loss: 0.249408, acc.: 91.41%] [G loss: 3.546964]\n",
      "epoch:16 step:13155 [D loss: 0.254507, acc.: 89.84%] [G loss: 3.686962]\n",
      "epoch:16 step:13156 [D loss: 0.306082, acc.: 85.16%] [G loss: 3.146524]\n",
      "epoch:16 step:13157 [D loss: 0.362216, acc.: 82.81%] [G loss: 2.465832]\n",
      "epoch:16 step:13158 [D loss: 0.300356, acc.: 89.06%] [G loss: 2.688339]\n",
      "epoch:16 step:13159 [D loss: 0.368658, acc.: 83.59%] [G loss: 2.927840]\n",
      "epoch:16 step:13160 [D loss: 0.325152, acc.: 82.03%] [G loss: 4.101829]\n",
      "epoch:16 step:13161 [D loss: 0.412842, acc.: 79.69%] [G loss: 2.702528]\n",
      "epoch:16 step:13162 [D loss: 0.412969, acc.: 77.34%] [G loss: 3.541408]\n",
      "epoch:16 step:13163 [D loss: 0.268919, acc.: 89.84%] [G loss: 3.275948]\n",
      "epoch:16 step:13164 [D loss: 0.276370, acc.: 90.62%] [G loss: 3.237127]\n",
      "epoch:16 step:13165 [D loss: 0.392938, acc.: 82.03%] [G loss: 2.960502]\n",
      "epoch:16 step:13166 [D loss: 0.322284, acc.: 86.72%] [G loss: 2.603503]\n",
      "epoch:16 step:13167 [D loss: 0.302466, acc.: 85.16%] [G loss: 2.420988]\n",
      "epoch:16 step:13168 [D loss: 0.378779, acc.: 84.38%] [G loss: 3.035638]\n",
      "epoch:16 step:13169 [D loss: 0.371641, acc.: 84.38%] [G loss: 2.761137]\n",
      "epoch:16 step:13170 [D loss: 0.436631, acc.: 75.78%] [G loss: 2.705680]\n",
      "epoch:16 step:13171 [D loss: 0.347703, acc.: 85.16%] [G loss: 2.760850]\n",
      "epoch:16 step:13172 [D loss: 0.479250, acc.: 75.00%] [G loss: 2.798262]\n",
      "epoch:16 step:13173 [D loss: 0.421157, acc.: 82.81%] [G loss: 2.213785]\n",
      "epoch:16 step:13174 [D loss: 0.333311, acc.: 86.72%] [G loss: 2.724371]\n",
      "epoch:16 step:13175 [D loss: 0.313630, acc.: 89.84%] [G loss: 3.218345]\n",
      "epoch:16 step:13176 [D loss: 0.447926, acc.: 81.25%] [G loss: 2.036107]\n",
      "epoch:16 step:13177 [D loss: 0.289653, acc.: 91.41%] [G loss: 3.033260]\n",
      "epoch:16 step:13178 [D loss: 0.400027, acc.: 82.03%] [G loss: 2.269474]\n",
      "epoch:16 step:13179 [D loss: 0.236810, acc.: 92.19%] [G loss: 3.066947]\n",
      "epoch:16 step:13180 [D loss: 0.450429, acc.: 80.47%] [G loss: 2.674840]\n",
      "epoch:16 step:13181 [D loss: 0.374869, acc.: 80.47%] [G loss: 2.968163]\n",
      "epoch:16 step:13182 [D loss: 0.364871, acc.: 80.47%] [G loss: 2.811268]\n",
      "epoch:16 step:13183 [D loss: 0.310066, acc.: 87.50%] [G loss: 3.157905]\n",
      "epoch:16 step:13184 [D loss: 0.290157, acc.: 89.84%] [G loss: 3.051229]\n",
      "epoch:16 step:13185 [D loss: 0.414595, acc.: 83.59%] [G loss: 3.147468]\n",
      "epoch:16 step:13186 [D loss: 0.420306, acc.: 77.34%] [G loss: 3.069918]\n",
      "epoch:16 step:13187 [D loss: 0.311312, acc.: 86.72%] [G loss: 3.135261]\n",
      "epoch:16 step:13188 [D loss: 0.388604, acc.: 78.91%] [G loss: 2.421029]\n",
      "epoch:16 step:13189 [D loss: 0.313610, acc.: 85.16%] [G loss: 3.018734]\n",
      "epoch:16 step:13190 [D loss: 0.446608, acc.: 82.81%] [G loss: 3.098890]\n",
      "epoch:16 step:13191 [D loss: 0.338731, acc.: 85.94%] [G loss: 4.258800]\n",
      "epoch:16 step:13192 [D loss: 0.395551, acc.: 84.38%] [G loss: 2.177315]\n",
      "epoch:16 step:13193 [D loss: 0.351520, acc.: 85.16%] [G loss: 2.271721]\n",
      "epoch:16 step:13194 [D loss: 0.324944, acc.: 87.50%] [G loss: 2.281000]\n",
      "epoch:16 step:13195 [D loss: 0.359338, acc.: 86.72%] [G loss: 3.207637]\n",
      "epoch:16 step:13196 [D loss: 0.257039, acc.: 88.28%] [G loss: 3.330156]\n",
      "epoch:16 step:13197 [D loss: 0.448648, acc.: 82.81%] [G loss: 2.892684]\n",
      "epoch:16 step:13198 [D loss: 0.291744, acc.: 87.50%] [G loss: 2.589009]\n",
      "epoch:16 step:13199 [D loss: 0.333655, acc.: 86.72%] [G loss: 2.624181]\n",
      "epoch:16 step:13200 [D loss: 0.342070, acc.: 85.94%] [G loss: 2.606073]\n",
      "epoch:16 step:13201 [D loss: 0.299671, acc.: 89.84%] [G loss: 3.471415]\n",
      "epoch:16 step:13202 [D loss: 0.227212, acc.: 90.62%] [G loss: 3.234833]\n",
      "epoch:16 step:13203 [D loss: 0.433359, acc.: 78.91%] [G loss: 3.320366]\n",
      "epoch:16 step:13204 [D loss: 0.308342, acc.: 86.72%] [G loss: 3.524726]\n",
      "epoch:16 step:13205 [D loss: 0.284370, acc.: 89.84%] [G loss: 2.673648]\n",
      "epoch:16 step:13206 [D loss: 0.427251, acc.: 79.69%] [G loss: 2.892500]\n",
      "epoch:16 step:13207 [D loss: 0.295966, acc.: 87.50%] [G loss: 3.072063]\n",
      "epoch:16 step:13208 [D loss: 0.282974, acc.: 89.84%] [G loss: 2.661228]\n",
      "epoch:16 step:13209 [D loss: 0.387362, acc.: 80.47%] [G loss: 3.160415]\n",
      "epoch:16 step:13210 [D loss: 0.348798, acc.: 84.38%] [G loss: 2.972270]\n",
      "epoch:16 step:13211 [D loss: 0.311387, acc.: 89.84%] [G loss: 2.293891]\n",
      "epoch:16 step:13212 [D loss: 0.258523, acc.: 88.28%] [G loss: 2.727941]\n",
      "epoch:16 step:13213 [D loss: 0.335149, acc.: 80.47%] [G loss: 2.808385]\n",
      "epoch:16 step:13214 [D loss: 0.347265, acc.: 82.03%] [G loss: 3.474053]\n",
      "epoch:16 step:13215 [D loss: 0.314384, acc.: 85.94%] [G loss: 4.335520]\n",
      "epoch:16 step:13216 [D loss: 0.381482, acc.: 83.59%] [G loss: 7.801985]\n",
      "epoch:16 step:13217 [D loss: 0.374724, acc.: 83.59%] [G loss: 2.692774]\n",
      "epoch:16 step:13218 [D loss: 0.217363, acc.: 93.75%] [G loss: 4.630675]\n",
      "epoch:16 step:13219 [D loss: 0.267300, acc.: 86.72%] [G loss: 2.975274]\n",
      "epoch:16 step:13220 [D loss: 0.255000, acc.: 89.84%] [G loss: 7.415224]\n",
      "epoch:16 step:13221 [D loss: 0.227592, acc.: 91.41%] [G loss: 3.407779]\n",
      "epoch:16 step:13222 [D loss: 0.187927, acc.: 92.19%] [G loss: 3.433903]\n",
      "epoch:16 step:13223 [D loss: 0.312046, acc.: 84.38%] [G loss: 3.764039]\n",
      "epoch:16 step:13224 [D loss: 0.388050, acc.: 80.47%] [G loss: 4.431723]\n",
      "epoch:16 step:13225 [D loss: 0.458589, acc.: 78.12%] [G loss: 3.551654]\n",
      "epoch:16 step:13226 [D loss: 0.355924, acc.: 82.03%] [G loss: 2.735369]\n",
      "epoch:16 step:13227 [D loss: 0.311101, acc.: 85.94%] [G loss: 3.576831]\n",
      "epoch:16 step:13228 [D loss: 0.245731, acc.: 87.50%] [G loss: 4.050359]\n",
      "epoch:16 step:13229 [D loss: 0.328598, acc.: 86.72%] [G loss: 3.532014]\n",
      "epoch:16 step:13230 [D loss: 0.318726, acc.: 86.72%] [G loss: 2.848071]\n",
      "epoch:16 step:13231 [D loss: 0.358502, acc.: 84.38%] [G loss: 2.807121]\n",
      "epoch:16 step:13232 [D loss: 0.332152, acc.: 88.28%] [G loss: 4.763851]\n",
      "epoch:16 step:13233 [D loss: 0.311241, acc.: 84.38%] [G loss: 5.097255]\n",
      "epoch:16 step:13234 [D loss: 0.342631, acc.: 85.94%] [G loss: 2.990996]\n",
      "epoch:16 step:13235 [D loss: 0.388650, acc.: 80.47%] [G loss: 4.626734]\n",
      "epoch:16 step:13236 [D loss: 0.348127, acc.: 86.72%] [G loss: 4.727431]\n",
      "epoch:16 step:13237 [D loss: 0.239309, acc.: 88.28%] [G loss: 4.454228]\n",
      "epoch:16 step:13238 [D loss: 0.275905, acc.: 85.16%] [G loss: 4.794327]\n",
      "epoch:16 step:13239 [D loss: 0.215381, acc.: 91.41%] [G loss: 3.175409]\n",
      "epoch:16 step:13240 [D loss: 0.396539, acc.: 83.59%] [G loss: 3.389286]\n",
      "epoch:16 step:13241 [D loss: 0.403241, acc.: 82.03%] [G loss: 3.356047]\n",
      "epoch:16 step:13242 [D loss: 0.281272, acc.: 89.06%] [G loss: 3.413115]\n",
      "epoch:16 step:13243 [D loss: 0.277772, acc.: 89.84%] [G loss: 2.885667]\n",
      "epoch:16 step:13244 [D loss: 0.334224, acc.: 85.16%] [G loss: 2.525517]\n",
      "epoch:16 step:13245 [D loss: 0.388551, acc.: 85.16%] [G loss: 3.538703]\n",
      "epoch:16 step:13246 [D loss: 0.299168, acc.: 88.28%] [G loss: 4.226988]\n",
      "epoch:16 step:13247 [D loss: 0.325108, acc.: 84.38%] [G loss: 3.046422]\n",
      "epoch:16 step:13248 [D loss: 0.323900, acc.: 84.38%] [G loss: 3.462436]\n",
      "epoch:16 step:13249 [D loss: 0.297994, acc.: 84.38%] [G loss: 4.410501]\n",
      "epoch:16 step:13250 [D loss: 0.260599, acc.: 89.84%] [G loss: 3.575702]\n",
      "epoch:16 step:13251 [D loss: 0.277149, acc.: 88.28%] [G loss: 3.630738]\n",
      "epoch:16 step:13252 [D loss: 0.316247, acc.: 84.38%] [G loss: 3.715219]\n",
      "epoch:16 step:13253 [D loss: 0.303787, acc.: 85.94%] [G loss: 4.252195]\n",
      "epoch:16 step:13254 [D loss: 0.426313, acc.: 78.12%] [G loss: 3.493849]\n",
      "epoch:16 step:13255 [D loss: 0.289324, acc.: 89.06%] [G loss: 4.831141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:13256 [D loss: 0.306584, acc.: 90.62%] [G loss: 5.551374]\n",
      "epoch:16 step:13257 [D loss: 0.395940, acc.: 80.47%] [G loss: 2.917038]\n",
      "epoch:16 step:13258 [D loss: 0.330961, acc.: 83.59%] [G loss: 4.303084]\n",
      "epoch:16 step:13259 [D loss: 0.323674, acc.: 84.38%] [G loss: 3.708066]\n",
      "epoch:16 step:13260 [D loss: 0.396663, acc.: 81.25%] [G loss: 3.286019]\n",
      "epoch:16 step:13261 [D loss: 0.377210, acc.: 85.16%] [G loss: 3.265792]\n",
      "epoch:16 step:13262 [D loss: 0.440523, acc.: 78.12%] [G loss: 2.739651]\n",
      "epoch:16 step:13263 [D loss: 0.339845, acc.: 83.59%] [G loss: 2.635566]\n",
      "epoch:16 step:13264 [D loss: 0.383250, acc.: 82.81%] [G loss: 3.085179]\n",
      "epoch:16 step:13265 [D loss: 0.260742, acc.: 88.28%] [G loss: 2.954553]\n",
      "epoch:16 step:13266 [D loss: 0.302425, acc.: 87.50%] [G loss: 3.414799]\n",
      "epoch:16 step:13267 [D loss: 0.385731, acc.: 82.03%] [G loss: 2.916286]\n",
      "epoch:16 step:13268 [D loss: 0.358767, acc.: 88.28%] [G loss: 2.144587]\n",
      "epoch:16 step:13269 [D loss: 0.289323, acc.: 88.28%] [G loss: 2.960992]\n",
      "epoch:16 step:13270 [D loss: 0.432196, acc.: 80.47%] [G loss: 4.092876]\n",
      "epoch:16 step:13271 [D loss: 0.232328, acc.: 89.84%] [G loss: 4.680263]\n",
      "epoch:16 step:13272 [D loss: 0.370664, acc.: 84.38%] [G loss: 6.070212]\n",
      "epoch:16 step:13273 [D loss: 0.470255, acc.: 76.56%] [G loss: 3.781013]\n",
      "epoch:16 step:13274 [D loss: 0.404670, acc.: 80.47%] [G loss: 4.570595]\n",
      "epoch:16 step:13275 [D loss: 0.252303, acc.: 90.62%] [G loss: 3.694092]\n",
      "epoch:16 step:13276 [D loss: 0.339320, acc.: 82.81%] [G loss: 3.813769]\n",
      "epoch:16 step:13277 [D loss: 0.271159, acc.: 88.28%] [G loss: 3.819093]\n",
      "epoch:17 step:13278 [D loss: 0.292688, acc.: 84.38%] [G loss: 3.941764]\n",
      "epoch:17 step:13279 [D loss: 0.291310, acc.: 87.50%] [G loss: 3.745707]\n",
      "epoch:17 step:13280 [D loss: 0.474312, acc.: 73.44%] [G loss: 3.168184]\n",
      "epoch:17 step:13281 [D loss: 0.260627, acc.: 90.62%] [G loss: 3.461038]\n",
      "epoch:17 step:13282 [D loss: 0.262724, acc.: 87.50%] [G loss: 3.336725]\n",
      "epoch:17 step:13283 [D loss: 0.274374, acc.: 89.84%] [G loss: 3.203937]\n",
      "epoch:17 step:13284 [D loss: 0.174444, acc.: 92.19%] [G loss: 4.535953]\n",
      "epoch:17 step:13285 [D loss: 0.258097, acc.: 90.62%] [G loss: 4.869801]\n",
      "epoch:17 step:13286 [D loss: 0.354478, acc.: 87.50%] [G loss: 2.775730]\n",
      "epoch:17 step:13287 [D loss: 0.260954, acc.: 89.84%] [G loss: 3.171178]\n",
      "epoch:17 step:13288 [D loss: 0.293713, acc.: 89.84%] [G loss: 3.340568]\n",
      "epoch:17 step:13289 [D loss: 0.302918, acc.: 87.50%] [G loss: 3.057735]\n",
      "epoch:17 step:13290 [D loss: 0.333061, acc.: 87.50%] [G loss: 4.281792]\n",
      "epoch:17 step:13291 [D loss: 0.379997, acc.: 84.38%] [G loss: 4.900755]\n",
      "epoch:17 step:13292 [D loss: 0.371085, acc.: 85.16%] [G loss: 4.057364]\n",
      "epoch:17 step:13293 [D loss: 0.320952, acc.: 85.94%] [G loss: 2.494995]\n",
      "epoch:17 step:13294 [D loss: 0.208847, acc.: 91.41%] [G loss: 4.851705]\n",
      "epoch:17 step:13295 [D loss: 0.349619, acc.: 81.25%] [G loss: 3.287199]\n",
      "epoch:17 step:13296 [D loss: 0.254839, acc.: 88.28%] [G loss: 2.569717]\n",
      "epoch:17 step:13297 [D loss: 0.338214, acc.: 85.94%] [G loss: 2.864130]\n",
      "epoch:17 step:13298 [D loss: 0.313821, acc.: 84.38%] [G loss: 2.739518]\n",
      "epoch:17 step:13299 [D loss: 0.352625, acc.: 85.94%] [G loss: 2.957456]\n",
      "epoch:17 step:13300 [D loss: 0.339805, acc.: 84.38%] [G loss: 2.623767]\n",
      "epoch:17 step:13301 [D loss: 0.413525, acc.: 83.59%] [G loss: 2.666149]\n",
      "epoch:17 step:13302 [D loss: 0.309742, acc.: 86.72%] [G loss: 2.514241]\n",
      "epoch:17 step:13303 [D loss: 0.328696, acc.: 84.38%] [G loss: 2.653610]\n",
      "epoch:17 step:13304 [D loss: 0.354069, acc.: 86.72%] [G loss: 1.988153]\n",
      "epoch:17 step:13305 [D loss: 0.332236, acc.: 82.81%] [G loss: 3.791121]\n",
      "epoch:17 step:13306 [D loss: 0.381407, acc.: 82.81%] [G loss: 4.347096]\n",
      "epoch:17 step:13307 [D loss: 0.369778, acc.: 86.72%] [G loss: 3.050575]\n",
      "epoch:17 step:13308 [D loss: 0.238286, acc.: 91.41%] [G loss: 4.400484]\n",
      "epoch:17 step:13309 [D loss: 0.373237, acc.: 83.59%] [G loss: 2.669175]\n",
      "epoch:17 step:13310 [D loss: 0.289973, acc.: 86.72%] [G loss: 3.851413]\n",
      "epoch:17 step:13311 [D loss: 0.299349, acc.: 88.28%] [G loss: 2.783171]\n",
      "epoch:17 step:13312 [D loss: 0.370681, acc.: 83.59%] [G loss: 3.001464]\n",
      "epoch:17 step:13313 [D loss: 0.443314, acc.: 82.03%] [G loss: 2.744995]\n",
      "epoch:17 step:13314 [D loss: 0.415790, acc.: 78.12%] [G loss: 3.008543]\n",
      "epoch:17 step:13315 [D loss: 0.362941, acc.: 81.25%] [G loss: 4.179327]\n",
      "epoch:17 step:13316 [D loss: 0.676782, acc.: 70.31%] [G loss: 4.569102]\n",
      "epoch:17 step:13317 [D loss: 0.907716, acc.: 60.16%] [G loss: 5.487429]\n",
      "epoch:17 step:13318 [D loss: 0.451531, acc.: 79.69%] [G loss: 3.385964]\n",
      "epoch:17 step:13319 [D loss: 0.462960, acc.: 80.47%] [G loss: 6.228811]\n",
      "epoch:17 step:13320 [D loss: 0.393632, acc.: 81.25%] [G loss: 3.784357]\n",
      "epoch:17 step:13321 [D loss: 0.220189, acc.: 89.84%] [G loss: 2.705816]\n",
      "epoch:17 step:13322 [D loss: 0.361615, acc.: 85.16%] [G loss: 2.857162]\n",
      "epoch:17 step:13323 [D loss: 0.396828, acc.: 82.03%] [G loss: 3.208192]\n",
      "epoch:17 step:13324 [D loss: 0.391201, acc.: 79.69%] [G loss: 2.590569]\n",
      "epoch:17 step:13325 [D loss: 0.334764, acc.: 82.81%] [G loss: 2.483302]\n",
      "epoch:17 step:13326 [D loss: 0.335710, acc.: 84.38%] [G loss: 2.629169]\n",
      "epoch:17 step:13327 [D loss: 0.315961, acc.: 83.59%] [G loss: 2.892065]\n",
      "epoch:17 step:13328 [D loss: 0.333991, acc.: 88.28%] [G loss: 2.489594]\n",
      "epoch:17 step:13329 [D loss: 0.320676, acc.: 85.16%] [G loss: 2.317354]\n",
      "epoch:17 step:13330 [D loss: 0.293578, acc.: 88.28%] [G loss: 2.671956]\n",
      "epoch:17 step:13331 [D loss: 0.231822, acc.: 92.19%] [G loss: 2.700016]\n",
      "epoch:17 step:13332 [D loss: 0.209412, acc.: 91.41%] [G loss: 2.997332]\n",
      "epoch:17 step:13333 [D loss: 0.306170, acc.: 87.50%] [G loss: 4.073090]\n",
      "epoch:17 step:13334 [D loss: 0.335305, acc.: 84.38%] [G loss: 3.122640]\n",
      "epoch:17 step:13335 [D loss: 0.289590, acc.: 84.38%] [G loss: 3.703412]\n",
      "epoch:17 step:13336 [D loss: 0.355694, acc.: 83.59%] [G loss: 2.783629]\n",
      "epoch:17 step:13337 [D loss: 0.349050, acc.: 85.94%] [G loss: 2.930694]\n",
      "epoch:17 step:13338 [D loss: 0.367349, acc.: 85.16%] [G loss: 2.959947]\n",
      "epoch:17 step:13339 [D loss: 0.367232, acc.: 84.38%] [G loss: 2.518411]\n",
      "epoch:17 step:13340 [D loss: 0.326495, acc.: 86.72%] [G loss: 3.171863]\n",
      "epoch:17 step:13341 [D loss: 0.331579, acc.: 86.72%] [G loss: 3.235578]\n",
      "epoch:17 step:13342 [D loss: 0.383748, acc.: 81.25%] [G loss: 2.724049]\n",
      "epoch:17 step:13343 [D loss: 0.264863, acc.: 89.84%] [G loss: 3.994318]\n",
      "epoch:17 step:13344 [D loss: 0.332503, acc.: 85.16%] [G loss: 3.774158]\n",
      "epoch:17 step:13345 [D loss: 0.282314, acc.: 86.72%] [G loss: 3.751196]\n",
      "epoch:17 step:13346 [D loss: 0.326874, acc.: 83.59%] [G loss: 3.087497]\n",
      "epoch:17 step:13347 [D loss: 0.322100, acc.: 85.16%] [G loss: 2.967989]\n",
      "epoch:17 step:13348 [D loss: 0.333924, acc.: 85.16%] [G loss: 2.886266]\n",
      "epoch:17 step:13349 [D loss: 0.349975, acc.: 82.81%] [G loss: 3.168170]\n",
      "epoch:17 step:13350 [D loss: 0.335307, acc.: 83.59%] [G loss: 2.879474]\n",
      "epoch:17 step:13351 [D loss: 0.397066, acc.: 83.59%] [G loss: 4.266652]\n",
      "epoch:17 step:13352 [D loss: 0.265559, acc.: 88.28%] [G loss: 3.059652]\n",
      "epoch:17 step:13353 [D loss: 0.273470, acc.: 89.06%] [G loss: 2.853978]\n",
      "epoch:17 step:13354 [D loss: 0.388627, acc.: 82.03%] [G loss: 3.407657]\n",
      "epoch:17 step:13355 [D loss: 0.237009, acc.: 92.97%] [G loss: 3.421369]\n",
      "epoch:17 step:13356 [D loss: 0.427799, acc.: 81.25%] [G loss: 4.395710]\n",
      "epoch:17 step:13357 [D loss: 0.368323, acc.: 85.16%] [G loss: 3.127404]\n",
      "epoch:17 step:13358 [D loss: 0.246765, acc.: 90.62%] [G loss: 4.417410]\n",
      "epoch:17 step:13359 [D loss: 0.402711, acc.: 85.94%] [G loss: 5.776404]\n",
      "epoch:17 step:13360 [D loss: 0.455546, acc.: 80.47%] [G loss: 3.461178]\n",
      "epoch:17 step:13361 [D loss: 0.723647, acc.: 75.78%] [G loss: 8.693027]\n",
      "epoch:17 step:13362 [D loss: 0.726813, acc.: 75.00%] [G loss: 4.071869]\n",
      "epoch:17 step:13363 [D loss: 0.433664, acc.: 75.78%] [G loss: 3.176432]\n",
      "epoch:17 step:13364 [D loss: 0.352882, acc.: 86.72%] [G loss: 3.806398]\n",
      "epoch:17 step:13365 [D loss: 0.252042, acc.: 89.06%] [G loss: 4.280175]\n",
      "epoch:17 step:13366 [D loss: 0.340848, acc.: 85.94%] [G loss: 3.698369]\n",
      "epoch:17 step:13367 [D loss: 0.361741, acc.: 82.03%] [G loss: 2.962502]\n",
      "epoch:17 step:13368 [D loss: 0.294210, acc.: 87.50%] [G loss: 3.057432]\n",
      "epoch:17 step:13369 [D loss: 0.322787, acc.: 85.94%] [G loss: 4.135392]\n",
      "epoch:17 step:13370 [D loss: 0.391385, acc.: 84.38%] [G loss: 4.117856]\n",
      "epoch:17 step:13371 [D loss: 0.423016, acc.: 83.59%] [G loss: 2.582405]\n",
      "epoch:17 step:13372 [D loss: 0.524018, acc.: 75.78%] [G loss: 3.454526]\n",
      "epoch:17 step:13373 [D loss: 0.397796, acc.: 85.16%] [G loss: 3.190103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13374 [D loss: 0.482313, acc.: 78.12%] [G loss: 4.054393]\n",
      "epoch:17 step:13375 [D loss: 0.380490, acc.: 85.16%] [G loss: 2.851540]\n",
      "epoch:17 step:13376 [D loss: 0.357680, acc.: 85.16%] [G loss: 3.539293]\n",
      "epoch:17 step:13377 [D loss: 0.544801, acc.: 73.44%] [G loss: 2.304818]\n",
      "epoch:17 step:13378 [D loss: 0.320775, acc.: 87.50%] [G loss: 2.735151]\n",
      "epoch:17 step:13379 [D loss: 0.370079, acc.: 85.94%] [G loss: 3.065623]\n",
      "epoch:17 step:13380 [D loss: 0.374480, acc.: 82.81%] [G loss: 3.132834]\n",
      "epoch:17 step:13381 [D loss: 0.367937, acc.: 85.94%] [G loss: 3.034929]\n",
      "epoch:17 step:13382 [D loss: 0.293342, acc.: 83.59%] [G loss: 3.761278]\n",
      "epoch:17 step:13383 [D loss: 0.420626, acc.: 75.78%] [G loss: 2.583542]\n",
      "epoch:17 step:13384 [D loss: 0.447578, acc.: 82.03%] [G loss: 3.137932]\n",
      "epoch:17 step:13385 [D loss: 0.300669, acc.: 85.16%] [G loss: 3.169765]\n",
      "epoch:17 step:13386 [D loss: 0.319896, acc.: 84.38%] [G loss: 2.879751]\n",
      "epoch:17 step:13387 [D loss: 0.321201, acc.: 89.84%] [G loss: 2.767424]\n",
      "epoch:17 step:13388 [D loss: 0.287428, acc.: 89.06%] [G loss: 3.900664]\n",
      "epoch:17 step:13389 [D loss: 0.242839, acc.: 89.84%] [G loss: 3.632077]\n",
      "epoch:17 step:13390 [D loss: 0.379490, acc.: 84.38%] [G loss: 4.136098]\n",
      "epoch:17 step:13391 [D loss: 0.382690, acc.: 86.72%] [G loss: 3.232262]\n",
      "epoch:17 step:13392 [D loss: 0.395170, acc.: 82.03%] [G loss: 3.083868]\n",
      "epoch:17 step:13393 [D loss: 0.364945, acc.: 82.03%] [G loss: 2.963348]\n",
      "epoch:17 step:13394 [D loss: 0.326030, acc.: 85.16%] [G loss: 4.010581]\n",
      "epoch:17 step:13395 [D loss: 0.290638, acc.: 89.06%] [G loss: 3.843605]\n",
      "epoch:17 step:13396 [D loss: 0.273354, acc.: 89.06%] [G loss: 3.350174]\n",
      "epoch:17 step:13397 [D loss: 0.375997, acc.: 84.38%] [G loss: 3.163805]\n",
      "epoch:17 step:13398 [D loss: 0.322523, acc.: 86.72%] [G loss: 2.757719]\n",
      "epoch:17 step:13399 [D loss: 0.349700, acc.: 78.91%] [G loss: 3.426031]\n",
      "epoch:17 step:13400 [D loss: 0.315348, acc.: 84.38%] [G loss: 3.432848]\n",
      "epoch:17 step:13401 [D loss: 0.430338, acc.: 81.25%] [G loss: 2.157727]\n",
      "epoch:17 step:13402 [D loss: 0.442078, acc.: 76.56%] [G loss: 2.712033]\n",
      "epoch:17 step:13403 [D loss: 0.290146, acc.: 91.41%] [G loss: 3.611577]\n",
      "epoch:17 step:13404 [D loss: 0.338143, acc.: 85.16%] [G loss: 3.293164]\n",
      "epoch:17 step:13405 [D loss: 0.310224, acc.: 86.72%] [G loss: 3.315554]\n",
      "epoch:17 step:13406 [D loss: 0.357429, acc.: 83.59%] [G loss: 3.282431]\n",
      "epoch:17 step:13407 [D loss: 0.336199, acc.: 89.06%] [G loss: 2.198112]\n",
      "epoch:17 step:13408 [D loss: 0.337614, acc.: 85.94%] [G loss: 3.046951]\n",
      "epoch:17 step:13409 [D loss: 0.474988, acc.: 78.12%] [G loss: 2.713825]\n",
      "epoch:17 step:13410 [D loss: 0.442980, acc.: 82.81%] [G loss: 3.213905]\n",
      "epoch:17 step:13411 [D loss: 0.294267, acc.: 87.50%] [G loss: 3.860362]\n",
      "epoch:17 step:13412 [D loss: 0.287190, acc.: 84.38%] [G loss: 4.422461]\n",
      "epoch:17 step:13413 [D loss: 0.274034, acc.: 85.94%] [G loss: 2.703031]\n",
      "epoch:17 step:13414 [D loss: 0.316477, acc.: 81.25%] [G loss: 3.962990]\n",
      "epoch:17 step:13415 [D loss: 0.276564, acc.: 89.06%] [G loss: 6.922668]\n",
      "epoch:17 step:13416 [D loss: 0.352221, acc.: 85.16%] [G loss: 3.252102]\n",
      "epoch:17 step:13417 [D loss: 0.249637, acc.: 88.28%] [G loss: 3.334603]\n",
      "epoch:17 step:13418 [D loss: 0.344578, acc.: 85.94%] [G loss: 6.537099]\n",
      "epoch:17 step:13419 [D loss: 0.487922, acc.: 75.00%] [G loss: 2.933171]\n",
      "epoch:17 step:13420 [D loss: 0.311476, acc.: 85.16%] [G loss: 4.289827]\n",
      "epoch:17 step:13421 [D loss: 0.398259, acc.: 82.81%] [G loss: 3.764913]\n",
      "epoch:17 step:13422 [D loss: 0.376023, acc.: 82.81%] [G loss: 2.754127]\n",
      "epoch:17 step:13423 [D loss: 0.390195, acc.: 80.47%] [G loss: 3.397075]\n",
      "epoch:17 step:13424 [D loss: 0.302894, acc.: 88.28%] [G loss: 4.506895]\n",
      "epoch:17 step:13425 [D loss: 0.435168, acc.: 77.34%] [G loss: 3.731722]\n",
      "epoch:17 step:13426 [D loss: 0.276605, acc.: 88.28%] [G loss: 3.118450]\n",
      "epoch:17 step:13427 [D loss: 0.353499, acc.: 83.59%] [G loss: 4.199265]\n",
      "epoch:17 step:13428 [D loss: 0.319407, acc.: 84.38%] [G loss: 3.936633]\n",
      "epoch:17 step:13429 [D loss: 0.283783, acc.: 90.62%] [G loss: 3.797351]\n",
      "epoch:17 step:13430 [D loss: 0.243039, acc.: 89.84%] [G loss: 3.148462]\n",
      "epoch:17 step:13431 [D loss: 0.434951, acc.: 83.59%] [G loss: 3.528000]\n",
      "epoch:17 step:13432 [D loss: 0.223659, acc.: 90.62%] [G loss: 5.048159]\n",
      "epoch:17 step:13433 [D loss: 0.311343, acc.: 88.28%] [G loss: 3.197635]\n",
      "epoch:17 step:13434 [D loss: 0.295893, acc.: 89.06%] [G loss: 2.957219]\n",
      "epoch:17 step:13435 [D loss: 0.400678, acc.: 80.47%] [G loss: 3.376016]\n",
      "epoch:17 step:13436 [D loss: 0.275433, acc.: 87.50%] [G loss: 3.803637]\n",
      "epoch:17 step:13437 [D loss: 0.467115, acc.: 82.03%] [G loss: 2.564375]\n",
      "epoch:17 step:13438 [D loss: 0.362649, acc.: 83.59%] [G loss: 2.754158]\n",
      "epoch:17 step:13439 [D loss: 0.280180, acc.: 87.50%] [G loss: 3.008357]\n",
      "epoch:17 step:13440 [D loss: 0.257722, acc.: 91.41%] [G loss: 3.297321]\n",
      "epoch:17 step:13441 [D loss: 0.326716, acc.: 85.16%] [G loss: 3.077482]\n",
      "epoch:17 step:13442 [D loss: 0.388953, acc.: 84.38%] [G loss: 2.950873]\n",
      "epoch:17 step:13443 [D loss: 0.415793, acc.: 85.16%] [G loss: 2.405181]\n",
      "epoch:17 step:13444 [D loss: 0.416556, acc.: 82.81%] [G loss: 2.884787]\n",
      "epoch:17 step:13445 [D loss: 0.552134, acc.: 78.12%] [G loss: 3.052819]\n",
      "epoch:17 step:13446 [D loss: 0.365409, acc.: 82.03%] [G loss: 3.572984]\n",
      "epoch:17 step:13447 [D loss: 0.387029, acc.: 81.25%] [G loss: 3.301137]\n",
      "epoch:17 step:13448 [D loss: 0.370217, acc.: 84.38%] [G loss: 5.559865]\n",
      "epoch:17 step:13449 [D loss: 0.670212, acc.: 71.09%] [G loss: 4.736581]\n",
      "epoch:17 step:13450 [D loss: 0.857052, acc.: 71.88%] [G loss: 8.389388]\n",
      "epoch:17 step:13451 [D loss: 2.089907, acc.: 60.94%] [G loss: 4.248543]\n",
      "epoch:17 step:13452 [D loss: 0.622034, acc.: 75.00%] [G loss: 4.022115]\n",
      "epoch:17 step:13453 [D loss: 0.553740, acc.: 79.69%] [G loss: 3.914185]\n",
      "epoch:17 step:13454 [D loss: 0.565698, acc.: 78.12%] [G loss: 3.058272]\n",
      "epoch:17 step:13455 [D loss: 0.435201, acc.: 84.38%] [G loss: 3.579702]\n",
      "epoch:17 step:13456 [D loss: 0.370643, acc.: 83.59%] [G loss: 2.913256]\n",
      "epoch:17 step:13457 [D loss: 0.316881, acc.: 88.28%] [G loss: 4.617464]\n",
      "epoch:17 step:13458 [D loss: 0.311861, acc.: 86.72%] [G loss: 4.955635]\n",
      "epoch:17 step:13459 [D loss: 0.394488, acc.: 82.81%] [G loss: 2.746597]\n",
      "epoch:17 step:13460 [D loss: 0.426800, acc.: 83.59%] [G loss: 3.122270]\n",
      "epoch:17 step:13461 [D loss: 0.528138, acc.: 75.78%] [G loss: 2.245105]\n",
      "epoch:17 step:13462 [D loss: 0.293219, acc.: 88.28%] [G loss: 2.375906]\n",
      "epoch:17 step:13463 [D loss: 0.260807, acc.: 90.62%] [G loss: 4.502933]\n",
      "epoch:17 step:13464 [D loss: 0.240544, acc.: 88.28%] [G loss: 3.937508]\n",
      "epoch:17 step:13465 [D loss: 0.329828, acc.: 85.94%] [G loss: 2.899693]\n",
      "epoch:17 step:13466 [D loss: 0.310595, acc.: 86.72%] [G loss: 3.731800]\n",
      "epoch:17 step:13467 [D loss: 0.302613, acc.: 87.50%] [G loss: 2.749965]\n",
      "epoch:17 step:13468 [D loss: 0.281038, acc.: 89.84%] [G loss: 2.665504]\n",
      "epoch:17 step:13469 [D loss: 0.302085, acc.: 83.59%] [G loss: 4.704019]\n",
      "epoch:17 step:13470 [D loss: 0.365282, acc.: 82.03%] [G loss: 3.214085]\n",
      "epoch:17 step:13471 [D loss: 0.315647, acc.: 87.50%] [G loss: 3.136879]\n",
      "epoch:17 step:13472 [D loss: 0.320388, acc.: 85.94%] [G loss: 3.084085]\n",
      "epoch:17 step:13473 [D loss: 0.277586, acc.: 89.84%] [G loss: 2.255707]\n",
      "epoch:17 step:13474 [D loss: 0.300937, acc.: 87.50%] [G loss: 2.813363]\n",
      "epoch:17 step:13475 [D loss: 0.364095, acc.: 85.94%] [G loss: 2.633325]\n",
      "epoch:17 step:13476 [D loss: 0.305092, acc.: 89.06%] [G loss: 2.317880]\n",
      "epoch:17 step:13477 [D loss: 0.299634, acc.: 89.84%] [G loss: 2.771363]\n",
      "epoch:17 step:13478 [D loss: 0.308027, acc.: 85.94%] [G loss: 2.691426]\n",
      "epoch:17 step:13479 [D loss: 0.361128, acc.: 85.16%] [G loss: 2.254873]\n",
      "epoch:17 step:13480 [D loss: 0.408213, acc.: 79.69%] [G loss: 2.215645]\n",
      "epoch:17 step:13481 [D loss: 0.342337, acc.: 81.25%] [G loss: 2.809741]\n",
      "epoch:17 step:13482 [D loss: 0.338479, acc.: 89.06%] [G loss: 2.667483]\n",
      "epoch:17 step:13483 [D loss: 0.443655, acc.: 82.03%] [G loss: 2.446252]\n",
      "epoch:17 step:13484 [D loss: 0.361602, acc.: 82.81%] [G loss: 3.275616]\n",
      "epoch:17 step:13485 [D loss: 0.482571, acc.: 80.47%] [G loss: 2.310606]\n",
      "epoch:17 step:13486 [D loss: 0.476691, acc.: 78.12%] [G loss: 3.114688]\n",
      "epoch:17 step:13487 [D loss: 0.394721, acc.: 80.47%] [G loss: 2.962576]\n",
      "epoch:17 step:13488 [D loss: 0.582514, acc.: 72.66%] [G loss: 2.215828]\n",
      "epoch:17 step:13489 [D loss: 0.297163, acc.: 88.28%] [G loss: 2.789803]\n",
      "epoch:17 step:13490 [D loss: 0.317517, acc.: 85.16%] [G loss: 2.568463]\n",
      "epoch:17 step:13491 [D loss: 0.391730, acc.: 80.47%] [G loss: 2.611362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13492 [D loss: 0.304901, acc.: 85.94%] [G loss: 2.191765]\n",
      "epoch:17 step:13493 [D loss: 0.381903, acc.: 85.94%] [G loss: 2.469456]\n",
      "epoch:17 step:13494 [D loss: 0.378568, acc.: 84.38%] [G loss: 2.575651]\n",
      "epoch:17 step:13495 [D loss: 0.262487, acc.: 87.50%] [G loss: 3.393680]\n",
      "epoch:17 step:13496 [D loss: 0.357766, acc.: 86.72%] [G loss: 2.804822]\n",
      "epoch:17 step:13497 [D loss: 0.358829, acc.: 82.81%] [G loss: 3.371483]\n",
      "epoch:17 step:13498 [D loss: 0.291184, acc.: 82.81%] [G loss: 5.198864]\n",
      "epoch:17 step:13499 [D loss: 0.399288, acc.: 82.03%] [G loss: 3.903030]\n",
      "epoch:17 step:13500 [D loss: 0.253194, acc.: 89.06%] [G loss: 3.840949]\n",
      "epoch:17 step:13501 [D loss: 0.285372, acc.: 83.59%] [G loss: 2.806977]\n",
      "epoch:17 step:13502 [D loss: 0.378025, acc.: 82.81%] [G loss: 3.158300]\n",
      "epoch:17 step:13503 [D loss: 0.356236, acc.: 85.16%] [G loss: 3.507621]\n",
      "epoch:17 step:13504 [D loss: 0.377095, acc.: 82.81%] [G loss: 2.907749]\n",
      "epoch:17 step:13505 [D loss: 0.217636, acc.: 89.84%] [G loss: 2.724544]\n",
      "epoch:17 step:13506 [D loss: 0.294980, acc.: 82.81%] [G loss: 3.019768]\n",
      "epoch:17 step:13507 [D loss: 0.363540, acc.: 85.94%] [G loss: 2.555741]\n",
      "epoch:17 step:13508 [D loss: 0.333741, acc.: 83.59%] [G loss: 4.153505]\n",
      "epoch:17 step:13509 [D loss: 0.252202, acc.: 91.41%] [G loss: 2.782529]\n",
      "epoch:17 step:13510 [D loss: 0.316321, acc.: 85.16%] [G loss: 2.566914]\n",
      "epoch:17 step:13511 [D loss: 0.277693, acc.: 89.84%] [G loss: 3.200636]\n",
      "epoch:17 step:13512 [D loss: 0.354938, acc.: 82.81%] [G loss: 3.164880]\n",
      "epoch:17 step:13513 [D loss: 0.334330, acc.: 85.94%] [G loss: 2.665867]\n",
      "epoch:17 step:13514 [D loss: 0.355672, acc.: 87.50%] [G loss: 2.946334]\n",
      "epoch:17 step:13515 [D loss: 0.382781, acc.: 81.25%] [G loss: 3.021862]\n",
      "epoch:17 step:13516 [D loss: 0.446093, acc.: 82.03%] [G loss: 2.674313]\n",
      "epoch:17 step:13517 [D loss: 0.289681, acc.: 84.38%] [G loss: 3.483331]\n",
      "epoch:17 step:13518 [D loss: 0.420878, acc.: 77.34%] [G loss: 2.373464]\n",
      "epoch:17 step:13519 [D loss: 0.464273, acc.: 78.91%] [G loss: 3.114512]\n",
      "epoch:17 step:13520 [D loss: 0.359944, acc.: 81.25%] [G loss: 3.198039]\n",
      "epoch:17 step:13521 [D loss: 0.294072, acc.: 85.16%] [G loss: 3.192361]\n",
      "epoch:17 step:13522 [D loss: 0.389065, acc.: 85.94%] [G loss: 4.804876]\n",
      "epoch:17 step:13523 [D loss: 0.321591, acc.: 88.28%] [G loss: 3.292634]\n",
      "epoch:17 step:13524 [D loss: 0.195715, acc.: 92.19%] [G loss: 3.892214]\n",
      "epoch:17 step:13525 [D loss: 0.295752, acc.: 89.84%] [G loss: 3.922752]\n",
      "epoch:17 step:13526 [D loss: 0.379969, acc.: 77.34%] [G loss: 3.212844]\n",
      "epoch:17 step:13527 [D loss: 0.474615, acc.: 75.00%] [G loss: 2.890403]\n",
      "epoch:17 step:13528 [D loss: 0.422894, acc.: 80.47%] [G loss: 3.496182]\n",
      "epoch:17 step:13529 [D loss: 0.410010, acc.: 80.47%] [G loss: 3.829442]\n",
      "epoch:17 step:13530 [D loss: 0.243195, acc.: 89.84%] [G loss: 4.265822]\n",
      "epoch:17 step:13531 [D loss: 0.206233, acc.: 92.19%] [G loss: 3.168683]\n",
      "epoch:17 step:13532 [D loss: 0.317633, acc.: 86.72%] [G loss: 3.243106]\n",
      "epoch:17 step:13533 [D loss: 0.281280, acc.: 86.72%] [G loss: 4.610803]\n",
      "epoch:17 step:13534 [D loss: 0.391154, acc.: 83.59%] [G loss: 2.983705]\n",
      "epoch:17 step:13535 [D loss: 0.262188, acc.: 86.72%] [G loss: 3.613529]\n",
      "epoch:17 step:13536 [D loss: 0.270526, acc.: 90.62%] [G loss: 3.381272]\n",
      "epoch:17 step:13537 [D loss: 0.328623, acc.: 85.16%] [G loss: 3.468694]\n",
      "epoch:17 step:13538 [D loss: 0.381154, acc.: 83.59%] [G loss: 3.203739]\n",
      "epoch:17 step:13539 [D loss: 0.252400, acc.: 87.50%] [G loss: 3.411012]\n",
      "epoch:17 step:13540 [D loss: 0.263843, acc.: 89.06%] [G loss: 2.958997]\n",
      "epoch:17 step:13541 [D loss: 0.319131, acc.: 85.16%] [G loss: 2.282619]\n",
      "epoch:17 step:13542 [D loss: 0.331913, acc.: 85.94%] [G loss: 2.266084]\n",
      "epoch:17 step:13543 [D loss: 0.300605, acc.: 87.50%] [G loss: 3.126481]\n",
      "epoch:17 step:13544 [D loss: 0.379705, acc.: 83.59%] [G loss: 2.744963]\n",
      "epoch:17 step:13545 [D loss: 0.394326, acc.: 80.47%] [G loss: 2.676777]\n",
      "epoch:17 step:13546 [D loss: 0.285497, acc.: 90.62%] [G loss: 2.908271]\n",
      "epoch:17 step:13547 [D loss: 0.359870, acc.: 84.38%] [G loss: 2.918037]\n",
      "epoch:17 step:13548 [D loss: 0.424038, acc.: 82.03%] [G loss: 2.648854]\n",
      "epoch:17 step:13549 [D loss: 0.464820, acc.: 78.91%] [G loss: 2.957951]\n",
      "epoch:17 step:13550 [D loss: 0.247751, acc.: 87.50%] [G loss: 5.128027]\n",
      "epoch:17 step:13551 [D loss: 0.314721, acc.: 83.59%] [G loss: 4.692809]\n",
      "epoch:17 step:13552 [D loss: 0.278539, acc.: 85.16%] [G loss: 4.679237]\n",
      "epoch:17 step:13553 [D loss: 0.332836, acc.: 87.50%] [G loss: 2.919561]\n",
      "epoch:17 step:13554 [D loss: 0.381257, acc.: 80.47%] [G loss: 2.856360]\n",
      "epoch:17 step:13555 [D loss: 0.357609, acc.: 87.50%] [G loss: 2.968825]\n",
      "epoch:17 step:13556 [D loss: 0.283173, acc.: 90.62%] [G loss: 2.896364]\n",
      "epoch:17 step:13557 [D loss: 0.254684, acc.: 88.28%] [G loss: 3.771952]\n",
      "epoch:17 step:13558 [D loss: 0.301886, acc.: 85.16%] [G loss: 2.578668]\n",
      "epoch:17 step:13559 [D loss: 0.266762, acc.: 89.84%] [G loss: 3.312438]\n",
      "epoch:17 step:13560 [D loss: 0.235265, acc.: 90.62%] [G loss: 5.559896]\n",
      "epoch:17 step:13561 [D loss: 0.393806, acc.: 85.94%] [G loss: 2.522765]\n",
      "epoch:17 step:13562 [D loss: 0.294026, acc.: 89.06%] [G loss: 2.818467]\n",
      "epoch:17 step:13563 [D loss: 0.282086, acc.: 89.84%] [G loss: 3.401213]\n",
      "epoch:17 step:13564 [D loss: 0.302114, acc.: 85.16%] [G loss: 2.851166]\n",
      "epoch:17 step:13565 [D loss: 0.345330, acc.: 82.81%] [G loss: 2.896358]\n",
      "epoch:17 step:13566 [D loss: 0.292232, acc.: 87.50%] [G loss: 2.856568]\n",
      "epoch:17 step:13567 [D loss: 0.397806, acc.: 82.81%] [G loss: 1.916492]\n",
      "epoch:17 step:13568 [D loss: 0.294085, acc.: 86.72%] [G loss: 2.479531]\n",
      "epoch:17 step:13569 [D loss: 0.315921, acc.: 85.94%] [G loss: 2.859210]\n",
      "epoch:17 step:13570 [D loss: 0.359511, acc.: 85.16%] [G loss: 3.559137]\n",
      "epoch:17 step:13571 [D loss: 0.299065, acc.: 86.72%] [G loss: 3.991060]\n",
      "epoch:17 step:13572 [D loss: 0.351482, acc.: 82.81%] [G loss: 2.449508]\n",
      "epoch:17 step:13573 [D loss: 0.508394, acc.: 78.12%] [G loss: 5.419106]\n",
      "epoch:17 step:13574 [D loss: 0.512332, acc.: 85.16%] [G loss: 7.546050]\n",
      "epoch:17 step:13575 [D loss: 0.573585, acc.: 77.34%] [G loss: 3.131931]\n",
      "epoch:17 step:13576 [D loss: 0.342423, acc.: 82.81%] [G loss: 2.430107]\n",
      "epoch:17 step:13577 [D loss: 0.306053, acc.: 89.06%] [G loss: 2.619204]\n",
      "epoch:17 step:13578 [D loss: 0.339072, acc.: 85.16%] [G loss: 3.526722]\n",
      "epoch:17 step:13579 [D loss: 0.358169, acc.: 83.59%] [G loss: 3.107037]\n",
      "epoch:17 step:13580 [D loss: 0.343708, acc.: 83.59%] [G loss: 2.885839]\n",
      "epoch:17 step:13581 [D loss: 0.325948, acc.: 85.94%] [G loss: 2.853294]\n",
      "epoch:17 step:13582 [D loss: 0.319005, acc.: 84.38%] [G loss: 2.369133]\n",
      "epoch:17 step:13583 [D loss: 0.341492, acc.: 85.94%] [G loss: 3.009461]\n",
      "epoch:17 step:13584 [D loss: 0.420540, acc.: 79.69%] [G loss: 2.522247]\n",
      "epoch:17 step:13585 [D loss: 0.319742, acc.: 88.28%] [G loss: 2.469626]\n",
      "epoch:17 step:13586 [D loss: 0.288658, acc.: 88.28%] [G loss: 2.218293]\n",
      "epoch:17 step:13587 [D loss: 0.256814, acc.: 89.84%] [G loss: 2.983525]\n",
      "epoch:17 step:13588 [D loss: 0.338812, acc.: 85.16%] [G loss: 2.389252]\n",
      "epoch:17 step:13589 [D loss: 0.361046, acc.: 81.25%] [G loss: 3.347955]\n",
      "epoch:17 step:13590 [D loss: 0.277847, acc.: 85.94%] [G loss: 2.710737]\n",
      "epoch:17 step:13591 [D loss: 0.316149, acc.: 83.59%] [G loss: 3.096025]\n",
      "epoch:17 step:13592 [D loss: 0.487824, acc.: 76.56%] [G loss: 4.487424]\n",
      "epoch:17 step:13593 [D loss: 0.450397, acc.: 80.47%] [G loss: 3.611296]\n",
      "epoch:17 step:13594 [D loss: 0.393748, acc.: 80.47%] [G loss: 2.583162]\n",
      "epoch:17 step:13595 [D loss: 0.375493, acc.: 83.59%] [G loss: 2.930229]\n",
      "epoch:17 step:13596 [D loss: 0.279608, acc.: 89.06%] [G loss: 2.506622]\n",
      "epoch:17 step:13597 [D loss: 0.336888, acc.: 84.38%] [G loss: 2.894379]\n",
      "epoch:17 step:13598 [D loss: 0.394644, acc.: 79.69%] [G loss: 3.133282]\n",
      "epoch:17 step:13599 [D loss: 0.305536, acc.: 85.94%] [G loss: 2.869562]\n",
      "epoch:17 step:13600 [D loss: 0.254945, acc.: 89.06%] [G loss: 2.909929]\n",
      "epoch:17 step:13601 [D loss: 0.369512, acc.: 81.25%] [G loss: 2.591515]\n",
      "epoch:17 step:13602 [D loss: 0.400491, acc.: 80.47%] [G loss: 2.254106]\n",
      "epoch:17 step:13603 [D loss: 0.313433, acc.: 85.16%] [G loss: 2.140406]\n",
      "epoch:17 step:13604 [D loss: 0.308459, acc.: 85.94%] [G loss: 3.066127]\n",
      "epoch:17 step:13605 [D loss: 0.370663, acc.: 84.38%] [G loss: 2.522268]\n",
      "epoch:17 step:13606 [D loss: 0.278122, acc.: 87.50%] [G loss: 3.616446]\n",
      "epoch:17 step:13607 [D loss: 0.335032, acc.: 86.72%] [G loss: 3.613776]\n",
      "epoch:17 step:13608 [D loss: 0.386795, acc.: 84.38%] [G loss: 2.351017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13609 [D loss: 0.388913, acc.: 85.16%] [G loss: 2.584124]\n",
      "epoch:17 step:13610 [D loss: 0.281441, acc.: 89.84%] [G loss: 3.015630]\n",
      "epoch:17 step:13611 [D loss: 0.400709, acc.: 80.47%] [G loss: 2.507481]\n",
      "epoch:17 step:13612 [D loss: 0.350408, acc.: 85.94%] [G loss: 2.557642]\n",
      "epoch:17 step:13613 [D loss: 0.358061, acc.: 85.94%] [G loss: 2.505309]\n",
      "epoch:17 step:13614 [D loss: 0.408860, acc.: 84.38%] [G loss: 2.601041]\n",
      "epoch:17 step:13615 [D loss: 0.429040, acc.: 82.81%] [G loss: 3.274862]\n",
      "epoch:17 step:13616 [D loss: 0.311569, acc.: 91.41%] [G loss: 3.376760]\n",
      "epoch:17 step:13617 [D loss: 0.450985, acc.: 78.12%] [G loss: 2.414782]\n",
      "epoch:17 step:13618 [D loss: 0.359699, acc.: 87.50%] [G loss: 2.400372]\n",
      "epoch:17 step:13619 [D loss: 0.307856, acc.: 89.84%] [G loss: 3.208207]\n",
      "epoch:17 step:13620 [D loss: 0.310379, acc.: 87.50%] [G loss: 2.964498]\n",
      "epoch:17 step:13621 [D loss: 0.207711, acc.: 89.06%] [G loss: 6.157988]\n",
      "epoch:17 step:13622 [D loss: 0.172003, acc.: 94.53%] [G loss: 5.958505]\n",
      "epoch:17 step:13623 [D loss: 0.355205, acc.: 82.81%] [G loss: 4.683556]\n",
      "epoch:17 step:13624 [D loss: 0.225504, acc.: 92.97%] [G loss: 4.147761]\n",
      "epoch:17 step:13625 [D loss: 0.349648, acc.: 85.16%] [G loss: 2.935211]\n",
      "epoch:17 step:13626 [D loss: 0.258520, acc.: 89.06%] [G loss: 3.675628]\n",
      "epoch:17 step:13627 [D loss: 0.288264, acc.: 87.50%] [G loss: 4.154932]\n",
      "epoch:17 step:13628 [D loss: 0.375192, acc.: 84.38%] [G loss: 3.372388]\n",
      "epoch:17 step:13629 [D loss: 0.527091, acc.: 77.34%] [G loss: 2.905093]\n",
      "epoch:17 step:13630 [D loss: 0.463916, acc.: 75.78%] [G loss: 2.954729]\n",
      "epoch:17 step:13631 [D loss: 0.267015, acc.: 90.62%] [G loss: 2.680224]\n",
      "epoch:17 step:13632 [D loss: 0.310932, acc.: 88.28%] [G loss: 3.432522]\n",
      "epoch:17 step:13633 [D loss: 0.336638, acc.: 86.72%] [G loss: 3.462158]\n",
      "epoch:17 step:13634 [D loss: 0.438750, acc.: 79.69%] [G loss: 5.004889]\n",
      "epoch:17 step:13635 [D loss: 0.436410, acc.: 85.16%] [G loss: 4.397977]\n",
      "epoch:17 step:13636 [D loss: 0.398614, acc.: 82.03%] [G loss: 2.835651]\n",
      "epoch:17 step:13637 [D loss: 0.399383, acc.: 84.38%] [G loss: 4.403336]\n",
      "epoch:17 step:13638 [D loss: 0.404402, acc.: 82.81%] [G loss: 4.818372]\n",
      "epoch:17 step:13639 [D loss: 0.272943, acc.: 90.62%] [G loss: 2.588861]\n",
      "epoch:17 step:13640 [D loss: 0.235819, acc.: 90.62%] [G loss: 3.607946]\n",
      "epoch:17 step:13641 [D loss: 0.402925, acc.: 75.78%] [G loss: 3.544184]\n",
      "epoch:17 step:13642 [D loss: 0.220825, acc.: 89.06%] [G loss: 4.175686]\n",
      "epoch:17 step:13643 [D loss: 0.304522, acc.: 87.50%] [G loss: 3.532628]\n",
      "epoch:17 step:13644 [D loss: 0.265088, acc.: 86.72%] [G loss: 4.244954]\n",
      "epoch:17 step:13645 [D loss: 0.323134, acc.: 86.72%] [G loss: 3.270765]\n",
      "epoch:17 step:13646 [D loss: 0.314082, acc.: 83.59%] [G loss: 3.935805]\n",
      "epoch:17 step:13647 [D loss: 0.395383, acc.: 82.81%] [G loss: 3.602288]\n",
      "epoch:17 step:13648 [D loss: 0.281120, acc.: 88.28%] [G loss: 5.227234]\n",
      "epoch:17 step:13649 [D loss: 0.270729, acc.: 89.06%] [G loss: 3.366354]\n",
      "epoch:17 step:13650 [D loss: 0.322128, acc.: 83.59%] [G loss: 5.093183]\n",
      "epoch:17 step:13651 [D loss: 0.454977, acc.: 78.12%] [G loss: 2.796069]\n",
      "epoch:17 step:13652 [D loss: 0.293364, acc.: 88.28%] [G loss: 2.278119]\n",
      "epoch:17 step:13653 [D loss: 0.395115, acc.: 85.16%] [G loss: 3.599705]\n",
      "epoch:17 step:13654 [D loss: 0.267430, acc.: 89.06%] [G loss: 5.524042]\n",
      "epoch:17 step:13655 [D loss: 0.298421, acc.: 85.16%] [G loss: 4.300645]\n",
      "epoch:17 step:13656 [D loss: 0.324452, acc.: 82.03%] [G loss: 3.299282]\n",
      "epoch:17 step:13657 [D loss: 0.279601, acc.: 91.41%] [G loss: 4.173782]\n",
      "epoch:17 step:13658 [D loss: 0.305789, acc.: 86.72%] [G loss: 3.075117]\n",
      "epoch:17 step:13659 [D loss: 0.345838, acc.: 81.25%] [G loss: 3.209451]\n",
      "epoch:17 step:13660 [D loss: 0.249030, acc.: 92.19%] [G loss: 3.551377]\n",
      "epoch:17 step:13661 [D loss: 0.278446, acc.: 87.50%] [G loss: 3.443928]\n",
      "epoch:17 step:13662 [D loss: 0.242752, acc.: 89.06%] [G loss: 2.730195]\n",
      "epoch:17 step:13663 [D loss: 0.344054, acc.: 83.59%] [G loss: 2.694678]\n",
      "epoch:17 step:13664 [D loss: 0.452155, acc.: 77.34%] [G loss: 2.598380]\n",
      "epoch:17 step:13665 [D loss: 0.309200, acc.: 84.38%] [G loss: 2.859890]\n",
      "epoch:17 step:13666 [D loss: 0.288920, acc.: 85.16%] [G loss: 2.946834]\n",
      "epoch:17 step:13667 [D loss: 0.349680, acc.: 84.38%] [G loss: 3.376622]\n",
      "epoch:17 step:13668 [D loss: 0.362993, acc.: 85.16%] [G loss: 2.728546]\n",
      "epoch:17 step:13669 [D loss: 0.245619, acc.: 89.06%] [G loss: 3.861877]\n",
      "epoch:17 step:13670 [D loss: 0.268104, acc.: 93.75%] [G loss: 3.144284]\n",
      "epoch:17 step:13671 [D loss: 0.350482, acc.: 85.16%] [G loss: 2.952510]\n",
      "epoch:17 step:13672 [D loss: 0.440849, acc.: 78.12%] [G loss: 3.126218]\n",
      "epoch:17 step:13673 [D loss: 0.364831, acc.: 83.59%] [G loss: 2.616362]\n",
      "epoch:17 step:13674 [D loss: 0.346918, acc.: 84.38%] [G loss: 2.914784]\n",
      "epoch:17 step:13675 [D loss: 0.269434, acc.: 89.06%] [G loss: 2.864362]\n",
      "epoch:17 step:13676 [D loss: 0.315906, acc.: 86.72%] [G loss: 3.708363]\n",
      "epoch:17 step:13677 [D loss: 0.407482, acc.: 80.47%] [G loss: 2.249638]\n",
      "epoch:17 step:13678 [D loss: 0.411451, acc.: 78.91%] [G loss: 2.721066]\n",
      "epoch:17 step:13679 [D loss: 0.278240, acc.: 89.84%] [G loss: 3.186981]\n",
      "epoch:17 step:13680 [D loss: 0.393326, acc.: 79.69%] [G loss: 4.247390]\n",
      "epoch:17 step:13681 [D loss: 0.225645, acc.: 92.19%] [G loss: 9.975918]\n",
      "epoch:17 step:13682 [D loss: 0.255726, acc.: 88.28%] [G loss: 4.753341]\n",
      "epoch:17 step:13683 [D loss: 0.286784, acc.: 89.84%] [G loss: 3.738402]\n",
      "epoch:17 step:13684 [D loss: 0.251652, acc.: 88.28%] [G loss: 5.383864]\n",
      "epoch:17 step:13685 [D loss: 0.306565, acc.: 86.72%] [G loss: 2.851496]\n",
      "epoch:17 step:13686 [D loss: 0.331310, acc.: 85.94%] [G loss: 3.268330]\n",
      "epoch:17 step:13687 [D loss: 0.375150, acc.: 82.03%] [G loss: 2.751008]\n",
      "epoch:17 step:13688 [D loss: 0.256086, acc.: 89.84%] [G loss: 3.264064]\n",
      "epoch:17 step:13689 [D loss: 0.303683, acc.: 86.72%] [G loss: 3.161801]\n",
      "epoch:17 step:13690 [D loss: 0.405833, acc.: 81.25%] [G loss: 3.089016]\n",
      "epoch:17 step:13691 [D loss: 0.303185, acc.: 89.84%] [G loss: 3.622337]\n",
      "epoch:17 step:13692 [D loss: 0.264264, acc.: 90.62%] [G loss: 2.661751]\n",
      "epoch:17 step:13693 [D loss: 0.379364, acc.: 80.47%] [G loss: 3.359557]\n",
      "epoch:17 step:13694 [D loss: 0.245898, acc.: 89.06%] [G loss: 4.310041]\n",
      "epoch:17 step:13695 [D loss: 0.247369, acc.: 89.84%] [G loss: 4.280063]\n",
      "epoch:17 step:13696 [D loss: 0.334460, acc.: 85.16%] [G loss: 2.784748]\n",
      "epoch:17 step:13697 [D loss: 0.300147, acc.: 85.16%] [G loss: 2.996135]\n",
      "epoch:17 step:13698 [D loss: 0.243822, acc.: 90.62%] [G loss: 3.091371]\n",
      "epoch:17 step:13699 [D loss: 0.313159, acc.: 88.28%] [G loss: 3.265557]\n",
      "epoch:17 step:13700 [D loss: 0.290005, acc.: 85.94%] [G loss: 3.117344]\n",
      "epoch:17 step:13701 [D loss: 0.312291, acc.: 85.16%] [G loss: 3.063259]\n",
      "epoch:17 step:13702 [D loss: 0.311498, acc.: 87.50%] [G loss: 3.691741]\n",
      "epoch:17 step:13703 [D loss: 0.328254, acc.: 85.94%] [G loss: 3.482193]\n",
      "epoch:17 step:13704 [D loss: 0.259871, acc.: 88.28%] [G loss: 3.660782]\n",
      "epoch:17 step:13705 [D loss: 0.269972, acc.: 87.50%] [G loss: 3.895691]\n",
      "epoch:17 step:13706 [D loss: 0.323366, acc.: 86.72%] [G loss: 2.749626]\n",
      "epoch:17 step:13707 [D loss: 0.465843, acc.: 78.91%] [G loss: 3.118772]\n",
      "epoch:17 step:13708 [D loss: 0.372977, acc.: 83.59%] [G loss: 3.525369]\n",
      "epoch:17 step:13709 [D loss: 0.324168, acc.: 85.16%] [G loss: 2.650728]\n",
      "epoch:17 step:13710 [D loss: 0.267725, acc.: 88.28%] [G loss: 3.738041]\n",
      "epoch:17 step:13711 [D loss: 0.280478, acc.: 87.50%] [G loss: 4.190276]\n",
      "epoch:17 step:13712 [D loss: 0.387298, acc.: 77.34%] [G loss: 2.739493]\n",
      "epoch:17 step:13713 [D loss: 0.300323, acc.: 85.94%] [G loss: 2.739094]\n",
      "epoch:17 step:13714 [D loss: 0.274653, acc.: 87.50%] [G loss: 5.312260]\n",
      "epoch:17 step:13715 [D loss: 0.349336, acc.: 84.38%] [G loss: 3.277186]\n",
      "epoch:17 step:13716 [D loss: 0.295145, acc.: 91.41%] [G loss: 4.646825]\n",
      "epoch:17 step:13717 [D loss: 0.163783, acc.: 94.53%] [G loss: 5.505084]\n",
      "epoch:17 step:13718 [D loss: 0.366914, acc.: 85.94%] [G loss: 3.741559]\n",
      "epoch:17 step:13719 [D loss: 0.297654, acc.: 90.62%] [G loss: 3.688822]\n",
      "epoch:17 step:13720 [D loss: 0.304004, acc.: 85.94%] [G loss: 3.477106]\n",
      "epoch:17 step:13721 [D loss: 0.323086, acc.: 85.94%] [G loss: 3.370900]\n",
      "epoch:17 step:13722 [D loss: 0.328719, acc.: 82.81%] [G loss: 3.550248]\n",
      "epoch:17 step:13723 [D loss: 0.300815, acc.: 87.50%] [G loss: 3.599379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13724 [D loss: 0.210954, acc.: 92.19%] [G loss: 3.529488]\n",
      "epoch:17 step:13725 [D loss: 0.337480, acc.: 82.81%] [G loss: 3.716540]\n",
      "epoch:17 step:13726 [D loss: 0.377935, acc.: 82.03%] [G loss: 3.591167]\n",
      "epoch:17 step:13727 [D loss: 0.362236, acc.: 85.94%] [G loss: 2.536278]\n",
      "epoch:17 step:13728 [D loss: 0.300209, acc.: 88.28%] [G loss: 2.997198]\n",
      "epoch:17 step:13729 [D loss: 0.349191, acc.: 85.94%] [G loss: 2.973455]\n",
      "epoch:17 step:13730 [D loss: 0.318071, acc.: 85.94%] [G loss: 2.849000]\n",
      "epoch:17 step:13731 [D loss: 0.369869, acc.: 82.03%] [G loss: 3.956796]\n",
      "epoch:17 step:13732 [D loss: 0.412426, acc.: 76.56%] [G loss: 5.409847]\n",
      "epoch:17 step:13733 [D loss: 0.516744, acc.: 77.34%] [G loss: 7.577930]\n",
      "epoch:17 step:13734 [D loss: 1.196770, acc.: 71.88%] [G loss: 7.898819]\n",
      "epoch:17 step:13735 [D loss: 2.460284, acc.: 46.09%] [G loss: 2.312106]\n",
      "epoch:17 step:13736 [D loss: 0.700707, acc.: 78.91%] [G loss: 4.105097]\n",
      "epoch:17 step:13737 [D loss: 0.533108, acc.: 82.81%] [G loss: 3.891186]\n",
      "epoch:17 step:13738 [D loss: 0.354557, acc.: 83.59%] [G loss: 2.445296]\n",
      "epoch:17 step:13739 [D loss: 0.369825, acc.: 86.72%] [G loss: 2.645155]\n",
      "epoch:17 step:13740 [D loss: 0.479962, acc.: 77.34%] [G loss: 2.929888]\n",
      "epoch:17 step:13741 [D loss: 0.271090, acc.: 89.84%] [G loss: 2.794145]\n",
      "epoch:17 step:13742 [D loss: 0.294761, acc.: 85.94%] [G loss: 2.913894]\n",
      "epoch:17 step:13743 [D loss: 0.369946, acc.: 85.94%] [G loss: 2.747160]\n",
      "epoch:17 step:13744 [D loss: 0.288392, acc.: 85.94%] [G loss: 2.861702]\n",
      "epoch:17 step:13745 [D loss: 0.280869, acc.: 85.94%] [G loss: 3.515737]\n",
      "epoch:17 step:13746 [D loss: 0.238118, acc.: 89.84%] [G loss: 3.587967]\n",
      "epoch:17 step:13747 [D loss: 0.280657, acc.: 91.41%] [G loss: 2.715819]\n",
      "epoch:17 step:13748 [D loss: 0.276679, acc.: 89.06%] [G loss: 2.742472]\n",
      "epoch:17 step:13749 [D loss: 0.374241, acc.: 83.59%] [G loss: 2.693440]\n",
      "epoch:17 step:13750 [D loss: 0.439596, acc.: 75.00%] [G loss: 3.590279]\n",
      "epoch:17 step:13751 [D loss: 0.393577, acc.: 86.72%] [G loss: 3.464258]\n",
      "epoch:17 step:13752 [D loss: 0.353796, acc.: 84.38%] [G loss: 3.820450]\n",
      "epoch:17 step:13753 [D loss: 0.228716, acc.: 91.41%] [G loss: 4.828807]\n",
      "epoch:17 step:13754 [D loss: 0.265656, acc.: 89.84%] [G loss: 3.162783]\n",
      "epoch:17 step:13755 [D loss: 0.244450, acc.: 87.50%] [G loss: 3.054466]\n",
      "epoch:17 step:13756 [D loss: 0.349601, acc.: 88.28%] [G loss: 2.527603]\n",
      "epoch:17 step:13757 [D loss: 0.227745, acc.: 91.41%] [G loss: 3.017040]\n",
      "epoch:17 step:13758 [D loss: 0.331867, acc.: 85.16%] [G loss: 2.457603]\n",
      "epoch:17 step:13759 [D loss: 0.400143, acc.: 82.81%] [G loss: 3.013525]\n",
      "epoch:17 step:13760 [D loss: 0.255315, acc.: 89.84%] [G loss: 4.860281]\n",
      "epoch:17 step:13761 [D loss: 0.345777, acc.: 84.38%] [G loss: 3.290867]\n",
      "epoch:17 step:13762 [D loss: 0.225986, acc.: 91.41%] [G loss: 4.498972]\n",
      "epoch:17 step:13763 [D loss: 0.468897, acc.: 77.34%] [G loss: 2.699661]\n",
      "epoch:17 step:13764 [D loss: 0.318232, acc.: 86.72%] [G loss: 2.466444]\n",
      "epoch:17 step:13765 [D loss: 0.276968, acc.: 85.94%] [G loss: 2.337697]\n",
      "epoch:17 step:13766 [D loss: 0.407692, acc.: 80.47%] [G loss: 2.998818]\n",
      "epoch:17 step:13767 [D loss: 0.400049, acc.: 83.59%] [G loss: 2.498264]\n",
      "epoch:17 step:13768 [D loss: 0.409070, acc.: 79.69%] [G loss: 3.368877]\n",
      "epoch:17 step:13769 [D loss: 0.423474, acc.: 82.81%] [G loss: 3.158526]\n",
      "epoch:17 step:13770 [D loss: 0.477518, acc.: 78.91%] [G loss: 3.005576]\n",
      "epoch:17 step:13771 [D loss: 0.331006, acc.: 85.16%] [G loss: 2.558393]\n",
      "epoch:17 step:13772 [D loss: 0.415928, acc.: 80.47%] [G loss: 3.388232]\n",
      "epoch:17 step:13773 [D loss: 0.529127, acc.: 73.44%] [G loss: 3.065376]\n",
      "epoch:17 step:13774 [D loss: 0.298142, acc.: 87.50%] [G loss: 3.298906]\n",
      "epoch:17 step:13775 [D loss: 0.289387, acc.: 89.06%] [G loss: 4.139634]\n",
      "epoch:17 step:13776 [D loss: 0.368044, acc.: 82.81%] [G loss: 4.157166]\n",
      "epoch:17 step:13777 [D loss: 0.274570, acc.: 87.50%] [G loss: 4.360525]\n",
      "epoch:17 step:13778 [D loss: 0.345317, acc.: 85.94%] [G loss: 4.553658]\n",
      "epoch:17 step:13779 [D loss: 0.331176, acc.: 85.94%] [G loss: 2.902023]\n",
      "epoch:17 step:13780 [D loss: 0.220309, acc.: 92.19%] [G loss: 4.771174]\n",
      "epoch:17 step:13781 [D loss: 0.413104, acc.: 82.03%] [G loss: 3.064818]\n",
      "epoch:17 step:13782 [D loss: 0.286651, acc.: 89.06%] [G loss: 3.165121]\n",
      "epoch:17 step:13783 [D loss: 0.254928, acc.: 90.62%] [G loss: 2.973235]\n",
      "epoch:17 step:13784 [D loss: 0.347900, acc.: 85.16%] [G loss: 2.977050]\n",
      "epoch:17 step:13785 [D loss: 0.260687, acc.: 89.06%] [G loss: 5.163409]\n",
      "epoch:17 step:13786 [D loss: 0.322635, acc.: 85.94%] [G loss: 2.795398]\n",
      "epoch:17 step:13787 [D loss: 0.225886, acc.: 92.19%] [G loss: 3.736269]\n",
      "epoch:17 step:13788 [D loss: 0.418069, acc.: 80.47%] [G loss: 5.533679]\n",
      "epoch:17 step:13789 [D loss: 0.364006, acc.: 81.25%] [G loss: 3.106737]\n",
      "epoch:17 step:13790 [D loss: 0.237595, acc.: 89.84%] [G loss: 5.924196]\n",
      "epoch:17 step:13791 [D loss: 0.186558, acc.: 92.97%] [G loss: 5.484818]\n",
      "epoch:17 step:13792 [D loss: 0.264265, acc.: 91.41%] [G loss: 5.943273]\n",
      "epoch:17 step:13793 [D loss: 0.383277, acc.: 82.81%] [G loss: 3.647979]\n",
      "epoch:17 step:13794 [D loss: 0.274541, acc.: 88.28%] [G loss: 4.881137]\n",
      "epoch:17 step:13795 [D loss: 0.280880, acc.: 89.84%] [G loss: 2.123163]\n",
      "epoch:17 step:13796 [D loss: 0.289663, acc.: 85.94%] [G loss: 5.480574]\n",
      "epoch:17 step:13797 [D loss: 0.201948, acc.: 93.75%] [G loss: 4.468563]\n",
      "epoch:17 step:13798 [D loss: 0.355523, acc.: 82.03%] [G loss: 3.060394]\n",
      "epoch:17 step:13799 [D loss: 0.338624, acc.: 85.94%] [G loss: 2.900623]\n",
      "epoch:17 step:13800 [D loss: 0.351410, acc.: 85.16%] [G loss: 2.763385]\n",
      "epoch:17 step:13801 [D loss: 0.420139, acc.: 78.91%] [G loss: 3.726964]\n",
      "epoch:17 step:13802 [D loss: 0.272996, acc.: 88.28%] [G loss: 4.327854]\n",
      "epoch:17 step:13803 [D loss: 0.415668, acc.: 82.81%] [G loss: 4.686996]\n",
      "epoch:17 step:13804 [D loss: 0.368005, acc.: 82.03%] [G loss: 2.605865]\n",
      "epoch:17 step:13805 [D loss: 0.406379, acc.: 85.94%] [G loss: 3.312908]\n",
      "epoch:17 step:13806 [D loss: 0.357809, acc.: 79.69%] [G loss: 3.966481]\n",
      "epoch:17 step:13807 [D loss: 0.273983, acc.: 89.84%] [G loss: 4.544738]\n",
      "epoch:17 step:13808 [D loss: 0.250188, acc.: 90.62%] [G loss: 3.856383]\n",
      "epoch:17 step:13809 [D loss: 0.280562, acc.: 87.50%] [G loss: 4.053854]\n",
      "epoch:17 step:13810 [D loss: 0.487124, acc.: 82.03%] [G loss: 3.475585]\n",
      "epoch:17 step:13811 [D loss: 0.285986, acc.: 87.50%] [G loss: 3.830230]\n",
      "epoch:17 step:13812 [D loss: 0.340446, acc.: 82.03%] [G loss: 5.513725]\n",
      "epoch:17 step:13813 [D loss: 0.347496, acc.: 86.72%] [G loss: 4.126298]\n",
      "epoch:17 step:13814 [D loss: 0.387683, acc.: 79.69%] [G loss: 2.721796]\n",
      "epoch:17 step:13815 [D loss: 0.192553, acc.: 92.97%] [G loss: 3.346059]\n",
      "epoch:17 step:13816 [D loss: 0.285211, acc.: 89.06%] [G loss: 3.040327]\n",
      "epoch:17 step:13817 [D loss: 0.298290, acc.: 86.72%] [G loss: 3.186592]\n",
      "epoch:17 step:13818 [D loss: 0.337635, acc.: 85.94%] [G loss: 2.752140]\n",
      "epoch:17 step:13819 [D loss: 0.373626, acc.: 81.25%] [G loss: 3.691611]\n",
      "epoch:17 step:13820 [D loss: 0.237026, acc.: 87.50%] [G loss: 4.197984]\n",
      "epoch:17 step:13821 [D loss: 0.282592, acc.: 90.62%] [G loss: 4.765213]\n",
      "epoch:17 step:13822 [D loss: 0.343633, acc.: 86.72%] [G loss: 4.399801]\n",
      "epoch:17 step:13823 [D loss: 0.295840, acc.: 89.84%] [G loss: 3.730243]\n",
      "epoch:17 step:13824 [D loss: 0.321333, acc.: 82.03%] [G loss: 4.057385]\n",
      "epoch:17 step:13825 [D loss: 0.255608, acc.: 88.28%] [G loss: 2.918779]\n",
      "epoch:17 step:13826 [D loss: 0.343555, acc.: 86.72%] [G loss: 3.675190]\n",
      "epoch:17 step:13827 [D loss: 0.282786, acc.: 86.72%] [G loss: 3.656107]\n",
      "epoch:17 step:13828 [D loss: 0.400516, acc.: 81.25%] [G loss: 4.100888]\n",
      "epoch:17 step:13829 [D loss: 0.313209, acc.: 87.50%] [G loss: 4.620755]\n",
      "epoch:17 step:13830 [D loss: 0.261533, acc.: 88.28%] [G loss: 4.010086]\n",
      "epoch:17 step:13831 [D loss: 0.412426, acc.: 79.69%] [G loss: 3.378006]\n",
      "epoch:17 step:13832 [D loss: 0.351582, acc.: 85.94%] [G loss: 3.608823]\n",
      "epoch:17 step:13833 [D loss: 0.296398, acc.: 89.84%] [G loss: 2.975107]\n",
      "epoch:17 step:13834 [D loss: 0.304566, acc.: 87.50%] [G loss: 2.125965]\n",
      "epoch:17 step:13835 [D loss: 0.391645, acc.: 84.38%] [G loss: 2.247207]\n",
      "epoch:17 step:13836 [D loss: 0.314210, acc.: 86.72%] [G loss: 3.358819]\n",
      "epoch:17 step:13837 [D loss: 0.362025, acc.: 83.59%] [G loss: 3.118825]\n",
      "epoch:17 step:13838 [D loss: 0.350790, acc.: 85.16%] [G loss: 4.623960]\n",
      "epoch:17 step:13839 [D loss: 0.373130, acc.: 78.91%] [G loss: 4.613231]\n",
      "epoch:17 step:13840 [D loss: 0.338091, acc.: 88.28%] [G loss: 3.412777]\n",
      "epoch:17 step:13841 [D loss: 0.304497, acc.: 87.50%] [G loss: 2.588785]\n",
      "epoch:17 step:13842 [D loss: 0.418305, acc.: 77.34%] [G loss: 3.169618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13843 [D loss: 0.220401, acc.: 92.19%] [G loss: 3.962183]\n",
      "epoch:17 step:13844 [D loss: 0.314156, acc.: 83.59%] [G loss: 3.287622]\n",
      "epoch:17 step:13845 [D loss: 0.443268, acc.: 78.91%] [G loss: 2.745557]\n",
      "epoch:17 step:13846 [D loss: 0.351094, acc.: 85.16%] [G loss: 2.734626]\n",
      "epoch:17 step:13847 [D loss: 0.276237, acc.: 88.28%] [G loss: 3.799936]\n",
      "epoch:17 step:13848 [D loss: 0.349534, acc.: 82.03%] [G loss: 3.427312]\n",
      "epoch:17 step:13849 [D loss: 0.296180, acc.: 88.28%] [G loss: 3.603390]\n",
      "epoch:17 step:13850 [D loss: 0.301752, acc.: 82.81%] [G loss: 3.263742]\n",
      "epoch:17 step:13851 [D loss: 0.238949, acc.: 88.28%] [G loss: 3.610134]\n",
      "epoch:17 step:13852 [D loss: 0.341707, acc.: 84.38%] [G loss: 3.691389]\n",
      "epoch:17 step:13853 [D loss: 0.236605, acc.: 89.06%] [G loss: 3.374667]\n",
      "epoch:17 step:13854 [D loss: 0.394111, acc.: 82.81%] [G loss: 2.983409]\n",
      "epoch:17 step:13855 [D loss: 0.461641, acc.: 79.69%] [G loss: 2.967467]\n",
      "epoch:17 step:13856 [D loss: 0.311375, acc.: 86.72%] [G loss: 2.916488]\n",
      "epoch:17 step:13857 [D loss: 0.321240, acc.: 86.72%] [G loss: 2.921109]\n",
      "epoch:17 step:13858 [D loss: 0.289119, acc.: 86.72%] [G loss: 2.617648]\n",
      "epoch:17 step:13859 [D loss: 0.318663, acc.: 88.28%] [G loss: 2.977844]\n",
      "epoch:17 step:13860 [D loss: 0.306744, acc.: 88.28%] [G loss: 4.098885]\n",
      "epoch:17 step:13861 [D loss: 0.237766, acc.: 87.50%] [G loss: 2.665998]\n",
      "epoch:17 step:13862 [D loss: 0.287195, acc.: 85.16%] [G loss: 2.728131]\n",
      "epoch:17 step:13863 [D loss: 0.298787, acc.: 88.28%] [G loss: 3.617948]\n",
      "epoch:17 step:13864 [D loss: 0.232973, acc.: 90.62%] [G loss: 3.796073]\n",
      "epoch:17 step:13865 [D loss: 0.321516, acc.: 83.59%] [G loss: 4.757801]\n",
      "epoch:17 step:13866 [D loss: 0.271206, acc.: 88.28%] [G loss: 3.295321]\n",
      "epoch:17 step:13867 [D loss: 0.261990, acc.: 90.62%] [G loss: 4.251737]\n",
      "epoch:17 step:13868 [D loss: 0.251434, acc.: 89.06%] [G loss: 2.962285]\n",
      "epoch:17 step:13869 [D loss: 0.253072, acc.: 86.72%] [G loss: 3.755334]\n",
      "epoch:17 step:13870 [D loss: 0.235748, acc.: 89.06%] [G loss: 3.812023]\n",
      "epoch:17 step:13871 [D loss: 0.388550, acc.: 86.72%] [G loss: 3.977391]\n",
      "epoch:17 step:13872 [D loss: 0.215081, acc.: 92.97%] [G loss: 4.165266]\n",
      "epoch:17 step:13873 [D loss: 0.226895, acc.: 91.41%] [G loss: 5.793232]\n",
      "epoch:17 step:13874 [D loss: 0.337131, acc.: 86.72%] [G loss: 4.430222]\n",
      "epoch:17 step:13875 [D loss: 0.249820, acc.: 90.62%] [G loss: 3.235907]\n",
      "epoch:17 step:13876 [D loss: 0.285041, acc.: 89.06%] [G loss: 3.396105]\n",
      "epoch:17 step:13877 [D loss: 0.308744, acc.: 85.16%] [G loss: 3.413337]\n",
      "epoch:17 step:13878 [D loss: 0.403549, acc.: 81.25%] [G loss: 5.284738]\n",
      "epoch:17 step:13879 [D loss: 0.595701, acc.: 71.09%] [G loss: 4.715166]\n",
      "epoch:17 step:13880 [D loss: 0.367130, acc.: 85.94%] [G loss: 3.203380]\n",
      "epoch:17 step:13881 [D loss: 0.282917, acc.: 89.06%] [G loss: 3.916714]\n",
      "epoch:17 step:13882 [D loss: 0.276842, acc.: 89.06%] [G loss: 3.161980]\n",
      "epoch:17 step:13883 [D loss: 0.367803, acc.: 83.59%] [G loss: 3.513600]\n",
      "epoch:17 step:13884 [D loss: 0.349455, acc.: 82.81%] [G loss: 3.109612]\n",
      "epoch:17 step:13885 [D loss: 0.315875, acc.: 85.16%] [G loss: 4.054283]\n",
      "epoch:17 step:13886 [D loss: 0.287466, acc.: 90.62%] [G loss: 3.677201]\n",
      "epoch:17 step:13887 [D loss: 0.316874, acc.: 86.72%] [G loss: 3.636395]\n",
      "epoch:17 step:13888 [D loss: 0.315122, acc.: 86.72%] [G loss: 3.272703]\n",
      "epoch:17 step:13889 [D loss: 0.372397, acc.: 85.94%] [G loss: 2.617051]\n",
      "epoch:17 step:13890 [D loss: 0.268013, acc.: 86.72%] [G loss: 4.052833]\n",
      "epoch:17 step:13891 [D loss: 0.286341, acc.: 88.28%] [G loss: 4.202030]\n",
      "epoch:17 step:13892 [D loss: 0.292962, acc.: 87.50%] [G loss: 4.392950]\n",
      "epoch:17 step:13893 [D loss: 0.221294, acc.: 90.62%] [G loss: 4.751137]\n",
      "epoch:17 step:13894 [D loss: 0.281202, acc.: 86.72%] [G loss: 3.954025]\n",
      "epoch:17 step:13895 [D loss: 0.324901, acc.: 84.38%] [G loss: 4.018295]\n",
      "epoch:17 step:13896 [D loss: 0.234847, acc.: 89.06%] [G loss: 3.419872]\n",
      "epoch:17 step:13897 [D loss: 0.287053, acc.: 85.94%] [G loss: 3.280045]\n",
      "epoch:17 step:13898 [D loss: 0.169806, acc.: 94.53%] [G loss: 3.856184]\n",
      "epoch:17 step:13899 [D loss: 0.209987, acc.: 91.41%] [G loss: 3.629818]\n",
      "epoch:17 step:13900 [D loss: 0.295413, acc.: 89.06%] [G loss: 3.156138]\n",
      "epoch:17 step:13901 [D loss: 0.274769, acc.: 89.06%] [G loss: 3.478439]\n",
      "epoch:17 step:13902 [D loss: 0.310410, acc.: 85.94%] [G loss: 2.911199]\n",
      "epoch:17 step:13903 [D loss: 0.264431, acc.: 89.06%] [G loss: 3.509319]\n",
      "epoch:17 step:13904 [D loss: 0.260658, acc.: 86.72%] [G loss: 2.938271]\n",
      "epoch:17 step:13905 [D loss: 0.343785, acc.: 82.03%] [G loss: 6.145378]\n",
      "epoch:17 step:13906 [D loss: 0.601891, acc.: 76.56%] [G loss: 2.538979]\n",
      "epoch:17 step:13907 [D loss: 0.290622, acc.: 87.50%] [G loss: 4.232039]\n",
      "epoch:17 step:13908 [D loss: 0.256055, acc.: 91.41%] [G loss: 7.602890]\n",
      "epoch:17 step:13909 [D loss: 0.195502, acc.: 93.75%] [G loss: 5.360323]\n",
      "epoch:17 step:13910 [D loss: 0.435229, acc.: 80.47%] [G loss: 4.338614]\n",
      "epoch:17 step:13911 [D loss: 0.488495, acc.: 78.12%] [G loss: 3.461534]\n",
      "epoch:17 step:13912 [D loss: 0.460223, acc.: 79.69%] [G loss: 2.989167]\n",
      "epoch:17 step:13913 [D loss: 0.348109, acc.: 80.47%] [G loss: 4.152097]\n",
      "epoch:17 step:13914 [D loss: 0.312313, acc.: 88.28%] [G loss: 3.271082]\n",
      "epoch:17 step:13915 [D loss: 0.253313, acc.: 91.41%] [G loss: 4.653292]\n",
      "epoch:17 step:13916 [D loss: 0.367936, acc.: 84.38%] [G loss: 2.918908]\n",
      "epoch:17 step:13917 [D loss: 0.323102, acc.: 83.59%] [G loss: 3.679120]\n",
      "epoch:17 step:13918 [D loss: 0.340497, acc.: 86.72%] [G loss: 4.290759]\n",
      "epoch:17 step:13919 [D loss: 0.277052, acc.: 85.16%] [G loss: 2.876091]\n",
      "epoch:17 step:13920 [D loss: 0.275352, acc.: 89.84%] [G loss: 2.703373]\n",
      "epoch:17 step:13921 [D loss: 0.321840, acc.: 84.38%] [G loss: 2.114364]\n",
      "epoch:17 step:13922 [D loss: 0.319097, acc.: 88.28%] [G loss: 2.834133]\n",
      "epoch:17 step:13923 [D loss: 0.303923, acc.: 88.28%] [G loss: 3.997402]\n",
      "epoch:17 step:13924 [D loss: 0.263068, acc.: 89.84%] [G loss: 3.669362]\n",
      "epoch:17 step:13925 [D loss: 0.248037, acc.: 89.84%] [G loss: 3.231496]\n",
      "epoch:17 step:13926 [D loss: 0.287789, acc.: 90.62%] [G loss: 2.305778]\n",
      "epoch:17 step:13927 [D loss: 0.396890, acc.: 85.16%] [G loss: 2.963877]\n",
      "epoch:17 step:13928 [D loss: 0.308831, acc.: 89.06%] [G loss: 2.943553]\n",
      "epoch:17 step:13929 [D loss: 0.258857, acc.: 85.16%] [G loss: 2.965148]\n",
      "epoch:17 step:13930 [D loss: 0.249906, acc.: 90.62%] [G loss: 2.552260]\n",
      "epoch:17 step:13931 [D loss: 0.371180, acc.: 85.16%] [G loss: 2.451988]\n",
      "epoch:17 step:13932 [D loss: 0.355000, acc.: 84.38%] [G loss: 3.313251]\n",
      "epoch:17 step:13933 [D loss: 0.370327, acc.: 82.81%] [G loss: 2.786272]\n",
      "epoch:17 step:13934 [D loss: 0.378866, acc.: 81.25%] [G loss: 2.892118]\n",
      "epoch:17 step:13935 [D loss: 0.246286, acc.: 90.62%] [G loss: 2.518466]\n",
      "epoch:17 step:13936 [D loss: 0.226734, acc.: 95.31%] [G loss: 2.406016]\n",
      "epoch:17 step:13937 [D loss: 0.329290, acc.: 84.38%] [G loss: 3.015671]\n",
      "epoch:17 step:13938 [D loss: 0.422850, acc.: 79.69%] [G loss: 2.365675]\n",
      "epoch:17 step:13939 [D loss: 0.425327, acc.: 82.81%] [G loss: 2.937415]\n",
      "epoch:17 step:13940 [D loss: 0.270296, acc.: 88.28%] [G loss: 3.169131]\n",
      "epoch:17 step:13941 [D loss: 0.329288, acc.: 84.38%] [G loss: 3.092807]\n",
      "epoch:17 step:13942 [D loss: 0.300648, acc.: 87.50%] [G loss: 3.101320]\n",
      "epoch:17 step:13943 [D loss: 0.306288, acc.: 86.72%] [G loss: 3.101832]\n",
      "epoch:17 step:13944 [D loss: 0.326520, acc.: 85.16%] [G loss: 3.607585]\n",
      "epoch:17 step:13945 [D loss: 0.241829, acc.: 92.97%] [G loss: 3.246793]\n",
      "epoch:17 step:13946 [D loss: 0.261792, acc.: 89.06%] [G loss: 2.768902]\n",
      "epoch:17 step:13947 [D loss: 0.284967, acc.: 87.50%] [G loss: 3.687435]\n",
      "epoch:17 step:13948 [D loss: 0.344055, acc.: 82.81%] [G loss: 2.724969]\n",
      "epoch:17 step:13949 [D loss: 0.331647, acc.: 85.94%] [G loss: 2.826517]\n",
      "epoch:17 step:13950 [D loss: 0.388744, acc.: 85.16%] [G loss: 2.783301]\n",
      "epoch:17 step:13951 [D loss: 0.297335, acc.: 86.72%] [G loss: 2.782255]\n",
      "epoch:17 step:13952 [D loss: 0.338840, acc.: 84.38%] [G loss: 3.771594]\n",
      "epoch:17 step:13953 [D loss: 0.323045, acc.: 85.94%] [G loss: 4.782084]\n",
      "epoch:17 step:13954 [D loss: 0.459373, acc.: 78.91%] [G loss: 3.306109]\n",
      "epoch:17 step:13955 [D loss: 0.277908, acc.: 85.94%] [G loss: 4.348618]\n",
      "epoch:17 step:13956 [D loss: 0.262328, acc.: 90.62%] [G loss: 4.359962]\n",
      "epoch:17 step:13957 [D loss: 0.305759, acc.: 87.50%] [G loss: 3.171647]\n",
      "epoch:17 step:13958 [D loss: 0.282492, acc.: 89.06%] [G loss: 4.432275]\n",
      "epoch:17 step:13959 [D loss: 0.396702, acc.: 83.59%] [G loss: 2.693337]\n",
      "epoch:17 step:13960 [D loss: 0.284886, acc.: 89.06%] [G loss: 5.728539]\n",
      "epoch:17 step:13961 [D loss: 0.320067, acc.: 82.81%] [G loss: 3.343825]\n",
      "epoch:17 step:13962 [D loss: 0.251081, acc.: 92.19%] [G loss: 2.854757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13963 [D loss: 0.399727, acc.: 78.12%] [G loss: 2.958520]\n",
      "epoch:17 step:13964 [D loss: 0.297117, acc.: 86.72%] [G loss: 3.343451]\n",
      "epoch:17 step:13965 [D loss: 0.330671, acc.: 84.38%] [G loss: 2.947630]\n",
      "epoch:17 step:13966 [D loss: 0.261556, acc.: 86.72%] [G loss: 3.049601]\n",
      "epoch:17 step:13967 [D loss: 0.210615, acc.: 94.53%] [G loss: 3.271224]\n",
      "epoch:17 step:13968 [D loss: 0.241105, acc.: 92.97%] [G loss: 3.403252]\n",
      "epoch:17 step:13969 [D loss: 0.403438, acc.: 82.81%] [G loss: 3.259746]\n",
      "epoch:17 step:13970 [D loss: 0.259527, acc.: 88.28%] [G loss: 4.193902]\n",
      "epoch:17 step:13971 [D loss: 0.226701, acc.: 87.50%] [G loss: 3.770357]\n",
      "epoch:17 step:13972 [D loss: 0.255722, acc.: 89.06%] [G loss: 3.152281]\n",
      "epoch:17 step:13973 [D loss: 0.247854, acc.: 88.28%] [G loss: 3.499662]\n",
      "epoch:17 step:13974 [D loss: 0.351972, acc.: 87.50%] [G loss: 3.063348]\n",
      "epoch:17 step:13975 [D loss: 0.308442, acc.: 88.28%] [G loss: 2.554371]\n",
      "epoch:17 step:13976 [D loss: 0.235082, acc.: 92.19%] [G loss: 2.380032]\n",
      "epoch:17 step:13977 [D loss: 0.321025, acc.: 85.94%] [G loss: 3.266811]\n",
      "epoch:17 step:13978 [D loss: 0.231113, acc.: 91.41%] [G loss: 8.114292]\n",
      "epoch:17 step:13979 [D loss: 0.178942, acc.: 92.19%] [G loss: 4.420568]\n",
      "epoch:17 step:13980 [D loss: 0.359156, acc.: 80.47%] [G loss: 4.603307]\n",
      "epoch:17 step:13981 [D loss: 0.547070, acc.: 78.12%] [G loss: 3.945691]\n",
      "epoch:17 step:13982 [D loss: 0.367551, acc.: 84.38%] [G loss: 4.991107]\n",
      "epoch:17 step:13983 [D loss: 0.263274, acc.: 90.62%] [G loss: 3.535176]\n",
      "epoch:17 step:13984 [D loss: 0.272312, acc.: 89.06%] [G loss: 3.183352]\n",
      "epoch:17 step:13985 [D loss: 0.293846, acc.: 85.16%] [G loss: 3.963734]\n",
      "epoch:17 step:13986 [D loss: 0.355774, acc.: 82.03%] [G loss: 4.208161]\n",
      "epoch:17 step:13987 [D loss: 0.293373, acc.: 85.16%] [G loss: 3.228257]\n",
      "epoch:17 step:13988 [D loss: 0.356369, acc.: 85.94%] [G loss: 3.289517]\n",
      "epoch:17 step:13989 [D loss: 0.272263, acc.: 87.50%] [G loss: 4.057792]\n",
      "epoch:17 step:13990 [D loss: 0.341696, acc.: 85.16%] [G loss: 2.879502]\n",
      "epoch:17 step:13991 [D loss: 0.264284, acc.: 87.50%] [G loss: 4.107110]\n",
      "epoch:17 step:13992 [D loss: 0.266844, acc.: 86.72%] [G loss: 2.957541]\n",
      "epoch:17 step:13993 [D loss: 0.337802, acc.: 85.16%] [G loss: 4.230801]\n",
      "epoch:17 step:13994 [D loss: 0.293976, acc.: 88.28%] [G loss: 6.519606]\n",
      "epoch:17 step:13995 [D loss: 0.431468, acc.: 79.69%] [G loss: 4.543268]\n",
      "epoch:17 step:13996 [D loss: 0.343006, acc.: 82.03%] [G loss: 2.948191]\n",
      "epoch:17 step:13997 [D loss: 0.292642, acc.: 89.06%] [G loss: 3.108258]\n",
      "epoch:17 step:13998 [D loss: 0.430338, acc.: 80.47%] [G loss: 3.952754]\n",
      "epoch:17 step:13999 [D loss: 0.350420, acc.: 82.81%] [G loss: 2.705956]\n",
      "epoch:17 step:14000 [D loss: 0.407154, acc.: 84.38%] [G loss: 2.875808]\n",
      "epoch:17 step:14001 [D loss: 0.235915, acc.: 88.28%] [G loss: 2.970189]\n",
      "epoch:17 step:14002 [D loss: 0.280984, acc.: 89.06%] [G loss: 3.422467]\n",
      "epoch:17 step:14003 [D loss: 0.242030, acc.: 91.41%] [G loss: 3.551700]\n",
      "epoch:17 step:14004 [D loss: 0.280420, acc.: 88.28%] [G loss: 2.552870]\n",
      "epoch:17 step:14005 [D loss: 0.323360, acc.: 85.94%] [G loss: 2.493632]\n",
      "epoch:17 step:14006 [D loss: 0.313489, acc.: 87.50%] [G loss: 2.732721]\n",
      "epoch:17 step:14007 [D loss: 0.346003, acc.: 86.72%] [G loss: 2.654938]\n",
      "epoch:17 step:14008 [D loss: 0.289772, acc.: 90.62%] [G loss: 3.109200]\n",
      "epoch:17 step:14009 [D loss: 0.401418, acc.: 82.81%] [G loss: 3.145561]\n",
      "epoch:17 step:14010 [D loss: 0.314514, acc.: 84.38%] [G loss: 4.561117]\n",
      "epoch:17 step:14011 [D loss: 0.224479, acc.: 91.41%] [G loss: 3.908109]\n",
      "epoch:17 step:14012 [D loss: 0.362937, acc.: 85.16%] [G loss: 3.731021]\n",
      "epoch:17 step:14013 [D loss: 0.315898, acc.: 87.50%] [G loss: 3.586838]\n",
      "epoch:17 step:14014 [D loss: 0.339824, acc.: 84.38%] [G loss: 3.789693]\n",
      "epoch:17 step:14015 [D loss: 0.327910, acc.: 86.72%] [G loss: 2.539248]\n",
      "epoch:17 step:14016 [D loss: 0.235637, acc.: 91.41%] [G loss: 3.032203]\n",
      "epoch:17 step:14017 [D loss: 0.241606, acc.: 89.84%] [G loss: 2.339634]\n",
      "epoch:17 step:14018 [D loss: 0.299149, acc.: 85.16%] [G loss: 3.396294]\n",
      "epoch:17 step:14019 [D loss: 0.328839, acc.: 86.72%] [G loss: 2.774802]\n",
      "epoch:17 step:14020 [D loss: 0.346036, acc.: 86.72%] [G loss: 2.523240]\n",
      "epoch:17 step:14021 [D loss: 0.374687, acc.: 82.81%] [G loss: 2.325104]\n",
      "epoch:17 step:14022 [D loss: 0.253740, acc.: 90.62%] [G loss: 3.089488]\n",
      "epoch:17 step:14023 [D loss: 0.301407, acc.: 89.06%] [G loss: 4.248259]\n",
      "epoch:17 step:14024 [D loss: 0.244181, acc.: 89.06%] [G loss: 4.733850]\n",
      "epoch:17 step:14025 [D loss: 0.273409, acc.: 89.06%] [G loss: 3.165210]\n",
      "epoch:17 step:14026 [D loss: 0.357289, acc.: 85.16%] [G loss: 4.128941]\n",
      "epoch:17 step:14027 [D loss: 0.297840, acc.: 84.38%] [G loss: 5.681067]\n",
      "epoch:17 step:14028 [D loss: 0.381118, acc.: 83.59%] [G loss: 4.327897]\n",
      "epoch:17 step:14029 [D loss: 0.192191, acc.: 94.53%] [G loss: 6.196194]\n",
      "epoch:17 step:14030 [D loss: 0.293369, acc.: 89.06%] [G loss: 4.536846]\n",
      "epoch:17 step:14031 [D loss: 0.328345, acc.: 85.16%] [G loss: 4.527374]\n",
      "epoch:17 step:14032 [D loss: 0.272084, acc.: 88.28%] [G loss: 3.388559]\n",
      "epoch:17 step:14033 [D loss: 0.417020, acc.: 83.59%] [G loss: 2.285835]\n",
      "epoch:17 step:14034 [D loss: 0.342909, acc.: 85.16%] [G loss: 2.696612]\n",
      "epoch:17 step:14035 [D loss: 0.402132, acc.: 79.69%] [G loss: 3.055540]\n",
      "epoch:17 step:14036 [D loss: 0.385469, acc.: 83.59%] [G loss: 2.282833]\n",
      "epoch:17 step:14037 [D loss: 0.317550, acc.: 89.06%] [G loss: 3.450936]\n",
      "epoch:17 step:14038 [D loss: 0.418418, acc.: 78.91%] [G loss: 2.862589]\n",
      "epoch:17 step:14039 [D loss: 0.357751, acc.: 84.38%] [G loss: 3.070058]\n",
      "epoch:17 step:14040 [D loss: 0.365174, acc.: 87.50%] [G loss: 2.863018]\n",
      "epoch:17 step:14041 [D loss: 0.303086, acc.: 86.72%] [G loss: 3.481798]\n",
      "epoch:17 step:14042 [D loss: 0.375295, acc.: 81.25%] [G loss: 4.973500]\n",
      "epoch:17 step:14043 [D loss: 0.445327, acc.: 73.44%] [G loss: 2.879104]\n",
      "epoch:17 step:14044 [D loss: 0.257952, acc.: 88.28%] [G loss: 3.743674]\n",
      "epoch:17 step:14045 [D loss: 0.370183, acc.: 85.94%] [G loss: 2.803268]\n",
      "epoch:17 step:14046 [D loss: 0.187257, acc.: 93.75%] [G loss: 6.387641]\n",
      "epoch:17 step:14047 [D loss: 0.301483, acc.: 86.72%] [G loss: 6.357753]\n",
      "epoch:17 step:14048 [D loss: 0.298495, acc.: 85.94%] [G loss: 6.043983]\n",
      "epoch:17 step:14049 [D loss: 0.274499, acc.: 85.16%] [G loss: 4.252196]\n",
      "epoch:17 step:14050 [D loss: 0.346572, acc.: 83.59%] [G loss: 3.861095]\n",
      "epoch:17 step:14051 [D loss: 0.306072, acc.: 86.72%] [G loss: 3.239991]\n",
      "epoch:17 step:14052 [D loss: 0.263416, acc.: 87.50%] [G loss: 3.854728]\n",
      "epoch:17 step:14053 [D loss: 0.217429, acc.: 93.75%] [G loss: 3.178253]\n",
      "epoch:17 step:14054 [D loss: 0.423732, acc.: 78.12%] [G loss: 3.395718]\n",
      "epoch:17 step:14055 [D loss: 0.315022, acc.: 89.06%] [G loss: 4.279203]\n",
      "epoch:17 step:14056 [D loss: 0.274356, acc.: 89.06%] [G loss: 4.376475]\n",
      "epoch:17 step:14057 [D loss: 0.311813, acc.: 84.38%] [G loss: 3.580010]\n",
      "epoch:17 step:14058 [D loss: 0.232837, acc.: 90.62%] [G loss: 3.661444]\n",
      "epoch:18 step:14059 [D loss: 0.243836, acc.: 87.50%] [G loss: 4.367746]\n",
      "epoch:18 step:14060 [D loss: 0.378567, acc.: 79.69%] [G loss: 3.124922]\n",
      "epoch:18 step:14061 [D loss: 0.330055, acc.: 83.59%] [G loss: 5.252007]\n",
      "epoch:18 step:14062 [D loss: 0.327624, acc.: 82.81%] [G loss: 2.752948]\n",
      "epoch:18 step:14063 [D loss: 0.299564, acc.: 87.50%] [G loss: 3.685072]\n",
      "epoch:18 step:14064 [D loss: 0.278762, acc.: 85.94%] [G loss: 3.261387]\n",
      "epoch:18 step:14065 [D loss: 0.275705, acc.: 85.94%] [G loss: 3.688576]\n",
      "epoch:18 step:14066 [D loss: 0.243777, acc.: 89.84%] [G loss: 4.479986]\n",
      "epoch:18 step:14067 [D loss: 0.308752, acc.: 85.16%] [G loss: 4.083791]\n",
      "epoch:18 step:14068 [D loss: 0.234302, acc.: 93.75%] [G loss: 3.201999]\n",
      "epoch:18 step:14069 [D loss: 0.257806, acc.: 89.06%] [G loss: 3.005064]\n",
      "epoch:18 step:14070 [D loss: 0.253353, acc.: 92.97%] [G loss: 3.446173]\n",
      "epoch:18 step:14071 [D loss: 0.305246, acc.: 87.50%] [G loss: 3.091747]\n",
      "epoch:18 step:14072 [D loss: 0.298912, acc.: 89.06%] [G loss: 3.438492]\n",
      "epoch:18 step:14073 [D loss: 0.331145, acc.: 82.81%] [G loss: 4.244735]\n",
      "epoch:18 step:14074 [D loss: 0.352435, acc.: 84.38%] [G loss: 3.149735]\n",
      "epoch:18 step:14075 [D loss: 0.204741, acc.: 91.41%] [G loss: 3.033113]\n",
      "epoch:18 step:14076 [D loss: 0.358631, acc.: 82.81%] [G loss: 3.502355]\n",
      "epoch:18 step:14077 [D loss: 0.318484, acc.: 89.84%] [G loss: 3.428805]\n",
      "epoch:18 step:14078 [D loss: 0.321524, acc.: 83.59%] [G loss: 3.027924]\n",
      "epoch:18 step:14079 [D loss: 0.322922, acc.: 83.59%] [G loss: 2.343252]\n",
      "epoch:18 step:14080 [D loss: 0.289105, acc.: 86.72%] [G loss: 2.929643]\n",
      "epoch:18 step:14081 [D loss: 0.356780, acc.: 85.94%] [G loss: 3.615876]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14082 [D loss: 0.349465, acc.: 85.94%] [G loss: 4.954311]\n",
      "epoch:18 step:14083 [D loss: 0.317502, acc.: 82.81%] [G loss: 4.863588]\n",
      "epoch:18 step:14084 [D loss: 0.206020, acc.: 90.62%] [G loss: 3.421645]\n",
      "epoch:18 step:14085 [D loss: 0.264727, acc.: 90.62%] [G loss: 3.571888]\n",
      "epoch:18 step:14086 [D loss: 0.219660, acc.: 90.62%] [G loss: 4.449484]\n",
      "epoch:18 step:14087 [D loss: 0.299185, acc.: 88.28%] [G loss: 3.546318]\n",
      "epoch:18 step:14088 [D loss: 0.252081, acc.: 86.72%] [G loss: 4.451731]\n",
      "epoch:18 step:14089 [D loss: 0.359373, acc.: 88.28%] [G loss: 4.695408]\n",
      "epoch:18 step:14090 [D loss: 0.369566, acc.: 83.59%] [G loss: 5.333048]\n",
      "epoch:18 step:14091 [D loss: 0.379939, acc.: 84.38%] [G loss: 4.177941]\n",
      "epoch:18 step:14092 [D loss: 0.286370, acc.: 89.06%] [G loss: 3.639267]\n",
      "epoch:18 step:14093 [D loss: 0.229936, acc.: 94.53%] [G loss: 3.130053]\n",
      "epoch:18 step:14094 [D loss: 0.306942, acc.: 85.94%] [G loss: 3.089213]\n",
      "epoch:18 step:14095 [D loss: 0.431571, acc.: 87.50%] [G loss: 4.666137]\n",
      "epoch:18 step:14096 [D loss: 0.579133, acc.: 76.56%] [G loss: 7.498158]\n",
      "epoch:18 step:14097 [D loss: 1.918956, acc.: 59.38%] [G loss: 7.313992]\n",
      "epoch:18 step:14098 [D loss: 1.675046, acc.: 69.53%] [G loss: 5.450845]\n",
      "epoch:18 step:14099 [D loss: 1.598092, acc.: 65.62%] [G loss: 10.663836]\n",
      "epoch:18 step:14100 [D loss: 1.671694, acc.: 65.62%] [G loss: 2.955792]\n",
      "epoch:18 step:14101 [D loss: 1.243194, acc.: 71.09%] [G loss: 3.765349]\n",
      "epoch:18 step:14102 [D loss: 0.948327, acc.: 75.78%] [G loss: 5.144254]\n",
      "epoch:18 step:14103 [D loss: 1.168452, acc.: 63.28%] [G loss: 3.697388]\n",
      "epoch:18 step:14104 [D loss: 0.542980, acc.: 80.47%] [G loss: 6.809162]\n",
      "epoch:18 step:14105 [D loss: 0.648507, acc.: 77.34%] [G loss: 3.857295]\n",
      "epoch:18 step:14106 [D loss: 0.536271, acc.: 75.00%] [G loss: 3.759248]\n",
      "epoch:18 step:14107 [D loss: 0.263824, acc.: 86.72%] [G loss: 3.549922]\n",
      "epoch:18 step:14108 [D loss: 0.376313, acc.: 81.25%] [G loss: 2.720727]\n",
      "epoch:18 step:14109 [D loss: 0.336119, acc.: 84.38%] [G loss: 2.544614]\n",
      "epoch:18 step:14110 [D loss: 0.267147, acc.: 89.84%] [G loss: 2.880713]\n",
      "epoch:18 step:14111 [D loss: 0.350103, acc.: 85.94%] [G loss: 2.885753]\n",
      "epoch:18 step:14112 [D loss: 0.263348, acc.: 92.19%] [G loss: 2.328940]\n",
      "epoch:18 step:14113 [D loss: 0.321373, acc.: 90.62%] [G loss: 2.253976]\n",
      "epoch:18 step:14114 [D loss: 0.408183, acc.: 83.59%] [G loss: 2.890238]\n",
      "epoch:18 step:14115 [D loss: 0.308324, acc.: 85.94%] [G loss: 3.335850]\n",
      "epoch:18 step:14116 [D loss: 0.366461, acc.: 86.72%] [G loss: 2.537241]\n",
      "epoch:18 step:14117 [D loss: 0.220026, acc.: 92.97%] [G loss: 3.194658]\n",
      "epoch:18 step:14118 [D loss: 0.305503, acc.: 88.28%] [G loss: 3.601152]\n",
      "epoch:18 step:14119 [D loss: 0.428367, acc.: 78.12%] [G loss: 2.864780]\n",
      "epoch:18 step:14120 [D loss: 0.328961, acc.: 84.38%] [G loss: 2.429772]\n",
      "epoch:18 step:14121 [D loss: 0.330511, acc.: 86.72%] [G loss: 2.754060]\n",
      "epoch:18 step:14122 [D loss: 0.397288, acc.: 82.03%] [G loss: 2.889789]\n",
      "epoch:18 step:14123 [D loss: 0.476170, acc.: 82.81%] [G loss: 3.360929]\n",
      "epoch:18 step:14124 [D loss: 0.336831, acc.: 87.50%] [G loss: 2.736797]\n",
      "epoch:18 step:14125 [D loss: 0.348372, acc.: 83.59%] [G loss: 2.090500]\n",
      "epoch:18 step:14126 [D loss: 0.352638, acc.: 85.16%] [G loss: 2.850523]\n",
      "epoch:18 step:14127 [D loss: 0.325291, acc.: 87.50%] [G loss: 2.610519]\n",
      "epoch:18 step:14128 [D loss: 0.337306, acc.: 82.81%] [G loss: 2.563985]\n",
      "epoch:18 step:14129 [D loss: 0.346052, acc.: 89.06%] [G loss: 2.688862]\n",
      "epoch:18 step:14130 [D loss: 0.277073, acc.: 89.06%] [G loss: 2.512471]\n",
      "epoch:18 step:14131 [D loss: 0.306720, acc.: 87.50%] [G loss: 2.451124]\n",
      "epoch:18 step:14132 [D loss: 0.322972, acc.: 84.38%] [G loss: 2.648582]\n",
      "epoch:18 step:14133 [D loss: 0.325116, acc.: 84.38%] [G loss: 2.653462]\n",
      "epoch:18 step:14134 [D loss: 0.311024, acc.: 85.94%] [G loss: 3.004424]\n",
      "epoch:18 step:14135 [D loss: 0.332323, acc.: 85.16%] [G loss: 2.843890]\n",
      "epoch:18 step:14136 [D loss: 0.330475, acc.: 85.94%] [G loss: 3.036641]\n",
      "epoch:18 step:14137 [D loss: 0.394064, acc.: 83.59%] [G loss: 2.807517]\n",
      "epoch:18 step:14138 [D loss: 0.378372, acc.: 79.69%] [G loss: 2.983654]\n",
      "epoch:18 step:14139 [D loss: 0.295947, acc.: 89.06%] [G loss: 4.110299]\n",
      "epoch:18 step:14140 [D loss: 0.362411, acc.: 84.38%] [G loss: 2.980669]\n",
      "epoch:18 step:14141 [D loss: 0.336947, acc.: 85.16%] [G loss: 2.853151]\n",
      "epoch:18 step:14142 [D loss: 0.291324, acc.: 85.94%] [G loss: 3.869517]\n",
      "epoch:18 step:14143 [D loss: 0.308371, acc.: 88.28%] [G loss: 3.872074]\n",
      "epoch:18 step:14144 [D loss: 0.206880, acc.: 90.62%] [G loss: 5.445787]\n",
      "epoch:18 step:14145 [D loss: 0.292385, acc.: 88.28%] [G loss: 4.823502]\n",
      "epoch:18 step:14146 [D loss: 0.232689, acc.: 87.50%] [G loss: 5.792630]\n",
      "epoch:18 step:14147 [D loss: 0.247533, acc.: 89.06%] [G loss: 4.639495]\n",
      "epoch:18 step:14148 [D loss: 0.268054, acc.: 89.06%] [G loss: 6.266899]\n",
      "epoch:18 step:14149 [D loss: 0.296261, acc.: 86.72%] [G loss: 2.983802]\n",
      "epoch:18 step:14150 [D loss: 0.310015, acc.: 85.16%] [G loss: 2.770745]\n",
      "epoch:18 step:14151 [D loss: 0.307792, acc.: 85.16%] [G loss: 2.664730]\n",
      "epoch:18 step:14152 [D loss: 0.271758, acc.: 89.84%] [G loss: 2.454827]\n",
      "epoch:18 step:14153 [D loss: 0.286584, acc.: 89.06%] [G loss: 2.884827]\n",
      "epoch:18 step:14154 [D loss: 0.256299, acc.: 85.94%] [G loss: 3.156339]\n",
      "epoch:18 step:14155 [D loss: 0.350936, acc.: 80.47%] [G loss: 2.511942]\n",
      "epoch:18 step:14156 [D loss: 0.343024, acc.: 85.16%] [G loss: 3.347547]\n",
      "epoch:18 step:14157 [D loss: 0.383224, acc.: 82.81%] [G loss: 4.304349]\n",
      "epoch:18 step:14158 [D loss: 0.285261, acc.: 87.50%] [G loss: 3.840407]\n",
      "epoch:18 step:14159 [D loss: 0.307067, acc.: 87.50%] [G loss: 3.442141]\n",
      "epoch:18 step:14160 [D loss: 0.333825, acc.: 84.38%] [G loss: 4.242788]\n",
      "epoch:18 step:14161 [D loss: 0.263023, acc.: 85.16%] [G loss: 3.322324]\n",
      "epoch:18 step:14162 [D loss: 0.456502, acc.: 81.25%] [G loss: 3.281031]\n",
      "epoch:18 step:14163 [D loss: 0.335481, acc.: 83.59%] [G loss: 5.592576]\n",
      "epoch:18 step:14164 [D loss: 0.347593, acc.: 84.38%] [G loss: 5.371790]\n",
      "epoch:18 step:14165 [D loss: 0.271597, acc.: 89.06%] [G loss: 3.148912]\n",
      "epoch:18 step:14166 [D loss: 0.213989, acc.: 95.31%] [G loss: 3.362544]\n",
      "epoch:18 step:14167 [D loss: 0.302713, acc.: 89.84%] [G loss: 2.921266]\n",
      "epoch:18 step:14168 [D loss: 0.327911, acc.: 81.25%] [G loss: 3.348321]\n",
      "epoch:18 step:14169 [D loss: 0.333707, acc.: 86.72%] [G loss: 2.087129]\n",
      "epoch:18 step:14170 [D loss: 0.314051, acc.: 85.16%] [G loss: 2.953365]\n",
      "epoch:18 step:14171 [D loss: 0.301312, acc.: 85.16%] [G loss: 3.858761]\n",
      "epoch:18 step:14172 [D loss: 0.316583, acc.: 88.28%] [G loss: 4.549738]\n",
      "epoch:18 step:14173 [D loss: 0.291945, acc.: 85.94%] [G loss: 3.146089]\n",
      "epoch:18 step:14174 [D loss: 0.349033, acc.: 85.94%] [G loss: 3.307734]\n",
      "epoch:18 step:14175 [D loss: 0.311036, acc.: 85.94%] [G loss: 2.624309]\n",
      "epoch:18 step:14176 [D loss: 0.485342, acc.: 80.47%] [G loss: 3.969122]\n",
      "epoch:18 step:14177 [D loss: 0.690645, acc.: 67.19%] [G loss: 4.210565]\n",
      "epoch:18 step:14178 [D loss: 0.390461, acc.: 80.47%] [G loss: 3.433261]\n",
      "epoch:18 step:14179 [D loss: 0.376904, acc.: 83.59%] [G loss: 3.764184]\n",
      "epoch:18 step:14180 [D loss: 0.201949, acc.: 92.97%] [G loss: 4.157238]\n",
      "epoch:18 step:14181 [D loss: 0.390848, acc.: 82.03%] [G loss: 4.822842]\n",
      "epoch:18 step:14182 [D loss: 0.329543, acc.: 86.72%] [G loss: 2.832424]\n",
      "epoch:18 step:14183 [D loss: 0.346005, acc.: 82.81%] [G loss: 3.255263]\n",
      "epoch:18 step:14184 [D loss: 0.380143, acc.: 82.03%] [G loss: 2.635073]\n",
      "epoch:18 step:14185 [D loss: 0.355656, acc.: 85.94%] [G loss: 3.351951]\n",
      "epoch:18 step:14186 [D loss: 0.254427, acc.: 89.84%] [G loss: 3.530082]\n",
      "epoch:18 step:14187 [D loss: 0.438645, acc.: 82.81%] [G loss: 3.417850]\n",
      "epoch:18 step:14188 [D loss: 0.278250, acc.: 87.50%] [G loss: 2.760880]\n",
      "epoch:18 step:14189 [D loss: 0.327712, acc.: 82.81%] [G loss: 3.591684]\n",
      "epoch:18 step:14190 [D loss: 0.284959, acc.: 89.84%] [G loss: 2.919553]\n",
      "epoch:18 step:14191 [D loss: 0.395998, acc.: 86.72%] [G loss: 2.709092]\n",
      "epoch:18 step:14192 [D loss: 0.258888, acc.: 88.28%] [G loss: 2.635890]\n",
      "epoch:18 step:14193 [D loss: 0.437382, acc.: 78.91%] [G loss: 3.058219]\n",
      "epoch:18 step:14194 [D loss: 0.208649, acc.: 90.62%] [G loss: 3.159109]\n",
      "epoch:18 step:14195 [D loss: 0.491917, acc.: 78.12%] [G loss: 3.388562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14196 [D loss: 0.341749, acc.: 88.28%] [G loss: 2.726343]\n",
      "epoch:18 step:14197 [D loss: 0.251730, acc.: 88.28%] [G loss: 2.693087]\n",
      "epoch:18 step:14198 [D loss: 0.322646, acc.: 91.41%] [G loss: 2.524503]\n",
      "epoch:18 step:14199 [D loss: 0.301595, acc.: 85.94%] [G loss: 3.794290]\n",
      "epoch:18 step:14200 [D loss: 0.406952, acc.: 81.25%] [G loss: 4.545307]\n",
      "epoch:18 step:14201 [D loss: 0.316262, acc.: 85.94%] [G loss: 5.083150]\n",
      "epoch:18 step:14202 [D loss: 0.289767, acc.: 88.28%] [G loss: 4.461181]\n",
      "epoch:18 step:14203 [D loss: 0.149485, acc.: 92.97%] [G loss: 6.838783]\n",
      "epoch:18 step:14204 [D loss: 0.307056, acc.: 85.16%] [G loss: 6.407224]\n",
      "epoch:18 step:14205 [D loss: 0.183673, acc.: 92.19%] [G loss: 4.706796]\n",
      "epoch:18 step:14206 [D loss: 0.356354, acc.: 81.25%] [G loss: 2.618568]\n",
      "epoch:18 step:14207 [D loss: 0.199651, acc.: 92.97%] [G loss: 3.150214]\n",
      "epoch:18 step:14208 [D loss: 0.254538, acc.: 92.97%] [G loss: 3.381128]\n",
      "epoch:18 step:14209 [D loss: 0.323393, acc.: 82.03%] [G loss: 3.452967]\n",
      "epoch:18 step:14210 [D loss: 0.266725, acc.: 88.28%] [G loss: 2.475048]\n",
      "epoch:18 step:14211 [D loss: 0.298866, acc.: 88.28%] [G loss: 3.303951]\n",
      "epoch:18 step:14212 [D loss: 0.295377, acc.: 87.50%] [G loss: 2.078897]\n",
      "epoch:18 step:14213 [D loss: 0.283026, acc.: 89.84%] [G loss: 2.898786]\n",
      "epoch:18 step:14214 [D loss: 0.378513, acc.: 81.25%] [G loss: 3.746579]\n",
      "epoch:18 step:14215 [D loss: 0.389168, acc.: 78.12%] [G loss: 2.409353]\n",
      "epoch:18 step:14216 [D loss: 0.404584, acc.: 82.81%] [G loss: 2.654024]\n",
      "epoch:18 step:14217 [D loss: 0.358955, acc.: 85.16%] [G loss: 2.448612]\n",
      "epoch:18 step:14218 [D loss: 0.286354, acc.: 89.06%] [G loss: 3.290873]\n",
      "epoch:18 step:14219 [D loss: 0.522094, acc.: 75.78%] [G loss: 4.664896]\n",
      "epoch:18 step:14220 [D loss: 0.292132, acc.: 88.28%] [G loss: 2.516335]\n",
      "epoch:18 step:14221 [D loss: 0.333834, acc.: 85.16%] [G loss: 3.850315]\n",
      "epoch:18 step:14222 [D loss: 0.326600, acc.: 88.28%] [G loss: 3.498111]\n",
      "epoch:18 step:14223 [D loss: 0.318305, acc.: 85.16%] [G loss: 3.970615]\n",
      "epoch:18 step:14224 [D loss: 0.502783, acc.: 81.25%] [G loss: 2.549665]\n",
      "epoch:18 step:14225 [D loss: 0.437030, acc.: 79.69%] [G loss: 3.183840]\n",
      "epoch:18 step:14226 [D loss: 0.336107, acc.: 83.59%] [G loss: 3.169854]\n",
      "epoch:18 step:14227 [D loss: 0.367023, acc.: 85.16%] [G loss: 2.211338]\n",
      "epoch:18 step:14228 [D loss: 0.274275, acc.: 87.50%] [G loss: 2.486197]\n",
      "epoch:18 step:14229 [D loss: 0.383700, acc.: 85.94%] [G loss: 3.519391]\n",
      "epoch:18 step:14230 [D loss: 0.306348, acc.: 87.50%] [G loss: 3.339808]\n",
      "epoch:18 step:14231 [D loss: 0.335564, acc.: 79.69%] [G loss: 5.161532]\n",
      "epoch:18 step:14232 [D loss: 0.481581, acc.: 75.78%] [G loss: 2.357390]\n",
      "epoch:18 step:14233 [D loss: 0.420131, acc.: 84.38%] [G loss: 2.733871]\n",
      "epoch:18 step:14234 [D loss: 0.291723, acc.: 87.50%] [G loss: 3.473536]\n",
      "epoch:18 step:14235 [D loss: 0.395074, acc.: 78.12%] [G loss: 2.963214]\n",
      "epoch:18 step:14236 [D loss: 0.276265, acc.: 87.50%] [G loss: 3.944520]\n",
      "epoch:18 step:14237 [D loss: 0.259991, acc.: 89.84%] [G loss: 4.366982]\n",
      "epoch:18 step:14238 [D loss: 0.455482, acc.: 80.47%] [G loss: 4.931237]\n",
      "epoch:18 step:14239 [D loss: 0.327296, acc.: 87.50%] [G loss: 3.509760]\n",
      "epoch:18 step:14240 [D loss: 0.575510, acc.: 75.78%] [G loss: 4.054926]\n",
      "epoch:18 step:14241 [D loss: 0.321336, acc.: 89.06%] [G loss: 2.691649]\n",
      "epoch:18 step:14242 [D loss: 0.293464, acc.: 84.38%] [G loss: 3.538299]\n",
      "epoch:18 step:14243 [D loss: 0.229444, acc.: 90.62%] [G loss: 3.322498]\n",
      "epoch:18 step:14244 [D loss: 0.352527, acc.: 82.81%] [G loss: 2.736082]\n",
      "epoch:18 step:14245 [D loss: 0.276132, acc.: 89.06%] [G loss: 2.723650]\n",
      "epoch:18 step:14246 [D loss: 0.358099, acc.: 85.16%] [G loss: 2.487536]\n",
      "epoch:18 step:14247 [D loss: 0.345196, acc.: 85.16%] [G loss: 2.598486]\n",
      "epoch:18 step:14248 [D loss: 0.354041, acc.: 84.38%] [G loss: 2.598355]\n",
      "epoch:18 step:14249 [D loss: 0.363114, acc.: 86.72%] [G loss: 2.603168]\n",
      "epoch:18 step:14250 [D loss: 0.403929, acc.: 79.69%] [G loss: 2.758424]\n",
      "epoch:18 step:14251 [D loss: 0.365165, acc.: 85.16%] [G loss: 3.259413]\n",
      "epoch:18 step:14252 [D loss: 0.290491, acc.: 85.16%] [G loss: 4.664546]\n",
      "epoch:18 step:14253 [D loss: 0.258285, acc.: 89.84%] [G loss: 5.221743]\n",
      "epoch:18 step:14254 [D loss: 0.308065, acc.: 86.72%] [G loss: 3.489056]\n",
      "epoch:18 step:14255 [D loss: 0.391282, acc.: 80.47%] [G loss: 2.465272]\n",
      "epoch:18 step:14256 [D loss: 0.377583, acc.: 81.25%] [G loss: 2.529642]\n",
      "epoch:18 step:14257 [D loss: 0.277837, acc.: 87.50%] [G loss: 3.392985]\n",
      "epoch:18 step:14258 [D loss: 0.255677, acc.: 88.28%] [G loss: 4.119248]\n",
      "epoch:18 step:14259 [D loss: 0.309015, acc.: 87.50%] [G loss: 3.659938]\n",
      "epoch:18 step:14260 [D loss: 0.269664, acc.: 89.84%] [G loss: 3.829162]\n",
      "epoch:18 step:14261 [D loss: 0.158043, acc.: 92.97%] [G loss: 4.991735]\n",
      "epoch:18 step:14262 [D loss: 0.332011, acc.: 85.94%] [G loss: 2.872761]\n",
      "epoch:18 step:14263 [D loss: 0.271246, acc.: 88.28%] [G loss: 2.970614]\n",
      "epoch:18 step:14264 [D loss: 0.283175, acc.: 87.50%] [G loss: 2.718180]\n",
      "epoch:18 step:14265 [D loss: 0.363635, acc.: 83.59%] [G loss: 2.678874]\n",
      "epoch:18 step:14266 [D loss: 0.390906, acc.: 84.38%] [G loss: 2.674865]\n",
      "epoch:18 step:14267 [D loss: 0.300246, acc.: 84.38%] [G loss: 3.110938]\n",
      "epoch:18 step:14268 [D loss: 0.264598, acc.: 86.72%] [G loss: 3.567127]\n",
      "epoch:18 step:14269 [D loss: 0.268402, acc.: 87.50%] [G loss: 3.389409]\n",
      "epoch:18 step:14270 [D loss: 0.250157, acc.: 92.97%] [G loss: 3.008054]\n",
      "epoch:18 step:14271 [D loss: 0.327449, acc.: 84.38%] [G loss: 2.469069]\n",
      "epoch:18 step:14272 [D loss: 0.350263, acc.: 83.59%] [G loss: 2.835411]\n",
      "epoch:18 step:14273 [D loss: 0.368340, acc.: 83.59%] [G loss: 2.468592]\n",
      "epoch:18 step:14274 [D loss: 0.424497, acc.: 79.69%] [G loss: 2.663965]\n",
      "epoch:18 step:14275 [D loss: 0.289082, acc.: 83.59%] [G loss: 2.931230]\n",
      "epoch:18 step:14276 [D loss: 0.320950, acc.: 85.16%] [G loss: 2.815815]\n",
      "epoch:18 step:14277 [D loss: 0.277025, acc.: 87.50%] [G loss: 3.106342]\n",
      "epoch:18 step:14278 [D loss: 0.314883, acc.: 83.59%] [G loss: 2.359653]\n",
      "epoch:18 step:14279 [D loss: 0.306212, acc.: 87.50%] [G loss: 3.249798]\n",
      "epoch:18 step:14280 [D loss: 0.248361, acc.: 87.50%] [G loss: 3.657166]\n",
      "epoch:18 step:14281 [D loss: 0.382217, acc.: 85.94%] [G loss: 3.290054]\n",
      "epoch:18 step:14282 [D loss: 0.326376, acc.: 86.72%] [G loss: 4.351258]\n",
      "epoch:18 step:14283 [D loss: 0.418346, acc.: 78.91%] [G loss: 4.482897]\n",
      "epoch:18 step:14284 [D loss: 0.463534, acc.: 78.91%] [G loss: 2.649895]\n",
      "epoch:18 step:14285 [D loss: 0.201785, acc.: 91.41%] [G loss: 3.683068]\n",
      "epoch:18 step:14286 [D loss: 0.463396, acc.: 76.56%] [G loss: 3.662462]\n",
      "epoch:18 step:14287 [D loss: 0.548337, acc.: 73.44%] [G loss: 3.778995]\n",
      "epoch:18 step:14288 [D loss: 0.257035, acc.: 87.50%] [G loss: 3.708632]\n",
      "epoch:18 step:14289 [D loss: 0.233068, acc.: 92.19%] [G loss: 3.690083]\n",
      "epoch:18 step:14290 [D loss: 0.326364, acc.: 86.72%] [G loss: 4.174539]\n",
      "epoch:18 step:14291 [D loss: 0.364849, acc.: 84.38%] [G loss: 4.092646]\n",
      "epoch:18 step:14292 [D loss: 0.371473, acc.: 82.03%] [G loss: 2.706560]\n",
      "epoch:18 step:14293 [D loss: 0.354769, acc.: 82.81%] [G loss: 2.894698]\n",
      "epoch:18 step:14294 [D loss: 0.386182, acc.: 81.25%] [G loss: 3.302705]\n",
      "epoch:18 step:14295 [D loss: 0.337039, acc.: 87.50%] [G loss: 3.080824]\n",
      "epoch:18 step:14296 [D loss: 0.349368, acc.: 83.59%] [G loss: 3.969301]\n",
      "epoch:18 step:14297 [D loss: 0.344291, acc.: 88.28%] [G loss: 2.674714]\n",
      "epoch:18 step:14298 [D loss: 0.315466, acc.: 89.06%] [G loss: 3.246127]\n",
      "epoch:18 step:14299 [D loss: 0.409101, acc.: 82.81%] [G loss: 3.304064]\n",
      "epoch:18 step:14300 [D loss: 0.259086, acc.: 89.06%] [G loss: 3.882245]\n",
      "epoch:18 step:14301 [D loss: 0.181510, acc.: 95.31%] [G loss: 3.866278]\n",
      "epoch:18 step:14302 [D loss: 0.348127, acc.: 84.38%] [G loss: 2.738256]\n",
      "epoch:18 step:14303 [D loss: 0.349317, acc.: 83.59%] [G loss: 4.084245]\n",
      "epoch:18 step:14304 [D loss: 0.285576, acc.: 87.50%] [G loss: 3.281696]\n",
      "epoch:18 step:14305 [D loss: 0.370753, acc.: 82.03%] [G loss: 2.978346]\n",
      "epoch:18 step:14306 [D loss: 0.288731, acc.: 88.28%] [G loss: 2.651621]\n",
      "epoch:18 step:14307 [D loss: 0.355792, acc.: 83.59%] [G loss: 2.836425]\n",
      "epoch:18 step:14308 [D loss: 0.436038, acc.: 83.59%] [G loss: 2.867939]\n",
      "epoch:18 step:14309 [D loss: 0.299336, acc.: 89.84%] [G loss: 3.615860]\n",
      "epoch:18 step:14310 [D loss: 0.231769, acc.: 91.41%] [G loss: 3.168296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14311 [D loss: 0.421541, acc.: 80.47%] [G loss: 3.926171]\n",
      "epoch:18 step:14312 [D loss: 0.238340, acc.: 91.41%] [G loss: 3.522986]\n",
      "epoch:18 step:14313 [D loss: 0.284578, acc.: 86.72%] [G loss: 4.309711]\n",
      "epoch:18 step:14314 [D loss: 0.264954, acc.: 85.16%] [G loss: 4.111633]\n",
      "epoch:18 step:14315 [D loss: 0.309008, acc.: 89.06%] [G loss: 3.519820]\n",
      "epoch:18 step:14316 [D loss: 0.231554, acc.: 90.62%] [G loss: 3.603890]\n",
      "epoch:18 step:14317 [D loss: 0.280813, acc.: 88.28%] [G loss: 3.709323]\n",
      "epoch:18 step:14318 [D loss: 0.384676, acc.: 82.81%] [G loss: 3.738272]\n",
      "epoch:18 step:14319 [D loss: 0.431836, acc.: 79.69%] [G loss: 3.733057]\n",
      "epoch:18 step:14320 [D loss: 0.321282, acc.: 85.94%] [G loss: 4.032499]\n",
      "epoch:18 step:14321 [D loss: 0.294661, acc.: 89.84%] [G loss: 3.034903]\n",
      "epoch:18 step:14322 [D loss: 0.326959, acc.: 88.28%] [G loss: 2.907306]\n",
      "epoch:18 step:14323 [D loss: 0.397509, acc.: 79.69%] [G loss: 2.631863]\n",
      "epoch:18 step:14324 [D loss: 0.335954, acc.: 79.69%] [G loss: 4.277654]\n",
      "epoch:18 step:14325 [D loss: 0.383131, acc.: 81.25%] [G loss: 3.424144]\n",
      "epoch:18 step:14326 [D loss: 0.395833, acc.: 79.69%] [G loss: 2.597497]\n",
      "epoch:18 step:14327 [D loss: 0.358699, acc.: 84.38%] [G loss: 3.086293]\n",
      "epoch:18 step:14328 [D loss: 0.358178, acc.: 86.72%] [G loss: 3.640906]\n",
      "epoch:18 step:14329 [D loss: 0.260187, acc.: 88.28%] [G loss: 4.025424]\n",
      "epoch:18 step:14330 [D loss: 0.264646, acc.: 89.84%] [G loss: 3.471801]\n",
      "epoch:18 step:14331 [D loss: 0.312536, acc.: 84.38%] [G loss: 4.649515]\n",
      "epoch:18 step:14332 [D loss: 0.226671, acc.: 90.62%] [G loss: 4.600592]\n",
      "epoch:18 step:14333 [D loss: 0.440851, acc.: 77.34%] [G loss: 2.948911]\n",
      "epoch:18 step:14334 [D loss: 0.309278, acc.: 86.72%] [G loss: 2.391008]\n",
      "epoch:18 step:14335 [D loss: 0.367486, acc.: 81.25%] [G loss: 2.944963]\n",
      "epoch:18 step:14336 [D loss: 0.261907, acc.: 91.41%] [G loss: 3.111997]\n",
      "epoch:18 step:14337 [D loss: 0.349554, acc.: 84.38%] [G loss: 2.153953]\n",
      "epoch:18 step:14338 [D loss: 0.321549, acc.: 85.16%] [G loss: 2.461847]\n",
      "epoch:18 step:14339 [D loss: 0.310663, acc.: 85.16%] [G loss: 3.195164]\n",
      "epoch:18 step:14340 [D loss: 0.352361, acc.: 86.72%] [G loss: 2.889452]\n",
      "epoch:18 step:14341 [D loss: 0.330621, acc.: 87.50%] [G loss: 2.793477]\n",
      "epoch:18 step:14342 [D loss: 0.322673, acc.: 86.72%] [G loss: 4.242328]\n",
      "epoch:18 step:14343 [D loss: 0.341864, acc.: 81.25%] [G loss: 3.227870]\n",
      "epoch:18 step:14344 [D loss: 0.371038, acc.: 79.69%] [G loss: 2.828068]\n",
      "epoch:18 step:14345 [D loss: 0.330715, acc.: 86.72%] [G loss: 2.864789]\n",
      "epoch:18 step:14346 [D loss: 0.448036, acc.: 77.34%] [G loss: 2.514974]\n",
      "epoch:18 step:14347 [D loss: 0.419508, acc.: 78.12%] [G loss: 3.110006]\n",
      "epoch:18 step:14348 [D loss: 0.359849, acc.: 82.81%] [G loss: 3.629958]\n",
      "epoch:18 step:14349 [D loss: 0.382465, acc.: 82.81%] [G loss: 2.466286]\n",
      "epoch:18 step:14350 [D loss: 0.272303, acc.: 85.16%] [G loss: 2.309697]\n",
      "epoch:18 step:14351 [D loss: 0.284543, acc.: 86.72%] [G loss: 2.883026]\n",
      "epoch:18 step:14352 [D loss: 0.287672, acc.: 89.06%] [G loss: 2.797031]\n",
      "epoch:18 step:14353 [D loss: 0.353402, acc.: 85.94%] [G loss: 4.469760]\n",
      "epoch:18 step:14354 [D loss: 0.284251, acc.: 87.50%] [G loss: 2.637069]\n",
      "epoch:18 step:14355 [D loss: 0.444853, acc.: 82.81%] [G loss: 2.861689]\n",
      "epoch:18 step:14356 [D loss: 0.362398, acc.: 86.72%] [G loss: 3.294909]\n",
      "epoch:18 step:14357 [D loss: 0.363801, acc.: 83.59%] [G loss: 4.520152]\n",
      "epoch:18 step:14358 [D loss: 0.325616, acc.: 83.59%] [G loss: 5.525309]\n",
      "epoch:18 step:14359 [D loss: 0.369965, acc.: 83.59%] [G loss: 3.670850]\n",
      "epoch:18 step:14360 [D loss: 0.282990, acc.: 82.81%] [G loss: 5.094448]\n",
      "epoch:18 step:14361 [D loss: 0.314671, acc.: 85.16%] [G loss: 3.467065]\n",
      "epoch:18 step:14362 [D loss: 0.272023, acc.: 89.06%] [G loss: 3.067294]\n",
      "epoch:18 step:14363 [D loss: 0.321290, acc.: 88.28%] [G loss: 2.842531]\n",
      "epoch:18 step:14364 [D loss: 0.258662, acc.: 91.41%] [G loss: 3.467728]\n",
      "epoch:18 step:14365 [D loss: 0.279190, acc.: 89.06%] [G loss: 2.996268]\n",
      "epoch:18 step:14366 [D loss: 0.349723, acc.: 85.94%] [G loss: 2.922600]\n",
      "epoch:18 step:14367 [D loss: 0.343623, acc.: 84.38%] [G loss: 3.219228]\n",
      "epoch:18 step:14368 [D loss: 0.273154, acc.: 90.62%] [G loss: 3.225554]\n",
      "epoch:18 step:14369 [D loss: 0.356481, acc.: 88.28%] [G loss: 3.121420]\n",
      "epoch:18 step:14370 [D loss: 0.355125, acc.: 85.94%] [G loss: 3.226747]\n",
      "epoch:18 step:14371 [D loss: 0.322026, acc.: 85.94%] [G loss: 4.970339]\n",
      "epoch:18 step:14372 [D loss: 0.366603, acc.: 85.94%] [G loss: 2.840972]\n",
      "epoch:18 step:14373 [D loss: 0.549237, acc.: 71.88%] [G loss: 2.375660]\n",
      "epoch:18 step:14374 [D loss: 0.396393, acc.: 82.81%] [G loss: 5.706321]\n",
      "epoch:18 step:14375 [D loss: 0.706924, acc.: 81.25%] [G loss: 8.919102]\n",
      "epoch:18 step:14376 [D loss: 2.284399, acc.: 57.03%] [G loss: 6.062426]\n",
      "epoch:18 step:14377 [D loss: 0.362996, acc.: 85.16%] [G loss: 8.348829]\n",
      "epoch:18 step:14378 [D loss: 0.272273, acc.: 89.84%] [G loss: 5.585797]\n",
      "epoch:18 step:14379 [D loss: 0.319064, acc.: 84.38%] [G loss: 5.545357]\n",
      "epoch:18 step:14380 [D loss: 0.267380, acc.: 87.50%] [G loss: 3.289975]\n",
      "epoch:18 step:14381 [D loss: 0.361422, acc.: 88.28%] [G loss: 7.231251]\n",
      "epoch:18 step:14382 [D loss: 0.248654, acc.: 88.28%] [G loss: 4.630800]\n",
      "epoch:18 step:14383 [D loss: 0.310096, acc.: 82.03%] [G loss: 3.708868]\n",
      "epoch:18 step:14384 [D loss: 0.210073, acc.: 89.84%] [G loss: 5.558094]\n",
      "epoch:18 step:14385 [D loss: 0.308870, acc.: 88.28%] [G loss: 2.934074]\n",
      "epoch:18 step:14386 [D loss: 0.341977, acc.: 84.38%] [G loss: 3.395257]\n",
      "epoch:18 step:14387 [D loss: 0.280068, acc.: 85.94%] [G loss: 2.232670]\n",
      "epoch:18 step:14388 [D loss: 0.369799, acc.: 85.94%] [G loss: 2.453194]\n",
      "epoch:18 step:14389 [D loss: 0.452605, acc.: 82.03%] [G loss: 3.066230]\n",
      "epoch:18 step:14390 [D loss: 0.312969, acc.: 87.50%] [G loss: 3.046338]\n",
      "epoch:18 step:14391 [D loss: 0.313929, acc.: 87.50%] [G loss: 3.296391]\n",
      "epoch:18 step:14392 [D loss: 0.416334, acc.: 76.56%] [G loss: 2.415787]\n",
      "epoch:18 step:14393 [D loss: 0.369090, acc.: 80.47%] [G loss: 2.714170]\n",
      "epoch:18 step:14394 [D loss: 0.306977, acc.: 86.72%] [G loss: 3.536716]\n",
      "epoch:18 step:14395 [D loss: 0.274459, acc.: 89.06%] [G loss: 2.643559]\n",
      "epoch:18 step:14396 [D loss: 0.263935, acc.: 87.50%] [G loss: 3.835386]\n",
      "epoch:18 step:14397 [D loss: 0.189181, acc.: 91.41%] [G loss: 5.899713]\n",
      "epoch:18 step:14398 [D loss: 0.381758, acc.: 85.94%] [G loss: 4.184810]\n",
      "epoch:18 step:14399 [D loss: 0.277222, acc.: 89.84%] [G loss: 4.565552]\n",
      "epoch:18 step:14400 [D loss: 0.348352, acc.: 86.72%] [G loss: 4.124561]\n",
      "epoch:18 step:14401 [D loss: 0.417955, acc.: 79.69%] [G loss: 3.003552]\n",
      "epoch:18 step:14402 [D loss: 0.248383, acc.: 88.28%] [G loss: 2.851880]\n",
      "epoch:18 step:14403 [D loss: 0.373043, acc.: 82.03%] [G loss: 3.896249]\n",
      "epoch:18 step:14404 [D loss: 0.313905, acc.: 83.59%] [G loss: 4.070690]\n",
      "epoch:18 step:14405 [D loss: 0.243927, acc.: 87.50%] [G loss: 5.268923]\n",
      "epoch:18 step:14406 [D loss: 0.335992, acc.: 82.03%] [G loss: 3.252036]\n",
      "epoch:18 step:14407 [D loss: 0.299785, acc.: 86.72%] [G loss: 2.695342]\n",
      "epoch:18 step:14408 [D loss: 0.425746, acc.: 82.03%] [G loss: 2.879215]\n",
      "epoch:18 step:14409 [D loss: 0.532858, acc.: 71.88%] [G loss: 3.667734]\n",
      "epoch:18 step:14410 [D loss: 0.462440, acc.: 75.00%] [G loss: 3.104675]\n",
      "epoch:18 step:14411 [D loss: 0.295607, acc.: 89.84%] [G loss: 3.721751]\n",
      "epoch:18 step:14412 [D loss: 0.316291, acc.: 89.06%] [G loss: 3.089447]\n",
      "epoch:18 step:14413 [D loss: 0.318967, acc.: 86.72%] [G loss: 2.907662]\n",
      "epoch:18 step:14414 [D loss: 0.362256, acc.: 81.25%] [G loss: 4.317269]\n",
      "epoch:18 step:14415 [D loss: 0.306039, acc.: 85.94%] [G loss: 6.616852]\n",
      "epoch:18 step:14416 [D loss: 0.227334, acc.: 89.06%] [G loss: 6.292402]\n",
      "epoch:18 step:14417 [D loss: 0.275882, acc.: 89.06%] [G loss: 5.933383]\n",
      "epoch:18 step:14418 [D loss: 0.295767, acc.: 86.72%] [G loss: 5.945233]\n",
      "epoch:18 step:14419 [D loss: 0.251981, acc.: 88.28%] [G loss: 3.829619]\n",
      "epoch:18 step:14420 [D loss: 0.284458, acc.: 88.28%] [G loss: 3.513927]\n",
      "epoch:18 step:14421 [D loss: 0.224718, acc.: 92.19%] [G loss: 4.629499]\n",
      "epoch:18 step:14422 [D loss: 0.296204, acc.: 86.72%] [G loss: 5.070204]\n",
      "epoch:18 step:14423 [D loss: 0.314573, acc.: 83.59%] [G loss: 3.431039]\n",
      "epoch:18 step:14424 [D loss: 0.277171, acc.: 86.72%] [G loss: 5.916923]\n",
      "epoch:18 step:14425 [D loss: 0.304486, acc.: 86.72%] [G loss: 3.845482]\n",
      "epoch:18 step:14426 [D loss: 0.411345, acc.: 81.25%] [G loss: 2.558769]\n",
      "epoch:18 step:14427 [D loss: 0.282399, acc.: 87.50%] [G loss: 3.954035]\n",
      "epoch:18 step:14428 [D loss: 0.336023, acc.: 82.81%] [G loss: 2.811081]\n",
      "epoch:18 step:14429 [D loss: 0.240343, acc.: 92.97%] [G loss: 3.732032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14430 [D loss: 0.336803, acc.: 88.28%] [G loss: 2.547440]\n",
      "epoch:18 step:14431 [D loss: 0.336764, acc.: 85.94%] [G loss: 3.273244]\n",
      "epoch:18 step:14432 [D loss: 0.269837, acc.: 87.50%] [G loss: 4.793831]\n",
      "epoch:18 step:14433 [D loss: 0.356353, acc.: 81.25%] [G loss: 2.874290]\n",
      "epoch:18 step:14434 [D loss: 0.297584, acc.: 86.72%] [G loss: 4.348647]\n",
      "epoch:18 step:14435 [D loss: 0.223194, acc.: 89.06%] [G loss: 5.489696]\n",
      "epoch:18 step:14436 [D loss: 0.389684, acc.: 78.91%] [G loss: 2.760993]\n",
      "epoch:18 step:14437 [D loss: 0.340826, acc.: 84.38%] [G loss: 3.275527]\n",
      "epoch:18 step:14438 [D loss: 0.288597, acc.: 89.06%] [G loss: 2.193362]\n",
      "epoch:18 step:14439 [D loss: 0.352762, acc.: 83.59%] [G loss: 2.900352]\n",
      "epoch:18 step:14440 [D loss: 0.457492, acc.: 78.91%] [G loss: 2.340940]\n",
      "epoch:18 step:14441 [D loss: 0.332523, acc.: 82.81%] [G loss: 2.772361]\n",
      "epoch:18 step:14442 [D loss: 0.281710, acc.: 87.50%] [G loss: 3.543223]\n",
      "epoch:18 step:14443 [D loss: 0.361589, acc.: 84.38%] [G loss: 3.406311]\n",
      "epoch:18 step:14444 [D loss: 0.371062, acc.: 82.81%] [G loss: 3.100996]\n",
      "epoch:18 step:14445 [D loss: 0.221408, acc.: 89.84%] [G loss: 2.967847]\n",
      "epoch:18 step:14446 [D loss: 0.333450, acc.: 86.72%] [G loss: 4.689219]\n",
      "epoch:18 step:14447 [D loss: 0.212321, acc.: 95.31%] [G loss: 3.300627]\n",
      "epoch:18 step:14448 [D loss: 0.283341, acc.: 86.72%] [G loss: 4.950993]\n",
      "epoch:18 step:14449 [D loss: 0.421649, acc.: 84.38%] [G loss: 6.102676]\n",
      "epoch:18 step:14450 [D loss: 0.238833, acc.: 89.84%] [G loss: 5.065673]\n",
      "epoch:18 step:14451 [D loss: 0.353969, acc.: 78.91%] [G loss: 4.404347]\n",
      "epoch:18 step:14452 [D loss: 0.246317, acc.: 91.41%] [G loss: 3.002900]\n",
      "epoch:18 step:14453 [D loss: 0.405593, acc.: 82.81%] [G loss: 6.139358]\n",
      "epoch:18 step:14454 [D loss: 0.359484, acc.: 86.72%] [G loss: 5.057019]\n",
      "epoch:18 step:14455 [D loss: 0.268518, acc.: 88.28%] [G loss: 3.650195]\n",
      "epoch:18 step:14456 [D loss: 0.345241, acc.: 86.72%] [G loss: 3.187016]\n",
      "epoch:18 step:14457 [D loss: 0.338037, acc.: 86.72%] [G loss: 2.850278]\n",
      "epoch:18 step:14458 [D loss: 0.337910, acc.: 88.28%] [G loss: 2.406239]\n",
      "epoch:18 step:14459 [D loss: 0.367094, acc.: 83.59%] [G loss: 3.683196]\n",
      "epoch:18 step:14460 [D loss: 0.321346, acc.: 87.50%] [G loss: 3.058304]\n",
      "epoch:18 step:14461 [D loss: 0.349807, acc.: 85.94%] [G loss: 3.361112]\n",
      "epoch:18 step:14462 [D loss: 0.280624, acc.: 85.94%] [G loss: 3.377317]\n",
      "epoch:18 step:14463 [D loss: 0.321522, acc.: 85.16%] [G loss: 3.410992]\n",
      "epoch:18 step:14464 [D loss: 0.266529, acc.: 87.50%] [G loss: 3.460656]\n",
      "epoch:18 step:14465 [D loss: 0.246222, acc.: 88.28%] [G loss: 4.417947]\n",
      "epoch:18 step:14466 [D loss: 0.347924, acc.: 81.25%] [G loss: 3.352431]\n",
      "epoch:18 step:14467 [D loss: 0.371678, acc.: 81.25%] [G loss: 2.754047]\n",
      "epoch:18 step:14468 [D loss: 0.398216, acc.: 77.34%] [G loss: 2.777219]\n",
      "epoch:18 step:14469 [D loss: 0.301926, acc.: 89.06%] [G loss: 2.468991]\n",
      "epoch:18 step:14470 [D loss: 0.304441, acc.: 85.94%] [G loss: 2.866550]\n",
      "epoch:18 step:14471 [D loss: 0.366044, acc.: 82.03%] [G loss: 2.630442]\n",
      "epoch:18 step:14472 [D loss: 0.343845, acc.: 85.94%] [G loss: 2.220735]\n",
      "epoch:18 step:14473 [D loss: 0.452831, acc.: 76.56%] [G loss: 2.453900]\n",
      "epoch:18 step:14474 [D loss: 0.321878, acc.: 83.59%] [G loss: 2.300555]\n",
      "epoch:18 step:14475 [D loss: 0.416646, acc.: 83.59%] [G loss: 2.513748]\n",
      "epoch:18 step:14476 [D loss: 0.259665, acc.: 91.41%] [G loss: 2.909421]\n",
      "epoch:18 step:14477 [D loss: 0.286887, acc.: 88.28%] [G loss: 3.080423]\n",
      "epoch:18 step:14478 [D loss: 0.218684, acc.: 85.94%] [G loss: 3.749755]\n",
      "epoch:18 step:14479 [D loss: 0.269401, acc.: 90.62%] [G loss: 3.125287]\n",
      "epoch:18 step:14480 [D loss: 0.317219, acc.: 85.94%] [G loss: 3.325717]\n",
      "epoch:18 step:14481 [D loss: 0.414856, acc.: 78.12%] [G loss: 4.349162]\n",
      "epoch:18 step:14482 [D loss: 0.250303, acc.: 90.62%] [G loss: 3.568931]\n",
      "epoch:18 step:14483 [D loss: 0.453400, acc.: 74.22%] [G loss: 3.309652]\n",
      "epoch:18 step:14484 [D loss: 0.399945, acc.: 83.59%] [G loss: 3.388602]\n",
      "epoch:18 step:14485 [D loss: 0.356051, acc.: 85.16%] [G loss: 3.292651]\n",
      "epoch:18 step:14486 [D loss: 0.391321, acc.: 85.94%] [G loss: 2.771264]\n",
      "epoch:18 step:14487 [D loss: 0.248701, acc.: 89.84%] [G loss: 2.648708]\n",
      "epoch:18 step:14488 [D loss: 0.331810, acc.: 85.94%] [G loss: 2.799870]\n",
      "epoch:18 step:14489 [D loss: 0.289060, acc.: 90.62%] [G loss: 3.170715]\n",
      "epoch:18 step:14490 [D loss: 0.351532, acc.: 84.38%] [G loss: 3.190415]\n",
      "epoch:18 step:14491 [D loss: 0.223689, acc.: 90.62%] [G loss: 3.149645]\n",
      "epoch:18 step:14492 [D loss: 0.364031, acc.: 86.72%] [G loss: 5.100188]\n",
      "epoch:18 step:14493 [D loss: 0.486062, acc.: 80.47%] [G loss: 5.496807]\n",
      "epoch:18 step:14494 [D loss: 0.680956, acc.: 72.66%] [G loss: 7.782815]\n",
      "epoch:18 step:14495 [D loss: 1.419568, acc.: 70.31%] [G loss: 2.910326]\n",
      "epoch:18 step:14496 [D loss: 0.818636, acc.: 66.41%] [G loss: 2.040184]\n",
      "epoch:18 step:14497 [D loss: 0.199370, acc.: 94.53%] [G loss: 3.233944]\n",
      "epoch:18 step:14498 [D loss: 0.389886, acc.: 80.47%] [G loss: 3.520176]\n",
      "epoch:18 step:14499 [D loss: 0.253576, acc.: 90.62%] [G loss: 3.596901]\n",
      "epoch:18 step:14500 [D loss: 0.292009, acc.: 86.72%] [G loss: 2.909048]\n",
      "epoch:18 step:14501 [D loss: 0.429155, acc.: 78.91%] [G loss: 2.928314]\n",
      "epoch:18 step:14502 [D loss: 0.394606, acc.: 78.12%] [G loss: 2.292980]\n",
      "epoch:18 step:14503 [D loss: 0.365745, acc.: 86.72%] [G loss: 2.192359]\n",
      "epoch:18 step:14504 [D loss: 0.327882, acc.: 85.94%] [G loss: 2.621914]\n",
      "epoch:18 step:14505 [D loss: 0.231452, acc.: 90.62%] [G loss: 2.877854]\n",
      "epoch:18 step:14506 [D loss: 0.375690, acc.: 81.25%] [G loss: 2.870929]\n",
      "epoch:18 step:14507 [D loss: 0.364038, acc.: 82.03%] [G loss: 3.041135]\n",
      "epoch:18 step:14508 [D loss: 0.325064, acc.: 85.94%] [G loss: 2.446416]\n",
      "epoch:18 step:14509 [D loss: 0.275017, acc.: 89.84%] [G loss: 2.787740]\n",
      "epoch:18 step:14510 [D loss: 0.385429, acc.: 82.81%] [G loss: 2.618582]\n",
      "epoch:18 step:14511 [D loss: 0.260397, acc.: 90.62%] [G loss: 2.722175]\n",
      "epoch:18 step:14512 [D loss: 0.247058, acc.: 91.41%] [G loss: 2.682859]\n",
      "epoch:18 step:14513 [D loss: 0.355252, acc.: 84.38%] [G loss: 2.152696]\n",
      "epoch:18 step:14514 [D loss: 0.346014, acc.: 83.59%] [G loss: 2.318585]\n",
      "epoch:18 step:14515 [D loss: 0.336571, acc.: 85.94%] [G loss: 2.529692]\n",
      "epoch:18 step:14516 [D loss: 0.329621, acc.: 84.38%] [G loss: 2.223096]\n",
      "epoch:18 step:14517 [D loss: 0.336723, acc.: 86.72%] [G loss: 2.086893]\n",
      "epoch:18 step:14518 [D loss: 0.380820, acc.: 81.25%] [G loss: 2.266229]\n",
      "epoch:18 step:14519 [D loss: 0.349434, acc.: 85.16%] [G loss: 2.301445]\n",
      "epoch:18 step:14520 [D loss: 0.349247, acc.: 84.38%] [G loss: 2.311537]\n",
      "epoch:18 step:14521 [D loss: 0.341987, acc.: 89.06%] [G loss: 2.528881]\n",
      "epoch:18 step:14522 [D loss: 0.318889, acc.: 86.72%] [G loss: 2.891160]\n",
      "epoch:18 step:14523 [D loss: 0.271397, acc.: 86.72%] [G loss: 2.141996]\n",
      "epoch:18 step:14524 [D loss: 0.376162, acc.: 84.38%] [G loss: 2.573092]\n",
      "epoch:18 step:14525 [D loss: 0.288656, acc.: 86.72%] [G loss: 2.631074]\n",
      "epoch:18 step:14526 [D loss: 0.338295, acc.: 85.16%] [G loss: 2.281242]\n",
      "epoch:18 step:14527 [D loss: 0.269476, acc.: 89.06%] [G loss: 2.371493]\n",
      "epoch:18 step:14528 [D loss: 0.248980, acc.: 89.84%] [G loss: 2.789673]\n",
      "epoch:18 step:14529 [D loss: 0.398179, acc.: 85.16%] [G loss: 3.150659]\n",
      "epoch:18 step:14530 [D loss: 0.344082, acc.: 86.72%] [G loss: 2.471684]\n",
      "epoch:18 step:14531 [D loss: 0.316608, acc.: 89.06%] [G loss: 3.693141]\n",
      "epoch:18 step:14532 [D loss: 0.346268, acc.: 78.91%] [G loss: 2.978779]\n",
      "epoch:18 step:14533 [D loss: 0.271094, acc.: 84.38%] [G loss: 3.318588]\n",
      "epoch:18 step:14534 [D loss: 0.306292, acc.: 88.28%] [G loss: 2.775516]\n",
      "epoch:18 step:14535 [D loss: 0.283587, acc.: 89.84%] [G loss: 2.386760]\n",
      "epoch:18 step:14536 [D loss: 0.296089, acc.: 90.62%] [G loss: 3.651305]\n",
      "epoch:18 step:14537 [D loss: 0.335257, acc.: 86.72%] [G loss: 2.184695]\n",
      "epoch:18 step:14538 [D loss: 0.200889, acc.: 94.53%] [G loss: 3.317858]\n",
      "epoch:18 step:14539 [D loss: 0.224585, acc.: 92.19%] [G loss: 3.296412]\n",
      "epoch:18 step:14540 [D loss: 0.238567, acc.: 90.62%] [G loss: 3.416074]\n",
      "epoch:18 step:14541 [D loss: 0.340681, acc.: 83.59%] [G loss: 3.466071]\n",
      "epoch:18 step:14542 [D loss: 0.335228, acc.: 83.59%] [G loss: 3.664704]\n",
      "epoch:18 step:14543 [D loss: 0.243498, acc.: 90.62%] [G loss: 5.108673]\n",
      "epoch:18 step:14544 [D loss: 0.308718, acc.: 85.16%] [G loss: 3.891128]\n",
      "epoch:18 step:14545 [D loss: 0.289767, acc.: 85.94%] [G loss: 3.116985]\n",
      "epoch:18 step:14546 [D loss: 0.266043, acc.: 90.62%] [G loss: 3.556797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14547 [D loss: 0.315537, acc.: 88.28%] [G loss: 4.346697]\n",
      "epoch:18 step:14548 [D loss: 0.290710, acc.: 83.59%] [G loss: 3.680492]\n",
      "epoch:18 step:14549 [D loss: 0.281557, acc.: 88.28%] [G loss: 4.682997]\n",
      "epoch:18 step:14550 [D loss: 0.206565, acc.: 91.41%] [G loss: 5.379530]\n",
      "epoch:18 step:14551 [D loss: 0.235566, acc.: 89.84%] [G loss: 6.340677]\n",
      "epoch:18 step:14552 [D loss: 0.139531, acc.: 93.75%] [G loss: 5.543199]\n",
      "epoch:18 step:14553 [D loss: 0.220592, acc.: 92.19%] [G loss: 4.246621]\n",
      "epoch:18 step:14554 [D loss: 0.223498, acc.: 88.28%] [G loss: 4.126416]\n",
      "epoch:18 step:14555 [D loss: 0.200477, acc.: 96.09%] [G loss: 2.802678]\n",
      "epoch:18 step:14556 [D loss: 0.228764, acc.: 91.41%] [G loss: 3.455801]\n",
      "epoch:18 step:14557 [D loss: 0.253372, acc.: 89.84%] [G loss: 3.482983]\n",
      "epoch:18 step:14558 [D loss: 0.299909, acc.: 89.06%] [G loss: 3.353790]\n",
      "epoch:18 step:14559 [D loss: 0.266234, acc.: 90.62%] [G loss: 3.979033]\n",
      "epoch:18 step:14560 [D loss: 0.324580, acc.: 85.16%] [G loss: 3.683926]\n",
      "epoch:18 step:14561 [D loss: 0.263425, acc.: 89.84%] [G loss: 3.227594]\n",
      "epoch:18 step:14562 [D loss: 0.304234, acc.: 86.72%] [G loss: 3.189835]\n",
      "epoch:18 step:14563 [D loss: 0.271380, acc.: 89.06%] [G loss: 3.081905]\n",
      "epoch:18 step:14564 [D loss: 0.384769, acc.: 83.59%] [G loss: 2.426164]\n",
      "epoch:18 step:14565 [D loss: 0.373406, acc.: 84.38%] [G loss: 2.733327]\n",
      "epoch:18 step:14566 [D loss: 0.324254, acc.: 89.06%] [G loss: 2.639729]\n",
      "epoch:18 step:14567 [D loss: 0.332846, acc.: 85.16%] [G loss: 4.105386]\n",
      "epoch:18 step:14568 [D loss: 0.275194, acc.: 90.62%] [G loss: 3.353768]\n",
      "epoch:18 step:14569 [D loss: 0.280434, acc.: 83.59%] [G loss: 3.629089]\n",
      "epoch:18 step:14570 [D loss: 0.233907, acc.: 89.84%] [G loss: 3.320277]\n",
      "epoch:18 step:14571 [D loss: 0.277471, acc.: 87.50%] [G loss: 4.279222]\n",
      "epoch:18 step:14572 [D loss: 0.301294, acc.: 87.50%] [G loss: 2.823276]\n",
      "epoch:18 step:14573 [D loss: 0.197851, acc.: 94.53%] [G loss: 3.524206]\n",
      "epoch:18 step:14574 [D loss: 0.279275, acc.: 89.06%] [G loss: 2.558485]\n",
      "epoch:18 step:14575 [D loss: 0.216639, acc.: 92.19%] [G loss: 4.261584]\n",
      "epoch:18 step:14576 [D loss: 0.162434, acc.: 92.97%] [G loss: 4.262941]\n",
      "epoch:18 step:14577 [D loss: 0.325678, acc.: 85.94%] [G loss: 2.948479]\n",
      "epoch:18 step:14578 [D loss: 0.201797, acc.: 95.31%] [G loss: 3.008750]\n",
      "epoch:18 step:14579 [D loss: 0.277164, acc.: 89.84%] [G loss: 4.376161]\n",
      "epoch:18 step:14580 [D loss: 0.377284, acc.: 80.47%] [G loss: 3.298382]\n",
      "epoch:18 step:14581 [D loss: 0.436690, acc.: 82.03%] [G loss: 3.195983]\n",
      "epoch:18 step:14582 [D loss: 0.385836, acc.: 82.81%] [G loss: 3.426361]\n",
      "epoch:18 step:14583 [D loss: 0.349325, acc.: 84.38%] [G loss: 2.994792]\n",
      "epoch:18 step:14584 [D loss: 0.391652, acc.: 85.94%] [G loss: 4.858248]\n",
      "epoch:18 step:14585 [D loss: 0.633217, acc.: 76.56%] [G loss: 5.314775]\n",
      "epoch:18 step:14586 [D loss: 0.835446, acc.: 71.88%] [G loss: 3.463186]\n",
      "epoch:18 step:14587 [D loss: 0.496840, acc.: 73.44%] [G loss: 2.913243]\n",
      "epoch:18 step:14588 [D loss: 0.450796, acc.: 84.38%] [G loss: 2.339279]\n",
      "epoch:18 step:14589 [D loss: 0.283804, acc.: 88.28%] [G loss: 2.508467]\n",
      "epoch:18 step:14590 [D loss: 0.312273, acc.: 85.16%] [G loss: 2.816618]\n",
      "epoch:18 step:14591 [D loss: 0.370409, acc.: 83.59%] [G loss: 2.749282]\n",
      "epoch:18 step:14592 [D loss: 0.291172, acc.: 88.28%] [G loss: 2.694937]\n",
      "epoch:18 step:14593 [D loss: 0.278837, acc.: 89.06%] [G loss: 2.733468]\n",
      "epoch:18 step:14594 [D loss: 0.284298, acc.: 85.94%] [G loss: 2.767082]\n",
      "epoch:18 step:14595 [D loss: 0.347499, acc.: 82.81%] [G loss: 2.940896]\n",
      "epoch:18 step:14596 [D loss: 0.277416, acc.: 89.84%] [G loss: 3.362407]\n",
      "epoch:18 step:14597 [D loss: 0.227773, acc.: 92.19%] [G loss: 4.652893]\n",
      "epoch:18 step:14598 [D loss: 0.376738, acc.: 83.59%] [G loss: 3.133588]\n",
      "epoch:18 step:14599 [D loss: 0.381318, acc.: 84.38%] [G loss: 3.434258]\n",
      "epoch:18 step:14600 [D loss: 0.239587, acc.: 88.28%] [G loss: 8.059681]\n",
      "epoch:18 step:14601 [D loss: 0.245054, acc.: 87.50%] [G loss: 6.540403]\n",
      "epoch:18 step:14602 [D loss: 0.216062, acc.: 93.75%] [G loss: 4.727020]\n",
      "epoch:18 step:14603 [D loss: 0.184229, acc.: 94.53%] [G loss: 5.460434]\n",
      "epoch:18 step:14604 [D loss: 0.374062, acc.: 85.16%] [G loss: 4.838981]\n",
      "epoch:18 step:14605 [D loss: 0.247004, acc.: 89.06%] [G loss: 4.248559]\n",
      "epoch:18 step:14606 [D loss: 0.252849, acc.: 88.28%] [G loss: 3.172989]\n",
      "epoch:18 step:14607 [D loss: 0.298800, acc.: 92.97%] [G loss: 2.549316]\n",
      "epoch:18 step:14608 [D loss: 0.278248, acc.: 88.28%] [G loss: 2.999651]\n",
      "epoch:18 step:14609 [D loss: 0.334951, acc.: 86.72%] [G loss: 2.371097]\n",
      "epoch:18 step:14610 [D loss: 0.478980, acc.: 73.44%] [G loss: 4.831176]\n",
      "epoch:18 step:14611 [D loss: 0.510095, acc.: 79.69%] [G loss: 4.151685]\n",
      "epoch:18 step:14612 [D loss: 0.902162, acc.: 70.31%] [G loss: 6.874186]\n",
      "epoch:18 step:14613 [D loss: 2.047663, acc.: 46.88%] [G loss: 4.234682]\n",
      "epoch:18 step:14614 [D loss: 0.930022, acc.: 66.41%] [G loss: 3.682177]\n",
      "epoch:18 step:14615 [D loss: 0.562244, acc.: 73.44%] [G loss: 5.533575]\n",
      "epoch:18 step:14616 [D loss: 0.397977, acc.: 82.03%] [G loss: 2.930482]\n",
      "epoch:18 step:14617 [D loss: 0.240474, acc.: 89.84%] [G loss: 3.630620]\n",
      "epoch:18 step:14618 [D loss: 0.439164, acc.: 85.16%] [G loss: 2.832237]\n",
      "epoch:18 step:14619 [D loss: 0.307761, acc.: 85.94%] [G loss: 3.539433]\n",
      "epoch:18 step:14620 [D loss: 0.459841, acc.: 82.03%] [G loss: 3.144447]\n",
      "epoch:18 step:14621 [D loss: 0.172651, acc.: 93.75%] [G loss: 4.114093]\n",
      "epoch:18 step:14622 [D loss: 0.462053, acc.: 81.25%] [G loss: 2.434153]\n",
      "epoch:18 step:14623 [D loss: 0.299534, acc.: 87.50%] [G loss: 3.588572]\n",
      "epoch:18 step:14624 [D loss: 0.301313, acc.: 87.50%] [G loss: 3.186600]\n",
      "epoch:18 step:14625 [D loss: 0.406301, acc.: 81.25%] [G loss: 2.572463]\n",
      "epoch:18 step:14626 [D loss: 0.280391, acc.: 88.28%] [G loss: 2.653734]\n",
      "epoch:18 step:14627 [D loss: 0.321955, acc.: 85.16%] [G loss: 2.759540]\n",
      "epoch:18 step:14628 [D loss: 0.333968, acc.: 86.72%] [G loss: 2.685947]\n",
      "epoch:18 step:14629 [D loss: 0.415598, acc.: 81.25%] [G loss: 2.719367]\n",
      "epoch:18 step:14630 [D loss: 0.384361, acc.: 87.50%] [G loss: 2.571504]\n",
      "epoch:18 step:14631 [D loss: 0.279187, acc.: 85.94%] [G loss: 2.605273]\n",
      "epoch:18 step:14632 [D loss: 0.310020, acc.: 88.28%] [G loss: 2.764973]\n",
      "epoch:18 step:14633 [D loss: 0.457226, acc.: 82.81%] [G loss: 2.316869]\n",
      "epoch:18 step:14634 [D loss: 0.381874, acc.: 86.72%] [G loss: 2.491429]\n",
      "epoch:18 step:14635 [D loss: 0.255799, acc.: 87.50%] [G loss: 2.833361]\n",
      "epoch:18 step:14636 [D loss: 0.394507, acc.: 82.81%] [G loss: 2.981350]\n",
      "epoch:18 step:14637 [D loss: 0.350456, acc.: 83.59%] [G loss: 2.842271]\n",
      "epoch:18 step:14638 [D loss: 0.424385, acc.: 83.59%] [G loss: 3.216364]\n",
      "epoch:18 step:14639 [D loss: 0.332258, acc.: 82.81%] [G loss: 3.894934]\n",
      "epoch:18 step:14640 [D loss: 0.296871, acc.: 89.84%] [G loss: 4.143366]\n",
      "epoch:18 step:14641 [D loss: 0.390144, acc.: 81.25%] [G loss: 2.136331]\n",
      "epoch:18 step:14642 [D loss: 0.361360, acc.: 82.03%] [G loss: 3.878281]\n",
      "epoch:18 step:14643 [D loss: 0.251450, acc.: 89.06%] [G loss: 4.309436]\n",
      "epoch:18 step:14644 [D loss: 0.445031, acc.: 82.81%] [G loss: 2.985709]\n",
      "epoch:18 step:14645 [D loss: 0.321208, acc.: 86.72%] [G loss: 2.717504]\n",
      "epoch:18 step:14646 [D loss: 0.313266, acc.: 87.50%] [G loss: 3.633288]\n",
      "epoch:18 step:14647 [D loss: 0.253781, acc.: 93.75%] [G loss: 3.211571]\n",
      "epoch:18 step:14648 [D loss: 0.353060, acc.: 85.94%] [G loss: 2.950569]\n",
      "epoch:18 step:14649 [D loss: 0.357946, acc.: 85.94%] [G loss: 2.276674]\n",
      "epoch:18 step:14650 [D loss: 0.354550, acc.: 83.59%] [G loss: 2.997928]\n",
      "epoch:18 step:14651 [D loss: 0.272141, acc.: 89.84%] [G loss: 3.876376]\n",
      "epoch:18 step:14652 [D loss: 0.310133, acc.: 84.38%] [G loss: 4.293106]\n",
      "epoch:18 step:14653 [D loss: 0.367209, acc.: 82.03%] [G loss: 2.262295]\n",
      "epoch:18 step:14654 [D loss: 0.368672, acc.: 86.72%] [G loss: 2.198959]\n",
      "epoch:18 step:14655 [D loss: 0.278169, acc.: 88.28%] [G loss: 2.117432]\n",
      "epoch:18 step:14656 [D loss: 0.320168, acc.: 85.94%] [G loss: 2.687108]\n",
      "epoch:18 step:14657 [D loss: 0.421162, acc.: 78.12%] [G loss: 3.909913]\n",
      "epoch:18 step:14658 [D loss: 0.257773, acc.: 89.84%] [G loss: 4.288060]\n",
      "epoch:18 step:14659 [D loss: 0.286010, acc.: 85.94%] [G loss: 2.937124]\n",
      "epoch:18 step:14660 [D loss: 0.196139, acc.: 93.75%] [G loss: 4.557280]\n",
      "epoch:18 step:14661 [D loss: 0.202166, acc.: 95.31%] [G loss: 5.601762]\n",
      "epoch:18 step:14662 [D loss: 0.344964, acc.: 82.81%] [G loss: 2.602214]\n",
      "epoch:18 step:14663 [D loss: 0.306429, acc.: 86.72%] [G loss: 6.016176]\n",
      "epoch:18 step:14664 [D loss: 0.417789, acc.: 81.25%] [G loss: 3.571820]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14665 [D loss: 0.290125, acc.: 88.28%] [G loss: 3.324477]\n",
      "epoch:18 step:14666 [D loss: 0.373810, acc.: 85.16%] [G loss: 3.209873]\n",
      "epoch:18 step:14667 [D loss: 0.395988, acc.: 83.59%] [G loss: 3.558847]\n",
      "epoch:18 step:14668 [D loss: 0.328451, acc.: 86.72%] [G loss: 3.207880]\n",
      "epoch:18 step:14669 [D loss: 0.380389, acc.: 82.81%] [G loss: 2.718033]\n",
      "epoch:18 step:14670 [D loss: 0.229911, acc.: 90.62%] [G loss: 3.778944]\n",
      "epoch:18 step:14671 [D loss: 0.491075, acc.: 77.34%] [G loss: 3.353021]\n",
      "epoch:18 step:14672 [D loss: 0.197105, acc.: 94.53%] [G loss: 3.853240]\n",
      "epoch:18 step:14673 [D loss: 0.357768, acc.: 83.59%] [G loss: 3.629821]\n",
      "epoch:18 step:14674 [D loss: 0.280387, acc.: 89.84%] [G loss: 4.263817]\n",
      "epoch:18 step:14675 [D loss: 0.464865, acc.: 81.25%] [G loss: 2.656830]\n",
      "epoch:18 step:14676 [D loss: 0.320859, acc.: 82.81%] [G loss: 2.746608]\n",
      "epoch:18 step:14677 [D loss: 0.323022, acc.: 88.28%] [G loss: 3.811141]\n",
      "epoch:18 step:14678 [D loss: 0.209050, acc.: 92.97%] [G loss: 4.464389]\n",
      "epoch:18 step:14679 [D loss: 0.286152, acc.: 86.72%] [G loss: 4.172885]\n",
      "epoch:18 step:14680 [D loss: 0.249154, acc.: 92.19%] [G loss: 3.073677]\n",
      "epoch:18 step:14681 [D loss: 0.258640, acc.: 85.16%] [G loss: 3.308140]\n",
      "epoch:18 step:14682 [D loss: 0.280070, acc.: 85.94%] [G loss: 3.347146]\n",
      "epoch:18 step:14683 [D loss: 0.309790, acc.: 85.16%] [G loss: 3.523939]\n",
      "epoch:18 step:14684 [D loss: 0.224353, acc.: 94.53%] [G loss: 4.370298]\n",
      "epoch:18 step:14685 [D loss: 0.379285, acc.: 82.81%] [G loss: 3.260738]\n",
      "epoch:18 step:14686 [D loss: 0.352051, acc.: 85.16%] [G loss: 5.494944]\n",
      "epoch:18 step:14687 [D loss: 0.275212, acc.: 85.94%] [G loss: 2.201870]\n",
      "epoch:18 step:14688 [D loss: 0.326471, acc.: 85.16%] [G loss: 4.120338]\n",
      "epoch:18 step:14689 [D loss: 0.317645, acc.: 86.72%] [G loss: 3.688136]\n",
      "epoch:18 step:14690 [D loss: 0.290947, acc.: 85.94%] [G loss: 3.776858]\n",
      "epoch:18 step:14691 [D loss: 0.337651, acc.: 82.03%] [G loss: 2.884057]\n",
      "epoch:18 step:14692 [D loss: 0.252615, acc.: 92.19%] [G loss: 3.656714]\n",
      "epoch:18 step:14693 [D loss: 0.258692, acc.: 89.06%] [G loss: 6.672510]\n",
      "epoch:18 step:14694 [D loss: 0.284139, acc.: 88.28%] [G loss: 3.044217]\n",
      "epoch:18 step:14695 [D loss: 0.276089, acc.: 87.50%] [G loss: 4.016176]\n",
      "epoch:18 step:14696 [D loss: 0.218056, acc.: 91.41%] [G loss: 4.866585]\n",
      "epoch:18 step:14697 [D loss: 0.334239, acc.: 85.16%] [G loss: 3.483704]\n",
      "epoch:18 step:14698 [D loss: 0.339061, acc.: 85.16%] [G loss: 2.263922]\n",
      "epoch:18 step:14699 [D loss: 0.283302, acc.: 87.50%] [G loss: 3.287188]\n",
      "epoch:18 step:14700 [D loss: 0.354720, acc.: 85.94%] [G loss: 2.690833]\n",
      "epoch:18 step:14701 [D loss: 0.383206, acc.: 80.47%] [G loss: 4.118473]\n",
      "epoch:18 step:14702 [D loss: 0.194393, acc.: 91.41%] [G loss: 5.525444]\n",
      "epoch:18 step:14703 [D loss: 0.314948, acc.: 88.28%] [G loss: 5.785725]\n",
      "epoch:18 step:14704 [D loss: 0.360928, acc.: 83.59%] [G loss: 3.298863]\n",
      "epoch:18 step:14705 [D loss: 0.244914, acc.: 89.06%] [G loss: 3.697717]\n",
      "epoch:18 step:14706 [D loss: 0.398427, acc.: 81.25%] [G loss: 3.309464]\n",
      "epoch:18 step:14707 [D loss: 0.358165, acc.: 78.12%] [G loss: 2.888652]\n",
      "epoch:18 step:14708 [D loss: 0.330314, acc.: 82.81%] [G loss: 2.931519]\n",
      "epoch:18 step:14709 [D loss: 0.293953, acc.: 86.72%] [G loss: 3.606153]\n",
      "epoch:18 step:14710 [D loss: 0.266778, acc.: 88.28%] [G loss: 3.346227]\n",
      "epoch:18 step:14711 [D loss: 0.187237, acc.: 93.75%] [G loss: 4.287084]\n",
      "epoch:18 step:14712 [D loss: 0.378258, acc.: 83.59%] [G loss: 3.438129]\n",
      "epoch:18 step:14713 [D loss: 0.264851, acc.: 88.28%] [G loss: 2.863859]\n",
      "epoch:18 step:14714 [D loss: 0.303426, acc.: 86.72%] [G loss: 3.696485]\n",
      "epoch:18 step:14715 [D loss: 0.420639, acc.: 76.56%] [G loss: 5.722976]\n",
      "epoch:18 step:14716 [D loss: 0.509445, acc.: 75.78%] [G loss: 3.723023]\n",
      "epoch:18 step:14717 [D loss: 0.396884, acc.: 81.25%] [G loss: 4.381642]\n",
      "epoch:18 step:14718 [D loss: 0.392542, acc.: 84.38%] [G loss: 6.826461]\n",
      "epoch:18 step:14719 [D loss: 0.619183, acc.: 76.56%] [G loss: 2.002266]\n",
      "epoch:18 step:14720 [D loss: 0.454580, acc.: 77.34%] [G loss: 3.406790]\n",
      "epoch:18 step:14721 [D loss: 0.354740, acc.: 82.81%] [G loss: 3.120685]\n",
      "epoch:18 step:14722 [D loss: 0.530596, acc.: 79.69%] [G loss: 3.252622]\n",
      "epoch:18 step:14723 [D loss: 0.370294, acc.: 85.94%] [G loss: 3.016171]\n",
      "epoch:18 step:14724 [D loss: 0.496421, acc.: 78.12%] [G loss: 7.425188]\n",
      "epoch:18 step:14725 [D loss: 0.473469, acc.: 77.34%] [G loss: 2.840553]\n",
      "epoch:18 step:14726 [D loss: 0.385506, acc.: 79.69%] [G loss: 3.702708]\n",
      "epoch:18 step:14727 [D loss: 0.307695, acc.: 85.16%] [G loss: 4.159294]\n",
      "epoch:18 step:14728 [D loss: 0.396565, acc.: 80.47%] [G loss: 3.358676]\n",
      "epoch:18 step:14729 [D loss: 0.313154, acc.: 87.50%] [G loss: 3.421444]\n",
      "epoch:18 step:14730 [D loss: 0.332798, acc.: 87.50%] [G loss: 2.943117]\n",
      "epoch:18 step:14731 [D loss: 0.227777, acc.: 92.97%] [G loss: 4.506691]\n",
      "epoch:18 step:14732 [D loss: 0.357019, acc.: 84.38%] [G loss: 3.220917]\n",
      "epoch:18 step:14733 [D loss: 0.399008, acc.: 78.12%] [G loss: 2.482374]\n",
      "epoch:18 step:14734 [D loss: 0.257521, acc.: 91.41%] [G loss: 2.730464]\n",
      "epoch:18 step:14735 [D loss: 0.325582, acc.: 87.50%] [G loss: 2.967004]\n",
      "epoch:18 step:14736 [D loss: 0.295203, acc.: 86.72%] [G loss: 2.892289]\n",
      "epoch:18 step:14737 [D loss: 0.354091, acc.: 81.25%] [G loss: 2.958304]\n",
      "epoch:18 step:14738 [D loss: 0.339590, acc.: 87.50%] [G loss: 2.664596]\n",
      "epoch:18 step:14739 [D loss: 0.251717, acc.: 89.06%] [G loss: 2.818412]\n",
      "epoch:18 step:14740 [D loss: 0.355218, acc.: 82.81%] [G loss: 1.884450]\n",
      "epoch:18 step:14741 [D loss: 0.336381, acc.: 85.16%] [G loss: 2.194980]\n",
      "epoch:18 step:14742 [D loss: 0.390558, acc.: 79.69%] [G loss: 2.545844]\n",
      "epoch:18 step:14743 [D loss: 0.308979, acc.: 85.94%] [G loss: 2.551106]\n",
      "epoch:18 step:14744 [D loss: 0.272524, acc.: 89.06%] [G loss: 2.320286]\n",
      "epoch:18 step:14745 [D loss: 0.240900, acc.: 88.28%] [G loss: 3.116172]\n",
      "epoch:18 step:14746 [D loss: 0.360823, acc.: 85.16%] [G loss: 4.162967]\n",
      "epoch:18 step:14747 [D loss: 0.377050, acc.: 80.47%] [G loss: 3.636161]\n",
      "epoch:18 step:14748 [D loss: 0.332206, acc.: 83.59%] [G loss: 3.991687]\n",
      "epoch:18 step:14749 [D loss: 0.190174, acc.: 93.75%] [G loss: 5.197046]\n",
      "epoch:18 step:14750 [D loss: 0.362967, acc.: 82.81%] [G loss: 3.107924]\n",
      "epoch:18 step:14751 [D loss: 0.281427, acc.: 85.94%] [G loss: 3.647120]\n",
      "epoch:18 step:14752 [D loss: 0.344398, acc.: 84.38%] [G loss: 2.488052]\n",
      "epoch:18 step:14753 [D loss: 0.332735, acc.: 85.16%] [G loss: 3.452872]\n",
      "epoch:18 step:14754 [D loss: 0.393147, acc.: 83.59%] [G loss: 2.726296]\n",
      "epoch:18 step:14755 [D loss: 0.360092, acc.: 83.59%] [G loss: 2.781293]\n",
      "epoch:18 step:14756 [D loss: 0.362725, acc.: 87.50%] [G loss: 2.714514]\n",
      "epoch:18 step:14757 [D loss: 0.300294, acc.: 85.94%] [G loss: 2.608114]\n",
      "epoch:18 step:14758 [D loss: 0.314299, acc.: 88.28%] [G loss: 2.805423]\n",
      "epoch:18 step:14759 [D loss: 0.329903, acc.: 85.16%] [G loss: 3.266689]\n",
      "epoch:18 step:14760 [D loss: 0.286030, acc.: 89.84%] [G loss: 3.339482]\n",
      "epoch:18 step:14761 [D loss: 0.464554, acc.: 75.78%] [G loss: 2.862311]\n",
      "epoch:18 step:14762 [D loss: 0.225594, acc.: 89.84%] [G loss: 5.091217]\n",
      "epoch:18 step:14763 [D loss: 0.328362, acc.: 85.16%] [G loss: 3.287936]\n",
      "epoch:18 step:14764 [D loss: 0.271609, acc.: 82.81%] [G loss: 3.889275]\n",
      "epoch:18 step:14765 [D loss: 0.181831, acc.: 90.62%] [G loss: 8.233437]\n",
      "epoch:18 step:14766 [D loss: 0.242359, acc.: 90.62%] [G loss: 4.539730]\n",
      "epoch:18 step:14767 [D loss: 0.262940, acc.: 92.19%] [G loss: 4.569528]\n",
      "epoch:18 step:14768 [D loss: 0.318521, acc.: 86.72%] [G loss: 4.427115]\n",
      "epoch:18 step:14769 [D loss: 0.200392, acc.: 92.19%] [G loss: 4.096955]\n",
      "epoch:18 step:14770 [D loss: 0.290271, acc.: 91.41%] [G loss: 4.495695]\n",
      "epoch:18 step:14771 [D loss: 0.311281, acc.: 85.94%] [G loss: 3.871339]\n",
      "epoch:18 step:14772 [D loss: 0.208085, acc.: 95.31%] [G loss: 4.591920]\n",
      "epoch:18 step:14773 [D loss: 0.258583, acc.: 91.41%] [G loss: 3.822842]\n",
      "epoch:18 step:14774 [D loss: 0.175905, acc.: 93.75%] [G loss: 3.332283]\n",
      "epoch:18 step:14775 [D loss: 0.267888, acc.: 87.50%] [G loss: 2.926380]\n",
      "epoch:18 step:14776 [D loss: 0.296184, acc.: 86.72%] [G loss: 4.683098]\n",
      "epoch:18 step:14777 [D loss: 0.299513, acc.: 87.50%] [G loss: 2.785591]\n",
      "epoch:18 step:14778 [D loss: 0.276157, acc.: 90.62%] [G loss: 4.982175]\n",
      "epoch:18 step:14779 [D loss: 0.357500, acc.: 82.81%] [G loss: 4.676673]\n",
      "epoch:18 step:14780 [D loss: 0.304313, acc.: 87.50%] [G loss: 3.482165]\n",
      "epoch:18 step:14781 [D loss: 0.228951, acc.: 90.62%] [G loss: 4.637024]\n",
      "epoch:18 step:14782 [D loss: 0.254779, acc.: 89.06%] [G loss: 2.313461]\n",
      "epoch:18 step:14783 [D loss: 0.263140, acc.: 89.06%] [G loss: 3.578310]\n",
      "epoch:18 step:14784 [D loss: 0.259133, acc.: 86.72%] [G loss: 2.680315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14785 [D loss: 0.325298, acc.: 85.16%] [G loss: 2.801331]\n",
      "epoch:18 step:14786 [D loss: 0.335969, acc.: 83.59%] [G loss: 2.317248]\n",
      "epoch:18 step:14787 [D loss: 0.253485, acc.: 94.53%] [G loss: 2.099878]\n",
      "epoch:18 step:14788 [D loss: 0.387661, acc.: 80.47%] [G loss: 2.977702]\n",
      "epoch:18 step:14789 [D loss: 0.372758, acc.: 82.03%] [G loss: 2.429563]\n",
      "epoch:18 step:14790 [D loss: 0.303409, acc.: 86.72%] [G loss: 3.061601]\n",
      "epoch:18 step:14791 [D loss: 0.297064, acc.: 89.06%] [G loss: 2.566963]\n",
      "epoch:18 step:14792 [D loss: 0.454265, acc.: 81.25%] [G loss: 2.992640]\n",
      "epoch:18 step:14793 [D loss: 0.356483, acc.: 81.25%] [G loss: 2.801741]\n",
      "epoch:18 step:14794 [D loss: 0.295817, acc.: 88.28%] [G loss: 3.185952]\n",
      "epoch:18 step:14795 [D loss: 0.356303, acc.: 82.81%] [G loss: 3.495203]\n",
      "epoch:18 step:14796 [D loss: 0.481990, acc.: 75.78%] [G loss: 3.318450]\n",
      "epoch:18 step:14797 [D loss: 0.315625, acc.: 85.94%] [G loss: 2.719927]\n",
      "epoch:18 step:14798 [D loss: 0.365258, acc.: 85.16%] [G loss: 3.070980]\n",
      "epoch:18 step:14799 [D loss: 0.378421, acc.: 85.94%] [G loss: 3.650891]\n",
      "epoch:18 step:14800 [D loss: 0.370816, acc.: 83.59%] [G loss: 3.319680]\n",
      "epoch:18 step:14801 [D loss: 0.337107, acc.: 85.94%] [G loss: 3.042489]\n",
      "epoch:18 step:14802 [D loss: 0.423679, acc.: 84.38%] [G loss: 4.642684]\n",
      "epoch:18 step:14803 [D loss: 0.282654, acc.: 87.50%] [G loss: 3.776700]\n",
      "epoch:18 step:14804 [D loss: 0.260155, acc.: 89.84%] [G loss: 3.136656]\n",
      "epoch:18 step:14805 [D loss: 0.283339, acc.: 86.72%] [G loss: 2.634060]\n",
      "epoch:18 step:14806 [D loss: 0.378834, acc.: 79.69%] [G loss: 2.708767]\n",
      "epoch:18 step:14807 [D loss: 0.383406, acc.: 85.16%] [G loss: 3.358552]\n",
      "epoch:18 step:14808 [D loss: 0.369580, acc.: 85.94%] [G loss: 2.339221]\n",
      "epoch:18 step:14809 [D loss: 0.371747, acc.: 82.81%] [G loss: 2.200980]\n",
      "epoch:18 step:14810 [D loss: 0.384332, acc.: 82.81%] [G loss: 4.282092]\n",
      "epoch:18 step:14811 [D loss: 0.589278, acc.: 73.44%] [G loss: 6.371244]\n",
      "epoch:18 step:14812 [D loss: 0.617430, acc.: 75.00%] [G loss: 5.226865]\n",
      "epoch:18 step:14813 [D loss: 1.028133, acc.: 60.94%] [G loss: 4.230228]\n",
      "epoch:18 step:14814 [D loss: 0.694861, acc.: 69.53%] [G loss: 3.017304]\n",
      "epoch:18 step:14815 [D loss: 0.337203, acc.: 85.94%] [G loss: 4.686341]\n",
      "epoch:18 step:14816 [D loss: 0.479294, acc.: 80.47%] [G loss: 4.098548]\n",
      "epoch:18 step:14817 [D loss: 0.218180, acc.: 92.97%] [G loss: 4.274224]\n",
      "epoch:18 step:14818 [D loss: 0.270132, acc.: 86.72%] [G loss: 3.299623]\n",
      "epoch:18 step:14819 [D loss: 0.421065, acc.: 86.72%] [G loss: 6.490623]\n",
      "epoch:18 step:14820 [D loss: 0.282556, acc.: 84.38%] [G loss: 2.611034]\n",
      "epoch:18 step:14821 [D loss: 0.456227, acc.: 77.34%] [G loss: 4.772640]\n",
      "epoch:18 step:14822 [D loss: 0.338367, acc.: 87.50%] [G loss: 6.543905]\n",
      "epoch:18 step:14823 [D loss: 0.654950, acc.: 75.78%] [G loss: 3.725819]\n",
      "epoch:18 step:14824 [D loss: 0.263256, acc.: 87.50%] [G loss: 6.176559]\n",
      "epoch:18 step:14825 [D loss: 0.303024, acc.: 85.94%] [G loss: 5.426591]\n",
      "epoch:18 step:14826 [D loss: 0.214962, acc.: 91.41%] [G loss: 3.746212]\n",
      "epoch:18 step:14827 [D loss: 0.291239, acc.: 90.62%] [G loss: 3.418229]\n",
      "epoch:18 step:14828 [D loss: 0.281603, acc.: 89.06%] [G loss: 3.796924]\n",
      "epoch:18 step:14829 [D loss: 0.276402, acc.: 87.50%] [G loss: 4.003000]\n",
      "epoch:18 step:14830 [D loss: 0.322734, acc.: 82.81%] [G loss: 4.388763]\n",
      "epoch:18 step:14831 [D loss: 0.216984, acc.: 90.62%] [G loss: 3.704168]\n",
      "epoch:18 step:14832 [D loss: 0.261304, acc.: 86.72%] [G loss: 2.928170]\n",
      "epoch:18 step:14833 [D loss: 0.292441, acc.: 87.50%] [G loss: 2.529850]\n",
      "epoch:18 step:14834 [D loss: 0.219779, acc.: 92.97%] [G loss: 2.981339]\n",
      "epoch:18 step:14835 [D loss: 0.379836, acc.: 82.81%] [G loss: 2.768532]\n",
      "epoch:18 step:14836 [D loss: 0.256910, acc.: 85.94%] [G loss: 4.564990]\n",
      "epoch:18 step:14837 [D loss: 0.217969, acc.: 91.41%] [G loss: 6.580039]\n",
      "epoch:18 step:14838 [D loss: 0.206167, acc.: 93.75%] [G loss: 5.147377]\n",
      "epoch:18 step:14839 [D loss: 0.349185, acc.: 85.16%] [G loss: 3.204293]\n",
      "epoch:19 step:14840 [D loss: 0.267833, acc.: 89.06%] [G loss: 3.792227]\n",
      "epoch:19 step:14841 [D loss: 0.387369, acc.: 84.38%] [G loss: 2.648773]\n",
      "epoch:19 step:14842 [D loss: 0.397260, acc.: 77.34%] [G loss: 3.529473]\n",
      "epoch:19 step:14843 [D loss: 0.294580, acc.: 87.50%] [G loss: 3.522389]\n",
      "epoch:19 step:14844 [D loss: 0.298486, acc.: 86.72%] [G loss: 2.836648]\n",
      "epoch:19 step:14845 [D loss: 0.278812, acc.: 89.84%] [G loss: 2.687892]\n",
      "epoch:19 step:14846 [D loss: 0.317137, acc.: 85.94%] [G loss: 2.530142]\n",
      "epoch:19 step:14847 [D loss: 0.222452, acc.: 91.41%] [G loss: 2.775697]\n",
      "epoch:19 step:14848 [D loss: 0.344485, acc.: 82.81%] [G loss: 2.513589]\n",
      "epoch:19 step:14849 [D loss: 0.287330, acc.: 89.84%] [G loss: 2.906980]\n",
      "epoch:19 step:14850 [D loss: 0.307388, acc.: 86.72%] [G loss: 3.239335]\n",
      "epoch:19 step:14851 [D loss: 0.282422, acc.: 89.84%] [G loss: 2.537862]\n",
      "epoch:19 step:14852 [D loss: 0.290089, acc.: 85.16%] [G loss: 3.181949]\n",
      "epoch:19 step:14853 [D loss: 0.241983, acc.: 88.28%] [G loss: 3.874405]\n",
      "epoch:19 step:14854 [D loss: 0.377893, acc.: 81.25%] [G loss: 1.973458]\n",
      "epoch:19 step:14855 [D loss: 0.267465, acc.: 89.06%] [G loss: 3.255289]\n",
      "epoch:19 step:14856 [D loss: 0.295162, acc.: 87.50%] [G loss: 2.580799]\n",
      "epoch:19 step:14857 [D loss: 0.315873, acc.: 85.16%] [G loss: 3.035001]\n",
      "epoch:19 step:14858 [D loss: 0.299178, acc.: 85.94%] [G loss: 2.868811]\n",
      "epoch:19 step:14859 [D loss: 0.369116, acc.: 82.81%] [G loss: 2.336896]\n",
      "epoch:19 step:14860 [D loss: 0.308448, acc.: 88.28%] [G loss: 2.657603]\n",
      "epoch:19 step:14861 [D loss: 0.360587, acc.: 85.16%] [G loss: 3.516171]\n",
      "epoch:19 step:14862 [D loss: 0.330728, acc.: 82.81%] [G loss: 3.250010]\n",
      "epoch:19 step:14863 [D loss: 0.262845, acc.: 88.28%] [G loss: 4.527719]\n",
      "epoch:19 step:14864 [D loss: 0.232959, acc.: 90.62%] [G loss: 3.600849]\n",
      "epoch:19 step:14865 [D loss: 0.270950, acc.: 88.28%] [G loss: 2.486019]\n",
      "epoch:19 step:14866 [D loss: 0.284696, acc.: 87.50%] [G loss: 3.700619]\n",
      "epoch:19 step:14867 [D loss: 0.362168, acc.: 81.25%] [G loss: 2.523484]\n",
      "epoch:19 step:14868 [D loss: 0.344666, acc.: 85.16%] [G loss: 2.962063]\n",
      "epoch:19 step:14869 [D loss: 0.214963, acc.: 92.97%] [G loss: 2.873374]\n",
      "epoch:19 step:14870 [D loss: 0.347939, acc.: 81.25%] [G loss: 3.110623]\n",
      "epoch:19 step:14871 [D loss: 0.320947, acc.: 85.16%] [G loss: 4.246897]\n",
      "epoch:19 step:14872 [D loss: 0.326755, acc.: 85.94%] [G loss: 3.387971]\n",
      "epoch:19 step:14873 [D loss: 0.322305, acc.: 86.72%] [G loss: 2.698333]\n",
      "epoch:19 step:14874 [D loss: 0.332544, acc.: 83.59%] [G loss: 2.922591]\n",
      "epoch:19 step:14875 [D loss: 0.277874, acc.: 89.06%] [G loss: 3.258155]\n",
      "epoch:19 step:14876 [D loss: 0.322483, acc.: 84.38%] [G loss: 2.909288]\n",
      "epoch:19 step:14877 [D loss: 0.401585, acc.: 78.12%] [G loss: 3.355332]\n",
      "epoch:19 step:14878 [D loss: 0.226976, acc.: 92.19%] [G loss: 3.818806]\n",
      "epoch:19 step:14879 [D loss: 0.413532, acc.: 80.47%] [G loss: 2.577136]\n",
      "epoch:19 step:14880 [D loss: 0.494343, acc.: 82.03%] [G loss: 2.522107]\n",
      "epoch:19 step:14881 [D loss: 0.327735, acc.: 87.50%] [G loss: 2.883748]\n",
      "epoch:19 step:14882 [D loss: 0.434170, acc.: 77.34%] [G loss: 2.824996]\n",
      "epoch:19 step:14883 [D loss: 0.406639, acc.: 78.91%] [G loss: 4.542497]\n",
      "epoch:19 step:14884 [D loss: 0.427589, acc.: 80.47%] [G loss: 4.783307]\n",
      "epoch:19 step:14885 [D loss: 0.483091, acc.: 77.34%] [G loss: 3.431645]\n",
      "epoch:19 step:14886 [D loss: 0.299705, acc.: 86.72%] [G loss: 4.256082]\n",
      "epoch:19 step:14887 [D loss: 0.311076, acc.: 86.72%] [G loss: 3.139808]\n",
      "epoch:19 step:14888 [D loss: 0.312675, acc.: 88.28%] [G loss: 2.927745]\n",
      "epoch:19 step:14889 [D loss: 0.249211, acc.: 91.41%] [G loss: 3.133286]\n",
      "epoch:19 step:14890 [D loss: 0.279637, acc.: 86.72%] [G loss: 3.705068]\n",
      "epoch:19 step:14891 [D loss: 0.299039, acc.: 86.72%] [G loss: 3.655957]\n",
      "epoch:19 step:14892 [D loss: 0.262527, acc.: 88.28%] [G loss: 2.822899]\n",
      "epoch:19 step:14893 [D loss: 0.290347, acc.: 85.16%] [G loss: 3.664151]\n",
      "epoch:19 step:14894 [D loss: 0.270124, acc.: 87.50%] [G loss: 2.747866]\n",
      "epoch:19 step:14895 [D loss: 0.285298, acc.: 83.59%] [G loss: 2.765756]\n",
      "epoch:19 step:14896 [D loss: 0.373393, acc.: 88.28%] [G loss: 2.479956]\n",
      "epoch:19 step:14897 [D loss: 0.365409, acc.: 82.03%] [G loss: 2.576394]\n",
      "epoch:19 step:14898 [D loss: 0.262350, acc.: 92.19%] [G loss: 2.498240]\n",
      "epoch:19 step:14899 [D loss: 0.374132, acc.: 82.81%] [G loss: 2.662680]\n",
      "epoch:19 step:14900 [D loss: 0.429257, acc.: 78.12%] [G loss: 2.604581]\n",
      "epoch:19 step:14901 [D loss: 0.399453, acc.: 79.69%] [G loss: 2.814100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:14902 [D loss: 0.267613, acc.: 89.84%] [G loss: 3.516176]\n",
      "epoch:19 step:14903 [D loss: 0.419404, acc.: 81.25%] [G loss: 2.874555]\n",
      "epoch:19 step:14904 [D loss: 0.321845, acc.: 87.50%] [G loss: 3.144467]\n",
      "epoch:19 step:14905 [D loss: 0.301709, acc.: 89.84%] [G loss: 2.994789]\n",
      "epoch:19 step:14906 [D loss: 0.347169, acc.: 86.72%] [G loss: 2.555809]\n",
      "epoch:19 step:14907 [D loss: 0.301994, acc.: 86.72%] [G loss: 3.758816]\n",
      "epoch:19 step:14908 [D loss: 0.297130, acc.: 85.16%] [G loss: 4.645904]\n",
      "epoch:19 step:14909 [D loss: 0.304357, acc.: 86.72%] [G loss: 3.026882]\n",
      "epoch:19 step:14910 [D loss: 0.294001, acc.: 85.16%] [G loss: 2.720478]\n",
      "epoch:19 step:14911 [D loss: 0.388461, acc.: 81.25%] [G loss: 3.492890]\n",
      "epoch:19 step:14912 [D loss: 0.451640, acc.: 75.00%] [G loss: 2.873479]\n",
      "epoch:19 step:14913 [D loss: 0.396560, acc.: 80.47%] [G loss: 4.176385]\n",
      "epoch:19 step:14914 [D loss: 0.522038, acc.: 75.78%] [G loss: 4.542752]\n",
      "epoch:19 step:14915 [D loss: 0.526202, acc.: 71.88%] [G loss: 3.052043]\n",
      "epoch:19 step:14916 [D loss: 0.413458, acc.: 81.25%] [G loss: 3.701586]\n",
      "epoch:19 step:14917 [D loss: 0.322526, acc.: 87.50%] [G loss: 3.818950]\n",
      "epoch:19 step:14918 [D loss: 0.357324, acc.: 83.59%] [G loss: 3.257920]\n",
      "epoch:19 step:14919 [D loss: 0.375425, acc.: 82.03%] [G loss: 2.809678]\n",
      "epoch:19 step:14920 [D loss: 0.269372, acc.: 88.28%] [G loss: 3.392556]\n",
      "epoch:19 step:14921 [D loss: 0.238001, acc.: 89.84%] [G loss: 4.087976]\n",
      "epoch:19 step:14922 [D loss: 0.280027, acc.: 87.50%] [G loss: 4.435413]\n",
      "epoch:19 step:14923 [D loss: 0.222875, acc.: 91.41%] [G loss: 4.295648]\n",
      "epoch:19 step:14924 [D loss: 0.331587, acc.: 88.28%] [G loss: 4.958539]\n",
      "epoch:19 step:14925 [D loss: 0.372780, acc.: 82.03%] [G loss: 3.331173]\n",
      "epoch:19 step:14926 [D loss: 0.235054, acc.: 92.19%] [G loss: 4.385611]\n",
      "epoch:19 step:14927 [D loss: 0.294368, acc.: 87.50%] [G loss: 2.812326]\n",
      "epoch:19 step:14928 [D loss: 0.373817, acc.: 82.81%] [G loss: 3.476913]\n",
      "epoch:19 step:14929 [D loss: 0.466795, acc.: 77.34%] [G loss: 3.841766]\n",
      "epoch:19 step:14930 [D loss: 0.354495, acc.: 85.94%] [G loss: 3.728290]\n",
      "epoch:19 step:14931 [D loss: 0.327128, acc.: 86.72%] [G loss: 3.861487]\n",
      "epoch:19 step:14932 [D loss: 0.487533, acc.: 78.12%] [G loss: 2.418500]\n",
      "epoch:19 step:14933 [D loss: 0.312464, acc.: 87.50%] [G loss: 2.828379]\n",
      "epoch:19 step:14934 [D loss: 0.444656, acc.: 81.25%] [G loss: 2.654933]\n",
      "epoch:19 step:14935 [D loss: 0.316051, acc.: 88.28%] [G loss: 3.377614]\n",
      "epoch:19 step:14936 [D loss: 0.305278, acc.: 85.94%] [G loss: 2.965892]\n",
      "epoch:19 step:14937 [D loss: 0.328394, acc.: 85.94%] [G loss: 3.034635]\n",
      "epoch:19 step:14938 [D loss: 0.264364, acc.: 89.06%] [G loss: 6.430842]\n",
      "epoch:19 step:14939 [D loss: 0.373434, acc.: 83.59%] [G loss: 3.813106]\n",
      "epoch:19 step:14940 [D loss: 0.321477, acc.: 86.72%] [G loss: 7.508348]\n",
      "epoch:19 step:14941 [D loss: 0.396240, acc.: 80.47%] [G loss: 3.595123]\n",
      "epoch:19 step:14942 [D loss: 0.290529, acc.: 86.72%] [G loss: 5.772656]\n",
      "epoch:19 step:14943 [D loss: 0.222436, acc.: 92.19%] [G loss: 4.990653]\n",
      "epoch:19 step:14944 [D loss: 0.260215, acc.: 89.84%] [G loss: 4.607168]\n",
      "epoch:19 step:14945 [D loss: 0.333521, acc.: 85.16%] [G loss: 2.809090]\n",
      "epoch:19 step:14946 [D loss: 0.238444, acc.: 88.28%] [G loss: 2.806061]\n",
      "epoch:19 step:14947 [D loss: 0.239477, acc.: 88.28%] [G loss: 4.639684]\n",
      "epoch:19 step:14948 [D loss: 0.360009, acc.: 84.38%] [G loss: 2.626080]\n",
      "epoch:19 step:14949 [D loss: 0.416311, acc.: 76.56%] [G loss: 2.258137]\n",
      "epoch:19 step:14950 [D loss: 0.368585, acc.: 80.47%] [G loss: 2.628982]\n",
      "epoch:19 step:14951 [D loss: 0.332931, acc.: 85.16%] [G loss: 2.728639]\n",
      "epoch:19 step:14952 [D loss: 0.404906, acc.: 76.56%] [G loss: 3.023717]\n",
      "epoch:19 step:14953 [D loss: 0.317334, acc.: 84.38%] [G loss: 4.346643]\n",
      "epoch:19 step:14954 [D loss: 0.309978, acc.: 85.16%] [G loss: 3.781695]\n",
      "epoch:19 step:14955 [D loss: 0.354360, acc.: 81.25%] [G loss: 3.072175]\n",
      "epoch:19 step:14956 [D loss: 0.357112, acc.: 82.81%] [G loss: 4.136044]\n",
      "epoch:19 step:14957 [D loss: 0.306438, acc.: 85.16%] [G loss: 2.904832]\n",
      "epoch:19 step:14958 [D loss: 0.277649, acc.: 87.50%] [G loss: 3.267668]\n",
      "epoch:19 step:14959 [D loss: 0.363037, acc.: 85.16%] [G loss: 3.143331]\n",
      "epoch:19 step:14960 [D loss: 0.360494, acc.: 82.81%] [G loss: 3.143023]\n",
      "epoch:19 step:14961 [D loss: 0.299480, acc.: 84.38%] [G loss: 3.329882]\n",
      "epoch:19 step:14962 [D loss: 0.361338, acc.: 79.69%] [G loss: 3.170498]\n",
      "epoch:19 step:14963 [D loss: 0.313366, acc.: 88.28%] [G loss: 3.527484]\n",
      "epoch:19 step:14964 [D loss: 0.344812, acc.: 86.72%] [G loss: 2.750590]\n",
      "epoch:19 step:14965 [D loss: 0.255377, acc.: 88.28%] [G loss: 2.826226]\n",
      "epoch:19 step:14966 [D loss: 0.317350, acc.: 82.81%] [G loss: 3.098011]\n",
      "epoch:19 step:14967 [D loss: 0.213361, acc.: 92.97%] [G loss: 4.005464]\n",
      "epoch:19 step:14968 [D loss: 0.245038, acc.: 92.97%] [G loss: 4.041824]\n",
      "epoch:19 step:14969 [D loss: 0.397231, acc.: 82.81%] [G loss: 2.643219]\n",
      "epoch:19 step:14970 [D loss: 0.408797, acc.: 78.12%] [G loss: 2.703225]\n",
      "epoch:19 step:14971 [D loss: 0.356029, acc.: 89.84%] [G loss: 3.194669]\n",
      "epoch:19 step:14972 [D loss: 0.282670, acc.: 89.06%] [G loss: 4.143968]\n",
      "epoch:19 step:14973 [D loss: 0.319342, acc.: 85.94%] [G loss: 4.691749]\n",
      "epoch:19 step:14974 [D loss: 0.418318, acc.: 81.25%] [G loss: 4.140832]\n",
      "epoch:19 step:14975 [D loss: 0.274717, acc.: 90.62%] [G loss: 4.471512]\n",
      "epoch:19 step:14976 [D loss: 0.371603, acc.: 81.25%] [G loss: 4.491565]\n",
      "epoch:19 step:14977 [D loss: 0.380250, acc.: 79.69%] [G loss: 2.548461]\n",
      "epoch:19 step:14978 [D loss: 0.303715, acc.: 89.06%] [G loss: 3.380108]\n",
      "epoch:19 step:14979 [D loss: 0.236370, acc.: 89.06%] [G loss: 3.014301]\n",
      "epoch:19 step:14980 [D loss: 0.330111, acc.: 84.38%] [G loss: 3.086117]\n",
      "epoch:19 step:14981 [D loss: 0.360395, acc.: 85.94%] [G loss: 3.406013]\n",
      "epoch:19 step:14982 [D loss: 0.328702, acc.: 85.16%] [G loss: 3.249532]\n",
      "epoch:19 step:14983 [D loss: 0.384614, acc.: 79.69%] [G loss: 5.487546]\n",
      "epoch:19 step:14984 [D loss: 0.648413, acc.: 71.88%] [G loss: 5.445608]\n",
      "epoch:19 step:14985 [D loss: 0.691472, acc.: 77.34%] [G loss: 6.250808]\n",
      "epoch:19 step:14986 [D loss: 0.716161, acc.: 68.75%] [G loss: 4.306448]\n",
      "epoch:19 step:14987 [D loss: 0.333274, acc.: 85.16%] [G loss: 4.756291]\n",
      "epoch:19 step:14988 [D loss: 0.457854, acc.: 78.12%] [G loss: 3.781175]\n",
      "epoch:19 step:14989 [D loss: 0.309725, acc.: 85.94%] [G loss: 3.731553]\n",
      "epoch:19 step:14990 [D loss: 0.426836, acc.: 82.03%] [G loss: 2.287433]\n",
      "epoch:19 step:14991 [D loss: 0.434189, acc.: 81.25%] [G loss: 3.048647]\n",
      "epoch:19 step:14992 [D loss: 0.389342, acc.: 82.81%] [G loss: 2.593929]\n",
      "epoch:19 step:14993 [D loss: 0.303424, acc.: 85.94%] [G loss: 2.874119]\n",
      "epoch:19 step:14994 [D loss: 0.342105, acc.: 85.94%] [G loss: 2.840119]\n",
      "epoch:19 step:14995 [D loss: 0.322403, acc.: 85.16%] [G loss: 2.408046]\n",
      "epoch:19 step:14996 [D loss: 0.366440, acc.: 81.25%] [G loss: 2.527475]\n",
      "epoch:19 step:14997 [D loss: 0.308323, acc.: 87.50%] [G loss: 2.872028]\n",
      "epoch:19 step:14998 [D loss: 0.277940, acc.: 85.94%] [G loss: 2.991414]\n",
      "epoch:19 step:14999 [D loss: 0.486938, acc.: 78.12%] [G loss: 3.120776]\n",
      "epoch:19 step:15000 [D loss: 0.341895, acc.: 82.81%] [G loss: 3.117803]\n",
      "epoch:19 step:15001 [D loss: 0.303449, acc.: 87.50%] [G loss: 4.046731]\n",
      "epoch:19 step:15002 [D loss: 0.383397, acc.: 85.94%] [G loss: 3.070956]\n",
      "epoch:19 step:15003 [D loss: 0.226238, acc.: 91.41%] [G loss: 3.894459]\n",
      "epoch:19 step:15004 [D loss: 0.315451, acc.: 82.81%] [G loss: 3.253218]\n",
      "epoch:19 step:15005 [D loss: 0.286857, acc.: 87.50%] [G loss: 3.720674]\n",
      "epoch:19 step:15006 [D loss: 0.256916, acc.: 90.62%] [G loss: 2.779606]\n",
      "epoch:19 step:15007 [D loss: 0.263759, acc.: 88.28%] [G loss: 3.507894]\n",
      "epoch:19 step:15008 [D loss: 0.405254, acc.: 78.12%] [G loss: 3.177208]\n",
      "epoch:19 step:15009 [D loss: 0.249977, acc.: 89.06%] [G loss: 3.171225]\n",
      "epoch:19 step:15010 [D loss: 0.372984, acc.: 82.81%] [G loss: 2.622622]\n",
      "epoch:19 step:15011 [D loss: 0.350536, acc.: 83.59%] [G loss: 3.317661]\n",
      "epoch:19 step:15012 [D loss: 0.283309, acc.: 89.84%] [G loss: 3.284160]\n",
      "epoch:19 step:15013 [D loss: 0.232914, acc.: 91.41%] [G loss: 2.400474]\n",
      "epoch:19 step:15014 [D loss: 0.289476, acc.: 89.06%] [G loss: 2.738488]\n",
      "epoch:19 step:15015 [D loss: 0.415731, acc.: 85.94%] [G loss: 2.424318]\n",
      "epoch:19 step:15016 [D loss: 0.330613, acc.: 82.81%] [G loss: 2.350197]\n",
      "epoch:19 step:15017 [D loss: 0.327961, acc.: 85.16%] [G loss: 3.293710]\n",
      "epoch:19 step:15018 [D loss: 0.282241, acc.: 85.94%] [G loss: 3.402213]\n",
      "epoch:19 step:15019 [D loss: 0.361257, acc.: 84.38%] [G loss: 3.272319]\n",
      "epoch:19 step:15020 [D loss: 0.295687, acc.: 86.72%] [G loss: 4.458169]\n",
      "epoch:19 step:15021 [D loss: 0.291380, acc.: 85.94%] [G loss: 3.286782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15022 [D loss: 0.323975, acc.: 85.16%] [G loss: 3.341420]\n",
      "epoch:19 step:15023 [D loss: 0.345011, acc.: 80.47%] [G loss: 6.550075]\n",
      "epoch:19 step:15024 [D loss: 0.446011, acc.: 77.34%] [G loss: 2.896290]\n",
      "epoch:19 step:15025 [D loss: 0.210360, acc.: 91.41%] [G loss: 4.762939]\n",
      "epoch:19 step:15026 [D loss: 0.323149, acc.: 89.06%] [G loss: 3.891078]\n",
      "epoch:19 step:15027 [D loss: 0.230317, acc.: 92.97%] [G loss: 5.437995]\n",
      "epoch:19 step:15028 [D loss: 0.287195, acc.: 87.50%] [G loss: 3.803147]\n",
      "epoch:19 step:15029 [D loss: 0.491355, acc.: 75.00%] [G loss: 3.872150]\n",
      "epoch:19 step:15030 [D loss: 0.287520, acc.: 85.94%] [G loss: 3.703167]\n",
      "epoch:19 step:15031 [D loss: 0.283153, acc.: 89.06%] [G loss: 2.866349]\n",
      "epoch:19 step:15032 [D loss: 0.284593, acc.: 86.72%] [G loss: 2.665443]\n",
      "epoch:19 step:15033 [D loss: 0.211629, acc.: 93.75%] [G loss: 3.275491]\n",
      "epoch:19 step:15034 [D loss: 0.383624, acc.: 82.03%] [G loss: 2.998274]\n",
      "epoch:19 step:15035 [D loss: 0.412151, acc.: 85.16%] [G loss: 2.904545]\n",
      "epoch:19 step:15036 [D loss: 0.431624, acc.: 80.47%] [G loss: 2.487285]\n",
      "epoch:19 step:15037 [D loss: 0.291441, acc.: 90.62%] [G loss: 3.259132]\n",
      "epoch:19 step:15038 [D loss: 0.273571, acc.: 93.75%] [G loss: 5.240807]\n",
      "epoch:19 step:15039 [D loss: 0.374680, acc.: 82.03%] [G loss: 4.274240]\n",
      "epoch:19 step:15040 [D loss: 0.313275, acc.: 87.50%] [G loss: 4.078421]\n",
      "epoch:19 step:15041 [D loss: 0.275304, acc.: 92.97%] [G loss: 3.421890]\n",
      "epoch:19 step:15042 [D loss: 0.261446, acc.: 91.41%] [G loss: 4.565741]\n",
      "epoch:19 step:15043 [D loss: 0.234201, acc.: 89.84%] [G loss: 5.891612]\n",
      "epoch:19 step:15044 [D loss: 0.267356, acc.: 89.06%] [G loss: 3.842284]\n",
      "epoch:19 step:15045 [D loss: 0.258595, acc.: 85.94%] [G loss: 5.859934]\n",
      "epoch:19 step:15046 [D loss: 0.306014, acc.: 84.38%] [G loss: 4.546354]\n",
      "epoch:19 step:15047 [D loss: 0.302999, acc.: 85.94%] [G loss: 3.855995]\n",
      "epoch:19 step:15048 [D loss: 0.262771, acc.: 88.28%] [G loss: 3.195468]\n",
      "epoch:19 step:15049 [D loss: 0.275923, acc.: 89.06%] [G loss: 2.817060]\n",
      "epoch:19 step:15050 [D loss: 0.322047, acc.: 83.59%] [G loss: 3.979942]\n",
      "epoch:19 step:15051 [D loss: 0.221186, acc.: 89.84%] [G loss: 3.034324]\n",
      "epoch:19 step:15052 [D loss: 0.342366, acc.: 82.81%] [G loss: 3.296290]\n",
      "epoch:19 step:15053 [D loss: 0.381845, acc.: 82.81%] [G loss: 3.361670]\n",
      "epoch:19 step:15054 [D loss: 0.354395, acc.: 87.50%] [G loss: 3.355267]\n",
      "epoch:19 step:15055 [D loss: 0.314246, acc.: 85.16%] [G loss: 2.797229]\n",
      "epoch:19 step:15056 [D loss: 0.349077, acc.: 83.59%] [G loss: 2.937840]\n",
      "epoch:19 step:15057 [D loss: 0.299534, acc.: 89.84%] [G loss: 2.519378]\n",
      "epoch:19 step:15058 [D loss: 0.304413, acc.: 86.72%] [G loss: 3.037103]\n",
      "epoch:19 step:15059 [D loss: 0.377257, acc.: 81.25%] [G loss: 2.852881]\n",
      "epoch:19 step:15060 [D loss: 0.292925, acc.: 89.06%] [G loss: 3.712669]\n",
      "epoch:19 step:15061 [D loss: 0.243878, acc.: 91.41%] [G loss: 3.912889]\n",
      "epoch:19 step:15062 [D loss: 0.392403, acc.: 82.81%] [G loss: 5.816042]\n",
      "epoch:19 step:15063 [D loss: 0.302913, acc.: 90.62%] [G loss: 3.193526]\n",
      "epoch:19 step:15064 [D loss: 0.325281, acc.: 83.59%] [G loss: 2.607500]\n",
      "epoch:19 step:15065 [D loss: 0.313816, acc.: 87.50%] [G loss: 3.195561]\n",
      "epoch:19 step:15066 [D loss: 0.264553, acc.: 88.28%] [G loss: 3.007030]\n",
      "epoch:19 step:15067 [D loss: 0.297094, acc.: 87.50%] [G loss: 3.782288]\n",
      "epoch:19 step:15068 [D loss: 0.358298, acc.: 82.81%] [G loss: 4.014535]\n",
      "epoch:19 step:15069 [D loss: 0.522360, acc.: 73.44%] [G loss: 5.843197]\n",
      "epoch:19 step:15070 [D loss: 0.520601, acc.: 78.12%] [G loss: 4.386880]\n",
      "epoch:19 step:15071 [D loss: 0.600376, acc.: 78.91%] [G loss: 5.274477]\n",
      "epoch:19 step:15072 [D loss: 0.573027, acc.: 75.00%] [G loss: 2.999278]\n",
      "epoch:19 step:15073 [D loss: 0.336360, acc.: 85.94%] [G loss: 3.894887]\n",
      "epoch:19 step:15074 [D loss: 0.391450, acc.: 81.25%] [G loss: 4.622649]\n",
      "epoch:19 step:15075 [D loss: 0.337114, acc.: 83.59%] [G loss: 5.062348]\n",
      "epoch:19 step:15076 [D loss: 0.353470, acc.: 82.81%] [G loss: 2.909343]\n",
      "epoch:19 step:15077 [D loss: 0.410576, acc.: 78.91%] [G loss: 3.228182]\n",
      "epoch:19 step:15078 [D loss: 0.322924, acc.: 86.72%] [G loss: 2.350143]\n",
      "epoch:19 step:15079 [D loss: 0.218369, acc.: 91.41%] [G loss: 2.564216]\n",
      "epoch:19 step:15080 [D loss: 0.249237, acc.: 89.06%] [G loss: 2.798572]\n",
      "epoch:19 step:15081 [D loss: 0.267463, acc.: 89.06%] [G loss: 2.847761]\n",
      "epoch:19 step:15082 [D loss: 0.315832, acc.: 86.72%] [G loss: 2.777109]\n",
      "epoch:19 step:15083 [D loss: 0.290202, acc.: 88.28%] [G loss: 2.837255]\n",
      "epoch:19 step:15084 [D loss: 0.345770, acc.: 85.16%] [G loss: 2.809229]\n",
      "epoch:19 step:15085 [D loss: 0.286723, acc.: 87.50%] [G loss: 2.503516]\n",
      "epoch:19 step:15086 [D loss: 0.314030, acc.: 85.16%] [G loss: 3.465610]\n",
      "epoch:19 step:15087 [D loss: 0.206843, acc.: 90.62%] [G loss: 4.711228]\n",
      "epoch:19 step:15088 [D loss: 0.342428, acc.: 82.81%] [G loss: 3.826101]\n",
      "epoch:19 step:15089 [D loss: 0.357004, acc.: 83.59%] [G loss: 2.922912]\n",
      "epoch:19 step:15090 [D loss: 0.385628, acc.: 82.81%] [G loss: 3.340173]\n",
      "epoch:19 step:15091 [D loss: 0.227866, acc.: 90.62%] [G loss: 3.165444]\n",
      "epoch:19 step:15092 [D loss: 0.246904, acc.: 89.84%] [G loss: 2.911692]\n",
      "epoch:19 step:15093 [D loss: 0.177399, acc.: 94.53%] [G loss: 3.191923]\n",
      "epoch:19 step:15094 [D loss: 0.377908, acc.: 84.38%] [G loss: 2.572817]\n",
      "epoch:19 step:15095 [D loss: 0.314048, acc.: 85.94%] [G loss: 2.953097]\n",
      "epoch:19 step:15096 [D loss: 0.369323, acc.: 82.81%] [G loss: 2.468124]\n",
      "epoch:19 step:15097 [D loss: 0.378094, acc.: 81.25%] [G loss: 2.994929]\n",
      "epoch:19 step:15098 [D loss: 0.329881, acc.: 85.94%] [G loss: 3.295851]\n",
      "epoch:19 step:15099 [D loss: 0.351894, acc.: 84.38%] [G loss: 3.615595]\n",
      "epoch:19 step:15100 [D loss: 0.241932, acc.: 89.84%] [G loss: 3.362840]\n",
      "epoch:19 step:15101 [D loss: 0.246291, acc.: 88.28%] [G loss: 3.341263]\n",
      "epoch:19 step:15102 [D loss: 0.345262, acc.: 82.03%] [G loss: 3.671572]\n",
      "epoch:19 step:15103 [D loss: 0.241565, acc.: 89.06%] [G loss: 3.277834]\n",
      "epoch:19 step:15104 [D loss: 0.320269, acc.: 86.72%] [G loss: 4.342727]\n",
      "epoch:19 step:15105 [D loss: 0.338084, acc.: 88.28%] [G loss: 5.195499]\n",
      "epoch:19 step:15106 [D loss: 0.369893, acc.: 85.94%] [G loss: 3.236848]\n",
      "epoch:19 step:15107 [D loss: 0.388286, acc.: 82.03%] [G loss: 2.966721]\n",
      "epoch:19 step:15108 [D loss: 0.224643, acc.: 92.19%] [G loss: 3.772403]\n",
      "epoch:19 step:15109 [D loss: 0.336443, acc.: 84.38%] [G loss: 2.313771]\n",
      "epoch:19 step:15110 [D loss: 0.218686, acc.: 89.06%] [G loss: 4.012710]\n",
      "epoch:19 step:15111 [D loss: 0.287365, acc.: 85.94%] [G loss: 3.817880]\n",
      "epoch:19 step:15112 [D loss: 0.416374, acc.: 78.91%] [G loss: 2.981857]\n",
      "epoch:19 step:15113 [D loss: 0.339936, acc.: 85.16%] [G loss: 3.365945]\n",
      "epoch:19 step:15114 [D loss: 0.518283, acc.: 73.44%] [G loss: 3.936954]\n",
      "epoch:19 step:15115 [D loss: 0.356381, acc.: 83.59%] [G loss: 5.291507]\n",
      "epoch:19 step:15116 [D loss: 0.305424, acc.: 84.38%] [G loss: 2.927801]\n",
      "epoch:19 step:15117 [D loss: 0.297086, acc.: 89.06%] [G loss: 4.305263]\n",
      "epoch:19 step:15118 [D loss: 0.207240, acc.: 90.62%] [G loss: 6.787474]\n",
      "epoch:19 step:15119 [D loss: 0.257220, acc.: 86.72%] [G loss: 3.096732]\n",
      "epoch:19 step:15120 [D loss: 0.232487, acc.: 88.28%] [G loss: 4.424039]\n",
      "epoch:19 step:15121 [D loss: 0.262859, acc.: 89.06%] [G loss: 3.213862]\n",
      "epoch:19 step:15122 [D loss: 0.337766, acc.: 82.81%] [G loss: 4.243712]\n",
      "epoch:19 step:15123 [D loss: 0.294056, acc.: 88.28%] [G loss: 3.382096]\n",
      "epoch:19 step:15124 [D loss: 0.259596, acc.: 88.28%] [G loss: 2.969012]\n",
      "epoch:19 step:15125 [D loss: 0.348825, acc.: 83.59%] [G loss: 3.039684]\n",
      "epoch:19 step:15126 [D loss: 0.250387, acc.: 86.72%] [G loss: 5.307813]\n",
      "epoch:19 step:15127 [D loss: 0.327032, acc.: 82.81%] [G loss: 3.892415]\n",
      "epoch:19 step:15128 [D loss: 0.299923, acc.: 85.94%] [G loss: 3.297472]\n",
      "epoch:19 step:15129 [D loss: 0.319927, acc.: 81.25%] [G loss: 4.431811]\n",
      "epoch:19 step:15130 [D loss: 0.201848, acc.: 92.97%] [G loss: 2.686278]\n",
      "epoch:19 step:15131 [D loss: 0.250285, acc.: 89.84%] [G loss: 3.560730]\n",
      "epoch:19 step:15132 [D loss: 0.273893, acc.: 89.06%] [G loss: 3.192097]\n",
      "epoch:19 step:15133 [D loss: 0.285668, acc.: 88.28%] [G loss: 3.482013]\n",
      "epoch:19 step:15134 [D loss: 0.407638, acc.: 82.03%] [G loss: 3.064497]\n",
      "epoch:19 step:15135 [D loss: 0.346116, acc.: 85.16%] [G loss: 3.525356]\n",
      "epoch:19 step:15136 [D loss: 0.337173, acc.: 85.16%] [G loss: 2.726265]\n",
      "epoch:19 step:15137 [D loss: 0.327752, acc.: 88.28%] [G loss: 2.968018]\n",
      "epoch:19 step:15138 [D loss: 0.352302, acc.: 84.38%] [G loss: 3.458880]\n",
      "epoch:19 step:15139 [D loss: 0.307895, acc.: 86.72%] [G loss: 2.431880]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15140 [D loss: 0.348223, acc.: 85.16%] [G loss: 4.436556]\n",
      "epoch:19 step:15141 [D loss: 0.492773, acc.: 78.12%] [G loss: 4.587152]\n",
      "epoch:19 step:15142 [D loss: 0.852741, acc.: 67.97%] [G loss: 7.942673]\n",
      "epoch:19 step:15143 [D loss: 2.017734, acc.: 50.78%] [G loss: 5.380077]\n",
      "epoch:19 step:15144 [D loss: 1.000237, acc.: 70.31%] [G loss: 5.667223]\n",
      "epoch:19 step:15145 [D loss: 0.663274, acc.: 73.44%] [G loss: 3.691426]\n",
      "epoch:19 step:15146 [D loss: 0.347166, acc.: 87.50%] [G loss: 4.321504]\n",
      "epoch:19 step:15147 [D loss: 0.342550, acc.: 87.50%] [G loss: 3.335746]\n",
      "epoch:19 step:15148 [D loss: 0.407378, acc.: 80.47%] [G loss: 3.407914]\n",
      "epoch:19 step:15149 [D loss: 0.203106, acc.: 89.84%] [G loss: 3.513570]\n",
      "epoch:19 step:15150 [D loss: 0.408004, acc.: 83.59%] [G loss: 2.284459]\n",
      "epoch:19 step:15151 [D loss: 0.356485, acc.: 83.59%] [G loss: 3.004139]\n",
      "epoch:19 step:15152 [D loss: 0.472749, acc.: 78.91%] [G loss: 2.222016]\n",
      "epoch:19 step:15153 [D loss: 0.331660, acc.: 83.59%] [G loss: 2.920722]\n",
      "epoch:19 step:15154 [D loss: 0.310049, acc.: 90.62%] [G loss: 2.535950]\n",
      "epoch:19 step:15155 [D loss: 0.311519, acc.: 87.50%] [G loss: 2.674975]\n",
      "epoch:19 step:15156 [D loss: 0.475426, acc.: 78.12%] [G loss: 2.977253]\n",
      "epoch:19 step:15157 [D loss: 0.438826, acc.: 78.12%] [G loss: 2.950223]\n",
      "epoch:19 step:15158 [D loss: 0.362091, acc.: 85.16%] [G loss: 3.264623]\n",
      "epoch:19 step:15159 [D loss: 0.292618, acc.: 87.50%] [G loss: 2.844411]\n",
      "epoch:19 step:15160 [D loss: 0.437544, acc.: 79.69%] [G loss: 3.183146]\n",
      "epoch:19 step:15161 [D loss: 0.223904, acc.: 90.62%] [G loss: 3.415257]\n",
      "epoch:19 step:15162 [D loss: 0.417181, acc.: 82.81%] [G loss: 3.140469]\n",
      "epoch:19 step:15163 [D loss: 0.366864, acc.: 87.50%] [G loss: 3.165717]\n",
      "epoch:19 step:15164 [D loss: 0.369326, acc.: 82.81%] [G loss: 3.523955]\n",
      "epoch:19 step:15165 [D loss: 0.302556, acc.: 87.50%] [G loss: 3.859019]\n",
      "epoch:19 step:15166 [D loss: 0.303988, acc.: 87.50%] [G loss: 2.359981]\n",
      "epoch:19 step:15167 [D loss: 0.246094, acc.: 87.50%] [G loss: 3.584412]\n",
      "epoch:19 step:15168 [D loss: 0.265761, acc.: 89.84%] [G loss: 3.618967]\n",
      "epoch:19 step:15169 [D loss: 0.359428, acc.: 85.16%] [G loss: 2.881824]\n",
      "epoch:19 step:15170 [D loss: 0.374607, acc.: 84.38%] [G loss: 2.620584]\n",
      "epoch:19 step:15171 [D loss: 0.378933, acc.: 83.59%] [G loss: 2.749549]\n",
      "epoch:19 step:15172 [D loss: 0.374644, acc.: 83.59%] [G loss: 2.127537]\n",
      "epoch:19 step:15173 [D loss: 0.449438, acc.: 73.44%] [G loss: 2.377388]\n",
      "epoch:19 step:15174 [D loss: 0.316303, acc.: 86.72%] [G loss: 2.495113]\n",
      "epoch:19 step:15175 [D loss: 0.214444, acc.: 91.41%] [G loss: 3.096843]\n",
      "epoch:19 step:15176 [D loss: 0.330020, acc.: 83.59%] [G loss: 2.813758]\n",
      "epoch:19 step:15177 [D loss: 0.294317, acc.: 84.38%] [G loss: 2.349093]\n",
      "epoch:19 step:15178 [D loss: 0.339629, acc.: 85.94%] [G loss: 3.283201]\n",
      "epoch:19 step:15179 [D loss: 0.368417, acc.: 79.69%] [G loss: 2.822530]\n",
      "epoch:19 step:15180 [D loss: 0.279816, acc.: 88.28%] [G loss: 2.697486]\n",
      "epoch:19 step:15181 [D loss: 0.390599, acc.: 84.38%] [G loss: 3.176552]\n",
      "epoch:19 step:15182 [D loss: 0.344108, acc.: 82.81%] [G loss: 2.489387]\n",
      "epoch:19 step:15183 [D loss: 0.306611, acc.: 87.50%] [G loss: 3.331353]\n",
      "epoch:19 step:15184 [D loss: 0.310787, acc.: 88.28%] [G loss: 2.871506]\n",
      "epoch:19 step:15185 [D loss: 0.394320, acc.: 80.47%] [G loss: 3.113518]\n",
      "epoch:19 step:15186 [D loss: 0.406904, acc.: 83.59%] [G loss: 3.040568]\n",
      "epoch:19 step:15187 [D loss: 0.392121, acc.: 82.03%] [G loss: 2.835770]\n",
      "epoch:19 step:15188 [D loss: 0.392685, acc.: 82.81%] [G loss: 2.556927]\n",
      "epoch:19 step:15189 [D loss: 0.530429, acc.: 72.66%] [G loss: 2.584076]\n",
      "epoch:19 step:15190 [D loss: 0.365276, acc.: 80.47%] [G loss: 4.082807]\n",
      "epoch:19 step:15191 [D loss: 0.271739, acc.: 87.50%] [G loss: 5.470316]\n",
      "epoch:19 step:15192 [D loss: 0.272925, acc.: 86.72%] [G loss: 3.421115]\n",
      "epoch:19 step:15193 [D loss: 0.258036, acc.: 88.28%] [G loss: 6.273102]\n",
      "epoch:19 step:15194 [D loss: 0.245621, acc.: 88.28%] [G loss: 4.268649]\n",
      "epoch:19 step:15195 [D loss: 0.281318, acc.: 85.94%] [G loss: 3.032910]\n",
      "epoch:19 step:15196 [D loss: 0.325391, acc.: 90.62%] [G loss: 3.269489]\n",
      "epoch:19 step:15197 [D loss: 0.338578, acc.: 85.16%] [G loss: 3.123952]\n",
      "epoch:19 step:15198 [D loss: 0.375435, acc.: 82.81%] [G loss: 2.915824]\n",
      "epoch:19 step:15199 [D loss: 0.288701, acc.: 87.50%] [G loss: 2.877867]\n",
      "epoch:19 step:15200 [D loss: 0.270870, acc.: 90.62%] [G loss: 3.145695]\n",
      "epoch:19 step:15201 [D loss: 0.450192, acc.: 77.34%] [G loss: 2.879825]\n",
      "epoch:19 step:15202 [D loss: 0.281896, acc.: 89.84%] [G loss: 2.904554]\n",
      "epoch:19 step:15203 [D loss: 0.317018, acc.: 84.38%] [G loss: 2.664775]\n",
      "epoch:19 step:15204 [D loss: 0.271711, acc.: 89.84%] [G loss: 2.862686]\n",
      "epoch:19 step:15205 [D loss: 0.348271, acc.: 86.72%] [G loss: 2.955007]\n",
      "epoch:19 step:15206 [D loss: 0.456499, acc.: 79.69%] [G loss: 3.306781]\n",
      "epoch:19 step:15207 [D loss: 0.401199, acc.: 79.69%] [G loss: 2.748090]\n",
      "epoch:19 step:15208 [D loss: 0.359551, acc.: 79.69%] [G loss: 3.440771]\n",
      "epoch:19 step:15209 [D loss: 0.263496, acc.: 90.62%] [G loss: 3.510629]\n",
      "epoch:19 step:15210 [D loss: 0.319294, acc.: 86.72%] [G loss: 2.670729]\n",
      "epoch:19 step:15211 [D loss: 0.194501, acc.: 92.97%] [G loss: 4.726762]\n",
      "epoch:19 step:15212 [D loss: 0.303732, acc.: 85.16%] [G loss: 3.328728]\n",
      "epoch:19 step:15213 [D loss: 0.343257, acc.: 82.81%] [G loss: 3.849004]\n",
      "epoch:19 step:15214 [D loss: 0.268650, acc.: 89.06%] [G loss: 4.716822]\n",
      "epoch:19 step:15215 [D loss: 0.202308, acc.: 89.06%] [G loss: 4.072724]\n",
      "epoch:19 step:15216 [D loss: 0.227180, acc.: 90.62%] [G loss: 5.103862]\n",
      "epoch:19 step:15217 [D loss: 0.377158, acc.: 83.59%] [G loss: 4.004601]\n",
      "epoch:19 step:15218 [D loss: 0.417234, acc.: 77.34%] [G loss: 4.151491]\n",
      "epoch:19 step:15219 [D loss: 0.425374, acc.: 84.38%] [G loss: 2.250326]\n",
      "epoch:19 step:15220 [D loss: 0.267425, acc.: 88.28%] [G loss: 2.958181]\n",
      "epoch:19 step:15221 [D loss: 0.293505, acc.: 85.16%] [G loss: 3.151490]\n",
      "epoch:19 step:15222 [D loss: 0.370061, acc.: 85.94%] [G loss: 3.143898]\n",
      "epoch:19 step:15223 [D loss: 0.271427, acc.: 88.28%] [G loss: 3.464103]\n",
      "epoch:19 step:15224 [D loss: 0.270063, acc.: 89.06%] [G loss: 3.094204]\n",
      "epoch:19 step:15225 [D loss: 0.234691, acc.: 90.62%] [G loss: 2.825368]\n",
      "epoch:19 step:15226 [D loss: 0.390888, acc.: 78.12%] [G loss: 2.212020]\n",
      "epoch:19 step:15227 [D loss: 0.280667, acc.: 89.06%] [G loss: 3.887844]\n",
      "epoch:19 step:15228 [D loss: 0.393018, acc.: 82.81%] [G loss: 3.633557]\n",
      "epoch:19 step:15229 [D loss: 0.355394, acc.: 84.38%] [G loss: 5.423610]\n",
      "epoch:19 step:15230 [D loss: 0.383765, acc.: 81.25%] [G loss: 2.931811]\n",
      "epoch:19 step:15231 [D loss: 0.328297, acc.: 85.94%] [G loss: 5.382226]\n",
      "epoch:19 step:15232 [D loss: 0.324556, acc.: 85.16%] [G loss: 2.700352]\n",
      "epoch:19 step:15233 [D loss: 0.372511, acc.: 84.38%] [G loss: 3.291747]\n",
      "epoch:19 step:15234 [D loss: 0.456539, acc.: 76.56%] [G loss: 4.298509]\n",
      "epoch:19 step:15235 [D loss: 0.431924, acc.: 81.25%] [G loss: 3.033290]\n",
      "epoch:19 step:15236 [D loss: 0.364356, acc.: 82.03%] [G loss: 3.708755]\n",
      "epoch:19 step:15237 [D loss: 0.480035, acc.: 75.78%] [G loss: 2.938202]\n",
      "epoch:19 step:15238 [D loss: 0.313845, acc.: 82.81%] [G loss: 3.583969]\n",
      "epoch:19 step:15239 [D loss: 0.488585, acc.: 75.00%] [G loss: 3.336416]\n",
      "epoch:19 step:15240 [D loss: 0.258532, acc.: 90.62%] [G loss: 2.685941]\n",
      "epoch:19 step:15241 [D loss: 0.390851, acc.: 81.25%] [G loss: 2.865807]\n",
      "epoch:19 step:15242 [D loss: 0.415824, acc.: 77.34%] [G loss: 2.938568]\n",
      "epoch:19 step:15243 [D loss: 0.366495, acc.: 79.69%] [G loss: 2.856765]\n",
      "epoch:19 step:15244 [D loss: 0.333074, acc.: 84.38%] [G loss: 2.852557]\n",
      "epoch:19 step:15245 [D loss: 0.544810, acc.: 72.66%] [G loss: 4.786404]\n",
      "epoch:19 step:15246 [D loss: 0.590837, acc.: 78.12%] [G loss: 4.092608]\n",
      "epoch:19 step:15247 [D loss: 0.467170, acc.: 75.78%] [G loss: 3.023536]\n",
      "epoch:19 step:15248 [D loss: 0.349536, acc.: 82.81%] [G loss: 3.406651]\n",
      "epoch:19 step:15249 [D loss: 0.422770, acc.: 74.22%] [G loss: 2.605449]\n",
      "epoch:19 step:15250 [D loss: 0.301806, acc.: 86.72%] [G loss: 3.070513]\n",
      "epoch:19 step:15251 [D loss: 0.226614, acc.: 92.19%] [G loss: 2.887033]\n",
      "epoch:19 step:15252 [D loss: 0.328082, acc.: 82.03%] [G loss: 3.409092]\n",
      "epoch:19 step:15253 [D loss: 0.303456, acc.: 86.72%] [G loss: 3.590821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15254 [D loss: 0.408271, acc.: 82.03%] [G loss: 2.449434]\n",
      "epoch:19 step:15255 [D loss: 0.356109, acc.: 82.81%] [G loss: 2.510000]\n",
      "epoch:19 step:15256 [D loss: 0.317082, acc.: 82.81%] [G loss: 3.409570]\n",
      "epoch:19 step:15257 [D loss: 0.329254, acc.: 82.81%] [G loss: 3.599737]\n",
      "epoch:19 step:15258 [D loss: 0.331250, acc.: 85.94%] [G loss: 3.323097]\n",
      "epoch:19 step:15259 [D loss: 0.285045, acc.: 89.06%] [G loss: 3.415390]\n",
      "epoch:19 step:15260 [D loss: 0.331232, acc.: 82.81%] [G loss: 4.228272]\n",
      "epoch:19 step:15261 [D loss: 0.407337, acc.: 79.69%] [G loss: 5.263916]\n",
      "epoch:19 step:15262 [D loss: 0.392805, acc.: 82.03%] [G loss: 5.238798]\n",
      "epoch:19 step:15263 [D loss: 0.274990, acc.: 87.50%] [G loss: 5.840680]\n",
      "epoch:19 step:15264 [D loss: 0.285622, acc.: 87.50%] [G loss: 4.699724]\n",
      "epoch:19 step:15265 [D loss: 0.271695, acc.: 88.28%] [G loss: 4.521976]\n",
      "epoch:19 step:15266 [D loss: 0.316078, acc.: 85.16%] [G loss: 2.961696]\n",
      "epoch:19 step:15267 [D loss: 0.315424, acc.: 85.94%] [G loss: 3.456166]\n",
      "epoch:19 step:15268 [D loss: 0.359611, acc.: 85.16%] [G loss: 3.068511]\n",
      "epoch:19 step:15269 [D loss: 0.385681, acc.: 84.38%] [G loss: 2.864386]\n",
      "epoch:19 step:15270 [D loss: 0.281018, acc.: 90.62%] [G loss: 2.631130]\n",
      "epoch:19 step:15271 [D loss: 0.546385, acc.: 72.66%] [G loss: 3.612084]\n",
      "epoch:19 step:15272 [D loss: 0.282363, acc.: 87.50%] [G loss: 3.588592]\n",
      "epoch:19 step:15273 [D loss: 0.288039, acc.: 87.50%] [G loss: 3.318377]\n",
      "epoch:19 step:15274 [D loss: 0.502455, acc.: 81.25%] [G loss: 4.441789]\n",
      "epoch:19 step:15275 [D loss: 0.495008, acc.: 78.91%] [G loss: 6.213263]\n",
      "epoch:19 step:15276 [D loss: 0.627448, acc.: 78.91%] [G loss: 4.884671]\n",
      "epoch:19 step:15277 [D loss: 0.766911, acc.: 69.53%] [G loss: 3.951818]\n",
      "epoch:19 step:15278 [D loss: 0.722954, acc.: 67.19%] [G loss: 4.080938]\n",
      "epoch:19 step:15279 [D loss: 0.735184, acc.: 78.12%] [G loss: 4.935779]\n",
      "epoch:19 step:15280 [D loss: 0.607765, acc.: 71.88%] [G loss: 4.569380]\n",
      "epoch:19 step:15281 [D loss: 0.334492, acc.: 82.81%] [G loss: 3.088660]\n",
      "epoch:19 step:15282 [D loss: 0.271606, acc.: 91.41%] [G loss: 3.560098]\n",
      "epoch:19 step:15283 [D loss: 0.489701, acc.: 83.59%] [G loss: 3.502301]\n",
      "epoch:19 step:15284 [D loss: 0.228482, acc.: 92.19%] [G loss: 3.655668]\n",
      "epoch:19 step:15285 [D loss: 0.255829, acc.: 89.84%] [G loss: 2.913684]\n",
      "epoch:19 step:15286 [D loss: 0.474076, acc.: 82.03%] [G loss: 2.569934]\n",
      "epoch:19 step:15287 [D loss: 0.429686, acc.: 78.91%] [G loss: 2.803926]\n",
      "epoch:19 step:15288 [D loss: 0.351509, acc.: 85.16%] [G loss: 2.716779]\n",
      "epoch:19 step:15289 [D loss: 0.404948, acc.: 80.47%] [G loss: 2.245364]\n",
      "epoch:19 step:15290 [D loss: 0.249958, acc.: 88.28%] [G loss: 3.651992]\n",
      "epoch:19 step:15291 [D loss: 0.312875, acc.: 86.72%] [G loss: 3.359855]\n",
      "epoch:19 step:15292 [D loss: 0.402195, acc.: 82.03%] [G loss: 2.188815]\n",
      "epoch:19 step:15293 [D loss: 0.305104, acc.: 87.50%] [G loss: 2.762634]\n",
      "epoch:19 step:15294 [D loss: 0.312492, acc.: 85.94%] [G loss: 3.812258]\n",
      "epoch:19 step:15295 [D loss: 0.388658, acc.: 83.59%] [G loss: 3.152302]\n",
      "epoch:19 step:15296 [D loss: 0.447381, acc.: 78.91%] [G loss: 2.560078]\n",
      "epoch:19 step:15297 [D loss: 0.331256, acc.: 87.50%] [G loss: 2.401072]\n",
      "epoch:19 step:15298 [D loss: 0.304484, acc.: 85.94%] [G loss: 4.607191]\n",
      "epoch:19 step:15299 [D loss: 0.278528, acc.: 90.62%] [G loss: 3.263754]\n",
      "epoch:19 step:15300 [D loss: 0.414009, acc.: 78.91%] [G loss: 3.325845]\n",
      "epoch:19 step:15301 [D loss: 0.293925, acc.: 83.59%] [G loss: 3.050828]\n",
      "epoch:19 step:15302 [D loss: 0.337402, acc.: 84.38%] [G loss: 3.528401]\n",
      "epoch:19 step:15303 [D loss: 0.306750, acc.: 88.28%] [G loss: 4.051456]\n",
      "epoch:19 step:15304 [D loss: 0.276084, acc.: 85.94%] [G loss: 3.173691]\n",
      "epoch:19 step:15305 [D loss: 0.441418, acc.: 81.25%] [G loss: 2.689426]\n",
      "epoch:19 step:15306 [D loss: 0.366824, acc.: 84.38%] [G loss: 3.164866]\n",
      "epoch:19 step:15307 [D loss: 0.445421, acc.: 83.59%] [G loss: 3.079768]\n",
      "epoch:19 step:15308 [D loss: 0.390802, acc.: 84.38%] [G loss: 3.282787]\n",
      "epoch:19 step:15309 [D loss: 0.367305, acc.: 85.16%] [G loss: 2.699525]\n",
      "epoch:19 step:15310 [D loss: 0.278348, acc.: 88.28%] [G loss: 2.532522]\n",
      "epoch:19 step:15311 [D loss: 0.346197, acc.: 82.03%] [G loss: 2.332729]\n",
      "epoch:19 step:15312 [D loss: 0.314522, acc.: 84.38%] [G loss: 2.244138]\n",
      "epoch:19 step:15313 [D loss: 0.407520, acc.: 81.25%] [G loss: 2.357019]\n",
      "epoch:19 step:15314 [D loss: 0.334992, acc.: 83.59%] [G loss: 2.834980]\n",
      "epoch:19 step:15315 [D loss: 0.349294, acc.: 82.03%] [G loss: 2.824946]\n",
      "epoch:19 step:15316 [D loss: 0.290065, acc.: 88.28%] [G loss: 2.808076]\n",
      "epoch:19 step:15317 [D loss: 0.306004, acc.: 85.94%] [G loss: 3.215499]\n",
      "epoch:19 step:15318 [D loss: 0.310169, acc.: 79.69%] [G loss: 3.958044]\n",
      "epoch:19 step:15319 [D loss: 0.253504, acc.: 91.41%] [G loss: 3.112815]\n",
      "epoch:19 step:15320 [D loss: 0.365335, acc.: 82.81%] [G loss: 3.332404]\n",
      "epoch:19 step:15321 [D loss: 0.314824, acc.: 86.72%] [G loss: 3.287384]\n",
      "epoch:19 step:15322 [D loss: 0.282368, acc.: 89.06%] [G loss: 2.516413]\n",
      "epoch:19 step:15323 [D loss: 0.302583, acc.: 89.06%] [G loss: 3.934038]\n",
      "epoch:19 step:15324 [D loss: 0.284861, acc.: 85.16%] [G loss: 5.124959]\n",
      "epoch:19 step:15325 [D loss: 0.421978, acc.: 73.44%] [G loss: 3.276796]\n",
      "epoch:19 step:15326 [D loss: 0.364891, acc.: 85.16%] [G loss: 5.232006]\n",
      "epoch:19 step:15327 [D loss: 0.386583, acc.: 83.59%] [G loss: 4.699136]\n",
      "epoch:19 step:15328 [D loss: 0.303977, acc.: 87.50%] [G loss: 3.311904]\n",
      "epoch:19 step:15329 [D loss: 0.204100, acc.: 94.53%] [G loss: 4.375173]\n",
      "epoch:19 step:15330 [D loss: 0.291623, acc.: 86.72%] [G loss: 2.906534]\n",
      "epoch:19 step:15331 [D loss: 0.406731, acc.: 80.47%] [G loss: 3.247181]\n",
      "epoch:19 step:15332 [D loss: 0.308671, acc.: 85.94%] [G loss: 3.443169]\n",
      "epoch:19 step:15333 [D loss: 0.276371, acc.: 89.84%] [G loss: 3.135845]\n",
      "epoch:19 step:15334 [D loss: 0.374551, acc.: 82.03%] [G loss: 3.230491]\n",
      "epoch:19 step:15335 [D loss: 0.362144, acc.: 81.25%] [G loss: 2.368597]\n",
      "epoch:19 step:15336 [D loss: 0.284253, acc.: 87.50%] [G loss: 3.634780]\n",
      "epoch:19 step:15337 [D loss: 0.271655, acc.: 85.94%] [G loss: 3.734238]\n",
      "epoch:19 step:15338 [D loss: 0.411709, acc.: 80.47%] [G loss: 2.990723]\n",
      "epoch:19 step:15339 [D loss: 0.370269, acc.: 81.25%] [G loss: 3.082778]\n",
      "epoch:19 step:15340 [D loss: 0.293281, acc.: 89.84%] [G loss: 3.426744]\n",
      "epoch:19 step:15341 [D loss: 0.325147, acc.: 81.25%] [G loss: 2.105874]\n",
      "epoch:19 step:15342 [D loss: 0.324526, acc.: 84.38%] [G loss: 3.858416]\n",
      "epoch:19 step:15343 [D loss: 0.287676, acc.: 85.94%] [G loss: 5.289964]\n",
      "epoch:19 step:15344 [D loss: 0.235809, acc.: 90.62%] [G loss: 4.827331]\n",
      "epoch:19 step:15345 [D loss: 0.252611, acc.: 89.84%] [G loss: 3.590913]\n",
      "epoch:19 step:15346 [D loss: 0.228950, acc.: 89.06%] [G loss: 6.828373]\n",
      "epoch:19 step:15347 [D loss: 0.279416, acc.: 88.28%] [G loss: 4.447309]\n",
      "epoch:19 step:15348 [D loss: 0.292501, acc.: 85.94%] [G loss: 3.501233]\n",
      "epoch:19 step:15349 [D loss: 0.378324, acc.: 77.34%] [G loss: 2.913217]\n",
      "epoch:19 step:15350 [D loss: 0.443280, acc.: 82.03%] [G loss: 3.706683]\n",
      "epoch:19 step:15351 [D loss: 0.273616, acc.: 90.62%] [G loss: 2.648873]\n",
      "epoch:19 step:15352 [D loss: 0.392687, acc.: 83.59%] [G loss: 3.685251]\n",
      "epoch:19 step:15353 [D loss: 0.355444, acc.: 83.59%] [G loss: 2.811813]\n",
      "epoch:19 step:15354 [D loss: 0.279603, acc.: 89.84%] [G loss: 3.341674]\n",
      "epoch:19 step:15355 [D loss: 0.234495, acc.: 91.41%] [G loss: 4.013267]\n",
      "epoch:19 step:15356 [D loss: 0.326816, acc.: 84.38%] [G loss: 3.703945]\n",
      "epoch:19 step:15357 [D loss: 0.315391, acc.: 83.59%] [G loss: 4.603407]\n",
      "epoch:19 step:15358 [D loss: 0.521113, acc.: 78.91%] [G loss: 4.397637]\n",
      "epoch:19 step:15359 [D loss: 0.437335, acc.: 85.94%] [G loss: 3.214898]\n",
      "epoch:19 step:15360 [D loss: 0.342672, acc.: 85.94%] [G loss: 3.484618]\n",
      "epoch:19 step:15361 [D loss: 0.361727, acc.: 85.16%] [G loss: 5.576721]\n",
      "epoch:19 step:15362 [D loss: 0.590232, acc.: 78.12%] [G loss: 3.588830]\n",
      "epoch:19 step:15363 [D loss: 0.606640, acc.: 78.12%] [G loss: 2.788488]\n",
      "epoch:19 step:15364 [D loss: 0.294468, acc.: 87.50%] [G loss: 3.014743]\n",
      "epoch:19 step:15365 [D loss: 0.446140, acc.: 78.91%] [G loss: 3.812735]\n",
      "epoch:19 step:15366 [D loss: 0.377104, acc.: 84.38%] [G loss: 2.504148]\n",
      "epoch:19 step:15367 [D loss: 0.298011, acc.: 89.84%] [G loss: 3.480415]\n",
      "epoch:19 step:15368 [D loss: 0.325937, acc.: 85.16%] [G loss: 2.411561]\n",
      "epoch:19 step:15369 [D loss: 0.323989, acc.: 85.94%] [G loss: 2.784616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15370 [D loss: 0.296283, acc.: 87.50%] [G loss: 2.473887]\n",
      "epoch:19 step:15371 [D loss: 0.360292, acc.: 82.81%] [G loss: 2.483454]\n",
      "epoch:19 step:15372 [D loss: 0.288161, acc.: 89.06%] [G loss: 2.825574]\n",
      "epoch:19 step:15373 [D loss: 0.364156, acc.: 85.16%] [G loss: 2.904645]\n",
      "epoch:19 step:15374 [D loss: 0.356327, acc.: 85.94%] [G loss: 2.468821]\n",
      "epoch:19 step:15375 [D loss: 0.338391, acc.: 86.72%] [G loss: 3.008358]\n",
      "epoch:19 step:15376 [D loss: 0.497303, acc.: 75.78%] [G loss: 2.518532]\n",
      "epoch:19 step:15377 [D loss: 0.288468, acc.: 91.41%] [G loss: 2.426996]\n",
      "epoch:19 step:15378 [D loss: 0.384968, acc.: 78.91%] [G loss: 3.430956]\n",
      "epoch:19 step:15379 [D loss: 0.338115, acc.: 87.50%] [G loss: 2.742561]\n",
      "epoch:19 step:15380 [D loss: 0.468293, acc.: 76.56%] [G loss: 2.134518]\n",
      "epoch:19 step:15381 [D loss: 0.329435, acc.: 84.38%] [G loss: 2.441409]\n",
      "epoch:19 step:15382 [D loss: 0.292587, acc.: 83.59%] [G loss: 2.302483]\n",
      "epoch:19 step:15383 [D loss: 0.294555, acc.: 88.28%] [G loss: 2.966586]\n",
      "epoch:19 step:15384 [D loss: 0.343626, acc.: 85.16%] [G loss: 2.850442]\n",
      "epoch:19 step:15385 [D loss: 0.290006, acc.: 85.94%] [G loss: 2.364335]\n",
      "epoch:19 step:15386 [D loss: 0.325637, acc.: 88.28%] [G loss: 2.268833]\n",
      "epoch:19 step:15387 [D loss: 0.326581, acc.: 85.16%] [G loss: 2.873262]\n",
      "epoch:19 step:15388 [D loss: 0.313683, acc.: 85.16%] [G loss: 2.235956]\n",
      "epoch:19 step:15389 [D loss: 0.400596, acc.: 83.59%] [G loss: 2.172382]\n",
      "epoch:19 step:15390 [D loss: 0.372083, acc.: 83.59%] [G loss: 2.924362]\n",
      "epoch:19 step:15391 [D loss: 0.355208, acc.: 85.94%] [G loss: 2.659834]\n",
      "epoch:19 step:15392 [D loss: 0.326878, acc.: 87.50%] [G loss: 3.239583]\n",
      "epoch:19 step:15393 [D loss: 0.366878, acc.: 82.81%] [G loss: 2.452262]\n",
      "epoch:19 step:15394 [D loss: 0.369415, acc.: 80.47%] [G loss: 2.962251]\n",
      "epoch:19 step:15395 [D loss: 0.370628, acc.: 83.59%] [G loss: 2.189006]\n",
      "epoch:19 step:15396 [D loss: 0.271403, acc.: 88.28%] [G loss: 2.146749]\n",
      "epoch:19 step:15397 [D loss: 0.398784, acc.: 81.25%] [G loss: 2.199873]\n",
      "epoch:19 step:15398 [D loss: 0.420558, acc.: 77.34%] [G loss: 2.309793]\n",
      "epoch:19 step:15399 [D loss: 0.359721, acc.: 82.81%] [G loss: 2.751480]\n",
      "epoch:19 step:15400 [D loss: 0.380218, acc.: 82.81%] [G loss: 3.646143]\n",
      "epoch:19 step:15401 [D loss: 0.268674, acc.: 89.84%] [G loss: 3.908065]\n",
      "epoch:19 step:15402 [D loss: 0.370368, acc.: 82.81%] [G loss: 2.968209]\n",
      "epoch:19 step:15403 [D loss: 0.373805, acc.: 82.81%] [G loss: 3.598626]\n",
      "epoch:19 step:15404 [D loss: 0.257837, acc.: 92.97%] [G loss: 3.379369]\n",
      "epoch:19 step:15405 [D loss: 0.237747, acc.: 89.84%] [G loss: 3.415478]\n",
      "epoch:19 step:15406 [D loss: 0.276769, acc.: 91.41%] [G loss: 3.355004]\n",
      "epoch:19 step:15407 [D loss: 0.346752, acc.: 83.59%] [G loss: 4.719692]\n",
      "epoch:19 step:15408 [D loss: 0.377739, acc.: 83.59%] [G loss: 3.307376]\n",
      "epoch:19 step:15409 [D loss: 0.314118, acc.: 86.72%] [G loss: 3.558639]\n",
      "epoch:19 step:15410 [D loss: 0.372971, acc.: 85.94%] [G loss: 4.717765]\n",
      "epoch:19 step:15411 [D loss: 0.214671, acc.: 91.41%] [G loss: 4.552967]\n",
      "epoch:19 step:15412 [D loss: 0.198675, acc.: 94.53%] [G loss: 3.508806]\n",
      "epoch:19 step:15413 [D loss: 0.328705, acc.: 81.25%] [G loss: 3.011194]\n",
      "epoch:19 step:15414 [D loss: 0.305562, acc.: 85.94%] [G loss: 2.597568]\n",
      "epoch:19 step:15415 [D loss: 0.334649, acc.: 86.72%] [G loss: 3.165840]\n",
      "epoch:19 step:15416 [D loss: 0.329593, acc.: 85.16%] [G loss: 2.689725]\n",
      "epoch:19 step:15417 [D loss: 0.261041, acc.: 91.41%] [G loss: 4.597242]\n",
      "epoch:19 step:15418 [D loss: 0.250326, acc.: 89.06%] [G loss: 2.963740]\n",
      "epoch:19 step:15419 [D loss: 0.484626, acc.: 75.00%] [G loss: 2.468486]\n",
      "epoch:19 step:15420 [D loss: 0.356797, acc.: 82.03%] [G loss: 3.405786]\n",
      "epoch:19 step:15421 [D loss: 0.343698, acc.: 85.94%] [G loss: 4.580337]\n",
      "epoch:19 step:15422 [D loss: 0.395999, acc.: 84.38%] [G loss: 3.620845]\n",
      "epoch:19 step:15423 [D loss: 0.248397, acc.: 91.41%] [G loss: 3.179505]\n",
      "epoch:19 step:15424 [D loss: 0.327798, acc.: 88.28%] [G loss: 4.012686]\n",
      "epoch:19 step:15425 [D loss: 0.294694, acc.: 81.25%] [G loss: 4.760929]\n",
      "epoch:19 step:15426 [D loss: 0.185910, acc.: 92.97%] [G loss: 3.372273]\n",
      "epoch:19 step:15427 [D loss: 0.204381, acc.: 91.41%] [G loss: 4.676322]\n",
      "epoch:19 step:15428 [D loss: 0.397217, acc.: 81.25%] [G loss: 2.283935]\n",
      "epoch:19 step:15429 [D loss: 0.265576, acc.: 88.28%] [G loss: 3.622717]\n",
      "epoch:19 step:15430 [D loss: 0.326942, acc.: 82.81%] [G loss: 3.118795]\n",
      "epoch:19 step:15431 [D loss: 0.284494, acc.: 84.38%] [G loss: 3.877165]\n",
      "epoch:19 step:15432 [D loss: 0.320092, acc.: 85.94%] [G loss: 6.046357]\n",
      "epoch:19 step:15433 [D loss: 0.244873, acc.: 90.62%] [G loss: 4.267079]\n",
      "epoch:19 step:15434 [D loss: 0.202512, acc.: 92.97%] [G loss: 4.083800]\n",
      "epoch:19 step:15435 [D loss: 0.394399, acc.: 81.25%] [G loss: 2.731650]\n",
      "epoch:19 step:15436 [D loss: 0.311435, acc.: 89.06%] [G loss: 2.480798]\n",
      "epoch:19 step:15437 [D loss: 0.237958, acc.: 88.28%] [G loss: 2.845398]\n",
      "epoch:19 step:15438 [D loss: 0.258482, acc.: 89.84%] [G loss: 4.017223]\n",
      "epoch:19 step:15439 [D loss: 0.298929, acc.: 85.94%] [G loss: 2.879815]\n",
      "epoch:19 step:15440 [D loss: 0.363513, acc.: 82.81%] [G loss: 3.147360]\n",
      "epoch:19 step:15441 [D loss: 0.207385, acc.: 92.19%] [G loss: 3.014040]\n",
      "epoch:19 step:15442 [D loss: 0.248101, acc.: 91.41%] [G loss: 4.047893]\n",
      "epoch:19 step:15443 [D loss: 0.297522, acc.: 87.50%] [G loss: 2.790909]\n",
      "epoch:19 step:15444 [D loss: 0.389287, acc.: 78.91%] [G loss: 2.657836]\n",
      "epoch:19 step:15445 [D loss: 0.391863, acc.: 81.25%] [G loss: 3.249291]\n",
      "epoch:19 step:15446 [D loss: 0.234623, acc.: 89.84%] [G loss: 3.427945]\n",
      "epoch:19 step:15447 [D loss: 0.279047, acc.: 85.94%] [G loss: 2.651363]\n",
      "epoch:19 step:15448 [D loss: 0.312461, acc.: 89.06%] [G loss: 2.449907]\n",
      "epoch:19 step:15449 [D loss: 0.282363, acc.: 88.28%] [G loss: 2.613548]\n",
      "epoch:19 step:15450 [D loss: 0.318807, acc.: 90.62%] [G loss: 4.529775]\n",
      "epoch:19 step:15451 [D loss: 0.505956, acc.: 73.44%] [G loss: 5.392553]\n",
      "epoch:19 step:15452 [D loss: 0.691482, acc.: 71.88%] [G loss: 7.737852]\n",
      "epoch:19 step:15453 [D loss: 0.796706, acc.: 74.22%] [G loss: 5.301825]\n",
      "epoch:19 step:15454 [D loss: 1.189889, acc.: 71.09%] [G loss: 3.004056]\n",
      "epoch:19 step:15455 [D loss: 0.413272, acc.: 84.38%] [G loss: 3.566968]\n",
      "epoch:19 step:15456 [D loss: 0.550393, acc.: 79.69%] [G loss: 4.395523]\n",
      "epoch:19 step:15457 [D loss: 0.407381, acc.: 81.25%] [G loss: 3.191111]\n",
      "epoch:19 step:15458 [D loss: 0.367663, acc.: 85.94%] [G loss: 2.881249]\n",
      "epoch:19 step:15459 [D loss: 0.237414, acc.: 90.62%] [G loss: 2.955325]\n",
      "epoch:19 step:15460 [D loss: 0.360007, acc.: 83.59%] [G loss: 2.511014]\n",
      "epoch:19 step:15461 [D loss: 0.366016, acc.: 85.94%] [G loss: 3.198773]\n",
      "epoch:19 step:15462 [D loss: 0.343433, acc.: 83.59%] [G loss: 3.307192]\n",
      "epoch:19 step:15463 [D loss: 0.316366, acc.: 89.06%] [G loss: 3.036238]\n",
      "epoch:19 step:15464 [D loss: 0.370814, acc.: 82.03%] [G loss: 3.157848]\n",
      "epoch:19 step:15465 [D loss: 0.305181, acc.: 88.28%] [G loss: 2.011240]\n",
      "epoch:19 step:15466 [D loss: 0.256257, acc.: 88.28%] [G loss: 3.390590]\n",
      "epoch:19 step:15467 [D loss: 0.386573, acc.: 83.59%] [G loss: 2.555568]\n",
      "epoch:19 step:15468 [D loss: 0.283940, acc.: 89.84%] [G loss: 2.564858]\n",
      "epoch:19 step:15469 [D loss: 0.436115, acc.: 77.34%] [G loss: 2.457454]\n",
      "epoch:19 step:15470 [D loss: 0.327822, acc.: 85.16%] [G loss: 4.077136]\n",
      "epoch:19 step:15471 [D loss: 0.315563, acc.: 87.50%] [G loss: 3.446389]\n",
      "epoch:19 step:15472 [D loss: 0.335597, acc.: 81.25%] [G loss: 4.187565]\n",
      "epoch:19 step:15473 [D loss: 0.410372, acc.: 80.47%] [G loss: 2.513648]\n",
      "epoch:19 step:15474 [D loss: 0.344977, acc.: 82.03%] [G loss: 2.991992]\n",
      "epoch:19 step:15475 [D loss: 0.252406, acc.: 89.84%] [G loss: 2.699317]\n",
      "epoch:19 step:15476 [D loss: 0.327654, acc.: 84.38%] [G loss: 3.382956]\n",
      "epoch:19 step:15477 [D loss: 0.262223, acc.: 88.28%] [G loss: 2.917099]\n",
      "epoch:19 step:15478 [D loss: 0.338318, acc.: 85.16%] [G loss: 2.660461]\n",
      "epoch:19 step:15479 [D loss: 0.293021, acc.: 89.06%] [G loss: 2.586213]\n",
      "epoch:19 step:15480 [D loss: 0.208349, acc.: 92.97%] [G loss: 3.166289]\n",
      "epoch:19 step:15481 [D loss: 0.326670, acc.: 85.16%] [G loss: 3.609866]\n",
      "epoch:19 step:15482 [D loss: 0.392509, acc.: 84.38%] [G loss: 2.340765]\n",
      "epoch:19 step:15483 [D loss: 0.284078, acc.: 89.06%] [G loss: 3.018933]\n",
      "epoch:19 step:15484 [D loss: 0.293629, acc.: 88.28%] [G loss: 3.761937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15485 [D loss: 0.288990, acc.: 89.06%] [G loss: 3.941888]\n",
      "epoch:19 step:15486 [D loss: 0.334827, acc.: 85.16%] [G loss: 2.806881]\n",
      "epoch:19 step:15487 [D loss: 0.341512, acc.: 82.81%] [G loss: 4.530529]\n",
      "epoch:19 step:15488 [D loss: 0.335279, acc.: 82.03%] [G loss: 3.522812]\n",
      "epoch:19 step:15489 [D loss: 0.347058, acc.: 82.81%] [G loss: 2.475698]\n",
      "epoch:19 step:15490 [D loss: 0.280468, acc.: 90.62%] [G loss: 2.103502]\n",
      "epoch:19 step:15491 [D loss: 0.307230, acc.: 87.50%] [G loss: 2.286953]\n",
      "epoch:19 step:15492 [D loss: 0.365546, acc.: 85.16%] [G loss: 2.945756]\n",
      "epoch:19 step:15493 [D loss: 0.351747, acc.: 80.47%] [G loss: 2.651254]\n",
      "epoch:19 step:15494 [D loss: 0.375561, acc.: 82.81%] [G loss: 2.430296]\n",
      "epoch:19 step:15495 [D loss: 0.368735, acc.: 83.59%] [G loss: 2.265542]\n",
      "epoch:19 step:15496 [D loss: 0.383858, acc.: 84.38%] [G loss: 2.199303]\n",
      "epoch:19 step:15497 [D loss: 0.343290, acc.: 82.03%] [G loss: 2.652531]\n",
      "epoch:19 step:15498 [D loss: 0.306475, acc.: 85.16%] [G loss: 2.866110]\n",
      "epoch:19 step:15499 [D loss: 0.457076, acc.: 79.69%] [G loss: 2.771891]\n",
      "epoch:19 step:15500 [D loss: 0.411975, acc.: 83.59%] [G loss: 3.307263]\n",
      "epoch:19 step:15501 [D loss: 0.257943, acc.: 89.06%] [G loss: 2.728369]\n",
      "epoch:19 step:15502 [D loss: 0.311384, acc.: 87.50%] [G loss: 3.169523]\n",
      "epoch:19 step:15503 [D loss: 0.276309, acc.: 87.50%] [G loss: 3.108119]\n",
      "epoch:19 step:15504 [D loss: 0.305600, acc.: 85.16%] [G loss: 2.453495]\n",
      "epoch:19 step:15505 [D loss: 0.364612, acc.: 82.81%] [G loss: 3.187413]\n",
      "epoch:19 step:15506 [D loss: 0.301227, acc.: 83.59%] [G loss: 2.556740]\n",
      "epoch:19 step:15507 [D loss: 0.414657, acc.: 79.69%] [G loss: 2.345701]\n",
      "epoch:19 step:15508 [D loss: 0.376540, acc.: 82.81%] [G loss: 2.584279]\n",
      "epoch:19 step:15509 [D loss: 0.280414, acc.: 87.50%] [G loss: 2.553822]\n",
      "epoch:19 step:15510 [D loss: 0.316640, acc.: 84.38%] [G loss: 2.643505]\n",
      "epoch:19 step:15511 [D loss: 0.296792, acc.: 88.28%] [G loss: 2.576785]\n",
      "epoch:19 step:15512 [D loss: 0.344093, acc.: 85.16%] [G loss: 2.737798]\n",
      "epoch:19 step:15513 [D loss: 0.324717, acc.: 85.16%] [G loss: 2.686630]\n",
      "epoch:19 step:15514 [D loss: 0.379134, acc.: 84.38%] [G loss: 2.561146]\n",
      "epoch:19 step:15515 [D loss: 0.314057, acc.: 85.16%] [G loss: 4.749712]\n",
      "epoch:19 step:15516 [D loss: 0.282323, acc.: 85.16%] [G loss: 3.992819]\n",
      "epoch:19 step:15517 [D loss: 0.290762, acc.: 85.94%] [G loss: 4.946484]\n",
      "epoch:19 step:15518 [D loss: 0.313038, acc.: 86.72%] [G loss: 3.535157]\n",
      "epoch:19 step:15519 [D loss: 0.413173, acc.: 78.12%] [G loss: 6.122215]\n",
      "epoch:19 step:15520 [D loss: 0.271419, acc.: 86.72%] [G loss: 3.845098]\n",
      "epoch:19 step:15521 [D loss: 0.298364, acc.: 85.16%] [G loss: 4.559029]\n",
      "epoch:19 step:15522 [D loss: 0.338980, acc.: 85.16%] [G loss: 4.136073]\n",
      "epoch:19 step:15523 [D loss: 0.319081, acc.: 82.81%] [G loss: 3.212969]\n",
      "epoch:19 step:15524 [D loss: 0.185801, acc.: 96.88%] [G loss: 4.696746]\n",
      "epoch:19 step:15525 [D loss: 0.332260, acc.: 85.16%] [G loss: 3.164037]\n",
      "epoch:19 step:15526 [D loss: 0.261950, acc.: 87.50%] [G loss: 3.768741]\n",
      "epoch:19 step:15527 [D loss: 0.358427, acc.: 84.38%] [G loss: 3.675189]\n",
      "epoch:19 step:15528 [D loss: 0.318018, acc.: 88.28%] [G loss: 3.354497]\n",
      "epoch:19 step:15529 [D loss: 0.361761, acc.: 82.81%] [G loss: 3.044640]\n",
      "epoch:19 step:15530 [D loss: 0.288347, acc.: 89.84%] [G loss: 2.512931]\n",
      "epoch:19 step:15531 [D loss: 0.529908, acc.: 73.44%] [G loss: 2.577049]\n",
      "epoch:19 step:15532 [D loss: 0.323261, acc.: 86.72%] [G loss: 3.428007]\n",
      "epoch:19 step:15533 [D loss: 0.394971, acc.: 80.47%] [G loss: 2.675733]\n",
      "epoch:19 step:15534 [D loss: 0.406314, acc.: 82.81%] [G loss: 2.486362]\n",
      "epoch:19 step:15535 [D loss: 0.310864, acc.: 85.16%] [G loss: 2.474591]\n",
      "epoch:19 step:15536 [D loss: 0.383919, acc.: 82.81%] [G loss: 2.500155]\n",
      "epoch:19 step:15537 [D loss: 0.274071, acc.: 89.84%] [G loss: 3.223424]\n",
      "epoch:19 step:15538 [D loss: 0.427097, acc.: 77.34%] [G loss: 3.398719]\n",
      "epoch:19 step:15539 [D loss: 0.437358, acc.: 81.25%] [G loss: 2.510884]\n",
      "epoch:19 step:15540 [D loss: 0.445267, acc.: 78.12%] [G loss: 2.903198]\n",
      "epoch:19 step:15541 [D loss: 0.390758, acc.: 83.59%] [G loss: 2.732972]\n",
      "epoch:19 step:15542 [D loss: 0.385902, acc.: 83.59%] [G loss: 5.209782]\n",
      "epoch:19 step:15543 [D loss: 0.615336, acc.: 75.78%] [G loss: 10.465310]\n",
      "epoch:19 step:15544 [D loss: 0.744049, acc.: 70.31%] [G loss: 5.151590]\n",
      "epoch:19 step:15545 [D loss: 1.087391, acc.: 67.97%] [G loss: 8.041965]\n",
      "epoch:19 step:15546 [D loss: 1.528682, acc.: 63.28%] [G loss: 4.393890]\n",
      "epoch:19 step:15547 [D loss: 0.566355, acc.: 82.03%] [G loss: 6.016160]\n",
      "epoch:19 step:15548 [D loss: 1.028403, acc.: 73.44%] [G loss: 3.638383]\n",
      "epoch:19 step:15549 [D loss: 0.521653, acc.: 78.12%] [G loss: 3.891193]\n",
      "epoch:19 step:15550 [D loss: 0.446069, acc.: 83.59%] [G loss: 3.044480]\n",
      "epoch:19 step:15551 [D loss: 0.363274, acc.: 84.38%] [G loss: 3.579778]\n",
      "epoch:19 step:15552 [D loss: 0.487626, acc.: 78.12%] [G loss: 2.243880]\n",
      "epoch:19 step:15553 [D loss: 0.303949, acc.: 89.06%] [G loss: 3.399590]\n",
      "epoch:19 step:15554 [D loss: 0.377664, acc.: 84.38%] [G loss: 2.259976]\n",
      "epoch:19 step:15555 [D loss: 0.342166, acc.: 85.94%] [G loss: 2.728891]\n",
      "epoch:19 step:15556 [D loss: 0.340348, acc.: 84.38%] [G loss: 2.656842]\n",
      "epoch:19 step:15557 [D loss: 0.352744, acc.: 82.81%] [G loss: 2.410922]\n",
      "epoch:19 step:15558 [D loss: 0.366522, acc.: 86.72%] [G loss: 3.362891]\n",
      "epoch:19 step:15559 [D loss: 0.456139, acc.: 75.00%] [G loss: 3.053300]\n",
      "epoch:19 step:15560 [D loss: 0.391852, acc.: 81.25%] [G loss: 2.543256]\n",
      "epoch:19 step:15561 [D loss: 0.393806, acc.: 78.91%] [G loss: 2.562965]\n",
      "epoch:19 step:15562 [D loss: 0.324772, acc.: 86.72%] [G loss: 2.654348]\n",
      "epoch:19 step:15563 [D loss: 0.280189, acc.: 88.28%] [G loss: 2.555856]\n",
      "epoch:19 step:15564 [D loss: 0.280231, acc.: 88.28%] [G loss: 3.034980]\n",
      "epoch:19 step:15565 [D loss: 0.312736, acc.: 91.41%] [G loss: 2.606963]\n",
      "epoch:19 step:15566 [D loss: 0.419504, acc.: 80.47%] [G loss: 2.196966]\n",
      "epoch:19 step:15567 [D loss: 0.405606, acc.: 78.91%] [G loss: 2.607798]\n",
      "epoch:19 step:15568 [D loss: 0.427183, acc.: 79.69%] [G loss: 1.959906]\n",
      "epoch:19 step:15569 [D loss: 0.319241, acc.: 87.50%] [G loss: 2.112620]\n",
      "epoch:19 step:15570 [D loss: 0.419134, acc.: 82.03%] [G loss: 2.459466]\n",
      "epoch:19 step:15571 [D loss: 0.398746, acc.: 80.47%] [G loss: 2.421758]\n",
      "epoch:19 step:15572 [D loss: 0.390886, acc.: 82.81%] [G loss: 2.486699]\n",
      "epoch:19 step:15573 [D loss: 0.400811, acc.: 78.91%] [G loss: 1.951293]\n",
      "epoch:19 step:15574 [D loss: 0.396059, acc.: 80.47%] [G loss: 1.886633]\n",
      "epoch:19 step:15575 [D loss: 0.396156, acc.: 82.81%] [G loss: 2.209080]\n",
      "epoch:19 step:15576 [D loss: 0.280240, acc.: 89.84%] [G loss: 2.431274]\n",
      "epoch:19 step:15577 [D loss: 0.370293, acc.: 80.47%] [G loss: 2.181728]\n",
      "epoch:19 step:15578 [D loss: 0.385460, acc.: 83.59%] [G loss: 2.474080]\n",
      "epoch:19 step:15579 [D loss: 0.314237, acc.: 89.06%] [G loss: 2.192879]\n",
      "epoch:19 step:15580 [D loss: 0.324553, acc.: 88.28%] [G loss: 2.717325]\n",
      "epoch:19 step:15581 [D loss: 0.319264, acc.: 85.94%] [G loss: 2.499429]\n",
      "epoch:19 step:15582 [D loss: 0.225070, acc.: 90.62%] [G loss: 3.062480]\n",
      "epoch:19 step:15583 [D loss: 0.309943, acc.: 89.84%] [G loss: 2.665539]\n",
      "epoch:19 step:15584 [D loss: 0.349328, acc.: 83.59%] [G loss: 3.009363]\n",
      "epoch:19 step:15585 [D loss: 0.276265, acc.: 89.06%] [G loss: 3.595912]\n",
      "epoch:19 step:15586 [D loss: 0.304693, acc.: 85.94%] [G loss: 3.611557]\n",
      "epoch:19 step:15587 [D loss: 0.316052, acc.: 84.38%] [G loss: 2.468650]\n",
      "epoch:19 step:15588 [D loss: 0.263333, acc.: 88.28%] [G loss: 3.504863]\n",
      "epoch:19 step:15589 [D loss: 0.201036, acc.: 91.41%] [G loss: 5.352472]\n",
      "epoch:19 step:15590 [D loss: 0.271322, acc.: 86.72%] [G loss: 2.564045]\n",
      "epoch:19 step:15591 [D loss: 0.271429, acc.: 91.41%] [G loss: 2.625867]\n",
      "epoch:19 step:15592 [D loss: 0.308858, acc.: 87.50%] [G loss: 2.087895]\n",
      "epoch:19 step:15593 [D loss: 0.313201, acc.: 85.16%] [G loss: 3.412505]\n",
      "epoch:19 step:15594 [D loss: 0.282542, acc.: 87.50%] [G loss: 2.707346]\n",
      "epoch:19 step:15595 [D loss: 0.298131, acc.: 86.72%] [G loss: 2.604723]\n",
      "epoch:19 step:15596 [D loss: 0.397141, acc.: 83.59%] [G loss: 2.731949]\n",
      "epoch:19 step:15597 [D loss: 0.340578, acc.: 85.94%] [G loss: 3.586504]\n",
      "epoch:19 step:15598 [D loss: 0.368840, acc.: 82.81%] [G loss: 3.730620]\n",
      "epoch:19 step:15599 [D loss: 0.296335, acc.: 84.38%] [G loss: 3.461271]\n",
      "epoch:19 step:15600 [D loss: 0.415265, acc.: 85.16%] [G loss: 2.813155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15601 [D loss: 0.330356, acc.: 85.94%] [G loss: 3.309513]\n",
      "epoch:19 step:15602 [D loss: 0.253947, acc.: 89.84%] [G loss: 2.938900]\n",
      "epoch:19 step:15603 [D loss: 0.285142, acc.: 87.50%] [G loss: 2.181150]\n",
      "epoch:19 step:15604 [D loss: 0.483955, acc.: 77.34%] [G loss: 2.089262]\n",
      "epoch:19 step:15605 [D loss: 0.264123, acc.: 88.28%] [G loss: 2.533145]\n",
      "epoch:19 step:15606 [D loss: 0.394667, acc.: 79.69%] [G loss: 2.723114]\n",
      "epoch:19 step:15607 [D loss: 0.388972, acc.: 79.69%] [G loss: 1.837609]\n",
      "epoch:19 step:15608 [D loss: 0.317228, acc.: 82.81%] [G loss: 2.311349]\n",
      "epoch:19 step:15609 [D loss: 0.358507, acc.: 83.59%] [G loss: 2.523283]\n",
      "epoch:19 step:15610 [D loss: 0.351245, acc.: 88.28%] [G loss: 2.114936]\n",
      "epoch:19 step:15611 [D loss: 0.404767, acc.: 78.91%] [G loss: 2.627910]\n",
      "epoch:19 step:15612 [D loss: 0.304069, acc.: 88.28%] [G loss: 2.564806]\n",
      "epoch:19 step:15613 [D loss: 0.349741, acc.: 83.59%] [G loss: 3.337440]\n",
      "epoch:19 step:15614 [D loss: 0.365295, acc.: 82.03%] [G loss: 2.964554]\n",
      "epoch:19 step:15615 [D loss: 0.187560, acc.: 94.53%] [G loss: 2.784095]\n",
      "epoch:19 step:15616 [D loss: 0.344832, acc.: 82.03%] [G loss: 3.465966]\n",
      "epoch:19 step:15617 [D loss: 0.297035, acc.: 85.94%] [G loss: 3.195421]\n",
      "epoch:19 step:15618 [D loss: 0.299001, acc.: 88.28%] [G loss: 2.863956]\n",
      "epoch:19 step:15619 [D loss: 0.377175, acc.: 82.81%] [G loss: 3.088948]\n",
      "epoch:19 step:15620 [D loss: 0.358518, acc.: 83.59%] [G loss: 2.687861]\n",
      "epoch:20 step:15621 [D loss: 0.293875, acc.: 88.28%] [G loss: 3.011291]\n",
      "epoch:20 step:15622 [D loss: 0.318376, acc.: 86.72%] [G loss: 2.766885]\n",
      "epoch:20 step:15623 [D loss: 0.437440, acc.: 77.34%] [G loss: 4.142763]\n",
      "epoch:20 step:15624 [D loss: 0.460217, acc.: 82.03%] [G loss: 3.459072]\n",
      "epoch:20 step:15625 [D loss: 0.232024, acc.: 89.06%] [G loss: 2.698512]\n",
      "epoch:20 step:15626 [D loss: 0.398374, acc.: 79.69%] [G loss: 2.146312]\n",
      "epoch:20 step:15627 [D loss: 0.247761, acc.: 89.84%] [G loss: 3.250216]\n",
      "epoch:20 step:15628 [D loss: 0.313125, acc.: 87.50%] [G loss: 3.136119]\n",
      "epoch:20 step:15629 [D loss: 0.400303, acc.: 85.16%] [G loss: 4.160428]\n",
      "epoch:20 step:15630 [D loss: 0.396221, acc.: 79.69%] [G loss: 2.557866]\n",
      "epoch:20 step:15631 [D loss: 0.243739, acc.: 90.62%] [G loss: 3.122459]\n",
      "epoch:20 step:15632 [D loss: 0.403338, acc.: 83.59%] [G loss: 4.227729]\n",
      "epoch:20 step:15633 [D loss: 0.380860, acc.: 84.38%] [G loss: 3.642612]\n",
      "epoch:20 step:15634 [D loss: 0.289109, acc.: 88.28%] [G loss: 5.848307]\n",
      "epoch:20 step:15635 [D loss: 0.270600, acc.: 85.94%] [G loss: 4.301450]\n",
      "epoch:20 step:15636 [D loss: 0.215530, acc.: 89.84%] [G loss: 3.589987]\n",
      "epoch:20 step:15637 [D loss: 0.273125, acc.: 88.28%] [G loss: 2.165153]\n",
      "epoch:20 step:15638 [D loss: 0.397137, acc.: 82.03%] [G loss: 2.922897]\n",
      "epoch:20 step:15639 [D loss: 0.308902, acc.: 87.50%] [G loss: 3.581618]\n",
      "epoch:20 step:15640 [D loss: 0.225973, acc.: 91.41%] [G loss: 4.236224]\n",
      "epoch:20 step:15641 [D loss: 0.445695, acc.: 79.69%] [G loss: 3.340717]\n",
      "epoch:20 step:15642 [D loss: 0.378885, acc.: 81.25%] [G loss: 2.381090]\n",
      "epoch:20 step:15643 [D loss: 0.270085, acc.: 89.84%] [G loss: 3.347813]\n",
      "epoch:20 step:15644 [D loss: 0.272139, acc.: 88.28%] [G loss: 3.765656]\n",
      "epoch:20 step:15645 [D loss: 0.370558, acc.: 84.38%] [G loss: 3.603176]\n",
      "epoch:20 step:15646 [D loss: 0.206068, acc.: 91.41%] [G loss: 5.611863]\n",
      "epoch:20 step:15647 [D loss: 0.293800, acc.: 88.28%] [G loss: 2.494104]\n",
      "epoch:20 step:15648 [D loss: 0.385231, acc.: 79.69%] [G loss: 2.958677]\n",
      "epoch:20 step:15649 [D loss: 0.349275, acc.: 85.16%] [G loss: 2.754942]\n",
      "epoch:20 step:15650 [D loss: 0.251350, acc.: 88.28%] [G loss: 3.305684]\n",
      "epoch:20 step:15651 [D loss: 0.422211, acc.: 83.59%] [G loss: 3.166474]\n",
      "epoch:20 step:15652 [D loss: 0.298545, acc.: 88.28%] [G loss: 3.358534]\n",
      "epoch:20 step:15653 [D loss: 0.260202, acc.: 89.84%] [G loss: 3.309190]\n",
      "epoch:20 step:15654 [D loss: 0.267900, acc.: 88.28%] [G loss: 3.433335]\n",
      "epoch:20 step:15655 [D loss: 0.274469, acc.: 85.16%] [G loss: 3.750936]\n",
      "epoch:20 step:15656 [D loss: 0.236199, acc.: 92.19%] [G loss: 2.177107]\n",
      "epoch:20 step:15657 [D loss: 0.343127, acc.: 84.38%] [G loss: 3.245463]\n",
      "epoch:20 step:15658 [D loss: 0.353578, acc.: 83.59%] [G loss: 2.821185]\n",
      "epoch:20 step:15659 [D loss: 0.319537, acc.: 87.50%] [G loss: 2.694076]\n",
      "epoch:20 step:15660 [D loss: 0.431973, acc.: 78.91%] [G loss: 2.172477]\n",
      "epoch:20 step:15661 [D loss: 0.424378, acc.: 80.47%] [G loss: 2.939018]\n",
      "epoch:20 step:15662 [D loss: 0.238103, acc.: 89.06%] [G loss: 2.791244]\n",
      "epoch:20 step:15663 [D loss: 0.369747, acc.: 82.03%] [G loss: 2.375085]\n",
      "epoch:20 step:15664 [D loss: 0.327235, acc.: 89.06%] [G loss: 3.022959]\n",
      "epoch:20 step:15665 [D loss: 0.275929, acc.: 89.84%] [G loss: 2.037443]\n",
      "epoch:20 step:15666 [D loss: 0.389497, acc.: 84.38%] [G loss: 2.854186]\n",
      "epoch:20 step:15667 [D loss: 0.408811, acc.: 81.25%] [G loss: 2.970726]\n",
      "epoch:20 step:15668 [D loss: 0.305817, acc.: 86.72%] [G loss: 3.722851]\n",
      "epoch:20 step:15669 [D loss: 0.338695, acc.: 84.38%] [G loss: 2.527036]\n",
      "epoch:20 step:15670 [D loss: 0.275857, acc.: 89.84%] [G loss: 2.574945]\n",
      "epoch:20 step:15671 [D loss: 0.426615, acc.: 76.56%] [G loss: 2.123330]\n",
      "epoch:20 step:15672 [D loss: 0.357460, acc.: 83.59%] [G loss: 2.775532]\n",
      "epoch:20 step:15673 [D loss: 0.357977, acc.: 85.16%] [G loss: 2.855915]\n",
      "epoch:20 step:15674 [D loss: 0.378708, acc.: 78.91%] [G loss: 2.795904]\n",
      "epoch:20 step:15675 [D loss: 0.316861, acc.: 86.72%] [G loss: 2.837435]\n",
      "epoch:20 step:15676 [D loss: 0.268487, acc.: 89.84%] [G loss: 2.451502]\n",
      "epoch:20 step:15677 [D loss: 0.313750, acc.: 85.94%] [G loss: 2.242616]\n",
      "epoch:20 step:15678 [D loss: 0.265838, acc.: 89.84%] [G loss: 3.525610]\n",
      "epoch:20 step:15679 [D loss: 0.259100, acc.: 86.72%] [G loss: 3.907240]\n",
      "epoch:20 step:15680 [D loss: 0.288486, acc.: 86.72%] [G loss: 3.773522]\n",
      "epoch:20 step:15681 [D loss: 0.346052, acc.: 82.81%] [G loss: 3.737151]\n",
      "epoch:20 step:15682 [D loss: 0.284246, acc.: 86.72%] [G loss: 4.839609]\n",
      "epoch:20 step:15683 [D loss: 0.216996, acc.: 92.97%] [G loss: 4.103912]\n",
      "epoch:20 step:15684 [D loss: 0.360872, acc.: 80.47%] [G loss: 4.859508]\n",
      "epoch:20 step:15685 [D loss: 0.499453, acc.: 76.56%] [G loss: 6.799803]\n",
      "epoch:20 step:15686 [D loss: 0.921519, acc.: 64.06%] [G loss: 6.083927]\n",
      "epoch:20 step:15687 [D loss: 1.026309, acc.: 66.41%] [G loss: 5.937226]\n",
      "epoch:20 step:15688 [D loss: 0.897216, acc.: 72.66%] [G loss: 4.905746]\n",
      "epoch:20 step:15689 [D loss: 0.728751, acc.: 71.09%] [G loss: 4.565243]\n",
      "epoch:20 step:15690 [D loss: 0.276919, acc.: 87.50%] [G loss: 4.619641]\n",
      "epoch:20 step:15691 [D loss: 0.578891, acc.: 74.22%] [G loss: 2.710778]\n",
      "epoch:20 step:15692 [D loss: 0.363476, acc.: 85.16%] [G loss: 2.620516]\n",
      "epoch:20 step:15693 [D loss: 0.366414, acc.: 84.38%] [G loss: 2.724768]\n",
      "epoch:20 step:15694 [D loss: 0.373996, acc.: 81.25%] [G loss: 3.093989]\n",
      "epoch:20 step:15695 [D loss: 0.331010, acc.: 83.59%] [G loss: 2.819179]\n",
      "epoch:20 step:15696 [D loss: 0.381039, acc.: 82.03%] [G loss: 2.229812]\n",
      "epoch:20 step:15697 [D loss: 0.395510, acc.: 81.25%] [G loss: 3.173694]\n",
      "epoch:20 step:15698 [D loss: 0.320630, acc.: 84.38%] [G loss: 2.359456]\n",
      "epoch:20 step:15699 [D loss: 0.313315, acc.: 82.81%] [G loss: 2.771273]\n",
      "epoch:20 step:15700 [D loss: 0.310690, acc.: 86.72%] [G loss: 2.120315]\n",
      "epoch:20 step:15701 [D loss: 0.321428, acc.: 86.72%] [G loss: 2.794378]\n",
      "epoch:20 step:15702 [D loss: 0.327273, acc.: 85.16%] [G loss: 2.584351]\n",
      "epoch:20 step:15703 [D loss: 0.309119, acc.: 85.94%] [G loss: 4.404098]\n",
      "epoch:20 step:15704 [D loss: 0.381178, acc.: 87.50%] [G loss: 4.157880]\n",
      "epoch:20 step:15705 [D loss: 0.290518, acc.: 88.28%] [G loss: 3.465123]\n",
      "epoch:20 step:15706 [D loss: 0.439545, acc.: 82.03%] [G loss: 4.177315]\n",
      "epoch:20 step:15707 [D loss: 0.399840, acc.: 82.03%] [G loss: 4.512318]\n",
      "epoch:20 step:15708 [D loss: 0.227282, acc.: 89.84%] [G loss: 4.386170]\n",
      "epoch:20 step:15709 [D loss: 0.307771, acc.: 87.50%] [G loss: 3.252043]\n",
      "epoch:20 step:15710 [D loss: 0.404036, acc.: 79.69%] [G loss: 3.569835]\n",
      "epoch:20 step:15711 [D loss: 0.252102, acc.: 91.41%] [G loss: 3.653542]\n",
      "epoch:20 step:15712 [D loss: 0.307994, acc.: 85.94%] [G loss: 3.483000]\n",
      "epoch:20 step:15713 [D loss: 0.284965, acc.: 89.84%] [G loss: 2.504555]\n",
      "epoch:20 step:15714 [D loss: 0.345553, acc.: 82.81%] [G loss: 2.489891]\n",
      "epoch:20 step:15715 [D loss: 0.323731, acc.: 85.16%] [G loss: 2.909277]\n",
      "epoch:20 step:15716 [D loss: 0.453149, acc.: 75.00%] [G loss: 2.765907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15717 [D loss: 0.406309, acc.: 83.59%] [G loss: 2.314090]\n",
      "epoch:20 step:15718 [D loss: 0.271389, acc.: 89.84%] [G loss: 2.609438]\n",
      "epoch:20 step:15719 [D loss: 0.388762, acc.: 81.25%] [G loss: 2.226950]\n",
      "epoch:20 step:15720 [D loss: 0.525227, acc.: 73.44%] [G loss: 2.315541]\n",
      "epoch:20 step:15721 [D loss: 0.302226, acc.: 86.72%] [G loss: 2.752133]\n",
      "epoch:20 step:15722 [D loss: 0.353532, acc.: 82.03%] [G loss: 4.028270]\n",
      "epoch:20 step:15723 [D loss: 0.353573, acc.: 84.38%] [G loss: 3.140764]\n",
      "epoch:20 step:15724 [D loss: 0.421789, acc.: 76.56%] [G loss: 2.943956]\n",
      "epoch:20 step:15725 [D loss: 0.383267, acc.: 81.25%] [G loss: 3.135302]\n",
      "epoch:20 step:15726 [D loss: 0.256429, acc.: 86.72%] [G loss: 5.048573]\n",
      "epoch:20 step:15727 [D loss: 0.414544, acc.: 81.25%] [G loss: 3.957185]\n",
      "epoch:20 step:15728 [D loss: 0.200223, acc.: 92.19%] [G loss: 4.963294]\n",
      "epoch:20 step:15729 [D loss: 0.284129, acc.: 86.72%] [G loss: 3.566633]\n",
      "epoch:20 step:15730 [D loss: 0.268885, acc.: 89.84%] [G loss: 3.526142]\n",
      "epoch:20 step:15731 [D loss: 0.354222, acc.: 85.94%] [G loss: 2.557395]\n",
      "epoch:20 step:15732 [D loss: 0.288993, acc.: 85.94%] [G loss: 3.181081]\n",
      "epoch:20 step:15733 [D loss: 0.298299, acc.: 89.84%] [G loss: 2.826867]\n",
      "epoch:20 step:15734 [D loss: 0.318370, acc.: 85.94%] [G loss: 2.435834]\n",
      "epoch:20 step:15735 [D loss: 0.368896, acc.: 81.25%] [G loss: 3.641762]\n",
      "epoch:20 step:15736 [D loss: 0.318202, acc.: 82.81%] [G loss: 5.146945]\n",
      "epoch:20 step:15737 [D loss: 0.369424, acc.: 84.38%] [G loss: 5.790557]\n",
      "epoch:20 step:15738 [D loss: 0.369281, acc.: 83.59%] [G loss: 3.372238]\n",
      "epoch:20 step:15739 [D loss: 0.315979, acc.: 86.72%] [G loss: 3.528176]\n",
      "epoch:20 step:15740 [D loss: 0.290710, acc.: 85.94%] [G loss: 2.819381]\n",
      "epoch:20 step:15741 [D loss: 0.314268, acc.: 86.72%] [G loss: 2.756804]\n",
      "epoch:20 step:15742 [D loss: 0.234174, acc.: 89.84%] [G loss: 4.264694]\n",
      "epoch:20 step:15743 [D loss: 0.244045, acc.: 88.28%] [G loss: 4.116990]\n",
      "epoch:20 step:15744 [D loss: 0.303791, acc.: 87.50%] [G loss: 3.712139]\n",
      "epoch:20 step:15745 [D loss: 0.312416, acc.: 87.50%] [G loss: 2.612553]\n",
      "epoch:20 step:15746 [D loss: 0.267202, acc.: 89.84%] [G loss: 3.763924]\n",
      "epoch:20 step:15747 [D loss: 0.235838, acc.: 90.62%] [G loss: 3.194455]\n",
      "epoch:20 step:15748 [D loss: 0.358415, acc.: 84.38%] [G loss: 2.538911]\n",
      "epoch:20 step:15749 [D loss: 0.332414, acc.: 85.16%] [G loss: 2.906205]\n",
      "epoch:20 step:15750 [D loss: 0.337173, acc.: 85.94%] [G loss: 2.850487]\n",
      "epoch:20 step:15751 [D loss: 0.339999, acc.: 85.16%] [G loss: 3.251855]\n",
      "epoch:20 step:15752 [D loss: 0.228371, acc.: 92.19%] [G loss: 3.118364]\n",
      "epoch:20 step:15753 [D loss: 0.295769, acc.: 88.28%] [G loss: 4.579708]\n",
      "epoch:20 step:15754 [D loss: 0.164675, acc.: 92.97%] [G loss: 7.362733]\n",
      "epoch:20 step:15755 [D loss: 0.184035, acc.: 93.75%] [G loss: 7.578707]\n",
      "epoch:20 step:15756 [D loss: 0.250310, acc.: 89.06%] [G loss: 5.331379]\n",
      "epoch:20 step:15757 [D loss: 0.210520, acc.: 89.84%] [G loss: 6.616565]\n",
      "epoch:20 step:15758 [D loss: 0.198775, acc.: 90.62%] [G loss: 4.986556]\n",
      "epoch:20 step:15759 [D loss: 0.281024, acc.: 87.50%] [G loss: 2.935156]\n",
      "epoch:20 step:15760 [D loss: 0.318560, acc.: 82.81%] [G loss: 3.112564]\n",
      "epoch:20 step:15761 [D loss: 0.338252, acc.: 85.94%] [G loss: 3.075815]\n",
      "epoch:20 step:15762 [D loss: 0.276498, acc.: 88.28%] [G loss: 2.805321]\n",
      "epoch:20 step:15763 [D loss: 0.269324, acc.: 86.72%] [G loss: 3.834920]\n",
      "epoch:20 step:15764 [D loss: 0.403968, acc.: 82.03%] [G loss: 3.522253]\n",
      "epoch:20 step:15765 [D loss: 0.250387, acc.: 87.50%] [G loss: 4.515479]\n",
      "epoch:20 step:15766 [D loss: 0.434512, acc.: 79.69%] [G loss: 3.215141]\n",
      "epoch:20 step:15767 [D loss: 0.352385, acc.: 85.16%] [G loss: 3.671337]\n",
      "epoch:20 step:15768 [D loss: 0.321596, acc.: 86.72%] [G loss: 4.371044]\n",
      "epoch:20 step:15769 [D loss: 0.335674, acc.: 85.94%] [G loss: 4.416736]\n",
      "epoch:20 step:15770 [D loss: 0.322726, acc.: 85.16%] [G loss: 3.412562]\n",
      "epoch:20 step:15771 [D loss: 0.323446, acc.: 84.38%] [G loss: 3.925797]\n",
      "epoch:20 step:15772 [D loss: 0.257938, acc.: 89.06%] [G loss: 3.120664]\n",
      "epoch:20 step:15773 [D loss: 0.280899, acc.: 91.41%] [G loss: 2.974704]\n",
      "epoch:20 step:15774 [D loss: 0.356052, acc.: 82.81%] [G loss: 2.496604]\n",
      "epoch:20 step:15775 [D loss: 0.324185, acc.: 83.59%] [G loss: 2.567958]\n",
      "epoch:20 step:15776 [D loss: 0.358467, acc.: 84.38%] [G loss: 4.191826]\n",
      "epoch:20 step:15777 [D loss: 0.597914, acc.: 78.12%] [G loss: 7.626074]\n",
      "epoch:20 step:15778 [D loss: 1.049756, acc.: 64.84%] [G loss: 5.641041]\n",
      "epoch:20 step:15779 [D loss: 0.713094, acc.: 70.31%] [G loss: 6.002463]\n",
      "epoch:20 step:15780 [D loss: 0.946596, acc.: 65.62%] [G loss: 5.659530]\n",
      "epoch:20 step:15781 [D loss: 1.344308, acc.: 57.03%] [G loss: 1.998813]\n",
      "epoch:20 step:15782 [D loss: 0.400847, acc.: 80.47%] [G loss: 3.823494]\n",
      "epoch:20 step:15783 [D loss: 0.372369, acc.: 82.03%] [G loss: 5.535257]\n",
      "epoch:20 step:15784 [D loss: 0.348922, acc.: 85.94%] [G loss: 3.954913]\n",
      "epoch:20 step:15785 [D loss: 0.380882, acc.: 80.47%] [G loss: 4.037664]\n",
      "epoch:20 step:15786 [D loss: 0.283522, acc.: 89.84%] [G loss: 3.019001]\n",
      "epoch:20 step:15787 [D loss: 0.466690, acc.: 78.91%] [G loss: 2.779957]\n",
      "epoch:20 step:15788 [D loss: 0.361205, acc.: 82.81%] [G loss: 2.872293]\n",
      "epoch:20 step:15789 [D loss: 0.313258, acc.: 84.38%] [G loss: 2.682925]\n",
      "epoch:20 step:15790 [D loss: 0.299895, acc.: 88.28%] [G loss: 2.912566]\n",
      "epoch:20 step:15791 [D loss: 0.413278, acc.: 77.34%] [G loss: 2.608690]\n",
      "epoch:20 step:15792 [D loss: 0.379422, acc.: 79.69%] [G loss: 3.253632]\n",
      "epoch:20 step:15793 [D loss: 0.434848, acc.: 80.47%] [G loss: 4.128587]\n",
      "epoch:20 step:15794 [D loss: 0.496046, acc.: 77.34%] [G loss: 2.857634]\n",
      "epoch:20 step:15795 [D loss: 0.257297, acc.: 86.72%] [G loss: 2.703082]\n",
      "epoch:20 step:15796 [D loss: 0.362740, acc.: 85.94%] [G loss: 2.376198]\n",
      "epoch:20 step:15797 [D loss: 0.296556, acc.: 88.28%] [G loss: 2.989034]\n",
      "epoch:20 step:15798 [D loss: 0.391852, acc.: 81.25%] [G loss: 3.558706]\n",
      "epoch:20 step:15799 [D loss: 0.353158, acc.: 86.72%] [G loss: 3.032962]\n",
      "epoch:20 step:15800 [D loss: 0.353249, acc.: 86.72%] [G loss: 2.794595]\n",
      "epoch:20 step:15801 [D loss: 0.280582, acc.: 87.50%] [G loss: 2.731029]\n",
      "epoch:20 step:15802 [D loss: 0.256871, acc.: 89.84%] [G loss: 2.753991]\n",
      "epoch:20 step:15803 [D loss: 0.308509, acc.: 85.94%] [G loss: 2.443147]\n",
      "epoch:20 step:15804 [D loss: 0.369438, acc.: 82.03%] [G loss: 2.580171]\n",
      "epoch:20 step:15805 [D loss: 0.280863, acc.: 87.50%] [G loss: 3.396331]\n",
      "epoch:20 step:15806 [D loss: 0.276987, acc.: 85.16%] [G loss: 3.343283]\n",
      "epoch:20 step:15807 [D loss: 0.379992, acc.: 83.59%] [G loss: 3.105373]\n",
      "epoch:20 step:15808 [D loss: 0.278519, acc.: 86.72%] [G loss: 2.863470]\n",
      "epoch:20 step:15809 [D loss: 0.337032, acc.: 86.72%] [G loss: 2.495302]\n",
      "epoch:20 step:15810 [D loss: 0.377695, acc.: 83.59%] [G loss: 2.837397]\n",
      "epoch:20 step:15811 [D loss: 0.474375, acc.: 81.25%] [G loss: 2.499616]\n",
      "epoch:20 step:15812 [D loss: 0.265229, acc.: 89.06%] [G loss: 2.446230]\n",
      "epoch:20 step:15813 [D loss: 0.391980, acc.: 80.47%] [G loss: 2.361317]\n",
      "epoch:20 step:15814 [D loss: 0.344428, acc.: 82.81%] [G loss: 2.734775]\n",
      "epoch:20 step:15815 [D loss: 0.340459, acc.: 88.28%] [G loss: 2.867937]\n",
      "epoch:20 step:15816 [D loss: 0.294247, acc.: 89.84%] [G loss: 2.549927]\n",
      "epoch:20 step:15817 [D loss: 0.327470, acc.: 82.81%] [G loss: 3.434120]\n",
      "epoch:20 step:15818 [D loss: 0.410063, acc.: 78.91%] [G loss: 2.894875]\n",
      "epoch:20 step:15819 [D loss: 0.265777, acc.: 88.28%] [G loss: 3.914521]\n",
      "epoch:20 step:15820 [D loss: 0.324519, acc.: 83.59%] [G loss: 3.726799]\n",
      "epoch:20 step:15821 [D loss: 0.422077, acc.: 82.03%] [G loss: 3.906548]\n",
      "epoch:20 step:15822 [D loss: 0.393925, acc.: 82.03%] [G loss: 4.204748]\n",
      "epoch:20 step:15823 [D loss: 0.299440, acc.: 86.72%] [G loss: 3.360889]\n",
      "epoch:20 step:15824 [D loss: 0.248578, acc.: 88.28%] [G loss: 3.840824]\n",
      "epoch:20 step:15825 [D loss: 0.306022, acc.: 89.06%] [G loss: 2.645189]\n",
      "epoch:20 step:15826 [D loss: 0.230442, acc.: 89.84%] [G loss: 3.247395]\n",
      "epoch:20 step:15827 [D loss: 0.291062, acc.: 85.94%] [G loss: 2.862488]\n",
      "epoch:20 step:15828 [D loss: 0.458995, acc.: 77.34%] [G loss: 3.114424]\n",
      "epoch:20 step:15829 [D loss: 0.380148, acc.: 82.03%] [G loss: 3.055127]\n",
      "epoch:20 step:15830 [D loss: 0.224352, acc.: 92.19%] [G loss: 3.517815]\n",
      "epoch:20 step:15831 [D loss: 0.396834, acc.: 80.47%] [G loss: 4.357853]\n",
      "epoch:20 step:15832 [D loss: 0.308102, acc.: 86.72%] [G loss: 4.425532]\n",
      "epoch:20 step:15833 [D loss: 0.266923, acc.: 87.50%] [G loss: 3.808347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15834 [D loss: 0.398162, acc.: 84.38%] [G loss: 3.100339]\n",
      "epoch:20 step:15835 [D loss: 0.293585, acc.: 85.16%] [G loss: 2.787866]\n",
      "epoch:20 step:15836 [D loss: 0.307797, acc.: 88.28%] [G loss: 2.982939]\n",
      "epoch:20 step:15837 [D loss: 0.345311, acc.: 85.94%] [G loss: 3.389452]\n",
      "epoch:20 step:15838 [D loss: 0.219042, acc.: 89.84%] [G loss: 5.037111]\n",
      "epoch:20 step:15839 [D loss: 0.214205, acc.: 93.75%] [G loss: 4.731027]\n",
      "epoch:20 step:15840 [D loss: 0.294878, acc.: 87.50%] [G loss: 3.772814]\n",
      "epoch:20 step:15841 [D loss: 0.263905, acc.: 87.50%] [G loss: 3.486540]\n",
      "epoch:20 step:15842 [D loss: 0.327946, acc.: 81.25%] [G loss: 2.569198]\n",
      "epoch:20 step:15843 [D loss: 0.274979, acc.: 88.28%] [G loss: 3.188053]\n",
      "epoch:20 step:15844 [D loss: 0.290094, acc.: 87.50%] [G loss: 2.637249]\n",
      "epoch:20 step:15845 [D loss: 0.418912, acc.: 85.16%] [G loss: 2.446714]\n",
      "epoch:20 step:15846 [D loss: 0.323005, acc.: 85.16%] [G loss: 2.697213]\n",
      "epoch:20 step:15847 [D loss: 0.285947, acc.: 88.28%] [G loss: 2.979314]\n",
      "epoch:20 step:15848 [D loss: 0.355575, acc.: 85.16%] [G loss: 2.337979]\n",
      "epoch:20 step:15849 [D loss: 0.253971, acc.: 89.84%] [G loss: 2.052650]\n",
      "epoch:20 step:15850 [D loss: 0.279592, acc.: 87.50%] [G loss: 2.864250]\n",
      "epoch:20 step:15851 [D loss: 0.236695, acc.: 91.41%] [G loss: 3.862970]\n",
      "epoch:20 step:15852 [D loss: 0.365851, acc.: 85.16%] [G loss: 4.662656]\n",
      "epoch:20 step:15853 [D loss: 0.355732, acc.: 85.16%] [G loss: 3.058428]\n",
      "epoch:20 step:15854 [D loss: 0.264490, acc.: 90.62%] [G loss: 3.063481]\n",
      "epoch:20 step:15855 [D loss: 0.395869, acc.: 80.47%] [G loss: 2.270913]\n",
      "epoch:20 step:15856 [D loss: 0.294539, acc.: 89.06%] [G loss: 3.053149]\n",
      "epoch:20 step:15857 [D loss: 0.325403, acc.: 85.94%] [G loss: 3.244933]\n",
      "epoch:20 step:15858 [D loss: 0.275275, acc.: 87.50%] [G loss: 2.440321]\n",
      "epoch:20 step:15859 [D loss: 0.369362, acc.: 85.94%] [G loss: 2.625706]\n",
      "epoch:20 step:15860 [D loss: 0.248067, acc.: 89.06%] [G loss: 3.397247]\n",
      "epoch:20 step:15861 [D loss: 0.212861, acc.: 90.62%] [G loss: 4.259318]\n",
      "epoch:20 step:15862 [D loss: 0.362982, acc.: 82.03%] [G loss: 3.108631]\n",
      "epoch:20 step:15863 [D loss: 0.350419, acc.: 85.16%] [G loss: 3.062028]\n",
      "epoch:20 step:15864 [D loss: 0.284826, acc.: 86.72%] [G loss: 2.687173]\n",
      "epoch:20 step:15865 [D loss: 0.379422, acc.: 82.81%] [G loss: 2.882212]\n",
      "epoch:20 step:15866 [D loss: 0.409398, acc.: 79.69%] [G loss: 2.890458]\n",
      "epoch:20 step:15867 [D loss: 0.313083, acc.: 85.16%] [G loss: 2.936048]\n",
      "epoch:20 step:15868 [D loss: 0.348342, acc.: 85.94%] [G loss: 2.493625]\n",
      "epoch:20 step:15869 [D loss: 0.400899, acc.: 81.25%] [G loss: 2.579518]\n",
      "epoch:20 step:15870 [D loss: 0.353963, acc.: 85.16%] [G loss: 2.057025]\n",
      "epoch:20 step:15871 [D loss: 0.249982, acc.: 91.41%] [G loss: 2.913205]\n",
      "epoch:20 step:15872 [D loss: 0.256220, acc.: 87.50%] [G loss: 3.360974]\n",
      "epoch:20 step:15873 [D loss: 0.315670, acc.: 83.59%] [G loss: 3.447306]\n",
      "epoch:20 step:15874 [D loss: 0.246277, acc.: 90.62%] [G loss: 3.824896]\n",
      "epoch:20 step:15875 [D loss: 0.290320, acc.: 88.28%] [G loss: 3.531879]\n",
      "epoch:20 step:15876 [D loss: 0.300618, acc.: 87.50%] [G loss: 3.077703]\n",
      "epoch:20 step:15877 [D loss: 0.301098, acc.: 85.16%] [G loss: 4.740362]\n",
      "epoch:20 step:15878 [D loss: 0.499936, acc.: 78.91%] [G loss: 4.488655]\n",
      "epoch:20 step:15879 [D loss: 0.315336, acc.: 86.72%] [G loss: 4.923450]\n",
      "epoch:20 step:15880 [D loss: 0.449450, acc.: 79.69%] [G loss: 3.277859]\n",
      "epoch:20 step:15881 [D loss: 0.358694, acc.: 81.25%] [G loss: 3.110708]\n",
      "epoch:20 step:15882 [D loss: 0.299934, acc.: 89.06%] [G loss: 3.117933]\n",
      "epoch:20 step:15883 [D loss: 0.241556, acc.: 89.06%] [G loss: 3.863684]\n",
      "epoch:20 step:15884 [D loss: 0.386673, acc.: 85.16%] [G loss: 2.981158]\n",
      "epoch:20 step:15885 [D loss: 0.370396, acc.: 85.16%] [G loss: 3.216074]\n",
      "epoch:20 step:15886 [D loss: 0.269904, acc.: 89.84%] [G loss: 3.101713]\n",
      "epoch:20 step:15887 [D loss: 0.371444, acc.: 83.59%] [G loss: 3.380559]\n",
      "epoch:20 step:15888 [D loss: 0.401884, acc.: 86.72%] [G loss: 3.385813]\n",
      "epoch:20 step:15889 [D loss: 0.486758, acc.: 78.12%] [G loss: 2.795141]\n",
      "epoch:20 step:15890 [D loss: 0.312958, acc.: 86.72%] [G loss: 2.571320]\n",
      "epoch:20 step:15891 [D loss: 0.263173, acc.: 91.41%] [G loss: 4.372734]\n",
      "epoch:20 step:15892 [D loss: 0.294997, acc.: 85.16%] [G loss: 3.975040]\n",
      "epoch:20 step:15893 [D loss: 0.219401, acc.: 92.97%] [G loss: 4.433487]\n",
      "epoch:20 step:15894 [D loss: 0.335668, acc.: 85.94%] [G loss: 3.722612]\n",
      "epoch:20 step:15895 [D loss: 0.389331, acc.: 78.91%] [G loss: 4.324212]\n",
      "epoch:20 step:15896 [D loss: 0.420545, acc.: 80.47%] [G loss: 2.375549]\n",
      "epoch:20 step:15897 [D loss: 0.307090, acc.: 84.38%] [G loss: 3.779089]\n",
      "epoch:20 step:15898 [D loss: 0.329908, acc.: 85.94%] [G loss: 3.300335]\n",
      "epoch:20 step:15899 [D loss: 0.304746, acc.: 87.50%] [G loss: 2.412127]\n",
      "epoch:20 step:15900 [D loss: 0.246776, acc.: 90.62%] [G loss: 2.488238]\n",
      "epoch:20 step:15901 [D loss: 0.284262, acc.: 89.06%] [G loss: 2.312058]\n",
      "epoch:20 step:15902 [D loss: 0.373251, acc.: 82.03%] [G loss: 2.881166]\n",
      "epoch:20 step:15903 [D loss: 0.355296, acc.: 85.16%] [G loss: 3.142991]\n",
      "epoch:20 step:15904 [D loss: 0.339445, acc.: 82.03%] [G loss: 2.336977]\n",
      "epoch:20 step:15905 [D loss: 0.302353, acc.: 88.28%] [G loss: 2.448988]\n",
      "epoch:20 step:15906 [D loss: 0.276124, acc.: 87.50%] [G loss: 2.431332]\n",
      "epoch:20 step:15907 [D loss: 0.204483, acc.: 90.62%] [G loss: 5.585725]\n",
      "epoch:20 step:15908 [D loss: 0.349356, acc.: 84.38%] [G loss: 2.690220]\n",
      "epoch:20 step:15909 [D loss: 0.376347, acc.: 81.25%] [G loss: 3.234552]\n",
      "epoch:20 step:15910 [D loss: 0.328961, acc.: 82.03%] [G loss: 3.514338]\n",
      "epoch:20 step:15911 [D loss: 0.357692, acc.: 82.03%] [G loss: 3.266381]\n",
      "epoch:20 step:15912 [D loss: 0.317773, acc.: 85.94%] [G loss: 3.038990]\n",
      "epoch:20 step:15913 [D loss: 0.381082, acc.: 82.81%] [G loss: 2.751283]\n",
      "epoch:20 step:15914 [D loss: 0.363609, acc.: 82.81%] [G loss: 4.023348]\n",
      "epoch:20 step:15915 [D loss: 0.449738, acc.: 84.38%] [G loss: 4.722087]\n",
      "epoch:20 step:15916 [D loss: 0.377195, acc.: 84.38%] [G loss: 3.406255]\n",
      "epoch:20 step:15917 [D loss: 0.387429, acc.: 83.59%] [G loss: 2.833387]\n",
      "epoch:20 step:15918 [D loss: 0.472449, acc.: 82.03%] [G loss: 6.359774]\n",
      "epoch:20 step:15919 [D loss: 0.750609, acc.: 75.00%] [G loss: 7.591367]\n",
      "epoch:20 step:15920 [D loss: 1.113984, acc.: 63.28%] [G loss: 7.001209]\n",
      "epoch:20 step:15921 [D loss: 2.410598, acc.: 45.31%] [G loss: 7.630424]\n",
      "epoch:20 step:15922 [D loss: 1.259171, acc.: 68.75%] [G loss: 5.897281]\n",
      "epoch:20 step:15923 [D loss: 0.571912, acc.: 78.91%] [G loss: 5.148116]\n",
      "epoch:20 step:15924 [D loss: 0.753645, acc.: 68.75%] [G loss: 2.906947]\n",
      "epoch:20 step:15925 [D loss: 0.292389, acc.: 86.72%] [G loss: 3.606505]\n",
      "epoch:20 step:15926 [D loss: 0.342432, acc.: 83.59%] [G loss: 3.237404]\n",
      "epoch:20 step:15927 [D loss: 0.354890, acc.: 82.03%] [G loss: 3.200394]\n",
      "epoch:20 step:15928 [D loss: 0.394285, acc.: 85.16%] [G loss: 3.928705]\n",
      "epoch:20 step:15929 [D loss: 0.379047, acc.: 82.81%] [G loss: 2.813733]\n",
      "epoch:20 step:15930 [D loss: 0.452616, acc.: 81.25%] [G loss: 3.590556]\n",
      "epoch:20 step:15931 [D loss: 0.403837, acc.: 82.03%] [G loss: 3.255493]\n",
      "epoch:20 step:15932 [D loss: 0.304222, acc.: 87.50%] [G loss: 2.923213]\n",
      "epoch:20 step:15933 [D loss: 0.391332, acc.: 79.69%] [G loss: 2.098231]\n",
      "epoch:20 step:15934 [D loss: 0.332905, acc.: 82.03%] [G loss: 3.084052]\n",
      "epoch:20 step:15935 [D loss: 0.521846, acc.: 70.31%] [G loss: 3.878911]\n",
      "epoch:20 step:15936 [D loss: 0.241459, acc.: 89.06%] [G loss: 4.011810]\n",
      "epoch:20 step:15937 [D loss: 0.338107, acc.: 85.94%] [G loss: 3.465969]\n",
      "epoch:20 step:15938 [D loss: 0.345719, acc.: 86.72%] [G loss: 3.871216]\n",
      "epoch:20 step:15939 [D loss: 0.247189, acc.: 89.84%] [G loss: 6.122024]\n",
      "epoch:20 step:15940 [D loss: 0.326348, acc.: 86.72%] [G loss: 3.420434]\n",
      "epoch:20 step:15941 [D loss: 0.268044, acc.: 85.16%] [G loss: 5.425778]\n",
      "epoch:20 step:15942 [D loss: 0.332935, acc.: 85.16%] [G loss: 2.768034]\n",
      "epoch:20 step:15943 [D loss: 0.241965, acc.: 89.84%] [G loss: 3.423179]\n",
      "epoch:20 step:15944 [D loss: 0.386018, acc.: 84.38%] [G loss: 2.504937]\n",
      "epoch:20 step:15945 [D loss: 0.401725, acc.: 80.47%] [G loss: 2.784788]\n",
      "epoch:20 step:15946 [D loss: 0.302870, acc.: 87.50%] [G loss: 2.715911]\n",
      "epoch:20 step:15947 [D loss: 0.260516, acc.: 92.19%] [G loss: 2.091561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15948 [D loss: 0.260748, acc.: 90.62%] [G loss: 2.719940]\n",
      "epoch:20 step:15949 [D loss: 0.277240, acc.: 91.41%] [G loss: 2.434285]\n",
      "epoch:20 step:15950 [D loss: 0.372379, acc.: 85.16%] [G loss: 1.948086]\n",
      "epoch:20 step:15951 [D loss: 0.406449, acc.: 81.25%] [G loss: 2.658417]\n",
      "epoch:20 step:15952 [D loss: 0.355526, acc.: 84.38%] [G loss: 2.370550]\n",
      "epoch:20 step:15953 [D loss: 0.302879, acc.: 84.38%] [G loss: 2.363192]\n",
      "epoch:20 step:15954 [D loss: 0.429440, acc.: 78.12%] [G loss: 2.213192]\n",
      "epoch:20 step:15955 [D loss: 0.402544, acc.: 82.81%] [G loss: 2.457411]\n",
      "epoch:20 step:15956 [D loss: 0.302990, acc.: 89.84%] [G loss: 3.353423]\n",
      "epoch:20 step:15957 [D loss: 0.182835, acc.: 92.19%] [G loss: 2.765156]\n",
      "epoch:20 step:15958 [D loss: 0.308444, acc.: 87.50%] [G loss: 4.127486]\n",
      "epoch:20 step:15959 [D loss: 0.199692, acc.: 92.19%] [G loss: 5.702552]\n",
      "epoch:20 step:15960 [D loss: 0.276958, acc.: 90.62%] [G loss: 4.630322]\n",
      "epoch:20 step:15961 [D loss: 0.294741, acc.: 89.84%] [G loss: 3.138676]\n",
      "epoch:20 step:15962 [D loss: 0.252927, acc.: 90.62%] [G loss: 4.182196]\n",
      "epoch:20 step:15963 [D loss: 0.407142, acc.: 75.78%] [G loss: 4.280881]\n",
      "epoch:20 step:15964 [D loss: 0.410770, acc.: 78.12%] [G loss: 3.516976]\n",
      "epoch:20 step:15965 [D loss: 0.288985, acc.: 90.62%] [G loss: 2.647327]\n",
      "epoch:20 step:15966 [D loss: 0.312247, acc.: 84.38%] [G loss: 2.436603]\n",
      "epoch:20 step:15967 [D loss: 0.257546, acc.: 89.06%] [G loss: 2.554619]\n",
      "epoch:20 step:15968 [D loss: 0.323068, acc.: 85.94%] [G loss: 3.107218]\n",
      "epoch:20 step:15969 [D loss: 0.348256, acc.: 83.59%] [G loss: 2.609519]\n",
      "epoch:20 step:15970 [D loss: 0.354734, acc.: 85.16%] [G loss: 2.561795]\n",
      "epoch:20 step:15971 [D loss: 0.435244, acc.: 78.91%] [G loss: 2.023786]\n",
      "epoch:20 step:15972 [D loss: 0.468177, acc.: 81.25%] [G loss: 2.085506]\n",
      "epoch:20 step:15973 [D loss: 0.332688, acc.: 87.50%] [G loss: 2.671125]\n",
      "epoch:20 step:15974 [D loss: 0.244593, acc.: 89.84%] [G loss: 2.600708]\n",
      "epoch:20 step:15975 [D loss: 0.226317, acc.: 92.97%] [G loss: 2.525037]\n",
      "epoch:20 step:15976 [D loss: 0.390299, acc.: 83.59%] [G loss: 2.619473]\n",
      "epoch:20 step:15977 [D loss: 0.330584, acc.: 85.94%] [G loss: 2.100837]\n",
      "epoch:20 step:15978 [D loss: 0.290029, acc.: 90.62%] [G loss: 3.780141]\n",
      "epoch:20 step:15979 [D loss: 0.314020, acc.: 84.38%] [G loss: 2.468309]\n",
      "epoch:20 step:15980 [D loss: 0.346817, acc.: 87.50%] [G loss: 3.819166]\n",
      "epoch:20 step:15981 [D loss: 0.346150, acc.: 85.16%] [G loss: 2.731870]\n",
      "epoch:20 step:15982 [D loss: 0.376306, acc.: 82.03%] [G loss: 3.923170]\n",
      "epoch:20 step:15983 [D loss: 0.413139, acc.: 78.91%] [G loss: 2.518183]\n",
      "epoch:20 step:15984 [D loss: 0.333905, acc.: 86.72%] [G loss: 2.717878]\n",
      "epoch:20 step:15985 [D loss: 0.246737, acc.: 89.84%] [G loss: 2.997995]\n",
      "epoch:20 step:15986 [D loss: 0.341395, acc.: 82.81%] [G loss: 2.829245]\n",
      "epoch:20 step:15987 [D loss: 0.305283, acc.: 88.28%] [G loss: 4.729390]\n",
      "epoch:20 step:15988 [D loss: 0.236239, acc.: 91.41%] [G loss: 5.805140]\n",
      "epoch:20 step:15989 [D loss: 0.179747, acc.: 91.41%] [G loss: 5.879080]\n",
      "epoch:20 step:15990 [D loss: 0.262527, acc.: 86.72%] [G loss: 4.587144]\n",
      "epoch:20 step:15991 [D loss: 0.356115, acc.: 81.25%] [G loss: 2.578750]\n",
      "epoch:20 step:15992 [D loss: 0.225755, acc.: 89.84%] [G loss: 3.350626]\n",
      "epoch:20 step:15993 [D loss: 0.274664, acc.: 89.84%] [G loss: 2.483104]\n",
      "epoch:20 step:15994 [D loss: 0.280405, acc.: 85.16%] [G loss: 3.041437]\n",
      "epoch:20 step:15995 [D loss: 0.225043, acc.: 90.62%] [G loss: 4.404312]\n",
      "epoch:20 step:15996 [D loss: 0.348708, acc.: 85.94%] [G loss: 2.756320]\n",
      "epoch:20 step:15997 [D loss: 0.424390, acc.: 78.12%] [G loss: 3.214817]\n",
      "epoch:20 step:15998 [D loss: 0.346369, acc.: 85.16%] [G loss: 2.244025]\n",
      "epoch:20 step:15999 [D loss: 0.345682, acc.: 82.03%] [G loss: 2.674808]\n",
      "epoch:20 step:16000 [D loss: 0.309194, acc.: 85.94%] [G loss: 2.529386]\n",
      "epoch:20 step:16001 [D loss: 0.311736, acc.: 85.94%] [G loss: 2.759524]\n",
      "epoch:20 step:16002 [D loss: 0.334124, acc.: 82.81%] [G loss: 2.665735]\n",
      "epoch:20 step:16003 [D loss: 0.282033, acc.: 88.28%] [G loss: 2.940741]\n",
      "epoch:20 step:16004 [D loss: 0.275826, acc.: 83.59%] [G loss: 3.490146]\n",
      "epoch:20 step:16005 [D loss: 0.258038, acc.: 90.62%] [G loss: 2.238044]\n",
      "epoch:20 step:16006 [D loss: 0.333852, acc.: 88.28%] [G loss: 2.923529]\n",
      "epoch:20 step:16007 [D loss: 0.337100, acc.: 85.16%] [G loss: 3.023847]\n",
      "epoch:20 step:16008 [D loss: 0.351841, acc.: 86.72%] [G loss: 2.727507]\n",
      "epoch:20 step:16009 [D loss: 0.300364, acc.: 87.50%] [G loss: 3.934957]\n",
      "epoch:20 step:16010 [D loss: 0.332068, acc.: 85.94%] [G loss: 2.707911]\n",
      "epoch:20 step:16011 [D loss: 0.391945, acc.: 81.25%] [G loss: 2.996193]\n",
      "epoch:20 step:16012 [D loss: 0.246249, acc.: 91.41%] [G loss: 3.255161]\n",
      "epoch:20 step:16013 [D loss: 0.331058, acc.: 83.59%] [G loss: 4.028449]\n",
      "epoch:20 step:16014 [D loss: 0.267223, acc.: 89.84%] [G loss: 3.634966]\n",
      "epoch:20 step:16015 [D loss: 0.369704, acc.: 79.69%] [G loss: 2.284772]\n",
      "epoch:20 step:16016 [D loss: 0.309336, acc.: 86.72%] [G loss: 3.543903]\n",
      "epoch:20 step:16017 [D loss: 0.228190, acc.: 90.62%] [G loss: 3.558631]\n",
      "epoch:20 step:16018 [D loss: 0.362929, acc.: 83.59%] [G loss: 2.903879]\n",
      "epoch:20 step:16019 [D loss: 0.363937, acc.: 82.81%] [G loss: 3.457880]\n",
      "epoch:20 step:16020 [D loss: 0.218276, acc.: 89.84%] [G loss: 4.468709]\n",
      "epoch:20 step:16021 [D loss: 0.262448, acc.: 89.84%] [G loss: 3.276669]\n",
      "epoch:20 step:16022 [D loss: 0.254207, acc.: 88.28%] [G loss: 3.778796]\n",
      "epoch:20 step:16023 [D loss: 0.227986, acc.: 90.62%] [G loss: 3.104788]\n",
      "epoch:20 step:16024 [D loss: 0.226446, acc.: 90.62%] [G loss: 3.732399]\n",
      "epoch:20 step:16025 [D loss: 0.356074, acc.: 82.81%] [G loss: 2.775047]\n",
      "epoch:20 step:16026 [D loss: 0.383115, acc.: 82.03%] [G loss: 2.783999]\n",
      "epoch:20 step:16027 [D loss: 0.324136, acc.: 86.72%] [G loss: 2.520884]\n",
      "epoch:20 step:16028 [D loss: 0.245003, acc.: 88.28%] [G loss: 4.041704]\n",
      "epoch:20 step:16029 [D loss: 0.303166, acc.: 86.72%] [G loss: 3.449025]\n",
      "epoch:20 step:16030 [D loss: 0.351849, acc.: 84.38%] [G loss: 2.598871]\n",
      "epoch:20 step:16031 [D loss: 0.229315, acc.: 92.19%] [G loss: 2.953697]\n",
      "epoch:20 step:16032 [D loss: 0.269078, acc.: 86.72%] [G loss: 2.290760]\n",
      "epoch:20 step:16033 [D loss: 0.204974, acc.: 93.75%] [G loss: 3.556103]\n",
      "epoch:20 step:16034 [D loss: 0.381366, acc.: 84.38%] [G loss: 3.308730]\n",
      "epoch:20 step:16035 [D loss: 0.367803, acc.: 82.03%] [G loss: 2.214518]\n",
      "epoch:20 step:16036 [D loss: 0.357629, acc.: 84.38%] [G loss: 2.976327]\n",
      "epoch:20 step:16037 [D loss: 0.436884, acc.: 79.69%] [G loss: 3.466693]\n",
      "epoch:20 step:16038 [D loss: 0.362831, acc.: 82.81%] [G loss: 3.302377]\n",
      "epoch:20 step:16039 [D loss: 0.346429, acc.: 82.81%] [G loss: 2.796102]\n",
      "epoch:20 step:16040 [D loss: 0.284111, acc.: 85.94%] [G loss: 4.934273]\n",
      "epoch:20 step:16041 [D loss: 0.253601, acc.: 90.62%] [G loss: 5.329209]\n",
      "epoch:20 step:16042 [D loss: 0.307258, acc.: 85.16%] [G loss: 2.712422]\n",
      "epoch:20 step:16043 [D loss: 0.283736, acc.: 89.84%] [G loss: 4.168887]\n",
      "epoch:20 step:16044 [D loss: 0.360557, acc.: 81.25%] [G loss: 2.911503]\n",
      "epoch:20 step:16045 [D loss: 0.291306, acc.: 87.50%] [G loss: 2.896933]\n",
      "epoch:20 step:16046 [D loss: 0.310085, acc.: 84.38%] [G loss: 2.927836]\n",
      "epoch:20 step:16047 [D loss: 0.334525, acc.: 85.16%] [G loss: 2.840888]\n",
      "epoch:20 step:16048 [D loss: 0.357331, acc.: 84.38%] [G loss: 2.380234]\n",
      "epoch:20 step:16049 [D loss: 0.374537, acc.: 83.59%] [G loss: 2.249532]\n",
      "epoch:20 step:16050 [D loss: 0.334841, acc.: 85.16%] [G loss: 2.576089]\n",
      "epoch:20 step:16051 [D loss: 0.299462, acc.: 89.06%] [G loss: 2.747524]\n",
      "epoch:20 step:16052 [D loss: 0.364405, acc.: 82.81%] [G loss: 4.010306]\n",
      "epoch:20 step:16053 [D loss: 0.299401, acc.: 89.06%] [G loss: 3.144847]\n",
      "epoch:20 step:16054 [D loss: 0.339080, acc.: 85.94%] [G loss: 3.658092]\n",
      "epoch:20 step:16055 [D loss: 0.300951, acc.: 86.72%] [G loss: 4.567997]\n",
      "epoch:20 step:16056 [D loss: 0.368508, acc.: 83.59%] [G loss: 2.872375]\n",
      "epoch:20 step:16057 [D loss: 0.265687, acc.: 92.19%] [G loss: 2.550025]\n",
      "epoch:20 step:16058 [D loss: 0.330219, acc.: 88.28%] [G loss: 2.916439]\n",
      "epoch:20 step:16059 [D loss: 0.341818, acc.: 84.38%] [G loss: 2.647409]\n",
      "epoch:20 step:16060 [D loss: 0.347333, acc.: 85.94%] [G loss: 3.216063]\n",
      "epoch:20 step:16061 [D loss: 0.289703, acc.: 87.50%] [G loss: 2.873896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16062 [D loss: 0.280074, acc.: 88.28%] [G loss: 3.147110]\n",
      "epoch:20 step:16063 [D loss: 0.311403, acc.: 88.28%] [G loss: 3.537772]\n",
      "epoch:20 step:16064 [D loss: 0.322702, acc.: 82.81%] [G loss: 2.825404]\n",
      "epoch:20 step:16065 [D loss: 0.218357, acc.: 92.19%] [G loss: 3.926890]\n",
      "epoch:20 step:16066 [D loss: 0.313729, acc.: 86.72%] [G loss: 3.614018]\n",
      "epoch:20 step:16067 [D loss: 0.216538, acc.: 92.19%] [G loss: 3.149747]\n",
      "epoch:20 step:16068 [D loss: 0.347825, acc.: 82.81%] [G loss: 2.772729]\n",
      "epoch:20 step:16069 [D loss: 0.245180, acc.: 89.84%] [G loss: 4.218357]\n",
      "epoch:20 step:16070 [D loss: 0.228684, acc.: 92.97%] [G loss: 2.407931]\n",
      "epoch:20 step:16071 [D loss: 0.283285, acc.: 85.16%] [G loss: 3.826857]\n",
      "epoch:20 step:16072 [D loss: 0.339276, acc.: 80.47%] [G loss: 2.928237]\n",
      "epoch:20 step:16073 [D loss: 0.215942, acc.: 90.62%] [G loss: 4.446705]\n",
      "epoch:20 step:16074 [D loss: 0.310489, acc.: 87.50%] [G loss: 3.105596]\n",
      "epoch:20 step:16075 [D loss: 0.296636, acc.: 85.16%] [G loss: 2.646807]\n",
      "epoch:20 step:16076 [D loss: 0.288184, acc.: 85.16%] [G loss: 4.153762]\n",
      "epoch:20 step:16077 [D loss: 0.322409, acc.: 87.50%] [G loss: 3.522151]\n",
      "epoch:20 step:16078 [D loss: 0.351629, acc.: 81.25%] [G loss: 3.154745]\n",
      "epoch:20 step:16079 [D loss: 0.282948, acc.: 87.50%] [G loss: 4.019716]\n",
      "epoch:20 step:16080 [D loss: 0.398823, acc.: 82.03%] [G loss: 3.119914]\n",
      "epoch:20 step:16081 [D loss: 0.288776, acc.: 89.06%] [G loss: 4.931499]\n",
      "epoch:20 step:16082 [D loss: 0.308679, acc.: 89.84%] [G loss: 3.393236]\n",
      "epoch:20 step:16083 [D loss: 0.250881, acc.: 89.84%] [G loss: 2.736293]\n",
      "epoch:20 step:16084 [D loss: 0.235585, acc.: 90.62%] [G loss: 2.578288]\n",
      "epoch:20 step:16085 [D loss: 0.293009, acc.: 90.62%] [G loss: 2.664920]\n",
      "epoch:20 step:16086 [D loss: 0.353946, acc.: 83.59%] [G loss: 3.189431]\n",
      "epoch:20 step:16087 [D loss: 0.369300, acc.: 82.03%] [G loss: 2.632612]\n",
      "epoch:20 step:16088 [D loss: 0.333501, acc.: 82.03%] [G loss: 2.720908]\n",
      "epoch:20 step:16089 [D loss: 0.302548, acc.: 83.59%] [G loss: 3.007918]\n",
      "epoch:20 step:16090 [D loss: 0.334709, acc.: 87.50%] [G loss: 3.565900]\n",
      "epoch:20 step:16091 [D loss: 0.322902, acc.: 88.28%] [G loss: 2.898800]\n",
      "epoch:20 step:16092 [D loss: 0.260149, acc.: 89.06%] [G loss: 2.760821]\n",
      "epoch:20 step:16093 [D loss: 0.284789, acc.: 87.50%] [G loss: 2.782227]\n",
      "epoch:20 step:16094 [D loss: 0.398024, acc.: 81.25%] [G loss: 2.684101]\n",
      "epoch:20 step:16095 [D loss: 0.389993, acc.: 82.81%] [G loss: 2.384933]\n",
      "epoch:20 step:16096 [D loss: 0.220027, acc.: 92.19%] [G loss: 3.908271]\n",
      "epoch:20 step:16097 [D loss: 0.349309, acc.: 85.16%] [G loss: 3.933360]\n",
      "epoch:20 step:16098 [D loss: 0.412152, acc.: 81.25%] [G loss: 3.292032]\n",
      "epoch:20 step:16099 [D loss: 0.316603, acc.: 85.94%] [G loss: 3.598130]\n",
      "epoch:20 step:16100 [D loss: 0.315091, acc.: 85.94%] [G loss: 3.263018]\n",
      "epoch:20 step:16101 [D loss: 0.337455, acc.: 82.03%] [G loss: 3.023576]\n",
      "epoch:20 step:16102 [D loss: 0.309314, acc.: 87.50%] [G loss: 3.082388]\n",
      "epoch:20 step:16103 [D loss: 0.405512, acc.: 82.03%] [G loss: 3.148232]\n",
      "epoch:20 step:16104 [D loss: 0.381595, acc.: 83.59%] [G loss: 3.688980]\n",
      "epoch:20 step:16105 [D loss: 0.352067, acc.: 85.16%] [G loss: 3.406840]\n",
      "epoch:20 step:16106 [D loss: 0.309974, acc.: 83.59%] [G loss: 3.335044]\n",
      "epoch:20 step:16107 [D loss: 0.305840, acc.: 86.72%] [G loss: 2.836146]\n",
      "epoch:20 step:16108 [D loss: 0.426980, acc.: 84.38%] [G loss: 2.598896]\n",
      "epoch:20 step:16109 [D loss: 0.322840, acc.: 84.38%] [G loss: 2.604922]\n",
      "epoch:20 step:16110 [D loss: 0.345795, acc.: 85.94%] [G loss: 3.288030]\n",
      "epoch:20 step:16111 [D loss: 0.337819, acc.: 85.94%] [G loss: 2.562877]\n",
      "epoch:20 step:16112 [D loss: 0.392497, acc.: 82.81%] [G loss: 2.817477]\n",
      "epoch:20 step:16113 [D loss: 0.334881, acc.: 84.38%] [G loss: 2.885952]\n",
      "epoch:20 step:16114 [D loss: 0.393304, acc.: 85.16%] [G loss: 2.537850]\n",
      "epoch:20 step:16115 [D loss: 0.351287, acc.: 84.38%] [G loss: 2.724434]\n",
      "epoch:20 step:16116 [D loss: 0.294880, acc.: 87.50%] [G loss: 2.313067]\n",
      "epoch:20 step:16117 [D loss: 0.284665, acc.: 88.28%] [G loss: 3.342640]\n",
      "epoch:20 step:16118 [D loss: 0.309976, acc.: 86.72%] [G loss: 3.002384]\n",
      "epoch:20 step:16119 [D loss: 0.315806, acc.: 83.59%] [G loss: 3.315952]\n",
      "epoch:20 step:16120 [D loss: 0.292643, acc.: 88.28%] [G loss: 3.419907]\n",
      "epoch:20 step:16121 [D loss: 0.411550, acc.: 78.12%] [G loss: 2.282200]\n",
      "epoch:20 step:16122 [D loss: 0.351786, acc.: 82.03%] [G loss: 2.839086]\n",
      "epoch:20 step:16123 [D loss: 0.362040, acc.: 85.94%] [G loss: 3.074549]\n",
      "epoch:20 step:16124 [D loss: 0.332572, acc.: 89.84%] [G loss: 3.108634]\n",
      "epoch:20 step:16125 [D loss: 0.339401, acc.: 87.50%] [G loss: 3.944623]\n",
      "epoch:20 step:16126 [D loss: 0.297067, acc.: 87.50%] [G loss: 4.174140]\n",
      "epoch:20 step:16127 [D loss: 0.405659, acc.: 82.03%] [G loss: 3.858880]\n",
      "epoch:20 step:16128 [D loss: 0.264281, acc.: 89.06%] [G loss: 3.041707]\n",
      "epoch:20 step:16129 [D loss: 0.383953, acc.: 85.94%] [G loss: 4.195950]\n",
      "epoch:20 step:16130 [D loss: 0.379892, acc.: 82.03%] [G loss: 2.453313]\n",
      "epoch:20 step:16131 [D loss: 0.318872, acc.: 83.59%] [G loss: 4.104290]\n",
      "epoch:20 step:16132 [D loss: 0.207376, acc.: 91.41%] [G loss: 5.714122]\n",
      "epoch:20 step:16133 [D loss: 0.290905, acc.: 84.38%] [G loss: 3.962830]\n",
      "epoch:20 step:16134 [D loss: 0.241618, acc.: 90.62%] [G loss: 5.054306]\n",
      "epoch:20 step:16135 [D loss: 0.244446, acc.: 88.28%] [G loss: 5.231159]\n",
      "epoch:20 step:16136 [D loss: 0.239797, acc.: 86.72%] [G loss: 3.710384]\n",
      "epoch:20 step:16137 [D loss: 0.252988, acc.: 91.41%] [G loss: 5.145594]\n",
      "epoch:20 step:16138 [D loss: 0.260471, acc.: 86.72%] [G loss: 3.092907]\n",
      "epoch:20 step:16139 [D loss: 0.303055, acc.: 87.50%] [G loss: 3.025792]\n",
      "epoch:20 step:16140 [D loss: 0.270519, acc.: 89.84%] [G loss: 2.841586]\n",
      "epoch:20 step:16141 [D loss: 0.270600, acc.: 87.50%] [G loss: 3.808665]\n",
      "epoch:20 step:16142 [D loss: 0.320715, acc.: 86.72%] [G loss: 3.015215]\n",
      "epoch:20 step:16143 [D loss: 0.224688, acc.: 88.28%] [G loss: 4.691404]\n",
      "epoch:20 step:16144 [D loss: 0.449446, acc.: 80.47%] [G loss: 2.987654]\n",
      "epoch:20 step:16145 [D loss: 0.248008, acc.: 91.41%] [G loss: 3.016362]\n",
      "epoch:20 step:16146 [D loss: 0.381979, acc.: 82.81%] [G loss: 4.513732]\n",
      "epoch:20 step:16147 [D loss: 0.413716, acc.: 82.03%] [G loss: 4.024142]\n",
      "epoch:20 step:16148 [D loss: 0.246879, acc.: 88.28%] [G loss: 3.016456]\n",
      "epoch:20 step:16149 [D loss: 0.326539, acc.: 82.03%] [G loss: 4.699958]\n",
      "epoch:20 step:16150 [D loss: 0.522113, acc.: 78.12%] [G loss: 5.894874]\n",
      "epoch:20 step:16151 [D loss: 0.782604, acc.: 72.66%] [G loss: 7.362658]\n",
      "epoch:20 step:16152 [D loss: 1.412757, acc.: 59.38%] [G loss: 7.511893]\n",
      "epoch:20 step:16153 [D loss: 1.737255, acc.: 65.62%] [G loss: 3.611940]\n",
      "epoch:20 step:16154 [D loss: 0.375271, acc.: 82.81%] [G loss: 4.496902]\n",
      "epoch:20 step:16155 [D loss: 0.354489, acc.: 87.50%] [G loss: 3.299903]\n",
      "epoch:20 step:16156 [D loss: 0.447116, acc.: 81.25%] [G loss: 2.535027]\n",
      "epoch:20 step:16157 [D loss: 0.385446, acc.: 79.69%] [G loss: 3.513124]\n",
      "epoch:20 step:16158 [D loss: 0.365978, acc.: 82.81%] [G loss: 2.825314]\n",
      "epoch:20 step:16159 [D loss: 0.353062, acc.: 88.28%] [G loss: 2.364140]\n",
      "epoch:20 step:16160 [D loss: 0.369492, acc.: 84.38%] [G loss: 2.245045]\n",
      "epoch:20 step:16161 [D loss: 0.304734, acc.: 87.50%] [G loss: 2.705985]\n",
      "epoch:20 step:16162 [D loss: 0.418928, acc.: 78.91%] [G loss: 3.195282]\n",
      "epoch:20 step:16163 [D loss: 0.354871, acc.: 85.16%] [G loss: 2.469298]\n",
      "epoch:20 step:16164 [D loss: 0.421020, acc.: 82.03%] [G loss: 3.450921]\n",
      "epoch:20 step:16165 [D loss: 0.303463, acc.: 85.16%] [G loss: 2.644586]\n",
      "epoch:20 step:16166 [D loss: 0.324330, acc.: 87.50%] [G loss: 2.206987]\n",
      "epoch:20 step:16167 [D loss: 0.374885, acc.: 81.25%] [G loss: 2.449821]\n",
      "epoch:20 step:16168 [D loss: 0.433945, acc.: 77.34%] [G loss: 3.151843]\n",
      "epoch:20 step:16169 [D loss: 0.333909, acc.: 85.16%] [G loss: 4.510971]\n",
      "epoch:20 step:16170 [D loss: 0.254346, acc.: 90.62%] [G loss: 3.256601]\n",
      "epoch:20 step:16171 [D loss: 0.266265, acc.: 89.06%] [G loss: 4.780856]\n",
      "epoch:20 step:16172 [D loss: 0.400665, acc.: 80.47%] [G loss: 2.806635]\n",
      "epoch:20 step:16173 [D loss: 0.328107, acc.: 84.38%] [G loss: 2.616794]\n",
      "epoch:20 step:16174 [D loss: 0.322948, acc.: 85.94%] [G loss: 2.723116]\n",
      "epoch:20 step:16175 [D loss: 0.360641, acc.: 86.72%] [G loss: 2.408943]\n",
      "epoch:20 step:16176 [D loss: 0.365076, acc.: 84.38%] [G loss: 2.703076]\n",
      "epoch:20 step:16177 [D loss: 0.285191, acc.: 89.06%] [G loss: 2.831694]\n",
      "epoch:20 step:16178 [D loss: 0.262970, acc.: 88.28%] [G loss: 2.135001]\n",
      "epoch:20 step:16179 [D loss: 0.377792, acc.: 84.38%] [G loss: 2.479076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16180 [D loss: 0.320644, acc.: 85.94%] [G loss: 3.148635]\n",
      "epoch:20 step:16181 [D loss: 0.346215, acc.: 85.94%] [G loss: 2.825464]\n",
      "epoch:20 step:16182 [D loss: 0.321303, acc.: 85.16%] [G loss: 2.369431]\n",
      "epoch:20 step:16183 [D loss: 0.228189, acc.: 92.97%] [G loss: 2.363905]\n",
      "epoch:20 step:16184 [D loss: 0.312997, acc.: 86.72%] [G loss: 2.387431]\n",
      "epoch:20 step:16185 [D loss: 0.314616, acc.: 88.28%] [G loss: 3.019939]\n",
      "epoch:20 step:16186 [D loss: 0.291774, acc.: 89.84%] [G loss: 2.850513]\n",
      "epoch:20 step:16187 [D loss: 0.291523, acc.: 86.72%] [G loss: 2.942885]\n",
      "epoch:20 step:16188 [D loss: 0.352484, acc.: 83.59%] [G loss: 2.329092]\n",
      "epoch:20 step:16189 [D loss: 0.241478, acc.: 90.62%] [G loss: 2.468643]\n",
      "epoch:20 step:16190 [D loss: 0.294899, acc.: 88.28%] [G loss: 2.407665]\n",
      "epoch:20 step:16191 [D loss: 0.323013, acc.: 87.50%] [G loss: 2.853331]\n",
      "epoch:20 step:16192 [D loss: 0.206599, acc.: 90.62%] [G loss: 3.268361]\n",
      "epoch:20 step:16193 [D loss: 0.251634, acc.: 89.84%] [G loss: 3.075645]\n",
      "epoch:20 step:16194 [D loss: 0.352208, acc.: 84.38%] [G loss: 3.538485]\n",
      "epoch:20 step:16195 [D loss: 0.321253, acc.: 85.94%] [G loss: 2.692872]\n",
      "epoch:20 step:16196 [D loss: 0.238296, acc.: 89.84%] [G loss: 2.999314]\n",
      "epoch:20 step:16197 [D loss: 0.294617, acc.: 86.72%] [G loss: 2.704005]\n",
      "epoch:20 step:16198 [D loss: 0.241898, acc.: 89.06%] [G loss: 2.543719]\n",
      "epoch:20 step:16199 [D loss: 0.384238, acc.: 84.38%] [G loss: 2.301412]\n",
      "epoch:20 step:16200 [D loss: 0.306305, acc.: 86.72%] [G loss: 4.075020]\n",
      "epoch:20 step:16201 [D loss: 0.268335, acc.: 88.28%] [G loss: 4.269225]\n",
      "epoch:20 step:16202 [D loss: 0.274322, acc.: 87.50%] [G loss: 3.753453]\n",
      "epoch:20 step:16203 [D loss: 0.309975, acc.: 87.50%] [G loss: 3.239318]\n",
      "epoch:20 step:16204 [D loss: 0.234414, acc.: 91.41%] [G loss: 3.914259]\n",
      "epoch:20 step:16205 [D loss: 0.298012, acc.: 85.16%] [G loss: 2.691081]\n",
      "epoch:20 step:16206 [D loss: 0.290072, acc.: 88.28%] [G loss: 2.914405]\n",
      "epoch:20 step:16207 [D loss: 0.281503, acc.: 87.50%] [G loss: 2.949287]\n",
      "epoch:20 step:16208 [D loss: 0.367999, acc.: 85.16%] [G loss: 2.786315]\n",
      "epoch:20 step:16209 [D loss: 0.327188, acc.: 86.72%] [G loss: 2.792063]\n",
      "epoch:20 step:16210 [D loss: 0.409234, acc.: 83.59%] [G loss: 2.648212]\n",
      "epoch:20 step:16211 [D loss: 0.338318, acc.: 89.06%] [G loss: 2.954871]\n",
      "epoch:20 step:16212 [D loss: 0.363171, acc.: 85.94%] [G loss: 3.553139]\n",
      "epoch:20 step:16213 [D loss: 0.326982, acc.: 85.16%] [G loss: 3.337889]\n",
      "epoch:20 step:16214 [D loss: 0.257978, acc.: 89.84%] [G loss: 3.749511]\n",
      "epoch:20 step:16215 [D loss: 0.334330, acc.: 82.81%] [G loss: 3.560531]\n",
      "epoch:20 step:16216 [D loss: 0.273979, acc.: 89.06%] [G loss: 2.623838]\n",
      "epoch:20 step:16217 [D loss: 0.283290, acc.: 87.50%] [G loss: 4.291206]\n",
      "epoch:20 step:16218 [D loss: 0.232979, acc.: 92.97%] [G loss: 4.155712]\n",
      "epoch:20 step:16219 [D loss: 0.219052, acc.: 91.41%] [G loss: 4.606627]\n",
      "epoch:20 step:16220 [D loss: 0.268706, acc.: 92.19%] [G loss: 2.423992]\n",
      "epoch:20 step:16221 [D loss: 0.352392, acc.: 85.16%] [G loss: 2.639680]\n",
      "epoch:20 step:16222 [D loss: 0.277655, acc.: 85.94%] [G loss: 3.542612]\n",
      "epoch:20 step:16223 [D loss: 0.185041, acc.: 92.97%] [G loss: 5.838297]\n",
      "epoch:20 step:16224 [D loss: 0.241075, acc.: 86.72%] [G loss: 6.618551]\n",
      "epoch:20 step:16225 [D loss: 0.333663, acc.: 85.16%] [G loss: 3.414794]\n",
      "epoch:20 step:16226 [D loss: 0.235023, acc.: 87.50%] [G loss: 4.533860]\n",
      "epoch:20 step:16227 [D loss: 0.299097, acc.: 87.50%] [G loss: 4.547985]\n",
      "epoch:20 step:16228 [D loss: 0.307185, acc.: 88.28%] [G loss: 3.234610]\n",
      "epoch:20 step:16229 [D loss: 0.335996, acc.: 85.94%] [G loss: 6.023225]\n",
      "epoch:20 step:16230 [D loss: 0.320015, acc.: 86.72%] [G loss: 3.212747]\n",
      "epoch:20 step:16231 [D loss: 0.326315, acc.: 85.16%] [G loss: 3.487627]\n",
      "epoch:20 step:16232 [D loss: 0.206064, acc.: 92.97%] [G loss: 3.187714]\n",
      "epoch:20 step:16233 [D loss: 0.301134, acc.: 83.59%] [G loss: 3.909166]\n",
      "epoch:20 step:16234 [D loss: 0.320683, acc.: 82.81%] [G loss: 5.432008]\n",
      "epoch:20 step:16235 [D loss: 0.286420, acc.: 85.94%] [G loss: 4.039419]\n",
      "epoch:20 step:16236 [D loss: 0.292405, acc.: 84.38%] [G loss: 3.015863]\n",
      "epoch:20 step:16237 [D loss: 0.258328, acc.: 90.62%] [G loss: 3.017147]\n",
      "epoch:20 step:16238 [D loss: 0.389470, acc.: 80.47%] [G loss: 2.975308]\n",
      "epoch:20 step:16239 [D loss: 0.253692, acc.: 89.06%] [G loss: 4.253076]\n",
      "epoch:20 step:16240 [D loss: 0.287029, acc.: 89.06%] [G loss: 2.907738]\n",
      "epoch:20 step:16241 [D loss: 0.288722, acc.: 87.50%] [G loss: 2.979162]\n",
      "epoch:20 step:16242 [D loss: 0.322956, acc.: 85.16%] [G loss: 3.014300]\n",
      "epoch:20 step:16243 [D loss: 0.327940, acc.: 84.38%] [G loss: 3.149106]\n",
      "epoch:20 step:16244 [D loss: 0.327594, acc.: 83.59%] [G loss: 2.519766]\n",
      "epoch:20 step:16245 [D loss: 0.363974, acc.: 84.38%] [G loss: 2.744192]\n",
      "epoch:20 step:16246 [D loss: 0.367774, acc.: 84.38%] [G loss: 2.564520]\n",
      "epoch:20 step:16247 [D loss: 0.458366, acc.: 79.69%] [G loss: 3.514934]\n",
      "epoch:20 step:16248 [D loss: 0.435521, acc.: 78.91%] [G loss: 4.925149]\n",
      "epoch:20 step:16249 [D loss: 0.406774, acc.: 80.47%] [G loss: 3.914537]\n",
      "epoch:20 step:16250 [D loss: 0.329362, acc.: 82.03%] [G loss: 2.840742]\n",
      "epoch:20 step:16251 [D loss: 0.277337, acc.: 88.28%] [G loss: 3.200983]\n",
      "epoch:20 step:16252 [D loss: 0.377413, acc.: 84.38%] [G loss: 4.094971]\n",
      "epoch:20 step:16253 [D loss: 0.490535, acc.: 78.91%] [G loss: 2.693909]\n",
      "epoch:20 step:16254 [D loss: 0.429528, acc.: 75.00%] [G loss: 2.755210]\n",
      "epoch:20 step:16255 [D loss: 0.390198, acc.: 84.38%] [G loss: 2.473432]\n",
      "epoch:20 step:16256 [D loss: 0.254027, acc.: 91.41%] [G loss: 2.846719]\n",
      "epoch:20 step:16257 [D loss: 0.275420, acc.: 89.06%] [G loss: 2.220563]\n",
      "epoch:20 step:16258 [D loss: 0.306737, acc.: 92.19%] [G loss: 2.621027]\n",
      "epoch:20 step:16259 [D loss: 0.234815, acc.: 89.84%] [G loss: 4.405204]\n",
      "epoch:20 step:16260 [D loss: 0.205192, acc.: 90.62%] [G loss: 4.394619]\n",
      "epoch:20 step:16261 [D loss: 0.273243, acc.: 85.94%] [G loss: 3.943499]\n",
      "epoch:20 step:16262 [D loss: 0.247527, acc.: 93.75%] [G loss: 4.157188]\n",
      "epoch:20 step:16263 [D loss: 0.318797, acc.: 88.28%] [G loss: 4.571157]\n",
      "epoch:20 step:16264 [D loss: 0.240263, acc.: 90.62%] [G loss: 4.141678]\n",
      "epoch:20 step:16265 [D loss: 0.303947, acc.: 88.28%] [G loss: 3.209839]\n",
      "epoch:20 step:16266 [D loss: 0.342331, acc.: 83.59%] [G loss: 3.024517]\n",
      "epoch:20 step:16267 [D loss: 0.259524, acc.: 90.62%] [G loss: 2.772614]\n",
      "epoch:20 step:16268 [D loss: 0.365775, acc.: 82.03%] [G loss: 2.795675]\n",
      "epoch:20 step:16269 [D loss: 0.344914, acc.: 85.94%] [G loss: 2.495893]\n",
      "epoch:20 step:16270 [D loss: 0.318588, acc.: 85.16%] [G loss: 2.917214]\n",
      "epoch:20 step:16271 [D loss: 0.337694, acc.: 85.94%] [G loss: 4.044790]\n",
      "epoch:20 step:16272 [D loss: 0.370867, acc.: 85.16%] [G loss: 3.995433]\n",
      "epoch:20 step:16273 [D loss: 0.405548, acc.: 83.59%] [G loss: 2.723766]\n",
      "epoch:20 step:16274 [D loss: 0.381055, acc.: 81.25%] [G loss: 3.167993]\n",
      "epoch:20 step:16275 [D loss: 0.415943, acc.: 80.47%] [G loss: 3.315440]\n",
      "epoch:20 step:16276 [D loss: 0.543257, acc.: 77.34%] [G loss: 4.369900]\n",
      "epoch:20 step:16277 [D loss: 0.474244, acc.: 76.56%] [G loss: 4.041521]\n",
      "epoch:20 step:16278 [D loss: 0.415801, acc.: 83.59%] [G loss: 3.282701]\n",
      "epoch:20 step:16279 [D loss: 0.283416, acc.: 88.28%] [G loss: 3.449781]\n",
      "epoch:20 step:16280 [D loss: 0.474747, acc.: 75.78%] [G loss: 6.161630]\n",
      "epoch:20 step:16281 [D loss: 0.429352, acc.: 79.69%] [G loss: 3.016156]\n",
      "epoch:20 step:16282 [D loss: 0.295453, acc.: 87.50%] [G loss: 3.771208]\n",
      "epoch:20 step:16283 [D loss: 0.200962, acc.: 93.75%] [G loss: 3.362216]\n",
      "epoch:20 step:16284 [D loss: 0.393283, acc.: 78.12%] [G loss: 3.369100]\n",
      "epoch:20 step:16285 [D loss: 0.207419, acc.: 92.19%] [G loss: 3.135581]\n",
      "epoch:20 step:16286 [D loss: 0.341376, acc.: 85.94%] [G loss: 3.370912]\n",
      "epoch:20 step:16287 [D loss: 0.253338, acc.: 86.72%] [G loss: 3.680035]\n",
      "epoch:20 step:16288 [D loss: 0.391826, acc.: 82.81%] [G loss: 3.307855]\n",
      "epoch:20 step:16289 [D loss: 0.451915, acc.: 79.69%] [G loss: 2.821314]\n",
      "epoch:20 step:16290 [D loss: 0.360690, acc.: 86.72%] [G loss: 3.034339]\n",
      "epoch:20 step:16291 [D loss: 0.404395, acc.: 82.03%] [G loss: 3.730606]\n",
      "epoch:20 step:16292 [D loss: 0.355013, acc.: 80.47%] [G loss: 4.037839]\n",
      "epoch:20 step:16293 [D loss: 0.228479, acc.: 92.97%] [G loss: 3.407621]\n",
      "epoch:20 step:16294 [D loss: 0.278725, acc.: 87.50%] [G loss: 4.076169]\n",
      "epoch:20 step:16295 [D loss: 0.317206, acc.: 84.38%] [G loss: 3.531197]\n",
      "epoch:20 step:16296 [D loss: 0.329860, acc.: 82.81%] [G loss: 3.237792]\n",
      "epoch:20 step:16297 [D loss: 0.275638, acc.: 89.84%] [G loss: 2.795631]\n",
      "epoch:20 step:16298 [D loss: 0.253053, acc.: 88.28%] [G loss: 2.864028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16299 [D loss: 0.218418, acc.: 89.06%] [G loss: 3.803188]\n",
      "epoch:20 step:16300 [D loss: 0.258138, acc.: 92.19%] [G loss: 3.698524]\n",
      "epoch:20 step:16301 [D loss: 0.236081, acc.: 95.31%] [G loss: 3.458674]\n",
      "epoch:20 step:16302 [D loss: 0.328326, acc.: 84.38%] [G loss: 4.438041]\n",
      "epoch:20 step:16303 [D loss: 0.428746, acc.: 80.47%] [G loss: 3.520812]\n",
      "epoch:20 step:16304 [D loss: 0.329570, acc.: 88.28%] [G loss: 3.311621]\n",
      "epoch:20 step:16305 [D loss: 0.355735, acc.: 82.81%] [G loss: 2.829817]\n",
      "epoch:20 step:16306 [D loss: 0.278128, acc.: 89.06%] [G loss: 2.955128]\n",
      "epoch:20 step:16307 [D loss: 0.255742, acc.: 89.06%] [G loss: 3.269862]\n",
      "epoch:20 step:16308 [D loss: 0.379683, acc.: 85.94%] [G loss: 2.699667]\n",
      "epoch:20 step:16309 [D loss: 0.378708, acc.: 82.03%] [G loss: 3.580969]\n",
      "epoch:20 step:16310 [D loss: 0.326632, acc.: 89.06%] [G loss: 2.758711]\n",
      "epoch:20 step:16311 [D loss: 0.296281, acc.: 86.72%] [G loss: 3.719161]\n",
      "epoch:20 step:16312 [D loss: 0.431792, acc.: 78.91%] [G loss: 2.903888]\n",
      "epoch:20 step:16313 [D loss: 0.280638, acc.: 88.28%] [G loss: 2.888872]\n",
      "epoch:20 step:16314 [D loss: 0.304650, acc.: 86.72%] [G loss: 3.069000]\n",
      "epoch:20 step:16315 [D loss: 0.322111, acc.: 85.16%] [G loss: 3.028528]\n",
      "epoch:20 step:16316 [D loss: 0.215246, acc.: 91.41%] [G loss: 2.652124]\n",
      "epoch:20 step:16317 [D loss: 0.260037, acc.: 92.19%] [G loss: 4.212669]\n",
      "epoch:20 step:16318 [D loss: 0.228502, acc.: 89.84%] [G loss: 4.012458]\n",
      "epoch:20 step:16319 [D loss: 0.322043, acc.: 89.06%] [G loss: 3.633958]\n",
      "epoch:20 step:16320 [D loss: 0.305884, acc.: 88.28%] [G loss: 2.926123]\n",
      "epoch:20 step:16321 [D loss: 0.345378, acc.: 85.94%] [G loss: 3.646703]\n",
      "epoch:20 step:16322 [D loss: 0.356623, acc.: 84.38%] [G loss: 3.301911]\n",
      "epoch:20 step:16323 [D loss: 0.360662, acc.: 85.16%] [G loss: 4.585804]\n",
      "epoch:20 step:16324 [D loss: 0.511279, acc.: 77.34%] [G loss: 4.044592]\n",
      "epoch:20 step:16325 [D loss: 0.335719, acc.: 85.94%] [G loss: 3.670548]\n",
      "epoch:20 step:16326 [D loss: 0.277577, acc.: 89.06%] [G loss: 3.599130]\n",
      "epoch:20 step:16327 [D loss: 0.385315, acc.: 81.25%] [G loss: 2.978101]\n",
      "epoch:20 step:16328 [D loss: 0.461969, acc.: 79.69%] [G loss: 2.034121]\n",
      "epoch:20 step:16329 [D loss: 0.411963, acc.: 82.03%] [G loss: 2.906192]\n",
      "epoch:20 step:16330 [D loss: 0.287199, acc.: 88.28%] [G loss: 2.995027]\n",
      "epoch:20 step:16331 [D loss: 0.283594, acc.: 89.06%] [G loss: 2.773849]\n",
      "epoch:20 step:16332 [D loss: 0.281018, acc.: 85.94%] [G loss: 4.227537]\n",
      "epoch:20 step:16333 [D loss: 0.308424, acc.: 85.16%] [G loss: 2.893613]\n",
      "epoch:20 step:16334 [D loss: 0.272203, acc.: 92.19%] [G loss: 3.603296]\n",
      "epoch:20 step:16335 [D loss: 0.315887, acc.: 87.50%] [G loss: 3.605155]\n",
      "epoch:20 step:16336 [D loss: 0.262254, acc.: 89.06%] [G loss: 3.277489]\n",
      "epoch:20 step:16337 [D loss: 0.176570, acc.: 94.53%] [G loss: 5.508058]\n",
      "epoch:20 step:16338 [D loss: 0.247023, acc.: 88.28%] [G loss: 2.761244]\n",
      "epoch:20 step:16339 [D loss: 0.288438, acc.: 86.72%] [G loss: 3.036239]\n",
      "epoch:20 step:16340 [D loss: 0.229745, acc.: 89.06%] [G loss: 3.572407]\n",
      "epoch:20 step:16341 [D loss: 0.354066, acc.: 85.16%] [G loss: 3.562583]\n",
      "epoch:20 step:16342 [D loss: 0.244941, acc.: 89.84%] [G loss: 3.111486]\n",
      "epoch:20 step:16343 [D loss: 0.269468, acc.: 91.41%] [G loss: 3.573127]\n",
      "epoch:20 step:16344 [D loss: 0.288778, acc.: 89.06%] [G loss: 3.140125]\n",
      "epoch:20 step:16345 [D loss: 0.203061, acc.: 95.31%] [G loss: 3.302431]\n",
      "epoch:20 step:16346 [D loss: 0.248759, acc.: 89.84%] [G loss: 3.288099]\n",
      "epoch:20 step:16347 [D loss: 0.290908, acc.: 89.06%] [G loss: 3.435215]\n",
      "epoch:20 step:16348 [D loss: 0.327750, acc.: 87.50%] [G loss: 3.571552]\n",
      "epoch:20 step:16349 [D loss: 0.266091, acc.: 85.94%] [G loss: 3.658483]\n",
      "epoch:20 step:16350 [D loss: 0.248172, acc.: 90.62%] [G loss: 2.716864]\n",
      "epoch:20 step:16351 [D loss: 0.266943, acc.: 88.28%] [G loss: 4.191524]\n",
      "epoch:20 step:16352 [D loss: 0.253374, acc.: 91.41%] [G loss: 3.929625]\n",
      "epoch:20 step:16353 [D loss: 0.275774, acc.: 89.06%] [G loss: 3.370164]\n",
      "epoch:20 step:16354 [D loss: 0.330964, acc.: 82.81%] [G loss: 3.059226]\n",
      "epoch:20 step:16355 [D loss: 0.298030, acc.: 84.38%] [G loss: 3.237649]\n",
      "epoch:20 step:16356 [D loss: 0.328092, acc.: 85.16%] [G loss: 3.855805]\n",
      "epoch:20 step:16357 [D loss: 0.375867, acc.: 83.59%] [G loss: 3.520808]\n",
      "epoch:20 step:16358 [D loss: 0.342268, acc.: 85.16%] [G loss: 3.967228]\n",
      "epoch:20 step:16359 [D loss: 0.257580, acc.: 86.72%] [G loss: 6.379672]\n",
      "epoch:20 step:16360 [D loss: 0.208219, acc.: 90.62%] [G loss: 4.865644]\n",
      "epoch:20 step:16361 [D loss: 0.238200, acc.: 91.41%] [G loss: 4.767777]\n",
      "epoch:20 step:16362 [D loss: 0.343588, acc.: 85.94%] [G loss: 4.362384]\n",
      "epoch:20 step:16363 [D loss: 0.363417, acc.: 87.50%] [G loss: 3.553634]\n",
      "epoch:20 step:16364 [D loss: 0.164528, acc.: 93.75%] [G loss: 3.962939]\n",
      "epoch:20 step:16365 [D loss: 0.379387, acc.: 83.59%] [G loss: 2.806961]\n",
      "epoch:20 step:16366 [D loss: 0.272362, acc.: 90.62%] [G loss: 3.244708]\n",
      "epoch:20 step:16367 [D loss: 0.202679, acc.: 92.19%] [G loss: 5.197598]\n",
      "epoch:20 step:16368 [D loss: 0.285340, acc.: 87.50%] [G loss: 4.146025]\n",
      "epoch:20 step:16369 [D loss: 0.292998, acc.: 86.72%] [G loss: 3.707692]\n",
      "epoch:20 step:16370 [D loss: 0.315292, acc.: 88.28%] [G loss: 3.706968]\n",
      "epoch:20 step:16371 [D loss: 0.229232, acc.: 90.62%] [G loss: 4.730712]\n",
      "epoch:20 step:16372 [D loss: 0.304515, acc.: 87.50%] [G loss: 4.726465]\n",
      "epoch:20 step:16373 [D loss: 0.233558, acc.: 88.28%] [G loss: 3.137407]\n",
      "epoch:20 step:16374 [D loss: 0.287047, acc.: 89.84%] [G loss: 3.726193]\n",
      "epoch:20 step:16375 [D loss: 0.320745, acc.: 85.94%] [G loss: 3.794300]\n",
      "epoch:20 step:16376 [D loss: 0.376889, acc.: 85.16%] [G loss: 3.676995]\n",
      "epoch:20 step:16377 [D loss: 0.261779, acc.: 89.84%] [G loss: 3.341452]\n",
      "epoch:20 step:16378 [D loss: 0.227960, acc.: 89.06%] [G loss: 4.471952]\n",
      "epoch:20 step:16379 [D loss: 0.335164, acc.: 85.94%] [G loss: 3.006733]\n",
      "epoch:20 step:16380 [D loss: 0.249540, acc.: 88.28%] [G loss: 3.143722]\n",
      "epoch:20 step:16381 [D loss: 0.276651, acc.: 85.16%] [G loss: 5.627344]\n",
      "epoch:20 step:16382 [D loss: 0.360556, acc.: 85.94%] [G loss: 3.459663]\n",
      "epoch:20 step:16383 [D loss: 0.315071, acc.: 86.72%] [G loss: 3.623680]\n",
      "epoch:20 step:16384 [D loss: 0.404390, acc.: 81.25%] [G loss: 3.489606]\n",
      "epoch:20 step:16385 [D loss: 0.325228, acc.: 86.72%] [G loss: 4.058973]\n",
      "epoch:20 step:16386 [D loss: 0.294289, acc.: 86.72%] [G loss: 3.204080]\n",
      "epoch:20 step:16387 [D loss: 0.244649, acc.: 90.62%] [G loss: 3.825535]\n",
      "epoch:20 step:16388 [D loss: 0.288951, acc.: 86.72%] [G loss: 4.338547]\n",
      "epoch:20 step:16389 [D loss: 0.274591, acc.: 87.50%] [G loss: 3.590027]\n",
      "epoch:20 step:16390 [D loss: 0.256449, acc.: 88.28%] [G loss: 3.057137]\n",
      "epoch:20 step:16391 [D loss: 0.308267, acc.: 85.16%] [G loss: 3.404435]\n",
      "epoch:20 step:16392 [D loss: 0.283077, acc.: 87.50%] [G loss: 3.974403]\n",
      "epoch:20 step:16393 [D loss: 0.369849, acc.: 85.94%] [G loss: 4.662597]\n",
      "epoch:20 step:16394 [D loss: 0.607926, acc.: 75.78%] [G loss: 6.164643]\n",
      "epoch:20 step:16395 [D loss: 0.674325, acc.: 74.22%] [G loss: 4.584027]\n",
      "epoch:20 step:16396 [D loss: 0.408835, acc.: 82.81%] [G loss: 3.213221]\n",
      "epoch:20 step:16397 [D loss: 0.515570, acc.: 72.66%] [G loss: 5.423659]\n",
      "epoch:20 step:16398 [D loss: 0.427837, acc.: 85.16%] [G loss: 3.500970]\n",
      "epoch:20 step:16399 [D loss: 0.387646, acc.: 82.81%] [G loss: 3.205678]\n",
      "epoch:20 step:16400 [D loss: 0.369713, acc.: 83.59%] [G loss: 3.036822]\n",
      "epoch:20 step:16401 [D loss: 0.403239, acc.: 79.69%] [G loss: 3.564949]\n",
      "epoch:21 step:16402 [D loss: 0.267834, acc.: 89.06%] [G loss: 3.458341]\n",
      "epoch:21 step:16403 [D loss: 0.250335, acc.: 90.62%] [G loss: 2.695463]\n",
      "epoch:21 step:16404 [D loss: 0.395056, acc.: 82.81%] [G loss: 2.376830]\n",
      "epoch:21 step:16405 [D loss: 0.258119, acc.: 91.41%] [G loss: 2.785201]\n",
      "epoch:21 step:16406 [D loss: 0.366380, acc.: 83.59%] [G loss: 2.614364]\n",
      "epoch:21 step:16407 [D loss: 0.387904, acc.: 85.16%] [G loss: 3.050462]\n",
      "epoch:21 step:16408 [D loss: 0.298055, acc.: 86.72%] [G loss: 2.445955]\n",
      "epoch:21 step:16409 [D loss: 0.311443, acc.: 88.28%] [G loss: 2.476083]\n",
      "epoch:21 step:16410 [D loss: 0.290918, acc.: 86.72%] [G loss: 2.928920]\n",
      "epoch:21 step:16411 [D loss: 0.377055, acc.: 82.03%] [G loss: 2.936692]\n",
      "epoch:21 step:16412 [D loss: 0.378763, acc.: 82.81%] [G loss: 2.717333]\n",
      "epoch:21 step:16413 [D loss: 0.311776, acc.: 87.50%] [G loss: 2.740438]\n",
      "epoch:21 step:16414 [D loss: 0.429201, acc.: 81.25%] [G loss: 2.880507]\n",
      "epoch:21 step:16415 [D loss: 0.370703, acc.: 80.47%] [G loss: 2.620790]\n",
      "epoch:21 step:16416 [D loss: 0.250801, acc.: 87.50%] [G loss: 3.152035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16417 [D loss: 0.338409, acc.: 85.94%] [G loss: 3.022244]\n",
      "epoch:21 step:16418 [D loss: 0.315385, acc.: 85.94%] [G loss: 3.084107]\n",
      "epoch:21 step:16419 [D loss: 0.360276, acc.: 82.81%] [G loss: 2.921554]\n",
      "epoch:21 step:16420 [D loss: 0.363970, acc.: 82.03%] [G loss: 2.980760]\n",
      "epoch:21 step:16421 [D loss: 0.291158, acc.: 88.28%] [G loss: 2.641265]\n",
      "epoch:21 step:16422 [D loss: 0.324645, acc.: 86.72%] [G loss: 2.565173]\n",
      "epoch:21 step:16423 [D loss: 0.302248, acc.: 87.50%] [G loss: 3.196636]\n",
      "epoch:21 step:16424 [D loss: 0.277699, acc.: 85.94%] [G loss: 4.100528]\n",
      "epoch:21 step:16425 [D loss: 0.359042, acc.: 85.16%] [G loss: 2.638629]\n",
      "epoch:21 step:16426 [D loss: 0.286167, acc.: 90.62%] [G loss: 5.026144]\n",
      "epoch:21 step:16427 [D loss: 0.243149, acc.: 90.62%] [G loss: 3.898117]\n",
      "epoch:21 step:16428 [D loss: 0.225117, acc.: 89.84%] [G loss: 3.330817]\n",
      "epoch:21 step:16429 [D loss: 0.312179, acc.: 81.25%] [G loss: 2.961420]\n",
      "epoch:21 step:16430 [D loss: 0.259937, acc.: 89.06%] [G loss: 4.285940]\n",
      "epoch:21 step:16431 [D loss: 0.266231, acc.: 88.28%] [G loss: 3.417593]\n",
      "epoch:21 step:16432 [D loss: 0.348028, acc.: 82.81%] [G loss: 4.155849]\n",
      "epoch:21 step:16433 [D loss: 0.320804, acc.: 84.38%] [G loss: 2.997709]\n",
      "epoch:21 step:16434 [D loss: 0.173378, acc.: 95.31%] [G loss: 2.658827]\n",
      "epoch:21 step:16435 [D loss: 0.394585, acc.: 84.38%] [G loss: 2.808302]\n",
      "epoch:21 step:16436 [D loss: 0.298829, acc.: 89.06%] [G loss: 2.653968]\n",
      "epoch:21 step:16437 [D loss: 0.291104, acc.: 84.38%] [G loss: 3.125608]\n",
      "epoch:21 step:16438 [D loss: 0.352241, acc.: 82.81%] [G loss: 2.944387]\n",
      "epoch:21 step:16439 [D loss: 0.329452, acc.: 82.03%] [G loss: 3.608469]\n",
      "epoch:21 step:16440 [D loss: 0.273077, acc.: 85.94%] [G loss: 3.280280]\n",
      "epoch:21 step:16441 [D loss: 0.260221, acc.: 87.50%] [G loss: 3.525928]\n",
      "epoch:21 step:16442 [D loss: 0.319834, acc.: 87.50%] [G loss: 3.021945]\n",
      "epoch:21 step:16443 [D loss: 0.267891, acc.: 89.84%] [G loss: 3.152536]\n",
      "epoch:21 step:16444 [D loss: 0.314410, acc.: 85.16%] [G loss: 3.802192]\n",
      "epoch:21 step:16445 [D loss: 0.236970, acc.: 90.62%] [G loss: 3.258444]\n",
      "epoch:21 step:16446 [D loss: 0.265540, acc.: 88.28%] [G loss: 2.839436]\n",
      "epoch:21 step:16447 [D loss: 0.308712, acc.: 85.16%] [G loss: 3.863014]\n",
      "epoch:21 step:16448 [D loss: 0.265411, acc.: 88.28%] [G loss: 3.322502]\n",
      "epoch:21 step:16449 [D loss: 0.362279, acc.: 82.81%] [G loss: 4.537165]\n",
      "epoch:21 step:16450 [D loss: 0.193691, acc.: 89.84%] [G loss: 5.058571]\n",
      "epoch:21 step:16451 [D loss: 0.240842, acc.: 90.62%] [G loss: 6.609542]\n",
      "epoch:21 step:16452 [D loss: 0.205951, acc.: 91.41%] [G loss: 7.424212]\n",
      "epoch:21 step:16453 [D loss: 0.201626, acc.: 93.75%] [G loss: 8.504662]\n",
      "epoch:21 step:16454 [D loss: 0.207660, acc.: 91.41%] [G loss: 4.301443]\n",
      "epoch:21 step:16455 [D loss: 0.193113, acc.: 93.75%] [G loss: 5.630573]\n",
      "epoch:21 step:16456 [D loss: 0.241902, acc.: 89.84%] [G loss: 3.826888]\n",
      "epoch:21 step:16457 [D loss: 0.219394, acc.: 92.19%] [G loss: 3.601619]\n",
      "epoch:21 step:16458 [D loss: 0.294941, acc.: 87.50%] [G loss: 2.547099]\n",
      "epoch:21 step:16459 [D loss: 0.331778, acc.: 84.38%] [G loss: 2.475918]\n",
      "epoch:21 step:16460 [D loss: 0.443512, acc.: 82.03%] [G loss: 2.641906]\n",
      "epoch:21 step:16461 [D loss: 0.256715, acc.: 91.41%] [G loss: 2.554286]\n",
      "epoch:21 step:16462 [D loss: 0.300477, acc.: 85.94%] [G loss: 2.647129]\n",
      "epoch:21 step:16463 [D loss: 0.341696, acc.: 86.72%] [G loss: 2.542336]\n",
      "epoch:21 step:16464 [D loss: 0.286584, acc.: 85.16%] [G loss: 2.977687]\n",
      "epoch:21 step:16465 [D loss: 0.347762, acc.: 88.28%] [G loss: 2.561230]\n",
      "epoch:21 step:16466 [D loss: 0.362429, acc.: 86.72%] [G loss: 3.262009]\n",
      "epoch:21 step:16467 [D loss: 0.380243, acc.: 83.59%] [G loss: 3.582359]\n",
      "epoch:21 step:16468 [D loss: 0.392351, acc.: 81.25%] [G loss: 2.787246]\n",
      "epoch:21 step:16469 [D loss: 0.364399, acc.: 81.25%] [G loss: 2.955587]\n",
      "epoch:21 step:16470 [D loss: 0.329598, acc.: 83.59%] [G loss: 2.559464]\n",
      "epoch:21 step:16471 [D loss: 0.334402, acc.: 83.59%] [G loss: 3.002839]\n",
      "epoch:21 step:16472 [D loss: 0.400107, acc.: 82.81%] [G loss: 3.114269]\n",
      "epoch:21 step:16473 [D loss: 0.305477, acc.: 90.62%] [G loss: 7.561256]\n",
      "epoch:21 step:16474 [D loss: 0.674785, acc.: 69.53%] [G loss: 7.579393]\n",
      "epoch:21 step:16475 [D loss: 1.561921, acc.: 62.50%] [G loss: 10.414062]\n",
      "epoch:21 step:16476 [D loss: 2.435418, acc.: 67.19%] [G loss: 7.225008]\n",
      "epoch:21 step:16477 [D loss: 3.462782, acc.: 47.66%] [G loss: 9.364223]\n",
      "epoch:21 step:16478 [D loss: 3.296827, acc.: 43.75%] [G loss: 2.656879]\n",
      "epoch:21 step:16479 [D loss: 0.527142, acc.: 78.12%] [G loss: 4.559724]\n",
      "epoch:21 step:16480 [D loss: 0.555941, acc.: 82.81%] [G loss: 5.165279]\n",
      "epoch:21 step:16481 [D loss: 0.340263, acc.: 84.38%] [G loss: 3.681609]\n",
      "epoch:21 step:16482 [D loss: 0.403477, acc.: 79.69%] [G loss: 3.342985]\n",
      "epoch:21 step:16483 [D loss: 0.349635, acc.: 82.81%] [G loss: 2.860239]\n",
      "epoch:21 step:16484 [D loss: 0.459427, acc.: 76.56%] [G loss: 2.772169]\n",
      "epoch:21 step:16485 [D loss: 0.428903, acc.: 82.81%] [G loss: 2.854318]\n",
      "epoch:21 step:16486 [D loss: 0.321965, acc.: 85.16%] [G loss: 2.409111]\n",
      "epoch:21 step:16487 [D loss: 0.455725, acc.: 78.91%] [G loss: 2.458312]\n",
      "epoch:21 step:16488 [D loss: 0.330254, acc.: 85.94%] [G loss: 2.100553]\n",
      "epoch:21 step:16489 [D loss: 0.358712, acc.: 85.16%] [G loss: 3.098314]\n",
      "epoch:21 step:16490 [D loss: 0.381741, acc.: 82.03%] [G loss: 2.119850]\n",
      "epoch:21 step:16491 [D loss: 0.468332, acc.: 75.78%] [G loss: 2.936889]\n",
      "epoch:21 step:16492 [D loss: 0.282713, acc.: 86.72%] [G loss: 2.377245]\n",
      "epoch:21 step:16493 [D loss: 0.360543, acc.: 84.38%] [G loss: 2.542242]\n",
      "epoch:21 step:16494 [D loss: 0.336979, acc.: 86.72%] [G loss: 2.258383]\n",
      "epoch:21 step:16495 [D loss: 0.550345, acc.: 78.12%] [G loss: 2.214447]\n",
      "epoch:21 step:16496 [D loss: 0.348688, acc.: 82.03%] [G loss: 2.230245]\n",
      "epoch:21 step:16497 [D loss: 0.363638, acc.: 86.72%] [G loss: 2.795853]\n",
      "epoch:21 step:16498 [D loss: 0.403544, acc.: 85.16%] [G loss: 2.896721]\n",
      "epoch:21 step:16499 [D loss: 0.366153, acc.: 83.59%] [G loss: 2.894168]\n",
      "epoch:21 step:16500 [D loss: 0.326216, acc.: 85.94%] [G loss: 2.560258]\n",
      "epoch:21 step:16501 [D loss: 0.324697, acc.: 85.94%] [G loss: 2.211871]\n",
      "epoch:21 step:16502 [D loss: 0.410827, acc.: 80.47%] [G loss: 2.628349]\n",
      "epoch:21 step:16503 [D loss: 0.342000, acc.: 84.38%] [G loss: 2.150901]\n",
      "epoch:21 step:16504 [D loss: 0.411225, acc.: 81.25%] [G loss: 2.922521]\n",
      "epoch:21 step:16505 [D loss: 0.260298, acc.: 86.72%] [G loss: 2.567933]\n",
      "epoch:21 step:16506 [D loss: 0.341387, acc.: 85.94%] [G loss: 2.179817]\n",
      "epoch:21 step:16507 [D loss: 0.408859, acc.: 81.25%] [G loss: 2.661239]\n",
      "epoch:21 step:16508 [D loss: 0.222598, acc.: 90.62%] [G loss: 3.324307]\n",
      "epoch:21 step:16509 [D loss: 0.345898, acc.: 85.16%] [G loss: 2.766716]\n",
      "epoch:21 step:16510 [D loss: 0.545198, acc.: 77.34%] [G loss: 2.274996]\n",
      "epoch:21 step:16511 [D loss: 0.225390, acc.: 92.19%] [G loss: 2.687397]\n",
      "epoch:21 step:16512 [D loss: 0.368319, acc.: 79.69%] [G loss: 1.872988]\n",
      "epoch:21 step:16513 [D loss: 0.362148, acc.: 85.94%] [G loss: 2.307008]\n",
      "epoch:21 step:16514 [D loss: 0.320799, acc.: 87.50%] [G loss: 2.464151]\n",
      "epoch:21 step:16515 [D loss: 0.370635, acc.: 80.47%] [G loss: 2.598780]\n",
      "epoch:21 step:16516 [D loss: 0.317701, acc.: 85.94%] [G loss: 2.810916]\n",
      "epoch:21 step:16517 [D loss: 0.300907, acc.: 89.06%] [G loss: 2.261806]\n",
      "epoch:21 step:16518 [D loss: 0.335866, acc.: 85.94%] [G loss: 3.140191]\n",
      "epoch:21 step:16519 [D loss: 0.355395, acc.: 85.16%] [G loss: 2.498700]\n",
      "epoch:21 step:16520 [D loss: 0.289103, acc.: 86.72%] [G loss: 2.195684]\n",
      "epoch:21 step:16521 [D loss: 0.339439, acc.: 85.94%] [G loss: 2.936623]\n",
      "epoch:21 step:16522 [D loss: 0.254475, acc.: 87.50%] [G loss: 3.129156]\n",
      "epoch:21 step:16523 [D loss: 0.232554, acc.: 89.06%] [G loss: 6.428820]\n",
      "epoch:21 step:16524 [D loss: 0.215638, acc.: 92.97%] [G loss: 3.268775]\n",
      "epoch:21 step:16525 [D loss: 0.219646, acc.: 90.62%] [G loss: 4.126897]\n",
      "epoch:21 step:16526 [D loss: 0.274552, acc.: 87.50%] [G loss: 4.490612]\n",
      "epoch:21 step:16527 [D loss: 0.261200, acc.: 89.06%] [G loss: 5.225656]\n",
      "epoch:21 step:16528 [D loss: 0.290528, acc.: 88.28%] [G loss: 2.815071]\n",
      "epoch:21 step:16529 [D loss: 0.253485, acc.: 90.62%] [G loss: 3.714622]\n",
      "epoch:21 step:16530 [D loss: 0.250466, acc.: 86.72%] [G loss: 4.903911]\n",
      "epoch:21 step:16531 [D loss: 0.296954, acc.: 85.94%] [G loss: 4.256053]\n",
      "epoch:21 step:16532 [D loss: 0.199371, acc.: 92.97%] [G loss: 5.919092]\n",
      "epoch:21 step:16533 [D loss: 0.326491, acc.: 82.81%] [G loss: 3.425428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16534 [D loss: 0.263042, acc.: 88.28%] [G loss: 3.082506]\n",
      "epoch:21 step:16535 [D loss: 0.313693, acc.: 87.50%] [G loss: 3.298395]\n",
      "epoch:21 step:16536 [D loss: 0.245780, acc.: 90.62%] [G loss: 3.268166]\n",
      "epoch:21 step:16537 [D loss: 0.402683, acc.: 82.03%] [G loss: 2.772313]\n",
      "epoch:21 step:16538 [D loss: 0.392667, acc.: 83.59%] [G loss: 3.093076]\n",
      "epoch:21 step:16539 [D loss: 0.354336, acc.: 87.50%] [G loss: 2.732191]\n",
      "epoch:21 step:16540 [D loss: 0.397029, acc.: 83.59%] [G loss: 2.486326]\n",
      "epoch:21 step:16541 [D loss: 0.429323, acc.: 80.47%] [G loss: 2.602797]\n",
      "epoch:21 step:16542 [D loss: 0.390178, acc.: 82.81%] [G loss: 2.866800]\n",
      "epoch:21 step:16543 [D loss: 0.397075, acc.: 81.25%] [G loss: 2.556439]\n",
      "epoch:21 step:16544 [D loss: 0.396188, acc.: 80.47%] [G loss: 2.875436]\n",
      "epoch:21 step:16545 [D loss: 0.434109, acc.: 81.25%] [G loss: 2.209559]\n",
      "epoch:21 step:16546 [D loss: 0.478882, acc.: 76.56%] [G loss: 2.510880]\n",
      "epoch:21 step:16547 [D loss: 0.363978, acc.: 83.59%] [G loss: 2.488129]\n",
      "epoch:21 step:16548 [D loss: 0.283922, acc.: 84.38%] [G loss: 3.410172]\n",
      "epoch:21 step:16549 [D loss: 0.408119, acc.: 82.03%] [G loss: 2.613903]\n",
      "epoch:21 step:16550 [D loss: 0.370484, acc.: 77.34%] [G loss: 4.888638]\n",
      "epoch:21 step:16551 [D loss: 0.175750, acc.: 90.62%] [G loss: 10.029664]\n",
      "epoch:21 step:16552 [D loss: 0.140287, acc.: 96.88%] [G loss: 12.661634]\n",
      "epoch:21 step:16553 [D loss: 0.139573, acc.: 93.75%] [G loss: 13.602823]\n",
      "epoch:21 step:16554 [D loss: 0.129167, acc.: 94.53%] [G loss: 9.192503]\n",
      "epoch:21 step:16555 [D loss: 0.152498, acc.: 92.97%] [G loss: 8.350481]\n",
      "epoch:21 step:16556 [D loss: 0.225524, acc.: 90.62%] [G loss: 9.138722]\n",
      "epoch:21 step:16557 [D loss: 0.201113, acc.: 92.19%] [G loss: 9.526390]\n",
      "epoch:21 step:16558 [D loss: 0.182310, acc.: 93.75%] [G loss: 8.364212]\n",
      "epoch:21 step:16559 [D loss: 0.194210, acc.: 91.41%] [G loss: 6.532729]\n",
      "epoch:21 step:16560 [D loss: 0.257909, acc.: 89.84%] [G loss: 5.345612]\n",
      "epoch:21 step:16561 [D loss: 0.385329, acc.: 80.47%] [G loss: 4.346828]\n",
      "epoch:21 step:16562 [D loss: 0.403207, acc.: 82.03%] [G loss: 3.309982]\n",
      "epoch:21 step:16563 [D loss: 0.282720, acc.: 86.72%] [G loss: 3.813553]\n",
      "epoch:21 step:16564 [D loss: 0.348118, acc.: 83.59%] [G loss: 2.740509]\n",
      "epoch:21 step:16565 [D loss: 0.254656, acc.: 92.97%] [G loss: 3.067021]\n",
      "epoch:21 step:16566 [D loss: 0.342467, acc.: 82.81%] [G loss: 2.997132]\n",
      "epoch:21 step:16567 [D loss: 0.322808, acc.: 84.38%] [G loss: 3.095504]\n",
      "epoch:21 step:16568 [D loss: 0.395904, acc.: 78.12%] [G loss: 3.065675]\n",
      "epoch:21 step:16569 [D loss: 0.351738, acc.: 85.94%] [G loss: 3.162936]\n",
      "epoch:21 step:16570 [D loss: 0.361440, acc.: 85.16%] [G loss: 2.933628]\n",
      "epoch:21 step:16571 [D loss: 0.275138, acc.: 88.28%] [G loss: 2.778955]\n",
      "epoch:21 step:16572 [D loss: 0.442720, acc.: 78.91%] [G loss: 2.445158]\n",
      "epoch:21 step:16573 [D loss: 0.306585, acc.: 84.38%] [G loss: 2.653369]\n",
      "epoch:21 step:16574 [D loss: 0.293710, acc.: 86.72%] [G loss: 3.459857]\n",
      "epoch:21 step:16575 [D loss: 0.379097, acc.: 79.69%] [G loss: 2.414841]\n",
      "epoch:21 step:16576 [D loss: 0.462602, acc.: 80.47%] [G loss: 2.668158]\n",
      "epoch:21 step:16577 [D loss: 0.380878, acc.: 86.72%] [G loss: 3.124166]\n",
      "epoch:21 step:16578 [D loss: 0.344837, acc.: 80.47%] [G loss: 3.349978]\n",
      "epoch:21 step:16579 [D loss: 0.444770, acc.: 78.12%] [G loss: 4.021963]\n",
      "epoch:21 step:16580 [D loss: 0.263214, acc.: 87.50%] [G loss: 3.008010]\n",
      "epoch:21 step:16581 [D loss: 0.340424, acc.: 82.81%] [G loss: 3.391287]\n",
      "epoch:21 step:16582 [D loss: 0.249414, acc.: 90.62%] [G loss: 3.435687]\n",
      "epoch:21 step:16583 [D loss: 0.319780, acc.: 87.50%] [G loss: 3.341751]\n",
      "epoch:21 step:16584 [D loss: 0.345922, acc.: 86.72%] [G loss: 2.283297]\n",
      "epoch:21 step:16585 [D loss: 0.400045, acc.: 80.47%] [G loss: 2.824104]\n",
      "epoch:21 step:16586 [D loss: 0.228283, acc.: 92.19%] [G loss: 2.455456]\n",
      "epoch:21 step:16587 [D loss: 0.320676, acc.: 85.16%] [G loss: 3.602961]\n",
      "epoch:21 step:16588 [D loss: 0.267846, acc.: 87.50%] [G loss: 5.570436]\n",
      "epoch:21 step:16589 [D loss: 0.344130, acc.: 85.94%] [G loss: 3.385757]\n",
      "epoch:21 step:16590 [D loss: 0.266388, acc.: 89.84%] [G loss: 2.588816]\n",
      "epoch:21 step:16591 [D loss: 0.323972, acc.: 87.50%] [G loss: 2.803298]\n",
      "epoch:21 step:16592 [D loss: 0.330309, acc.: 83.59%] [G loss: 3.219559]\n",
      "epoch:21 step:16593 [D loss: 0.299578, acc.: 88.28%] [G loss: 3.249261]\n",
      "epoch:21 step:16594 [D loss: 0.350682, acc.: 85.94%] [G loss: 2.634789]\n",
      "epoch:21 step:16595 [D loss: 0.190169, acc.: 92.19%] [G loss: 3.710328]\n",
      "epoch:21 step:16596 [D loss: 0.240001, acc.: 90.62%] [G loss: 4.347422]\n",
      "epoch:21 step:16597 [D loss: 0.253185, acc.: 85.94%] [G loss: 3.239785]\n",
      "epoch:21 step:16598 [D loss: 0.348777, acc.: 86.72%] [G loss: 3.368560]\n",
      "epoch:21 step:16599 [D loss: 0.320658, acc.: 84.38%] [G loss: 3.236573]\n",
      "epoch:21 step:16600 [D loss: 0.255703, acc.: 85.94%] [G loss: 3.964169]\n",
      "epoch:21 step:16601 [D loss: 0.280430, acc.: 89.06%] [G loss: 4.033152]\n",
      "epoch:21 step:16602 [D loss: 0.366913, acc.: 82.81%] [G loss: 2.485829]\n",
      "epoch:21 step:16603 [D loss: 0.301409, acc.: 82.03%] [G loss: 2.719037]\n",
      "epoch:21 step:16604 [D loss: 0.336189, acc.: 83.59%] [G loss: 2.585432]\n",
      "epoch:21 step:16605 [D loss: 0.351987, acc.: 85.16%] [G loss: 4.732613]\n",
      "epoch:21 step:16606 [D loss: 0.392573, acc.: 85.16%] [G loss: 3.266705]\n",
      "epoch:21 step:16607 [D loss: 0.392759, acc.: 83.59%] [G loss: 3.207768]\n",
      "epoch:21 step:16608 [D loss: 0.319240, acc.: 84.38%] [G loss: 2.582845]\n",
      "epoch:21 step:16609 [D loss: 0.436333, acc.: 82.81%] [G loss: 3.225654]\n",
      "epoch:21 step:16610 [D loss: 0.398273, acc.: 85.94%] [G loss: 2.388251]\n",
      "epoch:21 step:16611 [D loss: 0.271619, acc.: 86.72%] [G loss: 3.389418]\n",
      "epoch:21 step:16612 [D loss: 0.398848, acc.: 82.81%] [G loss: 3.241708]\n",
      "epoch:21 step:16613 [D loss: 0.314241, acc.: 82.81%] [G loss: 3.203651]\n",
      "epoch:21 step:16614 [D loss: 0.224758, acc.: 91.41%] [G loss: 2.915397]\n",
      "epoch:21 step:16615 [D loss: 0.377239, acc.: 85.16%] [G loss: 3.262671]\n",
      "epoch:21 step:16616 [D loss: 0.428719, acc.: 80.47%] [G loss: 2.356307]\n",
      "epoch:21 step:16617 [D loss: 0.368761, acc.: 83.59%] [G loss: 2.774855]\n",
      "epoch:21 step:16618 [D loss: 0.320725, acc.: 81.25%] [G loss: 3.240926]\n",
      "epoch:21 step:16619 [D loss: 0.255993, acc.: 91.41%] [G loss: 3.212553]\n",
      "epoch:21 step:16620 [D loss: 0.303817, acc.: 82.81%] [G loss: 3.377638]\n",
      "epoch:21 step:16621 [D loss: 0.356256, acc.: 85.94%] [G loss: 4.031233]\n",
      "epoch:21 step:16622 [D loss: 0.318809, acc.: 87.50%] [G loss: 4.232737]\n",
      "epoch:21 step:16623 [D loss: 0.272887, acc.: 89.06%] [G loss: 2.823957]\n",
      "epoch:21 step:16624 [D loss: 0.315085, acc.: 85.16%] [G loss: 3.052613]\n",
      "epoch:21 step:16625 [D loss: 0.332552, acc.: 79.69%] [G loss: 2.535630]\n",
      "epoch:21 step:16626 [D loss: 0.332793, acc.: 86.72%] [G loss: 3.127435]\n",
      "epoch:21 step:16627 [D loss: 0.312323, acc.: 85.94%] [G loss: 2.965629]\n",
      "epoch:21 step:16628 [D loss: 0.318760, acc.: 87.50%] [G loss: 3.594904]\n",
      "epoch:21 step:16629 [D loss: 0.367982, acc.: 81.25%] [G loss: 4.517554]\n",
      "epoch:21 step:16630 [D loss: 0.467144, acc.: 81.25%] [G loss: 4.668714]\n",
      "epoch:21 step:16631 [D loss: 0.224858, acc.: 92.97%] [G loss: 4.064914]\n",
      "epoch:21 step:16632 [D loss: 0.236608, acc.: 89.84%] [G loss: 5.049302]\n",
      "epoch:21 step:16633 [D loss: 0.315529, acc.: 85.94%] [G loss: 2.713429]\n",
      "epoch:21 step:16634 [D loss: 0.268683, acc.: 89.06%] [G loss: 5.075752]\n",
      "epoch:21 step:16635 [D loss: 0.220735, acc.: 90.62%] [G loss: 4.780055]\n",
      "epoch:21 step:16636 [D loss: 0.262292, acc.: 89.06%] [G loss: 4.233508]\n",
      "epoch:21 step:16637 [D loss: 0.241142, acc.: 89.84%] [G loss: 4.456167]\n",
      "epoch:21 step:16638 [D loss: 0.322433, acc.: 88.28%] [G loss: 3.873592]\n",
      "epoch:21 step:16639 [D loss: 0.404429, acc.: 82.03%] [G loss: 3.926156]\n",
      "epoch:21 step:16640 [D loss: 0.265658, acc.: 87.50%] [G loss: 4.053673]\n",
      "epoch:21 step:16641 [D loss: 0.384863, acc.: 77.34%] [G loss: 2.984492]\n",
      "epoch:21 step:16642 [D loss: 0.442576, acc.: 78.12%] [G loss: 3.053818]\n",
      "epoch:21 step:16643 [D loss: 0.347583, acc.: 84.38%] [G loss: 3.261299]\n",
      "epoch:21 step:16644 [D loss: 0.256371, acc.: 89.84%] [G loss: 2.792476]\n",
      "epoch:21 step:16645 [D loss: 0.275306, acc.: 89.06%] [G loss: 3.490509]\n",
      "epoch:21 step:16646 [D loss: 0.234140, acc.: 92.97%] [G loss: 3.064593]\n",
      "epoch:21 step:16647 [D loss: 0.284538, acc.: 88.28%] [G loss: 3.411930]\n",
      "epoch:21 step:16648 [D loss: 0.326005, acc.: 87.50%] [G loss: 2.904406]\n",
      "epoch:21 step:16649 [D loss: 0.326515, acc.: 85.16%] [G loss: 4.820296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16650 [D loss: 0.413924, acc.: 78.12%] [G loss: 4.717075]\n",
      "epoch:21 step:16651 [D loss: 0.401124, acc.: 82.81%] [G loss: 3.247788]\n",
      "epoch:21 step:16652 [D loss: 0.324232, acc.: 87.50%] [G loss: 3.657495]\n",
      "epoch:21 step:16653 [D loss: 0.412984, acc.: 81.25%] [G loss: 3.829303]\n",
      "epoch:21 step:16654 [D loss: 0.387249, acc.: 82.03%] [G loss: 3.301438]\n",
      "epoch:21 step:16655 [D loss: 0.365386, acc.: 86.72%] [G loss: 2.874524]\n",
      "epoch:21 step:16656 [D loss: 0.372500, acc.: 82.03%] [G loss: 2.947641]\n",
      "epoch:21 step:16657 [D loss: 0.253090, acc.: 89.84%] [G loss: 4.926223]\n",
      "epoch:21 step:16658 [D loss: 0.368105, acc.: 81.25%] [G loss: 2.700492]\n",
      "epoch:21 step:16659 [D loss: 0.222853, acc.: 90.62%] [G loss: 3.140793]\n",
      "epoch:21 step:16660 [D loss: 0.277896, acc.: 88.28%] [G loss: 3.252864]\n",
      "epoch:21 step:16661 [D loss: 0.306582, acc.: 84.38%] [G loss: 3.250773]\n",
      "epoch:21 step:16662 [D loss: 0.341391, acc.: 85.94%] [G loss: 2.846136]\n",
      "epoch:21 step:16663 [D loss: 0.338753, acc.: 85.16%] [G loss: 4.469779]\n",
      "epoch:21 step:16664 [D loss: 0.363932, acc.: 80.47%] [G loss: 3.941936]\n",
      "epoch:21 step:16665 [D loss: 0.364269, acc.: 83.59%] [G loss: 3.398209]\n",
      "epoch:21 step:16666 [D loss: 0.368800, acc.: 77.34%] [G loss: 5.708727]\n",
      "epoch:21 step:16667 [D loss: 0.466937, acc.: 78.91%] [G loss: 3.427374]\n",
      "epoch:21 step:16668 [D loss: 0.283475, acc.: 88.28%] [G loss: 2.600052]\n",
      "epoch:21 step:16669 [D loss: 0.243478, acc.: 91.41%] [G loss: 2.651435]\n",
      "epoch:21 step:16670 [D loss: 0.392920, acc.: 81.25%] [G loss: 2.689970]\n",
      "epoch:21 step:16671 [D loss: 0.314997, acc.: 85.16%] [G loss: 2.651050]\n",
      "epoch:21 step:16672 [D loss: 0.554902, acc.: 80.47%] [G loss: 3.698619]\n",
      "epoch:21 step:16673 [D loss: 0.340311, acc.: 82.03%] [G loss: 3.496231]\n",
      "epoch:21 step:16674 [D loss: 0.336821, acc.: 85.16%] [G loss: 4.093195]\n",
      "epoch:21 step:16675 [D loss: 0.330337, acc.: 88.28%] [G loss: 2.417351]\n",
      "epoch:21 step:16676 [D loss: 0.376881, acc.: 82.03%] [G loss: 2.758645]\n",
      "epoch:21 step:16677 [D loss: 0.405959, acc.: 79.69%] [G loss: 2.950821]\n",
      "epoch:21 step:16678 [D loss: 0.418758, acc.: 78.12%] [G loss: 2.667364]\n",
      "epoch:21 step:16679 [D loss: 0.391181, acc.: 81.25%] [G loss: 2.261079]\n",
      "epoch:21 step:16680 [D loss: 0.373410, acc.: 84.38%] [G loss: 2.965637]\n",
      "epoch:21 step:16681 [D loss: 0.383244, acc.: 80.47%] [G loss: 3.275789]\n",
      "epoch:21 step:16682 [D loss: 0.392083, acc.: 81.25%] [G loss: 3.082948]\n",
      "epoch:21 step:16683 [D loss: 0.353384, acc.: 83.59%] [G loss: 2.500186]\n",
      "epoch:21 step:16684 [D loss: 0.335615, acc.: 88.28%] [G loss: 4.332360]\n",
      "epoch:21 step:16685 [D loss: 0.393284, acc.: 82.81%] [G loss: 3.455327]\n",
      "epoch:21 step:16686 [D loss: 0.397816, acc.: 83.59%] [G loss: 3.534440]\n",
      "epoch:21 step:16687 [D loss: 0.332597, acc.: 84.38%] [G loss: 3.128068]\n",
      "epoch:21 step:16688 [D loss: 0.242677, acc.: 91.41%] [G loss: 3.099225]\n",
      "epoch:21 step:16689 [D loss: 0.427130, acc.: 78.12%] [G loss: 3.096836]\n",
      "epoch:21 step:16690 [D loss: 0.369896, acc.: 83.59%] [G loss: 4.186697]\n",
      "epoch:21 step:16691 [D loss: 0.403363, acc.: 78.91%] [G loss: 3.190625]\n",
      "epoch:21 step:16692 [D loss: 0.264166, acc.: 92.19%] [G loss: 3.800298]\n",
      "epoch:21 step:16693 [D loss: 0.406600, acc.: 82.03%] [G loss: 4.102856]\n",
      "epoch:21 step:16694 [D loss: 0.291109, acc.: 87.50%] [G loss: 3.580983]\n",
      "epoch:21 step:16695 [D loss: 0.340424, acc.: 84.38%] [G loss: 2.903273]\n",
      "epoch:21 step:16696 [D loss: 0.298761, acc.: 83.59%] [G loss: 3.571989]\n",
      "epoch:21 step:16697 [D loss: 0.276901, acc.: 88.28%] [G loss: 3.398445]\n",
      "epoch:21 step:16698 [D loss: 0.365113, acc.: 82.03%] [G loss: 2.813279]\n",
      "epoch:21 step:16699 [D loss: 0.351507, acc.: 87.50%] [G loss: 2.955378]\n",
      "epoch:21 step:16700 [D loss: 0.311034, acc.: 89.06%] [G loss: 3.593307]\n",
      "epoch:21 step:16701 [D loss: 0.362594, acc.: 80.47%] [G loss: 2.616810]\n",
      "epoch:21 step:16702 [D loss: 0.322046, acc.: 85.16%] [G loss: 3.836491]\n",
      "epoch:21 step:16703 [D loss: 0.399309, acc.: 82.81%] [G loss: 4.231400]\n",
      "epoch:21 step:16704 [D loss: 0.336969, acc.: 84.38%] [G loss: 5.795657]\n",
      "epoch:21 step:16705 [D loss: 0.310935, acc.: 87.50%] [G loss: 4.713861]\n",
      "epoch:21 step:16706 [D loss: 0.197765, acc.: 89.84%] [G loss: 3.713008]\n",
      "epoch:21 step:16707 [D loss: 0.302239, acc.: 88.28%] [G loss: 3.121138]\n",
      "epoch:21 step:16708 [D loss: 0.330802, acc.: 83.59%] [G loss: 3.953628]\n",
      "epoch:21 step:16709 [D loss: 0.258058, acc.: 88.28%] [G loss: 3.483073]\n",
      "epoch:21 step:16710 [D loss: 0.316639, acc.: 87.50%] [G loss: 3.043749]\n",
      "epoch:21 step:16711 [D loss: 0.321143, acc.: 85.16%] [G loss: 3.233856]\n",
      "epoch:21 step:16712 [D loss: 0.319891, acc.: 83.59%] [G loss: 2.821816]\n",
      "epoch:21 step:16713 [D loss: 0.317529, acc.: 86.72%] [G loss: 2.496696]\n",
      "epoch:21 step:16714 [D loss: 0.368869, acc.: 83.59%] [G loss: 2.223116]\n",
      "epoch:21 step:16715 [D loss: 0.404993, acc.: 80.47%] [G loss: 2.286943]\n",
      "epoch:21 step:16716 [D loss: 0.361985, acc.: 83.59%] [G loss: 2.984931]\n",
      "epoch:21 step:16717 [D loss: 0.350665, acc.: 85.16%] [G loss: 3.979743]\n",
      "epoch:21 step:16718 [D loss: 0.393384, acc.: 79.69%] [G loss: 2.450264]\n",
      "epoch:21 step:16719 [D loss: 0.283949, acc.: 89.06%] [G loss: 2.768463]\n",
      "epoch:21 step:16720 [D loss: 0.374866, acc.: 89.06%] [G loss: 2.463328]\n",
      "epoch:21 step:16721 [D loss: 0.407214, acc.: 80.47%] [G loss: 2.679036]\n",
      "epoch:21 step:16722 [D loss: 0.336305, acc.: 84.38%] [G loss: 2.749156]\n",
      "epoch:21 step:16723 [D loss: 0.292446, acc.: 85.16%] [G loss: 4.471278]\n",
      "epoch:21 step:16724 [D loss: 0.342895, acc.: 84.38%] [G loss: 2.806338]\n",
      "epoch:21 step:16725 [D loss: 0.382138, acc.: 84.38%] [G loss: 3.381245]\n",
      "epoch:21 step:16726 [D loss: 0.385878, acc.: 83.59%] [G loss: 4.080221]\n",
      "epoch:21 step:16727 [D loss: 0.274739, acc.: 89.06%] [G loss: 2.498987]\n",
      "epoch:21 step:16728 [D loss: 0.326160, acc.: 85.16%] [G loss: 4.277286]\n",
      "epoch:21 step:16729 [D loss: 0.305229, acc.: 88.28%] [G loss: 3.008929]\n",
      "epoch:21 step:16730 [D loss: 0.258937, acc.: 89.84%] [G loss: 3.350608]\n",
      "epoch:21 step:16731 [D loss: 0.274893, acc.: 91.41%] [G loss: 3.109084]\n",
      "epoch:21 step:16732 [D loss: 0.351914, acc.: 84.38%] [G loss: 4.731757]\n",
      "epoch:21 step:16733 [D loss: 0.508894, acc.: 76.56%] [G loss: 5.218754]\n",
      "epoch:21 step:16734 [D loss: 0.480377, acc.: 81.25%] [G loss: 3.013497]\n",
      "epoch:21 step:16735 [D loss: 0.384589, acc.: 80.47%] [G loss: 4.539198]\n",
      "epoch:21 step:16736 [D loss: 0.273316, acc.: 89.84%] [G loss: 2.764361]\n",
      "epoch:21 step:16737 [D loss: 0.353736, acc.: 82.03%] [G loss: 4.766439]\n",
      "epoch:21 step:16738 [D loss: 0.260211, acc.: 89.06%] [G loss: 5.770356]\n",
      "epoch:21 step:16739 [D loss: 0.312006, acc.: 87.50%] [G loss: 4.266372]\n",
      "epoch:21 step:16740 [D loss: 0.229238, acc.: 89.84%] [G loss: 4.718170]\n",
      "epoch:21 step:16741 [D loss: 0.317441, acc.: 89.06%] [G loss: 4.806374]\n",
      "epoch:21 step:16742 [D loss: 0.235409, acc.: 88.28%] [G loss: 3.132583]\n",
      "epoch:21 step:16743 [D loss: 0.266851, acc.: 88.28%] [G loss: 4.377797]\n",
      "epoch:21 step:16744 [D loss: 0.258207, acc.: 88.28%] [G loss: 4.334572]\n",
      "epoch:21 step:16745 [D loss: 0.295754, acc.: 86.72%] [G loss: 2.787365]\n",
      "epoch:21 step:16746 [D loss: 0.282218, acc.: 88.28%] [G loss: 2.942928]\n",
      "epoch:21 step:16747 [D loss: 0.267273, acc.: 85.94%] [G loss: 2.984440]\n",
      "epoch:21 step:16748 [D loss: 0.397766, acc.: 79.69%] [G loss: 2.662563]\n",
      "epoch:21 step:16749 [D loss: 0.307466, acc.: 87.50%] [G loss: 3.070396]\n",
      "epoch:21 step:16750 [D loss: 0.323806, acc.: 84.38%] [G loss: 4.171314]\n",
      "epoch:21 step:16751 [D loss: 0.368091, acc.: 81.25%] [G loss: 2.794819]\n",
      "epoch:21 step:16752 [D loss: 0.271050, acc.: 85.16%] [G loss: 3.831326]\n",
      "epoch:21 step:16753 [D loss: 0.316204, acc.: 86.72%] [G loss: 2.778606]\n",
      "epoch:21 step:16754 [D loss: 0.306231, acc.: 87.50%] [G loss: 3.358740]\n",
      "epoch:21 step:16755 [D loss: 0.291820, acc.: 86.72%] [G loss: 3.067737]\n",
      "epoch:21 step:16756 [D loss: 0.227127, acc.: 90.62%] [G loss: 5.115171]\n",
      "epoch:21 step:16757 [D loss: 0.311051, acc.: 85.16%] [G loss: 3.107943]\n",
      "epoch:21 step:16758 [D loss: 0.232951, acc.: 89.84%] [G loss: 4.755072]\n",
      "epoch:21 step:16759 [D loss: 0.406559, acc.: 81.25%] [G loss: 2.931320]\n",
      "epoch:21 step:16760 [D loss: 0.211262, acc.: 91.41%] [G loss: 3.933928]\n",
      "epoch:21 step:16761 [D loss: 0.301987, acc.: 88.28%] [G loss: 2.709393]\n",
      "epoch:21 step:16762 [D loss: 0.415455, acc.: 78.91%] [G loss: 2.874339]\n",
      "epoch:21 step:16763 [D loss: 0.396313, acc.: 83.59%] [G loss: 2.590213]\n",
      "epoch:21 step:16764 [D loss: 0.338733, acc.: 82.03%] [G loss: 3.307471]\n",
      "epoch:21 step:16765 [D loss: 0.394671, acc.: 84.38%] [G loss: 2.970809]\n",
      "epoch:21 step:16766 [D loss: 0.318072, acc.: 87.50%] [G loss: 2.995120]\n",
      "epoch:21 step:16767 [D loss: 0.302432, acc.: 88.28%] [G loss: 4.322937]\n",
      "epoch:21 step:16768 [D loss: 0.322816, acc.: 85.94%] [G loss: 2.402419]\n",
      "epoch:21 step:16769 [D loss: 0.314868, acc.: 88.28%] [G loss: 2.748689]\n",
      "epoch:21 step:16770 [D loss: 0.234878, acc.: 90.62%] [G loss: 2.616206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16771 [D loss: 0.291863, acc.: 87.50%] [G loss: 2.898648]\n",
      "epoch:21 step:16772 [D loss: 0.270982, acc.: 90.62%] [G loss: 2.905905]\n",
      "epoch:21 step:16773 [D loss: 0.275082, acc.: 89.84%] [G loss: 3.324826]\n",
      "epoch:21 step:16774 [D loss: 0.337029, acc.: 84.38%] [G loss: 3.186094]\n",
      "epoch:21 step:16775 [D loss: 0.306041, acc.: 89.06%] [G loss: 3.309610]\n",
      "epoch:21 step:16776 [D loss: 0.268619, acc.: 86.72%] [G loss: 2.537705]\n",
      "epoch:21 step:16777 [D loss: 0.221799, acc.: 92.97%] [G loss: 2.918890]\n",
      "epoch:21 step:16778 [D loss: 0.198269, acc.: 94.53%] [G loss: 4.967329]\n",
      "epoch:21 step:16779 [D loss: 0.338850, acc.: 84.38%] [G loss: 3.556375]\n",
      "epoch:21 step:16780 [D loss: 0.279334, acc.: 86.72%] [G loss: 3.378059]\n",
      "epoch:21 step:16781 [D loss: 0.317921, acc.: 85.94%] [G loss: 4.089286]\n",
      "epoch:21 step:16782 [D loss: 0.216444, acc.: 92.19%] [G loss: 4.837216]\n",
      "epoch:21 step:16783 [D loss: 0.341510, acc.: 85.94%] [G loss: 3.722114]\n",
      "epoch:21 step:16784 [D loss: 0.283400, acc.: 88.28%] [G loss: 3.539367]\n",
      "epoch:21 step:16785 [D loss: 0.297375, acc.: 89.06%] [G loss: 4.069562]\n",
      "epoch:21 step:16786 [D loss: 0.273632, acc.: 90.62%] [G loss: 3.834837]\n",
      "epoch:21 step:16787 [D loss: 0.265575, acc.: 91.41%] [G loss: 4.547530]\n",
      "epoch:21 step:16788 [D loss: 0.253749, acc.: 89.06%] [G loss: 3.696036]\n",
      "epoch:21 step:16789 [D loss: 0.366307, acc.: 80.47%] [G loss: 5.247878]\n",
      "epoch:21 step:16790 [D loss: 0.238712, acc.: 87.50%] [G loss: 6.575526]\n",
      "epoch:21 step:16791 [D loss: 0.252210, acc.: 87.50%] [G loss: 5.295654]\n",
      "epoch:21 step:16792 [D loss: 0.324273, acc.: 83.59%] [G loss: 5.123508]\n",
      "epoch:21 step:16793 [D loss: 0.290655, acc.: 85.16%] [G loss: 5.336420]\n",
      "epoch:21 step:16794 [D loss: 0.200138, acc.: 92.97%] [G loss: 4.373192]\n",
      "epoch:21 step:16795 [D loss: 0.333719, acc.: 82.03%] [G loss: 4.411350]\n",
      "epoch:21 step:16796 [D loss: 0.272638, acc.: 89.84%] [G loss: 3.156223]\n",
      "epoch:21 step:16797 [D loss: 0.315891, acc.: 88.28%] [G loss: 4.230196]\n",
      "epoch:21 step:16798 [D loss: 0.310976, acc.: 86.72%] [G loss: 3.512688]\n",
      "epoch:21 step:16799 [D loss: 0.335928, acc.: 82.03%] [G loss: 3.739333]\n",
      "epoch:21 step:16800 [D loss: 0.260917, acc.: 88.28%] [G loss: 2.773238]\n",
      "epoch:21 step:16801 [D loss: 0.220559, acc.: 92.19%] [G loss: 3.239776]\n",
      "epoch:21 step:16802 [D loss: 0.352617, acc.: 82.81%] [G loss: 2.937001]\n",
      "epoch:21 step:16803 [D loss: 0.266573, acc.: 88.28%] [G loss: 2.495055]\n",
      "epoch:21 step:16804 [D loss: 0.265955, acc.: 88.28%] [G loss: 3.225934]\n",
      "epoch:21 step:16805 [D loss: 0.232666, acc.: 92.19%] [G loss: 3.382486]\n",
      "epoch:21 step:16806 [D loss: 0.309417, acc.: 85.16%] [G loss: 3.301150]\n",
      "epoch:21 step:16807 [D loss: 0.322491, acc.: 89.06%] [G loss: 2.459441]\n",
      "epoch:21 step:16808 [D loss: 0.299911, acc.: 88.28%] [G loss: 3.108083]\n",
      "epoch:21 step:16809 [D loss: 0.374813, acc.: 78.91%] [G loss: 3.017910]\n",
      "epoch:21 step:16810 [D loss: 0.286124, acc.: 86.72%] [G loss: 3.243022]\n",
      "epoch:21 step:16811 [D loss: 0.304138, acc.: 89.06%] [G loss: 3.841143]\n",
      "epoch:21 step:16812 [D loss: 0.287991, acc.: 87.50%] [G loss: 3.386314]\n",
      "epoch:21 step:16813 [D loss: 0.362437, acc.: 84.38%] [G loss: 4.400766]\n",
      "epoch:21 step:16814 [D loss: 0.379505, acc.: 82.03%] [G loss: 2.856182]\n",
      "epoch:21 step:16815 [D loss: 0.254288, acc.: 91.41%] [G loss: 4.481208]\n",
      "epoch:21 step:16816 [D loss: 0.407743, acc.: 78.91%] [G loss: 3.807726]\n",
      "epoch:21 step:16817 [D loss: 0.352686, acc.: 81.25%] [G loss: 5.751075]\n",
      "epoch:21 step:16818 [D loss: 0.361060, acc.: 84.38%] [G loss: 3.302210]\n",
      "epoch:21 step:16819 [D loss: 0.270151, acc.: 86.72%] [G loss: 4.243098]\n",
      "epoch:21 step:16820 [D loss: 0.328778, acc.: 85.16%] [G loss: 3.476096]\n",
      "epoch:21 step:16821 [D loss: 0.294314, acc.: 86.72%] [G loss: 4.045409]\n",
      "epoch:21 step:16822 [D loss: 0.236634, acc.: 94.53%] [G loss: 5.021808]\n",
      "epoch:21 step:16823 [D loss: 0.262240, acc.: 85.94%] [G loss: 3.562529]\n",
      "epoch:21 step:16824 [D loss: 0.241648, acc.: 87.50%] [G loss: 6.125604]\n",
      "epoch:21 step:16825 [D loss: 0.292763, acc.: 86.72%] [G loss: 3.769404]\n",
      "epoch:21 step:16826 [D loss: 0.362144, acc.: 85.94%] [G loss: 2.927425]\n",
      "epoch:21 step:16827 [D loss: 0.277388, acc.: 89.06%] [G loss: 2.624580]\n",
      "epoch:21 step:16828 [D loss: 0.293239, acc.: 89.84%] [G loss: 2.618929]\n",
      "epoch:21 step:16829 [D loss: 0.518023, acc.: 72.66%] [G loss: 2.855338]\n",
      "epoch:21 step:16830 [D loss: 0.356725, acc.: 85.94%] [G loss: 4.427437]\n",
      "epoch:21 step:16831 [D loss: 0.496740, acc.: 72.66%] [G loss: 2.937557]\n",
      "epoch:21 step:16832 [D loss: 0.295769, acc.: 88.28%] [G loss: 3.051877]\n",
      "epoch:21 step:16833 [D loss: 0.503164, acc.: 81.25%] [G loss: 5.190499]\n",
      "epoch:21 step:16834 [D loss: 0.428081, acc.: 82.81%] [G loss: 3.915361]\n",
      "epoch:21 step:16835 [D loss: 0.328542, acc.: 84.38%] [G loss: 3.158127]\n",
      "epoch:21 step:16836 [D loss: 0.317339, acc.: 85.16%] [G loss: 3.624164]\n",
      "epoch:21 step:16837 [D loss: 0.500494, acc.: 75.78%] [G loss: 3.472322]\n",
      "epoch:21 step:16838 [D loss: 0.287788, acc.: 90.62%] [G loss: 3.571654]\n",
      "epoch:21 step:16839 [D loss: 0.328254, acc.: 86.72%] [G loss: 3.659598]\n",
      "epoch:21 step:16840 [D loss: 0.317173, acc.: 89.06%] [G loss: 3.133828]\n",
      "epoch:21 step:16841 [D loss: 0.385255, acc.: 84.38%] [G loss: 3.577883]\n",
      "epoch:21 step:16842 [D loss: 0.251125, acc.: 89.84%] [G loss: 3.339343]\n",
      "epoch:21 step:16843 [D loss: 0.249797, acc.: 88.28%] [G loss: 3.091437]\n",
      "epoch:21 step:16844 [D loss: 0.401876, acc.: 77.34%] [G loss: 3.465495]\n",
      "epoch:21 step:16845 [D loss: 0.381161, acc.: 85.16%] [G loss: 3.732159]\n",
      "epoch:21 step:16846 [D loss: 0.582312, acc.: 73.44%] [G loss: 3.316607]\n",
      "epoch:21 step:16847 [D loss: 0.420655, acc.: 80.47%] [G loss: 3.849031]\n",
      "epoch:21 step:16848 [D loss: 0.337660, acc.: 86.72%] [G loss: 3.963905]\n",
      "epoch:21 step:16849 [D loss: 0.254437, acc.: 89.84%] [G loss: 3.262327]\n",
      "epoch:21 step:16850 [D loss: 0.317379, acc.: 85.16%] [G loss: 3.137040]\n",
      "epoch:21 step:16851 [D loss: 0.501477, acc.: 80.47%] [G loss: 2.347860]\n",
      "epoch:21 step:16852 [D loss: 0.329185, acc.: 91.41%] [G loss: 2.443819]\n",
      "epoch:21 step:16853 [D loss: 0.427457, acc.: 76.56%] [G loss: 2.840046]\n",
      "epoch:21 step:16854 [D loss: 0.356697, acc.: 83.59%] [G loss: 3.238650]\n",
      "epoch:21 step:16855 [D loss: 0.243746, acc.: 91.41%] [G loss: 3.608285]\n",
      "epoch:21 step:16856 [D loss: 0.248188, acc.: 87.50%] [G loss: 3.886960]\n",
      "epoch:21 step:16857 [D loss: 0.690763, acc.: 72.66%] [G loss: 3.967479]\n",
      "epoch:21 step:16858 [D loss: 0.637406, acc.: 74.22%] [G loss: 5.184811]\n",
      "epoch:21 step:16859 [D loss: 0.293587, acc.: 86.72%] [G loss: 3.660443]\n",
      "epoch:21 step:16860 [D loss: 0.283655, acc.: 84.38%] [G loss: 3.973359]\n",
      "epoch:21 step:16861 [D loss: 0.356036, acc.: 85.16%] [G loss: 2.591565]\n",
      "epoch:21 step:16862 [D loss: 0.308500, acc.: 87.50%] [G loss: 4.107385]\n",
      "epoch:21 step:16863 [D loss: 0.247058, acc.: 86.72%] [G loss: 3.523255]\n",
      "epoch:21 step:16864 [D loss: 0.301945, acc.: 86.72%] [G loss: 2.547684]\n",
      "epoch:21 step:16865 [D loss: 0.301547, acc.: 85.94%] [G loss: 2.552639]\n",
      "epoch:21 step:16866 [D loss: 0.294783, acc.: 85.94%] [G loss: 3.227337]\n",
      "epoch:21 step:16867 [D loss: 0.294693, acc.: 89.06%] [G loss: 2.686552]\n",
      "epoch:21 step:16868 [D loss: 0.315076, acc.: 85.94%] [G loss: 2.989550]\n",
      "epoch:21 step:16869 [D loss: 0.275404, acc.: 87.50%] [G loss: 4.362043]\n",
      "epoch:21 step:16870 [D loss: 0.371563, acc.: 84.38%] [G loss: 2.375411]\n",
      "epoch:21 step:16871 [D loss: 0.272889, acc.: 89.06%] [G loss: 2.925266]\n",
      "epoch:21 step:16872 [D loss: 0.293205, acc.: 88.28%] [G loss: 2.702139]\n",
      "epoch:21 step:16873 [D loss: 0.390373, acc.: 82.03%] [G loss: 2.909469]\n",
      "epoch:21 step:16874 [D loss: 0.290231, acc.: 89.06%] [G loss: 3.256561]\n",
      "epoch:21 step:16875 [D loss: 0.364431, acc.: 82.81%] [G loss: 2.826660]\n",
      "epoch:21 step:16876 [D loss: 0.308374, acc.: 84.38%] [G loss: 3.043698]\n",
      "epoch:21 step:16877 [D loss: 0.310786, acc.: 86.72%] [G loss: 2.514071]\n",
      "epoch:21 step:16878 [D loss: 0.237500, acc.: 90.62%] [G loss: 2.524371]\n",
      "epoch:21 step:16879 [D loss: 0.264898, acc.: 88.28%] [G loss: 3.008608]\n",
      "epoch:21 step:16880 [D loss: 0.250417, acc.: 91.41%] [G loss: 2.471453]\n",
      "epoch:21 step:16881 [D loss: 0.237290, acc.: 90.62%] [G loss: 3.032881]\n",
      "epoch:21 step:16882 [D loss: 0.287110, acc.: 89.06%] [G loss: 2.150501]\n",
      "epoch:21 step:16883 [D loss: 0.496944, acc.: 72.66%] [G loss: 4.076520]\n",
      "epoch:21 step:16884 [D loss: 0.384051, acc.: 84.38%] [G loss: 3.561230]\n",
      "epoch:21 step:16885 [D loss: 0.193533, acc.: 92.97%] [G loss: 4.208896]\n",
      "epoch:21 step:16886 [D loss: 0.234108, acc.: 89.06%] [G loss: 3.878473]\n",
      "epoch:21 step:16887 [D loss: 0.255532, acc.: 89.84%] [G loss: 4.023036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16888 [D loss: 0.235719, acc.: 89.06%] [G loss: 3.321464]\n",
      "epoch:21 step:16889 [D loss: 0.319666, acc.: 82.81%] [G loss: 2.814176]\n",
      "epoch:21 step:16890 [D loss: 0.320808, acc.: 86.72%] [G loss: 2.475556]\n",
      "epoch:21 step:16891 [D loss: 0.252077, acc.: 90.62%] [G loss: 3.716269]\n",
      "epoch:21 step:16892 [D loss: 0.394096, acc.: 83.59%] [G loss: 3.338100]\n",
      "epoch:21 step:16893 [D loss: 0.331984, acc.: 85.94%] [G loss: 2.961525]\n",
      "epoch:21 step:16894 [D loss: 0.277693, acc.: 86.72%] [G loss: 3.468139]\n",
      "epoch:21 step:16895 [D loss: 0.240229, acc.: 89.84%] [G loss: 3.437208]\n",
      "epoch:21 step:16896 [D loss: 0.368638, acc.: 82.03%] [G loss: 2.814535]\n",
      "epoch:21 step:16897 [D loss: 0.307350, acc.: 83.59%] [G loss: 3.186680]\n",
      "epoch:21 step:16898 [D loss: 0.280040, acc.: 90.62%] [G loss: 3.689429]\n",
      "epoch:21 step:16899 [D loss: 0.305018, acc.: 85.94%] [G loss: 3.059194]\n",
      "epoch:21 step:16900 [D loss: 0.416126, acc.: 81.25%] [G loss: 3.738257]\n",
      "epoch:21 step:16901 [D loss: 0.287751, acc.: 86.72%] [G loss: 3.515214]\n",
      "epoch:21 step:16902 [D loss: 0.307153, acc.: 84.38%] [G loss: 3.407237]\n",
      "epoch:21 step:16903 [D loss: 0.303290, acc.: 85.94%] [G loss: 3.189044]\n",
      "epoch:21 step:16904 [D loss: 0.360611, acc.: 85.16%] [G loss: 1.807222]\n",
      "epoch:21 step:16905 [D loss: 0.307243, acc.: 83.59%] [G loss: 2.440093]\n",
      "epoch:21 step:16906 [D loss: 0.263332, acc.: 90.62%] [G loss: 2.458754]\n",
      "epoch:21 step:16907 [D loss: 0.309905, acc.: 85.16%] [G loss: 2.432076]\n",
      "epoch:21 step:16908 [D loss: 0.280403, acc.: 87.50%] [G loss: 3.040666]\n",
      "epoch:21 step:16909 [D loss: 0.268828, acc.: 89.84%] [G loss: 2.628077]\n",
      "epoch:21 step:16910 [D loss: 0.360811, acc.: 85.94%] [G loss: 2.439559]\n",
      "epoch:21 step:16911 [D loss: 0.352696, acc.: 82.81%] [G loss: 2.675398]\n",
      "epoch:21 step:16912 [D loss: 0.382469, acc.: 83.59%] [G loss: 3.119775]\n",
      "epoch:21 step:16913 [D loss: 0.305631, acc.: 86.72%] [G loss: 3.516588]\n",
      "epoch:21 step:16914 [D loss: 0.387354, acc.: 82.03%] [G loss: 2.985391]\n",
      "epoch:21 step:16915 [D loss: 0.265102, acc.: 89.06%] [G loss: 2.869080]\n",
      "epoch:21 step:16916 [D loss: 0.326122, acc.: 85.94%] [G loss: 3.335407]\n",
      "epoch:21 step:16917 [D loss: 0.311341, acc.: 85.16%] [G loss: 3.399477]\n",
      "epoch:21 step:16918 [D loss: 0.291386, acc.: 86.72%] [G loss: 2.777603]\n",
      "epoch:21 step:16919 [D loss: 0.234224, acc.: 89.84%] [G loss: 3.445571]\n",
      "epoch:21 step:16920 [D loss: 0.223918, acc.: 90.62%] [G loss: 3.024768]\n",
      "epoch:21 step:16921 [D loss: 0.250028, acc.: 89.06%] [G loss: 3.811753]\n",
      "epoch:21 step:16922 [D loss: 0.339671, acc.: 84.38%] [G loss: 3.599358]\n",
      "epoch:21 step:16923 [D loss: 0.372605, acc.: 85.16%] [G loss: 3.469486]\n",
      "epoch:21 step:16924 [D loss: 0.216530, acc.: 88.28%] [G loss: 6.696314]\n",
      "epoch:21 step:16925 [D loss: 0.374978, acc.: 81.25%] [G loss: 4.747996]\n",
      "epoch:21 step:16926 [D loss: 0.223213, acc.: 87.50%] [G loss: 9.679396]\n",
      "epoch:21 step:16927 [D loss: 0.301708, acc.: 88.28%] [G loss: 5.505628]\n",
      "epoch:21 step:16928 [D loss: 0.207545, acc.: 91.41%] [G loss: 6.233494]\n",
      "epoch:21 step:16929 [D loss: 0.190440, acc.: 92.97%] [G loss: 6.885831]\n",
      "epoch:21 step:16930 [D loss: 0.395394, acc.: 82.03%] [G loss: 4.394145]\n",
      "epoch:21 step:16931 [D loss: 0.250955, acc.: 88.28%] [G loss: 5.793911]\n",
      "epoch:21 step:16932 [D loss: 0.216349, acc.: 91.41%] [G loss: 8.004082]\n",
      "epoch:21 step:16933 [D loss: 0.194712, acc.: 90.62%] [G loss: 4.603794]\n",
      "epoch:21 step:16934 [D loss: 0.344958, acc.: 85.94%] [G loss: 4.062785]\n",
      "epoch:21 step:16935 [D loss: 0.180145, acc.: 94.53%] [G loss: 3.926977]\n",
      "epoch:21 step:16936 [D loss: 0.336619, acc.: 86.72%] [G loss: 3.639650]\n",
      "epoch:21 step:16937 [D loss: 0.401117, acc.: 82.81%] [G loss: 3.756060]\n",
      "epoch:21 step:16938 [D loss: 0.459338, acc.: 79.69%] [G loss: 3.632045]\n",
      "epoch:21 step:16939 [D loss: 0.346677, acc.: 82.03%] [G loss: 4.114768]\n",
      "epoch:21 step:16940 [D loss: 0.262330, acc.: 90.62%] [G loss: 3.806566]\n",
      "epoch:21 step:16941 [D loss: 0.314062, acc.: 88.28%] [G loss: 3.324688]\n",
      "epoch:21 step:16942 [D loss: 0.393825, acc.: 78.91%] [G loss: 3.718117]\n",
      "epoch:21 step:16943 [D loss: 0.436106, acc.: 79.69%] [G loss: 2.667683]\n",
      "epoch:21 step:16944 [D loss: 0.285718, acc.: 90.62%] [G loss: 2.935756]\n",
      "epoch:21 step:16945 [D loss: 0.363544, acc.: 84.38%] [G loss: 4.967443]\n",
      "epoch:21 step:16946 [D loss: 0.301328, acc.: 85.16%] [G loss: 4.086353]\n",
      "epoch:21 step:16947 [D loss: 0.282671, acc.: 85.16%] [G loss: 3.414136]\n",
      "epoch:21 step:16948 [D loss: 0.369164, acc.: 81.25%] [G loss: 2.649076]\n",
      "epoch:21 step:16949 [D loss: 0.246277, acc.: 89.84%] [G loss: 3.345938]\n",
      "epoch:21 step:16950 [D loss: 0.312790, acc.: 90.62%] [G loss: 2.257607]\n",
      "epoch:21 step:16951 [D loss: 0.307482, acc.: 82.81%] [G loss: 3.595193]\n",
      "epoch:21 step:16952 [D loss: 0.344924, acc.: 88.28%] [G loss: 3.571296]\n",
      "epoch:21 step:16953 [D loss: 0.396978, acc.: 80.47%] [G loss: 3.068589]\n",
      "epoch:21 step:16954 [D loss: 0.385253, acc.: 82.03%] [G loss: 2.892873]\n",
      "epoch:21 step:16955 [D loss: 0.383319, acc.: 82.03%] [G loss: 3.788591]\n",
      "epoch:21 step:16956 [D loss: 0.325978, acc.: 85.94%] [G loss: 4.216891]\n",
      "epoch:21 step:16957 [D loss: 0.272356, acc.: 89.06%] [G loss: 2.829818]\n",
      "epoch:21 step:16958 [D loss: 0.267722, acc.: 89.06%] [G loss: 3.042531]\n",
      "epoch:21 step:16959 [D loss: 0.336490, acc.: 85.94%] [G loss: 3.248387]\n",
      "epoch:21 step:16960 [D loss: 0.359929, acc.: 83.59%] [G loss: 4.037856]\n",
      "epoch:21 step:16961 [D loss: 0.216692, acc.: 92.19%] [G loss: 5.813077]\n",
      "epoch:21 step:16962 [D loss: 0.283638, acc.: 88.28%] [G loss: 4.128322]\n",
      "epoch:21 step:16963 [D loss: 0.312870, acc.: 85.16%] [G loss: 3.601971]\n",
      "epoch:21 step:16964 [D loss: 0.294693, acc.: 89.84%] [G loss: 5.634427]\n",
      "epoch:21 step:16965 [D loss: 0.438843, acc.: 82.03%] [G loss: 3.815684]\n",
      "epoch:21 step:16966 [D loss: 0.247553, acc.: 89.84%] [G loss: 6.545369]\n",
      "epoch:21 step:16967 [D loss: 0.348103, acc.: 86.72%] [G loss: 4.244068]\n",
      "epoch:21 step:16968 [D loss: 0.243833, acc.: 89.06%] [G loss: 4.516927]\n",
      "epoch:21 step:16969 [D loss: 0.334347, acc.: 82.81%] [G loss: 3.760080]\n",
      "epoch:21 step:16970 [D loss: 0.336921, acc.: 81.25%] [G loss: 4.294681]\n",
      "epoch:21 step:16971 [D loss: 0.340442, acc.: 82.03%] [G loss: 4.028750]\n",
      "epoch:21 step:16972 [D loss: 0.306341, acc.: 85.16%] [G loss: 3.674583]\n",
      "epoch:21 step:16973 [D loss: 0.419014, acc.: 77.34%] [G loss: 2.842712]\n",
      "epoch:21 step:16974 [D loss: 0.409484, acc.: 82.03%] [G loss: 3.577453]\n",
      "epoch:21 step:16975 [D loss: 0.272204, acc.: 88.28%] [G loss: 2.737209]\n",
      "epoch:21 step:16976 [D loss: 0.308451, acc.: 85.16%] [G loss: 3.182683]\n",
      "epoch:21 step:16977 [D loss: 0.343137, acc.: 85.16%] [G loss: 3.081544]\n",
      "epoch:21 step:16978 [D loss: 0.195100, acc.: 92.97%] [G loss: 3.161523]\n",
      "epoch:21 step:16979 [D loss: 0.374685, acc.: 83.59%] [G loss: 3.560222]\n",
      "epoch:21 step:16980 [D loss: 0.214508, acc.: 89.84%] [G loss: 3.414952]\n",
      "epoch:21 step:16981 [D loss: 0.381432, acc.: 84.38%] [G loss: 2.786242]\n",
      "epoch:21 step:16982 [D loss: 0.251589, acc.: 90.62%] [G loss: 2.966172]\n",
      "epoch:21 step:16983 [D loss: 0.288298, acc.: 89.06%] [G loss: 2.920441]\n",
      "epoch:21 step:16984 [D loss: 0.467181, acc.: 74.22%] [G loss: 2.971257]\n",
      "epoch:21 step:16985 [D loss: 0.524700, acc.: 72.66%] [G loss: 2.374659]\n",
      "epoch:21 step:16986 [D loss: 0.365375, acc.: 85.94%] [G loss: 2.521190]\n",
      "epoch:21 step:16987 [D loss: 0.282664, acc.: 88.28%] [G loss: 2.609857]\n",
      "epoch:21 step:16988 [D loss: 0.340195, acc.: 82.81%] [G loss: 2.785627]\n",
      "epoch:21 step:16989 [D loss: 0.384518, acc.: 78.91%] [G loss: 2.782197]\n",
      "epoch:21 step:16990 [D loss: 0.364283, acc.: 81.25%] [G loss: 3.735952]\n",
      "epoch:21 step:16991 [D loss: 0.452358, acc.: 78.91%] [G loss: 4.421791]\n",
      "epoch:21 step:16992 [D loss: 0.719035, acc.: 71.88%] [G loss: 6.173893]\n",
      "epoch:21 step:16993 [D loss: 1.122411, acc.: 63.28%] [G loss: 7.870197]\n",
      "epoch:21 step:16994 [D loss: 1.261277, acc.: 60.16%] [G loss: 4.027761]\n",
      "epoch:21 step:16995 [D loss: 0.854723, acc.: 71.88%] [G loss: 3.595976]\n",
      "epoch:21 step:16996 [D loss: 0.520971, acc.: 82.03%] [G loss: 2.472153]\n",
      "epoch:21 step:16997 [D loss: 0.434451, acc.: 83.59%] [G loss: 3.594610]\n",
      "epoch:21 step:16998 [D loss: 0.311053, acc.: 87.50%] [G loss: 2.728622]\n",
      "epoch:21 step:16999 [D loss: 0.314553, acc.: 87.50%] [G loss: 2.860372]\n",
      "epoch:21 step:17000 [D loss: 0.365252, acc.: 82.81%] [G loss: 4.391547]\n",
      "epoch:21 step:17001 [D loss: 0.421609, acc.: 78.12%] [G loss: 2.511209]\n",
      "epoch:21 step:17002 [D loss: 0.284739, acc.: 89.06%] [G loss: 3.094121]\n",
      "epoch:21 step:17003 [D loss: 0.439258, acc.: 82.03%] [G loss: 2.605582]\n",
      "epoch:21 step:17004 [D loss: 0.362734, acc.: 82.81%] [G loss: 2.872385]\n",
      "epoch:21 step:17005 [D loss: 0.192898, acc.: 92.97%] [G loss: 3.648796]\n",
      "epoch:21 step:17006 [D loss: 0.362123, acc.: 82.03%] [G loss: 4.018423]\n",
      "epoch:21 step:17007 [D loss: 0.390631, acc.: 81.25%] [G loss: 3.524060]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:17008 [D loss: 0.198380, acc.: 90.62%] [G loss: 2.358780]\n",
      "epoch:21 step:17009 [D loss: 0.261714, acc.: 87.50%] [G loss: 5.148300]\n",
      "epoch:21 step:17010 [D loss: 0.174677, acc.: 94.53%] [G loss: 3.983294]\n",
      "epoch:21 step:17011 [D loss: 0.195436, acc.: 93.75%] [G loss: 2.870692]\n",
      "epoch:21 step:17012 [D loss: 0.247939, acc.: 89.84%] [G loss: 2.512408]\n",
      "epoch:21 step:17013 [D loss: 0.380709, acc.: 81.25%] [G loss: 2.651492]\n",
      "epoch:21 step:17014 [D loss: 0.360522, acc.: 81.25%] [G loss: 2.135333]\n",
      "epoch:21 step:17015 [D loss: 0.334308, acc.: 84.38%] [G loss: 3.406654]\n",
      "epoch:21 step:17016 [D loss: 0.329796, acc.: 85.94%] [G loss: 3.236583]\n",
      "epoch:21 step:17017 [D loss: 0.369745, acc.: 83.59%] [G loss: 3.005057]\n",
      "epoch:21 step:17018 [D loss: 0.392800, acc.: 78.12%] [G loss: 2.794731]\n",
      "epoch:21 step:17019 [D loss: 0.276170, acc.: 86.72%] [G loss: 2.998793]\n",
      "epoch:21 step:17020 [D loss: 0.407163, acc.: 79.69%] [G loss: 2.624430]\n",
      "epoch:21 step:17021 [D loss: 0.270437, acc.: 88.28%] [G loss: 3.222236]\n",
      "epoch:21 step:17022 [D loss: 0.238748, acc.: 92.97%] [G loss: 3.182142]\n",
      "epoch:21 step:17023 [D loss: 0.247662, acc.: 90.62%] [G loss: 2.327746]\n",
      "epoch:21 step:17024 [D loss: 0.468092, acc.: 80.47%] [G loss: 2.029680]\n",
      "epoch:21 step:17025 [D loss: 0.287569, acc.: 85.94%] [G loss: 2.548147]\n",
      "epoch:21 step:17026 [D loss: 0.386571, acc.: 82.81%] [G loss: 2.099394]\n",
      "epoch:21 step:17027 [D loss: 0.371417, acc.: 85.94%] [G loss: 2.329802]\n",
      "epoch:21 step:17028 [D loss: 0.340599, acc.: 81.25%] [G loss: 3.023041]\n",
      "epoch:21 step:17029 [D loss: 0.371270, acc.: 85.16%] [G loss: 2.869597]\n",
      "epoch:21 step:17030 [D loss: 0.313814, acc.: 86.72%] [G loss: 3.064309]\n",
      "epoch:21 step:17031 [D loss: 0.388436, acc.: 79.69%] [G loss: 2.708251]\n",
      "epoch:21 step:17032 [D loss: 0.373665, acc.: 81.25%] [G loss: 2.680676]\n",
      "epoch:21 step:17033 [D loss: 0.332735, acc.: 87.50%] [G loss: 3.582316]\n",
      "epoch:21 step:17034 [D loss: 0.453151, acc.: 79.69%] [G loss: 4.159946]\n",
      "epoch:21 step:17035 [D loss: 0.441783, acc.: 79.69%] [G loss: 2.206832]\n",
      "epoch:21 step:17036 [D loss: 0.394831, acc.: 81.25%] [G loss: 2.295877]\n",
      "epoch:21 step:17037 [D loss: 0.297711, acc.: 85.16%] [G loss: 3.019657]\n",
      "epoch:21 step:17038 [D loss: 0.314786, acc.: 84.38%] [G loss: 3.106053]\n",
      "epoch:21 step:17039 [D loss: 0.421882, acc.: 78.91%] [G loss: 3.420929]\n",
      "epoch:21 step:17040 [D loss: 0.422417, acc.: 82.81%] [G loss: 3.137698]\n",
      "epoch:21 step:17041 [D loss: 0.294621, acc.: 88.28%] [G loss: 3.159216]\n",
      "epoch:21 step:17042 [D loss: 0.293354, acc.: 89.84%] [G loss: 2.785506]\n",
      "epoch:21 step:17043 [D loss: 0.368478, acc.: 83.59%] [G loss: 5.008144]\n",
      "epoch:21 step:17044 [D loss: 0.223783, acc.: 90.62%] [G loss: 3.550345]\n",
      "epoch:21 step:17045 [D loss: 0.351372, acc.: 85.16%] [G loss: 5.121112]\n",
      "epoch:21 step:17046 [D loss: 0.250289, acc.: 89.84%] [G loss: 4.682558]\n",
      "epoch:21 step:17047 [D loss: 0.260370, acc.: 87.50%] [G loss: 4.282966]\n",
      "epoch:21 step:17048 [D loss: 0.299732, acc.: 89.84%] [G loss: 3.652982]\n",
      "epoch:21 step:17049 [D loss: 0.255974, acc.: 90.62%] [G loss: 3.427068]\n",
      "epoch:21 step:17050 [D loss: 0.268053, acc.: 90.62%] [G loss: 4.494527]\n",
      "epoch:21 step:17051 [D loss: 0.435751, acc.: 80.47%] [G loss: 2.962319]\n",
      "epoch:21 step:17052 [D loss: 0.312105, acc.: 84.38%] [G loss: 2.582475]\n",
      "epoch:21 step:17053 [D loss: 0.359915, acc.: 82.03%] [G loss: 3.309000]\n",
      "epoch:21 step:17054 [D loss: 0.325259, acc.: 85.94%] [G loss: 2.684365]\n",
      "epoch:21 step:17055 [D loss: 0.386490, acc.: 82.81%] [G loss: 2.675112]\n",
      "epoch:21 step:17056 [D loss: 0.334302, acc.: 82.81%] [G loss: 3.159856]\n",
      "epoch:21 step:17057 [D loss: 0.278229, acc.: 88.28%] [G loss: 4.302884]\n",
      "epoch:21 step:17058 [D loss: 0.551716, acc.: 78.12%] [G loss: 3.488454]\n",
      "epoch:21 step:17059 [D loss: 0.433057, acc.: 81.25%] [G loss: 2.419665]\n",
      "epoch:21 step:17060 [D loss: 0.304177, acc.: 88.28%] [G loss: 3.586064]\n",
      "epoch:21 step:17061 [D loss: 0.257159, acc.: 88.28%] [G loss: 2.592719]\n",
      "epoch:21 step:17062 [D loss: 0.231936, acc.: 91.41%] [G loss: 3.868209]\n",
      "epoch:21 step:17063 [D loss: 0.315112, acc.: 87.50%] [G loss: 2.613420]\n",
      "epoch:21 step:17064 [D loss: 0.291198, acc.: 85.94%] [G loss: 3.546240]\n",
      "epoch:21 step:17065 [D loss: 0.379174, acc.: 85.94%] [G loss: 2.074025]\n",
      "epoch:21 step:17066 [D loss: 0.327693, acc.: 89.06%] [G loss: 2.724394]\n",
      "epoch:21 step:17067 [D loss: 0.358184, acc.: 85.16%] [G loss: 2.778635]\n",
      "epoch:21 step:17068 [D loss: 0.292640, acc.: 87.50%] [G loss: 1.916415]\n",
      "epoch:21 step:17069 [D loss: 0.310682, acc.: 89.06%] [G loss: 3.202916]\n",
      "epoch:21 step:17070 [D loss: 0.306084, acc.: 89.06%] [G loss: 4.247717]\n",
      "epoch:21 step:17071 [D loss: 0.251097, acc.: 91.41%] [G loss: 3.591031]\n",
      "epoch:21 step:17072 [D loss: 0.252455, acc.: 89.84%] [G loss: 2.746048]\n",
      "epoch:21 step:17073 [D loss: 0.338103, acc.: 85.94%] [G loss: 2.439361]\n",
      "epoch:21 step:17074 [D loss: 0.311174, acc.: 85.94%] [G loss: 2.380265]\n",
      "epoch:21 step:17075 [D loss: 0.310969, acc.: 87.50%] [G loss: 2.481595]\n",
      "epoch:21 step:17076 [D loss: 0.313996, acc.: 88.28%] [G loss: 2.036752]\n",
      "epoch:21 step:17077 [D loss: 0.328103, acc.: 86.72%] [G loss: 2.297842]\n",
      "epoch:21 step:17078 [D loss: 0.291686, acc.: 86.72%] [G loss: 2.457094]\n",
      "epoch:21 step:17079 [D loss: 0.301876, acc.: 87.50%] [G loss: 2.310690]\n",
      "epoch:21 step:17080 [D loss: 0.344946, acc.: 83.59%] [G loss: 2.915676]\n",
      "epoch:21 step:17081 [D loss: 0.395501, acc.: 86.72%] [G loss: 2.328423]\n",
      "epoch:21 step:17082 [D loss: 0.259221, acc.: 90.62%] [G loss: 2.935588]\n",
      "epoch:21 step:17083 [D loss: 0.402537, acc.: 81.25%] [G loss: 3.323160]\n",
      "epoch:21 step:17084 [D loss: 0.381693, acc.: 78.12%] [G loss: 3.353988]\n",
      "epoch:21 step:17085 [D loss: 0.531111, acc.: 77.34%] [G loss: 4.893196]\n",
      "epoch:21 step:17086 [D loss: 0.579416, acc.: 74.22%] [G loss: 5.797248]\n",
      "epoch:21 step:17087 [D loss: 0.367566, acc.: 82.03%] [G loss: 3.315627]\n",
      "epoch:21 step:17088 [D loss: 0.349139, acc.: 81.25%] [G loss: 4.709070]\n",
      "epoch:21 step:17089 [D loss: 0.324177, acc.: 82.03%] [G loss: 3.668299]\n",
      "epoch:21 step:17090 [D loss: 0.355519, acc.: 84.38%] [G loss: 3.029816]\n",
      "epoch:21 step:17091 [D loss: 0.296962, acc.: 88.28%] [G loss: 4.193589]\n",
      "epoch:21 step:17092 [D loss: 0.258917, acc.: 86.72%] [G loss: 3.761671]\n",
      "epoch:21 step:17093 [D loss: 0.332238, acc.: 84.38%] [G loss: 4.036115]\n",
      "epoch:21 step:17094 [D loss: 0.359347, acc.: 83.59%] [G loss: 6.234082]\n",
      "epoch:21 step:17095 [D loss: 0.346491, acc.: 84.38%] [G loss: 5.739516]\n",
      "epoch:21 step:17096 [D loss: 0.279433, acc.: 85.94%] [G loss: 6.375483]\n",
      "epoch:21 step:17097 [D loss: 0.267190, acc.: 84.38%] [G loss: 6.496799]\n",
      "epoch:21 step:17098 [D loss: 0.225375, acc.: 90.62%] [G loss: 3.747371]\n",
      "epoch:21 step:17099 [D loss: 0.287521, acc.: 89.06%] [G loss: 7.768206]\n",
      "epoch:21 step:17100 [D loss: 0.237719, acc.: 89.06%] [G loss: 3.610427]\n",
      "epoch:21 step:17101 [D loss: 0.259413, acc.: 89.84%] [G loss: 4.504794]\n",
      "epoch:21 step:17102 [D loss: 0.247483, acc.: 86.72%] [G loss: 6.461081]\n",
      "epoch:21 step:17103 [D loss: 0.237124, acc.: 92.19%] [G loss: 3.523046]\n",
      "epoch:21 step:17104 [D loss: 0.378590, acc.: 81.25%] [G loss: 3.723032]\n",
      "epoch:21 step:17105 [D loss: 0.290807, acc.: 85.94%] [G loss: 4.152729]\n",
      "epoch:21 step:17106 [D loss: 0.431945, acc.: 81.25%] [G loss: 4.370720]\n",
      "epoch:21 step:17107 [D loss: 0.279397, acc.: 88.28%] [G loss: 3.207808]\n",
      "epoch:21 step:17108 [D loss: 0.310872, acc.: 84.38%] [G loss: 3.100507]\n",
      "epoch:21 step:17109 [D loss: 0.365303, acc.: 85.16%] [G loss: 2.796429]\n",
      "epoch:21 step:17110 [D loss: 0.252718, acc.: 91.41%] [G loss: 2.481653]\n",
      "epoch:21 step:17111 [D loss: 0.389419, acc.: 81.25%] [G loss: 2.728546]\n",
      "epoch:21 step:17112 [D loss: 0.263634, acc.: 88.28%] [G loss: 3.266649]\n",
      "epoch:21 step:17113 [D loss: 0.262558, acc.: 85.94%] [G loss: 2.658463]\n",
      "epoch:21 step:17114 [D loss: 0.338356, acc.: 85.16%] [G loss: 3.042383]\n",
      "epoch:21 step:17115 [D loss: 0.356592, acc.: 82.81%] [G loss: 2.258628]\n",
      "epoch:21 step:17116 [D loss: 0.393372, acc.: 81.25%] [G loss: 2.329221]\n",
      "epoch:21 step:17117 [D loss: 0.283718, acc.: 88.28%] [G loss: 3.065389]\n",
      "epoch:21 step:17118 [D loss: 0.300277, acc.: 86.72%] [G loss: 2.818191]\n",
      "epoch:21 step:17119 [D loss: 0.278660, acc.: 89.06%] [G loss: 3.089013]\n",
      "epoch:21 step:17120 [D loss: 0.367194, acc.: 82.03%] [G loss: 3.024985]\n",
      "epoch:21 step:17121 [D loss: 0.260729, acc.: 86.72%] [G loss: 4.591351]\n",
      "epoch:21 step:17122 [D loss: 0.351848, acc.: 80.47%] [G loss: 3.160642]\n",
      "epoch:21 step:17123 [D loss: 0.217088, acc.: 89.06%] [G loss: 6.585589]\n",
      "epoch:21 step:17124 [D loss: 0.236722, acc.: 88.28%] [G loss: 4.318777]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:17125 [D loss: 0.209898, acc.: 92.97%] [G loss: 2.912485]\n",
      "epoch:21 step:17126 [D loss: 0.258716, acc.: 86.72%] [G loss: 3.912771]\n",
      "epoch:21 step:17127 [D loss: 0.264666, acc.: 88.28%] [G loss: 3.990872]\n",
      "epoch:21 step:17128 [D loss: 0.330293, acc.: 83.59%] [G loss: 3.476086]\n",
      "epoch:21 step:17129 [D loss: 0.309483, acc.: 85.94%] [G loss: 3.558226]\n",
      "epoch:21 step:17130 [D loss: 0.298169, acc.: 86.72%] [G loss: 2.627824]\n",
      "epoch:21 step:17131 [D loss: 0.281548, acc.: 88.28%] [G loss: 2.780670]\n",
      "epoch:21 step:17132 [D loss: 0.291922, acc.: 85.94%] [G loss: 3.129035]\n",
      "epoch:21 step:17133 [D loss: 0.264966, acc.: 89.06%] [G loss: 3.426710]\n",
      "epoch:21 step:17134 [D loss: 0.191533, acc.: 93.75%] [G loss: 2.664984]\n",
      "epoch:21 step:17135 [D loss: 0.342265, acc.: 85.16%] [G loss: 3.489396]\n",
      "epoch:21 step:17136 [D loss: 0.365946, acc.: 82.03%] [G loss: 2.491115]\n",
      "epoch:21 step:17137 [D loss: 0.302269, acc.: 86.72%] [G loss: 2.633072]\n",
      "epoch:21 step:17138 [D loss: 0.344197, acc.: 85.94%] [G loss: 2.605703]\n",
      "epoch:21 step:17139 [D loss: 0.371674, acc.: 82.03%] [G loss: 3.515633]\n",
      "epoch:21 step:17140 [D loss: 0.397038, acc.: 83.59%] [G loss: 3.728598]\n",
      "epoch:21 step:17141 [D loss: 0.292105, acc.: 89.84%] [G loss: 2.337941]\n",
      "epoch:21 step:17142 [D loss: 0.464631, acc.: 77.34%] [G loss: 3.063790]\n",
      "epoch:21 step:17143 [D loss: 0.384792, acc.: 83.59%] [G loss: 2.592473]\n",
      "epoch:21 step:17144 [D loss: 0.242128, acc.: 92.19%] [G loss: 3.252860]\n",
      "epoch:21 step:17145 [D loss: 0.349188, acc.: 85.16%] [G loss: 2.638403]\n",
      "epoch:21 step:17146 [D loss: 0.392672, acc.: 82.81%] [G loss: 2.162391]\n",
      "epoch:21 step:17147 [D loss: 0.306074, acc.: 86.72%] [G loss: 2.422052]\n",
      "epoch:21 step:17148 [D loss: 0.387514, acc.: 83.59%] [G loss: 3.035405]\n",
      "epoch:21 step:17149 [D loss: 0.311760, acc.: 87.50%] [G loss: 2.556957]\n",
      "epoch:21 step:17150 [D loss: 0.352734, acc.: 85.16%] [G loss: 3.280111]\n",
      "epoch:21 step:17151 [D loss: 0.358758, acc.: 85.16%] [G loss: 3.718409]\n",
      "epoch:21 step:17152 [D loss: 0.388421, acc.: 83.59%] [G loss: 2.215060]\n",
      "epoch:21 step:17153 [D loss: 0.295713, acc.: 89.06%] [G loss: 2.178597]\n",
      "epoch:21 step:17154 [D loss: 0.314803, acc.: 86.72%] [G loss: 2.610859]\n",
      "epoch:21 step:17155 [D loss: 0.465038, acc.: 76.56%] [G loss: 3.058600]\n",
      "epoch:21 step:17156 [D loss: 0.283394, acc.: 89.06%] [G loss: 3.169768]\n",
      "epoch:21 step:17157 [D loss: 0.282922, acc.: 83.59%] [G loss: 3.139312]\n",
      "epoch:21 step:17158 [D loss: 0.299928, acc.: 85.94%] [G loss: 2.748096]\n",
      "epoch:21 step:17159 [D loss: 0.302887, acc.: 90.62%] [G loss: 2.970779]\n",
      "epoch:21 step:17160 [D loss: 0.371459, acc.: 81.25%] [G loss: 2.345040]\n",
      "epoch:21 step:17161 [D loss: 0.381225, acc.: 80.47%] [G loss: 3.865382]\n",
      "epoch:21 step:17162 [D loss: 0.256856, acc.: 89.06%] [G loss: 4.400501]\n",
      "epoch:21 step:17163 [D loss: 0.356455, acc.: 85.16%] [G loss: 2.397738]\n",
      "epoch:21 step:17164 [D loss: 0.275693, acc.: 86.72%] [G loss: 2.266955]\n",
      "epoch:21 step:17165 [D loss: 0.312902, acc.: 85.16%] [G loss: 4.670442]\n",
      "epoch:21 step:17166 [D loss: 0.354144, acc.: 80.47%] [G loss: 5.455312]\n",
      "epoch:21 step:17167 [D loss: 0.353173, acc.: 82.81%] [G loss: 3.017771]\n",
      "epoch:21 step:17168 [D loss: 0.200819, acc.: 92.97%] [G loss: 3.390059]\n",
      "epoch:21 step:17169 [D loss: 0.446379, acc.: 80.47%] [G loss: 3.203676]\n",
      "epoch:21 step:17170 [D loss: 0.276414, acc.: 87.50%] [G loss: 3.123519]\n",
      "epoch:21 step:17171 [D loss: 0.257177, acc.: 91.41%] [G loss: 3.311691]\n",
      "epoch:21 step:17172 [D loss: 0.387126, acc.: 82.03%] [G loss: 2.323399]\n",
      "epoch:21 step:17173 [D loss: 0.288648, acc.: 85.94%] [G loss: 2.892738]\n",
      "epoch:21 step:17174 [D loss: 0.293095, acc.: 88.28%] [G loss: 3.344885]\n",
      "epoch:21 step:17175 [D loss: 0.371578, acc.: 85.16%] [G loss: 3.630560]\n",
      "epoch:21 step:17176 [D loss: 0.524825, acc.: 78.91%] [G loss: 5.144331]\n",
      "epoch:21 step:17177 [D loss: 0.755889, acc.: 78.12%] [G loss: 6.139853]\n",
      "epoch:21 step:17178 [D loss: 1.240856, acc.: 60.16%] [G loss: 3.430852]\n",
      "epoch:21 step:17179 [D loss: 0.690303, acc.: 71.88%] [G loss: 2.149506]\n",
      "epoch:21 step:17180 [D loss: 0.316860, acc.: 85.94%] [G loss: 5.247850]\n",
      "epoch:21 step:17181 [D loss: 0.376472, acc.: 86.72%] [G loss: 3.507813]\n",
      "epoch:21 step:17182 [D loss: 0.386296, acc.: 81.25%] [G loss: 3.721680]\n",
      "epoch:22 step:17183 [D loss: 0.411694, acc.: 82.81%] [G loss: 2.500354]\n",
      "epoch:22 step:17184 [D loss: 0.307183, acc.: 85.16%] [G loss: 2.457560]\n",
      "epoch:22 step:17185 [D loss: 0.255359, acc.: 91.41%] [G loss: 2.652072]\n",
      "epoch:22 step:17186 [D loss: 0.279869, acc.: 85.16%] [G loss: 2.721124]\n",
      "epoch:22 step:17187 [D loss: 0.315547, acc.: 85.94%] [G loss: 3.185650]\n",
      "epoch:22 step:17188 [D loss: 0.344529, acc.: 85.16%] [G loss: 2.502873]\n",
      "epoch:22 step:17189 [D loss: 0.361534, acc.: 84.38%] [G loss: 2.879642]\n",
      "epoch:22 step:17190 [D loss: 0.232537, acc.: 90.62%] [G loss: 2.534960]\n",
      "epoch:22 step:17191 [D loss: 0.372326, acc.: 80.47%] [G loss: 3.032813]\n",
      "epoch:22 step:17192 [D loss: 0.397771, acc.: 82.81%] [G loss: 2.738615]\n",
      "epoch:22 step:17193 [D loss: 0.252340, acc.: 89.84%] [G loss: 2.682600]\n",
      "epoch:22 step:17194 [D loss: 0.339276, acc.: 88.28%] [G loss: 2.794624]\n",
      "epoch:22 step:17195 [D loss: 0.364456, acc.: 84.38%] [G loss: 2.883627]\n",
      "epoch:22 step:17196 [D loss: 0.288475, acc.: 84.38%] [G loss: 3.668587]\n",
      "epoch:22 step:17197 [D loss: 0.257521, acc.: 89.84%] [G loss: 2.864628]\n",
      "epoch:22 step:17198 [D loss: 0.289847, acc.: 85.16%] [G loss: 2.786098]\n",
      "epoch:22 step:17199 [D loss: 0.382142, acc.: 82.03%] [G loss: 3.224088]\n",
      "epoch:22 step:17200 [D loss: 0.362691, acc.: 80.47%] [G loss: 3.277629]\n",
      "epoch:22 step:17201 [D loss: 0.256638, acc.: 89.06%] [G loss: 3.150516]\n",
      "epoch:22 step:17202 [D loss: 0.368171, acc.: 82.03%] [G loss: 2.965885]\n",
      "epoch:22 step:17203 [D loss: 0.343836, acc.: 87.50%] [G loss: 2.956396]\n",
      "epoch:22 step:17204 [D loss: 0.302609, acc.: 85.94%] [G loss: 2.512309]\n",
      "epoch:22 step:17205 [D loss: 0.355287, acc.: 82.81%] [G loss: 2.696832]\n",
      "epoch:22 step:17206 [D loss: 0.270163, acc.: 91.41%] [G loss: 2.584862]\n",
      "epoch:22 step:17207 [D loss: 0.408732, acc.: 82.81%] [G loss: 3.131498]\n",
      "epoch:22 step:17208 [D loss: 0.260141, acc.: 92.97%] [G loss: 2.848897]\n",
      "epoch:22 step:17209 [D loss: 0.420281, acc.: 80.47%] [G loss: 3.305770]\n",
      "epoch:22 step:17210 [D loss: 0.402321, acc.: 81.25%] [G loss: 2.805840]\n",
      "epoch:22 step:17211 [D loss: 0.356635, acc.: 84.38%] [G loss: 2.818660]\n",
      "epoch:22 step:17212 [D loss: 0.266982, acc.: 88.28%] [G loss: 3.063512]\n",
      "epoch:22 step:17213 [D loss: 0.433991, acc.: 78.12%] [G loss: 3.332847]\n",
      "epoch:22 step:17214 [D loss: 0.375217, acc.: 80.47%] [G loss: 3.027981]\n",
      "epoch:22 step:17215 [D loss: 0.331630, acc.: 83.59%] [G loss: 3.288172]\n",
      "epoch:22 step:17216 [D loss: 0.338706, acc.: 84.38%] [G loss: 3.210319]\n",
      "epoch:22 step:17217 [D loss: 0.276892, acc.: 87.50%] [G loss: 3.644603]\n",
      "epoch:22 step:17218 [D loss: 0.366522, acc.: 82.81%] [G loss: 2.839306]\n",
      "epoch:22 step:17219 [D loss: 0.392836, acc.: 83.59%] [G loss: 3.942989]\n",
      "epoch:22 step:17220 [D loss: 0.250708, acc.: 89.06%] [G loss: 3.511372]\n",
      "epoch:22 step:17221 [D loss: 0.328247, acc.: 83.59%] [G loss: 2.773856]\n",
      "epoch:22 step:17222 [D loss: 0.254455, acc.: 89.06%] [G loss: 3.990453]\n",
      "epoch:22 step:17223 [D loss: 0.359098, acc.: 84.38%] [G loss: 2.599818]\n",
      "epoch:22 step:17224 [D loss: 0.263016, acc.: 89.84%] [G loss: 3.574995]\n",
      "epoch:22 step:17225 [D loss: 0.412308, acc.: 81.25%] [G loss: 2.757249]\n",
      "epoch:22 step:17226 [D loss: 0.274923, acc.: 85.94%] [G loss: 3.333315]\n",
      "epoch:22 step:17227 [D loss: 0.255165, acc.: 86.72%] [G loss: 4.169786]\n",
      "epoch:22 step:17228 [D loss: 0.316481, acc.: 82.81%] [G loss: 5.148998]\n",
      "epoch:22 step:17229 [D loss: 0.335065, acc.: 85.16%] [G loss: 3.513854]\n",
      "epoch:22 step:17230 [D loss: 0.437020, acc.: 82.03%] [G loss: 2.842517]\n",
      "epoch:22 step:17231 [D loss: 0.232389, acc.: 92.19%] [G loss: 3.509641]\n",
      "epoch:22 step:17232 [D loss: 0.316720, acc.: 83.59%] [G loss: 4.271870]\n",
      "epoch:22 step:17233 [D loss: 0.326262, acc.: 85.94%] [G loss: 3.140130]\n",
      "epoch:22 step:17234 [D loss: 0.299776, acc.: 88.28%] [G loss: 2.917693]\n",
      "epoch:22 step:17235 [D loss: 0.251725, acc.: 89.06%] [G loss: 2.914525]\n",
      "epoch:22 step:17236 [D loss: 0.280608, acc.: 85.94%] [G loss: 3.044066]\n",
      "epoch:22 step:17237 [D loss: 0.308407, acc.: 85.94%] [G loss: 3.067533]\n",
      "epoch:22 step:17238 [D loss: 0.271136, acc.: 86.72%] [G loss: 2.726301]\n",
      "epoch:22 step:17239 [D loss: 0.240155, acc.: 87.50%] [G loss: 3.360256]\n",
      "epoch:22 step:17240 [D loss: 0.348865, acc.: 85.16%] [G loss: 4.596858]\n",
      "epoch:22 step:17241 [D loss: 0.242746, acc.: 92.19%] [G loss: 3.025388]\n",
      "epoch:22 step:17242 [D loss: 0.369176, acc.: 79.69%] [G loss: 4.126528]\n",
      "epoch:22 step:17243 [D loss: 0.212221, acc.: 89.06%] [G loss: 4.714887]\n",
      "epoch:22 step:17244 [D loss: 0.333893, acc.: 89.06%] [G loss: 2.673013]\n",
      "epoch:22 step:17245 [D loss: 0.449773, acc.: 78.91%] [G loss: 2.866408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17246 [D loss: 0.391085, acc.: 84.38%] [G loss: 3.388253]\n",
      "epoch:22 step:17247 [D loss: 0.370672, acc.: 84.38%] [G loss: 2.974035]\n",
      "epoch:22 step:17248 [D loss: 0.379284, acc.: 83.59%] [G loss: 2.982808]\n",
      "epoch:22 step:17249 [D loss: 0.209179, acc.: 93.75%] [G loss: 3.175720]\n",
      "epoch:22 step:17250 [D loss: 0.288245, acc.: 90.62%] [G loss: 3.785491]\n",
      "epoch:22 step:17251 [D loss: 0.406816, acc.: 82.03%] [G loss: 2.537594]\n",
      "epoch:22 step:17252 [D loss: 0.457029, acc.: 82.81%] [G loss: 3.243796]\n",
      "epoch:22 step:17253 [D loss: 0.592977, acc.: 73.44%] [G loss: 2.544942]\n",
      "epoch:22 step:17254 [D loss: 0.420897, acc.: 80.47%] [G loss: 2.347432]\n",
      "epoch:22 step:17255 [D loss: 0.277435, acc.: 85.94%] [G loss: 3.675033]\n",
      "epoch:22 step:17256 [D loss: 0.320708, acc.: 82.81%] [G loss: 3.195480]\n",
      "epoch:22 step:17257 [D loss: 0.275979, acc.: 90.62%] [G loss: 5.199850]\n",
      "epoch:22 step:17258 [D loss: 0.366132, acc.: 81.25%] [G loss: 6.112797]\n",
      "epoch:22 step:17259 [D loss: 0.289246, acc.: 87.50%] [G loss: 4.138484]\n",
      "epoch:22 step:17260 [D loss: 0.268169, acc.: 86.72%] [G loss: 3.753962]\n",
      "epoch:22 step:17261 [D loss: 0.339236, acc.: 82.03%] [G loss: 3.275253]\n",
      "epoch:22 step:17262 [D loss: 0.351435, acc.: 83.59%] [G loss: 3.174325]\n",
      "epoch:22 step:17263 [D loss: 0.341585, acc.: 85.16%] [G loss: 3.614017]\n",
      "epoch:22 step:17264 [D loss: 0.273101, acc.: 88.28%] [G loss: 2.886173]\n",
      "epoch:22 step:17265 [D loss: 0.317666, acc.: 81.25%] [G loss: 3.234176]\n",
      "epoch:22 step:17266 [D loss: 0.485064, acc.: 78.12%] [G loss: 2.963830]\n",
      "epoch:22 step:17267 [D loss: 0.300740, acc.: 84.38%] [G loss: 2.894943]\n",
      "epoch:22 step:17268 [D loss: 0.248677, acc.: 89.84%] [G loss: 3.447758]\n",
      "epoch:22 step:17269 [D loss: 0.372793, acc.: 81.25%] [G loss: 3.116439]\n",
      "epoch:22 step:17270 [D loss: 0.332278, acc.: 85.16%] [G loss: 2.708940]\n",
      "epoch:22 step:17271 [D loss: 0.344003, acc.: 82.81%] [G loss: 2.519560]\n",
      "epoch:22 step:17272 [D loss: 0.332084, acc.: 82.81%] [G loss: 4.954448]\n",
      "epoch:22 step:17273 [D loss: 0.276957, acc.: 87.50%] [G loss: 3.810044]\n",
      "epoch:22 step:17274 [D loss: 0.224154, acc.: 91.41%] [G loss: 4.910441]\n",
      "epoch:22 step:17275 [D loss: 0.366215, acc.: 82.81%] [G loss: 4.547348]\n",
      "epoch:22 step:17276 [D loss: 0.354911, acc.: 84.38%] [G loss: 2.838995]\n",
      "epoch:22 step:17277 [D loss: 0.328413, acc.: 87.50%] [G loss: 2.904320]\n",
      "epoch:22 step:17278 [D loss: 0.220583, acc.: 89.06%] [G loss: 3.135396]\n",
      "epoch:22 step:17279 [D loss: 0.324855, acc.: 87.50%] [G loss: 3.347550]\n",
      "epoch:22 step:17280 [D loss: 0.295708, acc.: 85.16%] [G loss: 3.956289]\n",
      "epoch:22 step:17281 [D loss: 0.412514, acc.: 82.03%] [G loss: 2.527731]\n",
      "epoch:22 step:17282 [D loss: 0.310765, acc.: 86.72%] [G loss: 2.632562]\n",
      "epoch:22 step:17283 [D loss: 0.256116, acc.: 87.50%] [G loss: 3.458109]\n",
      "epoch:22 step:17284 [D loss: 0.382603, acc.: 80.47%] [G loss: 3.690879]\n",
      "epoch:22 step:17285 [D loss: 0.344497, acc.: 84.38%] [G loss: 2.968875]\n",
      "epoch:22 step:17286 [D loss: 0.288259, acc.: 86.72%] [G loss: 3.677433]\n",
      "epoch:22 step:17287 [D loss: 0.258988, acc.: 89.84%] [G loss: 3.794982]\n",
      "epoch:22 step:17288 [D loss: 0.281949, acc.: 86.72%] [G loss: 4.709003]\n",
      "epoch:22 step:17289 [D loss: 0.269555, acc.: 89.84%] [G loss: 3.027209]\n",
      "epoch:22 step:17290 [D loss: 0.282821, acc.: 88.28%] [G loss: 4.175362]\n",
      "epoch:22 step:17291 [D loss: 0.346911, acc.: 83.59%] [G loss: 3.662367]\n",
      "epoch:22 step:17292 [D loss: 0.392045, acc.: 82.81%] [G loss: 3.495964]\n",
      "epoch:22 step:17293 [D loss: 0.393970, acc.: 82.03%] [G loss: 3.769314]\n",
      "epoch:22 step:17294 [D loss: 0.248677, acc.: 90.62%] [G loss: 4.037026]\n",
      "epoch:22 step:17295 [D loss: 0.344605, acc.: 85.16%] [G loss: 3.305617]\n",
      "epoch:22 step:17296 [D loss: 0.300588, acc.: 85.94%] [G loss: 2.952878]\n",
      "epoch:22 step:17297 [D loss: 0.340129, acc.: 84.38%] [G loss: 2.733850]\n",
      "epoch:22 step:17298 [D loss: 0.347858, acc.: 85.94%] [G loss: 5.448518]\n",
      "epoch:22 step:17299 [D loss: 0.353787, acc.: 84.38%] [G loss: 3.323881]\n",
      "epoch:22 step:17300 [D loss: 0.348097, acc.: 85.16%] [G loss: 4.616040]\n",
      "epoch:22 step:17301 [D loss: 0.244509, acc.: 90.62%] [G loss: 5.711153]\n",
      "epoch:22 step:17302 [D loss: 0.329242, acc.: 84.38%] [G loss: 3.716905]\n",
      "epoch:22 step:17303 [D loss: 0.231527, acc.: 90.62%] [G loss: 3.725605]\n",
      "epoch:22 step:17304 [D loss: 0.301478, acc.: 88.28%] [G loss: 2.451077]\n",
      "epoch:22 step:17305 [D loss: 0.245562, acc.: 87.50%] [G loss: 3.682086]\n",
      "epoch:22 step:17306 [D loss: 0.457189, acc.: 83.59%] [G loss: 2.387338]\n",
      "epoch:22 step:17307 [D loss: 0.360774, acc.: 81.25%] [G loss: 3.427732]\n",
      "epoch:22 step:17308 [D loss: 0.340684, acc.: 85.16%] [G loss: 4.567553]\n",
      "epoch:22 step:17309 [D loss: 0.294217, acc.: 84.38%] [G loss: 4.698466]\n",
      "epoch:22 step:17310 [D loss: 0.322526, acc.: 84.38%] [G loss: 3.391643]\n",
      "epoch:22 step:17311 [D loss: 0.351391, acc.: 84.38%] [G loss: 3.286854]\n",
      "epoch:22 step:17312 [D loss: 0.311658, acc.: 82.03%] [G loss: 3.921605]\n",
      "epoch:22 step:17313 [D loss: 0.415092, acc.: 82.03%] [G loss: 4.938108]\n",
      "epoch:22 step:17314 [D loss: 0.424842, acc.: 79.69%] [G loss: 4.424981]\n",
      "epoch:22 step:17315 [D loss: 0.365135, acc.: 84.38%] [G loss: 4.529686]\n",
      "epoch:22 step:17316 [D loss: 0.270272, acc.: 88.28%] [G loss: 3.420934]\n",
      "epoch:22 step:17317 [D loss: 0.255906, acc.: 88.28%] [G loss: 4.053904]\n",
      "epoch:22 step:17318 [D loss: 0.339323, acc.: 85.16%] [G loss: 3.492329]\n",
      "epoch:22 step:17319 [D loss: 0.391334, acc.: 82.03%] [G loss: 7.062625]\n",
      "epoch:22 step:17320 [D loss: 0.609858, acc.: 79.69%] [G loss: 6.233734]\n",
      "epoch:22 step:17321 [D loss: 0.783502, acc.: 72.66%] [G loss: 4.183125]\n",
      "epoch:22 step:17322 [D loss: 0.627943, acc.: 75.78%] [G loss: 5.162140]\n",
      "epoch:22 step:17323 [D loss: 0.636820, acc.: 70.31%] [G loss: 2.722431]\n",
      "epoch:22 step:17324 [D loss: 0.406214, acc.: 81.25%] [G loss: 3.498359]\n",
      "epoch:22 step:17325 [D loss: 0.369471, acc.: 86.72%] [G loss: 3.682865]\n",
      "epoch:22 step:17326 [D loss: 0.384265, acc.: 81.25%] [G loss: 3.486150]\n",
      "epoch:22 step:17327 [D loss: 0.318502, acc.: 85.94%] [G loss: 2.796905]\n",
      "epoch:22 step:17328 [D loss: 0.329448, acc.: 83.59%] [G loss: 3.703464]\n",
      "epoch:22 step:17329 [D loss: 0.358971, acc.: 84.38%] [G loss: 3.468117]\n",
      "epoch:22 step:17330 [D loss: 0.400953, acc.: 82.81%] [G loss: 2.664001]\n",
      "epoch:22 step:17331 [D loss: 0.304848, acc.: 85.16%] [G loss: 3.408266]\n",
      "epoch:22 step:17332 [D loss: 0.240215, acc.: 89.06%] [G loss: 3.491922]\n",
      "epoch:22 step:17333 [D loss: 0.256541, acc.: 88.28%] [G loss: 2.479262]\n",
      "epoch:22 step:17334 [D loss: 0.322132, acc.: 86.72%] [G loss: 2.821022]\n",
      "epoch:22 step:17335 [D loss: 0.264292, acc.: 89.84%] [G loss: 3.808945]\n",
      "epoch:22 step:17336 [D loss: 0.274646, acc.: 85.94%] [G loss: 3.940406]\n",
      "epoch:22 step:17337 [D loss: 0.321428, acc.: 85.16%] [G loss: 2.964315]\n",
      "epoch:22 step:17338 [D loss: 0.270145, acc.: 89.84%] [G loss: 3.317575]\n",
      "epoch:22 step:17339 [D loss: 0.249331, acc.: 89.06%] [G loss: 3.889987]\n",
      "epoch:22 step:17340 [D loss: 0.320582, acc.: 86.72%] [G loss: 4.136289]\n",
      "epoch:22 step:17341 [D loss: 0.250247, acc.: 88.28%] [G loss: 4.988101]\n",
      "epoch:22 step:17342 [D loss: 0.380682, acc.: 82.81%] [G loss: 2.644576]\n",
      "epoch:22 step:17343 [D loss: 0.248560, acc.: 89.84%] [G loss: 3.122365]\n",
      "epoch:22 step:17344 [D loss: 0.227949, acc.: 91.41%] [G loss: 3.985419]\n",
      "epoch:22 step:17345 [D loss: 0.367840, acc.: 84.38%] [G loss: 3.157445]\n",
      "epoch:22 step:17346 [D loss: 0.385701, acc.: 80.47%] [G loss: 2.993749]\n",
      "epoch:22 step:17347 [D loss: 0.287887, acc.: 86.72%] [G loss: 3.139815]\n",
      "epoch:22 step:17348 [D loss: 0.349984, acc.: 82.03%] [G loss: 4.800777]\n",
      "epoch:22 step:17349 [D loss: 0.313632, acc.: 85.94%] [G loss: 3.604456]\n",
      "epoch:22 step:17350 [D loss: 0.567280, acc.: 72.66%] [G loss: 2.928191]\n",
      "epoch:22 step:17351 [D loss: 0.354742, acc.: 85.16%] [G loss: 4.391864]\n",
      "epoch:22 step:17352 [D loss: 0.467238, acc.: 75.78%] [G loss: 3.225201]\n",
      "epoch:22 step:17353 [D loss: 0.333192, acc.: 85.94%] [G loss: 2.435812]\n",
      "epoch:22 step:17354 [D loss: 0.311494, acc.: 86.72%] [G loss: 2.577634]\n",
      "epoch:22 step:17355 [D loss: 0.350526, acc.: 84.38%] [G loss: 2.847702]\n",
      "epoch:22 step:17356 [D loss: 0.311414, acc.: 83.59%] [G loss: 2.662526]\n",
      "epoch:22 step:17357 [D loss: 0.306693, acc.: 89.06%] [G loss: 3.642328]\n",
      "epoch:22 step:17358 [D loss: 0.267059, acc.: 90.62%] [G loss: 3.229685]\n",
      "epoch:22 step:17359 [D loss: 0.354243, acc.: 81.25%] [G loss: 2.361557]\n",
      "epoch:22 step:17360 [D loss: 0.231232, acc.: 93.75%] [G loss: 3.127142]\n",
      "epoch:22 step:17361 [D loss: 0.335967, acc.: 88.28%] [G loss: 3.200960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17362 [D loss: 0.414848, acc.: 82.03%] [G loss: 2.369649]\n",
      "epoch:22 step:17363 [D loss: 0.269276, acc.: 89.06%] [G loss: 2.955172]\n",
      "epoch:22 step:17364 [D loss: 0.333562, acc.: 82.81%] [G loss: 2.834791]\n",
      "epoch:22 step:17365 [D loss: 0.302135, acc.: 85.16%] [G loss: 2.315915]\n",
      "epoch:22 step:17366 [D loss: 0.344396, acc.: 84.38%] [G loss: 2.860402]\n",
      "epoch:22 step:17367 [D loss: 0.313553, acc.: 86.72%] [G loss: 2.710164]\n",
      "epoch:22 step:17368 [D loss: 0.342291, acc.: 84.38%] [G loss: 2.943727]\n",
      "epoch:22 step:17369 [D loss: 0.342637, acc.: 88.28%] [G loss: 3.482529]\n",
      "epoch:22 step:17370 [D loss: 0.424986, acc.: 82.03%] [G loss: 2.671805]\n",
      "epoch:22 step:17371 [D loss: 0.371969, acc.: 85.16%] [G loss: 2.649564]\n",
      "epoch:22 step:17372 [D loss: 0.442873, acc.: 75.00%] [G loss: 2.517116]\n",
      "epoch:22 step:17373 [D loss: 0.391388, acc.: 83.59%] [G loss: 3.069477]\n",
      "epoch:22 step:17374 [D loss: 0.404321, acc.: 82.81%] [G loss: 2.627313]\n",
      "epoch:22 step:17375 [D loss: 0.280531, acc.: 87.50%] [G loss: 2.653290]\n",
      "epoch:22 step:17376 [D loss: 0.305503, acc.: 88.28%] [G loss: 2.513594]\n",
      "epoch:22 step:17377 [D loss: 0.490694, acc.: 79.69%] [G loss: 2.782784]\n",
      "epoch:22 step:17378 [D loss: 0.258619, acc.: 89.84%] [G loss: 3.250929]\n",
      "epoch:22 step:17379 [D loss: 0.380058, acc.: 85.16%] [G loss: 5.625685]\n",
      "epoch:22 step:17380 [D loss: 0.329009, acc.: 85.16%] [G loss: 3.570377]\n",
      "epoch:22 step:17381 [D loss: 0.280870, acc.: 88.28%] [G loss: 5.068409]\n",
      "epoch:22 step:17382 [D loss: 0.221929, acc.: 89.06%] [G loss: 5.061427]\n",
      "epoch:22 step:17383 [D loss: 0.269278, acc.: 89.84%] [G loss: 5.246381]\n",
      "epoch:22 step:17384 [D loss: 0.385010, acc.: 84.38%] [G loss: 5.937228]\n",
      "epoch:22 step:17385 [D loss: 0.423052, acc.: 83.59%] [G loss: 4.211646]\n",
      "epoch:22 step:17386 [D loss: 0.241014, acc.: 91.41%] [G loss: 4.594805]\n",
      "epoch:22 step:17387 [D loss: 0.392295, acc.: 82.81%] [G loss: 3.810413]\n",
      "epoch:22 step:17388 [D loss: 0.388467, acc.: 82.03%] [G loss: 3.708031]\n",
      "epoch:22 step:17389 [D loss: 0.332415, acc.: 85.94%] [G loss: 2.899642]\n",
      "epoch:22 step:17390 [D loss: 0.524080, acc.: 74.22%] [G loss: 3.329738]\n",
      "epoch:22 step:17391 [D loss: 0.697473, acc.: 64.84%] [G loss: 3.531796]\n",
      "epoch:22 step:17392 [D loss: 0.309876, acc.: 85.94%] [G loss: 4.284586]\n",
      "epoch:22 step:17393 [D loss: 0.540297, acc.: 69.53%] [G loss: 4.841485]\n",
      "epoch:22 step:17394 [D loss: 0.333343, acc.: 87.50%] [G loss: 4.155395]\n",
      "epoch:22 step:17395 [D loss: 0.481091, acc.: 76.56%] [G loss: 4.763601]\n",
      "epoch:22 step:17396 [D loss: 0.324077, acc.: 85.16%] [G loss: 3.252121]\n",
      "epoch:22 step:17397 [D loss: 0.266328, acc.: 86.72%] [G loss: 4.250224]\n",
      "epoch:22 step:17398 [D loss: 0.377890, acc.: 84.38%] [G loss: 2.623042]\n",
      "epoch:22 step:17399 [D loss: 0.347023, acc.: 82.81%] [G loss: 2.495713]\n",
      "epoch:22 step:17400 [D loss: 0.268215, acc.: 85.94%] [G loss: 2.883892]\n",
      "epoch:22 step:17401 [D loss: 0.271087, acc.: 87.50%] [G loss: 3.206491]\n",
      "epoch:22 step:17402 [D loss: 0.285486, acc.: 85.16%] [G loss: 3.365514]\n",
      "epoch:22 step:17403 [D loss: 0.219056, acc.: 91.41%] [G loss: 5.252307]\n",
      "epoch:22 step:17404 [D loss: 0.313660, acc.: 84.38%] [G loss: 3.387860]\n",
      "epoch:22 step:17405 [D loss: 0.343315, acc.: 86.72%] [G loss: 2.701141]\n",
      "epoch:22 step:17406 [D loss: 0.406216, acc.: 87.50%] [G loss: 3.477282]\n",
      "epoch:22 step:17407 [D loss: 0.232444, acc.: 90.62%] [G loss: 2.668215]\n",
      "epoch:22 step:17408 [D loss: 0.340962, acc.: 85.16%] [G loss: 3.510154]\n",
      "epoch:22 step:17409 [D loss: 0.202552, acc.: 91.41%] [G loss: 4.276325]\n",
      "epoch:22 step:17410 [D loss: 0.412524, acc.: 80.47%] [G loss: 3.916503]\n",
      "epoch:22 step:17411 [D loss: 0.434914, acc.: 78.12%] [G loss: 4.974472]\n",
      "epoch:22 step:17412 [D loss: 0.206300, acc.: 92.97%] [G loss: 6.138062]\n",
      "epoch:22 step:17413 [D loss: 0.329343, acc.: 82.03%] [G loss: 5.244941]\n",
      "epoch:22 step:17414 [D loss: 0.218306, acc.: 92.97%] [G loss: 5.319958]\n",
      "epoch:22 step:17415 [D loss: 0.422559, acc.: 81.25%] [G loss: 3.555156]\n",
      "epoch:22 step:17416 [D loss: 0.247879, acc.: 89.84%] [G loss: 4.129116]\n",
      "epoch:22 step:17417 [D loss: 0.301952, acc.: 86.72%] [G loss: 3.912184]\n",
      "epoch:22 step:17418 [D loss: 0.323150, acc.: 89.84%] [G loss: 3.873569]\n",
      "epoch:22 step:17419 [D loss: 0.302175, acc.: 84.38%] [G loss: 4.916496]\n",
      "epoch:22 step:17420 [D loss: 0.212127, acc.: 91.41%] [G loss: 4.065681]\n",
      "epoch:22 step:17421 [D loss: 0.357639, acc.: 81.25%] [G loss: 4.236593]\n",
      "epoch:22 step:17422 [D loss: 0.444571, acc.: 78.12%] [G loss: 4.951837]\n",
      "epoch:22 step:17423 [D loss: 0.648897, acc.: 71.09%] [G loss: 3.264066]\n",
      "epoch:22 step:17424 [D loss: 0.370865, acc.: 86.72%] [G loss: 2.924863]\n",
      "epoch:22 step:17425 [D loss: 0.297811, acc.: 86.72%] [G loss: 3.936462]\n",
      "epoch:22 step:17426 [D loss: 0.314916, acc.: 85.16%] [G loss: 4.152832]\n",
      "epoch:22 step:17427 [D loss: 0.407358, acc.: 85.16%] [G loss: 4.300335]\n",
      "epoch:22 step:17428 [D loss: 0.400660, acc.: 82.81%] [G loss: 3.709629]\n",
      "epoch:22 step:17429 [D loss: 0.290559, acc.: 87.50%] [G loss: 3.989942]\n",
      "epoch:22 step:17430 [D loss: 0.280722, acc.: 88.28%] [G loss: 4.044190]\n",
      "epoch:22 step:17431 [D loss: 0.299585, acc.: 83.59%] [G loss: 3.417824]\n",
      "epoch:22 step:17432 [D loss: 0.257294, acc.: 86.72%] [G loss: 4.475262]\n",
      "epoch:22 step:17433 [D loss: 0.364507, acc.: 82.03%] [G loss: 2.389915]\n",
      "epoch:22 step:17434 [D loss: 0.349342, acc.: 84.38%] [G loss: 3.933529]\n",
      "epoch:22 step:17435 [D loss: 0.239745, acc.: 91.41%] [G loss: 3.171814]\n",
      "epoch:22 step:17436 [D loss: 0.248998, acc.: 89.84%] [G loss: 3.801510]\n",
      "epoch:22 step:17437 [D loss: 0.338902, acc.: 86.72%] [G loss: 2.810282]\n",
      "epoch:22 step:17438 [D loss: 0.272399, acc.: 89.84%] [G loss: 3.257861]\n",
      "epoch:22 step:17439 [D loss: 0.536694, acc.: 76.56%] [G loss: 2.801332]\n",
      "epoch:22 step:17440 [D loss: 0.297981, acc.: 85.16%] [G loss: 3.312672]\n",
      "epoch:22 step:17441 [D loss: 0.407980, acc.: 82.03%] [G loss: 3.209945]\n",
      "epoch:22 step:17442 [D loss: 0.355147, acc.: 87.50%] [G loss: 3.653647]\n",
      "epoch:22 step:17443 [D loss: 0.473561, acc.: 75.78%] [G loss: 2.566113]\n",
      "epoch:22 step:17444 [D loss: 0.308174, acc.: 89.06%] [G loss: 2.825911]\n",
      "epoch:22 step:17445 [D loss: 0.273781, acc.: 88.28%] [G loss: 3.084976]\n",
      "epoch:22 step:17446 [D loss: 0.353123, acc.: 82.03%] [G loss: 3.181718]\n",
      "epoch:22 step:17447 [D loss: 0.475369, acc.: 78.12%] [G loss: 3.240669]\n",
      "epoch:22 step:17448 [D loss: 0.346014, acc.: 86.72%] [G loss: 2.801703]\n",
      "epoch:22 step:17449 [D loss: 0.267232, acc.: 86.72%] [G loss: 2.989579]\n",
      "epoch:22 step:17450 [D loss: 0.314865, acc.: 87.50%] [G loss: 2.573294]\n",
      "epoch:22 step:17451 [D loss: 0.389697, acc.: 78.91%] [G loss: 3.876276]\n",
      "epoch:22 step:17452 [D loss: 0.301464, acc.: 89.06%] [G loss: 3.747338]\n",
      "epoch:22 step:17453 [D loss: 0.224367, acc.: 91.41%] [G loss: 3.978776]\n",
      "epoch:22 step:17454 [D loss: 0.393337, acc.: 82.81%] [G loss: 3.229732]\n",
      "epoch:22 step:17455 [D loss: 0.362597, acc.: 82.81%] [G loss: 3.599753]\n",
      "epoch:22 step:17456 [D loss: 0.284007, acc.: 87.50%] [G loss: 4.946235]\n",
      "epoch:22 step:17457 [D loss: 0.440767, acc.: 80.47%] [G loss: 4.638224]\n",
      "epoch:22 step:17458 [D loss: 0.710995, acc.: 67.19%] [G loss: 5.414721]\n",
      "epoch:22 step:17459 [D loss: 0.818527, acc.: 67.19%] [G loss: 7.957424]\n",
      "epoch:22 step:17460 [D loss: 1.954909, acc.: 57.81%] [G loss: 8.053535]\n",
      "epoch:22 step:17461 [D loss: 2.312946, acc.: 53.91%] [G loss: 3.561834]\n",
      "epoch:22 step:17462 [D loss: 0.259030, acc.: 85.94%] [G loss: 5.205177]\n",
      "epoch:22 step:17463 [D loss: 0.449414, acc.: 81.25%] [G loss: 5.569034]\n",
      "epoch:22 step:17464 [D loss: 0.391384, acc.: 79.69%] [G loss: 3.655253]\n",
      "epoch:22 step:17465 [D loss: 0.361503, acc.: 82.81%] [G loss: 2.466487]\n",
      "epoch:22 step:17466 [D loss: 0.608386, acc.: 71.09%] [G loss: 2.890738]\n",
      "epoch:22 step:17467 [D loss: 0.425701, acc.: 80.47%] [G loss: 3.626101]\n",
      "epoch:22 step:17468 [D loss: 0.310976, acc.: 84.38%] [G loss: 3.238639]\n",
      "epoch:22 step:17469 [D loss: 0.362297, acc.: 84.38%] [G loss: 2.701577]\n",
      "epoch:22 step:17470 [D loss: 0.450046, acc.: 74.22%] [G loss: 2.630911]\n",
      "epoch:22 step:17471 [D loss: 0.347480, acc.: 82.81%] [G loss: 2.925409]\n",
      "epoch:22 step:17472 [D loss: 0.390754, acc.: 82.81%] [G loss: 3.193955]\n",
      "epoch:22 step:17473 [D loss: 0.415805, acc.: 83.59%] [G loss: 3.280247]\n",
      "epoch:22 step:17474 [D loss: 0.280884, acc.: 90.62%] [G loss: 2.605494]\n",
      "epoch:22 step:17475 [D loss: 0.346177, acc.: 82.03%] [G loss: 2.726638]\n",
      "epoch:22 step:17476 [D loss: 0.396956, acc.: 86.72%] [G loss: 2.508215]\n",
      "epoch:22 step:17477 [D loss: 0.497494, acc.: 78.12%] [G loss: 2.202467]\n",
      "epoch:22 step:17478 [D loss: 0.370054, acc.: 84.38%] [G loss: 2.814170]\n",
      "epoch:22 step:17479 [D loss: 0.370773, acc.: 86.72%] [G loss: 2.544937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17480 [D loss: 0.445676, acc.: 81.25%] [G loss: 2.466321]\n",
      "epoch:22 step:17481 [D loss: 0.439003, acc.: 78.12%] [G loss: 2.440310]\n",
      "epoch:22 step:17482 [D loss: 0.438742, acc.: 80.47%] [G loss: 2.891805]\n",
      "epoch:22 step:17483 [D loss: 0.416644, acc.: 82.81%] [G loss: 2.559224]\n",
      "epoch:22 step:17484 [D loss: 0.245378, acc.: 92.19%] [G loss: 2.547401]\n",
      "epoch:22 step:17485 [D loss: 0.257561, acc.: 91.41%] [G loss: 3.516530]\n",
      "epoch:22 step:17486 [D loss: 0.218489, acc.: 91.41%] [G loss: 2.579765]\n",
      "epoch:22 step:17487 [D loss: 0.399065, acc.: 76.56%] [G loss: 3.247100]\n",
      "epoch:22 step:17488 [D loss: 0.424533, acc.: 78.91%] [G loss: 2.473131]\n",
      "epoch:22 step:17489 [D loss: 0.231835, acc.: 90.62%] [G loss: 2.990825]\n",
      "epoch:22 step:17490 [D loss: 0.257375, acc.: 88.28%] [G loss: 2.831975]\n",
      "epoch:22 step:17491 [D loss: 0.342121, acc.: 87.50%] [G loss: 2.483101]\n",
      "epoch:22 step:17492 [D loss: 0.245167, acc.: 90.62%] [G loss: 3.660679]\n",
      "epoch:22 step:17493 [D loss: 0.270698, acc.: 90.62%] [G loss: 3.741919]\n",
      "epoch:22 step:17494 [D loss: 0.304554, acc.: 88.28%] [G loss: 2.669194]\n",
      "epoch:22 step:17495 [D loss: 0.230251, acc.: 92.19%] [G loss: 3.230741]\n",
      "epoch:22 step:17496 [D loss: 0.325936, acc.: 86.72%] [G loss: 2.755809]\n",
      "epoch:22 step:17497 [D loss: 0.458847, acc.: 75.78%] [G loss: 2.791019]\n",
      "epoch:22 step:17498 [D loss: 0.358176, acc.: 85.94%] [G loss: 2.814310]\n",
      "epoch:22 step:17499 [D loss: 0.368475, acc.: 81.25%] [G loss: 3.016879]\n",
      "epoch:22 step:17500 [D loss: 0.397101, acc.: 81.25%] [G loss: 2.736345]\n",
      "epoch:22 step:17501 [D loss: 0.296476, acc.: 84.38%] [G loss: 2.854959]\n",
      "epoch:22 step:17502 [D loss: 0.316963, acc.: 85.94%] [G loss: 2.467701]\n",
      "epoch:22 step:17503 [D loss: 0.267553, acc.: 88.28%] [G loss: 3.054407]\n",
      "epoch:22 step:17504 [D loss: 0.369261, acc.: 82.81%] [G loss: 5.843650]\n",
      "epoch:22 step:17505 [D loss: 0.446263, acc.: 76.56%] [G loss: 2.454392]\n",
      "epoch:22 step:17506 [D loss: 0.343695, acc.: 79.69%] [G loss: 3.907465]\n",
      "epoch:22 step:17507 [D loss: 0.275952, acc.: 89.06%] [G loss: 3.331442]\n",
      "epoch:22 step:17508 [D loss: 0.388890, acc.: 83.59%] [G loss: 2.562558]\n",
      "epoch:22 step:17509 [D loss: 0.258989, acc.: 89.06%] [G loss: 3.268710]\n",
      "epoch:22 step:17510 [D loss: 0.322838, acc.: 87.50%] [G loss: 4.463954]\n",
      "epoch:22 step:17511 [D loss: 0.204581, acc.: 92.97%] [G loss: 5.117028]\n",
      "epoch:22 step:17512 [D loss: 0.328786, acc.: 86.72%] [G loss: 3.237674]\n",
      "epoch:22 step:17513 [D loss: 0.298342, acc.: 86.72%] [G loss: 4.234578]\n",
      "epoch:22 step:17514 [D loss: 0.320680, acc.: 85.16%] [G loss: 2.530885]\n",
      "epoch:22 step:17515 [D loss: 0.263931, acc.: 92.19%] [G loss: 2.305905]\n",
      "epoch:22 step:17516 [D loss: 0.259135, acc.: 89.06%] [G loss: 2.726203]\n",
      "epoch:22 step:17517 [D loss: 0.328266, acc.: 85.16%] [G loss: 3.288304]\n",
      "epoch:22 step:17518 [D loss: 0.287352, acc.: 88.28%] [G loss: 4.093618]\n",
      "epoch:22 step:17519 [D loss: 0.377058, acc.: 81.25%] [G loss: 2.656025]\n",
      "epoch:22 step:17520 [D loss: 0.320205, acc.: 85.94%] [G loss: 3.004174]\n",
      "epoch:22 step:17521 [D loss: 0.280263, acc.: 89.06%] [G loss: 3.037531]\n",
      "epoch:22 step:17522 [D loss: 0.327962, acc.: 84.38%] [G loss: 2.696975]\n",
      "epoch:22 step:17523 [D loss: 0.205130, acc.: 92.19%] [G loss: 3.411969]\n",
      "epoch:22 step:17524 [D loss: 0.254549, acc.: 89.84%] [G loss: 2.095113]\n",
      "epoch:22 step:17525 [D loss: 0.389862, acc.: 78.91%] [G loss: 2.420644]\n",
      "epoch:22 step:17526 [D loss: 0.317296, acc.: 88.28%] [G loss: 3.075102]\n",
      "epoch:22 step:17527 [D loss: 0.248097, acc.: 88.28%] [G loss: 3.012842]\n",
      "epoch:22 step:17528 [D loss: 0.323222, acc.: 85.94%] [G loss: 2.335377]\n",
      "epoch:22 step:17529 [D loss: 0.286286, acc.: 86.72%] [G loss: 3.400856]\n",
      "epoch:22 step:17530 [D loss: 0.340818, acc.: 85.16%] [G loss: 2.926292]\n",
      "epoch:22 step:17531 [D loss: 0.269240, acc.: 90.62%] [G loss: 2.940437]\n",
      "epoch:22 step:17532 [D loss: 0.354996, acc.: 82.03%] [G loss: 2.293738]\n",
      "epoch:22 step:17533 [D loss: 0.375449, acc.: 85.94%] [G loss: 2.481929]\n",
      "epoch:22 step:17534 [D loss: 0.395796, acc.: 84.38%] [G loss: 2.326376]\n",
      "epoch:22 step:17535 [D loss: 0.300328, acc.: 85.16%] [G loss: 2.528958]\n",
      "epoch:22 step:17536 [D loss: 0.388381, acc.: 82.81%] [G loss: 3.060475]\n",
      "epoch:22 step:17537 [D loss: 0.325120, acc.: 83.59%] [G loss: 3.030479]\n",
      "epoch:22 step:17538 [D loss: 0.303377, acc.: 85.94%] [G loss: 6.267008]\n",
      "epoch:22 step:17539 [D loss: 0.368724, acc.: 82.81%] [G loss: 3.385712]\n",
      "epoch:22 step:17540 [D loss: 0.224483, acc.: 89.84%] [G loss: 4.495283]\n",
      "epoch:22 step:17541 [D loss: 0.342462, acc.: 85.94%] [G loss: 3.100643]\n",
      "epoch:22 step:17542 [D loss: 0.343239, acc.: 83.59%] [G loss: 2.544191]\n",
      "epoch:22 step:17543 [D loss: 0.365262, acc.: 82.81%] [G loss: 3.286293]\n",
      "epoch:22 step:17544 [D loss: 0.315455, acc.: 86.72%] [G loss: 2.917861]\n",
      "epoch:22 step:17545 [D loss: 0.266595, acc.: 92.19%] [G loss: 3.066092]\n",
      "epoch:22 step:17546 [D loss: 0.259721, acc.: 90.62%] [G loss: 3.936326]\n",
      "epoch:22 step:17547 [D loss: 0.343894, acc.: 83.59%] [G loss: 3.409849]\n",
      "epoch:22 step:17548 [D loss: 0.406453, acc.: 80.47%] [G loss: 3.412980]\n",
      "epoch:22 step:17549 [D loss: 0.428812, acc.: 79.69%] [G loss: 3.551412]\n",
      "epoch:22 step:17550 [D loss: 0.364410, acc.: 82.81%] [G loss: 3.390504]\n",
      "epoch:22 step:17551 [D loss: 0.331322, acc.: 90.62%] [G loss: 3.275795]\n",
      "epoch:22 step:17552 [D loss: 0.344460, acc.: 81.25%] [G loss: 2.905282]\n",
      "epoch:22 step:17553 [D loss: 0.292378, acc.: 89.84%] [G loss: 3.174058]\n",
      "epoch:22 step:17554 [D loss: 0.315351, acc.: 85.94%] [G loss: 2.705773]\n",
      "epoch:22 step:17555 [D loss: 0.385944, acc.: 82.03%] [G loss: 2.888532]\n",
      "epoch:22 step:17556 [D loss: 0.298878, acc.: 86.72%] [G loss: 3.805541]\n",
      "epoch:22 step:17557 [D loss: 0.236422, acc.: 90.62%] [G loss: 2.412063]\n",
      "epoch:22 step:17558 [D loss: 0.263507, acc.: 91.41%] [G loss: 3.344267]\n",
      "epoch:22 step:17559 [D loss: 0.320459, acc.: 86.72%] [G loss: 3.252319]\n",
      "epoch:22 step:17560 [D loss: 0.328981, acc.: 84.38%] [G loss: 2.792217]\n",
      "epoch:22 step:17561 [D loss: 0.406643, acc.: 82.81%] [G loss: 3.636146]\n",
      "epoch:22 step:17562 [D loss: 0.441741, acc.: 82.03%] [G loss: 2.483045]\n",
      "epoch:22 step:17563 [D loss: 0.293899, acc.: 86.72%] [G loss: 2.895166]\n",
      "epoch:22 step:17564 [D loss: 0.450556, acc.: 78.91%] [G loss: 3.722878]\n",
      "epoch:22 step:17565 [D loss: 0.466314, acc.: 79.69%] [G loss: 2.802607]\n",
      "epoch:22 step:17566 [D loss: 0.273944, acc.: 91.41%] [G loss: 3.771905]\n",
      "epoch:22 step:17567 [D loss: 0.297413, acc.: 85.16%] [G loss: 2.717580]\n",
      "epoch:22 step:17568 [D loss: 0.374814, acc.: 78.12%] [G loss: 2.303556]\n",
      "epoch:22 step:17569 [D loss: 0.297731, acc.: 85.94%] [G loss: 2.868497]\n",
      "epoch:22 step:17570 [D loss: 0.361228, acc.: 83.59%] [G loss: 2.492195]\n",
      "epoch:22 step:17571 [D loss: 0.333105, acc.: 82.03%] [G loss: 2.765226]\n",
      "epoch:22 step:17572 [D loss: 0.304909, acc.: 87.50%] [G loss: 2.539835]\n",
      "epoch:22 step:17573 [D loss: 0.337386, acc.: 87.50%] [G loss: 2.390310]\n",
      "epoch:22 step:17574 [D loss: 0.307975, acc.: 85.94%] [G loss: 2.299231]\n",
      "epoch:22 step:17575 [D loss: 0.304721, acc.: 85.16%] [G loss: 2.464395]\n",
      "epoch:22 step:17576 [D loss: 0.353384, acc.: 85.94%] [G loss: 2.975237]\n",
      "epoch:22 step:17577 [D loss: 0.384791, acc.: 83.59%] [G loss: 4.983241]\n",
      "epoch:22 step:17578 [D loss: 0.317307, acc.: 85.94%] [G loss: 4.653803]\n",
      "epoch:22 step:17579 [D loss: 0.294914, acc.: 86.72%] [G loss: 3.638527]\n",
      "epoch:22 step:17580 [D loss: 0.320586, acc.: 87.50%] [G loss: 3.318052]\n",
      "epoch:22 step:17581 [D loss: 0.377827, acc.: 85.16%] [G loss: 3.816207]\n",
      "epoch:22 step:17582 [D loss: 0.253150, acc.: 89.84%] [G loss: 2.685668]\n",
      "epoch:22 step:17583 [D loss: 0.295724, acc.: 85.94%] [G loss: 3.620562]\n",
      "epoch:22 step:17584 [D loss: 0.281727, acc.: 85.94%] [G loss: 3.186432]\n",
      "epoch:22 step:17585 [D loss: 0.238539, acc.: 91.41%] [G loss: 4.136043]\n",
      "epoch:22 step:17586 [D loss: 0.237266, acc.: 92.19%] [G loss: 2.924706]\n",
      "epoch:22 step:17587 [D loss: 0.332881, acc.: 85.94%] [G loss: 2.705260]\n",
      "epoch:22 step:17588 [D loss: 0.449998, acc.: 77.34%] [G loss: 3.028225]\n",
      "epoch:22 step:17589 [D loss: 0.312232, acc.: 86.72%] [G loss: 2.956222]\n",
      "epoch:22 step:17590 [D loss: 0.468234, acc.: 78.91%] [G loss: 3.105248]\n",
      "epoch:22 step:17591 [D loss: 0.403784, acc.: 84.38%] [G loss: 3.610519]\n",
      "epoch:22 step:17592 [D loss: 0.263215, acc.: 88.28%] [G loss: 2.905286]\n",
      "epoch:22 step:17593 [D loss: 0.362667, acc.: 81.25%] [G loss: 2.398232]\n",
      "epoch:22 step:17594 [D loss: 0.375882, acc.: 85.16%] [G loss: 2.876957]\n",
      "epoch:22 step:17595 [D loss: 0.378865, acc.: 83.59%] [G loss: 2.809408]\n",
      "epoch:22 step:17596 [D loss: 0.463352, acc.: 76.56%] [G loss: 2.443779]\n",
      "epoch:22 step:17597 [D loss: 0.304524, acc.: 85.16%] [G loss: 2.566843]\n",
      "epoch:22 step:17598 [D loss: 0.322226, acc.: 85.16%] [G loss: 2.670412]\n",
      "epoch:22 step:17599 [D loss: 0.358578, acc.: 83.59%] [G loss: 2.655511]\n",
      "epoch:22 step:17600 [D loss: 0.341466, acc.: 85.94%] [G loss: 2.902283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17601 [D loss: 0.294895, acc.: 87.50%] [G loss: 2.874999]\n",
      "epoch:22 step:17602 [D loss: 0.303915, acc.: 85.16%] [G loss: 2.791275]\n",
      "epoch:22 step:17603 [D loss: 0.391698, acc.: 82.03%] [G loss: 3.153893]\n",
      "epoch:22 step:17604 [D loss: 0.313765, acc.: 89.06%] [G loss: 2.419677]\n",
      "epoch:22 step:17605 [D loss: 0.375184, acc.: 82.03%] [G loss: 2.815858]\n",
      "epoch:22 step:17606 [D loss: 0.238851, acc.: 88.28%] [G loss: 4.326931]\n",
      "epoch:22 step:17607 [D loss: 0.373882, acc.: 79.69%] [G loss: 3.383979]\n",
      "epoch:22 step:17608 [D loss: 0.257287, acc.: 90.62%] [G loss: 2.184735]\n",
      "epoch:22 step:17609 [D loss: 0.241163, acc.: 89.84%] [G loss: 3.976400]\n",
      "epoch:22 step:17610 [D loss: 0.397041, acc.: 75.78%] [G loss: 3.308066]\n",
      "epoch:22 step:17611 [D loss: 0.316515, acc.: 88.28%] [G loss: 4.112541]\n",
      "epoch:22 step:17612 [D loss: 0.356368, acc.: 84.38%] [G loss: 5.087967]\n",
      "epoch:22 step:17613 [D loss: 0.278737, acc.: 88.28%] [G loss: 2.910589]\n",
      "epoch:22 step:17614 [D loss: 0.357673, acc.: 82.81%] [G loss: 3.001795]\n",
      "epoch:22 step:17615 [D loss: 0.279359, acc.: 88.28%] [G loss: 3.126164]\n",
      "epoch:22 step:17616 [D loss: 0.341947, acc.: 85.94%] [G loss: 3.633062]\n",
      "epoch:22 step:17617 [D loss: 0.309398, acc.: 87.50%] [G loss: 2.965074]\n",
      "epoch:22 step:17618 [D loss: 0.426598, acc.: 80.47%] [G loss: 2.358835]\n",
      "epoch:22 step:17619 [D loss: 0.338818, acc.: 83.59%] [G loss: 3.134881]\n",
      "epoch:22 step:17620 [D loss: 0.321590, acc.: 86.72%] [G loss: 2.415308]\n",
      "epoch:22 step:17621 [D loss: 0.367766, acc.: 81.25%] [G loss: 2.888914]\n",
      "epoch:22 step:17622 [D loss: 0.329807, acc.: 85.16%] [G loss: 3.371039]\n",
      "epoch:22 step:17623 [D loss: 0.408095, acc.: 80.47%] [G loss: 2.586879]\n",
      "epoch:22 step:17624 [D loss: 0.250225, acc.: 89.06%] [G loss: 3.452276]\n",
      "epoch:22 step:17625 [D loss: 0.320813, acc.: 89.06%] [G loss: 4.632592]\n",
      "epoch:22 step:17626 [D loss: 0.379463, acc.: 83.59%] [G loss: 2.950756]\n",
      "epoch:22 step:17627 [D loss: 0.222621, acc.: 90.62%] [G loss: 4.389523]\n",
      "epoch:22 step:17628 [D loss: 0.263202, acc.: 89.06%] [G loss: 2.677798]\n",
      "epoch:22 step:17629 [D loss: 0.243586, acc.: 86.72%] [G loss: 3.830649]\n",
      "epoch:22 step:17630 [D loss: 0.308905, acc.: 85.94%] [G loss: 3.068010]\n",
      "epoch:22 step:17631 [D loss: 0.274195, acc.: 88.28%] [G loss: 3.448159]\n",
      "epoch:22 step:17632 [D loss: 0.333134, acc.: 82.03%] [G loss: 3.694596]\n",
      "epoch:22 step:17633 [D loss: 0.206701, acc.: 90.62%] [G loss: 4.537521]\n",
      "epoch:22 step:17634 [D loss: 0.230899, acc.: 90.62%] [G loss: 3.075991]\n",
      "epoch:22 step:17635 [D loss: 0.205437, acc.: 92.97%] [G loss: 3.916951]\n",
      "epoch:22 step:17636 [D loss: 0.354029, acc.: 83.59%] [G loss: 3.531562]\n",
      "epoch:22 step:17637 [D loss: 0.268208, acc.: 86.72%] [G loss: 2.862666]\n",
      "epoch:22 step:17638 [D loss: 0.362412, acc.: 81.25%] [G loss: 3.238410]\n",
      "epoch:22 step:17639 [D loss: 0.291050, acc.: 87.50%] [G loss: 3.184054]\n",
      "epoch:22 step:17640 [D loss: 0.279247, acc.: 89.06%] [G loss: 2.745866]\n",
      "epoch:22 step:17641 [D loss: 0.340403, acc.: 85.16%] [G loss: 3.016653]\n",
      "epoch:22 step:17642 [D loss: 0.413771, acc.: 80.47%] [G loss: 3.482159]\n",
      "epoch:22 step:17643 [D loss: 0.290919, acc.: 87.50%] [G loss: 4.267905]\n",
      "epoch:22 step:17644 [D loss: 0.297428, acc.: 85.94%] [G loss: 3.817096]\n",
      "epoch:22 step:17645 [D loss: 0.307147, acc.: 87.50%] [G loss: 4.041933]\n",
      "epoch:22 step:17646 [D loss: 0.377750, acc.: 82.03%] [G loss: 3.139564]\n",
      "epoch:22 step:17647 [D loss: 0.363066, acc.: 85.94%] [G loss: 2.595634]\n",
      "epoch:22 step:17648 [D loss: 0.306875, acc.: 88.28%] [G loss: 2.752626]\n",
      "epoch:22 step:17649 [D loss: 0.335451, acc.: 80.47%] [G loss: 4.644192]\n",
      "epoch:22 step:17650 [D loss: 0.381936, acc.: 78.91%] [G loss: 3.261192]\n",
      "epoch:22 step:17651 [D loss: 0.249508, acc.: 87.50%] [G loss: 6.259097]\n",
      "epoch:22 step:17652 [D loss: 0.424644, acc.: 78.91%] [G loss: 2.215167]\n",
      "epoch:22 step:17653 [D loss: 0.306350, acc.: 87.50%] [G loss: 3.426512]\n",
      "epoch:22 step:17654 [D loss: 0.385806, acc.: 79.69%] [G loss: 2.637096]\n",
      "epoch:22 step:17655 [D loss: 0.340686, acc.: 85.16%] [G loss: 3.545279]\n",
      "epoch:22 step:17656 [D loss: 0.454696, acc.: 81.25%] [G loss: 8.587471]\n",
      "epoch:22 step:17657 [D loss: 0.788679, acc.: 73.44%] [G loss: 7.158948]\n",
      "epoch:22 step:17658 [D loss: 0.980152, acc.: 65.62%] [G loss: 5.471066]\n",
      "epoch:22 step:17659 [D loss: 0.669084, acc.: 82.03%] [G loss: 5.777331]\n",
      "epoch:22 step:17660 [D loss: 0.292807, acc.: 84.38%] [G loss: 6.254606]\n",
      "epoch:22 step:17661 [D loss: 0.505938, acc.: 80.47%] [G loss: 2.601874]\n",
      "epoch:22 step:17662 [D loss: 0.349260, acc.: 85.16%] [G loss: 3.121054]\n",
      "epoch:22 step:17663 [D loss: 0.369101, acc.: 85.16%] [G loss: 2.728518]\n",
      "epoch:22 step:17664 [D loss: 0.416672, acc.: 81.25%] [G loss: 3.105814]\n",
      "epoch:22 step:17665 [D loss: 0.411110, acc.: 81.25%] [G loss: 2.823321]\n",
      "epoch:22 step:17666 [D loss: 0.333568, acc.: 85.16%] [G loss: 2.649651]\n",
      "epoch:22 step:17667 [D loss: 0.385678, acc.: 83.59%] [G loss: 3.277036]\n",
      "epoch:22 step:17668 [D loss: 0.412390, acc.: 81.25%] [G loss: 2.753041]\n",
      "epoch:22 step:17669 [D loss: 0.287834, acc.: 88.28%] [G loss: 2.675022]\n",
      "epoch:22 step:17670 [D loss: 0.308089, acc.: 85.16%] [G loss: 2.790430]\n",
      "epoch:22 step:17671 [D loss: 0.352375, acc.: 83.59%] [G loss: 2.358352]\n",
      "epoch:22 step:17672 [D loss: 0.398778, acc.: 80.47%] [G loss: 2.147610]\n",
      "epoch:22 step:17673 [D loss: 0.460701, acc.: 76.56%] [G loss: 2.794714]\n",
      "epoch:22 step:17674 [D loss: 0.440158, acc.: 83.59%] [G loss: 3.039496]\n",
      "epoch:22 step:17675 [D loss: 0.423239, acc.: 82.03%] [G loss: 2.476471]\n",
      "epoch:22 step:17676 [D loss: 0.295776, acc.: 84.38%] [G loss: 3.312805]\n",
      "epoch:22 step:17677 [D loss: 0.295666, acc.: 85.94%] [G loss: 3.028860]\n",
      "epoch:22 step:17678 [D loss: 0.298335, acc.: 85.94%] [G loss: 2.314093]\n",
      "epoch:22 step:17679 [D loss: 0.387011, acc.: 83.59%] [G loss: 2.804671]\n",
      "epoch:22 step:17680 [D loss: 0.270334, acc.: 89.06%] [G loss: 3.092755]\n",
      "epoch:22 step:17681 [D loss: 0.239151, acc.: 89.84%] [G loss: 2.382360]\n",
      "epoch:22 step:17682 [D loss: 0.376495, acc.: 79.69%] [G loss: 2.714420]\n",
      "epoch:22 step:17683 [D loss: 0.367506, acc.: 82.81%] [G loss: 3.861133]\n",
      "epoch:22 step:17684 [D loss: 0.325024, acc.: 85.94%] [G loss: 3.412635]\n",
      "epoch:22 step:17685 [D loss: 0.270256, acc.: 84.38%] [G loss: 3.366336]\n",
      "epoch:22 step:17686 [D loss: 0.387712, acc.: 85.16%] [G loss: 2.803425]\n",
      "epoch:22 step:17687 [D loss: 0.294024, acc.: 86.72%] [G loss: 2.970438]\n",
      "epoch:22 step:17688 [D loss: 0.304176, acc.: 85.94%] [G loss: 3.873552]\n",
      "epoch:22 step:17689 [D loss: 0.324278, acc.: 86.72%] [G loss: 3.262231]\n",
      "epoch:22 step:17690 [D loss: 0.350133, acc.: 80.47%] [G loss: 2.449481]\n",
      "epoch:22 step:17691 [D loss: 0.356422, acc.: 83.59%] [G loss: 1.858610]\n",
      "epoch:22 step:17692 [D loss: 0.359143, acc.: 83.59%] [G loss: 2.404768]\n",
      "epoch:22 step:17693 [D loss: 0.391537, acc.: 83.59%] [G loss: 2.672929]\n",
      "epoch:22 step:17694 [D loss: 0.315756, acc.: 83.59%] [G loss: 2.777650]\n",
      "epoch:22 step:17695 [D loss: 0.470911, acc.: 82.81%] [G loss: 5.351603]\n",
      "epoch:22 step:17696 [D loss: 0.343308, acc.: 82.81%] [G loss: 4.428296]\n",
      "epoch:22 step:17697 [D loss: 0.305782, acc.: 88.28%] [G loss: 2.701529]\n",
      "epoch:22 step:17698 [D loss: 0.276166, acc.: 88.28%] [G loss: 5.192997]\n",
      "epoch:22 step:17699 [D loss: 0.245715, acc.: 89.84%] [G loss: 2.842894]\n",
      "epoch:22 step:17700 [D loss: 0.342441, acc.: 85.16%] [G loss: 3.171557]\n",
      "epoch:22 step:17701 [D loss: 0.234660, acc.: 91.41%] [G loss: 3.047050]\n",
      "epoch:22 step:17702 [D loss: 0.324126, acc.: 90.62%] [G loss: 2.758661]\n",
      "epoch:22 step:17703 [D loss: 0.351526, acc.: 85.16%] [G loss: 2.477324]\n",
      "epoch:22 step:17704 [D loss: 0.298900, acc.: 82.81%] [G loss: 2.697994]\n",
      "epoch:22 step:17705 [D loss: 0.361755, acc.: 82.81%] [G loss: 2.458990]\n",
      "epoch:22 step:17706 [D loss: 0.326115, acc.: 85.94%] [G loss: 5.909260]\n",
      "epoch:22 step:17707 [D loss: 0.281290, acc.: 87.50%] [G loss: 5.026330]\n",
      "epoch:22 step:17708 [D loss: 0.284485, acc.: 85.94%] [G loss: 5.181606]\n",
      "epoch:22 step:17709 [D loss: 0.411173, acc.: 81.25%] [G loss: 4.756050]\n",
      "epoch:22 step:17710 [D loss: 0.273767, acc.: 88.28%] [G loss: 3.836681]\n",
      "epoch:22 step:17711 [D loss: 0.283810, acc.: 85.16%] [G loss: 5.749917]\n",
      "epoch:22 step:17712 [D loss: 0.218302, acc.: 91.41%] [G loss: 4.717694]\n",
      "epoch:22 step:17713 [D loss: 0.307316, acc.: 88.28%] [G loss: 4.558715]\n",
      "epoch:22 step:17714 [D loss: 0.303372, acc.: 88.28%] [G loss: 3.945077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17715 [D loss: 0.349453, acc.: 83.59%] [G loss: 3.858467]\n",
      "epoch:22 step:17716 [D loss: 0.317120, acc.: 84.38%] [G loss: 4.883286]\n",
      "epoch:22 step:17717 [D loss: 0.358785, acc.: 82.81%] [G loss: 3.442357]\n",
      "epoch:22 step:17718 [D loss: 0.380151, acc.: 82.03%] [G loss: 2.710239]\n",
      "epoch:22 step:17719 [D loss: 0.345164, acc.: 88.28%] [G loss: 2.912953]\n",
      "epoch:22 step:17720 [D loss: 0.332563, acc.: 88.28%] [G loss: 2.470124]\n",
      "epoch:22 step:17721 [D loss: 0.290267, acc.: 89.06%] [G loss: 2.947580]\n",
      "epoch:22 step:17722 [D loss: 0.391474, acc.: 83.59%] [G loss: 3.165653]\n",
      "epoch:22 step:17723 [D loss: 0.323147, acc.: 83.59%] [G loss: 3.412160]\n",
      "epoch:22 step:17724 [D loss: 0.364840, acc.: 84.38%] [G loss: 3.592714]\n",
      "epoch:22 step:17725 [D loss: 0.322861, acc.: 89.06%] [G loss: 3.015970]\n",
      "epoch:22 step:17726 [D loss: 0.398267, acc.: 81.25%] [G loss: 5.594696]\n",
      "epoch:22 step:17727 [D loss: 0.359392, acc.: 83.59%] [G loss: 5.347346]\n",
      "epoch:22 step:17728 [D loss: 0.224138, acc.: 88.28%] [G loss: 4.186484]\n",
      "epoch:22 step:17729 [D loss: 0.314266, acc.: 89.06%] [G loss: 3.497345]\n",
      "epoch:22 step:17730 [D loss: 0.323124, acc.: 82.81%] [G loss: 3.506558]\n",
      "epoch:22 step:17731 [D loss: 0.291663, acc.: 87.50%] [G loss: 2.843253]\n",
      "epoch:22 step:17732 [D loss: 0.303656, acc.: 86.72%] [G loss: 3.155633]\n",
      "epoch:22 step:17733 [D loss: 0.283609, acc.: 86.72%] [G loss: 2.831682]\n",
      "epoch:22 step:17734 [D loss: 0.368240, acc.: 81.25%] [G loss: 3.190419]\n",
      "epoch:22 step:17735 [D loss: 0.376395, acc.: 78.91%] [G loss: 2.802877]\n",
      "epoch:22 step:17736 [D loss: 0.335506, acc.: 87.50%] [G loss: 3.931328]\n",
      "epoch:22 step:17737 [D loss: 0.437348, acc.: 83.59%] [G loss: 5.638843]\n",
      "epoch:22 step:17738 [D loss: 0.602856, acc.: 74.22%] [G loss: 2.645469]\n",
      "epoch:22 step:17739 [D loss: 0.361970, acc.: 84.38%] [G loss: 2.272364]\n",
      "epoch:22 step:17740 [D loss: 0.282470, acc.: 92.19%] [G loss: 3.366703]\n",
      "epoch:22 step:17741 [D loss: 0.268636, acc.: 90.62%] [G loss: 3.407930]\n",
      "epoch:22 step:17742 [D loss: 0.339249, acc.: 84.38%] [G loss: 3.202592]\n",
      "epoch:22 step:17743 [D loss: 0.270357, acc.: 87.50%] [G loss: 2.669383]\n",
      "epoch:22 step:17744 [D loss: 0.412734, acc.: 82.81%] [G loss: 3.071566]\n",
      "epoch:22 step:17745 [D loss: 0.354771, acc.: 85.94%] [G loss: 3.482864]\n",
      "epoch:22 step:17746 [D loss: 0.352052, acc.: 81.25%] [G loss: 3.160312]\n",
      "epoch:22 step:17747 [D loss: 0.328915, acc.: 83.59%] [G loss: 2.958910]\n",
      "epoch:22 step:17748 [D loss: 0.286361, acc.: 87.50%] [G loss: 2.782333]\n",
      "epoch:22 step:17749 [D loss: 0.254535, acc.: 90.62%] [G loss: 2.633140]\n",
      "epoch:22 step:17750 [D loss: 0.387906, acc.: 82.03%] [G loss: 2.960679]\n",
      "epoch:22 step:17751 [D loss: 0.243429, acc.: 91.41%] [G loss: 3.318135]\n",
      "epoch:22 step:17752 [D loss: 0.322222, acc.: 85.16%] [G loss: 3.731536]\n",
      "epoch:22 step:17753 [D loss: 0.406074, acc.: 84.38%] [G loss: 3.006149]\n",
      "epoch:22 step:17754 [D loss: 0.512970, acc.: 74.22%] [G loss: 3.497309]\n",
      "epoch:22 step:17755 [D loss: 0.381153, acc.: 82.81%] [G loss: 3.130892]\n",
      "epoch:22 step:17756 [D loss: 0.377229, acc.: 81.25%] [G loss: 2.888416]\n",
      "epoch:22 step:17757 [D loss: 0.316523, acc.: 87.50%] [G loss: 3.008959]\n",
      "epoch:22 step:17758 [D loss: 0.285616, acc.: 89.84%] [G loss: 2.957993]\n",
      "epoch:22 step:17759 [D loss: 0.394512, acc.: 82.81%] [G loss: 2.656581]\n",
      "epoch:22 step:17760 [D loss: 0.382876, acc.: 83.59%] [G loss: 3.192770]\n",
      "epoch:22 step:17761 [D loss: 0.360578, acc.: 82.03%] [G loss: 2.886781]\n",
      "epoch:22 step:17762 [D loss: 0.278568, acc.: 86.72%] [G loss: 2.130045]\n",
      "epoch:22 step:17763 [D loss: 0.304010, acc.: 85.16%] [G loss: 2.748806]\n",
      "epoch:22 step:17764 [D loss: 0.235755, acc.: 91.41%] [G loss: 3.810731]\n",
      "epoch:22 step:17765 [D loss: 0.527662, acc.: 78.12%] [G loss: 2.976881]\n",
      "epoch:22 step:17766 [D loss: 0.324699, acc.: 86.72%] [G loss: 3.185140]\n",
      "epoch:22 step:17767 [D loss: 0.290574, acc.: 88.28%] [G loss: 3.902971]\n",
      "epoch:22 step:17768 [D loss: 0.320501, acc.: 85.94%] [G loss: 3.699096]\n",
      "epoch:22 step:17769 [D loss: 0.274788, acc.: 85.94%] [G loss: 3.357626]\n",
      "epoch:22 step:17770 [D loss: 0.191330, acc.: 91.41%] [G loss: 3.594119]\n",
      "epoch:22 step:17771 [D loss: 0.245065, acc.: 87.50%] [G loss: 3.906291]\n",
      "epoch:22 step:17772 [D loss: 0.355692, acc.: 81.25%] [G loss: 3.003822]\n",
      "epoch:22 step:17773 [D loss: 0.387676, acc.: 79.69%] [G loss: 6.096104]\n",
      "epoch:22 step:17774 [D loss: 0.731577, acc.: 71.88%] [G loss: 3.891284]\n",
      "epoch:22 step:17775 [D loss: 0.256217, acc.: 89.84%] [G loss: 4.188884]\n",
      "epoch:22 step:17776 [D loss: 0.295814, acc.: 87.50%] [G loss: 4.518616]\n",
      "epoch:22 step:17777 [D loss: 0.295294, acc.: 86.72%] [G loss: 4.741447]\n",
      "epoch:22 step:17778 [D loss: 0.343922, acc.: 84.38%] [G loss: 3.425075]\n",
      "epoch:22 step:17779 [D loss: 0.336603, acc.: 86.72%] [G loss: 3.037656]\n",
      "epoch:22 step:17780 [D loss: 0.271364, acc.: 89.06%] [G loss: 3.249095]\n",
      "epoch:22 step:17781 [D loss: 0.306629, acc.: 86.72%] [G loss: 3.398171]\n",
      "epoch:22 step:17782 [D loss: 0.377337, acc.: 82.03%] [G loss: 3.009095]\n",
      "epoch:22 step:17783 [D loss: 0.381249, acc.: 82.81%] [G loss: 3.619273]\n",
      "epoch:22 step:17784 [D loss: 0.351439, acc.: 87.50%] [G loss: 3.395017]\n",
      "epoch:22 step:17785 [D loss: 0.297247, acc.: 89.06%] [G loss: 3.253845]\n",
      "epoch:22 step:17786 [D loss: 0.393513, acc.: 79.69%] [G loss: 2.664812]\n",
      "epoch:22 step:17787 [D loss: 0.373091, acc.: 84.38%] [G loss: 2.958265]\n",
      "epoch:22 step:17788 [D loss: 0.332341, acc.: 85.16%] [G loss: 2.703714]\n",
      "epoch:22 step:17789 [D loss: 0.285684, acc.: 86.72%] [G loss: 3.239519]\n",
      "epoch:22 step:17790 [D loss: 0.256923, acc.: 92.19%] [G loss: 3.416679]\n",
      "epoch:22 step:17791 [D loss: 0.321619, acc.: 85.94%] [G loss: 4.658868]\n",
      "epoch:22 step:17792 [D loss: 0.308221, acc.: 86.72%] [G loss: 4.871481]\n",
      "epoch:22 step:17793 [D loss: 0.224358, acc.: 91.41%] [G loss: 8.044680]\n",
      "epoch:22 step:17794 [D loss: 0.255875, acc.: 85.16%] [G loss: 4.222272]\n",
      "epoch:22 step:17795 [D loss: 0.228496, acc.: 92.97%] [G loss: 4.009440]\n",
      "epoch:22 step:17796 [D loss: 0.291435, acc.: 89.06%] [G loss: 3.214581]\n",
      "epoch:22 step:17797 [D loss: 0.306208, acc.: 85.94%] [G loss: 5.562745]\n",
      "epoch:22 step:17798 [D loss: 0.274086, acc.: 85.94%] [G loss: 3.995539]\n",
      "epoch:22 step:17799 [D loss: 0.396406, acc.: 80.47%] [G loss: 5.008754]\n",
      "epoch:22 step:17800 [D loss: 0.276629, acc.: 85.16%] [G loss: 4.104265]\n",
      "epoch:22 step:17801 [D loss: 0.315257, acc.: 83.59%] [G loss: 3.006787]\n",
      "epoch:22 step:17802 [D loss: 0.352150, acc.: 85.94%] [G loss: 3.640148]\n",
      "epoch:22 step:17803 [D loss: 0.305665, acc.: 82.03%] [G loss: 4.126166]\n",
      "epoch:22 step:17804 [D loss: 0.502118, acc.: 74.22%] [G loss: 5.535654]\n",
      "epoch:22 step:17805 [D loss: 0.264730, acc.: 89.84%] [G loss: 3.345346]\n",
      "epoch:22 step:17806 [D loss: 0.337193, acc.: 85.94%] [G loss: 4.436095]\n",
      "epoch:22 step:17807 [D loss: 0.288743, acc.: 86.72%] [G loss: 5.623612]\n",
      "epoch:22 step:17808 [D loss: 0.287081, acc.: 87.50%] [G loss: 3.937215]\n",
      "epoch:22 step:17809 [D loss: 0.308676, acc.: 84.38%] [G loss: 4.112972]\n",
      "epoch:22 step:17810 [D loss: 0.365601, acc.: 85.16%] [G loss: 4.013824]\n",
      "epoch:22 step:17811 [D loss: 0.396697, acc.: 81.25%] [G loss: 4.031662]\n",
      "epoch:22 step:17812 [D loss: 0.314354, acc.: 85.94%] [G loss: 3.829291]\n",
      "epoch:22 step:17813 [D loss: 0.350059, acc.: 83.59%] [G loss: 3.203237]\n",
      "epoch:22 step:17814 [D loss: 0.310200, acc.: 84.38%] [G loss: 3.290699]\n",
      "epoch:22 step:17815 [D loss: 0.382030, acc.: 82.03%] [G loss: 3.514285]\n",
      "epoch:22 step:17816 [D loss: 0.402214, acc.: 78.12%] [G loss: 2.230658]\n",
      "epoch:22 step:17817 [D loss: 0.456192, acc.: 80.47%] [G loss: 4.346686]\n",
      "epoch:22 step:17818 [D loss: 0.306059, acc.: 86.72%] [G loss: 2.917529]\n",
      "epoch:22 step:17819 [D loss: 0.311573, acc.: 84.38%] [G loss: 3.175173]\n",
      "epoch:22 step:17820 [D loss: 0.281231, acc.: 87.50%] [G loss: 4.545004]\n",
      "epoch:22 step:17821 [D loss: 0.257507, acc.: 91.41%] [G loss: 9.188591]\n",
      "epoch:22 step:17822 [D loss: 0.226496, acc.: 92.19%] [G loss: 6.110451]\n",
      "epoch:22 step:17823 [D loss: 0.287598, acc.: 84.38%] [G loss: 3.823207]\n",
      "epoch:22 step:17824 [D loss: 0.216023, acc.: 91.41%] [G loss: 4.017975]\n",
      "epoch:22 step:17825 [D loss: 0.282216, acc.: 87.50%] [G loss: 3.398023]\n",
      "epoch:22 step:17826 [D loss: 0.460743, acc.: 82.81%] [G loss: 3.101393]\n",
      "epoch:22 step:17827 [D loss: 0.354941, acc.: 83.59%] [G loss: 3.269424]\n",
      "epoch:22 step:17828 [D loss: 0.530542, acc.: 75.78%] [G loss: 3.615213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17829 [D loss: 0.386685, acc.: 84.38%] [G loss: 4.775954]\n",
      "epoch:22 step:17830 [D loss: 0.397711, acc.: 82.03%] [G loss: 2.950302]\n",
      "epoch:22 step:17831 [D loss: 0.346066, acc.: 85.94%] [G loss: 3.171911]\n",
      "epoch:22 step:17832 [D loss: 0.409143, acc.: 80.47%] [G loss: 3.171057]\n",
      "epoch:22 step:17833 [D loss: 0.404760, acc.: 82.81%] [G loss: 2.581990]\n",
      "epoch:22 step:17834 [D loss: 0.402439, acc.: 78.12%] [G loss: 2.413085]\n",
      "epoch:22 step:17835 [D loss: 0.264965, acc.: 90.62%] [G loss: 3.208493]\n",
      "epoch:22 step:17836 [D loss: 0.307884, acc.: 85.16%] [G loss: 2.831742]\n",
      "epoch:22 step:17837 [D loss: 0.323305, acc.: 89.84%] [G loss: 2.275410]\n",
      "epoch:22 step:17838 [D loss: 0.308169, acc.: 87.50%] [G loss: 2.988194]\n",
      "epoch:22 step:17839 [D loss: 0.491264, acc.: 75.78%] [G loss: 3.956753]\n",
      "epoch:22 step:17840 [D loss: 0.606228, acc.: 75.78%] [G loss: 5.514746]\n",
      "epoch:22 step:17841 [D loss: 0.639091, acc.: 70.31%] [G loss: 3.605772]\n",
      "epoch:22 step:17842 [D loss: 0.339489, acc.: 82.03%] [G loss: 3.469744]\n",
      "epoch:22 step:17843 [D loss: 0.470536, acc.: 79.69%] [G loss: 2.677052]\n",
      "epoch:22 step:17844 [D loss: 0.316058, acc.: 85.94%] [G loss: 3.447051]\n",
      "epoch:22 step:17845 [D loss: 0.357516, acc.: 82.81%] [G loss: 3.004486]\n",
      "epoch:22 step:17846 [D loss: 0.367385, acc.: 83.59%] [G loss: 3.047486]\n",
      "epoch:22 step:17847 [D loss: 0.408880, acc.: 78.12%] [G loss: 3.568450]\n",
      "epoch:22 step:17848 [D loss: 0.250403, acc.: 90.62%] [G loss: 4.047870]\n",
      "epoch:22 step:17849 [D loss: 0.346265, acc.: 82.81%] [G loss: 2.835421]\n",
      "epoch:22 step:17850 [D loss: 0.326061, acc.: 84.38%] [G loss: 2.132432]\n",
      "epoch:22 step:17851 [D loss: 0.307504, acc.: 85.16%] [G loss: 3.022412]\n",
      "epoch:22 step:17852 [D loss: 0.291681, acc.: 85.16%] [G loss: 2.334181]\n",
      "epoch:22 step:17853 [D loss: 0.362220, acc.: 83.59%] [G loss: 2.402151]\n",
      "epoch:22 step:17854 [D loss: 0.308478, acc.: 85.94%] [G loss: 2.639404]\n",
      "epoch:22 step:17855 [D loss: 0.334229, acc.: 85.16%] [G loss: 2.612063]\n",
      "epoch:22 step:17856 [D loss: 0.310729, acc.: 86.72%] [G loss: 2.799666]\n",
      "epoch:22 step:17857 [D loss: 0.336539, acc.: 86.72%] [G loss: 3.267097]\n",
      "epoch:22 step:17858 [D loss: 0.413644, acc.: 80.47%] [G loss: 2.310821]\n",
      "epoch:22 step:17859 [D loss: 0.385779, acc.: 82.81%] [G loss: 2.980989]\n",
      "epoch:22 step:17860 [D loss: 0.347023, acc.: 85.94%] [G loss: 2.485939]\n",
      "epoch:22 step:17861 [D loss: 0.331908, acc.: 85.16%] [G loss: 2.950246]\n",
      "epoch:22 step:17862 [D loss: 0.332580, acc.: 89.06%] [G loss: 2.889490]\n",
      "epoch:22 step:17863 [D loss: 0.323157, acc.: 85.94%] [G loss: 2.797386]\n",
      "epoch:22 step:17864 [D loss: 0.335797, acc.: 81.25%] [G loss: 5.525647]\n",
      "epoch:22 step:17865 [D loss: 0.661562, acc.: 78.91%] [G loss: 8.310638]\n",
      "epoch:22 step:17866 [D loss: 1.847169, acc.: 46.09%] [G loss: 5.367622]\n",
      "epoch:22 step:17867 [D loss: 1.860578, acc.: 66.41%] [G loss: 3.839039]\n",
      "epoch:22 step:17868 [D loss: 0.450260, acc.: 79.69%] [G loss: 5.362050]\n",
      "epoch:22 step:17869 [D loss: 0.547303, acc.: 75.78%] [G loss: 4.359900]\n",
      "epoch:22 step:17870 [D loss: 0.251401, acc.: 89.84%] [G loss: 4.736753]\n",
      "epoch:22 step:17871 [D loss: 0.336354, acc.: 87.50%] [G loss: 3.609136]\n",
      "epoch:22 step:17872 [D loss: 0.298674, acc.: 87.50%] [G loss: 4.110969]\n",
      "epoch:22 step:17873 [D loss: 0.332230, acc.: 87.50%] [G loss: 2.921563]\n",
      "epoch:22 step:17874 [D loss: 0.324686, acc.: 85.16%] [G loss: 4.533932]\n",
      "epoch:22 step:17875 [D loss: 0.352019, acc.: 82.03%] [G loss: 3.742631]\n",
      "epoch:22 step:17876 [D loss: 0.298022, acc.: 86.72%] [G loss: 3.340847]\n",
      "epoch:22 step:17877 [D loss: 0.235985, acc.: 89.84%] [G loss: 4.081236]\n",
      "epoch:22 step:17878 [D loss: 0.396843, acc.: 81.25%] [G loss: 3.453187]\n",
      "epoch:22 step:17879 [D loss: 0.311743, acc.: 88.28%] [G loss: 3.006957]\n",
      "epoch:22 step:17880 [D loss: 0.279906, acc.: 90.62%] [G loss: 2.839182]\n",
      "epoch:22 step:17881 [D loss: 0.253684, acc.: 90.62%] [G loss: 3.427678]\n",
      "epoch:22 step:17882 [D loss: 0.267313, acc.: 89.06%] [G loss: 4.101362]\n",
      "epoch:22 step:17883 [D loss: 0.378327, acc.: 80.47%] [G loss: 2.549403]\n",
      "epoch:22 step:17884 [D loss: 0.341806, acc.: 80.47%] [G loss: 2.612785]\n",
      "epoch:22 step:17885 [D loss: 0.367585, acc.: 81.25%] [G loss: 2.486062]\n",
      "epoch:22 step:17886 [D loss: 0.293292, acc.: 85.94%] [G loss: 2.671553]\n",
      "epoch:22 step:17887 [D loss: 0.309322, acc.: 85.94%] [G loss: 2.422407]\n",
      "epoch:22 step:17888 [D loss: 0.368798, acc.: 81.25%] [G loss: 2.744957]\n",
      "epoch:22 step:17889 [D loss: 0.352889, acc.: 82.81%] [G loss: 2.505809]\n",
      "epoch:22 step:17890 [D loss: 0.284551, acc.: 88.28%] [G loss: 3.430944]\n",
      "epoch:22 step:17891 [D loss: 0.372907, acc.: 82.03%] [G loss: 2.240034]\n",
      "epoch:22 step:17892 [D loss: 0.420162, acc.: 80.47%] [G loss: 3.055777]\n",
      "epoch:22 step:17893 [D loss: 0.317267, acc.: 85.16%] [G loss: 2.532406]\n",
      "epoch:22 step:17894 [D loss: 0.331463, acc.: 87.50%] [G loss: 2.940087]\n",
      "epoch:22 step:17895 [D loss: 0.356673, acc.: 82.03%] [G loss: 2.886144]\n",
      "epoch:22 step:17896 [D loss: 0.326745, acc.: 86.72%] [G loss: 3.501426]\n",
      "epoch:22 step:17897 [D loss: 0.350532, acc.: 80.47%] [G loss: 4.049438]\n",
      "epoch:22 step:17898 [D loss: 0.294440, acc.: 85.94%] [G loss: 4.819484]\n",
      "epoch:22 step:17899 [D loss: 0.304334, acc.: 85.16%] [G loss: 4.676588]\n",
      "epoch:22 step:17900 [D loss: 0.326107, acc.: 86.72%] [G loss: 3.345335]\n",
      "epoch:22 step:17901 [D loss: 0.241579, acc.: 88.28%] [G loss: 3.241276]\n",
      "epoch:22 step:17902 [D loss: 0.384232, acc.: 83.59%] [G loss: 3.929358]\n",
      "epoch:22 step:17903 [D loss: 0.370433, acc.: 82.81%] [G loss: 3.339521]\n",
      "epoch:22 step:17904 [D loss: 0.246439, acc.: 89.84%] [G loss: 3.474955]\n",
      "epoch:22 step:17905 [D loss: 0.331207, acc.: 82.81%] [G loss: 2.792408]\n",
      "epoch:22 step:17906 [D loss: 0.312882, acc.: 91.41%] [G loss: 2.937032]\n",
      "epoch:22 step:17907 [D loss: 0.261022, acc.: 89.84%] [G loss: 2.620548]\n",
      "epoch:22 step:17908 [D loss: 0.340602, acc.: 85.94%] [G loss: 4.303916]\n",
      "epoch:22 step:17909 [D loss: 0.224138, acc.: 89.84%] [G loss: 5.115669]\n",
      "epoch:22 step:17910 [D loss: 0.371400, acc.: 84.38%] [G loss: 3.557927]\n",
      "epoch:22 step:17911 [D loss: 0.290919, acc.: 85.16%] [G loss: 4.286723]\n",
      "epoch:22 step:17912 [D loss: 0.208112, acc.: 95.31%] [G loss: 2.788000]\n",
      "epoch:22 step:17913 [D loss: 0.269637, acc.: 88.28%] [G loss: 3.293183]\n",
      "epoch:22 step:17914 [D loss: 0.352853, acc.: 82.81%] [G loss: 2.529671]\n",
      "epoch:22 step:17915 [D loss: 0.327518, acc.: 85.94%] [G loss: 3.838955]\n",
      "epoch:22 step:17916 [D loss: 0.323285, acc.: 84.38%] [G loss: 2.997982]\n",
      "epoch:22 step:17917 [D loss: 0.413537, acc.: 81.25%] [G loss: 3.297801]\n",
      "epoch:22 step:17918 [D loss: 0.275653, acc.: 88.28%] [G loss: 2.819118]\n",
      "epoch:22 step:17919 [D loss: 0.258252, acc.: 86.72%] [G loss: 4.198203]\n",
      "epoch:22 step:17920 [D loss: 0.289710, acc.: 86.72%] [G loss: 3.304580]\n",
      "epoch:22 step:17921 [D loss: 0.401958, acc.: 81.25%] [G loss: 3.501007]\n",
      "epoch:22 step:17922 [D loss: 0.265714, acc.: 87.50%] [G loss: 4.091226]\n",
      "epoch:22 step:17923 [D loss: 0.393180, acc.: 82.03%] [G loss: 2.467321]\n",
      "epoch:22 step:17924 [D loss: 0.287334, acc.: 88.28%] [G loss: 3.480739]\n",
      "epoch:22 step:17925 [D loss: 0.393209, acc.: 85.94%] [G loss: 3.102409]\n",
      "epoch:22 step:17926 [D loss: 0.296338, acc.: 85.94%] [G loss: 4.268913]\n",
      "epoch:22 step:17927 [D loss: 0.270373, acc.: 85.16%] [G loss: 4.563460]\n",
      "epoch:22 step:17928 [D loss: 0.293894, acc.: 87.50%] [G loss: 3.418888]\n",
      "epoch:22 step:17929 [D loss: 0.274881, acc.: 88.28%] [G loss: 4.501389]\n",
      "epoch:22 step:17930 [D loss: 0.359616, acc.: 82.81%] [G loss: 3.427880]\n",
      "epoch:22 step:17931 [D loss: 0.343685, acc.: 85.94%] [G loss: 3.970528]\n",
      "epoch:22 step:17932 [D loss: 0.360896, acc.: 82.81%] [G loss: 4.033856]\n",
      "epoch:22 step:17933 [D loss: 0.326684, acc.: 82.81%] [G loss: 2.708482]\n",
      "epoch:22 step:17934 [D loss: 0.262309, acc.: 89.06%] [G loss: 3.336480]\n",
      "epoch:22 step:17935 [D loss: 0.389327, acc.: 80.47%] [G loss: 2.230868]\n",
      "epoch:22 step:17936 [D loss: 0.368977, acc.: 85.94%] [G loss: 2.942770]\n",
      "epoch:22 step:17937 [D loss: 0.245217, acc.: 90.62%] [G loss: 2.798281]\n",
      "epoch:22 step:17938 [D loss: 0.405688, acc.: 81.25%] [G loss: 2.587030]\n",
      "epoch:22 step:17939 [D loss: 0.365789, acc.: 84.38%] [G loss: 3.484890]\n",
      "epoch:22 step:17940 [D loss: 0.353333, acc.: 86.72%] [G loss: 2.850819]\n",
      "epoch:22 step:17941 [D loss: 0.324013, acc.: 83.59%] [G loss: 2.958722]\n",
      "epoch:22 step:17942 [D loss: 0.255326, acc.: 89.84%] [G loss: 3.269940]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17943 [D loss: 0.343802, acc.: 85.94%] [G loss: 3.243692]\n",
      "epoch:22 step:17944 [D loss: 0.329443, acc.: 84.38%] [G loss: 3.309383]\n",
      "epoch:22 step:17945 [D loss: 0.313297, acc.: 82.03%] [G loss: 2.472528]\n",
      "epoch:22 step:17946 [D loss: 0.270703, acc.: 87.50%] [G loss: 3.134099]\n",
      "epoch:22 step:17947 [D loss: 0.454368, acc.: 78.91%] [G loss: 4.496195]\n",
      "epoch:22 step:17948 [D loss: 0.384853, acc.: 84.38%] [G loss: 3.446808]\n",
      "epoch:22 step:17949 [D loss: 0.305583, acc.: 88.28%] [G loss: 3.016648]\n",
      "epoch:22 step:17950 [D loss: 0.290798, acc.: 86.72%] [G loss: 5.223002]\n",
      "epoch:22 step:17951 [D loss: 0.291904, acc.: 85.16%] [G loss: 4.505675]\n",
      "epoch:22 step:17952 [D loss: 0.254167, acc.: 92.19%] [G loss: 3.623644]\n",
      "epoch:22 step:17953 [D loss: 0.345743, acc.: 82.81%] [G loss: 4.742842]\n",
      "epoch:22 step:17954 [D loss: 0.293213, acc.: 89.06%] [G loss: 3.201403]\n",
      "epoch:22 step:17955 [D loss: 0.277520, acc.: 88.28%] [G loss: 3.475426]\n",
      "epoch:22 step:17956 [D loss: 0.225632, acc.: 95.31%] [G loss: 3.855473]\n",
      "epoch:22 step:17957 [D loss: 0.312259, acc.: 85.94%] [G loss: 3.510057]\n",
      "epoch:22 step:17958 [D loss: 0.330856, acc.: 84.38%] [G loss: 3.353190]\n",
      "epoch:22 step:17959 [D loss: 0.401686, acc.: 75.78%] [G loss: 3.384901]\n",
      "epoch:22 step:17960 [D loss: 0.299763, acc.: 87.50%] [G loss: 2.580780]\n",
      "epoch:22 step:17961 [D loss: 0.315275, acc.: 89.06%] [G loss: 3.650840]\n",
      "epoch:22 step:17962 [D loss: 0.406788, acc.: 82.81%] [G loss: 2.250623]\n",
      "epoch:22 step:17963 [D loss: 0.383750, acc.: 83.59%] [G loss: 2.767694]\n",
      "epoch:23 step:17964 [D loss: 0.358875, acc.: 82.03%] [G loss: 2.645262]\n",
      "epoch:23 step:17965 [D loss: 0.313648, acc.: 85.94%] [G loss: 3.307820]\n",
      "epoch:23 step:17966 [D loss: 0.353083, acc.: 84.38%] [G loss: 3.386102]\n",
      "epoch:23 step:17967 [D loss: 0.415967, acc.: 82.81%] [G loss: 2.860062]\n",
      "epoch:23 step:17968 [D loss: 0.326019, acc.: 82.03%] [G loss: 3.190092]\n",
      "epoch:23 step:17969 [D loss: 0.264118, acc.: 88.28%] [G loss: 3.199736]\n",
      "epoch:23 step:17970 [D loss: 0.332076, acc.: 84.38%] [G loss: 3.248676]\n",
      "epoch:23 step:17971 [D loss: 0.229027, acc.: 91.41%] [G loss: 3.452916]\n",
      "epoch:23 step:17972 [D loss: 0.361027, acc.: 83.59%] [G loss: 3.553723]\n",
      "epoch:23 step:17973 [D loss: 0.381439, acc.: 78.12%] [G loss: 2.862520]\n",
      "epoch:23 step:17974 [D loss: 0.443101, acc.: 79.69%] [G loss: 2.764250]\n",
      "epoch:23 step:17975 [D loss: 0.291436, acc.: 90.62%] [G loss: 3.093972]\n",
      "epoch:23 step:17976 [D loss: 0.273042, acc.: 85.94%] [G loss: 3.901163]\n",
      "epoch:23 step:17977 [D loss: 0.408028, acc.: 79.69%] [G loss: 3.338835]\n",
      "epoch:23 step:17978 [D loss: 0.368195, acc.: 82.03%] [G loss: 2.766448]\n",
      "epoch:23 step:17979 [D loss: 0.306373, acc.: 86.72%] [G loss: 3.345717]\n",
      "epoch:23 step:17980 [D loss: 0.280586, acc.: 88.28%] [G loss: 2.419631]\n",
      "epoch:23 step:17981 [D loss: 0.534837, acc.: 76.56%] [G loss: 2.574009]\n",
      "epoch:23 step:17982 [D loss: 0.457673, acc.: 75.78%] [G loss: 3.319935]\n",
      "epoch:23 step:17983 [D loss: 0.407326, acc.: 80.47%] [G loss: 3.404548]\n",
      "epoch:23 step:17984 [D loss: 0.284873, acc.: 88.28%] [G loss: 3.784976]\n",
      "epoch:23 step:17985 [D loss: 0.284866, acc.: 89.84%] [G loss: 3.822622]\n",
      "epoch:23 step:17986 [D loss: 0.359914, acc.: 86.72%] [G loss: 3.079980]\n",
      "epoch:23 step:17987 [D loss: 0.312082, acc.: 86.72%] [G loss: 2.744946]\n",
      "epoch:23 step:17988 [D loss: 0.305464, acc.: 87.50%] [G loss: 2.546687]\n",
      "epoch:23 step:17989 [D loss: 0.322696, acc.: 85.16%] [G loss: 2.891058]\n",
      "epoch:23 step:17990 [D loss: 0.294846, acc.: 89.06%] [G loss: 2.635754]\n",
      "epoch:23 step:17991 [D loss: 0.512913, acc.: 79.69%] [G loss: 2.151539]\n",
      "epoch:23 step:17992 [D loss: 0.435832, acc.: 78.12%] [G loss: 3.060647]\n",
      "epoch:23 step:17993 [D loss: 0.259779, acc.: 88.28%] [G loss: 3.288152]\n",
      "epoch:23 step:17994 [D loss: 0.406467, acc.: 82.03%] [G loss: 3.734218]\n",
      "epoch:23 step:17995 [D loss: 0.374684, acc.: 82.81%] [G loss: 4.051126]\n",
      "epoch:23 step:17996 [D loss: 0.233100, acc.: 89.06%] [G loss: 3.371051]\n",
      "epoch:23 step:17997 [D loss: 0.289154, acc.: 84.38%] [G loss: 3.588107]\n",
      "epoch:23 step:17998 [D loss: 0.326476, acc.: 89.06%] [G loss: 3.803066]\n",
      "epoch:23 step:17999 [D loss: 0.220675, acc.: 90.62%] [G loss: 4.765374]\n",
      "epoch:23 step:18000 [D loss: 0.237015, acc.: 89.06%] [G loss: 5.236547]\n",
      "epoch:23 step:18001 [D loss: 0.344718, acc.: 85.94%] [G loss: 4.228145]\n",
      "epoch:23 step:18002 [D loss: 0.256303, acc.: 90.62%] [G loss: 3.234908]\n",
      "epoch:23 step:18003 [D loss: 0.398285, acc.: 82.81%] [G loss: 3.204012]\n",
      "epoch:23 step:18004 [D loss: 0.348711, acc.: 83.59%] [G loss: 3.555863]\n",
      "epoch:23 step:18005 [D loss: 0.346576, acc.: 86.72%] [G loss: 3.411314]\n",
      "epoch:23 step:18006 [D loss: 0.452129, acc.: 75.78%] [G loss: 2.479280]\n",
      "epoch:23 step:18007 [D loss: 0.354022, acc.: 85.16%] [G loss: 4.839281]\n",
      "epoch:23 step:18008 [D loss: 0.503836, acc.: 77.34%] [G loss: 4.070837]\n",
      "epoch:23 step:18009 [D loss: 0.428959, acc.: 80.47%] [G loss: 2.766787]\n",
      "epoch:23 step:18010 [D loss: 0.333826, acc.: 83.59%] [G loss: 4.566858]\n",
      "epoch:23 step:18011 [D loss: 0.469082, acc.: 79.69%] [G loss: 2.591314]\n",
      "epoch:23 step:18012 [D loss: 0.317700, acc.: 87.50%] [G loss: 4.423345]\n",
      "epoch:23 step:18013 [D loss: 0.416497, acc.: 79.69%] [G loss: 4.043138]\n",
      "epoch:23 step:18014 [D loss: 0.550422, acc.: 75.00%] [G loss: 4.210464]\n",
      "epoch:23 step:18015 [D loss: 0.560820, acc.: 73.44%] [G loss: 3.441477]\n",
      "epoch:23 step:18016 [D loss: 0.388940, acc.: 85.16%] [G loss: 3.534652]\n",
      "epoch:23 step:18017 [D loss: 0.315433, acc.: 84.38%] [G loss: 4.794675]\n",
      "epoch:23 step:18018 [D loss: 0.299770, acc.: 86.72%] [G loss: 3.696385]\n",
      "epoch:23 step:18019 [D loss: 0.384514, acc.: 78.91%] [G loss: 3.905213]\n",
      "epoch:23 step:18020 [D loss: 0.318038, acc.: 83.59%] [G loss: 3.619997]\n",
      "epoch:23 step:18021 [D loss: 0.319693, acc.: 85.94%] [G loss: 4.345245]\n",
      "epoch:23 step:18022 [D loss: 0.229582, acc.: 89.84%] [G loss: 4.889886]\n",
      "epoch:23 step:18023 [D loss: 0.298549, acc.: 84.38%] [G loss: 4.418593]\n",
      "epoch:23 step:18024 [D loss: 0.205991, acc.: 94.53%] [G loss: 4.676478]\n",
      "epoch:23 step:18025 [D loss: 0.291769, acc.: 88.28%] [G loss: 4.061001]\n",
      "epoch:23 step:18026 [D loss: 0.197211, acc.: 90.62%] [G loss: 4.496992]\n",
      "epoch:23 step:18027 [D loss: 0.295065, acc.: 86.72%] [G loss: 3.385315]\n",
      "epoch:23 step:18028 [D loss: 0.296787, acc.: 84.38%] [G loss: 3.598147]\n",
      "epoch:23 step:18029 [D loss: 0.262674, acc.: 90.62%] [G loss: 3.925788]\n",
      "epoch:23 step:18030 [D loss: 0.297332, acc.: 89.06%] [G loss: 2.985134]\n",
      "epoch:23 step:18031 [D loss: 0.294545, acc.: 87.50%] [G loss: 4.684850]\n",
      "epoch:23 step:18032 [D loss: 0.285401, acc.: 88.28%] [G loss: 3.329691]\n",
      "epoch:23 step:18033 [D loss: 0.221874, acc.: 90.62%] [G loss: 4.443112]\n",
      "epoch:23 step:18034 [D loss: 0.313258, acc.: 82.81%] [G loss: 2.884335]\n",
      "epoch:23 step:18035 [D loss: 0.252779, acc.: 92.97%] [G loss: 4.194405]\n",
      "epoch:23 step:18036 [D loss: 0.314971, acc.: 87.50%] [G loss: 3.008695]\n",
      "epoch:23 step:18037 [D loss: 0.280164, acc.: 89.06%] [G loss: 2.511967]\n",
      "epoch:23 step:18038 [D loss: 0.304349, acc.: 87.50%] [G loss: 2.587908]\n",
      "epoch:23 step:18039 [D loss: 0.316803, acc.: 82.03%] [G loss: 2.919438]\n",
      "epoch:23 step:18040 [D loss: 0.284706, acc.: 89.84%] [G loss: 4.635591]\n",
      "epoch:23 step:18041 [D loss: 0.266994, acc.: 89.06%] [G loss: 3.655619]\n",
      "epoch:23 step:18042 [D loss: 0.386529, acc.: 80.47%] [G loss: 4.312687]\n",
      "epoch:23 step:18043 [D loss: 0.488312, acc.: 78.91%] [G loss: 3.888428]\n",
      "epoch:23 step:18044 [D loss: 0.397369, acc.: 81.25%] [G loss: 3.100950]\n",
      "epoch:23 step:18045 [D loss: 0.448164, acc.: 82.81%] [G loss: 2.965804]\n",
      "epoch:23 step:18046 [D loss: 0.315656, acc.: 85.16%] [G loss: 3.021990]\n",
      "epoch:23 step:18047 [D loss: 0.287140, acc.: 89.06%] [G loss: 3.680966]\n",
      "epoch:23 step:18048 [D loss: 0.242455, acc.: 85.16%] [G loss: 4.053668]\n",
      "epoch:23 step:18049 [D loss: 0.360390, acc.: 83.59%] [G loss: 3.390043]\n",
      "epoch:23 step:18050 [D loss: 0.322543, acc.: 87.50%] [G loss: 2.375833]\n",
      "epoch:23 step:18051 [D loss: 0.388103, acc.: 82.03%] [G loss: 2.938368]\n",
      "epoch:23 step:18052 [D loss: 0.408547, acc.: 80.47%] [G loss: 2.128965]\n",
      "epoch:23 step:18053 [D loss: 0.505742, acc.: 74.22%] [G loss: 2.712762]\n",
      "epoch:23 step:18054 [D loss: 0.296188, acc.: 85.16%] [G loss: 3.258609]\n",
      "epoch:23 step:18055 [D loss: 0.338951, acc.: 85.16%] [G loss: 3.250500]\n",
      "epoch:23 step:18056 [D loss: 0.324191, acc.: 88.28%] [G loss: 2.941636]\n",
      "epoch:23 step:18057 [D loss: 0.379002, acc.: 80.47%] [G loss: 2.974478]\n",
      "epoch:23 step:18058 [D loss: 0.355305, acc.: 83.59%] [G loss: 2.380543]\n",
      "epoch:23 step:18059 [D loss: 0.307659, acc.: 88.28%] [G loss: 2.380765]\n",
      "epoch:23 step:18060 [D loss: 0.253702, acc.: 87.50%] [G loss: 4.342335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18061 [D loss: 0.299076, acc.: 85.94%] [G loss: 3.232596]\n",
      "epoch:23 step:18062 [D loss: 0.334726, acc.: 84.38%] [G loss: 3.044600]\n",
      "epoch:23 step:18063 [D loss: 0.388475, acc.: 81.25%] [G loss: 2.579165]\n",
      "epoch:23 step:18064 [D loss: 0.336720, acc.: 83.59%] [G loss: 3.816851]\n",
      "epoch:23 step:18065 [D loss: 0.515942, acc.: 77.34%] [G loss: 2.798770]\n",
      "epoch:23 step:18066 [D loss: 0.361744, acc.: 85.16%] [G loss: 3.323987]\n",
      "epoch:23 step:18067 [D loss: 0.421067, acc.: 79.69%] [G loss: 3.261636]\n",
      "epoch:23 step:18068 [D loss: 0.420061, acc.: 82.03%] [G loss: 2.870953]\n",
      "epoch:23 step:18069 [D loss: 0.384700, acc.: 78.91%] [G loss: 2.941186]\n",
      "epoch:23 step:18070 [D loss: 0.218649, acc.: 92.97%] [G loss: 3.263991]\n",
      "epoch:23 step:18071 [D loss: 0.397060, acc.: 78.12%] [G loss: 2.330897]\n",
      "epoch:23 step:18072 [D loss: 0.293894, acc.: 89.06%] [G loss: 4.309990]\n",
      "epoch:23 step:18073 [D loss: 0.316722, acc.: 86.72%] [G loss: 4.339334]\n",
      "epoch:23 step:18074 [D loss: 0.208630, acc.: 94.53%] [G loss: 4.658390]\n",
      "epoch:23 step:18075 [D loss: 0.227020, acc.: 89.84%] [G loss: 4.170807]\n",
      "epoch:23 step:18076 [D loss: 0.306857, acc.: 86.72%] [G loss: 3.616798]\n",
      "epoch:23 step:18077 [D loss: 0.273635, acc.: 87.50%] [G loss: 3.342242]\n",
      "epoch:23 step:18078 [D loss: 0.357829, acc.: 84.38%] [G loss: 3.298360]\n",
      "epoch:23 step:18079 [D loss: 0.290810, acc.: 89.06%] [G loss: 3.829622]\n",
      "epoch:23 step:18080 [D loss: 0.349576, acc.: 89.06%] [G loss: 2.432742]\n",
      "epoch:23 step:18081 [D loss: 0.379357, acc.: 83.59%] [G loss: 4.300072]\n",
      "epoch:23 step:18082 [D loss: 0.301039, acc.: 85.94%] [G loss: 3.249280]\n",
      "epoch:23 step:18083 [D loss: 0.493159, acc.: 72.66%] [G loss: 2.558014]\n",
      "epoch:23 step:18084 [D loss: 0.232929, acc.: 92.19%] [G loss: 2.549071]\n",
      "epoch:23 step:18085 [D loss: 0.292921, acc.: 84.38%] [G loss: 2.790886]\n",
      "epoch:23 step:18086 [D loss: 0.365757, acc.: 81.25%] [G loss: 3.012932]\n",
      "epoch:23 step:18087 [D loss: 0.252097, acc.: 89.84%] [G loss: 3.404335]\n",
      "epoch:23 step:18088 [D loss: 0.325373, acc.: 85.94%] [G loss: 3.052050]\n",
      "epoch:23 step:18089 [D loss: 0.315491, acc.: 85.16%] [G loss: 2.863258]\n",
      "epoch:23 step:18090 [D loss: 0.216839, acc.: 92.97%] [G loss: 3.242782]\n",
      "epoch:23 step:18091 [D loss: 0.283491, acc.: 86.72%] [G loss: 3.271145]\n",
      "epoch:23 step:18092 [D loss: 0.250550, acc.: 91.41%] [G loss: 2.239410]\n",
      "epoch:23 step:18093 [D loss: 0.304405, acc.: 88.28%] [G loss: 2.483187]\n",
      "epoch:23 step:18094 [D loss: 0.262230, acc.: 90.62%] [G loss: 2.370457]\n",
      "epoch:23 step:18095 [D loss: 0.336283, acc.: 82.81%] [G loss: 2.404724]\n",
      "epoch:23 step:18096 [D loss: 0.277291, acc.: 85.94%] [G loss: 2.180592]\n",
      "epoch:23 step:18097 [D loss: 0.334749, acc.: 85.94%] [G loss: 3.611621]\n",
      "epoch:23 step:18098 [D loss: 0.574524, acc.: 73.44%] [G loss: 5.547329]\n",
      "epoch:23 step:18099 [D loss: 0.595350, acc.: 77.34%] [G loss: 5.852089]\n",
      "epoch:23 step:18100 [D loss: 1.313766, acc.: 70.31%] [G loss: 6.256371]\n",
      "epoch:23 step:18101 [D loss: 2.015800, acc.: 48.44%] [G loss: 3.673532]\n",
      "epoch:23 step:18102 [D loss: 1.338358, acc.: 70.31%] [G loss: 2.339955]\n",
      "epoch:23 step:18103 [D loss: 0.307435, acc.: 89.06%] [G loss: 2.608541]\n",
      "epoch:23 step:18104 [D loss: 0.409130, acc.: 83.59%] [G loss: 3.300121]\n",
      "epoch:23 step:18105 [D loss: 0.281120, acc.: 88.28%] [G loss: 3.370122]\n",
      "epoch:23 step:18106 [D loss: 0.339283, acc.: 85.94%] [G loss: 2.516410]\n",
      "epoch:23 step:18107 [D loss: 0.385331, acc.: 82.03%] [G loss: 3.517758]\n",
      "epoch:23 step:18108 [D loss: 0.244155, acc.: 89.84%] [G loss: 2.755641]\n",
      "epoch:23 step:18109 [D loss: 0.392475, acc.: 82.81%] [G loss: 2.670972]\n",
      "epoch:23 step:18110 [D loss: 0.308546, acc.: 86.72%] [G loss: 2.851723]\n",
      "epoch:23 step:18111 [D loss: 0.386507, acc.: 80.47%] [G loss: 2.277215]\n",
      "epoch:23 step:18112 [D loss: 0.297115, acc.: 89.84%] [G loss: 3.240814]\n",
      "epoch:23 step:18113 [D loss: 0.384859, acc.: 81.25%] [G loss: 2.096687]\n",
      "epoch:23 step:18114 [D loss: 0.322671, acc.: 87.50%] [G loss: 2.858016]\n",
      "epoch:23 step:18115 [D loss: 0.319432, acc.: 85.16%] [G loss: 2.192928]\n",
      "epoch:23 step:18116 [D loss: 0.441813, acc.: 78.91%] [G loss: 2.356615]\n",
      "epoch:23 step:18117 [D loss: 0.374086, acc.: 81.25%] [G loss: 2.454841]\n",
      "epoch:23 step:18118 [D loss: 0.329349, acc.: 85.94%] [G loss: 2.191303]\n",
      "epoch:23 step:18119 [D loss: 0.268135, acc.: 87.50%] [G loss: 2.806308]\n",
      "epoch:23 step:18120 [D loss: 0.344036, acc.: 81.25%] [G loss: 3.017036]\n",
      "epoch:23 step:18121 [D loss: 0.356017, acc.: 82.81%] [G loss: 2.260579]\n",
      "epoch:23 step:18122 [D loss: 0.285290, acc.: 89.84%] [G loss: 2.552772]\n",
      "epoch:23 step:18123 [D loss: 0.519074, acc.: 76.56%] [G loss: 2.574224]\n",
      "epoch:23 step:18124 [D loss: 0.397386, acc.: 81.25%] [G loss: 2.166409]\n",
      "epoch:23 step:18125 [D loss: 0.347515, acc.: 85.16%] [G loss: 2.922096]\n",
      "epoch:23 step:18126 [D loss: 0.264143, acc.: 90.62%] [G loss: 3.281342]\n",
      "epoch:23 step:18127 [D loss: 0.203495, acc.: 92.19%] [G loss: 4.163233]\n",
      "epoch:23 step:18128 [D loss: 0.312886, acc.: 86.72%] [G loss: 4.651196]\n",
      "epoch:23 step:18129 [D loss: 0.298589, acc.: 85.94%] [G loss: 3.558653]\n",
      "epoch:23 step:18130 [D loss: 0.326930, acc.: 82.03%] [G loss: 4.523304]\n",
      "epoch:23 step:18131 [D loss: 0.289011, acc.: 84.38%] [G loss: 4.038485]\n",
      "epoch:23 step:18132 [D loss: 0.305406, acc.: 89.06%] [G loss: 3.512673]\n",
      "epoch:23 step:18133 [D loss: 0.265416, acc.: 85.16%] [G loss: 2.996354]\n",
      "epoch:23 step:18134 [D loss: 0.385682, acc.: 80.47%] [G loss: 2.635462]\n",
      "epoch:23 step:18135 [D loss: 0.259959, acc.: 92.19%] [G loss: 3.011866]\n",
      "epoch:23 step:18136 [D loss: 0.425765, acc.: 82.81%] [G loss: 2.134713]\n",
      "epoch:23 step:18137 [D loss: 0.347469, acc.: 83.59%] [G loss: 2.591025]\n",
      "epoch:23 step:18138 [D loss: 0.475165, acc.: 80.47%] [G loss: 2.887982]\n",
      "epoch:23 step:18139 [D loss: 0.293878, acc.: 89.84%] [G loss: 2.153422]\n",
      "epoch:23 step:18140 [D loss: 0.399721, acc.: 82.81%] [G loss: 4.317992]\n",
      "epoch:23 step:18141 [D loss: 0.382781, acc.: 82.81%] [G loss: 3.423526]\n",
      "epoch:23 step:18142 [D loss: 0.243740, acc.: 92.97%] [G loss: 5.181162]\n",
      "epoch:23 step:18143 [D loss: 0.364170, acc.: 85.94%] [G loss: 2.724215]\n",
      "epoch:23 step:18144 [D loss: 0.300999, acc.: 82.81%] [G loss: 3.486997]\n",
      "epoch:23 step:18145 [D loss: 0.256028, acc.: 86.72%] [G loss: 3.877671]\n",
      "epoch:23 step:18146 [D loss: 0.262060, acc.: 92.19%] [G loss: 2.599588]\n",
      "epoch:23 step:18147 [D loss: 0.460248, acc.: 82.03%] [G loss: 2.754241]\n",
      "epoch:23 step:18148 [D loss: 0.390858, acc.: 80.47%] [G loss: 2.414504]\n",
      "epoch:23 step:18149 [D loss: 0.320859, acc.: 85.94%] [G loss: 2.951045]\n",
      "epoch:23 step:18150 [D loss: 0.373799, acc.: 82.81%] [G loss: 2.006351]\n",
      "epoch:23 step:18151 [D loss: 0.409272, acc.: 81.25%] [G loss: 2.073333]\n",
      "epoch:23 step:18152 [D loss: 0.242141, acc.: 89.84%] [G loss: 2.671994]\n",
      "epoch:23 step:18153 [D loss: 0.346005, acc.: 85.94%] [G loss: 2.230011]\n",
      "epoch:23 step:18154 [D loss: 0.293364, acc.: 85.94%] [G loss: 2.793606]\n",
      "epoch:23 step:18155 [D loss: 0.319368, acc.: 83.59%] [G loss: 3.325569]\n",
      "epoch:23 step:18156 [D loss: 0.414372, acc.: 80.47%] [G loss: 2.527678]\n",
      "epoch:23 step:18157 [D loss: 0.241706, acc.: 91.41%] [G loss: 3.096723]\n",
      "epoch:23 step:18158 [D loss: 0.262779, acc.: 89.06%] [G loss: 2.569778]\n",
      "epoch:23 step:18159 [D loss: 0.315585, acc.: 84.38%] [G loss: 2.536564]\n",
      "epoch:23 step:18160 [D loss: 0.420473, acc.: 79.69%] [G loss: 2.952041]\n",
      "epoch:23 step:18161 [D loss: 0.358590, acc.: 87.50%] [G loss: 2.835699]\n",
      "epoch:23 step:18162 [D loss: 0.303310, acc.: 83.59%] [G loss: 3.157041]\n",
      "epoch:23 step:18163 [D loss: 0.297722, acc.: 87.50%] [G loss: 3.162164]\n",
      "epoch:23 step:18164 [D loss: 0.399828, acc.: 81.25%] [G loss: 3.674457]\n",
      "epoch:23 step:18165 [D loss: 0.398749, acc.: 83.59%] [G loss: 3.618622]\n",
      "epoch:23 step:18166 [D loss: 0.262617, acc.: 89.84%] [G loss: 4.056240]\n",
      "epoch:23 step:18167 [D loss: 0.307686, acc.: 88.28%] [G loss: 2.363675]\n",
      "epoch:23 step:18168 [D loss: 0.297122, acc.: 85.94%] [G loss: 3.136613]\n",
      "epoch:23 step:18169 [D loss: 0.334585, acc.: 83.59%] [G loss: 3.182717]\n",
      "epoch:23 step:18170 [D loss: 0.343093, acc.: 81.25%] [G loss: 5.691466]\n",
      "epoch:23 step:18171 [D loss: 0.321116, acc.: 85.94%] [G loss: 5.092935]\n",
      "epoch:23 step:18172 [D loss: 0.373529, acc.: 85.16%] [G loss: 2.999488]\n",
      "epoch:23 step:18173 [D loss: 0.257713, acc.: 86.72%] [G loss: 3.951735]\n",
      "epoch:23 step:18174 [D loss: 0.338831, acc.: 83.59%] [G loss: 3.411050]\n",
      "epoch:23 step:18175 [D loss: 0.323752, acc.: 85.94%] [G loss: 3.726975]\n",
      "epoch:23 step:18176 [D loss: 0.274361, acc.: 87.50%] [G loss: 3.159721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18177 [D loss: 0.404291, acc.: 82.03%] [G loss: 2.923563]\n",
      "epoch:23 step:18178 [D loss: 0.360717, acc.: 85.16%] [G loss: 2.781587]\n",
      "epoch:23 step:18179 [D loss: 0.246198, acc.: 89.06%] [G loss: 2.850929]\n",
      "epoch:23 step:18180 [D loss: 0.343299, acc.: 87.50%] [G loss: 2.290267]\n",
      "epoch:23 step:18181 [D loss: 0.340801, acc.: 89.06%] [G loss: 2.762160]\n",
      "epoch:23 step:18182 [D loss: 0.406395, acc.: 86.72%] [G loss: 2.640453]\n",
      "epoch:23 step:18183 [D loss: 0.337287, acc.: 85.94%] [G loss: 2.919114]\n",
      "epoch:23 step:18184 [D loss: 0.294327, acc.: 89.06%] [G loss: 2.235275]\n",
      "epoch:23 step:18185 [D loss: 0.315727, acc.: 88.28%] [G loss: 2.373590]\n",
      "epoch:23 step:18186 [D loss: 0.289384, acc.: 88.28%] [G loss: 2.516387]\n",
      "epoch:23 step:18187 [D loss: 0.305600, acc.: 85.16%] [G loss: 2.814665]\n",
      "epoch:23 step:18188 [D loss: 0.414627, acc.: 82.03%] [G loss: 2.799837]\n",
      "epoch:23 step:18189 [D loss: 0.405503, acc.: 81.25%] [G loss: 2.484733]\n",
      "epoch:23 step:18190 [D loss: 0.294786, acc.: 89.06%] [G loss: 2.830850]\n",
      "epoch:23 step:18191 [D loss: 0.372428, acc.: 82.03%] [G loss: 2.284189]\n",
      "epoch:23 step:18192 [D loss: 0.325408, acc.: 82.03%] [G loss: 2.782865]\n",
      "epoch:23 step:18193 [D loss: 0.366519, acc.: 82.81%] [G loss: 2.383760]\n",
      "epoch:23 step:18194 [D loss: 0.272331, acc.: 89.84%] [G loss: 3.284200]\n",
      "epoch:23 step:18195 [D loss: 0.248815, acc.: 86.72%] [G loss: 3.536444]\n",
      "epoch:23 step:18196 [D loss: 0.313298, acc.: 84.38%] [G loss: 4.992379]\n",
      "epoch:23 step:18197 [D loss: 0.437686, acc.: 81.25%] [G loss: 2.328957]\n",
      "epoch:23 step:18198 [D loss: 0.252107, acc.: 91.41%] [G loss: 3.774040]\n",
      "epoch:23 step:18199 [D loss: 0.281191, acc.: 86.72%] [G loss: 2.482012]\n",
      "epoch:23 step:18200 [D loss: 0.261269, acc.: 90.62%] [G loss: 4.052247]\n",
      "epoch:23 step:18201 [D loss: 0.340748, acc.: 78.91%] [G loss: 2.925297]\n",
      "epoch:23 step:18202 [D loss: 0.277083, acc.: 87.50%] [G loss: 3.245282]\n",
      "epoch:23 step:18203 [D loss: 0.366319, acc.: 85.94%] [G loss: 3.016307]\n",
      "epoch:23 step:18204 [D loss: 0.328777, acc.: 84.38%] [G loss: 2.263760]\n",
      "epoch:23 step:18205 [D loss: 0.322675, acc.: 88.28%] [G loss: 2.990089]\n",
      "epoch:23 step:18206 [D loss: 0.336533, acc.: 83.59%] [G loss: 2.726022]\n",
      "epoch:23 step:18207 [D loss: 0.348562, acc.: 82.81%] [G loss: 4.868041]\n",
      "epoch:23 step:18208 [D loss: 0.268076, acc.: 87.50%] [G loss: 4.515279]\n",
      "epoch:23 step:18209 [D loss: 0.334013, acc.: 82.81%] [G loss: 2.699517]\n",
      "epoch:23 step:18210 [D loss: 0.278847, acc.: 89.84%] [G loss: 3.687520]\n",
      "epoch:23 step:18211 [D loss: 0.308835, acc.: 85.16%] [G loss: 4.329114]\n",
      "epoch:23 step:18212 [D loss: 0.330340, acc.: 81.25%] [G loss: 3.384201]\n",
      "epoch:23 step:18213 [D loss: 0.306505, acc.: 87.50%] [G loss: 4.116278]\n",
      "epoch:23 step:18214 [D loss: 0.247861, acc.: 88.28%] [G loss: 3.438394]\n",
      "epoch:23 step:18215 [D loss: 0.327797, acc.: 84.38%] [G loss: 5.201682]\n",
      "epoch:23 step:18216 [D loss: 0.295985, acc.: 88.28%] [G loss: 2.294659]\n",
      "epoch:23 step:18217 [D loss: 0.337483, acc.: 85.94%] [G loss: 2.788759]\n",
      "epoch:23 step:18218 [D loss: 0.306778, acc.: 89.06%] [G loss: 3.015345]\n",
      "epoch:23 step:18219 [D loss: 0.244073, acc.: 92.19%] [G loss: 2.551691]\n",
      "epoch:23 step:18220 [D loss: 0.308506, acc.: 86.72%] [G loss: 2.726797]\n",
      "epoch:23 step:18221 [D loss: 0.386324, acc.: 79.69%] [G loss: 2.467396]\n",
      "epoch:23 step:18222 [D loss: 0.331532, acc.: 87.50%] [G loss: 3.321196]\n",
      "epoch:23 step:18223 [D loss: 0.387556, acc.: 79.69%] [G loss: 2.943707]\n",
      "epoch:23 step:18224 [D loss: 0.274264, acc.: 91.41%] [G loss: 2.789965]\n",
      "epoch:23 step:18225 [D loss: 0.287337, acc.: 90.62%] [G loss: 3.369205]\n",
      "epoch:23 step:18226 [D loss: 0.311605, acc.: 85.94%] [G loss: 4.001012]\n",
      "epoch:23 step:18227 [D loss: 0.440717, acc.: 78.12%] [G loss: 2.298393]\n",
      "epoch:23 step:18228 [D loss: 0.333002, acc.: 83.59%] [G loss: 2.817116]\n",
      "epoch:23 step:18229 [D loss: 0.369281, acc.: 83.59%] [G loss: 3.693390]\n",
      "epoch:23 step:18230 [D loss: 0.744756, acc.: 71.09%] [G loss: 4.981069]\n",
      "epoch:23 step:18231 [D loss: 0.783453, acc.: 78.12%] [G loss: 4.687134]\n",
      "epoch:23 step:18232 [D loss: 0.502912, acc.: 80.47%] [G loss: 3.474849]\n",
      "epoch:23 step:18233 [D loss: 0.407231, acc.: 84.38%] [G loss: 4.784770]\n",
      "epoch:23 step:18234 [D loss: 0.477359, acc.: 79.69%] [G loss: 4.401908]\n",
      "epoch:23 step:18235 [D loss: 0.339894, acc.: 80.47%] [G loss: 3.761612]\n",
      "epoch:23 step:18236 [D loss: 0.343158, acc.: 82.03%] [G loss: 3.366061]\n",
      "epoch:23 step:18237 [D loss: 0.324076, acc.: 84.38%] [G loss: 3.379932]\n",
      "epoch:23 step:18238 [D loss: 0.387044, acc.: 78.12%] [G loss: 2.927654]\n",
      "epoch:23 step:18239 [D loss: 0.410961, acc.: 83.59%] [G loss: 2.724254]\n",
      "epoch:23 step:18240 [D loss: 0.308812, acc.: 84.38%] [G loss: 2.348157]\n",
      "epoch:23 step:18241 [D loss: 0.374703, acc.: 82.81%] [G loss: 2.919074]\n",
      "epoch:23 step:18242 [D loss: 0.393397, acc.: 80.47%] [G loss: 2.730451]\n",
      "epoch:23 step:18243 [D loss: 0.395536, acc.: 82.03%] [G loss: 3.072682]\n",
      "epoch:23 step:18244 [D loss: 0.349593, acc.: 85.16%] [G loss: 2.861191]\n",
      "epoch:23 step:18245 [D loss: 0.315155, acc.: 89.84%] [G loss: 3.022808]\n",
      "epoch:23 step:18246 [D loss: 0.397685, acc.: 83.59%] [G loss: 3.047254]\n",
      "epoch:23 step:18247 [D loss: 0.343437, acc.: 86.72%] [G loss: 3.838639]\n",
      "epoch:23 step:18248 [D loss: 0.349639, acc.: 82.81%] [G loss: 3.027632]\n",
      "epoch:23 step:18249 [D loss: 0.389982, acc.: 81.25%] [G loss: 3.510466]\n",
      "epoch:23 step:18250 [D loss: 0.357876, acc.: 84.38%] [G loss: 3.011554]\n",
      "epoch:23 step:18251 [D loss: 0.395998, acc.: 84.38%] [G loss: 2.472124]\n",
      "epoch:23 step:18252 [D loss: 0.432580, acc.: 81.25%] [G loss: 2.679142]\n",
      "epoch:23 step:18253 [D loss: 0.389749, acc.: 81.25%] [G loss: 2.900844]\n",
      "epoch:23 step:18254 [D loss: 0.319248, acc.: 85.16%] [G loss: 2.750730]\n",
      "epoch:23 step:18255 [D loss: 0.387132, acc.: 82.03%] [G loss: 2.589352]\n",
      "epoch:23 step:18256 [D loss: 0.264739, acc.: 86.72%] [G loss: 3.540090]\n",
      "epoch:23 step:18257 [D loss: 0.423613, acc.: 81.25%] [G loss: 2.374946]\n",
      "epoch:23 step:18258 [D loss: 0.400369, acc.: 82.03%] [G loss: 2.585214]\n",
      "epoch:23 step:18259 [D loss: 0.306792, acc.: 86.72%] [G loss: 2.409058]\n",
      "epoch:23 step:18260 [D loss: 0.324006, acc.: 88.28%] [G loss: 2.256033]\n",
      "epoch:23 step:18261 [D loss: 0.323684, acc.: 86.72%] [G loss: 2.552415]\n",
      "epoch:23 step:18262 [D loss: 0.254997, acc.: 89.06%] [G loss: 2.661052]\n",
      "epoch:23 step:18263 [D loss: 0.376003, acc.: 80.47%] [G loss: 2.384342]\n",
      "epoch:23 step:18264 [D loss: 0.454067, acc.: 79.69%] [G loss: 2.736903]\n",
      "epoch:23 step:18265 [D loss: 0.414662, acc.: 78.12%] [G loss: 2.806516]\n",
      "epoch:23 step:18266 [D loss: 0.340272, acc.: 85.16%] [G loss: 2.186670]\n",
      "epoch:23 step:18267 [D loss: 0.389602, acc.: 82.81%] [G loss: 2.606744]\n",
      "epoch:23 step:18268 [D loss: 0.263314, acc.: 89.06%] [G loss: 2.921777]\n",
      "epoch:23 step:18269 [D loss: 0.388909, acc.: 80.47%] [G loss: 3.263942]\n",
      "epoch:23 step:18270 [D loss: 0.313656, acc.: 83.59%] [G loss: 3.346028]\n",
      "epoch:23 step:18271 [D loss: 0.331516, acc.: 85.94%] [G loss: 3.857735]\n",
      "epoch:23 step:18272 [D loss: 0.406903, acc.: 81.25%] [G loss: 4.918976]\n",
      "epoch:23 step:18273 [D loss: 0.316609, acc.: 85.16%] [G loss: 2.739860]\n",
      "epoch:23 step:18274 [D loss: 0.483801, acc.: 76.56%] [G loss: 3.475553]\n",
      "epoch:23 step:18275 [D loss: 0.347104, acc.: 84.38%] [G loss: 3.546150]\n",
      "epoch:23 step:18276 [D loss: 0.397400, acc.: 80.47%] [G loss: 3.612882]\n",
      "epoch:23 step:18277 [D loss: 0.301190, acc.: 85.16%] [G loss: 2.586899]\n",
      "epoch:23 step:18278 [D loss: 0.406987, acc.: 81.25%] [G loss: 4.333718]\n",
      "epoch:23 step:18279 [D loss: 0.249813, acc.: 89.84%] [G loss: 4.398226]\n",
      "epoch:23 step:18280 [D loss: 0.444963, acc.: 78.12%] [G loss: 3.956058]\n",
      "epoch:23 step:18281 [D loss: 0.367745, acc.: 82.81%] [G loss: 3.526281]\n",
      "epoch:23 step:18282 [D loss: 0.298210, acc.: 88.28%] [G loss: 3.409459]\n",
      "epoch:23 step:18283 [D loss: 0.324712, acc.: 82.81%] [G loss: 3.655351]\n",
      "epoch:23 step:18284 [D loss: 0.240223, acc.: 89.06%] [G loss: 3.293626]\n",
      "epoch:23 step:18285 [D loss: 0.433024, acc.: 83.59%] [G loss: 6.787576]\n",
      "epoch:23 step:18286 [D loss: 0.645342, acc.: 74.22%] [G loss: 3.914970]\n",
      "epoch:23 step:18287 [D loss: 0.326138, acc.: 82.81%] [G loss: 4.532103]\n",
      "epoch:23 step:18288 [D loss: 0.340978, acc.: 82.81%] [G loss: 4.503249]\n",
      "epoch:23 step:18289 [D loss: 0.314482, acc.: 89.06%] [G loss: 4.066470]\n",
      "epoch:23 step:18290 [D loss: 0.354164, acc.: 85.16%] [G loss: 4.503954]\n",
      "epoch:23 step:18291 [D loss: 0.255417, acc.: 87.50%] [G loss: 2.498648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18292 [D loss: 0.267474, acc.: 85.16%] [G loss: 4.254861]\n",
      "epoch:23 step:18293 [D loss: 0.384802, acc.: 78.91%] [G loss: 3.416588]\n",
      "epoch:23 step:18294 [D loss: 0.442936, acc.: 77.34%] [G loss: 4.483859]\n",
      "epoch:23 step:18295 [D loss: 0.476645, acc.: 78.91%] [G loss: 3.061516]\n",
      "epoch:23 step:18296 [D loss: 0.303472, acc.: 84.38%] [G loss: 2.574874]\n",
      "epoch:23 step:18297 [D loss: 0.350132, acc.: 85.94%] [G loss: 2.849082]\n",
      "epoch:23 step:18298 [D loss: 0.286956, acc.: 87.50%] [G loss: 4.485722]\n",
      "epoch:23 step:18299 [D loss: 0.215602, acc.: 89.84%] [G loss: 4.664824]\n",
      "epoch:23 step:18300 [D loss: 0.293103, acc.: 86.72%] [G loss: 3.657112]\n",
      "epoch:23 step:18301 [D loss: 0.347588, acc.: 79.69%] [G loss: 3.200236]\n",
      "epoch:23 step:18302 [D loss: 0.147688, acc.: 96.88%] [G loss: 4.064961]\n",
      "epoch:23 step:18303 [D loss: 0.455581, acc.: 78.12%] [G loss: 3.735765]\n",
      "epoch:23 step:18304 [D loss: 0.358412, acc.: 82.81%] [G loss: 3.103196]\n",
      "epoch:23 step:18305 [D loss: 0.358052, acc.: 85.94%] [G loss: 3.928811]\n",
      "epoch:23 step:18306 [D loss: 0.280263, acc.: 85.16%] [G loss: 4.233805]\n",
      "epoch:23 step:18307 [D loss: 0.382747, acc.: 82.81%] [G loss: 2.901054]\n",
      "epoch:23 step:18308 [D loss: 0.279118, acc.: 88.28%] [G loss: 3.216929]\n",
      "epoch:23 step:18309 [D loss: 0.402571, acc.: 82.81%] [G loss: 3.223527]\n",
      "epoch:23 step:18310 [D loss: 0.374053, acc.: 81.25%] [G loss: 2.847033]\n",
      "epoch:23 step:18311 [D loss: 0.420402, acc.: 82.03%] [G loss: 2.738941]\n",
      "epoch:23 step:18312 [D loss: 0.335206, acc.: 85.94%] [G loss: 3.455627]\n",
      "epoch:23 step:18313 [D loss: 0.351336, acc.: 82.03%] [G loss: 3.261374]\n",
      "epoch:23 step:18314 [D loss: 0.423059, acc.: 82.81%] [G loss: 2.500410]\n",
      "epoch:23 step:18315 [D loss: 0.296978, acc.: 84.38%] [G loss: 3.322628]\n",
      "epoch:23 step:18316 [D loss: 0.349552, acc.: 85.16%] [G loss: 2.443397]\n",
      "epoch:23 step:18317 [D loss: 0.323821, acc.: 89.06%] [G loss: 2.880703]\n",
      "epoch:23 step:18318 [D loss: 0.281924, acc.: 89.84%] [G loss: 3.134808]\n",
      "epoch:23 step:18319 [D loss: 0.322440, acc.: 84.38%] [G loss: 4.290329]\n",
      "epoch:23 step:18320 [D loss: 0.348400, acc.: 82.03%] [G loss: 2.932420]\n",
      "epoch:23 step:18321 [D loss: 0.338622, acc.: 79.69%] [G loss: 2.857091]\n",
      "epoch:23 step:18322 [D loss: 0.319484, acc.: 84.38%] [G loss: 3.189850]\n",
      "epoch:23 step:18323 [D loss: 0.373706, acc.: 82.03%] [G loss: 3.093797]\n",
      "epoch:23 step:18324 [D loss: 0.311822, acc.: 85.94%] [G loss: 2.400480]\n",
      "epoch:23 step:18325 [D loss: 0.279875, acc.: 89.06%] [G loss: 4.140017]\n",
      "epoch:23 step:18326 [D loss: 0.320720, acc.: 82.81%] [G loss: 3.038209]\n",
      "epoch:23 step:18327 [D loss: 0.381263, acc.: 77.34%] [G loss: 3.345411]\n",
      "epoch:23 step:18328 [D loss: 0.350671, acc.: 82.81%] [G loss: 3.253401]\n",
      "epoch:23 step:18329 [D loss: 0.326559, acc.: 84.38%] [G loss: 3.591026]\n",
      "epoch:23 step:18330 [D loss: 0.324038, acc.: 82.81%] [G loss: 3.826392]\n",
      "epoch:23 step:18331 [D loss: 0.635779, acc.: 77.34%] [G loss: 6.443343]\n",
      "epoch:23 step:18332 [D loss: 1.387510, acc.: 62.50%] [G loss: 9.381909]\n",
      "epoch:23 step:18333 [D loss: 1.224762, acc.: 73.44%] [G loss: 4.905904]\n",
      "epoch:23 step:18334 [D loss: 0.697564, acc.: 75.00%] [G loss: 6.418896]\n",
      "epoch:23 step:18335 [D loss: 0.494092, acc.: 78.91%] [G loss: 3.005866]\n",
      "epoch:23 step:18336 [D loss: 0.424938, acc.: 84.38%] [G loss: 3.187805]\n",
      "epoch:23 step:18337 [D loss: 0.438276, acc.: 78.12%] [G loss: 3.146105]\n",
      "epoch:23 step:18338 [D loss: 0.362481, acc.: 85.16%] [G loss: 2.815585]\n",
      "epoch:23 step:18339 [D loss: 0.322015, acc.: 85.94%] [G loss: 3.135989]\n",
      "epoch:23 step:18340 [D loss: 0.397750, acc.: 82.81%] [G loss: 3.049254]\n",
      "epoch:23 step:18341 [D loss: 0.337965, acc.: 82.81%] [G loss: 3.124054]\n",
      "epoch:23 step:18342 [D loss: 0.312039, acc.: 86.72%] [G loss: 5.564930]\n",
      "epoch:23 step:18343 [D loss: 0.338227, acc.: 85.16%] [G loss: 5.376536]\n",
      "epoch:23 step:18344 [D loss: 0.212171, acc.: 89.84%] [G loss: 6.347496]\n",
      "epoch:23 step:18345 [D loss: 0.223603, acc.: 90.62%] [G loss: 6.549612]\n",
      "epoch:23 step:18346 [D loss: 0.210463, acc.: 91.41%] [G loss: 4.200106]\n",
      "epoch:23 step:18347 [D loss: 0.177860, acc.: 92.97%] [G loss: 6.263454]\n",
      "epoch:23 step:18348 [D loss: 0.287830, acc.: 85.94%] [G loss: 3.660874]\n",
      "epoch:23 step:18349 [D loss: 0.277857, acc.: 88.28%] [G loss: 3.582366]\n",
      "epoch:23 step:18350 [D loss: 0.329258, acc.: 87.50%] [G loss: 3.433519]\n",
      "epoch:23 step:18351 [D loss: 0.406050, acc.: 84.38%] [G loss: 3.125801]\n",
      "epoch:23 step:18352 [D loss: 0.362449, acc.: 83.59%] [G loss: 3.756821]\n",
      "epoch:23 step:18353 [D loss: 0.444663, acc.: 78.91%] [G loss: 2.760133]\n",
      "epoch:23 step:18354 [D loss: 0.401681, acc.: 80.47%] [G loss: 2.660378]\n",
      "epoch:23 step:18355 [D loss: 0.265975, acc.: 89.84%] [G loss: 4.408282]\n",
      "epoch:23 step:18356 [D loss: 0.403221, acc.: 86.72%] [G loss: 2.916842]\n",
      "epoch:23 step:18357 [D loss: 0.346708, acc.: 83.59%] [G loss: 2.487987]\n",
      "epoch:23 step:18358 [D loss: 0.413704, acc.: 82.03%] [G loss: 3.226281]\n",
      "epoch:23 step:18359 [D loss: 0.482989, acc.: 78.91%] [G loss: 2.441481]\n",
      "epoch:23 step:18360 [D loss: 0.312183, acc.: 84.38%] [G loss: 3.119935]\n",
      "epoch:23 step:18361 [D loss: 0.298797, acc.: 82.03%] [G loss: 2.724493]\n",
      "epoch:23 step:18362 [D loss: 0.448729, acc.: 78.12%] [G loss: 2.899395]\n",
      "epoch:23 step:18363 [D loss: 0.270584, acc.: 92.97%] [G loss: 2.919384]\n",
      "epoch:23 step:18364 [D loss: 0.494655, acc.: 79.69%] [G loss: 4.362769]\n",
      "epoch:23 step:18365 [D loss: 0.411290, acc.: 82.81%] [G loss: 4.207413]\n",
      "epoch:23 step:18366 [D loss: 0.580892, acc.: 68.75%] [G loss: 2.668636]\n",
      "epoch:23 step:18367 [D loss: 0.334242, acc.: 84.38%] [G loss: 4.761734]\n",
      "epoch:23 step:18368 [D loss: 0.361454, acc.: 83.59%] [G loss: 3.312407]\n",
      "epoch:23 step:18369 [D loss: 0.345909, acc.: 85.16%] [G loss: 5.746992]\n",
      "epoch:23 step:18370 [D loss: 0.390032, acc.: 79.69%] [G loss: 2.805058]\n",
      "epoch:23 step:18371 [D loss: 0.343020, acc.: 85.94%] [G loss: 3.855786]\n",
      "epoch:23 step:18372 [D loss: 0.291858, acc.: 88.28%] [G loss: 2.330987]\n",
      "epoch:23 step:18373 [D loss: 0.245542, acc.: 92.97%] [G loss: 3.403833]\n",
      "epoch:23 step:18374 [D loss: 0.316059, acc.: 88.28%] [G loss: 2.469162]\n",
      "epoch:23 step:18375 [D loss: 0.313956, acc.: 85.16%] [G loss: 3.036762]\n",
      "epoch:23 step:18376 [D loss: 0.337206, acc.: 82.81%] [G loss: 3.517597]\n",
      "epoch:23 step:18377 [D loss: 0.458936, acc.: 80.47%] [G loss: 4.297453]\n",
      "epoch:23 step:18378 [D loss: 0.378851, acc.: 85.16%] [G loss: 3.203584]\n",
      "epoch:23 step:18379 [D loss: 0.219957, acc.: 92.97%] [G loss: 3.489408]\n",
      "epoch:23 step:18380 [D loss: 0.378516, acc.: 83.59%] [G loss: 2.883002]\n",
      "epoch:23 step:18381 [D loss: 0.282510, acc.: 84.38%] [G loss: 3.362936]\n",
      "epoch:23 step:18382 [D loss: 0.389339, acc.: 81.25%] [G loss: 3.245873]\n",
      "epoch:23 step:18383 [D loss: 0.348620, acc.: 82.03%] [G loss: 2.516207]\n",
      "epoch:23 step:18384 [D loss: 0.253440, acc.: 89.06%] [G loss: 4.565095]\n",
      "epoch:23 step:18385 [D loss: 0.329239, acc.: 85.94%] [G loss: 4.151248]\n",
      "epoch:23 step:18386 [D loss: 0.283580, acc.: 89.06%] [G loss: 2.708988]\n",
      "epoch:23 step:18387 [D loss: 0.293794, acc.: 88.28%] [G loss: 3.296917]\n",
      "epoch:23 step:18388 [D loss: 0.263374, acc.: 89.06%] [G loss: 2.775439]\n",
      "epoch:23 step:18389 [D loss: 0.309740, acc.: 88.28%] [G loss: 3.222914]\n",
      "epoch:23 step:18390 [D loss: 0.321300, acc.: 84.38%] [G loss: 3.033954]\n",
      "epoch:23 step:18391 [D loss: 0.328932, acc.: 83.59%] [G loss: 2.408027]\n",
      "epoch:23 step:18392 [D loss: 0.399869, acc.: 81.25%] [G loss: 2.886934]\n",
      "epoch:23 step:18393 [D loss: 0.461539, acc.: 82.03%] [G loss: 2.081215]\n",
      "epoch:23 step:18394 [D loss: 0.281866, acc.: 89.06%] [G loss: 3.773852]\n",
      "epoch:23 step:18395 [D loss: 0.221860, acc.: 91.41%] [G loss: 4.403002]\n",
      "epoch:23 step:18396 [D loss: 0.241649, acc.: 91.41%] [G loss: 2.945425]\n",
      "epoch:23 step:18397 [D loss: 0.255288, acc.: 93.75%] [G loss: 2.714400]\n",
      "epoch:23 step:18398 [D loss: 0.296207, acc.: 87.50%] [G loss: 2.980437]\n",
      "epoch:23 step:18399 [D loss: 0.296233, acc.: 85.16%] [G loss: 3.652584]\n",
      "epoch:23 step:18400 [D loss: 0.349873, acc.: 88.28%] [G loss: 2.673226]\n",
      "epoch:23 step:18401 [D loss: 0.436012, acc.: 78.12%] [G loss: 2.700063]\n",
      "epoch:23 step:18402 [D loss: 0.234536, acc.: 91.41%] [G loss: 3.133300]\n",
      "epoch:23 step:18403 [D loss: 0.253560, acc.: 89.06%] [G loss: 3.278219]\n",
      "epoch:23 step:18404 [D loss: 0.407258, acc.: 82.81%] [G loss: 2.394819]\n",
      "epoch:23 step:18405 [D loss: 0.307471, acc.: 88.28%] [G loss: 2.523372]\n",
      "epoch:23 step:18406 [D loss: 0.318782, acc.: 89.84%] [G loss: 2.658273]\n",
      "epoch:23 step:18407 [D loss: 0.302095, acc.: 84.38%] [G loss: 3.756603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18408 [D loss: 0.413737, acc.: 76.56%] [G loss: 4.581635]\n",
      "epoch:23 step:18409 [D loss: 0.521431, acc.: 78.91%] [G loss: 2.921578]\n",
      "epoch:23 step:18410 [D loss: 0.279352, acc.: 88.28%] [G loss: 3.180911]\n",
      "epoch:23 step:18411 [D loss: 0.327269, acc.: 86.72%] [G loss: 3.733792]\n",
      "epoch:23 step:18412 [D loss: 0.293539, acc.: 88.28%] [G loss: 2.833241]\n",
      "epoch:23 step:18413 [D loss: 0.457265, acc.: 78.12%] [G loss: 3.984007]\n",
      "epoch:23 step:18414 [D loss: 0.394817, acc.: 81.25%] [G loss: 3.121707]\n",
      "epoch:23 step:18415 [D loss: 0.322868, acc.: 87.50%] [G loss: 3.197433]\n",
      "epoch:23 step:18416 [D loss: 0.375126, acc.: 82.03%] [G loss: 2.331505]\n",
      "epoch:23 step:18417 [D loss: 0.272541, acc.: 91.41%] [G loss: 2.579398]\n",
      "epoch:23 step:18418 [D loss: 0.314917, acc.: 85.16%] [G loss: 2.795659]\n",
      "epoch:23 step:18419 [D loss: 0.388530, acc.: 84.38%] [G loss: 2.421476]\n",
      "epoch:23 step:18420 [D loss: 0.371915, acc.: 82.81%] [G loss: 2.895293]\n",
      "epoch:23 step:18421 [D loss: 0.280367, acc.: 85.94%] [G loss: 2.169775]\n",
      "epoch:23 step:18422 [D loss: 0.282667, acc.: 86.72%] [G loss: 3.846940]\n",
      "epoch:23 step:18423 [D loss: 0.364273, acc.: 81.25%] [G loss: 5.671801]\n",
      "epoch:23 step:18424 [D loss: 0.352972, acc.: 84.38%] [G loss: 3.279262]\n",
      "epoch:23 step:18425 [D loss: 0.292490, acc.: 82.03%] [G loss: 4.142148]\n",
      "epoch:23 step:18426 [D loss: 0.326423, acc.: 85.16%] [G loss: 3.243797]\n",
      "epoch:23 step:18427 [D loss: 0.330322, acc.: 84.38%] [G loss: 3.127219]\n",
      "epoch:23 step:18428 [D loss: 0.303207, acc.: 86.72%] [G loss: 4.098166]\n",
      "epoch:23 step:18429 [D loss: 0.326262, acc.: 81.25%] [G loss: 3.392000]\n",
      "epoch:23 step:18430 [D loss: 0.384540, acc.: 80.47%] [G loss: 3.895370]\n",
      "epoch:23 step:18431 [D loss: 0.414219, acc.: 82.81%] [G loss: 3.326319]\n",
      "epoch:23 step:18432 [D loss: 0.362403, acc.: 85.16%] [G loss: 2.914567]\n",
      "epoch:23 step:18433 [D loss: 0.292411, acc.: 85.94%] [G loss: 2.511354]\n",
      "epoch:23 step:18434 [D loss: 0.379768, acc.: 80.47%] [G loss: 2.697435]\n",
      "epoch:23 step:18435 [D loss: 0.277024, acc.: 86.72%] [G loss: 2.806514]\n",
      "epoch:23 step:18436 [D loss: 0.301696, acc.: 87.50%] [G loss: 3.329769]\n",
      "epoch:23 step:18437 [D loss: 0.319674, acc.: 85.94%] [G loss: 2.735719]\n",
      "epoch:23 step:18438 [D loss: 0.292295, acc.: 86.72%] [G loss: 3.744295]\n",
      "epoch:23 step:18439 [D loss: 0.275109, acc.: 89.06%] [G loss: 3.330331]\n",
      "epoch:23 step:18440 [D loss: 0.209473, acc.: 91.41%] [G loss: 2.857028]\n",
      "epoch:23 step:18441 [D loss: 0.279758, acc.: 85.94%] [G loss: 3.904128]\n",
      "epoch:23 step:18442 [D loss: 0.353862, acc.: 83.59%] [G loss: 4.300967]\n",
      "epoch:23 step:18443 [D loss: 0.278484, acc.: 88.28%] [G loss: 3.130868]\n",
      "epoch:23 step:18444 [D loss: 0.299579, acc.: 86.72%] [G loss: 5.573670]\n",
      "epoch:23 step:18445 [D loss: 0.292813, acc.: 84.38%] [G loss: 3.584290]\n",
      "epoch:23 step:18446 [D loss: 0.306927, acc.: 88.28%] [G loss: 3.708141]\n",
      "epoch:23 step:18447 [D loss: 0.335866, acc.: 87.50%] [G loss: 4.238368]\n",
      "epoch:23 step:18448 [D loss: 0.268609, acc.: 89.06%] [G loss: 4.119794]\n",
      "epoch:23 step:18449 [D loss: 0.320548, acc.: 82.81%] [G loss: 2.799421]\n",
      "epoch:23 step:18450 [D loss: 0.269937, acc.: 87.50%] [G loss: 3.216362]\n",
      "epoch:23 step:18451 [D loss: 0.317246, acc.: 83.59%] [G loss: 2.824607]\n",
      "epoch:23 step:18452 [D loss: 0.378205, acc.: 83.59%] [G loss: 3.097957]\n",
      "epoch:23 step:18453 [D loss: 0.351002, acc.: 84.38%] [G loss: 2.602790]\n",
      "epoch:23 step:18454 [D loss: 0.313212, acc.: 84.38%] [G loss: 3.271108]\n",
      "epoch:23 step:18455 [D loss: 0.326003, acc.: 85.16%] [G loss: 2.338229]\n",
      "epoch:23 step:18456 [D loss: 0.358098, acc.: 78.91%] [G loss: 2.769977]\n",
      "epoch:23 step:18457 [D loss: 0.263194, acc.: 92.19%] [G loss: 3.218887]\n",
      "epoch:23 step:18458 [D loss: 0.409694, acc.: 84.38%] [G loss: 2.472660]\n",
      "epoch:23 step:18459 [D loss: 0.252347, acc.: 90.62%] [G loss: 2.796933]\n",
      "epoch:23 step:18460 [D loss: 0.352394, acc.: 82.81%] [G loss: 5.226057]\n",
      "epoch:23 step:18461 [D loss: 0.190116, acc.: 91.41%] [G loss: 4.924810]\n",
      "epoch:23 step:18462 [D loss: 0.310392, acc.: 82.03%] [G loss: 5.171383]\n",
      "epoch:23 step:18463 [D loss: 0.269265, acc.: 90.62%] [G loss: 3.598884]\n",
      "epoch:23 step:18464 [D loss: 0.328381, acc.: 87.50%] [G loss: 3.881289]\n",
      "epoch:23 step:18465 [D loss: 0.251736, acc.: 90.62%] [G loss: 3.655321]\n",
      "epoch:23 step:18466 [D loss: 0.267196, acc.: 89.06%] [G loss: 3.087471]\n",
      "epoch:23 step:18467 [D loss: 0.257564, acc.: 89.84%] [G loss: 2.880768]\n",
      "epoch:23 step:18468 [D loss: 0.277227, acc.: 90.62%] [G loss: 3.192591]\n",
      "epoch:23 step:18469 [D loss: 0.444018, acc.: 76.56%] [G loss: 3.615492]\n",
      "epoch:23 step:18470 [D loss: 0.308513, acc.: 88.28%] [G loss: 2.979190]\n",
      "epoch:23 step:18471 [D loss: 0.251876, acc.: 92.19%] [G loss: 3.170927]\n",
      "epoch:23 step:18472 [D loss: 0.323286, acc.: 86.72%] [G loss: 3.929813]\n",
      "epoch:23 step:18473 [D loss: 0.371864, acc.: 82.81%] [G loss: 2.635306]\n",
      "epoch:23 step:18474 [D loss: 0.332133, acc.: 82.81%] [G loss: 2.678345]\n",
      "epoch:23 step:18475 [D loss: 0.225886, acc.: 91.41%] [G loss: 3.394053]\n",
      "epoch:23 step:18476 [D loss: 0.334569, acc.: 84.38%] [G loss: 3.191904]\n",
      "epoch:23 step:18477 [D loss: 0.309292, acc.: 88.28%] [G loss: 2.955204]\n",
      "epoch:23 step:18478 [D loss: 0.383091, acc.: 85.94%] [G loss: 3.173758]\n",
      "epoch:23 step:18479 [D loss: 0.384312, acc.: 78.91%] [G loss: 2.778377]\n",
      "epoch:23 step:18480 [D loss: 0.363515, acc.: 80.47%] [G loss: 2.670030]\n",
      "epoch:23 step:18481 [D loss: 0.223796, acc.: 89.84%] [G loss: 3.387697]\n",
      "epoch:23 step:18482 [D loss: 0.340706, acc.: 87.50%] [G loss: 2.487500]\n",
      "epoch:23 step:18483 [D loss: 0.445804, acc.: 78.12%] [G loss: 3.052385]\n",
      "epoch:23 step:18484 [D loss: 0.276359, acc.: 88.28%] [G loss: 2.672503]\n",
      "epoch:23 step:18485 [D loss: 0.366701, acc.: 82.03%] [G loss: 3.880874]\n",
      "epoch:23 step:18486 [D loss: 0.303582, acc.: 85.94%] [G loss: 4.470585]\n",
      "epoch:23 step:18487 [D loss: 0.243619, acc.: 92.97%] [G loss: 2.611897]\n",
      "epoch:23 step:18488 [D loss: 0.326720, acc.: 85.16%] [G loss: 3.076405]\n",
      "epoch:23 step:18489 [D loss: 0.343487, acc.: 82.81%] [G loss: 4.325265]\n",
      "epoch:23 step:18490 [D loss: 0.299657, acc.: 90.62%] [G loss: 3.973350]\n",
      "epoch:23 step:18491 [D loss: 0.354441, acc.: 85.94%] [G loss: 2.841517]\n",
      "epoch:23 step:18492 [D loss: 0.250840, acc.: 86.72%] [G loss: 5.138667]\n",
      "epoch:23 step:18493 [D loss: 0.413800, acc.: 83.59%] [G loss: 3.590719]\n",
      "epoch:23 step:18494 [D loss: 0.315966, acc.: 83.59%] [G loss: 3.806229]\n",
      "epoch:23 step:18495 [D loss: 0.352259, acc.: 82.81%] [G loss: 3.649139]\n",
      "epoch:23 step:18496 [D loss: 0.306114, acc.: 89.06%] [G loss: 2.449634]\n",
      "epoch:23 step:18497 [D loss: 0.343068, acc.: 85.16%] [G loss: 4.237155]\n",
      "epoch:23 step:18498 [D loss: 0.273456, acc.: 85.16%] [G loss: 2.666386]\n",
      "epoch:23 step:18499 [D loss: 0.312359, acc.: 85.16%] [G loss: 3.656666]\n",
      "epoch:23 step:18500 [D loss: 0.353847, acc.: 83.59%] [G loss: 2.901294]\n",
      "epoch:23 step:18501 [D loss: 0.287764, acc.: 89.84%] [G loss: 3.190996]\n",
      "epoch:23 step:18502 [D loss: 0.432917, acc.: 78.91%] [G loss: 2.378941]\n",
      "epoch:23 step:18503 [D loss: 0.305408, acc.: 88.28%] [G loss: 2.479569]\n",
      "epoch:23 step:18504 [D loss: 0.377454, acc.: 84.38%] [G loss: 3.171858]\n",
      "epoch:23 step:18505 [D loss: 0.342759, acc.: 84.38%] [G loss: 3.509051]\n",
      "epoch:23 step:18506 [D loss: 0.304310, acc.: 83.59%] [G loss: 3.284492]\n",
      "epoch:23 step:18507 [D loss: 0.368031, acc.: 85.94%] [G loss: 2.589487]\n",
      "epoch:23 step:18508 [D loss: 0.246633, acc.: 89.84%] [G loss: 3.328127]\n",
      "epoch:23 step:18509 [D loss: 0.449012, acc.: 76.56%] [G loss: 3.016391]\n",
      "epoch:23 step:18510 [D loss: 0.439658, acc.: 77.34%] [G loss: 3.741723]\n",
      "epoch:23 step:18511 [D loss: 0.234253, acc.: 89.06%] [G loss: 3.813488]\n",
      "epoch:23 step:18512 [D loss: 0.365981, acc.: 82.03%] [G loss: 2.581854]\n",
      "epoch:23 step:18513 [D loss: 0.251018, acc.: 89.06%] [G loss: 2.510685]\n",
      "epoch:23 step:18514 [D loss: 0.302627, acc.: 86.72%] [G loss: 2.364224]\n",
      "epoch:23 step:18515 [D loss: 0.396711, acc.: 82.03%] [G loss: 2.835218]\n",
      "epoch:23 step:18516 [D loss: 0.337588, acc.: 85.94%] [G loss: 2.453528]\n",
      "epoch:23 step:18517 [D loss: 0.372752, acc.: 83.59%] [G loss: 2.487163]\n",
      "epoch:23 step:18518 [D loss: 0.323098, acc.: 87.50%] [G loss: 2.678272]\n",
      "epoch:23 step:18519 [D loss: 0.267577, acc.: 90.62%] [G loss: 2.947268]\n",
      "epoch:23 step:18520 [D loss: 0.317525, acc.: 85.94%] [G loss: 4.185064]\n",
      "epoch:23 step:18521 [D loss: 0.236775, acc.: 90.62%] [G loss: 4.750023]\n",
      "epoch:23 step:18522 [D loss: 0.354271, acc.: 81.25%] [G loss: 2.802956]\n",
      "epoch:23 step:18523 [D loss: 0.308396, acc.: 85.94%] [G loss: 3.120375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18524 [D loss: 0.253149, acc.: 87.50%] [G loss: 3.085353]\n",
      "epoch:23 step:18525 [D loss: 0.392181, acc.: 82.03%] [G loss: 3.036288]\n",
      "epoch:23 step:18526 [D loss: 0.295357, acc.: 89.06%] [G loss: 3.467698]\n",
      "epoch:23 step:18527 [D loss: 0.300024, acc.: 86.72%] [G loss: 3.861402]\n",
      "epoch:23 step:18528 [D loss: 0.318504, acc.: 87.50%] [G loss: 2.708210]\n",
      "epoch:23 step:18529 [D loss: 0.268622, acc.: 85.94%] [G loss: 3.123129]\n",
      "epoch:23 step:18530 [D loss: 0.266395, acc.: 87.50%] [G loss: 3.428106]\n",
      "epoch:23 step:18531 [D loss: 0.226146, acc.: 93.75%] [G loss: 2.859620]\n",
      "epoch:23 step:18532 [D loss: 0.279168, acc.: 89.06%] [G loss: 2.633286]\n",
      "epoch:23 step:18533 [D loss: 0.343324, acc.: 82.81%] [G loss: 2.734531]\n",
      "epoch:23 step:18534 [D loss: 0.244750, acc.: 90.62%] [G loss: 3.472980]\n",
      "epoch:23 step:18535 [D loss: 0.322406, acc.: 83.59%] [G loss: 3.459847]\n",
      "epoch:23 step:18536 [D loss: 0.243565, acc.: 92.97%] [G loss: 2.988520]\n",
      "epoch:23 step:18537 [D loss: 0.248103, acc.: 90.62%] [G loss: 3.682487]\n",
      "epoch:23 step:18538 [D loss: 0.366566, acc.: 83.59%] [G loss: 3.029133]\n",
      "epoch:23 step:18539 [D loss: 0.296828, acc.: 85.94%] [G loss: 3.464133]\n",
      "epoch:23 step:18540 [D loss: 0.436248, acc.: 81.25%] [G loss: 3.453855]\n",
      "epoch:23 step:18541 [D loss: 0.416531, acc.: 83.59%] [G loss: 3.715118]\n",
      "epoch:23 step:18542 [D loss: 0.308207, acc.: 86.72%] [G loss: 3.875794]\n",
      "epoch:23 step:18543 [D loss: 0.357523, acc.: 83.59%] [G loss: 3.809293]\n",
      "epoch:23 step:18544 [D loss: 0.312878, acc.: 85.16%] [G loss: 3.137682]\n",
      "epoch:23 step:18545 [D loss: 0.247186, acc.: 91.41%] [G loss: 2.995781]\n",
      "epoch:23 step:18546 [D loss: 0.413282, acc.: 82.03%] [G loss: 4.370395]\n",
      "epoch:23 step:18547 [D loss: 0.396776, acc.: 82.03%] [G loss: 3.260427]\n",
      "epoch:23 step:18548 [D loss: 0.357673, acc.: 89.06%] [G loss: 3.454162]\n",
      "epoch:23 step:18549 [D loss: 0.270102, acc.: 89.84%] [G loss: 4.706757]\n",
      "epoch:23 step:18550 [D loss: 0.323736, acc.: 85.16%] [G loss: 5.329271]\n",
      "epoch:23 step:18551 [D loss: 0.311714, acc.: 85.94%] [G loss: 3.317937]\n",
      "epoch:23 step:18552 [D loss: 0.299865, acc.: 85.16%] [G loss: 3.669714]\n",
      "epoch:23 step:18553 [D loss: 0.350136, acc.: 82.81%] [G loss: 3.303272]\n",
      "epoch:23 step:18554 [D loss: 0.362179, acc.: 82.81%] [G loss: 3.707330]\n",
      "epoch:23 step:18555 [D loss: 0.257806, acc.: 90.62%] [G loss: 3.315732]\n",
      "epoch:23 step:18556 [D loss: 0.366996, acc.: 82.81%] [G loss: 3.799347]\n",
      "epoch:23 step:18557 [D loss: 0.430698, acc.: 82.03%] [G loss: 2.580666]\n",
      "epoch:23 step:18558 [D loss: 0.328886, acc.: 84.38%] [G loss: 3.621318]\n",
      "epoch:23 step:18559 [D loss: 0.332558, acc.: 85.94%] [G loss: 4.436089]\n",
      "epoch:23 step:18560 [D loss: 0.328380, acc.: 85.16%] [G loss: 5.921474]\n",
      "epoch:23 step:18561 [D loss: 0.403577, acc.: 78.12%] [G loss: 2.453227]\n",
      "epoch:23 step:18562 [D loss: 0.244336, acc.: 92.19%] [G loss: 3.451595]\n",
      "epoch:23 step:18563 [D loss: 0.254815, acc.: 89.06%] [G loss: 3.494415]\n",
      "epoch:23 step:18564 [D loss: 0.379988, acc.: 84.38%] [G loss: 3.408928]\n",
      "epoch:23 step:18565 [D loss: 0.373720, acc.: 82.03%] [G loss: 3.800929]\n",
      "epoch:23 step:18566 [D loss: 0.327066, acc.: 85.16%] [G loss: 2.814354]\n",
      "epoch:23 step:18567 [D loss: 0.250153, acc.: 90.62%] [G loss: 2.672570]\n",
      "epoch:23 step:18568 [D loss: 0.378286, acc.: 85.94%] [G loss: 2.663282]\n",
      "epoch:23 step:18569 [D loss: 0.352961, acc.: 82.81%] [G loss: 3.840676]\n",
      "epoch:23 step:18570 [D loss: 0.286830, acc.: 85.94%] [G loss: 3.652349]\n",
      "epoch:23 step:18571 [D loss: 0.350708, acc.: 85.94%] [G loss: 3.655263]\n",
      "epoch:23 step:18572 [D loss: 0.299505, acc.: 86.72%] [G loss: 3.113595]\n",
      "epoch:23 step:18573 [D loss: 0.321193, acc.: 83.59%] [G loss: 2.758705]\n",
      "epoch:23 step:18574 [D loss: 0.289512, acc.: 87.50%] [G loss: 2.980360]\n",
      "epoch:23 step:18575 [D loss: 0.417280, acc.: 79.69%] [G loss: 2.603620]\n",
      "epoch:23 step:18576 [D loss: 0.384497, acc.: 78.91%] [G loss: 2.368141]\n",
      "epoch:23 step:18577 [D loss: 0.393284, acc.: 84.38%] [G loss: 3.840322]\n",
      "epoch:23 step:18578 [D loss: 0.380557, acc.: 85.16%] [G loss: 3.409185]\n",
      "epoch:23 step:18579 [D loss: 0.294540, acc.: 86.72%] [G loss: 3.306676]\n",
      "epoch:23 step:18580 [D loss: 0.338023, acc.: 85.16%] [G loss: 3.278650]\n",
      "epoch:23 step:18581 [D loss: 0.302086, acc.: 86.72%] [G loss: 4.444333]\n",
      "epoch:23 step:18582 [D loss: 0.290906, acc.: 85.94%] [G loss: 3.808465]\n",
      "epoch:23 step:18583 [D loss: 0.228503, acc.: 89.06%] [G loss: 3.184111]\n",
      "epoch:23 step:18584 [D loss: 0.183637, acc.: 93.75%] [G loss: 4.231715]\n",
      "epoch:23 step:18585 [D loss: 0.251061, acc.: 89.06%] [G loss: 4.117156]\n",
      "epoch:23 step:18586 [D loss: 0.280983, acc.: 89.84%] [G loss: 3.276535]\n",
      "epoch:23 step:18587 [D loss: 0.342525, acc.: 82.81%] [G loss: 2.674083]\n",
      "epoch:23 step:18588 [D loss: 0.368997, acc.: 78.91%] [G loss: 2.977971]\n",
      "epoch:23 step:18589 [D loss: 0.294344, acc.: 87.50%] [G loss: 2.878277]\n",
      "epoch:23 step:18590 [D loss: 0.250652, acc.: 90.62%] [G loss: 2.767180]\n",
      "epoch:23 step:18591 [D loss: 0.382506, acc.: 82.03%] [G loss: 2.581651]\n",
      "epoch:23 step:18592 [D loss: 0.333288, acc.: 88.28%] [G loss: 3.273679]\n",
      "epoch:23 step:18593 [D loss: 0.213201, acc.: 92.97%] [G loss: 3.806188]\n",
      "epoch:23 step:18594 [D loss: 0.283674, acc.: 89.06%] [G loss: 2.592389]\n",
      "epoch:23 step:18595 [D loss: 0.232217, acc.: 89.84%] [G loss: 3.770046]\n",
      "epoch:23 step:18596 [D loss: 0.344396, acc.: 79.69%] [G loss: 3.136446]\n",
      "epoch:23 step:18597 [D loss: 0.297223, acc.: 89.06%] [G loss: 2.929716]\n",
      "epoch:23 step:18598 [D loss: 0.364518, acc.: 83.59%] [G loss: 3.433840]\n",
      "epoch:23 step:18599 [D loss: 0.239928, acc.: 89.06%] [G loss: 3.883525]\n",
      "epoch:23 step:18600 [D loss: 0.291107, acc.: 88.28%] [G loss: 5.187814]\n",
      "epoch:23 step:18601 [D loss: 0.430105, acc.: 81.25%] [G loss: 3.049270]\n",
      "epoch:23 step:18602 [D loss: 0.451608, acc.: 77.34%] [G loss: 2.295146]\n",
      "epoch:23 step:18603 [D loss: 0.322336, acc.: 85.16%] [G loss: 2.613421]\n",
      "epoch:23 step:18604 [D loss: 0.249961, acc.: 90.62%] [G loss: 2.430871]\n",
      "epoch:23 step:18605 [D loss: 0.246381, acc.: 91.41%] [G loss: 3.591287]\n",
      "epoch:23 step:18606 [D loss: 0.340503, acc.: 85.94%] [G loss: 4.339178]\n",
      "epoch:23 step:18607 [D loss: 0.380034, acc.: 82.81%] [G loss: 2.893379]\n",
      "epoch:23 step:18608 [D loss: 0.328533, acc.: 85.16%] [G loss: 3.239272]\n",
      "epoch:23 step:18609 [D loss: 0.337021, acc.: 86.72%] [G loss: 3.128699]\n",
      "epoch:23 step:18610 [D loss: 0.235850, acc.: 85.94%] [G loss: 4.572477]\n",
      "epoch:23 step:18611 [D loss: 0.339184, acc.: 89.06%] [G loss: 3.034055]\n",
      "epoch:23 step:18612 [D loss: 0.211930, acc.: 90.62%] [G loss: 5.500952]\n",
      "epoch:23 step:18613 [D loss: 0.316129, acc.: 83.59%] [G loss: 3.879554]\n",
      "epoch:23 step:18614 [D loss: 0.262070, acc.: 90.62%] [G loss: 3.117060]\n",
      "epoch:23 step:18615 [D loss: 0.275874, acc.: 85.94%] [G loss: 3.787814]\n",
      "epoch:23 step:18616 [D loss: 0.301925, acc.: 88.28%] [G loss: 4.206479]\n",
      "epoch:23 step:18617 [D loss: 0.285306, acc.: 87.50%] [G loss: 4.349073]\n",
      "epoch:23 step:18618 [D loss: 0.330584, acc.: 82.81%] [G loss: 3.238703]\n",
      "epoch:23 step:18619 [D loss: 0.220985, acc.: 91.41%] [G loss: 2.934144]\n",
      "epoch:23 step:18620 [D loss: 0.405676, acc.: 80.47%] [G loss: 2.946315]\n",
      "epoch:23 step:18621 [D loss: 0.299501, acc.: 86.72%] [G loss: 2.945773]\n",
      "epoch:23 step:18622 [D loss: 0.285137, acc.: 87.50%] [G loss: 2.698534]\n",
      "epoch:23 step:18623 [D loss: 0.432989, acc.: 75.78%] [G loss: 4.948534]\n",
      "epoch:23 step:18624 [D loss: 0.606986, acc.: 76.56%] [G loss: 4.199705]\n",
      "epoch:23 step:18625 [D loss: 0.720212, acc.: 72.66%] [G loss: 4.330966]\n",
      "epoch:23 step:18626 [D loss: 0.346211, acc.: 85.94%] [G loss: 3.977446]\n",
      "epoch:23 step:18627 [D loss: 0.425658, acc.: 81.25%] [G loss: 4.930971]\n",
      "epoch:23 step:18628 [D loss: 0.214103, acc.: 89.06%] [G loss: 3.457169]\n",
      "epoch:23 step:18629 [D loss: 0.293241, acc.: 86.72%] [G loss: 4.243830]\n",
      "epoch:23 step:18630 [D loss: 0.259160, acc.: 88.28%] [G loss: 4.944858]\n",
      "epoch:23 step:18631 [D loss: 0.316907, acc.: 86.72%] [G loss: 3.355355]\n",
      "epoch:23 step:18632 [D loss: 0.427566, acc.: 80.47%] [G loss: 4.051653]\n",
      "epoch:23 step:18633 [D loss: 0.420727, acc.: 80.47%] [G loss: 4.715894]\n",
      "epoch:23 step:18634 [D loss: 0.341180, acc.: 85.94%] [G loss: 3.164713]\n",
      "epoch:23 step:18635 [D loss: 0.349807, acc.: 84.38%] [G loss: 5.193755]\n",
      "epoch:23 step:18636 [D loss: 0.292291, acc.: 88.28%] [G loss: 5.778204]\n",
      "epoch:23 step:18637 [D loss: 0.352846, acc.: 84.38%] [G loss: 4.545399]\n",
      "epoch:23 step:18638 [D loss: 0.315044, acc.: 84.38%] [G loss: 5.311054]\n",
      "epoch:23 step:18639 [D loss: 0.343902, acc.: 85.16%] [G loss: 3.059674]\n",
      "epoch:23 step:18640 [D loss: 0.239405, acc.: 89.06%] [G loss: 5.051961]\n",
      "epoch:23 step:18641 [D loss: 0.324190, acc.: 85.16%] [G loss: 4.222856]\n",
      "epoch:23 step:18642 [D loss: 0.251403, acc.: 91.41%] [G loss: 4.886415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18643 [D loss: 0.302354, acc.: 85.94%] [G loss: 5.709826]\n",
      "epoch:23 step:18644 [D loss: 0.183288, acc.: 92.19%] [G loss: 4.554767]\n",
      "epoch:23 step:18645 [D loss: 0.207230, acc.: 90.62%] [G loss: 4.286078]\n",
      "epoch:23 step:18646 [D loss: 0.276535, acc.: 89.06%] [G loss: 4.545205]\n",
      "epoch:23 step:18647 [D loss: 0.272118, acc.: 87.50%] [G loss: 2.768355]\n",
      "epoch:23 step:18648 [D loss: 0.259869, acc.: 85.94%] [G loss: 5.560928]\n",
      "epoch:23 step:18649 [D loss: 0.317565, acc.: 84.38%] [G loss: 3.585598]\n",
      "epoch:23 step:18650 [D loss: 0.251042, acc.: 87.50%] [G loss: 3.494962]\n",
      "epoch:23 step:18651 [D loss: 0.211256, acc.: 94.53%] [G loss: 3.571537]\n",
      "epoch:23 step:18652 [D loss: 0.343516, acc.: 83.59%] [G loss: 2.949796]\n",
      "epoch:23 step:18653 [D loss: 0.225306, acc.: 90.62%] [G loss: 3.563285]\n",
      "epoch:23 step:18654 [D loss: 0.221972, acc.: 91.41%] [G loss: 2.884009]\n",
      "epoch:23 step:18655 [D loss: 0.421935, acc.: 80.47%] [G loss: 3.401227]\n",
      "epoch:23 step:18656 [D loss: 0.427015, acc.: 78.12%] [G loss: 3.895055]\n",
      "epoch:23 step:18657 [D loss: 0.272458, acc.: 87.50%] [G loss: 4.683542]\n",
      "epoch:23 step:18658 [D loss: 0.245343, acc.: 89.06%] [G loss: 5.474141]\n",
      "epoch:23 step:18659 [D loss: 0.280677, acc.: 90.62%] [G loss: 3.802743]\n",
      "epoch:23 step:18660 [D loss: 0.178215, acc.: 93.75%] [G loss: 3.998255]\n",
      "epoch:23 step:18661 [D loss: 0.348088, acc.: 82.81%] [G loss: 2.998854]\n",
      "epoch:23 step:18662 [D loss: 0.314794, acc.: 85.94%] [G loss: 4.150342]\n",
      "epoch:23 step:18663 [D loss: 0.303528, acc.: 86.72%] [G loss: 4.507824]\n",
      "epoch:23 step:18664 [D loss: 0.275682, acc.: 85.16%] [G loss: 5.361794]\n",
      "epoch:23 step:18665 [D loss: 0.243317, acc.: 89.06%] [G loss: 6.214146]\n",
      "epoch:23 step:18666 [D loss: 0.395373, acc.: 83.59%] [G loss: 2.639336]\n",
      "epoch:23 step:18667 [D loss: 0.255066, acc.: 89.06%] [G loss: 3.367902]\n",
      "epoch:23 step:18668 [D loss: 0.333766, acc.: 87.50%] [G loss: 3.698034]\n",
      "epoch:23 step:18669 [D loss: 0.351657, acc.: 84.38%] [G loss: 3.498173]\n",
      "epoch:23 step:18670 [D loss: 0.489824, acc.: 76.56%] [G loss: 2.321084]\n",
      "epoch:23 step:18671 [D loss: 0.327141, acc.: 89.06%] [G loss: 3.008171]\n",
      "epoch:23 step:18672 [D loss: 0.409047, acc.: 82.81%] [G loss: 2.352207]\n",
      "epoch:23 step:18673 [D loss: 0.345859, acc.: 85.16%] [G loss: 2.530543]\n",
      "epoch:23 step:18674 [D loss: 0.330764, acc.: 87.50%] [G loss: 2.820610]\n",
      "epoch:23 step:18675 [D loss: 0.255314, acc.: 89.84%] [G loss: 3.060263]\n",
      "epoch:23 step:18676 [D loss: 0.332219, acc.: 85.16%] [G loss: 3.108437]\n",
      "epoch:23 step:18677 [D loss: 0.295087, acc.: 89.06%] [G loss: 2.712329]\n",
      "epoch:23 step:18678 [D loss: 0.311838, acc.: 85.94%] [G loss: 4.886129]\n",
      "epoch:23 step:18679 [D loss: 0.412072, acc.: 82.03%] [G loss: 3.628323]\n",
      "epoch:23 step:18680 [D loss: 0.219829, acc.: 88.28%] [G loss: 3.555654]\n",
      "epoch:23 step:18681 [D loss: 0.291791, acc.: 85.16%] [G loss: 3.056351]\n",
      "epoch:23 step:18682 [D loss: 0.216239, acc.: 88.28%] [G loss: 4.626582]\n",
      "epoch:23 step:18683 [D loss: 0.254722, acc.: 89.84%] [G loss: 4.236710]\n",
      "epoch:23 step:18684 [D loss: 0.344434, acc.: 83.59%] [G loss: 4.323463]\n",
      "epoch:23 step:18685 [D loss: 0.203368, acc.: 92.97%] [G loss: 4.364741]\n",
      "epoch:23 step:18686 [D loss: 0.210768, acc.: 93.75%] [G loss: 4.414733]\n",
      "epoch:23 step:18687 [D loss: 0.168656, acc.: 92.97%] [G loss: 4.856139]\n",
      "epoch:23 step:18688 [D loss: 0.243455, acc.: 89.84%] [G loss: 4.334423]\n",
      "epoch:23 step:18689 [D loss: 0.256272, acc.: 87.50%] [G loss: 3.170897]\n",
      "epoch:23 step:18690 [D loss: 0.252209, acc.: 89.06%] [G loss: 3.201973]\n",
      "epoch:23 step:18691 [D loss: 0.338322, acc.: 84.38%] [G loss: 3.372318]\n",
      "epoch:23 step:18692 [D loss: 0.331644, acc.: 83.59%] [G loss: 4.111038]\n",
      "epoch:23 step:18693 [D loss: 0.252324, acc.: 92.19%] [G loss: 4.215549]\n",
      "epoch:23 step:18694 [D loss: 0.395762, acc.: 82.81%] [G loss: 3.805263]\n",
      "epoch:23 step:18695 [D loss: 0.378987, acc.: 86.72%] [G loss: 4.027102]\n",
      "epoch:23 step:18696 [D loss: 0.368065, acc.: 85.16%] [G loss: 3.378664]\n",
      "epoch:23 step:18697 [D loss: 0.336817, acc.: 86.72%] [G loss: 2.825620]\n",
      "epoch:23 step:18698 [D loss: 0.293418, acc.: 88.28%] [G loss: 2.488734]\n",
      "epoch:23 step:18699 [D loss: 0.397922, acc.: 82.03%] [G loss: 3.562147]\n",
      "epoch:23 step:18700 [D loss: 0.511820, acc.: 75.00%] [G loss: 5.673586]\n",
      "epoch:23 step:18701 [D loss: 0.585195, acc.: 77.34%] [G loss: 3.927004]\n",
      "epoch:23 step:18702 [D loss: 0.477567, acc.: 77.34%] [G loss: 5.120985]\n",
      "epoch:23 step:18703 [D loss: 0.320574, acc.: 82.81%] [G loss: 3.894659]\n",
      "epoch:23 step:18704 [D loss: 0.302626, acc.: 86.72%] [G loss: 3.912544]\n",
      "epoch:23 step:18705 [D loss: 0.228254, acc.: 93.75%] [G loss: 3.242666]\n",
      "epoch:23 step:18706 [D loss: 0.262612, acc.: 85.94%] [G loss: 2.531364]\n",
      "epoch:23 step:18707 [D loss: 0.270172, acc.: 90.62%] [G loss: 2.981649]\n",
      "epoch:23 step:18708 [D loss: 0.277732, acc.: 89.84%] [G loss: 2.484090]\n",
      "epoch:23 step:18709 [D loss: 0.309004, acc.: 89.06%] [G loss: 3.132799]\n",
      "epoch:23 step:18710 [D loss: 0.395942, acc.: 83.59%] [G loss: 2.391239]\n",
      "epoch:23 step:18711 [D loss: 0.304985, acc.: 88.28%] [G loss: 2.956629]\n",
      "epoch:23 step:18712 [D loss: 0.314063, acc.: 85.16%] [G loss: 3.291816]\n",
      "epoch:23 step:18713 [D loss: 0.316681, acc.: 87.50%] [G loss: 2.308252]\n",
      "epoch:23 step:18714 [D loss: 0.295072, acc.: 87.50%] [G loss: 2.562465]\n",
      "epoch:23 step:18715 [D loss: 0.245258, acc.: 91.41%] [G loss: 2.891685]\n",
      "epoch:23 step:18716 [D loss: 0.371732, acc.: 87.50%] [G loss: 2.505508]\n",
      "epoch:23 step:18717 [D loss: 0.364792, acc.: 84.38%] [G loss: 2.818359]\n",
      "epoch:23 step:18718 [D loss: 0.240863, acc.: 91.41%] [G loss: 3.240808]\n",
      "epoch:23 step:18719 [D loss: 0.219220, acc.: 92.97%] [G loss: 5.719680]\n",
      "epoch:23 step:18720 [D loss: 0.334375, acc.: 82.03%] [G loss: 3.769369]\n",
      "epoch:23 step:18721 [D loss: 0.326477, acc.: 84.38%] [G loss: 3.150600]\n",
      "epoch:23 step:18722 [D loss: 0.350384, acc.: 82.81%] [G loss: 2.979317]\n",
      "epoch:23 step:18723 [D loss: 0.278829, acc.: 89.84%] [G loss: 3.251129]\n",
      "epoch:23 step:18724 [D loss: 0.351587, acc.: 84.38%] [G loss: 3.690300]\n",
      "epoch:23 step:18725 [D loss: 0.370185, acc.: 80.47%] [G loss: 4.153525]\n",
      "epoch:23 step:18726 [D loss: 0.293915, acc.: 86.72%] [G loss: 3.393030]\n",
      "epoch:23 step:18727 [D loss: 0.278981, acc.: 89.06%] [G loss: 2.608619]\n",
      "epoch:23 step:18728 [D loss: 0.366003, acc.: 77.34%] [G loss: 3.725449]\n",
      "epoch:23 step:18729 [D loss: 0.348947, acc.: 82.81%] [G loss: 3.646620]\n",
      "epoch:23 step:18730 [D loss: 0.234649, acc.: 89.06%] [G loss: 4.539373]\n",
      "epoch:23 step:18731 [D loss: 0.322978, acc.: 82.81%] [G loss: 3.089902]\n",
      "epoch:23 step:18732 [D loss: 0.250461, acc.: 88.28%] [G loss: 4.363020]\n",
      "epoch:23 step:18733 [D loss: 0.372454, acc.: 84.38%] [G loss: 3.491272]\n",
      "epoch:23 step:18734 [D loss: 0.411501, acc.: 82.03%] [G loss: 3.221212]\n",
      "epoch:23 step:18735 [D loss: 0.300151, acc.: 86.72%] [G loss: 3.335373]\n",
      "epoch:23 step:18736 [D loss: 0.378192, acc.: 82.81%] [G loss: 5.042735]\n",
      "epoch:23 step:18737 [D loss: 0.565440, acc.: 75.00%] [G loss: 5.633511]\n",
      "epoch:23 step:18738 [D loss: 0.457564, acc.: 80.47%] [G loss: 3.928980]\n",
      "epoch:23 step:18739 [D loss: 0.326635, acc.: 82.03%] [G loss: 4.612339]\n",
      "epoch:23 step:18740 [D loss: 0.366746, acc.: 84.38%] [G loss: 4.820342]\n",
      "epoch:23 step:18741 [D loss: 0.259628, acc.: 89.84%] [G loss: 2.704959]\n",
      "epoch:23 step:18742 [D loss: 0.224759, acc.: 86.72%] [G loss: 3.689840]\n",
      "epoch:23 step:18743 [D loss: 0.291208, acc.: 88.28%] [G loss: 3.499890]\n",
      "epoch:23 step:18744 [D loss: 0.286165, acc.: 85.16%] [G loss: 3.624279]\n",
      "epoch:24 step:18745 [D loss: 0.447278, acc.: 81.25%] [G loss: 2.469750]\n",
      "epoch:24 step:18746 [D loss: 0.220480, acc.: 91.41%] [G loss: 3.280529]\n",
      "epoch:24 step:18747 [D loss: 0.381791, acc.: 81.25%] [G loss: 3.039893]\n",
      "epoch:24 step:18748 [D loss: 0.292258, acc.: 86.72%] [G loss: 2.788916]\n",
      "epoch:24 step:18749 [D loss: 0.269994, acc.: 91.41%] [G loss: 2.734353]\n",
      "epoch:24 step:18750 [D loss: 0.462084, acc.: 76.56%] [G loss: 2.806702]\n",
      "epoch:24 step:18751 [D loss: 0.343694, acc.: 82.03%] [G loss: 2.699984]\n",
      "epoch:24 step:18752 [D loss: 0.180003, acc.: 93.75%] [G loss: 4.511427]\n",
      "epoch:24 step:18753 [D loss: 0.262097, acc.: 90.62%] [G loss: 6.171122]\n",
      "epoch:24 step:18754 [D loss: 0.246524, acc.: 89.84%] [G loss: 3.966759]\n",
      "epoch:24 step:18755 [D loss: 0.267847, acc.: 89.06%] [G loss: 4.082156]\n",
      "epoch:24 step:18756 [D loss: 0.358908, acc.: 81.25%] [G loss: 2.899271]\n",
      "epoch:24 step:18757 [D loss: 0.371185, acc.: 80.47%] [G loss: 2.872066]\n",
      "epoch:24 step:18758 [D loss: 0.279848, acc.: 89.06%] [G loss: 2.816859]\n",
      "epoch:24 step:18759 [D loss: 0.327021, acc.: 82.03%] [G loss: 2.207446]\n",
      "epoch:24 step:18760 [D loss: 0.329358, acc.: 85.94%] [G loss: 2.362461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:18761 [D loss: 0.372219, acc.: 85.94%] [G loss: 2.696999]\n",
      "epoch:24 step:18762 [D loss: 0.277832, acc.: 89.06%] [G loss: 2.788775]\n",
      "epoch:24 step:18763 [D loss: 0.284753, acc.: 86.72%] [G loss: 2.886307]\n",
      "epoch:24 step:18764 [D loss: 0.342054, acc.: 86.72%] [G loss: 2.649059]\n",
      "epoch:24 step:18765 [D loss: 0.435715, acc.: 82.81%] [G loss: 2.634156]\n",
      "epoch:24 step:18766 [D loss: 0.366980, acc.: 82.03%] [G loss: 2.743676]\n",
      "epoch:24 step:18767 [D loss: 0.394603, acc.: 82.03%] [G loss: 2.745288]\n",
      "epoch:24 step:18768 [D loss: 0.357464, acc.: 82.81%] [G loss: 4.264663]\n",
      "epoch:24 step:18769 [D loss: 0.320291, acc.: 88.28%] [G loss: 2.674297]\n",
      "epoch:24 step:18770 [D loss: 0.201723, acc.: 92.19%] [G loss: 3.473721]\n",
      "epoch:24 step:18771 [D loss: 0.251691, acc.: 92.19%] [G loss: 4.980956]\n",
      "epoch:24 step:18772 [D loss: 0.416457, acc.: 78.91%] [G loss: 3.385126]\n",
      "epoch:24 step:18773 [D loss: 0.262014, acc.: 88.28%] [G loss: 4.071571]\n",
      "epoch:24 step:18774 [D loss: 0.299899, acc.: 85.94%] [G loss: 4.166361]\n",
      "epoch:24 step:18775 [D loss: 0.271336, acc.: 89.84%] [G loss: 3.537655]\n",
      "epoch:24 step:18776 [D loss: 0.269160, acc.: 85.16%] [G loss: 3.847702]\n",
      "epoch:24 step:18777 [D loss: 0.200475, acc.: 89.06%] [G loss: 5.790076]\n",
      "epoch:24 step:18778 [D loss: 0.293104, acc.: 84.38%] [G loss: 5.834830]\n",
      "epoch:24 step:18779 [D loss: 0.280409, acc.: 89.84%] [G loss: 4.465251]\n",
      "epoch:24 step:18780 [D loss: 0.212832, acc.: 92.97%] [G loss: 5.313492]\n",
      "epoch:24 step:18781 [D loss: 0.331309, acc.: 85.94%] [G loss: 9.068460]\n",
      "epoch:24 step:18782 [D loss: 0.457234, acc.: 77.34%] [G loss: 4.177162]\n",
      "epoch:24 step:18783 [D loss: 0.172483, acc.: 91.41%] [G loss: 6.545676]\n",
      "epoch:24 step:18784 [D loss: 0.219212, acc.: 92.19%] [G loss: 3.658844]\n",
      "epoch:24 step:18785 [D loss: 0.362668, acc.: 81.25%] [G loss: 3.593456]\n",
      "epoch:24 step:18786 [D loss: 0.279711, acc.: 88.28%] [G loss: 4.035521]\n",
      "epoch:24 step:18787 [D loss: 0.330818, acc.: 84.38%] [G loss: 4.934385]\n",
      "epoch:24 step:18788 [D loss: 0.287150, acc.: 87.50%] [G loss: 3.518929]\n",
      "epoch:24 step:18789 [D loss: 0.219508, acc.: 88.28%] [G loss: 2.857720]\n",
      "epoch:24 step:18790 [D loss: 0.240349, acc.: 91.41%] [G loss: 3.709784]\n",
      "epoch:24 step:18791 [D loss: 0.337127, acc.: 81.25%] [G loss: 4.673833]\n",
      "epoch:24 step:18792 [D loss: 0.259870, acc.: 89.06%] [G loss: 3.408952]\n",
      "epoch:24 step:18793 [D loss: 0.289101, acc.: 88.28%] [G loss: 3.309847]\n",
      "epoch:24 step:18794 [D loss: 0.368109, acc.: 82.81%] [G loss: 3.622904]\n",
      "epoch:24 step:18795 [D loss: 0.250423, acc.: 90.62%] [G loss: 4.020764]\n",
      "epoch:24 step:18796 [D loss: 0.274338, acc.: 89.84%] [G loss: 3.198700]\n",
      "epoch:24 step:18797 [D loss: 0.307156, acc.: 86.72%] [G loss: 3.835377]\n",
      "epoch:24 step:18798 [D loss: 0.278131, acc.: 89.84%] [G loss: 4.192694]\n",
      "epoch:24 step:18799 [D loss: 0.261850, acc.: 90.62%] [G loss: 4.488824]\n",
      "epoch:24 step:18800 [D loss: 0.230039, acc.: 91.41%] [G loss: 4.090539]\n",
      "epoch:24 step:18801 [D loss: 0.272179, acc.: 90.62%] [G loss: 4.466967]\n",
      "epoch:24 step:18802 [D loss: 0.290049, acc.: 85.16%] [G loss: 4.096103]\n",
      "epoch:24 step:18803 [D loss: 0.256912, acc.: 89.84%] [G loss: 3.805026]\n",
      "epoch:24 step:18804 [D loss: 0.299619, acc.: 85.94%] [G loss: 4.256001]\n",
      "epoch:24 step:18805 [D loss: 0.443602, acc.: 78.91%] [G loss: 3.542290]\n",
      "epoch:24 step:18806 [D loss: 0.257849, acc.: 89.06%] [G loss: 4.151751]\n",
      "epoch:24 step:18807 [D loss: 0.296914, acc.: 87.50%] [G loss: 3.214387]\n",
      "epoch:24 step:18808 [D loss: 0.305797, acc.: 88.28%] [G loss: 4.046898]\n",
      "epoch:24 step:18809 [D loss: 0.461718, acc.: 79.69%] [G loss: 5.491263]\n",
      "epoch:24 step:18810 [D loss: 0.609021, acc.: 75.00%] [G loss: 4.093936]\n",
      "epoch:24 step:18811 [D loss: 0.640617, acc.: 75.00%] [G loss: 3.079260]\n",
      "epoch:24 step:18812 [D loss: 0.468476, acc.: 81.25%] [G loss: 3.321496]\n",
      "epoch:24 step:18813 [D loss: 0.438413, acc.: 77.34%] [G loss: 2.602771]\n",
      "epoch:24 step:18814 [D loss: 0.340255, acc.: 86.72%] [G loss: 3.688579]\n",
      "epoch:24 step:18815 [D loss: 0.433608, acc.: 82.03%] [G loss: 3.629373]\n",
      "epoch:24 step:18816 [D loss: 0.424215, acc.: 82.03%] [G loss: 4.076203]\n",
      "epoch:24 step:18817 [D loss: 0.471030, acc.: 82.03%] [G loss: 2.277721]\n",
      "epoch:24 step:18818 [D loss: 0.372252, acc.: 80.47%] [G loss: 3.270158]\n",
      "epoch:24 step:18819 [D loss: 0.348065, acc.: 83.59%] [G loss: 2.727348]\n",
      "epoch:24 step:18820 [D loss: 0.455140, acc.: 78.91%] [G loss: 2.899145]\n",
      "epoch:24 step:18821 [D loss: 0.576373, acc.: 74.22%] [G loss: 3.271949]\n",
      "epoch:24 step:18822 [D loss: 0.454465, acc.: 82.03%] [G loss: 6.495840]\n",
      "epoch:24 step:18823 [D loss: 1.002630, acc.: 57.81%] [G loss: 6.858068]\n",
      "epoch:24 step:18824 [D loss: 1.394999, acc.: 71.09%] [G loss: 4.253061]\n",
      "epoch:24 step:18825 [D loss: 0.551782, acc.: 75.78%] [G loss: 3.603234]\n",
      "epoch:24 step:18826 [D loss: 0.499857, acc.: 71.88%] [G loss: 3.686238]\n",
      "epoch:24 step:18827 [D loss: 0.447745, acc.: 78.91%] [G loss: 2.375617]\n",
      "epoch:24 step:18828 [D loss: 0.278180, acc.: 88.28%] [G loss: 3.484288]\n",
      "epoch:24 step:18829 [D loss: 0.292231, acc.: 86.72%] [G loss: 3.230482]\n",
      "epoch:24 step:18830 [D loss: 0.258229, acc.: 88.28%] [G loss: 3.328735]\n",
      "epoch:24 step:18831 [D loss: 0.408825, acc.: 78.91%] [G loss: 3.074079]\n",
      "epoch:24 step:18832 [D loss: 0.291875, acc.: 88.28%] [G loss: 2.451626]\n",
      "epoch:24 step:18833 [D loss: 0.301197, acc.: 85.16%] [G loss: 2.799208]\n",
      "epoch:24 step:18834 [D loss: 0.352160, acc.: 83.59%] [G loss: 2.882822]\n",
      "epoch:24 step:18835 [D loss: 0.272311, acc.: 88.28%] [G loss: 2.329987]\n",
      "epoch:24 step:18836 [D loss: 0.327169, acc.: 88.28%] [G loss: 3.425000]\n",
      "epoch:24 step:18837 [D loss: 0.466978, acc.: 78.91%] [G loss: 2.606764]\n",
      "epoch:24 step:18838 [D loss: 0.333357, acc.: 85.16%] [G loss: 2.889629]\n",
      "epoch:24 step:18839 [D loss: 0.310202, acc.: 88.28%] [G loss: 2.519775]\n",
      "epoch:24 step:18840 [D loss: 0.331250, acc.: 86.72%] [G loss: 2.984049]\n",
      "epoch:24 step:18841 [D loss: 0.305540, acc.: 86.72%] [G loss: 2.271467]\n",
      "epoch:24 step:18842 [D loss: 0.418284, acc.: 81.25%] [G loss: 2.582123]\n",
      "epoch:24 step:18843 [D loss: 0.352372, acc.: 82.81%] [G loss: 2.920456]\n",
      "epoch:24 step:18844 [D loss: 0.367107, acc.: 85.16%] [G loss: 2.872053]\n",
      "epoch:24 step:18845 [D loss: 0.352622, acc.: 79.69%] [G loss: 2.888903]\n",
      "epoch:24 step:18846 [D loss: 0.293549, acc.: 89.84%] [G loss: 2.867573]\n",
      "epoch:24 step:18847 [D loss: 0.457009, acc.: 74.22%] [G loss: 2.765062]\n",
      "epoch:24 step:18848 [D loss: 0.272019, acc.: 91.41%] [G loss: 2.521546]\n",
      "epoch:24 step:18849 [D loss: 0.228016, acc.: 89.06%] [G loss: 2.997808]\n",
      "epoch:24 step:18850 [D loss: 0.454261, acc.: 74.22%] [G loss: 3.241793]\n",
      "epoch:24 step:18851 [D loss: 0.383077, acc.: 82.03%] [G loss: 3.584839]\n",
      "epoch:24 step:18852 [D loss: 0.227008, acc.: 90.62%] [G loss: 3.760218]\n",
      "epoch:24 step:18853 [D loss: 0.461126, acc.: 75.78%] [G loss: 2.651226]\n",
      "epoch:24 step:18854 [D loss: 0.264251, acc.: 89.84%] [G loss: 2.909948]\n",
      "epoch:24 step:18855 [D loss: 0.542074, acc.: 76.56%] [G loss: 2.883296]\n",
      "epoch:24 step:18856 [D loss: 0.316298, acc.: 86.72%] [G loss: 2.792070]\n",
      "epoch:24 step:18857 [D loss: 0.356898, acc.: 84.38%] [G loss: 2.899287]\n",
      "epoch:24 step:18858 [D loss: 0.373486, acc.: 81.25%] [G loss: 2.845562]\n",
      "epoch:24 step:18859 [D loss: 0.345544, acc.: 85.16%] [G loss: 2.372182]\n",
      "epoch:24 step:18860 [D loss: 0.327875, acc.: 84.38%] [G loss: 2.871428]\n",
      "epoch:24 step:18861 [D loss: 0.289465, acc.: 89.06%] [G loss: 3.020424]\n",
      "epoch:24 step:18862 [D loss: 0.351019, acc.: 82.03%] [G loss: 5.725416]\n",
      "epoch:24 step:18863 [D loss: 0.145554, acc.: 95.31%] [G loss: 3.864996]\n",
      "epoch:24 step:18864 [D loss: 0.308589, acc.: 86.72%] [G loss: 6.284350]\n",
      "epoch:24 step:18865 [D loss: 0.239941, acc.: 89.84%] [G loss: 6.086002]\n",
      "epoch:24 step:18866 [D loss: 0.260042, acc.: 88.28%] [G loss: 3.893777]\n",
      "epoch:24 step:18867 [D loss: 0.315385, acc.: 86.72%] [G loss: 3.601239]\n",
      "epoch:24 step:18868 [D loss: 0.361802, acc.: 84.38%] [G loss: 2.364416]\n",
      "epoch:24 step:18869 [D loss: 0.345845, acc.: 82.81%] [G loss: 3.541626]\n",
      "epoch:24 step:18870 [D loss: 0.305973, acc.: 85.16%] [G loss: 3.087934]\n",
      "epoch:24 step:18871 [D loss: 0.280941, acc.: 86.72%] [G loss: 2.663177]\n",
      "epoch:24 step:18872 [D loss: 0.298728, acc.: 87.50%] [G loss: 2.845484]\n",
      "epoch:24 step:18873 [D loss: 0.391115, acc.: 80.47%] [G loss: 2.323206]\n",
      "epoch:24 step:18874 [D loss: 0.281510, acc.: 88.28%] [G loss: 2.860873]\n",
      "epoch:24 step:18875 [D loss: 0.327242, acc.: 85.94%] [G loss: 3.028899]\n",
      "epoch:24 step:18876 [D loss: 0.391067, acc.: 84.38%] [G loss: 3.719593]\n",
      "epoch:24 step:18877 [D loss: 0.293679, acc.: 86.72%] [G loss: 3.530188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:18878 [D loss: 0.317023, acc.: 86.72%] [G loss: 3.437213]\n",
      "epoch:24 step:18879 [D loss: 0.322155, acc.: 87.50%] [G loss: 4.568830]\n",
      "epoch:24 step:18880 [D loss: 0.275989, acc.: 89.84%] [G loss: 8.055813]\n",
      "epoch:24 step:18881 [D loss: 0.211399, acc.: 90.62%] [G loss: 9.193703]\n",
      "epoch:24 step:18882 [D loss: 0.177694, acc.: 92.97%] [G loss: 8.522993]\n",
      "epoch:24 step:18883 [D loss: 0.213999, acc.: 89.06%] [G loss: 6.443680]\n",
      "epoch:24 step:18884 [D loss: 0.192825, acc.: 93.75%] [G loss: 5.920477]\n",
      "epoch:24 step:18885 [D loss: 0.237392, acc.: 87.50%] [G loss: 7.084660]\n",
      "epoch:24 step:18886 [D loss: 0.313334, acc.: 84.38%] [G loss: 3.296944]\n",
      "epoch:24 step:18887 [D loss: 0.256136, acc.: 92.97%] [G loss: 4.285404]\n",
      "epoch:24 step:18888 [D loss: 0.357585, acc.: 82.03%] [G loss: 2.837141]\n",
      "epoch:24 step:18889 [D loss: 0.349185, acc.: 86.72%] [G loss: 3.174470]\n",
      "epoch:24 step:18890 [D loss: 0.328856, acc.: 88.28%] [G loss: 3.676955]\n",
      "epoch:24 step:18891 [D loss: 0.219180, acc.: 92.97%] [G loss: 4.370374]\n",
      "epoch:24 step:18892 [D loss: 0.492050, acc.: 75.78%] [G loss: 3.118721]\n",
      "epoch:24 step:18893 [D loss: 0.375144, acc.: 82.81%] [G loss: 2.623143]\n",
      "epoch:24 step:18894 [D loss: 0.335142, acc.: 83.59%] [G loss: 3.068506]\n",
      "epoch:24 step:18895 [D loss: 0.473611, acc.: 78.12%] [G loss: 2.664019]\n",
      "epoch:24 step:18896 [D loss: 0.394625, acc.: 84.38%] [G loss: 2.789410]\n",
      "epoch:24 step:18897 [D loss: 0.269535, acc.: 88.28%] [G loss: 2.100956]\n",
      "epoch:24 step:18898 [D loss: 0.353039, acc.: 82.81%] [G loss: 2.872134]\n",
      "epoch:24 step:18899 [D loss: 0.292192, acc.: 87.50%] [G loss: 2.685826]\n",
      "epoch:24 step:18900 [D loss: 0.335570, acc.: 87.50%] [G loss: 3.306489]\n",
      "epoch:24 step:18901 [D loss: 0.358259, acc.: 82.03%] [G loss: 3.575337]\n",
      "epoch:24 step:18902 [D loss: 0.405620, acc.: 81.25%] [G loss: 4.138713]\n",
      "epoch:24 step:18903 [D loss: 0.409776, acc.: 80.47%] [G loss: 3.483552]\n",
      "epoch:24 step:18904 [D loss: 0.344010, acc.: 83.59%] [G loss: 3.248658]\n",
      "epoch:24 step:18905 [D loss: 0.317770, acc.: 87.50%] [G loss: 2.828637]\n",
      "epoch:24 step:18906 [D loss: 0.386676, acc.: 84.38%] [G loss: 2.812043]\n",
      "epoch:24 step:18907 [D loss: 0.315574, acc.: 86.72%] [G loss: 3.320964]\n",
      "epoch:24 step:18908 [D loss: 0.316578, acc.: 85.94%] [G loss: 4.003293]\n",
      "epoch:24 step:18909 [D loss: 0.330937, acc.: 85.16%] [G loss: 3.630798]\n",
      "epoch:24 step:18910 [D loss: 0.291148, acc.: 87.50%] [G loss: 2.813280]\n",
      "epoch:24 step:18911 [D loss: 0.256745, acc.: 90.62%] [G loss: 3.529850]\n",
      "epoch:24 step:18912 [D loss: 0.396579, acc.: 80.47%] [G loss: 3.146207]\n",
      "epoch:24 step:18913 [D loss: 0.353507, acc.: 81.25%] [G loss: 3.615656]\n",
      "epoch:24 step:18914 [D loss: 0.340254, acc.: 84.38%] [G loss: 3.974124]\n",
      "epoch:24 step:18915 [D loss: 0.352975, acc.: 81.25%] [G loss: 4.148871]\n",
      "epoch:24 step:18916 [D loss: 0.234504, acc.: 89.84%] [G loss: 4.729523]\n",
      "epoch:24 step:18917 [D loss: 0.279908, acc.: 88.28%] [G loss: 3.756935]\n",
      "epoch:24 step:18918 [D loss: 0.304909, acc.: 86.72%] [G loss: 4.048313]\n",
      "epoch:24 step:18919 [D loss: 0.351996, acc.: 82.81%] [G loss: 5.016697]\n",
      "epoch:24 step:18920 [D loss: 0.192039, acc.: 91.41%] [G loss: 7.035485]\n",
      "epoch:24 step:18921 [D loss: 0.276145, acc.: 86.72%] [G loss: 6.005972]\n",
      "epoch:24 step:18922 [D loss: 0.170303, acc.: 94.53%] [G loss: 5.839775]\n",
      "epoch:24 step:18923 [D loss: 0.289717, acc.: 87.50%] [G loss: 6.316029]\n",
      "epoch:24 step:18924 [D loss: 0.150047, acc.: 92.97%] [G loss: 5.343465]\n",
      "epoch:24 step:18925 [D loss: 0.224642, acc.: 86.72%] [G loss: 5.182692]\n",
      "epoch:24 step:18926 [D loss: 0.359192, acc.: 85.94%] [G loss: 2.755117]\n",
      "epoch:24 step:18927 [D loss: 0.394312, acc.: 84.38%] [G loss: 2.690190]\n",
      "epoch:24 step:18928 [D loss: 0.431099, acc.: 79.69%] [G loss: 3.396609]\n",
      "epoch:24 step:18929 [D loss: 0.372541, acc.: 82.03%] [G loss: 3.300911]\n",
      "epoch:24 step:18930 [D loss: 0.285300, acc.: 86.72%] [G loss: 2.845421]\n",
      "epoch:24 step:18931 [D loss: 0.354061, acc.: 82.81%] [G loss: 3.103242]\n",
      "epoch:24 step:18932 [D loss: 0.369824, acc.: 82.81%] [G loss: 4.641572]\n",
      "epoch:24 step:18933 [D loss: 0.323887, acc.: 85.94%] [G loss: 3.466565]\n",
      "epoch:24 step:18934 [D loss: 0.415707, acc.: 77.34%] [G loss: 3.393091]\n",
      "epoch:24 step:18935 [D loss: 0.392755, acc.: 78.12%] [G loss: 3.645494]\n",
      "epoch:24 step:18936 [D loss: 0.261933, acc.: 87.50%] [G loss: 3.754864]\n",
      "epoch:24 step:18937 [D loss: 0.314204, acc.: 82.03%] [G loss: 3.695846]\n",
      "epoch:24 step:18938 [D loss: 0.222376, acc.: 92.19%] [G loss: 3.672900]\n",
      "epoch:24 step:18939 [D loss: 0.325161, acc.: 83.59%] [G loss: 4.703818]\n",
      "epoch:24 step:18940 [D loss: 0.268516, acc.: 85.94%] [G loss: 3.528040]\n",
      "epoch:24 step:18941 [D loss: 0.372677, acc.: 82.03%] [G loss: 2.884064]\n",
      "epoch:24 step:18942 [D loss: 0.353200, acc.: 84.38%] [G loss: 3.680758]\n",
      "epoch:24 step:18943 [D loss: 0.316501, acc.: 85.16%] [G loss: 3.757946]\n",
      "epoch:24 step:18944 [D loss: 0.337949, acc.: 82.03%] [G loss: 2.560297]\n",
      "epoch:24 step:18945 [D loss: 0.323529, acc.: 85.94%] [G loss: 3.206096]\n",
      "epoch:24 step:18946 [D loss: 0.396197, acc.: 82.03%] [G loss: 3.418371]\n",
      "epoch:24 step:18947 [D loss: 0.449479, acc.: 80.47%] [G loss: 4.140903]\n",
      "epoch:24 step:18948 [D loss: 0.319865, acc.: 81.25%] [G loss: 6.376826]\n",
      "epoch:24 step:18949 [D loss: 0.223784, acc.: 91.41%] [G loss: 5.330174]\n",
      "epoch:24 step:18950 [D loss: 0.191369, acc.: 92.19%] [G loss: 4.685859]\n",
      "epoch:24 step:18951 [D loss: 0.250082, acc.: 89.06%] [G loss: 3.736444]\n",
      "epoch:24 step:18952 [D loss: 0.324670, acc.: 81.25%] [G loss: 5.769263]\n",
      "epoch:24 step:18953 [D loss: 0.275153, acc.: 86.72%] [G loss: 5.733356]\n",
      "epoch:24 step:18954 [D loss: 0.287355, acc.: 85.16%] [G loss: 5.986351]\n",
      "epoch:24 step:18955 [D loss: 0.367259, acc.: 76.56%] [G loss: 4.931653]\n",
      "epoch:24 step:18956 [D loss: 0.402329, acc.: 80.47%] [G loss: 4.049792]\n",
      "epoch:24 step:18957 [D loss: 0.303729, acc.: 86.72%] [G loss: 4.823204]\n",
      "epoch:24 step:18958 [D loss: 0.357988, acc.: 78.91%] [G loss: 5.845954]\n",
      "epoch:24 step:18959 [D loss: 0.220092, acc.: 88.28%] [G loss: 3.422408]\n",
      "epoch:24 step:18960 [D loss: 0.340799, acc.: 82.03%] [G loss: 4.781203]\n",
      "epoch:24 step:18961 [D loss: 0.237831, acc.: 92.97%] [G loss: 3.902405]\n",
      "epoch:24 step:18962 [D loss: 0.259041, acc.: 89.06%] [G loss: 3.509508]\n",
      "epoch:24 step:18963 [D loss: 0.287580, acc.: 89.06%] [G loss: 5.158858]\n",
      "epoch:24 step:18964 [D loss: 0.405472, acc.: 75.00%] [G loss: 3.062916]\n",
      "epoch:24 step:18965 [D loss: 0.292983, acc.: 83.59%] [G loss: 3.098254]\n",
      "epoch:24 step:18966 [D loss: 0.312012, acc.: 83.59%] [G loss: 3.125208]\n",
      "epoch:24 step:18967 [D loss: 0.338276, acc.: 83.59%] [G loss: 3.491955]\n",
      "epoch:24 step:18968 [D loss: 0.470398, acc.: 75.78%] [G loss: 3.258117]\n",
      "epoch:24 step:18969 [D loss: 0.242246, acc.: 90.62%] [G loss: 3.358423]\n",
      "epoch:24 step:18970 [D loss: 0.373906, acc.: 82.03%] [G loss: 2.408389]\n",
      "epoch:24 step:18971 [D loss: 0.327748, acc.: 85.16%] [G loss: 3.614054]\n",
      "epoch:24 step:18972 [D loss: 0.345441, acc.: 83.59%] [G loss: 5.908262]\n",
      "epoch:24 step:18973 [D loss: 0.532108, acc.: 75.00%] [G loss: 3.381340]\n",
      "epoch:24 step:18974 [D loss: 0.386747, acc.: 82.03%] [G loss: 4.106269]\n",
      "epoch:24 step:18975 [D loss: 0.493841, acc.: 80.47%] [G loss: 3.492517]\n",
      "epoch:24 step:18976 [D loss: 0.357630, acc.: 84.38%] [G loss: 2.824956]\n",
      "epoch:24 step:18977 [D loss: 0.353666, acc.: 85.16%] [G loss: 4.221490]\n",
      "epoch:24 step:18978 [D loss: 0.358205, acc.: 83.59%] [G loss: 3.348370]\n",
      "epoch:24 step:18979 [D loss: 0.341756, acc.: 85.16%] [G loss: 4.145031]\n",
      "epoch:24 step:18980 [D loss: 0.410895, acc.: 80.47%] [G loss: 2.808617]\n",
      "epoch:24 step:18981 [D loss: 0.382438, acc.: 81.25%] [G loss: 3.264340]\n",
      "epoch:24 step:18982 [D loss: 0.425131, acc.: 80.47%] [G loss: 5.464687]\n",
      "epoch:24 step:18983 [D loss: 0.430436, acc.: 80.47%] [G loss: 5.348471]\n",
      "epoch:24 step:18984 [D loss: 0.659418, acc.: 71.09%] [G loss: 4.450065]\n",
      "epoch:24 step:18985 [D loss: 0.340442, acc.: 85.16%] [G loss: 3.979315]\n",
      "epoch:24 step:18986 [D loss: 0.385499, acc.: 85.94%] [G loss: 3.541332]\n",
      "epoch:24 step:18987 [D loss: 0.318671, acc.: 86.72%] [G loss: 4.227075]\n",
      "epoch:24 step:18988 [D loss: 0.284527, acc.: 93.75%] [G loss: 3.327047]\n",
      "epoch:24 step:18989 [D loss: 0.330760, acc.: 82.03%] [G loss: 2.283989]\n",
      "epoch:24 step:18990 [D loss: 0.246222, acc.: 88.28%] [G loss: 2.739144]\n",
      "epoch:24 step:18991 [D loss: 0.330269, acc.: 82.81%] [G loss: 2.403005]\n",
      "epoch:24 step:18992 [D loss: 0.220801, acc.: 90.62%] [G loss: 2.742978]\n",
      "epoch:24 step:18993 [D loss: 0.416012, acc.: 78.12%] [G loss: 3.234869]\n",
      "epoch:24 step:18994 [D loss: 0.348915, acc.: 83.59%] [G loss: 3.671228]\n",
      "epoch:24 step:18995 [D loss: 0.237033, acc.: 92.19%] [G loss: 3.188684]\n",
      "epoch:24 step:18996 [D loss: 0.381712, acc.: 77.34%] [G loss: 3.136566]\n",
      "epoch:24 step:18997 [D loss: 0.239333, acc.: 90.62%] [G loss: 2.919043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:18998 [D loss: 0.328726, acc.: 86.72%] [G loss: 3.102138]\n",
      "epoch:24 step:18999 [D loss: 0.274802, acc.: 86.72%] [G loss: 3.602761]\n",
      "epoch:24 step:19000 [D loss: 0.226562, acc.: 91.41%] [G loss: 3.946207]\n",
      "epoch:24 step:19001 [D loss: 0.177323, acc.: 92.19%] [G loss: 3.577986]\n",
      "epoch:24 step:19002 [D loss: 0.281242, acc.: 88.28%] [G loss: 2.417451]\n",
      "epoch:24 step:19003 [D loss: 0.327412, acc.: 85.94%] [G loss: 3.397841]\n",
      "epoch:24 step:19004 [D loss: 0.309008, acc.: 85.16%] [G loss: 3.007572]\n",
      "epoch:24 step:19005 [D loss: 0.436982, acc.: 79.69%] [G loss: 2.523553]\n",
      "epoch:24 step:19006 [D loss: 0.345178, acc.: 84.38%] [G loss: 2.697043]\n",
      "epoch:24 step:19007 [D loss: 0.238972, acc.: 89.84%] [G loss: 3.491503]\n",
      "epoch:24 step:19008 [D loss: 0.364874, acc.: 85.94%] [G loss: 2.263242]\n",
      "epoch:24 step:19009 [D loss: 0.429399, acc.: 80.47%] [G loss: 3.473512]\n",
      "epoch:24 step:19010 [D loss: 0.328562, acc.: 83.59%] [G loss: 2.687234]\n",
      "epoch:24 step:19011 [D loss: 0.271832, acc.: 89.06%] [G loss: 3.185981]\n",
      "epoch:24 step:19012 [D loss: 0.247415, acc.: 91.41%] [G loss: 3.405554]\n",
      "epoch:24 step:19013 [D loss: 0.295993, acc.: 90.62%] [G loss: 3.077201]\n",
      "epoch:24 step:19014 [D loss: 0.325431, acc.: 86.72%] [G loss: 3.250141]\n",
      "epoch:24 step:19015 [D loss: 0.331266, acc.: 82.81%] [G loss: 3.042614]\n",
      "epoch:24 step:19016 [D loss: 0.435326, acc.: 82.03%] [G loss: 3.926960]\n",
      "epoch:24 step:19017 [D loss: 0.332491, acc.: 84.38%] [G loss: 2.909170]\n",
      "epoch:24 step:19018 [D loss: 0.347437, acc.: 84.38%] [G loss: 4.290495]\n",
      "epoch:24 step:19019 [D loss: 0.392370, acc.: 79.69%] [G loss: 5.031649]\n",
      "epoch:24 step:19020 [D loss: 0.423256, acc.: 80.47%] [G loss: 2.842601]\n",
      "epoch:24 step:19021 [D loss: 0.336261, acc.: 88.28%] [G loss: 3.361543]\n",
      "epoch:24 step:19022 [D loss: 0.572906, acc.: 77.34%] [G loss: 2.812779]\n",
      "epoch:24 step:19023 [D loss: 0.420764, acc.: 83.59%] [G loss: 3.344188]\n",
      "epoch:24 step:19024 [D loss: 0.362728, acc.: 85.94%] [G loss: 4.206103]\n",
      "epoch:24 step:19025 [D loss: 0.326118, acc.: 85.94%] [G loss: 4.376234]\n",
      "epoch:24 step:19026 [D loss: 0.309129, acc.: 88.28%] [G loss: 4.225575]\n",
      "epoch:24 step:19027 [D loss: 0.240090, acc.: 91.41%] [G loss: 2.919650]\n",
      "epoch:24 step:19028 [D loss: 0.327266, acc.: 85.94%] [G loss: 3.978764]\n",
      "epoch:24 step:19029 [D loss: 0.354325, acc.: 87.50%] [G loss: 3.275964]\n",
      "epoch:24 step:19030 [D loss: 0.407282, acc.: 78.91%] [G loss: 2.534518]\n",
      "epoch:24 step:19031 [D loss: 0.328784, acc.: 82.03%] [G loss: 3.379907]\n",
      "epoch:24 step:19032 [D loss: 0.347361, acc.: 84.38%] [G loss: 2.605155]\n",
      "epoch:24 step:19033 [D loss: 0.283357, acc.: 85.16%] [G loss: 3.157365]\n",
      "epoch:24 step:19034 [D loss: 0.323974, acc.: 86.72%] [G loss: 2.860701]\n",
      "epoch:24 step:19035 [D loss: 0.359071, acc.: 78.12%] [G loss: 2.831763]\n",
      "epoch:24 step:19036 [D loss: 0.244952, acc.: 87.50%] [G loss: 2.850971]\n",
      "epoch:24 step:19037 [D loss: 0.311520, acc.: 84.38%] [G loss: 2.485544]\n",
      "epoch:24 step:19038 [D loss: 0.345083, acc.: 86.72%] [G loss: 3.100920]\n",
      "epoch:24 step:19039 [D loss: 0.406609, acc.: 81.25%] [G loss: 2.583207]\n",
      "epoch:24 step:19040 [D loss: 0.245173, acc.: 92.97%] [G loss: 2.932560]\n",
      "epoch:24 step:19041 [D loss: 0.377497, acc.: 84.38%] [G loss: 2.408805]\n",
      "epoch:24 step:19042 [D loss: 0.291149, acc.: 85.94%] [G loss: 4.588702]\n",
      "epoch:24 step:19043 [D loss: 0.334092, acc.: 85.16%] [G loss: 3.053968]\n",
      "epoch:24 step:19044 [D loss: 0.295044, acc.: 86.72%] [G loss: 6.013809]\n",
      "epoch:24 step:19045 [D loss: 0.303401, acc.: 83.59%] [G loss: 4.034024]\n",
      "epoch:24 step:19046 [D loss: 0.361090, acc.: 82.03%] [G loss: 3.574202]\n",
      "epoch:24 step:19047 [D loss: 0.416893, acc.: 80.47%] [G loss: 3.909520]\n",
      "epoch:24 step:19048 [D loss: 0.280578, acc.: 88.28%] [G loss: 3.535135]\n",
      "epoch:24 step:19049 [D loss: 0.240123, acc.: 90.62%] [G loss: 3.074404]\n",
      "epoch:24 step:19050 [D loss: 0.343474, acc.: 82.81%] [G loss: 3.629690]\n",
      "epoch:24 step:19051 [D loss: 0.229634, acc.: 89.84%] [G loss: 4.252984]\n",
      "epoch:24 step:19052 [D loss: 0.291175, acc.: 86.72%] [G loss: 3.272676]\n",
      "epoch:24 step:19053 [D loss: 0.351766, acc.: 84.38%] [G loss: 3.365640]\n",
      "epoch:24 step:19054 [D loss: 0.252512, acc.: 88.28%] [G loss: 4.449545]\n",
      "epoch:24 step:19055 [D loss: 0.373114, acc.: 83.59%] [G loss: 2.842139]\n",
      "epoch:24 step:19056 [D loss: 0.351312, acc.: 83.59%] [G loss: 2.956779]\n",
      "epoch:24 step:19057 [D loss: 0.366482, acc.: 79.69%] [G loss: 4.080273]\n",
      "epoch:24 step:19058 [D loss: 0.229073, acc.: 86.72%] [G loss: 5.192582]\n",
      "epoch:24 step:19059 [D loss: 0.422118, acc.: 78.91%] [G loss: 4.029611]\n",
      "epoch:24 step:19060 [D loss: 0.366791, acc.: 82.03%] [G loss: 4.260810]\n",
      "epoch:24 step:19061 [D loss: 0.322451, acc.: 81.25%] [G loss: 4.752543]\n",
      "epoch:24 step:19062 [D loss: 0.346500, acc.: 82.03%] [G loss: 3.394305]\n",
      "epoch:24 step:19063 [D loss: 0.249854, acc.: 87.50%] [G loss: 3.378243]\n",
      "epoch:24 step:19064 [D loss: 0.300352, acc.: 88.28%] [G loss: 3.046553]\n",
      "epoch:24 step:19065 [D loss: 0.227359, acc.: 91.41%] [G loss: 3.880794]\n",
      "epoch:24 step:19066 [D loss: 0.316720, acc.: 85.16%] [G loss: 3.124313]\n",
      "epoch:24 step:19067 [D loss: 0.345960, acc.: 84.38%] [G loss: 2.659528]\n",
      "epoch:24 step:19068 [D loss: 0.347199, acc.: 88.28%] [G loss: 3.702420]\n",
      "epoch:24 step:19069 [D loss: 0.434641, acc.: 78.91%] [G loss: 2.637198]\n",
      "epoch:24 step:19070 [D loss: 0.292859, acc.: 90.62%] [G loss: 3.079827]\n",
      "epoch:24 step:19071 [D loss: 0.281478, acc.: 89.84%] [G loss: 2.928626]\n",
      "epoch:24 step:19072 [D loss: 0.310239, acc.: 85.16%] [G loss: 4.067619]\n",
      "epoch:24 step:19073 [D loss: 0.396765, acc.: 78.91%] [G loss: 3.052330]\n",
      "epoch:24 step:19074 [D loss: 0.440191, acc.: 85.16%] [G loss: 3.163705]\n",
      "epoch:24 step:19075 [D loss: 0.369365, acc.: 84.38%] [G loss: 4.403202]\n",
      "epoch:24 step:19076 [D loss: 0.350779, acc.: 85.16%] [G loss: 3.679856]\n",
      "epoch:24 step:19077 [D loss: 0.371579, acc.: 80.47%] [G loss: 3.539388]\n",
      "epoch:24 step:19078 [D loss: 0.349673, acc.: 82.81%] [G loss: 3.847382]\n",
      "epoch:24 step:19079 [D loss: 0.362500, acc.: 85.16%] [G loss: 2.858006]\n",
      "epoch:24 step:19080 [D loss: 0.306219, acc.: 85.16%] [G loss: 3.423417]\n",
      "epoch:24 step:19081 [D loss: 0.279305, acc.: 85.16%] [G loss: 4.494559]\n",
      "epoch:24 step:19082 [D loss: 0.342226, acc.: 82.81%] [G loss: 3.275748]\n",
      "epoch:24 step:19083 [D loss: 0.219344, acc.: 89.84%] [G loss: 2.985463]\n",
      "epoch:24 step:19084 [D loss: 0.261032, acc.: 89.84%] [G loss: 4.184796]\n",
      "epoch:24 step:19085 [D loss: 0.503633, acc.: 80.47%] [G loss: 2.899156]\n",
      "epoch:24 step:19086 [D loss: 0.266601, acc.: 86.72%] [G loss: 3.753213]\n",
      "epoch:24 step:19087 [D loss: 0.267832, acc.: 89.84%] [G loss: 4.647010]\n",
      "epoch:24 step:19088 [D loss: 0.252208, acc.: 89.84%] [G loss: 3.907935]\n",
      "epoch:24 step:19089 [D loss: 0.243341, acc.: 89.06%] [G loss: 3.764530]\n",
      "epoch:24 step:19090 [D loss: 0.334697, acc.: 81.25%] [G loss: 3.473291]\n",
      "epoch:24 step:19091 [D loss: 0.421645, acc.: 78.12%] [G loss: 3.451172]\n",
      "epoch:24 step:19092 [D loss: 0.254288, acc.: 89.06%] [G loss: 2.676911]\n",
      "epoch:24 step:19093 [D loss: 0.291720, acc.: 89.06%] [G loss: 3.523795]\n",
      "epoch:24 step:19094 [D loss: 0.311346, acc.: 84.38%] [G loss: 3.345777]\n",
      "epoch:24 step:19095 [D loss: 0.373522, acc.: 78.91%] [G loss: 4.137509]\n",
      "epoch:24 step:19096 [D loss: 0.372971, acc.: 82.81%] [G loss: 3.034812]\n",
      "epoch:24 step:19097 [D loss: 0.285817, acc.: 87.50%] [G loss: 2.294169]\n",
      "epoch:24 step:19098 [D loss: 0.371672, acc.: 85.16%] [G loss: 2.535194]\n",
      "epoch:24 step:19099 [D loss: 0.395860, acc.: 79.69%] [G loss: 2.393648]\n",
      "epoch:24 step:19100 [D loss: 0.391391, acc.: 82.03%] [G loss: 3.048306]\n",
      "epoch:24 step:19101 [D loss: 0.421021, acc.: 76.56%] [G loss: 3.971005]\n",
      "epoch:24 step:19102 [D loss: 0.440802, acc.: 82.81%] [G loss: 4.309345]\n",
      "epoch:24 step:19103 [D loss: 0.481835, acc.: 79.69%] [G loss: 5.381160]\n",
      "epoch:24 step:19104 [D loss: 0.393174, acc.: 78.91%] [G loss: 3.596871]\n",
      "epoch:24 step:19105 [D loss: 0.319199, acc.: 84.38%] [G loss: 2.896539]\n",
      "epoch:24 step:19106 [D loss: 0.337095, acc.: 85.94%] [G loss: 2.974004]\n",
      "epoch:24 step:19107 [D loss: 0.367350, acc.: 85.94%] [G loss: 2.490648]\n",
      "epoch:24 step:19108 [D loss: 0.347949, acc.: 82.81%] [G loss: 3.216791]\n",
      "epoch:24 step:19109 [D loss: 0.324985, acc.: 84.38%] [G loss: 3.642151]\n",
      "epoch:24 step:19110 [D loss: 0.407993, acc.: 78.91%] [G loss: 3.220815]\n",
      "epoch:24 step:19111 [D loss: 0.411363, acc.: 80.47%] [G loss: 3.177803]\n",
      "epoch:24 step:19112 [D loss: 0.383736, acc.: 82.03%] [G loss: 3.582983]\n",
      "epoch:24 step:19113 [D loss: 0.286712, acc.: 87.50%] [G loss: 4.522299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19114 [D loss: 0.244957, acc.: 88.28%] [G loss: 2.781926]\n",
      "epoch:24 step:19115 [D loss: 0.349857, acc.: 85.94%] [G loss: 2.917620]\n",
      "epoch:24 step:19116 [D loss: 0.297211, acc.: 89.84%] [G loss: 3.509181]\n",
      "epoch:24 step:19117 [D loss: 0.359739, acc.: 83.59%] [G loss: 3.111996]\n",
      "epoch:24 step:19118 [D loss: 0.405262, acc.: 82.03%] [G loss: 2.736290]\n",
      "epoch:24 step:19119 [D loss: 0.313202, acc.: 87.50%] [G loss: 3.716141]\n",
      "epoch:24 step:19120 [D loss: 0.293301, acc.: 90.62%] [G loss: 5.555925]\n",
      "epoch:24 step:19121 [D loss: 0.266062, acc.: 85.16%] [G loss: 5.415850]\n",
      "epoch:24 step:19122 [D loss: 0.277187, acc.: 84.38%] [G loss: 4.178407]\n",
      "epoch:24 step:19123 [D loss: 0.337503, acc.: 82.03%] [G loss: 4.539857]\n",
      "epoch:24 step:19124 [D loss: 0.294145, acc.: 83.59%] [G loss: 3.326915]\n",
      "epoch:24 step:19125 [D loss: 0.421011, acc.: 80.47%] [G loss: 4.531054]\n",
      "epoch:24 step:19126 [D loss: 0.201945, acc.: 91.41%] [G loss: 3.911771]\n",
      "epoch:24 step:19127 [D loss: 0.438768, acc.: 77.34%] [G loss: 3.408797]\n",
      "epoch:24 step:19128 [D loss: 0.303250, acc.: 85.94%] [G loss: 2.699362]\n",
      "epoch:24 step:19129 [D loss: 0.292594, acc.: 85.94%] [G loss: 3.624829]\n",
      "epoch:24 step:19130 [D loss: 0.370384, acc.: 83.59%] [G loss: 4.153629]\n",
      "epoch:24 step:19131 [D loss: 0.464840, acc.: 80.47%] [G loss: 3.890539]\n",
      "epoch:24 step:19132 [D loss: 0.451818, acc.: 78.12%] [G loss: 3.029074]\n",
      "epoch:24 step:19133 [D loss: 0.352563, acc.: 86.72%] [G loss: 3.121205]\n",
      "epoch:24 step:19134 [D loss: 0.346092, acc.: 84.38%] [G loss: 3.811530]\n",
      "epoch:24 step:19135 [D loss: 0.385432, acc.: 82.03%] [G loss: 2.705976]\n",
      "epoch:24 step:19136 [D loss: 0.293079, acc.: 89.06%] [G loss: 3.212524]\n",
      "epoch:24 step:19137 [D loss: 0.394142, acc.: 82.81%] [G loss: 3.500019]\n",
      "epoch:24 step:19138 [D loss: 0.221005, acc.: 89.84%] [G loss: 3.993865]\n",
      "epoch:24 step:19139 [D loss: 0.211311, acc.: 93.75%] [G loss: 5.509358]\n",
      "epoch:24 step:19140 [D loss: 0.256969, acc.: 90.62%] [G loss: 3.578520]\n",
      "epoch:24 step:19141 [D loss: 0.418928, acc.: 75.78%] [G loss: 3.758978]\n",
      "epoch:24 step:19142 [D loss: 0.241177, acc.: 92.19%] [G loss: 3.392154]\n",
      "epoch:24 step:19143 [D loss: 0.492822, acc.: 77.34%] [G loss: 3.773794]\n",
      "epoch:24 step:19144 [D loss: 0.292609, acc.: 84.38%] [G loss: 4.800819]\n",
      "epoch:24 step:19145 [D loss: 0.327157, acc.: 89.06%] [G loss: 2.904996]\n",
      "epoch:24 step:19146 [D loss: 0.310079, acc.: 85.16%] [G loss: 2.875448]\n",
      "epoch:24 step:19147 [D loss: 0.291045, acc.: 85.94%] [G loss: 3.094468]\n",
      "epoch:24 step:19148 [D loss: 0.385004, acc.: 83.59%] [G loss: 2.981766]\n",
      "epoch:24 step:19149 [D loss: 0.310715, acc.: 89.06%] [G loss: 3.621594]\n",
      "epoch:24 step:19150 [D loss: 0.389328, acc.: 86.72%] [G loss: 3.818102]\n",
      "epoch:24 step:19151 [D loss: 0.379761, acc.: 82.03%] [G loss: 2.876904]\n",
      "epoch:24 step:19152 [D loss: 0.392598, acc.: 81.25%] [G loss: 3.026079]\n",
      "epoch:24 step:19153 [D loss: 0.323002, acc.: 85.16%] [G loss: 2.883264]\n",
      "epoch:24 step:19154 [D loss: 0.381345, acc.: 84.38%] [G loss: 2.897064]\n",
      "epoch:24 step:19155 [D loss: 0.225858, acc.: 91.41%] [G loss: 2.347410]\n",
      "epoch:24 step:19156 [D loss: 0.391427, acc.: 83.59%] [G loss: 3.043662]\n",
      "epoch:24 step:19157 [D loss: 0.355242, acc.: 83.59%] [G loss: 4.089717]\n",
      "epoch:24 step:19158 [D loss: 0.397195, acc.: 82.81%] [G loss: 2.876220]\n",
      "epoch:24 step:19159 [D loss: 0.304376, acc.: 87.50%] [G loss: 2.506514]\n",
      "epoch:24 step:19160 [D loss: 0.292740, acc.: 88.28%] [G loss: 3.100573]\n",
      "epoch:24 step:19161 [D loss: 0.332554, acc.: 85.94%] [G loss: 3.792735]\n",
      "epoch:24 step:19162 [D loss: 0.365440, acc.: 82.81%] [G loss: 3.535245]\n",
      "epoch:24 step:19163 [D loss: 0.301511, acc.: 88.28%] [G loss: 3.769668]\n",
      "epoch:24 step:19164 [D loss: 0.319169, acc.: 83.59%] [G loss: 3.064785]\n",
      "epoch:24 step:19165 [D loss: 0.352331, acc.: 84.38%] [G loss: 3.920632]\n",
      "epoch:24 step:19166 [D loss: 0.424371, acc.: 78.12%] [G loss: 2.918604]\n",
      "epoch:24 step:19167 [D loss: 0.339119, acc.: 83.59%] [G loss: 3.703407]\n",
      "epoch:24 step:19168 [D loss: 0.270410, acc.: 86.72%] [G loss: 3.286831]\n",
      "epoch:24 step:19169 [D loss: 0.355748, acc.: 82.03%] [G loss: 3.938678]\n",
      "epoch:24 step:19170 [D loss: 0.490047, acc.: 77.34%] [G loss: 3.520478]\n",
      "epoch:24 step:19171 [D loss: 0.358099, acc.: 85.16%] [G loss: 3.624787]\n",
      "epoch:24 step:19172 [D loss: 0.418547, acc.: 83.59%] [G loss: 7.133139]\n",
      "epoch:24 step:19173 [D loss: 0.686371, acc.: 75.00%] [G loss: 11.324553]\n",
      "epoch:24 step:19174 [D loss: 2.162816, acc.: 53.12%] [G loss: 4.181927]\n",
      "epoch:24 step:19175 [D loss: 0.690344, acc.: 76.56%] [G loss: 6.341697]\n",
      "epoch:24 step:19176 [D loss: 0.815884, acc.: 75.00%] [G loss: 4.234397]\n",
      "epoch:24 step:19177 [D loss: 0.307070, acc.: 85.16%] [G loss: 5.186379]\n",
      "epoch:24 step:19178 [D loss: 0.507846, acc.: 82.03%] [G loss: 6.545539]\n",
      "epoch:24 step:19179 [D loss: 0.449226, acc.: 81.25%] [G loss: 3.784721]\n",
      "epoch:24 step:19180 [D loss: 0.610944, acc.: 71.88%] [G loss: 3.564092]\n",
      "epoch:24 step:19181 [D loss: 0.327872, acc.: 86.72%] [G loss: 3.313663]\n",
      "epoch:24 step:19182 [D loss: 0.392282, acc.: 84.38%] [G loss: 2.513671]\n",
      "epoch:24 step:19183 [D loss: 0.474465, acc.: 81.25%] [G loss: 2.880925]\n",
      "epoch:24 step:19184 [D loss: 0.321954, acc.: 85.94%] [G loss: 2.989256]\n",
      "epoch:24 step:19185 [D loss: 0.291342, acc.: 85.94%] [G loss: 3.417113]\n",
      "epoch:24 step:19186 [D loss: 0.292138, acc.: 85.16%] [G loss: 2.983806]\n",
      "epoch:24 step:19187 [D loss: 0.482646, acc.: 78.91%] [G loss: 3.162686]\n",
      "epoch:24 step:19188 [D loss: 0.398574, acc.: 82.81%] [G loss: 2.812304]\n",
      "epoch:24 step:19189 [D loss: 0.409645, acc.: 80.47%] [G loss: 3.018240]\n",
      "epoch:24 step:19190 [D loss: 0.377053, acc.: 82.03%] [G loss: 2.810984]\n",
      "epoch:24 step:19191 [D loss: 0.349762, acc.: 82.03%] [G loss: 3.132606]\n",
      "epoch:24 step:19192 [D loss: 0.356585, acc.: 84.38%] [G loss: 3.451700]\n",
      "epoch:24 step:19193 [D loss: 0.373800, acc.: 81.25%] [G loss: 2.793457]\n",
      "epoch:24 step:19194 [D loss: 0.287892, acc.: 89.84%] [G loss: 3.972362]\n",
      "epoch:24 step:19195 [D loss: 0.273568, acc.: 89.84%] [G loss: 3.376033]\n",
      "epoch:24 step:19196 [D loss: 0.313143, acc.: 88.28%] [G loss: 2.250025]\n",
      "epoch:24 step:19197 [D loss: 0.401405, acc.: 81.25%] [G loss: 2.289045]\n",
      "epoch:24 step:19198 [D loss: 0.245594, acc.: 93.75%] [G loss: 3.137134]\n",
      "epoch:24 step:19199 [D loss: 0.439379, acc.: 83.59%] [G loss: 2.686407]\n",
      "epoch:24 step:19200 [D loss: 0.428110, acc.: 79.69%] [G loss: 2.298835]\n",
      "epoch:24 step:19201 [D loss: 0.376012, acc.: 85.16%] [G loss: 2.395372]\n",
      "epoch:24 step:19202 [D loss: 0.308514, acc.: 87.50%] [G loss: 3.045469]\n",
      "epoch:24 step:19203 [D loss: 0.326576, acc.: 85.16%] [G loss: 2.768419]\n",
      "epoch:24 step:19204 [D loss: 0.284607, acc.: 86.72%] [G loss: 3.299018]\n",
      "epoch:24 step:19205 [D loss: 0.237587, acc.: 89.84%] [G loss: 3.875120]\n",
      "epoch:24 step:19206 [D loss: 0.334854, acc.: 86.72%] [G loss: 3.475834]\n",
      "epoch:24 step:19207 [D loss: 0.310146, acc.: 85.16%] [G loss: 2.932633]\n",
      "epoch:24 step:19208 [D loss: 0.285647, acc.: 85.16%] [G loss: 2.212136]\n",
      "epoch:24 step:19209 [D loss: 0.324269, acc.: 83.59%] [G loss: 2.984074]\n",
      "epoch:24 step:19210 [D loss: 0.338637, acc.: 84.38%] [G loss: 3.371901]\n",
      "epoch:24 step:19211 [D loss: 0.374465, acc.: 80.47%] [G loss: 3.235038]\n",
      "epoch:24 step:19212 [D loss: 0.407983, acc.: 82.81%] [G loss: 3.019507]\n",
      "epoch:24 step:19213 [D loss: 0.313993, acc.: 83.59%] [G loss: 2.893810]\n",
      "epoch:24 step:19214 [D loss: 0.303134, acc.: 87.50%] [G loss: 2.822123]\n",
      "epoch:24 step:19215 [D loss: 0.350847, acc.: 85.94%] [G loss: 2.619166]\n",
      "epoch:24 step:19216 [D loss: 0.405810, acc.: 81.25%] [G loss: 2.057243]\n",
      "epoch:24 step:19217 [D loss: 0.412608, acc.: 78.91%] [G loss: 2.707554]\n",
      "epoch:24 step:19218 [D loss: 0.436976, acc.: 78.12%] [G loss: 3.104639]\n",
      "epoch:24 step:19219 [D loss: 0.309610, acc.: 83.59%] [G loss: 2.905903]\n",
      "epoch:24 step:19220 [D loss: 0.359156, acc.: 82.03%] [G loss: 2.811179]\n",
      "epoch:24 step:19221 [D loss: 0.415771, acc.: 79.69%] [G loss: 2.748036]\n",
      "epoch:24 step:19222 [D loss: 0.348772, acc.: 82.81%] [G loss: 2.740148]\n",
      "epoch:24 step:19223 [D loss: 0.382922, acc.: 82.03%] [G loss: 2.732954]\n",
      "epoch:24 step:19224 [D loss: 0.207113, acc.: 89.84%] [G loss: 2.779273]\n",
      "epoch:24 step:19225 [D loss: 0.338306, acc.: 83.59%] [G loss: 3.149740]\n",
      "epoch:24 step:19226 [D loss: 0.299809, acc.: 85.16%] [G loss: 4.304869]\n",
      "epoch:24 step:19227 [D loss: 0.393136, acc.: 78.12%] [G loss: 2.414764]\n",
      "epoch:24 step:19228 [D loss: 0.403194, acc.: 80.47%] [G loss: 2.446521]\n",
      "epoch:24 step:19229 [D loss: 0.384986, acc.: 82.81%] [G loss: 2.371527]\n",
      "epoch:24 step:19230 [D loss: 0.407856, acc.: 82.81%] [G loss: 2.443138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19231 [D loss: 0.328742, acc.: 83.59%] [G loss: 2.870162]\n",
      "epoch:24 step:19232 [D loss: 0.400312, acc.: 82.81%] [G loss: 2.592151]\n",
      "epoch:24 step:19233 [D loss: 0.432547, acc.: 76.56%] [G loss: 2.947565]\n",
      "epoch:24 step:19234 [D loss: 0.335784, acc.: 85.16%] [G loss: 2.416103]\n",
      "epoch:24 step:19235 [D loss: 0.293281, acc.: 87.50%] [G loss: 2.209006]\n",
      "epoch:24 step:19236 [D loss: 0.372915, acc.: 82.81%] [G loss: 2.188294]\n",
      "epoch:24 step:19237 [D loss: 0.327426, acc.: 86.72%] [G loss: 2.504175]\n",
      "epoch:24 step:19238 [D loss: 0.281929, acc.: 85.16%] [G loss: 3.385163]\n",
      "epoch:24 step:19239 [D loss: 0.247615, acc.: 89.84%] [G loss: 5.187643]\n",
      "epoch:24 step:19240 [D loss: 0.267048, acc.: 89.06%] [G loss: 3.341620]\n",
      "epoch:24 step:19241 [D loss: 0.358768, acc.: 84.38%] [G loss: 2.363318]\n",
      "epoch:24 step:19242 [D loss: 0.379690, acc.: 80.47%] [G loss: 3.850865]\n",
      "epoch:24 step:19243 [D loss: 0.300347, acc.: 85.94%] [G loss: 4.326832]\n",
      "epoch:24 step:19244 [D loss: 0.391841, acc.: 85.16%] [G loss: 4.874444]\n",
      "epoch:24 step:19245 [D loss: 0.478253, acc.: 78.12%] [G loss: 4.277328]\n",
      "epoch:24 step:19246 [D loss: 0.376541, acc.: 78.91%] [G loss: 3.781017]\n",
      "epoch:24 step:19247 [D loss: 0.254754, acc.: 88.28%] [G loss: 3.565431]\n",
      "epoch:24 step:19248 [D loss: 0.356064, acc.: 82.81%] [G loss: 3.037963]\n",
      "epoch:24 step:19249 [D loss: 0.307103, acc.: 88.28%] [G loss: 2.774166]\n",
      "epoch:24 step:19250 [D loss: 0.315829, acc.: 89.06%] [G loss: 3.318060]\n",
      "epoch:24 step:19251 [D loss: 0.315324, acc.: 88.28%] [G loss: 3.155231]\n",
      "epoch:24 step:19252 [D loss: 0.334577, acc.: 85.94%] [G loss: 2.501391]\n",
      "epoch:24 step:19253 [D loss: 0.312147, acc.: 85.94%] [G loss: 3.077998]\n",
      "epoch:24 step:19254 [D loss: 0.285166, acc.: 85.94%] [G loss: 2.774203]\n",
      "epoch:24 step:19255 [D loss: 0.388930, acc.: 85.16%] [G loss: 2.646576]\n",
      "epoch:24 step:19256 [D loss: 0.228044, acc.: 92.97%] [G loss: 3.264020]\n",
      "epoch:24 step:19257 [D loss: 0.315959, acc.: 85.94%] [G loss: 3.303995]\n",
      "epoch:24 step:19258 [D loss: 0.287745, acc.: 89.84%] [G loss: 3.134656]\n",
      "epoch:24 step:19259 [D loss: 0.428994, acc.: 82.03%] [G loss: 2.692907]\n",
      "epoch:24 step:19260 [D loss: 0.313641, acc.: 85.16%] [G loss: 2.938984]\n",
      "epoch:24 step:19261 [D loss: 0.387409, acc.: 82.81%] [G loss: 2.823236]\n",
      "epoch:24 step:19262 [D loss: 0.260566, acc.: 89.84%] [G loss: 2.693740]\n",
      "epoch:24 step:19263 [D loss: 0.269976, acc.: 87.50%] [G loss: 4.537470]\n",
      "epoch:24 step:19264 [D loss: 0.338881, acc.: 85.16%] [G loss: 4.019511]\n",
      "epoch:24 step:19265 [D loss: 0.344797, acc.: 85.16%] [G loss: 2.568570]\n",
      "epoch:24 step:19266 [D loss: 0.291840, acc.: 87.50%] [G loss: 4.479516]\n",
      "epoch:24 step:19267 [D loss: 0.284042, acc.: 89.06%] [G loss: 4.078287]\n",
      "epoch:24 step:19268 [D loss: 0.362749, acc.: 81.25%] [G loss: 5.845651]\n",
      "epoch:24 step:19269 [D loss: 0.308262, acc.: 85.94%] [G loss: 4.056774]\n",
      "epoch:24 step:19270 [D loss: 0.383476, acc.: 85.94%] [G loss: 2.961915]\n",
      "epoch:24 step:19271 [D loss: 0.264278, acc.: 89.06%] [G loss: 4.053302]\n",
      "epoch:24 step:19272 [D loss: 0.211310, acc.: 94.53%] [G loss: 3.539452]\n",
      "epoch:24 step:19273 [D loss: 0.341316, acc.: 83.59%] [G loss: 3.346169]\n",
      "epoch:24 step:19274 [D loss: 0.366863, acc.: 84.38%] [G loss: 2.665892]\n",
      "epoch:24 step:19275 [D loss: 0.370194, acc.: 84.38%] [G loss: 3.177868]\n",
      "epoch:24 step:19276 [D loss: 0.409032, acc.: 80.47%] [G loss: 2.889624]\n",
      "epoch:24 step:19277 [D loss: 0.246127, acc.: 92.19%] [G loss: 2.304211]\n",
      "epoch:24 step:19278 [D loss: 0.324634, acc.: 84.38%] [G loss: 2.998760]\n",
      "epoch:24 step:19279 [D loss: 0.290803, acc.: 86.72%] [G loss: 2.620057]\n",
      "epoch:24 step:19280 [D loss: 0.292504, acc.: 85.94%] [G loss: 3.158607]\n",
      "epoch:24 step:19281 [D loss: 0.404015, acc.: 77.34%] [G loss: 2.718073]\n",
      "epoch:24 step:19282 [D loss: 0.272754, acc.: 90.62%] [G loss: 2.878352]\n",
      "epoch:24 step:19283 [D loss: 0.315595, acc.: 85.16%] [G loss: 2.634490]\n",
      "epoch:24 step:19284 [D loss: 0.280217, acc.: 85.94%] [G loss: 3.294883]\n",
      "epoch:24 step:19285 [D loss: 0.282081, acc.: 86.72%] [G loss: 3.709485]\n",
      "epoch:24 step:19286 [D loss: 0.389731, acc.: 83.59%] [G loss: 3.349756]\n",
      "epoch:24 step:19287 [D loss: 0.294938, acc.: 85.94%] [G loss: 3.434685]\n",
      "epoch:24 step:19288 [D loss: 0.439877, acc.: 80.47%] [G loss: 2.272428]\n",
      "epoch:24 step:19289 [D loss: 0.379454, acc.: 82.03%] [G loss: 3.261912]\n",
      "epoch:24 step:19290 [D loss: 0.355142, acc.: 86.72%] [G loss: 2.640736]\n",
      "epoch:24 step:19291 [D loss: 0.382985, acc.: 81.25%] [G loss: 4.285169]\n",
      "epoch:24 step:19292 [D loss: 0.349499, acc.: 88.28%] [G loss: 2.843266]\n",
      "epoch:24 step:19293 [D loss: 0.292171, acc.: 91.41%] [G loss: 4.177074]\n",
      "epoch:24 step:19294 [D loss: 0.349517, acc.: 87.50%] [G loss: 4.187253]\n",
      "epoch:24 step:19295 [D loss: 0.394871, acc.: 85.94%] [G loss: 2.807101]\n",
      "epoch:24 step:19296 [D loss: 0.350763, acc.: 85.16%] [G loss: 2.793406]\n",
      "epoch:24 step:19297 [D loss: 0.344187, acc.: 82.81%] [G loss: 2.837208]\n",
      "epoch:24 step:19298 [D loss: 0.289202, acc.: 90.62%] [G loss: 2.786116]\n",
      "epoch:24 step:19299 [D loss: 0.353408, acc.: 82.03%] [G loss: 2.617043]\n",
      "epoch:24 step:19300 [D loss: 0.309036, acc.: 83.59%] [G loss: 2.742149]\n",
      "epoch:24 step:19301 [D loss: 0.283424, acc.: 89.06%] [G loss: 2.454561]\n",
      "epoch:24 step:19302 [D loss: 0.261291, acc.: 88.28%] [G loss: 3.648138]\n",
      "epoch:24 step:19303 [D loss: 0.390336, acc.: 82.03%] [G loss: 2.658581]\n",
      "epoch:24 step:19304 [D loss: 0.273771, acc.: 87.50%] [G loss: 4.367871]\n",
      "epoch:24 step:19305 [D loss: 0.299384, acc.: 84.38%] [G loss: 5.345819]\n",
      "epoch:24 step:19306 [D loss: 0.199034, acc.: 92.97%] [G loss: 5.265872]\n",
      "epoch:24 step:19307 [D loss: 0.241367, acc.: 88.28%] [G loss: 3.691456]\n",
      "epoch:24 step:19308 [D loss: 0.378117, acc.: 81.25%] [G loss: 3.213730]\n",
      "epoch:24 step:19309 [D loss: 0.263734, acc.: 89.84%] [G loss: 3.045236]\n",
      "epoch:24 step:19310 [D loss: 0.305967, acc.: 84.38%] [G loss: 2.791215]\n",
      "epoch:24 step:19311 [D loss: 0.354448, acc.: 80.47%] [G loss: 3.666286]\n",
      "epoch:24 step:19312 [D loss: 0.282177, acc.: 90.62%] [G loss: 2.620540]\n",
      "epoch:24 step:19313 [D loss: 0.346407, acc.: 82.03%] [G loss: 2.930879]\n",
      "epoch:24 step:19314 [D loss: 0.330460, acc.: 87.50%] [G loss: 2.373085]\n",
      "epoch:24 step:19315 [D loss: 0.374437, acc.: 83.59%] [G loss: 2.428957]\n",
      "epoch:24 step:19316 [D loss: 0.320355, acc.: 85.16%] [G loss: 3.112419]\n",
      "epoch:24 step:19317 [D loss: 0.268268, acc.: 86.72%] [G loss: 4.584052]\n",
      "epoch:24 step:19318 [D loss: 0.458331, acc.: 75.00%] [G loss: 3.991904]\n",
      "epoch:24 step:19319 [D loss: 0.427072, acc.: 79.69%] [G loss: 3.544901]\n",
      "epoch:24 step:19320 [D loss: 0.212336, acc.: 91.41%] [G loss: 3.915160]\n",
      "epoch:24 step:19321 [D loss: 0.231214, acc.: 90.62%] [G loss: 4.030300]\n",
      "epoch:24 step:19322 [D loss: 0.405518, acc.: 81.25%] [G loss: 3.155910]\n",
      "epoch:24 step:19323 [D loss: 0.294916, acc.: 86.72%] [G loss: 4.542949]\n",
      "epoch:24 step:19324 [D loss: 0.354491, acc.: 80.47%] [G loss: 2.611121]\n",
      "epoch:24 step:19325 [D loss: 0.232480, acc.: 89.84%] [G loss: 4.965259]\n",
      "epoch:24 step:19326 [D loss: 0.291388, acc.: 85.94%] [G loss: 3.524530]\n",
      "epoch:24 step:19327 [D loss: 0.298633, acc.: 85.94%] [G loss: 3.106138]\n",
      "epoch:24 step:19328 [D loss: 0.362438, acc.: 82.81%] [G loss: 3.329911]\n",
      "epoch:24 step:19329 [D loss: 0.305102, acc.: 86.72%] [G loss: 2.632161]\n",
      "epoch:24 step:19330 [D loss: 0.367231, acc.: 80.47%] [G loss: 3.052928]\n",
      "epoch:24 step:19331 [D loss: 0.234372, acc.: 90.62%] [G loss: 3.632261]\n",
      "epoch:24 step:19332 [D loss: 0.329097, acc.: 85.94%] [G loss: 2.950290]\n",
      "epoch:24 step:19333 [D loss: 0.360055, acc.: 84.38%] [G loss: 2.386183]\n",
      "epoch:24 step:19334 [D loss: 0.320784, acc.: 84.38%] [G loss: 2.670567]\n",
      "epoch:24 step:19335 [D loss: 0.503799, acc.: 74.22%] [G loss: 3.088113]\n",
      "epoch:24 step:19336 [D loss: 0.355660, acc.: 81.25%] [G loss: 3.362811]\n",
      "epoch:24 step:19337 [D loss: 0.347199, acc.: 83.59%] [G loss: 3.596200]\n",
      "epoch:24 step:19338 [D loss: 0.245210, acc.: 87.50%] [G loss: 3.651359]\n",
      "epoch:24 step:19339 [D loss: 0.346276, acc.: 86.72%] [G loss: 2.684046]\n",
      "epoch:24 step:19340 [D loss: 0.341360, acc.: 82.03%] [G loss: 2.513239]\n",
      "epoch:24 step:19341 [D loss: 0.347897, acc.: 85.94%] [G loss: 2.768272]\n",
      "epoch:24 step:19342 [D loss: 0.318467, acc.: 89.84%] [G loss: 2.485261]\n",
      "epoch:24 step:19343 [D loss: 0.289145, acc.: 87.50%] [G loss: 2.719945]\n",
      "epoch:24 step:19344 [D loss: 0.410626, acc.: 76.56%] [G loss: 2.750240]\n",
      "epoch:24 step:19345 [D loss: 0.351224, acc.: 79.69%] [G loss: 2.807107]\n",
      "epoch:24 step:19346 [D loss: 0.250215, acc.: 89.84%] [G loss: 2.697437]\n",
      "epoch:24 step:19347 [D loss: 0.267935, acc.: 90.62%] [G loss: 2.534462]\n",
      "epoch:24 step:19348 [D loss: 0.424338, acc.: 78.91%] [G loss: 2.492055]\n",
      "epoch:24 step:19349 [D loss: 0.375441, acc.: 85.16%] [G loss: 3.049065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19350 [D loss: 0.350502, acc.: 82.81%] [G loss: 3.441131]\n",
      "epoch:24 step:19351 [D loss: 0.335119, acc.: 81.25%] [G loss: 3.204869]\n",
      "epoch:24 step:19352 [D loss: 0.216640, acc.: 92.97%] [G loss: 3.070140]\n",
      "epoch:24 step:19353 [D loss: 0.406396, acc.: 83.59%] [G loss: 4.381180]\n",
      "epoch:24 step:19354 [D loss: 0.524510, acc.: 78.91%] [G loss: 5.739569]\n",
      "epoch:24 step:19355 [D loss: 0.769285, acc.: 68.75%] [G loss: 5.456745]\n",
      "epoch:24 step:19356 [D loss: 1.094871, acc.: 62.50%] [G loss: 4.848833]\n",
      "epoch:24 step:19357 [D loss: 0.465283, acc.: 78.12%] [G loss: 2.778527]\n",
      "epoch:24 step:19358 [D loss: 0.286644, acc.: 89.06%] [G loss: 3.121913]\n",
      "epoch:24 step:19359 [D loss: 0.309598, acc.: 87.50%] [G loss: 3.403030]\n",
      "epoch:24 step:19360 [D loss: 0.259767, acc.: 88.28%] [G loss: 2.720173]\n",
      "epoch:24 step:19361 [D loss: 0.420649, acc.: 79.69%] [G loss: 3.014069]\n",
      "epoch:24 step:19362 [D loss: 0.413797, acc.: 78.91%] [G loss: 2.777949]\n",
      "epoch:24 step:19363 [D loss: 0.355564, acc.: 86.72%] [G loss: 2.960476]\n",
      "epoch:24 step:19364 [D loss: 0.277485, acc.: 90.62%] [G loss: 3.175314]\n",
      "epoch:24 step:19365 [D loss: 0.361269, acc.: 81.25%] [G loss: 3.536958]\n",
      "epoch:24 step:19366 [D loss: 0.348722, acc.: 85.94%] [G loss: 2.848478]\n",
      "epoch:24 step:19367 [D loss: 0.298465, acc.: 83.59%] [G loss: 4.648720]\n",
      "epoch:24 step:19368 [D loss: 0.313912, acc.: 84.38%] [G loss: 3.492169]\n",
      "epoch:24 step:19369 [D loss: 0.353491, acc.: 82.81%] [G loss: 3.747489]\n",
      "epoch:24 step:19370 [D loss: 0.293656, acc.: 88.28%] [G loss: 5.874581]\n",
      "epoch:24 step:19371 [D loss: 0.192603, acc.: 91.41%] [G loss: 5.494510]\n",
      "epoch:24 step:19372 [D loss: 0.354915, acc.: 86.72%] [G loss: 3.455343]\n",
      "epoch:24 step:19373 [D loss: 0.330342, acc.: 84.38%] [G loss: 3.495517]\n",
      "epoch:24 step:19374 [D loss: 0.336344, acc.: 82.03%] [G loss: 3.100379]\n",
      "epoch:24 step:19375 [D loss: 0.441298, acc.: 79.69%] [G loss: 3.826293]\n",
      "epoch:24 step:19376 [D loss: 0.274004, acc.: 89.06%] [G loss: 3.603279]\n",
      "epoch:24 step:19377 [D loss: 0.387075, acc.: 86.72%] [G loss: 2.781049]\n",
      "epoch:24 step:19378 [D loss: 0.337290, acc.: 87.50%] [G loss: 3.328990]\n",
      "epoch:24 step:19379 [D loss: 0.266830, acc.: 90.62%] [G loss: 3.329640]\n",
      "epoch:24 step:19380 [D loss: 0.252787, acc.: 88.28%] [G loss: 2.565565]\n",
      "epoch:24 step:19381 [D loss: 0.279622, acc.: 89.06%] [G loss: 3.362074]\n",
      "epoch:24 step:19382 [D loss: 0.322190, acc.: 85.16%] [G loss: 4.092434]\n",
      "epoch:24 step:19383 [D loss: 0.412339, acc.: 78.91%] [G loss: 3.451835]\n",
      "epoch:24 step:19384 [D loss: 0.244337, acc.: 86.72%] [G loss: 4.490970]\n",
      "epoch:24 step:19385 [D loss: 0.319656, acc.: 85.94%] [G loss: 3.532046]\n",
      "epoch:24 step:19386 [D loss: 0.303284, acc.: 87.50%] [G loss: 3.607478]\n",
      "epoch:24 step:19387 [D loss: 0.329313, acc.: 88.28%] [G loss: 5.567844]\n",
      "epoch:24 step:19388 [D loss: 0.308079, acc.: 85.16%] [G loss: 3.643878]\n",
      "epoch:24 step:19389 [D loss: 0.245862, acc.: 89.06%] [G loss: 6.003959]\n",
      "epoch:24 step:19390 [D loss: 0.280766, acc.: 87.50%] [G loss: 4.967480]\n",
      "epoch:24 step:19391 [D loss: 0.300693, acc.: 85.94%] [G loss: 4.173201]\n",
      "epoch:24 step:19392 [D loss: 0.331259, acc.: 82.03%] [G loss: 4.041082]\n",
      "epoch:24 step:19393 [D loss: 0.282513, acc.: 87.50%] [G loss: 3.731295]\n",
      "epoch:24 step:19394 [D loss: 0.279378, acc.: 86.72%] [G loss: 4.347673]\n",
      "epoch:24 step:19395 [D loss: 0.352498, acc.: 84.38%] [G loss: 4.027876]\n",
      "epoch:24 step:19396 [D loss: 0.234465, acc.: 90.62%] [G loss: 2.752663]\n",
      "epoch:24 step:19397 [D loss: 0.277977, acc.: 90.62%] [G loss: 3.292120]\n",
      "epoch:24 step:19398 [D loss: 0.295958, acc.: 89.06%] [G loss: 3.362898]\n",
      "epoch:24 step:19399 [D loss: 0.186175, acc.: 92.19%] [G loss: 4.079082]\n",
      "epoch:24 step:19400 [D loss: 0.283490, acc.: 86.72%] [G loss: 3.332130]\n",
      "epoch:24 step:19401 [D loss: 0.338251, acc.: 86.72%] [G loss: 3.271844]\n",
      "epoch:24 step:19402 [D loss: 0.276517, acc.: 89.06%] [G loss: 2.532436]\n",
      "epoch:24 step:19403 [D loss: 0.283980, acc.: 87.50%] [G loss: 2.281056]\n",
      "epoch:24 step:19404 [D loss: 0.361298, acc.: 85.16%] [G loss: 2.876019]\n",
      "epoch:24 step:19405 [D loss: 0.304964, acc.: 85.16%] [G loss: 2.573871]\n",
      "epoch:24 step:19406 [D loss: 0.310214, acc.: 85.94%] [G loss: 3.181725]\n",
      "epoch:24 step:19407 [D loss: 0.262289, acc.: 92.19%] [G loss: 2.577757]\n",
      "epoch:24 step:19408 [D loss: 0.271997, acc.: 89.06%] [G loss: 4.641469]\n",
      "epoch:24 step:19409 [D loss: 0.208338, acc.: 92.97%] [G loss: 5.245881]\n",
      "epoch:24 step:19410 [D loss: 0.227137, acc.: 91.41%] [G loss: 6.031708]\n",
      "epoch:24 step:19411 [D loss: 0.198764, acc.: 92.19%] [G loss: 4.208161]\n",
      "epoch:24 step:19412 [D loss: 0.409520, acc.: 78.12%] [G loss: 5.827909]\n",
      "epoch:24 step:19413 [D loss: 0.317963, acc.: 85.16%] [G loss: 5.156019]\n",
      "epoch:24 step:19414 [D loss: 0.305353, acc.: 85.16%] [G loss: 4.033832]\n",
      "epoch:24 step:19415 [D loss: 0.297548, acc.: 89.06%] [G loss: 3.209300]\n",
      "epoch:24 step:19416 [D loss: 0.377761, acc.: 87.50%] [G loss: 4.250044]\n",
      "epoch:24 step:19417 [D loss: 0.384628, acc.: 86.72%] [G loss: 2.353578]\n",
      "epoch:24 step:19418 [D loss: 0.319329, acc.: 87.50%] [G loss: 2.214203]\n",
      "epoch:24 step:19419 [D loss: 0.288759, acc.: 86.72%] [G loss: 3.026818]\n",
      "epoch:24 step:19420 [D loss: 0.377274, acc.: 87.50%] [G loss: 3.294567]\n",
      "epoch:24 step:19421 [D loss: 0.352895, acc.: 82.03%] [G loss: 3.846381]\n",
      "epoch:24 step:19422 [D loss: 0.295814, acc.: 83.59%] [G loss: 3.906763]\n",
      "epoch:24 step:19423 [D loss: 0.248851, acc.: 86.72%] [G loss: 3.662344]\n",
      "epoch:24 step:19424 [D loss: 0.342338, acc.: 86.72%] [G loss: 5.909869]\n",
      "epoch:24 step:19425 [D loss: 0.259939, acc.: 88.28%] [G loss: 3.457465]\n",
      "epoch:24 step:19426 [D loss: 0.235407, acc.: 89.84%] [G loss: 3.370634]\n",
      "epoch:24 step:19427 [D loss: 0.383337, acc.: 84.38%] [G loss: 4.052542]\n",
      "epoch:24 step:19428 [D loss: 0.240164, acc.: 91.41%] [G loss: 3.051662]\n",
      "epoch:24 step:19429 [D loss: 0.359571, acc.: 79.69%] [G loss: 3.416605]\n",
      "epoch:24 step:19430 [D loss: 0.338340, acc.: 86.72%] [G loss: 2.680351]\n",
      "epoch:24 step:19431 [D loss: 0.258389, acc.: 87.50%] [G loss: 3.558591]\n",
      "epoch:24 step:19432 [D loss: 0.473466, acc.: 81.25%] [G loss: 4.860626]\n",
      "epoch:24 step:19433 [D loss: 0.437733, acc.: 81.25%] [G loss: 3.350754]\n",
      "epoch:24 step:19434 [D loss: 0.352287, acc.: 78.91%] [G loss: 4.613498]\n",
      "epoch:24 step:19435 [D loss: 0.336590, acc.: 88.28%] [G loss: 3.009247]\n",
      "epoch:24 step:19436 [D loss: 0.379139, acc.: 81.25%] [G loss: 4.056665]\n",
      "epoch:24 step:19437 [D loss: 0.375994, acc.: 82.03%] [G loss: 6.648231]\n",
      "epoch:24 step:19438 [D loss: 0.478969, acc.: 81.25%] [G loss: 4.092454]\n",
      "epoch:24 step:19439 [D loss: 0.377631, acc.: 82.03%] [G loss: 5.462975]\n",
      "epoch:24 step:19440 [D loss: 0.267757, acc.: 88.28%] [G loss: 4.410218]\n",
      "epoch:24 step:19441 [D loss: 0.337906, acc.: 85.94%] [G loss: 2.770989]\n",
      "epoch:24 step:19442 [D loss: 0.281039, acc.: 87.50%] [G loss: 3.925745]\n",
      "epoch:24 step:19443 [D loss: 0.324397, acc.: 85.94%] [G loss: 3.430862]\n",
      "epoch:24 step:19444 [D loss: 0.279569, acc.: 88.28%] [G loss: 3.710149]\n",
      "epoch:24 step:19445 [D loss: 0.300058, acc.: 85.16%] [G loss: 2.951204]\n",
      "epoch:24 step:19446 [D loss: 0.365875, acc.: 85.16%] [G loss: 4.123444]\n",
      "epoch:24 step:19447 [D loss: 0.318186, acc.: 83.59%] [G loss: 4.007209]\n",
      "epoch:24 step:19448 [D loss: 0.332145, acc.: 86.72%] [G loss: 3.763335]\n",
      "epoch:24 step:19449 [D loss: 0.228953, acc.: 92.97%] [G loss: 3.987295]\n",
      "epoch:24 step:19450 [D loss: 0.364644, acc.: 84.38%] [G loss: 3.433045]\n",
      "epoch:24 step:19451 [D loss: 0.391205, acc.: 80.47%] [G loss: 3.594631]\n",
      "epoch:24 step:19452 [D loss: 0.289013, acc.: 85.16%] [G loss: 3.372659]\n",
      "epoch:24 step:19453 [D loss: 0.313452, acc.: 86.72%] [G loss: 3.739211]\n",
      "epoch:24 step:19454 [D loss: 0.411162, acc.: 78.91%] [G loss: 2.915515]\n",
      "epoch:24 step:19455 [D loss: 0.256168, acc.: 91.41%] [G loss: 2.534619]\n",
      "epoch:24 step:19456 [D loss: 0.406792, acc.: 80.47%] [G loss: 2.281463]\n",
      "epoch:24 step:19457 [D loss: 0.368477, acc.: 82.81%] [G loss: 3.091617]\n",
      "epoch:24 step:19458 [D loss: 0.296615, acc.: 83.59%] [G loss: 3.241396]\n",
      "epoch:24 step:19459 [D loss: 0.405966, acc.: 83.59%] [G loss: 4.514592]\n",
      "epoch:24 step:19460 [D loss: 0.577193, acc.: 74.22%] [G loss: 4.002411]\n",
      "epoch:24 step:19461 [D loss: 0.338672, acc.: 85.16%] [G loss: 3.961861]\n",
      "epoch:24 step:19462 [D loss: 0.320446, acc.: 86.72%] [G loss: 4.423785]\n",
      "epoch:24 step:19463 [D loss: 0.369993, acc.: 78.12%] [G loss: 4.548310]\n",
      "epoch:24 step:19464 [D loss: 0.455186, acc.: 75.78%] [G loss: 5.547142]\n",
      "epoch:24 step:19465 [D loss: 0.271689, acc.: 86.72%] [G loss: 4.592309]\n",
      "epoch:24 step:19466 [D loss: 0.287620, acc.: 86.72%] [G loss: 3.150046]\n",
      "epoch:24 step:19467 [D loss: 0.191192, acc.: 92.97%] [G loss: 4.209682]\n",
      "epoch:24 step:19468 [D loss: 0.219554, acc.: 90.62%] [G loss: 3.375269]\n",
      "epoch:24 step:19469 [D loss: 0.326002, acc.: 85.94%] [G loss: 3.720289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19470 [D loss: 0.280339, acc.: 89.06%] [G loss: 4.499349]\n",
      "epoch:24 step:19471 [D loss: 0.318083, acc.: 85.94%] [G loss: 3.579602]\n",
      "epoch:24 step:19472 [D loss: 0.396546, acc.: 82.81%] [G loss: 3.260879]\n",
      "epoch:24 step:19473 [D loss: 0.319905, acc.: 85.16%] [G loss: 3.856989]\n",
      "epoch:24 step:19474 [D loss: 0.241991, acc.: 92.19%] [G loss: 3.586976]\n",
      "epoch:24 step:19475 [D loss: 0.260204, acc.: 89.84%] [G loss: 3.599576]\n",
      "epoch:24 step:19476 [D loss: 0.282549, acc.: 85.16%] [G loss: 3.438820]\n",
      "epoch:24 step:19477 [D loss: 0.356724, acc.: 85.16%] [G loss: 2.397217]\n",
      "epoch:24 step:19478 [D loss: 0.280424, acc.: 86.72%] [G loss: 3.346743]\n",
      "epoch:24 step:19479 [D loss: 0.326353, acc.: 85.94%] [G loss: 2.750317]\n",
      "epoch:24 step:19480 [D loss: 0.302839, acc.: 89.06%] [G loss: 2.278354]\n",
      "epoch:24 step:19481 [D loss: 0.313494, acc.: 85.94%] [G loss: 2.723476]\n",
      "epoch:24 step:19482 [D loss: 0.354719, acc.: 86.72%] [G loss: 2.704851]\n",
      "epoch:24 step:19483 [D loss: 0.292654, acc.: 84.38%] [G loss: 2.553734]\n",
      "epoch:24 step:19484 [D loss: 0.356600, acc.: 81.25%] [G loss: 3.003792]\n",
      "epoch:24 step:19485 [D loss: 0.349670, acc.: 82.81%] [G loss: 2.047328]\n",
      "epoch:24 step:19486 [D loss: 0.355532, acc.: 85.94%] [G loss: 3.932541]\n",
      "epoch:24 step:19487 [D loss: 0.303579, acc.: 90.62%] [G loss: 3.249753]\n",
      "epoch:24 step:19488 [D loss: 0.238438, acc.: 92.19%] [G loss: 3.830581]\n",
      "epoch:24 step:19489 [D loss: 0.302177, acc.: 87.50%] [G loss: 3.998489]\n",
      "epoch:24 step:19490 [D loss: 0.318715, acc.: 85.94%] [G loss: 2.681988]\n",
      "epoch:24 step:19491 [D loss: 0.218042, acc.: 92.19%] [G loss: 4.536785]\n",
      "epoch:24 step:19492 [D loss: 0.509626, acc.: 78.91%] [G loss: 3.479611]\n",
      "epoch:24 step:19493 [D loss: 0.318886, acc.: 84.38%] [G loss: 3.693203]\n",
      "epoch:24 step:19494 [D loss: 0.387326, acc.: 83.59%] [G loss: 2.801421]\n",
      "epoch:24 step:19495 [D loss: 0.370302, acc.: 84.38%] [G loss: 2.195974]\n",
      "epoch:24 step:19496 [D loss: 0.363891, acc.: 82.81%] [G loss: 2.616443]\n",
      "epoch:24 step:19497 [D loss: 0.453682, acc.: 82.03%] [G loss: 3.979083]\n",
      "epoch:24 step:19498 [D loss: 0.414412, acc.: 82.81%] [G loss: 5.862565]\n",
      "epoch:24 step:19499 [D loss: 0.898053, acc.: 67.19%] [G loss: 6.714504]\n",
      "epoch:24 step:19500 [D loss: 2.090400, acc.: 53.12%] [G loss: 6.345104]\n",
      "epoch:24 step:19501 [D loss: 0.642788, acc.: 70.31%] [G loss: 3.884476]\n",
      "epoch:24 step:19502 [D loss: 0.390751, acc.: 80.47%] [G loss: 4.207151]\n",
      "epoch:24 step:19503 [D loss: 0.245427, acc.: 88.28%] [G loss: 3.936406]\n",
      "epoch:24 step:19504 [D loss: 0.361688, acc.: 83.59%] [G loss: 3.565696]\n",
      "epoch:24 step:19505 [D loss: 0.307373, acc.: 86.72%] [G loss: 3.321707]\n",
      "epoch:24 step:19506 [D loss: 0.391281, acc.: 82.03%] [G loss: 2.888335]\n",
      "epoch:24 step:19507 [D loss: 0.227126, acc.: 89.84%] [G loss: 2.694379]\n",
      "epoch:24 step:19508 [D loss: 0.257325, acc.: 88.28%] [G loss: 3.304545]\n",
      "epoch:24 step:19509 [D loss: 0.372009, acc.: 82.81%] [G loss: 3.377681]\n",
      "epoch:24 step:19510 [D loss: 0.382674, acc.: 81.25%] [G loss: 3.571158]\n",
      "epoch:24 step:19511 [D loss: 0.339946, acc.: 82.81%] [G loss: 3.382741]\n",
      "epoch:24 step:19512 [D loss: 0.336685, acc.: 85.94%] [G loss: 3.315552]\n",
      "epoch:24 step:19513 [D loss: 0.316535, acc.: 85.94%] [G loss: 5.138883]\n",
      "epoch:24 step:19514 [D loss: 0.325030, acc.: 86.72%] [G loss: 2.799261]\n",
      "epoch:24 step:19515 [D loss: 0.336653, acc.: 82.81%] [G loss: 3.292262]\n",
      "epoch:24 step:19516 [D loss: 0.351380, acc.: 82.81%] [G loss: 4.001137]\n",
      "epoch:24 step:19517 [D loss: 0.324991, acc.: 84.38%] [G loss: 2.594626]\n",
      "epoch:24 step:19518 [D loss: 0.352740, acc.: 85.16%] [G loss: 2.374023]\n",
      "epoch:24 step:19519 [D loss: 0.390043, acc.: 80.47%] [G loss: 2.806834]\n",
      "epoch:24 step:19520 [D loss: 0.307723, acc.: 83.59%] [G loss: 3.750525]\n",
      "epoch:24 step:19521 [D loss: 0.394465, acc.: 79.69%] [G loss: 6.419233]\n",
      "epoch:24 step:19522 [D loss: 0.300047, acc.: 85.16%] [G loss: 3.266025]\n",
      "epoch:24 step:19523 [D loss: 0.284451, acc.: 86.72%] [G loss: 2.777424]\n",
      "epoch:24 step:19524 [D loss: 0.186813, acc.: 92.19%] [G loss: 3.573913]\n",
      "epoch:24 step:19525 [D loss: 0.344378, acc.: 85.16%] [G loss: 2.639653]\n",
      "epoch:25 step:19526 [D loss: 0.290599, acc.: 89.06%] [G loss: 3.986471]\n",
      "epoch:25 step:19527 [D loss: 0.213203, acc.: 92.19%] [G loss: 3.128406]\n",
      "epoch:25 step:19528 [D loss: 0.356195, acc.: 82.03%] [G loss: 3.320663]\n",
      "epoch:25 step:19529 [D loss: 0.330887, acc.: 84.38%] [G loss: 3.006016]\n",
      "epoch:25 step:19530 [D loss: 0.214212, acc.: 94.53%] [G loss: 3.342415]\n",
      "epoch:25 step:19531 [D loss: 0.289782, acc.: 89.06%] [G loss: 2.355555]\n",
      "epoch:25 step:19532 [D loss: 0.339086, acc.: 84.38%] [G loss: 2.605113]\n",
      "epoch:25 step:19533 [D loss: 0.256005, acc.: 89.84%] [G loss: 2.738245]\n",
      "epoch:25 step:19534 [D loss: 0.337636, acc.: 82.81%] [G loss: 4.214101]\n",
      "epoch:25 step:19535 [D loss: 0.333303, acc.: 82.81%] [G loss: 3.709036]\n",
      "epoch:25 step:19536 [D loss: 0.303899, acc.: 85.16%] [G loss: 3.054475]\n",
      "epoch:25 step:19537 [D loss: 0.289657, acc.: 84.38%] [G loss: 3.572682]\n",
      "epoch:25 step:19538 [D loss: 0.285932, acc.: 86.72%] [G loss: 3.294577]\n",
      "epoch:25 step:19539 [D loss: 0.338486, acc.: 84.38%] [G loss: 3.558972]\n",
      "epoch:25 step:19540 [D loss: 0.289752, acc.: 85.94%] [G loss: 3.536581]\n",
      "epoch:25 step:19541 [D loss: 0.299653, acc.: 86.72%] [G loss: 3.781335]\n",
      "epoch:25 step:19542 [D loss: 0.296541, acc.: 87.50%] [G loss: 3.072713]\n",
      "epoch:25 step:19543 [D loss: 0.289290, acc.: 86.72%] [G loss: 3.055840]\n",
      "epoch:25 step:19544 [D loss: 0.314864, acc.: 85.16%] [G loss: 3.098181]\n",
      "epoch:25 step:19545 [D loss: 0.389784, acc.: 82.81%] [G loss: 2.465438]\n",
      "epoch:25 step:19546 [D loss: 0.317934, acc.: 84.38%] [G loss: 2.299478]\n",
      "epoch:25 step:19547 [D loss: 0.342276, acc.: 85.94%] [G loss: 2.008408]\n",
      "epoch:25 step:19548 [D loss: 0.190649, acc.: 92.97%] [G loss: 3.387513]\n",
      "epoch:25 step:19549 [D loss: 0.477146, acc.: 75.00%] [G loss: 2.448557]\n",
      "epoch:25 step:19550 [D loss: 0.301858, acc.: 88.28%] [G loss: 3.078066]\n",
      "epoch:25 step:19551 [D loss: 0.306070, acc.: 89.84%] [G loss: 3.117740]\n",
      "epoch:25 step:19552 [D loss: 0.422552, acc.: 81.25%] [G loss: 2.960903]\n",
      "epoch:25 step:19553 [D loss: 0.367622, acc.: 82.03%] [G loss: 3.172000]\n",
      "epoch:25 step:19554 [D loss: 0.256494, acc.: 89.84%] [G loss: 3.833577]\n",
      "epoch:25 step:19555 [D loss: 0.246890, acc.: 89.06%] [G loss: 3.529313]\n",
      "epoch:25 step:19556 [D loss: 0.307116, acc.: 89.84%] [G loss: 3.822448]\n",
      "epoch:25 step:19557 [D loss: 0.317982, acc.: 85.94%] [G loss: 3.111564]\n",
      "epoch:25 step:19558 [D loss: 0.250401, acc.: 89.84%] [G loss: 3.086287]\n",
      "epoch:25 step:19559 [D loss: 0.341215, acc.: 82.81%] [G loss: 3.064270]\n",
      "epoch:25 step:19560 [D loss: 0.398964, acc.: 79.69%] [G loss: 4.143912]\n",
      "epoch:25 step:19561 [D loss: 0.430669, acc.: 81.25%] [G loss: 2.648517]\n",
      "epoch:25 step:19562 [D loss: 0.395215, acc.: 77.34%] [G loss: 3.467391]\n",
      "epoch:25 step:19563 [D loss: 0.377509, acc.: 81.25%] [G loss: 2.887379]\n",
      "epoch:25 step:19564 [D loss: 0.322711, acc.: 85.16%] [G loss: 2.769382]\n",
      "epoch:25 step:19565 [D loss: 0.308356, acc.: 84.38%] [G loss: 2.202888]\n",
      "epoch:25 step:19566 [D loss: 0.376877, acc.: 83.59%] [G loss: 2.691247]\n",
      "epoch:25 step:19567 [D loss: 0.292026, acc.: 88.28%] [G loss: 2.844199]\n",
      "epoch:25 step:19568 [D loss: 0.395585, acc.: 77.34%] [G loss: 3.224590]\n",
      "epoch:25 step:19569 [D loss: 0.408519, acc.: 82.03%] [G loss: 3.051100]\n",
      "epoch:25 step:19570 [D loss: 0.385038, acc.: 86.72%] [G loss: 3.304650]\n",
      "epoch:25 step:19571 [D loss: 0.379254, acc.: 86.72%] [G loss: 5.298604]\n",
      "epoch:25 step:19572 [D loss: 0.279829, acc.: 85.94%] [G loss: 4.886901]\n",
      "epoch:25 step:19573 [D loss: 0.324852, acc.: 85.94%] [G loss: 4.573873]\n",
      "epoch:25 step:19574 [D loss: 0.282155, acc.: 87.50%] [G loss: 2.525019]\n",
      "epoch:25 step:19575 [D loss: 0.410485, acc.: 82.81%] [G loss: 4.051655]\n",
      "epoch:25 step:19576 [D loss: 0.467342, acc.: 81.25%] [G loss: 5.290127]\n",
      "epoch:25 step:19577 [D loss: 0.265281, acc.: 85.16%] [G loss: 4.003585]\n",
      "epoch:25 step:19578 [D loss: 0.259298, acc.: 85.94%] [G loss: 3.956101]\n",
      "epoch:25 step:19579 [D loss: 0.351557, acc.: 85.16%] [G loss: 2.543890]\n",
      "epoch:25 step:19580 [D loss: 0.220420, acc.: 92.19%] [G loss: 3.753603]\n",
      "epoch:25 step:19581 [D loss: 0.415345, acc.: 76.56%] [G loss: 3.380339]\n",
      "epoch:25 step:19582 [D loss: 0.267511, acc.: 88.28%] [G loss: 3.140449]\n",
      "epoch:25 step:19583 [D loss: 0.404651, acc.: 81.25%] [G loss: 2.795615]\n",
      "epoch:25 step:19584 [D loss: 0.316339, acc.: 87.50%] [G loss: 3.713359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19585 [D loss: 0.303290, acc.: 86.72%] [G loss: 3.046454]\n",
      "epoch:25 step:19586 [D loss: 0.352785, acc.: 83.59%] [G loss: 3.374349]\n",
      "epoch:25 step:19587 [D loss: 0.430203, acc.: 81.25%] [G loss: 2.762585]\n",
      "epoch:25 step:19588 [D loss: 0.389596, acc.: 81.25%] [G loss: 3.297136]\n",
      "epoch:25 step:19589 [D loss: 0.413239, acc.: 85.94%] [G loss: 2.652789]\n",
      "epoch:25 step:19590 [D loss: 0.360300, acc.: 82.81%] [G loss: 2.642560]\n",
      "epoch:25 step:19591 [D loss: 0.337686, acc.: 84.38%] [G loss: 2.667846]\n",
      "epoch:25 step:19592 [D loss: 0.239099, acc.: 89.06%] [G loss: 3.860239]\n",
      "epoch:25 step:19593 [D loss: 0.332033, acc.: 82.81%] [G loss: 3.034231]\n",
      "epoch:25 step:19594 [D loss: 0.311077, acc.: 82.81%] [G loss: 4.509462]\n",
      "epoch:25 step:19595 [D loss: 0.231465, acc.: 89.06%] [G loss: 4.868933]\n",
      "epoch:25 step:19596 [D loss: 0.347072, acc.: 84.38%] [G loss: 5.314995]\n",
      "epoch:25 step:19597 [D loss: 0.264582, acc.: 86.72%] [G loss: 4.101981]\n",
      "epoch:25 step:19598 [D loss: 0.289176, acc.: 87.50%] [G loss: 3.457006]\n",
      "epoch:25 step:19599 [D loss: 0.313687, acc.: 82.03%] [G loss: 3.163805]\n",
      "epoch:25 step:19600 [D loss: 0.327252, acc.: 85.16%] [G loss: 2.978247]\n",
      "epoch:25 step:19601 [D loss: 0.301845, acc.: 89.84%] [G loss: 2.913353]\n",
      "epoch:25 step:19602 [D loss: 0.297484, acc.: 89.06%] [G loss: 2.977814]\n",
      "epoch:25 step:19603 [D loss: 0.385784, acc.: 82.81%] [G loss: 3.119136]\n",
      "epoch:25 step:19604 [D loss: 0.306341, acc.: 87.50%] [G loss: 2.917450]\n",
      "epoch:25 step:19605 [D loss: 0.395207, acc.: 82.81%] [G loss: 3.072131]\n",
      "epoch:25 step:19606 [D loss: 0.317015, acc.: 88.28%] [G loss: 3.784972]\n",
      "epoch:25 step:19607 [D loss: 0.471619, acc.: 77.34%] [G loss: 4.697961]\n",
      "epoch:25 step:19608 [D loss: 0.384638, acc.: 82.03%] [G loss: 4.064452]\n",
      "epoch:25 step:19609 [D loss: 0.288287, acc.: 85.94%] [G loss: 5.678804]\n",
      "epoch:25 step:19610 [D loss: 0.455350, acc.: 82.81%] [G loss: 3.947939]\n",
      "epoch:25 step:19611 [D loss: 0.321248, acc.: 86.72%] [G loss: 5.288860]\n",
      "epoch:25 step:19612 [D loss: 0.383917, acc.: 81.25%] [G loss: 3.873524]\n",
      "epoch:25 step:19613 [D loss: 0.304049, acc.: 90.62%] [G loss: 4.632792]\n",
      "epoch:25 step:19614 [D loss: 0.314085, acc.: 85.94%] [G loss: 3.520438]\n",
      "epoch:25 step:19615 [D loss: 0.379478, acc.: 78.12%] [G loss: 3.847804]\n",
      "epoch:25 step:19616 [D loss: 0.367060, acc.: 82.03%] [G loss: 3.407237]\n",
      "epoch:25 step:19617 [D loss: 0.283095, acc.: 91.41%] [G loss: 4.176933]\n",
      "epoch:25 step:19618 [D loss: 0.452604, acc.: 76.56%] [G loss: 2.867037]\n",
      "epoch:25 step:19619 [D loss: 0.202513, acc.: 92.97%] [G loss: 4.880413]\n",
      "epoch:25 step:19620 [D loss: 0.330005, acc.: 85.94%] [G loss: 2.760184]\n",
      "epoch:25 step:19621 [D loss: 0.231043, acc.: 90.62%] [G loss: 3.258637]\n",
      "epoch:25 step:19622 [D loss: 0.352855, acc.: 84.38%] [G loss: 3.173873]\n",
      "epoch:25 step:19623 [D loss: 0.518386, acc.: 76.56%] [G loss: 3.459716]\n",
      "epoch:25 step:19624 [D loss: 0.407089, acc.: 83.59%] [G loss: 2.486562]\n",
      "epoch:25 step:19625 [D loss: 0.434921, acc.: 80.47%] [G loss: 3.603917]\n",
      "epoch:25 step:19626 [D loss: 0.329033, acc.: 84.38%] [G loss: 3.977599]\n",
      "epoch:25 step:19627 [D loss: 0.347688, acc.: 84.38%] [G loss: 2.870061]\n",
      "epoch:25 step:19628 [D loss: 0.364631, acc.: 82.81%] [G loss: 2.528114]\n",
      "epoch:25 step:19629 [D loss: 0.318573, acc.: 86.72%] [G loss: 3.152924]\n",
      "epoch:25 step:19630 [D loss: 0.350390, acc.: 81.25%] [G loss: 2.752927]\n",
      "epoch:25 step:19631 [D loss: 0.298529, acc.: 88.28%] [G loss: 3.080456]\n",
      "epoch:25 step:19632 [D loss: 0.367457, acc.: 81.25%] [G loss: 2.871190]\n",
      "epoch:25 step:19633 [D loss: 0.282707, acc.: 85.94%] [G loss: 3.448320]\n",
      "epoch:25 step:19634 [D loss: 0.308346, acc.: 85.94%] [G loss: 2.761642]\n",
      "epoch:25 step:19635 [D loss: 0.334908, acc.: 85.16%] [G loss: 3.513264]\n",
      "epoch:25 step:19636 [D loss: 0.494813, acc.: 78.12%] [G loss: 2.057761]\n",
      "epoch:25 step:19637 [D loss: 0.326512, acc.: 88.28%] [G loss: 2.957907]\n",
      "epoch:25 step:19638 [D loss: 0.305674, acc.: 85.94%] [G loss: 2.648313]\n",
      "epoch:25 step:19639 [D loss: 0.455515, acc.: 77.34%] [G loss: 2.422466]\n",
      "epoch:25 step:19640 [D loss: 0.332600, acc.: 82.03%] [G loss: 3.475358]\n",
      "epoch:25 step:19641 [D loss: 0.417082, acc.: 84.38%] [G loss: 2.581365]\n",
      "epoch:25 step:19642 [D loss: 0.322078, acc.: 86.72%] [G loss: 3.848351]\n",
      "epoch:25 step:19643 [D loss: 0.228226, acc.: 90.62%] [G loss: 3.545573]\n",
      "epoch:25 step:19644 [D loss: 0.315684, acc.: 86.72%] [G loss: 6.103715]\n",
      "epoch:25 step:19645 [D loss: 0.289337, acc.: 85.94%] [G loss: 6.099650]\n",
      "epoch:25 step:19646 [D loss: 0.265282, acc.: 89.06%] [G loss: 5.490685]\n",
      "epoch:25 step:19647 [D loss: 0.230836, acc.: 89.84%] [G loss: 4.789086]\n",
      "epoch:25 step:19648 [D loss: 0.385296, acc.: 78.12%] [G loss: 3.203149]\n",
      "epoch:25 step:19649 [D loss: 0.319580, acc.: 85.16%] [G loss: 2.830979]\n",
      "epoch:25 step:19650 [D loss: 0.396890, acc.: 83.59%] [G loss: 2.318314]\n",
      "epoch:25 step:19651 [D loss: 0.399748, acc.: 82.03%] [G loss: 3.729926]\n",
      "epoch:25 step:19652 [D loss: 0.273229, acc.: 88.28%] [G loss: 3.422383]\n",
      "epoch:25 step:19653 [D loss: 0.200242, acc.: 91.41%] [G loss: 3.268649]\n",
      "epoch:25 step:19654 [D loss: 0.305571, acc.: 84.38%] [G loss: 4.419188]\n",
      "epoch:25 step:19655 [D loss: 0.288770, acc.: 87.50%] [G loss: 3.129964]\n",
      "epoch:25 step:19656 [D loss: 0.247575, acc.: 91.41%] [G loss: 3.188914]\n",
      "epoch:25 step:19657 [D loss: 0.300578, acc.: 84.38%] [G loss: 2.854121]\n",
      "epoch:25 step:19658 [D loss: 0.408938, acc.: 85.16%] [G loss: 3.284205]\n",
      "epoch:25 step:19659 [D loss: 0.262856, acc.: 92.19%] [G loss: 3.566617]\n",
      "epoch:25 step:19660 [D loss: 0.342422, acc.: 87.50%] [G loss: 2.881463]\n",
      "epoch:25 step:19661 [D loss: 0.269953, acc.: 89.84%] [G loss: 3.177584]\n",
      "epoch:25 step:19662 [D loss: 0.288416, acc.: 85.94%] [G loss: 3.080919]\n",
      "epoch:25 step:19663 [D loss: 0.209336, acc.: 90.62%] [G loss: 3.304780]\n",
      "epoch:25 step:19664 [D loss: 0.207665, acc.: 93.75%] [G loss: 3.772299]\n",
      "epoch:25 step:19665 [D loss: 0.291539, acc.: 89.84%] [G loss: 3.066849]\n",
      "epoch:25 step:19666 [D loss: 0.291084, acc.: 85.16%] [G loss: 2.855475]\n",
      "epoch:25 step:19667 [D loss: 0.330591, acc.: 84.38%] [G loss: 2.609956]\n",
      "epoch:25 step:19668 [D loss: 0.257182, acc.: 88.28%] [G loss: 4.075110]\n",
      "epoch:25 step:19669 [D loss: 0.267393, acc.: 89.84%] [G loss: 3.302818]\n",
      "epoch:25 step:19670 [D loss: 0.338053, acc.: 82.03%] [G loss: 3.456903]\n",
      "epoch:25 step:19671 [D loss: 0.293453, acc.: 87.50%] [G loss: 6.440779]\n",
      "epoch:25 step:19672 [D loss: 0.619418, acc.: 72.66%] [G loss: 4.173713]\n",
      "epoch:25 step:19673 [D loss: 0.281395, acc.: 89.06%] [G loss: 4.540714]\n",
      "epoch:25 step:19674 [D loss: 0.330360, acc.: 86.72%] [G loss: 3.072917]\n",
      "epoch:25 step:19675 [D loss: 0.307534, acc.: 83.59%] [G loss: 4.707534]\n",
      "epoch:25 step:19676 [D loss: 0.264115, acc.: 87.50%] [G loss: 3.676434]\n",
      "epoch:25 step:19677 [D loss: 0.276021, acc.: 89.84%] [G loss: 3.873953]\n",
      "epoch:25 step:19678 [D loss: 0.342704, acc.: 83.59%] [G loss: 3.882997]\n",
      "epoch:25 step:19679 [D loss: 0.309139, acc.: 87.50%] [G loss: 4.191449]\n",
      "epoch:25 step:19680 [D loss: 0.369784, acc.: 85.94%] [G loss: 2.599057]\n",
      "epoch:25 step:19681 [D loss: 0.311888, acc.: 85.94%] [G loss: 3.004909]\n",
      "epoch:25 step:19682 [D loss: 0.375250, acc.: 85.16%] [G loss: 3.512849]\n",
      "epoch:25 step:19683 [D loss: 0.369173, acc.: 82.81%] [G loss: 2.835826]\n",
      "epoch:25 step:19684 [D loss: 0.290355, acc.: 85.16%] [G loss: 2.983574]\n",
      "epoch:25 step:19685 [D loss: 0.427276, acc.: 78.12%] [G loss: 3.442680]\n",
      "epoch:25 step:19686 [D loss: 0.413664, acc.: 79.69%] [G loss: 4.556887]\n",
      "epoch:25 step:19687 [D loss: 0.393518, acc.: 79.69%] [G loss: 3.340246]\n",
      "epoch:25 step:19688 [D loss: 0.342885, acc.: 85.94%] [G loss: 4.019771]\n",
      "epoch:25 step:19689 [D loss: 0.200473, acc.: 91.41%] [G loss: 2.780501]\n",
      "epoch:25 step:19690 [D loss: 0.369180, acc.: 84.38%] [G loss: 2.684114]\n",
      "epoch:25 step:19691 [D loss: 0.309458, acc.: 87.50%] [G loss: 2.420604]\n",
      "epoch:25 step:19692 [D loss: 0.279929, acc.: 89.84%] [G loss: 2.541329]\n",
      "epoch:25 step:19693 [D loss: 0.242725, acc.: 89.84%] [G loss: 3.453122]\n",
      "epoch:25 step:19694 [D loss: 0.333654, acc.: 85.94%] [G loss: 3.524060]\n",
      "epoch:25 step:19695 [D loss: 0.236065, acc.: 90.62%] [G loss: 4.403438]\n",
      "epoch:25 step:19696 [D loss: 0.315051, acc.: 84.38%] [G loss: 2.644390]\n",
      "epoch:25 step:19697 [D loss: 0.369638, acc.: 79.69%] [G loss: 3.096751]\n",
      "epoch:25 step:19698 [D loss: 0.398420, acc.: 78.91%] [G loss: 2.891001]\n",
      "epoch:25 step:19699 [D loss: 0.326789, acc.: 85.16%] [G loss: 4.503563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19700 [D loss: 0.427545, acc.: 77.34%] [G loss: 4.141422]\n",
      "epoch:25 step:19701 [D loss: 0.351196, acc.: 82.81%] [G loss: 3.883200]\n",
      "epoch:25 step:19702 [D loss: 0.374931, acc.: 83.59%] [G loss: 2.901195]\n",
      "epoch:25 step:19703 [D loss: 0.276393, acc.: 87.50%] [G loss: 4.029552]\n",
      "epoch:25 step:19704 [D loss: 0.262738, acc.: 89.06%] [G loss: 3.210001]\n",
      "epoch:25 step:19705 [D loss: 0.310728, acc.: 88.28%] [G loss: 3.032705]\n",
      "epoch:25 step:19706 [D loss: 0.281245, acc.: 86.72%] [G loss: 3.302598]\n",
      "epoch:25 step:19707 [D loss: 0.346247, acc.: 86.72%] [G loss: 4.939507]\n",
      "epoch:25 step:19708 [D loss: 0.373889, acc.: 84.38%] [G loss: 5.760099]\n",
      "epoch:25 step:19709 [D loss: 0.564962, acc.: 77.34%] [G loss: 3.201370]\n",
      "epoch:25 step:19710 [D loss: 0.330109, acc.: 84.38%] [G loss: 2.603714]\n",
      "epoch:25 step:19711 [D loss: 0.415095, acc.: 82.81%] [G loss: 3.312616]\n",
      "epoch:25 step:19712 [D loss: 0.244064, acc.: 89.84%] [G loss: 3.165451]\n",
      "epoch:25 step:19713 [D loss: 0.291431, acc.: 85.94%] [G loss: 4.632861]\n",
      "epoch:25 step:19714 [D loss: 0.244932, acc.: 90.62%] [G loss: 3.805111]\n",
      "epoch:25 step:19715 [D loss: 0.335855, acc.: 82.81%] [G loss: 3.614913]\n",
      "epoch:25 step:19716 [D loss: 0.382244, acc.: 82.81%] [G loss: 2.709144]\n",
      "epoch:25 step:19717 [D loss: 0.328723, acc.: 85.16%] [G loss: 3.238708]\n",
      "epoch:25 step:19718 [D loss: 0.424992, acc.: 76.56%] [G loss: 2.732230]\n",
      "epoch:25 step:19719 [D loss: 0.195463, acc.: 92.97%] [G loss: 4.533686]\n",
      "epoch:25 step:19720 [D loss: 0.284843, acc.: 84.38%] [G loss: 4.944575]\n",
      "epoch:25 step:19721 [D loss: 0.306093, acc.: 86.72%] [G loss: 3.317802]\n",
      "epoch:25 step:19722 [D loss: 0.444538, acc.: 78.12%] [G loss: 3.622302]\n",
      "epoch:25 step:19723 [D loss: 0.406680, acc.: 77.34%] [G loss: 3.376847]\n",
      "epoch:25 step:19724 [D loss: 0.215211, acc.: 92.19%] [G loss: 3.658368]\n",
      "epoch:25 step:19725 [D loss: 0.360082, acc.: 85.16%] [G loss: 3.038957]\n",
      "epoch:25 step:19726 [D loss: 0.392092, acc.: 84.38%] [G loss: 2.287734]\n",
      "epoch:25 step:19727 [D loss: 0.370251, acc.: 84.38%] [G loss: 2.935878]\n",
      "epoch:25 step:19728 [D loss: 0.300206, acc.: 85.94%] [G loss: 3.723106]\n",
      "epoch:25 step:19729 [D loss: 0.331580, acc.: 83.59%] [G loss: 3.703892]\n",
      "epoch:25 step:19730 [D loss: 0.344929, acc.: 82.03%] [G loss: 3.535095]\n",
      "epoch:25 step:19731 [D loss: 0.283609, acc.: 89.84%] [G loss: 4.143379]\n",
      "epoch:25 step:19732 [D loss: 0.374575, acc.: 82.03%] [G loss: 3.659020]\n",
      "epoch:25 step:19733 [D loss: 0.341523, acc.: 88.28%] [G loss: 2.353064]\n",
      "epoch:25 step:19734 [D loss: 0.508613, acc.: 74.22%] [G loss: 3.250224]\n",
      "epoch:25 step:19735 [D loss: 0.388439, acc.: 81.25%] [G loss: 4.147626]\n",
      "epoch:25 step:19736 [D loss: 0.458736, acc.: 80.47%] [G loss: 2.612029]\n",
      "epoch:25 step:19737 [D loss: 0.300300, acc.: 83.59%] [G loss: 4.013529]\n",
      "epoch:25 step:19738 [D loss: 0.332165, acc.: 84.38%] [G loss: 4.410884]\n",
      "epoch:25 step:19739 [D loss: 0.389320, acc.: 85.94%] [G loss: 4.042096]\n",
      "epoch:25 step:19740 [D loss: 0.306350, acc.: 83.59%] [G loss: 3.757007]\n",
      "epoch:25 step:19741 [D loss: 0.308165, acc.: 85.16%] [G loss: 3.944093]\n",
      "epoch:25 step:19742 [D loss: 0.266630, acc.: 87.50%] [G loss: 3.128609]\n",
      "epoch:25 step:19743 [D loss: 0.240034, acc.: 92.97%] [G loss: 4.549729]\n",
      "epoch:25 step:19744 [D loss: 0.426975, acc.: 80.47%] [G loss: 3.034182]\n",
      "epoch:25 step:19745 [D loss: 0.254555, acc.: 87.50%] [G loss: 3.716647]\n",
      "epoch:25 step:19746 [D loss: 0.335212, acc.: 87.50%] [G loss: 3.032330]\n",
      "epoch:25 step:19747 [D loss: 0.417520, acc.: 84.38%] [G loss: 3.288345]\n",
      "epoch:25 step:19748 [D loss: 0.299413, acc.: 85.94%] [G loss: 4.868626]\n",
      "epoch:25 step:19749 [D loss: 0.243880, acc.: 87.50%] [G loss: 5.905417]\n",
      "epoch:25 step:19750 [D loss: 0.226636, acc.: 92.19%] [G loss: 5.238587]\n",
      "epoch:25 step:19751 [D loss: 0.294390, acc.: 85.16%] [G loss: 4.154131]\n",
      "epoch:25 step:19752 [D loss: 0.235449, acc.: 87.50%] [G loss: 6.172150]\n",
      "epoch:25 step:19753 [D loss: 0.259261, acc.: 88.28%] [G loss: 5.450522]\n",
      "epoch:25 step:19754 [D loss: 0.258846, acc.: 85.94%] [G loss: 3.912056]\n",
      "epoch:25 step:19755 [D loss: 0.320401, acc.: 85.16%] [G loss: 4.870214]\n",
      "epoch:25 step:19756 [D loss: 0.191807, acc.: 90.62%] [G loss: 4.124012]\n",
      "epoch:25 step:19757 [D loss: 0.203739, acc.: 90.62%] [G loss: 4.762269]\n",
      "epoch:25 step:19758 [D loss: 0.236732, acc.: 90.62%] [G loss: 3.815037]\n",
      "epoch:25 step:19759 [D loss: 0.314421, acc.: 85.94%] [G loss: 2.379225]\n",
      "epoch:25 step:19760 [D loss: 0.213833, acc.: 89.84%] [G loss: 3.490957]\n",
      "epoch:25 step:19761 [D loss: 0.385243, acc.: 81.25%] [G loss: 3.237123]\n",
      "epoch:25 step:19762 [D loss: 0.298315, acc.: 85.16%] [G loss: 2.965058]\n",
      "epoch:25 step:19763 [D loss: 0.306607, acc.: 89.06%] [G loss: 2.938429]\n",
      "epoch:25 step:19764 [D loss: 0.304847, acc.: 87.50%] [G loss: 2.708720]\n",
      "epoch:25 step:19765 [D loss: 0.403280, acc.: 81.25%] [G loss: 4.698542]\n",
      "epoch:25 step:19766 [D loss: 0.345463, acc.: 85.16%] [G loss: 2.135690]\n",
      "epoch:25 step:19767 [D loss: 0.336736, acc.: 87.50%] [G loss: 4.118237]\n",
      "epoch:25 step:19768 [D loss: 0.308787, acc.: 85.94%] [G loss: 4.054063]\n",
      "epoch:25 step:19769 [D loss: 0.329368, acc.: 86.72%] [G loss: 2.775341]\n",
      "epoch:25 step:19770 [D loss: 0.395673, acc.: 83.59%] [G loss: 8.392982]\n",
      "epoch:25 step:19771 [D loss: 0.424999, acc.: 81.25%] [G loss: 3.336159]\n",
      "epoch:25 step:19772 [D loss: 0.284546, acc.: 89.84%] [G loss: 3.946844]\n",
      "epoch:25 step:19773 [D loss: 0.296092, acc.: 87.50%] [G loss: 3.009338]\n",
      "epoch:25 step:19774 [D loss: 0.323731, acc.: 85.16%] [G loss: 3.353876]\n",
      "epoch:25 step:19775 [D loss: 0.290200, acc.: 88.28%] [G loss: 3.752228]\n",
      "epoch:25 step:19776 [D loss: 0.305453, acc.: 84.38%] [G loss: 4.000360]\n",
      "epoch:25 step:19777 [D loss: 0.349867, acc.: 85.94%] [G loss: 2.449628]\n",
      "epoch:25 step:19778 [D loss: 0.287004, acc.: 87.50%] [G loss: 3.985282]\n",
      "epoch:25 step:19779 [D loss: 0.336722, acc.: 85.16%] [G loss: 3.614336]\n",
      "epoch:25 step:19780 [D loss: 0.266032, acc.: 87.50%] [G loss: 2.682730]\n",
      "epoch:25 step:19781 [D loss: 0.309664, acc.: 85.94%] [G loss: 3.231782]\n",
      "epoch:25 step:19782 [D loss: 0.368513, acc.: 83.59%] [G loss: 3.048438]\n",
      "epoch:25 step:19783 [D loss: 0.303567, acc.: 84.38%] [G loss: 3.677928]\n",
      "epoch:25 step:19784 [D loss: 0.254776, acc.: 89.84%] [G loss: 2.971033]\n",
      "epoch:25 step:19785 [D loss: 0.291806, acc.: 84.38%] [G loss: 3.065683]\n",
      "epoch:25 step:19786 [D loss: 0.278354, acc.: 88.28%] [G loss: 3.073638]\n",
      "epoch:25 step:19787 [D loss: 0.279303, acc.: 86.72%] [G loss: 2.498597]\n",
      "epoch:25 step:19788 [D loss: 0.204888, acc.: 90.62%] [G loss: 3.625557]\n",
      "epoch:25 step:19789 [D loss: 0.305735, acc.: 85.94%] [G loss: 3.162664]\n",
      "epoch:25 step:19790 [D loss: 0.324865, acc.: 84.38%] [G loss: 2.816311]\n",
      "epoch:25 step:19791 [D loss: 0.260548, acc.: 89.06%] [G loss: 3.397556]\n",
      "epoch:25 step:19792 [D loss: 0.352404, acc.: 84.38%] [G loss: 2.877588]\n",
      "epoch:25 step:19793 [D loss: 0.307479, acc.: 86.72%] [G loss: 3.151699]\n",
      "epoch:25 step:19794 [D loss: 0.318078, acc.: 82.03%] [G loss: 2.953220]\n",
      "epoch:25 step:19795 [D loss: 0.347046, acc.: 84.38%] [G loss: 2.805962]\n",
      "epoch:25 step:19796 [D loss: 0.267494, acc.: 91.41%] [G loss: 2.974430]\n",
      "epoch:25 step:19797 [D loss: 0.299623, acc.: 85.16%] [G loss: 2.885204]\n",
      "epoch:25 step:19798 [D loss: 0.316664, acc.: 86.72%] [G loss: 2.663007]\n",
      "epoch:25 step:19799 [D loss: 0.344049, acc.: 84.38%] [G loss: 2.882046]\n",
      "epoch:25 step:19800 [D loss: 0.420651, acc.: 76.56%] [G loss: 2.876984]\n",
      "epoch:25 step:19801 [D loss: 0.317521, acc.: 87.50%] [G loss: 3.278219]\n",
      "epoch:25 step:19802 [D loss: 0.301994, acc.: 88.28%] [G loss: 2.797696]\n",
      "epoch:25 step:19803 [D loss: 0.432257, acc.: 80.47%] [G loss: 2.559262]\n",
      "epoch:25 step:19804 [D loss: 0.471914, acc.: 79.69%] [G loss: 2.689299]\n",
      "epoch:25 step:19805 [D loss: 0.196185, acc.: 92.97%] [G loss: 3.265165]\n",
      "epoch:25 step:19806 [D loss: 0.282827, acc.: 88.28%] [G loss: 3.985857]\n",
      "epoch:25 step:19807 [D loss: 0.227030, acc.: 89.06%] [G loss: 3.332273]\n",
      "epoch:25 step:19808 [D loss: 0.318654, acc.: 83.59%] [G loss: 3.205030]\n",
      "epoch:25 step:19809 [D loss: 0.262616, acc.: 90.62%] [G loss: 3.249825]\n",
      "epoch:25 step:19810 [D loss: 0.305836, acc.: 86.72%] [G loss: 2.756305]\n",
      "epoch:25 step:19811 [D loss: 0.258238, acc.: 86.72%] [G loss: 4.401720]\n",
      "epoch:25 step:19812 [D loss: 0.184917, acc.: 91.41%] [G loss: 5.409993]\n",
      "epoch:25 step:19813 [D loss: 0.285591, acc.: 86.72%] [G loss: 3.298506]\n",
      "epoch:25 step:19814 [D loss: 0.304337, acc.: 82.03%] [G loss: 5.076127]\n",
      "epoch:25 step:19815 [D loss: 0.236879, acc.: 89.06%] [G loss: 4.888957]\n",
      "epoch:25 step:19816 [D loss: 0.256753, acc.: 85.94%] [G loss: 3.661768]\n",
      "epoch:25 step:19817 [D loss: 0.360967, acc.: 85.94%] [G loss: 3.824771]\n",
      "epoch:25 step:19818 [D loss: 0.409181, acc.: 84.38%] [G loss: 3.400368]\n",
      "epoch:25 step:19819 [D loss: 0.386055, acc.: 86.72%] [G loss: 2.940462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19820 [D loss: 0.258191, acc.: 88.28%] [G loss: 2.871324]\n",
      "epoch:25 step:19821 [D loss: 0.253919, acc.: 89.84%] [G loss: 3.135264]\n",
      "epoch:25 step:19822 [D loss: 0.365004, acc.: 81.25%] [G loss: 2.601110]\n",
      "epoch:25 step:19823 [D loss: 0.353651, acc.: 85.16%] [G loss: 3.528406]\n",
      "epoch:25 step:19824 [D loss: 0.281730, acc.: 87.50%] [G loss: 2.143881]\n",
      "epoch:25 step:19825 [D loss: 0.335684, acc.: 87.50%] [G loss: 2.969022]\n",
      "epoch:25 step:19826 [D loss: 0.280587, acc.: 88.28%] [G loss: 3.168380]\n",
      "epoch:25 step:19827 [D loss: 0.390390, acc.: 87.50%] [G loss: 4.598554]\n",
      "epoch:25 step:19828 [D loss: 0.524169, acc.: 78.12%] [G loss: 6.252032]\n",
      "epoch:25 step:19829 [D loss: 0.670966, acc.: 73.44%] [G loss: 6.048243]\n",
      "epoch:25 step:19830 [D loss: 0.992301, acc.: 72.66%] [G loss: 5.830467]\n",
      "epoch:25 step:19831 [D loss: 1.055948, acc.: 68.75%] [G loss: 3.284634]\n",
      "epoch:25 step:19832 [D loss: 0.428019, acc.: 76.56%] [G loss: 4.490717]\n",
      "epoch:25 step:19833 [D loss: 0.511894, acc.: 79.69%] [G loss: 6.544582]\n",
      "epoch:25 step:19834 [D loss: 0.623462, acc.: 73.44%] [G loss: 4.443552]\n",
      "epoch:25 step:19835 [D loss: 0.236818, acc.: 92.19%] [G loss: 4.358750]\n",
      "epoch:25 step:19836 [D loss: 0.472589, acc.: 80.47%] [G loss: 5.945611]\n",
      "epoch:25 step:19837 [D loss: 0.636950, acc.: 71.88%] [G loss: 3.599372]\n",
      "epoch:25 step:19838 [D loss: 0.447631, acc.: 80.47%] [G loss: 3.145168]\n",
      "epoch:25 step:19839 [D loss: 0.258311, acc.: 89.06%] [G loss: 3.278747]\n",
      "epoch:25 step:19840 [D loss: 0.517341, acc.: 80.47%] [G loss: 3.189291]\n",
      "epoch:25 step:19841 [D loss: 0.271391, acc.: 89.06%] [G loss: 2.901548]\n",
      "epoch:25 step:19842 [D loss: 0.383924, acc.: 83.59%] [G loss: 3.205893]\n",
      "epoch:25 step:19843 [D loss: 0.328538, acc.: 86.72%] [G loss: 2.489254]\n",
      "epoch:25 step:19844 [D loss: 0.267209, acc.: 89.06%] [G loss: 3.007698]\n",
      "epoch:25 step:19845 [D loss: 0.422302, acc.: 77.34%] [G loss: 2.856560]\n",
      "epoch:25 step:19846 [D loss: 0.384460, acc.: 82.81%] [G loss: 2.879040]\n",
      "epoch:25 step:19847 [D loss: 0.298930, acc.: 82.03%] [G loss: 4.018589]\n",
      "epoch:25 step:19848 [D loss: 0.331007, acc.: 87.50%] [G loss: 2.571429]\n",
      "epoch:25 step:19849 [D loss: 0.333700, acc.: 86.72%] [G loss: 2.856882]\n",
      "epoch:25 step:19850 [D loss: 0.381120, acc.: 82.81%] [G loss: 2.345237]\n",
      "epoch:25 step:19851 [D loss: 0.280843, acc.: 86.72%] [G loss: 3.722114]\n",
      "epoch:25 step:19852 [D loss: 0.286749, acc.: 87.50%] [G loss: 3.233064]\n",
      "epoch:25 step:19853 [D loss: 0.234193, acc.: 88.28%] [G loss: 3.705219]\n",
      "epoch:25 step:19854 [D loss: 0.269016, acc.: 89.84%] [G loss: 3.070270]\n",
      "epoch:25 step:19855 [D loss: 0.300536, acc.: 86.72%] [G loss: 2.864872]\n",
      "epoch:25 step:19856 [D loss: 0.266156, acc.: 87.50%] [G loss: 2.675823]\n",
      "epoch:25 step:19857 [D loss: 0.378868, acc.: 82.81%] [G loss: 3.450025]\n",
      "epoch:25 step:19858 [D loss: 0.369580, acc.: 83.59%] [G loss: 3.197307]\n",
      "epoch:25 step:19859 [D loss: 0.305962, acc.: 83.59%] [G loss: 2.497823]\n",
      "epoch:25 step:19860 [D loss: 0.315711, acc.: 89.06%] [G loss: 3.021561]\n",
      "epoch:25 step:19861 [D loss: 0.287896, acc.: 85.16%] [G loss: 3.096469]\n",
      "epoch:25 step:19862 [D loss: 0.295718, acc.: 86.72%] [G loss: 2.889177]\n",
      "epoch:25 step:19863 [D loss: 0.253642, acc.: 89.84%] [G loss: 3.038435]\n",
      "epoch:25 step:19864 [D loss: 0.240510, acc.: 91.41%] [G loss: 4.878706]\n",
      "epoch:25 step:19865 [D loss: 0.372816, acc.: 82.03%] [G loss: 3.089371]\n",
      "epoch:25 step:19866 [D loss: 0.205643, acc.: 94.53%] [G loss: 2.739392]\n",
      "epoch:25 step:19867 [D loss: 0.287264, acc.: 87.50%] [G loss: 3.141951]\n",
      "epoch:25 step:19868 [D loss: 0.321727, acc.: 84.38%] [G loss: 4.129305]\n",
      "epoch:25 step:19869 [D loss: 0.258916, acc.: 86.72%] [G loss: 3.785319]\n",
      "epoch:25 step:19870 [D loss: 0.260454, acc.: 87.50%] [G loss: 4.532937]\n",
      "epoch:25 step:19871 [D loss: 0.360669, acc.: 85.16%] [G loss: 2.981194]\n",
      "epoch:25 step:19872 [D loss: 0.289638, acc.: 85.94%] [G loss: 4.143492]\n",
      "epoch:25 step:19873 [D loss: 0.322642, acc.: 82.03%] [G loss: 2.736613]\n",
      "epoch:25 step:19874 [D loss: 0.322998, acc.: 84.38%] [G loss: 4.101125]\n",
      "epoch:25 step:19875 [D loss: 0.344550, acc.: 82.03%] [G loss: 3.379368]\n",
      "epoch:25 step:19876 [D loss: 0.321452, acc.: 85.16%] [G loss: 3.101911]\n",
      "epoch:25 step:19877 [D loss: 0.277943, acc.: 88.28%] [G loss: 3.784096]\n",
      "epoch:25 step:19878 [D loss: 0.201752, acc.: 92.97%] [G loss: 2.591979]\n",
      "epoch:25 step:19879 [D loss: 0.245909, acc.: 89.84%] [G loss: 4.249068]\n",
      "epoch:25 step:19880 [D loss: 0.327661, acc.: 85.16%] [G loss: 3.345977]\n",
      "epoch:25 step:19881 [D loss: 0.286303, acc.: 86.72%] [G loss: 3.113014]\n",
      "epoch:25 step:19882 [D loss: 0.338224, acc.: 83.59%] [G loss: 2.946674]\n",
      "epoch:25 step:19883 [D loss: 0.359552, acc.: 82.03%] [G loss: 2.815139]\n",
      "epoch:25 step:19884 [D loss: 0.316287, acc.: 82.81%] [G loss: 2.372526]\n",
      "epoch:25 step:19885 [D loss: 0.335738, acc.: 85.94%] [G loss: 2.448659]\n",
      "epoch:25 step:19886 [D loss: 0.333551, acc.: 83.59%] [G loss: 2.595899]\n",
      "epoch:25 step:19887 [D loss: 0.401424, acc.: 89.06%] [G loss: 3.091052]\n",
      "epoch:25 step:19888 [D loss: 0.306772, acc.: 84.38%] [G loss: 3.632244]\n",
      "epoch:25 step:19889 [D loss: 0.289242, acc.: 90.62%] [G loss: 3.299298]\n",
      "epoch:25 step:19890 [D loss: 0.382536, acc.: 82.81%] [G loss: 3.853442]\n",
      "epoch:25 step:19891 [D loss: 0.257897, acc.: 86.72%] [G loss: 3.215855]\n",
      "epoch:25 step:19892 [D loss: 0.285437, acc.: 87.50%] [G loss: 3.417130]\n",
      "epoch:25 step:19893 [D loss: 0.245589, acc.: 87.50%] [G loss: 3.605943]\n",
      "epoch:25 step:19894 [D loss: 0.292459, acc.: 87.50%] [G loss: 3.044648]\n",
      "epoch:25 step:19895 [D loss: 0.356606, acc.: 82.81%] [G loss: 2.804101]\n",
      "epoch:25 step:19896 [D loss: 0.299080, acc.: 85.94%] [G loss: 3.595503]\n",
      "epoch:25 step:19897 [D loss: 0.257906, acc.: 87.50%] [G loss: 2.948986]\n",
      "epoch:25 step:19898 [D loss: 0.422747, acc.: 80.47%] [G loss: 4.644184]\n",
      "epoch:25 step:19899 [D loss: 0.443246, acc.: 80.47%] [G loss: 2.984690]\n",
      "epoch:25 step:19900 [D loss: 0.308295, acc.: 85.16%] [G loss: 3.570505]\n",
      "epoch:25 step:19901 [D loss: 0.314578, acc.: 83.59%] [G loss: 2.654083]\n",
      "epoch:25 step:19902 [D loss: 0.257549, acc.: 89.06%] [G loss: 2.801153]\n",
      "epoch:25 step:19903 [D loss: 0.304909, acc.: 87.50%] [G loss: 3.238508]\n",
      "epoch:25 step:19904 [D loss: 0.319348, acc.: 90.62%] [G loss: 2.924306]\n",
      "epoch:25 step:19905 [D loss: 0.307439, acc.: 89.06%] [G loss: 3.554171]\n",
      "epoch:25 step:19906 [D loss: 0.212368, acc.: 90.62%] [G loss: 2.659101]\n",
      "epoch:25 step:19907 [D loss: 0.282298, acc.: 86.72%] [G loss: 2.499936]\n",
      "epoch:25 step:19908 [D loss: 0.241063, acc.: 89.06%] [G loss: 3.151566]\n",
      "epoch:25 step:19909 [D loss: 0.217917, acc.: 91.41%] [G loss: 3.191983]\n",
      "epoch:25 step:19910 [D loss: 0.354930, acc.: 82.03%] [G loss: 3.449913]\n",
      "epoch:25 step:19911 [D loss: 0.437502, acc.: 77.34%] [G loss: 3.007511]\n",
      "epoch:25 step:19912 [D loss: 0.331155, acc.: 82.03%] [G loss: 3.010481]\n",
      "epoch:25 step:19913 [D loss: 0.255024, acc.: 90.62%] [G loss: 4.136131]\n",
      "epoch:25 step:19914 [D loss: 0.399078, acc.: 80.47%] [G loss: 3.261802]\n",
      "epoch:25 step:19915 [D loss: 0.274125, acc.: 86.72%] [G loss: 3.268698]\n",
      "epoch:25 step:19916 [D loss: 0.369979, acc.: 83.59%] [G loss: 3.271444]\n",
      "epoch:25 step:19917 [D loss: 0.344719, acc.: 79.69%] [G loss: 2.397554]\n",
      "epoch:25 step:19918 [D loss: 0.293460, acc.: 83.59%] [G loss: 2.538886]\n",
      "epoch:25 step:19919 [D loss: 0.496368, acc.: 71.88%] [G loss: 2.769998]\n",
      "epoch:25 step:19920 [D loss: 0.444723, acc.: 76.56%] [G loss: 2.753946]\n",
      "epoch:25 step:19921 [D loss: 0.319434, acc.: 84.38%] [G loss: 3.083118]\n",
      "epoch:25 step:19922 [D loss: 0.461386, acc.: 75.78%] [G loss: 3.306082]\n",
      "epoch:25 step:19923 [D loss: 0.561397, acc.: 69.53%] [G loss: 2.490642]\n",
      "epoch:25 step:19924 [D loss: 0.421297, acc.: 78.91%] [G loss: 4.036206]\n",
      "epoch:25 step:19925 [D loss: 0.328763, acc.: 89.06%] [G loss: 2.702963]\n",
      "epoch:25 step:19926 [D loss: 0.273014, acc.: 87.50%] [G loss: 4.751588]\n",
      "epoch:25 step:19927 [D loss: 0.298879, acc.: 87.50%] [G loss: 3.781230]\n",
      "epoch:25 step:19928 [D loss: 0.228629, acc.: 87.50%] [G loss: 4.487122]\n",
      "epoch:25 step:19929 [D loss: 0.302487, acc.: 85.94%] [G loss: 3.339007]\n",
      "epoch:25 step:19930 [D loss: 0.247112, acc.: 89.06%] [G loss: 4.387916]\n",
      "epoch:25 step:19931 [D loss: 0.401747, acc.: 82.81%] [G loss: 2.723210]\n",
      "epoch:25 step:19932 [D loss: 0.297698, acc.: 85.94%] [G loss: 2.750126]\n",
      "epoch:25 step:19933 [D loss: 0.310302, acc.: 85.94%] [G loss: 3.048210]\n",
      "epoch:25 step:19934 [D loss: 0.282615, acc.: 87.50%] [G loss: 2.954515]\n",
      "epoch:25 step:19935 [D loss: 0.286331, acc.: 85.16%] [G loss: 2.982846]\n",
      "epoch:25 step:19936 [D loss: 0.321388, acc.: 85.16%] [G loss: 2.885517]\n",
      "epoch:25 step:19937 [D loss: 0.393941, acc.: 85.16%] [G loss: 3.271155]\n",
      "epoch:25 step:19938 [D loss: 0.309848, acc.: 86.72%] [G loss: 4.593838]\n",
      "epoch:25 step:19939 [D loss: 0.362211, acc.: 82.81%] [G loss: 2.534855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19940 [D loss: 0.281141, acc.: 85.94%] [G loss: 3.553452]\n",
      "epoch:25 step:19941 [D loss: 0.270997, acc.: 85.16%] [G loss: 3.986465]\n",
      "epoch:25 step:19942 [D loss: 0.394019, acc.: 87.50%] [G loss: 3.256525]\n",
      "epoch:25 step:19943 [D loss: 0.393502, acc.: 80.47%] [G loss: 2.969274]\n",
      "epoch:25 step:19944 [D loss: 0.313767, acc.: 83.59%] [G loss: 3.104623]\n",
      "epoch:25 step:19945 [D loss: 0.402557, acc.: 82.03%] [G loss: 2.587883]\n",
      "epoch:25 step:19946 [D loss: 0.356601, acc.: 82.81%] [G loss: 2.627203]\n",
      "epoch:25 step:19947 [D loss: 0.289598, acc.: 86.72%] [G loss: 2.400433]\n",
      "epoch:25 step:19948 [D loss: 0.413524, acc.: 79.69%] [G loss: 2.617856]\n",
      "epoch:25 step:19949 [D loss: 0.359322, acc.: 88.28%] [G loss: 2.560653]\n",
      "epoch:25 step:19950 [D loss: 0.313958, acc.: 85.16%] [G loss: 2.573146]\n",
      "epoch:25 step:19951 [D loss: 0.340017, acc.: 85.16%] [G loss: 2.849597]\n",
      "epoch:25 step:19952 [D loss: 0.371942, acc.: 78.12%] [G loss: 2.809748]\n",
      "epoch:25 step:19953 [D loss: 0.306310, acc.: 82.03%] [G loss: 3.606265]\n",
      "epoch:25 step:19954 [D loss: 0.319568, acc.: 85.16%] [G loss: 2.904806]\n",
      "epoch:25 step:19955 [D loss: 0.337867, acc.: 87.50%] [G loss: 2.896418]\n",
      "epoch:25 step:19956 [D loss: 0.315104, acc.: 83.59%] [G loss: 3.037848]\n",
      "epoch:25 step:19957 [D loss: 0.368302, acc.: 85.16%] [G loss: 3.045116]\n",
      "epoch:25 step:19958 [D loss: 0.270745, acc.: 91.41%] [G loss: 3.571213]\n",
      "epoch:25 step:19959 [D loss: 0.275510, acc.: 87.50%] [G loss: 4.352403]\n",
      "epoch:25 step:19960 [D loss: 0.348511, acc.: 83.59%] [G loss: 5.265204]\n",
      "epoch:25 step:19961 [D loss: 0.423137, acc.: 82.81%] [G loss: 4.379516]\n",
      "epoch:25 step:19962 [D loss: 0.495590, acc.: 84.38%] [G loss: 7.030620]\n",
      "epoch:25 step:19963 [D loss: 0.425649, acc.: 82.03%] [G loss: 7.137516]\n",
      "epoch:25 step:19964 [D loss: 0.841921, acc.: 69.53%] [G loss: 4.346468]\n",
      "epoch:25 step:19965 [D loss: 0.222876, acc.: 93.75%] [G loss: 4.783597]\n",
      "epoch:25 step:19966 [D loss: 0.298378, acc.: 85.16%] [G loss: 3.576737]\n",
      "epoch:25 step:19967 [D loss: 0.238420, acc.: 89.06%] [G loss: 3.461981]\n",
      "epoch:25 step:19968 [D loss: 0.348941, acc.: 87.50%] [G loss: 3.110962]\n",
      "epoch:25 step:19969 [D loss: 0.363553, acc.: 85.16%] [G loss: 4.295176]\n",
      "epoch:25 step:19970 [D loss: 0.217586, acc.: 92.97%] [G loss: 3.068363]\n",
      "epoch:25 step:19971 [D loss: 0.239708, acc.: 89.06%] [G loss: 3.314345]\n",
      "epoch:25 step:19972 [D loss: 0.240339, acc.: 89.84%] [G loss: 2.579974]\n",
      "epoch:25 step:19973 [D loss: 0.346051, acc.: 83.59%] [G loss: 2.526745]\n",
      "epoch:25 step:19974 [D loss: 0.319083, acc.: 82.81%] [G loss: 2.905770]\n",
      "epoch:25 step:19975 [D loss: 0.417836, acc.: 77.34%] [G loss: 2.659978]\n",
      "epoch:25 step:19976 [D loss: 0.226869, acc.: 92.19%] [G loss: 3.072261]\n",
      "epoch:25 step:19977 [D loss: 0.272693, acc.: 89.84%] [G loss: 3.368044]\n",
      "epoch:25 step:19978 [D loss: 0.246666, acc.: 89.84%] [G loss: 3.650365]\n",
      "epoch:25 step:19979 [D loss: 0.341501, acc.: 86.72%] [G loss: 4.354856]\n",
      "epoch:25 step:19980 [D loss: 0.292477, acc.: 86.72%] [G loss: 5.474376]\n",
      "epoch:25 step:19981 [D loss: 0.392086, acc.: 82.03%] [G loss: 4.128156]\n",
      "epoch:25 step:19982 [D loss: 0.525535, acc.: 71.88%] [G loss: 2.686263]\n",
      "epoch:25 step:19983 [D loss: 0.514309, acc.: 78.91%] [G loss: 4.175933]\n",
      "epoch:25 step:19984 [D loss: 0.465226, acc.: 85.94%] [G loss: 2.925391]\n",
      "epoch:25 step:19985 [D loss: 0.340091, acc.: 83.59%] [G loss: 3.548096]\n",
      "epoch:25 step:19986 [D loss: 0.259257, acc.: 89.06%] [G loss: 3.052114]\n",
      "epoch:25 step:19987 [D loss: 0.248891, acc.: 89.84%] [G loss: 3.589179]\n",
      "epoch:25 step:19988 [D loss: 0.354002, acc.: 82.81%] [G loss: 3.106494]\n",
      "epoch:25 step:19989 [D loss: 0.284782, acc.: 88.28%] [G loss: 3.072313]\n",
      "epoch:25 step:19990 [D loss: 0.389476, acc.: 83.59%] [G loss: 4.655900]\n",
      "epoch:25 step:19991 [D loss: 0.411484, acc.: 82.03%] [G loss: 3.333403]\n",
      "epoch:25 step:19992 [D loss: 0.331459, acc.: 85.94%] [G loss: 3.323468]\n",
      "epoch:25 step:19993 [D loss: 0.460308, acc.: 76.56%] [G loss: 3.066849]\n",
      "epoch:25 step:19994 [D loss: 0.339632, acc.: 85.16%] [G loss: 3.025429]\n",
      "epoch:25 step:19995 [D loss: 0.280309, acc.: 89.84%] [G loss: 3.318299]\n",
      "epoch:25 step:19996 [D loss: 0.323330, acc.: 83.59%] [G loss: 3.679266]\n",
      "epoch:25 step:19997 [D loss: 0.297272, acc.: 85.16%] [G loss: 3.730996]\n",
      "epoch:25 step:19998 [D loss: 0.395529, acc.: 81.25%] [G loss: 3.430345]\n",
      "epoch:25 step:19999 [D loss: 0.336626, acc.: 85.16%] [G loss: 3.014852]\n",
      "epoch:25 step:20000 [D loss: 0.401286, acc.: 83.59%] [G loss: 2.679792]\n",
      "epoch:25 step:20001 [D loss: 0.390220, acc.: 78.91%] [G loss: 2.848098]\n",
      "epoch:25 step:20002 [D loss: 0.305942, acc.: 86.72%] [G loss: 2.629720]\n",
      "epoch:25 step:20003 [D loss: 0.319391, acc.: 85.16%] [G loss: 3.416935]\n",
      "epoch:25 step:20004 [D loss: 0.351798, acc.: 88.28%] [G loss: 2.322219]\n",
      "epoch:25 step:20005 [D loss: 0.281081, acc.: 84.38%] [G loss: 3.276391]\n",
      "epoch:25 step:20006 [D loss: 0.416845, acc.: 82.81%] [G loss: 2.525247]\n",
      "epoch:25 step:20007 [D loss: 0.329783, acc.: 85.94%] [G loss: 2.689155]\n",
      "epoch:25 step:20008 [D loss: 0.360723, acc.: 87.50%] [G loss: 2.539289]\n",
      "epoch:25 step:20009 [D loss: 0.346429, acc.: 82.81%] [G loss: 2.779725]\n",
      "epoch:25 step:20010 [D loss: 0.376190, acc.: 85.16%] [G loss: 3.549624]\n",
      "epoch:25 step:20011 [D loss: 0.432121, acc.: 81.25%] [G loss: 3.420816]\n",
      "epoch:25 step:20012 [D loss: 0.337726, acc.: 82.81%] [G loss: 3.076463]\n",
      "epoch:25 step:20013 [D loss: 0.259393, acc.: 87.50%] [G loss: 3.145906]\n",
      "epoch:25 step:20014 [D loss: 0.321134, acc.: 86.72%] [G loss: 2.748167]\n",
      "epoch:25 step:20015 [D loss: 0.340625, acc.: 85.94%] [G loss: 2.306441]\n",
      "epoch:25 step:20016 [D loss: 0.379504, acc.: 82.81%] [G loss: 2.753522]\n",
      "epoch:25 step:20017 [D loss: 0.376045, acc.: 78.12%] [G loss: 4.122169]\n",
      "epoch:25 step:20018 [D loss: 0.261677, acc.: 87.50%] [G loss: 3.203793]\n",
      "epoch:25 step:20019 [D loss: 0.299569, acc.: 87.50%] [G loss: 3.689731]\n",
      "epoch:25 step:20020 [D loss: 0.333609, acc.: 85.16%] [G loss: 3.810591]\n",
      "epoch:25 step:20021 [D loss: 0.349129, acc.: 86.72%] [G loss: 3.101648]\n",
      "epoch:25 step:20022 [D loss: 0.403461, acc.: 85.16%] [G loss: 3.252915]\n",
      "epoch:25 step:20023 [D loss: 0.343775, acc.: 84.38%] [G loss: 3.718163]\n",
      "epoch:25 step:20024 [D loss: 0.339815, acc.: 85.94%] [G loss: 6.109114]\n",
      "epoch:25 step:20025 [D loss: 0.308994, acc.: 84.38%] [G loss: 5.156076]\n",
      "epoch:25 step:20026 [D loss: 0.429274, acc.: 80.47%] [G loss: 4.462480]\n",
      "epoch:25 step:20027 [D loss: 0.310805, acc.: 85.94%] [G loss: 3.133583]\n",
      "epoch:25 step:20028 [D loss: 0.270482, acc.: 89.84%] [G loss: 3.172607]\n",
      "epoch:25 step:20029 [D loss: 0.339009, acc.: 82.03%] [G loss: 3.835237]\n",
      "epoch:25 step:20030 [D loss: 0.295068, acc.: 89.06%] [G loss: 3.940486]\n",
      "epoch:25 step:20031 [D loss: 0.297065, acc.: 89.06%] [G loss: 3.942345]\n",
      "epoch:25 step:20032 [D loss: 0.357262, acc.: 82.03%] [G loss: 3.433360]\n",
      "epoch:25 step:20033 [D loss: 0.389540, acc.: 81.25%] [G loss: 2.580664]\n",
      "epoch:25 step:20034 [D loss: 0.341207, acc.: 85.94%] [G loss: 2.679682]\n",
      "epoch:25 step:20035 [D loss: 0.372185, acc.: 84.38%] [G loss: 2.789577]\n",
      "epoch:25 step:20036 [D loss: 0.309056, acc.: 83.59%] [G loss: 3.308811]\n",
      "epoch:25 step:20037 [D loss: 0.342602, acc.: 83.59%] [G loss: 3.221366]\n",
      "epoch:25 step:20038 [D loss: 0.399932, acc.: 82.03%] [G loss: 5.044896]\n",
      "epoch:25 step:20039 [D loss: 0.642772, acc.: 74.22%] [G loss: 9.316484]\n",
      "epoch:25 step:20040 [D loss: 1.607427, acc.: 58.59%] [G loss: 6.491553]\n",
      "epoch:25 step:20041 [D loss: 2.232425, acc.: 51.56%] [G loss: 7.204926]\n",
      "epoch:25 step:20042 [D loss: 1.542719, acc.: 60.16%] [G loss: 3.887497]\n",
      "epoch:25 step:20043 [D loss: 0.483961, acc.: 80.47%] [G loss: 5.075898]\n",
      "epoch:25 step:20044 [D loss: 0.499172, acc.: 77.34%] [G loss: 3.451016]\n",
      "epoch:25 step:20045 [D loss: 0.726436, acc.: 73.44%] [G loss: 5.987552]\n",
      "epoch:25 step:20046 [D loss: 0.355703, acc.: 85.16%] [G loss: 4.964001]\n",
      "epoch:25 step:20047 [D loss: 0.316988, acc.: 86.72%] [G loss: 4.137439]\n",
      "epoch:25 step:20048 [D loss: 0.301187, acc.: 87.50%] [G loss: 4.031485]\n",
      "epoch:25 step:20049 [D loss: 0.350610, acc.: 78.12%] [G loss: 2.839526]\n",
      "epoch:25 step:20050 [D loss: 0.328345, acc.: 85.94%] [G loss: 3.582493]\n",
      "epoch:25 step:20051 [D loss: 0.470455, acc.: 80.47%] [G loss: 2.614630]\n",
      "epoch:25 step:20052 [D loss: 0.294954, acc.: 85.94%] [G loss: 3.115142]\n",
      "epoch:25 step:20053 [D loss: 0.254723, acc.: 90.62%] [G loss: 2.747119]\n",
      "epoch:25 step:20054 [D loss: 0.360910, acc.: 82.81%] [G loss: 2.380115]\n",
      "epoch:25 step:20055 [D loss: 0.393410, acc.: 85.16%] [G loss: 3.226901]\n",
      "epoch:25 step:20056 [D loss: 0.311103, acc.: 86.72%] [G loss: 2.278229]\n",
      "epoch:25 step:20057 [D loss: 0.383013, acc.: 86.72%] [G loss: 2.213086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:20058 [D loss: 0.294056, acc.: 89.06%] [G loss: 2.534341]\n",
      "epoch:25 step:20059 [D loss: 0.252145, acc.: 87.50%] [G loss: 2.851953]\n",
      "epoch:25 step:20060 [D loss: 0.352510, acc.: 84.38%] [G loss: 3.051128]\n",
      "epoch:25 step:20061 [D loss: 0.228235, acc.: 94.53%] [G loss: 3.036783]\n",
      "epoch:25 step:20062 [D loss: 0.311038, acc.: 84.38%] [G loss: 5.476870]\n",
      "epoch:25 step:20063 [D loss: 0.313214, acc.: 85.94%] [G loss: 3.437134]\n",
      "epoch:25 step:20064 [D loss: 0.290214, acc.: 83.59%] [G loss: 2.484990]\n",
      "epoch:25 step:20065 [D loss: 0.252961, acc.: 89.06%] [G loss: 4.150813]\n",
      "epoch:25 step:20066 [D loss: 0.315670, acc.: 85.94%] [G loss: 2.850513]\n",
      "epoch:25 step:20067 [D loss: 0.295849, acc.: 89.06%] [G loss: 2.336154]\n",
      "epoch:25 step:20068 [D loss: 0.410442, acc.: 84.38%] [G loss: 2.235178]\n",
      "epoch:25 step:20069 [D loss: 0.344636, acc.: 82.81%] [G loss: 2.534872]\n",
      "epoch:25 step:20070 [D loss: 0.387953, acc.: 81.25%] [G loss: 2.258846]\n",
      "epoch:25 step:20071 [D loss: 0.331589, acc.: 86.72%] [G loss: 2.764935]\n",
      "epoch:25 step:20072 [D loss: 0.449697, acc.: 78.12%] [G loss: 1.999099]\n",
      "epoch:25 step:20073 [D loss: 0.310192, acc.: 87.50%] [G loss: 2.728995]\n",
      "epoch:25 step:20074 [D loss: 0.447381, acc.: 80.47%] [G loss: 2.430310]\n",
      "epoch:25 step:20075 [D loss: 0.295543, acc.: 87.50%] [G loss: 3.197391]\n",
      "epoch:25 step:20076 [D loss: 0.268285, acc.: 89.84%] [G loss: 3.558209]\n",
      "epoch:25 step:20077 [D loss: 0.332121, acc.: 88.28%] [G loss: 3.643837]\n",
      "epoch:25 step:20078 [D loss: 0.353177, acc.: 83.59%] [G loss: 3.205973]\n",
      "epoch:25 step:20079 [D loss: 0.330430, acc.: 86.72%] [G loss: 3.302640]\n",
      "epoch:25 step:20080 [D loss: 0.369023, acc.: 83.59%] [G loss: 2.790428]\n",
      "epoch:25 step:20081 [D loss: 0.279691, acc.: 87.50%] [G loss: 3.047971]\n",
      "epoch:25 step:20082 [D loss: 0.378942, acc.: 83.59%] [G loss: 2.736892]\n",
      "epoch:25 step:20083 [D loss: 0.390461, acc.: 83.59%] [G loss: 2.528497]\n",
      "epoch:25 step:20084 [D loss: 0.310512, acc.: 89.06%] [G loss: 2.706057]\n",
      "epoch:25 step:20085 [D loss: 0.363748, acc.: 84.38%] [G loss: 2.298705]\n",
      "epoch:25 step:20086 [D loss: 0.359110, acc.: 82.81%] [G loss: 2.383440]\n",
      "epoch:25 step:20087 [D loss: 0.342806, acc.: 81.25%] [G loss: 2.515235]\n",
      "epoch:25 step:20088 [D loss: 0.334404, acc.: 83.59%] [G loss: 2.184265]\n",
      "epoch:25 step:20089 [D loss: 0.323208, acc.: 84.38%] [G loss: 2.624894]\n",
      "epoch:25 step:20090 [D loss: 0.327108, acc.: 89.84%] [G loss: 3.430979]\n",
      "epoch:25 step:20091 [D loss: 0.305522, acc.: 86.72%] [G loss: 3.106873]\n",
      "epoch:25 step:20092 [D loss: 0.324127, acc.: 85.16%] [G loss: 2.848136]\n",
      "epoch:25 step:20093 [D loss: 0.354851, acc.: 83.59%] [G loss: 2.588249]\n",
      "epoch:25 step:20094 [D loss: 0.333053, acc.: 89.84%] [G loss: 2.708190]\n",
      "epoch:25 step:20095 [D loss: 0.374050, acc.: 87.50%] [G loss: 2.271652]\n",
      "epoch:25 step:20096 [D loss: 0.339648, acc.: 86.72%] [G loss: 2.306703]\n",
      "epoch:25 step:20097 [D loss: 0.374184, acc.: 85.94%] [G loss: 2.769421]\n",
      "epoch:25 step:20098 [D loss: 0.314952, acc.: 86.72%] [G loss: 3.712345]\n",
      "epoch:25 step:20099 [D loss: 0.389316, acc.: 85.16%] [G loss: 2.413395]\n",
      "epoch:25 step:20100 [D loss: 0.337987, acc.: 84.38%] [G loss: 2.626550]\n",
      "epoch:25 step:20101 [D loss: 0.350994, acc.: 84.38%] [G loss: 1.966885]\n",
      "epoch:25 step:20102 [D loss: 0.389936, acc.: 84.38%] [G loss: 2.876033]\n",
      "epoch:25 step:20103 [D loss: 0.356735, acc.: 82.81%] [G loss: 2.299772]\n",
      "epoch:25 step:20104 [D loss: 0.366227, acc.: 81.25%] [G loss: 3.562117]\n",
      "epoch:25 step:20105 [D loss: 0.449185, acc.: 78.12%] [G loss: 2.200433]\n",
      "epoch:25 step:20106 [D loss: 0.340645, acc.: 85.94%] [G loss: 3.258341]\n",
      "epoch:25 step:20107 [D loss: 0.346515, acc.: 85.94%] [G loss: 3.199628]\n",
      "epoch:25 step:20108 [D loss: 0.387810, acc.: 83.59%] [G loss: 2.675420]\n",
      "epoch:25 step:20109 [D loss: 0.330887, acc.: 86.72%] [G loss: 2.609262]\n",
      "epoch:25 step:20110 [D loss: 0.290835, acc.: 88.28%] [G loss: 2.273901]\n",
      "epoch:25 step:20111 [D loss: 0.299724, acc.: 87.50%] [G loss: 2.562200]\n",
      "epoch:25 step:20112 [D loss: 0.292943, acc.: 85.94%] [G loss: 2.859695]\n",
      "epoch:25 step:20113 [D loss: 0.311911, acc.: 86.72%] [G loss: 3.284732]\n",
      "epoch:25 step:20114 [D loss: 0.270010, acc.: 87.50%] [G loss: 3.436612]\n",
      "epoch:25 step:20115 [D loss: 0.462429, acc.: 75.78%] [G loss: 3.339355]\n",
      "epoch:25 step:20116 [D loss: 0.336132, acc.: 84.38%] [G loss: 3.573633]\n",
      "epoch:25 step:20117 [D loss: 0.362804, acc.: 85.94%] [G loss: 3.160240]\n",
      "epoch:25 step:20118 [D loss: 0.252518, acc.: 90.62%] [G loss: 4.342120]\n",
      "epoch:25 step:20119 [D loss: 0.247563, acc.: 88.28%] [G loss: 4.524067]\n",
      "epoch:25 step:20120 [D loss: 0.312106, acc.: 84.38%] [G loss: 3.317438]\n",
      "epoch:25 step:20121 [D loss: 0.274105, acc.: 90.62%] [G loss: 4.002731]\n",
      "epoch:25 step:20122 [D loss: 0.355738, acc.: 83.59%] [G loss: 2.966280]\n",
      "epoch:25 step:20123 [D loss: 0.348066, acc.: 84.38%] [G loss: 3.407290]\n",
      "epoch:25 step:20124 [D loss: 0.222404, acc.: 91.41%] [G loss: 3.833793]\n",
      "epoch:25 step:20125 [D loss: 0.342321, acc.: 85.16%] [G loss: 3.469845]\n",
      "epoch:25 step:20126 [D loss: 0.237687, acc.: 89.06%] [G loss: 3.488296]\n",
      "epoch:25 step:20127 [D loss: 0.310761, acc.: 87.50%] [G loss: 3.893146]\n",
      "epoch:25 step:20128 [D loss: 0.449514, acc.: 77.34%] [G loss: 3.928266]\n",
      "epoch:25 step:20129 [D loss: 0.478691, acc.: 75.78%] [G loss: 3.045174]\n",
      "epoch:25 step:20130 [D loss: 0.312013, acc.: 85.16%] [G loss: 4.644045]\n",
      "epoch:25 step:20131 [D loss: 0.337046, acc.: 85.94%] [G loss: 3.429886]\n",
      "epoch:25 step:20132 [D loss: 0.279278, acc.: 88.28%] [G loss: 2.523114]\n",
      "epoch:25 step:20133 [D loss: 0.250632, acc.: 87.50%] [G loss: 3.317145]\n",
      "epoch:25 step:20134 [D loss: 0.291592, acc.: 89.06%] [G loss: 2.615849]\n",
      "epoch:25 step:20135 [D loss: 0.355511, acc.: 82.03%] [G loss: 2.301399]\n",
      "epoch:25 step:20136 [D loss: 0.397294, acc.: 84.38%] [G loss: 2.660501]\n",
      "epoch:25 step:20137 [D loss: 0.466649, acc.: 77.34%] [G loss: 2.574242]\n",
      "epoch:25 step:20138 [D loss: 0.544625, acc.: 67.97%] [G loss: 2.778964]\n",
      "epoch:25 step:20139 [D loss: 0.256157, acc.: 88.28%] [G loss: 4.418314]\n",
      "epoch:25 step:20140 [D loss: 0.365169, acc.: 81.25%] [G loss: 2.987320]\n",
      "epoch:25 step:20141 [D loss: 0.265647, acc.: 85.94%] [G loss: 4.238647]\n",
      "epoch:25 step:20142 [D loss: 0.305389, acc.: 85.16%] [G loss: 3.011393]\n",
      "epoch:25 step:20143 [D loss: 0.346115, acc.: 85.94%] [G loss: 3.317054]\n",
      "epoch:25 step:20144 [D loss: 0.252860, acc.: 87.50%] [G loss: 3.985104]\n",
      "epoch:25 step:20145 [D loss: 0.281736, acc.: 87.50%] [G loss: 4.570241]\n",
      "epoch:25 step:20146 [D loss: 0.269264, acc.: 87.50%] [G loss: 2.916183]\n",
      "epoch:25 step:20147 [D loss: 0.368785, acc.: 82.03%] [G loss: 3.729559]\n",
      "epoch:25 step:20148 [D loss: 0.298708, acc.: 85.94%] [G loss: 3.992423]\n",
      "epoch:25 step:20149 [D loss: 0.295409, acc.: 86.72%] [G loss: 3.802762]\n",
      "epoch:25 step:20150 [D loss: 0.287954, acc.: 86.72%] [G loss: 4.688909]\n",
      "epoch:25 step:20151 [D loss: 0.249201, acc.: 87.50%] [G loss: 3.958198]\n",
      "epoch:25 step:20152 [D loss: 0.298254, acc.: 83.59%] [G loss: 4.602570]\n",
      "epoch:25 step:20153 [D loss: 0.358372, acc.: 81.25%] [G loss: 3.477554]\n",
      "epoch:25 step:20154 [D loss: 0.358824, acc.: 85.94%] [G loss: 3.297536]\n",
      "epoch:25 step:20155 [D loss: 0.310725, acc.: 83.59%] [G loss: 2.282132]\n",
      "epoch:25 step:20156 [D loss: 0.267090, acc.: 89.06%] [G loss: 3.275133]\n",
      "epoch:25 step:20157 [D loss: 0.331578, acc.: 84.38%] [G loss: 2.302258]\n",
      "epoch:25 step:20158 [D loss: 0.371001, acc.: 81.25%] [G loss: 2.805302]\n",
      "epoch:25 step:20159 [D loss: 0.379177, acc.: 78.12%] [G loss: 3.098407]\n",
      "epoch:25 step:20160 [D loss: 0.294579, acc.: 87.50%] [G loss: 2.695490]\n",
      "epoch:25 step:20161 [D loss: 0.301767, acc.: 89.84%] [G loss: 2.560544]\n",
      "epoch:25 step:20162 [D loss: 0.340743, acc.: 85.16%] [G loss: 2.600259]\n",
      "epoch:25 step:20163 [D loss: 0.326529, acc.: 86.72%] [G loss: 3.707167]\n",
      "epoch:25 step:20164 [D loss: 0.435495, acc.: 78.91%] [G loss: 2.918994]\n",
      "epoch:25 step:20165 [D loss: 0.214172, acc.: 90.62%] [G loss: 3.444609]\n",
      "epoch:25 step:20166 [D loss: 0.350951, acc.: 85.16%] [G loss: 2.702165]\n",
      "epoch:25 step:20167 [D loss: 0.319714, acc.: 87.50%] [G loss: 4.582293]\n",
      "epoch:25 step:20168 [D loss: 0.376161, acc.: 82.03%] [G loss: 3.075289]\n",
      "epoch:25 step:20169 [D loss: 0.432672, acc.: 82.03%] [G loss: 3.546196]\n",
      "epoch:25 step:20170 [D loss: 0.390965, acc.: 78.91%] [G loss: 2.065563]\n",
      "epoch:25 step:20171 [D loss: 0.373472, acc.: 83.59%] [G loss: 3.334801]\n",
      "epoch:25 step:20172 [D loss: 0.257964, acc.: 89.06%] [G loss: 3.779618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:20173 [D loss: 0.382070, acc.: 84.38%] [G loss: 3.392573]\n",
      "epoch:25 step:20174 [D loss: 0.495201, acc.: 74.22%] [G loss: 2.783593]\n",
      "epoch:25 step:20175 [D loss: 0.278545, acc.: 87.50%] [G loss: 3.649994]\n",
      "epoch:25 step:20176 [D loss: 0.293352, acc.: 87.50%] [G loss: 2.930944]\n",
      "epoch:25 step:20177 [D loss: 0.328387, acc.: 85.94%] [G loss: 4.062100]\n",
      "epoch:25 step:20178 [D loss: 0.376361, acc.: 87.50%] [G loss: 4.618895]\n",
      "epoch:25 step:20179 [D loss: 0.441396, acc.: 82.81%] [G loss: 4.009739]\n",
      "epoch:25 step:20180 [D loss: 0.446003, acc.: 82.03%] [G loss: 3.622651]\n",
      "epoch:25 step:20181 [D loss: 0.387891, acc.: 81.25%] [G loss: 3.804645]\n",
      "epoch:25 step:20182 [D loss: 0.449489, acc.: 79.69%] [G loss: 2.181749]\n",
      "epoch:25 step:20183 [D loss: 0.361151, acc.: 84.38%] [G loss: 2.775604]\n",
      "epoch:25 step:20184 [D loss: 0.281474, acc.: 90.62%] [G loss: 3.357772]\n",
      "epoch:25 step:20185 [D loss: 0.332055, acc.: 82.81%] [G loss: 3.827199]\n",
      "epoch:25 step:20186 [D loss: 0.339539, acc.: 85.94%] [G loss: 3.429464]\n",
      "epoch:25 step:20187 [D loss: 0.245739, acc.: 89.06%] [G loss: 4.635359]\n",
      "epoch:25 step:20188 [D loss: 0.308776, acc.: 83.59%] [G loss: 5.450300]\n",
      "epoch:25 step:20189 [D loss: 0.178168, acc.: 94.53%] [G loss: 6.242129]\n",
      "epoch:25 step:20190 [D loss: 0.231745, acc.: 89.84%] [G loss: 4.789141]\n",
      "epoch:25 step:20191 [D loss: 0.187540, acc.: 93.75%] [G loss: 3.649773]\n",
      "epoch:25 step:20192 [D loss: 0.278468, acc.: 89.06%] [G loss: 3.670102]\n",
      "epoch:25 step:20193 [D loss: 0.360196, acc.: 85.94%] [G loss: 3.620961]\n",
      "epoch:25 step:20194 [D loss: 0.281483, acc.: 85.94%] [G loss: 3.823572]\n",
      "epoch:25 step:20195 [D loss: 0.374811, acc.: 82.03%] [G loss: 3.411978]\n",
      "epoch:25 step:20196 [D loss: 0.303745, acc.: 85.94%] [G loss: 3.567169]\n",
      "epoch:25 step:20197 [D loss: 0.341126, acc.: 81.25%] [G loss: 2.957779]\n",
      "epoch:25 step:20198 [D loss: 0.336737, acc.: 87.50%] [G loss: 2.676284]\n",
      "epoch:25 step:20199 [D loss: 0.368901, acc.: 86.72%] [G loss: 2.807073]\n",
      "epoch:25 step:20200 [D loss: 0.318374, acc.: 85.94%] [G loss: 2.580300]\n",
      "epoch:25 step:20201 [D loss: 0.263117, acc.: 89.84%] [G loss: 3.260021]\n",
      "epoch:25 step:20202 [D loss: 0.247401, acc.: 93.75%] [G loss: 3.497831]\n",
      "epoch:25 step:20203 [D loss: 0.268672, acc.: 90.62%] [G loss: 3.133685]\n",
      "epoch:25 step:20204 [D loss: 0.282090, acc.: 90.62%] [G loss: 3.181840]\n",
      "epoch:25 step:20205 [D loss: 0.327983, acc.: 84.38%] [G loss: 2.462972]\n",
      "epoch:25 step:20206 [D loss: 0.303290, acc.: 87.50%] [G loss: 2.569255]\n",
      "epoch:25 step:20207 [D loss: 0.353566, acc.: 82.81%] [G loss: 2.664472]\n",
      "epoch:25 step:20208 [D loss: 0.311855, acc.: 86.72%] [G loss: 2.943780]\n",
      "epoch:25 step:20209 [D loss: 0.324116, acc.: 85.16%] [G loss: 3.055585]\n",
      "epoch:25 step:20210 [D loss: 0.388754, acc.: 82.03%] [G loss: 2.439786]\n",
      "epoch:25 step:20211 [D loss: 0.292503, acc.: 85.94%] [G loss: 2.808849]\n",
      "epoch:25 step:20212 [D loss: 0.477968, acc.: 78.12%] [G loss: 2.831838]\n",
      "epoch:25 step:20213 [D loss: 0.354943, acc.: 85.16%] [G loss: 3.119995]\n",
      "epoch:25 step:20214 [D loss: 0.374618, acc.: 80.47%] [G loss: 2.318791]\n",
      "epoch:25 step:20215 [D loss: 0.400942, acc.: 82.03%] [G loss: 2.959418]\n",
      "epoch:25 step:20216 [D loss: 0.247290, acc.: 88.28%] [G loss: 4.392831]\n",
      "epoch:25 step:20217 [D loss: 0.300581, acc.: 83.59%] [G loss: 2.953826]\n",
      "epoch:25 step:20218 [D loss: 0.325470, acc.: 88.28%] [G loss: 2.856774]\n",
      "epoch:25 step:20219 [D loss: 0.362212, acc.: 81.25%] [G loss: 3.066790]\n",
      "epoch:25 step:20220 [D loss: 0.285750, acc.: 85.94%] [G loss: 2.998267]\n",
      "epoch:25 step:20221 [D loss: 0.364657, acc.: 83.59%] [G loss: 2.539834]\n",
      "epoch:25 step:20222 [D loss: 0.366640, acc.: 83.59%] [G loss: 2.880059]\n",
      "epoch:25 step:20223 [D loss: 0.306859, acc.: 87.50%] [G loss: 2.754616]\n",
      "epoch:25 step:20224 [D loss: 0.293535, acc.: 85.16%] [G loss: 2.591189]\n",
      "epoch:25 step:20225 [D loss: 0.305900, acc.: 86.72%] [G loss: 3.705244]\n",
      "epoch:25 step:20226 [D loss: 0.281156, acc.: 85.94%] [G loss: 3.837008]\n",
      "epoch:25 step:20227 [D loss: 0.332412, acc.: 83.59%] [G loss: 3.144979]\n",
      "epoch:25 step:20228 [D loss: 0.339307, acc.: 85.94%] [G loss: 3.033604]\n",
      "epoch:25 step:20229 [D loss: 0.399969, acc.: 82.03%] [G loss: 3.075476]\n",
      "epoch:25 step:20230 [D loss: 0.316220, acc.: 86.72%] [G loss: 3.152251]\n",
      "epoch:25 step:20231 [D loss: 0.426018, acc.: 82.03%] [G loss: 3.537215]\n",
      "epoch:25 step:20232 [D loss: 0.504060, acc.: 75.00%] [G loss: 2.602374]\n",
      "epoch:25 step:20233 [D loss: 0.425149, acc.: 78.91%] [G loss: 3.976720]\n",
      "epoch:25 step:20234 [D loss: 0.440701, acc.: 78.91%] [G loss: 4.461271]\n",
      "epoch:25 step:20235 [D loss: 0.359274, acc.: 81.25%] [G loss: 3.252808]\n",
      "epoch:25 step:20236 [D loss: 0.308722, acc.: 89.06%] [G loss: 3.824966]\n",
      "epoch:25 step:20237 [D loss: 0.340444, acc.: 86.72%] [G loss: 3.179032]\n",
      "epoch:25 step:20238 [D loss: 0.310029, acc.: 85.16%] [G loss: 2.827766]\n",
      "epoch:25 step:20239 [D loss: 0.287099, acc.: 90.62%] [G loss: 3.847308]\n",
      "epoch:25 step:20240 [D loss: 0.333490, acc.: 86.72%] [G loss: 3.393575]\n",
      "epoch:25 step:20241 [D loss: 0.362956, acc.: 84.38%] [G loss: 3.385925]\n",
      "epoch:25 step:20242 [D loss: 0.334407, acc.: 85.94%] [G loss: 3.018365]\n",
      "epoch:25 step:20243 [D loss: 0.314202, acc.: 85.94%] [G loss: 3.006709]\n",
      "epoch:25 step:20244 [D loss: 0.271008, acc.: 86.72%] [G loss: 2.545556]\n",
      "epoch:25 step:20245 [D loss: 0.319781, acc.: 87.50%] [G loss: 3.147955]\n",
      "epoch:25 step:20246 [D loss: 0.309580, acc.: 87.50%] [G loss: 5.070179]\n",
      "epoch:25 step:20247 [D loss: 0.212290, acc.: 91.41%] [G loss: 6.663496]\n",
      "epoch:25 step:20248 [D loss: 0.189923, acc.: 92.19%] [G loss: 4.889737]\n",
      "epoch:25 step:20249 [D loss: 0.249921, acc.: 92.19%] [G loss: 4.196746]\n",
      "epoch:25 step:20250 [D loss: 0.247943, acc.: 88.28%] [G loss: 3.781123]\n",
      "epoch:25 step:20251 [D loss: 0.227467, acc.: 88.28%] [G loss: 5.500611]\n",
      "epoch:25 step:20252 [D loss: 0.337223, acc.: 83.59%] [G loss: 3.474535]\n",
      "epoch:25 step:20253 [D loss: 0.301018, acc.: 88.28%] [G loss: 2.756060]\n",
      "epoch:25 step:20254 [D loss: 0.300697, acc.: 85.16%] [G loss: 3.307135]\n",
      "epoch:25 step:20255 [D loss: 0.217719, acc.: 91.41%] [G loss: 3.628833]\n",
      "epoch:25 step:20256 [D loss: 0.273159, acc.: 89.84%] [G loss: 3.899324]\n",
      "epoch:25 step:20257 [D loss: 0.263586, acc.: 87.50%] [G loss: 3.379857]\n",
      "epoch:25 step:20258 [D loss: 0.338989, acc.: 85.16%] [G loss: 2.971372]\n",
      "epoch:25 step:20259 [D loss: 0.406453, acc.: 80.47%] [G loss: 3.965143]\n",
      "epoch:25 step:20260 [D loss: 0.313746, acc.: 86.72%] [G loss: 2.829736]\n",
      "epoch:25 step:20261 [D loss: 0.351246, acc.: 87.50%] [G loss: 3.119817]\n",
      "epoch:25 step:20262 [D loss: 0.330867, acc.: 87.50%] [G loss: 2.943885]\n",
      "epoch:25 step:20263 [D loss: 0.284652, acc.: 85.94%] [G loss: 2.655891]\n",
      "epoch:25 step:20264 [D loss: 0.447896, acc.: 81.25%] [G loss: 2.883523]\n",
      "epoch:25 step:20265 [D loss: 0.326828, acc.: 83.59%] [G loss: 4.169530]\n",
      "epoch:25 step:20266 [D loss: 0.497558, acc.: 74.22%] [G loss: 3.215315]\n",
      "epoch:25 step:20267 [D loss: 0.302285, acc.: 88.28%] [G loss: 3.016524]\n",
      "epoch:25 step:20268 [D loss: 0.233271, acc.: 87.50%] [G loss: 4.161907]\n",
      "epoch:25 step:20269 [D loss: 0.303171, acc.: 82.81%] [G loss: 2.951201]\n",
      "epoch:25 step:20270 [D loss: 0.277740, acc.: 85.16%] [G loss: 4.383853]\n",
      "epoch:25 step:20271 [D loss: 0.350542, acc.: 82.81%] [G loss: 3.991124]\n",
      "epoch:25 step:20272 [D loss: 0.310796, acc.: 85.16%] [G loss: 3.215839]\n",
      "epoch:25 step:20273 [D loss: 0.353010, acc.: 78.12%] [G loss: 4.494628]\n",
      "epoch:25 step:20274 [D loss: 0.395867, acc.: 79.69%] [G loss: 2.963411]\n",
      "epoch:25 step:20275 [D loss: 0.386357, acc.: 82.03%] [G loss: 3.432702]\n",
      "epoch:25 step:20276 [D loss: 0.358324, acc.: 82.81%] [G loss: 3.285001]\n",
      "epoch:25 step:20277 [D loss: 0.353566, acc.: 85.16%] [G loss: 4.543205]\n",
      "epoch:25 step:20278 [D loss: 0.336547, acc.: 82.03%] [G loss: 6.300472]\n",
      "epoch:25 step:20279 [D loss: 0.328875, acc.: 85.16%] [G loss: 4.325793]\n",
      "epoch:25 step:20280 [D loss: 0.304491, acc.: 85.94%] [G loss: 3.514621]\n",
      "epoch:25 step:20281 [D loss: 0.317681, acc.: 85.94%] [G loss: 2.793178]\n",
      "epoch:25 step:20282 [D loss: 0.265303, acc.: 89.84%] [G loss: 2.715351]\n",
      "epoch:25 step:20283 [D loss: 0.420964, acc.: 76.56%] [G loss: 3.296220]\n",
      "epoch:25 step:20284 [D loss: 0.330731, acc.: 85.94%] [G loss: 4.543599]\n",
      "epoch:25 step:20285 [D loss: 0.364380, acc.: 82.81%] [G loss: 4.724854]\n",
      "epoch:25 step:20286 [D loss: 0.326994, acc.: 82.81%] [G loss: 4.284481]\n",
      "epoch:25 step:20287 [D loss: 0.286244, acc.: 89.84%] [G loss: 3.883494]\n",
      "epoch:25 step:20288 [D loss: 0.324080, acc.: 84.38%] [G loss: 3.772968]\n",
      "epoch:25 step:20289 [D loss: 0.292703, acc.: 84.38%] [G loss: 4.306601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:20290 [D loss: 0.296532, acc.: 87.50%] [G loss: 3.114328]\n",
      "epoch:25 step:20291 [D loss: 0.265568, acc.: 85.16%] [G loss: 3.615022]\n",
      "epoch:25 step:20292 [D loss: 0.274502, acc.: 86.72%] [G loss: 3.575109]\n",
      "epoch:25 step:20293 [D loss: 0.260456, acc.: 87.50%] [G loss: 3.323929]\n",
      "epoch:25 step:20294 [D loss: 0.277865, acc.: 89.06%] [G loss: 3.040136]\n",
      "epoch:25 step:20295 [D loss: 0.352739, acc.: 82.03%] [G loss: 3.468409]\n",
      "epoch:25 step:20296 [D loss: 0.375668, acc.: 82.03%] [G loss: 4.323969]\n",
      "epoch:25 step:20297 [D loss: 0.429969, acc.: 78.91%] [G loss: 2.987792]\n",
      "epoch:25 step:20298 [D loss: 0.366359, acc.: 78.91%] [G loss: 3.657206]\n",
      "epoch:25 step:20299 [D loss: 0.556365, acc.: 70.31%] [G loss: 2.532082]\n",
      "epoch:25 step:20300 [D loss: 0.372095, acc.: 84.38%] [G loss: 2.918335]\n",
      "epoch:25 step:20301 [D loss: 0.308238, acc.: 86.72%] [G loss: 2.883963]\n",
      "epoch:25 step:20302 [D loss: 0.383419, acc.: 80.47%] [G loss: 3.945797]\n",
      "epoch:25 step:20303 [D loss: 0.274807, acc.: 87.50%] [G loss: 3.096883]\n",
      "epoch:25 step:20304 [D loss: 0.373662, acc.: 81.25%] [G loss: 3.412505]\n",
      "epoch:25 step:20305 [D loss: 0.364700, acc.: 82.03%] [G loss: 4.052001]\n",
      "epoch:25 step:20306 [D loss: 0.356300, acc.: 85.16%] [G loss: 6.252073]\n",
      "epoch:26 step:20307 [D loss: 0.322653, acc.: 84.38%] [G loss: 3.979048]\n",
      "epoch:26 step:20308 [D loss: 0.258249, acc.: 92.19%] [G loss: 4.126233]\n",
      "epoch:26 step:20309 [D loss: 0.469749, acc.: 70.31%] [G loss: 3.193734]\n",
      "epoch:26 step:20310 [D loss: 0.181580, acc.: 92.97%] [G loss: 3.491724]\n",
      "epoch:26 step:20311 [D loss: 0.355423, acc.: 85.94%] [G loss: 3.274314]\n",
      "epoch:26 step:20312 [D loss: 0.293812, acc.: 84.38%] [G loss: 3.627225]\n",
      "epoch:26 step:20313 [D loss: 0.269346, acc.: 86.72%] [G loss: 3.564917]\n",
      "epoch:26 step:20314 [D loss: 0.225541, acc.: 92.19%] [G loss: 2.887012]\n",
      "epoch:26 step:20315 [D loss: 0.404886, acc.: 80.47%] [G loss: 2.722070]\n",
      "epoch:26 step:20316 [D loss: 0.425213, acc.: 85.16%] [G loss: 5.224585]\n",
      "epoch:26 step:20317 [D loss: 0.359441, acc.: 85.16%] [G loss: 4.076109]\n",
      "epoch:26 step:20318 [D loss: 0.234880, acc.: 90.62%] [G loss: 3.866388]\n",
      "epoch:26 step:20319 [D loss: 0.341277, acc.: 82.81%] [G loss: 6.125171]\n",
      "epoch:26 step:20320 [D loss: 0.567185, acc.: 75.78%] [G loss: 7.270465]\n",
      "epoch:26 step:20321 [D loss: 0.351824, acc.: 82.03%] [G loss: 6.383414]\n",
      "epoch:26 step:20322 [D loss: 0.242339, acc.: 90.62%] [G loss: 6.473215]\n",
      "epoch:26 step:20323 [D loss: 0.233973, acc.: 92.97%] [G loss: 5.843305]\n",
      "epoch:26 step:20324 [D loss: 0.286936, acc.: 83.59%] [G loss: 4.800741]\n",
      "epoch:26 step:20325 [D loss: 0.215078, acc.: 89.06%] [G loss: 5.891795]\n",
      "epoch:26 step:20326 [D loss: 0.236110, acc.: 93.75%] [G loss: 4.046921]\n",
      "epoch:26 step:20327 [D loss: 0.313787, acc.: 86.72%] [G loss: 4.195469]\n",
      "epoch:26 step:20328 [D loss: 0.323458, acc.: 85.16%] [G loss: 3.404499]\n",
      "epoch:26 step:20329 [D loss: 0.242229, acc.: 88.28%] [G loss: 3.670611]\n",
      "epoch:26 step:20330 [D loss: 0.413240, acc.: 80.47%] [G loss: 3.153792]\n",
      "epoch:26 step:20331 [D loss: 0.344739, acc.: 83.59%] [G loss: 2.746539]\n",
      "epoch:26 step:20332 [D loss: 0.270538, acc.: 84.38%] [G loss: 3.179139]\n",
      "epoch:26 step:20333 [D loss: 0.339606, acc.: 85.16%] [G loss: 3.096765]\n",
      "epoch:26 step:20334 [D loss: 0.446801, acc.: 79.69%] [G loss: 3.530366]\n",
      "epoch:26 step:20335 [D loss: 0.386383, acc.: 82.81%] [G loss: 2.706183]\n",
      "epoch:26 step:20336 [D loss: 0.225358, acc.: 92.97%] [G loss: 4.714672]\n",
      "epoch:26 step:20337 [D loss: 0.374799, acc.: 82.81%] [G loss: 3.690568]\n",
      "epoch:26 step:20338 [D loss: 0.364781, acc.: 81.25%] [G loss: 3.568241]\n",
      "epoch:26 step:20339 [D loss: 0.305460, acc.: 85.16%] [G loss: 2.942464]\n",
      "epoch:26 step:20340 [D loss: 0.315670, acc.: 85.16%] [G loss: 5.215784]\n",
      "epoch:26 step:20341 [D loss: 0.247913, acc.: 92.19%] [G loss: 2.501600]\n",
      "epoch:26 step:20342 [D loss: 0.316019, acc.: 86.72%] [G loss: 3.226862]\n",
      "epoch:26 step:20343 [D loss: 0.391842, acc.: 81.25%] [G loss: 2.530396]\n",
      "epoch:26 step:20344 [D loss: 0.340267, acc.: 85.94%] [G loss: 2.342005]\n",
      "epoch:26 step:20345 [D loss: 0.281794, acc.: 87.50%] [G loss: 2.640664]\n",
      "epoch:26 step:20346 [D loss: 0.340537, acc.: 85.16%] [G loss: 2.863308]\n",
      "epoch:26 step:20347 [D loss: 0.259215, acc.: 86.72%] [G loss: 3.743470]\n",
      "epoch:26 step:20348 [D loss: 0.252457, acc.: 89.84%] [G loss: 2.916741]\n",
      "epoch:26 step:20349 [D loss: 0.322199, acc.: 87.50%] [G loss: 3.560157]\n",
      "epoch:26 step:20350 [D loss: 0.273629, acc.: 89.06%] [G loss: 3.178631]\n",
      "epoch:26 step:20351 [D loss: 0.281287, acc.: 85.94%] [G loss: 3.488121]\n",
      "epoch:26 step:20352 [D loss: 0.352101, acc.: 85.94%] [G loss: 2.668809]\n",
      "epoch:26 step:20353 [D loss: 0.299054, acc.: 85.94%] [G loss: 2.875638]\n",
      "epoch:26 step:20354 [D loss: 0.258421, acc.: 93.75%] [G loss: 2.828400]\n",
      "epoch:26 step:20355 [D loss: 0.242869, acc.: 87.50%] [G loss: 4.019370]\n",
      "epoch:26 step:20356 [D loss: 0.380891, acc.: 82.03%] [G loss: 2.792504]\n",
      "epoch:26 step:20357 [D loss: 0.302441, acc.: 86.72%] [G loss: 2.943916]\n",
      "epoch:26 step:20358 [D loss: 0.283565, acc.: 89.06%] [G loss: 2.445317]\n",
      "epoch:26 step:20359 [D loss: 0.317535, acc.: 87.50%] [G loss: 4.331557]\n",
      "epoch:26 step:20360 [D loss: 0.380907, acc.: 82.03%] [G loss: 2.651806]\n",
      "epoch:26 step:20361 [D loss: 0.256343, acc.: 90.62%] [G loss: 2.847754]\n",
      "epoch:26 step:20362 [D loss: 0.351076, acc.: 86.72%] [G loss: 2.758147]\n",
      "epoch:26 step:20363 [D loss: 0.308841, acc.: 88.28%] [G loss: 2.465712]\n",
      "epoch:26 step:20364 [D loss: 0.431275, acc.: 78.91%] [G loss: 3.388256]\n",
      "epoch:26 step:20365 [D loss: 0.592773, acc.: 71.88%] [G loss: 5.042572]\n",
      "epoch:26 step:20366 [D loss: 0.602922, acc.: 78.12%] [G loss: 7.659575]\n",
      "epoch:26 step:20367 [D loss: 0.733638, acc.: 74.22%] [G loss: 7.687567]\n",
      "epoch:26 step:20368 [D loss: 1.266613, acc.: 61.72%] [G loss: 4.539321]\n",
      "epoch:26 step:20369 [D loss: 0.841760, acc.: 66.41%] [G loss: 2.928694]\n",
      "epoch:26 step:20370 [D loss: 0.490846, acc.: 80.47%] [G loss: 3.810337]\n",
      "epoch:26 step:20371 [D loss: 0.391392, acc.: 80.47%] [G loss: 2.779038]\n",
      "epoch:26 step:20372 [D loss: 0.362164, acc.: 82.81%] [G loss: 2.682369]\n",
      "epoch:26 step:20373 [D loss: 0.382137, acc.: 85.16%] [G loss: 3.605095]\n",
      "epoch:26 step:20374 [D loss: 0.345118, acc.: 83.59%] [G loss: 3.202561]\n",
      "epoch:26 step:20375 [D loss: 0.402600, acc.: 82.03%] [G loss: 2.459395]\n",
      "epoch:26 step:20376 [D loss: 0.300764, acc.: 87.50%] [G loss: 2.717861]\n",
      "epoch:26 step:20377 [D loss: 0.445824, acc.: 82.03%] [G loss: 3.808291]\n",
      "epoch:26 step:20378 [D loss: 0.337495, acc.: 84.38%] [G loss: 2.799918]\n",
      "epoch:26 step:20379 [D loss: 0.483444, acc.: 77.34%] [G loss: 2.681384]\n",
      "epoch:26 step:20380 [D loss: 0.367867, acc.: 82.03%] [G loss: 3.672683]\n",
      "epoch:26 step:20381 [D loss: 0.387582, acc.: 83.59%] [G loss: 2.507324]\n",
      "epoch:26 step:20382 [D loss: 0.248258, acc.: 85.94%] [G loss: 4.516294]\n",
      "epoch:26 step:20383 [D loss: 0.304016, acc.: 88.28%] [G loss: 4.376772]\n",
      "epoch:26 step:20384 [D loss: 0.265504, acc.: 86.72%] [G loss: 4.832365]\n",
      "epoch:26 step:20385 [D loss: 0.383756, acc.: 82.81%] [G loss: 3.544742]\n",
      "epoch:26 step:20386 [D loss: 0.254691, acc.: 90.62%] [G loss: 3.200736]\n",
      "epoch:26 step:20387 [D loss: 0.283575, acc.: 84.38%] [G loss: 3.578487]\n",
      "epoch:26 step:20388 [D loss: 0.221354, acc.: 91.41%] [G loss: 2.743119]\n",
      "epoch:26 step:20389 [D loss: 0.237657, acc.: 90.62%] [G loss: 3.083846]\n",
      "epoch:26 step:20390 [D loss: 0.237932, acc.: 91.41%] [G loss: 4.050739]\n",
      "epoch:26 step:20391 [D loss: 0.209806, acc.: 92.19%] [G loss: 3.601201]\n",
      "epoch:26 step:20392 [D loss: 0.290826, acc.: 87.50%] [G loss: 4.495194]\n",
      "epoch:26 step:20393 [D loss: 0.251595, acc.: 87.50%] [G loss: 3.787986]\n",
      "epoch:26 step:20394 [D loss: 0.269469, acc.: 85.16%] [G loss: 3.926447]\n",
      "epoch:26 step:20395 [D loss: 0.377135, acc.: 82.81%] [G loss: 3.227215]\n",
      "epoch:26 step:20396 [D loss: 0.328420, acc.: 78.91%] [G loss: 2.953040]\n",
      "epoch:26 step:20397 [D loss: 0.272043, acc.: 90.62%] [G loss: 2.921921]\n",
      "epoch:26 step:20398 [D loss: 0.297508, acc.: 89.06%] [G loss: 2.429934]\n",
      "epoch:26 step:20399 [D loss: 0.306603, acc.: 87.50%] [G loss: 3.052505]\n",
      "epoch:26 step:20400 [D loss: 0.292795, acc.: 90.62%] [G loss: 3.082397]\n",
      "epoch:26 step:20401 [D loss: 0.261627, acc.: 88.28%] [G loss: 2.272223]\n",
      "epoch:26 step:20402 [D loss: 0.380751, acc.: 81.25%] [G loss: 3.156259]\n",
      "epoch:26 step:20403 [D loss: 0.243698, acc.: 89.84%] [G loss: 2.537425]\n",
      "epoch:26 step:20404 [D loss: 0.338350, acc.: 83.59%] [G loss: 3.251051]\n",
      "epoch:26 step:20405 [D loss: 0.398000, acc.: 81.25%] [G loss: 2.887429]\n",
      "epoch:26 step:20406 [D loss: 0.351277, acc.: 82.81%] [G loss: 4.228284]\n",
      "epoch:26 step:20407 [D loss: 0.325002, acc.: 84.38%] [G loss: 2.917077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20408 [D loss: 0.346878, acc.: 82.81%] [G loss: 2.757310]\n",
      "epoch:26 step:20409 [D loss: 0.420526, acc.: 79.69%] [G loss: 3.080438]\n",
      "epoch:26 step:20410 [D loss: 0.284752, acc.: 89.84%] [G loss: 2.877448]\n",
      "epoch:26 step:20411 [D loss: 0.287988, acc.: 87.50%] [G loss: 2.196066]\n",
      "epoch:26 step:20412 [D loss: 0.271150, acc.: 88.28%] [G loss: 2.501413]\n",
      "epoch:26 step:20413 [D loss: 0.377736, acc.: 82.03%] [G loss: 2.632338]\n",
      "epoch:26 step:20414 [D loss: 0.307747, acc.: 86.72%] [G loss: 2.797421]\n",
      "epoch:26 step:20415 [D loss: 0.375017, acc.: 85.16%] [G loss: 4.089897]\n",
      "epoch:26 step:20416 [D loss: 0.386667, acc.: 82.03%] [G loss: 2.760408]\n",
      "epoch:26 step:20417 [D loss: 0.285854, acc.: 89.06%] [G loss: 3.097073]\n",
      "epoch:26 step:20418 [D loss: 0.295593, acc.: 87.50%] [G loss: 3.454141]\n",
      "epoch:26 step:20419 [D loss: 0.265130, acc.: 89.06%] [G loss: 2.867851]\n",
      "epoch:26 step:20420 [D loss: 0.322287, acc.: 86.72%] [G loss: 3.592453]\n",
      "epoch:26 step:20421 [D loss: 0.287539, acc.: 85.94%] [G loss: 2.605684]\n",
      "epoch:26 step:20422 [D loss: 0.223694, acc.: 92.19%] [G loss: 3.043066]\n",
      "epoch:26 step:20423 [D loss: 0.357644, acc.: 83.59%] [G loss: 2.438603]\n",
      "epoch:26 step:20424 [D loss: 0.293167, acc.: 85.16%] [G loss: 2.602823]\n",
      "epoch:26 step:20425 [D loss: 0.263055, acc.: 89.06%] [G loss: 3.873396]\n",
      "epoch:26 step:20426 [D loss: 0.362731, acc.: 84.38%] [G loss: 2.672811]\n",
      "epoch:26 step:20427 [D loss: 0.356519, acc.: 82.81%] [G loss: 2.776380]\n",
      "epoch:26 step:20428 [D loss: 0.220313, acc.: 92.97%] [G loss: 2.801871]\n",
      "epoch:26 step:20429 [D loss: 0.329842, acc.: 85.16%] [G loss: 2.406493]\n",
      "epoch:26 step:20430 [D loss: 0.308414, acc.: 88.28%] [G loss: 3.058626]\n",
      "epoch:26 step:20431 [D loss: 0.387857, acc.: 82.81%] [G loss: 2.283240]\n",
      "epoch:26 step:20432 [D loss: 0.395859, acc.: 83.59%] [G loss: 2.845251]\n",
      "epoch:26 step:20433 [D loss: 0.314671, acc.: 85.94%] [G loss: 2.725780]\n",
      "epoch:26 step:20434 [D loss: 0.284120, acc.: 89.06%] [G loss: 2.601869]\n",
      "epoch:26 step:20435 [D loss: 0.383679, acc.: 82.03%] [G loss: 4.449285]\n",
      "epoch:26 step:20436 [D loss: 0.450605, acc.: 82.81%] [G loss: 7.277987]\n",
      "epoch:26 step:20437 [D loss: 0.966114, acc.: 75.78%] [G loss: 7.787527]\n",
      "epoch:26 step:20438 [D loss: 1.891861, acc.: 53.12%] [G loss: 2.628244]\n",
      "epoch:26 step:20439 [D loss: 0.264153, acc.: 89.06%] [G loss: 6.124794]\n",
      "epoch:26 step:20440 [D loss: 0.385680, acc.: 82.81%] [G loss: 3.879252]\n",
      "epoch:26 step:20441 [D loss: 0.435032, acc.: 81.25%] [G loss: 4.581446]\n",
      "epoch:26 step:20442 [D loss: 0.252085, acc.: 88.28%] [G loss: 4.669501]\n",
      "epoch:26 step:20443 [D loss: 0.379612, acc.: 85.16%] [G loss: 2.797699]\n",
      "epoch:26 step:20444 [D loss: 0.317085, acc.: 85.16%] [G loss: 5.241062]\n",
      "epoch:26 step:20445 [D loss: 0.317838, acc.: 86.72%] [G loss: 2.972870]\n",
      "epoch:26 step:20446 [D loss: 0.342641, acc.: 82.81%] [G loss: 3.026262]\n",
      "epoch:26 step:20447 [D loss: 0.340786, acc.: 87.50%] [G loss: 3.395323]\n",
      "epoch:26 step:20448 [D loss: 0.303740, acc.: 85.16%] [G loss: 3.268195]\n",
      "epoch:26 step:20449 [D loss: 0.373755, acc.: 83.59%] [G loss: 3.356561]\n",
      "epoch:26 step:20450 [D loss: 0.238425, acc.: 92.19%] [G loss: 3.159176]\n",
      "epoch:26 step:20451 [D loss: 0.255569, acc.: 90.62%] [G loss: 2.685292]\n",
      "epoch:26 step:20452 [D loss: 0.391687, acc.: 79.69%] [G loss: 2.362002]\n",
      "epoch:26 step:20453 [D loss: 0.369489, acc.: 83.59%] [G loss: 3.437252]\n",
      "epoch:26 step:20454 [D loss: 0.344382, acc.: 85.16%] [G loss: 2.982129]\n",
      "epoch:26 step:20455 [D loss: 0.385694, acc.: 84.38%] [G loss: 2.445065]\n",
      "epoch:26 step:20456 [D loss: 0.372628, acc.: 82.81%] [G loss: 5.429888]\n",
      "epoch:26 step:20457 [D loss: 0.284105, acc.: 85.16%] [G loss: 3.958931]\n",
      "epoch:26 step:20458 [D loss: 0.348055, acc.: 82.03%] [G loss: 5.601600]\n",
      "epoch:26 step:20459 [D loss: 0.237339, acc.: 89.84%] [G loss: 5.169976]\n",
      "epoch:26 step:20460 [D loss: 0.399406, acc.: 84.38%] [G loss: 3.497673]\n",
      "epoch:26 step:20461 [D loss: 0.310899, acc.: 87.50%] [G loss: 5.030846]\n",
      "epoch:26 step:20462 [D loss: 0.396684, acc.: 80.47%] [G loss: 2.509197]\n",
      "epoch:26 step:20463 [D loss: 0.344803, acc.: 84.38%] [G loss: 3.431607]\n",
      "epoch:26 step:20464 [D loss: 0.449586, acc.: 76.56%] [G loss: 2.468401]\n",
      "epoch:26 step:20465 [D loss: 0.239060, acc.: 90.62%] [G loss: 3.493296]\n",
      "epoch:26 step:20466 [D loss: 0.365379, acc.: 81.25%] [G loss: 3.329015]\n",
      "epoch:26 step:20467 [D loss: 0.404280, acc.: 82.81%] [G loss: 2.539140]\n",
      "epoch:26 step:20468 [D loss: 0.249234, acc.: 91.41%] [G loss: 3.098741]\n",
      "epoch:26 step:20469 [D loss: 0.331970, acc.: 85.16%] [G loss: 2.202108]\n",
      "epoch:26 step:20470 [D loss: 0.298511, acc.: 85.94%] [G loss: 2.748618]\n",
      "epoch:26 step:20471 [D loss: 0.315255, acc.: 88.28%] [G loss: 2.777663]\n",
      "epoch:26 step:20472 [D loss: 0.325508, acc.: 83.59%] [G loss: 3.371317]\n",
      "epoch:26 step:20473 [D loss: 0.251322, acc.: 92.19%] [G loss: 2.782674]\n",
      "epoch:26 step:20474 [D loss: 0.362732, acc.: 86.72%] [G loss: 2.479816]\n",
      "epoch:26 step:20475 [D loss: 0.279389, acc.: 90.62%] [G loss: 2.567283]\n",
      "epoch:26 step:20476 [D loss: 0.359190, acc.: 83.59%] [G loss: 2.534865]\n",
      "epoch:26 step:20477 [D loss: 0.317728, acc.: 84.38%] [G loss: 2.310080]\n",
      "epoch:26 step:20478 [D loss: 0.313376, acc.: 85.94%] [G loss: 2.454507]\n",
      "epoch:26 step:20479 [D loss: 0.346735, acc.: 83.59%] [G loss: 2.456076]\n",
      "epoch:26 step:20480 [D loss: 0.388802, acc.: 82.03%] [G loss: 2.577647]\n",
      "epoch:26 step:20481 [D loss: 0.313110, acc.: 87.50%] [G loss: 2.665688]\n",
      "epoch:26 step:20482 [D loss: 0.337860, acc.: 83.59%] [G loss: 2.669207]\n",
      "epoch:26 step:20483 [D loss: 0.317939, acc.: 86.72%] [G loss: 2.714128]\n",
      "epoch:26 step:20484 [D loss: 0.301889, acc.: 86.72%] [G loss: 2.508361]\n",
      "epoch:26 step:20485 [D loss: 0.337431, acc.: 82.81%] [G loss: 2.966894]\n",
      "epoch:26 step:20486 [D loss: 0.374641, acc.: 82.81%] [G loss: 2.571309]\n",
      "epoch:26 step:20487 [D loss: 0.290893, acc.: 87.50%] [G loss: 2.670352]\n",
      "epoch:26 step:20488 [D loss: 0.416646, acc.: 81.25%] [G loss: 2.574742]\n",
      "epoch:26 step:20489 [D loss: 0.303178, acc.: 85.16%] [G loss: 2.876551]\n",
      "epoch:26 step:20490 [D loss: 0.382160, acc.: 85.94%] [G loss: 3.692509]\n",
      "epoch:26 step:20491 [D loss: 0.343882, acc.: 86.72%] [G loss: 3.405725]\n",
      "epoch:26 step:20492 [D loss: 0.302619, acc.: 85.94%] [G loss: 3.366669]\n",
      "epoch:26 step:20493 [D loss: 0.320626, acc.: 85.16%] [G loss: 3.316369]\n",
      "epoch:26 step:20494 [D loss: 0.350570, acc.: 84.38%] [G loss: 2.974247]\n",
      "epoch:26 step:20495 [D loss: 0.218345, acc.: 92.97%] [G loss: 3.615427]\n",
      "epoch:26 step:20496 [D loss: 0.343516, acc.: 85.16%] [G loss: 2.442498]\n",
      "epoch:26 step:20497 [D loss: 0.314845, acc.: 86.72%] [G loss: 2.788342]\n",
      "epoch:26 step:20498 [D loss: 0.243569, acc.: 85.94%] [G loss: 3.210177]\n",
      "epoch:26 step:20499 [D loss: 0.216830, acc.: 91.41%] [G loss: 3.115323]\n",
      "epoch:26 step:20500 [D loss: 0.257658, acc.: 88.28%] [G loss: 3.316506]\n",
      "epoch:26 step:20501 [D loss: 0.301431, acc.: 85.94%] [G loss: 3.146099]\n",
      "epoch:26 step:20502 [D loss: 0.365160, acc.: 83.59%] [G loss: 2.638856]\n",
      "epoch:26 step:20503 [D loss: 0.407180, acc.: 81.25%] [G loss: 2.593105]\n",
      "epoch:26 step:20504 [D loss: 0.310624, acc.: 85.16%] [G loss: 2.461841]\n",
      "epoch:26 step:20505 [D loss: 0.372275, acc.: 83.59%] [G loss: 2.580065]\n",
      "epoch:26 step:20506 [D loss: 0.234028, acc.: 91.41%] [G loss: 3.394488]\n",
      "epoch:26 step:20507 [D loss: 0.422376, acc.: 80.47%] [G loss: 4.634334]\n",
      "epoch:26 step:20508 [D loss: 0.387900, acc.: 83.59%] [G loss: 3.921262]\n",
      "epoch:26 step:20509 [D loss: 0.205616, acc.: 91.41%] [G loss: 4.191227]\n",
      "epoch:26 step:20510 [D loss: 0.382319, acc.: 82.81%] [G loss: 3.541289]\n",
      "epoch:26 step:20511 [D loss: 0.286267, acc.: 84.38%] [G loss: 2.929931]\n",
      "epoch:26 step:20512 [D loss: 0.404365, acc.: 80.47%] [G loss: 2.773973]\n",
      "epoch:26 step:20513 [D loss: 0.288797, acc.: 88.28%] [G loss: 3.656976]\n",
      "epoch:26 step:20514 [D loss: 0.421424, acc.: 79.69%] [G loss: 2.759091]\n",
      "epoch:26 step:20515 [D loss: 0.311127, acc.: 85.94%] [G loss: 3.370965]\n",
      "epoch:26 step:20516 [D loss: 0.394602, acc.: 82.03%] [G loss: 3.242891]\n",
      "epoch:26 step:20517 [D loss: 0.428163, acc.: 78.91%] [G loss: 3.296124]\n",
      "epoch:26 step:20518 [D loss: 0.300561, acc.: 88.28%] [G loss: 3.009913]\n",
      "epoch:26 step:20519 [D loss: 0.282071, acc.: 86.72%] [G loss: 4.368782]\n",
      "epoch:26 step:20520 [D loss: 0.467984, acc.: 79.69%] [G loss: 2.906953]\n",
      "epoch:26 step:20521 [D loss: 0.272916, acc.: 89.06%] [G loss: 3.740489]\n",
      "epoch:26 step:20522 [D loss: 0.367869, acc.: 81.25%] [G loss: 3.739262]\n",
      "epoch:26 step:20523 [D loss: 0.301878, acc.: 85.94%] [G loss: 3.217746]\n",
      "epoch:26 step:20524 [D loss: 0.284540, acc.: 85.94%] [G loss: 2.778364]\n",
      "epoch:26 step:20525 [D loss: 0.269901, acc.: 87.50%] [G loss: 2.918301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20526 [D loss: 0.287655, acc.: 87.50%] [G loss: 2.443672]\n",
      "epoch:26 step:20527 [D loss: 0.245801, acc.: 92.97%] [G loss: 2.579539]\n",
      "epoch:26 step:20528 [D loss: 0.306820, acc.: 84.38%] [G loss: 2.760220]\n",
      "epoch:26 step:20529 [D loss: 0.303501, acc.: 86.72%] [G loss: 2.241337]\n",
      "epoch:26 step:20530 [D loss: 0.340435, acc.: 85.94%] [G loss: 2.581278]\n",
      "epoch:26 step:20531 [D loss: 0.346087, acc.: 82.03%] [G loss: 2.249931]\n",
      "epoch:26 step:20532 [D loss: 0.248631, acc.: 90.62%] [G loss: 2.796519]\n",
      "epoch:26 step:20533 [D loss: 0.413846, acc.: 81.25%] [G loss: 2.397591]\n",
      "epoch:26 step:20534 [D loss: 0.420508, acc.: 82.81%] [G loss: 2.842208]\n",
      "epoch:26 step:20535 [D loss: 0.427646, acc.: 74.22%] [G loss: 3.223739]\n",
      "epoch:26 step:20536 [D loss: 0.224922, acc.: 89.06%] [G loss: 4.149786]\n",
      "epoch:26 step:20537 [D loss: 0.311603, acc.: 83.59%] [G loss: 2.539572]\n",
      "epoch:26 step:20538 [D loss: 0.325612, acc.: 85.16%] [G loss: 3.758809]\n",
      "epoch:26 step:20539 [D loss: 0.327528, acc.: 85.94%] [G loss: 2.554224]\n",
      "epoch:26 step:20540 [D loss: 0.385612, acc.: 78.12%] [G loss: 3.791252]\n",
      "epoch:26 step:20541 [D loss: 0.318532, acc.: 85.16%] [G loss: 2.606973]\n",
      "epoch:26 step:20542 [D loss: 0.375808, acc.: 82.03%] [G loss: 3.136342]\n",
      "epoch:26 step:20543 [D loss: 0.373465, acc.: 78.12%] [G loss: 3.117830]\n",
      "epoch:26 step:20544 [D loss: 0.299275, acc.: 82.81%] [G loss: 3.215117]\n",
      "epoch:26 step:20545 [D loss: 0.283652, acc.: 85.94%] [G loss: 3.717589]\n",
      "epoch:26 step:20546 [D loss: 0.269062, acc.: 85.16%] [G loss: 3.936481]\n",
      "epoch:26 step:20547 [D loss: 0.301946, acc.: 85.16%] [G loss: 4.287061]\n",
      "epoch:26 step:20548 [D loss: 0.299556, acc.: 85.16%] [G loss: 3.105218]\n",
      "epoch:26 step:20549 [D loss: 0.330734, acc.: 86.72%] [G loss: 3.352152]\n",
      "epoch:26 step:20550 [D loss: 0.287880, acc.: 88.28%] [G loss: 3.510614]\n",
      "epoch:26 step:20551 [D loss: 0.270116, acc.: 88.28%] [G loss: 3.367260]\n",
      "epoch:26 step:20552 [D loss: 0.330120, acc.: 83.59%] [G loss: 3.049505]\n",
      "epoch:26 step:20553 [D loss: 0.262682, acc.: 88.28%] [G loss: 3.506744]\n",
      "epoch:26 step:20554 [D loss: 0.417417, acc.: 81.25%] [G loss: 2.898761]\n",
      "epoch:26 step:20555 [D loss: 0.351399, acc.: 82.81%] [G loss: 2.535061]\n",
      "epoch:26 step:20556 [D loss: 0.445383, acc.: 79.69%] [G loss: 2.691563]\n",
      "epoch:26 step:20557 [D loss: 0.426767, acc.: 75.78%] [G loss: 2.604234]\n",
      "epoch:26 step:20558 [D loss: 0.285011, acc.: 85.94%] [G loss: 3.101105]\n",
      "epoch:26 step:20559 [D loss: 0.240253, acc.: 90.62%] [G loss: 2.886477]\n",
      "epoch:26 step:20560 [D loss: 0.276593, acc.: 87.50%] [G loss: 2.883946]\n",
      "epoch:26 step:20561 [D loss: 0.438769, acc.: 82.03%] [G loss: 2.561268]\n",
      "epoch:26 step:20562 [D loss: 0.331783, acc.: 85.16%] [G loss: 2.968838]\n",
      "epoch:26 step:20563 [D loss: 0.326035, acc.: 84.38%] [G loss: 3.024485]\n",
      "epoch:26 step:20564 [D loss: 0.317696, acc.: 82.81%] [G loss: 2.727934]\n",
      "epoch:26 step:20565 [D loss: 0.316448, acc.: 84.38%] [G loss: 3.481446]\n",
      "epoch:26 step:20566 [D loss: 0.325078, acc.: 85.16%] [G loss: 3.198233]\n",
      "epoch:26 step:20567 [D loss: 0.336543, acc.: 83.59%] [G loss: 4.190042]\n",
      "epoch:26 step:20568 [D loss: 0.383097, acc.: 78.12%] [G loss: 3.585324]\n",
      "epoch:26 step:20569 [D loss: 0.179572, acc.: 93.75%] [G loss: 4.113765]\n",
      "epoch:26 step:20570 [D loss: 0.391293, acc.: 85.16%] [G loss: 2.631669]\n",
      "epoch:26 step:20571 [D loss: 0.437537, acc.: 82.03%] [G loss: 4.302219]\n",
      "epoch:26 step:20572 [D loss: 0.534047, acc.: 80.47%] [G loss: 5.753787]\n",
      "epoch:26 step:20573 [D loss: 0.712090, acc.: 74.22%] [G loss: 6.997767]\n",
      "epoch:26 step:20574 [D loss: 1.657742, acc.: 59.38%] [G loss: 5.875693]\n",
      "epoch:26 step:20575 [D loss: 1.204853, acc.: 62.50%] [G loss: 5.606595]\n",
      "epoch:26 step:20576 [D loss: 0.860176, acc.: 69.53%] [G loss: 5.013494]\n",
      "epoch:26 step:20577 [D loss: 0.234738, acc.: 86.72%] [G loss: 5.995755]\n",
      "epoch:26 step:20578 [D loss: 0.460171, acc.: 82.81%] [G loss: 4.825894]\n",
      "epoch:26 step:20579 [D loss: 0.548491, acc.: 77.34%] [G loss: 3.076719]\n",
      "epoch:26 step:20580 [D loss: 0.418974, acc.: 81.25%] [G loss: 3.566268]\n",
      "epoch:26 step:20581 [D loss: 0.415208, acc.: 75.00%] [G loss: 3.605967]\n",
      "epoch:26 step:20582 [D loss: 0.598570, acc.: 72.66%] [G loss: 2.649279]\n",
      "epoch:26 step:20583 [D loss: 0.222820, acc.: 92.19%] [G loss: 4.459411]\n",
      "epoch:26 step:20584 [D loss: 0.241412, acc.: 91.41%] [G loss: 5.463421]\n",
      "epoch:26 step:20585 [D loss: 0.373226, acc.: 83.59%] [G loss: 3.785111]\n",
      "epoch:26 step:20586 [D loss: 0.200877, acc.: 94.53%] [G loss: 3.646158]\n",
      "epoch:26 step:20587 [D loss: 0.222842, acc.: 90.62%] [G loss: 3.054913]\n",
      "epoch:26 step:20588 [D loss: 0.343306, acc.: 84.38%] [G loss: 3.187586]\n",
      "epoch:26 step:20589 [D loss: 0.372743, acc.: 83.59%] [G loss: 2.721931]\n",
      "epoch:26 step:20590 [D loss: 0.351768, acc.: 85.16%] [G loss: 2.593835]\n",
      "epoch:26 step:20591 [D loss: 0.371649, acc.: 86.72%] [G loss: 2.603309]\n",
      "epoch:26 step:20592 [D loss: 0.328802, acc.: 84.38%] [G loss: 2.914005]\n",
      "epoch:26 step:20593 [D loss: 0.292002, acc.: 83.59%] [G loss: 2.674941]\n",
      "epoch:26 step:20594 [D loss: 0.432142, acc.: 79.69%] [G loss: 2.234855]\n",
      "epoch:26 step:20595 [D loss: 0.319690, acc.: 89.84%] [G loss: 2.515099]\n",
      "epoch:26 step:20596 [D loss: 0.317247, acc.: 87.50%] [G loss: 2.151905]\n",
      "epoch:26 step:20597 [D loss: 0.305527, acc.: 85.16%] [G loss: 2.282292]\n",
      "epoch:26 step:20598 [D loss: 0.367982, acc.: 82.81%] [G loss: 2.388068]\n",
      "epoch:26 step:20599 [D loss: 0.371233, acc.: 85.16%] [G loss: 2.981656]\n",
      "epoch:26 step:20600 [D loss: 0.400165, acc.: 81.25%] [G loss: 2.022615]\n",
      "epoch:26 step:20601 [D loss: 0.398037, acc.: 82.81%] [G loss: 2.191524]\n",
      "epoch:26 step:20602 [D loss: 0.397299, acc.: 83.59%] [G loss: 3.051204]\n",
      "epoch:26 step:20603 [D loss: 0.279296, acc.: 87.50%] [G loss: 2.918738]\n",
      "epoch:26 step:20604 [D loss: 0.268370, acc.: 89.06%] [G loss: 3.573220]\n",
      "epoch:26 step:20605 [D loss: 0.245828, acc.: 90.62%] [G loss: 3.626173]\n",
      "epoch:26 step:20606 [D loss: 0.243040, acc.: 89.84%] [G loss: 4.033706]\n",
      "epoch:26 step:20607 [D loss: 0.282567, acc.: 85.94%] [G loss: 4.128042]\n",
      "epoch:26 step:20608 [D loss: 0.270296, acc.: 85.94%] [G loss: 3.309377]\n",
      "epoch:26 step:20609 [D loss: 0.308809, acc.: 86.72%] [G loss: 3.681849]\n",
      "epoch:26 step:20610 [D loss: 0.266138, acc.: 87.50%] [G loss: 3.042182]\n",
      "epoch:26 step:20611 [D loss: 0.231643, acc.: 90.62%] [G loss: 3.235880]\n",
      "epoch:26 step:20612 [D loss: 0.326266, acc.: 84.38%] [G loss: 2.669030]\n",
      "epoch:26 step:20613 [D loss: 0.198626, acc.: 94.53%] [G loss: 3.047796]\n",
      "epoch:26 step:20614 [D loss: 0.308137, acc.: 86.72%] [G loss: 2.977492]\n",
      "epoch:26 step:20615 [D loss: 0.271743, acc.: 89.06%] [G loss: 2.884214]\n",
      "epoch:26 step:20616 [D loss: 0.287633, acc.: 85.16%] [G loss: 2.904190]\n",
      "epoch:26 step:20617 [D loss: 0.345688, acc.: 86.72%] [G loss: 3.629721]\n",
      "epoch:26 step:20618 [D loss: 0.367665, acc.: 81.25%] [G loss: 2.912380]\n",
      "epoch:26 step:20619 [D loss: 0.363787, acc.: 83.59%] [G loss: 2.051994]\n",
      "epoch:26 step:20620 [D loss: 0.283761, acc.: 87.50%] [G loss: 2.795950]\n",
      "epoch:26 step:20621 [D loss: 0.438803, acc.: 79.69%] [G loss: 2.320598]\n",
      "epoch:26 step:20622 [D loss: 0.350053, acc.: 85.94%] [G loss: 2.619599]\n",
      "epoch:26 step:20623 [D loss: 0.364753, acc.: 83.59%] [G loss: 2.509660]\n",
      "epoch:26 step:20624 [D loss: 0.320731, acc.: 85.16%] [G loss: 3.236934]\n",
      "epoch:26 step:20625 [D loss: 0.282333, acc.: 89.06%] [G loss: 3.250319]\n",
      "epoch:26 step:20626 [D loss: 0.366164, acc.: 82.81%] [G loss: 2.320987]\n",
      "epoch:26 step:20627 [D loss: 0.409438, acc.: 80.47%] [G loss: 2.663857]\n",
      "epoch:26 step:20628 [D loss: 0.389244, acc.: 78.12%] [G loss: 2.718120]\n",
      "epoch:26 step:20629 [D loss: 0.266366, acc.: 87.50%] [G loss: 3.995780]\n",
      "epoch:26 step:20630 [D loss: 0.383598, acc.: 84.38%] [G loss: 2.790188]\n",
      "epoch:26 step:20631 [D loss: 0.393269, acc.: 79.69%] [G loss: 3.526763]\n",
      "epoch:26 step:20632 [D loss: 0.376065, acc.: 83.59%] [G loss: 2.787041]\n",
      "epoch:26 step:20633 [D loss: 0.286941, acc.: 86.72%] [G loss: 3.432313]\n",
      "epoch:26 step:20634 [D loss: 0.271524, acc.: 87.50%] [G loss: 3.008405]\n",
      "epoch:26 step:20635 [D loss: 0.313114, acc.: 83.59%] [G loss: 2.242011]\n",
      "epoch:26 step:20636 [D loss: 0.281608, acc.: 88.28%] [G loss: 3.358091]\n",
      "epoch:26 step:20637 [D loss: 0.386503, acc.: 84.38%] [G loss: 2.682777]\n",
      "epoch:26 step:20638 [D loss: 0.323752, acc.: 87.50%] [G loss: 3.026124]\n",
      "epoch:26 step:20639 [D loss: 0.334232, acc.: 85.94%] [G loss: 3.631069]\n",
      "epoch:26 step:20640 [D loss: 0.522103, acc.: 73.44%] [G loss: 2.316660]\n",
      "epoch:26 step:20641 [D loss: 0.286733, acc.: 89.06%] [G loss: 2.900832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20642 [D loss: 0.325141, acc.: 86.72%] [G loss: 2.558784]\n",
      "epoch:26 step:20643 [D loss: 0.281857, acc.: 86.72%] [G loss: 2.855756]\n",
      "epoch:26 step:20644 [D loss: 0.286487, acc.: 86.72%] [G loss: 3.178917]\n",
      "epoch:26 step:20645 [D loss: 0.347506, acc.: 82.81%] [G loss: 3.755839]\n",
      "epoch:26 step:20646 [D loss: 0.266052, acc.: 89.06%] [G loss: 2.752595]\n",
      "epoch:26 step:20647 [D loss: 0.331750, acc.: 86.72%] [G loss: 3.552258]\n",
      "epoch:26 step:20648 [D loss: 0.396957, acc.: 82.81%] [G loss: 1.984533]\n",
      "epoch:26 step:20649 [D loss: 0.342148, acc.: 85.94%] [G loss: 2.197454]\n",
      "epoch:26 step:20650 [D loss: 0.244800, acc.: 90.62%] [G loss: 2.520163]\n",
      "epoch:26 step:20651 [D loss: 0.381933, acc.: 84.38%] [G loss: 2.632905]\n",
      "epoch:26 step:20652 [D loss: 0.432766, acc.: 78.91%] [G loss: 3.208066]\n",
      "epoch:26 step:20653 [D loss: 0.366231, acc.: 79.69%] [G loss: 2.767880]\n",
      "epoch:26 step:20654 [D loss: 0.303324, acc.: 86.72%] [G loss: 1.920071]\n",
      "epoch:26 step:20655 [D loss: 0.329661, acc.: 86.72%] [G loss: 2.213295]\n",
      "epoch:26 step:20656 [D loss: 0.379093, acc.: 83.59%] [G loss: 2.410884]\n",
      "epoch:26 step:20657 [D loss: 0.404488, acc.: 82.81%] [G loss: 2.549923]\n",
      "epoch:26 step:20658 [D loss: 0.364883, acc.: 80.47%] [G loss: 2.511461]\n",
      "epoch:26 step:20659 [D loss: 0.250228, acc.: 90.62%] [G loss: 2.853085]\n",
      "epoch:26 step:20660 [D loss: 0.225016, acc.: 92.19%] [G loss: 4.108452]\n",
      "epoch:26 step:20661 [D loss: 0.245853, acc.: 89.06%] [G loss: 4.759537]\n",
      "epoch:26 step:20662 [D loss: 0.322072, acc.: 82.03%] [G loss: 3.334827]\n",
      "epoch:26 step:20663 [D loss: 0.381142, acc.: 82.81%] [G loss: 2.965581]\n",
      "epoch:26 step:20664 [D loss: 0.299390, acc.: 85.16%] [G loss: 3.561350]\n",
      "epoch:26 step:20665 [D loss: 0.307145, acc.: 84.38%] [G loss: 3.630861]\n",
      "epoch:26 step:20666 [D loss: 0.367938, acc.: 82.03%] [G loss: 3.205655]\n",
      "epoch:26 step:20667 [D loss: 0.348515, acc.: 86.72%] [G loss: 4.947262]\n",
      "epoch:26 step:20668 [D loss: 0.254723, acc.: 92.97%] [G loss: 5.189034]\n",
      "epoch:26 step:20669 [D loss: 0.318364, acc.: 85.94%] [G loss: 3.160982]\n",
      "epoch:26 step:20670 [D loss: 0.281722, acc.: 89.06%] [G loss: 4.658500]\n",
      "epoch:26 step:20671 [D loss: 0.244883, acc.: 87.50%] [G loss: 3.463503]\n",
      "epoch:26 step:20672 [D loss: 0.286106, acc.: 83.59%] [G loss: 3.420961]\n",
      "epoch:26 step:20673 [D loss: 0.319698, acc.: 85.94%] [G loss: 3.757571]\n",
      "epoch:26 step:20674 [D loss: 0.348721, acc.: 84.38%] [G loss: 4.084859]\n",
      "epoch:26 step:20675 [D loss: 0.350832, acc.: 83.59%] [G loss: 2.940674]\n",
      "epoch:26 step:20676 [D loss: 0.276068, acc.: 86.72%] [G loss: 2.546670]\n",
      "epoch:26 step:20677 [D loss: 0.284644, acc.: 87.50%] [G loss: 3.806256]\n",
      "epoch:26 step:20678 [D loss: 0.280930, acc.: 89.06%] [G loss: 4.548191]\n",
      "epoch:26 step:20679 [D loss: 0.297492, acc.: 87.50%] [G loss: 4.102261]\n",
      "epoch:26 step:20680 [D loss: 0.404496, acc.: 80.47%] [G loss: 2.697164]\n",
      "epoch:26 step:20681 [D loss: 0.313052, acc.: 85.16%] [G loss: 2.856351]\n",
      "epoch:26 step:20682 [D loss: 0.285355, acc.: 89.84%] [G loss: 3.276360]\n",
      "epoch:26 step:20683 [D loss: 0.300193, acc.: 85.94%] [G loss: 4.357530]\n",
      "epoch:26 step:20684 [D loss: 0.389228, acc.: 81.25%] [G loss: 3.498669]\n",
      "epoch:26 step:20685 [D loss: 0.343722, acc.: 85.16%] [G loss: 3.859595]\n",
      "epoch:26 step:20686 [D loss: 0.286592, acc.: 85.94%] [G loss: 3.192350]\n",
      "epoch:26 step:20687 [D loss: 0.296533, acc.: 89.06%] [G loss: 3.390830]\n",
      "epoch:26 step:20688 [D loss: 0.301434, acc.: 86.72%] [G loss: 3.469478]\n",
      "epoch:26 step:20689 [D loss: 0.369356, acc.: 83.59%] [G loss: 4.079723]\n",
      "epoch:26 step:20690 [D loss: 0.257870, acc.: 87.50%] [G loss: 3.420933]\n",
      "epoch:26 step:20691 [D loss: 0.279960, acc.: 88.28%] [G loss: 3.127609]\n",
      "epoch:26 step:20692 [D loss: 0.358033, acc.: 82.81%] [G loss: 2.864096]\n",
      "epoch:26 step:20693 [D loss: 0.320958, acc.: 85.94%] [G loss: 3.050084]\n",
      "epoch:26 step:20694 [D loss: 0.262085, acc.: 90.62%] [G loss: 2.467491]\n",
      "epoch:26 step:20695 [D loss: 0.296937, acc.: 86.72%] [G loss: 3.530812]\n",
      "epoch:26 step:20696 [D loss: 0.339641, acc.: 85.94%] [G loss: 2.751323]\n",
      "epoch:26 step:20697 [D loss: 0.353554, acc.: 85.16%] [G loss: 3.125587]\n",
      "epoch:26 step:20698 [D loss: 0.253842, acc.: 85.94%] [G loss: 3.857937]\n",
      "epoch:26 step:20699 [D loss: 0.207479, acc.: 91.41%] [G loss: 3.697174]\n",
      "epoch:26 step:20700 [D loss: 0.295305, acc.: 90.62%] [G loss: 4.087395]\n",
      "epoch:26 step:20701 [D loss: 0.343333, acc.: 84.38%] [G loss: 3.349377]\n",
      "epoch:26 step:20702 [D loss: 0.295740, acc.: 87.50%] [G loss: 3.047830]\n",
      "epoch:26 step:20703 [D loss: 0.343371, acc.: 86.72%] [G loss: 3.286778]\n",
      "epoch:26 step:20704 [D loss: 0.232604, acc.: 91.41%] [G loss: 2.799791]\n",
      "epoch:26 step:20705 [D loss: 0.364475, acc.: 82.81%] [G loss: 3.449563]\n",
      "epoch:26 step:20706 [D loss: 0.355538, acc.: 85.94%] [G loss: 2.761191]\n",
      "epoch:26 step:20707 [D loss: 0.332767, acc.: 82.81%] [G loss: 5.065150]\n",
      "epoch:26 step:20708 [D loss: 0.426992, acc.: 80.47%] [G loss: 2.288681]\n",
      "epoch:26 step:20709 [D loss: 0.240995, acc.: 92.19%] [G loss: 3.958208]\n",
      "epoch:26 step:20710 [D loss: 0.346784, acc.: 85.94%] [G loss: 2.720073]\n",
      "epoch:26 step:20711 [D loss: 0.410108, acc.: 80.47%] [G loss: 2.809267]\n",
      "epoch:26 step:20712 [D loss: 0.389366, acc.: 84.38%] [G loss: 2.566462]\n",
      "epoch:26 step:20713 [D loss: 0.333056, acc.: 86.72%] [G loss: 2.959722]\n",
      "epoch:26 step:20714 [D loss: 0.338511, acc.: 83.59%] [G loss: 3.347583]\n",
      "epoch:26 step:20715 [D loss: 0.297606, acc.: 86.72%] [G loss: 2.892985]\n",
      "epoch:26 step:20716 [D loss: 0.417008, acc.: 80.47%] [G loss: 4.977125]\n",
      "epoch:26 step:20717 [D loss: 0.556674, acc.: 79.69%] [G loss: 6.906973]\n",
      "epoch:26 step:20718 [D loss: 1.064512, acc.: 71.09%] [G loss: 9.892023]\n",
      "epoch:26 step:20719 [D loss: 1.657833, acc.: 59.38%] [G loss: 3.564360]\n",
      "epoch:26 step:20720 [D loss: 0.722681, acc.: 67.97%] [G loss: 3.247870]\n",
      "epoch:26 step:20721 [D loss: 0.338237, acc.: 85.16%] [G loss: 3.396216]\n",
      "epoch:26 step:20722 [D loss: 0.338667, acc.: 85.94%] [G loss: 2.773507]\n",
      "epoch:26 step:20723 [D loss: 0.319974, acc.: 85.94%] [G loss: 4.833556]\n",
      "epoch:26 step:20724 [D loss: 0.355048, acc.: 87.50%] [G loss: 2.992045]\n",
      "epoch:26 step:20725 [D loss: 0.364166, acc.: 82.03%] [G loss: 3.791782]\n",
      "epoch:26 step:20726 [D loss: 0.231883, acc.: 89.84%] [G loss: 3.039154]\n",
      "epoch:26 step:20727 [D loss: 0.255098, acc.: 85.94%] [G loss: 3.274502]\n",
      "epoch:26 step:20728 [D loss: 0.370029, acc.: 85.94%] [G loss: 3.243104]\n",
      "epoch:26 step:20729 [D loss: 0.297826, acc.: 87.50%] [G loss: 3.613268]\n",
      "epoch:26 step:20730 [D loss: 0.280985, acc.: 87.50%] [G loss: 3.332870]\n",
      "epoch:26 step:20731 [D loss: 0.425756, acc.: 78.12%] [G loss: 2.504886]\n",
      "epoch:26 step:20732 [D loss: 0.372645, acc.: 83.59%] [G loss: 3.092000]\n",
      "epoch:26 step:20733 [D loss: 0.301077, acc.: 85.16%] [G loss: 3.219864]\n",
      "epoch:26 step:20734 [D loss: 0.472044, acc.: 76.56%] [G loss: 2.786019]\n",
      "epoch:26 step:20735 [D loss: 0.335716, acc.: 85.16%] [G loss: 2.482751]\n",
      "epoch:26 step:20736 [D loss: 0.312603, acc.: 87.50%] [G loss: 2.579243]\n",
      "epoch:26 step:20737 [D loss: 0.304440, acc.: 85.94%] [G loss: 2.431667]\n",
      "epoch:26 step:20738 [D loss: 0.421118, acc.: 79.69%] [G loss: 2.370882]\n",
      "epoch:26 step:20739 [D loss: 0.209039, acc.: 91.41%] [G loss: 2.544956]\n",
      "epoch:26 step:20740 [D loss: 0.342489, acc.: 83.59%] [G loss: 2.713557]\n",
      "epoch:26 step:20741 [D loss: 0.344492, acc.: 86.72%] [G loss: 2.774452]\n",
      "epoch:26 step:20742 [D loss: 0.336166, acc.: 87.50%] [G loss: 2.531925]\n",
      "epoch:26 step:20743 [D loss: 0.299082, acc.: 87.50%] [G loss: 2.270369]\n",
      "epoch:26 step:20744 [D loss: 0.401168, acc.: 82.03%] [G loss: 1.953839]\n",
      "epoch:26 step:20745 [D loss: 0.325742, acc.: 87.50%] [G loss: 3.311929]\n",
      "epoch:26 step:20746 [D loss: 0.293902, acc.: 86.72%] [G loss: 2.662728]\n",
      "epoch:26 step:20747 [D loss: 0.418111, acc.: 79.69%] [G loss: 3.168115]\n",
      "epoch:26 step:20748 [D loss: 0.295629, acc.: 85.94%] [G loss: 3.513399]\n",
      "epoch:26 step:20749 [D loss: 0.379048, acc.: 82.81%] [G loss: 4.437824]\n",
      "epoch:26 step:20750 [D loss: 0.366132, acc.: 78.91%] [G loss: 3.140065]\n",
      "epoch:26 step:20751 [D loss: 0.352823, acc.: 83.59%] [G loss: 2.811735]\n",
      "epoch:26 step:20752 [D loss: 0.289879, acc.: 85.16%] [G loss: 4.475928]\n",
      "epoch:26 step:20753 [D loss: 0.328380, acc.: 85.94%] [G loss: 5.084655]\n",
      "epoch:26 step:20754 [D loss: 0.334777, acc.: 82.81%] [G loss: 5.419743]\n",
      "epoch:26 step:20755 [D loss: 0.260022, acc.: 89.84%] [G loss: 7.790460]\n",
      "epoch:26 step:20756 [D loss: 0.265059, acc.: 86.72%] [G loss: 5.134626]\n",
      "epoch:26 step:20757 [D loss: 0.254879, acc.: 88.28%] [G loss: 4.686885]\n",
      "epoch:26 step:20758 [D loss: 0.240674, acc.: 91.41%] [G loss: 3.980718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20759 [D loss: 0.285948, acc.: 87.50%] [G loss: 3.512322]\n",
      "epoch:26 step:20760 [D loss: 0.336474, acc.: 85.16%] [G loss: 3.866969]\n",
      "epoch:26 step:20761 [D loss: 0.299614, acc.: 88.28%] [G loss: 3.554635]\n",
      "epoch:26 step:20762 [D loss: 0.275847, acc.: 88.28%] [G loss: 3.118855]\n",
      "epoch:26 step:20763 [D loss: 0.290537, acc.: 87.50%] [G loss: 2.789887]\n",
      "epoch:26 step:20764 [D loss: 0.362281, acc.: 77.34%] [G loss: 2.722849]\n",
      "epoch:26 step:20765 [D loss: 0.310167, acc.: 85.16%] [G loss: 2.729377]\n",
      "epoch:26 step:20766 [D loss: 0.414838, acc.: 82.03%] [G loss: 2.949667]\n",
      "epoch:26 step:20767 [D loss: 0.241144, acc.: 89.06%] [G loss: 2.847254]\n",
      "epoch:26 step:20768 [D loss: 0.272442, acc.: 89.84%] [G loss: 3.142157]\n",
      "epoch:26 step:20769 [D loss: 0.345608, acc.: 82.81%] [G loss: 2.613388]\n",
      "epoch:26 step:20770 [D loss: 0.370426, acc.: 81.25%] [G loss: 2.593407]\n",
      "epoch:26 step:20771 [D loss: 0.325112, acc.: 84.38%] [G loss: 2.537745]\n",
      "epoch:26 step:20772 [D loss: 0.433850, acc.: 80.47%] [G loss: 2.635810]\n",
      "epoch:26 step:20773 [D loss: 0.299592, acc.: 82.81%] [G loss: 2.722824]\n",
      "epoch:26 step:20774 [D loss: 0.351206, acc.: 82.03%] [G loss: 2.680849]\n",
      "epoch:26 step:20775 [D loss: 0.253593, acc.: 90.62%] [G loss: 3.809193]\n",
      "epoch:26 step:20776 [D loss: 0.349143, acc.: 85.94%] [G loss: 3.228183]\n",
      "epoch:26 step:20777 [D loss: 0.386447, acc.: 81.25%] [G loss: 3.596342]\n",
      "epoch:26 step:20778 [D loss: 0.356830, acc.: 82.03%] [G loss: 2.706749]\n",
      "epoch:26 step:20779 [D loss: 0.426264, acc.: 78.91%] [G loss: 2.129580]\n",
      "epoch:26 step:20780 [D loss: 0.287993, acc.: 89.06%] [G loss: 2.477354]\n",
      "epoch:26 step:20781 [D loss: 0.363243, acc.: 82.81%] [G loss: 2.809811]\n",
      "epoch:26 step:20782 [D loss: 0.342876, acc.: 84.38%] [G loss: 3.044338]\n",
      "epoch:26 step:20783 [D loss: 0.318572, acc.: 88.28%] [G loss: 2.570876]\n",
      "epoch:26 step:20784 [D loss: 0.229594, acc.: 91.41%] [G loss: 2.590373]\n",
      "epoch:26 step:20785 [D loss: 0.363997, acc.: 84.38%] [G loss: 3.615002]\n",
      "epoch:26 step:20786 [D loss: 0.210430, acc.: 89.84%] [G loss: 2.653934]\n",
      "epoch:26 step:20787 [D loss: 0.296703, acc.: 85.94%] [G loss: 2.506739]\n",
      "epoch:26 step:20788 [D loss: 0.352332, acc.: 83.59%] [G loss: 2.763831]\n",
      "epoch:26 step:20789 [D loss: 0.310786, acc.: 86.72%] [G loss: 2.702693]\n",
      "epoch:26 step:20790 [D loss: 0.430539, acc.: 80.47%] [G loss: 2.418375]\n",
      "epoch:26 step:20791 [D loss: 0.290096, acc.: 87.50%] [G loss: 2.659206]\n",
      "epoch:26 step:20792 [D loss: 0.268700, acc.: 89.84%] [G loss: 3.887932]\n",
      "epoch:26 step:20793 [D loss: 0.209727, acc.: 92.97%] [G loss: 3.844905]\n",
      "epoch:26 step:20794 [D loss: 0.208952, acc.: 90.62%] [G loss: 6.195372]\n",
      "epoch:26 step:20795 [D loss: 0.248698, acc.: 89.06%] [G loss: 4.870016]\n",
      "epoch:26 step:20796 [D loss: 0.187345, acc.: 91.41%] [G loss: 4.928527]\n",
      "epoch:26 step:20797 [D loss: 0.235863, acc.: 89.84%] [G loss: 5.186019]\n",
      "epoch:26 step:20798 [D loss: 0.337855, acc.: 86.72%] [G loss: 4.196305]\n",
      "epoch:26 step:20799 [D loss: 0.297492, acc.: 82.81%] [G loss: 3.633870]\n",
      "epoch:26 step:20800 [D loss: 0.256261, acc.: 89.84%] [G loss: 3.701879]\n",
      "epoch:26 step:20801 [D loss: 0.308481, acc.: 87.50%] [G loss: 2.744258]\n",
      "epoch:26 step:20802 [D loss: 0.312697, acc.: 89.06%] [G loss: 2.739006]\n",
      "epoch:26 step:20803 [D loss: 0.281940, acc.: 87.50%] [G loss: 3.622546]\n",
      "epoch:26 step:20804 [D loss: 0.294103, acc.: 89.06%] [G loss: 3.076683]\n",
      "epoch:26 step:20805 [D loss: 0.382133, acc.: 78.12%] [G loss: 3.645688]\n",
      "epoch:26 step:20806 [D loss: 0.346876, acc.: 85.16%] [G loss: 6.896496]\n",
      "epoch:26 step:20807 [D loss: 0.420632, acc.: 82.03%] [G loss: 6.368132]\n",
      "epoch:26 step:20808 [D loss: 0.417160, acc.: 78.12%] [G loss: 3.686310]\n",
      "epoch:26 step:20809 [D loss: 0.357829, acc.: 85.16%] [G loss: 6.982850]\n",
      "epoch:26 step:20810 [D loss: 0.323913, acc.: 85.94%] [G loss: 3.957396]\n",
      "epoch:26 step:20811 [D loss: 0.322047, acc.: 85.16%] [G loss: 3.419730]\n",
      "epoch:26 step:20812 [D loss: 0.250313, acc.: 88.28%] [G loss: 4.231472]\n",
      "epoch:26 step:20813 [D loss: 0.382352, acc.: 78.91%] [G loss: 3.707839]\n",
      "epoch:26 step:20814 [D loss: 0.304020, acc.: 87.50%] [G loss: 3.303576]\n",
      "epoch:26 step:20815 [D loss: 0.288085, acc.: 87.50%] [G loss: 2.906722]\n",
      "epoch:26 step:20816 [D loss: 0.277924, acc.: 87.50%] [G loss: 4.578275]\n",
      "epoch:26 step:20817 [D loss: 0.379493, acc.: 78.91%] [G loss: 3.721187]\n",
      "epoch:26 step:20818 [D loss: 0.270249, acc.: 90.62%] [G loss: 3.546483]\n",
      "epoch:26 step:20819 [D loss: 0.298803, acc.: 85.16%] [G loss: 3.219528]\n",
      "epoch:26 step:20820 [D loss: 0.262595, acc.: 86.72%] [G loss: 2.646592]\n",
      "epoch:26 step:20821 [D loss: 0.274905, acc.: 87.50%] [G loss: 3.346473]\n",
      "epoch:26 step:20822 [D loss: 0.257132, acc.: 90.62%] [G loss: 2.983281]\n",
      "epoch:26 step:20823 [D loss: 0.384864, acc.: 79.69%] [G loss: 3.029109]\n",
      "epoch:26 step:20824 [D loss: 0.282602, acc.: 90.62%] [G loss: 3.593677]\n",
      "epoch:26 step:20825 [D loss: 0.391621, acc.: 82.81%] [G loss: 3.070458]\n",
      "epoch:26 step:20826 [D loss: 0.325323, acc.: 82.81%] [G loss: 3.182871]\n",
      "epoch:26 step:20827 [D loss: 0.281004, acc.: 88.28%] [G loss: 3.026735]\n",
      "epoch:26 step:20828 [D loss: 0.335153, acc.: 84.38%] [G loss: 2.437843]\n",
      "epoch:26 step:20829 [D loss: 0.298921, acc.: 86.72%] [G loss: 2.625927]\n",
      "epoch:26 step:20830 [D loss: 0.293031, acc.: 87.50%] [G loss: 3.082611]\n",
      "epoch:26 step:20831 [D loss: 0.279843, acc.: 89.84%] [G loss: 2.919762]\n",
      "epoch:26 step:20832 [D loss: 0.432341, acc.: 78.91%] [G loss: 3.013459]\n",
      "epoch:26 step:20833 [D loss: 0.367825, acc.: 82.81%] [G loss: 3.187458]\n",
      "epoch:26 step:20834 [D loss: 0.316312, acc.: 86.72%] [G loss: 2.794635]\n",
      "epoch:26 step:20835 [D loss: 0.334222, acc.: 82.81%] [G loss: 3.701461]\n",
      "epoch:26 step:20836 [D loss: 0.330028, acc.: 87.50%] [G loss: 3.330846]\n",
      "epoch:26 step:20837 [D loss: 0.242527, acc.: 89.84%] [G loss: 4.782696]\n",
      "epoch:26 step:20838 [D loss: 0.296268, acc.: 88.28%] [G loss: 3.733869]\n",
      "epoch:26 step:20839 [D loss: 0.323004, acc.: 85.16%] [G loss: 3.133320]\n",
      "epoch:26 step:20840 [D loss: 0.271764, acc.: 86.72%] [G loss: 3.290946]\n",
      "epoch:26 step:20841 [D loss: 0.243459, acc.: 91.41%] [G loss: 2.720445]\n",
      "epoch:26 step:20842 [D loss: 0.320545, acc.: 87.50%] [G loss: 3.098829]\n",
      "epoch:26 step:20843 [D loss: 0.442089, acc.: 80.47%] [G loss: 2.566267]\n",
      "epoch:26 step:20844 [D loss: 0.301437, acc.: 91.41%] [G loss: 2.883036]\n",
      "epoch:26 step:20845 [D loss: 0.438886, acc.: 80.47%] [G loss: 3.309845]\n",
      "epoch:26 step:20846 [D loss: 0.316049, acc.: 87.50%] [G loss: 2.606217]\n",
      "epoch:26 step:20847 [D loss: 0.287054, acc.: 87.50%] [G loss: 4.008021]\n",
      "epoch:26 step:20848 [D loss: 0.366052, acc.: 82.03%] [G loss: 2.517554]\n",
      "epoch:26 step:20849 [D loss: 0.318246, acc.: 85.94%] [G loss: 4.907965]\n",
      "epoch:26 step:20850 [D loss: 0.278136, acc.: 87.50%] [G loss: 4.999577]\n",
      "epoch:26 step:20851 [D loss: 0.198765, acc.: 92.19%] [G loss: 7.645388]\n",
      "epoch:26 step:20852 [D loss: 0.282443, acc.: 89.06%] [G loss: 9.343904]\n",
      "epoch:26 step:20853 [D loss: 0.283175, acc.: 85.94%] [G loss: 6.920736]\n",
      "epoch:26 step:20854 [D loss: 0.162832, acc.: 95.31%] [G loss: 8.584421]\n",
      "epoch:26 step:20855 [D loss: 0.158130, acc.: 93.75%] [G loss: 5.849857]\n",
      "epoch:26 step:20856 [D loss: 0.265995, acc.: 85.16%] [G loss: 4.210353]\n",
      "epoch:26 step:20857 [D loss: 0.301265, acc.: 85.16%] [G loss: 4.797783]\n",
      "epoch:26 step:20858 [D loss: 0.286196, acc.: 89.84%] [G loss: 4.258295]\n",
      "epoch:26 step:20859 [D loss: 0.273330, acc.: 87.50%] [G loss: 4.667129]\n",
      "epoch:26 step:20860 [D loss: 0.341488, acc.: 82.81%] [G loss: 2.817150]\n",
      "epoch:26 step:20861 [D loss: 0.324866, acc.: 85.16%] [G loss: 3.114741]\n",
      "epoch:26 step:20862 [D loss: 0.348956, acc.: 82.81%] [G loss: 3.041421]\n",
      "epoch:26 step:20863 [D loss: 0.281139, acc.: 85.94%] [G loss: 2.643769]\n",
      "epoch:26 step:20864 [D loss: 0.291708, acc.: 86.72%] [G loss: 2.883963]\n",
      "epoch:26 step:20865 [D loss: 0.392563, acc.: 81.25%] [G loss: 2.700242]\n",
      "epoch:26 step:20866 [D loss: 0.360447, acc.: 84.38%] [G loss: 2.969306]\n",
      "epoch:26 step:20867 [D loss: 0.254080, acc.: 92.97%] [G loss: 3.564497]\n",
      "epoch:26 step:20868 [D loss: 0.420308, acc.: 78.12%] [G loss: 3.699889]\n",
      "epoch:26 step:20869 [D loss: 0.362551, acc.: 81.25%] [G loss: 3.055146]\n",
      "epoch:26 step:20870 [D loss: 0.276833, acc.: 87.50%] [G loss: 3.417658]\n",
      "epoch:26 step:20871 [D loss: 0.332630, acc.: 85.94%] [G loss: 3.263832]\n",
      "epoch:26 step:20872 [D loss: 0.436830, acc.: 77.34%] [G loss: 2.572509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20873 [D loss: 0.296387, acc.: 85.16%] [G loss: 3.567079]\n",
      "epoch:26 step:20874 [D loss: 0.279095, acc.: 85.94%] [G loss: 3.698883]\n",
      "epoch:26 step:20875 [D loss: 0.295632, acc.: 87.50%] [G loss: 3.582504]\n",
      "epoch:26 step:20876 [D loss: 0.365944, acc.: 80.47%] [G loss: 2.927365]\n",
      "epoch:26 step:20877 [D loss: 0.424582, acc.: 78.91%] [G loss: 5.685661]\n",
      "epoch:26 step:20878 [D loss: 0.248561, acc.: 88.28%] [G loss: 7.326358]\n",
      "epoch:26 step:20879 [D loss: 0.278259, acc.: 86.72%] [G loss: 6.468077]\n",
      "epoch:26 step:20880 [D loss: 0.250944, acc.: 86.72%] [G loss: 5.231727]\n",
      "epoch:26 step:20881 [D loss: 0.343062, acc.: 84.38%] [G loss: 3.738672]\n",
      "epoch:26 step:20882 [D loss: 0.298370, acc.: 84.38%] [G loss: 2.529279]\n",
      "epoch:26 step:20883 [D loss: 0.333158, acc.: 85.94%] [G loss: 3.524665]\n",
      "epoch:26 step:20884 [D loss: 0.221883, acc.: 88.28%] [G loss: 5.008964]\n",
      "epoch:26 step:20885 [D loss: 0.308797, acc.: 86.72%] [G loss: 3.327397]\n",
      "epoch:26 step:20886 [D loss: 0.368679, acc.: 83.59%] [G loss: 3.116784]\n",
      "epoch:26 step:20887 [D loss: 0.236468, acc.: 90.62%] [G loss: 2.993111]\n",
      "epoch:26 step:20888 [D loss: 0.305167, acc.: 89.06%] [G loss: 2.963056]\n",
      "epoch:26 step:20889 [D loss: 0.371311, acc.: 81.25%] [G loss: 4.997390]\n",
      "epoch:26 step:20890 [D loss: 0.261026, acc.: 87.50%] [G loss: 4.494258]\n",
      "epoch:26 step:20891 [D loss: 0.217007, acc.: 89.84%] [G loss: 4.542973]\n",
      "epoch:26 step:20892 [D loss: 0.240461, acc.: 89.06%] [G loss: 4.837182]\n",
      "epoch:26 step:20893 [D loss: 0.239295, acc.: 88.28%] [G loss: 4.568021]\n",
      "epoch:26 step:20894 [D loss: 0.253978, acc.: 91.41%] [G loss: 3.767757]\n",
      "epoch:26 step:20895 [D loss: 0.257508, acc.: 90.62%] [G loss: 4.625958]\n",
      "epoch:26 step:20896 [D loss: 0.350156, acc.: 82.03%] [G loss: 4.153224]\n",
      "epoch:26 step:20897 [D loss: 0.295784, acc.: 87.50%] [G loss: 4.545046]\n",
      "epoch:26 step:20898 [D loss: 0.295240, acc.: 85.94%] [G loss: 2.724018]\n",
      "epoch:26 step:20899 [D loss: 0.311966, acc.: 85.94%] [G loss: 3.500664]\n",
      "epoch:26 step:20900 [D loss: 0.303062, acc.: 87.50%] [G loss: 3.063012]\n",
      "epoch:26 step:20901 [D loss: 0.389729, acc.: 80.47%] [G loss: 2.579910]\n",
      "epoch:26 step:20902 [D loss: 0.382617, acc.: 85.16%] [G loss: 3.518544]\n",
      "epoch:26 step:20903 [D loss: 0.368858, acc.: 83.59%] [G loss: 3.421789]\n",
      "epoch:26 step:20904 [D loss: 0.346840, acc.: 80.47%] [G loss: 3.496746]\n",
      "epoch:26 step:20905 [D loss: 0.264882, acc.: 89.84%] [G loss: 3.132448]\n",
      "epoch:26 step:20906 [D loss: 0.337569, acc.: 85.94%] [G loss: 3.233699]\n",
      "epoch:26 step:20907 [D loss: 0.292797, acc.: 88.28%] [G loss: 3.117349]\n",
      "epoch:26 step:20908 [D loss: 0.276450, acc.: 91.41%] [G loss: 3.265493]\n",
      "epoch:26 step:20909 [D loss: 0.272760, acc.: 87.50%] [G loss: 3.686906]\n",
      "epoch:26 step:20910 [D loss: 0.277255, acc.: 88.28%] [G loss: 2.479208]\n",
      "epoch:26 step:20911 [D loss: 0.320174, acc.: 82.81%] [G loss: 2.805241]\n",
      "epoch:26 step:20912 [D loss: 0.439387, acc.: 73.44%] [G loss: 2.904630]\n",
      "epoch:26 step:20913 [D loss: 0.201234, acc.: 93.75%] [G loss: 2.919978]\n",
      "epoch:26 step:20914 [D loss: 0.331861, acc.: 86.72%] [G loss: 3.932710]\n",
      "epoch:26 step:20915 [D loss: 0.280890, acc.: 85.16%] [G loss: 3.102971]\n",
      "epoch:26 step:20916 [D loss: 0.284693, acc.: 88.28%] [G loss: 3.468894]\n",
      "epoch:26 step:20917 [D loss: 0.262784, acc.: 89.06%] [G loss: 2.443264]\n",
      "epoch:26 step:20918 [D loss: 0.420501, acc.: 82.81%] [G loss: 3.192487]\n",
      "epoch:26 step:20919 [D loss: 0.387505, acc.: 82.03%] [G loss: 3.452751]\n",
      "epoch:26 step:20920 [D loss: 0.253784, acc.: 89.84%] [G loss: 2.868341]\n",
      "epoch:26 step:20921 [D loss: 0.341680, acc.: 82.03%] [G loss: 3.240427]\n",
      "epoch:26 step:20922 [D loss: 0.297851, acc.: 87.50%] [G loss: 2.410918]\n",
      "epoch:26 step:20923 [D loss: 0.254710, acc.: 89.06%] [G loss: 2.421281]\n",
      "epoch:26 step:20924 [D loss: 0.379121, acc.: 78.91%] [G loss: 3.690283]\n",
      "epoch:26 step:20925 [D loss: 0.387446, acc.: 82.03%] [G loss: 3.761732]\n",
      "epoch:26 step:20926 [D loss: 0.298934, acc.: 85.94%] [G loss: 3.501900]\n",
      "epoch:26 step:20927 [D loss: 0.364431, acc.: 86.72%] [G loss: 4.699255]\n",
      "epoch:26 step:20928 [D loss: 0.225092, acc.: 92.19%] [G loss: 3.461175]\n",
      "epoch:26 step:20929 [D loss: 0.357219, acc.: 83.59%] [G loss: 3.291395]\n",
      "epoch:26 step:20930 [D loss: 0.265925, acc.: 89.84%] [G loss: 3.401390]\n",
      "epoch:26 step:20931 [D loss: 0.306888, acc.: 84.38%] [G loss: 4.165910]\n",
      "epoch:26 step:20932 [D loss: 0.366706, acc.: 85.16%] [G loss: 2.916657]\n",
      "epoch:26 step:20933 [D loss: 0.301537, acc.: 85.16%] [G loss: 3.469471]\n",
      "epoch:26 step:20934 [D loss: 0.341875, acc.: 84.38%] [G loss: 4.207950]\n",
      "epoch:26 step:20935 [D loss: 0.240578, acc.: 91.41%] [G loss: 3.864552]\n",
      "epoch:26 step:20936 [D loss: 0.279158, acc.: 86.72%] [G loss: 2.988125]\n",
      "epoch:26 step:20937 [D loss: 0.283241, acc.: 87.50%] [G loss: 2.892472]\n",
      "epoch:26 step:20938 [D loss: 0.197947, acc.: 90.62%] [G loss: 3.385009]\n",
      "epoch:26 step:20939 [D loss: 0.262384, acc.: 89.84%] [G loss: 3.156237]\n",
      "epoch:26 step:20940 [D loss: 0.357088, acc.: 82.03%] [G loss: 3.454950]\n",
      "epoch:26 step:20941 [D loss: 0.311919, acc.: 89.06%] [G loss: 2.981959]\n",
      "epoch:26 step:20942 [D loss: 0.312800, acc.: 86.72%] [G loss: 2.378338]\n",
      "epoch:26 step:20943 [D loss: 0.296426, acc.: 85.94%] [G loss: 3.182195]\n",
      "epoch:26 step:20944 [D loss: 0.305733, acc.: 87.50%] [G loss: 3.856638]\n",
      "epoch:26 step:20945 [D loss: 0.377608, acc.: 80.47%] [G loss: 3.024005]\n",
      "epoch:26 step:20946 [D loss: 0.430282, acc.: 81.25%] [G loss: 3.648119]\n",
      "epoch:26 step:20947 [D loss: 0.239809, acc.: 88.28%] [G loss: 7.289454]\n",
      "epoch:26 step:20948 [D loss: 0.333309, acc.: 85.16%] [G loss: 8.399694]\n",
      "epoch:26 step:20949 [D loss: 0.256939, acc.: 89.06%] [G loss: 6.801794]\n",
      "epoch:26 step:20950 [D loss: 0.321353, acc.: 81.25%] [G loss: 4.777220]\n",
      "epoch:26 step:20951 [D loss: 0.265397, acc.: 91.41%] [G loss: 5.533879]\n",
      "epoch:26 step:20952 [D loss: 0.260328, acc.: 89.06%] [G loss: 4.853755]\n",
      "epoch:26 step:20953 [D loss: 0.334070, acc.: 88.28%] [G loss: 4.015202]\n",
      "epoch:26 step:20954 [D loss: 0.323466, acc.: 85.16%] [G loss: 4.195724]\n",
      "epoch:26 step:20955 [D loss: 0.310471, acc.: 82.81%] [G loss: 4.978822]\n",
      "epoch:26 step:20956 [D loss: 0.282154, acc.: 85.94%] [G loss: 3.542958]\n",
      "epoch:26 step:20957 [D loss: 0.271506, acc.: 89.06%] [G loss: 5.017747]\n",
      "epoch:26 step:20958 [D loss: 0.365997, acc.: 85.94%] [G loss: 3.677086]\n",
      "epoch:26 step:20959 [D loss: 0.259031, acc.: 89.06%] [G loss: 3.135928]\n",
      "epoch:26 step:20960 [D loss: 0.288955, acc.: 85.94%] [G loss: 3.515362]\n",
      "epoch:26 step:20961 [D loss: 0.326032, acc.: 85.16%] [G loss: 2.655323]\n",
      "epoch:26 step:20962 [D loss: 0.454545, acc.: 78.12%] [G loss: 5.655778]\n",
      "epoch:26 step:20963 [D loss: 0.553447, acc.: 76.56%] [G loss: 7.845576]\n",
      "epoch:26 step:20964 [D loss: 1.315398, acc.: 53.91%] [G loss: 6.746649]\n",
      "epoch:26 step:20965 [D loss: 1.079390, acc.: 71.09%] [G loss: 4.049180]\n",
      "epoch:26 step:20966 [D loss: 0.441308, acc.: 79.69%] [G loss: 4.811056]\n",
      "epoch:26 step:20967 [D loss: 0.509178, acc.: 77.34%] [G loss: 3.317843]\n",
      "epoch:26 step:20968 [D loss: 0.291895, acc.: 89.84%] [G loss: 3.922483]\n",
      "epoch:26 step:20969 [D loss: 0.324779, acc.: 85.16%] [G loss: 3.047899]\n",
      "epoch:26 step:20970 [D loss: 0.351420, acc.: 85.16%] [G loss: 2.582887]\n",
      "epoch:26 step:20971 [D loss: 0.286771, acc.: 88.28%] [G loss: 3.761492]\n",
      "epoch:26 step:20972 [D loss: 0.360147, acc.: 85.16%] [G loss: 2.993298]\n",
      "epoch:26 step:20973 [D loss: 0.301178, acc.: 87.50%] [G loss: 2.679552]\n",
      "epoch:26 step:20974 [D loss: 0.322693, acc.: 83.59%] [G loss: 2.900709]\n",
      "epoch:26 step:20975 [D loss: 0.349304, acc.: 82.81%] [G loss: 2.583888]\n",
      "epoch:26 step:20976 [D loss: 0.352759, acc.: 86.72%] [G loss: 3.442374]\n",
      "epoch:26 step:20977 [D loss: 0.360965, acc.: 81.25%] [G loss: 2.698824]\n",
      "epoch:26 step:20978 [D loss: 0.389336, acc.: 78.91%] [G loss: 3.737396]\n",
      "epoch:26 step:20979 [D loss: 0.310735, acc.: 85.16%] [G loss: 2.554071]\n",
      "epoch:26 step:20980 [D loss: 0.357785, acc.: 82.81%] [G loss: 3.298903]\n",
      "epoch:26 step:20981 [D loss: 0.282034, acc.: 87.50%] [G loss: 3.116919]\n",
      "epoch:26 step:20982 [D loss: 0.312590, acc.: 83.59%] [G loss: 3.791535]\n",
      "epoch:26 step:20983 [D loss: 0.331233, acc.: 82.81%] [G loss: 2.461411]\n",
      "epoch:26 step:20984 [D loss: 0.322517, acc.: 84.38%] [G loss: 3.132754]\n",
      "epoch:26 step:20985 [D loss: 0.252051, acc.: 89.84%] [G loss: 3.089084]\n",
      "epoch:26 step:20986 [D loss: 0.468385, acc.: 78.91%] [G loss: 2.747231]\n",
      "epoch:26 step:20987 [D loss: 0.324621, acc.: 84.38%] [G loss: 3.981326]\n",
      "epoch:26 step:20988 [D loss: 0.254168, acc.: 89.06%] [G loss: 3.782238]\n",
      "epoch:26 step:20989 [D loss: 0.377706, acc.: 82.03%] [G loss: 4.375752]\n",
      "epoch:26 step:20990 [D loss: 0.505821, acc.: 75.00%] [G loss: 2.501837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20991 [D loss: 0.259836, acc.: 89.06%] [G loss: 2.879100]\n",
      "epoch:26 step:20992 [D loss: 0.306755, acc.: 86.72%] [G loss: 3.141573]\n",
      "epoch:26 step:20993 [D loss: 0.348487, acc.: 82.81%] [G loss: 2.938900]\n",
      "epoch:26 step:20994 [D loss: 0.289711, acc.: 87.50%] [G loss: 2.706376]\n",
      "epoch:26 step:20995 [D loss: 0.273007, acc.: 86.72%] [G loss: 3.580537]\n",
      "epoch:26 step:20996 [D loss: 0.193868, acc.: 91.41%] [G loss: 3.741899]\n",
      "epoch:26 step:20997 [D loss: 0.245911, acc.: 90.62%] [G loss: 3.018765]\n",
      "epoch:26 step:20998 [D loss: 0.311993, acc.: 84.38%] [G loss: 2.695419]\n",
      "epoch:26 step:20999 [D loss: 0.283817, acc.: 87.50%] [G loss: 2.874388]\n",
      "epoch:26 step:21000 [D loss: 0.290598, acc.: 89.06%] [G loss: 3.717022]\n",
      "epoch:26 step:21001 [D loss: 0.226748, acc.: 94.53%] [G loss: 2.946531]\n",
      "epoch:26 step:21002 [D loss: 0.308770, acc.: 85.94%] [G loss: 3.078146]\n",
      "epoch:26 step:21003 [D loss: 0.378221, acc.: 84.38%] [G loss: 4.457929]\n",
      "epoch:26 step:21004 [D loss: 0.276701, acc.: 90.62%] [G loss: 2.787436]\n",
      "epoch:26 step:21005 [D loss: 0.308340, acc.: 85.94%] [G loss: 2.593796]\n",
      "epoch:26 step:21006 [D loss: 0.268170, acc.: 89.84%] [G loss: 2.998699]\n",
      "epoch:26 step:21007 [D loss: 0.351524, acc.: 84.38%] [G loss: 2.747094]\n",
      "epoch:26 step:21008 [D loss: 0.239432, acc.: 89.06%] [G loss: 3.377704]\n",
      "epoch:26 step:21009 [D loss: 0.330791, acc.: 82.81%] [G loss: 2.706612]\n",
      "epoch:26 step:21010 [D loss: 0.289034, acc.: 84.38%] [G loss: 2.582806]\n",
      "epoch:26 step:21011 [D loss: 0.296435, acc.: 88.28%] [G loss: 2.810171]\n",
      "epoch:26 step:21012 [D loss: 0.462866, acc.: 77.34%] [G loss: 2.667281]\n",
      "epoch:26 step:21013 [D loss: 0.266973, acc.: 89.84%] [G loss: 2.567901]\n",
      "epoch:26 step:21014 [D loss: 0.327402, acc.: 86.72%] [G loss: 2.947912]\n",
      "epoch:26 step:21015 [D loss: 0.381567, acc.: 80.47%] [G loss: 3.562606]\n",
      "epoch:26 step:21016 [D loss: 0.364881, acc.: 82.03%] [G loss: 4.311297]\n",
      "epoch:26 step:21017 [D loss: 0.318263, acc.: 85.16%] [G loss: 3.697491]\n",
      "epoch:26 step:21018 [D loss: 0.267068, acc.: 85.94%] [G loss: 3.324315]\n",
      "epoch:26 step:21019 [D loss: 0.325850, acc.: 85.94%] [G loss: 3.982781]\n",
      "epoch:26 step:21020 [D loss: 0.288301, acc.: 86.72%] [G loss: 3.453756]\n",
      "epoch:26 step:21021 [D loss: 0.188839, acc.: 89.84%] [G loss: 5.395050]\n",
      "epoch:26 step:21022 [D loss: 0.277505, acc.: 86.72%] [G loss: 3.418858]\n",
      "epoch:26 step:21023 [D loss: 0.331114, acc.: 82.03%] [G loss: 4.512420]\n",
      "epoch:26 step:21024 [D loss: 0.316238, acc.: 87.50%] [G loss: 3.736087]\n",
      "epoch:26 step:21025 [D loss: 0.262333, acc.: 88.28%] [G loss: 4.026646]\n",
      "epoch:26 step:21026 [D loss: 0.384521, acc.: 88.28%] [G loss: 2.534062]\n",
      "epoch:26 step:21027 [D loss: 0.304873, acc.: 86.72%] [G loss: 4.531205]\n",
      "epoch:26 step:21028 [D loss: 0.300277, acc.: 82.81%] [G loss: 3.105457]\n",
      "epoch:26 step:21029 [D loss: 0.249656, acc.: 91.41%] [G loss: 2.949959]\n",
      "epoch:26 step:21030 [D loss: 0.229660, acc.: 91.41%] [G loss: 2.676416]\n",
      "epoch:26 step:21031 [D loss: 0.293110, acc.: 85.94%] [G loss: 4.030521]\n",
      "epoch:26 step:21032 [D loss: 0.328503, acc.: 85.16%] [G loss: 5.620978]\n",
      "epoch:26 step:21033 [D loss: 0.348308, acc.: 82.03%] [G loss: 4.625113]\n",
      "epoch:26 step:21034 [D loss: 0.301959, acc.: 86.72%] [G loss: 4.085600]\n",
      "epoch:26 step:21035 [D loss: 0.295047, acc.: 89.84%] [G loss: 2.811852]\n",
      "epoch:26 step:21036 [D loss: 0.253241, acc.: 91.41%] [G loss: 3.428333]\n",
      "epoch:26 step:21037 [D loss: 0.358245, acc.: 83.59%] [G loss: 3.388912]\n",
      "epoch:26 step:21038 [D loss: 0.412609, acc.: 80.47%] [G loss: 3.015809]\n",
      "epoch:26 step:21039 [D loss: 0.311191, acc.: 89.84%] [G loss: 3.305007]\n",
      "epoch:26 step:21040 [D loss: 0.409906, acc.: 85.94%] [G loss: 2.897109]\n",
      "epoch:26 step:21041 [D loss: 0.378324, acc.: 82.03%] [G loss: 2.933540]\n",
      "epoch:26 step:21042 [D loss: 0.397429, acc.: 82.81%] [G loss: 2.845891]\n",
      "epoch:26 step:21043 [D loss: 0.407065, acc.: 84.38%] [G loss: 3.015989]\n",
      "epoch:26 step:21044 [D loss: 0.395954, acc.: 81.25%] [G loss: 2.711270]\n",
      "epoch:26 step:21045 [D loss: 0.389126, acc.: 82.81%] [G loss: 3.395996]\n",
      "epoch:26 step:21046 [D loss: 0.292727, acc.: 85.94%] [G loss: 3.398084]\n",
      "epoch:26 step:21047 [D loss: 0.453566, acc.: 80.47%] [G loss: 3.598742]\n",
      "epoch:26 step:21048 [D loss: 0.258881, acc.: 89.06%] [G loss: 3.035131]\n",
      "epoch:26 step:21049 [D loss: 0.319548, acc.: 85.94%] [G loss: 3.776827]\n",
      "epoch:26 step:21050 [D loss: 0.279749, acc.: 85.16%] [G loss: 2.832995]\n",
      "epoch:26 step:21051 [D loss: 0.245147, acc.: 89.06%] [G loss: 3.261075]\n",
      "epoch:26 step:21052 [D loss: 0.378202, acc.: 82.03%] [G loss: 2.476205]\n",
      "epoch:26 step:21053 [D loss: 0.276927, acc.: 86.72%] [G loss: 3.172269]\n",
      "epoch:26 step:21054 [D loss: 0.337110, acc.: 85.16%] [G loss: 2.577962]\n",
      "epoch:26 step:21055 [D loss: 0.255893, acc.: 89.84%] [G loss: 3.489694]\n",
      "epoch:26 step:21056 [D loss: 0.282221, acc.: 86.72%] [G loss: 3.809670]\n",
      "epoch:26 step:21057 [D loss: 0.264253, acc.: 90.62%] [G loss: 3.987810]\n",
      "epoch:26 step:21058 [D loss: 0.342140, acc.: 88.28%] [G loss: 3.258849]\n",
      "epoch:26 step:21059 [D loss: 0.377793, acc.: 82.81%] [G loss: 3.139132]\n",
      "epoch:26 step:21060 [D loss: 0.393933, acc.: 81.25%] [G loss: 3.276913]\n",
      "epoch:26 step:21061 [D loss: 0.300436, acc.: 84.38%] [G loss: 2.726439]\n",
      "epoch:26 step:21062 [D loss: 0.335561, acc.: 85.94%] [G loss: 2.601470]\n",
      "epoch:26 step:21063 [D loss: 0.340082, acc.: 85.94%] [G loss: 4.292751]\n",
      "epoch:26 step:21064 [D loss: 0.401262, acc.: 80.47%] [G loss: 4.187935]\n",
      "epoch:26 step:21065 [D loss: 0.282650, acc.: 85.16%] [G loss: 3.776179]\n",
      "epoch:26 step:21066 [D loss: 0.349889, acc.: 82.81%] [G loss: 3.600711]\n",
      "epoch:26 step:21067 [D loss: 0.310908, acc.: 87.50%] [G loss: 3.057586]\n",
      "epoch:26 step:21068 [D loss: 0.488122, acc.: 76.56%] [G loss: 2.573478]\n",
      "epoch:26 step:21069 [D loss: 0.346139, acc.: 85.16%] [G loss: 4.305024]\n",
      "epoch:26 step:21070 [D loss: 0.337214, acc.: 85.16%] [G loss: 2.560153]\n",
      "epoch:26 step:21071 [D loss: 0.426950, acc.: 84.38%] [G loss: 2.845529]\n",
      "epoch:26 step:21072 [D loss: 0.236468, acc.: 89.84%] [G loss: 2.503524]\n",
      "epoch:26 step:21073 [D loss: 0.222654, acc.: 89.84%] [G loss: 4.369575]\n",
      "epoch:26 step:21074 [D loss: 0.359681, acc.: 81.25%] [G loss: 2.809790]\n",
      "epoch:26 step:21075 [D loss: 0.484918, acc.: 78.12%] [G loss: 4.101430]\n",
      "epoch:26 step:21076 [D loss: 0.380667, acc.: 82.81%] [G loss: 3.678539]\n",
      "epoch:26 step:21077 [D loss: 0.391176, acc.: 80.47%] [G loss: 4.272424]\n",
      "epoch:26 step:21078 [D loss: 0.391021, acc.: 81.25%] [G loss: 4.519672]\n",
      "epoch:26 step:21079 [D loss: 0.460210, acc.: 82.03%] [G loss: 3.351370]\n",
      "epoch:26 step:21080 [D loss: 0.441483, acc.: 81.25%] [G loss: 3.909396]\n",
      "epoch:26 step:21081 [D loss: 0.215170, acc.: 92.19%] [G loss: 5.607236]\n",
      "epoch:26 step:21082 [D loss: 0.294586, acc.: 86.72%] [G loss: 3.481699]\n",
      "epoch:26 step:21083 [D loss: 0.282771, acc.: 86.72%] [G loss: 3.121976]\n",
      "epoch:26 step:21084 [D loss: 0.255112, acc.: 89.84%] [G loss: 3.466015]\n",
      "epoch:26 step:21085 [D loss: 0.259910, acc.: 89.84%] [G loss: 3.352368]\n",
      "epoch:26 step:21086 [D loss: 0.177985, acc.: 91.41%] [G loss: 3.618293]\n",
      "epoch:26 step:21087 [D loss: 0.323833, acc.: 82.81%] [G loss: 2.631793]\n",
      "epoch:27 step:21088 [D loss: 0.308162, acc.: 89.06%] [G loss: 3.852618]\n",
      "epoch:27 step:21089 [D loss: 0.181583, acc.: 91.41%] [G loss: 6.387619]\n",
      "epoch:27 step:21090 [D loss: 0.201776, acc.: 91.41%] [G loss: 4.724346]\n",
      "epoch:27 step:21091 [D loss: 0.291952, acc.: 85.16%] [G loss: 3.541130]\n",
      "epoch:27 step:21092 [D loss: 0.314441, acc.: 87.50%] [G loss: 3.372921]\n",
      "epoch:27 step:21093 [D loss: 0.283956, acc.: 89.06%] [G loss: 2.541431]\n",
      "epoch:27 step:21094 [D loss: 0.244800, acc.: 91.41%] [G loss: 3.091050]\n",
      "epoch:27 step:21095 [D loss: 0.321995, acc.: 85.16%] [G loss: 3.042909]\n",
      "epoch:27 step:21096 [D loss: 0.451470, acc.: 79.69%] [G loss: 2.798651]\n",
      "epoch:27 step:21097 [D loss: 0.373797, acc.: 82.81%] [G loss: 4.402143]\n",
      "epoch:27 step:21098 [D loss: 0.435709, acc.: 80.47%] [G loss: 4.133768]\n",
      "epoch:27 step:21099 [D loss: 0.356673, acc.: 82.03%] [G loss: 3.886135]\n",
      "epoch:27 step:21100 [D loss: 0.516207, acc.: 81.25%] [G loss: 7.558695]\n",
      "epoch:27 step:21101 [D loss: 1.107723, acc.: 65.62%] [G loss: 4.872000]\n",
      "epoch:27 step:21102 [D loss: 0.950709, acc.: 70.31%] [G loss: 4.239309]\n",
      "epoch:27 step:21103 [D loss: 0.504367, acc.: 75.78%] [G loss: 3.885811]\n",
      "epoch:27 step:21104 [D loss: 0.366955, acc.: 84.38%] [G loss: 4.717685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21105 [D loss: 0.325409, acc.: 89.84%] [G loss: 5.882537]\n",
      "epoch:27 step:21106 [D loss: 0.327301, acc.: 84.38%] [G loss: 3.827494]\n",
      "epoch:27 step:21107 [D loss: 0.278385, acc.: 88.28%] [G loss: 3.717842]\n",
      "epoch:27 step:21108 [D loss: 0.250818, acc.: 89.84%] [G loss: 4.946061]\n",
      "epoch:27 step:21109 [D loss: 0.323607, acc.: 89.84%] [G loss: 2.745886]\n",
      "epoch:27 step:21110 [D loss: 0.297878, acc.: 88.28%] [G loss: 3.325192]\n",
      "epoch:27 step:21111 [D loss: 0.336127, acc.: 85.94%] [G loss: 4.527071]\n",
      "epoch:27 step:21112 [D loss: 0.407870, acc.: 79.69%] [G loss: 3.358184]\n",
      "epoch:27 step:21113 [D loss: 0.395569, acc.: 81.25%] [G loss: 4.402173]\n",
      "epoch:27 step:21114 [D loss: 0.245602, acc.: 89.84%] [G loss: 3.844123]\n",
      "epoch:27 step:21115 [D loss: 0.300381, acc.: 85.94%] [G loss: 3.295761]\n",
      "epoch:27 step:21116 [D loss: 0.321188, acc.: 85.94%] [G loss: 2.280382]\n",
      "epoch:27 step:21117 [D loss: 0.298944, acc.: 88.28%] [G loss: 3.742781]\n",
      "epoch:27 step:21118 [D loss: 0.359119, acc.: 82.81%] [G loss: 2.290530]\n",
      "epoch:27 step:21119 [D loss: 0.341810, acc.: 85.94%] [G loss: 2.358110]\n",
      "epoch:27 step:21120 [D loss: 0.279834, acc.: 90.62%] [G loss: 2.042695]\n",
      "epoch:27 step:21121 [D loss: 0.400437, acc.: 81.25%] [G loss: 2.257885]\n",
      "epoch:27 step:21122 [D loss: 0.393955, acc.: 82.81%] [G loss: 2.518263]\n",
      "epoch:27 step:21123 [D loss: 0.259835, acc.: 90.62%] [G loss: 2.577175]\n",
      "epoch:27 step:21124 [D loss: 0.298232, acc.: 87.50%] [G loss: 2.699920]\n",
      "epoch:27 step:21125 [D loss: 0.383379, acc.: 82.81%] [G loss: 2.787277]\n",
      "epoch:27 step:21126 [D loss: 0.380817, acc.: 84.38%] [G loss: 3.219664]\n",
      "epoch:27 step:21127 [D loss: 0.321762, acc.: 85.16%] [G loss: 2.618596]\n",
      "epoch:27 step:21128 [D loss: 0.357651, acc.: 83.59%] [G loss: 3.257251]\n",
      "epoch:27 step:21129 [D loss: 0.317492, acc.: 83.59%] [G loss: 3.414139]\n",
      "epoch:27 step:21130 [D loss: 0.382170, acc.: 79.69%] [G loss: 3.386282]\n",
      "epoch:27 step:21131 [D loss: 0.413677, acc.: 78.12%] [G loss: 3.293442]\n",
      "epoch:27 step:21132 [D loss: 0.313507, acc.: 84.38%] [G loss: 3.406234]\n",
      "epoch:27 step:21133 [D loss: 0.396554, acc.: 78.91%] [G loss: 3.091685]\n",
      "epoch:27 step:21134 [D loss: 0.508676, acc.: 75.00%] [G loss: 2.143258]\n",
      "epoch:27 step:21135 [D loss: 0.343600, acc.: 85.16%] [G loss: 2.776141]\n",
      "epoch:27 step:21136 [D loss: 0.263043, acc.: 86.72%] [G loss: 2.897319]\n",
      "epoch:27 step:21137 [D loss: 0.333470, acc.: 82.81%] [G loss: 2.363438]\n",
      "epoch:27 step:21138 [D loss: 0.240956, acc.: 92.97%] [G loss: 2.822940]\n",
      "epoch:27 step:21139 [D loss: 0.299348, acc.: 86.72%] [G loss: 3.105772]\n",
      "epoch:27 step:21140 [D loss: 0.296314, acc.: 82.03%] [G loss: 3.087952]\n",
      "epoch:27 step:21141 [D loss: 0.280800, acc.: 89.06%] [G loss: 3.267647]\n",
      "epoch:27 step:21142 [D loss: 0.339310, acc.: 87.50%] [G loss: 3.364653]\n",
      "epoch:27 step:21143 [D loss: 0.326905, acc.: 85.16%] [G loss: 2.704230]\n",
      "epoch:27 step:21144 [D loss: 0.299784, acc.: 89.06%] [G loss: 3.127558]\n",
      "epoch:27 step:21145 [D loss: 0.365453, acc.: 81.25%] [G loss: 2.978813]\n",
      "epoch:27 step:21146 [D loss: 0.323174, acc.: 84.38%] [G loss: 3.298826]\n",
      "epoch:27 step:21147 [D loss: 0.451447, acc.: 77.34%] [G loss: 3.725104]\n",
      "epoch:27 step:21148 [D loss: 0.419492, acc.: 76.56%] [G loss: 3.726558]\n",
      "epoch:27 step:21149 [D loss: 0.371383, acc.: 81.25%] [G loss: 2.834134]\n",
      "epoch:27 step:21150 [D loss: 0.310936, acc.: 85.94%] [G loss: 3.832355]\n",
      "epoch:27 step:21151 [D loss: 0.472844, acc.: 79.69%] [G loss: 3.958279]\n",
      "epoch:27 step:21152 [D loss: 0.420006, acc.: 78.12%] [G loss: 2.726395]\n",
      "epoch:27 step:21153 [D loss: 0.239964, acc.: 88.28%] [G loss: 3.373674]\n",
      "epoch:27 step:21154 [D loss: 0.325508, acc.: 86.72%] [G loss: 2.679084]\n",
      "epoch:27 step:21155 [D loss: 0.376780, acc.: 84.38%] [G loss: 2.955201]\n",
      "epoch:27 step:21156 [D loss: 0.280475, acc.: 88.28%] [G loss: 2.605544]\n",
      "epoch:27 step:21157 [D loss: 0.273377, acc.: 89.84%] [G loss: 2.951648]\n",
      "epoch:27 step:21158 [D loss: 0.295532, acc.: 87.50%] [G loss: 2.666569]\n",
      "epoch:27 step:21159 [D loss: 0.318828, acc.: 89.84%] [G loss: 2.465776]\n",
      "epoch:27 step:21160 [D loss: 0.352168, acc.: 85.94%] [G loss: 2.799484]\n",
      "epoch:27 step:21161 [D loss: 0.290660, acc.: 89.06%] [G loss: 3.058339]\n",
      "epoch:27 step:21162 [D loss: 0.403534, acc.: 84.38%] [G loss: 3.501439]\n",
      "epoch:27 step:21163 [D loss: 0.446868, acc.: 78.12%] [G loss: 4.018547]\n",
      "epoch:27 step:21164 [D loss: 0.464113, acc.: 77.34%] [G loss: 4.074237]\n",
      "epoch:27 step:21165 [D loss: 0.342243, acc.: 83.59%] [G loss: 3.706514]\n",
      "epoch:27 step:21166 [D loss: 0.288762, acc.: 85.16%] [G loss: 2.275900]\n",
      "epoch:27 step:21167 [D loss: 0.245957, acc.: 89.06%] [G loss: 3.501896]\n",
      "epoch:27 step:21168 [D loss: 0.417090, acc.: 82.03%] [G loss: 4.034141]\n",
      "epoch:27 step:21169 [D loss: 0.346845, acc.: 84.38%] [G loss: 3.782636]\n",
      "epoch:27 step:21170 [D loss: 0.461077, acc.: 85.16%] [G loss: 4.092597]\n",
      "epoch:27 step:21171 [D loss: 0.339000, acc.: 82.03%] [G loss: 3.685038]\n",
      "epoch:27 step:21172 [D loss: 0.337451, acc.: 83.59%] [G loss: 4.377599]\n",
      "epoch:27 step:21173 [D loss: 0.374887, acc.: 78.12%] [G loss: 3.067604]\n",
      "epoch:27 step:21174 [D loss: 0.288061, acc.: 91.41%] [G loss: 3.618269]\n",
      "epoch:27 step:21175 [D loss: 0.267629, acc.: 88.28%] [G loss: 2.892755]\n",
      "epoch:27 step:21176 [D loss: 0.302777, acc.: 85.94%] [G loss: 2.954130]\n",
      "epoch:27 step:21177 [D loss: 0.452359, acc.: 80.47%] [G loss: 3.677099]\n",
      "epoch:27 step:21178 [D loss: 0.227943, acc.: 91.41%] [G loss: 3.650775]\n",
      "epoch:27 step:21179 [D loss: 0.377030, acc.: 83.59%] [G loss: 3.268444]\n",
      "epoch:27 step:21180 [D loss: 0.310539, acc.: 85.94%] [G loss: 2.730090]\n",
      "epoch:27 step:21181 [D loss: 0.270699, acc.: 89.84%] [G loss: 3.647564]\n",
      "epoch:27 step:21182 [D loss: 0.304785, acc.: 85.16%] [G loss: 3.967571]\n",
      "epoch:27 step:21183 [D loss: 0.261359, acc.: 87.50%] [G loss: 3.310319]\n",
      "epoch:27 step:21184 [D loss: 0.357758, acc.: 80.47%] [G loss: 3.128926]\n",
      "epoch:27 step:21185 [D loss: 0.280959, acc.: 86.72%] [G loss: 3.173811]\n",
      "epoch:27 step:21186 [D loss: 0.334825, acc.: 83.59%] [G loss: 3.325019]\n",
      "epoch:27 step:21187 [D loss: 0.359860, acc.: 83.59%] [G loss: 3.111920]\n",
      "epoch:27 step:21188 [D loss: 0.248306, acc.: 91.41%] [G loss: 3.989695]\n",
      "epoch:27 step:21189 [D loss: 0.364900, acc.: 81.25%] [G loss: 3.276600]\n",
      "epoch:27 step:21190 [D loss: 0.318733, acc.: 78.91%] [G loss: 5.664755]\n",
      "epoch:27 step:21191 [D loss: 0.203282, acc.: 92.19%] [G loss: 4.941945]\n",
      "epoch:27 step:21192 [D loss: 0.284266, acc.: 86.72%] [G loss: 4.249777]\n",
      "epoch:27 step:21193 [D loss: 0.297672, acc.: 85.16%] [G loss: 3.354848]\n",
      "epoch:27 step:21194 [D loss: 0.273254, acc.: 85.94%] [G loss: 3.636344]\n",
      "epoch:27 step:21195 [D loss: 0.298434, acc.: 86.72%] [G loss: 2.693125]\n",
      "epoch:27 step:21196 [D loss: 0.367042, acc.: 85.94%] [G loss: 2.848406]\n",
      "epoch:27 step:21197 [D loss: 0.296333, acc.: 84.38%] [G loss: 2.686277]\n",
      "epoch:27 step:21198 [D loss: 0.403681, acc.: 82.81%] [G loss: 2.365752]\n",
      "epoch:27 step:21199 [D loss: 0.304632, acc.: 85.16%] [G loss: 3.222097]\n",
      "epoch:27 step:21200 [D loss: 0.369154, acc.: 82.03%] [G loss: 2.630608]\n",
      "epoch:27 step:21201 [D loss: 0.438649, acc.: 79.69%] [G loss: 2.857369]\n",
      "epoch:27 step:21202 [D loss: 0.349410, acc.: 81.25%] [G loss: 3.078127]\n",
      "epoch:27 step:21203 [D loss: 0.510615, acc.: 76.56%] [G loss: 4.838535]\n",
      "epoch:27 step:21204 [D loss: 0.360192, acc.: 82.03%] [G loss: 3.396881]\n",
      "epoch:27 step:21205 [D loss: 0.295672, acc.: 82.81%] [G loss: 4.806626]\n",
      "epoch:27 step:21206 [D loss: 0.458336, acc.: 82.81%] [G loss: 4.123387]\n",
      "epoch:27 step:21207 [D loss: 0.399150, acc.: 83.59%] [G loss: 2.991163]\n",
      "epoch:27 step:21208 [D loss: 0.258149, acc.: 90.62%] [G loss: 3.256683]\n",
      "epoch:27 step:21209 [D loss: 0.273282, acc.: 86.72%] [G loss: 3.462378]\n",
      "epoch:27 step:21210 [D loss: 0.319813, acc.: 89.06%] [G loss: 3.165908]\n",
      "epoch:27 step:21211 [D loss: 0.431881, acc.: 77.34%] [G loss: 2.409791]\n",
      "epoch:27 step:21212 [D loss: 0.298958, acc.: 86.72%] [G loss: 2.750736]\n",
      "epoch:27 step:21213 [D loss: 0.272331, acc.: 89.84%] [G loss: 2.917875]\n",
      "epoch:27 step:21214 [D loss: 0.358310, acc.: 81.25%] [G loss: 2.719715]\n",
      "epoch:27 step:21215 [D loss: 0.289569, acc.: 90.62%] [G loss: 3.057486]\n",
      "epoch:27 step:21216 [D loss: 0.324997, acc.: 89.84%] [G loss: 2.465737]\n",
      "epoch:27 step:21217 [D loss: 0.434694, acc.: 82.03%] [G loss: 3.371707]\n",
      "epoch:27 step:21218 [D loss: 0.386939, acc.: 80.47%] [G loss: 3.893309]\n",
      "epoch:27 step:21219 [D loss: 0.316352, acc.: 87.50%] [G loss: 2.694339]\n",
      "epoch:27 step:21220 [D loss: 0.309756, acc.: 83.59%] [G loss: 3.954924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21221 [D loss: 0.196518, acc.: 94.53%] [G loss: 3.377802]\n",
      "epoch:27 step:21222 [D loss: 0.248566, acc.: 89.84%] [G loss: 5.646443]\n",
      "epoch:27 step:21223 [D loss: 0.284584, acc.: 85.16%] [G loss: 5.435393]\n",
      "epoch:27 step:21224 [D loss: 0.296441, acc.: 84.38%] [G loss: 4.689429]\n",
      "epoch:27 step:21225 [D loss: 0.246289, acc.: 85.16%] [G loss: 4.581360]\n",
      "epoch:27 step:21226 [D loss: 0.310847, acc.: 86.72%] [G loss: 3.448315]\n",
      "epoch:27 step:21227 [D loss: 0.387374, acc.: 82.03%] [G loss: 2.696118]\n",
      "epoch:27 step:21228 [D loss: 0.302553, acc.: 85.16%] [G loss: 3.418568]\n",
      "epoch:27 step:21229 [D loss: 0.363971, acc.: 80.47%] [G loss: 3.336996]\n",
      "epoch:27 step:21230 [D loss: 0.265680, acc.: 89.06%] [G loss: 3.641389]\n",
      "epoch:27 step:21231 [D loss: 0.269856, acc.: 87.50%] [G loss: 4.747005]\n",
      "epoch:27 step:21232 [D loss: 0.412293, acc.: 78.12%] [G loss: 3.188173]\n",
      "epoch:27 step:21233 [D loss: 0.299274, acc.: 85.94%] [G loss: 3.185360]\n",
      "epoch:27 step:21234 [D loss: 0.335554, acc.: 82.81%] [G loss: 3.662369]\n",
      "epoch:27 step:21235 [D loss: 0.251231, acc.: 90.62%] [G loss: 2.864391]\n",
      "epoch:27 step:21236 [D loss: 0.309281, acc.: 85.94%] [G loss: 3.742871]\n",
      "epoch:27 step:21237 [D loss: 0.329598, acc.: 82.81%] [G loss: 3.312747]\n",
      "epoch:27 step:21238 [D loss: 0.342998, acc.: 86.72%] [G loss: 2.265131]\n",
      "epoch:27 step:21239 [D loss: 0.313533, acc.: 87.50%] [G loss: 2.650110]\n",
      "epoch:27 step:21240 [D loss: 0.289110, acc.: 86.72%] [G loss: 2.868208]\n",
      "epoch:27 step:21241 [D loss: 0.416297, acc.: 82.81%] [G loss: 3.733424]\n",
      "epoch:27 step:21242 [D loss: 0.334939, acc.: 85.16%] [G loss: 3.279988]\n",
      "epoch:27 step:21243 [D loss: 0.289597, acc.: 85.16%] [G loss: 4.393726]\n",
      "epoch:27 step:21244 [D loss: 0.288361, acc.: 85.94%] [G loss: 4.885383]\n",
      "epoch:27 step:21245 [D loss: 0.255328, acc.: 86.72%] [G loss: 4.761055]\n",
      "epoch:27 step:21246 [D loss: 0.355628, acc.: 85.16%] [G loss: 5.444241]\n",
      "epoch:27 step:21247 [D loss: 0.345312, acc.: 86.72%] [G loss: 3.772412]\n",
      "epoch:27 step:21248 [D loss: 0.277200, acc.: 85.94%] [G loss: 3.228447]\n",
      "epoch:27 step:21249 [D loss: 0.284880, acc.: 85.16%] [G loss: 3.637165]\n",
      "epoch:27 step:21250 [D loss: 0.283214, acc.: 88.28%] [G loss: 3.950675]\n",
      "epoch:27 step:21251 [D loss: 0.336396, acc.: 82.03%] [G loss: 2.979524]\n",
      "epoch:27 step:21252 [D loss: 0.311873, acc.: 85.94%] [G loss: 3.954923]\n",
      "epoch:27 step:21253 [D loss: 0.264564, acc.: 90.62%] [G loss: 4.318359]\n",
      "epoch:27 step:21254 [D loss: 0.337542, acc.: 84.38%] [G loss: 2.811773]\n",
      "epoch:27 step:21255 [D loss: 0.318698, acc.: 84.38%] [G loss: 2.989307]\n",
      "epoch:27 step:21256 [D loss: 0.271041, acc.: 89.06%] [G loss: 2.692652]\n",
      "epoch:27 step:21257 [D loss: 0.179125, acc.: 92.97%] [G loss: 2.867737]\n",
      "epoch:27 step:21258 [D loss: 0.402224, acc.: 80.47%] [G loss: 3.632179]\n",
      "epoch:27 step:21259 [D loss: 0.291060, acc.: 85.94%] [G loss: 2.904079]\n",
      "epoch:27 step:21260 [D loss: 0.363282, acc.: 78.91%] [G loss: 4.533302]\n",
      "epoch:27 step:21261 [D loss: 0.269350, acc.: 86.72%] [G loss: 4.474267]\n",
      "epoch:27 step:21262 [D loss: 0.438770, acc.: 80.47%] [G loss: 2.688123]\n",
      "epoch:27 step:21263 [D loss: 0.295214, acc.: 90.62%] [G loss: 3.735770]\n",
      "epoch:27 step:21264 [D loss: 0.418350, acc.: 78.91%] [G loss: 2.891303]\n",
      "epoch:27 step:21265 [D loss: 0.396202, acc.: 78.12%] [G loss: 3.237979]\n",
      "epoch:27 step:21266 [D loss: 0.389171, acc.: 85.94%] [G loss: 2.989418]\n",
      "epoch:27 step:21267 [D loss: 0.324707, acc.: 85.16%] [G loss: 4.021243]\n",
      "epoch:27 step:21268 [D loss: 0.291029, acc.: 86.72%] [G loss: 3.430951]\n",
      "epoch:27 step:21269 [D loss: 0.343126, acc.: 82.81%] [G loss: 2.679272]\n",
      "epoch:27 step:21270 [D loss: 0.207387, acc.: 89.84%] [G loss: 3.345315]\n",
      "epoch:27 step:21271 [D loss: 0.428177, acc.: 78.91%] [G loss: 3.785300]\n",
      "epoch:27 step:21272 [D loss: 0.330087, acc.: 85.16%] [G loss: 2.918819]\n",
      "epoch:27 step:21273 [D loss: 0.343982, acc.: 82.81%] [G loss: 4.791407]\n",
      "epoch:27 step:21274 [D loss: 0.392119, acc.: 81.25%] [G loss: 3.894171]\n",
      "epoch:27 step:21275 [D loss: 0.463271, acc.: 81.25%] [G loss: 3.027081]\n",
      "epoch:27 step:21276 [D loss: 0.263860, acc.: 89.84%] [G loss: 3.832052]\n",
      "epoch:27 step:21277 [D loss: 0.434335, acc.: 82.81%] [G loss: 3.677128]\n",
      "epoch:27 step:21278 [D loss: 0.570140, acc.: 78.91%] [G loss: 3.687123]\n",
      "epoch:27 step:21279 [D loss: 0.598837, acc.: 71.88%] [G loss: 4.287943]\n",
      "epoch:27 step:21280 [D loss: 0.635614, acc.: 68.75%] [G loss: 4.605071]\n",
      "epoch:27 step:21281 [D loss: 0.301673, acc.: 85.94%] [G loss: 4.823849]\n",
      "epoch:27 step:21282 [D loss: 0.263219, acc.: 88.28%] [G loss: 5.501725]\n",
      "epoch:27 step:21283 [D loss: 0.166167, acc.: 94.53%] [G loss: 4.066619]\n",
      "epoch:27 step:21284 [D loss: 0.395236, acc.: 83.59%] [G loss: 4.919505]\n",
      "epoch:27 step:21285 [D loss: 0.417169, acc.: 80.47%] [G loss: 3.002799]\n",
      "epoch:27 step:21286 [D loss: 0.214869, acc.: 89.06%] [G loss: 4.508828]\n",
      "epoch:27 step:21287 [D loss: 0.280591, acc.: 90.62%] [G loss: 4.505888]\n",
      "epoch:27 step:21288 [D loss: 0.330088, acc.: 85.94%] [G loss: 3.510769]\n",
      "epoch:27 step:21289 [D loss: 0.337573, acc.: 85.94%] [G loss: 3.362986]\n",
      "epoch:27 step:21290 [D loss: 0.337036, acc.: 82.03%] [G loss: 3.094744]\n",
      "epoch:27 step:21291 [D loss: 0.324565, acc.: 84.38%] [G loss: 2.757926]\n",
      "epoch:27 step:21292 [D loss: 0.304122, acc.: 85.16%] [G loss: 2.732662]\n",
      "epoch:27 step:21293 [D loss: 0.302184, acc.: 85.16%] [G loss: 2.731508]\n",
      "epoch:27 step:21294 [D loss: 0.413411, acc.: 83.59%] [G loss: 2.686396]\n",
      "epoch:27 step:21295 [D loss: 0.376972, acc.: 79.69%] [G loss: 2.849137]\n",
      "epoch:27 step:21296 [D loss: 0.249689, acc.: 90.62%] [G loss: 2.844596]\n",
      "epoch:27 step:21297 [D loss: 0.250315, acc.: 89.84%] [G loss: 2.963511]\n",
      "epoch:27 step:21298 [D loss: 0.413862, acc.: 80.47%] [G loss: 5.173064]\n",
      "epoch:27 step:21299 [D loss: 0.445277, acc.: 82.03%] [G loss: 3.156995]\n",
      "epoch:27 step:21300 [D loss: 0.240338, acc.: 89.84%] [G loss: 3.800117]\n",
      "epoch:27 step:21301 [D loss: 0.335268, acc.: 90.62%] [G loss: 3.495277]\n",
      "epoch:27 step:21302 [D loss: 0.337231, acc.: 86.72%] [G loss: 3.059188]\n",
      "epoch:27 step:21303 [D loss: 0.404128, acc.: 79.69%] [G loss: 2.711277]\n",
      "epoch:27 step:21304 [D loss: 0.309185, acc.: 84.38%] [G loss: 3.925929]\n",
      "epoch:27 step:21305 [D loss: 0.378212, acc.: 83.59%] [G loss: 3.018844]\n",
      "epoch:27 step:21306 [D loss: 0.263573, acc.: 87.50%] [G loss: 2.513809]\n",
      "epoch:27 step:21307 [D loss: 0.334850, acc.: 84.38%] [G loss: 3.192572]\n",
      "epoch:27 step:21308 [D loss: 0.296874, acc.: 85.94%] [G loss: 2.838128]\n",
      "epoch:27 step:21309 [D loss: 0.450660, acc.: 74.22%] [G loss: 4.175410]\n",
      "epoch:27 step:21310 [D loss: 0.389257, acc.: 82.81%] [G loss: 4.615659]\n",
      "epoch:27 step:21311 [D loss: 0.307801, acc.: 83.59%] [G loss: 5.065144]\n",
      "epoch:27 step:21312 [D loss: 0.409751, acc.: 81.25%] [G loss: 3.295720]\n",
      "epoch:27 step:21313 [D loss: 0.417014, acc.: 80.47%] [G loss: 2.874577]\n",
      "epoch:27 step:21314 [D loss: 0.267069, acc.: 90.62%] [G loss: 3.135982]\n",
      "epoch:27 step:21315 [D loss: 0.349340, acc.: 85.94%] [G loss: 3.153798]\n",
      "epoch:27 step:21316 [D loss: 0.310635, acc.: 86.72%] [G loss: 4.358758]\n",
      "epoch:27 step:21317 [D loss: 0.274446, acc.: 88.28%] [G loss: 5.605698]\n",
      "epoch:27 step:21318 [D loss: 0.248913, acc.: 91.41%] [G loss: 5.346211]\n",
      "epoch:27 step:21319 [D loss: 0.226455, acc.: 88.28%] [G loss: 4.918684]\n",
      "epoch:27 step:21320 [D loss: 0.194782, acc.: 89.84%] [G loss: 8.019621]\n",
      "epoch:27 step:21321 [D loss: 0.252040, acc.: 89.84%] [G loss: 3.779466]\n",
      "epoch:27 step:21322 [D loss: 0.263380, acc.: 88.28%] [G loss: 3.305589]\n",
      "epoch:27 step:21323 [D loss: 0.282458, acc.: 88.28%] [G loss: 3.883391]\n",
      "epoch:27 step:21324 [D loss: 0.359149, acc.: 82.03%] [G loss: 3.932543]\n",
      "epoch:27 step:21325 [D loss: 0.406454, acc.: 81.25%] [G loss: 3.161307]\n",
      "epoch:27 step:21326 [D loss: 0.401440, acc.: 78.91%] [G loss: 3.351995]\n",
      "epoch:27 step:21327 [D loss: 0.358124, acc.: 83.59%] [G loss: 4.060009]\n",
      "epoch:27 step:21328 [D loss: 0.272370, acc.: 88.28%] [G loss: 3.984393]\n",
      "epoch:27 step:21329 [D loss: 0.337778, acc.: 81.25%] [G loss: 4.997629]\n",
      "epoch:27 step:21330 [D loss: 0.351875, acc.: 82.81%] [G loss: 2.170108]\n",
      "epoch:27 step:21331 [D loss: 0.285534, acc.: 89.84%] [G loss: 4.099869]\n",
      "epoch:27 step:21332 [D loss: 0.341728, acc.: 83.59%] [G loss: 3.904682]\n",
      "epoch:27 step:21333 [D loss: 0.304666, acc.: 85.16%] [G loss: 3.044652]\n",
      "epoch:27 step:21334 [D loss: 0.276708, acc.: 85.94%] [G loss: 4.591703]\n",
      "epoch:27 step:21335 [D loss: 0.302266, acc.: 88.28%] [G loss: 3.720406]\n",
      "epoch:27 step:21336 [D loss: 0.394032, acc.: 77.34%] [G loss: 2.921820]\n",
      "epoch:27 step:21337 [D loss: 0.360400, acc.: 85.16%] [G loss: 3.189130]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21338 [D loss: 0.250334, acc.: 90.62%] [G loss: 3.492271]\n",
      "epoch:27 step:21339 [D loss: 0.370467, acc.: 81.25%] [G loss: 2.968575]\n",
      "epoch:27 step:21340 [D loss: 0.262872, acc.: 87.50%] [G loss: 3.771832]\n",
      "epoch:27 step:21341 [D loss: 0.374211, acc.: 84.38%] [G loss: 3.513142]\n",
      "epoch:27 step:21342 [D loss: 0.429944, acc.: 78.91%] [G loss: 2.601803]\n",
      "epoch:27 step:21343 [D loss: 0.315380, acc.: 82.03%] [G loss: 3.105170]\n",
      "epoch:27 step:21344 [D loss: 0.372535, acc.: 83.59%] [G loss: 3.894373]\n",
      "epoch:27 step:21345 [D loss: 0.354003, acc.: 86.72%] [G loss: 3.616541]\n",
      "epoch:27 step:21346 [D loss: 0.357434, acc.: 81.25%] [G loss: 2.959491]\n",
      "epoch:27 step:21347 [D loss: 0.409412, acc.: 81.25%] [G loss: 3.729856]\n",
      "epoch:27 step:21348 [D loss: 0.294917, acc.: 86.72%] [G loss: 2.704371]\n",
      "epoch:27 step:21349 [D loss: 0.279714, acc.: 90.62%] [G loss: 3.825505]\n",
      "epoch:27 step:21350 [D loss: 0.201797, acc.: 92.97%] [G loss: 2.945467]\n",
      "epoch:27 step:21351 [D loss: 0.340729, acc.: 84.38%] [G loss: 2.827570]\n",
      "epoch:27 step:21352 [D loss: 0.317781, acc.: 83.59%] [G loss: 3.313334]\n",
      "epoch:27 step:21353 [D loss: 0.409328, acc.: 79.69%] [G loss: 3.417013]\n",
      "epoch:27 step:21354 [D loss: 0.352393, acc.: 82.03%] [G loss: 5.168116]\n",
      "epoch:27 step:21355 [D loss: 0.400007, acc.: 83.59%] [G loss: 3.383500]\n",
      "epoch:27 step:21356 [D loss: 0.231898, acc.: 87.50%] [G loss: 6.382532]\n",
      "epoch:27 step:21357 [D loss: 0.351740, acc.: 85.16%] [G loss: 4.122160]\n",
      "epoch:27 step:21358 [D loss: 0.247733, acc.: 89.84%] [G loss: 3.171884]\n",
      "epoch:27 step:21359 [D loss: 0.244922, acc.: 87.50%] [G loss: 3.138959]\n",
      "epoch:27 step:21360 [D loss: 0.303672, acc.: 87.50%] [G loss: 3.407998]\n",
      "epoch:27 step:21361 [D loss: 0.320180, acc.: 85.94%] [G loss: 3.212848]\n",
      "epoch:27 step:21362 [D loss: 0.651555, acc.: 65.62%] [G loss: 3.514483]\n",
      "epoch:27 step:21363 [D loss: 0.497308, acc.: 77.34%] [G loss: 5.813348]\n",
      "epoch:27 step:21364 [D loss: 0.853073, acc.: 72.66%] [G loss: 7.996465]\n",
      "epoch:27 step:21365 [D loss: 2.137043, acc.: 48.44%] [G loss: 7.746241]\n",
      "epoch:27 step:21366 [D loss: 3.397091, acc.: 52.34%] [G loss: 3.070945]\n",
      "epoch:27 step:21367 [D loss: 0.621359, acc.: 80.47%] [G loss: 7.150595]\n",
      "epoch:27 step:21368 [D loss: 1.028498, acc.: 71.09%] [G loss: 4.901013]\n",
      "epoch:27 step:21369 [D loss: 0.681512, acc.: 73.44%] [G loss: 2.548024]\n",
      "epoch:27 step:21370 [D loss: 0.225818, acc.: 91.41%] [G loss: 3.754683]\n",
      "epoch:27 step:21371 [D loss: 0.496586, acc.: 83.59%] [G loss: 3.174411]\n",
      "epoch:27 step:21372 [D loss: 0.469395, acc.: 78.12%] [G loss: 2.105996]\n",
      "epoch:27 step:21373 [D loss: 0.434595, acc.: 77.34%] [G loss: 3.296013]\n",
      "epoch:27 step:21374 [D loss: 0.336947, acc.: 86.72%] [G loss: 3.394576]\n",
      "epoch:27 step:21375 [D loss: 0.281220, acc.: 90.62%] [G loss: 3.270399]\n",
      "epoch:27 step:21376 [D loss: 0.440937, acc.: 80.47%] [G loss: 3.248294]\n",
      "epoch:27 step:21377 [D loss: 0.277773, acc.: 87.50%] [G loss: 3.085963]\n",
      "epoch:27 step:21378 [D loss: 0.439870, acc.: 83.59%] [G loss: 2.087998]\n",
      "epoch:27 step:21379 [D loss: 0.253115, acc.: 89.84%] [G loss: 2.202795]\n",
      "epoch:27 step:21380 [D loss: 0.420869, acc.: 82.81%] [G loss: 3.026628]\n",
      "epoch:27 step:21381 [D loss: 0.300341, acc.: 87.50%] [G loss: 2.413933]\n",
      "epoch:27 step:21382 [D loss: 0.366202, acc.: 83.59%] [G loss: 2.544850]\n",
      "epoch:27 step:21383 [D loss: 0.381932, acc.: 84.38%] [G loss: 2.710325]\n",
      "epoch:27 step:21384 [D loss: 0.282191, acc.: 86.72%] [G loss: 2.458911]\n",
      "epoch:27 step:21385 [D loss: 0.363748, acc.: 84.38%] [G loss: 2.891995]\n",
      "epoch:27 step:21386 [D loss: 0.418782, acc.: 81.25%] [G loss: 2.749650]\n",
      "epoch:27 step:21387 [D loss: 0.328889, acc.: 86.72%] [G loss: 3.275908]\n",
      "epoch:27 step:21388 [D loss: 0.362581, acc.: 80.47%] [G loss: 2.901908]\n",
      "epoch:27 step:21389 [D loss: 0.301364, acc.: 86.72%] [G loss: 2.881426]\n",
      "epoch:27 step:21390 [D loss: 0.328618, acc.: 84.38%] [G loss: 3.251765]\n",
      "epoch:27 step:21391 [D loss: 0.276750, acc.: 87.50%] [G loss: 2.711740]\n",
      "epoch:27 step:21392 [D loss: 0.262672, acc.: 89.06%] [G loss: 4.655669]\n",
      "epoch:27 step:21393 [D loss: 0.374077, acc.: 80.47%] [G loss: 2.765935]\n",
      "epoch:27 step:21394 [D loss: 0.325934, acc.: 89.06%] [G loss: 2.723887]\n",
      "epoch:27 step:21395 [D loss: 0.247834, acc.: 90.62%] [G loss: 3.301098]\n",
      "epoch:27 step:21396 [D loss: 0.292430, acc.: 86.72%] [G loss: 2.759536]\n",
      "epoch:27 step:21397 [D loss: 0.373358, acc.: 85.16%] [G loss: 3.228674]\n",
      "epoch:27 step:21398 [D loss: 0.410496, acc.: 79.69%] [G loss: 2.638499]\n",
      "epoch:27 step:21399 [D loss: 0.439632, acc.: 83.59%] [G loss: 2.905217]\n",
      "epoch:27 step:21400 [D loss: 0.405860, acc.: 81.25%] [G loss: 2.831096]\n",
      "epoch:27 step:21401 [D loss: 0.284779, acc.: 88.28%] [G loss: 2.737577]\n",
      "epoch:27 step:21402 [D loss: 0.468476, acc.: 71.88%] [G loss: 5.379280]\n",
      "epoch:27 step:21403 [D loss: 0.220826, acc.: 89.06%] [G loss: 5.020863]\n",
      "epoch:27 step:21404 [D loss: 0.532031, acc.: 78.91%] [G loss: 2.601990]\n",
      "epoch:27 step:21405 [D loss: 0.394165, acc.: 79.69%] [G loss: 3.339239]\n",
      "epoch:27 step:21406 [D loss: 0.358580, acc.: 85.94%] [G loss: 3.375107]\n",
      "epoch:27 step:21407 [D loss: 0.302029, acc.: 89.06%] [G loss: 4.314565]\n",
      "epoch:27 step:21408 [D loss: 0.286286, acc.: 89.06%] [G loss: 3.016045]\n",
      "epoch:27 step:21409 [D loss: 0.356471, acc.: 82.81%] [G loss: 3.602948]\n",
      "epoch:27 step:21410 [D loss: 0.312161, acc.: 84.38%] [G loss: 2.905080]\n",
      "epoch:27 step:21411 [D loss: 0.336532, acc.: 85.16%] [G loss: 3.460323]\n",
      "epoch:27 step:21412 [D loss: 0.403801, acc.: 83.59%] [G loss: 2.680074]\n",
      "epoch:27 step:21413 [D loss: 0.318028, acc.: 85.94%] [G loss: 2.694317]\n",
      "epoch:27 step:21414 [D loss: 0.323879, acc.: 83.59%] [G loss: 3.987141]\n",
      "epoch:27 step:21415 [D loss: 0.155866, acc.: 94.53%] [G loss: 6.501436]\n",
      "epoch:27 step:21416 [D loss: 0.247102, acc.: 88.28%] [G loss: 7.458391]\n",
      "epoch:27 step:21417 [D loss: 0.333058, acc.: 83.59%] [G loss: 5.653371]\n",
      "epoch:27 step:21418 [D loss: 0.216149, acc.: 89.84%] [G loss: 4.491065]\n",
      "epoch:27 step:21419 [D loss: 0.258551, acc.: 87.50%] [G loss: 5.924149]\n",
      "epoch:27 step:21420 [D loss: 0.295506, acc.: 87.50%] [G loss: 3.689526]\n",
      "epoch:27 step:21421 [D loss: 0.299264, acc.: 88.28%] [G loss: 7.080645]\n",
      "epoch:27 step:21422 [D loss: 0.352175, acc.: 81.25%] [G loss: 2.825823]\n",
      "epoch:27 step:21423 [D loss: 0.191722, acc.: 91.41%] [G loss: 4.497794]\n",
      "epoch:27 step:21424 [D loss: 0.262192, acc.: 90.62%] [G loss: 3.897243]\n",
      "epoch:27 step:21425 [D loss: 0.257856, acc.: 89.84%] [G loss: 3.218187]\n",
      "epoch:27 step:21426 [D loss: 0.238660, acc.: 89.84%] [G loss: 4.299139]\n",
      "epoch:27 step:21427 [D loss: 0.286670, acc.: 88.28%] [G loss: 3.831033]\n",
      "epoch:27 step:21428 [D loss: 0.293031, acc.: 89.06%] [G loss: 3.178557]\n",
      "epoch:27 step:21429 [D loss: 0.272283, acc.: 86.72%] [G loss: 3.239873]\n",
      "epoch:27 step:21430 [D loss: 0.398089, acc.: 78.12%] [G loss: 3.903412]\n",
      "epoch:27 step:21431 [D loss: 0.249333, acc.: 86.72%] [G loss: 4.587108]\n",
      "epoch:27 step:21432 [D loss: 0.268368, acc.: 89.06%] [G loss: 3.442975]\n",
      "epoch:27 step:21433 [D loss: 0.272359, acc.: 87.50%] [G loss: 3.282991]\n",
      "epoch:27 step:21434 [D loss: 0.305311, acc.: 89.06%] [G loss: 3.374797]\n",
      "epoch:27 step:21435 [D loss: 0.249105, acc.: 89.84%] [G loss: 3.041253]\n",
      "epoch:27 step:21436 [D loss: 0.387849, acc.: 78.91%] [G loss: 3.080676]\n",
      "epoch:27 step:21437 [D loss: 0.331524, acc.: 85.16%] [G loss: 3.958557]\n",
      "epoch:27 step:21438 [D loss: 0.362904, acc.: 82.81%] [G loss: 3.774869]\n",
      "epoch:27 step:21439 [D loss: 0.367070, acc.: 79.69%] [G loss: 2.715361]\n",
      "epoch:27 step:21440 [D loss: 0.340384, acc.: 85.94%] [G loss: 2.801282]\n",
      "epoch:27 step:21441 [D loss: 0.376075, acc.: 84.38%] [G loss: 2.656465]\n",
      "epoch:27 step:21442 [D loss: 0.348541, acc.: 82.03%] [G loss: 2.283615]\n",
      "epoch:27 step:21443 [D loss: 0.450651, acc.: 74.22%] [G loss: 2.313585]\n",
      "epoch:27 step:21444 [D loss: 0.353954, acc.: 85.94%] [G loss: 2.767791]\n",
      "epoch:27 step:21445 [D loss: 0.320839, acc.: 88.28%] [G loss: 2.718426]\n",
      "epoch:27 step:21446 [D loss: 0.317192, acc.: 83.59%] [G loss: 2.789591]\n",
      "epoch:27 step:21447 [D loss: 0.373877, acc.: 82.03%] [G loss: 2.439570]\n",
      "epoch:27 step:21448 [D loss: 0.349354, acc.: 85.16%] [G loss: 2.397670]\n",
      "epoch:27 step:21449 [D loss: 0.350924, acc.: 81.25%] [G loss: 2.666389]\n",
      "epoch:27 step:21450 [D loss: 0.338659, acc.: 85.16%] [G loss: 2.473692]\n",
      "epoch:27 step:21451 [D loss: 0.358659, acc.: 85.16%] [G loss: 2.259424]\n",
      "epoch:27 step:21452 [D loss: 0.341505, acc.: 85.16%] [G loss: 2.320760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21453 [D loss: 0.423619, acc.: 78.12%] [G loss: 2.164618]\n",
      "epoch:27 step:21454 [D loss: 0.426308, acc.: 79.69%] [G loss: 2.307070]\n",
      "epoch:27 step:21455 [D loss: 0.287624, acc.: 86.72%] [G loss: 2.626015]\n",
      "epoch:27 step:21456 [D loss: 0.323235, acc.: 87.50%] [G loss: 2.334671]\n",
      "epoch:27 step:21457 [D loss: 0.327595, acc.: 85.94%] [G loss: 2.796893]\n",
      "epoch:27 step:21458 [D loss: 0.307930, acc.: 89.06%] [G loss: 2.238316]\n",
      "epoch:27 step:21459 [D loss: 0.318450, acc.: 86.72%] [G loss: 2.650765]\n",
      "epoch:27 step:21460 [D loss: 0.318161, acc.: 86.72%] [G loss: 2.674942]\n",
      "epoch:27 step:21461 [D loss: 0.408540, acc.: 81.25%] [G loss: 2.541296]\n",
      "epoch:27 step:21462 [D loss: 0.305590, acc.: 86.72%] [G loss: 3.052015]\n",
      "epoch:27 step:21463 [D loss: 0.260423, acc.: 89.06%] [G loss: 3.122681]\n",
      "epoch:27 step:21464 [D loss: 0.268777, acc.: 89.84%] [G loss: 3.760755]\n",
      "epoch:27 step:21465 [D loss: 0.325611, acc.: 85.94%] [G loss: 3.044586]\n",
      "epoch:27 step:21466 [D loss: 0.280652, acc.: 88.28%] [G loss: 3.057451]\n",
      "epoch:27 step:21467 [D loss: 0.344657, acc.: 85.94%] [G loss: 2.872678]\n",
      "epoch:27 step:21468 [D loss: 0.248404, acc.: 90.62%] [G loss: 2.423389]\n",
      "epoch:27 step:21469 [D loss: 0.326047, acc.: 85.16%] [G loss: 2.818783]\n",
      "epoch:27 step:21470 [D loss: 0.281565, acc.: 88.28%] [G loss: 2.483402]\n",
      "epoch:27 step:21471 [D loss: 0.334962, acc.: 82.81%] [G loss: 3.369769]\n",
      "epoch:27 step:21472 [D loss: 0.387623, acc.: 82.81%] [G loss: 2.857879]\n",
      "epoch:27 step:21473 [D loss: 0.376387, acc.: 78.91%] [G loss: 2.718076]\n",
      "epoch:27 step:21474 [D loss: 0.343100, acc.: 84.38%] [G loss: 2.598094]\n",
      "epoch:27 step:21475 [D loss: 0.419083, acc.: 80.47%] [G loss: 3.011524]\n",
      "epoch:27 step:21476 [D loss: 0.323077, acc.: 84.38%] [G loss: 3.905229]\n",
      "epoch:27 step:21477 [D loss: 0.316639, acc.: 85.94%] [G loss: 2.922104]\n",
      "epoch:27 step:21478 [D loss: 0.415303, acc.: 77.34%] [G loss: 3.362255]\n",
      "epoch:27 step:21479 [D loss: 0.276069, acc.: 87.50%] [G loss: 3.179934]\n",
      "epoch:27 step:21480 [D loss: 0.378278, acc.: 85.94%] [G loss: 2.600179]\n",
      "epoch:27 step:21481 [D loss: 0.373333, acc.: 82.03%] [G loss: 2.644673]\n",
      "epoch:27 step:21482 [D loss: 0.318738, acc.: 88.28%] [G loss: 2.870017]\n",
      "epoch:27 step:21483 [D loss: 0.397446, acc.: 83.59%] [G loss: 2.595550]\n",
      "epoch:27 step:21484 [D loss: 0.359945, acc.: 83.59%] [G loss: 2.714352]\n",
      "epoch:27 step:21485 [D loss: 0.326267, acc.: 88.28%] [G loss: 2.756300]\n",
      "epoch:27 step:21486 [D loss: 0.317536, acc.: 85.16%] [G loss: 3.680726]\n",
      "epoch:27 step:21487 [D loss: 0.317743, acc.: 84.38%] [G loss: 2.604987]\n",
      "epoch:27 step:21488 [D loss: 0.337583, acc.: 85.94%] [G loss: 4.425199]\n",
      "epoch:27 step:21489 [D loss: 0.260507, acc.: 90.62%] [G loss: 3.920766]\n",
      "epoch:27 step:21490 [D loss: 0.235245, acc.: 89.06%] [G loss: 5.571362]\n",
      "epoch:27 step:21491 [D loss: 0.271293, acc.: 87.50%] [G loss: 3.636565]\n",
      "epoch:27 step:21492 [D loss: 0.211222, acc.: 90.62%] [G loss: 4.397790]\n",
      "epoch:27 step:21493 [D loss: 0.386944, acc.: 83.59%] [G loss: 5.703118]\n",
      "epoch:27 step:21494 [D loss: 0.304884, acc.: 86.72%] [G loss: 3.634927]\n",
      "epoch:27 step:21495 [D loss: 0.309165, acc.: 87.50%] [G loss: 3.128761]\n",
      "epoch:27 step:21496 [D loss: 0.342678, acc.: 82.03%] [G loss: 3.643847]\n",
      "epoch:27 step:21497 [D loss: 0.287857, acc.: 85.94%] [G loss: 2.539229]\n",
      "epoch:27 step:21498 [D loss: 0.329098, acc.: 84.38%] [G loss: 2.776213]\n",
      "epoch:27 step:21499 [D loss: 0.405673, acc.: 82.03%] [G loss: 2.414104]\n",
      "epoch:27 step:21500 [D loss: 0.350022, acc.: 84.38%] [G loss: 2.686042]\n",
      "epoch:27 step:21501 [D loss: 0.265566, acc.: 91.41%] [G loss: 2.948878]\n",
      "epoch:27 step:21502 [D loss: 0.262970, acc.: 86.72%] [G loss: 2.924553]\n",
      "epoch:27 step:21503 [D loss: 0.241990, acc.: 89.06%] [G loss: 3.173119]\n",
      "epoch:27 step:21504 [D loss: 0.294986, acc.: 92.19%] [G loss: 1.940236]\n",
      "epoch:27 step:21505 [D loss: 0.298174, acc.: 85.94%] [G loss: 2.279170]\n",
      "epoch:27 step:21506 [D loss: 0.319319, acc.: 83.59%] [G loss: 2.431557]\n",
      "epoch:27 step:21507 [D loss: 0.349379, acc.: 85.94%] [G loss: 2.582534]\n",
      "epoch:27 step:21508 [D loss: 0.253925, acc.: 89.06%] [G loss: 3.574239]\n",
      "epoch:27 step:21509 [D loss: 0.326332, acc.: 86.72%] [G loss: 2.560953]\n",
      "epoch:27 step:21510 [D loss: 0.268254, acc.: 87.50%] [G loss: 3.334920]\n",
      "epoch:27 step:21511 [D loss: 0.414682, acc.: 82.03%] [G loss: 3.937857]\n",
      "epoch:27 step:21512 [D loss: 0.385526, acc.: 83.59%] [G loss: 5.685174]\n",
      "epoch:27 step:21513 [D loss: 0.455571, acc.: 81.25%] [G loss: 3.982517]\n",
      "epoch:27 step:21514 [D loss: 0.294256, acc.: 91.41%] [G loss: 3.053462]\n",
      "epoch:27 step:21515 [D loss: 0.479112, acc.: 77.34%] [G loss: 2.992598]\n",
      "epoch:27 step:21516 [D loss: 0.277069, acc.: 89.84%] [G loss: 4.707123]\n",
      "epoch:27 step:21517 [D loss: 0.380186, acc.: 78.91%] [G loss: 2.698203]\n",
      "epoch:27 step:21518 [D loss: 0.313257, acc.: 80.47%] [G loss: 3.151038]\n",
      "epoch:27 step:21519 [D loss: 0.413091, acc.: 78.12%] [G loss: 3.784374]\n",
      "epoch:27 step:21520 [D loss: 0.276297, acc.: 89.84%] [G loss: 3.212307]\n",
      "epoch:27 step:21521 [D loss: 0.372865, acc.: 82.81%] [G loss: 2.370920]\n",
      "epoch:27 step:21522 [D loss: 0.379179, acc.: 85.16%] [G loss: 3.464777]\n",
      "epoch:27 step:21523 [D loss: 0.394068, acc.: 80.47%] [G loss: 2.558924]\n",
      "epoch:27 step:21524 [D loss: 0.391702, acc.: 78.91%] [G loss: 3.325436]\n",
      "epoch:27 step:21525 [D loss: 0.312502, acc.: 85.94%] [G loss: 2.713700]\n",
      "epoch:27 step:21526 [D loss: 0.440071, acc.: 82.81%] [G loss: 2.506522]\n",
      "epoch:27 step:21527 [D loss: 0.232906, acc.: 92.19%] [G loss: 2.518471]\n",
      "epoch:27 step:21528 [D loss: 0.323111, acc.: 85.94%] [G loss: 3.108586]\n",
      "epoch:27 step:21529 [D loss: 0.335063, acc.: 82.81%] [G loss: 3.450127]\n",
      "epoch:27 step:21530 [D loss: 0.310173, acc.: 87.50%] [G loss: 3.087258]\n",
      "epoch:27 step:21531 [D loss: 0.358116, acc.: 88.28%] [G loss: 2.738419]\n",
      "epoch:27 step:21532 [D loss: 0.367904, acc.: 87.50%] [G loss: 2.457475]\n",
      "epoch:27 step:21533 [D loss: 0.370634, acc.: 85.94%] [G loss: 2.258931]\n",
      "epoch:27 step:21534 [D loss: 0.340943, acc.: 84.38%] [G loss: 2.746932]\n",
      "epoch:27 step:21535 [D loss: 0.382847, acc.: 83.59%] [G loss: 3.287935]\n",
      "epoch:27 step:21536 [D loss: 0.343405, acc.: 86.72%] [G loss: 2.806506]\n",
      "epoch:27 step:21537 [D loss: 0.365768, acc.: 81.25%] [G loss: 2.524414]\n",
      "epoch:27 step:21538 [D loss: 0.348214, acc.: 85.16%] [G loss: 3.211926]\n",
      "epoch:27 step:21539 [D loss: 0.362701, acc.: 85.16%] [G loss: 2.761053]\n",
      "epoch:27 step:21540 [D loss: 0.387956, acc.: 84.38%] [G loss: 3.130523]\n",
      "epoch:27 step:21541 [D loss: 0.370632, acc.: 83.59%] [G loss: 2.447573]\n",
      "epoch:27 step:21542 [D loss: 0.424164, acc.: 82.03%] [G loss: 3.055770]\n",
      "epoch:27 step:21543 [D loss: 0.359060, acc.: 86.72%] [G loss: 3.441823]\n",
      "epoch:27 step:21544 [D loss: 0.423516, acc.: 82.81%] [G loss: 2.887404]\n",
      "epoch:27 step:21545 [D loss: 0.261752, acc.: 90.62%] [G loss: 4.643993]\n",
      "epoch:27 step:21546 [D loss: 0.301430, acc.: 88.28%] [G loss: 4.652337]\n",
      "epoch:27 step:21547 [D loss: 0.287822, acc.: 90.62%] [G loss: 4.243525]\n",
      "epoch:27 step:21548 [D loss: 0.263924, acc.: 88.28%] [G loss: 3.285553]\n",
      "epoch:27 step:21549 [D loss: 0.285925, acc.: 85.94%] [G loss: 4.297297]\n",
      "epoch:27 step:21550 [D loss: 0.452223, acc.: 75.78%] [G loss: 2.244443]\n",
      "epoch:27 step:21551 [D loss: 0.312347, acc.: 85.16%] [G loss: 2.587789]\n",
      "epoch:27 step:21552 [D loss: 0.335081, acc.: 82.81%] [G loss: 3.882411]\n",
      "epoch:27 step:21553 [D loss: 0.375563, acc.: 81.25%] [G loss: 3.538387]\n",
      "epoch:27 step:21554 [D loss: 0.478761, acc.: 76.56%] [G loss: 3.551356]\n",
      "epoch:27 step:21555 [D loss: 0.393756, acc.: 81.25%] [G loss: 2.655937]\n",
      "epoch:27 step:21556 [D loss: 0.410689, acc.: 82.81%] [G loss: 2.303726]\n",
      "epoch:27 step:21557 [D loss: 0.438446, acc.: 82.81%] [G loss: 2.844812]\n",
      "epoch:27 step:21558 [D loss: 0.323707, acc.: 87.50%] [G loss: 2.427068]\n",
      "epoch:27 step:21559 [D loss: 0.349926, acc.: 86.72%] [G loss: 2.594427]\n",
      "epoch:27 step:21560 [D loss: 0.353257, acc.: 83.59%] [G loss: 2.874013]\n",
      "epoch:27 step:21561 [D loss: 0.408277, acc.: 84.38%] [G loss: 3.088220]\n",
      "epoch:27 step:21562 [D loss: 0.459525, acc.: 74.22%] [G loss: 2.483258]\n",
      "epoch:27 step:21563 [D loss: 0.353182, acc.: 82.81%] [G loss: 2.787363]\n",
      "epoch:27 step:21564 [D loss: 0.303243, acc.: 88.28%] [G loss: 4.072115]\n",
      "epoch:27 step:21565 [D loss: 0.408054, acc.: 83.59%] [G loss: 2.906316]\n",
      "epoch:27 step:21566 [D loss: 0.318308, acc.: 85.16%] [G loss: 4.316905]\n",
      "epoch:27 step:21567 [D loss: 0.256899, acc.: 89.06%] [G loss: 3.240376]\n",
      "epoch:27 step:21568 [D loss: 0.337009, acc.: 82.81%] [G loss: 3.304032]\n",
      "epoch:27 step:21569 [D loss: 0.307281, acc.: 85.94%] [G loss: 3.212084]\n",
      "epoch:27 step:21570 [D loss: 0.392503, acc.: 85.16%] [G loss: 3.516352]\n",
      "epoch:27 step:21571 [D loss: 0.347882, acc.: 82.81%] [G loss: 2.779882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21572 [D loss: 0.319988, acc.: 85.94%] [G loss: 3.051240]\n",
      "epoch:27 step:21573 [D loss: 0.363702, acc.: 84.38%] [G loss: 3.709479]\n",
      "epoch:27 step:21574 [D loss: 0.316725, acc.: 85.16%] [G loss: 5.028055]\n",
      "epoch:27 step:21575 [D loss: 0.298930, acc.: 85.94%] [G loss: 3.308318]\n",
      "epoch:27 step:21576 [D loss: 0.279317, acc.: 87.50%] [G loss: 3.271150]\n",
      "epoch:27 step:21577 [D loss: 0.323815, acc.: 85.94%] [G loss: 2.801285]\n",
      "epoch:27 step:21578 [D loss: 0.338477, acc.: 84.38%] [G loss: 2.779250]\n",
      "epoch:27 step:21579 [D loss: 0.387884, acc.: 82.03%] [G loss: 3.172082]\n",
      "epoch:27 step:21580 [D loss: 0.345384, acc.: 82.81%] [G loss: 4.169500]\n",
      "epoch:27 step:21581 [D loss: 0.272947, acc.: 89.06%] [G loss: 2.996289]\n",
      "epoch:27 step:21582 [D loss: 0.327461, acc.: 87.50%] [G loss: 2.551761]\n",
      "epoch:27 step:21583 [D loss: 0.329200, acc.: 85.94%] [G loss: 2.928450]\n",
      "epoch:27 step:21584 [D loss: 0.314929, acc.: 88.28%] [G loss: 3.847535]\n",
      "epoch:27 step:21585 [D loss: 0.280889, acc.: 86.72%] [G loss: 2.699031]\n",
      "epoch:27 step:21586 [D loss: 0.378612, acc.: 84.38%] [G loss: 3.565695]\n",
      "epoch:27 step:21587 [D loss: 0.340788, acc.: 86.72%] [G loss: 2.991375]\n",
      "epoch:27 step:21588 [D loss: 0.325539, acc.: 83.59%] [G loss: 3.539426]\n",
      "epoch:27 step:21589 [D loss: 0.337051, acc.: 86.72%] [G loss: 4.060717]\n",
      "epoch:27 step:21590 [D loss: 0.291774, acc.: 86.72%] [G loss: 5.591701]\n",
      "epoch:27 step:21591 [D loss: 0.230736, acc.: 91.41%] [G loss: 4.320492]\n",
      "epoch:27 step:21592 [D loss: 0.259866, acc.: 88.28%] [G loss: 3.051556]\n",
      "epoch:27 step:21593 [D loss: 0.262996, acc.: 86.72%] [G loss: 2.874773]\n",
      "epoch:27 step:21594 [D loss: 0.289113, acc.: 88.28%] [G loss: 3.674683]\n",
      "epoch:27 step:21595 [D loss: 0.346386, acc.: 85.16%] [G loss: 3.193950]\n",
      "epoch:27 step:21596 [D loss: 0.359248, acc.: 82.03%] [G loss: 2.614998]\n",
      "epoch:27 step:21597 [D loss: 0.257437, acc.: 92.19%] [G loss: 2.554212]\n",
      "epoch:27 step:21598 [D loss: 0.362289, acc.: 80.47%] [G loss: 2.817185]\n",
      "epoch:27 step:21599 [D loss: 0.273438, acc.: 89.84%] [G loss: 3.601803]\n",
      "epoch:27 step:21600 [D loss: 0.390855, acc.: 82.81%] [G loss: 2.772388]\n",
      "epoch:27 step:21601 [D loss: 0.334293, acc.: 88.28%] [G loss: 2.510123]\n",
      "epoch:27 step:21602 [D loss: 0.406505, acc.: 79.69%] [G loss: 2.801059]\n",
      "epoch:27 step:21603 [D loss: 0.299024, acc.: 89.06%] [G loss: 2.452336]\n",
      "epoch:27 step:21604 [D loss: 0.504945, acc.: 74.22%] [G loss: 2.525380]\n",
      "epoch:27 step:21605 [D loss: 0.344559, acc.: 85.94%] [G loss: 2.414761]\n",
      "epoch:27 step:21606 [D loss: 0.307251, acc.: 86.72%] [G loss: 3.710871]\n",
      "epoch:27 step:21607 [D loss: 0.367706, acc.: 82.03%] [G loss: 3.773045]\n",
      "epoch:27 step:21608 [D loss: 0.283949, acc.: 84.38%] [G loss: 3.664268]\n",
      "epoch:27 step:21609 [D loss: 0.372183, acc.: 80.47%] [G loss: 3.345735]\n",
      "epoch:27 step:21610 [D loss: 0.300422, acc.: 86.72%] [G loss: 2.972818]\n",
      "epoch:27 step:21611 [D loss: 0.315522, acc.: 87.50%] [G loss: 2.900465]\n",
      "epoch:27 step:21612 [D loss: 0.402712, acc.: 79.69%] [G loss: 3.138701]\n",
      "epoch:27 step:21613 [D loss: 0.353335, acc.: 84.38%] [G loss: 3.080590]\n",
      "epoch:27 step:21614 [D loss: 0.277359, acc.: 89.84%] [G loss: 2.971924]\n",
      "epoch:27 step:21615 [D loss: 0.301615, acc.: 87.50%] [G loss: 3.385846]\n",
      "epoch:27 step:21616 [D loss: 0.396043, acc.: 81.25%] [G loss: 3.061176]\n",
      "epoch:27 step:21617 [D loss: 0.338117, acc.: 85.94%] [G loss: 3.366944]\n",
      "epoch:27 step:21618 [D loss: 0.273774, acc.: 86.72%] [G loss: 2.913480]\n",
      "epoch:27 step:21619 [D loss: 0.357888, acc.: 83.59%] [G loss: 3.024933]\n",
      "epoch:27 step:21620 [D loss: 0.314294, acc.: 85.94%] [G loss: 2.667284]\n",
      "epoch:27 step:21621 [D loss: 0.310235, acc.: 85.94%] [G loss: 2.467347]\n",
      "epoch:27 step:21622 [D loss: 0.361147, acc.: 82.03%] [G loss: 2.918600]\n",
      "epoch:27 step:21623 [D loss: 0.390763, acc.: 78.91%] [G loss: 2.717067]\n",
      "epoch:27 step:21624 [D loss: 0.428696, acc.: 77.34%] [G loss: 3.620812]\n",
      "epoch:27 step:21625 [D loss: 0.409243, acc.: 82.81%] [G loss: 7.112494]\n",
      "epoch:27 step:21626 [D loss: 0.404631, acc.: 85.94%] [G loss: 4.695926]\n",
      "epoch:27 step:21627 [D loss: 0.383571, acc.: 81.25%] [G loss: 3.225779]\n",
      "epoch:27 step:21628 [D loss: 0.312456, acc.: 85.94%] [G loss: 3.927038]\n",
      "epoch:27 step:21629 [D loss: 0.319911, acc.: 82.81%] [G loss: 2.617104]\n",
      "epoch:27 step:21630 [D loss: 0.314476, acc.: 85.94%] [G loss: 3.358541]\n",
      "epoch:27 step:21631 [D loss: 0.361296, acc.: 84.38%] [G loss: 3.370373]\n",
      "epoch:27 step:21632 [D loss: 0.206837, acc.: 89.84%] [G loss: 4.031282]\n",
      "epoch:27 step:21633 [D loss: 0.325407, acc.: 85.94%] [G loss: 3.290862]\n",
      "epoch:27 step:21634 [D loss: 0.332583, acc.: 85.16%] [G loss: 3.487759]\n",
      "epoch:27 step:21635 [D loss: 0.312029, acc.: 85.16%] [G loss: 3.812912]\n",
      "epoch:27 step:21636 [D loss: 0.283650, acc.: 85.94%] [G loss: 4.168116]\n",
      "epoch:27 step:21637 [D loss: 0.334281, acc.: 85.16%] [G loss: 4.940990]\n",
      "epoch:27 step:21638 [D loss: 0.310624, acc.: 85.94%] [G loss: 3.084956]\n",
      "epoch:27 step:21639 [D loss: 0.363652, acc.: 85.16%] [G loss: 5.915077]\n",
      "epoch:27 step:21640 [D loss: 0.503957, acc.: 75.78%] [G loss: 4.034156]\n",
      "epoch:27 step:21641 [D loss: 0.402130, acc.: 82.81%] [G loss: 3.148207]\n",
      "epoch:27 step:21642 [D loss: 0.237207, acc.: 89.84%] [G loss: 3.125682]\n",
      "epoch:27 step:21643 [D loss: 0.256738, acc.: 87.50%] [G loss: 2.981257]\n",
      "epoch:27 step:21644 [D loss: 0.341003, acc.: 84.38%] [G loss: 3.599372]\n",
      "epoch:27 step:21645 [D loss: 0.330987, acc.: 82.03%] [G loss: 3.884182]\n",
      "epoch:27 step:21646 [D loss: 0.269356, acc.: 89.06%] [G loss: 3.667525]\n",
      "epoch:27 step:21647 [D loss: 0.292919, acc.: 86.72%] [G loss: 3.467370]\n",
      "epoch:27 step:21648 [D loss: 0.344177, acc.: 85.16%] [G loss: 3.550299]\n",
      "epoch:27 step:21649 [D loss: 0.431422, acc.: 78.12%] [G loss: 3.380186]\n",
      "epoch:27 step:21650 [D loss: 0.256365, acc.: 88.28%] [G loss: 2.822993]\n",
      "epoch:27 step:21651 [D loss: 0.348021, acc.: 83.59%] [G loss: 2.690396]\n",
      "epoch:27 step:21652 [D loss: 0.433712, acc.: 80.47%] [G loss: 2.825079]\n",
      "epoch:27 step:21653 [D loss: 0.322091, acc.: 85.16%] [G loss: 3.266434]\n",
      "epoch:27 step:21654 [D loss: 0.392378, acc.: 77.34%] [G loss: 3.225773]\n",
      "epoch:27 step:21655 [D loss: 0.374208, acc.: 82.81%] [G loss: 2.542140]\n",
      "epoch:27 step:21656 [D loss: 0.433794, acc.: 78.91%] [G loss: 2.502086]\n",
      "epoch:27 step:21657 [D loss: 0.348328, acc.: 83.59%] [G loss: 2.543934]\n",
      "epoch:27 step:21658 [D loss: 0.336549, acc.: 81.25%] [G loss: 2.927468]\n",
      "epoch:27 step:21659 [D loss: 0.318526, acc.: 83.59%] [G loss: 3.191937]\n",
      "epoch:27 step:21660 [D loss: 0.299232, acc.: 88.28%] [G loss: 3.419437]\n",
      "epoch:27 step:21661 [D loss: 0.343123, acc.: 84.38%] [G loss: 3.165336]\n",
      "epoch:27 step:21662 [D loss: 0.354066, acc.: 82.03%] [G loss: 3.705164]\n",
      "epoch:27 step:21663 [D loss: 0.276460, acc.: 89.06%] [G loss: 3.443573]\n",
      "epoch:27 step:21664 [D loss: 0.305720, acc.: 83.59%] [G loss: 3.629171]\n",
      "epoch:27 step:21665 [D loss: 0.428592, acc.: 82.03%] [G loss: 4.877230]\n",
      "epoch:27 step:21666 [D loss: 0.345233, acc.: 82.03%] [G loss: 3.270563]\n",
      "epoch:27 step:21667 [D loss: 0.274122, acc.: 86.72%] [G loss: 4.125774]\n",
      "epoch:27 step:21668 [D loss: 0.329370, acc.: 81.25%] [G loss: 3.085041]\n",
      "epoch:27 step:21669 [D loss: 0.398862, acc.: 82.03%] [G loss: 2.498196]\n",
      "epoch:27 step:21670 [D loss: 0.390356, acc.: 80.47%] [G loss: 2.730467]\n",
      "epoch:27 step:21671 [D loss: 0.358584, acc.: 82.81%] [G loss: 2.370445]\n",
      "epoch:27 step:21672 [D loss: 0.310662, acc.: 85.94%] [G loss: 3.075825]\n",
      "epoch:27 step:21673 [D loss: 0.301990, acc.: 86.72%] [G loss: 3.486581]\n",
      "epoch:27 step:21674 [D loss: 0.290065, acc.: 86.72%] [G loss: 3.172210]\n",
      "epoch:27 step:21675 [D loss: 0.291773, acc.: 88.28%] [G loss: 2.881292]\n",
      "epoch:27 step:21676 [D loss: 0.429238, acc.: 77.34%] [G loss: 2.343286]\n",
      "epoch:27 step:21677 [D loss: 0.389505, acc.: 77.34%] [G loss: 3.828430]\n",
      "epoch:27 step:21678 [D loss: 0.476166, acc.: 77.34%] [G loss: 4.990235]\n",
      "epoch:27 step:21679 [D loss: 0.738284, acc.: 70.31%] [G loss: 4.556928]\n",
      "epoch:27 step:21680 [D loss: 0.717586, acc.: 67.19%] [G loss: 4.954805]\n",
      "epoch:27 step:21681 [D loss: 0.309345, acc.: 88.28%] [G loss: 5.360887]\n",
      "epoch:27 step:21682 [D loss: 0.756872, acc.: 69.53%] [G loss: 6.577161]\n",
      "epoch:27 step:21683 [D loss: 0.465854, acc.: 79.69%] [G loss: 2.846008]\n",
      "epoch:27 step:21684 [D loss: 0.438772, acc.: 75.78%] [G loss: 4.522162]\n",
      "epoch:27 step:21685 [D loss: 0.319901, acc.: 88.28%] [G loss: 4.646856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21686 [D loss: 0.247486, acc.: 89.06%] [G loss: 4.730012]\n",
      "epoch:27 step:21687 [D loss: 0.302217, acc.: 85.16%] [G loss: 2.531008]\n",
      "epoch:27 step:21688 [D loss: 0.285298, acc.: 85.16%] [G loss: 2.970388]\n",
      "epoch:27 step:21689 [D loss: 0.273395, acc.: 90.62%] [G loss: 3.240679]\n",
      "epoch:27 step:21690 [D loss: 0.242539, acc.: 87.50%] [G loss: 3.384266]\n",
      "epoch:27 step:21691 [D loss: 0.370693, acc.: 80.47%] [G loss: 4.000798]\n",
      "epoch:27 step:21692 [D loss: 0.360614, acc.: 80.47%] [G loss: 3.133275]\n",
      "epoch:27 step:21693 [D loss: 0.299019, acc.: 86.72%] [G loss: 2.286660]\n",
      "epoch:27 step:21694 [D loss: 0.251614, acc.: 87.50%] [G loss: 4.090233]\n",
      "epoch:27 step:21695 [D loss: 0.303995, acc.: 88.28%] [G loss: 3.250412]\n",
      "epoch:27 step:21696 [D loss: 0.289871, acc.: 90.62%] [G loss: 3.264364]\n",
      "epoch:27 step:21697 [D loss: 0.395986, acc.: 81.25%] [G loss: 2.786077]\n",
      "epoch:27 step:21698 [D loss: 0.414984, acc.: 78.91%] [G loss: 2.838240]\n",
      "epoch:27 step:21699 [D loss: 0.266339, acc.: 85.16%] [G loss: 3.075985]\n",
      "epoch:27 step:21700 [D loss: 0.309558, acc.: 85.94%] [G loss: 3.328802]\n",
      "epoch:27 step:21701 [D loss: 0.378274, acc.: 80.47%] [G loss: 2.802916]\n",
      "epoch:27 step:21702 [D loss: 0.275883, acc.: 89.84%] [G loss: 3.072886]\n",
      "epoch:27 step:21703 [D loss: 0.233673, acc.: 89.06%] [G loss: 3.287253]\n",
      "epoch:27 step:21704 [D loss: 0.341939, acc.: 83.59%] [G loss: 2.791246]\n",
      "epoch:27 step:21705 [D loss: 0.309182, acc.: 87.50%] [G loss: 3.399985]\n",
      "epoch:27 step:21706 [D loss: 0.317194, acc.: 85.16%] [G loss: 2.593376]\n",
      "epoch:27 step:21707 [D loss: 0.310551, acc.: 85.16%] [G loss: 3.438543]\n",
      "epoch:27 step:21708 [D loss: 0.294386, acc.: 85.94%] [G loss: 2.932963]\n",
      "epoch:27 step:21709 [D loss: 0.286675, acc.: 89.06%] [G loss: 3.224511]\n",
      "epoch:27 step:21710 [D loss: 0.321503, acc.: 85.16%] [G loss: 3.517248]\n",
      "epoch:27 step:21711 [D loss: 0.319731, acc.: 89.06%] [G loss: 3.306782]\n",
      "epoch:27 step:21712 [D loss: 0.389084, acc.: 81.25%] [G loss: 2.800436]\n",
      "epoch:27 step:21713 [D loss: 0.263776, acc.: 88.28%] [G loss: 2.259412]\n",
      "epoch:27 step:21714 [D loss: 0.398946, acc.: 82.03%] [G loss: 2.441166]\n",
      "epoch:27 step:21715 [D loss: 0.400462, acc.: 79.69%] [G loss: 2.431203]\n",
      "epoch:27 step:21716 [D loss: 0.375155, acc.: 84.38%] [G loss: 1.873098]\n",
      "epoch:27 step:21717 [D loss: 0.245515, acc.: 90.62%] [G loss: 2.435049]\n",
      "epoch:27 step:21718 [D loss: 0.359994, acc.: 85.94%] [G loss: 2.521260]\n",
      "epoch:27 step:21719 [D loss: 0.382155, acc.: 82.81%] [G loss: 2.669103]\n",
      "epoch:27 step:21720 [D loss: 0.296264, acc.: 90.62%] [G loss: 3.819613]\n",
      "epoch:27 step:21721 [D loss: 0.412158, acc.: 81.25%] [G loss: 2.515516]\n",
      "epoch:27 step:21722 [D loss: 0.387395, acc.: 82.03%] [G loss: 3.036973]\n",
      "epoch:27 step:21723 [D loss: 0.260607, acc.: 86.72%] [G loss: 2.712266]\n",
      "epoch:27 step:21724 [D loss: 0.400358, acc.: 80.47%] [G loss: 3.256990]\n",
      "epoch:27 step:21725 [D loss: 0.282499, acc.: 89.84%] [G loss: 2.785836]\n",
      "epoch:27 step:21726 [D loss: 0.329736, acc.: 87.50%] [G loss: 3.274081]\n",
      "epoch:27 step:21727 [D loss: 0.309911, acc.: 84.38%] [G loss: 2.984518]\n",
      "epoch:27 step:21728 [D loss: 0.351000, acc.: 87.50%] [G loss: 3.618646]\n",
      "epoch:27 step:21729 [D loss: 0.243895, acc.: 90.62%] [G loss: 3.316453]\n",
      "epoch:27 step:21730 [D loss: 0.404961, acc.: 80.47%] [G loss: 2.944189]\n",
      "epoch:27 step:21731 [D loss: 0.410750, acc.: 77.34%] [G loss: 4.148917]\n",
      "epoch:27 step:21732 [D loss: 0.255351, acc.: 87.50%] [G loss: 2.640983]\n",
      "epoch:27 step:21733 [D loss: 0.290296, acc.: 89.06%] [G loss: 3.925864]\n",
      "epoch:27 step:21734 [D loss: 0.359325, acc.: 84.38%] [G loss: 3.882755]\n",
      "epoch:27 step:21735 [D loss: 0.274071, acc.: 88.28%] [G loss: 3.323888]\n",
      "epoch:27 step:21736 [D loss: 0.309394, acc.: 85.16%] [G loss: 2.871378]\n",
      "epoch:27 step:21737 [D loss: 0.306370, acc.: 88.28%] [G loss: 2.962744]\n",
      "epoch:27 step:21738 [D loss: 0.234225, acc.: 89.84%] [G loss: 3.264375]\n",
      "epoch:27 step:21739 [D loss: 0.244612, acc.: 92.19%] [G loss: 2.828403]\n",
      "epoch:27 step:21740 [D loss: 0.376973, acc.: 80.47%] [G loss: 3.246189]\n",
      "epoch:27 step:21741 [D loss: 0.351042, acc.: 83.59%] [G loss: 3.583975]\n",
      "epoch:27 step:21742 [D loss: 0.254082, acc.: 89.06%] [G loss: 3.068488]\n",
      "epoch:27 step:21743 [D loss: 0.361711, acc.: 80.47%] [G loss: 3.124964]\n",
      "epoch:27 step:21744 [D loss: 0.460227, acc.: 82.03%] [G loss: 2.963409]\n",
      "epoch:27 step:21745 [D loss: 0.278002, acc.: 87.50%] [G loss: 3.439288]\n",
      "epoch:27 step:21746 [D loss: 0.319204, acc.: 85.16%] [G loss: 3.576617]\n",
      "epoch:27 step:21747 [D loss: 0.400313, acc.: 80.47%] [G loss: 3.396818]\n",
      "epoch:27 step:21748 [D loss: 0.382897, acc.: 82.03%] [G loss: 3.633143]\n",
      "epoch:27 step:21749 [D loss: 0.297993, acc.: 85.16%] [G loss: 3.992562]\n",
      "epoch:27 step:21750 [D loss: 0.271015, acc.: 85.16%] [G loss: 2.666983]\n",
      "epoch:27 step:21751 [D loss: 0.325575, acc.: 85.16%] [G loss: 3.555416]\n",
      "epoch:27 step:21752 [D loss: 0.303934, acc.: 85.94%] [G loss: 2.865279]\n",
      "epoch:27 step:21753 [D loss: 0.344361, acc.: 87.50%] [G loss: 3.883622]\n",
      "epoch:27 step:21754 [D loss: 0.335080, acc.: 85.16%] [G loss: 3.150168]\n",
      "epoch:27 step:21755 [D loss: 0.438381, acc.: 78.91%] [G loss: 3.428693]\n",
      "epoch:27 step:21756 [D loss: 0.360999, acc.: 84.38%] [G loss: 2.768596]\n",
      "epoch:27 step:21757 [D loss: 0.249117, acc.: 89.06%] [G loss: 2.933032]\n",
      "epoch:27 step:21758 [D loss: 0.301338, acc.: 86.72%] [G loss: 2.692173]\n",
      "epoch:27 step:21759 [D loss: 0.270381, acc.: 89.06%] [G loss: 3.382646]\n",
      "epoch:27 step:21760 [D loss: 0.399741, acc.: 84.38%] [G loss: 2.577349]\n",
      "epoch:27 step:21761 [D loss: 0.349732, acc.: 85.16%] [G loss: 2.935139]\n",
      "epoch:27 step:21762 [D loss: 0.347424, acc.: 85.16%] [G loss: 3.657177]\n",
      "epoch:27 step:21763 [D loss: 0.469379, acc.: 74.22%] [G loss: 2.716682]\n",
      "epoch:27 step:21764 [D loss: 0.306569, acc.: 88.28%] [G loss: 2.837315]\n",
      "epoch:27 step:21765 [D loss: 0.307198, acc.: 87.50%] [G loss: 3.087127]\n",
      "epoch:27 step:21766 [D loss: 0.359997, acc.: 83.59%] [G loss: 3.241934]\n",
      "epoch:27 step:21767 [D loss: 0.333434, acc.: 83.59%] [G loss: 3.206323]\n",
      "epoch:27 step:21768 [D loss: 0.330393, acc.: 89.06%] [G loss: 2.934035]\n",
      "epoch:27 step:21769 [D loss: 0.352733, acc.: 80.47%] [G loss: 3.144995]\n",
      "epoch:27 step:21770 [D loss: 0.321504, acc.: 85.16%] [G loss: 2.527208]\n",
      "epoch:27 step:21771 [D loss: 0.389161, acc.: 81.25%] [G loss: 2.503506]\n",
      "epoch:27 step:21772 [D loss: 0.465350, acc.: 73.44%] [G loss: 2.998900]\n",
      "epoch:27 step:21773 [D loss: 0.285333, acc.: 90.62%] [G loss: 3.139535]\n",
      "epoch:27 step:21774 [D loss: 0.319568, acc.: 85.94%] [G loss: 3.964970]\n",
      "epoch:27 step:21775 [D loss: 0.409053, acc.: 81.25%] [G loss: 2.722514]\n",
      "epoch:27 step:21776 [D loss: 0.316665, acc.: 85.16%] [G loss: 4.090744]\n",
      "epoch:27 step:21777 [D loss: 0.296240, acc.: 89.84%] [G loss: 3.751382]\n",
      "epoch:27 step:21778 [D loss: 0.258983, acc.: 88.28%] [G loss: 2.833000]\n",
      "epoch:27 step:21779 [D loss: 0.310538, acc.: 85.94%] [G loss: 3.819769]\n",
      "epoch:27 step:21780 [D loss: 0.220734, acc.: 92.97%] [G loss: 2.843325]\n",
      "epoch:27 step:21781 [D loss: 0.326905, acc.: 84.38%] [G loss: 2.479359]\n",
      "epoch:27 step:21782 [D loss: 0.289448, acc.: 85.94%] [G loss: 3.043011]\n",
      "epoch:27 step:21783 [D loss: 0.378823, acc.: 78.91%] [G loss: 2.917398]\n",
      "epoch:27 step:21784 [D loss: 0.382725, acc.: 81.25%] [G loss: 2.784301]\n",
      "epoch:27 step:21785 [D loss: 0.256542, acc.: 88.28%] [G loss: 2.807912]\n",
      "epoch:27 step:21786 [D loss: 0.348579, acc.: 85.94%] [G loss: 2.259604]\n",
      "epoch:27 step:21787 [D loss: 0.319548, acc.: 87.50%] [G loss: 2.715774]\n",
      "epoch:27 step:21788 [D loss: 0.359011, acc.: 81.25%] [G loss: 3.405067]\n",
      "epoch:27 step:21789 [D loss: 0.313355, acc.: 85.94%] [G loss: 2.441229]\n",
      "epoch:27 step:21790 [D loss: 0.318738, acc.: 85.16%] [G loss: 2.480759]\n",
      "epoch:27 step:21791 [D loss: 0.401553, acc.: 79.69%] [G loss: 2.508601]\n",
      "epoch:27 step:21792 [D loss: 0.333053, acc.: 85.16%] [G loss: 2.646549]\n",
      "epoch:27 step:21793 [D loss: 0.463939, acc.: 78.12%] [G loss: 2.816744]\n",
      "epoch:27 step:21794 [D loss: 0.420723, acc.: 81.25%] [G loss: 3.211467]\n",
      "epoch:27 step:21795 [D loss: 0.371832, acc.: 83.59%] [G loss: 3.323379]\n",
      "epoch:27 step:21796 [D loss: 0.372012, acc.: 82.81%] [G loss: 3.016205]\n",
      "epoch:27 step:21797 [D loss: 0.403308, acc.: 82.03%] [G loss: 2.979171]\n",
      "epoch:27 step:21798 [D loss: 0.314841, acc.: 86.72%] [G loss: 2.986432]\n",
      "epoch:27 step:21799 [D loss: 0.336183, acc.: 85.16%] [G loss: 3.749246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21800 [D loss: 0.292131, acc.: 85.94%] [G loss: 2.925107]\n",
      "epoch:27 step:21801 [D loss: 0.282451, acc.: 85.16%] [G loss: 3.009101]\n",
      "epoch:27 step:21802 [D loss: 0.338172, acc.: 85.94%] [G loss: 2.985739]\n",
      "epoch:27 step:21803 [D loss: 0.332910, acc.: 85.16%] [G loss: 2.969283]\n",
      "epoch:27 step:21804 [D loss: 0.259016, acc.: 89.84%] [G loss: 3.530690]\n",
      "epoch:27 step:21805 [D loss: 0.306665, acc.: 89.84%] [G loss: 3.507661]\n",
      "epoch:27 step:21806 [D loss: 0.377896, acc.: 83.59%] [G loss: 3.171487]\n",
      "epoch:27 step:21807 [D loss: 0.331876, acc.: 85.16%] [G loss: 3.655901]\n",
      "epoch:27 step:21808 [D loss: 0.248898, acc.: 92.19%] [G loss: 2.697545]\n",
      "epoch:27 step:21809 [D loss: 0.254156, acc.: 91.41%] [G loss: 3.272453]\n",
      "epoch:27 step:21810 [D loss: 0.293287, acc.: 87.50%] [G loss: 3.809370]\n",
      "epoch:27 step:21811 [D loss: 0.250201, acc.: 89.84%] [G loss: 2.852787]\n",
      "epoch:27 step:21812 [D loss: 0.296722, acc.: 84.38%] [G loss: 3.207151]\n",
      "epoch:27 step:21813 [D loss: 0.287013, acc.: 89.06%] [G loss: 2.422728]\n",
      "epoch:27 step:21814 [D loss: 0.286371, acc.: 87.50%] [G loss: 4.339402]\n",
      "epoch:27 step:21815 [D loss: 0.280640, acc.: 88.28%] [G loss: 2.896261]\n",
      "epoch:27 step:21816 [D loss: 0.296520, acc.: 87.50%] [G loss: 3.502231]\n",
      "epoch:27 step:21817 [D loss: 0.189000, acc.: 92.19%] [G loss: 3.046380]\n",
      "epoch:27 step:21818 [D loss: 0.244992, acc.: 90.62%] [G loss: 3.399140]\n",
      "epoch:27 step:21819 [D loss: 0.376116, acc.: 80.47%] [G loss: 3.686380]\n",
      "epoch:27 step:21820 [D loss: 0.284644, acc.: 88.28%] [G loss: 3.613017]\n",
      "epoch:27 step:21821 [D loss: 0.261278, acc.: 89.06%] [G loss: 5.535493]\n",
      "epoch:27 step:21822 [D loss: 0.282778, acc.: 86.72%] [G loss: 4.874888]\n",
      "epoch:27 step:21823 [D loss: 0.258547, acc.: 89.06%] [G loss: 4.033181]\n",
      "epoch:27 step:21824 [D loss: 0.333556, acc.: 80.47%] [G loss: 4.880117]\n",
      "epoch:27 step:21825 [D loss: 0.373104, acc.: 80.47%] [G loss: 2.867924]\n",
      "epoch:27 step:21826 [D loss: 0.294044, acc.: 86.72%] [G loss: 2.645715]\n",
      "epoch:27 step:21827 [D loss: 0.261575, acc.: 90.62%] [G loss: 3.659255]\n",
      "epoch:27 step:21828 [D loss: 0.384966, acc.: 81.25%] [G loss: 3.098773]\n",
      "epoch:27 step:21829 [D loss: 0.304652, acc.: 89.06%] [G loss: 2.894735]\n",
      "epoch:27 step:21830 [D loss: 0.359798, acc.: 83.59%] [G loss: 6.033693]\n",
      "epoch:27 step:21831 [D loss: 0.298090, acc.: 89.06%] [G loss: 5.613972]\n",
      "epoch:27 step:21832 [D loss: 0.383670, acc.: 86.72%] [G loss: 8.887815]\n",
      "epoch:27 step:21833 [D loss: 0.287113, acc.: 85.94%] [G loss: 4.496075]\n",
      "epoch:27 step:21834 [D loss: 0.222581, acc.: 91.41%] [G loss: 4.644778]\n",
      "epoch:27 step:21835 [D loss: 0.204361, acc.: 91.41%] [G loss: 4.973000]\n",
      "epoch:27 step:21836 [D loss: 0.305173, acc.: 86.72%] [G loss: 3.546230]\n",
      "epoch:27 step:21837 [D loss: 0.314079, acc.: 87.50%] [G loss: 3.601712]\n",
      "epoch:27 step:21838 [D loss: 0.386519, acc.: 89.06%] [G loss: 3.635761]\n",
      "epoch:27 step:21839 [D loss: 0.260874, acc.: 89.06%] [G loss: 2.637093]\n",
      "epoch:27 step:21840 [D loss: 0.395261, acc.: 82.81%] [G loss: 2.984899]\n",
      "epoch:27 step:21841 [D loss: 0.325140, acc.: 81.25%] [G loss: 3.672685]\n",
      "epoch:27 step:21842 [D loss: 0.346301, acc.: 86.72%] [G loss: 3.356680]\n",
      "epoch:27 step:21843 [D loss: 0.278796, acc.: 83.59%] [G loss: 4.358942]\n",
      "epoch:27 step:21844 [D loss: 0.309109, acc.: 88.28%] [G loss: 3.398006]\n",
      "epoch:27 step:21845 [D loss: 0.334956, acc.: 80.47%] [G loss: 4.451606]\n",
      "epoch:27 step:21846 [D loss: 0.198036, acc.: 93.75%] [G loss: 4.808294]\n",
      "epoch:27 step:21847 [D loss: 0.289744, acc.: 85.94%] [G loss: 4.374207]\n",
      "epoch:27 step:21848 [D loss: 0.351474, acc.: 85.16%] [G loss: 3.895832]\n",
      "epoch:27 step:21849 [D loss: 0.328602, acc.: 85.16%] [G loss: 3.359594]\n",
      "epoch:27 step:21850 [D loss: 0.351881, acc.: 82.03%] [G loss: 3.656297]\n",
      "epoch:27 step:21851 [D loss: 0.289186, acc.: 85.94%] [G loss: 3.180236]\n",
      "epoch:27 step:21852 [D loss: 0.465046, acc.: 71.09%] [G loss: 2.582072]\n",
      "epoch:27 step:21853 [D loss: 0.352499, acc.: 85.94%] [G loss: 2.890277]\n",
      "epoch:27 step:21854 [D loss: 0.345256, acc.: 86.72%] [G loss: 2.707644]\n",
      "epoch:27 step:21855 [D loss: 0.301440, acc.: 85.16%] [G loss: 3.463277]\n",
      "epoch:27 step:21856 [D loss: 0.328713, acc.: 85.16%] [G loss: 2.454926]\n",
      "epoch:27 step:21857 [D loss: 0.332720, acc.: 85.16%] [G loss: 2.418164]\n",
      "epoch:27 step:21858 [D loss: 0.407787, acc.: 82.03%] [G loss: 2.722435]\n",
      "epoch:27 step:21859 [D loss: 0.326135, acc.: 86.72%] [G loss: 3.440857]\n",
      "epoch:27 step:21860 [D loss: 0.403946, acc.: 80.47%] [G loss: 5.672573]\n",
      "epoch:27 step:21861 [D loss: 0.807048, acc.: 76.56%] [G loss: 8.736471]\n",
      "epoch:27 step:21862 [D loss: 2.387230, acc.: 43.75%] [G loss: 3.708660]\n",
      "epoch:27 step:21863 [D loss: 0.702945, acc.: 74.22%] [G loss: 4.951281]\n",
      "epoch:27 step:21864 [D loss: 0.652054, acc.: 74.22%] [G loss: 5.693383]\n",
      "epoch:27 step:21865 [D loss: 0.809444, acc.: 78.91%] [G loss: 4.985747]\n",
      "epoch:27 step:21866 [D loss: 0.358608, acc.: 85.16%] [G loss: 8.957534]\n",
      "epoch:27 step:21867 [D loss: 0.350551, acc.: 82.81%] [G loss: 3.723954]\n",
      "epoch:27 step:21868 [D loss: 0.300925, acc.: 87.50%] [G loss: 5.291450]\n",
      "epoch:28 step:21869 [D loss: 0.250002, acc.: 85.94%] [G loss: 4.380098]\n",
      "epoch:28 step:21870 [D loss: 0.285878, acc.: 85.94%] [G loss: 4.743798]\n",
      "epoch:28 step:21871 [D loss: 0.447091, acc.: 80.47%] [G loss: 3.146373]\n",
      "epoch:28 step:21872 [D loss: 0.270326, acc.: 85.94%] [G loss: 3.052278]\n",
      "epoch:28 step:21873 [D loss: 0.273918, acc.: 88.28%] [G loss: 2.811432]\n",
      "epoch:28 step:21874 [D loss: 0.282575, acc.: 87.50%] [G loss: 2.426453]\n",
      "epoch:28 step:21875 [D loss: 0.248948, acc.: 90.62%] [G loss: 2.335504]\n",
      "epoch:28 step:21876 [D loss: 0.227254, acc.: 90.62%] [G loss: 2.820285]\n",
      "epoch:28 step:21877 [D loss: 0.275043, acc.: 88.28%] [G loss: 2.571641]\n",
      "epoch:28 step:21878 [D loss: 0.227586, acc.: 91.41%] [G loss: 3.544875]\n",
      "epoch:28 step:21879 [D loss: 0.359334, acc.: 85.94%] [G loss: 2.212973]\n",
      "epoch:28 step:21880 [D loss: 0.315813, acc.: 85.16%] [G loss: 2.837014]\n",
      "epoch:28 step:21881 [D loss: 0.407319, acc.: 83.59%] [G loss: 2.561944]\n",
      "epoch:28 step:21882 [D loss: 0.259303, acc.: 90.62%] [G loss: 3.442382]\n",
      "epoch:28 step:21883 [D loss: 0.380967, acc.: 82.81%] [G loss: 2.289429]\n",
      "epoch:28 step:21884 [D loss: 0.364821, acc.: 85.94%] [G loss: 2.609426]\n",
      "epoch:28 step:21885 [D loss: 0.288508, acc.: 89.06%] [G loss: 2.546347]\n",
      "epoch:28 step:21886 [D loss: 0.427298, acc.: 82.03%] [G loss: 2.470557]\n",
      "epoch:28 step:21887 [D loss: 0.411440, acc.: 82.03%] [G loss: 2.833959]\n",
      "epoch:28 step:21888 [D loss: 0.269543, acc.: 91.41%] [G loss: 2.535248]\n",
      "epoch:28 step:21889 [D loss: 0.335664, acc.: 85.16%] [G loss: 2.751713]\n",
      "epoch:28 step:21890 [D loss: 0.301833, acc.: 89.84%] [G loss: 2.644694]\n",
      "epoch:28 step:21891 [D loss: 0.318466, acc.: 85.94%] [G loss: 2.730955]\n",
      "epoch:28 step:21892 [D loss: 0.425395, acc.: 81.25%] [G loss: 2.347230]\n",
      "epoch:28 step:21893 [D loss: 0.281677, acc.: 87.50%] [G loss: 2.692446]\n",
      "epoch:28 step:21894 [D loss: 0.325133, acc.: 85.94%] [G loss: 2.499022]\n",
      "epoch:28 step:21895 [D loss: 0.273239, acc.: 88.28%] [G loss: 2.446773]\n",
      "epoch:28 step:21896 [D loss: 0.308376, acc.: 84.38%] [G loss: 2.903252]\n",
      "epoch:28 step:21897 [D loss: 0.383984, acc.: 85.94%] [G loss: 3.667623]\n",
      "epoch:28 step:21898 [D loss: 0.243916, acc.: 88.28%] [G loss: 3.510286]\n",
      "epoch:28 step:21899 [D loss: 0.530024, acc.: 73.44%] [G loss: 3.225483]\n",
      "epoch:28 step:21900 [D loss: 0.391738, acc.: 80.47%] [G loss: 3.775160]\n",
      "epoch:28 step:21901 [D loss: 0.240725, acc.: 87.50%] [G loss: 4.071195]\n",
      "epoch:28 step:21902 [D loss: 0.423581, acc.: 78.91%] [G loss: 3.168488]\n",
      "epoch:28 step:21903 [D loss: 0.274203, acc.: 88.28%] [G loss: 3.713734]\n",
      "epoch:28 step:21904 [D loss: 0.295974, acc.: 87.50%] [G loss: 3.366989]\n",
      "epoch:28 step:21905 [D loss: 0.330706, acc.: 84.38%] [G loss: 5.000618]\n",
      "epoch:28 step:21906 [D loss: 0.361406, acc.: 84.38%] [G loss: 3.228754]\n",
      "epoch:28 step:21907 [D loss: 0.297449, acc.: 85.94%] [G loss: 4.437415]\n",
      "epoch:28 step:21908 [D loss: 0.216578, acc.: 90.62%] [G loss: 5.871974]\n",
      "epoch:28 step:21909 [D loss: 0.248002, acc.: 89.84%] [G loss: 4.718620]\n",
      "epoch:28 step:21910 [D loss: 0.279040, acc.: 90.62%] [G loss: 4.334852]\n",
      "epoch:28 step:21911 [D loss: 0.331409, acc.: 84.38%] [G loss: 3.302052]\n",
      "epoch:28 step:21912 [D loss: 0.294446, acc.: 89.84%] [G loss: 3.766248]\n",
      "epoch:28 step:21913 [D loss: 0.303633, acc.: 85.94%] [G loss: 3.029921]\n",
      "epoch:28 step:21914 [D loss: 0.309367, acc.: 89.06%] [G loss: 3.523452]\n",
      "epoch:28 step:21915 [D loss: 0.344676, acc.: 87.50%] [G loss: 3.467478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:21916 [D loss: 0.347884, acc.: 84.38%] [G loss: 3.061975]\n",
      "epoch:28 step:21917 [D loss: 0.199874, acc.: 92.97%] [G loss: 4.374240]\n",
      "epoch:28 step:21918 [D loss: 0.295610, acc.: 85.16%] [G loss: 4.490137]\n",
      "epoch:28 step:21919 [D loss: 0.330113, acc.: 84.38%] [G loss: 2.915817]\n",
      "epoch:28 step:21920 [D loss: 0.265998, acc.: 88.28%] [G loss: 3.246848]\n",
      "epoch:28 step:21921 [D loss: 0.280056, acc.: 86.72%] [G loss: 2.543535]\n",
      "epoch:28 step:21922 [D loss: 0.307337, acc.: 85.94%] [G loss: 2.612889]\n",
      "epoch:28 step:21923 [D loss: 0.308511, acc.: 86.72%] [G loss: 2.476959]\n",
      "epoch:28 step:21924 [D loss: 0.321156, acc.: 84.38%] [G loss: 3.703970]\n",
      "epoch:28 step:21925 [D loss: 0.326988, acc.: 86.72%] [G loss: 3.783029]\n",
      "epoch:28 step:21926 [D loss: 0.331915, acc.: 85.16%] [G loss: 2.252319]\n",
      "epoch:28 step:21927 [D loss: 0.383947, acc.: 82.03%] [G loss: 3.894640]\n",
      "epoch:28 step:21928 [D loss: 0.242446, acc.: 90.62%] [G loss: 5.251801]\n",
      "epoch:28 step:21929 [D loss: 0.366843, acc.: 83.59%] [G loss: 3.705891]\n",
      "epoch:28 step:21930 [D loss: 0.317087, acc.: 85.94%] [G loss: 3.571448]\n",
      "epoch:28 step:21931 [D loss: 0.286002, acc.: 85.94%] [G loss: 3.015444]\n",
      "epoch:28 step:21932 [D loss: 0.317751, acc.: 87.50%] [G loss: 2.949220]\n",
      "epoch:28 step:21933 [D loss: 0.486947, acc.: 79.69%] [G loss: 2.770213]\n",
      "epoch:28 step:21934 [D loss: 0.472813, acc.: 78.91%] [G loss: 2.891784]\n",
      "epoch:28 step:21935 [D loss: 0.394141, acc.: 80.47%] [G loss: 3.646286]\n",
      "epoch:28 step:21936 [D loss: 0.316108, acc.: 83.59%] [G loss: 2.613844]\n",
      "epoch:28 step:21937 [D loss: 0.374862, acc.: 85.94%] [G loss: 3.734495]\n",
      "epoch:28 step:21938 [D loss: 0.408619, acc.: 77.34%] [G loss: 2.618879]\n",
      "epoch:28 step:21939 [D loss: 0.350420, acc.: 85.94%] [G loss: 2.787769]\n",
      "epoch:28 step:21940 [D loss: 0.354767, acc.: 84.38%] [G loss: 2.732675]\n",
      "epoch:28 step:21941 [D loss: 0.274485, acc.: 89.84%] [G loss: 2.535152]\n",
      "epoch:28 step:21942 [D loss: 0.261130, acc.: 89.06%] [G loss: 3.537196]\n",
      "epoch:28 step:21943 [D loss: 0.362954, acc.: 83.59%] [G loss: 2.621701]\n",
      "epoch:28 step:21944 [D loss: 0.352780, acc.: 85.94%] [G loss: 3.472162]\n",
      "epoch:28 step:21945 [D loss: 0.398734, acc.: 81.25%] [G loss: 2.836317]\n",
      "epoch:28 step:21946 [D loss: 0.263565, acc.: 86.72%] [G loss: 3.126584]\n",
      "epoch:28 step:21947 [D loss: 0.314133, acc.: 85.16%] [G loss: 3.363043]\n",
      "epoch:28 step:21948 [D loss: 0.323999, acc.: 86.72%] [G loss: 3.193562]\n",
      "epoch:28 step:21949 [D loss: 0.275773, acc.: 87.50%] [G loss: 3.545481]\n",
      "epoch:28 step:21950 [D loss: 0.287740, acc.: 89.06%] [G loss: 3.028342]\n",
      "epoch:28 step:21951 [D loss: 0.235692, acc.: 91.41%] [G loss: 3.048441]\n",
      "epoch:28 step:21952 [D loss: 0.316425, acc.: 85.94%] [G loss: 3.048211]\n",
      "epoch:28 step:21953 [D loss: 0.269109, acc.: 90.62%] [G loss: 3.009516]\n",
      "epoch:28 step:21954 [D loss: 0.412966, acc.: 76.56%] [G loss: 3.265370]\n",
      "epoch:28 step:21955 [D loss: 0.378530, acc.: 85.16%] [G loss: 2.830985]\n",
      "epoch:28 step:21956 [D loss: 0.418025, acc.: 79.69%] [G loss: 2.863855]\n",
      "epoch:28 step:21957 [D loss: 0.366452, acc.: 82.03%] [G loss: 3.925752]\n",
      "epoch:28 step:21958 [D loss: 0.376301, acc.: 82.81%] [G loss: 3.021830]\n",
      "epoch:28 step:21959 [D loss: 0.370061, acc.: 82.81%] [G loss: 4.513416]\n",
      "epoch:28 step:21960 [D loss: 0.362547, acc.: 81.25%] [G loss: 3.874286]\n",
      "epoch:28 step:21961 [D loss: 0.367853, acc.: 84.38%] [G loss: 2.895845]\n",
      "epoch:28 step:21962 [D loss: 0.225571, acc.: 89.84%] [G loss: 3.176349]\n",
      "epoch:28 step:21963 [D loss: 0.304638, acc.: 84.38%] [G loss: 2.647376]\n",
      "epoch:28 step:21964 [D loss: 0.308039, acc.: 84.38%] [G loss: 3.069046]\n",
      "epoch:28 step:21965 [D loss: 0.346728, acc.: 84.38%] [G loss: 2.849883]\n",
      "epoch:28 step:21966 [D loss: 0.327021, acc.: 85.94%] [G loss: 2.702569]\n",
      "epoch:28 step:21967 [D loss: 0.423469, acc.: 82.81%] [G loss: 3.995197]\n",
      "epoch:28 step:21968 [D loss: 0.380521, acc.: 82.81%] [G loss: 3.467644]\n",
      "epoch:28 step:21969 [D loss: 0.395815, acc.: 79.69%] [G loss: 3.020926]\n",
      "epoch:28 step:21970 [D loss: 0.291165, acc.: 85.94%] [G loss: 2.739693]\n",
      "epoch:28 step:21971 [D loss: 0.418452, acc.: 80.47%] [G loss: 4.707798]\n",
      "epoch:28 step:21972 [D loss: 0.376106, acc.: 84.38%] [G loss: 3.671054]\n",
      "epoch:28 step:21973 [D loss: 0.310434, acc.: 85.16%] [G loss: 3.891933]\n",
      "epoch:28 step:21974 [D loss: 0.368850, acc.: 82.03%] [G loss: 4.336266]\n",
      "epoch:28 step:21975 [D loss: 0.388533, acc.: 79.69%] [G loss: 4.426830]\n",
      "epoch:28 step:21976 [D loss: 0.227782, acc.: 92.19%] [G loss: 3.430423]\n",
      "epoch:28 step:21977 [D loss: 0.291028, acc.: 87.50%] [G loss: 2.654272]\n",
      "epoch:28 step:21978 [D loss: 0.378319, acc.: 84.38%] [G loss: 2.690829]\n",
      "epoch:28 step:21979 [D loss: 0.354822, acc.: 84.38%] [G loss: 2.248661]\n",
      "epoch:28 step:21980 [D loss: 0.349243, acc.: 82.81%] [G loss: 2.728850]\n",
      "epoch:28 step:21981 [D loss: 0.339570, acc.: 85.94%] [G loss: 3.215153]\n",
      "epoch:28 step:21982 [D loss: 0.261035, acc.: 88.28%] [G loss: 2.949668]\n",
      "epoch:28 step:21983 [D loss: 0.289574, acc.: 89.06%] [G loss: 2.813312]\n",
      "epoch:28 step:21984 [D loss: 0.406230, acc.: 78.91%] [G loss: 3.020649]\n",
      "epoch:28 step:21985 [D loss: 0.275506, acc.: 86.72%] [G loss: 3.740993]\n",
      "epoch:28 step:21986 [D loss: 0.251523, acc.: 88.28%] [G loss: 2.649973]\n",
      "epoch:28 step:21987 [D loss: 0.326520, acc.: 88.28%] [G loss: 2.836698]\n",
      "epoch:28 step:21988 [D loss: 0.330374, acc.: 85.94%] [G loss: 2.836954]\n",
      "epoch:28 step:21989 [D loss: 0.292569, acc.: 87.50%] [G loss: 2.631273]\n",
      "epoch:28 step:21990 [D loss: 0.275665, acc.: 89.84%] [G loss: 3.500214]\n",
      "epoch:28 step:21991 [D loss: 0.234225, acc.: 88.28%] [G loss: 2.907045]\n",
      "epoch:28 step:21992 [D loss: 0.379535, acc.: 83.59%] [G loss: 2.743927]\n",
      "epoch:28 step:21993 [D loss: 0.332440, acc.: 85.16%] [G loss: 2.996373]\n",
      "epoch:28 step:21994 [D loss: 0.328771, acc.: 85.94%] [G loss: 2.529792]\n",
      "epoch:28 step:21995 [D loss: 0.258695, acc.: 90.62%] [G loss: 2.587178]\n",
      "epoch:28 step:21996 [D loss: 0.241606, acc.: 90.62%] [G loss: 2.365253]\n",
      "epoch:28 step:21997 [D loss: 0.319099, acc.: 87.50%] [G loss: 3.001649]\n",
      "epoch:28 step:21998 [D loss: 0.336856, acc.: 84.38%] [G loss: 3.246166]\n",
      "epoch:28 step:21999 [D loss: 0.357194, acc.: 79.69%] [G loss: 2.552556]\n",
      "epoch:28 step:22000 [D loss: 0.305437, acc.: 85.16%] [G loss: 2.637285]\n",
      "epoch:28 step:22001 [D loss: 0.292444, acc.: 89.84%] [G loss: 2.628178]\n",
      "epoch:28 step:22002 [D loss: 0.240620, acc.: 92.97%] [G loss: 2.341658]\n",
      "epoch:28 step:22003 [D loss: 0.345054, acc.: 85.16%] [G loss: 2.144307]\n",
      "epoch:28 step:22004 [D loss: 0.348693, acc.: 89.06%] [G loss: 4.029573]\n",
      "epoch:28 step:22005 [D loss: 0.295661, acc.: 85.94%] [G loss: 5.001224]\n",
      "epoch:28 step:22006 [D loss: 0.254075, acc.: 86.72%] [G loss: 2.860617]\n",
      "epoch:28 step:22007 [D loss: 0.231203, acc.: 91.41%] [G loss: 4.486813]\n",
      "epoch:28 step:22008 [D loss: 0.275809, acc.: 84.38%] [G loss: 3.712962]\n",
      "epoch:28 step:22009 [D loss: 0.391034, acc.: 85.16%] [G loss: 3.456410]\n",
      "epoch:28 step:22010 [D loss: 0.447243, acc.: 81.25%] [G loss: 2.671856]\n",
      "epoch:28 step:22011 [D loss: 0.247683, acc.: 89.84%] [G loss: 3.139616]\n",
      "epoch:28 step:22012 [D loss: 0.356012, acc.: 85.16%] [G loss: 4.398135]\n",
      "epoch:28 step:22013 [D loss: 0.338559, acc.: 85.16%] [G loss: 3.027033]\n",
      "epoch:28 step:22014 [D loss: 0.366105, acc.: 79.69%] [G loss: 2.909755]\n",
      "epoch:28 step:22015 [D loss: 0.431804, acc.: 78.91%] [G loss: 2.221483]\n",
      "epoch:28 step:22016 [D loss: 0.300609, acc.: 89.06%] [G loss: 3.584828]\n",
      "epoch:28 step:22017 [D loss: 0.424453, acc.: 78.12%] [G loss: 3.702636]\n",
      "epoch:28 step:22018 [D loss: 0.391270, acc.: 83.59%] [G loss: 2.992314]\n",
      "epoch:28 step:22019 [D loss: 0.284839, acc.: 85.94%] [G loss: 3.323257]\n",
      "epoch:28 step:22020 [D loss: 0.318033, acc.: 85.16%] [G loss: 3.586972]\n",
      "epoch:28 step:22021 [D loss: 0.242913, acc.: 89.84%] [G loss: 4.015155]\n",
      "epoch:28 step:22022 [D loss: 0.315696, acc.: 86.72%] [G loss: 3.216892]\n",
      "epoch:28 step:22023 [D loss: 0.324035, acc.: 84.38%] [G loss: 3.469088]\n",
      "epoch:28 step:22024 [D loss: 0.344410, acc.: 85.94%] [G loss: 2.054573]\n",
      "epoch:28 step:22025 [D loss: 0.364796, acc.: 82.81%] [G loss: 3.144802]\n",
      "epoch:28 step:22026 [D loss: 0.303968, acc.: 83.59%] [G loss: 2.868356]\n",
      "epoch:28 step:22027 [D loss: 0.294755, acc.: 88.28%] [G loss: 3.734617]\n",
      "epoch:28 step:22028 [D loss: 0.405179, acc.: 82.03%] [G loss: 2.701020]\n",
      "epoch:28 step:22029 [D loss: 0.348789, acc.: 82.81%] [G loss: 3.212606]\n",
      "epoch:28 step:22030 [D loss: 0.291543, acc.: 88.28%] [G loss: 2.739974]\n",
      "epoch:28 step:22031 [D loss: 0.344832, acc.: 86.72%] [G loss: 4.385327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22032 [D loss: 0.317517, acc.: 85.94%] [G loss: 3.087152]\n",
      "epoch:28 step:22033 [D loss: 0.305231, acc.: 84.38%] [G loss: 5.332609]\n",
      "epoch:28 step:22034 [D loss: 0.448867, acc.: 75.78%] [G loss: 3.450439]\n",
      "epoch:28 step:22035 [D loss: 0.222558, acc.: 93.75%] [G loss: 3.160698]\n",
      "epoch:28 step:22036 [D loss: 0.263876, acc.: 84.38%] [G loss: 3.052122]\n",
      "epoch:28 step:22037 [D loss: 0.234399, acc.: 91.41%] [G loss: 2.989703]\n",
      "epoch:28 step:22038 [D loss: 0.189568, acc.: 91.41%] [G loss: 3.023084]\n",
      "epoch:28 step:22039 [D loss: 0.297315, acc.: 85.94%] [G loss: 2.701879]\n",
      "epoch:28 step:22040 [D loss: 0.328655, acc.: 88.28%] [G loss: 4.220271]\n",
      "epoch:28 step:22041 [D loss: 0.316843, acc.: 88.28%] [G loss: 5.098139]\n",
      "epoch:28 step:22042 [D loss: 0.390711, acc.: 81.25%] [G loss: 2.497799]\n",
      "epoch:28 step:22043 [D loss: 0.317474, acc.: 85.94%] [G loss: 3.172351]\n",
      "epoch:28 step:22044 [D loss: 0.270871, acc.: 87.50%] [G loss: 3.399501]\n",
      "epoch:28 step:22045 [D loss: 0.373549, acc.: 80.47%] [G loss: 3.981901]\n",
      "epoch:28 step:22046 [D loss: 0.289689, acc.: 89.06%] [G loss: 3.372242]\n",
      "epoch:28 step:22047 [D loss: 0.317424, acc.: 87.50%] [G loss: 2.754269]\n",
      "epoch:28 step:22048 [D loss: 0.275932, acc.: 89.06%] [G loss: 3.367988]\n",
      "epoch:28 step:22049 [D loss: 0.243785, acc.: 91.41%] [G loss: 3.079480]\n",
      "epoch:28 step:22050 [D loss: 0.317138, acc.: 82.03%] [G loss: 3.183712]\n",
      "epoch:28 step:22051 [D loss: 0.251752, acc.: 89.06%] [G loss: 2.822999]\n",
      "epoch:28 step:22052 [D loss: 0.403593, acc.: 82.81%] [G loss: 2.614328]\n",
      "epoch:28 step:22053 [D loss: 0.280633, acc.: 89.84%] [G loss: 3.441287]\n",
      "epoch:28 step:22054 [D loss: 0.302996, acc.: 84.38%] [G loss: 2.710218]\n",
      "epoch:28 step:22055 [D loss: 0.296160, acc.: 87.50%] [G loss: 3.278874]\n",
      "epoch:28 step:22056 [D loss: 0.257986, acc.: 89.84%] [G loss: 3.325188]\n",
      "epoch:28 step:22057 [D loss: 0.330485, acc.: 81.25%] [G loss: 3.088473]\n",
      "epoch:28 step:22058 [D loss: 0.275158, acc.: 85.94%] [G loss: 3.552388]\n",
      "epoch:28 step:22059 [D loss: 0.279616, acc.: 90.62%] [G loss: 3.145682]\n",
      "epoch:28 step:22060 [D loss: 0.277428, acc.: 89.06%] [G loss: 3.173944]\n",
      "epoch:28 step:22061 [D loss: 0.378331, acc.: 82.81%] [G loss: 3.056431]\n",
      "epoch:28 step:22062 [D loss: 0.295060, acc.: 86.72%] [G loss: 3.907402]\n",
      "epoch:28 step:22063 [D loss: 0.341984, acc.: 84.38%] [G loss: 4.883022]\n",
      "epoch:28 step:22064 [D loss: 0.261379, acc.: 87.50%] [G loss: 4.438108]\n",
      "epoch:28 step:22065 [D loss: 0.278340, acc.: 84.38%] [G loss: 9.374342]\n",
      "epoch:28 step:22066 [D loss: 0.481356, acc.: 78.12%] [G loss: 4.994321]\n",
      "epoch:28 step:22067 [D loss: 0.310070, acc.: 85.16%] [G loss: 4.775848]\n",
      "epoch:28 step:22068 [D loss: 0.287732, acc.: 85.16%] [G loss: 4.225121]\n",
      "epoch:28 step:22069 [D loss: 0.336223, acc.: 85.16%] [G loss: 2.684863]\n",
      "epoch:28 step:22070 [D loss: 0.277403, acc.: 88.28%] [G loss: 2.994766]\n",
      "epoch:28 step:22071 [D loss: 0.350027, acc.: 85.94%] [G loss: 3.119075]\n",
      "epoch:28 step:22072 [D loss: 0.384506, acc.: 80.47%] [G loss: 3.017865]\n",
      "epoch:28 step:22073 [D loss: 0.430574, acc.: 85.16%] [G loss: 2.681026]\n",
      "epoch:28 step:22074 [D loss: 0.370921, acc.: 84.38%] [G loss: 2.494827]\n",
      "epoch:28 step:22075 [D loss: 0.310611, acc.: 83.59%] [G loss: 2.365182]\n",
      "epoch:28 step:22076 [D loss: 0.311126, acc.: 86.72%] [G loss: 3.451050]\n",
      "epoch:28 step:22077 [D loss: 0.253483, acc.: 91.41%] [G loss: 4.035058]\n",
      "epoch:28 step:22078 [D loss: 0.396786, acc.: 84.38%] [G loss: 3.841940]\n",
      "epoch:28 step:22079 [D loss: 0.333651, acc.: 83.59%] [G loss: 2.917986]\n",
      "epoch:28 step:22080 [D loss: 0.287097, acc.: 90.62%] [G loss: 3.188360]\n",
      "epoch:28 step:22081 [D loss: 0.466011, acc.: 80.47%] [G loss: 2.808887]\n",
      "epoch:28 step:22082 [D loss: 0.450375, acc.: 79.69%] [G loss: 3.476099]\n",
      "epoch:28 step:22083 [D loss: 0.289366, acc.: 88.28%] [G loss: 4.280793]\n",
      "epoch:28 step:22084 [D loss: 0.433765, acc.: 81.25%] [G loss: 2.621194]\n",
      "epoch:28 step:22085 [D loss: 0.457290, acc.: 78.91%] [G loss: 2.613608]\n",
      "epoch:28 step:22086 [D loss: 0.352304, acc.: 82.81%] [G loss: 2.824172]\n",
      "epoch:28 step:22087 [D loss: 0.292688, acc.: 85.94%] [G loss: 2.795862]\n",
      "epoch:28 step:22088 [D loss: 0.365375, acc.: 83.59%] [G loss: 2.951042]\n",
      "epoch:28 step:22089 [D loss: 0.335057, acc.: 86.72%] [G loss: 2.604919]\n",
      "epoch:28 step:22090 [D loss: 0.317957, acc.: 85.16%] [G loss: 2.599167]\n",
      "epoch:28 step:22091 [D loss: 0.318997, acc.: 89.06%] [G loss: 3.102965]\n",
      "epoch:28 step:22092 [D loss: 0.422204, acc.: 82.03%] [G loss: 3.061367]\n",
      "epoch:28 step:22093 [D loss: 0.433773, acc.: 75.00%] [G loss: 2.732726]\n",
      "epoch:28 step:22094 [D loss: 0.283958, acc.: 85.16%] [G loss: 2.879261]\n",
      "epoch:28 step:22095 [D loss: 0.297972, acc.: 87.50%] [G loss: 2.726053]\n",
      "epoch:28 step:22096 [D loss: 0.352396, acc.: 83.59%] [G loss: 2.848615]\n",
      "epoch:28 step:22097 [D loss: 0.404809, acc.: 80.47%] [G loss: 3.094912]\n",
      "epoch:28 step:22098 [D loss: 0.378033, acc.: 85.16%] [G loss: 2.870728]\n",
      "epoch:28 step:22099 [D loss: 0.273642, acc.: 89.06%] [G loss: 4.236032]\n",
      "epoch:28 step:22100 [D loss: 0.269242, acc.: 86.72%] [G loss: 3.417335]\n",
      "epoch:28 step:22101 [D loss: 0.328809, acc.: 82.81%] [G loss: 3.203497]\n",
      "epoch:28 step:22102 [D loss: 0.251336, acc.: 88.28%] [G loss: 3.330743]\n",
      "epoch:28 step:22103 [D loss: 0.307717, acc.: 87.50%] [G loss: 3.757095]\n",
      "epoch:28 step:22104 [D loss: 0.212530, acc.: 92.97%] [G loss: 4.040086]\n",
      "epoch:28 step:22105 [D loss: 0.234850, acc.: 89.06%] [G loss: 3.517843]\n",
      "epoch:28 step:22106 [D loss: 0.315805, acc.: 88.28%] [G loss: 3.299481]\n",
      "epoch:28 step:22107 [D loss: 0.264813, acc.: 87.50%] [G loss: 3.731388]\n",
      "epoch:28 step:22108 [D loss: 0.300446, acc.: 84.38%] [G loss: 4.942506]\n",
      "epoch:28 step:22109 [D loss: 0.321223, acc.: 86.72%] [G loss: 3.672306]\n",
      "epoch:28 step:22110 [D loss: 0.297381, acc.: 86.72%] [G loss: 3.205346]\n",
      "epoch:28 step:22111 [D loss: 0.244192, acc.: 90.62%] [G loss: 3.430759]\n",
      "epoch:28 step:22112 [D loss: 0.319001, acc.: 87.50%] [G loss: 3.407235]\n",
      "epoch:28 step:22113 [D loss: 0.423945, acc.: 82.03%] [G loss: 3.212181]\n",
      "epoch:28 step:22114 [D loss: 0.225896, acc.: 91.41%] [G loss: 3.221842]\n",
      "epoch:28 step:22115 [D loss: 0.291409, acc.: 85.16%] [G loss: 4.400687]\n",
      "epoch:28 step:22116 [D loss: 0.304934, acc.: 85.94%] [G loss: 2.702660]\n",
      "epoch:28 step:22117 [D loss: 0.304589, acc.: 88.28%] [G loss: 2.756329]\n",
      "epoch:28 step:22118 [D loss: 0.290504, acc.: 87.50%] [G loss: 3.022696]\n",
      "epoch:28 step:22119 [D loss: 0.233806, acc.: 90.62%] [G loss: 4.443662]\n",
      "epoch:28 step:22120 [D loss: 0.219960, acc.: 92.97%] [G loss: 4.917922]\n",
      "epoch:28 step:22121 [D loss: 0.288540, acc.: 87.50%] [G loss: 4.116587]\n",
      "epoch:28 step:22122 [D loss: 0.238068, acc.: 90.62%] [G loss: 3.630779]\n",
      "epoch:28 step:22123 [D loss: 0.359988, acc.: 85.94%] [G loss: 3.586167]\n",
      "epoch:28 step:22124 [D loss: 0.360070, acc.: 79.69%] [G loss: 3.517587]\n",
      "epoch:28 step:22125 [D loss: 0.437910, acc.: 82.81%] [G loss: 4.901164]\n",
      "epoch:28 step:22126 [D loss: 0.657517, acc.: 78.91%] [G loss: 4.597786]\n",
      "epoch:28 step:22127 [D loss: 0.499528, acc.: 78.91%] [G loss: 3.633613]\n",
      "epoch:28 step:22128 [D loss: 0.366902, acc.: 80.47%] [G loss: 4.393666]\n",
      "epoch:28 step:22129 [D loss: 0.520911, acc.: 77.34%] [G loss: 5.080971]\n",
      "epoch:28 step:22130 [D loss: 0.403028, acc.: 85.16%] [G loss: 2.293046]\n",
      "epoch:28 step:22131 [D loss: 0.269008, acc.: 85.16%] [G loss: 4.121768]\n",
      "epoch:28 step:22132 [D loss: 0.267644, acc.: 89.06%] [G loss: 2.983063]\n",
      "epoch:28 step:22133 [D loss: 0.392099, acc.: 80.47%] [G loss: 2.997642]\n",
      "epoch:28 step:22134 [D loss: 0.398133, acc.: 84.38%] [G loss: 3.417423]\n",
      "epoch:28 step:22135 [D loss: 0.365368, acc.: 79.69%] [G loss: 2.873930]\n",
      "epoch:28 step:22136 [D loss: 0.290786, acc.: 89.84%] [G loss: 2.401087]\n",
      "epoch:28 step:22137 [D loss: 0.261121, acc.: 88.28%] [G loss: 2.996329]\n",
      "epoch:28 step:22138 [D loss: 0.254504, acc.: 85.94%] [G loss: 2.930863]\n",
      "epoch:28 step:22139 [D loss: 0.282647, acc.: 86.72%] [G loss: 3.608254]\n",
      "epoch:28 step:22140 [D loss: 0.300798, acc.: 89.84%] [G loss: 3.392435]\n",
      "epoch:28 step:22141 [D loss: 0.310110, acc.: 85.16%] [G loss: 2.470775]\n",
      "epoch:28 step:22142 [D loss: 0.302355, acc.: 89.84%] [G loss: 3.353053]\n",
      "epoch:28 step:22143 [D loss: 0.430237, acc.: 79.69%] [G loss: 4.721703]\n",
      "epoch:28 step:22144 [D loss: 0.346069, acc.: 83.59%] [G loss: 7.485293]\n",
      "epoch:28 step:22145 [D loss: 0.280798, acc.: 84.38%] [G loss: 4.936600]\n",
      "epoch:28 step:22146 [D loss: 0.223756, acc.: 91.41%] [G loss: 3.433281]\n",
      "epoch:28 step:22147 [D loss: 0.215599, acc.: 90.62%] [G loss: 3.552372]\n",
      "epoch:28 step:22148 [D loss: 0.170126, acc.: 96.09%] [G loss: 3.613282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22149 [D loss: 0.345236, acc.: 80.47%] [G loss: 3.006061]\n",
      "epoch:28 step:22150 [D loss: 0.262871, acc.: 91.41%] [G loss: 2.877894]\n",
      "epoch:28 step:22151 [D loss: 0.391422, acc.: 81.25%] [G loss: 2.918983]\n",
      "epoch:28 step:22152 [D loss: 0.313072, acc.: 83.59%] [G loss: 2.599250]\n",
      "epoch:28 step:22153 [D loss: 0.262664, acc.: 89.06%] [G loss: 3.123743]\n",
      "epoch:28 step:22154 [D loss: 0.303721, acc.: 83.59%] [G loss: 2.792338]\n",
      "epoch:28 step:22155 [D loss: 0.223517, acc.: 90.62%] [G loss: 3.624269]\n",
      "epoch:28 step:22156 [D loss: 0.268320, acc.: 87.50%] [G loss: 2.947040]\n",
      "epoch:28 step:22157 [D loss: 0.381237, acc.: 81.25%] [G loss: 2.980917]\n",
      "epoch:28 step:22158 [D loss: 0.307016, acc.: 85.94%] [G loss: 2.728013]\n",
      "epoch:28 step:22159 [D loss: 0.331096, acc.: 85.94%] [G loss: 3.451988]\n",
      "epoch:28 step:22160 [D loss: 0.263684, acc.: 89.84%] [G loss: 2.471509]\n",
      "epoch:28 step:22161 [D loss: 0.364664, acc.: 82.03%] [G loss: 2.657608]\n",
      "epoch:28 step:22162 [D loss: 0.350347, acc.: 85.94%] [G loss: 2.613912]\n",
      "epoch:28 step:22163 [D loss: 0.312209, acc.: 86.72%] [G loss: 2.074622]\n",
      "epoch:28 step:22164 [D loss: 0.436232, acc.: 80.47%] [G loss: 2.532213]\n",
      "epoch:28 step:22165 [D loss: 0.399095, acc.: 82.03%] [G loss: 2.866923]\n",
      "epoch:28 step:22166 [D loss: 0.240203, acc.: 89.06%] [G loss: 3.804482]\n",
      "epoch:28 step:22167 [D loss: 0.236902, acc.: 91.41%] [G loss: 2.580072]\n",
      "epoch:28 step:22168 [D loss: 0.394441, acc.: 82.03%] [G loss: 2.953531]\n",
      "epoch:28 step:22169 [D loss: 0.381660, acc.: 78.91%] [G loss: 5.056307]\n",
      "epoch:28 step:22170 [D loss: 0.455529, acc.: 82.03%] [G loss: 6.395589]\n",
      "epoch:28 step:22171 [D loss: 1.253519, acc.: 73.44%] [G loss: 6.817313]\n",
      "epoch:28 step:22172 [D loss: 1.733281, acc.: 57.81%] [G loss: 3.234483]\n",
      "epoch:28 step:22173 [D loss: 0.565640, acc.: 77.34%] [G loss: 2.875074]\n",
      "epoch:28 step:22174 [D loss: 0.526305, acc.: 81.25%] [G loss: 3.054604]\n",
      "epoch:28 step:22175 [D loss: 0.457205, acc.: 80.47%] [G loss: 2.544681]\n",
      "epoch:28 step:22176 [D loss: 0.513575, acc.: 75.78%] [G loss: 3.280027]\n",
      "epoch:28 step:22177 [D loss: 0.367381, acc.: 78.12%] [G loss: 3.650684]\n",
      "epoch:28 step:22178 [D loss: 0.246964, acc.: 90.62%] [G loss: 3.223577]\n",
      "epoch:28 step:22179 [D loss: 0.346151, acc.: 79.69%] [G loss: 5.716356]\n",
      "epoch:28 step:22180 [D loss: 0.311542, acc.: 85.94%] [G loss: 3.685810]\n",
      "epoch:28 step:22181 [D loss: 0.593105, acc.: 74.22%] [G loss: 2.581747]\n",
      "epoch:28 step:22182 [D loss: 0.268935, acc.: 86.72%] [G loss: 3.316489]\n",
      "epoch:28 step:22183 [D loss: 0.365048, acc.: 81.25%] [G loss: 3.580113]\n",
      "epoch:28 step:22184 [D loss: 0.290220, acc.: 90.62%] [G loss: 3.029604]\n",
      "epoch:28 step:22185 [D loss: 0.382420, acc.: 83.59%] [G loss: 2.729495]\n",
      "epoch:28 step:22186 [D loss: 0.261617, acc.: 90.62%] [G loss: 2.454644]\n",
      "epoch:28 step:22187 [D loss: 0.355649, acc.: 82.81%] [G loss: 3.367431]\n",
      "epoch:28 step:22188 [D loss: 0.262071, acc.: 90.62%] [G loss: 2.872844]\n",
      "epoch:28 step:22189 [D loss: 0.246891, acc.: 88.28%] [G loss: 3.152984]\n",
      "epoch:28 step:22190 [D loss: 0.417372, acc.: 78.12%] [G loss: 3.179998]\n",
      "epoch:28 step:22191 [D loss: 0.379452, acc.: 79.69%] [G loss: 3.337228]\n",
      "epoch:28 step:22192 [D loss: 0.327102, acc.: 85.94%] [G loss: 3.542244]\n",
      "epoch:28 step:22193 [D loss: 0.366015, acc.: 79.69%] [G loss: 2.854031]\n",
      "epoch:28 step:22194 [D loss: 0.290384, acc.: 85.16%] [G loss: 2.771710]\n",
      "epoch:28 step:22195 [D loss: 0.376788, acc.: 80.47%] [G loss: 3.169010]\n",
      "epoch:28 step:22196 [D loss: 0.247805, acc.: 89.84%] [G loss: 3.137628]\n",
      "epoch:28 step:22197 [D loss: 0.382810, acc.: 77.34%] [G loss: 4.019957]\n",
      "epoch:28 step:22198 [D loss: 0.307100, acc.: 84.38%] [G loss: 3.008762]\n",
      "epoch:28 step:22199 [D loss: 0.303766, acc.: 85.94%] [G loss: 3.953930]\n",
      "epoch:28 step:22200 [D loss: 0.261387, acc.: 90.62%] [G loss: 2.984841]\n",
      "epoch:28 step:22201 [D loss: 0.294955, acc.: 89.06%] [G loss: 2.818518]\n",
      "epoch:28 step:22202 [D loss: 0.343035, acc.: 83.59%] [G loss: 2.441376]\n",
      "epoch:28 step:22203 [D loss: 0.306873, acc.: 89.06%] [G loss: 2.602750]\n",
      "epoch:28 step:22204 [D loss: 0.367272, acc.: 83.59%] [G loss: 3.066310]\n",
      "epoch:28 step:22205 [D loss: 0.376977, acc.: 82.03%] [G loss: 2.625289]\n",
      "epoch:28 step:22206 [D loss: 0.368156, acc.: 85.16%] [G loss: 2.564037]\n",
      "epoch:28 step:22207 [D loss: 0.282862, acc.: 88.28%] [G loss: 2.971774]\n",
      "epoch:28 step:22208 [D loss: 0.448295, acc.: 82.03%] [G loss: 2.969657]\n",
      "epoch:28 step:22209 [D loss: 0.235495, acc.: 89.84%] [G loss: 4.658950]\n",
      "epoch:28 step:22210 [D loss: 0.250335, acc.: 89.06%] [G loss: 3.439242]\n",
      "epoch:28 step:22211 [D loss: 0.226132, acc.: 90.62%] [G loss: 4.381419]\n",
      "epoch:28 step:22212 [D loss: 0.177240, acc.: 96.09%] [G loss: 5.161154]\n",
      "epoch:28 step:22213 [D loss: 0.311957, acc.: 86.72%] [G loss: 5.917212]\n",
      "epoch:28 step:22214 [D loss: 0.288343, acc.: 86.72%] [G loss: 3.822769]\n",
      "epoch:28 step:22215 [D loss: 0.282028, acc.: 85.16%] [G loss: 3.257021]\n",
      "epoch:28 step:22216 [D loss: 0.261094, acc.: 91.41%] [G loss: 4.114924]\n",
      "epoch:28 step:22217 [D loss: 0.284861, acc.: 88.28%] [G loss: 2.540234]\n",
      "epoch:28 step:22218 [D loss: 0.291925, acc.: 85.94%] [G loss: 4.807710]\n",
      "epoch:28 step:22219 [D loss: 0.294233, acc.: 89.06%] [G loss: 3.713087]\n",
      "epoch:28 step:22220 [D loss: 0.290573, acc.: 88.28%] [G loss: 3.115879]\n",
      "epoch:28 step:22221 [D loss: 0.342984, acc.: 82.03%] [G loss: 3.225900]\n",
      "epoch:28 step:22222 [D loss: 0.272941, acc.: 92.19%] [G loss: 3.005832]\n",
      "epoch:28 step:22223 [D loss: 0.283861, acc.: 85.94%] [G loss: 3.193020]\n",
      "epoch:28 step:22224 [D loss: 0.398344, acc.: 79.69%] [G loss: 3.033407]\n",
      "epoch:28 step:22225 [D loss: 0.423352, acc.: 82.03%] [G loss: 2.792800]\n",
      "epoch:28 step:22226 [D loss: 0.288992, acc.: 88.28%] [G loss: 2.326232]\n",
      "epoch:28 step:22227 [D loss: 0.268796, acc.: 86.72%] [G loss: 2.982515]\n",
      "epoch:28 step:22228 [D loss: 0.270268, acc.: 89.06%] [G loss: 2.560154]\n",
      "epoch:28 step:22229 [D loss: 0.213315, acc.: 92.19%] [G loss: 2.046027]\n",
      "epoch:28 step:22230 [D loss: 0.319534, acc.: 89.06%] [G loss: 3.619521]\n",
      "epoch:28 step:22231 [D loss: 0.218486, acc.: 94.53%] [G loss: 3.201845]\n",
      "epoch:28 step:22232 [D loss: 0.313456, acc.: 84.38%] [G loss: 2.805012]\n",
      "epoch:28 step:22233 [D loss: 0.366408, acc.: 81.25%] [G loss: 2.883875]\n",
      "epoch:28 step:22234 [D loss: 0.420240, acc.: 78.91%] [G loss: 2.511657]\n",
      "epoch:28 step:22235 [D loss: 0.300323, acc.: 87.50%] [G loss: 3.746589]\n",
      "epoch:28 step:22236 [D loss: 0.466484, acc.: 76.56%] [G loss: 5.278248]\n",
      "epoch:28 step:22237 [D loss: 0.509423, acc.: 74.22%] [G loss: 7.159090]\n",
      "epoch:28 step:22238 [D loss: 0.959734, acc.: 60.94%] [G loss: 3.236862]\n",
      "epoch:28 step:22239 [D loss: 0.436465, acc.: 84.38%] [G loss: 3.428552]\n",
      "epoch:28 step:22240 [D loss: 0.377457, acc.: 83.59%] [G loss: 2.551236]\n",
      "epoch:28 step:22241 [D loss: 0.455958, acc.: 82.03%] [G loss: 5.907369]\n",
      "epoch:28 step:22242 [D loss: 0.745244, acc.: 72.66%] [G loss: 6.107910]\n",
      "epoch:28 step:22243 [D loss: 0.960968, acc.: 60.94%] [G loss: 3.305929]\n",
      "epoch:28 step:22244 [D loss: 0.386556, acc.: 84.38%] [G loss: 3.756601]\n",
      "epoch:28 step:22245 [D loss: 0.539177, acc.: 78.91%] [G loss: 3.920174]\n",
      "epoch:28 step:22246 [D loss: 0.282759, acc.: 85.94%] [G loss: 3.871565]\n",
      "epoch:28 step:22247 [D loss: 0.252748, acc.: 90.62%] [G loss: 3.010900]\n",
      "epoch:28 step:22248 [D loss: 0.256183, acc.: 86.72%] [G loss: 3.815239]\n",
      "epoch:28 step:22249 [D loss: 0.291756, acc.: 87.50%] [G loss: 2.626822]\n",
      "epoch:28 step:22250 [D loss: 0.340487, acc.: 83.59%] [G loss: 3.371445]\n",
      "epoch:28 step:22251 [D loss: 0.346952, acc.: 85.16%] [G loss: 2.962248]\n",
      "epoch:28 step:22252 [D loss: 0.314761, acc.: 85.16%] [G loss: 3.591222]\n",
      "epoch:28 step:22253 [D loss: 0.349641, acc.: 82.81%] [G loss: 3.483562]\n",
      "epoch:28 step:22254 [D loss: 0.399604, acc.: 79.69%] [G loss: 2.677488]\n",
      "epoch:28 step:22255 [D loss: 0.336955, acc.: 86.72%] [G loss: 3.585858]\n",
      "epoch:28 step:22256 [D loss: 0.298033, acc.: 85.94%] [G loss: 3.118360]\n",
      "epoch:28 step:22257 [D loss: 0.361993, acc.: 83.59%] [G loss: 3.157615]\n",
      "epoch:28 step:22258 [D loss: 0.276885, acc.: 86.72%] [G loss: 3.164428]\n",
      "epoch:28 step:22259 [D loss: 0.403641, acc.: 77.34%] [G loss: 2.241831]\n",
      "epoch:28 step:22260 [D loss: 0.293370, acc.: 85.16%] [G loss: 2.621664]\n",
      "epoch:28 step:22261 [D loss: 0.369647, acc.: 83.59%] [G loss: 2.837378]\n",
      "epoch:28 step:22262 [D loss: 0.286734, acc.: 88.28%] [G loss: 2.860201]\n",
      "epoch:28 step:22263 [D loss: 0.342760, acc.: 85.94%] [G loss: 2.067518]\n",
      "epoch:28 step:22264 [D loss: 0.285956, acc.: 89.84%] [G loss: 2.900983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22265 [D loss: 0.344623, acc.: 88.28%] [G loss: 2.441248]\n",
      "epoch:28 step:22266 [D loss: 0.287395, acc.: 86.72%] [G loss: 2.357245]\n",
      "epoch:28 step:22267 [D loss: 0.373566, acc.: 81.25%] [G loss: 2.905046]\n",
      "epoch:28 step:22268 [D loss: 0.416803, acc.: 82.81%] [G loss: 2.310897]\n",
      "epoch:28 step:22269 [D loss: 0.358691, acc.: 85.16%] [G loss: 2.627099]\n",
      "epoch:28 step:22270 [D loss: 0.275725, acc.: 88.28%] [G loss: 2.971885]\n",
      "epoch:28 step:22271 [D loss: 0.348455, acc.: 85.16%] [G loss: 2.707273]\n",
      "epoch:28 step:22272 [D loss: 0.356819, acc.: 85.16%] [G loss: 2.993185]\n",
      "epoch:28 step:22273 [D loss: 0.357585, acc.: 83.59%] [G loss: 2.745933]\n",
      "epoch:28 step:22274 [D loss: 0.471065, acc.: 71.88%] [G loss: 3.048874]\n",
      "epoch:28 step:22275 [D loss: 0.316739, acc.: 85.16%] [G loss: 2.731834]\n",
      "epoch:28 step:22276 [D loss: 0.420503, acc.: 80.47%] [G loss: 3.250725]\n",
      "epoch:28 step:22277 [D loss: 0.293043, acc.: 87.50%] [G loss: 2.885442]\n",
      "epoch:28 step:22278 [D loss: 0.394260, acc.: 80.47%] [G loss: 2.106910]\n",
      "epoch:28 step:22279 [D loss: 0.355143, acc.: 82.03%] [G loss: 2.383812]\n",
      "epoch:28 step:22280 [D loss: 0.344058, acc.: 86.72%] [G loss: 2.633850]\n",
      "epoch:28 step:22281 [D loss: 0.321760, acc.: 86.72%] [G loss: 2.720886]\n",
      "epoch:28 step:22282 [D loss: 0.364826, acc.: 83.59%] [G loss: 2.564408]\n",
      "epoch:28 step:22283 [D loss: 0.256094, acc.: 89.84%] [G loss: 2.667155]\n",
      "epoch:28 step:22284 [D loss: 0.313283, acc.: 84.38%] [G loss: 3.394230]\n",
      "epoch:28 step:22285 [D loss: 0.382403, acc.: 82.81%] [G loss: 2.948953]\n",
      "epoch:28 step:22286 [D loss: 0.285031, acc.: 88.28%] [G loss: 2.872756]\n",
      "epoch:28 step:22287 [D loss: 0.408627, acc.: 79.69%] [G loss: 2.660018]\n",
      "epoch:28 step:22288 [D loss: 0.404276, acc.: 82.03%] [G loss: 2.820848]\n",
      "epoch:28 step:22289 [D loss: 0.328293, acc.: 82.81%] [G loss: 3.697007]\n",
      "epoch:28 step:22290 [D loss: 0.254968, acc.: 89.84%] [G loss: 3.640198]\n",
      "epoch:28 step:22291 [D loss: 0.283988, acc.: 89.06%] [G loss: 5.030480]\n",
      "epoch:28 step:22292 [D loss: 0.241291, acc.: 90.62%] [G loss: 4.630264]\n",
      "epoch:28 step:22293 [D loss: 0.393351, acc.: 79.69%] [G loss: 2.795996]\n",
      "epoch:28 step:22294 [D loss: 0.229259, acc.: 90.62%] [G loss: 2.998446]\n",
      "epoch:28 step:22295 [D loss: 0.190920, acc.: 91.41%] [G loss: 3.600056]\n",
      "epoch:28 step:22296 [D loss: 0.450273, acc.: 75.00%] [G loss: 4.133133]\n",
      "epoch:28 step:22297 [D loss: 0.385299, acc.: 81.25%] [G loss: 3.354462]\n",
      "epoch:28 step:22298 [D loss: 0.234728, acc.: 89.84%] [G loss: 3.415773]\n",
      "epoch:28 step:22299 [D loss: 0.399740, acc.: 82.03%] [G loss: 2.988185]\n",
      "epoch:28 step:22300 [D loss: 0.267084, acc.: 88.28%] [G loss: 2.866481]\n",
      "epoch:28 step:22301 [D loss: 0.250980, acc.: 89.84%] [G loss: 3.349418]\n",
      "epoch:28 step:22302 [D loss: 0.254098, acc.: 89.84%] [G loss: 3.123077]\n",
      "epoch:28 step:22303 [D loss: 0.331755, acc.: 82.81%] [G loss: 3.320845]\n",
      "epoch:28 step:22304 [D loss: 0.286180, acc.: 84.38%] [G loss: 4.247588]\n",
      "epoch:28 step:22305 [D loss: 0.309788, acc.: 86.72%] [G loss: 3.409324]\n",
      "epoch:28 step:22306 [D loss: 0.357762, acc.: 83.59%] [G loss: 2.859294]\n",
      "epoch:28 step:22307 [D loss: 0.336568, acc.: 85.16%] [G loss: 5.178729]\n",
      "epoch:28 step:22308 [D loss: 0.275055, acc.: 89.06%] [G loss: 3.231014]\n",
      "epoch:28 step:22309 [D loss: 0.346996, acc.: 84.38%] [G loss: 4.070544]\n",
      "epoch:28 step:22310 [D loss: 0.389460, acc.: 82.03%] [G loss: 3.403088]\n",
      "epoch:28 step:22311 [D loss: 0.369352, acc.: 82.81%] [G loss: 3.563443]\n",
      "epoch:28 step:22312 [D loss: 0.419363, acc.: 84.38%] [G loss: 3.504130]\n",
      "epoch:28 step:22313 [D loss: 0.401157, acc.: 81.25%] [G loss: 3.523781]\n",
      "epoch:28 step:22314 [D loss: 0.281991, acc.: 89.84%] [G loss: 4.463530]\n",
      "epoch:28 step:22315 [D loss: 0.190876, acc.: 91.41%] [G loss: 3.789948]\n",
      "epoch:28 step:22316 [D loss: 0.368888, acc.: 85.94%] [G loss: 3.839641]\n",
      "epoch:28 step:22317 [D loss: 0.367038, acc.: 85.94%] [G loss: 3.310927]\n",
      "epoch:28 step:22318 [D loss: 0.262613, acc.: 91.41%] [G loss: 3.338571]\n",
      "epoch:28 step:22319 [D loss: 0.257232, acc.: 92.19%] [G loss: 2.867760]\n",
      "epoch:28 step:22320 [D loss: 0.340369, acc.: 83.59%] [G loss: 2.783127]\n",
      "epoch:28 step:22321 [D loss: 0.239512, acc.: 89.06%] [G loss: 2.731882]\n",
      "epoch:28 step:22322 [D loss: 0.296433, acc.: 86.72%] [G loss: 2.350695]\n",
      "epoch:28 step:22323 [D loss: 0.325454, acc.: 84.38%] [G loss: 2.983299]\n",
      "epoch:28 step:22324 [D loss: 0.321131, acc.: 85.94%] [G loss: 2.645108]\n",
      "epoch:28 step:22325 [D loss: 0.276036, acc.: 90.62%] [G loss: 3.352027]\n",
      "epoch:28 step:22326 [D loss: 0.234418, acc.: 89.84%] [G loss: 2.872059]\n",
      "epoch:28 step:22327 [D loss: 0.319121, acc.: 89.06%] [G loss: 2.799952]\n",
      "epoch:28 step:22328 [D loss: 0.377587, acc.: 79.69%] [G loss: 2.378499]\n",
      "epoch:28 step:22329 [D loss: 0.256712, acc.: 91.41%] [G loss: 2.680913]\n",
      "epoch:28 step:22330 [D loss: 0.309051, acc.: 86.72%] [G loss: 3.503654]\n",
      "epoch:28 step:22331 [D loss: 0.385524, acc.: 79.69%] [G loss: 2.343591]\n",
      "epoch:28 step:22332 [D loss: 0.316025, acc.: 85.16%] [G loss: 2.732445]\n",
      "epoch:28 step:22333 [D loss: 0.417028, acc.: 77.34%] [G loss: 2.589581]\n",
      "epoch:28 step:22334 [D loss: 0.432376, acc.: 78.91%] [G loss: 2.226789]\n",
      "epoch:28 step:22335 [D loss: 0.382447, acc.: 81.25%] [G loss: 2.338913]\n",
      "epoch:28 step:22336 [D loss: 0.323609, acc.: 83.59%] [G loss: 2.918451]\n",
      "epoch:28 step:22337 [D loss: 0.297702, acc.: 86.72%] [G loss: 3.493864]\n",
      "epoch:28 step:22338 [D loss: 0.267939, acc.: 89.84%] [G loss: 2.683766]\n",
      "epoch:28 step:22339 [D loss: 0.273151, acc.: 87.50%] [G loss: 3.246836]\n",
      "epoch:28 step:22340 [D loss: 0.362353, acc.: 83.59%] [G loss: 2.618470]\n",
      "epoch:28 step:22341 [D loss: 0.295309, acc.: 88.28%] [G loss: 2.671524]\n",
      "epoch:28 step:22342 [D loss: 0.419505, acc.: 78.91%] [G loss: 3.734196]\n",
      "epoch:28 step:22343 [D loss: 0.422784, acc.: 79.69%] [G loss: 3.949240]\n",
      "epoch:28 step:22344 [D loss: 0.330844, acc.: 83.59%] [G loss: 4.001910]\n",
      "epoch:28 step:22345 [D loss: 0.382032, acc.: 82.81%] [G loss: 4.961403]\n",
      "epoch:28 step:22346 [D loss: 0.266150, acc.: 87.50%] [G loss: 5.032327]\n",
      "epoch:28 step:22347 [D loss: 0.471258, acc.: 81.25%] [G loss: 5.033406]\n",
      "epoch:28 step:22348 [D loss: 0.527122, acc.: 78.12%] [G loss: 3.593791]\n",
      "epoch:28 step:22349 [D loss: 0.283495, acc.: 86.72%] [G loss: 4.117390]\n",
      "epoch:28 step:22350 [D loss: 0.367087, acc.: 85.94%] [G loss: 3.663591]\n",
      "epoch:28 step:22351 [D loss: 0.229505, acc.: 89.84%] [G loss: 4.024077]\n",
      "epoch:28 step:22352 [D loss: 0.271658, acc.: 86.72%] [G loss: 3.876604]\n",
      "epoch:28 step:22353 [D loss: 0.283178, acc.: 89.06%] [G loss: 3.090392]\n",
      "epoch:28 step:22354 [D loss: 0.287299, acc.: 86.72%] [G loss: 3.119623]\n",
      "epoch:28 step:22355 [D loss: 0.292240, acc.: 89.06%] [G loss: 2.831929]\n",
      "epoch:28 step:22356 [D loss: 0.326411, acc.: 86.72%] [G loss: 3.617827]\n",
      "epoch:28 step:22357 [D loss: 0.303220, acc.: 86.72%] [G loss: 3.184590]\n",
      "epoch:28 step:22358 [D loss: 0.262341, acc.: 88.28%] [G loss: 3.143302]\n",
      "epoch:28 step:22359 [D loss: 0.358357, acc.: 82.81%] [G loss: 2.449977]\n",
      "epoch:28 step:22360 [D loss: 0.281626, acc.: 89.06%] [G loss: 2.365794]\n",
      "epoch:28 step:22361 [D loss: 0.254286, acc.: 90.62%] [G loss: 2.741678]\n",
      "epoch:28 step:22362 [D loss: 0.285326, acc.: 90.62%] [G loss: 2.965783]\n",
      "epoch:28 step:22363 [D loss: 0.317686, acc.: 86.72%] [G loss: 2.582887]\n",
      "epoch:28 step:22364 [D loss: 0.357255, acc.: 84.38%] [G loss: 3.672146]\n",
      "epoch:28 step:22365 [D loss: 0.377475, acc.: 82.03%] [G loss: 3.183838]\n",
      "epoch:28 step:22366 [D loss: 0.224674, acc.: 89.84%] [G loss: 3.080138]\n",
      "epoch:28 step:22367 [D loss: 0.271547, acc.: 86.72%] [G loss: 4.999114]\n",
      "epoch:28 step:22368 [D loss: 0.258756, acc.: 89.84%] [G loss: 5.138297]\n",
      "epoch:28 step:22369 [D loss: 0.520606, acc.: 75.00%] [G loss: 5.502790]\n",
      "epoch:28 step:22370 [D loss: 0.400091, acc.: 82.81%] [G loss: 3.494093]\n",
      "epoch:28 step:22371 [D loss: 0.319765, acc.: 87.50%] [G loss: 2.422374]\n",
      "epoch:28 step:22372 [D loss: 0.315896, acc.: 85.94%] [G loss: 3.808949]\n",
      "epoch:28 step:22373 [D loss: 0.342734, acc.: 87.50%] [G loss: 3.753567]\n",
      "epoch:28 step:22374 [D loss: 0.273737, acc.: 86.72%] [G loss: 2.901428]\n",
      "epoch:28 step:22375 [D loss: 0.295611, acc.: 86.72%] [G loss: 3.629601]\n",
      "epoch:28 step:22376 [D loss: 0.208043, acc.: 93.75%] [G loss: 3.694423]\n",
      "epoch:28 step:22377 [D loss: 0.317296, acc.: 89.06%] [G loss: 2.992669]\n",
      "epoch:28 step:22378 [D loss: 0.240708, acc.: 89.06%] [G loss: 3.367758]\n",
      "epoch:28 step:22379 [D loss: 0.238794, acc.: 89.84%] [G loss: 2.907878]\n",
      "epoch:28 step:22380 [D loss: 0.281984, acc.: 85.16%] [G loss: 3.760293]\n",
      "epoch:28 step:22381 [D loss: 0.333044, acc.: 85.16%] [G loss: 2.916214]\n",
      "epoch:28 step:22382 [D loss: 0.321541, acc.: 81.25%] [G loss: 2.969774]\n",
      "epoch:28 step:22383 [D loss: 0.344963, acc.: 87.50%] [G loss: 2.503834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22384 [D loss: 0.397057, acc.: 80.47%] [G loss: 3.128548]\n",
      "epoch:28 step:22385 [D loss: 0.364601, acc.: 85.94%] [G loss: 3.131573]\n",
      "epoch:28 step:22386 [D loss: 0.231310, acc.: 89.84%] [G loss: 2.830352]\n",
      "epoch:28 step:22387 [D loss: 0.286636, acc.: 89.06%] [G loss: 3.105194]\n",
      "epoch:28 step:22388 [D loss: 0.304144, acc.: 90.62%] [G loss: 3.765244]\n",
      "epoch:28 step:22389 [D loss: 0.242004, acc.: 89.06%] [G loss: 4.198990]\n",
      "epoch:28 step:22390 [D loss: 0.343790, acc.: 87.50%] [G loss: 3.052672]\n",
      "epoch:28 step:22391 [D loss: 0.296216, acc.: 89.06%] [G loss: 3.348377]\n",
      "epoch:28 step:22392 [D loss: 0.350305, acc.: 82.03%] [G loss: 3.162888]\n",
      "epoch:28 step:22393 [D loss: 0.350234, acc.: 82.03%] [G loss: 4.055513]\n",
      "epoch:28 step:22394 [D loss: 0.499280, acc.: 76.56%] [G loss: 3.047124]\n",
      "epoch:28 step:22395 [D loss: 0.409594, acc.: 81.25%] [G loss: 3.049349]\n",
      "epoch:28 step:22396 [D loss: 0.277573, acc.: 89.06%] [G loss: 3.236077]\n",
      "epoch:28 step:22397 [D loss: 0.362252, acc.: 85.16%] [G loss: 2.980247]\n",
      "epoch:28 step:22398 [D loss: 0.400605, acc.: 85.16%] [G loss: 2.254536]\n",
      "epoch:28 step:22399 [D loss: 0.243055, acc.: 89.84%] [G loss: 2.481769]\n",
      "epoch:28 step:22400 [D loss: 0.375195, acc.: 80.47%] [G loss: 3.324867]\n",
      "epoch:28 step:22401 [D loss: 0.431514, acc.: 78.12%] [G loss: 2.747094]\n",
      "epoch:28 step:22402 [D loss: 0.405471, acc.: 84.38%] [G loss: 2.808625]\n",
      "epoch:28 step:22403 [D loss: 0.320709, acc.: 86.72%] [G loss: 3.744932]\n",
      "epoch:28 step:22404 [D loss: 0.326634, acc.: 86.72%] [G loss: 2.657090]\n",
      "epoch:28 step:22405 [D loss: 0.460703, acc.: 77.34%] [G loss: 2.894881]\n",
      "epoch:28 step:22406 [D loss: 0.269682, acc.: 86.72%] [G loss: 3.185702]\n",
      "epoch:28 step:22407 [D loss: 0.314822, acc.: 87.50%] [G loss: 3.342172]\n",
      "epoch:28 step:22408 [D loss: 0.250469, acc.: 90.62%] [G loss: 3.048469]\n",
      "epoch:28 step:22409 [D loss: 0.281081, acc.: 87.50%] [G loss: 2.429855]\n",
      "epoch:28 step:22410 [D loss: 0.287285, acc.: 85.94%] [G loss: 3.271107]\n",
      "epoch:28 step:22411 [D loss: 0.290232, acc.: 85.16%] [G loss: 3.031641]\n",
      "epoch:28 step:22412 [D loss: 0.346266, acc.: 82.81%] [G loss: 2.885816]\n",
      "epoch:28 step:22413 [D loss: 0.227853, acc.: 88.28%] [G loss: 3.276479]\n",
      "epoch:28 step:22414 [D loss: 0.264714, acc.: 87.50%] [G loss: 3.614671]\n",
      "epoch:28 step:22415 [D loss: 0.444374, acc.: 78.12%] [G loss: 3.386571]\n",
      "epoch:28 step:22416 [D loss: 0.301782, acc.: 89.06%] [G loss: 2.826070]\n",
      "epoch:28 step:22417 [D loss: 0.360513, acc.: 83.59%] [G loss: 3.426602]\n",
      "epoch:28 step:22418 [D loss: 0.309057, acc.: 85.16%] [G loss: 2.528081]\n",
      "epoch:28 step:22419 [D loss: 0.252473, acc.: 88.28%] [G loss: 3.217704]\n",
      "epoch:28 step:22420 [D loss: 0.266015, acc.: 88.28%] [G loss: 3.893337]\n",
      "epoch:28 step:22421 [D loss: 0.343443, acc.: 82.81%] [G loss: 3.496598]\n",
      "epoch:28 step:22422 [D loss: 0.329215, acc.: 83.59%] [G loss: 2.868124]\n",
      "epoch:28 step:22423 [D loss: 0.384824, acc.: 82.03%] [G loss: 5.123957]\n",
      "epoch:28 step:22424 [D loss: 0.388945, acc.: 82.81%] [G loss: 2.834659]\n",
      "epoch:28 step:22425 [D loss: 0.239521, acc.: 88.28%] [G loss: 4.023438]\n",
      "epoch:28 step:22426 [D loss: 0.409351, acc.: 78.12%] [G loss: 2.198816]\n",
      "epoch:28 step:22427 [D loss: 0.309892, acc.: 85.16%] [G loss: 2.449408]\n",
      "epoch:28 step:22428 [D loss: 0.374216, acc.: 83.59%] [G loss: 2.677197]\n",
      "epoch:28 step:22429 [D loss: 0.396876, acc.: 81.25%] [G loss: 2.595527]\n",
      "epoch:28 step:22430 [D loss: 0.445789, acc.: 79.69%] [G loss: 2.192548]\n",
      "epoch:28 step:22431 [D loss: 0.299142, acc.: 84.38%] [G loss: 2.873442]\n",
      "epoch:28 step:22432 [D loss: 0.339252, acc.: 82.03%] [G loss: 2.469406]\n",
      "epoch:28 step:22433 [D loss: 0.258077, acc.: 88.28%] [G loss: 2.676323]\n",
      "epoch:28 step:22434 [D loss: 0.322650, acc.: 85.94%] [G loss: 3.202179]\n",
      "epoch:28 step:22435 [D loss: 0.336428, acc.: 82.03%] [G loss: 3.093893]\n",
      "epoch:28 step:22436 [D loss: 0.361898, acc.: 84.38%] [G loss: 2.429015]\n",
      "epoch:28 step:22437 [D loss: 0.288082, acc.: 84.38%] [G loss: 2.603592]\n",
      "epoch:28 step:22438 [D loss: 0.356344, acc.: 82.81%] [G loss: 2.709717]\n",
      "epoch:28 step:22439 [D loss: 0.346288, acc.: 86.72%] [G loss: 2.369273]\n",
      "epoch:28 step:22440 [D loss: 0.304239, acc.: 85.16%] [G loss: 3.591895]\n",
      "epoch:28 step:22441 [D loss: 0.391438, acc.: 81.25%] [G loss: 3.372486]\n",
      "epoch:28 step:22442 [D loss: 0.331932, acc.: 82.81%] [G loss: 2.163518]\n",
      "epoch:28 step:22443 [D loss: 0.257890, acc.: 89.84%] [G loss: 2.284342]\n",
      "epoch:28 step:22444 [D loss: 0.354960, acc.: 85.94%] [G loss: 3.388438]\n",
      "epoch:28 step:22445 [D loss: 0.289189, acc.: 88.28%] [G loss: 2.807333]\n",
      "epoch:28 step:22446 [D loss: 0.270910, acc.: 86.72%] [G loss: 3.144888]\n",
      "epoch:28 step:22447 [D loss: 0.392372, acc.: 80.47%] [G loss: 2.274747]\n",
      "epoch:28 step:22448 [D loss: 0.255880, acc.: 89.06%] [G loss: 2.466703]\n",
      "epoch:28 step:22449 [D loss: 0.340207, acc.: 82.81%] [G loss: 2.929537]\n",
      "epoch:28 step:22450 [D loss: 0.287914, acc.: 87.50%] [G loss: 2.597598]\n",
      "epoch:28 step:22451 [D loss: 0.313569, acc.: 87.50%] [G loss: 3.612154]\n",
      "epoch:28 step:22452 [D loss: 0.394117, acc.: 82.81%] [G loss: 3.465499]\n",
      "epoch:28 step:22453 [D loss: 0.269935, acc.: 88.28%] [G loss: 3.019157]\n",
      "epoch:28 step:22454 [D loss: 0.369340, acc.: 80.47%] [G loss: 4.559793]\n",
      "epoch:28 step:22455 [D loss: 0.235710, acc.: 89.84%] [G loss: 2.959867]\n",
      "epoch:28 step:22456 [D loss: 0.202822, acc.: 90.62%] [G loss: 3.349661]\n",
      "epoch:28 step:22457 [D loss: 0.298661, acc.: 88.28%] [G loss: 4.047143]\n",
      "epoch:28 step:22458 [D loss: 0.230809, acc.: 93.75%] [G loss: 3.875292]\n",
      "epoch:28 step:22459 [D loss: 0.349497, acc.: 84.38%] [G loss: 3.565695]\n",
      "epoch:28 step:22460 [D loss: 0.293647, acc.: 88.28%] [G loss: 4.123492]\n",
      "epoch:28 step:22461 [D loss: 0.227480, acc.: 87.50%] [G loss: 5.548451]\n",
      "epoch:28 step:22462 [D loss: 0.285318, acc.: 85.16%] [G loss: 5.784794]\n",
      "epoch:28 step:22463 [D loss: 0.319625, acc.: 83.59%] [G loss: 4.021364]\n",
      "epoch:28 step:22464 [D loss: 0.212392, acc.: 89.84%] [G loss: 5.833249]\n",
      "epoch:28 step:22465 [D loss: 0.292601, acc.: 87.50%] [G loss: 6.459272]\n",
      "epoch:28 step:22466 [D loss: 0.241609, acc.: 89.06%] [G loss: 3.986184]\n",
      "epoch:28 step:22467 [D loss: 0.228470, acc.: 89.84%] [G loss: 3.891555]\n",
      "epoch:28 step:22468 [D loss: 0.263464, acc.: 90.62%] [G loss: 3.811438]\n",
      "epoch:28 step:22469 [D loss: 0.182505, acc.: 92.97%] [G loss: 4.245641]\n",
      "epoch:28 step:22470 [D loss: 0.261913, acc.: 87.50%] [G loss: 3.966881]\n",
      "epoch:28 step:22471 [D loss: 0.240208, acc.: 87.50%] [G loss: 3.435827]\n",
      "epoch:28 step:22472 [D loss: 0.239298, acc.: 89.06%] [G loss: 4.331038]\n",
      "epoch:28 step:22473 [D loss: 0.302712, acc.: 85.94%] [G loss: 5.258006]\n",
      "epoch:28 step:22474 [D loss: 0.242147, acc.: 88.28%] [G loss: 6.505678]\n",
      "epoch:28 step:22475 [D loss: 0.184628, acc.: 89.06%] [G loss: 12.811605]\n",
      "epoch:28 step:22476 [D loss: 0.126646, acc.: 96.88%] [G loss: 8.794448]\n",
      "epoch:28 step:22477 [D loss: 0.227116, acc.: 89.06%] [G loss: 5.169199]\n",
      "epoch:28 step:22478 [D loss: 0.215412, acc.: 92.19%] [G loss: 5.878013]\n",
      "epoch:28 step:22479 [D loss: 0.250978, acc.: 87.50%] [G loss: 4.576394]\n",
      "epoch:28 step:22480 [D loss: 0.183778, acc.: 93.75%] [G loss: 4.640098]\n",
      "epoch:28 step:22481 [D loss: 0.410119, acc.: 82.03%] [G loss: 4.563141]\n",
      "epoch:28 step:22482 [D loss: 0.249599, acc.: 89.06%] [G loss: 4.364824]\n",
      "epoch:28 step:22483 [D loss: 0.286375, acc.: 86.72%] [G loss: 3.909757]\n",
      "epoch:28 step:22484 [D loss: 0.287739, acc.: 88.28%] [G loss: 3.796985]\n",
      "epoch:28 step:22485 [D loss: 0.427777, acc.: 77.34%] [G loss: 2.776598]\n",
      "epoch:28 step:22486 [D loss: 0.278201, acc.: 89.06%] [G loss: 4.031378]\n",
      "epoch:28 step:22487 [D loss: 0.280495, acc.: 87.50%] [G loss: 4.381296]\n",
      "epoch:28 step:22488 [D loss: 0.336184, acc.: 85.16%] [G loss: 3.294515]\n",
      "epoch:28 step:22489 [D loss: 0.334162, acc.: 85.16%] [G loss: 3.141445]\n",
      "epoch:28 step:22490 [D loss: 0.353960, acc.: 85.16%] [G loss: 4.226675]\n",
      "epoch:28 step:22491 [D loss: 0.294473, acc.: 89.06%] [G loss: 3.991441]\n",
      "epoch:28 step:22492 [D loss: 0.376479, acc.: 84.38%] [G loss: 2.983085]\n",
      "epoch:28 step:22493 [D loss: 0.348916, acc.: 84.38%] [G loss: 3.299258]\n",
      "epoch:28 step:22494 [D loss: 0.226455, acc.: 87.50%] [G loss: 3.746341]\n",
      "epoch:28 step:22495 [D loss: 0.293068, acc.: 86.72%] [G loss: 3.475624]\n",
      "epoch:28 step:22496 [D loss: 0.419256, acc.: 82.03%] [G loss: 3.748439]\n",
      "epoch:28 step:22497 [D loss: 0.379519, acc.: 81.25%] [G loss: 3.011073]\n",
      "epoch:28 step:22498 [D loss: 0.351490, acc.: 85.16%] [G loss: 5.345735]\n",
      "epoch:28 step:22499 [D loss: 0.385325, acc.: 83.59%] [G loss: 3.378988]\n",
      "epoch:28 step:22500 [D loss: 0.607890, acc.: 79.69%] [G loss: 7.312595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22501 [D loss: 2.067038, acc.: 58.59%] [G loss: 6.795319]\n",
      "epoch:28 step:22502 [D loss: 1.666895, acc.: 53.91%] [G loss: 6.892368]\n",
      "epoch:28 step:22503 [D loss: 1.209237, acc.: 70.31%] [G loss: 5.283040]\n",
      "epoch:28 step:22504 [D loss: 1.086101, acc.: 73.44%] [G loss: 4.152622]\n",
      "epoch:28 step:22505 [D loss: 0.486009, acc.: 78.12%] [G loss: 2.431854]\n",
      "epoch:28 step:22506 [D loss: 0.626262, acc.: 72.66%] [G loss: 6.342219]\n",
      "epoch:28 step:22507 [D loss: 0.466959, acc.: 81.25%] [G loss: 3.733557]\n",
      "epoch:28 step:22508 [D loss: 0.282195, acc.: 89.06%] [G loss: 3.597505]\n",
      "epoch:28 step:22509 [D loss: 0.266511, acc.: 86.72%] [G loss: 3.005444]\n",
      "epoch:28 step:22510 [D loss: 0.365380, acc.: 82.81%] [G loss: 2.875759]\n",
      "epoch:28 step:22511 [D loss: 0.284108, acc.: 89.06%] [G loss: 2.608568]\n",
      "epoch:28 step:22512 [D loss: 0.302810, acc.: 85.16%] [G loss: 2.357732]\n",
      "epoch:28 step:22513 [D loss: 0.324435, acc.: 83.59%] [G loss: 2.467309]\n",
      "epoch:28 step:22514 [D loss: 0.457564, acc.: 77.34%] [G loss: 1.980653]\n",
      "epoch:28 step:22515 [D loss: 0.412142, acc.: 80.47%] [G loss: 2.677813]\n",
      "epoch:28 step:22516 [D loss: 0.401738, acc.: 82.81%] [G loss: 2.656516]\n",
      "epoch:28 step:22517 [D loss: 0.310414, acc.: 83.59%] [G loss: 3.571711]\n",
      "epoch:28 step:22518 [D loss: 0.315566, acc.: 85.16%] [G loss: 2.488558]\n",
      "epoch:28 step:22519 [D loss: 0.253562, acc.: 89.84%] [G loss: 3.078277]\n",
      "epoch:28 step:22520 [D loss: 0.259612, acc.: 85.94%] [G loss: 2.909472]\n",
      "epoch:28 step:22521 [D loss: 0.260736, acc.: 89.84%] [G loss: 2.918517]\n",
      "epoch:28 step:22522 [D loss: 0.329328, acc.: 83.59%] [G loss: 2.905780]\n",
      "epoch:28 step:22523 [D loss: 0.266626, acc.: 89.06%] [G loss: 2.511413]\n",
      "epoch:28 step:22524 [D loss: 0.296581, acc.: 86.72%] [G loss: 2.367846]\n",
      "epoch:28 step:22525 [D loss: 0.381041, acc.: 82.81%] [G loss: 2.282622]\n",
      "epoch:28 step:22526 [D loss: 0.285689, acc.: 89.84%] [G loss: 2.942586]\n",
      "epoch:28 step:22527 [D loss: 0.337565, acc.: 85.94%] [G loss: 2.518055]\n",
      "epoch:28 step:22528 [D loss: 0.251782, acc.: 91.41%] [G loss: 2.262105]\n",
      "epoch:28 step:22529 [D loss: 0.402482, acc.: 82.03%] [G loss: 2.217778]\n",
      "epoch:28 step:22530 [D loss: 0.309304, acc.: 83.59%] [G loss: 2.589096]\n",
      "epoch:28 step:22531 [D loss: 0.389318, acc.: 82.03%] [G loss: 2.457228]\n",
      "epoch:28 step:22532 [D loss: 0.280783, acc.: 89.84%] [G loss: 2.805997]\n",
      "epoch:28 step:22533 [D loss: 0.355007, acc.: 83.59%] [G loss: 3.131043]\n",
      "epoch:28 step:22534 [D loss: 0.326114, acc.: 88.28%] [G loss: 3.129433]\n",
      "epoch:28 step:22535 [D loss: 0.237111, acc.: 90.62%] [G loss: 2.307686]\n",
      "epoch:28 step:22536 [D loss: 0.306476, acc.: 89.06%] [G loss: 2.232514]\n",
      "epoch:28 step:22537 [D loss: 0.386996, acc.: 82.03%] [G loss: 2.915566]\n",
      "epoch:28 step:22538 [D loss: 0.279561, acc.: 88.28%] [G loss: 2.833992]\n",
      "epoch:28 step:22539 [D loss: 0.363035, acc.: 83.59%] [G loss: 2.811729]\n",
      "epoch:28 step:22540 [D loss: 0.323229, acc.: 86.72%] [G loss: 2.810147]\n",
      "epoch:28 step:22541 [D loss: 0.306946, acc.: 87.50%] [G loss: 2.688553]\n",
      "epoch:28 step:22542 [D loss: 0.294248, acc.: 85.94%] [G loss: 2.204864]\n",
      "epoch:28 step:22543 [D loss: 0.280355, acc.: 88.28%] [G loss: 2.620817]\n",
      "epoch:28 step:22544 [D loss: 0.331639, acc.: 83.59%] [G loss: 2.595909]\n",
      "epoch:28 step:22545 [D loss: 0.312371, acc.: 87.50%] [G loss: 2.574602]\n",
      "epoch:28 step:22546 [D loss: 0.255798, acc.: 89.84%] [G loss: 2.839284]\n",
      "epoch:28 step:22547 [D loss: 0.316932, acc.: 85.94%] [G loss: 2.647712]\n",
      "epoch:28 step:22548 [D loss: 0.411624, acc.: 78.12%] [G loss: 3.790693]\n",
      "epoch:28 step:22549 [D loss: 0.222440, acc.: 91.41%] [G loss: 3.942517]\n",
      "epoch:28 step:22550 [D loss: 0.309559, acc.: 83.59%] [G loss: 3.053274]\n",
      "epoch:28 step:22551 [D loss: 0.388405, acc.: 82.03%] [G loss: 2.193430]\n",
      "epoch:28 step:22552 [D loss: 0.334808, acc.: 83.59%] [G loss: 2.481073]\n",
      "epoch:28 step:22553 [D loss: 0.344496, acc.: 85.16%] [G loss: 2.632986]\n",
      "epoch:28 step:22554 [D loss: 0.320247, acc.: 85.94%] [G loss: 3.149401]\n",
      "epoch:28 step:22555 [D loss: 0.284874, acc.: 88.28%] [G loss: 3.186004]\n",
      "epoch:28 step:22556 [D loss: 0.282216, acc.: 87.50%] [G loss: 2.425112]\n",
      "epoch:28 step:22557 [D loss: 0.307930, acc.: 85.16%] [G loss: 2.305270]\n",
      "epoch:28 step:22558 [D loss: 0.286306, acc.: 87.50%] [G loss: 3.553095]\n",
      "epoch:28 step:22559 [D loss: 0.197444, acc.: 93.75%] [G loss: 3.153775]\n",
      "epoch:28 step:22560 [D loss: 0.332296, acc.: 85.94%] [G loss: 4.329509]\n",
      "epoch:28 step:22561 [D loss: 0.368755, acc.: 83.59%] [G loss: 3.364869]\n",
      "epoch:28 step:22562 [D loss: 0.221696, acc.: 92.97%] [G loss: 5.539871]\n",
      "epoch:28 step:22563 [D loss: 0.275882, acc.: 87.50%] [G loss: 4.321639]\n",
      "epoch:28 step:22564 [D loss: 0.374492, acc.: 82.03%] [G loss: 4.140350]\n",
      "epoch:28 step:22565 [D loss: 0.293009, acc.: 89.06%] [G loss: 3.905765]\n",
      "epoch:28 step:22566 [D loss: 0.350881, acc.: 82.81%] [G loss: 3.035324]\n",
      "epoch:28 step:22567 [D loss: 0.234742, acc.: 89.84%] [G loss: 3.747189]\n",
      "epoch:28 step:22568 [D loss: 0.333959, acc.: 82.81%] [G loss: 3.784204]\n",
      "epoch:28 step:22569 [D loss: 0.298739, acc.: 87.50%] [G loss: 3.786011]\n",
      "epoch:28 step:22570 [D loss: 0.330178, acc.: 85.16%] [G loss: 3.184277]\n",
      "epoch:28 step:22571 [D loss: 0.380551, acc.: 82.03%] [G loss: 3.709744]\n",
      "epoch:28 step:22572 [D loss: 0.227002, acc.: 88.28%] [G loss: 4.074505]\n",
      "epoch:28 step:22573 [D loss: 0.319931, acc.: 82.03%] [G loss: 3.967340]\n",
      "epoch:28 step:22574 [D loss: 0.272814, acc.: 85.94%] [G loss: 5.107251]\n",
      "epoch:28 step:22575 [D loss: 0.366630, acc.: 79.69%] [G loss: 4.019248]\n",
      "epoch:28 step:22576 [D loss: 0.259525, acc.: 87.50%] [G loss: 2.985459]\n",
      "epoch:28 step:22577 [D loss: 0.300474, acc.: 85.94%] [G loss: 3.609301]\n",
      "epoch:28 step:22578 [D loss: 0.259317, acc.: 87.50%] [G loss: 3.979613]\n",
      "epoch:28 step:22579 [D loss: 0.276830, acc.: 88.28%] [G loss: 2.557925]\n",
      "epoch:28 step:22580 [D loss: 0.271217, acc.: 89.84%] [G loss: 3.109389]\n",
      "epoch:28 step:22581 [D loss: 0.315144, acc.: 85.16%] [G loss: 2.833263]\n",
      "epoch:28 step:22582 [D loss: 0.317103, acc.: 85.94%] [G loss: 3.131521]\n",
      "epoch:28 step:22583 [D loss: 0.425514, acc.: 78.12%] [G loss: 2.121631]\n",
      "epoch:28 step:22584 [D loss: 0.413747, acc.: 78.12%] [G loss: 3.163912]\n",
      "epoch:28 step:22585 [D loss: 0.326651, acc.: 87.50%] [G loss: 2.904491]\n",
      "epoch:28 step:22586 [D loss: 0.369462, acc.: 82.03%] [G loss: 2.678994]\n",
      "epoch:28 step:22587 [D loss: 0.324047, acc.: 83.59%] [G loss: 3.201051]\n",
      "epoch:28 step:22588 [D loss: 0.298185, acc.: 85.16%] [G loss: 2.687131]\n",
      "epoch:28 step:22589 [D loss: 0.416846, acc.: 82.81%] [G loss: 3.641587]\n",
      "epoch:28 step:22590 [D loss: 0.266833, acc.: 90.62%] [G loss: 3.380186]\n",
      "epoch:28 step:22591 [D loss: 0.279753, acc.: 86.72%] [G loss: 3.195447]\n",
      "epoch:28 step:22592 [D loss: 0.207858, acc.: 92.97%] [G loss: 3.829568]\n",
      "epoch:28 step:22593 [D loss: 0.299278, acc.: 88.28%] [G loss: 2.527393]\n",
      "epoch:28 step:22594 [D loss: 0.280434, acc.: 86.72%] [G loss: 3.291884]\n",
      "epoch:28 step:22595 [D loss: 0.415103, acc.: 85.16%] [G loss: 2.644944]\n",
      "epoch:28 step:22596 [D loss: 0.479514, acc.: 80.47%] [G loss: 2.893721]\n",
      "epoch:28 step:22597 [D loss: 0.350393, acc.: 89.06%] [G loss: 3.385198]\n",
      "epoch:28 step:22598 [D loss: 0.323344, acc.: 85.94%] [G loss: 3.161401]\n",
      "epoch:28 step:22599 [D loss: 0.385390, acc.: 84.38%] [G loss: 3.202974]\n",
      "epoch:28 step:22600 [D loss: 0.300933, acc.: 88.28%] [G loss: 2.632236]\n",
      "epoch:28 step:22601 [D loss: 0.277066, acc.: 85.94%] [G loss: 4.170777]\n",
      "epoch:28 step:22602 [D loss: 0.385397, acc.: 79.69%] [G loss: 2.661302]\n",
      "epoch:28 step:22603 [D loss: 0.389289, acc.: 82.81%] [G loss: 2.695568]\n",
      "epoch:28 step:22604 [D loss: 0.307167, acc.: 85.94%] [G loss: 3.133625]\n",
      "epoch:28 step:22605 [D loss: 0.414159, acc.: 82.03%] [G loss: 2.337200]\n",
      "epoch:28 step:22606 [D loss: 0.313273, acc.: 83.59%] [G loss: 2.934743]\n",
      "epoch:28 step:22607 [D loss: 0.289902, acc.: 85.16%] [G loss: 3.292762]\n",
      "epoch:28 step:22608 [D loss: 0.321818, acc.: 85.94%] [G loss: 3.696123]\n",
      "epoch:28 step:22609 [D loss: 0.267799, acc.: 89.84%] [G loss: 2.301178]\n",
      "epoch:28 step:22610 [D loss: 0.243115, acc.: 89.06%] [G loss: 3.611426]\n",
      "epoch:28 step:22611 [D loss: 0.420460, acc.: 82.03%] [G loss: 2.994738]\n",
      "epoch:28 step:22612 [D loss: 0.329082, acc.: 84.38%] [G loss: 3.350163]\n",
      "epoch:28 step:22613 [D loss: 0.354792, acc.: 83.59%] [G loss: 3.465950]\n",
      "epoch:28 step:22614 [D loss: 0.308079, acc.: 86.72%] [G loss: 2.113614]\n",
      "epoch:28 step:22615 [D loss: 0.307564, acc.: 87.50%] [G loss: 2.982457]\n",
      "epoch:28 step:22616 [D loss: 0.303612, acc.: 86.72%] [G loss: 3.641815]\n",
      "epoch:28 step:22617 [D loss: 0.390090, acc.: 79.69%] [G loss: 3.798423]\n",
      "epoch:28 step:22618 [D loss: 0.308262, acc.: 86.72%] [G loss: 4.583383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22619 [D loss: 0.357314, acc.: 84.38%] [G loss: 3.268116]\n",
      "epoch:28 step:22620 [D loss: 0.256457, acc.: 87.50%] [G loss: 4.552313]\n",
      "epoch:28 step:22621 [D loss: 0.423634, acc.: 79.69%] [G loss: 3.550177]\n",
      "epoch:28 step:22622 [D loss: 0.395638, acc.: 83.59%] [G loss: 3.281415]\n",
      "epoch:28 step:22623 [D loss: 0.372941, acc.: 78.91%] [G loss: 3.702777]\n",
      "epoch:28 step:22624 [D loss: 0.292829, acc.: 86.72%] [G loss: 3.108768]\n",
      "epoch:28 step:22625 [D loss: 0.411750, acc.: 82.03%] [G loss: 3.189117]\n",
      "epoch:28 step:22626 [D loss: 0.231104, acc.: 93.75%] [G loss: 2.969423]\n",
      "epoch:28 step:22627 [D loss: 0.317122, acc.: 85.94%] [G loss: 3.016935]\n",
      "epoch:28 step:22628 [D loss: 0.254734, acc.: 90.62%] [G loss: 3.563542]\n",
      "epoch:28 step:22629 [D loss: 0.310624, acc.: 85.16%] [G loss: 3.656795]\n",
      "epoch:28 step:22630 [D loss: 0.349671, acc.: 85.16%] [G loss: 3.063594]\n",
      "epoch:28 step:22631 [D loss: 0.315934, acc.: 85.94%] [G loss: 2.764916]\n",
      "epoch:28 step:22632 [D loss: 0.358793, acc.: 83.59%] [G loss: 2.565841]\n",
      "epoch:28 step:22633 [D loss: 0.439898, acc.: 79.69%] [G loss: 2.044913]\n",
      "epoch:28 step:22634 [D loss: 0.314656, acc.: 86.72%] [G loss: 4.416806]\n",
      "epoch:28 step:22635 [D loss: 0.278721, acc.: 88.28%] [G loss: 2.936817]\n",
      "epoch:28 step:22636 [D loss: 0.318850, acc.: 84.38%] [G loss: 2.632215]\n",
      "epoch:28 step:22637 [D loss: 0.228441, acc.: 90.62%] [G loss: 4.179275]\n",
      "epoch:28 step:22638 [D loss: 0.324031, acc.: 85.94%] [G loss: 3.360016]\n",
      "epoch:28 step:22639 [D loss: 0.269854, acc.: 86.72%] [G loss: 3.130353]\n",
      "epoch:28 step:22640 [D loss: 0.276045, acc.: 87.50%] [G loss: 3.460739]\n",
      "epoch:28 step:22641 [D loss: 0.405349, acc.: 83.59%] [G loss: 5.568681]\n",
      "epoch:28 step:22642 [D loss: 0.412012, acc.: 78.91%] [G loss: 3.720792]\n",
      "epoch:28 step:22643 [D loss: 0.276264, acc.: 89.06%] [G loss: 3.372659]\n",
      "epoch:28 step:22644 [D loss: 0.329271, acc.: 85.16%] [G loss: 3.791484]\n",
      "epoch:28 step:22645 [D loss: 0.442380, acc.: 78.91%] [G loss: 2.394044]\n",
      "epoch:28 step:22646 [D loss: 0.312529, acc.: 87.50%] [G loss: 3.295805]\n",
      "epoch:28 step:22647 [D loss: 0.415099, acc.: 80.47%] [G loss: 2.872572]\n",
      "epoch:28 step:22648 [D loss: 0.409515, acc.: 84.38%] [G loss: 6.058027]\n",
      "epoch:28 step:22649 [D loss: 0.482432, acc.: 82.81%] [G loss: 4.073330]\n",
      "epoch:29 step:22650 [D loss: 0.298587, acc.: 87.50%] [G loss: 3.345846]\n",
      "epoch:29 step:22651 [D loss: 0.233132, acc.: 89.06%] [G loss: 3.151465]\n",
      "epoch:29 step:22652 [D loss: 0.348653, acc.: 81.25%] [G loss: 3.208366]\n",
      "epoch:29 step:22653 [D loss: 0.275679, acc.: 86.72%] [G loss: 3.188936]\n",
      "epoch:29 step:22654 [D loss: 0.337782, acc.: 83.59%] [G loss: 3.059586]\n",
      "epoch:29 step:22655 [D loss: 0.260661, acc.: 89.06%] [G loss: 3.046567]\n",
      "epoch:29 step:22656 [D loss: 0.280607, acc.: 89.06%] [G loss: 2.965082]\n",
      "epoch:29 step:22657 [D loss: 0.283833, acc.: 83.59%] [G loss: 2.782723]\n",
      "epoch:29 step:22658 [D loss: 0.354471, acc.: 83.59%] [G loss: 2.645278]\n",
      "epoch:29 step:22659 [D loss: 0.343860, acc.: 83.59%] [G loss: 2.771875]\n",
      "epoch:29 step:22660 [D loss: 0.311940, acc.: 85.16%] [G loss: 3.622643]\n",
      "epoch:29 step:22661 [D loss: 0.271200, acc.: 92.19%] [G loss: 3.049661]\n",
      "epoch:29 step:22662 [D loss: 0.352545, acc.: 85.16%] [G loss: 5.067670]\n",
      "epoch:29 step:22663 [D loss: 0.376712, acc.: 82.81%] [G loss: 3.353807]\n",
      "epoch:29 step:22664 [D loss: 0.377371, acc.: 79.69%] [G loss: 2.984858]\n",
      "epoch:29 step:22665 [D loss: 0.283363, acc.: 87.50%] [G loss: 3.330729]\n",
      "epoch:29 step:22666 [D loss: 0.300581, acc.: 85.16%] [G loss: 2.774905]\n",
      "epoch:29 step:22667 [D loss: 0.354703, acc.: 82.03%] [G loss: 2.731656]\n",
      "epoch:29 step:22668 [D loss: 0.294025, acc.: 85.94%] [G loss: 2.679254]\n",
      "epoch:29 step:22669 [D loss: 0.339728, acc.: 88.28%] [G loss: 2.974480]\n",
      "epoch:29 step:22670 [D loss: 0.322367, acc.: 84.38%] [G loss: 2.700256]\n",
      "epoch:29 step:22671 [D loss: 0.342191, acc.: 82.81%] [G loss: 1.981080]\n",
      "epoch:29 step:22672 [D loss: 0.255394, acc.: 89.06%] [G loss: 3.249367]\n",
      "epoch:29 step:22673 [D loss: 0.333589, acc.: 81.25%] [G loss: 3.018357]\n",
      "epoch:29 step:22674 [D loss: 0.277905, acc.: 86.72%] [G loss: 2.599978]\n",
      "epoch:29 step:22675 [D loss: 0.311591, acc.: 87.50%] [G loss: 2.741694]\n",
      "epoch:29 step:22676 [D loss: 0.333851, acc.: 84.38%] [G loss: 2.850749]\n",
      "epoch:29 step:22677 [D loss: 0.322213, acc.: 85.16%] [G loss: 2.904581]\n",
      "epoch:29 step:22678 [D loss: 0.430668, acc.: 78.91%] [G loss: 2.976052]\n",
      "epoch:29 step:22679 [D loss: 0.349640, acc.: 87.50%] [G loss: 3.031814]\n",
      "epoch:29 step:22680 [D loss: 0.415836, acc.: 79.69%] [G loss: 3.397862]\n",
      "epoch:29 step:22681 [D loss: 0.258961, acc.: 91.41%] [G loss: 5.032597]\n",
      "epoch:29 step:22682 [D loss: 0.324467, acc.: 88.28%] [G loss: 3.105575]\n",
      "epoch:29 step:22683 [D loss: 0.441978, acc.: 78.12%] [G loss: 2.930123]\n",
      "epoch:29 step:22684 [D loss: 0.262345, acc.: 89.06%] [G loss: 2.657694]\n",
      "epoch:29 step:22685 [D loss: 0.278694, acc.: 86.72%] [G loss: 3.598548]\n",
      "epoch:29 step:22686 [D loss: 0.258331, acc.: 90.62%] [G loss: 3.314286]\n",
      "epoch:29 step:22687 [D loss: 0.293942, acc.: 85.94%] [G loss: 3.242032]\n",
      "epoch:29 step:22688 [D loss: 0.299663, acc.: 84.38%] [G loss: 6.121383]\n",
      "epoch:29 step:22689 [D loss: 0.376491, acc.: 83.59%] [G loss: 3.417833]\n",
      "epoch:29 step:22690 [D loss: 0.363321, acc.: 81.25%] [G loss: 3.889143]\n",
      "epoch:29 step:22691 [D loss: 0.320060, acc.: 85.16%] [G loss: 3.454407]\n",
      "epoch:29 step:22692 [D loss: 0.309780, acc.: 85.16%] [G loss: 3.790950]\n",
      "epoch:29 step:22693 [D loss: 0.271668, acc.: 85.16%] [G loss: 3.615208]\n",
      "epoch:29 step:22694 [D loss: 0.235998, acc.: 91.41%] [G loss: 2.626343]\n",
      "epoch:29 step:22695 [D loss: 0.381097, acc.: 81.25%] [G loss: 2.786958]\n",
      "epoch:29 step:22696 [D loss: 0.417982, acc.: 80.47%] [G loss: 3.132857]\n",
      "epoch:29 step:22697 [D loss: 0.311881, acc.: 85.94%] [G loss: 3.005949]\n",
      "epoch:29 step:22698 [D loss: 0.302363, acc.: 87.50%] [G loss: 4.735353]\n",
      "epoch:29 step:22699 [D loss: 0.459825, acc.: 78.12%] [G loss: 3.233478]\n",
      "epoch:29 step:22700 [D loss: 0.269857, acc.: 89.84%] [G loss: 4.854499]\n",
      "epoch:29 step:22701 [D loss: 0.221439, acc.: 87.50%] [G loss: 3.719816]\n",
      "epoch:29 step:22702 [D loss: 0.213523, acc.: 91.41%] [G loss: 3.807299]\n",
      "epoch:29 step:22703 [D loss: 0.286859, acc.: 91.41%] [G loss: 3.327865]\n",
      "epoch:29 step:22704 [D loss: 0.277721, acc.: 90.62%] [G loss: 3.254050]\n",
      "epoch:29 step:22705 [D loss: 0.298908, acc.: 83.59%] [G loss: 3.052547]\n",
      "epoch:29 step:22706 [D loss: 0.408976, acc.: 84.38%] [G loss: 3.138391]\n",
      "epoch:29 step:22707 [D loss: 0.289492, acc.: 88.28%] [G loss: 3.792844]\n",
      "epoch:29 step:22708 [D loss: 0.271590, acc.: 87.50%] [G loss: 2.762436]\n",
      "epoch:29 step:22709 [D loss: 0.286906, acc.: 86.72%] [G loss: 3.139227]\n",
      "epoch:29 step:22710 [D loss: 0.316416, acc.: 83.59%] [G loss: 2.769846]\n",
      "epoch:29 step:22711 [D loss: 0.315685, acc.: 85.16%] [G loss: 2.937951]\n",
      "epoch:29 step:22712 [D loss: 0.305559, acc.: 85.94%] [G loss: 4.299583]\n",
      "epoch:29 step:22713 [D loss: 0.331711, acc.: 82.03%] [G loss: 3.615013]\n",
      "epoch:29 step:22714 [D loss: 0.370751, acc.: 85.16%] [G loss: 4.588742]\n",
      "epoch:29 step:22715 [D loss: 0.428882, acc.: 78.12%] [G loss: 3.410636]\n",
      "epoch:29 step:22716 [D loss: 0.384963, acc.: 76.56%] [G loss: 3.320046]\n",
      "epoch:29 step:22717 [D loss: 0.280315, acc.: 88.28%] [G loss: 3.036297]\n",
      "epoch:29 step:22718 [D loss: 0.306839, acc.: 85.16%] [G loss: 3.035639]\n",
      "epoch:29 step:22719 [D loss: 0.361638, acc.: 85.94%] [G loss: 3.857391]\n",
      "epoch:29 step:22720 [D loss: 0.342588, acc.: 82.03%] [G loss: 4.608603]\n",
      "epoch:29 step:22721 [D loss: 0.291598, acc.: 90.62%] [G loss: 4.224913]\n",
      "epoch:29 step:22722 [D loss: 0.258046, acc.: 88.28%] [G loss: 4.556110]\n",
      "epoch:29 step:22723 [D loss: 0.396639, acc.: 78.91%] [G loss: 2.275326]\n",
      "epoch:29 step:22724 [D loss: 0.194948, acc.: 92.97%] [G loss: 4.960070]\n",
      "epoch:29 step:22725 [D loss: 0.253501, acc.: 89.84%] [G loss: 3.193737]\n",
      "epoch:29 step:22726 [D loss: 0.329942, acc.: 84.38%] [G loss: 2.780922]\n",
      "epoch:29 step:22727 [D loss: 0.288317, acc.: 87.50%] [G loss: 2.978558]\n",
      "epoch:29 step:22728 [D loss: 0.327171, acc.: 84.38%] [G loss: 2.648791]\n",
      "epoch:29 step:22729 [D loss: 0.361995, acc.: 82.81%] [G loss: 3.563205]\n",
      "epoch:29 step:22730 [D loss: 0.303311, acc.: 85.16%] [G loss: 3.256471]\n",
      "epoch:29 step:22731 [D loss: 0.297840, acc.: 88.28%] [G loss: 3.435634]\n",
      "epoch:29 step:22732 [D loss: 0.264435, acc.: 91.41%] [G loss: 2.604713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:22733 [D loss: 0.293634, acc.: 89.06%] [G loss: 2.790498]\n",
      "epoch:29 step:22734 [D loss: 0.234802, acc.: 92.97%] [G loss: 3.196446]\n",
      "epoch:29 step:22735 [D loss: 0.313607, acc.: 84.38%] [G loss: 2.815569]\n",
      "epoch:29 step:22736 [D loss: 0.277479, acc.: 87.50%] [G loss: 2.380367]\n",
      "epoch:29 step:22737 [D loss: 0.336242, acc.: 82.03%] [G loss: 3.247805]\n",
      "epoch:29 step:22738 [D loss: 0.444443, acc.: 81.25%] [G loss: 2.500702]\n",
      "epoch:29 step:22739 [D loss: 0.465862, acc.: 76.56%] [G loss: 4.203077]\n",
      "epoch:29 step:22740 [D loss: 0.496144, acc.: 77.34%] [G loss: 3.806619]\n",
      "epoch:29 step:22741 [D loss: 0.368001, acc.: 85.94%] [G loss: 3.101478]\n",
      "epoch:29 step:22742 [D loss: 0.351292, acc.: 86.72%] [G loss: 3.052521]\n",
      "epoch:29 step:22743 [D loss: 0.332237, acc.: 84.38%] [G loss: 4.079173]\n",
      "epoch:29 step:22744 [D loss: 0.470665, acc.: 78.91%] [G loss: 5.097521]\n",
      "epoch:29 step:22745 [D loss: 0.383791, acc.: 79.69%] [G loss: 5.991575]\n",
      "epoch:29 step:22746 [D loss: 0.293675, acc.: 87.50%] [G loss: 4.920353]\n",
      "epoch:29 step:22747 [D loss: 0.348499, acc.: 87.50%] [G loss: 5.245231]\n",
      "epoch:29 step:22748 [D loss: 0.256004, acc.: 90.62%] [G loss: 5.267999]\n",
      "epoch:29 step:22749 [D loss: 0.239099, acc.: 92.19%] [G loss: 4.738127]\n",
      "epoch:29 step:22750 [D loss: 0.211917, acc.: 91.41%] [G loss: 4.151790]\n",
      "epoch:29 step:22751 [D loss: 0.272267, acc.: 85.16%] [G loss: 3.181437]\n",
      "epoch:29 step:22752 [D loss: 0.347655, acc.: 82.03%] [G loss: 3.919702]\n",
      "epoch:29 step:22753 [D loss: 0.318062, acc.: 86.72%] [G loss: 3.668486]\n",
      "epoch:29 step:22754 [D loss: 0.291624, acc.: 84.38%] [G loss: 3.015899]\n",
      "epoch:29 step:22755 [D loss: 0.345288, acc.: 85.16%] [G loss: 4.874268]\n",
      "epoch:29 step:22756 [D loss: 0.476952, acc.: 77.34%] [G loss: 4.143347]\n",
      "epoch:29 step:22757 [D loss: 0.252185, acc.: 89.84%] [G loss: 3.890120]\n",
      "epoch:29 step:22758 [D loss: 0.422040, acc.: 85.94%] [G loss: 3.853160]\n",
      "epoch:29 step:22759 [D loss: 0.384139, acc.: 81.25%] [G loss: 3.645155]\n",
      "epoch:29 step:22760 [D loss: 0.268264, acc.: 87.50%] [G loss: 4.591109]\n",
      "epoch:29 step:22761 [D loss: 0.169909, acc.: 96.09%] [G loss: 5.900581]\n",
      "epoch:29 step:22762 [D loss: 0.232803, acc.: 89.06%] [G loss: 3.697366]\n",
      "epoch:29 step:22763 [D loss: 0.276464, acc.: 85.94%] [G loss: 4.876408]\n",
      "epoch:29 step:22764 [D loss: 0.326939, acc.: 85.94%] [G loss: 5.224566]\n",
      "epoch:29 step:22765 [D loss: 0.404367, acc.: 82.81%] [G loss: 4.293995]\n",
      "epoch:29 step:22766 [D loss: 0.287763, acc.: 88.28%] [G loss: 4.833757]\n",
      "epoch:29 step:22767 [D loss: 0.427187, acc.: 81.25%] [G loss: 6.085355]\n",
      "epoch:29 step:22768 [D loss: 1.117507, acc.: 71.09%] [G loss: 6.446447]\n",
      "epoch:29 step:22769 [D loss: 1.970058, acc.: 64.84%] [G loss: 4.121188]\n",
      "epoch:29 step:22770 [D loss: 0.492909, acc.: 82.03%] [G loss: 5.999912]\n",
      "epoch:29 step:22771 [D loss: 1.304612, acc.: 61.72%] [G loss: 6.534641]\n",
      "epoch:29 step:22772 [D loss: 0.728724, acc.: 72.66%] [G loss: 3.419862]\n",
      "epoch:29 step:22773 [D loss: 0.395984, acc.: 78.12%] [G loss: 3.044682]\n",
      "epoch:29 step:22774 [D loss: 0.344040, acc.: 85.16%] [G loss: 3.029803]\n",
      "epoch:29 step:22775 [D loss: 0.348258, acc.: 83.59%] [G loss: 2.879812]\n",
      "epoch:29 step:22776 [D loss: 0.325654, acc.: 85.16%] [G loss: 2.669679]\n",
      "epoch:29 step:22777 [D loss: 0.305399, acc.: 85.94%] [G loss: 3.169184]\n",
      "epoch:29 step:22778 [D loss: 0.352330, acc.: 86.72%] [G loss: 2.418558]\n",
      "epoch:29 step:22779 [D loss: 0.342463, acc.: 83.59%] [G loss: 2.613219]\n",
      "epoch:29 step:22780 [D loss: 0.438758, acc.: 82.81%] [G loss: 2.221093]\n",
      "epoch:29 step:22781 [D loss: 0.390186, acc.: 81.25%] [G loss: 2.543999]\n",
      "epoch:29 step:22782 [D loss: 0.349121, acc.: 85.94%] [G loss: 2.879165]\n",
      "epoch:29 step:22783 [D loss: 0.383695, acc.: 82.03%] [G loss: 2.576294]\n",
      "epoch:29 step:22784 [D loss: 0.292730, acc.: 89.84%] [G loss: 2.497461]\n",
      "epoch:29 step:22785 [D loss: 0.364643, acc.: 82.81%] [G loss: 2.336570]\n",
      "epoch:29 step:22786 [D loss: 0.377052, acc.: 81.25%] [G loss: 2.884786]\n",
      "epoch:29 step:22787 [D loss: 0.334585, acc.: 85.16%] [G loss: 2.996538]\n",
      "epoch:29 step:22788 [D loss: 0.331359, acc.: 85.94%] [G loss: 2.855286]\n",
      "epoch:29 step:22789 [D loss: 0.300410, acc.: 89.06%] [G loss: 2.622139]\n",
      "epoch:29 step:22790 [D loss: 0.417579, acc.: 79.69%] [G loss: 2.429266]\n",
      "epoch:29 step:22791 [D loss: 0.398769, acc.: 83.59%] [G loss: 2.321646]\n",
      "epoch:29 step:22792 [D loss: 0.343654, acc.: 84.38%] [G loss: 2.431878]\n",
      "epoch:29 step:22793 [D loss: 0.320890, acc.: 84.38%] [G loss: 2.762991]\n",
      "epoch:29 step:22794 [D loss: 0.359959, acc.: 81.25%] [G loss: 3.444731]\n",
      "epoch:29 step:22795 [D loss: 0.248477, acc.: 89.06%] [G loss: 2.703817]\n",
      "epoch:29 step:22796 [D loss: 0.330318, acc.: 86.72%] [G loss: 2.675364]\n",
      "epoch:29 step:22797 [D loss: 0.443361, acc.: 76.56%] [G loss: 2.509309]\n",
      "epoch:29 step:22798 [D loss: 0.359814, acc.: 85.94%] [G loss: 2.812497]\n",
      "epoch:29 step:22799 [D loss: 0.350799, acc.: 85.16%] [G loss: 2.675391]\n",
      "epoch:29 step:22800 [D loss: 0.330478, acc.: 82.81%] [G loss: 3.452527]\n",
      "epoch:29 step:22801 [D loss: 0.385187, acc.: 82.81%] [G loss: 2.599136]\n",
      "epoch:29 step:22802 [D loss: 0.300463, acc.: 86.72%] [G loss: 3.167913]\n",
      "epoch:29 step:22803 [D loss: 0.299348, acc.: 85.16%] [G loss: 2.679346]\n",
      "epoch:29 step:22804 [D loss: 0.291725, acc.: 89.06%] [G loss: 3.046977]\n",
      "epoch:29 step:22805 [D loss: 0.290704, acc.: 88.28%] [G loss: 2.920473]\n",
      "epoch:29 step:22806 [D loss: 0.295666, acc.: 87.50%] [G loss: 3.431466]\n",
      "epoch:29 step:22807 [D loss: 0.462016, acc.: 78.12%] [G loss: 2.979188]\n",
      "epoch:29 step:22808 [D loss: 0.294533, acc.: 88.28%] [G loss: 2.583942]\n",
      "epoch:29 step:22809 [D loss: 0.248112, acc.: 90.62%] [G loss: 2.758594]\n",
      "epoch:29 step:22810 [D loss: 0.359321, acc.: 82.03%] [G loss: 2.785031]\n",
      "epoch:29 step:22811 [D loss: 0.200706, acc.: 92.97%] [G loss: 2.940881]\n",
      "epoch:29 step:22812 [D loss: 0.294153, acc.: 86.72%] [G loss: 3.442764]\n",
      "epoch:29 step:22813 [D loss: 0.324903, acc.: 84.38%] [G loss: 3.636236]\n",
      "epoch:29 step:22814 [D loss: 0.389839, acc.: 82.81%] [G loss: 3.351740]\n",
      "epoch:29 step:22815 [D loss: 0.327100, acc.: 85.94%] [G loss: 2.958159]\n",
      "epoch:29 step:22816 [D loss: 0.412262, acc.: 84.38%] [G loss: 7.610514]\n",
      "epoch:29 step:22817 [D loss: 0.535816, acc.: 86.72%] [G loss: 4.841494]\n",
      "epoch:29 step:22818 [D loss: 0.387953, acc.: 83.59%] [G loss: 2.786165]\n",
      "epoch:29 step:22819 [D loss: 0.255906, acc.: 89.06%] [G loss: 3.305141]\n",
      "epoch:29 step:22820 [D loss: 0.316986, acc.: 82.03%] [G loss: 4.036922]\n",
      "epoch:29 step:22821 [D loss: 0.322677, acc.: 86.72%] [G loss: 3.265145]\n",
      "epoch:29 step:22822 [D loss: 0.241927, acc.: 91.41%] [G loss: 3.330808]\n",
      "epoch:29 step:22823 [D loss: 0.412997, acc.: 81.25%] [G loss: 2.978331]\n",
      "epoch:29 step:22824 [D loss: 0.344022, acc.: 85.16%] [G loss: 3.697260]\n",
      "epoch:29 step:22825 [D loss: 0.368728, acc.: 85.94%] [G loss: 3.553271]\n",
      "epoch:29 step:22826 [D loss: 0.249802, acc.: 91.41%] [G loss: 3.825573]\n",
      "epoch:29 step:22827 [D loss: 0.313962, acc.: 84.38%] [G loss: 4.348040]\n",
      "epoch:29 step:22828 [D loss: 0.392032, acc.: 77.34%] [G loss: 3.804717]\n",
      "epoch:29 step:22829 [D loss: 0.419080, acc.: 82.81%] [G loss: 3.662103]\n",
      "epoch:29 step:22830 [D loss: 0.305018, acc.: 85.94%] [G loss: 3.637449]\n",
      "epoch:29 step:22831 [D loss: 0.389833, acc.: 79.69%] [G loss: 3.015687]\n",
      "epoch:29 step:22832 [D loss: 0.312808, acc.: 86.72%] [G loss: 2.895493]\n",
      "epoch:29 step:22833 [D loss: 0.377645, acc.: 82.81%] [G loss: 2.855707]\n",
      "epoch:29 step:22834 [D loss: 0.452819, acc.: 81.25%] [G loss: 2.749142]\n",
      "epoch:29 step:22835 [D loss: 0.376045, acc.: 88.28%] [G loss: 2.830429]\n",
      "epoch:29 step:22836 [D loss: 0.369275, acc.: 85.16%] [G loss: 2.429283]\n",
      "epoch:29 step:22837 [D loss: 0.403693, acc.: 77.34%] [G loss: 2.999340]\n",
      "epoch:29 step:22838 [D loss: 0.403980, acc.: 81.25%] [G loss: 2.283259]\n",
      "epoch:29 step:22839 [D loss: 0.378990, acc.: 82.03%] [G loss: 3.642168]\n",
      "epoch:29 step:22840 [D loss: 0.367901, acc.: 80.47%] [G loss: 3.592953]\n",
      "epoch:29 step:22841 [D loss: 0.257123, acc.: 87.50%] [G loss: 4.063455]\n",
      "epoch:29 step:22842 [D loss: 0.372298, acc.: 79.69%] [G loss: 2.788527]\n",
      "epoch:29 step:22843 [D loss: 0.272898, acc.: 85.94%] [G loss: 3.836992]\n",
      "epoch:29 step:22844 [D loss: 0.290236, acc.: 87.50%] [G loss: 3.537994]\n",
      "epoch:29 step:22845 [D loss: 0.341220, acc.: 82.81%] [G loss: 2.742151]\n",
      "epoch:29 step:22846 [D loss: 0.464330, acc.: 80.47%] [G loss: 2.689984]\n",
      "epoch:29 step:22847 [D loss: 0.401007, acc.: 80.47%] [G loss: 2.690676]\n",
      "epoch:29 step:22848 [D loss: 0.355724, acc.: 84.38%] [G loss: 2.977158]\n",
      "epoch:29 step:22849 [D loss: 0.484519, acc.: 80.47%] [G loss: 3.118757]\n",
      "epoch:29 step:22850 [D loss: 0.372185, acc.: 84.38%] [G loss: 2.525370]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:22851 [D loss: 0.448616, acc.: 78.91%] [G loss: 2.972081]\n",
      "epoch:29 step:22852 [D loss: 0.255484, acc.: 88.28%] [G loss: 3.552498]\n",
      "epoch:29 step:22853 [D loss: 0.382478, acc.: 77.34%] [G loss: 3.687480]\n",
      "epoch:29 step:22854 [D loss: 0.349212, acc.: 85.16%] [G loss: 2.175142]\n",
      "epoch:29 step:22855 [D loss: 0.378230, acc.: 80.47%] [G loss: 2.496223]\n",
      "epoch:29 step:22856 [D loss: 0.319358, acc.: 87.50%] [G loss: 2.354763]\n",
      "epoch:29 step:22857 [D loss: 0.440588, acc.: 78.91%] [G loss: 2.474432]\n",
      "epoch:29 step:22858 [D loss: 0.311186, acc.: 84.38%] [G loss: 2.572332]\n",
      "epoch:29 step:22859 [D loss: 0.310395, acc.: 85.16%] [G loss: 2.681269]\n",
      "epoch:29 step:22860 [D loss: 0.405042, acc.: 82.81%] [G loss: 3.545734]\n",
      "epoch:29 step:22861 [D loss: 0.274098, acc.: 88.28%] [G loss: 5.253416]\n",
      "epoch:29 step:22862 [D loss: 0.372703, acc.: 81.25%] [G loss: 2.972612]\n",
      "epoch:29 step:22863 [D loss: 0.393633, acc.: 82.81%] [G loss: 4.305264]\n",
      "epoch:29 step:22864 [D loss: 0.332864, acc.: 86.72%] [G loss: 3.712881]\n",
      "epoch:29 step:22865 [D loss: 0.289343, acc.: 83.59%] [G loss: 3.835387]\n",
      "epoch:29 step:22866 [D loss: 0.374556, acc.: 78.91%] [G loss: 3.144822]\n",
      "epoch:29 step:22867 [D loss: 0.235155, acc.: 93.75%] [G loss: 2.503407]\n",
      "epoch:29 step:22868 [D loss: 0.307648, acc.: 85.94%] [G loss: 3.483387]\n",
      "epoch:29 step:22869 [D loss: 0.302221, acc.: 88.28%] [G loss: 3.613636]\n",
      "epoch:29 step:22870 [D loss: 0.266443, acc.: 89.06%] [G loss: 4.374928]\n",
      "epoch:29 step:22871 [D loss: 0.234474, acc.: 85.94%] [G loss: 4.059720]\n",
      "epoch:29 step:22872 [D loss: 0.331283, acc.: 83.59%] [G loss: 3.798441]\n",
      "epoch:29 step:22873 [D loss: 0.329549, acc.: 85.94%] [G loss: 4.667803]\n",
      "epoch:29 step:22874 [D loss: 0.318399, acc.: 85.16%] [G loss: 4.113980]\n",
      "epoch:29 step:22875 [D loss: 0.377008, acc.: 83.59%] [G loss: 3.878658]\n",
      "epoch:29 step:22876 [D loss: 0.250686, acc.: 88.28%] [G loss: 3.478783]\n",
      "epoch:29 step:22877 [D loss: 0.256706, acc.: 88.28%] [G loss: 3.548987]\n",
      "epoch:29 step:22878 [D loss: 0.307541, acc.: 87.50%] [G loss: 3.944261]\n",
      "epoch:29 step:22879 [D loss: 0.312915, acc.: 85.94%] [G loss: 2.948781]\n",
      "epoch:29 step:22880 [D loss: 0.308995, acc.: 89.06%] [G loss: 2.877200]\n",
      "epoch:29 step:22881 [D loss: 0.235098, acc.: 89.06%] [G loss: 4.808425]\n",
      "epoch:29 step:22882 [D loss: 0.373300, acc.: 83.59%] [G loss: 4.984643]\n",
      "epoch:29 step:22883 [D loss: 0.257026, acc.: 92.19%] [G loss: 5.874283]\n",
      "epoch:29 step:22884 [D loss: 0.347377, acc.: 83.59%] [G loss: 3.844657]\n",
      "epoch:29 step:22885 [D loss: 0.350382, acc.: 85.16%] [G loss: 3.135464]\n",
      "epoch:29 step:22886 [D loss: 0.309044, acc.: 83.59%] [G loss: 3.406804]\n",
      "epoch:29 step:22887 [D loss: 0.327722, acc.: 82.81%] [G loss: 3.317074]\n",
      "epoch:29 step:22888 [D loss: 0.467892, acc.: 76.56%] [G loss: 2.145620]\n",
      "epoch:29 step:22889 [D loss: 0.457167, acc.: 77.34%] [G loss: 2.756893]\n",
      "epoch:29 step:22890 [D loss: 0.448160, acc.: 85.94%] [G loss: 2.928303]\n",
      "epoch:29 step:22891 [D loss: 0.405035, acc.: 85.16%] [G loss: 3.088374]\n",
      "epoch:29 step:22892 [D loss: 0.394569, acc.: 82.81%] [G loss: 2.748333]\n",
      "epoch:29 step:22893 [D loss: 0.362021, acc.: 85.94%] [G loss: 3.845631]\n",
      "epoch:29 step:22894 [D loss: 0.274816, acc.: 92.19%] [G loss: 3.056712]\n",
      "epoch:29 step:22895 [D loss: 0.400923, acc.: 83.59%] [G loss: 2.805193]\n",
      "epoch:29 step:22896 [D loss: 0.351882, acc.: 84.38%] [G loss: 2.709480]\n",
      "epoch:29 step:22897 [D loss: 0.344733, acc.: 84.38%] [G loss: 4.462317]\n",
      "epoch:29 step:22898 [D loss: 0.359827, acc.: 84.38%] [G loss: 3.173332]\n",
      "epoch:29 step:22899 [D loss: 0.374598, acc.: 81.25%] [G loss: 2.719146]\n",
      "epoch:29 step:22900 [D loss: 0.301482, acc.: 89.84%] [G loss: 2.853388]\n",
      "epoch:29 step:22901 [D loss: 0.403448, acc.: 82.03%] [G loss: 4.131262]\n",
      "epoch:29 step:22902 [D loss: 0.312373, acc.: 84.38%] [G loss: 3.069866]\n",
      "epoch:29 step:22903 [D loss: 0.310194, acc.: 85.94%] [G loss: 3.491413]\n",
      "epoch:29 step:22904 [D loss: 0.282782, acc.: 90.62%] [G loss: 2.863716]\n",
      "epoch:29 step:22905 [D loss: 0.305461, acc.: 85.16%] [G loss: 3.440573]\n",
      "epoch:29 step:22906 [D loss: 0.313529, acc.: 85.94%] [G loss: 3.168875]\n",
      "epoch:29 step:22907 [D loss: 0.379437, acc.: 82.03%] [G loss: 2.972204]\n",
      "epoch:29 step:22908 [D loss: 0.276503, acc.: 88.28%] [G loss: 2.957792]\n",
      "epoch:29 step:22909 [D loss: 0.308042, acc.: 85.94%] [G loss: 3.269140]\n",
      "epoch:29 step:22910 [D loss: 0.450920, acc.: 75.00%] [G loss: 2.529472]\n",
      "epoch:29 step:22911 [D loss: 0.404588, acc.: 78.12%] [G loss: 3.869463]\n",
      "epoch:29 step:22912 [D loss: 0.300924, acc.: 84.38%] [G loss: 3.024920]\n",
      "epoch:29 step:22913 [D loss: 0.314240, acc.: 87.50%] [G loss: 4.373420]\n",
      "epoch:29 step:22914 [D loss: 0.467576, acc.: 74.22%] [G loss: 4.097867]\n",
      "epoch:29 step:22915 [D loss: 0.376699, acc.: 84.38%] [G loss: 2.888579]\n",
      "epoch:29 step:22916 [D loss: 0.539604, acc.: 78.12%] [G loss: 6.758052]\n",
      "epoch:29 step:22917 [D loss: 0.744451, acc.: 75.00%] [G loss: 5.078231]\n",
      "epoch:29 step:22918 [D loss: 0.393986, acc.: 81.25%] [G loss: 3.574438]\n",
      "epoch:29 step:22919 [D loss: 0.448915, acc.: 78.12%] [G loss: 4.605063]\n",
      "epoch:29 step:22920 [D loss: 0.260448, acc.: 89.06%] [G loss: 4.050372]\n",
      "epoch:29 step:22921 [D loss: 0.361083, acc.: 80.47%] [G loss: 3.494635]\n",
      "epoch:29 step:22922 [D loss: 0.287788, acc.: 82.81%] [G loss: 4.774381]\n",
      "epoch:29 step:22923 [D loss: 0.364615, acc.: 83.59%] [G loss: 3.173139]\n",
      "epoch:29 step:22924 [D loss: 0.371908, acc.: 77.34%] [G loss: 3.675696]\n",
      "epoch:29 step:22925 [D loss: 0.401869, acc.: 81.25%] [G loss: 3.384466]\n",
      "epoch:29 step:22926 [D loss: 0.377064, acc.: 82.03%] [G loss: 2.548799]\n",
      "epoch:29 step:22927 [D loss: 0.440953, acc.: 75.00%] [G loss: 2.610943]\n",
      "epoch:29 step:22928 [D loss: 0.464125, acc.: 75.78%] [G loss: 3.370219]\n",
      "epoch:29 step:22929 [D loss: 0.327680, acc.: 82.81%] [G loss: 3.486187]\n",
      "epoch:29 step:22930 [D loss: 0.360217, acc.: 82.81%] [G loss: 3.412131]\n",
      "epoch:29 step:22931 [D loss: 0.278665, acc.: 85.94%] [G loss: 3.763255]\n",
      "epoch:29 step:22932 [D loss: 0.244855, acc.: 92.19%] [G loss: 4.393900]\n",
      "epoch:29 step:22933 [D loss: 0.330731, acc.: 84.38%] [G loss: 2.651209]\n",
      "epoch:29 step:22934 [D loss: 0.265550, acc.: 88.28%] [G loss: 2.771729]\n",
      "epoch:29 step:22935 [D loss: 0.438943, acc.: 78.91%] [G loss: 3.213437]\n",
      "epoch:29 step:22936 [D loss: 0.294853, acc.: 88.28%] [G loss: 2.672011]\n",
      "epoch:29 step:22937 [D loss: 0.345611, acc.: 80.47%] [G loss: 2.938878]\n",
      "epoch:29 step:22938 [D loss: 0.371096, acc.: 83.59%] [G loss: 2.521342]\n",
      "epoch:29 step:22939 [D loss: 0.302369, acc.: 84.38%] [G loss: 2.876896]\n",
      "epoch:29 step:22940 [D loss: 0.335071, acc.: 84.38%] [G loss: 3.398880]\n",
      "epoch:29 step:22941 [D loss: 0.371529, acc.: 86.72%] [G loss: 3.204913]\n",
      "epoch:29 step:22942 [D loss: 0.280954, acc.: 88.28%] [G loss: 3.746630]\n",
      "epoch:29 step:22943 [D loss: 0.279673, acc.: 87.50%] [G loss: 3.599805]\n",
      "epoch:29 step:22944 [D loss: 0.269556, acc.: 89.84%] [G loss: 3.456741]\n",
      "epoch:29 step:22945 [D loss: 0.310025, acc.: 88.28%] [G loss: 3.555704]\n",
      "epoch:29 step:22946 [D loss: 0.315253, acc.: 90.62%] [G loss: 3.638893]\n",
      "epoch:29 step:22947 [D loss: 0.243943, acc.: 91.41%] [G loss: 3.410780]\n",
      "epoch:29 step:22948 [D loss: 0.303878, acc.: 85.16%] [G loss: 3.112711]\n",
      "epoch:29 step:22949 [D loss: 0.353455, acc.: 82.03%] [G loss: 3.619146]\n",
      "epoch:29 step:22950 [D loss: 0.537338, acc.: 75.78%] [G loss: 6.089711]\n",
      "epoch:29 step:22951 [D loss: 0.725560, acc.: 69.53%] [G loss: 8.106869]\n",
      "epoch:29 step:22952 [D loss: 1.022361, acc.: 71.88%] [G loss: 5.080852]\n",
      "epoch:29 step:22953 [D loss: 0.844328, acc.: 73.44%] [G loss: 3.616267]\n",
      "epoch:29 step:22954 [D loss: 0.467467, acc.: 76.56%] [G loss: 4.278556]\n",
      "epoch:29 step:22955 [D loss: 0.299592, acc.: 87.50%] [G loss: 4.048578]\n",
      "epoch:29 step:22956 [D loss: 0.373748, acc.: 82.81%] [G loss: 3.487705]\n",
      "epoch:29 step:22957 [D loss: 0.333884, acc.: 85.94%] [G loss: 2.898673]\n",
      "epoch:29 step:22958 [D loss: 0.333664, acc.: 85.94%] [G loss: 3.088535]\n",
      "epoch:29 step:22959 [D loss: 0.237277, acc.: 92.97%] [G loss: 3.177776]\n",
      "epoch:29 step:22960 [D loss: 0.352651, acc.: 82.81%] [G loss: 3.306994]\n",
      "epoch:29 step:22961 [D loss: 0.314110, acc.: 85.16%] [G loss: 2.619345]\n",
      "epoch:29 step:22962 [D loss: 0.292690, acc.: 88.28%] [G loss: 3.026782]\n",
      "epoch:29 step:22963 [D loss: 0.260217, acc.: 91.41%] [G loss: 2.672523]\n",
      "epoch:29 step:22964 [D loss: 0.487794, acc.: 75.00%] [G loss: 4.414714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:22965 [D loss: 0.571113, acc.: 73.44%] [G loss: 4.931303]\n",
      "epoch:29 step:22966 [D loss: 0.810355, acc.: 75.78%] [G loss: 8.633711]\n",
      "epoch:29 step:22967 [D loss: 1.983169, acc.: 58.59%] [G loss: 1.709985]\n",
      "epoch:29 step:22968 [D loss: 0.512136, acc.: 81.25%] [G loss: 4.413799]\n",
      "epoch:29 step:22969 [D loss: 0.919522, acc.: 63.28%] [G loss: 3.579834]\n",
      "epoch:29 step:22970 [D loss: 0.400264, acc.: 85.16%] [G loss: 3.969177]\n",
      "epoch:29 step:22971 [D loss: 0.449991, acc.: 80.47%] [G loss: 3.158677]\n",
      "epoch:29 step:22972 [D loss: 0.328593, acc.: 85.94%] [G loss: 3.305625]\n",
      "epoch:29 step:22973 [D loss: 0.304961, acc.: 84.38%] [G loss: 2.647008]\n",
      "epoch:29 step:22974 [D loss: 0.448610, acc.: 78.12%] [G loss: 3.693723]\n",
      "epoch:29 step:22975 [D loss: 0.409651, acc.: 79.69%] [G loss: 3.258240]\n",
      "epoch:29 step:22976 [D loss: 0.349106, acc.: 85.16%] [G loss: 3.368983]\n",
      "epoch:29 step:22977 [D loss: 0.340848, acc.: 85.94%] [G loss: 2.364038]\n",
      "epoch:29 step:22978 [D loss: 0.372728, acc.: 85.16%] [G loss: 2.792880]\n",
      "epoch:29 step:22979 [D loss: 0.396892, acc.: 81.25%] [G loss: 2.407872]\n",
      "epoch:29 step:22980 [D loss: 0.469938, acc.: 77.34%] [G loss: 2.468641]\n",
      "epoch:29 step:22981 [D loss: 0.391898, acc.: 85.16%] [G loss: 2.923766]\n",
      "epoch:29 step:22982 [D loss: 0.299712, acc.: 85.94%] [G loss: 2.585652]\n",
      "epoch:29 step:22983 [D loss: 0.366355, acc.: 82.81%] [G loss: 2.763891]\n",
      "epoch:29 step:22984 [D loss: 0.321398, acc.: 89.06%] [G loss: 3.184178]\n",
      "epoch:29 step:22985 [D loss: 0.364800, acc.: 85.16%] [G loss: 3.109357]\n",
      "epoch:29 step:22986 [D loss: 0.306628, acc.: 89.84%] [G loss: 2.947107]\n",
      "epoch:29 step:22987 [D loss: 0.367725, acc.: 79.69%] [G loss: 3.092978]\n",
      "epoch:29 step:22988 [D loss: 0.375870, acc.: 87.50%] [G loss: 4.183162]\n",
      "epoch:29 step:22989 [D loss: 0.287347, acc.: 85.94%] [G loss: 4.193161]\n",
      "epoch:29 step:22990 [D loss: 0.325626, acc.: 84.38%] [G loss: 4.616422]\n",
      "epoch:29 step:22991 [D loss: 0.305748, acc.: 84.38%] [G loss: 3.427528]\n",
      "epoch:29 step:22992 [D loss: 0.316758, acc.: 83.59%] [G loss: 3.823359]\n",
      "epoch:29 step:22993 [D loss: 0.268002, acc.: 89.84%] [G loss: 3.199860]\n",
      "epoch:29 step:22994 [D loss: 0.272902, acc.: 88.28%] [G loss: 3.753283]\n",
      "epoch:29 step:22995 [D loss: 0.345987, acc.: 83.59%] [G loss: 2.124466]\n",
      "epoch:29 step:22996 [D loss: 0.308802, acc.: 86.72%] [G loss: 4.007344]\n",
      "epoch:29 step:22997 [D loss: 0.336327, acc.: 85.94%] [G loss: 3.145308]\n",
      "epoch:29 step:22998 [D loss: 0.340667, acc.: 83.59%] [G loss: 2.661106]\n",
      "epoch:29 step:22999 [D loss: 0.289757, acc.: 88.28%] [G loss: 2.544795]\n",
      "epoch:29 step:23000 [D loss: 0.327019, acc.: 83.59%] [G loss: 2.801540]\n",
      "epoch:29 step:23001 [D loss: 0.249117, acc.: 90.62%] [G loss: 2.803280]\n",
      "epoch:29 step:23002 [D loss: 0.373507, acc.: 80.47%] [G loss: 2.833690]\n",
      "epoch:29 step:23003 [D loss: 0.288033, acc.: 85.94%] [G loss: 2.918149]\n",
      "epoch:29 step:23004 [D loss: 0.256681, acc.: 90.62%] [G loss: 3.103766]\n",
      "epoch:29 step:23005 [D loss: 0.333875, acc.: 85.16%] [G loss: 2.647031]\n",
      "epoch:29 step:23006 [D loss: 0.359734, acc.: 79.69%] [G loss: 1.890644]\n",
      "epoch:29 step:23007 [D loss: 0.225957, acc.: 90.62%] [G loss: 2.487357]\n",
      "epoch:29 step:23008 [D loss: 0.226303, acc.: 90.62%] [G loss: 2.838220]\n",
      "epoch:29 step:23009 [D loss: 0.387916, acc.: 80.47%] [G loss: 2.420728]\n",
      "epoch:29 step:23010 [D loss: 0.355594, acc.: 82.03%] [G loss: 2.794529]\n",
      "epoch:29 step:23011 [D loss: 0.296144, acc.: 89.84%] [G loss: 3.444623]\n",
      "epoch:29 step:23012 [D loss: 0.270892, acc.: 89.84%] [G loss: 3.921214]\n",
      "epoch:29 step:23013 [D loss: 0.277639, acc.: 87.50%] [G loss: 2.883284]\n",
      "epoch:29 step:23014 [D loss: 0.284663, acc.: 86.72%] [G loss: 2.803448]\n",
      "epoch:29 step:23015 [D loss: 0.329784, acc.: 84.38%] [G loss: 2.693046]\n",
      "epoch:29 step:23016 [D loss: 0.333209, acc.: 85.16%] [G loss: 2.590932]\n",
      "epoch:29 step:23017 [D loss: 0.337251, acc.: 84.38%] [G loss: 2.464581]\n",
      "epoch:29 step:23018 [D loss: 0.268180, acc.: 89.84%] [G loss: 2.910429]\n",
      "epoch:29 step:23019 [D loss: 0.343597, acc.: 85.94%] [G loss: 3.263268]\n",
      "epoch:29 step:23020 [D loss: 0.389639, acc.: 81.25%] [G loss: 2.507726]\n",
      "epoch:29 step:23021 [D loss: 0.359468, acc.: 83.59%] [G loss: 4.706325]\n",
      "epoch:29 step:23022 [D loss: 0.394769, acc.: 89.06%] [G loss: 4.627452]\n",
      "epoch:29 step:23023 [D loss: 0.265186, acc.: 85.94%] [G loss: 6.927990]\n",
      "epoch:29 step:23024 [D loss: 0.362333, acc.: 79.69%] [G loss: 4.553885]\n",
      "epoch:29 step:23025 [D loss: 0.201199, acc.: 92.97%] [G loss: 4.666661]\n",
      "epoch:29 step:23026 [D loss: 0.325787, acc.: 85.94%] [G loss: 2.908762]\n",
      "epoch:29 step:23027 [D loss: 0.372589, acc.: 85.16%] [G loss: 2.808398]\n",
      "epoch:29 step:23028 [D loss: 0.289491, acc.: 85.94%] [G loss: 2.887285]\n",
      "epoch:29 step:23029 [D loss: 0.388142, acc.: 81.25%] [G loss: 1.941669]\n",
      "epoch:29 step:23030 [D loss: 0.377880, acc.: 85.16%] [G loss: 3.117330]\n",
      "epoch:29 step:23031 [D loss: 0.252575, acc.: 87.50%] [G loss: 3.232187]\n",
      "epoch:29 step:23032 [D loss: 0.263643, acc.: 90.62%] [G loss: 2.961735]\n",
      "epoch:29 step:23033 [D loss: 0.301388, acc.: 89.84%] [G loss: 2.435455]\n",
      "epoch:29 step:23034 [D loss: 0.297364, acc.: 85.94%] [G loss: 2.266340]\n",
      "epoch:29 step:23035 [D loss: 0.327883, acc.: 83.59%] [G loss: 3.601074]\n",
      "epoch:29 step:23036 [D loss: 0.394684, acc.: 81.25%] [G loss: 3.475601]\n",
      "epoch:29 step:23037 [D loss: 0.312226, acc.: 87.50%] [G loss: 3.259506]\n",
      "epoch:29 step:23038 [D loss: 0.259771, acc.: 90.62%] [G loss: 3.581777]\n",
      "epoch:29 step:23039 [D loss: 0.247144, acc.: 87.50%] [G loss: 4.129405]\n",
      "epoch:29 step:23040 [D loss: 0.332717, acc.: 85.16%] [G loss: 3.166539]\n",
      "epoch:29 step:23041 [D loss: 0.301828, acc.: 88.28%] [G loss: 4.087864]\n",
      "epoch:29 step:23042 [D loss: 0.264419, acc.: 88.28%] [G loss: 4.664423]\n",
      "epoch:29 step:23043 [D loss: 0.346810, acc.: 82.81%] [G loss: 3.050784]\n",
      "epoch:29 step:23044 [D loss: 0.346892, acc.: 86.72%] [G loss: 2.493907]\n",
      "epoch:29 step:23045 [D loss: 0.333949, acc.: 81.25%] [G loss: 2.695443]\n",
      "epoch:29 step:23046 [D loss: 0.298742, acc.: 85.16%] [G loss: 2.742532]\n",
      "epoch:29 step:23047 [D loss: 0.372593, acc.: 86.72%] [G loss: 2.895261]\n",
      "epoch:29 step:23048 [D loss: 0.418568, acc.: 80.47%] [G loss: 3.588615]\n",
      "epoch:29 step:23049 [D loss: 0.346010, acc.: 86.72%] [G loss: 3.088019]\n",
      "epoch:29 step:23050 [D loss: 0.294151, acc.: 86.72%] [G loss: 3.238112]\n",
      "epoch:29 step:23051 [D loss: 0.306682, acc.: 85.16%] [G loss: 3.783847]\n",
      "epoch:29 step:23052 [D loss: 0.331102, acc.: 85.16%] [G loss: 3.044964]\n",
      "epoch:29 step:23053 [D loss: 0.234799, acc.: 92.19%] [G loss: 3.933555]\n",
      "epoch:29 step:23054 [D loss: 0.315775, acc.: 85.94%] [G loss: 3.179490]\n",
      "epoch:29 step:23055 [D loss: 0.357340, acc.: 80.47%] [G loss: 3.053445]\n",
      "epoch:29 step:23056 [D loss: 0.347562, acc.: 82.03%] [G loss: 2.459244]\n",
      "epoch:29 step:23057 [D loss: 0.385800, acc.: 76.56%] [G loss: 3.442404]\n",
      "epoch:29 step:23058 [D loss: 0.291033, acc.: 87.50%] [G loss: 2.494837]\n",
      "epoch:29 step:23059 [D loss: 0.375139, acc.: 86.72%] [G loss: 4.096539]\n",
      "epoch:29 step:23060 [D loss: 0.261752, acc.: 89.06%] [G loss: 3.040445]\n",
      "epoch:29 step:23061 [D loss: 0.424450, acc.: 78.91%] [G loss: 3.663836]\n",
      "epoch:29 step:23062 [D loss: 0.247890, acc.: 88.28%] [G loss: 3.253434]\n",
      "epoch:29 step:23063 [D loss: 0.421381, acc.: 79.69%] [G loss: 3.420926]\n",
      "epoch:29 step:23064 [D loss: 0.292708, acc.: 86.72%] [G loss: 2.863252]\n",
      "epoch:29 step:23065 [D loss: 0.354480, acc.: 83.59%] [G loss: 2.642783]\n",
      "epoch:29 step:23066 [D loss: 0.268720, acc.: 87.50%] [G loss: 3.721453]\n",
      "epoch:29 step:23067 [D loss: 0.316959, acc.: 86.72%] [G loss: 3.007725]\n",
      "epoch:29 step:23068 [D loss: 0.284496, acc.: 89.06%] [G loss: 4.164533]\n",
      "epoch:29 step:23069 [D loss: 0.265936, acc.: 88.28%] [G loss: 3.757815]\n",
      "epoch:29 step:23070 [D loss: 0.282556, acc.: 91.41%] [G loss: 3.135286]\n",
      "epoch:29 step:23071 [D loss: 0.395488, acc.: 78.12%] [G loss: 2.252328]\n",
      "epoch:29 step:23072 [D loss: 0.266004, acc.: 87.50%] [G loss: 2.917477]\n",
      "epoch:29 step:23073 [D loss: 0.340094, acc.: 83.59%] [G loss: 2.761758]\n",
      "epoch:29 step:23074 [D loss: 0.358919, acc.: 87.50%] [G loss: 4.356043]\n",
      "epoch:29 step:23075 [D loss: 0.341788, acc.: 82.81%] [G loss: 2.123215]\n",
      "epoch:29 step:23076 [D loss: 0.245104, acc.: 87.50%] [G loss: 4.198283]\n",
      "epoch:29 step:23077 [D loss: 0.341319, acc.: 85.94%] [G loss: 2.843115]\n",
      "epoch:29 step:23078 [D loss: 0.337544, acc.: 85.16%] [G loss: 2.929575]\n",
      "epoch:29 step:23079 [D loss: 0.364287, acc.: 83.59%] [G loss: 3.170146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23080 [D loss: 0.331275, acc.: 87.50%] [G loss: 3.062321]\n",
      "epoch:29 step:23081 [D loss: 0.350368, acc.: 82.03%] [G loss: 3.432828]\n",
      "epoch:29 step:23082 [D loss: 0.189199, acc.: 92.97%] [G loss: 3.140357]\n",
      "epoch:29 step:23083 [D loss: 0.301068, acc.: 87.50%] [G loss: 3.472758]\n",
      "epoch:29 step:23084 [D loss: 0.339930, acc.: 85.16%] [G loss: 4.138162]\n",
      "epoch:29 step:23085 [D loss: 0.380039, acc.: 78.91%] [G loss: 4.037077]\n",
      "epoch:29 step:23086 [D loss: 0.358637, acc.: 83.59%] [G loss: 3.508180]\n",
      "epoch:29 step:23087 [D loss: 0.311318, acc.: 85.94%] [G loss: 3.265039]\n",
      "epoch:29 step:23088 [D loss: 0.303341, acc.: 87.50%] [G loss: 2.523767]\n",
      "epoch:29 step:23089 [D loss: 0.249548, acc.: 92.97%] [G loss: 3.615656]\n",
      "epoch:29 step:23090 [D loss: 0.335897, acc.: 86.72%] [G loss: 2.650214]\n",
      "epoch:29 step:23091 [D loss: 0.234577, acc.: 90.62%] [G loss: 3.417731]\n",
      "epoch:29 step:23092 [D loss: 0.380944, acc.: 83.59%] [G loss: 3.331566]\n",
      "epoch:29 step:23093 [D loss: 0.257121, acc.: 89.06%] [G loss: 2.621192]\n",
      "epoch:29 step:23094 [D loss: 0.292663, acc.: 86.72%] [G loss: 2.516488]\n",
      "epoch:29 step:23095 [D loss: 0.264390, acc.: 90.62%] [G loss: 2.490022]\n",
      "epoch:29 step:23096 [D loss: 0.342130, acc.: 84.38%] [G loss: 2.286199]\n",
      "epoch:29 step:23097 [D loss: 0.327038, acc.: 87.50%] [G loss: 3.097041]\n",
      "epoch:29 step:23098 [D loss: 0.359452, acc.: 82.81%] [G loss: 2.630245]\n",
      "epoch:29 step:23099 [D loss: 0.323990, acc.: 85.94%] [G loss: 3.481555]\n",
      "epoch:29 step:23100 [D loss: 0.316822, acc.: 85.16%] [G loss: 3.826932]\n",
      "epoch:29 step:23101 [D loss: 0.346050, acc.: 83.59%] [G loss: 3.546904]\n",
      "epoch:29 step:23102 [D loss: 0.273548, acc.: 89.84%] [G loss: 3.115511]\n",
      "epoch:29 step:23103 [D loss: 0.273545, acc.: 86.72%] [G loss: 5.325398]\n",
      "epoch:29 step:23104 [D loss: 0.475137, acc.: 76.56%] [G loss: 3.559001]\n",
      "epoch:29 step:23105 [D loss: 0.574655, acc.: 75.78%] [G loss: 6.527346]\n",
      "epoch:29 step:23106 [D loss: 1.551535, acc.: 59.38%] [G loss: 9.847895]\n",
      "epoch:29 step:23107 [D loss: 1.786526, acc.: 66.41%] [G loss: 4.134511]\n",
      "epoch:29 step:23108 [D loss: 0.502171, acc.: 84.38%] [G loss: 4.024721]\n",
      "epoch:29 step:23109 [D loss: 0.643917, acc.: 65.62%] [G loss: 3.702448]\n",
      "epoch:29 step:23110 [D loss: 0.383548, acc.: 85.16%] [G loss: 3.510794]\n",
      "epoch:29 step:23111 [D loss: 0.312921, acc.: 85.94%] [G loss: 4.922174]\n",
      "epoch:29 step:23112 [D loss: 0.473102, acc.: 78.91%] [G loss: 3.535853]\n",
      "epoch:29 step:23113 [D loss: 0.380699, acc.: 82.81%] [G loss: 4.174632]\n",
      "epoch:29 step:23114 [D loss: 0.336824, acc.: 82.81%] [G loss: 3.229342]\n",
      "epoch:29 step:23115 [D loss: 0.338949, acc.: 86.72%] [G loss: 3.027903]\n",
      "epoch:29 step:23116 [D loss: 0.358749, acc.: 81.25%] [G loss: 3.030628]\n",
      "epoch:29 step:23117 [D loss: 0.317630, acc.: 84.38%] [G loss: 2.218623]\n",
      "epoch:29 step:23118 [D loss: 0.312355, acc.: 86.72%] [G loss: 2.841753]\n",
      "epoch:29 step:23119 [D loss: 0.257855, acc.: 90.62%] [G loss: 3.281076]\n",
      "epoch:29 step:23120 [D loss: 0.342365, acc.: 85.94%] [G loss: 3.044294]\n",
      "epoch:29 step:23121 [D loss: 0.330480, acc.: 85.16%] [G loss: 2.983690]\n",
      "epoch:29 step:23122 [D loss: 0.359541, acc.: 82.03%] [G loss: 2.837479]\n",
      "epoch:29 step:23123 [D loss: 0.384163, acc.: 82.81%] [G loss: 2.464149]\n",
      "epoch:29 step:23124 [D loss: 0.333917, acc.: 86.72%] [G loss: 3.023299]\n",
      "epoch:29 step:23125 [D loss: 0.298373, acc.: 85.94%] [G loss: 2.802007]\n",
      "epoch:29 step:23126 [D loss: 0.235018, acc.: 89.06%] [G loss: 4.064281]\n",
      "epoch:29 step:23127 [D loss: 0.232798, acc.: 91.41%] [G loss: 2.723196]\n",
      "epoch:29 step:23128 [D loss: 0.295148, acc.: 85.16%] [G loss: 2.979841]\n",
      "epoch:29 step:23129 [D loss: 0.258285, acc.: 89.06%] [G loss: 3.517181]\n",
      "epoch:29 step:23130 [D loss: 0.279650, acc.: 86.72%] [G loss: 3.549974]\n",
      "epoch:29 step:23131 [D loss: 0.271847, acc.: 87.50%] [G loss: 4.575446]\n",
      "epoch:29 step:23132 [D loss: 0.300375, acc.: 81.25%] [G loss: 5.269327]\n",
      "epoch:29 step:23133 [D loss: 0.279551, acc.: 87.50%] [G loss: 3.257132]\n",
      "epoch:29 step:23134 [D loss: 0.228784, acc.: 89.06%] [G loss: 4.211338]\n",
      "epoch:29 step:23135 [D loss: 0.311079, acc.: 85.16%] [G loss: 3.249422]\n",
      "epoch:29 step:23136 [D loss: 0.246804, acc.: 88.28%] [G loss: 3.779796]\n",
      "epoch:29 step:23137 [D loss: 0.273132, acc.: 87.50%] [G loss: 3.053409]\n",
      "epoch:29 step:23138 [D loss: 0.282583, acc.: 89.84%] [G loss: 3.899736]\n",
      "epoch:29 step:23139 [D loss: 0.358279, acc.: 83.59%] [G loss: 2.813992]\n",
      "epoch:29 step:23140 [D loss: 0.292756, acc.: 88.28%] [G loss: 2.618487]\n",
      "epoch:29 step:23141 [D loss: 0.408021, acc.: 78.91%] [G loss: 2.857041]\n",
      "epoch:29 step:23142 [D loss: 0.318467, acc.: 85.94%] [G loss: 3.054071]\n",
      "epoch:29 step:23143 [D loss: 0.354648, acc.: 83.59%] [G loss: 3.169868]\n",
      "epoch:29 step:23144 [D loss: 0.281729, acc.: 90.62%] [G loss: 3.144853]\n",
      "epoch:29 step:23145 [D loss: 0.289388, acc.: 85.94%] [G loss: 3.745400]\n",
      "epoch:29 step:23146 [D loss: 0.301569, acc.: 87.50%] [G loss: 3.210984]\n",
      "epoch:29 step:23147 [D loss: 0.231506, acc.: 91.41%] [G loss: 2.533026]\n",
      "epoch:29 step:23148 [D loss: 0.286047, acc.: 86.72%] [G loss: 2.636569]\n",
      "epoch:29 step:23149 [D loss: 0.389630, acc.: 79.69%] [G loss: 3.210099]\n",
      "epoch:29 step:23150 [D loss: 0.381932, acc.: 80.47%] [G loss: 4.014838]\n",
      "epoch:29 step:23151 [D loss: 0.295086, acc.: 84.38%] [G loss: 3.557461]\n",
      "epoch:29 step:23152 [D loss: 0.300900, acc.: 85.94%] [G loss: 2.698390]\n",
      "epoch:29 step:23153 [D loss: 0.426671, acc.: 79.69%] [G loss: 2.449275]\n",
      "epoch:29 step:23154 [D loss: 0.366801, acc.: 83.59%] [G loss: 2.693571]\n",
      "epoch:29 step:23155 [D loss: 0.378608, acc.: 82.03%] [G loss: 3.147311]\n",
      "epoch:29 step:23156 [D loss: 0.397782, acc.: 80.47%] [G loss: 2.173379]\n",
      "epoch:29 step:23157 [D loss: 0.345680, acc.: 86.72%] [G loss: 2.943121]\n",
      "epoch:29 step:23158 [D loss: 0.436781, acc.: 78.12%] [G loss: 4.181722]\n",
      "epoch:29 step:23159 [D loss: 0.431980, acc.: 78.12%] [G loss: 2.846128]\n",
      "epoch:29 step:23160 [D loss: 0.209328, acc.: 93.75%] [G loss: 6.351594]\n",
      "epoch:29 step:23161 [D loss: 0.273234, acc.: 87.50%] [G loss: 4.175426]\n",
      "epoch:29 step:23162 [D loss: 0.312435, acc.: 85.94%] [G loss: 3.292748]\n",
      "epoch:29 step:23163 [D loss: 0.317544, acc.: 86.72%] [G loss: 2.659392]\n",
      "epoch:29 step:23164 [D loss: 0.365847, acc.: 82.81%] [G loss: 2.805683]\n",
      "epoch:29 step:23165 [D loss: 0.359471, acc.: 85.94%] [G loss: 3.022630]\n",
      "epoch:29 step:23166 [D loss: 0.316736, acc.: 85.94%] [G loss: 3.408412]\n",
      "epoch:29 step:23167 [D loss: 0.192783, acc.: 90.62%] [G loss: 4.076275]\n",
      "epoch:29 step:23168 [D loss: 0.365234, acc.: 83.59%] [G loss: 2.981803]\n",
      "epoch:29 step:23169 [D loss: 0.360448, acc.: 80.47%] [G loss: 2.587592]\n",
      "epoch:29 step:23170 [D loss: 0.328763, acc.: 84.38%] [G loss: 2.695653]\n",
      "epoch:29 step:23171 [D loss: 0.356843, acc.: 82.03%] [G loss: 3.338015]\n",
      "epoch:29 step:23172 [D loss: 0.455891, acc.: 72.66%] [G loss: 4.386003]\n",
      "epoch:29 step:23173 [D loss: 0.466160, acc.: 78.12%] [G loss: 3.771494]\n",
      "epoch:29 step:23174 [D loss: 0.309131, acc.: 85.94%] [G loss: 3.020377]\n",
      "epoch:29 step:23175 [D loss: 0.495427, acc.: 78.91%] [G loss: 4.382053]\n",
      "epoch:29 step:23176 [D loss: 0.236564, acc.: 91.41%] [G loss: 4.681394]\n",
      "epoch:29 step:23177 [D loss: 0.223841, acc.: 88.28%] [G loss: 2.540721]\n",
      "epoch:29 step:23178 [D loss: 0.310866, acc.: 85.16%] [G loss: 2.785007]\n",
      "epoch:29 step:23179 [D loss: 0.338126, acc.: 80.47%] [G loss: 4.677607]\n",
      "epoch:29 step:23180 [D loss: 0.195892, acc.: 90.62%] [G loss: 3.924147]\n",
      "epoch:29 step:23181 [D loss: 0.438686, acc.: 80.47%] [G loss: 2.978972]\n",
      "epoch:29 step:23182 [D loss: 0.291929, acc.: 89.84%] [G loss: 2.930502]\n",
      "epoch:29 step:23183 [D loss: 0.270313, acc.: 88.28%] [G loss: 3.166554]\n",
      "epoch:29 step:23184 [D loss: 0.345116, acc.: 84.38%] [G loss: 2.953450]\n",
      "epoch:29 step:23185 [D loss: 0.366291, acc.: 82.81%] [G loss: 2.327369]\n",
      "epoch:29 step:23186 [D loss: 0.435700, acc.: 77.34%] [G loss: 2.208689]\n",
      "epoch:29 step:23187 [D loss: 0.444363, acc.: 76.56%] [G loss: 2.699512]\n",
      "epoch:29 step:23188 [D loss: 0.323141, acc.: 85.16%] [G loss: 2.536986]\n",
      "epoch:29 step:23189 [D loss: 0.390234, acc.: 82.03%] [G loss: 2.989743]\n",
      "epoch:29 step:23190 [D loss: 0.410811, acc.: 78.12%] [G loss: 4.851225]\n",
      "epoch:29 step:23191 [D loss: 0.519131, acc.: 77.34%] [G loss: 5.023074]\n",
      "epoch:29 step:23192 [D loss: 0.321971, acc.: 82.03%] [G loss: 3.718793]\n",
      "epoch:29 step:23193 [D loss: 0.380300, acc.: 84.38%] [G loss: 5.016345]\n",
      "epoch:29 step:23194 [D loss: 0.267980, acc.: 89.84%] [G loss: 2.773378]\n",
      "epoch:29 step:23195 [D loss: 0.208843, acc.: 89.84%] [G loss: 4.586316]\n",
      "epoch:29 step:23196 [D loss: 0.325068, acc.: 85.94%] [G loss: 4.190794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23197 [D loss: 0.227704, acc.: 88.28%] [G loss: 3.851924]\n",
      "epoch:29 step:23198 [D loss: 0.351923, acc.: 85.94%] [G loss: 3.823957]\n",
      "epoch:29 step:23199 [D loss: 0.295708, acc.: 89.84%] [G loss: 4.085197]\n",
      "epoch:29 step:23200 [D loss: 0.253965, acc.: 89.06%] [G loss: 4.395976]\n",
      "epoch:29 step:23201 [D loss: 0.330786, acc.: 86.72%] [G loss: 4.797652]\n",
      "epoch:29 step:23202 [D loss: 0.470809, acc.: 79.69%] [G loss: 5.266941]\n",
      "epoch:29 step:23203 [D loss: 0.352343, acc.: 83.59%] [G loss: 4.452095]\n",
      "epoch:29 step:23204 [D loss: 0.301818, acc.: 86.72%] [G loss: 4.245396]\n",
      "epoch:29 step:23205 [D loss: 0.276584, acc.: 86.72%] [G loss: 3.289883]\n",
      "epoch:29 step:23206 [D loss: 0.242487, acc.: 89.06%] [G loss: 4.227518]\n",
      "epoch:29 step:23207 [D loss: 0.323078, acc.: 79.69%] [G loss: 2.705227]\n",
      "epoch:29 step:23208 [D loss: 0.370795, acc.: 84.38%] [G loss: 2.639843]\n",
      "epoch:29 step:23209 [D loss: 0.347872, acc.: 82.03%] [G loss: 2.451348]\n",
      "epoch:29 step:23210 [D loss: 0.314148, acc.: 87.50%] [G loss: 2.719953]\n",
      "epoch:29 step:23211 [D loss: 0.382481, acc.: 81.25%] [G loss: 2.610396]\n",
      "epoch:29 step:23212 [D loss: 0.380310, acc.: 80.47%] [G loss: 3.059010]\n",
      "epoch:29 step:23213 [D loss: 0.328279, acc.: 82.03%] [G loss: 2.831999]\n",
      "epoch:29 step:23214 [D loss: 0.280608, acc.: 90.62%] [G loss: 3.076829]\n",
      "epoch:29 step:23215 [D loss: 0.283337, acc.: 86.72%] [G loss: 2.940490]\n",
      "epoch:29 step:23216 [D loss: 0.338414, acc.: 82.81%] [G loss: 3.370705]\n",
      "epoch:29 step:23217 [D loss: 0.340298, acc.: 83.59%] [G loss: 3.041113]\n",
      "epoch:29 step:23218 [D loss: 0.308290, acc.: 88.28%] [G loss: 4.581228]\n",
      "epoch:29 step:23219 [D loss: 0.241045, acc.: 86.72%] [G loss: 4.924617]\n",
      "epoch:29 step:23220 [D loss: 0.233139, acc.: 91.41%] [G loss: 3.233722]\n",
      "epoch:29 step:23221 [D loss: 0.308107, acc.: 84.38%] [G loss: 2.320289]\n",
      "epoch:29 step:23222 [D loss: 0.284232, acc.: 89.84%] [G loss: 2.683996]\n",
      "epoch:29 step:23223 [D loss: 0.339210, acc.: 85.16%] [G loss: 3.122264]\n",
      "epoch:29 step:23224 [D loss: 0.439888, acc.: 81.25%] [G loss: 3.011065]\n",
      "epoch:29 step:23225 [D loss: 0.339781, acc.: 81.25%] [G loss: 3.752854]\n",
      "epoch:29 step:23226 [D loss: 0.284823, acc.: 86.72%] [G loss: 4.463716]\n",
      "epoch:29 step:23227 [D loss: 0.313237, acc.: 85.94%] [G loss: 3.716446]\n",
      "epoch:29 step:23228 [D loss: 0.368982, acc.: 84.38%] [G loss: 3.219125]\n",
      "epoch:29 step:23229 [D loss: 0.267567, acc.: 89.84%] [G loss: 2.928803]\n",
      "epoch:29 step:23230 [D loss: 0.277868, acc.: 86.72%] [G loss: 3.506720]\n",
      "epoch:29 step:23231 [D loss: 0.325293, acc.: 87.50%] [G loss: 3.384504]\n",
      "epoch:29 step:23232 [D loss: 0.405498, acc.: 80.47%] [G loss: 4.981251]\n",
      "epoch:29 step:23233 [D loss: 0.408597, acc.: 78.91%] [G loss: 2.405320]\n",
      "epoch:29 step:23234 [D loss: 0.420010, acc.: 82.81%] [G loss: 3.183593]\n",
      "epoch:29 step:23235 [D loss: 0.256440, acc.: 88.28%] [G loss: 3.659000]\n",
      "epoch:29 step:23236 [D loss: 0.389524, acc.: 82.03%] [G loss: 4.838050]\n",
      "epoch:29 step:23237 [D loss: 0.258059, acc.: 90.62%] [G loss: 5.132756]\n",
      "epoch:29 step:23238 [D loss: 0.362053, acc.: 86.72%] [G loss: 3.105059]\n",
      "epoch:29 step:23239 [D loss: 0.396961, acc.: 79.69%] [G loss: 2.336209]\n",
      "epoch:29 step:23240 [D loss: 0.413909, acc.: 81.25%] [G loss: 2.934092]\n",
      "epoch:29 step:23241 [D loss: 0.351673, acc.: 84.38%] [G loss: 2.547571]\n",
      "epoch:29 step:23242 [D loss: 0.408985, acc.: 84.38%] [G loss: 2.937062]\n",
      "epoch:29 step:23243 [D loss: 0.312326, acc.: 86.72%] [G loss: 3.666301]\n",
      "epoch:29 step:23244 [D loss: 0.493291, acc.: 80.47%] [G loss: 3.773246]\n",
      "epoch:29 step:23245 [D loss: 0.294671, acc.: 84.38%] [G loss: 5.569834]\n",
      "epoch:29 step:23246 [D loss: 0.301212, acc.: 84.38%] [G loss: 3.588319]\n",
      "epoch:29 step:23247 [D loss: 0.310740, acc.: 82.81%] [G loss: 3.314786]\n",
      "epoch:29 step:23248 [D loss: 0.306386, acc.: 85.16%] [G loss: 3.293726]\n",
      "epoch:29 step:23249 [D loss: 0.359678, acc.: 82.03%] [G loss: 2.423464]\n",
      "epoch:29 step:23250 [D loss: 0.361669, acc.: 82.03%] [G loss: 2.596127]\n",
      "epoch:29 step:23251 [D loss: 0.335210, acc.: 86.72%] [G loss: 2.985705]\n",
      "epoch:29 step:23252 [D loss: 0.338491, acc.: 84.38%] [G loss: 3.276574]\n",
      "epoch:29 step:23253 [D loss: 0.322307, acc.: 85.94%] [G loss: 2.829736]\n",
      "epoch:29 step:23254 [D loss: 0.413661, acc.: 79.69%] [G loss: 2.676305]\n",
      "epoch:29 step:23255 [D loss: 0.393994, acc.: 81.25%] [G loss: 2.668495]\n",
      "epoch:29 step:23256 [D loss: 0.269163, acc.: 90.62%] [G loss: 3.180262]\n",
      "epoch:29 step:23257 [D loss: 0.362993, acc.: 81.25%] [G loss: 3.745984]\n",
      "epoch:29 step:23258 [D loss: 0.333219, acc.: 84.38%] [G loss: 3.216377]\n",
      "epoch:29 step:23259 [D loss: 0.370458, acc.: 81.25%] [G loss: 2.943952]\n",
      "epoch:29 step:23260 [D loss: 0.406808, acc.: 82.03%] [G loss: 2.840656]\n",
      "epoch:29 step:23261 [D loss: 0.340318, acc.: 86.72%] [G loss: 2.856009]\n",
      "epoch:29 step:23262 [D loss: 0.443862, acc.: 80.47%] [G loss: 2.725300]\n",
      "epoch:29 step:23263 [D loss: 0.255363, acc.: 86.72%] [G loss: 2.935328]\n",
      "epoch:29 step:23264 [D loss: 0.358135, acc.: 83.59%] [G loss: 3.579119]\n",
      "epoch:29 step:23265 [D loss: 0.310506, acc.: 88.28%] [G loss: 2.502285]\n",
      "epoch:29 step:23266 [D loss: 0.444932, acc.: 77.34%] [G loss: 2.348075]\n",
      "epoch:29 step:23267 [D loss: 0.353913, acc.: 83.59%] [G loss: 2.764043]\n",
      "epoch:29 step:23268 [D loss: 0.358828, acc.: 85.16%] [G loss: 2.543996]\n",
      "epoch:29 step:23269 [D loss: 0.323131, acc.: 85.94%] [G loss: 2.627691]\n",
      "epoch:29 step:23270 [D loss: 0.321788, acc.: 83.59%] [G loss: 2.882899]\n",
      "epoch:29 step:23271 [D loss: 0.268791, acc.: 89.84%] [G loss: 2.695502]\n",
      "epoch:29 step:23272 [D loss: 0.444542, acc.: 79.69%] [G loss: 2.894033]\n",
      "epoch:29 step:23273 [D loss: 0.289707, acc.: 85.94%] [G loss: 2.403846]\n",
      "epoch:29 step:23274 [D loss: 0.391116, acc.: 81.25%] [G loss: 2.280075]\n",
      "epoch:29 step:23275 [D loss: 0.321582, acc.: 86.72%] [G loss: 2.547326]\n",
      "epoch:29 step:23276 [D loss: 0.322516, acc.: 84.38%] [G loss: 3.251874]\n",
      "epoch:29 step:23277 [D loss: 0.430318, acc.: 80.47%] [G loss: 3.519410]\n",
      "epoch:29 step:23278 [D loss: 0.360210, acc.: 79.69%] [G loss: 3.859919]\n",
      "epoch:29 step:23279 [D loss: 0.308884, acc.: 82.03%] [G loss: 4.463857]\n",
      "epoch:29 step:23280 [D loss: 0.377424, acc.: 82.03%] [G loss: 6.339137]\n",
      "epoch:29 step:23281 [D loss: 0.388139, acc.: 83.59%] [G loss: 9.592827]\n",
      "epoch:29 step:23282 [D loss: 0.425233, acc.: 81.25%] [G loss: 9.372198]\n",
      "epoch:29 step:23283 [D loss: 0.358608, acc.: 84.38%] [G loss: 6.717591]\n",
      "epoch:29 step:23284 [D loss: 0.327328, acc.: 86.72%] [G loss: 6.693967]\n",
      "epoch:29 step:23285 [D loss: 0.251763, acc.: 84.38%] [G loss: 5.038257]\n",
      "epoch:29 step:23286 [D loss: 0.273543, acc.: 88.28%] [G loss: 4.487999]\n",
      "epoch:29 step:23287 [D loss: 0.260097, acc.: 89.06%] [G loss: 4.132415]\n",
      "epoch:29 step:23288 [D loss: 0.410913, acc.: 85.16%] [G loss: 2.925655]\n",
      "epoch:29 step:23289 [D loss: 0.378334, acc.: 79.69%] [G loss: 2.631054]\n",
      "epoch:29 step:23290 [D loss: 0.277782, acc.: 88.28%] [G loss: 3.467741]\n",
      "epoch:29 step:23291 [D loss: 0.334939, acc.: 86.72%] [G loss: 3.148201]\n",
      "epoch:29 step:23292 [D loss: 0.288215, acc.: 88.28%] [G loss: 3.194470]\n",
      "epoch:29 step:23293 [D loss: 0.279910, acc.: 88.28%] [G loss: 4.500377]\n",
      "epoch:29 step:23294 [D loss: 0.261531, acc.: 86.72%] [G loss: 3.066024]\n",
      "epoch:29 step:23295 [D loss: 0.372411, acc.: 81.25%] [G loss: 3.319052]\n",
      "epoch:29 step:23296 [D loss: 0.294859, acc.: 87.50%] [G loss: 3.001124]\n",
      "epoch:29 step:23297 [D loss: 0.316744, acc.: 87.50%] [G loss: 2.684171]\n",
      "epoch:29 step:23298 [D loss: 0.331930, acc.: 86.72%] [G loss: 2.752429]\n",
      "epoch:29 step:23299 [D loss: 0.315060, acc.: 84.38%] [G loss: 3.130888]\n",
      "epoch:29 step:23300 [D loss: 0.322533, acc.: 85.16%] [G loss: 3.443420]\n",
      "epoch:29 step:23301 [D loss: 0.372891, acc.: 82.81%] [G loss: 4.514439]\n",
      "epoch:29 step:23302 [D loss: 0.340275, acc.: 85.16%] [G loss: 3.933587]\n",
      "epoch:29 step:23303 [D loss: 0.328838, acc.: 81.25%] [G loss: 3.618869]\n",
      "epoch:29 step:23304 [D loss: 0.287329, acc.: 85.16%] [G loss: 4.144686]\n",
      "epoch:29 step:23305 [D loss: 0.312367, acc.: 84.38%] [G loss: 3.701625]\n",
      "epoch:29 step:23306 [D loss: 0.438314, acc.: 78.91%] [G loss: 2.816174]\n",
      "epoch:29 step:23307 [D loss: 0.368948, acc.: 83.59%] [G loss: 2.776082]\n",
      "epoch:29 step:23308 [D loss: 0.317162, acc.: 83.59%] [G loss: 3.773481]\n",
      "epoch:29 step:23309 [D loss: 0.450623, acc.: 75.78%] [G loss: 3.254516]\n",
      "epoch:29 step:23310 [D loss: 0.449670, acc.: 78.12%] [G loss: 4.835803]\n",
      "epoch:29 step:23311 [D loss: 0.373525, acc.: 82.03%] [G loss: 3.534341]\n",
      "epoch:29 step:23312 [D loss: 0.363299, acc.: 84.38%] [G loss: 4.251880]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23313 [D loss: 0.307829, acc.: 86.72%] [G loss: 3.297253]\n",
      "epoch:29 step:23314 [D loss: 0.291933, acc.: 89.06%] [G loss: 3.470502]\n",
      "epoch:29 step:23315 [D loss: 0.459790, acc.: 79.69%] [G loss: 3.611552]\n",
      "epoch:29 step:23316 [D loss: 0.328278, acc.: 86.72%] [G loss: 3.493950]\n",
      "epoch:29 step:23317 [D loss: 0.295404, acc.: 87.50%] [G loss: 3.094101]\n",
      "epoch:29 step:23318 [D loss: 0.377448, acc.: 78.12%] [G loss: 3.219210]\n",
      "epoch:29 step:23319 [D loss: 0.298351, acc.: 85.16%] [G loss: 3.944696]\n",
      "epoch:29 step:23320 [D loss: 0.364158, acc.: 78.91%] [G loss: 2.897343]\n",
      "epoch:29 step:23321 [D loss: 0.332084, acc.: 83.59%] [G loss: 3.590564]\n",
      "epoch:29 step:23322 [D loss: 0.324976, acc.: 85.16%] [G loss: 2.601213]\n",
      "epoch:29 step:23323 [D loss: 0.354452, acc.: 80.47%] [G loss: 2.828848]\n",
      "epoch:29 step:23324 [D loss: 0.348090, acc.: 81.25%] [G loss: 2.741425]\n",
      "epoch:29 step:23325 [D loss: 0.356254, acc.: 85.94%] [G loss: 3.429638]\n",
      "epoch:29 step:23326 [D loss: 0.465842, acc.: 78.91%] [G loss: 3.533240]\n",
      "epoch:29 step:23327 [D loss: 0.508237, acc.: 80.47%] [G loss: 6.194289]\n",
      "epoch:29 step:23328 [D loss: 0.598816, acc.: 76.56%] [G loss: 4.228646]\n",
      "epoch:29 step:23329 [D loss: 0.700680, acc.: 67.19%] [G loss: 3.923552]\n",
      "epoch:29 step:23330 [D loss: 0.888862, acc.: 67.19%] [G loss: 8.954157]\n",
      "epoch:29 step:23331 [D loss: 1.314042, acc.: 67.19%] [G loss: 4.936300]\n",
      "epoch:29 step:23332 [D loss: 0.608062, acc.: 72.66%] [G loss: 4.876288]\n",
      "epoch:29 step:23333 [D loss: 0.385507, acc.: 81.25%] [G loss: 2.944938]\n",
      "epoch:29 step:23334 [D loss: 0.398359, acc.: 84.38%] [G loss: 4.082559]\n",
      "epoch:29 step:23335 [D loss: 0.357490, acc.: 85.16%] [G loss: 3.835654]\n",
      "epoch:29 step:23336 [D loss: 0.349409, acc.: 85.16%] [G loss: 2.893144]\n",
      "epoch:29 step:23337 [D loss: 0.240607, acc.: 91.41%] [G loss: 2.696397]\n",
      "epoch:29 step:23338 [D loss: 0.357854, acc.: 86.72%] [G loss: 2.568136]\n",
      "epoch:29 step:23339 [D loss: 0.303264, acc.: 85.16%] [G loss: 2.597310]\n",
      "epoch:29 step:23340 [D loss: 0.207917, acc.: 92.97%] [G loss: 3.514516]\n",
      "epoch:29 step:23341 [D loss: 0.359765, acc.: 83.59%] [G loss: 2.601417]\n",
      "epoch:29 step:23342 [D loss: 0.369238, acc.: 78.91%] [G loss: 3.061518]\n",
      "epoch:29 step:23343 [D loss: 0.267464, acc.: 87.50%] [G loss: 3.335186]\n",
      "epoch:29 step:23344 [D loss: 0.310321, acc.: 87.50%] [G loss: 3.188044]\n",
      "epoch:29 step:23345 [D loss: 0.333922, acc.: 85.94%] [G loss: 3.997068]\n",
      "epoch:29 step:23346 [D loss: 0.263731, acc.: 86.72%] [G loss: 2.806846]\n",
      "epoch:29 step:23347 [D loss: 0.264605, acc.: 88.28%] [G loss: 2.880419]\n",
      "epoch:29 step:23348 [D loss: 0.288924, acc.: 88.28%] [G loss: 2.985963]\n",
      "epoch:29 step:23349 [D loss: 0.296580, acc.: 87.50%] [G loss: 4.128335]\n",
      "epoch:29 step:23350 [D loss: 0.280027, acc.: 88.28%] [G loss: 2.484344]\n",
      "epoch:29 step:23351 [D loss: 0.241345, acc.: 93.75%] [G loss: 3.076315]\n",
      "epoch:29 step:23352 [D loss: 0.464550, acc.: 75.78%] [G loss: 3.281257]\n",
      "epoch:29 step:23353 [D loss: 0.283697, acc.: 85.94%] [G loss: 2.814752]\n",
      "epoch:29 step:23354 [D loss: 0.404290, acc.: 83.59%] [G loss: 3.720109]\n",
      "epoch:29 step:23355 [D loss: 0.439980, acc.: 81.25%] [G loss: 2.607077]\n",
      "epoch:29 step:23356 [D loss: 0.433544, acc.: 78.12%] [G loss: 3.020179]\n",
      "epoch:29 step:23357 [D loss: 0.302039, acc.: 87.50%] [G loss: 2.907180]\n",
      "epoch:29 step:23358 [D loss: 0.349278, acc.: 85.16%] [G loss: 3.658166]\n",
      "epoch:29 step:23359 [D loss: 0.325729, acc.: 84.38%] [G loss: 2.920198]\n",
      "epoch:29 step:23360 [D loss: 0.330021, acc.: 85.16%] [G loss: 2.728496]\n",
      "epoch:29 step:23361 [D loss: 0.342825, acc.: 86.72%] [G loss: 2.647477]\n",
      "epoch:29 step:23362 [D loss: 0.340755, acc.: 85.16%] [G loss: 2.608060]\n",
      "epoch:29 step:23363 [D loss: 0.306675, acc.: 87.50%] [G loss: 3.228213]\n",
      "epoch:29 step:23364 [D loss: 0.323138, acc.: 86.72%] [G loss: 3.328319]\n",
      "epoch:29 step:23365 [D loss: 0.276225, acc.: 87.50%] [G loss: 2.987437]\n",
      "epoch:29 step:23366 [D loss: 0.285254, acc.: 89.06%] [G loss: 3.792356]\n",
      "epoch:29 step:23367 [D loss: 0.329893, acc.: 83.59%] [G loss: 2.520316]\n",
      "epoch:29 step:23368 [D loss: 0.323431, acc.: 85.94%] [G loss: 2.914370]\n",
      "epoch:29 step:23369 [D loss: 0.312696, acc.: 86.72%] [G loss: 2.334221]\n",
      "epoch:29 step:23370 [D loss: 0.390472, acc.: 78.91%] [G loss: 2.518731]\n",
      "epoch:29 step:23371 [D loss: 0.330615, acc.: 87.50%] [G loss: 3.260468]\n",
      "epoch:29 step:23372 [D loss: 0.460972, acc.: 78.91%] [G loss: 2.825732]\n",
      "epoch:29 step:23373 [D loss: 0.284386, acc.: 85.94%] [G loss: 3.345085]\n",
      "epoch:29 step:23374 [D loss: 0.290711, acc.: 88.28%] [G loss: 3.164813]\n",
      "epoch:29 step:23375 [D loss: 0.338524, acc.: 85.16%] [G loss: 2.749010]\n",
      "epoch:29 step:23376 [D loss: 0.372785, acc.: 82.81%] [G loss: 3.067424]\n",
      "epoch:29 step:23377 [D loss: 0.324936, acc.: 82.81%] [G loss: 2.808373]\n",
      "epoch:29 step:23378 [D loss: 0.305697, acc.: 84.38%] [G loss: 2.812969]\n",
      "epoch:29 step:23379 [D loss: 0.286614, acc.: 87.50%] [G loss: 2.784592]\n",
      "epoch:29 step:23380 [D loss: 0.359359, acc.: 85.16%] [G loss: 2.985502]\n",
      "epoch:29 step:23381 [D loss: 0.361477, acc.: 83.59%] [G loss: 2.460813]\n",
      "epoch:29 step:23382 [D loss: 0.431463, acc.: 82.03%] [G loss: 2.891906]\n",
      "epoch:29 step:23383 [D loss: 0.321656, acc.: 85.94%] [G loss: 2.312355]\n",
      "epoch:29 step:23384 [D loss: 0.370835, acc.: 83.59%] [G loss: 2.675320]\n",
      "epoch:29 step:23385 [D loss: 0.343509, acc.: 84.38%] [G loss: 2.939217]\n",
      "epoch:29 step:23386 [D loss: 0.355422, acc.: 85.94%] [G loss: 3.030077]\n",
      "epoch:29 step:23387 [D loss: 0.340750, acc.: 85.16%] [G loss: 2.808645]\n",
      "epoch:29 step:23388 [D loss: 0.388178, acc.: 78.91%] [G loss: 2.414130]\n",
      "epoch:29 step:23389 [D loss: 0.349895, acc.: 82.81%] [G loss: 3.242953]\n",
      "epoch:29 step:23390 [D loss: 0.386164, acc.: 80.47%] [G loss: 2.960670]\n",
      "epoch:29 step:23391 [D loss: 0.378405, acc.: 82.81%] [G loss: 3.423758]\n",
      "epoch:29 step:23392 [D loss: 0.293997, acc.: 87.50%] [G loss: 3.254592]\n",
      "epoch:29 step:23393 [D loss: 0.318463, acc.: 86.72%] [G loss: 3.810521]\n",
      "epoch:29 step:23394 [D loss: 0.352518, acc.: 81.25%] [G loss: 2.618636]\n",
      "epoch:29 step:23395 [D loss: 0.276083, acc.: 89.84%] [G loss: 3.226059]\n",
      "epoch:29 step:23396 [D loss: 0.430714, acc.: 79.69%] [G loss: 3.888266]\n",
      "epoch:29 step:23397 [D loss: 0.311509, acc.: 85.94%] [G loss: 2.803701]\n",
      "epoch:29 step:23398 [D loss: 0.291292, acc.: 87.50%] [G loss: 3.992176]\n",
      "epoch:29 step:23399 [D loss: 0.271841, acc.: 87.50%] [G loss: 2.813246]\n",
      "epoch:29 step:23400 [D loss: 0.289917, acc.: 85.94%] [G loss: 2.950871]\n",
      "epoch:29 step:23401 [D loss: 0.400788, acc.: 82.03%] [G loss: 3.022642]\n",
      "epoch:29 step:23402 [D loss: 0.234222, acc.: 93.75%] [G loss: 2.913306]\n",
      "epoch:29 step:23403 [D loss: 0.422287, acc.: 78.91%] [G loss: 2.699789]\n",
      "epoch:29 step:23404 [D loss: 0.272016, acc.: 89.06%] [G loss: 3.149745]\n",
      "epoch:29 step:23405 [D loss: 0.441358, acc.: 78.12%] [G loss: 2.300901]\n",
      "epoch:29 step:23406 [D loss: 0.276851, acc.: 88.28%] [G loss: 2.883861]\n",
      "epoch:29 step:23407 [D loss: 0.371634, acc.: 84.38%] [G loss: 2.427356]\n",
      "epoch:29 step:23408 [D loss: 0.360580, acc.: 82.03%] [G loss: 2.828643]\n",
      "epoch:29 step:23409 [D loss: 0.240021, acc.: 91.41%] [G loss: 3.062260]\n",
      "epoch:29 step:23410 [D loss: 0.329415, acc.: 85.16%] [G loss: 3.075184]\n",
      "epoch:29 step:23411 [D loss: 0.312348, acc.: 84.38%] [G loss: 2.483835]\n",
      "epoch:29 step:23412 [D loss: 0.285802, acc.: 85.16%] [G loss: 2.371929]\n",
      "epoch:29 step:23413 [D loss: 0.333880, acc.: 85.16%] [G loss: 3.143244]\n",
      "epoch:29 step:23414 [D loss: 0.415413, acc.: 81.25%] [G loss: 3.381396]\n",
      "epoch:29 step:23415 [D loss: 0.339194, acc.: 84.38%] [G loss: 2.791772]\n",
      "epoch:29 step:23416 [D loss: 0.344812, acc.: 85.16%] [G loss: 3.028827]\n",
      "epoch:29 step:23417 [D loss: 0.363453, acc.: 86.72%] [G loss: 3.056545]\n",
      "epoch:29 step:23418 [D loss: 0.257781, acc.: 90.62%] [G loss: 3.158792]\n",
      "epoch:29 step:23419 [D loss: 0.220995, acc.: 92.97%] [G loss: 2.763190]\n",
      "epoch:29 step:23420 [D loss: 0.543872, acc.: 74.22%] [G loss: 2.799429]\n",
      "epoch:29 step:23421 [D loss: 0.282989, acc.: 87.50%] [G loss: 2.367640]\n",
      "epoch:29 step:23422 [D loss: 0.361018, acc.: 80.47%] [G loss: 2.666911]\n",
      "epoch:29 step:23423 [D loss: 0.381122, acc.: 87.50%] [G loss: 2.523121]\n",
      "epoch:29 step:23424 [D loss: 0.336898, acc.: 85.94%] [G loss: 2.615796]\n",
      "epoch:29 step:23425 [D loss: 0.355911, acc.: 84.38%] [G loss: 4.636586]\n",
      "epoch:29 step:23426 [D loss: 0.504210, acc.: 75.78%] [G loss: 3.209623]\n",
      "epoch:29 step:23427 [D loss: 0.464993, acc.: 78.91%] [G loss: 2.386037]\n",
      "epoch:29 step:23428 [D loss: 0.400284, acc.: 83.59%] [G loss: 3.958270]\n",
      "epoch:29 step:23429 [D loss: 0.413548, acc.: 82.03%] [G loss: 2.970638]\n",
      "epoch:29 step:23430 [D loss: 0.255891, acc.: 87.50%] [G loss: 3.554790]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23431 [D loss: 0.316441, acc.: 84.38%] [G loss: 3.411936]\n",
      "epoch:30 step:23432 [D loss: 0.390471, acc.: 85.94%] [G loss: 3.483715]\n",
      "epoch:30 step:23433 [D loss: 0.342090, acc.: 81.25%] [G loss: 3.665294]\n",
      "epoch:30 step:23434 [D loss: 0.270230, acc.: 89.06%] [G loss: 4.095587]\n",
      "epoch:30 step:23435 [D loss: 0.280948, acc.: 87.50%] [G loss: 3.902357]\n",
      "epoch:30 step:23436 [D loss: 0.362284, acc.: 84.38%] [G loss: 2.940610]\n",
      "epoch:30 step:23437 [D loss: 0.278067, acc.: 88.28%] [G loss: 4.040778]\n",
      "epoch:30 step:23438 [D loss: 0.263521, acc.: 88.28%] [G loss: 3.117620]\n",
      "epoch:30 step:23439 [D loss: 0.334283, acc.: 82.81%] [G loss: 3.189376]\n",
      "epoch:30 step:23440 [D loss: 0.352413, acc.: 85.16%] [G loss: 2.549199]\n",
      "epoch:30 step:23441 [D loss: 0.360149, acc.: 86.72%] [G loss: 3.359344]\n",
      "epoch:30 step:23442 [D loss: 0.304499, acc.: 86.72%] [G loss: 3.829674]\n",
      "epoch:30 step:23443 [D loss: 0.340608, acc.: 82.03%] [G loss: 3.186717]\n",
      "epoch:30 step:23444 [D loss: 0.319766, acc.: 83.59%] [G loss: 2.877689]\n",
      "epoch:30 step:23445 [D loss: 0.383913, acc.: 84.38%] [G loss: 3.143878]\n",
      "epoch:30 step:23446 [D loss: 0.374407, acc.: 84.38%] [G loss: 3.387998]\n",
      "epoch:30 step:23447 [D loss: 0.305415, acc.: 87.50%] [G loss: 3.960337]\n",
      "epoch:30 step:23448 [D loss: 0.319851, acc.: 82.81%] [G loss: 3.125557]\n",
      "epoch:30 step:23449 [D loss: 0.451726, acc.: 76.56%] [G loss: 2.470305]\n",
      "epoch:30 step:23450 [D loss: 0.259345, acc.: 92.19%] [G loss: 2.991374]\n",
      "epoch:30 step:23451 [D loss: 0.291598, acc.: 88.28%] [G loss: 2.681096]\n",
      "epoch:30 step:23452 [D loss: 0.381932, acc.: 85.16%] [G loss: 2.910448]\n",
      "epoch:30 step:23453 [D loss: 0.276266, acc.: 89.06%] [G loss: 3.837585]\n",
      "epoch:30 step:23454 [D loss: 0.341136, acc.: 85.16%] [G loss: 2.647495]\n",
      "epoch:30 step:23455 [D loss: 0.196841, acc.: 94.53%] [G loss: 3.899464]\n",
      "epoch:30 step:23456 [D loss: 0.255486, acc.: 89.84%] [G loss: 3.499985]\n",
      "epoch:30 step:23457 [D loss: 0.350696, acc.: 79.69%] [G loss: 3.358212]\n",
      "epoch:30 step:23458 [D loss: 0.397792, acc.: 76.56%] [G loss: 3.094252]\n",
      "epoch:30 step:23459 [D loss: 0.316602, acc.: 87.50%] [G loss: 2.049830]\n",
      "epoch:30 step:23460 [D loss: 0.382572, acc.: 82.81%] [G loss: 2.766671]\n",
      "epoch:30 step:23461 [D loss: 0.293009, acc.: 84.38%] [G loss: 2.707990]\n",
      "epoch:30 step:23462 [D loss: 0.322685, acc.: 87.50%] [G loss: 3.137745]\n",
      "epoch:30 step:23463 [D loss: 0.334700, acc.: 85.94%] [G loss: 2.698398]\n",
      "epoch:30 step:23464 [D loss: 0.298976, acc.: 87.50%] [G loss: 2.989352]\n",
      "epoch:30 step:23465 [D loss: 0.229518, acc.: 92.19%] [G loss: 2.673707]\n",
      "epoch:30 step:23466 [D loss: 0.293296, acc.: 87.50%] [G loss: 2.709223]\n",
      "epoch:30 step:23467 [D loss: 0.318766, acc.: 84.38%] [G loss: 3.653631]\n",
      "epoch:30 step:23468 [D loss: 0.352358, acc.: 82.03%] [G loss: 2.493006]\n",
      "epoch:30 step:23469 [D loss: 0.258180, acc.: 88.28%] [G loss: 3.106901]\n",
      "epoch:30 step:23470 [D loss: 0.290364, acc.: 87.50%] [G loss: 2.824202]\n",
      "epoch:30 step:23471 [D loss: 0.322343, acc.: 82.81%] [G loss: 2.703013]\n",
      "epoch:30 step:23472 [D loss: 0.372604, acc.: 82.81%] [G loss: 2.506516]\n",
      "epoch:30 step:23473 [D loss: 0.364910, acc.: 83.59%] [G loss: 2.738141]\n",
      "epoch:30 step:23474 [D loss: 0.374187, acc.: 85.16%] [G loss: 2.757238]\n",
      "epoch:30 step:23475 [D loss: 0.331262, acc.: 85.16%] [G loss: 3.292753]\n",
      "epoch:30 step:23476 [D loss: 0.342481, acc.: 81.25%] [G loss: 3.196068]\n",
      "epoch:30 step:23477 [D loss: 0.350945, acc.: 83.59%] [G loss: 2.612921]\n",
      "epoch:30 step:23478 [D loss: 0.325023, acc.: 86.72%] [G loss: 2.770570]\n",
      "epoch:30 step:23479 [D loss: 0.233879, acc.: 91.41%] [G loss: 2.883582]\n",
      "epoch:30 step:23480 [D loss: 0.396785, acc.: 82.81%] [G loss: 3.140357]\n",
      "epoch:30 step:23481 [D loss: 0.349247, acc.: 85.16%] [G loss: 2.886048]\n",
      "epoch:30 step:23482 [D loss: 0.269502, acc.: 85.94%] [G loss: 3.377892]\n",
      "epoch:30 step:23483 [D loss: 0.350362, acc.: 86.72%] [G loss: 3.165261]\n",
      "epoch:30 step:23484 [D loss: 0.263450, acc.: 86.72%] [G loss: 2.986712]\n",
      "epoch:30 step:23485 [D loss: 0.389338, acc.: 82.03%] [G loss: 3.335827]\n",
      "epoch:30 step:23486 [D loss: 0.328459, acc.: 82.03%] [G loss: 2.490778]\n",
      "epoch:30 step:23487 [D loss: 0.260680, acc.: 90.62%] [G loss: 2.794426]\n",
      "epoch:30 step:23488 [D loss: 0.293110, acc.: 87.50%] [G loss: 3.082572]\n",
      "epoch:30 step:23489 [D loss: 0.411722, acc.: 77.34%] [G loss: 3.043304]\n",
      "epoch:30 step:23490 [D loss: 0.336025, acc.: 85.16%] [G loss: 2.783091]\n",
      "epoch:30 step:23491 [D loss: 0.327969, acc.: 82.81%] [G loss: 4.314191]\n",
      "epoch:30 step:23492 [D loss: 0.399116, acc.: 80.47%] [G loss: 2.651918]\n",
      "epoch:30 step:23493 [D loss: 0.304539, acc.: 85.94%] [G loss: 3.729302]\n",
      "epoch:30 step:23494 [D loss: 0.257900, acc.: 88.28%] [G loss: 4.589969]\n",
      "epoch:30 step:23495 [D loss: 0.353967, acc.: 85.94%] [G loss: 4.452442]\n",
      "epoch:30 step:23496 [D loss: 0.239970, acc.: 89.84%] [G loss: 5.449044]\n",
      "epoch:30 step:23497 [D loss: 0.279059, acc.: 87.50%] [G loss: 3.797513]\n",
      "epoch:30 step:23498 [D loss: 0.274565, acc.: 87.50%] [G loss: 4.489117]\n",
      "epoch:30 step:23499 [D loss: 0.204722, acc.: 92.97%] [G loss: 4.815791]\n",
      "epoch:30 step:23500 [D loss: 0.187391, acc.: 92.97%] [G loss: 6.178093]\n",
      "epoch:30 step:23501 [D loss: 0.321225, acc.: 84.38%] [G loss: 3.599448]\n",
      "epoch:30 step:23502 [D loss: 0.327167, acc.: 85.94%] [G loss: 3.579181]\n",
      "epoch:30 step:23503 [D loss: 0.259730, acc.: 85.94%] [G loss: 2.996641]\n",
      "epoch:30 step:23504 [D loss: 0.312953, acc.: 85.94%] [G loss: 3.446430]\n",
      "epoch:30 step:23505 [D loss: 0.403739, acc.: 78.91%] [G loss: 3.530230]\n",
      "epoch:30 step:23506 [D loss: 0.265149, acc.: 86.72%] [G loss: 4.071260]\n",
      "epoch:30 step:23507 [D loss: 0.396601, acc.: 80.47%] [G loss: 2.667130]\n",
      "epoch:30 step:23508 [D loss: 0.386033, acc.: 81.25%] [G loss: 3.016231]\n",
      "epoch:30 step:23509 [D loss: 0.223032, acc.: 93.75%] [G loss: 2.709127]\n",
      "epoch:30 step:23510 [D loss: 0.341000, acc.: 79.69%] [G loss: 3.043167]\n",
      "epoch:30 step:23511 [D loss: 0.257310, acc.: 90.62%] [G loss: 2.828917]\n",
      "epoch:30 step:23512 [D loss: 0.308803, acc.: 86.72%] [G loss: 2.508149]\n",
      "epoch:30 step:23513 [D loss: 0.228069, acc.: 92.97%] [G loss: 2.895334]\n",
      "epoch:30 step:23514 [D loss: 0.181671, acc.: 95.31%] [G loss: 2.920281]\n",
      "epoch:30 step:23515 [D loss: 0.327906, acc.: 84.38%] [G loss: 2.383800]\n",
      "epoch:30 step:23516 [D loss: 0.312713, acc.: 86.72%] [G loss: 2.865159]\n",
      "epoch:30 step:23517 [D loss: 0.382930, acc.: 80.47%] [G loss: 3.521878]\n",
      "epoch:30 step:23518 [D loss: 0.275611, acc.: 85.16%] [G loss: 3.414802]\n",
      "epoch:30 step:23519 [D loss: 0.368478, acc.: 81.25%] [G loss: 2.743583]\n",
      "epoch:30 step:23520 [D loss: 0.380446, acc.: 82.81%] [G loss: 3.347584]\n",
      "epoch:30 step:23521 [D loss: 0.292255, acc.: 85.16%] [G loss: 3.438931]\n",
      "epoch:30 step:23522 [D loss: 0.348161, acc.: 83.59%] [G loss: 4.062171]\n",
      "epoch:30 step:23523 [D loss: 0.302804, acc.: 87.50%] [G loss: 4.837201]\n",
      "epoch:30 step:23524 [D loss: 0.295574, acc.: 85.16%] [G loss: 3.615923]\n",
      "epoch:30 step:23525 [D loss: 0.337087, acc.: 84.38%] [G loss: 2.676855]\n",
      "epoch:30 step:23526 [D loss: 0.221936, acc.: 92.19%] [G loss: 3.734422]\n",
      "epoch:30 step:23527 [D loss: 0.244285, acc.: 89.84%] [G loss: 3.555235]\n",
      "epoch:30 step:23528 [D loss: 0.239738, acc.: 92.19%] [G loss: 2.508859]\n",
      "epoch:30 step:23529 [D loss: 0.381932, acc.: 79.69%] [G loss: 3.988311]\n",
      "epoch:30 step:23530 [D loss: 0.338405, acc.: 82.03%] [G loss: 3.428477]\n",
      "epoch:30 step:23531 [D loss: 0.321346, acc.: 88.28%] [G loss: 2.901297]\n",
      "epoch:30 step:23532 [D loss: 0.322791, acc.: 85.16%] [G loss: 2.970620]\n",
      "epoch:30 step:23533 [D loss: 0.316616, acc.: 89.84%] [G loss: 2.871585]\n",
      "epoch:30 step:23534 [D loss: 0.311171, acc.: 85.16%] [G loss: 3.400665]\n",
      "epoch:30 step:23535 [D loss: 0.281645, acc.: 92.19%] [G loss: 2.884236]\n",
      "epoch:30 step:23536 [D loss: 0.379739, acc.: 82.03%] [G loss: 2.219744]\n",
      "epoch:30 step:23537 [D loss: 0.296343, acc.: 88.28%] [G loss: 3.079544]\n",
      "epoch:30 step:23538 [D loss: 0.273393, acc.: 86.72%] [G loss: 2.556611]\n",
      "epoch:30 step:23539 [D loss: 0.307269, acc.: 84.38%] [G loss: 2.411186]\n",
      "epoch:30 step:23540 [D loss: 0.308478, acc.: 89.06%] [G loss: 2.395248]\n",
      "epoch:30 step:23541 [D loss: 0.327055, acc.: 83.59%] [G loss: 2.554583]\n",
      "epoch:30 step:23542 [D loss: 0.342476, acc.: 85.16%] [G loss: 2.804089]\n",
      "epoch:30 step:23543 [D loss: 0.372447, acc.: 88.28%] [G loss: 2.728045]\n",
      "epoch:30 step:23544 [D loss: 0.372879, acc.: 81.25%] [G loss: 3.513983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23545 [D loss: 0.371958, acc.: 85.94%] [G loss: 2.524675]\n",
      "epoch:30 step:23546 [D loss: 0.345072, acc.: 86.72%] [G loss: 3.067939]\n",
      "epoch:30 step:23547 [D loss: 0.301456, acc.: 85.16%] [G loss: 3.349775]\n",
      "epoch:30 step:23548 [D loss: 0.326628, acc.: 82.81%] [G loss: 2.681569]\n",
      "epoch:30 step:23549 [D loss: 0.256630, acc.: 88.28%] [G loss: 3.141845]\n",
      "epoch:30 step:23550 [D loss: 0.278371, acc.: 93.75%] [G loss: 2.477937]\n",
      "epoch:30 step:23551 [D loss: 0.290411, acc.: 88.28%] [G loss: 3.057745]\n",
      "epoch:30 step:23552 [D loss: 0.321842, acc.: 87.50%] [G loss: 3.946509]\n",
      "epoch:30 step:23553 [D loss: 0.422456, acc.: 79.69%] [G loss: 2.507236]\n",
      "epoch:30 step:23554 [D loss: 0.331888, acc.: 84.38%] [G loss: 2.986691]\n",
      "epoch:30 step:23555 [D loss: 0.427987, acc.: 85.94%] [G loss: 2.948370]\n",
      "epoch:30 step:23556 [D loss: 0.364182, acc.: 85.94%] [G loss: 2.692547]\n",
      "epoch:30 step:23557 [D loss: 0.252474, acc.: 89.06%] [G loss: 3.297152]\n",
      "epoch:30 step:23558 [D loss: 0.255146, acc.: 90.62%] [G loss: 3.082530]\n",
      "epoch:30 step:23559 [D loss: 0.245787, acc.: 87.50%] [G loss: 4.433074]\n",
      "epoch:30 step:23560 [D loss: 0.373836, acc.: 83.59%] [G loss: 2.348890]\n",
      "epoch:30 step:23561 [D loss: 0.337122, acc.: 82.81%] [G loss: 2.641268]\n",
      "epoch:30 step:23562 [D loss: 0.355588, acc.: 81.25%] [G loss: 2.414433]\n",
      "epoch:30 step:23563 [D loss: 0.344643, acc.: 85.94%] [G loss: 2.688428]\n",
      "epoch:30 step:23564 [D loss: 0.292133, acc.: 88.28%] [G loss: 2.830703]\n",
      "epoch:30 step:23565 [D loss: 0.355273, acc.: 82.81%] [G loss: 3.090755]\n",
      "epoch:30 step:23566 [D loss: 0.276263, acc.: 86.72%] [G loss: 3.201288]\n",
      "epoch:30 step:23567 [D loss: 0.323796, acc.: 85.94%] [G loss: 2.815291]\n",
      "epoch:30 step:23568 [D loss: 0.273640, acc.: 87.50%] [G loss: 2.820316]\n",
      "epoch:30 step:23569 [D loss: 0.226271, acc.: 89.84%] [G loss: 3.858847]\n",
      "epoch:30 step:23570 [D loss: 0.306374, acc.: 86.72%] [G loss: 2.720355]\n",
      "epoch:30 step:23571 [D loss: 0.371111, acc.: 82.03%] [G loss: 3.749140]\n",
      "epoch:30 step:23572 [D loss: 0.456557, acc.: 78.12%] [G loss: 3.026520]\n",
      "epoch:30 step:23573 [D loss: 0.356126, acc.: 85.16%] [G loss: 3.639203]\n",
      "epoch:30 step:23574 [D loss: 0.416079, acc.: 81.25%] [G loss: 5.705044]\n",
      "epoch:30 step:23575 [D loss: 0.470305, acc.: 80.47%] [G loss: 5.116714]\n",
      "epoch:30 step:23576 [D loss: 0.495065, acc.: 78.12%] [G loss: 3.390655]\n",
      "epoch:30 step:23577 [D loss: 0.375017, acc.: 82.81%] [G loss: 3.481205]\n",
      "epoch:30 step:23578 [D loss: 0.333273, acc.: 86.72%] [G loss: 3.729437]\n",
      "epoch:30 step:23579 [D loss: 0.292372, acc.: 86.72%] [G loss: 3.366098]\n",
      "epoch:30 step:23580 [D loss: 0.408730, acc.: 82.81%] [G loss: 3.073846]\n",
      "epoch:30 step:23581 [D loss: 0.439491, acc.: 78.12%] [G loss: 3.154078]\n",
      "epoch:30 step:23582 [D loss: 0.394060, acc.: 84.38%] [G loss: 3.476325]\n",
      "epoch:30 step:23583 [D loss: 0.359831, acc.: 81.25%] [G loss: 2.656426]\n",
      "epoch:30 step:23584 [D loss: 0.341182, acc.: 80.47%] [G loss: 2.457171]\n",
      "epoch:30 step:23585 [D loss: 0.275061, acc.: 86.72%] [G loss: 2.756458]\n",
      "epoch:30 step:23586 [D loss: 0.390291, acc.: 82.81%] [G loss: 3.038775]\n",
      "epoch:30 step:23587 [D loss: 0.350954, acc.: 79.69%] [G loss: 3.218554]\n",
      "epoch:30 step:23588 [D loss: 0.425574, acc.: 79.69%] [G loss: 2.480625]\n",
      "epoch:30 step:23589 [D loss: 0.261825, acc.: 89.84%] [G loss: 3.282342]\n",
      "epoch:30 step:23590 [D loss: 0.375321, acc.: 84.38%] [G loss: 3.796758]\n",
      "epoch:30 step:23591 [D loss: 0.364242, acc.: 87.50%] [G loss: 3.334010]\n",
      "epoch:30 step:23592 [D loss: 0.232334, acc.: 90.62%] [G loss: 2.908831]\n",
      "epoch:30 step:23593 [D loss: 0.381971, acc.: 80.47%] [G loss: 3.788692]\n",
      "epoch:30 step:23594 [D loss: 0.320383, acc.: 88.28%] [G loss: 2.630415]\n",
      "epoch:30 step:23595 [D loss: 0.288027, acc.: 83.59%] [G loss: 2.857433]\n",
      "epoch:30 step:23596 [D loss: 0.296934, acc.: 84.38%] [G loss: 2.588298]\n",
      "epoch:30 step:23597 [D loss: 0.347314, acc.: 85.16%] [G loss: 2.441527]\n",
      "epoch:30 step:23598 [D loss: 0.244484, acc.: 92.19%] [G loss: 3.836046]\n",
      "epoch:30 step:23599 [D loss: 0.261431, acc.: 87.50%] [G loss: 4.200723]\n",
      "epoch:30 step:23600 [D loss: 0.367328, acc.: 82.03%] [G loss: 3.020543]\n",
      "epoch:30 step:23601 [D loss: 0.301306, acc.: 85.16%] [G loss: 4.529620]\n",
      "epoch:30 step:23602 [D loss: 0.198713, acc.: 92.97%] [G loss: 4.021360]\n",
      "epoch:30 step:23603 [D loss: 0.262740, acc.: 85.16%] [G loss: 4.297704]\n",
      "epoch:30 step:23604 [D loss: 0.321111, acc.: 85.94%] [G loss: 4.375206]\n",
      "epoch:30 step:23605 [D loss: 0.326201, acc.: 85.94%] [G loss: 3.207486]\n",
      "epoch:30 step:23606 [D loss: 0.265398, acc.: 89.06%] [G loss: 5.023778]\n",
      "epoch:30 step:23607 [D loss: 0.332862, acc.: 86.72%] [G loss: 3.168652]\n",
      "epoch:30 step:23608 [D loss: 0.303782, acc.: 85.94%] [G loss: 4.252643]\n",
      "epoch:30 step:23609 [D loss: 0.353044, acc.: 81.25%] [G loss: 3.163365]\n",
      "epoch:30 step:23610 [D loss: 0.339093, acc.: 85.16%] [G loss: 3.185642]\n",
      "epoch:30 step:23611 [D loss: 0.306300, acc.: 86.72%] [G loss: 3.133842]\n",
      "epoch:30 step:23612 [D loss: 0.322500, acc.: 84.38%] [G loss: 2.537161]\n",
      "epoch:30 step:23613 [D loss: 0.268372, acc.: 89.06%] [G loss: 2.667953]\n",
      "epoch:30 step:23614 [D loss: 0.368591, acc.: 82.81%] [G loss: 2.281415]\n",
      "epoch:30 step:23615 [D loss: 0.299363, acc.: 86.72%] [G loss: 3.511003]\n",
      "epoch:30 step:23616 [D loss: 0.350084, acc.: 85.16%] [G loss: 2.880785]\n",
      "epoch:30 step:23617 [D loss: 0.389854, acc.: 81.25%] [G loss: 3.162086]\n",
      "epoch:30 step:23618 [D loss: 0.356699, acc.: 85.16%] [G loss: 2.928888]\n",
      "epoch:30 step:23619 [D loss: 0.332620, acc.: 85.16%] [G loss: 2.802460]\n",
      "epoch:30 step:23620 [D loss: 0.295526, acc.: 89.06%] [G loss: 3.005805]\n",
      "epoch:30 step:23621 [D loss: 0.384196, acc.: 82.81%] [G loss: 2.556943]\n",
      "epoch:30 step:23622 [D loss: 0.224378, acc.: 95.31%] [G loss: 2.414354]\n",
      "epoch:30 step:23623 [D loss: 0.441907, acc.: 78.12%] [G loss: 2.902324]\n",
      "epoch:30 step:23624 [D loss: 0.420312, acc.: 84.38%] [G loss: 2.348793]\n",
      "epoch:30 step:23625 [D loss: 0.332758, acc.: 88.28%] [G loss: 3.598697]\n",
      "epoch:30 step:23626 [D loss: 0.311133, acc.: 85.16%] [G loss: 3.110236]\n",
      "epoch:30 step:23627 [D loss: 0.385958, acc.: 80.47%] [G loss: 2.422451]\n",
      "epoch:30 step:23628 [D loss: 0.255192, acc.: 89.84%] [G loss: 3.258297]\n",
      "epoch:30 step:23629 [D loss: 0.397919, acc.: 82.03%] [G loss: 2.815553]\n",
      "epoch:30 step:23630 [D loss: 0.301408, acc.: 86.72%] [G loss: 3.102075]\n",
      "epoch:30 step:23631 [D loss: 0.259959, acc.: 88.28%] [G loss: 3.984524]\n",
      "epoch:30 step:23632 [D loss: 0.314002, acc.: 86.72%] [G loss: 3.622218]\n",
      "epoch:30 step:23633 [D loss: 0.266907, acc.: 89.84%] [G loss: 3.188307]\n",
      "epoch:30 step:23634 [D loss: 0.337793, acc.: 82.81%] [G loss: 3.137484]\n",
      "epoch:30 step:23635 [D loss: 0.271733, acc.: 88.28%] [G loss: 2.731547]\n",
      "epoch:30 step:23636 [D loss: 0.294624, acc.: 87.50%] [G loss: 2.976852]\n",
      "epoch:30 step:23637 [D loss: 0.268812, acc.: 86.72%] [G loss: 3.531928]\n",
      "epoch:30 step:23638 [D loss: 0.456265, acc.: 74.22%] [G loss: 4.188894]\n",
      "epoch:30 step:23639 [D loss: 0.234495, acc.: 89.84%] [G loss: 3.250241]\n",
      "epoch:30 step:23640 [D loss: 0.371670, acc.: 79.69%] [G loss: 4.666589]\n",
      "epoch:30 step:23641 [D loss: 0.321156, acc.: 85.94%] [G loss: 4.771632]\n",
      "epoch:30 step:23642 [D loss: 0.329460, acc.: 88.28%] [G loss: 4.155647]\n",
      "epoch:30 step:23643 [D loss: 0.272781, acc.: 87.50%] [G loss: 3.711462]\n",
      "epoch:30 step:23644 [D loss: 0.314215, acc.: 82.03%] [G loss: 2.362864]\n",
      "epoch:30 step:23645 [D loss: 0.316489, acc.: 83.59%] [G loss: 2.831624]\n",
      "epoch:30 step:23646 [D loss: 0.289751, acc.: 85.94%] [G loss: 3.472688]\n",
      "epoch:30 step:23647 [D loss: 0.333434, acc.: 86.72%] [G loss: 2.657396]\n",
      "epoch:30 step:23648 [D loss: 0.311950, acc.: 84.38%] [G loss: 3.092085]\n",
      "epoch:30 step:23649 [D loss: 0.301314, acc.: 85.94%] [G loss: 3.585270]\n",
      "epoch:30 step:23650 [D loss: 0.353333, acc.: 85.16%] [G loss: 5.133481]\n",
      "epoch:30 step:23651 [D loss: 0.342258, acc.: 83.59%] [G loss: 3.764493]\n",
      "epoch:30 step:23652 [D loss: 0.410447, acc.: 82.03%] [G loss: 3.274298]\n",
      "epoch:30 step:23653 [D loss: 0.392971, acc.: 80.47%] [G loss: 3.753367]\n",
      "epoch:30 step:23654 [D loss: 0.370671, acc.: 84.38%] [G loss: 3.437414]\n",
      "epoch:30 step:23655 [D loss: 0.369433, acc.: 82.03%] [G loss: 3.351919]\n",
      "epoch:30 step:23656 [D loss: 0.338033, acc.: 85.94%] [G loss: 2.985520]\n",
      "epoch:30 step:23657 [D loss: 0.337103, acc.: 87.50%] [G loss: 2.652825]\n",
      "epoch:30 step:23658 [D loss: 0.332432, acc.: 86.72%] [G loss: 2.851647]\n",
      "epoch:30 step:23659 [D loss: 0.388277, acc.: 79.69%] [G loss: 2.719150]\n",
      "epoch:30 step:23660 [D loss: 0.384068, acc.: 78.91%] [G loss: 4.006356]\n",
      "epoch:30 step:23661 [D loss: 0.317472, acc.: 88.28%] [G loss: 3.405870]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23662 [D loss: 0.441024, acc.: 82.81%] [G loss: 3.645292]\n",
      "epoch:30 step:23663 [D loss: 0.343206, acc.: 84.38%] [G loss: 3.684420]\n",
      "epoch:30 step:23664 [D loss: 0.301444, acc.: 89.84%] [G loss: 3.587992]\n",
      "epoch:30 step:23665 [D loss: 0.297805, acc.: 82.03%] [G loss: 3.876846]\n",
      "epoch:30 step:23666 [D loss: 0.341366, acc.: 85.16%] [G loss: 3.014090]\n",
      "epoch:30 step:23667 [D loss: 0.336433, acc.: 82.03%] [G loss: 2.735814]\n",
      "epoch:30 step:23668 [D loss: 0.334011, acc.: 84.38%] [G loss: 2.430867]\n",
      "epoch:30 step:23669 [D loss: 0.519493, acc.: 75.00%] [G loss: 3.840271]\n",
      "epoch:30 step:23670 [D loss: 0.435385, acc.: 76.56%] [G loss: 8.180580]\n",
      "epoch:30 step:23671 [D loss: 0.885514, acc.: 67.19%] [G loss: 4.442333]\n",
      "epoch:30 step:23672 [D loss: 0.349971, acc.: 87.50%] [G loss: 4.866602]\n",
      "epoch:30 step:23673 [D loss: 0.673178, acc.: 76.56%] [G loss: 2.372064]\n",
      "epoch:30 step:23674 [D loss: 0.327811, acc.: 85.94%] [G loss: 3.804118]\n",
      "epoch:30 step:23675 [D loss: 0.384211, acc.: 82.81%] [G loss: 3.818946]\n",
      "epoch:30 step:23676 [D loss: 0.266881, acc.: 89.84%] [G loss: 2.413460]\n",
      "epoch:30 step:23677 [D loss: 0.416760, acc.: 74.22%] [G loss: 2.607944]\n",
      "epoch:30 step:23678 [D loss: 0.273370, acc.: 89.06%] [G loss: 3.135651]\n",
      "epoch:30 step:23679 [D loss: 0.394942, acc.: 78.91%] [G loss: 2.721797]\n",
      "epoch:30 step:23680 [D loss: 0.305010, acc.: 86.72%] [G loss: 2.777616]\n",
      "epoch:30 step:23681 [D loss: 0.258018, acc.: 89.06%] [G loss: 2.838232]\n",
      "epoch:30 step:23682 [D loss: 0.380701, acc.: 80.47%] [G loss: 3.349499]\n",
      "epoch:30 step:23683 [D loss: 0.332253, acc.: 85.94%] [G loss: 3.811433]\n",
      "epoch:30 step:23684 [D loss: 0.213970, acc.: 91.41%] [G loss: 3.299232]\n",
      "epoch:30 step:23685 [D loss: 0.327372, acc.: 84.38%] [G loss: 2.883983]\n",
      "epoch:30 step:23686 [D loss: 0.300158, acc.: 91.41%] [G loss: 3.111055]\n",
      "epoch:30 step:23687 [D loss: 0.372575, acc.: 83.59%] [G loss: 2.847979]\n",
      "epoch:30 step:23688 [D loss: 0.234372, acc.: 92.19%] [G loss: 2.773589]\n",
      "epoch:30 step:23689 [D loss: 0.335986, acc.: 87.50%] [G loss: 3.585522]\n",
      "epoch:30 step:23690 [D loss: 0.204772, acc.: 91.41%] [G loss: 4.042752]\n",
      "epoch:30 step:23691 [D loss: 0.355318, acc.: 84.38%] [G loss: 4.775954]\n",
      "epoch:30 step:23692 [D loss: 0.273601, acc.: 85.94%] [G loss: 5.299977]\n",
      "epoch:30 step:23693 [D loss: 0.163256, acc.: 94.53%] [G loss: 4.172497]\n",
      "epoch:30 step:23694 [D loss: 0.232574, acc.: 89.84%] [G loss: 5.495390]\n",
      "epoch:30 step:23695 [D loss: 0.279464, acc.: 85.94%] [G loss: 5.080881]\n",
      "epoch:30 step:23696 [D loss: 0.233823, acc.: 92.97%] [G loss: 4.034991]\n",
      "epoch:30 step:23697 [D loss: 0.251729, acc.: 85.94%] [G loss: 3.685390]\n",
      "epoch:30 step:23698 [D loss: 0.332861, acc.: 83.59%] [G loss: 4.797867]\n",
      "epoch:30 step:23699 [D loss: 0.303284, acc.: 88.28%] [G loss: 4.174426]\n",
      "epoch:30 step:23700 [D loss: 0.323043, acc.: 84.38%] [G loss: 3.634288]\n",
      "epoch:30 step:23701 [D loss: 0.243773, acc.: 89.06%] [G loss: 3.868932]\n",
      "epoch:30 step:23702 [D loss: 0.346156, acc.: 81.25%] [G loss: 3.978038]\n",
      "epoch:30 step:23703 [D loss: 0.258458, acc.: 89.84%] [G loss: 4.132061]\n",
      "epoch:30 step:23704 [D loss: 0.345025, acc.: 82.81%] [G loss: 4.347737]\n",
      "epoch:30 step:23705 [D loss: 0.739362, acc.: 67.97%] [G loss: 5.816422]\n",
      "epoch:30 step:23706 [D loss: 1.090055, acc.: 62.50%] [G loss: 4.248375]\n",
      "epoch:30 step:23707 [D loss: 0.420319, acc.: 84.38%] [G loss: 3.436827]\n",
      "epoch:30 step:23708 [D loss: 0.456027, acc.: 74.22%] [G loss: 3.642826]\n",
      "epoch:30 step:23709 [D loss: 0.390417, acc.: 76.56%] [G loss: 3.190672]\n",
      "epoch:30 step:23710 [D loss: 0.216794, acc.: 92.97%] [G loss: 2.982387]\n",
      "epoch:30 step:23711 [D loss: 0.388543, acc.: 82.03%] [G loss: 2.535787]\n",
      "epoch:30 step:23712 [D loss: 0.388503, acc.: 78.91%] [G loss: 3.141275]\n",
      "epoch:30 step:23713 [D loss: 0.409275, acc.: 80.47%] [G loss: 2.576590]\n",
      "epoch:30 step:23714 [D loss: 0.367316, acc.: 81.25%] [G loss: 2.958831]\n",
      "epoch:30 step:23715 [D loss: 0.269172, acc.: 87.50%] [G loss: 3.224837]\n",
      "epoch:30 step:23716 [D loss: 0.297274, acc.: 86.72%] [G loss: 3.684440]\n",
      "epoch:30 step:23717 [D loss: 0.192375, acc.: 94.53%] [G loss: 3.865151]\n",
      "epoch:30 step:23718 [D loss: 0.296749, acc.: 83.59%] [G loss: 4.525036]\n",
      "epoch:30 step:23719 [D loss: 0.303439, acc.: 85.16%] [G loss: 3.811683]\n",
      "epoch:30 step:23720 [D loss: 0.379063, acc.: 85.94%] [G loss: 3.014842]\n",
      "epoch:30 step:23721 [D loss: 0.316821, acc.: 87.50%] [G loss: 3.099089]\n",
      "epoch:30 step:23722 [D loss: 0.279543, acc.: 89.84%] [G loss: 3.023126]\n",
      "epoch:30 step:23723 [D loss: 0.290505, acc.: 86.72%] [G loss: 2.734494]\n",
      "epoch:30 step:23724 [D loss: 0.356886, acc.: 81.25%] [G loss: 3.996831]\n",
      "epoch:30 step:23725 [D loss: 0.351361, acc.: 82.81%] [G loss: 3.563350]\n",
      "epoch:30 step:23726 [D loss: 0.441887, acc.: 82.03%] [G loss: 2.723007]\n",
      "epoch:30 step:23727 [D loss: 0.320817, acc.: 85.16%] [G loss: 3.025897]\n",
      "epoch:30 step:23728 [D loss: 0.364985, acc.: 85.94%] [G loss: 2.407154]\n",
      "epoch:30 step:23729 [D loss: 0.356427, acc.: 86.72%] [G loss: 2.614481]\n",
      "epoch:30 step:23730 [D loss: 0.313435, acc.: 88.28%] [G loss: 3.045855]\n",
      "epoch:30 step:23731 [D loss: 0.331763, acc.: 83.59%] [G loss: 3.342005]\n",
      "epoch:30 step:23732 [D loss: 0.398176, acc.: 82.03%] [G loss: 2.476524]\n",
      "epoch:30 step:23733 [D loss: 0.322310, acc.: 85.16%] [G loss: 3.217623]\n",
      "epoch:30 step:23734 [D loss: 0.270304, acc.: 87.50%] [G loss: 2.489465]\n",
      "epoch:30 step:23735 [D loss: 0.319466, acc.: 89.06%] [G loss: 3.008903]\n",
      "epoch:30 step:23736 [D loss: 0.409244, acc.: 81.25%] [G loss: 2.430888]\n",
      "epoch:30 step:23737 [D loss: 0.282433, acc.: 88.28%] [G loss: 2.407462]\n",
      "epoch:30 step:23738 [D loss: 0.275459, acc.: 88.28%] [G loss: 2.990899]\n",
      "epoch:30 step:23739 [D loss: 0.350111, acc.: 85.94%] [G loss: 3.073723]\n",
      "epoch:30 step:23740 [D loss: 0.211658, acc.: 91.41%] [G loss: 3.728555]\n",
      "epoch:30 step:23741 [D loss: 0.506243, acc.: 76.56%] [G loss: 2.860651]\n",
      "epoch:30 step:23742 [D loss: 0.337079, acc.: 86.72%] [G loss: 3.153863]\n",
      "epoch:30 step:23743 [D loss: 0.397297, acc.: 81.25%] [G loss: 2.656869]\n",
      "epoch:30 step:23744 [D loss: 0.278000, acc.: 88.28%] [G loss: 3.336300]\n",
      "epoch:30 step:23745 [D loss: 0.403507, acc.: 82.03%] [G loss: 3.031404]\n",
      "epoch:30 step:23746 [D loss: 0.280462, acc.: 89.84%] [G loss: 2.866903]\n",
      "epoch:30 step:23747 [D loss: 0.310731, acc.: 86.72%] [G loss: 5.758410]\n",
      "epoch:30 step:23748 [D loss: 0.452923, acc.: 79.69%] [G loss: 3.892571]\n",
      "epoch:30 step:23749 [D loss: 0.360506, acc.: 82.03%] [G loss: 2.347393]\n",
      "epoch:30 step:23750 [D loss: 0.327791, acc.: 85.94%] [G loss: 3.421892]\n",
      "epoch:30 step:23751 [D loss: 0.433143, acc.: 79.69%] [G loss: 2.539355]\n",
      "epoch:30 step:23752 [D loss: 0.350621, acc.: 84.38%] [G loss: 3.703855]\n",
      "epoch:30 step:23753 [D loss: 0.374466, acc.: 85.16%] [G loss: 3.208893]\n",
      "epoch:30 step:23754 [D loss: 0.418266, acc.: 79.69%] [G loss: 4.113704]\n",
      "epoch:30 step:23755 [D loss: 0.315092, acc.: 83.59%] [G loss: 3.078851]\n",
      "epoch:30 step:23756 [D loss: 0.325023, acc.: 85.94%] [G loss: 3.953352]\n",
      "epoch:30 step:23757 [D loss: 0.237552, acc.: 89.06%] [G loss: 4.579686]\n",
      "epoch:30 step:23758 [D loss: 0.336882, acc.: 82.81%] [G loss: 2.892581]\n",
      "epoch:30 step:23759 [D loss: 0.331727, acc.: 82.03%] [G loss: 4.833120]\n",
      "epoch:30 step:23760 [D loss: 0.351288, acc.: 82.03%] [G loss: 3.474470]\n",
      "epoch:30 step:23761 [D loss: 0.358818, acc.: 85.94%] [G loss: 2.598722]\n",
      "epoch:30 step:23762 [D loss: 0.366755, acc.: 83.59%] [G loss: 2.567060]\n",
      "epoch:30 step:23763 [D loss: 0.363212, acc.: 83.59%] [G loss: 2.504540]\n",
      "epoch:30 step:23764 [D loss: 0.444080, acc.: 80.47%] [G loss: 2.353081]\n",
      "epoch:30 step:23765 [D loss: 0.187621, acc.: 93.75%] [G loss: 4.152812]\n",
      "epoch:30 step:23766 [D loss: 0.398353, acc.: 79.69%] [G loss: 3.382617]\n",
      "epoch:30 step:23767 [D loss: 0.282454, acc.: 89.84%] [G loss: 3.514432]\n",
      "epoch:30 step:23768 [D loss: 0.332484, acc.: 85.94%] [G loss: 2.970755]\n",
      "epoch:30 step:23769 [D loss: 0.260162, acc.: 87.50%] [G loss: 2.918577]\n",
      "epoch:30 step:23770 [D loss: 0.343834, acc.: 85.16%] [G loss: 2.627522]\n",
      "epoch:30 step:23771 [D loss: 0.285731, acc.: 89.84%] [G loss: 3.031024]\n",
      "epoch:30 step:23772 [D loss: 0.359655, acc.: 86.72%] [G loss: 3.616973]\n",
      "epoch:30 step:23773 [D loss: 0.304941, acc.: 85.16%] [G loss: 4.081475]\n",
      "epoch:30 step:23774 [D loss: 0.282635, acc.: 85.94%] [G loss: 3.441819]\n",
      "epoch:30 step:23775 [D loss: 0.256112, acc.: 87.50%] [G loss: 4.024673]\n",
      "epoch:30 step:23776 [D loss: 0.349692, acc.: 83.59%] [G loss: 3.033957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23777 [D loss: 0.509539, acc.: 74.22%] [G loss: 2.458210]\n",
      "epoch:30 step:23778 [D loss: 0.358123, acc.: 84.38%] [G loss: 4.277420]\n",
      "epoch:30 step:23779 [D loss: 0.420877, acc.: 76.56%] [G loss: 3.405234]\n",
      "epoch:30 step:23780 [D loss: 0.404415, acc.: 75.78%] [G loss: 2.728223]\n",
      "epoch:30 step:23781 [D loss: 0.230048, acc.: 91.41%] [G loss: 5.110487]\n",
      "epoch:30 step:23782 [D loss: 0.309767, acc.: 85.16%] [G loss: 3.788122]\n",
      "epoch:30 step:23783 [D loss: 0.369071, acc.: 85.16%] [G loss: 2.146303]\n",
      "epoch:30 step:23784 [D loss: 0.258835, acc.: 88.28%] [G loss: 3.055902]\n",
      "epoch:30 step:23785 [D loss: 0.275593, acc.: 90.62%] [G loss: 3.264277]\n",
      "epoch:30 step:23786 [D loss: 0.368941, acc.: 83.59%] [G loss: 3.076900]\n",
      "epoch:30 step:23787 [D loss: 0.487964, acc.: 75.78%] [G loss: 3.916248]\n",
      "epoch:30 step:23788 [D loss: 0.451052, acc.: 81.25%] [G loss: 2.685503]\n",
      "epoch:30 step:23789 [D loss: 0.400898, acc.: 82.81%] [G loss: 2.546938]\n",
      "epoch:30 step:23790 [D loss: 0.249149, acc.: 89.84%] [G loss: 3.586682]\n",
      "epoch:30 step:23791 [D loss: 0.350832, acc.: 80.47%] [G loss: 2.778872]\n",
      "epoch:30 step:23792 [D loss: 0.364724, acc.: 81.25%] [G loss: 2.973340]\n",
      "epoch:30 step:23793 [D loss: 0.321640, acc.: 85.94%] [G loss: 3.639142]\n",
      "epoch:30 step:23794 [D loss: 0.367189, acc.: 85.16%] [G loss: 3.369403]\n",
      "epoch:30 step:23795 [D loss: 0.258086, acc.: 89.06%] [G loss: 3.253081]\n",
      "epoch:30 step:23796 [D loss: 0.400333, acc.: 78.91%] [G loss: 3.734352]\n",
      "epoch:30 step:23797 [D loss: 0.419198, acc.: 80.47%] [G loss: 2.738508]\n",
      "epoch:30 step:23798 [D loss: 0.332895, acc.: 84.38%] [G loss: 3.494614]\n",
      "epoch:30 step:23799 [D loss: 0.340409, acc.: 85.16%] [G loss: 2.948246]\n",
      "epoch:30 step:23800 [D loss: 0.295997, acc.: 86.72%] [G loss: 2.707128]\n",
      "epoch:30 step:23801 [D loss: 0.266356, acc.: 89.06%] [G loss: 3.065041]\n",
      "epoch:30 step:23802 [D loss: 0.384809, acc.: 84.38%] [G loss: 3.107879]\n",
      "epoch:30 step:23803 [D loss: 0.334191, acc.: 85.94%] [G loss: 3.614112]\n",
      "epoch:30 step:23804 [D loss: 0.444990, acc.: 78.12%] [G loss: 2.900662]\n",
      "epoch:30 step:23805 [D loss: 0.323149, acc.: 85.16%] [G loss: 3.578433]\n",
      "epoch:30 step:23806 [D loss: 0.499915, acc.: 82.03%] [G loss: 2.791831]\n",
      "epoch:30 step:23807 [D loss: 0.304492, acc.: 85.94%] [G loss: 3.674024]\n",
      "epoch:30 step:23808 [D loss: 0.248015, acc.: 88.28%] [G loss: 6.361840]\n",
      "epoch:30 step:23809 [D loss: 0.249446, acc.: 88.28%] [G loss: 7.261977]\n",
      "epoch:30 step:23810 [D loss: 0.232961, acc.: 89.06%] [G loss: 5.698765]\n",
      "epoch:30 step:23811 [D loss: 0.217455, acc.: 88.28%] [G loss: 3.846941]\n",
      "epoch:30 step:23812 [D loss: 0.298020, acc.: 88.28%] [G loss: 3.651640]\n",
      "epoch:30 step:23813 [D loss: 0.320696, acc.: 82.81%] [G loss: 3.756724]\n",
      "epoch:30 step:23814 [D loss: 0.271800, acc.: 85.16%] [G loss: 4.350746]\n",
      "epoch:30 step:23815 [D loss: 0.427015, acc.: 80.47%] [G loss: 4.132821]\n",
      "epoch:30 step:23816 [D loss: 0.426690, acc.: 78.12%] [G loss: 3.419913]\n",
      "epoch:30 step:23817 [D loss: 0.377030, acc.: 86.72%] [G loss: 2.913983]\n",
      "epoch:30 step:23818 [D loss: 0.459117, acc.: 75.78%] [G loss: 3.863222]\n",
      "epoch:30 step:23819 [D loss: 0.428739, acc.: 75.78%] [G loss: 3.698451]\n",
      "epoch:30 step:23820 [D loss: 0.256566, acc.: 91.41%] [G loss: 3.461186]\n",
      "epoch:30 step:23821 [D loss: 0.437714, acc.: 81.25%] [G loss: 3.339954]\n",
      "epoch:30 step:23822 [D loss: 0.270360, acc.: 90.62%] [G loss: 3.319875]\n",
      "epoch:30 step:23823 [D loss: 0.335825, acc.: 86.72%] [G loss: 3.224204]\n",
      "epoch:30 step:23824 [D loss: 0.273144, acc.: 87.50%] [G loss: 2.752215]\n",
      "epoch:30 step:23825 [D loss: 0.413532, acc.: 81.25%] [G loss: 3.864035]\n",
      "epoch:30 step:23826 [D loss: 0.335967, acc.: 82.81%] [G loss: 3.654892]\n",
      "epoch:30 step:23827 [D loss: 0.336778, acc.: 82.03%] [G loss: 4.859803]\n",
      "epoch:30 step:23828 [D loss: 0.448239, acc.: 81.25%] [G loss: 4.058598]\n",
      "epoch:30 step:23829 [D loss: 0.346070, acc.: 82.03%] [G loss: 3.832647]\n",
      "epoch:30 step:23830 [D loss: 0.251679, acc.: 86.72%] [G loss: 4.091775]\n",
      "epoch:30 step:23831 [D loss: 0.268646, acc.: 87.50%] [G loss: 2.895407]\n",
      "epoch:30 step:23832 [D loss: 0.305388, acc.: 89.84%] [G loss: 4.157706]\n",
      "epoch:30 step:23833 [D loss: 0.274724, acc.: 87.50%] [G loss: 4.549070]\n",
      "epoch:30 step:23834 [D loss: 0.334767, acc.: 85.16%] [G loss: 4.031929]\n",
      "epoch:30 step:23835 [D loss: 0.289383, acc.: 89.84%] [G loss: 4.009351]\n",
      "epoch:30 step:23836 [D loss: 0.373326, acc.: 82.81%] [G loss: 3.088704]\n",
      "epoch:30 step:23837 [D loss: 0.266787, acc.: 86.72%] [G loss: 3.047409]\n",
      "epoch:30 step:23838 [D loss: 0.314524, acc.: 85.94%] [G loss: 2.996174]\n",
      "epoch:30 step:23839 [D loss: 0.345583, acc.: 81.25%] [G loss: 2.757453]\n",
      "epoch:30 step:23840 [D loss: 0.277743, acc.: 89.06%] [G loss: 2.630380]\n",
      "epoch:30 step:23841 [D loss: 0.350780, acc.: 83.59%] [G loss: 3.257043]\n",
      "epoch:30 step:23842 [D loss: 0.386017, acc.: 82.03%] [G loss: 2.027064]\n",
      "epoch:30 step:23843 [D loss: 0.311830, acc.: 85.94%] [G loss: 3.285361]\n",
      "epoch:30 step:23844 [D loss: 0.393644, acc.: 81.25%] [G loss: 3.773373]\n",
      "epoch:30 step:23845 [D loss: 0.339064, acc.: 84.38%] [G loss: 2.462696]\n",
      "epoch:30 step:23846 [D loss: 0.245161, acc.: 92.19%] [G loss: 2.847567]\n",
      "epoch:30 step:23847 [D loss: 0.354660, acc.: 84.38%] [G loss: 3.079511]\n",
      "epoch:30 step:23848 [D loss: 0.262146, acc.: 89.06%] [G loss: 2.073709]\n",
      "epoch:30 step:23849 [D loss: 0.418699, acc.: 80.47%] [G loss: 2.454866]\n",
      "epoch:30 step:23850 [D loss: 0.393145, acc.: 79.69%] [G loss: 2.835330]\n",
      "epoch:30 step:23851 [D loss: 0.285541, acc.: 89.06%] [G loss: 2.125428]\n",
      "epoch:30 step:23852 [D loss: 0.337842, acc.: 84.38%] [G loss: 2.755525]\n",
      "epoch:30 step:23853 [D loss: 0.371219, acc.: 82.81%] [G loss: 2.778151]\n",
      "epoch:30 step:23854 [D loss: 0.516308, acc.: 78.12%] [G loss: 2.866201]\n",
      "epoch:30 step:23855 [D loss: 0.335146, acc.: 86.72%] [G loss: 2.066514]\n",
      "epoch:30 step:23856 [D loss: 0.365714, acc.: 85.16%] [G loss: 2.505509]\n",
      "epoch:30 step:23857 [D loss: 0.291972, acc.: 87.50%] [G loss: 2.900139]\n",
      "epoch:30 step:23858 [D loss: 0.447340, acc.: 76.56%] [G loss: 2.283926]\n",
      "epoch:30 step:23859 [D loss: 0.298499, acc.: 87.50%] [G loss: 2.595150]\n",
      "epoch:30 step:23860 [D loss: 0.313682, acc.: 88.28%] [G loss: 3.597156]\n",
      "epoch:30 step:23861 [D loss: 0.302043, acc.: 86.72%] [G loss: 2.538567]\n",
      "epoch:30 step:23862 [D loss: 0.307532, acc.: 85.94%] [G loss: 2.479273]\n",
      "epoch:30 step:23863 [D loss: 0.261510, acc.: 88.28%] [G loss: 3.274358]\n",
      "epoch:30 step:23864 [D loss: 0.229298, acc.: 90.62%] [G loss: 3.929225]\n",
      "epoch:30 step:23865 [D loss: 0.331844, acc.: 82.03%] [G loss: 2.711349]\n",
      "epoch:30 step:23866 [D loss: 0.408229, acc.: 82.03%] [G loss: 2.596937]\n",
      "epoch:30 step:23867 [D loss: 0.228256, acc.: 93.75%] [G loss: 2.843911]\n",
      "epoch:30 step:23868 [D loss: 0.354308, acc.: 83.59%] [G loss: 4.094032]\n",
      "epoch:30 step:23869 [D loss: 0.278861, acc.: 89.06%] [G loss: 2.933504]\n",
      "epoch:30 step:23870 [D loss: 0.293860, acc.: 89.84%] [G loss: 2.754129]\n",
      "epoch:30 step:23871 [D loss: 0.349350, acc.: 83.59%] [G loss: 2.673290]\n",
      "epoch:30 step:23872 [D loss: 0.370751, acc.: 80.47%] [G loss: 2.715854]\n",
      "epoch:30 step:23873 [D loss: 0.351897, acc.: 81.25%] [G loss: 3.440775]\n",
      "epoch:30 step:23874 [D loss: 0.426962, acc.: 78.12%] [G loss: 3.922609]\n",
      "epoch:30 step:23875 [D loss: 0.332359, acc.: 85.16%] [G loss: 4.588032]\n",
      "epoch:30 step:23876 [D loss: 0.275473, acc.: 89.06%] [G loss: 3.721619]\n",
      "epoch:30 step:23877 [D loss: 0.238886, acc.: 90.62%] [G loss: 3.903574]\n",
      "epoch:30 step:23878 [D loss: 0.294371, acc.: 88.28%] [G loss: 4.250462]\n",
      "epoch:30 step:23879 [D loss: 0.307983, acc.: 89.84%] [G loss: 4.108735]\n",
      "epoch:30 step:23880 [D loss: 0.251052, acc.: 86.72%] [G loss: 4.459965]\n",
      "epoch:30 step:23881 [D loss: 0.275573, acc.: 84.38%] [G loss: 4.448298]\n",
      "epoch:30 step:23882 [D loss: 0.355582, acc.: 87.50%] [G loss: 4.021464]\n",
      "epoch:30 step:23883 [D loss: 0.277987, acc.: 85.16%] [G loss: 3.217196]\n",
      "epoch:30 step:23884 [D loss: 0.277513, acc.: 87.50%] [G loss: 3.192256]\n",
      "epoch:30 step:23885 [D loss: 0.311258, acc.: 88.28%] [G loss: 2.909374]\n",
      "epoch:30 step:23886 [D loss: 0.298573, acc.: 85.94%] [G loss: 3.223227]\n",
      "epoch:30 step:23887 [D loss: 0.347137, acc.: 85.16%] [G loss: 2.817984]\n",
      "epoch:30 step:23888 [D loss: 0.188595, acc.: 92.97%] [G loss: 3.951051]\n",
      "epoch:30 step:23889 [D loss: 0.291679, acc.: 84.38%] [G loss: 3.363776]\n",
      "epoch:30 step:23890 [D loss: 0.357419, acc.: 82.81%] [G loss: 2.889632]\n",
      "epoch:30 step:23891 [D loss: 0.272213, acc.: 89.84%] [G loss: 4.783970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23892 [D loss: 0.314069, acc.: 85.94%] [G loss: 4.296799]\n",
      "epoch:30 step:23893 [D loss: 0.240735, acc.: 89.06%] [G loss: 5.960157]\n",
      "epoch:30 step:23894 [D loss: 0.300931, acc.: 87.50%] [G loss: 3.704792]\n",
      "epoch:30 step:23895 [D loss: 0.367244, acc.: 82.03%] [G loss: 5.156236]\n",
      "epoch:30 step:23896 [D loss: 0.303288, acc.: 84.38%] [G loss: 4.433042]\n",
      "epoch:30 step:23897 [D loss: 0.352173, acc.: 83.59%] [G loss: 4.388633]\n",
      "epoch:30 step:23898 [D loss: 0.271417, acc.: 91.41%] [G loss: 3.119481]\n",
      "epoch:30 step:23899 [D loss: 0.238350, acc.: 89.06%] [G loss: 4.801826]\n",
      "epoch:30 step:23900 [D loss: 0.290870, acc.: 85.16%] [G loss: 3.615808]\n",
      "epoch:30 step:23901 [D loss: 0.298179, acc.: 89.84%] [G loss: 3.833585]\n",
      "epoch:30 step:23902 [D loss: 0.289423, acc.: 88.28%] [G loss: 3.341839]\n",
      "epoch:30 step:23903 [D loss: 0.357666, acc.: 85.16%] [G loss: 3.983426]\n",
      "epoch:30 step:23904 [D loss: 0.394901, acc.: 83.59%] [G loss: 3.602521]\n",
      "epoch:30 step:23905 [D loss: 0.405703, acc.: 77.34%] [G loss: 3.985351]\n",
      "epoch:30 step:23906 [D loss: 0.504595, acc.: 79.69%] [G loss: 4.795686]\n",
      "epoch:30 step:23907 [D loss: 0.320711, acc.: 82.81%] [G loss: 4.995100]\n",
      "epoch:30 step:23908 [D loss: 0.160788, acc.: 95.31%] [G loss: 5.585727]\n",
      "epoch:30 step:23909 [D loss: 0.300242, acc.: 85.94%] [G loss: 4.482081]\n",
      "epoch:30 step:23910 [D loss: 0.267060, acc.: 87.50%] [G loss: 4.464213]\n",
      "epoch:30 step:23911 [D loss: 0.331434, acc.: 85.16%] [G loss: 3.774621]\n",
      "epoch:30 step:23912 [D loss: 0.379730, acc.: 81.25%] [G loss: 4.565651]\n",
      "epoch:30 step:23913 [D loss: 0.384229, acc.: 79.69%] [G loss: 3.025086]\n",
      "epoch:30 step:23914 [D loss: 0.377434, acc.: 85.16%] [G loss: 8.316675]\n",
      "epoch:30 step:23915 [D loss: 0.265646, acc.: 87.50%] [G loss: 6.912542]\n",
      "epoch:30 step:23916 [D loss: 0.246856, acc.: 87.50%] [G loss: 6.577179]\n",
      "epoch:30 step:23917 [D loss: 0.206183, acc.: 90.62%] [G loss: 6.023879]\n",
      "epoch:30 step:23918 [D loss: 0.259496, acc.: 87.50%] [G loss: 5.148793]\n",
      "epoch:30 step:23919 [D loss: 0.322873, acc.: 84.38%] [G loss: 4.140002]\n",
      "epoch:30 step:23920 [D loss: 0.269520, acc.: 87.50%] [G loss: 3.889550]\n",
      "epoch:30 step:23921 [D loss: 0.214199, acc.: 92.97%] [G loss: 4.424415]\n",
      "epoch:30 step:23922 [D loss: 0.285270, acc.: 88.28%] [G loss: 3.882245]\n",
      "epoch:30 step:23923 [D loss: 0.264322, acc.: 91.41%] [G loss: 3.177643]\n",
      "epoch:30 step:23924 [D loss: 0.287727, acc.: 89.06%] [G loss: 3.598105]\n",
      "epoch:30 step:23925 [D loss: 0.265900, acc.: 91.41%] [G loss: 3.555940]\n",
      "epoch:30 step:23926 [D loss: 0.321892, acc.: 86.72%] [G loss: 4.374946]\n",
      "epoch:30 step:23927 [D loss: 0.346252, acc.: 82.81%] [G loss: 3.380847]\n",
      "epoch:30 step:23928 [D loss: 0.264447, acc.: 86.72%] [G loss: 4.258397]\n",
      "epoch:30 step:23929 [D loss: 0.285782, acc.: 87.50%] [G loss: 3.277234]\n",
      "epoch:30 step:23930 [D loss: 0.383952, acc.: 79.69%] [G loss: 4.271869]\n",
      "epoch:30 step:23931 [D loss: 0.269862, acc.: 86.72%] [G loss: 4.643510]\n",
      "epoch:30 step:23932 [D loss: 0.394691, acc.: 80.47%] [G loss: 2.768967]\n",
      "epoch:30 step:23933 [D loss: 0.291953, acc.: 88.28%] [G loss: 4.647195]\n",
      "epoch:30 step:23934 [D loss: 0.412146, acc.: 82.03%] [G loss: 2.651703]\n",
      "epoch:30 step:23935 [D loss: 0.341694, acc.: 82.81%] [G loss: 3.783140]\n",
      "epoch:30 step:23936 [D loss: 0.400316, acc.: 81.25%] [G loss: 3.475718]\n",
      "epoch:30 step:23937 [D loss: 0.335510, acc.: 85.16%] [G loss: 2.454480]\n",
      "epoch:30 step:23938 [D loss: 0.381246, acc.: 80.47%] [G loss: 2.919893]\n",
      "epoch:30 step:23939 [D loss: 0.331301, acc.: 84.38%] [G loss: 2.813982]\n",
      "epoch:30 step:23940 [D loss: 0.322484, acc.: 83.59%] [G loss: 3.345363]\n",
      "epoch:30 step:23941 [D loss: 0.432519, acc.: 79.69%] [G loss: 2.460069]\n",
      "epoch:30 step:23942 [D loss: 0.346783, acc.: 88.28%] [G loss: 2.912745]\n",
      "epoch:30 step:23943 [D loss: 0.328338, acc.: 83.59%] [G loss: 2.797778]\n",
      "epoch:30 step:23944 [D loss: 0.349089, acc.: 82.81%] [G loss: 3.088223]\n",
      "epoch:30 step:23945 [D loss: 0.320760, acc.: 86.72%] [G loss: 3.521914]\n",
      "epoch:30 step:23946 [D loss: 0.341522, acc.: 83.59%] [G loss: 2.208193]\n",
      "epoch:30 step:23947 [D loss: 0.395760, acc.: 79.69%] [G loss: 3.198300]\n",
      "epoch:30 step:23948 [D loss: 0.353287, acc.: 82.03%] [G loss: 2.710916]\n",
      "epoch:30 step:23949 [D loss: 0.309924, acc.: 86.72%] [G loss: 3.687011]\n",
      "epoch:30 step:23950 [D loss: 0.413192, acc.: 83.59%] [G loss: 2.550556]\n",
      "epoch:30 step:23951 [D loss: 0.340796, acc.: 85.94%] [G loss: 2.360903]\n",
      "epoch:30 step:23952 [D loss: 0.287136, acc.: 89.06%] [G loss: 2.421817]\n",
      "epoch:30 step:23953 [D loss: 0.268477, acc.: 88.28%] [G loss: 2.247330]\n",
      "epoch:30 step:23954 [D loss: 0.373982, acc.: 82.03%] [G loss: 2.358256]\n",
      "epoch:30 step:23955 [D loss: 0.362951, acc.: 84.38%] [G loss: 3.199394]\n",
      "epoch:30 step:23956 [D loss: 0.383999, acc.: 85.16%] [G loss: 5.438881]\n",
      "epoch:30 step:23957 [D loss: 0.308216, acc.: 85.16%] [G loss: 2.848104]\n",
      "epoch:30 step:23958 [D loss: 0.328680, acc.: 84.38%] [G loss: 4.132456]\n",
      "epoch:30 step:23959 [D loss: 0.380817, acc.: 82.03%] [G loss: 5.022081]\n",
      "epoch:30 step:23960 [D loss: 0.673033, acc.: 74.22%] [G loss: 6.501075]\n",
      "epoch:30 step:23961 [D loss: 1.038538, acc.: 70.31%] [G loss: 7.865253]\n",
      "epoch:30 step:23962 [D loss: 2.784976, acc.: 48.44%] [G loss: 6.691317]\n",
      "epoch:30 step:23963 [D loss: 1.369120, acc.: 75.78%] [G loss: 6.497488]\n",
      "epoch:30 step:23964 [D loss: 0.807381, acc.: 69.53%] [G loss: 4.531215]\n",
      "epoch:30 step:23965 [D loss: 0.719595, acc.: 79.69%] [G loss: 4.445958]\n",
      "epoch:30 step:23966 [D loss: 0.706018, acc.: 73.44%] [G loss: 2.415990]\n",
      "epoch:30 step:23967 [D loss: 0.680826, acc.: 70.31%] [G loss: 3.312685]\n",
      "epoch:30 step:23968 [D loss: 0.384773, acc.: 82.03%] [G loss: 3.493986]\n",
      "epoch:30 step:23969 [D loss: 0.304248, acc.: 85.94%] [G loss: 3.776891]\n",
      "epoch:30 step:23970 [D loss: 0.349903, acc.: 85.94%] [G loss: 2.905657]\n",
      "epoch:30 step:23971 [D loss: 0.390786, acc.: 81.25%] [G loss: 4.330407]\n",
      "epoch:30 step:23972 [D loss: 0.394086, acc.: 82.03%] [G loss: 3.492165]\n",
      "epoch:30 step:23973 [D loss: 0.301489, acc.: 85.94%] [G loss: 2.833599]\n",
      "epoch:30 step:23974 [D loss: 0.437772, acc.: 75.00%] [G loss: 2.725235]\n",
      "epoch:30 step:23975 [D loss: 0.257265, acc.: 90.62%] [G loss: 2.756572]\n",
      "epoch:30 step:23976 [D loss: 0.335233, acc.: 85.94%] [G loss: 3.090192]\n",
      "epoch:30 step:23977 [D loss: 0.323909, acc.: 83.59%] [G loss: 2.699567]\n",
      "epoch:30 step:23978 [D loss: 0.253978, acc.: 90.62%] [G loss: 2.535105]\n",
      "epoch:30 step:23979 [D loss: 0.351894, acc.: 83.59%] [G loss: 2.512944]\n",
      "epoch:30 step:23980 [D loss: 0.280569, acc.: 86.72%] [G loss: 2.354954]\n",
      "epoch:30 step:23981 [D loss: 0.308596, acc.: 86.72%] [G loss: 2.425440]\n",
      "epoch:30 step:23982 [D loss: 0.361697, acc.: 85.16%] [G loss: 2.407343]\n",
      "epoch:30 step:23983 [D loss: 0.287944, acc.: 89.84%] [G loss: 2.626450]\n",
      "epoch:30 step:23984 [D loss: 0.368175, acc.: 82.03%] [G loss: 2.984461]\n",
      "epoch:30 step:23985 [D loss: 0.363258, acc.: 83.59%] [G loss: 2.503968]\n",
      "epoch:30 step:23986 [D loss: 0.359335, acc.: 85.94%] [G loss: 2.164219]\n",
      "epoch:30 step:23987 [D loss: 0.302880, acc.: 86.72%] [G loss: 3.112637]\n",
      "epoch:30 step:23988 [D loss: 0.426701, acc.: 82.03%] [G loss: 3.168780]\n",
      "epoch:30 step:23989 [D loss: 0.340002, acc.: 84.38%] [G loss: 2.728290]\n",
      "epoch:30 step:23990 [D loss: 0.338796, acc.: 85.94%] [G loss: 2.485896]\n",
      "epoch:30 step:23991 [D loss: 0.354101, acc.: 84.38%] [G loss: 2.711917]\n",
      "epoch:30 step:23992 [D loss: 0.374014, acc.: 81.25%] [G loss: 2.474586]\n",
      "epoch:30 step:23993 [D loss: 0.254911, acc.: 90.62%] [G loss: 2.679762]\n",
      "epoch:30 step:23994 [D loss: 0.331380, acc.: 84.38%] [G loss: 2.242339]\n",
      "epoch:30 step:23995 [D loss: 0.339371, acc.: 85.16%] [G loss: 2.968867]\n",
      "epoch:30 step:23996 [D loss: 0.278116, acc.: 88.28%] [G loss: 3.819144]\n",
      "epoch:30 step:23997 [D loss: 0.369243, acc.: 82.81%] [G loss: 2.780468]\n",
      "epoch:30 step:23998 [D loss: 0.460745, acc.: 75.78%] [G loss: 3.227857]\n",
      "epoch:30 step:23999 [D loss: 0.342928, acc.: 82.81%] [G loss: 4.184089]\n",
      "epoch:30 step:24000 [D loss: 0.329082, acc.: 85.16%] [G loss: 3.666014]\n",
      "epoch:30 step:24001 [D loss: 0.344345, acc.: 80.47%] [G loss: 2.881349]\n",
      "epoch:30 step:24002 [D loss: 0.307372, acc.: 82.03%] [G loss: 3.244747]\n",
      "epoch:30 step:24003 [D loss: 0.347893, acc.: 78.12%] [G loss: 3.537765]\n",
      "epoch:30 step:24004 [D loss: 0.418439, acc.: 79.69%] [G loss: 3.620916]\n",
      "epoch:30 step:24005 [D loss: 0.463600, acc.: 77.34%] [G loss: 3.472796]\n",
      "epoch:30 step:24006 [D loss: 0.423040, acc.: 79.69%] [G loss: 2.507315]\n",
      "epoch:30 step:24007 [D loss: 0.328716, acc.: 85.94%] [G loss: 3.115452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:24008 [D loss: 0.402143, acc.: 80.47%] [G loss: 2.843806]\n",
      "epoch:30 step:24009 [D loss: 0.286976, acc.: 88.28%] [G loss: 2.868337]\n",
      "epoch:30 step:24010 [D loss: 0.282296, acc.: 90.62%] [G loss: 3.112648]\n",
      "epoch:30 step:24011 [D loss: 0.308495, acc.: 86.72%] [G loss: 3.336565]\n",
      "epoch:30 step:24012 [D loss: 0.297789, acc.: 89.84%] [G loss: 3.010739]\n",
      "epoch:30 step:24013 [D loss: 0.414422, acc.: 79.69%] [G loss: 2.977463]\n",
      "epoch:30 step:24014 [D loss: 0.373408, acc.: 83.59%] [G loss: 2.873666]\n",
      "epoch:30 step:24015 [D loss: 0.332761, acc.: 85.16%] [G loss: 4.371981]\n",
      "epoch:30 step:24016 [D loss: 0.295693, acc.: 86.72%] [G loss: 3.989426]\n",
      "epoch:30 step:24017 [D loss: 0.216285, acc.: 91.41%] [G loss: 4.255661]\n",
      "epoch:30 step:24018 [D loss: 0.280366, acc.: 89.84%] [G loss: 3.611464]\n",
      "epoch:30 step:24019 [D loss: 0.342483, acc.: 88.28%] [G loss: 4.073658]\n",
      "epoch:30 step:24020 [D loss: 0.319600, acc.: 85.94%] [G loss: 3.491520]\n",
      "epoch:30 step:24021 [D loss: 0.320192, acc.: 84.38%] [G loss: 4.925447]\n",
      "epoch:30 step:24022 [D loss: 0.319059, acc.: 85.94%] [G loss: 3.822690]\n",
      "epoch:30 step:24023 [D loss: 0.345196, acc.: 85.94%] [G loss: 3.857881]\n",
      "epoch:30 step:24024 [D loss: 0.215387, acc.: 92.19%] [G loss: 3.054070]\n",
      "epoch:30 step:24025 [D loss: 0.356921, acc.: 84.38%] [G loss: 2.470911]\n",
      "epoch:30 step:24026 [D loss: 0.335418, acc.: 85.94%] [G loss: 2.437379]\n",
      "epoch:30 step:24027 [D loss: 0.303347, acc.: 89.84%] [G loss: 2.481929]\n",
      "epoch:30 step:24028 [D loss: 0.327314, acc.: 86.72%] [G loss: 2.472656]\n",
      "epoch:30 step:24029 [D loss: 0.233944, acc.: 93.75%] [G loss: 3.489261]\n",
      "epoch:30 step:24030 [D loss: 0.271013, acc.: 86.72%] [G loss: 5.674621]\n",
      "epoch:30 step:24031 [D loss: 0.383681, acc.: 86.72%] [G loss: 2.755027]\n",
      "epoch:30 step:24032 [D loss: 0.339149, acc.: 85.16%] [G loss: 3.323002]\n",
      "epoch:30 step:24033 [D loss: 0.333109, acc.: 83.59%] [G loss: 2.319196]\n",
      "epoch:30 step:24034 [D loss: 0.335657, acc.: 82.81%] [G loss: 2.474185]\n",
      "epoch:30 step:24035 [D loss: 0.349249, acc.: 83.59%] [G loss: 3.167349]\n",
      "epoch:30 step:24036 [D loss: 0.315038, acc.: 86.72%] [G loss: 2.576411]\n",
      "epoch:30 step:24037 [D loss: 0.246608, acc.: 93.75%] [G loss: 2.916191]\n",
      "epoch:30 step:24038 [D loss: 0.399645, acc.: 85.16%] [G loss: 2.965429]\n",
      "epoch:30 step:24039 [D loss: 0.314104, acc.: 87.50%] [G loss: 2.137785]\n",
      "epoch:30 step:24040 [D loss: 0.373082, acc.: 82.03%] [G loss: 2.597009]\n",
      "epoch:30 step:24041 [D loss: 0.401816, acc.: 81.25%] [G loss: 2.726699]\n",
      "epoch:30 step:24042 [D loss: 0.337307, acc.: 84.38%] [G loss: 2.685522]\n",
      "epoch:30 step:24043 [D loss: 0.445769, acc.: 75.78%] [G loss: 4.140447]\n",
      "epoch:30 step:24044 [D loss: 0.407801, acc.: 80.47%] [G loss: 3.589540]\n",
      "epoch:30 step:24045 [D loss: 0.356241, acc.: 83.59%] [G loss: 4.388592]\n",
      "epoch:30 step:24046 [D loss: 0.342596, acc.: 84.38%] [G loss: 3.186153]\n",
      "epoch:30 step:24047 [D loss: 0.369200, acc.: 82.03%] [G loss: 3.196916]\n",
      "epoch:30 step:24048 [D loss: 0.436274, acc.: 78.91%] [G loss: 3.247149]\n",
      "epoch:30 step:24049 [D loss: 0.402821, acc.: 78.12%] [G loss: 3.570062]\n",
      "epoch:30 step:24050 [D loss: 0.338073, acc.: 86.72%] [G loss: 3.722095]\n",
      "epoch:30 step:24051 [D loss: 0.452156, acc.: 82.03%] [G loss: 5.250906]\n",
      "epoch:30 step:24052 [D loss: 0.350864, acc.: 85.16%] [G loss: 2.693604]\n",
      "epoch:30 step:24053 [D loss: 0.330591, acc.: 86.72%] [G loss: 4.951434]\n",
      "epoch:30 step:24054 [D loss: 0.387558, acc.: 83.59%] [G loss: 3.766582]\n",
      "epoch:30 step:24055 [D loss: 0.364530, acc.: 80.47%] [G loss: 5.321277]\n",
      "epoch:30 step:24056 [D loss: 0.311277, acc.: 85.16%] [G loss: 5.465293]\n",
      "epoch:30 step:24057 [D loss: 0.232951, acc.: 91.41%] [G loss: 4.208311]\n",
      "epoch:30 step:24058 [D loss: 0.302076, acc.: 85.94%] [G loss: 5.232100]\n",
      "epoch:30 step:24059 [D loss: 0.273963, acc.: 86.72%] [G loss: 3.976095]\n",
      "epoch:30 step:24060 [D loss: 0.312251, acc.: 82.81%] [G loss: 3.612807]\n",
      "epoch:30 step:24061 [D loss: 0.402681, acc.: 83.59%] [G loss: 3.941007]\n",
      "epoch:30 step:24062 [D loss: 0.255099, acc.: 89.06%] [G loss: 4.051819]\n",
      "epoch:30 step:24063 [D loss: 0.396198, acc.: 82.81%] [G loss: 3.136827]\n",
      "epoch:30 step:24064 [D loss: 0.363121, acc.: 83.59%] [G loss: 3.593775]\n",
      "epoch:30 step:24065 [D loss: 0.294524, acc.: 87.50%] [G loss: 4.300888]\n",
      "epoch:30 step:24066 [D loss: 0.300639, acc.: 85.16%] [G loss: 3.421553]\n",
      "epoch:30 step:24067 [D loss: 0.383225, acc.: 79.69%] [G loss: 3.934737]\n",
      "epoch:30 step:24068 [D loss: 0.284670, acc.: 89.06%] [G loss: 2.542342]\n",
      "epoch:30 step:24069 [D loss: 0.288350, acc.: 87.50%] [G loss: 3.244828]\n",
      "epoch:30 step:24070 [D loss: 0.320061, acc.: 84.38%] [G loss: 2.828578]\n",
      "epoch:30 step:24071 [D loss: 0.308186, acc.: 85.16%] [G loss: 3.290733]\n",
      "epoch:30 step:24072 [D loss: 0.300164, acc.: 87.50%] [G loss: 4.619312]\n",
      "epoch:30 step:24073 [D loss: 0.318279, acc.: 85.94%] [G loss: 2.809561]\n",
      "epoch:30 step:24074 [D loss: 0.270306, acc.: 87.50%] [G loss: 2.873954]\n",
      "epoch:30 step:24075 [D loss: 0.292368, acc.: 85.94%] [G loss: 3.073938]\n",
      "epoch:30 step:24076 [D loss: 0.596631, acc.: 69.53%] [G loss: 2.786434]\n",
      "epoch:30 step:24077 [D loss: 0.319856, acc.: 83.59%] [G loss: 2.979413]\n",
      "epoch:30 step:24078 [D loss: 0.366761, acc.: 85.16%] [G loss: 3.043633]\n",
      "epoch:30 step:24079 [D loss: 0.384899, acc.: 79.69%] [G loss: 3.148705]\n",
      "epoch:30 step:24080 [D loss: 0.332576, acc.: 85.94%] [G loss: 2.926895]\n",
      "epoch:30 step:24081 [D loss: 0.341253, acc.: 85.16%] [G loss: 3.265003]\n",
      "epoch:30 step:24082 [D loss: 0.301900, acc.: 88.28%] [G loss: 2.804116]\n",
      "epoch:30 step:24083 [D loss: 0.288743, acc.: 89.06%] [G loss: 3.527492]\n",
      "epoch:30 step:24084 [D loss: 0.296326, acc.: 85.94%] [G loss: 3.764282]\n",
      "epoch:30 step:24085 [D loss: 0.327432, acc.: 84.38%] [G loss: 2.677630]\n",
      "epoch:30 step:24086 [D loss: 0.220408, acc.: 90.62%] [G loss: 3.174053]\n",
      "epoch:30 step:24087 [D loss: 0.363698, acc.: 83.59%] [G loss: 2.729868]\n",
      "epoch:30 step:24088 [D loss: 0.411058, acc.: 80.47%] [G loss: 2.935781]\n",
      "epoch:30 step:24089 [D loss: 0.301125, acc.: 85.16%] [G loss: 3.287350]\n",
      "epoch:30 step:24090 [D loss: 0.406548, acc.: 78.12%] [G loss: 3.413771]\n",
      "epoch:30 step:24091 [D loss: 0.345893, acc.: 85.16%] [G loss: 3.403074]\n",
      "epoch:30 step:24092 [D loss: 0.347883, acc.: 82.81%] [G loss: 3.467762]\n",
      "epoch:30 step:24093 [D loss: 0.245524, acc.: 88.28%] [G loss: 3.947027]\n",
      "epoch:30 step:24094 [D loss: 0.350624, acc.: 82.03%] [G loss: 3.298142]\n",
      "epoch:30 step:24095 [D loss: 0.296926, acc.: 88.28%] [G loss: 3.017740]\n",
      "epoch:30 step:24096 [D loss: 0.334700, acc.: 84.38%] [G loss: 3.056537]\n",
      "epoch:30 step:24097 [D loss: 0.277441, acc.: 87.50%] [G loss: 2.673940]\n",
      "epoch:30 step:24098 [D loss: 0.313696, acc.: 84.38%] [G loss: 2.971069]\n",
      "epoch:30 step:24099 [D loss: 0.310206, acc.: 88.28%] [G loss: 2.777828]\n",
      "epoch:30 step:24100 [D loss: 0.342846, acc.: 84.38%] [G loss: 3.394329]\n",
      "epoch:30 step:24101 [D loss: 0.393290, acc.: 78.12%] [G loss: 2.810069]\n",
      "epoch:30 step:24102 [D loss: 0.291660, acc.: 86.72%] [G loss: 3.064191]\n",
      "epoch:30 step:24103 [D loss: 0.280297, acc.: 83.59%] [G loss: 4.077135]\n",
      "epoch:30 step:24104 [D loss: 0.335009, acc.: 83.59%] [G loss: 3.478164]\n",
      "epoch:30 step:24105 [D loss: 0.393400, acc.: 82.81%] [G loss: 3.127316]\n",
      "epoch:30 step:24106 [D loss: 0.344614, acc.: 80.47%] [G loss: 4.220673]\n",
      "epoch:30 step:24107 [D loss: 0.376597, acc.: 80.47%] [G loss: 4.084006]\n",
      "epoch:30 step:24108 [D loss: 0.309704, acc.: 83.59%] [G loss: 3.665002]\n",
      "epoch:30 step:24109 [D loss: 0.291633, acc.: 85.94%] [G loss: 3.033330]\n",
      "epoch:30 step:24110 [D loss: 0.339917, acc.: 85.94%] [G loss: 3.048686]\n",
      "epoch:30 step:24111 [D loss: 0.251126, acc.: 90.62%] [G loss: 3.300501]\n",
      "epoch:30 step:24112 [D loss: 0.356024, acc.: 83.59%] [G loss: 3.123253]\n",
      "epoch:30 step:24113 [D loss: 0.413487, acc.: 84.38%] [G loss: 6.074447]\n",
      "epoch:30 step:24114 [D loss: 0.653123, acc.: 69.53%] [G loss: 8.066368]\n",
      "epoch:30 step:24115 [D loss: 1.995322, acc.: 48.44%] [G loss: 6.723709]\n",
      "epoch:30 step:24116 [D loss: 1.274837, acc.: 71.88%] [G loss: 3.913060]\n",
      "epoch:30 step:24117 [D loss: 0.721321, acc.: 74.22%] [G loss: 4.886024]\n",
      "epoch:30 step:24118 [D loss: 0.693576, acc.: 73.44%] [G loss: 3.063174]\n",
      "epoch:30 step:24119 [D loss: 0.334312, acc.: 86.72%] [G loss: 3.189110]\n",
      "epoch:30 step:24120 [D loss: 0.336661, acc.: 85.16%] [G loss: 3.481431]\n",
      "epoch:30 step:24121 [D loss: 0.297648, acc.: 85.94%] [G loss: 3.374862]\n",
      "epoch:30 step:24122 [D loss: 0.432028, acc.: 78.91%] [G loss: 4.457048]\n",
      "epoch:30 step:24123 [D loss: 0.424109, acc.: 83.59%] [G loss: 3.931076]\n",
      "epoch:30 step:24124 [D loss: 0.446593, acc.: 79.69%] [G loss: 2.909371]\n",
      "epoch:30 step:24125 [D loss: 0.224526, acc.: 92.19%] [G loss: 3.263702]\n",
      "epoch:30 step:24126 [D loss: 0.379927, acc.: 82.81%] [G loss: 3.800868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:24127 [D loss: 0.355255, acc.: 83.59%] [G loss: 4.115732]\n",
      "epoch:30 step:24128 [D loss: 0.220642, acc.: 86.72%] [G loss: 6.942689]\n",
      "epoch:30 step:24129 [D loss: 0.259484, acc.: 88.28%] [G loss: 6.367287]\n",
      "epoch:30 step:24130 [D loss: 0.182392, acc.: 92.97%] [G loss: 8.980194]\n",
      "epoch:30 step:24131 [D loss: 0.258509, acc.: 86.72%] [G loss: 7.300657]\n",
      "epoch:30 step:24132 [D loss: 0.207217, acc.: 91.41%] [G loss: 5.268810]\n",
      "epoch:30 step:24133 [D loss: 0.314950, acc.: 82.03%] [G loss: 4.668105]\n",
      "epoch:30 step:24134 [D loss: 0.261565, acc.: 89.06%] [G loss: 4.465889]\n",
      "epoch:30 step:24135 [D loss: 0.300584, acc.: 89.06%] [G loss: 3.550247]\n",
      "epoch:30 step:24136 [D loss: 0.184564, acc.: 93.75%] [G loss: 4.154445]\n",
      "epoch:30 step:24137 [D loss: 0.419587, acc.: 81.25%] [G loss: 3.445674]\n",
      "epoch:30 step:24138 [D loss: 0.287492, acc.: 86.72%] [G loss: 3.250035]\n",
      "epoch:30 step:24139 [D loss: 0.396422, acc.: 80.47%] [G loss: 3.183129]\n",
      "epoch:30 step:24140 [D loss: 0.302728, acc.: 84.38%] [G loss: 3.705285]\n",
      "epoch:30 step:24141 [D loss: 0.340107, acc.: 86.72%] [G loss: 3.744657]\n",
      "epoch:30 step:24142 [D loss: 0.281254, acc.: 86.72%] [G loss: 3.098449]\n",
      "epoch:30 step:24143 [D loss: 0.371108, acc.: 82.03%] [G loss: 2.765303]\n",
      "epoch:30 step:24144 [D loss: 0.350703, acc.: 82.81%] [G loss: 2.620981]\n",
      "epoch:30 step:24145 [D loss: 0.397699, acc.: 79.69%] [G loss: 2.318607]\n",
      "epoch:30 step:24146 [D loss: 0.387250, acc.: 82.03%] [G loss: 2.650451]\n",
      "epoch:30 step:24147 [D loss: 0.432361, acc.: 82.03%] [G loss: 3.544713]\n",
      "epoch:30 step:24148 [D loss: 0.314846, acc.: 86.72%] [G loss: 2.664120]\n",
      "epoch:30 step:24149 [D loss: 0.242172, acc.: 88.28%] [G loss: 3.088259]\n",
      "epoch:30 step:24150 [D loss: 0.351266, acc.: 84.38%] [G loss: 2.659999]\n",
      "epoch:30 step:24151 [D loss: 0.328568, acc.: 85.16%] [G loss: 2.964422]\n",
      "epoch:30 step:24152 [D loss: 0.301405, acc.: 88.28%] [G loss: 2.658077]\n",
      "epoch:30 step:24153 [D loss: 0.322630, acc.: 87.50%] [G loss: 2.276198]\n",
      "epoch:30 step:24154 [D loss: 0.314934, acc.: 84.38%] [G loss: 3.276499]\n",
      "epoch:30 step:24155 [D loss: 0.341593, acc.: 88.28%] [G loss: 2.913750]\n",
      "epoch:30 step:24156 [D loss: 0.281993, acc.: 86.72%] [G loss: 3.264473]\n",
      "epoch:30 step:24157 [D loss: 0.293968, acc.: 87.50%] [G loss: 3.886746]\n",
      "epoch:30 step:24158 [D loss: 0.365410, acc.: 80.47%] [G loss: 3.034334]\n",
      "epoch:30 step:24159 [D loss: 0.252227, acc.: 91.41%] [G loss: 3.206347]\n",
      "epoch:30 step:24160 [D loss: 0.279041, acc.: 89.06%] [G loss: 3.697921]\n",
      "epoch:30 step:24161 [D loss: 0.375206, acc.: 82.03%] [G loss: 2.968323]\n",
      "epoch:30 step:24162 [D loss: 0.383786, acc.: 84.38%] [G loss: 4.155292]\n",
      "epoch:30 step:24163 [D loss: 0.297177, acc.: 89.06%] [G loss: 3.472077]\n",
      "epoch:30 step:24164 [D loss: 0.381410, acc.: 79.69%] [G loss: 2.851505]\n",
      "epoch:30 step:24165 [D loss: 0.331388, acc.: 84.38%] [G loss: 3.297425]\n",
      "epoch:30 step:24166 [D loss: 0.309517, acc.: 87.50%] [G loss: 3.419139]\n",
      "epoch:30 step:24167 [D loss: 0.354086, acc.: 84.38%] [G loss: 2.616535]\n",
      "epoch:30 step:24168 [D loss: 0.342042, acc.: 85.16%] [G loss: 3.331001]\n",
      "epoch:30 step:24169 [D loss: 0.367068, acc.: 82.81%] [G loss: 4.560764]\n",
      "epoch:30 step:24170 [D loss: 0.259010, acc.: 89.06%] [G loss: 3.941160]\n",
      "epoch:30 step:24171 [D loss: 0.310365, acc.: 89.84%] [G loss: 3.572131]\n",
      "epoch:30 step:24172 [D loss: 0.309923, acc.: 85.16%] [G loss: 2.291904]\n",
      "epoch:30 step:24173 [D loss: 0.367228, acc.: 82.03%] [G loss: 2.690391]\n",
      "epoch:30 step:24174 [D loss: 0.288873, acc.: 88.28%] [G loss: 2.415908]\n",
      "epoch:30 step:24175 [D loss: 0.337980, acc.: 83.59%] [G loss: 3.183534]\n",
      "epoch:30 step:24176 [D loss: 0.364682, acc.: 85.94%] [G loss: 3.002852]\n",
      "epoch:30 step:24177 [D loss: 0.310514, acc.: 90.62%] [G loss: 2.432101]\n",
      "epoch:30 step:24178 [D loss: 0.301648, acc.: 85.94%] [G loss: 2.966803]\n",
      "epoch:30 step:24179 [D loss: 0.313598, acc.: 83.59%] [G loss: 2.184531]\n",
      "epoch:30 step:24180 [D loss: 0.371746, acc.: 82.03%] [G loss: 3.105131]\n",
      "epoch:30 step:24181 [D loss: 0.366756, acc.: 84.38%] [G loss: 3.260147]\n",
      "epoch:30 step:24182 [D loss: 0.304256, acc.: 88.28%] [G loss: 4.231588]\n",
      "epoch:30 step:24183 [D loss: 0.392011, acc.: 78.12%] [G loss: 3.519527]\n",
      "epoch:30 step:24184 [D loss: 0.391031, acc.: 82.81%] [G loss: 2.885978]\n",
      "epoch:30 step:24185 [D loss: 0.285509, acc.: 88.28%] [G loss: 2.983816]\n",
      "epoch:30 step:24186 [D loss: 0.302754, acc.: 85.94%] [G loss: 2.709399]\n",
      "epoch:30 step:24187 [D loss: 0.320967, acc.: 89.84%] [G loss: 2.876180]\n",
      "epoch:30 step:24188 [D loss: 0.371505, acc.: 79.69%] [G loss: 2.630757]\n",
      "epoch:30 step:24189 [D loss: 0.366680, acc.: 82.81%] [G loss: 2.960833]\n",
      "epoch:30 step:24190 [D loss: 0.325154, acc.: 86.72%] [G loss: 2.735020]\n",
      "epoch:30 step:24191 [D loss: 0.260792, acc.: 85.94%] [G loss: 2.887784]\n",
      "epoch:30 step:24192 [D loss: 0.402007, acc.: 79.69%] [G loss: 2.652430]\n",
      "epoch:30 step:24193 [D loss: 0.330108, acc.: 86.72%] [G loss: 2.705190]\n",
      "epoch:30 step:24194 [D loss: 0.314168, acc.: 87.50%] [G loss: 2.527325]\n",
      "epoch:30 step:24195 [D loss: 0.440231, acc.: 81.25%] [G loss: 2.674732]\n",
      "epoch:30 step:24196 [D loss: 0.345866, acc.: 85.16%] [G loss: 2.624176]\n",
      "epoch:30 step:24197 [D loss: 0.234945, acc.: 89.06%] [G loss: 3.933150]\n",
      "epoch:30 step:24198 [D loss: 0.308173, acc.: 86.72%] [G loss: 3.583860]\n",
      "epoch:30 step:24199 [D loss: 0.358329, acc.: 84.38%] [G loss: 3.615975]\n",
      "epoch:30 step:24200 [D loss: 0.304194, acc.: 88.28%] [G loss: 2.876076]\n",
      "epoch:30 step:24201 [D loss: 0.375609, acc.: 80.47%] [G loss: 4.374975]\n",
      "epoch:30 step:24202 [D loss: 0.325207, acc.: 85.16%] [G loss: 2.960325]\n",
      "epoch:30 step:24203 [D loss: 0.389208, acc.: 84.38%] [G loss: 3.196446]\n",
      "epoch:30 step:24204 [D loss: 0.225001, acc.: 93.75%] [G loss: 6.579425]\n",
      "epoch:30 step:24205 [D loss: 0.223115, acc.: 88.28%] [G loss: 5.905656]\n",
      "epoch:30 step:24206 [D loss: 0.242165, acc.: 89.84%] [G loss: 3.785129]\n",
      "epoch:30 step:24207 [D loss: 0.349285, acc.: 81.25%] [G loss: 4.205637]\n",
      "epoch:30 step:24208 [D loss: 0.290485, acc.: 87.50%] [G loss: 3.202694]\n",
      "epoch:30 step:24209 [D loss: 0.429852, acc.: 83.59%] [G loss: 3.136467]\n",
      "epoch:30 step:24210 [D loss: 0.315423, acc.: 88.28%] [G loss: 4.678175]\n",
      "epoch:30 step:24211 [D loss: 0.441583, acc.: 81.25%] [G loss: 3.251817]\n",
      "epoch:31 step:24212 [D loss: 0.394038, acc.: 81.25%] [G loss: 2.899185]\n",
      "epoch:31 step:24213 [D loss: 0.291722, acc.: 85.16%] [G loss: 3.523213]\n",
      "epoch:31 step:24214 [D loss: 0.495112, acc.: 75.78%] [G loss: 3.594557]\n",
      "epoch:31 step:24215 [D loss: 0.474175, acc.: 78.91%] [G loss: 2.973855]\n",
      "epoch:31 step:24216 [D loss: 0.243891, acc.: 89.06%] [G loss: 3.970338]\n",
      "epoch:31 step:24217 [D loss: 0.392354, acc.: 84.38%] [G loss: 4.794545]\n",
      "epoch:31 step:24218 [D loss: 0.389904, acc.: 78.12%] [G loss: 2.861802]\n",
      "epoch:31 step:24219 [D loss: 0.188195, acc.: 92.97%] [G loss: 3.133233]\n",
      "epoch:31 step:24220 [D loss: 0.365465, acc.: 83.59%] [G loss: 2.581347]\n",
      "epoch:31 step:24221 [D loss: 0.331073, acc.: 83.59%] [G loss: 2.861565]\n",
      "epoch:31 step:24222 [D loss: 0.307318, acc.: 85.16%] [G loss: 2.985218]\n",
      "epoch:31 step:24223 [D loss: 0.300454, acc.: 85.94%] [G loss: 3.432973]\n",
      "epoch:31 step:24224 [D loss: 0.363547, acc.: 87.50%] [G loss: 3.545202]\n",
      "epoch:31 step:24225 [D loss: 0.348271, acc.: 82.81%] [G loss: 3.750037]\n",
      "epoch:31 step:24226 [D loss: 0.325914, acc.: 85.16%] [G loss: 3.855773]\n",
      "epoch:31 step:24227 [D loss: 0.371044, acc.: 85.16%] [G loss: 3.132427]\n",
      "epoch:31 step:24228 [D loss: 0.252995, acc.: 91.41%] [G loss: 3.672691]\n",
      "epoch:31 step:24229 [D loss: 0.294448, acc.: 84.38%] [G loss: 3.107120]\n",
      "epoch:31 step:24230 [D loss: 0.266634, acc.: 89.06%] [G loss: 4.159970]\n",
      "epoch:31 step:24231 [D loss: 0.297097, acc.: 87.50%] [G loss: 3.222759]\n",
      "epoch:31 step:24232 [D loss: 0.276196, acc.: 88.28%] [G loss: 2.994342]\n",
      "epoch:31 step:24233 [D loss: 0.404761, acc.: 82.03%] [G loss: 2.899163]\n",
      "epoch:31 step:24234 [D loss: 0.280905, acc.: 85.94%] [G loss: 3.875475]\n",
      "epoch:31 step:24235 [D loss: 0.358377, acc.: 84.38%] [G loss: 2.418244]\n",
      "epoch:31 step:24236 [D loss: 0.390902, acc.: 80.47%] [G loss: 3.350210]\n",
      "epoch:31 step:24237 [D loss: 0.366531, acc.: 85.16%] [G loss: 3.101687]\n",
      "epoch:31 step:24238 [D loss: 0.358797, acc.: 79.69%] [G loss: 3.442378]\n",
      "epoch:31 step:24239 [D loss: 0.335067, acc.: 81.25%] [G loss: 3.056922]\n",
      "epoch:31 step:24240 [D loss: 0.335113, acc.: 85.16%] [G loss: 2.808100]\n",
      "epoch:31 step:24241 [D loss: 0.227867, acc.: 91.41%] [G loss: 3.367647]\n",
      "epoch:31 step:24242 [D loss: 0.257003, acc.: 85.94%] [G loss: 5.253756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24243 [D loss: 0.284788, acc.: 83.59%] [G loss: 3.104261]\n",
      "epoch:31 step:24244 [D loss: 0.292723, acc.: 86.72%] [G loss: 2.853431]\n",
      "epoch:31 step:24245 [D loss: 0.292251, acc.: 85.94%] [G loss: 3.707603]\n",
      "epoch:31 step:24246 [D loss: 0.332780, acc.: 85.16%] [G loss: 3.566472]\n",
      "epoch:31 step:24247 [D loss: 0.333051, acc.: 83.59%] [G loss: 2.821946]\n",
      "epoch:31 step:24248 [D loss: 0.319117, acc.: 84.38%] [G loss: 5.439078]\n",
      "epoch:31 step:24249 [D loss: 0.501514, acc.: 78.91%] [G loss: 5.554626]\n",
      "epoch:31 step:24250 [D loss: 0.845223, acc.: 70.31%] [G loss: 7.313896]\n",
      "epoch:31 step:24251 [D loss: 1.275320, acc.: 64.84%] [G loss: 4.180044]\n",
      "epoch:31 step:24252 [D loss: 0.415881, acc.: 83.59%] [G loss: 6.724889]\n",
      "epoch:31 step:24253 [D loss: 0.514968, acc.: 77.34%] [G loss: 2.835654]\n",
      "epoch:31 step:24254 [D loss: 0.322293, acc.: 85.94%] [G loss: 3.979795]\n",
      "epoch:31 step:24255 [D loss: 0.335771, acc.: 85.94%] [G loss: 2.872958]\n",
      "epoch:31 step:24256 [D loss: 0.292832, acc.: 88.28%] [G loss: 3.176417]\n",
      "epoch:31 step:24257 [D loss: 0.443251, acc.: 81.25%] [G loss: 2.862227]\n",
      "epoch:31 step:24258 [D loss: 0.435817, acc.: 75.78%] [G loss: 2.668302]\n",
      "epoch:31 step:24259 [D loss: 0.310821, acc.: 89.06%] [G loss: 2.827759]\n",
      "epoch:31 step:24260 [D loss: 0.280743, acc.: 90.62%] [G loss: 3.111482]\n",
      "epoch:31 step:24261 [D loss: 0.370831, acc.: 86.72%] [G loss: 3.429074]\n",
      "epoch:31 step:24262 [D loss: 0.384046, acc.: 82.03%] [G loss: 2.802962]\n",
      "epoch:31 step:24263 [D loss: 0.244767, acc.: 88.28%] [G loss: 2.735636]\n",
      "epoch:31 step:24264 [D loss: 0.309898, acc.: 82.81%] [G loss: 3.247917]\n",
      "epoch:31 step:24265 [D loss: 0.463194, acc.: 76.56%] [G loss: 2.763338]\n",
      "epoch:31 step:24266 [D loss: 0.278716, acc.: 84.38%] [G loss: 3.838485]\n",
      "epoch:31 step:24267 [D loss: 0.323125, acc.: 85.94%] [G loss: 2.657028]\n",
      "epoch:31 step:24268 [D loss: 0.338169, acc.: 81.25%] [G loss: 2.881819]\n",
      "epoch:31 step:24269 [D loss: 0.253170, acc.: 90.62%] [G loss: 3.544834]\n",
      "epoch:31 step:24270 [D loss: 0.275684, acc.: 87.50%] [G loss: 3.357178]\n",
      "epoch:31 step:24271 [D loss: 0.290379, acc.: 85.16%] [G loss: 4.005060]\n",
      "epoch:31 step:24272 [D loss: 0.312120, acc.: 85.94%] [G loss: 3.202604]\n",
      "epoch:31 step:24273 [D loss: 0.389781, acc.: 83.59%] [G loss: 3.569960]\n",
      "epoch:31 step:24274 [D loss: 0.261153, acc.: 89.06%] [G loss: 3.135717]\n",
      "epoch:31 step:24275 [D loss: 0.333768, acc.: 83.59%] [G loss: 2.758570]\n",
      "epoch:31 step:24276 [D loss: 0.358578, acc.: 79.69%] [G loss: 2.421422]\n",
      "epoch:31 step:24277 [D loss: 0.435882, acc.: 78.12%] [G loss: 2.777391]\n",
      "epoch:31 step:24278 [D loss: 0.261247, acc.: 89.84%] [G loss: 3.224490]\n",
      "epoch:31 step:24279 [D loss: 0.351234, acc.: 82.81%] [G loss: 2.528719]\n",
      "epoch:31 step:24280 [D loss: 0.276771, acc.: 89.84%] [G loss: 3.341630]\n",
      "epoch:31 step:24281 [D loss: 0.340857, acc.: 83.59%] [G loss: 3.313606]\n",
      "epoch:31 step:24282 [D loss: 0.393886, acc.: 78.91%] [G loss: 3.700599]\n",
      "epoch:31 step:24283 [D loss: 0.343046, acc.: 82.03%] [G loss: 2.970991]\n",
      "epoch:31 step:24284 [D loss: 0.354827, acc.: 85.16%] [G loss: 2.229531]\n",
      "epoch:31 step:24285 [D loss: 0.223097, acc.: 91.41%] [G loss: 2.632663]\n",
      "epoch:31 step:24286 [D loss: 0.291236, acc.: 85.16%] [G loss: 2.621330]\n",
      "epoch:31 step:24287 [D loss: 0.271814, acc.: 92.19%] [G loss: 2.844925]\n",
      "epoch:31 step:24288 [D loss: 0.369759, acc.: 83.59%] [G loss: 2.953820]\n",
      "epoch:31 step:24289 [D loss: 0.264539, acc.: 89.84%] [G loss: 2.587074]\n",
      "epoch:31 step:24290 [D loss: 0.287168, acc.: 89.84%] [G loss: 3.588768]\n",
      "epoch:31 step:24291 [D loss: 0.292310, acc.: 85.94%] [G loss: 2.902482]\n",
      "epoch:31 step:24292 [D loss: 0.376051, acc.: 78.91%] [G loss: 3.645430]\n",
      "epoch:31 step:24293 [D loss: 0.294222, acc.: 86.72%] [G loss: 2.862772]\n",
      "epoch:31 step:24294 [D loss: 0.291518, acc.: 88.28%] [G loss: 3.524510]\n",
      "epoch:31 step:24295 [D loss: 0.283401, acc.: 87.50%] [G loss: 2.908544]\n",
      "epoch:31 step:24296 [D loss: 0.339818, acc.: 85.16%] [G loss: 2.834934]\n",
      "epoch:31 step:24297 [D loss: 0.306485, acc.: 85.16%] [G loss: 3.461066]\n",
      "epoch:31 step:24298 [D loss: 0.348744, acc.: 82.81%] [G loss: 2.856291]\n",
      "epoch:31 step:24299 [D loss: 0.233180, acc.: 93.75%] [G loss: 2.636438]\n",
      "epoch:31 step:24300 [D loss: 0.369307, acc.: 82.81%] [G loss: 3.064797]\n",
      "epoch:31 step:24301 [D loss: 0.568309, acc.: 71.88%] [G loss: 2.408401]\n",
      "epoch:31 step:24302 [D loss: 0.247840, acc.: 91.41%] [G loss: 3.010855]\n",
      "epoch:31 step:24303 [D loss: 0.327656, acc.: 84.38%] [G loss: 2.593799]\n",
      "epoch:31 step:24304 [D loss: 0.372998, acc.: 85.16%] [G loss: 2.464876]\n",
      "epoch:31 step:24305 [D loss: 0.371695, acc.: 78.91%] [G loss: 3.685575]\n",
      "epoch:31 step:24306 [D loss: 0.431605, acc.: 83.59%] [G loss: 4.103459]\n",
      "epoch:31 step:24307 [D loss: 0.337965, acc.: 84.38%] [G loss: 3.546261]\n",
      "epoch:31 step:24308 [D loss: 0.280158, acc.: 85.94%] [G loss: 4.554685]\n",
      "epoch:31 step:24309 [D loss: 0.279722, acc.: 90.62%] [G loss: 5.776124]\n",
      "epoch:31 step:24310 [D loss: 0.298800, acc.: 88.28%] [G loss: 4.751190]\n",
      "epoch:31 step:24311 [D loss: 0.339086, acc.: 85.16%] [G loss: 4.857060]\n",
      "epoch:31 step:24312 [D loss: 0.384979, acc.: 80.47%] [G loss: 2.688435]\n",
      "epoch:31 step:24313 [D loss: 0.301081, acc.: 85.94%] [G loss: 4.125753]\n",
      "epoch:31 step:24314 [D loss: 0.301634, acc.: 84.38%] [G loss: 3.090697]\n",
      "epoch:31 step:24315 [D loss: 0.322112, acc.: 85.94%] [G loss: 2.789811]\n",
      "epoch:31 step:24316 [D loss: 0.259106, acc.: 85.94%] [G loss: 2.942693]\n",
      "epoch:31 step:24317 [D loss: 0.391779, acc.: 81.25%] [G loss: 3.326880]\n",
      "epoch:31 step:24318 [D loss: 0.240532, acc.: 92.19%] [G loss: 3.525386]\n",
      "epoch:31 step:24319 [D loss: 0.179708, acc.: 92.19%] [G loss: 4.364092]\n",
      "epoch:31 step:24320 [D loss: 0.327543, acc.: 85.94%] [G loss: 4.275426]\n",
      "epoch:31 step:24321 [D loss: 0.176586, acc.: 93.75%] [G loss: 5.918176]\n",
      "epoch:31 step:24322 [D loss: 0.234465, acc.: 92.19%] [G loss: 2.961863]\n",
      "epoch:31 step:24323 [D loss: 0.281236, acc.: 87.50%] [G loss: 4.324867]\n",
      "epoch:31 step:24324 [D loss: 0.235225, acc.: 89.84%] [G loss: 3.371939]\n",
      "epoch:31 step:24325 [D loss: 0.360685, acc.: 81.25%] [G loss: 5.519962]\n",
      "epoch:31 step:24326 [D loss: 0.284959, acc.: 88.28%] [G loss: 4.327785]\n",
      "epoch:31 step:24327 [D loss: 0.424255, acc.: 81.25%] [G loss: 5.205370]\n",
      "epoch:31 step:24328 [D loss: 0.263630, acc.: 85.16%] [G loss: 4.521924]\n",
      "epoch:31 step:24329 [D loss: 0.362126, acc.: 82.03%] [G loss: 4.469997]\n",
      "epoch:31 step:24330 [D loss: 0.224864, acc.: 91.41%] [G loss: 4.225416]\n",
      "epoch:31 step:24331 [D loss: 0.262440, acc.: 88.28%] [G loss: 2.600922]\n",
      "epoch:31 step:24332 [D loss: 0.335699, acc.: 85.16%] [G loss: 2.805537]\n",
      "epoch:31 step:24333 [D loss: 0.267752, acc.: 90.62%] [G loss: 2.985403]\n",
      "epoch:31 step:24334 [D loss: 0.220586, acc.: 91.41%] [G loss: 4.196814]\n",
      "epoch:31 step:24335 [D loss: 0.358308, acc.: 85.94%] [G loss: 4.783483]\n",
      "epoch:31 step:24336 [D loss: 0.412956, acc.: 82.03%] [G loss: 5.066483]\n",
      "epoch:31 step:24337 [D loss: 0.316159, acc.: 87.50%] [G loss: 3.373207]\n",
      "epoch:31 step:24338 [D loss: 0.287646, acc.: 86.72%] [G loss: 3.019556]\n",
      "epoch:31 step:24339 [D loss: 0.276697, acc.: 90.62%] [G loss: 2.412582]\n",
      "epoch:31 step:24340 [D loss: 0.361423, acc.: 85.16%] [G loss: 2.839677]\n",
      "epoch:31 step:24341 [D loss: 0.310939, acc.: 88.28%] [G loss: 2.774727]\n",
      "epoch:31 step:24342 [D loss: 0.325218, acc.: 85.16%] [G loss: 3.008542]\n",
      "epoch:31 step:24343 [D loss: 0.405236, acc.: 84.38%] [G loss: 3.096139]\n",
      "epoch:31 step:24344 [D loss: 0.325563, acc.: 84.38%] [G loss: 3.250144]\n",
      "epoch:31 step:24345 [D loss: 0.276212, acc.: 87.50%] [G loss: 3.399561]\n",
      "epoch:31 step:24346 [D loss: 0.313731, acc.: 85.16%] [G loss: 3.463388]\n",
      "epoch:31 step:24347 [D loss: 0.367333, acc.: 82.81%] [G loss: 4.901614]\n",
      "epoch:31 step:24348 [D loss: 0.276994, acc.: 86.72%] [G loss: 4.443122]\n",
      "epoch:31 step:24349 [D loss: 0.306433, acc.: 88.28%] [G loss: 4.458100]\n",
      "epoch:31 step:24350 [D loss: 0.248242, acc.: 89.84%] [G loss: 3.619994]\n",
      "epoch:31 step:24351 [D loss: 0.304864, acc.: 85.16%] [G loss: 4.232795]\n",
      "epoch:31 step:24352 [D loss: 0.272745, acc.: 89.84%] [G loss: 3.851889]\n",
      "epoch:31 step:24353 [D loss: 0.341680, acc.: 83.59%] [G loss: 3.327835]\n",
      "epoch:31 step:24354 [D loss: 0.249229, acc.: 90.62%] [G loss: 3.881546]\n",
      "epoch:31 step:24355 [D loss: 0.482047, acc.: 78.91%] [G loss: 3.690548]\n",
      "epoch:31 step:24356 [D loss: 0.452705, acc.: 81.25%] [G loss: 2.413589]\n",
      "epoch:31 step:24357 [D loss: 0.308537, acc.: 88.28%] [G loss: 4.006389]\n",
      "epoch:31 step:24358 [D loss: 0.325545, acc.: 84.38%] [G loss: 4.390841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24359 [D loss: 0.306759, acc.: 85.94%] [G loss: 4.664994]\n",
      "epoch:31 step:24360 [D loss: 0.274762, acc.: 86.72%] [G loss: 3.049129]\n",
      "epoch:31 step:24361 [D loss: 0.323640, acc.: 89.06%] [G loss: 3.390604]\n",
      "epoch:31 step:24362 [D loss: 0.314517, acc.: 89.06%] [G loss: 3.246532]\n",
      "epoch:31 step:24363 [D loss: 0.437212, acc.: 77.34%] [G loss: 2.082254]\n",
      "epoch:31 step:24364 [D loss: 0.266930, acc.: 89.84%] [G loss: 2.715367]\n",
      "epoch:31 step:24365 [D loss: 0.352468, acc.: 82.81%] [G loss: 2.609871]\n",
      "epoch:31 step:24366 [D loss: 0.301669, acc.: 88.28%] [G loss: 2.931124]\n",
      "epoch:31 step:24367 [D loss: 0.359585, acc.: 82.03%] [G loss: 3.344976]\n",
      "epoch:31 step:24368 [D loss: 0.328877, acc.: 82.81%] [G loss: 2.521716]\n",
      "epoch:31 step:24369 [D loss: 0.245589, acc.: 92.19%] [G loss: 2.776857]\n",
      "epoch:31 step:24370 [D loss: 0.218668, acc.: 90.62%] [G loss: 2.002002]\n",
      "epoch:31 step:24371 [D loss: 0.475293, acc.: 74.22%] [G loss: 3.253430]\n",
      "epoch:31 step:24372 [D loss: 0.328463, acc.: 81.25%] [G loss: 3.333935]\n",
      "epoch:31 step:24373 [D loss: 0.223120, acc.: 90.62%] [G loss: 3.496374]\n",
      "epoch:31 step:24374 [D loss: 0.350204, acc.: 82.03%] [G loss: 4.199033]\n",
      "epoch:31 step:24375 [D loss: 0.338806, acc.: 84.38%] [G loss: 3.191560]\n",
      "epoch:31 step:24376 [D loss: 0.323541, acc.: 82.03%] [G loss: 2.663671]\n",
      "epoch:31 step:24377 [D loss: 0.426979, acc.: 85.16%] [G loss: 2.641625]\n",
      "epoch:31 step:24378 [D loss: 0.434304, acc.: 83.59%] [G loss: 4.319489]\n",
      "epoch:31 step:24379 [D loss: 0.585483, acc.: 77.34%] [G loss: 7.063500]\n",
      "epoch:31 step:24380 [D loss: 0.733169, acc.: 72.66%] [G loss: 5.076962]\n",
      "epoch:31 step:24381 [D loss: 0.684174, acc.: 72.66%] [G loss: 2.846771]\n",
      "epoch:31 step:24382 [D loss: 0.255790, acc.: 88.28%] [G loss: 4.753185]\n",
      "epoch:31 step:24383 [D loss: 0.276816, acc.: 90.62%] [G loss: 2.657804]\n",
      "epoch:31 step:24384 [D loss: 0.307919, acc.: 88.28%] [G loss: 3.427716]\n",
      "epoch:31 step:24385 [D loss: 0.226995, acc.: 93.75%] [G loss: 2.869089]\n",
      "epoch:31 step:24386 [D loss: 0.304152, acc.: 87.50%] [G loss: 2.796146]\n",
      "epoch:31 step:24387 [D loss: 0.304602, acc.: 85.94%] [G loss: 3.118169]\n",
      "epoch:31 step:24388 [D loss: 0.330474, acc.: 84.38%] [G loss: 2.615531]\n",
      "epoch:31 step:24389 [D loss: 0.423669, acc.: 78.91%] [G loss: 2.880375]\n",
      "epoch:31 step:24390 [D loss: 0.415871, acc.: 81.25%] [G loss: 2.566913]\n",
      "epoch:31 step:24391 [D loss: 0.536806, acc.: 77.34%] [G loss: 1.902598]\n",
      "epoch:31 step:24392 [D loss: 0.307578, acc.: 85.16%] [G loss: 2.531776]\n",
      "epoch:31 step:24393 [D loss: 0.374011, acc.: 81.25%] [G loss: 2.546477]\n",
      "epoch:31 step:24394 [D loss: 0.302675, acc.: 88.28%] [G loss: 2.338002]\n",
      "epoch:31 step:24395 [D loss: 0.466422, acc.: 76.56%] [G loss: 2.397443]\n",
      "epoch:31 step:24396 [D loss: 0.292289, acc.: 89.06%] [G loss: 2.764080]\n",
      "epoch:31 step:24397 [D loss: 0.286001, acc.: 89.84%] [G loss: 2.060104]\n",
      "epoch:31 step:24398 [D loss: 0.341299, acc.: 85.16%] [G loss: 2.818406]\n",
      "epoch:31 step:24399 [D loss: 0.354638, acc.: 82.03%] [G loss: 1.959998]\n",
      "epoch:31 step:24400 [D loss: 0.335084, acc.: 86.72%] [G loss: 3.008916]\n",
      "epoch:31 step:24401 [D loss: 0.363007, acc.: 82.03%] [G loss: 2.836693]\n",
      "epoch:31 step:24402 [D loss: 0.335580, acc.: 84.38%] [G loss: 2.736613]\n",
      "epoch:31 step:24403 [D loss: 0.324848, acc.: 85.16%] [G loss: 2.903872]\n",
      "epoch:31 step:24404 [D loss: 0.380746, acc.: 82.81%] [G loss: 2.539124]\n",
      "epoch:31 step:24405 [D loss: 0.390305, acc.: 82.03%] [G loss: 2.507572]\n",
      "epoch:31 step:24406 [D loss: 0.286669, acc.: 86.72%] [G loss: 3.404377]\n",
      "epoch:31 step:24407 [D loss: 0.220154, acc.: 90.62%] [G loss: 3.667745]\n",
      "epoch:31 step:24408 [D loss: 0.373753, acc.: 81.25%] [G loss: 3.677568]\n",
      "epoch:31 step:24409 [D loss: 0.324136, acc.: 86.72%] [G loss: 2.813002]\n",
      "epoch:31 step:24410 [D loss: 0.299622, acc.: 84.38%] [G loss: 4.033249]\n",
      "epoch:31 step:24411 [D loss: 0.337319, acc.: 85.94%] [G loss: 3.135068]\n",
      "epoch:31 step:24412 [D loss: 0.401871, acc.: 82.03%] [G loss: 3.904672]\n",
      "epoch:31 step:24413 [D loss: 0.270236, acc.: 86.72%] [G loss: 5.910279]\n",
      "epoch:31 step:24414 [D loss: 0.323679, acc.: 84.38%] [G loss: 3.815508]\n",
      "epoch:31 step:24415 [D loss: 0.290038, acc.: 88.28%] [G loss: 3.354226]\n",
      "epoch:31 step:24416 [D loss: 0.267824, acc.: 89.84%] [G loss: 3.339762]\n",
      "epoch:31 step:24417 [D loss: 0.297708, acc.: 86.72%] [G loss: 2.784816]\n",
      "epoch:31 step:24418 [D loss: 0.379701, acc.: 80.47%] [G loss: 2.903005]\n",
      "epoch:31 step:24419 [D loss: 0.314306, acc.: 85.16%] [G loss: 2.790753]\n",
      "epoch:31 step:24420 [D loss: 0.297272, acc.: 86.72%] [G loss: 2.887509]\n",
      "epoch:31 step:24421 [D loss: 0.343875, acc.: 82.03%] [G loss: 2.672876]\n",
      "epoch:31 step:24422 [D loss: 0.394646, acc.: 85.16%] [G loss: 3.690170]\n",
      "epoch:31 step:24423 [D loss: 0.291575, acc.: 89.06%] [G loss: 2.468047]\n",
      "epoch:31 step:24424 [D loss: 0.319937, acc.: 84.38%] [G loss: 2.874787]\n",
      "epoch:31 step:24425 [D loss: 0.352916, acc.: 78.91%] [G loss: 3.693933]\n",
      "epoch:31 step:24426 [D loss: 0.248685, acc.: 88.28%] [G loss: 3.935022]\n",
      "epoch:31 step:24427 [D loss: 0.355510, acc.: 84.38%] [G loss: 3.087047]\n",
      "epoch:31 step:24428 [D loss: 0.332188, acc.: 84.38%] [G loss: 4.715315]\n",
      "epoch:31 step:24429 [D loss: 0.281645, acc.: 89.06%] [G loss: 2.846586]\n",
      "epoch:31 step:24430 [D loss: 0.365582, acc.: 82.81%] [G loss: 3.223859]\n",
      "epoch:31 step:24431 [D loss: 0.323427, acc.: 88.28%] [G loss: 3.314228]\n",
      "epoch:31 step:24432 [D loss: 0.303715, acc.: 88.28%] [G loss: 2.277734]\n",
      "epoch:31 step:24433 [D loss: 0.459466, acc.: 78.12%] [G loss: 2.946631]\n",
      "epoch:31 step:24434 [D loss: 0.328638, acc.: 84.38%] [G loss: 4.420419]\n",
      "epoch:31 step:24435 [D loss: 0.328243, acc.: 85.16%] [G loss: 2.541109]\n",
      "epoch:31 step:24436 [D loss: 0.330288, acc.: 81.25%] [G loss: 2.826636]\n",
      "epoch:31 step:24437 [D loss: 0.326983, acc.: 82.03%] [G loss: 2.848486]\n",
      "epoch:31 step:24438 [D loss: 0.330975, acc.: 85.16%] [G loss: 2.749763]\n",
      "epoch:31 step:24439 [D loss: 0.310903, acc.: 88.28%] [G loss: 3.972396]\n",
      "epoch:31 step:24440 [D loss: 0.433232, acc.: 78.91%] [G loss: 4.461686]\n",
      "epoch:31 step:24441 [D loss: 0.316386, acc.: 83.59%] [G loss: 3.722677]\n",
      "epoch:31 step:24442 [D loss: 0.361159, acc.: 85.16%] [G loss: 4.130124]\n",
      "epoch:31 step:24443 [D loss: 0.234029, acc.: 89.84%] [G loss: 3.868714]\n",
      "epoch:31 step:24444 [D loss: 0.274485, acc.: 84.38%] [G loss: 3.928944]\n",
      "epoch:31 step:24445 [D loss: 0.339344, acc.: 83.59%] [G loss: 4.464549]\n",
      "epoch:31 step:24446 [D loss: 0.252757, acc.: 89.84%] [G loss: 5.765231]\n",
      "epoch:31 step:24447 [D loss: 0.289735, acc.: 85.94%] [G loss: 3.784910]\n",
      "epoch:31 step:24448 [D loss: 0.267661, acc.: 86.72%] [G loss: 6.174778]\n",
      "epoch:31 step:24449 [D loss: 0.262229, acc.: 86.72%] [G loss: 3.501373]\n",
      "epoch:31 step:24450 [D loss: 0.211198, acc.: 90.62%] [G loss: 4.207874]\n",
      "epoch:31 step:24451 [D loss: 0.333290, acc.: 85.94%] [G loss: 3.485033]\n",
      "epoch:31 step:24452 [D loss: 0.301876, acc.: 87.50%] [G loss: 4.502659]\n",
      "epoch:31 step:24453 [D loss: 0.226135, acc.: 91.41%] [G loss: 3.094138]\n",
      "epoch:31 step:24454 [D loss: 0.229495, acc.: 92.19%] [G loss: 4.182778]\n",
      "epoch:31 step:24455 [D loss: 0.278459, acc.: 89.06%] [G loss: 3.203239]\n",
      "epoch:31 step:24456 [D loss: 0.260107, acc.: 89.84%] [G loss: 3.627741]\n",
      "epoch:31 step:24457 [D loss: 0.389331, acc.: 81.25%] [G loss: 3.232444]\n",
      "epoch:31 step:24458 [D loss: 0.394567, acc.: 82.81%] [G loss: 2.977957]\n",
      "epoch:31 step:24459 [D loss: 0.236715, acc.: 88.28%] [G loss: 3.339363]\n",
      "epoch:31 step:24460 [D loss: 0.276608, acc.: 85.94%] [G loss: 3.267088]\n",
      "epoch:31 step:24461 [D loss: 0.352746, acc.: 83.59%] [G loss: 2.207709]\n",
      "epoch:31 step:24462 [D loss: 0.236676, acc.: 92.19%] [G loss: 2.980450]\n",
      "epoch:31 step:24463 [D loss: 0.297535, acc.: 88.28%] [G loss: 3.223877]\n",
      "epoch:31 step:24464 [D loss: 0.296502, acc.: 88.28%] [G loss: 2.724274]\n",
      "epoch:31 step:24465 [D loss: 0.248577, acc.: 88.28%] [G loss: 2.820971]\n",
      "epoch:31 step:24466 [D loss: 0.319577, acc.: 86.72%] [G loss: 2.950246]\n",
      "epoch:31 step:24467 [D loss: 0.279779, acc.: 89.84%] [G loss: 2.732895]\n",
      "epoch:31 step:24468 [D loss: 0.317789, acc.: 84.38%] [G loss: 3.364138]\n",
      "epoch:31 step:24469 [D loss: 0.282370, acc.: 85.94%] [G loss: 4.112246]\n",
      "epoch:31 step:24470 [D loss: 0.317860, acc.: 85.94%] [G loss: 3.932791]\n",
      "epoch:31 step:24471 [D loss: 0.364578, acc.: 80.47%] [G loss: 3.501442]\n",
      "epoch:31 step:24472 [D loss: 0.376363, acc.: 79.69%] [G loss: 3.721520]\n",
      "epoch:31 step:24473 [D loss: 0.287253, acc.: 85.94%] [G loss: 3.877280]\n",
      "epoch:31 step:24474 [D loss: 0.444897, acc.: 82.03%] [G loss: 5.322700]\n",
      "epoch:31 step:24475 [D loss: 0.526492, acc.: 72.66%] [G loss: 4.022965]\n",
      "epoch:31 step:24476 [D loss: 0.276958, acc.: 85.94%] [G loss: 4.675542]\n",
      "epoch:31 step:24477 [D loss: 0.567239, acc.: 72.66%] [G loss: 2.909993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24478 [D loss: 0.355176, acc.: 82.81%] [G loss: 4.362445]\n",
      "epoch:31 step:24479 [D loss: 0.517070, acc.: 78.12%] [G loss: 4.380761]\n",
      "epoch:31 step:24480 [D loss: 0.298828, acc.: 85.16%] [G loss: 4.139041]\n",
      "epoch:31 step:24481 [D loss: 0.291342, acc.: 87.50%] [G loss: 3.852962]\n",
      "epoch:31 step:24482 [D loss: 0.254894, acc.: 85.94%] [G loss: 3.747148]\n",
      "epoch:31 step:24483 [D loss: 0.322098, acc.: 85.94%] [G loss: 4.632132]\n",
      "epoch:31 step:24484 [D loss: 0.191614, acc.: 89.84%] [G loss: 4.766111]\n",
      "epoch:31 step:24485 [D loss: 0.328643, acc.: 82.81%] [G loss: 3.945961]\n",
      "epoch:31 step:24486 [D loss: 0.402152, acc.: 82.03%] [G loss: 3.604655]\n",
      "epoch:31 step:24487 [D loss: 0.340901, acc.: 81.25%] [G loss: 2.888922]\n",
      "epoch:31 step:24488 [D loss: 0.249853, acc.: 86.72%] [G loss: 4.157880]\n",
      "epoch:31 step:24489 [D loss: 0.322883, acc.: 84.38%] [G loss: 2.784391]\n",
      "epoch:31 step:24490 [D loss: 0.331427, acc.: 83.59%] [G loss: 3.568839]\n",
      "epoch:31 step:24491 [D loss: 0.273225, acc.: 89.84%] [G loss: 3.365136]\n",
      "epoch:31 step:24492 [D loss: 0.396250, acc.: 79.69%] [G loss: 2.813052]\n",
      "epoch:31 step:24493 [D loss: 0.273870, acc.: 90.62%] [G loss: 3.142736]\n",
      "epoch:31 step:24494 [D loss: 0.254129, acc.: 88.28%] [G loss: 3.596062]\n",
      "epoch:31 step:24495 [D loss: 0.353005, acc.: 84.38%] [G loss: 4.883978]\n",
      "epoch:31 step:24496 [D loss: 0.277228, acc.: 88.28%] [G loss: 2.911937]\n",
      "epoch:31 step:24497 [D loss: 0.304769, acc.: 87.50%] [G loss: 3.891889]\n",
      "epoch:31 step:24498 [D loss: 0.274799, acc.: 87.50%] [G loss: 3.002348]\n",
      "epoch:31 step:24499 [D loss: 0.260519, acc.: 92.19%] [G loss: 3.714160]\n",
      "epoch:31 step:24500 [D loss: 0.334482, acc.: 84.38%] [G loss: 4.191409]\n",
      "epoch:31 step:24501 [D loss: 0.351616, acc.: 86.72%] [G loss: 3.532310]\n",
      "epoch:31 step:24502 [D loss: 0.263561, acc.: 89.06%] [G loss: 3.585665]\n",
      "epoch:31 step:24503 [D loss: 0.367771, acc.: 81.25%] [G loss: 3.579131]\n",
      "epoch:31 step:24504 [D loss: 0.306171, acc.: 86.72%] [G loss: 4.213438]\n",
      "epoch:31 step:24505 [D loss: 0.299541, acc.: 82.81%] [G loss: 3.981367]\n",
      "epoch:31 step:24506 [D loss: 0.326342, acc.: 90.62%] [G loss: 4.257408]\n",
      "epoch:31 step:24507 [D loss: 0.338561, acc.: 88.28%] [G loss: 3.101784]\n",
      "epoch:31 step:24508 [D loss: 0.243340, acc.: 89.06%] [G loss: 3.528319]\n",
      "epoch:31 step:24509 [D loss: 0.290980, acc.: 85.94%] [G loss: 2.880378]\n",
      "epoch:31 step:24510 [D loss: 0.285380, acc.: 87.50%] [G loss: 4.763813]\n",
      "epoch:31 step:24511 [D loss: 0.280872, acc.: 85.94%] [G loss: 3.208779]\n",
      "epoch:31 step:24512 [D loss: 0.237701, acc.: 91.41%] [G loss: 4.019211]\n",
      "epoch:31 step:24513 [D loss: 0.298275, acc.: 85.16%] [G loss: 2.825985]\n",
      "epoch:31 step:24514 [D loss: 0.321223, acc.: 88.28%] [G loss: 3.357248]\n",
      "epoch:31 step:24515 [D loss: 0.276979, acc.: 86.72%] [G loss: 3.148230]\n",
      "epoch:31 step:24516 [D loss: 0.287607, acc.: 87.50%] [G loss: 2.995637]\n",
      "epoch:31 step:24517 [D loss: 0.306206, acc.: 85.16%] [G loss: 3.035852]\n",
      "epoch:31 step:24518 [D loss: 0.285345, acc.: 87.50%] [G loss: 3.045471]\n",
      "epoch:31 step:24519 [D loss: 0.334843, acc.: 83.59%] [G loss: 2.799875]\n",
      "epoch:31 step:24520 [D loss: 0.385500, acc.: 83.59%] [G loss: 3.867922]\n",
      "epoch:31 step:24521 [D loss: 0.382254, acc.: 85.16%] [G loss: 3.723268]\n",
      "epoch:31 step:24522 [D loss: 0.298067, acc.: 86.72%] [G loss: 4.745836]\n",
      "epoch:31 step:24523 [D loss: 0.370996, acc.: 83.59%] [G loss: 5.588169]\n",
      "epoch:31 step:24524 [D loss: 0.313901, acc.: 85.94%] [G loss: 4.741901]\n",
      "epoch:31 step:24525 [D loss: 0.260255, acc.: 85.94%] [G loss: 5.436714]\n",
      "epoch:31 step:24526 [D loss: 0.521716, acc.: 71.88%] [G loss: 4.499787]\n",
      "epoch:31 step:24527 [D loss: 0.374787, acc.: 85.94%] [G loss: 4.098866]\n",
      "epoch:31 step:24528 [D loss: 0.437237, acc.: 79.69%] [G loss: 4.047001]\n",
      "epoch:31 step:24529 [D loss: 0.346662, acc.: 82.81%] [G loss: 3.435915]\n",
      "epoch:31 step:24530 [D loss: 0.250808, acc.: 90.62%] [G loss: 3.577397]\n",
      "epoch:31 step:24531 [D loss: 0.425303, acc.: 78.12%] [G loss: 3.214501]\n",
      "epoch:31 step:24532 [D loss: 0.287396, acc.: 86.72%] [G loss: 3.598143]\n",
      "epoch:31 step:24533 [D loss: 0.385443, acc.: 82.81%] [G loss: 4.218718]\n",
      "epoch:31 step:24534 [D loss: 0.411771, acc.: 79.69%] [G loss: 2.629013]\n",
      "epoch:31 step:24535 [D loss: 0.326307, acc.: 86.72%] [G loss: 3.750980]\n",
      "epoch:31 step:24536 [D loss: 0.366759, acc.: 78.91%] [G loss: 3.079321]\n",
      "epoch:31 step:24537 [D loss: 0.335067, acc.: 85.94%] [G loss: 3.419613]\n",
      "epoch:31 step:24538 [D loss: 0.289976, acc.: 85.94%] [G loss: 3.258434]\n",
      "epoch:31 step:24539 [D loss: 0.328399, acc.: 86.72%] [G loss: 2.905243]\n",
      "epoch:31 step:24540 [D loss: 0.275370, acc.: 91.41%] [G loss: 4.396322]\n",
      "epoch:31 step:24541 [D loss: 0.307046, acc.: 84.38%] [G loss: 2.562854]\n",
      "epoch:31 step:24542 [D loss: 0.342505, acc.: 83.59%] [G loss: 3.860681]\n",
      "epoch:31 step:24543 [D loss: 0.308323, acc.: 84.38%] [G loss: 3.757458]\n",
      "epoch:31 step:24544 [D loss: 0.229334, acc.: 89.84%] [G loss: 3.320472]\n",
      "epoch:31 step:24545 [D loss: 0.392588, acc.: 83.59%] [G loss: 3.537950]\n",
      "epoch:31 step:24546 [D loss: 0.409360, acc.: 82.03%] [G loss: 3.113012]\n",
      "epoch:31 step:24547 [D loss: 0.289051, acc.: 89.84%] [G loss: 4.286179]\n",
      "epoch:31 step:24548 [D loss: 0.219229, acc.: 93.75%] [G loss: 4.871189]\n",
      "epoch:31 step:24549 [D loss: 0.291008, acc.: 86.72%] [G loss: 5.279923]\n",
      "epoch:31 step:24550 [D loss: 0.212544, acc.: 92.97%] [G loss: 3.931280]\n",
      "epoch:31 step:24551 [D loss: 0.311448, acc.: 89.06%] [G loss: 3.367355]\n",
      "epoch:31 step:24552 [D loss: 0.278103, acc.: 83.59%] [G loss: 3.697394]\n",
      "epoch:31 step:24553 [D loss: 0.308994, acc.: 84.38%] [G loss: 3.850217]\n",
      "epoch:31 step:24554 [D loss: 0.304210, acc.: 84.38%] [G loss: 3.405407]\n",
      "epoch:31 step:24555 [D loss: 0.298781, acc.: 85.16%] [G loss: 4.674425]\n",
      "epoch:31 step:24556 [D loss: 0.256958, acc.: 89.84%] [G loss: 3.923022]\n",
      "epoch:31 step:24557 [D loss: 0.347845, acc.: 84.38%] [G loss: 3.633220]\n",
      "epoch:31 step:24558 [D loss: 0.276386, acc.: 86.72%] [G loss: 2.435281]\n",
      "epoch:31 step:24559 [D loss: 0.377416, acc.: 81.25%] [G loss: 3.698579]\n",
      "epoch:31 step:24560 [D loss: 0.373869, acc.: 82.03%] [G loss: 5.181499]\n",
      "epoch:31 step:24561 [D loss: 0.800110, acc.: 58.59%] [G loss: 5.382467]\n",
      "epoch:31 step:24562 [D loss: 0.690067, acc.: 78.12%] [G loss: 6.458467]\n",
      "epoch:31 step:24563 [D loss: 1.310333, acc.: 71.09%] [G loss: 5.326687]\n",
      "epoch:31 step:24564 [D loss: 0.936312, acc.: 65.62%] [G loss: 1.937872]\n",
      "epoch:31 step:24565 [D loss: 0.441091, acc.: 79.69%] [G loss: 5.630031]\n",
      "epoch:31 step:24566 [D loss: 0.440321, acc.: 78.91%] [G loss: 3.439742]\n",
      "epoch:31 step:24567 [D loss: 0.388305, acc.: 79.69%] [G loss: 4.449176]\n",
      "epoch:31 step:24568 [D loss: 0.381431, acc.: 79.69%] [G loss: 3.866362]\n",
      "epoch:31 step:24569 [D loss: 0.222016, acc.: 89.06%] [G loss: 3.472319]\n",
      "epoch:31 step:24570 [D loss: 0.320308, acc.: 85.16%] [G loss: 2.829424]\n",
      "epoch:31 step:24571 [D loss: 0.316360, acc.: 85.16%] [G loss: 2.940308]\n",
      "epoch:31 step:24572 [D loss: 0.283941, acc.: 88.28%] [G loss: 2.952523]\n",
      "epoch:31 step:24573 [D loss: 0.249279, acc.: 90.62%] [G loss: 2.419220]\n",
      "epoch:31 step:24574 [D loss: 0.319343, acc.: 85.16%] [G loss: 3.392669]\n",
      "epoch:31 step:24575 [D loss: 0.330009, acc.: 84.38%] [G loss: 2.376955]\n",
      "epoch:31 step:24576 [D loss: 0.315542, acc.: 85.94%] [G loss: 2.846486]\n",
      "epoch:31 step:24577 [D loss: 0.542136, acc.: 68.75%] [G loss: 2.580183]\n",
      "epoch:31 step:24578 [D loss: 0.354485, acc.: 82.03%] [G loss: 2.791213]\n",
      "epoch:31 step:24579 [D loss: 0.358865, acc.: 80.47%] [G loss: 2.487041]\n",
      "epoch:31 step:24580 [D loss: 0.236606, acc.: 91.41%] [G loss: 3.487150]\n",
      "epoch:31 step:24581 [D loss: 0.455105, acc.: 78.12%] [G loss: 2.731159]\n",
      "epoch:31 step:24582 [D loss: 0.337007, acc.: 84.38%] [G loss: 2.393762]\n",
      "epoch:31 step:24583 [D loss: 0.318970, acc.: 84.38%] [G loss: 3.021013]\n",
      "epoch:31 step:24584 [D loss: 0.287822, acc.: 88.28%] [G loss: 2.435839]\n",
      "epoch:31 step:24585 [D loss: 0.372148, acc.: 83.59%] [G loss: 2.070323]\n",
      "epoch:31 step:24586 [D loss: 0.306795, acc.: 86.72%] [G loss: 2.183215]\n",
      "epoch:31 step:24587 [D loss: 0.314239, acc.: 84.38%] [G loss: 2.256701]\n",
      "epoch:31 step:24588 [D loss: 0.363008, acc.: 85.94%] [G loss: 2.584255]\n",
      "epoch:31 step:24589 [D loss: 0.352870, acc.: 83.59%] [G loss: 2.169320]\n",
      "epoch:31 step:24590 [D loss: 0.337211, acc.: 85.94%] [G loss: 2.458483]\n",
      "epoch:31 step:24591 [D loss: 0.340323, acc.: 80.47%] [G loss: 2.637539]\n",
      "epoch:31 step:24592 [D loss: 0.324797, acc.: 82.03%] [G loss: 2.282479]\n",
      "epoch:31 step:24593 [D loss: 0.293457, acc.: 88.28%] [G loss: 2.237019]\n",
      "epoch:31 step:24594 [D loss: 0.389491, acc.: 85.94%] [G loss: 2.845321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24595 [D loss: 0.284475, acc.: 87.50%] [G loss: 2.780126]\n",
      "epoch:31 step:24596 [D loss: 0.363734, acc.: 87.50%] [G loss: 2.472604]\n",
      "epoch:31 step:24597 [D loss: 0.368983, acc.: 83.59%] [G loss: 3.441679]\n",
      "epoch:31 step:24598 [D loss: 0.372354, acc.: 82.03%] [G loss: 2.491440]\n",
      "epoch:31 step:24599 [D loss: 0.402238, acc.: 82.81%] [G loss: 2.395370]\n",
      "epoch:31 step:24600 [D loss: 0.281782, acc.: 87.50%] [G loss: 2.615910]\n",
      "epoch:31 step:24601 [D loss: 0.301422, acc.: 86.72%] [G loss: 3.104493]\n",
      "epoch:31 step:24602 [D loss: 0.474640, acc.: 75.78%] [G loss: 2.399404]\n",
      "epoch:31 step:24603 [D loss: 0.320702, acc.: 85.94%] [G loss: 3.043819]\n",
      "epoch:31 step:24604 [D loss: 0.271104, acc.: 88.28%] [G loss: 3.045593]\n",
      "epoch:31 step:24605 [D loss: 0.363557, acc.: 85.16%] [G loss: 2.294053]\n",
      "epoch:31 step:24606 [D loss: 0.363877, acc.: 83.59%] [G loss: 2.088540]\n",
      "epoch:31 step:24607 [D loss: 0.302035, acc.: 86.72%] [G loss: 2.742580]\n",
      "epoch:31 step:24608 [D loss: 0.324666, acc.: 84.38%] [G loss: 2.230663]\n",
      "epoch:31 step:24609 [D loss: 0.348832, acc.: 85.16%] [G loss: 1.854840]\n",
      "epoch:31 step:24610 [D loss: 0.392955, acc.: 79.69%] [G loss: 1.768564]\n",
      "epoch:31 step:24611 [D loss: 0.265609, acc.: 89.06%] [G loss: 2.701342]\n",
      "epoch:31 step:24612 [D loss: 0.500629, acc.: 76.56%] [G loss: 2.955099]\n",
      "epoch:31 step:24613 [D loss: 0.360990, acc.: 85.94%] [G loss: 2.595388]\n",
      "epoch:31 step:24614 [D loss: 0.303304, acc.: 89.06%] [G loss: 2.994581]\n",
      "epoch:31 step:24615 [D loss: 0.288421, acc.: 87.50%] [G loss: 3.289448]\n",
      "epoch:31 step:24616 [D loss: 0.304497, acc.: 88.28%] [G loss: 2.608736]\n",
      "epoch:31 step:24617 [D loss: 0.403679, acc.: 80.47%] [G loss: 2.989818]\n",
      "epoch:31 step:24618 [D loss: 0.293142, acc.: 86.72%] [G loss: 2.845174]\n",
      "epoch:31 step:24619 [D loss: 0.337425, acc.: 86.72%] [G loss: 3.270453]\n",
      "epoch:31 step:24620 [D loss: 0.304362, acc.: 85.16%] [G loss: 2.745635]\n",
      "epoch:31 step:24621 [D loss: 0.367947, acc.: 83.59%] [G loss: 2.562294]\n",
      "epoch:31 step:24622 [D loss: 0.412523, acc.: 82.81%] [G loss: 2.375525]\n",
      "epoch:31 step:24623 [D loss: 0.279008, acc.: 87.50%] [G loss: 2.507317]\n",
      "epoch:31 step:24624 [D loss: 0.307144, acc.: 90.62%] [G loss: 2.480830]\n",
      "epoch:31 step:24625 [D loss: 0.343580, acc.: 85.94%] [G loss: 2.183490]\n",
      "epoch:31 step:24626 [D loss: 0.261802, acc.: 88.28%] [G loss: 2.597666]\n",
      "epoch:31 step:24627 [D loss: 0.274606, acc.: 92.19%] [G loss: 3.604921]\n",
      "epoch:31 step:24628 [D loss: 0.188439, acc.: 92.19%] [G loss: 5.124820]\n",
      "epoch:31 step:24629 [D loss: 0.186114, acc.: 91.41%] [G loss: 4.097813]\n",
      "epoch:31 step:24630 [D loss: 0.265148, acc.: 88.28%] [G loss: 3.948399]\n",
      "epoch:31 step:24631 [D loss: 0.202147, acc.: 91.41%] [G loss: 3.922431]\n",
      "epoch:31 step:24632 [D loss: 0.218430, acc.: 94.53%] [G loss: 2.583628]\n",
      "epoch:31 step:24633 [D loss: 0.321554, acc.: 85.16%] [G loss: 3.401742]\n",
      "epoch:31 step:24634 [D loss: 0.318374, acc.: 83.59%] [G loss: 3.528726]\n",
      "epoch:31 step:24635 [D loss: 0.282746, acc.: 88.28%] [G loss: 4.356133]\n",
      "epoch:31 step:24636 [D loss: 0.315409, acc.: 85.16%] [G loss: 4.358356]\n",
      "epoch:31 step:24637 [D loss: 0.278307, acc.: 85.16%] [G loss: 3.283201]\n",
      "epoch:31 step:24638 [D loss: 0.226218, acc.: 92.97%] [G loss: 3.272359]\n",
      "epoch:31 step:24639 [D loss: 0.383575, acc.: 78.91%] [G loss: 4.783975]\n",
      "epoch:31 step:24640 [D loss: 0.410913, acc.: 80.47%] [G loss: 3.594061]\n",
      "epoch:31 step:24641 [D loss: 0.321596, acc.: 82.81%] [G loss: 3.976398]\n",
      "epoch:31 step:24642 [D loss: 0.345455, acc.: 88.28%] [G loss: 3.098740]\n",
      "epoch:31 step:24643 [D loss: 0.366742, acc.: 84.38%] [G loss: 4.091986]\n",
      "epoch:31 step:24644 [D loss: 0.257835, acc.: 89.84%] [G loss: 2.629256]\n",
      "epoch:31 step:24645 [D loss: 0.395179, acc.: 83.59%] [G loss: 4.382294]\n",
      "epoch:31 step:24646 [D loss: 0.514344, acc.: 82.03%] [G loss: 5.294804]\n",
      "epoch:31 step:24647 [D loss: 0.740384, acc.: 78.91%] [G loss: 7.100834]\n",
      "epoch:31 step:24648 [D loss: 1.107606, acc.: 71.09%] [G loss: 5.906624]\n",
      "epoch:31 step:24649 [D loss: 1.091351, acc.: 65.62%] [G loss: 6.132957]\n",
      "epoch:31 step:24650 [D loss: 0.594829, acc.: 78.12%] [G loss: 5.499478]\n",
      "epoch:31 step:24651 [D loss: 0.548373, acc.: 75.00%] [G loss: 3.416638]\n",
      "epoch:31 step:24652 [D loss: 0.439861, acc.: 82.81%] [G loss: 4.991266]\n",
      "epoch:31 step:24653 [D loss: 0.265421, acc.: 87.50%] [G loss: 3.531443]\n",
      "epoch:31 step:24654 [D loss: 0.402352, acc.: 82.81%] [G loss: 4.552076]\n",
      "epoch:31 step:24655 [D loss: 0.324438, acc.: 83.59%] [G loss: 3.282178]\n",
      "epoch:31 step:24656 [D loss: 0.321543, acc.: 85.16%] [G loss: 3.729286]\n",
      "epoch:31 step:24657 [D loss: 0.310017, acc.: 84.38%] [G loss: 3.898395]\n",
      "epoch:31 step:24658 [D loss: 0.255866, acc.: 89.84%] [G loss: 3.114651]\n",
      "epoch:31 step:24659 [D loss: 0.318487, acc.: 83.59%] [G loss: 3.603500]\n",
      "epoch:31 step:24660 [D loss: 0.351364, acc.: 85.94%] [G loss: 2.791802]\n",
      "epoch:31 step:24661 [D loss: 0.343840, acc.: 85.16%] [G loss: 3.312824]\n",
      "epoch:31 step:24662 [D loss: 0.234379, acc.: 90.62%] [G loss: 2.660638]\n",
      "epoch:31 step:24663 [D loss: 0.256968, acc.: 91.41%] [G loss: 3.374127]\n",
      "epoch:31 step:24664 [D loss: 0.271404, acc.: 90.62%] [G loss: 3.728293]\n",
      "epoch:31 step:24665 [D loss: 0.326735, acc.: 85.16%] [G loss: 2.492745]\n",
      "epoch:31 step:24666 [D loss: 0.307670, acc.: 85.16%] [G loss: 2.910780]\n",
      "epoch:31 step:24667 [D loss: 0.335477, acc.: 85.16%] [G loss: 2.523398]\n",
      "epoch:31 step:24668 [D loss: 0.291414, acc.: 90.62%] [G loss: 2.212310]\n",
      "epoch:31 step:24669 [D loss: 0.285287, acc.: 88.28%] [G loss: 3.246205]\n",
      "epoch:31 step:24670 [D loss: 0.262735, acc.: 88.28%] [G loss: 2.628929]\n",
      "epoch:31 step:24671 [D loss: 0.373626, acc.: 81.25%] [G loss: 3.217575]\n",
      "epoch:31 step:24672 [D loss: 0.308863, acc.: 86.72%] [G loss: 3.862585]\n",
      "epoch:31 step:24673 [D loss: 0.316983, acc.: 85.94%] [G loss: 3.470139]\n",
      "epoch:31 step:24674 [D loss: 0.321402, acc.: 83.59%] [G loss: 3.353892]\n",
      "epoch:31 step:24675 [D loss: 0.314372, acc.: 89.06%] [G loss: 2.893917]\n",
      "epoch:31 step:24676 [D loss: 0.335633, acc.: 83.59%] [G loss: 4.356206]\n",
      "epoch:31 step:24677 [D loss: 0.303831, acc.: 85.94%] [G loss: 2.659884]\n",
      "epoch:31 step:24678 [D loss: 0.319842, acc.: 87.50%] [G loss: 4.178010]\n",
      "epoch:31 step:24679 [D loss: 0.366167, acc.: 84.38%] [G loss: 2.975184]\n",
      "epoch:31 step:24680 [D loss: 0.303844, acc.: 85.94%] [G loss: 2.844829]\n",
      "epoch:31 step:24681 [D loss: 0.291739, acc.: 85.94%] [G loss: 2.765091]\n",
      "epoch:31 step:24682 [D loss: 0.262980, acc.: 89.06%] [G loss: 2.552565]\n",
      "epoch:31 step:24683 [D loss: 0.333621, acc.: 82.81%] [G loss: 3.014103]\n",
      "epoch:31 step:24684 [D loss: 0.324467, acc.: 85.94%] [G loss: 2.426984]\n",
      "epoch:31 step:24685 [D loss: 0.356518, acc.: 86.72%] [G loss: 2.985117]\n",
      "epoch:31 step:24686 [D loss: 0.401290, acc.: 80.47%] [G loss: 2.385475]\n",
      "epoch:31 step:24687 [D loss: 0.327426, acc.: 87.50%] [G loss: 2.122405]\n",
      "epoch:31 step:24688 [D loss: 0.221725, acc.: 93.75%] [G loss: 2.742612]\n",
      "epoch:31 step:24689 [D loss: 0.290098, acc.: 92.97%] [G loss: 2.788599]\n",
      "epoch:31 step:24690 [D loss: 0.303802, acc.: 88.28%] [G loss: 3.322456]\n",
      "epoch:31 step:24691 [D loss: 0.314742, acc.: 85.94%] [G loss: 2.747532]\n",
      "epoch:31 step:24692 [D loss: 0.333688, acc.: 83.59%] [G loss: 3.007058]\n",
      "epoch:31 step:24693 [D loss: 0.361675, acc.: 84.38%] [G loss: 2.930667]\n",
      "epoch:31 step:24694 [D loss: 0.284647, acc.: 87.50%] [G loss: 3.431632]\n",
      "epoch:31 step:24695 [D loss: 0.381254, acc.: 83.59%] [G loss: 2.782877]\n",
      "epoch:31 step:24696 [D loss: 0.361462, acc.: 82.81%] [G loss: 2.289309]\n",
      "epoch:31 step:24697 [D loss: 0.305679, acc.: 85.16%] [G loss: 2.382412]\n",
      "epoch:31 step:24698 [D loss: 0.380775, acc.: 81.25%] [G loss: 2.999623]\n",
      "epoch:31 step:24699 [D loss: 0.312712, acc.: 85.16%] [G loss: 2.730201]\n",
      "epoch:31 step:24700 [D loss: 0.330822, acc.: 82.81%] [G loss: 2.864208]\n",
      "epoch:31 step:24701 [D loss: 0.280360, acc.: 89.06%] [G loss: 2.525546]\n",
      "epoch:31 step:24702 [D loss: 0.345621, acc.: 85.16%] [G loss: 2.422335]\n",
      "epoch:31 step:24703 [D loss: 0.437339, acc.: 81.25%] [G loss: 2.909318]\n",
      "epoch:31 step:24704 [D loss: 0.280830, acc.: 87.50%] [G loss: 2.866952]\n",
      "epoch:31 step:24705 [D loss: 0.247963, acc.: 89.06%] [G loss: 2.687482]\n",
      "epoch:31 step:24706 [D loss: 0.334026, acc.: 85.16%] [G loss: 2.893811]\n",
      "epoch:31 step:24707 [D loss: 0.307540, acc.: 91.41%] [G loss: 3.036999]\n",
      "epoch:31 step:24708 [D loss: 0.253463, acc.: 91.41%] [G loss: 3.316600]\n",
      "epoch:31 step:24709 [D loss: 0.398448, acc.: 83.59%] [G loss: 2.178098]\n",
      "epoch:31 step:24710 [D loss: 0.322234, acc.: 85.16%] [G loss: 2.576410]\n",
      "epoch:31 step:24711 [D loss: 0.316283, acc.: 86.72%] [G loss: 2.432832]\n",
      "epoch:31 step:24712 [D loss: 0.303597, acc.: 85.16%] [G loss: 3.287045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24713 [D loss: 0.382350, acc.: 82.81%] [G loss: 2.591748]\n",
      "epoch:31 step:24714 [D loss: 0.329788, acc.: 83.59%] [G loss: 2.432405]\n",
      "epoch:31 step:24715 [D loss: 0.293083, acc.: 85.94%] [G loss: 2.835394]\n",
      "epoch:31 step:24716 [D loss: 0.300804, acc.: 88.28%] [G loss: 2.484178]\n",
      "epoch:31 step:24717 [D loss: 0.327618, acc.: 85.16%] [G loss: 2.703058]\n",
      "epoch:31 step:24718 [D loss: 0.351911, acc.: 82.03%] [G loss: 2.684910]\n",
      "epoch:31 step:24719 [D loss: 0.359917, acc.: 87.50%] [G loss: 2.991294]\n",
      "epoch:31 step:24720 [D loss: 0.288474, acc.: 89.84%] [G loss: 3.841832]\n",
      "epoch:31 step:24721 [D loss: 0.325736, acc.: 83.59%] [G loss: 5.171331]\n",
      "epoch:31 step:24722 [D loss: 0.209212, acc.: 92.19%] [G loss: 3.870653]\n",
      "epoch:31 step:24723 [D loss: 0.266686, acc.: 87.50%] [G loss: 2.764542]\n",
      "epoch:31 step:24724 [D loss: 0.295053, acc.: 85.94%] [G loss: 2.979674]\n",
      "epoch:31 step:24725 [D loss: 0.273302, acc.: 88.28%] [G loss: 2.794443]\n",
      "epoch:31 step:24726 [D loss: 0.293169, acc.: 85.94%] [G loss: 2.635749]\n",
      "epoch:31 step:24727 [D loss: 0.348505, acc.: 83.59%] [G loss: 2.749331]\n",
      "epoch:31 step:24728 [D loss: 0.319487, acc.: 84.38%] [G loss: 3.108141]\n",
      "epoch:31 step:24729 [D loss: 0.225471, acc.: 89.84%] [G loss: 5.232668]\n",
      "epoch:31 step:24730 [D loss: 0.240730, acc.: 88.28%] [G loss: 4.018604]\n",
      "epoch:31 step:24731 [D loss: 0.350357, acc.: 81.25%] [G loss: 2.680274]\n",
      "epoch:31 step:24732 [D loss: 0.297676, acc.: 87.50%] [G loss: 3.479371]\n",
      "epoch:31 step:24733 [D loss: 0.376941, acc.: 82.03%] [G loss: 2.908846]\n",
      "epoch:31 step:24734 [D loss: 0.303883, acc.: 85.94%] [G loss: 3.269336]\n",
      "epoch:31 step:24735 [D loss: 0.247113, acc.: 90.62%] [G loss: 3.179861]\n",
      "epoch:31 step:24736 [D loss: 0.303223, acc.: 85.94%] [G loss: 3.374519]\n",
      "epoch:31 step:24737 [D loss: 0.285528, acc.: 89.06%] [G loss: 4.734274]\n",
      "epoch:31 step:24738 [D loss: 0.310300, acc.: 87.50%] [G loss: 3.788862]\n",
      "epoch:31 step:24739 [D loss: 0.290117, acc.: 89.84%] [G loss: 3.672744]\n",
      "epoch:31 step:24740 [D loss: 0.279181, acc.: 89.06%] [G loss: 3.123108]\n",
      "epoch:31 step:24741 [D loss: 0.244956, acc.: 90.62%] [G loss: 3.437268]\n",
      "epoch:31 step:24742 [D loss: 0.308218, acc.: 87.50%] [G loss: 3.392937]\n",
      "epoch:31 step:24743 [D loss: 0.360481, acc.: 83.59%] [G loss: 4.460081]\n",
      "epoch:31 step:24744 [D loss: 0.332785, acc.: 85.16%] [G loss: 2.917697]\n",
      "epoch:31 step:24745 [D loss: 0.344554, acc.: 78.91%] [G loss: 4.123897]\n",
      "epoch:31 step:24746 [D loss: 0.378820, acc.: 83.59%] [G loss: 3.426544]\n",
      "epoch:31 step:24747 [D loss: 0.307059, acc.: 83.59%] [G loss: 2.766900]\n",
      "epoch:31 step:24748 [D loss: 0.375831, acc.: 81.25%] [G loss: 3.355372]\n",
      "epoch:31 step:24749 [D loss: 0.291757, acc.: 88.28%] [G loss: 3.112837]\n",
      "epoch:31 step:24750 [D loss: 0.286097, acc.: 90.62%] [G loss: 3.120032]\n",
      "epoch:31 step:24751 [D loss: 0.178117, acc.: 92.19%] [G loss: 4.083757]\n",
      "epoch:31 step:24752 [D loss: 0.280481, acc.: 86.72%] [G loss: 3.913269]\n",
      "epoch:31 step:24753 [D loss: 0.281992, acc.: 89.06%] [G loss: 4.081932]\n",
      "epoch:31 step:24754 [D loss: 0.242643, acc.: 91.41%] [G loss: 6.239649]\n",
      "epoch:31 step:24755 [D loss: 0.380673, acc.: 78.12%] [G loss: 3.592614]\n",
      "epoch:31 step:24756 [D loss: 0.271905, acc.: 87.50%] [G loss: 3.912047]\n",
      "epoch:31 step:24757 [D loss: 0.300136, acc.: 87.50%] [G loss: 3.696734]\n",
      "epoch:31 step:24758 [D loss: 0.321903, acc.: 89.06%] [G loss: 3.156353]\n",
      "epoch:31 step:24759 [D loss: 0.313155, acc.: 90.62%] [G loss: 3.365356]\n",
      "epoch:31 step:24760 [D loss: 0.281448, acc.: 87.50%] [G loss: 3.179354]\n",
      "epoch:31 step:24761 [D loss: 0.317454, acc.: 89.84%] [G loss: 3.200128]\n",
      "epoch:31 step:24762 [D loss: 0.242295, acc.: 89.84%] [G loss: 2.463090]\n",
      "epoch:31 step:24763 [D loss: 0.320482, acc.: 88.28%] [G loss: 5.254508]\n",
      "epoch:31 step:24764 [D loss: 0.422855, acc.: 83.59%] [G loss: 5.995300]\n",
      "epoch:31 step:24765 [D loss: 0.211075, acc.: 92.97%] [G loss: 3.320174]\n",
      "epoch:31 step:24766 [D loss: 0.323771, acc.: 84.38%] [G loss: 5.542036]\n",
      "epoch:31 step:24767 [D loss: 0.305129, acc.: 89.84%] [G loss: 3.181116]\n",
      "epoch:31 step:24768 [D loss: 0.223817, acc.: 89.06%] [G loss: 5.337676]\n",
      "epoch:31 step:24769 [D loss: 0.295771, acc.: 86.72%] [G loss: 3.687856]\n",
      "epoch:31 step:24770 [D loss: 0.327272, acc.: 86.72%] [G loss: 3.790089]\n",
      "epoch:31 step:24771 [D loss: 0.300433, acc.: 87.50%] [G loss: 3.934258]\n",
      "epoch:31 step:24772 [D loss: 0.300005, acc.: 86.72%] [G loss: 2.908277]\n",
      "epoch:31 step:24773 [D loss: 0.291184, acc.: 86.72%] [G loss: 5.110135]\n",
      "epoch:31 step:24774 [D loss: 0.193963, acc.: 92.19%] [G loss: 3.908167]\n",
      "epoch:31 step:24775 [D loss: 0.253959, acc.: 85.16%] [G loss: 4.376269]\n",
      "epoch:31 step:24776 [D loss: 0.284900, acc.: 87.50%] [G loss: 4.745563]\n",
      "epoch:31 step:24777 [D loss: 0.274680, acc.: 86.72%] [G loss: 3.530907]\n",
      "epoch:31 step:24778 [D loss: 0.218231, acc.: 92.97%] [G loss: 5.502239]\n",
      "epoch:31 step:24779 [D loss: 0.287998, acc.: 87.50%] [G loss: 4.646008]\n",
      "epoch:31 step:24780 [D loss: 0.237977, acc.: 90.62%] [G loss: 4.364396]\n",
      "epoch:31 step:24781 [D loss: 0.305313, acc.: 85.94%] [G loss: 3.980765]\n",
      "epoch:31 step:24782 [D loss: 0.278919, acc.: 85.16%] [G loss: 4.494184]\n",
      "epoch:31 step:24783 [D loss: 0.306582, acc.: 89.84%] [G loss: 4.198374]\n",
      "epoch:31 step:24784 [D loss: 0.331688, acc.: 88.28%] [G loss: 4.127452]\n",
      "epoch:31 step:24785 [D loss: 0.383334, acc.: 80.47%] [G loss: 3.018449]\n",
      "epoch:31 step:24786 [D loss: 0.344544, acc.: 85.94%] [G loss: 3.315182]\n",
      "epoch:31 step:24787 [D loss: 0.222592, acc.: 91.41%] [G loss: 3.934755]\n",
      "epoch:31 step:24788 [D loss: 0.349648, acc.: 89.06%] [G loss: 3.512479]\n",
      "epoch:31 step:24789 [D loss: 0.333434, acc.: 83.59%] [G loss: 3.032887]\n",
      "epoch:31 step:24790 [D loss: 0.244160, acc.: 89.84%] [G loss: 2.392767]\n",
      "epoch:31 step:24791 [D loss: 0.346470, acc.: 80.47%] [G loss: 3.957102]\n",
      "epoch:31 step:24792 [D loss: 0.334548, acc.: 84.38%] [G loss: 3.281135]\n",
      "epoch:31 step:24793 [D loss: 0.293628, acc.: 84.38%] [G loss: 4.380786]\n",
      "epoch:31 step:24794 [D loss: 0.399246, acc.: 82.03%] [G loss: 2.824419]\n",
      "epoch:31 step:24795 [D loss: 0.343100, acc.: 82.03%] [G loss: 4.234930]\n",
      "epoch:31 step:24796 [D loss: 0.263803, acc.: 89.84%] [G loss: 3.815670]\n",
      "epoch:31 step:24797 [D loss: 0.305033, acc.: 88.28%] [G loss: 6.199639]\n",
      "epoch:31 step:24798 [D loss: 0.184117, acc.: 91.41%] [G loss: 6.754465]\n",
      "epoch:31 step:24799 [D loss: 0.256961, acc.: 89.84%] [G loss: 8.684951]\n",
      "epoch:31 step:24800 [D loss: 0.177231, acc.: 93.75%] [G loss: 6.686821]\n",
      "epoch:31 step:24801 [D loss: 0.187744, acc.: 91.41%] [G loss: 5.605165]\n",
      "epoch:31 step:24802 [D loss: 0.236364, acc.: 92.19%] [G loss: 4.273628]\n",
      "epoch:31 step:24803 [D loss: 0.283578, acc.: 85.16%] [G loss: 4.605115]\n",
      "epoch:31 step:24804 [D loss: 0.301707, acc.: 84.38%] [G loss: 4.555175]\n",
      "epoch:31 step:24805 [D loss: 0.288185, acc.: 89.84%] [G loss: 6.302848]\n",
      "epoch:31 step:24806 [D loss: 0.341652, acc.: 85.16%] [G loss: 4.039590]\n",
      "epoch:31 step:24807 [D loss: 0.217321, acc.: 90.62%] [G loss: 4.086702]\n",
      "epoch:31 step:24808 [D loss: 0.252749, acc.: 89.84%] [G loss: 2.801126]\n",
      "epoch:31 step:24809 [D loss: 0.222818, acc.: 91.41%] [G loss: 3.725988]\n",
      "epoch:31 step:24810 [D loss: 0.285125, acc.: 86.72%] [G loss: 3.003703]\n",
      "epoch:31 step:24811 [D loss: 0.299015, acc.: 85.94%] [G loss: 4.904679]\n",
      "epoch:31 step:24812 [D loss: 0.319258, acc.: 85.94%] [G loss: 4.135214]\n",
      "epoch:31 step:24813 [D loss: 0.365092, acc.: 83.59%] [G loss: 5.525627]\n",
      "epoch:31 step:24814 [D loss: 0.282353, acc.: 88.28%] [G loss: 4.472677]\n",
      "epoch:31 step:24815 [D loss: 0.357588, acc.: 84.38%] [G loss: 4.221545]\n",
      "epoch:31 step:24816 [D loss: 0.324842, acc.: 89.06%] [G loss: 3.678460]\n",
      "epoch:31 step:24817 [D loss: 0.286545, acc.: 85.94%] [G loss: 2.963564]\n",
      "epoch:31 step:24818 [D loss: 0.396090, acc.: 80.47%] [G loss: 3.199060]\n",
      "epoch:31 step:24819 [D loss: 0.310800, acc.: 89.06%] [G loss: 3.209933]\n",
      "epoch:31 step:24820 [D loss: 0.313738, acc.: 87.50%] [G loss: 2.964256]\n",
      "epoch:31 step:24821 [D loss: 0.302106, acc.: 90.62%] [G loss: 3.419967]\n",
      "epoch:31 step:24822 [D loss: 0.346150, acc.: 83.59%] [G loss: 3.946989]\n",
      "epoch:31 step:24823 [D loss: 0.317431, acc.: 85.94%] [G loss: 2.518326]\n",
      "epoch:31 step:24824 [D loss: 0.340579, acc.: 82.03%] [G loss: 2.856531]\n",
      "epoch:31 step:24825 [D loss: 0.269388, acc.: 87.50%] [G loss: 3.700390]\n",
      "epoch:31 step:24826 [D loss: 0.307924, acc.: 89.06%] [G loss: 2.998699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24827 [D loss: 0.297346, acc.: 86.72%] [G loss: 3.147259]\n",
      "epoch:31 step:24828 [D loss: 0.268321, acc.: 85.16%] [G loss: 2.781628]\n",
      "epoch:31 step:24829 [D loss: 0.260602, acc.: 90.62%] [G loss: 4.743165]\n",
      "epoch:31 step:24830 [D loss: 0.363414, acc.: 82.81%] [G loss: 2.549007]\n",
      "epoch:31 step:24831 [D loss: 0.264790, acc.: 88.28%] [G loss: 2.413555]\n",
      "epoch:31 step:24832 [D loss: 0.354236, acc.: 87.50%] [G loss: 2.850112]\n",
      "epoch:31 step:24833 [D loss: 0.331163, acc.: 82.03%] [G loss: 3.040941]\n",
      "epoch:31 step:24834 [D loss: 0.294792, acc.: 88.28%] [G loss: 4.114339]\n",
      "epoch:31 step:24835 [D loss: 0.404261, acc.: 81.25%] [G loss: 4.503979]\n",
      "epoch:31 step:24836 [D loss: 0.397565, acc.: 81.25%] [G loss: 3.469469]\n",
      "epoch:31 step:24837 [D loss: 0.349382, acc.: 83.59%] [G loss: 2.839352]\n",
      "epoch:31 step:24838 [D loss: 0.315228, acc.: 85.94%] [G loss: 3.721159]\n",
      "epoch:31 step:24839 [D loss: 0.382834, acc.: 82.03%] [G loss: 3.050937]\n",
      "epoch:31 step:24840 [D loss: 0.358572, acc.: 82.03%] [G loss: 3.609731]\n",
      "epoch:31 step:24841 [D loss: 0.272338, acc.: 89.84%] [G loss: 3.785985]\n",
      "epoch:31 step:24842 [D loss: 0.349567, acc.: 85.16%] [G loss: 4.453107]\n",
      "epoch:31 step:24843 [D loss: 0.329165, acc.: 85.16%] [G loss: 6.265499]\n",
      "epoch:31 step:24844 [D loss: 0.545918, acc.: 75.78%] [G loss: 4.327197]\n",
      "epoch:31 step:24845 [D loss: 0.700952, acc.: 73.44%] [G loss: 5.504566]\n",
      "epoch:31 step:24846 [D loss: 0.685185, acc.: 73.44%] [G loss: 6.971520]\n",
      "epoch:31 step:24847 [D loss: 1.182524, acc.: 71.88%] [G loss: 6.342869]\n",
      "epoch:31 step:24848 [D loss: 0.552995, acc.: 75.78%] [G loss: 3.722424]\n",
      "epoch:31 step:24849 [D loss: 0.421146, acc.: 81.25%] [G loss: 3.136880]\n",
      "epoch:31 step:24850 [D loss: 0.396684, acc.: 88.28%] [G loss: 3.706795]\n",
      "epoch:31 step:24851 [D loss: 0.263505, acc.: 89.06%] [G loss: 3.847829]\n",
      "epoch:31 step:24852 [D loss: 0.243696, acc.: 89.84%] [G loss: 3.846135]\n",
      "epoch:31 step:24853 [D loss: 0.318650, acc.: 83.59%] [G loss: 4.042744]\n",
      "epoch:31 step:24854 [D loss: 0.321055, acc.: 86.72%] [G loss: 4.489331]\n",
      "epoch:31 step:24855 [D loss: 0.249466, acc.: 86.72%] [G loss: 4.284257]\n",
      "epoch:31 step:24856 [D loss: 0.250274, acc.: 85.94%] [G loss: 2.665515]\n",
      "epoch:31 step:24857 [D loss: 0.337622, acc.: 85.16%] [G loss: 3.048286]\n",
      "epoch:31 step:24858 [D loss: 0.291279, acc.: 89.06%] [G loss: 2.444107]\n",
      "epoch:31 step:24859 [D loss: 0.263097, acc.: 91.41%] [G loss: 2.918519]\n",
      "epoch:31 step:24860 [D loss: 0.406203, acc.: 78.12%] [G loss: 3.390065]\n",
      "epoch:31 step:24861 [D loss: 0.350881, acc.: 85.94%] [G loss: 3.235276]\n",
      "epoch:31 step:24862 [D loss: 0.366357, acc.: 85.94%] [G loss: 2.885387]\n",
      "epoch:31 step:24863 [D loss: 0.323421, acc.: 84.38%] [G loss: 2.424074]\n",
      "epoch:31 step:24864 [D loss: 0.367714, acc.: 85.16%] [G loss: 2.265851]\n",
      "epoch:31 step:24865 [D loss: 0.266065, acc.: 90.62%] [G loss: 3.541322]\n",
      "epoch:31 step:24866 [D loss: 0.273013, acc.: 89.84%] [G loss: 3.193469]\n",
      "epoch:31 step:24867 [D loss: 0.363747, acc.: 85.16%] [G loss: 2.582328]\n",
      "epoch:31 step:24868 [D loss: 0.392437, acc.: 84.38%] [G loss: 2.704602]\n",
      "epoch:31 step:24869 [D loss: 0.283176, acc.: 88.28%] [G loss: 3.133406]\n",
      "epoch:31 step:24870 [D loss: 0.278228, acc.: 86.72%] [G loss: 2.074098]\n",
      "epoch:31 step:24871 [D loss: 0.417119, acc.: 79.69%] [G loss: 2.835206]\n",
      "epoch:31 step:24872 [D loss: 0.374322, acc.: 85.16%] [G loss: 3.970192]\n",
      "epoch:31 step:24873 [D loss: 0.285442, acc.: 85.94%] [G loss: 2.587453]\n",
      "epoch:31 step:24874 [D loss: 0.369519, acc.: 79.69%] [G loss: 3.960804]\n",
      "epoch:31 step:24875 [D loss: 0.369732, acc.: 84.38%] [G loss: 3.191192]\n",
      "epoch:31 step:24876 [D loss: 0.299581, acc.: 86.72%] [G loss: 3.240292]\n",
      "epoch:31 step:24877 [D loss: 0.286296, acc.: 87.50%] [G loss: 3.503156]\n",
      "epoch:31 step:24878 [D loss: 0.202855, acc.: 90.62%] [G loss: 2.485878]\n",
      "epoch:31 step:24879 [D loss: 0.263785, acc.: 92.19%] [G loss: 3.292771]\n",
      "epoch:31 step:24880 [D loss: 0.277417, acc.: 85.94%] [G loss: 3.910166]\n",
      "epoch:31 step:24881 [D loss: 0.287256, acc.: 87.50%] [G loss: 2.787874]\n",
      "epoch:31 step:24882 [D loss: 0.252125, acc.: 92.19%] [G loss: 3.445730]\n",
      "epoch:31 step:24883 [D loss: 0.283115, acc.: 83.59%] [G loss: 3.044086]\n",
      "epoch:31 step:24884 [D loss: 0.278316, acc.: 86.72%] [G loss: 4.126575]\n",
      "epoch:31 step:24885 [D loss: 0.301905, acc.: 85.94%] [G loss: 4.247235]\n",
      "epoch:31 step:24886 [D loss: 0.255898, acc.: 89.06%] [G loss: 4.133107]\n",
      "epoch:31 step:24887 [D loss: 0.319560, acc.: 89.84%] [G loss: 5.255761]\n",
      "epoch:31 step:24888 [D loss: 0.313095, acc.: 87.50%] [G loss: 3.912789]\n",
      "epoch:31 step:24889 [D loss: 0.312456, acc.: 83.59%] [G loss: 3.887754]\n",
      "epoch:31 step:24890 [D loss: 0.258102, acc.: 91.41%] [G loss: 3.920403]\n",
      "epoch:31 step:24891 [D loss: 0.384601, acc.: 82.03%] [G loss: 2.556916]\n",
      "epoch:31 step:24892 [D loss: 0.274970, acc.: 90.62%] [G loss: 4.198974]\n",
      "epoch:31 step:24893 [D loss: 0.284898, acc.: 85.94%] [G loss: 4.364451]\n",
      "epoch:31 step:24894 [D loss: 0.197194, acc.: 92.97%] [G loss: 5.462565]\n",
      "epoch:31 step:24895 [D loss: 0.228609, acc.: 92.19%] [G loss: 4.729778]\n",
      "epoch:31 step:24896 [D loss: 0.349476, acc.: 82.03%] [G loss: 3.786323]\n",
      "epoch:31 step:24897 [D loss: 0.198858, acc.: 91.41%] [G loss: 4.082229]\n",
      "epoch:31 step:24898 [D loss: 0.310536, acc.: 89.84%] [G loss: 3.120540]\n",
      "epoch:31 step:24899 [D loss: 0.314842, acc.: 83.59%] [G loss: 3.959060]\n",
      "epoch:31 step:24900 [D loss: 0.299158, acc.: 85.16%] [G loss: 3.100957]\n",
      "epoch:31 step:24901 [D loss: 0.236204, acc.: 90.62%] [G loss: 3.329094]\n",
      "epoch:31 step:24902 [D loss: 0.358631, acc.: 85.94%] [G loss: 3.952718]\n",
      "epoch:31 step:24903 [D loss: 0.290427, acc.: 90.62%] [G loss: 4.331686]\n",
      "epoch:31 step:24904 [D loss: 0.235907, acc.: 90.62%] [G loss: 2.900815]\n",
      "epoch:31 step:24905 [D loss: 0.305121, acc.: 88.28%] [G loss: 3.564761]\n",
      "epoch:31 step:24906 [D loss: 0.255160, acc.: 89.06%] [G loss: 4.246703]\n",
      "epoch:31 step:24907 [D loss: 0.384942, acc.: 82.81%] [G loss: 3.232661]\n",
      "epoch:31 step:24908 [D loss: 0.280462, acc.: 89.84%] [G loss: 3.617010]\n",
      "epoch:31 step:24909 [D loss: 0.240989, acc.: 89.06%] [G loss: 2.908588]\n",
      "epoch:31 step:24910 [D loss: 0.316529, acc.: 85.94%] [G loss: 2.681072]\n",
      "epoch:31 step:24911 [D loss: 0.300659, acc.: 88.28%] [G loss: 2.911040]\n",
      "epoch:31 step:24912 [D loss: 0.287071, acc.: 89.06%] [G loss: 3.371423]\n",
      "epoch:31 step:24913 [D loss: 0.353136, acc.: 83.59%] [G loss: 3.017799]\n",
      "epoch:31 step:24914 [D loss: 0.432546, acc.: 80.47%] [G loss: 6.327641]\n",
      "epoch:31 step:24915 [D loss: 0.632317, acc.: 75.00%] [G loss: 5.718606]\n",
      "epoch:31 step:24916 [D loss: 0.549833, acc.: 79.69%] [G loss: 3.566199]\n",
      "epoch:31 step:24917 [D loss: 0.479060, acc.: 79.69%] [G loss: 4.544913]\n",
      "epoch:31 step:24918 [D loss: 0.388800, acc.: 85.94%] [G loss: 2.659174]\n",
      "epoch:31 step:24919 [D loss: 0.429646, acc.: 78.91%] [G loss: 2.830365]\n",
      "epoch:31 step:24920 [D loss: 0.301526, acc.: 85.94%] [G loss: 2.626212]\n",
      "epoch:31 step:24921 [D loss: 0.310764, acc.: 87.50%] [G loss: 4.578209]\n",
      "epoch:31 step:24922 [D loss: 0.318551, acc.: 85.16%] [G loss: 4.682863]\n",
      "epoch:31 step:24923 [D loss: 0.312295, acc.: 85.16%] [G loss: 2.869508]\n",
      "epoch:31 step:24924 [D loss: 0.269531, acc.: 87.50%] [G loss: 6.661456]\n",
      "epoch:31 step:24925 [D loss: 0.225447, acc.: 88.28%] [G loss: 4.387883]\n",
      "epoch:31 step:24926 [D loss: 0.376647, acc.: 82.03%] [G loss: 5.651351]\n",
      "epoch:31 step:24927 [D loss: 0.157485, acc.: 92.97%] [G loss: 8.371384]\n",
      "epoch:31 step:24928 [D loss: 0.234610, acc.: 91.41%] [G loss: 5.877930]\n",
      "epoch:31 step:24929 [D loss: 0.236496, acc.: 89.84%] [G loss: 4.194885]\n",
      "epoch:31 step:24930 [D loss: 0.205924, acc.: 91.41%] [G loss: 3.472307]\n",
      "epoch:31 step:24931 [D loss: 0.274254, acc.: 93.75%] [G loss: 3.028449]\n",
      "epoch:31 step:24932 [D loss: 0.413771, acc.: 86.72%] [G loss: 3.323745]\n",
      "epoch:31 step:24933 [D loss: 0.214189, acc.: 90.62%] [G loss: 3.581564]\n",
      "epoch:31 step:24934 [D loss: 0.264189, acc.: 87.50%] [G loss: 3.758823]\n",
      "epoch:31 step:24935 [D loss: 0.298529, acc.: 87.50%] [G loss: 3.171181]\n",
      "epoch:31 step:24936 [D loss: 0.228732, acc.: 91.41%] [G loss: 3.265485]\n",
      "epoch:31 step:24937 [D loss: 0.305391, acc.: 89.06%] [G loss: 3.357506]\n",
      "epoch:31 step:24938 [D loss: 0.341722, acc.: 82.81%] [G loss: 3.069266]\n",
      "epoch:31 step:24939 [D loss: 0.383467, acc.: 81.25%] [G loss: 3.937757]\n",
      "epoch:31 step:24940 [D loss: 0.343854, acc.: 85.94%] [G loss: 2.571015]\n",
      "epoch:31 step:24941 [D loss: 0.244712, acc.: 90.62%] [G loss: 3.112498]\n",
      "epoch:31 step:24942 [D loss: 0.324368, acc.: 86.72%] [G loss: 2.883915]\n",
      "epoch:31 step:24943 [D loss: 0.415914, acc.: 79.69%] [G loss: 3.543196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24944 [D loss: 0.288546, acc.: 85.16%] [G loss: 4.211376]\n",
      "epoch:31 step:24945 [D loss: 0.337575, acc.: 84.38%] [G loss: 3.175193]\n",
      "epoch:31 step:24946 [D loss: 0.283368, acc.: 89.06%] [G loss: 3.605135]\n",
      "epoch:31 step:24947 [D loss: 0.368846, acc.: 83.59%] [G loss: 3.355812]\n",
      "epoch:31 step:24948 [D loss: 0.309184, acc.: 87.50%] [G loss: 3.889040]\n",
      "epoch:31 step:24949 [D loss: 0.322818, acc.: 83.59%] [G loss: 2.653722]\n",
      "epoch:31 step:24950 [D loss: 0.269465, acc.: 86.72%] [G loss: 3.376418]\n",
      "epoch:31 step:24951 [D loss: 0.198816, acc.: 93.75%] [G loss: 4.133923]\n",
      "epoch:31 step:24952 [D loss: 0.227513, acc.: 92.19%] [G loss: 3.611131]\n",
      "epoch:31 step:24953 [D loss: 0.343917, acc.: 83.59%] [G loss: 2.655097]\n",
      "epoch:31 step:24954 [D loss: 0.239414, acc.: 90.62%] [G loss: 4.195841]\n",
      "epoch:31 step:24955 [D loss: 0.333193, acc.: 85.16%] [G loss: 2.634019]\n",
      "epoch:31 step:24956 [D loss: 0.260984, acc.: 89.84%] [G loss: 2.697840]\n",
      "epoch:31 step:24957 [D loss: 0.296477, acc.: 85.16%] [G loss: 2.885087]\n",
      "epoch:31 step:24958 [D loss: 0.270710, acc.: 88.28%] [G loss: 3.613688]\n",
      "epoch:31 step:24959 [D loss: 0.417178, acc.: 79.69%] [G loss: 3.455385]\n",
      "epoch:31 step:24960 [D loss: 0.355144, acc.: 84.38%] [G loss: 4.415215]\n",
      "epoch:31 step:24961 [D loss: 0.274848, acc.: 86.72%] [G loss: 4.437370]\n",
      "epoch:31 step:24962 [D loss: 0.248640, acc.: 88.28%] [G loss: 3.095777]\n",
      "epoch:31 step:24963 [D loss: 0.252749, acc.: 87.50%] [G loss: 3.375576]\n",
      "epoch:31 step:24964 [D loss: 0.230051, acc.: 89.84%] [G loss: 3.670172]\n",
      "epoch:31 step:24965 [D loss: 0.361497, acc.: 82.81%] [G loss: 3.483597]\n",
      "epoch:31 step:24966 [D loss: 0.339839, acc.: 83.59%] [G loss: 3.515521]\n",
      "epoch:31 step:24967 [D loss: 0.424550, acc.: 81.25%] [G loss: 2.642474]\n",
      "epoch:31 step:24968 [D loss: 0.232428, acc.: 90.62%] [G loss: 4.268705]\n",
      "epoch:31 step:24969 [D loss: 0.243257, acc.: 89.84%] [G loss: 3.952953]\n",
      "epoch:31 step:24970 [D loss: 0.373620, acc.: 79.69%] [G loss: 3.162657]\n",
      "epoch:31 step:24971 [D loss: 0.285418, acc.: 92.19%] [G loss: 2.865805]\n",
      "epoch:31 step:24972 [D loss: 0.270458, acc.: 89.84%] [G loss: 3.067886]\n",
      "epoch:31 step:24973 [D loss: 0.337721, acc.: 85.16%] [G loss: 3.129898]\n",
      "epoch:31 step:24974 [D loss: 0.390242, acc.: 81.25%] [G loss: 3.607759]\n",
      "epoch:31 step:24975 [D loss: 0.296656, acc.: 88.28%] [G loss: 4.564168]\n",
      "epoch:31 step:24976 [D loss: 0.271243, acc.: 89.06%] [G loss: 3.686148]\n",
      "epoch:31 step:24977 [D loss: 0.278584, acc.: 85.94%] [G loss: 4.466053]\n",
      "epoch:31 step:24978 [D loss: 0.205702, acc.: 89.84%] [G loss: 7.100666]\n",
      "epoch:31 step:24979 [D loss: 0.194547, acc.: 92.97%] [G loss: 6.343924]\n",
      "epoch:31 step:24980 [D loss: 0.144369, acc.: 94.53%] [G loss: 5.636641]\n",
      "epoch:31 step:24981 [D loss: 0.251557, acc.: 89.84%] [G loss: 4.427841]\n",
      "epoch:31 step:24982 [D loss: 0.270207, acc.: 89.84%] [G loss: 3.628447]\n",
      "epoch:31 step:24983 [D loss: 0.342011, acc.: 83.59%] [G loss: 4.080344]\n",
      "epoch:31 step:24984 [D loss: 0.402277, acc.: 83.59%] [G loss: 3.418930]\n",
      "epoch:31 step:24985 [D loss: 0.328483, acc.: 83.59%] [G loss: 3.962658]\n",
      "epoch:31 step:24986 [D loss: 0.351814, acc.: 82.03%] [G loss: 4.875439]\n",
      "epoch:31 step:24987 [D loss: 0.367883, acc.: 84.38%] [G loss: 2.559835]\n",
      "epoch:31 step:24988 [D loss: 0.330950, acc.: 84.38%] [G loss: 2.472792]\n",
      "epoch:31 step:24989 [D loss: 0.229069, acc.: 92.19%] [G loss: 3.347105]\n",
      "epoch:31 step:24990 [D loss: 0.226571, acc.: 90.62%] [G loss: 2.848969]\n",
      "epoch:31 step:24991 [D loss: 0.264164, acc.: 89.84%] [G loss: 2.629143]\n",
      "epoch:31 step:24992 [D loss: 0.281977, acc.: 87.50%] [G loss: 3.853768]\n",
      "epoch:32 step:24993 [D loss: 0.403024, acc.: 82.03%] [G loss: 2.720431]\n",
      "epoch:32 step:24994 [D loss: 0.248610, acc.: 90.62%] [G loss: 3.129819]\n",
      "epoch:32 step:24995 [D loss: 0.359301, acc.: 85.16%] [G loss: 4.419518]\n",
      "epoch:32 step:24996 [D loss: 0.320000, acc.: 84.38%] [G loss: 4.031212]\n",
      "epoch:32 step:24997 [D loss: 0.269980, acc.: 90.62%] [G loss: 3.892371]\n",
      "epoch:32 step:24998 [D loss: 0.263304, acc.: 87.50%] [G loss: 2.751411]\n",
      "epoch:32 step:24999 [D loss: 0.217548, acc.: 91.41%] [G loss: 3.514795]\n",
      "epoch:32 step:25000 [D loss: 0.273450, acc.: 89.06%] [G loss: 3.807415]\n",
      "epoch:32 step:25001 [D loss: 0.262603, acc.: 89.84%] [G loss: 2.715886]\n",
      "epoch:32 step:25002 [D loss: 0.356710, acc.: 84.38%] [G loss: 3.634823]\n",
      "epoch:32 step:25003 [D loss: 0.364972, acc.: 78.91%] [G loss: 2.979448]\n",
      "epoch:32 step:25004 [D loss: 0.243664, acc.: 85.16%] [G loss: 3.651536]\n",
      "epoch:32 step:25005 [D loss: 0.224209, acc.: 92.19%] [G loss: 3.695256]\n",
      "epoch:32 step:25006 [D loss: 0.276452, acc.: 88.28%] [G loss: 4.026178]\n",
      "epoch:32 step:25007 [D loss: 0.282333, acc.: 85.16%] [G loss: 3.940768]\n",
      "epoch:32 step:25008 [D loss: 0.255009, acc.: 89.84%] [G loss: 3.783950]\n",
      "epoch:32 step:25009 [D loss: 0.290323, acc.: 87.50%] [G loss: 4.400542]\n",
      "epoch:32 step:25010 [D loss: 0.380243, acc.: 78.91%] [G loss: 3.696303]\n",
      "epoch:32 step:25011 [D loss: 0.259934, acc.: 90.62%] [G loss: 3.539683]\n",
      "epoch:32 step:25012 [D loss: 0.311052, acc.: 82.81%] [G loss: 3.080932]\n",
      "epoch:32 step:25013 [D loss: 0.349388, acc.: 84.38%] [G loss: 2.567332]\n",
      "epoch:32 step:25014 [D loss: 0.350470, acc.: 85.16%] [G loss: 2.788886]\n",
      "epoch:32 step:25015 [D loss: 0.263029, acc.: 88.28%] [G loss: 3.230298]\n",
      "epoch:32 step:25016 [D loss: 0.279159, acc.: 91.41%] [G loss: 3.291666]\n",
      "epoch:32 step:25017 [D loss: 0.336568, acc.: 85.16%] [G loss: 2.073529]\n",
      "epoch:32 step:25018 [D loss: 0.314251, acc.: 85.16%] [G loss: 2.721741]\n",
      "epoch:32 step:25019 [D loss: 0.324736, acc.: 83.59%] [G loss: 2.461310]\n",
      "epoch:32 step:25020 [D loss: 0.424990, acc.: 72.66%] [G loss: 4.620173]\n",
      "epoch:32 step:25021 [D loss: 0.515645, acc.: 78.91%] [G loss: 5.911271]\n",
      "epoch:32 step:25022 [D loss: 0.242788, acc.: 91.41%] [G loss: 4.011813]\n",
      "epoch:32 step:25023 [D loss: 0.376083, acc.: 85.16%] [G loss: 7.488610]\n",
      "epoch:32 step:25024 [D loss: 0.745727, acc.: 66.41%] [G loss: 4.824595]\n",
      "epoch:32 step:25025 [D loss: 0.413748, acc.: 82.03%] [G loss: 3.645147]\n",
      "epoch:32 step:25026 [D loss: 0.705084, acc.: 75.00%] [G loss: 2.904671]\n",
      "epoch:32 step:25027 [D loss: 0.478664, acc.: 75.78%] [G loss: 3.048971]\n",
      "epoch:32 step:25028 [D loss: 0.421374, acc.: 83.59%] [G loss: 3.854045]\n",
      "epoch:32 step:25029 [D loss: 0.357259, acc.: 85.16%] [G loss: 3.390928]\n",
      "epoch:32 step:25030 [D loss: 0.300491, acc.: 82.81%] [G loss: 3.380700]\n",
      "epoch:32 step:25031 [D loss: 0.297424, acc.: 85.94%] [G loss: 4.372260]\n",
      "epoch:32 step:25032 [D loss: 0.372484, acc.: 81.25%] [G loss: 2.785760]\n",
      "epoch:32 step:25033 [D loss: 0.366006, acc.: 84.38%] [G loss: 3.097756]\n",
      "epoch:32 step:25034 [D loss: 0.202723, acc.: 92.97%] [G loss: 3.095262]\n",
      "epoch:32 step:25035 [D loss: 0.430241, acc.: 76.56%] [G loss: 3.101577]\n",
      "epoch:32 step:25036 [D loss: 0.461702, acc.: 78.91%] [G loss: 2.962837]\n",
      "epoch:32 step:25037 [D loss: 0.215201, acc.: 92.19%] [G loss: 2.327075]\n",
      "epoch:32 step:25038 [D loss: 0.329723, acc.: 84.38%] [G loss: 3.371408]\n",
      "epoch:32 step:25039 [D loss: 0.394628, acc.: 84.38%] [G loss: 3.306432]\n",
      "epoch:32 step:25040 [D loss: 0.253190, acc.: 89.06%] [G loss: 3.246533]\n",
      "epoch:32 step:25041 [D loss: 0.269035, acc.: 90.62%] [G loss: 3.851972]\n",
      "epoch:32 step:25042 [D loss: 0.375505, acc.: 81.25%] [G loss: 3.452176]\n",
      "epoch:32 step:25043 [D loss: 0.254770, acc.: 85.94%] [G loss: 4.133591]\n",
      "epoch:32 step:25044 [D loss: 0.336776, acc.: 83.59%] [G loss: 2.859737]\n",
      "epoch:32 step:25045 [D loss: 0.262262, acc.: 89.06%] [G loss: 2.572014]\n",
      "epoch:32 step:25046 [D loss: 0.324512, acc.: 86.72%] [G loss: 2.898476]\n",
      "epoch:32 step:25047 [D loss: 0.387937, acc.: 82.03%] [G loss: 2.942377]\n",
      "epoch:32 step:25048 [D loss: 0.274785, acc.: 88.28%] [G loss: 3.059715]\n",
      "epoch:32 step:25049 [D loss: 0.297336, acc.: 88.28%] [G loss: 3.487653]\n",
      "epoch:32 step:25050 [D loss: 0.418596, acc.: 80.47%] [G loss: 2.618563]\n",
      "epoch:32 step:25051 [D loss: 0.273724, acc.: 85.94%] [G loss: 4.348234]\n",
      "epoch:32 step:25052 [D loss: 0.318125, acc.: 85.94%] [G loss: 3.765336]\n",
      "epoch:32 step:25053 [D loss: 0.300363, acc.: 89.84%] [G loss: 4.220493]\n",
      "epoch:32 step:25054 [D loss: 0.303900, acc.: 85.94%] [G loss: 3.742567]\n",
      "epoch:32 step:25055 [D loss: 0.223829, acc.: 92.19%] [G loss: 4.668779]\n",
      "epoch:32 step:25056 [D loss: 0.290348, acc.: 85.16%] [G loss: 3.936254]\n",
      "epoch:32 step:25057 [D loss: 0.241896, acc.: 91.41%] [G loss: 6.381003]\n",
      "epoch:32 step:25058 [D loss: 0.256925, acc.: 89.06%] [G loss: 3.591139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25059 [D loss: 0.270044, acc.: 87.50%] [G loss: 4.092970]\n",
      "epoch:32 step:25060 [D loss: 0.303523, acc.: 89.06%] [G loss: 4.189798]\n",
      "epoch:32 step:25061 [D loss: 0.217608, acc.: 89.06%] [G loss: 3.908812]\n",
      "epoch:32 step:25062 [D loss: 0.260729, acc.: 88.28%] [G loss: 3.433801]\n",
      "epoch:32 step:25063 [D loss: 0.295328, acc.: 84.38%] [G loss: 3.676711]\n",
      "epoch:32 step:25064 [D loss: 0.283857, acc.: 89.06%] [G loss: 3.493022]\n",
      "epoch:32 step:25065 [D loss: 0.283343, acc.: 88.28%] [G loss: 3.483460]\n",
      "epoch:32 step:25066 [D loss: 0.318463, acc.: 87.50%] [G loss: 2.522415]\n",
      "epoch:32 step:25067 [D loss: 0.303112, acc.: 90.62%] [G loss: 2.690007]\n",
      "epoch:32 step:25068 [D loss: 0.317856, acc.: 87.50%] [G loss: 2.659823]\n",
      "epoch:32 step:25069 [D loss: 0.373530, acc.: 85.16%] [G loss: 2.956327]\n",
      "epoch:32 step:25070 [D loss: 0.341598, acc.: 85.16%] [G loss: 3.680372]\n",
      "epoch:32 step:25071 [D loss: 0.338852, acc.: 83.59%] [G loss: 2.959302]\n",
      "epoch:32 step:25072 [D loss: 0.307725, acc.: 85.94%] [G loss: 2.947460]\n",
      "epoch:32 step:25073 [D loss: 0.301226, acc.: 83.59%] [G loss: 4.224693]\n",
      "epoch:32 step:25074 [D loss: 0.333060, acc.: 83.59%] [G loss: 4.281166]\n",
      "epoch:32 step:25075 [D loss: 0.154090, acc.: 97.66%] [G loss: 6.333076]\n",
      "epoch:32 step:25076 [D loss: 0.277466, acc.: 87.50%] [G loss: 5.920438]\n",
      "epoch:32 step:25077 [D loss: 0.264000, acc.: 94.53%] [G loss: 4.187495]\n",
      "epoch:32 step:25078 [D loss: 0.229402, acc.: 92.19%] [G loss: 4.505744]\n",
      "epoch:32 step:25079 [D loss: 0.317739, acc.: 86.72%] [G loss: 4.293271]\n",
      "epoch:32 step:25080 [D loss: 0.321971, acc.: 86.72%] [G loss: 4.463668]\n",
      "epoch:32 step:25081 [D loss: 0.424854, acc.: 82.03%] [G loss: 4.829477]\n",
      "epoch:32 step:25082 [D loss: 0.471509, acc.: 74.22%] [G loss: 3.538551]\n",
      "epoch:32 step:25083 [D loss: 0.278396, acc.: 86.72%] [G loss: 4.340879]\n",
      "epoch:32 step:25084 [D loss: 0.305897, acc.: 88.28%] [G loss: 3.678387]\n",
      "epoch:32 step:25085 [D loss: 0.303827, acc.: 85.94%] [G loss: 3.247615]\n",
      "epoch:32 step:25086 [D loss: 0.270793, acc.: 87.50%] [G loss: 3.976988]\n",
      "epoch:32 step:25087 [D loss: 0.327777, acc.: 85.16%] [G loss: 3.574345]\n",
      "epoch:32 step:25088 [D loss: 0.307013, acc.: 85.16%] [G loss: 3.541342]\n",
      "epoch:32 step:25089 [D loss: 0.234563, acc.: 89.84%] [G loss: 3.528230]\n",
      "epoch:32 step:25090 [D loss: 0.243231, acc.: 88.28%] [G loss: 3.415186]\n",
      "epoch:32 step:25091 [D loss: 0.265341, acc.: 87.50%] [G loss: 5.008015]\n",
      "epoch:32 step:25092 [D loss: 0.296517, acc.: 81.25%] [G loss: 3.502589]\n",
      "epoch:32 step:25093 [D loss: 0.344219, acc.: 85.16%] [G loss: 3.725721]\n",
      "epoch:32 step:25094 [D loss: 0.264906, acc.: 88.28%] [G loss: 4.349544]\n",
      "epoch:32 step:25095 [D loss: 0.281642, acc.: 88.28%] [G loss: 2.874001]\n",
      "epoch:32 step:25096 [D loss: 0.419059, acc.: 82.03%] [G loss: 3.230101]\n",
      "epoch:32 step:25097 [D loss: 0.344261, acc.: 84.38%] [G loss: 2.973775]\n",
      "epoch:32 step:25098 [D loss: 0.277189, acc.: 85.94%] [G loss: 3.830512]\n",
      "epoch:32 step:25099 [D loss: 0.255641, acc.: 89.06%] [G loss: 4.137400]\n",
      "epoch:32 step:25100 [D loss: 0.302956, acc.: 89.84%] [G loss: 3.132632]\n",
      "epoch:32 step:25101 [D loss: 0.390408, acc.: 83.59%] [G loss: 3.880710]\n",
      "epoch:32 step:25102 [D loss: 0.292176, acc.: 83.59%] [G loss: 2.678870]\n",
      "epoch:32 step:25103 [D loss: 0.331806, acc.: 85.94%] [G loss: 2.883028]\n",
      "epoch:32 step:25104 [D loss: 0.313086, acc.: 84.38%] [G loss: 3.110801]\n",
      "epoch:32 step:25105 [D loss: 0.371944, acc.: 82.03%] [G loss: 3.088167]\n",
      "epoch:32 step:25106 [D loss: 0.347408, acc.: 85.94%] [G loss: 3.084433]\n",
      "epoch:32 step:25107 [D loss: 0.355899, acc.: 85.16%] [G loss: 2.598258]\n",
      "epoch:32 step:25108 [D loss: 0.331106, acc.: 85.94%] [G loss: 3.122654]\n",
      "epoch:32 step:25109 [D loss: 0.276632, acc.: 90.62%] [G loss: 3.240557]\n",
      "epoch:32 step:25110 [D loss: 0.229869, acc.: 91.41%] [G loss: 3.268306]\n",
      "epoch:32 step:25111 [D loss: 0.248265, acc.: 92.19%] [G loss: 2.946905]\n",
      "epoch:32 step:25112 [D loss: 0.301465, acc.: 89.06%] [G loss: 2.656773]\n",
      "epoch:32 step:25113 [D loss: 0.243874, acc.: 92.97%] [G loss: 2.523608]\n",
      "epoch:32 step:25114 [D loss: 0.235715, acc.: 90.62%] [G loss: 3.096907]\n",
      "epoch:32 step:25115 [D loss: 0.299542, acc.: 86.72%] [G loss: 3.002028]\n",
      "epoch:32 step:25116 [D loss: 0.346788, acc.: 88.28%] [G loss: 2.949748]\n",
      "epoch:32 step:25117 [D loss: 0.416248, acc.: 78.91%] [G loss: 2.796747]\n",
      "epoch:32 step:25118 [D loss: 0.264513, acc.: 89.84%] [G loss: 3.511788]\n",
      "epoch:32 step:25119 [D loss: 0.301058, acc.: 89.84%] [G loss: 3.101129]\n",
      "epoch:32 step:25120 [D loss: 0.258218, acc.: 88.28%] [G loss: 4.349908]\n",
      "epoch:32 step:25121 [D loss: 0.316407, acc.: 85.94%] [G loss: 3.161192]\n",
      "epoch:32 step:25122 [D loss: 0.289082, acc.: 88.28%] [G loss: 3.202588]\n",
      "epoch:32 step:25123 [D loss: 0.299586, acc.: 88.28%] [G loss: 3.297279]\n",
      "epoch:32 step:25124 [D loss: 0.299988, acc.: 86.72%] [G loss: 3.144581]\n",
      "epoch:32 step:25125 [D loss: 0.265847, acc.: 89.06%] [G loss: 3.275788]\n",
      "epoch:32 step:25126 [D loss: 0.209877, acc.: 92.19%] [G loss: 3.420105]\n",
      "epoch:32 step:25127 [D loss: 0.350860, acc.: 85.94%] [G loss: 4.485814]\n",
      "epoch:32 step:25128 [D loss: 0.256271, acc.: 86.72%] [G loss: 3.519404]\n",
      "epoch:32 step:25129 [D loss: 0.335843, acc.: 85.94%] [G loss: 3.509726]\n",
      "epoch:32 step:25130 [D loss: 0.310881, acc.: 85.94%] [G loss: 3.048425]\n",
      "epoch:32 step:25131 [D loss: 0.202741, acc.: 90.62%] [G loss: 3.044632]\n",
      "epoch:32 step:25132 [D loss: 0.330307, acc.: 87.50%] [G loss: 3.630319]\n",
      "epoch:32 step:25133 [D loss: 0.308560, acc.: 86.72%] [G loss: 3.852165]\n",
      "epoch:32 step:25134 [D loss: 0.320916, acc.: 82.81%] [G loss: 3.199665]\n",
      "epoch:32 step:25135 [D loss: 0.209209, acc.: 92.19%] [G loss: 2.667136]\n",
      "epoch:32 step:25136 [D loss: 0.333595, acc.: 86.72%] [G loss: 3.133324]\n",
      "epoch:32 step:25137 [D loss: 0.275681, acc.: 86.72%] [G loss: 3.099734]\n",
      "epoch:32 step:25138 [D loss: 0.316775, acc.: 84.38%] [G loss: 3.943233]\n",
      "epoch:32 step:25139 [D loss: 0.369141, acc.: 82.03%] [G loss: 2.990129]\n",
      "epoch:32 step:25140 [D loss: 0.396845, acc.: 82.03%] [G loss: 3.246536]\n",
      "epoch:32 step:25141 [D loss: 0.369373, acc.: 83.59%] [G loss: 4.974617]\n",
      "epoch:32 step:25142 [D loss: 0.361487, acc.: 82.03%] [G loss: 6.434214]\n",
      "epoch:32 step:25143 [D loss: 0.492117, acc.: 75.78%] [G loss: 3.641698]\n",
      "epoch:32 step:25144 [D loss: 0.281910, acc.: 86.72%] [G loss: 4.300004]\n",
      "epoch:32 step:25145 [D loss: 0.171729, acc.: 92.97%] [G loss: 3.751722]\n",
      "epoch:32 step:25146 [D loss: 0.213998, acc.: 89.06%] [G loss: 3.807527]\n",
      "epoch:32 step:25147 [D loss: 0.358204, acc.: 84.38%] [G loss: 4.184322]\n",
      "epoch:32 step:25148 [D loss: 0.214010, acc.: 92.19%] [G loss: 3.968206]\n",
      "epoch:32 step:25149 [D loss: 0.327931, acc.: 85.16%] [G loss: 3.688519]\n",
      "epoch:32 step:25150 [D loss: 0.302531, acc.: 86.72%] [G loss: 4.792946]\n",
      "epoch:32 step:25151 [D loss: 0.271225, acc.: 85.16%] [G loss: 3.479221]\n",
      "epoch:32 step:25152 [D loss: 0.350217, acc.: 84.38%] [G loss: 3.564833]\n",
      "epoch:32 step:25153 [D loss: 0.446228, acc.: 76.56%] [G loss: 5.024665]\n",
      "epoch:32 step:25154 [D loss: 0.340649, acc.: 86.72%] [G loss: 3.327692]\n",
      "epoch:32 step:25155 [D loss: 0.260331, acc.: 89.06%] [G loss: 4.035369]\n",
      "epoch:32 step:25156 [D loss: 0.305029, acc.: 85.94%] [G loss: 3.329381]\n",
      "epoch:32 step:25157 [D loss: 0.283404, acc.: 84.38%] [G loss: 4.798833]\n",
      "epoch:32 step:25158 [D loss: 0.334769, acc.: 85.16%] [G loss: 3.776293]\n",
      "epoch:32 step:25159 [D loss: 0.328242, acc.: 85.94%] [G loss: 3.519990]\n",
      "epoch:32 step:25160 [D loss: 0.330640, acc.: 85.94%] [G loss: 2.990676]\n",
      "epoch:32 step:25161 [D loss: 0.320676, acc.: 84.38%] [G loss: 2.928931]\n",
      "epoch:32 step:25162 [D loss: 0.375324, acc.: 78.91%] [G loss: 5.829834]\n",
      "epoch:32 step:25163 [D loss: 0.392506, acc.: 83.59%] [G loss: 5.751455]\n",
      "epoch:32 step:25164 [D loss: 0.277563, acc.: 85.94%] [G loss: 3.766181]\n",
      "epoch:32 step:25165 [D loss: 0.256475, acc.: 88.28%] [G loss: 4.826105]\n",
      "epoch:32 step:25166 [D loss: 0.374151, acc.: 85.94%] [G loss: 3.600909]\n",
      "epoch:32 step:25167 [D loss: 0.367333, acc.: 84.38%] [G loss: 4.877903]\n",
      "epoch:32 step:25168 [D loss: 0.223576, acc.: 91.41%] [G loss: 4.570035]\n",
      "epoch:32 step:25169 [D loss: 0.189177, acc.: 92.97%] [G loss: 4.193273]\n",
      "epoch:32 step:25170 [D loss: 0.262794, acc.: 87.50%] [G loss: 3.458986]\n",
      "epoch:32 step:25171 [D loss: 0.316272, acc.: 86.72%] [G loss: 4.466846]\n",
      "epoch:32 step:25172 [D loss: 0.348262, acc.: 82.81%] [G loss: 4.991690]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25173 [D loss: 0.309894, acc.: 88.28%] [G loss: 3.847588]\n",
      "epoch:32 step:25174 [D loss: 0.252530, acc.: 89.06%] [G loss: 4.266095]\n",
      "epoch:32 step:25175 [D loss: 0.327569, acc.: 86.72%] [G loss: 3.546642]\n",
      "epoch:32 step:25176 [D loss: 0.332689, acc.: 85.16%] [G loss: 3.569784]\n",
      "epoch:32 step:25177 [D loss: 0.218939, acc.: 89.06%] [G loss: 3.348969]\n",
      "epoch:32 step:25178 [D loss: 0.293342, acc.: 85.16%] [G loss: 3.628787]\n",
      "epoch:32 step:25179 [D loss: 0.447204, acc.: 77.34%] [G loss: 3.223087]\n",
      "epoch:32 step:25180 [D loss: 0.314637, acc.: 85.16%] [G loss: 4.349908]\n",
      "epoch:32 step:25181 [D loss: 0.268680, acc.: 85.94%] [G loss: 3.222746]\n",
      "epoch:32 step:25182 [D loss: 0.371356, acc.: 83.59%] [G loss: 3.792544]\n",
      "epoch:32 step:25183 [D loss: 0.342182, acc.: 88.28%] [G loss: 4.193614]\n",
      "epoch:32 step:25184 [D loss: 0.271311, acc.: 89.06%] [G loss: 2.711266]\n",
      "epoch:32 step:25185 [D loss: 0.336666, acc.: 87.50%] [G loss: 3.029611]\n",
      "epoch:32 step:25186 [D loss: 0.259762, acc.: 88.28%] [G loss: 3.815042]\n",
      "epoch:32 step:25187 [D loss: 0.341715, acc.: 85.94%] [G loss: 3.589910]\n",
      "epoch:32 step:25188 [D loss: 0.239479, acc.: 90.62%] [G loss: 4.345385]\n",
      "epoch:32 step:25189 [D loss: 0.426451, acc.: 81.25%] [G loss: 3.327356]\n",
      "epoch:32 step:25190 [D loss: 0.180131, acc.: 95.31%] [G loss: 4.980092]\n",
      "epoch:32 step:25191 [D loss: 0.264093, acc.: 89.06%] [G loss: 3.392915]\n",
      "epoch:32 step:25192 [D loss: 0.281581, acc.: 89.84%] [G loss: 3.946839]\n",
      "epoch:32 step:25193 [D loss: 0.250068, acc.: 89.06%] [G loss: 3.437565]\n",
      "epoch:32 step:25194 [D loss: 0.241428, acc.: 89.84%] [G loss: 4.225965]\n",
      "epoch:32 step:25195 [D loss: 0.186002, acc.: 92.19%] [G loss: 5.130437]\n",
      "epoch:32 step:25196 [D loss: 0.285489, acc.: 87.50%] [G loss: 3.876537]\n",
      "epoch:32 step:25197 [D loss: 0.298451, acc.: 85.94%] [G loss: 3.508755]\n",
      "epoch:32 step:25198 [D loss: 0.223989, acc.: 91.41%] [G loss: 3.520484]\n",
      "epoch:32 step:25199 [D loss: 0.306840, acc.: 88.28%] [G loss: 3.155115]\n",
      "epoch:32 step:25200 [D loss: 0.404669, acc.: 84.38%] [G loss: 3.635373]\n",
      "epoch:32 step:25201 [D loss: 0.198691, acc.: 93.75%] [G loss: 2.816463]\n",
      "epoch:32 step:25202 [D loss: 0.432335, acc.: 79.69%] [G loss: 3.138574]\n",
      "epoch:32 step:25203 [D loss: 0.333302, acc.: 87.50%] [G loss: 3.086951]\n",
      "epoch:32 step:25204 [D loss: 0.257615, acc.: 90.62%] [G loss: 3.709421]\n",
      "epoch:32 step:25205 [D loss: 0.270640, acc.: 87.50%] [G loss: 3.305664]\n",
      "epoch:32 step:25206 [D loss: 0.447272, acc.: 79.69%] [G loss: 2.969771]\n",
      "epoch:32 step:25207 [D loss: 0.232564, acc.: 91.41%] [G loss: 3.522274]\n",
      "epoch:32 step:25208 [D loss: 0.303150, acc.: 86.72%] [G loss: 2.937179]\n",
      "epoch:32 step:25209 [D loss: 0.272100, acc.: 89.06%] [G loss: 3.347496]\n",
      "epoch:32 step:25210 [D loss: 0.386246, acc.: 82.03%] [G loss: 2.356818]\n",
      "epoch:32 step:25211 [D loss: 0.447792, acc.: 82.81%] [G loss: 2.573932]\n",
      "epoch:32 step:25212 [D loss: 0.327871, acc.: 80.47%] [G loss: 4.357875]\n",
      "epoch:32 step:25213 [D loss: 0.347796, acc.: 85.94%] [G loss: 2.730164]\n",
      "epoch:32 step:25214 [D loss: 0.416260, acc.: 77.34%] [G loss: 3.022575]\n",
      "epoch:32 step:25215 [D loss: 0.356183, acc.: 82.81%] [G loss: 3.480340]\n",
      "epoch:32 step:25216 [D loss: 0.366728, acc.: 84.38%] [G loss: 3.517808]\n",
      "epoch:32 step:25217 [D loss: 0.309687, acc.: 85.94%] [G loss: 3.719702]\n",
      "epoch:32 step:25218 [D loss: 0.435800, acc.: 78.12%] [G loss: 3.037075]\n",
      "epoch:32 step:25219 [D loss: 0.321430, acc.: 85.94%] [G loss: 3.762438]\n",
      "epoch:32 step:25220 [D loss: 0.426362, acc.: 82.81%] [G loss: 2.729449]\n",
      "epoch:32 step:25221 [D loss: 0.384436, acc.: 82.03%] [G loss: 3.519100]\n",
      "epoch:32 step:25222 [D loss: 0.425199, acc.: 81.25%] [G loss: 3.098563]\n",
      "epoch:32 step:25223 [D loss: 0.299162, acc.: 88.28%] [G loss: 3.688524]\n",
      "epoch:32 step:25224 [D loss: 0.231904, acc.: 89.06%] [G loss: 3.352730]\n",
      "epoch:32 step:25225 [D loss: 0.420973, acc.: 83.59%] [G loss: 3.124073]\n",
      "epoch:32 step:25226 [D loss: 0.341199, acc.: 80.47%] [G loss: 2.297050]\n",
      "epoch:32 step:25227 [D loss: 0.261323, acc.: 91.41%] [G loss: 4.268619]\n",
      "epoch:32 step:25228 [D loss: 0.295488, acc.: 85.94%] [G loss: 3.383870]\n",
      "epoch:32 step:25229 [D loss: 0.289884, acc.: 85.16%] [G loss: 3.556052]\n",
      "epoch:32 step:25230 [D loss: 0.408906, acc.: 83.59%] [G loss: 5.924482]\n",
      "epoch:32 step:25231 [D loss: 0.676651, acc.: 74.22%] [G loss: 8.083252]\n",
      "epoch:32 step:25232 [D loss: 2.118822, acc.: 54.69%] [G loss: 10.245790]\n",
      "epoch:32 step:25233 [D loss: 1.829063, acc.: 71.09%] [G loss: 5.110836]\n",
      "epoch:32 step:25234 [D loss: 0.500438, acc.: 83.59%] [G loss: 5.164270]\n",
      "epoch:32 step:25235 [D loss: 0.561803, acc.: 80.47%] [G loss: 5.858536]\n",
      "epoch:32 step:25236 [D loss: 0.444389, acc.: 86.72%] [G loss: 3.700688]\n",
      "epoch:32 step:25237 [D loss: 0.451081, acc.: 80.47%] [G loss: 3.923654]\n",
      "epoch:32 step:25238 [D loss: 0.387362, acc.: 80.47%] [G loss: 3.041857]\n",
      "epoch:32 step:25239 [D loss: 0.293191, acc.: 85.94%] [G loss: 3.504492]\n",
      "epoch:32 step:25240 [D loss: 0.266863, acc.: 86.72%] [G loss: 4.410844]\n",
      "epoch:32 step:25241 [D loss: 0.375320, acc.: 83.59%] [G loss: 3.163120]\n",
      "epoch:32 step:25242 [D loss: 0.298400, acc.: 86.72%] [G loss: 4.111906]\n",
      "epoch:32 step:25243 [D loss: 0.359440, acc.: 84.38%] [G loss: 2.752806]\n",
      "epoch:32 step:25244 [D loss: 0.359845, acc.: 83.59%] [G loss: 3.386624]\n",
      "epoch:32 step:25245 [D loss: 0.362327, acc.: 84.38%] [G loss: 3.339692]\n",
      "epoch:32 step:25246 [D loss: 0.275811, acc.: 87.50%] [G loss: 2.624579]\n",
      "epoch:32 step:25247 [D loss: 0.375581, acc.: 85.16%] [G loss: 3.680175]\n",
      "epoch:32 step:25248 [D loss: 0.338881, acc.: 82.81%] [G loss: 3.965536]\n",
      "epoch:32 step:25249 [D loss: 0.330448, acc.: 86.72%] [G loss: 3.218170]\n",
      "epoch:32 step:25250 [D loss: 0.345875, acc.: 82.81%] [G loss: 3.126031]\n",
      "epoch:32 step:25251 [D loss: 0.337885, acc.: 84.38%] [G loss: 3.091893]\n",
      "epoch:32 step:25252 [D loss: 0.251331, acc.: 88.28%] [G loss: 2.456965]\n",
      "epoch:32 step:25253 [D loss: 0.376172, acc.: 84.38%] [G loss: 3.219848]\n",
      "epoch:32 step:25254 [D loss: 0.367663, acc.: 82.03%] [G loss: 3.054087]\n",
      "epoch:32 step:25255 [D loss: 0.185483, acc.: 90.62%] [G loss: 3.856486]\n",
      "epoch:32 step:25256 [D loss: 0.328623, acc.: 83.59%] [G loss: 2.524871]\n",
      "epoch:32 step:25257 [D loss: 0.237541, acc.: 90.62%] [G loss: 2.883096]\n",
      "epoch:32 step:25258 [D loss: 0.345028, acc.: 85.16%] [G loss: 2.858696]\n",
      "epoch:32 step:25259 [D loss: 0.337845, acc.: 81.25%] [G loss: 2.073677]\n",
      "epoch:32 step:25260 [D loss: 0.316610, acc.: 84.38%] [G loss: 2.695205]\n",
      "epoch:32 step:25261 [D loss: 0.301066, acc.: 87.50%] [G loss: 3.154455]\n",
      "epoch:32 step:25262 [D loss: 0.350365, acc.: 83.59%] [G loss: 2.951416]\n",
      "epoch:32 step:25263 [D loss: 0.269097, acc.: 86.72%] [G loss: 3.735112]\n",
      "epoch:32 step:25264 [D loss: 0.321527, acc.: 87.50%] [G loss: 4.102646]\n",
      "epoch:32 step:25265 [D loss: 0.286364, acc.: 85.94%] [G loss: 3.400442]\n",
      "epoch:32 step:25266 [D loss: 0.287542, acc.: 86.72%] [G loss: 3.162327]\n",
      "epoch:32 step:25267 [D loss: 0.529668, acc.: 78.12%] [G loss: 3.250808]\n",
      "epoch:32 step:25268 [D loss: 0.484298, acc.: 75.78%] [G loss: 3.373592]\n",
      "epoch:32 step:25269 [D loss: 0.320993, acc.: 82.81%] [G loss: 3.152167]\n",
      "epoch:32 step:25270 [D loss: 0.318171, acc.: 84.38%] [G loss: 4.268598]\n",
      "epoch:32 step:25271 [D loss: 0.349482, acc.: 82.81%] [G loss: 3.585896]\n",
      "epoch:32 step:25272 [D loss: 0.244712, acc.: 89.84%] [G loss: 3.721992]\n",
      "epoch:32 step:25273 [D loss: 0.264649, acc.: 85.16%] [G loss: 2.906958]\n",
      "epoch:32 step:25274 [D loss: 0.334420, acc.: 85.16%] [G loss: 3.378476]\n",
      "epoch:32 step:25275 [D loss: 0.283117, acc.: 89.06%] [G loss: 3.393273]\n",
      "epoch:32 step:25276 [D loss: 0.327310, acc.: 85.16%] [G loss: 3.854969]\n",
      "epoch:32 step:25277 [D loss: 0.363015, acc.: 83.59%] [G loss: 3.095596]\n",
      "epoch:32 step:25278 [D loss: 0.353821, acc.: 84.38%] [G loss: 3.507936]\n",
      "epoch:32 step:25279 [D loss: 0.284696, acc.: 85.94%] [G loss: 3.500365]\n",
      "epoch:32 step:25280 [D loss: 0.337021, acc.: 84.38%] [G loss: 3.461331]\n",
      "epoch:32 step:25281 [D loss: 0.330743, acc.: 85.16%] [G loss: 2.841214]\n",
      "epoch:32 step:25282 [D loss: 0.261801, acc.: 89.84%] [G loss: 3.028463]\n",
      "epoch:32 step:25283 [D loss: 0.236342, acc.: 91.41%] [G loss: 3.031143]\n",
      "epoch:32 step:25284 [D loss: 0.251364, acc.: 88.28%] [G loss: 3.019868]\n",
      "epoch:32 step:25285 [D loss: 0.244588, acc.: 89.84%] [G loss: 3.056772]\n",
      "epoch:32 step:25286 [D loss: 0.294153, acc.: 87.50%] [G loss: 2.958956]\n",
      "epoch:32 step:25287 [D loss: 0.440267, acc.: 78.12%] [G loss: 2.247678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25288 [D loss: 0.359913, acc.: 85.94%] [G loss: 2.372995]\n",
      "epoch:32 step:25289 [D loss: 0.245931, acc.: 92.19%] [G loss: 2.986168]\n",
      "epoch:32 step:25290 [D loss: 0.348770, acc.: 79.69%] [G loss: 3.275758]\n",
      "epoch:32 step:25291 [D loss: 0.282525, acc.: 88.28%] [G loss: 3.781408]\n",
      "epoch:32 step:25292 [D loss: 0.377673, acc.: 83.59%] [G loss: 2.816240]\n",
      "epoch:32 step:25293 [D loss: 0.276870, acc.: 87.50%] [G loss: 4.828048]\n",
      "epoch:32 step:25294 [D loss: 0.214690, acc.: 91.41%] [G loss: 5.531417]\n",
      "epoch:32 step:25295 [D loss: 0.265246, acc.: 85.16%] [G loss: 10.006420]\n",
      "epoch:32 step:25296 [D loss: 0.197273, acc.: 93.75%] [G loss: 4.142888]\n",
      "epoch:32 step:25297 [D loss: 0.271019, acc.: 91.41%] [G loss: 4.301409]\n",
      "epoch:32 step:25298 [D loss: 0.294836, acc.: 82.81%] [G loss: 4.005823]\n",
      "epoch:32 step:25299 [D loss: 0.184497, acc.: 92.97%] [G loss: 4.828421]\n",
      "epoch:32 step:25300 [D loss: 0.288584, acc.: 86.72%] [G loss: 3.955313]\n",
      "epoch:32 step:25301 [D loss: 0.325808, acc.: 84.38%] [G loss: 3.117630]\n",
      "epoch:32 step:25302 [D loss: 0.264623, acc.: 87.50%] [G loss: 3.576615]\n",
      "epoch:32 step:25303 [D loss: 0.308200, acc.: 87.50%] [G loss: 3.214874]\n",
      "epoch:32 step:25304 [D loss: 0.459762, acc.: 81.25%] [G loss: 3.439236]\n",
      "epoch:32 step:25305 [D loss: 0.400183, acc.: 78.12%] [G loss: 3.189666]\n",
      "epoch:32 step:25306 [D loss: 0.366982, acc.: 84.38%] [G loss: 3.014832]\n",
      "epoch:32 step:25307 [D loss: 0.358406, acc.: 82.81%] [G loss: 3.791283]\n",
      "epoch:32 step:25308 [D loss: 0.299728, acc.: 89.06%] [G loss: 3.326183]\n",
      "epoch:32 step:25309 [D loss: 0.349805, acc.: 84.38%] [G loss: 3.112803]\n",
      "epoch:32 step:25310 [D loss: 0.446944, acc.: 79.69%] [G loss: 2.935459]\n",
      "epoch:32 step:25311 [D loss: 0.242039, acc.: 89.06%] [G loss: 3.412803]\n",
      "epoch:32 step:25312 [D loss: 0.341956, acc.: 82.81%] [G loss: 2.830844]\n",
      "epoch:32 step:25313 [D loss: 0.351765, acc.: 85.16%] [G loss: 3.195502]\n",
      "epoch:32 step:25314 [D loss: 0.282409, acc.: 90.62%] [G loss: 4.518269]\n",
      "epoch:32 step:25315 [D loss: 0.199982, acc.: 93.75%] [G loss: 4.406826]\n",
      "epoch:32 step:25316 [D loss: 0.266881, acc.: 87.50%] [G loss: 4.550835]\n",
      "epoch:32 step:25317 [D loss: 0.259598, acc.: 89.06%] [G loss: 4.714781]\n",
      "epoch:32 step:25318 [D loss: 0.209695, acc.: 92.19%] [G loss: 5.354115]\n",
      "epoch:32 step:25319 [D loss: 0.301458, acc.: 86.72%] [G loss: 4.765817]\n",
      "epoch:32 step:25320 [D loss: 0.216456, acc.: 93.75%] [G loss: 3.512664]\n",
      "epoch:32 step:25321 [D loss: 0.322291, acc.: 86.72%] [G loss: 3.134912]\n",
      "epoch:32 step:25322 [D loss: 0.264624, acc.: 90.62%] [G loss: 3.220865]\n",
      "epoch:32 step:25323 [D loss: 0.345312, acc.: 84.38%] [G loss: 3.852206]\n",
      "epoch:32 step:25324 [D loss: 0.347857, acc.: 83.59%] [G loss: 2.800482]\n",
      "epoch:32 step:25325 [D loss: 0.322139, acc.: 84.38%] [G loss: 3.000717]\n",
      "epoch:32 step:25326 [D loss: 0.323968, acc.: 85.94%] [G loss: 2.624036]\n",
      "epoch:32 step:25327 [D loss: 0.347918, acc.: 82.03%] [G loss: 2.478287]\n",
      "epoch:32 step:25328 [D loss: 0.299403, acc.: 84.38%] [G loss: 2.453804]\n",
      "epoch:32 step:25329 [D loss: 0.350905, acc.: 82.81%] [G loss: 2.259485]\n",
      "epoch:32 step:25330 [D loss: 0.331235, acc.: 84.38%] [G loss: 2.367012]\n",
      "epoch:32 step:25331 [D loss: 0.294113, acc.: 89.84%] [G loss: 2.778600]\n",
      "epoch:32 step:25332 [D loss: 0.353279, acc.: 82.03%] [G loss: 2.490435]\n",
      "epoch:32 step:25333 [D loss: 0.248188, acc.: 92.19%] [G loss: 2.857591]\n",
      "epoch:32 step:25334 [D loss: 0.315179, acc.: 89.06%] [G loss: 2.839914]\n",
      "epoch:32 step:25335 [D loss: 0.309897, acc.: 87.50%] [G loss: 3.560005]\n",
      "epoch:32 step:25336 [D loss: 0.286037, acc.: 89.84%] [G loss: 4.101475]\n",
      "epoch:32 step:25337 [D loss: 0.283399, acc.: 85.94%] [G loss: 3.648209]\n",
      "epoch:32 step:25338 [D loss: 0.269506, acc.: 88.28%] [G loss: 3.412193]\n",
      "epoch:32 step:25339 [D loss: 0.334859, acc.: 83.59%] [G loss: 4.120346]\n",
      "epoch:32 step:25340 [D loss: 0.368224, acc.: 82.03%] [G loss: 3.478498]\n",
      "epoch:32 step:25341 [D loss: 0.274470, acc.: 88.28%] [G loss: 3.907979]\n",
      "epoch:32 step:25342 [D loss: 0.412047, acc.: 78.91%] [G loss: 3.392836]\n",
      "epoch:32 step:25343 [D loss: 0.196000, acc.: 95.31%] [G loss: 3.379847]\n",
      "epoch:32 step:25344 [D loss: 0.304270, acc.: 83.59%] [G loss: 3.345852]\n",
      "epoch:32 step:25345 [D loss: 0.380863, acc.: 85.16%] [G loss: 3.763364]\n",
      "epoch:32 step:25346 [D loss: 0.265435, acc.: 87.50%] [G loss: 3.681760]\n",
      "epoch:32 step:25347 [D loss: 0.288011, acc.: 89.06%] [G loss: 3.807458]\n",
      "epoch:32 step:25348 [D loss: 0.201942, acc.: 92.19%] [G loss: 3.475094]\n",
      "epoch:32 step:25349 [D loss: 0.332600, acc.: 82.81%] [G loss: 4.304243]\n",
      "epoch:32 step:25350 [D loss: 0.288222, acc.: 88.28%] [G loss: 4.009825]\n",
      "epoch:32 step:25351 [D loss: 0.259112, acc.: 90.62%] [G loss: 4.269452]\n",
      "epoch:32 step:25352 [D loss: 0.225150, acc.: 89.84%] [G loss: 5.067529]\n",
      "epoch:32 step:25353 [D loss: 0.345462, acc.: 80.47%] [G loss: 3.551877]\n",
      "epoch:32 step:25354 [D loss: 0.212357, acc.: 90.62%] [G loss: 4.178481]\n",
      "epoch:32 step:25355 [D loss: 0.353072, acc.: 78.91%] [G loss: 2.687016]\n",
      "epoch:32 step:25356 [D loss: 0.330504, acc.: 84.38%] [G loss: 3.474823]\n",
      "epoch:32 step:25357 [D loss: 0.264759, acc.: 85.94%] [G loss: 3.519109]\n",
      "epoch:32 step:25358 [D loss: 0.325942, acc.: 84.38%] [G loss: 3.223238]\n",
      "epoch:32 step:25359 [D loss: 0.412552, acc.: 84.38%] [G loss: 2.994019]\n",
      "epoch:32 step:25360 [D loss: 0.313296, acc.: 86.72%] [G loss: 2.623203]\n",
      "epoch:32 step:25361 [D loss: 0.291224, acc.: 91.41%] [G loss: 3.344973]\n",
      "epoch:32 step:25362 [D loss: 0.319794, acc.: 85.16%] [G loss: 3.540343]\n",
      "epoch:32 step:25363 [D loss: 0.342617, acc.: 85.94%] [G loss: 3.633119]\n",
      "epoch:32 step:25364 [D loss: 0.265843, acc.: 91.41%] [G loss: 3.242190]\n",
      "epoch:32 step:25365 [D loss: 0.252687, acc.: 90.62%] [G loss: 3.909321]\n",
      "epoch:32 step:25366 [D loss: 0.213244, acc.: 89.84%] [G loss: 3.296116]\n",
      "epoch:32 step:25367 [D loss: 0.339166, acc.: 81.25%] [G loss: 3.726663]\n",
      "epoch:32 step:25368 [D loss: 0.257652, acc.: 89.06%] [G loss: 2.899820]\n",
      "epoch:32 step:25369 [D loss: 0.235349, acc.: 90.62%] [G loss: 3.980006]\n",
      "epoch:32 step:25370 [D loss: 0.265883, acc.: 85.94%] [G loss: 4.012737]\n",
      "epoch:32 step:25371 [D loss: 0.250827, acc.: 89.84%] [G loss: 3.030332]\n",
      "epoch:32 step:25372 [D loss: 0.262713, acc.: 89.84%] [G loss: 3.179880]\n",
      "epoch:32 step:25373 [D loss: 0.357970, acc.: 80.47%] [G loss: 3.116776]\n",
      "epoch:32 step:25374 [D loss: 0.333362, acc.: 83.59%] [G loss: 2.786185]\n",
      "epoch:32 step:25375 [D loss: 0.351838, acc.: 82.03%] [G loss: 3.096674]\n",
      "epoch:32 step:25376 [D loss: 0.245791, acc.: 89.06%] [G loss: 3.158852]\n",
      "epoch:32 step:25377 [D loss: 0.298310, acc.: 89.06%] [G loss: 2.755990]\n",
      "epoch:32 step:25378 [D loss: 0.338494, acc.: 83.59%] [G loss: 2.905140]\n",
      "epoch:32 step:25379 [D loss: 0.321274, acc.: 84.38%] [G loss: 3.766331]\n",
      "epoch:32 step:25380 [D loss: 0.294895, acc.: 85.94%] [G loss: 4.070008]\n",
      "epoch:32 step:25381 [D loss: 0.331066, acc.: 86.72%] [G loss: 3.299246]\n",
      "epoch:32 step:25382 [D loss: 0.297787, acc.: 85.94%] [G loss: 3.663586]\n",
      "epoch:32 step:25383 [D loss: 0.360474, acc.: 79.69%] [G loss: 4.965929]\n",
      "epoch:32 step:25384 [D loss: 0.269665, acc.: 89.84%] [G loss: 4.431736]\n",
      "epoch:32 step:25385 [D loss: 0.338118, acc.: 86.72%] [G loss: 2.896711]\n",
      "epoch:32 step:25386 [D loss: 0.209126, acc.: 91.41%] [G loss: 3.959534]\n",
      "epoch:32 step:25387 [D loss: 0.408555, acc.: 79.69%] [G loss: 5.643270]\n",
      "epoch:32 step:25388 [D loss: 0.594311, acc.: 76.56%] [G loss: 5.366420]\n",
      "epoch:32 step:25389 [D loss: 0.531865, acc.: 78.12%] [G loss: 6.334361]\n",
      "epoch:32 step:25390 [D loss: 0.442962, acc.: 80.47%] [G loss: 2.937633]\n",
      "epoch:32 step:25391 [D loss: 0.402255, acc.: 81.25%] [G loss: 2.656650]\n",
      "epoch:32 step:25392 [D loss: 0.307832, acc.: 89.84%] [G loss: 4.159647]\n",
      "epoch:32 step:25393 [D loss: 0.363471, acc.: 83.59%] [G loss: 4.352009]\n",
      "epoch:32 step:25394 [D loss: 0.321013, acc.: 87.50%] [G loss: 3.717359]\n",
      "epoch:32 step:25395 [D loss: 0.273889, acc.: 92.19%] [G loss: 3.503307]\n",
      "epoch:32 step:25396 [D loss: 0.293413, acc.: 86.72%] [G loss: 2.649043]\n",
      "epoch:32 step:25397 [D loss: 0.421642, acc.: 81.25%] [G loss: 3.916775]\n",
      "epoch:32 step:25398 [D loss: 0.475383, acc.: 78.12%] [G loss: 3.915995]\n",
      "epoch:32 step:25399 [D loss: 0.496230, acc.: 71.88%] [G loss: 2.801322]\n",
      "epoch:32 step:25400 [D loss: 0.365976, acc.: 81.25%] [G loss: 4.373725]\n",
      "epoch:32 step:25401 [D loss: 0.371257, acc.: 82.81%] [G loss: 5.313234]\n",
      "epoch:32 step:25402 [D loss: 0.546844, acc.: 75.00%] [G loss: 2.681494]\n",
      "epoch:32 step:25403 [D loss: 0.274892, acc.: 90.62%] [G loss: 3.450104]\n",
      "epoch:32 step:25404 [D loss: 0.291317, acc.: 89.84%] [G loss: 2.192083]\n",
      "epoch:32 step:25405 [D loss: 0.258409, acc.: 88.28%] [G loss: 3.653669]\n",
      "epoch:32 step:25406 [D loss: 0.348509, acc.: 85.94%] [G loss: 4.317636]\n",
      "epoch:32 step:25407 [D loss: 0.327964, acc.: 85.16%] [G loss: 3.271269]\n",
      "epoch:32 step:25408 [D loss: 0.292127, acc.: 85.94%] [G loss: 4.650345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25409 [D loss: 0.400470, acc.: 80.47%] [G loss: 5.208114]\n",
      "epoch:32 step:25410 [D loss: 0.201806, acc.: 92.19%] [G loss: 4.318575]\n",
      "epoch:32 step:25411 [D loss: 0.255883, acc.: 87.50%] [G loss: 4.873964]\n",
      "epoch:32 step:25412 [D loss: 0.290312, acc.: 86.72%] [G loss: 3.523213]\n",
      "epoch:32 step:25413 [D loss: 0.226417, acc.: 91.41%] [G loss: 3.285223]\n",
      "epoch:32 step:25414 [D loss: 0.306597, acc.: 86.72%] [G loss: 2.854493]\n",
      "epoch:32 step:25415 [D loss: 0.290752, acc.: 86.72%] [G loss: 2.629729]\n",
      "epoch:32 step:25416 [D loss: 0.318215, acc.: 84.38%] [G loss: 3.519788]\n",
      "epoch:32 step:25417 [D loss: 0.432031, acc.: 82.81%] [G loss: 2.421062]\n",
      "epoch:32 step:25418 [D loss: 0.306475, acc.: 85.16%] [G loss: 2.705165]\n",
      "epoch:32 step:25419 [D loss: 0.233531, acc.: 93.75%] [G loss: 2.939962]\n",
      "epoch:32 step:25420 [D loss: 0.407213, acc.: 80.47%] [G loss: 3.137579]\n",
      "epoch:32 step:25421 [D loss: 0.356328, acc.: 79.69%] [G loss: 3.278721]\n",
      "epoch:32 step:25422 [D loss: 0.322948, acc.: 85.94%] [G loss: 2.396882]\n",
      "epoch:32 step:25423 [D loss: 0.334855, acc.: 86.72%] [G loss: 2.868519]\n",
      "epoch:32 step:25424 [D loss: 0.452456, acc.: 79.69%] [G loss: 2.201046]\n",
      "epoch:32 step:25425 [D loss: 0.264800, acc.: 89.06%] [G loss: 2.620621]\n",
      "epoch:32 step:25426 [D loss: 0.225172, acc.: 92.97%] [G loss: 2.564927]\n",
      "epoch:32 step:25427 [D loss: 0.392171, acc.: 81.25%] [G loss: 2.143849]\n",
      "epoch:32 step:25428 [D loss: 0.394184, acc.: 79.69%] [G loss: 3.975830]\n",
      "epoch:32 step:25429 [D loss: 0.421182, acc.: 81.25%] [G loss: 4.438913]\n",
      "epoch:32 step:25430 [D loss: 0.593604, acc.: 73.44%] [G loss: 5.285642]\n",
      "epoch:32 step:25431 [D loss: 0.375079, acc.: 78.91%] [G loss: 3.902153]\n",
      "epoch:32 step:25432 [D loss: 0.220992, acc.: 92.97%] [G loss: 5.490804]\n",
      "epoch:32 step:25433 [D loss: 0.336289, acc.: 84.38%] [G loss: 3.898742]\n",
      "epoch:32 step:25434 [D loss: 0.277975, acc.: 85.94%] [G loss: 3.126116]\n",
      "epoch:32 step:25435 [D loss: 0.311241, acc.: 88.28%] [G loss: 3.178660]\n",
      "epoch:32 step:25436 [D loss: 0.341971, acc.: 84.38%] [G loss: 3.887213]\n",
      "epoch:32 step:25437 [D loss: 0.427565, acc.: 78.12%] [G loss: 3.184676]\n",
      "epoch:32 step:25438 [D loss: 0.290849, acc.: 87.50%] [G loss: 3.235406]\n",
      "epoch:32 step:25439 [D loss: 0.349253, acc.: 83.59%] [G loss: 3.209105]\n",
      "epoch:32 step:25440 [D loss: 0.263363, acc.: 90.62%] [G loss: 3.597914]\n",
      "epoch:32 step:25441 [D loss: 0.290043, acc.: 85.16%] [G loss: 3.904555]\n",
      "epoch:32 step:25442 [D loss: 0.292728, acc.: 86.72%] [G loss: 2.749604]\n",
      "epoch:32 step:25443 [D loss: 0.272408, acc.: 92.19%] [G loss: 3.303792]\n",
      "epoch:32 step:25444 [D loss: 0.361669, acc.: 84.38%] [G loss: 3.985386]\n",
      "epoch:32 step:25445 [D loss: 0.294851, acc.: 86.72%] [G loss: 2.898196]\n",
      "epoch:32 step:25446 [D loss: 0.301501, acc.: 86.72%] [G loss: 2.822395]\n",
      "epoch:32 step:25447 [D loss: 0.248972, acc.: 89.06%] [G loss: 3.503106]\n",
      "epoch:32 step:25448 [D loss: 0.364638, acc.: 83.59%] [G loss: 2.321630]\n",
      "epoch:32 step:25449 [D loss: 0.321407, acc.: 82.81%] [G loss: 2.515579]\n",
      "epoch:32 step:25450 [D loss: 0.288625, acc.: 89.06%] [G loss: 2.719499]\n",
      "epoch:32 step:25451 [D loss: 0.245639, acc.: 90.62%] [G loss: 2.792067]\n",
      "epoch:32 step:25452 [D loss: 0.306951, acc.: 86.72%] [G loss: 3.018672]\n",
      "epoch:32 step:25453 [D loss: 0.259200, acc.: 92.97%] [G loss: 2.329716]\n",
      "epoch:32 step:25454 [D loss: 0.269063, acc.: 86.72%] [G loss: 3.375076]\n",
      "epoch:32 step:25455 [D loss: 0.384359, acc.: 81.25%] [G loss: 3.005534]\n",
      "epoch:32 step:25456 [D loss: 0.284834, acc.: 84.38%] [G loss: 3.520141]\n",
      "epoch:32 step:25457 [D loss: 0.321045, acc.: 87.50%] [G loss: 2.626037]\n",
      "epoch:32 step:25458 [D loss: 0.280765, acc.: 87.50%] [G loss: 2.715204]\n",
      "epoch:32 step:25459 [D loss: 0.286473, acc.: 85.16%] [G loss: 3.033734]\n",
      "epoch:32 step:25460 [D loss: 0.304335, acc.: 85.16%] [G loss: 3.655826]\n",
      "epoch:32 step:25461 [D loss: 0.329451, acc.: 84.38%] [G loss: 3.017385]\n",
      "epoch:32 step:25462 [D loss: 0.252302, acc.: 88.28%] [G loss: 3.729633]\n",
      "epoch:32 step:25463 [D loss: 0.320390, acc.: 88.28%] [G loss: 2.951986]\n",
      "epoch:32 step:25464 [D loss: 0.357586, acc.: 82.81%] [G loss: 3.756868]\n",
      "epoch:32 step:25465 [D loss: 0.332736, acc.: 87.50%] [G loss: 3.022386]\n",
      "epoch:32 step:25466 [D loss: 0.428793, acc.: 78.12%] [G loss: 3.324219]\n",
      "epoch:32 step:25467 [D loss: 0.263989, acc.: 88.28%] [G loss: 4.408261]\n",
      "epoch:32 step:25468 [D loss: 0.196257, acc.: 92.97%] [G loss: 4.114761]\n",
      "epoch:32 step:25469 [D loss: 0.256776, acc.: 91.41%] [G loss: 3.120640]\n",
      "epoch:32 step:25470 [D loss: 0.248056, acc.: 89.06%] [G loss: 3.683075]\n",
      "epoch:32 step:25471 [D loss: 0.357762, acc.: 82.81%] [G loss: 2.978673]\n",
      "epoch:32 step:25472 [D loss: 0.265061, acc.: 88.28%] [G loss: 3.866057]\n",
      "epoch:32 step:25473 [D loss: 0.429253, acc.: 79.69%] [G loss: 2.659515]\n",
      "epoch:32 step:25474 [D loss: 0.344704, acc.: 82.03%] [G loss: 3.579738]\n",
      "epoch:32 step:25475 [D loss: 0.305049, acc.: 86.72%] [G loss: 3.424942]\n",
      "epoch:32 step:25476 [D loss: 0.297233, acc.: 86.72%] [G loss: 3.340633]\n",
      "epoch:32 step:25477 [D loss: 0.222347, acc.: 90.62%] [G loss: 2.710932]\n",
      "epoch:32 step:25478 [D loss: 0.322265, acc.: 83.59%] [G loss: 3.437083]\n",
      "epoch:32 step:25479 [D loss: 0.451104, acc.: 79.69%] [G loss: 3.208249]\n",
      "epoch:32 step:25480 [D loss: 0.237132, acc.: 91.41%] [G loss: 4.205793]\n",
      "epoch:32 step:25481 [D loss: 0.345606, acc.: 83.59%] [G loss: 5.032129]\n",
      "epoch:32 step:25482 [D loss: 0.383043, acc.: 85.16%] [G loss: 4.164242]\n",
      "epoch:32 step:25483 [D loss: 0.303444, acc.: 83.59%] [G loss: 3.767431]\n",
      "epoch:32 step:25484 [D loss: 0.411142, acc.: 79.69%] [G loss: 3.741187]\n",
      "epoch:32 step:25485 [D loss: 0.556707, acc.: 78.12%] [G loss: 2.843011]\n",
      "epoch:32 step:25486 [D loss: 0.421863, acc.: 83.59%] [G loss: 5.903498]\n",
      "epoch:32 step:25487 [D loss: 0.701310, acc.: 74.22%] [G loss: 7.571005]\n",
      "epoch:32 step:25488 [D loss: 1.359870, acc.: 67.97%] [G loss: 7.920277]\n",
      "epoch:32 step:25489 [D loss: 1.810860, acc.: 61.72%] [G loss: 4.007108]\n",
      "epoch:32 step:25490 [D loss: 0.460168, acc.: 82.81%] [G loss: 5.079889]\n",
      "epoch:32 step:25491 [D loss: 0.306469, acc.: 88.28%] [G loss: 4.380138]\n",
      "epoch:32 step:25492 [D loss: 0.336885, acc.: 85.94%] [G loss: 3.672491]\n",
      "epoch:32 step:25493 [D loss: 0.384129, acc.: 82.03%] [G loss: 3.780596]\n",
      "epoch:32 step:25494 [D loss: 0.275784, acc.: 86.72%] [G loss: 3.389834]\n",
      "epoch:32 step:25495 [D loss: 0.314259, acc.: 85.94%] [G loss: 4.284318]\n",
      "epoch:32 step:25496 [D loss: 0.267983, acc.: 89.06%] [G loss: 2.791758]\n",
      "epoch:32 step:25497 [D loss: 0.346991, acc.: 83.59%] [G loss: 2.528463]\n",
      "epoch:32 step:25498 [D loss: 0.378328, acc.: 83.59%] [G loss: 3.395984]\n",
      "epoch:32 step:25499 [D loss: 0.401603, acc.: 82.81%] [G loss: 2.563168]\n",
      "epoch:32 step:25500 [D loss: 0.313269, acc.: 87.50%] [G loss: 3.430076]\n",
      "epoch:32 step:25501 [D loss: 0.325725, acc.: 84.38%] [G loss: 3.833523]\n",
      "epoch:32 step:25502 [D loss: 0.282642, acc.: 87.50%] [G loss: 3.049475]\n",
      "epoch:32 step:25503 [D loss: 0.310321, acc.: 86.72%] [G loss: 4.215009]\n",
      "epoch:32 step:25504 [D loss: 0.210035, acc.: 92.97%] [G loss: 2.933304]\n",
      "epoch:32 step:25505 [D loss: 0.312811, acc.: 84.38%] [G loss: 3.263693]\n",
      "epoch:32 step:25506 [D loss: 0.402489, acc.: 85.16%] [G loss: 2.772299]\n",
      "epoch:32 step:25507 [D loss: 0.376634, acc.: 83.59%] [G loss: 2.546659]\n",
      "epoch:32 step:25508 [D loss: 0.299117, acc.: 88.28%] [G loss: 2.568509]\n",
      "epoch:32 step:25509 [D loss: 0.281694, acc.: 87.50%] [G loss: 3.439819]\n",
      "epoch:32 step:25510 [D loss: 0.234661, acc.: 90.62%] [G loss: 2.712093]\n",
      "epoch:32 step:25511 [D loss: 0.368468, acc.: 83.59%] [G loss: 2.792449]\n",
      "epoch:32 step:25512 [D loss: 0.405866, acc.: 79.69%] [G loss: 2.632416]\n",
      "epoch:32 step:25513 [D loss: 0.307362, acc.: 87.50%] [G loss: 3.115104]\n",
      "epoch:32 step:25514 [D loss: 0.371209, acc.: 83.59%] [G loss: 2.990771]\n",
      "epoch:32 step:25515 [D loss: 0.338745, acc.: 87.50%] [G loss: 3.207242]\n",
      "epoch:32 step:25516 [D loss: 0.248567, acc.: 91.41%] [G loss: 2.375953]\n",
      "epoch:32 step:25517 [D loss: 0.313828, acc.: 86.72%] [G loss: 3.201415]\n",
      "epoch:32 step:25518 [D loss: 0.423685, acc.: 78.91%] [G loss: 2.553722]\n",
      "epoch:32 step:25519 [D loss: 0.296909, acc.: 86.72%] [G loss: 2.333952]\n",
      "epoch:32 step:25520 [D loss: 0.376429, acc.: 83.59%] [G loss: 3.330874]\n",
      "epoch:32 step:25521 [D loss: 0.323721, acc.: 81.25%] [G loss: 2.682096]\n",
      "epoch:32 step:25522 [D loss: 0.454633, acc.: 80.47%] [G loss: 2.775657]\n",
      "epoch:32 step:25523 [D loss: 0.332154, acc.: 84.38%] [G loss: 2.428624]\n",
      "epoch:32 step:25524 [D loss: 0.348265, acc.: 83.59%] [G loss: 3.180523]\n",
      "epoch:32 step:25525 [D loss: 0.345370, acc.: 83.59%] [G loss: 2.809306]\n",
      "epoch:32 step:25526 [D loss: 0.343742, acc.: 84.38%] [G loss: 5.032481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25527 [D loss: 0.475712, acc.: 78.12%] [G loss: 1.599061]\n",
      "epoch:32 step:25528 [D loss: 0.396672, acc.: 82.03%] [G loss: 3.419712]\n",
      "epoch:32 step:25529 [D loss: 0.452869, acc.: 79.69%] [G loss: 3.389555]\n",
      "epoch:32 step:25530 [D loss: 0.326935, acc.: 82.81%] [G loss: 2.980132]\n",
      "epoch:32 step:25531 [D loss: 0.255315, acc.: 89.84%] [G loss: 3.162931]\n",
      "epoch:32 step:25532 [D loss: 0.287859, acc.: 86.72%] [G loss: 2.636488]\n",
      "epoch:32 step:25533 [D loss: 0.376787, acc.: 80.47%] [G loss: 4.433610]\n",
      "epoch:32 step:25534 [D loss: 0.372767, acc.: 80.47%] [G loss: 4.344909]\n",
      "epoch:32 step:25535 [D loss: 0.233508, acc.: 88.28%] [G loss: 4.992890]\n",
      "epoch:32 step:25536 [D loss: 0.290047, acc.: 82.03%] [G loss: 4.099058]\n",
      "epoch:32 step:25537 [D loss: 0.155488, acc.: 92.97%] [G loss: 5.165825]\n",
      "epoch:32 step:25538 [D loss: 0.198125, acc.: 89.84%] [G loss: 5.640955]\n",
      "epoch:32 step:25539 [D loss: 0.368222, acc.: 84.38%] [G loss: 4.093102]\n",
      "epoch:32 step:25540 [D loss: 0.279321, acc.: 89.06%] [G loss: 3.987169]\n",
      "epoch:32 step:25541 [D loss: 0.314432, acc.: 86.72%] [G loss: 4.218313]\n",
      "epoch:32 step:25542 [D loss: 0.203709, acc.: 93.75%] [G loss: 4.158270]\n",
      "epoch:32 step:25543 [D loss: 0.288044, acc.: 85.94%] [G loss: 3.894802]\n",
      "epoch:32 step:25544 [D loss: 0.295094, acc.: 85.94%] [G loss: 4.158510]\n",
      "epoch:32 step:25545 [D loss: 0.264581, acc.: 84.38%] [G loss: 3.052692]\n",
      "epoch:32 step:25546 [D loss: 0.333058, acc.: 85.94%] [G loss: 2.498464]\n",
      "epoch:32 step:25547 [D loss: 0.527041, acc.: 76.56%] [G loss: 2.736887]\n",
      "epoch:32 step:25548 [D loss: 0.396023, acc.: 82.81%] [G loss: 3.000895]\n",
      "epoch:32 step:25549 [D loss: 0.274381, acc.: 89.06%] [G loss: 3.274519]\n",
      "epoch:32 step:25550 [D loss: 0.350667, acc.: 82.81%] [G loss: 3.968570]\n",
      "epoch:32 step:25551 [D loss: 0.442192, acc.: 82.03%] [G loss: 3.452542]\n",
      "epoch:32 step:25552 [D loss: 0.425695, acc.: 82.81%] [G loss: 2.879045]\n",
      "epoch:32 step:25553 [D loss: 0.338303, acc.: 82.81%] [G loss: 2.870759]\n",
      "epoch:32 step:25554 [D loss: 0.379925, acc.: 82.81%] [G loss: 2.877857]\n",
      "epoch:32 step:25555 [D loss: 0.264198, acc.: 89.06%] [G loss: 3.381263]\n",
      "epoch:32 step:25556 [D loss: 0.254603, acc.: 86.72%] [G loss: 3.019805]\n",
      "epoch:32 step:25557 [D loss: 0.244494, acc.: 89.84%] [G loss: 3.237676]\n",
      "epoch:32 step:25558 [D loss: 0.345223, acc.: 85.94%] [G loss: 3.237510]\n",
      "epoch:32 step:25559 [D loss: 0.440708, acc.: 78.91%] [G loss: 2.882366]\n",
      "epoch:32 step:25560 [D loss: 0.410576, acc.: 79.69%] [G loss: 3.788430]\n",
      "epoch:32 step:25561 [D loss: 0.246913, acc.: 91.41%] [G loss: 2.976606]\n",
      "epoch:32 step:25562 [D loss: 0.409106, acc.: 78.91%] [G loss: 2.881294]\n",
      "epoch:32 step:25563 [D loss: 0.277257, acc.: 89.84%] [G loss: 3.510232]\n",
      "epoch:32 step:25564 [D loss: 0.327592, acc.: 84.38%] [G loss: 3.370446]\n",
      "epoch:32 step:25565 [D loss: 0.323716, acc.: 85.94%] [G loss: 2.332947]\n",
      "epoch:32 step:25566 [D loss: 0.391384, acc.: 80.47%] [G loss: 5.109818]\n",
      "epoch:32 step:25567 [D loss: 0.486138, acc.: 76.56%] [G loss: 3.485666]\n",
      "epoch:32 step:25568 [D loss: 0.499396, acc.: 76.56%] [G loss: 3.318089]\n",
      "epoch:32 step:25569 [D loss: 0.329459, acc.: 85.94%] [G loss: 3.293737]\n",
      "epoch:32 step:25570 [D loss: 0.372988, acc.: 83.59%] [G loss: 3.173460]\n",
      "epoch:32 step:25571 [D loss: 0.310729, acc.: 86.72%] [G loss: 3.428077]\n",
      "epoch:32 step:25572 [D loss: 0.311489, acc.: 89.06%] [G loss: 2.404185]\n",
      "epoch:32 step:25573 [D loss: 0.223299, acc.: 88.28%] [G loss: 4.800197]\n",
      "epoch:32 step:25574 [D loss: 0.352609, acc.: 84.38%] [G loss: 3.009269]\n",
      "epoch:32 step:25575 [D loss: 0.357393, acc.: 83.59%] [G loss: 2.838822]\n",
      "epoch:32 step:25576 [D loss: 0.316459, acc.: 83.59%] [G loss: 3.848586]\n",
      "epoch:32 step:25577 [D loss: 0.279249, acc.: 89.06%] [G loss: 3.365088]\n",
      "epoch:32 step:25578 [D loss: 0.359547, acc.: 84.38%] [G loss: 3.748888]\n",
      "epoch:32 step:25579 [D loss: 0.249836, acc.: 88.28%] [G loss: 3.130564]\n",
      "epoch:32 step:25580 [D loss: 0.228989, acc.: 89.84%] [G loss: 3.573117]\n",
      "epoch:32 step:25581 [D loss: 0.377544, acc.: 84.38%] [G loss: 3.396909]\n",
      "epoch:32 step:25582 [D loss: 0.243879, acc.: 87.50%] [G loss: 3.966625]\n",
      "epoch:32 step:25583 [D loss: 0.302140, acc.: 83.59%] [G loss: 3.831183]\n",
      "epoch:32 step:25584 [D loss: 0.219833, acc.: 92.97%] [G loss: 2.784004]\n",
      "epoch:32 step:25585 [D loss: 0.308042, acc.: 85.16%] [G loss: 2.882332]\n",
      "epoch:32 step:25586 [D loss: 0.259592, acc.: 89.84%] [G loss: 3.129739]\n",
      "epoch:32 step:25587 [D loss: 0.508636, acc.: 78.91%] [G loss: 2.947507]\n",
      "epoch:32 step:25588 [D loss: 0.315924, acc.: 85.94%] [G loss: 3.693743]\n",
      "epoch:32 step:25589 [D loss: 0.296203, acc.: 89.84%] [G loss: 3.289264]\n",
      "epoch:32 step:25590 [D loss: 0.284142, acc.: 87.50%] [G loss: 4.441921]\n",
      "epoch:32 step:25591 [D loss: 0.321705, acc.: 86.72%] [G loss: 3.417878]\n",
      "epoch:32 step:25592 [D loss: 0.315816, acc.: 86.72%] [G loss: 3.498353]\n",
      "epoch:32 step:25593 [D loss: 0.538073, acc.: 72.66%] [G loss: 4.767093]\n",
      "epoch:32 step:25594 [D loss: 0.373271, acc.: 82.03%] [G loss: 3.062796]\n",
      "epoch:32 step:25595 [D loss: 0.396379, acc.: 80.47%] [G loss: 3.574407]\n",
      "epoch:32 step:25596 [D loss: 0.302352, acc.: 86.72%] [G loss: 2.467458]\n",
      "epoch:32 step:25597 [D loss: 0.453655, acc.: 81.25%] [G loss: 4.479001]\n",
      "epoch:32 step:25598 [D loss: 0.582679, acc.: 72.66%] [G loss: 3.960341]\n",
      "epoch:32 step:25599 [D loss: 0.318993, acc.: 86.72%] [G loss: 3.368606]\n",
      "epoch:32 step:25600 [D loss: 0.244492, acc.: 90.62%] [G loss: 3.240189]\n",
      "epoch:32 step:25601 [D loss: 0.360277, acc.: 85.16%] [G loss: 2.958698]\n",
      "epoch:32 step:25602 [D loss: 0.306467, acc.: 85.16%] [G loss: 3.267613]\n",
      "epoch:32 step:25603 [D loss: 0.279979, acc.: 87.50%] [G loss: 3.526930]\n",
      "epoch:32 step:25604 [D loss: 0.287281, acc.: 88.28%] [G loss: 2.587454]\n",
      "epoch:32 step:25605 [D loss: 0.390517, acc.: 81.25%] [G loss: 3.942028]\n",
      "epoch:32 step:25606 [D loss: 0.241484, acc.: 89.84%] [G loss: 4.599305]\n",
      "epoch:32 step:25607 [D loss: 0.293287, acc.: 84.38%] [G loss: 3.283890]\n",
      "epoch:32 step:25608 [D loss: 0.220909, acc.: 90.62%] [G loss: 3.617869]\n",
      "epoch:32 step:25609 [D loss: 0.342328, acc.: 82.81%] [G loss: 5.273291]\n",
      "epoch:32 step:25610 [D loss: 0.295723, acc.: 87.50%] [G loss: 3.355159]\n",
      "epoch:32 step:25611 [D loss: 0.311364, acc.: 82.81%] [G loss: 3.447027]\n",
      "epoch:32 step:25612 [D loss: 0.275927, acc.: 88.28%] [G loss: 3.027658]\n",
      "epoch:32 step:25613 [D loss: 0.224647, acc.: 92.97%] [G loss: 2.726950]\n",
      "epoch:32 step:25614 [D loss: 0.239002, acc.: 89.06%] [G loss: 4.222798]\n",
      "epoch:32 step:25615 [D loss: 0.293217, acc.: 89.06%] [G loss: 2.672227]\n",
      "epoch:32 step:25616 [D loss: 0.402816, acc.: 77.34%] [G loss: 4.659354]\n",
      "epoch:32 step:25617 [D loss: 0.507365, acc.: 75.00%] [G loss: 2.846296]\n",
      "epoch:32 step:25618 [D loss: 0.336638, acc.: 84.38%] [G loss: 3.006533]\n",
      "epoch:32 step:25619 [D loss: 0.342468, acc.: 79.69%] [G loss: 4.004295]\n",
      "epoch:32 step:25620 [D loss: 0.337960, acc.: 85.16%] [G loss: 3.226179]\n",
      "epoch:32 step:25621 [D loss: 0.258772, acc.: 89.06%] [G loss: 4.050164]\n",
      "epoch:32 step:25622 [D loss: 0.281330, acc.: 89.84%] [G loss: 3.528780]\n",
      "epoch:32 step:25623 [D loss: 0.265404, acc.: 85.94%] [G loss: 2.581612]\n",
      "epoch:32 step:25624 [D loss: 0.327193, acc.: 82.81%] [G loss: 2.923179]\n",
      "epoch:32 step:25625 [D loss: 0.251653, acc.: 86.72%] [G loss: 3.164455]\n",
      "epoch:32 step:25626 [D loss: 0.346300, acc.: 79.69%] [G loss: 2.590439]\n",
      "epoch:32 step:25627 [D loss: 0.373463, acc.: 82.81%] [G loss: 2.638760]\n",
      "epoch:32 step:25628 [D loss: 0.200035, acc.: 92.19%] [G loss: 3.411076]\n",
      "epoch:32 step:25629 [D loss: 0.455613, acc.: 80.47%] [G loss: 3.133764]\n",
      "epoch:32 step:25630 [D loss: 0.380120, acc.: 85.16%] [G loss: 3.057281]\n",
      "epoch:32 step:25631 [D loss: 0.314657, acc.: 85.16%] [G loss: 2.668415]\n",
      "epoch:32 step:25632 [D loss: 0.252719, acc.: 89.84%] [G loss: 2.700501]\n",
      "epoch:32 step:25633 [D loss: 0.358512, acc.: 81.25%] [G loss: 3.095261]\n",
      "epoch:32 step:25634 [D loss: 0.276923, acc.: 87.50%] [G loss: 3.545033]\n",
      "epoch:32 step:25635 [D loss: 0.394481, acc.: 81.25%] [G loss: 4.137810]\n",
      "epoch:32 step:25636 [D loss: 0.342507, acc.: 83.59%] [G loss: 3.822388]\n",
      "epoch:32 step:25637 [D loss: 0.392231, acc.: 82.03%] [G loss: 3.553683]\n",
      "epoch:32 step:25638 [D loss: 0.326167, acc.: 87.50%] [G loss: 3.569624]\n",
      "epoch:32 step:25639 [D loss: 0.245990, acc.: 90.62%] [G loss: 4.027299]\n",
      "epoch:32 step:25640 [D loss: 0.342772, acc.: 85.16%] [G loss: 4.991338]\n",
      "epoch:32 step:25641 [D loss: 0.256408, acc.: 88.28%] [G loss: 4.395419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25642 [D loss: 0.285566, acc.: 87.50%] [G loss: 4.773320]\n",
      "epoch:32 step:25643 [D loss: 0.307959, acc.: 85.94%] [G loss: 3.722709]\n",
      "epoch:32 step:25644 [D loss: 0.230931, acc.: 90.62%] [G loss: 3.504678]\n",
      "epoch:32 step:25645 [D loss: 0.270645, acc.: 86.72%] [G loss: 3.878286]\n",
      "epoch:32 step:25646 [D loss: 0.268548, acc.: 88.28%] [G loss: 3.719447]\n",
      "epoch:32 step:25647 [D loss: 0.242901, acc.: 89.06%] [G loss: 2.604212]\n",
      "epoch:32 step:25648 [D loss: 0.279233, acc.: 87.50%] [G loss: 3.464965]\n",
      "epoch:32 step:25649 [D loss: 0.442006, acc.: 82.81%] [G loss: 2.580644]\n",
      "epoch:32 step:25650 [D loss: 0.276621, acc.: 89.06%] [G loss: 2.968153]\n",
      "epoch:32 step:25651 [D loss: 0.339267, acc.: 85.94%] [G loss: 2.816827]\n",
      "epoch:32 step:25652 [D loss: 0.360762, acc.: 84.38%] [G loss: 3.076823]\n",
      "epoch:32 step:25653 [D loss: 0.363243, acc.: 83.59%] [G loss: 4.082772]\n",
      "epoch:32 step:25654 [D loss: 0.562210, acc.: 67.19%] [G loss: 2.966574]\n",
      "epoch:32 step:25655 [D loss: 0.377936, acc.: 84.38%] [G loss: 4.060189]\n",
      "epoch:32 step:25656 [D loss: 0.380368, acc.: 82.03%] [G loss: 2.376992]\n",
      "epoch:32 step:25657 [D loss: 0.301959, acc.: 87.50%] [G loss: 3.869733]\n",
      "epoch:32 step:25658 [D loss: 0.457272, acc.: 78.91%] [G loss: 3.750685]\n",
      "epoch:32 step:25659 [D loss: 0.259176, acc.: 89.06%] [G loss: 3.595902]\n",
      "epoch:32 step:25660 [D loss: 0.322235, acc.: 87.50%] [G loss: 4.309525]\n",
      "epoch:32 step:25661 [D loss: 0.352275, acc.: 85.16%] [G loss: 2.997647]\n",
      "epoch:32 step:25662 [D loss: 0.365397, acc.: 84.38%] [G loss: 3.576846]\n",
      "epoch:32 step:25663 [D loss: 0.295581, acc.: 86.72%] [G loss: 3.293458]\n",
      "epoch:32 step:25664 [D loss: 0.317028, acc.: 87.50%] [G loss: 3.451420]\n",
      "epoch:32 step:25665 [D loss: 0.272659, acc.: 89.06%] [G loss: 2.612878]\n",
      "epoch:32 step:25666 [D loss: 0.328607, acc.: 84.38%] [G loss: 2.673486]\n",
      "epoch:32 step:25667 [D loss: 0.294708, acc.: 85.94%] [G loss: 2.437437]\n",
      "epoch:32 step:25668 [D loss: 0.311610, acc.: 89.84%] [G loss: 2.749973]\n",
      "epoch:32 step:25669 [D loss: 0.300245, acc.: 88.28%] [G loss: 3.520826]\n",
      "epoch:32 step:25670 [D loss: 0.418508, acc.: 81.25%] [G loss: 3.615164]\n",
      "epoch:32 step:25671 [D loss: 0.386086, acc.: 83.59%] [G loss: 3.042434]\n",
      "epoch:32 step:25672 [D loss: 0.307143, acc.: 89.84%] [G loss: 5.205824]\n",
      "epoch:32 step:25673 [D loss: 0.257694, acc.: 89.06%] [G loss: 3.309308]\n",
      "epoch:32 step:25674 [D loss: 0.353626, acc.: 83.59%] [G loss: 6.262615]\n",
      "epoch:32 step:25675 [D loss: 0.532194, acc.: 76.56%] [G loss: 7.253538]\n",
      "epoch:32 step:25676 [D loss: 0.756269, acc.: 70.31%] [G loss: 4.745271]\n",
      "epoch:32 step:25677 [D loss: 0.786955, acc.: 72.66%] [G loss: 3.604638]\n",
      "epoch:32 step:25678 [D loss: 0.383183, acc.: 81.25%] [G loss: 3.650187]\n",
      "epoch:32 step:25679 [D loss: 0.434189, acc.: 77.34%] [G loss: 5.782345]\n",
      "epoch:32 step:25680 [D loss: 0.707910, acc.: 78.12%] [G loss: 6.334274]\n",
      "epoch:32 step:25681 [D loss: 0.649036, acc.: 79.69%] [G loss: 3.269608]\n",
      "epoch:32 step:25682 [D loss: 0.719041, acc.: 71.09%] [G loss: 3.791874]\n",
      "epoch:32 step:25683 [D loss: 0.163137, acc.: 93.75%] [G loss: 5.768070]\n",
      "epoch:32 step:25684 [D loss: 0.394825, acc.: 81.25%] [G loss: 3.318394]\n",
      "epoch:32 step:25685 [D loss: 0.342648, acc.: 80.47%] [G loss: 2.804211]\n",
      "epoch:32 step:25686 [D loss: 0.425767, acc.: 84.38%] [G loss: 3.450286]\n",
      "epoch:32 step:25687 [D loss: 0.314398, acc.: 87.50%] [G loss: 3.423673]\n",
      "epoch:32 step:25688 [D loss: 0.392772, acc.: 82.81%] [G loss: 2.827838]\n",
      "epoch:32 step:25689 [D loss: 0.350107, acc.: 82.81%] [G loss: 3.097810]\n",
      "epoch:32 step:25690 [D loss: 0.349061, acc.: 86.72%] [G loss: 2.658621]\n",
      "epoch:32 step:25691 [D loss: 0.264478, acc.: 88.28%] [G loss: 3.591534]\n",
      "epoch:32 step:25692 [D loss: 0.276462, acc.: 88.28%] [G loss: 3.140357]\n",
      "epoch:32 step:25693 [D loss: 0.365029, acc.: 82.81%] [G loss: 2.643010]\n",
      "epoch:32 step:25694 [D loss: 0.353932, acc.: 83.59%] [G loss: 2.802789]\n",
      "epoch:32 step:25695 [D loss: 0.330678, acc.: 84.38%] [G loss: 2.409869]\n",
      "epoch:32 step:25696 [D loss: 0.363445, acc.: 83.59%] [G loss: 2.259946]\n",
      "epoch:32 step:25697 [D loss: 0.366615, acc.: 85.16%] [G loss: 2.736645]\n",
      "epoch:32 step:25698 [D loss: 0.403142, acc.: 81.25%] [G loss: 2.418365]\n",
      "epoch:32 step:25699 [D loss: 0.454524, acc.: 77.34%] [G loss: 2.229749]\n",
      "epoch:32 step:25700 [D loss: 0.339998, acc.: 88.28%] [G loss: 2.483269]\n",
      "epoch:32 step:25701 [D loss: 0.395808, acc.: 82.03%] [G loss: 2.039048]\n",
      "epoch:32 step:25702 [D loss: 0.332979, acc.: 85.16%] [G loss: 2.655301]\n",
      "epoch:32 step:25703 [D loss: 0.457479, acc.: 82.03%] [G loss: 2.714268]\n",
      "epoch:32 step:25704 [D loss: 0.362093, acc.: 84.38%] [G loss: 2.506426]\n",
      "epoch:32 step:25705 [D loss: 0.371018, acc.: 83.59%] [G loss: 2.811658]\n",
      "epoch:32 step:25706 [D loss: 0.435072, acc.: 78.91%] [G loss: 2.477923]\n",
      "epoch:32 step:25707 [D loss: 0.312491, acc.: 85.94%] [G loss: 2.126387]\n",
      "epoch:32 step:25708 [D loss: 0.330446, acc.: 85.94%] [G loss: 2.411156]\n",
      "epoch:32 step:25709 [D loss: 0.358273, acc.: 84.38%] [G loss: 2.843828]\n",
      "epoch:32 step:25710 [D loss: 0.422326, acc.: 79.69%] [G loss: 2.547326]\n",
      "epoch:32 step:25711 [D loss: 0.292074, acc.: 87.50%] [G loss: 2.602609]\n",
      "epoch:32 step:25712 [D loss: 0.409457, acc.: 75.78%] [G loss: 2.492204]\n",
      "epoch:32 step:25713 [D loss: 0.355024, acc.: 82.81%] [G loss: 2.654705]\n",
      "epoch:32 step:25714 [D loss: 0.321755, acc.: 86.72%] [G loss: 3.047347]\n",
      "epoch:32 step:25715 [D loss: 0.250139, acc.: 88.28%] [G loss: 3.408546]\n",
      "epoch:32 step:25716 [D loss: 0.215515, acc.: 92.97%] [G loss: 2.813080]\n",
      "epoch:32 step:25717 [D loss: 0.320069, acc.: 85.16%] [G loss: 3.631913]\n",
      "epoch:32 step:25718 [D loss: 0.368250, acc.: 82.81%] [G loss: 3.481384]\n",
      "epoch:32 step:25719 [D loss: 0.324848, acc.: 82.03%] [G loss: 3.371804]\n",
      "epoch:32 step:25720 [D loss: 0.413590, acc.: 81.25%] [G loss: 2.595625]\n",
      "epoch:32 step:25721 [D loss: 0.399825, acc.: 81.25%] [G loss: 3.051317]\n",
      "epoch:32 step:25722 [D loss: 0.298387, acc.: 85.16%] [G loss: 2.914091]\n",
      "epoch:32 step:25723 [D loss: 0.277314, acc.: 89.06%] [G loss: 2.553925]\n",
      "epoch:32 step:25724 [D loss: 0.477207, acc.: 76.56%] [G loss: 2.177859]\n",
      "epoch:32 step:25725 [D loss: 0.298258, acc.: 88.28%] [G loss: 2.544814]\n",
      "epoch:32 step:25726 [D loss: 0.375935, acc.: 82.81%] [G loss: 2.115737]\n",
      "epoch:32 step:25727 [D loss: 0.370604, acc.: 82.81%] [G loss: 2.361519]\n",
      "epoch:32 step:25728 [D loss: 0.224087, acc.: 91.41%] [G loss: 2.629193]\n",
      "epoch:32 step:25729 [D loss: 0.342330, acc.: 84.38%] [G loss: 2.406703]\n",
      "epoch:32 step:25730 [D loss: 0.403272, acc.: 77.34%] [G loss: 2.755756]\n",
      "epoch:32 step:25731 [D loss: 0.410815, acc.: 80.47%] [G loss: 3.353224]\n",
      "epoch:32 step:25732 [D loss: 0.367318, acc.: 82.81%] [G loss: 3.695052]\n",
      "epoch:32 step:25733 [D loss: 0.338166, acc.: 85.94%] [G loss: 3.509983]\n",
      "epoch:32 step:25734 [D loss: 0.309518, acc.: 89.84%] [G loss: 3.423012]\n",
      "epoch:32 step:25735 [D loss: 0.243892, acc.: 88.28%] [G loss: 3.411812]\n",
      "epoch:32 step:25736 [D loss: 0.288789, acc.: 85.16%] [G loss: 3.102006]\n",
      "epoch:32 step:25737 [D loss: 0.366151, acc.: 82.81%] [G loss: 2.819405]\n",
      "epoch:32 step:25738 [D loss: 0.306709, acc.: 85.94%] [G loss: 3.507698]\n",
      "epoch:32 step:25739 [D loss: 0.389492, acc.: 81.25%] [G loss: 2.376398]\n",
      "epoch:32 step:25740 [D loss: 0.303026, acc.: 88.28%] [G loss: 4.381566]\n",
      "epoch:32 step:25741 [D loss: 0.287956, acc.: 86.72%] [G loss: 4.328230]\n",
      "epoch:32 step:25742 [D loss: 0.219728, acc.: 88.28%] [G loss: 3.532963]\n",
      "epoch:32 step:25743 [D loss: 0.239730, acc.: 89.84%] [G loss: 2.615047]\n",
      "epoch:32 step:25744 [D loss: 0.272842, acc.: 86.72%] [G loss: 3.514318]\n",
      "epoch:32 step:25745 [D loss: 0.275215, acc.: 89.06%] [G loss: 3.274675]\n",
      "epoch:32 step:25746 [D loss: 0.375754, acc.: 79.69%] [G loss: 3.234116]\n",
      "epoch:32 step:25747 [D loss: 0.308820, acc.: 85.94%] [G loss: 3.215157]\n",
      "epoch:32 step:25748 [D loss: 0.286362, acc.: 87.50%] [G loss: 3.936551]\n",
      "epoch:32 step:25749 [D loss: 0.232534, acc.: 89.84%] [G loss: 3.195851]\n",
      "epoch:32 step:25750 [D loss: 0.283888, acc.: 88.28%] [G loss: 3.313834]\n",
      "epoch:32 step:25751 [D loss: 0.307081, acc.: 89.06%] [G loss: 3.063934]\n",
      "epoch:32 step:25752 [D loss: 0.258558, acc.: 89.06%] [G loss: 3.074724]\n",
      "epoch:32 step:25753 [D loss: 0.286841, acc.: 86.72%] [G loss: 3.573852]\n",
      "epoch:32 step:25754 [D loss: 0.286725, acc.: 89.84%] [G loss: 3.076769]\n",
      "epoch:32 step:25755 [D loss: 0.396989, acc.: 78.91%] [G loss: 3.315040]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25756 [D loss: 0.265421, acc.: 86.72%] [G loss: 3.623511]\n",
      "epoch:32 step:25757 [D loss: 0.282523, acc.: 89.06%] [G loss: 3.934690]\n",
      "epoch:32 step:25758 [D loss: 0.371674, acc.: 83.59%] [G loss: 4.268715]\n",
      "epoch:32 step:25759 [D loss: 0.268610, acc.: 90.62%] [G loss: 3.736684]\n",
      "epoch:32 step:25760 [D loss: 0.295535, acc.: 82.81%] [G loss: 4.256243]\n",
      "epoch:32 step:25761 [D loss: 0.275516, acc.: 89.84%] [G loss: 4.193292]\n",
      "epoch:32 step:25762 [D loss: 0.225992, acc.: 92.19%] [G loss: 3.514996]\n",
      "epoch:32 step:25763 [D loss: 0.365587, acc.: 82.03%] [G loss: 4.138000]\n",
      "epoch:32 step:25764 [D loss: 0.207471, acc.: 88.28%] [G loss: 5.751470]\n",
      "epoch:32 step:25765 [D loss: 0.284792, acc.: 87.50%] [G loss: 5.504600]\n",
      "epoch:32 step:25766 [D loss: 0.307895, acc.: 84.38%] [G loss: 3.698296]\n",
      "epoch:32 step:25767 [D loss: 0.320706, acc.: 84.38%] [G loss: 5.184193]\n",
      "epoch:32 step:25768 [D loss: 0.176686, acc.: 92.97%] [G loss: 3.974439]\n",
      "epoch:32 step:25769 [D loss: 0.262281, acc.: 89.06%] [G loss: 3.661874]\n",
      "epoch:32 step:25770 [D loss: 0.266562, acc.: 87.50%] [G loss: 4.183320]\n",
      "epoch:32 step:25771 [D loss: 0.252071, acc.: 88.28%] [G loss: 4.037858]\n",
      "epoch:32 step:25772 [D loss: 0.273567, acc.: 88.28%] [G loss: 3.464709]\n",
      "epoch:32 step:25773 [D loss: 0.283008, acc.: 88.28%] [G loss: 3.336238]\n",
      "epoch:33 step:25774 [D loss: 0.281602, acc.: 86.72%] [G loss: 3.363213]\n",
      "epoch:33 step:25775 [D loss: 0.297038, acc.: 86.72%] [G loss: 3.622959]\n",
      "epoch:33 step:25776 [D loss: 0.338586, acc.: 84.38%] [G loss: 3.597103]\n",
      "epoch:33 step:25777 [D loss: 0.325495, acc.: 83.59%] [G loss: 2.921988]\n",
      "epoch:33 step:25778 [D loss: 0.325351, acc.: 84.38%] [G loss: 3.650154]\n",
      "epoch:33 step:25779 [D loss: 0.307764, acc.: 89.06%] [G loss: 3.168094]\n",
      "epoch:33 step:25780 [D loss: 0.369189, acc.: 83.59%] [G loss: 3.719783]\n",
      "epoch:33 step:25781 [D loss: 0.209091, acc.: 91.41%] [G loss: 3.576728]\n",
      "epoch:33 step:25782 [D loss: 0.268408, acc.: 89.06%] [G loss: 4.676146]\n",
      "epoch:33 step:25783 [D loss: 0.267983, acc.: 88.28%] [G loss: 5.181448]\n",
      "epoch:33 step:25784 [D loss: 0.294438, acc.: 85.94%] [G loss: 3.942901]\n",
      "epoch:33 step:25785 [D loss: 0.221711, acc.: 88.28%] [G loss: 7.221643]\n",
      "epoch:33 step:25786 [D loss: 0.164993, acc.: 92.19%] [G loss: 8.569986]\n",
      "epoch:33 step:25787 [D loss: 0.269889, acc.: 89.84%] [G loss: 6.083100]\n",
      "epoch:33 step:25788 [D loss: 0.174732, acc.: 90.62%] [G loss: 7.293015]\n",
      "epoch:33 step:25789 [D loss: 0.177542, acc.: 95.31%] [G loss: 9.844343]\n",
      "epoch:33 step:25790 [D loss: 0.305780, acc.: 89.06%] [G loss: 5.618425]\n",
      "epoch:33 step:25791 [D loss: 0.211242, acc.: 90.62%] [G loss: 4.685814]\n",
      "epoch:33 step:25792 [D loss: 0.176301, acc.: 92.19%] [G loss: 4.405938]\n",
      "epoch:33 step:25793 [D loss: 0.251537, acc.: 88.28%] [G loss: 3.116513]\n",
      "epoch:33 step:25794 [D loss: 0.308458, acc.: 85.94%] [G loss: 3.864497]\n",
      "epoch:33 step:25795 [D loss: 0.368296, acc.: 89.06%] [G loss: 3.854329]\n",
      "epoch:33 step:25796 [D loss: 0.325715, acc.: 87.50%] [G loss: 2.744673]\n",
      "epoch:33 step:25797 [D loss: 0.371359, acc.: 81.25%] [G loss: 3.708070]\n",
      "epoch:33 step:25798 [D loss: 0.277648, acc.: 87.50%] [G loss: 4.449176]\n",
      "epoch:33 step:25799 [D loss: 0.319383, acc.: 84.38%] [G loss: 3.114549]\n",
      "epoch:33 step:25800 [D loss: 0.394065, acc.: 79.69%] [G loss: 2.988502]\n",
      "epoch:33 step:25801 [D loss: 0.396288, acc.: 79.69%] [G loss: 2.694600]\n",
      "epoch:33 step:25802 [D loss: 0.480921, acc.: 74.22%] [G loss: 3.131135]\n",
      "epoch:33 step:25803 [D loss: 0.319252, acc.: 83.59%] [G loss: 2.472668]\n",
      "epoch:33 step:25804 [D loss: 0.446254, acc.: 75.00%] [G loss: 3.303037]\n",
      "epoch:33 step:25805 [D loss: 0.359089, acc.: 82.03%] [G loss: 3.828671]\n",
      "epoch:33 step:25806 [D loss: 0.265303, acc.: 87.50%] [G loss: 3.517991]\n",
      "epoch:33 step:25807 [D loss: 0.377773, acc.: 82.03%] [G loss: 3.982870]\n",
      "epoch:33 step:25808 [D loss: 0.317591, acc.: 86.72%] [G loss: 3.675976]\n",
      "epoch:33 step:25809 [D loss: 0.349302, acc.: 84.38%] [G loss: 4.074854]\n",
      "epoch:33 step:25810 [D loss: 0.330873, acc.: 86.72%] [G loss: 3.189690]\n",
      "epoch:33 step:25811 [D loss: 0.298565, acc.: 88.28%] [G loss: 3.468080]\n",
      "epoch:33 step:25812 [D loss: 0.327391, acc.: 85.16%] [G loss: 2.909393]\n",
      "epoch:33 step:25813 [D loss: 0.252843, acc.: 89.06%] [G loss: 2.959052]\n",
      "epoch:33 step:25814 [D loss: 0.415617, acc.: 83.59%] [G loss: 2.551280]\n",
      "epoch:33 step:25815 [D loss: 0.236931, acc.: 92.19%] [G loss: 2.711159]\n",
      "epoch:33 step:25816 [D loss: 0.388145, acc.: 81.25%] [G loss: 2.727030]\n",
      "epoch:33 step:25817 [D loss: 0.299666, acc.: 85.16%] [G loss: 2.217926]\n",
      "epoch:33 step:25818 [D loss: 0.326535, acc.: 85.16%] [G loss: 2.858305]\n",
      "epoch:33 step:25819 [D loss: 0.308668, acc.: 85.94%] [G loss: 3.080192]\n",
      "epoch:33 step:25820 [D loss: 0.226667, acc.: 90.62%] [G loss: 3.310030]\n",
      "epoch:33 step:25821 [D loss: 0.410984, acc.: 81.25%] [G loss: 3.313295]\n",
      "epoch:33 step:25822 [D loss: 0.275988, acc.: 91.41%] [G loss: 2.503438]\n",
      "epoch:33 step:25823 [D loss: 0.355325, acc.: 81.25%] [G loss: 3.143257]\n",
      "epoch:33 step:25824 [D loss: 0.337380, acc.: 85.94%] [G loss: 2.989030]\n",
      "epoch:33 step:25825 [D loss: 0.320938, acc.: 82.81%] [G loss: 3.511377]\n",
      "epoch:33 step:25826 [D loss: 0.426095, acc.: 80.47%] [G loss: 6.275260]\n",
      "epoch:33 step:25827 [D loss: 1.044203, acc.: 59.38%] [G loss: 8.297323]\n",
      "epoch:33 step:25828 [D loss: 2.607150, acc.: 62.50%] [G loss: 4.007290]\n",
      "epoch:33 step:25829 [D loss: 0.485638, acc.: 79.69%] [G loss: 3.701904]\n",
      "epoch:33 step:25830 [D loss: 0.913429, acc.: 71.88%] [G loss: 7.104488]\n",
      "epoch:33 step:25831 [D loss: 0.764575, acc.: 77.34%] [G loss: 3.902663]\n",
      "epoch:33 step:25832 [D loss: 0.523741, acc.: 75.78%] [G loss: 3.731002]\n",
      "epoch:33 step:25833 [D loss: 0.399261, acc.: 78.91%] [G loss: 2.412665]\n",
      "epoch:33 step:25834 [D loss: 0.325807, acc.: 82.81%] [G loss: 3.874003]\n",
      "epoch:33 step:25835 [D loss: 0.292692, acc.: 87.50%] [G loss: 3.674752]\n",
      "epoch:33 step:25836 [D loss: 0.359400, acc.: 83.59%] [G loss: 3.053923]\n",
      "epoch:33 step:25837 [D loss: 0.368884, acc.: 85.16%] [G loss: 2.379280]\n",
      "epoch:33 step:25838 [D loss: 0.473262, acc.: 75.78%] [G loss: 3.125428]\n",
      "epoch:33 step:25839 [D loss: 0.435840, acc.: 82.81%] [G loss: 2.573283]\n",
      "epoch:33 step:25840 [D loss: 0.391774, acc.: 82.81%] [G loss: 3.013704]\n",
      "epoch:33 step:25841 [D loss: 0.340318, acc.: 84.38%] [G loss: 2.668516]\n",
      "epoch:33 step:25842 [D loss: 0.345808, acc.: 80.47%] [G loss: 2.410622]\n",
      "epoch:33 step:25843 [D loss: 0.311812, acc.: 85.94%] [G loss: 3.170277]\n",
      "epoch:33 step:25844 [D loss: 0.362340, acc.: 82.81%] [G loss: 2.632711]\n",
      "epoch:33 step:25845 [D loss: 0.292623, acc.: 85.16%] [G loss: 3.438630]\n",
      "epoch:33 step:25846 [D loss: 0.367690, acc.: 81.25%] [G loss: 3.242799]\n",
      "epoch:33 step:25847 [D loss: 0.301054, acc.: 85.16%] [G loss: 3.685070]\n",
      "epoch:33 step:25848 [D loss: 0.354861, acc.: 82.81%] [G loss: 3.170477]\n",
      "epoch:33 step:25849 [D loss: 0.304400, acc.: 87.50%] [G loss: 2.878723]\n",
      "epoch:33 step:25850 [D loss: 0.348412, acc.: 85.16%] [G loss: 3.082623]\n",
      "epoch:33 step:25851 [D loss: 0.360050, acc.: 81.25%] [G loss: 2.900658]\n",
      "epoch:33 step:25852 [D loss: 0.291509, acc.: 88.28%] [G loss: 3.570536]\n",
      "epoch:33 step:25853 [D loss: 0.290565, acc.: 85.94%] [G loss: 2.757189]\n",
      "epoch:33 step:25854 [D loss: 0.354840, acc.: 82.03%] [G loss: 3.764779]\n",
      "epoch:33 step:25855 [D loss: 0.301233, acc.: 89.06%] [G loss: 3.227685]\n",
      "epoch:33 step:25856 [D loss: 0.328766, acc.: 88.28%] [G loss: 2.052556]\n",
      "epoch:33 step:25857 [D loss: 0.213278, acc.: 93.75%] [G loss: 2.765292]\n",
      "epoch:33 step:25858 [D loss: 0.303779, acc.: 85.94%] [G loss: 3.191280]\n",
      "epoch:33 step:25859 [D loss: 0.317163, acc.: 85.94%] [G loss: 2.790971]\n",
      "epoch:33 step:25860 [D loss: 0.246993, acc.: 89.06%] [G loss: 3.261653]\n",
      "epoch:33 step:25861 [D loss: 0.313353, acc.: 84.38%] [G loss: 2.912354]\n",
      "epoch:33 step:25862 [D loss: 0.383196, acc.: 83.59%] [G loss: 2.904822]\n",
      "epoch:33 step:25863 [D loss: 0.405762, acc.: 78.91%] [G loss: 3.516467]\n",
      "epoch:33 step:25864 [D loss: 0.144600, acc.: 96.88%] [G loss: 3.475039]\n",
      "epoch:33 step:25865 [D loss: 0.381879, acc.: 85.16%] [G loss: 3.661959]\n",
      "epoch:33 step:25866 [D loss: 0.425231, acc.: 79.69%] [G loss: 2.869289]\n",
      "epoch:33 step:25867 [D loss: 0.217400, acc.: 91.41%] [G loss: 2.475197]\n",
      "epoch:33 step:25868 [D loss: 0.352087, acc.: 84.38%] [G loss: 2.329141]\n",
      "epoch:33 step:25869 [D loss: 0.357904, acc.: 85.16%] [G loss: 3.510973]\n",
      "epoch:33 step:25870 [D loss: 0.312338, acc.: 85.94%] [G loss: 2.472969]\n",
      "epoch:33 step:25871 [D loss: 0.442149, acc.: 75.78%] [G loss: 2.431293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:25872 [D loss: 0.305933, acc.: 86.72%] [G loss: 2.797108]\n",
      "epoch:33 step:25873 [D loss: 0.273339, acc.: 85.94%] [G loss: 2.543487]\n",
      "epoch:33 step:25874 [D loss: 0.258197, acc.: 88.28%] [G loss: 3.092661]\n",
      "epoch:33 step:25875 [D loss: 0.344656, acc.: 85.16%] [G loss: 3.044628]\n",
      "epoch:33 step:25876 [D loss: 0.312626, acc.: 86.72%] [G loss: 3.225327]\n",
      "epoch:33 step:25877 [D loss: 0.284108, acc.: 89.84%] [G loss: 2.582416]\n",
      "epoch:33 step:25878 [D loss: 0.438050, acc.: 81.25%] [G loss: 2.871632]\n",
      "epoch:33 step:25879 [D loss: 0.264659, acc.: 86.72%] [G loss: 3.791903]\n",
      "epoch:33 step:25880 [D loss: 0.261562, acc.: 89.84%] [G loss: 3.252799]\n",
      "epoch:33 step:25881 [D loss: 0.268430, acc.: 88.28%] [G loss: 3.564027]\n",
      "epoch:33 step:25882 [D loss: 0.363531, acc.: 82.03%] [G loss: 2.900846]\n",
      "epoch:33 step:25883 [D loss: 0.366745, acc.: 84.38%] [G loss: 2.584545]\n",
      "epoch:33 step:25884 [D loss: 0.282917, acc.: 86.72%] [G loss: 2.758273]\n",
      "epoch:33 step:25885 [D loss: 0.291354, acc.: 87.50%] [G loss: 3.427458]\n",
      "epoch:33 step:25886 [D loss: 0.424619, acc.: 79.69%] [G loss: 2.330556]\n",
      "epoch:33 step:25887 [D loss: 0.354476, acc.: 82.03%] [G loss: 2.336448]\n",
      "epoch:33 step:25888 [D loss: 0.279786, acc.: 89.06%] [G loss: 3.270844]\n",
      "epoch:33 step:25889 [D loss: 0.281599, acc.: 87.50%] [G loss: 3.332656]\n",
      "epoch:33 step:25890 [D loss: 0.332708, acc.: 88.28%] [G loss: 3.426382]\n",
      "epoch:33 step:25891 [D loss: 0.299500, acc.: 86.72%] [G loss: 4.692138]\n",
      "epoch:33 step:25892 [D loss: 0.267597, acc.: 87.50%] [G loss: 3.475012]\n",
      "epoch:33 step:25893 [D loss: 0.365837, acc.: 82.81%] [G loss: 3.438400]\n",
      "epoch:33 step:25894 [D loss: 0.278261, acc.: 85.16%] [G loss: 3.400932]\n",
      "epoch:33 step:25895 [D loss: 0.305500, acc.: 85.94%] [G loss: 3.254035]\n",
      "epoch:33 step:25896 [D loss: 0.368541, acc.: 84.38%] [G loss: 2.763633]\n",
      "epoch:33 step:25897 [D loss: 0.363996, acc.: 85.16%] [G loss: 3.476746]\n",
      "epoch:33 step:25898 [D loss: 0.408698, acc.: 80.47%] [G loss: 2.964613]\n",
      "epoch:33 step:25899 [D loss: 0.398844, acc.: 84.38%] [G loss: 3.782516]\n",
      "epoch:33 step:25900 [D loss: 0.323918, acc.: 87.50%] [G loss: 3.003464]\n",
      "epoch:33 step:25901 [D loss: 0.279713, acc.: 89.06%] [G loss: 3.531203]\n",
      "epoch:33 step:25902 [D loss: 0.288878, acc.: 87.50%] [G loss: 2.517229]\n",
      "epoch:33 step:25903 [D loss: 0.225350, acc.: 93.75%] [G loss: 3.580778]\n",
      "epoch:33 step:25904 [D loss: 0.343796, acc.: 82.81%] [G loss: 2.296856]\n",
      "epoch:33 step:25905 [D loss: 0.398838, acc.: 85.16%] [G loss: 2.673316]\n",
      "epoch:33 step:25906 [D loss: 0.281016, acc.: 86.72%] [G loss: 2.538281]\n",
      "epoch:33 step:25907 [D loss: 0.229706, acc.: 89.84%] [G loss: 2.802487]\n",
      "epoch:33 step:25908 [D loss: 0.382186, acc.: 82.03%] [G loss: 2.324485]\n",
      "epoch:33 step:25909 [D loss: 0.234379, acc.: 90.62%] [G loss: 4.209642]\n",
      "epoch:33 step:25910 [D loss: 0.360720, acc.: 82.81%] [G loss: 3.269089]\n",
      "epoch:33 step:25911 [D loss: 0.279989, acc.: 87.50%] [G loss: 2.830533]\n",
      "epoch:33 step:25912 [D loss: 0.292661, acc.: 87.50%] [G loss: 3.475714]\n",
      "epoch:33 step:25913 [D loss: 0.219973, acc.: 89.84%] [G loss: 3.705147]\n",
      "epoch:33 step:25914 [D loss: 0.280449, acc.: 90.62%] [G loss: 4.547603]\n",
      "epoch:33 step:25915 [D loss: 0.351763, acc.: 85.16%] [G loss: 4.242701]\n",
      "epoch:33 step:25916 [D loss: 0.178156, acc.: 92.97%] [G loss: 3.228036]\n",
      "epoch:33 step:25917 [D loss: 0.420536, acc.: 73.44%] [G loss: 3.375879]\n",
      "epoch:33 step:25918 [D loss: 0.227094, acc.: 91.41%] [G loss: 3.778555]\n",
      "epoch:33 step:25919 [D loss: 0.246471, acc.: 89.84%] [G loss: 2.769825]\n",
      "epoch:33 step:25920 [D loss: 0.318814, acc.: 85.16%] [G loss: 2.804241]\n",
      "epoch:33 step:25921 [D loss: 0.355849, acc.: 82.81%] [G loss: 3.467472]\n",
      "epoch:33 step:25922 [D loss: 0.320592, acc.: 85.94%] [G loss: 3.728214]\n",
      "epoch:33 step:25923 [D loss: 0.314083, acc.: 88.28%] [G loss: 3.024430]\n",
      "epoch:33 step:25924 [D loss: 0.310910, acc.: 86.72%] [G loss: 2.899641]\n",
      "epoch:33 step:25925 [D loss: 0.261841, acc.: 88.28%] [G loss: 3.697033]\n",
      "epoch:33 step:25926 [D loss: 0.257323, acc.: 87.50%] [G loss: 3.398431]\n",
      "epoch:33 step:25927 [D loss: 0.380855, acc.: 81.25%] [G loss: 3.326122]\n",
      "epoch:33 step:25928 [D loss: 0.325944, acc.: 88.28%] [G loss: 3.049676]\n",
      "epoch:33 step:25929 [D loss: 0.306013, acc.: 86.72%] [G loss: 3.000475]\n",
      "epoch:33 step:25930 [D loss: 0.359588, acc.: 83.59%] [G loss: 3.434813]\n",
      "epoch:33 step:25931 [D loss: 0.307548, acc.: 85.94%] [G loss: 3.343377]\n",
      "epoch:33 step:25932 [D loss: 0.266012, acc.: 87.50%] [G loss: 3.281071]\n",
      "epoch:33 step:25933 [D loss: 0.484628, acc.: 77.34%] [G loss: 3.311758]\n",
      "epoch:33 step:25934 [D loss: 0.370288, acc.: 82.03%] [G loss: 2.356382]\n",
      "epoch:33 step:25935 [D loss: 0.315064, acc.: 87.50%] [G loss: 3.628932]\n",
      "epoch:33 step:25936 [D loss: 0.301167, acc.: 87.50%] [G loss: 3.523896]\n",
      "epoch:33 step:25937 [D loss: 0.326650, acc.: 85.16%] [G loss: 2.927196]\n",
      "epoch:33 step:25938 [D loss: 0.336037, acc.: 87.50%] [G loss: 2.667735]\n",
      "epoch:33 step:25939 [D loss: 0.287945, acc.: 88.28%] [G loss: 2.339821]\n",
      "epoch:33 step:25940 [D loss: 0.359415, acc.: 85.16%] [G loss: 2.655507]\n",
      "epoch:33 step:25941 [D loss: 0.319235, acc.: 87.50%] [G loss: 2.910429]\n",
      "epoch:33 step:25942 [D loss: 0.370076, acc.: 85.94%] [G loss: 1.819950]\n",
      "epoch:33 step:25943 [D loss: 0.253429, acc.: 86.72%] [G loss: 2.327105]\n",
      "epoch:33 step:25944 [D loss: 0.308871, acc.: 85.94%] [G loss: 2.744925]\n",
      "epoch:33 step:25945 [D loss: 0.407550, acc.: 78.91%] [G loss: 4.373296]\n",
      "epoch:33 step:25946 [D loss: 0.304624, acc.: 85.94%] [G loss: 4.758206]\n",
      "epoch:33 step:25947 [D loss: 0.435796, acc.: 78.91%] [G loss: 2.714828]\n",
      "epoch:33 step:25948 [D loss: 0.305069, acc.: 87.50%] [G loss: 2.585513]\n",
      "epoch:33 step:25949 [D loss: 0.250385, acc.: 89.84%] [G loss: 2.574225]\n",
      "epoch:33 step:25950 [D loss: 0.345038, acc.: 84.38%] [G loss: 2.684294]\n",
      "epoch:33 step:25951 [D loss: 0.280917, acc.: 89.06%] [G loss: 2.604569]\n",
      "epoch:33 step:25952 [D loss: 0.330463, acc.: 82.81%] [G loss: 3.307145]\n",
      "epoch:33 step:25953 [D loss: 0.390539, acc.: 84.38%] [G loss: 2.736686]\n",
      "epoch:33 step:25954 [D loss: 0.194980, acc.: 92.97%] [G loss: 3.262897]\n",
      "epoch:33 step:25955 [D loss: 0.372034, acc.: 82.81%] [G loss: 3.360364]\n",
      "epoch:33 step:25956 [D loss: 0.304258, acc.: 87.50%] [G loss: 3.365666]\n",
      "epoch:33 step:25957 [D loss: 0.425803, acc.: 78.12%] [G loss: 3.700612]\n",
      "epoch:33 step:25958 [D loss: 0.222435, acc.: 89.06%] [G loss: 4.979370]\n",
      "epoch:33 step:25959 [D loss: 0.232388, acc.: 93.75%] [G loss: 4.039915]\n",
      "epoch:33 step:25960 [D loss: 0.396416, acc.: 81.25%] [G loss: 2.782722]\n",
      "epoch:33 step:25961 [D loss: 0.304817, acc.: 87.50%] [G loss: 2.760219]\n",
      "epoch:33 step:25962 [D loss: 0.258037, acc.: 86.72%] [G loss: 3.104439]\n",
      "epoch:33 step:25963 [D loss: 0.313392, acc.: 83.59%] [G loss: 3.821295]\n",
      "epoch:33 step:25964 [D loss: 0.386534, acc.: 79.69%] [G loss: 2.853208]\n",
      "epoch:33 step:25965 [D loss: 0.259561, acc.: 89.06%] [G loss: 3.037239]\n",
      "epoch:33 step:25966 [D loss: 0.319277, acc.: 88.28%] [G loss: 2.892142]\n",
      "epoch:33 step:25967 [D loss: 0.419318, acc.: 83.59%] [G loss: 2.935829]\n",
      "epoch:33 step:25968 [D loss: 0.369940, acc.: 80.47%] [G loss: 3.670469]\n",
      "epoch:33 step:25969 [D loss: 0.254604, acc.: 85.16%] [G loss: 2.762837]\n",
      "epoch:33 step:25970 [D loss: 0.390401, acc.: 81.25%] [G loss: 3.936683]\n",
      "epoch:33 step:25971 [D loss: 0.256768, acc.: 86.72%] [G loss: 3.104395]\n",
      "epoch:33 step:25972 [D loss: 0.326202, acc.: 86.72%] [G loss: 3.346848]\n",
      "epoch:33 step:25973 [D loss: 0.372055, acc.: 81.25%] [G loss: 4.266713]\n",
      "epoch:33 step:25974 [D loss: 0.323496, acc.: 89.06%] [G loss: 3.708801]\n",
      "epoch:33 step:25975 [D loss: 0.250952, acc.: 90.62%] [G loss: 3.142227]\n",
      "epoch:33 step:25976 [D loss: 0.273695, acc.: 88.28%] [G loss: 2.663473]\n",
      "epoch:33 step:25977 [D loss: 0.311016, acc.: 85.16%] [G loss: 3.787165]\n",
      "epoch:33 step:25978 [D loss: 0.301173, acc.: 86.72%] [G loss: 3.096104]\n",
      "epoch:33 step:25979 [D loss: 0.212010, acc.: 91.41%] [G loss: 4.094775]\n",
      "epoch:33 step:25980 [D loss: 0.322263, acc.: 85.16%] [G loss: 2.271676]\n",
      "epoch:33 step:25981 [D loss: 0.335145, acc.: 82.81%] [G loss: 2.918740]\n",
      "epoch:33 step:25982 [D loss: 0.319016, acc.: 85.94%] [G loss: 2.885564]\n",
      "epoch:33 step:25983 [D loss: 0.234237, acc.: 92.19%] [G loss: 3.787576]\n",
      "epoch:33 step:25984 [D loss: 0.317599, acc.: 85.94%] [G loss: 3.528259]\n",
      "epoch:33 step:25985 [D loss: 0.352845, acc.: 82.03%] [G loss: 4.038964]\n",
      "epoch:33 step:25986 [D loss: 0.231073, acc.: 91.41%] [G loss: 3.438691]\n",
      "epoch:33 step:25987 [D loss: 0.390396, acc.: 79.69%] [G loss: 2.855232]\n",
      "epoch:33 step:25988 [D loss: 0.251519, acc.: 89.06%] [G loss: 3.692726]\n",
      "epoch:33 step:25989 [D loss: 0.360650, acc.: 81.25%] [G loss: 2.937086]\n",
      "epoch:33 step:25990 [D loss: 0.238398, acc.: 91.41%] [G loss: 3.484745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:25991 [D loss: 0.363824, acc.: 83.59%] [G loss: 3.050183]\n",
      "epoch:33 step:25992 [D loss: 0.381415, acc.: 83.59%] [G loss: 2.968904]\n",
      "epoch:33 step:25993 [D loss: 0.317347, acc.: 87.50%] [G loss: 3.250332]\n",
      "epoch:33 step:25994 [D loss: 0.290246, acc.: 86.72%] [G loss: 2.198123]\n",
      "epoch:33 step:25995 [D loss: 0.391910, acc.: 79.69%] [G loss: 2.477775]\n",
      "epoch:33 step:25996 [D loss: 0.465091, acc.: 75.78%] [G loss: 3.707713]\n",
      "epoch:33 step:25997 [D loss: 0.410819, acc.: 78.91%] [G loss: 2.727307]\n",
      "epoch:33 step:25998 [D loss: 0.371533, acc.: 81.25%] [G loss: 2.482177]\n",
      "epoch:33 step:25999 [D loss: 0.341649, acc.: 84.38%] [G loss: 4.371357]\n",
      "epoch:33 step:26000 [D loss: 0.244538, acc.: 88.28%] [G loss: 4.127277]\n",
      "epoch:33 step:26001 [D loss: 0.275736, acc.: 88.28%] [G loss: 2.556740]\n",
      "epoch:33 step:26002 [D loss: 0.346637, acc.: 83.59%] [G loss: 2.483150]\n",
      "epoch:33 step:26003 [D loss: 0.412614, acc.: 80.47%] [G loss: 3.220579]\n",
      "epoch:33 step:26004 [D loss: 0.361681, acc.: 82.03%] [G loss: 2.959085]\n",
      "epoch:33 step:26005 [D loss: 0.293492, acc.: 88.28%] [G loss: 3.746087]\n",
      "epoch:33 step:26006 [D loss: 0.271445, acc.: 88.28%] [G loss: 3.313028]\n",
      "epoch:33 step:26007 [D loss: 0.334939, acc.: 85.16%] [G loss: 3.826988]\n",
      "epoch:33 step:26008 [D loss: 0.380855, acc.: 80.47%] [G loss: 2.426111]\n",
      "epoch:33 step:26009 [D loss: 0.341481, acc.: 82.81%] [G loss: 4.028714]\n",
      "epoch:33 step:26010 [D loss: 0.478964, acc.: 80.47%] [G loss: 2.851922]\n",
      "epoch:33 step:26011 [D loss: 0.335513, acc.: 83.59%] [G loss: 2.684638]\n",
      "epoch:33 step:26012 [D loss: 0.292356, acc.: 88.28%] [G loss: 2.920579]\n",
      "epoch:33 step:26013 [D loss: 0.400217, acc.: 80.47%] [G loss: 3.029581]\n",
      "epoch:33 step:26014 [D loss: 0.363910, acc.: 82.03%] [G loss: 2.723081]\n",
      "epoch:33 step:26015 [D loss: 0.332176, acc.: 82.81%] [G loss: 3.429791]\n",
      "epoch:33 step:26016 [D loss: 0.399943, acc.: 82.03%] [G loss: 4.087502]\n",
      "epoch:33 step:26017 [D loss: 0.390659, acc.: 86.72%] [G loss: 3.035680]\n",
      "epoch:33 step:26018 [D loss: 0.437768, acc.: 77.34%] [G loss: 4.148418]\n",
      "epoch:33 step:26019 [D loss: 0.404882, acc.: 82.03%] [G loss: 3.859175]\n",
      "epoch:33 step:26020 [D loss: 0.337930, acc.: 83.59%] [G loss: 4.105508]\n",
      "epoch:33 step:26021 [D loss: 0.266240, acc.: 86.72%] [G loss: 2.635379]\n",
      "epoch:33 step:26022 [D loss: 0.368457, acc.: 83.59%] [G loss: 3.969135]\n",
      "epoch:33 step:26023 [D loss: 0.383456, acc.: 84.38%] [G loss: 3.168998]\n",
      "epoch:33 step:26024 [D loss: 0.275213, acc.: 86.72%] [G loss: 3.943074]\n",
      "epoch:33 step:26025 [D loss: 0.319709, acc.: 87.50%] [G loss: 3.798657]\n",
      "epoch:33 step:26026 [D loss: 0.308144, acc.: 83.59%] [G loss: 3.185089]\n",
      "epoch:33 step:26027 [D loss: 0.235987, acc.: 92.19%] [G loss: 3.422548]\n",
      "epoch:33 step:26028 [D loss: 0.396000, acc.: 83.59%] [G loss: 3.946718]\n",
      "epoch:33 step:26029 [D loss: 0.350170, acc.: 85.16%] [G loss: 3.920917]\n",
      "epoch:33 step:26030 [D loss: 0.322317, acc.: 82.03%] [G loss: 4.147916]\n",
      "epoch:33 step:26031 [D loss: 0.450789, acc.: 78.12%] [G loss: 3.689522]\n",
      "epoch:33 step:26032 [D loss: 0.310693, acc.: 88.28%] [G loss: 3.571128]\n",
      "epoch:33 step:26033 [D loss: 0.302641, acc.: 87.50%] [G loss: 4.316391]\n",
      "epoch:33 step:26034 [D loss: 0.263162, acc.: 86.72%] [G loss: 3.829873]\n",
      "epoch:33 step:26035 [D loss: 0.300481, acc.: 88.28%] [G loss: 3.663736]\n",
      "epoch:33 step:26036 [D loss: 0.338107, acc.: 83.59%] [G loss: 2.683753]\n",
      "epoch:33 step:26037 [D loss: 0.303404, acc.: 84.38%] [G loss: 4.983067]\n",
      "epoch:33 step:26038 [D loss: 0.346461, acc.: 85.16%] [G loss: 2.671125]\n",
      "epoch:33 step:26039 [D loss: 0.242778, acc.: 89.06%] [G loss: 3.752718]\n",
      "epoch:33 step:26040 [D loss: 0.411921, acc.: 79.69%] [G loss: 3.812318]\n",
      "epoch:33 step:26041 [D loss: 0.263005, acc.: 87.50%] [G loss: 3.458581]\n",
      "epoch:33 step:26042 [D loss: 0.286892, acc.: 84.38%] [G loss: 3.015336]\n",
      "epoch:33 step:26043 [D loss: 0.282807, acc.: 88.28%] [G loss: 3.421845]\n",
      "epoch:33 step:26044 [D loss: 0.282092, acc.: 88.28%] [G loss: 2.941302]\n",
      "epoch:33 step:26045 [D loss: 0.288490, acc.: 87.50%] [G loss: 4.281301]\n",
      "epoch:33 step:26046 [D loss: 0.222567, acc.: 94.53%] [G loss: 3.621219]\n",
      "epoch:33 step:26047 [D loss: 0.340596, acc.: 84.38%] [G loss: 3.459547]\n",
      "epoch:33 step:26048 [D loss: 0.391303, acc.: 77.34%] [G loss: 4.299497]\n",
      "epoch:33 step:26049 [D loss: 0.530742, acc.: 72.66%] [G loss: 4.588905]\n",
      "epoch:33 step:26050 [D loss: 0.358363, acc.: 82.03%] [G loss: 2.648964]\n",
      "epoch:33 step:26051 [D loss: 0.326697, acc.: 86.72%] [G loss: 2.428795]\n",
      "epoch:33 step:26052 [D loss: 0.255508, acc.: 90.62%] [G loss: 3.308465]\n",
      "epoch:33 step:26053 [D loss: 0.267992, acc.: 88.28%] [G loss: 3.219208]\n",
      "epoch:33 step:26054 [D loss: 0.297646, acc.: 85.16%] [G loss: 3.310589]\n",
      "epoch:33 step:26055 [D loss: 0.241938, acc.: 89.06%] [G loss: 3.447603]\n",
      "epoch:33 step:26056 [D loss: 0.261685, acc.: 87.50%] [G loss: 4.083562]\n",
      "epoch:33 step:26057 [D loss: 0.302619, acc.: 86.72%] [G loss: 3.788337]\n",
      "epoch:33 step:26058 [D loss: 0.262450, acc.: 89.06%] [G loss: 3.975492]\n",
      "epoch:33 step:26059 [D loss: 0.267827, acc.: 87.50%] [G loss: 3.934605]\n",
      "epoch:33 step:26060 [D loss: 0.287817, acc.: 87.50%] [G loss: 4.543713]\n",
      "epoch:33 step:26061 [D loss: 0.299051, acc.: 85.16%] [G loss: 3.864162]\n",
      "epoch:33 step:26062 [D loss: 0.267821, acc.: 88.28%] [G loss: 3.174996]\n",
      "epoch:33 step:26063 [D loss: 0.377642, acc.: 82.03%] [G loss: 2.602544]\n",
      "epoch:33 step:26064 [D loss: 0.328478, acc.: 82.81%] [G loss: 2.957098]\n",
      "epoch:33 step:26065 [D loss: 0.420569, acc.: 78.12%] [G loss: 3.238398]\n",
      "epoch:33 step:26066 [D loss: 0.302728, acc.: 87.50%] [G loss: 2.967894]\n",
      "epoch:33 step:26067 [D loss: 0.351087, acc.: 79.69%] [G loss: 2.896768]\n",
      "epoch:33 step:26068 [D loss: 0.366867, acc.: 83.59%] [G loss: 3.103170]\n",
      "epoch:33 step:26069 [D loss: 0.562781, acc.: 76.56%] [G loss: 2.536155]\n",
      "epoch:33 step:26070 [D loss: 0.294535, acc.: 87.50%] [G loss: 3.469725]\n",
      "epoch:33 step:26071 [D loss: 0.368514, acc.: 86.72%] [G loss: 3.517592]\n",
      "epoch:33 step:26072 [D loss: 0.286113, acc.: 85.94%] [G loss: 3.084998]\n",
      "epoch:33 step:26073 [D loss: 0.535009, acc.: 69.53%] [G loss: 2.320395]\n",
      "epoch:33 step:26074 [D loss: 0.281992, acc.: 85.94%] [G loss: 3.047831]\n",
      "epoch:33 step:26075 [D loss: 0.318583, acc.: 82.81%] [G loss: 3.152485]\n",
      "epoch:33 step:26076 [D loss: 0.336647, acc.: 83.59%] [G loss: 3.212828]\n",
      "epoch:33 step:26077 [D loss: 0.322086, acc.: 82.03%] [G loss: 2.883080]\n",
      "epoch:33 step:26078 [D loss: 0.349721, acc.: 80.47%] [G loss: 2.568985]\n",
      "epoch:33 step:26079 [D loss: 0.304495, acc.: 85.94%] [G loss: 3.123892]\n",
      "epoch:33 step:26080 [D loss: 0.253274, acc.: 89.06%] [G loss: 3.538323]\n",
      "epoch:33 step:26081 [D loss: 0.309656, acc.: 81.25%] [G loss: 3.168375]\n",
      "epoch:33 step:26082 [D loss: 0.323353, acc.: 82.81%] [G loss: 3.738393]\n",
      "epoch:33 step:26083 [D loss: 0.266126, acc.: 88.28%] [G loss: 4.025865]\n",
      "epoch:33 step:26084 [D loss: 0.411623, acc.: 78.12%] [G loss: 3.718157]\n",
      "epoch:33 step:26085 [D loss: 0.352401, acc.: 80.47%] [G loss: 2.537931]\n",
      "epoch:33 step:26086 [D loss: 0.424806, acc.: 78.91%] [G loss: 2.587662]\n",
      "epoch:33 step:26087 [D loss: 0.353936, acc.: 80.47%] [G loss: 4.118451]\n",
      "epoch:33 step:26088 [D loss: 0.372848, acc.: 80.47%] [G loss: 4.694854]\n",
      "epoch:33 step:26089 [D loss: 0.346577, acc.: 85.16%] [G loss: 3.134894]\n",
      "epoch:33 step:26090 [D loss: 0.294973, acc.: 85.94%] [G loss: 3.301276]\n",
      "epoch:33 step:26091 [D loss: 0.300625, acc.: 85.94%] [G loss: 3.069312]\n",
      "epoch:33 step:26092 [D loss: 0.249839, acc.: 92.19%] [G loss: 3.236593]\n",
      "epoch:33 step:26093 [D loss: 0.400921, acc.: 79.69%] [G loss: 3.966933]\n",
      "epoch:33 step:26094 [D loss: 0.272027, acc.: 87.50%] [G loss: 3.671865]\n",
      "epoch:33 step:26095 [D loss: 0.339962, acc.: 83.59%] [G loss: 4.156187]\n",
      "epoch:33 step:26096 [D loss: 0.441774, acc.: 79.69%] [G loss: 4.959587]\n",
      "epoch:33 step:26097 [D loss: 0.280721, acc.: 89.06%] [G loss: 3.602070]\n",
      "epoch:33 step:26098 [D loss: 0.297649, acc.: 86.72%] [G loss: 3.739732]\n",
      "epoch:33 step:26099 [D loss: 0.289040, acc.: 86.72%] [G loss: 3.261239]\n",
      "epoch:33 step:26100 [D loss: 0.267867, acc.: 86.72%] [G loss: 4.110986]\n",
      "epoch:33 step:26101 [D loss: 0.268396, acc.: 88.28%] [G loss: 4.465709]\n",
      "epoch:33 step:26102 [D loss: 0.358508, acc.: 85.16%] [G loss: 3.314074]\n",
      "epoch:33 step:26103 [D loss: 0.271242, acc.: 89.06%] [G loss: 3.265161]\n",
      "epoch:33 step:26104 [D loss: 0.356403, acc.: 82.81%] [G loss: 3.970253]\n",
      "epoch:33 step:26105 [D loss: 0.368687, acc.: 84.38%] [G loss: 2.436670]\n",
      "epoch:33 step:26106 [D loss: 0.359282, acc.: 84.38%] [G loss: 3.403390]\n",
      "epoch:33 step:26107 [D loss: 0.327270, acc.: 85.16%] [G loss: 4.004021]\n",
      "epoch:33 step:26108 [D loss: 0.444899, acc.: 78.91%] [G loss: 2.819714]\n",
      "epoch:33 step:26109 [D loss: 0.248013, acc.: 89.84%] [G loss: 4.151356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26110 [D loss: 0.310632, acc.: 87.50%] [G loss: 2.820687]\n",
      "epoch:33 step:26111 [D loss: 0.359103, acc.: 78.91%] [G loss: 3.304596]\n",
      "epoch:33 step:26112 [D loss: 0.295602, acc.: 88.28%] [G loss: 4.156933]\n",
      "epoch:33 step:26113 [D loss: 0.487572, acc.: 78.12%] [G loss: 3.100581]\n",
      "epoch:33 step:26114 [D loss: 0.248594, acc.: 90.62%] [G loss: 3.952681]\n",
      "epoch:33 step:26115 [D loss: 0.323805, acc.: 87.50%] [G loss: 4.527365]\n",
      "epoch:33 step:26116 [D loss: 0.294303, acc.: 87.50%] [G loss: 2.982389]\n",
      "epoch:33 step:26117 [D loss: 0.339341, acc.: 86.72%] [G loss: 2.508317]\n",
      "epoch:33 step:26118 [D loss: 0.285136, acc.: 89.06%] [G loss: 3.486377]\n",
      "epoch:33 step:26119 [D loss: 0.462560, acc.: 75.78%] [G loss: 3.788980]\n",
      "epoch:33 step:26120 [D loss: 0.329427, acc.: 87.50%] [G loss: 3.079864]\n",
      "epoch:33 step:26121 [D loss: 0.459979, acc.: 78.91%] [G loss: 2.521403]\n",
      "epoch:33 step:26122 [D loss: 0.303563, acc.: 86.72%] [G loss: 2.817734]\n",
      "epoch:33 step:26123 [D loss: 0.371122, acc.: 82.81%] [G loss: 3.065052]\n",
      "epoch:33 step:26124 [D loss: 0.349424, acc.: 85.94%] [G loss: 2.815178]\n",
      "epoch:33 step:26125 [D loss: 0.356145, acc.: 83.59%] [G loss: 2.466115]\n",
      "epoch:33 step:26126 [D loss: 0.389775, acc.: 82.03%] [G loss: 2.550865]\n",
      "epoch:33 step:26127 [D loss: 0.240550, acc.: 91.41%] [G loss: 3.570239]\n",
      "epoch:33 step:26128 [D loss: 0.260693, acc.: 86.72%] [G loss: 3.378962]\n",
      "epoch:33 step:26129 [D loss: 0.415278, acc.: 75.00%] [G loss: 3.350883]\n",
      "epoch:33 step:26130 [D loss: 0.333411, acc.: 85.94%] [G loss: 4.070383]\n",
      "epoch:33 step:26131 [D loss: 0.406431, acc.: 84.38%] [G loss: 3.217101]\n",
      "epoch:33 step:26132 [D loss: 0.298508, acc.: 84.38%] [G loss: 3.254920]\n",
      "epoch:33 step:26133 [D loss: 0.307505, acc.: 87.50%] [G loss: 3.359861]\n",
      "epoch:33 step:26134 [D loss: 0.409915, acc.: 82.81%] [G loss: 3.306566]\n",
      "epoch:33 step:26135 [D loss: 0.347755, acc.: 85.16%] [G loss: 2.755237]\n",
      "epoch:33 step:26136 [D loss: 0.265752, acc.: 88.28%] [G loss: 4.658533]\n",
      "epoch:33 step:26137 [D loss: 0.368354, acc.: 82.81%] [G loss: 3.477085]\n",
      "epoch:33 step:26138 [D loss: 0.270293, acc.: 90.62%] [G loss: 2.656942]\n",
      "epoch:33 step:26139 [D loss: 0.246069, acc.: 89.84%] [G loss: 3.773136]\n",
      "epoch:33 step:26140 [D loss: 0.323217, acc.: 85.16%] [G loss: 2.830902]\n",
      "epoch:33 step:26141 [D loss: 0.314922, acc.: 85.94%] [G loss: 2.698179]\n",
      "epoch:33 step:26142 [D loss: 0.238060, acc.: 89.84%] [G loss: 2.368964]\n",
      "epoch:33 step:26143 [D loss: 0.401657, acc.: 82.03%] [G loss: 2.864280]\n",
      "epoch:33 step:26144 [D loss: 0.297989, acc.: 89.06%] [G loss: 2.595584]\n",
      "epoch:33 step:26145 [D loss: 0.291466, acc.: 87.50%] [G loss: 2.564920]\n",
      "epoch:33 step:26146 [D loss: 0.421205, acc.: 83.59%] [G loss: 2.844836]\n",
      "epoch:33 step:26147 [D loss: 0.447924, acc.: 79.69%] [G loss: 3.732499]\n",
      "epoch:33 step:26148 [D loss: 0.504623, acc.: 76.56%] [G loss: 2.414811]\n",
      "epoch:33 step:26149 [D loss: 0.256104, acc.: 91.41%] [G loss: 3.717592]\n",
      "epoch:33 step:26150 [D loss: 0.482698, acc.: 82.03%] [G loss: 5.017483]\n",
      "epoch:33 step:26151 [D loss: 0.577828, acc.: 75.00%] [G loss: 4.738689]\n",
      "epoch:33 step:26152 [D loss: 1.005675, acc.: 68.75%] [G loss: 9.071168]\n",
      "epoch:33 step:26153 [D loss: 1.454426, acc.: 65.62%] [G loss: 4.282170]\n",
      "epoch:33 step:26154 [D loss: 0.539338, acc.: 84.38%] [G loss: 4.287398]\n",
      "epoch:33 step:26155 [D loss: 0.644735, acc.: 72.66%] [G loss: 5.830673]\n",
      "epoch:33 step:26156 [D loss: 0.410880, acc.: 80.47%] [G loss: 4.951630]\n",
      "epoch:33 step:26157 [D loss: 0.300330, acc.: 86.72%] [G loss: 3.969322]\n",
      "epoch:33 step:26158 [D loss: 0.364999, acc.: 82.03%] [G loss: 2.866564]\n",
      "epoch:33 step:26159 [D loss: 0.263241, acc.: 89.06%] [G loss: 2.646165]\n",
      "epoch:33 step:26160 [D loss: 0.279491, acc.: 88.28%] [G loss: 2.588583]\n",
      "epoch:33 step:26161 [D loss: 0.507674, acc.: 76.56%] [G loss: 2.498411]\n",
      "epoch:33 step:26162 [D loss: 0.432991, acc.: 82.03%] [G loss: 3.706549]\n",
      "epoch:33 step:26163 [D loss: 0.483431, acc.: 76.56%] [G loss: 3.058867]\n",
      "epoch:33 step:26164 [D loss: 0.459416, acc.: 75.78%] [G loss: 2.876772]\n",
      "epoch:33 step:26165 [D loss: 0.438959, acc.: 78.12%] [G loss: 4.304231]\n",
      "epoch:33 step:26166 [D loss: 0.312116, acc.: 85.94%] [G loss: 5.602921]\n",
      "epoch:33 step:26167 [D loss: 0.414122, acc.: 82.81%] [G loss: 6.046620]\n",
      "epoch:33 step:26168 [D loss: 0.360184, acc.: 82.03%] [G loss: 5.082478]\n",
      "epoch:33 step:26169 [D loss: 0.200344, acc.: 91.41%] [G loss: 4.547698]\n",
      "epoch:33 step:26170 [D loss: 0.304271, acc.: 88.28%] [G loss: 5.188873]\n",
      "epoch:33 step:26171 [D loss: 0.297205, acc.: 85.94%] [G loss: 3.401451]\n",
      "epoch:33 step:26172 [D loss: 0.344553, acc.: 81.25%] [G loss: 3.814402]\n",
      "epoch:33 step:26173 [D loss: 0.412318, acc.: 82.81%] [G loss: 4.473633]\n",
      "epoch:33 step:26174 [D loss: 0.267946, acc.: 86.72%] [G loss: 2.789386]\n",
      "epoch:33 step:26175 [D loss: 0.264521, acc.: 89.84%] [G loss: 3.185786]\n",
      "epoch:33 step:26176 [D loss: 0.280423, acc.: 87.50%] [G loss: 4.194912]\n",
      "epoch:33 step:26177 [D loss: 0.286460, acc.: 85.16%] [G loss: 2.816104]\n",
      "epoch:33 step:26178 [D loss: 0.345768, acc.: 84.38%] [G loss: 2.962989]\n",
      "epoch:33 step:26179 [D loss: 0.459027, acc.: 79.69%] [G loss: 2.922155]\n",
      "epoch:33 step:26180 [D loss: 0.425892, acc.: 79.69%] [G loss: 2.290366]\n",
      "epoch:33 step:26181 [D loss: 0.314169, acc.: 85.16%] [G loss: 2.461301]\n",
      "epoch:33 step:26182 [D loss: 0.415703, acc.: 76.56%] [G loss: 2.303693]\n",
      "epoch:33 step:26183 [D loss: 0.330240, acc.: 84.38%] [G loss: 2.667436]\n",
      "epoch:33 step:26184 [D loss: 0.374679, acc.: 80.47%] [G loss: 2.056309]\n",
      "epoch:33 step:26185 [D loss: 0.388041, acc.: 80.47%] [G loss: 2.503562]\n",
      "epoch:33 step:26186 [D loss: 0.314440, acc.: 85.16%] [G loss: 3.178856]\n",
      "epoch:33 step:26187 [D loss: 0.406591, acc.: 82.03%] [G loss: 2.627876]\n",
      "epoch:33 step:26188 [D loss: 0.320458, acc.: 83.59%] [G loss: 3.158512]\n",
      "epoch:33 step:26189 [D loss: 0.400170, acc.: 81.25%] [G loss: 2.295599]\n",
      "epoch:33 step:26190 [D loss: 0.344720, acc.: 86.72%] [G loss: 3.633054]\n",
      "epoch:33 step:26191 [D loss: 0.317983, acc.: 89.84%] [G loss: 3.056552]\n",
      "epoch:33 step:26192 [D loss: 0.298427, acc.: 85.16%] [G loss: 2.907736]\n",
      "epoch:33 step:26193 [D loss: 0.322437, acc.: 87.50%] [G loss: 2.759067]\n",
      "epoch:33 step:26194 [D loss: 0.295894, acc.: 85.94%] [G loss: 3.173512]\n",
      "epoch:33 step:26195 [D loss: 0.309195, acc.: 86.72%] [G loss: 3.237662]\n",
      "epoch:33 step:26196 [D loss: 0.313686, acc.: 86.72%] [G loss: 2.530685]\n",
      "epoch:33 step:26197 [D loss: 0.333276, acc.: 82.81%] [G loss: 2.612967]\n",
      "epoch:33 step:26198 [D loss: 0.281265, acc.: 86.72%] [G loss: 3.733036]\n",
      "epoch:33 step:26199 [D loss: 0.217317, acc.: 91.41%] [G loss: 3.709939]\n",
      "epoch:33 step:26200 [D loss: 0.271249, acc.: 89.06%] [G loss: 4.556494]\n",
      "epoch:33 step:26201 [D loss: 0.443993, acc.: 83.59%] [G loss: 2.958370]\n",
      "epoch:33 step:26202 [D loss: 0.395322, acc.: 80.47%] [G loss: 2.465047]\n",
      "epoch:33 step:26203 [D loss: 0.354298, acc.: 85.16%] [G loss: 3.309583]\n",
      "epoch:33 step:26204 [D loss: 0.349818, acc.: 83.59%] [G loss: 4.476055]\n",
      "epoch:33 step:26205 [D loss: 0.434078, acc.: 82.03%] [G loss: 4.631155]\n",
      "epoch:33 step:26206 [D loss: 0.270509, acc.: 89.84%] [G loss: 3.792075]\n",
      "epoch:33 step:26207 [D loss: 0.316771, acc.: 85.94%] [G loss: 4.769429]\n",
      "epoch:33 step:26208 [D loss: 0.361124, acc.: 82.81%] [G loss: 3.754144]\n",
      "epoch:33 step:26209 [D loss: 0.352191, acc.: 81.25%] [G loss: 3.574361]\n",
      "epoch:33 step:26210 [D loss: 0.230381, acc.: 89.84%] [G loss: 3.090494]\n",
      "epoch:33 step:26211 [D loss: 0.358527, acc.: 85.16%] [G loss: 3.957833]\n",
      "epoch:33 step:26212 [D loss: 0.230547, acc.: 90.62%] [G loss: 4.935212]\n",
      "epoch:33 step:26213 [D loss: 0.302786, acc.: 85.94%] [G loss: 5.193881]\n",
      "epoch:33 step:26214 [D loss: 0.365544, acc.: 82.03%] [G loss: 4.934203]\n",
      "epoch:33 step:26215 [D loss: 0.212878, acc.: 92.19%] [G loss: 4.427384]\n",
      "epoch:33 step:26216 [D loss: 0.241683, acc.: 89.06%] [G loss: 4.142896]\n",
      "epoch:33 step:26217 [D loss: 0.322945, acc.: 81.25%] [G loss: 3.246364]\n",
      "epoch:33 step:26218 [D loss: 0.189767, acc.: 92.97%] [G loss: 4.082962]\n",
      "epoch:33 step:26219 [D loss: 0.264053, acc.: 87.50%] [G loss: 3.612989]\n",
      "epoch:33 step:26220 [D loss: 0.194188, acc.: 90.62%] [G loss: 3.229331]\n",
      "epoch:33 step:26221 [D loss: 0.398881, acc.: 77.34%] [G loss: 2.669534]\n",
      "epoch:33 step:26222 [D loss: 0.329232, acc.: 86.72%] [G loss: 2.779850]\n",
      "epoch:33 step:26223 [D loss: 0.296497, acc.: 88.28%] [G loss: 2.732962]\n",
      "epoch:33 step:26224 [D loss: 0.288229, acc.: 88.28%] [G loss: 2.436328]\n",
      "epoch:33 step:26225 [D loss: 0.277670, acc.: 88.28%] [G loss: 2.946452]\n",
      "epoch:33 step:26226 [D loss: 0.288126, acc.: 86.72%] [G loss: 2.645300]\n",
      "epoch:33 step:26227 [D loss: 0.381158, acc.: 85.16%] [G loss: 3.487269]\n",
      "epoch:33 step:26228 [D loss: 0.314208, acc.: 89.06%] [G loss: 2.509606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26229 [D loss: 0.469103, acc.: 78.91%] [G loss: 3.825034]\n",
      "epoch:33 step:26230 [D loss: 0.265193, acc.: 87.50%] [G loss: 3.838013]\n",
      "epoch:33 step:26231 [D loss: 0.335402, acc.: 82.81%] [G loss: 3.612694]\n",
      "epoch:33 step:26232 [D loss: 0.363487, acc.: 81.25%] [G loss: 3.975936]\n",
      "epoch:33 step:26233 [D loss: 0.318346, acc.: 87.50%] [G loss: 3.524023]\n",
      "epoch:33 step:26234 [D loss: 0.340770, acc.: 85.16%] [G loss: 3.610661]\n",
      "epoch:33 step:26235 [D loss: 0.384646, acc.: 79.69%] [G loss: 4.185369]\n",
      "epoch:33 step:26236 [D loss: 0.451363, acc.: 78.12%] [G loss: 2.892146]\n",
      "epoch:33 step:26237 [D loss: 0.366888, acc.: 82.81%] [G loss: 3.714237]\n",
      "epoch:33 step:26238 [D loss: 0.393525, acc.: 79.69%] [G loss: 2.627445]\n",
      "epoch:33 step:26239 [D loss: 0.421966, acc.: 79.69%] [G loss: 2.552054]\n",
      "epoch:33 step:26240 [D loss: 0.388621, acc.: 79.69%] [G loss: 3.156154]\n",
      "epoch:33 step:26241 [D loss: 0.443591, acc.: 78.91%] [G loss: 2.787776]\n",
      "epoch:33 step:26242 [D loss: 0.313408, acc.: 85.94%] [G loss: 2.546834]\n",
      "epoch:33 step:26243 [D loss: 0.473146, acc.: 79.69%] [G loss: 3.145847]\n",
      "epoch:33 step:26244 [D loss: 0.434746, acc.: 81.25%] [G loss: 3.211534]\n",
      "epoch:33 step:26245 [D loss: 0.340589, acc.: 85.16%] [G loss: 3.523816]\n",
      "epoch:33 step:26246 [D loss: 0.408858, acc.: 80.47%] [G loss: 2.333484]\n",
      "epoch:33 step:26247 [D loss: 0.403121, acc.: 82.03%] [G loss: 3.308314]\n",
      "epoch:33 step:26248 [D loss: 0.369194, acc.: 82.03%] [G loss: 4.756485]\n",
      "epoch:33 step:26249 [D loss: 0.287811, acc.: 86.72%] [G loss: 2.924629]\n",
      "epoch:33 step:26250 [D loss: 0.211332, acc.: 90.62%] [G loss: 3.014393]\n",
      "epoch:33 step:26251 [D loss: 0.255306, acc.: 88.28%] [G loss: 3.141670]\n",
      "epoch:33 step:26252 [D loss: 0.399417, acc.: 82.81%] [G loss: 3.823872]\n",
      "epoch:33 step:26253 [D loss: 0.293500, acc.: 91.41%] [G loss: 4.373314]\n",
      "epoch:33 step:26254 [D loss: 0.438852, acc.: 77.34%] [G loss: 3.086603]\n",
      "epoch:33 step:26255 [D loss: 0.352969, acc.: 83.59%] [G loss: 2.854015]\n",
      "epoch:33 step:26256 [D loss: 0.295983, acc.: 88.28%] [G loss: 2.543504]\n",
      "epoch:33 step:26257 [D loss: 0.283153, acc.: 88.28%] [G loss: 3.223282]\n",
      "epoch:33 step:26258 [D loss: 0.294399, acc.: 87.50%] [G loss: 2.571681]\n",
      "epoch:33 step:26259 [D loss: 0.302694, acc.: 85.94%] [G loss: 4.367195]\n",
      "epoch:33 step:26260 [D loss: 0.314132, acc.: 87.50%] [G loss: 4.412425]\n",
      "epoch:33 step:26261 [D loss: 0.284061, acc.: 84.38%] [G loss: 5.350569]\n",
      "epoch:33 step:26262 [D loss: 0.392026, acc.: 78.12%] [G loss: 5.165127]\n",
      "epoch:33 step:26263 [D loss: 0.372955, acc.: 83.59%] [G loss: 4.530348]\n",
      "epoch:33 step:26264 [D loss: 0.536614, acc.: 77.34%] [G loss: 8.799850]\n",
      "epoch:33 step:26265 [D loss: 1.163110, acc.: 64.84%] [G loss: 3.697840]\n",
      "epoch:33 step:26266 [D loss: 0.495387, acc.: 78.91%] [G loss: 3.385495]\n",
      "epoch:33 step:26267 [D loss: 0.373554, acc.: 82.81%] [G loss: 6.162037]\n",
      "epoch:33 step:26268 [D loss: 0.490673, acc.: 78.12%] [G loss: 3.853605]\n",
      "epoch:33 step:26269 [D loss: 0.288085, acc.: 87.50%] [G loss: 3.986200]\n",
      "epoch:33 step:26270 [D loss: 0.349901, acc.: 83.59%] [G loss: 3.602064]\n",
      "epoch:33 step:26271 [D loss: 0.287213, acc.: 87.50%] [G loss: 3.513084]\n",
      "epoch:33 step:26272 [D loss: 0.297043, acc.: 86.72%] [G loss: 4.391009]\n",
      "epoch:33 step:26273 [D loss: 0.357423, acc.: 80.47%] [G loss: 3.252917]\n",
      "epoch:33 step:26274 [D loss: 0.345477, acc.: 82.03%] [G loss: 3.743140]\n",
      "epoch:33 step:26275 [D loss: 0.330502, acc.: 85.94%] [G loss: 3.721525]\n",
      "epoch:33 step:26276 [D loss: 0.310157, acc.: 86.72%] [G loss: 3.013319]\n",
      "epoch:33 step:26277 [D loss: 0.337923, acc.: 85.16%] [G loss: 2.759132]\n",
      "epoch:33 step:26278 [D loss: 0.299695, acc.: 86.72%] [G loss: 2.560609]\n",
      "epoch:33 step:26279 [D loss: 0.392271, acc.: 82.81%] [G loss: 2.095513]\n",
      "epoch:33 step:26280 [D loss: 0.292062, acc.: 88.28%] [G loss: 3.737739]\n",
      "epoch:33 step:26281 [D loss: 0.307449, acc.: 83.59%] [G loss: 2.483064]\n",
      "epoch:33 step:26282 [D loss: 0.474404, acc.: 78.12%] [G loss: 3.168283]\n",
      "epoch:33 step:26283 [D loss: 0.378982, acc.: 85.94%] [G loss: 3.123141]\n",
      "epoch:33 step:26284 [D loss: 0.431847, acc.: 79.69%] [G loss: 3.361476]\n",
      "epoch:33 step:26285 [D loss: 0.301482, acc.: 86.72%] [G loss: 3.801345]\n",
      "epoch:33 step:26286 [D loss: 0.430695, acc.: 77.34%] [G loss: 3.189996]\n",
      "epoch:33 step:26287 [D loss: 0.425924, acc.: 77.34%] [G loss: 6.282856]\n",
      "epoch:33 step:26288 [D loss: 0.529151, acc.: 75.78%] [G loss: 2.445231]\n",
      "epoch:33 step:26289 [D loss: 0.397217, acc.: 78.91%] [G loss: 3.286485]\n",
      "epoch:33 step:26290 [D loss: 0.333898, acc.: 84.38%] [G loss: 3.189512]\n",
      "epoch:33 step:26291 [D loss: 0.277086, acc.: 89.06%] [G loss: 3.041237]\n",
      "epoch:33 step:26292 [D loss: 0.324358, acc.: 85.94%] [G loss: 3.379148]\n",
      "epoch:33 step:26293 [D loss: 0.430266, acc.: 81.25%] [G loss: 2.561620]\n",
      "epoch:33 step:26294 [D loss: 0.322400, acc.: 85.16%] [G loss: 3.258455]\n",
      "epoch:33 step:26295 [D loss: 0.401713, acc.: 78.12%] [G loss: 4.074058]\n",
      "epoch:33 step:26296 [D loss: 0.319006, acc.: 85.94%] [G loss: 2.766257]\n",
      "epoch:33 step:26297 [D loss: 0.387269, acc.: 78.91%] [G loss: 2.042333]\n",
      "epoch:33 step:26298 [D loss: 0.317683, acc.: 89.84%] [G loss: 2.898748]\n",
      "epoch:33 step:26299 [D loss: 0.418867, acc.: 82.81%] [G loss: 2.587316]\n",
      "epoch:33 step:26300 [D loss: 0.298041, acc.: 87.50%] [G loss: 2.008304]\n",
      "epoch:33 step:26301 [D loss: 0.275465, acc.: 88.28%] [G loss: 2.699867]\n",
      "epoch:33 step:26302 [D loss: 0.365481, acc.: 82.81%] [G loss: 2.627805]\n",
      "epoch:33 step:26303 [D loss: 0.312504, acc.: 88.28%] [G loss: 2.792242]\n",
      "epoch:33 step:26304 [D loss: 0.277776, acc.: 88.28%] [G loss: 2.660781]\n",
      "epoch:33 step:26305 [D loss: 0.391690, acc.: 80.47%] [G loss: 3.190884]\n",
      "epoch:33 step:26306 [D loss: 0.249332, acc.: 88.28%] [G loss: 2.750624]\n",
      "epoch:33 step:26307 [D loss: 0.241490, acc.: 88.28%] [G loss: 3.224056]\n",
      "epoch:33 step:26308 [D loss: 0.432501, acc.: 79.69%] [G loss: 2.640513]\n",
      "epoch:33 step:26309 [D loss: 0.310585, acc.: 87.50%] [G loss: 2.691590]\n",
      "epoch:33 step:26310 [D loss: 0.393580, acc.: 76.56%] [G loss: 2.555821]\n",
      "epoch:33 step:26311 [D loss: 0.308569, acc.: 84.38%] [G loss: 3.020879]\n",
      "epoch:33 step:26312 [D loss: 0.267203, acc.: 86.72%] [G loss: 2.795301]\n",
      "epoch:33 step:26313 [D loss: 0.245128, acc.: 91.41%] [G loss: 2.921668]\n",
      "epoch:33 step:26314 [D loss: 0.472245, acc.: 81.25%] [G loss: 2.930492]\n",
      "epoch:33 step:26315 [D loss: 0.305773, acc.: 85.16%] [G loss: 3.790939]\n",
      "epoch:33 step:26316 [D loss: 0.419821, acc.: 80.47%] [G loss: 4.257042]\n",
      "epoch:33 step:26317 [D loss: 0.327059, acc.: 86.72%] [G loss: 2.488151]\n",
      "epoch:33 step:26318 [D loss: 0.250286, acc.: 89.84%] [G loss: 4.094504]\n",
      "epoch:33 step:26319 [D loss: 0.296400, acc.: 88.28%] [G loss: 3.441251]\n",
      "epoch:33 step:26320 [D loss: 0.384044, acc.: 80.47%] [G loss: 3.344288]\n",
      "epoch:33 step:26321 [D loss: 0.215911, acc.: 89.06%] [G loss: 4.011046]\n",
      "epoch:33 step:26322 [D loss: 0.408820, acc.: 79.69%] [G loss: 4.686069]\n",
      "epoch:33 step:26323 [D loss: 0.324958, acc.: 86.72%] [G loss: 3.963927]\n",
      "epoch:33 step:26324 [D loss: 0.264041, acc.: 92.19%] [G loss: 3.223117]\n",
      "epoch:33 step:26325 [D loss: 0.367821, acc.: 85.16%] [G loss: 3.598963]\n",
      "epoch:33 step:26326 [D loss: 0.438503, acc.: 81.25%] [G loss: 3.500520]\n",
      "epoch:33 step:26327 [D loss: 0.358023, acc.: 85.16%] [G loss: 3.786016]\n",
      "epoch:33 step:26328 [D loss: 0.286865, acc.: 87.50%] [G loss: 2.927695]\n",
      "epoch:33 step:26329 [D loss: 0.324937, acc.: 85.16%] [G loss: 2.712485]\n",
      "epoch:33 step:26330 [D loss: 0.334215, acc.: 88.28%] [G loss: 3.806269]\n",
      "epoch:33 step:26331 [D loss: 0.314493, acc.: 86.72%] [G loss: 2.803354]\n",
      "epoch:33 step:26332 [D loss: 0.280474, acc.: 86.72%] [G loss: 3.747024]\n",
      "epoch:33 step:26333 [D loss: 0.305690, acc.: 81.25%] [G loss: 3.549117]\n",
      "epoch:33 step:26334 [D loss: 0.295953, acc.: 85.94%] [G loss: 4.002578]\n",
      "epoch:33 step:26335 [D loss: 0.317328, acc.: 83.59%] [G loss: 3.299000]\n",
      "epoch:33 step:26336 [D loss: 0.254396, acc.: 89.84%] [G loss: 3.717734]\n",
      "epoch:33 step:26337 [D loss: 0.233727, acc.: 90.62%] [G loss: 3.101926]\n",
      "epoch:33 step:26338 [D loss: 0.295928, acc.: 88.28%] [G loss: 3.152633]\n",
      "epoch:33 step:26339 [D loss: 0.285656, acc.: 86.72%] [G loss: 2.387619]\n",
      "epoch:33 step:26340 [D loss: 0.293043, acc.: 85.94%] [G loss: 3.152965]\n",
      "epoch:33 step:26341 [D loss: 0.335140, acc.: 89.06%] [G loss: 2.528355]\n",
      "epoch:33 step:26342 [D loss: 0.277180, acc.: 87.50%] [G loss: 3.521860]\n",
      "epoch:33 step:26343 [D loss: 0.330442, acc.: 86.72%] [G loss: 3.871948]\n",
      "epoch:33 step:26344 [D loss: 0.246510, acc.: 89.06%] [G loss: 3.364421]\n",
      "epoch:33 step:26345 [D loss: 0.350008, acc.: 85.16%] [G loss: 3.822124]\n",
      "epoch:33 step:26346 [D loss: 0.349699, acc.: 83.59%] [G loss: 4.561091]\n",
      "epoch:33 step:26347 [D loss: 0.399562, acc.: 83.59%] [G loss: 4.557452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26348 [D loss: 0.317987, acc.: 89.06%] [G loss: 3.908520]\n",
      "epoch:33 step:26349 [D loss: 0.313082, acc.: 83.59%] [G loss: 3.667466]\n",
      "epoch:33 step:26350 [D loss: 0.396240, acc.: 82.81%] [G loss: 2.621234]\n",
      "epoch:33 step:26351 [D loss: 0.319686, acc.: 88.28%] [G loss: 3.794431]\n",
      "epoch:33 step:26352 [D loss: 0.309528, acc.: 89.06%] [G loss: 3.418881]\n",
      "epoch:33 step:26353 [D loss: 0.371824, acc.: 82.81%] [G loss: 3.431971]\n",
      "epoch:33 step:26354 [D loss: 0.348866, acc.: 82.81%] [G loss: 3.376307]\n",
      "epoch:33 step:26355 [D loss: 0.307264, acc.: 85.16%] [G loss: 3.949756]\n",
      "epoch:33 step:26356 [D loss: 0.380553, acc.: 82.81%] [G loss: 3.530180]\n",
      "epoch:33 step:26357 [D loss: 0.487104, acc.: 75.78%] [G loss: 2.245404]\n",
      "epoch:33 step:26358 [D loss: 0.318136, acc.: 85.94%] [G loss: 2.895844]\n",
      "epoch:33 step:26359 [D loss: 0.293752, acc.: 86.72%] [G loss: 4.882124]\n",
      "epoch:33 step:26360 [D loss: 0.199425, acc.: 93.75%] [G loss: 4.386081]\n",
      "epoch:33 step:26361 [D loss: 0.255257, acc.: 87.50%] [G loss: 3.155732]\n",
      "epoch:33 step:26362 [D loss: 0.421218, acc.: 81.25%] [G loss: 3.725384]\n",
      "epoch:33 step:26363 [D loss: 0.333891, acc.: 85.94%] [G loss: 5.369112]\n",
      "epoch:33 step:26364 [D loss: 0.380330, acc.: 79.69%] [G loss: 4.093765]\n",
      "epoch:33 step:26365 [D loss: 0.266745, acc.: 86.72%] [G loss: 4.040616]\n",
      "epoch:33 step:26366 [D loss: 0.275852, acc.: 88.28%] [G loss: 3.272214]\n",
      "epoch:33 step:26367 [D loss: 0.267158, acc.: 87.50%] [G loss: 4.939713]\n",
      "epoch:33 step:26368 [D loss: 0.286586, acc.: 86.72%] [G loss: 4.429629]\n",
      "epoch:33 step:26369 [D loss: 0.252350, acc.: 87.50%] [G loss: 2.635450]\n",
      "epoch:33 step:26370 [D loss: 0.308958, acc.: 86.72%] [G loss: 3.205376]\n",
      "epoch:33 step:26371 [D loss: 0.374627, acc.: 78.91%] [G loss: 2.798782]\n",
      "epoch:33 step:26372 [D loss: 0.340826, acc.: 86.72%] [G loss: 3.515364]\n",
      "epoch:33 step:26373 [D loss: 0.397072, acc.: 82.03%] [G loss: 4.263929]\n",
      "epoch:33 step:26374 [D loss: 0.376379, acc.: 83.59%] [G loss: 6.324780]\n",
      "epoch:33 step:26375 [D loss: 0.338733, acc.: 82.81%] [G loss: 4.432592]\n",
      "epoch:33 step:26376 [D loss: 0.248984, acc.: 91.41%] [G loss: 6.215395]\n",
      "epoch:33 step:26377 [D loss: 0.278821, acc.: 89.06%] [G loss: 4.512814]\n",
      "epoch:33 step:26378 [D loss: 0.295411, acc.: 89.84%] [G loss: 3.911764]\n",
      "epoch:33 step:26379 [D loss: 0.339996, acc.: 83.59%] [G loss: 4.070908]\n",
      "epoch:33 step:26380 [D loss: 0.462483, acc.: 77.34%] [G loss: 3.049011]\n",
      "epoch:33 step:26381 [D loss: 0.307165, acc.: 85.94%] [G loss: 2.959859]\n",
      "epoch:33 step:26382 [D loss: 0.204287, acc.: 93.75%] [G loss: 3.316677]\n",
      "epoch:33 step:26383 [D loss: 0.344431, acc.: 81.25%] [G loss: 3.794774]\n",
      "epoch:33 step:26384 [D loss: 0.409681, acc.: 85.16%] [G loss: 3.560028]\n",
      "epoch:33 step:26385 [D loss: 0.304182, acc.: 89.06%] [G loss: 3.473578]\n",
      "epoch:33 step:26386 [D loss: 0.312757, acc.: 86.72%] [G loss: 5.223398]\n",
      "epoch:33 step:26387 [D loss: 0.254818, acc.: 89.84%] [G loss: 3.133654]\n",
      "epoch:33 step:26388 [D loss: 0.209222, acc.: 90.62%] [G loss: 5.477518]\n",
      "epoch:33 step:26389 [D loss: 0.375445, acc.: 83.59%] [G loss: 5.619322]\n",
      "epoch:33 step:26390 [D loss: 0.344791, acc.: 85.94%] [G loss: 5.312644]\n",
      "epoch:33 step:26391 [D loss: 0.332546, acc.: 85.94%] [G loss: 3.786726]\n",
      "epoch:33 step:26392 [D loss: 0.395707, acc.: 80.47%] [G loss: 3.818504]\n",
      "epoch:33 step:26393 [D loss: 0.322160, acc.: 85.94%] [G loss: 3.788970]\n",
      "epoch:33 step:26394 [D loss: 0.312532, acc.: 83.59%] [G loss: 3.636765]\n",
      "epoch:33 step:26395 [D loss: 0.273619, acc.: 88.28%] [G loss: 3.405403]\n",
      "epoch:33 step:26396 [D loss: 0.489879, acc.: 76.56%] [G loss: 3.178285]\n",
      "epoch:33 step:26397 [D loss: 0.355919, acc.: 81.25%] [G loss: 3.782377]\n",
      "epoch:33 step:26398 [D loss: 0.366975, acc.: 82.81%] [G loss: 5.392999]\n",
      "epoch:33 step:26399 [D loss: 0.223153, acc.: 92.97%] [G loss: 4.586822]\n",
      "epoch:33 step:26400 [D loss: 0.368070, acc.: 82.03%] [G loss: 4.008377]\n",
      "epoch:33 step:26401 [D loss: 0.297400, acc.: 85.94%] [G loss: 3.352612]\n",
      "epoch:33 step:26402 [D loss: 0.470272, acc.: 78.91%] [G loss: 3.006934]\n",
      "epoch:33 step:26403 [D loss: 0.292792, acc.: 89.84%] [G loss: 3.557421]\n",
      "epoch:33 step:26404 [D loss: 0.308826, acc.: 88.28%] [G loss: 3.332577]\n",
      "epoch:33 step:26405 [D loss: 0.324204, acc.: 84.38%] [G loss: 3.364216]\n",
      "epoch:33 step:26406 [D loss: 0.399335, acc.: 82.81%] [G loss: 4.052195]\n",
      "epoch:33 step:26407 [D loss: 0.392011, acc.: 82.03%] [G loss: 3.756697]\n",
      "epoch:33 step:26408 [D loss: 0.347877, acc.: 86.72%] [G loss: 3.954339]\n",
      "epoch:33 step:26409 [D loss: 0.243515, acc.: 92.19%] [G loss: 4.077829]\n",
      "epoch:33 step:26410 [D loss: 0.315619, acc.: 85.16%] [G loss: 4.505280]\n",
      "epoch:33 step:26411 [D loss: 0.309625, acc.: 86.72%] [G loss: 4.082146]\n",
      "epoch:33 step:26412 [D loss: 0.372187, acc.: 83.59%] [G loss: 4.540229]\n",
      "epoch:33 step:26413 [D loss: 0.267971, acc.: 89.06%] [G loss: 4.960341]\n",
      "epoch:33 step:26414 [D loss: 0.323584, acc.: 85.94%] [G loss: 4.164818]\n",
      "epoch:33 step:26415 [D loss: 0.246727, acc.: 89.06%] [G loss: 3.590842]\n",
      "epoch:33 step:26416 [D loss: 0.359628, acc.: 82.81%] [G loss: 3.775388]\n",
      "epoch:33 step:26417 [D loss: 0.229553, acc.: 88.28%] [G loss: 3.989006]\n",
      "epoch:33 step:26418 [D loss: 0.345022, acc.: 80.47%] [G loss: 2.532021]\n",
      "epoch:33 step:26419 [D loss: 0.374735, acc.: 81.25%] [G loss: 4.582391]\n",
      "epoch:33 step:26420 [D loss: 0.471190, acc.: 83.59%] [G loss: 3.953657]\n",
      "epoch:33 step:26421 [D loss: 0.297624, acc.: 85.94%] [G loss: 3.688337]\n",
      "epoch:33 step:26422 [D loss: 0.355563, acc.: 85.94%] [G loss: 3.121857]\n",
      "epoch:33 step:26423 [D loss: 0.364952, acc.: 83.59%] [G loss: 3.819615]\n",
      "epoch:33 step:26424 [D loss: 0.286713, acc.: 87.50%] [G loss: 2.884017]\n",
      "epoch:33 step:26425 [D loss: 0.294523, acc.: 83.59%] [G loss: 3.371819]\n",
      "epoch:33 step:26426 [D loss: 0.397309, acc.: 81.25%] [G loss: 2.907407]\n",
      "epoch:33 step:26427 [D loss: 0.256394, acc.: 85.94%] [G loss: 3.868699]\n",
      "epoch:33 step:26428 [D loss: 0.442377, acc.: 79.69%] [G loss: 4.790845]\n",
      "epoch:33 step:26429 [D loss: 0.464258, acc.: 81.25%] [G loss: 5.788045]\n",
      "epoch:33 step:26430 [D loss: 0.977873, acc.: 71.88%] [G loss: 9.866018]\n",
      "epoch:33 step:26431 [D loss: 3.041250, acc.: 44.53%] [G loss: 4.701510]\n",
      "epoch:33 step:26432 [D loss: 0.547206, acc.: 75.78%] [G loss: 6.260326]\n",
      "epoch:33 step:26433 [D loss: 0.497853, acc.: 75.00%] [G loss: 4.879884]\n",
      "epoch:33 step:26434 [D loss: 0.260006, acc.: 87.50%] [G loss: 4.328994]\n",
      "epoch:33 step:26435 [D loss: 0.200935, acc.: 92.97%] [G loss: 5.033234]\n",
      "epoch:33 step:26436 [D loss: 0.255712, acc.: 88.28%] [G loss: 3.714940]\n",
      "epoch:33 step:26437 [D loss: 0.320993, acc.: 85.94%] [G loss: 3.709675]\n",
      "epoch:33 step:26438 [D loss: 0.267426, acc.: 89.06%] [G loss: 3.758069]\n",
      "epoch:33 step:26439 [D loss: 0.364977, acc.: 78.91%] [G loss: 2.946786]\n",
      "epoch:33 step:26440 [D loss: 0.278029, acc.: 85.94%] [G loss: 4.579036]\n",
      "epoch:33 step:26441 [D loss: 0.373054, acc.: 84.38%] [G loss: 3.970204]\n",
      "epoch:33 step:26442 [D loss: 0.335733, acc.: 80.47%] [G loss: 3.169767]\n",
      "epoch:33 step:26443 [D loss: 0.308453, acc.: 85.16%] [G loss: 2.920891]\n",
      "epoch:33 step:26444 [D loss: 0.251471, acc.: 88.28%] [G loss: 3.118882]\n",
      "epoch:33 step:26445 [D loss: 0.292629, acc.: 84.38%] [G loss: 3.659084]\n",
      "epoch:33 step:26446 [D loss: 0.304810, acc.: 85.94%] [G loss: 2.882257]\n",
      "epoch:33 step:26447 [D loss: 0.330767, acc.: 87.50%] [G loss: 2.835852]\n",
      "epoch:33 step:26448 [D loss: 0.345691, acc.: 84.38%] [G loss: 3.179846]\n",
      "epoch:33 step:26449 [D loss: 0.361569, acc.: 79.69%] [G loss: 3.102508]\n",
      "epoch:33 step:26450 [D loss: 0.300532, acc.: 85.94%] [G loss: 2.395380]\n",
      "epoch:33 step:26451 [D loss: 0.306353, acc.: 88.28%] [G loss: 3.199032]\n",
      "epoch:33 step:26452 [D loss: 0.264737, acc.: 87.50%] [G loss: 2.689404]\n",
      "epoch:33 step:26453 [D loss: 0.302082, acc.: 85.16%] [G loss: 3.030121]\n",
      "epoch:33 step:26454 [D loss: 0.211334, acc.: 92.97%] [G loss: 3.437549]\n",
      "epoch:33 step:26455 [D loss: 0.367431, acc.: 82.81%] [G loss: 3.110171]\n",
      "epoch:33 step:26456 [D loss: 0.358198, acc.: 84.38%] [G loss: 3.030016]\n",
      "epoch:33 step:26457 [D loss: 0.348773, acc.: 85.16%] [G loss: 4.110813]\n",
      "epoch:33 step:26458 [D loss: 0.503069, acc.: 80.47%] [G loss: 3.062000]\n",
      "epoch:33 step:26459 [D loss: 0.279995, acc.: 86.72%] [G loss: 3.369627]\n",
      "epoch:33 step:26460 [D loss: 0.227762, acc.: 90.62%] [G loss: 3.479465]\n",
      "epoch:33 step:26461 [D loss: 0.240887, acc.: 92.19%] [G loss: 3.710047]\n",
      "epoch:33 step:26462 [D loss: 0.245931, acc.: 90.62%] [G loss: 3.819157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26463 [D loss: 0.334203, acc.: 87.50%] [G loss: 2.695724]\n",
      "epoch:33 step:26464 [D loss: 0.281840, acc.: 89.84%] [G loss: 2.988784]\n",
      "epoch:33 step:26465 [D loss: 0.355132, acc.: 84.38%] [G loss: 3.344976]\n",
      "epoch:33 step:26466 [D loss: 0.354848, acc.: 86.72%] [G loss: 3.447418]\n",
      "epoch:33 step:26467 [D loss: 0.230878, acc.: 92.19%] [G loss: 3.422754]\n",
      "epoch:33 step:26468 [D loss: 0.305604, acc.: 86.72%] [G loss: 2.904979]\n",
      "epoch:33 step:26469 [D loss: 0.273059, acc.: 89.84%] [G loss: 2.829672]\n",
      "epoch:33 step:26470 [D loss: 0.310054, acc.: 85.16%] [G loss: 2.629407]\n",
      "epoch:33 step:26471 [D loss: 0.280553, acc.: 86.72%] [G loss: 2.797397]\n",
      "epoch:33 step:26472 [D loss: 0.255641, acc.: 87.50%] [G loss: 2.732248]\n",
      "epoch:33 step:26473 [D loss: 0.317342, acc.: 86.72%] [G loss: 3.459619]\n",
      "epoch:33 step:26474 [D loss: 0.277319, acc.: 86.72%] [G loss: 3.419669]\n",
      "epoch:33 step:26475 [D loss: 0.257975, acc.: 87.50%] [G loss: 2.755078]\n",
      "epoch:33 step:26476 [D loss: 0.295962, acc.: 84.38%] [G loss: 2.647035]\n",
      "epoch:33 step:26477 [D loss: 0.391763, acc.: 79.69%] [G loss: 4.287411]\n",
      "epoch:33 step:26478 [D loss: 0.297060, acc.: 88.28%] [G loss: 2.700918]\n",
      "epoch:33 step:26479 [D loss: 0.249249, acc.: 89.84%] [G loss: 3.595279]\n",
      "epoch:33 step:26480 [D loss: 0.265126, acc.: 89.06%] [G loss: 3.175482]\n",
      "epoch:33 step:26481 [D loss: 0.314913, acc.: 83.59%] [G loss: 3.158071]\n",
      "epoch:33 step:26482 [D loss: 0.343979, acc.: 82.81%] [G loss: 2.614169]\n",
      "epoch:33 step:26483 [D loss: 0.267406, acc.: 88.28%] [G loss: 2.944981]\n",
      "epoch:33 step:26484 [D loss: 0.298028, acc.: 87.50%] [G loss: 2.780822]\n",
      "epoch:33 step:26485 [D loss: 0.245402, acc.: 89.84%] [G loss: 3.361771]\n",
      "epoch:33 step:26486 [D loss: 0.302119, acc.: 86.72%] [G loss: 2.459124]\n",
      "epoch:33 step:26487 [D loss: 0.344469, acc.: 82.81%] [G loss: 2.570398]\n",
      "epoch:33 step:26488 [D loss: 0.350958, acc.: 84.38%] [G loss: 1.995877]\n",
      "epoch:33 step:26489 [D loss: 0.306103, acc.: 86.72%] [G loss: 2.757949]\n",
      "epoch:33 step:26490 [D loss: 0.371401, acc.: 83.59%] [G loss: 2.443000]\n",
      "epoch:33 step:26491 [D loss: 0.342939, acc.: 86.72%] [G loss: 2.810026]\n",
      "epoch:33 step:26492 [D loss: 0.300058, acc.: 87.50%] [G loss: 2.837920]\n",
      "epoch:33 step:26493 [D loss: 0.323818, acc.: 89.06%] [G loss: 2.790769]\n",
      "epoch:33 step:26494 [D loss: 0.311245, acc.: 89.84%] [G loss: 2.531463]\n",
      "epoch:33 step:26495 [D loss: 0.251357, acc.: 87.50%] [G loss: 2.978141]\n",
      "epoch:33 step:26496 [D loss: 0.343757, acc.: 85.94%] [G loss: 3.019214]\n",
      "epoch:33 step:26497 [D loss: 0.313872, acc.: 87.50%] [G loss: 2.958884]\n",
      "epoch:33 step:26498 [D loss: 0.355779, acc.: 84.38%] [G loss: 2.591382]\n",
      "epoch:33 step:26499 [D loss: 0.276771, acc.: 89.06%] [G loss: 3.166775]\n",
      "epoch:33 step:26500 [D loss: 0.280342, acc.: 91.41%] [G loss: 3.098786]\n",
      "epoch:33 step:26501 [D loss: 0.293087, acc.: 86.72%] [G loss: 3.652230]\n",
      "epoch:33 step:26502 [D loss: 0.300327, acc.: 83.59%] [G loss: 3.715980]\n",
      "epoch:33 step:26503 [D loss: 0.251308, acc.: 88.28%] [G loss: 2.929699]\n",
      "epoch:33 step:26504 [D loss: 0.312338, acc.: 87.50%] [G loss: 4.274391]\n",
      "epoch:33 step:26505 [D loss: 0.294961, acc.: 88.28%] [G loss: 2.798891]\n",
      "epoch:33 step:26506 [D loss: 0.204582, acc.: 91.41%] [G loss: 3.127158]\n",
      "epoch:33 step:26507 [D loss: 0.272282, acc.: 85.94%] [G loss: 3.049768]\n",
      "epoch:33 step:26508 [D loss: 0.286698, acc.: 86.72%] [G loss: 4.375783]\n",
      "epoch:33 step:26509 [D loss: 0.285645, acc.: 85.94%] [G loss: 3.102239]\n",
      "epoch:33 step:26510 [D loss: 0.378612, acc.: 80.47%] [G loss: 3.030510]\n",
      "epoch:33 step:26511 [D loss: 0.281358, acc.: 88.28%] [G loss: 2.362148]\n",
      "epoch:33 step:26512 [D loss: 0.372579, acc.: 83.59%] [G loss: 2.679508]\n",
      "epoch:33 step:26513 [D loss: 0.273309, acc.: 89.84%] [G loss: 2.727229]\n",
      "epoch:33 step:26514 [D loss: 0.329459, acc.: 83.59%] [G loss: 3.254294]\n",
      "epoch:33 step:26515 [D loss: 0.393625, acc.: 78.12%] [G loss: 2.687253]\n",
      "epoch:33 step:26516 [D loss: 0.266553, acc.: 89.84%] [G loss: 2.558888]\n",
      "epoch:33 step:26517 [D loss: 0.358677, acc.: 82.81%] [G loss: 3.575739]\n",
      "epoch:33 step:26518 [D loss: 0.397369, acc.: 80.47%] [G loss: 2.999590]\n",
      "epoch:33 step:26519 [D loss: 0.346922, acc.: 86.72%] [G loss: 2.624974]\n",
      "epoch:33 step:26520 [D loss: 0.310425, acc.: 83.59%] [G loss: 3.636898]\n",
      "epoch:33 step:26521 [D loss: 0.388454, acc.: 81.25%] [G loss: 3.807447]\n",
      "epoch:33 step:26522 [D loss: 0.272622, acc.: 87.50%] [G loss: 4.037261]\n",
      "epoch:33 step:26523 [D loss: 0.279237, acc.: 89.06%] [G loss: 4.411908]\n",
      "epoch:33 step:26524 [D loss: 0.330335, acc.: 88.28%] [G loss: 3.668868]\n",
      "epoch:33 step:26525 [D loss: 0.281937, acc.: 86.72%] [G loss: 4.061555]\n",
      "epoch:33 step:26526 [D loss: 0.252131, acc.: 89.84%] [G loss: 4.072055]\n",
      "epoch:33 step:26527 [D loss: 0.356574, acc.: 84.38%] [G loss: 3.390938]\n",
      "epoch:33 step:26528 [D loss: 0.216124, acc.: 88.28%] [G loss: 3.666531]\n",
      "epoch:33 step:26529 [D loss: 0.278127, acc.: 86.72%] [G loss: 3.687312]\n",
      "epoch:33 step:26530 [D loss: 0.336141, acc.: 85.94%] [G loss: 2.896454]\n",
      "epoch:33 step:26531 [D loss: 0.360171, acc.: 82.81%] [G loss: 3.095864]\n",
      "epoch:33 step:26532 [D loss: 0.273732, acc.: 89.06%] [G loss: 2.702485]\n",
      "epoch:33 step:26533 [D loss: 0.305229, acc.: 88.28%] [G loss: 1.928148]\n",
      "epoch:33 step:26534 [D loss: 0.256898, acc.: 91.41%] [G loss: 2.917857]\n",
      "epoch:33 step:26535 [D loss: 0.385404, acc.: 78.12%] [G loss: 3.401975]\n",
      "epoch:33 step:26536 [D loss: 0.238968, acc.: 90.62%] [G loss: 3.629725]\n",
      "epoch:33 step:26537 [D loss: 0.321328, acc.: 87.50%] [G loss: 3.435740]\n",
      "epoch:33 step:26538 [D loss: 0.358325, acc.: 81.25%] [G loss: 2.580542]\n",
      "epoch:33 step:26539 [D loss: 0.354597, acc.: 82.81%] [G loss: 3.022682]\n",
      "epoch:33 step:26540 [D loss: 0.245144, acc.: 91.41%] [G loss: 2.551321]\n",
      "epoch:33 step:26541 [D loss: 0.372211, acc.: 82.81%] [G loss: 2.553230]\n",
      "epoch:33 step:26542 [D loss: 0.296186, acc.: 89.06%] [G loss: 3.565627]\n",
      "epoch:33 step:26543 [D loss: 0.274979, acc.: 89.84%] [G loss: 4.408603]\n",
      "epoch:33 step:26544 [D loss: 0.354202, acc.: 80.47%] [G loss: 6.092707]\n",
      "epoch:33 step:26545 [D loss: 0.387677, acc.: 81.25%] [G loss: 4.910315]\n",
      "epoch:33 step:26546 [D loss: 0.313753, acc.: 84.38%] [G loss: 4.679904]\n",
      "epoch:33 step:26547 [D loss: 0.220018, acc.: 90.62%] [G loss: 3.064124]\n",
      "epoch:33 step:26548 [D loss: 0.257234, acc.: 89.06%] [G loss: 3.817963]\n",
      "epoch:33 step:26549 [D loss: 0.337653, acc.: 85.16%] [G loss: 3.161458]\n",
      "epoch:33 step:26550 [D loss: 0.349808, acc.: 82.81%] [G loss: 3.146061]\n",
      "epoch:33 step:26551 [D loss: 0.231675, acc.: 90.62%] [G loss: 3.061852]\n",
      "epoch:33 step:26552 [D loss: 0.358304, acc.: 84.38%] [G loss: 4.330560]\n",
      "epoch:33 step:26553 [D loss: 0.408520, acc.: 84.38%] [G loss: 4.425493]\n",
      "epoch:33 step:26554 [D loss: 0.348289, acc.: 85.16%] [G loss: 3.341846]\n",
      "epoch:34 step:26555 [D loss: 0.292217, acc.: 85.94%] [G loss: 5.039519]\n",
      "epoch:34 step:26556 [D loss: 0.214076, acc.: 91.41%] [G loss: 4.680532]\n",
      "epoch:34 step:26557 [D loss: 0.347322, acc.: 82.81%] [G loss: 4.592841]\n",
      "epoch:34 step:26558 [D loss: 0.333070, acc.: 85.16%] [G loss: 3.476802]\n",
      "epoch:34 step:26559 [D loss: 0.261137, acc.: 91.41%] [G loss: 2.980059]\n",
      "epoch:34 step:26560 [D loss: 0.358472, acc.: 79.69%] [G loss: 3.999050]\n",
      "epoch:34 step:26561 [D loss: 0.284246, acc.: 86.72%] [G loss: 3.240121]\n",
      "epoch:34 step:26562 [D loss: 0.268696, acc.: 89.06%] [G loss: 3.808796]\n",
      "epoch:34 step:26563 [D loss: 0.439776, acc.: 80.47%] [G loss: 2.820751]\n",
      "epoch:34 step:26564 [D loss: 0.324989, acc.: 82.03%] [G loss: 3.893883]\n",
      "epoch:34 step:26565 [D loss: 0.266340, acc.: 91.41%] [G loss: 3.916519]\n",
      "epoch:34 step:26566 [D loss: 0.264765, acc.: 89.84%] [G loss: 3.407400]\n",
      "epoch:34 step:26567 [D loss: 0.171300, acc.: 95.31%] [G loss: 3.488121]\n",
      "epoch:34 step:26568 [D loss: 0.330175, acc.: 86.72%] [G loss: 3.134194]\n",
      "epoch:34 step:26569 [D loss: 0.252902, acc.: 92.19%] [G loss: 2.691719]\n",
      "epoch:34 step:26570 [D loss: 0.178802, acc.: 92.19%] [G loss: 4.202958]\n",
      "epoch:34 step:26571 [D loss: 0.245709, acc.: 89.06%] [G loss: 4.390803]\n",
      "epoch:34 step:26572 [D loss: 0.293998, acc.: 84.38%] [G loss: 4.238111]\n",
      "epoch:34 step:26573 [D loss: 0.298099, acc.: 85.16%] [G loss: 3.995060]\n",
      "epoch:34 step:26574 [D loss: 0.323815, acc.: 88.28%] [G loss: 5.779414]\n",
      "epoch:34 step:26575 [D loss: 0.531083, acc.: 69.53%] [G loss: 4.315782]\n",
      "epoch:34 step:26576 [D loss: 0.251874, acc.: 89.84%] [G loss: 4.818851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26577 [D loss: 0.298450, acc.: 88.28%] [G loss: 5.373317]\n",
      "epoch:34 step:26578 [D loss: 0.365769, acc.: 83.59%] [G loss: 3.908577]\n",
      "epoch:34 step:26579 [D loss: 0.300352, acc.: 85.94%] [G loss: 3.626918]\n",
      "epoch:34 step:26580 [D loss: 0.263277, acc.: 90.62%] [G loss: 3.297564]\n",
      "epoch:34 step:26581 [D loss: 0.240283, acc.: 86.72%] [G loss: 3.039323]\n",
      "epoch:34 step:26582 [D loss: 0.448224, acc.: 74.22%] [G loss: 2.489319]\n",
      "epoch:34 step:26583 [D loss: 0.327418, acc.: 82.81%] [G loss: 3.805786]\n",
      "epoch:34 step:26584 [D loss: 0.266008, acc.: 86.72%] [G loss: 3.175750]\n",
      "epoch:34 step:26585 [D loss: 0.367855, acc.: 81.25%] [G loss: 4.191013]\n",
      "epoch:34 step:26586 [D loss: 0.347111, acc.: 88.28%] [G loss: 2.394559]\n",
      "epoch:34 step:26587 [D loss: 0.253530, acc.: 89.84%] [G loss: 4.024327]\n",
      "epoch:34 step:26588 [D loss: 0.331192, acc.: 87.50%] [G loss: 2.558453]\n",
      "epoch:34 step:26589 [D loss: 0.383941, acc.: 85.94%] [G loss: 2.224450]\n",
      "epoch:34 step:26590 [D loss: 0.240557, acc.: 92.19%] [G loss: 2.835637]\n",
      "epoch:34 step:26591 [D loss: 0.350470, acc.: 82.81%] [G loss: 2.949400]\n",
      "epoch:34 step:26592 [D loss: 0.285179, acc.: 85.94%] [G loss: 2.905428]\n",
      "epoch:34 step:26593 [D loss: 0.451443, acc.: 78.12%] [G loss: 2.450943]\n",
      "epoch:34 step:26594 [D loss: 0.281237, acc.: 88.28%] [G loss: 3.408236]\n",
      "epoch:34 step:26595 [D loss: 0.345252, acc.: 82.81%] [G loss: 3.589124]\n",
      "epoch:34 step:26596 [D loss: 0.339225, acc.: 85.16%] [G loss: 2.973301]\n",
      "epoch:34 step:26597 [D loss: 0.377879, acc.: 81.25%] [G loss: 2.775122]\n",
      "epoch:34 step:26598 [D loss: 0.350941, acc.: 82.81%] [G loss: 2.482086]\n",
      "epoch:34 step:26599 [D loss: 0.335152, acc.: 85.16%] [G loss: 2.924610]\n",
      "epoch:34 step:26600 [D loss: 0.367107, acc.: 85.16%] [G loss: 2.613241]\n",
      "epoch:34 step:26601 [D loss: 0.440987, acc.: 75.78%] [G loss: 4.098421]\n",
      "epoch:34 step:26602 [D loss: 0.394804, acc.: 84.38%] [G loss: 4.224041]\n",
      "epoch:34 step:26603 [D loss: 0.340317, acc.: 86.72%] [G loss: 3.670443]\n",
      "epoch:34 step:26604 [D loss: 0.319529, acc.: 80.47%] [G loss: 3.839345]\n",
      "epoch:34 step:26605 [D loss: 0.295309, acc.: 87.50%] [G loss: 4.387264]\n",
      "epoch:34 step:26606 [D loss: 0.213359, acc.: 92.19%] [G loss: 2.800239]\n",
      "epoch:34 step:26607 [D loss: 0.323969, acc.: 84.38%] [G loss: 4.749329]\n",
      "epoch:34 step:26608 [D loss: 0.425106, acc.: 81.25%] [G loss: 3.334827]\n",
      "epoch:34 step:26609 [D loss: 0.331935, acc.: 84.38%] [G loss: 3.634962]\n",
      "epoch:34 step:26610 [D loss: 0.427771, acc.: 80.47%] [G loss: 2.852589]\n",
      "epoch:34 step:26611 [D loss: 0.469179, acc.: 80.47%] [G loss: 3.734421]\n",
      "epoch:34 step:26612 [D loss: 0.354029, acc.: 84.38%] [G loss: 2.987760]\n",
      "epoch:34 step:26613 [D loss: 0.415014, acc.: 84.38%] [G loss: 3.245279]\n",
      "epoch:34 step:26614 [D loss: 0.277033, acc.: 91.41%] [G loss: 3.062743]\n",
      "epoch:34 step:26615 [D loss: 0.392171, acc.: 80.47%] [G loss: 3.108622]\n",
      "epoch:34 step:26616 [D loss: 0.269762, acc.: 89.84%] [G loss: 3.523718]\n",
      "epoch:34 step:26617 [D loss: 0.317727, acc.: 85.94%] [G loss: 3.483186]\n",
      "epoch:34 step:26618 [D loss: 0.220197, acc.: 89.84%] [G loss: 3.033942]\n",
      "epoch:34 step:26619 [D loss: 0.272655, acc.: 87.50%] [G loss: 3.609112]\n",
      "epoch:34 step:26620 [D loss: 0.264849, acc.: 89.84%] [G loss: 2.560899]\n",
      "epoch:34 step:26621 [D loss: 0.259451, acc.: 92.19%] [G loss: 3.671754]\n",
      "epoch:34 step:26622 [D loss: 0.418124, acc.: 78.12%] [G loss: 3.165609]\n",
      "epoch:34 step:26623 [D loss: 0.328809, acc.: 85.94%] [G loss: 4.680363]\n",
      "epoch:34 step:26624 [D loss: 0.381862, acc.: 81.25%] [G loss: 4.549757]\n",
      "epoch:34 step:26625 [D loss: 0.379032, acc.: 82.03%] [G loss: 3.487088]\n",
      "epoch:34 step:26626 [D loss: 0.309975, acc.: 83.59%] [G loss: 5.733918]\n",
      "epoch:34 step:26627 [D loss: 0.330208, acc.: 84.38%] [G loss: 5.093182]\n",
      "epoch:34 step:26628 [D loss: 0.257678, acc.: 89.06%] [G loss: 4.743078]\n",
      "epoch:34 step:26629 [D loss: 0.236201, acc.: 91.41%] [G loss: 3.535513]\n",
      "epoch:34 step:26630 [D loss: 0.319105, acc.: 89.84%] [G loss: 3.528697]\n",
      "epoch:34 step:26631 [D loss: 0.342295, acc.: 82.03%] [G loss: 4.963944]\n",
      "epoch:34 step:26632 [D loss: 0.429823, acc.: 78.91%] [G loss: 2.973674]\n",
      "epoch:34 step:26633 [D loss: 0.280046, acc.: 86.72%] [G loss: 4.361039]\n",
      "epoch:34 step:26634 [D loss: 0.270401, acc.: 87.50%] [G loss: 3.132247]\n",
      "epoch:34 step:26635 [D loss: 0.274306, acc.: 86.72%] [G loss: 4.547859]\n",
      "epoch:34 step:26636 [D loss: 0.325791, acc.: 88.28%] [G loss: 4.054983]\n",
      "epoch:34 step:26637 [D loss: 0.228997, acc.: 91.41%] [G loss: 3.314554]\n",
      "epoch:34 step:26638 [D loss: 0.307132, acc.: 88.28%] [G loss: 3.568588]\n",
      "epoch:34 step:26639 [D loss: 0.360157, acc.: 87.50%] [G loss: 3.343237]\n",
      "epoch:34 step:26640 [D loss: 0.387614, acc.: 82.03%] [G loss: 3.530348]\n",
      "epoch:34 step:26641 [D loss: 0.288966, acc.: 88.28%] [G loss: 2.829243]\n",
      "epoch:34 step:26642 [D loss: 0.335441, acc.: 84.38%] [G loss: 2.551600]\n",
      "epoch:34 step:26643 [D loss: 0.442539, acc.: 81.25%] [G loss: 2.683500]\n",
      "epoch:34 step:26644 [D loss: 0.484982, acc.: 76.56%] [G loss: 3.251878]\n",
      "epoch:34 step:26645 [D loss: 0.197465, acc.: 94.53%] [G loss: 3.753839]\n",
      "epoch:34 step:26646 [D loss: 0.366722, acc.: 85.94%] [G loss: 3.589627]\n",
      "epoch:34 step:26647 [D loss: 0.384387, acc.: 82.03%] [G loss: 4.057101]\n",
      "epoch:34 step:26648 [D loss: 0.283491, acc.: 89.06%] [G loss: 3.207158]\n",
      "epoch:34 step:26649 [D loss: 0.335513, acc.: 82.03%] [G loss: 3.331758]\n",
      "epoch:34 step:26650 [D loss: 0.283818, acc.: 89.06%] [G loss: 2.978732]\n",
      "epoch:34 step:26651 [D loss: 0.292444, acc.: 85.16%] [G loss: 3.219107]\n",
      "epoch:34 step:26652 [D loss: 0.276886, acc.: 89.06%] [G loss: 2.886348]\n",
      "epoch:34 step:26653 [D loss: 0.404951, acc.: 79.69%] [G loss: 2.686945]\n",
      "epoch:34 step:26654 [D loss: 0.258341, acc.: 89.84%] [G loss: 2.594175]\n",
      "epoch:34 step:26655 [D loss: 0.373489, acc.: 82.81%] [G loss: 2.911138]\n",
      "epoch:34 step:26656 [D loss: 0.280053, acc.: 90.62%] [G loss: 2.744840]\n",
      "epoch:34 step:26657 [D loss: 0.374474, acc.: 82.03%] [G loss: 2.739376]\n",
      "epoch:34 step:26658 [D loss: 0.339776, acc.: 86.72%] [G loss: 3.614686]\n",
      "epoch:34 step:26659 [D loss: 0.273715, acc.: 85.94%] [G loss: 4.329742]\n",
      "epoch:34 step:26660 [D loss: 0.363162, acc.: 80.47%] [G loss: 2.856057]\n",
      "epoch:34 step:26661 [D loss: 0.320941, acc.: 87.50%] [G loss: 4.100576]\n",
      "epoch:34 step:26662 [D loss: 0.298722, acc.: 85.94%] [G loss: 4.682100]\n",
      "epoch:34 step:26663 [D loss: 0.472553, acc.: 78.12%] [G loss: 3.172159]\n",
      "epoch:34 step:26664 [D loss: 0.260768, acc.: 90.62%] [G loss: 3.782640]\n",
      "epoch:34 step:26665 [D loss: 0.319467, acc.: 89.06%] [G loss: 2.333431]\n",
      "epoch:34 step:26666 [D loss: 0.247389, acc.: 91.41%] [G loss: 3.564765]\n",
      "epoch:34 step:26667 [D loss: 0.357087, acc.: 83.59%] [G loss: 4.131999]\n",
      "epoch:34 step:26668 [D loss: 0.437804, acc.: 77.34%] [G loss: 4.929174]\n",
      "epoch:34 step:26669 [D loss: 0.650504, acc.: 72.66%] [G loss: 3.861569]\n",
      "epoch:34 step:26670 [D loss: 0.713095, acc.: 71.09%] [G loss: 3.723227]\n",
      "epoch:34 step:26671 [D loss: 0.266867, acc.: 89.84%] [G loss: 6.250507]\n",
      "epoch:34 step:26672 [D loss: 0.320729, acc.: 87.50%] [G loss: 4.570714]\n",
      "epoch:34 step:26673 [D loss: 0.309515, acc.: 87.50%] [G loss: 4.029505]\n",
      "epoch:34 step:26674 [D loss: 0.456120, acc.: 78.91%] [G loss: 3.245061]\n",
      "epoch:34 step:26675 [D loss: 0.229646, acc.: 89.84%] [G loss: 4.696529]\n",
      "epoch:34 step:26676 [D loss: 0.301264, acc.: 86.72%] [G loss: 3.034462]\n",
      "epoch:34 step:26677 [D loss: 0.271427, acc.: 89.06%] [G loss: 3.978797]\n",
      "epoch:34 step:26678 [D loss: 0.308429, acc.: 86.72%] [G loss: 2.578880]\n",
      "epoch:34 step:26679 [D loss: 0.321277, acc.: 86.72%] [G loss: 3.397034]\n",
      "epoch:34 step:26680 [D loss: 0.250118, acc.: 90.62%] [G loss: 6.676368]\n",
      "epoch:34 step:26681 [D loss: 0.218074, acc.: 88.28%] [G loss: 6.614690]\n",
      "epoch:34 step:26682 [D loss: 0.158144, acc.: 93.75%] [G loss: 6.442700]\n",
      "epoch:34 step:26683 [D loss: 0.216473, acc.: 91.41%] [G loss: 6.719057]\n",
      "epoch:34 step:26684 [D loss: 0.164483, acc.: 94.53%] [G loss: 6.453106]\n",
      "epoch:34 step:26685 [D loss: 0.245417, acc.: 89.06%] [G loss: 7.292503]\n",
      "epoch:34 step:26686 [D loss: 0.169513, acc.: 90.62%] [G loss: 6.048342]\n",
      "epoch:34 step:26687 [D loss: 0.269803, acc.: 86.72%] [G loss: 4.823466]\n",
      "epoch:34 step:26688 [D loss: 0.169075, acc.: 94.53%] [G loss: 3.888807]\n",
      "epoch:34 step:26689 [D loss: 0.236001, acc.: 86.72%] [G loss: 4.605938]\n",
      "epoch:34 step:26690 [D loss: 0.221931, acc.: 89.84%] [G loss: 3.508652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26691 [D loss: 0.384667, acc.: 83.59%] [G loss: 3.369364]\n",
      "epoch:34 step:26692 [D loss: 0.256227, acc.: 85.94%] [G loss: 3.372087]\n",
      "epoch:34 step:26693 [D loss: 0.287447, acc.: 84.38%] [G loss: 4.098000]\n",
      "epoch:34 step:26694 [D loss: 0.239139, acc.: 88.28%] [G loss: 3.851648]\n",
      "epoch:34 step:26695 [D loss: 0.305722, acc.: 82.03%] [G loss: 4.074306]\n",
      "epoch:34 step:26696 [D loss: 0.349972, acc.: 82.03%] [G loss: 4.084551]\n",
      "epoch:34 step:26697 [D loss: 0.281844, acc.: 85.94%] [G loss: 2.871615]\n",
      "epoch:34 step:26698 [D loss: 0.255364, acc.: 89.06%] [G loss: 3.381728]\n",
      "epoch:34 step:26699 [D loss: 0.400837, acc.: 80.47%] [G loss: 2.938418]\n",
      "epoch:34 step:26700 [D loss: 0.332536, acc.: 82.03%] [G loss: 3.412134]\n",
      "epoch:34 step:26701 [D loss: 0.305064, acc.: 85.94%] [G loss: 2.851658]\n",
      "epoch:34 step:26702 [D loss: 0.354478, acc.: 81.25%] [G loss: 3.670094]\n",
      "epoch:34 step:26703 [D loss: 0.334781, acc.: 82.03%] [G loss: 3.128968]\n",
      "epoch:34 step:26704 [D loss: 0.274190, acc.: 87.50%] [G loss: 3.952639]\n",
      "epoch:34 step:26705 [D loss: 0.275772, acc.: 87.50%] [G loss: 3.519283]\n",
      "epoch:34 step:26706 [D loss: 0.323090, acc.: 86.72%] [G loss: 1.798779]\n",
      "epoch:34 step:26707 [D loss: 0.274306, acc.: 87.50%] [G loss: 3.004714]\n",
      "epoch:34 step:26708 [D loss: 0.311731, acc.: 85.94%] [G loss: 3.575386]\n",
      "epoch:34 step:26709 [D loss: 0.253594, acc.: 89.06%] [G loss: 2.980179]\n",
      "epoch:34 step:26710 [D loss: 0.243503, acc.: 90.62%] [G loss: 3.867851]\n",
      "epoch:34 step:26711 [D loss: 0.341395, acc.: 82.81%] [G loss: 3.306573]\n",
      "epoch:34 step:26712 [D loss: 0.270816, acc.: 88.28%] [G loss: 3.537599]\n",
      "epoch:34 step:26713 [D loss: 0.321814, acc.: 83.59%] [G loss: 3.458020]\n",
      "epoch:34 step:26714 [D loss: 0.263328, acc.: 88.28%] [G loss: 2.988490]\n",
      "epoch:34 step:26715 [D loss: 0.314164, acc.: 83.59%] [G loss: 2.378875]\n",
      "epoch:34 step:26716 [D loss: 0.236828, acc.: 89.84%] [G loss: 2.660456]\n",
      "epoch:34 step:26717 [D loss: 0.361114, acc.: 84.38%] [G loss: 3.732846]\n",
      "epoch:34 step:26718 [D loss: 0.300817, acc.: 86.72%] [G loss: 2.314737]\n",
      "epoch:34 step:26719 [D loss: 0.327894, acc.: 85.16%] [G loss: 2.587225]\n",
      "epoch:34 step:26720 [D loss: 0.368104, acc.: 82.81%] [G loss: 2.674762]\n",
      "epoch:34 step:26721 [D loss: 0.341125, acc.: 82.03%] [G loss: 4.173788]\n",
      "epoch:34 step:26722 [D loss: 0.413814, acc.: 82.81%] [G loss: 2.768894]\n",
      "epoch:34 step:26723 [D loss: 0.252986, acc.: 89.06%] [G loss: 3.565934]\n",
      "epoch:34 step:26724 [D loss: 0.289256, acc.: 86.72%] [G loss: 3.681597]\n",
      "epoch:34 step:26725 [D loss: 0.382103, acc.: 83.59%] [G loss: 2.864289]\n",
      "epoch:34 step:26726 [D loss: 0.229122, acc.: 89.06%] [G loss: 2.956817]\n",
      "epoch:34 step:26727 [D loss: 0.265022, acc.: 89.06%] [G loss: 2.945948]\n",
      "epoch:34 step:26728 [D loss: 0.365708, acc.: 84.38%] [G loss: 3.391452]\n",
      "epoch:34 step:26729 [D loss: 0.414683, acc.: 82.81%] [G loss: 2.471086]\n",
      "epoch:34 step:26730 [D loss: 0.209873, acc.: 91.41%] [G loss: 2.851864]\n",
      "epoch:34 step:26731 [D loss: 0.362110, acc.: 82.81%] [G loss: 3.212852]\n",
      "epoch:34 step:26732 [D loss: 0.308671, acc.: 85.94%] [G loss: 2.782658]\n",
      "epoch:34 step:26733 [D loss: 0.232295, acc.: 91.41%] [G loss: 2.981188]\n",
      "epoch:34 step:26734 [D loss: 0.356535, acc.: 85.16%] [G loss: 2.802601]\n",
      "epoch:34 step:26735 [D loss: 0.302036, acc.: 88.28%] [G loss: 2.403998]\n",
      "epoch:34 step:26736 [D loss: 0.324103, acc.: 85.16%] [G loss: 4.944106]\n",
      "epoch:34 step:26737 [D loss: 0.345181, acc.: 85.16%] [G loss: 4.038429]\n",
      "epoch:34 step:26738 [D loss: 0.418362, acc.: 79.69%] [G loss: 2.670712]\n",
      "epoch:34 step:26739 [D loss: 0.291426, acc.: 85.16%] [G loss: 4.075810]\n",
      "epoch:34 step:26740 [D loss: 0.268290, acc.: 87.50%] [G loss: 4.834222]\n",
      "epoch:34 step:26741 [D loss: 0.245932, acc.: 89.84%] [G loss: 3.492543]\n",
      "epoch:34 step:26742 [D loss: 0.311952, acc.: 88.28%] [G loss: 2.903063]\n",
      "epoch:34 step:26743 [D loss: 0.251157, acc.: 92.19%] [G loss: 4.105463]\n",
      "epoch:34 step:26744 [D loss: 0.332567, acc.: 86.72%] [G loss: 5.394417]\n",
      "epoch:34 step:26745 [D loss: 0.455963, acc.: 76.56%] [G loss: 3.596486]\n",
      "epoch:34 step:26746 [D loss: 0.312220, acc.: 85.16%] [G loss: 3.122108]\n",
      "epoch:34 step:26747 [D loss: 0.226156, acc.: 91.41%] [G loss: 4.195583]\n",
      "epoch:34 step:26748 [D loss: 0.303249, acc.: 85.16%] [G loss: 3.914610]\n",
      "epoch:34 step:26749 [D loss: 0.268900, acc.: 91.41%] [G loss: 3.931425]\n",
      "epoch:34 step:26750 [D loss: 0.295812, acc.: 87.50%] [G loss: 5.259992]\n",
      "epoch:34 step:26751 [D loss: 0.442597, acc.: 78.91%] [G loss: 3.455450]\n",
      "epoch:34 step:26752 [D loss: 0.343960, acc.: 88.28%] [G loss: 3.562829]\n",
      "epoch:34 step:26753 [D loss: 0.266679, acc.: 89.84%] [G loss: 3.683447]\n",
      "epoch:34 step:26754 [D loss: 0.354473, acc.: 86.72%] [G loss: 3.158818]\n",
      "epoch:34 step:26755 [D loss: 0.319337, acc.: 85.94%] [G loss: 2.844708]\n",
      "epoch:34 step:26756 [D loss: 0.281694, acc.: 88.28%] [G loss: 2.823747]\n",
      "epoch:34 step:26757 [D loss: 0.261006, acc.: 90.62%] [G loss: 3.049213]\n",
      "epoch:34 step:26758 [D loss: 0.282379, acc.: 88.28%] [G loss: 2.892921]\n",
      "epoch:34 step:26759 [D loss: 0.264729, acc.: 88.28%] [G loss: 3.164721]\n",
      "epoch:34 step:26760 [D loss: 0.292091, acc.: 88.28%] [G loss: 2.662857]\n",
      "epoch:34 step:26761 [D loss: 0.294739, acc.: 87.50%] [G loss: 2.669142]\n",
      "epoch:34 step:26762 [D loss: 0.345123, acc.: 89.06%] [G loss: 2.947831]\n",
      "epoch:34 step:26763 [D loss: 0.235447, acc.: 90.62%] [G loss: 3.537444]\n",
      "epoch:34 step:26764 [D loss: 0.282656, acc.: 87.50%] [G loss: 3.805884]\n",
      "epoch:34 step:26765 [D loss: 0.367665, acc.: 84.38%] [G loss: 2.696134]\n",
      "epoch:34 step:26766 [D loss: 0.236949, acc.: 90.62%] [G loss: 2.769534]\n",
      "epoch:34 step:26767 [D loss: 0.298495, acc.: 85.16%] [G loss: 3.105039]\n",
      "epoch:34 step:26768 [D loss: 0.339227, acc.: 85.16%] [G loss: 2.442535]\n",
      "epoch:34 step:26769 [D loss: 0.195268, acc.: 92.97%] [G loss: 3.409601]\n",
      "epoch:34 step:26770 [D loss: 0.374801, acc.: 80.47%] [G loss: 2.609903]\n",
      "epoch:34 step:26771 [D loss: 0.237558, acc.: 88.28%] [G loss: 3.471417]\n",
      "epoch:34 step:26772 [D loss: 0.311168, acc.: 83.59%] [G loss: 3.523517]\n",
      "epoch:34 step:26773 [D loss: 0.299129, acc.: 85.94%] [G loss: 3.367218]\n",
      "epoch:34 step:26774 [D loss: 0.358411, acc.: 82.81%] [G loss: 3.917624]\n",
      "epoch:34 step:26775 [D loss: 0.331705, acc.: 86.72%] [G loss: 3.350456]\n",
      "epoch:34 step:26776 [D loss: 0.390398, acc.: 80.47%] [G loss: 3.884647]\n",
      "epoch:34 step:26777 [D loss: 0.336031, acc.: 83.59%] [G loss: 3.676826]\n",
      "epoch:34 step:26778 [D loss: 0.346571, acc.: 83.59%] [G loss: 4.279424]\n",
      "epoch:34 step:26779 [D loss: 0.307200, acc.: 89.06%] [G loss: 3.034376]\n",
      "epoch:34 step:26780 [D loss: 0.386244, acc.: 82.03%] [G loss: 3.074632]\n",
      "epoch:34 step:26781 [D loss: 0.222897, acc.: 88.28%] [G loss: 3.560548]\n",
      "epoch:34 step:26782 [D loss: 0.371737, acc.: 78.12%] [G loss: 3.068843]\n",
      "epoch:34 step:26783 [D loss: 0.427902, acc.: 78.12%] [G loss: 3.023309]\n",
      "epoch:34 step:26784 [D loss: 0.356253, acc.: 80.47%] [G loss: 4.278474]\n",
      "epoch:34 step:26785 [D loss: 0.315394, acc.: 88.28%] [G loss: 4.022952]\n",
      "epoch:34 step:26786 [D loss: 0.349994, acc.: 82.81%] [G loss: 4.492215]\n",
      "epoch:34 step:26787 [D loss: 0.277340, acc.: 86.72%] [G loss: 6.129200]\n",
      "epoch:34 step:26788 [D loss: 0.328687, acc.: 85.94%] [G loss: 5.611891]\n",
      "epoch:34 step:26789 [D loss: 0.234821, acc.: 91.41%] [G loss: 4.069146]\n",
      "epoch:34 step:26790 [D loss: 0.251500, acc.: 88.28%] [G loss: 5.342604]\n",
      "epoch:34 step:26791 [D loss: 0.238306, acc.: 88.28%] [G loss: 4.323705]\n",
      "epoch:34 step:26792 [D loss: 0.308544, acc.: 87.50%] [G loss: 4.226267]\n",
      "epoch:34 step:26793 [D loss: 0.310160, acc.: 88.28%] [G loss: 3.073317]\n",
      "epoch:34 step:26794 [D loss: 0.404394, acc.: 78.12%] [G loss: 3.234720]\n",
      "epoch:34 step:26795 [D loss: 0.329027, acc.: 84.38%] [G loss: 3.919311]\n",
      "epoch:34 step:26796 [D loss: 0.334590, acc.: 85.94%] [G loss: 3.405787]\n",
      "epoch:34 step:26797 [D loss: 0.294083, acc.: 89.84%] [G loss: 3.671319]\n",
      "epoch:34 step:26798 [D loss: 0.295397, acc.: 87.50%] [G loss: 3.613214]\n",
      "epoch:34 step:26799 [D loss: 0.306218, acc.: 87.50%] [G loss: 3.234317]\n",
      "epoch:34 step:26800 [D loss: 0.315243, acc.: 83.59%] [G loss: 4.574556]\n",
      "epoch:34 step:26801 [D loss: 0.311721, acc.: 86.72%] [G loss: 3.512475]\n",
      "epoch:34 step:26802 [D loss: 0.201410, acc.: 92.97%] [G loss: 4.942987]\n",
      "epoch:34 step:26803 [D loss: 0.328876, acc.: 85.94%] [G loss: 5.515665]\n",
      "epoch:34 step:26804 [D loss: 0.212908, acc.: 90.62%] [G loss: 4.201753]\n",
      "epoch:34 step:26805 [D loss: 0.272353, acc.: 85.16%] [G loss: 4.388976]\n",
      "epoch:34 step:26806 [D loss: 0.355247, acc.: 82.81%] [G loss: 4.817826]\n",
      "epoch:34 step:26807 [D loss: 0.344403, acc.: 85.16%] [G loss: 2.786970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26808 [D loss: 0.226258, acc.: 92.19%] [G loss: 4.292345]\n",
      "epoch:34 step:26809 [D loss: 0.313983, acc.: 86.72%] [G loss: 3.126141]\n",
      "epoch:34 step:26810 [D loss: 0.334945, acc.: 84.38%] [G loss: 3.010585]\n",
      "epoch:34 step:26811 [D loss: 0.313890, acc.: 84.38%] [G loss: 3.658020]\n",
      "epoch:34 step:26812 [D loss: 0.235329, acc.: 92.97%] [G loss: 3.864212]\n",
      "epoch:34 step:26813 [D loss: 0.312177, acc.: 88.28%] [G loss: 4.313066]\n",
      "epoch:34 step:26814 [D loss: 0.286969, acc.: 87.50%] [G loss: 4.047093]\n",
      "epoch:34 step:26815 [D loss: 0.430438, acc.: 82.03%] [G loss: 5.476130]\n",
      "epoch:34 step:26816 [D loss: 0.663660, acc.: 70.31%] [G loss: 7.758643]\n",
      "epoch:34 step:26817 [D loss: 0.709989, acc.: 75.78%] [G loss: 6.485360]\n",
      "epoch:34 step:26818 [D loss: 0.526097, acc.: 78.12%] [G loss: 3.187972]\n",
      "epoch:34 step:26819 [D loss: 0.345713, acc.: 82.81%] [G loss: 3.056365]\n",
      "epoch:34 step:26820 [D loss: 0.310957, acc.: 84.38%] [G loss: 2.733027]\n",
      "epoch:34 step:26821 [D loss: 0.374711, acc.: 79.69%] [G loss: 3.550949]\n",
      "epoch:34 step:26822 [D loss: 0.311255, acc.: 85.94%] [G loss: 3.219842]\n",
      "epoch:34 step:26823 [D loss: 0.327715, acc.: 84.38%] [G loss: 4.018677]\n",
      "epoch:34 step:26824 [D loss: 0.285277, acc.: 89.84%] [G loss: 4.515497]\n",
      "epoch:34 step:26825 [D loss: 0.184004, acc.: 94.53%] [G loss: 3.955922]\n",
      "epoch:34 step:26826 [D loss: 0.227365, acc.: 88.28%] [G loss: 3.716956]\n",
      "epoch:34 step:26827 [D loss: 0.218818, acc.: 91.41%] [G loss: 3.221363]\n",
      "epoch:34 step:26828 [D loss: 0.301851, acc.: 90.62%] [G loss: 2.894552]\n",
      "epoch:34 step:26829 [D loss: 0.339513, acc.: 82.81%] [G loss: 3.466746]\n",
      "epoch:34 step:26830 [D loss: 0.400907, acc.: 81.25%] [G loss: 3.969645]\n",
      "epoch:34 step:26831 [D loss: 0.339594, acc.: 82.81%] [G loss: 4.461145]\n",
      "epoch:34 step:26832 [D loss: 0.357364, acc.: 85.94%] [G loss: 2.809232]\n",
      "epoch:34 step:26833 [D loss: 0.333177, acc.: 86.72%] [G loss: 3.035402]\n",
      "epoch:34 step:26834 [D loss: 0.263985, acc.: 89.06%] [G loss: 3.429097]\n",
      "epoch:34 step:26835 [D loss: 0.303693, acc.: 85.94%] [G loss: 3.712068]\n",
      "epoch:34 step:26836 [D loss: 0.345594, acc.: 84.38%] [G loss: 3.385361]\n",
      "epoch:34 step:26837 [D loss: 0.325316, acc.: 83.59%] [G loss: 2.400937]\n",
      "epoch:34 step:26838 [D loss: 0.322772, acc.: 85.94%] [G loss: 2.844844]\n",
      "epoch:34 step:26839 [D loss: 0.283894, acc.: 87.50%] [G loss: 2.381999]\n",
      "epoch:34 step:26840 [D loss: 0.296098, acc.: 88.28%] [G loss: 2.580861]\n",
      "epoch:34 step:26841 [D loss: 0.221012, acc.: 92.19%] [G loss: 2.827038]\n",
      "epoch:34 step:26842 [D loss: 0.352864, acc.: 84.38%] [G loss: 2.352998]\n",
      "epoch:34 step:26843 [D loss: 0.298281, acc.: 87.50%] [G loss: 2.391286]\n",
      "epoch:34 step:26844 [D loss: 0.336134, acc.: 82.03%] [G loss: 2.289617]\n",
      "epoch:34 step:26845 [D loss: 0.339595, acc.: 83.59%] [G loss: 2.818995]\n",
      "epoch:34 step:26846 [D loss: 0.336193, acc.: 85.94%] [G loss: 2.853027]\n",
      "epoch:34 step:26847 [D loss: 0.327655, acc.: 86.72%] [G loss: 2.982324]\n",
      "epoch:34 step:26848 [D loss: 0.362468, acc.: 84.38%] [G loss: 3.292203]\n",
      "epoch:34 step:26849 [D loss: 0.312735, acc.: 84.38%] [G loss: 4.391172]\n",
      "epoch:34 step:26850 [D loss: 0.363715, acc.: 82.03%] [G loss: 3.158298]\n",
      "epoch:34 step:26851 [D loss: 0.262786, acc.: 89.84%] [G loss: 2.800905]\n",
      "epoch:34 step:26852 [D loss: 0.223365, acc.: 92.19%] [G loss: 3.142447]\n",
      "epoch:34 step:26853 [D loss: 0.291298, acc.: 88.28%] [G loss: 2.621516]\n",
      "epoch:34 step:26854 [D loss: 0.328137, acc.: 82.81%] [G loss: 2.692290]\n",
      "epoch:34 step:26855 [D loss: 0.345094, acc.: 84.38%] [G loss: 2.188595]\n",
      "epoch:34 step:26856 [D loss: 0.407373, acc.: 81.25%] [G loss: 2.568441]\n",
      "epoch:34 step:26857 [D loss: 0.330028, acc.: 84.38%] [G loss: 3.381644]\n",
      "epoch:34 step:26858 [D loss: 0.286503, acc.: 86.72%] [G loss: 3.811526]\n",
      "epoch:34 step:26859 [D loss: 0.467981, acc.: 75.00%] [G loss: 3.136346]\n",
      "epoch:34 step:26860 [D loss: 0.403009, acc.: 82.81%] [G loss: 3.302929]\n",
      "epoch:34 step:26861 [D loss: 0.324819, acc.: 84.38%] [G loss: 3.032498]\n",
      "epoch:34 step:26862 [D loss: 0.431735, acc.: 77.34%] [G loss: 3.247952]\n",
      "epoch:34 step:26863 [D loss: 0.403431, acc.: 82.81%] [G loss: 3.142395]\n",
      "epoch:34 step:26864 [D loss: 0.306222, acc.: 85.94%] [G loss: 3.946929]\n",
      "epoch:34 step:26865 [D loss: 0.390192, acc.: 82.03%] [G loss: 6.191051]\n",
      "epoch:34 step:26866 [D loss: 0.399547, acc.: 80.47%] [G loss: 6.873367]\n",
      "epoch:34 step:26867 [D loss: 0.410715, acc.: 78.91%] [G loss: 4.596331]\n",
      "epoch:34 step:26868 [D loss: 0.327104, acc.: 85.94%] [G loss: 5.061702]\n",
      "epoch:34 step:26869 [D loss: 0.262389, acc.: 86.72%] [G loss: 5.768730]\n",
      "epoch:34 step:26870 [D loss: 0.315488, acc.: 85.16%] [G loss: 5.250011]\n",
      "epoch:34 step:26871 [D loss: 0.307498, acc.: 87.50%] [G loss: 3.765919]\n",
      "epoch:34 step:26872 [D loss: 0.410453, acc.: 79.69%] [G loss: 4.669615]\n",
      "epoch:34 step:26873 [D loss: 0.268970, acc.: 87.50%] [G loss: 3.440399]\n",
      "epoch:34 step:26874 [D loss: 0.361705, acc.: 82.03%] [G loss: 4.909627]\n",
      "epoch:34 step:26875 [D loss: 0.294503, acc.: 86.72%] [G loss: 4.568896]\n",
      "epoch:34 step:26876 [D loss: 0.357464, acc.: 81.25%] [G loss: 4.412153]\n",
      "epoch:34 step:26877 [D loss: 0.383905, acc.: 82.03%] [G loss: 3.762228]\n",
      "epoch:34 step:26878 [D loss: 0.387079, acc.: 81.25%] [G loss: 4.963461]\n",
      "epoch:34 step:26879 [D loss: 0.316998, acc.: 85.16%] [G loss: 4.254856]\n",
      "epoch:34 step:26880 [D loss: 0.392516, acc.: 78.91%] [G loss: 4.398394]\n",
      "epoch:34 step:26881 [D loss: 0.310751, acc.: 87.50%] [G loss: 4.918200]\n",
      "epoch:34 step:26882 [D loss: 0.290676, acc.: 82.03%] [G loss: 5.108840]\n",
      "epoch:34 step:26883 [D loss: 0.362380, acc.: 82.03%] [G loss: 2.136804]\n",
      "epoch:34 step:26884 [D loss: 0.245367, acc.: 91.41%] [G loss: 3.045271]\n",
      "epoch:34 step:26885 [D loss: 0.351951, acc.: 86.72%] [G loss: 3.775188]\n",
      "epoch:34 step:26886 [D loss: 0.325063, acc.: 86.72%] [G loss: 3.234774]\n",
      "epoch:34 step:26887 [D loss: 0.267308, acc.: 89.84%] [G loss: 3.536136]\n",
      "epoch:34 step:26888 [D loss: 0.355099, acc.: 83.59%] [G loss: 2.959589]\n",
      "epoch:34 step:26889 [D loss: 0.243241, acc.: 89.06%] [G loss: 3.748367]\n",
      "epoch:34 step:26890 [D loss: 0.330333, acc.: 87.50%] [G loss: 3.798843]\n",
      "epoch:34 step:26891 [D loss: 0.328232, acc.: 84.38%] [G loss: 2.712887]\n",
      "epoch:34 step:26892 [D loss: 0.295356, acc.: 86.72%] [G loss: 4.280247]\n",
      "epoch:34 step:26893 [D loss: 0.311409, acc.: 85.16%] [G loss: 3.351932]\n",
      "epoch:34 step:26894 [D loss: 0.351615, acc.: 87.50%] [G loss: 2.752787]\n",
      "epoch:34 step:26895 [D loss: 0.269636, acc.: 87.50%] [G loss: 2.692733]\n",
      "epoch:34 step:26896 [D loss: 0.334514, acc.: 85.16%] [G loss: 2.240038]\n",
      "epoch:34 step:26897 [D loss: 0.329149, acc.: 85.16%] [G loss: 2.969886]\n",
      "epoch:34 step:26898 [D loss: 0.261167, acc.: 90.62%] [G loss: 2.517226]\n",
      "epoch:34 step:26899 [D loss: 0.352297, acc.: 84.38%] [G loss: 2.668215]\n",
      "epoch:34 step:26900 [D loss: 0.393340, acc.: 84.38%] [G loss: 2.842867]\n",
      "epoch:34 step:26901 [D loss: 0.355170, acc.: 82.03%] [G loss: 2.707371]\n",
      "epoch:34 step:26902 [D loss: 0.312664, acc.: 85.94%] [G loss: 3.437216]\n",
      "epoch:34 step:26903 [D loss: 0.236491, acc.: 90.62%] [G loss: 2.769534]\n",
      "epoch:34 step:26904 [D loss: 0.469821, acc.: 76.56%] [G loss: 5.203723]\n",
      "epoch:34 step:26905 [D loss: 0.444379, acc.: 78.91%] [G loss: 4.494811]\n",
      "epoch:34 step:26906 [D loss: 0.317092, acc.: 83.59%] [G loss: 3.492508]\n",
      "epoch:34 step:26907 [D loss: 0.302456, acc.: 82.81%] [G loss: 3.961758]\n",
      "epoch:34 step:26908 [D loss: 0.373806, acc.: 82.81%] [G loss: 5.170190]\n",
      "epoch:34 step:26909 [D loss: 0.305111, acc.: 86.72%] [G loss: 2.921354]\n",
      "epoch:34 step:26910 [D loss: 0.283618, acc.: 88.28%] [G loss: 3.858710]\n",
      "epoch:34 step:26911 [D loss: 0.286291, acc.: 86.72%] [G loss: 3.113889]\n",
      "epoch:34 step:26912 [D loss: 0.272669, acc.: 90.62%] [G loss: 3.134204]\n",
      "epoch:34 step:26913 [D loss: 0.299942, acc.: 86.72%] [G loss: 2.561158]\n",
      "epoch:34 step:26914 [D loss: 0.294796, acc.: 82.81%] [G loss: 3.019257]\n",
      "epoch:34 step:26915 [D loss: 0.279167, acc.: 89.84%] [G loss: 3.781045]\n",
      "epoch:34 step:26916 [D loss: 0.186080, acc.: 92.97%] [G loss: 3.610803]\n",
      "epoch:34 step:26917 [D loss: 0.272929, acc.: 88.28%] [G loss: 3.570363]\n",
      "epoch:34 step:26918 [D loss: 0.313682, acc.: 89.84%] [G loss: 3.402763]\n",
      "epoch:34 step:26919 [D loss: 0.267331, acc.: 87.50%] [G loss: 3.042388]\n",
      "epoch:34 step:26920 [D loss: 0.307635, acc.: 88.28%] [G loss: 4.063242]\n",
      "epoch:34 step:26921 [D loss: 0.321092, acc.: 83.59%] [G loss: 3.048336]\n",
      "epoch:34 step:26922 [D loss: 0.385926, acc.: 81.25%] [G loss: 2.795285]\n",
      "epoch:34 step:26923 [D loss: 0.307264, acc.: 88.28%] [G loss: 3.505769]\n",
      "epoch:34 step:26924 [D loss: 0.267312, acc.: 89.06%] [G loss: 3.113963]\n",
      "epoch:34 step:26925 [D loss: 0.324932, acc.: 88.28%] [G loss: 3.809808]\n",
      "epoch:34 step:26926 [D loss: 0.311869, acc.: 87.50%] [G loss: 3.926366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26927 [D loss: 0.448500, acc.: 80.47%] [G loss: 3.305686]\n",
      "epoch:34 step:26928 [D loss: 0.345675, acc.: 87.50%] [G loss: 2.991454]\n",
      "epoch:34 step:26929 [D loss: 0.336965, acc.: 83.59%] [G loss: 2.653628]\n",
      "epoch:34 step:26930 [D loss: 0.342428, acc.: 84.38%] [G loss: 3.082631]\n",
      "epoch:34 step:26931 [D loss: 0.283960, acc.: 85.94%] [G loss: 2.762806]\n",
      "epoch:34 step:26932 [D loss: 0.356455, acc.: 85.94%] [G loss: 2.327588]\n",
      "epoch:34 step:26933 [D loss: 0.339256, acc.: 84.38%] [G loss: 2.556299]\n",
      "epoch:34 step:26934 [D loss: 0.342240, acc.: 83.59%] [G loss: 2.463149]\n",
      "epoch:34 step:26935 [D loss: 0.340170, acc.: 85.16%] [G loss: 3.393833]\n",
      "epoch:34 step:26936 [D loss: 0.322355, acc.: 87.50%] [G loss: 3.560899]\n",
      "epoch:34 step:26937 [D loss: 0.280491, acc.: 89.06%] [G loss: 2.895511]\n",
      "epoch:34 step:26938 [D loss: 0.237750, acc.: 90.62%] [G loss: 2.916871]\n",
      "epoch:34 step:26939 [D loss: 0.319145, acc.: 82.03%] [G loss: 3.863886]\n",
      "epoch:34 step:26940 [D loss: 0.285275, acc.: 88.28%] [G loss: 4.167293]\n",
      "epoch:34 step:26941 [D loss: 0.326995, acc.: 85.94%] [G loss: 4.444976]\n",
      "epoch:34 step:26942 [D loss: 0.414762, acc.: 82.03%] [G loss: 3.945962]\n",
      "epoch:34 step:26943 [D loss: 0.296601, acc.: 88.28%] [G loss: 3.821978]\n",
      "epoch:34 step:26944 [D loss: 0.223852, acc.: 90.62%] [G loss: 4.405209]\n",
      "epoch:34 step:26945 [D loss: 0.380944, acc.: 85.16%] [G loss: 3.152110]\n",
      "epoch:34 step:26946 [D loss: 0.225455, acc.: 90.62%] [G loss: 3.748122]\n",
      "epoch:34 step:26947 [D loss: 0.244348, acc.: 92.19%] [G loss: 3.862211]\n",
      "epoch:34 step:26948 [D loss: 0.298858, acc.: 85.94%] [G loss: 3.160252]\n",
      "epoch:34 step:26949 [D loss: 0.395541, acc.: 83.59%] [G loss: 4.154757]\n",
      "epoch:34 step:26950 [D loss: 0.324950, acc.: 85.16%] [G loss: 4.713588]\n",
      "epoch:34 step:26951 [D loss: 0.344189, acc.: 85.16%] [G loss: 3.423906]\n",
      "epoch:34 step:26952 [D loss: 0.311448, acc.: 89.84%] [G loss: 3.994125]\n",
      "epoch:34 step:26953 [D loss: 0.400950, acc.: 82.81%] [G loss: 3.901819]\n",
      "epoch:34 step:26954 [D loss: 0.231255, acc.: 92.19%] [G loss: 3.815166]\n",
      "epoch:34 step:26955 [D loss: 0.418437, acc.: 77.34%] [G loss: 2.405351]\n",
      "epoch:34 step:26956 [D loss: 0.254344, acc.: 89.84%] [G loss: 5.191048]\n",
      "epoch:34 step:26957 [D loss: 0.314730, acc.: 86.72%] [G loss: 3.172245]\n",
      "epoch:34 step:26958 [D loss: 0.281036, acc.: 85.94%] [G loss: 2.810635]\n",
      "epoch:34 step:26959 [D loss: 0.340661, acc.: 84.38%] [G loss: 2.980222]\n",
      "epoch:34 step:26960 [D loss: 0.392074, acc.: 80.47%] [G loss: 3.611152]\n",
      "epoch:34 step:26961 [D loss: 0.270797, acc.: 89.06%] [G loss: 3.502984]\n",
      "epoch:34 step:26962 [D loss: 0.286617, acc.: 88.28%] [G loss: 3.624664]\n",
      "epoch:34 step:26963 [D loss: 0.373521, acc.: 85.16%] [G loss: 2.750357]\n",
      "epoch:34 step:26964 [D loss: 0.363331, acc.: 82.81%] [G loss: 2.527213]\n",
      "epoch:34 step:26965 [D loss: 0.356310, acc.: 84.38%] [G loss: 3.305374]\n",
      "epoch:34 step:26966 [D loss: 0.287911, acc.: 87.50%] [G loss: 4.509350]\n",
      "epoch:34 step:26967 [D loss: 0.292852, acc.: 84.38%] [G loss: 3.103108]\n",
      "epoch:34 step:26968 [D loss: 0.255774, acc.: 87.50%] [G loss: 2.648612]\n",
      "epoch:34 step:26969 [D loss: 0.261483, acc.: 87.50%] [G loss: 3.663603]\n",
      "epoch:34 step:26970 [D loss: 0.331977, acc.: 87.50%] [G loss: 4.347650]\n",
      "epoch:34 step:26971 [D loss: 0.293967, acc.: 85.16%] [G loss: 5.987896]\n",
      "epoch:34 step:26972 [D loss: 0.248262, acc.: 91.41%] [G loss: 5.954144]\n",
      "epoch:34 step:26973 [D loss: 0.315593, acc.: 84.38%] [G loss: 4.509723]\n",
      "epoch:34 step:26974 [D loss: 0.203015, acc.: 90.62%] [G loss: 3.823861]\n",
      "epoch:34 step:26975 [D loss: 0.318199, acc.: 85.16%] [G loss: 2.984073]\n",
      "epoch:34 step:26976 [D loss: 0.299290, acc.: 82.81%] [G loss: 3.452617]\n",
      "epoch:34 step:26977 [D loss: 0.333819, acc.: 85.16%] [G loss: 2.684899]\n",
      "epoch:34 step:26978 [D loss: 0.338942, acc.: 85.94%] [G loss: 3.547359]\n",
      "epoch:34 step:26979 [D loss: 0.344989, acc.: 80.47%] [G loss: 3.883574]\n",
      "epoch:34 step:26980 [D loss: 0.300553, acc.: 85.94%] [G loss: 2.278430]\n",
      "epoch:34 step:26981 [D loss: 0.328472, acc.: 84.38%] [G loss: 3.082975]\n",
      "epoch:34 step:26982 [D loss: 0.265472, acc.: 89.84%] [G loss: 3.132660]\n",
      "epoch:34 step:26983 [D loss: 0.311148, acc.: 83.59%] [G loss: 2.746329]\n",
      "epoch:34 step:26984 [D loss: 0.267234, acc.: 88.28%] [G loss: 3.232891]\n",
      "epoch:34 step:26985 [D loss: 0.340937, acc.: 85.94%] [G loss: 3.063821]\n",
      "epoch:34 step:26986 [D loss: 0.422502, acc.: 79.69%] [G loss: 2.853775]\n",
      "epoch:34 step:26987 [D loss: 0.297576, acc.: 85.16%] [G loss: 3.428394]\n",
      "epoch:34 step:26988 [D loss: 0.298429, acc.: 86.72%] [G loss: 5.035701]\n",
      "epoch:34 step:26989 [D loss: 0.325983, acc.: 82.81%] [G loss: 5.546488]\n",
      "epoch:34 step:26990 [D loss: 0.268120, acc.: 85.16%] [G loss: 4.754746]\n",
      "epoch:34 step:26991 [D loss: 0.243678, acc.: 88.28%] [G loss: 3.693581]\n",
      "epoch:34 step:26992 [D loss: 0.382277, acc.: 80.47%] [G loss: 3.109310]\n",
      "epoch:34 step:26993 [D loss: 0.373527, acc.: 80.47%] [G loss: 2.734895]\n",
      "epoch:34 step:26994 [D loss: 0.315976, acc.: 86.72%] [G loss: 3.385488]\n",
      "epoch:34 step:26995 [D loss: 0.347101, acc.: 85.16%] [G loss: 3.867189]\n",
      "epoch:34 step:26996 [D loss: 0.278421, acc.: 85.94%] [G loss: 2.573036]\n",
      "epoch:34 step:26997 [D loss: 0.265647, acc.: 89.06%] [G loss: 3.086229]\n",
      "epoch:34 step:26998 [D loss: 0.312643, acc.: 86.72%] [G loss: 3.068897]\n",
      "epoch:34 step:26999 [D loss: 0.317909, acc.: 84.38%] [G loss: 2.694082]\n",
      "epoch:34 step:27000 [D loss: 0.272833, acc.: 89.06%] [G loss: 3.287617]\n",
      "epoch:34 step:27001 [D loss: 0.279474, acc.: 86.72%] [G loss: 3.207314]\n",
      "epoch:34 step:27002 [D loss: 0.306367, acc.: 88.28%] [G loss: 3.107841]\n",
      "epoch:34 step:27003 [D loss: 0.301444, acc.: 87.50%] [G loss: 3.149576]\n",
      "epoch:34 step:27004 [D loss: 0.240503, acc.: 89.84%] [G loss: 3.019980]\n",
      "epoch:34 step:27005 [D loss: 0.288096, acc.: 85.16%] [G loss: 2.933419]\n",
      "epoch:34 step:27006 [D loss: 0.282749, acc.: 86.72%] [G loss: 2.992413]\n",
      "epoch:34 step:27007 [D loss: 0.298419, acc.: 87.50%] [G loss: 3.116302]\n",
      "epoch:34 step:27008 [D loss: 0.287394, acc.: 87.50%] [G loss: 3.585125]\n",
      "epoch:34 step:27009 [D loss: 0.247799, acc.: 90.62%] [G loss: 3.432565]\n",
      "epoch:34 step:27010 [D loss: 0.349250, acc.: 84.38%] [G loss: 3.962404]\n",
      "epoch:34 step:27011 [D loss: 0.312656, acc.: 84.38%] [G loss: 4.454293]\n",
      "epoch:34 step:27012 [D loss: 0.306111, acc.: 82.81%] [G loss: 3.447204]\n",
      "epoch:34 step:27013 [D loss: 0.259607, acc.: 88.28%] [G loss: 3.938595]\n",
      "epoch:34 step:27014 [D loss: 0.337959, acc.: 82.81%] [G loss: 3.208278]\n",
      "epoch:34 step:27015 [D loss: 0.317726, acc.: 87.50%] [G loss: 3.389310]\n",
      "epoch:34 step:27016 [D loss: 0.292222, acc.: 88.28%] [G loss: 2.946684]\n",
      "epoch:34 step:27017 [D loss: 0.310255, acc.: 87.50%] [G loss: 2.932894]\n",
      "epoch:34 step:27018 [D loss: 0.178427, acc.: 94.53%] [G loss: 2.939685]\n",
      "epoch:34 step:27019 [D loss: 0.317115, acc.: 86.72%] [G loss: 3.227884]\n",
      "epoch:34 step:27020 [D loss: 0.328025, acc.: 85.16%] [G loss: 2.769350]\n",
      "epoch:34 step:27021 [D loss: 0.362871, acc.: 87.50%] [G loss: 2.752631]\n",
      "epoch:34 step:27022 [D loss: 0.374378, acc.: 82.03%] [G loss: 3.290632]\n",
      "epoch:34 step:27023 [D loss: 0.267826, acc.: 86.72%] [G loss: 3.369503]\n",
      "epoch:34 step:27024 [D loss: 0.307332, acc.: 84.38%] [G loss: 3.042678]\n",
      "epoch:34 step:27025 [D loss: 0.330612, acc.: 83.59%] [G loss: 2.712697]\n",
      "epoch:34 step:27026 [D loss: 0.373446, acc.: 81.25%] [G loss: 3.137908]\n",
      "epoch:34 step:27027 [D loss: 0.300449, acc.: 88.28%] [G loss: 3.273338]\n",
      "epoch:34 step:27028 [D loss: 0.465628, acc.: 77.34%] [G loss: 4.929861]\n",
      "epoch:34 step:27029 [D loss: 0.659592, acc.: 74.22%] [G loss: 4.612431]\n",
      "epoch:34 step:27030 [D loss: 0.836178, acc.: 68.75%] [G loss: 4.079823]\n",
      "epoch:34 step:27031 [D loss: 1.152647, acc.: 71.09%] [G loss: 9.212752]\n",
      "epoch:34 step:27032 [D loss: 1.315041, acc.: 64.06%] [G loss: 3.311526]\n",
      "epoch:34 step:27033 [D loss: 0.464661, acc.: 81.25%] [G loss: 2.962039]\n",
      "epoch:34 step:27034 [D loss: 0.438651, acc.: 79.69%] [G loss: 3.632987]\n",
      "epoch:34 step:27035 [D loss: 0.519754, acc.: 79.69%] [G loss: 4.398526]\n",
      "epoch:34 step:27036 [D loss: 0.318607, acc.: 82.81%] [G loss: 3.185285]\n",
      "epoch:34 step:27037 [D loss: 0.323060, acc.: 86.72%] [G loss: 3.677391]\n",
      "epoch:34 step:27038 [D loss: 0.336558, acc.: 83.59%] [G loss: 3.074711]\n",
      "epoch:34 step:27039 [D loss: 0.284193, acc.: 88.28%] [G loss: 3.736012]\n",
      "epoch:34 step:27040 [D loss: 0.320140, acc.: 88.28%] [G loss: 2.554176]\n",
      "epoch:34 step:27041 [D loss: 0.312382, acc.: 86.72%] [G loss: 3.478312]\n",
      "epoch:34 step:27042 [D loss: 0.263825, acc.: 86.72%] [G loss: 2.993468]\n",
      "epoch:34 step:27043 [D loss: 0.262384, acc.: 90.62%] [G loss: 2.827462]\n",
      "epoch:34 step:27044 [D loss: 0.256988, acc.: 89.06%] [G loss: 2.716516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:27045 [D loss: 0.366901, acc.: 84.38%] [G loss: 2.431269]\n",
      "epoch:34 step:27046 [D loss: 0.350706, acc.: 79.69%] [G loss: 2.740095]\n",
      "epoch:34 step:27047 [D loss: 0.341975, acc.: 85.16%] [G loss: 2.373754]\n",
      "epoch:34 step:27048 [D loss: 0.468678, acc.: 78.12%] [G loss: 2.520895]\n",
      "epoch:34 step:27049 [D loss: 0.293153, acc.: 88.28%] [G loss: 2.609382]\n",
      "epoch:34 step:27050 [D loss: 0.302835, acc.: 89.84%] [G loss: 1.856237]\n",
      "epoch:34 step:27051 [D loss: 0.403769, acc.: 82.03%] [G loss: 2.954391]\n",
      "epoch:34 step:27052 [D loss: 0.403053, acc.: 82.03%] [G loss: 2.712173]\n",
      "epoch:34 step:27053 [D loss: 0.346312, acc.: 83.59%] [G loss: 2.800003]\n",
      "epoch:34 step:27054 [D loss: 0.394700, acc.: 80.47%] [G loss: 3.175488]\n",
      "epoch:34 step:27055 [D loss: 0.394258, acc.: 82.81%] [G loss: 2.988553]\n",
      "epoch:34 step:27056 [D loss: 0.307941, acc.: 86.72%] [G loss: 2.131161]\n",
      "epoch:34 step:27057 [D loss: 0.274495, acc.: 89.06%] [G loss: 3.121464]\n",
      "epoch:34 step:27058 [D loss: 0.315465, acc.: 84.38%] [G loss: 2.897783]\n",
      "epoch:34 step:27059 [D loss: 0.327514, acc.: 84.38%] [G loss: 3.563303]\n",
      "epoch:34 step:27060 [D loss: 0.306848, acc.: 86.72%] [G loss: 2.395183]\n",
      "epoch:34 step:27061 [D loss: 0.311773, acc.: 84.38%] [G loss: 3.329938]\n",
      "epoch:34 step:27062 [D loss: 0.356431, acc.: 83.59%] [G loss: 3.069416]\n",
      "epoch:34 step:27063 [D loss: 0.333139, acc.: 82.81%] [G loss: 3.157112]\n",
      "epoch:34 step:27064 [D loss: 0.276790, acc.: 89.06%] [G loss: 3.418907]\n",
      "epoch:34 step:27065 [D loss: 0.325149, acc.: 85.16%] [G loss: 3.453917]\n",
      "epoch:34 step:27066 [D loss: 0.358229, acc.: 82.81%] [G loss: 3.111254]\n",
      "epoch:34 step:27067 [D loss: 0.378575, acc.: 79.69%] [G loss: 3.850577]\n",
      "epoch:34 step:27068 [D loss: 0.458689, acc.: 78.91%] [G loss: 4.248668]\n",
      "epoch:34 step:27069 [D loss: 0.331868, acc.: 85.94%] [G loss: 4.406156]\n",
      "epoch:34 step:27070 [D loss: 0.228085, acc.: 90.62%] [G loss: 4.954564]\n",
      "epoch:34 step:27071 [D loss: 0.289927, acc.: 86.72%] [G loss: 4.702691]\n",
      "epoch:34 step:27072 [D loss: 0.208490, acc.: 89.84%] [G loss: 5.111079]\n",
      "epoch:34 step:27073 [D loss: 0.201369, acc.: 93.75%] [G loss: 3.272080]\n",
      "epoch:34 step:27074 [D loss: 0.253808, acc.: 90.62%] [G loss: 3.449030]\n",
      "epoch:34 step:27075 [D loss: 0.356481, acc.: 80.47%] [G loss: 2.655949]\n",
      "epoch:34 step:27076 [D loss: 0.357242, acc.: 84.38%] [G loss: 3.009246]\n",
      "epoch:34 step:27077 [D loss: 0.346391, acc.: 83.59%] [G loss: 2.570580]\n",
      "epoch:34 step:27078 [D loss: 0.381961, acc.: 82.81%] [G loss: 2.931296]\n",
      "epoch:34 step:27079 [D loss: 0.336334, acc.: 84.38%] [G loss: 2.764030]\n",
      "epoch:34 step:27080 [D loss: 0.482510, acc.: 74.22%] [G loss: 3.180655]\n",
      "epoch:34 step:27081 [D loss: 0.273819, acc.: 90.62%] [G loss: 2.954957]\n",
      "epoch:34 step:27082 [D loss: 0.285486, acc.: 87.50%] [G loss: 3.604076]\n",
      "epoch:34 step:27083 [D loss: 0.350984, acc.: 81.25%] [G loss: 3.844221]\n",
      "epoch:34 step:27084 [D loss: 0.391457, acc.: 80.47%] [G loss: 2.990165]\n",
      "epoch:34 step:27085 [D loss: 0.282147, acc.: 87.50%] [G loss: 3.436226]\n",
      "epoch:34 step:27086 [D loss: 0.301215, acc.: 88.28%] [G loss: 3.652107]\n",
      "epoch:34 step:27087 [D loss: 0.280461, acc.: 89.06%] [G loss: 2.695101]\n",
      "epoch:34 step:27088 [D loss: 0.375831, acc.: 81.25%] [G loss: 2.993900]\n",
      "epoch:34 step:27089 [D loss: 0.370171, acc.: 85.16%] [G loss: 3.251814]\n",
      "epoch:34 step:27090 [D loss: 0.450792, acc.: 78.91%] [G loss: 3.710990]\n",
      "epoch:34 step:27091 [D loss: 0.532508, acc.: 71.88%] [G loss: 3.595313]\n",
      "epoch:34 step:27092 [D loss: 0.498927, acc.: 77.34%] [G loss: 6.180001]\n",
      "epoch:34 step:27093 [D loss: 0.637467, acc.: 79.69%] [G loss: 4.085544]\n",
      "epoch:34 step:27094 [D loss: 0.413869, acc.: 79.69%] [G loss: 4.745825]\n",
      "epoch:34 step:27095 [D loss: 0.360889, acc.: 82.03%] [G loss: 2.946060]\n",
      "epoch:34 step:27096 [D loss: 0.358763, acc.: 85.16%] [G loss: 2.863535]\n",
      "epoch:34 step:27097 [D loss: 0.426313, acc.: 77.34%] [G loss: 3.467420]\n",
      "epoch:34 step:27098 [D loss: 0.244663, acc.: 89.06%] [G loss: 4.857333]\n",
      "epoch:34 step:27099 [D loss: 0.232416, acc.: 86.72%] [G loss: 6.009627]\n",
      "epoch:34 step:27100 [D loss: 0.246531, acc.: 87.50%] [G loss: 5.978718]\n",
      "epoch:34 step:27101 [D loss: 0.294710, acc.: 84.38%] [G loss: 7.388679]\n",
      "epoch:34 step:27102 [D loss: 0.212695, acc.: 89.84%] [G loss: 4.540406]\n",
      "epoch:34 step:27103 [D loss: 0.372173, acc.: 78.91%] [G loss: 4.633254]\n",
      "epoch:34 step:27104 [D loss: 0.285244, acc.: 86.72%] [G loss: 4.301538]\n",
      "epoch:34 step:27105 [D loss: 0.270453, acc.: 87.50%] [G loss: 2.953964]\n",
      "epoch:34 step:27106 [D loss: 0.295955, acc.: 86.72%] [G loss: 3.198964]\n",
      "epoch:34 step:27107 [D loss: 0.312576, acc.: 86.72%] [G loss: 2.547951]\n",
      "epoch:34 step:27108 [D loss: 0.366944, acc.: 82.81%] [G loss: 2.677093]\n",
      "epoch:34 step:27109 [D loss: 0.253786, acc.: 89.84%] [G loss: 3.321581]\n",
      "epoch:34 step:27110 [D loss: 0.368656, acc.: 82.03%] [G loss: 2.789932]\n",
      "epoch:34 step:27111 [D loss: 0.337289, acc.: 85.16%] [G loss: 3.222451]\n",
      "epoch:34 step:27112 [D loss: 0.367118, acc.: 84.38%] [G loss: 3.404063]\n",
      "epoch:34 step:27113 [D loss: 0.188753, acc.: 92.97%] [G loss: 3.812348]\n",
      "epoch:34 step:27114 [D loss: 0.353393, acc.: 82.81%] [G loss: 2.680532]\n",
      "epoch:34 step:27115 [D loss: 0.315973, acc.: 83.59%] [G loss: 3.484117]\n",
      "epoch:34 step:27116 [D loss: 0.358236, acc.: 82.03%] [G loss: 3.308060]\n",
      "epoch:34 step:27117 [D loss: 0.273906, acc.: 90.62%] [G loss: 2.910378]\n",
      "epoch:34 step:27118 [D loss: 0.407892, acc.: 82.03%] [G loss: 3.101271]\n",
      "epoch:34 step:27119 [D loss: 0.389609, acc.: 80.47%] [G loss: 3.164541]\n",
      "epoch:34 step:27120 [D loss: 0.345909, acc.: 84.38%] [G loss: 3.128496]\n",
      "epoch:34 step:27121 [D loss: 0.325876, acc.: 85.16%] [G loss: 2.918821]\n",
      "epoch:34 step:27122 [D loss: 0.303662, acc.: 85.94%] [G loss: 3.043770]\n",
      "epoch:34 step:27123 [D loss: 0.327250, acc.: 84.38%] [G loss: 3.514917]\n",
      "epoch:34 step:27124 [D loss: 0.322138, acc.: 85.94%] [G loss: 2.781894]\n",
      "epoch:34 step:27125 [D loss: 0.294043, acc.: 87.50%] [G loss: 3.385890]\n",
      "epoch:34 step:27126 [D loss: 0.324887, acc.: 86.72%] [G loss: 3.078965]\n",
      "epoch:34 step:27127 [D loss: 0.293954, acc.: 88.28%] [G loss: 2.723650]\n",
      "epoch:34 step:27128 [D loss: 0.338946, acc.: 82.03%] [G loss: 2.945043]\n",
      "epoch:34 step:27129 [D loss: 0.477437, acc.: 78.12%] [G loss: 2.582225]\n",
      "epoch:34 step:27130 [D loss: 0.301270, acc.: 87.50%] [G loss: 2.445083]\n",
      "epoch:34 step:27131 [D loss: 0.374180, acc.: 84.38%] [G loss: 2.679986]\n",
      "epoch:34 step:27132 [D loss: 0.362194, acc.: 84.38%] [G loss: 2.620887]\n",
      "epoch:34 step:27133 [D loss: 0.303636, acc.: 87.50%] [G loss: 3.145404]\n",
      "epoch:34 step:27134 [D loss: 0.355511, acc.: 81.25%] [G loss: 3.414550]\n",
      "epoch:34 step:27135 [D loss: 0.392183, acc.: 82.81%] [G loss: 2.783477]\n",
      "epoch:34 step:27136 [D loss: 0.274042, acc.: 89.84%] [G loss: 3.858424]\n",
      "epoch:34 step:27137 [D loss: 0.400052, acc.: 82.03%] [G loss: 3.946251]\n",
      "epoch:34 step:27138 [D loss: 0.370293, acc.: 78.91%] [G loss: 2.748813]\n",
      "epoch:34 step:27139 [D loss: 0.232371, acc.: 93.75%] [G loss: 4.941164]\n",
      "epoch:34 step:27140 [D loss: 0.380623, acc.: 81.25%] [G loss: 2.802387]\n",
      "epoch:34 step:27141 [D loss: 0.323868, acc.: 86.72%] [G loss: 3.719391]\n",
      "epoch:34 step:27142 [D loss: 0.299393, acc.: 85.16%] [G loss: 3.834627]\n",
      "epoch:34 step:27143 [D loss: 0.382657, acc.: 85.94%] [G loss: 2.436249]\n",
      "epoch:34 step:27144 [D loss: 0.417751, acc.: 82.03%] [G loss: 3.098452]\n",
      "epoch:34 step:27145 [D loss: 0.322824, acc.: 82.81%] [G loss: 3.107886]\n",
      "epoch:34 step:27146 [D loss: 0.281459, acc.: 86.72%] [G loss: 3.390038]\n",
      "epoch:34 step:27147 [D loss: 0.260825, acc.: 89.84%] [G loss: 3.351199]\n",
      "epoch:34 step:27148 [D loss: 0.322741, acc.: 82.81%] [G loss: 3.345692]\n",
      "epoch:34 step:27149 [D loss: 0.376672, acc.: 81.25%] [G loss: 3.106131]\n",
      "epoch:34 step:27150 [D loss: 0.313927, acc.: 88.28%] [G loss: 2.754891]\n",
      "epoch:34 step:27151 [D loss: 0.406320, acc.: 79.69%] [G loss: 3.195211]\n",
      "epoch:34 step:27152 [D loss: 0.298550, acc.: 88.28%] [G loss: 5.285372]\n",
      "epoch:34 step:27153 [D loss: 0.313100, acc.: 89.06%] [G loss: 4.270577]\n",
      "epoch:34 step:27154 [D loss: 0.268494, acc.: 89.84%] [G loss: 3.263134]\n",
      "epoch:34 step:27155 [D loss: 0.275377, acc.: 88.28%] [G loss: 3.674314]\n",
      "epoch:34 step:27156 [D loss: 0.276522, acc.: 88.28%] [G loss: 3.011012]\n",
      "epoch:34 step:27157 [D loss: 0.250234, acc.: 87.50%] [G loss: 3.685602]\n",
      "epoch:34 step:27158 [D loss: 0.392502, acc.: 77.34%] [G loss: 2.794733]\n",
      "epoch:34 step:27159 [D loss: 0.307203, acc.: 88.28%] [G loss: 2.678341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:27160 [D loss: 0.332292, acc.: 82.81%] [G loss: 3.848671]\n",
      "epoch:34 step:27161 [D loss: 0.567369, acc.: 73.44%] [G loss: 3.221156]\n",
      "epoch:34 step:27162 [D loss: 0.246613, acc.: 89.84%] [G loss: 4.400618]\n",
      "epoch:34 step:27163 [D loss: 0.348214, acc.: 86.72%] [G loss: 3.029356]\n",
      "epoch:34 step:27164 [D loss: 0.431285, acc.: 79.69%] [G loss: 3.295811]\n",
      "epoch:34 step:27165 [D loss: 0.318238, acc.: 86.72%] [G loss: 3.032338]\n",
      "epoch:34 step:27166 [D loss: 0.393765, acc.: 83.59%] [G loss: 2.816319]\n",
      "epoch:34 step:27167 [D loss: 0.435860, acc.: 80.47%] [G loss: 3.008056]\n",
      "epoch:34 step:27168 [D loss: 0.309195, acc.: 83.59%] [G loss: 2.810245]\n",
      "epoch:34 step:27169 [D loss: 0.313342, acc.: 83.59%] [G loss: 3.240364]\n",
      "epoch:34 step:27170 [D loss: 0.271178, acc.: 88.28%] [G loss: 3.711337]\n",
      "epoch:34 step:27171 [D loss: 0.407634, acc.: 79.69%] [G loss: 2.695974]\n",
      "epoch:34 step:27172 [D loss: 0.325658, acc.: 84.38%] [G loss: 2.917353]\n",
      "epoch:34 step:27173 [D loss: 0.405037, acc.: 82.81%] [G loss: 2.929921]\n",
      "epoch:34 step:27174 [D loss: 0.273971, acc.: 89.84%] [G loss: 3.426589]\n",
      "epoch:34 step:27175 [D loss: 0.212043, acc.: 90.62%] [G loss: 3.374959]\n",
      "epoch:34 step:27176 [D loss: 0.334656, acc.: 87.50%] [G loss: 2.775547]\n",
      "epoch:34 step:27177 [D loss: 0.479049, acc.: 78.12%] [G loss: 3.039675]\n",
      "epoch:34 step:27178 [D loss: 0.322250, acc.: 86.72%] [G loss: 3.078521]\n",
      "epoch:34 step:27179 [D loss: 0.435534, acc.: 78.91%] [G loss: 3.631729]\n",
      "epoch:34 step:27180 [D loss: 0.273580, acc.: 87.50%] [G loss: 4.077333]\n",
      "epoch:34 step:27181 [D loss: 0.359281, acc.: 82.03%] [G loss: 3.325578]\n",
      "epoch:34 step:27182 [D loss: 0.414652, acc.: 81.25%] [G loss: 4.263512]\n",
      "epoch:34 step:27183 [D loss: 0.417403, acc.: 77.34%] [G loss: 4.543850]\n",
      "epoch:34 step:27184 [D loss: 0.257897, acc.: 88.28%] [G loss: 3.690674]\n",
      "epoch:34 step:27185 [D loss: 0.265212, acc.: 89.06%] [G loss: 2.947072]\n",
      "epoch:34 step:27186 [D loss: 0.283278, acc.: 87.50%] [G loss: 3.263973]\n",
      "epoch:34 step:27187 [D loss: 0.396911, acc.: 81.25%] [G loss: 4.071615]\n",
      "epoch:34 step:27188 [D loss: 0.385855, acc.: 83.59%] [G loss: 2.648898]\n",
      "epoch:34 step:27189 [D loss: 0.453236, acc.: 79.69%] [G loss: 2.111477]\n",
      "epoch:34 step:27190 [D loss: 0.362182, acc.: 82.03%] [G loss: 3.635280]\n",
      "epoch:34 step:27191 [D loss: 0.339362, acc.: 85.94%] [G loss: 3.482321]\n",
      "epoch:34 step:27192 [D loss: 0.296940, acc.: 89.84%] [G loss: 3.749930]\n",
      "epoch:34 step:27193 [D loss: 0.255318, acc.: 89.06%] [G loss: 2.936009]\n",
      "epoch:34 step:27194 [D loss: 0.322492, acc.: 86.72%] [G loss: 2.816551]\n",
      "epoch:34 step:27195 [D loss: 0.329927, acc.: 85.94%] [G loss: 2.932998]\n",
      "epoch:34 step:27196 [D loss: 0.336111, acc.: 84.38%] [G loss: 2.981317]\n",
      "epoch:34 step:27197 [D loss: 0.367061, acc.: 86.72%] [G loss: 2.884999]\n",
      "epoch:34 step:27198 [D loss: 0.338113, acc.: 82.81%] [G loss: 3.789611]\n",
      "epoch:34 step:27199 [D loss: 0.513950, acc.: 81.25%] [G loss: 2.920920]\n",
      "epoch:34 step:27200 [D loss: 0.403189, acc.: 75.78%] [G loss: 3.480194]\n",
      "epoch:34 step:27201 [D loss: 0.419115, acc.: 75.78%] [G loss: 3.827824]\n",
      "epoch:34 step:27202 [D loss: 0.327111, acc.: 85.94%] [G loss: 3.762488]\n",
      "epoch:34 step:27203 [D loss: 0.327706, acc.: 84.38%] [G loss: 3.842654]\n",
      "epoch:34 step:27204 [D loss: 0.343700, acc.: 83.59%] [G loss: 4.023451]\n",
      "epoch:34 step:27205 [D loss: 0.317588, acc.: 88.28%] [G loss: 3.901984]\n",
      "epoch:34 step:27206 [D loss: 0.330760, acc.: 85.94%] [G loss: 3.266583]\n",
      "epoch:34 step:27207 [D loss: 0.322164, acc.: 84.38%] [G loss: 3.049204]\n",
      "epoch:34 step:27208 [D loss: 0.443272, acc.: 76.56%] [G loss: 4.947611]\n",
      "epoch:34 step:27209 [D loss: 0.557686, acc.: 78.12%] [G loss: 4.923382]\n",
      "epoch:34 step:27210 [D loss: 0.476717, acc.: 81.25%] [G loss: 3.277178]\n",
      "epoch:34 step:27211 [D loss: 0.429844, acc.: 81.25%] [G loss: 2.382910]\n",
      "epoch:34 step:27212 [D loss: 0.234927, acc.: 88.28%] [G loss: 3.880228]\n",
      "epoch:34 step:27213 [D loss: 0.235481, acc.: 88.28%] [G loss: 3.908711]\n",
      "epoch:34 step:27214 [D loss: 0.315296, acc.: 85.94%] [G loss: 3.970308]\n",
      "epoch:34 step:27215 [D loss: 0.511251, acc.: 76.56%] [G loss: 3.188555]\n",
      "epoch:34 step:27216 [D loss: 0.295033, acc.: 86.72%] [G loss: 3.848098]\n",
      "epoch:34 step:27217 [D loss: 0.270256, acc.: 89.84%] [G loss: 3.415257]\n",
      "epoch:34 step:27218 [D loss: 0.349734, acc.: 84.38%] [G loss: 3.425290]\n",
      "epoch:34 step:27219 [D loss: 0.265188, acc.: 92.19%] [G loss: 3.709362]\n",
      "epoch:34 step:27220 [D loss: 0.341534, acc.: 84.38%] [G loss: 4.428314]\n",
      "epoch:34 step:27221 [D loss: 0.334064, acc.: 83.59%] [G loss: 2.916766]\n",
      "epoch:34 step:27222 [D loss: 0.238967, acc.: 89.06%] [G loss: 2.928988]\n",
      "epoch:34 step:27223 [D loss: 0.339032, acc.: 85.94%] [G loss: 2.688962]\n",
      "epoch:34 step:27224 [D loss: 0.203600, acc.: 93.75%] [G loss: 2.977534]\n",
      "epoch:34 step:27225 [D loss: 0.327698, acc.: 84.38%] [G loss: 3.386164]\n",
      "epoch:34 step:27226 [D loss: 0.333202, acc.: 83.59%] [G loss: 3.853737]\n",
      "epoch:34 step:27227 [D loss: 0.303049, acc.: 85.16%] [G loss: 3.077868]\n",
      "epoch:34 step:27228 [D loss: 0.270576, acc.: 90.62%] [G loss: 4.073021]\n",
      "epoch:34 step:27229 [D loss: 0.310591, acc.: 83.59%] [G loss: 3.546350]\n",
      "epoch:34 step:27230 [D loss: 0.274718, acc.: 87.50%] [G loss: 3.574851]\n",
      "epoch:34 step:27231 [D loss: 0.275279, acc.: 89.06%] [G loss: 4.149130]\n",
      "epoch:34 step:27232 [D loss: 0.312245, acc.: 88.28%] [G loss: 3.336172]\n",
      "epoch:34 step:27233 [D loss: 0.293123, acc.: 87.50%] [G loss: 4.402447]\n",
      "epoch:34 step:27234 [D loss: 0.274212, acc.: 88.28%] [G loss: 4.430235]\n",
      "epoch:34 step:27235 [D loss: 0.299206, acc.: 87.50%] [G loss: 4.742618]\n",
      "epoch:34 step:27236 [D loss: 0.396503, acc.: 83.59%] [G loss: 4.169356]\n",
      "epoch:34 step:27237 [D loss: 0.334206, acc.: 85.94%] [G loss: 3.689612]\n",
      "epoch:34 step:27238 [D loss: 0.293339, acc.: 89.06%] [G loss: 3.404408]\n",
      "epoch:34 step:27239 [D loss: 0.308462, acc.: 85.94%] [G loss: 3.574773]\n",
      "epoch:34 step:27240 [D loss: 0.301141, acc.: 85.16%] [G loss: 3.262785]\n",
      "epoch:34 step:27241 [D loss: 0.288157, acc.: 87.50%] [G loss: 3.477580]\n",
      "epoch:34 step:27242 [D loss: 0.301193, acc.: 85.94%] [G loss: 3.048743]\n",
      "epoch:34 step:27243 [D loss: 0.353528, acc.: 82.03%] [G loss: 2.531171]\n",
      "epoch:34 step:27244 [D loss: 0.402941, acc.: 85.94%] [G loss: 2.489780]\n",
      "epoch:34 step:27245 [D loss: 0.210536, acc.: 93.75%] [G loss: 2.880272]\n",
      "epoch:34 step:27246 [D loss: 0.356354, acc.: 82.81%] [G loss: 3.720450]\n",
      "epoch:34 step:27247 [D loss: 0.498742, acc.: 73.44%] [G loss: 3.519384]\n",
      "epoch:34 step:27248 [D loss: 0.336161, acc.: 90.62%] [G loss: 3.172363]\n",
      "epoch:34 step:27249 [D loss: 0.218135, acc.: 92.97%] [G loss: 3.358630]\n",
      "epoch:34 step:27250 [D loss: 0.369463, acc.: 83.59%] [G loss: 2.287402]\n",
      "epoch:34 step:27251 [D loss: 0.250248, acc.: 93.75%] [G loss: 3.533846]\n",
      "epoch:34 step:27252 [D loss: 0.361352, acc.: 83.59%] [G loss: 2.419978]\n",
      "epoch:34 step:27253 [D loss: 0.262460, acc.: 86.72%] [G loss: 2.572270]\n",
      "epoch:34 step:27254 [D loss: 0.297270, acc.: 88.28%] [G loss: 2.375499]\n",
      "epoch:34 step:27255 [D loss: 0.475137, acc.: 79.69%] [G loss: 2.907497]\n",
      "epoch:34 step:27256 [D loss: 0.362960, acc.: 85.94%] [G loss: 2.821946]\n",
      "epoch:34 step:27257 [D loss: 0.422373, acc.: 78.12%] [G loss: 2.976494]\n",
      "epoch:34 step:27258 [D loss: 0.367477, acc.: 81.25%] [G loss: 2.784801]\n",
      "epoch:34 step:27259 [D loss: 0.331972, acc.: 83.59%] [G loss: 3.010721]\n",
      "epoch:34 step:27260 [D loss: 0.255595, acc.: 87.50%] [G loss: 3.385526]\n",
      "epoch:34 step:27261 [D loss: 0.285233, acc.: 85.16%] [G loss: 3.228040]\n",
      "epoch:34 step:27262 [D loss: 0.366025, acc.: 85.94%] [G loss: 2.382848]\n",
      "epoch:34 step:27263 [D loss: 0.323504, acc.: 84.38%] [G loss: 2.647696]\n",
      "epoch:34 step:27264 [D loss: 0.307055, acc.: 85.16%] [G loss: 3.281111]\n",
      "epoch:34 step:27265 [D loss: 0.325411, acc.: 86.72%] [G loss: 2.782674]\n",
      "epoch:34 step:27266 [D loss: 0.298262, acc.: 87.50%] [G loss: 2.942034]\n",
      "epoch:34 step:27267 [D loss: 0.437332, acc.: 74.22%] [G loss: 2.448189]\n",
      "epoch:34 step:27268 [D loss: 0.311549, acc.: 83.59%] [G loss: 3.315291]\n",
      "epoch:34 step:27269 [D loss: 0.392599, acc.: 82.81%] [G loss: 2.653717]\n",
      "epoch:34 step:27270 [D loss: 0.359847, acc.: 83.59%] [G loss: 2.553398]\n",
      "epoch:34 step:27271 [D loss: 0.331949, acc.: 85.16%] [G loss: 2.716219]\n",
      "epoch:34 step:27272 [D loss: 0.390242, acc.: 80.47%] [G loss: 3.114283]\n",
      "epoch:34 step:27273 [D loss: 0.252623, acc.: 90.62%] [G loss: 3.351619]\n",
      "epoch:34 step:27274 [D loss: 0.272545, acc.: 88.28%] [G loss: 4.335748]\n",
      "epoch:34 step:27275 [D loss: 0.380815, acc.: 84.38%] [G loss: 7.712145]\n",
      "epoch:34 step:27276 [D loss: 0.548958, acc.: 79.69%] [G loss: 4.863196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:27277 [D loss: 0.201073, acc.: 92.97%] [G loss: 4.834747]\n",
      "epoch:34 step:27278 [D loss: 0.418734, acc.: 81.25%] [G loss: 4.072771]\n",
      "epoch:34 step:27279 [D loss: 0.206541, acc.: 90.62%] [G loss: 4.768034]\n",
      "epoch:34 step:27280 [D loss: 0.343964, acc.: 84.38%] [G loss: 3.638181]\n",
      "epoch:34 step:27281 [D loss: 0.348208, acc.: 82.81%] [G loss: 3.522101]\n",
      "epoch:34 step:27282 [D loss: 0.299478, acc.: 88.28%] [G loss: 3.061408]\n",
      "epoch:34 step:27283 [D loss: 0.316104, acc.: 86.72%] [G loss: 2.427902]\n",
      "epoch:34 step:27284 [D loss: 0.248000, acc.: 89.84%] [G loss: 2.820334]\n",
      "epoch:34 step:27285 [D loss: 0.255772, acc.: 91.41%] [G loss: 2.761357]\n",
      "epoch:34 step:27286 [D loss: 0.321193, acc.: 85.94%] [G loss: 2.918694]\n",
      "epoch:34 step:27287 [D loss: 0.314391, acc.: 87.50%] [G loss: 2.653808]\n",
      "epoch:34 step:27288 [D loss: 0.374693, acc.: 80.47%] [G loss: 2.962890]\n",
      "epoch:34 step:27289 [D loss: 0.281782, acc.: 89.06%] [G loss: 2.868790]\n",
      "epoch:34 step:27290 [D loss: 0.314519, acc.: 88.28%] [G loss: 3.265364]\n",
      "epoch:34 step:27291 [D loss: 0.330008, acc.: 85.16%] [G loss: 2.995486]\n",
      "epoch:34 step:27292 [D loss: 0.297970, acc.: 87.50%] [G loss: 3.193040]\n",
      "epoch:34 step:27293 [D loss: 0.342982, acc.: 84.38%] [G loss: 2.599020]\n",
      "epoch:34 step:27294 [D loss: 0.326346, acc.: 86.72%] [G loss: 3.148843]\n",
      "epoch:34 step:27295 [D loss: 0.327571, acc.: 85.94%] [G loss: 3.266542]\n",
      "epoch:34 step:27296 [D loss: 0.354045, acc.: 86.72%] [G loss: 2.564655]\n",
      "epoch:34 step:27297 [D loss: 0.246835, acc.: 90.62%] [G loss: 3.089776]\n",
      "epoch:34 step:27298 [D loss: 0.381212, acc.: 85.16%] [G loss: 4.254139]\n",
      "epoch:34 step:27299 [D loss: 0.240137, acc.: 90.62%] [G loss: 2.592480]\n",
      "epoch:34 step:27300 [D loss: 0.327312, acc.: 86.72%] [G loss: 2.966668]\n",
      "epoch:34 step:27301 [D loss: 0.305808, acc.: 85.16%] [G loss: 3.463583]\n",
      "epoch:34 step:27302 [D loss: 0.378347, acc.: 86.72%] [G loss: 3.056932]\n",
      "epoch:34 step:27303 [D loss: 0.332839, acc.: 88.28%] [G loss: 4.528757]\n",
      "epoch:34 step:27304 [D loss: 0.301465, acc.: 86.72%] [G loss: 2.780560]\n",
      "epoch:34 step:27305 [D loss: 0.377157, acc.: 82.03%] [G loss: 2.116555]\n",
      "epoch:34 step:27306 [D loss: 0.368475, acc.: 85.16%] [G loss: 2.985684]\n",
      "epoch:34 step:27307 [D loss: 0.394616, acc.: 82.03%] [G loss: 6.509127]\n",
      "epoch:34 step:27308 [D loss: 0.228405, acc.: 89.84%] [G loss: 4.096791]\n",
      "epoch:34 step:27309 [D loss: 0.261563, acc.: 87.50%] [G loss: 4.014704]\n",
      "epoch:34 step:27310 [D loss: 0.216065, acc.: 90.62%] [G loss: 2.879036]\n",
      "epoch:34 step:27311 [D loss: 0.280439, acc.: 89.84%] [G loss: 2.536032]\n",
      "epoch:34 step:27312 [D loss: 0.277410, acc.: 88.28%] [G loss: 3.277247]\n",
      "epoch:34 step:27313 [D loss: 0.350284, acc.: 86.72%] [G loss: 2.819553]\n",
      "epoch:34 step:27314 [D loss: 0.188355, acc.: 92.97%] [G loss: 2.952189]\n",
      "epoch:34 step:27315 [D loss: 0.259775, acc.: 89.06%] [G loss: 3.331343]\n",
      "epoch:34 step:27316 [D loss: 0.380641, acc.: 81.25%] [G loss: 3.315638]\n",
      "epoch:34 step:27317 [D loss: 0.341093, acc.: 84.38%] [G loss: 2.804091]\n",
      "epoch:34 step:27318 [D loss: 0.333048, acc.: 87.50%] [G loss: 2.590753]\n",
      "epoch:34 step:27319 [D loss: 0.357098, acc.: 81.25%] [G loss: 4.394433]\n",
      "epoch:34 step:27320 [D loss: 0.256101, acc.: 89.84%] [G loss: 2.651881]\n",
      "epoch:34 step:27321 [D loss: 0.223271, acc.: 92.19%] [G loss: 3.397685]\n",
      "epoch:34 step:27322 [D loss: 0.294678, acc.: 85.94%] [G loss: 4.460840]\n",
      "epoch:34 step:27323 [D loss: 0.287073, acc.: 88.28%] [G loss: 4.088997]\n",
      "epoch:34 step:27324 [D loss: 0.306783, acc.: 85.16%] [G loss: 5.736503]\n",
      "epoch:34 step:27325 [D loss: 0.277165, acc.: 87.50%] [G loss: 3.848508]\n",
      "epoch:34 step:27326 [D loss: 0.263192, acc.: 85.94%] [G loss: 4.030936]\n",
      "epoch:34 step:27327 [D loss: 0.233624, acc.: 91.41%] [G loss: 4.018124]\n",
      "epoch:34 step:27328 [D loss: 0.310758, acc.: 81.25%] [G loss: 2.939322]\n",
      "epoch:34 step:27329 [D loss: 0.296703, acc.: 86.72%] [G loss: 3.511987]\n",
      "epoch:34 step:27330 [D loss: 0.234496, acc.: 92.19%] [G loss: 3.672050]\n",
      "epoch:34 step:27331 [D loss: 0.479508, acc.: 78.12%] [G loss: 3.508585]\n",
      "epoch:34 step:27332 [D loss: 0.414884, acc.: 82.81%] [G loss: 3.291331]\n",
      "epoch:34 step:27333 [D loss: 0.312228, acc.: 89.84%] [G loss: 2.868752]\n",
      "epoch:34 step:27334 [D loss: 0.269201, acc.: 88.28%] [G loss: 4.227617]\n",
      "epoch:34 step:27335 [D loss: 0.381788, acc.: 80.47%] [G loss: 4.346148]\n",
      "epoch:35 step:27336 [D loss: 0.299787, acc.: 83.59%] [G loss: 2.642321]\n",
      "epoch:35 step:27337 [D loss: 0.295842, acc.: 86.72%] [G loss: 3.972989]\n",
      "epoch:35 step:27338 [D loss: 0.332749, acc.: 83.59%] [G loss: 2.456771]\n",
      "epoch:35 step:27339 [D loss: 0.216492, acc.: 94.53%] [G loss: 3.471268]\n",
      "epoch:35 step:27340 [D loss: 0.276126, acc.: 86.72%] [G loss: 2.798313]\n",
      "epoch:35 step:27341 [D loss: 0.288520, acc.: 85.94%] [G loss: 4.133430]\n",
      "epoch:35 step:27342 [D loss: 0.268267, acc.: 87.50%] [G loss: 4.273276]\n",
      "epoch:35 step:27343 [D loss: 0.244834, acc.: 89.84%] [G loss: 5.016283]\n",
      "epoch:35 step:27344 [D loss: 0.299987, acc.: 83.59%] [G loss: 4.528194]\n",
      "epoch:35 step:27345 [D loss: 0.340129, acc.: 86.72%] [G loss: 2.652292]\n",
      "epoch:35 step:27346 [D loss: 0.267802, acc.: 89.84%] [G loss: 5.231676]\n",
      "epoch:35 step:27347 [D loss: 0.241314, acc.: 87.50%] [G loss: 6.690581]\n",
      "epoch:35 step:27348 [D loss: 0.300835, acc.: 88.28%] [G loss: 3.376031]\n",
      "epoch:35 step:27349 [D loss: 0.279434, acc.: 85.94%] [G loss: 3.954677]\n",
      "epoch:35 step:27350 [D loss: 0.311391, acc.: 84.38%] [G loss: 3.951669]\n",
      "epoch:35 step:27351 [D loss: 0.315948, acc.: 84.38%] [G loss: 3.047095]\n",
      "epoch:35 step:27352 [D loss: 0.360139, acc.: 81.25%] [G loss: 2.591821]\n",
      "epoch:35 step:27353 [D loss: 0.319066, acc.: 88.28%] [G loss: 2.556322]\n",
      "epoch:35 step:27354 [D loss: 0.262523, acc.: 90.62%] [G loss: 2.965348]\n",
      "epoch:35 step:27355 [D loss: 0.285473, acc.: 85.94%] [G loss: 2.804409]\n",
      "epoch:35 step:27356 [D loss: 0.295835, acc.: 86.72%] [G loss: 3.389142]\n",
      "epoch:35 step:27357 [D loss: 0.265473, acc.: 89.06%] [G loss: 2.395422]\n",
      "epoch:35 step:27358 [D loss: 0.267068, acc.: 89.06%] [G loss: 3.818188]\n",
      "epoch:35 step:27359 [D loss: 0.285727, acc.: 87.50%] [G loss: 3.766511]\n",
      "epoch:35 step:27360 [D loss: 0.293779, acc.: 79.69%] [G loss: 3.781073]\n",
      "epoch:35 step:27361 [D loss: 0.240694, acc.: 88.28%] [G loss: 3.826705]\n",
      "epoch:35 step:27362 [D loss: 0.238295, acc.: 88.28%] [G loss: 3.404458]\n",
      "epoch:35 step:27363 [D loss: 0.343943, acc.: 82.03%] [G loss: 3.270483]\n",
      "epoch:35 step:27364 [D loss: 0.333804, acc.: 82.81%] [G loss: 4.621691]\n",
      "epoch:35 step:27365 [D loss: 0.350327, acc.: 78.91%] [G loss: 3.184501]\n",
      "epoch:35 step:27366 [D loss: 0.361691, acc.: 82.81%] [G loss: 5.034420]\n",
      "epoch:35 step:27367 [D loss: 0.455135, acc.: 77.34%] [G loss: 2.514679]\n",
      "epoch:35 step:27368 [D loss: 0.270204, acc.: 87.50%] [G loss: 3.144401]\n",
      "epoch:35 step:27369 [D loss: 0.300142, acc.: 85.16%] [G loss: 3.929034]\n",
      "epoch:35 step:27370 [D loss: 0.327655, acc.: 85.94%] [G loss: 4.112914]\n",
      "epoch:35 step:27371 [D loss: 0.307214, acc.: 85.16%] [G loss: 4.211629]\n",
      "epoch:35 step:27372 [D loss: 0.318589, acc.: 85.94%] [G loss: 3.493497]\n",
      "epoch:35 step:27373 [D loss: 0.299353, acc.: 85.16%] [G loss: 3.850683]\n",
      "epoch:35 step:27374 [D loss: 0.237311, acc.: 90.62%] [G loss: 3.538681]\n",
      "epoch:35 step:27375 [D loss: 0.283825, acc.: 88.28%] [G loss: 3.093146]\n",
      "epoch:35 step:27376 [D loss: 0.296283, acc.: 85.94%] [G loss: 3.112277]\n",
      "epoch:35 step:27377 [D loss: 0.260116, acc.: 88.28%] [G loss: 2.608060]\n",
      "epoch:35 step:27378 [D loss: 0.360492, acc.: 78.91%] [G loss: 2.771763]\n",
      "epoch:35 step:27379 [D loss: 0.308689, acc.: 87.50%] [G loss: 3.096376]\n",
      "epoch:35 step:27380 [D loss: 0.289356, acc.: 90.62%] [G loss: 2.368183]\n",
      "epoch:35 step:27381 [D loss: 0.313299, acc.: 85.16%] [G loss: 3.247972]\n",
      "epoch:35 step:27382 [D loss: 0.358891, acc.: 80.47%] [G loss: 3.327605]\n",
      "epoch:35 step:27383 [D loss: 0.309740, acc.: 88.28%] [G loss: 4.236603]\n",
      "epoch:35 step:27384 [D loss: 0.190181, acc.: 92.19%] [G loss: 3.936328]\n",
      "epoch:35 step:27385 [D loss: 0.297756, acc.: 85.16%] [G loss: 4.905633]\n",
      "epoch:35 step:27386 [D loss: 0.383924, acc.: 84.38%] [G loss: 6.118753]\n",
      "epoch:35 step:27387 [D loss: 0.724831, acc.: 75.00%] [G loss: 4.547682]\n",
      "epoch:35 step:27388 [D loss: 0.699045, acc.: 72.66%] [G loss: 6.008414]\n",
      "epoch:35 step:27389 [D loss: 0.394765, acc.: 83.59%] [G loss: 4.289608]\n",
      "epoch:35 step:27390 [D loss: 0.235382, acc.: 91.41%] [G loss: 5.607391]\n",
      "epoch:35 step:27391 [D loss: 0.263020, acc.: 85.94%] [G loss: 5.602393]\n",
      "epoch:35 step:27392 [D loss: 0.258428, acc.: 87.50%] [G loss: 3.018197]\n",
      "epoch:35 step:27393 [D loss: 0.433744, acc.: 78.91%] [G loss: 3.999593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27394 [D loss: 0.259280, acc.: 88.28%] [G loss: 3.952746]\n",
      "epoch:35 step:27395 [D loss: 0.267880, acc.: 89.84%] [G loss: 4.939110]\n",
      "epoch:35 step:27396 [D loss: 0.319491, acc.: 87.50%] [G loss: 3.203414]\n",
      "epoch:35 step:27397 [D loss: 0.337762, acc.: 79.69%] [G loss: 4.915026]\n",
      "epoch:35 step:27398 [D loss: 0.210778, acc.: 93.75%] [G loss: 3.683267]\n",
      "epoch:35 step:27399 [D loss: 0.367691, acc.: 82.03%] [G loss: 3.020862]\n",
      "epoch:35 step:27400 [D loss: 0.227915, acc.: 89.06%] [G loss: 3.852432]\n",
      "epoch:35 step:27401 [D loss: 0.299279, acc.: 87.50%] [G loss: 2.799021]\n",
      "epoch:35 step:27402 [D loss: 0.367947, acc.: 79.69%] [G loss: 3.457116]\n",
      "epoch:35 step:27403 [D loss: 0.268972, acc.: 87.50%] [G loss: 2.914608]\n",
      "epoch:35 step:27404 [D loss: 0.214368, acc.: 92.19%] [G loss: 3.119412]\n",
      "epoch:35 step:27405 [D loss: 0.297803, acc.: 81.25%] [G loss: 4.226797]\n",
      "epoch:35 step:27406 [D loss: 0.312129, acc.: 82.81%] [G loss: 3.230860]\n",
      "epoch:35 step:27407 [D loss: 0.212849, acc.: 92.19%] [G loss: 3.320068]\n",
      "epoch:35 step:27408 [D loss: 0.188503, acc.: 93.75%] [G loss: 3.426640]\n",
      "epoch:35 step:27409 [D loss: 0.328859, acc.: 82.03%] [G loss: 3.756260]\n",
      "epoch:35 step:27410 [D loss: 0.416304, acc.: 82.03%] [G loss: 2.904933]\n",
      "epoch:35 step:27411 [D loss: 0.334396, acc.: 85.94%] [G loss: 2.501119]\n",
      "epoch:35 step:27412 [D loss: 0.271590, acc.: 87.50%] [G loss: 2.874137]\n",
      "epoch:35 step:27413 [D loss: 0.364407, acc.: 82.81%] [G loss: 3.737036]\n",
      "epoch:35 step:27414 [D loss: 0.380852, acc.: 80.47%] [G loss: 6.061073]\n",
      "epoch:35 step:27415 [D loss: 0.395781, acc.: 83.59%] [G loss: 3.733763]\n",
      "epoch:35 step:27416 [D loss: 0.249200, acc.: 90.62%] [G loss: 5.408757]\n",
      "epoch:35 step:27417 [D loss: 0.252520, acc.: 88.28%] [G loss: 6.161394]\n",
      "epoch:35 step:27418 [D loss: 0.186757, acc.: 92.19%] [G loss: 6.587387]\n",
      "epoch:35 step:27419 [D loss: 0.257331, acc.: 91.41%] [G loss: 4.844957]\n",
      "epoch:35 step:27420 [D loss: 0.260760, acc.: 88.28%] [G loss: 4.415469]\n",
      "epoch:35 step:27421 [D loss: 0.297766, acc.: 89.84%] [G loss: 4.572704]\n",
      "epoch:35 step:27422 [D loss: 0.228989, acc.: 87.50%] [G loss: 4.561208]\n",
      "epoch:35 step:27423 [D loss: 0.213467, acc.: 91.41%] [G loss: 3.533075]\n",
      "epoch:35 step:27424 [D loss: 0.321706, acc.: 84.38%] [G loss: 3.634885]\n",
      "epoch:35 step:27425 [D loss: 0.459820, acc.: 75.00%] [G loss: 4.230815]\n",
      "epoch:35 step:27426 [D loss: 0.326339, acc.: 83.59%] [G loss: 3.543720]\n",
      "epoch:35 step:27427 [D loss: 0.298753, acc.: 84.38%] [G loss: 3.464906]\n",
      "epoch:35 step:27428 [D loss: 0.363068, acc.: 84.38%] [G loss: 3.293690]\n",
      "epoch:35 step:27429 [D loss: 0.206378, acc.: 90.62%] [G loss: 4.116199]\n",
      "epoch:35 step:27430 [D loss: 0.369333, acc.: 86.72%] [G loss: 3.683695]\n",
      "epoch:35 step:27431 [D loss: 0.336644, acc.: 82.81%] [G loss: 4.030863]\n",
      "epoch:35 step:27432 [D loss: 0.317167, acc.: 85.16%] [G loss: 4.268435]\n",
      "epoch:35 step:27433 [D loss: 0.374191, acc.: 84.38%] [G loss: 3.629633]\n",
      "epoch:35 step:27434 [D loss: 0.266417, acc.: 87.50%] [G loss: 4.093063]\n",
      "epoch:35 step:27435 [D loss: 0.254052, acc.: 86.72%] [G loss: 5.653263]\n",
      "epoch:35 step:27436 [D loss: 0.335512, acc.: 85.94%] [G loss: 5.982208]\n",
      "epoch:35 step:27437 [D loss: 0.323857, acc.: 85.16%] [G loss: 4.577918]\n",
      "epoch:35 step:27438 [D loss: 0.271128, acc.: 89.06%] [G loss: 3.593112]\n",
      "epoch:35 step:27439 [D loss: 0.351398, acc.: 85.16%] [G loss: 2.894792]\n",
      "epoch:35 step:27440 [D loss: 0.362224, acc.: 82.03%] [G loss: 4.503552]\n",
      "epoch:35 step:27441 [D loss: 0.499921, acc.: 76.56%] [G loss: 7.179872]\n",
      "epoch:35 step:27442 [D loss: 0.555920, acc.: 81.25%] [G loss: 3.506844]\n",
      "epoch:35 step:27443 [D loss: 0.255952, acc.: 89.06%] [G loss: 4.846147]\n",
      "epoch:35 step:27444 [D loss: 0.374249, acc.: 81.25%] [G loss: 3.342580]\n",
      "epoch:35 step:27445 [D loss: 0.219155, acc.: 92.19%] [G loss: 3.597360]\n",
      "epoch:35 step:27446 [D loss: 0.360283, acc.: 85.94%] [G loss: 2.816562]\n",
      "epoch:35 step:27447 [D loss: 0.333300, acc.: 84.38%] [G loss: 5.586306]\n",
      "epoch:35 step:27448 [D loss: 0.326587, acc.: 84.38%] [G loss: 3.921132]\n",
      "epoch:35 step:27449 [D loss: 0.247615, acc.: 89.84%] [G loss: 3.926947]\n",
      "epoch:35 step:27450 [D loss: 0.227104, acc.: 91.41%] [G loss: 4.747981]\n",
      "epoch:35 step:27451 [D loss: 0.308105, acc.: 85.94%] [G loss: 3.633178]\n",
      "epoch:35 step:27452 [D loss: 0.201002, acc.: 91.41%] [G loss: 4.057295]\n",
      "epoch:35 step:27453 [D loss: 0.238895, acc.: 90.62%] [G loss: 2.781349]\n",
      "epoch:35 step:27454 [D loss: 0.252570, acc.: 90.62%] [G loss: 2.985487]\n",
      "epoch:35 step:27455 [D loss: 0.293871, acc.: 88.28%] [G loss: 3.092808]\n",
      "epoch:35 step:27456 [D loss: 0.343052, acc.: 87.50%] [G loss: 3.316648]\n",
      "epoch:35 step:27457 [D loss: 0.291621, acc.: 84.38%] [G loss: 3.340041]\n",
      "epoch:35 step:27458 [D loss: 0.248181, acc.: 89.06%] [G loss: 3.105443]\n",
      "epoch:35 step:27459 [D loss: 0.314470, acc.: 83.59%] [G loss: 4.441746]\n",
      "epoch:35 step:27460 [D loss: 0.414879, acc.: 79.69%] [G loss: 4.723147]\n",
      "epoch:35 step:27461 [D loss: 0.419573, acc.: 81.25%] [G loss: 3.822438]\n",
      "epoch:35 step:27462 [D loss: 0.394289, acc.: 82.03%] [G loss: 5.825631]\n",
      "epoch:35 step:27463 [D loss: 0.269519, acc.: 91.41%] [G loss: 6.722361]\n",
      "epoch:35 step:27464 [D loss: 0.227943, acc.: 88.28%] [G loss: 4.996064]\n",
      "epoch:35 step:27465 [D loss: 0.382131, acc.: 82.03%] [G loss: 4.141808]\n",
      "epoch:35 step:27466 [D loss: 0.337635, acc.: 87.50%] [G loss: 3.578594]\n",
      "epoch:35 step:27467 [D loss: 0.214443, acc.: 93.75%] [G loss: 3.053587]\n",
      "epoch:35 step:27468 [D loss: 0.266337, acc.: 89.84%] [G loss: 4.368147]\n",
      "epoch:35 step:27469 [D loss: 0.312035, acc.: 83.59%] [G loss: 3.079209]\n",
      "epoch:35 step:27470 [D loss: 0.270395, acc.: 86.72%] [G loss: 3.489999]\n",
      "epoch:35 step:27471 [D loss: 0.248580, acc.: 88.28%] [G loss: 3.220205]\n",
      "epoch:35 step:27472 [D loss: 0.245561, acc.: 89.06%] [G loss: 3.356227]\n",
      "epoch:35 step:27473 [D loss: 0.296139, acc.: 86.72%] [G loss: 3.433910]\n",
      "epoch:35 step:27474 [D loss: 0.205475, acc.: 90.62%] [G loss: 3.413053]\n",
      "epoch:35 step:27475 [D loss: 0.289793, acc.: 89.06%] [G loss: 3.562802]\n",
      "epoch:35 step:27476 [D loss: 0.215230, acc.: 92.19%] [G loss: 3.397673]\n",
      "epoch:35 step:27477 [D loss: 0.294123, acc.: 87.50%] [G loss: 3.216947]\n",
      "epoch:35 step:27478 [D loss: 0.248767, acc.: 90.62%] [G loss: 3.132741]\n",
      "epoch:35 step:27479 [D loss: 0.401063, acc.: 80.47%] [G loss: 3.385314]\n",
      "epoch:35 step:27480 [D loss: 0.345654, acc.: 86.72%] [G loss: 3.132208]\n",
      "epoch:35 step:27481 [D loss: 0.319266, acc.: 85.94%] [G loss: 3.124640]\n",
      "epoch:35 step:27482 [D loss: 0.258593, acc.: 86.72%] [G loss: 3.427835]\n",
      "epoch:35 step:27483 [D loss: 0.271692, acc.: 89.84%] [G loss: 3.158247]\n",
      "epoch:35 step:27484 [D loss: 0.313541, acc.: 87.50%] [G loss: 2.535714]\n",
      "epoch:35 step:27485 [D loss: 0.389397, acc.: 82.81%] [G loss: 3.675513]\n",
      "epoch:35 step:27486 [D loss: 0.347926, acc.: 84.38%] [G loss: 3.111913]\n",
      "epoch:35 step:27487 [D loss: 0.328681, acc.: 85.94%] [G loss: 2.572215]\n",
      "epoch:35 step:27488 [D loss: 0.362198, acc.: 82.81%] [G loss: 3.703258]\n",
      "epoch:35 step:27489 [D loss: 0.336608, acc.: 84.38%] [G loss: 2.589146]\n",
      "epoch:35 step:27490 [D loss: 0.298093, acc.: 86.72%] [G loss: 2.838302]\n",
      "epoch:35 step:27491 [D loss: 0.303183, acc.: 85.16%] [G loss: 3.307664]\n",
      "epoch:35 step:27492 [D loss: 0.344150, acc.: 83.59%] [G loss: 3.388834]\n",
      "epoch:35 step:27493 [D loss: 0.293866, acc.: 88.28%] [G loss: 4.541904]\n",
      "epoch:35 step:27494 [D loss: 0.244759, acc.: 89.06%] [G loss: 3.588410]\n",
      "epoch:35 step:27495 [D loss: 0.339953, acc.: 83.59%] [G loss: 3.058780]\n",
      "epoch:35 step:27496 [D loss: 0.364422, acc.: 83.59%] [G loss: 2.660884]\n",
      "epoch:35 step:27497 [D loss: 0.308819, acc.: 87.50%] [G loss: 4.964511]\n",
      "epoch:35 step:27498 [D loss: 0.213288, acc.: 87.50%] [G loss: 5.320581]\n",
      "epoch:35 step:27499 [D loss: 0.162831, acc.: 94.53%] [G loss: 5.818618]\n",
      "epoch:35 step:27500 [D loss: 0.324495, acc.: 84.38%] [G loss: 4.573071]\n",
      "epoch:35 step:27501 [D loss: 0.280977, acc.: 86.72%] [G loss: 4.048087]\n",
      "epoch:35 step:27502 [D loss: 0.211182, acc.: 89.06%] [G loss: 4.111357]\n",
      "epoch:35 step:27503 [D loss: 0.341566, acc.: 86.72%] [G loss: 3.662028]\n",
      "epoch:35 step:27504 [D loss: 0.380676, acc.: 85.16%] [G loss: 2.678918]\n",
      "epoch:35 step:27505 [D loss: 0.408269, acc.: 78.91%] [G loss: 3.318738]\n",
      "epoch:35 step:27506 [D loss: 0.347195, acc.: 85.94%] [G loss: 4.733155]\n",
      "epoch:35 step:27507 [D loss: 0.385287, acc.: 83.59%] [G loss: 4.207087]\n",
      "epoch:35 step:27508 [D loss: 0.277607, acc.: 85.16%] [G loss: 4.359458]\n",
      "epoch:35 step:27509 [D loss: 0.353229, acc.: 85.16%] [G loss: 3.745144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27510 [D loss: 0.263289, acc.: 87.50%] [G loss: 4.287921]\n",
      "epoch:35 step:27511 [D loss: 0.303227, acc.: 89.06%] [G loss: 3.365271]\n",
      "epoch:35 step:27512 [D loss: 0.234248, acc.: 89.84%] [G loss: 3.579680]\n",
      "epoch:35 step:27513 [D loss: 0.285520, acc.: 85.94%] [G loss: 3.839574]\n",
      "epoch:35 step:27514 [D loss: 0.275162, acc.: 85.16%] [G loss: 3.165978]\n",
      "epoch:35 step:27515 [D loss: 0.370605, acc.: 84.38%] [G loss: 5.354414]\n",
      "epoch:35 step:27516 [D loss: 0.347344, acc.: 82.81%] [G loss: 5.011918]\n",
      "epoch:35 step:27517 [D loss: 0.323674, acc.: 82.81%] [G loss: 2.808665]\n",
      "epoch:35 step:27518 [D loss: 0.313124, acc.: 88.28%] [G loss: 2.774226]\n",
      "epoch:35 step:27519 [D loss: 0.375941, acc.: 82.81%] [G loss: 4.581789]\n",
      "epoch:35 step:27520 [D loss: 0.443341, acc.: 79.69%] [G loss: 4.168314]\n",
      "epoch:35 step:27521 [D loss: 0.423708, acc.: 82.81%] [G loss: 2.424220]\n",
      "epoch:35 step:27522 [D loss: 0.331496, acc.: 84.38%] [G loss: 4.383138]\n",
      "epoch:35 step:27523 [D loss: 0.338623, acc.: 85.94%] [G loss: 4.027643]\n",
      "epoch:35 step:27524 [D loss: 0.320249, acc.: 86.72%] [G loss: 4.120894]\n",
      "epoch:35 step:27525 [D loss: 0.362280, acc.: 82.03%] [G loss: 3.960751]\n",
      "epoch:35 step:27526 [D loss: 0.406859, acc.: 84.38%] [G loss: 2.625324]\n",
      "epoch:35 step:27527 [D loss: 0.331912, acc.: 85.94%] [G loss: 3.104685]\n",
      "epoch:35 step:27528 [D loss: 0.329647, acc.: 85.16%] [G loss: 2.978341]\n",
      "epoch:35 step:27529 [D loss: 0.283378, acc.: 82.81%] [G loss: 3.162696]\n",
      "epoch:35 step:27530 [D loss: 0.353581, acc.: 83.59%] [G loss: 4.582623]\n",
      "epoch:35 step:27531 [D loss: 0.261207, acc.: 87.50%] [G loss: 4.833319]\n",
      "epoch:35 step:27532 [D loss: 0.450927, acc.: 77.34%] [G loss: 6.195991]\n",
      "epoch:35 step:27533 [D loss: 0.461051, acc.: 81.25%] [G loss: 5.106161]\n",
      "epoch:35 step:27534 [D loss: 0.260433, acc.: 87.50%] [G loss: 4.848655]\n",
      "epoch:35 step:27535 [D loss: 0.333306, acc.: 87.50%] [G loss: 3.185474]\n",
      "epoch:35 step:27536 [D loss: 0.317669, acc.: 85.16%] [G loss: 3.658741]\n",
      "epoch:35 step:27537 [D loss: 0.529236, acc.: 80.47%] [G loss: 6.773760]\n",
      "epoch:35 step:27538 [D loss: 0.677981, acc.: 79.69%] [G loss: 4.701189]\n",
      "epoch:35 step:27539 [D loss: 0.425165, acc.: 82.81%] [G loss: 4.219839]\n",
      "epoch:35 step:27540 [D loss: 0.462916, acc.: 77.34%] [G loss: 3.806829]\n",
      "epoch:35 step:27541 [D loss: 0.307085, acc.: 89.06%] [G loss: 5.510968]\n",
      "epoch:35 step:27542 [D loss: 0.269187, acc.: 89.84%] [G loss: 3.142241]\n",
      "epoch:35 step:27543 [D loss: 0.297398, acc.: 85.94%] [G loss: 4.757874]\n",
      "epoch:35 step:27544 [D loss: 0.282785, acc.: 89.06%] [G loss: 4.389092]\n",
      "epoch:35 step:27545 [D loss: 0.296111, acc.: 90.62%] [G loss: 3.538313]\n",
      "epoch:35 step:27546 [D loss: 0.344651, acc.: 85.16%] [G loss: 5.344828]\n",
      "epoch:35 step:27547 [D loss: 0.595897, acc.: 78.91%] [G loss: 5.316382]\n",
      "epoch:35 step:27548 [D loss: 0.554746, acc.: 85.94%] [G loss: 4.023572]\n",
      "epoch:35 step:27549 [D loss: 0.456747, acc.: 82.03%] [G loss: 2.423214]\n",
      "epoch:35 step:27550 [D loss: 0.335539, acc.: 85.94%] [G loss: 3.681152]\n",
      "epoch:35 step:27551 [D loss: 0.378522, acc.: 79.69%] [G loss: 3.104471]\n",
      "epoch:35 step:27552 [D loss: 0.349425, acc.: 82.81%] [G loss: 3.026516]\n",
      "epoch:35 step:27553 [D loss: 0.321365, acc.: 87.50%] [G loss: 2.607492]\n",
      "epoch:35 step:27554 [D loss: 0.300705, acc.: 88.28%] [G loss: 3.294483]\n",
      "epoch:35 step:27555 [D loss: 0.338409, acc.: 85.94%] [G loss: 2.938264]\n",
      "epoch:35 step:27556 [D loss: 0.305983, acc.: 85.94%] [G loss: 2.831104]\n",
      "epoch:35 step:27557 [D loss: 0.282871, acc.: 88.28%] [G loss: 3.057880]\n",
      "epoch:35 step:27558 [D loss: 0.360497, acc.: 82.03%] [G loss: 3.305652]\n",
      "epoch:35 step:27559 [D loss: 0.336845, acc.: 85.16%] [G loss: 2.657398]\n",
      "epoch:35 step:27560 [D loss: 0.474326, acc.: 77.34%] [G loss: 2.570903]\n",
      "epoch:35 step:27561 [D loss: 0.402844, acc.: 82.81%] [G loss: 2.530917]\n",
      "epoch:35 step:27562 [D loss: 0.295496, acc.: 85.16%] [G loss: 2.656304]\n",
      "epoch:35 step:27563 [D loss: 0.278861, acc.: 89.06%] [G loss: 3.206977]\n",
      "epoch:35 step:27564 [D loss: 0.414912, acc.: 77.34%] [G loss: 2.558050]\n",
      "epoch:35 step:27565 [D loss: 0.247267, acc.: 90.62%] [G loss: 3.659163]\n",
      "epoch:35 step:27566 [D loss: 0.286228, acc.: 90.62%] [G loss: 2.361222]\n",
      "epoch:35 step:27567 [D loss: 0.218984, acc.: 91.41%] [G loss: 3.802501]\n",
      "epoch:35 step:27568 [D loss: 0.312032, acc.: 88.28%] [G loss: 3.611379]\n",
      "epoch:35 step:27569 [D loss: 0.237031, acc.: 89.06%] [G loss: 3.033128]\n",
      "epoch:35 step:27570 [D loss: 0.295710, acc.: 85.16%] [G loss: 3.772346]\n",
      "epoch:35 step:27571 [D loss: 0.324974, acc.: 85.94%] [G loss: 3.012145]\n",
      "epoch:35 step:27572 [D loss: 0.446874, acc.: 79.69%] [G loss: 2.621555]\n",
      "epoch:35 step:27573 [D loss: 0.272327, acc.: 89.06%] [G loss: 2.720126]\n",
      "epoch:35 step:27574 [D loss: 0.304385, acc.: 89.06%] [G loss: 3.075065]\n",
      "epoch:35 step:27575 [D loss: 0.270778, acc.: 89.06%] [G loss: 2.760014]\n",
      "epoch:35 step:27576 [D loss: 0.319289, acc.: 83.59%] [G loss: 3.022366]\n",
      "epoch:35 step:27577 [D loss: 0.320429, acc.: 87.50%] [G loss: 2.747707]\n",
      "epoch:35 step:27578 [D loss: 0.216517, acc.: 89.84%] [G loss: 3.139269]\n",
      "epoch:35 step:27579 [D loss: 0.302621, acc.: 86.72%] [G loss: 3.705165]\n",
      "epoch:35 step:27580 [D loss: 0.222277, acc.: 89.06%] [G loss: 4.567022]\n",
      "epoch:35 step:27581 [D loss: 0.304676, acc.: 89.84%] [G loss: 5.697875]\n",
      "epoch:35 step:27582 [D loss: 0.248282, acc.: 89.84%] [G loss: 5.613480]\n",
      "epoch:35 step:27583 [D loss: 0.251551, acc.: 85.94%] [G loss: 3.902041]\n",
      "epoch:35 step:27584 [D loss: 0.282187, acc.: 86.72%] [G loss: 3.141199]\n",
      "epoch:35 step:27585 [D loss: 0.293572, acc.: 82.81%] [G loss: 4.080196]\n",
      "epoch:35 step:27586 [D loss: 0.261956, acc.: 89.06%] [G loss: 3.593003]\n",
      "epoch:35 step:27587 [D loss: 0.324883, acc.: 89.06%] [G loss: 2.223212]\n",
      "epoch:35 step:27588 [D loss: 0.376397, acc.: 82.81%] [G loss: 2.587161]\n",
      "epoch:35 step:27589 [D loss: 0.314765, acc.: 85.94%] [G loss: 3.038898]\n",
      "epoch:35 step:27590 [D loss: 0.308234, acc.: 88.28%] [G loss: 3.311202]\n",
      "epoch:35 step:27591 [D loss: 0.261268, acc.: 89.84%] [G loss: 3.319330]\n",
      "epoch:35 step:27592 [D loss: 0.390659, acc.: 78.91%] [G loss: 2.851651]\n",
      "epoch:35 step:27593 [D loss: 0.267946, acc.: 86.72%] [G loss: 3.582363]\n",
      "epoch:35 step:27594 [D loss: 0.388586, acc.: 83.59%] [G loss: 2.716505]\n",
      "epoch:35 step:27595 [D loss: 0.308247, acc.: 86.72%] [G loss: 3.250201]\n",
      "epoch:35 step:27596 [D loss: 0.385919, acc.: 83.59%] [G loss: 3.294984]\n",
      "epoch:35 step:27597 [D loss: 0.360672, acc.: 83.59%] [G loss: 3.367237]\n",
      "epoch:35 step:27598 [D loss: 0.361695, acc.: 84.38%] [G loss: 3.541265]\n",
      "epoch:35 step:27599 [D loss: 0.281745, acc.: 89.06%] [G loss: 3.131631]\n",
      "epoch:35 step:27600 [D loss: 0.391725, acc.: 87.50%] [G loss: 5.464066]\n",
      "epoch:35 step:27601 [D loss: 0.684178, acc.: 75.00%] [G loss: 6.858667]\n",
      "epoch:35 step:27602 [D loss: 1.028002, acc.: 69.53%] [G loss: 7.943152]\n",
      "epoch:35 step:27603 [D loss: 1.803940, acc.: 57.81%] [G loss: 4.004424]\n",
      "epoch:35 step:27604 [D loss: 0.660220, acc.: 72.66%] [G loss: 4.688137]\n",
      "epoch:35 step:27605 [D loss: 0.901836, acc.: 71.09%] [G loss: 4.663461]\n",
      "epoch:35 step:27606 [D loss: 0.300161, acc.: 82.81%] [G loss: 5.747192]\n",
      "epoch:35 step:27607 [D loss: 0.744051, acc.: 72.66%] [G loss: 4.074540]\n",
      "epoch:35 step:27608 [D loss: 0.399181, acc.: 79.69%] [G loss: 4.027339]\n",
      "epoch:35 step:27609 [D loss: 0.547504, acc.: 72.66%] [G loss: 4.234246]\n",
      "epoch:35 step:27610 [D loss: 0.662896, acc.: 70.31%] [G loss: 3.727573]\n",
      "epoch:35 step:27611 [D loss: 0.317786, acc.: 83.59%] [G loss: 2.596650]\n",
      "epoch:35 step:27612 [D loss: 0.263560, acc.: 89.84%] [G loss: 2.829477]\n",
      "epoch:35 step:27613 [D loss: 0.278761, acc.: 89.06%] [G loss: 3.263026]\n",
      "epoch:35 step:27614 [D loss: 0.320558, acc.: 86.72%] [G loss: 3.263349]\n",
      "epoch:35 step:27615 [D loss: 0.250989, acc.: 93.75%] [G loss: 3.346876]\n",
      "epoch:35 step:27616 [D loss: 0.250661, acc.: 90.62%] [G loss: 2.677882]\n",
      "epoch:35 step:27617 [D loss: 0.339613, acc.: 84.38%] [G loss: 2.906960]\n",
      "epoch:35 step:27618 [D loss: 0.284054, acc.: 85.16%] [G loss: 3.086443]\n",
      "epoch:35 step:27619 [D loss: 0.356045, acc.: 89.06%] [G loss: 2.535876]\n",
      "epoch:35 step:27620 [D loss: 0.308985, acc.: 88.28%] [G loss: 2.596973]\n",
      "epoch:35 step:27621 [D loss: 0.322078, acc.: 84.38%] [G loss: 2.622824]\n",
      "epoch:35 step:27622 [D loss: 0.256291, acc.: 88.28%] [G loss: 2.765370]\n",
      "epoch:35 step:27623 [D loss: 0.317493, acc.: 83.59%] [G loss: 2.648132]\n",
      "epoch:35 step:27624 [D loss: 0.363024, acc.: 82.03%] [G loss: 2.486905]\n",
      "epoch:35 step:27625 [D loss: 0.271941, acc.: 89.06%] [G loss: 2.769906]\n",
      "epoch:35 step:27626 [D loss: 0.253755, acc.: 90.62%] [G loss: 3.089127]\n",
      "epoch:35 step:27627 [D loss: 0.359609, acc.: 83.59%] [G loss: 2.832852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27628 [D loss: 0.313600, acc.: 85.94%] [G loss: 2.500074]\n",
      "epoch:35 step:27629 [D loss: 0.293191, acc.: 92.19%] [G loss: 2.268156]\n",
      "epoch:35 step:27630 [D loss: 0.510532, acc.: 78.12%] [G loss: 2.869203]\n",
      "epoch:35 step:27631 [D loss: 0.338859, acc.: 85.94%] [G loss: 2.262948]\n",
      "epoch:35 step:27632 [D loss: 0.378778, acc.: 82.81%] [G loss: 2.892791]\n",
      "epoch:35 step:27633 [D loss: 0.343678, acc.: 85.16%] [G loss: 2.516373]\n",
      "epoch:35 step:27634 [D loss: 0.445527, acc.: 78.12%] [G loss: 2.461499]\n",
      "epoch:35 step:27635 [D loss: 0.364565, acc.: 84.38%] [G loss: 2.414731]\n",
      "epoch:35 step:27636 [D loss: 0.393641, acc.: 82.03%] [G loss: 2.951223]\n",
      "epoch:35 step:27637 [D loss: 0.352499, acc.: 85.94%] [G loss: 2.209552]\n",
      "epoch:35 step:27638 [D loss: 0.303198, acc.: 86.72%] [G loss: 3.442465]\n",
      "epoch:35 step:27639 [D loss: 0.237535, acc.: 90.62%] [G loss: 2.602268]\n",
      "epoch:35 step:27640 [D loss: 0.350977, acc.: 84.38%] [G loss: 2.303876]\n",
      "epoch:35 step:27641 [D loss: 0.240974, acc.: 91.41%] [G loss: 2.978829]\n",
      "epoch:35 step:27642 [D loss: 0.317511, acc.: 86.72%] [G loss: 2.796963]\n",
      "epoch:35 step:27643 [D loss: 0.310402, acc.: 86.72%] [G loss: 4.561430]\n",
      "epoch:35 step:27644 [D loss: 0.321971, acc.: 83.59%] [G loss: 2.747119]\n",
      "epoch:35 step:27645 [D loss: 0.210562, acc.: 92.19%] [G loss: 3.735994]\n",
      "epoch:35 step:27646 [D loss: 0.376605, acc.: 83.59%] [G loss: 3.310853]\n",
      "epoch:35 step:27647 [D loss: 0.284105, acc.: 91.41%] [G loss: 2.586313]\n",
      "epoch:35 step:27648 [D loss: 0.343046, acc.: 83.59%] [G loss: 3.083306]\n",
      "epoch:35 step:27649 [D loss: 0.317130, acc.: 84.38%] [G loss: 2.958575]\n",
      "epoch:35 step:27650 [D loss: 0.559634, acc.: 69.53%] [G loss: 3.924267]\n",
      "epoch:35 step:27651 [D loss: 0.292039, acc.: 86.72%] [G loss: 3.570849]\n",
      "epoch:35 step:27652 [D loss: 0.440403, acc.: 78.12%] [G loss: 4.103697]\n",
      "epoch:35 step:27653 [D loss: 0.394865, acc.: 80.47%] [G loss: 2.984072]\n",
      "epoch:35 step:27654 [D loss: 0.330668, acc.: 85.94%] [G loss: 3.793254]\n",
      "epoch:35 step:27655 [D loss: 0.267095, acc.: 89.06%] [G loss: 4.591255]\n",
      "epoch:35 step:27656 [D loss: 0.337065, acc.: 82.03%] [G loss: 3.100638]\n",
      "epoch:35 step:27657 [D loss: 0.255256, acc.: 88.28%] [G loss: 3.117299]\n",
      "epoch:35 step:27658 [D loss: 0.395816, acc.: 82.81%] [G loss: 2.925043]\n",
      "epoch:35 step:27659 [D loss: 0.415204, acc.: 82.03%] [G loss: 2.842378]\n",
      "epoch:35 step:27660 [D loss: 0.407582, acc.: 85.16%] [G loss: 2.350300]\n",
      "epoch:35 step:27661 [D loss: 0.304640, acc.: 86.72%] [G loss: 2.869544]\n",
      "epoch:35 step:27662 [D loss: 0.281617, acc.: 89.06%] [G loss: 2.448662]\n",
      "epoch:35 step:27663 [D loss: 0.311137, acc.: 87.50%] [G loss: 2.928820]\n",
      "epoch:35 step:27664 [D loss: 0.439414, acc.: 83.59%] [G loss: 2.122955]\n",
      "epoch:35 step:27665 [D loss: 0.310870, acc.: 87.50%] [G loss: 2.581893]\n",
      "epoch:35 step:27666 [D loss: 0.372778, acc.: 83.59%] [G loss: 2.852901]\n",
      "epoch:35 step:27667 [D loss: 0.380343, acc.: 85.94%] [G loss: 1.916003]\n",
      "epoch:35 step:27668 [D loss: 0.352778, acc.: 83.59%] [G loss: 2.363508]\n",
      "epoch:35 step:27669 [D loss: 0.343324, acc.: 86.72%] [G loss: 2.733899]\n",
      "epoch:35 step:27670 [D loss: 0.306764, acc.: 89.84%] [G loss: 2.220952]\n",
      "epoch:35 step:27671 [D loss: 0.257323, acc.: 89.84%] [G loss: 2.757128]\n",
      "epoch:35 step:27672 [D loss: 0.376012, acc.: 84.38%] [G loss: 2.402808]\n",
      "epoch:35 step:27673 [D loss: 0.322146, acc.: 86.72%] [G loss: 2.823921]\n",
      "epoch:35 step:27674 [D loss: 0.239591, acc.: 89.84%] [G loss: 3.080440]\n",
      "epoch:35 step:27675 [D loss: 0.353786, acc.: 83.59%] [G loss: 2.320251]\n",
      "epoch:35 step:27676 [D loss: 0.262910, acc.: 90.62%] [G loss: 3.139905]\n",
      "epoch:35 step:27677 [D loss: 0.359163, acc.: 82.81%] [G loss: 3.302184]\n",
      "epoch:35 step:27678 [D loss: 0.382180, acc.: 82.81%] [G loss: 2.265600]\n",
      "epoch:35 step:27679 [D loss: 0.254496, acc.: 88.28%] [G loss: 3.556726]\n",
      "epoch:35 step:27680 [D loss: 0.323858, acc.: 85.16%] [G loss: 3.170965]\n",
      "epoch:35 step:27681 [D loss: 0.324072, acc.: 82.03%] [G loss: 3.468393]\n",
      "epoch:35 step:27682 [D loss: 0.394459, acc.: 81.25%] [G loss: 2.802114]\n",
      "epoch:35 step:27683 [D loss: 0.364479, acc.: 84.38%] [G loss: 3.155642]\n",
      "epoch:35 step:27684 [D loss: 0.406544, acc.: 80.47%] [G loss: 3.278797]\n",
      "epoch:35 step:27685 [D loss: 0.314888, acc.: 84.38%] [G loss: 3.046715]\n",
      "epoch:35 step:27686 [D loss: 0.453400, acc.: 79.69%] [G loss: 2.864113]\n",
      "epoch:35 step:27687 [D loss: 0.309654, acc.: 83.59%] [G loss: 3.157603]\n",
      "epoch:35 step:27688 [D loss: 0.396113, acc.: 81.25%] [G loss: 2.938820]\n",
      "epoch:35 step:27689 [D loss: 0.391813, acc.: 82.81%] [G loss: 2.733526]\n",
      "epoch:35 step:27690 [D loss: 0.256058, acc.: 86.72%] [G loss: 3.120007]\n",
      "epoch:35 step:27691 [D loss: 0.345269, acc.: 84.38%] [G loss: 3.738026]\n",
      "epoch:35 step:27692 [D loss: 0.359902, acc.: 82.81%] [G loss: 3.726995]\n",
      "epoch:35 step:27693 [D loss: 0.211349, acc.: 93.75%] [G loss: 3.359494]\n",
      "epoch:35 step:27694 [D loss: 0.427823, acc.: 82.81%] [G loss: 3.684212]\n",
      "epoch:35 step:27695 [D loss: 0.285853, acc.: 87.50%] [G loss: 5.753501]\n",
      "epoch:35 step:27696 [D loss: 0.288387, acc.: 88.28%] [G loss: 5.186501]\n",
      "epoch:35 step:27697 [D loss: 0.251296, acc.: 89.06%] [G loss: 4.599604]\n",
      "epoch:35 step:27698 [D loss: 0.280373, acc.: 87.50%] [G loss: 2.756993]\n",
      "epoch:35 step:27699 [D loss: 0.230559, acc.: 90.62%] [G loss: 4.064921]\n",
      "epoch:35 step:27700 [D loss: 0.261203, acc.: 88.28%] [G loss: 3.728787]\n",
      "epoch:35 step:27701 [D loss: 0.335422, acc.: 83.59%] [G loss: 4.859630]\n",
      "epoch:35 step:27702 [D loss: 0.318963, acc.: 85.94%] [G loss: 3.601081]\n",
      "epoch:35 step:27703 [D loss: 0.286151, acc.: 87.50%] [G loss: 3.374745]\n",
      "epoch:35 step:27704 [D loss: 0.281539, acc.: 87.50%] [G loss: 3.563361]\n",
      "epoch:35 step:27705 [D loss: 0.430832, acc.: 75.78%] [G loss: 2.370672]\n",
      "epoch:35 step:27706 [D loss: 0.259583, acc.: 88.28%] [G loss: 3.491848]\n",
      "epoch:35 step:27707 [D loss: 0.248455, acc.: 90.62%] [G loss: 3.521536]\n",
      "epoch:35 step:27708 [D loss: 0.326708, acc.: 82.81%] [G loss: 3.624404]\n",
      "epoch:35 step:27709 [D loss: 0.362091, acc.: 83.59%] [G loss: 3.241819]\n",
      "epoch:35 step:27710 [D loss: 0.358778, acc.: 85.16%] [G loss: 4.549994]\n",
      "epoch:35 step:27711 [D loss: 0.278231, acc.: 86.72%] [G loss: 3.843655]\n",
      "epoch:35 step:27712 [D loss: 0.297602, acc.: 86.72%] [G loss: 3.496201]\n",
      "epoch:35 step:27713 [D loss: 0.335841, acc.: 85.16%] [G loss: 3.277470]\n",
      "epoch:35 step:27714 [D loss: 0.332208, acc.: 83.59%] [G loss: 3.375152]\n",
      "epoch:35 step:27715 [D loss: 0.363723, acc.: 82.81%] [G loss: 2.775813]\n",
      "epoch:35 step:27716 [D loss: 0.273046, acc.: 90.62%] [G loss: 3.405535]\n",
      "epoch:35 step:27717 [D loss: 0.324205, acc.: 85.94%] [G loss: 2.766019]\n",
      "epoch:35 step:27718 [D loss: 0.253556, acc.: 87.50%] [G loss: 2.255900]\n",
      "epoch:35 step:27719 [D loss: 0.276024, acc.: 88.28%] [G loss: 3.173224]\n",
      "epoch:35 step:27720 [D loss: 0.348206, acc.: 81.25%] [G loss: 2.867517]\n",
      "epoch:35 step:27721 [D loss: 0.391202, acc.: 84.38%] [G loss: 4.500087]\n",
      "epoch:35 step:27722 [D loss: 0.352194, acc.: 81.25%] [G loss: 4.118285]\n",
      "epoch:35 step:27723 [D loss: 0.304369, acc.: 87.50%] [G loss: 4.101178]\n",
      "epoch:35 step:27724 [D loss: 0.271025, acc.: 88.28%] [G loss: 4.526958]\n",
      "epoch:35 step:27725 [D loss: 0.222389, acc.: 86.72%] [G loss: 4.376618]\n",
      "epoch:35 step:27726 [D loss: 0.405920, acc.: 80.47%] [G loss: 3.888399]\n",
      "epoch:35 step:27727 [D loss: 0.205195, acc.: 92.97%] [G loss: 3.740047]\n",
      "epoch:35 step:27728 [D loss: 0.297574, acc.: 86.72%] [G loss: 3.030217]\n",
      "epoch:35 step:27729 [D loss: 0.311360, acc.: 85.94%] [G loss: 3.417089]\n",
      "epoch:35 step:27730 [D loss: 0.315996, acc.: 86.72%] [G loss: 4.184052]\n",
      "epoch:35 step:27731 [D loss: 0.223050, acc.: 92.97%] [G loss: 3.628635]\n",
      "epoch:35 step:27732 [D loss: 0.254547, acc.: 88.28%] [G loss: 3.927959]\n",
      "epoch:35 step:27733 [D loss: 0.304210, acc.: 84.38%] [G loss: 4.642510]\n",
      "epoch:35 step:27734 [D loss: 0.480260, acc.: 75.78%] [G loss: 3.768789]\n",
      "epoch:35 step:27735 [D loss: 0.286558, acc.: 87.50%] [G loss: 2.986676]\n",
      "epoch:35 step:27736 [D loss: 0.338293, acc.: 89.06%] [G loss: 3.582198]\n",
      "epoch:35 step:27737 [D loss: 0.256372, acc.: 89.84%] [G loss: 2.887540]\n",
      "epoch:35 step:27738 [D loss: 0.304590, acc.: 85.16%] [G loss: 2.714328]\n",
      "epoch:35 step:27739 [D loss: 0.263623, acc.: 87.50%] [G loss: 2.800830]\n",
      "epoch:35 step:27740 [D loss: 0.385321, acc.: 80.47%] [G loss: 2.740968]\n",
      "epoch:35 step:27741 [D loss: 0.371049, acc.: 80.47%] [G loss: 2.779640]\n",
      "epoch:35 step:27742 [D loss: 0.279959, acc.: 87.50%] [G loss: 3.073037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27743 [D loss: 0.496350, acc.: 78.91%] [G loss: 3.411336]\n",
      "epoch:35 step:27744 [D loss: 0.595126, acc.: 72.66%] [G loss: 5.196699]\n",
      "epoch:35 step:27745 [D loss: 0.555595, acc.: 74.22%] [G loss: 4.666826]\n",
      "epoch:35 step:27746 [D loss: 0.610054, acc.: 78.12%] [G loss: 7.304201]\n",
      "epoch:35 step:27747 [D loss: 1.286981, acc.: 51.56%] [G loss: 10.167206]\n",
      "epoch:35 step:27748 [D loss: 2.265771, acc.: 63.28%] [G loss: 6.323190]\n",
      "epoch:35 step:27749 [D loss: 0.463049, acc.: 82.81%] [G loss: 5.069116]\n",
      "epoch:35 step:27750 [D loss: 0.647170, acc.: 71.88%] [G loss: 5.555544]\n",
      "epoch:35 step:27751 [D loss: 0.368001, acc.: 80.47%] [G loss: 3.843575]\n",
      "epoch:35 step:27752 [D loss: 0.531033, acc.: 78.91%] [G loss: 3.437563]\n",
      "epoch:35 step:27753 [D loss: 0.392956, acc.: 83.59%] [G loss: 3.409633]\n",
      "epoch:35 step:27754 [D loss: 0.311311, acc.: 85.94%] [G loss: 4.072587]\n",
      "epoch:35 step:27755 [D loss: 0.266963, acc.: 87.50%] [G loss: 3.397482]\n",
      "epoch:35 step:27756 [D loss: 0.299951, acc.: 85.94%] [G loss: 3.713202]\n",
      "epoch:35 step:27757 [D loss: 0.360049, acc.: 86.72%] [G loss: 2.507630]\n",
      "epoch:35 step:27758 [D loss: 0.363143, acc.: 85.94%] [G loss: 2.258562]\n",
      "epoch:35 step:27759 [D loss: 0.253155, acc.: 89.84%] [G loss: 2.685081]\n",
      "epoch:35 step:27760 [D loss: 0.343823, acc.: 83.59%] [G loss: 2.342963]\n",
      "epoch:35 step:27761 [D loss: 0.384290, acc.: 86.72%] [G loss: 2.820467]\n",
      "epoch:35 step:27762 [D loss: 0.300810, acc.: 87.50%] [G loss: 2.844532]\n",
      "epoch:35 step:27763 [D loss: 0.349268, acc.: 81.25%] [G loss: 2.299331]\n",
      "epoch:35 step:27764 [D loss: 0.371060, acc.: 82.03%] [G loss: 3.841850]\n",
      "epoch:35 step:27765 [D loss: 0.341927, acc.: 84.38%] [G loss: 3.057666]\n",
      "epoch:35 step:27766 [D loss: 0.344242, acc.: 83.59%] [G loss: 3.818859]\n",
      "epoch:35 step:27767 [D loss: 0.490078, acc.: 75.00%] [G loss: 2.960803]\n",
      "epoch:35 step:27768 [D loss: 0.265564, acc.: 89.84%] [G loss: 3.954873]\n",
      "epoch:35 step:27769 [D loss: 0.345695, acc.: 84.38%] [G loss: 2.689129]\n",
      "epoch:35 step:27770 [D loss: 0.326951, acc.: 86.72%] [G loss: 2.921421]\n",
      "epoch:35 step:27771 [D loss: 0.511560, acc.: 73.44%] [G loss: 1.810817]\n",
      "epoch:35 step:27772 [D loss: 0.274200, acc.: 89.06%] [G loss: 2.428819]\n",
      "epoch:35 step:27773 [D loss: 0.304155, acc.: 88.28%] [G loss: 2.903177]\n",
      "epoch:35 step:27774 [D loss: 0.253505, acc.: 87.50%] [G loss: 3.435940]\n",
      "epoch:35 step:27775 [D loss: 0.278359, acc.: 87.50%] [G loss: 2.347509]\n",
      "epoch:35 step:27776 [D loss: 0.381774, acc.: 82.81%] [G loss: 3.067884]\n",
      "epoch:35 step:27777 [D loss: 0.302299, acc.: 86.72%] [G loss: 3.122659]\n",
      "epoch:35 step:27778 [D loss: 0.264525, acc.: 89.84%] [G loss: 4.599751]\n",
      "epoch:35 step:27779 [D loss: 0.435604, acc.: 80.47%] [G loss: 2.714788]\n",
      "epoch:35 step:27780 [D loss: 0.327505, acc.: 84.38%] [G loss: 2.355148]\n",
      "epoch:35 step:27781 [D loss: 0.327758, acc.: 82.03%] [G loss: 2.790684]\n",
      "epoch:35 step:27782 [D loss: 0.188977, acc.: 95.31%] [G loss: 3.195551]\n",
      "epoch:35 step:27783 [D loss: 0.462965, acc.: 78.12%] [G loss: 2.730062]\n",
      "epoch:35 step:27784 [D loss: 0.336650, acc.: 87.50%] [G loss: 2.863267]\n",
      "epoch:35 step:27785 [D loss: 0.317844, acc.: 84.38%] [G loss: 2.961831]\n",
      "epoch:35 step:27786 [D loss: 0.248483, acc.: 86.72%] [G loss: 1.957912]\n",
      "epoch:35 step:27787 [D loss: 0.242225, acc.: 92.19%] [G loss: 3.544553]\n",
      "epoch:35 step:27788 [D loss: 0.270004, acc.: 86.72%] [G loss: 2.854897]\n",
      "epoch:35 step:27789 [D loss: 0.312089, acc.: 85.94%] [G loss: 2.807780]\n",
      "epoch:35 step:27790 [D loss: 0.379827, acc.: 83.59%] [G loss: 2.771778]\n",
      "epoch:35 step:27791 [D loss: 0.411030, acc.: 82.81%] [G loss: 2.110830]\n",
      "epoch:35 step:27792 [D loss: 0.380122, acc.: 86.72%] [G loss: 2.675678]\n",
      "epoch:35 step:27793 [D loss: 0.313704, acc.: 85.16%] [G loss: 2.289992]\n",
      "epoch:35 step:27794 [D loss: 0.331333, acc.: 84.38%] [G loss: 2.394007]\n",
      "epoch:35 step:27795 [D loss: 0.400078, acc.: 81.25%] [G loss: 1.956072]\n",
      "epoch:35 step:27796 [D loss: 0.379273, acc.: 81.25%] [G loss: 2.860027]\n",
      "epoch:35 step:27797 [D loss: 0.397140, acc.: 81.25%] [G loss: 2.449616]\n",
      "epoch:35 step:27798 [D loss: 0.346736, acc.: 79.69%] [G loss: 2.361940]\n",
      "epoch:35 step:27799 [D loss: 0.311756, acc.: 88.28%] [G loss: 2.393250]\n",
      "epoch:35 step:27800 [D loss: 0.319903, acc.: 85.16%] [G loss: 2.417728]\n",
      "epoch:35 step:27801 [D loss: 0.404966, acc.: 78.12%] [G loss: 2.620868]\n",
      "epoch:35 step:27802 [D loss: 0.303740, acc.: 85.16%] [G loss: 3.116499]\n",
      "epoch:35 step:27803 [D loss: 0.247418, acc.: 89.84%] [G loss: 4.322965]\n",
      "epoch:35 step:27804 [D loss: 0.322682, acc.: 79.69%] [G loss: 4.265036]\n",
      "epoch:35 step:27805 [D loss: 0.289557, acc.: 87.50%] [G loss: 2.961988]\n",
      "epoch:35 step:27806 [D loss: 0.360128, acc.: 84.38%] [G loss: 3.286596]\n",
      "epoch:35 step:27807 [D loss: 0.221567, acc.: 89.84%] [G loss: 4.009124]\n",
      "epoch:35 step:27808 [D loss: 0.327018, acc.: 84.38%] [G loss: 2.401037]\n",
      "epoch:35 step:27809 [D loss: 0.256499, acc.: 89.06%] [G loss: 2.592113]\n",
      "epoch:35 step:27810 [D loss: 0.312579, acc.: 85.94%] [G loss: 3.615291]\n",
      "epoch:35 step:27811 [D loss: 0.283078, acc.: 87.50%] [G loss: 3.024506]\n",
      "epoch:35 step:27812 [D loss: 0.256022, acc.: 89.06%] [G loss: 3.530648]\n",
      "epoch:35 step:27813 [D loss: 0.255084, acc.: 89.06%] [G loss: 3.361378]\n",
      "epoch:35 step:27814 [D loss: 0.273117, acc.: 85.16%] [G loss: 3.395851]\n",
      "epoch:35 step:27815 [D loss: 0.328759, acc.: 85.94%] [G loss: 3.127334]\n",
      "epoch:35 step:27816 [D loss: 0.438505, acc.: 81.25%] [G loss: 2.716327]\n",
      "epoch:35 step:27817 [D loss: 0.318492, acc.: 83.59%] [G loss: 3.609211]\n",
      "epoch:35 step:27818 [D loss: 0.388982, acc.: 78.12%] [G loss: 2.675498]\n",
      "epoch:35 step:27819 [D loss: 0.344146, acc.: 86.72%] [G loss: 3.519745]\n",
      "epoch:35 step:27820 [D loss: 0.296609, acc.: 85.94%] [G loss: 2.508136]\n",
      "epoch:35 step:27821 [D loss: 0.301202, acc.: 86.72%] [G loss: 3.260420]\n",
      "epoch:35 step:27822 [D loss: 0.255132, acc.: 92.19%] [G loss: 3.369486]\n",
      "epoch:35 step:27823 [D loss: 0.308905, acc.: 84.38%] [G loss: 2.734692]\n",
      "epoch:35 step:27824 [D loss: 0.411345, acc.: 79.69%] [G loss: 3.075969]\n",
      "epoch:35 step:27825 [D loss: 0.300365, acc.: 87.50%] [G loss: 3.025866]\n",
      "epoch:35 step:27826 [D loss: 0.362931, acc.: 85.16%] [G loss: 2.625327]\n",
      "epoch:35 step:27827 [D loss: 0.307207, acc.: 81.25%] [G loss: 2.691872]\n",
      "epoch:35 step:27828 [D loss: 0.317559, acc.: 85.16%] [G loss: 2.664131]\n",
      "epoch:35 step:27829 [D loss: 0.277059, acc.: 89.84%] [G loss: 2.996826]\n",
      "epoch:35 step:27830 [D loss: 0.238155, acc.: 93.75%] [G loss: 2.937337]\n",
      "epoch:35 step:27831 [D loss: 0.282554, acc.: 85.94%] [G loss: 3.142792]\n",
      "epoch:35 step:27832 [D loss: 0.327771, acc.: 81.25%] [G loss: 2.955845]\n",
      "epoch:35 step:27833 [D loss: 0.323310, acc.: 85.16%] [G loss: 2.789288]\n",
      "epoch:35 step:27834 [D loss: 0.301448, acc.: 83.59%] [G loss: 2.557070]\n",
      "epoch:35 step:27835 [D loss: 0.347151, acc.: 84.38%] [G loss: 2.889491]\n",
      "epoch:35 step:27836 [D loss: 0.347533, acc.: 84.38%] [G loss: 2.282501]\n",
      "epoch:35 step:27837 [D loss: 0.419678, acc.: 81.25%] [G loss: 2.487411]\n",
      "epoch:35 step:27838 [D loss: 0.480220, acc.: 76.56%] [G loss: 2.560449]\n",
      "epoch:35 step:27839 [D loss: 0.232237, acc.: 92.19%] [G loss: 2.889197]\n",
      "epoch:35 step:27840 [D loss: 0.299219, acc.: 87.50%] [G loss: 3.172506]\n",
      "epoch:35 step:27841 [D loss: 0.342773, acc.: 83.59%] [G loss: 2.607434]\n",
      "epoch:35 step:27842 [D loss: 0.283337, acc.: 88.28%] [G loss: 3.141681]\n",
      "epoch:35 step:27843 [D loss: 0.311551, acc.: 83.59%] [G loss: 2.864834]\n",
      "epoch:35 step:27844 [D loss: 0.340205, acc.: 83.59%] [G loss: 3.226536]\n",
      "epoch:35 step:27845 [D loss: 0.244689, acc.: 89.84%] [G loss: 4.000774]\n",
      "epoch:35 step:27846 [D loss: 0.298259, acc.: 85.16%] [G loss: 2.734260]\n",
      "epoch:35 step:27847 [D loss: 0.266362, acc.: 92.19%] [G loss: 3.007627]\n",
      "epoch:35 step:27848 [D loss: 0.320380, acc.: 88.28%] [G loss: 3.022898]\n",
      "epoch:35 step:27849 [D loss: 0.337610, acc.: 81.25%] [G loss: 2.745210]\n",
      "epoch:35 step:27850 [D loss: 0.302587, acc.: 85.16%] [G loss: 2.804135]\n",
      "epoch:35 step:27851 [D loss: 0.284805, acc.: 89.06%] [G loss: 3.763514]\n",
      "epoch:35 step:27852 [D loss: 0.403496, acc.: 81.25%] [G loss: 2.536937]\n",
      "epoch:35 step:27853 [D loss: 0.281202, acc.: 85.94%] [G loss: 3.456654]\n",
      "epoch:35 step:27854 [D loss: 0.362139, acc.: 78.91%] [G loss: 2.982046]\n",
      "epoch:35 step:27855 [D loss: 0.363986, acc.: 79.69%] [G loss: 4.310780]\n",
      "epoch:35 step:27856 [D loss: 0.329968, acc.: 85.16%] [G loss: 3.327181]\n",
      "epoch:35 step:27857 [D loss: 0.340002, acc.: 85.16%] [G loss: 4.094599]\n",
      "epoch:35 step:27858 [D loss: 0.370874, acc.: 81.25%] [G loss: 2.726226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27859 [D loss: 0.329162, acc.: 84.38%] [G loss: 3.369381]\n",
      "epoch:35 step:27860 [D loss: 0.330556, acc.: 83.59%] [G loss: 2.776206]\n",
      "epoch:35 step:27861 [D loss: 0.357397, acc.: 83.59%] [G loss: 3.406126]\n",
      "epoch:35 step:27862 [D loss: 0.307650, acc.: 85.94%] [G loss: 2.707493]\n",
      "epoch:35 step:27863 [D loss: 0.326374, acc.: 86.72%] [G loss: 3.403704]\n",
      "epoch:35 step:27864 [D loss: 0.238921, acc.: 89.06%] [G loss: 3.276665]\n",
      "epoch:35 step:27865 [D loss: 0.412127, acc.: 78.91%] [G loss: 2.373583]\n",
      "epoch:35 step:27866 [D loss: 0.224506, acc.: 92.97%] [G loss: 2.859103]\n",
      "epoch:35 step:27867 [D loss: 0.439375, acc.: 77.34%] [G loss: 3.703346]\n",
      "epoch:35 step:27868 [D loss: 0.470727, acc.: 78.91%] [G loss: 2.871069]\n",
      "epoch:35 step:27869 [D loss: 0.174971, acc.: 94.53%] [G loss: 4.340468]\n",
      "epoch:35 step:27870 [D loss: 0.393735, acc.: 79.69%] [G loss: 2.481268]\n",
      "epoch:35 step:27871 [D loss: 0.275397, acc.: 87.50%] [G loss: 5.018737]\n",
      "epoch:35 step:27872 [D loss: 0.332550, acc.: 83.59%] [G loss: 2.906345]\n",
      "epoch:35 step:27873 [D loss: 0.259607, acc.: 89.84%] [G loss: 4.121139]\n",
      "epoch:35 step:27874 [D loss: 0.339902, acc.: 82.81%] [G loss: 3.284594]\n",
      "epoch:35 step:27875 [D loss: 0.259486, acc.: 88.28%] [G loss: 2.953535]\n",
      "epoch:35 step:27876 [D loss: 0.368760, acc.: 84.38%] [G loss: 3.785792]\n",
      "epoch:35 step:27877 [D loss: 0.413441, acc.: 85.94%] [G loss: 2.583234]\n",
      "epoch:35 step:27878 [D loss: 0.210799, acc.: 92.97%] [G loss: 3.031086]\n",
      "epoch:35 step:27879 [D loss: 0.329986, acc.: 85.94%] [G loss: 3.380133]\n",
      "epoch:35 step:27880 [D loss: 0.349902, acc.: 85.16%] [G loss: 1.870731]\n",
      "epoch:35 step:27881 [D loss: 0.416345, acc.: 80.47%] [G loss: 3.307314]\n",
      "epoch:35 step:27882 [D loss: 0.386525, acc.: 81.25%] [G loss: 2.475651]\n",
      "epoch:35 step:27883 [D loss: 0.307280, acc.: 88.28%] [G loss: 2.693260]\n",
      "epoch:35 step:27884 [D loss: 0.203120, acc.: 92.97%] [G loss: 3.372423]\n",
      "epoch:35 step:27885 [D loss: 0.249219, acc.: 88.28%] [G loss: 3.652999]\n",
      "epoch:35 step:27886 [D loss: 0.320003, acc.: 83.59%] [G loss: 2.262710]\n",
      "epoch:35 step:27887 [D loss: 0.345650, acc.: 85.94%] [G loss: 3.336570]\n",
      "epoch:35 step:27888 [D loss: 0.267091, acc.: 85.94%] [G loss: 2.446971]\n",
      "epoch:35 step:27889 [D loss: 0.321514, acc.: 85.16%] [G loss: 3.563753]\n",
      "epoch:35 step:27890 [D loss: 0.434835, acc.: 78.12%] [G loss: 2.967887]\n",
      "epoch:35 step:27891 [D loss: 0.374356, acc.: 81.25%] [G loss: 3.343934]\n",
      "epoch:35 step:27892 [D loss: 0.308934, acc.: 84.38%] [G loss: 2.874696]\n",
      "epoch:35 step:27893 [D loss: 0.297571, acc.: 89.06%] [G loss: 3.160316]\n",
      "epoch:35 step:27894 [D loss: 0.272604, acc.: 85.94%] [G loss: 3.886154]\n",
      "epoch:35 step:27895 [D loss: 0.301566, acc.: 81.25%] [G loss: 2.919941]\n",
      "epoch:35 step:27896 [D loss: 0.323122, acc.: 83.59%] [G loss: 3.241254]\n",
      "epoch:35 step:27897 [D loss: 0.395517, acc.: 79.69%] [G loss: 3.313322]\n",
      "epoch:35 step:27898 [D loss: 0.216737, acc.: 89.06%] [G loss: 3.261912]\n",
      "epoch:35 step:27899 [D loss: 0.301004, acc.: 85.94%] [G loss: 2.595777]\n",
      "epoch:35 step:27900 [D loss: 0.345581, acc.: 79.69%] [G loss: 2.990789]\n",
      "epoch:35 step:27901 [D loss: 0.291728, acc.: 88.28%] [G loss: 2.465804]\n",
      "epoch:35 step:27902 [D loss: 0.286835, acc.: 89.84%] [G loss: 2.713844]\n",
      "epoch:35 step:27903 [D loss: 0.336147, acc.: 81.25%] [G loss: 2.838202]\n",
      "epoch:35 step:27904 [D loss: 0.353779, acc.: 83.59%] [G loss: 2.547998]\n",
      "epoch:35 step:27905 [D loss: 0.316773, acc.: 86.72%] [G loss: 3.213135]\n",
      "epoch:35 step:27906 [D loss: 0.322934, acc.: 84.38%] [G loss: 2.930330]\n",
      "epoch:35 step:27907 [D loss: 0.280438, acc.: 89.84%] [G loss: 2.808042]\n",
      "epoch:35 step:27908 [D loss: 0.307878, acc.: 85.94%] [G loss: 2.868144]\n",
      "epoch:35 step:27909 [D loss: 0.334146, acc.: 84.38%] [G loss: 2.483862]\n",
      "epoch:35 step:27910 [D loss: 0.356847, acc.: 86.72%] [G loss: 2.711686]\n",
      "epoch:35 step:27911 [D loss: 0.318283, acc.: 85.16%] [G loss: 3.029047]\n",
      "epoch:35 step:27912 [D loss: 0.334823, acc.: 82.03%] [G loss: 3.301643]\n",
      "epoch:35 step:27913 [D loss: 0.335810, acc.: 83.59%] [G loss: 3.187405]\n",
      "epoch:35 step:27914 [D loss: 0.221486, acc.: 89.06%] [G loss: 2.222486]\n",
      "epoch:35 step:27915 [D loss: 0.380168, acc.: 81.25%] [G loss: 2.908522]\n",
      "epoch:35 step:27916 [D loss: 0.336607, acc.: 82.81%] [G loss: 3.032001]\n",
      "epoch:35 step:27917 [D loss: 0.339130, acc.: 84.38%] [G loss: 3.051234]\n",
      "epoch:35 step:27918 [D loss: 0.443539, acc.: 77.34%] [G loss: 3.608270]\n",
      "epoch:35 step:27919 [D loss: 0.339354, acc.: 82.81%] [G loss: 3.528356]\n",
      "epoch:35 step:27920 [D loss: 0.228311, acc.: 92.19%] [G loss: 3.258324]\n",
      "epoch:35 step:27921 [D loss: 0.337066, acc.: 82.81%] [G loss: 2.728771]\n",
      "epoch:35 step:27922 [D loss: 0.334859, acc.: 78.12%] [G loss: 2.957553]\n",
      "epoch:35 step:27923 [D loss: 0.274951, acc.: 88.28%] [G loss: 2.789219]\n",
      "epoch:35 step:27924 [D loss: 0.312101, acc.: 88.28%] [G loss: 3.233608]\n",
      "epoch:35 step:27925 [D loss: 0.341532, acc.: 84.38%] [G loss: 3.442694]\n",
      "epoch:35 step:27926 [D loss: 0.251280, acc.: 85.16%] [G loss: 3.289188]\n",
      "epoch:35 step:27927 [D loss: 0.290986, acc.: 85.16%] [G loss: 3.172388]\n",
      "epoch:35 step:27928 [D loss: 0.302481, acc.: 87.50%] [G loss: 2.721760]\n",
      "epoch:35 step:27929 [D loss: 0.395581, acc.: 80.47%] [G loss: 2.981850]\n",
      "epoch:35 step:27930 [D loss: 0.389442, acc.: 81.25%] [G loss: 2.849488]\n",
      "epoch:35 step:27931 [D loss: 0.224477, acc.: 89.06%] [G loss: 3.584275]\n",
      "epoch:35 step:27932 [D loss: 0.296404, acc.: 85.94%] [G loss: 3.310030]\n",
      "epoch:35 step:27933 [D loss: 0.360802, acc.: 82.03%] [G loss: 3.044662]\n",
      "epoch:35 step:27934 [D loss: 0.293973, acc.: 89.84%] [G loss: 3.495840]\n",
      "epoch:35 step:27935 [D loss: 0.313805, acc.: 85.94%] [G loss: 2.469595]\n",
      "epoch:35 step:27936 [D loss: 0.296152, acc.: 89.06%] [G loss: 3.097550]\n",
      "epoch:35 step:27937 [D loss: 0.275816, acc.: 87.50%] [G loss: 3.657611]\n",
      "epoch:35 step:27938 [D loss: 0.471879, acc.: 78.91%] [G loss: 4.001002]\n",
      "epoch:35 step:27939 [D loss: 0.341798, acc.: 83.59%] [G loss: 2.499513]\n",
      "epoch:35 step:27940 [D loss: 0.362640, acc.: 83.59%] [G loss: 3.994692]\n",
      "epoch:35 step:27941 [D loss: 0.322643, acc.: 83.59%] [G loss: 2.925616]\n",
      "epoch:35 step:27942 [D loss: 0.328454, acc.: 82.03%] [G loss: 3.023856]\n",
      "epoch:35 step:27943 [D loss: 0.265914, acc.: 86.72%] [G loss: 3.186218]\n",
      "epoch:35 step:27944 [D loss: 0.291392, acc.: 85.94%] [G loss: 3.333877]\n",
      "epoch:35 step:27945 [D loss: 0.374728, acc.: 82.81%] [G loss: 2.170966]\n",
      "epoch:35 step:27946 [D loss: 0.261488, acc.: 87.50%] [G loss: 3.066239]\n",
      "epoch:35 step:27947 [D loss: 0.333380, acc.: 89.06%] [G loss: 2.520064]\n",
      "epoch:35 step:27948 [D loss: 0.396361, acc.: 78.91%] [G loss: 3.546825]\n",
      "epoch:35 step:27949 [D loss: 0.361880, acc.: 85.94%] [G loss: 3.101577]\n",
      "epoch:35 step:27950 [D loss: 0.323347, acc.: 85.16%] [G loss: 3.235171]\n",
      "epoch:35 step:27951 [D loss: 0.350855, acc.: 82.81%] [G loss: 3.002436]\n",
      "epoch:35 step:27952 [D loss: 0.405125, acc.: 82.03%] [G loss: 3.094369]\n",
      "epoch:35 step:27953 [D loss: 0.307229, acc.: 89.06%] [G loss: 2.705657]\n",
      "epoch:35 step:27954 [D loss: 0.285609, acc.: 89.84%] [G loss: 2.945576]\n",
      "epoch:35 step:27955 [D loss: 0.310841, acc.: 84.38%] [G loss: 3.068104]\n",
      "epoch:35 step:27956 [D loss: 0.262488, acc.: 87.50%] [G loss: 2.495108]\n",
      "epoch:35 step:27957 [D loss: 0.276037, acc.: 88.28%] [G loss: 2.396597]\n",
      "epoch:35 step:27958 [D loss: 0.361671, acc.: 88.28%] [G loss: 3.019341]\n",
      "epoch:35 step:27959 [D loss: 0.411009, acc.: 82.81%] [G loss: 2.751698]\n",
      "epoch:35 step:27960 [D loss: 0.403473, acc.: 82.03%] [G loss: 3.244646]\n",
      "epoch:35 step:27961 [D loss: 0.300403, acc.: 88.28%] [G loss: 3.110829]\n",
      "epoch:35 step:27962 [D loss: 0.340422, acc.: 86.72%] [G loss: 4.628884]\n",
      "epoch:35 step:27963 [D loss: 0.334100, acc.: 85.16%] [G loss: 3.195794]\n",
      "epoch:35 step:27964 [D loss: 0.357201, acc.: 84.38%] [G loss: 2.882447]\n",
      "epoch:35 step:27965 [D loss: 0.376947, acc.: 82.81%] [G loss: 2.720138]\n",
      "epoch:35 step:27966 [D loss: 0.289670, acc.: 88.28%] [G loss: 2.337270]\n",
      "epoch:35 step:27967 [D loss: 0.284498, acc.: 88.28%] [G loss: 2.715523]\n",
      "epoch:35 step:27968 [D loss: 0.267305, acc.: 89.84%] [G loss: 3.629540]\n",
      "epoch:35 step:27969 [D loss: 0.341719, acc.: 82.81%] [G loss: 3.443887]\n",
      "epoch:35 step:27970 [D loss: 0.325887, acc.: 84.38%] [G loss: 2.379222]\n",
      "epoch:35 step:27971 [D loss: 0.284664, acc.: 89.06%] [G loss: 3.015681]\n",
      "epoch:35 step:27972 [D loss: 0.298012, acc.: 89.84%] [G loss: 3.008945]\n",
      "epoch:35 step:27973 [D loss: 0.268090, acc.: 89.06%] [G loss: 3.237164]\n",
      "epoch:35 step:27974 [D loss: 0.463521, acc.: 77.34%] [G loss: 3.285094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27975 [D loss: 0.326995, acc.: 86.72%] [G loss: 2.560596]\n",
      "epoch:35 step:27976 [D loss: 0.296385, acc.: 82.81%] [G loss: 3.143535]\n",
      "epoch:35 step:27977 [D loss: 0.346368, acc.: 84.38%] [G loss: 3.262041]\n",
      "epoch:35 step:27978 [D loss: 0.423677, acc.: 82.03%] [G loss: 2.900118]\n",
      "epoch:35 step:27979 [D loss: 0.405225, acc.: 75.78%] [G loss: 3.595187]\n",
      "epoch:35 step:27980 [D loss: 0.276227, acc.: 89.06%] [G loss: 5.456211]\n",
      "epoch:35 step:27981 [D loss: 0.430542, acc.: 79.69%] [G loss: 3.107590]\n",
      "epoch:35 step:27982 [D loss: 0.362984, acc.: 86.72%] [G loss: 3.510235]\n",
      "epoch:35 step:27983 [D loss: 0.450428, acc.: 82.81%] [G loss: 3.364674]\n",
      "epoch:35 step:27984 [D loss: 0.333177, acc.: 77.34%] [G loss: 4.377213]\n",
      "epoch:35 step:27985 [D loss: 0.344644, acc.: 82.81%] [G loss: 5.321717]\n",
      "epoch:35 step:27986 [D loss: 0.322873, acc.: 89.06%] [G loss: 7.057852]\n",
      "epoch:35 step:27987 [D loss: 0.229664, acc.: 90.62%] [G loss: 10.012396]\n",
      "epoch:35 step:27988 [D loss: 0.221654, acc.: 92.97%] [G loss: 5.891901]\n",
      "epoch:35 step:27989 [D loss: 0.349488, acc.: 82.03%] [G loss: 4.566752]\n",
      "epoch:35 step:27990 [D loss: 0.305906, acc.: 85.94%] [G loss: 4.326892]\n",
      "epoch:35 step:27991 [D loss: 0.200306, acc.: 89.84%] [G loss: 6.020164]\n",
      "epoch:35 step:27992 [D loss: 0.359490, acc.: 81.25%] [G loss: 4.713804]\n",
      "epoch:35 step:27993 [D loss: 0.345859, acc.: 84.38%] [G loss: 5.875135]\n",
      "epoch:35 step:27994 [D loss: 0.294862, acc.: 85.16%] [G loss: 6.030897]\n",
      "epoch:35 step:27995 [D loss: 0.200289, acc.: 89.06%] [G loss: 6.742903]\n",
      "epoch:35 step:27996 [D loss: 0.314225, acc.: 83.59%] [G loss: 4.614956]\n",
      "epoch:35 step:27997 [D loss: 0.248284, acc.: 89.06%] [G loss: 4.474899]\n",
      "epoch:35 step:27998 [D loss: 0.271957, acc.: 86.72%] [G loss: 3.892330]\n",
      "epoch:35 step:27999 [D loss: 0.257256, acc.: 88.28%] [G loss: 3.999406]\n",
      "epoch:35 step:28000 [D loss: 0.365551, acc.: 83.59%] [G loss: 3.559929]\n",
      "epoch:35 step:28001 [D loss: 0.310884, acc.: 84.38%] [G loss: 2.924923]\n",
      "epoch:35 step:28002 [D loss: 0.242795, acc.: 90.62%] [G loss: 4.765324]\n",
      "epoch:35 step:28003 [D loss: 0.291270, acc.: 88.28%] [G loss: 3.323192]\n",
      "epoch:35 step:28004 [D loss: 0.351703, acc.: 81.25%] [G loss: 3.777388]\n",
      "epoch:35 step:28005 [D loss: 0.359978, acc.: 84.38%] [G loss: 2.450327]\n",
      "epoch:35 step:28006 [D loss: 0.307327, acc.: 83.59%] [G loss: 3.413248]\n",
      "epoch:35 step:28007 [D loss: 0.387818, acc.: 86.72%] [G loss: 3.689908]\n",
      "epoch:35 step:28008 [D loss: 0.589414, acc.: 74.22%] [G loss: 5.418986]\n",
      "epoch:35 step:28009 [D loss: 0.543132, acc.: 75.00%] [G loss: 5.095741]\n",
      "epoch:35 step:28010 [D loss: 0.386311, acc.: 82.03%] [G loss: 2.926851]\n",
      "epoch:35 step:28011 [D loss: 0.367055, acc.: 83.59%] [G loss: 2.419068]\n",
      "epoch:35 step:28012 [D loss: 0.238570, acc.: 89.06%] [G loss: 2.530138]\n",
      "epoch:35 step:28013 [D loss: 0.218791, acc.: 90.62%] [G loss: 4.521442]\n",
      "epoch:35 step:28014 [D loss: 0.353213, acc.: 83.59%] [G loss: 3.156021]\n",
      "epoch:35 step:28015 [D loss: 0.350098, acc.: 80.47%] [G loss: 3.419559]\n",
      "epoch:35 step:28016 [D loss: 0.272208, acc.: 91.41%] [G loss: 3.103344]\n",
      "epoch:35 step:28017 [D loss: 0.363926, acc.: 85.16%] [G loss: 2.752618]\n",
      "epoch:35 step:28018 [D loss: 0.317124, acc.: 84.38%] [G loss: 2.775564]\n",
      "epoch:35 step:28019 [D loss: 0.391326, acc.: 79.69%] [G loss: 3.989916]\n",
      "epoch:35 step:28020 [D loss: 0.440583, acc.: 78.91%] [G loss: 3.340140]\n",
      "epoch:35 step:28021 [D loss: 0.303282, acc.: 85.16%] [G loss: 3.648492]\n",
      "epoch:35 step:28022 [D loss: 0.247846, acc.: 89.84%] [G loss: 4.808787]\n",
      "epoch:35 step:28023 [D loss: 0.263271, acc.: 88.28%] [G loss: 3.406923]\n",
      "epoch:35 step:28024 [D loss: 0.270700, acc.: 87.50%] [G loss: 3.372178]\n",
      "epoch:35 step:28025 [D loss: 0.304036, acc.: 86.72%] [G loss: 3.072786]\n",
      "epoch:35 step:28026 [D loss: 0.315270, acc.: 87.50%] [G loss: 3.606712]\n",
      "epoch:35 step:28027 [D loss: 0.395389, acc.: 82.03%] [G loss: 4.003717]\n",
      "epoch:35 step:28028 [D loss: 0.357256, acc.: 84.38%] [G loss: 3.768467]\n",
      "epoch:35 step:28029 [D loss: 0.255646, acc.: 89.06%] [G loss: 3.456561]\n",
      "epoch:35 step:28030 [D loss: 0.314653, acc.: 84.38%] [G loss: 6.563295]\n",
      "epoch:35 step:28031 [D loss: 0.432421, acc.: 78.12%] [G loss: 4.103189]\n",
      "epoch:35 step:28032 [D loss: 0.375733, acc.: 83.59%] [G loss: 3.340845]\n",
      "epoch:35 step:28033 [D loss: 0.344434, acc.: 85.94%] [G loss: 3.630446]\n",
      "epoch:35 step:28034 [D loss: 0.229122, acc.: 89.06%] [G loss: 3.433802]\n",
      "epoch:35 step:28035 [D loss: 0.255770, acc.: 89.06%] [G loss: 2.851750]\n",
      "epoch:35 step:28036 [D loss: 0.255833, acc.: 90.62%] [G loss: 2.772637]\n",
      "epoch:35 step:28037 [D loss: 0.325416, acc.: 90.62%] [G loss: 2.793861]\n",
      "epoch:35 step:28038 [D loss: 0.356050, acc.: 82.81%] [G loss: 3.664464]\n",
      "epoch:35 step:28039 [D loss: 0.377762, acc.: 79.69%] [G loss: 3.824767]\n",
      "epoch:35 step:28040 [D loss: 0.379366, acc.: 82.03%] [G loss: 2.750790]\n",
      "epoch:35 step:28041 [D loss: 0.355267, acc.: 82.03%] [G loss: 3.743568]\n",
      "epoch:35 step:28042 [D loss: 0.397291, acc.: 80.47%] [G loss: 2.553101]\n",
      "epoch:35 step:28043 [D loss: 0.281650, acc.: 90.62%] [G loss: 2.976586]\n",
      "epoch:35 step:28044 [D loss: 0.378533, acc.: 81.25%] [G loss: 3.634582]\n",
      "epoch:35 step:28045 [D loss: 0.250766, acc.: 90.62%] [G loss: 3.425044]\n",
      "epoch:35 step:28046 [D loss: 0.245598, acc.: 92.19%] [G loss: 5.450100]\n",
      "epoch:35 step:28047 [D loss: 0.468849, acc.: 78.91%] [G loss: 4.747718]\n",
      "epoch:35 step:28048 [D loss: 0.288215, acc.: 85.16%] [G loss: 4.873583]\n",
      "epoch:35 step:28049 [D loss: 0.415641, acc.: 75.78%] [G loss: 3.495652]\n",
      "epoch:35 step:28050 [D loss: 0.284488, acc.: 85.94%] [G loss: 3.556789]\n",
      "epoch:35 step:28051 [D loss: 0.363680, acc.: 84.38%] [G loss: 3.639132]\n",
      "epoch:35 step:28052 [D loss: 0.262148, acc.: 89.06%] [G loss: 3.411752]\n",
      "epoch:35 step:28053 [D loss: 0.333632, acc.: 83.59%] [G loss: 3.171660]\n",
      "epoch:35 step:28054 [D loss: 0.254866, acc.: 88.28%] [G loss: 3.763553]\n",
      "epoch:35 step:28055 [D loss: 0.317334, acc.: 83.59%] [G loss: 3.195399]\n",
      "epoch:35 step:28056 [D loss: 0.339094, acc.: 82.81%] [G loss: 3.805131]\n",
      "epoch:35 step:28057 [D loss: 0.219519, acc.: 89.84%] [G loss: 3.056272]\n",
      "epoch:35 step:28058 [D loss: 0.358602, acc.: 82.81%] [G loss: 3.318455]\n",
      "epoch:35 step:28059 [D loss: 0.221840, acc.: 90.62%] [G loss: 3.411996]\n",
      "epoch:35 step:28060 [D loss: 0.214433, acc.: 90.62%] [G loss: 3.096962]\n",
      "epoch:35 step:28061 [D loss: 0.289168, acc.: 86.72%] [G loss: 2.927219]\n",
      "epoch:35 step:28062 [D loss: 0.319366, acc.: 87.50%] [G loss: 2.958699]\n",
      "epoch:35 step:28063 [D loss: 0.344470, acc.: 85.94%] [G loss: 3.250608]\n",
      "epoch:35 step:28064 [D loss: 0.379188, acc.: 80.47%] [G loss: 3.107643]\n",
      "epoch:35 step:28065 [D loss: 0.290902, acc.: 85.16%] [G loss: 2.834261]\n",
      "epoch:35 step:28066 [D loss: 0.207466, acc.: 89.06%] [G loss: 2.906032]\n",
      "epoch:35 step:28067 [D loss: 0.306637, acc.: 87.50%] [G loss: 3.694440]\n",
      "epoch:35 step:28068 [D loss: 0.333680, acc.: 85.16%] [G loss: 2.656693]\n",
      "epoch:35 step:28069 [D loss: 0.393238, acc.: 80.47%] [G loss: 2.533567]\n",
      "epoch:35 step:28070 [D loss: 0.396641, acc.: 82.81%] [G loss: 2.298159]\n",
      "epoch:35 step:28071 [D loss: 0.272526, acc.: 89.84%] [G loss: 2.780262]\n",
      "epoch:35 step:28072 [D loss: 0.279254, acc.: 85.94%] [G loss: 2.567507]\n",
      "epoch:35 step:28073 [D loss: 0.334071, acc.: 84.38%] [G loss: 2.509120]\n",
      "epoch:35 step:28074 [D loss: 0.342317, acc.: 85.16%] [G loss: 3.352492]\n",
      "epoch:35 step:28075 [D loss: 0.319272, acc.: 81.25%] [G loss: 2.522761]\n",
      "epoch:35 step:28076 [D loss: 0.353151, acc.: 86.72%] [G loss: 3.583003]\n",
      "epoch:35 step:28077 [D loss: 0.370321, acc.: 75.78%] [G loss: 3.376219]\n",
      "epoch:35 step:28078 [D loss: 0.307348, acc.: 84.38%] [G loss: 3.286386]\n",
      "epoch:35 step:28079 [D loss: 0.316779, acc.: 82.81%] [G loss: 4.049016]\n",
      "epoch:35 step:28080 [D loss: 0.439249, acc.: 76.56%] [G loss: 3.767284]\n",
      "epoch:35 step:28081 [D loss: 0.277819, acc.: 89.06%] [G loss: 3.929740]\n",
      "epoch:35 step:28082 [D loss: 0.293973, acc.: 85.94%] [G loss: 2.739241]\n",
      "epoch:35 step:28083 [D loss: 0.325413, acc.: 82.03%] [G loss: 3.079337]\n",
      "epoch:35 step:28084 [D loss: 0.271473, acc.: 90.62%] [G loss: 3.959524]\n",
      "epoch:35 step:28085 [D loss: 0.286217, acc.: 86.72%] [G loss: 4.232174]\n",
      "epoch:35 step:28086 [D loss: 0.319487, acc.: 83.59%] [G loss: 3.324389]\n",
      "epoch:35 step:28087 [D loss: 0.245314, acc.: 87.50%] [G loss: 4.493364]\n",
      "epoch:35 step:28088 [D loss: 0.318969, acc.: 84.38%] [G loss: 2.948659]\n",
      "epoch:35 step:28089 [D loss: 0.271640, acc.: 89.84%] [G loss: 2.434947]\n",
      "epoch:35 step:28090 [D loss: 0.404671, acc.: 80.47%] [G loss: 3.555849]\n",
      "epoch:35 step:28091 [D loss: 0.335740, acc.: 86.72%] [G loss: 2.716384]\n",
      "epoch:35 step:28092 [D loss: 0.247301, acc.: 87.50%] [G loss: 2.871129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:28093 [D loss: 0.352819, acc.: 86.72%] [G loss: 2.626112]\n",
      "epoch:35 step:28094 [D loss: 0.316416, acc.: 85.16%] [G loss: 3.223084]\n",
      "epoch:35 step:28095 [D loss: 0.316633, acc.: 85.94%] [G loss: 2.765479]\n",
      "epoch:35 step:28096 [D loss: 0.208125, acc.: 92.97%] [G loss: 2.567214]\n",
      "epoch:35 step:28097 [D loss: 0.372486, acc.: 84.38%] [G loss: 3.168849]\n",
      "epoch:35 step:28098 [D loss: 0.361709, acc.: 82.03%] [G loss: 3.106171]\n",
      "epoch:35 step:28099 [D loss: 0.302278, acc.: 85.94%] [G loss: 3.404067]\n",
      "epoch:35 step:28100 [D loss: 0.383449, acc.: 82.81%] [G loss: 4.543768]\n",
      "epoch:35 step:28101 [D loss: 0.378914, acc.: 84.38%] [G loss: 3.120817]\n",
      "epoch:35 step:28102 [D loss: 0.255104, acc.: 89.84%] [G loss: 4.279780]\n",
      "epoch:35 step:28103 [D loss: 0.278531, acc.: 86.72%] [G loss: 5.225255]\n",
      "epoch:35 step:28104 [D loss: 0.244833, acc.: 85.94%] [G loss: 5.842482]\n",
      "epoch:35 step:28105 [D loss: 0.304486, acc.: 80.47%] [G loss: 2.588017]\n",
      "epoch:35 step:28106 [D loss: 0.333429, acc.: 82.81%] [G loss: 3.435012]\n",
      "epoch:35 step:28107 [D loss: 0.211581, acc.: 90.62%] [G loss: 3.232849]\n",
      "epoch:35 step:28108 [D loss: 0.300666, acc.: 86.72%] [G loss: 3.292692]\n",
      "epoch:35 step:28109 [D loss: 0.240312, acc.: 90.62%] [G loss: 3.742732]\n",
      "epoch:35 step:28110 [D loss: 0.350546, acc.: 82.03%] [G loss: 3.916627]\n",
      "epoch:35 step:28111 [D loss: 0.416330, acc.: 82.03%] [G loss: 3.747277]\n",
      "epoch:35 step:28112 [D loss: 0.333553, acc.: 82.81%] [G loss: 4.052569]\n",
      "epoch:35 step:28113 [D loss: 0.295437, acc.: 85.94%] [G loss: 4.716677]\n",
      "epoch:35 step:28114 [D loss: 0.294366, acc.: 88.28%] [G loss: 5.070422]\n",
      "epoch:35 step:28115 [D loss: 0.316476, acc.: 83.59%] [G loss: 4.336865]\n",
      "epoch:35 step:28116 [D loss: 0.396774, acc.: 83.59%] [G loss: 3.482196]\n",
      "epoch:36 step:28117 [D loss: 0.305150, acc.: 84.38%] [G loss: 3.519625]\n",
      "epoch:36 step:28118 [D loss: 0.182290, acc.: 95.31%] [G loss: 3.104143]\n",
      "epoch:36 step:28119 [D loss: 0.420115, acc.: 80.47%] [G loss: 2.972751]\n",
      "epoch:36 step:28120 [D loss: 0.365690, acc.: 82.03%] [G loss: 3.361361]\n",
      "epoch:36 step:28121 [D loss: 0.377164, acc.: 81.25%] [G loss: 3.599281]\n",
      "epoch:36 step:28122 [D loss: 0.365197, acc.: 84.38%] [G loss: 4.176670]\n",
      "epoch:36 step:28123 [D loss: 0.383278, acc.: 81.25%] [G loss: 3.016339]\n",
      "epoch:36 step:28124 [D loss: 0.327949, acc.: 85.94%] [G loss: 2.870363]\n",
      "epoch:36 step:28125 [D loss: 0.386663, acc.: 82.81%] [G loss: 2.281131]\n",
      "epoch:36 step:28126 [D loss: 0.372885, acc.: 79.69%] [G loss: 3.399180]\n",
      "epoch:36 step:28127 [D loss: 0.334203, acc.: 88.28%] [G loss: 2.727626]\n",
      "epoch:36 step:28128 [D loss: 0.242216, acc.: 89.06%] [G loss: 3.625799]\n",
      "epoch:36 step:28129 [D loss: 0.328754, acc.: 84.38%] [G loss: 4.247015]\n",
      "epoch:36 step:28130 [D loss: 0.336728, acc.: 80.47%] [G loss: 2.875337]\n",
      "epoch:36 step:28131 [D loss: 0.303125, acc.: 87.50%] [G loss: 4.326520]\n",
      "epoch:36 step:28132 [D loss: 0.255773, acc.: 86.72%] [G loss: 4.198817]\n",
      "epoch:36 step:28133 [D loss: 0.373939, acc.: 81.25%] [G loss: 4.185372]\n",
      "epoch:36 step:28134 [D loss: 0.358393, acc.: 85.16%] [G loss: 3.133582]\n",
      "epoch:36 step:28135 [D loss: 0.309685, acc.: 88.28%] [G loss: 3.687302]\n",
      "epoch:36 step:28136 [D loss: 0.386945, acc.: 77.34%] [G loss: 3.902952]\n",
      "epoch:36 step:28137 [D loss: 0.429739, acc.: 82.81%] [G loss: 3.109104]\n",
      "epoch:36 step:28138 [D loss: 0.465849, acc.: 75.78%] [G loss: 3.393748]\n",
      "epoch:36 step:28139 [D loss: 0.292576, acc.: 89.06%] [G loss: 3.312664]\n",
      "epoch:36 step:28140 [D loss: 0.420854, acc.: 79.69%] [G loss: 3.164478]\n",
      "epoch:36 step:28141 [D loss: 0.300259, acc.: 85.94%] [G loss: 2.973189]\n",
      "epoch:36 step:28142 [D loss: 0.378980, acc.: 80.47%] [G loss: 2.979775]\n",
      "epoch:36 step:28143 [D loss: 0.347293, acc.: 82.81%] [G loss: 3.511841]\n",
      "epoch:36 step:28144 [D loss: 0.522466, acc.: 77.34%] [G loss: 2.786937]\n",
      "epoch:36 step:28145 [D loss: 0.315070, acc.: 85.94%] [G loss: 2.247870]\n",
      "epoch:36 step:28146 [D loss: 0.274990, acc.: 89.06%] [G loss: 2.817593]\n",
      "epoch:36 step:28147 [D loss: 0.270928, acc.: 86.72%] [G loss: 3.259190]\n",
      "epoch:36 step:28148 [D loss: 0.427922, acc.: 78.91%] [G loss: 3.763024]\n",
      "epoch:36 step:28149 [D loss: 0.338463, acc.: 83.59%] [G loss: 2.777209]\n",
      "epoch:36 step:28150 [D loss: 0.274403, acc.: 87.50%] [G loss: 4.215356]\n",
      "epoch:36 step:28151 [D loss: 0.223183, acc.: 92.19%] [G loss: 4.220933]\n",
      "epoch:36 step:28152 [D loss: 0.214236, acc.: 91.41%] [G loss: 3.168032]\n",
      "epoch:36 step:28153 [D loss: 0.245635, acc.: 89.84%] [G loss: 3.441598]\n",
      "epoch:36 step:28154 [D loss: 0.313417, acc.: 84.38%] [G loss: 2.736287]\n",
      "epoch:36 step:28155 [D loss: 0.240873, acc.: 91.41%] [G loss: 2.900537]\n",
      "epoch:36 step:28156 [D loss: 0.288565, acc.: 88.28%] [G loss: 3.573661]\n",
      "epoch:36 step:28157 [D loss: 0.344295, acc.: 82.03%] [G loss: 3.277239]\n",
      "epoch:36 step:28158 [D loss: 0.291907, acc.: 88.28%] [G loss: 3.255599]\n",
      "epoch:36 step:28159 [D loss: 0.295518, acc.: 87.50%] [G loss: 4.212131]\n",
      "epoch:36 step:28160 [D loss: 0.310904, acc.: 85.94%] [G loss: 4.147384]\n",
      "epoch:36 step:28161 [D loss: 0.265091, acc.: 85.94%] [G loss: 4.346437]\n",
      "epoch:36 step:28162 [D loss: 0.305256, acc.: 89.06%] [G loss: 2.916530]\n",
      "epoch:36 step:28163 [D loss: 0.295113, acc.: 85.94%] [G loss: 3.954065]\n",
      "epoch:36 step:28164 [D loss: 0.320576, acc.: 85.94%] [G loss: 3.225416]\n",
      "epoch:36 step:28165 [D loss: 0.204065, acc.: 91.41%] [G loss: 3.660509]\n",
      "epoch:36 step:28166 [D loss: 0.313670, acc.: 84.38%] [G loss: 3.142226]\n",
      "epoch:36 step:28167 [D loss: 0.441950, acc.: 77.34%] [G loss: 3.096924]\n",
      "epoch:36 step:28168 [D loss: 0.298760, acc.: 86.72%] [G loss: 3.553500]\n",
      "epoch:36 step:28169 [D loss: 0.324194, acc.: 83.59%] [G loss: 3.886429]\n",
      "epoch:36 step:28170 [D loss: 0.245040, acc.: 90.62%] [G loss: 3.046559]\n",
      "epoch:36 step:28171 [D loss: 0.236542, acc.: 92.19%] [G loss: 4.994150]\n",
      "epoch:36 step:28172 [D loss: 0.312787, acc.: 89.06%] [G loss: 3.356641]\n",
      "epoch:36 step:28173 [D loss: 0.289651, acc.: 87.50%] [G loss: 3.969534]\n",
      "epoch:36 step:28174 [D loss: 0.314959, acc.: 85.16%] [G loss: 3.679842]\n",
      "epoch:36 step:28175 [D loss: 0.235569, acc.: 89.06%] [G loss: 2.710084]\n",
      "epoch:36 step:28176 [D loss: 0.264360, acc.: 90.62%] [G loss: 3.540147]\n",
      "epoch:36 step:28177 [D loss: 0.262132, acc.: 90.62%] [G loss: 2.985271]\n",
      "epoch:36 step:28178 [D loss: 0.310803, acc.: 82.03%] [G loss: 3.569780]\n",
      "epoch:36 step:28179 [D loss: 0.270934, acc.: 90.62%] [G loss: 2.912806]\n",
      "epoch:36 step:28180 [D loss: 0.416703, acc.: 81.25%] [G loss: 4.058248]\n",
      "epoch:36 step:28181 [D loss: 0.506119, acc.: 75.00%] [G loss: 3.748072]\n",
      "epoch:36 step:28182 [D loss: 0.476431, acc.: 78.12%] [G loss: 5.358652]\n",
      "epoch:36 step:28183 [D loss: 0.717356, acc.: 70.31%] [G loss: 5.979060]\n",
      "epoch:36 step:28184 [D loss: 1.139902, acc.: 68.75%] [G loss: 8.582545]\n",
      "epoch:36 step:28185 [D loss: 1.864108, acc.: 60.16%] [G loss: 2.966517]\n",
      "epoch:36 step:28186 [D loss: 0.283775, acc.: 89.06%] [G loss: 3.435464]\n",
      "epoch:36 step:28187 [D loss: 0.468361, acc.: 79.69%] [G loss: 2.443903]\n",
      "epoch:36 step:28188 [D loss: 0.338808, acc.: 85.16%] [G loss: 4.656649]\n",
      "epoch:36 step:28189 [D loss: 0.497271, acc.: 80.47%] [G loss: 2.475325]\n",
      "epoch:36 step:28190 [D loss: 0.249876, acc.: 86.72%] [G loss: 4.400760]\n",
      "epoch:36 step:28191 [D loss: 0.461592, acc.: 78.91%] [G loss: 3.641514]\n",
      "epoch:36 step:28192 [D loss: 0.355903, acc.: 82.81%] [G loss: 2.695323]\n",
      "epoch:36 step:28193 [D loss: 0.430121, acc.: 78.12%] [G loss: 3.543351]\n",
      "epoch:36 step:28194 [D loss: 0.350355, acc.: 84.38%] [G loss: 3.338665]\n",
      "epoch:36 step:28195 [D loss: 0.394833, acc.: 82.03%] [G loss: 2.418772]\n",
      "epoch:36 step:28196 [D loss: 0.307922, acc.: 85.94%] [G loss: 3.025266]\n",
      "epoch:36 step:28197 [D loss: 0.388798, acc.: 81.25%] [G loss: 2.780888]\n",
      "epoch:36 step:28198 [D loss: 0.371743, acc.: 82.03%] [G loss: 3.011199]\n",
      "epoch:36 step:28199 [D loss: 0.270072, acc.: 91.41%] [G loss: 2.796910]\n",
      "epoch:36 step:28200 [D loss: 0.186729, acc.: 96.09%] [G loss: 2.688688]\n",
      "epoch:36 step:28201 [D loss: 0.243661, acc.: 92.97%] [G loss: 2.501702]\n",
      "epoch:36 step:28202 [D loss: 0.397223, acc.: 84.38%] [G loss: 2.455578]\n",
      "epoch:36 step:28203 [D loss: 0.290332, acc.: 87.50%] [G loss: 2.865836]\n",
      "epoch:36 step:28204 [D loss: 0.276626, acc.: 89.06%] [G loss: 3.031487]\n",
      "epoch:36 step:28205 [D loss: 0.396307, acc.: 82.81%] [G loss: 2.425993]\n",
      "epoch:36 step:28206 [D loss: 0.335372, acc.: 85.16%] [G loss: 2.467317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28207 [D loss: 0.284758, acc.: 89.06%] [G loss: 2.870076]\n",
      "epoch:36 step:28208 [D loss: 0.319151, acc.: 86.72%] [G loss: 2.891260]\n",
      "epoch:36 step:28209 [D loss: 0.292190, acc.: 86.72%] [G loss: 2.815573]\n",
      "epoch:36 step:28210 [D loss: 0.253586, acc.: 92.97%] [G loss: 2.818387]\n",
      "epoch:36 step:28211 [D loss: 0.389175, acc.: 82.03%] [G loss: 2.586708]\n",
      "epoch:36 step:28212 [D loss: 0.372131, acc.: 84.38%] [G loss: 2.912132]\n",
      "epoch:36 step:28213 [D loss: 0.325953, acc.: 84.38%] [G loss: 3.342823]\n",
      "epoch:36 step:28214 [D loss: 0.400956, acc.: 82.81%] [G loss: 2.400283]\n",
      "epoch:36 step:28215 [D loss: 0.308011, acc.: 85.94%] [G loss: 2.719904]\n",
      "epoch:36 step:28216 [D loss: 0.297333, acc.: 89.06%] [G loss: 2.773175]\n",
      "epoch:36 step:28217 [D loss: 0.371706, acc.: 78.12%] [G loss: 4.800310]\n",
      "epoch:36 step:28218 [D loss: 0.348752, acc.: 82.81%] [G loss: 3.577402]\n",
      "epoch:36 step:28219 [D loss: 0.334971, acc.: 84.38%] [G loss: 3.650908]\n",
      "epoch:36 step:28220 [D loss: 0.303811, acc.: 85.16%] [G loss: 3.249427]\n",
      "epoch:36 step:28221 [D loss: 0.261954, acc.: 89.06%] [G loss: 2.576589]\n",
      "epoch:36 step:28222 [D loss: 0.310772, acc.: 85.16%] [G loss: 2.262924]\n",
      "epoch:36 step:28223 [D loss: 0.368439, acc.: 82.81%] [G loss: 2.765462]\n",
      "epoch:36 step:28224 [D loss: 0.256701, acc.: 89.84%] [G loss: 4.815127]\n",
      "epoch:36 step:28225 [D loss: 0.339188, acc.: 82.81%] [G loss: 3.140308]\n",
      "epoch:36 step:28226 [D loss: 0.357119, acc.: 82.81%] [G loss: 4.053401]\n",
      "epoch:36 step:28227 [D loss: 0.373285, acc.: 82.81%] [G loss: 3.466837]\n",
      "epoch:36 step:28228 [D loss: 0.325665, acc.: 87.50%] [G loss: 2.997713]\n",
      "epoch:36 step:28229 [D loss: 0.338840, acc.: 84.38%] [G loss: 2.841536]\n",
      "epoch:36 step:28230 [D loss: 0.274059, acc.: 89.06%] [G loss: 3.912550]\n",
      "epoch:36 step:28231 [D loss: 0.331762, acc.: 85.16%] [G loss: 3.274026]\n",
      "epoch:36 step:28232 [D loss: 0.355760, acc.: 83.59%] [G loss: 4.706522]\n",
      "epoch:36 step:28233 [D loss: 0.293027, acc.: 87.50%] [G loss: 3.240974]\n",
      "epoch:36 step:28234 [D loss: 0.275222, acc.: 87.50%] [G loss: 3.174849]\n",
      "epoch:36 step:28235 [D loss: 0.299551, acc.: 86.72%] [G loss: 3.643032]\n",
      "epoch:36 step:28236 [D loss: 0.392552, acc.: 82.03%] [G loss: 3.355496]\n",
      "epoch:36 step:28237 [D loss: 0.303666, acc.: 82.81%] [G loss: 4.216137]\n",
      "epoch:36 step:28238 [D loss: 0.301095, acc.: 87.50%] [G loss: 4.242465]\n",
      "epoch:36 step:28239 [D loss: 0.317463, acc.: 85.94%] [G loss: 3.905914]\n",
      "epoch:36 step:28240 [D loss: 0.331526, acc.: 86.72%] [G loss: 4.229999]\n",
      "epoch:36 step:28241 [D loss: 0.353159, acc.: 85.94%] [G loss: 3.338707]\n",
      "epoch:36 step:28242 [D loss: 0.285834, acc.: 88.28%] [G loss: 4.489566]\n",
      "epoch:36 step:28243 [D loss: 0.322197, acc.: 87.50%] [G loss: 3.539473]\n",
      "epoch:36 step:28244 [D loss: 0.286465, acc.: 89.84%] [G loss: 3.208243]\n",
      "epoch:36 step:28245 [D loss: 0.302761, acc.: 85.94%] [G loss: 3.223831]\n",
      "epoch:36 step:28246 [D loss: 0.290259, acc.: 87.50%] [G loss: 3.678144]\n",
      "epoch:36 step:28247 [D loss: 0.292704, acc.: 86.72%] [G loss: 4.517830]\n",
      "epoch:36 step:28248 [D loss: 0.287933, acc.: 89.84%] [G loss: 3.774885]\n",
      "epoch:36 step:28249 [D loss: 0.258811, acc.: 90.62%] [G loss: 3.498283]\n",
      "epoch:36 step:28250 [D loss: 0.369659, acc.: 85.16%] [G loss: 3.499218]\n",
      "epoch:36 step:28251 [D loss: 0.284238, acc.: 85.94%] [G loss: 3.586843]\n",
      "epoch:36 step:28252 [D loss: 0.210121, acc.: 91.41%] [G loss: 2.912935]\n",
      "epoch:36 step:28253 [D loss: 0.372467, acc.: 82.81%] [G loss: 3.252113]\n",
      "epoch:36 step:28254 [D loss: 0.326631, acc.: 84.38%] [G loss: 2.705669]\n",
      "epoch:36 step:28255 [D loss: 0.356947, acc.: 85.16%] [G loss: 2.526171]\n",
      "epoch:36 step:28256 [D loss: 0.277586, acc.: 88.28%] [G loss: 2.505000]\n",
      "epoch:36 step:28257 [D loss: 0.366224, acc.: 78.91%] [G loss: 2.972773]\n",
      "epoch:36 step:28258 [D loss: 0.416539, acc.: 81.25%] [G loss: 2.355153]\n",
      "epoch:36 step:28259 [D loss: 0.257607, acc.: 85.16%] [G loss: 2.807610]\n",
      "epoch:36 step:28260 [D loss: 0.337326, acc.: 84.38%] [G loss: 2.999736]\n",
      "epoch:36 step:28261 [D loss: 0.269658, acc.: 87.50%] [G loss: 3.822465]\n",
      "epoch:36 step:28262 [D loss: 0.250453, acc.: 90.62%] [G loss: 3.112632]\n",
      "epoch:36 step:28263 [D loss: 0.327864, acc.: 84.38%] [G loss: 4.944233]\n",
      "epoch:36 step:28264 [D loss: 0.299605, acc.: 85.16%] [G loss: 4.693775]\n",
      "epoch:36 step:28265 [D loss: 0.244227, acc.: 88.28%] [G loss: 5.245942]\n",
      "epoch:36 step:28266 [D loss: 0.223646, acc.: 89.84%] [G loss: 2.845887]\n",
      "epoch:36 step:28267 [D loss: 0.221091, acc.: 90.62%] [G loss: 3.654142]\n",
      "epoch:36 step:28268 [D loss: 0.319760, acc.: 83.59%] [G loss: 3.095502]\n",
      "epoch:36 step:28269 [D loss: 0.336278, acc.: 85.16%] [G loss: 3.161567]\n",
      "epoch:36 step:28270 [D loss: 0.256060, acc.: 90.62%] [G loss: 3.189444]\n",
      "epoch:36 step:28271 [D loss: 0.386338, acc.: 84.38%] [G loss: 2.889000]\n",
      "epoch:36 step:28272 [D loss: 0.304511, acc.: 87.50%] [G loss: 3.010774]\n",
      "epoch:36 step:28273 [D loss: 0.335368, acc.: 83.59%] [G loss: 3.507771]\n",
      "epoch:36 step:28274 [D loss: 0.313312, acc.: 85.94%] [G loss: 3.526098]\n",
      "epoch:36 step:28275 [D loss: 0.349191, acc.: 87.50%] [G loss: 3.148455]\n",
      "epoch:36 step:28276 [D loss: 0.395568, acc.: 82.81%] [G loss: 3.341640]\n",
      "epoch:36 step:28277 [D loss: 0.447990, acc.: 78.12%] [G loss: 4.199718]\n",
      "epoch:36 step:28278 [D loss: 0.358629, acc.: 85.16%] [G loss: 4.000456]\n",
      "epoch:36 step:28279 [D loss: 0.321670, acc.: 86.72%] [G loss: 3.900339]\n",
      "epoch:36 step:28280 [D loss: 0.289245, acc.: 85.94%] [G loss: 3.651046]\n",
      "epoch:36 step:28281 [D loss: 0.257602, acc.: 86.72%] [G loss: 4.554898]\n",
      "epoch:36 step:28282 [D loss: 0.300756, acc.: 84.38%] [G loss: 3.832271]\n",
      "epoch:36 step:28283 [D loss: 0.338122, acc.: 85.94%] [G loss: 5.750966]\n",
      "epoch:36 step:28284 [D loss: 0.441232, acc.: 80.47%] [G loss: 3.057061]\n",
      "epoch:36 step:28285 [D loss: 0.332994, acc.: 84.38%] [G loss: 3.390345]\n",
      "epoch:36 step:28286 [D loss: 0.291445, acc.: 86.72%] [G loss: 3.743822]\n",
      "epoch:36 step:28287 [D loss: 0.345640, acc.: 85.94%] [G loss: 3.450380]\n",
      "epoch:36 step:28288 [D loss: 0.313349, acc.: 85.16%] [G loss: 3.164611]\n",
      "epoch:36 step:28289 [D loss: 0.474968, acc.: 78.12%] [G loss: 2.903249]\n",
      "epoch:36 step:28290 [D loss: 0.378175, acc.: 80.47%] [G loss: 2.756316]\n",
      "epoch:36 step:28291 [D loss: 0.389614, acc.: 83.59%] [G loss: 3.610661]\n",
      "epoch:36 step:28292 [D loss: 0.352909, acc.: 83.59%] [G loss: 2.627156]\n",
      "epoch:36 step:28293 [D loss: 0.325877, acc.: 85.94%] [G loss: 3.742922]\n",
      "epoch:36 step:28294 [D loss: 0.326377, acc.: 85.16%] [G loss: 3.454758]\n",
      "epoch:36 step:28295 [D loss: 0.425741, acc.: 80.47%] [G loss: 2.987202]\n",
      "epoch:36 step:28296 [D loss: 0.280454, acc.: 88.28%] [G loss: 3.978817]\n",
      "epoch:36 step:28297 [D loss: 0.286068, acc.: 82.81%] [G loss: 3.323814]\n",
      "epoch:36 step:28298 [D loss: 0.330872, acc.: 82.81%] [G loss: 4.734912]\n",
      "epoch:36 step:28299 [D loss: 0.362277, acc.: 84.38%] [G loss: 3.926712]\n",
      "epoch:36 step:28300 [D loss: 0.379059, acc.: 82.81%] [G loss: 2.838765]\n",
      "epoch:36 step:28301 [D loss: 0.337999, acc.: 83.59%] [G loss: 3.107082]\n",
      "epoch:36 step:28302 [D loss: 0.277376, acc.: 87.50%] [G loss: 3.768255]\n",
      "epoch:36 step:28303 [D loss: 0.295471, acc.: 89.84%] [G loss: 2.644377]\n",
      "epoch:36 step:28304 [D loss: 0.355127, acc.: 85.94%] [G loss: 3.124658]\n",
      "epoch:36 step:28305 [D loss: 0.386749, acc.: 82.03%] [G loss: 3.206245]\n",
      "epoch:36 step:28306 [D loss: 0.341398, acc.: 82.03%] [G loss: 3.772929]\n",
      "epoch:36 step:28307 [D loss: 0.423115, acc.: 78.91%] [G loss: 2.766262]\n",
      "epoch:36 step:28308 [D loss: 0.220648, acc.: 89.84%] [G loss: 3.552765]\n",
      "epoch:36 step:28309 [D loss: 0.380878, acc.: 81.25%] [G loss: 2.797453]\n",
      "epoch:36 step:28310 [D loss: 0.305732, acc.: 84.38%] [G loss: 3.297914]\n",
      "epoch:36 step:28311 [D loss: 0.291676, acc.: 87.50%] [G loss: 2.731671]\n",
      "epoch:36 step:28312 [D loss: 0.356673, acc.: 85.16%] [G loss: 3.289061]\n",
      "epoch:36 step:28313 [D loss: 0.364595, acc.: 81.25%] [G loss: 3.414595]\n",
      "epoch:36 step:28314 [D loss: 0.624394, acc.: 72.66%] [G loss: 4.637706]\n",
      "epoch:36 step:28315 [D loss: 0.345418, acc.: 85.16%] [G loss: 2.891637]\n",
      "epoch:36 step:28316 [D loss: 0.347333, acc.: 83.59%] [G loss: 3.578115]\n",
      "epoch:36 step:28317 [D loss: 0.428934, acc.: 75.00%] [G loss: 3.095253]\n",
      "epoch:36 step:28318 [D loss: 0.402562, acc.: 82.03%] [G loss: 3.548230]\n",
      "epoch:36 step:28319 [D loss: 0.315032, acc.: 85.16%] [G loss: 3.260419]\n",
      "epoch:36 step:28320 [D loss: 0.375727, acc.: 81.25%] [G loss: 4.147879]\n",
      "epoch:36 step:28321 [D loss: 0.248372, acc.: 87.50%] [G loss: 3.557776]\n",
      "epoch:36 step:28322 [D loss: 0.292525, acc.: 89.84%] [G loss: 3.110401]\n",
      "epoch:36 step:28323 [D loss: 0.310235, acc.: 86.72%] [G loss: 3.395164]\n",
      "epoch:36 step:28324 [D loss: 0.376083, acc.: 82.81%] [G loss: 2.925343]\n",
      "epoch:36 step:28325 [D loss: 0.465195, acc.: 78.91%] [G loss: 3.475240]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28326 [D loss: 0.300935, acc.: 85.94%] [G loss: 3.643604]\n",
      "epoch:36 step:28327 [D loss: 0.397348, acc.: 80.47%] [G loss: 2.938591]\n",
      "epoch:36 step:28328 [D loss: 0.345819, acc.: 85.94%] [G loss: 3.755807]\n",
      "epoch:36 step:28329 [D loss: 0.267877, acc.: 88.28%] [G loss: 3.502479]\n",
      "epoch:36 step:28330 [D loss: 0.378827, acc.: 83.59%] [G loss: 2.249063]\n",
      "epoch:36 step:28331 [D loss: 0.217826, acc.: 92.97%] [G loss: 3.747463]\n",
      "epoch:36 step:28332 [D loss: 0.396098, acc.: 80.47%] [G loss: 3.817608]\n",
      "epoch:36 step:28333 [D loss: 0.245634, acc.: 92.97%] [G loss: 3.009009]\n",
      "epoch:36 step:28334 [D loss: 0.261838, acc.: 85.94%] [G loss: 3.227278]\n",
      "epoch:36 step:28335 [D loss: 0.239883, acc.: 89.06%] [G loss: 3.139181]\n",
      "epoch:36 step:28336 [D loss: 0.308150, acc.: 89.84%] [G loss: 4.257096]\n",
      "epoch:36 step:28337 [D loss: 0.401125, acc.: 82.03%] [G loss: 2.572243]\n",
      "epoch:36 step:28338 [D loss: 0.309693, acc.: 89.06%] [G loss: 3.181287]\n",
      "epoch:36 step:28339 [D loss: 0.296193, acc.: 89.06%] [G loss: 3.917603]\n",
      "epoch:36 step:28340 [D loss: 0.275823, acc.: 90.62%] [G loss: 3.221147]\n",
      "epoch:36 step:28341 [D loss: 0.425484, acc.: 80.47%] [G loss: 3.520876]\n",
      "epoch:36 step:28342 [D loss: 0.464502, acc.: 78.12%] [G loss: 2.937452]\n",
      "epoch:36 step:28343 [D loss: 0.257746, acc.: 90.62%] [G loss: 4.309374]\n",
      "epoch:36 step:28344 [D loss: 0.343324, acc.: 87.50%] [G loss: 3.337917]\n",
      "epoch:36 step:28345 [D loss: 0.302450, acc.: 85.16%] [G loss: 4.007424]\n",
      "epoch:36 step:28346 [D loss: 0.326376, acc.: 85.16%] [G loss: 4.278944]\n",
      "epoch:36 step:28347 [D loss: 0.225539, acc.: 86.72%] [G loss: 5.224519]\n",
      "epoch:36 step:28348 [D loss: 0.328668, acc.: 84.38%] [G loss: 4.447381]\n",
      "epoch:36 step:28349 [D loss: 0.225183, acc.: 89.84%] [G loss: 3.157303]\n",
      "epoch:36 step:28350 [D loss: 0.243751, acc.: 90.62%] [G loss: 7.363837]\n",
      "epoch:36 step:28351 [D loss: 0.230853, acc.: 89.84%] [G loss: 5.145842]\n",
      "epoch:36 step:28352 [D loss: 0.210858, acc.: 90.62%] [G loss: 4.421000]\n",
      "epoch:36 step:28353 [D loss: 0.180424, acc.: 93.75%] [G loss: 4.371759]\n",
      "epoch:36 step:28354 [D loss: 0.233580, acc.: 92.19%] [G loss: 4.268739]\n",
      "epoch:36 step:28355 [D loss: 0.427444, acc.: 81.25%] [G loss: 3.358655]\n",
      "epoch:36 step:28356 [D loss: 0.341265, acc.: 83.59%] [G loss: 4.371975]\n",
      "epoch:36 step:28357 [D loss: 0.249582, acc.: 90.62%] [G loss: 4.380978]\n",
      "epoch:36 step:28358 [D loss: 0.352268, acc.: 82.03%] [G loss: 3.793123]\n",
      "epoch:36 step:28359 [D loss: 0.255197, acc.: 87.50%] [G loss: 6.444165]\n",
      "epoch:36 step:28360 [D loss: 0.242495, acc.: 90.62%] [G loss: 4.349964]\n",
      "epoch:36 step:28361 [D loss: 0.237186, acc.: 89.84%] [G loss: 6.655845]\n",
      "epoch:36 step:28362 [D loss: 0.314008, acc.: 82.81%] [G loss: 3.918061]\n",
      "epoch:36 step:28363 [D loss: 0.319457, acc.: 82.03%] [G loss: 3.097081]\n",
      "epoch:36 step:28364 [D loss: 0.257281, acc.: 87.50%] [G loss: 3.565972]\n",
      "epoch:36 step:28365 [D loss: 0.440942, acc.: 80.47%] [G loss: 2.999127]\n",
      "epoch:36 step:28366 [D loss: 0.422361, acc.: 79.69%] [G loss: 3.326334]\n",
      "epoch:36 step:28367 [D loss: 0.318275, acc.: 85.94%] [G loss: 3.735173]\n",
      "epoch:36 step:28368 [D loss: 0.418967, acc.: 75.00%] [G loss: 4.026178]\n",
      "epoch:36 step:28369 [D loss: 0.288681, acc.: 89.06%] [G loss: 4.349005]\n",
      "epoch:36 step:28370 [D loss: 0.260194, acc.: 89.84%] [G loss: 3.173222]\n",
      "epoch:36 step:28371 [D loss: 0.258742, acc.: 91.41%] [G loss: 5.315231]\n",
      "epoch:36 step:28372 [D loss: 0.433807, acc.: 75.78%] [G loss: 3.146830]\n",
      "epoch:36 step:28373 [D loss: 0.312257, acc.: 84.38%] [G loss: 3.296616]\n",
      "epoch:36 step:28374 [D loss: 0.355086, acc.: 77.34%] [G loss: 3.351435]\n",
      "epoch:36 step:28375 [D loss: 0.260063, acc.: 86.72%] [G loss: 4.965827]\n",
      "epoch:36 step:28376 [D loss: 0.259067, acc.: 89.84%] [G loss: 3.981610]\n",
      "epoch:36 step:28377 [D loss: 0.440730, acc.: 80.47%] [G loss: 5.182439]\n",
      "epoch:36 step:28378 [D loss: 0.381070, acc.: 79.69%] [G loss: 3.782898]\n",
      "epoch:36 step:28379 [D loss: 0.243678, acc.: 88.28%] [G loss: 4.021206]\n",
      "epoch:36 step:28380 [D loss: 0.387928, acc.: 80.47%] [G loss: 2.748350]\n",
      "epoch:36 step:28381 [D loss: 0.269952, acc.: 91.41%] [G loss: 3.400142]\n",
      "epoch:36 step:28382 [D loss: 0.315600, acc.: 87.50%] [G loss: 3.350057]\n",
      "epoch:36 step:28383 [D loss: 0.244718, acc.: 87.50%] [G loss: 3.720218]\n",
      "epoch:36 step:28384 [D loss: 0.263329, acc.: 88.28%] [G loss: 2.168491]\n",
      "epoch:36 step:28385 [D loss: 0.298845, acc.: 86.72%] [G loss: 3.246338]\n",
      "epoch:36 step:28386 [D loss: 0.295010, acc.: 90.62%] [G loss: 3.371871]\n",
      "epoch:36 step:28387 [D loss: 0.289541, acc.: 85.94%] [G loss: 3.404676]\n",
      "epoch:36 step:28388 [D loss: 0.306395, acc.: 85.94%] [G loss: 3.261295]\n",
      "epoch:36 step:28389 [D loss: 0.279471, acc.: 88.28%] [G loss: 3.019140]\n",
      "epoch:36 step:28390 [D loss: 0.325901, acc.: 86.72%] [G loss: 2.867532]\n",
      "epoch:36 step:28391 [D loss: 0.410080, acc.: 82.81%] [G loss: 3.613702]\n",
      "epoch:36 step:28392 [D loss: 0.520301, acc.: 77.34%] [G loss: 3.388947]\n",
      "epoch:36 step:28393 [D loss: 0.359486, acc.: 83.59%] [G loss: 2.573867]\n",
      "epoch:36 step:28394 [D loss: 0.243710, acc.: 89.06%] [G loss: 4.315830]\n",
      "epoch:36 step:28395 [D loss: 0.300167, acc.: 87.50%] [G loss: 3.081856]\n",
      "epoch:36 step:28396 [D loss: 0.254605, acc.: 91.41%] [G loss: 3.199535]\n",
      "epoch:36 step:28397 [D loss: 0.248955, acc.: 86.72%] [G loss: 2.668840]\n",
      "epoch:36 step:28398 [D loss: 0.340121, acc.: 85.94%] [G loss: 3.350986]\n",
      "epoch:36 step:28399 [D loss: 0.254900, acc.: 90.62%] [G loss: 3.827860]\n",
      "epoch:36 step:28400 [D loss: 0.296008, acc.: 87.50%] [G loss: 3.462210]\n",
      "epoch:36 step:28401 [D loss: 0.416047, acc.: 78.91%] [G loss: 2.496734]\n",
      "epoch:36 step:28402 [D loss: 0.478980, acc.: 81.25%] [G loss: 2.420087]\n",
      "epoch:36 step:28403 [D loss: 0.361387, acc.: 78.91%] [G loss: 2.031424]\n",
      "epoch:36 step:28404 [D loss: 0.264391, acc.: 89.84%] [G loss: 3.626998]\n",
      "epoch:36 step:28405 [D loss: 0.294204, acc.: 87.50%] [G loss: 3.483807]\n",
      "epoch:36 step:28406 [D loss: 0.284940, acc.: 87.50%] [G loss: 3.534328]\n",
      "epoch:36 step:28407 [D loss: 0.309217, acc.: 85.94%] [G loss: 3.690038]\n",
      "epoch:36 step:28408 [D loss: 0.270455, acc.: 87.50%] [G loss: 2.790140]\n",
      "epoch:36 step:28409 [D loss: 0.431881, acc.: 75.00%] [G loss: 2.812443]\n",
      "epoch:36 step:28410 [D loss: 0.213470, acc.: 93.75%] [G loss: 3.567596]\n",
      "epoch:36 step:28411 [D loss: 0.437455, acc.: 79.69%] [G loss: 2.990613]\n",
      "epoch:36 step:28412 [D loss: 0.312418, acc.: 87.50%] [G loss: 2.827227]\n",
      "epoch:36 step:28413 [D loss: 0.328050, acc.: 85.16%] [G loss: 2.967761]\n",
      "epoch:36 step:28414 [D loss: 0.321580, acc.: 86.72%] [G loss: 3.350464]\n",
      "epoch:36 step:28415 [D loss: 0.404385, acc.: 82.81%] [G loss: 3.070814]\n",
      "epoch:36 step:28416 [D loss: 0.383046, acc.: 81.25%] [G loss: 4.555688]\n",
      "epoch:36 step:28417 [D loss: 0.323687, acc.: 85.94%] [G loss: 5.244173]\n",
      "epoch:36 step:28418 [D loss: 0.331270, acc.: 88.28%] [G loss: 3.411344]\n",
      "epoch:36 step:28419 [D loss: 0.204134, acc.: 91.41%] [G loss: 4.771600]\n",
      "epoch:36 step:28420 [D loss: 0.306699, acc.: 85.16%] [G loss: 2.760358]\n",
      "epoch:36 step:28421 [D loss: 0.282749, acc.: 86.72%] [G loss: 3.057392]\n",
      "epoch:36 step:28422 [D loss: 0.376127, acc.: 82.81%] [G loss: 3.631568]\n",
      "epoch:36 step:28423 [D loss: 0.344766, acc.: 84.38%] [G loss: 5.259814]\n",
      "epoch:36 step:28424 [D loss: 0.503673, acc.: 78.12%] [G loss: 3.654596]\n",
      "epoch:36 step:28425 [D loss: 0.304908, acc.: 85.94%] [G loss: 3.007761]\n",
      "epoch:36 step:28426 [D loss: 0.302486, acc.: 87.50%] [G loss: 3.983662]\n",
      "epoch:36 step:28427 [D loss: 0.286257, acc.: 88.28%] [G loss: 3.325784]\n",
      "epoch:36 step:28428 [D loss: 0.288239, acc.: 85.94%] [G loss: 2.922455]\n",
      "epoch:36 step:28429 [D loss: 0.337749, acc.: 85.16%] [G loss: 2.782099]\n",
      "epoch:36 step:28430 [D loss: 0.273579, acc.: 88.28%] [G loss: 2.970999]\n",
      "epoch:36 step:28431 [D loss: 0.383122, acc.: 80.47%] [G loss: 3.216258]\n",
      "epoch:36 step:28432 [D loss: 0.296803, acc.: 90.62%] [G loss: 4.407226]\n",
      "epoch:36 step:28433 [D loss: 0.368050, acc.: 85.16%] [G loss: 3.773813]\n",
      "epoch:36 step:28434 [D loss: 0.387790, acc.: 84.38%] [G loss: 5.335545]\n",
      "epoch:36 step:28435 [D loss: 0.285824, acc.: 86.72%] [G loss: 3.499370]\n",
      "epoch:36 step:28436 [D loss: 0.419642, acc.: 79.69%] [G loss: 3.801908]\n",
      "epoch:36 step:28437 [D loss: 0.364308, acc.: 82.81%] [G loss: 3.588172]\n",
      "epoch:36 step:28438 [D loss: 0.222715, acc.: 92.97%] [G loss: 4.756836]\n",
      "epoch:36 step:28439 [D loss: 0.362807, acc.: 82.81%] [G loss: 2.869906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28440 [D loss: 0.370413, acc.: 82.81%] [G loss: 5.009578]\n",
      "epoch:36 step:28441 [D loss: 0.424811, acc.: 83.59%] [G loss: 3.499964]\n",
      "epoch:36 step:28442 [D loss: 0.273857, acc.: 86.72%] [G loss: 3.477479]\n",
      "epoch:36 step:28443 [D loss: 0.346382, acc.: 84.38%] [G loss: 2.881582]\n",
      "epoch:36 step:28444 [D loss: 0.293128, acc.: 89.06%] [G loss: 3.382088]\n",
      "epoch:36 step:28445 [D loss: 0.316528, acc.: 85.94%] [G loss: 2.522344]\n",
      "epoch:36 step:28446 [D loss: 0.299436, acc.: 89.06%] [G loss: 2.590319]\n",
      "epoch:36 step:28447 [D loss: 0.350817, acc.: 85.16%] [G loss: 3.412542]\n",
      "epoch:36 step:28448 [D loss: 0.349183, acc.: 83.59%] [G loss: 3.977199]\n",
      "epoch:36 step:28449 [D loss: 0.363627, acc.: 85.16%] [G loss: 3.098410]\n",
      "epoch:36 step:28450 [D loss: 0.346347, acc.: 85.16%] [G loss: 3.001564]\n",
      "epoch:36 step:28451 [D loss: 0.284822, acc.: 88.28%] [G loss: 3.586769]\n",
      "epoch:36 step:28452 [D loss: 0.260909, acc.: 87.50%] [G loss: 4.519545]\n",
      "epoch:36 step:28453 [D loss: 0.277668, acc.: 89.06%] [G loss: 3.003129]\n",
      "epoch:36 step:28454 [D loss: 0.384812, acc.: 80.47%] [G loss: 3.007871]\n",
      "epoch:36 step:28455 [D loss: 0.224151, acc.: 89.84%] [G loss: 4.582524]\n",
      "epoch:36 step:28456 [D loss: 0.303129, acc.: 88.28%] [G loss: 3.004292]\n",
      "epoch:36 step:28457 [D loss: 0.238157, acc.: 85.94%] [G loss: 2.702515]\n",
      "epoch:36 step:28458 [D loss: 0.283724, acc.: 89.06%] [G loss: 3.109643]\n",
      "epoch:36 step:28459 [D loss: 0.352527, acc.: 84.38%] [G loss: 3.459243]\n",
      "epoch:36 step:28460 [D loss: 0.440615, acc.: 81.25%] [G loss: 3.909862]\n",
      "epoch:36 step:28461 [D loss: 0.326805, acc.: 88.28%] [G loss: 3.794239]\n",
      "epoch:36 step:28462 [D loss: 0.301881, acc.: 86.72%] [G loss: 3.024859]\n",
      "epoch:36 step:28463 [D loss: 0.330602, acc.: 83.59%] [G loss: 2.787520]\n",
      "epoch:36 step:28464 [D loss: 0.367012, acc.: 83.59%] [G loss: 3.565778]\n",
      "epoch:36 step:28465 [D loss: 0.461276, acc.: 78.12%] [G loss: 5.795560]\n",
      "epoch:36 step:28466 [D loss: 0.560267, acc.: 75.78%] [G loss: 4.715613]\n",
      "epoch:36 step:28467 [D loss: 0.285744, acc.: 87.50%] [G loss: 4.337953]\n",
      "epoch:36 step:28468 [D loss: 0.290811, acc.: 85.16%] [G loss: 4.218686]\n",
      "epoch:36 step:28469 [D loss: 0.295437, acc.: 86.72%] [G loss: 3.008558]\n",
      "epoch:36 step:28470 [D loss: 0.375863, acc.: 85.16%] [G loss: 2.930601]\n",
      "epoch:36 step:28471 [D loss: 0.298560, acc.: 85.94%] [G loss: 2.893705]\n",
      "epoch:36 step:28472 [D loss: 0.330308, acc.: 85.94%] [G loss: 3.467737]\n",
      "epoch:36 step:28473 [D loss: 0.332558, acc.: 85.94%] [G loss: 2.987803]\n",
      "epoch:36 step:28474 [D loss: 0.299599, acc.: 84.38%] [G loss: 2.959730]\n",
      "epoch:36 step:28475 [D loss: 0.334864, acc.: 83.59%] [G loss: 2.935710]\n",
      "epoch:36 step:28476 [D loss: 0.368586, acc.: 80.47%] [G loss: 3.491893]\n",
      "epoch:36 step:28477 [D loss: 0.251238, acc.: 89.84%] [G loss: 3.747728]\n",
      "epoch:36 step:28478 [D loss: 0.360533, acc.: 78.91%] [G loss: 5.827258]\n",
      "epoch:36 step:28479 [D loss: 0.366055, acc.: 78.12%] [G loss: 5.277926]\n",
      "epoch:36 step:28480 [D loss: 0.356936, acc.: 80.47%] [G loss: 3.569304]\n",
      "epoch:36 step:28481 [D loss: 0.329418, acc.: 84.38%] [G loss: 4.509044]\n",
      "epoch:36 step:28482 [D loss: 0.283755, acc.: 91.41%] [G loss: 3.147755]\n",
      "epoch:36 step:28483 [D loss: 0.256906, acc.: 92.19%] [G loss: 3.273459]\n",
      "epoch:36 step:28484 [D loss: 0.330050, acc.: 85.16%] [G loss: 2.473968]\n",
      "epoch:36 step:28485 [D loss: 0.277881, acc.: 85.94%] [G loss: 3.146177]\n",
      "epoch:36 step:28486 [D loss: 0.256686, acc.: 91.41%] [G loss: 3.761905]\n",
      "epoch:36 step:28487 [D loss: 0.217113, acc.: 91.41%] [G loss: 3.756697]\n",
      "epoch:36 step:28488 [D loss: 0.298288, acc.: 84.38%] [G loss: 3.328798]\n",
      "epoch:36 step:28489 [D loss: 0.279004, acc.: 88.28%] [G loss: 4.249044]\n",
      "epoch:36 step:28490 [D loss: 0.281905, acc.: 87.50%] [G loss: 3.274097]\n",
      "epoch:36 step:28491 [D loss: 0.313005, acc.: 84.38%] [G loss: 3.157159]\n",
      "epoch:36 step:28492 [D loss: 0.368893, acc.: 80.47%] [G loss: 3.485574]\n",
      "epoch:36 step:28493 [D loss: 0.402862, acc.: 78.91%] [G loss: 3.430036]\n",
      "epoch:36 step:28494 [D loss: 0.236784, acc.: 90.62%] [G loss: 4.092181]\n",
      "epoch:36 step:28495 [D loss: 0.233235, acc.: 88.28%] [G loss: 3.204852]\n",
      "epoch:36 step:28496 [D loss: 0.222567, acc.: 90.62%] [G loss: 5.290158]\n",
      "epoch:36 step:28497 [D loss: 0.205254, acc.: 91.41%] [G loss: 3.833749]\n",
      "epoch:36 step:28498 [D loss: 0.331367, acc.: 87.50%] [G loss: 5.104673]\n",
      "epoch:36 step:28499 [D loss: 0.196409, acc.: 93.75%] [G loss: 3.059989]\n",
      "epoch:36 step:28500 [D loss: 0.222941, acc.: 92.97%] [G loss: 3.509898]\n",
      "epoch:36 step:28501 [D loss: 0.286182, acc.: 85.94%] [G loss: 3.656671]\n",
      "epoch:36 step:28502 [D loss: 0.328304, acc.: 84.38%] [G loss: 4.306880]\n",
      "epoch:36 step:28503 [D loss: 0.290162, acc.: 85.16%] [G loss: 4.864720]\n",
      "epoch:36 step:28504 [D loss: 0.352542, acc.: 82.81%] [G loss: 4.282181]\n",
      "epoch:36 step:28505 [D loss: 0.341172, acc.: 85.94%] [G loss: 2.690789]\n",
      "epoch:36 step:28506 [D loss: 0.255165, acc.: 89.06%] [G loss: 4.208797]\n",
      "epoch:36 step:28507 [D loss: 0.425103, acc.: 82.03%] [G loss: 3.466081]\n",
      "epoch:36 step:28508 [D loss: 0.221754, acc.: 89.06%] [G loss: 3.939693]\n",
      "epoch:36 step:28509 [D loss: 0.308880, acc.: 89.06%] [G loss: 5.351824]\n",
      "epoch:36 step:28510 [D loss: 0.358024, acc.: 79.69%] [G loss: 3.307639]\n",
      "epoch:36 step:28511 [D loss: 0.210619, acc.: 90.62%] [G loss: 3.836725]\n",
      "epoch:36 step:28512 [D loss: 0.300210, acc.: 85.94%] [G loss: 3.256830]\n",
      "epoch:36 step:28513 [D loss: 0.267802, acc.: 88.28%] [G loss: 3.211406]\n",
      "epoch:36 step:28514 [D loss: 0.360560, acc.: 83.59%] [G loss: 4.328201]\n",
      "epoch:36 step:28515 [D loss: 0.317845, acc.: 87.50%] [G loss: 3.532398]\n",
      "epoch:36 step:28516 [D loss: 0.257367, acc.: 88.28%] [G loss: 3.261140]\n",
      "epoch:36 step:28517 [D loss: 0.419969, acc.: 82.81%] [G loss: 5.658287]\n",
      "epoch:36 step:28518 [D loss: 0.351941, acc.: 81.25%] [G loss: 2.853318]\n",
      "epoch:36 step:28519 [D loss: 0.236706, acc.: 89.06%] [G loss: 6.228628]\n",
      "epoch:36 step:28520 [D loss: 0.284910, acc.: 87.50%] [G loss: 4.690179]\n",
      "epoch:36 step:28521 [D loss: 0.254399, acc.: 88.28%] [G loss: 3.789993]\n",
      "epoch:36 step:28522 [D loss: 0.266788, acc.: 90.62%] [G loss: 3.633385]\n",
      "epoch:36 step:28523 [D loss: 0.291437, acc.: 88.28%] [G loss: 2.661884]\n",
      "epoch:36 step:28524 [D loss: 0.315044, acc.: 85.16%] [G loss: 3.276481]\n",
      "epoch:36 step:28525 [D loss: 0.337330, acc.: 86.72%] [G loss: 2.487423]\n",
      "epoch:36 step:28526 [D loss: 0.362350, acc.: 83.59%] [G loss: 2.907947]\n",
      "epoch:36 step:28527 [D loss: 0.307401, acc.: 85.16%] [G loss: 3.008807]\n",
      "epoch:36 step:28528 [D loss: 0.314159, acc.: 86.72%] [G loss: 2.454685]\n",
      "epoch:36 step:28529 [D loss: 0.290096, acc.: 85.94%] [G loss: 3.056890]\n",
      "epoch:36 step:28530 [D loss: 0.467060, acc.: 79.69%] [G loss: 2.986471]\n",
      "epoch:36 step:28531 [D loss: 0.370732, acc.: 85.16%] [G loss: 4.025723]\n",
      "epoch:36 step:28532 [D loss: 0.401398, acc.: 79.69%] [G loss: 3.644943]\n",
      "epoch:36 step:28533 [D loss: 0.383843, acc.: 79.69%] [G loss: 4.529402]\n",
      "epoch:36 step:28534 [D loss: 0.613793, acc.: 76.56%] [G loss: 6.543790]\n",
      "epoch:36 step:28535 [D loss: 0.742387, acc.: 68.75%] [G loss: 3.319113]\n",
      "epoch:36 step:28536 [D loss: 0.349330, acc.: 83.59%] [G loss: 2.093811]\n",
      "epoch:36 step:28537 [D loss: 0.395987, acc.: 83.59%] [G loss: 4.485558]\n",
      "epoch:36 step:28538 [D loss: 0.290969, acc.: 87.50%] [G loss: 3.772398]\n",
      "epoch:36 step:28539 [D loss: 0.293940, acc.: 85.16%] [G loss: 4.728861]\n",
      "epoch:36 step:28540 [D loss: 0.221979, acc.: 89.06%] [G loss: 4.594121]\n",
      "epoch:36 step:28541 [D loss: 0.453155, acc.: 80.47%] [G loss: 3.390705]\n",
      "epoch:36 step:28542 [D loss: 0.318368, acc.: 89.06%] [G loss: 3.256904]\n",
      "epoch:36 step:28543 [D loss: 0.298231, acc.: 89.84%] [G loss: 2.706214]\n",
      "epoch:36 step:28544 [D loss: 0.454119, acc.: 79.69%] [G loss: 2.860512]\n",
      "epoch:36 step:28545 [D loss: 0.395903, acc.: 81.25%] [G loss: 3.112783]\n",
      "epoch:36 step:28546 [D loss: 0.339795, acc.: 82.81%] [G loss: 2.860545]\n",
      "epoch:36 step:28547 [D loss: 0.369979, acc.: 84.38%] [G loss: 3.095218]\n",
      "epoch:36 step:28548 [D loss: 0.307864, acc.: 89.84%] [G loss: 3.923141]\n",
      "epoch:36 step:28549 [D loss: 0.198406, acc.: 90.62%] [G loss: 3.386505]\n",
      "epoch:36 step:28550 [D loss: 0.276831, acc.: 89.06%] [G loss: 3.009299]\n",
      "epoch:36 step:28551 [D loss: 0.396107, acc.: 85.94%] [G loss: 2.921332]\n",
      "epoch:36 step:28552 [D loss: 0.498086, acc.: 72.66%] [G loss: 2.842126]\n",
      "epoch:36 step:28553 [D loss: 0.312687, acc.: 88.28%] [G loss: 2.534270]\n",
      "epoch:36 step:28554 [D loss: 0.269259, acc.: 90.62%] [G loss: 3.335578]\n",
      "epoch:36 step:28555 [D loss: 0.359709, acc.: 85.16%] [G loss: 2.659871]\n",
      "epoch:36 step:28556 [D loss: 0.260305, acc.: 89.84%] [G loss: 3.101505]\n",
      "epoch:36 step:28557 [D loss: 0.278392, acc.: 91.41%] [G loss: 2.434139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28558 [D loss: 0.280583, acc.: 89.84%] [G loss: 3.022729]\n",
      "epoch:36 step:28559 [D loss: 0.275926, acc.: 92.19%] [G loss: 2.255839]\n",
      "epoch:36 step:28560 [D loss: 0.313355, acc.: 87.50%] [G loss: 2.978736]\n",
      "epoch:36 step:28561 [D loss: 0.326292, acc.: 87.50%] [G loss: 2.682580]\n",
      "epoch:36 step:28562 [D loss: 0.339380, acc.: 85.16%] [G loss: 3.061319]\n",
      "epoch:36 step:28563 [D loss: 0.324786, acc.: 83.59%] [G loss: 2.926691]\n",
      "epoch:36 step:28564 [D loss: 0.461128, acc.: 77.34%] [G loss: 2.494425]\n",
      "epoch:36 step:28565 [D loss: 0.250754, acc.: 91.41%] [G loss: 2.727945]\n",
      "epoch:36 step:28566 [D loss: 0.320550, acc.: 86.72%] [G loss: 3.345645]\n",
      "epoch:36 step:28567 [D loss: 0.313179, acc.: 87.50%] [G loss: 3.068670]\n",
      "epoch:36 step:28568 [D loss: 0.276064, acc.: 89.84%] [G loss: 3.581394]\n",
      "epoch:36 step:28569 [D loss: 0.239713, acc.: 91.41%] [G loss: 3.050571]\n",
      "epoch:36 step:28570 [D loss: 0.243933, acc.: 88.28%] [G loss: 3.332967]\n",
      "epoch:36 step:28571 [D loss: 0.278144, acc.: 85.94%] [G loss: 3.085237]\n",
      "epoch:36 step:28572 [D loss: 0.243352, acc.: 90.62%] [G loss: 3.248372]\n",
      "epoch:36 step:28573 [D loss: 0.337067, acc.: 85.16%] [G loss: 2.608704]\n",
      "epoch:36 step:28574 [D loss: 0.269953, acc.: 88.28%] [G loss: 3.182429]\n",
      "epoch:36 step:28575 [D loss: 0.321662, acc.: 87.50%] [G loss: 2.943692]\n",
      "epoch:36 step:28576 [D loss: 0.376851, acc.: 81.25%] [G loss: 3.890371]\n",
      "epoch:36 step:28577 [D loss: 0.337639, acc.: 82.03%] [G loss: 3.866279]\n",
      "epoch:36 step:28578 [D loss: 0.227228, acc.: 92.19%] [G loss: 3.397599]\n",
      "epoch:36 step:28579 [D loss: 0.425927, acc.: 79.69%] [G loss: 3.560654]\n",
      "epoch:36 step:28580 [D loss: 0.283992, acc.: 83.59%] [G loss: 3.478081]\n",
      "epoch:36 step:28581 [D loss: 0.346647, acc.: 83.59%] [G loss: 2.593734]\n",
      "epoch:36 step:28582 [D loss: 0.326662, acc.: 88.28%] [G loss: 3.189554]\n",
      "epoch:36 step:28583 [D loss: 0.259000, acc.: 89.06%] [G loss: 3.699171]\n",
      "epoch:36 step:28584 [D loss: 0.347517, acc.: 81.25%] [G loss: 2.914929]\n",
      "epoch:36 step:28585 [D loss: 0.381043, acc.: 82.03%] [G loss: 2.872721]\n",
      "epoch:36 step:28586 [D loss: 0.346134, acc.: 87.50%] [G loss: 2.392102]\n",
      "epoch:36 step:28587 [D loss: 0.262264, acc.: 91.41%] [G loss: 3.496513]\n",
      "epoch:36 step:28588 [D loss: 0.325585, acc.: 85.16%] [G loss: 3.378723]\n",
      "epoch:36 step:28589 [D loss: 0.344767, acc.: 85.16%] [G loss: 2.316846]\n",
      "epoch:36 step:28590 [D loss: 0.323904, acc.: 89.06%] [G loss: 2.776172]\n",
      "epoch:36 step:28591 [D loss: 0.340290, acc.: 85.16%] [G loss: 5.357908]\n",
      "epoch:36 step:28592 [D loss: 0.364077, acc.: 81.25%] [G loss: 4.361506]\n",
      "epoch:36 step:28593 [D loss: 0.167417, acc.: 95.31%] [G loss: 4.175941]\n",
      "epoch:36 step:28594 [D loss: 0.315105, acc.: 86.72%] [G loss: 3.741935]\n",
      "epoch:36 step:28595 [D loss: 0.257340, acc.: 91.41%] [G loss: 3.190121]\n",
      "epoch:36 step:28596 [D loss: 0.229137, acc.: 90.62%] [G loss: 3.858486]\n",
      "epoch:36 step:28597 [D loss: 0.320529, acc.: 81.25%] [G loss: 3.197119]\n",
      "epoch:36 step:28598 [D loss: 0.307141, acc.: 85.16%] [G loss: 3.217700]\n",
      "epoch:36 step:28599 [D loss: 0.214956, acc.: 91.41%] [G loss: 2.707687]\n",
      "epoch:36 step:28600 [D loss: 0.211721, acc.: 92.97%] [G loss: 2.919569]\n",
      "epoch:36 step:28601 [D loss: 0.238602, acc.: 89.84%] [G loss: 4.003977]\n",
      "epoch:36 step:28602 [D loss: 0.270100, acc.: 89.84%] [G loss: 5.837469]\n",
      "epoch:36 step:28603 [D loss: 0.271766, acc.: 86.72%] [G loss: 4.991791]\n",
      "epoch:36 step:28604 [D loss: 0.221118, acc.: 92.19%] [G loss: 4.601685]\n",
      "epoch:36 step:28605 [D loss: 0.338224, acc.: 85.16%] [G loss: 4.228431]\n",
      "epoch:36 step:28606 [D loss: 0.301612, acc.: 87.50%] [G loss: 4.183941]\n",
      "epoch:36 step:28607 [D loss: 0.251190, acc.: 89.06%] [G loss: 3.444507]\n",
      "epoch:36 step:28608 [D loss: 0.322987, acc.: 85.16%] [G loss: 3.008752]\n",
      "epoch:36 step:28609 [D loss: 0.365647, acc.: 86.72%] [G loss: 3.057248]\n",
      "epoch:36 step:28610 [D loss: 0.326136, acc.: 86.72%] [G loss: 4.814147]\n",
      "epoch:36 step:28611 [D loss: 0.401148, acc.: 79.69%] [G loss: 4.102444]\n",
      "epoch:36 step:28612 [D loss: 0.382235, acc.: 82.03%] [G loss: 3.530149]\n",
      "epoch:36 step:28613 [D loss: 0.411943, acc.: 81.25%] [G loss: 6.656411]\n",
      "epoch:36 step:28614 [D loss: 0.537949, acc.: 74.22%] [G loss: 2.892440]\n",
      "epoch:36 step:28615 [D loss: 0.374736, acc.: 82.81%] [G loss: 3.871398]\n",
      "epoch:36 step:28616 [D loss: 0.313184, acc.: 85.16%] [G loss: 3.226547]\n",
      "epoch:36 step:28617 [D loss: 0.285758, acc.: 88.28%] [G loss: 3.865007]\n",
      "epoch:36 step:28618 [D loss: 0.417392, acc.: 80.47%] [G loss: 3.713102]\n",
      "epoch:36 step:28619 [D loss: 0.347891, acc.: 77.34%] [G loss: 3.656892]\n",
      "epoch:36 step:28620 [D loss: 0.342987, acc.: 85.94%] [G loss: 3.404292]\n",
      "epoch:36 step:28621 [D loss: 0.319326, acc.: 84.38%] [G loss: 3.720922]\n",
      "epoch:36 step:28622 [D loss: 0.351229, acc.: 82.03%] [G loss: 4.504639]\n",
      "epoch:36 step:28623 [D loss: 0.356163, acc.: 87.50%] [G loss: 4.126556]\n",
      "epoch:36 step:28624 [D loss: 0.301333, acc.: 87.50%] [G loss: 3.303557]\n",
      "epoch:36 step:28625 [D loss: 0.411029, acc.: 80.47%] [G loss: 3.083088]\n",
      "epoch:36 step:28626 [D loss: 0.306360, acc.: 86.72%] [G loss: 3.852339]\n",
      "epoch:36 step:28627 [D loss: 0.437591, acc.: 78.91%] [G loss: 3.104304]\n",
      "epoch:36 step:28628 [D loss: 0.323490, acc.: 84.38%] [G loss: 3.410688]\n",
      "epoch:36 step:28629 [D loss: 0.279152, acc.: 84.38%] [G loss: 3.637524]\n",
      "epoch:36 step:28630 [D loss: 0.367621, acc.: 84.38%] [G loss: 2.777563]\n",
      "epoch:36 step:28631 [D loss: 0.313735, acc.: 88.28%] [G loss: 2.607153]\n",
      "epoch:36 step:28632 [D loss: 0.319838, acc.: 83.59%] [G loss: 3.252323]\n",
      "epoch:36 step:28633 [D loss: 0.271874, acc.: 89.06%] [G loss: 3.594145]\n",
      "epoch:36 step:28634 [D loss: 0.229634, acc.: 92.97%] [G loss: 3.396655]\n",
      "epoch:36 step:28635 [D loss: 0.295242, acc.: 85.16%] [G loss: 3.305355]\n",
      "epoch:36 step:28636 [D loss: 0.440463, acc.: 80.47%] [G loss: 3.003537]\n",
      "epoch:36 step:28637 [D loss: 0.362549, acc.: 80.47%] [G loss: 3.520695]\n",
      "epoch:36 step:28638 [D loss: 0.324257, acc.: 84.38%] [G loss: 3.235651]\n",
      "epoch:36 step:28639 [D loss: 0.328999, acc.: 86.72%] [G loss: 3.681432]\n",
      "epoch:36 step:28640 [D loss: 0.404791, acc.: 83.59%] [G loss: 2.170950]\n",
      "epoch:36 step:28641 [D loss: 0.327373, acc.: 83.59%] [G loss: 2.944881]\n",
      "epoch:36 step:28642 [D loss: 0.346575, acc.: 81.25%] [G loss: 2.815329]\n",
      "epoch:36 step:28643 [D loss: 0.286958, acc.: 86.72%] [G loss: 3.349585]\n",
      "epoch:36 step:28644 [D loss: 0.280328, acc.: 88.28%] [G loss: 3.394844]\n",
      "epoch:36 step:28645 [D loss: 0.354395, acc.: 84.38%] [G loss: 3.599433]\n",
      "epoch:36 step:28646 [D loss: 0.448176, acc.: 82.03%] [G loss: 6.088238]\n",
      "epoch:36 step:28647 [D loss: 1.059813, acc.: 66.41%] [G loss: 10.534523]\n",
      "epoch:36 step:28648 [D loss: 1.779092, acc.: 60.94%] [G loss: 7.774476]\n",
      "epoch:36 step:28649 [D loss: 1.100222, acc.: 66.41%] [G loss: 5.605515]\n",
      "epoch:36 step:28650 [D loss: 0.745227, acc.: 73.44%] [G loss: 4.764282]\n",
      "epoch:36 step:28651 [D loss: 0.428051, acc.: 79.69%] [G loss: 4.075168]\n",
      "epoch:36 step:28652 [D loss: 0.505022, acc.: 75.78%] [G loss: 3.367072]\n",
      "epoch:36 step:28653 [D loss: 0.381861, acc.: 85.16%] [G loss: 3.548016]\n",
      "epoch:36 step:28654 [D loss: 0.242909, acc.: 87.50%] [G loss: 2.766903]\n",
      "epoch:36 step:28655 [D loss: 0.261635, acc.: 92.19%] [G loss: 3.934012]\n",
      "epoch:36 step:28656 [D loss: 0.400630, acc.: 81.25%] [G loss: 3.049380]\n",
      "epoch:36 step:28657 [D loss: 0.366994, acc.: 82.03%] [G loss: 4.248951]\n",
      "epoch:36 step:28658 [D loss: 0.537759, acc.: 75.78%] [G loss: 2.565552]\n",
      "epoch:36 step:28659 [D loss: 0.361304, acc.: 82.03%] [G loss: 3.216950]\n",
      "epoch:36 step:28660 [D loss: 0.352839, acc.: 85.94%] [G loss: 3.039465]\n",
      "epoch:36 step:28661 [D loss: 0.324275, acc.: 88.28%] [G loss: 2.583920]\n",
      "epoch:36 step:28662 [D loss: 0.359098, acc.: 85.16%] [G loss: 2.281806]\n",
      "epoch:36 step:28663 [D loss: 0.469467, acc.: 73.44%] [G loss: 2.237555]\n",
      "epoch:36 step:28664 [D loss: 0.260378, acc.: 87.50%] [G loss: 2.753128]\n",
      "epoch:36 step:28665 [D loss: 0.322302, acc.: 86.72%] [G loss: 3.494236]\n",
      "epoch:36 step:28666 [D loss: 0.282300, acc.: 85.94%] [G loss: 2.959044]\n",
      "epoch:36 step:28667 [D loss: 0.253794, acc.: 88.28%] [G loss: 3.301636]\n",
      "epoch:36 step:28668 [D loss: 0.366324, acc.: 85.16%] [G loss: 3.975651]\n",
      "epoch:36 step:28669 [D loss: 0.263561, acc.: 87.50%] [G loss: 4.502087]\n",
      "epoch:36 step:28670 [D loss: 0.301845, acc.: 88.28%] [G loss: 3.740903]\n",
      "epoch:36 step:28671 [D loss: 0.310809, acc.: 85.16%] [G loss: 3.669655]\n",
      "epoch:36 step:28672 [D loss: 0.325936, acc.: 84.38%] [G loss: 3.905700]\n",
      "epoch:36 step:28673 [D loss: 0.290540, acc.: 86.72%] [G loss: 4.081957]\n",
      "epoch:36 step:28674 [D loss: 0.344210, acc.: 82.81%] [G loss: 2.625990]\n",
      "epoch:36 step:28675 [D loss: 0.273684, acc.: 87.50%] [G loss: 3.139291]\n",
      "epoch:36 step:28676 [D loss: 0.412522, acc.: 84.38%] [G loss: 2.679610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28677 [D loss: 0.360043, acc.: 84.38%] [G loss: 2.775377]\n",
      "epoch:36 step:28678 [D loss: 0.387211, acc.: 83.59%] [G loss: 3.212265]\n",
      "epoch:36 step:28679 [D loss: 0.334250, acc.: 82.81%] [G loss: 2.704946]\n",
      "epoch:36 step:28680 [D loss: 0.215034, acc.: 92.97%] [G loss: 2.893929]\n",
      "epoch:36 step:28681 [D loss: 0.275792, acc.: 89.06%] [G loss: 2.666013]\n",
      "epoch:36 step:28682 [D loss: 0.333989, acc.: 83.59%] [G loss: 3.481777]\n",
      "epoch:36 step:28683 [D loss: 0.406498, acc.: 82.81%] [G loss: 2.484201]\n",
      "epoch:36 step:28684 [D loss: 0.314475, acc.: 89.06%] [G loss: 2.481706]\n",
      "epoch:36 step:28685 [D loss: 0.289427, acc.: 85.16%] [G loss: 3.076615]\n",
      "epoch:36 step:28686 [D loss: 0.338515, acc.: 85.94%] [G loss: 3.113210]\n",
      "epoch:36 step:28687 [D loss: 0.371454, acc.: 83.59%] [G loss: 3.000343]\n",
      "epoch:36 step:28688 [D loss: 0.220255, acc.: 91.41%] [G loss: 3.042943]\n",
      "epoch:36 step:28689 [D loss: 0.258771, acc.: 88.28%] [G loss: 2.758754]\n",
      "epoch:36 step:28690 [D loss: 0.395253, acc.: 80.47%] [G loss: 2.772852]\n",
      "epoch:36 step:28691 [D loss: 0.391003, acc.: 85.16%] [G loss: 2.100408]\n",
      "epoch:36 step:28692 [D loss: 0.320042, acc.: 85.16%] [G loss: 2.781804]\n",
      "epoch:36 step:28693 [D loss: 0.224300, acc.: 94.53%] [G loss: 2.490949]\n",
      "epoch:36 step:28694 [D loss: 0.336876, acc.: 85.16%] [G loss: 3.117503]\n",
      "epoch:36 step:28695 [D loss: 0.457394, acc.: 79.69%] [G loss: 2.134012]\n",
      "epoch:36 step:28696 [D loss: 0.321932, acc.: 85.94%] [G loss: 2.718739]\n",
      "epoch:36 step:28697 [D loss: 0.308402, acc.: 85.94%] [G loss: 2.989758]\n",
      "epoch:36 step:28698 [D loss: 0.345946, acc.: 85.16%] [G loss: 2.934677]\n",
      "epoch:36 step:28699 [D loss: 0.342302, acc.: 84.38%] [G loss: 2.724874]\n",
      "epoch:36 step:28700 [D loss: 0.316874, acc.: 85.94%] [G loss: 3.244020]\n",
      "epoch:36 step:28701 [D loss: 0.199534, acc.: 92.97%] [G loss: 3.072140]\n",
      "epoch:36 step:28702 [D loss: 0.232497, acc.: 90.62%] [G loss: 3.726842]\n",
      "epoch:36 step:28703 [D loss: 0.293504, acc.: 89.84%] [G loss: 4.154117]\n",
      "epoch:36 step:28704 [D loss: 0.331499, acc.: 85.94%] [G loss: 2.861929]\n",
      "epoch:36 step:28705 [D loss: 0.251447, acc.: 88.28%] [G loss: 2.984077]\n",
      "epoch:36 step:28706 [D loss: 0.378609, acc.: 78.12%] [G loss: 3.308205]\n",
      "epoch:36 step:28707 [D loss: 0.309592, acc.: 86.72%] [G loss: 3.826268]\n",
      "epoch:36 step:28708 [D loss: 0.257530, acc.: 89.06%] [G loss: 3.209630]\n",
      "epoch:36 step:28709 [D loss: 0.370086, acc.: 84.38%] [G loss: 2.492078]\n",
      "epoch:36 step:28710 [D loss: 0.333307, acc.: 83.59%] [G loss: 2.717736]\n",
      "epoch:36 step:28711 [D loss: 0.371262, acc.: 82.81%] [G loss: 2.756033]\n",
      "epoch:36 step:28712 [D loss: 0.326390, acc.: 85.16%] [G loss: 2.340050]\n",
      "epoch:36 step:28713 [D loss: 0.290806, acc.: 85.94%] [G loss: 3.294980]\n",
      "epoch:36 step:28714 [D loss: 0.351951, acc.: 84.38%] [G loss: 3.079017]\n",
      "epoch:36 step:28715 [D loss: 0.323315, acc.: 83.59%] [G loss: 2.972581]\n",
      "epoch:36 step:28716 [D loss: 0.269013, acc.: 87.50%] [G loss: 2.715385]\n",
      "epoch:36 step:28717 [D loss: 0.344436, acc.: 82.81%] [G loss: 2.697568]\n",
      "epoch:36 step:28718 [D loss: 0.316448, acc.: 85.16%] [G loss: 2.531581]\n",
      "epoch:36 step:28719 [D loss: 0.281260, acc.: 88.28%] [G loss: 2.691571]\n",
      "epoch:36 step:28720 [D loss: 0.315258, acc.: 85.16%] [G loss: 3.275542]\n",
      "epoch:36 step:28721 [D loss: 0.300380, acc.: 86.72%] [G loss: 2.522250]\n",
      "epoch:36 step:28722 [D loss: 0.328675, acc.: 84.38%] [G loss: 2.880933]\n",
      "epoch:36 step:28723 [D loss: 0.299984, acc.: 87.50%] [G loss: 2.429795]\n",
      "epoch:36 step:28724 [D loss: 0.361643, acc.: 82.81%] [G loss: 2.888549]\n",
      "epoch:36 step:28725 [D loss: 0.333167, acc.: 85.94%] [G loss: 3.317212]\n",
      "epoch:36 step:28726 [D loss: 0.422197, acc.: 78.91%] [G loss: 4.130432]\n",
      "epoch:36 step:28727 [D loss: 0.607082, acc.: 77.34%] [G loss: 2.763421]\n",
      "epoch:36 step:28728 [D loss: 0.214912, acc.: 90.62%] [G loss: 2.604450]\n",
      "epoch:36 step:28729 [D loss: 0.420711, acc.: 78.12%] [G loss: 2.006754]\n",
      "epoch:36 step:28730 [D loss: 0.319199, acc.: 89.06%] [G loss: 2.869847]\n",
      "epoch:36 step:28731 [D loss: 0.320972, acc.: 85.94%] [G loss: 3.066265]\n",
      "epoch:36 step:28732 [D loss: 0.260109, acc.: 84.38%] [G loss: 3.163208]\n",
      "epoch:36 step:28733 [D loss: 0.336551, acc.: 81.25%] [G loss: 3.368930]\n",
      "epoch:36 step:28734 [D loss: 0.337400, acc.: 81.25%] [G loss: 3.352041]\n",
      "epoch:36 step:28735 [D loss: 0.210513, acc.: 91.41%] [G loss: 2.642993]\n",
      "epoch:36 step:28736 [D loss: 0.383423, acc.: 80.47%] [G loss: 3.080804]\n",
      "epoch:36 step:28737 [D loss: 0.287270, acc.: 89.84%] [G loss: 2.878013]\n",
      "epoch:36 step:28738 [D loss: 0.288420, acc.: 86.72%] [G loss: 2.660788]\n",
      "epoch:36 step:28739 [D loss: 0.416461, acc.: 81.25%] [G loss: 2.390182]\n",
      "epoch:36 step:28740 [D loss: 0.334235, acc.: 88.28%] [G loss: 2.958982]\n",
      "epoch:36 step:28741 [D loss: 0.369338, acc.: 85.94%] [G loss: 2.771821]\n",
      "epoch:36 step:28742 [D loss: 0.305271, acc.: 88.28%] [G loss: 2.753557]\n",
      "epoch:36 step:28743 [D loss: 0.416329, acc.: 76.56%] [G loss: 2.540357]\n",
      "epoch:36 step:28744 [D loss: 0.363141, acc.: 86.72%] [G loss: 3.477767]\n",
      "epoch:36 step:28745 [D loss: 0.324787, acc.: 87.50%] [G loss: 3.620253]\n",
      "epoch:36 step:28746 [D loss: 0.279403, acc.: 85.94%] [G loss: 2.403246]\n",
      "epoch:36 step:28747 [D loss: 0.328482, acc.: 87.50%] [G loss: 3.047987]\n",
      "epoch:36 step:28748 [D loss: 0.290950, acc.: 89.06%] [G loss: 2.868000]\n",
      "epoch:36 step:28749 [D loss: 0.318444, acc.: 89.06%] [G loss: 2.726413]\n",
      "epoch:36 step:28750 [D loss: 0.317243, acc.: 88.28%] [G loss: 2.393890]\n",
      "epoch:36 step:28751 [D loss: 0.307925, acc.: 86.72%] [G loss: 3.560853]\n",
      "epoch:36 step:28752 [D loss: 0.310749, acc.: 86.72%] [G loss: 3.038446]\n",
      "epoch:36 step:28753 [D loss: 0.285213, acc.: 85.94%] [G loss: 5.090241]\n",
      "epoch:36 step:28754 [D loss: 0.302047, acc.: 87.50%] [G loss: 4.964006]\n",
      "epoch:36 step:28755 [D loss: 0.287358, acc.: 89.06%] [G loss: 3.899923]\n",
      "epoch:36 step:28756 [D loss: 0.353819, acc.: 78.12%] [G loss: 3.098367]\n",
      "epoch:36 step:28757 [D loss: 0.230722, acc.: 91.41%] [G loss: 4.478310]\n",
      "epoch:36 step:28758 [D loss: 0.500367, acc.: 78.12%] [G loss: 3.319873]\n",
      "epoch:36 step:28759 [D loss: 0.302153, acc.: 84.38%] [G loss: 3.621585]\n",
      "epoch:36 step:28760 [D loss: 0.272521, acc.: 84.38%] [G loss: 4.002027]\n",
      "epoch:36 step:28761 [D loss: 0.401153, acc.: 83.59%] [G loss: 3.482898]\n",
      "epoch:36 step:28762 [D loss: 0.340100, acc.: 87.50%] [G loss: 3.862609]\n",
      "epoch:36 step:28763 [D loss: 0.266869, acc.: 91.41%] [G loss: 3.294894]\n",
      "epoch:36 step:28764 [D loss: 0.315890, acc.: 87.50%] [G loss: 4.859353]\n",
      "epoch:36 step:28765 [D loss: 0.350320, acc.: 82.81%] [G loss: 3.380421]\n",
      "epoch:36 step:28766 [D loss: 0.305806, acc.: 83.59%] [G loss: 2.938105]\n",
      "epoch:36 step:28767 [D loss: 0.276089, acc.: 88.28%] [G loss: 3.242670]\n",
      "epoch:36 step:28768 [D loss: 0.353612, acc.: 85.16%] [G loss: 2.788430]\n",
      "epoch:36 step:28769 [D loss: 0.313694, acc.: 84.38%] [G loss: 4.334797]\n",
      "epoch:36 step:28770 [D loss: 0.350717, acc.: 83.59%] [G loss: 3.526971]\n",
      "epoch:36 step:28771 [D loss: 0.298962, acc.: 88.28%] [G loss: 3.157286]\n",
      "epoch:36 step:28772 [D loss: 0.277558, acc.: 86.72%] [G loss: 3.024483]\n",
      "epoch:36 step:28773 [D loss: 0.332330, acc.: 84.38%] [G loss: 3.093747]\n",
      "epoch:36 step:28774 [D loss: 0.203940, acc.: 92.19%] [G loss: 4.110777]\n",
      "epoch:36 step:28775 [D loss: 0.426280, acc.: 82.03%] [G loss: 2.817119]\n",
      "epoch:36 step:28776 [D loss: 0.286601, acc.: 89.84%] [G loss: 3.448841]\n",
      "epoch:36 step:28777 [D loss: 0.362409, acc.: 80.47%] [G loss: 2.733376]\n",
      "epoch:36 step:28778 [D loss: 0.253339, acc.: 86.72%] [G loss: 2.470196]\n",
      "epoch:36 step:28779 [D loss: 0.323190, acc.: 85.94%] [G loss: 2.957371]\n",
      "epoch:36 step:28780 [D loss: 0.414919, acc.: 81.25%] [G loss: 3.882741]\n",
      "epoch:36 step:28781 [D loss: 0.281300, acc.: 85.16%] [G loss: 3.386609]\n",
      "epoch:36 step:28782 [D loss: 0.377150, acc.: 80.47%] [G loss: 3.181139]\n",
      "epoch:36 step:28783 [D loss: 0.342738, acc.: 82.81%] [G loss: 3.193248]\n",
      "epoch:36 step:28784 [D loss: 0.228034, acc.: 90.62%] [G loss: 3.504603]\n",
      "epoch:36 step:28785 [D loss: 0.289551, acc.: 90.62%] [G loss: 3.794418]\n",
      "epoch:36 step:28786 [D loss: 0.451597, acc.: 81.25%] [G loss: 2.867896]\n",
      "epoch:36 step:28787 [D loss: 0.354681, acc.: 82.03%] [G loss: 3.022349]\n",
      "epoch:36 step:28788 [D loss: 0.270003, acc.: 88.28%] [G loss: 2.995290]\n",
      "epoch:36 step:28789 [D loss: 0.250989, acc.: 87.50%] [G loss: 4.142518]\n",
      "epoch:36 step:28790 [D loss: 0.262589, acc.: 87.50%] [G loss: 3.476417]\n",
      "epoch:36 step:28791 [D loss: 0.354133, acc.: 88.28%] [G loss: 3.475071]\n",
      "epoch:36 step:28792 [D loss: 0.406557, acc.: 79.69%] [G loss: 3.001894]\n",
      "epoch:36 step:28793 [D loss: 0.247170, acc.: 88.28%] [G loss: 3.602346]\n",
      "epoch:36 step:28794 [D loss: 0.360559, acc.: 87.50%] [G loss: 4.326269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28795 [D loss: 0.352496, acc.: 85.94%] [G loss: 2.999665]\n",
      "epoch:36 step:28796 [D loss: 0.299002, acc.: 83.59%] [G loss: 3.628047]\n",
      "epoch:36 step:28797 [D loss: 0.336441, acc.: 83.59%] [G loss: 3.025675]\n",
      "epoch:36 step:28798 [D loss: 0.408447, acc.: 83.59%] [G loss: 3.097001]\n",
      "epoch:36 step:28799 [D loss: 0.409905, acc.: 77.34%] [G loss: 3.864495]\n",
      "epoch:36 step:28800 [D loss: 0.357834, acc.: 83.59%] [G loss: 3.551218]\n",
      "epoch:36 step:28801 [D loss: 0.333482, acc.: 84.38%] [G loss: 3.944570]\n",
      "epoch:36 step:28802 [D loss: 0.328115, acc.: 84.38%] [G loss: 2.402483]\n",
      "epoch:36 step:28803 [D loss: 0.280362, acc.: 89.06%] [G loss: 3.312346]\n",
      "epoch:36 step:28804 [D loss: 0.282819, acc.: 86.72%] [G loss: 2.617049]\n",
      "epoch:36 step:28805 [D loss: 0.344681, acc.: 86.72%] [G loss: 3.292790]\n",
      "epoch:36 step:28806 [D loss: 0.260138, acc.: 89.06%] [G loss: 3.435014]\n",
      "epoch:36 step:28807 [D loss: 0.357530, acc.: 81.25%] [G loss: 3.367381]\n",
      "epoch:36 step:28808 [D loss: 0.362789, acc.: 88.28%] [G loss: 3.028060]\n",
      "epoch:36 step:28809 [D loss: 0.258731, acc.: 87.50%] [G loss: 2.949767]\n",
      "epoch:36 step:28810 [D loss: 0.369945, acc.: 84.38%] [G loss: 3.271440]\n",
      "epoch:36 step:28811 [D loss: 0.337930, acc.: 85.16%] [G loss: 3.042074]\n",
      "epoch:36 step:28812 [D loss: 0.360816, acc.: 81.25%] [G loss: 3.429454]\n",
      "epoch:36 step:28813 [D loss: 0.316550, acc.: 82.81%] [G loss: 3.077346]\n",
      "epoch:36 step:28814 [D loss: 0.270907, acc.: 89.84%] [G loss: 2.744584]\n",
      "epoch:36 step:28815 [D loss: 0.376215, acc.: 82.81%] [G loss: 3.156789]\n",
      "epoch:36 step:28816 [D loss: 0.318435, acc.: 85.16%] [G loss: 2.893077]\n",
      "epoch:36 step:28817 [D loss: 0.372465, acc.: 82.03%] [G loss: 3.212468]\n",
      "epoch:36 step:28818 [D loss: 0.293941, acc.: 89.84%] [G loss: 2.453840]\n",
      "epoch:36 step:28819 [D loss: 0.422879, acc.: 82.03%] [G loss: 2.427864]\n",
      "epoch:36 step:28820 [D loss: 0.330245, acc.: 87.50%] [G loss: 4.037036]\n",
      "epoch:36 step:28821 [D loss: 0.281413, acc.: 89.84%] [G loss: 3.457020]\n",
      "epoch:36 step:28822 [D loss: 0.280307, acc.: 89.84%] [G loss: 4.195217]\n",
      "epoch:36 step:28823 [D loss: 0.381640, acc.: 85.94%] [G loss: 4.088025]\n",
      "epoch:36 step:28824 [D loss: 0.241416, acc.: 88.28%] [G loss: 4.337743]\n",
      "epoch:36 step:28825 [D loss: 0.312785, acc.: 90.62%] [G loss: 3.396850]\n",
      "epoch:36 step:28826 [D loss: 0.341073, acc.: 83.59%] [G loss: 3.406871]\n",
      "epoch:36 step:28827 [D loss: 0.256028, acc.: 90.62%] [G loss: 3.326725]\n",
      "epoch:36 step:28828 [D loss: 0.253758, acc.: 87.50%] [G loss: 3.247203]\n",
      "epoch:36 step:28829 [D loss: 0.395186, acc.: 78.91%] [G loss: 4.180750]\n",
      "epoch:36 step:28830 [D loss: 0.371934, acc.: 82.03%] [G loss: 2.761019]\n",
      "epoch:36 step:28831 [D loss: 0.290437, acc.: 87.50%] [G loss: 4.629741]\n",
      "epoch:36 step:28832 [D loss: 0.281416, acc.: 88.28%] [G loss: 4.643291]\n",
      "epoch:36 step:28833 [D loss: 0.261021, acc.: 89.84%] [G loss: 3.569752]\n",
      "epoch:36 step:28834 [D loss: 0.340326, acc.: 84.38%] [G loss: 4.503855]\n",
      "epoch:36 step:28835 [D loss: 0.267038, acc.: 88.28%] [G loss: 3.654755]\n",
      "epoch:36 step:28836 [D loss: 0.281467, acc.: 87.50%] [G loss: 3.629537]\n",
      "epoch:36 step:28837 [D loss: 0.284818, acc.: 89.06%] [G loss: 3.061352]\n",
      "epoch:36 step:28838 [D loss: 0.255306, acc.: 89.84%] [G loss: 2.772302]\n",
      "epoch:36 step:28839 [D loss: 0.285093, acc.: 89.84%] [G loss: 4.059088]\n",
      "epoch:36 step:28840 [D loss: 0.204860, acc.: 91.41%] [G loss: 3.613775]\n",
      "epoch:36 step:28841 [D loss: 0.227789, acc.: 89.06%] [G loss: 3.504031]\n",
      "epoch:36 step:28842 [D loss: 0.321680, acc.: 85.94%] [G loss: 3.035346]\n",
      "epoch:36 step:28843 [D loss: 0.252204, acc.: 89.06%] [G loss: 2.858813]\n",
      "epoch:36 step:28844 [D loss: 0.373601, acc.: 84.38%] [G loss: 2.907249]\n",
      "epoch:36 step:28845 [D loss: 0.354175, acc.: 81.25%] [G loss: 2.975098]\n",
      "epoch:36 step:28846 [D loss: 0.292203, acc.: 88.28%] [G loss: 3.651744]\n",
      "epoch:36 step:28847 [D loss: 0.248118, acc.: 89.06%] [G loss: 3.475320]\n",
      "epoch:36 step:28848 [D loss: 0.331493, acc.: 82.03%] [G loss: 4.108356]\n",
      "epoch:36 step:28849 [D loss: 0.256370, acc.: 90.62%] [G loss: 2.867887]\n",
      "epoch:36 step:28850 [D loss: 0.251447, acc.: 89.84%] [G loss: 3.724920]\n",
      "epoch:36 step:28851 [D loss: 0.265034, acc.: 86.72%] [G loss: 3.235412]\n",
      "epoch:36 step:28852 [D loss: 0.262699, acc.: 89.84%] [G loss: 2.851161]\n",
      "epoch:36 step:28853 [D loss: 0.268772, acc.: 89.06%] [G loss: 2.842869]\n",
      "epoch:36 step:28854 [D loss: 0.297890, acc.: 84.38%] [G loss: 2.828051]\n",
      "epoch:36 step:28855 [D loss: 0.499675, acc.: 76.56%] [G loss: 2.567929]\n",
      "epoch:36 step:28856 [D loss: 0.309856, acc.: 88.28%] [G loss: 4.115831]\n",
      "epoch:36 step:28857 [D loss: 0.357364, acc.: 82.03%] [G loss: 3.293521]\n",
      "epoch:36 step:28858 [D loss: 0.379247, acc.: 82.81%] [G loss: 2.887367]\n",
      "epoch:36 step:28859 [D loss: 0.342914, acc.: 85.16%] [G loss: 3.547510]\n",
      "epoch:36 step:28860 [D loss: 0.272785, acc.: 88.28%] [G loss: 5.207502]\n",
      "epoch:36 step:28861 [D loss: 0.308128, acc.: 85.16%] [G loss: 4.110329]\n",
      "epoch:36 step:28862 [D loss: 0.248535, acc.: 87.50%] [G loss: 3.827064]\n",
      "epoch:36 step:28863 [D loss: 0.263252, acc.: 85.94%] [G loss: 4.032004]\n",
      "epoch:36 step:28864 [D loss: 0.341930, acc.: 77.34%] [G loss: 4.425979]\n",
      "epoch:36 step:28865 [D loss: 0.273785, acc.: 89.84%] [G loss: 3.405347]\n",
      "epoch:36 step:28866 [D loss: 0.262555, acc.: 90.62%] [G loss: 4.622670]\n",
      "epoch:36 step:28867 [D loss: 0.275080, acc.: 88.28%] [G loss: 3.240701]\n",
      "epoch:36 step:28868 [D loss: 0.248624, acc.: 91.41%] [G loss: 2.964979]\n",
      "epoch:36 step:28869 [D loss: 0.256174, acc.: 92.19%] [G loss: 2.775614]\n",
      "epoch:36 step:28870 [D loss: 0.297369, acc.: 85.16%] [G loss: 3.378829]\n",
      "epoch:36 step:28871 [D loss: 0.306542, acc.: 85.16%] [G loss: 3.077474]\n",
      "epoch:36 step:28872 [D loss: 0.340924, acc.: 88.28%] [G loss: 2.521104]\n",
      "epoch:36 step:28873 [D loss: 0.294961, acc.: 89.06%] [G loss: 3.667290]\n",
      "epoch:36 step:28874 [D loss: 0.284929, acc.: 88.28%] [G loss: 2.562179]\n",
      "epoch:36 step:28875 [D loss: 0.396100, acc.: 83.59%] [G loss: 3.744071]\n",
      "epoch:36 step:28876 [D loss: 0.340967, acc.: 86.72%] [G loss: 3.091290]\n",
      "epoch:36 step:28877 [D loss: 0.299502, acc.: 87.50%] [G loss: 4.353311]\n",
      "epoch:36 step:28878 [D loss: 0.371309, acc.: 82.03%] [G loss: 3.542731]\n",
      "epoch:36 step:28879 [D loss: 0.342847, acc.: 85.16%] [G loss: 4.632629]\n",
      "epoch:36 step:28880 [D loss: 0.273231, acc.: 85.94%] [G loss: 3.651003]\n",
      "epoch:36 step:28881 [D loss: 0.339295, acc.: 82.81%] [G loss: 3.027718]\n",
      "epoch:36 step:28882 [D loss: 0.297265, acc.: 87.50%] [G loss: 3.481681]\n",
      "epoch:36 step:28883 [D loss: 0.289270, acc.: 88.28%] [G loss: 4.363521]\n",
      "epoch:36 step:28884 [D loss: 0.321840, acc.: 88.28%] [G loss: 3.399199]\n",
      "epoch:36 step:28885 [D loss: 0.326922, acc.: 85.94%] [G loss: 4.113997]\n",
      "epoch:36 step:28886 [D loss: 0.265486, acc.: 88.28%] [G loss: 2.417136]\n",
      "epoch:36 step:28887 [D loss: 0.311392, acc.: 84.38%] [G loss: 2.659078]\n",
      "epoch:36 step:28888 [D loss: 0.344199, acc.: 83.59%] [G loss: 3.498877]\n",
      "epoch:36 step:28889 [D loss: 0.409654, acc.: 78.91%] [G loss: 3.726889]\n",
      "epoch:36 step:28890 [D loss: 0.479253, acc.: 77.34%] [G loss: 9.652101]\n",
      "epoch:36 step:28891 [D loss: 1.924519, acc.: 61.72%] [G loss: 6.841116]\n",
      "epoch:36 step:28892 [D loss: 2.117621, acc.: 53.91%] [G loss: 4.175286]\n",
      "epoch:36 step:28893 [D loss: 0.702533, acc.: 74.22%] [G loss: 4.987183]\n",
      "epoch:36 step:28894 [D loss: 0.270718, acc.: 86.72%] [G loss: 3.509253]\n",
      "epoch:36 step:28895 [D loss: 0.500224, acc.: 82.81%] [G loss: 6.059570]\n",
      "epoch:36 step:28896 [D loss: 0.988772, acc.: 73.44%] [G loss: 4.527471]\n",
      "epoch:36 step:28897 [D loss: 1.188962, acc.: 69.53%] [G loss: 3.404042]\n",
      "epoch:37 step:28898 [D loss: 0.399243, acc.: 82.81%] [G loss: 3.503413]\n",
      "epoch:37 step:28899 [D loss: 0.320874, acc.: 88.28%] [G loss: 2.964441]\n",
      "epoch:37 step:28900 [D loss: 0.490507, acc.: 80.47%] [G loss: 3.669822]\n",
      "epoch:37 step:28901 [D loss: 0.271409, acc.: 85.94%] [G loss: 3.059881]\n",
      "epoch:37 step:28902 [D loss: 0.283994, acc.: 86.72%] [G loss: 2.815918]\n",
      "epoch:37 step:28903 [D loss: 0.361265, acc.: 81.25%] [G loss: 2.531415]\n",
      "epoch:37 step:28904 [D loss: 0.313637, acc.: 87.50%] [G loss: 3.542495]\n",
      "epoch:37 step:28905 [D loss: 0.288370, acc.: 87.50%] [G loss: 2.997204]\n",
      "epoch:37 step:28906 [D loss: 0.390861, acc.: 81.25%] [G loss: 3.272021]\n",
      "epoch:37 step:28907 [D loss: 0.337812, acc.: 84.38%] [G loss: 2.786302]\n",
      "epoch:37 step:28908 [D loss: 0.381149, acc.: 81.25%] [G loss: 2.068587]\n",
      "epoch:37 step:28909 [D loss: 0.378125, acc.: 82.81%] [G loss: 2.706136]\n",
      "epoch:37 step:28910 [D loss: 0.316372, acc.: 86.72%] [G loss: 2.908668]\n",
      "epoch:37 step:28911 [D loss: 0.248487, acc.: 88.28%] [G loss: 2.916474]\n",
      "epoch:37 step:28912 [D loss: 0.332018, acc.: 85.94%] [G loss: 4.046236]\n",
      "epoch:37 step:28913 [D loss: 0.209210, acc.: 90.62%] [G loss: 3.045105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:28914 [D loss: 0.346300, acc.: 86.72%] [G loss: 3.283459]\n",
      "epoch:37 step:28915 [D loss: 0.260914, acc.: 87.50%] [G loss: 3.340292]\n",
      "epoch:37 step:28916 [D loss: 0.301660, acc.: 84.38%] [G loss: 2.827681]\n",
      "epoch:37 step:28917 [D loss: 0.191144, acc.: 94.53%] [G loss: 3.107481]\n",
      "epoch:37 step:28918 [D loss: 0.269164, acc.: 87.50%] [G loss: 3.154241]\n",
      "epoch:37 step:28919 [D loss: 0.299384, acc.: 88.28%] [G loss: 2.704485]\n",
      "epoch:37 step:28920 [D loss: 0.281665, acc.: 90.62%] [G loss: 2.870843]\n",
      "epoch:37 step:28921 [D loss: 0.263557, acc.: 89.84%] [G loss: 2.905396]\n",
      "epoch:37 step:28922 [D loss: 0.305922, acc.: 85.16%] [G loss: 3.258935]\n",
      "epoch:37 step:28923 [D loss: 0.179084, acc.: 92.97%] [G loss: 3.100619]\n",
      "epoch:37 step:28924 [D loss: 0.346601, acc.: 85.16%] [G loss: 2.950052]\n",
      "epoch:37 step:28925 [D loss: 0.433369, acc.: 78.91%] [G loss: 2.154010]\n",
      "epoch:37 step:28926 [D loss: 0.332064, acc.: 85.16%] [G loss: 3.286604]\n",
      "epoch:37 step:28927 [D loss: 0.288643, acc.: 87.50%] [G loss: 3.869198]\n",
      "epoch:37 step:28928 [D loss: 0.407729, acc.: 81.25%] [G loss: 12.962778]\n",
      "epoch:37 step:28929 [D loss: 0.337106, acc.: 84.38%] [G loss: 4.032215]\n",
      "epoch:37 step:28930 [D loss: 0.255158, acc.: 89.06%] [G loss: 4.099595]\n",
      "epoch:37 step:28931 [D loss: 0.281562, acc.: 90.62%] [G loss: 4.072347]\n",
      "epoch:37 step:28932 [D loss: 0.310392, acc.: 87.50%] [G loss: 3.360861]\n",
      "epoch:37 step:28933 [D loss: 0.150721, acc.: 94.53%] [G loss: 3.984526]\n",
      "epoch:37 step:28934 [D loss: 0.242892, acc.: 91.41%] [G loss: 3.888359]\n",
      "epoch:37 step:28935 [D loss: 0.348010, acc.: 83.59%] [G loss: 3.251105]\n",
      "epoch:37 step:28936 [D loss: 0.313412, acc.: 88.28%] [G loss: 2.898468]\n",
      "epoch:37 step:28937 [D loss: 0.387648, acc.: 80.47%] [G loss: 3.083423]\n",
      "epoch:37 step:28938 [D loss: 0.345307, acc.: 81.25%] [G loss: 3.341045]\n",
      "epoch:37 step:28939 [D loss: 0.311736, acc.: 85.94%] [G loss: 3.455525]\n",
      "epoch:37 step:28940 [D loss: 0.320125, acc.: 84.38%] [G loss: 2.550664]\n",
      "epoch:37 step:28941 [D loss: 0.254223, acc.: 91.41%] [G loss: 2.298906]\n",
      "epoch:37 step:28942 [D loss: 0.285529, acc.: 88.28%] [G loss: 3.191658]\n",
      "epoch:37 step:28943 [D loss: 0.275902, acc.: 88.28%] [G loss: 3.137179]\n",
      "epoch:37 step:28944 [D loss: 0.279540, acc.: 87.50%] [G loss: 3.988381]\n",
      "epoch:37 step:28945 [D loss: 0.285695, acc.: 85.94%] [G loss: 4.260898]\n",
      "epoch:37 step:28946 [D loss: 0.187623, acc.: 92.97%] [G loss: 4.738555]\n",
      "epoch:37 step:28947 [D loss: 0.288612, acc.: 85.16%] [G loss: 3.831559]\n",
      "epoch:37 step:28948 [D loss: 0.201917, acc.: 88.28%] [G loss: 6.893441]\n",
      "epoch:37 step:28949 [D loss: 0.191516, acc.: 92.19%] [G loss: 4.607824]\n",
      "epoch:37 step:28950 [D loss: 0.148889, acc.: 93.75%] [G loss: 4.661320]\n",
      "epoch:37 step:28951 [D loss: 0.214208, acc.: 89.84%] [G loss: 3.511218]\n",
      "epoch:37 step:28952 [D loss: 0.205226, acc.: 92.97%] [G loss: 2.734991]\n",
      "epoch:37 step:28953 [D loss: 0.384046, acc.: 85.16%] [G loss: 2.975975]\n",
      "epoch:37 step:28954 [D loss: 0.359975, acc.: 84.38%] [G loss: 2.639749]\n",
      "epoch:37 step:28955 [D loss: 0.386297, acc.: 78.91%] [G loss: 2.817295]\n",
      "epoch:37 step:28956 [D loss: 0.285129, acc.: 88.28%] [G loss: 2.851071]\n",
      "epoch:37 step:28957 [D loss: 0.424276, acc.: 78.91%] [G loss: 2.918736]\n",
      "epoch:37 step:28958 [D loss: 0.222159, acc.: 91.41%] [G loss: 2.739460]\n",
      "epoch:37 step:28959 [D loss: 0.258414, acc.: 90.62%] [G loss: 2.922116]\n",
      "epoch:37 step:28960 [D loss: 0.353696, acc.: 83.59%] [G loss: 2.174041]\n",
      "epoch:37 step:28961 [D loss: 0.305543, acc.: 88.28%] [G loss: 2.943497]\n",
      "epoch:37 step:28962 [D loss: 0.416415, acc.: 82.03%] [G loss: 2.799983]\n",
      "epoch:37 step:28963 [D loss: 0.344283, acc.: 82.81%] [G loss: 3.761357]\n",
      "epoch:37 step:28964 [D loss: 0.328544, acc.: 85.94%] [G loss: 2.911036]\n",
      "epoch:37 step:28965 [D loss: 0.325337, acc.: 85.94%] [G loss: 2.687787]\n",
      "epoch:37 step:28966 [D loss: 0.263032, acc.: 86.72%] [G loss: 3.451902]\n",
      "epoch:37 step:28967 [D loss: 0.345593, acc.: 79.69%] [G loss: 3.140074]\n",
      "epoch:37 step:28968 [D loss: 0.314941, acc.: 85.16%] [G loss: 2.559821]\n",
      "epoch:37 step:28969 [D loss: 0.337701, acc.: 81.25%] [G loss: 2.887808]\n",
      "epoch:37 step:28970 [D loss: 0.277929, acc.: 85.94%] [G loss: 3.017211]\n",
      "epoch:37 step:28971 [D loss: 0.334485, acc.: 83.59%] [G loss: 3.064731]\n",
      "epoch:37 step:28972 [D loss: 0.321656, acc.: 85.94%] [G loss: 2.727980]\n",
      "epoch:37 step:28973 [D loss: 0.259123, acc.: 89.06%] [G loss: 2.722236]\n",
      "epoch:37 step:28974 [D loss: 0.299785, acc.: 84.38%] [G loss: 2.922195]\n",
      "epoch:37 step:28975 [D loss: 0.378831, acc.: 83.59%] [G loss: 2.474989]\n",
      "epoch:37 step:28976 [D loss: 0.337110, acc.: 85.16%] [G loss: 2.513195]\n",
      "epoch:37 step:28977 [D loss: 0.288491, acc.: 85.94%] [G loss: 2.562411]\n",
      "epoch:37 step:28978 [D loss: 0.315595, acc.: 88.28%] [G loss: 3.765137]\n",
      "epoch:37 step:28979 [D loss: 0.381622, acc.: 81.25%] [G loss: 3.657887]\n",
      "epoch:37 step:28980 [D loss: 0.239568, acc.: 91.41%] [G loss: 3.284700]\n",
      "epoch:37 step:28981 [D loss: 0.319332, acc.: 83.59%] [G loss: 4.176517]\n",
      "epoch:37 step:28982 [D loss: 0.277200, acc.: 90.62%] [G loss: 3.677020]\n",
      "epoch:37 step:28983 [D loss: 0.224041, acc.: 92.19%] [G loss: 3.498932]\n",
      "epoch:37 step:28984 [D loss: 0.339226, acc.: 86.72%] [G loss: 3.320319]\n",
      "epoch:37 step:28985 [D loss: 0.301024, acc.: 87.50%] [G loss: 3.321862]\n",
      "epoch:37 step:28986 [D loss: 0.346210, acc.: 84.38%] [G loss: 2.911939]\n",
      "epoch:37 step:28987 [D loss: 0.372694, acc.: 81.25%] [G loss: 2.675308]\n",
      "epoch:37 step:28988 [D loss: 0.301242, acc.: 86.72%] [G loss: 2.847121]\n",
      "epoch:37 step:28989 [D loss: 0.405745, acc.: 81.25%] [G loss: 2.600506]\n",
      "epoch:37 step:28990 [D loss: 0.338682, acc.: 83.59%] [G loss: 3.208996]\n",
      "epoch:37 step:28991 [D loss: 0.271808, acc.: 89.06%] [G loss: 3.422576]\n",
      "epoch:37 step:28992 [D loss: 0.250940, acc.: 86.72%] [G loss: 3.876746]\n",
      "epoch:37 step:28993 [D loss: 0.256964, acc.: 87.50%] [G loss: 3.568709]\n",
      "epoch:37 step:28994 [D loss: 0.286999, acc.: 84.38%] [G loss: 3.870263]\n",
      "epoch:37 step:28995 [D loss: 0.352930, acc.: 83.59%] [G loss: 3.316653]\n",
      "epoch:37 step:28996 [D loss: 0.288294, acc.: 86.72%] [G loss: 3.695939]\n",
      "epoch:37 step:28997 [D loss: 0.409569, acc.: 82.03%] [G loss: 2.934147]\n",
      "epoch:37 step:28998 [D loss: 0.368930, acc.: 85.16%] [G loss: 2.671445]\n",
      "epoch:37 step:28999 [D loss: 0.438771, acc.: 76.56%] [G loss: 2.243585]\n",
      "epoch:37 step:29000 [D loss: 0.254453, acc.: 91.41%] [G loss: 2.939684]\n",
      "epoch:37 step:29001 [D loss: 0.366274, acc.: 81.25%] [G loss: 2.605104]\n",
      "epoch:37 step:29002 [D loss: 0.425287, acc.: 82.81%] [G loss: 2.334204]\n",
      "epoch:37 step:29003 [D loss: 0.399888, acc.: 82.03%] [G loss: 2.428180]\n",
      "epoch:37 step:29004 [D loss: 0.242231, acc.: 89.06%] [G loss: 2.696269]\n",
      "epoch:37 step:29005 [D loss: 0.282245, acc.: 85.94%] [G loss: 3.162974]\n",
      "epoch:37 step:29006 [D loss: 0.416876, acc.: 78.91%] [G loss: 2.501167]\n",
      "epoch:37 step:29007 [D loss: 0.351530, acc.: 82.81%] [G loss: 3.044516]\n",
      "epoch:37 step:29008 [D loss: 0.388481, acc.: 81.25%] [G loss: 2.796210]\n",
      "epoch:37 step:29009 [D loss: 0.341873, acc.: 83.59%] [G loss: 2.901968]\n",
      "epoch:37 step:29010 [D loss: 0.325219, acc.: 84.38%] [G loss: 2.757028]\n",
      "epoch:37 step:29011 [D loss: 0.347497, acc.: 81.25%] [G loss: 2.888755]\n",
      "epoch:37 step:29012 [D loss: 0.358750, acc.: 84.38%] [G loss: 3.505151]\n",
      "epoch:37 step:29013 [D loss: 0.369801, acc.: 85.94%] [G loss: 2.691618]\n",
      "epoch:37 step:29014 [D loss: 0.395296, acc.: 83.59%] [G loss: 2.755450]\n",
      "epoch:37 step:29015 [D loss: 0.265477, acc.: 88.28%] [G loss: 2.541008]\n",
      "epoch:37 step:29016 [D loss: 0.243025, acc.: 90.62%] [G loss: 2.988040]\n",
      "epoch:37 step:29017 [D loss: 0.290236, acc.: 88.28%] [G loss: 3.164383]\n",
      "epoch:37 step:29018 [D loss: 0.308522, acc.: 85.16%] [G loss: 3.434915]\n",
      "epoch:37 step:29019 [D loss: 0.321938, acc.: 85.16%] [G loss: 2.558153]\n",
      "epoch:37 step:29020 [D loss: 0.221409, acc.: 92.19%] [G loss: 2.607588]\n",
      "epoch:37 step:29021 [D loss: 0.336642, acc.: 83.59%] [G loss: 2.776051]\n",
      "epoch:37 step:29022 [D loss: 0.366704, acc.: 80.47%] [G loss: 3.338095]\n",
      "epoch:37 step:29023 [D loss: 0.330527, acc.: 86.72%] [G loss: 4.048282]\n",
      "epoch:37 step:29024 [D loss: 0.221587, acc.: 90.62%] [G loss: 2.717966]\n",
      "epoch:37 step:29025 [D loss: 0.243138, acc.: 90.62%] [G loss: 3.930101]\n",
      "epoch:37 step:29026 [D loss: 0.240776, acc.: 91.41%] [G loss: 2.917554]\n",
      "epoch:37 step:29027 [D loss: 0.288535, acc.: 88.28%] [G loss: 3.607094]\n",
      "epoch:37 step:29028 [D loss: 0.336763, acc.: 85.94%] [G loss: 2.859175]\n",
      "epoch:37 step:29029 [D loss: 0.301718, acc.: 85.16%] [G loss: 2.853614]\n",
      "epoch:37 step:29030 [D loss: 0.335253, acc.: 85.94%] [G loss: 2.532399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29031 [D loss: 0.304765, acc.: 86.72%] [G loss: 2.361733]\n",
      "epoch:37 step:29032 [D loss: 0.317759, acc.: 87.50%] [G loss: 3.448945]\n",
      "epoch:37 step:29033 [D loss: 0.301072, acc.: 86.72%] [G loss: 2.963188]\n",
      "epoch:37 step:29034 [D loss: 0.359591, acc.: 83.59%] [G loss: 2.810000]\n",
      "epoch:37 step:29035 [D loss: 0.291713, acc.: 88.28%] [G loss: 2.787042]\n",
      "epoch:37 step:29036 [D loss: 0.279548, acc.: 87.50%] [G loss: 3.299506]\n",
      "epoch:37 step:29037 [D loss: 0.360599, acc.: 85.94%] [G loss: 2.497935]\n",
      "epoch:37 step:29038 [D loss: 0.288348, acc.: 89.06%] [G loss: 3.030639]\n",
      "epoch:37 step:29039 [D loss: 0.489689, acc.: 75.78%] [G loss: 3.682990]\n",
      "epoch:37 step:29040 [D loss: 0.253497, acc.: 91.41%] [G loss: 1.901673]\n",
      "epoch:37 step:29041 [D loss: 0.507949, acc.: 74.22%] [G loss: 5.014544]\n",
      "epoch:37 step:29042 [D loss: 0.738873, acc.: 71.88%] [G loss: 5.251863]\n",
      "epoch:37 step:29043 [D loss: 0.463357, acc.: 82.81%] [G loss: 4.177092]\n",
      "epoch:37 step:29044 [D loss: 0.471591, acc.: 81.25%] [G loss: 3.207885]\n",
      "epoch:37 step:29045 [D loss: 0.367231, acc.: 84.38%] [G loss: 2.617100]\n",
      "epoch:37 step:29046 [D loss: 0.373470, acc.: 82.81%] [G loss: 4.006333]\n",
      "epoch:37 step:29047 [D loss: 0.331097, acc.: 83.59%] [G loss: 3.797772]\n",
      "epoch:37 step:29048 [D loss: 0.506371, acc.: 77.34%] [G loss: 3.297214]\n",
      "epoch:37 step:29049 [D loss: 0.399694, acc.: 78.12%] [G loss: 2.817985]\n",
      "epoch:37 step:29050 [D loss: 0.323091, acc.: 85.16%] [G loss: 2.854191]\n",
      "epoch:37 step:29051 [D loss: 0.350423, acc.: 82.81%] [G loss: 2.782595]\n",
      "epoch:37 step:29052 [D loss: 0.293978, acc.: 85.94%] [G loss: 2.393978]\n",
      "epoch:37 step:29053 [D loss: 0.261306, acc.: 87.50%] [G loss: 2.747374]\n",
      "epoch:37 step:29054 [D loss: 0.337648, acc.: 87.50%] [G loss: 3.021952]\n",
      "epoch:37 step:29055 [D loss: 0.325148, acc.: 85.94%] [G loss: 3.351468]\n",
      "epoch:37 step:29056 [D loss: 0.288022, acc.: 85.94%] [G loss: 3.646392]\n",
      "epoch:37 step:29057 [D loss: 0.407321, acc.: 82.03%] [G loss: 4.345419]\n",
      "epoch:37 step:29058 [D loss: 0.433419, acc.: 79.69%] [G loss: 2.554225]\n",
      "epoch:37 step:29059 [D loss: 0.145094, acc.: 95.31%] [G loss: 3.563447]\n",
      "epoch:37 step:29060 [D loss: 0.244934, acc.: 88.28%] [G loss: 3.721631]\n",
      "epoch:37 step:29061 [D loss: 0.328173, acc.: 89.84%] [G loss: 2.661288]\n",
      "epoch:37 step:29062 [D loss: 0.251757, acc.: 89.84%] [G loss: 3.137357]\n",
      "epoch:37 step:29063 [D loss: 0.263330, acc.: 89.84%] [G loss: 3.118790]\n",
      "epoch:37 step:29064 [D loss: 0.355201, acc.: 82.03%] [G loss: 3.091920]\n",
      "epoch:37 step:29065 [D loss: 0.294774, acc.: 89.06%] [G loss: 2.997761]\n",
      "epoch:37 step:29066 [D loss: 0.299049, acc.: 83.59%] [G loss: 2.997189]\n",
      "epoch:37 step:29067 [D loss: 0.416155, acc.: 78.91%] [G loss: 2.387788]\n",
      "epoch:37 step:29068 [D loss: 0.272474, acc.: 90.62%] [G loss: 2.314193]\n",
      "epoch:37 step:29069 [D loss: 0.423393, acc.: 80.47%] [G loss: 2.005762]\n",
      "epoch:37 step:29070 [D loss: 0.291708, acc.: 88.28%] [G loss: 2.477368]\n",
      "epoch:37 step:29071 [D loss: 0.276093, acc.: 88.28%] [G loss: 3.100291]\n",
      "epoch:37 step:29072 [D loss: 0.264868, acc.: 89.06%] [G loss: 2.767929]\n",
      "epoch:37 step:29073 [D loss: 0.342835, acc.: 85.16%] [G loss: 2.409712]\n",
      "epoch:37 step:29074 [D loss: 0.256189, acc.: 91.41%] [G loss: 2.742402]\n",
      "epoch:37 step:29075 [D loss: 0.250338, acc.: 93.75%] [G loss: 2.642218]\n",
      "epoch:37 step:29076 [D loss: 0.372971, acc.: 81.25%] [G loss: 2.924174]\n",
      "epoch:37 step:29077 [D loss: 0.449703, acc.: 80.47%] [G loss: 2.968052]\n",
      "epoch:37 step:29078 [D loss: 0.471918, acc.: 80.47%] [G loss: 2.522273]\n",
      "epoch:37 step:29079 [D loss: 0.342482, acc.: 85.16%] [G loss: 3.320852]\n",
      "epoch:37 step:29080 [D loss: 0.295047, acc.: 88.28%] [G loss: 3.863901]\n",
      "epoch:37 step:29081 [D loss: 0.413499, acc.: 81.25%] [G loss: 3.154007]\n",
      "epoch:37 step:29082 [D loss: 0.295196, acc.: 85.94%] [G loss: 3.066915]\n",
      "epoch:37 step:29083 [D loss: 0.340070, acc.: 85.16%] [G loss: 2.878496]\n",
      "epoch:37 step:29084 [D loss: 0.316301, acc.: 82.81%] [G loss: 3.880689]\n",
      "epoch:37 step:29085 [D loss: 0.370619, acc.: 84.38%] [G loss: 2.514494]\n",
      "epoch:37 step:29086 [D loss: 0.359772, acc.: 81.25%] [G loss: 3.335813]\n",
      "epoch:37 step:29087 [D loss: 0.325456, acc.: 83.59%] [G loss: 3.051968]\n",
      "epoch:37 step:29088 [D loss: 0.330556, acc.: 85.16%] [G loss: 3.075652]\n",
      "epoch:37 step:29089 [D loss: 0.285865, acc.: 84.38%] [G loss: 3.158026]\n",
      "epoch:37 step:29090 [D loss: 0.300581, acc.: 85.94%] [G loss: 4.313287]\n",
      "epoch:37 step:29091 [D loss: 0.305994, acc.: 88.28%] [G loss: 4.136974]\n",
      "epoch:37 step:29092 [D loss: 0.269580, acc.: 88.28%] [G loss: 3.353198]\n",
      "epoch:37 step:29093 [D loss: 0.263480, acc.: 89.84%] [G loss: 3.586916]\n",
      "epoch:37 step:29094 [D loss: 0.417513, acc.: 81.25%] [G loss: 2.640153]\n",
      "epoch:37 step:29095 [D loss: 0.316250, acc.: 85.16%] [G loss: 3.076526]\n",
      "epoch:37 step:29096 [D loss: 0.304278, acc.: 86.72%] [G loss: 3.478014]\n",
      "epoch:37 step:29097 [D loss: 0.246250, acc.: 88.28%] [G loss: 3.711800]\n",
      "epoch:37 step:29098 [D loss: 0.412490, acc.: 85.94%] [G loss: 3.541753]\n",
      "epoch:37 step:29099 [D loss: 0.298664, acc.: 86.72%] [G loss: 4.145999]\n",
      "epoch:37 step:29100 [D loss: 0.426084, acc.: 81.25%] [G loss: 2.896688]\n",
      "epoch:37 step:29101 [D loss: 0.305629, acc.: 86.72%] [G loss: 3.085802]\n",
      "epoch:37 step:29102 [D loss: 0.285609, acc.: 88.28%] [G loss: 2.835630]\n",
      "epoch:37 step:29103 [D loss: 0.332509, acc.: 84.38%] [G loss: 2.952094]\n",
      "epoch:37 step:29104 [D loss: 0.406485, acc.: 77.34%] [G loss: 3.247407]\n",
      "epoch:37 step:29105 [D loss: 0.385650, acc.: 82.03%] [G loss: 5.197598]\n",
      "epoch:37 step:29106 [D loss: 0.373618, acc.: 78.91%] [G loss: 3.238198]\n",
      "epoch:37 step:29107 [D loss: 0.271059, acc.: 89.84%] [G loss: 3.312192]\n",
      "epoch:37 step:29108 [D loss: 0.274424, acc.: 85.94%] [G loss: 3.543597]\n",
      "epoch:37 step:29109 [D loss: 0.271690, acc.: 85.94%] [G loss: 3.205060]\n",
      "epoch:37 step:29110 [D loss: 0.358739, acc.: 82.81%] [G loss: 3.143973]\n",
      "epoch:37 step:29111 [D loss: 0.322189, acc.: 85.94%] [G loss: 3.571890]\n",
      "epoch:37 step:29112 [D loss: 0.324972, acc.: 85.16%] [G loss: 2.788188]\n",
      "epoch:37 step:29113 [D loss: 0.336634, acc.: 84.38%] [G loss: 2.420953]\n",
      "epoch:37 step:29114 [D loss: 0.352611, acc.: 85.16%] [G loss: 2.963562]\n",
      "epoch:37 step:29115 [D loss: 0.370199, acc.: 82.03%] [G loss: 2.593579]\n",
      "epoch:37 step:29116 [D loss: 0.318897, acc.: 84.38%] [G loss: 2.589550]\n",
      "epoch:37 step:29117 [D loss: 0.332805, acc.: 84.38%] [G loss: 3.523612]\n",
      "epoch:37 step:29118 [D loss: 0.350642, acc.: 88.28%] [G loss: 2.596404]\n",
      "epoch:37 step:29119 [D loss: 0.286506, acc.: 87.50%] [G loss: 3.261073]\n",
      "epoch:37 step:29120 [D loss: 0.348720, acc.: 83.59%] [G loss: 3.106156]\n",
      "epoch:37 step:29121 [D loss: 0.338879, acc.: 87.50%] [G loss: 3.533906]\n",
      "epoch:37 step:29122 [D loss: 0.387532, acc.: 81.25%] [G loss: 4.168440]\n",
      "epoch:37 step:29123 [D loss: 0.327846, acc.: 86.72%] [G loss: 2.610993]\n",
      "epoch:37 step:29124 [D loss: 0.202190, acc.: 94.53%] [G loss: 3.379021]\n",
      "epoch:37 step:29125 [D loss: 0.402489, acc.: 79.69%] [G loss: 2.918688]\n",
      "epoch:37 step:29126 [D loss: 0.432636, acc.: 74.22%] [G loss: 2.389798]\n",
      "epoch:37 step:29127 [D loss: 0.225281, acc.: 91.41%] [G loss: 3.681620]\n",
      "epoch:37 step:29128 [D loss: 0.223236, acc.: 89.84%] [G loss: 3.088871]\n",
      "epoch:37 step:29129 [D loss: 0.236731, acc.: 87.50%] [G loss: 3.899477]\n",
      "epoch:37 step:29130 [D loss: 0.332499, acc.: 82.03%] [G loss: 2.526027]\n",
      "epoch:37 step:29131 [D loss: 0.288639, acc.: 86.72%] [G loss: 3.509646]\n",
      "epoch:37 step:29132 [D loss: 0.363831, acc.: 87.50%] [G loss: 2.962864]\n",
      "epoch:37 step:29133 [D loss: 0.297258, acc.: 85.94%] [G loss: 3.404979]\n",
      "epoch:37 step:29134 [D loss: 0.318140, acc.: 85.94%] [G loss: 3.939430]\n",
      "epoch:37 step:29135 [D loss: 0.347996, acc.: 83.59%] [G loss: 2.910865]\n",
      "epoch:37 step:29136 [D loss: 0.316375, acc.: 83.59%] [G loss: 3.580124]\n",
      "epoch:37 step:29137 [D loss: 0.406470, acc.: 79.69%] [G loss: 3.435359]\n",
      "epoch:37 step:29138 [D loss: 0.322800, acc.: 87.50%] [G loss: 3.848770]\n",
      "epoch:37 step:29139 [D loss: 0.366395, acc.: 84.38%] [G loss: 4.788715]\n",
      "epoch:37 step:29140 [D loss: 0.314469, acc.: 91.41%] [G loss: 4.258689]\n",
      "epoch:37 step:29141 [D loss: 0.477067, acc.: 77.34%] [G loss: 3.705317]\n",
      "epoch:37 step:29142 [D loss: 0.288014, acc.: 86.72%] [G loss: 2.474942]\n",
      "epoch:37 step:29143 [D loss: 0.300643, acc.: 88.28%] [G loss: 3.369785]\n",
      "epoch:37 step:29144 [D loss: 0.447099, acc.: 77.34%] [G loss: 3.819561]\n",
      "epoch:37 step:29145 [D loss: 0.249322, acc.: 89.84%] [G loss: 4.019660]\n",
      "epoch:37 step:29146 [D loss: 0.417055, acc.: 79.69%] [G loss: 3.881419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29147 [D loss: 0.411385, acc.: 79.69%] [G loss: 4.423171]\n",
      "epoch:37 step:29148 [D loss: 0.355232, acc.: 83.59%] [G loss: 3.689430]\n",
      "epoch:37 step:29149 [D loss: 0.317664, acc.: 85.16%] [G loss: 4.528743]\n",
      "epoch:37 step:29150 [D loss: 0.220355, acc.: 91.41%] [G loss: 5.344463]\n",
      "epoch:37 step:29151 [D loss: 0.176442, acc.: 91.41%] [G loss: 5.032218]\n",
      "epoch:37 step:29152 [D loss: 0.315739, acc.: 85.94%] [G loss: 4.759966]\n",
      "epoch:37 step:29153 [D loss: 0.294969, acc.: 86.72%] [G loss: 3.661027]\n",
      "epoch:37 step:29154 [D loss: 0.334172, acc.: 81.25%] [G loss: 2.534505]\n",
      "epoch:37 step:29155 [D loss: 0.265307, acc.: 89.06%] [G loss: 2.514275]\n",
      "epoch:37 step:29156 [D loss: 0.335154, acc.: 84.38%] [G loss: 4.829332]\n",
      "epoch:37 step:29157 [D loss: 0.246225, acc.: 88.28%] [G loss: 3.540555]\n",
      "epoch:37 step:29158 [D loss: 0.346301, acc.: 82.81%] [G loss: 3.935717]\n",
      "epoch:37 step:29159 [D loss: 0.207225, acc.: 91.41%] [G loss: 3.910679]\n",
      "epoch:37 step:29160 [D loss: 0.176834, acc.: 93.75%] [G loss: 3.559733]\n",
      "epoch:37 step:29161 [D loss: 0.318706, acc.: 83.59%] [G loss: 3.597988]\n",
      "epoch:37 step:29162 [D loss: 0.365099, acc.: 83.59%] [G loss: 2.849061]\n",
      "epoch:37 step:29163 [D loss: 0.364539, acc.: 80.47%] [G loss: 2.581792]\n",
      "epoch:37 step:29164 [D loss: 0.414442, acc.: 80.47%] [G loss: 3.286688]\n",
      "epoch:37 step:29165 [D loss: 0.267467, acc.: 86.72%] [G loss: 4.193102]\n",
      "epoch:37 step:29166 [D loss: 0.374095, acc.: 81.25%] [G loss: 3.182046]\n",
      "epoch:37 step:29167 [D loss: 0.213036, acc.: 92.97%] [G loss: 3.706845]\n",
      "epoch:37 step:29168 [D loss: 0.321443, acc.: 85.16%] [G loss: 3.575569]\n",
      "epoch:37 step:29169 [D loss: 0.365983, acc.: 84.38%] [G loss: 3.490746]\n",
      "epoch:37 step:29170 [D loss: 0.356784, acc.: 85.94%] [G loss: 3.579042]\n",
      "epoch:37 step:29171 [D loss: 0.206300, acc.: 89.84%] [G loss: 3.384446]\n",
      "epoch:37 step:29172 [D loss: 0.426089, acc.: 76.56%] [G loss: 3.006902]\n",
      "epoch:37 step:29173 [D loss: 0.276435, acc.: 85.94%] [G loss: 3.497676]\n",
      "epoch:37 step:29174 [D loss: 0.289894, acc.: 88.28%] [G loss: 3.626390]\n",
      "epoch:37 step:29175 [D loss: 0.294877, acc.: 85.94%] [G loss: 4.290200]\n",
      "epoch:37 step:29176 [D loss: 0.378667, acc.: 82.03%] [G loss: 2.808840]\n",
      "epoch:37 step:29177 [D loss: 0.229569, acc.: 92.19%] [G loss: 2.975964]\n",
      "epoch:37 step:29178 [D loss: 0.352850, acc.: 84.38%] [G loss: 2.737546]\n",
      "epoch:37 step:29179 [D loss: 0.250788, acc.: 89.06%] [G loss: 3.370210]\n",
      "epoch:37 step:29180 [D loss: 0.293123, acc.: 84.38%] [G loss: 3.960032]\n",
      "epoch:37 step:29181 [D loss: 0.419595, acc.: 78.12%] [G loss: 3.298525]\n",
      "epoch:37 step:29182 [D loss: 0.324381, acc.: 85.16%] [G loss: 4.104508]\n",
      "epoch:37 step:29183 [D loss: 0.276042, acc.: 87.50%] [G loss: 3.647421]\n",
      "epoch:37 step:29184 [D loss: 0.339474, acc.: 82.81%] [G loss: 3.955266]\n",
      "epoch:37 step:29185 [D loss: 0.339050, acc.: 82.03%] [G loss: 3.394064]\n",
      "epoch:37 step:29186 [D loss: 0.407282, acc.: 78.91%] [G loss: 4.005313]\n",
      "epoch:37 step:29187 [D loss: 0.269396, acc.: 89.06%] [G loss: 3.649299]\n",
      "epoch:37 step:29188 [D loss: 0.422514, acc.: 82.03%] [G loss: 4.268863]\n",
      "epoch:37 step:29189 [D loss: 0.315761, acc.: 87.50%] [G loss: 3.693076]\n",
      "epoch:37 step:29190 [D loss: 0.337276, acc.: 82.03%] [G loss: 3.392151]\n",
      "epoch:37 step:29191 [D loss: 0.319731, acc.: 87.50%] [G loss: 3.512771]\n",
      "epoch:37 step:29192 [D loss: 0.518295, acc.: 73.44%] [G loss: 9.257934]\n",
      "epoch:37 step:29193 [D loss: 0.529967, acc.: 78.91%] [G loss: 8.037017]\n",
      "epoch:37 step:29194 [D loss: 0.757715, acc.: 78.12%] [G loss: 5.324676]\n",
      "epoch:37 step:29195 [D loss: 0.496747, acc.: 78.12%] [G loss: 6.919406]\n",
      "epoch:37 step:29196 [D loss: 0.255226, acc.: 88.28%] [G loss: 4.362236]\n",
      "epoch:37 step:29197 [D loss: 0.244047, acc.: 87.50%] [G loss: 4.366230]\n",
      "epoch:37 step:29198 [D loss: 0.391588, acc.: 83.59%] [G loss: 4.220023]\n",
      "epoch:37 step:29199 [D loss: 0.288136, acc.: 85.16%] [G loss: 2.701968]\n",
      "epoch:37 step:29200 [D loss: 0.505065, acc.: 81.25%] [G loss: 3.267393]\n",
      "epoch:37 step:29201 [D loss: 0.404319, acc.: 85.16%] [G loss: 4.172615]\n",
      "epoch:37 step:29202 [D loss: 0.402564, acc.: 78.91%] [G loss: 3.001503]\n",
      "epoch:37 step:29203 [D loss: 0.369184, acc.: 83.59%] [G loss: 5.385986]\n",
      "epoch:37 step:29204 [D loss: 0.298515, acc.: 82.81%] [G loss: 3.891716]\n",
      "epoch:37 step:29205 [D loss: 0.336416, acc.: 83.59%] [G loss: 4.004041]\n",
      "epoch:37 step:29206 [D loss: 0.316941, acc.: 82.03%] [G loss: 3.922091]\n",
      "epoch:37 step:29207 [D loss: 0.216474, acc.: 91.41%] [G loss: 3.655613]\n",
      "epoch:37 step:29208 [D loss: 0.237497, acc.: 88.28%] [G loss: 3.718277]\n",
      "epoch:37 step:29209 [D loss: 0.291331, acc.: 89.06%] [G loss: 3.176482]\n",
      "epoch:37 step:29210 [D loss: 0.278550, acc.: 88.28%] [G loss: 3.145318]\n",
      "epoch:37 step:29211 [D loss: 0.337522, acc.: 85.94%] [G loss: 3.350476]\n",
      "epoch:37 step:29212 [D loss: 0.364792, acc.: 83.59%] [G loss: 2.903641]\n",
      "epoch:37 step:29213 [D loss: 0.285599, acc.: 86.72%] [G loss: 3.883697]\n",
      "epoch:37 step:29214 [D loss: 0.326764, acc.: 82.03%] [G loss: 4.049045]\n",
      "epoch:37 step:29215 [D loss: 0.298441, acc.: 85.16%] [G loss: 3.746146]\n",
      "epoch:37 step:29216 [D loss: 0.327880, acc.: 83.59%] [G loss: 4.246067]\n",
      "epoch:37 step:29217 [D loss: 0.323264, acc.: 85.16%] [G loss: 3.702821]\n",
      "epoch:37 step:29218 [D loss: 0.347616, acc.: 82.03%] [G loss: 3.091816]\n",
      "epoch:37 step:29219 [D loss: 0.355930, acc.: 85.16%] [G loss: 2.888296]\n",
      "epoch:37 step:29220 [D loss: 0.265571, acc.: 90.62%] [G loss: 2.444311]\n",
      "epoch:37 step:29221 [D loss: 0.373843, acc.: 85.16%] [G loss: 2.795493]\n",
      "epoch:37 step:29222 [D loss: 0.411090, acc.: 78.91%] [G loss: 2.908605]\n",
      "epoch:37 step:29223 [D loss: 0.442893, acc.: 81.25%] [G loss: 4.014801]\n",
      "epoch:37 step:29224 [D loss: 0.411096, acc.: 80.47%] [G loss: 2.539184]\n",
      "epoch:37 step:29225 [D loss: 0.256163, acc.: 89.84%] [G loss: 2.763518]\n",
      "epoch:37 step:29226 [D loss: 0.423581, acc.: 81.25%] [G loss: 3.084535]\n",
      "epoch:37 step:29227 [D loss: 0.282438, acc.: 85.16%] [G loss: 3.756545]\n",
      "epoch:37 step:29228 [D loss: 0.337929, acc.: 82.03%] [G loss: 2.732449]\n",
      "epoch:37 step:29229 [D loss: 0.319756, acc.: 87.50%] [G loss: 4.548540]\n",
      "epoch:37 step:29230 [D loss: 0.441327, acc.: 80.47%] [G loss: 3.110721]\n",
      "epoch:37 step:29231 [D loss: 0.310267, acc.: 85.16%] [G loss: 2.793670]\n",
      "epoch:37 step:29232 [D loss: 0.302672, acc.: 87.50%] [G loss: 3.199029]\n",
      "epoch:37 step:29233 [D loss: 0.262843, acc.: 86.72%] [G loss: 3.665002]\n",
      "epoch:37 step:29234 [D loss: 0.315678, acc.: 87.50%] [G loss: 4.102224]\n",
      "epoch:37 step:29235 [D loss: 0.238129, acc.: 91.41%] [G loss: 2.709704]\n",
      "epoch:37 step:29236 [D loss: 0.218676, acc.: 89.84%] [G loss: 3.645523]\n",
      "epoch:37 step:29237 [D loss: 0.246686, acc.: 90.62%] [G loss: 2.518920]\n",
      "epoch:37 step:29238 [D loss: 0.273965, acc.: 87.50%] [G loss: 3.596084]\n",
      "epoch:37 step:29239 [D loss: 0.320138, acc.: 85.16%] [G loss: 2.616396]\n",
      "epoch:37 step:29240 [D loss: 0.327311, acc.: 86.72%] [G loss: 3.236738]\n",
      "epoch:37 step:29241 [D loss: 0.361758, acc.: 81.25%] [G loss: 4.046956]\n",
      "epoch:37 step:29242 [D loss: 0.332190, acc.: 84.38%] [G loss: 4.168828]\n",
      "epoch:37 step:29243 [D loss: 0.266735, acc.: 89.06%] [G loss: 4.843145]\n",
      "epoch:37 step:29244 [D loss: 0.160503, acc.: 92.97%] [G loss: 3.539768]\n",
      "epoch:37 step:29245 [D loss: 0.420779, acc.: 80.47%] [G loss: 4.201615]\n",
      "epoch:37 step:29246 [D loss: 0.278056, acc.: 89.06%] [G loss: 4.020994]\n",
      "epoch:37 step:29247 [D loss: 0.385044, acc.: 77.34%] [G loss: 3.479165]\n",
      "epoch:37 step:29248 [D loss: 0.273627, acc.: 88.28%] [G loss: 2.870811]\n",
      "epoch:37 step:29249 [D loss: 0.298060, acc.: 85.94%] [G loss: 7.183117]\n",
      "epoch:37 step:29250 [D loss: 0.376437, acc.: 84.38%] [G loss: 4.957621]\n",
      "epoch:37 step:29251 [D loss: 0.326523, acc.: 81.25%] [G loss: 5.053493]\n",
      "epoch:37 step:29252 [D loss: 0.381397, acc.: 82.03%] [G loss: 4.084421]\n",
      "epoch:37 step:29253 [D loss: 0.343761, acc.: 85.94%] [G loss: 4.005027]\n",
      "epoch:37 step:29254 [D loss: 0.508951, acc.: 80.47%] [G loss: 3.350731]\n",
      "epoch:37 step:29255 [D loss: 0.284579, acc.: 86.72%] [G loss: 3.220632]\n",
      "epoch:37 step:29256 [D loss: 0.290469, acc.: 85.94%] [G loss: 2.907080]\n",
      "epoch:37 step:29257 [D loss: 0.280693, acc.: 89.06%] [G loss: 2.630360]\n",
      "epoch:37 step:29258 [D loss: 0.267679, acc.: 87.50%] [G loss: 3.174522]\n",
      "epoch:37 step:29259 [D loss: 0.346333, acc.: 85.16%] [G loss: 2.792830]\n",
      "epoch:37 step:29260 [D loss: 0.386707, acc.: 82.03%] [G loss: 2.371651]\n",
      "epoch:37 step:29261 [D loss: 0.316738, acc.: 85.16%] [G loss: 2.953752]\n",
      "epoch:37 step:29262 [D loss: 0.299015, acc.: 86.72%] [G loss: 2.806108]\n",
      "epoch:37 step:29263 [D loss: 0.343663, acc.: 83.59%] [G loss: 2.845897]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29264 [D loss: 0.332351, acc.: 88.28%] [G loss: 2.992132]\n",
      "epoch:37 step:29265 [D loss: 0.341187, acc.: 85.16%] [G loss: 3.280965]\n",
      "epoch:37 step:29266 [D loss: 0.273147, acc.: 91.41%] [G loss: 2.428168]\n",
      "epoch:37 step:29267 [D loss: 0.321700, acc.: 85.94%] [G loss: 2.767827]\n",
      "epoch:37 step:29268 [D loss: 0.284416, acc.: 85.94%] [G loss: 2.564155]\n",
      "epoch:37 step:29269 [D loss: 0.348059, acc.: 83.59%] [G loss: 2.771188]\n",
      "epoch:37 step:29270 [D loss: 0.295564, acc.: 85.94%] [G loss: 2.672811]\n",
      "epoch:37 step:29271 [D loss: 0.294749, acc.: 85.16%] [G loss: 3.540561]\n",
      "epoch:37 step:29272 [D loss: 0.389620, acc.: 79.69%] [G loss: 2.459172]\n",
      "epoch:37 step:29273 [D loss: 0.294541, acc.: 85.16%] [G loss: 2.807752]\n",
      "epoch:37 step:29274 [D loss: 0.379593, acc.: 82.81%] [G loss: 2.817080]\n",
      "epoch:37 step:29275 [D loss: 0.355621, acc.: 82.03%] [G loss: 2.570380]\n",
      "epoch:37 step:29276 [D loss: 0.408096, acc.: 82.03%] [G loss: 3.524877]\n",
      "epoch:37 step:29277 [D loss: 0.332050, acc.: 85.16%] [G loss: 2.226889]\n",
      "epoch:37 step:29278 [D loss: 0.305300, acc.: 87.50%] [G loss: 2.510250]\n",
      "epoch:37 step:29279 [D loss: 0.386987, acc.: 83.59%] [G loss: 3.665965]\n",
      "epoch:37 step:29280 [D loss: 0.322518, acc.: 85.94%] [G loss: 2.936944]\n",
      "epoch:37 step:29281 [D loss: 0.276301, acc.: 86.72%] [G loss: 3.201499]\n",
      "epoch:37 step:29282 [D loss: 0.333384, acc.: 82.03%] [G loss: 2.926697]\n",
      "epoch:37 step:29283 [D loss: 0.376149, acc.: 80.47%] [G loss: 3.021255]\n",
      "epoch:37 step:29284 [D loss: 0.326481, acc.: 85.94%] [G loss: 3.389045]\n",
      "epoch:37 step:29285 [D loss: 0.320878, acc.: 88.28%] [G loss: 3.229099]\n",
      "epoch:37 step:29286 [D loss: 0.346590, acc.: 82.81%] [G loss: 2.814065]\n",
      "epoch:37 step:29287 [D loss: 0.321268, acc.: 85.94%] [G loss: 3.481070]\n",
      "epoch:37 step:29288 [D loss: 0.369039, acc.: 81.25%] [G loss: 3.469981]\n",
      "epoch:37 step:29289 [D loss: 0.319615, acc.: 85.94%] [G loss: 3.559404]\n",
      "epoch:37 step:29290 [D loss: 0.332603, acc.: 84.38%] [G loss: 3.987906]\n",
      "epoch:37 step:29291 [D loss: 0.259779, acc.: 88.28%] [G loss: 3.380349]\n",
      "epoch:37 step:29292 [D loss: 0.296618, acc.: 85.16%] [G loss: 3.913033]\n",
      "epoch:37 step:29293 [D loss: 0.388625, acc.: 79.69%] [G loss: 3.394666]\n",
      "epoch:37 step:29294 [D loss: 0.443605, acc.: 81.25%] [G loss: 2.714331]\n",
      "epoch:37 step:29295 [D loss: 0.355965, acc.: 87.50%] [G loss: 2.263910]\n",
      "epoch:37 step:29296 [D loss: 0.425745, acc.: 77.34%] [G loss: 2.544067]\n",
      "epoch:37 step:29297 [D loss: 0.305884, acc.: 86.72%] [G loss: 3.132515]\n",
      "epoch:37 step:29298 [D loss: 0.396346, acc.: 81.25%] [G loss: 2.730208]\n",
      "epoch:37 step:29299 [D loss: 0.354298, acc.: 85.94%] [G loss: 2.757535]\n",
      "epoch:37 step:29300 [D loss: 0.246419, acc.: 91.41%] [G loss: 3.081453]\n",
      "epoch:37 step:29301 [D loss: 0.436774, acc.: 79.69%] [G loss: 2.653106]\n",
      "epoch:37 step:29302 [D loss: 0.394108, acc.: 82.81%] [G loss: 3.006502]\n",
      "epoch:37 step:29303 [D loss: 0.436371, acc.: 79.69%] [G loss: 2.553519]\n",
      "epoch:37 step:29304 [D loss: 0.386677, acc.: 82.81%] [G loss: 2.758338]\n",
      "epoch:37 step:29305 [D loss: 0.225206, acc.: 89.84%] [G loss: 3.706942]\n",
      "epoch:37 step:29306 [D loss: 0.341616, acc.: 89.06%] [G loss: 2.827589]\n",
      "epoch:37 step:29307 [D loss: 0.355346, acc.: 82.81%] [G loss: 2.571286]\n",
      "epoch:37 step:29308 [D loss: 0.359776, acc.: 88.28%] [G loss: 2.618394]\n",
      "epoch:37 step:29309 [D loss: 0.332870, acc.: 82.81%] [G loss: 3.620518]\n",
      "epoch:37 step:29310 [D loss: 0.266580, acc.: 87.50%] [G loss: 4.635427]\n",
      "epoch:37 step:29311 [D loss: 0.342589, acc.: 85.16%] [G loss: 3.717226]\n",
      "epoch:37 step:29312 [D loss: 0.420150, acc.: 82.03%] [G loss: 4.908314]\n",
      "epoch:37 step:29313 [D loss: 0.841722, acc.: 79.69%] [G loss: 7.915462]\n",
      "epoch:37 step:29314 [D loss: 2.024167, acc.: 57.03%] [G loss: 4.861075]\n",
      "epoch:37 step:29315 [D loss: 0.935364, acc.: 76.56%] [G loss: 3.322349]\n",
      "epoch:37 step:29316 [D loss: 0.478641, acc.: 77.34%] [G loss: 3.776821]\n",
      "epoch:37 step:29317 [D loss: 0.415698, acc.: 82.03%] [G loss: 4.296142]\n",
      "epoch:37 step:29318 [D loss: 0.381416, acc.: 83.59%] [G loss: 3.756565]\n",
      "epoch:37 step:29319 [D loss: 0.262469, acc.: 89.06%] [G loss: 3.684658]\n",
      "epoch:37 step:29320 [D loss: 0.325368, acc.: 87.50%] [G loss: 3.841347]\n",
      "epoch:37 step:29321 [D loss: 0.322309, acc.: 87.50%] [G loss: 3.091644]\n",
      "epoch:37 step:29322 [D loss: 0.389251, acc.: 85.16%] [G loss: 3.056427]\n",
      "epoch:37 step:29323 [D loss: 0.374361, acc.: 85.16%] [G loss: 2.302578]\n",
      "epoch:37 step:29324 [D loss: 0.225230, acc.: 90.62%] [G loss: 3.165780]\n",
      "epoch:37 step:29325 [D loss: 0.456036, acc.: 80.47%] [G loss: 3.263267]\n",
      "epoch:37 step:29326 [D loss: 0.395631, acc.: 85.94%] [G loss: 3.436822]\n",
      "epoch:37 step:29327 [D loss: 0.371473, acc.: 81.25%] [G loss: 2.622214]\n",
      "epoch:37 step:29328 [D loss: 0.238463, acc.: 89.84%] [G loss: 3.282842]\n",
      "epoch:37 step:29329 [D loss: 0.489478, acc.: 80.47%] [G loss: 2.913088]\n",
      "epoch:37 step:29330 [D loss: 0.225759, acc.: 92.19%] [G loss: 2.437297]\n",
      "epoch:37 step:29331 [D loss: 0.308624, acc.: 85.16%] [G loss: 2.948216]\n",
      "epoch:37 step:29332 [D loss: 0.317738, acc.: 87.50%] [G loss: 2.588603]\n",
      "epoch:37 step:29333 [D loss: 0.350711, acc.: 83.59%] [G loss: 2.652872]\n",
      "epoch:37 step:29334 [D loss: 0.293069, acc.: 87.50%] [G loss: 2.931245]\n",
      "epoch:37 step:29335 [D loss: 0.438361, acc.: 78.12%] [G loss: 3.426513]\n",
      "epoch:37 step:29336 [D loss: 0.375747, acc.: 85.94%] [G loss: 5.570457]\n",
      "epoch:37 step:29337 [D loss: 0.261694, acc.: 89.84%] [G loss: 3.682939]\n",
      "epoch:37 step:29338 [D loss: 0.305483, acc.: 86.72%] [G loss: 3.795157]\n",
      "epoch:37 step:29339 [D loss: 0.262836, acc.: 86.72%] [G loss: 4.211640]\n",
      "epoch:37 step:29340 [D loss: 0.428495, acc.: 82.81%] [G loss: 4.477080]\n",
      "epoch:37 step:29341 [D loss: 0.275581, acc.: 89.06%] [G loss: 3.749112]\n",
      "epoch:37 step:29342 [D loss: 0.403184, acc.: 78.91%] [G loss: 4.425586]\n",
      "epoch:37 step:29343 [D loss: 0.370050, acc.: 79.69%] [G loss: 2.973610]\n",
      "epoch:37 step:29344 [D loss: 0.297592, acc.: 86.72%] [G loss: 2.770888]\n",
      "epoch:37 step:29345 [D loss: 0.381668, acc.: 80.47%] [G loss: 2.999150]\n",
      "epoch:37 step:29346 [D loss: 0.262054, acc.: 88.28%] [G loss: 2.889224]\n",
      "epoch:37 step:29347 [D loss: 0.280440, acc.: 92.97%] [G loss: 2.682055]\n",
      "epoch:37 step:29348 [D loss: 0.336558, acc.: 83.59%] [G loss: 3.158601]\n",
      "epoch:37 step:29349 [D loss: 0.286737, acc.: 86.72%] [G loss: 3.369474]\n",
      "epoch:37 step:29350 [D loss: 0.310832, acc.: 86.72%] [G loss: 3.466260]\n",
      "epoch:37 step:29351 [D loss: 0.278277, acc.: 89.06%] [G loss: 3.599920]\n",
      "epoch:37 step:29352 [D loss: 0.382604, acc.: 83.59%] [G loss: 2.349063]\n",
      "epoch:37 step:29353 [D loss: 0.322662, acc.: 89.06%] [G loss: 2.515013]\n",
      "epoch:37 step:29354 [D loss: 0.334518, acc.: 82.03%] [G loss: 2.773767]\n",
      "epoch:37 step:29355 [D loss: 0.222680, acc.: 92.19%] [G loss: 2.791716]\n",
      "epoch:37 step:29356 [D loss: 0.310639, acc.: 85.16%] [G loss: 2.796551]\n",
      "epoch:37 step:29357 [D loss: 0.299837, acc.: 85.94%] [G loss: 3.630076]\n",
      "epoch:37 step:29358 [D loss: 0.338639, acc.: 84.38%] [G loss: 3.806895]\n",
      "epoch:37 step:29359 [D loss: 0.213125, acc.: 93.75%] [G loss: 3.529194]\n",
      "epoch:37 step:29360 [D loss: 0.364933, acc.: 85.94%] [G loss: 3.283029]\n",
      "epoch:37 step:29361 [D loss: 0.388890, acc.: 80.47%] [G loss: 3.752687]\n",
      "epoch:37 step:29362 [D loss: 0.399871, acc.: 75.78%] [G loss: 2.979758]\n",
      "epoch:37 step:29363 [D loss: 0.262728, acc.: 89.84%] [G loss: 4.394805]\n",
      "epoch:37 step:29364 [D loss: 0.222627, acc.: 89.84%] [G loss: 4.023319]\n",
      "epoch:37 step:29365 [D loss: 0.273915, acc.: 87.50%] [G loss: 6.581289]\n",
      "epoch:37 step:29366 [D loss: 0.218500, acc.: 89.84%] [G loss: 3.443152]\n",
      "epoch:37 step:29367 [D loss: 0.281180, acc.: 85.94%] [G loss: 3.999421]\n",
      "epoch:37 step:29368 [D loss: 0.259949, acc.: 86.72%] [G loss: 4.103552]\n",
      "epoch:37 step:29369 [D loss: 0.266313, acc.: 89.84%] [G loss: 3.840299]\n",
      "epoch:37 step:29370 [D loss: 0.311517, acc.: 88.28%] [G loss: 3.191798]\n",
      "epoch:37 step:29371 [D loss: 0.394407, acc.: 83.59%] [G loss: 3.821855]\n",
      "epoch:37 step:29372 [D loss: 0.419453, acc.: 79.69%] [G loss: 3.052371]\n",
      "epoch:37 step:29373 [D loss: 0.238157, acc.: 90.62%] [G loss: 3.407377]\n",
      "epoch:37 step:29374 [D loss: 0.392990, acc.: 81.25%] [G loss: 3.413751]\n",
      "epoch:37 step:29375 [D loss: 0.253713, acc.: 89.06%] [G loss: 3.047112]\n",
      "epoch:37 step:29376 [D loss: 0.338348, acc.: 86.72%] [G loss: 3.276176]\n",
      "epoch:37 step:29377 [D loss: 0.274255, acc.: 87.50%] [G loss: 3.186952]\n",
      "epoch:37 step:29378 [D loss: 0.353219, acc.: 83.59%] [G loss: 3.127651]\n",
      "epoch:37 step:29379 [D loss: 0.410345, acc.: 81.25%] [G loss: 2.952496]\n",
      "epoch:37 step:29380 [D loss: 0.330633, acc.: 83.59%] [G loss: 3.524699]\n",
      "epoch:37 step:29381 [D loss: 0.311690, acc.: 87.50%] [G loss: 3.387845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29382 [D loss: 0.307870, acc.: 86.72%] [G loss: 3.362919]\n",
      "epoch:37 step:29383 [D loss: 0.318533, acc.: 84.38%] [G loss: 2.768963]\n",
      "epoch:37 step:29384 [D loss: 0.321317, acc.: 85.16%] [G loss: 2.707427]\n",
      "epoch:37 step:29385 [D loss: 0.309672, acc.: 85.16%] [G loss: 3.030651]\n",
      "epoch:37 step:29386 [D loss: 0.318768, acc.: 84.38%] [G loss: 2.558849]\n",
      "epoch:37 step:29387 [D loss: 0.276523, acc.: 87.50%] [G loss: 3.330333]\n",
      "epoch:37 step:29388 [D loss: 0.402201, acc.: 80.47%] [G loss: 2.391932]\n",
      "epoch:37 step:29389 [D loss: 0.306534, acc.: 89.06%] [G loss: 2.643847]\n",
      "epoch:37 step:29390 [D loss: 0.334124, acc.: 84.38%] [G loss: 2.786633]\n",
      "epoch:37 step:29391 [D loss: 0.259654, acc.: 92.97%] [G loss: 2.961838]\n",
      "epoch:37 step:29392 [D loss: 0.396904, acc.: 82.03%] [G loss: 2.848660]\n",
      "epoch:37 step:29393 [D loss: 0.398271, acc.: 82.03%] [G loss: 3.210831]\n",
      "epoch:37 step:29394 [D loss: 0.394887, acc.: 82.03%] [G loss: 3.500673]\n",
      "epoch:37 step:29395 [D loss: 0.334917, acc.: 78.91%] [G loss: 3.679913]\n",
      "epoch:37 step:29396 [D loss: 0.457727, acc.: 77.34%] [G loss: 4.043036]\n",
      "epoch:37 step:29397 [D loss: 0.412137, acc.: 78.12%] [G loss: 3.169767]\n",
      "epoch:37 step:29398 [D loss: 0.318598, acc.: 88.28%] [G loss: 2.704526]\n",
      "epoch:37 step:29399 [D loss: 0.328957, acc.: 84.38%] [G loss: 2.895091]\n",
      "epoch:37 step:29400 [D loss: 0.369111, acc.: 84.38%] [G loss: 3.759688]\n",
      "epoch:37 step:29401 [D loss: 0.283262, acc.: 89.06%] [G loss: 3.218482]\n",
      "epoch:37 step:29402 [D loss: 0.274749, acc.: 89.06%] [G loss: 3.059494]\n",
      "epoch:37 step:29403 [D loss: 0.370179, acc.: 86.72%] [G loss: 2.959987]\n",
      "epoch:37 step:29404 [D loss: 0.347162, acc.: 85.16%] [G loss: 3.327740]\n",
      "epoch:37 step:29405 [D loss: 0.251882, acc.: 87.50%] [G loss: 5.264475]\n",
      "epoch:37 step:29406 [D loss: 0.418860, acc.: 77.34%] [G loss: 4.197993]\n",
      "epoch:37 step:29407 [D loss: 0.369546, acc.: 82.81%] [G loss: 4.428953]\n",
      "epoch:37 step:29408 [D loss: 0.384840, acc.: 81.25%] [G loss: 3.139650]\n",
      "epoch:37 step:29409 [D loss: 0.379456, acc.: 82.03%] [G loss: 4.719460]\n",
      "epoch:37 step:29410 [D loss: 0.258599, acc.: 91.41%] [G loss: 5.166371]\n",
      "epoch:37 step:29411 [D loss: 0.305257, acc.: 88.28%] [G loss: 3.136649]\n",
      "epoch:37 step:29412 [D loss: 0.322874, acc.: 86.72%] [G loss: 3.189692]\n",
      "epoch:37 step:29413 [D loss: 0.300286, acc.: 84.38%] [G loss: 3.153128]\n",
      "epoch:37 step:29414 [D loss: 0.314714, acc.: 86.72%] [G loss: 3.342853]\n",
      "epoch:37 step:29415 [D loss: 0.264051, acc.: 87.50%] [G loss: 2.698379]\n",
      "epoch:37 step:29416 [D loss: 0.342900, acc.: 83.59%] [G loss: 2.946461]\n",
      "epoch:37 step:29417 [D loss: 0.427057, acc.: 79.69%] [G loss: 2.593676]\n",
      "epoch:37 step:29418 [D loss: 0.273896, acc.: 87.50%] [G loss: 2.932485]\n",
      "epoch:37 step:29419 [D loss: 0.360421, acc.: 85.94%] [G loss: 2.479795]\n",
      "epoch:37 step:29420 [D loss: 0.330307, acc.: 84.38%] [G loss: 2.473345]\n",
      "epoch:37 step:29421 [D loss: 0.355009, acc.: 82.03%] [G loss: 3.218875]\n",
      "epoch:37 step:29422 [D loss: 0.381647, acc.: 82.81%] [G loss: 2.858010]\n",
      "epoch:37 step:29423 [D loss: 0.424901, acc.: 78.91%] [G loss: 2.789774]\n",
      "epoch:37 step:29424 [D loss: 0.334107, acc.: 87.50%] [G loss: 2.618758]\n",
      "epoch:37 step:29425 [D loss: 0.227762, acc.: 91.41%] [G loss: 2.955496]\n",
      "epoch:37 step:29426 [D loss: 0.318832, acc.: 87.50%] [G loss: 2.500618]\n",
      "epoch:37 step:29427 [D loss: 0.397601, acc.: 78.91%] [G loss: 2.703839]\n",
      "epoch:37 step:29428 [D loss: 0.313609, acc.: 85.16%] [G loss: 3.191461]\n",
      "epoch:37 step:29429 [D loss: 0.306556, acc.: 86.72%] [G loss: 4.367756]\n",
      "epoch:37 step:29430 [D loss: 0.371002, acc.: 87.50%] [G loss: 5.756346]\n",
      "epoch:37 step:29431 [D loss: 0.327074, acc.: 82.03%] [G loss: 3.866706]\n",
      "epoch:37 step:29432 [D loss: 0.237503, acc.: 87.50%] [G loss: 4.387574]\n",
      "epoch:37 step:29433 [D loss: 0.191390, acc.: 91.41%] [G loss: 4.021817]\n",
      "epoch:37 step:29434 [D loss: 0.321483, acc.: 84.38%] [G loss: 2.980247]\n",
      "epoch:37 step:29435 [D loss: 0.309795, acc.: 85.94%] [G loss: 2.931336]\n",
      "epoch:37 step:29436 [D loss: 0.264817, acc.: 86.72%] [G loss: 3.348989]\n",
      "epoch:37 step:29437 [D loss: 0.370125, acc.: 82.03%] [G loss: 2.931450]\n",
      "epoch:37 step:29438 [D loss: 0.297109, acc.: 86.72%] [G loss: 3.717512]\n",
      "epoch:37 step:29439 [D loss: 0.388590, acc.: 83.59%] [G loss: 5.022939]\n",
      "epoch:37 step:29440 [D loss: 0.256550, acc.: 89.06%] [G loss: 3.588068]\n",
      "epoch:37 step:29441 [D loss: 0.306246, acc.: 82.81%] [G loss: 4.719352]\n",
      "epoch:37 step:29442 [D loss: 0.272130, acc.: 86.72%] [G loss: 4.102296]\n",
      "epoch:37 step:29443 [D loss: 0.258535, acc.: 85.16%] [G loss: 4.649376]\n",
      "epoch:37 step:29444 [D loss: 0.488065, acc.: 76.56%] [G loss: 3.929342]\n",
      "epoch:37 step:29445 [D loss: 0.195280, acc.: 92.97%] [G loss: 4.143937]\n",
      "epoch:37 step:29446 [D loss: 0.362513, acc.: 82.03%] [G loss: 3.207655]\n",
      "epoch:37 step:29447 [D loss: 0.300995, acc.: 85.16%] [G loss: 3.383832]\n",
      "epoch:37 step:29448 [D loss: 0.256300, acc.: 89.84%] [G loss: 3.139755]\n",
      "epoch:37 step:29449 [D loss: 0.427002, acc.: 82.81%] [G loss: 7.446177]\n",
      "epoch:37 step:29450 [D loss: 0.925608, acc.: 71.09%] [G loss: 3.896149]\n",
      "epoch:37 step:29451 [D loss: 0.737440, acc.: 69.53%] [G loss: 4.610198]\n",
      "epoch:37 step:29452 [D loss: 0.461611, acc.: 77.34%] [G loss: 3.806465]\n",
      "epoch:37 step:29453 [D loss: 0.432435, acc.: 75.00%] [G loss: 3.338133]\n",
      "epoch:37 step:29454 [D loss: 0.424091, acc.: 79.69%] [G loss: 2.621325]\n",
      "epoch:37 step:29455 [D loss: 0.329919, acc.: 85.94%] [G loss: 3.484164]\n",
      "epoch:37 step:29456 [D loss: 0.321140, acc.: 85.94%] [G loss: 2.776625]\n",
      "epoch:37 step:29457 [D loss: 0.384926, acc.: 78.12%] [G loss: 2.772011]\n",
      "epoch:37 step:29458 [D loss: 0.267555, acc.: 85.94%] [G loss: 3.309143]\n",
      "epoch:37 step:29459 [D loss: 0.370570, acc.: 79.69%] [G loss: 2.409732]\n",
      "epoch:37 step:29460 [D loss: 0.215520, acc.: 89.84%] [G loss: 2.577222]\n",
      "epoch:37 step:29461 [D loss: 0.227240, acc.: 91.41%] [G loss: 3.182721]\n",
      "epoch:37 step:29462 [D loss: 0.336103, acc.: 82.03%] [G loss: 2.568525]\n",
      "epoch:37 step:29463 [D loss: 0.273082, acc.: 88.28%] [G loss: 3.260026]\n",
      "epoch:37 step:29464 [D loss: 0.372611, acc.: 80.47%] [G loss: 2.649814]\n",
      "epoch:37 step:29465 [D loss: 0.382303, acc.: 81.25%] [G loss: 2.901373]\n",
      "epoch:37 step:29466 [D loss: 0.351812, acc.: 88.28%] [G loss: 3.352300]\n",
      "epoch:37 step:29467 [D loss: 0.349112, acc.: 85.94%] [G loss: 4.377469]\n",
      "epoch:37 step:29468 [D loss: 0.300942, acc.: 85.94%] [G loss: 4.600449]\n",
      "epoch:37 step:29469 [D loss: 0.307985, acc.: 88.28%] [G loss: 4.546988]\n",
      "epoch:37 step:29470 [D loss: 0.388803, acc.: 81.25%] [G loss: 4.324281]\n",
      "epoch:37 step:29471 [D loss: 0.327423, acc.: 85.16%] [G loss: 3.623128]\n",
      "epoch:37 step:29472 [D loss: 0.368636, acc.: 80.47%] [G loss: 3.678189]\n",
      "epoch:37 step:29473 [D loss: 0.273871, acc.: 87.50%] [G loss: 3.779465]\n",
      "epoch:37 step:29474 [D loss: 0.272545, acc.: 89.84%] [G loss: 3.039450]\n",
      "epoch:37 step:29475 [D loss: 0.311547, acc.: 86.72%] [G loss: 3.809295]\n",
      "epoch:37 step:29476 [D loss: 0.252325, acc.: 89.84%] [G loss: 4.069074]\n",
      "epoch:37 step:29477 [D loss: 0.352060, acc.: 82.03%] [G loss: 4.031309]\n",
      "epoch:37 step:29478 [D loss: 0.292398, acc.: 88.28%] [G loss: 3.566138]\n",
      "epoch:37 step:29479 [D loss: 0.187702, acc.: 94.53%] [G loss: 3.780405]\n",
      "epoch:37 step:29480 [D loss: 0.323387, acc.: 82.81%] [G loss: 3.332593]\n",
      "epoch:37 step:29481 [D loss: 0.338426, acc.: 82.81%] [G loss: 3.237908]\n",
      "epoch:37 step:29482 [D loss: 0.304461, acc.: 86.72%] [G loss: 3.217014]\n",
      "epoch:37 step:29483 [D loss: 0.268978, acc.: 89.06%] [G loss: 3.412081]\n",
      "epoch:37 step:29484 [D loss: 0.308332, acc.: 83.59%] [G loss: 3.169950]\n",
      "epoch:37 step:29485 [D loss: 0.272381, acc.: 89.84%] [G loss: 4.235782]\n",
      "epoch:37 step:29486 [D loss: 0.233386, acc.: 90.62%] [G loss: 3.446016]\n",
      "epoch:37 step:29487 [D loss: 0.263768, acc.: 87.50%] [G loss: 3.339755]\n",
      "epoch:37 step:29488 [D loss: 0.294294, acc.: 85.16%] [G loss: 2.892286]\n",
      "epoch:37 step:29489 [D loss: 0.436124, acc.: 78.12%] [G loss: 3.389649]\n",
      "epoch:37 step:29490 [D loss: 0.283465, acc.: 88.28%] [G loss: 4.129601]\n",
      "epoch:37 step:29491 [D loss: 0.441414, acc.: 82.03%] [G loss: 5.091452]\n",
      "epoch:37 step:29492 [D loss: 0.602094, acc.: 78.12%] [G loss: 5.746176]\n",
      "epoch:37 step:29493 [D loss: 1.401612, acc.: 66.41%] [G loss: 7.729999]\n",
      "epoch:37 step:29494 [D loss: 1.883414, acc.: 55.47%] [G loss: 5.346297]\n",
      "epoch:37 step:29495 [D loss: 0.403788, acc.: 81.25%] [G loss: 6.924488]\n",
      "epoch:37 step:29496 [D loss: 0.611087, acc.: 77.34%] [G loss: 3.258729]\n",
      "epoch:37 step:29497 [D loss: 0.314914, acc.: 84.38%] [G loss: 3.086379]\n",
      "epoch:37 step:29498 [D loss: 0.383106, acc.: 83.59%] [G loss: 4.638425]\n",
      "epoch:37 step:29499 [D loss: 0.210450, acc.: 89.06%] [G loss: 2.887569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29500 [D loss: 0.262499, acc.: 89.06%] [G loss: 4.323756]\n",
      "epoch:37 step:29501 [D loss: 0.379557, acc.: 81.25%] [G loss: 3.811386]\n",
      "epoch:37 step:29502 [D loss: 0.325491, acc.: 86.72%] [G loss: 3.555610]\n",
      "epoch:37 step:29503 [D loss: 0.271876, acc.: 86.72%] [G loss: 3.193031]\n",
      "epoch:37 step:29504 [D loss: 0.274346, acc.: 87.50%] [G loss: 3.697740]\n",
      "epoch:37 step:29505 [D loss: 0.250668, acc.: 92.19%] [G loss: 3.504622]\n",
      "epoch:37 step:29506 [D loss: 0.358886, acc.: 85.94%] [G loss: 3.573774]\n",
      "epoch:37 step:29507 [D loss: 0.368046, acc.: 85.16%] [G loss: 2.728392]\n",
      "epoch:37 step:29508 [D loss: 0.379659, acc.: 83.59%] [G loss: 2.294960]\n",
      "epoch:37 step:29509 [D loss: 0.279304, acc.: 89.84%] [G loss: 3.236755]\n",
      "epoch:37 step:29510 [D loss: 0.362472, acc.: 84.38%] [G loss: 3.233041]\n",
      "epoch:37 step:29511 [D loss: 0.301388, acc.: 87.50%] [G loss: 2.852626]\n",
      "epoch:37 step:29512 [D loss: 0.302393, acc.: 85.94%] [G loss: 2.980650]\n",
      "epoch:37 step:29513 [D loss: 0.398506, acc.: 79.69%] [G loss: 3.278159]\n",
      "epoch:37 step:29514 [D loss: 0.334608, acc.: 84.38%] [G loss: 3.148829]\n",
      "epoch:37 step:29515 [D loss: 0.463485, acc.: 77.34%] [G loss: 3.278166]\n",
      "epoch:37 step:29516 [D loss: 0.295409, acc.: 87.50%] [G loss: 2.707617]\n",
      "epoch:37 step:29517 [D loss: 0.342514, acc.: 83.59%] [G loss: 2.639431]\n",
      "epoch:37 step:29518 [D loss: 0.203604, acc.: 89.84%] [G loss: 2.379420]\n",
      "epoch:37 step:29519 [D loss: 0.339115, acc.: 83.59%] [G loss: 2.756840]\n",
      "epoch:37 step:29520 [D loss: 0.441378, acc.: 84.38%] [G loss: 2.848762]\n",
      "epoch:37 step:29521 [D loss: 0.376398, acc.: 77.34%] [G loss: 2.716755]\n",
      "epoch:37 step:29522 [D loss: 0.334295, acc.: 87.50%] [G loss: 2.522910]\n",
      "epoch:37 step:29523 [D loss: 0.322099, acc.: 86.72%] [G loss: 3.164290]\n",
      "epoch:37 step:29524 [D loss: 0.299983, acc.: 87.50%] [G loss: 2.320379]\n",
      "epoch:37 step:29525 [D loss: 0.364374, acc.: 82.03%] [G loss: 2.236366]\n",
      "epoch:37 step:29526 [D loss: 0.285321, acc.: 92.19%] [G loss: 2.187838]\n",
      "epoch:37 step:29527 [D loss: 0.315299, acc.: 84.38%] [G loss: 2.645421]\n",
      "epoch:37 step:29528 [D loss: 0.367139, acc.: 85.94%] [G loss: 2.356832]\n",
      "epoch:37 step:29529 [D loss: 0.296127, acc.: 85.94%] [G loss: 2.443249]\n",
      "epoch:37 step:29530 [D loss: 0.403174, acc.: 80.47%] [G loss: 2.433863]\n",
      "epoch:37 step:29531 [D loss: 0.316919, acc.: 85.94%] [G loss: 2.655117]\n",
      "epoch:37 step:29532 [D loss: 0.389472, acc.: 79.69%] [G loss: 2.808972]\n",
      "epoch:37 step:29533 [D loss: 0.332930, acc.: 83.59%] [G loss: 2.645896]\n",
      "epoch:37 step:29534 [D loss: 0.341850, acc.: 81.25%] [G loss: 2.945758]\n",
      "epoch:37 step:29535 [D loss: 0.377655, acc.: 82.81%] [G loss: 3.330883]\n",
      "epoch:37 step:29536 [D loss: 0.445712, acc.: 82.81%] [G loss: 3.687394]\n",
      "epoch:37 step:29537 [D loss: 0.364632, acc.: 83.59%] [G loss: 2.431740]\n",
      "epoch:37 step:29538 [D loss: 0.245666, acc.: 88.28%] [G loss: 3.760818]\n",
      "epoch:37 step:29539 [D loss: 0.449837, acc.: 84.38%] [G loss: 3.200537]\n",
      "epoch:37 step:29540 [D loss: 0.296054, acc.: 87.50%] [G loss: 3.352023]\n",
      "epoch:37 step:29541 [D loss: 0.227973, acc.: 88.28%] [G loss: 4.404194]\n",
      "epoch:37 step:29542 [D loss: 0.259393, acc.: 89.06%] [G loss: 3.583672]\n",
      "epoch:37 step:29543 [D loss: 0.410020, acc.: 77.34%] [G loss: 3.187275]\n",
      "epoch:37 step:29544 [D loss: 0.270521, acc.: 90.62%] [G loss: 3.417762]\n",
      "epoch:37 step:29545 [D loss: 0.360516, acc.: 82.03%] [G loss: 2.956391]\n",
      "epoch:37 step:29546 [D loss: 0.322966, acc.: 85.94%] [G loss: 3.462097]\n",
      "epoch:37 step:29547 [D loss: 0.283925, acc.: 84.38%] [G loss: 3.476816]\n",
      "epoch:37 step:29548 [D loss: 0.288889, acc.: 87.50%] [G loss: 3.381376]\n",
      "epoch:37 step:29549 [D loss: 0.254656, acc.: 89.06%] [G loss: 2.888203]\n",
      "epoch:37 step:29550 [D loss: 0.290626, acc.: 87.50%] [G loss: 2.911333]\n",
      "epoch:37 step:29551 [D loss: 0.447192, acc.: 78.12%] [G loss: 3.390965]\n",
      "epoch:37 step:29552 [D loss: 0.345261, acc.: 82.81%] [G loss: 3.014833]\n",
      "epoch:37 step:29553 [D loss: 0.337543, acc.: 81.25%] [G loss: 3.135616]\n",
      "epoch:37 step:29554 [D loss: 0.245230, acc.: 88.28%] [G loss: 3.219756]\n",
      "epoch:37 step:29555 [D loss: 0.333337, acc.: 89.84%] [G loss: 2.843574]\n",
      "epoch:37 step:29556 [D loss: 0.271483, acc.: 86.72%] [G loss: 3.056023]\n",
      "epoch:37 step:29557 [D loss: 0.305667, acc.: 85.16%] [G loss: 3.328686]\n",
      "epoch:37 step:29558 [D loss: 0.358112, acc.: 84.38%] [G loss: 3.511068]\n",
      "epoch:37 step:29559 [D loss: 0.384965, acc.: 82.81%] [G loss: 2.671054]\n",
      "epoch:37 step:29560 [D loss: 0.386984, acc.: 82.81%] [G loss: 5.024189]\n",
      "epoch:37 step:29561 [D loss: 0.394741, acc.: 79.69%] [G loss: 3.398175]\n",
      "epoch:37 step:29562 [D loss: 0.351893, acc.: 84.38%] [G loss: 4.183891]\n",
      "epoch:37 step:29563 [D loss: 0.313304, acc.: 85.16%] [G loss: 2.554825]\n",
      "epoch:37 step:29564 [D loss: 0.203416, acc.: 90.62%] [G loss: 3.369587]\n",
      "epoch:37 step:29565 [D loss: 0.302135, acc.: 87.50%] [G loss: 2.705122]\n",
      "epoch:37 step:29566 [D loss: 0.370939, acc.: 82.81%] [G loss: 2.789891]\n",
      "epoch:37 step:29567 [D loss: 0.289690, acc.: 89.06%] [G loss: 2.717859]\n",
      "epoch:37 step:29568 [D loss: 0.308942, acc.: 85.94%] [G loss: 2.743416]\n",
      "epoch:37 step:29569 [D loss: 0.344955, acc.: 88.28%] [G loss: 2.840457]\n",
      "epoch:37 step:29570 [D loss: 0.283815, acc.: 85.94%] [G loss: 3.165248]\n",
      "epoch:37 step:29571 [D loss: 0.317393, acc.: 86.72%] [G loss: 2.208639]\n",
      "epoch:37 step:29572 [D loss: 0.356214, acc.: 85.94%] [G loss: 3.330389]\n",
      "epoch:37 step:29573 [D loss: 0.297930, acc.: 88.28%] [G loss: 3.175793]\n",
      "epoch:37 step:29574 [D loss: 0.316121, acc.: 85.16%] [G loss: 3.826535]\n",
      "epoch:37 step:29575 [D loss: 0.314660, acc.: 84.38%] [G loss: 3.228051]\n",
      "epoch:37 step:29576 [D loss: 0.254943, acc.: 90.62%] [G loss: 3.706795]\n",
      "epoch:37 step:29577 [D loss: 0.262018, acc.: 90.62%] [G loss: 3.323861]\n",
      "epoch:37 step:29578 [D loss: 0.285667, acc.: 89.06%] [G loss: 3.277952]\n",
      "epoch:37 step:29579 [D loss: 0.313348, acc.: 85.94%] [G loss: 2.937024]\n",
      "epoch:37 step:29580 [D loss: 0.360537, acc.: 85.94%] [G loss: 3.072726]\n",
      "epoch:37 step:29581 [D loss: 0.359621, acc.: 81.25%] [G loss: 3.012388]\n",
      "epoch:37 step:29582 [D loss: 0.346161, acc.: 82.03%] [G loss: 3.892586]\n",
      "epoch:37 step:29583 [D loss: 0.363501, acc.: 82.03%] [G loss: 3.810263]\n",
      "epoch:37 step:29584 [D loss: 0.414503, acc.: 82.81%] [G loss: 3.170595]\n",
      "epoch:37 step:29585 [D loss: 0.322472, acc.: 85.16%] [G loss: 4.189759]\n",
      "epoch:37 step:29586 [D loss: 0.229141, acc.: 89.06%] [G loss: 3.997765]\n",
      "epoch:37 step:29587 [D loss: 0.382154, acc.: 85.94%] [G loss: 9.165113]\n",
      "epoch:37 step:29588 [D loss: 0.206608, acc.: 91.41%] [G loss: 5.911442]\n",
      "epoch:37 step:29589 [D loss: 0.359022, acc.: 84.38%] [G loss: 4.538236]\n",
      "epoch:37 step:29590 [D loss: 0.318573, acc.: 84.38%] [G loss: 3.017387]\n",
      "epoch:37 step:29591 [D loss: 0.316035, acc.: 86.72%] [G loss: 3.114333]\n",
      "epoch:37 step:29592 [D loss: 0.205623, acc.: 89.06%] [G loss: 4.006325]\n",
      "epoch:37 step:29593 [D loss: 0.312119, acc.: 83.59%] [G loss: 3.938432]\n",
      "epoch:37 step:29594 [D loss: 0.280572, acc.: 86.72%] [G loss: 4.067359]\n",
      "epoch:37 step:29595 [D loss: 0.363450, acc.: 85.16%] [G loss: 4.459043]\n",
      "epoch:37 step:29596 [D loss: 0.307536, acc.: 86.72%] [G loss: 4.415781]\n",
      "epoch:37 step:29597 [D loss: 0.199726, acc.: 93.75%] [G loss: 3.493712]\n",
      "epoch:37 step:29598 [D loss: 0.409541, acc.: 83.59%] [G loss: 7.399243]\n",
      "epoch:37 step:29599 [D loss: 0.185791, acc.: 90.62%] [G loss: 6.459177]\n",
      "epoch:37 step:29600 [D loss: 0.305135, acc.: 85.16%] [G loss: 5.672814]\n",
      "epoch:37 step:29601 [D loss: 0.454960, acc.: 75.78%] [G loss: 3.702782]\n",
      "epoch:37 step:29602 [D loss: 0.224237, acc.: 89.06%] [G loss: 4.131635]\n",
      "epoch:37 step:29603 [D loss: 0.294317, acc.: 87.50%] [G loss: 4.184379]\n",
      "epoch:37 step:29604 [D loss: 0.328890, acc.: 85.94%] [G loss: 4.535100]\n",
      "epoch:37 step:29605 [D loss: 0.354367, acc.: 84.38%] [G loss: 4.665273]\n",
      "epoch:37 step:29606 [D loss: 0.375496, acc.: 82.81%] [G loss: 3.183930]\n",
      "epoch:37 step:29607 [D loss: 0.362201, acc.: 84.38%] [G loss: 3.095024]\n",
      "epoch:37 step:29608 [D loss: 0.367695, acc.: 85.16%] [G loss: 3.745154]\n",
      "epoch:37 step:29609 [D loss: 0.361938, acc.: 85.16%] [G loss: 3.256389]\n",
      "epoch:37 step:29610 [D loss: 0.480784, acc.: 82.03%] [G loss: 4.154047]\n",
      "epoch:37 step:29611 [D loss: 0.497725, acc.: 77.34%] [G loss: 3.854330]\n",
      "epoch:37 step:29612 [D loss: 0.368522, acc.: 85.16%] [G loss: 3.374083]\n",
      "epoch:37 step:29613 [D loss: 0.367497, acc.: 84.38%] [G loss: 2.969595]\n",
      "epoch:37 step:29614 [D loss: 0.294136, acc.: 85.16%] [G loss: 3.434667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29615 [D loss: 0.249784, acc.: 88.28%] [G loss: 4.178884]\n",
      "epoch:37 step:29616 [D loss: 0.198615, acc.: 92.19%] [G loss: 4.165857]\n",
      "epoch:37 step:29617 [D loss: 0.390110, acc.: 80.47%] [G loss: 4.035995]\n",
      "epoch:37 step:29618 [D loss: 0.299060, acc.: 86.72%] [G loss: 3.750902]\n",
      "epoch:37 step:29619 [D loss: 0.301111, acc.: 86.72%] [G loss: 3.253351]\n",
      "epoch:37 step:29620 [D loss: 0.268265, acc.: 87.50%] [G loss: 3.433598]\n",
      "epoch:37 step:29621 [D loss: 0.264804, acc.: 90.62%] [G loss: 2.932272]\n",
      "epoch:37 step:29622 [D loss: 0.239323, acc.: 90.62%] [G loss: 3.401430]\n",
      "epoch:37 step:29623 [D loss: 0.328418, acc.: 83.59%] [G loss: 3.124098]\n",
      "epoch:37 step:29624 [D loss: 0.284924, acc.: 87.50%] [G loss: 3.114256]\n",
      "epoch:37 step:29625 [D loss: 0.382785, acc.: 79.69%] [G loss: 3.049849]\n",
      "epoch:37 step:29626 [D loss: 0.292149, acc.: 85.94%] [G loss: 2.795817]\n",
      "epoch:37 step:29627 [D loss: 0.224536, acc.: 90.62%] [G loss: 3.851639]\n",
      "epoch:37 step:29628 [D loss: 0.311326, acc.: 86.72%] [G loss: 3.264730]\n",
      "epoch:37 step:29629 [D loss: 0.331892, acc.: 86.72%] [G loss: 2.537239]\n",
      "epoch:37 step:29630 [D loss: 0.314247, acc.: 84.38%] [G loss: 2.940228]\n",
      "epoch:37 step:29631 [D loss: 0.336068, acc.: 84.38%] [G loss: 3.540287]\n",
      "epoch:37 step:29632 [D loss: 0.304776, acc.: 86.72%] [G loss: 2.441895]\n",
      "epoch:37 step:29633 [D loss: 0.382540, acc.: 83.59%] [G loss: 2.695747]\n",
      "epoch:37 step:29634 [D loss: 0.343563, acc.: 85.94%] [G loss: 2.893072]\n",
      "epoch:37 step:29635 [D loss: 0.474860, acc.: 75.00%] [G loss: 2.671679]\n",
      "epoch:37 step:29636 [D loss: 0.408183, acc.: 84.38%] [G loss: 2.672271]\n",
      "epoch:37 step:29637 [D loss: 0.332035, acc.: 85.16%] [G loss: 2.827593]\n",
      "epoch:37 step:29638 [D loss: 0.427431, acc.: 81.25%] [G loss: 2.323196]\n",
      "epoch:37 step:29639 [D loss: 0.291483, acc.: 88.28%] [G loss: 2.492496]\n",
      "epoch:37 step:29640 [D loss: 0.296501, acc.: 86.72%] [G loss: 2.700686]\n",
      "epoch:37 step:29641 [D loss: 0.234052, acc.: 91.41%] [G loss: 2.876002]\n",
      "epoch:37 step:29642 [D loss: 0.448337, acc.: 82.03%] [G loss: 2.509970]\n",
      "epoch:37 step:29643 [D loss: 0.406961, acc.: 82.03%] [G loss: 3.006217]\n",
      "epoch:37 step:29644 [D loss: 0.241752, acc.: 92.19%] [G loss: 3.404778]\n",
      "epoch:37 step:29645 [D loss: 0.327613, acc.: 82.81%] [G loss: 2.949626]\n",
      "epoch:37 step:29646 [D loss: 0.303778, acc.: 89.84%] [G loss: 3.619525]\n",
      "epoch:37 step:29647 [D loss: 0.329758, acc.: 85.94%] [G loss: 3.073554]\n",
      "epoch:37 step:29648 [D loss: 0.292845, acc.: 88.28%] [G loss: 3.106085]\n",
      "epoch:37 step:29649 [D loss: 0.283792, acc.: 90.62%] [G loss: 2.898616]\n",
      "epoch:37 step:29650 [D loss: 0.316639, acc.: 89.06%] [G loss: 2.632366]\n",
      "epoch:37 step:29651 [D loss: 0.370174, acc.: 84.38%] [G loss: 2.786284]\n",
      "epoch:37 step:29652 [D loss: 0.357335, acc.: 84.38%] [G loss: 3.205451]\n",
      "epoch:37 step:29653 [D loss: 0.322623, acc.: 87.50%] [G loss: 3.318743]\n",
      "epoch:37 step:29654 [D loss: 0.355553, acc.: 82.03%] [G loss: 3.732118]\n",
      "epoch:37 step:29655 [D loss: 0.402585, acc.: 82.81%] [G loss: 2.809874]\n",
      "epoch:37 step:29656 [D loss: 0.362997, acc.: 85.94%] [G loss: 3.373642]\n",
      "epoch:37 step:29657 [D loss: 0.332035, acc.: 86.72%] [G loss: 2.429016]\n",
      "epoch:37 step:29658 [D loss: 0.381343, acc.: 82.81%] [G loss: 2.907553]\n",
      "epoch:37 step:29659 [D loss: 0.368253, acc.: 78.12%] [G loss: 2.767321]\n",
      "epoch:37 step:29660 [D loss: 0.353346, acc.: 88.28%] [G loss: 3.188653]\n",
      "epoch:37 step:29661 [D loss: 0.388387, acc.: 84.38%] [G loss: 2.747853]\n",
      "epoch:37 step:29662 [D loss: 0.366811, acc.: 82.03%] [G loss: 3.788077]\n",
      "epoch:37 step:29663 [D loss: 0.371731, acc.: 82.03%] [G loss: 3.573708]\n",
      "epoch:37 step:29664 [D loss: 0.283138, acc.: 88.28%] [G loss: 4.281197]\n",
      "epoch:37 step:29665 [D loss: 0.351014, acc.: 89.84%] [G loss: 3.865920]\n",
      "epoch:37 step:29666 [D loss: 0.231803, acc.: 89.84%] [G loss: 4.445859]\n",
      "epoch:37 step:29667 [D loss: 0.331395, acc.: 82.81%] [G loss: 2.567037]\n",
      "epoch:37 step:29668 [D loss: 0.401425, acc.: 79.69%] [G loss: 3.038689]\n",
      "epoch:37 step:29669 [D loss: 0.347655, acc.: 87.50%] [G loss: 3.095858]\n",
      "epoch:37 step:29670 [D loss: 0.316442, acc.: 85.16%] [G loss: 3.011946]\n",
      "epoch:37 step:29671 [D loss: 0.264621, acc.: 88.28%] [G loss: 2.988543]\n",
      "epoch:37 step:29672 [D loss: 0.294502, acc.: 89.84%] [G loss: 2.906270]\n",
      "epoch:37 step:29673 [D loss: 0.321430, acc.: 85.94%] [G loss: 3.028354]\n",
      "epoch:37 step:29674 [D loss: 0.428304, acc.: 79.69%] [G loss: 2.570371]\n",
      "epoch:37 step:29675 [D loss: 0.339172, acc.: 88.28%] [G loss: 3.766690]\n",
      "epoch:37 step:29676 [D loss: 0.340824, acc.: 83.59%] [G loss: 4.766080]\n",
      "epoch:37 step:29677 [D loss: 0.423922, acc.: 82.03%] [G loss: 3.759992]\n",
      "epoch:37 step:29678 [D loss: 0.350055, acc.: 86.72%] [G loss: 3.215530]\n",
      "epoch:38 step:29679 [D loss: 0.348505, acc.: 84.38%] [G loss: 4.190087]\n",
      "epoch:38 step:29680 [D loss: 0.318287, acc.: 83.59%] [G loss: 4.051740]\n",
      "epoch:38 step:29681 [D loss: 0.405784, acc.: 78.12%] [G loss: 4.291254]\n",
      "epoch:38 step:29682 [D loss: 0.347289, acc.: 84.38%] [G loss: 5.143804]\n",
      "epoch:38 step:29683 [D loss: 0.208639, acc.: 90.62%] [G loss: 4.271795]\n",
      "epoch:38 step:29684 [D loss: 0.264641, acc.: 88.28%] [G loss: 5.463950]\n",
      "epoch:38 step:29685 [D loss: 0.345306, acc.: 84.38%] [G loss: 3.019163]\n",
      "epoch:38 step:29686 [D loss: 0.231590, acc.: 90.62%] [G loss: 4.388950]\n",
      "epoch:38 step:29687 [D loss: 0.291851, acc.: 87.50%] [G loss: 3.037244]\n",
      "epoch:38 step:29688 [D loss: 0.482438, acc.: 81.25%] [G loss: 4.689243]\n",
      "epoch:38 step:29689 [D loss: 0.371375, acc.: 79.69%] [G loss: 5.078461]\n",
      "epoch:38 step:29690 [D loss: 0.281867, acc.: 88.28%] [G loss: 3.282406]\n",
      "epoch:38 step:29691 [D loss: 0.320485, acc.: 85.94%] [G loss: 3.260588]\n",
      "epoch:38 step:29692 [D loss: 0.232180, acc.: 92.19%] [G loss: 3.449434]\n",
      "epoch:38 step:29693 [D loss: 0.325955, acc.: 85.16%] [G loss: 3.180281]\n",
      "epoch:38 step:29694 [D loss: 0.280119, acc.: 90.62%] [G loss: 3.146626]\n",
      "epoch:38 step:29695 [D loss: 0.236575, acc.: 91.41%] [G loss: 3.478564]\n",
      "epoch:38 step:29696 [D loss: 0.251887, acc.: 90.62%] [G loss: 3.572130]\n",
      "epoch:38 step:29697 [D loss: 0.327988, acc.: 87.50%] [G loss: 3.492301]\n",
      "epoch:38 step:29698 [D loss: 0.251473, acc.: 89.06%] [G loss: 3.671491]\n",
      "epoch:38 step:29699 [D loss: 0.442330, acc.: 81.25%] [G loss: 2.359194]\n",
      "epoch:38 step:29700 [D loss: 0.351981, acc.: 84.38%] [G loss: 2.831924]\n",
      "epoch:38 step:29701 [D loss: 0.320497, acc.: 85.94%] [G loss: 4.061736]\n",
      "epoch:38 step:29702 [D loss: 0.284189, acc.: 86.72%] [G loss: 3.635507]\n",
      "epoch:38 step:29703 [D loss: 0.371664, acc.: 83.59%] [G loss: 3.252060]\n",
      "epoch:38 step:29704 [D loss: 0.320121, acc.: 82.81%] [G loss: 3.537497]\n",
      "epoch:38 step:29705 [D loss: 0.363049, acc.: 83.59%] [G loss: 3.302845]\n",
      "epoch:38 step:29706 [D loss: 0.282359, acc.: 90.62%] [G loss: 2.932969]\n",
      "epoch:38 step:29707 [D loss: 0.306272, acc.: 85.94%] [G loss: 3.958693]\n",
      "epoch:38 step:29708 [D loss: 0.282163, acc.: 88.28%] [G loss: 2.490917]\n",
      "epoch:38 step:29709 [D loss: 0.419468, acc.: 80.47%] [G loss: 5.715272]\n",
      "epoch:38 step:29710 [D loss: 0.380816, acc.: 78.91%] [G loss: 8.412375]\n",
      "epoch:38 step:29711 [D loss: 0.244660, acc.: 88.28%] [G loss: 5.754659]\n",
      "epoch:38 step:29712 [D loss: 0.282091, acc.: 87.50%] [G loss: 4.286924]\n",
      "epoch:38 step:29713 [D loss: 0.206250, acc.: 91.41%] [G loss: 4.326889]\n",
      "epoch:38 step:29714 [D loss: 0.328765, acc.: 84.38%] [G loss: 3.758432]\n",
      "epoch:38 step:29715 [D loss: 0.278674, acc.: 87.50%] [G loss: 4.499489]\n",
      "epoch:38 step:29716 [D loss: 0.387195, acc.: 78.91%] [G loss: 4.583834]\n",
      "epoch:38 step:29717 [D loss: 0.379310, acc.: 87.50%] [G loss: 2.510601]\n",
      "epoch:38 step:29718 [D loss: 0.372011, acc.: 85.94%] [G loss: 3.055788]\n",
      "epoch:38 step:29719 [D loss: 0.309638, acc.: 86.72%] [G loss: 3.433584]\n",
      "epoch:38 step:29720 [D loss: 0.334003, acc.: 85.94%] [G loss: 3.247705]\n",
      "epoch:38 step:29721 [D loss: 0.341943, acc.: 85.94%] [G loss: 3.099524]\n",
      "epoch:38 step:29722 [D loss: 0.356794, acc.: 83.59%] [G loss: 2.422233]\n",
      "epoch:38 step:29723 [D loss: 0.322878, acc.: 85.16%] [G loss: 2.824960]\n",
      "epoch:38 step:29724 [D loss: 0.406352, acc.: 81.25%] [G loss: 3.098827]\n",
      "epoch:38 step:29725 [D loss: 0.388458, acc.: 76.56%] [G loss: 3.984037]\n",
      "epoch:38 step:29726 [D loss: 0.268156, acc.: 89.06%] [G loss: 3.090123]\n",
      "epoch:38 step:29727 [D loss: 0.305062, acc.: 85.16%] [G loss: 5.409892]\n",
      "epoch:38 step:29728 [D loss: 0.388440, acc.: 78.12%] [G loss: 4.753306]\n",
      "epoch:38 step:29729 [D loss: 0.363896, acc.: 82.03%] [G loss: 3.074069]\n",
      "epoch:38 step:29730 [D loss: 0.348943, acc.: 87.50%] [G loss: 3.424290]\n",
      "epoch:38 step:29731 [D loss: 0.278982, acc.: 88.28%] [G loss: 4.091027]\n",
      "epoch:38 step:29732 [D loss: 0.335498, acc.: 87.50%] [G loss: 4.097464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:29733 [D loss: 0.258340, acc.: 88.28%] [G loss: 4.066844]\n",
      "epoch:38 step:29734 [D loss: 0.338711, acc.: 84.38%] [G loss: 4.072819]\n",
      "epoch:38 step:29735 [D loss: 0.355518, acc.: 85.16%] [G loss: 3.941367]\n",
      "epoch:38 step:29736 [D loss: 0.346105, acc.: 83.59%] [G loss: 3.612943]\n",
      "epoch:38 step:29737 [D loss: 0.412803, acc.: 82.81%] [G loss: 3.951856]\n",
      "epoch:38 step:29738 [D loss: 0.254819, acc.: 89.84%] [G loss: 3.571607]\n",
      "epoch:38 step:29739 [D loss: 0.262985, acc.: 89.84%] [G loss: 2.599113]\n",
      "epoch:38 step:29740 [D loss: 0.234837, acc.: 89.84%] [G loss: 2.763938]\n",
      "epoch:38 step:29741 [D loss: 0.310406, acc.: 85.16%] [G loss: 2.859898]\n",
      "epoch:38 step:29742 [D loss: 0.313514, acc.: 87.50%] [G loss: 3.957063]\n",
      "epoch:38 step:29743 [D loss: 0.416889, acc.: 84.38%] [G loss: 2.031017]\n",
      "epoch:38 step:29744 [D loss: 0.334812, acc.: 86.72%] [G loss: 3.106898]\n",
      "epoch:38 step:29745 [D loss: 0.270192, acc.: 89.06%] [G loss: 2.642355]\n",
      "epoch:38 step:29746 [D loss: 0.314523, acc.: 88.28%] [G loss: 2.883396]\n",
      "epoch:38 step:29747 [D loss: 0.363997, acc.: 82.03%] [G loss: 2.824520]\n",
      "epoch:38 step:29748 [D loss: 0.278840, acc.: 87.50%] [G loss: 3.004795]\n",
      "epoch:38 step:29749 [D loss: 0.371005, acc.: 88.28%] [G loss: 2.508479]\n",
      "epoch:38 step:29750 [D loss: 0.306728, acc.: 89.06%] [G loss: 3.024490]\n",
      "epoch:38 step:29751 [D loss: 0.307820, acc.: 87.50%] [G loss: 4.005014]\n",
      "epoch:38 step:29752 [D loss: 0.406254, acc.: 83.59%] [G loss: 4.774261]\n",
      "epoch:38 step:29753 [D loss: 0.741068, acc.: 75.00%] [G loss: 5.699750]\n",
      "epoch:38 step:29754 [D loss: 0.610630, acc.: 69.53%] [G loss: 3.122113]\n",
      "epoch:38 step:29755 [D loss: 0.377669, acc.: 84.38%] [G loss: 5.258415]\n",
      "epoch:38 step:29756 [D loss: 0.338690, acc.: 82.81%] [G loss: 3.259375]\n",
      "epoch:38 step:29757 [D loss: 0.319350, acc.: 87.50%] [G loss: 3.651376]\n",
      "epoch:38 step:29758 [D loss: 0.285423, acc.: 87.50%] [G loss: 3.251393]\n",
      "epoch:38 step:29759 [D loss: 0.334950, acc.: 89.06%] [G loss: 4.246303]\n",
      "epoch:38 step:29760 [D loss: 0.274943, acc.: 86.72%] [G loss: 2.729097]\n",
      "epoch:38 step:29761 [D loss: 0.274364, acc.: 89.06%] [G loss: 3.214355]\n",
      "epoch:38 step:29762 [D loss: 0.319805, acc.: 84.38%] [G loss: 3.514354]\n",
      "epoch:38 step:29763 [D loss: 0.303650, acc.: 86.72%] [G loss: 3.165175]\n",
      "epoch:38 step:29764 [D loss: 0.339750, acc.: 89.06%] [G loss: 3.273821]\n",
      "epoch:38 step:29765 [D loss: 0.364224, acc.: 82.03%] [G loss: 3.632474]\n",
      "epoch:38 step:29766 [D loss: 0.262048, acc.: 86.72%] [G loss: 4.206316]\n",
      "epoch:38 step:29767 [D loss: 0.245972, acc.: 88.28%] [G loss: 3.613341]\n",
      "epoch:38 step:29768 [D loss: 0.480329, acc.: 76.56%] [G loss: 3.734375]\n",
      "epoch:38 step:29769 [D loss: 0.263003, acc.: 87.50%] [G loss: 3.538972]\n",
      "epoch:38 step:29770 [D loss: 0.378355, acc.: 84.38%] [G loss: 3.726793]\n",
      "epoch:38 step:29771 [D loss: 0.432016, acc.: 77.34%] [G loss: 3.225122]\n",
      "epoch:38 step:29772 [D loss: 0.291435, acc.: 89.06%] [G loss: 3.280458]\n",
      "epoch:38 step:29773 [D loss: 0.290429, acc.: 90.62%] [G loss: 3.809945]\n",
      "epoch:38 step:29774 [D loss: 0.363320, acc.: 82.81%] [G loss: 3.306067]\n",
      "epoch:38 step:29775 [D loss: 0.312841, acc.: 85.94%] [G loss: 2.865887]\n",
      "epoch:38 step:29776 [D loss: 0.440757, acc.: 78.12%] [G loss: 3.634774]\n",
      "epoch:38 step:29777 [D loss: 0.431933, acc.: 82.03%] [G loss: 2.697626]\n",
      "epoch:38 step:29778 [D loss: 0.355585, acc.: 82.81%] [G loss: 2.686980]\n",
      "epoch:38 step:29779 [D loss: 0.305979, acc.: 87.50%] [G loss: 2.548286]\n",
      "epoch:38 step:29780 [D loss: 0.333045, acc.: 80.47%] [G loss: 2.569744]\n",
      "epoch:38 step:29781 [D loss: 0.242479, acc.: 89.84%] [G loss: 2.617096]\n",
      "epoch:38 step:29782 [D loss: 0.209339, acc.: 94.53%] [G loss: 3.214854]\n",
      "epoch:38 step:29783 [D loss: 0.349110, acc.: 85.94%] [G loss: 2.435807]\n",
      "epoch:38 step:29784 [D loss: 0.269457, acc.: 89.06%] [G loss: 2.695820]\n",
      "epoch:38 step:29785 [D loss: 0.356661, acc.: 82.03%] [G loss: 2.158435]\n",
      "epoch:38 step:29786 [D loss: 0.366342, acc.: 85.94%] [G loss: 3.728039]\n",
      "epoch:38 step:29787 [D loss: 0.319955, acc.: 87.50%] [G loss: 2.679484]\n",
      "epoch:38 step:29788 [D loss: 0.268169, acc.: 89.06%] [G loss: 3.338326]\n",
      "epoch:38 step:29789 [D loss: 0.443693, acc.: 75.00%] [G loss: 2.524206]\n",
      "epoch:38 step:29790 [D loss: 0.231414, acc.: 87.50%] [G loss: 2.636892]\n",
      "epoch:38 step:29791 [D loss: 0.379864, acc.: 82.81%] [G loss: 3.838896]\n",
      "epoch:38 step:29792 [D loss: 0.299566, acc.: 87.50%] [G loss: 3.841082]\n",
      "epoch:38 step:29793 [D loss: 0.227616, acc.: 89.84%] [G loss: 3.945425]\n",
      "epoch:38 step:29794 [D loss: 0.370426, acc.: 82.03%] [G loss: 2.900748]\n",
      "epoch:38 step:29795 [D loss: 0.263469, acc.: 89.84%] [G loss: 4.072780]\n",
      "epoch:38 step:29796 [D loss: 0.237591, acc.: 89.06%] [G loss: 5.040774]\n",
      "epoch:38 step:29797 [D loss: 0.324507, acc.: 86.72%] [G loss: 3.015503]\n",
      "epoch:38 step:29798 [D loss: 0.288469, acc.: 85.16%] [G loss: 3.105248]\n",
      "epoch:38 step:29799 [D loss: 0.304532, acc.: 86.72%] [G loss: 4.092316]\n",
      "epoch:38 step:29800 [D loss: 0.202053, acc.: 91.41%] [G loss: 3.874578]\n",
      "epoch:38 step:29801 [D loss: 0.258979, acc.: 89.84%] [G loss: 4.111632]\n",
      "epoch:38 step:29802 [D loss: 0.387972, acc.: 84.38%] [G loss: 4.481980]\n",
      "epoch:38 step:29803 [D loss: 0.297682, acc.: 85.94%] [G loss: 4.379443]\n",
      "epoch:38 step:29804 [D loss: 0.377273, acc.: 82.81%] [G loss: 4.888968]\n",
      "epoch:38 step:29805 [D loss: 0.270891, acc.: 85.16%] [G loss: 3.474667]\n",
      "epoch:38 step:29806 [D loss: 0.203769, acc.: 92.19%] [G loss: 3.942454]\n",
      "epoch:38 step:29807 [D loss: 0.327830, acc.: 83.59%] [G loss: 4.373819]\n",
      "epoch:38 step:29808 [D loss: 0.282398, acc.: 88.28%] [G loss: 3.555537]\n",
      "epoch:38 step:29809 [D loss: 0.299091, acc.: 83.59%] [G loss: 4.228567]\n",
      "epoch:38 step:29810 [D loss: 0.213121, acc.: 92.97%] [G loss: 4.252817]\n",
      "epoch:38 step:29811 [D loss: 0.267328, acc.: 89.84%] [G loss: 3.123079]\n",
      "epoch:38 step:29812 [D loss: 0.238407, acc.: 90.62%] [G loss: 3.412895]\n",
      "epoch:38 step:29813 [D loss: 0.291194, acc.: 88.28%] [G loss: 3.248265]\n",
      "epoch:38 step:29814 [D loss: 0.311408, acc.: 84.38%] [G loss: 3.458982]\n",
      "epoch:38 step:29815 [D loss: 0.278293, acc.: 88.28%] [G loss: 4.678369]\n",
      "epoch:38 step:29816 [D loss: 0.250699, acc.: 87.50%] [G loss: 3.699991]\n",
      "epoch:38 step:29817 [D loss: 0.196524, acc.: 92.97%] [G loss: 4.819835]\n",
      "epoch:38 step:29818 [D loss: 0.214691, acc.: 93.75%] [G loss: 3.829895]\n",
      "epoch:38 step:29819 [D loss: 0.292079, acc.: 86.72%] [G loss: 2.972782]\n",
      "epoch:38 step:29820 [D loss: 0.368755, acc.: 83.59%] [G loss: 3.386221]\n",
      "epoch:38 step:29821 [D loss: 0.341447, acc.: 85.94%] [G loss: 2.990405]\n",
      "epoch:38 step:29822 [D loss: 0.342563, acc.: 81.25%] [G loss: 2.923145]\n",
      "epoch:38 step:29823 [D loss: 0.302699, acc.: 89.06%] [G loss: 2.771950]\n",
      "epoch:38 step:29824 [D loss: 0.385287, acc.: 82.03%] [G loss: 3.292568]\n",
      "epoch:38 step:29825 [D loss: 0.254048, acc.: 92.19%] [G loss: 2.544504]\n",
      "epoch:38 step:29826 [D loss: 0.315012, acc.: 89.84%] [G loss: 2.956626]\n",
      "epoch:38 step:29827 [D loss: 0.316481, acc.: 81.25%] [G loss: 2.692745]\n",
      "epoch:38 step:29828 [D loss: 0.263690, acc.: 89.84%] [G loss: 2.518191]\n",
      "epoch:38 step:29829 [D loss: 0.349839, acc.: 85.16%] [G loss: 2.847230]\n",
      "epoch:38 step:29830 [D loss: 0.347660, acc.: 85.94%] [G loss: 2.976564]\n",
      "epoch:38 step:29831 [D loss: 0.251055, acc.: 86.72%] [G loss: 3.853038]\n",
      "epoch:38 step:29832 [D loss: 0.328431, acc.: 82.81%] [G loss: 3.129475]\n",
      "epoch:38 step:29833 [D loss: 0.317079, acc.: 84.38%] [G loss: 3.807100]\n",
      "epoch:38 step:29834 [D loss: 0.356573, acc.: 84.38%] [G loss: 3.866823]\n",
      "epoch:38 step:29835 [D loss: 0.325486, acc.: 86.72%] [G loss: 3.460468]\n",
      "epoch:38 step:29836 [D loss: 0.307835, acc.: 83.59%] [G loss: 3.812425]\n",
      "epoch:38 step:29837 [D loss: 0.220979, acc.: 93.75%] [G loss: 3.397286]\n",
      "epoch:38 step:29838 [D loss: 0.385179, acc.: 78.91%] [G loss: 3.697778]\n",
      "epoch:38 step:29839 [D loss: 0.345131, acc.: 81.25%] [G loss: 2.620260]\n",
      "epoch:38 step:29840 [D loss: 0.276112, acc.: 88.28%] [G loss: 2.750077]\n",
      "epoch:38 step:29841 [D loss: 0.285627, acc.: 87.50%] [G loss: 3.015804]\n",
      "epoch:38 step:29842 [D loss: 0.247953, acc.: 90.62%] [G loss: 5.322140]\n",
      "epoch:38 step:29843 [D loss: 0.332083, acc.: 83.59%] [G loss: 3.136485]\n",
      "epoch:38 step:29844 [D loss: 0.316653, acc.: 87.50%] [G loss: 4.919899]\n",
      "epoch:38 step:29845 [D loss: 0.244985, acc.: 89.06%] [G loss: 4.798658]\n",
      "epoch:38 step:29846 [D loss: 0.218164, acc.: 89.84%] [G loss: 4.794638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:29847 [D loss: 0.238558, acc.: 91.41%] [G loss: 4.375085]\n",
      "epoch:38 step:29848 [D loss: 0.241174, acc.: 90.62%] [G loss: 5.237170]\n",
      "epoch:38 step:29849 [D loss: 0.312563, acc.: 86.72%] [G loss: 3.554904]\n",
      "epoch:38 step:29850 [D loss: 0.280348, acc.: 85.94%] [G loss: 4.082013]\n",
      "epoch:38 step:29851 [D loss: 0.396941, acc.: 82.81%] [G loss: 3.777085]\n",
      "epoch:38 step:29852 [D loss: 0.546219, acc.: 74.22%] [G loss: 2.933388]\n",
      "epoch:38 step:29853 [D loss: 0.350443, acc.: 83.59%] [G loss: 4.366737]\n",
      "epoch:38 step:29854 [D loss: 0.244292, acc.: 92.97%] [G loss: 3.099566]\n",
      "epoch:38 step:29855 [D loss: 0.320419, acc.: 81.25%] [G loss: 5.086888]\n",
      "epoch:38 step:29856 [D loss: 0.489206, acc.: 75.78%] [G loss: 3.633486]\n",
      "epoch:38 step:29857 [D loss: 0.363942, acc.: 82.81%] [G loss: 3.306156]\n",
      "epoch:38 step:29858 [D loss: 0.408087, acc.: 80.47%] [G loss: 3.510408]\n",
      "epoch:38 step:29859 [D loss: 0.350723, acc.: 85.94%] [G loss: 2.950661]\n",
      "epoch:38 step:29860 [D loss: 0.313698, acc.: 85.16%] [G loss: 3.781289]\n",
      "epoch:38 step:29861 [D loss: 0.274815, acc.: 88.28%] [G loss: 2.858402]\n",
      "epoch:38 step:29862 [D loss: 0.446535, acc.: 78.91%] [G loss: 2.602559]\n",
      "epoch:38 step:29863 [D loss: 0.276940, acc.: 88.28%] [G loss: 3.323583]\n",
      "epoch:38 step:29864 [D loss: 0.296935, acc.: 89.06%] [G loss: 3.263730]\n",
      "epoch:38 step:29865 [D loss: 0.424845, acc.: 79.69%] [G loss: 2.506752]\n",
      "epoch:38 step:29866 [D loss: 0.365533, acc.: 82.03%] [G loss: 3.007560]\n",
      "epoch:38 step:29867 [D loss: 0.330994, acc.: 83.59%] [G loss: 2.704470]\n",
      "epoch:38 step:29868 [D loss: 0.609309, acc.: 69.53%] [G loss: 2.946396]\n",
      "epoch:38 step:29869 [D loss: 0.312196, acc.: 84.38%] [G loss: 3.201508]\n",
      "epoch:38 step:29870 [D loss: 0.245357, acc.: 90.62%] [G loss: 2.409006]\n",
      "epoch:38 step:29871 [D loss: 0.508993, acc.: 72.66%] [G loss: 2.691172]\n",
      "epoch:38 step:29872 [D loss: 0.311603, acc.: 84.38%] [G loss: 2.889024]\n",
      "epoch:38 step:29873 [D loss: 0.262172, acc.: 88.28%] [G loss: 3.204219]\n",
      "epoch:38 step:29874 [D loss: 0.299512, acc.: 84.38%] [G loss: 2.261956]\n",
      "epoch:38 step:29875 [D loss: 0.327931, acc.: 83.59%] [G loss: 3.537982]\n",
      "epoch:38 step:29876 [D loss: 0.353832, acc.: 85.94%] [G loss: 2.846532]\n",
      "epoch:38 step:29877 [D loss: 0.299239, acc.: 88.28%] [G loss: 2.863012]\n",
      "epoch:38 step:29878 [D loss: 0.263834, acc.: 87.50%] [G loss: 3.616942]\n",
      "epoch:38 step:29879 [D loss: 0.283160, acc.: 86.72%] [G loss: 2.778926]\n",
      "epoch:38 step:29880 [D loss: 0.189252, acc.: 91.41%] [G loss: 2.616544]\n",
      "epoch:38 step:29881 [D loss: 0.239454, acc.: 89.84%] [G loss: 3.854925]\n",
      "epoch:38 step:29882 [D loss: 0.375412, acc.: 82.03%] [G loss: 3.759732]\n",
      "epoch:38 step:29883 [D loss: 0.318995, acc.: 84.38%] [G loss: 2.788084]\n",
      "epoch:38 step:29884 [D loss: 0.257873, acc.: 87.50%] [G loss: 3.765174]\n",
      "epoch:38 step:29885 [D loss: 0.265562, acc.: 86.72%] [G loss: 3.279844]\n",
      "epoch:38 step:29886 [D loss: 0.352718, acc.: 83.59%] [G loss: 2.838287]\n",
      "epoch:38 step:29887 [D loss: 0.350759, acc.: 82.03%] [G loss: 2.265618]\n",
      "epoch:38 step:29888 [D loss: 0.379558, acc.: 82.03%] [G loss: 3.728133]\n",
      "epoch:38 step:29889 [D loss: 0.391666, acc.: 80.47%] [G loss: 2.603642]\n",
      "epoch:38 step:29890 [D loss: 0.300135, acc.: 88.28%] [G loss: 2.887851]\n",
      "epoch:38 step:29891 [D loss: 0.289403, acc.: 85.16%] [G loss: 3.176409]\n",
      "epoch:38 step:29892 [D loss: 0.391998, acc.: 78.91%] [G loss: 3.643709]\n",
      "epoch:38 step:29893 [D loss: 0.272117, acc.: 87.50%] [G loss: 2.680760]\n",
      "epoch:38 step:29894 [D loss: 0.338909, acc.: 81.25%] [G loss: 3.485385]\n",
      "epoch:38 step:29895 [D loss: 0.325505, acc.: 85.16%] [G loss: 3.642038]\n",
      "epoch:38 step:29896 [D loss: 0.291271, acc.: 85.16%] [G loss: 3.888575]\n",
      "epoch:38 step:29897 [D loss: 0.393451, acc.: 84.38%] [G loss: 3.763632]\n",
      "epoch:38 step:29898 [D loss: 0.334342, acc.: 82.81%] [G loss: 3.817970]\n",
      "epoch:38 step:29899 [D loss: 0.384655, acc.: 81.25%] [G loss: 3.406565]\n",
      "epoch:38 step:29900 [D loss: 0.268470, acc.: 86.72%] [G loss: 3.259705]\n",
      "epoch:38 step:29901 [D loss: 0.250253, acc.: 90.62%] [G loss: 3.318159]\n",
      "epoch:38 step:29902 [D loss: 0.350007, acc.: 85.94%] [G loss: 3.459898]\n",
      "epoch:38 step:29903 [D loss: 0.377025, acc.: 82.03%] [G loss: 2.911780]\n",
      "epoch:38 step:29904 [D loss: 0.424799, acc.: 81.25%] [G loss: 3.302738]\n",
      "epoch:38 step:29905 [D loss: 0.283040, acc.: 88.28%] [G loss: 3.277848]\n",
      "epoch:38 step:29906 [D loss: 0.480323, acc.: 77.34%] [G loss: 4.278718]\n",
      "epoch:38 step:29907 [D loss: 0.448094, acc.: 78.12%] [G loss: 4.990632]\n",
      "epoch:38 step:29908 [D loss: 0.333344, acc.: 85.94%] [G loss: 4.174455]\n",
      "epoch:38 step:29909 [D loss: 0.321038, acc.: 82.81%] [G loss: 3.616347]\n",
      "epoch:38 step:29910 [D loss: 0.290482, acc.: 89.84%] [G loss: 3.318500]\n",
      "epoch:38 step:29911 [D loss: 0.302576, acc.: 88.28%] [G loss: 3.598942]\n",
      "epoch:38 step:29912 [D loss: 0.337070, acc.: 84.38%] [G loss: 2.801595]\n",
      "epoch:38 step:29913 [D loss: 0.271811, acc.: 87.50%] [G loss: 3.832228]\n",
      "epoch:38 step:29914 [D loss: 0.280990, acc.: 86.72%] [G loss: 2.762303]\n",
      "epoch:38 step:29915 [D loss: 0.389572, acc.: 82.03%] [G loss: 3.200962]\n",
      "epoch:38 step:29916 [D loss: 0.299348, acc.: 85.16%] [G loss: 2.625196]\n",
      "epoch:38 step:29917 [D loss: 0.313969, acc.: 84.38%] [G loss: 3.737076]\n",
      "epoch:38 step:29918 [D loss: 0.248871, acc.: 90.62%] [G loss: 4.665206]\n",
      "epoch:38 step:29919 [D loss: 0.362291, acc.: 85.16%] [G loss: 2.921352]\n",
      "epoch:38 step:29920 [D loss: 0.270212, acc.: 89.06%] [G loss: 3.027471]\n",
      "epoch:38 step:29921 [D loss: 0.284977, acc.: 88.28%] [G loss: 3.355427]\n",
      "epoch:38 step:29922 [D loss: 0.264425, acc.: 89.84%] [G loss: 3.588359]\n",
      "epoch:38 step:29923 [D loss: 0.311241, acc.: 88.28%] [G loss: 3.779151]\n",
      "epoch:38 step:29924 [D loss: 0.253566, acc.: 87.50%] [G loss: 3.981814]\n",
      "epoch:38 step:29925 [D loss: 0.337981, acc.: 81.25%] [G loss: 5.254424]\n",
      "epoch:38 step:29926 [D loss: 0.222249, acc.: 91.41%] [G loss: 4.373899]\n",
      "epoch:38 step:29927 [D loss: 0.271506, acc.: 87.50%] [G loss: 4.558432]\n",
      "epoch:38 step:29928 [D loss: 0.427920, acc.: 78.12%] [G loss: 3.743464]\n",
      "epoch:38 step:29929 [D loss: 0.483334, acc.: 81.25%] [G loss: 3.078994]\n",
      "epoch:38 step:29930 [D loss: 0.354636, acc.: 81.25%] [G loss: 3.934128]\n",
      "epoch:38 step:29931 [D loss: 0.252995, acc.: 86.72%] [G loss: 4.070491]\n",
      "epoch:38 step:29932 [D loss: 0.246720, acc.: 87.50%] [G loss: 4.344715]\n",
      "epoch:38 step:29933 [D loss: 0.298647, acc.: 84.38%] [G loss: 3.499436]\n",
      "epoch:38 step:29934 [D loss: 0.350327, acc.: 85.94%] [G loss: 3.778869]\n",
      "epoch:38 step:29935 [D loss: 0.329262, acc.: 86.72%] [G loss: 5.382952]\n",
      "epoch:38 step:29936 [D loss: 0.400843, acc.: 84.38%] [G loss: 3.457613]\n",
      "epoch:38 step:29937 [D loss: 0.237510, acc.: 89.84%] [G loss: 5.049358]\n",
      "epoch:38 step:29938 [D loss: 0.225729, acc.: 90.62%] [G loss: 3.424445]\n",
      "epoch:38 step:29939 [D loss: 0.410137, acc.: 82.03%] [G loss: 4.192050]\n",
      "epoch:38 step:29940 [D loss: 0.331011, acc.: 87.50%] [G loss: 5.367179]\n",
      "epoch:38 step:29941 [D loss: 0.241355, acc.: 92.19%] [G loss: 3.962721]\n",
      "epoch:38 step:29942 [D loss: 0.271231, acc.: 90.62%] [G loss: 4.845021]\n",
      "epoch:38 step:29943 [D loss: 0.227418, acc.: 89.84%] [G loss: 5.272040]\n",
      "epoch:38 step:29944 [D loss: 0.269327, acc.: 89.06%] [G loss: 2.679558]\n",
      "epoch:38 step:29945 [D loss: 0.262357, acc.: 87.50%] [G loss: 3.942868]\n",
      "epoch:38 step:29946 [D loss: 0.239256, acc.: 90.62%] [G loss: 4.247625]\n",
      "epoch:38 step:29947 [D loss: 0.326068, acc.: 89.06%] [G loss: 3.282731]\n",
      "epoch:38 step:29948 [D loss: 0.298273, acc.: 83.59%] [G loss: 4.432940]\n",
      "epoch:38 step:29949 [D loss: 0.225804, acc.: 90.62%] [G loss: 3.442907]\n",
      "epoch:38 step:29950 [D loss: 0.271773, acc.: 88.28%] [G loss: 3.544916]\n",
      "epoch:38 step:29951 [D loss: 0.215655, acc.: 89.84%] [G loss: 4.065231]\n",
      "epoch:38 step:29952 [D loss: 0.245668, acc.: 88.28%] [G loss: 3.221177]\n",
      "epoch:38 step:29953 [D loss: 0.369748, acc.: 80.47%] [G loss: 3.155532]\n",
      "epoch:38 step:29954 [D loss: 0.361702, acc.: 81.25%] [G loss: 3.126773]\n",
      "epoch:38 step:29955 [D loss: 0.461087, acc.: 75.00%] [G loss: 3.707296]\n",
      "epoch:38 step:29956 [D loss: 0.397889, acc.: 85.94%] [G loss: 3.353360]\n",
      "epoch:38 step:29957 [D loss: 0.381716, acc.: 85.16%] [G loss: 3.306681]\n",
      "epoch:38 step:29958 [D loss: 0.372317, acc.: 83.59%] [G loss: 3.053642]\n",
      "epoch:38 step:29959 [D loss: 0.352645, acc.: 85.16%] [G loss: 4.049851]\n",
      "epoch:38 step:29960 [D loss: 0.362874, acc.: 84.38%] [G loss: 2.586003]\n",
      "epoch:38 step:29961 [D loss: 0.289057, acc.: 87.50%] [G loss: 3.186936]\n",
      "epoch:38 step:29962 [D loss: 0.306041, acc.: 86.72%] [G loss: 3.412237]\n",
      "epoch:38 step:29963 [D loss: 0.308889, acc.: 88.28%] [G loss: 3.048519]\n",
      "epoch:38 step:29964 [D loss: 0.433918, acc.: 77.34%] [G loss: 3.805151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:29965 [D loss: 0.353283, acc.: 85.16%] [G loss: 2.588104]\n",
      "epoch:38 step:29966 [D loss: 0.254802, acc.: 87.50%] [G loss: 3.568686]\n",
      "epoch:38 step:29967 [D loss: 0.392117, acc.: 81.25%] [G loss: 2.786268]\n",
      "epoch:38 step:29968 [D loss: 0.349662, acc.: 82.03%] [G loss: 2.863741]\n",
      "epoch:38 step:29969 [D loss: 0.452263, acc.: 78.12%] [G loss: 2.650166]\n",
      "epoch:38 step:29970 [D loss: 0.345887, acc.: 84.38%] [G loss: 2.753979]\n",
      "epoch:38 step:29971 [D loss: 0.295788, acc.: 85.94%] [G loss: 3.059993]\n",
      "epoch:38 step:29972 [D loss: 0.362481, acc.: 82.03%] [G loss: 2.809220]\n",
      "epoch:38 step:29973 [D loss: 0.362748, acc.: 85.94%] [G loss: 3.203327]\n",
      "epoch:38 step:29974 [D loss: 0.385666, acc.: 79.69%] [G loss: 3.454446]\n",
      "epoch:38 step:29975 [D loss: 0.337409, acc.: 87.50%] [G loss: 3.132601]\n",
      "epoch:38 step:29976 [D loss: 0.240514, acc.: 92.19%] [G loss: 4.308377]\n",
      "epoch:38 step:29977 [D loss: 0.270780, acc.: 90.62%] [G loss: 4.681353]\n",
      "epoch:38 step:29978 [D loss: 0.351243, acc.: 81.25%] [G loss: 4.815272]\n",
      "epoch:38 step:29979 [D loss: 0.319367, acc.: 86.72%] [G loss: 3.488663]\n",
      "epoch:38 step:29980 [D loss: 0.221148, acc.: 92.97%] [G loss: 3.805086]\n",
      "epoch:38 step:29981 [D loss: 0.291780, acc.: 88.28%] [G loss: 3.586255]\n",
      "epoch:38 step:29982 [D loss: 0.259318, acc.: 90.62%] [G loss: 3.005296]\n",
      "epoch:38 step:29983 [D loss: 0.225620, acc.: 93.75%] [G loss: 4.528526]\n",
      "epoch:38 step:29984 [D loss: 0.291981, acc.: 89.06%] [G loss: 3.982468]\n",
      "epoch:38 step:29985 [D loss: 0.275418, acc.: 87.50%] [G loss: 3.222836]\n",
      "epoch:38 step:29986 [D loss: 0.355356, acc.: 79.69%] [G loss: 2.741518]\n",
      "epoch:38 step:29987 [D loss: 0.397813, acc.: 82.81%] [G loss: 2.962001]\n",
      "epoch:38 step:29988 [D loss: 0.373785, acc.: 85.16%] [G loss: 2.794238]\n",
      "epoch:38 step:29989 [D loss: 0.414158, acc.: 78.12%] [G loss: 3.523669]\n",
      "epoch:38 step:29990 [D loss: 0.314578, acc.: 88.28%] [G loss: 3.199250]\n",
      "epoch:38 step:29991 [D loss: 0.363503, acc.: 83.59%] [G loss: 3.234079]\n",
      "epoch:38 step:29992 [D loss: 0.329040, acc.: 82.81%] [G loss: 3.449633]\n",
      "epoch:38 step:29993 [D loss: 0.426151, acc.: 75.78%] [G loss: 6.978770]\n",
      "epoch:38 step:29994 [D loss: 0.361853, acc.: 87.50%] [G loss: 4.662572]\n",
      "epoch:38 step:29995 [D loss: 0.432892, acc.: 78.12%] [G loss: 5.070831]\n",
      "epoch:38 step:29996 [D loss: 0.448630, acc.: 76.56%] [G loss: 3.045890]\n",
      "epoch:38 step:29997 [D loss: 0.343427, acc.: 82.03%] [G loss: 5.766447]\n",
      "epoch:38 step:29998 [D loss: 0.491067, acc.: 80.47%] [G loss: 4.667439]\n",
      "epoch:38 step:29999 [D loss: 0.445455, acc.: 79.69%] [G loss: 4.794837]\n",
      "epoch:38 step:30000 [D loss: 0.287184, acc.: 89.06%] [G loss: 5.286981]\n",
      "epoch:38 step:30001 [D loss: 0.366353, acc.: 81.25%] [G loss: 4.217929]\n",
      "epoch:38 step:30002 [D loss: 0.376807, acc.: 82.81%] [G loss: 4.381035]\n",
      "epoch:38 step:30003 [D loss: 0.406907, acc.: 78.91%] [G loss: 2.558940]\n",
      "epoch:38 step:30004 [D loss: 0.407181, acc.: 77.34%] [G loss: 2.895157]\n",
      "epoch:38 step:30005 [D loss: 0.304943, acc.: 84.38%] [G loss: 2.963954]\n",
      "epoch:38 step:30006 [D loss: 0.339559, acc.: 85.94%] [G loss: 3.352834]\n",
      "epoch:38 step:30007 [D loss: 0.225920, acc.: 92.97%] [G loss: 3.861198]\n",
      "epoch:38 step:30008 [D loss: 0.383179, acc.: 82.03%] [G loss: 4.108932]\n",
      "epoch:38 step:30009 [D loss: 0.369624, acc.: 83.59%] [G loss: 3.708109]\n",
      "epoch:38 step:30010 [D loss: 0.272480, acc.: 89.06%] [G loss: 4.057153]\n",
      "epoch:38 step:30011 [D loss: 0.232529, acc.: 91.41%] [G loss: 3.412807]\n",
      "epoch:38 step:30012 [D loss: 0.372793, acc.: 80.47%] [G loss: 3.473138]\n",
      "epoch:38 step:30013 [D loss: 0.242125, acc.: 89.84%] [G loss: 3.452513]\n",
      "epoch:38 step:30014 [D loss: 0.387613, acc.: 82.81%] [G loss: 2.831945]\n",
      "epoch:38 step:30015 [D loss: 0.322213, acc.: 80.47%] [G loss: 2.929806]\n",
      "epoch:38 step:30016 [D loss: 0.337814, acc.: 82.81%] [G loss: 3.104860]\n",
      "epoch:38 step:30017 [D loss: 0.258729, acc.: 89.06%] [G loss: 3.020036]\n",
      "epoch:38 step:30018 [D loss: 0.411432, acc.: 80.47%] [G loss: 3.415484]\n",
      "epoch:38 step:30019 [D loss: 0.268699, acc.: 89.06%] [G loss: 2.897649]\n",
      "epoch:38 step:30020 [D loss: 0.309539, acc.: 85.16%] [G loss: 3.405455]\n",
      "epoch:38 step:30021 [D loss: 0.253392, acc.: 87.50%] [G loss: 4.096122]\n",
      "epoch:38 step:30022 [D loss: 0.303647, acc.: 86.72%] [G loss: 3.602820]\n",
      "epoch:38 step:30023 [D loss: 0.349684, acc.: 86.72%] [G loss: 2.995697]\n",
      "epoch:38 step:30024 [D loss: 0.355415, acc.: 84.38%] [G loss: 3.892120]\n",
      "epoch:38 step:30025 [D loss: 0.263757, acc.: 86.72%] [G loss: 3.034912]\n",
      "epoch:38 step:30026 [D loss: 0.519978, acc.: 76.56%] [G loss: 2.556502]\n",
      "epoch:38 step:30027 [D loss: 0.315186, acc.: 82.81%] [G loss: 4.069891]\n",
      "epoch:38 step:30028 [D loss: 0.333541, acc.: 82.03%] [G loss: 3.319451]\n",
      "epoch:38 step:30029 [D loss: 0.366936, acc.: 83.59%] [G loss: 2.898074]\n",
      "epoch:38 step:30030 [D loss: 0.365026, acc.: 83.59%] [G loss: 2.443834]\n",
      "epoch:38 step:30031 [D loss: 0.368299, acc.: 83.59%] [G loss: 3.029142]\n",
      "epoch:38 step:30032 [D loss: 0.269716, acc.: 87.50%] [G loss: 2.492833]\n",
      "epoch:38 step:30033 [D loss: 0.253291, acc.: 85.94%] [G loss: 3.121711]\n",
      "epoch:38 step:30034 [D loss: 0.360134, acc.: 82.03%] [G loss: 2.832914]\n",
      "epoch:38 step:30035 [D loss: 0.342451, acc.: 82.81%] [G loss: 2.777562]\n",
      "epoch:38 step:30036 [D loss: 0.400789, acc.: 81.25%] [G loss: 2.452758]\n",
      "epoch:38 step:30037 [D loss: 0.359990, acc.: 85.94%] [G loss: 3.202132]\n",
      "epoch:38 step:30038 [D loss: 0.353915, acc.: 86.72%] [G loss: 4.993378]\n",
      "epoch:38 step:30039 [D loss: 0.479409, acc.: 82.03%] [G loss: 4.170917]\n",
      "epoch:38 step:30040 [D loss: 0.464719, acc.: 77.34%] [G loss: 4.058563]\n",
      "epoch:38 step:30041 [D loss: 0.529694, acc.: 75.78%] [G loss: 5.824386]\n",
      "epoch:38 step:30042 [D loss: 0.740198, acc.: 70.31%] [G loss: 5.412981]\n",
      "epoch:38 step:30043 [D loss: 0.507039, acc.: 78.91%] [G loss: 3.431618]\n",
      "epoch:38 step:30044 [D loss: 0.376027, acc.: 83.59%] [G loss: 3.675281]\n",
      "epoch:38 step:30045 [D loss: 0.436318, acc.: 76.56%] [G loss: 2.808429]\n",
      "epoch:38 step:30046 [D loss: 0.316932, acc.: 85.16%] [G loss: 4.392807]\n",
      "epoch:38 step:30047 [D loss: 0.307603, acc.: 85.16%] [G loss: 3.299978]\n",
      "epoch:38 step:30048 [D loss: 0.289335, acc.: 89.84%] [G loss: 3.206070]\n",
      "epoch:38 step:30049 [D loss: 0.345436, acc.: 85.16%] [G loss: 3.114811]\n",
      "epoch:38 step:30050 [D loss: 0.245876, acc.: 89.06%] [G loss: 2.657120]\n",
      "epoch:38 step:30051 [D loss: 0.425722, acc.: 85.16%] [G loss: 2.985524]\n",
      "epoch:38 step:30052 [D loss: 0.379702, acc.: 83.59%] [G loss: 3.125170]\n",
      "epoch:38 step:30053 [D loss: 0.432900, acc.: 78.91%] [G loss: 3.406030]\n",
      "epoch:38 step:30054 [D loss: 0.368041, acc.: 83.59%] [G loss: 2.665064]\n",
      "epoch:38 step:30055 [D loss: 0.334064, acc.: 86.72%] [G loss: 2.839669]\n",
      "epoch:38 step:30056 [D loss: 0.374255, acc.: 85.16%] [G loss: 2.038150]\n",
      "epoch:38 step:30057 [D loss: 0.345923, acc.: 87.50%] [G loss: 3.067651]\n",
      "epoch:38 step:30058 [D loss: 0.379963, acc.: 85.16%] [G loss: 2.362356]\n",
      "epoch:38 step:30059 [D loss: 0.273363, acc.: 89.84%] [G loss: 2.657207]\n",
      "epoch:38 step:30060 [D loss: 0.391236, acc.: 82.03%] [G loss: 2.555810]\n",
      "epoch:38 step:30061 [D loss: 0.439445, acc.: 80.47%] [G loss: 2.772398]\n",
      "epoch:38 step:30062 [D loss: 0.230659, acc.: 89.84%] [G loss: 2.968156]\n",
      "epoch:38 step:30063 [D loss: 0.387991, acc.: 86.72%] [G loss: 3.412966]\n",
      "epoch:38 step:30064 [D loss: 0.415359, acc.: 79.69%] [G loss: 3.938952]\n",
      "epoch:38 step:30065 [D loss: 0.389395, acc.: 82.03%] [G loss: 3.804432]\n",
      "epoch:38 step:30066 [D loss: 0.431746, acc.: 76.56%] [G loss: 3.079751]\n",
      "epoch:38 step:30067 [D loss: 0.336180, acc.: 80.47%] [G loss: 3.086043]\n",
      "epoch:38 step:30068 [D loss: 0.354077, acc.: 88.28%] [G loss: 4.008132]\n",
      "epoch:38 step:30069 [D loss: 0.422291, acc.: 81.25%] [G loss: 3.323176]\n",
      "epoch:38 step:30070 [D loss: 0.358684, acc.: 82.81%] [G loss: 2.857267]\n",
      "epoch:38 step:30071 [D loss: 0.242308, acc.: 88.28%] [G loss: 3.806156]\n",
      "epoch:38 step:30072 [D loss: 0.395308, acc.: 81.25%] [G loss: 4.367846]\n",
      "epoch:38 step:30073 [D loss: 0.340240, acc.: 91.41%] [G loss: 3.513541]\n",
      "epoch:38 step:30074 [D loss: 0.263745, acc.: 87.50%] [G loss: 3.134215]\n",
      "epoch:38 step:30075 [D loss: 0.321597, acc.: 84.38%] [G loss: 2.961896]\n",
      "epoch:38 step:30076 [D loss: 0.421197, acc.: 79.69%] [G loss: 2.640589]\n",
      "epoch:38 step:30077 [D loss: 0.280107, acc.: 89.06%] [G loss: 2.804789]\n",
      "epoch:38 step:30078 [D loss: 0.355395, acc.: 86.72%] [G loss: 3.027194]\n",
      "epoch:38 step:30079 [D loss: 0.320341, acc.: 86.72%] [G loss: 2.484423]\n",
      "epoch:38 step:30080 [D loss: 0.321487, acc.: 85.94%] [G loss: 2.788506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30081 [D loss: 0.383798, acc.: 82.03%] [G loss: 2.807966]\n",
      "epoch:38 step:30082 [D loss: 0.382233, acc.: 84.38%] [G loss: 2.624588]\n",
      "epoch:38 step:30083 [D loss: 0.317282, acc.: 85.94%] [G loss: 2.835393]\n",
      "epoch:38 step:30084 [D loss: 0.332242, acc.: 85.16%] [G loss: 3.178319]\n",
      "epoch:38 step:30085 [D loss: 0.342556, acc.: 83.59%] [G loss: 2.601758]\n",
      "epoch:38 step:30086 [D loss: 0.290263, acc.: 86.72%] [G loss: 2.602468]\n",
      "epoch:38 step:30087 [D loss: 0.232061, acc.: 92.19%] [G loss: 2.972289]\n",
      "epoch:38 step:30088 [D loss: 0.362575, acc.: 83.59%] [G loss: 2.519653]\n",
      "epoch:38 step:30089 [D loss: 0.368930, acc.: 84.38%] [G loss: 3.001015]\n",
      "epoch:38 step:30090 [D loss: 0.270917, acc.: 85.94%] [G loss: 3.384655]\n",
      "epoch:38 step:30091 [D loss: 0.303254, acc.: 85.94%] [G loss: 3.012440]\n",
      "epoch:38 step:30092 [D loss: 0.402298, acc.: 82.81%] [G loss: 2.926475]\n",
      "epoch:38 step:30093 [D loss: 0.259316, acc.: 89.84%] [G loss: 3.073872]\n",
      "epoch:38 step:30094 [D loss: 0.319216, acc.: 82.81%] [G loss: 3.346723]\n",
      "epoch:38 step:30095 [D loss: 0.271237, acc.: 85.94%] [G loss: 3.409622]\n",
      "epoch:38 step:30096 [D loss: 0.276476, acc.: 88.28%] [G loss: 3.357715]\n",
      "epoch:38 step:30097 [D loss: 0.323140, acc.: 85.94%] [G loss: 2.570638]\n",
      "epoch:38 step:30098 [D loss: 0.271390, acc.: 86.72%] [G loss: 3.379264]\n",
      "epoch:38 step:30099 [D loss: 0.267236, acc.: 85.94%] [G loss: 3.184082]\n",
      "epoch:38 step:30100 [D loss: 0.332712, acc.: 85.94%] [G loss: 4.043045]\n",
      "epoch:38 step:30101 [D loss: 0.367654, acc.: 82.03%] [G loss: 3.536385]\n",
      "epoch:38 step:30102 [D loss: 0.255870, acc.: 87.50%] [G loss: 4.517861]\n",
      "epoch:38 step:30103 [D loss: 0.324439, acc.: 85.16%] [G loss: 2.485751]\n",
      "epoch:38 step:30104 [D loss: 0.316890, acc.: 86.72%] [G loss: 3.065369]\n",
      "epoch:38 step:30105 [D loss: 0.299706, acc.: 87.50%] [G loss: 3.036471]\n",
      "epoch:38 step:30106 [D loss: 0.406793, acc.: 80.47%] [G loss: 5.043674]\n",
      "epoch:38 step:30107 [D loss: 0.435289, acc.: 78.91%] [G loss: 3.929929]\n",
      "epoch:38 step:30108 [D loss: 0.392094, acc.: 81.25%] [G loss: 4.724077]\n",
      "epoch:38 step:30109 [D loss: 0.423998, acc.: 82.03%] [G loss: 3.820069]\n",
      "epoch:38 step:30110 [D loss: 0.344841, acc.: 87.50%] [G loss: 3.627776]\n",
      "epoch:38 step:30111 [D loss: 0.183931, acc.: 91.41%] [G loss: 3.441214]\n",
      "epoch:38 step:30112 [D loss: 0.232749, acc.: 90.62%] [G loss: 4.092118]\n",
      "epoch:38 step:30113 [D loss: 0.304583, acc.: 88.28%] [G loss: 4.548296]\n",
      "epoch:38 step:30114 [D loss: 0.371782, acc.: 82.03%] [G loss: 3.410276]\n",
      "epoch:38 step:30115 [D loss: 0.252689, acc.: 89.06%] [G loss: 3.503609]\n",
      "epoch:38 step:30116 [D loss: 0.370569, acc.: 82.03%] [G loss: 3.764122]\n",
      "epoch:38 step:30117 [D loss: 0.323324, acc.: 84.38%] [G loss: 3.303731]\n",
      "epoch:38 step:30118 [D loss: 0.318026, acc.: 86.72%] [G loss: 3.642885]\n",
      "epoch:38 step:30119 [D loss: 0.375162, acc.: 80.47%] [G loss: 3.334374]\n",
      "epoch:38 step:30120 [D loss: 0.257634, acc.: 88.28%] [G loss: 2.841911]\n",
      "epoch:38 step:30121 [D loss: 0.362245, acc.: 83.59%] [G loss: 3.227719]\n",
      "epoch:38 step:30122 [D loss: 0.520939, acc.: 71.09%] [G loss: 4.101424]\n",
      "epoch:38 step:30123 [D loss: 0.591220, acc.: 78.91%] [G loss: 6.061519]\n",
      "epoch:38 step:30124 [D loss: 0.454664, acc.: 82.81%] [G loss: 6.289041]\n",
      "epoch:38 step:30125 [D loss: 0.210713, acc.: 90.62%] [G loss: 5.504755]\n",
      "epoch:38 step:30126 [D loss: 0.455916, acc.: 83.59%] [G loss: 3.803071]\n",
      "epoch:38 step:30127 [D loss: 0.250335, acc.: 90.62%] [G loss: 5.372060]\n",
      "epoch:38 step:30128 [D loss: 0.257270, acc.: 89.84%] [G loss: 3.838049]\n",
      "epoch:38 step:30129 [D loss: 0.302804, acc.: 86.72%] [G loss: 2.914621]\n",
      "epoch:38 step:30130 [D loss: 0.274618, acc.: 88.28%] [G loss: 4.652543]\n",
      "epoch:38 step:30131 [D loss: 0.266370, acc.: 87.50%] [G loss: 4.436569]\n",
      "epoch:38 step:30132 [D loss: 0.350326, acc.: 85.16%] [G loss: 3.302559]\n",
      "epoch:38 step:30133 [D loss: 0.265546, acc.: 86.72%] [G loss: 4.372852]\n",
      "epoch:38 step:30134 [D loss: 0.352718, acc.: 83.59%] [G loss: 4.544566]\n",
      "epoch:38 step:30135 [D loss: 0.223090, acc.: 91.41%] [G loss: 4.427065]\n",
      "epoch:38 step:30136 [D loss: 0.301067, acc.: 85.94%] [G loss: 4.231793]\n",
      "epoch:38 step:30137 [D loss: 0.214498, acc.: 89.06%] [G loss: 4.148800]\n",
      "epoch:38 step:30138 [D loss: 0.304735, acc.: 83.59%] [G loss: 3.920816]\n",
      "epoch:38 step:30139 [D loss: 0.249538, acc.: 90.62%] [G loss: 3.478728]\n",
      "epoch:38 step:30140 [D loss: 0.269196, acc.: 90.62%] [G loss: 2.943721]\n",
      "epoch:38 step:30141 [D loss: 0.357942, acc.: 83.59%] [G loss: 3.878368]\n",
      "epoch:38 step:30142 [D loss: 0.360756, acc.: 84.38%] [G loss: 3.517324]\n",
      "epoch:38 step:30143 [D loss: 0.274860, acc.: 88.28%] [G loss: 4.620963]\n",
      "epoch:38 step:30144 [D loss: 0.311730, acc.: 84.38%] [G loss: 3.507585]\n",
      "epoch:38 step:30145 [D loss: 0.372492, acc.: 82.81%] [G loss: 3.548171]\n",
      "epoch:38 step:30146 [D loss: 0.334133, acc.: 87.50%] [G loss: 3.318456]\n",
      "epoch:38 step:30147 [D loss: 0.290184, acc.: 85.94%] [G loss: 3.102332]\n",
      "epoch:38 step:30148 [D loss: 0.198073, acc.: 92.19%] [G loss: 2.877029]\n",
      "epoch:38 step:30149 [D loss: 0.240314, acc.: 90.62%] [G loss: 2.509482]\n",
      "epoch:38 step:30150 [D loss: 0.348504, acc.: 84.38%] [G loss: 3.200118]\n",
      "epoch:38 step:30151 [D loss: 0.413469, acc.: 81.25%] [G loss: 2.214113]\n",
      "epoch:38 step:30152 [D loss: 0.353887, acc.: 84.38%] [G loss: 3.187029]\n",
      "epoch:38 step:30153 [D loss: 0.436438, acc.: 76.56%] [G loss: 2.740168]\n",
      "epoch:38 step:30154 [D loss: 0.299704, acc.: 85.94%] [G loss: 3.281131]\n",
      "epoch:38 step:30155 [D loss: 0.361163, acc.: 84.38%] [G loss: 3.337814]\n",
      "epoch:38 step:30156 [D loss: 0.401779, acc.: 82.03%] [G loss: 2.796781]\n",
      "epoch:38 step:30157 [D loss: 0.296318, acc.: 89.06%] [G loss: 3.713135]\n",
      "epoch:38 step:30158 [D loss: 0.253599, acc.: 87.50%] [G loss: 3.762798]\n",
      "epoch:38 step:30159 [D loss: 0.324816, acc.: 81.25%] [G loss: 6.463537]\n",
      "epoch:38 step:30160 [D loss: 0.251533, acc.: 89.06%] [G loss: 4.574776]\n",
      "epoch:38 step:30161 [D loss: 0.228170, acc.: 89.06%] [G loss: 5.569071]\n",
      "epoch:38 step:30162 [D loss: 0.250527, acc.: 88.28%] [G loss: 3.238320]\n",
      "epoch:38 step:30163 [D loss: 0.262547, acc.: 91.41%] [G loss: 4.041792]\n",
      "epoch:38 step:30164 [D loss: 0.221904, acc.: 89.84%] [G loss: 4.088861]\n",
      "epoch:38 step:30165 [D loss: 0.357785, acc.: 83.59%] [G loss: 4.677843]\n",
      "epoch:38 step:30166 [D loss: 0.377344, acc.: 82.81%] [G loss: 4.569775]\n",
      "epoch:38 step:30167 [D loss: 0.425916, acc.: 82.03%] [G loss: 3.931131]\n",
      "epoch:38 step:30168 [D loss: 0.462937, acc.: 78.91%] [G loss: 2.789786]\n",
      "epoch:38 step:30169 [D loss: 0.314837, acc.: 82.81%] [G loss: 4.171602]\n",
      "epoch:38 step:30170 [D loss: 0.420513, acc.: 83.59%] [G loss: 4.039351]\n",
      "epoch:38 step:30171 [D loss: 0.402656, acc.: 79.69%] [G loss: 3.072003]\n",
      "epoch:38 step:30172 [D loss: 0.311504, acc.: 85.94%] [G loss: 3.300560]\n",
      "epoch:38 step:30173 [D loss: 0.250680, acc.: 89.84%] [G loss: 3.172315]\n",
      "epoch:38 step:30174 [D loss: 0.329415, acc.: 84.38%] [G loss: 2.860920]\n",
      "epoch:38 step:30175 [D loss: 0.426733, acc.: 83.59%] [G loss: 3.033890]\n",
      "epoch:38 step:30176 [D loss: 0.392707, acc.: 80.47%] [G loss: 2.793836]\n",
      "epoch:38 step:30177 [D loss: 0.305789, acc.: 86.72%] [G loss: 3.767234]\n",
      "epoch:38 step:30178 [D loss: 0.377238, acc.: 82.03%] [G loss: 3.196229]\n",
      "epoch:38 step:30179 [D loss: 0.328194, acc.: 85.16%] [G loss: 4.084470]\n",
      "epoch:38 step:30180 [D loss: 0.425367, acc.: 82.03%] [G loss: 3.212232]\n",
      "epoch:38 step:30181 [D loss: 0.338261, acc.: 82.81%] [G loss: 2.679512]\n",
      "epoch:38 step:30182 [D loss: 0.338843, acc.: 85.94%] [G loss: 3.283555]\n",
      "epoch:38 step:30183 [D loss: 0.327441, acc.: 87.50%] [G loss: 3.091685]\n",
      "epoch:38 step:30184 [D loss: 0.353270, acc.: 82.81%] [G loss: 2.802974]\n",
      "epoch:38 step:30185 [D loss: 0.328208, acc.: 82.81%] [G loss: 3.014723]\n",
      "epoch:38 step:30186 [D loss: 0.227015, acc.: 92.97%] [G loss: 2.411451]\n",
      "epoch:38 step:30187 [D loss: 0.361265, acc.: 82.81%] [G loss: 2.783029]\n",
      "epoch:38 step:30188 [D loss: 0.280659, acc.: 86.72%] [G loss: 2.496797]\n",
      "epoch:38 step:30189 [D loss: 0.328583, acc.: 85.16%] [G loss: 3.123594]\n",
      "epoch:38 step:30190 [D loss: 0.289982, acc.: 88.28%] [G loss: 2.800689]\n",
      "epoch:38 step:30191 [D loss: 0.448215, acc.: 76.56%] [G loss: 3.905626]\n",
      "epoch:38 step:30192 [D loss: 0.370288, acc.: 86.72%] [G loss: 3.053656]\n",
      "epoch:38 step:30193 [D loss: 0.374085, acc.: 85.16%] [G loss: 3.337815]\n",
      "epoch:38 step:30194 [D loss: 0.327338, acc.: 86.72%] [G loss: 5.725826]\n",
      "epoch:38 step:30195 [D loss: 0.511325, acc.: 74.22%] [G loss: 5.751759]\n",
      "epoch:38 step:30196 [D loss: 0.699187, acc.: 74.22%] [G loss: 9.242072]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30197 [D loss: 1.296165, acc.: 67.19%] [G loss: 7.120443]\n",
      "epoch:38 step:30198 [D loss: 1.251230, acc.: 69.53%] [G loss: 4.181671]\n",
      "epoch:38 step:30199 [D loss: 0.362801, acc.: 86.72%] [G loss: 4.289282]\n",
      "epoch:38 step:30200 [D loss: 0.679690, acc.: 73.44%] [G loss: 5.211287]\n",
      "epoch:38 step:30201 [D loss: 0.378602, acc.: 90.62%] [G loss: 4.677824]\n",
      "epoch:38 step:30202 [D loss: 0.314271, acc.: 89.06%] [G loss: 4.132609]\n",
      "epoch:38 step:30203 [D loss: 0.428161, acc.: 77.34%] [G loss: 6.461756]\n",
      "epoch:38 step:30204 [D loss: 0.600142, acc.: 76.56%] [G loss: 4.626433]\n",
      "epoch:38 step:30205 [D loss: 0.313552, acc.: 82.81%] [G loss: 3.384875]\n",
      "epoch:38 step:30206 [D loss: 0.410966, acc.: 79.69%] [G loss: 3.413421]\n",
      "epoch:38 step:30207 [D loss: 0.321812, acc.: 86.72%] [G loss: 2.672198]\n",
      "epoch:38 step:30208 [D loss: 0.433420, acc.: 85.94%] [G loss: 3.124870]\n",
      "epoch:38 step:30209 [D loss: 0.452980, acc.: 83.59%] [G loss: 3.017158]\n",
      "epoch:38 step:30210 [D loss: 0.361526, acc.: 82.03%] [G loss: 2.185129]\n",
      "epoch:38 step:30211 [D loss: 0.298738, acc.: 87.50%] [G loss: 2.323629]\n",
      "epoch:38 step:30212 [D loss: 0.274102, acc.: 87.50%] [G loss: 2.868041]\n",
      "epoch:38 step:30213 [D loss: 0.312421, acc.: 85.16%] [G loss: 2.791384]\n",
      "epoch:38 step:30214 [D loss: 0.343749, acc.: 83.59%] [G loss: 2.313710]\n",
      "epoch:38 step:30215 [D loss: 0.390983, acc.: 78.91%] [G loss: 2.655300]\n",
      "epoch:38 step:30216 [D loss: 0.354254, acc.: 82.81%] [G loss: 3.024338]\n",
      "epoch:38 step:30217 [D loss: 0.306105, acc.: 86.72%] [G loss: 2.971464]\n",
      "epoch:38 step:30218 [D loss: 0.292064, acc.: 87.50%] [G loss: 2.852514]\n",
      "epoch:38 step:30219 [D loss: 0.300064, acc.: 85.94%] [G loss: 3.001080]\n",
      "epoch:38 step:30220 [D loss: 0.392575, acc.: 85.16%] [G loss: 2.212890]\n",
      "epoch:38 step:30221 [D loss: 0.318737, acc.: 86.72%] [G loss: 3.262082]\n",
      "epoch:38 step:30222 [D loss: 0.470043, acc.: 79.69%] [G loss: 2.659800]\n",
      "epoch:38 step:30223 [D loss: 0.283456, acc.: 86.72%] [G loss: 2.448827]\n",
      "epoch:38 step:30224 [D loss: 0.426667, acc.: 75.78%] [G loss: 2.585900]\n",
      "epoch:38 step:30225 [D loss: 0.480206, acc.: 75.78%] [G loss: 2.753823]\n",
      "epoch:38 step:30226 [D loss: 0.260374, acc.: 90.62%] [G loss: 2.682938]\n",
      "epoch:38 step:30227 [D loss: 0.315131, acc.: 85.94%] [G loss: 2.147238]\n",
      "epoch:38 step:30228 [D loss: 0.298987, acc.: 89.06%] [G loss: 2.761071]\n",
      "epoch:38 step:30229 [D loss: 0.322257, acc.: 85.94%] [G loss: 2.674332]\n",
      "epoch:38 step:30230 [D loss: 0.256113, acc.: 90.62%] [G loss: 2.480280]\n",
      "epoch:38 step:30231 [D loss: 0.315713, acc.: 89.06%] [G loss: 2.420758]\n",
      "epoch:38 step:30232 [D loss: 0.366998, acc.: 85.94%] [G loss: 2.659426]\n",
      "epoch:38 step:30233 [D loss: 0.331257, acc.: 85.16%] [G loss: 2.494119]\n",
      "epoch:38 step:30234 [D loss: 0.289486, acc.: 88.28%] [G loss: 3.680381]\n",
      "epoch:38 step:30235 [D loss: 0.264844, acc.: 90.62%] [G loss: 3.872850]\n",
      "epoch:38 step:30236 [D loss: 0.266576, acc.: 87.50%] [G loss: 3.334651]\n",
      "epoch:38 step:30237 [D loss: 0.303478, acc.: 85.94%] [G loss: 3.633442]\n",
      "epoch:38 step:30238 [D loss: 0.271260, acc.: 88.28%] [G loss: 3.325191]\n",
      "epoch:38 step:30239 [D loss: 0.382291, acc.: 87.50%] [G loss: 3.249846]\n",
      "epoch:38 step:30240 [D loss: 0.330918, acc.: 85.94%] [G loss: 3.576035]\n",
      "epoch:38 step:30241 [D loss: 0.272152, acc.: 87.50%] [G loss: 3.237143]\n",
      "epoch:38 step:30242 [D loss: 0.265778, acc.: 89.84%] [G loss: 3.320597]\n",
      "epoch:38 step:30243 [D loss: 0.305718, acc.: 88.28%] [G loss: 3.850673]\n",
      "epoch:38 step:30244 [D loss: 0.325056, acc.: 85.16%] [G loss: 4.318266]\n",
      "epoch:38 step:30245 [D loss: 0.368518, acc.: 82.81%] [G loss: 2.870628]\n",
      "epoch:38 step:30246 [D loss: 0.399131, acc.: 80.47%] [G loss: 4.172307]\n",
      "epoch:38 step:30247 [D loss: 0.323703, acc.: 88.28%] [G loss: 4.746992]\n",
      "epoch:38 step:30248 [D loss: 0.231848, acc.: 92.19%] [G loss: 5.093788]\n",
      "epoch:38 step:30249 [D loss: 0.360934, acc.: 84.38%] [G loss: 4.912751]\n",
      "epoch:38 step:30250 [D loss: 0.255713, acc.: 88.28%] [G loss: 5.305351]\n",
      "epoch:38 step:30251 [D loss: 0.276057, acc.: 89.06%] [G loss: 6.404427]\n",
      "epoch:38 step:30252 [D loss: 0.199336, acc.: 90.62%] [G loss: 5.071869]\n",
      "epoch:38 step:30253 [D loss: 0.212645, acc.: 89.06%] [G loss: 3.670369]\n",
      "epoch:38 step:30254 [D loss: 0.328555, acc.: 85.16%] [G loss: 4.675603]\n",
      "epoch:38 step:30255 [D loss: 0.348433, acc.: 85.94%] [G loss: 3.288822]\n",
      "epoch:38 step:30256 [D loss: 0.371469, acc.: 82.03%] [G loss: 3.271421]\n",
      "epoch:38 step:30257 [D loss: 0.433109, acc.: 79.69%] [G loss: 2.515106]\n",
      "epoch:38 step:30258 [D loss: 0.352877, acc.: 82.03%] [G loss: 2.619461]\n",
      "epoch:38 step:30259 [D loss: 0.383650, acc.: 82.81%] [G loss: 3.453149]\n",
      "epoch:38 step:30260 [D loss: 0.332696, acc.: 83.59%] [G loss: 3.449864]\n",
      "epoch:38 step:30261 [D loss: 0.411672, acc.: 81.25%] [G loss: 4.215408]\n",
      "epoch:38 step:30262 [D loss: 0.406678, acc.: 80.47%] [G loss: 2.809471]\n",
      "epoch:38 step:30263 [D loss: 0.216807, acc.: 90.62%] [G loss: 2.941521]\n",
      "epoch:38 step:30264 [D loss: 0.468473, acc.: 77.34%] [G loss: 3.032162]\n",
      "epoch:38 step:30265 [D loss: 0.284832, acc.: 87.50%] [G loss: 3.252529]\n",
      "epoch:38 step:30266 [D loss: 0.310622, acc.: 82.81%] [G loss: 5.850393]\n",
      "epoch:38 step:30267 [D loss: 0.288344, acc.: 86.72%] [G loss: 5.363924]\n",
      "epoch:38 step:30268 [D loss: 0.277289, acc.: 84.38%] [G loss: 5.138088]\n",
      "epoch:38 step:30269 [D loss: 0.277895, acc.: 85.16%] [G loss: 5.024746]\n",
      "epoch:38 step:30270 [D loss: 0.234159, acc.: 87.50%] [G loss: 4.708099]\n",
      "epoch:38 step:30271 [D loss: 0.275100, acc.: 87.50%] [G loss: 5.299023]\n",
      "epoch:38 step:30272 [D loss: 0.234190, acc.: 92.19%] [G loss: 4.473182]\n",
      "epoch:38 step:30273 [D loss: 0.357529, acc.: 84.38%] [G loss: 4.798848]\n",
      "epoch:38 step:30274 [D loss: 0.321308, acc.: 88.28%] [G loss: 2.606629]\n",
      "epoch:38 step:30275 [D loss: 0.277361, acc.: 89.84%] [G loss: 3.064930]\n",
      "epoch:38 step:30276 [D loss: 0.452115, acc.: 77.34%] [G loss: 3.355360]\n",
      "epoch:38 step:30277 [D loss: 0.302070, acc.: 86.72%] [G loss: 4.378440]\n",
      "epoch:38 step:30278 [D loss: 0.366090, acc.: 84.38%] [G loss: 2.751997]\n",
      "epoch:38 step:30279 [D loss: 0.213692, acc.: 91.41%] [G loss: 3.014618]\n",
      "epoch:38 step:30280 [D loss: 0.301776, acc.: 87.50%] [G loss: 3.409074]\n",
      "epoch:38 step:30281 [D loss: 0.267826, acc.: 89.06%] [G loss: 2.695892]\n",
      "epoch:38 step:30282 [D loss: 0.327356, acc.: 89.06%] [G loss: 2.961620]\n",
      "epoch:38 step:30283 [D loss: 0.401525, acc.: 82.03%] [G loss: 2.248110]\n",
      "epoch:38 step:30284 [D loss: 0.506476, acc.: 74.22%] [G loss: 2.697522]\n",
      "epoch:38 step:30285 [D loss: 0.228463, acc.: 89.84%] [G loss: 2.794158]\n",
      "epoch:38 step:30286 [D loss: 0.250643, acc.: 89.06%] [G loss: 2.883268]\n",
      "epoch:38 step:30287 [D loss: 0.348163, acc.: 82.03%] [G loss: 3.383463]\n",
      "epoch:38 step:30288 [D loss: 0.427621, acc.: 80.47%] [G loss: 3.183207]\n",
      "epoch:38 step:30289 [D loss: 0.451575, acc.: 78.12%] [G loss: 2.638146]\n",
      "epoch:38 step:30290 [D loss: 0.374529, acc.: 86.72%] [G loss: 3.195476]\n",
      "epoch:38 step:30291 [D loss: 0.364490, acc.: 83.59%] [G loss: 2.868578]\n",
      "epoch:38 step:30292 [D loss: 0.393994, acc.: 79.69%] [G loss: 3.983934]\n",
      "epoch:38 step:30293 [D loss: 0.316417, acc.: 85.94%] [G loss: 2.827982]\n",
      "epoch:38 step:30294 [D loss: 0.224475, acc.: 88.28%] [G loss: 2.439841]\n",
      "epoch:38 step:30295 [D loss: 0.426186, acc.: 82.81%] [G loss: 3.396682]\n",
      "epoch:38 step:30296 [D loss: 0.411641, acc.: 79.69%] [G loss: 2.445036]\n",
      "epoch:38 step:30297 [D loss: 0.361243, acc.: 85.16%] [G loss: 3.005832]\n",
      "epoch:38 step:30298 [D loss: 0.189497, acc.: 92.97%] [G loss: 3.351759]\n",
      "epoch:38 step:30299 [D loss: 0.332824, acc.: 85.94%] [G loss: 2.972732]\n",
      "epoch:38 step:30300 [D loss: 0.284034, acc.: 86.72%] [G loss: 2.494305]\n",
      "epoch:38 step:30301 [D loss: 0.352351, acc.: 86.72%] [G loss: 2.713274]\n",
      "epoch:38 step:30302 [D loss: 0.314883, acc.: 86.72%] [G loss: 2.822525]\n",
      "epoch:38 step:30303 [D loss: 0.458717, acc.: 83.59%] [G loss: 2.355441]\n",
      "epoch:38 step:30304 [D loss: 0.365250, acc.: 81.25%] [G loss: 2.962671]\n",
      "epoch:38 step:30305 [D loss: 0.289744, acc.: 89.84%] [G loss: 4.633509]\n",
      "epoch:38 step:30306 [D loss: 0.356031, acc.: 83.59%] [G loss: 3.271025]\n",
      "epoch:38 step:30307 [D loss: 0.272724, acc.: 86.72%] [G loss: 4.731551]\n",
      "epoch:38 step:30308 [D loss: 0.246174, acc.: 87.50%] [G loss: 4.168893]\n",
      "epoch:38 step:30309 [D loss: 0.369536, acc.: 81.25%] [G loss: 3.192635]\n",
      "epoch:38 step:30310 [D loss: 0.254978, acc.: 86.72%] [G loss: 3.445994]\n",
      "epoch:38 step:30311 [D loss: 0.415172, acc.: 79.69%] [G loss: 2.549897]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30312 [D loss: 0.281736, acc.: 91.41%] [G loss: 3.867415]\n",
      "epoch:38 step:30313 [D loss: 0.271101, acc.: 89.84%] [G loss: 3.198085]\n",
      "epoch:38 step:30314 [D loss: 0.276103, acc.: 91.41%] [G loss: 3.236344]\n",
      "epoch:38 step:30315 [D loss: 0.363043, acc.: 83.59%] [G loss: 2.311360]\n",
      "epoch:38 step:30316 [D loss: 0.349720, acc.: 85.16%] [G loss: 2.761876]\n",
      "epoch:38 step:30317 [D loss: 0.241702, acc.: 89.06%] [G loss: 3.638644]\n",
      "epoch:38 step:30318 [D loss: 0.378205, acc.: 82.03%] [G loss: 3.692111]\n",
      "epoch:38 step:30319 [D loss: 0.398293, acc.: 82.81%] [G loss: 3.229819]\n",
      "epoch:38 step:30320 [D loss: 0.268567, acc.: 89.06%] [G loss: 2.544940]\n",
      "epoch:38 step:30321 [D loss: 0.395415, acc.: 82.03%] [G loss: 2.657941]\n",
      "epoch:38 step:30322 [D loss: 0.364941, acc.: 82.81%] [G loss: 3.465922]\n",
      "epoch:38 step:30323 [D loss: 0.329632, acc.: 89.06%] [G loss: 2.948349]\n",
      "epoch:38 step:30324 [D loss: 0.401310, acc.: 82.81%] [G loss: 2.342665]\n",
      "epoch:38 step:30325 [D loss: 0.346230, acc.: 85.94%] [G loss: 2.789283]\n",
      "epoch:38 step:30326 [D loss: 0.327526, acc.: 85.16%] [G loss: 2.668860]\n",
      "epoch:38 step:30327 [D loss: 0.326245, acc.: 88.28%] [G loss: 3.154140]\n",
      "epoch:38 step:30328 [D loss: 0.363761, acc.: 89.06%] [G loss: 3.490481]\n",
      "epoch:38 step:30329 [D loss: 0.348545, acc.: 85.16%] [G loss: 2.634057]\n",
      "epoch:38 step:30330 [D loss: 0.395453, acc.: 82.03%] [G loss: 2.934007]\n",
      "epoch:38 step:30331 [D loss: 0.215958, acc.: 92.19%] [G loss: 3.766081]\n",
      "epoch:38 step:30332 [D loss: 0.378871, acc.: 84.38%] [G loss: 2.602958]\n",
      "epoch:38 step:30333 [D loss: 0.298835, acc.: 88.28%] [G loss: 3.318845]\n",
      "epoch:38 step:30334 [D loss: 0.364574, acc.: 83.59%] [G loss: 2.893276]\n",
      "epoch:38 step:30335 [D loss: 0.362585, acc.: 83.59%] [G loss: 2.943976]\n",
      "epoch:38 step:30336 [D loss: 0.292344, acc.: 85.16%] [G loss: 3.518817]\n",
      "epoch:38 step:30337 [D loss: 0.312166, acc.: 88.28%] [G loss: 3.311871]\n",
      "epoch:38 step:30338 [D loss: 0.285520, acc.: 90.62%] [G loss: 2.982100]\n",
      "epoch:38 step:30339 [D loss: 0.293701, acc.: 87.50%] [G loss: 3.430255]\n",
      "epoch:38 step:30340 [D loss: 0.285723, acc.: 89.06%] [G loss: 3.870450]\n",
      "epoch:38 step:30341 [D loss: 0.271175, acc.: 87.50%] [G loss: 5.271943]\n",
      "epoch:38 step:30342 [D loss: 0.279592, acc.: 83.59%] [G loss: 4.703087]\n",
      "epoch:38 step:30343 [D loss: 0.345033, acc.: 84.38%] [G loss: 3.891115]\n",
      "epoch:38 step:30344 [D loss: 0.199907, acc.: 94.53%] [G loss: 4.810915]\n",
      "epoch:38 step:30345 [D loss: 0.229408, acc.: 91.41%] [G loss: 4.126695]\n",
      "epoch:38 step:30346 [D loss: 0.186284, acc.: 94.53%] [G loss: 4.034266]\n",
      "epoch:38 step:30347 [D loss: 0.343253, acc.: 82.81%] [G loss: 3.194653]\n",
      "epoch:38 step:30348 [D loss: 0.325041, acc.: 84.38%] [G loss: 2.877014]\n",
      "epoch:38 step:30349 [D loss: 0.386706, acc.: 83.59%] [G loss: 2.833157]\n",
      "epoch:38 step:30350 [D loss: 0.383131, acc.: 81.25%] [G loss: 2.862947]\n",
      "epoch:38 step:30351 [D loss: 0.413052, acc.: 83.59%] [G loss: 2.362800]\n",
      "epoch:38 step:30352 [D loss: 0.403630, acc.: 78.91%] [G loss: 2.997169]\n",
      "epoch:38 step:30353 [D loss: 0.291480, acc.: 88.28%] [G loss: 3.260346]\n",
      "epoch:38 step:30354 [D loss: 0.336367, acc.: 85.16%] [G loss: 2.964474]\n",
      "epoch:38 step:30355 [D loss: 0.339145, acc.: 83.59%] [G loss: 2.715756]\n",
      "epoch:38 step:30356 [D loss: 0.432734, acc.: 78.91%] [G loss: 3.320394]\n",
      "epoch:38 step:30357 [D loss: 0.287090, acc.: 87.50%] [G loss: 3.426479]\n",
      "epoch:38 step:30358 [D loss: 0.328476, acc.: 83.59%] [G loss: 2.787820]\n",
      "epoch:38 step:30359 [D loss: 0.308271, acc.: 89.84%] [G loss: 2.429295]\n",
      "epoch:38 step:30360 [D loss: 0.292311, acc.: 87.50%] [G loss: 2.840672]\n",
      "epoch:38 step:30361 [D loss: 0.340172, acc.: 85.94%] [G loss: 3.236716]\n",
      "epoch:38 step:30362 [D loss: 0.300287, acc.: 89.84%] [G loss: 3.350757]\n",
      "epoch:38 step:30363 [D loss: 0.412813, acc.: 79.69%] [G loss: 2.808425]\n",
      "epoch:38 step:30364 [D loss: 0.261560, acc.: 87.50%] [G loss: 3.802601]\n",
      "epoch:38 step:30365 [D loss: 0.341345, acc.: 88.28%] [G loss: 9.441615]\n",
      "epoch:38 step:30366 [D loss: 0.340345, acc.: 84.38%] [G loss: 3.645465]\n",
      "epoch:38 step:30367 [D loss: 0.206192, acc.: 92.19%] [G loss: 5.529388]\n",
      "epoch:38 step:30368 [D loss: 0.319091, acc.: 82.03%] [G loss: 4.096404]\n",
      "epoch:38 step:30369 [D loss: 0.297865, acc.: 86.72%] [G loss: 3.502558]\n",
      "epoch:38 step:30370 [D loss: 0.381403, acc.: 82.03%] [G loss: 3.158857]\n",
      "epoch:38 step:30371 [D loss: 0.329690, acc.: 85.16%] [G loss: 3.219717]\n",
      "epoch:38 step:30372 [D loss: 0.275609, acc.: 87.50%] [G loss: 2.899429]\n",
      "epoch:38 step:30373 [D loss: 0.237809, acc.: 90.62%] [G loss: 2.717749]\n",
      "epoch:38 step:30374 [D loss: 0.307038, acc.: 85.94%] [G loss: 2.464042]\n",
      "epoch:38 step:30375 [D loss: 0.316553, acc.: 87.50%] [G loss: 3.060242]\n",
      "epoch:38 step:30376 [D loss: 0.254919, acc.: 89.06%] [G loss: 3.332133]\n",
      "epoch:38 step:30377 [D loss: 0.298979, acc.: 89.84%] [G loss: 2.457595]\n",
      "epoch:38 step:30378 [D loss: 0.285315, acc.: 86.72%] [G loss: 2.807605]\n",
      "epoch:38 step:30379 [D loss: 0.396418, acc.: 78.91%] [G loss: 2.780799]\n",
      "epoch:38 step:30380 [D loss: 0.262582, acc.: 89.06%] [G loss: 2.796088]\n",
      "epoch:38 step:30381 [D loss: 0.371847, acc.: 80.47%] [G loss: 5.153784]\n",
      "epoch:38 step:30382 [D loss: 0.619369, acc.: 68.75%] [G loss: 3.994557]\n",
      "epoch:38 step:30383 [D loss: 0.499643, acc.: 78.91%] [G loss: 3.425252]\n",
      "epoch:38 step:30384 [D loss: 0.413614, acc.: 81.25%] [G loss: 2.213127]\n",
      "epoch:38 step:30385 [D loss: 0.317238, acc.: 84.38%] [G loss: 2.456294]\n",
      "epoch:38 step:30386 [D loss: 0.408575, acc.: 85.16%] [G loss: 3.911376]\n",
      "epoch:38 step:30387 [D loss: 0.346960, acc.: 85.94%] [G loss: 2.576498]\n",
      "epoch:38 step:30388 [D loss: 0.375826, acc.: 86.72%] [G loss: 3.429573]\n",
      "epoch:38 step:30389 [D loss: 0.387584, acc.: 82.03%] [G loss: 3.378312]\n",
      "epoch:38 step:30390 [D loss: 0.321349, acc.: 85.94%] [G loss: 3.264615]\n",
      "epoch:38 step:30391 [D loss: 0.345885, acc.: 85.94%] [G loss: 2.647067]\n",
      "epoch:38 step:30392 [D loss: 0.292202, acc.: 90.62%] [G loss: 2.852030]\n",
      "epoch:38 step:30393 [D loss: 0.314536, acc.: 89.84%] [G loss: 3.389277]\n",
      "epoch:38 step:30394 [D loss: 0.343785, acc.: 84.38%] [G loss: 3.518022]\n",
      "epoch:38 step:30395 [D loss: 0.298492, acc.: 88.28%] [G loss: 3.855724]\n",
      "epoch:38 step:30396 [D loss: 0.367444, acc.: 85.16%] [G loss: 2.850496]\n",
      "epoch:38 step:30397 [D loss: 0.338460, acc.: 87.50%] [G loss: 3.156028]\n",
      "epoch:38 step:30398 [D loss: 0.300031, acc.: 85.16%] [G loss: 3.127385]\n",
      "epoch:38 step:30399 [D loss: 0.305482, acc.: 91.41%] [G loss: 3.114993]\n",
      "epoch:38 step:30400 [D loss: 0.342947, acc.: 84.38%] [G loss: 2.813172]\n",
      "epoch:38 step:30401 [D loss: 0.228397, acc.: 92.19%] [G loss: 3.334083]\n",
      "epoch:38 step:30402 [D loss: 0.284562, acc.: 88.28%] [G loss: 2.600511]\n",
      "epoch:38 step:30403 [D loss: 0.252923, acc.: 92.97%] [G loss: 2.605517]\n",
      "epoch:38 step:30404 [D loss: 0.287604, acc.: 89.06%] [G loss: 2.998915]\n",
      "epoch:38 step:30405 [D loss: 0.367443, acc.: 82.81%] [G loss: 2.504331]\n",
      "epoch:38 step:30406 [D loss: 0.362005, acc.: 84.38%] [G loss: 2.474999]\n",
      "epoch:38 step:30407 [D loss: 0.328585, acc.: 83.59%] [G loss: 2.828810]\n",
      "epoch:38 step:30408 [D loss: 0.324752, acc.: 88.28%] [G loss: 2.888301]\n",
      "epoch:38 step:30409 [D loss: 0.304046, acc.: 86.72%] [G loss: 4.111381]\n",
      "epoch:38 step:30410 [D loss: 0.330061, acc.: 84.38%] [G loss: 3.227093]\n",
      "epoch:38 step:30411 [D loss: 0.330207, acc.: 85.94%] [G loss: 2.819149]\n",
      "epoch:38 step:30412 [D loss: 0.288540, acc.: 86.72%] [G loss: 4.164543]\n",
      "epoch:38 step:30413 [D loss: 0.339128, acc.: 85.16%] [G loss: 2.991299]\n",
      "epoch:38 step:30414 [D loss: 0.258749, acc.: 85.94%] [G loss: 3.205665]\n",
      "epoch:38 step:30415 [D loss: 0.336297, acc.: 85.94%] [G loss: 2.677078]\n",
      "epoch:38 step:30416 [D loss: 0.296979, acc.: 89.84%] [G loss: 3.608657]\n",
      "epoch:38 step:30417 [D loss: 0.281895, acc.: 85.94%] [G loss: 3.546679]\n",
      "epoch:38 step:30418 [D loss: 0.239496, acc.: 92.97%] [G loss: 3.447307]\n",
      "epoch:38 step:30419 [D loss: 0.296151, acc.: 87.50%] [G loss: 2.517760]\n",
      "epoch:38 step:30420 [D loss: 0.313407, acc.: 87.50%] [G loss: 2.419027]\n",
      "epoch:38 step:30421 [D loss: 0.279332, acc.: 88.28%] [G loss: 3.048034]\n",
      "epoch:38 step:30422 [D loss: 0.284091, acc.: 89.06%] [G loss: 2.764154]\n",
      "epoch:38 step:30423 [D loss: 0.296362, acc.: 86.72%] [G loss: 3.307394]\n",
      "epoch:38 step:30424 [D loss: 0.329247, acc.: 85.16%] [G loss: 2.938168]\n",
      "epoch:38 step:30425 [D loss: 0.282551, acc.: 89.84%] [G loss: 3.272175]\n",
      "epoch:38 step:30426 [D loss: 0.349052, acc.: 85.16%] [G loss: 2.774286]\n",
      "epoch:38 step:30427 [D loss: 0.417687, acc.: 79.69%] [G loss: 3.367119]\n",
      "epoch:38 step:30428 [D loss: 0.415851, acc.: 83.59%] [G loss: 3.349617]\n",
      "epoch:38 step:30429 [D loss: 0.299852, acc.: 85.16%] [G loss: 4.582989]\n",
      "epoch:38 step:30430 [D loss: 0.494291, acc.: 82.03%] [G loss: 4.117475]\n",
      "epoch:38 step:30431 [D loss: 0.538700, acc.: 75.00%] [G loss: 3.590760]\n",
      "epoch:38 step:30432 [D loss: 0.412402, acc.: 79.69%] [G loss: 5.796471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30433 [D loss: 0.384672, acc.: 82.81%] [G loss: 4.144878]\n",
      "epoch:38 step:30434 [D loss: 0.330449, acc.: 82.81%] [G loss: 5.162510]\n",
      "epoch:38 step:30435 [D loss: 0.245503, acc.: 89.06%] [G loss: 4.484664]\n",
      "epoch:38 step:30436 [D loss: 0.364383, acc.: 87.50%] [G loss: 2.849589]\n",
      "epoch:38 step:30437 [D loss: 0.263493, acc.: 90.62%] [G loss: 4.415896]\n",
      "epoch:38 step:30438 [D loss: 0.210799, acc.: 92.97%] [G loss: 4.302218]\n",
      "epoch:38 step:30439 [D loss: 0.278304, acc.: 85.94%] [G loss: 3.033577]\n",
      "epoch:38 step:30440 [D loss: 0.390594, acc.: 85.16%] [G loss: 2.372642]\n",
      "epoch:38 step:30441 [D loss: 0.440096, acc.: 78.12%] [G loss: 2.581081]\n",
      "epoch:38 step:30442 [D loss: 0.429687, acc.: 81.25%] [G loss: 2.708854]\n",
      "epoch:38 step:30443 [D loss: 0.391489, acc.: 78.12%] [G loss: 2.832273]\n",
      "epoch:38 step:30444 [D loss: 0.259370, acc.: 89.06%] [G loss: 2.553008]\n",
      "epoch:38 step:30445 [D loss: 0.296423, acc.: 88.28%] [G loss: 2.580633]\n",
      "epoch:38 step:30446 [D loss: 0.304751, acc.: 88.28%] [G loss: 2.519073]\n",
      "epoch:38 step:30447 [D loss: 0.260842, acc.: 89.84%] [G loss: 2.904711]\n",
      "epoch:38 step:30448 [D loss: 0.324167, acc.: 85.94%] [G loss: 2.854533]\n",
      "epoch:38 step:30449 [D loss: 0.370968, acc.: 87.50%] [G loss: 2.769345]\n",
      "epoch:38 step:30450 [D loss: 0.329813, acc.: 85.94%] [G loss: 2.503229]\n",
      "epoch:38 step:30451 [D loss: 0.304504, acc.: 85.16%] [G loss: 3.077253]\n",
      "epoch:38 step:30452 [D loss: 0.379985, acc.: 81.25%] [G loss: 2.413683]\n",
      "epoch:38 step:30453 [D loss: 0.324019, acc.: 85.94%] [G loss: 3.040785]\n",
      "epoch:38 step:30454 [D loss: 0.247023, acc.: 89.06%] [G loss: 3.076389]\n",
      "epoch:38 step:30455 [D loss: 0.309840, acc.: 84.38%] [G loss: 3.332020]\n",
      "epoch:38 step:30456 [D loss: 0.329656, acc.: 85.94%] [G loss: 3.140728]\n",
      "epoch:38 step:30457 [D loss: 0.328436, acc.: 82.03%] [G loss: 3.844848]\n",
      "epoch:38 step:30458 [D loss: 0.298755, acc.: 84.38%] [G loss: 3.917309]\n",
      "epoch:38 step:30459 [D loss: 0.370428, acc.: 78.91%] [G loss: 3.041878]\n",
      "epoch:39 step:30460 [D loss: 0.357703, acc.: 85.16%] [G loss: 2.810829]\n",
      "epoch:39 step:30461 [D loss: 0.237984, acc.: 89.06%] [G loss: 3.320968]\n",
      "epoch:39 step:30462 [D loss: 0.385229, acc.: 82.81%] [G loss: 2.758561]\n",
      "epoch:39 step:30463 [D loss: 0.382324, acc.: 82.81%] [G loss: 3.240185]\n",
      "epoch:39 step:30464 [D loss: 0.238161, acc.: 89.84%] [G loss: 3.944626]\n",
      "epoch:39 step:30465 [D loss: 0.331903, acc.: 85.16%] [G loss: 2.987742]\n",
      "epoch:39 step:30466 [D loss: 0.283626, acc.: 83.59%] [G loss: 3.949256]\n",
      "epoch:39 step:30467 [D loss: 0.231283, acc.: 90.62%] [G loss: 3.653920]\n",
      "epoch:39 step:30468 [D loss: 0.360742, acc.: 83.59%] [G loss: 4.589769]\n",
      "epoch:39 step:30469 [D loss: 0.411940, acc.: 84.38%] [G loss: 4.311675]\n",
      "epoch:39 step:30470 [D loss: 0.563770, acc.: 76.56%] [G loss: 6.367777]\n",
      "epoch:39 step:30471 [D loss: 0.690942, acc.: 77.34%] [G loss: 3.847502]\n",
      "epoch:39 step:30472 [D loss: 0.598818, acc.: 78.12%] [G loss: 3.543906]\n",
      "epoch:39 step:30473 [D loss: 0.658409, acc.: 78.12%] [G loss: 6.861649]\n",
      "epoch:39 step:30474 [D loss: 0.469729, acc.: 75.78%] [G loss: 5.250242]\n",
      "epoch:39 step:30475 [D loss: 0.238526, acc.: 90.62%] [G loss: 4.469469]\n",
      "epoch:39 step:30476 [D loss: 0.246412, acc.: 90.62%] [G loss: 3.959697]\n",
      "epoch:39 step:30477 [D loss: 0.412829, acc.: 80.47%] [G loss: 4.375699]\n",
      "epoch:39 step:30478 [D loss: 0.295726, acc.: 84.38%] [G loss: 4.228676]\n",
      "epoch:39 step:30479 [D loss: 0.316397, acc.: 89.06%] [G loss: 3.222048]\n",
      "epoch:39 step:30480 [D loss: 0.356977, acc.: 80.47%] [G loss: 3.496694]\n",
      "epoch:39 step:30481 [D loss: 0.355813, acc.: 83.59%] [G loss: 3.112487]\n",
      "epoch:39 step:30482 [D loss: 0.236648, acc.: 91.41%] [G loss: 4.283188]\n",
      "epoch:39 step:30483 [D loss: 0.355920, acc.: 81.25%] [G loss: 3.991085]\n",
      "epoch:39 step:30484 [D loss: 0.263984, acc.: 88.28%] [G loss: 3.036642]\n",
      "epoch:39 step:30485 [D loss: 0.171835, acc.: 95.31%] [G loss: 4.121966]\n",
      "epoch:39 step:30486 [D loss: 0.220134, acc.: 92.19%] [G loss: 2.684753]\n",
      "epoch:39 step:30487 [D loss: 0.334059, acc.: 84.38%] [G loss: 2.483142]\n",
      "epoch:39 step:30488 [D loss: 0.349356, acc.: 83.59%] [G loss: 2.237735]\n",
      "epoch:39 step:30489 [D loss: 0.322401, acc.: 85.16%] [G loss: 2.597285]\n",
      "epoch:39 step:30490 [D loss: 0.397459, acc.: 82.03%] [G loss: 2.378659]\n",
      "epoch:39 step:30491 [D loss: 0.362309, acc.: 85.16%] [G loss: 3.292644]\n",
      "epoch:39 step:30492 [D loss: 0.283697, acc.: 89.06%] [G loss: 2.315824]\n",
      "epoch:39 step:30493 [D loss: 0.431031, acc.: 78.91%] [G loss: 3.604677]\n",
      "epoch:39 step:30494 [D loss: 0.480819, acc.: 77.34%] [G loss: 3.889229]\n",
      "epoch:39 step:30495 [D loss: 0.437626, acc.: 78.91%] [G loss: 3.194057]\n",
      "epoch:39 step:30496 [D loss: 0.319359, acc.: 85.16%] [G loss: 2.547812]\n",
      "epoch:39 step:30497 [D loss: 0.407016, acc.: 79.69%] [G loss: 3.432703]\n",
      "epoch:39 step:30498 [D loss: 0.429887, acc.: 80.47%] [G loss: 3.084994]\n",
      "epoch:39 step:30499 [D loss: 0.279507, acc.: 85.94%] [G loss: 2.834546]\n",
      "epoch:39 step:30500 [D loss: 0.352873, acc.: 82.03%] [G loss: 2.849091]\n",
      "epoch:39 step:30501 [D loss: 0.360702, acc.: 89.06%] [G loss: 3.670746]\n",
      "epoch:39 step:30502 [D loss: 0.323327, acc.: 85.94%] [G loss: 3.597960]\n",
      "epoch:39 step:30503 [D loss: 0.377458, acc.: 82.03%] [G loss: 3.254744]\n",
      "epoch:39 step:30504 [D loss: 0.283956, acc.: 89.06%] [G loss: 2.563061]\n",
      "epoch:39 step:30505 [D loss: 0.324330, acc.: 84.38%] [G loss: 2.593451]\n",
      "epoch:39 step:30506 [D loss: 0.276243, acc.: 88.28%] [G loss: 2.815050]\n",
      "epoch:39 step:30507 [D loss: 0.273338, acc.: 87.50%] [G loss: 3.264096]\n",
      "epoch:39 step:30508 [D loss: 0.250065, acc.: 86.72%] [G loss: 3.080493]\n",
      "epoch:39 step:30509 [D loss: 0.445035, acc.: 78.12%] [G loss: 3.196544]\n",
      "epoch:39 step:30510 [D loss: 0.335501, acc.: 82.03%] [G loss: 3.793702]\n",
      "epoch:39 step:30511 [D loss: 0.439682, acc.: 81.25%] [G loss: 2.451710]\n",
      "epoch:39 step:30512 [D loss: 0.299816, acc.: 86.72%] [G loss: 2.922255]\n",
      "epoch:39 step:30513 [D loss: 0.273167, acc.: 88.28%] [G loss: 2.920289]\n",
      "epoch:39 step:30514 [D loss: 0.230095, acc.: 91.41%] [G loss: 3.237822]\n",
      "epoch:39 step:30515 [D loss: 0.331865, acc.: 82.81%] [G loss: 5.063601]\n",
      "epoch:39 step:30516 [D loss: 0.217252, acc.: 91.41%] [G loss: 3.819950]\n",
      "epoch:39 step:30517 [D loss: 0.244492, acc.: 90.62%] [G loss: 7.559381]\n",
      "epoch:39 step:30518 [D loss: 0.154273, acc.: 91.41%] [G loss: 6.805490]\n",
      "epoch:39 step:30519 [D loss: 0.254496, acc.: 89.06%] [G loss: 4.923135]\n",
      "epoch:39 step:30520 [D loss: 0.280523, acc.: 86.72%] [G loss: 4.633554]\n",
      "epoch:39 step:30521 [D loss: 0.192870, acc.: 91.41%] [G loss: 4.107735]\n",
      "epoch:39 step:30522 [D loss: 0.249293, acc.: 89.84%] [G loss: 2.747385]\n",
      "epoch:39 step:30523 [D loss: 0.340726, acc.: 85.16%] [G loss: 3.833929]\n",
      "epoch:39 step:30524 [D loss: 0.379584, acc.: 82.03%] [G loss: 3.026631]\n",
      "epoch:39 step:30525 [D loss: 0.380459, acc.: 82.81%] [G loss: 2.716966]\n",
      "epoch:39 step:30526 [D loss: 0.243671, acc.: 89.06%] [G loss: 2.846957]\n",
      "epoch:39 step:30527 [D loss: 0.332587, acc.: 85.94%] [G loss: 3.453573]\n",
      "epoch:39 step:30528 [D loss: 0.349090, acc.: 80.47%] [G loss: 2.516085]\n",
      "epoch:39 step:30529 [D loss: 0.315786, acc.: 88.28%] [G loss: 3.656956]\n",
      "epoch:39 step:30530 [D loss: 0.353836, acc.: 84.38%] [G loss: 2.680670]\n",
      "epoch:39 step:30531 [D loss: 0.329027, acc.: 83.59%] [G loss: 4.470071]\n",
      "epoch:39 step:30532 [D loss: 0.330277, acc.: 84.38%] [G loss: 5.039551]\n",
      "epoch:39 step:30533 [D loss: 0.308550, acc.: 84.38%] [G loss: 2.942737]\n",
      "epoch:39 step:30534 [D loss: 0.257889, acc.: 88.28%] [G loss: 3.328449]\n",
      "epoch:39 step:30535 [D loss: 0.355302, acc.: 82.81%] [G loss: 2.826827]\n",
      "epoch:39 step:30536 [D loss: 0.336464, acc.: 87.50%] [G loss: 3.780262]\n",
      "epoch:39 step:30537 [D loss: 0.227617, acc.: 92.97%] [G loss: 4.657284]\n",
      "epoch:39 step:30538 [D loss: 0.302502, acc.: 86.72%] [G loss: 4.219727]\n",
      "epoch:39 step:30539 [D loss: 0.291690, acc.: 86.72%] [G loss: 3.532513]\n",
      "epoch:39 step:30540 [D loss: 0.297460, acc.: 85.94%] [G loss: 4.014236]\n",
      "epoch:39 step:30541 [D loss: 0.287031, acc.: 89.06%] [G loss: 4.378461]\n",
      "epoch:39 step:30542 [D loss: 0.329380, acc.: 83.59%] [G loss: 3.361847]\n",
      "epoch:39 step:30543 [D loss: 0.244220, acc.: 89.84%] [G loss: 3.529554]\n",
      "epoch:39 step:30544 [D loss: 0.406218, acc.: 85.94%] [G loss: 2.361883]\n",
      "epoch:39 step:30545 [D loss: 0.309176, acc.: 83.59%] [G loss: 3.111419]\n",
      "epoch:39 step:30546 [D loss: 0.297592, acc.: 86.72%] [G loss: 3.319344]\n",
      "epoch:39 step:30547 [D loss: 0.379451, acc.: 83.59%] [G loss: 3.013925]\n",
      "epoch:39 step:30548 [D loss: 0.380883, acc.: 83.59%] [G loss: 3.502602]\n",
      "epoch:39 step:30549 [D loss: 0.424735, acc.: 78.91%] [G loss: 4.156789]\n",
      "epoch:39 step:30550 [D loss: 0.319204, acc.: 83.59%] [G loss: 4.343711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30551 [D loss: 0.332262, acc.: 85.94%] [G loss: 4.005558]\n",
      "epoch:39 step:30552 [D loss: 0.405479, acc.: 78.91%] [G loss: 2.732533]\n",
      "epoch:39 step:30553 [D loss: 0.311470, acc.: 89.84%] [G loss: 3.795060]\n",
      "epoch:39 step:30554 [D loss: 0.301762, acc.: 89.06%] [G loss: 3.294008]\n",
      "epoch:39 step:30555 [D loss: 0.308740, acc.: 88.28%] [G loss: 2.295222]\n",
      "epoch:39 step:30556 [D loss: 0.310453, acc.: 85.94%] [G loss: 4.028851]\n",
      "epoch:39 step:30557 [D loss: 0.337887, acc.: 85.16%] [G loss: 3.772709]\n",
      "epoch:39 step:30558 [D loss: 0.339264, acc.: 82.03%] [G loss: 4.584435]\n",
      "epoch:39 step:30559 [D loss: 0.385274, acc.: 79.69%] [G loss: 3.900455]\n",
      "epoch:39 step:30560 [D loss: 0.336048, acc.: 85.16%] [G loss: 4.031675]\n",
      "epoch:39 step:30561 [D loss: 0.323262, acc.: 82.03%] [G loss: 2.873323]\n",
      "epoch:39 step:30562 [D loss: 0.280207, acc.: 87.50%] [G loss: 4.670568]\n",
      "epoch:39 step:30563 [D loss: 0.351439, acc.: 85.16%] [G loss: 3.396112]\n",
      "epoch:39 step:30564 [D loss: 0.274116, acc.: 82.81%] [G loss: 2.720259]\n",
      "epoch:39 step:30565 [D loss: 0.279182, acc.: 87.50%] [G loss: 2.277410]\n",
      "epoch:39 step:30566 [D loss: 0.242359, acc.: 91.41%] [G loss: 3.060532]\n",
      "epoch:39 step:30567 [D loss: 0.356896, acc.: 83.59%] [G loss: 3.071682]\n",
      "epoch:39 step:30568 [D loss: 0.276752, acc.: 86.72%] [G loss: 4.856175]\n",
      "epoch:39 step:30569 [D loss: 0.330194, acc.: 87.50%] [G loss: 3.380426]\n",
      "epoch:39 step:30570 [D loss: 0.364418, acc.: 86.72%] [G loss: 3.713377]\n",
      "epoch:39 step:30571 [D loss: 0.218696, acc.: 90.62%] [G loss: 3.815720]\n",
      "epoch:39 step:30572 [D loss: 0.256697, acc.: 89.06%] [G loss: 6.389562]\n",
      "epoch:39 step:30573 [D loss: 0.335687, acc.: 85.16%] [G loss: 5.436600]\n",
      "epoch:39 step:30574 [D loss: 0.323238, acc.: 86.72%] [G loss: 6.699154]\n",
      "epoch:39 step:30575 [D loss: 0.386661, acc.: 79.69%] [G loss: 3.251031]\n",
      "epoch:39 step:30576 [D loss: 0.182656, acc.: 93.75%] [G loss: 5.172013]\n",
      "epoch:39 step:30577 [D loss: 0.456149, acc.: 80.47%] [G loss: 2.147340]\n",
      "epoch:39 step:30578 [D loss: 0.254743, acc.: 88.28%] [G loss: 3.620754]\n",
      "epoch:39 step:30579 [D loss: 0.399390, acc.: 80.47%] [G loss: 3.530001]\n",
      "epoch:39 step:30580 [D loss: 0.312024, acc.: 86.72%] [G loss: 4.369433]\n",
      "epoch:39 step:30581 [D loss: 0.377593, acc.: 80.47%] [G loss: 2.683688]\n",
      "epoch:39 step:30582 [D loss: 0.365821, acc.: 85.16%] [G loss: 3.348366]\n",
      "epoch:39 step:30583 [D loss: 0.270042, acc.: 87.50%] [G loss: 3.140766]\n",
      "epoch:39 step:30584 [D loss: 0.304509, acc.: 89.06%] [G loss: 3.163713]\n",
      "epoch:39 step:30585 [D loss: 0.213572, acc.: 89.84%] [G loss: 3.154327]\n",
      "epoch:39 step:30586 [D loss: 0.262021, acc.: 88.28%] [G loss: 2.797318]\n",
      "epoch:39 step:30587 [D loss: 0.344560, acc.: 85.16%] [G loss: 3.109224]\n",
      "epoch:39 step:30588 [D loss: 0.282279, acc.: 90.62%] [G loss: 2.634254]\n",
      "epoch:39 step:30589 [D loss: 0.322831, acc.: 85.16%] [G loss: 2.733132]\n",
      "epoch:39 step:30590 [D loss: 0.279326, acc.: 87.50%] [G loss: 2.923452]\n",
      "epoch:39 step:30591 [D loss: 0.322103, acc.: 85.94%] [G loss: 2.992000]\n",
      "epoch:39 step:30592 [D loss: 0.347946, acc.: 87.50%] [G loss: 3.019672]\n",
      "epoch:39 step:30593 [D loss: 0.347533, acc.: 84.38%] [G loss: 2.936517]\n",
      "epoch:39 step:30594 [D loss: 0.429831, acc.: 79.69%] [G loss: 2.489644]\n",
      "epoch:39 step:30595 [D loss: 0.271771, acc.: 89.06%] [G loss: 2.428212]\n",
      "epoch:39 step:30596 [D loss: 0.241981, acc.: 89.84%] [G loss: 2.493926]\n",
      "epoch:39 step:30597 [D loss: 0.343586, acc.: 83.59%] [G loss: 2.621023]\n",
      "epoch:39 step:30598 [D loss: 0.285854, acc.: 89.06%] [G loss: 3.058789]\n",
      "epoch:39 step:30599 [D loss: 0.355455, acc.: 84.38%] [G loss: 2.459427]\n",
      "epoch:39 step:30600 [D loss: 0.337420, acc.: 85.16%] [G loss: 2.946134]\n",
      "epoch:39 step:30601 [D loss: 0.356391, acc.: 84.38%] [G loss: 2.730434]\n",
      "epoch:39 step:30602 [D loss: 0.258217, acc.: 89.84%] [G loss: 3.306017]\n",
      "epoch:39 step:30603 [D loss: 0.375991, acc.: 82.81%] [G loss: 3.334680]\n",
      "epoch:39 step:30604 [D loss: 0.397283, acc.: 80.47%] [G loss: 2.618091]\n",
      "epoch:39 step:30605 [D loss: 0.342607, acc.: 83.59%] [G loss: 4.347610]\n",
      "epoch:39 step:30606 [D loss: 0.307251, acc.: 87.50%] [G loss: 4.453324]\n",
      "epoch:39 step:30607 [D loss: 0.272532, acc.: 89.06%] [G loss: 5.557020]\n",
      "epoch:39 step:30608 [D loss: 0.371455, acc.: 82.03%] [G loss: 3.436407]\n",
      "epoch:39 step:30609 [D loss: 0.329379, acc.: 85.16%] [G loss: 5.320186]\n",
      "epoch:39 step:30610 [D loss: 0.328285, acc.: 86.72%] [G loss: 4.910266]\n",
      "epoch:39 step:30611 [D loss: 0.352394, acc.: 83.59%] [G loss: 2.947678]\n",
      "epoch:39 step:30612 [D loss: 0.285862, acc.: 86.72%] [G loss: 4.256533]\n",
      "epoch:39 step:30613 [D loss: 0.212458, acc.: 90.62%] [G loss: 4.627872]\n",
      "epoch:39 step:30614 [D loss: 0.296797, acc.: 88.28%] [G loss: 3.673558]\n",
      "epoch:39 step:30615 [D loss: 0.308131, acc.: 86.72%] [G loss: 3.301653]\n",
      "epoch:39 step:30616 [D loss: 0.387511, acc.: 83.59%] [G loss: 3.633407]\n",
      "epoch:39 step:30617 [D loss: 0.341244, acc.: 83.59%] [G loss: 4.379576]\n",
      "epoch:39 step:30618 [D loss: 0.242858, acc.: 90.62%] [G loss: 5.560264]\n",
      "epoch:39 step:30619 [D loss: 0.342012, acc.: 85.16%] [G loss: 6.222860]\n",
      "epoch:39 step:30620 [D loss: 0.239967, acc.: 91.41%] [G loss: 5.589246]\n",
      "epoch:39 step:30621 [D loss: 0.187547, acc.: 92.19%] [G loss: 6.121419]\n",
      "epoch:39 step:30622 [D loss: 0.297690, acc.: 87.50%] [G loss: 6.398271]\n",
      "epoch:39 step:30623 [D loss: 0.290435, acc.: 85.16%] [G loss: 4.544738]\n",
      "epoch:39 step:30624 [D loss: 0.271040, acc.: 87.50%] [G loss: 3.936604]\n",
      "epoch:39 step:30625 [D loss: 0.314502, acc.: 85.94%] [G loss: 3.882741]\n",
      "epoch:39 step:30626 [D loss: 0.272598, acc.: 89.84%] [G loss: 3.597821]\n",
      "epoch:39 step:30627 [D loss: 0.313915, acc.: 85.16%] [G loss: 3.152713]\n",
      "epoch:39 step:30628 [D loss: 0.257363, acc.: 92.19%] [G loss: 2.858067]\n",
      "epoch:39 step:30629 [D loss: 0.244812, acc.: 90.62%] [G loss: 3.424302]\n",
      "epoch:39 step:30630 [D loss: 0.245148, acc.: 91.41%] [G loss: 2.747030]\n",
      "epoch:39 step:30631 [D loss: 0.313564, acc.: 86.72%] [G loss: 2.768305]\n",
      "epoch:39 step:30632 [D loss: 0.353016, acc.: 86.72%] [G loss: 3.725284]\n",
      "epoch:39 step:30633 [D loss: 0.367672, acc.: 81.25%] [G loss: 7.404227]\n",
      "epoch:39 step:30634 [D loss: 0.670677, acc.: 73.44%] [G loss: 6.138363]\n",
      "epoch:39 step:30635 [D loss: 1.605955, acc.: 61.72%] [G loss: 9.785668]\n",
      "epoch:39 step:30636 [D loss: 2.201529, acc.: 46.88%] [G loss: 3.650315]\n",
      "epoch:39 step:30637 [D loss: 0.787073, acc.: 75.78%] [G loss: 3.756098]\n",
      "epoch:39 step:30638 [D loss: 0.727395, acc.: 76.56%] [G loss: 3.201844]\n",
      "epoch:39 step:30639 [D loss: 0.554642, acc.: 82.03%] [G loss: 3.440268]\n",
      "epoch:39 step:30640 [D loss: 0.358706, acc.: 83.59%] [G loss: 3.905677]\n",
      "epoch:39 step:30641 [D loss: 0.451641, acc.: 78.91%] [G loss: 2.722180]\n",
      "epoch:39 step:30642 [D loss: 0.275298, acc.: 86.72%] [G loss: 4.333910]\n",
      "epoch:39 step:30643 [D loss: 0.460402, acc.: 78.12%] [G loss: 3.961528]\n",
      "epoch:39 step:30644 [D loss: 0.356608, acc.: 88.28%] [G loss: 3.432980]\n",
      "epoch:39 step:30645 [D loss: 0.253547, acc.: 89.84%] [G loss: 3.110953]\n",
      "epoch:39 step:30646 [D loss: 0.236030, acc.: 93.75%] [G loss: 3.039991]\n",
      "epoch:39 step:30647 [D loss: 0.411565, acc.: 83.59%] [G loss: 2.510002]\n",
      "epoch:39 step:30648 [D loss: 0.347247, acc.: 85.16%] [G loss: 3.146397]\n",
      "epoch:39 step:30649 [D loss: 0.403992, acc.: 82.81%] [G loss: 3.003321]\n",
      "epoch:39 step:30650 [D loss: 0.292538, acc.: 86.72%] [G loss: 2.968337]\n",
      "epoch:39 step:30651 [D loss: 0.292694, acc.: 87.50%] [G loss: 2.507764]\n",
      "epoch:39 step:30652 [D loss: 0.414483, acc.: 81.25%] [G loss: 2.491291]\n",
      "epoch:39 step:30653 [D loss: 0.282526, acc.: 91.41%] [G loss: 3.215261]\n",
      "epoch:39 step:30654 [D loss: 0.286747, acc.: 88.28%] [G loss: 2.412936]\n",
      "epoch:39 step:30655 [D loss: 0.284921, acc.: 87.50%] [G loss: 2.889553]\n",
      "epoch:39 step:30656 [D loss: 0.411313, acc.: 82.03%] [G loss: 3.270837]\n",
      "epoch:39 step:30657 [D loss: 0.372832, acc.: 83.59%] [G loss: 2.402621]\n",
      "epoch:39 step:30658 [D loss: 0.234291, acc.: 89.84%] [G loss: 3.057377]\n",
      "epoch:39 step:30659 [D loss: 0.286326, acc.: 88.28%] [G loss: 3.013630]\n",
      "epoch:39 step:30660 [D loss: 0.282753, acc.: 87.50%] [G loss: 2.692321]\n",
      "epoch:39 step:30661 [D loss: 0.236019, acc.: 88.28%] [G loss: 2.452164]\n",
      "epoch:39 step:30662 [D loss: 0.237526, acc.: 87.50%] [G loss: 2.551152]\n",
      "epoch:39 step:30663 [D loss: 0.308413, acc.: 84.38%] [G loss: 2.251777]\n",
      "epoch:39 step:30664 [D loss: 0.288065, acc.: 85.94%] [G loss: 2.915301]\n",
      "epoch:39 step:30665 [D loss: 0.320484, acc.: 86.72%] [G loss: 2.752267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30666 [D loss: 0.290864, acc.: 93.75%] [G loss: 2.173395]\n",
      "epoch:39 step:30667 [D loss: 0.352977, acc.: 84.38%] [G loss: 2.425907]\n",
      "epoch:39 step:30668 [D loss: 0.318908, acc.: 88.28%] [G loss: 2.816076]\n",
      "epoch:39 step:30669 [D loss: 0.319416, acc.: 87.50%] [G loss: 2.843907]\n",
      "epoch:39 step:30670 [D loss: 0.350662, acc.: 83.59%] [G loss: 2.469224]\n",
      "epoch:39 step:30671 [D loss: 0.317502, acc.: 85.94%] [G loss: 2.457481]\n",
      "epoch:39 step:30672 [D loss: 0.274901, acc.: 86.72%] [G loss: 2.220963]\n",
      "epoch:39 step:30673 [D loss: 0.403856, acc.: 83.59%] [G loss: 2.533388]\n",
      "epoch:39 step:30674 [D loss: 0.296145, acc.: 87.50%] [G loss: 2.518433]\n",
      "epoch:39 step:30675 [D loss: 0.313219, acc.: 83.59%] [G loss: 2.650535]\n",
      "epoch:39 step:30676 [D loss: 0.307834, acc.: 89.06%] [G loss: 2.321795]\n",
      "epoch:39 step:30677 [D loss: 0.263149, acc.: 88.28%] [G loss: 2.822983]\n",
      "epoch:39 step:30678 [D loss: 0.283202, acc.: 89.06%] [G loss: 2.556274]\n",
      "epoch:39 step:30679 [D loss: 0.316071, acc.: 85.94%] [G loss: 2.498458]\n",
      "epoch:39 step:30680 [D loss: 0.241787, acc.: 92.97%] [G loss: 2.418696]\n",
      "epoch:39 step:30681 [D loss: 0.293259, acc.: 87.50%] [G loss: 2.781505]\n",
      "epoch:39 step:30682 [D loss: 0.246353, acc.: 90.62%] [G loss: 2.657324]\n",
      "epoch:39 step:30683 [D loss: 0.356989, acc.: 85.16%] [G loss: 2.895362]\n",
      "epoch:39 step:30684 [D loss: 0.282485, acc.: 89.06%] [G loss: 2.938892]\n",
      "epoch:39 step:30685 [D loss: 0.328075, acc.: 87.50%] [G loss: 2.805915]\n",
      "epoch:39 step:30686 [D loss: 0.355567, acc.: 82.03%] [G loss: 2.619461]\n",
      "epoch:39 step:30687 [D loss: 0.386938, acc.: 81.25%] [G loss: 3.262354]\n",
      "epoch:39 step:30688 [D loss: 0.349466, acc.: 83.59%] [G loss: 2.842999]\n",
      "epoch:39 step:30689 [D loss: 0.261097, acc.: 88.28%] [G loss: 2.699308]\n",
      "epoch:39 step:30690 [D loss: 0.191131, acc.: 92.97%] [G loss: 3.378580]\n",
      "epoch:39 step:30691 [D loss: 0.261903, acc.: 90.62%] [G loss: 2.611263]\n",
      "epoch:39 step:30692 [D loss: 0.316298, acc.: 86.72%] [G loss: 2.279051]\n",
      "epoch:39 step:30693 [D loss: 0.276961, acc.: 90.62%] [G loss: 3.517133]\n",
      "epoch:39 step:30694 [D loss: 0.323225, acc.: 85.94%] [G loss: 2.529199]\n",
      "epoch:39 step:30695 [D loss: 0.267625, acc.: 87.50%] [G loss: 2.711803]\n",
      "epoch:39 step:30696 [D loss: 0.237282, acc.: 89.84%] [G loss: 2.815016]\n",
      "epoch:39 step:30697 [D loss: 0.363338, acc.: 82.81%] [G loss: 3.269734]\n",
      "epoch:39 step:30698 [D loss: 0.255259, acc.: 90.62%] [G loss: 3.060087]\n",
      "epoch:39 step:30699 [D loss: 0.492493, acc.: 72.66%] [G loss: 3.261872]\n",
      "epoch:39 step:30700 [D loss: 0.291690, acc.: 88.28%] [G loss: 2.739342]\n",
      "epoch:39 step:30701 [D loss: 0.267108, acc.: 91.41%] [G loss: 2.709905]\n",
      "epoch:39 step:30702 [D loss: 0.321480, acc.: 85.94%] [G loss: 2.867382]\n",
      "epoch:39 step:30703 [D loss: 0.273034, acc.: 88.28%] [G loss: 2.727302]\n",
      "epoch:39 step:30704 [D loss: 0.290864, acc.: 88.28%] [G loss: 2.735351]\n",
      "epoch:39 step:30705 [D loss: 0.337920, acc.: 82.81%] [G loss: 2.718541]\n",
      "epoch:39 step:30706 [D loss: 0.316568, acc.: 85.16%] [G loss: 2.936430]\n",
      "epoch:39 step:30707 [D loss: 0.307250, acc.: 84.38%] [G loss: 2.457333]\n",
      "epoch:39 step:30708 [D loss: 0.417340, acc.: 76.56%] [G loss: 2.761165]\n",
      "epoch:39 step:30709 [D loss: 0.407895, acc.: 80.47%] [G loss: 2.275375]\n",
      "epoch:39 step:30710 [D loss: 0.247829, acc.: 90.62%] [G loss: 2.286381]\n",
      "epoch:39 step:30711 [D loss: 0.421011, acc.: 78.91%] [G loss: 3.654291]\n",
      "epoch:39 step:30712 [D loss: 0.336182, acc.: 85.16%] [G loss: 2.669625]\n",
      "epoch:39 step:30713 [D loss: 0.298229, acc.: 86.72%] [G loss: 3.061918]\n",
      "epoch:39 step:30714 [D loss: 0.311338, acc.: 86.72%] [G loss: 3.764873]\n",
      "epoch:39 step:30715 [D loss: 0.214001, acc.: 95.31%] [G loss: 3.094197]\n",
      "epoch:39 step:30716 [D loss: 0.208519, acc.: 91.41%] [G loss: 3.532505]\n",
      "epoch:39 step:30717 [D loss: 0.226259, acc.: 90.62%] [G loss: 3.273136]\n",
      "epoch:39 step:30718 [D loss: 0.259395, acc.: 91.41%] [G loss: 2.865067]\n",
      "epoch:39 step:30719 [D loss: 0.219432, acc.: 92.97%] [G loss: 3.212907]\n",
      "epoch:39 step:30720 [D loss: 0.391500, acc.: 81.25%] [G loss: 2.833630]\n",
      "epoch:39 step:30721 [D loss: 0.286722, acc.: 88.28%] [G loss: 3.321773]\n",
      "epoch:39 step:30722 [D loss: 0.216505, acc.: 91.41%] [G loss: 2.943706]\n",
      "epoch:39 step:30723 [D loss: 0.277912, acc.: 88.28%] [G loss: 3.796853]\n",
      "epoch:39 step:30724 [D loss: 0.422346, acc.: 78.12%] [G loss: 2.490676]\n",
      "epoch:39 step:30725 [D loss: 0.315594, acc.: 86.72%] [G loss: 3.533742]\n",
      "epoch:39 step:30726 [D loss: 0.383235, acc.: 81.25%] [G loss: 2.340981]\n",
      "epoch:39 step:30727 [D loss: 0.315158, acc.: 83.59%] [G loss: 3.309300]\n",
      "epoch:39 step:30728 [D loss: 0.310319, acc.: 84.38%] [G loss: 3.483343]\n",
      "epoch:39 step:30729 [D loss: 0.424056, acc.: 83.59%] [G loss: 3.118882]\n",
      "epoch:39 step:30730 [D loss: 0.189871, acc.: 92.19%] [G loss: 4.022130]\n",
      "epoch:39 step:30731 [D loss: 0.241413, acc.: 89.06%] [G loss: 4.334405]\n",
      "epoch:39 step:30732 [D loss: 0.238632, acc.: 88.28%] [G loss: 3.012060]\n",
      "epoch:39 step:30733 [D loss: 0.283847, acc.: 83.59%] [G loss: 4.183513]\n",
      "epoch:39 step:30734 [D loss: 0.562035, acc.: 70.31%] [G loss: 2.475403]\n",
      "epoch:39 step:30735 [D loss: 0.454526, acc.: 76.56%] [G loss: 3.934291]\n",
      "epoch:39 step:30736 [D loss: 0.283343, acc.: 87.50%] [G loss: 2.867613]\n",
      "epoch:39 step:30737 [D loss: 0.221816, acc.: 89.84%] [G loss: 3.308756]\n",
      "epoch:39 step:30738 [D loss: 0.268048, acc.: 89.06%] [G loss: 2.460704]\n",
      "epoch:39 step:30739 [D loss: 0.188230, acc.: 92.19%] [G loss: 2.727312]\n",
      "epoch:39 step:30740 [D loss: 0.304429, acc.: 87.50%] [G loss: 4.323531]\n",
      "epoch:39 step:30741 [D loss: 0.352015, acc.: 84.38%] [G loss: 3.143517]\n",
      "epoch:39 step:30742 [D loss: 0.278076, acc.: 86.72%] [G loss: 3.108224]\n",
      "epoch:39 step:30743 [D loss: 0.277502, acc.: 85.94%] [G loss: 4.026144]\n",
      "epoch:39 step:30744 [D loss: 0.365618, acc.: 83.59%] [G loss: 2.829044]\n",
      "epoch:39 step:30745 [D loss: 0.319032, acc.: 85.16%] [G loss: 3.202989]\n",
      "epoch:39 step:30746 [D loss: 0.219018, acc.: 90.62%] [G loss: 4.364711]\n",
      "epoch:39 step:30747 [D loss: 0.258427, acc.: 86.72%] [G loss: 3.652261]\n",
      "epoch:39 step:30748 [D loss: 0.239294, acc.: 88.28%] [G loss: 3.202636]\n",
      "epoch:39 step:30749 [D loss: 0.274482, acc.: 87.50%] [G loss: 3.647149]\n",
      "epoch:39 step:30750 [D loss: 0.258495, acc.: 90.62%] [G loss: 3.449011]\n",
      "epoch:39 step:30751 [D loss: 0.284733, acc.: 85.94%] [G loss: 4.669318]\n",
      "epoch:39 step:30752 [D loss: 0.313488, acc.: 85.16%] [G loss: 3.990130]\n",
      "epoch:39 step:30753 [D loss: 0.223744, acc.: 89.84%] [G loss: 2.454416]\n",
      "epoch:39 step:30754 [D loss: 0.361828, acc.: 84.38%] [G loss: 3.270867]\n",
      "epoch:39 step:30755 [D loss: 0.330445, acc.: 83.59%] [G loss: 2.856751]\n",
      "epoch:39 step:30756 [D loss: 0.303280, acc.: 85.16%] [G loss: 3.642899]\n",
      "epoch:39 step:30757 [D loss: 0.384690, acc.: 84.38%] [G loss: 2.579009]\n",
      "epoch:39 step:30758 [D loss: 0.346911, acc.: 81.25%] [G loss: 2.393691]\n",
      "epoch:39 step:30759 [D loss: 0.307100, acc.: 88.28%] [G loss: 3.047079]\n",
      "epoch:39 step:30760 [D loss: 0.344104, acc.: 81.25%] [G loss: 3.027563]\n",
      "epoch:39 step:30761 [D loss: 0.390244, acc.: 82.03%] [G loss: 2.783790]\n",
      "epoch:39 step:30762 [D loss: 0.201810, acc.: 92.19%] [G loss: 4.176753]\n",
      "epoch:39 step:30763 [D loss: 0.260202, acc.: 92.97%] [G loss: 2.676059]\n",
      "epoch:39 step:30764 [D loss: 0.397505, acc.: 77.34%] [G loss: 2.069142]\n",
      "epoch:39 step:30765 [D loss: 0.257939, acc.: 91.41%] [G loss: 3.785159]\n",
      "epoch:39 step:30766 [D loss: 0.416214, acc.: 81.25%] [G loss: 3.016508]\n",
      "epoch:39 step:30767 [D loss: 0.280124, acc.: 88.28%] [G loss: 2.575455]\n",
      "epoch:39 step:30768 [D loss: 0.339031, acc.: 85.16%] [G loss: 2.551698]\n",
      "epoch:39 step:30769 [D loss: 0.203418, acc.: 94.53%] [G loss: 3.008617]\n",
      "epoch:39 step:30770 [D loss: 0.373455, acc.: 81.25%] [G loss: 2.427060]\n",
      "epoch:39 step:30771 [D loss: 0.322061, acc.: 85.16%] [G loss: 2.526363]\n",
      "epoch:39 step:30772 [D loss: 0.410345, acc.: 78.91%] [G loss: 2.905004]\n",
      "epoch:39 step:30773 [D loss: 0.331657, acc.: 82.03%] [G loss: 2.926310]\n",
      "epoch:39 step:30774 [D loss: 0.544621, acc.: 75.00%] [G loss: 4.114283]\n",
      "epoch:39 step:30775 [D loss: 0.511247, acc.: 83.59%] [G loss: 8.767524]\n",
      "epoch:39 step:30776 [D loss: 0.744571, acc.: 74.22%] [G loss: 7.765073]\n",
      "epoch:39 step:30777 [D loss: 1.012168, acc.: 70.31%] [G loss: 5.973960]\n",
      "epoch:39 step:30778 [D loss: 0.429956, acc.: 85.94%] [G loss: 5.624080]\n",
      "epoch:39 step:30779 [D loss: 0.205677, acc.: 90.62%] [G loss: 4.933867]\n",
      "epoch:39 step:30780 [D loss: 0.301451, acc.: 87.50%] [G loss: 5.636827]\n",
      "epoch:39 step:30781 [D loss: 0.212744, acc.: 89.84%] [G loss: 6.095922]\n",
      "epoch:39 step:30782 [D loss: 0.294846, acc.: 85.16%] [G loss: 4.863204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30783 [D loss: 0.330698, acc.: 83.59%] [G loss: 3.819219]\n",
      "epoch:39 step:30784 [D loss: 0.322741, acc.: 85.94%] [G loss: 3.465063]\n",
      "epoch:39 step:30785 [D loss: 0.389309, acc.: 84.38%] [G loss: 2.747771]\n",
      "epoch:39 step:30786 [D loss: 0.324892, acc.: 85.94%] [G loss: 3.266425]\n",
      "epoch:39 step:30787 [D loss: 0.230465, acc.: 87.50%] [G loss: 3.092275]\n",
      "epoch:39 step:30788 [D loss: 0.327416, acc.: 83.59%] [G loss: 2.623535]\n",
      "epoch:39 step:30789 [D loss: 0.289964, acc.: 85.16%] [G loss: 3.802487]\n",
      "epoch:39 step:30790 [D loss: 0.296541, acc.: 83.59%] [G loss: 2.490514]\n",
      "epoch:39 step:30791 [D loss: 0.333098, acc.: 85.94%] [G loss: 2.380513]\n",
      "epoch:39 step:30792 [D loss: 0.356314, acc.: 82.03%] [G loss: 2.950762]\n",
      "epoch:39 step:30793 [D loss: 0.306298, acc.: 88.28%] [G loss: 2.478590]\n",
      "epoch:39 step:30794 [D loss: 0.286266, acc.: 88.28%] [G loss: 3.289830]\n",
      "epoch:39 step:30795 [D loss: 0.221732, acc.: 91.41%] [G loss: 3.344491]\n",
      "epoch:39 step:30796 [D loss: 0.264587, acc.: 86.72%] [G loss: 3.275786]\n",
      "epoch:39 step:30797 [D loss: 0.307252, acc.: 85.16%] [G loss: 2.908705]\n",
      "epoch:39 step:30798 [D loss: 0.220504, acc.: 92.19%] [G loss: 3.542877]\n",
      "epoch:39 step:30799 [D loss: 0.232419, acc.: 92.97%] [G loss: 3.135892]\n",
      "epoch:39 step:30800 [D loss: 0.234397, acc.: 88.28%] [G loss: 3.301415]\n",
      "epoch:39 step:30801 [D loss: 0.242186, acc.: 89.06%] [G loss: 2.851484]\n",
      "epoch:39 step:30802 [D loss: 0.287741, acc.: 88.28%] [G loss: 3.173396]\n",
      "epoch:39 step:30803 [D loss: 0.389740, acc.: 82.03%] [G loss: 3.051686]\n",
      "epoch:39 step:30804 [D loss: 0.244914, acc.: 88.28%] [G loss: 3.150322]\n",
      "epoch:39 step:30805 [D loss: 0.301903, acc.: 83.59%] [G loss: 3.138988]\n",
      "epoch:39 step:30806 [D loss: 0.302137, acc.: 85.16%] [G loss: 2.454861]\n",
      "epoch:39 step:30807 [D loss: 0.363550, acc.: 85.94%] [G loss: 1.923315]\n",
      "epoch:39 step:30808 [D loss: 0.338427, acc.: 88.28%] [G loss: 3.058624]\n",
      "epoch:39 step:30809 [D loss: 0.411446, acc.: 83.59%] [G loss: 2.718893]\n",
      "epoch:39 step:30810 [D loss: 0.266322, acc.: 88.28%] [G loss: 3.257142]\n",
      "epoch:39 step:30811 [D loss: 0.315568, acc.: 86.72%] [G loss: 2.787374]\n",
      "epoch:39 step:30812 [D loss: 0.275896, acc.: 89.06%] [G loss: 3.135546]\n",
      "epoch:39 step:30813 [D loss: 0.335767, acc.: 83.59%] [G loss: 3.399700]\n",
      "epoch:39 step:30814 [D loss: 0.244763, acc.: 90.62%] [G loss: 4.281059]\n",
      "epoch:39 step:30815 [D loss: 0.299400, acc.: 88.28%] [G loss: 2.979598]\n",
      "epoch:39 step:30816 [D loss: 0.293625, acc.: 87.50%] [G loss: 4.085794]\n",
      "epoch:39 step:30817 [D loss: 0.189018, acc.: 93.75%] [G loss: 3.012659]\n",
      "epoch:39 step:30818 [D loss: 0.205470, acc.: 93.75%] [G loss: 2.974371]\n",
      "epoch:39 step:30819 [D loss: 0.311533, acc.: 89.84%] [G loss: 2.560910]\n",
      "epoch:39 step:30820 [D loss: 0.294755, acc.: 85.94%] [G loss: 3.486234]\n",
      "epoch:39 step:30821 [D loss: 0.297685, acc.: 83.59%] [G loss: 3.659509]\n",
      "epoch:39 step:30822 [D loss: 0.238246, acc.: 90.62%] [G loss: 4.473042]\n",
      "epoch:39 step:30823 [D loss: 0.230326, acc.: 87.50%] [G loss: 3.275408]\n",
      "epoch:39 step:30824 [D loss: 0.304644, acc.: 84.38%] [G loss: 3.324088]\n",
      "epoch:39 step:30825 [D loss: 0.326378, acc.: 83.59%] [G loss: 2.827644]\n",
      "epoch:39 step:30826 [D loss: 0.304101, acc.: 85.94%] [G loss: 3.405230]\n",
      "epoch:39 step:30827 [D loss: 0.417464, acc.: 79.69%] [G loss: 2.821305]\n",
      "epoch:39 step:30828 [D loss: 0.260110, acc.: 92.19%] [G loss: 2.403603]\n",
      "epoch:39 step:30829 [D loss: 0.361760, acc.: 82.03%] [G loss: 2.918732]\n",
      "epoch:39 step:30830 [D loss: 0.324907, acc.: 85.94%] [G loss: 2.849288]\n",
      "epoch:39 step:30831 [D loss: 0.250121, acc.: 92.19%] [G loss: 2.267497]\n",
      "epoch:39 step:30832 [D loss: 0.490581, acc.: 76.56%] [G loss: 2.690368]\n",
      "epoch:39 step:30833 [D loss: 0.361098, acc.: 85.94%] [G loss: 2.521574]\n",
      "epoch:39 step:30834 [D loss: 0.299764, acc.: 89.84%] [G loss: 3.408966]\n",
      "epoch:39 step:30835 [D loss: 0.269473, acc.: 89.06%] [G loss: 3.009903]\n",
      "epoch:39 step:30836 [D loss: 0.248556, acc.: 90.62%] [G loss: 2.865162]\n",
      "epoch:39 step:30837 [D loss: 0.344564, acc.: 82.03%] [G loss: 3.179839]\n",
      "epoch:39 step:30838 [D loss: 0.257741, acc.: 87.50%] [G loss: 2.823943]\n",
      "epoch:39 step:30839 [D loss: 0.244442, acc.: 86.72%] [G loss: 3.650942]\n",
      "epoch:39 step:30840 [D loss: 0.313445, acc.: 86.72%] [G loss: 2.999735]\n",
      "epoch:39 step:30841 [D loss: 0.301697, acc.: 85.16%] [G loss: 3.498871]\n",
      "epoch:39 step:30842 [D loss: 0.345501, acc.: 86.72%] [G loss: 4.831223]\n",
      "epoch:39 step:30843 [D loss: 0.255587, acc.: 89.06%] [G loss: 2.937833]\n",
      "epoch:39 step:30844 [D loss: 0.323828, acc.: 86.72%] [G loss: 3.499900]\n",
      "epoch:39 step:30845 [D loss: 0.341980, acc.: 83.59%] [G loss: 3.359701]\n",
      "epoch:39 step:30846 [D loss: 0.293465, acc.: 85.94%] [G loss: 3.161605]\n",
      "epoch:39 step:30847 [D loss: 0.371579, acc.: 86.72%] [G loss: 3.504430]\n",
      "epoch:39 step:30848 [D loss: 0.309947, acc.: 86.72%] [G loss: 2.666698]\n",
      "epoch:39 step:30849 [D loss: 0.235700, acc.: 91.41%] [G loss: 3.126449]\n",
      "epoch:39 step:30850 [D loss: 0.374235, acc.: 81.25%] [G loss: 2.465069]\n",
      "epoch:39 step:30851 [D loss: 0.204103, acc.: 93.75%] [G loss: 2.954713]\n",
      "epoch:39 step:30852 [D loss: 0.289296, acc.: 85.94%] [G loss: 2.756696]\n",
      "epoch:39 step:30853 [D loss: 0.315136, acc.: 85.16%] [G loss: 2.440068]\n",
      "epoch:39 step:30854 [D loss: 0.286158, acc.: 87.50%] [G loss: 2.505658]\n",
      "epoch:39 step:30855 [D loss: 0.309752, acc.: 87.50%] [G loss: 2.412714]\n",
      "epoch:39 step:30856 [D loss: 0.412316, acc.: 77.34%] [G loss: 2.378345]\n",
      "epoch:39 step:30857 [D loss: 0.352471, acc.: 84.38%] [G loss: 3.083188]\n",
      "epoch:39 step:30858 [D loss: 0.384575, acc.: 80.47%] [G loss: 3.179917]\n",
      "epoch:39 step:30859 [D loss: 0.284740, acc.: 85.94%] [G loss: 3.548743]\n",
      "epoch:39 step:30860 [D loss: 0.266702, acc.: 88.28%] [G loss: 4.990367]\n",
      "epoch:39 step:30861 [D loss: 0.366510, acc.: 83.59%] [G loss: 2.846908]\n",
      "epoch:39 step:30862 [D loss: 0.360229, acc.: 85.94%] [G loss: 2.809926]\n",
      "epoch:39 step:30863 [D loss: 0.247225, acc.: 92.97%] [G loss: 2.980004]\n",
      "epoch:39 step:30864 [D loss: 0.301638, acc.: 84.38%] [G loss: 3.251123]\n",
      "epoch:39 step:30865 [D loss: 0.348274, acc.: 83.59%] [G loss: 4.758399]\n",
      "epoch:39 step:30866 [D loss: 0.279996, acc.: 88.28%] [G loss: 3.952550]\n",
      "epoch:39 step:30867 [D loss: 0.323211, acc.: 83.59%] [G loss: 3.327742]\n",
      "epoch:39 step:30868 [D loss: 0.265268, acc.: 92.19%] [G loss: 2.702372]\n",
      "epoch:39 step:30869 [D loss: 0.329345, acc.: 83.59%] [G loss: 2.316136]\n",
      "epoch:39 step:30870 [D loss: 0.322608, acc.: 85.94%] [G loss: 2.980243]\n",
      "epoch:39 step:30871 [D loss: 0.281685, acc.: 86.72%] [G loss: 3.440102]\n",
      "epoch:39 step:30872 [D loss: 0.271742, acc.: 92.19%] [G loss: 2.678236]\n",
      "epoch:39 step:30873 [D loss: 0.398959, acc.: 81.25%] [G loss: 2.187065]\n",
      "epoch:39 step:30874 [D loss: 0.330069, acc.: 80.47%] [G loss: 3.299808]\n",
      "epoch:39 step:30875 [D loss: 0.328785, acc.: 83.59%] [G loss: 3.631083]\n",
      "epoch:39 step:30876 [D loss: 0.386585, acc.: 82.81%] [G loss: 2.679653]\n",
      "epoch:39 step:30877 [D loss: 0.271623, acc.: 87.50%] [G loss: 3.494606]\n",
      "epoch:39 step:30878 [D loss: 0.363641, acc.: 85.16%] [G loss: 2.662251]\n",
      "epoch:39 step:30879 [D loss: 0.312416, acc.: 87.50%] [G loss: 2.882524]\n",
      "epoch:39 step:30880 [D loss: 0.241318, acc.: 90.62%] [G loss: 4.000054]\n",
      "epoch:39 step:30881 [D loss: 0.240990, acc.: 92.19%] [G loss: 3.306065]\n",
      "epoch:39 step:30882 [D loss: 0.357436, acc.: 84.38%] [G loss: 3.991753]\n",
      "epoch:39 step:30883 [D loss: 0.285511, acc.: 86.72%] [G loss: 2.959178]\n",
      "epoch:39 step:30884 [D loss: 0.310595, acc.: 85.16%] [G loss: 3.003067]\n",
      "epoch:39 step:30885 [D loss: 0.342191, acc.: 85.94%] [G loss: 3.247313]\n",
      "epoch:39 step:30886 [D loss: 0.215816, acc.: 91.41%] [G loss: 3.698498]\n",
      "epoch:39 step:30887 [D loss: 0.441241, acc.: 78.91%] [G loss: 2.359640]\n",
      "epoch:39 step:30888 [D loss: 0.307718, acc.: 88.28%] [G loss: 2.911504]\n",
      "epoch:39 step:30889 [D loss: 0.275773, acc.: 86.72%] [G loss: 3.318588]\n",
      "epoch:39 step:30890 [D loss: 0.334716, acc.: 85.94%] [G loss: 3.379752]\n",
      "epoch:39 step:30891 [D loss: 0.343787, acc.: 82.03%] [G loss: 4.935343]\n",
      "epoch:39 step:30892 [D loss: 0.364764, acc.: 82.81%] [G loss: 3.673780]\n",
      "epoch:39 step:30893 [D loss: 0.251997, acc.: 89.06%] [G loss: 4.112833]\n",
      "epoch:39 step:30894 [D loss: 0.357442, acc.: 82.03%] [G loss: 5.377141]\n",
      "epoch:39 step:30895 [D loss: 0.546881, acc.: 69.53%] [G loss: 3.401021]\n",
      "epoch:39 step:30896 [D loss: 0.185563, acc.: 92.19%] [G loss: 3.981352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30897 [D loss: 0.384616, acc.: 81.25%] [G loss: 2.636467]\n",
      "epoch:39 step:30898 [D loss: 0.272609, acc.: 90.62%] [G loss: 3.371834]\n",
      "epoch:39 step:30899 [D loss: 0.308864, acc.: 85.16%] [G loss: 4.733815]\n",
      "epoch:39 step:30900 [D loss: 0.427102, acc.: 82.03%] [G loss: 3.939054]\n",
      "epoch:39 step:30901 [D loss: 0.374667, acc.: 82.03%] [G loss: 3.348164]\n",
      "epoch:39 step:30902 [D loss: 0.284395, acc.: 85.16%] [G loss: 3.054229]\n",
      "epoch:39 step:30903 [D loss: 0.283161, acc.: 87.50%] [G loss: 3.768375]\n",
      "epoch:39 step:30904 [D loss: 0.568613, acc.: 77.34%] [G loss: 3.939770]\n",
      "epoch:39 step:30905 [D loss: 0.250957, acc.: 89.84%] [G loss: 4.096910]\n",
      "epoch:39 step:30906 [D loss: 0.323250, acc.: 84.38%] [G loss: 3.816891]\n",
      "epoch:39 step:30907 [D loss: 0.256194, acc.: 89.84%] [G loss: 4.011647]\n",
      "epoch:39 step:30908 [D loss: 0.289996, acc.: 88.28%] [G loss: 3.745846]\n",
      "epoch:39 step:30909 [D loss: 0.211849, acc.: 92.97%] [G loss: 2.932394]\n",
      "epoch:39 step:30910 [D loss: 0.295273, acc.: 85.16%] [G loss: 3.938485]\n",
      "epoch:39 step:30911 [D loss: 0.369352, acc.: 86.72%] [G loss: 3.548624]\n",
      "epoch:39 step:30912 [D loss: 0.282434, acc.: 87.50%] [G loss: 3.324146]\n",
      "epoch:39 step:30913 [D loss: 0.253603, acc.: 89.06%] [G loss: 3.417421]\n",
      "epoch:39 step:30914 [D loss: 0.294963, acc.: 89.06%] [G loss: 3.276624]\n",
      "epoch:39 step:30915 [D loss: 0.361751, acc.: 81.25%] [G loss: 3.159907]\n",
      "epoch:39 step:30916 [D loss: 0.285315, acc.: 88.28%] [G loss: 3.850115]\n",
      "epoch:39 step:30917 [D loss: 0.288809, acc.: 86.72%] [G loss: 5.289977]\n",
      "epoch:39 step:30918 [D loss: 0.403751, acc.: 84.38%] [G loss: 4.208608]\n",
      "epoch:39 step:30919 [D loss: 0.442370, acc.: 79.69%] [G loss: 3.962533]\n",
      "epoch:39 step:30920 [D loss: 0.265396, acc.: 88.28%] [G loss: 4.849670]\n",
      "epoch:39 step:30921 [D loss: 0.347244, acc.: 81.25%] [G loss: 5.704965]\n",
      "epoch:39 step:30922 [D loss: 0.331020, acc.: 84.38%] [G loss: 3.620562]\n",
      "epoch:39 step:30923 [D loss: 0.287555, acc.: 89.06%] [G loss: 5.444767]\n",
      "epoch:39 step:30924 [D loss: 0.279092, acc.: 89.84%] [G loss: 3.089287]\n",
      "epoch:39 step:30925 [D loss: 0.361920, acc.: 85.16%] [G loss: 4.018717]\n",
      "epoch:39 step:30926 [D loss: 0.297213, acc.: 88.28%] [G loss: 2.958105]\n",
      "epoch:39 step:30927 [D loss: 0.201439, acc.: 92.19%] [G loss: 3.046636]\n",
      "epoch:39 step:30928 [D loss: 0.326546, acc.: 85.16%] [G loss: 2.486349]\n",
      "epoch:39 step:30929 [D loss: 0.310708, acc.: 88.28%] [G loss: 2.997013]\n",
      "epoch:39 step:30930 [D loss: 0.373934, acc.: 82.03%] [G loss: 3.087448]\n",
      "epoch:39 step:30931 [D loss: 0.413645, acc.: 78.12%] [G loss: 2.925110]\n",
      "epoch:39 step:30932 [D loss: 0.253066, acc.: 89.06%] [G loss: 2.593375]\n",
      "epoch:39 step:30933 [D loss: 0.330296, acc.: 84.38%] [G loss: 2.636066]\n",
      "epoch:39 step:30934 [D loss: 0.344307, acc.: 82.03%] [G loss: 3.204173]\n",
      "epoch:39 step:30935 [D loss: 0.392979, acc.: 83.59%] [G loss: 3.289104]\n",
      "epoch:39 step:30936 [D loss: 0.264823, acc.: 90.62%] [G loss: 3.351598]\n",
      "epoch:39 step:30937 [D loss: 0.188407, acc.: 92.97%] [G loss: 4.972255]\n",
      "epoch:39 step:30938 [D loss: 0.292980, acc.: 87.50%] [G loss: 3.429793]\n",
      "epoch:39 step:30939 [D loss: 0.260552, acc.: 89.06%] [G loss: 3.429581]\n",
      "epoch:39 step:30940 [D loss: 0.291427, acc.: 89.06%] [G loss: 4.465196]\n",
      "epoch:39 step:30941 [D loss: 0.348483, acc.: 84.38%] [G loss: 3.832068]\n",
      "epoch:39 step:30942 [D loss: 0.381325, acc.: 77.34%] [G loss: 3.164984]\n",
      "epoch:39 step:30943 [D loss: 0.307634, acc.: 88.28%] [G loss: 3.145551]\n",
      "epoch:39 step:30944 [D loss: 0.344309, acc.: 86.72%] [G loss: 2.613792]\n",
      "epoch:39 step:30945 [D loss: 0.229740, acc.: 88.28%] [G loss: 2.636861]\n",
      "epoch:39 step:30946 [D loss: 0.404280, acc.: 82.81%] [G loss: 3.092065]\n",
      "epoch:39 step:30947 [D loss: 0.302816, acc.: 86.72%] [G loss: 3.171213]\n",
      "epoch:39 step:30948 [D loss: 0.398557, acc.: 79.69%] [G loss: 2.479768]\n",
      "epoch:39 step:30949 [D loss: 0.285783, acc.: 88.28%] [G loss: 2.122705]\n",
      "epoch:39 step:30950 [D loss: 0.337071, acc.: 85.94%] [G loss: 3.115476]\n",
      "epoch:39 step:30951 [D loss: 0.236495, acc.: 89.84%] [G loss: 2.906473]\n",
      "epoch:39 step:30952 [D loss: 0.302629, acc.: 89.06%] [G loss: 3.201279]\n",
      "epoch:39 step:30953 [D loss: 0.341625, acc.: 81.25%] [G loss: 3.528366]\n",
      "epoch:39 step:30954 [D loss: 0.293369, acc.: 86.72%] [G loss: 2.730377]\n",
      "epoch:39 step:30955 [D loss: 0.287830, acc.: 85.94%] [G loss: 2.647820]\n",
      "epoch:39 step:30956 [D loss: 0.313272, acc.: 84.38%] [G loss: 5.714602]\n",
      "epoch:39 step:30957 [D loss: 0.215818, acc.: 91.41%] [G loss: 7.419365]\n",
      "epoch:39 step:30958 [D loss: 0.305969, acc.: 87.50%] [G loss: 2.924004]\n",
      "epoch:39 step:30959 [D loss: 0.300217, acc.: 85.94%] [G loss: 3.060426]\n",
      "epoch:39 step:30960 [D loss: 0.330404, acc.: 87.50%] [G loss: 2.814221]\n",
      "epoch:39 step:30961 [D loss: 0.335673, acc.: 84.38%] [G loss: 2.793643]\n",
      "epoch:39 step:30962 [D loss: 0.303610, acc.: 88.28%] [G loss: 2.751941]\n",
      "epoch:39 step:30963 [D loss: 0.285472, acc.: 85.94%] [G loss: 4.246962]\n",
      "epoch:39 step:30964 [D loss: 0.284320, acc.: 87.50%] [G loss: 3.790821]\n",
      "epoch:39 step:30965 [D loss: 0.237021, acc.: 92.19%] [G loss: 4.326884]\n",
      "epoch:39 step:30966 [D loss: 0.373534, acc.: 83.59%] [G loss: 4.473422]\n",
      "epoch:39 step:30967 [D loss: 0.375700, acc.: 80.47%] [G loss: 4.338229]\n",
      "epoch:39 step:30968 [D loss: 0.232522, acc.: 89.84%] [G loss: 5.713880]\n",
      "epoch:39 step:30969 [D loss: 0.273728, acc.: 86.72%] [G loss: 4.048619]\n",
      "epoch:39 step:30970 [D loss: 0.216045, acc.: 88.28%] [G loss: 3.616838]\n",
      "epoch:39 step:30971 [D loss: 0.222508, acc.: 89.06%] [G loss: 4.077199]\n",
      "epoch:39 step:30972 [D loss: 0.320963, acc.: 86.72%] [G loss: 3.644722]\n",
      "epoch:39 step:30973 [D loss: 0.416731, acc.: 78.12%] [G loss: 3.682433]\n",
      "epoch:39 step:30974 [D loss: 0.397803, acc.: 81.25%] [G loss: 3.201154]\n",
      "epoch:39 step:30975 [D loss: 0.322899, acc.: 86.72%] [G loss: 4.209675]\n",
      "epoch:39 step:30976 [D loss: 0.347755, acc.: 84.38%] [G loss: 3.634720]\n",
      "epoch:39 step:30977 [D loss: 0.202122, acc.: 92.19%] [G loss: 4.549534]\n",
      "epoch:39 step:30978 [D loss: 0.248722, acc.: 88.28%] [G loss: 3.803258]\n",
      "epoch:39 step:30979 [D loss: 0.402816, acc.: 81.25%] [G loss: 4.061535]\n",
      "epoch:39 step:30980 [D loss: 0.316071, acc.: 89.06%] [G loss: 2.877984]\n",
      "epoch:39 step:30981 [D loss: 0.327232, acc.: 81.25%] [G loss: 3.800796]\n",
      "epoch:39 step:30982 [D loss: 0.258412, acc.: 89.84%] [G loss: 3.874508]\n",
      "epoch:39 step:30983 [D loss: 0.385810, acc.: 80.47%] [G loss: 2.681802]\n",
      "epoch:39 step:30984 [D loss: 0.336474, acc.: 83.59%] [G loss: 3.583056]\n",
      "epoch:39 step:30985 [D loss: 0.341458, acc.: 82.81%] [G loss: 3.201720]\n",
      "epoch:39 step:30986 [D loss: 0.318061, acc.: 87.50%] [G loss: 3.628628]\n",
      "epoch:39 step:30987 [D loss: 0.358346, acc.: 85.16%] [G loss: 4.147025]\n",
      "epoch:39 step:30988 [D loss: 0.317235, acc.: 85.94%] [G loss: 4.340366]\n",
      "epoch:39 step:30989 [D loss: 0.433535, acc.: 78.12%] [G loss: 6.164678]\n",
      "epoch:39 step:30990 [D loss: 0.412568, acc.: 84.38%] [G loss: 4.142271]\n",
      "epoch:39 step:30991 [D loss: 0.278156, acc.: 85.16%] [G loss: 3.599855]\n",
      "epoch:39 step:30992 [D loss: 0.280712, acc.: 85.16%] [G loss: 3.583473]\n",
      "epoch:39 step:30993 [D loss: 0.168387, acc.: 94.53%] [G loss: 3.604116]\n",
      "epoch:39 step:30994 [D loss: 0.346493, acc.: 82.03%] [G loss: 3.099063]\n",
      "epoch:39 step:30995 [D loss: 0.284561, acc.: 87.50%] [G loss: 2.950159]\n",
      "epoch:39 step:30996 [D loss: 0.333843, acc.: 82.81%] [G loss: 3.403482]\n",
      "epoch:39 step:30997 [D loss: 0.312922, acc.: 87.50%] [G loss: 3.259818]\n",
      "epoch:39 step:30998 [D loss: 0.251983, acc.: 87.50%] [G loss: 3.827114]\n",
      "epoch:39 step:30999 [D loss: 0.226949, acc.: 92.97%] [G loss: 3.528850]\n",
      "epoch:39 step:31000 [D loss: 0.294124, acc.: 88.28%] [G loss: 3.217808]\n",
      "epoch:39 step:31001 [D loss: 0.357664, acc.: 81.25%] [G loss: 2.923243]\n",
      "epoch:39 step:31002 [D loss: 0.264571, acc.: 88.28%] [G loss: 3.935358]\n",
      "epoch:39 step:31003 [D loss: 0.362767, acc.: 82.03%] [G loss: 3.148258]\n",
      "epoch:39 step:31004 [D loss: 0.264373, acc.: 89.84%] [G loss: 3.275876]\n",
      "epoch:39 step:31005 [D loss: 0.301614, acc.: 83.59%] [G loss: 2.990932]\n",
      "epoch:39 step:31006 [D loss: 0.339721, acc.: 82.81%] [G loss: 4.897650]\n",
      "epoch:39 step:31007 [D loss: 0.317918, acc.: 85.16%] [G loss: 3.889848]\n",
      "epoch:39 step:31008 [D loss: 0.288844, acc.: 88.28%] [G loss: 3.569683]\n",
      "epoch:39 step:31009 [D loss: 0.368294, acc.: 78.91%] [G loss: 3.860296]\n",
      "epoch:39 step:31010 [D loss: 0.266109, acc.: 88.28%] [G loss: 3.555351]\n",
      "epoch:39 step:31011 [D loss: 0.510147, acc.: 78.12%] [G loss: 4.923599]\n",
      "epoch:39 step:31012 [D loss: 0.524744, acc.: 78.91%] [G loss: 8.130669]\n",
      "epoch:39 step:31013 [D loss: 0.660679, acc.: 71.09%] [G loss: 3.906400]\n",
      "epoch:39 step:31014 [D loss: 0.288735, acc.: 86.72%] [G loss: 3.640224]\n",
      "epoch:39 step:31015 [D loss: 0.373005, acc.: 84.38%] [G loss: 2.930939]\n",
      "epoch:39 step:31016 [D loss: 0.322599, acc.: 86.72%] [G loss: 2.206286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:31017 [D loss: 0.372792, acc.: 80.47%] [G loss: 3.012380]\n",
      "epoch:39 step:31018 [D loss: 0.301904, acc.: 85.94%] [G loss: 2.607376]\n",
      "epoch:39 step:31019 [D loss: 0.349852, acc.: 83.59%] [G loss: 2.816069]\n",
      "epoch:39 step:31020 [D loss: 0.334994, acc.: 83.59%] [G loss: 2.686058]\n",
      "epoch:39 step:31021 [D loss: 0.308379, acc.: 86.72%] [G loss: 2.038335]\n",
      "epoch:39 step:31022 [D loss: 0.355271, acc.: 83.59%] [G loss: 3.119598]\n",
      "epoch:39 step:31023 [D loss: 0.255360, acc.: 88.28%] [G loss: 3.204303]\n",
      "epoch:39 step:31024 [D loss: 0.261231, acc.: 92.97%] [G loss: 3.752368]\n",
      "epoch:39 step:31025 [D loss: 0.297985, acc.: 89.06%] [G loss: 3.424582]\n",
      "epoch:39 step:31026 [D loss: 0.355195, acc.: 83.59%] [G loss: 3.202754]\n",
      "epoch:39 step:31027 [D loss: 0.474190, acc.: 78.91%] [G loss: 3.107718]\n",
      "epoch:39 step:31028 [D loss: 0.319094, acc.: 84.38%] [G loss: 3.462058]\n",
      "epoch:39 step:31029 [D loss: 0.334572, acc.: 81.25%] [G loss: 5.344984]\n",
      "epoch:39 step:31030 [D loss: 0.362028, acc.: 82.81%] [G loss: 3.724332]\n",
      "epoch:39 step:31031 [D loss: 0.266008, acc.: 87.50%] [G loss: 3.393667]\n",
      "epoch:39 step:31032 [D loss: 0.315943, acc.: 85.16%] [G loss: 3.654398]\n",
      "epoch:39 step:31033 [D loss: 0.310996, acc.: 83.59%] [G loss: 2.950414]\n",
      "epoch:39 step:31034 [D loss: 0.278607, acc.: 88.28%] [G loss: 2.968376]\n",
      "epoch:39 step:31035 [D loss: 0.343261, acc.: 85.94%] [G loss: 2.768408]\n",
      "epoch:39 step:31036 [D loss: 0.272287, acc.: 85.16%] [G loss: 2.832695]\n",
      "epoch:39 step:31037 [D loss: 0.237902, acc.: 88.28%] [G loss: 2.941156]\n",
      "epoch:39 step:31038 [D loss: 0.358227, acc.: 88.28%] [G loss: 3.003001]\n",
      "epoch:39 step:31039 [D loss: 0.342474, acc.: 83.59%] [G loss: 2.807783]\n",
      "epoch:39 step:31040 [D loss: 0.275046, acc.: 86.72%] [G loss: 3.239737]\n",
      "epoch:39 step:31041 [D loss: 0.340887, acc.: 81.25%] [G loss: 3.056783]\n",
      "epoch:39 step:31042 [D loss: 0.351772, acc.: 86.72%] [G loss: 2.452477]\n",
      "epoch:39 step:31043 [D loss: 0.322967, acc.: 85.94%] [G loss: 3.851233]\n",
      "epoch:39 step:31044 [D loss: 0.228967, acc.: 89.84%] [G loss: 3.569880]\n",
      "epoch:39 step:31045 [D loss: 0.292768, acc.: 84.38%] [G loss: 2.830607]\n",
      "epoch:39 step:31046 [D loss: 0.297279, acc.: 86.72%] [G loss: 4.243583]\n",
      "epoch:39 step:31047 [D loss: 0.336307, acc.: 86.72%] [G loss: 3.186400]\n",
      "epoch:39 step:31048 [D loss: 0.430441, acc.: 78.91%] [G loss: 3.126861]\n",
      "epoch:39 step:31049 [D loss: 0.404965, acc.: 82.03%] [G loss: 2.771822]\n",
      "epoch:39 step:31050 [D loss: 0.424170, acc.: 81.25%] [G loss: 2.481641]\n",
      "epoch:39 step:31051 [D loss: 0.313104, acc.: 85.16%] [G loss: 2.914824]\n",
      "epoch:39 step:31052 [D loss: 0.464688, acc.: 80.47%] [G loss: 2.875968]\n",
      "epoch:39 step:31053 [D loss: 0.383212, acc.: 82.81%] [G loss: 3.419426]\n",
      "epoch:39 step:31054 [D loss: 0.377831, acc.: 83.59%] [G loss: 4.752794]\n",
      "epoch:39 step:31055 [D loss: 0.467462, acc.: 82.03%] [G loss: 4.495305]\n",
      "epoch:39 step:31056 [D loss: 0.402592, acc.: 87.50%] [G loss: 3.080910]\n",
      "epoch:39 step:31057 [D loss: 0.278714, acc.: 89.84%] [G loss: 3.184276]\n",
      "epoch:39 step:31058 [D loss: 0.344989, acc.: 83.59%] [G loss: 3.373615]\n",
      "epoch:39 step:31059 [D loss: 0.377893, acc.: 83.59%] [G loss: 3.452649]\n",
      "epoch:39 step:31060 [D loss: 0.305477, acc.: 85.16%] [G loss: 2.107153]\n",
      "epoch:39 step:31061 [D loss: 0.241410, acc.: 89.06%] [G loss: 4.724482]\n",
      "epoch:39 step:31062 [D loss: 0.325462, acc.: 89.84%] [G loss: 3.048704]\n",
      "epoch:39 step:31063 [D loss: 0.308301, acc.: 88.28%] [G loss: 4.395435]\n",
      "epoch:39 step:31064 [D loss: 0.261242, acc.: 85.94%] [G loss: 5.339752]\n",
      "epoch:39 step:31065 [D loss: 0.319269, acc.: 82.81%] [G loss: 3.473001]\n",
      "epoch:39 step:31066 [D loss: 0.282803, acc.: 89.84%] [G loss: 4.245414]\n",
      "epoch:39 step:31067 [D loss: 0.311914, acc.: 89.06%] [G loss: 3.671263]\n",
      "epoch:39 step:31068 [D loss: 0.279955, acc.: 88.28%] [G loss: 3.509038]\n",
      "epoch:39 step:31069 [D loss: 0.286839, acc.: 89.84%] [G loss: 3.892658]\n",
      "epoch:39 step:31070 [D loss: 0.342902, acc.: 84.38%] [G loss: 3.077332]\n",
      "epoch:39 step:31071 [D loss: 0.289188, acc.: 85.94%] [G loss: 3.368796]\n",
      "epoch:39 step:31072 [D loss: 0.302434, acc.: 85.94%] [G loss: 2.338135]\n",
      "epoch:39 step:31073 [D loss: 0.229451, acc.: 91.41%] [G loss: 3.073884]\n",
      "epoch:39 step:31074 [D loss: 0.319826, acc.: 84.38%] [G loss: 3.589695]\n",
      "epoch:39 step:31075 [D loss: 0.349182, acc.: 86.72%] [G loss: 3.667245]\n",
      "epoch:39 step:31076 [D loss: 0.339925, acc.: 82.81%] [G loss: 4.144744]\n",
      "epoch:39 step:31077 [D loss: 0.366656, acc.: 79.69%] [G loss: 3.193074]\n",
      "epoch:39 step:31078 [D loss: 0.259767, acc.: 91.41%] [G loss: 3.894381]\n",
      "epoch:39 step:31079 [D loss: 0.225795, acc.: 89.06%] [G loss: 3.209195]\n",
      "epoch:39 step:31080 [D loss: 0.293829, acc.: 87.50%] [G loss: 3.066573]\n",
      "epoch:39 step:31081 [D loss: 0.212651, acc.: 92.19%] [G loss: 3.087831]\n",
      "epoch:39 step:31082 [D loss: 0.332336, acc.: 87.50%] [G loss: 3.107876]\n",
      "epoch:39 step:31083 [D loss: 0.378930, acc.: 80.47%] [G loss: 5.046925]\n",
      "epoch:39 step:31084 [D loss: 0.374208, acc.: 82.81%] [G loss: 3.868002]\n",
      "epoch:39 step:31085 [D loss: 0.281439, acc.: 88.28%] [G loss: 5.065383]\n",
      "epoch:39 step:31086 [D loss: 0.294851, acc.: 85.94%] [G loss: 4.670358]\n",
      "epoch:39 step:31087 [D loss: 0.353666, acc.: 81.25%] [G loss: 5.797028]\n",
      "epoch:39 step:31088 [D loss: 0.345171, acc.: 83.59%] [G loss: 3.840779]\n",
      "epoch:39 step:31089 [D loss: 0.337494, acc.: 86.72%] [G loss: 2.955221]\n",
      "epoch:39 step:31090 [D loss: 0.270710, acc.: 90.62%] [G loss: 3.749564]\n",
      "epoch:39 step:31091 [D loss: 0.273680, acc.: 86.72%] [G loss: 2.983625]\n",
      "epoch:39 step:31092 [D loss: 0.342868, acc.: 82.81%] [G loss: 2.605914]\n",
      "epoch:39 step:31093 [D loss: 0.340185, acc.: 87.50%] [G loss: 3.021125]\n",
      "epoch:39 step:31094 [D loss: 0.242800, acc.: 89.06%] [G loss: 2.668369]\n",
      "epoch:39 step:31095 [D loss: 0.239385, acc.: 90.62%] [G loss: 2.804310]\n",
      "epoch:39 step:31096 [D loss: 0.312059, acc.: 86.72%] [G loss: 2.672591]\n",
      "epoch:39 step:31097 [D loss: 0.307661, acc.: 85.16%] [G loss: 3.288690]\n",
      "epoch:39 step:31098 [D loss: 0.324912, acc.: 85.94%] [G loss: 3.169306]\n",
      "epoch:39 step:31099 [D loss: 0.388659, acc.: 82.03%] [G loss: 3.148017]\n",
      "epoch:39 step:31100 [D loss: 0.308839, acc.: 84.38%] [G loss: 3.862327]\n",
      "epoch:39 step:31101 [D loss: 0.302880, acc.: 83.59%] [G loss: 4.411834]\n",
      "epoch:39 step:31102 [D loss: 0.367640, acc.: 81.25%] [G loss: 4.616060]\n",
      "epoch:39 step:31103 [D loss: 0.292766, acc.: 87.50%] [G loss: 5.323273]\n",
      "epoch:39 step:31104 [D loss: 0.414529, acc.: 78.91%] [G loss: 3.346891]\n",
      "epoch:39 step:31105 [D loss: 0.338211, acc.: 80.47%] [G loss: 3.268614]\n",
      "epoch:39 step:31106 [D loss: 0.182330, acc.: 92.97%] [G loss: 3.028618]\n",
      "epoch:39 step:31107 [D loss: 0.301739, acc.: 87.50%] [G loss: 3.126012]\n",
      "epoch:39 step:31108 [D loss: 0.349061, acc.: 85.16%] [G loss: 3.281169]\n",
      "epoch:39 step:31109 [D loss: 0.257246, acc.: 89.06%] [G loss: 3.240642]\n",
      "epoch:39 step:31110 [D loss: 0.258047, acc.: 88.28%] [G loss: 2.732520]\n",
      "epoch:39 step:31111 [D loss: 0.272429, acc.: 89.84%] [G loss: 3.157440]\n",
      "epoch:39 step:31112 [D loss: 0.267359, acc.: 90.62%] [G loss: 3.522851]\n",
      "epoch:39 step:31113 [D loss: 0.345029, acc.: 85.16%] [G loss: 2.434577]\n",
      "epoch:39 step:31114 [D loss: 0.323996, acc.: 83.59%] [G loss: 2.326941]\n",
      "epoch:39 step:31115 [D loss: 0.229962, acc.: 89.84%] [G loss: 2.323471]\n",
      "epoch:39 step:31116 [D loss: 0.377551, acc.: 82.03%] [G loss: 3.083400]\n",
      "epoch:39 step:31117 [D loss: 0.326542, acc.: 87.50%] [G loss: 2.697588]\n",
      "epoch:39 step:31118 [D loss: 0.271074, acc.: 86.72%] [G loss: 2.816510]\n",
      "epoch:39 step:31119 [D loss: 0.306302, acc.: 87.50%] [G loss: 2.846369]\n",
      "epoch:39 step:31120 [D loss: 0.326392, acc.: 85.16%] [G loss: 3.286195]\n",
      "epoch:39 step:31121 [D loss: 0.232041, acc.: 89.06%] [G loss: 2.490923]\n",
      "epoch:39 step:31122 [D loss: 0.284715, acc.: 89.06%] [G loss: 3.022212]\n",
      "epoch:39 step:31123 [D loss: 0.379543, acc.: 80.47%] [G loss: 4.098001]\n",
      "epoch:39 step:31124 [D loss: 0.405354, acc.: 82.81%] [G loss: 2.692507]\n",
      "epoch:39 step:31125 [D loss: 0.312995, acc.: 85.94%] [G loss: 2.717400]\n",
      "epoch:39 step:31126 [D loss: 0.375767, acc.: 82.81%] [G loss: 3.395109]\n",
      "epoch:39 step:31127 [D loss: 0.247091, acc.: 89.84%] [G loss: 3.197227]\n",
      "epoch:39 step:31128 [D loss: 0.423675, acc.: 75.78%] [G loss: 4.475816]\n",
      "epoch:39 step:31129 [D loss: 0.387475, acc.: 80.47%] [G loss: 3.022817]\n",
      "epoch:39 step:31130 [D loss: 0.332011, acc.: 85.94%] [G loss: 3.551175]\n",
      "epoch:39 step:31131 [D loss: 0.368804, acc.: 85.16%] [G loss: 3.382127]\n",
      "epoch:39 step:31132 [D loss: 0.370283, acc.: 82.03%] [G loss: 3.843045]\n",
      "epoch:39 step:31133 [D loss: 0.275892, acc.: 89.84%] [G loss: 3.250157]\n",
      "epoch:39 step:31134 [D loss: 0.282177, acc.: 87.50%] [G loss: 2.903938]\n",
      "epoch:39 step:31135 [D loss: 0.273826, acc.: 87.50%] [G loss: 2.418315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:31136 [D loss: 0.301955, acc.: 86.72%] [G loss: 2.551963]\n",
      "epoch:39 step:31137 [D loss: 0.348935, acc.: 82.81%] [G loss: 2.468636]\n",
      "epoch:39 step:31138 [D loss: 0.260037, acc.: 86.72%] [G loss: 3.120968]\n",
      "epoch:39 step:31139 [D loss: 0.397541, acc.: 82.81%] [G loss: 2.999984]\n",
      "epoch:39 step:31140 [D loss: 0.265026, acc.: 83.59%] [G loss: 3.023135]\n",
      "epoch:39 step:31141 [D loss: 0.444533, acc.: 77.34%] [G loss: 2.497393]\n",
      "epoch:39 step:31142 [D loss: 0.300754, acc.: 82.03%] [G loss: 3.015076]\n",
      "epoch:39 step:31143 [D loss: 0.349726, acc.: 81.25%] [G loss: 3.095812]\n",
      "epoch:39 step:31144 [D loss: 0.436206, acc.: 80.47%] [G loss: 3.517319]\n",
      "epoch:39 step:31145 [D loss: 0.327214, acc.: 82.81%] [G loss: 2.707408]\n",
      "epoch:39 step:31146 [D loss: 0.313638, acc.: 88.28%] [G loss: 3.411595]\n",
      "epoch:39 step:31147 [D loss: 0.284014, acc.: 87.50%] [G loss: 3.026134]\n",
      "epoch:39 step:31148 [D loss: 0.376523, acc.: 87.50%] [G loss: 3.403482]\n",
      "epoch:39 step:31149 [D loss: 0.286811, acc.: 85.94%] [G loss: 3.199375]\n",
      "epoch:39 step:31150 [D loss: 0.285707, acc.: 86.72%] [G loss: 3.394997]\n",
      "epoch:39 step:31151 [D loss: 0.413683, acc.: 83.59%] [G loss: 5.161028]\n",
      "epoch:39 step:31152 [D loss: 0.426882, acc.: 82.81%] [G loss: 3.558725]\n",
      "epoch:39 step:31153 [D loss: 0.482062, acc.: 78.91%] [G loss: 3.810944]\n",
      "epoch:39 step:31154 [D loss: 0.486393, acc.: 85.94%] [G loss: 8.090416]\n",
      "epoch:39 step:31155 [D loss: 1.524573, acc.: 67.19%] [G loss: 6.218385]\n",
      "epoch:39 step:31156 [D loss: 1.538486, acc.: 62.50%] [G loss: 5.965390]\n",
      "epoch:39 step:31157 [D loss: 0.408651, acc.: 83.59%] [G loss: 5.359215]\n",
      "epoch:39 step:31158 [D loss: 0.976726, acc.: 64.06%] [G loss: 5.327040]\n",
      "epoch:39 step:31159 [D loss: 0.675802, acc.: 78.12%] [G loss: 5.411633]\n",
      "epoch:39 step:31160 [D loss: 0.458176, acc.: 82.81%] [G loss: 3.548696]\n",
      "epoch:39 step:31161 [D loss: 0.344226, acc.: 84.38%] [G loss: 3.473982]\n",
      "epoch:39 step:31162 [D loss: 0.340106, acc.: 85.16%] [G loss: 3.064127]\n",
      "epoch:39 step:31163 [D loss: 0.398271, acc.: 82.03%] [G loss: 3.661641]\n",
      "epoch:39 step:31164 [D loss: 0.308891, acc.: 84.38%] [G loss: 2.673620]\n",
      "epoch:39 step:31165 [D loss: 0.354426, acc.: 82.03%] [G loss: 3.701506]\n",
      "epoch:39 step:31166 [D loss: 0.414086, acc.: 79.69%] [G loss: 2.080092]\n",
      "epoch:39 step:31167 [D loss: 0.318994, acc.: 86.72%] [G loss: 2.797149]\n",
      "epoch:39 step:31168 [D loss: 0.324671, acc.: 86.72%] [G loss: 2.298316]\n",
      "epoch:39 step:31169 [D loss: 0.294835, acc.: 85.16%] [G loss: 2.772834]\n",
      "epoch:39 step:31170 [D loss: 0.336394, acc.: 85.16%] [G loss: 2.614468]\n",
      "epoch:39 step:31171 [D loss: 0.302736, acc.: 89.06%] [G loss: 2.791258]\n",
      "epoch:39 step:31172 [D loss: 0.327808, acc.: 83.59%] [G loss: 3.409085]\n",
      "epoch:39 step:31173 [D loss: 0.207551, acc.: 90.62%] [G loss: 2.845898]\n",
      "epoch:39 step:31174 [D loss: 0.335445, acc.: 84.38%] [G loss: 3.091549]\n",
      "epoch:39 step:31175 [D loss: 0.364651, acc.: 82.81%] [G loss: 3.133191]\n",
      "epoch:39 step:31176 [D loss: 0.295175, acc.: 89.84%] [G loss: 2.811242]\n",
      "epoch:39 step:31177 [D loss: 0.281939, acc.: 88.28%] [G loss: 2.903666]\n",
      "epoch:39 step:31178 [D loss: 0.264227, acc.: 89.06%] [G loss: 2.980715]\n",
      "epoch:39 step:31179 [D loss: 0.244971, acc.: 92.97%] [G loss: 2.542153]\n",
      "epoch:39 step:31180 [D loss: 0.269400, acc.: 89.06%] [G loss: 2.693895]\n",
      "epoch:39 step:31181 [D loss: 0.248245, acc.: 89.84%] [G loss: 2.711196]\n",
      "epoch:39 step:31182 [D loss: 0.266465, acc.: 89.06%] [G loss: 2.908143]\n",
      "epoch:39 step:31183 [D loss: 0.292775, acc.: 86.72%] [G loss: 2.861189]\n",
      "epoch:39 step:31184 [D loss: 0.254295, acc.: 88.28%] [G loss: 2.862739]\n",
      "epoch:39 step:31185 [D loss: 0.253899, acc.: 92.19%] [G loss: 2.651234]\n",
      "epoch:39 step:31186 [D loss: 0.287123, acc.: 89.06%] [G loss: 3.411240]\n",
      "epoch:39 step:31187 [D loss: 0.370488, acc.: 82.81%] [G loss: 2.942686]\n",
      "epoch:39 step:31188 [D loss: 0.412828, acc.: 75.78%] [G loss: 2.483622]\n",
      "epoch:39 step:31189 [D loss: 0.224529, acc.: 89.84%] [G loss: 2.908615]\n",
      "epoch:39 step:31190 [D loss: 0.334016, acc.: 84.38%] [G loss: 3.326885]\n",
      "epoch:39 step:31191 [D loss: 0.320275, acc.: 82.03%] [G loss: 4.063354]\n",
      "epoch:39 step:31192 [D loss: 0.257757, acc.: 88.28%] [G loss: 3.383029]\n",
      "epoch:39 step:31193 [D loss: 0.269820, acc.: 86.72%] [G loss: 3.910759]\n",
      "epoch:39 step:31194 [D loss: 0.241439, acc.: 90.62%] [G loss: 2.691914]\n",
      "epoch:39 step:31195 [D loss: 0.315935, acc.: 89.06%] [G loss: 2.668904]\n",
      "epoch:39 step:31196 [D loss: 0.393086, acc.: 83.59%] [G loss: 2.581862]\n",
      "epoch:39 step:31197 [D loss: 0.376189, acc.: 83.59%] [G loss: 2.708502]\n",
      "epoch:39 step:31198 [D loss: 0.308815, acc.: 83.59%] [G loss: 2.857537]\n",
      "epoch:39 step:31199 [D loss: 0.327386, acc.: 86.72%] [G loss: 4.083529]\n",
      "epoch:39 step:31200 [D loss: 0.276532, acc.: 88.28%] [G loss: 2.631073]\n",
      "epoch:39 step:31201 [D loss: 0.336683, acc.: 85.16%] [G loss: 3.779346]\n",
      "epoch:39 step:31202 [D loss: 0.331256, acc.: 85.16%] [G loss: 2.677349]\n",
      "epoch:39 step:31203 [D loss: 0.291736, acc.: 85.94%] [G loss: 3.271508]\n",
      "epoch:39 step:31204 [D loss: 0.255399, acc.: 90.62%] [G loss: 3.058402]\n",
      "epoch:39 step:31205 [D loss: 0.393770, acc.: 80.47%] [G loss: 2.765841]\n",
      "epoch:39 step:31206 [D loss: 0.181513, acc.: 93.75%] [G loss: 3.247346]\n",
      "epoch:39 step:31207 [D loss: 0.284208, acc.: 89.06%] [G loss: 4.132073]\n",
      "epoch:39 step:31208 [D loss: 0.302644, acc.: 85.94%] [G loss: 4.511458]\n",
      "epoch:39 step:31209 [D loss: 0.282239, acc.: 88.28%] [G loss: 4.166122]\n",
      "epoch:39 step:31210 [D loss: 0.305080, acc.: 89.06%] [G loss: 3.606366]\n",
      "epoch:39 step:31211 [D loss: 0.265435, acc.: 87.50%] [G loss: 4.011736]\n",
      "epoch:39 step:31212 [D loss: 0.397688, acc.: 80.47%] [G loss: 4.323238]\n",
      "epoch:39 step:31213 [D loss: 0.308405, acc.: 86.72%] [G loss: 3.730464]\n",
      "epoch:39 step:31214 [D loss: 0.349343, acc.: 85.16%] [G loss: 3.169551]\n",
      "epoch:39 step:31215 [D loss: 0.289120, acc.: 86.72%] [G loss: 2.808790]\n",
      "epoch:39 step:31216 [D loss: 0.269402, acc.: 89.84%] [G loss: 3.428295]\n",
      "epoch:39 step:31217 [D loss: 0.391396, acc.: 80.47%] [G loss: 2.378073]\n",
      "epoch:39 step:31218 [D loss: 0.352871, acc.: 88.28%] [G loss: 3.048120]\n",
      "epoch:39 step:31219 [D loss: 0.271585, acc.: 86.72%] [G loss: 3.276715]\n",
      "epoch:39 step:31220 [D loss: 0.261611, acc.: 89.06%] [G loss: 3.433903]\n",
      "epoch:39 step:31221 [D loss: 0.331054, acc.: 88.28%] [G loss: 2.814697]\n",
      "epoch:39 step:31222 [D loss: 0.325655, acc.: 85.16%] [G loss: 2.868052]\n",
      "epoch:39 step:31223 [D loss: 0.323690, acc.: 85.16%] [G loss: 3.009509]\n",
      "epoch:39 step:31224 [D loss: 0.361253, acc.: 85.16%] [G loss: 3.041392]\n",
      "epoch:39 step:31225 [D loss: 0.334304, acc.: 86.72%] [G loss: 2.747759]\n",
      "epoch:39 step:31226 [D loss: 0.293626, acc.: 89.06%] [G loss: 2.974023]\n",
      "epoch:39 step:31227 [D loss: 0.326815, acc.: 85.94%] [G loss: 3.202724]\n",
      "epoch:39 step:31228 [D loss: 0.232964, acc.: 90.62%] [G loss: 3.377979]\n",
      "epoch:39 step:31229 [D loss: 0.322708, acc.: 85.16%] [G loss: 2.145398]\n",
      "epoch:39 step:31230 [D loss: 0.334007, acc.: 85.16%] [G loss: 2.774917]\n",
      "epoch:39 step:31231 [D loss: 0.245099, acc.: 89.06%] [G loss: 3.283127]\n",
      "epoch:39 step:31232 [D loss: 0.275360, acc.: 90.62%] [G loss: 2.709460]\n",
      "epoch:39 step:31233 [D loss: 0.309081, acc.: 87.50%] [G loss: 3.129588]\n",
      "epoch:39 step:31234 [D loss: 0.330065, acc.: 83.59%] [G loss: 3.794748]\n",
      "epoch:39 step:31235 [D loss: 0.340042, acc.: 87.50%] [G loss: 3.314761]\n",
      "epoch:39 step:31236 [D loss: 0.459065, acc.: 79.69%] [G loss: 2.688756]\n",
      "epoch:39 step:31237 [D loss: 0.305237, acc.: 84.38%] [G loss: 3.027944]\n",
      "epoch:39 step:31238 [D loss: 0.323723, acc.: 87.50%] [G loss: 2.496449]\n",
      "epoch:39 step:31239 [D loss: 0.306247, acc.: 85.94%] [G loss: 2.217614]\n",
      "epoch:39 step:31240 [D loss: 0.333893, acc.: 83.59%] [G loss: 3.069656]\n",
      "epoch:40 step:31241 [D loss: 0.305698, acc.: 88.28%] [G loss: 2.666689]\n",
      "epoch:40 step:31242 [D loss: 0.335473, acc.: 85.94%] [G loss: 3.518056]\n",
      "epoch:40 step:31243 [D loss: 0.259083, acc.: 90.62%] [G loss: 3.422601]\n",
      "epoch:40 step:31244 [D loss: 0.285816, acc.: 89.06%] [G loss: 2.844187]\n",
      "epoch:40 step:31245 [D loss: 0.351529, acc.: 82.03%] [G loss: 4.997671]\n",
      "epoch:40 step:31246 [D loss: 0.404446, acc.: 84.38%] [G loss: 3.020195]\n",
      "epoch:40 step:31247 [D loss: 0.400672, acc.: 81.25%] [G loss: 4.647462]\n",
      "epoch:40 step:31248 [D loss: 0.336691, acc.: 85.94%] [G loss: 3.603151]\n",
      "epoch:40 step:31249 [D loss: 0.394118, acc.: 81.25%] [G loss: 3.920093]\n",
      "epoch:40 step:31250 [D loss: 0.337613, acc.: 87.50%] [G loss: 2.723383]\n",
      "epoch:40 step:31251 [D loss: 0.366666, acc.: 85.16%] [G loss: 3.504493]\n",
      "epoch:40 step:31252 [D loss: 0.250212, acc.: 88.28%] [G loss: 3.797862]\n",
      "epoch:40 step:31253 [D loss: 0.456214, acc.: 79.69%] [G loss: 2.684884]\n",
      "epoch:40 step:31254 [D loss: 0.291689, acc.: 89.06%] [G loss: 3.574942]\n",
      "epoch:40 step:31255 [D loss: 0.314279, acc.: 86.72%] [G loss: 3.150418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31256 [D loss: 0.290186, acc.: 86.72%] [G loss: 3.001578]\n",
      "epoch:40 step:31257 [D loss: 0.247762, acc.: 91.41%] [G loss: 3.435413]\n",
      "epoch:40 step:31258 [D loss: 0.351713, acc.: 85.16%] [G loss: 3.678514]\n",
      "epoch:40 step:31259 [D loss: 0.441241, acc.: 75.78%] [G loss: 3.178962]\n",
      "epoch:40 step:31260 [D loss: 0.408916, acc.: 82.03%] [G loss: 2.493218]\n",
      "epoch:40 step:31261 [D loss: 0.298385, acc.: 85.94%] [G loss: 2.523548]\n",
      "epoch:40 step:31262 [D loss: 0.431171, acc.: 78.12%] [G loss: 2.381184]\n",
      "epoch:40 step:31263 [D loss: 0.269914, acc.: 88.28%] [G loss: 2.543065]\n",
      "epoch:40 step:31264 [D loss: 0.254274, acc.: 89.84%] [G loss: 3.842248]\n",
      "epoch:40 step:31265 [D loss: 0.391889, acc.: 81.25%] [G loss: 2.905097]\n",
      "epoch:40 step:31266 [D loss: 0.227327, acc.: 92.19%] [G loss: 2.814347]\n",
      "epoch:40 step:31267 [D loss: 0.381286, acc.: 80.47%] [G loss: 2.851245]\n",
      "epoch:40 step:31268 [D loss: 0.278964, acc.: 88.28%] [G loss: 2.869886]\n",
      "epoch:40 step:31269 [D loss: 0.344273, acc.: 82.03%] [G loss: 2.709421]\n",
      "epoch:40 step:31270 [D loss: 0.267736, acc.: 90.62%] [G loss: 2.993546]\n",
      "epoch:40 step:31271 [D loss: 0.310828, acc.: 84.38%] [G loss: 3.272797]\n",
      "epoch:40 step:31272 [D loss: 0.326455, acc.: 85.94%] [G loss: 2.880408]\n",
      "epoch:40 step:31273 [D loss: 0.309100, acc.: 84.38%] [G loss: 2.898536]\n",
      "epoch:40 step:31274 [D loss: 0.342447, acc.: 83.59%] [G loss: 4.269423]\n",
      "epoch:40 step:31275 [D loss: 0.411823, acc.: 82.03%] [G loss: 4.112210]\n",
      "epoch:40 step:31276 [D loss: 0.297848, acc.: 85.94%] [G loss: 5.318086]\n",
      "epoch:40 step:31277 [D loss: 0.272803, acc.: 88.28%] [G loss: 3.919361]\n",
      "epoch:40 step:31278 [D loss: 0.293193, acc.: 88.28%] [G loss: 3.462107]\n",
      "epoch:40 step:31279 [D loss: 0.293961, acc.: 84.38%] [G loss: 2.826955]\n",
      "epoch:40 step:31280 [D loss: 0.300943, acc.: 88.28%] [G loss: 3.384498]\n",
      "epoch:40 step:31281 [D loss: 0.300192, acc.: 87.50%] [G loss: 3.882174]\n",
      "epoch:40 step:31282 [D loss: 0.278406, acc.: 85.16%] [G loss: 3.526565]\n",
      "epoch:40 step:31283 [D loss: 0.379170, acc.: 78.12%] [G loss: 3.073151]\n",
      "epoch:40 step:31284 [D loss: 0.313778, acc.: 85.16%] [G loss: 3.943699]\n",
      "epoch:40 step:31285 [D loss: 0.384328, acc.: 82.81%] [G loss: 2.934012]\n",
      "epoch:40 step:31286 [D loss: 0.336008, acc.: 87.50%] [G loss: 4.054478]\n",
      "epoch:40 step:31287 [D loss: 0.583564, acc.: 71.09%] [G loss: 4.428579]\n",
      "epoch:40 step:31288 [D loss: 0.832316, acc.: 69.53%] [G loss: 6.628765]\n",
      "epoch:40 step:31289 [D loss: 1.031752, acc.: 71.09%] [G loss: 8.214684]\n",
      "epoch:40 step:31290 [D loss: 1.038128, acc.: 67.97%] [G loss: 4.762770]\n",
      "epoch:40 step:31291 [D loss: 0.417393, acc.: 82.03%] [G loss: 4.263125]\n",
      "epoch:40 step:31292 [D loss: 0.332844, acc.: 85.94%] [G loss: 4.146941]\n",
      "epoch:40 step:31293 [D loss: 0.343314, acc.: 85.16%] [G loss: 4.989970]\n",
      "epoch:40 step:31294 [D loss: 0.234573, acc.: 91.41%] [G loss: 4.187609]\n",
      "epoch:40 step:31295 [D loss: 0.409063, acc.: 85.94%] [G loss: 3.298728]\n",
      "epoch:40 step:31296 [D loss: 0.492497, acc.: 78.91%] [G loss: 3.158106]\n",
      "epoch:40 step:31297 [D loss: 0.296102, acc.: 88.28%] [G loss: 2.705494]\n",
      "epoch:40 step:31298 [D loss: 0.384550, acc.: 81.25%] [G loss: 2.975699]\n",
      "epoch:40 step:31299 [D loss: 0.287765, acc.: 84.38%] [G loss: 4.731960]\n",
      "epoch:40 step:31300 [D loss: 0.463285, acc.: 78.12%] [G loss: 3.578608]\n",
      "epoch:40 step:31301 [D loss: 0.333460, acc.: 84.38%] [G loss: 2.696502]\n",
      "epoch:40 step:31302 [D loss: 0.326838, acc.: 85.16%] [G loss: 3.691155]\n",
      "epoch:40 step:31303 [D loss: 0.195945, acc.: 94.53%] [G loss: 4.898699]\n",
      "epoch:40 step:31304 [D loss: 0.281846, acc.: 87.50%] [G loss: 4.467361]\n",
      "epoch:40 step:31305 [D loss: 0.377464, acc.: 85.94%] [G loss: 4.152304]\n",
      "epoch:40 step:31306 [D loss: 0.269028, acc.: 86.72%] [G loss: 2.596607]\n",
      "epoch:40 step:31307 [D loss: 0.252818, acc.: 91.41%] [G loss: 3.227143]\n",
      "epoch:40 step:31308 [D loss: 0.217494, acc.: 92.19%] [G loss: 2.963404]\n",
      "epoch:40 step:31309 [D loss: 0.274692, acc.: 89.84%] [G loss: 2.814877]\n",
      "epoch:40 step:31310 [D loss: 0.374782, acc.: 79.69%] [G loss: 2.360043]\n",
      "epoch:40 step:31311 [D loss: 0.313913, acc.: 87.50%] [G loss: 2.781259]\n",
      "epoch:40 step:31312 [D loss: 0.283976, acc.: 89.06%] [G loss: 2.693682]\n",
      "epoch:40 step:31313 [D loss: 0.277360, acc.: 88.28%] [G loss: 2.454746]\n",
      "epoch:40 step:31314 [D loss: 0.389805, acc.: 82.03%] [G loss: 3.125414]\n",
      "epoch:40 step:31315 [D loss: 0.333612, acc.: 86.72%] [G loss: 3.615377]\n",
      "epoch:40 step:31316 [D loss: 0.356999, acc.: 80.47%] [G loss: 3.610362]\n",
      "epoch:40 step:31317 [D loss: 0.327985, acc.: 85.94%] [G loss: 3.583560]\n",
      "epoch:40 step:31318 [D loss: 0.374479, acc.: 79.69%] [G loss: 3.232105]\n",
      "epoch:40 step:31319 [D loss: 0.315215, acc.: 86.72%] [G loss: 3.269224]\n",
      "epoch:40 step:31320 [D loss: 0.253498, acc.: 91.41%] [G loss: 2.883654]\n",
      "epoch:40 step:31321 [D loss: 0.310725, acc.: 86.72%] [G loss: 3.231800]\n",
      "epoch:40 step:31322 [D loss: 0.200940, acc.: 89.06%] [G loss: 4.451352]\n",
      "epoch:40 step:31323 [D loss: 0.268311, acc.: 88.28%] [G loss: 3.766177]\n",
      "epoch:40 step:31324 [D loss: 0.230383, acc.: 91.41%] [G loss: 3.366148]\n",
      "epoch:40 step:31325 [D loss: 0.253596, acc.: 90.62%] [G loss: 3.130490]\n",
      "epoch:40 step:31326 [D loss: 0.275674, acc.: 86.72%] [G loss: 4.403927]\n",
      "epoch:40 step:31327 [D loss: 0.246030, acc.: 89.84%] [G loss: 4.129017]\n",
      "epoch:40 step:31328 [D loss: 0.245480, acc.: 88.28%] [G loss: 4.522851]\n",
      "epoch:40 step:31329 [D loss: 0.301492, acc.: 87.50%] [G loss: 4.024834]\n",
      "epoch:40 step:31330 [D loss: 0.272129, acc.: 88.28%] [G loss: 3.380144]\n",
      "epoch:40 step:31331 [D loss: 0.249819, acc.: 90.62%] [G loss: 2.815179]\n",
      "epoch:40 step:31332 [D loss: 0.272183, acc.: 87.50%] [G loss: 2.951877]\n",
      "epoch:40 step:31333 [D loss: 0.311101, acc.: 85.16%] [G loss: 2.448812]\n",
      "epoch:40 step:31334 [D loss: 0.334357, acc.: 83.59%] [G loss: 2.978956]\n",
      "epoch:40 step:31335 [D loss: 0.392842, acc.: 84.38%] [G loss: 2.699792]\n",
      "epoch:40 step:31336 [D loss: 0.400571, acc.: 84.38%] [G loss: 2.593462]\n",
      "epoch:40 step:31337 [D loss: 0.368993, acc.: 80.47%] [G loss: 2.801304]\n",
      "epoch:40 step:31338 [D loss: 0.281566, acc.: 86.72%] [G loss: 3.109893]\n",
      "epoch:40 step:31339 [D loss: 0.402317, acc.: 79.69%] [G loss: 3.233620]\n",
      "epoch:40 step:31340 [D loss: 0.266117, acc.: 89.84%] [G loss: 3.357730]\n",
      "epoch:40 step:31341 [D loss: 0.375793, acc.: 81.25%] [G loss: 3.619331]\n",
      "epoch:40 step:31342 [D loss: 0.378347, acc.: 83.59%] [G loss: 2.958510]\n",
      "epoch:40 step:31343 [D loss: 0.338792, acc.: 85.16%] [G loss: 2.476920]\n",
      "epoch:40 step:31344 [D loss: 0.303706, acc.: 89.06%] [G loss: 2.734775]\n",
      "epoch:40 step:31345 [D loss: 0.287086, acc.: 83.59%] [G loss: 3.004976]\n",
      "epoch:40 step:31346 [D loss: 0.302500, acc.: 86.72%] [G loss: 2.969933]\n",
      "epoch:40 step:31347 [D loss: 0.421897, acc.: 81.25%] [G loss: 3.294164]\n",
      "epoch:40 step:31348 [D loss: 0.361420, acc.: 85.16%] [G loss: 2.992525]\n",
      "epoch:40 step:31349 [D loss: 0.316038, acc.: 85.16%] [G loss: 2.762898]\n",
      "epoch:40 step:31350 [D loss: 0.369031, acc.: 81.25%] [G loss: 2.873666]\n",
      "epoch:40 step:31351 [D loss: 0.396114, acc.: 83.59%] [G loss: 3.149673]\n",
      "epoch:40 step:31352 [D loss: 0.270122, acc.: 85.94%] [G loss: 4.172808]\n",
      "epoch:40 step:31353 [D loss: 0.342518, acc.: 83.59%] [G loss: 3.039148]\n",
      "epoch:40 step:31354 [D loss: 0.334353, acc.: 87.50%] [G loss: 3.732306]\n",
      "epoch:40 step:31355 [D loss: 0.275955, acc.: 88.28%] [G loss: 3.738775]\n",
      "epoch:40 step:31356 [D loss: 0.311018, acc.: 85.94%] [G loss: 4.597265]\n",
      "epoch:40 step:31357 [D loss: 0.347787, acc.: 83.59%] [G loss: 2.775057]\n",
      "epoch:40 step:31358 [D loss: 0.279502, acc.: 86.72%] [G loss: 3.287190]\n",
      "epoch:40 step:31359 [D loss: 0.310659, acc.: 85.94%] [G loss: 3.236536]\n",
      "epoch:40 step:31360 [D loss: 0.424187, acc.: 79.69%] [G loss: 2.729967]\n",
      "epoch:40 step:31361 [D loss: 0.284091, acc.: 87.50%] [G loss: 3.251471]\n",
      "epoch:40 step:31362 [D loss: 0.281916, acc.: 89.06%] [G loss: 2.861468]\n",
      "epoch:40 step:31363 [D loss: 0.339853, acc.: 85.94%] [G loss: 2.261669]\n",
      "epoch:40 step:31364 [D loss: 0.347339, acc.: 83.59%] [G loss: 2.398839]\n",
      "epoch:40 step:31365 [D loss: 0.339745, acc.: 87.50%] [G loss: 2.528738]\n",
      "epoch:40 step:31366 [D loss: 0.302722, acc.: 89.06%] [G loss: 2.865006]\n",
      "epoch:40 step:31367 [D loss: 0.319505, acc.: 85.94%] [G loss: 2.263378]\n",
      "epoch:40 step:31368 [D loss: 0.277126, acc.: 88.28%] [G loss: 2.975889]\n",
      "epoch:40 step:31369 [D loss: 0.370266, acc.: 82.81%] [G loss: 3.598032]\n",
      "epoch:40 step:31370 [D loss: 0.388404, acc.: 81.25%] [G loss: 3.494969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31371 [D loss: 0.338931, acc.: 86.72%] [G loss: 2.248047]\n",
      "epoch:40 step:31372 [D loss: 0.404245, acc.: 81.25%] [G loss: 2.666683]\n",
      "epoch:40 step:31373 [D loss: 0.364128, acc.: 85.16%] [G loss: 2.546937]\n",
      "epoch:40 step:31374 [D loss: 0.311680, acc.: 86.72%] [G loss: 3.861627]\n",
      "epoch:40 step:31375 [D loss: 0.348874, acc.: 86.72%] [G loss: 2.829354]\n",
      "epoch:40 step:31376 [D loss: 0.346068, acc.: 83.59%] [G loss: 3.136442]\n",
      "epoch:40 step:31377 [D loss: 0.265107, acc.: 87.50%] [G loss: 3.095047]\n",
      "epoch:40 step:31378 [D loss: 0.288956, acc.: 89.06%] [G loss: 3.273115]\n",
      "epoch:40 step:31379 [D loss: 0.268373, acc.: 89.84%] [G loss: 2.617773]\n",
      "epoch:40 step:31380 [D loss: 0.295113, acc.: 88.28%] [G loss: 3.878059]\n",
      "epoch:40 step:31381 [D loss: 0.309476, acc.: 86.72%] [G loss: 2.796606]\n",
      "epoch:40 step:31382 [D loss: 0.565890, acc.: 73.44%] [G loss: 4.588788]\n",
      "epoch:40 step:31383 [D loss: 0.453111, acc.: 78.91%] [G loss: 3.073487]\n",
      "epoch:40 step:31384 [D loss: 0.334850, acc.: 82.81%] [G loss: 3.834116]\n",
      "epoch:40 step:31385 [D loss: 0.250148, acc.: 91.41%] [G loss: 3.302702]\n",
      "epoch:40 step:31386 [D loss: 0.321005, acc.: 82.81%] [G loss: 2.647893]\n",
      "epoch:40 step:31387 [D loss: 0.337786, acc.: 83.59%] [G loss: 3.259057]\n",
      "epoch:40 step:31388 [D loss: 0.369975, acc.: 83.59%] [G loss: 2.697193]\n",
      "epoch:40 step:31389 [D loss: 0.344245, acc.: 83.59%] [G loss: 2.568433]\n",
      "epoch:40 step:31390 [D loss: 0.350285, acc.: 87.50%] [G loss: 2.983654]\n",
      "epoch:40 step:31391 [D loss: 0.306562, acc.: 87.50%] [G loss: 2.660501]\n",
      "epoch:40 step:31392 [D loss: 0.327020, acc.: 85.94%] [G loss: 3.412184]\n",
      "epoch:40 step:31393 [D loss: 0.391192, acc.: 82.03%] [G loss: 3.144620]\n",
      "epoch:40 step:31394 [D loss: 0.348347, acc.: 84.38%] [G loss: 4.249815]\n",
      "epoch:40 step:31395 [D loss: 0.226007, acc.: 93.75%] [G loss: 3.897816]\n",
      "epoch:40 step:31396 [D loss: 0.339042, acc.: 82.81%] [G loss: 4.127908]\n",
      "epoch:40 step:31397 [D loss: 0.392337, acc.: 84.38%] [G loss: 3.056591]\n",
      "epoch:40 step:31398 [D loss: 0.280145, acc.: 85.94%] [G loss: 2.999241]\n",
      "epoch:40 step:31399 [D loss: 0.283509, acc.: 88.28%] [G loss: 3.750673]\n",
      "epoch:40 step:31400 [D loss: 0.409426, acc.: 79.69%] [G loss: 3.420281]\n",
      "epoch:40 step:31401 [D loss: 0.321090, acc.: 85.94%] [G loss: 4.045998]\n",
      "epoch:40 step:31402 [D loss: 0.253690, acc.: 91.41%] [G loss: 3.294340]\n",
      "epoch:40 step:31403 [D loss: 0.359352, acc.: 85.16%] [G loss: 2.955729]\n",
      "epoch:40 step:31404 [D loss: 0.300887, acc.: 89.06%] [G loss: 3.199935]\n",
      "epoch:40 step:31405 [D loss: 0.352446, acc.: 83.59%] [G loss: 3.332847]\n",
      "epoch:40 step:31406 [D loss: 0.405165, acc.: 81.25%] [G loss: 3.492672]\n",
      "epoch:40 step:31407 [D loss: 0.285776, acc.: 89.06%] [G loss: 3.748734]\n",
      "epoch:40 step:31408 [D loss: 0.365520, acc.: 84.38%] [G loss: 2.909029]\n",
      "epoch:40 step:31409 [D loss: 0.426959, acc.: 80.47%] [G loss: 3.026373]\n",
      "epoch:40 step:31410 [D loss: 0.382498, acc.: 80.47%] [G loss: 3.349559]\n",
      "epoch:40 step:31411 [D loss: 0.338954, acc.: 82.81%] [G loss: 3.091084]\n",
      "epoch:40 step:31412 [D loss: 0.349628, acc.: 82.81%] [G loss: 3.154443]\n",
      "epoch:40 step:31413 [D loss: 0.398780, acc.: 84.38%] [G loss: 3.220286]\n",
      "epoch:40 step:31414 [D loss: 0.275935, acc.: 88.28%] [G loss: 3.373298]\n",
      "epoch:40 step:31415 [D loss: 0.365147, acc.: 85.16%] [G loss: 3.571335]\n",
      "epoch:40 step:31416 [D loss: 0.399645, acc.: 79.69%] [G loss: 4.286637]\n",
      "epoch:40 step:31417 [D loss: 0.374917, acc.: 76.56%] [G loss: 3.994075]\n",
      "epoch:40 step:31418 [D loss: 0.293242, acc.: 87.50%] [G loss: 3.193806]\n",
      "epoch:40 step:31419 [D loss: 0.399379, acc.: 81.25%] [G loss: 3.331336]\n",
      "epoch:40 step:31420 [D loss: 0.294180, acc.: 89.06%] [G loss: 3.500554]\n",
      "epoch:40 step:31421 [D loss: 0.297628, acc.: 88.28%] [G loss: 3.707531]\n",
      "epoch:40 step:31422 [D loss: 0.340713, acc.: 83.59%] [G loss: 3.648819]\n",
      "epoch:40 step:31423 [D loss: 0.315589, acc.: 85.16%] [G loss: 3.410843]\n",
      "epoch:40 step:31424 [D loss: 0.298976, acc.: 87.50%] [G loss: 4.191383]\n",
      "epoch:40 step:31425 [D loss: 0.284792, acc.: 87.50%] [G loss: 3.496296]\n",
      "epoch:40 step:31426 [D loss: 0.240409, acc.: 89.84%] [G loss: 4.621614]\n",
      "epoch:40 step:31427 [D loss: 0.335419, acc.: 86.72%] [G loss: 4.287818]\n",
      "epoch:40 step:31428 [D loss: 0.299231, acc.: 87.50%] [G loss: 4.538255]\n",
      "epoch:40 step:31429 [D loss: 0.244983, acc.: 93.75%] [G loss: 4.159244]\n",
      "epoch:40 step:31430 [D loss: 0.353955, acc.: 85.16%] [G loss: 4.216782]\n",
      "epoch:40 step:31431 [D loss: 0.268695, acc.: 87.50%] [G loss: 3.476157]\n",
      "epoch:40 step:31432 [D loss: 0.304584, acc.: 86.72%] [G loss: 3.181405]\n",
      "epoch:40 step:31433 [D loss: 0.335926, acc.: 82.81%] [G loss: 3.672116]\n",
      "epoch:40 step:31434 [D loss: 0.271325, acc.: 88.28%] [G loss: 3.674688]\n",
      "epoch:40 step:31435 [D loss: 0.264380, acc.: 88.28%] [G loss: 3.415287]\n",
      "epoch:40 step:31436 [D loss: 0.264940, acc.: 89.06%] [G loss: 3.911422]\n",
      "epoch:40 step:31437 [D loss: 0.488117, acc.: 78.91%] [G loss: 3.956593]\n",
      "epoch:40 step:31438 [D loss: 0.257313, acc.: 88.28%] [G loss: 4.720765]\n",
      "epoch:40 step:31439 [D loss: 0.223639, acc.: 90.62%] [G loss: 5.726894]\n",
      "epoch:40 step:31440 [D loss: 0.277645, acc.: 86.72%] [G loss: 5.110568]\n",
      "epoch:40 step:31441 [D loss: 0.220585, acc.: 91.41%] [G loss: 3.515147]\n",
      "epoch:40 step:31442 [D loss: 0.297543, acc.: 87.50%] [G loss: 4.450897]\n",
      "epoch:40 step:31443 [D loss: 0.267678, acc.: 92.19%] [G loss: 4.473652]\n",
      "epoch:40 step:31444 [D loss: 0.239604, acc.: 89.06%] [G loss: 3.286861]\n",
      "epoch:40 step:31445 [D loss: 0.307416, acc.: 85.94%] [G loss: 5.023798]\n",
      "epoch:40 step:31446 [D loss: 0.256636, acc.: 91.41%] [G loss: 2.742057]\n",
      "epoch:40 step:31447 [D loss: 0.322618, acc.: 85.94%] [G loss: 4.046048]\n",
      "epoch:40 step:31448 [D loss: 0.413974, acc.: 82.03%] [G loss: 3.537479]\n",
      "epoch:40 step:31449 [D loss: 0.290360, acc.: 86.72%] [G loss: 4.301217]\n",
      "epoch:40 step:31450 [D loss: 0.223916, acc.: 89.84%] [G loss: 4.174691]\n",
      "epoch:40 step:31451 [D loss: 0.289644, acc.: 87.50%] [G loss: 3.377341]\n",
      "epoch:40 step:31452 [D loss: 0.281511, acc.: 89.06%] [G loss: 3.050524]\n",
      "epoch:40 step:31453 [D loss: 0.322643, acc.: 85.94%] [G loss: 2.880508]\n",
      "epoch:40 step:31454 [D loss: 0.360507, acc.: 84.38%] [G loss: 3.087703]\n",
      "epoch:40 step:31455 [D loss: 0.279434, acc.: 86.72%] [G loss: 3.126190]\n",
      "epoch:40 step:31456 [D loss: 0.454963, acc.: 78.91%] [G loss: 3.287621]\n",
      "epoch:40 step:31457 [D loss: 0.425162, acc.: 76.56%] [G loss: 2.945968]\n",
      "epoch:40 step:31458 [D loss: 0.388193, acc.: 84.38%] [G loss: 2.239261]\n",
      "epoch:40 step:31459 [D loss: 0.229372, acc.: 92.19%] [G loss: 3.146721]\n",
      "epoch:40 step:31460 [D loss: 0.308154, acc.: 86.72%] [G loss: 2.694179]\n",
      "epoch:40 step:31461 [D loss: 0.289929, acc.: 84.38%] [G loss: 3.526752]\n",
      "epoch:40 step:31462 [D loss: 0.301778, acc.: 85.16%] [G loss: 3.614592]\n",
      "epoch:40 step:31463 [D loss: 0.329422, acc.: 84.38%] [G loss: 2.855463]\n",
      "epoch:40 step:31464 [D loss: 0.351581, acc.: 85.16%] [G loss: 3.264184]\n",
      "epoch:40 step:31465 [D loss: 0.376718, acc.: 80.47%] [G loss: 3.597567]\n",
      "epoch:40 step:31466 [D loss: 0.389500, acc.: 77.34%] [G loss: 4.054053]\n",
      "epoch:40 step:31467 [D loss: 0.415248, acc.: 74.22%] [G loss: 3.410631]\n",
      "epoch:40 step:31468 [D loss: 0.404351, acc.: 82.81%] [G loss: 4.314789]\n",
      "epoch:40 step:31469 [D loss: 0.614893, acc.: 71.09%] [G loss: 3.744157]\n",
      "epoch:40 step:31470 [D loss: 0.472315, acc.: 75.00%] [G loss: 3.207827]\n",
      "epoch:40 step:31471 [D loss: 0.266007, acc.: 89.06%] [G loss: 3.030361]\n",
      "epoch:40 step:31472 [D loss: 0.309928, acc.: 86.72%] [G loss: 3.599079]\n",
      "epoch:40 step:31473 [D loss: 0.459692, acc.: 75.78%] [G loss: 2.895424]\n",
      "epoch:40 step:31474 [D loss: 0.325220, acc.: 85.94%] [G loss: 2.409791]\n",
      "epoch:40 step:31475 [D loss: 0.290434, acc.: 86.72%] [G loss: 2.578835]\n",
      "epoch:40 step:31476 [D loss: 0.346254, acc.: 83.59%] [G loss: 3.121555]\n",
      "epoch:40 step:31477 [D loss: 0.345913, acc.: 82.03%] [G loss: 4.753661]\n",
      "epoch:40 step:31478 [D loss: 0.407539, acc.: 81.25%] [G loss: 3.662564]\n",
      "epoch:40 step:31479 [D loss: 0.426354, acc.: 82.03%] [G loss: 3.995322]\n",
      "epoch:40 step:31480 [D loss: 0.499089, acc.: 75.78%] [G loss: 2.669394]\n",
      "epoch:40 step:31481 [D loss: 0.392004, acc.: 77.34%] [G loss: 2.756988]\n",
      "epoch:40 step:31482 [D loss: 0.381102, acc.: 82.03%] [G loss: 2.873897]\n",
      "epoch:40 step:31483 [D loss: 0.309044, acc.: 84.38%] [G loss: 3.792619]\n",
      "epoch:40 step:31484 [D loss: 0.295269, acc.: 87.50%] [G loss: 3.067592]\n",
      "epoch:40 step:31485 [D loss: 0.444033, acc.: 78.12%] [G loss: 3.678973]\n",
      "epoch:40 step:31486 [D loss: 0.283719, acc.: 85.94%] [G loss: 3.405602]\n",
      "epoch:40 step:31487 [D loss: 0.375376, acc.: 83.59%] [G loss: 3.666683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31488 [D loss: 0.287534, acc.: 86.72%] [G loss: 3.400071]\n",
      "epoch:40 step:31489 [D loss: 0.362086, acc.: 82.81%] [G loss: 3.135215]\n",
      "epoch:40 step:31490 [D loss: 0.378980, acc.: 78.12%] [G loss: 3.993579]\n",
      "epoch:40 step:31491 [D loss: 0.271941, acc.: 87.50%] [G loss: 3.173102]\n",
      "epoch:40 step:31492 [D loss: 0.310085, acc.: 85.16%] [G loss: 3.693526]\n",
      "epoch:40 step:31493 [D loss: 0.269914, acc.: 89.06%] [G loss: 4.311513]\n",
      "epoch:40 step:31494 [D loss: 0.314029, acc.: 85.16%] [G loss: 3.751013]\n",
      "epoch:40 step:31495 [D loss: 0.228955, acc.: 90.62%] [G loss: 5.915258]\n",
      "epoch:40 step:31496 [D loss: 0.272648, acc.: 89.06%] [G loss: 3.358963]\n",
      "epoch:40 step:31497 [D loss: 0.362862, acc.: 82.03%] [G loss: 4.627771]\n",
      "epoch:40 step:31498 [D loss: 0.207858, acc.: 91.41%] [G loss: 3.092227]\n",
      "epoch:40 step:31499 [D loss: 0.374451, acc.: 86.72%] [G loss: 4.508538]\n",
      "epoch:40 step:31500 [D loss: 0.497244, acc.: 78.91%] [G loss: 3.005780]\n",
      "epoch:40 step:31501 [D loss: 0.265732, acc.: 89.84%] [G loss: 4.352246]\n",
      "epoch:40 step:31502 [D loss: 0.343057, acc.: 86.72%] [G loss: 3.466793]\n",
      "epoch:40 step:31503 [D loss: 0.326770, acc.: 87.50%] [G loss: 4.931191]\n",
      "epoch:40 step:31504 [D loss: 0.291115, acc.: 87.50%] [G loss: 2.867400]\n",
      "epoch:40 step:31505 [D loss: 0.199114, acc.: 92.97%] [G loss: 3.098062]\n",
      "epoch:40 step:31506 [D loss: 0.336343, acc.: 89.06%] [G loss: 3.541554]\n",
      "epoch:40 step:31507 [D loss: 0.236676, acc.: 89.06%] [G loss: 3.617709]\n",
      "epoch:40 step:31508 [D loss: 0.226946, acc.: 86.72%] [G loss: 3.638497]\n",
      "epoch:40 step:31509 [D loss: 0.333270, acc.: 83.59%] [G loss: 3.149077]\n",
      "epoch:40 step:31510 [D loss: 0.246050, acc.: 88.28%] [G loss: 2.803281]\n",
      "epoch:40 step:31511 [D loss: 0.301262, acc.: 83.59%] [G loss: 3.196837]\n",
      "epoch:40 step:31512 [D loss: 0.257185, acc.: 85.94%] [G loss: 3.206684]\n",
      "epoch:40 step:31513 [D loss: 0.331622, acc.: 85.16%] [G loss: 3.110989]\n",
      "epoch:40 step:31514 [D loss: 0.314023, acc.: 89.06%] [G loss: 2.860462]\n",
      "epoch:40 step:31515 [D loss: 0.350735, acc.: 85.94%] [G loss: 3.328502]\n",
      "epoch:40 step:31516 [D loss: 0.420340, acc.: 80.47%] [G loss: 3.259797]\n",
      "epoch:40 step:31517 [D loss: 0.327893, acc.: 83.59%] [G loss: 3.074775]\n",
      "epoch:40 step:31518 [D loss: 0.251746, acc.: 86.72%] [G loss: 3.676277]\n",
      "epoch:40 step:31519 [D loss: 0.391050, acc.: 83.59%] [G loss: 3.107228]\n",
      "epoch:40 step:31520 [D loss: 0.204411, acc.: 93.75%] [G loss: 3.265391]\n",
      "epoch:40 step:31521 [D loss: 0.287341, acc.: 85.94%] [G loss: 3.181903]\n",
      "epoch:40 step:31522 [D loss: 0.272087, acc.: 89.06%] [G loss: 3.530381]\n",
      "epoch:40 step:31523 [D loss: 0.315638, acc.: 85.16%] [G loss: 3.405410]\n",
      "epoch:40 step:31524 [D loss: 0.357116, acc.: 79.69%] [G loss: 5.954285]\n",
      "epoch:40 step:31525 [D loss: 0.426760, acc.: 78.12%] [G loss: 3.526649]\n",
      "epoch:40 step:31526 [D loss: 0.339879, acc.: 87.50%] [G loss: 3.449329]\n",
      "epoch:40 step:31527 [D loss: 0.330087, acc.: 87.50%] [G loss: 3.377191]\n",
      "epoch:40 step:31528 [D loss: 0.252777, acc.: 89.06%] [G loss: 2.752425]\n",
      "epoch:40 step:31529 [D loss: 0.331925, acc.: 84.38%] [G loss: 3.208804]\n",
      "epoch:40 step:31530 [D loss: 0.291114, acc.: 85.16%] [G loss: 2.944676]\n",
      "epoch:40 step:31531 [D loss: 0.321942, acc.: 88.28%] [G loss: 3.255604]\n",
      "epoch:40 step:31532 [D loss: 0.249139, acc.: 89.84%] [G loss: 3.090064]\n",
      "epoch:40 step:31533 [D loss: 0.378005, acc.: 84.38%] [G loss: 3.213363]\n",
      "epoch:40 step:31534 [D loss: 0.307779, acc.: 87.50%] [G loss: 3.325235]\n",
      "epoch:40 step:31535 [D loss: 0.407976, acc.: 82.03%] [G loss: 2.987468]\n",
      "epoch:40 step:31536 [D loss: 0.317363, acc.: 89.06%] [G loss: 4.105035]\n",
      "epoch:40 step:31537 [D loss: 0.304322, acc.: 86.72%] [G loss: 3.094487]\n",
      "epoch:40 step:31538 [D loss: 0.296172, acc.: 88.28%] [G loss: 3.291075]\n",
      "epoch:40 step:31539 [D loss: 0.313393, acc.: 88.28%] [G loss: 3.236494]\n",
      "epoch:40 step:31540 [D loss: 0.373638, acc.: 85.94%] [G loss: 3.029530]\n",
      "epoch:40 step:31541 [D loss: 0.353436, acc.: 79.69%] [G loss: 3.471322]\n",
      "epoch:40 step:31542 [D loss: 0.263516, acc.: 89.84%] [G loss: 3.778621]\n",
      "epoch:40 step:31543 [D loss: 0.331239, acc.: 82.03%] [G loss: 3.267361]\n",
      "epoch:40 step:31544 [D loss: 0.347745, acc.: 84.38%] [G loss: 3.645665]\n",
      "epoch:40 step:31545 [D loss: 0.315611, acc.: 85.16%] [G loss: 3.212091]\n",
      "epoch:40 step:31546 [D loss: 0.282920, acc.: 88.28%] [G loss: 3.074576]\n",
      "epoch:40 step:31547 [D loss: 0.286295, acc.: 89.84%] [G loss: 2.598243]\n",
      "epoch:40 step:31548 [D loss: 0.229251, acc.: 89.06%] [G loss: 2.967762]\n",
      "epoch:40 step:31549 [D loss: 0.310330, acc.: 85.94%] [G loss: 2.534055]\n",
      "epoch:40 step:31550 [D loss: 0.232846, acc.: 89.06%] [G loss: 3.486782]\n",
      "epoch:40 step:31551 [D loss: 0.326804, acc.: 86.72%] [G loss: 2.090817]\n",
      "epoch:40 step:31552 [D loss: 0.336864, acc.: 85.94%] [G loss: 3.138211]\n",
      "epoch:40 step:31553 [D loss: 0.361676, acc.: 85.16%] [G loss: 3.148824]\n",
      "epoch:40 step:31554 [D loss: 0.318046, acc.: 86.72%] [G loss: 3.157958]\n",
      "epoch:40 step:31555 [D loss: 0.472548, acc.: 75.78%] [G loss: 3.029223]\n",
      "epoch:40 step:31556 [D loss: 0.297749, acc.: 86.72%] [G loss: 3.594773]\n",
      "epoch:40 step:31557 [D loss: 0.489728, acc.: 72.66%] [G loss: 5.445095]\n",
      "epoch:40 step:31558 [D loss: 0.501936, acc.: 82.03%] [G loss: 5.354525]\n",
      "epoch:40 step:31559 [D loss: 0.427074, acc.: 80.47%] [G loss: 3.553430]\n",
      "epoch:40 step:31560 [D loss: 0.430939, acc.: 83.59%] [G loss: 3.336452]\n",
      "epoch:40 step:31561 [D loss: 0.420880, acc.: 78.12%] [G loss: 3.832815]\n",
      "epoch:40 step:31562 [D loss: 0.330867, acc.: 82.03%] [G loss: 4.038863]\n",
      "epoch:40 step:31563 [D loss: 0.307258, acc.: 87.50%] [G loss: 3.023154]\n",
      "epoch:40 step:31564 [D loss: 0.325030, acc.: 82.03%] [G loss: 4.064115]\n",
      "epoch:40 step:31565 [D loss: 0.467998, acc.: 77.34%] [G loss: 3.128173]\n",
      "epoch:40 step:31566 [D loss: 0.447957, acc.: 79.69%] [G loss: 2.757742]\n",
      "epoch:40 step:31567 [D loss: 0.286704, acc.: 87.50%] [G loss: 2.611480]\n",
      "epoch:40 step:31568 [D loss: 0.309808, acc.: 84.38%] [G loss: 2.924286]\n",
      "epoch:40 step:31569 [D loss: 0.362557, acc.: 84.38%] [G loss: 3.574534]\n",
      "epoch:40 step:31570 [D loss: 0.280541, acc.: 89.06%] [G loss: 2.610264]\n",
      "epoch:40 step:31571 [D loss: 0.465928, acc.: 77.34%] [G loss: 3.310009]\n",
      "epoch:40 step:31572 [D loss: 0.396141, acc.: 80.47%] [G loss: 3.361205]\n",
      "epoch:40 step:31573 [D loss: 0.200501, acc.: 92.97%] [G loss: 2.485875]\n",
      "epoch:40 step:31574 [D loss: 0.355873, acc.: 83.59%] [G loss: 2.567575]\n",
      "epoch:40 step:31575 [D loss: 0.284968, acc.: 89.06%] [G loss: 2.797914]\n",
      "epoch:40 step:31576 [D loss: 0.279566, acc.: 84.38%] [G loss: 2.692833]\n",
      "epoch:40 step:31577 [D loss: 0.230160, acc.: 89.84%] [G loss: 2.796280]\n",
      "epoch:40 step:31578 [D loss: 0.242129, acc.: 91.41%] [G loss: 3.664201]\n",
      "epoch:40 step:31579 [D loss: 0.273317, acc.: 88.28%] [G loss: 3.173959]\n",
      "epoch:40 step:31580 [D loss: 0.334226, acc.: 84.38%] [G loss: 3.368968]\n",
      "epoch:40 step:31581 [D loss: 0.358807, acc.: 82.81%] [G loss: 2.681684]\n",
      "epoch:40 step:31582 [D loss: 0.272027, acc.: 89.06%] [G loss: 2.923306]\n",
      "epoch:40 step:31583 [D loss: 0.383134, acc.: 82.81%] [G loss: 3.406767]\n",
      "epoch:40 step:31584 [D loss: 0.332615, acc.: 82.81%] [G loss: 3.134194]\n",
      "epoch:40 step:31585 [D loss: 0.257894, acc.: 86.72%] [G loss: 4.173437]\n",
      "epoch:40 step:31586 [D loss: 0.419477, acc.: 82.03%] [G loss: 3.202347]\n",
      "epoch:40 step:31587 [D loss: 0.300556, acc.: 85.16%] [G loss: 3.525284]\n",
      "epoch:40 step:31588 [D loss: 0.405634, acc.: 82.03%] [G loss: 3.059749]\n",
      "epoch:40 step:31589 [D loss: 0.254194, acc.: 90.62%] [G loss: 3.097996]\n",
      "epoch:40 step:31590 [D loss: 0.327417, acc.: 83.59%] [G loss: 2.768941]\n",
      "epoch:40 step:31591 [D loss: 0.258999, acc.: 89.06%] [G loss: 3.227315]\n",
      "epoch:40 step:31592 [D loss: 0.349040, acc.: 85.94%] [G loss: 3.045052]\n",
      "epoch:40 step:31593 [D loss: 0.459607, acc.: 78.12%] [G loss: 2.637966]\n",
      "epoch:40 step:31594 [D loss: 0.247196, acc.: 89.84%] [G loss: 2.906054]\n",
      "epoch:40 step:31595 [D loss: 0.308996, acc.: 89.06%] [G loss: 2.654129]\n",
      "epoch:40 step:31596 [D loss: 0.389162, acc.: 85.16%] [G loss: 2.246928]\n",
      "epoch:40 step:31597 [D loss: 0.304101, acc.: 86.72%] [G loss: 4.044091]\n",
      "epoch:40 step:31598 [D loss: 0.289058, acc.: 86.72%] [G loss: 4.353770]\n",
      "epoch:40 step:31599 [D loss: 0.247529, acc.: 89.84%] [G loss: 4.068320]\n",
      "epoch:40 step:31600 [D loss: 0.446629, acc.: 77.34%] [G loss: 3.074313]\n",
      "epoch:40 step:31601 [D loss: 0.226087, acc.: 91.41%] [G loss: 4.339221]\n",
      "epoch:40 step:31602 [D loss: 0.314028, acc.: 89.06%] [G loss: 3.117306]\n",
      "epoch:40 step:31603 [D loss: 0.273652, acc.: 86.72%] [G loss: 2.959961]\n",
      "epoch:40 step:31604 [D loss: 0.271764, acc.: 86.72%] [G loss: 2.723141]\n",
      "epoch:40 step:31605 [D loss: 0.251806, acc.: 89.84%] [G loss: 4.572412]\n",
      "epoch:40 step:31606 [D loss: 0.335533, acc.: 82.81%] [G loss: 3.501391]\n",
      "epoch:40 step:31607 [D loss: 0.329046, acc.: 89.06%] [G loss: 4.819321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31608 [D loss: 0.280556, acc.: 89.06%] [G loss: 3.437603]\n",
      "epoch:40 step:31609 [D loss: 0.310581, acc.: 88.28%] [G loss: 3.132754]\n",
      "epoch:40 step:31610 [D loss: 0.265914, acc.: 87.50%] [G loss: 3.075262]\n",
      "epoch:40 step:31611 [D loss: 0.227186, acc.: 91.41%] [G loss: 3.440161]\n",
      "epoch:40 step:31612 [D loss: 0.319247, acc.: 83.59%] [G loss: 2.792693]\n",
      "epoch:40 step:31613 [D loss: 0.333932, acc.: 85.16%] [G loss: 3.879340]\n",
      "epoch:40 step:31614 [D loss: 0.324382, acc.: 87.50%] [G loss: 2.516651]\n",
      "epoch:40 step:31615 [D loss: 0.297700, acc.: 85.94%] [G loss: 2.739099]\n",
      "epoch:40 step:31616 [D loss: 0.306519, acc.: 83.59%] [G loss: 3.119605]\n",
      "epoch:40 step:31617 [D loss: 0.220540, acc.: 90.62%] [G loss: 3.046154]\n",
      "epoch:40 step:31618 [D loss: 0.332889, acc.: 84.38%] [G loss: 3.601243]\n",
      "epoch:40 step:31619 [D loss: 0.266549, acc.: 89.84%] [G loss: 3.601476]\n",
      "epoch:40 step:31620 [D loss: 0.339092, acc.: 84.38%] [G loss: 2.999121]\n",
      "epoch:40 step:31621 [D loss: 0.263215, acc.: 88.28%] [G loss: 3.343326]\n",
      "epoch:40 step:31622 [D loss: 0.354190, acc.: 86.72%] [G loss: 4.862547]\n",
      "epoch:40 step:31623 [D loss: 0.371762, acc.: 83.59%] [G loss: 4.468435]\n",
      "epoch:40 step:31624 [D loss: 0.250880, acc.: 88.28%] [G loss: 3.763767]\n",
      "epoch:40 step:31625 [D loss: 0.346318, acc.: 85.16%] [G loss: 3.616937]\n",
      "epoch:40 step:31626 [D loss: 0.386241, acc.: 81.25%] [G loss: 3.055020]\n",
      "epoch:40 step:31627 [D loss: 0.271516, acc.: 87.50%] [G loss: 2.736564]\n",
      "epoch:40 step:31628 [D loss: 0.248046, acc.: 89.06%] [G loss: 3.173303]\n",
      "epoch:40 step:31629 [D loss: 0.300051, acc.: 89.06%] [G loss: 3.303193]\n",
      "epoch:40 step:31630 [D loss: 0.283949, acc.: 88.28%] [G loss: 3.813242]\n",
      "epoch:40 step:31631 [D loss: 0.431250, acc.: 81.25%] [G loss: 3.613707]\n",
      "epoch:40 step:31632 [D loss: 0.382955, acc.: 78.12%] [G loss: 3.166977]\n",
      "epoch:40 step:31633 [D loss: 0.262422, acc.: 88.28%] [G loss: 3.219134]\n",
      "epoch:40 step:31634 [D loss: 0.408834, acc.: 83.59%] [G loss: 3.589248]\n",
      "epoch:40 step:31635 [D loss: 0.302969, acc.: 87.50%] [G loss: 3.223983]\n",
      "epoch:40 step:31636 [D loss: 0.312069, acc.: 85.16%] [G loss: 3.914069]\n",
      "epoch:40 step:31637 [D loss: 0.396427, acc.: 81.25%] [G loss: 3.277839]\n",
      "epoch:40 step:31638 [D loss: 0.288994, acc.: 85.94%] [G loss: 2.583793]\n",
      "epoch:40 step:31639 [D loss: 0.328449, acc.: 82.81%] [G loss: 3.523007]\n",
      "epoch:40 step:31640 [D loss: 0.396052, acc.: 78.91%] [G loss: 3.153402]\n",
      "epoch:40 step:31641 [D loss: 0.289842, acc.: 85.16%] [G loss: 4.143814]\n",
      "epoch:40 step:31642 [D loss: 0.295208, acc.: 86.72%] [G loss: 3.259768]\n",
      "epoch:40 step:31643 [D loss: 0.228583, acc.: 92.19%] [G loss: 3.916188]\n",
      "epoch:40 step:31644 [D loss: 0.238691, acc.: 90.62%] [G loss: 2.524282]\n",
      "epoch:40 step:31645 [D loss: 0.315020, acc.: 87.50%] [G loss: 3.590359]\n",
      "epoch:40 step:31646 [D loss: 0.328475, acc.: 86.72%] [G loss: 3.137837]\n",
      "epoch:40 step:31647 [D loss: 0.314272, acc.: 85.94%] [G loss: 2.788671]\n",
      "epoch:40 step:31648 [D loss: 0.514954, acc.: 77.34%] [G loss: 2.836510]\n",
      "epoch:40 step:31649 [D loss: 0.455917, acc.: 75.78%] [G loss: 2.918137]\n",
      "epoch:40 step:31650 [D loss: 0.405305, acc.: 81.25%] [G loss: 4.844805]\n",
      "epoch:40 step:31651 [D loss: 0.657502, acc.: 73.44%] [G loss: 10.313993]\n",
      "epoch:40 step:31652 [D loss: 2.726711, acc.: 46.09%] [G loss: 6.013313]\n",
      "epoch:40 step:31653 [D loss: 1.335438, acc.: 71.09%] [G loss: 4.877065]\n",
      "epoch:40 step:31654 [D loss: 0.670422, acc.: 75.78%] [G loss: 4.178650]\n",
      "epoch:40 step:31655 [D loss: 0.821585, acc.: 73.44%] [G loss: 4.109006]\n",
      "epoch:40 step:31656 [D loss: 0.297727, acc.: 88.28%] [G loss: 5.063139]\n",
      "epoch:40 step:31657 [D loss: 0.608578, acc.: 74.22%] [G loss: 3.356345]\n",
      "epoch:40 step:31658 [D loss: 0.415368, acc.: 78.12%] [G loss: 3.233258]\n",
      "epoch:40 step:31659 [D loss: 0.394333, acc.: 84.38%] [G loss: 3.195829]\n",
      "epoch:40 step:31660 [D loss: 0.425538, acc.: 78.91%] [G loss: 2.717677]\n",
      "epoch:40 step:31661 [D loss: 0.283010, acc.: 90.62%] [G loss: 3.158826]\n",
      "epoch:40 step:31662 [D loss: 0.280352, acc.: 85.94%] [G loss: 3.577891]\n",
      "epoch:40 step:31663 [D loss: 0.370646, acc.: 83.59%] [G loss: 2.838960]\n",
      "epoch:40 step:31664 [D loss: 0.395405, acc.: 82.03%] [G loss: 3.061394]\n",
      "epoch:40 step:31665 [D loss: 0.369714, acc.: 81.25%] [G loss: 2.699078]\n",
      "epoch:40 step:31666 [D loss: 0.283881, acc.: 89.06%] [G loss: 2.453957]\n",
      "epoch:40 step:31667 [D loss: 0.375359, acc.: 82.81%] [G loss: 3.147610]\n",
      "epoch:40 step:31668 [D loss: 0.418458, acc.: 80.47%] [G loss: 2.477541]\n",
      "epoch:40 step:31669 [D loss: 0.397435, acc.: 81.25%] [G loss: 2.793421]\n",
      "epoch:40 step:31670 [D loss: 0.378371, acc.: 82.03%] [G loss: 2.637667]\n",
      "epoch:40 step:31671 [D loss: 0.279247, acc.: 89.06%] [G loss: 4.008153]\n",
      "epoch:40 step:31672 [D loss: 0.353604, acc.: 85.94%] [G loss: 4.768525]\n",
      "epoch:40 step:31673 [D loss: 0.242224, acc.: 89.84%] [G loss: 3.431273]\n",
      "epoch:40 step:31674 [D loss: 0.286326, acc.: 87.50%] [G loss: 3.477156]\n",
      "epoch:40 step:31675 [D loss: 0.373776, acc.: 88.28%] [G loss: 2.841950]\n",
      "epoch:40 step:31676 [D loss: 0.416830, acc.: 79.69%] [G loss: 2.537801]\n",
      "epoch:40 step:31677 [D loss: 0.298479, acc.: 85.94%] [G loss: 2.471731]\n",
      "epoch:40 step:31678 [D loss: 0.312281, acc.: 85.94%] [G loss: 2.532723]\n",
      "epoch:40 step:31679 [D loss: 0.318765, acc.: 83.59%] [G loss: 2.644651]\n",
      "epoch:40 step:31680 [D loss: 0.243443, acc.: 91.41%] [G loss: 3.511894]\n",
      "epoch:40 step:31681 [D loss: 0.280519, acc.: 87.50%] [G loss: 3.960773]\n",
      "epoch:40 step:31682 [D loss: 0.277299, acc.: 88.28%] [G loss: 3.769893]\n",
      "epoch:40 step:31683 [D loss: 0.433276, acc.: 78.91%] [G loss: 3.727988]\n",
      "epoch:40 step:31684 [D loss: 0.347964, acc.: 83.59%] [G loss: 3.760292]\n",
      "epoch:40 step:31685 [D loss: 0.194677, acc.: 93.75%] [G loss: 3.855689]\n",
      "epoch:40 step:31686 [D loss: 0.257265, acc.: 89.84%] [G loss: 3.583620]\n",
      "epoch:40 step:31687 [D loss: 0.286894, acc.: 86.72%] [G loss: 2.905469]\n",
      "epoch:40 step:31688 [D loss: 0.300057, acc.: 86.72%] [G loss: 2.809002]\n",
      "epoch:40 step:31689 [D loss: 0.297515, acc.: 89.06%] [G loss: 2.755733]\n",
      "epoch:40 step:31690 [D loss: 0.312920, acc.: 86.72%] [G loss: 2.935773]\n",
      "epoch:40 step:31691 [D loss: 0.237573, acc.: 91.41%] [G loss: 2.844776]\n",
      "epoch:40 step:31692 [D loss: 0.334877, acc.: 84.38%] [G loss: 3.249941]\n",
      "epoch:40 step:31693 [D loss: 0.195421, acc.: 92.97%] [G loss: 2.989042]\n",
      "epoch:40 step:31694 [D loss: 0.297940, acc.: 89.06%] [G loss: 4.021345]\n",
      "epoch:40 step:31695 [D loss: 0.291511, acc.: 88.28%] [G loss: 2.682717]\n",
      "epoch:40 step:31696 [D loss: 0.326550, acc.: 82.03%] [G loss: 2.724057]\n",
      "epoch:40 step:31697 [D loss: 0.291765, acc.: 87.50%] [G loss: 2.907657]\n",
      "epoch:40 step:31698 [D loss: 0.264663, acc.: 89.84%] [G loss: 2.827837]\n",
      "epoch:40 step:31699 [D loss: 0.345497, acc.: 85.94%] [G loss: 2.677489]\n",
      "epoch:40 step:31700 [D loss: 0.354163, acc.: 78.91%] [G loss: 3.153314]\n",
      "epoch:40 step:31701 [D loss: 0.259135, acc.: 91.41%] [G loss: 3.297117]\n",
      "epoch:40 step:31702 [D loss: 0.346460, acc.: 86.72%] [G loss: 3.282988]\n",
      "epoch:40 step:31703 [D loss: 0.282527, acc.: 86.72%] [G loss: 3.038713]\n",
      "epoch:40 step:31704 [D loss: 0.336232, acc.: 85.16%] [G loss: 2.575425]\n",
      "epoch:40 step:31705 [D loss: 0.322875, acc.: 85.16%] [G loss: 2.343547]\n",
      "epoch:40 step:31706 [D loss: 0.315025, acc.: 86.72%] [G loss: 2.819388]\n",
      "epoch:40 step:31707 [D loss: 0.343318, acc.: 85.16%] [G loss: 2.999902]\n",
      "epoch:40 step:31708 [D loss: 0.488741, acc.: 77.34%] [G loss: 2.969748]\n",
      "epoch:40 step:31709 [D loss: 0.348436, acc.: 82.03%] [G loss: 2.650342]\n",
      "epoch:40 step:31710 [D loss: 0.316497, acc.: 89.06%] [G loss: 3.554880]\n",
      "epoch:40 step:31711 [D loss: 0.321609, acc.: 87.50%] [G loss: 2.725153]\n",
      "epoch:40 step:31712 [D loss: 0.352558, acc.: 82.03%] [G loss: 2.421162]\n",
      "epoch:40 step:31713 [D loss: 0.388304, acc.: 86.72%] [G loss: 2.736043]\n",
      "epoch:40 step:31714 [D loss: 0.372461, acc.: 82.81%] [G loss: 2.935857]\n",
      "epoch:40 step:31715 [D loss: 0.350805, acc.: 82.03%] [G loss: 2.311099]\n",
      "epoch:40 step:31716 [D loss: 0.316197, acc.: 85.94%] [G loss: 2.980509]\n",
      "epoch:40 step:31717 [D loss: 0.412134, acc.: 84.38%] [G loss: 2.654122]\n",
      "epoch:40 step:31718 [D loss: 0.326889, acc.: 87.50%] [G loss: 2.762592]\n",
      "epoch:40 step:31719 [D loss: 0.301665, acc.: 86.72%] [G loss: 2.675160]\n",
      "epoch:40 step:31720 [D loss: 0.312186, acc.: 84.38%] [G loss: 2.552255]\n",
      "epoch:40 step:31721 [D loss: 0.302793, acc.: 85.16%] [G loss: 3.207789]\n",
      "epoch:40 step:31722 [D loss: 0.357893, acc.: 83.59%] [G loss: 3.689725]\n",
      "epoch:40 step:31723 [D loss: 0.311119, acc.: 87.50%] [G loss: 3.103709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31724 [D loss: 0.307954, acc.: 86.72%] [G loss: 3.987480]\n",
      "epoch:40 step:31725 [D loss: 0.309537, acc.: 82.81%] [G loss: 3.008013]\n",
      "epoch:40 step:31726 [D loss: 0.268253, acc.: 87.50%] [G loss: 3.734481]\n",
      "epoch:40 step:31727 [D loss: 0.275044, acc.: 89.06%] [G loss: 2.825695]\n",
      "epoch:40 step:31728 [D loss: 0.318333, acc.: 85.94%] [G loss: 3.261836]\n",
      "epoch:40 step:31729 [D loss: 0.376047, acc.: 81.25%] [G loss: 3.312344]\n",
      "epoch:40 step:31730 [D loss: 0.248019, acc.: 89.84%] [G loss: 2.778214]\n",
      "epoch:40 step:31731 [D loss: 0.349077, acc.: 81.25%] [G loss: 3.383803]\n",
      "epoch:40 step:31732 [D loss: 0.325882, acc.: 83.59%] [G loss: 3.656432]\n",
      "epoch:40 step:31733 [D loss: 0.289781, acc.: 84.38%] [G loss: 3.297659]\n",
      "epoch:40 step:31734 [D loss: 0.262054, acc.: 89.06%] [G loss: 3.260931]\n",
      "epoch:40 step:31735 [D loss: 0.310655, acc.: 85.16%] [G loss: 2.918264]\n",
      "epoch:40 step:31736 [D loss: 0.307029, acc.: 89.06%] [G loss: 2.740674]\n",
      "epoch:40 step:31737 [D loss: 0.388272, acc.: 84.38%] [G loss: 2.710436]\n",
      "epoch:40 step:31738 [D loss: 0.218083, acc.: 89.84%] [G loss: 3.194939]\n",
      "epoch:40 step:31739 [D loss: 0.441441, acc.: 81.25%] [G loss: 2.545289]\n",
      "epoch:40 step:31740 [D loss: 0.370861, acc.: 85.16%] [G loss: 2.647892]\n",
      "epoch:40 step:31741 [D loss: 0.464884, acc.: 78.12%] [G loss: 2.827660]\n",
      "epoch:40 step:31742 [D loss: 0.383029, acc.: 78.12%] [G loss: 2.757795]\n",
      "epoch:40 step:31743 [D loss: 0.344458, acc.: 84.38%] [G loss: 3.200770]\n",
      "epoch:40 step:31744 [D loss: 0.316002, acc.: 89.06%] [G loss: 2.715490]\n",
      "epoch:40 step:31745 [D loss: 0.273017, acc.: 89.06%] [G loss: 3.916325]\n",
      "epoch:40 step:31746 [D loss: 0.409716, acc.: 80.47%] [G loss: 2.879386]\n",
      "epoch:40 step:31747 [D loss: 0.296238, acc.: 85.16%] [G loss: 3.299292]\n",
      "epoch:40 step:31748 [D loss: 0.260425, acc.: 89.06%] [G loss: 3.808885]\n",
      "epoch:40 step:31749 [D loss: 0.274421, acc.: 86.72%] [G loss: 3.540545]\n",
      "epoch:40 step:31750 [D loss: 0.258185, acc.: 91.41%] [G loss: 3.455544]\n",
      "epoch:40 step:31751 [D loss: 0.334831, acc.: 87.50%] [G loss: 2.802418]\n",
      "epoch:40 step:31752 [D loss: 0.251304, acc.: 90.62%] [G loss: 2.855523]\n",
      "epoch:40 step:31753 [D loss: 0.326673, acc.: 85.94%] [G loss: 3.772566]\n",
      "epoch:40 step:31754 [D loss: 0.410453, acc.: 78.12%] [G loss: 2.926334]\n",
      "epoch:40 step:31755 [D loss: 0.321280, acc.: 82.81%] [G loss: 3.562063]\n",
      "epoch:40 step:31756 [D loss: 0.341489, acc.: 85.94%] [G loss: 3.180752]\n",
      "epoch:40 step:31757 [D loss: 0.280188, acc.: 87.50%] [G loss: 2.896346]\n",
      "epoch:40 step:31758 [D loss: 0.299012, acc.: 85.94%] [G loss: 3.332869]\n",
      "epoch:40 step:31759 [D loss: 0.200021, acc.: 89.84%] [G loss: 3.699306]\n",
      "epoch:40 step:31760 [D loss: 0.336855, acc.: 83.59%] [G loss: 3.810767]\n",
      "epoch:40 step:31761 [D loss: 0.272960, acc.: 85.94%] [G loss: 7.293383]\n",
      "epoch:40 step:31762 [D loss: 0.368317, acc.: 82.03%] [G loss: 2.907215]\n",
      "epoch:40 step:31763 [D loss: 0.344050, acc.: 83.59%] [G loss: 4.860600]\n",
      "epoch:40 step:31764 [D loss: 0.429464, acc.: 81.25%] [G loss: 3.442150]\n",
      "epoch:40 step:31765 [D loss: 0.376429, acc.: 78.91%] [G loss: 3.493919]\n",
      "epoch:40 step:31766 [D loss: 0.300217, acc.: 86.72%] [G loss: 3.040340]\n",
      "epoch:40 step:31767 [D loss: 0.260421, acc.: 87.50%] [G loss: 3.408133]\n",
      "epoch:40 step:31768 [D loss: 0.332531, acc.: 83.59%] [G loss: 3.437798]\n",
      "epoch:40 step:31769 [D loss: 0.373533, acc.: 81.25%] [G loss: 3.131106]\n",
      "epoch:40 step:31770 [D loss: 0.476395, acc.: 82.03%] [G loss: 3.088026]\n",
      "epoch:40 step:31771 [D loss: 0.433761, acc.: 80.47%] [G loss: 2.125401]\n",
      "epoch:40 step:31772 [D loss: 0.402125, acc.: 82.03%] [G loss: 2.891988]\n",
      "epoch:40 step:31773 [D loss: 0.343053, acc.: 82.81%] [G loss: 2.586790]\n",
      "epoch:40 step:31774 [D loss: 0.311717, acc.: 85.16%] [G loss: 3.476293]\n",
      "epoch:40 step:31775 [D loss: 0.346281, acc.: 84.38%] [G loss: 4.000215]\n",
      "epoch:40 step:31776 [D loss: 0.320995, acc.: 85.16%] [G loss: 3.779937]\n",
      "epoch:40 step:31777 [D loss: 0.348745, acc.: 81.25%] [G loss: 4.195627]\n",
      "epoch:40 step:31778 [D loss: 0.390173, acc.: 83.59%] [G loss: 2.786332]\n",
      "epoch:40 step:31779 [D loss: 0.242700, acc.: 89.06%] [G loss: 3.841695]\n",
      "epoch:40 step:31780 [D loss: 0.257994, acc.: 92.19%] [G loss: 3.469642]\n",
      "epoch:40 step:31781 [D loss: 0.356516, acc.: 81.25%] [G loss: 3.034752]\n",
      "epoch:40 step:31782 [D loss: 0.314418, acc.: 89.06%] [G loss: 3.023646]\n",
      "epoch:40 step:31783 [D loss: 0.260591, acc.: 90.62%] [G loss: 3.731049]\n",
      "epoch:40 step:31784 [D loss: 0.323887, acc.: 87.50%] [G loss: 3.928159]\n",
      "epoch:40 step:31785 [D loss: 0.271199, acc.: 86.72%] [G loss: 3.742335]\n",
      "epoch:40 step:31786 [D loss: 0.176335, acc.: 94.53%] [G loss: 3.871256]\n",
      "epoch:40 step:31787 [D loss: 0.327840, acc.: 82.03%] [G loss: 2.821767]\n",
      "epoch:40 step:31788 [D loss: 0.324529, acc.: 87.50%] [G loss: 2.974149]\n",
      "epoch:40 step:31789 [D loss: 0.296340, acc.: 84.38%] [G loss: 2.382391]\n",
      "epoch:40 step:31790 [D loss: 0.320592, acc.: 86.72%] [G loss: 3.635567]\n",
      "epoch:40 step:31791 [D loss: 0.320020, acc.: 87.50%] [G loss: 2.626138]\n",
      "epoch:40 step:31792 [D loss: 0.400038, acc.: 80.47%] [G loss: 5.781562]\n",
      "epoch:40 step:31793 [D loss: 0.641756, acc.: 75.78%] [G loss: 2.648402]\n",
      "epoch:40 step:31794 [D loss: 0.334722, acc.: 85.16%] [G loss: 4.261439]\n",
      "epoch:40 step:31795 [D loss: 0.512229, acc.: 73.44%] [G loss: 5.630644]\n",
      "epoch:40 step:31796 [D loss: 0.645586, acc.: 77.34%] [G loss: 4.915300]\n",
      "epoch:40 step:31797 [D loss: 0.497179, acc.: 75.78%] [G loss: 2.833691]\n",
      "epoch:40 step:31798 [D loss: 0.349012, acc.: 83.59%] [G loss: 3.409901]\n",
      "epoch:40 step:31799 [D loss: 0.396642, acc.: 80.47%] [G loss: 3.258090]\n",
      "epoch:40 step:31800 [D loss: 0.341182, acc.: 86.72%] [G loss: 3.472088]\n",
      "epoch:40 step:31801 [D loss: 0.234594, acc.: 89.84%] [G loss: 3.666589]\n",
      "epoch:40 step:31802 [D loss: 0.381429, acc.: 82.03%] [G loss: 3.206789]\n",
      "epoch:40 step:31803 [D loss: 0.273148, acc.: 89.06%] [G loss: 2.923046]\n",
      "epoch:40 step:31804 [D loss: 0.381710, acc.: 79.69%] [G loss: 2.531601]\n",
      "epoch:40 step:31805 [D loss: 0.315092, acc.: 85.16%] [G loss: 2.543126]\n",
      "epoch:40 step:31806 [D loss: 0.398067, acc.: 78.12%] [G loss: 3.026854]\n",
      "epoch:40 step:31807 [D loss: 0.288233, acc.: 86.72%] [G loss: 3.180966]\n",
      "epoch:40 step:31808 [D loss: 0.412094, acc.: 80.47%] [G loss: 3.219933]\n",
      "epoch:40 step:31809 [D loss: 0.345064, acc.: 85.16%] [G loss: 3.488037]\n",
      "epoch:40 step:31810 [D loss: 0.339960, acc.: 85.94%] [G loss: 3.378667]\n",
      "epoch:40 step:31811 [D loss: 0.405331, acc.: 81.25%] [G loss: 4.135567]\n",
      "epoch:40 step:31812 [D loss: 0.385095, acc.: 83.59%] [G loss: 3.025341]\n",
      "epoch:40 step:31813 [D loss: 0.269088, acc.: 86.72%] [G loss: 3.308512]\n",
      "epoch:40 step:31814 [D loss: 0.368732, acc.: 81.25%] [G loss: 3.697186]\n",
      "epoch:40 step:31815 [D loss: 0.443124, acc.: 79.69%] [G loss: 3.522482]\n",
      "epoch:40 step:31816 [D loss: 0.265185, acc.: 91.41%] [G loss: 3.348353]\n",
      "epoch:40 step:31817 [D loss: 0.180062, acc.: 95.31%] [G loss: 3.433650]\n",
      "epoch:40 step:31818 [D loss: 0.253526, acc.: 90.62%] [G loss: 2.973413]\n",
      "epoch:40 step:31819 [D loss: 0.217160, acc.: 89.84%] [G loss: 3.510882]\n",
      "epoch:40 step:31820 [D loss: 0.343814, acc.: 84.38%] [G loss: 3.531654]\n",
      "epoch:40 step:31821 [D loss: 0.370798, acc.: 78.12%] [G loss: 3.038945]\n",
      "epoch:40 step:31822 [D loss: 0.256069, acc.: 92.19%] [G loss: 3.698727]\n",
      "epoch:40 step:31823 [D loss: 0.398783, acc.: 80.47%] [G loss: 2.493172]\n",
      "epoch:40 step:31824 [D loss: 0.291609, acc.: 83.59%] [G loss: 2.911070]\n",
      "epoch:40 step:31825 [D loss: 0.207241, acc.: 92.97%] [G loss: 3.290765]\n",
      "epoch:40 step:31826 [D loss: 0.430794, acc.: 75.00%] [G loss: 2.856314]\n",
      "epoch:40 step:31827 [D loss: 0.260987, acc.: 89.06%] [G loss: 3.519311]\n",
      "epoch:40 step:31828 [D loss: 0.202683, acc.: 90.62%] [G loss: 3.536208]\n",
      "epoch:40 step:31829 [D loss: 0.325397, acc.: 83.59%] [G loss: 3.911665]\n",
      "epoch:40 step:31830 [D loss: 0.314446, acc.: 85.94%] [G loss: 4.781345]\n",
      "epoch:40 step:31831 [D loss: 0.277841, acc.: 86.72%] [G loss: 3.175652]\n",
      "epoch:40 step:31832 [D loss: 0.298861, acc.: 86.72%] [G loss: 4.366138]\n",
      "epoch:40 step:31833 [D loss: 0.238382, acc.: 92.19%] [G loss: 3.548071]\n",
      "epoch:40 step:31834 [D loss: 0.388403, acc.: 81.25%] [G loss: 3.057473]\n",
      "epoch:40 step:31835 [D loss: 0.308816, acc.: 87.50%] [G loss: 2.757567]\n",
      "epoch:40 step:31836 [D loss: 0.353142, acc.: 84.38%] [G loss: 3.969532]\n",
      "epoch:40 step:31837 [D loss: 0.331998, acc.: 85.16%] [G loss: 3.490657]\n",
      "epoch:40 step:31838 [D loss: 0.279299, acc.: 89.84%] [G loss: 2.376753]\n",
      "epoch:40 step:31839 [D loss: 0.248463, acc.: 89.84%] [G loss: 3.281143]\n",
      "epoch:40 step:31840 [D loss: 0.325425, acc.: 86.72%] [G loss: 4.603557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31841 [D loss: 0.421144, acc.: 82.81%] [G loss: 3.188002]\n",
      "epoch:40 step:31842 [D loss: 0.288429, acc.: 85.94%] [G loss: 3.211187]\n",
      "epoch:40 step:31843 [D loss: 0.286832, acc.: 89.06%] [G loss: 3.651348]\n",
      "epoch:40 step:31844 [D loss: 0.368202, acc.: 86.72%] [G loss: 2.824256]\n",
      "epoch:40 step:31845 [D loss: 0.253336, acc.: 88.28%] [G loss: 3.778239]\n",
      "epoch:40 step:31846 [D loss: 0.346439, acc.: 82.03%] [G loss: 3.127642]\n",
      "epoch:40 step:31847 [D loss: 0.269753, acc.: 86.72%] [G loss: 3.524055]\n",
      "epoch:40 step:31848 [D loss: 0.303621, acc.: 84.38%] [G loss: 3.450161]\n",
      "epoch:40 step:31849 [D loss: 0.221342, acc.: 89.84%] [G loss: 3.511388]\n",
      "epoch:40 step:31850 [D loss: 0.335906, acc.: 86.72%] [G loss: 2.861235]\n",
      "epoch:40 step:31851 [D loss: 0.261990, acc.: 89.06%] [G loss: 2.012020]\n",
      "epoch:40 step:31852 [D loss: 0.295426, acc.: 85.16%] [G loss: 2.460432]\n",
      "epoch:40 step:31853 [D loss: 0.327074, acc.: 87.50%] [G loss: 3.161160]\n",
      "epoch:40 step:31854 [D loss: 0.260876, acc.: 88.28%] [G loss: 2.567023]\n",
      "epoch:40 step:31855 [D loss: 0.348896, acc.: 82.81%] [G loss: 3.299015]\n",
      "epoch:40 step:31856 [D loss: 0.355286, acc.: 85.16%] [G loss: 3.850748]\n",
      "epoch:40 step:31857 [D loss: 0.365136, acc.: 83.59%] [G loss: 3.274780]\n",
      "epoch:40 step:31858 [D loss: 0.451648, acc.: 78.12%] [G loss: 2.627918]\n",
      "epoch:40 step:31859 [D loss: 0.243726, acc.: 92.97%] [G loss: 2.991150]\n",
      "epoch:40 step:31860 [D loss: 0.321426, acc.: 87.50%] [G loss: 3.333458]\n",
      "epoch:40 step:31861 [D loss: 0.264400, acc.: 89.84%] [G loss: 3.766305]\n",
      "epoch:40 step:31862 [D loss: 0.301989, acc.: 84.38%] [G loss: 3.387137]\n",
      "epoch:40 step:31863 [D loss: 0.408713, acc.: 81.25%] [G loss: 3.508008]\n",
      "epoch:40 step:31864 [D loss: 0.409112, acc.: 80.47%] [G loss: 4.343452]\n",
      "epoch:40 step:31865 [D loss: 0.396161, acc.: 84.38%] [G loss: 2.656477]\n",
      "epoch:40 step:31866 [D loss: 0.357017, acc.: 86.72%] [G loss: 3.541113]\n",
      "epoch:40 step:31867 [D loss: 0.304900, acc.: 86.72%] [G loss: 3.403316]\n",
      "epoch:40 step:31868 [D loss: 0.359805, acc.: 82.81%] [G loss: 3.889098]\n",
      "epoch:40 step:31869 [D loss: 0.494170, acc.: 79.69%] [G loss: 5.840507]\n",
      "epoch:40 step:31870 [D loss: 0.500920, acc.: 75.78%] [G loss: 2.795257]\n",
      "epoch:40 step:31871 [D loss: 0.279549, acc.: 88.28%] [G loss: 4.471851]\n",
      "epoch:40 step:31872 [D loss: 0.319240, acc.: 87.50%] [G loss: 3.417784]\n",
      "epoch:40 step:31873 [D loss: 0.276448, acc.: 87.50%] [G loss: 3.556116]\n",
      "epoch:40 step:31874 [D loss: 0.389241, acc.: 81.25%] [G loss: 3.236606]\n",
      "epoch:40 step:31875 [D loss: 0.329968, acc.: 83.59%] [G loss: 3.124603]\n",
      "epoch:40 step:31876 [D loss: 0.280080, acc.: 85.94%] [G loss: 2.873852]\n",
      "epoch:40 step:31877 [D loss: 0.301457, acc.: 85.16%] [G loss: 3.147742]\n",
      "epoch:40 step:31878 [D loss: 0.363603, acc.: 85.16%] [G loss: 2.756300]\n",
      "epoch:40 step:31879 [D loss: 0.323693, acc.: 82.03%] [G loss: 3.093368]\n",
      "epoch:40 step:31880 [D loss: 0.315605, acc.: 86.72%] [G loss: 3.245954]\n",
      "epoch:40 step:31881 [D loss: 0.179691, acc.: 94.53%] [G loss: 3.020136]\n",
      "epoch:40 step:31882 [D loss: 0.310176, acc.: 83.59%] [G loss: 3.165133]\n",
      "epoch:40 step:31883 [D loss: 0.361511, acc.: 87.50%] [G loss: 3.264156]\n",
      "epoch:40 step:31884 [D loss: 0.223512, acc.: 90.62%] [G loss: 3.022087]\n",
      "epoch:40 step:31885 [D loss: 0.243925, acc.: 91.41%] [G loss: 3.565416]\n",
      "epoch:40 step:31886 [D loss: 0.404971, acc.: 78.12%] [G loss: 3.349748]\n",
      "epoch:40 step:31887 [D loss: 0.261215, acc.: 89.06%] [G loss: 3.193321]\n",
      "epoch:40 step:31888 [D loss: 0.320938, acc.: 87.50%] [G loss: 3.141654]\n",
      "epoch:40 step:31889 [D loss: 0.393743, acc.: 82.03%] [G loss: 3.050871]\n",
      "epoch:40 step:31890 [D loss: 0.331477, acc.: 86.72%] [G loss: 3.240314]\n",
      "epoch:40 step:31891 [D loss: 0.358619, acc.: 84.38%] [G loss: 3.168560]\n",
      "epoch:40 step:31892 [D loss: 0.285055, acc.: 89.06%] [G loss: 2.598239]\n",
      "epoch:40 step:31893 [D loss: 0.317346, acc.: 85.16%] [G loss: 3.182549]\n",
      "epoch:40 step:31894 [D loss: 0.317684, acc.: 86.72%] [G loss: 3.031524]\n",
      "epoch:40 step:31895 [D loss: 0.359979, acc.: 82.81%] [G loss: 3.074662]\n",
      "epoch:40 step:31896 [D loss: 0.351167, acc.: 82.81%] [G loss: 3.242120]\n",
      "epoch:40 step:31897 [D loss: 0.447889, acc.: 75.78%] [G loss: 2.607898]\n",
      "epoch:40 step:31898 [D loss: 0.270989, acc.: 91.41%] [G loss: 2.908267]\n",
      "epoch:40 step:31899 [D loss: 0.422449, acc.: 77.34%] [G loss: 2.786340]\n",
      "epoch:40 step:31900 [D loss: 0.417592, acc.: 80.47%] [G loss: 3.028489]\n",
      "epoch:40 step:31901 [D loss: 0.356363, acc.: 82.81%] [G loss: 3.388341]\n",
      "epoch:40 step:31902 [D loss: 0.376610, acc.: 83.59%] [G loss: 3.683273]\n",
      "epoch:40 step:31903 [D loss: 0.291284, acc.: 85.94%] [G loss: 4.748106]\n",
      "epoch:40 step:31904 [D loss: 0.356364, acc.: 84.38%] [G loss: 4.288939]\n",
      "epoch:40 step:31905 [D loss: 0.306194, acc.: 85.94%] [G loss: 2.841741]\n",
      "epoch:40 step:31906 [D loss: 0.201653, acc.: 91.41%] [G loss: 4.109008]\n",
      "epoch:40 step:31907 [D loss: 0.255237, acc.: 89.84%] [G loss: 4.014133]\n",
      "epoch:40 step:31908 [D loss: 0.279012, acc.: 87.50%] [G loss: 4.922153]\n",
      "epoch:40 step:31909 [D loss: 0.283748, acc.: 85.94%] [G loss: 3.122558]\n",
      "epoch:40 step:31910 [D loss: 0.306652, acc.: 87.50%] [G loss: 7.035040]\n",
      "epoch:40 step:31911 [D loss: 0.280640, acc.: 86.72%] [G loss: 3.387208]\n",
      "epoch:40 step:31912 [D loss: 0.286208, acc.: 87.50%] [G loss: 6.451685]\n",
      "epoch:40 step:31913 [D loss: 0.276701, acc.: 85.16%] [G loss: 4.286781]\n",
      "epoch:40 step:31914 [D loss: 0.247750, acc.: 90.62%] [G loss: 4.669677]\n",
      "epoch:40 step:31915 [D loss: 0.350998, acc.: 82.81%] [G loss: 6.937470]\n",
      "epoch:40 step:31916 [D loss: 0.534998, acc.: 78.12%] [G loss: 5.984547]\n",
      "epoch:40 step:31917 [D loss: 0.329402, acc.: 82.81%] [G loss: 5.453355]\n",
      "epoch:40 step:31918 [D loss: 0.282785, acc.: 87.50%] [G loss: 4.475629]\n",
      "epoch:40 step:31919 [D loss: 0.304858, acc.: 83.59%] [G loss: 4.549993]\n",
      "epoch:40 step:31920 [D loss: 0.275136, acc.: 85.16%] [G loss: 3.290307]\n",
      "epoch:40 step:31921 [D loss: 0.359626, acc.: 82.81%] [G loss: 3.334949]\n",
      "epoch:40 step:31922 [D loss: 0.460593, acc.: 81.25%] [G loss: 3.975852]\n",
      "epoch:40 step:31923 [D loss: 0.422825, acc.: 80.47%] [G loss: 5.693681]\n",
      "epoch:40 step:31924 [D loss: 0.714355, acc.: 71.88%] [G loss: 7.491065]\n",
      "epoch:40 step:31925 [D loss: 1.039399, acc.: 68.75%] [G loss: 4.921748]\n",
      "epoch:40 step:31926 [D loss: 0.286260, acc.: 86.72%] [G loss: 3.974427]\n",
      "epoch:40 step:31927 [D loss: 0.318014, acc.: 88.28%] [G loss: 4.528539]\n",
      "epoch:40 step:31928 [D loss: 0.354376, acc.: 85.16%] [G loss: 3.597178]\n",
      "epoch:40 step:31929 [D loss: 0.235716, acc.: 92.19%] [G loss: 5.610206]\n",
      "epoch:40 step:31930 [D loss: 0.284864, acc.: 89.06%] [G loss: 3.918711]\n",
      "epoch:40 step:31931 [D loss: 0.281141, acc.: 88.28%] [G loss: 5.462410]\n",
      "epoch:40 step:31932 [D loss: 0.307314, acc.: 89.06%] [G loss: 5.434328]\n",
      "epoch:40 step:31933 [D loss: 0.225247, acc.: 92.19%] [G loss: 5.067838]\n",
      "epoch:40 step:31934 [D loss: 0.344676, acc.: 89.06%] [G loss: 4.433639]\n",
      "epoch:40 step:31935 [D loss: 0.228440, acc.: 92.19%] [G loss: 4.895082]\n",
      "epoch:40 step:31936 [D loss: 0.317648, acc.: 85.16%] [G loss: 4.394998]\n",
      "epoch:40 step:31937 [D loss: 0.326392, acc.: 85.94%] [G loss: 3.513774]\n",
      "epoch:40 step:31938 [D loss: 0.279874, acc.: 88.28%] [G loss: 3.657309]\n",
      "epoch:40 step:31939 [D loss: 0.296227, acc.: 85.16%] [G loss: 2.547056]\n",
      "epoch:40 step:31940 [D loss: 0.285289, acc.: 86.72%] [G loss: 2.549631]\n",
      "epoch:40 step:31941 [D loss: 0.310318, acc.: 85.16%] [G loss: 2.613172]\n",
      "epoch:40 step:31942 [D loss: 0.332787, acc.: 91.41%] [G loss: 2.744932]\n",
      "epoch:40 step:31943 [D loss: 0.423654, acc.: 79.69%] [G loss: 3.284456]\n",
      "epoch:40 step:31944 [D loss: 0.378631, acc.: 83.59%] [G loss: 3.427234]\n",
      "epoch:40 step:31945 [D loss: 0.333311, acc.: 85.94%] [G loss: 3.129709]\n",
      "epoch:40 step:31946 [D loss: 0.321262, acc.: 83.59%] [G loss: 2.936301]\n",
      "epoch:40 step:31947 [D loss: 0.402941, acc.: 80.47%] [G loss: 4.119479]\n",
      "epoch:40 step:31948 [D loss: 0.395024, acc.: 79.69%] [G loss: 2.747000]\n",
      "epoch:40 step:31949 [D loss: 0.297426, acc.: 89.84%] [G loss: 3.936536]\n",
      "epoch:40 step:31950 [D loss: 0.283355, acc.: 87.50%] [G loss: 3.079541]\n",
      "epoch:40 step:31951 [D loss: 0.359831, acc.: 80.47%] [G loss: 3.361549]\n",
      "epoch:40 step:31952 [D loss: 0.256110, acc.: 90.62%] [G loss: 4.116848]\n",
      "epoch:40 step:31953 [D loss: 0.280635, acc.: 84.38%] [G loss: 5.074296]\n",
      "epoch:40 step:31954 [D loss: 0.395282, acc.: 77.34%] [G loss: 3.650875]\n",
      "epoch:40 step:31955 [D loss: 0.406195, acc.: 78.91%] [G loss: 3.824136]\n",
      "epoch:40 step:31956 [D loss: 0.225182, acc.: 89.84%] [G loss: 3.457250]\n",
      "epoch:40 step:31957 [D loss: 0.315285, acc.: 83.59%] [G loss: 3.219842]\n",
      "epoch:40 step:31958 [D loss: 0.262407, acc.: 91.41%] [G loss: 2.991049]\n",
      "epoch:40 step:31959 [D loss: 0.333544, acc.: 87.50%] [G loss: 2.663970]\n",
      "epoch:40 step:31960 [D loss: 0.347300, acc.: 81.25%] [G loss: 3.150264]\n",
      "epoch:40 step:31961 [D loss: 0.309393, acc.: 85.16%] [G loss: 3.126007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31962 [D loss: 0.444901, acc.: 82.03%] [G loss: 3.554759]\n",
      "epoch:40 step:31963 [D loss: 0.298614, acc.: 82.03%] [G loss: 3.419694]\n",
      "epoch:40 step:31964 [D loss: 0.243084, acc.: 92.19%] [G loss: 3.091014]\n",
      "epoch:40 step:31965 [D loss: 0.340754, acc.: 86.72%] [G loss: 3.266596]\n",
      "epoch:40 step:31966 [D loss: 0.260807, acc.: 89.06%] [G loss: 2.690935]\n",
      "epoch:40 step:31967 [D loss: 0.289453, acc.: 88.28%] [G loss: 2.870529]\n",
      "epoch:40 step:31968 [D loss: 0.405302, acc.: 82.03%] [G loss: 2.408425]\n",
      "epoch:40 step:31969 [D loss: 0.336845, acc.: 85.16%] [G loss: 2.491283]\n",
      "epoch:40 step:31970 [D loss: 0.221959, acc.: 92.19%] [G loss: 2.793203]\n",
      "epoch:40 step:31971 [D loss: 0.441317, acc.: 80.47%] [G loss: 3.039165]\n",
      "epoch:40 step:31972 [D loss: 0.290740, acc.: 82.81%] [G loss: 3.265764]\n",
      "epoch:40 step:31973 [D loss: 0.273300, acc.: 85.94%] [G loss: 2.774011]\n",
      "epoch:40 step:31974 [D loss: 0.294600, acc.: 88.28%] [G loss: 2.542724]\n",
      "epoch:40 step:31975 [D loss: 0.301107, acc.: 86.72%] [G loss: 3.574020]\n",
      "epoch:40 step:31976 [D loss: 0.247954, acc.: 87.50%] [G loss: 2.904821]\n",
      "epoch:40 step:31977 [D loss: 0.324754, acc.: 86.72%] [G loss: 2.517721]\n",
      "epoch:40 step:31978 [D loss: 0.340012, acc.: 85.94%] [G loss: 2.426398]\n",
      "epoch:40 step:31979 [D loss: 0.387616, acc.: 80.47%] [G loss: 2.682150]\n",
      "epoch:40 step:31980 [D loss: 0.352368, acc.: 82.81%] [G loss: 2.448184]\n",
      "epoch:40 step:31981 [D loss: 0.317775, acc.: 88.28%] [G loss: 2.793784]\n",
      "epoch:40 step:31982 [D loss: 0.425947, acc.: 85.16%] [G loss: 2.463904]\n",
      "epoch:40 step:31983 [D loss: 0.428373, acc.: 80.47%] [G loss: 3.386776]\n",
      "epoch:40 step:31984 [D loss: 0.308337, acc.: 87.50%] [G loss: 2.831710]\n",
      "epoch:40 step:31985 [D loss: 0.323338, acc.: 85.16%] [G loss: 2.993538]\n",
      "epoch:40 step:31986 [D loss: 0.331742, acc.: 85.16%] [G loss: 2.697819]\n",
      "epoch:40 step:31987 [D loss: 0.362601, acc.: 82.03%] [G loss: 3.162133]\n",
      "epoch:40 step:31988 [D loss: 0.324040, acc.: 85.16%] [G loss: 2.869809]\n",
      "epoch:40 step:31989 [D loss: 0.356384, acc.: 85.16%] [G loss: 3.567744]\n",
      "epoch:40 step:31990 [D loss: 0.277635, acc.: 88.28%] [G loss: 3.309750]\n",
      "epoch:40 step:31991 [D loss: 0.260041, acc.: 91.41%] [G loss: 3.221405]\n",
      "epoch:40 step:31992 [D loss: 0.422355, acc.: 78.91%] [G loss: 2.285440]\n",
      "epoch:40 step:31993 [D loss: 0.326138, acc.: 88.28%] [G loss: 2.555567]\n",
      "epoch:40 step:31994 [D loss: 0.293024, acc.: 85.16%] [G loss: 3.846724]\n",
      "epoch:40 step:31995 [D loss: 0.238629, acc.: 87.50%] [G loss: 3.496341]\n",
      "epoch:40 step:31996 [D loss: 0.317105, acc.: 85.94%] [G loss: 3.792500]\n",
      "epoch:40 step:31997 [D loss: 0.272800, acc.: 88.28%] [G loss: 3.719351]\n",
      "epoch:40 step:31998 [D loss: 0.379652, acc.: 84.38%] [G loss: 3.403405]\n",
      "epoch:40 step:31999 [D loss: 0.365984, acc.: 84.38%] [G loss: 2.112624]\n",
      "epoch:40 step:32000 [D loss: 0.369469, acc.: 86.72%] [G loss: 2.200515]\n",
      "epoch:40 step:32001 [D loss: 0.278903, acc.: 85.94%] [G loss: 2.641011]\n",
      "epoch:40 step:32002 [D loss: 0.302474, acc.: 89.06%] [G loss: 2.691366]\n",
      "epoch:40 step:32003 [D loss: 0.348579, acc.: 86.72%] [G loss: 2.747655]\n",
      "epoch:40 step:32004 [D loss: 0.300299, acc.: 85.94%] [G loss: 3.030546]\n",
      "epoch:40 step:32005 [D loss: 0.494616, acc.: 79.69%] [G loss: 5.912308]\n",
      "epoch:40 step:32006 [D loss: 0.460855, acc.: 75.78%] [G loss: 5.198631]\n",
      "epoch:40 step:32007 [D loss: 0.276171, acc.: 86.72%] [G loss: 3.525505]\n",
      "epoch:40 step:32008 [D loss: 0.305987, acc.: 89.84%] [G loss: 2.992497]\n",
      "epoch:40 step:32009 [D loss: 0.332983, acc.: 84.38%] [G loss: 2.545693]\n",
      "epoch:40 step:32010 [D loss: 0.286798, acc.: 85.16%] [G loss: 3.131960]\n",
      "epoch:40 step:32011 [D loss: 0.325483, acc.: 85.16%] [G loss: 2.418655]\n",
      "epoch:40 step:32012 [D loss: 0.430057, acc.: 78.91%] [G loss: 2.566080]\n",
      "epoch:40 step:32013 [D loss: 0.499553, acc.: 78.12%] [G loss: 2.920643]\n",
      "epoch:40 step:32014 [D loss: 0.308819, acc.: 85.16%] [G loss: 4.358960]\n",
      "epoch:40 step:32015 [D loss: 0.335264, acc.: 87.50%] [G loss: 2.914004]\n",
      "epoch:40 step:32016 [D loss: 0.250736, acc.: 87.50%] [G loss: 3.806152]\n",
      "epoch:40 step:32017 [D loss: 0.357632, acc.: 84.38%] [G loss: 3.765069]\n",
      "epoch:40 step:32018 [D loss: 0.308329, acc.: 84.38%] [G loss: 3.918878]\n",
      "epoch:40 step:32019 [D loss: 0.290052, acc.: 87.50%] [G loss: 4.138527]\n",
      "epoch:40 step:32020 [D loss: 0.267687, acc.: 88.28%] [G loss: 4.723342]\n",
      "epoch:40 step:32021 [D loss: 0.265064, acc.: 89.84%] [G loss: 3.999002]\n",
      "epoch:41 step:32022 [D loss: 0.250964, acc.: 88.28%] [G loss: 3.568162]\n",
      "epoch:41 step:32023 [D loss: 0.208108, acc.: 91.41%] [G loss: 3.973952]\n",
      "epoch:41 step:32024 [D loss: 0.346988, acc.: 83.59%] [G loss: 4.747722]\n",
      "epoch:41 step:32025 [D loss: 0.240143, acc.: 90.62%] [G loss: 4.070897]\n",
      "epoch:41 step:32026 [D loss: 0.219172, acc.: 91.41%] [G loss: 3.912574]\n",
      "epoch:41 step:32027 [D loss: 0.348432, acc.: 81.25%] [G loss: 3.339510]\n",
      "epoch:41 step:32028 [D loss: 0.252405, acc.: 89.84%] [G loss: 2.708090]\n",
      "epoch:41 step:32029 [D loss: 0.351285, acc.: 84.38%] [G loss: 3.070060]\n",
      "epoch:41 step:32030 [D loss: 0.283930, acc.: 87.50%] [G loss: 4.100045]\n",
      "epoch:41 step:32031 [D loss: 0.302754, acc.: 89.06%] [G loss: 3.044408]\n",
      "epoch:41 step:32032 [D loss: 0.252663, acc.: 87.50%] [G loss: 3.190498]\n",
      "epoch:41 step:32033 [D loss: 0.225306, acc.: 92.19%] [G loss: 2.614987]\n",
      "epoch:41 step:32034 [D loss: 0.369439, acc.: 83.59%] [G loss: 2.884198]\n",
      "epoch:41 step:32035 [D loss: 0.294866, acc.: 85.94%] [G loss: 2.603478]\n",
      "epoch:41 step:32036 [D loss: 0.407044, acc.: 80.47%] [G loss: 3.727606]\n",
      "epoch:41 step:32037 [D loss: 0.341916, acc.: 84.38%] [G loss: 3.686328]\n",
      "epoch:41 step:32038 [D loss: 0.275355, acc.: 88.28%] [G loss: 3.182980]\n",
      "epoch:41 step:32039 [D loss: 0.329866, acc.: 87.50%] [G loss: 3.717896]\n",
      "epoch:41 step:32040 [D loss: 0.361636, acc.: 82.03%] [G loss: 3.993880]\n",
      "epoch:41 step:32041 [D loss: 0.298118, acc.: 86.72%] [G loss: 4.211820]\n",
      "epoch:41 step:32042 [D loss: 0.349741, acc.: 84.38%] [G loss: 3.629275]\n",
      "epoch:41 step:32043 [D loss: 0.407612, acc.: 82.03%] [G loss: 2.377825]\n",
      "epoch:41 step:32044 [D loss: 0.253787, acc.: 89.06%] [G loss: 3.572859]\n",
      "epoch:41 step:32045 [D loss: 0.211921, acc.: 92.19%] [G loss: 3.195812]\n",
      "epoch:41 step:32046 [D loss: 0.310381, acc.: 86.72%] [G loss: 3.472726]\n",
      "epoch:41 step:32047 [D loss: 0.222486, acc.: 91.41%] [G loss: 3.621432]\n",
      "epoch:41 step:32048 [D loss: 0.283933, acc.: 88.28%] [G loss: 3.029338]\n",
      "epoch:41 step:32049 [D loss: 0.185302, acc.: 95.31%] [G loss: 3.610191]\n",
      "epoch:41 step:32050 [D loss: 0.393958, acc.: 79.69%] [G loss: 2.604582]\n",
      "epoch:41 step:32051 [D loss: 0.184978, acc.: 92.97%] [G loss: 4.216178]\n",
      "epoch:41 step:32052 [D loss: 0.358030, acc.: 82.81%] [G loss: 2.285488]\n",
      "epoch:41 step:32053 [D loss: 0.373311, acc.: 81.25%] [G loss: 2.654993]\n",
      "epoch:41 step:32054 [D loss: 0.231225, acc.: 89.84%] [G loss: 3.024769]\n",
      "epoch:41 step:32055 [D loss: 0.437140, acc.: 80.47%] [G loss: 2.348689]\n",
      "epoch:41 step:32056 [D loss: 0.409468, acc.: 79.69%] [G loss: 3.258303]\n",
      "epoch:41 step:32057 [D loss: 0.233354, acc.: 92.19%] [G loss: 3.323925]\n",
      "epoch:41 step:32058 [D loss: 0.334337, acc.: 85.16%] [G loss: 2.819643]\n",
      "epoch:41 step:32059 [D loss: 0.326185, acc.: 85.16%] [G loss: 3.882938]\n",
      "epoch:41 step:32060 [D loss: 0.335451, acc.: 87.50%] [G loss: 3.221243]\n",
      "epoch:41 step:32061 [D loss: 0.332114, acc.: 87.50%] [G loss: 3.422679]\n",
      "epoch:41 step:32062 [D loss: 0.307520, acc.: 82.81%] [G loss: 3.543204]\n",
      "epoch:41 step:32063 [D loss: 0.268425, acc.: 86.72%] [G loss: 2.660147]\n",
      "epoch:41 step:32064 [D loss: 0.312256, acc.: 83.59%] [G loss: 3.991018]\n",
      "epoch:41 step:32065 [D loss: 0.377100, acc.: 82.03%] [G loss: 3.683622]\n",
      "epoch:41 step:32066 [D loss: 0.391129, acc.: 83.59%] [G loss: 3.614128]\n",
      "epoch:41 step:32067 [D loss: 0.415167, acc.: 80.47%] [G loss: 2.984641]\n",
      "epoch:41 step:32068 [D loss: 0.305908, acc.: 85.94%] [G loss: 3.073977]\n",
      "epoch:41 step:32069 [D loss: 0.359984, acc.: 83.59%] [G loss: 3.307632]\n",
      "epoch:41 step:32070 [D loss: 0.351579, acc.: 83.59%] [G loss: 2.698911]\n",
      "epoch:41 step:32071 [D loss: 0.340638, acc.: 85.94%] [G loss: 3.529922]\n",
      "epoch:41 step:32072 [D loss: 0.324790, acc.: 87.50%] [G loss: 2.884003]\n",
      "epoch:41 step:32073 [D loss: 0.228135, acc.: 89.84%] [G loss: 3.283187]\n",
      "epoch:41 step:32074 [D loss: 0.298272, acc.: 86.72%] [G loss: 2.577740]\n",
      "epoch:41 step:32075 [D loss: 0.297290, acc.: 89.84%] [G loss: 3.661426]\n",
      "epoch:41 step:32076 [D loss: 0.348030, acc.: 81.25%] [G loss: 3.444007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32077 [D loss: 0.265607, acc.: 87.50%] [G loss: 3.563772]\n",
      "epoch:41 step:32078 [D loss: 0.253858, acc.: 90.62%] [G loss: 2.687022]\n",
      "epoch:41 step:32079 [D loss: 0.242813, acc.: 89.84%] [G loss: 3.727168]\n",
      "epoch:41 step:32080 [D loss: 0.313126, acc.: 82.81%] [G loss: 6.444036]\n",
      "epoch:41 step:32081 [D loss: 0.228870, acc.: 92.97%] [G loss: 4.165414]\n",
      "epoch:41 step:32082 [D loss: 0.445671, acc.: 79.69%] [G loss: 3.617885]\n",
      "epoch:41 step:32083 [D loss: 0.289412, acc.: 87.50%] [G loss: 3.062125]\n",
      "epoch:41 step:32084 [D loss: 0.251654, acc.: 87.50%] [G loss: 3.763228]\n",
      "epoch:41 step:32085 [D loss: 0.249862, acc.: 87.50%] [G loss: 3.850689]\n",
      "epoch:41 step:32086 [D loss: 0.371642, acc.: 84.38%] [G loss: 3.558708]\n",
      "epoch:41 step:32087 [D loss: 0.254030, acc.: 89.84%] [G loss: 3.322045]\n",
      "epoch:41 step:32088 [D loss: 0.294056, acc.: 86.72%] [G loss: 3.507111]\n",
      "epoch:41 step:32089 [D loss: 0.318748, acc.: 85.16%] [G loss: 3.926223]\n",
      "epoch:41 step:32090 [D loss: 0.290548, acc.: 88.28%] [G loss: 4.089623]\n",
      "epoch:41 step:32091 [D loss: 0.209027, acc.: 92.19%] [G loss: 3.824456]\n",
      "epoch:41 step:32092 [D loss: 0.279460, acc.: 89.06%] [G loss: 4.540604]\n",
      "epoch:41 step:32093 [D loss: 0.296114, acc.: 85.94%] [G loss: 5.408073]\n",
      "epoch:41 step:32094 [D loss: 0.465551, acc.: 82.81%] [G loss: 6.793344]\n",
      "epoch:41 step:32095 [D loss: 0.578925, acc.: 75.78%] [G loss: 8.996502]\n",
      "epoch:41 step:32096 [D loss: 1.749261, acc.: 58.59%] [G loss: 5.687163]\n",
      "epoch:41 step:32097 [D loss: 0.342975, acc.: 84.38%] [G loss: 6.172219]\n",
      "epoch:41 step:32098 [D loss: 0.460576, acc.: 79.69%] [G loss: 5.425364]\n",
      "epoch:41 step:32099 [D loss: 0.541684, acc.: 78.91%] [G loss: 3.264543]\n",
      "epoch:41 step:32100 [D loss: 0.483543, acc.: 78.91%] [G loss: 8.196356]\n",
      "epoch:41 step:32101 [D loss: 0.872584, acc.: 68.75%] [G loss: 4.812023]\n",
      "epoch:41 step:32102 [D loss: 0.385939, acc.: 82.81%] [G loss: 4.950796]\n",
      "epoch:41 step:32103 [D loss: 0.298188, acc.: 89.06%] [G loss: 3.228659]\n",
      "epoch:41 step:32104 [D loss: 0.288898, acc.: 89.06%] [G loss: 3.689625]\n",
      "epoch:41 step:32105 [D loss: 0.281924, acc.: 87.50%] [G loss: 3.705076]\n",
      "epoch:41 step:32106 [D loss: 0.220187, acc.: 92.19%] [G loss: 3.256021]\n",
      "epoch:41 step:32107 [D loss: 0.344075, acc.: 85.94%] [G loss: 2.507490]\n",
      "epoch:41 step:32108 [D loss: 0.284108, acc.: 89.06%] [G loss: 2.721146]\n",
      "epoch:41 step:32109 [D loss: 0.305447, acc.: 87.50%] [G loss: 3.109602]\n",
      "epoch:41 step:32110 [D loss: 0.392246, acc.: 87.50%] [G loss: 2.592640]\n",
      "epoch:41 step:32111 [D loss: 0.442757, acc.: 82.81%] [G loss: 2.509132]\n",
      "epoch:41 step:32112 [D loss: 0.323871, acc.: 88.28%] [G loss: 2.808414]\n",
      "epoch:41 step:32113 [D loss: 0.346867, acc.: 84.38%] [G loss: 3.065281]\n",
      "epoch:41 step:32114 [D loss: 0.314685, acc.: 84.38%] [G loss: 2.754946]\n",
      "epoch:41 step:32115 [D loss: 0.317521, acc.: 85.16%] [G loss: 3.303095]\n",
      "epoch:41 step:32116 [D loss: 0.403745, acc.: 85.94%] [G loss: 3.020022]\n",
      "epoch:41 step:32117 [D loss: 0.340282, acc.: 84.38%] [G loss: 3.190704]\n",
      "epoch:41 step:32118 [D loss: 0.340805, acc.: 84.38%] [G loss: 3.776080]\n",
      "epoch:41 step:32119 [D loss: 0.450444, acc.: 80.47%] [G loss: 3.236020]\n",
      "epoch:41 step:32120 [D loss: 0.269361, acc.: 85.94%] [G loss: 3.230374]\n",
      "epoch:41 step:32121 [D loss: 0.392379, acc.: 82.81%] [G loss: 2.727173]\n",
      "epoch:41 step:32122 [D loss: 0.349558, acc.: 85.16%] [G loss: 2.919915]\n",
      "epoch:41 step:32123 [D loss: 0.459387, acc.: 76.56%] [G loss: 2.696389]\n",
      "epoch:41 step:32124 [D loss: 0.382278, acc.: 81.25%] [G loss: 2.361759]\n",
      "epoch:41 step:32125 [D loss: 0.338813, acc.: 84.38%] [G loss: 3.257936]\n",
      "epoch:41 step:32126 [D loss: 0.292117, acc.: 89.06%] [G loss: 2.638851]\n",
      "epoch:41 step:32127 [D loss: 0.305738, acc.: 88.28%] [G loss: 2.771573]\n",
      "epoch:41 step:32128 [D loss: 0.250666, acc.: 92.19%] [G loss: 2.841668]\n",
      "epoch:41 step:32129 [D loss: 0.258225, acc.: 89.06%] [G loss: 3.143501]\n",
      "epoch:41 step:32130 [D loss: 0.399854, acc.: 78.12%] [G loss: 2.556781]\n",
      "epoch:41 step:32131 [D loss: 0.316228, acc.: 84.38%] [G loss: 3.692014]\n",
      "epoch:41 step:32132 [D loss: 0.361267, acc.: 83.59%] [G loss: 2.978236]\n",
      "epoch:41 step:32133 [D loss: 0.242447, acc.: 89.06%] [G loss: 3.309174]\n",
      "epoch:41 step:32134 [D loss: 0.307504, acc.: 85.94%] [G loss: 3.088037]\n",
      "epoch:41 step:32135 [D loss: 0.338411, acc.: 85.16%] [G loss: 3.481126]\n",
      "epoch:41 step:32136 [D loss: 0.246880, acc.: 89.84%] [G loss: 3.279625]\n",
      "epoch:41 step:32137 [D loss: 0.386301, acc.: 85.16%] [G loss: 3.691962]\n",
      "epoch:41 step:32138 [D loss: 0.247469, acc.: 90.62%] [G loss: 3.436711]\n",
      "epoch:41 step:32139 [D loss: 0.276063, acc.: 86.72%] [G loss: 3.411757]\n",
      "epoch:41 step:32140 [D loss: 0.278594, acc.: 87.50%] [G loss: 3.152678]\n",
      "epoch:41 step:32141 [D loss: 0.213544, acc.: 89.06%] [G loss: 3.380290]\n",
      "epoch:41 step:32142 [D loss: 0.263638, acc.: 88.28%] [G loss: 3.997287]\n",
      "epoch:41 step:32143 [D loss: 0.308989, acc.: 86.72%] [G loss: 3.401738]\n",
      "epoch:41 step:32144 [D loss: 0.298461, acc.: 85.94%] [G loss: 3.381052]\n",
      "epoch:41 step:32145 [D loss: 0.267795, acc.: 85.94%] [G loss: 3.266014]\n",
      "epoch:41 step:32146 [D loss: 0.295284, acc.: 87.50%] [G loss: 5.012903]\n",
      "epoch:41 step:32147 [D loss: 0.280067, acc.: 85.94%] [G loss: 3.409461]\n",
      "epoch:41 step:32148 [D loss: 0.288425, acc.: 87.50%] [G loss: 3.074984]\n",
      "epoch:41 step:32149 [D loss: 0.292454, acc.: 86.72%] [G loss: 2.604707]\n",
      "epoch:41 step:32150 [D loss: 0.334175, acc.: 83.59%] [G loss: 3.909275]\n",
      "epoch:41 step:32151 [D loss: 0.235060, acc.: 91.41%] [G loss: 3.127591]\n",
      "epoch:41 step:32152 [D loss: 0.300257, acc.: 89.84%] [G loss: 2.764802]\n",
      "epoch:41 step:32153 [D loss: 0.282504, acc.: 90.62%] [G loss: 3.027596]\n",
      "epoch:41 step:32154 [D loss: 0.312639, acc.: 87.50%] [G loss: 2.855288]\n",
      "epoch:41 step:32155 [D loss: 0.384675, acc.: 84.38%] [G loss: 2.875633]\n",
      "epoch:41 step:32156 [D loss: 0.342059, acc.: 83.59%] [G loss: 3.417899]\n",
      "epoch:41 step:32157 [D loss: 0.267548, acc.: 87.50%] [G loss: 3.295413]\n",
      "epoch:41 step:32158 [D loss: 0.312087, acc.: 87.50%] [G loss: 3.449114]\n",
      "epoch:41 step:32159 [D loss: 0.350499, acc.: 82.03%] [G loss: 2.992218]\n",
      "epoch:41 step:32160 [D loss: 0.222557, acc.: 91.41%] [G loss: 3.605066]\n",
      "epoch:41 step:32161 [D loss: 0.283178, acc.: 86.72%] [G loss: 3.218367]\n",
      "epoch:41 step:32162 [D loss: 0.321377, acc.: 84.38%] [G loss: 2.883972]\n",
      "epoch:41 step:32163 [D loss: 0.374130, acc.: 82.81%] [G loss: 2.413823]\n",
      "epoch:41 step:32164 [D loss: 0.295281, acc.: 87.50%] [G loss: 2.602791]\n",
      "epoch:41 step:32165 [D loss: 0.355240, acc.: 82.81%] [G loss: 2.472144]\n",
      "epoch:41 step:32166 [D loss: 0.306389, acc.: 86.72%] [G loss: 2.342906]\n",
      "epoch:41 step:32167 [D loss: 0.321895, acc.: 84.38%] [G loss: 2.551427]\n",
      "epoch:41 step:32168 [D loss: 0.248939, acc.: 90.62%] [G loss: 2.681358]\n",
      "epoch:41 step:32169 [D loss: 0.429979, acc.: 81.25%] [G loss: 3.026717]\n",
      "epoch:41 step:32170 [D loss: 0.300698, acc.: 89.84%] [G loss: 2.682511]\n",
      "epoch:41 step:32171 [D loss: 0.361551, acc.: 81.25%] [G loss: 3.400262]\n",
      "epoch:41 step:32172 [D loss: 0.323947, acc.: 87.50%] [G loss: 2.579391]\n",
      "epoch:41 step:32173 [D loss: 0.371151, acc.: 82.03%] [G loss: 2.819344]\n",
      "epoch:41 step:32174 [D loss: 0.322714, acc.: 88.28%] [G loss: 2.752499]\n",
      "epoch:41 step:32175 [D loss: 0.233455, acc.: 89.84%] [G loss: 2.666825]\n",
      "epoch:41 step:32176 [D loss: 0.269792, acc.: 89.84%] [G loss: 3.791730]\n",
      "epoch:41 step:32177 [D loss: 0.266390, acc.: 87.50%] [G loss: 5.739459]\n",
      "epoch:41 step:32178 [D loss: 0.270099, acc.: 90.62%] [G loss: 4.670419]\n",
      "epoch:41 step:32179 [D loss: 0.356204, acc.: 85.94%] [G loss: 4.484797]\n",
      "epoch:41 step:32180 [D loss: 0.202720, acc.: 90.62%] [G loss: 5.271045]\n",
      "epoch:41 step:32181 [D loss: 0.338627, acc.: 82.81%] [G loss: 3.470817]\n",
      "epoch:41 step:32182 [D loss: 0.310077, acc.: 87.50%] [G loss: 3.464811]\n",
      "epoch:41 step:32183 [D loss: 0.229282, acc.: 89.84%] [G loss: 3.238168]\n",
      "epoch:41 step:32184 [D loss: 0.327902, acc.: 85.16%] [G loss: 6.877828]\n",
      "epoch:41 step:32185 [D loss: 0.434962, acc.: 80.47%] [G loss: 6.298087]\n",
      "epoch:41 step:32186 [D loss: 0.270231, acc.: 86.72%] [G loss: 7.054676]\n",
      "epoch:41 step:32187 [D loss: 0.216493, acc.: 90.62%] [G loss: 5.401170]\n",
      "epoch:41 step:32188 [D loss: 0.240968, acc.: 90.62%] [G loss: 7.332877]\n",
      "epoch:41 step:32189 [D loss: 0.218498, acc.: 89.84%] [G loss: 4.058332]\n",
      "epoch:41 step:32190 [D loss: 0.205112, acc.: 92.97%] [G loss: 4.238111]\n",
      "epoch:41 step:32191 [D loss: 0.202148, acc.: 89.84%] [G loss: 2.828488]\n",
      "epoch:41 step:32192 [D loss: 0.432072, acc.: 79.69%] [G loss: 2.879426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32193 [D loss: 0.234720, acc.: 89.06%] [G loss: 2.643303]\n",
      "epoch:41 step:32194 [D loss: 0.346939, acc.: 85.16%] [G loss: 3.147780]\n",
      "epoch:41 step:32195 [D loss: 0.272784, acc.: 88.28%] [G loss: 3.371860]\n",
      "epoch:41 step:32196 [D loss: 0.273516, acc.: 85.94%] [G loss: 2.677398]\n",
      "epoch:41 step:32197 [D loss: 0.251144, acc.: 89.06%] [G loss: 2.987480]\n",
      "epoch:41 step:32198 [D loss: 0.263588, acc.: 91.41%] [G loss: 2.864409]\n",
      "epoch:41 step:32199 [D loss: 0.322766, acc.: 89.84%] [G loss: 2.943788]\n",
      "epoch:41 step:32200 [D loss: 0.388637, acc.: 80.47%] [G loss: 4.438204]\n",
      "epoch:41 step:32201 [D loss: 0.287008, acc.: 85.16%] [G loss: 2.870149]\n",
      "epoch:41 step:32202 [D loss: 0.212997, acc.: 90.62%] [G loss: 3.187766]\n",
      "epoch:41 step:32203 [D loss: 0.280000, acc.: 89.84%] [G loss: 2.866105]\n",
      "epoch:41 step:32204 [D loss: 0.280417, acc.: 85.94%] [G loss: 3.040179]\n",
      "epoch:41 step:32205 [D loss: 0.296965, acc.: 88.28%] [G loss: 3.892650]\n",
      "epoch:41 step:32206 [D loss: 0.232088, acc.: 88.28%] [G loss: 3.030596]\n",
      "epoch:41 step:32207 [D loss: 0.231923, acc.: 92.19%] [G loss: 3.770215]\n",
      "epoch:41 step:32208 [D loss: 0.237774, acc.: 89.84%] [G loss: 3.046732]\n",
      "epoch:41 step:32209 [D loss: 0.269936, acc.: 89.06%] [G loss: 3.381823]\n",
      "epoch:41 step:32210 [D loss: 0.313761, acc.: 84.38%] [G loss: 2.791811]\n",
      "epoch:41 step:32211 [D loss: 0.264887, acc.: 88.28%] [G loss: 3.720692]\n",
      "epoch:41 step:32212 [D loss: 0.468490, acc.: 76.56%] [G loss: 4.315989]\n",
      "epoch:41 step:32213 [D loss: 0.366142, acc.: 84.38%] [G loss: 3.651211]\n",
      "epoch:41 step:32214 [D loss: 0.505522, acc.: 76.56%] [G loss: 3.975125]\n",
      "epoch:41 step:32215 [D loss: 0.408050, acc.: 78.91%] [G loss: 3.838429]\n",
      "epoch:41 step:32216 [D loss: 0.343556, acc.: 82.03%] [G loss: 6.719332]\n",
      "epoch:41 step:32217 [D loss: 0.240898, acc.: 90.62%] [G loss: 2.497114]\n",
      "epoch:41 step:32218 [D loss: 0.437846, acc.: 81.25%] [G loss: 2.941637]\n",
      "epoch:41 step:32219 [D loss: 0.390732, acc.: 81.25%] [G loss: 3.907285]\n",
      "epoch:41 step:32220 [D loss: 0.281003, acc.: 87.50%] [G loss: 3.708697]\n",
      "epoch:41 step:32221 [D loss: 0.318999, acc.: 86.72%] [G loss: 5.082665]\n",
      "epoch:41 step:32222 [D loss: 0.375196, acc.: 84.38%] [G loss: 4.958544]\n",
      "epoch:41 step:32223 [D loss: 0.212293, acc.: 89.84%] [G loss: 4.233259]\n",
      "epoch:41 step:32224 [D loss: 0.261160, acc.: 89.84%] [G loss: 3.737198]\n",
      "epoch:41 step:32225 [D loss: 0.333306, acc.: 85.94%] [G loss: 3.522726]\n",
      "epoch:41 step:32226 [D loss: 0.231360, acc.: 90.62%] [G loss: 3.266829]\n",
      "epoch:41 step:32227 [D loss: 0.225261, acc.: 90.62%] [G loss: 2.861726]\n",
      "epoch:41 step:32228 [D loss: 0.369177, acc.: 82.03%] [G loss: 3.180807]\n",
      "epoch:41 step:32229 [D loss: 0.404584, acc.: 82.03%] [G loss: 3.220548]\n",
      "epoch:41 step:32230 [D loss: 0.341879, acc.: 85.16%] [G loss: 3.433363]\n",
      "epoch:41 step:32231 [D loss: 0.285806, acc.: 88.28%] [G loss: 3.205124]\n",
      "epoch:41 step:32232 [D loss: 0.304164, acc.: 85.16%] [G loss: 2.623773]\n",
      "epoch:41 step:32233 [D loss: 0.219857, acc.: 92.97%] [G loss: 3.162344]\n",
      "epoch:41 step:32234 [D loss: 0.271943, acc.: 87.50%] [G loss: 2.836651]\n",
      "epoch:41 step:32235 [D loss: 0.374041, acc.: 86.72%] [G loss: 2.184603]\n",
      "epoch:41 step:32236 [D loss: 0.289306, acc.: 85.16%] [G loss: 2.390895]\n",
      "epoch:41 step:32237 [D loss: 0.296514, acc.: 83.59%] [G loss: 2.348465]\n",
      "epoch:41 step:32238 [D loss: 0.367973, acc.: 82.81%] [G loss: 2.329069]\n",
      "epoch:41 step:32239 [D loss: 0.326079, acc.: 84.38%] [G loss: 3.111698]\n",
      "epoch:41 step:32240 [D loss: 0.298912, acc.: 86.72%] [G loss: 2.608334]\n",
      "epoch:41 step:32241 [D loss: 0.266210, acc.: 89.06%] [G loss: 2.848489]\n",
      "epoch:41 step:32242 [D loss: 0.225173, acc.: 91.41%] [G loss: 2.465294]\n",
      "epoch:41 step:32243 [D loss: 0.220225, acc.: 89.84%] [G loss: 2.827008]\n",
      "epoch:41 step:32244 [D loss: 0.316795, acc.: 85.16%] [G loss: 3.594403]\n",
      "epoch:41 step:32245 [D loss: 0.209361, acc.: 91.41%] [G loss: 3.643183]\n",
      "epoch:41 step:32246 [D loss: 0.330036, acc.: 84.38%] [G loss: 2.965071]\n",
      "epoch:41 step:32247 [D loss: 0.279842, acc.: 87.50%] [G loss: 3.270761]\n",
      "epoch:41 step:32248 [D loss: 0.308238, acc.: 85.16%] [G loss: 3.728814]\n",
      "epoch:41 step:32249 [D loss: 0.300032, acc.: 83.59%] [G loss: 2.952110]\n",
      "epoch:41 step:32250 [D loss: 0.364333, acc.: 82.81%] [G loss: 3.954971]\n",
      "epoch:41 step:32251 [D loss: 0.455331, acc.: 79.69%] [G loss: 3.406674]\n",
      "epoch:41 step:32252 [D loss: 0.209132, acc.: 91.41%] [G loss: 2.902321]\n",
      "epoch:41 step:32253 [D loss: 0.226304, acc.: 89.06%] [G loss: 4.295018]\n",
      "epoch:41 step:32254 [D loss: 0.243499, acc.: 90.62%] [G loss: 2.789258]\n",
      "epoch:41 step:32255 [D loss: 0.226232, acc.: 89.06%] [G loss: 2.978885]\n",
      "epoch:41 step:32256 [D loss: 0.281335, acc.: 88.28%] [G loss: 3.065169]\n",
      "epoch:41 step:32257 [D loss: 0.226271, acc.: 89.84%] [G loss: 4.347589]\n",
      "epoch:41 step:32258 [D loss: 0.202412, acc.: 92.19%] [G loss: 3.429469]\n",
      "epoch:41 step:32259 [D loss: 0.358878, acc.: 82.03%] [G loss: 4.739740]\n",
      "epoch:41 step:32260 [D loss: 0.354205, acc.: 85.94%] [G loss: 2.972944]\n",
      "epoch:41 step:32261 [D loss: 0.218600, acc.: 91.41%] [G loss: 3.806089]\n",
      "epoch:41 step:32262 [D loss: 0.351127, acc.: 85.94%] [G loss: 4.273276]\n",
      "epoch:41 step:32263 [D loss: 0.216644, acc.: 92.19%] [G loss: 3.957809]\n",
      "epoch:41 step:32264 [D loss: 0.306893, acc.: 85.94%] [G loss: 3.357129]\n",
      "epoch:41 step:32265 [D loss: 0.258921, acc.: 87.50%] [G loss: 3.022724]\n",
      "epoch:41 step:32266 [D loss: 0.220321, acc.: 91.41%] [G loss: 2.813592]\n",
      "epoch:41 step:32267 [D loss: 0.392322, acc.: 83.59%] [G loss: 2.879751]\n",
      "epoch:41 step:32268 [D loss: 0.370342, acc.: 85.16%] [G loss: 3.224736]\n",
      "epoch:41 step:32269 [D loss: 0.192876, acc.: 92.97%] [G loss: 3.763673]\n",
      "epoch:41 step:32270 [D loss: 0.322085, acc.: 86.72%] [G loss: 2.619330]\n",
      "epoch:41 step:32271 [D loss: 0.479920, acc.: 75.00%] [G loss: 3.246584]\n",
      "epoch:41 step:32272 [D loss: 0.216333, acc.: 91.41%] [G loss: 3.588842]\n",
      "epoch:41 step:32273 [D loss: 0.259949, acc.: 89.06%] [G loss: 4.311667]\n",
      "epoch:41 step:32274 [D loss: 0.305011, acc.: 86.72%] [G loss: 3.148762]\n",
      "epoch:41 step:32275 [D loss: 0.275606, acc.: 89.84%] [G loss: 3.431549]\n",
      "epoch:41 step:32276 [D loss: 0.382674, acc.: 86.72%] [G loss: 3.928526]\n",
      "epoch:41 step:32277 [D loss: 0.270584, acc.: 85.94%] [G loss: 3.617841]\n",
      "epoch:41 step:32278 [D loss: 0.303046, acc.: 84.38%] [G loss: 3.477659]\n",
      "epoch:41 step:32279 [D loss: 0.208487, acc.: 91.41%] [G loss: 3.438665]\n",
      "epoch:41 step:32280 [D loss: 0.285218, acc.: 87.50%] [G loss: 3.018328]\n",
      "epoch:41 step:32281 [D loss: 0.337112, acc.: 82.81%] [G loss: 3.143528]\n",
      "epoch:41 step:32282 [D loss: 0.413290, acc.: 84.38%] [G loss: 2.578871]\n",
      "epoch:41 step:32283 [D loss: 0.367172, acc.: 85.94%] [G loss: 3.724707]\n",
      "epoch:41 step:32284 [D loss: 0.418181, acc.: 83.59%] [G loss: 3.851036]\n",
      "epoch:41 step:32285 [D loss: 0.273806, acc.: 87.50%] [G loss: 5.115211]\n",
      "epoch:41 step:32286 [D loss: 0.316404, acc.: 88.28%] [G loss: 6.802116]\n",
      "epoch:41 step:32287 [D loss: 0.554558, acc.: 75.78%] [G loss: 4.563325]\n",
      "epoch:41 step:32288 [D loss: 0.521822, acc.: 77.34%] [G loss: 7.013335]\n",
      "epoch:41 step:32289 [D loss: 0.594618, acc.: 81.25%] [G loss: 9.036957]\n",
      "epoch:41 step:32290 [D loss: 0.400638, acc.: 83.59%] [G loss: 5.400453]\n",
      "epoch:41 step:32291 [D loss: 0.275030, acc.: 86.72%] [G loss: 3.417403]\n",
      "epoch:41 step:32292 [D loss: 0.222939, acc.: 89.84%] [G loss: 3.544628]\n",
      "epoch:41 step:32293 [D loss: 0.333370, acc.: 86.72%] [G loss: 3.585306]\n",
      "epoch:41 step:32294 [D loss: 0.277592, acc.: 85.94%] [G loss: 3.373445]\n",
      "epoch:41 step:32295 [D loss: 0.308304, acc.: 85.16%] [G loss: 3.399102]\n",
      "epoch:41 step:32296 [D loss: 0.328188, acc.: 85.16%] [G loss: 3.575677]\n",
      "epoch:41 step:32297 [D loss: 0.455486, acc.: 74.22%] [G loss: 2.360943]\n",
      "epoch:41 step:32298 [D loss: 0.362891, acc.: 81.25%] [G loss: 2.886003]\n",
      "epoch:41 step:32299 [D loss: 0.280293, acc.: 87.50%] [G loss: 2.775160]\n",
      "epoch:41 step:32300 [D loss: 0.332735, acc.: 85.94%] [G loss: 4.163662]\n",
      "epoch:41 step:32301 [D loss: 0.229957, acc.: 88.28%] [G loss: 4.189045]\n",
      "epoch:41 step:32302 [D loss: 0.251929, acc.: 89.84%] [G loss: 3.852238]\n",
      "epoch:41 step:32303 [D loss: 0.334390, acc.: 87.50%] [G loss: 3.985370]\n",
      "epoch:41 step:32304 [D loss: 0.294524, acc.: 85.16%] [G loss: 3.092042]\n",
      "epoch:41 step:32305 [D loss: 0.305042, acc.: 85.16%] [G loss: 3.587025]\n",
      "epoch:41 step:32306 [D loss: 0.274139, acc.: 90.62%] [G loss: 2.881622]\n",
      "epoch:41 step:32307 [D loss: 0.277331, acc.: 87.50%] [G loss: 3.756753]\n",
      "epoch:41 step:32308 [D loss: 0.183892, acc.: 93.75%] [G loss: 3.442753]\n",
      "epoch:41 step:32309 [D loss: 0.310089, acc.: 85.94%] [G loss: 3.734275]\n",
      "epoch:41 step:32310 [D loss: 0.237932, acc.: 92.19%] [G loss: 3.691396]\n",
      "epoch:41 step:32311 [D loss: 0.242051, acc.: 91.41%] [G loss: 3.181729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32312 [D loss: 0.314068, acc.: 88.28%] [G loss: 3.457907]\n",
      "epoch:41 step:32313 [D loss: 0.352777, acc.: 87.50%] [G loss: 5.538811]\n",
      "epoch:41 step:32314 [D loss: 0.255238, acc.: 88.28%] [G loss: 2.978749]\n",
      "epoch:41 step:32315 [D loss: 0.274769, acc.: 90.62%] [G loss: 3.729410]\n",
      "epoch:41 step:32316 [D loss: 0.264948, acc.: 86.72%] [G loss: 3.410858]\n",
      "epoch:41 step:32317 [D loss: 0.313108, acc.: 88.28%] [G loss: 3.363880]\n",
      "epoch:41 step:32318 [D loss: 0.247362, acc.: 89.06%] [G loss: 3.505415]\n",
      "epoch:41 step:32319 [D loss: 0.249375, acc.: 88.28%] [G loss: 2.747695]\n",
      "epoch:41 step:32320 [D loss: 0.309594, acc.: 88.28%] [G loss: 3.004784]\n",
      "epoch:41 step:32321 [D loss: 0.349158, acc.: 82.81%] [G loss: 3.156686]\n",
      "epoch:41 step:32322 [D loss: 0.251670, acc.: 88.28%] [G loss: 2.859991]\n",
      "epoch:41 step:32323 [D loss: 0.288320, acc.: 89.84%] [G loss: 2.543055]\n",
      "epoch:41 step:32324 [D loss: 0.260401, acc.: 88.28%] [G loss: 2.980693]\n",
      "epoch:41 step:32325 [D loss: 0.282946, acc.: 87.50%] [G loss: 2.861403]\n",
      "epoch:41 step:32326 [D loss: 0.207134, acc.: 92.19%] [G loss: 3.147058]\n",
      "epoch:41 step:32327 [D loss: 0.304483, acc.: 87.50%] [G loss: 3.229355]\n",
      "epoch:41 step:32328 [D loss: 0.338319, acc.: 85.94%] [G loss: 3.337355]\n",
      "epoch:41 step:32329 [D loss: 0.322911, acc.: 86.72%] [G loss: 2.458177]\n",
      "epoch:41 step:32330 [D loss: 0.244566, acc.: 92.97%] [G loss: 3.455915]\n",
      "epoch:41 step:32331 [D loss: 0.285875, acc.: 83.59%] [G loss: 3.130452]\n",
      "epoch:41 step:32332 [D loss: 0.339802, acc.: 84.38%] [G loss: 3.284759]\n",
      "epoch:41 step:32333 [D loss: 0.273337, acc.: 91.41%] [G loss: 3.715278]\n",
      "epoch:41 step:32334 [D loss: 0.335671, acc.: 81.25%] [G loss: 3.270780]\n",
      "epoch:41 step:32335 [D loss: 0.353798, acc.: 82.81%] [G loss: 3.118299]\n",
      "epoch:41 step:32336 [D loss: 0.339342, acc.: 81.25%] [G loss: 3.654287]\n",
      "epoch:41 step:32337 [D loss: 0.245005, acc.: 91.41%] [G loss: 3.148431]\n",
      "epoch:41 step:32338 [D loss: 0.308753, acc.: 89.06%] [G loss: 3.462072]\n",
      "epoch:41 step:32339 [D loss: 0.381546, acc.: 82.03%] [G loss: 4.164845]\n",
      "epoch:41 step:32340 [D loss: 0.252313, acc.: 85.94%] [G loss: 3.199189]\n",
      "epoch:41 step:32341 [D loss: 0.244051, acc.: 91.41%] [G loss: 3.718002]\n",
      "epoch:41 step:32342 [D loss: 0.349847, acc.: 85.16%] [G loss: 3.896663]\n",
      "epoch:41 step:32343 [D loss: 0.272675, acc.: 87.50%] [G loss: 4.085778]\n",
      "epoch:41 step:32344 [D loss: 0.185250, acc.: 92.19%] [G loss: 5.788159]\n",
      "epoch:41 step:32345 [D loss: 0.243153, acc.: 89.06%] [G loss: 3.457059]\n",
      "epoch:41 step:32346 [D loss: 0.404545, acc.: 78.12%] [G loss: 3.381559]\n",
      "epoch:41 step:32347 [D loss: 0.340472, acc.: 83.59%] [G loss: 4.447904]\n",
      "epoch:41 step:32348 [D loss: 0.259242, acc.: 87.50%] [G loss: 3.156639]\n",
      "epoch:41 step:32349 [D loss: 0.202478, acc.: 92.19%] [G loss: 3.902942]\n",
      "epoch:41 step:32350 [D loss: 0.279126, acc.: 87.50%] [G loss: 3.514120]\n",
      "epoch:41 step:32351 [D loss: 0.303007, acc.: 83.59%] [G loss: 3.260472]\n",
      "epoch:41 step:32352 [D loss: 0.292692, acc.: 86.72%] [G loss: 3.864967]\n",
      "epoch:41 step:32353 [D loss: 0.299601, acc.: 86.72%] [G loss: 3.719020]\n",
      "epoch:41 step:32354 [D loss: 0.248230, acc.: 89.84%] [G loss: 4.764795]\n",
      "epoch:41 step:32355 [D loss: 0.314916, acc.: 89.84%] [G loss: 3.730521]\n",
      "epoch:41 step:32356 [D loss: 0.248479, acc.: 89.06%] [G loss: 3.762640]\n",
      "epoch:41 step:32357 [D loss: 0.349968, acc.: 83.59%] [G loss: 2.979686]\n",
      "epoch:41 step:32358 [D loss: 0.181182, acc.: 92.19%] [G loss: 3.431178]\n",
      "epoch:41 step:32359 [D loss: 0.311562, acc.: 89.84%] [G loss: 3.855247]\n",
      "epoch:41 step:32360 [D loss: 0.202583, acc.: 90.62%] [G loss: 3.983458]\n",
      "epoch:41 step:32361 [D loss: 0.293452, acc.: 85.94%] [G loss: 2.874489]\n",
      "epoch:41 step:32362 [D loss: 0.216458, acc.: 91.41%] [G loss: 3.222391]\n",
      "epoch:41 step:32363 [D loss: 0.292443, acc.: 88.28%] [G loss: 3.769155]\n",
      "epoch:41 step:32364 [D loss: 0.296558, acc.: 85.94%] [G loss: 4.274284]\n",
      "epoch:41 step:32365 [D loss: 0.336497, acc.: 83.59%] [G loss: 3.026231]\n",
      "epoch:41 step:32366 [D loss: 0.369163, acc.: 84.38%] [G loss: 3.427308]\n",
      "epoch:41 step:32367 [D loss: 0.323693, acc.: 84.38%] [G loss: 4.962302]\n",
      "epoch:41 step:32368 [D loss: 0.324010, acc.: 83.59%] [G loss: 3.352606]\n",
      "epoch:41 step:32369 [D loss: 0.308977, acc.: 88.28%] [G loss: 4.081911]\n",
      "epoch:41 step:32370 [D loss: 0.322857, acc.: 87.50%] [G loss: 4.284172]\n",
      "epoch:41 step:32371 [D loss: 0.483555, acc.: 80.47%] [G loss: 4.786377]\n",
      "epoch:41 step:32372 [D loss: 0.275321, acc.: 90.62%] [G loss: 4.853578]\n",
      "epoch:41 step:32373 [D loss: 0.363243, acc.: 85.16%] [G loss: 5.223716]\n",
      "epoch:41 step:32374 [D loss: 0.310032, acc.: 85.16%] [G loss: 2.950944]\n",
      "epoch:41 step:32375 [D loss: 0.259740, acc.: 86.72%] [G loss: 4.615127]\n",
      "epoch:41 step:32376 [D loss: 0.256660, acc.: 89.06%] [G loss: 3.926445]\n",
      "epoch:41 step:32377 [D loss: 0.430103, acc.: 85.16%] [G loss: 4.648086]\n",
      "epoch:41 step:32378 [D loss: 0.509357, acc.: 73.44%] [G loss: 3.262046]\n",
      "epoch:41 step:32379 [D loss: 0.328810, acc.: 84.38%] [G loss: 3.832253]\n",
      "epoch:41 step:32380 [D loss: 0.363005, acc.: 85.94%] [G loss: 3.907259]\n",
      "epoch:41 step:32381 [D loss: 0.332458, acc.: 83.59%] [G loss: 3.291320]\n",
      "epoch:41 step:32382 [D loss: 0.288074, acc.: 84.38%] [G loss: 2.977188]\n",
      "epoch:41 step:32383 [D loss: 0.342797, acc.: 82.03%] [G loss: 3.052204]\n",
      "epoch:41 step:32384 [D loss: 0.300455, acc.: 85.16%] [G loss: 3.422793]\n",
      "epoch:41 step:32385 [D loss: 0.355426, acc.: 82.81%] [G loss: 3.117482]\n",
      "epoch:41 step:32386 [D loss: 0.372852, acc.: 86.72%] [G loss: 2.529339]\n",
      "epoch:41 step:32387 [D loss: 0.300214, acc.: 87.50%] [G loss: 3.747811]\n",
      "epoch:41 step:32388 [D loss: 0.334003, acc.: 89.84%] [G loss: 2.905821]\n",
      "epoch:41 step:32389 [D loss: 0.455484, acc.: 75.78%] [G loss: 3.375288]\n",
      "epoch:41 step:32390 [D loss: 0.335700, acc.: 82.81%] [G loss: 3.434256]\n",
      "epoch:41 step:32391 [D loss: 0.332886, acc.: 84.38%] [G loss: 3.370350]\n",
      "epoch:41 step:32392 [D loss: 0.239598, acc.: 92.97%] [G loss: 4.211461]\n",
      "epoch:41 step:32393 [D loss: 0.267035, acc.: 89.06%] [G loss: 3.189554]\n",
      "epoch:41 step:32394 [D loss: 0.271505, acc.: 89.06%] [G loss: 3.408304]\n",
      "epoch:41 step:32395 [D loss: 0.312415, acc.: 87.50%] [G loss: 8.865839]\n",
      "epoch:41 step:32396 [D loss: 0.271137, acc.: 89.06%] [G loss: 3.600206]\n",
      "epoch:41 step:32397 [D loss: 0.179832, acc.: 92.97%] [G loss: 4.781909]\n",
      "epoch:41 step:32398 [D loss: 0.353622, acc.: 83.59%] [G loss: 4.834735]\n",
      "epoch:41 step:32399 [D loss: 0.174365, acc.: 93.75%] [G loss: 4.284290]\n",
      "epoch:41 step:32400 [D loss: 0.268803, acc.: 87.50%] [G loss: 4.228861]\n",
      "epoch:41 step:32401 [D loss: 0.323687, acc.: 82.81%] [G loss: 3.096400]\n",
      "epoch:41 step:32402 [D loss: 0.219416, acc.: 92.97%] [G loss: 3.450133]\n",
      "epoch:41 step:32403 [D loss: 0.430194, acc.: 81.25%] [G loss: 3.207668]\n",
      "epoch:41 step:32404 [D loss: 0.435306, acc.: 84.38%] [G loss: 3.013155]\n",
      "epoch:41 step:32405 [D loss: 0.187625, acc.: 91.41%] [G loss: 3.555207]\n",
      "epoch:41 step:32406 [D loss: 0.397781, acc.: 85.94%] [G loss: 4.262997]\n",
      "epoch:41 step:32407 [D loss: 0.305827, acc.: 85.94%] [G loss: 5.200302]\n",
      "epoch:41 step:32408 [D loss: 0.263669, acc.: 89.06%] [G loss: 3.472299]\n",
      "epoch:41 step:32409 [D loss: 0.288033, acc.: 87.50%] [G loss: 4.050906]\n",
      "epoch:41 step:32410 [D loss: 0.270491, acc.: 89.84%] [G loss: 5.037185]\n",
      "epoch:41 step:32411 [D loss: 0.310701, acc.: 86.72%] [G loss: 3.474415]\n",
      "epoch:41 step:32412 [D loss: 0.514455, acc.: 77.34%] [G loss: 3.057335]\n",
      "epoch:41 step:32413 [D loss: 0.299491, acc.: 86.72%] [G loss: 3.359735]\n",
      "epoch:41 step:32414 [D loss: 0.244891, acc.: 91.41%] [G loss: 3.108622]\n",
      "epoch:41 step:32415 [D loss: 0.225006, acc.: 94.53%] [G loss: 2.823597]\n",
      "epoch:41 step:32416 [D loss: 0.340074, acc.: 83.59%] [G loss: 3.556767]\n",
      "epoch:41 step:32417 [D loss: 0.207394, acc.: 93.75%] [G loss: 3.990077]\n",
      "epoch:41 step:32418 [D loss: 0.285216, acc.: 87.50%] [G loss: 2.889589]\n",
      "epoch:41 step:32419 [D loss: 0.350346, acc.: 81.25%] [G loss: 4.044447]\n",
      "epoch:41 step:32420 [D loss: 0.356883, acc.: 83.59%] [G loss: 5.142897]\n",
      "epoch:41 step:32421 [D loss: 0.474574, acc.: 76.56%] [G loss: 2.453928]\n",
      "epoch:41 step:32422 [D loss: 0.219277, acc.: 91.41%] [G loss: 3.505165]\n",
      "epoch:41 step:32423 [D loss: 0.264301, acc.: 89.84%] [G loss: 3.017432]\n",
      "epoch:41 step:32424 [D loss: 0.287129, acc.: 86.72%] [G loss: 3.372425]\n",
      "epoch:41 step:32425 [D loss: 0.215175, acc.: 93.75%] [G loss: 2.460383]\n",
      "epoch:41 step:32426 [D loss: 0.310344, acc.: 87.50%] [G loss: 3.367994]\n",
      "epoch:41 step:32427 [D loss: 0.428802, acc.: 80.47%] [G loss: 3.395918]\n",
      "epoch:41 step:32428 [D loss: 0.343244, acc.: 81.25%] [G loss: 4.122199]\n",
      "epoch:41 step:32429 [D loss: 0.389102, acc.: 80.47%] [G loss: 2.670170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32430 [D loss: 0.343590, acc.: 82.81%] [G loss: 3.265264]\n",
      "epoch:41 step:32431 [D loss: 0.234346, acc.: 94.53%] [G loss: 3.355820]\n",
      "epoch:41 step:32432 [D loss: 0.343466, acc.: 82.03%] [G loss: 3.786522]\n",
      "epoch:41 step:32433 [D loss: 0.270444, acc.: 88.28%] [G loss: 3.502190]\n",
      "epoch:41 step:32434 [D loss: 0.260362, acc.: 86.72%] [G loss: 3.291490]\n",
      "epoch:41 step:32435 [D loss: 0.321711, acc.: 88.28%] [G loss: 4.385887]\n",
      "epoch:41 step:32436 [D loss: 0.338036, acc.: 84.38%] [G loss: 3.596622]\n",
      "epoch:41 step:32437 [D loss: 0.209271, acc.: 90.62%] [G loss: 4.673027]\n",
      "epoch:41 step:32438 [D loss: 0.317953, acc.: 85.94%] [G loss: 3.272573]\n",
      "epoch:41 step:32439 [D loss: 0.334565, acc.: 84.38%] [G loss: 2.819631]\n",
      "epoch:41 step:32440 [D loss: 0.220650, acc.: 93.75%] [G loss: 3.107661]\n",
      "epoch:41 step:32441 [D loss: 0.276295, acc.: 86.72%] [G loss: 2.905994]\n",
      "epoch:41 step:32442 [D loss: 0.278985, acc.: 85.16%] [G loss: 2.862589]\n",
      "epoch:41 step:32443 [D loss: 0.311016, acc.: 82.81%] [G loss: 3.430140]\n",
      "epoch:41 step:32444 [D loss: 0.361759, acc.: 82.03%] [G loss: 2.916817]\n",
      "epoch:41 step:32445 [D loss: 0.291379, acc.: 86.72%] [G loss: 4.140617]\n",
      "epoch:41 step:32446 [D loss: 0.350032, acc.: 86.72%] [G loss: 6.575257]\n",
      "epoch:41 step:32447 [D loss: 0.468472, acc.: 82.03%] [G loss: 3.998210]\n",
      "epoch:41 step:32448 [D loss: 0.271038, acc.: 87.50%] [G loss: 3.069378]\n",
      "epoch:41 step:32449 [D loss: 0.391175, acc.: 81.25%] [G loss: 3.314072]\n",
      "epoch:41 step:32450 [D loss: 0.373593, acc.: 81.25%] [G loss: 3.344684]\n",
      "epoch:41 step:32451 [D loss: 0.308104, acc.: 83.59%] [G loss: 3.944624]\n",
      "epoch:41 step:32452 [D loss: 0.269196, acc.: 88.28%] [G loss: 2.944050]\n",
      "epoch:41 step:32453 [D loss: 0.364927, acc.: 85.94%] [G loss: 3.757533]\n",
      "epoch:41 step:32454 [D loss: 0.280582, acc.: 89.84%] [G loss: 3.399602]\n",
      "epoch:41 step:32455 [D loss: 0.321363, acc.: 82.03%] [G loss: 3.881297]\n",
      "epoch:41 step:32456 [D loss: 0.376281, acc.: 82.81%] [G loss: 3.833979]\n",
      "epoch:41 step:32457 [D loss: 0.424653, acc.: 81.25%] [G loss: 4.120028]\n",
      "epoch:41 step:32458 [D loss: 0.377021, acc.: 82.81%] [G loss: 4.588834]\n",
      "epoch:41 step:32459 [D loss: 0.511537, acc.: 82.81%] [G loss: 7.742820]\n",
      "epoch:41 step:32460 [D loss: 1.140397, acc.: 66.41%] [G loss: 6.583937]\n",
      "epoch:41 step:32461 [D loss: 1.695383, acc.: 65.62%] [G loss: 7.237534]\n",
      "epoch:41 step:32462 [D loss: 1.149497, acc.: 66.41%] [G loss: 5.271833]\n",
      "epoch:41 step:32463 [D loss: 0.608620, acc.: 82.81%] [G loss: 4.557377]\n",
      "epoch:41 step:32464 [D loss: 0.490712, acc.: 77.34%] [G loss: 3.306130]\n",
      "epoch:41 step:32465 [D loss: 0.433387, acc.: 81.25%] [G loss: 2.279001]\n",
      "epoch:41 step:32466 [D loss: 0.372860, acc.: 85.94%] [G loss: 3.622728]\n",
      "epoch:41 step:32467 [D loss: 0.329281, acc.: 86.72%] [G loss: 3.366876]\n",
      "epoch:41 step:32468 [D loss: 0.277903, acc.: 89.06%] [G loss: 3.019032]\n",
      "epoch:41 step:32469 [D loss: 0.365216, acc.: 83.59%] [G loss: 2.554145]\n",
      "epoch:41 step:32470 [D loss: 0.308179, acc.: 87.50%] [G loss: 2.660006]\n",
      "epoch:41 step:32471 [D loss: 0.259397, acc.: 88.28%] [G loss: 3.279746]\n",
      "epoch:41 step:32472 [D loss: 0.230395, acc.: 92.19%] [G loss: 2.821337]\n",
      "epoch:41 step:32473 [D loss: 0.279028, acc.: 85.16%] [G loss: 2.714784]\n",
      "epoch:41 step:32474 [D loss: 0.309463, acc.: 85.94%] [G loss: 3.138610]\n",
      "epoch:41 step:32475 [D loss: 0.260789, acc.: 88.28%] [G loss: 2.781252]\n",
      "epoch:41 step:32476 [D loss: 0.277602, acc.: 89.06%] [G loss: 2.726088]\n",
      "epoch:41 step:32477 [D loss: 0.392384, acc.: 82.03%] [G loss: 2.951635]\n",
      "epoch:41 step:32478 [D loss: 0.297737, acc.: 87.50%] [G loss: 2.731822]\n",
      "epoch:41 step:32479 [D loss: 0.166854, acc.: 94.53%] [G loss: 3.082209]\n",
      "epoch:41 step:32480 [D loss: 0.280890, acc.: 87.50%] [G loss: 3.719059]\n",
      "epoch:41 step:32481 [D loss: 0.256779, acc.: 89.06%] [G loss: 2.774690]\n",
      "epoch:41 step:32482 [D loss: 0.317617, acc.: 85.94%] [G loss: 2.746480]\n",
      "epoch:41 step:32483 [D loss: 0.244807, acc.: 90.62%] [G loss: 2.759067]\n",
      "epoch:41 step:32484 [D loss: 0.346214, acc.: 85.16%] [G loss: 2.678189]\n",
      "epoch:41 step:32485 [D loss: 0.296596, acc.: 84.38%] [G loss: 3.151495]\n",
      "epoch:41 step:32486 [D loss: 0.383946, acc.: 78.12%] [G loss: 2.635080]\n",
      "epoch:41 step:32487 [D loss: 0.231956, acc.: 89.84%] [G loss: 3.028475]\n",
      "epoch:41 step:32488 [D loss: 0.293568, acc.: 89.84%] [G loss: 3.587258]\n",
      "epoch:41 step:32489 [D loss: 0.282046, acc.: 88.28%] [G loss: 3.208748]\n",
      "epoch:41 step:32490 [D loss: 0.239132, acc.: 87.50%] [G loss: 4.625559]\n",
      "epoch:41 step:32491 [D loss: 0.208326, acc.: 90.62%] [G loss: 4.512195]\n",
      "epoch:41 step:32492 [D loss: 0.229783, acc.: 90.62%] [G loss: 3.518244]\n",
      "epoch:41 step:32493 [D loss: 0.347697, acc.: 82.81%] [G loss: 3.411580]\n",
      "epoch:41 step:32494 [D loss: 0.270001, acc.: 89.84%] [G loss: 3.104348]\n",
      "epoch:41 step:32495 [D loss: 0.373189, acc.: 84.38%] [G loss: 2.679771]\n",
      "epoch:41 step:32496 [D loss: 0.402554, acc.: 81.25%] [G loss: 2.357596]\n",
      "epoch:41 step:32497 [D loss: 0.241764, acc.: 90.62%] [G loss: 2.952149]\n",
      "epoch:41 step:32498 [D loss: 0.275388, acc.: 89.06%] [G loss: 2.702620]\n",
      "epoch:41 step:32499 [D loss: 0.363026, acc.: 85.16%] [G loss: 2.770530]\n",
      "epoch:41 step:32500 [D loss: 0.339311, acc.: 84.38%] [G loss: 2.739896]\n",
      "epoch:41 step:32501 [D loss: 0.233459, acc.: 94.53%] [G loss: 3.398880]\n",
      "epoch:41 step:32502 [D loss: 0.297056, acc.: 85.94%] [G loss: 2.701314]\n",
      "epoch:41 step:32503 [D loss: 0.352121, acc.: 85.94%] [G loss: 2.899461]\n",
      "epoch:41 step:32504 [D loss: 0.328327, acc.: 87.50%] [G loss: 3.101358]\n",
      "epoch:41 step:32505 [D loss: 0.325617, acc.: 86.72%] [G loss: 2.460811]\n",
      "epoch:41 step:32506 [D loss: 0.214072, acc.: 89.84%] [G loss: 2.939826]\n",
      "epoch:41 step:32507 [D loss: 0.380591, acc.: 85.94%] [G loss: 2.545903]\n",
      "epoch:41 step:32508 [D loss: 0.271600, acc.: 86.72%] [G loss: 2.744632]\n",
      "epoch:41 step:32509 [D loss: 0.219392, acc.: 92.19%] [G loss: 3.275821]\n",
      "epoch:41 step:32510 [D loss: 0.347921, acc.: 87.50%] [G loss: 2.443177]\n",
      "epoch:41 step:32511 [D loss: 0.306370, acc.: 86.72%] [G loss: 2.673777]\n",
      "epoch:41 step:32512 [D loss: 0.297463, acc.: 85.16%] [G loss: 2.930217]\n",
      "epoch:41 step:32513 [D loss: 0.385029, acc.: 82.03%] [G loss: 3.525998]\n",
      "epoch:41 step:32514 [D loss: 0.318624, acc.: 85.16%] [G loss: 2.837430]\n",
      "epoch:41 step:32515 [D loss: 0.298977, acc.: 85.94%] [G loss: 3.837484]\n",
      "epoch:41 step:32516 [D loss: 0.320408, acc.: 85.94%] [G loss: 2.582403]\n",
      "epoch:41 step:32517 [D loss: 0.244318, acc.: 89.06%] [G loss: 2.706414]\n",
      "epoch:41 step:32518 [D loss: 0.252826, acc.: 89.84%] [G loss: 3.150511]\n",
      "epoch:41 step:32519 [D loss: 0.328326, acc.: 82.81%] [G loss: 2.348394]\n",
      "epoch:41 step:32520 [D loss: 0.223314, acc.: 89.84%] [G loss: 2.690808]\n",
      "epoch:41 step:32521 [D loss: 0.269788, acc.: 89.06%] [G loss: 2.684717]\n",
      "epoch:41 step:32522 [D loss: 0.364308, acc.: 82.81%] [G loss: 2.591919]\n",
      "epoch:41 step:32523 [D loss: 0.340250, acc.: 85.94%] [G loss: 2.282943]\n",
      "epoch:41 step:32524 [D loss: 0.347323, acc.: 85.16%] [G loss: 3.291288]\n",
      "epoch:41 step:32525 [D loss: 0.337539, acc.: 82.81%] [G loss: 2.627454]\n",
      "epoch:41 step:32526 [D loss: 0.230138, acc.: 90.62%] [G loss: 3.736904]\n",
      "epoch:41 step:32527 [D loss: 0.303627, acc.: 84.38%] [G loss: 2.573938]\n",
      "epoch:41 step:32528 [D loss: 0.220835, acc.: 89.84%] [G loss: 4.387737]\n",
      "epoch:41 step:32529 [D loss: 0.276567, acc.: 89.06%] [G loss: 3.262643]\n",
      "epoch:41 step:32530 [D loss: 0.238199, acc.: 92.97%] [G loss: 3.901552]\n",
      "epoch:41 step:32531 [D loss: 0.223041, acc.: 91.41%] [G loss: 3.313808]\n",
      "epoch:41 step:32532 [D loss: 0.361254, acc.: 82.81%] [G loss: 3.492194]\n",
      "epoch:41 step:32533 [D loss: 0.263368, acc.: 88.28%] [G loss: 3.810395]\n",
      "epoch:41 step:32534 [D loss: 0.312244, acc.: 84.38%] [G loss: 3.092305]\n",
      "epoch:41 step:32535 [D loss: 0.239822, acc.: 91.41%] [G loss: 2.653336]\n",
      "epoch:41 step:32536 [D loss: 0.347145, acc.: 87.50%] [G loss: 2.787092]\n",
      "epoch:41 step:32537 [D loss: 0.287621, acc.: 90.62%] [G loss: 3.165496]\n",
      "epoch:41 step:32538 [D loss: 0.328599, acc.: 85.94%] [G loss: 2.913384]\n",
      "epoch:41 step:32539 [D loss: 0.274212, acc.: 89.84%] [G loss: 3.122651]\n",
      "epoch:41 step:32540 [D loss: 0.320428, acc.: 85.16%] [G loss: 3.355263]\n",
      "epoch:41 step:32541 [D loss: 0.376116, acc.: 85.94%] [G loss: 2.604240]\n",
      "epoch:41 step:32542 [D loss: 0.369536, acc.: 82.03%] [G loss: 3.702639]\n",
      "epoch:41 step:32543 [D loss: 0.296713, acc.: 85.16%] [G loss: 3.529009]\n",
      "epoch:41 step:32544 [D loss: 0.327015, acc.: 85.16%] [G loss: 2.766170]\n",
      "epoch:41 step:32545 [D loss: 0.254295, acc.: 90.62%] [G loss: 2.496829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32546 [D loss: 0.342355, acc.: 85.16%] [G loss: 2.266514]\n",
      "epoch:41 step:32547 [D loss: 0.363868, acc.: 86.72%] [G loss: 2.482267]\n",
      "epoch:41 step:32548 [D loss: 0.291898, acc.: 85.16%] [G loss: 3.068802]\n",
      "epoch:41 step:32549 [D loss: 0.258801, acc.: 89.84%] [G loss: 3.343967]\n",
      "epoch:41 step:32550 [D loss: 0.413890, acc.: 78.91%] [G loss: 4.943905]\n",
      "epoch:41 step:32551 [D loss: 0.362444, acc.: 84.38%] [G loss: 3.095437]\n",
      "epoch:41 step:32552 [D loss: 0.269023, acc.: 88.28%] [G loss: 4.619227]\n",
      "epoch:41 step:32553 [D loss: 0.369996, acc.: 81.25%] [G loss: 3.489217]\n",
      "epoch:41 step:32554 [D loss: 0.235937, acc.: 90.62%] [G loss: 5.743104]\n",
      "epoch:41 step:32555 [D loss: 0.431108, acc.: 80.47%] [G loss: 5.462910]\n",
      "epoch:41 step:32556 [D loss: 0.377091, acc.: 78.12%] [G loss: 4.098464]\n",
      "epoch:41 step:32557 [D loss: 0.344075, acc.: 85.94%] [G loss: 3.182853]\n",
      "epoch:41 step:32558 [D loss: 0.446270, acc.: 80.47%] [G loss: 3.276720]\n",
      "epoch:41 step:32559 [D loss: 0.219998, acc.: 92.97%] [G loss: 3.881876]\n",
      "epoch:41 step:32560 [D loss: 0.249711, acc.: 92.19%] [G loss: 5.816771]\n",
      "epoch:41 step:32561 [D loss: 0.275260, acc.: 89.06%] [G loss: 3.848615]\n",
      "epoch:41 step:32562 [D loss: 0.238036, acc.: 92.19%] [G loss: 3.713571]\n",
      "epoch:41 step:32563 [D loss: 0.377636, acc.: 84.38%] [G loss: 4.164890]\n",
      "epoch:41 step:32564 [D loss: 0.196103, acc.: 93.75%] [G loss: 3.366507]\n",
      "epoch:41 step:32565 [D loss: 0.314686, acc.: 88.28%] [G loss: 3.277243]\n",
      "epoch:41 step:32566 [D loss: 0.224535, acc.: 92.97%] [G loss: 3.204772]\n",
      "epoch:41 step:32567 [D loss: 0.352751, acc.: 85.94%] [G loss: 3.239942]\n",
      "epoch:41 step:32568 [D loss: 0.439277, acc.: 77.34%] [G loss: 3.860116]\n",
      "epoch:41 step:32569 [D loss: 0.198588, acc.: 92.97%] [G loss: 3.388813]\n",
      "epoch:41 step:32570 [D loss: 0.303086, acc.: 86.72%] [G loss: 3.351727]\n",
      "epoch:41 step:32571 [D loss: 0.328357, acc.: 83.59%] [G loss: 3.331239]\n",
      "epoch:41 step:32572 [D loss: 0.314032, acc.: 88.28%] [G loss: 3.798440]\n",
      "epoch:41 step:32573 [D loss: 0.339095, acc.: 87.50%] [G loss: 6.749392]\n",
      "epoch:41 step:32574 [D loss: 0.490967, acc.: 82.81%] [G loss: 3.591160]\n",
      "epoch:41 step:32575 [D loss: 0.300410, acc.: 86.72%] [G loss: 4.068696]\n",
      "epoch:41 step:32576 [D loss: 0.446057, acc.: 79.69%] [G loss: 3.042188]\n",
      "epoch:41 step:32577 [D loss: 0.221149, acc.: 91.41%] [G loss: 4.371008]\n",
      "epoch:41 step:32578 [D loss: 0.366535, acc.: 88.28%] [G loss: 2.883173]\n",
      "epoch:41 step:32579 [D loss: 0.336006, acc.: 85.94%] [G loss: 2.772461]\n",
      "epoch:41 step:32580 [D loss: 0.297041, acc.: 88.28%] [G loss: 2.864051]\n",
      "epoch:41 step:32581 [D loss: 0.313133, acc.: 86.72%] [G loss: 4.788755]\n",
      "epoch:41 step:32582 [D loss: 0.334714, acc.: 84.38%] [G loss: 3.168004]\n",
      "epoch:41 step:32583 [D loss: 0.395101, acc.: 80.47%] [G loss: 3.111899]\n",
      "epoch:41 step:32584 [D loss: 0.294895, acc.: 87.50%] [G loss: 3.077848]\n",
      "epoch:41 step:32585 [D loss: 0.234719, acc.: 88.28%] [G loss: 4.584082]\n",
      "epoch:41 step:32586 [D loss: 0.264631, acc.: 88.28%] [G loss: 3.456750]\n",
      "epoch:41 step:32587 [D loss: 0.326330, acc.: 85.16%] [G loss: 3.400964]\n",
      "epoch:41 step:32588 [D loss: 0.306485, acc.: 89.06%] [G loss: 2.721296]\n",
      "epoch:41 step:32589 [D loss: 0.239748, acc.: 89.06%] [G loss: 3.839911]\n",
      "epoch:41 step:32590 [D loss: 0.206067, acc.: 89.84%] [G loss: 3.532780]\n",
      "epoch:41 step:32591 [D loss: 0.351686, acc.: 83.59%] [G loss: 2.804698]\n",
      "epoch:41 step:32592 [D loss: 0.336772, acc.: 86.72%] [G loss: 3.308619]\n",
      "epoch:41 step:32593 [D loss: 0.339156, acc.: 84.38%] [G loss: 3.484001]\n",
      "epoch:41 step:32594 [D loss: 0.297092, acc.: 85.94%] [G loss: 3.091834]\n",
      "epoch:41 step:32595 [D loss: 0.352344, acc.: 85.16%] [G loss: 3.082421]\n",
      "epoch:41 step:32596 [D loss: 0.386327, acc.: 82.81%] [G loss: 2.509349]\n",
      "epoch:41 step:32597 [D loss: 0.404799, acc.: 80.47%] [G loss: 3.028222]\n",
      "epoch:41 step:32598 [D loss: 0.303685, acc.: 88.28%] [G loss: 2.759034]\n",
      "epoch:41 step:32599 [D loss: 0.330921, acc.: 82.81%] [G loss: 2.446802]\n",
      "epoch:41 step:32600 [D loss: 0.342451, acc.: 81.25%] [G loss: 3.228977]\n",
      "epoch:41 step:32601 [D loss: 0.382915, acc.: 82.81%] [G loss: 2.766635]\n",
      "epoch:41 step:32602 [D loss: 0.282373, acc.: 85.16%] [G loss: 3.059198]\n",
      "epoch:41 step:32603 [D loss: 0.277175, acc.: 90.62%] [G loss: 2.839916]\n",
      "epoch:41 step:32604 [D loss: 0.350754, acc.: 86.72%] [G loss: 3.483351]\n",
      "epoch:41 step:32605 [D loss: 0.444172, acc.: 77.34%] [G loss: 2.795290]\n",
      "epoch:41 step:32606 [D loss: 0.377372, acc.: 83.59%] [G loss: 3.168657]\n",
      "epoch:41 step:32607 [D loss: 0.291400, acc.: 85.16%] [G loss: 2.612460]\n",
      "epoch:41 step:32608 [D loss: 0.274363, acc.: 88.28%] [G loss: 5.209041]\n",
      "epoch:41 step:32609 [D loss: 0.256489, acc.: 89.84%] [G loss: 4.745158]\n",
      "epoch:41 step:32610 [D loss: 0.329236, acc.: 86.72%] [G loss: 4.000266]\n",
      "epoch:41 step:32611 [D loss: 0.253132, acc.: 89.84%] [G loss: 3.309738]\n",
      "epoch:41 step:32612 [D loss: 0.275735, acc.: 86.72%] [G loss: 5.307946]\n",
      "epoch:41 step:32613 [D loss: 0.263712, acc.: 88.28%] [G loss: 3.012475]\n",
      "epoch:41 step:32614 [D loss: 0.291812, acc.: 91.41%] [G loss: 3.275688]\n",
      "epoch:41 step:32615 [D loss: 0.171295, acc.: 95.31%] [G loss: 4.103457]\n",
      "epoch:41 step:32616 [D loss: 0.294384, acc.: 89.84%] [G loss: 4.380453]\n",
      "epoch:41 step:32617 [D loss: 0.219128, acc.: 92.19%] [G loss: 4.099848]\n",
      "epoch:41 step:32618 [D loss: 0.193621, acc.: 91.41%] [G loss: 4.195147]\n",
      "epoch:41 step:32619 [D loss: 0.324447, acc.: 84.38%] [G loss: 4.857121]\n",
      "epoch:41 step:32620 [D loss: 0.296587, acc.: 89.84%] [G loss: 2.505930]\n",
      "epoch:41 step:32621 [D loss: 0.234529, acc.: 89.84%] [G loss: 3.920110]\n",
      "epoch:41 step:32622 [D loss: 0.276524, acc.: 89.06%] [G loss: 3.163708]\n",
      "epoch:41 step:32623 [D loss: 0.339537, acc.: 83.59%] [G loss: 4.459500]\n",
      "epoch:41 step:32624 [D loss: 0.359771, acc.: 85.94%] [G loss: 3.994321]\n",
      "epoch:41 step:32625 [D loss: 0.200932, acc.: 90.62%] [G loss: 4.554166]\n",
      "epoch:41 step:32626 [D loss: 0.294157, acc.: 89.84%] [G loss: 5.491879]\n",
      "epoch:41 step:32627 [D loss: 0.274810, acc.: 85.94%] [G loss: 5.021426]\n",
      "epoch:41 step:32628 [D loss: 0.263117, acc.: 89.06%] [G loss: 4.466305]\n",
      "epoch:41 step:32629 [D loss: 0.213333, acc.: 89.84%] [G loss: 4.719175]\n",
      "epoch:41 step:32630 [D loss: 0.258944, acc.: 89.06%] [G loss: 4.610583]\n",
      "epoch:41 step:32631 [D loss: 0.333505, acc.: 83.59%] [G loss: 4.462528]\n",
      "epoch:41 step:32632 [D loss: 0.315853, acc.: 82.81%] [G loss: 3.985968]\n",
      "epoch:41 step:32633 [D loss: 0.234796, acc.: 89.06%] [G loss: 4.028250]\n",
      "epoch:41 step:32634 [D loss: 0.251129, acc.: 88.28%] [G loss: 4.061708]\n",
      "epoch:41 step:32635 [D loss: 0.287176, acc.: 88.28%] [G loss: 3.593088]\n",
      "epoch:41 step:32636 [D loss: 0.260277, acc.: 86.72%] [G loss: 3.271207]\n",
      "epoch:41 step:32637 [D loss: 0.278632, acc.: 89.06%] [G loss: 4.073673]\n",
      "epoch:41 step:32638 [D loss: 0.293152, acc.: 85.16%] [G loss: 3.041263]\n",
      "epoch:41 step:32639 [D loss: 0.264671, acc.: 88.28%] [G loss: 3.058165]\n",
      "epoch:41 step:32640 [D loss: 0.300910, acc.: 86.72%] [G loss: 2.935653]\n",
      "epoch:41 step:32641 [D loss: 0.344331, acc.: 86.72%] [G loss: 2.945883]\n",
      "epoch:41 step:32642 [D loss: 0.234689, acc.: 89.84%] [G loss: 3.449981]\n",
      "epoch:41 step:32643 [D loss: 0.307724, acc.: 84.38%] [G loss: 2.809008]\n",
      "epoch:41 step:32644 [D loss: 0.373321, acc.: 79.69%] [G loss: 2.931669]\n",
      "epoch:41 step:32645 [D loss: 0.312782, acc.: 85.16%] [G loss: 2.547139]\n",
      "epoch:41 step:32646 [D loss: 0.369070, acc.: 82.03%] [G loss: 3.491312]\n",
      "epoch:41 step:32647 [D loss: 0.387681, acc.: 83.59%] [G loss: 2.910342]\n",
      "epoch:41 step:32648 [D loss: 0.342085, acc.: 85.16%] [G loss: 4.330594]\n",
      "epoch:41 step:32649 [D loss: 0.512836, acc.: 77.34%] [G loss: 4.653255]\n",
      "epoch:41 step:32650 [D loss: 0.418450, acc.: 82.03%] [G loss: 5.552060]\n",
      "epoch:41 step:32651 [D loss: 0.500083, acc.: 79.69%] [G loss: 3.907756]\n",
      "epoch:41 step:32652 [D loss: 0.537372, acc.: 79.69%] [G loss: 6.098495]\n",
      "epoch:41 step:32653 [D loss: 0.405967, acc.: 82.03%] [G loss: 4.383710]\n",
      "epoch:41 step:32654 [D loss: 0.271349, acc.: 89.84%] [G loss: 4.298903]\n",
      "epoch:41 step:32655 [D loss: 0.303151, acc.: 85.16%] [G loss: 3.484210]\n",
      "epoch:41 step:32656 [D loss: 0.233164, acc.: 88.28%] [G loss: 4.072568]\n",
      "epoch:41 step:32657 [D loss: 0.229184, acc.: 89.84%] [G loss: 3.507497]\n",
      "epoch:41 step:32658 [D loss: 0.390002, acc.: 85.16%] [G loss: 3.217480]\n",
      "epoch:41 step:32659 [D loss: 0.310707, acc.: 85.16%] [G loss: 3.512100]\n",
      "epoch:41 step:32660 [D loss: 0.353546, acc.: 84.38%] [G loss: 2.529883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32661 [D loss: 0.261870, acc.: 91.41%] [G loss: 3.314314]\n",
      "epoch:41 step:32662 [D loss: 0.200095, acc.: 93.75%] [G loss: 2.910296]\n",
      "epoch:41 step:32663 [D loss: 0.319129, acc.: 86.72%] [G loss: 3.278157]\n",
      "epoch:41 step:32664 [D loss: 0.328132, acc.: 84.38%] [G loss: 4.021959]\n",
      "epoch:41 step:32665 [D loss: 0.265978, acc.: 89.06%] [G loss: 4.880389]\n",
      "epoch:41 step:32666 [D loss: 0.299207, acc.: 85.94%] [G loss: 3.297815]\n",
      "epoch:41 step:32667 [D loss: 0.310645, acc.: 88.28%] [G loss: 3.593714]\n",
      "epoch:41 step:32668 [D loss: 0.299127, acc.: 82.03%] [G loss: 3.749156]\n",
      "epoch:41 step:32669 [D loss: 0.316277, acc.: 85.94%] [G loss: 2.844149]\n",
      "epoch:41 step:32670 [D loss: 0.243765, acc.: 89.84%] [G loss: 3.063495]\n",
      "epoch:41 step:32671 [D loss: 0.317244, acc.: 83.59%] [G loss: 3.070285]\n",
      "epoch:41 step:32672 [D loss: 0.294074, acc.: 87.50%] [G loss: 2.481834]\n",
      "epoch:41 step:32673 [D loss: 0.273564, acc.: 89.06%] [G loss: 2.790638]\n",
      "epoch:41 step:32674 [D loss: 0.387098, acc.: 85.94%] [G loss: 2.660716]\n",
      "epoch:41 step:32675 [D loss: 0.417748, acc.: 76.56%] [G loss: 4.015015]\n",
      "epoch:41 step:32676 [D loss: 0.417529, acc.: 82.03%] [G loss: 2.940553]\n",
      "epoch:41 step:32677 [D loss: 0.355330, acc.: 85.94%] [G loss: 3.663284]\n",
      "epoch:41 step:32678 [D loss: 0.451668, acc.: 78.12%] [G loss: 4.023239]\n",
      "epoch:41 step:32679 [D loss: 0.403637, acc.: 82.81%] [G loss: 7.398898]\n",
      "epoch:41 step:32680 [D loss: 0.497248, acc.: 75.00%] [G loss: 4.283036]\n",
      "epoch:41 step:32681 [D loss: 0.302851, acc.: 84.38%] [G loss: 4.345728]\n",
      "epoch:41 step:32682 [D loss: 0.390080, acc.: 85.94%] [G loss: 3.621394]\n",
      "epoch:41 step:32683 [D loss: 0.319749, acc.: 84.38%] [G loss: 3.275662]\n",
      "epoch:41 step:32684 [D loss: 0.322007, acc.: 85.94%] [G loss: 3.710461]\n",
      "epoch:41 step:32685 [D loss: 0.330432, acc.: 87.50%] [G loss: 3.047812]\n",
      "epoch:41 step:32686 [D loss: 0.224828, acc.: 90.62%] [G loss: 3.109725]\n",
      "epoch:41 step:32687 [D loss: 0.347836, acc.: 85.16%] [G loss: 2.965520]\n",
      "epoch:41 step:32688 [D loss: 0.291023, acc.: 89.84%] [G loss: 3.135998]\n",
      "epoch:41 step:32689 [D loss: 0.283020, acc.: 86.72%] [G loss: 2.475908]\n",
      "epoch:41 step:32690 [D loss: 0.363756, acc.: 84.38%] [G loss: 2.995854]\n",
      "epoch:41 step:32691 [D loss: 0.402542, acc.: 82.03%] [G loss: 2.715844]\n",
      "epoch:41 step:32692 [D loss: 0.340824, acc.: 88.28%] [G loss: 2.537084]\n",
      "epoch:41 step:32693 [D loss: 0.277950, acc.: 91.41%] [G loss: 2.949543]\n",
      "epoch:41 step:32694 [D loss: 0.270640, acc.: 85.94%] [G loss: 2.829650]\n",
      "epoch:41 step:32695 [D loss: 0.344923, acc.: 82.03%] [G loss: 2.776876]\n",
      "epoch:41 step:32696 [D loss: 0.400705, acc.: 82.03%] [G loss: 2.483895]\n",
      "epoch:41 step:32697 [D loss: 0.376344, acc.: 80.47%] [G loss: 3.015200]\n",
      "epoch:41 step:32698 [D loss: 0.256376, acc.: 88.28%] [G loss: 2.389542]\n",
      "epoch:41 step:32699 [D loss: 0.281672, acc.: 89.06%] [G loss: 2.712604]\n",
      "epoch:41 step:32700 [D loss: 0.332145, acc.: 84.38%] [G loss: 2.348596]\n",
      "epoch:41 step:32701 [D loss: 0.260013, acc.: 89.06%] [G loss: 3.694980]\n",
      "epoch:41 step:32702 [D loss: 0.225941, acc.: 89.84%] [G loss: 5.310957]\n",
      "epoch:41 step:32703 [D loss: 0.445543, acc.: 78.91%] [G loss: 2.955682]\n",
      "epoch:41 step:32704 [D loss: 0.283095, acc.: 88.28%] [G loss: 3.815403]\n",
      "epoch:41 step:32705 [D loss: 0.241632, acc.: 89.06%] [G loss: 3.633986]\n",
      "epoch:41 step:32706 [D loss: 0.292763, acc.: 88.28%] [G loss: 3.098276]\n",
      "epoch:41 step:32707 [D loss: 0.264868, acc.: 89.06%] [G loss: 3.005929]\n",
      "epoch:41 step:32708 [D loss: 0.285374, acc.: 89.84%] [G loss: 4.095912]\n",
      "epoch:41 step:32709 [D loss: 0.277896, acc.: 86.72%] [G loss: 3.385804]\n",
      "epoch:41 step:32710 [D loss: 0.224837, acc.: 92.97%] [G loss: 3.737855]\n",
      "epoch:41 step:32711 [D loss: 0.247528, acc.: 92.19%] [G loss: 3.856533]\n",
      "epoch:41 step:32712 [D loss: 0.201697, acc.: 90.62%] [G loss: 3.636864]\n",
      "epoch:41 step:32713 [D loss: 0.410076, acc.: 83.59%] [G loss: 3.427753]\n",
      "epoch:41 step:32714 [D loss: 0.331255, acc.: 86.72%] [G loss: 3.166510]\n",
      "epoch:41 step:32715 [D loss: 0.289969, acc.: 88.28%] [G loss: 4.149599]\n",
      "epoch:41 step:32716 [D loss: 0.237404, acc.: 86.72%] [G loss: 3.683669]\n",
      "epoch:41 step:32717 [D loss: 0.342918, acc.: 86.72%] [G loss: 3.956066]\n",
      "epoch:41 step:32718 [D loss: 0.264116, acc.: 87.50%] [G loss: 4.373553]\n",
      "epoch:41 step:32719 [D loss: 0.240939, acc.: 87.50%] [G loss: 4.321497]\n",
      "epoch:41 step:32720 [D loss: 0.154025, acc.: 93.75%] [G loss: 5.050508]\n",
      "epoch:41 step:32721 [D loss: 0.278968, acc.: 89.84%] [G loss: 3.403517]\n",
      "epoch:41 step:32722 [D loss: 0.285492, acc.: 85.16%] [G loss: 3.150721]\n",
      "epoch:41 step:32723 [D loss: 0.296768, acc.: 89.06%] [G loss: 4.233311]\n",
      "epoch:41 step:32724 [D loss: 0.302169, acc.: 82.81%] [G loss: 4.428753]\n",
      "epoch:41 step:32725 [D loss: 0.290293, acc.: 90.62%] [G loss: 2.822898]\n",
      "epoch:41 step:32726 [D loss: 0.304395, acc.: 87.50%] [G loss: 3.089243]\n",
      "epoch:41 step:32727 [D loss: 0.305122, acc.: 86.72%] [G loss: 2.932003]\n",
      "epoch:41 step:32728 [D loss: 0.279904, acc.: 87.50%] [G loss: 2.740783]\n",
      "epoch:41 step:32729 [D loss: 0.385003, acc.: 79.69%] [G loss: 2.864737]\n",
      "epoch:41 step:32730 [D loss: 0.339884, acc.: 82.03%] [G loss: 2.119931]\n",
      "epoch:41 step:32731 [D loss: 0.292102, acc.: 88.28%] [G loss: 3.490565]\n",
      "epoch:41 step:32732 [D loss: 0.297978, acc.: 85.16%] [G loss: 2.782202]\n",
      "epoch:41 step:32733 [D loss: 0.403854, acc.: 85.16%] [G loss: 2.924274]\n",
      "epoch:41 step:32734 [D loss: 0.324078, acc.: 85.94%] [G loss: 2.984362]\n",
      "epoch:41 step:32735 [D loss: 0.338754, acc.: 86.72%] [G loss: 2.888223]\n",
      "epoch:41 step:32736 [D loss: 0.301181, acc.: 87.50%] [G loss: 3.535518]\n",
      "epoch:41 step:32737 [D loss: 0.388953, acc.: 85.16%] [G loss: 3.486812]\n",
      "epoch:41 step:32738 [D loss: 0.289329, acc.: 89.84%] [G loss: 4.200833]\n",
      "epoch:41 step:32739 [D loss: 0.375936, acc.: 80.47%] [G loss: 2.525757]\n",
      "epoch:41 step:32740 [D loss: 0.269922, acc.: 85.16%] [G loss: 3.770056]\n",
      "epoch:41 step:32741 [D loss: 0.327730, acc.: 85.94%] [G loss: 3.482618]\n",
      "epoch:41 step:32742 [D loss: 0.355448, acc.: 81.25%] [G loss: 3.203565]\n",
      "epoch:41 step:32743 [D loss: 0.258477, acc.: 89.84%] [G loss: 4.018874]\n",
      "epoch:41 step:32744 [D loss: 0.329672, acc.: 85.16%] [G loss: 3.272805]\n",
      "epoch:41 step:32745 [D loss: 0.212700, acc.: 94.53%] [G loss: 3.054835]\n",
      "epoch:41 step:32746 [D loss: 0.292394, acc.: 86.72%] [G loss: 2.384038]\n",
      "epoch:41 step:32747 [D loss: 0.304684, acc.: 85.16%] [G loss: 2.608353]\n",
      "epoch:41 step:32748 [D loss: 0.374272, acc.: 83.59%] [G loss: 2.453390]\n",
      "epoch:41 step:32749 [D loss: 0.323258, acc.: 85.94%] [G loss: 2.566855]\n",
      "epoch:41 step:32750 [D loss: 0.294825, acc.: 86.72%] [G loss: 3.227100]\n",
      "epoch:41 step:32751 [D loss: 0.265770, acc.: 90.62%] [G loss: 3.666237]\n",
      "epoch:41 step:32752 [D loss: 0.266290, acc.: 88.28%] [G loss: 3.002079]\n",
      "epoch:41 step:32753 [D loss: 0.331929, acc.: 85.16%] [G loss: 2.276300]\n",
      "epoch:41 step:32754 [D loss: 0.310365, acc.: 88.28%] [G loss: 3.013798]\n",
      "epoch:41 step:32755 [D loss: 0.319246, acc.: 85.94%] [G loss: 3.408698]\n",
      "epoch:41 step:32756 [D loss: 0.291362, acc.: 87.50%] [G loss: 3.150512]\n",
      "epoch:41 step:32757 [D loss: 0.250917, acc.: 87.50%] [G loss: 2.886579]\n",
      "epoch:41 step:32758 [D loss: 0.283940, acc.: 88.28%] [G loss: 3.048303]\n",
      "epoch:41 step:32759 [D loss: 0.416496, acc.: 82.03%] [G loss: 2.727802]\n",
      "epoch:41 step:32760 [D loss: 0.227074, acc.: 90.62%] [G loss: 3.076047]\n",
      "epoch:41 step:32761 [D loss: 0.255652, acc.: 89.84%] [G loss: 3.069436]\n",
      "epoch:41 step:32762 [D loss: 0.230428, acc.: 92.19%] [G loss: 3.264900]\n",
      "epoch:41 step:32763 [D loss: 0.283025, acc.: 85.94%] [G loss: 3.170722]\n",
      "epoch:41 step:32764 [D loss: 0.272072, acc.: 87.50%] [G loss: 3.342128]\n",
      "epoch:41 step:32765 [D loss: 0.241858, acc.: 90.62%] [G loss: 4.371456]\n",
      "epoch:41 step:32766 [D loss: 0.400118, acc.: 82.03%] [G loss: 2.896852]\n",
      "epoch:41 step:32767 [D loss: 0.303201, acc.: 83.59%] [G loss: 3.077722]\n",
      "epoch:41 step:32768 [D loss: 0.302407, acc.: 85.94%] [G loss: 3.677174]\n",
      "epoch:41 step:32769 [D loss: 0.359962, acc.: 82.03%] [G loss: 3.498663]\n",
      "epoch:41 step:32770 [D loss: 0.434470, acc.: 83.59%] [G loss: 4.307478]\n",
      "epoch:41 step:32771 [D loss: 0.469073, acc.: 79.69%] [G loss: 5.061745]\n",
      "epoch:41 step:32772 [D loss: 0.281263, acc.: 86.72%] [G loss: 2.958909]\n",
      "epoch:41 step:32773 [D loss: 0.260339, acc.: 89.06%] [G loss: 5.344008]\n",
      "epoch:41 step:32774 [D loss: 0.428488, acc.: 81.25%] [G loss: 4.028962]\n",
      "epoch:41 step:32775 [D loss: 0.233863, acc.: 89.06%] [G loss: 3.600128]\n",
      "epoch:41 step:32776 [D loss: 0.295014, acc.: 85.94%] [G loss: 3.135447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32777 [D loss: 0.228788, acc.: 93.75%] [G loss: 3.029243]\n",
      "epoch:41 step:32778 [D loss: 0.242705, acc.: 89.84%] [G loss: 3.481396]\n",
      "epoch:41 step:32779 [D loss: 0.260583, acc.: 91.41%] [G loss: 3.372747]\n",
      "epoch:41 step:32780 [D loss: 0.335156, acc.: 84.38%] [G loss: 3.312197]\n",
      "epoch:41 step:32781 [D loss: 0.202784, acc.: 91.41%] [G loss: 4.153947]\n",
      "epoch:41 step:32782 [D loss: 0.221072, acc.: 92.19%] [G loss: 3.512430]\n",
      "epoch:41 step:32783 [D loss: 0.301415, acc.: 86.72%] [G loss: 3.997919]\n",
      "epoch:41 step:32784 [D loss: 0.404460, acc.: 83.59%] [G loss: 2.758444]\n",
      "epoch:41 step:32785 [D loss: 0.234572, acc.: 89.84%] [G loss: 4.630832]\n",
      "epoch:41 step:32786 [D loss: 0.394086, acc.: 85.94%] [G loss: 6.036029]\n",
      "epoch:41 step:32787 [D loss: 0.690922, acc.: 68.75%] [G loss: 4.350335]\n",
      "epoch:41 step:32788 [D loss: 0.332146, acc.: 85.94%] [G loss: 4.736321]\n",
      "epoch:41 step:32789 [D loss: 0.363530, acc.: 78.91%] [G loss: 3.681987]\n",
      "epoch:41 step:32790 [D loss: 0.251973, acc.: 88.28%] [G loss: 4.027470]\n",
      "epoch:41 step:32791 [D loss: 0.397694, acc.: 83.59%] [G loss: 3.101651]\n",
      "epoch:41 step:32792 [D loss: 0.338954, acc.: 88.28%] [G loss: 3.062940]\n",
      "epoch:41 step:32793 [D loss: 0.400765, acc.: 80.47%] [G loss: 3.000134]\n",
      "epoch:41 step:32794 [D loss: 0.272164, acc.: 85.16%] [G loss: 3.439902]\n",
      "epoch:41 step:32795 [D loss: 0.377372, acc.: 82.81%] [G loss: 2.930466]\n",
      "epoch:41 step:32796 [D loss: 0.289749, acc.: 88.28%] [G loss: 3.554040]\n",
      "epoch:41 step:32797 [D loss: 0.288394, acc.: 89.06%] [G loss: 3.179836]\n",
      "epoch:41 step:32798 [D loss: 0.350549, acc.: 82.81%] [G loss: 3.300199]\n",
      "epoch:41 step:32799 [D loss: 0.290984, acc.: 88.28%] [G loss: 3.955181]\n",
      "epoch:41 step:32800 [D loss: 0.267436, acc.: 85.94%] [G loss: 4.167864]\n",
      "epoch:41 step:32801 [D loss: 0.221683, acc.: 90.62%] [G loss: 3.505925]\n",
      "epoch:41 step:32802 [D loss: 0.347134, acc.: 84.38%] [G loss: 3.268699]\n",
      "epoch:42 step:32803 [D loss: 0.208383, acc.: 92.97%] [G loss: 3.389290]\n",
      "epoch:42 step:32804 [D loss: 0.275498, acc.: 87.50%] [G loss: 3.466093]\n",
      "epoch:42 step:32805 [D loss: 0.343444, acc.: 84.38%] [G loss: 3.542194]\n",
      "epoch:42 step:32806 [D loss: 0.333109, acc.: 84.38%] [G loss: 3.588871]\n",
      "epoch:42 step:32807 [D loss: 0.252612, acc.: 89.84%] [G loss: 4.767828]\n",
      "epoch:42 step:32808 [D loss: 0.179735, acc.: 94.53%] [G loss: 5.550104]\n",
      "epoch:42 step:32809 [D loss: 0.238966, acc.: 88.28%] [G loss: 5.693425]\n",
      "epoch:42 step:32810 [D loss: 0.220310, acc.: 90.62%] [G loss: 4.497735]\n",
      "epoch:42 step:32811 [D loss: 0.235822, acc.: 91.41%] [G loss: 6.393624]\n",
      "epoch:42 step:32812 [D loss: 0.221863, acc.: 91.41%] [G loss: 5.056298]\n",
      "epoch:42 step:32813 [D loss: 0.345600, acc.: 82.81%] [G loss: 4.640770]\n",
      "epoch:42 step:32814 [D loss: 0.250375, acc.: 87.50%] [G loss: 4.541622]\n",
      "epoch:42 step:32815 [D loss: 0.277296, acc.: 87.50%] [G loss: 4.887600]\n",
      "epoch:42 step:32816 [D loss: 0.276660, acc.: 86.72%] [G loss: 3.785094]\n",
      "epoch:42 step:32817 [D loss: 0.295543, acc.: 84.38%] [G loss: 4.239285]\n",
      "epoch:42 step:32818 [D loss: 0.400050, acc.: 82.81%] [G loss: 3.116508]\n",
      "epoch:42 step:32819 [D loss: 0.147426, acc.: 96.09%] [G loss: 3.539282]\n",
      "epoch:42 step:32820 [D loss: 0.372550, acc.: 84.38%] [G loss: 4.074942]\n",
      "epoch:42 step:32821 [D loss: 0.346317, acc.: 83.59%] [G loss: 2.951432]\n",
      "epoch:42 step:32822 [D loss: 0.241100, acc.: 91.41%] [G loss: 3.590929]\n",
      "epoch:42 step:32823 [D loss: 0.345547, acc.: 82.03%] [G loss: 3.342925]\n",
      "epoch:42 step:32824 [D loss: 0.312853, acc.: 84.38%] [G loss: 2.948866]\n",
      "epoch:42 step:32825 [D loss: 0.288260, acc.: 89.84%] [G loss: 3.044971]\n",
      "epoch:42 step:32826 [D loss: 0.382334, acc.: 85.16%] [G loss: 3.893242]\n",
      "epoch:42 step:32827 [D loss: 0.343810, acc.: 83.59%] [G loss: 3.296905]\n",
      "epoch:42 step:32828 [D loss: 0.252409, acc.: 89.84%] [G loss: 4.213414]\n",
      "epoch:42 step:32829 [D loss: 0.293993, acc.: 87.50%] [G loss: 2.878052]\n",
      "epoch:42 step:32830 [D loss: 0.374853, acc.: 81.25%] [G loss: 3.952031]\n",
      "epoch:42 step:32831 [D loss: 0.299182, acc.: 85.94%] [G loss: 4.706923]\n",
      "epoch:42 step:32832 [D loss: 0.366315, acc.: 82.81%] [G loss: 4.447111]\n",
      "epoch:42 step:32833 [D loss: 0.330017, acc.: 82.81%] [G loss: 6.744035]\n",
      "epoch:42 step:32834 [D loss: 0.491843, acc.: 75.78%] [G loss: 4.214809]\n",
      "epoch:42 step:32835 [D loss: 0.249854, acc.: 89.84%] [G loss: 4.443508]\n",
      "epoch:42 step:32836 [D loss: 0.303716, acc.: 86.72%] [G loss: 3.277466]\n",
      "epoch:42 step:32837 [D loss: 0.272661, acc.: 87.50%] [G loss: 3.646215]\n",
      "epoch:42 step:32838 [D loss: 0.320987, acc.: 85.16%] [G loss: 3.877594]\n",
      "epoch:42 step:32839 [D loss: 0.209459, acc.: 92.97%] [G loss: 3.550011]\n",
      "epoch:42 step:32840 [D loss: 0.380782, acc.: 83.59%] [G loss: 3.678808]\n",
      "epoch:42 step:32841 [D loss: 0.271312, acc.: 87.50%] [G loss: 3.187025]\n",
      "epoch:42 step:32842 [D loss: 0.314542, acc.: 83.59%] [G loss: 3.810901]\n",
      "epoch:42 step:32843 [D loss: 0.325860, acc.: 86.72%] [G loss: 3.932811]\n",
      "epoch:42 step:32844 [D loss: 0.341615, acc.: 84.38%] [G loss: 3.298975]\n",
      "epoch:42 step:32845 [D loss: 0.304489, acc.: 84.38%] [G loss: 4.739000]\n",
      "epoch:42 step:32846 [D loss: 0.397011, acc.: 74.22%] [G loss: 5.094693]\n",
      "epoch:42 step:32847 [D loss: 0.577241, acc.: 73.44%] [G loss: 5.079396]\n",
      "epoch:42 step:32848 [D loss: 0.618609, acc.: 75.00%] [G loss: 3.777089]\n",
      "epoch:42 step:32849 [D loss: 0.367942, acc.: 83.59%] [G loss: 3.436691]\n",
      "epoch:42 step:32850 [D loss: 0.285996, acc.: 85.94%] [G loss: 3.680550]\n",
      "epoch:42 step:32851 [D loss: 0.255241, acc.: 89.06%] [G loss: 3.917483]\n",
      "epoch:42 step:32852 [D loss: 0.474638, acc.: 78.12%] [G loss: 3.832958]\n",
      "epoch:42 step:32853 [D loss: 0.613755, acc.: 77.34%] [G loss: 7.411728]\n",
      "epoch:42 step:32854 [D loss: 1.695715, acc.: 74.22%] [G loss: 8.197308]\n",
      "epoch:42 step:32855 [D loss: 0.569757, acc.: 83.59%] [G loss: 4.734891]\n",
      "epoch:42 step:32856 [D loss: 0.792888, acc.: 71.09%] [G loss: 3.242427]\n",
      "epoch:42 step:32857 [D loss: 0.176503, acc.: 92.97%] [G loss: 3.987306]\n",
      "epoch:42 step:32858 [D loss: 0.354580, acc.: 82.03%] [G loss: 2.877443]\n",
      "epoch:42 step:32859 [D loss: 0.338982, acc.: 82.81%] [G loss: 3.100234]\n",
      "epoch:42 step:32860 [D loss: 0.384935, acc.: 81.25%] [G loss: 3.014096]\n",
      "epoch:42 step:32861 [D loss: 0.337340, acc.: 82.03%] [G loss: 3.211459]\n",
      "epoch:42 step:32862 [D loss: 0.368725, acc.: 81.25%] [G loss: 2.984901]\n",
      "epoch:42 step:32863 [D loss: 0.383205, acc.: 82.81%] [G loss: 3.227949]\n",
      "epoch:42 step:32864 [D loss: 0.380915, acc.: 82.03%] [G loss: 2.612587]\n",
      "epoch:42 step:32865 [D loss: 0.211211, acc.: 92.19%] [G loss: 2.858635]\n",
      "epoch:42 step:32866 [D loss: 0.288934, acc.: 85.94%] [G loss: 2.680893]\n",
      "epoch:42 step:32867 [D loss: 0.449203, acc.: 74.22%] [G loss: 2.381591]\n",
      "epoch:42 step:32868 [D loss: 0.373526, acc.: 82.81%] [G loss: 2.587840]\n",
      "epoch:42 step:32869 [D loss: 0.321667, acc.: 85.94%] [G loss: 2.461223]\n",
      "epoch:42 step:32870 [D loss: 0.324758, acc.: 88.28%] [G loss: 2.588568]\n",
      "epoch:42 step:32871 [D loss: 0.385637, acc.: 80.47%] [G loss: 2.761931]\n",
      "epoch:42 step:32872 [D loss: 0.300643, acc.: 84.38%] [G loss: 2.795294]\n",
      "epoch:42 step:32873 [D loss: 0.360085, acc.: 81.25%] [G loss: 2.799419]\n",
      "epoch:42 step:32874 [D loss: 0.294176, acc.: 87.50%] [G loss: 2.934866]\n",
      "epoch:42 step:32875 [D loss: 0.388262, acc.: 82.81%] [G loss: 2.642981]\n",
      "epoch:42 step:32876 [D loss: 0.252800, acc.: 87.50%] [G loss: 2.556550]\n",
      "epoch:42 step:32877 [D loss: 0.276476, acc.: 90.62%] [G loss: 3.407445]\n",
      "epoch:42 step:32878 [D loss: 0.339108, acc.: 83.59%] [G loss: 2.727302]\n",
      "epoch:42 step:32879 [D loss: 0.348849, acc.: 87.50%] [G loss: 2.650517]\n",
      "epoch:42 step:32880 [D loss: 0.347738, acc.: 82.81%] [G loss: 2.509186]\n",
      "epoch:42 step:32881 [D loss: 0.313470, acc.: 87.50%] [G loss: 2.305448]\n",
      "epoch:42 step:32882 [D loss: 0.247099, acc.: 87.50%] [G loss: 3.332511]\n",
      "epoch:42 step:32883 [D loss: 0.293355, acc.: 86.72%] [G loss: 3.242382]\n",
      "epoch:42 step:32884 [D loss: 0.244629, acc.: 89.06%] [G loss: 2.714498]\n",
      "epoch:42 step:32885 [D loss: 0.281933, acc.: 89.06%] [G loss: 2.931623]\n",
      "epoch:42 step:32886 [D loss: 0.257589, acc.: 89.06%] [G loss: 3.349915]\n",
      "epoch:42 step:32887 [D loss: 0.266067, acc.: 88.28%] [G loss: 2.398529]\n",
      "epoch:42 step:32888 [D loss: 0.362832, acc.: 82.81%] [G loss: 2.937860]\n",
      "epoch:42 step:32889 [D loss: 0.281960, acc.: 88.28%] [G loss: 2.573168]\n",
      "epoch:42 step:32890 [D loss: 0.269690, acc.: 88.28%] [G loss: 2.647944]\n",
      "epoch:42 step:32891 [D loss: 0.344999, acc.: 85.16%] [G loss: 3.084039]\n",
      "epoch:42 step:32892 [D loss: 0.393539, acc.: 82.03%] [G loss: 2.500769]\n",
      "epoch:42 step:32893 [D loss: 0.300298, acc.: 87.50%] [G loss: 2.408227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:32894 [D loss: 0.262495, acc.: 89.84%] [G loss: 2.293463]\n",
      "epoch:42 step:32895 [D loss: 0.246713, acc.: 90.62%] [G loss: 2.156023]\n",
      "epoch:42 step:32896 [D loss: 0.307299, acc.: 85.16%] [G loss: 2.492884]\n",
      "epoch:42 step:32897 [D loss: 0.358893, acc.: 83.59%] [G loss: 2.919867]\n",
      "epoch:42 step:32898 [D loss: 0.307274, acc.: 83.59%] [G loss: 3.059568]\n",
      "epoch:42 step:32899 [D loss: 0.280746, acc.: 87.50%] [G loss: 2.903127]\n",
      "epoch:42 step:32900 [D loss: 0.262785, acc.: 89.06%] [G loss: 2.641347]\n",
      "epoch:42 step:32901 [D loss: 0.325739, acc.: 85.94%] [G loss: 3.093570]\n",
      "epoch:42 step:32902 [D loss: 0.280709, acc.: 85.94%] [G loss: 3.056410]\n",
      "epoch:42 step:32903 [D loss: 0.375347, acc.: 82.03%] [G loss: 2.762014]\n",
      "epoch:42 step:32904 [D loss: 0.269057, acc.: 89.06%] [G loss: 2.676617]\n",
      "epoch:42 step:32905 [D loss: 0.319807, acc.: 86.72%] [G loss: 2.839404]\n",
      "epoch:42 step:32906 [D loss: 0.406570, acc.: 82.03%] [G loss: 2.809126]\n",
      "epoch:42 step:32907 [D loss: 0.381560, acc.: 83.59%] [G loss: 2.495749]\n",
      "epoch:42 step:32908 [D loss: 0.307094, acc.: 86.72%] [G loss: 2.788070]\n",
      "epoch:42 step:32909 [D loss: 0.271497, acc.: 86.72%] [G loss: 3.360990]\n",
      "epoch:42 step:32910 [D loss: 0.284069, acc.: 87.50%] [G loss: 3.099161]\n",
      "epoch:42 step:32911 [D loss: 0.263527, acc.: 89.06%] [G loss: 2.357050]\n",
      "epoch:42 step:32912 [D loss: 0.288668, acc.: 88.28%] [G loss: 2.770956]\n",
      "epoch:42 step:32913 [D loss: 0.335327, acc.: 87.50%] [G loss: 2.952790]\n",
      "epoch:42 step:32914 [D loss: 0.308703, acc.: 86.72%] [G loss: 3.548077]\n",
      "epoch:42 step:32915 [D loss: 0.250678, acc.: 89.06%] [G loss: 3.034142]\n",
      "epoch:42 step:32916 [D loss: 0.331776, acc.: 85.16%] [G loss: 4.291678]\n",
      "epoch:42 step:32917 [D loss: 0.333611, acc.: 88.28%] [G loss: 3.077543]\n",
      "epoch:42 step:32918 [D loss: 0.396610, acc.: 83.59%] [G loss: 2.720994]\n",
      "epoch:42 step:32919 [D loss: 0.272492, acc.: 90.62%] [G loss: 3.269722]\n",
      "epoch:42 step:32920 [D loss: 0.241636, acc.: 87.50%] [G loss: 3.523036]\n",
      "epoch:42 step:32921 [D loss: 0.293601, acc.: 85.94%] [G loss: 3.554431]\n",
      "epoch:42 step:32922 [D loss: 0.280917, acc.: 85.94%] [G loss: 3.273364]\n",
      "epoch:42 step:32923 [D loss: 0.236229, acc.: 88.28%] [G loss: 3.140914]\n",
      "epoch:42 step:32924 [D loss: 0.201090, acc.: 90.62%] [G loss: 3.341943]\n",
      "epoch:42 step:32925 [D loss: 0.225247, acc.: 90.62%] [G loss: 2.676858]\n",
      "epoch:42 step:32926 [D loss: 0.281529, acc.: 88.28%] [G loss: 2.798064]\n",
      "epoch:42 step:32927 [D loss: 0.329318, acc.: 84.38%] [G loss: 2.884541]\n",
      "epoch:42 step:32928 [D loss: 0.294148, acc.: 88.28%] [G loss: 2.928380]\n",
      "epoch:42 step:32929 [D loss: 0.248749, acc.: 92.19%] [G loss: 3.061368]\n",
      "epoch:42 step:32930 [D loss: 0.266852, acc.: 89.84%] [G loss: 3.285532]\n",
      "epoch:42 step:32931 [D loss: 0.458119, acc.: 76.56%] [G loss: 3.391220]\n",
      "epoch:42 step:32932 [D loss: 0.377077, acc.: 80.47%] [G loss: 3.445306]\n",
      "epoch:42 step:32933 [D loss: 0.457930, acc.: 79.69%] [G loss: 4.009234]\n",
      "epoch:42 step:32934 [D loss: 0.272767, acc.: 88.28%] [G loss: 3.832121]\n",
      "epoch:42 step:32935 [D loss: 0.265222, acc.: 90.62%] [G loss: 3.872080]\n",
      "epoch:42 step:32936 [D loss: 0.391211, acc.: 82.03%] [G loss: 3.424499]\n",
      "epoch:42 step:32937 [D loss: 0.405097, acc.: 82.81%] [G loss: 2.828643]\n",
      "epoch:42 step:32938 [D loss: 0.311142, acc.: 88.28%] [G loss: 3.261201]\n",
      "epoch:42 step:32939 [D loss: 0.313929, acc.: 87.50%] [G loss: 2.987949]\n",
      "epoch:42 step:32940 [D loss: 0.225013, acc.: 91.41%] [G loss: 3.143177]\n",
      "epoch:42 step:32941 [D loss: 0.249233, acc.: 90.62%] [G loss: 3.055847]\n",
      "epoch:42 step:32942 [D loss: 0.290942, acc.: 86.72%] [G loss: 3.453066]\n",
      "epoch:42 step:32943 [D loss: 0.407283, acc.: 82.03%] [G loss: 3.648087]\n",
      "epoch:42 step:32944 [D loss: 0.309451, acc.: 85.16%] [G loss: 4.405046]\n",
      "epoch:42 step:32945 [D loss: 0.259591, acc.: 88.28%] [G loss: 3.084046]\n",
      "epoch:42 step:32946 [D loss: 0.403772, acc.: 76.56%] [G loss: 3.218088]\n",
      "epoch:42 step:32947 [D loss: 0.383260, acc.: 80.47%] [G loss: 3.865654]\n",
      "epoch:42 step:32948 [D loss: 0.379139, acc.: 78.91%] [G loss: 2.678302]\n",
      "epoch:42 step:32949 [D loss: 0.299763, acc.: 89.06%] [G loss: 2.647907]\n",
      "epoch:42 step:32950 [D loss: 0.337884, acc.: 86.72%] [G loss: 2.129161]\n",
      "epoch:42 step:32951 [D loss: 0.332548, acc.: 83.59%] [G loss: 2.762590]\n",
      "epoch:42 step:32952 [D loss: 0.396823, acc.: 80.47%] [G loss: 3.912550]\n",
      "epoch:42 step:32953 [D loss: 0.322334, acc.: 86.72%] [G loss: 2.704733]\n",
      "epoch:42 step:32954 [D loss: 0.438833, acc.: 78.91%] [G loss: 3.134434]\n",
      "epoch:42 step:32955 [D loss: 0.281859, acc.: 91.41%] [G loss: 3.141603]\n",
      "epoch:42 step:32956 [D loss: 0.326787, acc.: 85.94%] [G loss: 4.668679]\n",
      "epoch:42 step:32957 [D loss: 0.386122, acc.: 82.03%] [G loss: 3.436128]\n",
      "epoch:42 step:32958 [D loss: 0.276293, acc.: 85.94%] [G loss: 3.740870]\n",
      "epoch:42 step:32959 [D loss: 0.409968, acc.: 79.69%] [G loss: 3.440537]\n",
      "epoch:42 step:32960 [D loss: 0.256684, acc.: 87.50%] [G loss: 3.306520]\n",
      "epoch:42 step:32961 [D loss: 0.215266, acc.: 92.19%] [G loss: 3.877442]\n",
      "epoch:42 step:32962 [D loss: 0.360063, acc.: 83.59%] [G loss: 3.444392]\n",
      "epoch:42 step:32963 [D loss: 0.365577, acc.: 82.81%] [G loss: 3.760226]\n",
      "epoch:42 step:32964 [D loss: 0.250362, acc.: 88.28%] [G loss: 3.777910]\n",
      "epoch:42 step:32965 [D loss: 0.266113, acc.: 88.28%] [G loss: 4.018388]\n",
      "epoch:42 step:32966 [D loss: 0.196537, acc.: 93.75%] [G loss: 4.897561]\n",
      "epoch:42 step:32967 [D loss: 0.314206, acc.: 87.50%] [G loss: 3.651710]\n",
      "epoch:42 step:32968 [D loss: 0.283063, acc.: 88.28%] [G loss: 2.990806]\n",
      "epoch:42 step:32969 [D loss: 0.310416, acc.: 87.50%] [G loss: 4.271985]\n",
      "epoch:42 step:32970 [D loss: 0.298973, acc.: 86.72%] [G loss: 3.164028]\n",
      "epoch:42 step:32971 [D loss: 0.239952, acc.: 89.06%] [G loss: 3.133241]\n",
      "epoch:42 step:32972 [D loss: 0.266441, acc.: 89.06%] [G loss: 3.112092]\n",
      "epoch:42 step:32973 [D loss: 0.254511, acc.: 91.41%] [G loss: 4.050150]\n",
      "epoch:42 step:32974 [D loss: 0.373862, acc.: 78.91%] [G loss: 3.478335]\n",
      "epoch:42 step:32975 [D loss: 0.344323, acc.: 85.16%] [G loss: 4.638591]\n",
      "epoch:42 step:32976 [D loss: 0.382357, acc.: 85.16%] [G loss: 3.187089]\n",
      "epoch:42 step:32977 [D loss: 0.310561, acc.: 87.50%] [G loss: 2.370340]\n",
      "epoch:42 step:32978 [D loss: 0.474079, acc.: 78.12%] [G loss: 3.191626]\n",
      "epoch:42 step:32979 [D loss: 0.309334, acc.: 86.72%] [G loss: 2.905026]\n",
      "epoch:42 step:32980 [D loss: 0.333229, acc.: 84.38%] [G loss: 2.140617]\n",
      "epoch:42 step:32981 [D loss: 0.328779, acc.: 83.59%] [G loss: 3.016243]\n",
      "epoch:42 step:32982 [D loss: 0.326783, acc.: 85.16%] [G loss: 4.441743]\n",
      "epoch:42 step:32983 [D loss: 0.245448, acc.: 90.62%] [G loss: 3.776431]\n",
      "epoch:42 step:32984 [D loss: 0.442990, acc.: 78.91%] [G loss: 3.982091]\n",
      "epoch:42 step:32985 [D loss: 0.395048, acc.: 81.25%] [G loss: 5.455393]\n",
      "epoch:42 step:32986 [D loss: 0.422854, acc.: 79.69%] [G loss: 3.664124]\n",
      "epoch:42 step:32987 [D loss: 0.350223, acc.: 83.59%] [G loss: 3.635814]\n",
      "epoch:42 step:32988 [D loss: 0.347784, acc.: 84.38%] [G loss: 4.995111]\n",
      "epoch:42 step:32989 [D loss: 0.339987, acc.: 84.38%] [G loss: 4.266033]\n",
      "epoch:42 step:32990 [D loss: 0.348682, acc.: 85.16%] [G loss: 7.292488]\n",
      "epoch:42 step:32991 [D loss: 0.321600, acc.: 84.38%] [G loss: 4.935153]\n",
      "epoch:42 step:32992 [D loss: 0.303205, acc.: 84.38%] [G loss: 4.657214]\n",
      "epoch:42 step:32993 [D loss: 0.273756, acc.: 89.06%] [G loss: 5.114027]\n",
      "epoch:42 step:32994 [D loss: 0.252958, acc.: 86.72%] [G loss: 5.006176]\n",
      "epoch:42 step:32995 [D loss: 0.389786, acc.: 80.47%] [G loss: 3.778036]\n",
      "epoch:42 step:32996 [D loss: 0.348632, acc.: 88.28%] [G loss: 4.676152]\n",
      "epoch:42 step:32997 [D loss: 0.263317, acc.: 89.84%] [G loss: 2.849602]\n",
      "epoch:42 step:32998 [D loss: 0.294073, acc.: 88.28%] [G loss: 3.024967]\n",
      "epoch:42 step:32999 [D loss: 0.402470, acc.: 82.03%] [G loss: 3.761635]\n",
      "epoch:42 step:33000 [D loss: 0.290162, acc.: 88.28%] [G loss: 3.380902]\n",
      "epoch:42 step:33001 [D loss: 0.384282, acc.: 80.47%] [G loss: 3.139678]\n",
      "epoch:42 step:33002 [D loss: 0.415291, acc.: 81.25%] [G loss: 2.732759]\n",
      "epoch:42 step:33003 [D loss: 0.348122, acc.: 85.16%] [G loss: 2.545682]\n",
      "epoch:42 step:33004 [D loss: 0.377160, acc.: 80.47%] [G loss: 3.148413]\n",
      "epoch:42 step:33005 [D loss: 0.236759, acc.: 86.72%] [G loss: 2.453577]\n",
      "epoch:42 step:33006 [D loss: 0.361640, acc.: 82.81%] [G loss: 3.195638]\n",
      "epoch:42 step:33007 [D loss: 0.323888, acc.: 85.16%] [G loss: 4.373674]\n",
      "epoch:42 step:33008 [D loss: 0.440704, acc.: 80.47%] [G loss: 4.705359]\n",
      "epoch:42 step:33009 [D loss: 0.331631, acc.: 85.16%] [G loss: 4.403775]\n",
      "epoch:42 step:33010 [D loss: 0.320893, acc.: 85.16%] [G loss: 4.588396]\n",
      "epoch:42 step:33011 [D loss: 0.273763, acc.: 87.50%] [G loss: 4.447360]\n",
      "epoch:42 step:33012 [D loss: 0.272888, acc.: 87.50%] [G loss: 3.427675]\n",
      "epoch:42 step:33013 [D loss: 0.288145, acc.: 85.16%] [G loss: 3.907953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33014 [D loss: 0.323358, acc.: 82.03%] [G loss: 3.887656]\n",
      "epoch:42 step:33015 [D loss: 0.417524, acc.: 85.16%] [G loss: 8.719068]\n",
      "epoch:42 step:33016 [D loss: 0.429911, acc.: 80.47%] [G loss: 5.777894]\n",
      "epoch:42 step:33017 [D loss: 0.223473, acc.: 88.28%] [G loss: 5.452731]\n",
      "epoch:42 step:33018 [D loss: 0.355359, acc.: 83.59%] [G loss: 4.465762]\n",
      "epoch:42 step:33019 [D loss: 0.267772, acc.: 89.06%] [G loss: 3.644227]\n",
      "epoch:42 step:33020 [D loss: 0.244668, acc.: 88.28%] [G loss: 2.835893]\n",
      "epoch:42 step:33021 [D loss: 0.253709, acc.: 88.28%] [G loss: 2.815225]\n",
      "epoch:42 step:33022 [D loss: 0.346749, acc.: 82.81%] [G loss: 2.809500]\n",
      "epoch:42 step:33023 [D loss: 0.265245, acc.: 91.41%] [G loss: 3.246386]\n",
      "epoch:42 step:33024 [D loss: 0.323450, acc.: 85.16%] [G loss: 2.459116]\n",
      "epoch:42 step:33025 [D loss: 0.338087, acc.: 84.38%] [G loss: 3.749225]\n",
      "epoch:42 step:33026 [D loss: 0.377077, acc.: 83.59%] [G loss: 3.226246]\n",
      "epoch:42 step:33027 [D loss: 0.445195, acc.: 78.91%] [G loss: 2.915306]\n",
      "epoch:42 step:33028 [D loss: 0.378107, acc.: 81.25%] [G loss: 2.956199]\n",
      "epoch:42 step:33029 [D loss: 0.289681, acc.: 92.97%] [G loss: 2.887487]\n",
      "epoch:42 step:33030 [D loss: 0.383673, acc.: 82.81%] [G loss: 2.875172]\n",
      "epoch:42 step:33031 [D loss: 0.529085, acc.: 77.34%] [G loss: 2.792298]\n",
      "epoch:42 step:33032 [D loss: 0.406766, acc.: 82.81%] [G loss: 2.850800]\n",
      "epoch:42 step:33033 [D loss: 0.249926, acc.: 90.62%] [G loss: 3.411676]\n",
      "epoch:42 step:33034 [D loss: 0.301972, acc.: 86.72%] [G loss: 3.148613]\n",
      "epoch:42 step:33035 [D loss: 0.339866, acc.: 83.59%] [G loss: 4.065419]\n",
      "epoch:42 step:33036 [D loss: 0.321508, acc.: 85.94%] [G loss: 3.203245]\n",
      "epoch:42 step:33037 [D loss: 0.219173, acc.: 92.19%] [G loss: 3.272805]\n",
      "epoch:42 step:33038 [D loss: 0.281867, acc.: 85.16%] [G loss: 3.431216]\n",
      "epoch:42 step:33039 [D loss: 0.392655, acc.: 81.25%] [G loss: 2.538790]\n",
      "epoch:42 step:33040 [D loss: 0.288337, acc.: 88.28%] [G loss: 3.019575]\n",
      "epoch:42 step:33041 [D loss: 0.342932, acc.: 85.94%] [G loss: 2.831460]\n",
      "epoch:42 step:33042 [D loss: 0.355187, acc.: 85.94%] [G loss: 2.891364]\n",
      "epoch:42 step:33043 [D loss: 0.318824, acc.: 85.94%] [G loss: 2.716064]\n",
      "epoch:42 step:33044 [D loss: 0.297370, acc.: 86.72%] [G loss: 3.719821]\n",
      "epoch:42 step:33045 [D loss: 0.252737, acc.: 89.06%] [G loss: 2.302978]\n",
      "epoch:42 step:33046 [D loss: 0.368704, acc.: 82.81%] [G loss: 3.302445]\n",
      "epoch:42 step:33047 [D loss: 0.375514, acc.: 83.59%] [G loss: 3.599687]\n",
      "epoch:42 step:33048 [D loss: 0.355340, acc.: 86.72%] [G loss: 2.718647]\n",
      "epoch:42 step:33049 [D loss: 0.317043, acc.: 85.16%] [G loss: 2.716094]\n",
      "epoch:42 step:33050 [D loss: 0.371078, acc.: 85.94%] [G loss: 3.002766]\n",
      "epoch:42 step:33051 [D loss: 0.377799, acc.: 82.81%] [G loss: 3.218022]\n",
      "epoch:42 step:33052 [D loss: 0.393044, acc.: 83.59%] [G loss: 2.871985]\n",
      "epoch:42 step:33053 [D loss: 0.389346, acc.: 85.94%] [G loss: 3.088325]\n",
      "epoch:42 step:33054 [D loss: 0.252693, acc.: 87.50%] [G loss: 3.249049]\n",
      "epoch:42 step:33055 [D loss: 0.388462, acc.: 79.69%] [G loss: 2.696232]\n",
      "epoch:42 step:33056 [D loss: 0.324931, acc.: 81.25%] [G loss: 3.053519]\n",
      "epoch:42 step:33057 [D loss: 0.299612, acc.: 88.28%] [G loss: 3.236320]\n",
      "epoch:42 step:33058 [D loss: 0.445674, acc.: 79.69%] [G loss: 3.145469]\n",
      "epoch:42 step:33059 [D loss: 0.299481, acc.: 91.41%] [G loss: 3.164937]\n",
      "epoch:42 step:33060 [D loss: 0.247323, acc.: 86.72%] [G loss: 4.016494]\n",
      "epoch:42 step:33061 [D loss: 0.397101, acc.: 84.38%] [G loss: 4.444517]\n",
      "epoch:42 step:33062 [D loss: 0.357822, acc.: 80.47%] [G loss: 4.019771]\n",
      "epoch:42 step:33063 [D loss: 0.345078, acc.: 80.47%] [G loss: 5.449138]\n",
      "epoch:42 step:33064 [D loss: 0.268618, acc.: 92.19%] [G loss: 4.667937]\n",
      "epoch:42 step:33065 [D loss: 0.193610, acc.: 89.84%] [G loss: 6.001991]\n",
      "epoch:42 step:33066 [D loss: 0.221658, acc.: 88.28%] [G loss: 5.471734]\n",
      "epoch:42 step:33067 [D loss: 0.262453, acc.: 88.28%] [G loss: 5.375536]\n",
      "epoch:42 step:33068 [D loss: 0.285065, acc.: 86.72%] [G loss: 4.626698]\n",
      "epoch:42 step:33069 [D loss: 0.249913, acc.: 88.28%] [G loss: 3.787036]\n",
      "epoch:42 step:33070 [D loss: 0.264723, acc.: 85.94%] [G loss: 4.514378]\n",
      "epoch:42 step:33071 [D loss: 0.376035, acc.: 82.03%] [G loss: 4.398644]\n",
      "epoch:42 step:33072 [D loss: 0.355455, acc.: 81.25%] [G loss: 4.815683]\n",
      "epoch:42 step:33073 [D loss: 0.265867, acc.: 87.50%] [G loss: 3.631298]\n",
      "epoch:42 step:33074 [D loss: 0.427782, acc.: 78.91%] [G loss: 3.929714]\n",
      "epoch:42 step:33075 [D loss: 0.264244, acc.: 91.41%] [G loss: 3.347432]\n",
      "epoch:42 step:33076 [D loss: 0.394962, acc.: 84.38%] [G loss: 3.704124]\n",
      "epoch:42 step:33077 [D loss: 0.472071, acc.: 76.56%] [G loss: 2.573668]\n",
      "epoch:42 step:33078 [D loss: 0.343219, acc.: 84.38%] [G loss: 4.125167]\n",
      "epoch:42 step:33079 [D loss: 0.380852, acc.: 82.81%] [G loss: 3.188036]\n",
      "epoch:42 step:33080 [D loss: 0.318655, acc.: 85.16%] [G loss: 3.071127]\n",
      "epoch:42 step:33081 [D loss: 0.432650, acc.: 78.91%] [G loss: 3.832860]\n",
      "epoch:42 step:33082 [D loss: 0.243481, acc.: 89.06%] [G loss: 3.624334]\n",
      "epoch:42 step:33083 [D loss: 0.421473, acc.: 76.56%] [G loss: 3.535092]\n",
      "epoch:42 step:33084 [D loss: 0.383313, acc.: 82.81%] [G loss: 2.992704]\n",
      "epoch:42 step:33085 [D loss: 0.370138, acc.: 85.16%] [G loss: 3.146798]\n",
      "epoch:42 step:33086 [D loss: 0.425502, acc.: 83.59%] [G loss: 2.971972]\n",
      "epoch:42 step:33087 [D loss: 0.307686, acc.: 85.94%] [G loss: 3.313709]\n",
      "epoch:42 step:33088 [D loss: 0.402605, acc.: 81.25%] [G loss: 2.552610]\n",
      "epoch:42 step:33089 [D loss: 0.360959, acc.: 81.25%] [G loss: 2.703536]\n",
      "epoch:42 step:33090 [D loss: 0.369596, acc.: 82.81%] [G loss: 3.064126]\n",
      "epoch:42 step:33091 [D loss: 0.292582, acc.: 85.16%] [G loss: 2.832299]\n",
      "epoch:42 step:33092 [D loss: 0.358367, acc.: 84.38%] [G loss: 3.346001]\n",
      "epoch:42 step:33093 [D loss: 0.293890, acc.: 85.16%] [G loss: 3.777032]\n",
      "epoch:42 step:33094 [D loss: 0.211846, acc.: 93.75%] [G loss: 4.369349]\n",
      "epoch:42 step:33095 [D loss: 0.304961, acc.: 85.94%] [G loss: 3.178280]\n",
      "epoch:42 step:33096 [D loss: 0.340191, acc.: 87.50%] [G loss: 2.451133]\n",
      "epoch:42 step:33097 [D loss: 0.326864, acc.: 85.16%] [G loss: 3.064589]\n",
      "epoch:42 step:33098 [D loss: 0.357858, acc.: 86.72%] [G loss: 2.932138]\n",
      "epoch:42 step:33099 [D loss: 0.292767, acc.: 85.94%] [G loss: 3.336506]\n",
      "epoch:42 step:33100 [D loss: 0.328926, acc.: 86.72%] [G loss: 2.917686]\n",
      "epoch:42 step:33101 [D loss: 0.395896, acc.: 81.25%] [G loss: 3.476203]\n",
      "epoch:42 step:33102 [D loss: 0.448821, acc.: 79.69%] [G loss: 4.350760]\n",
      "epoch:42 step:33103 [D loss: 0.570296, acc.: 74.22%] [G loss: 6.704052]\n",
      "epoch:42 step:33104 [D loss: 1.112975, acc.: 68.75%] [G loss: 9.915281]\n",
      "epoch:42 step:33105 [D loss: 2.515654, acc.: 64.84%] [G loss: 6.303334]\n",
      "epoch:42 step:33106 [D loss: 0.726068, acc.: 78.91%] [G loss: 7.512441]\n",
      "epoch:42 step:33107 [D loss: 0.541084, acc.: 76.56%] [G loss: 4.597042]\n",
      "epoch:42 step:33108 [D loss: 0.563355, acc.: 77.34%] [G loss: 3.762515]\n",
      "epoch:42 step:33109 [D loss: 0.292871, acc.: 86.72%] [G loss: 6.318793]\n",
      "epoch:42 step:33110 [D loss: 0.504863, acc.: 79.69%] [G loss: 3.502827]\n",
      "epoch:42 step:33111 [D loss: 0.322123, acc.: 83.59%] [G loss: 3.802685]\n",
      "epoch:42 step:33112 [D loss: 0.256300, acc.: 88.28%] [G loss: 3.501460]\n",
      "epoch:42 step:33113 [D loss: 0.456239, acc.: 81.25%] [G loss: 2.994044]\n",
      "epoch:42 step:33114 [D loss: 0.302982, acc.: 83.59%] [G loss: 3.126761]\n",
      "epoch:42 step:33115 [D loss: 0.409208, acc.: 82.03%] [G loss: 3.130718]\n",
      "epoch:42 step:33116 [D loss: 0.311733, acc.: 85.16%] [G loss: 3.183808]\n",
      "epoch:42 step:33117 [D loss: 0.505078, acc.: 73.44%] [G loss: 2.522554]\n",
      "epoch:42 step:33118 [D loss: 0.223901, acc.: 92.19%] [G loss: 3.864703]\n",
      "epoch:42 step:33119 [D loss: 0.266984, acc.: 91.41%] [G loss: 3.443585]\n",
      "epoch:42 step:33120 [D loss: 0.265780, acc.: 87.50%] [G loss: 3.347354]\n",
      "epoch:42 step:33121 [D loss: 0.318449, acc.: 89.06%] [G loss: 3.195469]\n",
      "epoch:42 step:33122 [D loss: 0.362372, acc.: 82.81%] [G loss: 2.603581]\n",
      "epoch:42 step:33123 [D loss: 0.324378, acc.: 80.47%] [G loss: 2.708834]\n",
      "epoch:42 step:33124 [D loss: 0.355893, acc.: 84.38%] [G loss: 3.063998]\n",
      "epoch:42 step:33125 [D loss: 0.360885, acc.: 82.03%] [G loss: 2.612838]\n",
      "epoch:42 step:33126 [D loss: 0.410504, acc.: 77.34%] [G loss: 3.316576]\n",
      "epoch:42 step:33127 [D loss: 0.424565, acc.: 76.56%] [G loss: 2.841628]\n",
      "epoch:42 step:33128 [D loss: 0.425660, acc.: 82.03%] [G loss: 3.285507]\n",
      "epoch:42 step:33129 [D loss: 0.385685, acc.: 81.25%] [G loss: 2.836209]\n",
      "epoch:42 step:33130 [D loss: 0.366934, acc.: 82.03%] [G loss: 3.638444]\n",
      "epoch:42 step:33131 [D loss: 0.372489, acc.: 85.16%] [G loss: 3.222915]\n",
      "epoch:42 step:33132 [D loss: 0.276685, acc.: 86.72%] [G loss: 3.409989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33133 [D loss: 0.286243, acc.: 87.50%] [G loss: 2.635124]\n",
      "epoch:42 step:33134 [D loss: 0.261505, acc.: 89.84%] [G loss: 3.334706]\n",
      "epoch:42 step:33135 [D loss: 0.302583, acc.: 82.81%] [G loss: 2.648113]\n",
      "epoch:42 step:33136 [D loss: 0.309294, acc.: 89.06%] [G loss: 2.950001]\n",
      "epoch:42 step:33137 [D loss: 0.268942, acc.: 89.84%] [G loss: 2.286341]\n",
      "epoch:42 step:33138 [D loss: 0.281255, acc.: 85.94%] [G loss: 2.909009]\n",
      "epoch:42 step:33139 [D loss: 0.287724, acc.: 86.72%] [G loss: 3.214080]\n",
      "epoch:42 step:33140 [D loss: 0.301334, acc.: 89.84%] [G loss: 2.972628]\n",
      "epoch:42 step:33141 [D loss: 0.325968, acc.: 85.94%] [G loss: 2.739542]\n",
      "epoch:42 step:33142 [D loss: 0.267063, acc.: 88.28%] [G loss: 3.467268]\n",
      "epoch:42 step:33143 [D loss: 0.228749, acc.: 89.84%] [G loss: 3.280984]\n",
      "epoch:42 step:33144 [D loss: 0.361312, acc.: 85.94%] [G loss: 3.372310]\n",
      "epoch:42 step:33145 [D loss: 0.387979, acc.: 82.03%] [G loss: 5.053261]\n",
      "epoch:42 step:33146 [D loss: 0.399680, acc.: 83.59%] [G loss: 3.684466]\n",
      "epoch:42 step:33147 [D loss: 0.230372, acc.: 88.28%] [G loss: 3.244059]\n",
      "epoch:42 step:33148 [D loss: 0.237635, acc.: 89.84%] [G loss: 3.295549]\n",
      "epoch:42 step:33149 [D loss: 0.271077, acc.: 85.16%] [G loss: 2.963694]\n",
      "epoch:42 step:33150 [D loss: 0.349035, acc.: 81.25%] [G loss: 3.546542]\n",
      "epoch:42 step:33151 [D loss: 0.264589, acc.: 89.06%] [G loss: 5.011111]\n",
      "epoch:42 step:33152 [D loss: 0.359802, acc.: 85.16%] [G loss: 4.140996]\n",
      "epoch:42 step:33153 [D loss: 0.303481, acc.: 89.06%] [G loss: 4.460864]\n",
      "epoch:42 step:33154 [D loss: 0.281743, acc.: 86.72%] [G loss: 3.338535]\n",
      "epoch:42 step:33155 [D loss: 0.308830, acc.: 88.28%] [G loss: 5.362678]\n",
      "epoch:42 step:33156 [D loss: 0.223881, acc.: 91.41%] [G loss: 4.848013]\n",
      "epoch:42 step:33157 [D loss: 0.243352, acc.: 89.84%] [G loss: 4.434179]\n",
      "epoch:42 step:33158 [D loss: 0.326274, acc.: 84.38%] [G loss: 5.583673]\n",
      "epoch:42 step:33159 [D loss: 0.275903, acc.: 88.28%] [G loss: 4.178081]\n",
      "epoch:42 step:33160 [D loss: 0.356165, acc.: 83.59%] [G loss: 3.690314]\n",
      "epoch:42 step:33161 [D loss: 0.310434, acc.: 87.50%] [G loss: 3.347543]\n",
      "epoch:42 step:33162 [D loss: 0.298992, acc.: 85.16%] [G loss: 2.875852]\n",
      "epoch:42 step:33163 [D loss: 0.373052, acc.: 85.94%] [G loss: 3.090493]\n",
      "epoch:42 step:33164 [D loss: 0.340015, acc.: 85.94%] [G loss: 2.575045]\n",
      "epoch:42 step:33165 [D loss: 0.297049, acc.: 87.50%] [G loss: 2.644348]\n",
      "epoch:42 step:33166 [D loss: 0.345147, acc.: 82.03%] [G loss: 2.451203]\n",
      "epoch:42 step:33167 [D loss: 0.289064, acc.: 85.16%] [G loss: 2.924683]\n",
      "epoch:42 step:33168 [D loss: 0.494937, acc.: 79.69%] [G loss: 4.107608]\n",
      "epoch:42 step:33169 [D loss: 0.266491, acc.: 89.06%] [G loss: 4.042689]\n",
      "epoch:42 step:33170 [D loss: 0.396672, acc.: 85.16%] [G loss: 3.087590]\n",
      "epoch:42 step:33171 [D loss: 0.258273, acc.: 88.28%] [G loss: 3.410134]\n",
      "epoch:42 step:33172 [D loss: 0.381097, acc.: 83.59%] [G loss: 2.798110]\n",
      "epoch:42 step:33173 [D loss: 0.326940, acc.: 85.94%] [G loss: 3.586406]\n",
      "epoch:42 step:33174 [D loss: 0.478571, acc.: 77.34%] [G loss: 2.874841]\n",
      "epoch:42 step:33175 [D loss: 0.402989, acc.: 84.38%] [G loss: 2.966045]\n",
      "epoch:42 step:33176 [D loss: 0.356581, acc.: 86.72%] [G loss: 3.093645]\n",
      "epoch:42 step:33177 [D loss: 0.455897, acc.: 75.00%] [G loss: 2.461504]\n",
      "epoch:42 step:33178 [D loss: 0.338487, acc.: 84.38%] [G loss: 4.481496]\n",
      "epoch:42 step:33179 [D loss: 0.322464, acc.: 84.38%] [G loss: 3.178484]\n",
      "epoch:42 step:33180 [D loss: 0.334932, acc.: 84.38%] [G loss: 4.966998]\n",
      "epoch:42 step:33181 [D loss: 0.458372, acc.: 80.47%] [G loss: 3.036418]\n",
      "epoch:42 step:33182 [D loss: 0.334980, acc.: 82.03%] [G loss: 2.866861]\n",
      "epoch:42 step:33183 [D loss: 0.272441, acc.: 91.41%] [G loss: 2.715798]\n",
      "epoch:42 step:33184 [D loss: 0.319534, acc.: 85.94%] [G loss: 2.944704]\n",
      "epoch:42 step:33185 [D loss: 0.304777, acc.: 88.28%] [G loss: 2.398821]\n",
      "epoch:42 step:33186 [D loss: 0.259068, acc.: 90.62%] [G loss: 2.660119]\n",
      "epoch:42 step:33187 [D loss: 0.284839, acc.: 88.28%] [G loss: 2.332379]\n",
      "epoch:42 step:33188 [D loss: 0.333326, acc.: 85.94%] [G loss: 2.350789]\n",
      "epoch:42 step:33189 [D loss: 0.337583, acc.: 85.94%] [G loss: 2.528489]\n",
      "epoch:42 step:33190 [D loss: 0.387809, acc.: 83.59%] [G loss: 2.796478]\n",
      "epoch:42 step:33191 [D loss: 0.362195, acc.: 82.81%] [G loss: 2.885676]\n",
      "epoch:42 step:33192 [D loss: 0.322799, acc.: 85.94%] [G loss: 2.694572]\n",
      "epoch:42 step:33193 [D loss: 0.391991, acc.: 83.59%] [G loss: 2.486691]\n",
      "epoch:42 step:33194 [D loss: 0.273770, acc.: 91.41%] [G loss: 3.198712]\n",
      "epoch:42 step:33195 [D loss: 0.315565, acc.: 85.94%] [G loss: 2.324278]\n",
      "epoch:42 step:33196 [D loss: 0.346132, acc.: 86.72%] [G loss: 2.508429]\n",
      "epoch:42 step:33197 [D loss: 0.295856, acc.: 82.03%] [G loss: 3.404354]\n",
      "epoch:42 step:33198 [D loss: 0.333461, acc.: 87.50%] [G loss: 2.870185]\n",
      "epoch:42 step:33199 [D loss: 0.276072, acc.: 86.72%] [G loss: 3.062521]\n",
      "epoch:42 step:33200 [D loss: 0.443039, acc.: 78.91%] [G loss: 2.727770]\n",
      "epoch:42 step:33201 [D loss: 0.309812, acc.: 86.72%] [G loss: 2.971290]\n",
      "epoch:42 step:33202 [D loss: 0.353553, acc.: 83.59%] [G loss: 2.132694]\n",
      "epoch:42 step:33203 [D loss: 0.244219, acc.: 90.62%] [G loss: 2.466979]\n",
      "epoch:42 step:33204 [D loss: 0.357558, acc.: 83.59%] [G loss: 2.745348]\n",
      "epoch:42 step:33205 [D loss: 0.357926, acc.: 85.16%] [G loss: 2.437598]\n",
      "epoch:42 step:33206 [D loss: 0.329285, acc.: 89.06%] [G loss: 2.763547]\n",
      "epoch:42 step:33207 [D loss: 0.423036, acc.: 81.25%] [G loss: 2.690001]\n",
      "epoch:42 step:33208 [D loss: 0.373146, acc.: 79.69%] [G loss: 2.580510]\n",
      "epoch:42 step:33209 [D loss: 0.299897, acc.: 86.72%] [G loss: 2.755588]\n",
      "epoch:42 step:33210 [D loss: 0.323359, acc.: 83.59%] [G loss: 2.162262]\n",
      "epoch:42 step:33211 [D loss: 0.326410, acc.: 85.94%] [G loss: 2.919446]\n",
      "epoch:42 step:33212 [D loss: 0.299436, acc.: 87.50%] [G loss: 3.048777]\n",
      "epoch:42 step:33213 [D loss: 0.288897, acc.: 87.50%] [G loss: 2.551753]\n",
      "epoch:42 step:33214 [D loss: 0.322486, acc.: 85.16%] [G loss: 2.396263]\n",
      "epoch:42 step:33215 [D loss: 0.312959, acc.: 85.94%] [G loss: 2.235448]\n",
      "epoch:42 step:33216 [D loss: 0.419389, acc.: 78.91%] [G loss: 2.685603]\n",
      "epoch:42 step:33217 [D loss: 0.422506, acc.: 82.03%] [G loss: 3.500010]\n",
      "epoch:42 step:33218 [D loss: 0.422266, acc.: 78.12%] [G loss: 2.707804]\n",
      "epoch:42 step:33219 [D loss: 0.318772, acc.: 85.94%] [G loss: 4.094427]\n",
      "epoch:42 step:33220 [D loss: 0.317139, acc.: 85.94%] [G loss: 2.951004]\n",
      "epoch:42 step:33221 [D loss: 0.319421, acc.: 85.16%] [G loss: 3.780667]\n",
      "epoch:42 step:33222 [D loss: 0.457492, acc.: 76.56%] [G loss: 3.211344]\n",
      "epoch:42 step:33223 [D loss: 0.247457, acc.: 90.62%] [G loss: 3.593438]\n",
      "epoch:42 step:33224 [D loss: 0.332858, acc.: 84.38%] [G loss: 3.136089]\n",
      "epoch:42 step:33225 [D loss: 0.297239, acc.: 85.16%] [G loss: 2.870474]\n",
      "epoch:42 step:33226 [D loss: 0.369463, acc.: 83.59%] [G loss: 3.594366]\n",
      "epoch:42 step:33227 [D loss: 0.290188, acc.: 87.50%] [G loss: 3.308316]\n",
      "epoch:42 step:33228 [D loss: 0.254399, acc.: 90.62%] [G loss: 3.609519]\n",
      "epoch:42 step:33229 [D loss: 0.240552, acc.: 89.06%] [G loss: 2.997734]\n",
      "epoch:42 step:33230 [D loss: 0.286335, acc.: 87.50%] [G loss: 2.824667]\n",
      "epoch:42 step:33231 [D loss: 0.396445, acc.: 83.59%] [G loss: 2.626393]\n",
      "epoch:42 step:33232 [D loss: 0.302718, acc.: 87.50%] [G loss: 2.734823]\n",
      "epoch:42 step:33233 [D loss: 0.347847, acc.: 82.81%] [G loss: 2.405427]\n",
      "epoch:42 step:33234 [D loss: 0.302407, acc.: 85.94%] [G loss: 2.784186]\n",
      "epoch:42 step:33235 [D loss: 0.224381, acc.: 91.41%] [G loss: 3.532627]\n",
      "epoch:42 step:33236 [D loss: 0.266062, acc.: 88.28%] [G loss: 3.988444]\n",
      "epoch:42 step:33237 [D loss: 0.349542, acc.: 85.16%] [G loss: 3.082785]\n",
      "epoch:42 step:33238 [D loss: 0.329287, acc.: 83.59%] [G loss: 2.471254]\n",
      "epoch:42 step:33239 [D loss: 0.234846, acc.: 92.19%] [G loss: 3.672799]\n",
      "epoch:42 step:33240 [D loss: 0.326974, acc.: 85.94%] [G loss: 4.691817]\n",
      "epoch:42 step:33241 [D loss: 0.317130, acc.: 86.72%] [G loss: 3.289655]\n",
      "epoch:42 step:33242 [D loss: 0.256625, acc.: 87.50%] [G loss: 4.183990]\n",
      "epoch:42 step:33243 [D loss: 0.348187, acc.: 85.16%] [G loss: 3.076024]\n",
      "epoch:42 step:33244 [D loss: 0.276053, acc.: 88.28%] [G loss: 2.805286]\n",
      "epoch:42 step:33245 [D loss: 0.299030, acc.: 89.84%] [G loss: 3.096423]\n",
      "epoch:42 step:33246 [D loss: 0.310916, acc.: 86.72%] [G loss: 3.171341]\n",
      "epoch:42 step:33247 [D loss: 0.381745, acc.: 81.25%] [G loss: 3.328935]\n",
      "epoch:42 step:33248 [D loss: 0.333938, acc.: 86.72%] [G loss: 2.932782]\n",
      "epoch:42 step:33249 [D loss: 0.281865, acc.: 89.84%] [G loss: 3.204139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33250 [D loss: 0.278033, acc.: 85.94%] [G loss: 4.154739]\n",
      "epoch:42 step:33251 [D loss: 0.325971, acc.: 83.59%] [G loss: 3.524893]\n",
      "epoch:42 step:33252 [D loss: 0.300807, acc.: 85.94%] [G loss: 3.144041]\n",
      "epoch:42 step:33253 [D loss: 0.267056, acc.: 89.06%] [G loss: 2.842479]\n",
      "epoch:42 step:33254 [D loss: 0.294956, acc.: 87.50%] [G loss: 3.022091]\n",
      "epoch:42 step:33255 [D loss: 0.285310, acc.: 87.50%] [G loss: 3.500574]\n",
      "epoch:42 step:33256 [D loss: 0.269497, acc.: 87.50%] [G loss: 4.414277]\n",
      "epoch:42 step:33257 [D loss: 0.363764, acc.: 85.16%] [G loss: 4.489842]\n",
      "epoch:42 step:33258 [D loss: 0.476632, acc.: 82.03%] [G loss: 9.223066]\n",
      "epoch:42 step:33259 [D loss: 1.300727, acc.: 64.84%] [G loss: 6.043808]\n",
      "epoch:42 step:33260 [D loss: 1.589549, acc.: 57.81%] [G loss: 4.067616]\n",
      "epoch:42 step:33261 [D loss: 0.568774, acc.: 76.56%] [G loss: 3.118705]\n",
      "epoch:42 step:33262 [D loss: 0.473865, acc.: 77.34%] [G loss: 5.844262]\n",
      "epoch:42 step:33263 [D loss: 0.447651, acc.: 87.50%] [G loss: 5.315645]\n",
      "epoch:42 step:33264 [D loss: 0.393321, acc.: 77.34%] [G loss: 3.065826]\n",
      "epoch:42 step:33265 [D loss: 0.344552, acc.: 84.38%] [G loss: 6.362396]\n",
      "epoch:42 step:33266 [D loss: 0.313757, acc.: 87.50%] [G loss: 4.184807]\n",
      "epoch:42 step:33267 [D loss: 0.273436, acc.: 88.28%] [G loss: 3.445852]\n",
      "epoch:42 step:33268 [D loss: 0.302684, acc.: 84.38%] [G loss: 3.369652]\n",
      "epoch:42 step:33269 [D loss: 0.383414, acc.: 83.59%] [G loss: 3.423742]\n",
      "epoch:42 step:33270 [D loss: 0.312335, acc.: 88.28%] [G loss: 3.408675]\n",
      "epoch:42 step:33271 [D loss: 0.343325, acc.: 85.94%] [G loss: 3.436777]\n",
      "epoch:42 step:33272 [D loss: 0.334076, acc.: 85.94%] [G loss: 3.416490]\n",
      "epoch:42 step:33273 [D loss: 0.295685, acc.: 86.72%] [G loss: 2.997203]\n",
      "epoch:42 step:33274 [D loss: 0.407467, acc.: 79.69%] [G loss: 2.914986]\n",
      "epoch:42 step:33275 [D loss: 0.289744, acc.: 87.50%] [G loss: 2.789109]\n",
      "epoch:42 step:33276 [D loss: 0.425825, acc.: 79.69%] [G loss: 2.843271]\n",
      "epoch:42 step:33277 [D loss: 0.307206, acc.: 88.28%] [G loss: 2.616569]\n",
      "epoch:42 step:33278 [D loss: 0.311693, acc.: 85.16%] [G loss: 3.157266]\n",
      "epoch:42 step:33279 [D loss: 0.267820, acc.: 91.41%] [G loss: 2.591897]\n",
      "epoch:42 step:33280 [D loss: 0.331109, acc.: 87.50%] [G loss: 2.734890]\n",
      "epoch:42 step:33281 [D loss: 0.376068, acc.: 84.38%] [G loss: 3.456370]\n",
      "epoch:42 step:33282 [D loss: 0.263592, acc.: 86.72%] [G loss: 2.923670]\n",
      "epoch:42 step:33283 [D loss: 0.329997, acc.: 89.06%] [G loss: 3.791181]\n",
      "epoch:42 step:33284 [D loss: 0.328978, acc.: 85.16%] [G loss: 2.939450]\n",
      "epoch:42 step:33285 [D loss: 0.334554, acc.: 82.81%] [G loss: 2.967619]\n",
      "epoch:42 step:33286 [D loss: 0.250114, acc.: 85.16%] [G loss: 2.949031]\n",
      "epoch:42 step:33287 [D loss: 0.256482, acc.: 87.50%] [G loss: 2.605457]\n",
      "epoch:42 step:33288 [D loss: 0.265081, acc.: 86.72%] [G loss: 2.463760]\n",
      "epoch:42 step:33289 [D loss: 0.304171, acc.: 88.28%] [G loss: 3.408237]\n",
      "epoch:42 step:33290 [D loss: 0.513502, acc.: 74.22%] [G loss: 2.358462]\n",
      "epoch:42 step:33291 [D loss: 0.437941, acc.: 82.81%] [G loss: 2.526011]\n",
      "epoch:42 step:33292 [D loss: 0.243963, acc.: 92.19%] [G loss: 2.536545]\n",
      "epoch:42 step:33293 [D loss: 0.366942, acc.: 82.81%] [G loss: 2.204086]\n",
      "epoch:42 step:33294 [D loss: 0.490162, acc.: 75.78%] [G loss: 2.684386]\n",
      "epoch:42 step:33295 [D loss: 0.398427, acc.: 78.91%] [G loss: 2.792844]\n",
      "epoch:42 step:33296 [D loss: 0.442184, acc.: 79.69%] [G loss: 4.608738]\n",
      "epoch:42 step:33297 [D loss: 0.495946, acc.: 75.78%] [G loss: 2.257430]\n",
      "epoch:42 step:33298 [D loss: 0.257851, acc.: 86.72%] [G loss: 3.407310]\n",
      "epoch:42 step:33299 [D loss: 0.369084, acc.: 82.81%] [G loss: 2.898157]\n",
      "epoch:42 step:33300 [D loss: 0.279018, acc.: 88.28%] [G loss: 2.882330]\n",
      "epoch:42 step:33301 [D loss: 0.445477, acc.: 77.34%] [G loss: 2.567091]\n",
      "epoch:42 step:33302 [D loss: 0.425503, acc.: 78.91%] [G loss: 2.007509]\n",
      "epoch:42 step:33303 [D loss: 0.413726, acc.: 76.56%] [G loss: 2.819781]\n",
      "epoch:42 step:33304 [D loss: 0.337942, acc.: 85.94%] [G loss: 2.936752]\n",
      "epoch:42 step:33305 [D loss: 0.317974, acc.: 85.16%] [G loss: 3.175231]\n",
      "epoch:42 step:33306 [D loss: 0.264307, acc.: 88.28%] [G loss: 3.149109]\n",
      "epoch:42 step:33307 [D loss: 0.229808, acc.: 90.62%] [G loss: 3.069780]\n",
      "epoch:42 step:33308 [D loss: 0.285806, acc.: 88.28%] [G loss: 3.345330]\n",
      "epoch:42 step:33309 [D loss: 0.374263, acc.: 84.38%] [G loss: 4.137349]\n",
      "epoch:42 step:33310 [D loss: 0.322374, acc.: 82.81%] [G loss: 3.197188]\n",
      "epoch:42 step:33311 [D loss: 0.383095, acc.: 81.25%] [G loss: 3.931859]\n",
      "epoch:42 step:33312 [D loss: 0.298147, acc.: 85.16%] [G loss: 3.587876]\n",
      "epoch:42 step:33313 [D loss: 0.363233, acc.: 82.81%] [G loss: 3.413303]\n",
      "epoch:42 step:33314 [D loss: 0.299441, acc.: 89.84%] [G loss: 3.650527]\n",
      "epoch:42 step:33315 [D loss: 0.302707, acc.: 88.28%] [G loss: 3.445568]\n",
      "epoch:42 step:33316 [D loss: 0.296230, acc.: 85.94%] [G loss: 4.056690]\n",
      "epoch:42 step:33317 [D loss: 0.429582, acc.: 79.69%] [G loss: 2.577660]\n",
      "epoch:42 step:33318 [D loss: 0.310913, acc.: 85.16%] [G loss: 3.641546]\n",
      "epoch:42 step:33319 [D loss: 0.313515, acc.: 88.28%] [G loss: 2.864814]\n",
      "epoch:42 step:33320 [D loss: 0.267514, acc.: 86.72%] [G loss: 3.858776]\n",
      "epoch:42 step:33321 [D loss: 0.396227, acc.: 79.69%] [G loss: 3.715294]\n",
      "epoch:42 step:33322 [D loss: 0.331198, acc.: 82.81%] [G loss: 2.773258]\n",
      "epoch:42 step:33323 [D loss: 0.314352, acc.: 87.50%] [G loss: 2.649183]\n",
      "epoch:42 step:33324 [D loss: 0.363295, acc.: 85.94%] [G loss: 2.411096]\n",
      "epoch:42 step:33325 [D loss: 0.322549, acc.: 88.28%] [G loss: 2.559900]\n",
      "epoch:42 step:33326 [D loss: 0.288015, acc.: 85.94%] [G loss: 2.122721]\n",
      "epoch:42 step:33327 [D loss: 0.311363, acc.: 88.28%] [G loss: 2.619578]\n",
      "epoch:42 step:33328 [D loss: 0.306170, acc.: 88.28%] [G loss: 2.532799]\n",
      "epoch:42 step:33329 [D loss: 0.227293, acc.: 91.41%] [G loss: 2.937093]\n",
      "epoch:42 step:33330 [D loss: 0.406903, acc.: 79.69%] [G loss: 2.788397]\n",
      "epoch:42 step:33331 [D loss: 0.281574, acc.: 88.28%] [G loss: 3.324960]\n",
      "epoch:42 step:33332 [D loss: 0.322597, acc.: 89.84%] [G loss: 3.030633]\n",
      "epoch:42 step:33333 [D loss: 0.233047, acc.: 89.06%] [G loss: 4.014789]\n",
      "epoch:42 step:33334 [D loss: 0.233376, acc.: 90.62%] [G loss: 2.589183]\n",
      "epoch:42 step:33335 [D loss: 0.264009, acc.: 89.06%] [G loss: 2.711530]\n",
      "epoch:42 step:33336 [D loss: 0.262552, acc.: 89.06%] [G loss: 3.102687]\n",
      "epoch:42 step:33337 [D loss: 0.293441, acc.: 89.06%] [G loss: 2.899300]\n",
      "epoch:42 step:33338 [D loss: 0.390365, acc.: 84.38%] [G loss: 2.905290]\n",
      "epoch:42 step:33339 [D loss: 0.389766, acc.: 81.25%] [G loss: 3.036530]\n",
      "epoch:42 step:33340 [D loss: 0.385219, acc.: 82.81%] [G loss: 3.218035]\n",
      "epoch:42 step:33341 [D loss: 0.362293, acc.: 80.47%] [G loss: 3.357235]\n",
      "epoch:42 step:33342 [D loss: 0.256992, acc.: 91.41%] [G loss: 3.069592]\n",
      "epoch:42 step:33343 [D loss: 0.287124, acc.: 87.50%] [G loss: 3.325963]\n",
      "epoch:42 step:33344 [D loss: 0.339662, acc.: 83.59%] [G loss: 2.832683]\n",
      "epoch:42 step:33345 [D loss: 0.332865, acc.: 83.59%] [G loss: 3.420177]\n",
      "epoch:42 step:33346 [D loss: 0.281597, acc.: 89.84%] [G loss: 2.946822]\n",
      "epoch:42 step:33347 [D loss: 0.343852, acc.: 81.25%] [G loss: 2.974034]\n",
      "epoch:42 step:33348 [D loss: 0.215721, acc.: 87.50%] [G loss: 2.649464]\n",
      "epoch:42 step:33349 [D loss: 0.324502, acc.: 85.94%] [G loss: 3.474529]\n",
      "epoch:42 step:33350 [D loss: 0.258141, acc.: 89.84%] [G loss: 3.022645]\n",
      "epoch:42 step:33351 [D loss: 0.332719, acc.: 84.38%] [G loss: 3.149801]\n",
      "epoch:42 step:33352 [D loss: 0.338981, acc.: 84.38%] [G loss: 2.943962]\n",
      "epoch:42 step:33353 [D loss: 0.314926, acc.: 86.72%] [G loss: 2.928442]\n",
      "epoch:42 step:33354 [D loss: 0.318631, acc.: 85.16%] [G loss: 5.878971]\n",
      "epoch:42 step:33355 [D loss: 0.519869, acc.: 80.47%] [G loss: 6.291873]\n",
      "epoch:42 step:33356 [D loss: 0.353496, acc.: 83.59%] [G loss: 3.633710]\n",
      "epoch:42 step:33357 [D loss: 0.334853, acc.: 82.81%] [G loss: 10.814564]\n",
      "epoch:42 step:33358 [D loss: 0.262820, acc.: 89.06%] [G loss: 8.060288]\n",
      "epoch:42 step:33359 [D loss: 0.347275, acc.: 82.81%] [G loss: 3.759181]\n",
      "epoch:42 step:33360 [D loss: 0.363268, acc.: 81.25%] [G loss: 4.333251]\n",
      "epoch:42 step:33361 [D loss: 0.182781, acc.: 93.75%] [G loss: 4.629853]\n",
      "epoch:42 step:33362 [D loss: 0.371822, acc.: 82.03%] [G loss: 3.738915]\n",
      "epoch:42 step:33363 [D loss: 0.328966, acc.: 85.94%] [G loss: 6.465486]\n",
      "epoch:42 step:33364 [D loss: 0.466445, acc.: 80.47%] [G loss: 3.678165]\n",
      "epoch:42 step:33365 [D loss: 0.269521, acc.: 90.62%] [G loss: 4.161170]\n",
      "epoch:42 step:33366 [D loss: 0.282161, acc.: 83.59%] [G loss: 2.820522]\n",
      "epoch:42 step:33367 [D loss: 0.309098, acc.: 89.06%] [G loss: 3.372380]\n",
      "epoch:42 step:33368 [D loss: 0.331688, acc.: 84.38%] [G loss: 3.089978]\n",
      "epoch:42 step:33369 [D loss: 0.406761, acc.: 79.69%] [G loss: 2.902643]\n",
      "epoch:42 step:33370 [D loss: 0.334009, acc.: 85.94%] [G loss: 3.089510]\n",
      "epoch:42 step:33371 [D loss: 0.324706, acc.: 87.50%] [G loss: 3.739784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33372 [D loss: 0.277636, acc.: 87.50%] [G loss: 2.958010]\n",
      "epoch:42 step:33373 [D loss: 0.429258, acc.: 79.69%] [G loss: 2.793384]\n",
      "epoch:42 step:33374 [D loss: 0.297560, acc.: 88.28%] [G loss: 3.132233]\n",
      "epoch:42 step:33375 [D loss: 0.391946, acc.: 80.47%] [G loss: 3.011107]\n",
      "epoch:42 step:33376 [D loss: 0.310319, acc.: 89.84%] [G loss: 3.202671]\n",
      "epoch:42 step:33377 [D loss: 0.268159, acc.: 91.41%] [G loss: 2.741232]\n",
      "epoch:42 step:33378 [D loss: 0.323670, acc.: 88.28%] [G loss: 4.032596]\n",
      "epoch:42 step:33379 [D loss: 0.359483, acc.: 83.59%] [G loss: 2.092343]\n",
      "epoch:42 step:33380 [D loss: 0.254353, acc.: 89.84%] [G loss: 4.053087]\n",
      "epoch:42 step:33381 [D loss: 0.196363, acc.: 94.53%] [G loss: 3.789514]\n",
      "epoch:42 step:33382 [D loss: 0.270518, acc.: 89.06%] [G loss: 3.223032]\n",
      "epoch:42 step:33383 [D loss: 0.383737, acc.: 79.69%] [G loss: 3.176115]\n",
      "epoch:42 step:33384 [D loss: 0.228041, acc.: 90.62%] [G loss: 3.416381]\n",
      "epoch:42 step:33385 [D loss: 0.303627, acc.: 87.50%] [G loss: 2.970769]\n",
      "epoch:42 step:33386 [D loss: 0.289264, acc.: 85.94%] [G loss: 3.879353]\n",
      "epoch:42 step:33387 [D loss: 0.256761, acc.: 90.62%] [G loss: 3.415067]\n",
      "epoch:42 step:33388 [D loss: 0.338443, acc.: 85.94%] [G loss: 3.323460]\n",
      "epoch:42 step:33389 [D loss: 0.205313, acc.: 92.97%] [G loss: 3.669054]\n",
      "epoch:42 step:33390 [D loss: 0.397071, acc.: 82.81%] [G loss: 3.479547]\n",
      "epoch:42 step:33391 [D loss: 0.242053, acc.: 89.84%] [G loss: 4.723843]\n",
      "epoch:42 step:33392 [D loss: 0.372988, acc.: 81.25%] [G loss: 3.500986]\n",
      "epoch:42 step:33393 [D loss: 0.288165, acc.: 86.72%] [G loss: 3.582491]\n",
      "epoch:42 step:33394 [D loss: 0.315508, acc.: 88.28%] [G loss: 2.822948]\n",
      "epoch:42 step:33395 [D loss: 0.361574, acc.: 83.59%] [G loss: 3.305568]\n",
      "epoch:42 step:33396 [D loss: 0.303200, acc.: 85.16%] [G loss: 3.325871]\n",
      "epoch:42 step:33397 [D loss: 0.383355, acc.: 83.59%] [G loss: 3.818178]\n",
      "epoch:42 step:33398 [D loss: 0.352517, acc.: 86.72%] [G loss: 2.313345]\n",
      "epoch:42 step:33399 [D loss: 0.343411, acc.: 85.94%] [G loss: 2.701636]\n",
      "epoch:42 step:33400 [D loss: 0.336445, acc.: 84.38%] [G loss: 2.945707]\n",
      "epoch:42 step:33401 [D loss: 0.296382, acc.: 87.50%] [G loss: 4.765119]\n",
      "epoch:42 step:33402 [D loss: 0.268799, acc.: 89.84%] [G loss: 3.598615]\n",
      "epoch:42 step:33403 [D loss: 0.387200, acc.: 85.94%] [G loss: 3.238855]\n",
      "epoch:42 step:33404 [D loss: 0.263266, acc.: 86.72%] [G loss: 3.330802]\n",
      "epoch:42 step:33405 [D loss: 0.295673, acc.: 85.16%] [G loss: 2.609399]\n",
      "epoch:42 step:33406 [D loss: 0.283730, acc.: 87.50%] [G loss: 2.722022]\n",
      "epoch:42 step:33407 [D loss: 0.400093, acc.: 82.81%] [G loss: 2.412025]\n",
      "epoch:42 step:33408 [D loss: 0.400143, acc.: 79.69%] [G loss: 3.158924]\n",
      "epoch:42 step:33409 [D loss: 0.288166, acc.: 85.94%] [G loss: 3.002788]\n",
      "epoch:42 step:33410 [D loss: 0.348967, acc.: 82.03%] [G loss: 2.395058]\n",
      "epoch:42 step:33411 [D loss: 0.320334, acc.: 86.72%] [G loss: 3.106125]\n",
      "epoch:42 step:33412 [D loss: 0.381705, acc.: 79.69%] [G loss: 3.022419]\n",
      "epoch:42 step:33413 [D loss: 0.408410, acc.: 83.59%] [G loss: 3.233928]\n",
      "epoch:42 step:33414 [D loss: 0.412004, acc.: 81.25%] [G loss: 3.307378]\n",
      "epoch:42 step:33415 [D loss: 0.310142, acc.: 86.72%] [G loss: 3.686317]\n",
      "epoch:42 step:33416 [D loss: 0.261431, acc.: 91.41%] [G loss: 3.326401]\n",
      "epoch:42 step:33417 [D loss: 0.263207, acc.: 87.50%] [G loss: 5.126480]\n",
      "epoch:42 step:33418 [D loss: 0.306259, acc.: 85.16%] [G loss: 3.947547]\n",
      "epoch:42 step:33419 [D loss: 0.239313, acc.: 88.28%] [G loss: 3.982335]\n",
      "epoch:42 step:33420 [D loss: 0.351487, acc.: 82.81%] [G loss: 4.257071]\n",
      "epoch:42 step:33421 [D loss: 0.241660, acc.: 89.84%] [G loss: 3.924287]\n",
      "epoch:42 step:33422 [D loss: 0.261028, acc.: 89.06%] [G loss: 3.946258]\n",
      "epoch:42 step:33423 [D loss: 0.366324, acc.: 83.59%] [G loss: 5.628163]\n",
      "epoch:42 step:33424 [D loss: 0.333966, acc.: 83.59%] [G loss: 3.497792]\n",
      "epoch:42 step:33425 [D loss: 0.244157, acc.: 90.62%] [G loss: 3.922051]\n",
      "epoch:42 step:33426 [D loss: 0.308063, acc.: 88.28%] [G loss: 4.362211]\n",
      "epoch:42 step:33427 [D loss: 0.363333, acc.: 81.25%] [G loss: 2.572771]\n",
      "epoch:42 step:33428 [D loss: 0.332428, acc.: 85.94%] [G loss: 2.813796]\n",
      "epoch:42 step:33429 [D loss: 0.257552, acc.: 90.62%] [G loss: 4.288722]\n",
      "epoch:42 step:33430 [D loss: 0.433228, acc.: 82.03%] [G loss: 3.391455]\n",
      "epoch:42 step:33431 [D loss: 0.348052, acc.: 85.94%] [G loss: 3.480703]\n",
      "epoch:42 step:33432 [D loss: 0.213064, acc.: 92.19%] [G loss: 4.249701]\n",
      "epoch:42 step:33433 [D loss: 0.313854, acc.: 85.16%] [G loss: 3.066211]\n",
      "epoch:42 step:33434 [D loss: 0.335196, acc.: 84.38%] [G loss: 3.058316]\n",
      "epoch:42 step:33435 [D loss: 0.350854, acc.: 82.81%] [G loss: 3.357340]\n",
      "epoch:42 step:33436 [D loss: 0.337726, acc.: 88.28%] [G loss: 3.019091]\n",
      "epoch:42 step:33437 [D loss: 0.300905, acc.: 90.62%] [G loss: 3.563555]\n",
      "epoch:42 step:33438 [D loss: 0.209089, acc.: 94.53%] [G loss: 3.687124]\n",
      "epoch:42 step:33439 [D loss: 0.278966, acc.: 89.06%] [G loss: 3.000328]\n",
      "epoch:42 step:33440 [D loss: 0.237984, acc.: 89.84%] [G loss: 3.407730]\n",
      "epoch:42 step:33441 [D loss: 0.340126, acc.: 84.38%] [G loss: 3.268840]\n",
      "epoch:42 step:33442 [D loss: 0.241185, acc.: 89.06%] [G loss: 3.381225]\n",
      "epoch:42 step:33443 [D loss: 0.349788, acc.: 80.47%] [G loss: 3.940996]\n",
      "epoch:42 step:33444 [D loss: 0.267121, acc.: 86.72%] [G loss: 3.380538]\n",
      "epoch:42 step:33445 [D loss: 0.290070, acc.: 86.72%] [G loss: 3.000814]\n",
      "epoch:42 step:33446 [D loss: 0.325379, acc.: 82.81%] [G loss: 3.305743]\n",
      "epoch:42 step:33447 [D loss: 0.289961, acc.: 89.06%] [G loss: 3.054087]\n",
      "epoch:42 step:33448 [D loss: 0.448752, acc.: 77.34%] [G loss: 4.786798]\n",
      "epoch:42 step:33449 [D loss: 0.427609, acc.: 82.03%] [G loss: 4.175416]\n",
      "epoch:42 step:33450 [D loss: 0.474605, acc.: 71.88%] [G loss: 4.577442]\n",
      "epoch:42 step:33451 [D loss: 0.441681, acc.: 81.25%] [G loss: 3.406889]\n",
      "epoch:42 step:33452 [D loss: 0.366375, acc.: 81.25%] [G loss: 3.892179]\n",
      "epoch:42 step:33453 [D loss: 0.328338, acc.: 86.72%] [G loss: 3.315595]\n",
      "epoch:42 step:33454 [D loss: 0.339272, acc.: 85.94%] [G loss: 4.304809]\n",
      "epoch:42 step:33455 [D loss: 0.340498, acc.: 82.81%] [G loss: 3.580503]\n",
      "epoch:42 step:33456 [D loss: 0.317917, acc.: 85.94%] [G loss: 5.318925]\n",
      "epoch:42 step:33457 [D loss: 0.257380, acc.: 89.84%] [G loss: 3.113781]\n",
      "epoch:42 step:33458 [D loss: 0.291783, acc.: 87.50%] [G loss: 4.246225]\n",
      "epoch:42 step:33459 [D loss: 0.402995, acc.: 82.81%] [G loss: 4.355830]\n",
      "epoch:42 step:33460 [D loss: 0.305944, acc.: 84.38%] [G loss: 3.162584]\n",
      "epoch:42 step:33461 [D loss: 0.329814, acc.: 86.72%] [G loss: 3.350010]\n",
      "epoch:42 step:33462 [D loss: 0.374087, acc.: 80.47%] [G loss: 5.635095]\n",
      "epoch:42 step:33463 [D loss: 0.699802, acc.: 74.22%] [G loss: 5.420847]\n",
      "epoch:42 step:33464 [D loss: 0.588020, acc.: 74.22%] [G loss: 3.234351]\n",
      "epoch:42 step:33465 [D loss: 0.428142, acc.: 78.91%] [G loss: 3.628289]\n",
      "epoch:42 step:33466 [D loss: 0.427869, acc.: 78.91%] [G loss: 3.046827]\n",
      "epoch:42 step:33467 [D loss: 0.364455, acc.: 87.50%] [G loss: 2.755391]\n",
      "epoch:42 step:33468 [D loss: 0.320520, acc.: 89.84%] [G loss: 4.182882]\n",
      "epoch:42 step:33469 [D loss: 0.258807, acc.: 88.28%] [G loss: 3.349725]\n",
      "epoch:42 step:33470 [D loss: 0.451337, acc.: 79.69%] [G loss: 2.898279]\n",
      "epoch:42 step:33471 [D loss: 0.324750, acc.: 85.94%] [G loss: 3.017590]\n",
      "epoch:42 step:33472 [D loss: 0.339556, acc.: 85.94%] [G loss: 2.854100]\n",
      "epoch:42 step:33473 [D loss: 0.310143, acc.: 85.94%] [G loss: 3.461937]\n",
      "epoch:42 step:33474 [D loss: 0.311562, acc.: 85.16%] [G loss: 3.414274]\n",
      "epoch:42 step:33475 [D loss: 0.338596, acc.: 83.59%] [G loss: 3.216937]\n",
      "epoch:42 step:33476 [D loss: 0.352672, acc.: 86.72%] [G loss: 2.745400]\n",
      "epoch:42 step:33477 [D loss: 0.274037, acc.: 87.50%] [G loss: 2.719460]\n",
      "epoch:42 step:33478 [D loss: 0.210507, acc.: 93.75%] [G loss: 3.065255]\n",
      "epoch:42 step:33479 [D loss: 0.314664, acc.: 83.59%] [G loss: 3.023909]\n",
      "epoch:42 step:33480 [D loss: 0.333367, acc.: 86.72%] [G loss: 3.056955]\n",
      "epoch:42 step:33481 [D loss: 0.311228, acc.: 84.38%] [G loss: 5.582827]\n",
      "epoch:42 step:33482 [D loss: 0.491887, acc.: 78.91%] [G loss: 8.341883]\n",
      "epoch:42 step:33483 [D loss: 0.745920, acc.: 73.44%] [G loss: 3.414054]\n",
      "epoch:42 step:33484 [D loss: 0.587621, acc.: 67.97%] [G loss: 5.082796]\n",
      "epoch:42 step:33485 [D loss: 0.389999, acc.: 82.03%] [G loss: 4.565450]\n",
      "epoch:42 step:33486 [D loss: 0.308199, acc.: 85.16%] [G loss: 5.015604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33487 [D loss: 0.398960, acc.: 82.03%] [G loss: 3.925872]\n",
      "epoch:42 step:33488 [D loss: 0.182395, acc.: 92.19%] [G loss: 4.717313]\n",
      "epoch:42 step:33489 [D loss: 0.416306, acc.: 81.25%] [G loss: 3.681983]\n",
      "epoch:42 step:33490 [D loss: 0.340067, acc.: 84.38%] [G loss: 3.710696]\n",
      "epoch:42 step:33491 [D loss: 0.322939, acc.: 85.16%] [G loss: 4.336277]\n",
      "epoch:42 step:33492 [D loss: 0.219499, acc.: 89.84%] [G loss: 3.448907]\n",
      "epoch:42 step:33493 [D loss: 0.259775, acc.: 86.72%] [G loss: 3.132168]\n",
      "epoch:42 step:33494 [D loss: 0.333788, acc.: 85.16%] [G loss: 2.180166]\n",
      "epoch:42 step:33495 [D loss: 0.295163, acc.: 86.72%] [G loss: 3.136949]\n",
      "epoch:42 step:33496 [D loss: 0.223679, acc.: 87.50%] [G loss: 4.039598]\n",
      "epoch:42 step:33497 [D loss: 0.265524, acc.: 84.38%] [G loss: 4.693065]\n",
      "epoch:42 step:33498 [D loss: 0.363187, acc.: 84.38%] [G loss: 4.771090]\n",
      "epoch:42 step:33499 [D loss: 0.305396, acc.: 89.84%] [G loss: 3.880087]\n",
      "epoch:42 step:33500 [D loss: 0.363178, acc.: 85.16%] [G loss: 3.380977]\n",
      "epoch:42 step:33501 [D loss: 0.270239, acc.: 90.62%] [G loss: 3.752847]\n",
      "epoch:42 step:33502 [D loss: 0.303227, acc.: 89.06%] [G loss: 2.545153]\n",
      "epoch:42 step:33503 [D loss: 0.250929, acc.: 89.84%] [G loss: 3.158313]\n",
      "epoch:42 step:33504 [D loss: 0.328264, acc.: 84.38%] [G loss: 3.198850]\n",
      "epoch:42 step:33505 [D loss: 0.378426, acc.: 78.91%] [G loss: 2.903721]\n",
      "epoch:42 step:33506 [D loss: 0.325705, acc.: 84.38%] [G loss: 3.444079]\n",
      "epoch:42 step:33507 [D loss: 0.341387, acc.: 85.16%] [G loss: 2.981984]\n",
      "epoch:42 step:33508 [D loss: 0.382510, acc.: 84.38%] [G loss: 3.619138]\n",
      "epoch:42 step:33509 [D loss: 0.385853, acc.: 82.03%] [G loss: 3.293838]\n",
      "epoch:42 step:33510 [D loss: 0.307482, acc.: 87.50%] [G loss: 2.982555]\n",
      "epoch:42 step:33511 [D loss: 0.293091, acc.: 88.28%] [G loss: 3.458593]\n",
      "epoch:42 step:33512 [D loss: 0.363826, acc.: 86.72%] [G loss: 3.469080]\n",
      "epoch:42 step:33513 [D loss: 0.414065, acc.: 78.91%] [G loss: 3.108441]\n",
      "epoch:42 step:33514 [D loss: 0.379007, acc.: 83.59%] [G loss: 2.602612]\n",
      "epoch:42 step:33515 [D loss: 0.325778, acc.: 87.50%] [G loss: 3.119291]\n",
      "epoch:42 step:33516 [D loss: 0.245240, acc.: 90.62%] [G loss: 3.203149]\n",
      "epoch:42 step:33517 [D loss: 0.345603, acc.: 80.47%] [G loss: 3.204621]\n",
      "epoch:42 step:33518 [D loss: 0.375643, acc.: 85.94%] [G loss: 3.037893]\n",
      "epoch:42 step:33519 [D loss: 0.343499, acc.: 83.59%] [G loss: 4.399250]\n",
      "epoch:42 step:33520 [D loss: 0.308381, acc.: 88.28%] [G loss: 3.139774]\n",
      "epoch:42 step:33521 [D loss: 0.286422, acc.: 91.41%] [G loss: 4.291041]\n",
      "epoch:42 step:33522 [D loss: 0.360589, acc.: 79.69%] [G loss: 3.812003]\n",
      "epoch:42 step:33523 [D loss: 0.272870, acc.: 88.28%] [G loss: 3.438272]\n",
      "epoch:42 step:33524 [D loss: 0.286688, acc.: 88.28%] [G loss: 3.687256]\n",
      "epoch:42 step:33525 [D loss: 0.262415, acc.: 89.06%] [G loss: 3.473464]\n",
      "epoch:42 step:33526 [D loss: 0.362168, acc.: 84.38%] [G loss: 2.623324]\n",
      "epoch:42 step:33527 [D loss: 0.231937, acc.: 91.41%] [G loss: 2.943126]\n",
      "epoch:42 step:33528 [D loss: 0.289332, acc.: 89.06%] [G loss: 3.648848]\n",
      "epoch:42 step:33529 [D loss: 0.340695, acc.: 86.72%] [G loss: 3.234577]\n",
      "epoch:42 step:33530 [D loss: 0.449573, acc.: 80.47%] [G loss: 3.711363]\n",
      "epoch:42 step:33531 [D loss: 0.406841, acc.: 78.91%] [G loss: 3.018801]\n",
      "epoch:42 step:33532 [D loss: 0.308601, acc.: 88.28%] [G loss: 3.104671]\n",
      "epoch:42 step:33533 [D loss: 0.289624, acc.: 89.06%] [G loss: 2.779656]\n",
      "epoch:42 step:33534 [D loss: 0.344265, acc.: 85.94%] [G loss: 2.879323]\n",
      "epoch:42 step:33535 [D loss: 0.311374, acc.: 87.50%] [G loss: 2.886042]\n",
      "epoch:42 step:33536 [D loss: 0.310483, acc.: 89.84%] [G loss: 3.433748]\n",
      "epoch:42 step:33537 [D loss: 0.370713, acc.: 80.47%] [G loss: 2.340935]\n",
      "epoch:42 step:33538 [D loss: 0.287188, acc.: 89.06%] [G loss: 2.832991]\n",
      "epoch:42 step:33539 [D loss: 0.217334, acc.: 93.75%] [G loss: 2.842515]\n",
      "epoch:42 step:33540 [D loss: 0.386973, acc.: 80.47%] [G loss: 5.531176]\n",
      "epoch:42 step:33541 [D loss: 0.433565, acc.: 78.91%] [G loss: 2.962340]\n",
      "epoch:42 step:33542 [D loss: 0.342986, acc.: 85.16%] [G loss: 3.939181]\n",
      "epoch:42 step:33543 [D loss: 0.296234, acc.: 85.94%] [G loss: 2.892176]\n",
      "epoch:42 step:33544 [D loss: 0.257322, acc.: 88.28%] [G loss: 3.795682]\n",
      "epoch:42 step:33545 [D loss: 0.275063, acc.: 90.62%] [G loss: 3.002918]\n",
      "epoch:42 step:33546 [D loss: 0.267274, acc.: 89.84%] [G loss: 3.100043]\n",
      "epoch:42 step:33547 [D loss: 0.415604, acc.: 77.34%] [G loss: 3.816673]\n",
      "epoch:42 step:33548 [D loss: 0.303792, acc.: 86.72%] [G loss: 4.104443]\n",
      "epoch:42 step:33549 [D loss: 0.322227, acc.: 86.72%] [G loss: 4.394265]\n",
      "epoch:42 step:33550 [D loss: 0.310853, acc.: 85.94%] [G loss: 3.562119]\n",
      "epoch:42 step:33551 [D loss: 0.294380, acc.: 89.06%] [G loss: 2.434015]\n",
      "epoch:42 step:33552 [D loss: 0.292044, acc.: 85.16%] [G loss: 2.760461]\n",
      "epoch:42 step:33553 [D loss: 0.311308, acc.: 85.16%] [G loss: 2.876256]\n",
      "epoch:42 step:33554 [D loss: 0.476781, acc.: 82.03%] [G loss: 2.899628]\n",
      "epoch:42 step:33555 [D loss: 0.317428, acc.: 86.72%] [G loss: 3.568465]\n",
      "epoch:42 step:33556 [D loss: 0.274994, acc.: 88.28%] [G loss: 3.263974]\n",
      "epoch:42 step:33557 [D loss: 0.229948, acc.: 94.53%] [G loss: 4.140458]\n",
      "epoch:42 step:33558 [D loss: 0.249060, acc.: 87.50%] [G loss: 4.883715]\n",
      "epoch:42 step:33559 [D loss: 0.274544, acc.: 90.62%] [G loss: 2.892625]\n",
      "epoch:42 step:33560 [D loss: 0.310461, acc.: 86.72%] [G loss: 3.186819]\n",
      "epoch:42 step:33561 [D loss: 0.359462, acc.: 82.03%] [G loss: 2.705064]\n",
      "epoch:42 step:33562 [D loss: 0.300107, acc.: 85.16%] [G loss: 3.797968]\n",
      "epoch:42 step:33563 [D loss: 0.317950, acc.: 86.72%] [G loss: 5.016042]\n",
      "epoch:42 step:33564 [D loss: 0.324545, acc.: 88.28%] [G loss: 3.572524]\n",
      "epoch:42 step:33565 [D loss: 0.297911, acc.: 85.94%] [G loss: 4.081978]\n",
      "epoch:42 step:33566 [D loss: 0.287758, acc.: 90.62%] [G loss: 3.108760]\n",
      "epoch:42 step:33567 [D loss: 0.411088, acc.: 82.03%] [G loss: 3.535432]\n",
      "epoch:42 step:33568 [D loss: 0.275639, acc.: 87.50%] [G loss: 4.645940]\n",
      "epoch:42 step:33569 [D loss: 0.256064, acc.: 89.06%] [G loss: 3.736044]\n",
      "epoch:42 step:33570 [D loss: 0.280799, acc.: 89.84%] [G loss: 3.203647]\n",
      "epoch:42 step:33571 [D loss: 0.256989, acc.: 89.84%] [G loss: 3.149639]\n",
      "epoch:42 step:33572 [D loss: 0.299502, acc.: 85.16%] [G loss: 3.868719]\n",
      "epoch:42 step:33573 [D loss: 0.388658, acc.: 81.25%] [G loss: 3.387887]\n",
      "epoch:42 step:33574 [D loss: 0.321088, acc.: 84.38%] [G loss: 3.021521]\n",
      "epoch:42 step:33575 [D loss: 0.325439, acc.: 84.38%] [G loss: 3.247293]\n",
      "epoch:42 step:33576 [D loss: 0.348595, acc.: 80.47%] [G loss: 3.149055]\n",
      "epoch:42 step:33577 [D loss: 0.235597, acc.: 89.06%] [G loss: 2.998741]\n",
      "epoch:42 step:33578 [D loss: 0.245127, acc.: 91.41%] [G loss: 2.749052]\n",
      "epoch:42 step:33579 [D loss: 0.381660, acc.: 83.59%] [G loss: 2.815557]\n",
      "epoch:42 step:33580 [D loss: 0.355730, acc.: 85.16%] [G loss: 2.406775]\n",
      "epoch:42 step:33581 [D loss: 0.256899, acc.: 89.06%] [G loss: 3.026233]\n",
      "epoch:42 step:33582 [D loss: 0.272157, acc.: 87.50%] [G loss: 2.571507]\n",
      "epoch:42 step:33583 [D loss: 0.317994, acc.: 87.50%] [G loss: 2.932178]\n",
      "epoch:43 step:33584 [D loss: 0.343132, acc.: 82.03%] [G loss: 3.470930]\n",
      "epoch:43 step:33585 [D loss: 0.275403, acc.: 88.28%] [G loss: 4.667886]\n",
      "epoch:43 step:33586 [D loss: 0.338561, acc.: 85.94%] [G loss: 2.818088]\n",
      "epoch:43 step:33587 [D loss: 0.287721, acc.: 85.16%] [G loss: 3.105196]\n",
      "epoch:43 step:33588 [D loss: 0.223332, acc.: 93.75%] [G loss: 3.013271]\n",
      "epoch:43 step:33589 [D loss: 0.304357, acc.: 86.72%] [G loss: 2.819642]\n",
      "epoch:43 step:33590 [D loss: 0.254651, acc.: 87.50%] [G loss: 3.562444]\n",
      "epoch:43 step:33591 [D loss: 0.288927, acc.: 87.50%] [G loss: 3.084764]\n",
      "epoch:43 step:33592 [D loss: 0.384398, acc.: 78.12%] [G loss: 2.968680]\n",
      "epoch:43 step:33593 [D loss: 0.353349, acc.: 82.81%] [G loss: 3.930510]\n",
      "epoch:43 step:33594 [D loss: 0.262803, acc.: 89.06%] [G loss: 2.815252]\n",
      "epoch:43 step:33595 [D loss: 0.304283, acc.: 88.28%] [G loss: 3.027079]\n",
      "epoch:43 step:33596 [D loss: 0.338491, acc.: 82.03%] [G loss: 1.798781]\n",
      "epoch:43 step:33597 [D loss: 0.343230, acc.: 82.03%] [G loss: 3.155250]\n",
      "epoch:43 step:33598 [D loss: 0.308210, acc.: 82.03%] [G loss: 3.349380]\n",
      "epoch:43 step:33599 [D loss: 0.323644, acc.: 86.72%] [G loss: 2.452873]\n",
      "epoch:43 step:33600 [D loss: 0.265252, acc.: 89.84%] [G loss: 2.484773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33601 [D loss: 0.469463, acc.: 77.34%] [G loss: 3.146120]\n",
      "epoch:43 step:33602 [D loss: 0.432469, acc.: 83.59%] [G loss: 2.862798]\n",
      "epoch:43 step:33603 [D loss: 0.334535, acc.: 83.59%] [G loss: 3.281613]\n",
      "epoch:43 step:33604 [D loss: 0.426072, acc.: 81.25%] [G loss: 3.191797]\n",
      "epoch:43 step:33605 [D loss: 0.440214, acc.: 80.47%] [G loss: 3.903183]\n",
      "epoch:43 step:33606 [D loss: 0.330780, acc.: 82.03%] [G loss: 3.070945]\n",
      "epoch:43 step:33607 [D loss: 0.342600, acc.: 83.59%] [G loss: 3.774029]\n",
      "epoch:43 step:33608 [D loss: 0.301360, acc.: 92.19%] [G loss: 3.258520]\n",
      "epoch:43 step:33609 [D loss: 0.300395, acc.: 88.28%] [G loss: 3.667734]\n",
      "epoch:43 step:33610 [D loss: 0.324767, acc.: 84.38%] [G loss: 3.555087]\n",
      "epoch:43 step:33611 [D loss: 0.255548, acc.: 85.94%] [G loss: 3.860729]\n",
      "epoch:43 step:33612 [D loss: 0.359688, acc.: 80.47%] [G loss: 3.390090]\n",
      "epoch:43 step:33613 [D loss: 0.287058, acc.: 88.28%] [G loss: 3.558688]\n",
      "epoch:43 step:33614 [D loss: 0.476530, acc.: 76.56%] [G loss: 2.487801]\n",
      "epoch:43 step:33615 [D loss: 0.363178, acc.: 81.25%] [G loss: 3.054452]\n",
      "epoch:43 step:33616 [D loss: 0.347638, acc.: 86.72%] [G loss: 3.539171]\n",
      "epoch:43 step:33617 [D loss: 0.323414, acc.: 82.81%] [G loss: 2.214690]\n",
      "epoch:43 step:33618 [D loss: 0.278253, acc.: 87.50%] [G loss: 3.552057]\n",
      "epoch:43 step:33619 [D loss: 0.293153, acc.: 89.84%] [G loss: 3.106964]\n",
      "epoch:43 step:33620 [D loss: 0.289866, acc.: 85.94%] [G loss: 3.675084]\n",
      "epoch:43 step:33621 [D loss: 0.289219, acc.: 85.94%] [G loss: 2.852593]\n",
      "epoch:43 step:33622 [D loss: 0.274673, acc.: 87.50%] [G loss: 3.202815]\n",
      "epoch:43 step:33623 [D loss: 0.431975, acc.: 80.47%] [G loss: 2.788568]\n",
      "epoch:43 step:33624 [D loss: 0.377104, acc.: 82.03%] [G loss: 3.700806]\n",
      "epoch:43 step:33625 [D loss: 0.314056, acc.: 84.38%] [G loss: 2.879538]\n",
      "epoch:43 step:33626 [D loss: 0.381455, acc.: 80.47%] [G loss: 3.658643]\n",
      "epoch:43 step:33627 [D loss: 0.453606, acc.: 81.25%] [G loss: 5.396333]\n",
      "epoch:43 step:33628 [D loss: 0.461607, acc.: 85.16%] [G loss: 4.250031]\n",
      "epoch:43 step:33629 [D loss: 0.327053, acc.: 83.59%] [G loss: 3.767821]\n",
      "epoch:43 step:33630 [D loss: 0.375724, acc.: 82.81%] [G loss: 3.060642]\n",
      "epoch:43 step:33631 [D loss: 0.218494, acc.: 92.19%] [G loss: 3.424993]\n",
      "epoch:43 step:33632 [D loss: 0.212300, acc.: 92.19%] [G loss: 3.947436]\n",
      "epoch:43 step:33633 [D loss: 0.502287, acc.: 71.88%] [G loss: 3.882173]\n",
      "epoch:43 step:33634 [D loss: 0.259073, acc.: 91.41%] [G loss: 2.822035]\n",
      "epoch:43 step:33635 [D loss: 0.327511, acc.: 84.38%] [G loss: 3.851414]\n",
      "epoch:43 step:33636 [D loss: 0.276825, acc.: 85.16%] [G loss: 3.697425]\n",
      "epoch:43 step:33637 [D loss: 0.235050, acc.: 92.19%] [G loss: 3.594336]\n",
      "epoch:43 step:33638 [D loss: 0.313962, acc.: 83.59%] [G loss: 4.320779]\n",
      "epoch:43 step:33639 [D loss: 0.295116, acc.: 82.81%] [G loss: 3.036324]\n",
      "epoch:43 step:33640 [D loss: 0.325315, acc.: 82.81%] [G loss: 4.468137]\n",
      "epoch:43 step:33641 [D loss: 0.321842, acc.: 85.94%] [G loss: 2.277663]\n",
      "epoch:43 step:33642 [D loss: 0.250213, acc.: 89.06%] [G loss: 3.350913]\n",
      "epoch:43 step:33643 [D loss: 0.317030, acc.: 84.38%] [G loss: 3.227618]\n",
      "epoch:43 step:33644 [D loss: 0.339703, acc.: 84.38%] [G loss: 4.454516]\n",
      "epoch:43 step:33645 [D loss: 0.369536, acc.: 82.81%] [G loss: 3.459392]\n",
      "epoch:43 step:33646 [D loss: 0.286957, acc.: 87.50%] [G loss: 3.741987]\n",
      "epoch:43 step:33647 [D loss: 0.222494, acc.: 92.97%] [G loss: 4.798374]\n",
      "epoch:43 step:33648 [D loss: 0.276520, acc.: 85.94%] [G loss: 3.611833]\n",
      "epoch:43 step:33649 [D loss: 0.384878, acc.: 82.81%] [G loss: 4.163512]\n",
      "epoch:43 step:33650 [D loss: 0.252786, acc.: 89.84%] [G loss: 2.593283]\n",
      "epoch:43 step:33651 [D loss: 0.315605, acc.: 84.38%] [G loss: 3.771532]\n",
      "epoch:43 step:33652 [D loss: 0.343821, acc.: 86.72%] [G loss: 3.346933]\n",
      "epoch:43 step:33653 [D loss: 0.283962, acc.: 89.84%] [G loss: 3.354961]\n",
      "epoch:43 step:33654 [D loss: 0.334104, acc.: 85.16%] [G loss: 3.851333]\n",
      "epoch:43 step:33655 [D loss: 0.323955, acc.: 83.59%] [G loss: 2.593990]\n",
      "epoch:43 step:33656 [D loss: 0.346655, acc.: 83.59%] [G loss: 3.690787]\n",
      "epoch:43 step:33657 [D loss: 0.333985, acc.: 85.94%] [G loss: 2.916167]\n",
      "epoch:43 step:33658 [D loss: 0.314793, acc.: 89.84%] [G loss: 3.009258]\n",
      "epoch:43 step:33659 [D loss: 0.320534, acc.: 87.50%] [G loss: 2.570172]\n",
      "epoch:43 step:33660 [D loss: 0.430783, acc.: 82.03%] [G loss: 3.489965]\n",
      "epoch:43 step:33661 [D loss: 0.449714, acc.: 78.91%] [G loss: 2.585783]\n",
      "epoch:43 step:33662 [D loss: 0.369477, acc.: 80.47%] [G loss: 3.285595]\n",
      "epoch:43 step:33663 [D loss: 0.337202, acc.: 82.81%] [G loss: 3.330657]\n",
      "epoch:43 step:33664 [D loss: 0.330169, acc.: 85.94%] [G loss: 3.367108]\n",
      "epoch:43 step:33665 [D loss: 0.286936, acc.: 85.94%] [G loss: 2.916171]\n",
      "epoch:43 step:33666 [D loss: 0.251755, acc.: 89.06%] [G loss: 3.055436]\n",
      "epoch:43 step:33667 [D loss: 0.223082, acc.: 92.19%] [G loss: 3.786116]\n",
      "epoch:43 step:33668 [D loss: 0.328579, acc.: 86.72%] [G loss: 5.221383]\n",
      "epoch:43 step:33669 [D loss: 0.315379, acc.: 82.81%] [G loss: 3.939576]\n",
      "epoch:43 step:33670 [D loss: 0.207252, acc.: 92.19%] [G loss: 4.185951]\n",
      "epoch:43 step:33671 [D loss: 0.328989, acc.: 82.81%] [G loss: 4.881202]\n",
      "epoch:43 step:33672 [D loss: 0.303289, acc.: 85.94%] [G loss: 3.626836]\n",
      "epoch:43 step:33673 [D loss: 0.390645, acc.: 85.16%] [G loss: 3.792013]\n",
      "epoch:43 step:33674 [D loss: 0.274370, acc.: 88.28%] [G loss: 3.278580]\n",
      "epoch:43 step:33675 [D loss: 0.280440, acc.: 89.06%] [G loss: 3.162296]\n",
      "epoch:43 step:33676 [D loss: 0.308612, acc.: 85.94%] [G loss: 3.419644]\n",
      "epoch:43 step:33677 [D loss: 0.250885, acc.: 89.06%] [G loss: 3.415413]\n",
      "epoch:43 step:33678 [D loss: 0.380731, acc.: 83.59%] [G loss: 2.371454]\n",
      "epoch:43 step:33679 [D loss: 0.409108, acc.: 81.25%] [G loss: 2.888673]\n",
      "epoch:43 step:33680 [D loss: 0.373501, acc.: 83.59%] [G loss: 3.043265]\n",
      "epoch:43 step:33681 [D loss: 0.387680, acc.: 85.16%] [G loss: 2.527944]\n",
      "epoch:43 step:33682 [D loss: 0.396796, acc.: 78.91%] [G loss: 2.475465]\n",
      "epoch:43 step:33683 [D loss: 0.409015, acc.: 83.59%] [G loss: 3.571774]\n",
      "epoch:43 step:33684 [D loss: 0.311538, acc.: 85.16%] [G loss: 12.069584]\n",
      "epoch:43 step:33685 [D loss: 0.579646, acc.: 80.47%] [G loss: 4.206140]\n",
      "epoch:43 step:33686 [D loss: 0.584540, acc.: 76.56%] [G loss: 5.089556]\n",
      "epoch:43 step:33687 [D loss: 1.047002, acc.: 61.72%] [G loss: 8.134302]\n",
      "epoch:43 step:33688 [D loss: 1.039705, acc.: 78.91%] [G loss: 4.565379]\n",
      "epoch:43 step:33689 [D loss: 1.586661, acc.: 55.47%] [G loss: 4.435227]\n",
      "epoch:43 step:33690 [D loss: 0.892930, acc.: 75.78%] [G loss: 4.716686]\n",
      "epoch:43 step:33691 [D loss: 0.321704, acc.: 86.72%] [G loss: 3.748810]\n",
      "epoch:43 step:33692 [D loss: 0.531392, acc.: 76.56%] [G loss: 4.394028]\n",
      "epoch:43 step:33693 [D loss: 0.371270, acc.: 87.50%] [G loss: 3.618214]\n",
      "epoch:43 step:33694 [D loss: 0.524424, acc.: 76.56%] [G loss: 3.686649]\n",
      "epoch:43 step:33695 [D loss: 0.219247, acc.: 90.62%] [G loss: 3.405190]\n",
      "epoch:43 step:33696 [D loss: 0.331618, acc.: 85.94%] [G loss: 3.087749]\n",
      "epoch:43 step:33697 [D loss: 0.273754, acc.: 89.06%] [G loss: 2.905355]\n",
      "epoch:43 step:33698 [D loss: 0.366217, acc.: 84.38%] [G loss: 3.177177]\n",
      "epoch:43 step:33699 [D loss: 0.409004, acc.: 82.81%] [G loss: 2.756405]\n",
      "epoch:43 step:33700 [D loss: 0.297837, acc.: 85.16%] [G loss: 3.586126]\n",
      "epoch:43 step:33701 [D loss: 0.288355, acc.: 85.94%] [G loss: 2.951388]\n",
      "epoch:43 step:33702 [D loss: 0.279659, acc.: 91.41%] [G loss: 3.439876]\n",
      "epoch:43 step:33703 [D loss: 0.322244, acc.: 84.38%] [G loss: 2.621118]\n",
      "epoch:43 step:33704 [D loss: 0.320815, acc.: 85.94%] [G loss: 3.000101]\n",
      "epoch:43 step:33705 [D loss: 0.422264, acc.: 82.03%] [G loss: 3.090261]\n",
      "epoch:43 step:33706 [D loss: 0.384357, acc.: 80.47%] [G loss: 2.965188]\n",
      "epoch:43 step:33707 [D loss: 0.357978, acc.: 82.03%] [G loss: 3.020363]\n",
      "epoch:43 step:33708 [D loss: 0.357471, acc.: 82.81%] [G loss: 3.177882]\n",
      "epoch:43 step:33709 [D loss: 0.299203, acc.: 89.06%] [G loss: 2.818803]\n",
      "epoch:43 step:33710 [D loss: 0.311054, acc.: 87.50%] [G loss: 3.068778]\n",
      "epoch:43 step:33711 [D loss: 0.309242, acc.: 85.94%] [G loss: 2.638195]\n",
      "epoch:43 step:33712 [D loss: 0.295698, acc.: 90.62%] [G loss: 3.298143]\n",
      "epoch:43 step:33713 [D loss: 0.274469, acc.: 89.06%] [G loss: 3.353904]\n",
      "epoch:43 step:33714 [D loss: 0.273023, acc.: 88.28%] [G loss: 2.867838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33715 [D loss: 0.279155, acc.: 90.62%] [G loss: 3.392995]\n",
      "epoch:43 step:33716 [D loss: 0.293125, acc.: 85.16%] [G loss: 2.398264]\n",
      "epoch:43 step:33717 [D loss: 0.251243, acc.: 91.41%] [G loss: 2.863937]\n",
      "epoch:43 step:33718 [D loss: 0.257982, acc.: 90.62%] [G loss: 2.954742]\n",
      "epoch:43 step:33719 [D loss: 0.244480, acc.: 91.41%] [G loss: 2.966321]\n",
      "epoch:43 step:33720 [D loss: 0.258100, acc.: 88.28%] [G loss: 3.903956]\n",
      "epoch:43 step:33721 [D loss: 0.329385, acc.: 85.16%] [G loss: 2.112549]\n",
      "epoch:43 step:33722 [D loss: 0.267384, acc.: 89.06%] [G loss: 3.365708]\n",
      "epoch:43 step:33723 [D loss: 0.315349, acc.: 87.50%] [G loss: 2.545175]\n",
      "epoch:43 step:33724 [D loss: 0.250052, acc.: 89.06%] [G loss: 2.583536]\n",
      "epoch:43 step:33725 [D loss: 0.400273, acc.: 78.12%] [G loss: 2.633021]\n",
      "epoch:43 step:33726 [D loss: 0.346907, acc.: 83.59%] [G loss: 1.947938]\n",
      "epoch:43 step:33727 [D loss: 0.437613, acc.: 79.69%] [G loss: 2.258187]\n",
      "epoch:43 step:33728 [D loss: 0.255363, acc.: 87.50%] [G loss: 2.678252]\n",
      "epoch:43 step:33729 [D loss: 0.319758, acc.: 85.16%] [G loss: 2.223707]\n",
      "epoch:43 step:33730 [D loss: 0.332728, acc.: 82.03%] [G loss: 2.803251]\n",
      "epoch:43 step:33731 [D loss: 0.386413, acc.: 85.94%] [G loss: 2.583094]\n",
      "epoch:43 step:33732 [D loss: 0.328372, acc.: 85.16%] [G loss: 3.056870]\n",
      "epoch:43 step:33733 [D loss: 0.257080, acc.: 89.06%] [G loss: 2.752988]\n",
      "epoch:43 step:33734 [D loss: 0.253752, acc.: 91.41%] [G loss: 2.794250]\n",
      "epoch:43 step:33735 [D loss: 0.404431, acc.: 83.59%] [G loss: 2.453866]\n",
      "epoch:43 step:33736 [D loss: 0.302081, acc.: 84.38%] [G loss: 2.834381]\n",
      "epoch:43 step:33737 [D loss: 0.292655, acc.: 87.50%] [G loss: 2.489790]\n",
      "epoch:43 step:33738 [D loss: 0.345861, acc.: 85.94%] [G loss: 2.921091]\n",
      "epoch:43 step:33739 [D loss: 0.261677, acc.: 88.28%] [G loss: 3.011415]\n",
      "epoch:43 step:33740 [D loss: 0.468383, acc.: 77.34%] [G loss: 3.544768]\n",
      "epoch:43 step:33741 [D loss: 0.365384, acc.: 83.59%] [G loss: 2.121696]\n",
      "epoch:43 step:33742 [D loss: 0.251549, acc.: 88.28%] [G loss: 4.375995]\n",
      "epoch:43 step:33743 [D loss: 0.311351, acc.: 85.94%] [G loss: 2.930950]\n",
      "epoch:43 step:33744 [D loss: 0.245176, acc.: 90.62%] [G loss: 4.538364]\n",
      "epoch:43 step:33745 [D loss: 0.251780, acc.: 90.62%] [G loss: 3.634667]\n",
      "epoch:43 step:33746 [D loss: 0.214383, acc.: 89.84%] [G loss: 4.585559]\n",
      "epoch:43 step:33747 [D loss: 0.251911, acc.: 92.97%] [G loss: 2.935146]\n",
      "epoch:43 step:33748 [D loss: 0.301809, acc.: 85.94%] [G loss: 2.571630]\n",
      "epoch:43 step:33749 [D loss: 0.369690, acc.: 82.03%] [G loss: 2.798225]\n",
      "epoch:43 step:33750 [D loss: 0.307126, acc.: 85.16%] [G loss: 3.086449]\n",
      "epoch:43 step:33751 [D loss: 0.293132, acc.: 87.50%] [G loss: 2.492229]\n",
      "epoch:43 step:33752 [D loss: 0.404494, acc.: 84.38%] [G loss: 2.929845]\n",
      "epoch:43 step:33753 [D loss: 0.309054, acc.: 88.28%] [G loss: 2.544621]\n",
      "epoch:43 step:33754 [D loss: 0.312812, acc.: 89.06%] [G loss: 3.078582]\n",
      "epoch:43 step:33755 [D loss: 0.317648, acc.: 82.81%] [G loss: 3.286248]\n",
      "epoch:43 step:33756 [D loss: 0.333393, acc.: 85.94%] [G loss: 3.415109]\n",
      "epoch:43 step:33757 [D loss: 0.386569, acc.: 79.69%] [G loss: 2.634809]\n",
      "epoch:43 step:33758 [D loss: 0.275865, acc.: 85.16%] [G loss: 3.950097]\n",
      "epoch:43 step:33759 [D loss: 0.242436, acc.: 85.94%] [G loss: 3.355369]\n",
      "epoch:43 step:33760 [D loss: 0.316670, acc.: 89.84%] [G loss: 3.341201]\n",
      "epoch:43 step:33761 [D loss: 0.295906, acc.: 86.72%] [G loss: 2.803927]\n",
      "epoch:43 step:33762 [D loss: 0.248938, acc.: 88.28%] [G loss: 2.594552]\n",
      "epoch:43 step:33763 [D loss: 0.359989, acc.: 82.81%] [G loss: 3.704860]\n",
      "epoch:43 step:33764 [D loss: 0.269522, acc.: 84.38%] [G loss: 3.250081]\n",
      "epoch:43 step:33765 [D loss: 0.326122, acc.: 81.25%] [G loss: 3.490056]\n",
      "epoch:43 step:33766 [D loss: 0.305100, acc.: 88.28%] [G loss: 2.842072]\n",
      "epoch:43 step:33767 [D loss: 0.353869, acc.: 83.59%] [G loss: 3.170157]\n",
      "epoch:43 step:33768 [D loss: 0.239256, acc.: 88.28%] [G loss: 2.990055]\n",
      "epoch:43 step:33769 [D loss: 0.300737, acc.: 86.72%] [G loss: 4.165380]\n",
      "epoch:43 step:33770 [D loss: 0.349553, acc.: 85.16%] [G loss: 2.931719]\n",
      "epoch:43 step:33771 [D loss: 0.394102, acc.: 80.47%] [G loss: 4.849041]\n",
      "epoch:43 step:33772 [D loss: 0.210100, acc.: 92.19%] [G loss: 4.510388]\n",
      "epoch:43 step:33773 [D loss: 0.389448, acc.: 82.03%] [G loss: 4.953943]\n",
      "epoch:43 step:33774 [D loss: 0.274441, acc.: 90.62%] [G loss: 4.653207]\n",
      "epoch:43 step:33775 [D loss: 0.370943, acc.: 82.81%] [G loss: 4.091760]\n",
      "epoch:43 step:33776 [D loss: 0.291683, acc.: 89.06%] [G loss: 4.182209]\n",
      "epoch:43 step:33777 [D loss: 0.318938, acc.: 84.38%] [G loss: 5.092083]\n",
      "epoch:43 step:33778 [D loss: 0.376987, acc.: 82.81%] [G loss: 3.525061]\n",
      "epoch:43 step:33779 [D loss: 0.243038, acc.: 89.84%] [G loss: 3.394307]\n",
      "epoch:43 step:33780 [D loss: 0.367576, acc.: 82.03%] [G loss: 5.069934]\n",
      "epoch:43 step:33781 [D loss: 0.276798, acc.: 86.72%] [G loss: 3.497481]\n",
      "epoch:43 step:33782 [D loss: 0.242622, acc.: 87.50%] [G loss: 3.132515]\n",
      "epoch:43 step:33783 [D loss: 0.353350, acc.: 88.28%] [G loss: 3.424118]\n",
      "epoch:43 step:33784 [D loss: 0.351077, acc.: 84.38%] [G loss: 2.653829]\n",
      "epoch:43 step:33785 [D loss: 0.328765, acc.: 85.16%] [G loss: 3.157435]\n",
      "epoch:43 step:33786 [D loss: 0.245979, acc.: 92.97%] [G loss: 3.025034]\n",
      "epoch:43 step:33787 [D loss: 0.341268, acc.: 84.38%] [G loss: 2.469901]\n",
      "epoch:43 step:33788 [D loss: 0.299599, acc.: 87.50%] [G loss: 3.349312]\n",
      "epoch:43 step:33789 [D loss: 0.264467, acc.: 89.06%] [G loss: 2.831026]\n",
      "epoch:43 step:33790 [D loss: 0.396979, acc.: 82.03%] [G loss: 3.220907]\n",
      "epoch:43 step:33791 [D loss: 0.373033, acc.: 83.59%] [G loss: 2.757351]\n",
      "epoch:43 step:33792 [D loss: 0.316613, acc.: 86.72%] [G loss: 2.665397]\n",
      "epoch:43 step:33793 [D loss: 0.277032, acc.: 87.50%] [G loss: 2.706426]\n",
      "epoch:43 step:33794 [D loss: 0.316175, acc.: 84.38%] [G loss: 3.173606]\n",
      "epoch:43 step:33795 [D loss: 0.291944, acc.: 86.72%] [G loss: 2.918188]\n",
      "epoch:43 step:33796 [D loss: 0.337594, acc.: 85.94%] [G loss: 2.518357]\n",
      "epoch:43 step:33797 [D loss: 0.442951, acc.: 84.38%] [G loss: 2.782315]\n",
      "epoch:43 step:33798 [D loss: 0.354689, acc.: 85.94%] [G loss: 2.513680]\n",
      "epoch:43 step:33799 [D loss: 0.331059, acc.: 84.38%] [G loss: 3.067231]\n",
      "epoch:43 step:33800 [D loss: 0.310842, acc.: 85.94%] [G loss: 3.317298]\n",
      "epoch:43 step:33801 [D loss: 0.429800, acc.: 80.47%] [G loss: 3.523564]\n",
      "epoch:43 step:33802 [D loss: 0.282299, acc.: 90.62%] [G loss: 3.458059]\n",
      "epoch:43 step:33803 [D loss: 0.329341, acc.: 83.59%] [G loss: 3.805521]\n",
      "epoch:43 step:33804 [D loss: 0.360553, acc.: 84.38%] [G loss: 3.167274]\n",
      "epoch:43 step:33805 [D loss: 0.259840, acc.: 88.28%] [G loss: 3.565194]\n",
      "epoch:43 step:33806 [D loss: 0.333257, acc.: 81.25%] [G loss: 2.908877]\n",
      "epoch:43 step:33807 [D loss: 0.298456, acc.: 87.50%] [G loss: 3.639162]\n",
      "epoch:43 step:33808 [D loss: 0.238796, acc.: 90.62%] [G loss: 3.548369]\n",
      "epoch:43 step:33809 [D loss: 0.286770, acc.: 88.28%] [G loss: 3.251950]\n",
      "epoch:43 step:33810 [D loss: 0.380219, acc.: 83.59%] [G loss: 2.982011]\n",
      "epoch:43 step:33811 [D loss: 0.303809, acc.: 88.28%] [G loss: 2.827241]\n",
      "epoch:43 step:33812 [D loss: 0.457947, acc.: 76.56%] [G loss: 3.130279]\n",
      "epoch:43 step:33813 [D loss: 0.395843, acc.: 82.03%] [G loss: 4.527208]\n",
      "epoch:43 step:33814 [D loss: 0.285711, acc.: 86.72%] [G loss: 3.223516]\n",
      "epoch:43 step:33815 [D loss: 0.324017, acc.: 86.72%] [G loss: 3.384046]\n",
      "epoch:43 step:33816 [D loss: 0.284848, acc.: 85.94%] [G loss: 3.189037]\n",
      "epoch:43 step:33817 [D loss: 0.376441, acc.: 82.81%] [G loss: 3.396662]\n",
      "epoch:43 step:33818 [D loss: 0.322156, acc.: 80.47%] [G loss: 3.642803]\n",
      "epoch:43 step:33819 [D loss: 0.431066, acc.: 79.69%] [G loss: 3.580400]\n",
      "epoch:43 step:33820 [D loss: 0.261919, acc.: 88.28%] [G loss: 3.285339]\n",
      "epoch:43 step:33821 [D loss: 0.308075, acc.: 86.72%] [G loss: 2.885936]\n",
      "epoch:43 step:33822 [D loss: 0.288392, acc.: 85.16%] [G loss: 3.102472]\n",
      "epoch:43 step:33823 [D loss: 0.339640, acc.: 81.25%] [G loss: 3.710070]\n",
      "epoch:43 step:33824 [D loss: 0.396331, acc.: 83.59%] [G loss: 4.270243]\n",
      "epoch:43 step:33825 [D loss: 0.376513, acc.: 80.47%] [G loss: 4.040621]\n",
      "epoch:43 step:33826 [D loss: 0.290832, acc.: 85.16%] [G loss: 3.724945]\n",
      "epoch:43 step:33827 [D loss: 0.207515, acc.: 91.41%] [G loss: 3.472984]\n",
      "epoch:43 step:33828 [D loss: 0.317521, acc.: 83.59%] [G loss: 3.708941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33829 [D loss: 0.278115, acc.: 89.84%] [G loss: 3.489913]\n",
      "epoch:43 step:33830 [D loss: 0.298427, acc.: 85.16%] [G loss: 2.935499]\n",
      "epoch:43 step:33831 [D loss: 0.404238, acc.: 84.38%] [G loss: 4.059629]\n",
      "epoch:43 step:33832 [D loss: 0.305560, acc.: 85.94%] [G loss: 2.922573]\n",
      "epoch:43 step:33833 [D loss: 0.436772, acc.: 78.91%] [G loss: 3.297830]\n",
      "epoch:43 step:33834 [D loss: 0.307646, acc.: 86.72%] [G loss: 2.664979]\n",
      "epoch:43 step:33835 [D loss: 0.397150, acc.: 82.81%] [G loss: 4.309095]\n",
      "epoch:43 step:33836 [D loss: 0.511055, acc.: 73.44%] [G loss: 4.192504]\n",
      "epoch:43 step:33837 [D loss: 0.340353, acc.: 84.38%] [G loss: 5.588012]\n",
      "epoch:43 step:33838 [D loss: 0.266968, acc.: 88.28%] [G loss: 4.126215]\n",
      "epoch:43 step:33839 [D loss: 0.276488, acc.: 91.41%] [G loss: 3.822066]\n",
      "epoch:43 step:33840 [D loss: 0.458513, acc.: 81.25%] [G loss: 3.387083]\n",
      "epoch:43 step:33841 [D loss: 0.244691, acc.: 89.06%] [G loss: 3.291297]\n",
      "epoch:43 step:33842 [D loss: 0.312928, acc.: 84.38%] [G loss: 3.087222]\n",
      "epoch:43 step:33843 [D loss: 0.288861, acc.: 86.72%] [G loss: 2.484650]\n",
      "epoch:43 step:33844 [D loss: 0.409085, acc.: 79.69%] [G loss: 3.123607]\n",
      "epoch:43 step:33845 [D loss: 0.212320, acc.: 89.84%] [G loss: 3.408478]\n",
      "epoch:43 step:33846 [D loss: 0.229396, acc.: 90.62%] [G loss: 3.514966]\n",
      "epoch:43 step:33847 [D loss: 0.367048, acc.: 85.94%] [G loss: 2.747378]\n",
      "epoch:43 step:33848 [D loss: 0.331714, acc.: 86.72%] [G loss: 2.902694]\n",
      "epoch:43 step:33849 [D loss: 0.257142, acc.: 89.06%] [G loss: 3.090892]\n",
      "epoch:43 step:33850 [D loss: 0.371182, acc.: 85.16%] [G loss: 4.538257]\n",
      "epoch:43 step:33851 [D loss: 0.213719, acc.: 91.41%] [G loss: 3.490854]\n",
      "epoch:43 step:33852 [D loss: 0.409585, acc.: 83.59%] [G loss: 2.840027]\n",
      "epoch:43 step:33853 [D loss: 0.285814, acc.: 85.94%] [G loss: 3.299792]\n",
      "epoch:43 step:33854 [D loss: 0.341609, acc.: 83.59%] [G loss: 3.132856]\n",
      "epoch:43 step:33855 [D loss: 0.332183, acc.: 81.25%] [G loss: 3.688479]\n",
      "epoch:43 step:33856 [D loss: 0.255381, acc.: 85.16%] [G loss: 3.447061]\n",
      "epoch:43 step:33857 [D loss: 0.423592, acc.: 79.69%] [G loss: 3.893639]\n",
      "epoch:43 step:33858 [D loss: 0.445172, acc.: 76.56%] [G loss: 3.913147]\n",
      "epoch:43 step:33859 [D loss: 0.354303, acc.: 84.38%] [G loss: 2.726125]\n",
      "epoch:43 step:33860 [D loss: 0.393667, acc.: 83.59%] [G loss: 2.643204]\n",
      "epoch:43 step:33861 [D loss: 0.384073, acc.: 78.12%] [G loss: 3.233560]\n",
      "epoch:43 step:33862 [D loss: 0.301116, acc.: 85.16%] [G loss: 2.884393]\n",
      "epoch:43 step:33863 [D loss: 0.319633, acc.: 83.59%] [G loss: 2.758857]\n",
      "epoch:43 step:33864 [D loss: 0.308062, acc.: 85.94%] [G loss: 3.611630]\n",
      "epoch:43 step:33865 [D loss: 0.504006, acc.: 78.91%] [G loss: 2.972430]\n",
      "epoch:43 step:33866 [D loss: 0.338214, acc.: 83.59%] [G loss: 4.436902]\n",
      "epoch:43 step:33867 [D loss: 0.524075, acc.: 79.69%] [G loss: 2.752824]\n",
      "epoch:43 step:33868 [D loss: 0.484322, acc.: 75.00%] [G loss: 5.218526]\n",
      "epoch:43 step:33869 [D loss: 0.903124, acc.: 64.84%] [G loss: 7.417717]\n",
      "epoch:43 step:33870 [D loss: 1.538825, acc.: 62.50%] [G loss: 5.195880]\n",
      "epoch:43 step:33871 [D loss: 1.176426, acc.: 64.84%] [G loss: 7.360978]\n",
      "epoch:43 step:33872 [D loss: 0.926698, acc.: 68.75%] [G loss: 3.483029]\n",
      "epoch:43 step:33873 [D loss: 0.477321, acc.: 77.34%] [G loss: 4.403583]\n",
      "epoch:43 step:33874 [D loss: 0.365114, acc.: 84.38%] [G loss: 3.922209]\n",
      "epoch:43 step:33875 [D loss: 0.379184, acc.: 82.81%] [G loss: 3.802031]\n",
      "epoch:43 step:33876 [D loss: 0.264485, acc.: 88.28%] [G loss: 3.075286]\n",
      "epoch:43 step:33877 [D loss: 0.332801, acc.: 83.59%] [G loss: 3.284263]\n",
      "epoch:43 step:33878 [D loss: 0.552910, acc.: 74.22%] [G loss: 2.772028]\n",
      "epoch:43 step:33879 [D loss: 0.301640, acc.: 86.72%] [G loss: 3.299966]\n",
      "epoch:43 step:33880 [D loss: 0.348507, acc.: 83.59%] [G loss: 2.494242]\n",
      "epoch:43 step:33881 [D loss: 0.310879, acc.: 87.50%] [G loss: 2.900430]\n",
      "epoch:43 step:33882 [D loss: 0.281459, acc.: 89.06%] [G loss: 2.999753]\n",
      "epoch:43 step:33883 [D loss: 0.395266, acc.: 79.69%] [G loss: 2.479320]\n",
      "epoch:43 step:33884 [D loss: 0.350924, acc.: 85.94%] [G loss: 3.158976]\n",
      "epoch:43 step:33885 [D loss: 0.381156, acc.: 85.94%] [G loss: 2.802778]\n",
      "epoch:43 step:33886 [D loss: 0.322855, acc.: 85.16%] [G loss: 2.762257]\n",
      "epoch:43 step:33887 [D loss: 0.342119, acc.: 85.16%] [G loss: 2.394032]\n",
      "epoch:43 step:33888 [D loss: 0.266336, acc.: 92.19%] [G loss: 2.724171]\n",
      "epoch:43 step:33889 [D loss: 0.375407, acc.: 82.81%] [G loss: 2.872612]\n",
      "epoch:43 step:33890 [D loss: 0.273423, acc.: 86.72%] [G loss: 2.333625]\n",
      "epoch:43 step:33891 [D loss: 0.342093, acc.: 82.03%] [G loss: 2.760018]\n",
      "epoch:43 step:33892 [D loss: 0.304585, acc.: 87.50%] [G loss: 2.977783]\n",
      "epoch:43 step:33893 [D loss: 0.229499, acc.: 89.06%] [G loss: 2.522661]\n",
      "epoch:43 step:33894 [D loss: 0.456038, acc.: 79.69%] [G loss: 2.538212]\n",
      "epoch:43 step:33895 [D loss: 0.302331, acc.: 85.94%] [G loss: 2.286756]\n",
      "epoch:43 step:33896 [D loss: 0.316486, acc.: 85.94%] [G loss: 2.983917]\n",
      "epoch:43 step:33897 [D loss: 0.316563, acc.: 85.94%] [G loss: 2.239824]\n",
      "epoch:43 step:33898 [D loss: 0.404038, acc.: 79.69%] [G loss: 2.865868]\n",
      "epoch:43 step:33899 [D loss: 0.336664, acc.: 84.38%] [G loss: 2.908687]\n",
      "epoch:43 step:33900 [D loss: 0.374359, acc.: 84.38%] [G loss: 2.597417]\n",
      "epoch:43 step:33901 [D loss: 0.318185, acc.: 86.72%] [G loss: 2.204464]\n",
      "epoch:43 step:33902 [D loss: 0.339835, acc.: 82.03%] [G loss: 2.898586]\n",
      "epoch:43 step:33903 [D loss: 0.390263, acc.: 82.81%] [G loss: 2.603612]\n",
      "epoch:43 step:33904 [D loss: 0.327839, acc.: 82.03%] [G loss: 2.324358]\n",
      "epoch:43 step:33905 [D loss: 0.346310, acc.: 83.59%] [G loss: 2.745414]\n",
      "epoch:43 step:33906 [D loss: 0.421402, acc.: 82.81%] [G loss: 2.349661]\n",
      "epoch:43 step:33907 [D loss: 0.304592, acc.: 85.16%] [G loss: 2.425227]\n",
      "epoch:43 step:33908 [D loss: 0.439924, acc.: 78.12%] [G loss: 2.927139]\n",
      "epoch:43 step:33909 [D loss: 0.447129, acc.: 78.12%] [G loss: 3.124249]\n",
      "epoch:43 step:33910 [D loss: 0.216545, acc.: 89.84%] [G loss: 3.647841]\n",
      "epoch:43 step:33911 [D loss: 0.332595, acc.: 87.50%] [G loss: 2.394436]\n",
      "epoch:43 step:33912 [D loss: 0.341816, acc.: 85.94%] [G loss: 2.341684]\n",
      "epoch:43 step:33913 [D loss: 0.368167, acc.: 82.81%] [G loss: 2.978530]\n",
      "epoch:43 step:33914 [D loss: 0.452046, acc.: 78.91%] [G loss: 2.579116]\n",
      "epoch:43 step:33915 [D loss: 0.378765, acc.: 80.47%] [G loss: 3.172185]\n",
      "epoch:43 step:33916 [D loss: 0.423467, acc.: 81.25%] [G loss: 2.646189]\n",
      "epoch:43 step:33917 [D loss: 0.411027, acc.: 82.03%] [G loss: 2.907981]\n",
      "epoch:43 step:33918 [D loss: 0.281538, acc.: 88.28%] [G loss: 2.658674]\n",
      "epoch:43 step:33919 [D loss: 0.289555, acc.: 88.28%] [G loss: 2.706052]\n",
      "epoch:43 step:33920 [D loss: 0.356582, acc.: 82.81%] [G loss: 2.526198]\n",
      "epoch:43 step:33921 [D loss: 0.323201, acc.: 84.38%] [G loss: 2.590757]\n",
      "epoch:43 step:33922 [D loss: 0.279582, acc.: 92.19%] [G loss: 2.754453]\n",
      "epoch:43 step:33923 [D loss: 0.357688, acc.: 84.38%] [G loss: 2.656109]\n",
      "epoch:43 step:33924 [D loss: 0.352762, acc.: 85.94%] [G loss: 2.935006]\n",
      "epoch:43 step:33925 [D loss: 0.263599, acc.: 88.28%] [G loss: 2.798966]\n",
      "epoch:43 step:33926 [D loss: 0.345945, acc.: 84.38%] [G loss: 3.494124]\n",
      "epoch:43 step:33927 [D loss: 0.339270, acc.: 83.59%] [G loss: 3.327737]\n",
      "epoch:43 step:33928 [D loss: 0.285387, acc.: 88.28%] [G loss: 4.242978]\n",
      "epoch:43 step:33929 [D loss: 0.390010, acc.: 84.38%] [G loss: 3.491896]\n",
      "epoch:43 step:33930 [D loss: 0.344325, acc.: 86.72%] [G loss: 2.672621]\n",
      "epoch:43 step:33931 [D loss: 0.294839, acc.: 88.28%] [G loss: 2.499369]\n",
      "epoch:43 step:33932 [D loss: 0.295875, acc.: 89.06%] [G loss: 3.978853]\n",
      "epoch:43 step:33933 [D loss: 0.397789, acc.: 83.59%] [G loss: 2.888469]\n",
      "epoch:43 step:33934 [D loss: 0.449481, acc.: 78.12%] [G loss: 2.777161]\n",
      "epoch:43 step:33935 [D loss: 0.476425, acc.: 77.34%] [G loss: 2.186956]\n",
      "epoch:43 step:33936 [D loss: 0.353808, acc.: 84.38%] [G loss: 2.214467]\n",
      "epoch:43 step:33937 [D loss: 0.281312, acc.: 91.41%] [G loss: 2.694239]\n",
      "epoch:43 step:33938 [D loss: 0.331380, acc.: 84.38%] [G loss: 2.481395]\n",
      "epoch:43 step:33939 [D loss: 0.340107, acc.: 82.81%] [G loss: 2.478896]\n",
      "epoch:43 step:33940 [D loss: 0.327334, acc.: 90.62%] [G loss: 3.157031]\n",
      "epoch:43 step:33941 [D loss: 0.324709, acc.: 84.38%] [G loss: 3.599948]\n",
      "epoch:43 step:33942 [D loss: 0.396186, acc.: 79.69%] [G loss: 3.322998]\n",
      "epoch:43 step:33943 [D loss: 0.396780, acc.: 81.25%] [G loss: 3.519491]\n",
      "epoch:43 step:33944 [D loss: 0.226664, acc.: 92.19%] [G loss: 3.378333]\n",
      "epoch:43 step:33945 [D loss: 0.272859, acc.: 85.16%] [G loss: 3.884438]\n",
      "epoch:43 step:33946 [D loss: 0.463857, acc.: 78.12%] [G loss: 2.618438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33947 [D loss: 0.218997, acc.: 93.75%] [G loss: 2.493550]\n",
      "epoch:43 step:33948 [D loss: 0.281636, acc.: 89.06%] [G loss: 2.749791]\n",
      "epoch:43 step:33949 [D loss: 0.326625, acc.: 84.38%] [G loss: 2.928172]\n",
      "epoch:43 step:33950 [D loss: 0.301525, acc.: 88.28%] [G loss: 4.223135]\n",
      "epoch:43 step:33951 [D loss: 0.387680, acc.: 79.69%] [G loss: 6.832938]\n",
      "epoch:43 step:33952 [D loss: 0.281288, acc.: 90.62%] [G loss: 4.390006]\n",
      "epoch:43 step:33953 [D loss: 0.221444, acc.: 89.84%] [G loss: 4.596210]\n",
      "epoch:43 step:33954 [D loss: 0.290322, acc.: 89.06%] [G loss: 4.455605]\n",
      "epoch:43 step:33955 [D loss: 0.291210, acc.: 85.16%] [G loss: 3.972859]\n",
      "epoch:43 step:33956 [D loss: 0.358906, acc.: 82.03%] [G loss: 4.322190]\n",
      "epoch:43 step:33957 [D loss: 0.708554, acc.: 75.00%] [G loss: 1.642372]\n",
      "epoch:43 step:33958 [D loss: 0.653823, acc.: 71.09%] [G loss: 7.437212]\n",
      "epoch:43 step:33959 [D loss: 0.859237, acc.: 71.88%] [G loss: 4.235294]\n",
      "epoch:43 step:33960 [D loss: 0.423069, acc.: 79.69%] [G loss: 5.244193]\n",
      "epoch:43 step:33961 [D loss: 0.730573, acc.: 75.78%] [G loss: 5.416581]\n",
      "epoch:43 step:33962 [D loss: 0.506040, acc.: 77.34%] [G loss: 3.311074]\n",
      "epoch:43 step:33963 [D loss: 0.514485, acc.: 77.34%] [G loss: 3.386499]\n",
      "epoch:43 step:33964 [D loss: 0.386613, acc.: 82.81%] [G loss: 4.938047]\n",
      "epoch:43 step:33965 [D loss: 0.430078, acc.: 84.38%] [G loss: 3.838850]\n",
      "epoch:43 step:33966 [D loss: 0.323670, acc.: 83.59%] [G loss: 3.235595]\n",
      "epoch:43 step:33967 [D loss: 0.260177, acc.: 88.28%] [G loss: 4.271667]\n",
      "epoch:43 step:33968 [D loss: 0.347072, acc.: 82.03%] [G loss: 3.921182]\n",
      "epoch:43 step:33969 [D loss: 0.313049, acc.: 85.94%] [G loss: 4.699011]\n",
      "epoch:43 step:33970 [D loss: 0.323791, acc.: 86.72%] [G loss: 5.750123]\n",
      "epoch:43 step:33971 [D loss: 0.321139, acc.: 88.28%] [G loss: 3.597871]\n",
      "epoch:43 step:33972 [D loss: 0.332174, acc.: 80.47%] [G loss: 4.136337]\n",
      "epoch:43 step:33973 [D loss: 0.314219, acc.: 89.06%] [G loss: 3.825034]\n",
      "epoch:43 step:33974 [D loss: 0.423837, acc.: 78.91%] [G loss: 2.765456]\n",
      "epoch:43 step:33975 [D loss: 0.340841, acc.: 85.94%] [G loss: 3.407546]\n",
      "epoch:43 step:33976 [D loss: 0.340138, acc.: 83.59%] [G loss: 2.657230]\n",
      "epoch:43 step:33977 [D loss: 0.282222, acc.: 87.50%] [G loss: 2.877035]\n",
      "epoch:43 step:33978 [D loss: 0.462600, acc.: 75.78%] [G loss: 3.037934]\n",
      "epoch:43 step:33979 [D loss: 0.386673, acc.: 82.81%] [G loss: 3.242388]\n",
      "epoch:43 step:33980 [D loss: 0.304307, acc.: 85.94%] [G loss: 4.179691]\n",
      "epoch:43 step:33981 [D loss: 0.310715, acc.: 85.94%] [G loss: 2.650106]\n",
      "epoch:43 step:33982 [D loss: 0.329709, acc.: 85.94%] [G loss: 3.146704]\n",
      "epoch:43 step:33983 [D loss: 0.309218, acc.: 89.06%] [G loss: 2.742860]\n",
      "epoch:43 step:33984 [D loss: 0.267550, acc.: 88.28%] [G loss: 2.609984]\n",
      "epoch:43 step:33985 [D loss: 0.290794, acc.: 88.28%] [G loss: 2.784199]\n",
      "epoch:43 step:33986 [D loss: 0.358028, acc.: 82.03%] [G loss: 2.599375]\n",
      "epoch:43 step:33987 [D loss: 0.230721, acc.: 92.97%] [G loss: 3.396818]\n",
      "epoch:43 step:33988 [D loss: 0.369324, acc.: 83.59%] [G loss: 3.222785]\n",
      "epoch:43 step:33989 [D loss: 0.313769, acc.: 85.16%] [G loss: 3.582648]\n",
      "epoch:43 step:33990 [D loss: 0.370457, acc.: 80.47%] [G loss: 2.640986]\n",
      "epoch:43 step:33991 [D loss: 0.334242, acc.: 87.50%] [G loss: 2.981794]\n",
      "epoch:43 step:33992 [D loss: 0.423684, acc.: 84.38%] [G loss: 2.743444]\n",
      "epoch:43 step:33993 [D loss: 0.479721, acc.: 77.34%] [G loss: 2.773886]\n",
      "epoch:43 step:33994 [D loss: 0.390862, acc.: 82.03%] [G loss: 2.698240]\n",
      "epoch:43 step:33995 [D loss: 0.364972, acc.: 83.59%] [G loss: 2.822677]\n",
      "epoch:43 step:33996 [D loss: 0.328411, acc.: 80.47%] [G loss: 2.686181]\n",
      "epoch:43 step:33997 [D loss: 0.407978, acc.: 82.81%] [G loss: 2.592030]\n",
      "epoch:43 step:33998 [D loss: 0.345318, acc.: 83.59%] [G loss: 2.820385]\n",
      "epoch:43 step:33999 [D loss: 0.308514, acc.: 86.72%] [G loss: 2.570448]\n",
      "epoch:43 step:34000 [D loss: 0.438787, acc.: 80.47%] [G loss: 3.153934]\n",
      "epoch:43 step:34001 [D loss: 0.464445, acc.: 79.69%] [G loss: 2.283955]\n",
      "epoch:43 step:34002 [D loss: 0.373454, acc.: 80.47%] [G loss: 2.474351]\n",
      "epoch:43 step:34003 [D loss: 0.281842, acc.: 90.62%] [G loss: 2.487300]\n",
      "epoch:43 step:34004 [D loss: 0.290136, acc.: 84.38%] [G loss: 2.214529]\n",
      "epoch:43 step:34005 [D loss: 0.264266, acc.: 92.97%] [G loss: 2.572021]\n",
      "epoch:43 step:34006 [D loss: 0.380397, acc.: 84.38%] [G loss: 2.567284]\n",
      "epoch:43 step:34007 [D loss: 0.289170, acc.: 89.84%] [G loss: 2.296704]\n",
      "epoch:43 step:34008 [D loss: 0.426563, acc.: 80.47%] [G loss: 2.289899]\n",
      "epoch:43 step:34009 [D loss: 0.357903, acc.: 82.81%] [G loss: 2.842277]\n",
      "epoch:43 step:34010 [D loss: 0.349741, acc.: 84.38%] [G loss: 2.736946]\n",
      "epoch:43 step:34011 [D loss: 0.408196, acc.: 82.81%] [G loss: 4.800080]\n",
      "epoch:43 step:34012 [D loss: 0.476676, acc.: 78.91%] [G loss: 2.907284]\n",
      "epoch:43 step:34013 [D loss: 0.438553, acc.: 81.25%] [G loss: 5.037542]\n",
      "epoch:43 step:34014 [D loss: 0.440513, acc.: 78.91%] [G loss: 5.643122]\n",
      "epoch:43 step:34015 [D loss: 0.278237, acc.: 89.06%] [G loss: 4.285810]\n",
      "epoch:43 step:34016 [D loss: 0.211549, acc.: 91.41%] [G loss: 4.589313]\n",
      "epoch:43 step:34017 [D loss: 0.307621, acc.: 85.16%] [G loss: 4.594864]\n",
      "epoch:43 step:34018 [D loss: 0.291002, acc.: 86.72%] [G loss: 3.730705]\n",
      "epoch:43 step:34019 [D loss: 0.334684, acc.: 85.16%] [G loss: 3.934736]\n",
      "epoch:43 step:34020 [D loss: 0.288210, acc.: 86.72%] [G loss: 3.575883]\n",
      "epoch:43 step:34021 [D loss: 0.241816, acc.: 91.41%] [G loss: 3.000159]\n",
      "epoch:43 step:34022 [D loss: 0.352010, acc.: 81.25%] [G loss: 3.027057]\n",
      "epoch:43 step:34023 [D loss: 0.288304, acc.: 88.28%] [G loss: 3.893181]\n",
      "epoch:43 step:34024 [D loss: 0.318926, acc.: 83.59%] [G loss: 2.794184]\n",
      "epoch:43 step:34025 [D loss: 0.245129, acc.: 90.62%] [G loss: 3.307996]\n",
      "epoch:43 step:34026 [D loss: 0.292737, acc.: 89.84%] [G loss: 3.196650]\n",
      "epoch:43 step:34027 [D loss: 0.335778, acc.: 83.59%] [G loss: 2.897568]\n",
      "epoch:43 step:34028 [D loss: 0.366471, acc.: 84.38%] [G loss: 2.555548]\n",
      "epoch:43 step:34029 [D loss: 0.311913, acc.: 84.38%] [G loss: 3.827222]\n",
      "epoch:43 step:34030 [D loss: 0.198033, acc.: 89.84%] [G loss: 4.695684]\n",
      "epoch:43 step:34031 [D loss: 0.343890, acc.: 85.16%] [G loss: 3.108612]\n",
      "epoch:43 step:34032 [D loss: 0.331375, acc.: 85.16%] [G loss: 3.127511]\n",
      "epoch:43 step:34033 [D loss: 0.264654, acc.: 87.50%] [G loss: 3.143141]\n",
      "epoch:43 step:34034 [D loss: 0.268186, acc.: 85.94%] [G loss: 4.306198]\n",
      "epoch:43 step:34035 [D loss: 0.382648, acc.: 79.69%] [G loss: 2.837719]\n",
      "epoch:43 step:34036 [D loss: 0.252790, acc.: 91.41%] [G loss: 2.256890]\n",
      "epoch:43 step:34037 [D loss: 0.312197, acc.: 85.94%] [G loss: 3.224313]\n",
      "epoch:43 step:34038 [D loss: 0.306933, acc.: 86.72%] [G loss: 3.562335]\n",
      "epoch:43 step:34039 [D loss: 0.296013, acc.: 89.06%] [G loss: 2.870185]\n",
      "epoch:43 step:34040 [D loss: 0.327240, acc.: 83.59%] [G loss: 2.880959]\n",
      "epoch:43 step:34041 [D loss: 0.349066, acc.: 83.59%] [G loss: 2.657159]\n",
      "epoch:43 step:34042 [D loss: 0.326457, acc.: 84.38%] [G loss: 2.659320]\n",
      "epoch:43 step:34043 [D loss: 0.390459, acc.: 81.25%] [G loss: 2.719473]\n",
      "epoch:43 step:34044 [D loss: 0.436279, acc.: 80.47%] [G loss: 2.533053]\n",
      "epoch:43 step:34045 [D loss: 0.276672, acc.: 91.41%] [G loss: 3.153781]\n",
      "epoch:43 step:34046 [D loss: 0.336616, acc.: 85.94%] [G loss: 2.783847]\n",
      "epoch:43 step:34047 [D loss: 0.416119, acc.: 80.47%] [G loss: 2.910025]\n",
      "epoch:43 step:34048 [D loss: 0.379723, acc.: 82.03%] [G loss: 3.280797]\n",
      "epoch:43 step:34049 [D loss: 0.388191, acc.: 81.25%] [G loss: 2.637219]\n",
      "epoch:43 step:34050 [D loss: 0.263242, acc.: 88.28%] [G loss: 2.880913]\n",
      "epoch:43 step:34051 [D loss: 0.292679, acc.: 87.50%] [G loss: 2.523386]\n",
      "epoch:43 step:34052 [D loss: 0.324002, acc.: 85.94%] [G loss: 2.626821]\n",
      "epoch:43 step:34053 [D loss: 0.289531, acc.: 88.28%] [G loss: 2.862272]\n",
      "epoch:43 step:34054 [D loss: 0.350232, acc.: 85.16%] [G loss: 2.760243]\n",
      "epoch:43 step:34055 [D loss: 0.406701, acc.: 82.03%] [G loss: 2.431499]\n",
      "epoch:43 step:34056 [D loss: 0.383554, acc.: 82.03%] [G loss: 3.234674]\n",
      "epoch:43 step:34057 [D loss: 0.348676, acc.: 85.94%] [G loss: 3.541348]\n",
      "epoch:43 step:34058 [D loss: 0.348503, acc.: 85.16%] [G loss: 3.289442]\n",
      "epoch:43 step:34059 [D loss: 0.405845, acc.: 82.81%] [G loss: 2.928969]\n",
      "epoch:43 step:34060 [D loss: 0.251855, acc.: 89.84%] [G loss: 3.480679]\n",
      "epoch:43 step:34061 [D loss: 0.313156, acc.: 89.84%] [G loss: 3.593364]\n",
      "epoch:43 step:34062 [D loss: 0.288205, acc.: 87.50%] [G loss: 3.032743]\n",
      "epoch:43 step:34063 [D loss: 0.269664, acc.: 88.28%] [G loss: 3.303502]\n",
      "epoch:43 step:34064 [D loss: 0.386568, acc.: 85.16%] [G loss: 3.568801]\n",
      "epoch:43 step:34065 [D loss: 0.350339, acc.: 88.28%] [G loss: 3.310794]\n",
      "epoch:43 step:34066 [D loss: 0.376212, acc.: 81.25%] [G loss: 2.803417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:34067 [D loss: 0.346385, acc.: 84.38%] [G loss: 3.043916]\n",
      "epoch:43 step:34068 [D loss: 0.347565, acc.: 85.16%] [G loss: 2.533957]\n",
      "epoch:43 step:34069 [D loss: 0.311006, acc.: 84.38%] [G loss: 2.222543]\n",
      "epoch:43 step:34070 [D loss: 0.324660, acc.: 86.72%] [G loss: 2.969877]\n",
      "epoch:43 step:34071 [D loss: 0.270554, acc.: 85.94%] [G loss: 2.552410]\n",
      "epoch:43 step:34072 [D loss: 0.335388, acc.: 83.59%] [G loss: 2.589166]\n",
      "epoch:43 step:34073 [D loss: 0.302918, acc.: 87.50%] [G loss: 2.777501]\n",
      "epoch:43 step:34074 [D loss: 0.394011, acc.: 82.03%] [G loss: 3.427520]\n",
      "epoch:43 step:34075 [D loss: 0.383112, acc.: 82.03%] [G loss: 3.409693]\n",
      "epoch:43 step:34076 [D loss: 0.324437, acc.: 85.94%] [G loss: 3.023401]\n",
      "epoch:43 step:34077 [D loss: 0.340695, acc.: 82.03%] [G loss: 2.844697]\n",
      "epoch:43 step:34078 [D loss: 0.346575, acc.: 84.38%] [G loss: 3.053091]\n",
      "epoch:43 step:34079 [D loss: 0.392823, acc.: 79.69%] [G loss: 2.793320]\n",
      "epoch:43 step:34080 [D loss: 0.348960, acc.: 84.38%] [G loss: 2.786029]\n",
      "epoch:43 step:34081 [D loss: 0.202380, acc.: 93.75%] [G loss: 3.277758]\n",
      "epoch:43 step:34082 [D loss: 0.363086, acc.: 82.81%] [G loss: 2.910029]\n",
      "epoch:43 step:34083 [D loss: 0.397308, acc.: 82.03%] [G loss: 3.338958]\n",
      "epoch:43 step:34084 [D loss: 0.375955, acc.: 80.47%] [G loss: 3.829170]\n",
      "epoch:43 step:34085 [D loss: 0.294552, acc.: 90.62%] [G loss: 3.177027]\n",
      "epoch:43 step:34086 [D loss: 0.381613, acc.: 82.81%] [G loss: 2.916157]\n",
      "epoch:43 step:34087 [D loss: 0.325782, acc.: 85.16%] [G loss: 3.351388]\n",
      "epoch:43 step:34088 [D loss: 0.378120, acc.: 78.91%] [G loss: 2.852658]\n",
      "epoch:43 step:34089 [D loss: 0.421491, acc.: 81.25%] [G loss: 4.325774]\n",
      "epoch:43 step:34090 [D loss: 0.370087, acc.: 85.16%] [G loss: 3.943215]\n",
      "epoch:43 step:34091 [D loss: 0.376353, acc.: 86.72%] [G loss: 3.484889]\n",
      "epoch:43 step:34092 [D loss: 0.465765, acc.: 76.56%] [G loss: 2.703751]\n",
      "epoch:43 step:34093 [D loss: 0.320148, acc.: 87.50%] [G loss: 3.123061]\n",
      "epoch:43 step:34094 [D loss: 0.321545, acc.: 86.72%] [G loss: 3.677332]\n",
      "epoch:43 step:34095 [D loss: 0.355195, acc.: 83.59%] [G loss: 3.441318]\n",
      "epoch:43 step:34096 [D loss: 0.373481, acc.: 81.25%] [G loss: 3.631289]\n",
      "epoch:43 step:34097 [D loss: 0.416054, acc.: 79.69%] [G loss: 2.719051]\n",
      "epoch:43 step:34098 [D loss: 0.318224, acc.: 82.03%] [G loss: 2.615381]\n",
      "epoch:43 step:34099 [D loss: 0.344913, acc.: 85.94%] [G loss: 2.476001]\n",
      "epoch:43 step:34100 [D loss: 0.321884, acc.: 85.16%] [G loss: 1.725213]\n",
      "epoch:43 step:34101 [D loss: 0.333188, acc.: 84.38%] [G loss: 2.643429]\n",
      "epoch:43 step:34102 [D loss: 0.379535, acc.: 84.38%] [G loss: 3.341059]\n",
      "epoch:43 step:34103 [D loss: 0.430101, acc.: 82.03%] [G loss: 2.457914]\n",
      "epoch:43 step:34104 [D loss: 0.249863, acc.: 89.06%] [G loss: 3.446876]\n",
      "epoch:43 step:34105 [D loss: 0.411003, acc.: 79.69%] [G loss: 3.692733]\n",
      "epoch:43 step:34106 [D loss: 0.349929, acc.: 80.47%] [G loss: 2.986281]\n",
      "epoch:43 step:34107 [D loss: 0.310678, acc.: 86.72%] [G loss: 3.687958]\n",
      "epoch:43 step:34108 [D loss: 0.324464, acc.: 87.50%] [G loss: 2.821148]\n",
      "epoch:43 step:34109 [D loss: 0.294872, acc.: 88.28%] [G loss: 3.814157]\n",
      "epoch:43 step:34110 [D loss: 0.269017, acc.: 89.06%] [G loss: 2.994504]\n",
      "epoch:43 step:34111 [D loss: 0.327600, acc.: 84.38%] [G loss: 2.585250]\n",
      "epoch:43 step:34112 [D loss: 0.327749, acc.: 80.47%] [G loss: 2.739768]\n",
      "epoch:43 step:34113 [D loss: 0.307675, acc.: 89.84%] [G loss: 2.828120]\n",
      "epoch:43 step:34114 [D loss: 0.336632, acc.: 83.59%] [G loss: 3.244934]\n",
      "epoch:43 step:34115 [D loss: 0.482335, acc.: 78.91%] [G loss: 3.159848]\n",
      "epoch:43 step:34116 [D loss: 0.349677, acc.: 85.16%] [G loss: 2.573981]\n",
      "epoch:43 step:34117 [D loss: 0.215646, acc.: 91.41%] [G loss: 2.901363]\n",
      "epoch:43 step:34118 [D loss: 0.344487, acc.: 85.94%] [G loss: 2.984080]\n",
      "epoch:43 step:34119 [D loss: 0.298651, acc.: 85.94%] [G loss: 2.951506]\n",
      "epoch:43 step:34120 [D loss: 0.350695, acc.: 82.03%] [G loss: 2.843285]\n",
      "epoch:43 step:34121 [D loss: 0.312089, acc.: 87.50%] [G loss: 2.616316]\n",
      "epoch:43 step:34122 [D loss: 0.219776, acc.: 93.75%] [G loss: 3.172874]\n",
      "epoch:43 step:34123 [D loss: 0.358093, acc.: 87.50%] [G loss: 2.602256]\n",
      "epoch:43 step:34124 [D loss: 0.353653, acc.: 83.59%] [G loss: 4.963711]\n",
      "epoch:43 step:34125 [D loss: 0.575798, acc.: 75.00%] [G loss: 6.207042]\n",
      "epoch:43 step:34126 [D loss: 0.727844, acc.: 73.44%] [G loss: 7.419148]\n",
      "epoch:43 step:34127 [D loss: 1.383084, acc.: 64.84%] [G loss: 4.228528]\n",
      "epoch:43 step:34128 [D loss: 0.754243, acc.: 72.66%] [G loss: 3.967790]\n",
      "epoch:43 step:34129 [D loss: 0.315242, acc.: 89.06%] [G loss: 3.740692]\n",
      "epoch:43 step:34130 [D loss: 0.595894, acc.: 70.31%] [G loss: 3.430963]\n",
      "epoch:43 step:34131 [D loss: 0.345207, acc.: 88.28%] [G loss: 4.286777]\n",
      "epoch:43 step:34132 [D loss: 0.358027, acc.: 85.16%] [G loss: 3.496379]\n",
      "epoch:43 step:34133 [D loss: 0.291454, acc.: 87.50%] [G loss: 4.863109]\n",
      "epoch:43 step:34134 [D loss: 0.324739, acc.: 85.16%] [G loss: 2.857843]\n",
      "epoch:43 step:34135 [D loss: 0.361555, acc.: 85.16%] [G loss: 3.863043]\n",
      "epoch:43 step:34136 [D loss: 0.258381, acc.: 89.84%] [G loss: 4.708754]\n",
      "epoch:43 step:34137 [D loss: 0.320798, acc.: 83.59%] [G loss: 3.680394]\n",
      "epoch:43 step:34138 [D loss: 0.421759, acc.: 79.69%] [G loss: 2.985789]\n",
      "epoch:43 step:34139 [D loss: 0.340830, acc.: 82.81%] [G loss: 3.009950]\n",
      "epoch:43 step:34140 [D loss: 0.378296, acc.: 86.72%] [G loss: 2.494645]\n",
      "epoch:43 step:34141 [D loss: 0.386270, acc.: 81.25%] [G loss: 2.833532]\n",
      "epoch:43 step:34142 [D loss: 0.293016, acc.: 89.06%] [G loss: 2.470381]\n",
      "epoch:43 step:34143 [D loss: 0.313814, acc.: 89.06%] [G loss: 2.142404]\n",
      "epoch:43 step:34144 [D loss: 0.342929, acc.: 85.16%] [G loss: 2.262892]\n",
      "epoch:43 step:34145 [D loss: 0.418090, acc.: 79.69%] [G loss: 3.273818]\n",
      "epoch:43 step:34146 [D loss: 0.249976, acc.: 89.84%] [G loss: 3.291675]\n",
      "epoch:43 step:34147 [D loss: 0.324019, acc.: 85.94%] [G loss: 3.794833]\n",
      "epoch:43 step:34148 [D loss: 0.233827, acc.: 90.62%] [G loss: 3.023862]\n",
      "epoch:43 step:34149 [D loss: 0.330186, acc.: 86.72%] [G loss: 4.318724]\n",
      "epoch:43 step:34150 [D loss: 0.271698, acc.: 86.72%] [G loss: 4.127847]\n",
      "epoch:43 step:34151 [D loss: 0.306798, acc.: 82.81%] [G loss: 4.200992]\n",
      "epoch:43 step:34152 [D loss: 0.297938, acc.: 85.16%] [G loss: 2.646599]\n",
      "epoch:43 step:34153 [D loss: 0.255708, acc.: 90.62%] [G loss: 3.188322]\n",
      "epoch:43 step:34154 [D loss: 0.249231, acc.: 91.41%] [G loss: 2.956561]\n",
      "epoch:43 step:34155 [D loss: 0.296139, acc.: 87.50%] [G loss: 2.747425]\n",
      "epoch:43 step:34156 [D loss: 0.401184, acc.: 80.47%] [G loss: 3.395534]\n",
      "epoch:43 step:34157 [D loss: 0.368270, acc.: 81.25%] [G loss: 2.816293]\n",
      "epoch:43 step:34158 [D loss: 0.314010, acc.: 89.06%] [G loss: 2.916882]\n",
      "epoch:43 step:34159 [D loss: 0.343344, acc.: 84.38%] [G loss: 3.187077]\n",
      "epoch:43 step:34160 [D loss: 0.319803, acc.: 85.94%] [G loss: 2.922059]\n",
      "epoch:43 step:34161 [D loss: 0.411296, acc.: 81.25%] [G loss: 4.591390]\n",
      "epoch:43 step:34162 [D loss: 0.267529, acc.: 90.62%] [G loss: 4.612026]\n",
      "epoch:43 step:34163 [D loss: 0.219669, acc.: 92.19%] [G loss: 3.670217]\n",
      "epoch:43 step:34164 [D loss: 0.256859, acc.: 87.50%] [G loss: 4.667516]\n",
      "epoch:43 step:34165 [D loss: 0.268847, acc.: 87.50%] [G loss: 3.540600]\n",
      "epoch:43 step:34166 [D loss: 0.337041, acc.: 86.72%] [G loss: 3.342031]\n",
      "epoch:43 step:34167 [D loss: 0.269260, acc.: 89.06%] [G loss: 3.787560]\n",
      "epoch:43 step:34168 [D loss: 0.240064, acc.: 89.84%] [G loss: 3.318001]\n",
      "epoch:43 step:34169 [D loss: 0.311760, acc.: 84.38%] [G loss: 3.448236]\n",
      "epoch:43 step:34170 [D loss: 0.252669, acc.: 89.84%] [G loss: 3.330703]\n",
      "epoch:43 step:34171 [D loss: 0.275307, acc.: 87.50%] [G loss: 2.281278]\n",
      "epoch:43 step:34172 [D loss: 0.323214, acc.: 85.94%] [G loss: 3.173925]\n",
      "epoch:43 step:34173 [D loss: 0.269550, acc.: 89.84%] [G loss: 3.249867]\n",
      "epoch:43 step:34174 [D loss: 0.349040, acc.: 84.38%] [G loss: 3.208354]\n",
      "epoch:43 step:34175 [D loss: 0.351288, acc.: 84.38%] [G loss: 3.607764]\n",
      "epoch:43 step:34176 [D loss: 0.387280, acc.: 84.38%] [G loss: 2.803672]\n",
      "epoch:43 step:34177 [D loss: 0.263755, acc.: 88.28%] [G loss: 3.361654]\n",
      "epoch:43 step:34178 [D loss: 0.333203, acc.: 84.38%] [G loss: 2.714592]\n",
      "epoch:43 step:34179 [D loss: 0.341794, acc.: 86.72%] [G loss: 2.987953]\n",
      "epoch:43 step:34180 [D loss: 0.370856, acc.: 85.16%] [G loss: 1.988811]\n",
      "epoch:43 step:34181 [D loss: 0.233112, acc.: 90.62%] [G loss: 3.810697]\n",
      "epoch:43 step:34182 [D loss: 0.423355, acc.: 82.81%] [G loss: 3.331616]\n",
      "epoch:43 step:34183 [D loss: 0.366653, acc.: 82.03%] [G loss: 2.855022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:34184 [D loss: 0.427097, acc.: 78.12%] [G loss: 3.279047]\n",
      "epoch:43 step:34185 [D loss: 0.316107, acc.: 85.94%] [G loss: 2.888190]\n",
      "epoch:43 step:34186 [D loss: 0.261353, acc.: 85.16%] [G loss: 4.216479]\n",
      "epoch:43 step:34187 [D loss: 0.383294, acc.: 81.25%] [G loss: 3.426274]\n",
      "epoch:43 step:34188 [D loss: 0.392832, acc.: 83.59%] [G loss: 3.291140]\n",
      "epoch:43 step:34189 [D loss: 0.361517, acc.: 80.47%] [G loss: 3.223324]\n",
      "epoch:43 step:34190 [D loss: 0.393941, acc.: 78.91%] [G loss: 4.422136]\n",
      "epoch:43 step:34191 [D loss: 0.262478, acc.: 87.50%] [G loss: 2.945959]\n",
      "epoch:43 step:34192 [D loss: 0.286436, acc.: 85.16%] [G loss: 3.332575]\n",
      "epoch:43 step:34193 [D loss: 0.306947, acc.: 85.16%] [G loss: 2.942148]\n",
      "epoch:43 step:34194 [D loss: 0.351273, acc.: 85.16%] [G loss: 3.609333]\n",
      "epoch:43 step:34195 [D loss: 0.377496, acc.: 85.94%] [G loss: 2.880251]\n",
      "epoch:43 step:34196 [D loss: 0.271325, acc.: 89.84%] [G loss: 2.806515]\n",
      "epoch:43 step:34197 [D loss: 0.235853, acc.: 90.62%] [G loss: 3.518222]\n",
      "epoch:43 step:34198 [D loss: 0.263280, acc.: 91.41%] [G loss: 2.611009]\n",
      "epoch:43 step:34199 [D loss: 0.299135, acc.: 84.38%] [G loss: 3.304421]\n",
      "epoch:43 step:34200 [D loss: 0.289675, acc.: 85.94%] [G loss: 2.954856]\n",
      "epoch:43 step:34201 [D loss: 0.341732, acc.: 85.16%] [G loss: 2.408330]\n",
      "epoch:43 step:34202 [D loss: 0.313004, acc.: 89.06%] [G loss: 2.721941]\n",
      "epoch:43 step:34203 [D loss: 0.249157, acc.: 90.62%] [G loss: 2.513384]\n",
      "epoch:43 step:34204 [D loss: 0.325730, acc.: 87.50%] [G loss: 2.163395]\n",
      "epoch:43 step:34205 [D loss: 0.353237, acc.: 85.94%] [G loss: 3.104154]\n",
      "epoch:43 step:34206 [D loss: 0.323443, acc.: 85.16%] [G loss: 4.144418]\n",
      "epoch:43 step:34207 [D loss: 0.271434, acc.: 87.50%] [G loss: 3.686903]\n",
      "epoch:43 step:34208 [D loss: 0.276261, acc.: 87.50%] [G loss: 3.444560]\n",
      "epoch:43 step:34209 [D loss: 0.407849, acc.: 78.91%] [G loss: 2.318639]\n",
      "epoch:43 step:34210 [D loss: 0.320361, acc.: 87.50%] [G loss: 3.039259]\n",
      "epoch:43 step:34211 [D loss: 0.367159, acc.: 85.16%] [G loss: 2.474712]\n",
      "epoch:43 step:34212 [D loss: 0.321558, acc.: 84.38%] [G loss: 2.713343]\n",
      "epoch:43 step:34213 [D loss: 0.232927, acc.: 92.19%] [G loss: 3.281826]\n",
      "epoch:43 step:34214 [D loss: 0.311368, acc.: 87.50%] [G loss: 4.291734]\n",
      "epoch:43 step:34215 [D loss: 0.294207, acc.: 85.16%] [G loss: 2.619468]\n",
      "epoch:43 step:34216 [D loss: 0.364690, acc.: 78.91%] [G loss: 2.470188]\n",
      "epoch:43 step:34217 [D loss: 0.389807, acc.: 81.25%] [G loss: 3.439686]\n",
      "epoch:43 step:34218 [D loss: 0.210115, acc.: 93.75%] [G loss: 3.286715]\n",
      "epoch:43 step:34219 [D loss: 0.182826, acc.: 93.75%] [G loss: 4.456694]\n",
      "epoch:43 step:34220 [D loss: 0.240719, acc.: 88.28%] [G loss: 3.079221]\n",
      "epoch:43 step:34221 [D loss: 0.335860, acc.: 85.16%] [G loss: 3.223798]\n",
      "epoch:43 step:34222 [D loss: 0.282666, acc.: 86.72%] [G loss: 2.296129]\n",
      "epoch:43 step:34223 [D loss: 0.252708, acc.: 89.06%] [G loss: 2.910194]\n",
      "epoch:43 step:34224 [D loss: 0.258580, acc.: 89.84%] [G loss: 2.706530]\n",
      "epoch:43 step:34225 [D loss: 0.417151, acc.: 83.59%] [G loss: 3.086432]\n",
      "epoch:43 step:34226 [D loss: 0.336677, acc.: 85.16%] [G loss: 2.378676]\n",
      "epoch:43 step:34227 [D loss: 0.280469, acc.: 86.72%] [G loss: 2.778043]\n",
      "epoch:43 step:34228 [D loss: 0.357896, acc.: 84.38%] [G loss: 2.521060]\n",
      "epoch:43 step:34229 [D loss: 0.369814, acc.: 82.81%] [G loss: 3.260541]\n",
      "epoch:43 step:34230 [D loss: 0.269476, acc.: 88.28%] [G loss: 2.700402]\n",
      "epoch:43 step:34231 [D loss: 0.270872, acc.: 91.41%] [G loss: 2.821501]\n",
      "epoch:43 step:34232 [D loss: 0.425767, acc.: 80.47%] [G loss: 3.488719]\n",
      "epoch:43 step:34233 [D loss: 0.334784, acc.: 82.81%] [G loss: 3.491061]\n",
      "epoch:43 step:34234 [D loss: 0.321228, acc.: 86.72%] [G loss: 3.628923]\n",
      "epoch:43 step:34235 [D loss: 0.265263, acc.: 89.84%] [G loss: 3.348578]\n",
      "epoch:43 step:34236 [D loss: 0.260854, acc.: 89.06%] [G loss: 4.403911]\n",
      "epoch:43 step:34237 [D loss: 0.355028, acc.: 82.03%] [G loss: 3.353477]\n",
      "epoch:43 step:34238 [D loss: 0.281381, acc.: 89.06%] [G loss: 4.218047]\n",
      "epoch:43 step:34239 [D loss: 0.297857, acc.: 83.59%] [G loss: 2.892545]\n",
      "epoch:43 step:34240 [D loss: 0.304098, acc.: 86.72%] [G loss: 3.727567]\n",
      "epoch:43 step:34241 [D loss: 0.390000, acc.: 82.81%] [G loss: 3.385327]\n",
      "epoch:43 step:34242 [D loss: 0.338437, acc.: 85.16%] [G loss: 3.394668]\n",
      "epoch:43 step:34243 [D loss: 0.267382, acc.: 88.28%] [G loss: 4.013772]\n",
      "epoch:43 step:34244 [D loss: 0.236836, acc.: 91.41%] [G loss: 3.343228]\n",
      "epoch:43 step:34245 [D loss: 0.299547, acc.: 88.28%] [G loss: 4.039151]\n",
      "epoch:43 step:34246 [D loss: 0.316248, acc.: 84.38%] [G loss: 3.668663]\n",
      "epoch:43 step:34247 [D loss: 0.317180, acc.: 85.94%] [G loss: 3.834816]\n",
      "epoch:43 step:34248 [D loss: 0.307713, acc.: 89.06%] [G loss: 3.679917]\n",
      "epoch:43 step:34249 [D loss: 0.348962, acc.: 83.59%] [G loss: 3.424771]\n",
      "epoch:43 step:34250 [D loss: 0.262115, acc.: 86.72%] [G loss: 4.759311]\n",
      "epoch:43 step:34251 [D loss: 0.249096, acc.: 89.06%] [G loss: 4.257931]\n",
      "epoch:43 step:34252 [D loss: 0.312257, acc.: 85.94%] [G loss: 3.795297]\n",
      "epoch:43 step:34253 [D loss: 0.314781, acc.: 87.50%] [G loss: 2.874259]\n",
      "epoch:43 step:34254 [D loss: 0.262817, acc.: 90.62%] [G loss: 3.815873]\n",
      "epoch:43 step:34255 [D loss: 0.392312, acc.: 81.25%] [G loss: 3.559983]\n",
      "epoch:43 step:34256 [D loss: 0.302067, acc.: 85.94%] [G loss: 3.710446]\n",
      "epoch:43 step:34257 [D loss: 0.355558, acc.: 83.59%] [G loss: 2.743043]\n",
      "epoch:43 step:34258 [D loss: 0.349967, acc.: 86.72%] [G loss: 4.557708]\n",
      "epoch:43 step:34259 [D loss: 0.267570, acc.: 89.06%] [G loss: 3.364432]\n",
      "epoch:43 step:34260 [D loss: 0.324569, acc.: 84.38%] [G loss: 4.551948]\n",
      "epoch:43 step:34261 [D loss: 0.214165, acc.: 92.19%] [G loss: 3.034621]\n",
      "epoch:43 step:34262 [D loss: 0.398101, acc.: 82.03%] [G loss: 4.468708]\n",
      "epoch:43 step:34263 [D loss: 0.314744, acc.: 86.72%] [G loss: 3.453142]\n",
      "epoch:43 step:34264 [D loss: 0.221691, acc.: 89.06%] [G loss: 2.718503]\n",
      "epoch:43 step:34265 [D loss: 0.466507, acc.: 81.25%] [G loss: 2.791766]\n",
      "epoch:43 step:34266 [D loss: 0.292989, acc.: 85.16%] [G loss: 3.297550]\n",
      "epoch:43 step:34267 [D loss: 0.313062, acc.: 85.16%] [G loss: 3.054897]\n",
      "epoch:43 step:34268 [D loss: 0.336176, acc.: 85.16%] [G loss: 3.145727]\n",
      "epoch:43 step:34269 [D loss: 0.378114, acc.: 82.81%] [G loss: 2.554005]\n",
      "epoch:43 step:34270 [D loss: 0.317377, acc.: 86.72%] [G loss: 2.876869]\n",
      "epoch:43 step:34271 [D loss: 0.377995, acc.: 78.12%] [G loss: 3.133672]\n",
      "epoch:43 step:34272 [D loss: 0.278719, acc.: 88.28%] [G loss: 3.520188]\n",
      "epoch:43 step:34273 [D loss: 0.288037, acc.: 88.28%] [G loss: 3.248495]\n",
      "epoch:43 step:34274 [D loss: 0.292688, acc.: 88.28%] [G loss: 3.603078]\n",
      "epoch:43 step:34275 [D loss: 0.396118, acc.: 81.25%] [G loss: 3.298038]\n",
      "epoch:43 step:34276 [D loss: 0.348102, acc.: 83.59%] [G loss: 3.957804]\n",
      "epoch:43 step:34277 [D loss: 0.337579, acc.: 82.81%] [G loss: 4.531415]\n",
      "epoch:43 step:34278 [D loss: 0.386357, acc.: 85.16%] [G loss: 4.287659]\n",
      "epoch:43 step:34279 [D loss: 0.336639, acc.: 84.38%] [G loss: 4.193719]\n",
      "epoch:43 step:34280 [D loss: 0.386509, acc.: 85.16%] [G loss: 3.277742]\n",
      "epoch:43 step:34281 [D loss: 0.221185, acc.: 88.28%] [G loss: 2.842845]\n",
      "epoch:43 step:34282 [D loss: 0.223807, acc.: 91.41%] [G loss: 2.869568]\n",
      "epoch:43 step:34283 [D loss: 0.235578, acc.: 89.84%] [G loss: 2.555062]\n",
      "epoch:43 step:34284 [D loss: 0.237281, acc.: 90.62%] [G loss: 2.678443]\n",
      "epoch:43 step:34285 [D loss: 0.245276, acc.: 89.06%] [G loss: 2.854475]\n",
      "epoch:43 step:34286 [D loss: 0.381649, acc.: 80.47%] [G loss: 4.398480]\n",
      "epoch:43 step:34287 [D loss: 0.631726, acc.: 76.56%] [G loss: 5.761891]\n",
      "epoch:43 step:34288 [D loss: 0.695080, acc.: 76.56%] [G loss: 5.812612]\n",
      "epoch:43 step:34289 [D loss: 0.617421, acc.: 70.31%] [G loss: 4.088387]\n",
      "epoch:43 step:34290 [D loss: 0.409740, acc.: 84.38%] [G loss: 5.421385]\n",
      "epoch:43 step:34291 [D loss: 0.381732, acc.: 83.59%] [G loss: 5.651220]\n",
      "epoch:43 step:34292 [D loss: 0.308273, acc.: 85.94%] [G loss: 5.113731]\n",
      "epoch:43 step:34293 [D loss: 0.285997, acc.: 86.72%] [G loss: 3.831910]\n",
      "epoch:43 step:34294 [D loss: 0.381028, acc.: 79.69%] [G loss: 3.232713]\n",
      "epoch:43 step:34295 [D loss: 0.346662, acc.: 87.50%] [G loss: 2.860444]\n",
      "epoch:43 step:34296 [D loss: 0.402218, acc.: 82.03%] [G loss: 3.186224]\n",
      "epoch:43 step:34297 [D loss: 0.426021, acc.: 84.38%] [G loss: 3.623554]\n",
      "epoch:43 step:34298 [D loss: 0.381191, acc.: 84.38%] [G loss: 2.909810]\n",
      "epoch:43 step:34299 [D loss: 0.306539, acc.: 89.06%] [G loss: 2.877958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:34300 [D loss: 0.352080, acc.: 89.06%] [G loss: 2.836376]\n",
      "epoch:43 step:34301 [D loss: 0.391682, acc.: 82.81%] [G loss: 3.574489]\n",
      "epoch:43 step:34302 [D loss: 0.224548, acc.: 89.84%] [G loss: 4.038344]\n",
      "epoch:43 step:34303 [D loss: 0.312233, acc.: 85.16%] [G loss: 3.653346]\n",
      "epoch:43 step:34304 [D loss: 0.325848, acc.: 85.16%] [G loss: 3.767899]\n",
      "epoch:43 step:34305 [D loss: 0.341097, acc.: 85.16%] [G loss: 3.205796]\n",
      "epoch:43 step:34306 [D loss: 0.388691, acc.: 82.81%] [G loss: 2.987021]\n",
      "epoch:43 step:34307 [D loss: 0.289200, acc.: 83.59%] [G loss: 2.927616]\n",
      "epoch:43 step:34308 [D loss: 0.230383, acc.: 91.41%] [G loss: 3.703635]\n",
      "epoch:43 step:34309 [D loss: 0.300329, acc.: 87.50%] [G loss: 2.758666]\n",
      "epoch:43 step:34310 [D loss: 0.295920, acc.: 84.38%] [G loss: 2.560364]\n",
      "epoch:43 step:34311 [D loss: 0.352854, acc.: 82.03%] [G loss: 3.915515]\n",
      "epoch:43 step:34312 [D loss: 0.272490, acc.: 91.41%] [G loss: 4.698377]\n",
      "epoch:43 step:34313 [D loss: 0.294567, acc.: 88.28%] [G loss: 4.156244]\n",
      "epoch:43 step:34314 [D loss: 0.298589, acc.: 85.94%] [G loss: 3.453608]\n",
      "epoch:43 step:34315 [D loss: 0.325893, acc.: 85.94%] [G loss: 2.592556]\n",
      "epoch:43 step:34316 [D loss: 0.227679, acc.: 92.19%] [G loss: 3.423153]\n",
      "epoch:43 step:34317 [D loss: 0.438392, acc.: 75.78%] [G loss: 3.088345]\n",
      "epoch:43 step:34318 [D loss: 0.327163, acc.: 87.50%] [G loss: 4.373796]\n",
      "epoch:43 step:34319 [D loss: 0.325303, acc.: 87.50%] [G loss: 2.930045]\n",
      "epoch:43 step:34320 [D loss: 0.249543, acc.: 85.94%] [G loss: 4.126860]\n",
      "epoch:43 step:34321 [D loss: 0.304954, acc.: 87.50%] [G loss: 2.689563]\n",
      "epoch:43 step:34322 [D loss: 0.254555, acc.: 89.06%] [G loss: 3.992864]\n",
      "epoch:43 step:34323 [D loss: 0.388047, acc.: 83.59%] [G loss: 3.015798]\n",
      "epoch:43 step:34324 [D loss: 0.205866, acc.: 91.41%] [G loss: 3.362622]\n",
      "epoch:43 step:34325 [D loss: 0.277862, acc.: 89.84%] [G loss: 3.350309]\n",
      "epoch:43 step:34326 [D loss: 0.292591, acc.: 85.94%] [G loss: 2.964170]\n",
      "epoch:43 step:34327 [D loss: 0.323946, acc.: 85.94%] [G loss: 2.654170]\n",
      "epoch:43 step:34328 [D loss: 0.349664, acc.: 89.06%] [G loss: 3.067652]\n",
      "epoch:43 step:34329 [D loss: 0.340423, acc.: 86.72%] [G loss: 3.860998]\n",
      "epoch:43 step:34330 [D loss: 0.346787, acc.: 84.38%] [G loss: 3.595319]\n",
      "epoch:43 step:34331 [D loss: 0.309529, acc.: 85.94%] [G loss: 2.946398]\n",
      "epoch:43 step:34332 [D loss: 0.348989, acc.: 85.94%] [G loss: 3.014201]\n",
      "epoch:43 step:34333 [D loss: 0.326044, acc.: 83.59%] [G loss: 3.477867]\n",
      "epoch:43 step:34334 [D loss: 0.231752, acc.: 90.62%] [G loss: 3.400962]\n",
      "epoch:43 step:34335 [D loss: 0.253988, acc.: 89.84%] [G loss: 3.025198]\n",
      "epoch:43 step:34336 [D loss: 0.484769, acc.: 78.91%] [G loss: 4.545176]\n",
      "epoch:43 step:34337 [D loss: 0.378830, acc.: 83.59%] [G loss: 5.158100]\n",
      "epoch:43 step:34338 [D loss: 0.484019, acc.: 77.34%] [G loss: 3.628819]\n",
      "epoch:43 step:34339 [D loss: 0.438693, acc.: 81.25%] [G loss: 3.056757]\n",
      "epoch:43 step:34340 [D loss: 0.381012, acc.: 83.59%] [G loss: 3.390932]\n",
      "epoch:43 step:34341 [D loss: 0.377426, acc.: 83.59%] [G loss: 3.087611]\n",
      "epoch:43 step:34342 [D loss: 0.304827, acc.: 87.50%] [G loss: 2.913843]\n",
      "epoch:43 step:34343 [D loss: 0.312005, acc.: 86.72%] [G loss: 2.997952]\n",
      "epoch:43 step:34344 [D loss: 0.307006, acc.: 87.50%] [G loss: 3.208875]\n",
      "epoch:43 step:34345 [D loss: 0.338964, acc.: 85.16%] [G loss: 4.322152]\n",
      "epoch:43 step:34346 [D loss: 0.274765, acc.: 88.28%] [G loss: 3.084323]\n",
      "epoch:43 step:34347 [D loss: 0.322608, acc.: 79.69%] [G loss: 2.958213]\n",
      "epoch:43 step:34348 [D loss: 0.386008, acc.: 78.91%] [G loss: 2.336384]\n",
      "epoch:43 step:34349 [D loss: 0.309802, acc.: 87.50%] [G loss: 2.830857]\n",
      "epoch:43 step:34350 [D loss: 0.275034, acc.: 90.62%] [G loss: 3.222476]\n",
      "epoch:43 step:34351 [D loss: 0.294210, acc.: 85.94%] [G loss: 3.045660]\n",
      "epoch:43 step:34352 [D loss: 0.231022, acc.: 89.84%] [G loss: 4.250281]\n",
      "epoch:43 step:34353 [D loss: 0.218386, acc.: 91.41%] [G loss: 4.334212]\n",
      "epoch:43 step:34354 [D loss: 0.335528, acc.: 85.16%] [G loss: 3.498676]\n",
      "epoch:43 step:34355 [D loss: 0.211455, acc.: 91.41%] [G loss: 3.275468]\n",
      "epoch:43 step:34356 [D loss: 0.336301, acc.: 82.03%] [G loss: 2.933132]\n",
      "epoch:43 step:34357 [D loss: 0.214385, acc.: 92.19%] [G loss: 2.779215]\n",
      "epoch:43 step:34358 [D loss: 0.459314, acc.: 77.34%] [G loss: 2.722875]\n",
      "epoch:43 step:34359 [D loss: 0.185552, acc.: 93.75%] [G loss: 2.896104]\n",
      "epoch:43 step:34360 [D loss: 0.373526, acc.: 81.25%] [G loss: 2.628833]\n",
      "epoch:43 step:34361 [D loss: 0.310375, acc.: 86.72%] [G loss: 3.389431]\n",
      "epoch:43 step:34362 [D loss: 0.231141, acc.: 89.84%] [G loss: 3.174347]\n",
      "epoch:43 step:34363 [D loss: 0.235406, acc.: 91.41%] [G loss: 3.821760]\n",
      "epoch:43 step:34364 [D loss: 0.305523, acc.: 86.72%] [G loss: 2.866703]\n",
      "epoch:44 step:34365 [D loss: 0.270273, acc.: 89.06%] [G loss: 2.680957]\n",
      "epoch:44 step:34366 [D loss: 0.362128, acc.: 83.59%] [G loss: 2.564560]\n",
      "epoch:44 step:34367 [D loss: 0.311778, acc.: 85.16%] [G loss: 4.163383]\n",
      "epoch:44 step:34368 [D loss: 0.272854, acc.: 89.06%] [G loss: 3.392795]\n",
      "epoch:44 step:34369 [D loss: 0.318614, acc.: 84.38%] [G loss: 3.469877]\n",
      "epoch:44 step:34370 [D loss: 0.280573, acc.: 87.50%] [G loss: 4.028740]\n",
      "epoch:44 step:34371 [D loss: 0.244307, acc.: 88.28%] [G loss: 3.065985]\n",
      "epoch:44 step:34372 [D loss: 0.258256, acc.: 83.59%] [G loss: 3.008043]\n",
      "epoch:44 step:34373 [D loss: 0.260188, acc.: 87.50%] [G loss: 2.626604]\n",
      "epoch:44 step:34374 [D loss: 0.295228, acc.: 89.06%] [G loss: 2.724826]\n",
      "epoch:44 step:34375 [D loss: 0.315329, acc.: 87.50%] [G loss: 1.898977]\n",
      "epoch:44 step:34376 [D loss: 0.388976, acc.: 83.59%] [G loss: 2.564083]\n",
      "epoch:44 step:34377 [D loss: 0.314136, acc.: 89.06%] [G loss: 2.342010]\n",
      "epoch:44 step:34378 [D loss: 0.294878, acc.: 87.50%] [G loss: 2.662982]\n",
      "epoch:44 step:34379 [D loss: 0.346285, acc.: 83.59%] [G loss: 2.496449]\n",
      "epoch:44 step:34380 [D loss: 0.385982, acc.: 81.25%] [G loss: 2.684281]\n",
      "epoch:44 step:34381 [D loss: 0.273833, acc.: 89.06%] [G loss: 3.063286]\n",
      "epoch:44 step:34382 [D loss: 0.288203, acc.: 87.50%] [G loss: 3.312357]\n",
      "epoch:44 step:34383 [D loss: 0.292375, acc.: 87.50%] [G loss: 3.608709]\n",
      "epoch:44 step:34384 [D loss: 0.284975, acc.: 88.28%] [G loss: 2.298116]\n",
      "epoch:44 step:34385 [D loss: 0.344416, acc.: 86.72%] [G loss: 2.799206]\n",
      "epoch:44 step:34386 [D loss: 0.379761, acc.: 84.38%] [G loss: 2.687871]\n",
      "epoch:44 step:34387 [D loss: 0.265560, acc.: 90.62%] [G loss: 2.861906]\n",
      "epoch:44 step:34388 [D loss: 0.403757, acc.: 82.03%] [G loss: 2.929606]\n",
      "epoch:44 step:34389 [D loss: 0.293153, acc.: 84.38%] [G loss: 2.823791]\n",
      "epoch:44 step:34390 [D loss: 0.398202, acc.: 85.16%] [G loss: 2.478968]\n",
      "epoch:44 step:34391 [D loss: 0.223302, acc.: 92.97%] [G loss: 3.021549]\n",
      "epoch:44 step:34392 [D loss: 0.264121, acc.: 85.16%] [G loss: 3.604578]\n",
      "epoch:44 step:34393 [D loss: 0.454995, acc.: 78.12%] [G loss: 3.877666]\n",
      "epoch:44 step:34394 [D loss: 0.201388, acc.: 91.41%] [G loss: 2.917659]\n",
      "epoch:44 step:34395 [D loss: 0.297541, acc.: 85.16%] [G loss: 3.275377]\n",
      "epoch:44 step:34396 [D loss: 0.339812, acc.: 82.03%] [G loss: 2.971811]\n",
      "epoch:44 step:34397 [D loss: 0.265702, acc.: 87.50%] [G loss: 2.925800]\n",
      "epoch:44 step:34398 [D loss: 0.337399, acc.: 83.59%] [G loss: 4.222596]\n",
      "epoch:44 step:34399 [D loss: 0.310733, acc.: 86.72%] [G loss: 4.062175]\n",
      "epoch:44 step:34400 [D loss: 0.266918, acc.: 86.72%] [G loss: 2.905595]\n",
      "epoch:44 step:34401 [D loss: 0.282386, acc.: 87.50%] [G loss: 3.586159]\n",
      "epoch:44 step:34402 [D loss: 0.398293, acc.: 78.91%] [G loss: 3.309311]\n",
      "epoch:44 step:34403 [D loss: 0.365775, acc.: 81.25%] [G loss: 2.525970]\n",
      "epoch:44 step:34404 [D loss: 0.348536, acc.: 82.03%] [G loss: 2.688072]\n",
      "epoch:44 step:34405 [D loss: 0.399177, acc.: 81.25%] [G loss: 2.813267]\n",
      "epoch:44 step:34406 [D loss: 0.303264, acc.: 80.47%] [G loss: 2.542356]\n",
      "epoch:44 step:34407 [D loss: 0.399526, acc.: 77.34%] [G loss: 3.128428]\n",
      "epoch:44 step:34408 [D loss: 0.407790, acc.: 82.81%] [G loss: 3.153744]\n",
      "epoch:44 step:34409 [D loss: 0.392390, acc.: 84.38%] [G loss: 3.640873]\n",
      "epoch:44 step:34410 [D loss: 0.328321, acc.: 85.16%] [G loss: 2.352390]\n",
      "epoch:44 step:34411 [D loss: 0.396304, acc.: 81.25%] [G loss: 2.853528]\n",
      "epoch:44 step:34412 [D loss: 0.323004, acc.: 85.94%] [G loss: 2.526055]\n",
      "epoch:44 step:34413 [D loss: 0.390658, acc.: 82.03%] [G loss: 2.985442]\n",
      "epoch:44 step:34414 [D loss: 0.388611, acc.: 83.59%] [G loss: 2.633728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34415 [D loss: 0.282017, acc.: 84.38%] [G loss: 2.088660]\n",
      "epoch:44 step:34416 [D loss: 0.321906, acc.: 85.16%] [G loss: 2.432667]\n",
      "epoch:44 step:34417 [D loss: 0.251116, acc.: 89.06%] [G loss: 2.342135]\n",
      "epoch:44 step:34418 [D loss: 0.308479, acc.: 85.94%] [G loss: 3.160871]\n",
      "epoch:44 step:34419 [D loss: 0.305459, acc.: 89.06%] [G loss: 3.073819]\n",
      "epoch:44 step:34420 [D loss: 0.301976, acc.: 81.25%] [G loss: 2.541276]\n",
      "epoch:44 step:34421 [D loss: 0.273385, acc.: 88.28%] [G loss: 2.839361]\n",
      "epoch:44 step:34422 [D loss: 0.305703, acc.: 85.94%] [G loss: 3.052281]\n",
      "epoch:44 step:34423 [D loss: 0.331384, acc.: 82.03%] [G loss: 2.742592]\n",
      "epoch:44 step:34424 [D loss: 0.264562, acc.: 89.84%] [G loss: 2.607979]\n",
      "epoch:44 step:34425 [D loss: 0.306913, acc.: 85.16%] [G loss: 3.132005]\n",
      "epoch:44 step:34426 [D loss: 0.302070, acc.: 87.50%] [G loss: 3.036649]\n",
      "epoch:44 step:34427 [D loss: 0.225858, acc.: 92.19%] [G loss: 4.095711]\n",
      "epoch:44 step:34428 [D loss: 0.326520, acc.: 84.38%] [G loss: 2.964228]\n",
      "epoch:44 step:34429 [D loss: 0.303048, acc.: 87.50%] [G loss: 2.764771]\n",
      "epoch:44 step:34430 [D loss: 0.321782, acc.: 82.81%] [G loss: 3.518186]\n",
      "epoch:44 step:34431 [D loss: 0.261254, acc.: 89.06%] [G loss: 3.876981]\n",
      "epoch:44 step:34432 [D loss: 0.358904, acc.: 81.25%] [G loss: 3.531082]\n",
      "epoch:44 step:34433 [D loss: 0.276585, acc.: 87.50%] [G loss: 3.040539]\n",
      "epoch:44 step:34434 [D loss: 0.411857, acc.: 80.47%] [G loss: 2.556903]\n",
      "epoch:44 step:34435 [D loss: 0.333470, acc.: 86.72%] [G loss: 3.214895]\n",
      "epoch:44 step:34436 [D loss: 0.259812, acc.: 91.41%] [G loss: 2.780146]\n",
      "epoch:44 step:34437 [D loss: 0.384583, acc.: 81.25%] [G loss: 2.924921]\n",
      "epoch:44 step:34438 [D loss: 0.284361, acc.: 87.50%] [G loss: 4.486001]\n",
      "epoch:44 step:34439 [D loss: 0.483318, acc.: 78.12%] [G loss: 5.570615]\n",
      "epoch:44 step:34440 [D loss: 0.313917, acc.: 83.59%] [G loss: 4.085071]\n",
      "epoch:44 step:34441 [D loss: 0.353812, acc.: 83.59%] [G loss: 4.718013]\n",
      "epoch:44 step:34442 [D loss: 0.310557, acc.: 84.38%] [G loss: 3.084683]\n",
      "epoch:44 step:34443 [D loss: 0.411459, acc.: 82.03%] [G loss: 4.909681]\n",
      "epoch:44 step:34444 [D loss: 0.498008, acc.: 75.78%] [G loss: 3.696540]\n",
      "epoch:44 step:34445 [D loss: 0.304350, acc.: 86.72%] [G loss: 4.525125]\n",
      "epoch:44 step:34446 [D loss: 0.427659, acc.: 83.59%] [G loss: 7.194745]\n",
      "epoch:44 step:34447 [D loss: 0.245591, acc.: 91.41%] [G loss: 4.290784]\n",
      "epoch:44 step:34448 [D loss: 0.260606, acc.: 88.28%] [G loss: 6.330463]\n",
      "epoch:44 step:34449 [D loss: 0.342396, acc.: 80.47%] [G loss: 3.388345]\n",
      "epoch:44 step:34450 [D loss: 0.256555, acc.: 87.50%] [G loss: 4.776772]\n",
      "epoch:44 step:34451 [D loss: 0.380669, acc.: 83.59%] [G loss: 3.233154]\n",
      "epoch:44 step:34452 [D loss: 0.319803, acc.: 86.72%] [G loss: 3.182856]\n",
      "epoch:44 step:34453 [D loss: 0.299601, acc.: 85.16%] [G loss: 4.417164]\n",
      "epoch:44 step:34454 [D loss: 0.434744, acc.: 78.91%] [G loss: 3.429317]\n",
      "epoch:44 step:34455 [D loss: 0.276917, acc.: 84.38%] [G loss: 3.952043]\n",
      "epoch:44 step:34456 [D loss: 0.297855, acc.: 88.28%] [G loss: 3.732747]\n",
      "epoch:44 step:34457 [D loss: 0.376835, acc.: 83.59%] [G loss: 3.441832]\n",
      "epoch:44 step:34458 [D loss: 0.217453, acc.: 92.97%] [G loss: 3.401434]\n",
      "epoch:44 step:34459 [D loss: 0.343940, acc.: 84.38%] [G loss: 3.565211]\n",
      "epoch:44 step:34460 [D loss: 0.391150, acc.: 81.25%] [G loss: 3.381665]\n",
      "epoch:44 step:34461 [D loss: 0.304500, acc.: 86.72%] [G loss: 3.331299]\n",
      "epoch:44 step:34462 [D loss: 0.343792, acc.: 83.59%] [G loss: 3.047711]\n",
      "epoch:44 step:34463 [D loss: 0.351663, acc.: 85.16%] [G loss: 2.956556]\n",
      "epoch:44 step:34464 [D loss: 0.364022, acc.: 83.59%] [G loss: 2.619506]\n",
      "epoch:44 step:34465 [D loss: 0.295122, acc.: 86.72%] [G loss: 3.818870]\n",
      "epoch:44 step:34466 [D loss: 0.311088, acc.: 88.28%] [G loss: 3.986288]\n",
      "epoch:44 step:34467 [D loss: 0.314841, acc.: 88.28%] [G loss: 3.127840]\n",
      "epoch:44 step:34468 [D loss: 0.350993, acc.: 85.94%] [G loss: 5.377914]\n",
      "epoch:44 step:34469 [D loss: 0.349792, acc.: 86.72%] [G loss: 4.234599]\n",
      "epoch:44 step:34470 [D loss: 0.337092, acc.: 83.59%] [G loss: 3.278783]\n",
      "epoch:44 step:34471 [D loss: 0.391835, acc.: 80.47%] [G loss: 3.102216]\n",
      "epoch:44 step:34472 [D loss: 0.367668, acc.: 82.81%] [G loss: 3.116474]\n",
      "epoch:44 step:34473 [D loss: 0.456958, acc.: 81.25%] [G loss: 2.686760]\n",
      "epoch:44 step:34474 [D loss: 0.312744, acc.: 85.16%] [G loss: 2.439543]\n",
      "epoch:44 step:34475 [D loss: 0.383246, acc.: 79.69%] [G loss: 2.350436]\n",
      "epoch:44 step:34476 [D loss: 0.317154, acc.: 85.94%] [G loss: 2.425091]\n",
      "epoch:44 step:34477 [D loss: 0.462076, acc.: 78.12%] [G loss: 3.096345]\n",
      "epoch:44 step:34478 [D loss: 0.367211, acc.: 78.91%] [G loss: 4.069088]\n",
      "epoch:44 step:34479 [D loss: 0.359019, acc.: 82.81%] [G loss: 2.512185]\n",
      "epoch:44 step:34480 [D loss: 0.235195, acc.: 89.84%] [G loss: 4.054669]\n",
      "epoch:44 step:34481 [D loss: 0.215237, acc.: 93.75%] [G loss: 4.444509]\n",
      "epoch:44 step:34482 [D loss: 0.289064, acc.: 85.16%] [G loss: 5.425510]\n",
      "epoch:44 step:34483 [D loss: 0.284291, acc.: 86.72%] [G loss: 2.905537]\n",
      "epoch:44 step:34484 [D loss: 0.244746, acc.: 89.84%] [G loss: 2.860782]\n",
      "epoch:44 step:34485 [D loss: 0.330265, acc.: 83.59%] [G loss: 2.975187]\n",
      "epoch:44 step:34486 [D loss: 0.301136, acc.: 84.38%] [G loss: 2.429914]\n",
      "epoch:44 step:34487 [D loss: 0.275339, acc.: 85.94%] [G loss: 3.112191]\n",
      "epoch:44 step:34488 [D loss: 0.393738, acc.: 76.56%] [G loss: 2.885413]\n",
      "epoch:44 step:34489 [D loss: 0.298011, acc.: 83.59%] [G loss: 3.442568]\n",
      "epoch:44 step:34490 [D loss: 0.237667, acc.: 91.41%] [G loss: 3.219405]\n",
      "epoch:44 step:34491 [D loss: 0.330836, acc.: 86.72%] [G loss: 2.596449]\n",
      "epoch:44 step:34492 [D loss: 0.282101, acc.: 86.72%] [G loss: 2.814813]\n",
      "epoch:44 step:34493 [D loss: 0.288904, acc.: 89.84%] [G loss: 2.817418]\n",
      "epoch:44 step:34494 [D loss: 0.309165, acc.: 87.50%] [G loss: 2.844777]\n",
      "epoch:44 step:34495 [D loss: 0.473973, acc.: 77.34%] [G loss: 2.208024]\n",
      "epoch:44 step:34496 [D loss: 0.240458, acc.: 89.84%] [G loss: 2.347334]\n",
      "epoch:44 step:34497 [D loss: 0.296625, acc.: 89.06%] [G loss: 4.568584]\n",
      "epoch:44 step:34498 [D loss: 0.236356, acc.: 87.50%] [G loss: 5.407958]\n",
      "epoch:44 step:34499 [D loss: 0.240353, acc.: 89.06%] [G loss: 6.364645]\n",
      "epoch:44 step:34500 [D loss: 0.264800, acc.: 87.50%] [G loss: 5.305138]\n",
      "epoch:44 step:34501 [D loss: 0.220905, acc.: 88.28%] [G loss: 8.140708]\n",
      "epoch:44 step:34502 [D loss: 0.307382, acc.: 83.59%] [G loss: 6.680331]\n",
      "epoch:44 step:34503 [D loss: 0.246591, acc.: 92.19%] [G loss: 5.516816]\n",
      "epoch:44 step:34504 [D loss: 0.265348, acc.: 88.28%] [G loss: 5.799576]\n",
      "epoch:44 step:34505 [D loss: 0.231043, acc.: 89.84%] [G loss: 3.831252]\n",
      "epoch:44 step:34506 [D loss: 0.253764, acc.: 89.06%] [G loss: 3.209847]\n",
      "epoch:44 step:34507 [D loss: 0.317899, acc.: 87.50%] [G loss: 3.058824]\n",
      "epoch:44 step:34508 [D loss: 0.321755, acc.: 86.72%] [G loss: 3.517760]\n",
      "epoch:44 step:34509 [D loss: 0.313299, acc.: 83.59%] [G loss: 3.236894]\n",
      "epoch:44 step:34510 [D loss: 0.305901, acc.: 84.38%] [G loss: 3.368178]\n",
      "epoch:44 step:34511 [D loss: 0.296661, acc.: 85.94%] [G loss: 3.359743]\n",
      "epoch:44 step:34512 [D loss: 0.292631, acc.: 89.84%] [G loss: 3.366547]\n",
      "epoch:44 step:34513 [D loss: 0.368593, acc.: 85.16%] [G loss: 3.727534]\n",
      "epoch:44 step:34514 [D loss: 0.334047, acc.: 85.94%] [G loss: 3.093134]\n",
      "epoch:44 step:34515 [D loss: 0.250679, acc.: 89.06%] [G loss: 3.128579]\n",
      "epoch:44 step:34516 [D loss: 0.386879, acc.: 82.81%] [G loss: 3.143396]\n",
      "epoch:44 step:34517 [D loss: 0.380786, acc.: 80.47%] [G loss: 3.298164]\n",
      "epoch:44 step:34518 [D loss: 0.344211, acc.: 81.25%] [G loss: 6.030679]\n",
      "epoch:44 step:34519 [D loss: 0.466690, acc.: 86.72%] [G loss: 5.685555]\n",
      "epoch:44 step:34520 [D loss: 0.415805, acc.: 84.38%] [G loss: 4.512469]\n",
      "epoch:44 step:34521 [D loss: 0.300704, acc.: 88.28%] [G loss: 4.361153]\n",
      "epoch:44 step:34522 [D loss: 0.335855, acc.: 87.50%] [G loss: 4.064184]\n",
      "epoch:44 step:34523 [D loss: 0.256966, acc.: 89.84%] [G loss: 3.594462]\n",
      "epoch:44 step:34524 [D loss: 0.384675, acc.: 79.69%] [G loss: 2.959276]\n",
      "epoch:44 step:34525 [D loss: 0.280492, acc.: 88.28%] [G loss: 3.181495]\n",
      "epoch:44 step:34526 [D loss: 0.243816, acc.: 89.06%] [G loss: 3.012639]\n",
      "epoch:44 step:34527 [D loss: 0.226212, acc.: 91.41%] [G loss: 2.998843]\n",
      "epoch:44 step:34528 [D loss: 0.330267, acc.: 85.16%] [G loss: 2.976266]\n",
      "epoch:44 step:34529 [D loss: 0.255947, acc.: 85.94%] [G loss: 3.319317]\n",
      "epoch:44 step:34530 [D loss: 0.239907, acc.: 88.28%] [G loss: 4.446436]\n",
      "epoch:44 step:34531 [D loss: 0.458737, acc.: 80.47%] [G loss: 4.148849]\n",
      "epoch:44 step:34532 [D loss: 0.488999, acc.: 78.12%] [G loss: 4.297021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34533 [D loss: 0.397233, acc.: 83.59%] [G loss: 3.097965]\n",
      "epoch:44 step:34534 [D loss: 0.228929, acc.: 89.84%] [G loss: 4.827109]\n",
      "epoch:44 step:34535 [D loss: 0.403280, acc.: 83.59%] [G loss: 3.563396]\n",
      "epoch:44 step:34536 [D loss: 0.304129, acc.: 87.50%] [G loss: 2.836161]\n",
      "epoch:44 step:34537 [D loss: 0.307319, acc.: 85.16%] [G loss: 3.337040]\n",
      "epoch:44 step:34538 [D loss: 0.327717, acc.: 84.38%] [G loss: 2.627559]\n",
      "epoch:44 step:34539 [D loss: 0.323338, acc.: 87.50%] [G loss: 3.604951]\n",
      "epoch:44 step:34540 [D loss: 0.277960, acc.: 90.62%] [G loss: 2.522765]\n",
      "epoch:44 step:34541 [D loss: 0.384733, acc.: 81.25%] [G loss: 2.432450]\n",
      "epoch:44 step:34542 [D loss: 0.302349, acc.: 85.94%] [G loss: 2.544349]\n",
      "epoch:44 step:34543 [D loss: 0.377408, acc.: 82.81%] [G loss: 2.834450]\n",
      "epoch:44 step:34544 [D loss: 0.286291, acc.: 89.84%] [G loss: 2.905865]\n",
      "epoch:44 step:34545 [D loss: 0.279630, acc.: 88.28%] [G loss: 3.084281]\n",
      "epoch:44 step:34546 [D loss: 0.311426, acc.: 85.94%] [G loss: 5.037982]\n",
      "epoch:44 step:34547 [D loss: 0.458311, acc.: 78.12%] [G loss: 5.500870]\n",
      "epoch:44 step:34548 [D loss: 0.374114, acc.: 87.50%] [G loss: 3.596555]\n",
      "epoch:44 step:34549 [D loss: 0.304234, acc.: 90.62%] [G loss: 3.736893]\n",
      "epoch:44 step:34550 [D loss: 0.298674, acc.: 89.06%] [G loss: 4.901215]\n",
      "epoch:44 step:34551 [D loss: 0.326812, acc.: 82.03%] [G loss: 3.708422]\n",
      "epoch:44 step:34552 [D loss: 0.316330, acc.: 81.25%] [G loss: 3.056118]\n",
      "epoch:44 step:34553 [D loss: 0.258693, acc.: 91.41%] [G loss: 3.588105]\n",
      "epoch:44 step:34554 [D loss: 0.327026, acc.: 86.72%] [G loss: 2.543273]\n",
      "epoch:44 step:34555 [D loss: 0.413173, acc.: 82.81%] [G loss: 2.925859]\n",
      "epoch:44 step:34556 [D loss: 0.267441, acc.: 86.72%] [G loss: 4.058582]\n",
      "epoch:44 step:34557 [D loss: 0.375054, acc.: 82.03%] [G loss: 2.937520]\n",
      "epoch:44 step:34558 [D loss: 0.484947, acc.: 73.44%] [G loss: 2.486742]\n",
      "epoch:44 step:34559 [D loss: 0.366399, acc.: 85.94%] [G loss: 2.898291]\n",
      "epoch:44 step:34560 [D loss: 0.328929, acc.: 84.38%] [G loss: 3.302705]\n",
      "epoch:44 step:34561 [D loss: 0.436065, acc.: 81.25%] [G loss: 4.222655]\n",
      "epoch:44 step:34562 [D loss: 0.261528, acc.: 89.84%] [G loss: 4.072876]\n",
      "epoch:44 step:34563 [D loss: 0.268075, acc.: 86.72%] [G loss: 3.900224]\n",
      "epoch:44 step:34564 [D loss: 0.293189, acc.: 86.72%] [G loss: 2.138065]\n",
      "epoch:44 step:34565 [D loss: 0.218564, acc.: 92.19%] [G loss: 3.092155]\n",
      "epoch:44 step:34566 [D loss: 0.354474, acc.: 83.59%] [G loss: 3.118819]\n",
      "epoch:44 step:34567 [D loss: 0.227592, acc.: 90.62%] [G loss: 3.585227]\n",
      "epoch:44 step:34568 [D loss: 0.305698, acc.: 88.28%] [G loss: 3.146253]\n",
      "epoch:44 step:34569 [D loss: 0.345041, acc.: 86.72%] [G loss: 3.579697]\n",
      "epoch:44 step:34570 [D loss: 0.284517, acc.: 87.50%] [G loss: 3.792165]\n",
      "epoch:44 step:34571 [D loss: 0.279739, acc.: 87.50%] [G loss: 4.346739]\n",
      "epoch:44 step:34572 [D loss: 0.398765, acc.: 80.47%] [G loss: 2.706216]\n",
      "epoch:44 step:34573 [D loss: 0.354014, acc.: 83.59%] [G loss: 3.919685]\n",
      "epoch:44 step:34574 [D loss: 0.314195, acc.: 85.94%] [G loss: 3.625787]\n",
      "epoch:44 step:34575 [D loss: 0.306497, acc.: 83.59%] [G loss: 6.764622]\n",
      "epoch:44 step:34576 [D loss: 0.262626, acc.: 85.94%] [G loss: 5.098564]\n",
      "epoch:44 step:34577 [D loss: 0.268495, acc.: 86.72%] [G loss: 3.355793]\n",
      "epoch:44 step:34578 [D loss: 0.271226, acc.: 87.50%] [G loss: 3.169029]\n",
      "epoch:44 step:34579 [D loss: 0.289341, acc.: 87.50%] [G loss: 2.991753]\n",
      "epoch:44 step:34580 [D loss: 0.360807, acc.: 83.59%] [G loss: 2.728475]\n",
      "epoch:44 step:34581 [D loss: 0.295602, acc.: 88.28%] [G loss: 3.125852]\n",
      "epoch:44 step:34582 [D loss: 0.290158, acc.: 85.16%] [G loss: 3.439088]\n",
      "epoch:44 step:34583 [D loss: 0.270750, acc.: 87.50%] [G loss: 3.139191]\n",
      "epoch:44 step:34584 [D loss: 0.291546, acc.: 88.28%] [G loss: 3.359483]\n",
      "epoch:44 step:34585 [D loss: 0.316309, acc.: 87.50%] [G loss: 3.752128]\n",
      "epoch:44 step:34586 [D loss: 0.358685, acc.: 85.16%] [G loss: 4.154266]\n",
      "epoch:44 step:34587 [D loss: 0.289447, acc.: 85.94%] [G loss: 4.673882]\n",
      "epoch:44 step:34588 [D loss: 0.416055, acc.: 82.81%] [G loss: 2.909543]\n",
      "epoch:44 step:34589 [D loss: 0.297983, acc.: 85.94%] [G loss: 2.545078]\n",
      "epoch:44 step:34590 [D loss: 0.388655, acc.: 79.69%] [G loss: 3.238790]\n",
      "epoch:44 step:34591 [D loss: 0.219829, acc.: 91.41%] [G loss: 4.647521]\n",
      "epoch:44 step:34592 [D loss: 0.308953, acc.: 85.16%] [G loss: 4.064124]\n",
      "epoch:44 step:34593 [D loss: 0.240389, acc.: 87.50%] [G loss: 3.408852]\n",
      "epoch:44 step:34594 [D loss: 0.323828, acc.: 85.94%] [G loss: 3.681822]\n",
      "epoch:44 step:34595 [D loss: 0.251512, acc.: 89.84%] [G loss: 4.018628]\n",
      "epoch:44 step:34596 [D loss: 0.260015, acc.: 86.72%] [G loss: 4.125751]\n",
      "epoch:44 step:34597 [D loss: 0.360242, acc.: 83.59%] [G loss: 4.445258]\n",
      "epoch:44 step:34598 [D loss: 0.389426, acc.: 83.59%] [G loss: 4.152919]\n",
      "epoch:44 step:34599 [D loss: 0.403470, acc.: 76.56%] [G loss: 2.752107]\n",
      "epoch:44 step:34600 [D loss: 0.309146, acc.: 82.81%] [G loss: 2.870682]\n",
      "epoch:44 step:34601 [D loss: 0.332731, acc.: 83.59%] [G loss: 2.709903]\n",
      "epoch:44 step:34602 [D loss: 0.347966, acc.: 85.16%] [G loss: 4.665049]\n",
      "epoch:44 step:34603 [D loss: 0.355983, acc.: 83.59%] [G loss: 4.325623]\n",
      "epoch:44 step:34604 [D loss: 0.379802, acc.: 85.16%] [G loss: 3.629957]\n",
      "epoch:44 step:34605 [D loss: 0.255296, acc.: 92.19%] [G loss: 3.984774]\n",
      "epoch:44 step:34606 [D loss: 0.304054, acc.: 85.16%] [G loss: 3.175986]\n",
      "epoch:44 step:34607 [D loss: 0.291497, acc.: 87.50%] [G loss: 3.859700]\n",
      "epoch:44 step:34608 [D loss: 0.323143, acc.: 88.28%] [G loss: 3.588160]\n",
      "epoch:44 step:34609 [D loss: 0.307842, acc.: 85.16%] [G loss: 3.645225]\n",
      "epoch:44 step:34610 [D loss: 0.402775, acc.: 83.59%] [G loss: 2.603184]\n",
      "epoch:44 step:34611 [D loss: 0.352612, acc.: 85.94%] [G loss: 3.583434]\n",
      "epoch:44 step:34612 [D loss: 0.203022, acc.: 90.62%] [G loss: 3.863168]\n",
      "epoch:44 step:34613 [D loss: 0.336934, acc.: 86.72%] [G loss: 3.022401]\n",
      "epoch:44 step:34614 [D loss: 0.439330, acc.: 78.12%] [G loss: 2.862574]\n",
      "epoch:44 step:34615 [D loss: 0.279016, acc.: 86.72%] [G loss: 2.896348]\n",
      "epoch:44 step:34616 [D loss: 0.320865, acc.: 85.94%] [G loss: 2.719457]\n",
      "epoch:44 step:34617 [D loss: 0.287215, acc.: 89.06%] [G loss: 2.553195]\n",
      "epoch:44 step:34618 [D loss: 0.315085, acc.: 87.50%] [G loss: 3.081512]\n",
      "epoch:44 step:34619 [D loss: 0.393001, acc.: 82.03%] [G loss: 2.565278]\n",
      "epoch:44 step:34620 [D loss: 0.339483, acc.: 81.25%] [G loss: 2.569363]\n",
      "epoch:44 step:34621 [D loss: 0.405373, acc.: 83.59%] [G loss: 2.590682]\n",
      "epoch:44 step:34622 [D loss: 0.368734, acc.: 83.59%] [G loss: 2.943417]\n",
      "epoch:44 step:34623 [D loss: 0.520235, acc.: 78.91%] [G loss: 4.404522]\n",
      "epoch:44 step:34624 [D loss: 0.309443, acc.: 89.84%] [G loss: 3.158753]\n",
      "epoch:44 step:34625 [D loss: 0.451439, acc.: 77.34%] [G loss: 2.909742]\n",
      "epoch:44 step:34626 [D loss: 0.478403, acc.: 72.66%] [G loss: 5.983086]\n",
      "epoch:44 step:34627 [D loss: 0.344425, acc.: 85.16%] [G loss: 3.568330]\n",
      "epoch:44 step:34628 [D loss: 0.262232, acc.: 87.50%] [G loss: 4.922059]\n",
      "epoch:44 step:34629 [D loss: 0.287910, acc.: 86.72%] [G loss: 3.878199]\n",
      "epoch:44 step:34630 [D loss: 0.267242, acc.: 87.50%] [G loss: 3.329736]\n",
      "epoch:44 step:34631 [D loss: 0.320629, acc.: 87.50%] [G loss: 3.055789]\n",
      "epoch:44 step:34632 [D loss: 0.363347, acc.: 85.16%] [G loss: 2.823797]\n",
      "epoch:44 step:34633 [D loss: 0.320439, acc.: 83.59%] [G loss: 4.086454]\n",
      "epoch:44 step:34634 [D loss: 0.352619, acc.: 85.16%] [G loss: 3.050568]\n",
      "epoch:44 step:34635 [D loss: 0.352132, acc.: 88.28%] [G loss: 3.422050]\n",
      "epoch:44 step:34636 [D loss: 0.310957, acc.: 85.94%] [G loss: 3.184438]\n",
      "epoch:44 step:34637 [D loss: 0.254606, acc.: 89.06%] [G loss: 3.677053]\n",
      "epoch:44 step:34638 [D loss: 0.296014, acc.: 89.06%] [G loss: 2.667953]\n",
      "epoch:44 step:34639 [D loss: 0.488125, acc.: 80.47%] [G loss: 3.032695]\n",
      "epoch:44 step:34640 [D loss: 0.346670, acc.: 85.94%] [G loss: 2.832881]\n",
      "epoch:44 step:34641 [D loss: 0.359239, acc.: 83.59%] [G loss: 3.446810]\n",
      "epoch:44 step:34642 [D loss: 0.306180, acc.: 89.06%] [G loss: 2.882321]\n",
      "epoch:44 step:34643 [D loss: 0.320464, acc.: 89.06%] [G loss: 2.606094]\n",
      "epoch:44 step:34644 [D loss: 0.251682, acc.: 88.28%] [G loss: 4.088387]\n",
      "epoch:44 step:34645 [D loss: 0.318746, acc.: 86.72%] [G loss: 3.128708]\n",
      "epoch:44 step:34646 [D loss: 0.297701, acc.: 88.28%] [G loss: 3.313191]\n",
      "epoch:44 step:34647 [D loss: 0.350440, acc.: 82.81%] [G loss: 3.354571]\n",
      "epoch:44 step:34648 [D loss: 0.320126, acc.: 83.59%] [G loss: 4.806721]\n",
      "epoch:44 step:34649 [D loss: 0.296653, acc.: 89.84%] [G loss: 3.448141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34650 [D loss: 0.395719, acc.: 80.47%] [G loss: 3.964051]\n",
      "epoch:44 step:34651 [D loss: 0.368457, acc.: 82.03%] [G loss: 3.795935]\n",
      "epoch:44 step:34652 [D loss: 0.337982, acc.: 86.72%] [G loss: 3.121815]\n",
      "epoch:44 step:34653 [D loss: 0.355215, acc.: 83.59%] [G loss: 2.641076]\n",
      "epoch:44 step:34654 [D loss: 0.278423, acc.: 89.06%] [G loss: 3.489931]\n",
      "epoch:44 step:34655 [D loss: 0.223207, acc.: 94.53%] [G loss: 3.566131]\n",
      "epoch:44 step:34656 [D loss: 0.296338, acc.: 85.16%] [G loss: 3.258987]\n",
      "epoch:44 step:34657 [D loss: 0.363246, acc.: 82.03%] [G loss: 3.207352]\n",
      "epoch:44 step:34658 [D loss: 0.392155, acc.: 80.47%] [G loss: 3.169413]\n",
      "epoch:44 step:34659 [D loss: 0.335398, acc.: 84.38%] [G loss: 4.229654]\n",
      "epoch:44 step:34660 [D loss: 0.349476, acc.: 83.59%] [G loss: 3.607392]\n",
      "epoch:44 step:34661 [D loss: 0.467029, acc.: 74.22%] [G loss: 3.377889]\n",
      "epoch:44 step:34662 [D loss: 0.326201, acc.: 85.94%] [G loss: 3.003916]\n",
      "epoch:44 step:34663 [D loss: 0.418103, acc.: 83.59%] [G loss: 3.225881]\n",
      "epoch:44 step:34664 [D loss: 0.440613, acc.: 81.25%] [G loss: 2.966201]\n",
      "epoch:44 step:34665 [D loss: 0.359274, acc.: 81.25%] [G loss: 2.792262]\n",
      "epoch:44 step:34666 [D loss: 0.354603, acc.: 81.25%] [G loss: 3.348702]\n",
      "epoch:44 step:34667 [D loss: 0.262930, acc.: 88.28%] [G loss: 2.761652]\n",
      "epoch:44 step:34668 [D loss: 0.339497, acc.: 84.38%] [G loss: 3.915364]\n",
      "epoch:44 step:34669 [D loss: 0.330893, acc.: 85.16%] [G loss: 2.671291]\n",
      "epoch:44 step:34670 [D loss: 0.287307, acc.: 89.06%] [G loss: 3.473523]\n",
      "epoch:44 step:34671 [D loss: 0.287336, acc.: 86.72%] [G loss: 3.434315]\n",
      "epoch:44 step:34672 [D loss: 0.351519, acc.: 86.72%] [G loss: 2.595768]\n",
      "epoch:44 step:34673 [D loss: 0.467322, acc.: 77.34%] [G loss: 2.931297]\n",
      "epoch:44 step:34674 [D loss: 0.445412, acc.: 78.12%] [G loss: 2.616273]\n",
      "epoch:44 step:34675 [D loss: 0.301608, acc.: 87.50%] [G loss: 3.593292]\n",
      "epoch:44 step:34676 [D loss: 0.510808, acc.: 75.78%] [G loss: 2.923488]\n",
      "epoch:44 step:34677 [D loss: 0.266807, acc.: 89.84%] [G loss: 3.017477]\n",
      "epoch:44 step:34678 [D loss: 0.305416, acc.: 82.81%] [G loss: 3.496654]\n",
      "epoch:44 step:34679 [D loss: 0.417406, acc.: 80.47%] [G loss: 3.966949]\n",
      "epoch:44 step:34680 [D loss: 0.388800, acc.: 81.25%] [G loss: 2.841090]\n",
      "epoch:44 step:34681 [D loss: 0.404166, acc.: 80.47%] [G loss: 3.293396]\n",
      "epoch:44 step:34682 [D loss: 0.478156, acc.: 80.47%] [G loss: 3.152539]\n",
      "epoch:44 step:34683 [D loss: 0.263321, acc.: 89.84%] [G loss: 3.037544]\n",
      "epoch:44 step:34684 [D loss: 0.447480, acc.: 79.69%] [G loss: 3.520618]\n",
      "epoch:44 step:34685 [D loss: 0.350621, acc.: 84.38%] [G loss: 3.553202]\n",
      "epoch:44 step:34686 [D loss: 0.447583, acc.: 78.12%] [G loss: 5.963532]\n",
      "epoch:44 step:34687 [D loss: 0.883492, acc.: 64.06%] [G loss: 3.396001]\n",
      "epoch:44 step:34688 [D loss: 0.290621, acc.: 85.16%] [G loss: 2.560280]\n",
      "epoch:44 step:34689 [D loss: 0.292654, acc.: 84.38%] [G loss: 2.791363]\n",
      "epoch:44 step:34690 [D loss: 0.365217, acc.: 82.81%] [G loss: 2.964278]\n",
      "epoch:44 step:34691 [D loss: 0.253400, acc.: 91.41%] [G loss: 2.265389]\n",
      "epoch:44 step:34692 [D loss: 0.390309, acc.: 81.25%] [G loss: 2.916740]\n",
      "epoch:44 step:34693 [D loss: 0.437499, acc.: 79.69%] [G loss: 3.063337]\n",
      "epoch:44 step:34694 [D loss: 0.289571, acc.: 86.72%] [G loss: 3.094337]\n",
      "epoch:44 step:34695 [D loss: 0.348101, acc.: 80.47%] [G loss: 2.303114]\n",
      "epoch:44 step:34696 [D loss: 0.255948, acc.: 89.06%] [G loss: 3.576164]\n",
      "epoch:44 step:34697 [D loss: 0.299025, acc.: 88.28%] [G loss: 2.329189]\n",
      "epoch:44 step:34698 [D loss: 0.336598, acc.: 83.59%] [G loss: 2.363455]\n",
      "epoch:44 step:34699 [D loss: 0.298325, acc.: 86.72%] [G loss: 2.821061]\n",
      "epoch:44 step:34700 [D loss: 0.264437, acc.: 88.28%] [G loss: 2.832737]\n",
      "epoch:44 step:34701 [D loss: 0.366050, acc.: 85.94%] [G loss: 2.589129]\n",
      "epoch:44 step:34702 [D loss: 0.379098, acc.: 78.91%] [G loss: 3.906612]\n",
      "epoch:44 step:34703 [D loss: 0.254757, acc.: 85.94%] [G loss: 3.140085]\n",
      "epoch:44 step:34704 [D loss: 0.380917, acc.: 84.38%] [G loss: 3.148067]\n",
      "epoch:44 step:34705 [D loss: 0.356458, acc.: 85.16%] [G loss: 3.843683]\n",
      "epoch:44 step:34706 [D loss: 0.370595, acc.: 82.03%] [G loss: 2.688200]\n",
      "epoch:44 step:34707 [D loss: 0.321661, acc.: 88.28%] [G loss: 3.513512]\n",
      "epoch:44 step:34708 [D loss: 0.326369, acc.: 83.59%] [G loss: 3.474018]\n",
      "epoch:44 step:34709 [D loss: 0.213243, acc.: 89.84%] [G loss: 2.891178]\n",
      "epoch:44 step:34710 [D loss: 0.439900, acc.: 81.25%] [G loss: 2.193048]\n",
      "epoch:44 step:34711 [D loss: 0.240062, acc.: 89.84%] [G loss: 2.314148]\n",
      "epoch:44 step:34712 [D loss: 0.353870, acc.: 86.72%] [G loss: 3.482165]\n",
      "epoch:44 step:34713 [D loss: 0.337685, acc.: 87.50%] [G loss: 3.288846]\n",
      "epoch:44 step:34714 [D loss: 0.336803, acc.: 84.38%] [G loss: 3.127670]\n",
      "epoch:44 step:34715 [D loss: 0.335078, acc.: 84.38%] [G loss: 3.295679]\n",
      "epoch:44 step:34716 [D loss: 0.394893, acc.: 82.03%] [G loss: 3.967238]\n",
      "epoch:44 step:34717 [D loss: 0.378345, acc.: 81.25%] [G loss: 2.877497]\n",
      "epoch:44 step:34718 [D loss: 0.209560, acc.: 92.97%] [G loss: 2.925853]\n",
      "epoch:44 step:34719 [D loss: 0.252600, acc.: 90.62%] [G loss: 2.801704]\n",
      "epoch:44 step:34720 [D loss: 0.241879, acc.: 88.28%] [G loss: 3.350038]\n",
      "epoch:44 step:34721 [D loss: 0.327366, acc.: 82.81%] [G loss: 2.882559]\n",
      "epoch:44 step:34722 [D loss: 0.207532, acc.: 93.75%] [G loss: 3.047188]\n",
      "epoch:44 step:34723 [D loss: 0.257571, acc.: 88.28%] [G loss: 2.736125]\n",
      "epoch:44 step:34724 [D loss: 0.337160, acc.: 82.03%] [G loss: 2.910091]\n",
      "epoch:44 step:34725 [D loss: 0.304289, acc.: 85.16%] [G loss: 2.912561]\n",
      "epoch:44 step:34726 [D loss: 0.311030, acc.: 85.94%] [G loss: 3.165648]\n",
      "epoch:44 step:34727 [D loss: 0.325221, acc.: 83.59%] [G loss: 3.003142]\n",
      "epoch:44 step:34728 [D loss: 0.307968, acc.: 85.16%] [G loss: 2.448421]\n",
      "epoch:44 step:34729 [D loss: 0.229674, acc.: 93.75%] [G loss: 3.397672]\n",
      "epoch:44 step:34730 [D loss: 0.500733, acc.: 73.44%] [G loss: 3.512811]\n",
      "epoch:44 step:34731 [D loss: 0.360969, acc.: 87.50%] [G loss: 3.458364]\n",
      "epoch:44 step:34732 [D loss: 0.476348, acc.: 75.78%] [G loss: 5.984000]\n",
      "epoch:44 step:34733 [D loss: 0.989559, acc.: 71.09%] [G loss: 11.132170]\n",
      "epoch:44 step:34734 [D loss: 1.923992, acc.: 64.84%] [G loss: 6.980821]\n",
      "epoch:44 step:34735 [D loss: 1.298473, acc.: 67.19%] [G loss: 7.863840]\n",
      "epoch:44 step:34736 [D loss: 1.230365, acc.: 67.19%] [G loss: 5.370070]\n",
      "epoch:44 step:34737 [D loss: 0.357983, acc.: 85.16%] [G loss: 3.860332]\n",
      "epoch:44 step:34738 [D loss: 0.740725, acc.: 71.09%] [G loss: 2.855717]\n",
      "epoch:44 step:34739 [D loss: 0.406506, acc.: 82.81%] [G loss: 2.544539]\n",
      "epoch:44 step:34740 [D loss: 0.415063, acc.: 84.38%] [G loss: 3.995288]\n",
      "epoch:44 step:34741 [D loss: 0.246769, acc.: 89.06%] [G loss: 3.514479]\n",
      "epoch:44 step:34742 [D loss: 0.575556, acc.: 69.53%] [G loss: 2.627672]\n",
      "epoch:44 step:34743 [D loss: 0.318817, acc.: 87.50%] [G loss: 3.485102]\n",
      "epoch:44 step:34744 [D loss: 0.313520, acc.: 86.72%] [G loss: 2.589786]\n",
      "epoch:44 step:34745 [D loss: 0.307604, acc.: 88.28%] [G loss: 3.581558]\n",
      "epoch:44 step:34746 [D loss: 0.225614, acc.: 91.41%] [G loss: 3.618783]\n",
      "epoch:44 step:34747 [D loss: 0.402445, acc.: 84.38%] [G loss: 2.578914]\n",
      "epoch:44 step:34748 [D loss: 0.216644, acc.: 90.62%] [G loss: 2.881503]\n",
      "epoch:44 step:34749 [D loss: 0.289361, acc.: 87.50%] [G loss: 2.617959]\n",
      "epoch:44 step:34750 [D loss: 0.461864, acc.: 81.25%] [G loss: 2.555092]\n",
      "epoch:44 step:34751 [D loss: 0.335267, acc.: 85.16%] [G loss: 3.084908]\n",
      "epoch:44 step:34752 [D loss: 0.351779, acc.: 83.59%] [G loss: 2.800043]\n",
      "epoch:44 step:34753 [D loss: 0.256952, acc.: 91.41%] [G loss: 3.192996]\n",
      "epoch:44 step:34754 [D loss: 0.333664, acc.: 85.94%] [G loss: 2.685259]\n",
      "epoch:44 step:34755 [D loss: 0.374863, acc.: 82.03%] [G loss: 2.471113]\n",
      "epoch:44 step:34756 [D loss: 0.236855, acc.: 90.62%] [G loss: 3.440344]\n",
      "epoch:44 step:34757 [D loss: 0.292886, acc.: 85.94%] [G loss: 2.933914]\n",
      "epoch:44 step:34758 [D loss: 0.407930, acc.: 80.47%] [G loss: 2.559939]\n",
      "epoch:44 step:34759 [D loss: 0.267748, acc.: 85.94%] [G loss: 3.193077]\n",
      "epoch:44 step:34760 [D loss: 0.264854, acc.: 89.06%] [G loss: 2.927315]\n",
      "epoch:44 step:34761 [D loss: 0.369832, acc.: 82.81%] [G loss: 2.394619]\n",
      "epoch:44 step:34762 [D loss: 0.338883, acc.: 86.72%] [G loss: 2.244714]\n",
      "epoch:44 step:34763 [D loss: 0.364671, acc.: 82.81%] [G loss: 2.129159]\n",
      "epoch:44 step:34764 [D loss: 0.346653, acc.: 82.81%] [G loss: 2.668939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34765 [D loss: 0.275277, acc.: 89.84%] [G loss: 1.988768]\n",
      "epoch:44 step:34766 [D loss: 0.285248, acc.: 89.06%] [G loss: 2.509955]\n",
      "epoch:44 step:34767 [D loss: 0.384106, acc.: 84.38%] [G loss: 2.360712]\n",
      "epoch:44 step:34768 [D loss: 0.243950, acc.: 92.97%] [G loss: 3.334869]\n",
      "epoch:44 step:34769 [D loss: 0.293058, acc.: 89.06%] [G loss: 3.315776]\n",
      "epoch:44 step:34770 [D loss: 0.472662, acc.: 77.34%] [G loss: 2.860105]\n",
      "epoch:44 step:34771 [D loss: 0.231825, acc.: 90.62%] [G loss: 3.979327]\n",
      "epoch:44 step:34772 [D loss: 0.322041, acc.: 85.16%] [G loss: 2.870975]\n",
      "epoch:44 step:34773 [D loss: 0.313416, acc.: 86.72%] [G loss: 3.007153]\n",
      "epoch:44 step:34774 [D loss: 0.358553, acc.: 84.38%] [G loss: 3.682150]\n",
      "epoch:44 step:34775 [D loss: 0.466900, acc.: 80.47%] [G loss: 2.640817]\n",
      "epoch:44 step:34776 [D loss: 0.404439, acc.: 82.03%] [G loss: 2.437556]\n",
      "epoch:44 step:34777 [D loss: 0.281512, acc.: 89.06%] [G loss: 3.293161]\n",
      "epoch:44 step:34778 [D loss: 0.362130, acc.: 82.03%] [G loss: 2.364234]\n",
      "epoch:44 step:34779 [D loss: 0.339413, acc.: 85.94%] [G loss: 2.653869]\n",
      "epoch:44 step:34780 [D loss: 0.303218, acc.: 80.47%] [G loss: 3.182303]\n",
      "epoch:44 step:34781 [D loss: 0.366972, acc.: 81.25%] [G loss: 2.392116]\n",
      "epoch:44 step:34782 [D loss: 0.347399, acc.: 86.72%] [G loss: 3.382318]\n",
      "epoch:44 step:34783 [D loss: 0.368541, acc.: 82.81%] [G loss: 2.705362]\n",
      "epoch:44 step:34784 [D loss: 0.298592, acc.: 87.50%] [G loss: 2.232684]\n",
      "epoch:44 step:34785 [D loss: 0.301908, acc.: 87.50%] [G loss: 2.947366]\n",
      "epoch:44 step:34786 [D loss: 0.264085, acc.: 89.84%] [G loss: 2.483828]\n",
      "epoch:44 step:34787 [D loss: 0.360772, acc.: 82.03%] [G loss: 2.231310]\n",
      "epoch:44 step:34788 [D loss: 0.242652, acc.: 91.41%] [G loss: 2.482753]\n",
      "epoch:44 step:34789 [D loss: 0.447709, acc.: 76.56%] [G loss: 2.733447]\n",
      "epoch:44 step:34790 [D loss: 0.285032, acc.: 87.50%] [G loss: 2.571261]\n",
      "epoch:44 step:34791 [D loss: 0.285946, acc.: 88.28%] [G loss: 2.509213]\n",
      "epoch:44 step:34792 [D loss: 0.479702, acc.: 75.78%] [G loss: 2.334518]\n",
      "epoch:44 step:34793 [D loss: 0.460968, acc.: 78.91%] [G loss: 2.072367]\n",
      "epoch:44 step:34794 [D loss: 0.353207, acc.: 81.25%] [G loss: 2.372513]\n",
      "epoch:44 step:34795 [D loss: 0.299068, acc.: 85.94%] [G loss: 2.412957]\n",
      "epoch:44 step:34796 [D loss: 0.347248, acc.: 82.81%] [G loss: 3.182377]\n",
      "epoch:44 step:34797 [D loss: 0.328871, acc.: 85.94%] [G loss: 2.409593]\n",
      "epoch:44 step:34798 [D loss: 0.324425, acc.: 84.38%] [G loss: 2.605567]\n",
      "epoch:44 step:34799 [D loss: 0.370113, acc.: 81.25%] [G loss: 2.240460]\n",
      "epoch:44 step:34800 [D loss: 0.372947, acc.: 78.91%] [G loss: 2.488991]\n",
      "epoch:44 step:34801 [D loss: 0.394970, acc.: 82.81%] [G loss: 3.861583]\n",
      "epoch:44 step:34802 [D loss: 0.450962, acc.: 78.12%] [G loss: 3.366674]\n",
      "epoch:44 step:34803 [D loss: 0.259656, acc.: 90.62%] [G loss: 4.089361]\n",
      "epoch:44 step:34804 [D loss: 0.234866, acc.: 89.84%] [G loss: 4.738435]\n",
      "epoch:44 step:34805 [D loss: 0.323965, acc.: 85.16%] [G loss: 4.070201]\n",
      "epoch:44 step:34806 [D loss: 0.428634, acc.: 80.47%] [G loss: 6.165761]\n",
      "epoch:44 step:34807 [D loss: 0.303789, acc.: 86.72%] [G loss: 4.299646]\n",
      "epoch:44 step:34808 [D loss: 0.527023, acc.: 73.44%] [G loss: 2.911275]\n",
      "epoch:44 step:34809 [D loss: 0.354917, acc.: 84.38%] [G loss: 3.360370]\n",
      "epoch:44 step:34810 [D loss: 0.331547, acc.: 84.38%] [G loss: 3.206465]\n",
      "epoch:44 step:34811 [D loss: 0.263717, acc.: 87.50%] [G loss: 2.947891]\n",
      "epoch:44 step:34812 [D loss: 0.416328, acc.: 80.47%] [G loss: 2.644347]\n",
      "epoch:44 step:34813 [D loss: 0.340859, acc.: 83.59%] [G loss: 2.552464]\n",
      "epoch:44 step:34814 [D loss: 0.341893, acc.: 86.72%] [G loss: 2.734727]\n",
      "epoch:44 step:34815 [D loss: 0.296434, acc.: 85.94%] [G loss: 2.784573]\n",
      "epoch:44 step:34816 [D loss: 0.342671, acc.: 86.72%] [G loss: 2.438661]\n",
      "epoch:44 step:34817 [D loss: 0.287675, acc.: 87.50%] [G loss: 2.543987]\n",
      "epoch:44 step:34818 [D loss: 0.274963, acc.: 85.94%] [G loss: 2.973769]\n",
      "epoch:44 step:34819 [D loss: 0.411451, acc.: 80.47%] [G loss: 2.515320]\n",
      "epoch:44 step:34820 [D loss: 0.335908, acc.: 82.81%] [G loss: 2.875319]\n",
      "epoch:44 step:34821 [D loss: 0.321690, acc.: 84.38%] [G loss: 2.563114]\n",
      "epoch:44 step:34822 [D loss: 0.345069, acc.: 85.94%] [G loss: 2.389859]\n",
      "epoch:44 step:34823 [D loss: 0.332875, acc.: 82.03%] [G loss: 2.830331]\n",
      "epoch:44 step:34824 [D loss: 0.310892, acc.: 85.16%] [G loss: 3.080119]\n",
      "epoch:44 step:34825 [D loss: 0.341054, acc.: 83.59%] [G loss: 2.430548]\n",
      "epoch:44 step:34826 [D loss: 0.314186, acc.: 89.06%] [G loss: 2.737472]\n",
      "epoch:44 step:34827 [D loss: 0.307592, acc.: 87.50%] [G loss: 2.456363]\n",
      "epoch:44 step:34828 [D loss: 0.245016, acc.: 90.62%] [G loss: 3.010432]\n",
      "epoch:44 step:34829 [D loss: 0.356403, acc.: 83.59%] [G loss: 2.828832]\n",
      "epoch:44 step:34830 [D loss: 0.302729, acc.: 85.94%] [G loss: 2.064334]\n",
      "epoch:44 step:34831 [D loss: 0.363099, acc.: 83.59%] [G loss: 2.840834]\n",
      "epoch:44 step:34832 [D loss: 0.412872, acc.: 80.47%] [G loss: 2.928392]\n",
      "epoch:44 step:34833 [D loss: 0.246109, acc.: 89.06%] [G loss: 3.042593]\n",
      "epoch:44 step:34834 [D loss: 0.327756, acc.: 85.94%] [G loss: 3.155781]\n",
      "epoch:44 step:34835 [D loss: 0.362545, acc.: 84.38%] [G loss: 4.411102]\n",
      "epoch:44 step:34836 [D loss: 0.372868, acc.: 82.03%] [G loss: 3.287162]\n",
      "epoch:44 step:34837 [D loss: 0.310853, acc.: 87.50%] [G loss: 3.292160]\n",
      "epoch:44 step:34838 [D loss: 0.284307, acc.: 87.50%] [G loss: 3.411873]\n",
      "epoch:44 step:34839 [D loss: 0.340525, acc.: 83.59%] [G loss: 2.835501]\n",
      "epoch:44 step:34840 [D loss: 0.274017, acc.: 89.84%] [G loss: 2.934441]\n",
      "epoch:44 step:34841 [D loss: 0.252267, acc.: 89.06%] [G loss: 2.648657]\n",
      "epoch:44 step:34842 [D loss: 0.277936, acc.: 87.50%] [G loss: 3.524197]\n",
      "epoch:44 step:34843 [D loss: 0.243658, acc.: 86.72%] [G loss: 2.592374]\n",
      "epoch:44 step:34844 [D loss: 0.240781, acc.: 88.28%] [G loss: 3.516989]\n",
      "epoch:44 step:34845 [D loss: 0.343445, acc.: 85.94%] [G loss: 3.218805]\n",
      "epoch:44 step:34846 [D loss: 0.304617, acc.: 85.94%] [G loss: 6.742942]\n",
      "epoch:44 step:34847 [D loss: 0.323178, acc.: 85.94%] [G loss: 4.345308]\n",
      "epoch:44 step:34848 [D loss: 0.299394, acc.: 89.06%] [G loss: 6.548826]\n",
      "epoch:44 step:34849 [D loss: 0.302463, acc.: 87.50%] [G loss: 4.145093]\n",
      "epoch:44 step:34850 [D loss: 0.309554, acc.: 86.72%] [G loss: 5.847970]\n",
      "epoch:44 step:34851 [D loss: 0.190850, acc.: 93.75%] [G loss: 4.614894]\n",
      "epoch:44 step:34852 [D loss: 0.192434, acc.: 93.75%] [G loss: 5.753836]\n",
      "epoch:44 step:34853 [D loss: 0.187973, acc.: 91.41%] [G loss: 5.590393]\n",
      "epoch:44 step:34854 [D loss: 0.268475, acc.: 88.28%] [G loss: 6.951721]\n",
      "epoch:44 step:34855 [D loss: 0.222236, acc.: 89.84%] [G loss: 6.909413]\n",
      "epoch:44 step:34856 [D loss: 0.245977, acc.: 88.28%] [G loss: 4.371945]\n",
      "epoch:44 step:34857 [D loss: 0.309617, acc.: 86.72%] [G loss: 5.019563]\n",
      "epoch:44 step:34858 [D loss: 0.287984, acc.: 86.72%] [G loss: 4.004791]\n",
      "epoch:44 step:34859 [D loss: 0.282372, acc.: 90.62%] [G loss: 4.041223]\n",
      "epoch:44 step:34860 [D loss: 0.275573, acc.: 86.72%] [G loss: 4.296047]\n",
      "epoch:44 step:34861 [D loss: 0.297366, acc.: 83.59%] [G loss: 3.445192]\n",
      "epoch:44 step:34862 [D loss: 0.245805, acc.: 89.06%] [G loss: 3.357906]\n",
      "epoch:44 step:34863 [D loss: 0.236132, acc.: 91.41%] [G loss: 2.957835]\n",
      "epoch:44 step:34864 [D loss: 0.426004, acc.: 81.25%] [G loss: 4.302255]\n",
      "epoch:44 step:34865 [D loss: 0.408348, acc.: 83.59%] [G loss: 2.410134]\n",
      "epoch:44 step:34866 [D loss: 0.488862, acc.: 71.88%] [G loss: 3.304872]\n",
      "epoch:44 step:34867 [D loss: 0.344095, acc.: 85.16%] [G loss: 4.271171]\n",
      "epoch:44 step:34868 [D loss: 0.310761, acc.: 85.16%] [G loss: 3.146183]\n",
      "epoch:44 step:34869 [D loss: 0.335617, acc.: 85.16%] [G loss: 2.841897]\n",
      "epoch:44 step:34870 [D loss: 0.318855, acc.: 84.38%] [G loss: 2.877116]\n",
      "epoch:44 step:34871 [D loss: 0.312161, acc.: 84.38%] [G loss: 4.916259]\n",
      "epoch:44 step:34872 [D loss: 0.285504, acc.: 89.84%] [G loss: 3.082943]\n",
      "epoch:44 step:34873 [D loss: 0.326876, acc.: 82.03%] [G loss: 3.447272]\n",
      "epoch:44 step:34874 [D loss: 0.391609, acc.: 82.81%] [G loss: 3.300288]\n",
      "epoch:44 step:34875 [D loss: 0.254012, acc.: 90.62%] [G loss: 3.783378]\n",
      "epoch:44 step:34876 [D loss: 0.347683, acc.: 83.59%] [G loss: 2.687918]\n",
      "epoch:44 step:34877 [D loss: 0.291836, acc.: 89.84%] [G loss: 3.201557]\n",
      "epoch:44 step:34878 [D loss: 0.383629, acc.: 82.81%] [G loss: 3.501811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34879 [D loss: 0.396649, acc.: 83.59%] [G loss: 3.230540]\n",
      "epoch:44 step:34880 [D loss: 0.295651, acc.: 89.06%] [G loss: 2.950347]\n",
      "epoch:44 step:34881 [D loss: 0.353455, acc.: 83.59%] [G loss: 2.449032]\n",
      "epoch:44 step:34882 [D loss: 0.263207, acc.: 89.84%] [G loss: 2.467665]\n",
      "epoch:44 step:34883 [D loss: 0.334687, acc.: 85.94%] [G loss: 2.975459]\n",
      "epoch:44 step:34884 [D loss: 0.357805, acc.: 79.69%] [G loss: 2.934119]\n",
      "epoch:44 step:34885 [D loss: 0.385401, acc.: 81.25%] [G loss: 3.570105]\n",
      "epoch:44 step:34886 [D loss: 0.350313, acc.: 82.03%] [G loss: 3.271694]\n",
      "epoch:44 step:34887 [D loss: 0.299333, acc.: 83.59%] [G loss: 4.148592]\n",
      "epoch:44 step:34888 [D loss: 0.355847, acc.: 83.59%] [G loss: 2.749214]\n",
      "epoch:44 step:34889 [D loss: 0.352942, acc.: 88.28%] [G loss: 3.469171]\n",
      "epoch:44 step:34890 [D loss: 0.408022, acc.: 81.25%] [G loss: 2.662503]\n",
      "epoch:44 step:34891 [D loss: 0.307034, acc.: 90.62%] [G loss: 3.482747]\n",
      "epoch:44 step:34892 [D loss: 0.334560, acc.: 87.50%] [G loss: 2.691118]\n",
      "epoch:44 step:34893 [D loss: 0.334051, acc.: 82.03%] [G loss: 3.359684]\n",
      "epoch:44 step:34894 [D loss: 0.326874, acc.: 84.38%] [G loss: 4.069503]\n",
      "epoch:44 step:34895 [D loss: 0.259177, acc.: 90.62%] [G loss: 5.506433]\n",
      "epoch:44 step:34896 [D loss: 0.275660, acc.: 89.06%] [G loss: 6.166511]\n",
      "epoch:44 step:34897 [D loss: 0.365710, acc.: 82.81%] [G loss: 4.284636]\n",
      "epoch:44 step:34898 [D loss: 0.297288, acc.: 86.72%] [G loss: 3.537170]\n",
      "epoch:44 step:34899 [D loss: 0.361677, acc.: 84.38%] [G loss: 6.963605]\n",
      "epoch:44 step:34900 [D loss: 0.623687, acc.: 71.88%] [G loss: 3.580783]\n",
      "epoch:44 step:34901 [D loss: 0.483451, acc.: 75.00%] [G loss: 3.928163]\n",
      "epoch:44 step:34902 [D loss: 0.280478, acc.: 85.16%] [G loss: 3.775578]\n",
      "epoch:44 step:34903 [D loss: 0.356935, acc.: 86.72%] [G loss: 4.017756]\n",
      "epoch:44 step:34904 [D loss: 0.358677, acc.: 82.03%] [G loss: 3.466335]\n",
      "epoch:44 step:34905 [D loss: 0.467027, acc.: 80.47%] [G loss: 3.350183]\n",
      "epoch:44 step:34906 [D loss: 0.438772, acc.: 79.69%] [G loss: 4.518062]\n",
      "epoch:44 step:34907 [D loss: 0.728292, acc.: 75.78%] [G loss: 2.640346]\n",
      "epoch:44 step:34908 [D loss: 0.346820, acc.: 82.81%] [G loss: 4.048550]\n",
      "epoch:44 step:34909 [D loss: 0.364296, acc.: 82.81%] [G loss: 2.925339]\n",
      "epoch:44 step:34910 [D loss: 0.306914, acc.: 87.50%] [G loss: 3.927860]\n",
      "epoch:44 step:34911 [D loss: 0.432121, acc.: 78.12%] [G loss: 3.310428]\n",
      "epoch:44 step:34912 [D loss: 0.215892, acc.: 90.62%] [G loss: 4.592842]\n",
      "epoch:44 step:34913 [D loss: 0.480906, acc.: 81.25%] [G loss: 4.590316]\n",
      "epoch:44 step:34914 [D loss: 0.540643, acc.: 74.22%] [G loss: 3.366970]\n",
      "epoch:44 step:34915 [D loss: 0.286109, acc.: 86.72%] [G loss: 4.196024]\n",
      "epoch:44 step:34916 [D loss: 0.452147, acc.: 84.38%] [G loss: 6.345220]\n",
      "epoch:44 step:34917 [D loss: 0.991437, acc.: 67.97%] [G loss: 4.674707]\n",
      "epoch:44 step:34918 [D loss: 0.597843, acc.: 75.00%] [G loss: 2.607632]\n",
      "epoch:44 step:34919 [D loss: 0.454707, acc.: 78.91%] [G loss: 3.045436]\n",
      "epoch:44 step:34920 [D loss: 0.398279, acc.: 81.25%] [G loss: 3.439656]\n",
      "epoch:44 step:34921 [D loss: 0.332574, acc.: 83.59%] [G loss: 3.062912]\n",
      "epoch:44 step:34922 [D loss: 0.371194, acc.: 82.81%] [G loss: 4.069546]\n",
      "epoch:44 step:34923 [D loss: 0.354158, acc.: 82.81%] [G loss: 2.752306]\n",
      "epoch:44 step:34924 [D loss: 0.419212, acc.: 81.25%] [G loss: 4.674953]\n",
      "epoch:44 step:34925 [D loss: 0.344909, acc.: 87.50%] [G loss: 5.770185]\n",
      "epoch:44 step:34926 [D loss: 0.432751, acc.: 82.81%] [G loss: 5.064383]\n",
      "epoch:44 step:34927 [D loss: 0.244382, acc.: 89.06%] [G loss: 3.896390]\n",
      "epoch:44 step:34928 [D loss: 0.310627, acc.: 85.94%] [G loss: 3.577072]\n",
      "epoch:44 step:34929 [D loss: 0.261069, acc.: 87.50%] [G loss: 3.551252]\n",
      "epoch:44 step:34930 [D loss: 0.359972, acc.: 84.38%] [G loss: 2.667734]\n",
      "epoch:44 step:34931 [D loss: 0.272277, acc.: 88.28%] [G loss: 3.242249]\n",
      "epoch:44 step:34932 [D loss: 0.353100, acc.: 83.59%] [G loss: 2.724131]\n",
      "epoch:44 step:34933 [D loss: 0.282999, acc.: 85.94%] [G loss: 3.445967]\n",
      "epoch:44 step:34934 [D loss: 0.261298, acc.: 89.84%] [G loss: 3.513601]\n",
      "epoch:44 step:34935 [D loss: 0.474336, acc.: 78.91%] [G loss: 2.669804]\n",
      "epoch:44 step:34936 [D loss: 0.378948, acc.: 82.81%] [G loss: 2.909440]\n",
      "epoch:44 step:34937 [D loss: 0.388049, acc.: 83.59%] [G loss: 2.715918]\n",
      "epoch:44 step:34938 [D loss: 0.266640, acc.: 86.72%] [G loss: 3.622534]\n",
      "epoch:44 step:34939 [D loss: 0.356204, acc.: 84.38%] [G loss: 2.643255]\n",
      "epoch:44 step:34940 [D loss: 0.311028, acc.: 86.72%] [G loss: 2.892998]\n",
      "epoch:44 step:34941 [D loss: 0.403042, acc.: 78.12%] [G loss: 2.780208]\n",
      "epoch:44 step:34942 [D loss: 0.303345, acc.: 86.72%] [G loss: 3.441570]\n",
      "epoch:44 step:34943 [D loss: 0.291132, acc.: 90.62%] [G loss: 3.351343]\n",
      "epoch:44 step:34944 [D loss: 0.317624, acc.: 82.81%] [G loss: 2.664466]\n",
      "epoch:44 step:34945 [D loss: 0.364324, acc.: 82.03%] [G loss: 2.583849]\n",
      "epoch:44 step:34946 [D loss: 0.340934, acc.: 84.38%] [G loss: 3.313975]\n",
      "epoch:44 step:34947 [D loss: 0.439542, acc.: 82.03%] [G loss: 3.737077]\n",
      "epoch:44 step:34948 [D loss: 0.354424, acc.: 84.38%] [G loss: 2.476624]\n",
      "epoch:44 step:34949 [D loss: 0.221664, acc.: 91.41%] [G loss: 3.429574]\n",
      "epoch:44 step:34950 [D loss: 0.302196, acc.: 86.72%] [G loss: 3.815432]\n",
      "epoch:44 step:34951 [D loss: 0.233445, acc.: 89.84%] [G loss: 3.482192]\n",
      "epoch:44 step:34952 [D loss: 0.238064, acc.: 90.62%] [G loss: 3.979277]\n",
      "epoch:44 step:34953 [D loss: 0.356176, acc.: 84.38%] [G loss: 3.872779]\n",
      "epoch:44 step:34954 [D loss: 0.336117, acc.: 89.06%] [G loss: 2.585958]\n",
      "epoch:44 step:34955 [D loss: 0.331181, acc.: 82.03%] [G loss: 2.862361]\n",
      "epoch:44 step:34956 [D loss: 0.310095, acc.: 86.72%] [G loss: 3.761322]\n",
      "epoch:44 step:34957 [D loss: 0.339483, acc.: 83.59%] [G loss: 2.668664]\n",
      "epoch:44 step:34958 [D loss: 0.289437, acc.: 87.50%] [G loss: 2.707411]\n",
      "epoch:44 step:34959 [D loss: 0.404710, acc.: 84.38%] [G loss: 2.559659]\n",
      "epoch:44 step:34960 [D loss: 0.359630, acc.: 82.03%] [G loss: 2.514870]\n",
      "epoch:44 step:34961 [D loss: 0.344742, acc.: 85.16%] [G loss: 3.325286]\n",
      "epoch:44 step:34962 [D loss: 0.275490, acc.: 88.28%] [G loss: 2.909306]\n",
      "epoch:44 step:34963 [D loss: 0.304371, acc.: 85.94%] [G loss: 3.148137]\n",
      "epoch:44 step:34964 [D loss: 0.272964, acc.: 92.19%] [G loss: 2.871564]\n",
      "epoch:44 step:34965 [D loss: 0.248819, acc.: 89.06%] [G loss: 3.621192]\n",
      "epoch:44 step:34966 [D loss: 0.350657, acc.: 82.03%] [G loss: 3.584861]\n",
      "epoch:44 step:34967 [D loss: 0.287334, acc.: 88.28%] [G loss: 4.661638]\n",
      "epoch:44 step:34968 [D loss: 0.289353, acc.: 87.50%] [G loss: 3.429617]\n",
      "epoch:44 step:34969 [D loss: 0.362609, acc.: 82.03%] [G loss: 4.163116]\n",
      "epoch:44 step:34970 [D loss: 0.393248, acc.: 81.25%] [G loss: 3.040356]\n",
      "epoch:44 step:34971 [D loss: 0.360279, acc.: 83.59%] [G loss: 3.284116]\n",
      "epoch:44 step:34972 [D loss: 0.233502, acc.: 92.97%] [G loss: 3.643337]\n",
      "epoch:44 step:34973 [D loss: 0.436022, acc.: 75.00%] [G loss: 3.720433]\n",
      "epoch:44 step:34974 [D loss: 0.240557, acc.: 89.06%] [G loss: 3.877269]\n",
      "epoch:44 step:34975 [D loss: 0.259455, acc.: 89.84%] [G loss: 4.988359]\n",
      "epoch:44 step:34976 [D loss: 0.331964, acc.: 89.06%] [G loss: 4.653564]\n",
      "epoch:44 step:34977 [D loss: 0.425518, acc.: 82.03%] [G loss: 2.912437]\n",
      "epoch:44 step:34978 [D loss: 0.248864, acc.: 89.84%] [G loss: 3.006417]\n",
      "epoch:44 step:34979 [D loss: 0.373356, acc.: 85.94%] [G loss: 3.046814]\n",
      "epoch:44 step:34980 [D loss: 0.376162, acc.: 86.72%] [G loss: 2.838006]\n",
      "epoch:44 step:34981 [D loss: 0.316069, acc.: 88.28%] [G loss: 2.853507]\n",
      "epoch:44 step:34982 [D loss: 0.285678, acc.: 89.84%] [G loss: 3.317111]\n",
      "epoch:44 step:34983 [D loss: 0.293531, acc.: 87.50%] [G loss: 2.352541]\n",
      "epoch:44 step:34984 [D loss: 0.293061, acc.: 86.72%] [G loss: 3.532995]\n",
      "epoch:44 step:34985 [D loss: 0.245252, acc.: 89.84%] [G loss: 3.261533]\n",
      "epoch:44 step:34986 [D loss: 0.269211, acc.: 90.62%] [G loss: 3.185216]\n",
      "epoch:44 step:34987 [D loss: 0.392169, acc.: 82.81%] [G loss: 2.762780]\n",
      "epoch:44 step:34988 [D loss: 0.398106, acc.: 84.38%] [G loss: 2.836128]\n",
      "epoch:44 step:34989 [D loss: 0.383522, acc.: 82.81%] [G loss: 2.802238]\n",
      "epoch:44 step:34990 [D loss: 0.254726, acc.: 87.50%] [G loss: 2.582495]\n",
      "epoch:44 step:34991 [D loss: 0.363775, acc.: 84.38%] [G loss: 3.556331]\n",
      "epoch:44 step:34992 [D loss: 0.331043, acc.: 85.16%] [G loss: 2.777120]\n",
      "epoch:44 step:34993 [D loss: 0.405765, acc.: 82.03%] [G loss: 3.475210]\n",
      "epoch:44 step:34994 [D loss: 0.352858, acc.: 85.94%] [G loss: 2.812845]\n",
      "epoch:44 step:34995 [D loss: 0.408629, acc.: 79.69%] [G loss: 3.277212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34996 [D loss: 0.324540, acc.: 85.94%] [G loss: 3.319946]\n",
      "epoch:44 step:34997 [D loss: 0.302437, acc.: 82.81%] [G loss: 2.454371]\n",
      "epoch:44 step:34998 [D loss: 0.335639, acc.: 87.50%] [G loss: 3.422854]\n",
      "epoch:44 step:34999 [D loss: 0.344207, acc.: 83.59%] [G loss: 3.289334]\n",
      "epoch:44 step:35000 [D loss: 0.276611, acc.: 89.84%] [G loss: 3.349145]\n",
      "epoch:44 step:35001 [D loss: 0.360655, acc.: 83.59%] [G loss: 4.504265]\n",
      "epoch:44 step:35002 [D loss: 0.364760, acc.: 80.47%] [G loss: 2.974198]\n",
      "epoch:44 step:35003 [D loss: 0.305281, acc.: 85.16%] [G loss: 3.669752]\n",
      "epoch:44 step:35004 [D loss: 0.308804, acc.: 85.94%] [G loss: 3.767704]\n",
      "epoch:44 step:35005 [D loss: 0.272941, acc.: 85.94%] [G loss: 4.848672]\n",
      "epoch:44 step:35006 [D loss: 0.346810, acc.: 89.06%] [G loss: 3.603069]\n",
      "epoch:44 step:35007 [D loss: 0.317889, acc.: 85.94%] [G loss: 6.911012]\n",
      "epoch:44 step:35008 [D loss: 0.530424, acc.: 75.78%] [G loss: 4.765772]\n",
      "epoch:44 step:35009 [D loss: 0.256666, acc.: 89.84%] [G loss: 4.304888]\n",
      "epoch:44 step:35010 [D loss: 0.417875, acc.: 75.78%] [G loss: 5.042239]\n",
      "epoch:44 step:35011 [D loss: 0.357960, acc.: 87.50%] [G loss: 4.259710]\n",
      "epoch:44 step:35012 [D loss: 0.358303, acc.: 82.81%] [G loss: 3.777633]\n",
      "epoch:44 step:35013 [D loss: 0.335060, acc.: 80.47%] [G loss: 3.025998]\n",
      "epoch:44 step:35014 [D loss: 0.293942, acc.: 85.94%] [G loss: 3.800147]\n",
      "epoch:44 step:35015 [D loss: 0.369937, acc.: 84.38%] [G loss: 4.171747]\n",
      "epoch:44 step:35016 [D loss: 0.223909, acc.: 89.84%] [G loss: 3.369924]\n",
      "epoch:44 step:35017 [D loss: 0.251007, acc.: 92.19%] [G loss: 3.256036]\n",
      "epoch:44 step:35018 [D loss: 0.361407, acc.: 85.16%] [G loss: 2.723046]\n",
      "epoch:44 step:35019 [D loss: 0.317271, acc.: 87.50%] [G loss: 2.492335]\n",
      "epoch:44 step:35020 [D loss: 0.265853, acc.: 89.06%] [G loss: 3.002176]\n",
      "epoch:44 step:35021 [D loss: 0.355121, acc.: 84.38%] [G loss: 3.884793]\n",
      "epoch:44 step:35022 [D loss: 0.284156, acc.: 86.72%] [G loss: 2.582214]\n",
      "epoch:44 step:35023 [D loss: 0.315525, acc.: 83.59%] [G loss: 3.559894]\n",
      "epoch:44 step:35024 [D loss: 0.312317, acc.: 84.38%] [G loss: 3.311102]\n",
      "epoch:44 step:35025 [D loss: 0.386021, acc.: 82.81%] [G loss: 3.609529]\n",
      "epoch:44 step:35026 [D loss: 0.367652, acc.: 78.12%] [G loss: 2.737634]\n",
      "epoch:44 step:35027 [D loss: 0.295633, acc.: 85.94%] [G loss: 4.482697]\n",
      "epoch:44 step:35028 [D loss: 0.374980, acc.: 82.03%] [G loss: 3.166587]\n",
      "epoch:44 step:35029 [D loss: 0.375847, acc.: 83.59%] [G loss: 3.088700]\n",
      "epoch:44 step:35030 [D loss: 0.386334, acc.: 85.16%] [G loss: 4.059792]\n",
      "epoch:44 step:35031 [D loss: 0.283839, acc.: 86.72%] [G loss: 3.700109]\n",
      "epoch:44 step:35032 [D loss: 0.218410, acc.: 90.62%] [G loss: 5.336277]\n",
      "epoch:44 step:35033 [D loss: 0.250429, acc.: 89.84%] [G loss: 6.332892]\n",
      "epoch:44 step:35034 [D loss: 0.273169, acc.: 85.94%] [G loss: 5.117362]\n",
      "epoch:44 step:35035 [D loss: 0.289780, acc.: 88.28%] [G loss: 4.812800]\n",
      "epoch:44 step:35036 [D loss: 0.203320, acc.: 91.41%] [G loss: 4.425241]\n",
      "epoch:44 step:35037 [D loss: 0.170331, acc.: 96.09%] [G loss: 4.477901]\n",
      "epoch:44 step:35038 [D loss: 0.234069, acc.: 89.06%] [G loss: 3.875269]\n",
      "epoch:44 step:35039 [D loss: 0.285188, acc.: 90.62%] [G loss: 4.503172]\n",
      "epoch:44 step:35040 [D loss: 0.377648, acc.: 81.25%] [G loss: 3.499354]\n",
      "epoch:44 step:35041 [D loss: 0.257540, acc.: 89.84%] [G loss: 3.134397]\n",
      "epoch:44 step:35042 [D loss: 0.292865, acc.: 88.28%] [G loss: 3.483434]\n",
      "epoch:44 step:35043 [D loss: 0.258108, acc.: 90.62%] [G loss: 3.767240]\n",
      "epoch:44 step:35044 [D loss: 0.314767, acc.: 85.16%] [G loss: 3.419100]\n",
      "epoch:44 step:35045 [D loss: 0.355758, acc.: 81.25%] [G loss: 2.840418]\n",
      "epoch:44 step:35046 [D loss: 0.325408, acc.: 85.94%] [G loss: 3.104941]\n",
      "epoch:44 step:35047 [D loss: 0.381028, acc.: 84.38%] [G loss: 2.528067]\n",
      "epoch:44 step:35048 [D loss: 0.269915, acc.: 88.28%] [G loss: 2.957874]\n",
      "epoch:44 step:35049 [D loss: 0.374184, acc.: 81.25%] [G loss: 3.197523]\n",
      "epoch:44 step:35050 [D loss: 0.359489, acc.: 83.59%] [G loss: 2.496485]\n",
      "epoch:44 step:35051 [D loss: 0.228670, acc.: 93.75%] [G loss: 3.216509]\n",
      "epoch:44 step:35052 [D loss: 0.233995, acc.: 91.41%] [G loss: 3.248468]\n",
      "epoch:44 step:35053 [D loss: 0.287361, acc.: 92.19%] [G loss: 2.495624]\n",
      "epoch:44 step:35054 [D loss: 0.236271, acc.: 87.50%] [G loss: 3.772218]\n",
      "epoch:44 step:35055 [D loss: 0.196594, acc.: 91.41%] [G loss: 5.343278]\n",
      "epoch:44 step:35056 [D loss: 0.387734, acc.: 84.38%] [G loss: 4.082749]\n",
      "epoch:44 step:35057 [D loss: 0.269042, acc.: 87.50%] [G loss: 3.700829]\n",
      "epoch:44 step:35058 [D loss: 0.164693, acc.: 90.62%] [G loss: 4.250153]\n",
      "epoch:44 step:35059 [D loss: 0.253726, acc.: 89.06%] [G loss: 3.830776]\n",
      "epoch:44 step:35060 [D loss: 0.355722, acc.: 84.38%] [G loss: 3.870702]\n",
      "epoch:44 step:35061 [D loss: 0.299684, acc.: 87.50%] [G loss: 3.411504]\n",
      "epoch:44 step:35062 [D loss: 0.286324, acc.: 88.28%] [G loss: 2.638510]\n",
      "epoch:44 step:35063 [D loss: 0.270011, acc.: 89.06%] [G loss: 2.830519]\n",
      "epoch:44 step:35064 [D loss: 0.330738, acc.: 84.38%] [G loss: 2.896162]\n",
      "epoch:44 step:35065 [D loss: 0.327189, acc.: 85.94%] [G loss: 2.379905]\n",
      "epoch:44 step:35066 [D loss: 0.382441, acc.: 85.94%] [G loss: 2.521236]\n",
      "epoch:44 step:35067 [D loss: 0.378039, acc.: 84.38%] [G loss: 2.336490]\n",
      "epoch:44 step:35068 [D loss: 0.435312, acc.: 81.25%] [G loss: 3.139196]\n",
      "epoch:44 step:35069 [D loss: 0.332181, acc.: 85.16%] [G loss: 3.639611]\n",
      "epoch:44 step:35070 [D loss: 0.349656, acc.: 82.03%] [G loss: 3.911858]\n",
      "epoch:44 step:35071 [D loss: 0.470284, acc.: 76.56%] [G loss: 3.243176]\n",
      "epoch:44 step:35072 [D loss: 0.289827, acc.: 88.28%] [G loss: 2.503344]\n",
      "epoch:44 step:35073 [D loss: 0.235431, acc.: 92.19%] [G loss: 2.691634]\n",
      "epoch:44 step:35074 [D loss: 0.300324, acc.: 89.06%] [G loss: 2.982923]\n",
      "epoch:44 step:35075 [D loss: 0.274739, acc.: 90.62%] [G loss: 3.746996]\n",
      "epoch:44 step:35076 [D loss: 0.353442, acc.: 85.16%] [G loss: 4.543618]\n",
      "epoch:44 step:35077 [D loss: 0.359122, acc.: 84.38%] [G loss: 3.081420]\n",
      "epoch:44 step:35078 [D loss: 0.288798, acc.: 86.72%] [G loss: 3.100904]\n",
      "epoch:44 step:35079 [D loss: 0.309341, acc.: 90.62%] [G loss: 3.307113]\n",
      "epoch:44 step:35080 [D loss: 0.325395, acc.: 86.72%] [G loss: 3.005414]\n",
      "epoch:44 step:35081 [D loss: 0.233131, acc.: 89.84%] [G loss: 4.222264]\n",
      "epoch:44 step:35082 [D loss: 0.334260, acc.: 85.94%] [G loss: 3.515248]\n",
      "epoch:44 step:35083 [D loss: 0.331903, acc.: 83.59%] [G loss: 3.207557]\n",
      "epoch:44 step:35084 [D loss: 0.284718, acc.: 88.28%] [G loss: 5.302640]\n",
      "epoch:44 step:35085 [D loss: 0.358392, acc.: 85.94%] [G loss: 3.750850]\n",
      "epoch:44 step:35086 [D loss: 0.289296, acc.: 86.72%] [G loss: 3.948734]\n",
      "epoch:44 step:35087 [D loss: 0.350802, acc.: 85.16%] [G loss: 5.506055]\n",
      "epoch:44 step:35088 [D loss: 0.457980, acc.: 79.69%] [G loss: 2.211964]\n",
      "epoch:44 step:35089 [D loss: 0.282744, acc.: 85.16%] [G loss: 2.899721]\n",
      "epoch:44 step:35090 [D loss: 0.355874, acc.: 82.81%] [G loss: 3.658271]\n",
      "epoch:44 step:35091 [D loss: 0.302425, acc.: 89.06%] [G loss: 3.391653]\n",
      "epoch:44 step:35092 [D loss: 0.389329, acc.: 80.47%] [G loss: 3.753232]\n",
      "epoch:44 step:35093 [D loss: 0.257227, acc.: 92.19%] [G loss: 3.540838]\n",
      "epoch:44 step:35094 [D loss: 0.322790, acc.: 85.94%] [G loss: 3.203176]\n",
      "epoch:44 step:35095 [D loss: 0.361114, acc.: 84.38%] [G loss: 2.760387]\n",
      "epoch:44 step:35096 [D loss: 0.305168, acc.: 86.72%] [G loss: 2.963284]\n",
      "epoch:44 step:35097 [D loss: 0.281413, acc.: 85.94%] [G loss: 3.036999]\n",
      "epoch:44 step:35098 [D loss: 0.343218, acc.: 82.03%] [G loss: 2.283885]\n",
      "epoch:44 step:35099 [D loss: 0.266393, acc.: 86.72%] [G loss: 3.347416]\n",
      "epoch:44 step:35100 [D loss: 0.313206, acc.: 85.94%] [G loss: 3.317013]\n",
      "epoch:44 step:35101 [D loss: 0.442846, acc.: 79.69%] [G loss: 2.984131]\n",
      "epoch:44 step:35102 [D loss: 0.295664, acc.: 85.16%] [G loss: 3.452279]\n",
      "epoch:44 step:35103 [D loss: 0.341979, acc.: 83.59%] [G loss: 3.476013]\n",
      "epoch:44 step:35104 [D loss: 0.306660, acc.: 85.94%] [G loss: 3.319788]\n",
      "epoch:44 step:35105 [D loss: 0.361385, acc.: 78.91%] [G loss: 2.735525]\n",
      "epoch:44 step:35106 [D loss: 0.254908, acc.: 89.84%] [G loss: 3.109763]\n",
      "epoch:44 step:35107 [D loss: 0.281265, acc.: 87.50%] [G loss: 3.118563]\n",
      "epoch:44 step:35108 [D loss: 0.356643, acc.: 85.16%] [G loss: 2.540470]\n",
      "epoch:44 step:35109 [D loss: 0.261940, acc.: 88.28%] [G loss: 3.149667]\n",
      "epoch:44 step:35110 [D loss: 0.283416, acc.: 88.28%] [G loss: 2.638401]\n",
      "epoch:44 step:35111 [D loss: 0.416311, acc.: 84.38%] [G loss: 3.994011]\n",
      "epoch:44 step:35112 [D loss: 0.327879, acc.: 85.16%] [G loss: 2.783262]\n",
      "epoch:44 step:35113 [D loss: 0.239956, acc.: 90.62%] [G loss: 4.139356]\n",
      "epoch:44 step:35114 [D loss: 0.242450, acc.: 92.19%] [G loss: 3.121450]\n",
      "epoch:44 step:35115 [D loss: 0.275760, acc.: 92.19%] [G loss: 4.303553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:35116 [D loss: 0.274335, acc.: 87.50%] [G loss: 5.448286]\n",
      "epoch:44 step:35117 [D loss: 0.337030, acc.: 85.16%] [G loss: 2.819770]\n",
      "epoch:44 step:35118 [D loss: 0.273591, acc.: 91.41%] [G loss: 3.685094]\n",
      "epoch:44 step:35119 [D loss: 0.278811, acc.: 89.06%] [G loss: 3.502921]\n",
      "epoch:44 step:35120 [D loss: 0.253064, acc.: 90.62%] [G loss: 2.687460]\n",
      "epoch:44 step:35121 [D loss: 0.292241, acc.: 86.72%] [G loss: 3.301652]\n",
      "epoch:44 step:35122 [D loss: 0.345255, acc.: 85.16%] [G loss: 2.970768]\n",
      "epoch:44 step:35123 [D loss: 0.259898, acc.: 88.28%] [G loss: 3.706925]\n",
      "epoch:44 step:35124 [D loss: 0.311916, acc.: 85.94%] [G loss: 2.916425]\n",
      "epoch:44 step:35125 [D loss: 0.282812, acc.: 86.72%] [G loss: 3.693005]\n",
      "epoch:44 step:35126 [D loss: 0.375310, acc.: 85.16%] [G loss: 3.441928]\n",
      "epoch:44 step:35127 [D loss: 0.259428, acc.: 92.19%] [G loss: 2.936701]\n",
      "epoch:44 step:35128 [D loss: 0.335457, acc.: 87.50%] [G loss: 4.400668]\n",
      "epoch:44 step:35129 [D loss: 0.374738, acc.: 83.59%] [G loss: 4.738186]\n",
      "epoch:44 step:35130 [D loss: 0.266192, acc.: 90.62%] [G loss: 2.651335]\n",
      "epoch:44 step:35131 [D loss: 0.227786, acc.: 89.84%] [G loss: 4.297531]\n",
      "epoch:44 step:35132 [D loss: 0.288242, acc.: 88.28%] [G loss: 3.929437]\n",
      "epoch:44 step:35133 [D loss: 0.212155, acc.: 91.41%] [G loss: 2.923592]\n",
      "epoch:44 step:35134 [D loss: 0.275881, acc.: 87.50%] [G loss: 5.053432]\n",
      "epoch:44 step:35135 [D loss: 0.366334, acc.: 84.38%] [G loss: 3.495061]\n",
      "epoch:44 step:35136 [D loss: 0.246598, acc.: 89.06%] [G loss: 2.771796]\n",
      "epoch:44 step:35137 [D loss: 0.282620, acc.: 89.06%] [G loss: 2.816584]\n",
      "epoch:44 step:35138 [D loss: 0.358345, acc.: 82.03%] [G loss: 2.460046]\n",
      "epoch:44 step:35139 [D loss: 0.267571, acc.: 88.28%] [G loss: 3.686582]\n",
      "epoch:44 step:35140 [D loss: 0.317623, acc.: 86.72%] [G loss: 2.660547]\n",
      "epoch:44 step:35141 [D loss: 0.421955, acc.: 81.25%] [G loss: 2.694989]\n",
      "epoch:44 step:35142 [D loss: 0.325355, acc.: 86.72%] [G loss: 2.982263]\n",
      "epoch:44 step:35143 [D loss: 0.306748, acc.: 87.50%] [G loss: 2.704556]\n",
      "epoch:44 step:35144 [D loss: 0.264041, acc.: 88.28%] [G loss: 2.909776]\n",
      "epoch:44 step:35145 [D loss: 0.276067, acc.: 86.72%] [G loss: 2.991409]\n",
      "epoch:45 step:35146 [D loss: 0.394822, acc.: 79.69%] [G loss: 3.195773]\n",
      "epoch:45 step:35147 [D loss: 0.249952, acc.: 89.84%] [G loss: 3.244293]\n",
      "epoch:45 step:35148 [D loss: 0.382490, acc.: 82.03%] [G loss: 4.076951]\n",
      "epoch:45 step:35149 [D loss: 0.410290, acc.: 82.81%] [G loss: 3.442101]\n",
      "epoch:45 step:35150 [D loss: 0.333553, acc.: 90.62%] [G loss: 4.025799]\n",
      "epoch:45 step:35151 [D loss: 0.451639, acc.: 81.25%] [G loss: 3.673990]\n",
      "epoch:45 step:35152 [D loss: 0.317687, acc.: 87.50%] [G loss: 3.351419]\n",
      "epoch:45 step:35153 [D loss: 0.311996, acc.: 86.72%] [G loss: 4.415722]\n",
      "epoch:45 step:35154 [D loss: 0.304616, acc.: 88.28%] [G loss: 3.412289]\n",
      "epoch:45 step:35155 [D loss: 0.237361, acc.: 90.62%] [G loss: 4.343922]\n",
      "epoch:45 step:35156 [D loss: 0.168992, acc.: 94.53%] [G loss: 4.516073]\n",
      "epoch:45 step:35157 [D loss: 0.219376, acc.: 89.84%] [G loss: 4.788351]\n",
      "epoch:45 step:35158 [D loss: 0.329971, acc.: 86.72%] [G loss: 5.140058]\n",
      "epoch:45 step:35159 [D loss: 0.316156, acc.: 86.72%] [G loss: 5.129876]\n",
      "epoch:45 step:35160 [D loss: 0.399159, acc.: 82.03%] [G loss: 4.107721]\n",
      "epoch:45 step:35161 [D loss: 0.252322, acc.: 89.06%] [G loss: 3.463795]\n",
      "epoch:45 step:35162 [D loss: 0.220389, acc.: 92.19%] [G loss: 3.865884]\n",
      "epoch:45 step:35163 [D loss: 0.359169, acc.: 81.25%] [G loss: 4.329145]\n",
      "epoch:45 step:35164 [D loss: 0.275171, acc.: 86.72%] [G loss: 3.639605]\n",
      "epoch:45 step:35165 [D loss: 0.341830, acc.: 86.72%] [G loss: 3.157426]\n",
      "epoch:45 step:35166 [D loss: 0.298794, acc.: 84.38%] [G loss: 3.212875]\n",
      "epoch:45 step:35167 [D loss: 0.343663, acc.: 84.38%] [G loss: 3.352577]\n",
      "epoch:45 step:35168 [D loss: 0.340122, acc.: 84.38%] [G loss: 2.939409]\n",
      "epoch:45 step:35169 [D loss: 0.340422, acc.: 83.59%] [G loss: 2.963176]\n",
      "epoch:45 step:35170 [D loss: 0.313197, acc.: 85.16%] [G loss: 3.305376]\n",
      "epoch:45 step:35171 [D loss: 0.337248, acc.: 85.94%] [G loss: 2.756599]\n",
      "epoch:45 step:35172 [D loss: 0.314743, acc.: 80.47%] [G loss: 3.206852]\n",
      "epoch:45 step:35173 [D loss: 0.378184, acc.: 82.03%] [G loss: 3.024617]\n",
      "epoch:45 step:35174 [D loss: 0.285846, acc.: 89.06%] [G loss: 2.820353]\n",
      "epoch:45 step:35175 [D loss: 0.263539, acc.: 89.06%] [G loss: 2.811873]\n",
      "epoch:45 step:35176 [D loss: 0.333614, acc.: 82.81%] [G loss: 2.892476]\n",
      "epoch:45 step:35177 [D loss: 0.360815, acc.: 82.03%] [G loss: 2.963587]\n",
      "epoch:45 step:35178 [D loss: 0.360768, acc.: 85.16%] [G loss: 2.558975]\n",
      "epoch:45 step:35179 [D loss: 0.355711, acc.: 79.69%] [G loss: 2.742769]\n",
      "epoch:45 step:35180 [D loss: 0.244085, acc.: 90.62%] [G loss: 3.117926]\n",
      "epoch:45 step:35181 [D loss: 0.270300, acc.: 88.28%] [G loss: 3.737232]\n",
      "epoch:45 step:35182 [D loss: 0.391854, acc.: 82.81%] [G loss: 5.582418]\n",
      "epoch:45 step:35183 [D loss: 0.293450, acc.: 86.72%] [G loss: 2.938424]\n",
      "epoch:45 step:35184 [D loss: 0.350567, acc.: 81.25%] [G loss: 2.629288]\n",
      "epoch:45 step:35185 [D loss: 0.309093, acc.: 89.06%] [G loss: 3.431741]\n",
      "epoch:45 step:35186 [D loss: 0.269965, acc.: 88.28%] [G loss: 3.220828]\n",
      "epoch:45 step:35187 [D loss: 0.327305, acc.: 85.16%] [G loss: 4.156522]\n",
      "epoch:45 step:35188 [D loss: 0.392290, acc.: 78.91%] [G loss: 3.362983]\n",
      "epoch:45 step:35189 [D loss: 0.244062, acc.: 91.41%] [G loss: 2.821110]\n",
      "epoch:45 step:35190 [D loss: 0.286138, acc.: 85.94%] [G loss: 4.358934]\n",
      "epoch:45 step:35191 [D loss: 0.248248, acc.: 89.84%] [G loss: 4.288199]\n",
      "epoch:45 step:35192 [D loss: 0.299455, acc.: 86.72%] [G loss: 4.689329]\n",
      "epoch:45 step:35193 [D loss: 0.283523, acc.: 87.50%] [G loss: 3.967565]\n",
      "epoch:45 step:35194 [D loss: 0.274669, acc.: 89.06%] [G loss: 3.848752]\n",
      "epoch:45 step:35195 [D loss: 0.355056, acc.: 82.81%] [G loss: 3.936948]\n",
      "epoch:45 step:35196 [D loss: 0.276137, acc.: 88.28%] [G loss: 5.499043]\n",
      "epoch:45 step:35197 [D loss: 0.344513, acc.: 87.50%] [G loss: 3.636570]\n",
      "epoch:45 step:35198 [D loss: 0.223079, acc.: 89.06%] [G loss: 3.086753]\n",
      "epoch:45 step:35199 [D loss: 0.211580, acc.: 89.06%] [G loss: 2.882123]\n",
      "epoch:45 step:35200 [D loss: 0.270254, acc.: 87.50%] [G loss: 3.037182]\n",
      "epoch:45 step:35201 [D loss: 0.360020, acc.: 82.81%] [G loss: 3.766556]\n",
      "epoch:45 step:35202 [D loss: 0.296914, acc.: 87.50%] [G loss: 4.351614]\n",
      "epoch:45 step:35203 [D loss: 0.263042, acc.: 87.50%] [G loss: 3.794389]\n",
      "epoch:45 step:35204 [D loss: 0.268742, acc.: 92.19%] [G loss: 3.903514]\n",
      "epoch:45 step:35205 [D loss: 0.166267, acc.: 95.31%] [G loss: 4.080217]\n",
      "epoch:45 step:35206 [D loss: 0.273518, acc.: 85.94%] [G loss: 4.267783]\n",
      "epoch:45 step:35207 [D loss: 0.203761, acc.: 89.06%] [G loss: 3.776554]\n",
      "epoch:45 step:35208 [D loss: 0.160819, acc.: 96.09%] [G loss: 3.841866]\n",
      "epoch:45 step:35209 [D loss: 0.318976, acc.: 85.16%] [G loss: 3.177589]\n",
      "epoch:45 step:35210 [D loss: 0.408488, acc.: 83.59%] [G loss: 4.221385]\n",
      "epoch:45 step:35211 [D loss: 0.351041, acc.: 84.38%] [G loss: 3.070043]\n",
      "epoch:45 step:35212 [D loss: 0.352681, acc.: 83.59%] [G loss: 2.876127]\n",
      "epoch:45 step:35213 [D loss: 0.317125, acc.: 85.16%] [G loss: 3.496349]\n",
      "epoch:45 step:35214 [D loss: 0.340270, acc.: 85.94%] [G loss: 3.641419]\n",
      "epoch:45 step:35215 [D loss: 0.303920, acc.: 86.72%] [G loss: 3.652576]\n",
      "epoch:45 step:35216 [D loss: 0.287669, acc.: 88.28%] [G loss: 3.531250]\n",
      "epoch:45 step:35217 [D loss: 0.387524, acc.: 82.03%] [G loss: 3.500152]\n",
      "epoch:45 step:35218 [D loss: 0.399100, acc.: 85.94%] [G loss: 5.723889]\n",
      "epoch:45 step:35219 [D loss: 0.701231, acc.: 78.12%] [G loss: 10.299645]\n",
      "epoch:45 step:35220 [D loss: 1.986637, acc.: 64.06%] [G loss: 5.165365]\n",
      "epoch:45 step:35221 [D loss: 0.495133, acc.: 84.38%] [G loss: 4.291457]\n",
      "epoch:45 step:35222 [D loss: 0.803484, acc.: 71.09%] [G loss: 2.499808]\n",
      "epoch:45 step:35223 [D loss: 0.493800, acc.: 76.56%] [G loss: 3.887250]\n",
      "epoch:45 step:35224 [D loss: 0.416813, acc.: 84.38%] [G loss: 3.301993]\n",
      "epoch:45 step:35225 [D loss: 0.392439, acc.: 80.47%] [G loss: 4.214187]\n",
      "epoch:45 step:35226 [D loss: 0.355326, acc.: 85.16%] [G loss: 2.998794]\n",
      "epoch:45 step:35227 [D loss: 0.321978, acc.: 83.59%] [G loss: 3.353693]\n",
      "epoch:45 step:35228 [D loss: 0.287111, acc.: 84.38%] [G loss: 3.762406]\n",
      "epoch:45 step:35229 [D loss: 0.339223, acc.: 84.38%] [G loss: 3.102523]\n",
      "epoch:45 step:35230 [D loss: 0.445021, acc.: 77.34%] [G loss: 3.282750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35231 [D loss: 0.326773, acc.: 85.16%] [G loss: 2.736453]\n",
      "epoch:45 step:35232 [D loss: 0.381292, acc.: 82.03%] [G loss: 2.648241]\n",
      "epoch:45 step:35233 [D loss: 0.280126, acc.: 86.72%] [G loss: 2.728285]\n",
      "epoch:45 step:35234 [D loss: 0.345439, acc.: 87.50%] [G loss: 2.537206]\n",
      "epoch:45 step:35235 [D loss: 0.352043, acc.: 82.03%] [G loss: 2.823884]\n",
      "epoch:45 step:35236 [D loss: 0.425055, acc.: 83.59%] [G loss: 2.664171]\n",
      "epoch:45 step:35237 [D loss: 0.378639, acc.: 83.59%] [G loss: 3.413163]\n",
      "epoch:45 step:35238 [D loss: 0.397584, acc.: 80.47%] [G loss: 2.590035]\n",
      "epoch:45 step:35239 [D loss: 0.256528, acc.: 85.16%] [G loss: 3.892835]\n",
      "epoch:45 step:35240 [D loss: 0.319520, acc.: 85.16%] [G loss: 2.925828]\n",
      "epoch:45 step:35241 [D loss: 0.323619, acc.: 83.59%] [G loss: 4.458609]\n",
      "epoch:45 step:35242 [D loss: 0.283493, acc.: 87.50%] [G loss: 5.374530]\n",
      "epoch:45 step:35243 [D loss: 0.213754, acc.: 92.19%] [G loss: 7.570473]\n",
      "epoch:45 step:35244 [D loss: 0.298275, acc.: 88.28%] [G loss: 4.445016]\n",
      "epoch:45 step:35245 [D loss: 0.202038, acc.: 92.19%] [G loss: 3.471547]\n",
      "epoch:45 step:35246 [D loss: 0.259091, acc.: 87.50%] [G loss: 5.583975]\n",
      "epoch:45 step:35247 [D loss: 0.216594, acc.: 90.62%] [G loss: 3.857188]\n",
      "epoch:45 step:35248 [D loss: 0.285027, acc.: 86.72%] [G loss: 4.684290]\n",
      "epoch:45 step:35249 [D loss: 0.257675, acc.: 89.84%] [G loss: 3.529660]\n",
      "epoch:45 step:35250 [D loss: 0.332489, acc.: 82.81%] [G loss: 4.644139]\n",
      "epoch:45 step:35251 [D loss: 0.251927, acc.: 88.28%] [G loss: 4.463551]\n",
      "epoch:45 step:35252 [D loss: 0.323042, acc.: 85.94%] [G loss: 3.613198]\n",
      "epoch:45 step:35253 [D loss: 0.239396, acc.: 91.41%] [G loss: 4.300942]\n",
      "epoch:45 step:35254 [D loss: 0.365859, acc.: 82.03%] [G loss: 4.478620]\n",
      "epoch:45 step:35255 [D loss: 0.322643, acc.: 86.72%] [G loss: 3.564300]\n",
      "epoch:45 step:35256 [D loss: 0.274157, acc.: 85.16%] [G loss: 3.608522]\n",
      "epoch:45 step:35257 [D loss: 0.277518, acc.: 83.59%] [G loss: 3.184466]\n",
      "epoch:45 step:35258 [D loss: 0.366207, acc.: 82.03%] [G loss: 3.155798]\n",
      "epoch:45 step:35259 [D loss: 0.185288, acc.: 96.09%] [G loss: 3.701535]\n",
      "epoch:45 step:35260 [D loss: 0.226356, acc.: 90.62%] [G loss: 3.955712]\n",
      "epoch:45 step:35261 [D loss: 0.267195, acc.: 88.28%] [G loss: 3.251029]\n",
      "epoch:45 step:35262 [D loss: 0.275849, acc.: 86.72%] [G loss: 3.150836]\n",
      "epoch:45 step:35263 [D loss: 0.250494, acc.: 89.84%] [G loss: 2.751571]\n",
      "epoch:45 step:35264 [D loss: 0.183161, acc.: 92.97%] [G loss: 2.998849]\n",
      "epoch:45 step:35265 [D loss: 0.316633, acc.: 86.72%] [G loss: 3.326939]\n",
      "epoch:45 step:35266 [D loss: 0.291047, acc.: 88.28%] [G loss: 2.727850]\n",
      "epoch:45 step:35267 [D loss: 0.284851, acc.: 85.94%] [G loss: 3.244067]\n",
      "epoch:45 step:35268 [D loss: 0.263920, acc.: 89.84%] [G loss: 2.767779]\n",
      "epoch:45 step:35269 [D loss: 0.377124, acc.: 81.25%] [G loss: 3.298539]\n",
      "epoch:45 step:35270 [D loss: 0.293661, acc.: 88.28%] [G loss: 3.488509]\n",
      "epoch:45 step:35271 [D loss: 0.291581, acc.: 84.38%] [G loss: 4.352768]\n",
      "epoch:45 step:35272 [D loss: 0.260215, acc.: 89.06%] [G loss: 3.751572]\n",
      "epoch:45 step:35273 [D loss: 0.266187, acc.: 89.06%] [G loss: 5.617104]\n",
      "epoch:45 step:35274 [D loss: 0.295269, acc.: 89.06%] [G loss: 4.073130]\n",
      "epoch:45 step:35275 [D loss: 0.276073, acc.: 87.50%] [G loss: 5.545973]\n",
      "epoch:45 step:35276 [D loss: 0.281976, acc.: 85.16%] [G loss: 3.946075]\n",
      "epoch:45 step:35277 [D loss: 0.224929, acc.: 90.62%] [G loss: 4.592066]\n",
      "epoch:45 step:35278 [D loss: 0.356593, acc.: 82.81%] [G loss: 3.581013]\n",
      "epoch:45 step:35279 [D loss: 0.286947, acc.: 88.28%] [G loss: 4.553604]\n",
      "epoch:45 step:35280 [D loss: 0.286027, acc.: 88.28%] [G loss: 3.918357]\n",
      "epoch:45 step:35281 [D loss: 0.200154, acc.: 92.19%] [G loss: 3.838359]\n",
      "epoch:45 step:35282 [D loss: 0.259088, acc.: 89.84%] [G loss: 4.098484]\n",
      "epoch:45 step:35283 [D loss: 0.352757, acc.: 83.59%] [G loss: 4.367078]\n",
      "epoch:45 step:35284 [D loss: 0.223986, acc.: 89.84%] [G loss: 3.334013]\n",
      "epoch:45 step:35285 [D loss: 0.257092, acc.: 85.94%] [G loss: 2.413536]\n",
      "epoch:45 step:35286 [D loss: 0.322798, acc.: 82.81%] [G loss: 3.598430]\n",
      "epoch:45 step:35287 [D loss: 0.355102, acc.: 82.81%] [G loss: 2.708775]\n",
      "epoch:45 step:35288 [D loss: 0.274647, acc.: 88.28%] [G loss: 3.225588]\n",
      "epoch:45 step:35289 [D loss: 0.370555, acc.: 79.69%] [G loss: 2.397579]\n",
      "epoch:45 step:35290 [D loss: 0.337894, acc.: 82.03%] [G loss: 2.487684]\n",
      "epoch:45 step:35291 [D loss: 0.310553, acc.: 85.94%] [G loss: 2.443670]\n",
      "epoch:45 step:35292 [D loss: 0.251407, acc.: 88.28%] [G loss: 2.670205]\n",
      "epoch:45 step:35293 [D loss: 0.377946, acc.: 78.12%] [G loss: 2.254011]\n",
      "epoch:45 step:35294 [D loss: 0.370131, acc.: 85.16%] [G loss: 2.894430]\n",
      "epoch:45 step:35295 [D loss: 0.400026, acc.: 83.59%] [G loss: 3.109554]\n",
      "epoch:45 step:35296 [D loss: 0.294899, acc.: 90.62%] [G loss: 2.768878]\n",
      "epoch:45 step:35297 [D loss: 0.299355, acc.: 85.16%] [G loss: 3.694361]\n",
      "epoch:45 step:35298 [D loss: 0.345489, acc.: 82.81%] [G loss: 2.685959]\n",
      "epoch:45 step:35299 [D loss: 0.288293, acc.: 86.72%] [G loss: 3.154743]\n",
      "epoch:45 step:35300 [D loss: 0.330957, acc.: 85.94%] [G loss: 4.213498]\n",
      "epoch:45 step:35301 [D loss: 0.363025, acc.: 84.38%] [G loss: 2.528527]\n",
      "epoch:45 step:35302 [D loss: 0.292775, acc.: 85.94%] [G loss: 4.268776]\n",
      "epoch:45 step:35303 [D loss: 0.316998, acc.: 85.94%] [G loss: 3.098347]\n",
      "epoch:45 step:35304 [D loss: 0.200419, acc.: 90.62%] [G loss: 3.119668]\n",
      "epoch:45 step:35305 [D loss: 0.429921, acc.: 77.34%] [G loss: 3.239904]\n",
      "epoch:45 step:35306 [D loss: 0.246273, acc.: 89.84%] [G loss: 3.590451]\n",
      "epoch:45 step:35307 [D loss: 0.295177, acc.: 86.72%] [G loss: 2.378330]\n",
      "epoch:45 step:35308 [D loss: 0.274806, acc.: 88.28%] [G loss: 3.112781]\n",
      "epoch:45 step:35309 [D loss: 0.264460, acc.: 92.19%] [G loss: 2.492714]\n",
      "epoch:45 step:35310 [D loss: 0.251638, acc.: 90.62%] [G loss: 4.328077]\n",
      "epoch:45 step:35311 [D loss: 0.240160, acc.: 89.06%] [G loss: 3.766633]\n",
      "epoch:45 step:35312 [D loss: 0.355910, acc.: 87.50%] [G loss: 4.170716]\n",
      "epoch:45 step:35313 [D loss: 0.272760, acc.: 85.16%] [G loss: 4.523977]\n",
      "epoch:45 step:35314 [D loss: 0.297696, acc.: 86.72%] [G loss: 3.177335]\n",
      "epoch:45 step:35315 [D loss: 0.286326, acc.: 88.28%] [G loss: 4.029403]\n",
      "epoch:45 step:35316 [D loss: 0.277250, acc.: 89.06%] [G loss: 3.329032]\n",
      "epoch:45 step:35317 [D loss: 0.206171, acc.: 92.97%] [G loss: 4.267566]\n",
      "epoch:45 step:35318 [D loss: 0.251532, acc.: 89.84%] [G loss: 3.139997]\n",
      "epoch:45 step:35319 [D loss: 0.337428, acc.: 84.38%] [G loss: 3.363788]\n",
      "epoch:45 step:35320 [D loss: 0.251563, acc.: 91.41%] [G loss: 3.024232]\n",
      "epoch:45 step:35321 [D loss: 0.286689, acc.: 86.72%] [G loss: 2.623311]\n",
      "epoch:45 step:35322 [D loss: 0.317848, acc.: 85.94%] [G loss: 3.507839]\n",
      "epoch:45 step:35323 [D loss: 0.250525, acc.: 89.84%] [G loss: 2.702014]\n",
      "epoch:45 step:35324 [D loss: 0.310001, acc.: 85.94%] [G loss: 3.736660]\n",
      "epoch:45 step:35325 [D loss: 0.342452, acc.: 85.16%] [G loss: 3.326390]\n",
      "epoch:45 step:35326 [D loss: 0.267675, acc.: 90.62%] [G loss: 2.970463]\n",
      "epoch:45 step:35327 [D loss: 0.325646, acc.: 83.59%] [G loss: 5.460497]\n",
      "epoch:45 step:35328 [D loss: 0.436552, acc.: 87.50%] [G loss: 4.932666]\n",
      "epoch:45 step:35329 [D loss: 0.396654, acc.: 83.59%] [G loss: 4.600394]\n",
      "epoch:45 step:35330 [D loss: 0.253403, acc.: 90.62%] [G loss: 3.649268]\n",
      "epoch:45 step:35331 [D loss: 0.334017, acc.: 84.38%] [G loss: 3.237001]\n",
      "epoch:45 step:35332 [D loss: 0.368712, acc.: 85.16%] [G loss: 3.500774]\n",
      "epoch:45 step:35333 [D loss: 0.320583, acc.: 86.72%] [G loss: 3.294112]\n",
      "epoch:45 step:35334 [D loss: 0.329611, acc.: 84.38%] [G loss: 3.242435]\n",
      "epoch:45 step:35335 [D loss: 0.302818, acc.: 85.94%] [G loss: 3.166765]\n",
      "epoch:45 step:35336 [D loss: 0.373206, acc.: 82.81%] [G loss: 4.571958]\n",
      "epoch:45 step:35337 [D loss: 0.315231, acc.: 86.72%] [G loss: 3.648363]\n",
      "epoch:45 step:35338 [D loss: 0.274214, acc.: 88.28%] [G loss: 2.739752]\n",
      "epoch:45 step:35339 [D loss: 0.589799, acc.: 75.00%] [G loss: 3.148618]\n",
      "epoch:45 step:35340 [D loss: 0.250543, acc.: 89.84%] [G loss: 2.572777]\n",
      "epoch:45 step:35341 [D loss: 0.419643, acc.: 82.03%] [G loss: 2.876688]\n",
      "epoch:45 step:35342 [D loss: 0.315670, acc.: 86.72%] [G loss: 2.746538]\n",
      "epoch:45 step:35343 [D loss: 0.337348, acc.: 83.59%] [G loss: 2.982641]\n",
      "epoch:45 step:35344 [D loss: 0.400567, acc.: 80.47%] [G loss: 3.142257]\n",
      "epoch:45 step:35345 [D loss: 0.342760, acc.: 86.72%] [G loss: 3.513861]\n",
      "epoch:45 step:35346 [D loss: 0.327749, acc.: 82.03%] [G loss: 3.971134]\n",
      "epoch:45 step:35347 [D loss: 0.264782, acc.: 89.06%] [G loss: 4.036172]\n",
      "epoch:45 step:35348 [D loss: 0.396816, acc.: 85.16%] [G loss: 4.406685]\n",
      "epoch:45 step:35349 [D loss: 0.258624, acc.: 89.84%] [G loss: 2.959470]\n",
      "epoch:45 step:35350 [D loss: 0.297870, acc.: 88.28%] [G loss: 5.838966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35351 [D loss: 0.595151, acc.: 75.00%] [G loss: 4.505576]\n",
      "epoch:45 step:35352 [D loss: 0.684991, acc.: 71.88%] [G loss: 5.038007]\n",
      "epoch:45 step:35353 [D loss: 0.808116, acc.: 75.78%] [G loss: 7.948091]\n",
      "epoch:45 step:35354 [D loss: 1.833680, acc.: 50.00%] [G loss: 4.732178]\n",
      "epoch:45 step:35355 [D loss: 0.575798, acc.: 72.66%] [G loss: 4.141877]\n",
      "epoch:45 step:35356 [D loss: 0.674530, acc.: 71.88%] [G loss: 6.278917]\n",
      "epoch:45 step:35357 [D loss: 0.779076, acc.: 73.44%] [G loss: 2.116556]\n",
      "epoch:45 step:35358 [D loss: 0.586636, acc.: 78.91%] [G loss: 3.259617]\n",
      "epoch:45 step:35359 [D loss: 0.400099, acc.: 81.25%] [G loss: 2.695852]\n",
      "epoch:45 step:35360 [D loss: 0.404835, acc.: 83.59%] [G loss: 4.024585]\n",
      "epoch:45 step:35361 [D loss: 0.394059, acc.: 80.47%] [G loss: 2.799584]\n",
      "epoch:45 step:35362 [D loss: 0.358036, acc.: 82.03%] [G loss: 3.085941]\n",
      "epoch:45 step:35363 [D loss: 0.362496, acc.: 83.59%] [G loss: 4.205507]\n",
      "epoch:45 step:35364 [D loss: 0.285474, acc.: 89.84%] [G loss: 3.246809]\n",
      "epoch:45 step:35365 [D loss: 0.316516, acc.: 87.50%] [G loss: 2.672887]\n",
      "epoch:45 step:35366 [D loss: 0.212701, acc.: 90.62%] [G loss: 3.125013]\n",
      "epoch:45 step:35367 [D loss: 0.274003, acc.: 86.72%] [G loss: 2.466345]\n",
      "epoch:45 step:35368 [D loss: 0.279533, acc.: 88.28%] [G loss: 2.811161]\n",
      "epoch:45 step:35369 [D loss: 0.323684, acc.: 85.94%] [G loss: 2.881680]\n",
      "epoch:45 step:35370 [D loss: 0.284069, acc.: 86.72%] [G loss: 2.322609]\n",
      "epoch:45 step:35371 [D loss: 0.326638, acc.: 89.06%] [G loss: 2.866008]\n",
      "epoch:45 step:35372 [D loss: 0.366586, acc.: 83.59%] [G loss: 2.441703]\n",
      "epoch:45 step:35373 [D loss: 0.349186, acc.: 85.16%] [G loss: 2.407011]\n",
      "epoch:45 step:35374 [D loss: 0.421545, acc.: 78.91%] [G loss: 2.826372]\n",
      "epoch:45 step:35375 [D loss: 0.289436, acc.: 87.50%] [G loss: 2.743513]\n",
      "epoch:45 step:35376 [D loss: 0.316569, acc.: 85.16%] [G loss: 2.630521]\n",
      "epoch:45 step:35377 [D loss: 0.249401, acc.: 87.50%] [G loss: 2.399743]\n",
      "epoch:45 step:35378 [D loss: 0.296405, acc.: 89.06%] [G loss: 2.257074]\n",
      "epoch:45 step:35379 [D loss: 0.262748, acc.: 89.06%] [G loss: 2.468100]\n",
      "epoch:45 step:35380 [D loss: 0.371577, acc.: 81.25%] [G loss: 2.977568]\n",
      "epoch:45 step:35381 [D loss: 0.345209, acc.: 86.72%] [G loss: 2.227968]\n",
      "epoch:45 step:35382 [D loss: 0.287073, acc.: 89.06%] [G loss: 2.509607]\n",
      "epoch:45 step:35383 [D loss: 0.330989, acc.: 82.81%] [G loss: 2.684257]\n",
      "epoch:45 step:35384 [D loss: 0.276714, acc.: 91.41%] [G loss: 2.793000]\n",
      "epoch:45 step:35385 [D loss: 0.352697, acc.: 84.38%] [G loss: 3.159205]\n",
      "epoch:45 step:35386 [D loss: 0.382263, acc.: 80.47%] [G loss: 3.119857]\n",
      "epoch:45 step:35387 [D loss: 0.323520, acc.: 84.38%] [G loss: 2.599624]\n",
      "epoch:45 step:35388 [D loss: 0.439960, acc.: 82.03%] [G loss: 2.811633]\n",
      "epoch:45 step:35389 [D loss: 0.299355, acc.: 87.50%] [G loss: 3.108479]\n",
      "epoch:45 step:35390 [D loss: 0.341745, acc.: 84.38%] [G loss: 2.484313]\n",
      "epoch:45 step:35391 [D loss: 0.407815, acc.: 78.12%] [G loss: 2.617871]\n",
      "epoch:45 step:35392 [D loss: 0.340395, acc.: 82.03%] [G loss: 3.016110]\n",
      "epoch:45 step:35393 [D loss: 0.308465, acc.: 87.50%] [G loss: 2.782784]\n",
      "epoch:45 step:35394 [D loss: 0.345562, acc.: 85.16%] [G loss: 3.052034]\n",
      "epoch:45 step:35395 [D loss: 0.450013, acc.: 78.12%] [G loss: 2.898222]\n",
      "epoch:45 step:35396 [D loss: 0.277008, acc.: 87.50%] [G loss: 3.196433]\n",
      "epoch:45 step:35397 [D loss: 0.328179, acc.: 83.59%] [G loss: 3.396711]\n",
      "epoch:45 step:35398 [D loss: 0.343704, acc.: 81.25%] [G loss: 3.130595]\n",
      "epoch:45 step:35399 [D loss: 0.289229, acc.: 87.50%] [G loss: 2.804108]\n",
      "epoch:45 step:35400 [D loss: 0.262366, acc.: 90.62%] [G loss: 3.617867]\n",
      "epoch:45 step:35401 [D loss: 0.363649, acc.: 85.94%] [G loss: 2.866022]\n",
      "epoch:45 step:35402 [D loss: 0.358408, acc.: 85.16%] [G loss: 3.211459]\n",
      "epoch:45 step:35403 [D loss: 0.370902, acc.: 82.81%] [G loss: 2.697259]\n",
      "epoch:45 step:35404 [D loss: 0.374807, acc.: 82.03%] [G loss: 3.149295]\n",
      "epoch:45 step:35405 [D loss: 0.290078, acc.: 85.94%] [G loss: 3.155584]\n",
      "epoch:45 step:35406 [D loss: 0.374783, acc.: 85.94%] [G loss: 2.395507]\n",
      "epoch:45 step:35407 [D loss: 0.257448, acc.: 85.94%] [G loss: 2.643865]\n",
      "epoch:45 step:35408 [D loss: 0.284418, acc.: 88.28%] [G loss: 2.514171]\n",
      "epoch:45 step:35409 [D loss: 0.337269, acc.: 84.38%] [G loss: 2.595694]\n",
      "epoch:45 step:35410 [D loss: 0.312309, acc.: 87.50%] [G loss: 2.568972]\n",
      "epoch:45 step:35411 [D loss: 0.335086, acc.: 82.03%] [G loss: 3.105008]\n",
      "epoch:45 step:35412 [D loss: 0.229138, acc.: 91.41%] [G loss: 3.018384]\n",
      "epoch:45 step:35413 [D loss: 0.386663, acc.: 83.59%] [G loss: 2.839329]\n",
      "epoch:45 step:35414 [D loss: 0.281317, acc.: 87.50%] [G loss: 3.525054]\n",
      "epoch:45 step:35415 [D loss: 0.303080, acc.: 85.94%] [G loss: 3.671793]\n",
      "epoch:45 step:35416 [D loss: 0.347767, acc.: 83.59%] [G loss: 3.967588]\n",
      "epoch:45 step:35417 [D loss: 0.290053, acc.: 87.50%] [G loss: 3.316487]\n",
      "epoch:45 step:35418 [D loss: 0.264630, acc.: 88.28%] [G loss: 3.181081]\n",
      "epoch:45 step:35419 [D loss: 0.306707, acc.: 86.72%] [G loss: 2.828643]\n",
      "epoch:45 step:35420 [D loss: 0.382266, acc.: 82.03%] [G loss: 3.001757]\n",
      "epoch:45 step:35421 [D loss: 0.356744, acc.: 77.34%] [G loss: 2.999944]\n",
      "epoch:45 step:35422 [D loss: 0.450312, acc.: 78.91%] [G loss: 2.730471]\n",
      "epoch:45 step:35423 [D loss: 0.295643, acc.: 85.94%] [G loss: 2.439677]\n",
      "epoch:45 step:35424 [D loss: 0.267541, acc.: 89.06%] [G loss: 3.364849]\n",
      "epoch:45 step:35425 [D loss: 0.233080, acc.: 92.19%] [G loss: 3.191422]\n",
      "epoch:45 step:35426 [D loss: 0.237984, acc.: 89.84%] [G loss: 2.599098]\n",
      "epoch:45 step:35427 [D loss: 0.282862, acc.: 87.50%] [G loss: 2.415150]\n",
      "epoch:45 step:35428 [D loss: 0.287721, acc.: 88.28%] [G loss: 2.867832]\n",
      "epoch:45 step:35429 [D loss: 0.298892, acc.: 88.28%] [G loss: 2.631618]\n",
      "epoch:45 step:35430 [D loss: 0.250945, acc.: 88.28%] [G loss: 3.377816]\n",
      "epoch:45 step:35431 [D loss: 0.342342, acc.: 85.16%] [G loss: 4.745916]\n",
      "epoch:45 step:35432 [D loss: 0.296420, acc.: 86.72%] [G loss: 3.289733]\n",
      "epoch:45 step:35433 [D loss: 0.333814, acc.: 83.59%] [G loss: 4.645516]\n",
      "epoch:45 step:35434 [D loss: 0.361121, acc.: 80.47%] [G loss: 3.513383]\n",
      "epoch:45 step:35435 [D loss: 0.300574, acc.: 90.62%] [G loss: 3.175281]\n",
      "epoch:45 step:35436 [D loss: 0.213395, acc.: 92.97%] [G loss: 3.055103]\n",
      "epoch:45 step:35437 [D loss: 0.237412, acc.: 89.84%] [G loss: 3.806623]\n",
      "epoch:45 step:35438 [D loss: 0.381356, acc.: 84.38%] [G loss: 2.720808]\n",
      "epoch:45 step:35439 [D loss: 0.320365, acc.: 85.16%] [G loss: 2.772461]\n",
      "epoch:45 step:35440 [D loss: 0.369846, acc.: 83.59%] [G loss: 2.657182]\n",
      "epoch:45 step:35441 [D loss: 0.374110, acc.: 82.81%] [G loss: 2.973201]\n",
      "epoch:45 step:35442 [D loss: 0.297450, acc.: 85.94%] [G loss: 3.105762]\n",
      "epoch:45 step:35443 [D loss: 0.320801, acc.: 88.28%] [G loss: 2.696142]\n",
      "epoch:45 step:35444 [D loss: 0.286330, acc.: 89.06%] [G loss: 2.976198]\n",
      "epoch:45 step:35445 [D loss: 0.322568, acc.: 86.72%] [G loss: 2.943504]\n",
      "epoch:45 step:35446 [D loss: 0.329415, acc.: 82.81%] [G loss: 3.899589]\n",
      "epoch:45 step:35447 [D loss: 0.310570, acc.: 87.50%] [G loss: 2.800557]\n",
      "epoch:45 step:35448 [D loss: 0.233393, acc.: 89.06%] [G loss: 2.742280]\n",
      "epoch:45 step:35449 [D loss: 0.267039, acc.: 88.28%] [G loss: 3.563963]\n",
      "epoch:45 step:35450 [D loss: 0.319015, acc.: 82.81%] [G loss: 3.641123]\n",
      "epoch:45 step:35451 [D loss: 0.278615, acc.: 89.06%] [G loss: 3.520444]\n",
      "epoch:45 step:35452 [D loss: 0.216130, acc.: 90.62%] [G loss: 3.738955]\n",
      "epoch:45 step:35453 [D loss: 0.341949, acc.: 87.50%] [G loss: 3.432614]\n",
      "epoch:45 step:35454 [D loss: 0.431895, acc.: 81.25%] [G loss: 3.197821]\n",
      "epoch:45 step:35455 [D loss: 0.220068, acc.: 92.97%] [G loss: 3.866792]\n",
      "epoch:45 step:35456 [D loss: 0.308734, acc.: 82.03%] [G loss: 5.016847]\n",
      "epoch:45 step:35457 [D loss: 0.631823, acc.: 67.19%] [G loss: 4.741918]\n",
      "epoch:45 step:35458 [D loss: 0.751758, acc.: 71.09%] [G loss: 5.691535]\n",
      "epoch:45 step:35459 [D loss: 0.634086, acc.: 76.56%] [G loss: 5.815787]\n",
      "epoch:45 step:35460 [D loss: 0.462682, acc.: 73.44%] [G loss: 3.346533]\n",
      "epoch:45 step:35461 [D loss: 0.360048, acc.: 84.38%] [G loss: 5.996023]\n",
      "epoch:45 step:35462 [D loss: 0.474189, acc.: 81.25%] [G loss: 4.584245]\n",
      "epoch:45 step:35463 [D loss: 0.387578, acc.: 82.81%] [G loss: 5.686374]\n",
      "epoch:45 step:35464 [D loss: 0.226976, acc.: 89.84%] [G loss: 5.518425]\n",
      "epoch:45 step:35465 [D loss: 0.362984, acc.: 82.81%] [G loss: 4.413557]\n",
      "epoch:45 step:35466 [D loss: 0.268650, acc.: 86.72%] [G loss: 3.488316]\n",
      "epoch:45 step:35467 [D loss: 0.354959, acc.: 82.81%] [G loss: 3.423557]\n",
      "epoch:45 step:35468 [D loss: 0.299021, acc.: 84.38%] [G loss: 3.434645]\n",
      "epoch:45 step:35469 [D loss: 0.352458, acc.: 83.59%] [G loss: 2.918303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35470 [D loss: 0.402296, acc.: 81.25%] [G loss: 3.377124]\n",
      "epoch:45 step:35471 [D loss: 0.276217, acc.: 87.50%] [G loss: 3.653423]\n",
      "epoch:45 step:35472 [D loss: 0.370712, acc.: 80.47%] [G loss: 2.872910]\n",
      "epoch:45 step:35473 [D loss: 0.248234, acc.: 88.28%] [G loss: 3.364920]\n",
      "epoch:45 step:35474 [D loss: 0.289627, acc.: 83.59%] [G loss: 2.924791]\n",
      "epoch:45 step:35475 [D loss: 0.312235, acc.: 85.16%] [G loss: 2.842617]\n",
      "epoch:45 step:35476 [D loss: 0.328445, acc.: 85.16%] [G loss: 4.486254]\n",
      "epoch:45 step:35477 [D loss: 0.298836, acc.: 85.94%] [G loss: 3.264525]\n",
      "epoch:45 step:35478 [D loss: 0.301921, acc.: 87.50%] [G loss: 4.714588]\n",
      "epoch:45 step:35479 [D loss: 0.287170, acc.: 83.59%] [G loss: 5.177669]\n",
      "epoch:45 step:35480 [D loss: 0.280378, acc.: 85.94%] [G loss: 4.163150]\n",
      "epoch:45 step:35481 [D loss: 0.220534, acc.: 91.41%] [G loss: 3.688562]\n",
      "epoch:45 step:35482 [D loss: 0.370005, acc.: 82.03%] [G loss: 2.954037]\n",
      "epoch:45 step:35483 [D loss: 0.299521, acc.: 87.50%] [G loss: 4.071553]\n",
      "epoch:45 step:35484 [D loss: 0.247938, acc.: 87.50%] [G loss: 3.073432]\n",
      "epoch:45 step:35485 [D loss: 0.316279, acc.: 85.94%] [G loss: 3.179897]\n",
      "epoch:45 step:35486 [D loss: 0.269968, acc.: 89.84%] [G loss: 3.104628]\n",
      "epoch:45 step:35487 [D loss: 0.164680, acc.: 92.19%] [G loss: 2.708015]\n",
      "epoch:45 step:35488 [D loss: 0.287102, acc.: 88.28%] [G loss: 3.297591]\n",
      "epoch:45 step:35489 [D loss: 0.304598, acc.: 85.16%] [G loss: 3.661947]\n",
      "epoch:45 step:35490 [D loss: 0.228567, acc.: 91.41%] [G loss: 3.790123]\n",
      "epoch:45 step:35491 [D loss: 0.306622, acc.: 86.72%] [G loss: 3.127458]\n",
      "epoch:45 step:35492 [D loss: 0.298205, acc.: 87.50%] [G loss: 2.911662]\n",
      "epoch:45 step:35493 [D loss: 0.260623, acc.: 89.06%] [G loss: 3.648606]\n",
      "epoch:45 step:35494 [D loss: 0.268429, acc.: 89.06%] [G loss: 3.305715]\n",
      "epoch:45 step:35495 [D loss: 0.391596, acc.: 81.25%] [G loss: 2.523957]\n",
      "epoch:45 step:35496 [D loss: 0.310935, acc.: 89.06%] [G loss: 3.176893]\n",
      "epoch:45 step:35497 [D loss: 0.355571, acc.: 85.16%] [G loss: 2.546537]\n",
      "epoch:45 step:35498 [D loss: 0.344169, acc.: 83.59%] [G loss: 2.768973]\n",
      "epoch:45 step:35499 [D loss: 0.248076, acc.: 88.28%] [G loss: 3.457992]\n",
      "epoch:45 step:35500 [D loss: 0.264319, acc.: 88.28%] [G loss: 3.404199]\n",
      "epoch:45 step:35501 [D loss: 0.338938, acc.: 80.47%] [G loss: 2.856153]\n",
      "epoch:45 step:35502 [D loss: 0.303360, acc.: 86.72%] [G loss: 3.009531]\n",
      "epoch:45 step:35503 [D loss: 0.256512, acc.: 90.62%] [G loss: 2.886537]\n",
      "epoch:45 step:35504 [D loss: 0.249039, acc.: 90.62%] [G loss: 3.276230]\n",
      "epoch:45 step:35505 [D loss: 0.325843, acc.: 81.25%] [G loss: 3.250767]\n",
      "epoch:45 step:35506 [D loss: 0.344346, acc.: 87.50%] [G loss: 2.536770]\n",
      "epoch:45 step:35507 [D loss: 0.249942, acc.: 90.62%] [G loss: 2.705745]\n",
      "epoch:45 step:35508 [D loss: 0.339773, acc.: 84.38%] [G loss: 2.513438]\n",
      "epoch:45 step:35509 [D loss: 0.314514, acc.: 87.50%] [G loss: 3.235444]\n",
      "epoch:45 step:35510 [D loss: 0.312694, acc.: 86.72%] [G loss: 2.546703]\n",
      "epoch:45 step:35511 [D loss: 0.337424, acc.: 82.03%] [G loss: 2.268012]\n",
      "epoch:45 step:35512 [D loss: 0.314473, acc.: 85.94%] [G loss: 2.897031]\n",
      "epoch:45 step:35513 [D loss: 0.385746, acc.: 83.59%] [G loss: 2.187734]\n",
      "epoch:45 step:35514 [D loss: 0.321195, acc.: 86.72%] [G loss: 2.262770]\n",
      "epoch:45 step:35515 [D loss: 0.336484, acc.: 86.72%] [G loss: 2.984649]\n",
      "epoch:45 step:35516 [D loss: 0.230295, acc.: 89.84%] [G loss: 3.001040]\n",
      "epoch:45 step:35517 [D loss: 0.358065, acc.: 86.72%] [G loss: 3.254000]\n",
      "epoch:45 step:35518 [D loss: 0.397143, acc.: 82.81%] [G loss: 3.939789]\n",
      "epoch:45 step:35519 [D loss: 0.303786, acc.: 87.50%] [G loss: 3.076692]\n",
      "epoch:45 step:35520 [D loss: 0.233709, acc.: 90.62%] [G loss: 3.988606]\n",
      "epoch:45 step:35521 [D loss: 0.263252, acc.: 87.50%] [G loss: 2.859939]\n",
      "epoch:45 step:35522 [D loss: 0.241690, acc.: 91.41%] [G loss: 3.645437]\n",
      "epoch:45 step:35523 [D loss: 0.360075, acc.: 87.50%] [G loss: 2.649856]\n",
      "epoch:45 step:35524 [D loss: 0.298464, acc.: 84.38%] [G loss: 3.235784]\n",
      "epoch:45 step:35525 [D loss: 0.381838, acc.: 85.16%] [G loss: 2.942676]\n",
      "epoch:45 step:35526 [D loss: 0.295845, acc.: 85.16%] [G loss: 3.470722]\n",
      "epoch:45 step:35527 [D loss: 0.287729, acc.: 89.06%] [G loss: 2.947611]\n",
      "epoch:45 step:35528 [D loss: 0.266858, acc.: 92.97%] [G loss: 2.997833]\n",
      "epoch:45 step:35529 [D loss: 0.221931, acc.: 92.97%] [G loss: 3.857653]\n",
      "epoch:45 step:35530 [D loss: 0.318079, acc.: 86.72%] [G loss: 3.590611]\n",
      "epoch:45 step:35531 [D loss: 0.358887, acc.: 85.94%] [G loss: 5.951805]\n",
      "epoch:45 step:35532 [D loss: 0.402088, acc.: 82.03%] [G loss: 2.955019]\n",
      "epoch:45 step:35533 [D loss: 0.227285, acc.: 90.62%] [G loss: 3.279121]\n",
      "epoch:45 step:35534 [D loss: 0.288715, acc.: 86.72%] [G loss: 3.633088]\n",
      "epoch:45 step:35535 [D loss: 0.215365, acc.: 91.41%] [G loss: 3.432159]\n",
      "epoch:45 step:35536 [D loss: 0.270234, acc.: 87.50%] [G loss: 3.398849]\n",
      "epoch:45 step:35537 [D loss: 0.262270, acc.: 89.06%] [G loss: 3.153774]\n",
      "epoch:45 step:35538 [D loss: 0.300593, acc.: 89.06%] [G loss: 3.283183]\n",
      "epoch:45 step:35539 [D loss: 0.310462, acc.: 85.16%] [G loss: 2.600031]\n",
      "epoch:45 step:35540 [D loss: 0.339754, acc.: 84.38%] [G loss: 3.748487]\n",
      "epoch:45 step:35541 [D loss: 0.238054, acc.: 89.84%] [G loss: 2.838512]\n",
      "epoch:45 step:35542 [D loss: 0.309993, acc.: 88.28%] [G loss: 3.970701]\n",
      "epoch:45 step:35543 [D loss: 0.265251, acc.: 89.84%] [G loss: 3.218608]\n",
      "epoch:45 step:35544 [D loss: 0.408967, acc.: 83.59%] [G loss: 3.061778]\n",
      "epoch:45 step:35545 [D loss: 0.260183, acc.: 89.06%] [G loss: 3.025434]\n",
      "epoch:45 step:35546 [D loss: 0.227569, acc.: 91.41%] [G loss: 3.596442]\n",
      "epoch:45 step:35547 [D loss: 0.236137, acc.: 92.19%] [G loss: 2.917376]\n",
      "epoch:45 step:35548 [D loss: 0.352759, acc.: 82.81%] [G loss: 3.205808]\n",
      "epoch:45 step:35549 [D loss: 0.269761, acc.: 88.28%] [G loss: 2.573341]\n",
      "epoch:45 step:35550 [D loss: 0.274488, acc.: 87.50%] [G loss: 2.844880]\n",
      "epoch:45 step:35551 [D loss: 0.300278, acc.: 86.72%] [G loss: 2.713886]\n",
      "epoch:45 step:35552 [D loss: 0.324302, acc.: 89.06%] [G loss: 2.741847]\n",
      "epoch:45 step:35553 [D loss: 0.381610, acc.: 86.72%] [G loss: 3.196840]\n",
      "epoch:45 step:35554 [D loss: 0.369125, acc.: 85.16%] [G loss: 3.103461]\n",
      "epoch:45 step:35555 [D loss: 0.330639, acc.: 86.72%] [G loss: 2.284995]\n",
      "epoch:45 step:35556 [D loss: 0.329503, acc.: 87.50%] [G loss: 3.517924]\n",
      "epoch:45 step:35557 [D loss: 0.346689, acc.: 81.25%] [G loss: 3.361908]\n",
      "epoch:45 step:35558 [D loss: 0.325635, acc.: 87.50%] [G loss: 2.701352]\n",
      "epoch:45 step:35559 [D loss: 0.331788, acc.: 85.94%] [G loss: 3.672223]\n",
      "epoch:45 step:35560 [D loss: 0.304501, acc.: 85.16%] [G loss: 4.053636]\n",
      "epoch:45 step:35561 [D loss: 0.250901, acc.: 89.84%] [G loss: 3.796814]\n",
      "epoch:45 step:35562 [D loss: 0.312411, acc.: 85.94%] [G loss: 3.721689]\n",
      "epoch:45 step:35563 [D loss: 0.214018, acc.: 89.06%] [G loss: 4.813154]\n",
      "epoch:45 step:35564 [D loss: 0.404172, acc.: 82.81%] [G loss: 4.232321]\n",
      "epoch:45 step:35565 [D loss: 0.248324, acc.: 88.28%] [G loss: 3.267063]\n",
      "epoch:45 step:35566 [D loss: 0.258714, acc.: 89.06%] [G loss: 4.052612]\n",
      "epoch:45 step:35567 [D loss: 0.203813, acc.: 93.75%] [G loss: 4.113544]\n",
      "epoch:45 step:35568 [D loss: 0.310896, acc.: 86.72%] [G loss: 4.533921]\n",
      "epoch:45 step:35569 [D loss: 0.259380, acc.: 89.84%] [G loss: 3.441088]\n",
      "epoch:45 step:35570 [D loss: 0.270375, acc.: 92.19%] [G loss: 3.725492]\n",
      "epoch:45 step:35571 [D loss: 0.201964, acc.: 92.97%] [G loss: 4.267983]\n",
      "epoch:45 step:35572 [D loss: 0.194359, acc.: 92.19%] [G loss: 3.705240]\n",
      "epoch:45 step:35573 [D loss: 0.341063, acc.: 81.25%] [G loss: 3.592355]\n",
      "epoch:45 step:35574 [D loss: 0.426928, acc.: 78.91%] [G loss: 5.013330]\n",
      "epoch:45 step:35575 [D loss: 0.316466, acc.: 84.38%] [G loss: 4.784602]\n",
      "epoch:45 step:35576 [D loss: 0.253658, acc.: 89.06%] [G loss: 5.412124]\n",
      "epoch:45 step:35577 [D loss: 0.347764, acc.: 85.94%] [G loss: 4.260942]\n",
      "epoch:45 step:35578 [D loss: 0.250701, acc.: 87.50%] [G loss: 3.410814]\n",
      "epoch:45 step:35579 [D loss: 0.308268, acc.: 89.06%] [G loss: 2.737800]\n",
      "epoch:45 step:35580 [D loss: 0.388966, acc.: 82.81%] [G loss: 3.594359]\n",
      "epoch:45 step:35581 [D loss: 0.331766, acc.: 86.72%] [G loss: 3.603020]\n",
      "epoch:45 step:35582 [D loss: 0.328198, acc.: 89.06%] [G loss: 3.486781]\n",
      "epoch:45 step:35583 [D loss: 0.390270, acc.: 81.25%] [G loss: 3.154603]\n",
      "epoch:45 step:35584 [D loss: 0.292452, acc.: 89.06%] [G loss: 2.957926]\n",
      "epoch:45 step:35585 [D loss: 0.303430, acc.: 83.59%] [G loss: 3.263271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35586 [D loss: 0.302558, acc.: 88.28%] [G loss: 2.670276]\n",
      "epoch:45 step:35587 [D loss: 0.206286, acc.: 92.19%] [G loss: 4.075662]\n",
      "epoch:45 step:35588 [D loss: 0.318169, acc.: 85.94%] [G loss: 3.445824]\n",
      "epoch:45 step:35589 [D loss: 0.341269, acc.: 85.16%] [G loss: 4.377597]\n",
      "epoch:45 step:35590 [D loss: 0.324444, acc.: 86.72%] [G loss: 3.723841]\n",
      "epoch:45 step:35591 [D loss: 0.335549, acc.: 86.72%] [G loss: 3.843035]\n",
      "epoch:45 step:35592 [D loss: 0.372591, acc.: 83.59%] [G loss: 2.861788]\n",
      "epoch:45 step:35593 [D loss: 0.312514, acc.: 87.50%] [G loss: 3.286140]\n",
      "epoch:45 step:35594 [D loss: 0.314171, acc.: 84.38%] [G loss: 2.646483]\n",
      "epoch:45 step:35595 [D loss: 0.238665, acc.: 90.62%] [G loss: 4.076685]\n",
      "epoch:45 step:35596 [D loss: 0.305002, acc.: 85.16%] [G loss: 3.210989]\n",
      "epoch:45 step:35597 [D loss: 0.332362, acc.: 84.38%] [G loss: 3.463857]\n",
      "epoch:45 step:35598 [D loss: 0.236974, acc.: 92.19%] [G loss: 3.282381]\n",
      "epoch:45 step:35599 [D loss: 0.316327, acc.: 85.16%] [G loss: 3.203723]\n",
      "epoch:45 step:35600 [D loss: 0.241522, acc.: 89.06%] [G loss: 3.675322]\n",
      "epoch:45 step:35601 [D loss: 0.308394, acc.: 86.72%] [G loss: 3.476596]\n",
      "epoch:45 step:35602 [D loss: 0.272225, acc.: 89.06%] [G loss: 2.816746]\n",
      "epoch:45 step:35603 [D loss: 0.272022, acc.: 88.28%] [G loss: 3.521807]\n",
      "epoch:45 step:35604 [D loss: 0.358514, acc.: 82.81%] [G loss: 3.425867]\n",
      "epoch:45 step:35605 [D loss: 0.315694, acc.: 85.94%] [G loss: 3.188767]\n",
      "epoch:45 step:35606 [D loss: 0.376327, acc.: 82.81%] [G loss: 3.261218]\n",
      "epoch:45 step:35607 [D loss: 0.192534, acc.: 92.19%] [G loss: 3.294004]\n",
      "epoch:45 step:35608 [D loss: 0.390817, acc.: 82.03%] [G loss: 2.962060]\n",
      "epoch:45 step:35609 [D loss: 0.304883, acc.: 90.62%] [G loss: 2.521951]\n",
      "epoch:45 step:35610 [D loss: 0.341974, acc.: 86.72%] [G loss: 2.610402]\n",
      "epoch:45 step:35611 [D loss: 0.203844, acc.: 92.97%] [G loss: 2.732861]\n",
      "epoch:45 step:35612 [D loss: 0.375135, acc.: 82.03%] [G loss: 2.530530]\n",
      "epoch:45 step:35613 [D loss: 0.280922, acc.: 88.28%] [G loss: 3.187531]\n",
      "epoch:45 step:35614 [D loss: 0.368693, acc.: 88.28%] [G loss: 2.582055]\n",
      "epoch:45 step:35615 [D loss: 0.237103, acc.: 90.62%] [G loss: 2.832042]\n",
      "epoch:45 step:35616 [D loss: 0.284660, acc.: 86.72%] [G loss: 4.630935]\n",
      "epoch:45 step:35617 [D loss: 0.360950, acc.: 85.94%] [G loss: 2.669032]\n",
      "epoch:45 step:35618 [D loss: 0.407641, acc.: 78.12%] [G loss: 3.366748]\n",
      "epoch:45 step:35619 [D loss: 0.345288, acc.: 83.59%] [G loss: 3.634593]\n",
      "epoch:45 step:35620 [D loss: 0.306022, acc.: 87.50%] [G loss: 2.328602]\n",
      "epoch:45 step:35621 [D loss: 0.357775, acc.: 86.72%] [G loss: 2.690895]\n",
      "epoch:45 step:35622 [D loss: 0.269547, acc.: 88.28%] [G loss: 3.097652]\n",
      "epoch:45 step:35623 [D loss: 0.276535, acc.: 88.28%] [G loss: 2.637028]\n",
      "epoch:45 step:35624 [D loss: 0.411752, acc.: 82.81%] [G loss: 3.096241]\n",
      "epoch:45 step:35625 [D loss: 0.323199, acc.: 85.94%] [G loss: 3.205674]\n",
      "epoch:45 step:35626 [D loss: 0.323174, acc.: 84.38%] [G loss: 3.006355]\n",
      "epoch:45 step:35627 [D loss: 0.384685, acc.: 81.25%] [G loss: 2.822917]\n",
      "epoch:45 step:35628 [D loss: 0.316821, acc.: 83.59%] [G loss: 3.371125]\n",
      "epoch:45 step:35629 [D loss: 0.344468, acc.: 85.16%] [G loss: 2.707441]\n",
      "epoch:45 step:35630 [D loss: 0.276215, acc.: 89.84%] [G loss: 3.312791]\n",
      "epoch:45 step:35631 [D loss: 0.278785, acc.: 89.06%] [G loss: 2.805820]\n",
      "epoch:45 step:35632 [D loss: 0.328116, acc.: 84.38%] [G loss: 2.847703]\n",
      "epoch:45 step:35633 [D loss: 0.257490, acc.: 87.50%] [G loss: 3.642790]\n",
      "epoch:45 step:35634 [D loss: 0.283657, acc.: 85.94%] [G loss: 2.666584]\n",
      "epoch:45 step:35635 [D loss: 0.248978, acc.: 88.28%] [G loss: 4.966220]\n",
      "epoch:45 step:35636 [D loss: 0.366927, acc.: 78.91%] [G loss: 3.802830]\n",
      "epoch:45 step:35637 [D loss: 0.368767, acc.: 82.03%] [G loss: 3.509987]\n",
      "epoch:45 step:35638 [D loss: 0.253246, acc.: 87.50%] [G loss: 2.802777]\n",
      "epoch:45 step:35639 [D loss: 0.291771, acc.: 84.38%] [G loss: 4.000738]\n",
      "epoch:45 step:35640 [D loss: 0.203856, acc.: 91.41%] [G loss: 4.444778]\n",
      "epoch:45 step:35641 [D loss: 0.254201, acc.: 89.84%] [G loss: 3.307206]\n",
      "epoch:45 step:35642 [D loss: 0.305299, acc.: 89.06%] [G loss: 3.992822]\n",
      "epoch:45 step:35643 [D loss: 0.229593, acc.: 89.06%] [G loss: 3.222186]\n",
      "epoch:45 step:35644 [D loss: 0.329240, acc.: 84.38%] [G loss: 3.769142]\n",
      "epoch:45 step:35645 [D loss: 0.371943, acc.: 85.94%] [G loss: 3.462533]\n",
      "epoch:45 step:35646 [D loss: 0.339969, acc.: 83.59%] [G loss: 3.072230]\n",
      "epoch:45 step:35647 [D loss: 0.335678, acc.: 85.16%] [G loss: 2.902810]\n",
      "epoch:45 step:35648 [D loss: 0.312077, acc.: 87.50%] [G loss: 3.294779]\n",
      "epoch:45 step:35649 [D loss: 0.309907, acc.: 82.81%] [G loss: 3.671048]\n",
      "epoch:45 step:35650 [D loss: 0.276881, acc.: 88.28%] [G loss: 3.894448]\n",
      "epoch:45 step:35651 [D loss: 0.390360, acc.: 82.03%] [G loss: 4.462033]\n",
      "epoch:45 step:35652 [D loss: 0.307200, acc.: 85.16%] [G loss: 4.500169]\n",
      "epoch:45 step:35653 [D loss: 0.282265, acc.: 86.72%] [G loss: 4.006252]\n",
      "epoch:45 step:35654 [D loss: 0.315841, acc.: 89.06%] [G loss: 3.463945]\n",
      "epoch:45 step:35655 [D loss: 0.388627, acc.: 82.03%] [G loss: 3.220864]\n",
      "epoch:45 step:35656 [D loss: 0.366940, acc.: 79.69%] [G loss: 3.787837]\n",
      "epoch:45 step:35657 [D loss: 0.255150, acc.: 92.19%] [G loss: 4.014586]\n",
      "epoch:45 step:35658 [D loss: 0.334044, acc.: 85.16%] [G loss: 4.456563]\n",
      "epoch:45 step:35659 [D loss: 0.413001, acc.: 82.03%] [G loss: 3.160193]\n",
      "epoch:45 step:35660 [D loss: 0.330151, acc.: 85.16%] [G loss: 2.777184]\n",
      "epoch:45 step:35661 [D loss: 0.288109, acc.: 85.16%] [G loss: 2.773310]\n",
      "epoch:45 step:35662 [D loss: 0.214124, acc.: 89.84%] [G loss: 3.756637]\n",
      "epoch:45 step:35663 [D loss: 0.335117, acc.: 87.50%] [G loss: 3.482705]\n",
      "epoch:45 step:35664 [D loss: 0.300808, acc.: 85.16%] [G loss: 2.687320]\n",
      "epoch:45 step:35665 [D loss: 0.344435, acc.: 86.72%] [G loss: 2.696254]\n",
      "epoch:45 step:35666 [D loss: 0.273386, acc.: 92.97%] [G loss: 3.428400]\n",
      "epoch:45 step:35667 [D loss: 0.341843, acc.: 83.59%] [G loss: 3.297158]\n",
      "epoch:45 step:35668 [D loss: 0.263538, acc.: 88.28%] [G loss: 4.205928]\n",
      "epoch:45 step:35669 [D loss: 0.310304, acc.: 85.16%] [G loss: 3.795950]\n",
      "epoch:45 step:35670 [D loss: 0.378296, acc.: 85.16%] [G loss: 4.058745]\n",
      "epoch:45 step:35671 [D loss: 0.461739, acc.: 81.25%] [G loss: 3.388748]\n",
      "epoch:45 step:35672 [D loss: 0.427070, acc.: 80.47%] [G loss: 2.503563]\n",
      "epoch:45 step:35673 [D loss: 0.350749, acc.: 84.38%] [G loss: 3.732621]\n",
      "epoch:45 step:35674 [D loss: 0.509646, acc.: 74.22%] [G loss: 5.914101]\n",
      "epoch:45 step:35675 [D loss: 0.707780, acc.: 75.78%] [G loss: 6.109248]\n",
      "epoch:45 step:35676 [D loss: 1.674391, acc.: 69.53%] [G loss: 8.115883]\n",
      "epoch:45 step:35677 [D loss: 1.895251, acc.: 54.69%] [G loss: 4.564476]\n",
      "epoch:45 step:35678 [D loss: 1.053199, acc.: 75.00%] [G loss: 4.493666]\n",
      "epoch:45 step:35679 [D loss: 0.439529, acc.: 79.69%] [G loss: 3.837869]\n",
      "epoch:45 step:35680 [D loss: 0.392340, acc.: 82.81%] [G loss: 3.950034]\n",
      "epoch:45 step:35681 [D loss: 0.288701, acc.: 89.84%] [G loss: 3.750088]\n",
      "epoch:45 step:35682 [D loss: 0.477710, acc.: 78.91%] [G loss: 5.088945]\n",
      "epoch:45 step:35683 [D loss: 0.368828, acc.: 84.38%] [G loss: 3.013798]\n",
      "epoch:45 step:35684 [D loss: 0.356619, acc.: 82.03%] [G loss: 3.664991]\n",
      "epoch:45 step:35685 [D loss: 0.294124, acc.: 88.28%] [G loss: 3.675987]\n",
      "epoch:45 step:35686 [D loss: 0.336546, acc.: 87.50%] [G loss: 3.153070]\n",
      "epoch:45 step:35687 [D loss: 0.316467, acc.: 86.72%] [G loss: 2.959581]\n",
      "epoch:45 step:35688 [D loss: 0.306463, acc.: 84.38%] [G loss: 3.426172]\n",
      "epoch:45 step:35689 [D loss: 0.307364, acc.: 87.50%] [G loss: 3.280995]\n",
      "epoch:45 step:35690 [D loss: 0.318574, acc.: 87.50%] [G loss: 2.588254]\n",
      "epoch:45 step:35691 [D loss: 0.273573, acc.: 88.28%] [G loss: 2.612797]\n",
      "epoch:45 step:35692 [D loss: 0.370965, acc.: 82.81%] [G loss: 2.555510]\n",
      "epoch:45 step:35693 [D loss: 0.276062, acc.: 89.84%] [G loss: 2.377400]\n",
      "epoch:45 step:35694 [D loss: 0.376470, acc.: 82.81%] [G loss: 2.838013]\n",
      "epoch:45 step:35695 [D loss: 0.299489, acc.: 91.41%] [G loss: 2.833783]\n",
      "epoch:45 step:35696 [D loss: 0.253386, acc.: 88.28%] [G loss: 2.558428]\n",
      "epoch:45 step:35697 [D loss: 0.313341, acc.: 86.72%] [G loss: 2.637531]\n",
      "epoch:45 step:35698 [D loss: 0.257916, acc.: 92.19%] [G loss: 2.585825]\n",
      "epoch:45 step:35699 [D loss: 0.331181, acc.: 87.50%] [G loss: 3.073705]\n",
      "epoch:45 step:35700 [D loss: 0.358394, acc.: 79.69%] [G loss: 2.142037]\n",
      "epoch:45 step:35701 [D loss: 0.348019, acc.: 83.59%] [G loss: 3.051430]\n",
      "epoch:45 step:35702 [D loss: 0.297613, acc.: 89.06%] [G loss: 3.364476]\n",
      "epoch:45 step:35703 [D loss: 0.441016, acc.: 81.25%] [G loss: 4.063929]\n",
      "epoch:45 step:35704 [D loss: 0.360650, acc.: 85.94%] [G loss: 3.819402]\n",
      "epoch:45 step:35705 [D loss: 0.318087, acc.: 85.94%] [G loss: 3.133826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35706 [D loss: 0.313714, acc.: 84.38%] [G loss: 2.935154]\n",
      "epoch:45 step:35707 [D loss: 0.296425, acc.: 87.50%] [G loss: 3.941373]\n",
      "epoch:45 step:35708 [D loss: 0.225136, acc.: 91.41%] [G loss: 2.893147]\n",
      "epoch:45 step:35709 [D loss: 0.376056, acc.: 83.59%] [G loss: 3.170703]\n",
      "epoch:45 step:35710 [D loss: 0.328909, acc.: 85.94%] [G loss: 3.176577]\n",
      "epoch:45 step:35711 [D loss: 0.307554, acc.: 87.50%] [G loss: 2.632357]\n",
      "epoch:45 step:35712 [D loss: 0.359726, acc.: 82.81%] [G loss: 3.679339]\n",
      "epoch:45 step:35713 [D loss: 0.373977, acc.: 80.47%] [G loss: 2.597574]\n",
      "epoch:45 step:35714 [D loss: 0.271277, acc.: 85.94%] [G loss: 2.901459]\n",
      "epoch:45 step:35715 [D loss: 0.338393, acc.: 85.94%] [G loss: 2.999678]\n",
      "epoch:45 step:35716 [D loss: 0.268860, acc.: 88.28%] [G loss: 3.157437]\n",
      "epoch:45 step:35717 [D loss: 0.364264, acc.: 83.59%] [G loss: 2.684005]\n",
      "epoch:45 step:35718 [D loss: 0.239632, acc.: 90.62%] [G loss: 2.880480]\n",
      "epoch:45 step:35719 [D loss: 0.355791, acc.: 82.03%] [G loss: 3.154152]\n",
      "epoch:45 step:35720 [D loss: 0.412000, acc.: 84.38%] [G loss: 2.181051]\n",
      "epoch:45 step:35721 [D loss: 0.357425, acc.: 83.59%] [G loss: 2.751311]\n",
      "epoch:45 step:35722 [D loss: 0.355452, acc.: 84.38%] [G loss: 2.565099]\n",
      "epoch:45 step:35723 [D loss: 0.304540, acc.: 86.72%] [G loss: 3.083969]\n",
      "epoch:45 step:35724 [D loss: 0.294572, acc.: 88.28%] [G loss: 2.915820]\n",
      "epoch:45 step:35725 [D loss: 0.352142, acc.: 84.38%] [G loss: 2.861776]\n",
      "epoch:45 step:35726 [D loss: 0.236530, acc.: 92.97%] [G loss: 3.129575]\n",
      "epoch:45 step:35727 [D loss: 0.343554, acc.: 84.38%] [G loss: 2.860914]\n",
      "epoch:45 step:35728 [D loss: 0.298998, acc.: 87.50%] [G loss: 3.611121]\n",
      "epoch:45 step:35729 [D loss: 0.340137, acc.: 83.59%] [G loss: 3.310418]\n",
      "epoch:45 step:35730 [D loss: 0.268449, acc.: 87.50%] [G loss: 3.843393]\n",
      "epoch:45 step:35731 [D loss: 0.301438, acc.: 85.94%] [G loss: 3.115940]\n",
      "epoch:45 step:35732 [D loss: 0.311482, acc.: 87.50%] [G loss: 2.820045]\n",
      "epoch:45 step:35733 [D loss: 0.254163, acc.: 89.84%] [G loss: 3.499576]\n",
      "epoch:45 step:35734 [D loss: 0.378174, acc.: 79.69%] [G loss: 2.118879]\n",
      "epoch:45 step:35735 [D loss: 0.418621, acc.: 82.03%] [G loss: 2.512146]\n",
      "epoch:45 step:35736 [D loss: 0.450514, acc.: 78.12%] [G loss: 2.572703]\n",
      "epoch:45 step:35737 [D loss: 0.389174, acc.: 80.47%] [G loss: 2.399442]\n",
      "epoch:45 step:35738 [D loss: 0.300953, acc.: 85.94%] [G loss: 3.456564]\n",
      "epoch:45 step:35739 [D loss: 0.333365, acc.: 89.06%] [G loss: 4.260599]\n",
      "epoch:45 step:35740 [D loss: 0.429805, acc.: 79.69%] [G loss: 2.999047]\n",
      "epoch:45 step:35741 [D loss: 0.315498, acc.: 83.59%] [G loss: 2.929990]\n",
      "epoch:45 step:35742 [D loss: 0.253678, acc.: 89.06%] [G loss: 3.141539]\n",
      "epoch:45 step:35743 [D loss: 0.278530, acc.: 89.84%] [G loss: 3.001067]\n",
      "epoch:45 step:35744 [D loss: 0.245137, acc.: 89.84%] [G loss: 3.355755]\n",
      "epoch:45 step:35745 [D loss: 0.261015, acc.: 89.06%] [G loss: 2.567792]\n",
      "epoch:45 step:35746 [D loss: 0.289852, acc.: 86.72%] [G loss: 3.268592]\n",
      "epoch:45 step:35747 [D loss: 0.365323, acc.: 82.03%] [G loss: 2.873380]\n",
      "epoch:45 step:35748 [D loss: 0.324764, acc.: 85.94%] [G loss: 3.214314]\n",
      "epoch:45 step:35749 [D loss: 0.314270, acc.: 85.16%] [G loss: 3.023432]\n",
      "epoch:45 step:35750 [D loss: 0.341314, acc.: 85.94%] [G loss: 3.234862]\n",
      "epoch:45 step:35751 [D loss: 0.471838, acc.: 75.78%] [G loss: 3.036235]\n",
      "epoch:45 step:35752 [D loss: 0.349417, acc.: 85.94%] [G loss: 2.708506]\n",
      "epoch:45 step:35753 [D loss: 0.318326, acc.: 85.94%] [G loss: 3.076693]\n",
      "epoch:45 step:35754 [D loss: 0.358081, acc.: 82.81%] [G loss: 3.359013]\n",
      "epoch:45 step:35755 [D loss: 0.249300, acc.: 91.41%] [G loss: 3.190804]\n",
      "epoch:45 step:35756 [D loss: 0.335883, acc.: 84.38%] [G loss: 3.615670]\n",
      "epoch:45 step:35757 [D loss: 0.384720, acc.: 85.16%] [G loss: 2.904035]\n",
      "epoch:45 step:35758 [D loss: 0.392665, acc.: 79.69%] [G loss: 3.614532]\n",
      "epoch:45 step:35759 [D loss: 0.374430, acc.: 83.59%] [G loss: 3.047925]\n",
      "epoch:45 step:35760 [D loss: 0.243310, acc.: 88.28%] [G loss: 2.683397]\n",
      "epoch:45 step:35761 [D loss: 0.301175, acc.: 86.72%] [G loss: 3.682249]\n",
      "epoch:45 step:35762 [D loss: 0.210316, acc.: 90.62%] [G loss: 3.124310]\n",
      "epoch:45 step:35763 [D loss: 0.288953, acc.: 85.94%] [G loss: 4.256886]\n",
      "epoch:45 step:35764 [D loss: 0.287142, acc.: 86.72%] [G loss: 2.458098]\n",
      "epoch:45 step:35765 [D loss: 0.281799, acc.: 85.94%] [G loss: 3.495471]\n",
      "epoch:45 step:35766 [D loss: 0.222189, acc.: 89.84%] [G loss: 3.455845]\n",
      "epoch:45 step:35767 [D loss: 0.258041, acc.: 89.06%] [G loss: 2.625511]\n",
      "epoch:45 step:35768 [D loss: 0.284033, acc.: 88.28%] [G loss: 3.942779]\n",
      "epoch:45 step:35769 [D loss: 0.279445, acc.: 87.50%] [G loss: 2.762999]\n",
      "epoch:45 step:35770 [D loss: 0.456910, acc.: 77.34%] [G loss: 2.393389]\n",
      "epoch:45 step:35771 [D loss: 0.266442, acc.: 85.94%] [G loss: 2.925242]\n",
      "epoch:45 step:35772 [D loss: 0.343068, acc.: 84.38%] [G loss: 2.635954]\n",
      "epoch:45 step:35773 [D loss: 0.453124, acc.: 78.91%] [G loss: 3.367355]\n",
      "epoch:45 step:35774 [D loss: 0.344756, acc.: 86.72%] [G loss: 3.329796]\n",
      "epoch:45 step:35775 [D loss: 0.336340, acc.: 84.38%] [G loss: 1.961297]\n",
      "epoch:45 step:35776 [D loss: 0.246693, acc.: 89.84%] [G loss: 3.027424]\n",
      "epoch:45 step:35777 [D loss: 0.297891, acc.: 87.50%] [G loss: 2.752580]\n",
      "epoch:45 step:35778 [D loss: 0.314898, acc.: 84.38%] [G loss: 2.931138]\n",
      "epoch:45 step:35779 [D loss: 0.236561, acc.: 89.84%] [G loss: 3.594867]\n",
      "epoch:45 step:35780 [D loss: 0.366196, acc.: 84.38%] [G loss: 3.045283]\n",
      "epoch:45 step:35781 [D loss: 0.325914, acc.: 87.50%] [G loss: 2.606763]\n",
      "epoch:45 step:35782 [D loss: 0.295027, acc.: 88.28%] [G loss: 3.869877]\n",
      "epoch:45 step:35783 [D loss: 0.252767, acc.: 88.28%] [G loss: 3.523806]\n",
      "epoch:45 step:35784 [D loss: 0.222916, acc.: 90.62%] [G loss: 8.829660]\n",
      "epoch:45 step:35785 [D loss: 0.195704, acc.: 92.19%] [G loss: 6.540751]\n",
      "epoch:45 step:35786 [D loss: 0.209964, acc.: 89.84%] [G loss: 4.194770]\n",
      "epoch:45 step:35787 [D loss: 0.335964, acc.: 82.81%] [G loss: 5.397750]\n",
      "epoch:45 step:35788 [D loss: 0.312555, acc.: 83.59%] [G loss: 4.139175]\n",
      "epoch:45 step:35789 [D loss: 0.217080, acc.: 89.06%] [G loss: 3.041755]\n",
      "epoch:45 step:35790 [D loss: 0.227599, acc.: 87.50%] [G loss: 4.447196]\n",
      "epoch:45 step:35791 [D loss: 0.308141, acc.: 89.84%] [G loss: 2.998022]\n",
      "epoch:45 step:35792 [D loss: 0.341369, acc.: 83.59%] [G loss: 3.795770]\n",
      "epoch:45 step:35793 [D loss: 0.262809, acc.: 87.50%] [G loss: 3.092078]\n",
      "epoch:45 step:35794 [D loss: 0.303873, acc.: 85.94%] [G loss: 3.050390]\n",
      "epoch:45 step:35795 [D loss: 0.254871, acc.: 89.06%] [G loss: 2.157143]\n",
      "epoch:45 step:35796 [D loss: 0.358300, acc.: 85.94%] [G loss: 2.683190]\n",
      "epoch:45 step:35797 [D loss: 0.264820, acc.: 88.28%] [G loss: 3.185846]\n",
      "epoch:45 step:35798 [D loss: 0.453845, acc.: 79.69%] [G loss: 2.484152]\n",
      "epoch:45 step:35799 [D loss: 0.428515, acc.: 81.25%] [G loss: 4.257844]\n",
      "epoch:45 step:35800 [D loss: 0.500850, acc.: 78.91%] [G loss: 3.929121]\n",
      "epoch:45 step:35801 [D loss: 0.171011, acc.: 93.75%] [G loss: 3.595214]\n",
      "epoch:45 step:35802 [D loss: 0.291100, acc.: 85.16%] [G loss: 3.540521]\n",
      "epoch:45 step:35803 [D loss: 0.359371, acc.: 85.94%] [G loss: 3.254421]\n",
      "epoch:45 step:35804 [D loss: 0.310976, acc.: 89.84%] [G loss: 2.736185]\n",
      "epoch:45 step:35805 [D loss: 0.238444, acc.: 91.41%] [G loss: 3.315238]\n",
      "epoch:45 step:35806 [D loss: 0.346798, acc.: 84.38%] [G loss: 4.447708]\n",
      "epoch:45 step:35807 [D loss: 0.492390, acc.: 79.69%] [G loss: 3.223922]\n",
      "epoch:45 step:35808 [D loss: 0.365127, acc.: 83.59%] [G loss: 3.340605]\n",
      "epoch:45 step:35809 [D loss: 0.413946, acc.: 82.81%] [G loss: 3.152315]\n",
      "epoch:45 step:35810 [D loss: 0.355690, acc.: 87.50%] [G loss: 4.838822]\n",
      "epoch:45 step:35811 [D loss: 0.414287, acc.: 80.47%] [G loss: 3.202755]\n",
      "epoch:45 step:35812 [D loss: 0.257250, acc.: 89.06%] [G loss: 3.675286]\n",
      "epoch:45 step:35813 [D loss: 0.372311, acc.: 83.59%] [G loss: 2.563282]\n",
      "epoch:45 step:35814 [D loss: 0.352895, acc.: 82.81%] [G loss: 2.910979]\n",
      "epoch:45 step:35815 [D loss: 0.367534, acc.: 78.12%] [G loss: 3.836288]\n",
      "epoch:45 step:35816 [D loss: 0.246725, acc.: 90.62%] [G loss: 3.627783]\n",
      "epoch:45 step:35817 [D loss: 0.340885, acc.: 85.94%] [G loss: 6.582772]\n",
      "epoch:45 step:35818 [D loss: 0.564259, acc.: 78.12%] [G loss: 6.666118]\n",
      "epoch:45 step:35819 [D loss: 1.365710, acc.: 62.50%] [G loss: 8.555786]\n",
      "epoch:45 step:35820 [D loss: 1.390224, acc.: 65.62%] [G loss: 5.533584]\n",
      "epoch:45 step:35821 [D loss: 0.955559, acc.: 74.22%] [G loss: 4.076914]\n",
      "epoch:45 step:35822 [D loss: 0.368332, acc.: 85.16%] [G loss: 3.836120]\n",
      "epoch:45 step:35823 [D loss: 0.366673, acc.: 85.16%] [G loss: 2.948527]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35824 [D loss: 0.505467, acc.: 78.91%] [G loss: 3.728593]\n",
      "epoch:45 step:35825 [D loss: 0.405385, acc.: 80.47%] [G loss: 4.633444]\n",
      "epoch:45 step:35826 [D loss: 0.480879, acc.: 82.81%] [G loss: 3.494467]\n",
      "epoch:45 step:35827 [D loss: 0.533563, acc.: 79.69%] [G loss: 2.231959]\n",
      "epoch:45 step:35828 [D loss: 0.321443, acc.: 84.38%] [G loss: 2.658379]\n",
      "epoch:45 step:35829 [D loss: 0.254841, acc.: 88.28%] [G loss: 2.755361]\n",
      "epoch:45 step:35830 [D loss: 0.349033, acc.: 82.03%] [G loss: 2.886578]\n",
      "epoch:45 step:35831 [D loss: 0.364238, acc.: 81.25%] [G loss: 2.317026]\n",
      "epoch:45 step:35832 [D loss: 0.364321, acc.: 83.59%] [G loss: 2.123025]\n",
      "epoch:45 step:35833 [D loss: 0.349442, acc.: 86.72%] [G loss: 2.814299]\n",
      "epoch:45 step:35834 [D loss: 0.284154, acc.: 85.94%] [G loss: 2.754239]\n",
      "epoch:45 step:35835 [D loss: 0.342964, acc.: 84.38%] [G loss: 2.378887]\n",
      "epoch:45 step:35836 [D loss: 0.345477, acc.: 82.03%] [G loss: 2.854655]\n",
      "epoch:45 step:35837 [D loss: 0.394327, acc.: 85.16%] [G loss: 2.766020]\n",
      "epoch:45 step:35838 [D loss: 0.274815, acc.: 88.28%] [G loss: 2.215047]\n",
      "epoch:45 step:35839 [D loss: 0.379420, acc.: 83.59%] [G loss: 2.479636]\n",
      "epoch:45 step:35840 [D loss: 0.273894, acc.: 87.50%] [G loss: 2.654992]\n",
      "epoch:45 step:35841 [D loss: 0.321708, acc.: 88.28%] [G loss: 2.673344]\n",
      "epoch:45 step:35842 [D loss: 0.377074, acc.: 80.47%] [G loss: 2.326847]\n",
      "epoch:45 step:35843 [D loss: 0.215935, acc.: 92.19%] [G loss: 2.440436]\n",
      "epoch:45 step:35844 [D loss: 0.323677, acc.: 87.50%] [G loss: 2.602083]\n",
      "epoch:45 step:35845 [D loss: 0.329570, acc.: 85.94%] [G loss: 2.471891]\n",
      "epoch:45 step:35846 [D loss: 0.296723, acc.: 88.28%] [G loss: 2.558565]\n",
      "epoch:45 step:35847 [D loss: 0.225695, acc.: 91.41%] [G loss: 2.214290]\n",
      "epoch:45 step:35848 [D loss: 0.419603, acc.: 80.47%] [G loss: 2.101742]\n",
      "epoch:45 step:35849 [D loss: 0.354163, acc.: 82.81%] [G loss: 2.560556]\n",
      "epoch:45 step:35850 [D loss: 0.339811, acc.: 85.94%] [G loss: 2.685760]\n",
      "epoch:45 step:35851 [D loss: 0.361199, acc.: 82.81%] [G loss: 2.298267]\n",
      "epoch:45 step:35852 [D loss: 0.415710, acc.: 82.03%] [G loss: 2.436937]\n",
      "epoch:45 step:35853 [D loss: 0.363197, acc.: 80.47%] [G loss: 2.709284]\n",
      "epoch:45 step:35854 [D loss: 0.419086, acc.: 83.59%] [G loss: 2.261757]\n",
      "epoch:45 step:35855 [D loss: 0.394385, acc.: 79.69%] [G loss: 2.759866]\n",
      "epoch:45 step:35856 [D loss: 0.321918, acc.: 85.16%] [G loss: 2.950634]\n",
      "epoch:45 step:35857 [D loss: 0.342440, acc.: 82.81%] [G loss: 2.893517]\n",
      "epoch:45 step:35858 [D loss: 0.351892, acc.: 84.38%] [G loss: 3.243088]\n",
      "epoch:45 step:35859 [D loss: 0.347510, acc.: 87.50%] [G loss: 3.077379]\n",
      "epoch:45 step:35860 [D loss: 0.364713, acc.: 81.25%] [G loss: 3.697757]\n",
      "epoch:45 step:35861 [D loss: 0.284438, acc.: 89.06%] [G loss: 3.288998]\n",
      "epoch:45 step:35862 [D loss: 0.323132, acc.: 85.16%] [G loss: 3.850335]\n",
      "epoch:45 step:35863 [D loss: 0.331183, acc.: 84.38%] [G loss: 2.702747]\n",
      "epoch:45 step:35864 [D loss: 0.323897, acc.: 87.50%] [G loss: 2.738708]\n",
      "epoch:45 step:35865 [D loss: 0.296318, acc.: 85.94%] [G loss: 3.048065]\n",
      "epoch:45 step:35866 [D loss: 0.267184, acc.: 86.72%] [G loss: 2.809259]\n",
      "epoch:45 step:35867 [D loss: 0.275053, acc.: 85.94%] [G loss: 3.293102]\n",
      "epoch:45 step:35868 [D loss: 0.387008, acc.: 77.34%] [G loss: 3.463807]\n",
      "epoch:45 step:35869 [D loss: 0.255817, acc.: 90.62%] [G loss: 2.862577]\n",
      "epoch:45 step:35870 [D loss: 0.175166, acc.: 92.19%] [G loss: 3.369724]\n",
      "epoch:45 step:35871 [D loss: 0.294213, acc.: 87.50%] [G loss: 2.815839]\n",
      "epoch:45 step:35872 [D loss: 0.360218, acc.: 84.38%] [G loss: 2.834946]\n",
      "epoch:45 step:35873 [D loss: 0.429688, acc.: 80.47%] [G loss: 3.077260]\n",
      "epoch:45 step:35874 [D loss: 0.329577, acc.: 83.59%] [G loss: 2.583169]\n",
      "epoch:45 step:35875 [D loss: 0.392130, acc.: 85.16%] [G loss: 2.478091]\n",
      "epoch:45 step:35876 [D loss: 0.200502, acc.: 93.75%] [G loss: 3.088749]\n",
      "epoch:45 step:35877 [D loss: 0.344782, acc.: 85.16%] [G loss: 3.394646]\n",
      "epoch:45 step:35878 [D loss: 0.329341, acc.: 85.94%] [G loss: 2.937954]\n",
      "epoch:45 step:35879 [D loss: 0.386749, acc.: 82.03%] [G loss: 2.286333]\n",
      "epoch:45 step:35880 [D loss: 0.269524, acc.: 90.62%] [G loss: 2.649389]\n",
      "epoch:45 step:35881 [D loss: 0.229945, acc.: 90.62%] [G loss: 2.579581]\n",
      "epoch:45 step:35882 [D loss: 0.248585, acc.: 90.62%] [G loss: 2.837042]\n",
      "epoch:45 step:35883 [D loss: 0.287241, acc.: 86.72%] [G loss: 2.589464]\n",
      "epoch:45 step:35884 [D loss: 0.265246, acc.: 89.84%] [G loss: 2.415541]\n",
      "epoch:45 step:35885 [D loss: 0.286690, acc.: 88.28%] [G loss: 2.333210]\n",
      "epoch:45 step:35886 [D loss: 0.326076, acc.: 84.38%] [G loss: 2.102685]\n",
      "epoch:45 step:35887 [D loss: 0.274589, acc.: 89.84%] [G loss: 2.775084]\n",
      "epoch:45 step:35888 [D loss: 0.364866, acc.: 78.91%] [G loss: 3.143087]\n",
      "epoch:45 step:35889 [D loss: 0.340514, acc.: 86.72%] [G loss: 2.395619]\n",
      "epoch:45 step:35890 [D loss: 0.306043, acc.: 91.41%] [G loss: 2.581010]\n",
      "epoch:45 step:35891 [D loss: 0.317189, acc.: 86.72%] [G loss: 2.695107]\n",
      "epoch:45 step:35892 [D loss: 0.239852, acc.: 89.06%] [G loss: 3.726033]\n",
      "epoch:45 step:35893 [D loss: 0.290665, acc.: 85.16%] [G loss: 3.087088]\n",
      "epoch:45 step:35894 [D loss: 0.327219, acc.: 85.94%] [G loss: 3.528847]\n",
      "epoch:45 step:35895 [D loss: 0.238230, acc.: 89.06%] [G loss: 3.169862]\n",
      "epoch:45 step:35896 [D loss: 0.307513, acc.: 87.50%] [G loss: 2.888428]\n",
      "epoch:45 step:35897 [D loss: 0.263753, acc.: 89.84%] [G loss: 3.207316]\n",
      "epoch:45 step:35898 [D loss: 0.286946, acc.: 89.06%] [G loss: 2.342763]\n",
      "epoch:45 step:35899 [D loss: 0.336870, acc.: 85.16%] [G loss: 2.929146]\n",
      "epoch:45 step:35900 [D loss: 0.232089, acc.: 92.97%] [G loss: 3.152892]\n",
      "epoch:45 step:35901 [D loss: 0.311853, acc.: 85.94%] [G loss: 3.392693]\n",
      "epoch:45 step:35902 [D loss: 0.216355, acc.: 90.62%] [G loss: 3.215147]\n",
      "epoch:45 step:35903 [D loss: 0.332022, acc.: 83.59%] [G loss: 3.029582]\n",
      "epoch:45 step:35904 [D loss: 0.267371, acc.: 87.50%] [G loss: 3.132419]\n",
      "epoch:45 step:35905 [D loss: 0.284892, acc.: 92.19%] [G loss: 2.995086]\n",
      "epoch:45 step:35906 [D loss: 0.341556, acc.: 87.50%] [G loss: 3.210922]\n",
      "epoch:45 step:35907 [D loss: 0.333051, acc.: 84.38%] [G loss: 2.685629]\n",
      "epoch:45 step:35908 [D loss: 0.358025, acc.: 82.81%] [G loss: 3.036116]\n",
      "epoch:45 step:35909 [D loss: 0.278259, acc.: 86.72%] [G loss: 2.759226]\n",
      "epoch:45 step:35910 [D loss: 0.396458, acc.: 79.69%] [G loss: 2.947671]\n",
      "epoch:45 step:35911 [D loss: 0.291853, acc.: 86.72%] [G loss: 2.947480]\n",
      "epoch:45 step:35912 [D loss: 0.301937, acc.: 85.94%] [G loss: 2.522477]\n",
      "epoch:45 step:35913 [D loss: 0.299320, acc.: 85.16%] [G loss: 2.665510]\n",
      "epoch:45 step:35914 [D loss: 0.307632, acc.: 87.50%] [G loss: 2.560077]\n",
      "epoch:45 step:35915 [D loss: 0.305745, acc.: 85.94%] [G loss: 3.122711]\n",
      "epoch:45 step:35916 [D loss: 0.438011, acc.: 77.34%] [G loss: 2.688913]\n",
      "epoch:45 step:35917 [D loss: 0.258761, acc.: 87.50%] [G loss: 2.997792]\n",
      "epoch:45 step:35918 [D loss: 0.293446, acc.: 86.72%] [G loss: 2.588525]\n",
      "epoch:45 step:35919 [D loss: 0.341788, acc.: 86.72%] [G loss: 3.150287]\n",
      "epoch:45 step:35920 [D loss: 0.417265, acc.: 85.16%] [G loss: 2.939006]\n",
      "epoch:45 step:35921 [D loss: 0.377703, acc.: 82.03%] [G loss: 3.667595]\n",
      "epoch:45 step:35922 [D loss: 0.362379, acc.: 86.72%] [G loss: 5.144171]\n",
      "epoch:45 step:35923 [D loss: 0.313554, acc.: 82.81%] [G loss: 4.788771]\n",
      "epoch:45 step:35924 [D loss: 0.303218, acc.: 89.06%] [G loss: 3.846266]\n",
      "epoch:45 step:35925 [D loss: 0.256644, acc.: 90.62%] [G loss: 2.540704]\n",
      "epoch:45 step:35926 [D loss: 0.251970, acc.: 89.06%] [G loss: 2.886437]\n",
      "epoch:46 step:35927 [D loss: 0.246231, acc.: 89.84%] [G loss: 2.104337]\n",
      "epoch:46 step:35928 [D loss: 0.229445, acc.: 93.75%] [G loss: 3.148244]\n",
      "epoch:46 step:35929 [D loss: 0.267371, acc.: 89.06%] [G loss: 2.964236]\n",
      "epoch:46 step:35930 [D loss: 0.365112, acc.: 82.03%] [G loss: 3.519095]\n",
      "epoch:46 step:35931 [D loss: 0.236392, acc.: 91.41%] [G loss: 3.041857]\n",
      "epoch:46 step:35932 [D loss: 0.305020, acc.: 83.59%] [G loss: 5.300967]\n",
      "epoch:46 step:35933 [D loss: 0.354846, acc.: 82.03%] [G loss: 6.049863]\n",
      "epoch:46 step:35934 [D loss: 0.244678, acc.: 88.28%] [G loss: 5.061593]\n",
      "epoch:46 step:35935 [D loss: 0.479470, acc.: 79.69%] [G loss: 4.179504]\n",
      "epoch:46 step:35936 [D loss: 0.412359, acc.: 79.69%] [G loss: 3.627866]\n",
      "epoch:46 step:35937 [D loss: 0.231798, acc.: 92.97%] [G loss: 2.654691]\n",
      "epoch:46 step:35938 [D loss: 0.228451, acc.: 89.06%] [G loss: 4.242996]\n",
      "epoch:46 step:35939 [D loss: 0.281700, acc.: 88.28%] [G loss: 3.210093]\n",
      "epoch:46 step:35940 [D loss: 0.335581, acc.: 85.94%] [G loss: 3.330059]\n",
      "epoch:46 step:35941 [D loss: 0.266971, acc.: 87.50%] [G loss: 5.096404]\n",
      "epoch:46 step:35942 [D loss: 0.272489, acc.: 85.94%] [G loss: 3.189277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:35943 [D loss: 0.297033, acc.: 89.84%] [G loss: 3.151909]\n",
      "epoch:46 step:35944 [D loss: 0.340469, acc.: 81.25%] [G loss: 3.143510]\n",
      "epoch:46 step:35945 [D loss: 0.244113, acc.: 88.28%] [G loss: 3.642512]\n",
      "epoch:46 step:35946 [D loss: 0.275672, acc.: 89.06%] [G loss: 2.832965]\n",
      "epoch:46 step:35947 [D loss: 0.222003, acc.: 91.41%] [G loss: 3.464471]\n",
      "epoch:46 step:35948 [D loss: 0.306473, acc.: 85.94%] [G loss: 3.659234]\n",
      "epoch:46 step:35949 [D loss: 0.372926, acc.: 82.81%] [G loss: 3.239295]\n",
      "epoch:46 step:35950 [D loss: 0.267535, acc.: 87.50%] [G loss: 5.676808]\n",
      "epoch:46 step:35951 [D loss: 0.232117, acc.: 88.28%] [G loss: 5.123231]\n",
      "epoch:46 step:35952 [D loss: 0.220305, acc.: 92.19%] [G loss: 5.260988]\n",
      "epoch:46 step:35953 [D loss: 0.277533, acc.: 85.94%] [G loss: 4.987873]\n",
      "epoch:46 step:35954 [D loss: 0.278440, acc.: 86.72%] [G loss: 4.732062]\n",
      "epoch:46 step:35955 [D loss: 0.399817, acc.: 88.28%] [G loss: 5.058731]\n",
      "epoch:46 step:35956 [D loss: 0.235418, acc.: 89.84%] [G loss: 3.667059]\n",
      "epoch:46 step:35957 [D loss: 0.266446, acc.: 85.94%] [G loss: 3.214370]\n",
      "epoch:46 step:35958 [D loss: 0.386110, acc.: 81.25%] [G loss: 3.496920]\n",
      "epoch:46 step:35959 [D loss: 0.282522, acc.: 91.41%] [G loss: 3.399982]\n",
      "epoch:46 step:35960 [D loss: 0.278556, acc.: 86.72%] [G loss: 4.440358]\n",
      "epoch:46 step:35961 [D loss: 0.415011, acc.: 80.47%] [G loss: 3.992809]\n",
      "epoch:46 step:35962 [D loss: 0.335631, acc.: 87.50%] [G loss: 2.726646]\n",
      "epoch:46 step:35963 [D loss: 0.285864, acc.: 89.84%] [G loss: 2.886632]\n",
      "epoch:46 step:35964 [D loss: 0.459659, acc.: 78.12%] [G loss: 3.291215]\n",
      "epoch:46 step:35965 [D loss: 0.272992, acc.: 89.84%] [G loss: 2.323089]\n",
      "epoch:46 step:35966 [D loss: 0.419668, acc.: 81.25%] [G loss: 2.901864]\n",
      "epoch:46 step:35967 [D loss: 0.287814, acc.: 89.06%] [G loss: 2.990565]\n",
      "epoch:46 step:35968 [D loss: 0.304451, acc.: 85.94%] [G loss: 2.633401]\n",
      "epoch:46 step:35969 [D loss: 0.420825, acc.: 79.69%] [G loss: 3.871419]\n",
      "epoch:46 step:35970 [D loss: 0.469172, acc.: 75.78%] [G loss: 5.980465]\n",
      "epoch:46 step:35971 [D loss: 0.662940, acc.: 69.53%] [G loss: 3.517453]\n",
      "epoch:46 step:35972 [D loss: 0.297307, acc.: 89.06%] [G loss: 3.326110]\n",
      "epoch:46 step:35973 [D loss: 0.280738, acc.: 88.28%] [G loss: 3.316849]\n",
      "epoch:46 step:35974 [D loss: 0.305474, acc.: 86.72%] [G loss: 3.074980]\n",
      "epoch:46 step:35975 [D loss: 0.258442, acc.: 92.19%] [G loss: 3.406676]\n",
      "epoch:46 step:35976 [D loss: 0.275001, acc.: 88.28%] [G loss: 3.834578]\n",
      "epoch:46 step:35977 [D loss: 0.378030, acc.: 80.47%] [G loss: 3.181772]\n",
      "epoch:46 step:35978 [D loss: 0.251836, acc.: 90.62%] [G loss: 3.040392]\n",
      "epoch:46 step:35979 [D loss: 0.474315, acc.: 81.25%] [G loss: 5.865123]\n",
      "epoch:46 step:35980 [D loss: 0.338943, acc.: 84.38%] [G loss: 2.822308]\n",
      "epoch:46 step:35981 [D loss: 0.224599, acc.: 91.41%] [G loss: 3.510850]\n",
      "epoch:46 step:35982 [D loss: 0.375760, acc.: 82.03%] [G loss: 3.782229]\n",
      "epoch:46 step:35983 [D loss: 0.326333, acc.: 82.81%] [G loss: 3.148398]\n",
      "epoch:46 step:35984 [D loss: 0.299233, acc.: 85.94%] [G loss: 3.913011]\n",
      "epoch:46 step:35985 [D loss: 0.245008, acc.: 89.84%] [G loss: 2.911705]\n",
      "epoch:46 step:35986 [D loss: 0.307312, acc.: 86.72%] [G loss: 2.560985]\n",
      "epoch:46 step:35987 [D loss: 0.301907, acc.: 83.59%] [G loss: 2.872600]\n",
      "epoch:46 step:35988 [D loss: 0.356010, acc.: 82.03%] [G loss: 2.649845]\n",
      "epoch:46 step:35989 [D loss: 0.258796, acc.: 89.84%] [G loss: 3.391113]\n",
      "epoch:46 step:35990 [D loss: 0.331430, acc.: 87.50%] [G loss: 2.540906]\n",
      "epoch:46 step:35991 [D loss: 0.359842, acc.: 86.72%] [G loss: 2.975955]\n",
      "epoch:46 step:35992 [D loss: 0.352365, acc.: 82.03%] [G loss: 2.549283]\n",
      "epoch:46 step:35993 [D loss: 0.327414, acc.: 79.69%] [G loss: 2.716006]\n",
      "epoch:46 step:35994 [D loss: 0.355254, acc.: 85.16%] [G loss: 2.328538]\n",
      "epoch:46 step:35995 [D loss: 0.347263, acc.: 82.81%] [G loss: 2.132181]\n",
      "epoch:46 step:35996 [D loss: 0.330321, acc.: 85.16%] [G loss: 2.808824]\n",
      "epoch:46 step:35997 [D loss: 0.360083, acc.: 85.94%] [G loss: 3.309240]\n",
      "epoch:46 step:35998 [D loss: 0.286141, acc.: 88.28%] [G loss: 3.142315]\n",
      "epoch:46 step:35999 [D loss: 0.347758, acc.: 85.16%] [G loss: 2.868705]\n",
      "epoch:46 step:36000 [D loss: 0.326600, acc.: 86.72%] [G loss: 2.778348]\n",
      "epoch:46 step:36001 [D loss: 0.333032, acc.: 84.38%] [G loss: 3.293863]\n",
      "epoch:46 step:36002 [D loss: 0.523802, acc.: 72.66%] [G loss: 2.997324]\n",
      "epoch:46 step:36003 [D loss: 0.350971, acc.: 85.16%] [G loss: 3.907221]\n",
      "epoch:46 step:36004 [D loss: 0.408771, acc.: 82.81%] [G loss: 3.990386]\n",
      "epoch:46 step:36005 [D loss: 0.241605, acc.: 89.06%] [G loss: 3.508701]\n",
      "epoch:46 step:36006 [D loss: 0.351283, acc.: 87.50%] [G loss: 4.187276]\n",
      "epoch:46 step:36007 [D loss: 0.214376, acc.: 89.84%] [G loss: 4.502923]\n",
      "epoch:46 step:36008 [D loss: 0.255462, acc.: 88.28%] [G loss: 3.696928]\n",
      "epoch:46 step:36009 [D loss: 0.242668, acc.: 88.28%] [G loss: 3.857928]\n",
      "epoch:46 step:36010 [D loss: 0.250180, acc.: 90.62%] [G loss: 3.194653]\n",
      "epoch:46 step:36011 [D loss: 0.368490, acc.: 82.81%] [G loss: 3.518393]\n",
      "epoch:46 step:36012 [D loss: 0.263766, acc.: 89.06%] [G loss: 2.474518]\n",
      "epoch:46 step:36013 [D loss: 0.372914, acc.: 84.38%] [G loss: 5.133504]\n",
      "epoch:46 step:36014 [D loss: 0.299836, acc.: 85.16%] [G loss: 4.036964]\n",
      "epoch:46 step:36015 [D loss: 0.243461, acc.: 92.19%] [G loss: 4.494637]\n",
      "epoch:46 step:36016 [D loss: 0.256084, acc.: 90.62%] [G loss: 3.825470]\n",
      "epoch:46 step:36017 [D loss: 0.298471, acc.: 84.38%] [G loss: 4.877283]\n",
      "epoch:46 step:36018 [D loss: 0.306586, acc.: 82.81%] [G loss: 4.742197]\n",
      "epoch:46 step:36019 [D loss: 0.236732, acc.: 89.84%] [G loss: 4.147938]\n",
      "epoch:46 step:36020 [D loss: 0.270880, acc.: 86.72%] [G loss: 4.736100]\n",
      "epoch:46 step:36021 [D loss: 0.361721, acc.: 84.38%] [G loss: 4.608900]\n",
      "epoch:46 step:36022 [D loss: 0.469405, acc.: 78.12%] [G loss: 3.738514]\n",
      "epoch:46 step:36023 [D loss: 0.338710, acc.: 84.38%] [G loss: 4.120991]\n",
      "epoch:46 step:36024 [D loss: 0.284143, acc.: 87.50%] [G loss: 4.432997]\n",
      "epoch:46 step:36025 [D loss: 0.315617, acc.: 80.47%] [G loss: 3.988217]\n",
      "epoch:46 step:36026 [D loss: 0.317548, acc.: 87.50%] [G loss: 3.741715]\n",
      "epoch:46 step:36027 [D loss: 0.398811, acc.: 82.03%] [G loss: 2.530339]\n",
      "epoch:46 step:36028 [D loss: 0.250354, acc.: 89.06%] [G loss: 3.655793]\n",
      "epoch:46 step:36029 [D loss: 0.315380, acc.: 87.50%] [G loss: 2.963924]\n",
      "epoch:46 step:36030 [D loss: 0.308970, acc.: 86.72%] [G loss: 3.022254]\n",
      "epoch:46 step:36031 [D loss: 0.360607, acc.: 85.94%] [G loss: 2.605726]\n",
      "epoch:46 step:36032 [D loss: 0.373400, acc.: 82.03%] [G loss: 3.653622]\n",
      "epoch:46 step:36033 [D loss: 0.294609, acc.: 89.84%] [G loss: 3.385111]\n",
      "epoch:46 step:36034 [D loss: 0.275843, acc.: 88.28%] [G loss: 4.047935]\n",
      "epoch:46 step:36035 [D loss: 0.306405, acc.: 86.72%] [G loss: 4.235085]\n",
      "epoch:46 step:36036 [D loss: 0.305542, acc.: 86.72%] [G loss: 3.313672]\n",
      "epoch:46 step:36037 [D loss: 0.496598, acc.: 79.69%] [G loss: 3.421936]\n",
      "epoch:46 step:36038 [D loss: 0.280432, acc.: 88.28%] [G loss: 3.202441]\n",
      "epoch:46 step:36039 [D loss: 0.268537, acc.: 92.19%] [G loss: 3.251936]\n",
      "epoch:46 step:36040 [D loss: 0.396402, acc.: 84.38%] [G loss: 2.851115]\n",
      "epoch:46 step:36041 [D loss: 0.308834, acc.: 87.50%] [G loss: 2.776465]\n",
      "epoch:46 step:36042 [D loss: 0.327687, acc.: 88.28%] [G loss: 3.066643]\n",
      "epoch:46 step:36043 [D loss: 0.242151, acc.: 91.41%] [G loss: 3.206026]\n",
      "epoch:46 step:36044 [D loss: 0.257203, acc.: 89.84%] [G loss: 3.117578]\n",
      "epoch:46 step:36045 [D loss: 0.253753, acc.: 89.06%] [G loss: 3.912068]\n",
      "epoch:46 step:36046 [D loss: 0.361075, acc.: 82.81%] [G loss: 2.850530]\n",
      "epoch:46 step:36047 [D loss: 0.223693, acc.: 89.84%] [G loss: 3.390141]\n",
      "epoch:46 step:36048 [D loss: 0.266600, acc.: 86.72%] [G loss: 3.610736]\n",
      "epoch:46 step:36049 [D loss: 0.295584, acc.: 89.84%] [G loss: 2.211843]\n",
      "epoch:46 step:36050 [D loss: 0.370799, acc.: 83.59%] [G loss: 2.940467]\n",
      "epoch:46 step:36051 [D loss: 0.274347, acc.: 88.28%] [G loss: 2.636747]\n",
      "epoch:46 step:36052 [D loss: 0.251445, acc.: 89.84%] [G loss: 3.656602]\n",
      "epoch:46 step:36053 [D loss: 0.266367, acc.: 89.06%] [G loss: 3.777920]\n",
      "epoch:46 step:36054 [D loss: 0.253736, acc.: 87.50%] [G loss: 3.969978]\n",
      "epoch:46 step:36055 [D loss: 0.323736, acc.: 83.59%] [G loss: 3.499315]\n",
      "epoch:46 step:36056 [D loss: 0.216386, acc.: 91.41%] [G loss: 3.664904]\n",
      "epoch:46 step:36057 [D loss: 0.297344, acc.: 84.38%] [G loss: 3.751191]\n",
      "epoch:46 step:36058 [D loss: 0.314165, acc.: 85.94%] [G loss: 3.189752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36059 [D loss: 0.285575, acc.: 85.94%] [G loss: 3.663968]\n",
      "epoch:46 step:36060 [D loss: 0.306053, acc.: 89.06%] [G loss: 3.705853]\n",
      "epoch:46 step:36061 [D loss: 0.294227, acc.: 85.94%] [G loss: 3.050789]\n",
      "epoch:46 step:36062 [D loss: 0.254940, acc.: 87.50%] [G loss: 3.498893]\n",
      "epoch:46 step:36063 [D loss: 0.363405, acc.: 82.03%] [G loss: 5.163751]\n",
      "epoch:46 step:36064 [D loss: 0.359289, acc.: 87.50%] [G loss: 4.264425]\n",
      "epoch:46 step:36065 [D loss: 0.170315, acc.: 91.41%] [G loss: 4.146967]\n",
      "epoch:46 step:36066 [D loss: 0.299026, acc.: 84.38%] [G loss: 5.455293]\n",
      "epoch:46 step:36067 [D loss: 0.297825, acc.: 85.94%] [G loss: 3.842080]\n",
      "epoch:46 step:36068 [D loss: 0.327348, acc.: 85.94%] [G loss: 3.160334]\n",
      "epoch:46 step:36069 [D loss: 0.217241, acc.: 91.41%] [G loss: 3.754950]\n",
      "epoch:46 step:36070 [D loss: 0.434048, acc.: 76.56%] [G loss: 3.433676]\n",
      "epoch:46 step:36071 [D loss: 0.217215, acc.: 90.62%] [G loss: 3.734888]\n",
      "epoch:46 step:36072 [D loss: 0.393530, acc.: 82.81%] [G loss: 3.003791]\n",
      "epoch:46 step:36073 [D loss: 0.225431, acc.: 88.28%] [G loss: 2.763272]\n",
      "epoch:46 step:36074 [D loss: 0.299081, acc.: 88.28%] [G loss: 3.588907]\n",
      "epoch:46 step:36075 [D loss: 0.331065, acc.: 84.38%] [G loss: 3.021996]\n",
      "epoch:46 step:36076 [D loss: 0.252321, acc.: 90.62%] [G loss: 4.829942]\n",
      "epoch:46 step:36077 [D loss: 0.301815, acc.: 84.38%] [G loss: 2.729085]\n",
      "epoch:46 step:36078 [D loss: 0.253989, acc.: 90.62%] [G loss: 3.750608]\n",
      "epoch:46 step:36079 [D loss: 0.285403, acc.: 89.84%] [G loss: 3.073544]\n",
      "epoch:46 step:36080 [D loss: 0.339082, acc.: 85.16%] [G loss: 3.475977]\n",
      "epoch:46 step:36081 [D loss: 0.346224, acc.: 82.81%] [G loss: 3.283528]\n",
      "epoch:46 step:36082 [D loss: 0.312414, acc.: 84.38%] [G loss: 3.785484]\n",
      "epoch:46 step:36083 [D loss: 0.326908, acc.: 84.38%] [G loss: 2.810167]\n",
      "epoch:46 step:36084 [D loss: 0.388870, acc.: 82.81%] [G loss: 3.866095]\n",
      "epoch:46 step:36085 [D loss: 0.306271, acc.: 85.94%] [G loss: 3.982251]\n",
      "epoch:46 step:36086 [D loss: 0.351576, acc.: 83.59%] [G loss: 3.341997]\n",
      "epoch:46 step:36087 [D loss: 0.515538, acc.: 78.12%] [G loss: 2.806537]\n",
      "epoch:46 step:36088 [D loss: 0.239670, acc.: 91.41%] [G loss: 3.071421]\n",
      "epoch:46 step:36089 [D loss: 0.327851, acc.: 86.72%] [G loss: 3.793396]\n",
      "epoch:46 step:36090 [D loss: 0.344141, acc.: 83.59%] [G loss: 4.492180]\n",
      "epoch:46 step:36091 [D loss: 0.395104, acc.: 82.81%] [G loss: 3.444276]\n",
      "epoch:46 step:36092 [D loss: 0.281952, acc.: 86.72%] [G loss: 4.297626]\n",
      "epoch:46 step:36093 [D loss: 0.257791, acc.: 85.94%] [G loss: 5.259803]\n",
      "epoch:46 step:36094 [D loss: 0.255166, acc.: 88.28%] [G loss: 4.390314]\n",
      "epoch:46 step:36095 [D loss: 0.237462, acc.: 89.06%] [G loss: 8.056494]\n",
      "epoch:46 step:36096 [D loss: 0.194403, acc.: 94.53%] [G loss: 5.241194]\n",
      "epoch:46 step:36097 [D loss: 0.369857, acc.: 85.16%] [G loss: 3.547306]\n",
      "epoch:46 step:36098 [D loss: 0.247668, acc.: 92.19%] [G loss: 4.229671]\n",
      "epoch:46 step:36099 [D loss: 0.315979, acc.: 83.59%] [G loss: 3.860485]\n",
      "epoch:46 step:36100 [D loss: 0.303450, acc.: 88.28%] [G loss: 3.930541]\n",
      "epoch:46 step:36101 [D loss: 0.260669, acc.: 86.72%] [G loss: 3.628312]\n",
      "epoch:46 step:36102 [D loss: 0.357242, acc.: 86.72%] [G loss: 3.118162]\n",
      "epoch:46 step:36103 [D loss: 0.244554, acc.: 89.84%] [G loss: 2.902636]\n",
      "epoch:46 step:36104 [D loss: 0.244202, acc.: 91.41%] [G loss: 2.692077]\n",
      "epoch:46 step:36105 [D loss: 0.225727, acc.: 92.19%] [G loss: 2.904003]\n",
      "epoch:46 step:36106 [D loss: 0.385311, acc.: 84.38%] [G loss: 3.643831]\n",
      "epoch:46 step:36107 [D loss: 0.300258, acc.: 84.38%] [G loss: 2.896994]\n",
      "epoch:46 step:36108 [D loss: 0.254521, acc.: 88.28%] [G loss: 3.096532]\n",
      "epoch:46 step:36109 [D loss: 0.205069, acc.: 92.19%] [G loss: 3.378747]\n",
      "epoch:46 step:36110 [D loss: 0.362984, acc.: 84.38%] [G loss: 3.193166]\n",
      "epoch:46 step:36111 [D loss: 0.335909, acc.: 84.38%] [G loss: 3.626942]\n",
      "epoch:46 step:36112 [D loss: 0.314831, acc.: 85.94%] [G loss: 3.467846]\n",
      "epoch:46 step:36113 [D loss: 0.243321, acc.: 91.41%] [G loss: 3.328134]\n",
      "epoch:46 step:36114 [D loss: 0.445736, acc.: 79.69%] [G loss: 3.325968]\n",
      "epoch:46 step:36115 [D loss: 0.306325, acc.: 85.16%] [G loss: 2.706116]\n",
      "epoch:46 step:36116 [D loss: 0.312963, acc.: 83.59%] [G loss: 3.579534]\n",
      "epoch:46 step:36117 [D loss: 0.342668, acc.: 84.38%] [G loss: 4.200931]\n",
      "epoch:46 step:36118 [D loss: 0.326387, acc.: 86.72%] [G loss: 3.266198]\n",
      "epoch:46 step:36119 [D loss: 0.275144, acc.: 88.28%] [G loss: 2.977662]\n",
      "epoch:46 step:36120 [D loss: 0.301084, acc.: 85.16%] [G loss: 2.559400]\n",
      "epoch:46 step:36121 [D loss: 0.267024, acc.: 89.06%] [G loss: 2.956103]\n",
      "epoch:46 step:36122 [D loss: 0.226887, acc.: 92.19%] [G loss: 3.208549]\n",
      "epoch:46 step:36123 [D loss: 0.434966, acc.: 79.69%] [G loss: 3.285435]\n",
      "epoch:46 step:36124 [D loss: 0.295412, acc.: 85.94%] [G loss: 2.645711]\n",
      "epoch:46 step:36125 [D loss: 0.237788, acc.: 91.41%] [G loss: 3.862007]\n",
      "epoch:46 step:36126 [D loss: 0.330765, acc.: 84.38%] [G loss: 3.109465]\n",
      "epoch:46 step:36127 [D loss: 0.311299, acc.: 86.72%] [G loss: 2.821107]\n",
      "epoch:46 step:36128 [D loss: 0.363289, acc.: 82.81%] [G loss: 3.266897]\n",
      "epoch:46 step:36129 [D loss: 0.365844, acc.: 82.81%] [G loss: 3.343749]\n",
      "epoch:46 step:36130 [D loss: 0.335723, acc.: 87.50%] [G loss: 3.027739]\n",
      "epoch:46 step:36131 [D loss: 0.258778, acc.: 88.28%] [G loss: 4.331824]\n",
      "epoch:46 step:36132 [D loss: 0.401259, acc.: 81.25%] [G loss: 3.514076]\n",
      "epoch:46 step:36133 [D loss: 0.327123, acc.: 84.38%] [G loss: 2.539259]\n",
      "epoch:46 step:36134 [D loss: 0.462710, acc.: 75.00%] [G loss: 3.239717]\n",
      "epoch:46 step:36135 [D loss: 0.398271, acc.: 83.59%] [G loss: 3.428361]\n",
      "epoch:46 step:36136 [D loss: 0.278574, acc.: 89.06%] [G loss: 4.251066]\n",
      "epoch:46 step:36137 [D loss: 0.281021, acc.: 85.16%] [G loss: 5.596476]\n",
      "epoch:46 step:36138 [D loss: 0.357483, acc.: 81.25%] [G loss: 4.440793]\n",
      "epoch:46 step:36139 [D loss: 0.242356, acc.: 87.50%] [G loss: 3.390978]\n",
      "epoch:46 step:36140 [D loss: 0.350428, acc.: 80.47%] [G loss: 3.336819]\n",
      "epoch:46 step:36141 [D loss: 0.215288, acc.: 88.28%] [G loss: 3.346238]\n",
      "epoch:46 step:36142 [D loss: 0.240825, acc.: 89.84%] [G loss: 3.895052]\n",
      "epoch:46 step:36143 [D loss: 0.268085, acc.: 89.06%] [G loss: 3.044388]\n",
      "epoch:46 step:36144 [D loss: 0.257151, acc.: 88.28%] [G loss: 3.681590]\n",
      "epoch:46 step:36145 [D loss: 0.302527, acc.: 89.84%] [G loss: 3.524207]\n",
      "epoch:46 step:36146 [D loss: 0.299375, acc.: 86.72%] [G loss: 3.632700]\n",
      "epoch:46 step:36147 [D loss: 0.224729, acc.: 90.62%] [G loss: 3.768679]\n",
      "epoch:46 step:36148 [D loss: 0.300612, acc.: 86.72%] [G loss: 3.175058]\n",
      "epoch:46 step:36149 [D loss: 0.400203, acc.: 80.47%] [G loss: 3.129530]\n",
      "epoch:46 step:36150 [D loss: 0.225580, acc.: 91.41%] [G loss: 2.830157]\n",
      "epoch:46 step:36151 [D loss: 0.263435, acc.: 87.50%] [G loss: 3.049974]\n",
      "epoch:46 step:36152 [D loss: 0.253711, acc.: 89.84%] [G loss: 4.534533]\n",
      "epoch:46 step:36153 [D loss: 0.339601, acc.: 83.59%] [G loss: 5.001443]\n",
      "epoch:46 step:36154 [D loss: 0.468535, acc.: 79.69%] [G loss: 3.317413]\n",
      "epoch:46 step:36155 [D loss: 0.583058, acc.: 75.00%] [G loss: 2.587246]\n",
      "epoch:46 step:36156 [D loss: 0.262593, acc.: 90.62%] [G loss: 3.572957]\n",
      "epoch:46 step:36157 [D loss: 0.275641, acc.: 88.28%] [G loss: 3.025155]\n",
      "epoch:46 step:36158 [D loss: 0.313325, acc.: 83.59%] [G loss: 3.321435]\n",
      "epoch:46 step:36159 [D loss: 0.305373, acc.: 87.50%] [G loss: 3.585247]\n",
      "epoch:46 step:36160 [D loss: 0.307283, acc.: 85.16%] [G loss: 2.566694]\n",
      "epoch:46 step:36161 [D loss: 0.278407, acc.: 86.72%] [G loss: 2.949522]\n",
      "epoch:46 step:36162 [D loss: 0.253676, acc.: 90.62%] [G loss: 3.109606]\n",
      "epoch:46 step:36163 [D loss: 0.316012, acc.: 87.50%] [G loss: 3.298498]\n",
      "epoch:46 step:36164 [D loss: 0.345281, acc.: 84.38%] [G loss: 4.007320]\n",
      "epoch:46 step:36165 [D loss: 0.308878, acc.: 84.38%] [G loss: 3.791166]\n",
      "epoch:46 step:36166 [D loss: 0.288536, acc.: 85.94%] [G loss: 5.554671]\n",
      "epoch:46 step:36167 [D loss: 0.391475, acc.: 80.47%] [G loss: 3.840561]\n",
      "epoch:46 step:36168 [D loss: 0.423698, acc.: 78.91%] [G loss: 4.144509]\n",
      "epoch:46 step:36169 [D loss: 0.280457, acc.: 87.50%] [G loss: 4.150450]\n",
      "epoch:46 step:36170 [D loss: 0.327073, acc.: 88.28%] [G loss: 6.510970]\n",
      "epoch:46 step:36171 [D loss: 0.235464, acc.: 89.06%] [G loss: 4.798774]\n",
      "epoch:46 step:36172 [D loss: 0.240682, acc.: 90.62%] [G loss: 5.796554]\n",
      "epoch:46 step:36173 [D loss: 0.299322, acc.: 87.50%] [G loss: 6.631511]\n",
      "epoch:46 step:36174 [D loss: 0.265562, acc.: 89.84%] [G loss: 5.099972]\n",
      "epoch:46 step:36175 [D loss: 0.263582, acc.: 85.94%] [G loss: 4.990662]\n",
      "epoch:46 step:36176 [D loss: 0.413215, acc.: 84.38%] [G loss: 4.918868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36177 [D loss: 0.293728, acc.: 85.16%] [G loss: 3.243527]\n",
      "epoch:46 step:36178 [D loss: 0.338646, acc.: 85.16%] [G loss: 3.920821]\n",
      "epoch:46 step:36179 [D loss: 0.293265, acc.: 85.16%] [G loss: 4.177896]\n",
      "epoch:46 step:36180 [D loss: 0.315382, acc.: 85.16%] [G loss: 4.728191]\n",
      "epoch:46 step:36181 [D loss: 0.366344, acc.: 83.59%] [G loss: 3.637930]\n",
      "epoch:46 step:36182 [D loss: 0.494228, acc.: 78.12%] [G loss: 6.676296]\n",
      "epoch:46 step:36183 [D loss: 0.490506, acc.: 74.22%] [G loss: 2.484042]\n",
      "epoch:46 step:36184 [D loss: 0.276206, acc.: 88.28%] [G loss: 4.728142]\n",
      "epoch:46 step:36185 [D loss: 0.295238, acc.: 84.38%] [G loss: 2.495358]\n",
      "epoch:46 step:36186 [D loss: 0.296320, acc.: 84.38%] [G loss: 3.434917]\n",
      "epoch:46 step:36187 [D loss: 0.381637, acc.: 84.38%] [G loss: 3.184556]\n",
      "epoch:46 step:36188 [D loss: 0.362450, acc.: 85.94%] [G loss: 3.299828]\n",
      "epoch:46 step:36189 [D loss: 0.224249, acc.: 90.62%] [G loss: 3.544411]\n",
      "epoch:46 step:36190 [D loss: 0.440676, acc.: 75.00%] [G loss: 3.032669]\n",
      "epoch:46 step:36191 [D loss: 0.311306, acc.: 85.16%] [G loss: 2.985217]\n",
      "epoch:46 step:36192 [D loss: 0.326743, acc.: 85.16%] [G loss: 3.651774]\n",
      "epoch:46 step:36193 [D loss: 0.291243, acc.: 85.94%] [G loss: 3.560127]\n",
      "epoch:46 step:36194 [D loss: 0.202389, acc.: 89.06%] [G loss: 2.641441]\n",
      "epoch:46 step:36195 [D loss: 0.329874, acc.: 86.72%] [G loss: 2.608386]\n",
      "epoch:46 step:36196 [D loss: 0.244776, acc.: 89.84%] [G loss: 2.506410]\n",
      "epoch:46 step:36197 [D loss: 0.303215, acc.: 88.28%] [G loss: 3.374689]\n",
      "epoch:46 step:36198 [D loss: 0.251134, acc.: 88.28%] [G loss: 3.548595]\n",
      "epoch:46 step:36199 [D loss: 0.257915, acc.: 89.84%] [G loss: 3.390193]\n",
      "epoch:46 step:36200 [D loss: 0.322232, acc.: 85.94%] [G loss: 2.960220]\n",
      "epoch:46 step:36201 [D loss: 0.403236, acc.: 85.94%] [G loss: 3.514723]\n",
      "epoch:46 step:36202 [D loss: 0.379836, acc.: 81.25%] [G loss: 2.738441]\n",
      "epoch:46 step:36203 [D loss: 0.390875, acc.: 81.25%] [G loss: 2.992215]\n",
      "epoch:46 step:36204 [D loss: 0.341593, acc.: 87.50%] [G loss: 3.845845]\n",
      "epoch:46 step:36205 [D loss: 0.313118, acc.: 85.16%] [G loss: 3.825702]\n",
      "epoch:46 step:36206 [D loss: 0.286615, acc.: 89.84%] [G loss: 3.061278]\n",
      "epoch:46 step:36207 [D loss: 0.231724, acc.: 89.84%] [G loss: 3.034042]\n",
      "epoch:46 step:36208 [D loss: 0.238131, acc.: 93.75%] [G loss: 3.021532]\n",
      "epoch:46 step:36209 [D loss: 0.226730, acc.: 92.19%] [G loss: 2.868607]\n",
      "epoch:46 step:36210 [D loss: 0.302562, acc.: 82.03%] [G loss: 3.089027]\n",
      "epoch:46 step:36211 [D loss: 0.340380, acc.: 83.59%] [G loss: 2.810975]\n",
      "epoch:46 step:36212 [D loss: 0.390512, acc.: 79.69%] [G loss: 4.262051]\n",
      "epoch:46 step:36213 [D loss: 0.209855, acc.: 92.19%] [G loss: 3.226707]\n",
      "epoch:46 step:36214 [D loss: 0.294131, acc.: 87.50%] [G loss: 4.051818]\n",
      "epoch:46 step:36215 [D loss: 0.303749, acc.: 85.94%] [G loss: 3.084188]\n",
      "epoch:46 step:36216 [D loss: 0.357238, acc.: 84.38%] [G loss: 3.225680]\n",
      "epoch:46 step:36217 [D loss: 0.315011, acc.: 85.94%] [G loss: 2.830318]\n",
      "epoch:46 step:36218 [D loss: 0.340793, acc.: 84.38%] [G loss: 3.088695]\n",
      "epoch:46 step:36219 [D loss: 0.333234, acc.: 85.16%] [G loss: 3.451083]\n",
      "epoch:46 step:36220 [D loss: 0.332287, acc.: 89.06%] [G loss: 3.674624]\n",
      "epoch:46 step:36221 [D loss: 0.383826, acc.: 83.59%] [G loss: 2.772161]\n",
      "epoch:46 step:36222 [D loss: 0.321267, acc.: 84.38%] [G loss: 4.201451]\n",
      "epoch:46 step:36223 [D loss: 0.315412, acc.: 88.28%] [G loss: 2.411359]\n",
      "epoch:46 step:36224 [D loss: 0.376337, acc.: 86.72%] [G loss: 2.700698]\n",
      "epoch:46 step:36225 [D loss: 0.357736, acc.: 82.81%] [G loss: 3.981453]\n",
      "epoch:46 step:36226 [D loss: 0.432153, acc.: 79.69%] [G loss: 4.158994]\n",
      "epoch:46 step:36227 [D loss: 0.310038, acc.: 85.94%] [G loss: 5.756436]\n",
      "epoch:46 step:36228 [D loss: 0.300979, acc.: 85.94%] [G loss: 3.854596]\n",
      "epoch:46 step:36229 [D loss: 0.216216, acc.: 89.84%] [G loss: 5.293477]\n",
      "epoch:46 step:36230 [D loss: 0.239025, acc.: 90.62%] [G loss: 4.409178]\n",
      "epoch:46 step:36231 [D loss: 0.340066, acc.: 85.94%] [G loss: 3.588432]\n",
      "epoch:46 step:36232 [D loss: 0.293510, acc.: 89.84%] [G loss: 2.706839]\n",
      "epoch:46 step:36233 [D loss: 0.268456, acc.: 87.50%] [G loss: 3.483849]\n",
      "epoch:46 step:36234 [D loss: 0.288104, acc.: 85.94%] [G loss: 2.788212]\n",
      "epoch:46 step:36235 [D loss: 0.228076, acc.: 92.19%] [G loss: 3.141583]\n",
      "epoch:46 step:36236 [D loss: 0.281079, acc.: 89.84%] [G loss: 3.805550]\n",
      "epoch:46 step:36237 [D loss: 0.236236, acc.: 88.28%] [G loss: 3.668673]\n",
      "epoch:46 step:36238 [D loss: 0.353610, acc.: 84.38%] [G loss: 4.434562]\n",
      "epoch:46 step:36239 [D loss: 0.327144, acc.: 83.59%] [G loss: 3.376702]\n",
      "epoch:46 step:36240 [D loss: 0.297013, acc.: 85.94%] [G loss: 4.014596]\n",
      "epoch:46 step:36241 [D loss: 0.387171, acc.: 83.59%] [G loss: 4.043451]\n",
      "epoch:46 step:36242 [D loss: 0.310225, acc.: 85.16%] [G loss: 4.770859]\n",
      "epoch:46 step:36243 [D loss: 0.722829, acc.: 71.09%] [G loss: 7.210376]\n",
      "epoch:46 step:36244 [D loss: 1.830154, acc.: 53.91%] [G loss: 5.619203]\n",
      "epoch:46 step:36245 [D loss: 0.538164, acc.: 75.78%] [G loss: 3.336685]\n",
      "epoch:46 step:36246 [D loss: 0.404597, acc.: 81.25%] [G loss: 5.187523]\n",
      "epoch:46 step:36247 [D loss: 0.418461, acc.: 85.94%] [G loss: 3.656149]\n",
      "epoch:46 step:36248 [D loss: 0.353223, acc.: 83.59%] [G loss: 6.326776]\n",
      "epoch:46 step:36249 [D loss: 0.368645, acc.: 80.47%] [G loss: 4.765670]\n",
      "epoch:46 step:36250 [D loss: 0.290464, acc.: 85.16%] [G loss: 4.060284]\n",
      "epoch:46 step:36251 [D loss: 0.214239, acc.: 91.41%] [G loss: 4.428043]\n",
      "epoch:46 step:36252 [D loss: 0.260093, acc.: 89.06%] [G loss: 4.544264]\n",
      "epoch:46 step:36253 [D loss: 0.216170, acc.: 88.28%] [G loss: 3.527065]\n",
      "epoch:46 step:36254 [D loss: 0.156815, acc.: 93.75%] [G loss: 3.491637]\n",
      "epoch:46 step:36255 [D loss: 0.229198, acc.: 89.06%] [G loss: 3.065814]\n",
      "epoch:46 step:36256 [D loss: 0.303180, acc.: 87.50%] [G loss: 4.534102]\n",
      "epoch:46 step:36257 [D loss: 0.371447, acc.: 85.16%] [G loss: 5.565796]\n",
      "epoch:46 step:36258 [D loss: 0.259266, acc.: 85.94%] [G loss: 3.558767]\n",
      "epoch:46 step:36259 [D loss: 0.227773, acc.: 92.97%] [G loss: 3.310898]\n",
      "epoch:46 step:36260 [D loss: 0.270089, acc.: 89.06%] [G loss: 2.986285]\n",
      "epoch:46 step:36261 [D loss: 0.366223, acc.: 80.47%] [G loss: 3.436601]\n",
      "epoch:46 step:36262 [D loss: 0.290112, acc.: 86.72%] [G loss: 3.219139]\n",
      "epoch:46 step:36263 [D loss: 0.316643, acc.: 85.16%] [G loss: 3.812126]\n",
      "epoch:46 step:36264 [D loss: 0.221841, acc.: 90.62%] [G loss: 3.900887]\n",
      "epoch:46 step:36265 [D loss: 0.264503, acc.: 88.28%] [G loss: 3.664061]\n",
      "epoch:46 step:36266 [D loss: 0.245263, acc.: 89.06%] [G loss: 3.248977]\n",
      "epoch:46 step:36267 [D loss: 0.272046, acc.: 89.84%] [G loss: 2.992795]\n",
      "epoch:46 step:36268 [D loss: 0.370178, acc.: 85.16%] [G loss: 4.594532]\n",
      "epoch:46 step:36269 [D loss: 0.391412, acc.: 80.47%] [G loss: 2.406429]\n",
      "epoch:46 step:36270 [D loss: 0.221668, acc.: 90.62%] [G loss: 2.739941]\n",
      "epoch:46 step:36271 [D loss: 0.294850, acc.: 84.38%] [G loss: 3.234245]\n",
      "epoch:46 step:36272 [D loss: 0.319553, acc.: 85.94%] [G loss: 2.887505]\n",
      "epoch:46 step:36273 [D loss: 0.332224, acc.: 82.03%] [G loss: 3.408932]\n",
      "epoch:46 step:36274 [D loss: 0.475165, acc.: 73.44%] [G loss: 3.927121]\n",
      "epoch:46 step:36275 [D loss: 0.405696, acc.: 82.03%] [G loss: 3.624392]\n",
      "epoch:46 step:36276 [D loss: 0.346134, acc.: 82.81%] [G loss: 3.001488]\n",
      "epoch:46 step:36277 [D loss: 0.296473, acc.: 86.72%] [G loss: 3.215148]\n",
      "epoch:46 step:36278 [D loss: 0.419649, acc.: 79.69%] [G loss: 2.344789]\n",
      "epoch:46 step:36279 [D loss: 0.309284, acc.: 82.81%] [G loss: 3.384779]\n",
      "epoch:46 step:36280 [D loss: 0.295672, acc.: 87.50%] [G loss: 2.750649]\n",
      "epoch:46 step:36281 [D loss: 0.242892, acc.: 85.94%] [G loss: 2.959127]\n",
      "epoch:46 step:36282 [D loss: 0.203695, acc.: 91.41%] [G loss: 3.424523]\n",
      "epoch:46 step:36283 [D loss: 0.291235, acc.: 89.06%] [G loss: 2.904726]\n",
      "epoch:46 step:36284 [D loss: 0.248727, acc.: 87.50%] [G loss: 2.743824]\n",
      "epoch:46 step:36285 [D loss: 0.335976, acc.: 84.38%] [G loss: 3.750858]\n",
      "epoch:46 step:36286 [D loss: 0.267344, acc.: 89.06%] [G loss: 4.084028]\n",
      "epoch:46 step:36287 [D loss: 0.239743, acc.: 92.19%] [G loss: 5.191621]\n",
      "epoch:46 step:36288 [D loss: 0.321138, acc.: 84.38%] [G loss: 3.808222]\n",
      "epoch:46 step:36289 [D loss: 0.293537, acc.: 87.50%] [G loss: 3.811180]\n",
      "epoch:46 step:36290 [D loss: 0.301823, acc.: 87.50%] [G loss: 3.334799]\n",
      "epoch:46 step:36291 [D loss: 0.273283, acc.: 89.06%] [G loss: 2.325377]\n",
      "epoch:46 step:36292 [D loss: 0.328179, acc.: 85.94%] [G loss: 2.653047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36293 [D loss: 0.312243, acc.: 86.72%] [G loss: 3.064260]\n",
      "epoch:46 step:36294 [D loss: 0.371131, acc.: 85.16%] [G loss: 2.753865]\n",
      "epoch:46 step:36295 [D loss: 0.289567, acc.: 90.62%] [G loss: 3.389727]\n",
      "epoch:46 step:36296 [D loss: 0.264512, acc.: 90.62%] [G loss: 3.048339]\n",
      "epoch:46 step:36297 [D loss: 0.237603, acc.: 89.06%] [G loss: 3.832369]\n",
      "epoch:46 step:36298 [D loss: 0.257968, acc.: 89.84%] [G loss: 3.289236]\n",
      "epoch:46 step:36299 [D loss: 0.308882, acc.: 85.94%] [G loss: 2.774145]\n",
      "epoch:46 step:36300 [D loss: 0.410297, acc.: 78.12%] [G loss: 2.565109]\n",
      "epoch:46 step:36301 [D loss: 0.236306, acc.: 89.84%] [G loss: 3.867469]\n",
      "epoch:46 step:36302 [D loss: 0.298099, acc.: 85.16%] [G loss: 3.009001]\n",
      "epoch:46 step:36303 [D loss: 0.244600, acc.: 89.84%] [G loss: 2.970612]\n",
      "epoch:46 step:36304 [D loss: 0.295054, acc.: 88.28%] [G loss: 2.751189]\n",
      "epoch:46 step:36305 [D loss: 0.263541, acc.: 85.94%] [G loss: 3.327593]\n",
      "epoch:46 step:36306 [D loss: 0.286204, acc.: 89.06%] [G loss: 2.594477]\n",
      "epoch:46 step:36307 [D loss: 0.263623, acc.: 92.19%] [G loss: 3.153081]\n",
      "epoch:46 step:36308 [D loss: 0.385769, acc.: 85.16%] [G loss: 4.823107]\n",
      "epoch:46 step:36309 [D loss: 0.280366, acc.: 89.06%] [G loss: 3.336320]\n",
      "epoch:46 step:36310 [D loss: 0.246768, acc.: 89.06%] [G loss: 3.638353]\n",
      "epoch:46 step:36311 [D loss: 0.225660, acc.: 90.62%] [G loss: 3.360542]\n",
      "epoch:46 step:36312 [D loss: 0.394656, acc.: 81.25%] [G loss: 3.174969]\n",
      "epoch:46 step:36313 [D loss: 0.266378, acc.: 88.28%] [G loss: 2.443734]\n",
      "epoch:46 step:36314 [D loss: 0.262341, acc.: 88.28%] [G loss: 3.743127]\n",
      "epoch:46 step:36315 [D loss: 0.322560, acc.: 86.72%] [G loss: 3.414783]\n",
      "epoch:46 step:36316 [D loss: 0.308602, acc.: 86.72%] [G loss: 2.601331]\n",
      "epoch:46 step:36317 [D loss: 0.264272, acc.: 88.28%] [G loss: 3.309532]\n",
      "epoch:46 step:36318 [D loss: 0.263793, acc.: 89.84%] [G loss: 3.297349]\n",
      "epoch:46 step:36319 [D loss: 0.316509, acc.: 88.28%] [G loss: 3.152943]\n",
      "epoch:46 step:36320 [D loss: 0.213593, acc.: 92.19%] [G loss: 5.217179]\n",
      "epoch:46 step:36321 [D loss: 0.420105, acc.: 79.69%] [G loss: 4.045418]\n",
      "epoch:46 step:36322 [D loss: 0.182129, acc.: 95.31%] [G loss: 4.021064]\n",
      "epoch:46 step:36323 [D loss: 0.278856, acc.: 88.28%] [G loss: 4.206539]\n",
      "epoch:46 step:36324 [D loss: 0.291306, acc.: 88.28%] [G loss: 3.506234]\n",
      "epoch:46 step:36325 [D loss: 0.292508, acc.: 91.41%] [G loss: 3.280237]\n",
      "epoch:46 step:36326 [D loss: 0.266298, acc.: 87.50%] [G loss: 3.204542]\n",
      "epoch:46 step:36327 [D loss: 0.275864, acc.: 86.72%] [G loss: 4.940349]\n",
      "epoch:46 step:36328 [D loss: 0.201408, acc.: 92.97%] [G loss: 5.478179]\n",
      "epoch:46 step:36329 [D loss: 0.207509, acc.: 93.75%] [G loss: 4.590231]\n",
      "epoch:46 step:36330 [D loss: 0.260297, acc.: 91.41%] [G loss: 4.856398]\n",
      "epoch:46 step:36331 [D loss: 0.268129, acc.: 89.06%] [G loss: 5.365824]\n",
      "epoch:46 step:36332 [D loss: 0.400288, acc.: 80.47%] [G loss: 4.054133]\n",
      "epoch:46 step:36333 [D loss: 0.285991, acc.: 87.50%] [G loss: 4.130928]\n",
      "epoch:46 step:36334 [D loss: 0.334336, acc.: 83.59%] [G loss: 3.620153]\n",
      "epoch:46 step:36335 [D loss: 0.243112, acc.: 90.62%] [G loss: 4.269580]\n",
      "epoch:46 step:36336 [D loss: 0.322185, acc.: 84.38%] [G loss: 3.337463]\n",
      "epoch:46 step:36337 [D loss: 0.352141, acc.: 85.16%] [G loss: 3.323931]\n",
      "epoch:46 step:36338 [D loss: 0.211418, acc.: 92.19%] [G loss: 4.102164]\n",
      "epoch:46 step:36339 [D loss: 0.249205, acc.: 89.84%] [G loss: 3.116039]\n",
      "epoch:46 step:36340 [D loss: 0.362736, acc.: 83.59%] [G loss: 3.380789]\n",
      "epoch:46 step:36341 [D loss: 0.288996, acc.: 88.28%] [G loss: 2.984834]\n",
      "epoch:46 step:36342 [D loss: 0.296787, acc.: 89.84%] [G loss: 3.138371]\n",
      "epoch:46 step:36343 [D loss: 0.301659, acc.: 84.38%] [G loss: 3.278524]\n",
      "epoch:46 step:36344 [D loss: 0.203068, acc.: 91.41%] [G loss: 3.592028]\n",
      "epoch:46 step:36345 [D loss: 0.225689, acc.: 90.62%] [G loss: 3.103261]\n",
      "epoch:46 step:36346 [D loss: 0.266058, acc.: 89.84%] [G loss: 3.065874]\n",
      "epoch:46 step:36347 [D loss: 0.255584, acc.: 89.84%] [G loss: 4.154441]\n",
      "epoch:46 step:36348 [D loss: 0.333342, acc.: 88.28%] [G loss: 2.557493]\n",
      "epoch:46 step:36349 [D loss: 0.314037, acc.: 86.72%] [G loss: 2.950457]\n",
      "epoch:46 step:36350 [D loss: 0.244867, acc.: 89.84%] [G loss: 2.576482]\n",
      "epoch:46 step:36351 [D loss: 0.379197, acc.: 81.25%] [G loss: 2.311076]\n",
      "epoch:46 step:36352 [D loss: 0.307066, acc.: 85.94%] [G loss: 2.429642]\n",
      "epoch:46 step:36353 [D loss: 0.233479, acc.: 92.19%] [G loss: 3.082600]\n",
      "epoch:46 step:36354 [D loss: 0.324181, acc.: 85.16%] [G loss: 3.199704]\n",
      "epoch:46 step:36355 [D loss: 0.323989, acc.: 85.16%] [G loss: 2.278087]\n",
      "epoch:46 step:36356 [D loss: 0.411685, acc.: 85.16%] [G loss: 2.998848]\n",
      "epoch:46 step:36357 [D loss: 0.423386, acc.: 80.47%] [G loss: 4.148653]\n",
      "epoch:46 step:36358 [D loss: 0.488737, acc.: 78.91%] [G loss: 5.195066]\n",
      "epoch:46 step:36359 [D loss: 0.323048, acc.: 88.28%] [G loss: 5.393093]\n",
      "epoch:46 step:36360 [D loss: 0.277360, acc.: 92.19%] [G loss: 4.913788]\n",
      "epoch:46 step:36361 [D loss: 0.355233, acc.: 83.59%] [G loss: 3.989556]\n",
      "epoch:46 step:36362 [D loss: 0.286553, acc.: 85.16%] [G loss: 4.038271]\n",
      "epoch:46 step:36363 [D loss: 0.205431, acc.: 92.19%] [G loss: 3.250025]\n",
      "epoch:46 step:36364 [D loss: 0.230127, acc.: 90.62%] [G loss: 4.186205]\n",
      "epoch:46 step:36365 [D loss: 0.252522, acc.: 86.72%] [G loss: 4.375137]\n",
      "epoch:46 step:36366 [D loss: 0.211122, acc.: 92.19%] [G loss: 4.004663]\n",
      "epoch:46 step:36367 [D loss: 0.411731, acc.: 81.25%] [G loss: 2.634219]\n",
      "epoch:46 step:36368 [D loss: 0.325718, acc.: 87.50%] [G loss: 2.841341]\n",
      "epoch:46 step:36369 [D loss: 0.234221, acc.: 87.50%] [G loss: 3.932643]\n",
      "epoch:46 step:36370 [D loss: 0.489643, acc.: 80.47%] [G loss: 2.715499]\n",
      "epoch:46 step:36371 [D loss: 0.366997, acc.: 82.03%] [G loss: 4.017170]\n",
      "epoch:46 step:36372 [D loss: 0.265963, acc.: 88.28%] [G loss: 3.574344]\n",
      "epoch:46 step:36373 [D loss: 0.282244, acc.: 87.50%] [G loss: 3.412119]\n",
      "epoch:46 step:36374 [D loss: 0.389000, acc.: 85.94%] [G loss: 3.938180]\n",
      "epoch:46 step:36375 [D loss: 0.247897, acc.: 89.06%] [G loss: 2.804395]\n",
      "epoch:46 step:36376 [D loss: 0.309222, acc.: 82.81%] [G loss: 5.074946]\n",
      "epoch:46 step:36377 [D loss: 0.237424, acc.: 89.06%] [G loss: 3.679651]\n",
      "epoch:46 step:36378 [D loss: 0.255196, acc.: 88.28%] [G loss: 4.704310]\n",
      "epoch:46 step:36379 [D loss: 0.278587, acc.: 89.06%] [G loss: 4.356884]\n",
      "epoch:46 step:36380 [D loss: 0.220267, acc.: 91.41%] [G loss: 4.734096]\n",
      "epoch:46 step:36381 [D loss: 0.237183, acc.: 90.62%] [G loss: 3.978535]\n",
      "epoch:46 step:36382 [D loss: 0.215747, acc.: 90.62%] [G loss: 3.796268]\n",
      "epoch:46 step:36383 [D loss: 0.257374, acc.: 85.16%] [G loss: 3.123333]\n",
      "epoch:46 step:36384 [D loss: 0.229669, acc.: 89.06%] [G loss: 3.406657]\n",
      "epoch:46 step:36385 [D loss: 0.367343, acc.: 82.81%] [G loss: 3.776893]\n",
      "epoch:46 step:36386 [D loss: 0.397640, acc.: 82.03%] [G loss: 3.153681]\n",
      "epoch:46 step:36387 [D loss: 0.190623, acc.: 89.06%] [G loss: 2.778648]\n",
      "epoch:46 step:36388 [D loss: 0.254317, acc.: 89.06%] [G loss: 3.353647]\n",
      "epoch:46 step:36389 [D loss: 0.365342, acc.: 82.81%] [G loss: 3.656598]\n",
      "epoch:46 step:36390 [D loss: 0.287255, acc.: 91.41%] [G loss: 3.425575]\n",
      "epoch:46 step:36391 [D loss: 0.255901, acc.: 87.50%] [G loss: 4.677351]\n",
      "epoch:46 step:36392 [D loss: 0.284402, acc.: 83.59%] [G loss: 6.070598]\n",
      "epoch:46 step:36393 [D loss: 0.260693, acc.: 87.50%] [G loss: 3.703451]\n",
      "epoch:46 step:36394 [D loss: 0.280509, acc.: 90.62%] [G loss: 3.622279]\n",
      "epoch:46 step:36395 [D loss: 0.266858, acc.: 84.38%] [G loss: 4.808487]\n",
      "epoch:46 step:36396 [D loss: 0.276050, acc.: 86.72%] [G loss: 4.584354]\n",
      "epoch:46 step:36397 [D loss: 0.247901, acc.: 88.28%] [G loss: 4.694111]\n",
      "epoch:46 step:36398 [D loss: 0.252870, acc.: 89.06%] [G loss: 4.289424]\n",
      "epoch:46 step:36399 [D loss: 0.283411, acc.: 85.94%] [G loss: 4.237196]\n",
      "epoch:46 step:36400 [D loss: 0.383492, acc.: 83.59%] [G loss: 3.206636]\n",
      "epoch:46 step:36401 [D loss: 0.249347, acc.: 89.06%] [G loss: 4.052835]\n",
      "epoch:46 step:36402 [D loss: 0.333971, acc.: 82.81%] [G loss: 3.124319]\n",
      "epoch:46 step:36403 [D loss: 0.194561, acc.: 91.41%] [G loss: 3.851620]\n",
      "epoch:46 step:36404 [D loss: 0.318282, acc.: 87.50%] [G loss: 3.033463]\n",
      "epoch:46 step:36405 [D loss: 0.287734, acc.: 85.94%] [G loss: 3.358718]\n",
      "epoch:46 step:36406 [D loss: 0.198018, acc.: 94.53%] [G loss: 4.381529]\n",
      "epoch:46 step:36407 [D loss: 0.256992, acc.: 86.72%] [G loss: 3.249214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36408 [D loss: 0.260678, acc.: 87.50%] [G loss: 5.114948]\n",
      "epoch:46 step:36409 [D loss: 0.237586, acc.: 90.62%] [G loss: 3.854623]\n",
      "epoch:46 step:36410 [D loss: 0.253590, acc.: 88.28%] [G loss: 4.713362]\n",
      "epoch:46 step:36411 [D loss: 0.167984, acc.: 93.75%] [G loss: 3.748179]\n",
      "epoch:46 step:36412 [D loss: 0.236395, acc.: 88.28%] [G loss: 4.620451]\n",
      "epoch:46 step:36413 [D loss: 0.290681, acc.: 84.38%] [G loss: 3.625817]\n",
      "epoch:46 step:36414 [D loss: 0.239969, acc.: 88.28%] [G loss: 3.191530]\n",
      "epoch:46 step:36415 [D loss: 0.279779, acc.: 89.84%] [G loss: 4.637495]\n",
      "epoch:46 step:36416 [D loss: 0.207355, acc.: 89.84%] [G loss: 3.469511]\n",
      "epoch:46 step:36417 [D loss: 0.422190, acc.: 82.81%] [G loss: 5.175757]\n",
      "epoch:46 step:36418 [D loss: 0.398017, acc.: 79.69%] [G loss: 3.563354]\n",
      "epoch:46 step:36419 [D loss: 0.272581, acc.: 85.94%] [G loss: 4.992356]\n",
      "epoch:46 step:36420 [D loss: 0.344706, acc.: 86.72%] [G loss: 4.279178]\n",
      "epoch:46 step:36421 [D loss: 0.316144, acc.: 89.06%] [G loss: 4.076089]\n",
      "epoch:46 step:36422 [D loss: 0.326514, acc.: 86.72%] [G loss: 3.424453]\n",
      "epoch:46 step:36423 [D loss: 0.372533, acc.: 83.59%] [G loss: 3.423640]\n",
      "epoch:46 step:36424 [D loss: 0.301044, acc.: 85.94%] [G loss: 3.532129]\n",
      "epoch:46 step:36425 [D loss: 0.317500, acc.: 85.16%] [G loss: 3.692883]\n",
      "epoch:46 step:36426 [D loss: 0.362473, acc.: 84.38%] [G loss: 4.476798]\n",
      "epoch:46 step:36427 [D loss: 0.621894, acc.: 72.66%] [G loss: 3.803071]\n",
      "epoch:46 step:36428 [D loss: 0.387350, acc.: 85.16%] [G loss: 3.909988]\n",
      "epoch:46 step:36429 [D loss: 0.334852, acc.: 86.72%] [G loss: 3.673805]\n",
      "epoch:46 step:36430 [D loss: 0.396251, acc.: 83.59%] [G loss: 5.820712]\n",
      "epoch:46 step:36431 [D loss: 0.657439, acc.: 71.09%] [G loss: 5.201773]\n",
      "epoch:46 step:36432 [D loss: 1.315162, acc.: 57.81%] [G loss: 6.898281]\n",
      "epoch:46 step:36433 [D loss: 1.209705, acc.: 73.44%] [G loss: 5.376748]\n",
      "epoch:46 step:36434 [D loss: 0.842553, acc.: 67.97%] [G loss: 3.426691]\n",
      "epoch:46 step:36435 [D loss: 0.629041, acc.: 76.56%] [G loss: 2.504995]\n",
      "epoch:46 step:36436 [D loss: 0.536970, acc.: 79.69%] [G loss: 5.462784]\n",
      "epoch:46 step:36437 [D loss: 0.420049, acc.: 83.59%] [G loss: 2.012786]\n",
      "epoch:46 step:36438 [D loss: 0.338593, acc.: 84.38%] [G loss: 2.965586]\n",
      "epoch:46 step:36439 [D loss: 0.442526, acc.: 82.81%] [G loss: 3.096919]\n",
      "epoch:46 step:36440 [D loss: 0.459606, acc.: 78.12%] [G loss: 3.719043]\n",
      "epoch:46 step:36441 [D loss: 0.432812, acc.: 78.91%] [G loss: 3.794584]\n",
      "epoch:46 step:36442 [D loss: 0.425438, acc.: 79.69%] [G loss: 3.412911]\n",
      "epoch:46 step:36443 [D loss: 0.401588, acc.: 83.59%] [G loss: 2.309545]\n",
      "epoch:46 step:36444 [D loss: 0.287724, acc.: 85.94%] [G loss: 3.180931]\n",
      "epoch:46 step:36445 [D loss: 0.317271, acc.: 84.38%] [G loss: 3.170074]\n",
      "epoch:46 step:36446 [D loss: 0.358484, acc.: 85.94%] [G loss: 2.536133]\n",
      "epoch:46 step:36447 [D loss: 0.293056, acc.: 88.28%] [G loss: 2.909096]\n",
      "epoch:46 step:36448 [D loss: 0.373001, acc.: 82.81%] [G loss: 2.669721]\n",
      "epoch:46 step:36449 [D loss: 0.316244, acc.: 87.50%] [G loss: 2.716680]\n",
      "epoch:46 step:36450 [D loss: 0.368466, acc.: 81.25%] [G loss: 2.540673]\n",
      "epoch:46 step:36451 [D loss: 0.223787, acc.: 90.62%] [G loss: 2.816040]\n",
      "epoch:46 step:36452 [D loss: 0.505707, acc.: 78.12%] [G loss: 2.566274]\n",
      "epoch:46 step:36453 [D loss: 0.283693, acc.: 84.38%] [G loss: 3.431331]\n",
      "epoch:46 step:36454 [D loss: 0.182236, acc.: 95.31%] [G loss: 2.358273]\n",
      "epoch:46 step:36455 [D loss: 0.277279, acc.: 87.50%] [G loss: 2.666314]\n",
      "epoch:46 step:36456 [D loss: 0.444921, acc.: 76.56%] [G loss: 2.582406]\n",
      "epoch:46 step:36457 [D loss: 0.395299, acc.: 84.38%] [G loss: 2.873910]\n",
      "epoch:46 step:36458 [D loss: 0.369531, acc.: 84.38%] [G loss: 2.990726]\n",
      "epoch:46 step:36459 [D loss: 0.287405, acc.: 85.16%] [G loss: 3.010233]\n",
      "epoch:46 step:36460 [D loss: 0.280362, acc.: 88.28%] [G loss: 3.395591]\n",
      "epoch:46 step:36461 [D loss: 0.305197, acc.: 85.94%] [G loss: 3.026337]\n",
      "epoch:46 step:36462 [D loss: 0.275874, acc.: 89.06%] [G loss: 2.753283]\n",
      "epoch:46 step:36463 [D loss: 0.351635, acc.: 81.25%] [G loss: 2.633183]\n",
      "epoch:46 step:36464 [D loss: 0.316471, acc.: 82.81%] [G loss: 2.506623]\n",
      "epoch:46 step:36465 [D loss: 0.415000, acc.: 82.81%] [G loss: 3.007762]\n",
      "epoch:46 step:36466 [D loss: 0.344067, acc.: 85.94%] [G loss: 4.115127]\n",
      "epoch:46 step:36467 [D loss: 0.296392, acc.: 85.94%] [G loss: 2.193215]\n",
      "epoch:46 step:36468 [D loss: 0.263528, acc.: 86.72%] [G loss: 3.761374]\n",
      "epoch:46 step:36469 [D loss: 0.365940, acc.: 85.94%] [G loss: 3.254140]\n",
      "epoch:46 step:36470 [D loss: 0.266689, acc.: 89.06%] [G loss: 4.188450]\n",
      "epoch:46 step:36471 [D loss: 0.385539, acc.: 80.47%] [G loss: 2.465013]\n",
      "epoch:46 step:36472 [D loss: 0.290176, acc.: 86.72%] [G loss: 2.578946]\n",
      "epoch:46 step:36473 [D loss: 0.277504, acc.: 85.16%] [G loss: 2.984199]\n",
      "epoch:46 step:36474 [D loss: 0.321852, acc.: 88.28%] [G loss: 2.768559]\n",
      "epoch:46 step:36475 [D loss: 0.385532, acc.: 81.25%] [G loss: 3.348536]\n",
      "epoch:46 step:36476 [D loss: 0.275368, acc.: 88.28%] [G loss: 3.056112]\n",
      "epoch:46 step:36477 [D loss: 0.309468, acc.: 89.84%] [G loss: 2.669279]\n",
      "epoch:46 step:36478 [D loss: 0.330437, acc.: 84.38%] [G loss: 2.977528]\n",
      "epoch:46 step:36479 [D loss: 0.310858, acc.: 89.06%] [G loss: 2.632397]\n",
      "epoch:46 step:36480 [D loss: 0.307471, acc.: 85.94%] [G loss: 3.106432]\n",
      "epoch:46 step:36481 [D loss: 0.268087, acc.: 86.72%] [G loss: 3.739126]\n",
      "epoch:46 step:36482 [D loss: 0.281564, acc.: 87.50%] [G loss: 3.486926]\n",
      "epoch:46 step:36483 [D loss: 0.273250, acc.: 90.62%] [G loss: 3.039150]\n",
      "epoch:46 step:36484 [D loss: 0.268467, acc.: 88.28%] [G loss: 3.116486]\n",
      "epoch:46 step:36485 [D loss: 0.287112, acc.: 88.28%] [G loss: 3.594586]\n",
      "epoch:46 step:36486 [D loss: 0.326639, acc.: 86.72%] [G loss: 3.906795]\n",
      "epoch:46 step:36487 [D loss: 0.323036, acc.: 84.38%] [G loss: 3.154536]\n",
      "epoch:46 step:36488 [D loss: 0.301303, acc.: 85.16%] [G loss: 2.713687]\n",
      "epoch:46 step:36489 [D loss: 0.277352, acc.: 92.19%] [G loss: 2.687946]\n",
      "epoch:46 step:36490 [D loss: 0.290037, acc.: 85.94%] [G loss: 2.799495]\n",
      "epoch:46 step:36491 [D loss: 0.259339, acc.: 86.72%] [G loss: 2.864173]\n",
      "epoch:46 step:36492 [D loss: 0.404179, acc.: 83.59%] [G loss: 2.730801]\n",
      "epoch:46 step:36493 [D loss: 0.279587, acc.: 89.06%] [G loss: 3.885563]\n",
      "epoch:46 step:36494 [D loss: 0.315130, acc.: 86.72%] [G loss: 4.070469]\n",
      "epoch:46 step:36495 [D loss: 0.341280, acc.: 88.28%] [G loss: 3.840845]\n",
      "epoch:46 step:36496 [D loss: 0.275007, acc.: 89.84%] [G loss: 3.067188]\n",
      "epoch:46 step:36497 [D loss: 0.270780, acc.: 85.94%] [G loss: 5.280198]\n",
      "epoch:46 step:36498 [D loss: 0.296582, acc.: 88.28%] [G loss: 3.961598]\n",
      "epoch:46 step:36499 [D loss: 0.247525, acc.: 91.41%] [G loss: 3.508580]\n",
      "epoch:46 step:36500 [D loss: 0.230473, acc.: 88.28%] [G loss: 3.362745]\n",
      "epoch:46 step:36501 [D loss: 0.268922, acc.: 87.50%] [G loss: 3.871942]\n",
      "epoch:46 step:36502 [D loss: 0.334279, acc.: 85.16%] [G loss: 3.034971]\n",
      "epoch:46 step:36503 [D loss: 0.342603, acc.: 81.25%] [G loss: 3.318750]\n",
      "epoch:46 step:36504 [D loss: 0.333423, acc.: 83.59%] [G loss: 3.485841]\n",
      "epoch:46 step:36505 [D loss: 0.303370, acc.: 85.94%] [G loss: 2.848274]\n",
      "epoch:46 step:36506 [D loss: 0.452850, acc.: 78.91%] [G loss: 2.339579]\n",
      "epoch:46 step:36507 [D loss: 0.378861, acc.: 84.38%] [G loss: 2.435802]\n",
      "epoch:46 step:36508 [D loss: 0.337496, acc.: 85.16%] [G loss: 2.719936]\n",
      "epoch:46 step:36509 [D loss: 0.303027, acc.: 85.16%] [G loss: 3.023863]\n",
      "epoch:46 step:36510 [D loss: 0.420787, acc.: 80.47%] [G loss: 2.581879]\n",
      "epoch:46 step:36511 [D loss: 0.307595, acc.: 85.94%] [G loss: 2.358778]\n",
      "epoch:46 step:36512 [D loss: 0.341761, acc.: 87.50%] [G loss: 2.142432]\n",
      "epoch:46 step:36513 [D loss: 0.291468, acc.: 89.84%] [G loss: 2.708842]\n",
      "epoch:46 step:36514 [D loss: 0.265477, acc.: 90.62%] [G loss: 2.870978]\n",
      "epoch:46 step:36515 [D loss: 0.353229, acc.: 82.81%] [G loss: 3.520800]\n",
      "epoch:46 step:36516 [D loss: 0.410887, acc.: 82.81%] [G loss: 3.162317]\n",
      "epoch:46 step:36517 [D loss: 0.306421, acc.: 87.50%] [G loss: 5.258228]\n",
      "epoch:46 step:36518 [D loss: 0.293399, acc.: 85.94%] [G loss: 2.745984]\n",
      "epoch:46 step:36519 [D loss: 0.341512, acc.: 82.81%] [G loss: 2.924147]\n",
      "epoch:46 step:36520 [D loss: 0.215021, acc.: 89.84%] [G loss: 4.564453]\n",
      "epoch:46 step:36521 [D loss: 0.382627, acc.: 82.81%] [G loss: 3.284242]\n",
      "epoch:46 step:36522 [D loss: 0.458168, acc.: 82.81%] [G loss: 3.274009]\n",
      "epoch:46 step:36523 [D loss: 0.296043, acc.: 88.28%] [G loss: 2.770188]\n",
      "epoch:46 step:36524 [D loss: 0.307573, acc.: 85.94%] [G loss: 3.299150]\n",
      "epoch:46 step:36525 [D loss: 0.301748, acc.: 86.72%] [G loss: 2.934271]\n",
      "epoch:46 step:36526 [D loss: 0.496990, acc.: 75.00%] [G loss: 2.520586]\n",
      "epoch:46 step:36527 [D loss: 0.276543, acc.: 88.28%] [G loss: 3.620666]\n",
      "epoch:46 step:36528 [D loss: 0.316610, acc.: 82.81%] [G loss: 4.351217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36529 [D loss: 0.236358, acc.: 91.41%] [G loss: 4.288665]\n",
      "epoch:46 step:36530 [D loss: 0.237177, acc.: 90.62%] [G loss: 3.271037]\n",
      "epoch:46 step:36531 [D loss: 0.355430, acc.: 84.38%] [G loss: 4.341135]\n",
      "epoch:46 step:36532 [D loss: 0.350485, acc.: 82.81%] [G loss: 2.383475]\n",
      "epoch:46 step:36533 [D loss: 0.310209, acc.: 82.03%] [G loss: 4.044489]\n",
      "epoch:46 step:36534 [D loss: 0.271491, acc.: 85.16%] [G loss: 4.048232]\n",
      "epoch:46 step:36535 [D loss: 0.277245, acc.: 88.28%] [G loss: 3.552157]\n",
      "epoch:46 step:36536 [D loss: 0.323089, acc.: 85.16%] [G loss: 3.587926]\n",
      "epoch:46 step:36537 [D loss: 0.403968, acc.: 83.59%] [G loss: 3.480225]\n",
      "epoch:46 step:36538 [D loss: 0.355389, acc.: 82.81%] [G loss: 2.798276]\n",
      "epoch:46 step:36539 [D loss: 0.344833, acc.: 85.94%] [G loss: 3.927338]\n",
      "epoch:46 step:36540 [D loss: 0.334046, acc.: 85.94%] [G loss: 3.335524]\n",
      "epoch:46 step:36541 [D loss: 0.217646, acc.: 89.84%] [G loss: 3.881379]\n",
      "epoch:46 step:36542 [D loss: 0.227933, acc.: 89.06%] [G loss: 3.612772]\n",
      "epoch:46 step:36543 [D loss: 0.375917, acc.: 85.16%] [G loss: 3.205698]\n",
      "epoch:46 step:36544 [D loss: 0.390781, acc.: 81.25%] [G loss: 2.961989]\n",
      "epoch:46 step:36545 [D loss: 0.378524, acc.: 82.81%] [G loss: 3.356185]\n",
      "epoch:46 step:36546 [D loss: 0.253995, acc.: 86.72%] [G loss: 3.956910]\n",
      "epoch:46 step:36547 [D loss: 0.262298, acc.: 90.62%] [G loss: 3.010191]\n",
      "epoch:46 step:36548 [D loss: 0.213045, acc.: 91.41%] [G loss: 3.096742]\n",
      "epoch:46 step:36549 [D loss: 0.328945, acc.: 85.16%] [G loss: 3.402571]\n",
      "epoch:46 step:36550 [D loss: 0.302832, acc.: 88.28%] [G loss: 3.816137]\n",
      "epoch:46 step:36551 [D loss: 0.306625, acc.: 84.38%] [G loss: 3.283708]\n",
      "epoch:46 step:36552 [D loss: 0.288008, acc.: 88.28%] [G loss: 3.286247]\n",
      "epoch:46 step:36553 [D loss: 0.386005, acc.: 80.47%] [G loss: 3.985758]\n",
      "epoch:46 step:36554 [D loss: 0.288941, acc.: 89.06%] [G loss: 3.812065]\n",
      "epoch:46 step:36555 [D loss: 0.253484, acc.: 89.84%] [G loss: 3.191998]\n",
      "epoch:46 step:36556 [D loss: 0.296817, acc.: 85.16%] [G loss: 3.310657]\n",
      "epoch:46 step:36557 [D loss: 0.391800, acc.: 84.38%] [G loss: 2.715934]\n",
      "epoch:46 step:36558 [D loss: 0.221117, acc.: 89.84%] [G loss: 3.762665]\n",
      "epoch:46 step:36559 [D loss: 0.355820, acc.: 84.38%] [G loss: 3.028663]\n",
      "epoch:46 step:36560 [D loss: 0.351362, acc.: 82.03%] [G loss: 3.435234]\n",
      "epoch:46 step:36561 [D loss: 0.266108, acc.: 90.62%] [G loss: 3.301630]\n",
      "epoch:46 step:36562 [D loss: 0.396611, acc.: 84.38%] [G loss: 3.425003]\n",
      "epoch:46 step:36563 [D loss: 0.208086, acc.: 91.41%] [G loss: 3.033393]\n",
      "epoch:46 step:36564 [D loss: 0.441852, acc.: 78.12%] [G loss: 2.825119]\n",
      "epoch:46 step:36565 [D loss: 0.313651, acc.: 87.50%] [G loss: 4.001806]\n",
      "epoch:46 step:36566 [D loss: 0.283844, acc.: 85.16%] [G loss: 4.974785]\n",
      "epoch:46 step:36567 [D loss: 0.403838, acc.: 80.47%] [G loss: 4.106548]\n",
      "epoch:46 step:36568 [D loss: 0.365432, acc.: 85.16%] [G loss: 3.967596]\n",
      "epoch:46 step:36569 [D loss: 0.434905, acc.: 80.47%] [G loss: 4.829391]\n",
      "epoch:46 step:36570 [D loss: 0.289607, acc.: 84.38%] [G loss: 3.829542]\n",
      "epoch:46 step:36571 [D loss: 0.287680, acc.: 88.28%] [G loss: 3.921924]\n",
      "epoch:46 step:36572 [D loss: 0.480282, acc.: 78.12%] [G loss: 3.934705]\n",
      "epoch:46 step:36573 [D loss: 0.289159, acc.: 87.50%] [G loss: 3.578720]\n",
      "epoch:46 step:36574 [D loss: 0.385826, acc.: 80.47%] [G loss: 2.965621]\n",
      "epoch:46 step:36575 [D loss: 0.379742, acc.: 82.03%] [G loss: 2.834394]\n",
      "epoch:46 step:36576 [D loss: 0.418149, acc.: 78.91%] [G loss: 4.035805]\n",
      "epoch:46 step:36577 [D loss: 0.345401, acc.: 83.59%] [G loss: 4.075566]\n",
      "epoch:46 step:36578 [D loss: 0.390187, acc.: 82.81%] [G loss: 3.968129]\n",
      "epoch:46 step:36579 [D loss: 0.345602, acc.: 84.38%] [G loss: 4.166394]\n",
      "epoch:46 step:36580 [D loss: 0.454125, acc.: 73.44%] [G loss: 4.043418]\n",
      "epoch:46 step:36581 [D loss: 0.580570, acc.: 74.22%] [G loss: 3.525811]\n",
      "epoch:46 step:36582 [D loss: 0.275545, acc.: 88.28%] [G loss: 2.738546]\n",
      "epoch:46 step:36583 [D loss: 0.347945, acc.: 82.03%] [G loss: 3.450902]\n",
      "epoch:46 step:36584 [D loss: 0.307257, acc.: 86.72%] [G loss: 3.387787]\n",
      "epoch:46 step:36585 [D loss: 0.269066, acc.: 88.28%] [G loss: 3.740029]\n",
      "epoch:46 step:36586 [D loss: 0.339929, acc.: 85.94%] [G loss: 3.774923]\n",
      "epoch:46 step:36587 [D loss: 0.304862, acc.: 88.28%] [G loss: 4.067308]\n",
      "epoch:46 step:36588 [D loss: 0.399711, acc.: 81.25%] [G loss: 3.152470]\n",
      "epoch:46 step:36589 [D loss: 0.258445, acc.: 88.28%] [G loss: 3.891328]\n",
      "epoch:46 step:36590 [D loss: 0.332435, acc.: 82.81%] [G loss: 3.432783]\n",
      "epoch:46 step:36591 [D loss: 0.269829, acc.: 89.84%] [G loss: 3.550973]\n",
      "epoch:46 step:36592 [D loss: 0.291999, acc.: 87.50%] [G loss: 3.356164]\n",
      "epoch:46 step:36593 [D loss: 0.300776, acc.: 89.06%] [G loss: 3.895050]\n",
      "epoch:46 step:36594 [D loss: 0.357240, acc.: 83.59%] [G loss: 2.442844]\n",
      "epoch:46 step:36595 [D loss: 0.328370, acc.: 83.59%] [G loss: 3.090318]\n",
      "epoch:46 step:36596 [D loss: 0.292961, acc.: 87.50%] [G loss: 2.944388]\n",
      "epoch:46 step:36597 [D loss: 0.349681, acc.: 83.59%] [G loss: 3.233971]\n",
      "epoch:46 step:36598 [D loss: 0.349948, acc.: 84.38%] [G loss: 2.879408]\n",
      "epoch:46 step:36599 [D loss: 0.295344, acc.: 85.94%] [G loss: 2.418493]\n",
      "epoch:46 step:36600 [D loss: 0.305508, acc.: 85.16%] [G loss: 4.012469]\n",
      "epoch:46 step:36601 [D loss: 0.428733, acc.: 76.56%] [G loss: 3.831198]\n",
      "epoch:46 step:36602 [D loss: 0.422178, acc.: 79.69%] [G loss: 3.360687]\n",
      "epoch:46 step:36603 [D loss: 0.259065, acc.: 89.06%] [G loss: 4.662975]\n",
      "epoch:46 step:36604 [D loss: 0.354774, acc.: 84.38%] [G loss: 4.480795]\n",
      "epoch:46 step:36605 [D loss: 0.447084, acc.: 80.47%] [G loss: 2.810935]\n",
      "epoch:46 step:36606 [D loss: 0.277840, acc.: 89.06%] [G loss: 4.278647]\n",
      "epoch:46 step:36607 [D loss: 0.266869, acc.: 88.28%] [G loss: 4.312926]\n",
      "epoch:46 step:36608 [D loss: 0.382212, acc.: 83.59%] [G loss: 3.881042]\n",
      "epoch:46 step:36609 [D loss: 0.320959, acc.: 84.38%] [G loss: 3.610145]\n",
      "epoch:46 step:36610 [D loss: 0.322882, acc.: 86.72%] [G loss: 3.431694]\n",
      "epoch:46 step:36611 [D loss: 0.324415, acc.: 88.28%] [G loss: 3.501785]\n",
      "epoch:46 step:36612 [D loss: 0.332770, acc.: 82.03%] [G loss: 3.134874]\n",
      "epoch:46 step:36613 [D loss: 0.380276, acc.: 80.47%] [G loss: 3.748006]\n",
      "epoch:46 step:36614 [D loss: 0.313578, acc.: 88.28%] [G loss: 3.709146]\n",
      "epoch:46 step:36615 [D loss: 0.336727, acc.: 87.50%] [G loss: 2.941095]\n",
      "epoch:46 step:36616 [D loss: 0.298994, acc.: 89.06%] [G loss: 3.004506]\n",
      "epoch:46 step:36617 [D loss: 0.343376, acc.: 82.81%] [G loss: 2.470375]\n",
      "epoch:46 step:36618 [D loss: 0.416797, acc.: 81.25%] [G loss: 4.948727]\n",
      "epoch:46 step:36619 [D loss: 0.312185, acc.: 81.25%] [G loss: 3.110581]\n",
      "epoch:46 step:36620 [D loss: 0.279075, acc.: 87.50%] [G loss: 3.536734]\n",
      "epoch:46 step:36621 [D loss: 0.289368, acc.: 85.94%] [G loss: 3.279852]\n",
      "epoch:46 step:36622 [D loss: 0.287037, acc.: 89.06%] [G loss: 3.252877]\n",
      "epoch:46 step:36623 [D loss: 0.412508, acc.: 78.91%] [G loss: 3.270014]\n",
      "epoch:46 step:36624 [D loss: 0.289891, acc.: 88.28%] [G loss: 2.748264]\n",
      "epoch:46 step:36625 [D loss: 0.409053, acc.: 78.12%] [G loss: 2.509898]\n",
      "epoch:46 step:36626 [D loss: 0.274232, acc.: 89.84%] [G loss: 2.331593]\n",
      "epoch:46 step:36627 [D loss: 0.379812, acc.: 85.94%] [G loss: 2.815327]\n",
      "epoch:46 step:36628 [D loss: 0.310208, acc.: 89.06%] [G loss: 2.308095]\n",
      "epoch:46 step:36629 [D loss: 0.376970, acc.: 85.16%] [G loss: 2.329138]\n",
      "epoch:46 step:36630 [D loss: 0.448271, acc.: 78.91%] [G loss: 3.093103]\n",
      "epoch:46 step:36631 [D loss: 0.323880, acc.: 87.50%] [G loss: 3.419245]\n",
      "epoch:46 step:36632 [D loss: 0.329203, acc.: 84.38%] [G loss: 2.589730]\n",
      "epoch:46 step:36633 [D loss: 0.374876, acc.: 84.38%] [G loss: 2.532861]\n",
      "epoch:46 step:36634 [D loss: 0.360223, acc.: 80.47%] [G loss: 2.472306]\n",
      "epoch:46 step:36635 [D loss: 0.300506, acc.: 87.50%] [G loss: 3.832199]\n",
      "epoch:46 step:36636 [D loss: 0.241751, acc.: 89.06%] [G loss: 3.364754]\n",
      "epoch:46 step:36637 [D loss: 0.326175, acc.: 84.38%] [G loss: 3.147443]\n",
      "epoch:46 step:36638 [D loss: 0.432034, acc.: 80.47%] [G loss: 4.980910]\n",
      "epoch:46 step:36639 [D loss: 0.485062, acc.: 76.56%] [G loss: 3.223144]\n",
      "epoch:46 step:36640 [D loss: 0.315947, acc.: 86.72%] [G loss: 2.934672]\n",
      "epoch:46 step:36641 [D loss: 0.295407, acc.: 88.28%] [G loss: 3.414891]\n",
      "epoch:46 step:36642 [D loss: 0.418733, acc.: 81.25%] [G loss: 2.885867]\n",
      "epoch:46 step:36643 [D loss: 0.274209, acc.: 89.06%] [G loss: 3.366138]\n",
      "epoch:46 step:36644 [D loss: 0.414715, acc.: 83.59%] [G loss: 3.728973]\n",
      "epoch:46 step:36645 [D loss: 0.297207, acc.: 85.94%] [G loss: 3.352064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36646 [D loss: 0.383289, acc.: 82.03%] [G loss: 3.589628]\n",
      "epoch:46 step:36647 [D loss: 0.446589, acc.: 83.59%] [G loss: 3.162683]\n",
      "epoch:46 step:36648 [D loss: 0.409468, acc.: 83.59%] [G loss: 2.576758]\n",
      "epoch:46 step:36649 [D loss: 0.316492, acc.: 80.47%] [G loss: 3.314145]\n",
      "epoch:46 step:36650 [D loss: 0.260153, acc.: 89.84%] [G loss: 4.356865]\n",
      "epoch:46 step:36651 [D loss: 0.413825, acc.: 84.38%] [G loss: 5.571976]\n",
      "epoch:46 step:36652 [D loss: 0.324627, acc.: 84.38%] [G loss: 4.799135]\n",
      "epoch:46 step:36653 [D loss: 0.482135, acc.: 76.56%] [G loss: 3.054345]\n",
      "epoch:46 step:36654 [D loss: 0.229813, acc.: 90.62%] [G loss: 2.798735]\n",
      "epoch:46 step:36655 [D loss: 0.263583, acc.: 86.72%] [G loss: 4.820674]\n",
      "epoch:46 step:36656 [D loss: 0.157635, acc.: 94.53%] [G loss: 4.086583]\n",
      "epoch:46 step:36657 [D loss: 0.188519, acc.: 91.41%] [G loss: 4.625840]\n",
      "epoch:46 step:36658 [D loss: 0.231268, acc.: 90.62%] [G loss: 3.470753]\n",
      "epoch:46 step:36659 [D loss: 0.247227, acc.: 88.28%] [G loss: 4.056637]\n",
      "epoch:46 step:36660 [D loss: 0.331013, acc.: 84.38%] [G loss: 3.694043]\n",
      "epoch:46 step:36661 [D loss: 0.306228, acc.: 88.28%] [G loss: 3.084375]\n",
      "epoch:46 step:36662 [D loss: 0.269716, acc.: 88.28%] [G loss: 3.165926]\n",
      "epoch:46 step:36663 [D loss: 0.390440, acc.: 78.12%] [G loss: 2.349369]\n",
      "epoch:46 step:36664 [D loss: 0.371946, acc.: 85.94%] [G loss: 3.113565]\n",
      "epoch:46 step:36665 [D loss: 0.262609, acc.: 91.41%] [G loss: 3.597504]\n",
      "epoch:46 step:36666 [D loss: 0.236852, acc.: 88.28%] [G loss: 4.726788]\n",
      "epoch:46 step:36667 [D loss: 0.390699, acc.: 84.38%] [G loss: 3.938029]\n",
      "epoch:46 step:36668 [D loss: 0.294682, acc.: 85.94%] [G loss: 5.437432]\n",
      "epoch:46 step:36669 [D loss: 0.221856, acc.: 90.62%] [G loss: 4.673158]\n",
      "epoch:46 step:36670 [D loss: 0.273353, acc.: 87.50%] [G loss: 3.600344]\n",
      "epoch:46 step:36671 [D loss: 0.262595, acc.: 91.41%] [G loss: 3.993453]\n",
      "epoch:46 step:36672 [D loss: 0.263166, acc.: 92.19%] [G loss: 4.518676]\n",
      "epoch:46 step:36673 [D loss: 0.288580, acc.: 85.94%] [G loss: 4.218393]\n",
      "epoch:46 step:36674 [D loss: 0.346275, acc.: 88.28%] [G loss: 3.893132]\n",
      "epoch:46 step:36675 [D loss: 0.376609, acc.: 83.59%] [G loss: 3.886715]\n",
      "epoch:46 step:36676 [D loss: 0.327600, acc.: 84.38%] [G loss: 3.470356]\n",
      "epoch:46 step:36677 [D loss: 0.294045, acc.: 88.28%] [G loss: 3.595245]\n",
      "epoch:46 step:36678 [D loss: 0.306443, acc.: 86.72%] [G loss: 3.325831]\n",
      "epoch:46 step:36679 [D loss: 0.414829, acc.: 81.25%] [G loss: 5.050367]\n",
      "epoch:46 step:36680 [D loss: 0.355225, acc.: 83.59%] [G loss: 3.393581]\n",
      "epoch:46 step:36681 [D loss: 0.305735, acc.: 86.72%] [G loss: 3.362869]\n",
      "epoch:46 step:36682 [D loss: 0.366983, acc.: 83.59%] [G loss: 4.529402]\n",
      "epoch:46 step:36683 [D loss: 0.424281, acc.: 81.25%] [G loss: 4.154208]\n",
      "epoch:46 step:36684 [D loss: 0.295008, acc.: 88.28%] [G loss: 3.630680]\n",
      "epoch:46 step:36685 [D loss: 0.296705, acc.: 85.16%] [G loss: 3.167155]\n",
      "epoch:46 step:36686 [D loss: 0.243923, acc.: 90.62%] [G loss: 3.693060]\n",
      "epoch:46 step:36687 [D loss: 0.281418, acc.: 86.72%] [G loss: 2.657789]\n",
      "epoch:46 step:36688 [D loss: 0.291005, acc.: 85.94%] [G loss: 2.683487]\n",
      "epoch:46 step:36689 [D loss: 0.393595, acc.: 82.03%] [G loss: 3.622446]\n",
      "epoch:46 step:36690 [D loss: 0.349527, acc.: 82.03%] [G loss: 3.112285]\n",
      "epoch:46 step:36691 [D loss: 0.470465, acc.: 76.56%] [G loss: 2.724667]\n",
      "epoch:46 step:36692 [D loss: 0.361495, acc.: 84.38%] [G loss: 2.869745]\n",
      "epoch:46 step:36693 [D loss: 0.303149, acc.: 84.38%] [G loss: 3.585964]\n",
      "epoch:46 step:36694 [D loss: 0.265304, acc.: 86.72%] [G loss: 3.871453]\n",
      "epoch:46 step:36695 [D loss: 0.255630, acc.: 85.16%] [G loss: 3.608149]\n",
      "epoch:46 step:36696 [D loss: 0.256244, acc.: 89.84%] [G loss: 3.874960]\n",
      "epoch:46 step:36697 [D loss: 0.414539, acc.: 81.25%] [G loss: 2.876445]\n",
      "epoch:46 step:36698 [D loss: 0.270597, acc.: 90.62%] [G loss: 4.302196]\n",
      "epoch:46 step:36699 [D loss: 0.298217, acc.: 86.72%] [G loss: 2.699398]\n",
      "epoch:46 step:36700 [D loss: 0.300136, acc.: 88.28%] [G loss: 2.884961]\n",
      "epoch:46 step:36701 [D loss: 0.252492, acc.: 90.62%] [G loss: 2.748875]\n",
      "epoch:46 step:36702 [D loss: 0.284645, acc.: 84.38%] [G loss: 3.383696]\n",
      "epoch:46 step:36703 [D loss: 0.282862, acc.: 85.94%] [G loss: 5.276134]\n",
      "epoch:46 step:36704 [D loss: 0.426662, acc.: 84.38%] [G loss: 4.573295]\n",
      "epoch:46 step:36705 [D loss: 0.524421, acc.: 78.91%] [G loss: 4.494116]\n",
      "epoch:46 step:36706 [D loss: 0.597210, acc.: 72.66%] [G loss: 4.143017]\n",
      "epoch:46 step:36707 [D loss: 0.324423, acc.: 83.59%] [G loss: 3.845103]\n",
      "epoch:47 step:36708 [D loss: 0.370379, acc.: 82.81%] [G loss: 3.259438]\n",
      "epoch:47 step:36709 [D loss: 0.306583, acc.: 86.72%] [G loss: 3.420240]\n",
      "epoch:47 step:36710 [D loss: 0.396160, acc.: 82.81%] [G loss: 6.237271]\n",
      "epoch:47 step:36711 [D loss: 0.314565, acc.: 85.16%] [G loss: 3.443949]\n",
      "epoch:47 step:36712 [D loss: 0.257653, acc.: 89.06%] [G loss: 4.032232]\n",
      "epoch:47 step:36713 [D loss: 0.357445, acc.: 83.59%] [G loss: 3.535020]\n",
      "epoch:47 step:36714 [D loss: 0.371940, acc.: 81.25%] [G loss: 3.202287]\n",
      "epoch:47 step:36715 [D loss: 0.315569, acc.: 85.94%] [G loss: 3.015817]\n",
      "epoch:47 step:36716 [D loss: 0.372335, acc.: 82.03%] [G loss: 3.151671]\n",
      "epoch:47 step:36717 [D loss: 0.336843, acc.: 84.38%] [G loss: 3.108671]\n",
      "epoch:47 step:36718 [D loss: 0.307348, acc.: 82.81%] [G loss: 3.204147]\n",
      "epoch:47 step:36719 [D loss: 0.309565, acc.: 86.72%] [G loss: 3.185187]\n",
      "epoch:47 step:36720 [D loss: 0.268300, acc.: 89.84%] [G loss: 2.180961]\n",
      "epoch:47 step:36721 [D loss: 0.326515, acc.: 85.16%] [G loss: 2.763513]\n",
      "epoch:47 step:36722 [D loss: 0.368942, acc.: 82.81%] [G loss: 3.211287]\n",
      "epoch:47 step:36723 [D loss: 0.295581, acc.: 85.94%] [G loss: 3.174162]\n",
      "epoch:47 step:36724 [D loss: 0.327178, acc.: 83.59%] [G loss: 2.553570]\n",
      "epoch:47 step:36725 [D loss: 0.308542, acc.: 85.94%] [G loss: 2.636057]\n",
      "epoch:47 step:36726 [D loss: 0.352316, acc.: 85.16%] [G loss: 3.155537]\n",
      "epoch:47 step:36727 [D loss: 0.299839, acc.: 83.59%] [G loss: 3.358736]\n",
      "epoch:47 step:36728 [D loss: 0.350439, acc.: 82.81%] [G loss: 2.773343]\n",
      "epoch:47 step:36729 [D loss: 0.402497, acc.: 80.47%] [G loss: 2.892260]\n",
      "epoch:47 step:36730 [D loss: 0.229640, acc.: 92.19%] [G loss: 3.497064]\n",
      "epoch:47 step:36731 [D loss: 0.357642, acc.: 84.38%] [G loss: 2.427890]\n",
      "epoch:47 step:36732 [D loss: 0.214035, acc.: 92.19%] [G loss: 3.239838]\n",
      "epoch:47 step:36733 [D loss: 0.328005, acc.: 83.59%] [G loss: 2.642474]\n",
      "epoch:47 step:36734 [D loss: 0.405140, acc.: 81.25%] [G loss: 3.226533]\n",
      "epoch:47 step:36735 [D loss: 0.405978, acc.: 80.47%] [G loss: 3.279722]\n",
      "epoch:47 step:36736 [D loss: 0.449135, acc.: 81.25%] [G loss: 3.885077]\n",
      "epoch:47 step:36737 [D loss: 0.266677, acc.: 89.06%] [G loss: 3.328810]\n",
      "epoch:47 step:36738 [D loss: 0.348301, acc.: 85.94%] [G loss: 5.457228]\n",
      "epoch:47 step:36739 [D loss: 0.351464, acc.: 86.72%] [G loss: 3.467655]\n",
      "epoch:47 step:36740 [D loss: 0.266196, acc.: 85.16%] [G loss: 5.986753]\n",
      "epoch:47 step:36741 [D loss: 0.297421, acc.: 85.94%] [G loss: 4.325366]\n",
      "epoch:47 step:36742 [D loss: 0.266152, acc.: 87.50%] [G loss: 4.002134]\n",
      "epoch:47 step:36743 [D loss: 0.265657, acc.: 89.84%] [G loss: 4.703074]\n",
      "epoch:47 step:36744 [D loss: 0.206998, acc.: 91.41%] [G loss: 3.724229]\n",
      "epoch:47 step:36745 [D loss: 0.291864, acc.: 86.72%] [G loss: 4.584612]\n",
      "epoch:47 step:36746 [D loss: 0.318709, acc.: 82.81%] [G loss: 3.652597]\n",
      "epoch:47 step:36747 [D loss: 0.273559, acc.: 85.94%] [G loss: 3.424025]\n",
      "epoch:47 step:36748 [D loss: 0.360242, acc.: 82.81%] [G loss: 3.719846]\n",
      "epoch:47 step:36749 [D loss: 0.333115, acc.: 86.72%] [G loss: 4.965698]\n",
      "epoch:47 step:36750 [D loss: 0.787294, acc.: 67.97%] [G loss: 6.023307]\n",
      "epoch:47 step:36751 [D loss: 1.985941, acc.: 56.25%] [G loss: 9.914846]\n",
      "epoch:47 step:36752 [D loss: 3.086461, acc.: 55.47%] [G loss: 6.826424]\n",
      "epoch:47 step:36753 [D loss: 1.620772, acc.: 53.12%] [G loss: 4.245266]\n",
      "epoch:47 step:36754 [D loss: 0.580373, acc.: 81.25%] [G loss: 4.980997]\n",
      "epoch:47 step:36755 [D loss: 0.961098, acc.: 65.62%] [G loss: 3.686425]\n",
      "epoch:47 step:36756 [D loss: 0.332286, acc.: 86.72%] [G loss: 4.125316]\n",
      "epoch:47 step:36757 [D loss: 0.446553, acc.: 81.25%] [G loss: 3.181264]\n",
      "epoch:47 step:36758 [D loss: 0.319298, acc.: 84.38%] [G loss: 2.934851]\n",
      "epoch:47 step:36759 [D loss: 0.260718, acc.: 87.50%] [G loss: 3.025004]\n",
      "epoch:47 step:36760 [D loss: 0.402909, acc.: 80.47%] [G loss: 2.500651]\n",
      "epoch:47 step:36761 [D loss: 0.401687, acc.: 78.91%] [G loss: 2.453701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:36762 [D loss: 0.268710, acc.: 90.62%] [G loss: 2.887334]\n",
      "epoch:47 step:36763 [D loss: 0.397990, acc.: 82.03%] [G loss: 2.803856]\n",
      "epoch:47 step:36764 [D loss: 0.399481, acc.: 83.59%] [G loss: 2.627233]\n",
      "epoch:47 step:36765 [D loss: 0.378394, acc.: 82.81%] [G loss: 2.312388]\n",
      "epoch:47 step:36766 [D loss: 0.353273, acc.: 83.59%] [G loss: 2.413367]\n",
      "epoch:47 step:36767 [D loss: 0.392473, acc.: 82.81%] [G loss: 2.593395]\n",
      "epoch:47 step:36768 [D loss: 0.440115, acc.: 82.81%] [G loss: 2.536174]\n",
      "epoch:47 step:36769 [D loss: 0.332384, acc.: 85.16%] [G loss: 2.437478]\n",
      "epoch:47 step:36770 [D loss: 0.316906, acc.: 89.06%] [G loss: 2.369518]\n",
      "epoch:47 step:36771 [D loss: 0.401579, acc.: 82.81%] [G loss: 1.950415]\n",
      "epoch:47 step:36772 [D loss: 0.447856, acc.: 78.12%] [G loss: 2.776371]\n",
      "epoch:47 step:36773 [D loss: 0.383400, acc.: 83.59%] [G loss: 2.409848]\n",
      "epoch:47 step:36774 [D loss: 0.325723, acc.: 82.81%] [G loss: 2.967058]\n",
      "epoch:47 step:36775 [D loss: 0.311104, acc.: 86.72%] [G loss: 2.910803]\n",
      "epoch:47 step:36776 [D loss: 0.397837, acc.: 80.47%] [G loss: 2.991864]\n",
      "epoch:47 step:36777 [D loss: 0.385360, acc.: 83.59%] [G loss: 3.010588]\n",
      "epoch:47 step:36778 [D loss: 0.330478, acc.: 88.28%] [G loss: 2.810019]\n",
      "epoch:47 step:36779 [D loss: 0.322863, acc.: 84.38%] [G loss: 3.757635]\n",
      "epoch:47 step:36780 [D loss: 0.416075, acc.: 78.91%] [G loss: 2.897957]\n",
      "epoch:47 step:36781 [D loss: 0.223308, acc.: 91.41%] [G loss: 3.696161]\n",
      "epoch:47 step:36782 [D loss: 0.288612, acc.: 87.50%] [G loss: 3.715007]\n",
      "epoch:47 step:36783 [D loss: 0.329703, acc.: 85.16%] [G loss: 3.805815]\n",
      "epoch:47 step:36784 [D loss: 0.306591, acc.: 83.59%] [G loss: 3.308136]\n",
      "epoch:47 step:36785 [D loss: 0.388782, acc.: 85.94%] [G loss: 3.489483]\n",
      "epoch:47 step:36786 [D loss: 0.297257, acc.: 86.72%] [G loss: 2.916055]\n",
      "epoch:47 step:36787 [D loss: 0.324435, acc.: 83.59%] [G loss: 5.018083]\n",
      "epoch:47 step:36788 [D loss: 0.281924, acc.: 89.06%] [G loss: 4.661071]\n",
      "epoch:47 step:36789 [D loss: 0.254795, acc.: 91.41%] [G loss: 2.596230]\n",
      "epoch:47 step:36790 [D loss: 0.274253, acc.: 87.50%] [G loss: 2.620932]\n",
      "epoch:47 step:36791 [D loss: 0.247056, acc.: 90.62%] [G loss: 2.799736]\n",
      "epoch:47 step:36792 [D loss: 0.287413, acc.: 87.50%] [G loss: 3.508202]\n",
      "epoch:47 step:36793 [D loss: 0.311200, acc.: 85.16%] [G loss: 3.523081]\n",
      "epoch:47 step:36794 [D loss: 0.275061, acc.: 85.94%] [G loss: 2.994647]\n",
      "epoch:47 step:36795 [D loss: 0.258205, acc.: 89.06%] [G loss: 2.879058]\n",
      "epoch:47 step:36796 [D loss: 0.315891, acc.: 90.62%] [G loss: 3.724286]\n",
      "epoch:47 step:36797 [D loss: 0.314993, acc.: 85.94%] [G loss: 3.289908]\n",
      "epoch:47 step:36798 [D loss: 0.216200, acc.: 89.84%] [G loss: 3.966432]\n",
      "epoch:47 step:36799 [D loss: 0.427776, acc.: 82.03%] [G loss: 4.379266]\n",
      "epoch:47 step:36800 [D loss: 0.358310, acc.: 82.81%] [G loss: 2.883097]\n",
      "epoch:47 step:36801 [D loss: 0.208397, acc.: 91.41%] [G loss: 3.550448]\n",
      "epoch:47 step:36802 [D loss: 0.338464, acc.: 86.72%] [G loss: 3.952816]\n",
      "epoch:47 step:36803 [D loss: 0.377315, acc.: 80.47%] [G loss: 3.455295]\n",
      "epoch:47 step:36804 [D loss: 0.322172, acc.: 82.81%] [G loss: 3.334836]\n",
      "epoch:47 step:36805 [D loss: 0.314473, acc.: 85.16%] [G loss: 3.568980]\n",
      "epoch:47 step:36806 [D loss: 0.332295, acc.: 84.38%] [G loss: 3.547482]\n",
      "epoch:47 step:36807 [D loss: 0.245760, acc.: 89.06%] [G loss: 3.428812]\n",
      "epoch:47 step:36808 [D loss: 0.325044, acc.: 88.28%] [G loss: 2.888486]\n",
      "epoch:47 step:36809 [D loss: 0.255273, acc.: 87.50%] [G loss: 3.355268]\n",
      "epoch:47 step:36810 [D loss: 0.359076, acc.: 89.06%] [G loss: 2.650565]\n",
      "epoch:47 step:36811 [D loss: 0.307410, acc.: 89.06%] [G loss: 3.076537]\n",
      "epoch:47 step:36812 [D loss: 0.358200, acc.: 85.94%] [G loss: 2.515890]\n",
      "epoch:47 step:36813 [D loss: 0.366594, acc.: 83.59%] [G loss: 3.361087]\n",
      "epoch:47 step:36814 [D loss: 0.356664, acc.: 85.94%] [G loss: 2.910086]\n",
      "epoch:47 step:36815 [D loss: 0.299828, acc.: 88.28%] [G loss: 2.829381]\n",
      "epoch:47 step:36816 [D loss: 0.374122, acc.: 83.59%] [G loss: 2.688127]\n",
      "epoch:47 step:36817 [D loss: 0.242953, acc.: 90.62%] [G loss: 2.543029]\n",
      "epoch:47 step:36818 [D loss: 0.425382, acc.: 78.12%] [G loss: 2.683216]\n",
      "epoch:47 step:36819 [D loss: 0.356414, acc.: 84.38%] [G loss: 3.004143]\n",
      "epoch:47 step:36820 [D loss: 0.321413, acc.: 87.50%] [G loss: 3.433871]\n",
      "epoch:47 step:36821 [D loss: 0.334445, acc.: 82.81%] [G loss: 2.265271]\n",
      "epoch:47 step:36822 [D loss: 0.235350, acc.: 90.62%] [G loss: 3.050147]\n",
      "epoch:47 step:36823 [D loss: 0.411870, acc.: 82.03%] [G loss: 2.995782]\n",
      "epoch:47 step:36824 [D loss: 0.241385, acc.: 90.62%] [G loss: 2.911507]\n",
      "epoch:47 step:36825 [D loss: 0.282906, acc.: 88.28%] [G loss: 2.681766]\n",
      "epoch:47 step:36826 [D loss: 0.302339, acc.: 84.38%] [G loss: 2.884354]\n",
      "epoch:47 step:36827 [D loss: 0.282593, acc.: 85.94%] [G loss: 3.199085]\n",
      "epoch:47 step:36828 [D loss: 0.285646, acc.: 87.50%] [G loss: 3.618675]\n",
      "epoch:47 step:36829 [D loss: 0.357619, acc.: 85.16%] [G loss: 2.392689]\n",
      "epoch:47 step:36830 [D loss: 0.420032, acc.: 79.69%] [G loss: 2.627254]\n",
      "epoch:47 step:36831 [D loss: 0.283414, acc.: 89.06%] [G loss: 3.224092]\n",
      "epoch:47 step:36832 [D loss: 0.310948, acc.: 85.16%] [G loss: 2.472278]\n",
      "epoch:47 step:36833 [D loss: 0.372473, acc.: 86.72%] [G loss: 2.621814]\n",
      "epoch:47 step:36834 [D loss: 0.240204, acc.: 92.19%] [G loss: 3.110368]\n",
      "epoch:47 step:36835 [D loss: 0.254561, acc.: 91.41%] [G loss: 3.698812]\n",
      "epoch:47 step:36836 [D loss: 0.310259, acc.: 85.94%] [G loss: 3.260930]\n",
      "epoch:47 step:36837 [D loss: 0.297055, acc.: 87.50%] [G loss: 3.494925]\n",
      "epoch:47 step:36838 [D loss: 0.341784, acc.: 86.72%] [G loss: 3.085488]\n",
      "epoch:47 step:36839 [D loss: 0.381092, acc.: 83.59%] [G loss: 3.928663]\n",
      "epoch:47 step:36840 [D loss: 0.429093, acc.: 81.25%] [G loss: 2.586864]\n",
      "epoch:47 step:36841 [D loss: 0.269389, acc.: 89.06%] [G loss: 2.858258]\n",
      "epoch:47 step:36842 [D loss: 0.346577, acc.: 83.59%] [G loss: 2.980206]\n",
      "epoch:47 step:36843 [D loss: 0.229346, acc.: 89.84%] [G loss: 3.308521]\n",
      "epoch:47 step:36844 [D loss: 0.328710, acc.: 85.16%] [G loss: 1.983547]\n",
      "epoch:47 step:36845 [D loss: 0.314671, acc.: 85.16%] [G loss: 3.053792]\n",
      "epoch:47 step:36846 [D loss: 0.280074, acc.: 86.72%] [G loss: 3.567753]\n",
      "epoch:47 step:36847 [D loss: 0.339055, acc.: 86.72%] [G loss: 3.332899]\n",
      "epoch:47 step:36848 [D loss: 0.398230, acc.: 78.12%] [G loss: 2.410288]\n",
      "epoch:47 step:36849 [D loss: 0.399668, acc.: 82.03%] [G loss: 2.863385]\n",
      "epoch:47 step:36850 [D loss: 0.382175, acc.: 83.59%] [G loss: 2.708990]\n",
      "epoch:47 step:36851 [D loss: 0.472244, acc.: 78.91%] [G loss: 2.430102]\n",
      "epoch:47 step:36852 [D loss: 0.287727, acc.: 87.50%] [G loss: 3.344729]\n",
      "epoch:47 step:36853 [D loss: 0.408128, acc.: 81.25%] [G loss: 3.366989]\n",
      "epoch:47 step:36854 [D loss: 0.345799, acc.: 85.94%] [G loss: 2.674293]\n",
      "epoch:47 step:36855 [D loss: 0.311157, acc.: 87.50%] [G loss: 2.726573]\n",
      "epoch:47 step:36856 [D loss: 0.354233, acc.: 85.94%] [G loss: 2.872184]\n",
      "epoch:47 step:36857 [D loss: 0.394573, acc.: 83.59%] [G loss: 2.905308]\n",
      "epoch:47 step:36858 [D loss: 0.331338, acc.: 84.38%] [G loss: 2.928044]\n",
      "epoch:47 step:36859 [D loss: 0.298691, acc.: 90.62%] [G loss: 3.124898]\n",
      "epoch:47 step:36860 [D loss: 0.324362, acc.: 85.94%] [G loss: 2.235438]\n",
      "epoch:47 step:36861 [D loss: 0.286520, acc.: 88.28%] [G loss: 3.817660]\n",
      "epoch:47 step:36862 [D loss: 0.383031, acc.: 81.25%] [G loss: 3.505593]\n",
      "epoch:47 step:36863 [D loss: 0.281827, acc.: 88.28%] [G loss: 3.377079]\n",
      "epoch:47 step:36864 [D loss: 0.300068, acc.: 84.38%] [G loss: 6.656036]\n",
      "epoch:47 step:36865 [D loss: 0.432912, acc.: 75.00%] [G loss: 5.112518]\n",
      "epoch:47 step:36866 [D loss: 0.265961, acc.: 88.28%] [G loss: 5.570937]\n",
      "epoch:47 step:36867 [D loss: 0.480885, acc.: 77.34%] [G loss: 6.406354]\n",
      "epoch:47 step:36868 [D loss: 0.394886, acc.: 84.38%] [G loss: 4.693187]\n",
      "epoch:47 step:36869 [D loss: 0.304736, acc.: 88.28%] [G loss: 4.320865]\n",
      "epoch:47 step:36870 [D loss: 0.325624, acc.: 84.38%] [G loss: 3.630263]\n",
      "epoch:47 step:36871 [D loss: 0.349666, acc.: 86.72%] [G loss: 3.518149]\n",
      "epoch:47 step:36872 [D loss: 0.319105, acc.: 88.28%] [G loss: 3.444783]\n",
      "epoch:47 step:36873 [D loss: 0.289619, acc.: 88.28%] [G loss: 3.183599]\n",
      "epoch:47 step:36874 [D loss: 0.433870, acc.: 83.59%] [G loss: 3.362605]\n",
      "epoch:47 step:36875 [D loss: 0.328636, acc.: 83.59%] [G loss: 3.114383]\n",
      "epoch:47 step:36876 [D loss: 0.323854, acc.: 85.94%] [G loss: 3.181240]\n",
      "epoch:47 step:36877 [D loss: 0.308508, acc.: 87.50%] [G loss: 3.402706]\n",
      "epoch:47 step:36878 [D loss: 0.361402, acc.: 82.03%] [G loss: 2.887307]\n",
      "epoch:47 step:36879 [D loss: 0.378831, acc.: 85.94%] [G loss: 3.320662]\n",
      "epoch:47 step:36880 [D loss: 0.305585, acc.: 87.50%] [G loss: 3.675565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:36881 [D loss: 0.351934, acc.: 85.94%] [G loss: 3.093189]\n",
      "epoch:47 step:36882 [D loss: 0.392731, acc.: 82.81%] [G loss: 2.745664]\n",
      "epoch:47 step:36883 [D loss: 0.284755, acc.: 87.50%] [G loss: 3.336508]\n",
      "epoch:47 step:36884 [D loss: 0.398378, acc.: 82.03%] [G loss: 3.773942]\n",
      "epoch:47 step:36885 [D loss: 0.371576, acc.: 82.03%] [G loss: 3.598501]\n",
      "epoch:47 step:36886 [D loss: 0.380968, acc.: 80.47%] [G loss: 4.872262]\n",
      "epoch:47 step:36887 [D loss: 0.349275, acc.: 85.94%] [G loss: 3.212250]\n",
      "epoch:47 step:36888 [D loss: 0.214040, acc.: 92.97%] [G loss: 4.032892]\n",
      "epoch:47 step:36889 [D loss: 0.335049, acc.: 84.38%] [G loss: 3.149030]\n",
      "epoch:47 step:36890 [D loss: 0.351632, acc.: 85.94%] [G loss: 2.494887]\n",
      "epoch:47 step:36891 [D loss: 0.451855, acc.: 78.91%] [G loss: 3.258727]\n",
      "epoch:47 step:36892 [D loss: 0.291861, acc.: 85.16%] [G loss: 3.665396]\n",
      "epoch:47 step:36893 [D loss: 0.197657, acc.: 89.84%] [G loss: 3.806412]\n",
      "epoch:47 step:36894 [D loss: 0.326081, acc.: 88.28%] [G loss: 3.072596]\n",
      "epoch:47 step:36895 [D loss: 0.503867, acc.: 73.44%] [G loss: 2.569359]\n",
      "epoch:47 step:36896 [D loss: 0.313773, acc.: 88.28%] [G loss: 3.728039]\n",
      "epoch:47 step:36897 [D loss: 0.448565, acc.: 77.34%] [G loss: 2.688300]\n",
      "epoch:47 step:36898 [D loss: 0.294853, acc.: 86.72%] [G loss: 3.378184]\n",
      "epoch:47 step:36899 [D loss: 0.356862, acc.: 83.59%] [G loss: 3.241868]\n",
      "epoch:47 step:36900 [D loss: 0.340027, acc.: 81.25%] [G loss: 3.073874]\n",
      "epoch:47 step:36901 [D loss: 0.395415, acc.: 81.25%] [G loss: 2.409045]\n",
      "epoch:47 step:36902 [D loss: 0.383619, acc.: 84.38%] [G loss: 2.487394]\n",
      "epoch:47 step:36903 [D loss: 0.327333, acc.: 83.59%] [G loss: 3.044645]\n",
      "epoch:47 step:36904 [D loss: 0.352239, acc.: 86.72%] [G loss: 3.156220]\n",
      "epoch:47 step:36905 [D loss: 0.340287, acc.: 81.25%] [G loss: 2.743320]\n",
      "epoch:47 step:36906 [D loss: 0.321797, acc.: 89.06%] [G loss: 3.229940]\n",
      "epoch:47 step:36907 [D loss: 0.339966, acc.: 85.16%] [G loss: 3.207717]\n",
      "epoch:47 step:36908 [D loss: 0.352769, acc.: 85.16%] [G loss: 2.814803]\n",
      "epoch:47 step:36909 [D loss: 0.444047, acc.: 83.59%] [G loss: 3.023000]\n",
      "epoch:47 step:36910 [D loss: 0.264820, acc.: 89.84%] [G loss: 2.972824]\n",
      "epoch:47 step:36911 [D loss: 0.314982, acc.: 87.50%] [G loss: 3.850563]\n",
      "epoch:47 step:36912 [D loss: 0.349500, acc.: 81.25%] [G loss: 4.262743]\n",
      "epoch:47 step:36913 [D loss: 0.447098, acc.: 86.72%] [G loss: 3.173958]\n",
      "epoch:47 step:36914 [D loss: 0.354122, acc.: 83.59%] [G loss: 3.012062]\n",
      "epoch:47 step:36915 [D loss: 0.326962, acc.: 85.94%] [G loss: 2.999135]\n",
      "epoch:47 step:36916 [D loss: 0.347323, acc.: 79.69%] [G loss: 3.237364]\n",
      "epoch:47 step:36917 [D loss: 0.332450, acc.: 87.50%] [G loss: 2.965760]\n",
      "epoch:47 step:36918 [D loss: 0.347149, acc.: 85.16%] [G loss: 2.674971]\n",
      "epoch:47 step:36919 [D loss: 0.338335, acc.: 83.59%] [G loss: 3.506157]\n",
      "epoch:47 step:36920 [D loss: 0.348233, acc.: 84.38%] [G loss: 4.044182]\n",
      "epoch:47 step:36921 [D loss: 0.359814, acc.: 84.38%] [G loss: 3.787802]\n",
      "epoch:47 step:36922 [D loss: 0.333587, acc.: 85.94%] [G loss: 3.119481]\n",
      "epoch:47 step:36923 [D loss: 0.424986, acc.: 82.81%] [G loss: 2.395602]\n",
      "epoch:47 step:36924 [D loss: 0.348804, acc.: 82.81%] [G loss: 3.146537]\n",
      "epoch:47 step:36925 [D loss: 0.411331, acc.: 78.12%] [G loss: 3.336957]\n",
      "epoch:47 step:36926 [D loss: 0.325189, acc.: 84.38%] [G loss: 3.445986]\n",
      "epoch:47 step:36927 [D loss: 0.314212, acc.: 88.28%] [G loss: 2.850850]\n",
      "epoch:47 step:36928 [D loss: 0.250344, acc.: 89.06%] [G loss: 2.598112]\n",
      "epoch:47 step:36929 [D loss: 0.295233, acc.: 83.59%] [G loss: 2.700108]\n",
      "epoch:47 step:36930 [D loss: 0.282950, acc.: 86.72%] [G loss: 3.087670]\n",
      "epoch:47 step:36931 [D loss: 0.305906, acc.: 88.28%] [G loss: 2.535916]\n",
      "epoch:47 step:36932 [D loss: 0.397284, acc.: 79.69%] [G loss: 2.537140]\n",
      "epoch:47 step:36933 [D loss: 0.337390, acc.: 82.03%] [G loss: 3.704180]\n",
      "epoch:47 step:36934 [D loss: 0.229656, acc.: 91.41%] [G loss: 3.146098]\n",
      "epoch:47 step:36935 [D loss: 0.414350, acc.: 77.34%] [G loss: 3.926136]\n",
      "epoch:47 step:36936 [D loss: 0.348324, acc.: 82.03%] [G loss: 5.557626]\n",
      "epoch:47 step:36937 [D loss: 0.489350, acc.: 75.00%] [G loss: 2.937668]\n",
      "epoch:47 step:36938 [D loss: 0.326949, acc.: 87.50%] [G loss: 4.077412]\n",
      "epoch:47 step:36939 [D loss: 0.213588, acc.: 90.62%] [G loss: 5.680231]\n",
      "epoch:47 step:36940 [D loss: 0.302397, acc.: 87.50%] [G loss: 4.257297]\n",
      "epoch:47 step:36941 [D loss: 0.224325, acc.: 92.19%] [G loss: 3.534407]\n",
      "epoch:47 step:36942 [D loss: 0.288139, acc.: 86.72%] [G loss: 3.489653]\n",
      "epoch:47 step:36943 [D loss: 0.343743, acc.: 82.03%] [G loss: 4.007542]\n",
      "epoch:47 step:36944 [D loss: 0.333674, acc.: 86.72%] [G loss: 3.166062]\n",
      "epoch:47 step:36945 [D loss: 0.221362, acc.: 90.62%] [G loss: 3.605717]\n",
      "epoch:47 step:36946 [D loss: 0.311857, acc.: 87.50%] [G loss: 3.783059]\n",
      "epoch:47 step:36947 [D loss: 0.310783, acc.: 89.06%] [G loss: 3.878499]\n",
      "epoch:47 step:36948 [D loss: 0.380136, acc.: 80.47%] [G loss: 2.915269]\n",
      "epoch:47 step:36949 [D loss: 0.397015, acc.: 83.59%] [G loss: 3.159509]\n",
      "epoch:47 step:36950 [D loss: 0.234790, acc.: 89.84%] [G loss: 3.041602]\n",
      "epoch:47 step:36951 [D loss: 0.289862, acc.: 87.50%] [G loss: 3.499396]\n",
      "epoch:47 step:36952 [D loss: 0.370898, acc.: 81.25%] [G loss: 3.432505]\n",
      "epoch:47 step:36953 [D loss: 0.339255, acc.: 84.38%] [G loss: 2.715279]\n",
      "epoch:47 step:36954 [D loss: 0.264438, acc.: 87.50%] [G loss: 3.147456]\n",
      "epoch:47 step:36955 [D loss: 0.311280, acc.: 85.94%] [G loss: 3.041038]\n",
      "epoch:47 step:36956 [D loss: 0.330903, acc.: 83.59%] [G loss: 3.283159]\n",
      "epoch:47 step:36957 [D loss: 0.431970, acc.: 77.34%] [G loss: 5.394482]\n",
      "epoch:47 step:36958 [D loss: 0.362090, acc.: 82.03%] [G loss: 4.242520]\n",
      "epoch:47 step:36959 [D loss: 0.287066, acc.: 85.94%] [G loss: 3.704403]\n",
      "epoch:47 step:36960 [D loss: 0.298413, acc.: 84.38%] [G loss: 4.119209]\n",
      "epoch:47 step:36961 [D loss: 0.310924, acc.: 85.16%] [G loss: 4.282911]\n",
      "epoch:47 step:36962 [D loss: 0.242777, acc.: 89.84%] [G loss: 3.157010]\n",
      "epoch:47 step:36963 [D loss: 0.283002, acc.: 89.84%] [G loss: 3.480976]\n",
      "epoch:47 step:36964 [D loss: 0.278390, acc.: 88.28%] [G loss: 3.853560]\n",
      "epoch:47 step:36965 [D loss: 0.320927, acc.: 82.81%] [G loss: 3.572157]\n",
      "epoch:47 step:36966 [D loss: 0.317546, acc.: 85.94%] [G loss: 4.281275]\n",
      "epoch:47 step:36967 [D loss: 0.315425, acc.: 87.50%] [G loss: 3.745078]\n",
      "epoch:47 step:36968 [D loss: 0.346049, acc.: 85.16%] [G loss: 3.385950]\n",
      "epoch:47 step:36969 [D loss: 0.441848, acc.: 78.12%] [G loss: 3.936327]\n",
      "epoch:47 step:36970 [D loss: 0.264084, acc.: 89.84%] [G loss: 4.966899]\n",
      "epoch:47 step:36971 [D loss: 0.362187, acc.: 82.03%] [G loss: 4.212999]\n",
      "epoch:47 step:36972 [D loss: 0.322466, acc.: 80.47%] [G loss: 7.014405]\n",
      "epoch:47 step:36973 [D loss: 0.319611, acc.: 84.38%] [G loss: 6.005060]\n",
      "epoch:47 step:36974 [D loss: 0.372482, acc.: 80.47%] [G loss: 3.880337]\n",
      "epoch:47 step:36975 [D loss: 0.287944, acc.: 84.38%] [G loss: 4.006925]\n",
      "epoch:47 step:36976 [D loss: 0.345707, acc.: 81.25%] [G loss: 4.408647]\n",
      "epoch:47 step:36977 [D loss: 0.267101, acc.: 92.19%] [G loss: 3.532195]\n",
      "epoch:47 step:36978 [D loss: 0.283988, acc.: 91.41%] [G loss: 3.706790]\n",
      "epoch:47 step:36979 [D loss: 0.250519, acc.: 88.28%] [G loss: 3.671423]\n",
      "epoch:47 step:36980 [D loss: 0.283505, acc.: 89.06%] [G loss: 3.213186]\n",
      "epoch:47 step:36981 [D loss: 0.287750, acc.: 89.06%] [G loss: 3.426843]\n",
      "epoch:47 step:36982 [D loss: 0.542148, acc.: 71.09%] [G loss: 2.801135]\n",
      "epoch:47 step:36983 [D loss: 0.406937, acc.: 85.16%] [G loss: 4.633943]\n",
      "epoch:47 step:36984 [D loss: 0.457206, acc.: 75.00%] [G loss: 3.886153]\n",
      "epoch:47 step:36985 [D loss: 0.455744, acc.: 82.81%] [G loss: 5.600904]\n",
      "epoch:47 step:36986 [D loss: 0.610226, acc.: 75.78%] [G loss: 4.391479]\n",
      "epoch:47 step:36987 [D loss: 0.276698, acc.: 85.16%] [G loss: 3.553272]\n",
      "epoch:47 step:36988 [D loss: 0.380686, acc.: 81.25%] [G loss: 4.625432]\n",
      "epoch:47 step:36989 [D loss: 0.360785, acc.: 82.03%] [G loss: 2.735446]\n",
      "epoch:47 step:36990 [D loss: 0.360241, acc.: 86.72%] [G loss: 3.595177]\n",
      "epoch:47 step:36991 [D loss: 0.335704, acc.: 86.72%] [G loss: 4.582342]\n",
      "epoch:47 step:36992 [D loss: 0.383971, acc.: 80.47%] [G loss: 3.571550]\n",
      "epoch:47 step:36993 [D loss: 0.358609, acc.: 80.47%] [G loss: 3.616336]\n",
      "epoch:47 step:36994 [D loss: 0.286643, acc.: 89.84%] [G loss: 2.719689]\n",
      "epoch:47 step:36995 [D loss: 0.243366, acc.: 87.50%] [G loss: 4.843849]\n",
      "epoch:47 step:36996 [D loss: 0.269575, acc.: 88.28%] [G loss: 3.649269]\n",
      "epoch:47 step:36997 [D loss: 0.369365, acc.: 80.47%] [G loss: 2.343835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:36998 [D loss: 0.278111, acc.: 85.94%] [G loss: 3.925194]\n",
      "epoch:47 step:36999 [D loss: 0.278731, acc.: 84.38%] [G loss: 3.824123]\n",
      "epoch:47 step:37000 [D loss: 0.313292, acc.: 85.94%] [G loss: 3.444646]\n",
      "epoch:47 step:37001 [D loss: 0.314848, acc.: 84.38%] [G loss: 3.417958]\n",
      "epoch:47 step:37002 [D loss: 0.427757, acc.: 81.25%] [G loss: 3.086403]\n",
      "epoch:47 step:37003 [D loss: 0.335603, acc.: 85.94%] [G loss: 3.550781]\n",
      "epoch:47 step:37004 [D loss: 0.279204, acc.: 87.50%] [G loss: 2.476888]\n",
      "epoch:47 step:37005 [D loss: 0.261935, acc.: 89.84%] [G loss: 3.240985]\n",
      "epoch:47 step:37006 [D loss: 0.300012, acc.: 88.28%] [G loss: 3.542286]\n",
      "epoch:47 step:37007 [D loss: 0.321904, acc.: 85.16%] [G loss: 3.410280]\n",
      "epoch:47 step:37008 [D loss: 0.410337, acc.: 80.47%] [G loss: 2.631622]\n",
      "epoch:47 step:37009 [D loss: 0.237362, acc.: 91.41%] [G loss: 2.444431]\n",
      "epoch:47 step:37010 [D loss: 0.307998, acc.: 86.72%] [G loss: 3.018907]\n",
      "epoch:47 step:37011 [D loss: 0.282291, acc.: 87.50%] [G loss: 3.114117]\n",
      "epoch:47 step:37012 [D loss: 0.281954, acc.: 87.50%] [G loss: 2.596604]\n",
      "epoch:47 step:37013 [D loss: 0.406024, acc.: 79.69%] [G loss: 2.541084]\n",
      "epoch:47 step:37014 [D loss: 0.266503, acc.: 89.84%] [G loss: 2.617287]\n",
      "epoch:47 step:37015 [D loss: 0.380966, acc.: 77.34%] [G loss: 2.851380]\n",
      "epoch:47 step:37016 [D loss: 0.291311, acc.: 85.16%] [G loss: 3.335613]\n",
      "epoch:47 step:37017 [D loss: 0.225301, acc.: 92.19%] [G loss: 3.052092]\n",
      "epoch:47 step:37018 [D loss: 0.292070, acc.: 86.72%] [G loss: 3.251585]\n",
      "epoch:47 step:37019 [D loss: 0.317481, acc.: 84.38%] [G loss: 2.676059]\n",
      "epoch:47 step:37020 [D loss: 0.357800, acc.: 86.72%] [G loss: 3.572921]\n",
      "epoch:47 step:37021 [D loss: 0.235108, acc.: 90.62%] [G loss: 3.492184]\n",
      "epoch:47 step:37022 [D loss: 0.419899, acc.: 78.12%] [G loss: 5.300695]\n",
      "epoch:47 step:37023 [D loss: 0.373734, acc.: 85.16%] [G loss: 3.785954]\n",
      "epoch:47 step:37024 [D loss: 0.399503, acc.: 81.25%] [G loss: 2.543222]\n",
      "epoch:47 step:37025 [D loss: 0.361944, acc.: 86.72%] [G loss: 5.465802]\n",
      "epoch:47 step:37026 [D loss: 0.158302, acc.: 92.97%] [G loss: 5.367081]\n",
      "epoch:47 step:37027 [D loss: 0.278220, acc.: 86.72%] [G loss: 5.729678]\n",
      "epoch:47 step:37028 [D loss: 0.335620, acc.: 87.50%] [G loss: 4.588161]\n",
      "epoch:47 step:37029 [D loss: 0.329395, acc.: 83.59%] [G loss: 3.343436]\n",
      "epoch:47 step:37030 [D loss: 0.355386, acc.: 83.59%] [G loss: 2.973646]\n",
      "epoch:47 step:37031 [D loss: 0.340710, acc.: 85.94%] [G loss: 3.255634]\n",
      "epoch:47 step:37032 [D loss: 0.276808, acc.: 85.94%] [G loss: 2.897326]\n",
      "epoch:47 step:37033 [D loss: 0.370410, acc.: 84.38%] [G loss: 3.592776]\n",
      "epoch:47 step:37034 [D loss: 0.330665, acc.: 83.59%] [G loss: 3.046902]\n",
      "epoch:47 step:37035 [D loss: 0.315802, acc.: 86.72%] [G loss: 2.915436]\n",
      "epoch:47 step:37036 [D loss: 0.335389, acc.: 85.16%] [G loss: 2.908165]\n",
      "epoch:47 step:37037 [D loss: 0.385166, acc.: 82.81%] [G loss: 3.476878]\n",
      "epoch:47 step:37038 [D loss: 0.439050, acc.: 80.47%] [G loss: 4.126458]\n",
      "epoch:47 step:37039 [D loss: 0.330440, acc.: 86.72%] [G loss: 3.078449]\n",
      "epoch:47 step:37040 [D loss: 0.278235, acc.: 89.84%] [G loss: 3.596837]\n",
      "epoch:47 step:37041 [D loss: 0.393526, acc.: 81.25%] [G loss: 4.115406]\n",
      "epoch:47 step:37042 [D loss: 0.359221, acc.: 82.81%] [G loss: 2.440598]\n",
      "epoch:47 step:37043 [D loss: 0.317973, acc.: 86.72%] [G loss: 3.016080]\n",
      "epoch:47 step:37044 [D loss: 0.304970, acc.: 85.94%] [G loss: 2.868124]\n",
      "epoch:47 step:37045 [D loss: 0.231559, acc.: 89.06%] [G loss: 3.105505]\n",
      "epoch:47 step:37046 [D loss: 0.266494, acc.: 89.06%] [G loss: 2.806901]\n",
      "epoch:47 step:37047 [D loss: 0.311951, acc.: 87.50%] [G loss: 4.037382]\n",
      "epoch:47 step:37048 [D loss: 0.356696, acc.: 82.81%] [G loss: 4.898811]\n",
      "epoch:47 step:37049 [D loss: 0.189567, acc.: 91.41%] [G loss: 4.735200]\n",
      "epoch:47 step:37050 [D loss: 0.397915, acc.: 78.91%] [G loss: 4.887856]\n",
      "epoch:47 step:37051 [D loss: 0.333100, acc.: 85.94%] [G loss: 2.891009]\n",
      "epoch:47 step:37052 [D loss: 0.229257, acc.: 89.84%] [G loss: 4.546716]\n",
      "epoch:47 step:37053 [D loss: 0.280837, acc.: 88.28%] [G loss: 4.718126]\n",
      "epoch:47 step:37054 [D loss: 0.219552, acc.: 89.84%] [G loss: 4.084585]\n",
      "epoch:47 step:37055 [D loss: 0.292930, acc.: 88.28%] [G loss: 3.520605]\n",
      "epoch:47 step:37056 [D loss: 0.221695, acc.: 89.06%] [G loss: 3.059262]\n",
      "epoch:47 step:37057 [D loss: 0.257653, acc.: 87.50%] [G loss: 6.091306]\n",
      "epoch:47 step:37058 [D loss: 0.260344, acc.: 89.84%] [G loss: 4.101456]\n",
      "epoch:47 step:37059 [D loss: 0.320541, acc.: 85.16%] [G loss: 4.058414]\n",
      "epoch:47 step:37060 [D loss: 0.340308, acc.: 84.38%] [G loss: 5.005701]\n",
      "epoch:47 step:37061 [D loss: 0.342416, acc.: 85.94%] [G loss: 3.792732]\n",
      "epoch:47 step:37062 [D loss: 0.226692, acc.: 91.41%] [G loss: 3.697709]\n",
      "epoch:47 step:37063 [D loss: 0.308696, acc.: 85.16%] [G loss: 4.162937]\n",
      "epoch:47 step:37064 [D loss: 0.290009, acc.: 85.16%] [G loss: 8.540873]\n",
      "epoch:47 step:37065 [D loss: 0.574311, acc.: 77.34%] [G loss: 5.989055]\n",
      "epoch:47 step:37066 [D loss: 0.778740, acc.: 73.44%] [G loss: 4.743178]\n",
      "epoch:47 step:37067 [D loss: 0.456749, acc.: 82.81%] [G loss: 4.355236]\n",
      "epoch:47 step:37068 [D loss: 0.472694, acc.: 80.47%] [G loss: 2.619863]\n",
      "epoch:47 step:37069 [D loss: 0.332479, acc.: 86.72%] [G loss: 2.936311]\n",
      "epoch:47 step:37070 [D loss: 0.284130, acc.: 88.28%] [G loss: 3.223345]\n",
      "epoch:47 step:37071 [D loss: 0.368748, acc.: 83.59%] [G loss: 3.644044]\n",
      "epoch:47 step:37072 [D loss: 0.343185, acc.: 81.25%] [G loss: 4.804236]\n",
      "epoch:47 step:37073 [D loss: 0.353888, acc.: 82.81%] [G loss: 5.666323]\n",
      "epoch:47 step:37074 [D loss: 0.388819, acc.: 82.03%] [G loss: 4.882152]\n",
      "epoch:47 step:37075 [D loss: 0.391601, acc.: 79.69%] [G loss: 4.143031]\n",
      "epoch:47 step:37076 [D loss: 0.253836, acc.: 89.84%] [G loss: 3.537848]\n",
      "epoch:47 step:37077 [D loss: 0.335584, acc.: 83.59%] [G loss: 3.881712]\n",
      "epoch:47 step:37078 [D loss: 0.232965, acc.: 93.75%] [G loss: 3.106376]\n",
      "epoch:47 step:37079 [D loss: 0.385893, acc.: 80.47%] [G loss: 2.752560]\n",
      "epoch:47 step:37080 [D loss: 0.286894, acc.: 89.06%] [G loss: 3.526475]\n",
      "epoch:47 step:37081 [D loss: 0.308131, acc.: 85.94%] [G loss: 2.843947]\n",
      "epoch:47 step:37082 [D loss: 0.352540, acc.: 85.16%] [G loss: 3.064375]\n",
      "epoch:47 step:37083 [D loss: 0.174319, acc.: 90.62%] [G loss: 3.775841]\n",
      "epoch:47 step:37084 [D loss: 0.335475, acc.: 86.72%] [G loss: 3.753639]\n",
      "epoch:47 step:37085 [D loss: 0.341302, acc.: 82.03%] [G loss: 3.699881]\n",
      "epoch:47 step:37086 [D loss: 0.303519, acc.: 82.81%] [G loss: 4.122244]\n",
      "epoch:47 step:37087 [D loss: 0.322876, acc.: 85.16%] [G loss: 3.134694]\n",
      "epoch:47 step:37088 [D loss: 0.205134, acc.: 89.06%] [G loss: 2.969129]\n",
      "epoch:47 step:37089 [D loss: 0.236190, acc.: 89.84%] [G loss: 3.429341]\n",
      "epoch:47 step:37090 [D loss: 0.275924, acc.: 87.50%] [G loss: 3.966358]\n",
      "epoch:47 step:37091 [D loss: 0.273147, acc.: 86.72%] [G loss: 3.283152]\n",
      "epoch:47 step:37092 [D loss: 0.371898, acc.: 81.25%] [G loss: 2.860620]\n",
      "epoch:47 step:37093 [D loss: 0.341727, acc.: 82.81%] [G loss: 2.598564]\n",
      "epoch:47 step:37094 [D loss: 0.277612, acc.: 85.16%] [G loss: 2.818525]\n",
      "epoch:47 step:37095 [D loss: 0.356224, acc.: 82.03%] [G loss: 2.043820]\n",
      "epoch:47 step:37096 [D loss: 0.467814, acc.: 78.12%] [G loss: 3.260581]\n",
      "epoch:47 step:37097 [D loss: 0.262170, acc.: 88.28%] [G loss: 3.263577]\n",
      "epoch:47 step:37098 [D loss: 0.336108, acc.: 82.03%] [G loss: 3.663272]\n",
      "epoch:47 step:37099 [D loss: 0.320849, acc.: 85.94%] [G loss: 4.276063]\n",
      "epoch:47 step:37100 [D loss: 0.232568, acc.: 87.50%] [G loss: 3.547009]\n",
      "epoch:47 step:37101 [D loss: 0.398117, acc.: 84.38%] [G loss: 2.630479]\n",
      "epoch:47 step:37102 [D loss: 0.399330, acc.: 85.16%] [G loss: 2.240435]\n",
      "epoch:47 step:37103 [D loss: 0.303156, acc.: 86.72%] [G loss: 3.191525]\n",
      "epoch:47 step:37104 [D loss: 0.388498, acc.: 82.81%] [G loss: 2.917848]\n",
      "epoch:47 step:37105 [D loss: 0.268771, acc.: 89.06%] [G loss: 3.174287]\n",
      "epoch:47 step:37106 [D loss: 0.415276, acc.: 81.25%] [G loss: 2.789408]\n",
      "epoch:47 step:37107 [D loss: 0.290288, acc.: 86.72%] [G loss: 2.880102]\n",
      "epoch:47 step:37108 [D loss: 0.331261, acc.: 85.94%] [G loss: 3.765830]\n",
      "epoch:47 step:37109 [D loss: 0.298021, acc.: 86.72%] [G loss: 4.561343]\n",
      "epoch:47 step:37110 [D loss: 0.415996, acc.: 81.25%] [G loss: 3.077197]\n",
      "epoch:47 step:37111 [D loss: 0.227422, acc.: 91.41%] [G loss: 3.024326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37112 [D loss: 0.232801, acc.: 93.75%] [G loss: 3.454526]\n",
      "epoch:47 step:37113 [D loss: 0.334108, acc.: 86.72%] [G loss: 2.706317]\n",
      "epoch:47 step:37114 [D loss: 0.322174, acc.: 82.81%] [G loss: 2.506788]\n",
      "epoch:47 step:37115 [D loss: 0.256973, acc.: 88.28%] [G loss: 3.684856]\n",
      "epoch:47 step:37116 [D loss: 0.311669, acc.: 80.47%] [G loss: 3.094573]\n",
      "epoch:47 step:37117 [D loss: 0.308155, acc.: 86.72%] [G loss: 2.865112]\n",
      "epoch:47 step:37118 [D loss: 0.319045, acc.: 84.38%] [G loss: 2.492608]\n",
      "epoch:47 step:37119 [D loss: 0.369102, acc.: 83.59%] [G loss: 2.902031]\n",
      "epoch:47 step:37120 [D loss: 0.311924, acc.: 85.94%] [G loss: 3.622506]\n",
      "epoch:47 step:37121 [D loss: 0.490961, acc.: 76.56%] [G loss: 2.829771]\n",
      "epoch:47 step:37122 [D loss: 0.336464, acc.: 88.28%] [G loss: 2.979871]\n",
      "epoch:47 step:37123 [D loss: 0.301677, acc.: 85.16%] [G loss: 3.227623]\n",
      "epoch:47 step:37124 [D loss: 0.346768, acc.: 82.03%] [G loss: 2.799849]\n",
      "epoch:47 step:37125 [D loss: 0.302599, acc.: 89.06%] [G loss: 3.244335]\n",
      "epoch:47 step:37126 [D loss: 0.358976, acc.: 87.50%] [G loss: 2.983372]\n",
      "epoch:47 step:37127 [D loss: 0.363196, acc.: 83.59%] [G loss: 2.824787]\n",
      "epoch:47 step:37128 [D loss: 0.287993, acc.: 91.41%] [G loss: 3.004930]\n",
      "epoch:47 step:37129 [D loss: 0.273335, acc.: 92.19%] [G loss: 2.855093]\n",
      "epoch:47 step:37130 [D loss: 0.297368, acc.: 89.84%] [G loss: 2.563255]\n",
      "epoch:47 step:37131 [D loss: 0.356881, acc.: 80.47%] [G loss: 2.590603]\n",
      "epoch:47 step:37132 [D loss: 0.393218, acc.: 83.59%] [G loss: 2.609704]\n",
      "epoch:47 step:37133 [D loss: 0.359656, acc.: 84.38%] [G loss: 2.812366]\n",
      "epoch:47 step:37134 [D loss: 0.224511, acc.: 91.41%] [G loss: 2.896217]\n",
      "epoch:47 step:37135 [D loss: 0.388113, acc.: 75.78%] [G loss: 2.560350]\n",
      "epoch:47 step:37136 [D loss: 0.326401, acc.: 85.94%] [G loss: 2.475474]\n",
      "epoch:47 step:37137 [D loss: 0.331813, acc.: 84.38%] [G loss: 2.901696]\n",
      "epoch:47 step:37138 [D loss: 0.339446, acc.: 84.38%] [G loss: 3.632414]\n",
      "epoch:47 step:37139 [D loss: 0.371653, acc.: 85.16%] [G loss: 2.721900]\n",
      "epoch:47 step:37140 [D loss: 0.262786, acc.: 88.28%] [G loss: 3.282978]\n",
      "epoch:47 step:37141 [D loss: 0.280973, acc.: 88.28%] [G loss: 3.082207]\n",
      "epoch:47 step:37142 [D loss: 0.280907, acc.: 88.28%] [G loss: 3.310695]\n",
      "epoch:47 step:37143 [D loss: 0.421866, acc.: 80.47%] [G loss: 2.191883]\n",
      "epoch:47 step:37144 [D loss: 0.234040, acc.: 92.97%] [G loss: 2.826592]\n",
      "epoch:47 step:37145 [D loss: 0.270169, acc.: 89.06%] [G loss: 3.209892]\n",
      "epoch:47 step:37146 [D loss: 0.432246, acc.: 82.03%] [G loss: 3.202251]\n",
      "epoch:47 step:37147 [D loss: 0.319551, acc.: 86.72%] [G loss: 2.490596]\n",
      "epoch:47 step:37148 [D loss: 0.309337, acc.: 84.38%] [G loss: 2.474044]\n",
      "epoch:47 step:37149 [D loss: 0.290798, acc.: 85.16%] [G loss: 2.973776]\n",
      "epoch:47 step:37150 [D loss: 0.406532, acc.: 78.91%] [G loss: 2.686479]\n",
      "epoch:47 step:37151 [D loss: 0.374244, acc.: 82.81%] [G loss: 5.275501]\n",
      "epoch:47 step:37152 [D loss: 0.393598, acc.: 81.25%] [G loss: 4.481953]\n",
      "epoch:47 step:37153 [D loss: 0.390582, acc.: 82.03%] [G loss: 2.440895]\n",
      "epoch:47 step:37154 [D loss: 0.240622, acc.: 90.62%] [G loss: 3.841088]\n",
      "epoch:47 step:37155 [D loss: 0.408647, acc.: 76.56%] [G loss: 3.922765]\n",
      "epoch:47 step:37156 [D loss: 0.467636, acc.: 78.12%] [G loss: 3.653555]\n",
      "epoch:47 step:37157 [D loss: 0.300291, acc.: 87.50%] [G loss: 3.285489]\n",
      "epoch:47 step:37158 [D loss: 0.255021, acc.: 85.94%] [G loss: 2.982739]\n",
      "epoch:47 step:37159 [D loss: 0.227241, acc.: 90.62%] [G loss: 3.063303]\n",
      "epoch:47 step:37160 [D loss: 0.227867, acc.: 89.84%] [G loss: 4.254166]\n",
      "epoch:47 step:37161 [D loss: 0.262017, acc.: 87.50%] [G loss: 4.361844]\n",
      "epoch:47 step:37162 [D loss: 0.372047, acc.: 84.38%] [G loss: 5.350620]\n",
      "epoch:47 step:37163 [D loss: 0.560579, acc.: 82.81%] [G loss: 7.840854]\n",
      "epoch:47 step:37164 [D loss: 2.406148, acc.: 54.69%] [G loss: 9.375879]\n",
      "epoch:47 step:37165 [D loss: 1.417283, acc.: 67.97%] [G loss: 4.896705]\n",
      "epoch:47 step:37166 [D loss: 0.434667, acc.: 84.38%] [G loss: 4.664168]\n",
      "epoch:47 step:37167 [D loss: 0.678963, acc.: 67.97%] [G loss: 6.122107]\n",
      "epoch:47 step:37168 [D loss: 0.434256, acc.: 82.81%] [G loss: 5.951541]\n",
      "epoch:47 step:37169 [D loss: 0.363554, acc.: 84.38%] [G loss: 6.310140]\n",
      "epoch:47 step:37170 [D loss: 0.386572, acc.: 85.16%] [G loss: 5.964083]\n",
      "epoch:47 step:37171 [D loss: 0.516336, acc.: 79.69%] [G loss: 3.931576]\n",
      "epoch:47 step:37172 [D loss: 0.390952, acc.: 83.59%] [G loss: 3.659489]\n",
      "epoch:47 step:37173 [D loss: 0.377209, acc.: 82.81%] [G loss: 3.483868]\n",
      "epoch:47 step:37174 [D loss: 0.304403, acc.: 85.16%] [G loss: 4.029398]\n",
      "epoch:47 step:37175 [D loss: 0.257920, acc.: 88.28%] [G loss: 2.898201]\n",
      "epoch:47 step:37176 [D loss: 0.284708, acc.: 89.06%] [G loss: 4.725657]\n",
      "epoch:47 step:37177 [D loss: 0.412758, acc.: 83.59%] [G loss: 2.983471]\n",
      "epoch:47 step:37178 [D loss: 0.245950, acc.: 89.84%] [G loss: 2.832591]\n",
      "epoch:47 step:37179 [D loss: 0.457809, acc.: 77.34%] [G loss: 3.098203]\n",
      "epoch:47 step:37180 [D loss: 0.333633, acc.: 84.38%] [G loss: 3.073146]\n",
      "epoch:47 step:37181 [D loss: 0.457443, acc.: 81.25%] [G loss: 2.703064]\n",
      "epoch:47 step:37182 [D loss: 0.366643, acc.: 81.25%] [G loss: 3.048525]\n",
      "epoch:47 step:37183 [D loss: 0.306826, acc.: 87.50%] [G loss: 2.615288]\n",
      "epoch:47 step:37184 [D loss: 0.296788, acc.: 88.28%] [G loss: 3.258206]\n",
      "epoch:47 step:37185 [D loss: 0.326370, acc.: 85.16%] [G loss: 2.813684]\n",
      "epoch:47 step:37186 [D loss: 0.355852, acc.: 82.03%] [G loss: 2.973273]\n",
      "epoch:47 step:37187 [D loss: 0.229433, acc.: 89.06%] [G loss: 3.859076]\n",
      "epoch:47 step:37188 [D loss: 0.345262, acc.: 85.94%] [G loss: 2.122395]\n",
      "epoch:47 step:37189 [D loss: 0.347509, acc.: 87.50%] [G loss: 2.740749]\n",
      "epoch:47 step:37190 [D loss: 0.328867, acc.: 85.94%] [G loss: 2.536675]\n",
      "epoch:47 step:37191 [D loss: 0.321988, acc.: 85.16%] [G loss: 3.185541]\n",
      "epoch:47 step:37192 [D loss: 0.295754, acc.: 87.50%] [G loss: 3.597086]\n",
      "epoch:47 step:37193 [D loss: 0.333061, acc.: 84.38%] [G loss: 2.904596]\n",
      "epoch:47 step:37194 [D loss: 0.336213, acc.: 83.59%] [G loss: 3.159160]\n",
      "epoch:47 step:37195 [D loss: 0.325240, acc.: 84.38%] [G loss: 3.244974]\n",
      "epoch:47 step:37196 [D loss: 0.268568, acc.: 92.19%] [G loss: 3.346693]\n",
      "epoch:47 step:37197 [D loss: 0.298458, acc.: 87.50%] [G loss: 3.329159]\n",
      "epoch:47 step:37198 [D loss: 0.345494, acc.: 82.81%] [G loss: 3.512081]\n",
      "epoch:47 step:37199 [D loss: 0.212555, acc.: 89.84%] [G loss: 4.050120]\n",
      "epoch:47 step:37200 [D loss: 0.276484, acc.: 90.62%] [G loss: 3.423282]\n",
      "epoch:47 step:37201 [D loss: 0.206370, acc.: 92.97%] [G loss: 3.333025]\n",
      "epoch:47 step:37202 [D loss: 0.393534, acc.: 80.47%] [G loss: 3.603358]\n",
      "epoch:47 step:37203 [D loss: 0.213566, acc.: 90.62%] [G loss: 3.215929]\n",
      "epoch:47 step:37204 [D loss: 0.243776, acc.: 89.06%] [G loss: 3.132480]\n",
      "epoch:47 step:37205 [D loss: 0.339038, acc.: 85.16%] [G loss: 3.412116]\n",
      "epoch:47 step:37206 [D loss: 0.247199, acc.: 91.41%] [G loss: 2.906116]\n",
      "epoch:47 step:37207 [D loss: 0.305206, acc.: 87.50%] [G loss: 3.027139]\n",
      "epoch:47 step:37208 [D loss: 0.287827, acc.: 89.84%] [G loss: 2.573137]\n",
      "epoch:47 step:37209 [D loss: 0.369359, acc.: 84.38%] [G loss: 2.855010]\n",
      "epoch:47 step:37210 [D loss: 0.311884, acc.: 87.50%] [G loss: 3.045228]\n",
      "epoch:47 step:37211 [D loss: 0.304521, acc.: 82.81%] [G loss: 3.843842]\n",
      "epoch:47 step:37212 [D loss: 0.315306, acc.: 86.72%] [G loss: 2.161563]\n",
      "epoch:47 step:37213 [D loss: 0.339399, acc.: 86.72%] [G loss: 3.120448]\n",
      "epoch:47 step:37214 [D loss: 0.335523, acc.: 84.38%] [G loss: 2.377062]\n",
      "epoch:47 step:37215 [D loss: 0.269231, acc.: 91.41%] [G loss: 2.768724]\n",
      "epoch:47 step:37216 [D loss: 0.405378, acc.: 81.25%] [G loss: 3.251845]\n",
      "epoch:47 step:37217 [D loss: 0.268304, acc.: 88.28%] [G loss: 3.369203]\n",
      "epoch:47 step:37218 [D loss: 0.320516, acc.: 86.72%] [G loss: 3.542822]\n",
      "epoch:47 step:37219 [D loss: 0.231383, acc.: 92.19%] [G loss: 3.414154]\n",
      "epoch:47 step:37220 [D loss: 0.397270, acc.: 80.47%] [G loss: 2.617249]\n",
      "epoch:47 step:37221 [D loss: 0.309230, acc.: 86.72%] [G loss: 3.431814]\n",
      "epoch:47 step:37222 [D loss: 0.337807, acc.: 85.16%] [G loss: 2.718086]\n",
      "epoch:47 step:37223 [D loss: 0.233127, acc.: 90.62%] [G loss: 2.953455]\n",
      "epoch:47 step:37224 [D loss: 0.317268, acc.: 87.50%] [G loss: 2.587118]\n",
      "epoch:47 step:37225 [D loss: 0.157491, acc.: 95.31%] [G loss: 3.324187]\n",
      "epoch:47 step:37226 [D loss: 0.302834, acc.: 85.16%] [G loss: 3.613611]\n",
      "epoch:47 step:37227 [D loss: 0.379545, acc.: 82.03%] [G loss: 3.166733]\n",
      "epoch:47 step:37228 [D loss: 0.272240, acc.: 88.28%] [G loss: 4.380373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37229 [D loss: 0.261291, acc.: 88.28%] [G loss: 2.686840]\n",
      "epoch:47 step:37230 [D loss: 0.348333, acc.: 85.16%] [G loss: 3.788906]\n",
      "epoch:47 step:37231 [D loss: 0.168654, acc.: 95.31%] [G loss: 4.101088]\n",
      "epoch:47 step:37232 [D loss: 0.258752, acc.: 90.62%] [G loss: 4.815125]\n",
      "epoch:47 step:37233 [D loss: 0.344084, acc.: 87.50%] [G loss: 3.320919]\n",
      "epoch:47 step:37234 [D loss: 0.224424, acc.: 92.19%] [G loss: 4.864267]\n",
      "epoch:47 step:37235 [D loss: 0.225142, acc.: 90.62%] [G loss: 3.912362]\n",
      "epoch:47 step:37236 [D loss: 0.328438, acc.: 84.38%] [G loss: 3.056129]\n",
      "epoch:47 step:37237 [D loss: 0.356730, acc.: 84.38%] [G loss: 2.910396]\n",
      "epoch:47 step:37238 [D loss: 0.356008, acc.: 82.03%] [G loss: 2.438294]\n",
      "epoch:47 step:37239 [D loss: 0.265473, acc.: 89.84%] [G loss: 2.383943]\n",
      "epoch:47 step:37240 [D loss: 0.307143, acc.: 85.16%] [G loss: 3.342135]\n",
      "epoch:47 step:37241 [D loss: 0.313837, acc.: 85.94%] [G loss: 3.736589]\n",
      "epoch:47 step:37242 [D loss: 0.249412, acc.: 86.72%] [G loss: 3.412413]\n",
      "epoch:47 step:37243 [D loss: 0.383924, acc.: 82.03%] [G loss: 3.183742]\n",
      "epoch:47 step:37244 [D loss: 0.332551, acc.: 83.59%] [G loss: 3.858352]\n",
      "epoch:47 step:37245 [D loss: 0.338296, acc.: 83.59%] [G loss: 3.338864]\n",
      "epoch:47 step:37246 [D loss: 0.305701, acc.: 86.72%] [G loss: 3.452402]\n",
      "epoch:47 step:37247 [D loss: 0.328198, acc.: 85.94%] [G loss: 2.891449]\n",
      "epoch:47 step:37248 [D loss: 0.316331, acc.: 83.59%] [G loss: 3.972910]\n",
      "epoch:47 step:37249 [D loss: 0.376162, acc.: 83.59%] [G loss: 3.357475]\n",
      "epoch:47 step:37250 [D loss: 0.365452, acc.: 82.03%] [G loss: 3.702974]\n",
      "epoch:47 step:37251 [D loss: 0.445852, acc.: 74.22%] [G loss: 3.600915]\n",
      "epoch:47 step:37252 [D loss: 0.402783, acc.: 78.12%] [G loss: 3.596290]\n",
      "epoch:47 step:37253 [D loss: 0.259976, acc.: 88.28%] [G loss: 2.840712]\n",
      "epoch:47 step:37254 [D loss: 0.408692, acc.: 80.47%] [G loss: 3.070196]\n",
      "epoch:47 step:37255 [D loss: 0.205369, acc.: 92.19%] [G loss: 3.509430]\n",
      "epoch:47 step:37256 [D loss: 0.308539, acc.: 83.59%] [G loss: 2.530269]\n",
      "epoch:47 step:37257 [D loss: 0.314090, acc.: 84.38%] [G loss: 2.914825]\n",
      "epoch:47 step:37258 [D loss: 0.297133, acc.: 87.50%] [G loss: 2.937511]\n",
      "epoch:47 step:37259 [D loss: 0.408721, acc.: 78.91%] [G loss: 2.572742]\n",
      "epoch:47 step:37260 [D loss: 0.244952, acc.: 88.28%] [G loss: 2.299763]\n",
      "epoch:47 step:37261 [D loss: 0.405447, acc.: 79.69%] [G loss: 3.570458]\n",
      "epoch:47 step:37262 [D loss: 0.380290, acc.: 80.47%] [G loss: 4.048527]\n",
      "epoch:47 step:37263 [D loss: 0.331808, acc.: 85.16%] [G loss: 3.555852]\n",
      "epoch:47 step:37264 [D loss: 0.208663, acc.: 89.06%] [G loss: 4.256865]\n",
      "epoch:47 step:37265 [D loss: 0.288661, acc.: 89.06%] [G loss: 4.499526]\n",
      "epoch:47 step:37266 [D loss: 0.240222, acc.: 88.28%] [G loss: 4.617510]\n",
      "epoch:47 step:37267 [D loss: 0.332187, acc.: 83.59%] [G loss: 4.246873]\n",
      "epoch:47 step:37268 [D loss: 0.260386, acc.: 89.84%] [G loss: 5.125630]\n",
      "epoch:47 step:37269 [D loss: 0.350273, acc.: 85.16%] [G loss: 2.889893]\n",
      "epoch:47 step:37270 [D loss: 0.197286, acc.: 93.75%] [G loss: 3.849335]\n",
      "epoch:47 step:37271 [D loss: 0.269663, acc.: 87.50%] [G loss: 2.373106]\n",
      "epoch:47 step:37272 [D loss: 0.388414, acc.: 82.03%] [G loss: 4.182928]\n",
      "epoch:47 step:37273 [D loss: 0.423990, acc.: 82.03%] [G loss: 3.115198]\n",
      "epoch:47 step:37274 [D loss: 0.274222, acc.: 86.72%] [G loss: 3.477254]\n",
      "epoch:47 step:37275 [D loss: 0.315466, acc.: 85.16%] [G loss: 3.493069]\n",
      "epoch:47 step:37276 [D loss: 0.343405, acc.: 86.72%] [G loss: 3.269897]\n",
      "epoch:47 step:37277 [D loss: 0.389733, acc.: 82.81%] [G loss: 3.581334]\n",
      "epoch:47 step:37278 [D loss: 0.381207, acc.: 83.59%] [G loss: 3.358617]\n",
      "epoch:47 step:37279 [D loss: 0.359655, acc.: 84.38%] [G loss: 4.647801]\n",
      "epoch:47 step:37280 [D loss: 0.313740, acc.: 86.72%] [G loss: 5.917959]\n",
      "epoch:47 step:37281 [D loss: 0.224375, acc.: 89.06%] [G loss: 7.668415]\n",
      "epoch:47 step:37282 [D loss: 0.285850, acc.: 86.72%] [G loss: 3.840273]\n",
      "epoch:47 step:37283 [D loss: 0.296356, acc.: 86.72%] [G loss: 5.545027]\n",
      "epoch:47 step:37284 [D loss: 0.247807, acc.: 89.06%] [G loss: 5.100443]\n",
      "epoch:47 step:37285 [D loss: 0.272505, acc.: 84.38%] [G loss: 4.251059]\n",
      "epoch:47 step:37286 [D loss: 0.325411, acc.: 83.59%] [G loss: 3.419406]\n",
      "epoch:47 step:37287 [D loss: 0.311382, acc.: 89.84%] [G loss: 3.056327]\n",
      "epoch:47 step:37288 [D loss: 0.322806, acc.: 88.28%] [G loss: 2.626134]\n",
      "epoch:47 step:37289 [D loss: 0.189355, acc.: 93.75%] [G loss: 3.149104]\n",
      "epoch:47 step:37290 [D loss: 0.309860, acc.: 88.28%] [G loss: 3.195818]\n",
      "epoch:47 step:37291 [D loss: 0.279766, acc.: 87.50%] [G loss: 3.793097]\n",
      "epoch:47 step:37292 [D loss: 0.206270, acc.: 92.19%] [G loss: 3.114170]\n",
      "epoch:47 step:37293 [D loss: 0.316889, acc.: 85.16%] [G loss: 2.845882]\n",
      "epoch:47 step:37294 [D loss: 0.253863, acc.: 89.84%] [G loss: 3.804145]\n",
      "epoch:47 step:37295 [D loss: 0.257956, acc.: 89.84%] [G loss: 2.740503]\n",
      "epoch:47 step:37296 [D loss: 0.265896, acc.: 90.62%] [G loss: 2.299810]\n",
      "epoch:47 step:37297 [D loss: 0.326180, acc.: 83.59%] [G loss: 2.969974]\n",
      "epoch:47 step:37298 [D loss: 0.321833, acc.: 83.59%] [G loss: 3.215945]\n",
      "epoch:47 step:37299 [D loss: 0.245789, acc.: 89.06%] [G loss: 3.694994]\n",
      "epoch:47 step:37300 [D loss: 0.275107, acc.: 88.28%] [G loss: 3.307718]\n",
      "epoch:47 step:37301 [D loss: 0.251072, acc.: 88.28%] [G loss: 3.423099]\n",
      "epoch:47 step:37302 [D loss: 0.291346, acc.: 89.06%] [G loss: 3.104540]\n",
      "epoch:47 step:37303 [D loss: 0.377881, acc.: 80.47%] [G loss: 3.111999]\n",
      "epoch:47 step:37304 [D loss: 0.321785, acc.: 83.59%] [G loss: 3.613104]\n",
      "epoch:47 step:37305 [D loss: 0.347468, acc.: 84.38%] [G loss: 3.273187]\n",
      "epoch:47 step:37306 [D loss: 0.243080, acc.: 89.06%] [G loss: 3.474226]\n",
      "epoch:47 step:37307 [D loss: 0.292553, acc.: 82.81%] [G loss: 2.557935]\n",
      "epoch:47 step:37308 [D loss: 0.208449, acc.: 92.19%] [G loss: 3.844449]\n",
      "epoch:47 step:37309 [D loss: 0.342144, acc.: 84.38%] [G loss: 3.047270]\n",
      "epoch:47 step:37310 [D loss: 0.245856, acc.: 91.41%] [G loss: 2.527921]\n",
      "epoch:47 step:37311 [D loss: 0.270574, acc.: 89.06%] [G loss: 2.967735]\n",
      "epoch:47 step:37312 [D loss: 0.298804, acc.: 89.06%] [G loss: 2.461411]\n",
      "epoch:47 step:37313 [D loss: 0.309506, acc.: 84.38%] [G loss: 2.534121]\n",
      "epoch:47 step:37314 [D loss: 0.300673, acc.: 85.94%] [G loss: 2.644066]\n",
      "epoch:47 step:37315 [D loss: 0.203115, acc.: 92.19%] [G loss: 2.973695]\n",
      "epoch:47 step:37316 [D loss: 0.320244, acc.: 88.28%] [G loss: 3.392900]\n",
      "epoch:47 step:37317 [D loss: 0.421658, acc.: 82.81%] [G loss: 3.300877]\n",
      "epoch:47 step:37318 [D loss: 0.240498, acc.: 89.06%] [G loss: 3.106173]\n",
      "epoch:47 step:37319 [D loss: 0.218601, acc.: 89.84%] [G loss: 5.849856]\n",
      "epoch:47 step:37320 [D loss: 0.241800, acc.: 92.19%] [G loss: 3.858729]\n",
      "epoch:47 step:37321 [D loss: 0.309248, acc.: 89.06%] [G loss: 4.324249]\n",
      "epoch:47 step:37322 [D loss: 0.190146, acc.: 91.41%] [G loss: 4.018765]\n",
      "epoch:47 step:37323 [D loss: 0.268095, acc.: 88.28%] [G loss: 2.855170]\n",
      "epoch:47 step:37324 [D loss: 0.309205, acc.: 84.38%] [G loss: 4.316944]\n",
      "epoch:47 step:37325 [D loss: 0.315968, acc.: 86.72%] [G loss: 3.436289]\n",
      "epoch:47 step:37326 [D loss: 0.244692, acc.: 90.62%] [G loss: 3.403667]\n",
      "epoch:47 step:37327 [D loss: 0.331660, acc.: 86.72%] [G loss: 3.960375]\n",
      "epoch:47 step:37328 [D loss: 0.280455, acc.: 85.94%] [G loss: 2.671890]\n",
      "epoch:47 step:37329 [D loss: 0.391233, acc.: 84.38%] [G loss: 2.684041]\n",
      "epoch:47 step:37330 [D loss: 0.323032, acc.: 89.06%] [G loss: 2.568288]\n",
      "epoch:47 step:37331 [D loss: 0.274512, acc.: 85.16%] [G loss: 3.452287]\n",
      "epoch:47 step:37332 [D loss: 0.334891, acc.: 84.38%] [G loss: 3.372394]\n",
      "epoch:47 step:37333 [D loss: 0.325904, acc.: 83.59%] [G loss: 2.975128]\n",
      "epoch:47 step:37334 [D loss: 0.327277, acc.: 86.72%] [G loss: 2.755536]\n",
      "epoch:47 step:37335 [D loss: 0.303713, acc.: 89.84%] [G loss: 3.485339]\n",
      "epoch:47 step:37336 [D loss: 0.393829, acc.: 82.03%] [G loss: 3.630504]\n",
      "epoch:47 step:37337 [D loss: 0.407580, acc.: 79.69%] [G loss: 3.426096]\n",
      "epoch:47 step:37338 [D loss: 0.272864, acc.: 89.84%] [G loss: 3.433824]\n",
      "epoch:47 step:37339 [D loss: 0.283864, acc.: 85.94%] [G loss: 2.917247]\n",
      "epoch:47 step:37340 [D loss: 0.326934, acc.: 85.94%] [G loss: 2.831638]\n",
      "epoch:47 step:37341 [D loss: 0.419944, acc.: 79.69%] [G loss: 2.218384]\n",
      "epoch:47 step:37342 [D loss: 0.238778, acc.: 91.41%] [G loss: 2.863564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37343 [D loss: 0.321055, acc.: 88.28%] [G loss: 3.432693]\n",
      "epoch:47 step:37344 [D loss: 0.273939, acc.: 89.84%] [G loss: 4.327752]\n",
      "epoch:47 step:37345 [D loss: 0.241847, acc.: 88.28%] [G loss: 2.921700]\n",
      "epoch:47 step:37346 [D loss: 0.299585, acc.: 84.38%] [G loss: 4.263720]\n",
      "epoch:47 step:37347 [D loss: 0.170969, acc.: 94.53%] [G loss: 3.857738]\n",
      "epoch:47 step:37348 [D loss: 0.295231, acc.: 84.38%] [G loss: 3.883964]\n",
      "epoch:47 step:37349 [D loss: 0.275628, acc.: 85.94%] [G loss: 4.433020]\n",
      "epoch:47 step:37350 [D loss: 0.442271, acc.: 76.56%] [G loss: 3.604732]\n",
      "epoch:47 step:37351 [D loss: 0.297239, acc.: 85.16%] [G loss: 3.028422]\n",
      "epoch:47 step:37352 [D loss: 0.265627, acc.: 88.28%] [G loss: 2.615530]\n",
      "epoch:47 step:37353 [D loss: 0.274867, acc.: 91.41%] [G loss: 2.609678]\n",
      "epoch:47 step:37354 [D loss: 0.244271, acc.: 92.19%] [G loss: 2.896003]\n",
      "epoch:47 step:37355 [D loss: 0.331409, acc.: 84.38%] [G loss: 2.704484]\n",
      "epoch:47 step:37356 [D loss: 0.332226, acc.: 83.59%] [G loss: 2.597996]\n",
      "epoch:47 step:37357 [D loss: 0.352949, acc.: 82.81%] [G loss: 2.887359]\n",
      "epoch:47 step:37358 [D loss: 0.437562, acc.: 78.91%] [G loss: 3.134944]\n",
      "epoch:47 step:37359 [D loss: 0.354011, acc.: 83.59%] [G loss: 2.458901]\n",
      "epoch:47 step:37360 [D loss: 0.317726, acc.: 85.16%] [G loss: 3.558224]\n",
      "epoch:47 step:37361 [D loss: 0.386465, acc.: 81.25%] [G loss: 3.418684]\n",
      "epoch:47 step:37362 [D loss: 0.217355, acc.: 90.62%] [G loss: 3.788332]\n",
      "epoch:47 step:37363 [D loss: 0.238946, acc.: 89.84%] [G loss: 3.576822]\n",
      "epoch:47 step:37364 [D loss: 0.283480, acc.: 87.50%] [G loss: 2.845183]\n",
      "epoch:47 step:37365 [D loss: 0.230535, acc.: 92.19%] [G loss: 3.144554]\n",
      "epoch:47 step:37366 [D loss: 0.264337, acc.: 89.06%] [G loss: 3.548021]\n",
      "epoch:47 step:37367 [D loss: 0.291207, acc.: 86.72%] [G loss: 3.681226]\n",
      "epoch:47 step:37368 [D loss: 0.315775, acc.: 85.94%] [G loss: 3.375188]\n",
      "epoch:47 step:37369 [D loss: 0.317980, acc.: 84.38%] [G loss: 4.271647]\n",
      "epoch:47 step:37370 [D loss: 0.314818, acc.: 84.38%] [G loss: 5.033553]\n",
      "epoch:47 step:37371 [D loss: 0.304301, acc.: 87.50%] [G loss: 3.387066]\n",
      "epoch:47 step:37372 [D loss: 0.219124, acc.: 91.41%] [G loss: 4.385372]\n",
      "epoch:47 step:37373 [D loss: 0.256118, acc.: 89.84%] [G loss: 3.500263]\n",
      "epoch:47 step:37374 [D loss: 0.173255, acc.: 95.31%] [G loss: 3.458585]\n",
      "epoch:47 step:37375 [D loss: 0.252952, acc.: 88.28%] [G loss: 3.342681]\n",
      "epoch:47 step:37376 [D loss: 0.336841, acc.: 85.16%] [G loss: 2.812913]\n",
      "epoch:47 step:37377 [D loss: 0.238830, acc.: 91.41%] [G loss: 3.916521]\n",
      "epoch:47 step:37378 [D loss: 0.217593, acc.: 90.62%] [G loss: 3.353817]\n",
      "epoch:47 step:37379 [D loss: 0.290750, acc.: 85.94%] [G loss: 3.329547]\n",
      "epoch:47 step:37380 [D loss: 0.317091, acc.: 85.94%] [G loss: 2.716520]\n",
      "epoch:47 step:37381 [D loss: 0.369900, acc.: 82.81%] [G loss: 3.727957]\n",
      "epoch:47 step:37382 [D loss: 0.360261, acc.: 86.72%] [G loss: 3.984940]\n",
      "epoch:47 step:37383 [D loss: 0.392379, acc.: 82.03%] [G loss: 4.431512]\n",
      "epoch:47 step:37384 [D loss: 0.346783, acc.: 83.59%] [G loss: 3.901722]\n",
      "epoch:47 step:37385 [D loss: 0.291226, acc.: 86.72%] [G loss: 3.907179]\n",
      "epoch:47 step:37386 [D loss: 0.321478, acc.: 85.16%] [G loss: 2.606874]\n",
      "epoch:47 step:37387 [D loss: 0.349025, acc.: 86.72%] [G loss: 2.737471]\n",
      "epoch:47 step:37388 [D loss: 0.282830, acc.: 88.28%] [G loss: 2.961381]\n",
      "epoch:47 step:37389 [D loss: 0.403856, acc.: 78.91%] [G loss: 2.761628]\n",
      "epoch:47 step:37390 [D loss: 0.303056, acc.: 85.16%] [G loss: 3.409332]\n",
      "epoch:47 step:37391 [D loss: 0.284785, acc.: 87.50%] [G loss: 4.303417]\n",
      "epoch:47 step:37392 [D loss: 0.292047, acc.: 90.62%] [G loss: 3.553307]\n",
      "epoch:47 step:37393 [D loss: 0.266096, acc.: 89.84%] [G loss: 3.228807]\n",
      "epoch:47 step:37394 [D loss: 0.214479, acc.: 92.19%] [G loss: 2.846232]\n",
      "epoch:47 step:37395 [D loss: 0.217738, acc.: 90.62%] [G loss: 3.328282]\n",
      "epoch:47 step:37396 [D loss: 0.307076, acc.: 86.72%] [G loss: 3.692762]\n",
      "epoch:47 step:37397 [D loss: 0.273868, acc.: 91.41%] [G loss: 3.190922]\n",
      "epoch:47 step:37398 [D loss: 0.408111, acc.: 81.25%] [G loss: 3.831434]\n",
      "epoch:47 step:37399 [D loss: 0.433226, acc.: 78.91%] [G loss: 2.626120]\n",
      "epoch:47 step:37400 [D loss: 0.267004, acc.: 89.06%] [G loss: 2.754912]\n",
      "epoch:47 step:37401 [D loss: 0.281477, acc.: 88.28%] [G loss: 2.960991]\n",
      "epoch:47 step:37402 [D loss: 0.241156, acc.: 89.06%] [G loss: 4.850458]\n",
      "epoch:47 step:37403 [D loss: 0.241733, acc.: 91.41%] [G loss: 7.665488]\n",
      "epoch:47 step:37404 [D loss: 0.250525, acc.: 88.28%] [G loss: 4.399466]\n",
      "epoch:47 step:37405 [D loss: 0.332122, acc.: 85.16%] [G loss: 4.352013]\n",
      "epoch:47 step:37406 [D loss: 0.243570, acc.: 89.06%] [G loss: 5.060657]\n",
      "epoch:47 step:37407 [D loss: 0.267026, acc.: 90.62%] [G loss: 4.434602]\n",
      "epoch:47 step:37408 [D loss: 0.236656, acc.: 93.75%] [G loss: 4.139181]\n",
      "epoch:47 step:37409 [D loss: 0.205160, acc.: 91.41%] [G loss: 4.861893]\n",
      "epoch:47 step:37410 [D loss: 0.384964, acc.: 84.38%] [G loss: 4.565779]\n",
      "epoch:47 step:37411 [D loss: 0.281167, acc.: 86.72%] [G loss: 3.383736]\n",
      "epoch:47 step:37412 [D loss: 0.265847, acc.: 88.28%] [G loss: 2.995653]\n",
      "epoch:47 step:37413 [D loss: 0.342583, acc.: 83.59%] [G loss: 3.123492]\n",
      "epoch:47 step:37414 [D loss: 0.421948, acc.: 77.34%] [G loss: 3.970894]\n",
      "epoch:47 step:37415 [D loss: 0.306034, acc.: 84.38%] [G loss: 3.474981]\n",
      "epoch:47 step:37416 [D loss: 0.239686, acc.: 90.62%] [G loss: 3.917668]\n",
      "epoch:47 step:37417 [D loss: 0.328203, acc.: 83.59%] [G loss: 3.735759]\n",
      "epoch:47 step:37418 [D loss: 0.224102, acc.: 92.19%] [G loss: 4.024549]\n",
      "epoch:47 step:37419 [D loss: 0.391458, acc.: 83.59%] [G loss: 3.264584]\n",
      "epoch:47 step:37420 [D loss: 0.326846, acc.: 82.81%] [G loss: 3.651605]\n",
      "epoch:47 step:37421 [D loss: 0.287708, acc.: 85.16%] [G loss: 4.018960]\n",
      "epoch:47 step:37422 [D loss: 0.294093, acc.: 85.94%] [G loss: 3.105502]\n",
      "epoch:47 step:37423 [D loss: 0.243355, acc.: 92.19%] [G loss: 4.005305]\n",
      "epoch:47 step:37424 [D loss: 0.268753, acc.: 88.28%] [G loss: 2.753602]\n",
      "epoch:47 step:37425 [D loss: 0.316549, acc.: 85.94%] [G loss: 4.328478]\n",
      "epoch:47 step:37426 [D loss: 0.252159, acc.: 89.06%] [G loss: 2.750571]\n",
      "epoch:47 step:37427 [D loss: 0.269310, acc.: 90.62%] [G loss: 2.518521]\n",
      "epoch:47 step:37428 [D loss: 0.338925, acc.: 82.81%] [G loss: 2.840537]\n",
      "epoch:47 step:37429 [D loss: 0.374691, acc.: 82.03%] [G loss: 3.770715]\n",
      "epoch:47 step:37430 [D loss: 0.381013, acc.: 85.94%] [G loss: 3.957998]\n",
      "epoch:47 step:37431 [D loss: 0.333060, acc.: 85.94%] [G loss: 3.864598]\n",
      "epoch:47 step:37432 [D loss: 0.187376, acc.: 92.19%] [G loss: 4.120775]\n",
      "epoch:47 step:37433 [D loss: 0.275751, acc.: 87.50%] [G loss: 2.841426]\n",
      "epoch:47 step:37434 [D loss: 0.263364, acc.: 85.94%] [G loss: 3.130724]\n",
      "epoch:47 step:37435 [D loss: 0.464532, acc.: 78.91%] [G loss: 2.562827]\n",
      "epoch:47 step:37436 [D loss: 0.287882, acc.: 87.50%] [G loss: 3.629398]\n",
      "epoch:47 step:37437 [D loss: 0.295151, acc.: 87.50%] [G loss: 3.334567]\n",
      "epoch:47 step:37438 [D loss: 0.271774, acc.: 88.28%] [G loss: 3.179498]\n",
      "epoch:47 step:37439 [D loss: 0.253821, acc.: 90.62%] [G loss: 2.750269]\n",
      "epoch:47 step:37440 [D loss: 0.234423, acc.: 90.62%] [G loss: 2.555394]\n",
      "epoch:47 step:37441 [D loss: 0.332911, acc.: 85.94%] [G loss: 2.676083]\n",
      "epoch:47 step:37442 [D loss: 0.387695, acc.: 86.72%] [G loss: 2.835936]\n",
      "epoch:47 step:37443 [D loss: 0.276265, acc.: 85.16%] [G loss: 3.162141]\n",
      "epoch:47 step:37444 [D loss: 0.301345, acc.: 88.28%] [G loss: 4.193190]\n",
      "epoch:47 step:37445 [D loss: 0.334063, acc.: 85.16%] [G loss: 2.703936]\n",
      "epoch:47 step:37446 [D loss: 0.212431, acc.: 92.97%] [G loss: 2.686583]\n",
      "epoch:47 step:37447 [D loss: 0.254428, acc.: 89.06%] [G loss: 3.264538]\n",
      "epoch:47 step:37448 [D loss: 0.212852, acc.: 93.75%] [G loss: 3.097784]\n",
      "epoch:47 step:37449 [D loss: 0.206847, acc.: 93.75%] [G loss: 3.309097]\n",
      "epoch:47 step:37450 [D loss: 0.353151, acc.: 82.81%] [G loss: 3.707964]\n",
      "epoch:47 step:37451 [D loss: 0.268754, acc.: 87.50%] [G loss: 3.406171]\n",
      "epoch:47 step:37452 [D loss: 0.275580, acc.: 86.72%] [G loss: 2.514492]\n",
      "epoch:47 step:37453 [D loss: 0.284238, acc.: 86.72%] [G loss: 3.347146]\n",
      "epoch:47 step:37454 [D loss: 0.227380, acc.: 93.75%] [G loss: 3.350518]\n",
      "epoch:47 step:37455 [D loss: 0.255186, acc.: 86.72%] [G loss: 2.903175]\n",
      "epoch:47 step:37456 [D loss: 0.296904, acc.: 89.06%] [G loss: 2.601946]\n",
      "epoch:47 step:37457 [D loss: 0.262331, acc.: 86.72%] [G loss: 3.168558]\n",
      "epoch:47 step:37458 [D loss: 0.312807, acc.: 83.59%] [G loss: 3.209658]\n",
      "epoch:47 step:37459 [D loss: 0.282363, acc.: 88.28%] [G loss: 3.321243]\n",
      "epoch:47 step:37460 [D loss: 0.375723, acc.: 79.69%] [G loss: 2.938852]\n",
      "epoch:47 step:37461 [D loss: 0.258968, acc.: 88.28%] [G loss: 3.117588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37462 [D loss: 0.289466, acc.: 88.28%] [G loss: 2.518704]\n",
      "epoch:47 step:37463 [D loss: 0.311516, acc.: 85.94%] [G loss: 2.937250]\n",
      "epoch:47 step:37464 [D loss: 0.247525, acc.: 90.62%] [G loss: 2.716330]\n",
      "epoch:47 step:37465 [D loss: 0.236898, acc.: 89.84%] [G loss: 3.170920]\n",
      "epoch:47 step:37466 [D loss: 0.333441, acc.: 85.16%] [G loss: 2.824251]\n",
      "epoch:47 step:37467 [D loss: 0.285393, acc.: 88.28%] [G loss: 3.665664]\n",
      "epoch:47 step:37468 [D loss: 0.211516, acc.: 90.62%] [G loss: 3.677488]\n",
      "epoch:47 step:37469 [D loss: 0.364216, acc.: 86.72%] [G loss: 3.085919]\n",
      "epoch:47 step:37470 [D loss: 0.425189, acc.: 80.47%] [G loss: 3.930217]\n",
      "epoch:47 step:37471 [D loss: 0.261785, acc.: 88.28%] [G loss: 3.962988]\n",
      "epoch:47 step:37472 [D loss: 0.404118, acc.: 81.25%] [G loss: 5.046729]\n",
      "epoch:47 step:37473 [D loss: 0.270583, acc.: 86.72%] [G loss: 4.331499]\n",
      "epoch:47 step:37474 [D loss: 0.225852, acc.: 89.84%] [G loss: 5.489810]\n",
      "epoch:47 step:37475 [D loss: 0.265802, acc.: 88.28%] [G loss: 3.696266]\n",
      "epoch:47 step:37476 [D loss: 0.229788, acc.: 91.41%] [G loss: 3.308447]\n",
      "epoch:47 step:37477 [D loss: 0.277665, acc.: 87.50%] [G loss: 4.041717]\n",
      "epoch:47 step:37478 [D loss: 0.468328, acc.: 78.91%] [G loss: 2.606188]\n",
      "epoch:47 step:37479 [D loss: 0.309424, acc.: 85.94%] [G loss: 5.145283]\n",
      "epoch:47 step:37480 [D loss: 0.288105, acc.: 85.94%] [G loss: 3.846760]\n",
      "epoch:47 step:37481 [D loss: 0.472333, acc.: 76.56%] [G loss: 4.268162]\n",
      "epoch:47 step:37482 [D loss: 0.668217, acc.: 75.00%] [G loss: 7.960899]\n",
      "epoch:47 step:37483 [D loss: 0.687349, acc.: 75.78%] [G loss: 7.976814]\n",
      "epoch:47 step:37484 [D loss: 1.335771, acc.: 64.84%] [G loss: 8.054609]\n",
      "epoch:47 step:37485 [D loss: 1.823186, acc.: 56.25%] [G loss: 4.342171]\n",
      "epoch:47 step:37486 [D loss: 0.389255, acc.: 83.59%] [G loss: 4.833830]\n",
      "epoch:47 step:37487 [D loss: 1.085587, acc.: 72.66%] [G loss: 6.759098]\n",
      "epoch:47 step:37488 [D loss: 0.922868, acc.: 76.56%] [G loss: 3.669257]\n",
      "epoch:48 step:37489 [D loss: 0.510851, acc.: 78.91%] [G loss: 4.194241]\n",
      "epoch:48 step:37490 [D loss: 0.291707, acc.: 85.94%] [G loss: 5.776433]\n",
      "epoch:48 step:37491 [D loss: 0.464712, acc.: 84.38%] [G loss: 3.957711]\n",
      "epoch:48 step:37492 [D loss: 0.323164, acc.: 87.50%] [G loss: 2.634543]\n",
      "epoch:48 step:37493 [D loss: 0.269324, acc.: 92.19%] [G loss: 3.418531]\n",
      "epoch:48 step:37494 [D loss: 0.230643, acc.: 89.06%] [G loss: 2.691863]\n",
      "epoch:48 step:37495 [D loss: 0.281439, acc.: 84.38%] [G loss: 3.415087]\n",
      "epoch:48 step:37496 [D loss: 0.358758, acc.: 82.81%] [G loss: 2.766876]\n",
      "epoch:48 step:37497 [D loss: 0.384237, acc.: 82.81%] [G loss: 3.070002]\n",
      "epoch:48 step:37498 [D loss: 0.393686, acc.: 81.25%] [G loss: 2.550908]\n",
      "epoch:48 step:37499 [D loss: 0.356964, acc.: 85.94%] [G loss: 3.662887]\n",
      "epoch:48 step:37500 [D loss: 0.253967, acc.: 89.84%] [G loss: 3.101164]\n",
      "epoch:48 step:37501 [D loss: 0.303674, acc.: 87.50%] [G loss: 3.699340]\n",
      "epoch:48 step:37502 [D loss: 0.289731, acc.: 87.50%] [G loss: 3.192077]\n",
      "epoch:48 step:37503 [D loss: 0.335725, acc.: 84.38%] [G loss: 3.104415]\n",
      "epoch:48 step:37504 [D loss: 0.328869, acc.: 85.16%] [G loss: 2.723449]\n",
      "epoch:48 step:37505 [D loss: 0.259645, acc.: 85.94%] [G loss: 3.101830]\n",
      "epoch:48 step:37506 [D loss: 0.342773, acc.: 84.38%] [G loss: 3.087992]\n",
      "epoch:48 step:37507 [D loss: 0.444026, acc.: 76.56%] [G loss: 2.571075]\n",
      "epoch:48 step:37508 [D loss: 0.262059, acc.: 89.84%] [G loss: 2.892837]\n",
      "epoch:48 step:37509 [D loss: 0.333821, acc.: 85.16%] [G loss: 2.704109]\n",
      "epoch:48 step:37510 [D loss: 0.358443, acc.: 84.38%] [G loss: 2.833133]\n",
      "epoch:48 step:37511 [D loss: 0.225080, acc.: 93.75%] [G loss: 2.970628]\n",
      "epoch:48 step:37512 [D loss: 0.287606, acc.: 82.81%] [G loss: 2.749567]\n",
      "epoch:48 step:37513 [D loss: 0.303928, acc.: 85.94%] [G loss: 2.704290]\n",
      "epoch:48 step:37514 [D loss: 0.285352, acc.: 88.28%] [G loss: 2.634643]\n",
      "epoch:48 step:37515 [D loss: 0.250199, acc.: 88.28%] [G loss: 2.580078]\n",
      "epoch:48 step:37516 [D loss: 0.301242, acc.: 89.06%] [G loss: 2.317488]\n",
      "epoch:48 step:37517 [D loss: 0.341155, acc.: 85.94%] [G loss: 3.191468]\n",
      "epoch:48 step:37518 [D loss: 0.262955, acc.: 89.06%] [G loss: 3.263965]\n",
      "epoch:48 step:37519 [D loss: 0.338149, acc.: 87.50%] [G loss: 3.575337]\n",
      "epoch:48 step:37520 [D loss: 0.356267, acc.: 86.72%] [G loss: 2.716779]\n",
      "epoch:48 step:37521 [D loss: 0.240970, acc.: 92.19%] [G loss: 3.327372]\n",
      "epoch:48 step:37522 [D loss: 0.273802, acc.: 87.50%] [G loss: 2.477630]\n",
      "epoch:48 step:37523 [D loss: 0.272472, acc.: 89.84%] [G loss: 4.049215]\n",
      "epoch:48 step:37524 [D loss: 0.264629, acc.: 89.84%] [G loss: 3.387712]\n",
      "epoch:48 step:37525 [D loss: 0.374869, acc.: 80.47%] [G loss: 3.855427]\n",
      "epoch:48 step:37526 [D loss: 0.352865, acc.: 87.50%] [G loss: 2.936586]\n",
      "epoch:48 step:37527 [D loss: 0.260778, acc.: 91.41%] [G loss: 2.990652]\n",
      "epoch:48 step:37528 [D loss: 0.269942, acc.: 89.06%] [G loss: 2.964330]\n",
      "epoch:48 step:37529 [D loss: 0.218533, acc.: 90.62%] [G loss: 3.246675]\n",
      "epoch:48 step:37530 [D loss: 0.350200, acc.: 84.38%] [G loss: 2.619892]\n",
      "epoch:48 step:37531 [D loss: 0.375082, acc.: 82.81%] [G loss: 2.714709]\n",
      "epoch:48 step:37532 [D loss: 0.344308, acc.: 84.38%] [G loss: 2.249609]\n",
      "epoch:48 step:37533 [D loss: 0.373267, acc.: 82.81%] [G loss: 2.125426]\n",
      "epoch:48 step:37534 [D loss: 0.297941, acc.: 85.94%] [G loss: 2.506829]\n",
      "epoch:48 step:37535 [D loss: 0.403610, acc.: 80.47%] [G loss: 2.349973]\n",
      "epoch:48 step:37536 [D loss: 0.233943, acc.: 91.41%] [G loss: 2.747672]\n",
      "epoch:48 step:37537 [D loss: 0.227931, acc.: 91.41%] [G loss: 2.595817]\n",
      "epoch:48 step:37538 [D loss: 0.332901, acc.: 85.16%] [G loss: 2.728065]\n",
      "epoch:48 step:37539 [D loss: 0.211292, acc.: 91.41%] [G loss: 2.584352]\n",
      "epoch:48 step:37540 [D loss: 0.226125, acc.: 94.53%] [G loss: 2.487736]\n",
      "epoch:48 step:37541 [D loss: 0.260234, acc.: 88.28%] [G loss: 2.704412]\n",
      "epoch:48 step:37542 [D loss: 0.283429, acc.: 86.72%] [G loss: 3.115632]\n",
      "epoch:48 step:37543 [D loss: 0.287926, acc.: 84.38%] [G loss: 3.037866]\n",
      "epoch:48 step:37544 [D loss: 0.334693, acc.: 83.59%] [G loss: 2.361107]\n",
      "epoch:48 step:37545 [D loss: 0.235560, acc.: 89.06%] [G loss: 3.108566]\n",
      "epoch:48 step:37546 [D loss: 0.259811, acc.: 86.72%] [G loss: 3.170377]\n",
      "epoch:48 step:37547 [D loss: 0.217433, acc.: 92.97%] [G loss: 2.842613]\n",
      "epoch:48 step:37548 [D loss: 0.271946, acc.: 89.06%] [G loss: 2.727916]\n",
      "epoch:48 step:37549 [D loss: 0.347354, acc.: 89.84%] [G loss: 2.640595]\n",
      "epoch:48 step:37550 [D loss: 0.272907, acc.: 86.72%] [G loss: 3.562590]\n",
      "epoch:48 step:37551 [D loss: 0.329855, acc.: 85.16%] [G loss: 3.115143]\n",
      "epoch:48 step:37552 [D loss: 0.325212, acc.: 83.59%] [G loss: 3.106990]\n",
      "epoch:48 step:37553 [D loss: 0.286894, acc.: 89.06%] [G loss: 2.991285]\n",
      "epoch:48 step:37554 [D loss: 0.325422, acc.: 85.94%] [G loss: 2.988226]\n",
      "epoch:48 step:37555 [D loss: 0.321526, acc.: 84.38%] [G loss: 3.415188]\n",
      "epoch:48 step:37556 [D loss: 0.358719, acc.: 85.16%] [G loss: 2.739625]\n",
      "epoch:48 step:37557 [D loss: 0.339494, acc.: 84.38%] [G loss: 3.466441]\n",
      "epoch:48 step:37558 [D loss: 0.350572, acc.: 82.81%] [G loss: 3.201486]\n",
      "epoch:48 step:37559 [D loss: 0.334919, acc.: 85.16%] [G loss: 3.005347]\n",
      "epoch:48 step:37560 [D loss: 0.417106, acc.: 77.34%] [G loss: 3.158124]\n",
      "epoch:48 step:37561 [D loss: 0.296988, acc.: 85.94%] [G loss: 3.021725]\n",
      "epoch:48 step:37562 [D loss: 0.235340, acc.: 90.62%] [G loss: 2.469079]\n",
      "epoch:48 step:37563 [D loss: 0.367416, acc.: 82.81%] [G loss: 2.311955]\n",
      "epoch:48 step:37564 [D loss: 0.258297, acc.: 89.84%] [G loss: 2.685133]\n",
      "epoch:48 step:37565 [D loss: 0.507862, acc.: 77.34%] [G loss: 3.854597]\n",
      "epoch:48 step:37566 [D loss: 0.347836, acc.: 83.59%] [G loss: 3.039210]\n",
      "epoch:48 step:37567 [D loss: 0.228453, acc.: 92.19%] [G loss: 3.096696]\n",
      "epoch:48 step:37568 [D loss: 0.344865, acc.: 82.81%] [G loss: 3.045442]\n",
      "epoch:48 step:37569 [D loss: 0.352214, acc.: 84.38%] [G loss: 2.539599]\n",
      "epoch:48 step:37570 [D loss: 0.221601, acc.: 91.41%] [G loss: 2.683978]\n",
      "epoch:48 step:37571 [D loss: 0.201316, acc.: 94.53%] [G loss: 2.941270]\n",
      "epoch:48 step:37572 [D loss: 0.354193, acc.: 80.47%] [G loss: 3.309956]\n",
      "epoch:48 step:37573 [D loss: 0.368461, acc.: 83.59%] [G loss: 3.216849]\n",
      "epoch:48 step:37574 [D loss: 0.316347, acc.: 83.59%] [G loss: 2.818045]\n",
      "epoch:48 step:37575 [D loss: 0.295516, acc.: 84.38%] [G loss: 3.254189]\n",
      "epoch:48 step:37576 [D loss: 0.348861, acc.: 82.81%] [G loss: 3.144594]\n",
      "epoch:48 step:37577 [D loss: 0.284501, acc.: 86.72%] [G loss: 3.538041]\n",
      "epoch:48 step:37578 [D loss: 0.311004, acc.: 83.59%] [G loss: 3.559463]\n",
      "epoch:48 step:37579 [D loss: 0.155937, acc.: 95.31%] [G loss: 4.036303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37580 [D loss: 0.282069, acc.: 89.06%] [G loss: 3.376636]\n",
      "epoch:48 step:37581 [D loss: 0.234828, acc.: 90.62%] [G loss: 3.667087]\n",
      "epoch:48 step:37582 [D loss: 0.221211, acc.: 90.62%] [G loss: 3.749842]\n",
      "epoch:48 step:37583 [D loss: 0.298641, acc.: 89.84%] [G loss: 4.176250]\n",
      "epoch:48 step:37584 [D loss: 0.237975, acc.: 89.06%] [G loss: 3.123041]\n",
      "epoch:48 step:37585 [D loss: 0.243969, acc.: 88.28%] [G loss: 4.762034]\n",
      "epoch:48 step:37586 [D loss: 0.310994, acc.: 82.03%] [G loss: 3.248686]\n",
      "epoch:48 step:37587 [D loss: 0.299779, acc.: 87.50%] [G loss: 3.463307]\n",
      "epoch:48 step:37588 [D loss: 0.293973, acc.: 86.72%] [G loss: 4.344042]\n",
      "epoch:48 step:37589 [D loss: 0.356707, acc.: 82.81%] [G loss: 5.032214]\n",
      "epoch:48 step:37590 [D loss: 0.377282, acc.: 83.59%] [G loss: 4.212925]\n",
      "epoch:48 step:37591 [D loss: 0.391774, acc.: 82.81%] [G loss: 4.353279]\n",
      "epoch:48 step:37592 [D loss: 0.471552, acc.: 75.00%] [G loss: 3.259934]\n",
      "epoch:48 step:37593 [D loss: 0.375196, acc.: 85.94%] [G loss: 2.716492]\n",
      "epoch:48 step:37594 [D loss: 0.467864, acc.: 77.34%] [G loss: 3.168291]\n",
      "epoch:48 step:37595 [D loss: 0.401326, acc.: 79.69%] [G loss: 6.525146]\n",
      "epoch:48 step:37596 [D loss: 0.217420, acc.: 91.41%] [G loss: 5.003020]\n",
      "epoch:48 step:37597 [D loss: 0.466196, acc.: 78.12%] [G loss: 3.276744]\n",
      "epoch:48 step:37598 [D loss: 0.204515, acc.: 92.19%] [G loss: 3.513757]\n",
      "epoch:48 step:37599 [D loss: 0.331112, acc.: 85.94%] [G loss: 3.107118]\n",
      "epoch:48 step:37600 [D loss: 0.298902, acc.: 85.94%] [G loss: 2.521150]\n",
      "epoch:48 step:37601 [D loss: 0.353166, acc.: 85.16%] [G loss: 3.511366]\n",
      "epoch:48 step:37602 [D loss: 0.316714, acc.: 89.06%] [G loss: 4.115136]\n",
      "epoch:48 step:37603 [D loss: 0.292346, acc.: 89.06%] [G loss: 4.213009]\n",
      "epoch:48 step:37604 [D loss: 0.329703, acc.: 86.72%] [G loss: 3.468458]\n",
      "epoch:48 step:37605 [D loss: 0.351912, acc.: 88.28%] [G loss: 3.725780]\n",
      "epoch:48 step:37606 [D loss: 0.344399, acc.: 85.16%] [G loss: 3.045049]\n",
      "epoch:48 step:37607 [D loss: 0.299291, acc.: 86.72%] [G loss: 5.298080]\n",
      "epoch:48 step:37608 [D loss: 0.252583, acc.: 88.28%] [G loss: 3.955997]\n",
      "epoch:48 step:37609 [D loss: 0.263361, acc.: 91.41%] [G loss: 5.943390]\n",
      "epoch:48 step:37610 [D loss: 0.334212, acc.: 85.16%] [G loss: 5.261264]\n",
      "epoch:48 step:37611 [D loss: 0.362966, acc.: 86.72%] [G loss: 3.426103]\n",
      "epoch:48 step:37612 [D loss: 0.347749, acc.: 85.94%] [G loss: 3.303707]\n",
      "epoch:48 step:37613 [D loss: 0.340544, acc.: 82.03%] [G loss: 3.306351]\n",
      "epoch:48 step:37614 [D loss: 0.296254, acc.: 88.28%] [G loss: 2.962663]\n",
      "epoch:48 step:37615 [D loss: 0.314445, acc.: 86.72%] [G loss: 3.166765]\n",
      "epoch:48 step:37616 [D loss: 0.181069, acc.: 91.41%] [G loss: 3.236945]\n",
      "epoch:48 step:37617 [D loss: 0.321293, acc.: 88.28%] [G loss: 3.393929]\n",
      "epoch:48 step:37618 [D loss: 0.377823, acc.: 82.03%] [G loss: 3.166369]\n",
      "epoch:48 step:37619 [D loss: 0.256223, acc.: 89.84%] [G loss: 2.365454]\n",
      "epoch:48 step:37620 [D loss: 0.407421, acc.: 81.25%] [G loss: 2.850781]\n",
      "epoch:48 step:37621 [D loss: 0.242009, acc.: 87.50%] [G loss: 3.568789]\n",
      "epoch:48 step:37622 [D loss: 0.269216, acc.: 85.94%] [G loss: 3.756989]\n",
      "epoch:48 step:37623 [D loss: 0.367847, acc.: 85.16%] [G loss: 4.234409]\n",
      "epoch:48 step:37624 [D loss: 0.257217, acc.: 88.28%] [G loss: 4.872864]\n",
      "epoch:48 step:37625 [D loss: 0.297498, acc.: 87.50%] [G loss: 5.275904]\n",
      "epoch:48 step:37626 [D loss: 0.291359, acc.: 85.16%] [G loss: 3.625573]\n",
      "epoch:48 step:37627 [D loss: 0.437251, acc.: 82.03%] [G loss: 5.604548]\n",
      "epoch:48 step:37628 [D loss: 0.329536, acc.: 83.59%] [G loss: 4.452000]\n",
      "epoch:48 step:37629 [D loss: 0.374145, acc.: 82.81%] [G loss: 3.705550]\n",
      "epoch:48 step:37630 [D loss: 0.333220, acc.: 86.72%] [G loss: 3.653163]\n",
      "epoch:48 step:37631 [D loss: 0.231154, acc.: 91.41%] [G loss: 2.623374]\n",
      "epoch:48 step:37632 [D loss: 0.323214, acc.: 82.03%] [G loss: 3.007601]\n",
      "epoch:48 step:37633 [D loss: 0.331978, acc.: 85.16%] [G loss: 3.229501]\n",
      "epoch:48 step:37634 [D loss: 0.343549, acc.: 83.59%] [G loss: 3.127251]\n",
      "epoch:48 step:37635 [D loss: 0.266560, acc.: 85.94%] [G loss: 3.384748]\n",
      "epoch:48 step:37636 [D loss: 0.256746, acc.: 91.41%] [G loss: 3.568965]\n",
      "epoch:48 step:37637 [D loss: 0.346548, acc.: 83.59%] [G loss: 3.384361]\n",
      "epoch:48 step:37638 [D loss: 0.275128, acc.: 89.84%] [G loss: 3.110034]\n",
      "epoch:48 step:37639 [D loss: 0.271742, acc.: 89.84%] [G loss: 2.761526]\n",
      "epoch:48 step:37640 [D loss: 0.393550, acc.: 82.03%] [G loss: 3.302332]\n",
      "epoch:48 step:37641 [D loss: 0.321068, acc.: 84.38%] [G loss: 2.565796]\n",
      "epoch:48 step:37642 [D loss: 0.290649, acc.: 86.72%] [G loss: 3.345176]\n",
      "epoch:48 step:37643 [D loss: 0.247726, acc.: 87.50%] [G loss: 3.346928]\n",
      "epoch:48 step:37644 [D loss: 0.273379, acc.: 89.84%] [G loss: 4.151400]\n",
      "epoch:48 step:37645 [D loss: 0.345839, acc.: 85.16%] [G loss: 3.209393]\n",
      "epoch:48 step:37646 [D loss: 0.278876, acc.: 87.50%] [G loss: 4.467219]\n",
      "epoch:48 step:37647 [D loss: 0.268308, acc.: 89.84%] [G loss: 3.745297]\n",
      "epoch:48 step:37648 [D loss: 0.308098, acc.: 87.50%] [G loss: 3.505700]\n",
      "epoch:48 step:37649 [D loss: 0.292001, acc.: 88.28%] [G loss: 3.861462]\n",
      "epoch:48 step:37650 [D loss: 0.257686, acc.: 89.84%] [G loss: 4.879987]\n",
      "epoch:48 step:37651 [D loss: 0.301121, acc.: 88.28%] [G loss: 3.736791]\n",
      "epoch:48 step:37652 [D loss: 0.281019, acc.: 87.50%] [G loss: 4.939042]\n",
      "epoch:48 step:37653 [D loss: 0.378826, acc.: 84.38%] [G loss: 3.101021]\n",
      "epoch:48 step:37654 [D loss: 0.363101, acc.: 81.25%] [G loss: 3.236818]\n",
      "epoch:48 step:37655 [D loss: 0.292956, acc.: 84.38%] [G loss: 3.326987]\n",
      "epoch:48 step:37656 [D loss: 0.215477, acc.: 92.97%] [G loss: 3.069808]\n",
      "epoch:48 step:37657 [D loss: 0.316381, acc.: 88.28%] [G loss: 2.795536]\n",
      "epoch:48 step:37658 [D loss: 0.245391, acc.: 89.84%] [G loss: 3.243064]\n",
      "epoch:48 step:37659 [D loss: 0.309837, acc.: 86.72%] [G loss: 2.461448]\n",
      "epoch:48 step:37660 [D loss: 0.266359, acc.: 85.94%] [G loss: 3.540497]\n",
      "epoch:48 step:37661 [D loss: 0.256237, acc.: 89.84%] [G loss: 3.531860]\n",
      "epoch:48 step:37662 [D loss: 0.317596, acc.: 85.94%] [G loss: 3.620642]\n",
      "epoch:48 step:37663 [D loss: 0.288203, acc.: 88.28%] [G loss: 3.036028]\n",
      "epoch:48 step:37664 [D loss: 0.340284, acc.: 85.16%] [G loss: 3.185749]\n",
      "epoch:48 step:37665 [D loss: 0.295012, acc.: 88.28%] [G loss: 3.323160]\n",
      "epoch:48 step:37666 [D loss: 0.284442, acc.: 85.16%] [G loss: 2.792466]\n",
      "epoch:48 step:37667 [D loss: 0.258707, acc.: 90.62%] [G loss: 3.024096]\n",
      "epoch:48 step:37668 [D loss: 0.287322, acc.: 87.50%] [G loss: 2.953586]\n",
      "epoch:48 step:37669 [D loss: 0.297689, acc.: 89.06%] [G loss: 2.455938]\n",
      "epoch:48 step:37670 [D loss: 0.251698, acc.: 89.84%] [G loss: 2.746447]\n",
      "epoch:48 step:37671 [D loss: 0.234126, acc.: 91.41%] [G loss: 2.760570]\n",
      "epoch:48 step:37672 [D loss: 0.311768, acc.: 82.03%] [G loss: 2.790511]\n",
      "epoch:48 step:37673 [D loss: 0.321692, acc.: 86.72%] [G loss: 3.092533]\n",
      "epoch:48 step:37674 [D loss: 0.247019, acc.: 88.28%] [G loss: 3.655378]\n",
      "epoch:48 step:37675 [D loss: 0.308962, acc.: 87.50%] [G loss: 3.305388]\n",
      "epoch:48 step:37676 [D loss: 0.343826, acc.: 84.38%] [G loss: 2.723097]\n",
      "epoch:48 step:37677 [D loss: 0.302066, acc.: 87.50%] [G loss: 3.607599]\n",
      "epoch:48 step:37678 [D loss: 0.318329, acc.: 83.59%] [G loss: 3.802785]\n",
      "epoch:48 step:37679 [D loss: 0.288747, acc.: 85.16%] [G loss: 3.986483]\n",
      "epoch:48 step:37680 [D loss: 0.236393, acc.: 89.06%] [G loss: 4.122500]\n",
      "epoch:48 step:37681 [D loss: 0.314917, acc.: 87.50%] [G loss: 3.618512]\n",
      "epoch:48 step:37682 [D loss: 0.351349, acc.: 82.81%] [G loss: 3.516823]\n",
      "epoch:48 step:37683 [D loss: 0.389395, acc.: 80.47%] [G loss: 2.934677]\n",
      "epoch:48 step:37684 [D loss: 0.297308, acc.: 89.06%] [G loss: 2.922023]\n",
      "epoch:48 step:37685 [D loss: 0.459225, acc.: 82.03%] [G loss: 5.370113]\n",
      "epoch:48 step:37686 [D loss: 0.337818, acc.: 83.59%] [G loss: 3.044628]\n",
      "epoch:48 step:37687 [D loss: 0.259321, acc.: 89.84%] [G loss: 3.729170]\n",
      "epoch:48 step:37688 [D loss: 0.250481, acc.: 89.84%] [G loss: 3.063788]\n",
      "epoch:48 step:37689 [D loss: 0.232414, acc.: 89.06%] [G loss: 3.370460]\n",
      "epoch:48 step:37690 [D loss: 0.355531, acc.: 84.38%] [G loss: 2.910534]\n",
      "epoch:48 step:37691 [D loss: 0.292225, acc.: 86.72%] [G loss: 2.578793]\n",
      "epoch:48 step:37692 [D loss: 0.330680, acc.: 85.94%] [G loss: 2.861823]\n",
      "epoch:48 step:37693 [D loss: 0.233355, acc.: 91.41%] [G loss: 2.581236]\n",
      "epoch:48 step:37694 [D loss: 0.335265, acc.: 85.94%] [G loss: 3.359541]\n",
      "epoch:48 step:37695 [D loss: 0.294182, acc.: 88.28%] [G loss: 3.238109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37696 [D loss: 0.359139, acc.: 85.16%] [G loss: 3.649821]\n",
      "epoch:48 step:37697 [D loss: 0.431458, acc.: 79.69%] [G loss: 5.796586]\n",
      "epoch:48 step:37698 [D loss: 0.299709, acc.: 85.94%] [G loss: 6.263557]\n",
      "epoch:48 step:37699 [D loss: 0.215512, acc.: 89.84%] [G loss: 8.373455]\n",
      "epoch:48 step:37700 [D loss: 0.298026, acc.: 87.50%] [G loss: 9.923904]\n",
      "epoch:48 step:37701 [D loss: 0.185238, acc.: 89.84%] [G loss: 5.700304]\n",
      "epoch:48 step:37702 [D loss: 0.174406, acc.: 92.19%] [G loss: 6.283454]\n",
      "epoch:48 step:37703 [D loss: 0.222294, acc.: 88.28%] [G loss: 6.622076]\n",
      "epoch:48 step:37704 [D loss: 0.300574, acc.: 87.50%] [G loss: 6.048513]\n",
      "epoch:48 step:37705 [D loss: 0.235721, acc.: 92.97%] [G loss: 4.065334]\n",
      "epoch:48 step:37706 [D loss: 0.229894, acc.: 86.72%] [G loss: 5.150424]\n",
      "epoch:48 step:37707 [D loss: 0.384272, acc.: 82.81%] [G loss: 4.594232]\n",
      "epoch:48 step:37708 [D loss: 0.259168, acc.: 91.41%] [G loss: 3.739014]\n",
      "epoch:48 step:37709 [D loss: 0.204471, acc.: 91.41%] [G loss: 3.929879]\n",
      "epoch:48 step:37710 [D loss: 0.275909, acc.: 87.50%] [G loss: 3.369716]\n",
      "epoch:48 step:37711 [D loss: 0.299813, acc.: 85.16%] [G loss: 3.093612]\n",
      "epoch:48 step:37712 [D loss: 0.354985, acc.: 83.59%] [G loss: 3.337108]\n",
      "epoch:48 step:37713 [D loss: 0.273721, acc.: 86.72%] [G loss: 3.042639]\n",
      "epoch:48 step:37714 [D loss: 0.317905, acc.: 85.16%] [G loss: 3.129392]\n",
      "epoch:48 step:37715 [D loss: 0.253123, acc.: 89.84%] [G loss: 2.676452]\n",
      "epoch:48 step:37716 [D loss: 0.501874, acc.: 74.22%] [G loss: 2.756696]\n",
      "epoch:48 step:37717 [D loss: 0.291487, acc.: 86.72%] [G loss: 3.786831]\n",
      "epoch:48 step:37718 [D loss: 0.304917, acc.: 85.16%] [G loss: 5.476625]\n",
      "epoch:48 step:37719 [D loss: 0.171292, acc.: 92.97%] [G loss: 3.973490]\n",
      "epoch:48 step:37720 [D loss: 0.307534, acc.: 87.50%] [G loss: 3.739250]\n",
      "epoch:48 step:37721 [D loss: 0.320693, acc.: 85.16%] [G loss: 3.473270]\n",
      "epoch:48 step:37722 [D loss: 0.287959, acc.: 84.38%] [G loss: 3.811711]\n",
      "epoch:48 step:37723 [D loss: 0.270184, acc.: 87.50%] [G loss: 3.073274]\n",
      "epoch:48 step:37724 [D loss: 0.326649, acc.: 86.72%] [G loss: 2.616297]\n",
      "epoch:48 step:37725 [D loss: 0.207913, acc.: 92.19%] [G loss: 2.395281]\n",
      "epoch:48 step:37726 [D loss: 0.296209, acc.: 85.16%] [G loss: 2.744306]\n",
      "epoch:48 step:37727 [D loss: 0.305852, acc.: 88.28%] [G loss: 3.246269]\n",
      "epoch:48 step:37728 [D loss: 0.349563, acc.: 84.38%] [G loss: 3.099176]\n",
      "epoch:48 step:37729 [D loss: 0.285744, acc.: 88.28%] [G loss: 2.806252]\n",
      "epoch:48 step:37730 [D loss: 0.377465, acc.: 82.81%] [G loss: 2.897428]\n",
      "epoch:48 step:37731 [D loss: 0.326225, acc.: 84.38%] [G loss: 3.892084]\n",
      "epoch:48 step:37732 [D loss: 0.242810, acc.: 90.62%] [G loss: 4.492247]\n",
      "epoch:48 step:37733 [D loss: 0.320424, acc.: 87.50%] [G loss: 4.031535]\n",
      "epoch:48 step:37734 [D loss: 0.262038, acc.: 89.06%] [G loss: 4.297383]\n",
      "epoch:48 step:37735 [D loss: 0.284751, acc.: 85.94%] [G loss: 3.648025]\n",
      "epoch:48 step:37736 [D loss: 0.334066, acc.: 85.16%] [G loss: 3.622890]\n",
      "epoch:48 step:37737 [D loss: 0.295586, acc.: 85.94%] [G loss: 3.676599]\n",
      "epoch:48 step:37738 [D loss: 0.363386, acc.: 82.81%] [G loss: 3.503145]\n",
      "epoch:48 step:37739 [D loss: 0.243130, acc.: 92.97%] [G loss: 3.466413]\n",
      "epoch:48 step:37740 [D loss: 0.247989, acc.: 90.62%] [G loss: 3.069854]\n",
      "epoch:48 step:37741 [D loss: 0.259439, acc.: 92.97%] [G loss: 3.733397]\n",
      "epoch:48 step:37742 [D loss: 0.267131, acc.: 89.84%] [G loss: 3.382840]\n",
      "epoch:48 step:37743 [D loss: 0.259425, acc.: 89.84%] [G loss: 3.382548]\n",
      "epoch:48 step:37744 [D loss: 0.304943, acc.: 89.06%] [G loss: 3.212163]\n",
      "epoch:48 step:37745 [D loss: 0.389069, acc.: 81.25%] [G loss: 3.351788]\n",
      "epoch:48 step:37746 [D loss: 0.278470, acc.: 89.06%] [G loss: 2.890251]\n",
      "epoch:48 step:37747 [D loss: 0.311521, acc.: 86.72%] [G loss: 6.573586]\n",
      "epoch:48 step:37748 [D loss: 0.531857, acc.: 75.78%] [G loss: 2.967969]\n",
      "epoch:48 step:37749 [D loss: 0.417414, acc.: 78.91%] [G loss: 3.992348]\n",
      "epoch:48 step:37750 [D loss: 0.289010, acc.: 89.06%] [G loss: 3.367331]\n",
      "epoch:48 step:37751 [D loss: 0.227136, acc.: 89.84%] [G loss: 3.673267]\n",
      "epoch:48 step:37752 [D loss: 0.231403, acc.: 89.84%] [G loss: 4.325768]\n",
      "epoch:48 step:37753 [D loss: 0.350858, acc.: 83.59%] [G loss: 3.403704]\n",
      "epoch:48 step:37754 [D loss: 0.356001, acc.: 84.38%] [G loss: 4.281837]\n",
      "epoch:48 step:37755 [D loss: 0.287814, acc.: 88.28%] [G loss: 2.981024]\n",
      "epoch:48 step:37756 [D loss: 0.230554, acc.: 89.06%] [G loss: 3.223074]\n",
      "epoch:48 step:37757 [D loss: 0.322474, acc.: 85.94%] [G loss: 2.908359]\n",
      "epoch:48 step:37758 [D loss: 0.298412, acc.: 85.16%] [G loss: 3.718529]\n",
      "epoch:48 step:37759 [D loss: 0.240191, acc.: 89.84%] [G loss: 3.788243]\n",
      "epoch:48 step:37760 [D loss: 0.249230, acc.: 90.62%] [G loss: 4.153498]\n",
      "epoch:48 step:37761 [D loss: 0.233139, acc.: 89.84%] [G loss: 4.137147]\n",
      "epoch:48 step:37762 [D loss: 0.322937, acc.: 83.59%] [G loss: 4.189560]\n",
      "epoch:48 step:37763 [D loss: 0.363537, acc.: 82.81%] [G loss: 3.788869]\n",
      "epoch:48 step:37764 [D loss: 0.279636, acc.: 88.28%] [G loss: 4.213862]\n",
      "epoch:48 step:37765 [D loss: 0.233924, acc.: 92.19%] [G loss: 3.538751]\n",
      "epoch:48 step:37766 [D loss: 0.309248, acc.: 87.50%] [G loss: 5.218380]\n",
      "epoch:48 step:37767 [D loss: 0.270419, acc.: 92.19%] [G loss: 4.215889]\n",
      "epoch:48 step:37768 [D loss: 0.201742, acc.: 95.31%] [G loss: 3.934273]\n",
      "epoch:48 step:37769 [D loss: 0.321420, acc.: 84.38%] [G loss: 4.265782]\n",
      "epoch:48 step:37770 [D loss: 0.370266, acc.: 82.81%] [G loss: 2.728183]\n",
      "epoch:48 step:37771 [D loss: 0.296053, acc.: 86.72%] [G loss: 3.100781]\n",
      "epoch:48 step:37772 [D loss: 0.315893, acc.: 87.50%] [G loss: 3.125817]\n",
      "epoch:48 step:37773 [D loss: 0.283488, acc.: 84.38%] [G loss: 3.657335]\n",
      "epoch:48 step:37774 [D loss: 0.298466, acc.: 87.50%] [G loss: 2.614783]\n",
      "epoch:48 step:37775 [D loss: 0.312472, acc.: 87.50%] [G loss: 2.961206]\n",
      "epoch:48 step:37776 [D loss: 0.320370, acc.: 82.81%] [G loss: 4.328214]\n",
      "epoch:48 step:37777 [D loss: 0.396432, acc.: 83.59%] [G loss: 3.808465]\n",
      "epoch:48 step:37778 [D loss: 0.275510, acc.: 92.19%] [G loss: 3.788391]\n",
      "epoch:48 step:37779 [D loss: 0.228654, acc.: 89.84%] [G loss: 3.009735]\n",
      "epoch:48 step:37780 [D loss: 0.223121, acc.: 91.41%] [G loss: 5.061407]\n",
      "epoch:48 step:37781 [D loss: 0.274524, acc.: 89.06%] [G loss: 5.409527]\n",
      "epoch:48 step:37782 [D loss: 0.251707, acc.: 89.06%] [G loss: 3.494351]\n",
      "epoch:48 step:37783 [D loss: 0.277513, acc.: 87.50%] [G loss: 4.808801]\n",
      "epoch:48 step:37784 [D loss: 0.393973, acc.: 80.47%] [G loss: 4.128185]\n",
      "epoch:48 step:37785 [D loss: 0.292782, acc.: 85.94%] [G loss: 3.999385]\n",
      "epoch:48 step:37786 [D loss: 0.347877, acc.: 82.03%] [G loss: 3.820164]\n",
      "epoch:48 step:37787 [D loss: 0.317003, acc.: 84.38%] [G loss: 3.486751]\n",
      "epoch:48 step:37788 [D loss: 0.267358, acc.: 86.72%] [G loss: 3.228057]\n",
      "epoch:48 step:37789 [D loss: 0.339168, acc.: 85.94%] [G loss: 2.907840]\n",
      "epoch:48 step:37790 [D loss: 0.317647, acc.: 91.41%] [G loss: 2.933493]\n",
      "epoch:48 step:37791 [D loss: 0.277889, acc.: 90.62%] [G loss: 3.115353]\n",
      "epoch:48 step:37792 [D loss: 0.271764, acc.: 89.84%] [G loss: 3.146655]\n",
      "epoch:48 step:37793 [D loss: 0.390528, acc.: 82.03%] [G loss: 3.228399]\n",
      "epoch:48 step:37794 [D loss: 0.352058, acc.: 85.94%] [G loss: 2.625282]\n",
      "epoch:48 step:37795 [D loss: 0.325949, acc.: 89.06%] [G loss: 3.103011]\n",
      "epoch:48 step:37796 [D loss: 0.333353, acc.: 86.72%] [G loss: 3.053199]\n",
      "epoch:48 step:37797 [D loss: 0.244568, acc.: 89.06%] [G loss: 3.254762]\n",
      "epoch:48 step:37798 [D loss: 0.300259, acc.: 85.16%] [G loss: 2.925806]\n",
      "epoch:48 step:37799 [D loss: 0.303342, acc.: 86.72%] [G loss: 2.587103]\n",
      "epoch:48 step:37800 [D loss: 0.340740, acc.: 84.38%] [G loss: 3.207052]\n",
      "epoch:48 step:37801 [D loss: 0.319530, acc.: 83.59%] [G loss: 3.255415]\n",
      "epoch:48 step:37802 [D loss: 0.402666, acc.: 81.25%] [G loss: 2.822102]\n",
      "epoch:48 step:37803 [D loss: 0.449155, acc.: 81.25%] [G loss: 3.611583]\n",
      "epoch:48 step:37804 [D loss: 0.254182, acc.: 87.50%] [G loss: 3.979386]\n",
      "epoch:48 step:37805 [D loss: 0.290996, acc.: 83.59%] [G loss: 5.214972]\n",
      "epoch:48 step:37806 [D loss: 0.352902, acc.: 80.47%] [G loss: 3.661470]\n",
      "epoch:48 step:37807 [D loss: 0.184892, acc.: 95.31%] [G loss: 4.766901]\n",
      "epoch:48 step:37808 [D loss: 0.243144, acc.: 89.06%] [G loss: 2.855428]\n",
      "epoch:48 step:37809 [D loss: 0.261044, acc.: 86.72%] [G loss: 4.692783]\n",
      "epoch:48 step:37810 [D loss: 0.261160, acc.: 89.84%] [G loss: 4.256238]\n",
      "epoch:48 step:37811 [D loss: 0.347104, acc.: 82.03%] [G loss: 5.090873]\n",
      "epoch:48 step:37812 [D loss: 0.265203, acc.: 88.28%] [G loss: 4.186147]\n",
      "epoch:48 step:37813 [D loss: 0.237928, acc.: 85.94%] [G loss: 5.065712]\n",
      "epoch:48 step:37814 [D loss: 0.361449, acc.: 84.38%] [G loss: 3.993742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37815 [D loss: 0.256185, acc.: 89.06%] [G loss: 3.767621]\n",
      "epoch:48 step:37816 [D loss: 0.188193, acc.: 94.53%] [G loss: 4.168066]\n",
      "epoch:48 step:37817 [D loss: 0.214073, acc.: 91.41%] [G loss: 3.564270]\n",
      "epoch:48 step:37818 [D loss: 0.201584, acc.: 91.41%] [G loss: 3.830967]\n",
      "epoch:48 step:37819 [D loss: 0.299015, acc.: 89.06%] [G loss: 3.322021]\n",
      "epoch:48 step:37820 [D loss: 0.166531, acc.: 93.75%] [G loss: 3.383504]\n",
      "epoch:48 step:37821 [D loss: 0.205208, acc.: 93.75%] [G loss: 3.524638]\n",
      "epoch:48 step:37822 [D loss: 0.258178, acc.: 89.84%] [G loss: 4.016790]\n",
      "epoch:48 step:37823 [D loss: 0.264022, acc.: 92.19%] [G loss: 2.769895]\n",
      "epoch:48 step:37824 [D loss: 0.244354, acc.: 90.62%] [G loss: 3.536918]\n",
      "epoch:48 step:37825 [D loss: 0.231514, acc.: 89.06%] [G loss: 3.416923]\n",
      "epoch:48 step:37826 [D loss: 0.330743, acc.: 85.94%] [G loss: 2.868676]\n",
      "epoch:48 step:37827 [D loss: 0.218996, acc.: 89.84%] [G loss: 3.354992]\n",
      "epoch:48 step:37828 [D loss: 0.290909, acc.: 88.28%] [G loss: 3.094815]\n",
      "epoch:48 step:37829 [D loss: 0.181855, acc.: 95.31%] [G loss: 2.419698]\n",
      "epoch:48 step:37830 [D loss: 0.334650, acc.: 86.72%] [G loss: 2.887321]\n",
      "epoch:48 step:37831 [D loss: 0.373147, acc.: 84.38%] [G loss: 2.780857]\n",
      "epoch:48 step:37832 [D loss: 0.396830, acc.: 78.91%] [G loss: 3.102369]\n",
      "epoch:48 step:37833 [D loss: 0.279553, acc.: 87.50%] [G loss: 3.448624]\n",
      "epoch:48 step:37834 [D loss: 0.301668, acc.: 86.72%] [G loss: 3.388859]\n",
      "epoch:48 step:37835 [D loss: 0.277034, acc.: 88.28%] [G loss: 5.093454]\n",
      "epoch:48 step:37836 [D loss: 0.261348, acc.: 86.72%] [G loss: 3.170701]\n",
      "epoch:48 step:37837 [D loss: 0.357364, acc.: 85.94%] [G loss: 4.759213]\n",
      "epoch:48 step:37838 [D loss: 0.313284, acc.: 85.16%] [G loss: 3.969942]\n",
      "epoch:48 step:37839 [D loss: 0.242431, acc.: 89.84%] [G loss: 4.181800]\n",
      "epoch:48 step:37840 [D loss: 0.325101, acc.: 86.72%] [G loss: 4.926646]\n",
      "epoch:48 step:37841 [D loss: 0.287502, acc.: 85.16%] [G loss: 3.837949]\n",
      "epoch:48 step:37842 [D loss: 0.292606, acc.: 89.84%] [G loss: 4.201509]\n",
      "epoch:48 step:37843 [D loss: 0.262564, acc.: 85.94%] [G loss: 3.491714]\n",
      "epoch:48 step:37844 [D loss: 0.300371, acc.: 85.94%] [G loss: 3.097150]\n",
      "epoch:48 step:37845 [D loss: 0.396168, acc.: 82.81%] [G loss: 3.888076]\n",
      "epoch:48 step:37846 [D loss: 0.267242, acc.: 90.62%] [G loss: 3.713172]\n",
      "epoch:48 step:37847 [D loss: 0.308673, acc.: 89.84%] [G loss: 3.122994]\n",
      "epoch:48 step:37848 [D loss: 0.325301, acc.: 85.94%] [G loss: 3.691121]\n",
      "epoch:48 step:37849 [D loss: 0.290362, acc.: 89.84%] [G loss: 2.578042]\n",
      "epoch:48 step:37850 [D loss: 0.292212, acc.: 87.50%] [G loss: 2.835083]\n",
      "epoch:48 step:37851 [D loss: 0.396591, acc.: 80.47%] [G loss: 3.398716]\n",
      "epoch:48 step:37852 [D loss: 0.377130, acc.: 83.59%] [G loss: 3.195579]\n",
      "epoch:48 step:37853 [D loss: 0.245563, acc.: 90.62%] [G loss: 3.138896]\n",
      "epoch:48 step:37854 [D loss: 0.313505, acc.: 85.94%] [G loss: 3.129655]\n",
      "epoch:48 step:37855 [D loss: 0.318611, acc.: 85.94%] [G loss: 2.861380]\n",
      "epoch:48 step:37856 [D loss: 0.380741, acc.: 83.59%] [G loss: 4.421315]\n",
      "epoch:48 step:37857 [D loss: 0.426757, acc.: 81.25%] [G loss: 8.831444]\n",
      "epoch:48 step:37858 [D loss: 0.774972, acc.: 74.22%] [G loss: 5.255668]\n",
      "epoch:48 step:37859 [D loss: 0.295729, acc.: 89.06%] [G loss: 6.538225]\n",
      "epoch:48 step:37860 [D loss: 0.325349, acc.: 84.38%] [G loss: 5.591044]\n",
      "epoch:48 step:37861 [D loss: 0.294965, acc.: 86.72%] [G loss: 5.482702]\n",
      "epoch:48 step:37862 [D loss: 0.387214, acc.: 85.16%] [G loss: 4.972661]\n",
      "epoch:48 step:37863 [D loss: 0.195432, acc.: 92.97%] [G loss: 4.405763]\n",
      "epoch:48 step:37864 [D loss: 0.286969, acc.: 85.94%] [G loss: 3.516323]\n",
      "epoch:48 step:37865 [D loss: 0.212330, acc.: 91.41%] [G loss: 5.264090]\n",
      "epoch:48 step:37866 [D loss: 0.243516, acc.: 89.84%] [G loss: 3.410640]\n",
      "epoch:48 step:37867 [D loss: 0.217802, acc.: 92.19%] [G loss: 3.296530]\n",
      "epoch:48 step:37868 [D loss: 0.333983, acc.: 85.16%] [G loss: 3.530280]\n",
      "epoch:48 step:37869 [D loss: 0.265413, acc.: 89.84%] [G loss: 3.309229]\n",
      "epoch:48 step:37870 [D loss: 0.279545, acc.: 88.28%] [G loss: 2.617148]\n",
      "epoch:48 step:37871 [D loss: 0.249756, acc.: 89.84%] [G loss: 3.681111]\n",
      "epoch:48 step:37872 [D loss: 0.230090, acc.: 87.50%] [G loss: 3.508412]\n",
      "epoch:48 step:37873 [D loss: 0.309539, acc.: 85.16%] [G loss: 3.081953]\n",
      "epoch:48 step:37874 [D loss: 0.284710, acc.: 89.84%] [G loss: 3.449307]\n",
      "epoch:48 step:37875 [D loss: 0.273735, acc.: 87.50%] [G loss: 3.345354]\n",
      "epoch:48 step:37876 [D loss: 0.242861, acc.: 89.84%] [G loss: 2.579284]\n",
      "epoch:48 step:37877 [D loss: 0.396468, acc.: 83.59%] [G loss: 2.884226]\n",
      "epoch:48 step:37878 [D loss: 0.279913, acc.: 89.06%] [G loss: 2.968936]\n",
      "epoch:48 step:37879 [D loss: 0.297167, acc.: 83.59%] [G loss: 3.894360]\n",
      "epoch:48 step:37880 [D loss: 0.283020, acc.: 89.06%] [G loss: 3.954017]\n",
      "epoch:48 step:37881 [D loss: 0.211819, acc.: 92.97%] [G loss: 3.890545]\n",
      "epoch:48 step:37882 [D loss: 0.338195, acc.: 84.38%] [G loss: 3.092362]\n",
      "epoch:48 step:37883 [D loss: 0.300563, acc.: 85.94%] [G loss: 2.692287]\n",
      "epoch:48 step:37884 [D loss: 0.292827, acc.: 89.84%] [G loss: 3.265340]\n",
      "epoch:48 step:37885 [D loss: 0.292776, acc.: 87.50%] [G loss: 2.823034]\n",
      "epoch:48 step:37886 [D loss: 0.276940, acc.: 89.06%] [G loss: 3.514720]\n",
      "epoch:48 step:37887 [D loss: 0.320298, acc.: 86.72%] [G loss: 2.736941]\n",
      "epoch:48 step:37888 [D loss: 0.266967, acc.: 88.28%] [G loss: 4.323006]\n",
      "epoch:48 step:37889 [D loss: 0.240409, acc.: 92.19%] [G loss: 5.354846]\n",
      "epoch:48 step:37890 [D loss: 0.295147, acc.: 87.50%] [G loss: 3.579051]\n",
      "epoch:48 step:37891 [D loss: 0.219553, acc.: 92.97%] [G loss: 4.944751]\n",
      "epoch:48 step:37892 [D loss: 0.306556, acc.: 85.16%] [G loss: 4.305253]\n",
      "epoch:48 step:37893 [D loss: 0.235612, acc.: 91.41%] [G loss: 2.845317]\n",
      "epoch:48 step:37894 [D loss: 0.292853, acc.: 89.06%] [G loss: 3.495591]\n",
      "epoch:48 step:37895 [D loss: 0.201213, acc.: 92.97%] [G loss: 3.478164]\n",
      "epoch:48 step:37896 [D loss: 0.301108, acc.: 84.38%] [G loss: 3.020022]\n",
      "epoch:48 step:37897 [D loss: 0.266270, acc.: 90.62%] [G loss: 5.841396]\n",
      "epoch:48 step:37898 [D loss: 0.395412, acc.: 83.59%] [G loss: 5.751134]\n",
      "epoch:48 step:37899 [D loss: 0.547037, acc.: 80.47%] [G loss: 5.012224]\n",
      "epoch:48 step:37900 [D loss: 0.512499, acc.: 78.91%] [G loss: 4.278894]\n",
      "epoch:48 step:37901 [D loss: 0.268474, acc.: 89.06%] [G loss: 3.681623]\n",
      "epoch:48 step:37902 [D loss: 0.518669, acc.: 79.69%] [G loss: 7.477272]\n",
      "epoch:48 step:37903 [D loss: 0.377211, acc.: 84.38%] [G loss: 4.516400]\n",
      "epoch:48 step:37904 [D loss: 0.314755, acc.: 87.50%] [G loss: 4.844633]\n",
      "epoch:48 step:37905 [D loss: 0.243748, acc.: 89.84%] [G loss: 3.893511]\n",
      "epoch:48 step:37906 [D loss: 0.298996, acc.: 85.16%] [G loss: 4.512174]\n",
      "epoch:48 step:37907 [D loss: 0.310639, acc.: 85.16%] [G loss: 3.249495]\n",
      "epoch:48 step:37908 [D loss: 0.371294, acc.: 84.38%] [G loss: 2.932202]\n",
      "epoch:48 step:37909 [D loss: 0.238295, acc.: 88.28%] [G loss: 4.164715]\n",
      "epoch:48 step:37910 [D loss: 0.335781, acc.: 87.50%] [G loss: 3.196740]\n",
      "epoch:48 step:37911 [D loss: 0.353521, acc.: 85.16%] [G loss: 2.581823]\n",
      "epoch:48 step:37912 [D loss: 0.256674, acc.: 91.41%] [G loss: 3.058366]\n",
      "epoch:48 step:37913 [D loss: 0.300274, acc.: 85.94%] [G loss: 2.549052]\n",
      "epoch:48 step:37914 [D loss: 0.251187, acc.: 91.41%] [G loss: 2.035987]\n",
      "epoch:48 step:37915 [D loss: 0.261555, acc.: 89.84%] [G loss: 2.983220]\n",
      "epoch:48 step:37916 [D loss: 0.448567, acc.: 81.25%] [G loss: 3.070625]\n",
      "epoch:48 step:37917 [D loss: 0.305809, acc.: 85.16%] [G loss: 4.102856]\n",
      "epoch:48 step:37918 [D loss: 0.389133, acc.: 81.25%] [G loss: 3.137504]\n",
      "epoch:48 step:37919 [D loss: 0.295361, acc.: 83.59%] [G loss: 3.127345]\n",
      "epoch:48 step:37920 [D loss: 0.396077, acc.: 85.16%] [G loss: 3.660491]\n",
      "epoch:48 step:37921 [D loss: 0.213503, acc.: 94.53%] [G loss: 3.500319]\n",
      "epoch:48 step:37922 [D loss: 0.486622, acc.: 76.56%] [G loss: 7.344941]\n",
      "epoch:48 step:37923 [D loss: 0.897169, acc.: 72.66%] [G loss: 7.183259]\n",
      "epoch:48 step:37924 [D loss: 1.582831, acc.: 60.16%] [G loss: 9.974463]\n",
      "epoch:48 step:37925 [D loss: 1.933909, acc.: 64.84%] [G loss: 6.137338]\n",
      "epoch:48 step:37926 [D loss: 1.579299, acc.: 56.25%] [G loss: 7.692037]\n",
      "epoch:48 step:37927 [D loss: 0.681075, acc.: 83.59%] [G loss: 4.657797]\n",
      "epoch:48 step:37928 [D loss: 0.605992, acc.: 76.56%] [G loss: 3.642029]\n",
      "epoch:48 step:37929 [D loss: 0.511270, acc.: 77.34%] [G loss: 3.382161]\n",
      "epoch:48 step:37930 [D loss: 0.355906, acc.: 84.38%] [G loss: 3.938783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37931 [D loss: 0.412312, acc.: 78.91%] [G loss: 2.998823]\n",
      "epoch:48 step:37932 [D loss: 0.380048, acc.: 79.69%] [G loss: 3.652294]\n",
      "epoch:48 step:37933 [D loss: 0.408352, acc.: 82.81%] [G loss: 2.798578]\n",
      "epoch:48 step:37934 [D loss: 0.372863, acc.: 82.03%] [G loss: 3.512238]\n",
      "epoch:48 step:37935 [D loss: 0.284770, acc.: 85.94%] [G loss: 2.665415]\n",
      "epoch:48 step:37936 [D loss: 0.362632, acc.: 85.16%] [G loss: 2.698748]\n",
      "epoch:48 step:37937 [D loss: 0.275559, acc.: 89.06%] [G loss: 3.115787]\n",
      "epoch:48 step:37938 [D loss: 0.308787, acc.: 88.28%] [G loss: 3.225338]\n",
      "epoch:48 step:37939 [D loss: 0.310922, acc.: 85.94%] [G loss: 3.160623]\n",
      "epoch:48 step:37940 [D loss: 0.322220, acc.: 88.28%] [G loss: 2.523497]\n",
      "epoch:48 step:37941 [D loss: 0.342394, acc.: 85.16%] [G loss: 2.208406]\n",
      "epoch:48 step:37942 [D loss: 0.266062, acc.: 88.28%] [G loss: 2.247159]\n",
      "epoch:48 step:37943 [D loss: 0.365351, acc.: 83.59%] [G loss: 2.397713]\n",
      "epoch:48 step:37944 [D loss: 0.289941, acc.: 86.72%] [G loss: 2.731244]\n",
      "epoch:48 step:37945 [D loss: 0.221335, acc.: 91.41%] [G loss: 3.232662]\n",
      "epoch:48 step:37946 [D loss: 0.239488, acc.: 92.19%] [G loss: 3.360263]\n",
      "epoch:48 step:37947 [D loss: 0.310729, acc.: 85.94%] [G loss: 3.491277]\n",
      "epoch:48 step:37948 [D loss: 0.358390, acc.: 82.81%] [G loss: 2.997949]\n",
      "epoch:48 step:37949 [D loss: 0.243055, acc.: 91.41%] [G loss: 3.168281]\n",
      "epoch:48 step:37950 [D loss: 0.239376, acc.: 93.75%] [G loss: 3.491700]\n",
      "epoch:48 step:37951 [D loss: 0.322793, acc.: 84.38%] [G loss: 2.985256]\n",
      "epoch:48 step:37952 [D loss: 0.356385, acc.: 83.59%] [G loss: 2.540154]\n",
      "epoch:48 step:37953 [D loss: 0.328925, acc.: 89.06%] [G loss: 2.948828]\n",
      "epoch:48 step:37954 [D loss: 0.295653, acc.: 89.84%] [G loss: 3.231592]\n",
      "epoch:48 step:37955 [D loss: 0.267021, acc.: 89.84%] [G loss: 3.640423]\n",
      "epoch:48 step:37956 [D loss: 0.272324, acc.: 87.50%] [G loss: 3.238867]\n",
      "epoch:48 step:37957 [D loss: 0.291927, acc.: 87.50%] [G loss: 2.751481]\n",
      "epoch:48 step:37958 [D loss: 0.255415, acc.: 89.84%] [G loss: 3.190991]\n",
      "epoch:48 step:37959 [D loss: 0.262748, acc.: 91.41%] [G loss: 2.341504]\n",
      "epoch:48 step:37960 [D loss: 0.299855, acc.: 85.16%] [G loss: 2.252880]\n",
      "epoch:48 step:37961 [D loss: 0.294019, acc.: 88.28%] [G loss: 3.181631]\n",
      "epoch:48 step:37962 [D loss: 0.334385, acc.: 83.59%] [G loss: 3.066100]\n",
      "epoch:48 step:37963 [D loss: 0.297614, acc.: 86.72%] [G loss: 2.933129]\n",
      "epoch:48 step:37964 [D loss: 0.356738, acc.: 85.94%] [G loss: 2.682036]\n",
      "epoch:48 step:37965 [D loss: 0.239770, acc.: 89.84%] [G loss: 4.087681]\n",
      "epoch:48 step:37966 [D loss: 0.282806, acc.: 87.50%] [G loss: 2.698384]\n",
      "epoch:48 step:37967 [D loss: 0.216141, acc.: 90.62%] [G loss: 2.671950]\n",
      "epoch:48 step:37968 [D loss: 0.170944, acc.: 95.31%] [G loss: 2.648718]\n",
      "epoch:48 step:37969 [D loss: 0.251469, acc.: 92.19%] [G loss: 2.986822]\n",
      "epoch:48 step:37970 [D loss: 0.203378, acc.: 91.41%] [G loss: 3.384232]\n",
      "epoch:48 step:37971 [D loss: 0.318569, acc.: 84.38%] [G loss: 2.589629]\n",
      "epoch:48 step:37972 [D loss: 0.287533, acc.: 86.72%] [G loss: 2.392350]\n",
      "epoch:48 step:37973 [D loss: 0.179502, acc.: 92.97%] [G loss: 3.206644]\n",
      "epoch:48 step:37974 [D loss: 0.240629, acc.: 88.28%] [G loss: 3.058190]\n",
      "epoch:48 step:37975 [D loss: 0.294439, acc.: 86.72%] [G loss: 2.452398]\n",
      "epoch:48 step:37976 [D loss: 0.316376, acc.: 91.41%] [G loss: 3.020124]\n",
      "epoch:48 step:37977 [D loss: 0.469478, acc.: 80.47%] [G loss: 2.966678]\n",
      "epoch:48 step:37978 [D loss: 0.235643, acc.: 93.75%] [G loss: 2.434731]\n",
      "epoch:48 step:37979 [D loss: 0.340320, acc.: 87.50%] [G loss: 2.933318]\n",
      "epoch:48 step:37980 [D loss: 0.215562, acc.: 92.97%] [G loss: 3.872563]\n",
      "epoch:48 step:37981 [D loss: 0.276536, acc.: 89.06%] [G loss: 3.161950]\n",
      "epoch:48 step:37982 [D loss: 0.266142, acc.: 86.72%] [G loss: 3.155549]\n",
      "epoch:48 step:37983 [D loss: 0.259850, acc.: 90.62%] [G loss: 3.362649]\n",
      "epoch:48 step:37984 [D loss: 0.260693, acc.: 91.41%] [G loss: 2.664736]\n",
      "epoch:48 step:37985 [D loss: 0.328918, acc.: 82.03%] [G loss: 3.997836]\n",
      "epoch:48 step:37986 [D loss: 0.277986, acc.: 86.72%] [G loss: 2.563803]\n",
      "epoch:48 step:37987 [D loss: 0.271506, acc.: 89.06%] [G loss: 3.172960]\n",
      "epoch:48 step:37988 [D loss: 0.288385, acc.: 87.50%] [G loss: 3.067478]\n",
      "epoch:48 step:37989 [D loss: 0.264936, acc.: 87.50%] [G loss: 3.701328]\n",
      "epoch:48 step:37990 [D loss: 0.371039, acc.: 83.59%] [G loss: 2.598179]\n",
      "epoch:48 step:37991 [D loss: 0.259060, acc.: 89.84%] [G loss: 3.290532]\n",
      "epoch:48 step:37992 [D loss: 0.310329, acc.: 82.81%] [G loss: 3.081436]\n",
      "epoch:48 step:37993 [D loss: 0.298532, acc.: 85.94%] [G loss: 2.692071]\n",
      "epoch:48 step:37994 [D loss: 0.274145, acc.: 87.50%] [G loss: 3.667564]\n",
      "epoch:48 step:37995 [D loss: 0.331735, acc.: 85.94%] [G loss: 2.779313]\n",
      "epoch:48 step:37996 [D loss: 0.366502, acc.: 84.38%] [G loss: 2.844949]\n",
      "epoch:48 step:37997 [D loss: 0.262013, acc.: 89.84%] [G loss: 3.022354]\n",
      "epoch:48 step:37998 [D loss: 0.282778, acc.: 86.72%] [G loss: 3.108315]\n",
      "epoch:48 step:37999 [D loss: 0.346028, acc.: 85.16%] [G loss: 2.619049]\n",
      "epoch:48 step:38000 [D loss: 0.273947, acc.: 89.06%] [G loss: 4.437290]\n",
      "epoch:48 step:38001 [D loss: 0.256860, acc.: 89.84%] [G loss: 3.282430]\n",
      "epoch:48 step:38002 [D loss: 0.170862, acc.: 95.31%] [G loss: 4.226624]\n",
      "epoch:48 step:38003 [D loss: 0.209925, acc.: 92.97%] [G loss: 3.860892]\n",
      "epoch:48 step:38004 [D loss: 0.229637, acc.: 92.19%] [G loss: 3.542704]\n",
      "epoch:48 step:38005 [D loss: 0.319722, acc.: 84.38%] [G loss: 2.835822]\n",
      "epoch:48 step:38006 [D loss: 0.246051, acc.: 91.41%] [G loss: 2.698627]\n",
      "epoch:48 step:38007 [D loss: 0.259801, acc.: 88.28%] [G loss: 3.098062]\n",
      "epoch:48 step:38008 [D loss: 0.450799, acc.: 76.56%] [G loss: 2.565593]\n",
      "epoch:48 step:38009 [D loss: 0.306279, acc.: 85.16%] [G loss: 2.662557]\n",
      "epoch:48 step:38010 [D loss: 0.366578, acc.: 82.03%] [G loss: 2.589689]\n",
      "epoch:48 step:38011 [D loss: 0.312226, acc.: 83.59%] [G loss: 2.404027]\n",
      "epoch:48 step:38012 [D loss: 0.344769, acc.: 88.28%] [G loss: 2.475858]\n",
      "epoch:48 step:38013 [D loss: 0.250285, acc.: 89.06%] [G loss: 2.789811]\n",
      "epoch:48 step:38014 [D loss: 0.349971, acc.: 86.72%] [G loss: 2.684328]\n",
      "epoch:48 step:38015 [D loss: 0.311753, acc.: 87.50%] [G loss: 2.949331]\n",
      "epoch:48 step:38016 [D loss: 0.399127, acc.: 82.03%] [G loss: 2.392389]\n",
      "epoch:48 step:38017 [D loss: 0.276003, acc.: 91.41%] [G loss: 2.918875]\n",
      "epoch:48 step:38018 [D loss: 0.368100, acc.: 80.47%] [G loss: 2.477318]\n",
      "epoch:48 step:38019 [D loss: 0.299583, acc.: 88.28%] [G loss: 2.935633]\n",
      "epoch:48 step:38020 [D loss: 0.420444, acc.: 81.25%] [G loss: 2.366826]\n",
      "epoch:48 step:38021 [D loss: 0.339662, acc.: 89.06%] [G loss: 3.063090]\n",
      "epoch:48 step:38022 [D loss: 0.261378, acc.: 90.62%] [G loss: 2.999437]\n",
      "epoch:48 step:38023 [D loss: 0.313326, acc.: 86.72%] [G loss: 3.312942]\n",
      "epoch:48 step:38024 [D loss: 0.330833, acc.: 85.16%] [G loss: 4.080989]\n",
      "epoch:48 step:38025 [D loss: 0.498932, acc.: 73.44%] [G loss: 4.143118]\n",
      "epoch:48 step:38026 [D loss: 0.429397, acc.: 82.81%] [G loss: 4.407718]\n",
      "epoch:48 step:38027 [D loss: 0.216445, acc.: 90.62%] [G loss: 4.980243]\n",
      "epoch:48 step:38028 [D loss: 0.313368, acc.: 86.72%] [G loss: 3.797402]\n",
      "epoch:48 step:38029 [D loss: 0.256900, acc.: 82.81%] [G loss: 4.141957]\n",
      "epoch:48 step:38030 [D loss: 0.273057, acc.: 89.84%] [G loss: 4.163818]\n",
      "epoch:48 step:38031 [D loss: 0.227487, acc.: 89.84%] [G loss: 3.865796]\n",
      "epoch:48 step:38032 [D loss: 0.280592, acc.: 89.84%] [G loss: 3.400821]\n",
      "epoch:48 step:38033 [D loss: 0.320064, acc.: 85.94%] [G loss: 3.714365]\n",
      "epoch:48 step:38034 [D loss: 0.338067, acc.: 83.59%] [G loss: 2.681411]\n",
      "epoch:48 step:38035 [D loss: 0.362445, acc.: 86.72%] [G loss: 3.288513]\n",
      "epoch:48 step:38036 [D loss: 0.310694, acc.: 88.28%] [G loss: 3.351753]\n",
      "epoch:48 step:38037 [D loss: 0.216932, acc.: 90.62%] [G loss: 2.846910]\n",
      "epoch:48 step:38038 [D loss: 0.347495, acc.: 83.59%] [G loss: 3.644619]\n",
      "epoch:48 step:38039 [D loss: 0.280189, acc.: 86.72%] [G loss: 3.092700]\n",
      "epoch:48 step:38040 [D loss: 0.338468, acc.: 85.16%] [G loss: 6.759777]\n",
      "epoch:48 step:38041 [D loss: 0.679583, acc.: 79.69%] [G loss: 6.174245]\n",
      "epoch:48 step:38042 [D loss: 0.483403, acc.: 79.69%] [G loss: 4.000204]\n",
      "epoch:48 step:38043 [D loss: 0.359731, acc.: 82.03%] [G loss: 4.762523]\n",
      "epoch:48 step:38044 [D loss: 0.399285, acc.: 81.25%] [G loss: 5.196949]\n",
      "epoch:48 step:38045 [D loss: 0.309517, acc.: 85.94%] [G loss: 3.426714]\n",
      "epoch:48 step:38046 [D loss: 0.287162, acc.: 84.38%] [G loss: 4.515003]\n",
      "epoch:48 step:38047 [D loss: 0.298872, acc.: 85.94%] [G loss: 3.706996]\n",
      "epoch:48 step:38048 [D loss: 0.317728, acc.: 85.16%] [G loss: 3.298871]\n",
      "epoch:48 step:38049 [D loss: 0.304329, acc.: 88.28%] [G loss: 3.983526]\n",
      "epoch:48 step:38050 [D loss: 0.469573, acc.: 77.34%] [G loss: 3.731761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:38051 [D loss: 0.264549, acc.: 88.28%] [G loss: 2.944601]\n",
      "epoch:48 step:38052 [D loss: 0.340996, acc.: 85.94%] [G loss: 2.761391]\n",
      "epoch:48 step:38053 [D loss: 0.404245, acc.: 83.59%] [G loss: 3.241220]\n",
      "epoch:48 step:38054 [D loss: 0.306318, acc.: 89.84%] [G loss: 2.270381]\n",
      "epoch:48 step:38055 [D loss: 0.362433, acc.: 81.25%] [G loss: 3.470773]\n",
      "epoch:48 step:38056 [D loss: 0.257884, acc.: 91.41%] [G loss: 3.333810]\n",
      "epoch:48 step:38057 [D loss: 0.225849, acc.: 92.19%] [G loss: 4.317423]\n",
      "epoch:48 step:38058 [D loss: 0.305253, acc.: 85.16%] [G loss: 3.054288]\n",
      "epoch:48 step:38059 [D loss: 0.352402, acc.: 85.16%] [G loss: 4.876566]\n",
      "epoch:48 step:38060 [D loss: 0.274557, acc.: 88.28%] [G loss: 5.956251]\n",
      "epoch:48 step:38061 [D loss: 0.350184, acc.: 84.38%] [G loss: 4.038852]\n",
      "epoch:48 step:38062 [D loss: 0.265379, acc.: 88.28%] [G loss: 3.500258]\n",
      "epoch:48 step:38063 [D loss: 0.337139, acc.: 83.59%] [G loss: 3.187669]\n",
      "epoch:48 step:38064 [D loss: 0.343584, acc.: 83.59%] [G loss: 2.840387]\n",
      "epoch:48 step:38065 [D loss: 0.227075, acc.: 92.19%] [G loss: 4.097417]\n",
      "epoch:48 step:38066 [D loss: 0.376909, acc.: 81.25%] [G loss: 3.214073]\n",
      "epoch:48 step:38067 [D loss: 0.358455, acc.: 82.81%] [G loss: 3.271480]\n",
      "epoch:48 step:38068 [D loss: 0.366314, acc.: 82.03%] [G loss: 2.817643]\n",
      "epoch:48 step:38069 [D loss: 0.316068, acc.: 84.38%] [G loss: 3.324779]\n",
      "epoch:48 step:38070 [D loss: 0.312356, acc.: 88.28%] [G loss: 7.537739]\n",
      "epoch:48 step:38071 [D loss: 0.288393, acc.: 85.16%] [G loss: 4.248343]\n",
      "epoch:48 step:38072 [D loss: 0.404559, acc.: 82.81%] [G loss: 7.468536]\n",
      "epoch:48 step:38073 [D loss: 0.306933, acc.: 88.28%] [G loss: 3.552346]\n",
      "epoch:48 step:38074 [D loss: 0.337996, acc.: 86.72%] [G loss: 3.934487]\n",
      "epoch:48 step:38075 [D loss: 0.269004, acc.: 88.28%] [G loss: 3.903036]\n",
      "epoch:48 step:38076 [D loss: 0.331851, acc.: 82.81%] [G loss: 3.461015]\n",
      "epoch:48 step:38077 [D loss: 0.312756, acc.: 86.72%] [G loss: 4.064635]\n",
      "epoch:48 step:38078 [D loss: 0.367924, acc.: 82.81%] [G loss: 2.703671]\n",
      "epoch:48 step:38079 [D loss: 0.283394, acc.: 87.50%] [G loss: 3.572265]\n",
      "epoch:48 step:38080 [D loss: 0.269581, acc.: 88.28%] [G loss: 3.230947]\n",
      "epoch:48 step:38081 [D loss: 0.393515, acc.: 82.81%] [G loss: 3.131227]\n",
      "epoch:48 step:38082 [D loss: 0.285037, acc.: 85.94%] [G loss: 3.134597]\n",
      "epoch:48 step:38083 [D loss: 0.469040, acc.: 76.56%] [G loss: 3.362785]\n",
      "epoch:48 step:38084 [D loss: 0.324188, acc.: 85.16%] [G loss: 3.737276]\n",
      "epoch:48 step:38085 [D loss: 0.327083, acc.: 86.72%] [G loss: 3.246333]\n",
      "epoch:48 step:38086 [D loss: 0.353072, acc.: 86.72%] [G loss: 3.107074]\n",
      "epoch:48 step:38087 [D loss: 0.300866, acc.: 85.94%] [G loss: 2.418816]\n",
      "epoch:48 step:38088 [D loss: 0.338962, acc.: 82.03%] [G loss: 2.807425]\n",
      "epoch:48 step:38089 [D loss: 0.347908, acc.: 86.72%] [G loss: 2.912703]\n",
      "epoch:48 step:38090 [D loss: 0.388424, acc.: 82.81%] [G loss: 2.588571]\n",
      "epoch:48 step:38091 [D loss: 0.252747, acc.: 90.62%] [G loss: 3.451574]\n",
      "epoch:48 step:38092 [D loss: 0.329244, acc.: 86.72%] [G loss: 2.786756]\n",
      "epoch:48 step:38093 [D loss: 0.309644, acc.: 87.50%] [G loss: 2.891122]\n",
      "epoch:48 step:38094 [D loss: 0.321197, acc.: 85.94%] [G loss: 2.844040]\n",
      "epoch:48 step:38095 [D loss: 0.319297, acc.: 86.72%] [G loss: 2.831311]\n",
      "epoch:48 step:38096 [D loss: 0.407293, acc.: 80.47%] [G loss: 3.006036]\n",
      "epoch:48 step:38097 [D loss: 0.371669, acc.: 86.72%] [G loss: 2.995594]\n",
      "epoch:48 step:38098 [D loss: 0.448011, acc.: 77.34%] [G loss: 3.611706]\n",
      "epoch:48 step:38099 [D loss: 0.293063, acc.: 87.50%] [G loss: 3.430193]\n",
      "epoch:48 step:38100 [D loss: 0.316992, acc.: 86.72%] [G loss: 2.758054]\n",
      "epoch:48 step:38101 [D loss: 0.371508, acc.: 83.59%] [G loss: 2.697123]\n",
      "epoch:48 step:38102 [D loss: 0.301361, acc.: 84.38%] [G loss: 2.846555]\n",
      "epoch:48 step:38103 [D loss: 0.286152, acc.: 89.84%] [G loss: 3.166565]\n",
      "epoch:48 step:38104 [D loss: 0.308668, acc.: 84.38%] [G loss: 3.038860]\n",
      "epoch:48 step:38105 [D loss: 0.335880, acc.: 89.06%] [G loss: 2.710209]\n",
      "epoch:48 step:38106 [D loss: 0.414145, acc.: 82.81%] [G loss: 2.435539]\n",
      "epoch:48 step:38107 [D loss: 0.270134, acc.: 86.72%] [G loss: 2.666831]\n",
      "epoch:48 step:38108 [D loss: 0.246430, acc.: 91.41%] [G loss: 2.961826]\n",
      "epoch:48 step:38109 [D loss: 0.277868, acc.: 88.28%] [G loss: 2.853647]\n",
      "epoch:48 step:38110 [D loss: 0.340451, acc.: 85.16%] [G loss: 2.507349]\n",
      "epoch:48 step:38111 [D loss: 0.288908, acc.: 85.94%] [G loss: 2.833179]\n",
      "epoch:48 step:38112 [D loss: 0.285312, acc.: 89.84%] [G loss: 3.139793]\n",
      "epoch:48 step:38113 [D loss: 0.317203, acc.: 85.94%] [G loss: 2.601458]\n",
      "epoch:48 step:38114 [D loss: 0.211161, acc.: 91.41%] [G loss: 2.678460]\n",
      "epoch:48 step:38115 [D loss: 0.420594, acc.: 78.91%] [G loss: 2.332357]\n",
      "epoch:48 step:38116 [D loss: 0.404564, acc.: 80.47%] [G loss: 3.012884]\n",
      "epoch:48 step:38117 [D loss: 0.392524, acc.: 80.47%] [G loss: 3.234980]\n",
      "epoch:48 step:38118 [D loss: 0.373587, acc.: 83.59%] [G loss: 2.684544]\n",
      "epoch:48 step:38119 [D loss: 0.274322, acc.: 86.72%] [G loss: 2.603137]\n",
      "epoch:48 step:38120 [D loss: 0.249881, acc.: 89.06%] [G loss: 2.648601]\n",
      "epoch:48 step:38121 [D loss: 0.436374, acc.: 78.12%] [G loss: 2.647270]\n",
      "epoch:48 step:38122 [D loss: 0.338697, acc.: 86.72%] [G loss: 2.795948]\n",
      "epoch:48 step:38123 [D loss: 0.265848, acc.: 88.28%] [G loss: 3.443061]\n",
      "epoch:48 step:38124 [D loss: 0.272630, acc.: 89.06%] [G loss: 2.967974]\n",
      "epoch:48 step:38125 [D loss: 0.282123, acc.: 87.50%] [G loss: 3.083989]\n",
      "epoch:48 step:38126 [D loss: 0.238118, acc.: 89.06%] [G loss: 2.911837]\n",
      "epoch:48 step:38127 [D loss: 0.279755, acc.: 88.28%] [G loss: 3.371266]\n",
      "epoch:48 step:38128 [D loss: 0.329558, acc.: 85.94%] [G loss: 3.751745]\n",
      "epoch:48 step:38129 [D loss: 0.361138, acc.: 79.69%] [G loss: 4.161168]\n",
      "epoch:48 step:38130 [D loss: 0.318693, acc.: 87.50%] [G loss: 3.518283]\n",
      "epoch:48 step:38131 [D loss: 0.269087, acc.: 89.84%] [G loss: 3.681086]\n",
      "epoch:48 step:38132 [D loss: 0.224225, acc.: 89.84%] [G loss: 4.338517]\n",
      "epoch:48 step:38133 [D loss: 0.274863, acc.: 84.38%] [G loss: 3.238934]\n",
      "epoch:48 step:38134 [D loss: 0.373342, acc.: 82.81%] [G loss: 3.219753]\n",
      "epoch:48 step:38135 [D loss: 0.220177, acc.: 90.62%] [G loss: 3.961844]\n",
      "epoch:48 step:38136 [D loss: 0.387294, acc.: 83.59%] [G loss: 5.071564]\n",
      "epoch:48 step:38137 [D loss: 0.375675, acc.: 82.81%] [G loss: 3.855625]\n",
      "epoch:48 step:38138 [D loss: 0.253210, acc.: 92.19%] [G loss: 6.740277]\n",
      "epoch:48 step:38139 [D loss: 0.739951, acc.: 68.75%] [G loss: 4.549777]\n",
      "epoch:48 step:38140 [D loss: 0.499355, acc.: 78.12%] [G loss: 4.309866]\n",
      "epoch:48 step:38141 [D loss: 0.425792, acc.: 82.81%] [G loss: 4.596772]\n",
      "epoch:48 step:38142 [D loss: 0.399199, acc.: 82.03%] [G loss: 3.689386]\n",
      "epoch:48 step:38143 [D loss: 0.408691, acc.: 80.47%] [G loss: 4.895067]\n",
      "epoch:48 step:38144 [D loss: 0.264888, acc.: 88.28%] [G loss: 4.015322]\n",
      "epoch:48 step:38145 [D loss: 0.432019, acc.: 78.91%] [G loss: 3.395239]\n",
      "epoch:48 step:38146 [D loss: 0.235575, acc.: 91.41%] [G loss: 3.880068]\n",
      "epoch:48 step:38147 [D loss: 0.349662, acc.: 84.38%] [G loss: 3.752436]\n",
      "epoch:48 step:38148 [D loss: 0.255895, acc.: 89.06%] [G loss: 3.575832]\n",
      "epoch:48 step:38149 [D loss: 0.293688, acc.: 89.06%] [G loss: 3.513835]\n",
      "epoch:48 step:38150 [D loss: 0.320085, acc.: 83.59%] [G loss: 3.068220]\n",
      "epoch:48 step:38151 [D loss: 0.294695, acc.: 86.72%] [G loss: 3.655843]\n",
      "epoch:48 step:38152 [D loss: 0.350078, acc.: 84.38%] [G loss: 2.995354]\n",
      "epoch:48 step:38153 [D loss: 0.349970, acc.: 85.94%] [G loss: 2.771389]\n",
      "epoch:48 step:38154 [D loss: 0.228068, acc.: 90.62%] [G loss: 3.878551]\n",
      "epoch:48 step:38155 [D loss: 0.227540, acc.: 89.84%] [G loss: 2.812418]\n",
      "epoch:48 step:38156 [D loss: 0.359458, acc.: 82.03%] [G loss: 3.596482]\n",
      "epoch:48 step:38157 [D loss: 0.266299, acc.: 89.84%] [G loss: 3.182163]\n",
      "epoch:48 step:38158 [D loss: 0.316499, acc.: 83.59%] [G loss: 2.967169]\n",
      "epoch:48 step:38159 [D loss: 0.371000, acc.: 82.03%] [G loss: 2.795732]\n",
      "epoch:48 step:38160 [D loss: 0.262430, acc.: 90.62%] [G loss: 3.967173]\n",
      "epoch:48 step:38161 [D loss: 0.254868, acc.: 89.84%] [G loss: 3.864165]\n",
      "epoch:48 step:38162 [D loss: 0.314730, acc.: 87.50%] [G loss: 3.769396]\n",
      "epoch:48 step:38163 [D loss: 0.374642, acc.: 85.16%] [G loss: 3.667210]\n",
      "epoch:48 step:38164 [D loss: 0.329821, acc.: 85.94%] [G loss: 2.558019]\n",
      "epoch:48 step:38165 [D loss: 0.313637, acc.: 84.38%] [G loss: 2.533151]\n",
      "epoch:48 step:38166 [D loss: 0.224314, acc.: 92.19%] [G loss: 3.039814]\n",
      "epoch:48 step:38167 [D loss: 0.326229, acc.: 82.81%] [G loss: 2.582540]\n",
      "epoch:48 step:38168 [D loss: 0.251367, acc.: 89.06%] [G loss: 2.544985]\n",
      "epoch:48 step:38169 [D loss: 0.308392, acc.: 82.81%] [G loss: 2.733175]\n",
      "epoch:48 step:38170 [D loss: 0.344875, acc.: 85.16%] [G loss: 3.002303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:38171 [D loss: 0.332464, acc.: 86.72%] [G loss: 2.848116]\n",
      "epoch:48 step:38172 [D loss: 0.300449, acc.: 87.50%] [G loss: 3.655688]\n",
      "epoch:48 step:38173 [D loss: 0.322052, acc.: 83.59%] [G loss: 2.609721]\n",
      "epoch:48 step:38174 [D loss: 0.302706, acc.: 83.59%] [G loss: 4.158113]\n",
      "epoch:48 step:38175 [D loss: 0.316026, acc.: 86.72%] [G loss: 3.150169]\n",
      "epoch:48 step:38176 [D loss: 0.253222, acc.: 92.19%] [G loss: 3.667387]\n",
      "epoch:48 step:38177 [D loss: 0.279376, acc.: 89.06%] [G loss: 3.058814]\n",
      "epoch:48 step:38178 [D loss: 0.250490, acc.: 91.41%] [G loss: 3.559064]\n",
      "epoch:48 step:38179 [D loss: 0.394518, acc.: 79.69%] [G loss: 2.851448]\n",
      "epoch:48 step:38180 [D loss: 0.367850, acc.: 84.38%] [G loss: 6.919964]\n",
      "epoch:48 step:38181 [D loss: 0.400737, acc.: 81.25%] [G loss: 6.544336]\n",
      "epoch:48 step:38182 [D loss: 0.189485, acc.: 92.97%] [G loss: 5.588803]\n",
      "epoch:48 step:38183 [D loss: 0.478012, acc.: 80.47%] [G loss: 4.038375]\n",
      "epoch:48 step:38184 [D loss: 0.303619, acc.: 87.50%] [G loss: 4.665434]\n",
      "epoch:48 step:38185 [D loss: 0.269705, acc.: 88.28%] [G loss: 4.565122]\n",
      "epoch:48 step:38186 [D loss: 0.437262, acc.: 80.47%] [G loss: 4.395542]\n",
      "epoch:48 step:38187 [D loss: 0.353936, acc.: 86.72%] [G loss: 3.408655]\n",
      "epoch:48 step:38188 [D loss: 0.351425, acc.: 84.38%] [G loss: 4.010385]\n",
      "epoch:48 step:38189 [D loss: 0.338307, acc.: 84.38%] [G loss: 3.059063]\n",
      "epoch:48 step:38190 [D loss: 0.231662, acc.: 90.62%] [G loss: 3.832206]\n",
      "epoch:48 step:38191 [D loss: 0.434324, acc.: 80.47%] [G loss: 4.232221]\n",
      "epoch:48 step:38192 [D loss: 0.359073, acc.: 81.25%] [G loss: 2.287207]\n",
      "epoch:48 step:38193 [D loss: 0.335522, acc.: 84.38%] [G loss: 3.773373]\n",
      "epoch:48 step:38194 [D loss: 0.400291, acc.: 80.47%] [G loss: 3.317638]\n",
      "epoch:48 step:38195 [D loss: 0.323531, acc.: 82.81%] [G loss: 5.093835]\n",
      "epoch:48 step:38196 [D loss: 0.293018, acc.: 89.06%] [G loss: 4.078001]\n",
      "epoch:48 step:38197 [D loss: 0.257835, acc.: 90.62%] [G loss: 3.764331]\n",
      "epoch:48 step:38198 [D loss: 0.348857, acc.: 85.94%] [G loss: 3.671576]\n",
      "epoch:48 step:38199 [D loss: 0.276497, acc.: 88.28%] [G loss: 2.902528]\n",
      "epoch:48 step:38200 [D loss: 0.270162, acc.: 90.62%] [G loss: 2.992004]\n",
      "epoch:48 step:38201 [D loss: 0.376056, acc.: 83.59%] [G loss: 2.915596]\n",
      "epoch:48 step:38202 [D loss: 0.270517, acc.: 86.72%] [G loss: 3.068659]\n",
      "epoch:48 step:38203 [D loss: 0.403179, acc.: 81.25%] [G loss: 3.688787]\n",
      "epoch:48 step:38204 [D loss: 0.368208, acc.: 83.59%] [G loss: 2.577036]\n",
      "epoch:48 step:38205 [D loss: 0.336093, acc.: 86.72%] [G loss: 3.422154]\n",
      "epoch:48 step:38206 [D loss: 0.312248, acc.: 87.50%] [G loss: 2.968068]\n",
      "epoch:48 step:38207 [D loss: 0.401293, acc.: 82.81%] [G loss: 2.840933]\n",
      "epoch:48 step:38208 [D loss: 0.385190, acc.: 82.03%] [G loss: 3.349946]\n",
      "epoch:48 step:38209 [D loss: 0.326784, acc.: 87.50%] [G loss: 3.168690]\n",
      "epoch:48 step:38210 [D loss: 0.260494, acc.: 90.62%] [G loss: 2.785072]\n",
      "epoch:48 step:38211 [D loss: 0.207826, acc.: 92.97%] [G loss: 4.320602]\n",
      "epoch:48 step:38212 [D loss: 0.248208, acc.: 92.97%] [G loss: 3.095900]\n",
      "epoch:48 step:38213 [D loss: 0.353974, acc.: 83.59%] [G loss: 4.868746]\n",
      "epoch:48 step:38214 [D loss: 0.295428, acc.: 88.28%] [G loss: 3.632666]\n",
      "epoch:48 step:38215 [D loss: 0.273792, acc.: 88.28%] [G loss: 4.191216]\n",
      "epoch:48 step:38216 [D loss: 0.371846, acc.: 86.72%] [G loss: 3.148179]\n",
      "epoch:48 step:38217 [D loss: 0.203413, acc.: 91.41%] [G loss: 4.450548]\n",
      "epoch:48 step:38218 [D loss: 0.241198, acc.: 88.28%] [G loss: 4.420048]\n",
      "epoch:48 step:38219 [D loss: 0.353332, acc.: 85.16%] [G loss: 3.525094]\n",
      "epoch:48 step:38220 [D loss: 0.275712, acc.: 88.28%] [G loss: 3.449638]\n",
      "epoch:48 step:38221 [D loss: 0.177118, acc.: 93.75%] [G loss: 4.005492]\n",
      "epoch:48 step:38222 [D loss: 0.352172, acc.: 82.81%] [G loss: 3.342757]\n",
      "epoch:48 step:38223 [D loss: 0.270574, acc.: 87.50%] [G loss: 3.243176]\n",
      "epoch:48 step:38224 [D loss: 0.304521, acc.: 87.50%] [G loss: 3.400122]\n",
      "epoch:48 step:38225 [D loss: 0.414105, acc.: 78.91%] [G loss: 2.699560]\n",
      "epoch:48 step:38226 [D loss: 0.312347, acc.: 86.72%] [G loss: 3.038316]\n",
      "epoch:48 step:38227 [D loss: 0.461245, acc.: 80.47%] [G loss: 3.884547]\n",
      "epoch:48 step:38228 [D loss: 0.229178, acc.: 91.41%] [G loss: 5.064208]\n",
      "epoch:48 step:38229 [D loss: 0.309243, acc.: 88.28%] [G loss: 4.259709]\n",
      "epoch:48 step:38230 [D loss: 0.370462, acc.: 79.69%] [G loss: 5.746159]\n",
      "epoch:48 step:38231 [D loss: 0.239843, acc.: 91.41%] [G loss: 3.846688]\n",
      "epoch:48 step:38232 [D loss: 0.245546, acc.: 88.28%] [G loss: 5.347967]\n",
      "epoch:48 step:38233 [D loss: 0.353691, acc.: 84.38%] [G loss: 2.975061]\n",
      "epoch:48 step:38234 [D loss: 0.227503, acc.: 89.84%] [G loss: 4.053539]\n",
      "epoch:48 step:38235 [D loss: 0.244506, acc.: 86.72%] [G loss: 3.796908]\n",
      "epoch:48 step:38236 [D loss: 0.291996, acc.: 86.72%] [G loss: 2.574706]\n",
      "epoch:48 step:38237 [D loss: 0.275155, acc.: 88.28%] [G loss: 2.914780]\n",
      "epoch:48 step:38238 [D loss: 0.300649, acc.: 86.72%] [G loss: 3.377741]\n",
      "epoch:48 step:38239 [D loss: 0.261879, acc.: 86.72%] [G loss: 3.216436]\n",
      "epoch:48 step:38240 [D loss: 0.316670, acc.: 85.16%] [G loss: 3.444640]\n",
      "epoch:48 step:38241 [D loss: 0.383911, acc.: 83.59%] [G loss: 3.033909]\n",
      "epoch:48 step:38242 [D loss: 0.309393, acc.: 89.84%] [G loss: 3.447700]\n",
      "epoch:48 step:38243 [D loss: 0.346049, acc.: 85.94%] [G loss: 3.148673]\n",
      "epoch:48 step:38244 [D loss: 0.276153, acc.: 86.72%] [G loss: 3.130699]\n",
      "epoch:48 step:38245 [D loss: 0.278954, acc.: 89.84%] [G loss: 3.161988]\n",
      "epoch:48 step:38246 [D loss: 0.389749, acc.: 80.47%] [G loss: 2.890270]\n",
      "epoch:48 step:38247 [D loss: 0.316769, acc.: 85.94%] [G loss: 2.856740]\n",
      "epoch:48 step:38248 [D loss: 0.246754, acc.: 89.84%] [G loss: 3.627392]\n",
      "epoch:48 step:38249 [D loss: 0.242928, acc.: 91.41%] [G loss: 4.152219]\n",
      "epoch:48 step:38250 [D loss: 0.232351, acc.: 90.62%] [G loss: 5.487935]\n",
      "epoch:48 step:38251 [D loss: 0.360866, acc.: 81.25%] [G loss: 3.410395]\n",
      "epoch:48 step:38252 [D loss: 0.296127, acc.: 87.50%] [G loss: 3.912288]\n",
      "epoch:48 step:38253 [D loss: 0.399615, acc.: 80.47%] [G loss: 3.279531]\n",
      "epoch:48 step:38254 [D loss: 0.206763, acc.: 92.97%] [G loss: 4.062167]\n",
      "epoch:48 step:38255 [D loss: 0.294528, acc.: 89.84%] [G loss: 4.142998]\n",
      "epoch:48 step:38256 [D loss: 0.378405, acc.: 82.03%] [G loss: 5.134290]\n",
      "epoch:48 step:38257 [D loss: 0.229116, acc.: 88.28%] [G loss: 5.719761]\n",
      "epoch:48 step:38258 [D loss: 0.322166, acc.: 84.38%] [G loss: 3.671694]\n",
      "epoch:48 step:38259 [D loss: 0.367415, acc.: 82.81%] [G loss: 3.676004]\n",
      "epoch:48 step:38260 [D loss: 0.330891, acc.: 85.94%] [G loss: 3.464108]\n",
      "epoch:48 step:38261 [D loss: 0.309632, acc.: 89.06%] [G loss: 3.720360]\n",
      "epoch:48 step:38262 [D loss: 0.392787, acc.: 82.03%] [G loss: 2.552762]\n",
      "epoch:48 step:38263 [D loss: 0.407182, acc.: 84.38%] [G loss: 2.764006]\n",
      "epoch:48 step:38264 [D loss: 0.311890, acc.: 87.50%] [G loss: 3.132853]\n",
      "epoch:48 step:38265 [D loss: 0.377964, acc.: 82.03%] [G loss: 2.349336]\n",
      "epoch:48 step:38266 [D loss: 0.285443, acc.: 89.06%] [G loss: 3.443885]\n",
      "epoch:48 step:38267 [D loss: 0.336882, acc.: 83.59%] [G loss: 2.803535]\n",
      "epoch:48 step:38268 [D loss: 0.235772, acc.: 91.41%] [G loss: 2.498843]\n",
      "epoch:48 step:38269 [D loss: 0.322446, acc.: 85.94%] [G loss: 3.765066]\n",
      "epoch:49 step:38270 [D loss: 0.292068, acc.: 90.62%] [G loss: 3.087518]\n",
      "epoch:49 step:38271 [D loss: 0.261737, acc.: 90.62%] [G loss: 2.985422]\n",
      "epoch:49 step:38272 [D loss: 0.368000, acc.: 80.47%] [G loss: 4.810673]\n",
      "epoch:49 step:38273 [D loss: 0.405087, acc.: 79.69%] [G loss: 2.874954]\n",
      "epoch:49 step:38274 [D loss: 0.272971, acc.: 85.94%] [G loss: 3.346271]\n",
      "epoch:49 step:38275 [D loss: 0.260552, acc.: 90.62%] [G loss: 4.643411]\n",
      "epoch:49 step:38276 [D loss: 0.361826, acc.: 82.03%] [G loss: 4.418289]\n",
      "epoch:49 step:38277 [D loss: 0.183544, acc.: 93.75%] [G loss: 4.916958]\n",
      "epoch:49 step:38278 [D loss: 0.411716, acc.: 81.25%] [G loss: 4.579824]\n",
      "epoch:49 step:38279 [D loss: 0.243065, acc.: 89.84%] [G loss: 4.626596]\n",
      "epoch:49 step:38280 [D loss: 0.321690, acc.: 84.38%] [G loss: 3.371868]\n",
      "epoch:49 step:38281 [D loss: 0.211330, acc.: 90.62%] [G loss: 3.315285]\n",
      "epoch:49 step:38282 [D loss: 0.309919, acc.: 88.28%] [G loss: 3.567286]\n",
      "epoch:49 step:38283 [D loss: 0.344781, acc.: 85.94%] [G loss: 5.563617]\n",
      "epoch:49 step:38284 [D loss: 0.462931, acc.: 80.47%] [G loss: 4.548293]\n",
      "epoch:49 step:38285 [D loss: 0.234838, acc.: 91.41%] [G loss: 7.976707]\n",
      "epoch:49 step:38286 [D loss: 0.249633, acc.: 89.06%] [G loss: 5.449467]\n",
      "epoch:49 step:38287 [D loss: 0.317489, acc.: 85.16%] [G loss: 4.256465]\n",
      "epoch:49 step:38288 [D loss: 0.351868, acc.: 85.94%] [G loss: 4.782118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38289 [D loss: 0.203272, acc.: 92.97%] [G loss: 4.675049]\n",
      "epoch:49 step:38290 [D loss: 0.279855, acc.: 85.16%] [G loss: 2.769297]\n",
      "epoch:49 step:38291 [D loss: 0.280873, acc.: 87.50%] [G loss: 4.650269]\n",
      "epoch:49 step:38292 [D loss: 0.274680, acc.: 88.28%] [G loss: 3.076536]\n",
      "epoch:49 step:38293 [D loss: 0.198277, acc.: 92.97%] [G loss: 3.832440]\n",
      "epoch:49 step:38294 [D loss: 0.293639, acc.: 88.28%] [G loss: 3.495898]\n",
      "epoch:49 step:38295 [D loss: 0.252394, acc.: 89.06%] [G loss: 3.474005]\n",
      "epoch:49 step:38296 [D loss: 0.242490, acc.: 90.62%] [G loss: 3.201454]\n",
      "epoch:49 step:38297 [D loss: 0.369298, acc.: 81.25%] [G loss: 2.377759]\n",
      "epoch:49 step:38298 [D loss: 0.370568, acc.: 83.59%] [G loss: 3.646004]\n",
      "epoch:49 step:38299 [D loss: 0.270441, acc.: 85.94%] [G loss: 3.097106]\n",
      "epoch:49 step:38300 [D loss: 0.407244, acc.: 82.03%] [G loss: 4.639725]\n",
      "epoch:49 step:38301 [D loss: 0.440350, acc.: 80.47%] [G loss: 5.227063]\n",
      "epoch:49 step:38302 [D loss: 0.462847, acc.: 78.91%] [G loss: 3.425017]\n",
      "epoch:49 step:38303 [D loss: 0.305490, acc.: 89.06%] [G loss: 2.992057]\n",
      "epoch:49 step:38304 [D loss: 0.382349, acc.: 84.38%] [G loss: 3.475573]\n",
      "epoch:49 step:38305 [D loss: 0.343150, acc.: 87.50%] [G loss: 2.930964]\n",
      "epoch:49 step:38306 [D loss: 0.330442, acc.: 85.94%] [G loss: 3.256328]\n",
      "epoch:49 step:38307 [D loss: 0.296010, acc.: 86.72%] [G loss: 3.058907]\n",
      "epoch:49 step:38308 [D loss: 0.289888, acc.: 88.28%] [G loss: 3.524768]\n",
      "epoch:49 step:38309 [D loss: 0.271000, acc.: 88.28%] [G loss: 3.643480]\n",
      "epoch:49 step:38310 [D loss: 0.318955, acc.: 85.16%] [G loss: 2.988474]\n",
      "epoch:49 step:38311 [D loss: 0.282027, acc.: 85.94%] [G loss: 3.142585]\n",
      "epoch:49 step:38312 [D loss: 0.370523, acc.: 82.03%] [G loss: 2.545930]\n",
      "epoch:49 step:38313 [D loss: 0.290808, acc.: 89.84%] [G loss: 3.449081]\n",
      "epoch:49 step:38314 [D loss: 0.222251, acc.: 89.06%] [G loss: 3.286004]\n",
      "epoch:49 step:38315 [D loss: 0.337314, acc.: 85.16%] [G loss: 2.867282]\n",
      "epoch:49 step:38316 [D loss: 0.326272, acc.: 84.38%] [G loss: 3.350083]\n",
      "epoch:49 step:38317 [D loss: 0.247227, acc.: 90.62%] [G loss: 3.487223]\n",
      "epoch:49 step:38318 [D loss: 0.304738, acc.: 88.28%] [G loss: 3.322315]\n",
      "epoch:49 step:38319 [D loss: 0.324446, acc.: 84.38%] [G loss: 4.934549]\n",
      "epoch:49 step:38320 [D loss: 0.271992, acc.: 86.72%] [G loss: 5.308678]\n",
      "epoch:49 step:38321 [D loss: 0.193887, acc.: 92.97%] [G loss: 6.584017]\n",
      "epoch:49 step:38322 [D loss: 0.330787, acc.: 83.59%] [G loss: 3.703858]\n",
      "epoch:49 step:38323 [D loss: 0.222493, acc.: 91.41%] [G loss: 4.278922]\n",
      "epoch:49 step:38324 [D loss: 0.227075, acc.: 92.19%] [G loss: 3.137044]\n",
      "epoch:49 step:38325 [D loss: 0.294955, acc.: 84.38%] [G loss: 2.990750]\n",
      "epoch:49 step:38326 [D loss: 0.304048, acc.: 86.72%] [G loss: 3.321951]\n",
      "epoch:49 step:38327 [D loss: 0.361749, acc.: 85.94%] [G loss: 3.873017]\n",
      "epoch:49 step:38328 [D loss: 0.205559, acc.: 92.19%] [G loss: 3.705915]\n",
      "epoch:49 step:38329 [D loss: 0.351668, acc.: 82.03%] [G loss: 3.271687]\n",
      "epoch:49 step:38330 [D loss: 0.352631, acc.: 81.25%] [G loss: 2.897034]\n",
      "epoch:49 step:38331 [D loss: 0.318995, acc.: 85.16%] [G loss: 3.590344]\n",
      "epoch:49 step:38332 [D loss: 0.252630, acc.: 89.84%] [G loss: 3.448790]\n",
      "epoch:49 step:38333 [D loss: 0.260313, acc.: 85.94%] [G loss: 3.163471]\n",
      "epoch:49 step:38334 [D loss: 0.413924, acc.: 80.47%] [G loss: 3.306171]\n",
      "epoch:49 step:38335 [D loss: 0.209753, acc.: 92.19%] [G loss: 3.742625]\n",
      "epoch:49 step:38336 [D loss: 0.331617, acc.: 87.50%] [G loss: 3.628222]\n",
      "epoch:49 step:38337 [D loss: 0.414355, acc.: 81.25%] [G loss: 3.089154]\n",
      "epoch:49 step:38338 [D loss: 0.273881, acc.: 88.28%] [G loss: 3.951433]\n",
      "epoch:49 step:38339 [D loss: 0.332910, acc.: 82.81%] [G loss: 3.337036]\n",
      "epoch:49 step:38340 [D loss: 0.395020, acc.: 78.91%] [G loss: 3.026868]\n",
      "epoch:49 step:38341 [D loss: 0.313154, acc.: 81.25%] [G loss: 4.171303]\n",
      "epoch:49 step:38342 [D loss: 0.325416, acc.: 85.94%] [G loss: 2.681301]\n",
      "epoch:49 step:38343 [D loss: 0.293130, acc.: 88.28%] [G loss: 3.138604]\n",
      "epoch:49 step:38344 [D loss: 0.339392, acc.: 86.72%] [G loss: 3.113345]\n",
      "epoch:49 step:38345 [D loss: 0.357935, acc.: 85.16%] [G loss: 4.421190]\n",
      "epoch:49 step:38346 [D loss: 0.339685, acc.: 82.03%] [G loss: 2.648568]\n",
      "epoch:49 step:38347 [D loss: 0.261783, acc.: 86.72%] [G loss: 3.677161]\n",
      "epoch:49 step:38348 [D loss: 0.452094, acc.: 74.22%] [G loss: 3.653256]\n",
      "epoch:49 step:38349 [D loss: 0.270558, acc.: 87.50%] [G loss: 3.637262]\n",
      "epoch:49 step:38350 [D loss: 0.277910, acc.: 84.38%] [G loss: 3.979182]\n",
      "epoch:49 step:38351 [D loss: 0.253876, acc.: 89.84%] [G loss: 3.876107]\n",
      "epoch:49 step:38352 [D loss: 0.201465, acc.: 95.31%] [G loss: 3.467846]\n",
      "epoch:49 step:38353 [D loss: 0.276077, acc.: 87.50%] [G loss: 3.163820]\n",
      "epoch:49 step:38354 [D loss: 0.294283, acc.: 84.38%] [G loss: 3.308644]\n",
      "epoch:49 step:38355 [D loss: 0.349601, acc.: 85.16%] [G loss: 3.193000]\n",
      "epoch:49 step:38356 [D loss: 0.368215, acc.: 82.81%] [G loss: 2.851177]\n",
      "epoch:49 step:38357 [D loss: 0.254390, acc.: 89.06%] [G loss: 3.172082]\n",
      "epoch:49 step:38358 [D loss: 0.383759, acc.: 81.25%] [G loss: 3.166941]\n",
      "epoch:49 step:38359 [D loss: 0.400074, acc.: 80.47%] [G loss: 3.382989]\n",
      "epoch:49 step:38360 [D loss: 0.300643, acc.: 85.94%] [G loss: 3.090896]\n",
      "epoch:49 step:38361 [D loss: 0.314351, acc.: 85.94%] [G loss: 4.438876]\n",
      "epoch:49 step:38362 [D loss: 0.383693, acc.: 79.69%] [G loss: 4.227710]\n",
      "epoch:49 step:38363 [D loss: 0.276247, acc.: 90.62%] [G loss: 3.302598]\n",
      "epoch:49 step:38364 [D loss: 0.340733, acc.: 84.38%] [G loss: 3.517522]\n",
      "epoch:49 step:38365 [D loss: 0.300749, acc.: 85.16%] [G loss: 2.521210]\n",
      "epoch:49 step:38366 [D loss: 0.304724, acc.: 87.50%] [G loss: 3.417779]\n",
      "epoch:49 step:38367 [D loss: 0.363411, acc.: 84.38%] [G loss: 3.165929]\n",
      "epoch:49 step:38368 [D loss: 0.348711, acc.: 85.16%] [G loss: 3.652306]\n",
      "epoch:49 step:38369 [D loss: 0.272041, acc.: 87.50%] [G loss: 4.697532]\n",
      "epoch:49 step:38370 [D loss: 0.320442, acc.: 85.94%] [G loss: 2.864534]\n",
      "epoch:49 step:38371 [D loss: 0.250740, acc.: 87.50%] [G loss: 3.581120]\n",
      "epoch:49 step:38372 [D loss: 0.401684, acc.: 79.69%] [G loss: 3.263223]\n",
      "epoch:49 step:38373 [D loss: 0.265636, acc.: 89.84%] [G loss: 2.575200]\n",
      "epoch:49 step:38374 [D loss: 0.347825, acc.: 85.94%] [G loss: 2.374815]\n",
      "epoch:49 step:38375 [D loss: 0.358048, acc.: 85.16%] [G loss: 3.037037]\n",
      "epoch:49 step:38376 [D loss: 0.329761, acc.: 87.50%] [G loss: 2.505010]\n",
      "epoch:49 step:38377 [D loss: 0.288520, acc.: 86.72%] [G loss: 3.346690]\n",
      "epoch:49 step:38378 [D loss: 0.305449, acc.: 85.94%] [G loss: 2.469491]\n",
      "epoch:49 step:38379 [D loss: 0.332425, acc.: 88.28%] [G loss: 2.870449]\n",
      "epoch:49 step:38380 [D loss: 0.299279, acc.: 85.16%] [G loss: 3.158389]\n",
      "epoch:49 step:38381 [D loss: 0.341402, acc.: 84.38%] [G loss: 2.709156]\n",
      "epoch:49 step:38382 [D loss: 0.347064, acc.: 85.16%] [G loss: 2.939996]\n",
      "epoch:49 step:38383 [D loss: 0.437968, acc.: 81.25%] [G loss: 2.649250]\n",
      "epoch:49 step:38384 [D loss: 0.353624, acc.: 84.38%] [G loss: 2.709736]\n",
      "epoch:49 step:38385 [D loss: 0.332723, acc.: 85.16%] [G loss: 2.461535]\n",
      "epoch:49 step:38386 [D loss: 0.242851, acc.: 89.06%] [G loss: 2.943563]\n",
      "epoch:49 step:38387 [D loss: 0.283293, acc.: 86.72%] [G loss: 3.269170]\n",
      "epoch:49 step:38388 [D loss: 0.268189, acc.: 86.72%] [G loss: 2.737499]\n",
      "epoch:49 step:38389 [D loss: 0.269818, acc.: 87.50%] [G loss: 3.270782]\n",
      "epoch:49 step:38390 [D loss: 0.382062, acc.: 80.47%] [G loss: 3.306357]\n",
      "epoch:49 step:38391 [D loss: 0.347545, acc.: 85.16%] [G loss: 3.061448]\n",
      "epoch:49 step:38392 [D loss: 0.260082, acc.: 89.06%] [G loss: 3.443734]\n",
      "epoch:49 step:38393 [D loss: 0.354795, acc.: 82.81%] [G loss: 3.155964]\n",
      "epoch:49 step:38394 [D loss: 0.316613, acc.: 85.94%] [G loss: 3.758265]\n",
      "epoch:49 step:38395 [D loss: 0.277757, acc.: 88.28%] [G loss: 3.175190]\n",
      "epoch:49 step:38396 [D loss: 0.256332, acc.: 89.84%] [G loss: 4.020884]\n",
      "epoch:49 step:38397 [D loss: 0.211679, acc.: 91.41%] [G loss: 3.403987]\n",
      "epoch:49 step:38398 [D loss: 0.234959, acc.: 87.50%] [G loss: 4.330217]\n",
      "epoch:49 step:38399 [D loss: 0.264794, acc.: 88.28%] [G loss: 4.004405]\n",
      "epoch:49 step:38400 [D loss: 0.455007, acc.: 82.03%] [G loss: 4.003777]\n",
      "epoch:49 step:38401 [D loss: 0.338432, acc.: 85.94%] [G loss: 3.329913]\n",
      "epoch:49 step:38402 [D loss: 0.315598, acc.: 86.72%] [G loss: 3.410877]\n",
      "epoch:49 step:38403 [D loss: 0.217894, acc.: 90.62%] [G loss: 4.113241]\n",
      "epoch:49 step:38404 [D loss: 0.356141, acc.: 86.72%] [G loss: 3.880697]\n",
      "epoch:49 step:38405 [D loss: 0.255204, acc.: 87.50%] [G loss: 3.588981]\n",
      "epoch:49 step:38406 [D loss: 0.302703, acc.: 84.38%] [G loss: 3.776984]\n",
      "epoch:49 step:38407 [D loss: 0.294374, acc.: 85.94%] [G loss: 3.703810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38408 [D loss: 0.259113, acc.: 90.62%] [G loss: 4.411992]\n",
      "epoch:49 step:38409 [D loss: 0.204105, acc.: 89.84%] [G loss: 5.451931]\n",
      "epoch:49 step:38410 [D loss: 0.204957, acc.: 89.84%] [G loss: 4.174625]\n",
      "epoch:49 step:38411 [D loss: 0.453785, acc.: 76.56%] [G loss: 3.087642]\n",
      "epoch:49 step:38412 [D loss: 0.244084, acc.: 89.06%] [G loss: 3.269543]\n",
      "epoch:49 step:38413 [D loss: 0.315313, acc.: 82.03%] [G loss: 4.551822]\n",
      "epoch:49 step:38414 [D loss: 0.366546, acc.: 84.38%] [G loss: 3.664328]\n",
      "epoch:49 step:38415 [D loss: 0.318047, acc.: 83.59%] [G loss: 3.662509]\n",
      "epoch:49 step:38416 [D loss: 0.269575, acc.: 88.28%] [G loss: 3.567728]\n",
      "epoch:49 step:38417 [D loss: 0.333615, acc.: 85.94%] [G loss: 3.490412]\n",
      "epoch:49 step:38418 [D loss: 0.284401, acc.: 85.94%] [G loss: 2.851971]\n",
      "epoch:49 step:38419 [D loss: 0.276938, acc.: 88.28%] [G loss: 2.927502]\n",
      "epoch:49 step:38420 [D loss: 0.373426, acc.: 82.03%] [G loss: 3.687275]\n",
      "epoch:49 step:38421 [D loss: 0.314678, acc.: 87.50%] [G loss: 4.051277]\n",
      "epoch:49 step:38422 [D loss: 0.370879, acc.: 82.81%] [G loss: 2.548456]\n",
      "epoch:49 step:38423 [D loss: 0.330975, acc.: 85.94%] [G loss: 4.637122]\n",
      "epoch:49 step:38424 [D loss: 0.346912, acc.: 84.38%] [G loss: 3.368156]\n",
      "epoch:49 step:38425 [D loss: 0.444240, acc.: 83.59%] [G loss: 3.292166]\n",
      "epoch:49 step:38426 [D loss: 0.316642, acc.: 85.94%] [G loss: 3.517285]\n",
      "epoch:49 step:38427 [D loss: 0.284987, acc.: 87.50%] [G loss: 3.448494]\n",
      "epoch:49 step:38428 [D loss: 0.240031, acc.: 89.84%] [G loss: 3.476755]\n",
      "epoch:49 step:38429 [D loss: 0.356770, acc.: 84.38%] [G loss: 3.626295]\n",
      "epoch:49 step:38430 [D loss: 0.385711, acc.: 83.59%] [G loss: 3.055701]\n",
      "epoch:49 step:38431 [D loss: 0.235896, acc.: 91.41%] [G loss: 3.621317]\n",
      "epoch:49 step:38432 [D loss: 0.192792, acc.: 92.97%] [G loss: 3.360299]\n",
      "epoch:49 step:38433 [D loss: 0.293484, acc.: 86.72%] [G loss: 4.152174]\n",
      "epoch:49 step:38434 [D loss: 0.221931, acc.: 89.06%] [G loss: 3.379334]\n",
      "epoch:49 step:38435 [D loss: 0.289344, acc.: 88.28%] [G loss: 3.249328]\n",
      "epoch:49 step:38436 [D loss: 0.242434, acc.: 89.84%] [G loss: 3.566284]\n",
      "epoch:49 step:38437 [D loss: 0.365490, acc.: 83.59%] [G loss: 3.002807]\n",
      "epoch:49 step:38438 [D loss: 0.227117, acc.: 89.06%] [G loss: 3.223700]\n",
      "epoch:49 step:38439 [D loss: 0.296285, acc.: 87.50%] [G loss: 3.185829]\n",
      "epoch:49 step:38440 [D loss: 0.379475, acc.: 80.47%] [G loss: 3.562971]\n",
      "epoch:49 step:38441 [D loss: 0.247333, acc.: 89.84%] [G loss: 3.508823]\n",
      "epoch:49 step:38442 [D loss: 0.357822, acc.: 86.72%] [G loss: 3.536440]\n",
      "epoch:49 step:38443 [D loss: 0.380055, acc.: 82.81%] [G loss: 4.168719]\n",
      "epoch:49 step:38444 [D loss: 0.323509, acc.: 85.16%] [G loss: 4.704756]\n",
      "epoch:49 step:38445 [D loss: 0.311473, acc.: 86.72%] [G loss: 3.498187]\n",
      "epoch:49 step:38446 [D loss: 0.195811, acc.: 92.97%] [G loss: 2.718748]\n",
      "epoch:49 step:38447 [D loss: 0.300937, acc.: 83.59%] [G loss: 3.789177]\n",
      "epoch:49 step:38448 [D loss: 0.215914, acc.: 91.41%] [G loss: 3.913654]\n",
      "epoch:49 step:38449 [D loss: 0.334500, acc.: 85.94%] [G loss: 2.941855]\n",
      "epoch:49 step:38450 [D loss: 0.204614, acc.: 91.41%] [G loss: 3.530250]\n",
      "epoch:49 step:38451 [D loss: 0.304457, acc.: 86.72%] [G loss: 3.350347]\n",
      "epoch:49 step:38452 [D loss: 0.325891, acc.: 83.59%] [G loss: 3.055768]\n",
      "epoch:49 step:38453 [D loss: 0.348534, acc.: 83.59%] [G loss: 2.929464]\n",
      "epoch:49 step:38454 [D loss: 0.269770, acc.: 88.28%] [G loss: 2.597086]\n",
      "epoch:49 step:38455 [D loss: 0.284559, acc.: 86.72%] [G loss: 3.132234]\n",
      "epoch:49 step:38456 [D loss: 0.431894, acc.: 78.91%] [G loss: 3.037539]\n",
      "epoch:49 step:38457 [D loss: 0.379280, acc.: 84.38%] [G loss: 2.935990]\n",
      "epoch:49 step:38458 [D loss: 0.309038, acc.: 86.72%] [G loss: 3.083758]\n",
      "epoch:49 step:38459 [D loss: 0.348211, acc.: 87.50%] [G loss: 2.498878]\n",
      "epoch:49 step:38460 [D loss: 0.358046, acc.: 82.03%] [G loss: 2.577511]\n",
      "epoch:49 step:38461 [D loss: 0.376577, acc.: 83.59%] [G loss: 2.471368]\n",
      "epoch:49 step:38462 [D loss: 0.366518, acc.: 82.81%] [G loss: 2.734090]\n",
      "epoch:49 step:38463 [D loss: 0.298881, acc.: 85.94%] [G loss: 3.077969]\n",
      "epoch:49 step:38464 [D loss: 0.435385, acc.: 80.47%] [G loss: 3.077503]\n",
      "epoch:49 step:38465 [D loss: 0.319108, acc.: 85.94%] [G loss: 4.050750]\n",
      "epoch:49 step:38466 [D loss: 0.396886, acc.: 85.16%] [G loss: 3.165518]\n",
      "epoch:49 step:38467 [D loss: 0.413807, acc.: 77.34%] [G loss: 4.928229]\n",
      "epoch:49 step:38468 [D loss: 0.283861, acc.: 88.28%] [G loss: 3.159362]\n",
      "epoch:49 step:38469 [D loss: 0.271848, acc.: 85.94%] [G loss: 4.868011]\n",
      "epoch:49 step:38470 [D loss: 0.242870, acc.: 90.62%] [G loss: 3.689705]\n",
      "epoch:49 step:38471 [D loss: 0.243408, acc.: 89.84%] [G loss: 4.017283]\n",
      "epoch:49 step:38472 [D loss: 0.239796, acc.: 89.84%] [G loss: 3.479278]\n",
      "epoch:49 step:38473 [D loss: 0.276422, acc.: 87.50%] [G loss: 3.998845]\n",
      "epoch:49 step:38474 [D loss: 0.359145, acc.: 87.50%] [G loss: 2.673123]\n",
      "epoch:49 step:38475 [D loss: 0.382210, acc.: 82.81%] [G loss: 3.439361]\n",
      "epoch:49 step:38476 [D loss: 0.323413, acc.: 86.72%] [G loss: 3.597569]\n",
      "epoch:49 step:38477 [D loss: 0.295082, acc.: 85.94%] [G loss: 5.564798]\n",
      "epoch:49 step:38478 [D loss: 0.294035, acc.: 85.94%] [G loss: 5.862270]\n",
      "epoch:49 step:38479 [D loss: 0.275879, acc.: 87.50%] [G loss: 4.554576]\n",
      "epoch:49 step:38480 [D loss: 0.359416, acc.: 81.25%] [G loss: 3.297010]\n",
      "epoch:49 step:38481 [D loss: 0.293673, acc.: 88.28%] [G loss: 3.333628]\n",
      "epoch:49 step:38482 [D loss: 0.338980, acc.: 85.16%] [G loss: 4.302004]\n",
      "epoch:49 step:38483 [D loss: 0.267760, acc.: 89.06%] [G loss: 3.969273]\n",
      "epoch:49 step:38484 [D loss: 0.241665, acc.: 90.62%] [G loss: 4.888607]\n",
      "epoch:49 step:38485 [D loss: 0.268616, acc.: 87.50%] [G loss: 4.086038]\n",
      "epoch:49 step:38486 [D loss: 0.247317, acc.: 89.84%] [G loss: 2.881882]\n",
      "epoch:49 step:38487 [D loss: 0.294236, acc.: 86.72%] [G loss: 3.792370]\n",
      "epoch:49 step:38488 [D loss: 0.293337, acc.: 85.16%] [G loss: 2.968168]\n",
      "epoch:49 step:38489 [D loss: 0.219668, acc.: 90.62%] [G loss: 3.697555]\n",
      "epoch:49 step:38490 [D loss: 0.271882, acc.: 86.72%] [G loss: 3.982380]\n",
      "epoch:49 step:38491 [D loss: 0.311952, acc.: 88.28%] [G loss: 3.332047]\n",
      "epoch:49 step:38492 [D loss: 0.371849, acc.: 78.91%] [G loss: 3.521477]\n",
      "epoch:49 step:38493 [D loss: 0.307678, acc.: 87.50%] [G loss: 3.333424]\n",
      "epoch:49 step:38494 [D loss: 0.325984, acc.: 85.94%] [G loss: 3.035948]\n",
      "epoch:49 step:38495 [D loss: 0.295739, acc.: 87.50%] [G loss: 3.397032]\n",
      "epoch:49 step:38496 [D loss: 0.308768, acc.: 84.38%] [G loss: 3.076934]\n",
      "epoch:49 step:38497 [D loss: 0.426301, acc.: 78.12%] [G loss: 4.364660]\n",
      "epoch:49 step:38498 [D loss: 0.342354, acc.: 85.94%] [G loss: 3.514624]\n",
      "epoch:49 step:38499 [D loss: 0.319242, acc.: 80.47%] [G loss: 2.842284]\n",
      "epoch:49 step:38500 [D loss: 0.179953, acc.: 92.97%] [G loss: 3.529782]\n",
      "epoch:49 step:38501 [D loss: 0.210065, acc.: 88.28%] [G loss: 4.410210]\n",
      "epoch:49 step:38502 [D loss: 0.267858, acc.: 88.28%] [G loss: 3.528480]\n",
      "epoch:49 step:38503 [D loss: 0.264696, acc.: 85.94%] [G loss: 3.346996]\n",
      "epoch:49 step:38504 [D loss: 0.305057, acc.: 86.72%] [G loss: 4.216448]\n",
      "epoch:49 step:38505 [D loss: 0.187083, acc.: 92.97%] [G loss: 4.419563]\n",
      "epoch:49 step:38506 [D loss: 0.214385, acc.: 92.97%] [G loss: 3.719809]\n",
      "epoch:49 step:38507 [D loss: 0.266291, acc.: 88.28%] [G loss: 4.653483]\n",
      "epoch:49 step:38508 [D loss: 0.263229, acc.: 88.28%] [G loss: 4.122163]\n",
      "epoch:49 step:38509 [D loss: 0.314810, acc.: 83.59%] [G loss: 3.779541]\n",
      "epoch:49 step:38510 [D loss: 0.281845, acc.: 88.28%] [G loss: 3.382385]\n",
      "epoch:49 step:38511 [D loss: 0.303558, acc.: 86.72%] [G loss: 3.006135]\n",
      "epoch:49 step:38512 [D loss: 0.331034, acc.: 82.81%] [G loss: 3.321030]\n",
      "epoch:49 step:38513 [D loss: 0.268905, acc.: 89.06%] [G loss: 3.920908]\n",
      "epoch:49 step:38514 [D loss: 0.326200, acc.: 85.94%] [G loss: 3.348785]\n",
      "epoch:49 step:38515 [D loss: 0.277134, acc.: 88.28%] [G loss: 3.128752]\n",
      "epoch:49 step:38516 [D loss: 0.286011, acc.: 86.72%] [G loss: 4.641934]\n",
      "epoch:49 step:38517 [D loss: 0.474353, acc.: 78.12%] [G loss: 3.199697]\n",
      "epoch:49 step:38518 [D loss: 0.362293, acc.: 82.81%] [G loss: 3.516519]\n",
      "epoch:49 step:38519 [D loss: 0.349822, acc.: 85.16%] [G loss: 3.397213]\n",
      "epoch:49 step:38520 [D loss: 0.216240, acc.: 91.41%] [G loss: 3.217124]\n",
      "epoch:49 step:38521 [D loss: 0.401914, acc.: 83.59%] [G loss: 3.715757]\n",
      "epoch:49 step:38522 [D loss: 0.378491, acc.: 83.59%] [G loss: 2.949262]\n",
      "epoch:49 step:38523 [D loss: 0.254506, acc.: 89.84%] [G loss: 3.846895]\n",
      "epoch:49 step:38524 [D loss: 0.264974, acc.: 87.50%] [G loss: 3.156410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38525 [D loss: 0.313283, acc.: 85.16%] [G loss: 3.105251]\n",
      "epoch:49 step:38526 [D loss: 0.307196, acc.: 85.16%] [G loss: 3.486682]\n",
      "epoch:49 step:38527 [D loss: 0.278784, acc.: 91.41%] [G loss: 3.104979]\n",
      "epoch:49 step:38528 [D loss: 0.278955, acc.: 85.94%] [G loss: 3.016917]\n",
      "epoch:49 step:38529 [D loss: 0.396169, acc.: 81.25%] [G loss: 3.229456]\n",
      "epoch:49 step:38530 [D loss: 0.398672, acc.: 84.38%] [G loss: 2.872988]\n",
      "epoch:49 step:38531 [D loss: 0.364569, acc.: 87.50%] [G loss: 3.219414]\n",
      "epoch:49 step:38532 [D loss: 0.370527, acc.: 87.50%] [G loss: 4.080089]\n",
      "epoch:49 step:38533 [D loss: 0.302106, acc.: 85.16%] [G loss: 3.599767]\n",
      "epoch:49 step:38534 [D loss: 0.363183, acc.: 86.72%] [G loss: 2.800434]\n",
      "epoch:49 step:38535 [D loss: 0.290192, acc.: 87.50%] [G loss: 3.623133]\n",
      "epoch:49 step:38536 [D loss: 0.271943, acc.: 89.84%] [G loss: 3.469874]\n",
      "epoch:49 step:38537 [D loss: 0.233527, acc.: 90.62%] [G loss: 3.217194]\n",
      "epoch:49 step:38538 [D loss: 0.212833, acc.: 89.06%] [G loss: 3.855067]\n",
      "epoch:49 step:38539 [D loss: 0.317494, acc.: 84.38%] [G loss: 2.928547]\n",
      "epoch:49 step:38540 [D loss: 0.202098, acc.: 91.41%] [G loss: 3.713067]\n",
      "epoch:49 step:38541 [D loss: 0.296238, acc.: 85.94%] [G loss: 3.634108]\n",
      "epoch:49 step:38542 [D loss: 0.278715, acc.: 85.16%] [G loss: 2.768737]\n",
      "epoch:49 step:38543 [D loss: 0.326272, acc.: 84.38%] [G loss: 2.529729]\n",
      "epoch:49 step:38544 [D loss: 0.285808, acc.: 85.16%] [G loss: 2.867173]\n",
      "epoch:49 step:38545 [D loss: 0.277486, acc.: 87.50%] [G loss: 3.867871]\n",
      "epoch:49 step:38546 [D loss: 0.351563, acc.: 85.94%] [G loss: 4.369592]\n",
      "epoch:49 step:38547 [D loss: 0.418887, acc.: 84.38%] [G loss: 3.340603]\n",
      "epoch:49 step:38548 [D loss: 0.520922, acc.: 78.91%] [G loss: 3.434462]\n",
      "epoch:49 step:38549 [D loss: 0.292174, acc.: 89.84%] [G loss: 3.326959]\n",
      "epoch:49 step:38550 [D loss: 0.334980, acc.: 85.16%] [G loss: 4.312901]\n",
      "epoch:49 step:38551 [D loss: 0.297231, acc.: 86.72%] [G loss: 4.215840]\n",
      "epoch:49 step:38552 [D loss: 0.250339, acc.: 90.62%] [G loss: 3.929720]\n",
      "epoch:49 step:38553 [D loss: 0.246627, acc.: 90.62%] [G loss: 3.519487]\n",
      "epoch:49 step:38554 [D loss: 0.396900, acc.: 82.03%] [G loss: 2.385296]\n",
      "epoch:49 step:38555 [D loss: 0.288386, acc.: 86.72%] [G loss: 2.908550]\n",
      "epoch:49 step:38556 [D loss: 0.273218, acc.: 91.41%] [G loss: 3.478870]\n",
      "epoch:49 step:38557 [D loss: 0.279732, acc.: 89.84%] [G loss: 3.545451]\n",
      "epoch:49 step:38558 [D loss: 0.289149, acc.: 85.94%] [G loss: 3.592346]\n",
      "epoch:49 step:38559 [D loss: 0.356707, acc.: 81.25%] [G loss: 3.550948]\n",
      "epoch:49 step:38560 [D loss: 0.266182, acc.: 87.50%] [G loss: 4.050800]\n",
      "epoch:49 step:38561 [D loss: 0.289641, acc.: 86.72%] [G loss: 3.262159]\n",
      "epoch:49 step:38562 [D loss: 0.366511, acc.: 81.25%] [G loss: 4.068782]\n",
      "epoch:49 step:38563 [D loss: 0.243054, acc.: 87.50%] [G loss: 4.963405]\n",
      "epoch:49 step:38564 [D loss: 0.228317, acc.: 90.62%] [G loss: 5.258329]\n",
      "epoch:49 step:38565 [D loss: 0.272081, acc.: 85.16%] [G loss: 4.259385]\n",
      "epoch:49 step:38566 [D loss: 0.307092, acc.: 86.72%] [G loss: 2.936366]\n",
      "epoch:49 step:38567 [D loss: 0.306435, acc.: 88.28%] [G loss: 4.201815]\n",
      "epoch:49 step:38568 [D loss: 0.254075, acc.: 90.62%] [G loss: 4.640427]\n",
      "epoch:49 step:38569 [D loss: 0.457609, acc.: 79.69%] [G loss: 4.477942]\n",
      "epoch:49 step:38570 [D loss: 0.436296, acc.: 77.34%] [G loss: 9.686935]\n",
      "epoch:49 step:38571 [D loss: 0.989412, acc.: 75.78%] [G loss: 8.914032]\n",
      "epoch:49 step:38572 [D loss: 1.574075, acc.: 68.75%] [G loss: 6.255206]\n",
      "epoch:49 step:38573 [D loss: 1.653683, acc.: 61.72%] [G loss: 7.006643]\n",
      "epoch:49 step:38574 [D loss: 0.579256, acc.: 79.69%] [G loss: 5.761418]\n",
      "epoch:49 step:38575 [D loss: 0.525757, acc.: 81.25%] [G loss: 6.111130]\n",
      "epoch:49 step:38576 [D loss: 0.443948, acc.: 82.03%] [G loss: 4.055837]\n",
      "epoch:49 step:38577 [D loss: 0.323087, acc.: 85.16%] [G loss: 3.679288]\n",
      "epoch:49 step:38578 [D loss: 0.318800, acc.: 88.28%] [G loss: 4.649733]\n",
      "epoch:49 step:38579 [D loss: 0.298420, acc.: 88.28%] [G loss: 2.992757]\n",
      "epoch:49 step:38580 [D loss: 0.484179, acc.: 81.25%] [G loss: 3.923738]\n",
      "epoch:49 step:38581 [D loss: 0.383358, acc.: 82.03%] [G loss: 3.152790]\n",
      "epoch:49 step:38582 [D loss: 0.375463, acc.: 82.81%] [G loss: 3.042484]\n",
      "epoch:49 step:38583 [D loss: 0.345260, acc.: 84.38%] [G loss: 2.908237]\n",
      "epoch:49 step:38584 [D loss: 0.612581, acc.: 67.97%] [G loss: 2.613823]\n",
      "epoch:49 step:38585 [D loss: 0.300869, acc.: 85.16%] [G loss: 3.006882]\n",
      "epoch:49 step:38586 [D loss: 0.362591, acc.: 83.59%] [G loss: 2.710474]\n",
      "epoch:49 step:38587 [D loss: 0.392038, acc.: 82.03%] [G loss: 2.285927]\n",
      "epoch:49 step:38588 [D loss: 0.320516, acc.: 85.16%] [G loss: 2.977051]\n",
      "epoch:49 step:38589 [D loss: 0.400276, acc.: 78.91%] [G loss: 2.813883]\n",
      "epoch:49 step:38590 [D loss: 0.328905, acc.: 85.16%] [G loss: 3.270314]\n",
      "epoch:49 step:38591 [D loss: 0.421170, acc.: 80.47%] [G loss: 2.201606]\n",
      "epoch:49 step:38592 [D loss: 0.263836, acc.: 85.94%] [G loss: 3.386813]\n",
      "epoch:49 step:38593 [D loss: 0.322158, acc.: 86.72%] [G loss: 2.520473]\n",
      "epoch:49 step:38594 [D loss: 0.266824, acc.: 89.06%] [G loss: 3.166456]\n",
      "epoch:49 step:38595 [D loss: 0.315169, acc.: 84.38%] [G loss: 2.210822]\n",
      "epoch:49 step:38596 [D loss: 0.353100, acc.: 84.38%] [G loss: 3.501245]\n",
      "epoch:49 step:38597 [D loss: 0.291812, acc.: 89.06%] [G loss: 3.647148]\n",
      "epoch:49 step:38598 [D loss: 0.362032, acc.: 84.38%] [G loss: 3.124472]\n",
      "epoch:49 step:38599 [D loss: 0.234833, acc.: 89.84%] [G loss: 4.380193]\n",
      "epoch:49 step:38600 [D loss: 0.432875, acc.: 80.47%] [G loss: 3.196913]\n",
      "epoch:49 step:38601 [D loss: 0.257981, acc.: 88.28%] [G loss: 2.739014]\n",
      "epoch:49 step:38602 [D loss: 0.253865, acc.: 84.38%] [G loss: 2.966430]\n",
      "epoch:49 step:38603 [D loss: 0.448909, acc.: 76.56%] [G loss: 2.521039]\n",
      "epoch:49 step:38604 [D loss: 0.287561, acc.: 85.94%] [G loss: 2.718906]\n",
      "epoch:49 step:38605 [D loss: 0.355152, acc.: 87.50%] [G loss: 3.399420]\n",
      "epoch:49 step:38606 [D loss: 0.258403, acc.: 87.50%] [G loss: 3.045452]\n",
      "epoch:49 step:38607 [D loss: 0.232398, acc.: 91.41%] [G loss: 2.648954]\n",
      "epoch:49 step:38608 [D loss: 0.254252, acc.: 89.84%] [G loss: 2.612942]\n",
      "epoch:49 step:38609 [D loss: 0.306610, acc.: 85.94%] [G loss: 2.482457]\n",
      "epoch:49 step:38610 [D loss: 0.321129, acc.: 86.72%] [G loss: 3.165388]\n",
      "epoch:49 step:38611 [D loss: 0.312997, acc.: 83.59%] [G loss: 2.607216]\n",
      "epoch:49 step:38612 [D loss: 0.323032, acc.: 83.59%] [G loss: 2.712178]\n",
      "epoch:49 step:38613 [D loss: 0.368923, acc.: 84.38%] [G loss: 2.486544]\n",
      "epoch:49 step:38614 [D loss: 0.281151, acc.: 87.50%] [G loss: 3.717411]\n",
      "epoch:49 step:38615 [D loss: 0.425133, acc.: 84.38%] [G loss: 3.025437]\n",
      "epoch:49 step:38616 [D loss: 0.315236, acc.: 88.28%] [G loss: 3.590491]\n",
      "epoch:49 step:38617 [D loss: 0.308300, acc.: 87.50%] [G loss: 3.542876]\n",
      "epoch:49 step:38618 [D loss: 0.221679, acc.: 92.97%] [G loss: 3.422469]\n",
      "epoch:49 step:38619 [D loss: 0.350657, acc.: 85.94%] [G loss: 2.008501]\n",
      "epoch:49 step:38620 [D loss: 0.375281, acc.: 85.16%] [G loss: 2.728221]\n",
      "epoch:49 step:38621 [D loss: 0.370451, acc.: 81.25%] [G loss: 2.628062]\n",
      "epoch:49 step:38622 [D loss: 0.341625, acc.: 83.59%] [G loss: 3.090716]\n",
      "epoch:49 step:38623 [D loss: 0.206438, acc.: 90.62%] [G loss: 3.091640]\n",
      "epoch:49 step:38624 [D loss: 0.238334, acc.: 89.84%] [G loss: 3.233687]\n",
      "epoch:49 step:38625 [D loss: 0.312113, acc.: 87.50%] [G loss: 2.846667]\n",
      "epoch:49 step:38626 [D loss: 0.268740, acc.: 88.28%] [G loss: 3.098960]\n",
      "epoch:49 step:38627 [D loss: 0.267037, acc.: 88.28%] [G loss: 3.127458]\n",
      "epoch:49 step:38628 [D loss: 0.313285, acc.: 85.94%] [G loss: 2.914835]\n",
      "epoch:49 step:38629 [D loss: 0.419167, acc.: 79.69%] [G loss: 2.936341]\n",
      "epoch:49 step:38630 [D loss: 0.312601, acc.: 85.94%] [G loss: 3.028871]\n",
      "epoch:49 step:38631 [D loss: 0.284304, acc.: 88.28%] [G loss: 3.091844]\n",
      "epoch:49 step:38632 [D loss: 0.307858, acc.: 87.50%] [G loss: 3.372519]\n",
      "epoch:49 step:38633 [D loss: 0.294604, acc.: 89.84%] [G loss: 2.781119]\n",
      "epoch:49 step:38634 [D loss: 0.290174, acc.: 85.94%] [G loss: 2.458955]\n",
      "epoch:49 step:38635 [D loss: 0.316192, acc.: 85.94%] [G loss: 2.727969]\n",
      "epoch:49 step:38636 [D loss: 0.366441, acc.: 85.16%] [G loss: 2.840194]\n",
      "epoch:49 step:38637 [D loss: 0.319287, acc.: 84.38%] [G loss: 2.686952]\n",
      "epoch:49 step:38638 [D loss: 0.328848, acc.: 88.28%] [G loss: 2.494223]\n",
      "epoch:49 step:38639 [D loss: 0.261557, acc.: 92.97%] [G loss: 2.516116]\n",
      "epoch:49 step:38640 [D loss: 0.253501, acc.: 86.72%] [G loss: 3.387580]\n",
      "epoch:49 step:38641 [D loss: 0.287726, acc.: 88.28%] [G loss: 3.046120]\n",
      "epoch:49 step:38642 [D loss: 0.207664, acc.: 91.41%] [G loss: 3.105510]\n",
      "epoch:49 step:38643 [D loss: 0.323799, acc.: 83.59%] [G loss: 2.463266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38644 [D loss: 0.217739, acc.: 92.19%] [G loss: 5.162095]\n",
      "epoch:49 step:38645 [D loss: 0.263408, acc.: 89.06%] [G loss: 4.751713]\n",
      "epoch:49 step:38646 [D loss: 0.186407, acc.: 89.84%] [G loss: 4.530573]\n",
      "epoch:49 step:38647 [D loss: 0.231670, acc.: 91.41%] [G loss: 3.896189]\n",
      "epoch:49 step:38648 [D loss: 0.310189, acc.: 84.38%] [G loss: 2.448632]\n",
      "epoch:49 step:38649 [D loss: 0.282807, acc.: 88.28%] [G loss: 3.673183]\n",
      "epoch:49 step:38650 [D loss: 0.221377, acc.: 89.84%] [G loss: 3.249962]\n",
      "epoch:49 step:38651 [D loss: 0.391969, acc.: 80.47%] [G loss: 2.879026]\n",
      "epoch:49 step:38652 [D loss: 0.288166, acc.: 89.84%] [G loss: 2.878206]\n",
      "epoch:49 step:38653 [D loss: 0.270741, acc.: 86.72%] [G loss: 2.855588]\n",
      "epoch:49 step:38654 [D loss: 0.206122, acc.: 91.41%] [G loss: 2.667804]\n",
      "epoch:49 step:38655 [D loss: 0.424846, acc.: 78.91%] [G loss: 2.730810]\n",
      "epoch:49 step:38656 [D loss: 0.316129, acc.: 85.16%] [G loss: 2.774994]\n",
      "epoch:49 step:38657 [D loss: 0.314251, acc.: 85.16%] [G loss: 2.826321]\n",
      "epoch:49 step:38658 [D loss: 0.331452, acc.: 85.94%] [G loss: 2.518010]\n",
      "epoch:49 step:38659 [D loss: 0.333277, acc.: 80.47%] [G loss: 2.064376]\n",
      "epoch:49 step:38660 [D loss: 0.356956, acc.: 83.59%] [G loss: 2.658499]\n",
      "epoch:49 step:38661 [D loss: 0.231444, acc.: 90.62%] [G loss: 2.481041]\n",
      "epoch:49 step:38662 [D loss: 0.374426, acc.: 80.47%] [G loss: 2.337792]\n",
      "epoch:49 step:38663 [D loss: 0.404373, acc.: 85.16%] [G loss: 2.638064]\n",
      "epoch:49 step:38664 [D loss: 0.322695, acc.: 86.72%] [G loss: 2.938935]\n",
      "epoch:49 step:38665 [D loss: 0.314729, acc.: 88.28%] [G loss: 2.860451]\n",
      "epoch:49 step:38666 [D loss: 0.308011, acc.: 88.28%] [G loss: 2.574068]\n",
      "epoch:49 step:38667 [D loss: 0.331954, acc.: 86.72%] [G loss: 2.771824]\n",
      "epoch:49 step:38668 [D loss: 0.319229, acc.: 85.16%] [G loss: 3.621120]\n",
      "epoch:49 step:38669 [D loss: 0.320986, acc.: 85.94%] [G loss: 3.351216]\n",
      "epoch:49 step:38670 [D loss: 0.279730, acc.: 87.50%] [G loss: 2.755847]\n",
      "epoch:49 step:38671 [D loss: 0.283153, acc.: 88.28%] [G loss: 2.900262]\n",
      "epoch:49 step:38672 [D loss: 0.233985, acc.: 89.84%] [G loss: 2.923205]\n",
      "epoch:49 step:38673 [D loss: 0.365246, acc.: 85.16%] [G loss: 4.456135]\n",
      "epoch:49 step:38674 [D loss: 0.272003, acc.: 89.84%] [G loss: 2.765984]\n",
      "epoch:49 step:38675 [D loss: 0.288560, acc.: 86.72%] [G loss: 3.413332]\n",
      "epoch:49 step:38676 [D loss: 0.287456, acc.: 87.50%] [G loss: 2.305013]\n",
      "epoch:49 step:38677 [D loss: 0.311597, acc.: 82.81%] [G loss: 2.708743]\n",
      "epoch:49 step:38678 [D loss: 0.250753, acc.: 89.06%] [G loss: 3.417401]\n",
      "epoch:49 step:38679 [D loss: 0.289056, acc.: 85.16%] [G loss: 3.200881]\n",
      "epoch:49 step:38680 [D loss: 0.342988, acc.: 85.16%] [G loss: 2.906482]\n",
      "epoch:49 step:38681 [D loss: 0.233687, acc.: 89.06%] [G loss: 3.331981]\n",
      "epoch:49 step:38682 [D loss: 0.321892, acc.: 82.81%] [G loss: 2.760237]\n",
      "epoch:49 step:38683 [D loss: 0.327769, acc.: 84.38%] [G loss: 2.215694]\n",
      "epoch:49 step:38684 [D loss: 0.263270, acc.: 90.62%] [G loss: 2.861182]\n",
      "epoch:49 step:38685 [D loss: 0.278955, acc.: 86.72%] [G loss: 2.396519]\n",
      "epoch:49 step:38686 [D loss: 0.291343, acc.: 90.62%] [G loss: 2.963499]\n",
      "epoch:49 step:38687 [D loss: 0.305844, acc.: 85.94%] [G loss: 3.573452]\n",
      "epoch:49 step:38688 [D loss: 0.253631, acc.: 91.41%] [G loss: 3.288722]\n",
      "epoch:49 step:38689 [D loss: 0.335551, acc.: 87.50%] [G loss: 3.454483]\n",
      "epoch:49 step:38690 [D loss: 0.288868, acc.: 85.16%] [G loss: 3.495719]\n",
      "epoch:49 step:38691 [D loss: 0.371564, acc.: 81.25%] [G loss: 2.944606]\n",
      "epoch:49 step:38692 [D loss: 0.243336, acc.: 91.41%] [G loss: 2.995335]\n",
      "epoch:49 step:38693 [D loss: 0.379923, acc.: 81.25%] [G loss: 2.687368]\n",
      "epoch:49 step:38694 [D loss: 0.282069, acc.: 88.28%] [G loss: 3.388573]\n",
      "epoch:49 step:38695 [D loss: 0.327405, acc.: 85.16%] [G loss: 2.915657]\n",
      "epoch:49 step:38696 [D loss: 0.224425, acc.: 90.62%] [G loss: 3.181155]\n",
      "epoch:49 step:38697 [D loss: 0.463069, acc.: 76.56%] [G loss: 5.158887]\n",
      "epoch:49 step:38698 [D loss: 0.506178, acc.: 76.56%] [G loss: 4.939356]\n",
      "epoch:49 step:38699 [D loss: 0.353809, acc.: 85.16%] [G loss: 5.786715]\n",
      "epoch:49 step:38700 [D loss: 0.397567, acc.: 85.16%] [G loss: 3.978938]\n",
      "epoch:49 step:38701 [D loss: 0.374527, acc.: 85.94%] [G loss: 5.826484]\n",
      "epoch:49 step:38702 [D loss: 0.323481, acc.: 86.72%] [G loss: 3.691744]\n",
      "epoch:49 step:38703 [D loss: 0.187851, acc.: 94.53%] [G loss: 5.915872]\n",
      "epoch:49 step:38704 [D loss: 0.379981, acc.: 86.72%] [G loss: 3.309593]\n",
      "epoch:49 step:38705 [D loss: 0.241472, acc.: 88.28%] [G loss: 2.690113]\n",
      "epoch:49 step:38706 [D loss: 0.210765, acc.: 92.19%] [G loss: 3.404600]\n",
      "epoch:49 step:38707 [D loss: 0.305885, acc.: 89.06%] [G loss: 3.875134]\n",
      "epoch:49 step:38708 [D loss: 0.378177, acc.: 76.56%] [G loss: 2.766729]\n",
      "epoch:49 step:38709 [D loss: 0.206836, acc.: 91.41%] [G loss: 4.057674]\n",
      "epoch:49 step:38710 [D loss: 0.310134, acc.: 87.50%] [G loss: 3.796918]\n",
      "epoch:49 step:38711 [D loss: 0.327843, acc.: 87.50%] [G loss: 3.376731]\n",
      "epoch:49 step:38712 [D loss: 0.276151, acc.: 83.59%] [G loss: 4.256052]\n",
      "epoch:49 step:38713 [D loss: 0.401231, acc.: 82.81%] [G loss: 2.933654]\n",
      "epoch:49 step:38714 [D loss: 0.331003, acc.: 83.59%] [G loss: 2.761694]\n",
      "epoch:49 step:38715 [D loss: 0.240879, acc.: 89.84%] [G loss: 2.808369]\n",
      "epoch:49 step:38716 [D loss: 0.259765, acc.: 92.19%] [G loss: 2.661809]\n",
      "epoch:49 step:38717 [D loss: 0.359847, acc.: 82.81%] [G loss: 3.665225]\n",
      "epoch:49 step:38718 [D loss: 0.317786, acc.: 86.72%] [G loss: 3.146697]\n",
      "epoch:49 step:38719 [D loss: 0.340727, acc.: 85.94%] [G loss: 2.787156]\n",
      "epoch:49 step:38720 [D loss: 0.235982, acc.: 89.06%] [G loss: 3.174449]\n",
      "epoch:49 step:38721 [D loss: 0.310471, acc.: 85.94%] [G loss: 3.976910]\n",
      "epoch:49 step:38722 [D loss: 0.197772, acc.: 92.97%] [G loss: 3.598137]\n",
      "epoch:49 step:38723 [D loss: 0.227228, acc.: 90.62%] [G loss: 3.258313]\n",
      "epoch:49 step:38724 [D loss: 0.270034, acc.: 92.97%] [G loss: 3.364933]\n",
      "epoch:49 step:38725 [D loss: 0.259379, acc.: 90.62%] [G loss: 3.209846]\n",
      "epoch:49 step:38726 [D loss: 0.277338, acc.: 90.62%] [G loss: 3.040532]\n",
      "epoch:49 step:38727 [D loss: 0.258823, acc.: 88.28%] [G loss: 3.220769]\n",
      "epoch:49 step:38728 [D loss: 0.315651, acc.: 85.94%] [G loss: 3.256967]\n",
      "epoch:49 step:38729 [D loss: 0.321498, acc.: 85.16%] [G loss: 3.060343]\n",
      "epoch:49 step:38730 [D loss: 0.349245, acc.: 84.38%] [G loss: 3.084872]\n",
      "epoch:49 step:38731 [D loss: 0.245766, acc.: 90.62%] [G loss: 2.951675]\n",
      "epoch:49 step:38732 [D loss: 0.319398, acc.: 85.16%] [G loss: 2.832545]\n",
      "epoch:49 step:38733 [D loss: 0.307536, acc.: 84.38%] [G loss: 4.141922]\n",
      "epoch:49 step:38734 [D loss: 0.347634, acc.: 85.94%] [G loss: 3.443815]\n",
      "epoch:49 step:38735 [D loss: 0.309663, acc.: 88.28%] [G loss: 3.551281]\n",
      "epoch:49 step:38736 [D loss: 0.214213, acc.: 91.41%] [G loss: 4.310416]\n",
      "epoch:49 step:38737 [D loss: 0.299416, acc.: 85.16%] [G loss: 3.860780]\n",
      "epoch:49 step:38738 [D loss: 0.247814, acc.: 89.84%] [G loss: 5.005713]\n",
      "epoch:49 step:38739 [D loss: 0.200238, acc.: 92.19%] [G loss: 4.283336]\n",
      "epoch:49 step:38740 [D loss: 0.214474, acc.: 90.62%] [G loss: 4.145360]\n",
      "epoch:49 step:38741 [D loss: 0.312399, acc.: 86.72%] [G loss: 3.309085]\n",
      "epoch:49 step:38742 [D loss: 0.283599, acc.: 86.72%] [G loss: 3.582807]\n",
      "epoch:49 step:38743 [D loss: 0.335790, acc.: 82.03%] [G loss: 3.644546]\n",
      "epoch:49 step:38744 [D loss: 0.276282, acc.: 88.28%] [G loss: 3.460091]\n",
      "epoch:49 step:38745 [D loss: 0.311570, acc.: 85.16%] [G loss: 3.487876]\n",
      "epoch:49 step:38746 [D loss: 0.280751, acc.: 89.06%] [G loss: 3.018633]\n",
      "epoch:49 step:38747 [D loss: 0.301123, acc.: 87.50%] [G loss: 3.945086]\n",
      "epoch:49 step:38748 [D loss: 0.235188, acc.: 88.28%] [G loss: 3.242653]\n",
      "epoch:49 step:38749 [D loss: 0.224942, acc.: 91.41%] [G loss: 3.600885]\n",
      "epoch:49 step:38750 [D loss: 0.415221, acc.: 77.34%] [G loss: 3.596336]\n",
      "epoch:49 step:38751 [D loss: 0.421057, acc.: 78.91%] [G loss: 3.543938]\n",
      "epoch:49 step:38752 [D loss: 0.285122, acc.: 86.72%] [G loss: 3.494008]\n",
      "epoch:49 step:38753 [D loss: 0.355004, acc.: 84.38%] [G loss: 3.522766]\n",
      "epoch:49 step:38754 [D loss: 0.329918, acc.: 85.94%] [G loss: 5.803441]\n",
      "epoch:49 step:38755 [D loss: 0.174528, acc.: 94.53%] [G loss: 3.053578]\n",
      "epoch:49 step:38756 [D loss: 0.361738, acc.: 85.16%] [G loss: 5.026045]\n",
      "epoch:49 step:38757 [D loss: 0.317097, acc.: 87.50%] [G loss: 3.181613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38758 [D loss: 0.247421, acc.: 88.28%] [G loss: 5.301094]\n",
      "epoch:49 step:38759 [D loss: 0.329545, acc.: 84.38%] [G loss: 3.011705]\n",
      "epoch:49 step:38760 [D loss: 0.284792, acc.: 85.94%] [G loss: 4.915022]\n",
      "epoch:49 step:38761 [D loss: 0.302914, acc.: 89.84%] [G loss: 2.543296]\n",
      "epoch:49 step:38762 [D loss: 0.197939, acc.: 94.53%] [G loss: 3.052564]\n",
      "epoch:49 step:38763 [D loss: 0.339851, acc.: 85.16%] [G loss: 3.810809]\n",
      "epoch:49 step:38764 [D loss: 0.302139, acc.: 89.06%] [G loss: 2.759498]\n",
      "epoch:49 step:38765 [D loss: 0.275756, acc.: 89.06%] [G loss: 3.300409]\n",
      "epoch:49 step:38766 [D loss: 0.220611, acc.: 93.75%] [G loss: 4.055134]\n",
      "epoch:49 step:38767 [D loss: 0.302296, acc.: 87.50%] [G loss: 3.061065]\n",
      "epoch:49 step:38768 [D loss: 0.235085, acc.: 90.62%] [G loss: 3.187948]\n",
      "epoch:49 step:38769 [D loss: 0.327170, acc.: 87.50%] [G loss: 2.957452]\n",
      "epoch:49 step:38770 [D loss: 0.272673, acc.: 86.72%] [G loss: 3.341798]\n",
      "epoch:49 step:38771 [D loss: 0.305643, acc.: 85.16%] [G loss: 3.607617]\n",
      "epoch:49 step:38772 [D loss: 0.315669, acc.: 85.94%] [G loss: 3.132410]\n",
      "epoch:49 step:38773 [D loss: 0.404004, acc.: 82.03%] [G loss: 2.920805]\n",
      "epoch:49 step:38774 [D loss: 0.250619, acc.: 90.62%] [G loss: 3.244615]\n",
      "epoch:49 step:38775 [D loss: 0.296781, acc.: 85.94%] [G loss: 2.719806]\n",
      "epoch:49 step:38776 [D loss: 0.341544, acc.: 81.25%] [G loss: 2.443579]\n",
      "epoch:49 step:38777 [D loss: 0.265739, acc.: 89.84%] [G loss: 4.096330]\n",
      "epoch:49 step:38778 [D loss: 0.273871, acc.: 85.94%] [G loss: 3.117229]\n",
      "epoch:49 step:38779 [D loss: 0.264649, acc.: 87.50%] [G loss: 3.238493]\n",
      "epoch:49 step:38780 [D loss: 0.276635, acc.: 89.06%] [G loss: 3.092458]\n",
      "epoch:49 step:38781 [D loss: 0.292080, acc.: 86.72%] [G loss: 3.397104]\n",
      "epoch:49 step:38782 [D loss: 0.343767, acc.: 85.16%] [G loss: 2.852142]\n",
      "epoch:49 step:38783 [D loss: 0.281561, acc.: 89.84%] [G loss: 3.523086]\n",
      "epoch:49 step:38784 [D loss: 0.353678, acc.: 85.16%] [G loss: 3.212006]\n",
      "epoch:49 step:38785 [D loss: 0.249263, acc.: 86.72%] [G loss: 3.136310]\n",
      "epoch:49 step:38786 [D loss: 0.317909, acc.: 89.84%] [G loss: 2.764253]\n",
      "epoch:49 step:38787 [D loss: 0.238049, acc.: 88.28%] [G loss: 2.602323]\n",
      "epoch:49 step:38788 [D loss: 0.248678, acc.: 89.84%] [G loss: 2.894938]\n",
      "epoch:49 step:38789 [D loss: 0.340876, acc.: 85.94%] [G loss: 3.001857]\n",
      "epoch:49 step:38790 [D loss: 0.239659, acc.: 90.62%] [G loss: 3.300370]\n",
      "epoch:49 step:38791 [D loss: 0.288602, acc.: 89.06%] [G loss: 3.234841]\n",
      "epoch:49 step:38792 [D loss: 0.316118, acc.: 85.94%] [G loss: 3.205638]\n",
      "epoch:49 step:38793 [D loss: 0.269170, acc.: 89.06%] [G loss: 3.146544]\n",
      "epoch:49 step:38794 [D loss: 0.376530, acc.: 83.59%] [G loss: 3.123170]\n",
      "epoch:49 step:38795 [D loss: 0.585155, acc.: 71.88%] [G loss: 3.618428]\n",
      "epoch:49 step:38796 [D loss: 0.322783, acc.: 85.16%] [G loss: 2.994550]\n",
      "epoch:49 step:38797 [D loss: 0.253208, acc.: 89.84%] [G loss: 3.338776]\n",
      "epoch:49 step:38798 [D loss: 0.413013, acc.: 79.69%] [G loss: 3.310329]\n",
      "epoch:49 step:38799 [D loss: 0.347869, acc.: 80.47%] [G loss: 3.225263]\n",
      "epoch:49 step:38800 [D loss: 0.334478, acc.: 86.72%] [G loss: 4.106032]\n",
      "epoch:49 step:38801 [D loss: 0.395525, acc.: 84.38%] [G loss: 3.124470]\n",
      "epoch:49 step:38802 [D loss: 0.330574, acc.: 85.16%] [G loss: 2.743549]\n",
      "epoch:49 step:38803 [D loss: 0.296015, acc.: 89.84%] [G loss: 3.032193]\n",
      "epoch:49 step:38804 [D loss: 0.255866, acc.: 92.19%] [G loss: 3.042228]\n",
      "epoch:49 step:38805 [D loss: 0.272004, acc.: 87.50%] [G loss: 4.758370]\n",
      "epoch:49 step:38806 [D loss: 0.340476, acc.: 80.47%] [G loss: 4.019712]\n",
      "epoch:49 step:38807 [D loss: 0.251229, acc.: 89.84%] [G loss: 4.633683]\n",
      "epoch:49 step:38808 [D loss: 0.183355, acc.: 94.53%] [G loss: 4.180034]\n",
      "epoch:49 step:38809 [D loss: 0.283184, acc.: 88.28%] [G loss: 3.452109]\n",
      "epoch:49 step:38810 [D loss: 0.214261, acc.: 91.41%] [G loss: 4.232292]\n",
      "epoch:49 step:38811 [D loss: 0.316052, acc.: 84.38%] [G loss: 4.035931]\n",
      "epoch:49 step:38812 [D loss: 0.257872, acc.: 89.06%] [G loss: 3.170145]\n",
      "epoch:49 step:38813 [D loss: 0.315017, acc.: 85.94%] [G loss: 3.772677]\n",
      "epoch:49 step:38814 [D loss: 0.252655, acc.: 91.41%] [G loss: 3.329738]\n",
      "epoch:49 step:38815 [D loss: 0.225450, acc.: 92.97%] [G loss: 3.115806]\n",
      "epoch:49 step:38816 [D loss: 0.335605, acc.: 85.16%] [G loss: 3.618255]\n",
      "epoch:49 step:38817 [D loss: 0.177444, acc.: 93.75%] [G loss: 3.312695]\n",
      "epoch:49 step:38818 [D loss: 0.238065, acc.: 92.97%] [G loss: 2.842315]\n",
      "epoch:49 step:38819 [D loss: 0.228744, acc.: 89.84%] [G loss: 3.415271]\n",
      "epoch:49 step:38820 [D loss: 0.298868, acc.: 84.38%] [G loss: 2.867883]\n",
      "epoch:49 step:38821 [D loss: 0.312556, acc.: 89.84%] [G loss: 5.813411]\n",
      "epoch:49 step:38822 [D loss: 0.478391, acc.: 82.03%] [G loss: 4.309787]\n",
      "epoch:49 step:38823 [D loss: 0.340073, acc.: 84.38%] [G loss: 2.773628]\n",
      "epoch:49 step:38824 [D loss: 0.323634, acc.: 89.06%] [G loss: 3.845929]\n",
      "epoch:49 step:38825 [D loss: 0.382761, acc.: 82.03%] [G loss: 2.954169]\n",
      "epoch:49 step:38826 [D loss: 0.325904, acc.: 85.16%] [G loss: 3.823821]\n",
      "epoch:49 step:38827 [D loss: 0.238123, acc.: 90.62%] [G loss: 3.429435]\n",
      "epoch:49 step:38828 [D loss: 0.239301, acc.: 92.19%] [G loss: 3.165470]\n",
      "epoch:49 step:38829 [D loss: 0.365185, acc.: 82.81%] [G loss: 2.941664]\n",
      "epoch:49 step:38830 [D loss: 0.350207, acc.: 84.38%] [G loss: 3.608638]\n",
      "epoch:49 step:38831 [D loss: 0.384166, acc.: 81.25%] [G loss: 3.446352]\n",
      "epoch:49 step:38832 [D loss: 0.205788, acc.: 90.62%] [G loss: 3.963361]\n",
      "epoch:49 step:38833 [D loss: 0.211449, acc.: 92.19%] [G loss: 3.767404]\n",
      "epoch:49 step:38834 [D loss: 0.245569, acc.: 89.06%] [G loss: 2.662323]\n",
      "epoch:49 step:38835 [D loss: 0.356024, acc.: 85.16%] [G loss: 3.082224]\n",
      "epoch:49 step:38836 [D loss: 0.221933, acc.: 93.75%] [G loss: 2.638925]\n",
      "epoch:49 step:38837 [D loss: 0.285833, acc.: 89.06%] [G loss: 3.035641]\n",
      "epoch:49 step:38838 [D loss: 0.305446, acc.: 85.16%] [G loss: 2.753797]\n",
      "epoch:49 step:38839 [D loss: 0.218330, acc.: 91.41%] [G loss: 3.578835]\n",
      "epoch:49 step:38840 [D loss: 0.283485, acc.: 86.72%] [G loss: 5.613931]\n",
      "epoch:49 step:38841 [D loss: 0.380567, acc.: 81.25%] [G loss: 4.694136]\n",
      "epoch:49 step:38842 [D loss: 0.400535, acc.: 88.28%] [G loss: 5.921207]\n",
      "epoch:49 step:38843 [D loss: 0.386991, acc.: 85.16%] [G loss: 4.554971]\n",
      "epoch:49 step:38844 [D loss: 0.334636, acc.: 82.03%] [G loss: 5.568993]\n",
      "epoch:49 step:38845 [D loss: 0.359414, acc.: 84.38%] [G loss: 4.908793]\n",
      "epoch:49 step:38846 [D loss: 0.265514, acc.: 87.50%] [G loss: 4.236113]\n",
      "epoch:49 step:38847 [D loss: 0.188544, acc.: 95.31%] [G loss: 3.305824]\n",
      "epoch:49 step:38848 [D loss: 0.211935, acc.: 92.19%] [G loss: 3.660017]\n",
      "epoch:49 step:38849 [D loss: 0.275303, acc.: 88.28%] [G loss: 3.106273]\n",
      "epoch:49 step:38850 [D loss: 0.284737, acc.: 89.84%] [G loss: 4.018984]\n",
      "epoch:49 step:38851 [D loss: 0.266946, acc.: 85.94%] [G loss: 3.373164]\n",
      "epoch:49 step:38852 [D loss: 0.348473, acc.: 85.16%] [G loss: 3.924839]\n",
      "epoch:49 step:38853 [D loss: 0.310252, acc.: 87.50%] [G loss: 3.599359]\n",
      "epoch:49 step:38854 [D loss: 0.246830, acc.: 89.84%] [G loss: 3.646924]\n",
      "epoch:49 step:38855 [D loss: 0.331962, acc.: 85.16%] [G loss: 2.323765]\n",
      "epoch:49 step:38856 [D loss: 0.251893, acc.: 89.06%] [G loss: 3.751515]\n",
      "epoch:49 step:38857 [D loss: 0.240957, acc.: 88.28%] [G loss: 3.123719]\n",
      "epoch:49 step:38858 [D loss: 0.231765, acc.: 91.41%] [G loss: 3.362485]\n",
      "epoch:49 step:38859 [D loss: 0.331803, acc.: 84.38%] [G loss: 3.792198]\n",
      "epoch:49 step:38860 [D loss: 0.322053, acc.: 83.59%] [G loss: 4.062832]\n",
      "epoch:49 step:38861 [D loss: 0.332965, acc.: 85.94%] [G loss: 3.255857]\n",
      "epoch:49 step:38862 [D loss: 0.369310, acc.: 80.47%] [G loss: 4.992666]\n",
      "epoch:49 step:38863 [D loss: 0.275180, acc.: 89.06%] [G loss: 3.219956]\n",
      "epoch:49 step:38864 [D loss: 0.304035, acc.: 87.50%] [G loss: 4.971413]\n",
      "epoch:49 step:38865 [D loss: 0.453479, acc.: 76.56%] [G loss: 4.108168]\n",
      "epoch:49 step:38866 [D loss: 0.376616, acc.: 82.03%] [G loss: 3.142411]\n",
      "epoch:49 step:38867 [D loss: 0.185714, acc.: 92.19%] [G loss: 3.605189]\n",
      "epoch:49 step:38868 [D loss: 0.387913, acc.: 85.94%] [G loss: 3.554737]\n",
      "epoch:49 step:38869 [D loss: 0.299856, acc.: 88.28%] [G loss: 3.325930]\n",
      "epoch:49 step:38870 [D loss: 0.287340, acc.: 85.94%] [G loss: 3.481157]\n",
      "epoch:49 step:38871 [D loss: 0.299988, acc.: 88.28%] [G loss: 3.190978]\n",
      "epoch:49 step:38872 [D loss: 0.295662, acc.: 85.94%] [G loss: 2.892172]\n",
      "epoch:49 step:38873 [D loss: 0.212837, acc.: 92.97%] [G loss: 3.190523]\n",
      "epoch:49 step:38874 [D loss: 0.344959, acc.: 82.03%] [G loss: 3.631366]\n",
      "epoch:49 step:38875 [D loss: 0.338822, acc.: 81.25%] [G loss: 2.737633]\n",
      "epoch:49 step:38876 [D loss: 0.272738, acc.: 87.50%] [G loss: 3.246937]\n",
      "epoch:49 step:38877 [D loss: 0.234584, acc.: 91.41%] [G loss: 2.842244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38878 [D loss: 0.332676, acc.: 85.16%] [G loss: 3.262698]\n",
      "epoch:49 step:38879 [D loss: 0.392115, acc.: 81.25%] [G loss: 2.654981]\n",
      "epoch:49 step:38880 [D loss: 0.356029, acc.: 82.03%] [G loss: 3.921971]\n",
      "epoch:49 step:38881 [D loss: 0.284003, acc.: 89.84%] [G loss: 2.627433]\n",
      "epoch:49 step:38882 [D loss: 0.309366, acc.: 86.72%] [G loss: 2.764746]\n",
      "epoch:49 step:38883 [D loss: 0.192204, acc.: 93.75%] [G loss: 3.067501]\n",
      "epoch:49 step:38884 [D loss: 0.333305, acc.: 85.94%] [G loss: 2.862129]\n",
      "epoch:49 step:38885 [D loss: 0.247539, acc.: 88.28%] [G loss: 3.412580]\n",
      "epoch:49 step:38886 [D loss: 0.386051, acc.: 84.38%] [G loss: 3.462824]\n",
      "epoch:49 step:38887 [D loss: 0.320284, acc.: 84.38%] [G loss: 3.896941]\n",
      "epoch:49 step:38888 [D loss: 0.249221, acc.: 91.41%] [G loss: 3.090870]\n",
      "epoch:49 step:38889 [D loss: 0.274817, acc.: 89.06%] [G loss: 3.858710]\n",
      "epoch:49 step:38890 [D loss: 0.181114, acc.: 92.97%] [G loss: 3.655122]\n",
      "epoch:49 step:38891 [D loss: 0.257232, acc.: 90.62%] [G loss: 3.524651]\n",
      "epoch:49 step:38892 [D loss: 0.245095, acc.: 89.06%] [G loss: 3.191417]\n",
      "epoch:49 step:38893 [D loss: 0.342423, acc.: 85.94%] [G loss: 3.552338]\n",
      "epoch:49 step:38894 [D loss: 0.315309, acc.: 87.50%] [G loss: 3.939848]\n",
      "epoch:49 step:38895 [D loss: 0.290685, acc.: 85.94%] [G loss: 4.911730]\n",
      "epoch:49 step:38896 [D loss: 0.240434, acc.: 87.50%] [G loss: 3.467921]\n",
      "epoch:49 step:38897 [D loss: 0.341090, acc.: 85.94%] [G loss: 2.926723]\n",
      "epoch:49 step:38898 [D loss: 0.322593, acc.: 85.16%] [G loss: 3.063839]\n",
      "epoch:49 step:38899 [D loss: 0.243421, acc.: 92.19%] [G loss: 2.437938]\n",
      "epoch:49 step:38900 [D loss: 0.289649, acc.: 89.84%] [G loss: 3.018047]\n",
      "epoch:49 step:38901 [D loss: 0.250068, acc.: 89.06%] [G loss: 2.923992]\n",
      "epoch:49 step:38902 [D loss: 0.363706, acc.: 83.59%] [G loss: 3.899460]\n",
      "epoch:49 step:38903 [D loss: 0.484222, acc.: 82.81%] [G loss: 6.246375]\n",
      "epoch:49 step:38904 [D loss: 1.113129, acc.: 70.31%] [G loss: 9.059551]\n",
      "epoch:49 step:38905 [D loss: 2.389924, acc.: 57.03%] [G loss: 3.333580]\n",
      "epoch:49 step:38906 [D loss: 0.509217, acc.: 75.78%] [G loss: 4.952824]\n",
      "epoch:49 step:38907 [D loss: 0.464364, acc.: 81.25%] [G loss: 4.082334]\n",
      "epoch:49 step:38908 [D loss: 0.371227, acc.: 86.72%] [G loss: 4.166251]\n",
      "epoch:49 step:38909 [D loss: 0.304837, acc.: 89.84%] [G loss: 3.817334]\n",
      "epoch:49 step:38910 [D loss: 0.275089, acc.: 85.94%] [G loss: 2.912008]\n",
      "epoch:49 step:38911 [D loss: 0.278992, acc.: 86.72%] [G loss: 4.246859]\n",
      "epoch:49 step:38912 [D loss: 0.373722, acc.: 82.81%] [G loss: 2.563863]\n",
      "epoch:49 step:38913 [D loss: 0.262513, acc.: 84.38%] [G loss: 3.197719]\n",
      "epoch:49 step:38914 [D loss: 0.194808, acc.: 93.75%] [G loss: 4.250529]\n",
      "epoch:49 step:38915 [D loss: 0.316426, acc.: 85.16%] [G loss: 4.785526]\n",
      "epoch:49 step:38916 [D loss: 0.287921, acc.: 85.94%] [G loss: 3.501174]\n",
      "epoch:49 step:38917 [D loss: 0.332835, acc.: 82.03%] [G loss: 2.896629]\n",
      "epoch:49 step:38918 [D loss: 0.210699, acc.: 92.97%] [G loss: 2.751385]\n",
      "epoch:49 step:38919 [D loss: 0.335250, acc.: 85.16%] [G loss: 3.682367]\n",
      "epoch:49 step:38920 [D loss: 0.330371, acc.: 85.16%] [G loss: 3.255952]\n",
      "epoch:49 step:38921 [D loss: 0.255762, acc.: 89.84%] [G loss: 3.440538]\n",
      "epoch:49 step:38922 [D loss: 0.296804, acc.: 85.16%] [G loss: 3.590236]\n",
      "epoch:49 step:38923 [D loss: 0.292412, acc.: 88.28%] [G loss: 2.905504]\n",
      "epoch:49 step:38924 [D loss: 0.315142, acc.: 83.59%] [G loss: 3.421446]\n",
      "epoch:49 step:38925 [D loss: 0.246162, acc.: 92.97%] [G loss: 2.669933]\n",
      "epoch:49 step:38926 [D loss: 0.353226, acc.: 85.16%] [G loss: 3.546029]\n",
      "epoch:49 step:38927 [D loss: 0.383899, acc.: 80.47%] [G loss: 2.835812]\n",
      "epoch:49 step:38928 [D loss: 0.260912, acc.: 87.50%] [G loss: 2.761403]\n",
      "epoch:49 step:38929 [D loss: 0.309167, acc.: 85.16%] [G loss: 2.982534]\n",
      "epoch:49 step:38930 [D loss: 0.352005, acc.: 85.16%] [G loss: 3.425183]\n",
      "epoch:49 step:38931 [D loss: 0.305857, acc.: 84.38%] [G loss: 3.126462]\n",
      "epoch:49 step:38932 [D loss: 0.333228, acc.: 88.28%] [G loss: 2.875884]\n",
      "epoch:49 step:38933 [D loss: 0.440717, acc.: 80.47%] [G loss: 3.090051]\n",
      "epoch:49 step:38934 [D loss: 0.368104, acc.: 82.03%] [G loss: 4.025298]\n",
      "epoch:49 step:38935 [D loss: 0.303624, acc.: 89.06%] [G loss: 2.373939]\n",
      "epoch:49 step:38936 [D loss: 0.299041, acc.: 88.28%] [G loss: 2.590239]\n",
      "epoch:49 step:38937 [D loss: 0.291870, acc.: 87.50%] [G loss: 2.963682]\n",
      "epoch:49 step:38938 [D loss: 0.313847, acc.: 87.50%] [G loss: 2.911602]\n",
      "epoch:49 step:38939 [D loss: 0.320208, acc.: 84.38%] [G loss: 2.587420]\n",
      "epoch:49 step:38940 [D loss: 0.315117, acc.: 83.59%] [G loss: 2.801508]\n",
      "epoch:49 step:38941 [D loss: 0.266266, acc.: 89.06%] [G loss: 2.964539]\n",
      "epoch:49 step:38942 [D loss: 0.379822, acc.: 83.59%] [G loss: 2.510362]\n",
      "epoch:49 step:38943 [D loss: 0.315773, acc.: 83.59%] [G loss: 2.620392]\n",
      "epoch:49 step:38944 [D loss: 0.289659, acc.: 89.84%] [G loss: 2.840589]\n",
      "epoch:49 step:38945 [D loss: 0.365333, acc.: 82.81%] [G loss: 3.857412]\n",
      "epoch:49 step:38946 [D loss: 0.275761, acc.: 87.50%] [G loss: 2.951658]\n",
      "epoch:49 step:38947 [D loss: 0.275865, acc.: 89.06%] [G loss: 3.830748]\n",
      "epoch:49 step:38948 [D loss: 0.310079, acc.: 85.94%] [G loss: 3.153439]\n",
      "epoch:49 step:38949 [D loss: 0.218238, acc.: 92.97%] [G loss: 3.755945]\n",
      "epoch:49 step:38950 [D loss: 0.207066, acc.: 89.06%] [G loss: 3.237457]\n",
      "epoch:49 step:38951 [D loss: 0.309412, acc.: 87.50%] [G loss: 4.921227]\n",
      "epoch:49 step:38952 [D loss: 0.601329, acc.: 75.78%] [G loss: 3.080840]\n",
      "epoch:49 step:38953 [D loss: 0.273653, acc.: 89.84%] [G loss: 3.468507]\n",
      "epoch:49 step:38954 [D loss: 0.373986, acc.: 82.03%] [G loss: 4.099475]\n",
      "epoch:49 step:38955 [D loss: 0.290175, acc.: 85.94%] [G loss: 3.963521]\n",
      "epoch:49 step:38956 [D loss: 0.261220, acc.: 87.50%] [G loss: 3.132018]\n",
      "epoch:49 step:38957 [D loss: 0.353515, acc.: 82.03%] [G loss: 3.480732]\n",
      "epoch:49 step:38958 [D loss: 0.240053, acc.: 89.84%] [G loss: 3.370139]\n",
      "epoch:49 step:38959 [D loss: 0.301207, acc.: 89.84%] [G loss: 3.525886]\n",
      "epoch:49 step:38960 [D loss: 0.317738, acc.: 88.28%] [G loss: 2.654876]\n",
      "epoch:49 step:38961 [D loss: 0.350257, acc.: 85.94%] [G loss: 3.193171]\n",
      "epoch:49 step:38962 [D loss: 0.254979, acc.: 89.84%] [G loss: 3.184194]\n",
      "epoch:49 step:38963 [D loss: 0.260168, acc.: 88.28%] [G loss: 3.039834]\n",
      "epoch:49 step:38964 [D loss: 0.243294, acc.: 89.06%] [G loss: 2.571435]\n",
      "epoch:49 step:38965 [D loss: 0.295048, acc.: 86.72%] [G loss: 2.226285]\n",
      "epoch:49 step:38966 [D loss: 0.357122, acc.: 83.59%] [G loss: 2.483521]\n",
      "epoch:49 step:38967 [D loss: 0.390699, acc.: 85.94%] [G loss: 3.072898]\n",
      "epoch:49 step:38968 [D loss: 0.304252, acc.: 87.50%] [G loss: 3.084677]\n",
      "epoch:49 step:38969 [D loss: 0.276241, acc.: 89.84%] [G loss: 2.900846]\n",
      "epoch:49 step:38970 [D loss: 0.376679, acc.: 81.25%] [G loss: 2.915896]\n",
      "epoch:49 step:38971 [D loss: 0.217052, acc.: 91.41%] [G loss: 2.483150]\n",
      "epoch:49 step:38972 [D loss: 0.347613, acc.: 82.81%] [G loss: 3.193465]\n",
      "epoch:49 step:38973 [D loss: 0.337461, acc.: 82.81%] [G loss: 2.565078]\n",
      "epoch:49 step:38974 [D loss: 0.270416, acc.: 86.72%] [G loss: 2.838569]\n",
      "epoch:49 step:38975 [D loss: 0.367972, acc.: 83.59%] [G loss: 2.627097]\n",
      "epoch:49 step:38976 [D loss: 0.406571, acc.: 78.91%] [G loss: 3.222635]\n",
      "epoch:49 step:38977 [D loss: 0.279913, acc.: 88.28%] [G loss: 3.805757]\n",
      "epoch:49 step:38978 [D loss: 0.316467, acc.: 85.94%] [G loss: 2.997385]\n",
      "epoch:49 step:38979 [D loss: 0.290478, acc.: 89.06%] [G loss: 3.523727]\n",
      "epoch:49 step:38980 [D loss: 0.234805, acc.: 89.84%] [G loss: 3.074616]\n",
      "epoch:49 step:38981 [D loss: 0.222725, acc.: 89.84%] [G loss: 3.155191]\n",
      "epoch:49 step:38982 [D loss: 0.345512, acc.: 83.59%] [G loss: 4.138842]\n",
      "epoch:49 step:38983 [D loss: 0.305349, acc.: 85.94%] [G loss: 3.838798]\n",
      "epoch:49 step:38984 [D loss: 0.340373, acc.: 83.59%] [G loss: 3.140677]\n",
      "epoch:49 step:38985 [D loss: 0.414444, acc.: 82.03%] [G loss: 3.890011]\n",
      "epoch:49 step:38986 [D loss: 0.265212, acc.: 89.06%] [G loss: 2.786767]\n",
      "epoch:49 step:38987 [D loss: 0.305938, acc.: 85.16%] [G loss: 5.980648]\n",
      "epoch:49 step:38988 [D loss: 0.335709, acc.: 82.81%] [G loss: 3.548820]\n",
      "epoch:49 step:38989 [D loss: 0.307044, acc.: 86.72%] [G loss: 4.283838]\n",
      "epoch:49 step:38990 [D loss: 0.303234, acc.: 87.50%] [G loss: 4.019420]\n",
      "epoch:49 step:38991 [D loss: 0.224292, acc.: 89.06%] [G loss: 4.881907]\n",
      "epoch:49 step:38992 [D loss: 0.183275, acc.: 91.41%] [G loss: 4.010346]\n",
      "epoch:49 step:38993 [D loss: 0.234842, acc.: 88.28%] [G loss: 3.296672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38994 [D loss: 0.181437, acc.: 91.41%] [G loss: 3.871508]\n",
      "epoch:49 step:38995 [D loss: 0.298179, acc.: 85.94%] [G loss: 2.508578]\n",
      "epoch:49 step:38996 [D loss: 0.259960, acc.: 88.28%] [G loss: 3.618102]\n",
      "epoch:49 step:38997 [D loss: 0.299381, acc.: 84.38%] [G loss: 3.600430]\n",
      "epoch:49 step:38998 [D loss: 0.252123, acc.: 90.62%] [G loss: 3.261596]\n",
      "epoch:49 step:38999 [D loss: 0.264440, acc.: 87.50%] [G loss: 3.501021]\n",
      "epoch:49 step:39000 [D loss: 0.311698, acc.: 86.72%] [G loss: 2.892127]\n",
      "epoch:49 step:39001 [D loss: 0.315091, acc.: 85.94%] [G loss: 4.002990]\n",
      "epoch:49 step:39002 [D loss: 0.275611, acc.: 88.28%] [G loss: 3.520416]\n",
      "epoch:49 step:39003 [D loss: 0.312554, acc.: 85.94%] [G loss: 3.695688]\n",
      "epoch:49 step:39004 [D loss: 0.242845, acc.: 92.19%] [G loss: 2.795382]\n",
      "epoch:49 step:39005 [D loss: 0.252721, acc.: 85.94%] [G loss: 3.677499]\n",
      "epoch:49 step:39006 [D loss: 0.336928, acc.: 87.50%] [G loss: 2.727203]\n",
      "epoch:49 step:39007 [D loss: 0.385652, acc.: 77.34%] [G loss: 3.288568]\n",
      "epoch:49 step:39008 [D loss: 0.242964, acc.: 89.84%] [G loss: 3.395106]\n",
      "epoch:49 step:39009 [D loss: 0.328694, acc.: 83.59%] [G loss: 3.305592]\n",
      "epoch:49 step:39010 [D loss: 0.271431, acc.: 86.72%] [G loss: 2.494626]\n",
      "epoch:49 step:39011 [D loss: 0.300633, acc.: 85.16%] [G loss: 2.646488]\n",
      "epoch:49 step:39012 [D loss: 0.200390, acc.: 93.75%] [G loss: 3.452758]\n",
      "epoch:49 step:39013 [D loss: 0.334537, acc.: 81.25%] [G loss: 2.710798]\n",
      "epoch:49 step:39014 [D loss: 0.201602, acc.: 92.97%] [G loss: 3.058865]\n",
      "epoch:49 step:39015 [D loss: 0.341405, acc.: 85.16%] [G loss: 3.347633]\n",
      "epoch:49 step:39016 [D loss: 0.278553, acc.: 89.84%] [G loss: 2.729815]\n",
      "epoch:49 step:39017 [D loss: 0.266431, acc.: 89.84%] [G loss: 2.745716]\n",
      "epoch:49 step:39018 [D loss: 0.367808, acc.: 84.38%] [G loss: 2.937165]\n",
      "epoch:49 step:39019 [D loss: 0.251791, acc.: 87.50%] [G loss: 3.354851]\n",
      "epoch:49 step:39020 [D loss: 0.265041, acc.: 89.06%] [G loss: 3.038685]\n",
      "epoch:49 step:39021 [D loss: 0.353273, acc.: 84.38%] [G loss: 3.244710]\n",
      "epoch:49 step:39022 [D loss: 0.337124, acc.: 84.38%] [G loss: 2.921703]\n",
      "epoch:49 step:39023 [D loss: 0.274293, acc.: 86.72%] [G loss: 3.892668]\n",
      "epoch:49 step:39024 [D loss: 0.257341, acc.: 91.41%] [G loss: 2.677994]\n",
      "epoch:49 step:39025 [D loss: 0.277032, acc.: 87.50%] [G loss: 2.827314]\n",
      "epoch:49 step:39026 [D loss: 0.238596, acc.: 89.84%] [G loss: 3.023898]\n",
      "epoch:49 step:39027 [D loss: 0.451169, acc.: 77.34%] [G loss: 3.688350]\n",
      "epoch:49 step:39028 [D loss: 0.327025, acc.: 82.03%] [G loss: 2.528844]\n",
      "epoch:49 step:39029 [D loss: 0.309566, acc.: 89.84%] [G loss: 3.773515]\n",
      "epoch:49 step:39030 [D loss: 0.271303, acc.: 89.84%] [G loss: 3.143231]\n",
      "epoch:49 step:39031 [D loss: 0.297351, acc.: 89.84%] [G loss: 2.878376]\n",
      "epoch:49 step:39032 [D loss: 0.386431, acc.: 82.03%] [G loss: 3.457887]\n",
      "epoch:49 step:39033 [D loss: 0.293189, acc.: 85.16%] [G loss: 3.098991]\n",
      "epoch:49 step:39034 [D loss: 0.315187, acc.: 85.16%] [G loss: 3.335314]\n",
      "epoch:49 step:39035 [D loss: 0.350992, acc.: 85.94%] [G loss: 3.717232]\n",
      "epoch:49 step:39036 [D loss: 0.220747, acc.: 91.41%] [G loss: 4.118558]\n",
      "epoch:49 step:39037 [D loss: 0.345289, acc.: 81.25%] [G loss: 3.110703]\n",
      "epoch:49 step:39038 [D loss: 0.353509, acc.: 84.38%] [G loss: 3.755687]\n",
      "epoch:49 step:39039 [D loss: 0.283274, acc.: 86.72%] [G loss: 4.330562]\n",
      "epoch:49 step:39040 [D loss: 0.465413, acc.: 77.34%] [G loss: 4.528640]\n",
      "epoch:49 step:39041 [D loss: 0.378386, acc.: 78.12%] [G loss: 4.704461]\n",
      "epoch:49 step:39042 [D loss: 0.366084, acc.: 84.38%] [G loss: 3.586134]\n",
      "epoch:49 step:39043 [D loss: 0.302905, acc.: 87.50%] [G loss: 4.027786]\n",
      "epoch:49 step:39044 [D loss: 0.364539, acc.: 84.38%] [G loss: 3.463590]\n",
      "epoch:49 step:39045 [D loss: 0.290166, acc.: 89.06%] [G loss: 3.309810]\n",
      "epoch:49 step:39046 [D loss: 0.386368, acc.: 80.47%] [G loss: 3.663248]\n",
      "epoch:49 step:39047 [D loss: 0.285089, acc.: 85.16%] [G loss: 2.987379]\n",
      "epoch:49 step:39048 [D loss: 0.367343, acc.: 82.03%] [G loss: 3.492220]\n",
      "epoch:49 step:39049 [D loss: 0.305568, acc.: 85.94%] [G loss: 3.274117]\n",
      "epoch:49 step:39050 [D loss: 0.413723, acc.: 80.47%] [G loss: 3.215991]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as Data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, imgs, transform=None):\n",
    "        # super().__init__()\n",
    "        self.imgs = imgs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = torch.from_numpy(img)\n",
    "        img=img.reshape([3,32,32])\n",
    "        return img\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import model\n",
    "import torch.nn.functional as F\n",
    "model = model.cifar10(128)\n",
    "model.load_state_dict(torch.load('./log/default/best-85.pth'))\n",
    "model.cuda()\n",
    "def get_mean_var(y_logit):\n",
    "    y_logit=np.abs(y_logit-0.1)\n",
    "    return np.mean(y_logit,axis=1)\n",
    "\n",
    "def get_possibility(images):\n",
    "    x_dataset = MyDataset(images)\n",
    "    # print(x_dataset[0].shape)\n",
    "    x_real_loader = Data.DataLoader(dataset=x_dataset, batch_size=100, shuffle=True)\n",
    "    y_logits = []\n",
    "    for i, data in enumerate(x_real_loader):\n",
    "        # indx_target = target.clone()\n",
    "        data = data.cuda()\n",
    "        data = Variable(data, volatile=True)\n",
    "        output = model(data)\n",
    "        pred = F.softmax(output).cpu().detach().numpy()\n",
    "        y_logits += [i for i in pred]\n",
    "        y_logits=np.array(y_logits)\n",
    "    return y_logits\n",
    "import os\n",
    "if not os.path.isdir('saved_models_{}'.format('gan')):\n",
    "    os.mkdir('saved_models_{}'.format('gan'))\n",
    "f = open('saved_models_{}/log_collapse1.txt'.format('gan'), mode='w')\n",
    "import torch.utils.data as Data\n",
    "import cv2\n",
    "\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam #optimizer of keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 32\n",
    "        self.img_cols = 32\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels) #shape of image\n",
    "        self.latent_dim = 100\n",
    "        self.x = []\n",
    "        self.y = np.zeros((31, 1), dtype=np.int)\n",
    "        self.y = list(self.y)\n",
    "        for i in range(31):\n",
    "            self.y[i] = []\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5) #optimizer of gan\n",
    "\n",
    "        # Build and compile the discriminator,only to keras\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))  #Input():用来实例化一个keras张量\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (X_test, _) = cifar10.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "        # X_train = np.expand_dims(X_train, axis=3)  #expand_dims用于扩充数组形状\n",
    "        print(np.shape(X_train))\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "        steps = []\n",
    "        values = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                imgs = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "                # Train the generator (to have the discriminator label samples as valid)\n",
    "                g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"epoch:%d step:%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (\n",
    "                    epoch, global_step, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "                if global_step % sample_interval == 0:\n",
    "                    self.mode_drop(epoch, global_step)\n",
    "                    \n",
    "\n",
    "    def mode_drop(self, epoch, global_step):\n",
    "        r, c = 10, 1000\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        # sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise])\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        y_logits = get_possibility(gen_imgs)\n",
    "        metrics = get_mean_var(y_logits)\n",
    "\n",
    "\n",
    "        f.writelines('\\n')\n",
    "        f.writelines('global_step:' + str(global_step))\n",
    "        f.writelines('\\n')\n",
    "        f.writelines(' %.8f ' % (i) for i in metrics)\n",
    "        f.writelines('\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    gan.train(epochs=50, batch_size=64, sample_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
